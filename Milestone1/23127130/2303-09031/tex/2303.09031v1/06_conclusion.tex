\section{Conclusion}
We present a simple approach for planning from pixels building on the planning commonsense knowledge acquired by large language models.
Compared to prior work, which indirectly incorporates observation information by captions or affordance, our approach is simpler, does not use external data, and benefits from directly coupling the planner language model and environment observations.
Experimentally, we showed our approach performs better than these prior methods on two embodied agent benchmarks.