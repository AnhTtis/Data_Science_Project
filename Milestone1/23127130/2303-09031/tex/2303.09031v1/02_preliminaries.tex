\section{Preliminary: Prompt Tuning}
\label{sec:prelim}


Given a sequence of tokens $x_1,\ldots,x_t$, an auto-regressive language model predicts a probability distribution over the next token $p_\text{LM}(\cdot|x_1,\ldots,x_t)$.
While a PLM expects to see natural language tokens in its context, the model can be extended to process a sequence of input embeddings. %
The input layer of a PLM converts tokens $x_1,\ldots,x_t$ into token embeddings $e_1,\ldots,e_t$ which are passed on to subsequent layers.
Soft prompt tuning introduces additional tunable embeddings $p_1,\ldots,p_k$ in the input layer $p_\text{LM}(\cdot|e_1,\ldots,e_t,p_1,\ldots,p_k)$\footnote{In an abuse of notation, we will use $p_\text{LM}$ with token inputs or embedding inputs interchangeably.} which can be optimized with respect to a target training objective using gradient descent.
In addition, for cross modal reasoning tasks the soft embeddings $p_i$ can be a function of data-modalities other than text such as images (e.g., $p_i = f_i(v;\theta)$ where $v$ is an image and $\theta$ are trainable parameters).
We use this approach to augment the language model context with visual observations.