\section{Approach}


\paragraph{Problem Setting.}
We assume a goal-based MDP setting, parameterized by $\mathcal{M} = \left(\mathcal{S}, \mathcal{A}, \mathcal{G}, P, R_\mathcal{G}\right)$: a state space $\mathcal{S}$, action space $\mathcal{A}$, a goal space $\mathcal{G}$, transition probabilities $P$, and reward $R_\mathcal{G}$. %
The planner is given $N$ expert demonstrations 
$\mathcal{D} = \{ (g^{(i)}, o_0^{(i)}, a_0^{(i)}, o_1^{(i)}, a_1^{(i)}, \dots, o_T^{(i)}, a_T^{(i)}) \}_{i=1}^N$
where goals $g^{(i)}\in \mathcal{G}$ and actions $a^{(i)} \in \mathcal{A}$ are available as text and observations $o^{(i)} \in \mathbb{R}^{H \times W \times C}$ are images of size $H\times W\times C$.
Further, we do not assume the list of possible actions available to the agent is known, or any pretrained admissibility or affordance function is known. 
Given goal description $g$, past actions $a_1, \ldots, a_{t-1}$ and observations $o_1, \ldots, o_t$, we seek to build a policy $\pi$ which models the next action probability $\pi(a_t|g,a_{1\cdots t-1},o_{1\cdots t})$.


\paragraph{\oursfull.}
If goal description, actions and observations are available in the form of discrete token sequences, predicting the next action is similar to a language modeling task and a PLM can be fine-tuned for next action prediction: maximize $\log p_\text{LM} ( a_t \mid \cxt_t)$, where  
$\cxt_t = \concat\left(g, o_1, a_1, o_2, a_2, \dots, a_{t-1}, o_t\right)$.
However, observations may not be available in the form of text or discrete tokens in practice and we attempt to tackle this scenario.

As we discuss in \Cref{sec:prelim}, PLMs are capable of processing a sequence of embeddings (which may not necessarily correspond to actual text tokens).
The context can be re-written in terms of embedding sequences as 
$\cxt_t = \concat\left(g^e, o^e_1, a^e_1, o^e_2, a^e_2, \dots, a^e_{t-1}, o^e_t\right)$\footnote{Note that each of $g^e, o^e_i, a^e_i$ are embedding sequences and the concat operation concatenates these sequences.} 
where $g^e, o_i^e, a_i^e$ respectively represent \emph{embedding sequences} corresponding to the goal description, observations and actions.
We assume that the goal description and actions are available in the form of text and $g^e, a_i^e$ can be obtained as the corresponding token embedding sequences.

\paragraph{Observation Encoder.}
To obtain observation embeddings $o_i^e$, we propose to learn an observation encoder $f: \mathbb{R}^{H \times W \times C} \to \mathbb{R}^{m \times E}$. $f$ maps visual observations $o$ to a sequence of $m$ embeddings each of size $E$, where $m$ is a hyperparameter and $E$ is the PLM's embedding dimensionality. %
In our experiments we consider an observation encoder of the form shown in \Cref{eq:obsenc} where $f_\text{pretrained}$ is a pre-trained visual encoder and $f_\text{FFN}$ is a feedforward network.
\begin{equation}
\fobsembed(o) = f_\text{FFN}(f_\text{pretrained}(o))
\label{eq:obsenc}
\end{equation}

\paragraph{Training Objective.}
We learn to model the next action given previous actions, observations, and the goal. %
Similar to prior approaches~\cite{micheli-fleuret-2021-language, huang2022language}, we define the loss as in \Cref{eq:trainobj},
where $\cxt^{(i)}_t$ is a concatenation of goal, action and observation embeddings as previously described.
\begin{equation}
\mathcal{L}_\mathcal{D} = - \frac{1}{N} \sum_{i, t} \log p_\text{LM} (a^{(i)}_t \mid \cxt^{(i)}_t)
\label{eq:trainobj}
\end{equation}












