\section{Introduction}

The ability to reason about plans is critical for performing long-horizon tasks \citep{erol1996hierarchical, sohn2018hierarchical, sharma-etal-2022-skill}, compositional generalization \citep{corona-etal-2021-modular} and generalization to unseen tasks and environments \citep{shridhar2020alfred}.
Consider a simple long-horizon planning scenario where a robot is tasked with preparing a meal and serving it on the table. 
This presents a non-trivial planning problem since the agent needs to understand the sequence of operations required to perform the task and search for the relevant objects in the unfamiliar environment by interacting with various objects. %



Large language models have been recently shown to possess commonsense knowledge about the world such as object affordances and physical dynamics \citep{ouyang2022training,chowdhery2022palm}.
Early approaches considered text based environments and fine-tuned PLMs to predict actions given the history of past observations and actions \citep{jansen-2020-visually,micheli-fleuret-2021-language,yao-etal-2020-keep}.
Recent work has used this ability to reason about plans from text instructions in simulated household environments with simplifying assumptions such as text-only environment observations or feedback \citep{huang2022language,ahn2022can,li2022pre,logeswaran-etal-2022-shot}.


We focus on \emph{visually grounded planning} with PLMs --- the ability to adapt plans based on interaction and visual feedback from the environment.
While PLMs have strong planning commonsense priors, predictions from a PLM may not be directly realizable in the environment since the observation and action spaces are unknown.
This requires \emph{grounding} the PLM in the environment and adapting it to observe visual feedback, which is highly non-trivial.
Some prior works assume the availability of a pre-trained affordance function \citep{ahn2022can} or a success detector \citep{mirchandani2021ella}.
Notably, SayCan \citep{ahn2022can} completely decouples the PLM from observation information by selecting actions that have both high affordability (through a pre-trained affordance model) and high PLM likelihood.
Although this partially addresses the grounding problem, the use of visual feedback for action affordance alone is limited.
Often an agent must choose one of many affordable actions using information from observations.
For example, a driving agent should re-navigate and possibly turn around when encountering a ``road closed'' sign, but both turning around and driving forward are indistinguishable to SayCan because they are both affordable and the PLM is blind to observations.

Another workaround explored in prior work is translating the information in the visual observations to text using a pre-trained captioning system \citep{shridhar2021alfworld,huang2022language}.
However, it can be difficult to faithfully describe an image in words and information is lost in this inherently noisy process, which limits the information available to the planner.



Recent work shows that PLMs can be adapted for various natural language tasks by inserting tunable embeddings or soft prompts at the input of the PLM (also called prompt tuning or prefix tuning)~\citep{li-liang-2021-prefix,lester-etal-2021-power}.
This approach also extends to multi-modal understanding tasks such as image captioning \citep{mokady2021clipcap} and VQA \citep{tsimpoukelli2021multimodal} where images are encoded as soft prompts and finetuned for the target task.
Transformer based architectures have also been successfully applied to offline Reinforcement Learning in recent work \citep{chen2021decision,janner2021offline,li2022pre,reid2022can}.

Taking inspiration from these works, we propose the simple approach of embedding visual observations (`visual prompts') and \textit{directly inserting them as PLM input embeddings}.
The visual encoder and PLM are jointly trained for the target task, an approach we call \textbf{\oursfull}~(\ours).
By teaching the PLM to use observations for planning in an end to end manner, we remove the dependency on external data such as captions and affordability information that was used in prior work.
We show that this simple approach performs better than prior PLM-based planning approaches on two embodied planning benchmarks based on ALFWorld~\citep{shridhar2021alfworld} and Virtualhome~\cite{puig2018virtualhome}.


