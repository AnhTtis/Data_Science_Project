\documentclass{article}
% \usepackage{authblk}
\usepackage[font={small,it}]{caption}

% \usepackage[authoryear,compress]{natbib}
\usepackage[english]{babel}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[style=apa]{biblatex}
\addbibresource{biblio.bib}

\usepackage{soul, color}
\usepackage{colortbl}
\usepackage{bm}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{tikz}
\usepackage{caption}
\usepackage{arydshln}
\usepackage{subcaption}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{dsfont}
\usepackage{bbold}
\usepackage{listings}
\usepackage{setspace}
\usepackage{url}
\usepackage{verbatim}
\usepackage{dsfont}
\usepackage{float}
\usetikzlibrary{positioning,decorations.pathreplacing,shapes}

\newcommand{\E}{\mathbb{E}}
\newcommand{\Var}{\textrm{Var}}
\newcommand{\prob}{Prob}

\newcommand{\yT}{\widetilde{\mathbf{y}}}
\newcommand{\uT}{\widetilde{\mathbf{u}}}
\newcommand{\bT}{\widetilde{\mathbf{b}}}
\newcommand{\eT}{\widetilde{\mathbf{e}}}
\newcommand{\qT}{\widetilde{\mathbf{q}}}
\newcommand{\BT}{\widetilde{\mathbf{B}}}
\newcommand{\UT}{\widetilde{\mathbf{U}}}
\newcommand{\SigT}{\widetilde{\mathbf{\Sigma}}}

\newcommand{\yH}{\widehat{\mathbf{y}}}
\newcommand{\bH}{\widehat{\mathbf{b}}}
\newcommand{\uH}{\widehat{\mathbf{u}}}
\newcommand{\eH}{\widehat{\mathbf{e}}}
\newcommand{\qH}{\widehat{\mathbf{q}}}
\newcommand{\BH}{\widehat{\mathbf{B}}}
\newcommand{\UH}{\widehat{\mathbf{U}}}
\newcommand{\YH}{\widehat{\mathbf{Y}}}
\newcommand{\SigH}{\widehat{\mathbf{\Sigma}}}

\newcommand{\mutil}{\widetilde{\mu}}
\newcommand{\sigtil}{\widetilde{\sigma}}
\newcommand{\nutil}{\widetilde{\nu}}
\newcommand{\pitil}{\widetilde{\pi}}
\newcommand{\ptil}{\widetilde{p}}
\newcommand{\qtil}{\widetilde{q}}
\newcommand{\util}{\widetilde{u}}
\newcommand{\btil}{\widetilde{b}}
\newcommand{\Btil}{\widetilde{B}}
\newcommand{\Util}{\widetilde{U}}

\newcommand{\Bhat}{\widehat{B}}
\newcommand{\Uhat}{\widehat{U}}
\newcommand{\bhat}{\widehat{b}}
\newcommand{\uhat}{\widehat{u}}
\newcommand{\phat}{\widehat{p}}
\newcommand{\qhat}{\widehat{q}}
\newcommand{\yhat}{\widehat{y}}
\newcommand{\sigmahat}{\widehat{\sigma}}
\newcommand{\pihat}{\widehat{\pi}}
\newcommand{\nuhat}{\widehat{\nu}}
\newcommand{\lambdahat}{\widehat{\lambda}}

\newcommand{\Bbf}{\mathbf{B}}
\newcommand{\Ubf}{\mathbf{U}}
\newcommand{\Ybf}{\mathbf{Y}}
\newcommand{\Sbf}{\mathbf{S}}
\newcommand{\Gbf}{\mathbf{G}}
\newcommand{\Ibf}{\mathbf{I}}
\newcommand{\Abf}{\mathbf{A}}
\newcommand{\Wbf}{\mathbf{W}}
\newcommand{\Hbf}{\mathbf{H}}
\newcommand{\Zbf}{\mathbf{Z}}
\newcommand{\Qbf}{\mathbf{Q}}
\newcommand{\Tbf}{\mathbf{T}}
\newcommand{\Fbf}{\mathbf{F}}
\newcommand{\Lbf}{\mathbf{L}}
\newcommand{\Dbf}{\mathbf{D}}
\newcommand{\Mbf}{\mathbf{M}}
\newcommand{\Cbf}{\mathbf{C}}
\newcommand{\Ebf}{\mathbf{E}}
\newcommand{\Scal}{\mathcal{S}}
\newcommand{\bbf}{\mathbf{b}}
\newcommand{\ubf}{\mathbf{u}}
\newcommand{\ybf}{\mathbf{y}}
\newcommand{\xbf}{\mathbf{x}}
\newcommand{\zbf}{\mathbf{z}}
\newcommand{\sbf}{\mathbf{s}}
\newcommand{\fbf}{\mathbf{f}}
\newcommand{\ebf}{\mathbf{e}}
\newcommand{\gammabf}{\bm{\gamma}}
\newcommand{\alphabf}{\bm{\alpha}}
\newcommand{\deltabf}{\bm{\delta}}
\newcommand{\mubf}{\bm{\mu}}
\newcommand{\Sigmabf}{\mathbf{\Sigma}}

\newcommand{\rr}{\mathbb{R}}
\newcommand{\pp}{\mathcal{P}}
\newcommand{\Poi}{Poi}
\newcommand{\resample}{\textbf{Resample}}

\def\pred#1#2#3{\hat{#1}_{#2|#3}}
\newcommand\norm[1][\cdot]{\left\lVert#1\right\rVert}
\newcommand\abs[1][\cdot]{\left\lvert#1\right\rvert}

\definecolor{codegreen}{rgb}{0,0.6,0}
\newcommand{\comm}[1]{\textcolor{codegreen}{#1}}

% \newcommand{\hlcyan}[1]{{\sethlcolor{cyan}\hl{#1}}}
% \DeclareRobustCommand{\hlcyan}[1]{{\sethlcolor{cyan}\hl{#1}}}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}
\definecolor{Gray}{gray}{0.8}
\definecolor{lightyellow}{RGB}{245,238,197}
\newcolumntype{a}{>{\columncolor{lightyellow}}c}
\newcolumntype{b}{>{\columncolor{backcolour}}c}
\renewcommand\arraystretch{1.2}
\newcommand{\figwidth}{0.45\textwidth}
\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,
    breaklines=true,
    captionpos=b,
    keepspaces=true,
    numbers=left,
    numbersep=5pt,
    showspaces=false,
    showstringspaces=false,
    showtabs=false,
    tabsize=2
}

\newcommand{\thickbar}[1]{\mathbf{\bar{\text{$#1$}}}}

\DeclareFontFamily{U}{mathx}{}
\DeclareFontShape{U}{mathx}{m}{n}{<-> mathx10}{}
\DeclareSymbolFont{mathx}{U}{mathx}{m}{n}
\DeclareMathAccent{\widehat}{0}{mathx}{"70}
\DeclareMathAccent{\widecheck}{0}{mathx}{"71}

\newtheorem{definition}{Definition}
\newtheorem{proposition}{Proposition}

\newcommand{\keywords}[1]{\noindent \textbf{Keywords:}\; #1}

\lstset{style=mystyle}
\begin{document}

\title{Properties of the reconciled distributions for Gaussian and count forecasts}
\author{Lorenzo Zambon\thanks{IDSIA, Dalle Molle Institute for Artificial Intelligence, CH-6962, Lugano, Switzerland; \texttt{\{lorenzo.zambon, giorgio.corani\}@idsia.ch}}
\and Arianna Agosto\thanks{Department of Economics and Management, University of Pavia, 27100, Pavia, Italy; \texttt{\{arianna.agosto, paolo.giudici\}@idsia.ch}}
\and Paolo Giudici \footnotemark[2]
\and Giorgio Corani\footnotemark[1]}
\date{}

\maketitle

\abstract{
Reconciliation enforces coherence between hierarchical forecasts, in order to satisfy a set of linear constraints.
However, most works focus on the reconciliation of the point forecasts.
We instead focus on  probabilistic reconciliation and
we  analyze the properties of the reconciled distributions by considering
reconciliation via conditioning. 
We provide a formal analysis of the variance of the reconciled distribution,
treating separately  the 
case of Gaussian forecasts and count forecasts. 
We also study the behavior of the reconciled upper mean in the case of 1-level hierarchies; also in this case we analyze separately the case of Gaussian forecasts and count forecasts.
We then show experiments on the reconciliation of intermittent time series related to the count of extreme market events. 
The experiments confirm our theoretical results about the mean and variance of the reconciled distribution and show that reconciliation yields a major gain in forecasting accuracy compared to the base forecasts.}

\bigskip

\keywords{Reconciliation, hierarchical forecasting, importance sampling, intermittent time series, probabilistic forecasts}

% \begin{frontmatter}
% \title{Properties of the reconciled distributions for Gaussian and count forecasts}
% \author{Blind authors}
% \begin{abstract}
% Reconciliation enforces coherence between hierarchical forecasts, in order to satisfy a set of linear constraints.
% However, most works focus on the reconciliation of the point forecasts.
% We instead focus on  probabilistic reconciliation and
% we  analyze the properties of the reconciled distributions by considering
% reconciliation via conditioning. 
% We provide a formal analysis of the variance of the reconciled distribution,
% treating separately  the 
% case of Gaussian forecasts and count forecasts. 
% We also study the behavior of the reconciled upper mean in the case of 1-level hierarchies; also in this case we analyze separately the case of Gaussian forecasts and count forecasts.
% We then show experiments on the reconciliation of intermittent time series related to the count of extreme market events. 
% The experiments confirm our theoretical results about the mean and variance of the reconciled distribution and show that reconciliation yields a major gain in forecasting accuracy compared to the base forecasts.
% \end{abstract}

% \begin{keyword}
% Reconciliation \sep 
% hierarchical forecasting \sep
% importance sampling \sep
% intermittent time series \sep
% probabilistic forecasts
% \end{keyword}
% \end{frontmatter}

\section{Introduction}

Many practical applications require generating \textit{coherent} 
forecast for hierarchical time series.
The  forecasts are coherent if they respect a set of  linear constraints determined by the  structure of the hierarchy.
The most common approach to obtain coherent forecasts is  to  compute first  the base forecasts, by fitting independent models on  each time series; 
later the  base forecast are adjusted (\textit{reconciliation}) to become coherent.

Most  reconciliation methodologies \parencite{hyndman2011optimal, Wickramasuriya.etal2018, PANAGIOTELIS2021343, difonzo2022forecast}
produce reconciled point forecasts. However, decision making often requires a \textit{reconciled predictive distribution} \parencite{kolassa22we, gneiting2014probabilistic}.
\cite{panagiotelis2023probabilistic} propose a seminal framework
for  probabilistic reconciliation based on projection,
learning the parameters of the projection via stochastic gradient descent.
Yet, it is impossible to analytically characterize  the distribution  reconciled in this way; moreover, this method cannot reconcile count variables.
These considerations also apply to other methods for  probabilistic reconciliation \parencite{jeon2019probabilistic, taieb2021hierarchical, rangapuram2021end, hollyman2022hierarchies}.

On the other hand, probabilistic reconciliation based on conditioning
\parencite{corani_reconc, corani2022probabilistic, zambon2022} is 
better suited to  both analytical studies and count variables.
Indeed, in the  Gaussian case, reconciliation via conditioning can be solved 
in closed form \parencite{corani_reconc}, providing the same mean and variance of MinT \parencite{Wickramasuriya.etal2018}.
In the non-Gaussian case, the reconciled distribution can be obtained via sampling, for instance using MCMC  \parencite{corani2022probabilistic} or importance sampling \parencite{zambon2022}.
However, the properties of the reconciled distribution  have been not yet studied.

We thus  study the properties of the  reconciled distribution,
focusing on reconciliation via conditioning.
As a first result, we extend the analysis of reconciliation in the  Gaussian case, 
proving that the variance of every variable of the hierarchy decreases after
reconciliation; this holds regardless of the size of the incoherence of the base forecasts.
We then discover a different pattern for the discrete case: we prove that   reconciliation can  increase the  variance of the reconciled bottom variables if  the base  forecasts have a low probability of being coherent. 

We then analyze the reconciled mean,  restricting however our analysis to the case of 1-level hierarchies. As it is well known, in the Gaussian case 
the reconciled upper mean is a combination of the bottom-up mean and the mean of the  base forecast of the  upper variable \parencite{corani_reconc, hollyman2021understanding}.
We call this the \textit{compromise} effect.
%
However, we empirically show that in the case of count variables,
the reconciled mean  of the upper time series can  also  be lower than both the bottom-up  and the base mean.
This happens when the base forecasts are skewed distributions, with moderate  incoherence.  After reconciliation, variance decreases shortening the long right tail and pulling the reconciled means of all time series towards zero.
We call this the \textit{strengthening} effect, as the information of low counts provided by the different base forecasts are mutually reinforced.

We present experiments on the reconciliation of  intermittent time series referring to counts of extreme market events. 
They  confirm our theoretical results and show that,  at least on
intermittent time series,  
the strengthening effect  on the mean is more common than the compromise effect.
We moreover report a major increase in accuracy for the reconciled forecasts over the base forecasts, further confirming the beneficial effect of reconciliation for
forecasting intermittent time series \parencite{athanasopoulos2017_temporal, KOURENTZES-elucidate,
 corani2022probabilistic, zambon2022}.

The paper is organized as follows. 
In Section~\ref{sec: prob fore rec}, we recall probabilistic reconciliation through conditioning, and analyze the reconciled mean and variance of the reconciled distribution in the Gaussian and in the non-Gaussian case.
In Section~\ref{sec:experiments}, we present our case study.
Finally, the conclusions are in Section~\ref{sec: conclusions}.

\section{Probabilistic forecast reconciliation}
\label{sec: prob fore rec}

Given a hierarchy,
we denote by $\bbf = [b_1,\dots,b_m]^T$ the vector of bottom variables, and by $\ubf = [u_1,\dots,u_{n-m}]^T$ the vector of upper variables. 
%
We then denote the vector of all the variables by
\[
\ybf = \begin{bmatrix}
          \ubf \\
          \bbf
         \end{bmatrix} \in \rr^n.
\]
%
The hierarchy may be expressed as a set of linear constraints:
\begin{equation}\label{eq: def S}
\ybf = \Sbf \bbf, \quad \text{with} \; \Sbf= \begin{bmatrix}
          \Abf \\ \hdashline[2pt/2pt]
          \Ibf
         \end{bmatrix},
\end{equation}
where $\Ibf \in \rr^{m \times m}$ is the identity matrix. 
$\Sbf \in \rr^{n \times m}$ is the \textit{summing matrix}, while $\Abf \in \rr^{(n-m) \times m}$ is the \textit{aggregating matrix}. 
The summing constraints can thus be written as $\ubf = \Abf \bbf$.

We assume the base forecasts to be in the form of predictive distributions.
We denote by $\pihat$ the base forecast distribution for the entire hierarchy,
and by $\pihat_U$ and $\pihat_B$ the  base forecasts for the upper and the bottom variables.
%
The aim of probabilistic reconciliation is to find a reconciled distribution $\pitil$ that gives positive probability only to coherent points.
To this end, we first obtain  a reconciled bottom distribution $\pitil_B$ from the base forecast distribution $\pihat$.
%
Then, we obtain the reconciled distribution $\pitil$ on the entire hierarchy as: 
\[
\pitil(\ubf, \bbf) =
\begin{cases}
\pitil_B(\bbf) \quad \quad &\text{if } \ubf = \Abf \bbf \\
0  &\text{if } \ubf \neq \Abf \bbf,
\end{cases}
\]
so that the probability of any incoherent point is zero.

\paragraph{Probabilistic bottom-up}
If we set $\pitil_B = \pihat_B$, we have the probabilistic bottom-up, which ignores the base forecast $\pihat_U$ of the upper variables of the hierarchy.
The reconciled bottom-up distribution $\pitil_{bu}$ is thus given by:
\begin{equation}\label{eq: probabilistic bottom up}
 \pitil_{bu}(\ubf, \bbf) =
\begin{cases}
\pihat_B(\bbf) \quad \quad &\text{if } \ubf = \Abf \bbf \\
0  &\text{if } \ubf \neq \Abf \bbf.
\end{cases}   
\end{equation}

\paragraph{Reconciliation through conditioning}
In this work, we follow the approach of reconciling forecasts by conditioning \parencite{corani_reconc,corani2022probabilistic, zambon2022}.
In order to  take into account  the base forecasts of all variables,
we introduce the random vector
\[
% \YH = \Big[ \UH^T, \BH^T \Big]^T \sim \pihat,
\YH = \begin{bmatrix} \UH \\ \BH \end{bmatrix} \sim \pihat,
\]
so that $\pihat_U$ and $\pihat_B$ are the distributions of  $\UH$ and $\BH$. 
We then define the reconciled bottom distribution by conditioning
on the hierarchy constraints:
\begin{align}\label{eq: reconc density}
\pitil_B(\bbf) &= \prob\big(\BH = \bbf \mid \UH = \Abf \BH \big) \nonumber\\
&\propto \pihat(\Abf \bbf,\bbf).
\end{align}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Gaussian reconciliation}
\label{sec: gaussian case}
In the Gaussian case, reconciliation via conditioning can be solved in closed form
\parencite{corani_reconc}; its  reconciled mean and variance are numerically equivalent to those of  MinT \parencite{Wickramasuriya.etal2018}, despite the different derivation. 

In \ref{appendix: derivation gaussian} we derive in a novel way the formulas of reconciliation via conditioning. 
Our formulas are equivalent to those of \cite{corani_reconc} and hence to MinT,  but they are more convenient to illustrate  some properties of the reconciled distribution.
Let us assume the base forecasts for the entire hierarchy to be multivariate Gaussian:
%
\begin{equation}
\YH = \begin{bmatrix} \UH \\ \BH \end{bmatrix}
\sim \mathcal{N}\left( \yH, \,\SigH_Y \right),
\end{equation}
where
\begin{equation*}
\yH = \begin{bmatrix} \uH \\ \bH \end{bmatrix}, \quad
\SigH_Y = \begin{bmatrix} \SigH_U & \SigH_{UB} \\
\SigH_{UB}^T & \SigH_B \end{bmatrix}.
\end{equation*}
%
The reconciled bottom and upper distributions are  multivariate Gaussian:
\begin{equation*}
\BT \sim \mathcal{N}\left(\bT, \,\SigT_B \right), \qquad
\UT \sim \mathcal{N}\left(\uT, \,\SigT_U \right),
\end{equation*}
where
\begin{align}
\bT &= \bH + \left(\SigH_{UB}^T - \SigH_B A^T\right) \Qbf^{-1} (\Abf \bH - \uH), \label{eq: reconciled gaussian mean bottom} \\
\uT &= \uH + \left(\SigH_U - \SigH_{UB} \Abf^T\right) \Qbf^{-1} (\Abf \bH - \uH), \label{eq: reconciled gaussian mean upper} \\
\SigT_B &= \SigH_B - \left(\SigH_{UB}^T - \SigH_B \Abf^T\right) \Qbf^{-1} \left(\SigH_{UB}^T - \SigH_B \Abf^T\right)^T, \label{eq: reconciled gaussian variance bottom} \\
\SigT_U &= \SigH_U - \left(\SigH_U - \SigH_{UB} \Abf^T\right) \Qbf^{-1} \left(\SigH_U - \SigH_{UB} \Abf^T\right)^T, \label{eq: reconciled gaussian variance upper}
\end{align}
%
and $\Qbf:= \SigH_U - \SigH_{UB} \Abf^T - \Abf \SigH_{UB}^T + \Abf \SigH_B \Abf^T$.
%
We can then prove the following new result, which is valid for any hierarchy:

\begin{proposition}[Reconciled Gaussian variance]\label{prop: var gauss}
In the Gaussian framework, the variance of each variable decreases after reconciliation.\\
Indeed, for each $i=1,\dots,m$, and $j=1,\dots,n-m$:
\begin{align}\label{eq: variance inequality prop}
\Var(\Btil_i) \le \Var(\Bhat_i), \nonumber \\
\Var(\Util_j) \le \Var(\Uhat_j).
\end{align}
\end{proposition}
%
The proof is given in \ref{app: proof var ineq}. 
Remarkably, the reconciled variances (Eq. \eqref{eq: reconciled gaussian variance bottom} and \eqref{eq: reconciled gaussian variance upper}) depend on the variances of the base forecasts but not on their means. 

\paragraph{Reconciled Gaussian mean}
In order to study the reconciled mean, we need some 
restrictive assumptions (which, however, do apply to the  case study of  Section \ref{sec:experiments}).
In particular, assuming that:
\begin{itemize}
    \item there is only one upper variable,
    \item  there is no correlation between the bottom and the upper base forecasts,
\end{itemize}
%
the reconciled upper mean is a convex combination of the base upper mean $\uhat$ and the bottom-up mean $\Abf \bH$, weighted by the uncertainty of the base upper and bottom forecasts.
Indeed, from \eqref{eq: reconciled gaussian mean upper}, we have:
\begin{equation}\label{eq: gauss reconciled mean upper}
\util = \frac{\sigmahat^2_{bu}}{\sigmahat^2_U + \sigmahat^2_{bu}} \, \uhat + \frac{\sigmahat^2_U}{\sigmahat^2_U + \sigmahat^2_{bu}} \, \Abf \bH,
\end{equation}
where  $\sigmahat^2_U$ is the variance of the upper base forecast and $\sigmahat^2_{bu} := \Abf \SigH_B \Abf^T \ge 0$ is the variance of the probabilistic bottom-up, defined in \eqref{eq: probabilistic bottom up}.
The reconciled mean is thus a compromise between the base and the bottom-up mean, 
as already noted  in the literature \parencite{corani_reconc, hollyman2021understanding};
we call this the \textit{compromise effect}.


We can draw an analogy with  the Gaussian conjugate model in Bayesian statistics: 
if we condition on new observations,
 the posterior variance is guaranteed to be smaller than the prior variance \parencite{Gelman2011} and
the posterior expectation is a convex combination of the prior expectation and the sample mean \parencite[Ch.~2.5]{bda2013}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Sampling from the reconciled distribution}
\label{sec: sampling from the rec distr}
In the non-Gaussian  case,  the reconciled distribution $\pitil$  cannot be obtained in closed 
form.
In order to draw samples from $\pitil$, we follow the approach of  \cite{zambon2022} based on importance sampling \parencite{elvira2021advances}.
Denoting by $N$ the number of samples, the algorithms is as follows:
\begin{enumerate}
    \item Sample $\big(\bH^{(i)}\big)_{i=1,\dots,N}$ from $\pihat_B$
    \item Compute the unnormalized weights $\big(\widecheck{w}^{(i)}\big)_{i=1,\dots,N}$ as 
    \[\widecheck{w}^{(i)} = \frac{\pihat\big(\Abf \bH^{(i)}, \bH^{(i)}\big)}{\pihat_B\big(\bH^{(i)}\big)}.\]
    If we assume the  bottom and upper base forecasts to be independent, as we do in this paper, we have
    \[\widecheck{w}^{(i)} = \frac{\pihat_U\big(\Abf \bH^{(i)}\big) \pihat_B\big(\bH^{(i)}\big)}{\pihat_B\big(\bH^{(i)}\big)}
    = \pihat_U\big(\Abf \bH^{(i)}\big)\]
    \item Compute the normalized weigths as $w^{(i)} = \widecheck{w}^{(i)} / \sum_h \widecheck{w}^{(h)}$
    \item Sample $\big(\bT^{(i)}\big)_i$ with replacement from the weighted sample $\big( \bH^{(i)}, w^{(i)} \big)_{i=1,\dots,N}$
\end{enumerate}
%
The output $\big(\bT^{(i)}\big)_i$ is an unweighted sample from the reconciled distribution $\pitil_B$.
This algorithm effectively reconciles the simple hierarchies (a single upper variable) considered in this paper. 
More complex hierarchies can be reconciled by a more sophisticated version of this algorithm \parencite{zambon2022}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Reconciled variance of discrete variables}
\label{sec: rec var}

As a  further contribution, we show that for \textit{discrete} variables the variance of the reconciled distribution 
can be larger than the variance of the base distribution.
This happens when the  probability $p_c := \prob\big(\UH=\Abf\BH\big)$
of the base forecasts  being coherent is small.  This is thus a major difference with the Gaussian reconciliation.

\begin{proposition} 
\label{prop: reconc variance}
Let us assume that $\BH$ and $\UH$ are discrete random variables, and that $p_c > 0$.
Then, for any $j=1,\dots,m$, we have
\begin{equation}
\label{eq: reconciled variance}
\Var\big[\Btil_j\big]
= \frac{\Var\big(\Bhat_j\big) - (1-p_c) \, \Var\big[\Bhat_j \,|\, \UH \neq \Abf \BH\big] 
- p_c (1-p_c) \, (a-b)^2}{p_c},
\end{equation}
where $a := \E\big[\Bhat_j \,|\, \UH \neq \Abf \BH \big]$ and $b := \E\big[\Bhat_j \,|\, \UH = \Abf \BH \big]$. 
\end{proposition}
%
The proof is given in  \ref{app: proof rec var}.
Note that the probability of coherence $p_c$ is the denominator of \eqref{eq: reconciled variance}: as it gets smaller, the reconciled variance gets larger.
Again, we can draw an analogy with Bayesian statistics:
in non-Gaussian models, the posterior variance can be larger than the prior variance if we condition on observations that are conflicting with the prior beliefs 
(\cite[Ch.~2.2]{bda2013}; \cite{Gelman2011}).
% \citetext{\citealp[Ch.~2.2]{bda2013}; \citealp{Gelman2011}}.

\paragraph*{Reconciling a minimal hierarchy}
We now illustrate  the increase of variance after reconciliation on the minimal hierarchy  of Fig.~\ref{fig: simple hier gauss}.
%
Consider the following independent base forecasts:
\begin{align*}
\Bhat_1 \sim &\text{Bernoulli}\,(\phat_1), \quad
\Bhat_2 \sim \text{Bernoulli}\,(\phat_2), \\
& \quad \Uhat = \begin{cases}
0 \qquad \text{prob} = \qhat_0 \\
1 \qquad \text{prob} = \qhat_1 \\
2 \qquad \text{prob} = \qhat_2,
\end{cases} 
\end{align*}
where $\phat_1,\phat_2 \in [0,1]$, $\qhat_0, \qhat_1, \qhat_2 \in [0,1]$, and $\qhat_0 + \qhat_1 + \qhat_2 = 1$.
In \ref{app: example bernoulli} we derive the analytical expression of the  parameters $\ptil_1, \ptil_2$, and $\qT$ of the reconciled distribution.

\begin{figure}[!h]
    \centering
\begin{tikzpicture}[level/.style={sibling distance=30mm/#1}]
\node [circle,draw] {$U$}
  child {node [circle,draw]  {$B_1$}
  }
  child {node [circle,draw]  {$B_2$}
};
\end{tikzpicture}
 \caption{\label{fig: simple hier gauss} A minimal hierarchy.}
\end{figure}

To induce large incoherence we adopt the parameters of Table~\ref{tab: var}, 
which imply $\E[\Bhat_1 + \Bhat_2] = 0.5$ and $\E[\Uhat] = 1.6$.
The probability of coherence is $p_c = 0.17$, computed as:
\begin{equation}\label{eq: c_coherence}
p_c = \sum_{\substack{u, b_1, b_2: \\ u = b_1 + b_2}} \pihat(u, b_1, b_2).
\end{equation} 
In this particular case,  the variance of all variables increases after reconciliation
(Table~\ref{tab: var}).

\paragraph*{Poisson base forecast}

We now consider the case of Poisson independent base forecasts:
\[\Bhat_1 \sim \Poi\big(\lambdahat_1\big), \quad \Bhat_2 \sim \Poi\big(\lambdahat_2\big), \quad \Uhat \sim \Poi\big(\lambdahat_u\big),\]
with $\lambdahat_1,\lambdahat_2, \lambdahat_u > 0$.
%
To induce large incoherence, we set $\lambdahat_1 = 0.5$, $\lambdahat_2 = 0.8$, and $\lambdahat_u = 6.0$, resulting in  $p_c = 0.03$.
We perform reconciliation via importance sampling (Section~\ref{sec: sampling from the rec distr}). 
The variance of the bottom variables increases after reconciliation (Table~\ref{tab: var}).

\begin{table}[!ht]
    \centering
    \small
\begin{tabular}{c @{\hskip 8mm} c c @{\hskip 8mm} c c c}
\toprule
   &  \multicolumn{2}{c@{\hspace{8mm}}}{\textbf{mean}} & \multicolumn{3}{c}{\textbf{variance}}\\
   & \textit{base} & \textit{reconc} & \textit{base} & \textit{reconc}
   & $\Delta$\\
\midrule
\textit{Bernoulli} & & & & &\\
$B_1$ & 0.3 & 0.52 & 0.21 & 0.25 & \cellcolor{backcolour} 0.04\\
$B_2$ & 0.2 & 0.40 & 0.16 & 0.24 & \cellcolor{backcolour} 0.08\\
$U$   & 1.6 & 0.92 & 0.44 & 0.56 & 0.12\\
\textit{Poisson} & & & & &\\
$B_1$ & 0.5 & 0.97 & 0.5 & 0.81 & \cellcolor{backcolour} 0.31\\
$B_2$ & 0.8 & 1.56 & 0.8 & 1.13 & \cellcolor{backcolour} 0.33\\
$U$   & 6.0 & 2.53 & 6.0 & 1.41 & -4.59\\
\bottomrule
\end{tabular}
\caption{\label{tab: var} 
Examples of reconciliations which increase the  variance of the bottom variables.
For the Bernoulli case, we set $\phat_1 = 0.3$, $\phat_2 = 0.2$, and $\qH = [0.1, 0.2, 0.7]$, and after reconciliation we obtain $\ptil_1 = 0.52$, $\ptil_2 = 0.40$, and $\qT \approx [0.32, 0.44, 0.24]$.}
\end{table}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Reconciled mean in the non-Gaussian case}
\label{sec: rec mean}
We now discuss
a behavior of the reconciled mean, which is alternative to the compromise effect.
Consider again the minimal hierarchy, assuming
the base forecasts to be
asymmetric (as is often the case with count variables), but  not too   incoherent, so that reconciliation decreases the variance. 
Then the reconciled mean of the upper variable can be lower than \textit{both} the mean  of its base forecast and the  bottom-up mean. We call  this the ``strengthening effect''.

To show the strengthening effect on the minimal hierarchy (Fig.~\ref{fig: simple hier gauss}),
we consider independent Poisson base forecasts with  $\lambdahat_1 = 0.5$, $\lambdahat_2 = 0.8$, and $\lambdahat_u = 1.5$. 
Thus, all base  forecasts convey information of low counts.
Reconciliation fuses  the base forecasts 
emphasizing the tendency towards 0: the mean of all the variables decreases after reconciliation (Table~\ref{tab: mean poisson}).
The reconciled distribution of the upper variable has a lower mean than both the base and bottom-up distributions (Fig.~\ref{fig: poisson 0.5 0.8 1.5}).
On the other hand, we can induce the compromise effect (Fig.~\ref{fig: poisson 5 7 18})  by making  the base forecasts   closer to Gaussian, e.g. by setting $\lambdahat_1 = 5$, $\lambdahat_2 = 7$, and $\lambdahat_u = 18$ (Table~\ref{tab: mean poisson}).

\begin{figure}[!h]
     \centering
     \begin{subfigure}[b]{0.45\textwidth}
         \centering
         \includegraphics[width=\textwidth]{poisson/U_lb_0.5_0.8_lu_1.5.pdf}
         \caption{Strengthening effect}
         \label{fig: poisson 0.5 0.8 1.5}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.45\textwidth}
         \centering
         \includegraphics[width=\textwidth]{poisson/U_lb_5.0_7.0_lu_18.0.pdf}
         \caption{Compromise effect}
         \label{fig: poisson 5 7 18}
     \end{subfigure}
        \caption{Base, bottom-up, and reconciled  probability mass functions of $U$ in case of Poisson base forecasts.
        (a) $\lambdahat_1 = 0.5$, $\lambdahat_2 = 0.8$, $\lambdahat_u = 1.5$
        (b) $\lambdahat_1 = 5$, $\lambdahat_2 = 7$, $\lambdahat_u = 18$}
        \label{fig: poisson}
\end{figure}


\begin{table}[!ht]
    \centering
    \small
\begin{tabular}{l @{\hskip 8mm} c c c @{\hskip 8mm} c c c}
\toprule
& \multicolumn{3}{c @{\hskip 8mm}}{\textbf{strengthening effect}} & \multicolumn{3}{c}{\textbf{compromise effect}} \\
& \textit{base mean} & \textit{rec. mean} & $\Delta$ & \textit{base mean} & \textit{rec. mean} & $\Delta$ \\
\midrule
%\rowcolor{backcolour}
$B_1$ & 0.50 & 0.43 & \cellcolor{backcolour} -0.07 & 5.00 & 6.02 & \cellcolor{backcolour} 1.02 \\
$B_2$ & 0.80 & 0.68 & \cellcolor{backcolour} -0.12 & 7.00 & 8.43 & \cellcolor{backcolour} 1.43 \\
%\rowcolor{backcolour}
$U$ & 1.50 & 1.11 & \cellcolor{backcolour} -0.39 & 18.0 & 14.44 & \cellcolor{backcolour} -3.56 \\
\bottomrule
\end{tabular}
\caption{\label{tab: mean poisson} Mean before and after reconciliation; the base forecasts are Poisson.}
\end{table}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\section{Case study: modeling extreme market events}
\label{sec:experiments}
Credit default swaps (CDS) are financial instruments that guarantee insurance against the possible default of a given company (called ``reference company'') to the buyer. The CDS price is a function of the probability of default estimated by the market for that company. Thus, a  high value of the CDS spread corresponds to an increase in the risk of a company default. 

Following  \parencite{raunig2011value},  an \textit{extreme market event} takes place when the value of the CDS spread on a given day exceeds the 90-th percentile of its distribution in the last trading year. In particular, we  forecast the extreme market events for the companies of the Euro Stoxx 50 index (\url{https://www.stoxx.com/}), in the  period 2005-2018. As in \cite{agosto2020tree}, we consider the 29 
companies included in the index  having a regularly quoted CDS  and we divide them into five economic sectors: Financial (FIN), Information and Communication Technology (ICT), Manufacturing (MFG), Energy (ENG), and Trade (TRD). 
Following \cite{agosto2022multivariate}, we start from the CDS spread  time series retrieved from Bloomberg and we count the daily number of extreme events  for each sector, obtaining five daily time series.
The series - for a total of 3508 data points each - have low counts and a high frequency of zeros (Table~\ref{tab:descr}); they are all intermittent, with large  average inter-demand interval (ADI).  
%


\begin{table}[!htp] 
	\centering
	\small
	\begin{tabular}{lcccc}
		\toprule
		Sector             & Time series       & Mean         & Proportion of zeros & ADI\\
		\midrule
		FIN                & 10                        & 0.96       & 0.73 & 3.8\\
        
        \rowcolor{backcolour}
		ICT                & 4                         & 0.38       & 0.79 & 4.7\\
		
        MFG                & 7                         & 0.67       & 0.73 & 3.7\\
        
        \rowcolor{backcolour}
		ENG                & 5	                       & 0.48       & 0.80 & 4.9\\
		
        TRD                & 3                         & 0.29       & 0.81 & 5.2\\
		\bottomrule
	\end{tabular}
	\caption{Main characteristics of the  count time series.  A time series is considered intermittent if its ADI is \textgreater 1.32 \parencite{SyntetosBoylan2005}.
 \label{tab:descr}}
\end{table}

\paragraph{Base forecasts and hierarchical structure}
\tikzset{every picture/.style={line width=0.75pt}} %set default line width to 0.75pt        
\begin{figure}[!h]
    \centering
\resizebox{0.8\textwidth}{!}{%
\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
%uncomment if require: \path (0,218); %set diagram left start at 0, and has height of 218

%Shape: Circle [id:dp2631367700141882] 
\draw   (271,43.5) .. controls (271,27.21) and (284.21,14) .. (300.5,14) .. controls (316.79,14) and (330,27.21) .. (330,43.5) .. controls (330,59.79) and (316.79,73) .. (300.5,73) .. controls (284.21,73) and (271,59.79) .. (271,43.5) -- cycle ;
%Shape: Circle [id:dp2996596639668663] 
\draw   (381,148.5) .. controls (381,132.21) and (394.21,119) .. (410.5,119) .. controls (426.79,119) and (440,132.21) .. (440,148.5) .. controls (440,164.79) and (426.79,178) .. (410.5,178) .. controls (394.21,178) and (381,164.79) .. (381,148.5) -- cycle ;
%Shape: Circle [id:dp3602457432031798] 
\draw   (492,148.5) .. controls (492,132.21) and (505.21,119) .. (521.5,119) .. controls (537.79,119) and (551,132.21) .. (551,148.5) .. controls (551,164.79) and (537.79,178) .. (521.5,178) .. controls (505.21,178) and (492,164.79) .. (492,148.5) -- cycle ;
%Shape: Circle [id:dp08508111986892297] 
\draw   (161,151.5) .. controls (161,135.21) and (174.21,122) .. (190.5,122) .. controls (206.79,122) and (220,135.21) .. (220,151.5) .. controls (220,167.79) and (206.79,181) .. (190.5,181) .. controls (174.21,181) and (161,167.79) .. (161,151.5) -- cycle ;
%Shape: Circle [id:dp4265021764596202] 
\draw   (59,151.5) .. controls (59,135.21) and (72.21,122) .. (88.5,122) .. controls (104.79,122) and (118,135.21) .. (118,151.5) .. controls (118,167.79) and (104.79,181) .. (88.5,181) .. controls (72.21,181) and (59,167.79) .. (59,151.5) -- cycle ;
%Shape: Circle [id:dp40358661333368206] 
\draw   (272,149.5) .. controls (272,133.21) and (285.21,120) .. (301.5,120) .. controls (317.79,120) and (331,133.21) .. (331,149.5) .. controls (331,165.79) and (317.79,179) .. (301.5,179) .. controls (285.21,179) and (272,165.79) .. (272,149.5) -- cycle ;
%Straight Lines [id:da0774387360344253] 
\draw    (108,128.81) -- (273,56.81) ;
%Straight Lines [id:da817143972345908] 
\draw    (281,66.81) -- (211,128.81) ;
%Straight Lines [id:da8676652564357934] 
\draw    (300.5,73) -- (301.5,120) ;
%Straight Lines [id:da23567223934131598] 
\draw    (321,64.81) -- (387,128.81) ;
%Straight Lines [id:da16269638908095385] 
\draw    (329,54.81) -- (498,129.81) ;

% Text Node
\draw (285,34) node [anchor=north west][inner sep=0.75pt]   [align=left] {\textbf{ALL}};
% Text Node
\draw (284,139.5) node [anchor=north west][inner sep=0.75pt]   [align=left] {\textbf{MFG}};
% Text Node
\draw (75,140) node [anchor=north west][inner sep=0.75pt]   [align=left] {\textbf{FIN}};
% Text Node
\draw (177,140) node [anchor=north west][inner sep=0.75pt]   [align=left] {\textbf{ICT}};
% Text Node
\draw (393,140) node [anchor=north west][inner sep=0.75pt]   [align=left] {\textbf{ENG}};
% Text Node
\draw (506,140) node [anchor=north west][inner sep=0.75pt]   [align=left] {\textbf{TRD}};

\end{tikzpicture}
}%

 \caption{\label{fig: finTS hier} Hierarchical structure of the extreme events time series.}
\end{figure}

We organize the time series into a hierarchy with 5 bottom and 1 upper time series:
the five economic sectors constitute the bottom level, while their sum constitutes the  top level (Fig.~\ref{fig: finTS hier}). 

We compute the base forecasts for the bottom time series (counts in the different sectors) using the model of \cite{agosto2022multivariate}, whose 
predictive distribution is a multivariate negative binomial with a static vector of dispersion parameters  and a time-varying vector of location parameters following a score-driven dynamics \parencite{Harvey_2013}. The model of \cite{agosto2022multivariate} extends to the multivariate case the modeling approach of \cite{agosto2016}, who proposed a Poisson autoregressive model with exogenous covariates (PARX) to measure systemic risk in the corporate default dynamics. According to \cite{agosto2022multivariate}, the predictive distribution computed at time $t$ for the count of time $t+1$ in sector $i$ is:
\begin{equation} \label{nb}
\yhat_{i,t+1} \sim NB(\mu_{i,t+1},\alpha_i),
\end{equation}
%
where $\mubf_t$ is the $k \times 1$ vector of location parameters, $k$ is the number of sectors, and $\alpha_i \geq 0$.
% with $\alpha_i \geq 0$, and for the $k \times 1$ vector of location parameters $\mubf_t$, where $k$ is the number of sectors. 
The model assumes the following dynamic:
\begin{equation} \label{score_driv}
\log(\mubf_{t+1})= \Cbf + \Dbf \log{\mubf_t} + \Ebf \dfrac{\ybf_t-\mubf_t}{\alphabf \mubf_t+1},
\end{equation}
where $\Cbf$ is a $k \times 1$ vector, while $\Dbf$ and $\Ebf$ are $k \times k$ matrices (see \cite{agosto2022multivariate} for detailed properties and estimation details).
Thus, the predicted event count in a given sector is a function of the past expectations ($\mubf_t$) and forecast errors ($\ybf_t-\mubf_t$) in the same sector and in the other sectors.  
The base forecasts for the upper time series are computed by fitting a univariate version of the model \parencite{blasques2018} on the aggregate count time series.

The base forecasts for the bottom series measure financial risks at the sector level, accounting for the dependencies between sectors, through a shock propagation mechanism, besides an autoregressive component. The base upper forecasts express instead a measure of aggregate systemic financial risk.



\subsection{Experimental procedure}
The goal of our experiments is to compare  the base and the reconciled forecasts and  to 
analyze the mean and variance of the reconciled distributions.
We reconcile
using importance sampling the 1-day ahead forecasts, assuming the base forecasts of the upper and the bottom variables to be independent.
%
We perform 3508 reconciliations, drawing each time  $N=100,000$ samples from the reconciled distribution.
Each reconciliation is almost instantaneous (\textless 0.1 sec); for a discussion on the computation times, see \cite{zambon2022}.

As recommended by \cite{panagiotelis2023probabilistic}, 
we compare base and reconciled distributions through the energy score (ES, \cite{szekely2013energy}):
\[
ES(P_t,\ybf_t) = \E_{P_t}\left[\|\ybf_t-\sbf_t\|^{\beta}\right] 
- \frac{1}{2} \E_{P_t}\left[\|\sbf_t-\sbf_t'\|^{\beta}\right],
\]
where $P_t$ is the forecast distribution on the whole hierarchy, $\sbf_t, \sbf_t'$ are a pair of independent random variables distributed as $P_t$, and $\ybf_t$ is the vector of the actual values of all the time series at time $t$.
We compute the ES, with $\beta = 2$, using the sampling approach of \cite{wickramasuriya2023probabilistic}.
We compute the ES of the joint predictive distribution of upper and bottom time series; we thus have  a single ES for the entire hierarchy.
 

Moreover, we evaluate the prediction intervals using  the interval score (IS, \cite{gneiting2007strictly}):
\[
\text{IS}_\alpha(l_t,u_t;y_t) = (u_t - l_t) + \frac{2}{\alpha} (l_t - y_t) \mathbb{1}(y_t<l_t) + \frac{2}{\alpha} (y_t-u_t) \mathbb{1}(y_t>u_t),
\]
where $\alpha \in (0,1)$, $l_t$ and $u_t$ are the lower and upper bounds of the $(1-\alpha) \times 100 \,\%$  prediction interval and $y_t$ is the actual value of the time series at time $t$. We use $\alpha = 0.1$, i.e. we score prediction interval whose nominal coverage is 90\%.


Finally, we evaluate the point forecasts measuring the squared error (SE)
and the absolute error (AE):
\begin{align*}
\text{SE} &= \left( y_t - \hat{y}_{t\mid t-1}\right)^2,\\
\text{AE} &= \left| y_t - \hat{y}_{t\mid t-1}\right|,
\end{align*}
where $\hat{y}_{t\mid t-1}$ denotes the optimal point forecast.
The optimal point forecast depends on the error measure:
it is the median of the predictive distribution for AE, and the expected value for SE \parencite{kolassa2016evaluating, kolassa2020best}.

\paragraph{Skill score}
The \textit{skill score} measures the improvement of the reconciled forecasts over the  base  forecasts.
For example, the skill score for AE is defined as:
\begin{equation} \label{eq: def skill score}
\text{skill}(\textit{reconc}, \textit{base}) 
= \frac{\text{AE}(\textit{base}) - \text{AE}(\textit{reconc})}
{\left(\text{AE}(\textit{base}) + \text{AE}(\textit{reconc})\right) / 2}.
\end{equation}
For all indicators, a positive  skill score implies  an improvement 
of the reconciled forecast compared to the base forecasts, and vice versa.
The skill score defined in \eqref{eq: def skill score} is symmetric, allowing  to fairly  compare base and reconciled forecasts. 
For instance, a skill score of 1 implies that the loss function has been reduced by three times: $(3-1) / ((3+1)/2) = 2/2 = 1$. Analogously, a skill score of $-1$ implies a three-fold worsening of the loss function.
Moreover, the skill score is bounded between $-2$ and $2$.

\begin{table}[!ht]
    \centering
    \small
\begin{tabular}{c b c b c b c}
\toprule
   & \textit{ALL} & \textit{FIN} & \textit{ICT} & \textit{MFG} & \textit{ENG} & \textit{TRD} \\
\midrule
\textbf{ES} & \multicolumn{6}{c}{0.89}  \\
\textbf{IS} & 0.87 & 1.15 & 0.20 & 1.07 & 0.22 & 0.18 \\
\textbf{SE} & 0.82 & 1.10 & 1.11 & 1.07 & 1.12 & 1.11 \\
\textbf{AE} & -0.02 & -0.02 & -0.02 & -0.02 & -0.02 & -0.02 \\
\bottomrule
\end{tabular}
\caption{\label{tab: skill scores} Average skill scores for  the different  time series. Positive values indicate an improvement of the reconciled forecasts over the base forecasts. The ES is computed with respect to the joint distribution on the entire hierarchy.}
\end{table}


\begin{figure}[h!]
    \centering
    \includegraphics[width=0.65\textwidth]{Boxplot_ss_ES_all.pdf}
    \caption{\label{fig: boxplot ss es} Boxplot of the skill scores on ES, over 3508 reconciliations.}
\end{figure}


\begin{table}[!htp]
    \centering
    \small
\begin{tabular}{cbcbcbc}
\toprule
   & \textit{ALL} & \textit{FIN} & \textit{ICT} & \textit{MFG} & \textit{ENG} & \textit{TRD} \\
\midrule
\textit{base} & 6.9 & 3.2 & 1.1 & 1.9 & 1.3 & 0.9 \\
% \midrule
\textit{reconc.} & 3.3 & 2.1 & 0.9 & 1.2 & 1.0 & 0.8 \\
\bottomrule
\end{tabular}
    \caption{Average width of the $90\%$ prediction interval.}
    \label{tab: width intervals average} 

\bigskip

\begin{tabular}{cbcbcbc}
\toprule
   & \textit{ALL} & \textit{FIN} & \textit{ICT} & \textit{MFG} & \textit{ENG} & \textit{TRD} \\
\midrule
\textit{base} & 96\% & 98\% & 98\% & 98\% & 98\% & 99\% \\
\textit{reconc.} & 91\% & 95\% & 97\% & 97\% & 97\% & 98\% \\
\bottomrule
\end{tabular}
\caption{Empirical coverage of the $90\%$ prediction intervals.}
\label{tab: coverage intervals}
\end{table}

In Table~\ref{tab: skill scores} we report the skill scores averaged over the 3508 reconciliations.
Reconciliation largely improves the ES (with an average skill score of $0.89$) and the 
IS  (average skill score ranging between 0.2 and 1.1, depending on the chosen time series).
The boxplot of the skill scores on ES on each day (Fig.~\ref{fig: boxplot ss es}) confirms the improvement due to reconciliation. 
As a further insight, reconciliation  reduces by 15-50\% the width of the prediction intervals  (Table~\ref{tab: width intervals average}) without compromising their coverage  (Table~\ref{tab: coverage intervals}).
We observe large skill scores (0.8-1) on the SE.
% In most cases, reconciliation  reduces the variance,  shortening the right tail: hence, the mean of the reconciled distribution is closer to 0, reducing the SE.
%
On the other hand, the skill core on AE is close to 0.
Indeed, often the median of the predictive distribution is already  0,
and it does not change after reconciliation.

Hence, in our experiments, reconciliation yields a major improvement over  the base forecasts, confirming  the positive effect of 
probabilistic reconciliation
\parencite{corani2022probabilistic,zambon2022} and
point forecast reconciliation \parencite{KOURENTZES-elucidate}
for intermittent time series.


\paragraph{Coherent vs optimal point forecast for the upper time series}

Even if we have a reconciled distribution,
only the SE-optimal point forecasts are  coherent.
The AE-optimal point forecasts, i.e., the medians of the reconciled distributions, are instead generally incoherent \parencite{kolassa22we}.

Hence, besides optimal point forecasts, we evaluate the coherent point forecasts: we  take the sum of the medians of the reconciled bottom distributions and use it as upper point forecast.
We compute the mean absolute error for the upper time series obtained using the AE-optimal, the coherent, and the base point forecasts (Table~\ref{tab: AE ALL}). 
As expected the AE worsens by imposing coherence rather than optimality; yet, it remains better than the AE of the base forecasts.

\begin{table}[!ht]
    \centering
    \small
\begin{tabular}{l c c c}
\toprule
   & \textit{optimal} & \textit{coherent} & \textit{base} \\
\midrule
\textbf{ALL} & 1.10 & 1.24 & 1.33 \\
\bottomrule
\end{tabular}
\caption{\label{tab: AE ALL} Mean absolute error for the upper time series ``ALL''.}
\end{table}


\begin{figure}[!htp]
    \centering
    \begin{subfigure}{0.8\textwidth}
        \includegraphics[width=\linewidth]{pmf/Pmf_123_2B_text_False.pdf}
        \subcaption{Strengthening effect. The reduction of variance of the asymmetric distribution shortens the right tail and decreases the mean.}
        \label{fig: pmf 123}
    \end{subfigure}

\bigskip
\bigskip

    \begin{subfigure}{0.8\textwidth}
        \includegraphics[width=\linewidth]{pmf/Pmf_1699_2B_text_False.pdf}
        \subcaption{Compromise effect}
        \label{fig: pmf 1699}
    \end{subfigure}
    \caption{Strenghtening vs compromise effect. We plot the base and reconciled probability mass functions for  ``ALL'', ``FIN'', and ``ICT''. For  ``ALL'', we also show the bottom-up pmf.}
\end{figure}


\subsection{Analysis of the reconciled mean and variance}



In most reconciliations (96\%), we observe the strengthening effect, i.e., the reconciled upper mean is lower than both the bottom-up and the base upper mean.
In Fig.~\ref{fig: pmf 123}, we show an example.
Reconciliation largely reduces the variance
of ``ALL'' and ``FIN'',    shortening the right tail of the distribution.
Within the bottom time series, reconciliation mostly affects FIN, which is characterized by the largest counts and overdispersion.
The predictive distribution for other time series, such as  ICT (shown in figure), are less affected by  reconciliation, being characterized by lower counts and variability.
The reduction of the variance  shifts  the expected values towards zero, since the base distributions have a positive skewness.


In Fig.~\ref{fig: pmf 1699}, we show an example of the compromise effect.
The reconciled distribution of ``ALL'' is a compromise between its  base forecast and the probabilistic bottom-up.  
Reconciliation decreases the expected values of the bottom time series, while increasing the  expected value of the upper time series. 

\begin{figure}[!h]
    \centering
    \begin{subfigure}{0.8\textwidth}
        \includegraphics[width=\linewidth]{pmf/Pmf_2307_2B_text_False.pdf}
    \end{subfigure}
    \caption[]{An example of reconciliation which increases the variance of the bottom forecasts, due to large a incoherence of the base forecasts (note the $X$-axis of ``ALL'').
    Variance of ``FIN'': $11.4$ (base), $14.3$ (reconciled).
    Variance of ``ICT'': $2.2$ (base), $2.6$ (reconciled).}
    \label{fig: pmf 2307}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The variance of all the variables decreases in most cases (97\%).
However, in Fig.~\ref{fig: pmf 2307} we show an example in which  the variance of the  bottom variables increases after reconciliation because of conflicting  base forecasts.
The forecast of the upper time series, which is characterized by large uncertainty, is instead 
sharply shifted towards smaller values.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Conclusions}
\label{sec: conclusions}

This is the first work to formally study the properties of the reconciled distributions.
We  proved that the reconciliation  surely decreases the variance of all variables in the Gaussian case, while it can increase or decrease the variance of count forecasts, depending on the incoherence of the base forecasts.
Moreover, we have observed two different effects of reconciliation on the upper mean.
We confirmed our theoretical study through the empirical analysis of time series of extreme market events. 

Future work might include the development of more sophisticated approaches for modeling the correlations between the base forecasts and the extension of this analysis to the cross-temporal reconciliation.

% \bibliographystyle{apalike}
% \bibliography{biblio}

\printbibliography

\newpage
\appendix

\section{Reconciled distribution in the Gaussian case}
\label{appendix: derivation gaussian}

\paragraph{Bottom distribution}

Let us define $\Tbf \in \rr^{n \times n}$ as
\begin{equation*}
\Tbf = \begin{bmatrix} \textbf{0} & \Ibf_m \\
\Ibf_{n-m} & -\Abf \end{bmatrix},
\end{equation*}
%
and let $\Zbf := \Tbf\YH$.
Hence, $\Zbf$ is Gaussian:
\begin{equation}
\Zbf \sim \mathcal{N}\left( \Tbf \yH, \,\Tbf\SigH_Y \Tbf^T \right).
\end{equation}
We have
\begin{align}\label{eq: T Sigma T}
\Tbf \yH &= \begin{bmatrix} \bH \\ \uH - \Abf \bH \end{bmatrix}, \nonumber \\
\Tbf \SigH_Y \Tbf^T &= \begin{bmatrix} \SigH_B & \SigH_{UB}^T - \SigH_B \Abf^T \\
\SigH_{UB} - \Abf \SigH_B & \Qbf \end{bmatrix}, \end{align}
where $\Qbf= \SigH_U - \SigH_{UB} \Abf^T - \Abf \SigH_{UB}^T + \Abf \SigH_B \Abf^T$.
%
Since 
\[\Zbf = \begin{bmatrix} \BH \\ \UH - \Abf\BH \end{bmatrix} =: \begin{bmatrix} \Zbf_1 \\ \Zbf_2 \end{bmatrix},\]
the reconciled bottom distribution is given by the conditional distribution of $\Zbf_1$ given $\Zbf_2=0$. %, hence it is a multivariate normal:
Assuming that the covariance matrix of $\Zbf_2$ is positive definite, we have 
\begin{equation*}
\Zbf_1 \,|\, \Zbf_2 = 0 \sim \mathcal{N}\left(\bT, \,\SigT_B \right), 
\end{equation*}
where
\begin{align}
\bT &= \bH + \left(\SigH_{UB}^T - \SigH_B \Abf^T\right) \Qbf^{-1} (\Abf \bH - \uH), \label{eq: reconciled gaussian mean bottom appendix} \\
\SigT_B &= \SigH_B - \left(\SigH_{UB}^T - \SigH_B \Abf^T\right) \Qbf^{-1} \left(\SigH_{UB}^T - \SigH_B \Abf^T\right)^T. \label{eq: reconciled gaussian variance bottom appendix}
\end{align}
%
Note that \eqref{eq: reconciled gaussian mean bottom appendix} and \eqref{eq: reconciled gaussian variance bottom appendix} are precisely the formulas (11) and (12) obtained by \cite{corani_reconc}, if we set
\begin{equation*}
\SigH_{UB} = - k_h \Mbf_1^T, \quad
\SigH_B = k_h \SigH_{B,1}, \quad
\SigH_U = k_h \SigH_{U,1}.
\end{equation*}

\paragraph{Upper distribution}

Since $\UT = \Abf \BT$, we have that $\UT \sim \mathcal{N}\left( \uT, \,\SigT_U \right)$, with
\begin{align} \label{eq: gauss rec upper mean and var}
\uT = \Abf \bT, \qquad
\SigT_U = \Abf \SigT_B \Abf^T.
\end{align}
%
If we define $\Dbf := \SigH_U - \SigH_{UB} \Abf^T$, from \eqref{eq: gauss rec upper mean and var} and \eqref{eq: reconciled gaussian mean bottom appendix} we have
\begin{align}
\uT &= \Abf \bH + \left(\Abf \SigH_{UB}^T - \Abf \SigH_B \Abf^T\right) \Qbf^{-1} (\Abf \bH - \uH) \nonumber \\
&= \Abf \bH + \left(\Dbf - \Qbf\right) \Qbf^{-1} (\Abf \bH - \uH) \nonumber \\
&= \Abf \bH + \Dbf \Qbf^{-1} (\Abf \bH - \uH) - (\Abf \bH - \uH) \nonumber \\
&= \uH + \left(\SigH_U - \SigH_{UB} \Abf^T\right) \Qbf^{-1} (\Abf \bH - \uH). \nonumber
\end{align}
%
Moreover, from \eqref{eq: gauss rec upper mean and var} and \eqref{eq: reconciled gaussian variance bottom appendix}:
\begin{align}
\SigT_U &= \Abf \SigH_B \Abf^T - \left(\Abf \SigH_{UB}^T - \Abf \SigH_B \Abf^T\right) \Qbf^{-1} \left(\Abf \SigH_{UB}^T - \Abf \SigH_B \Abf^T\right)^T  \nonumber \\
&=  \Abf \SigH_B \Abf^T - \left(\Dbf - \Qbf\right) \Qbf^{-1} \left(\Dbf^T - \Qbf\right)  \nonumber \\
&=  \Abf \SigH_B \Abf^T - \Dbf \Qbf^{-1} \Dbf^T + \Dbf + \Dbf^T - \Qbf \nonumber \\
&= \SigH_U - \left(\SigH_U - \SigH_{UB} \Abf^T\right) \Qbf^{-1} \left(\SigH_U - \SigH_{UB} \Abf^T\right)^T. \nonumber
\end{align}


\section{Proof of Proposition \ref{prop: var gauss}}
\label{app: proof var ineq}

% From \eqref{eq: T Sigma T}, since $\Tbf \SigH_Y \Tbf^T$ is positive semi-definite, we have that $\Qbf$ is also positive semi-definite. 
Since the matrix $\Qbf$ is positive definite, $\Qbf^{-1}$ is also positive definite.
Therefore, the matrices
\begin{align}
\Gbf := \left(\SigH_{UB}^T - \SigH_B \Abf^T\right) \Qbf^{-1} \left(\SigH_{UB}^T - \SigH_B \Abf^T\right)^T, \nonumber \\
\Hbf := \left(\SigH_U - \SigH_{UB} \Abf^T\right) \Qbf^{-1} \left(\SigH_U - \SigH_{UB} \Abf^T\right)^T, \nonumber
\end{align}
are positive semi-definite. 

From \eqref{eq: reconciled gaussian variance bottom}, we have that, for each $i=1,\dots,m$
\[\Var(\Btil_i) = \Var(\Bhat_i) - G_{ii} \le \Var(\Bhat_i),\]
as $G_{ii} \ge 0$ since the matrix $\Gbf$ is positive semi-definite.
Analogously, we have 
\[\Var(\Util_j) = \Var(\Uhat_j) - H_{jj} \le \Var(\Uhat_j),\]
for all $j=1,\dots,n-m$.

\section{Proof of Proposition \ref{prop: reconc variance}}
\label{app: proof rec var}

Let us denote $Z := \mathbb{1}_{\{ \UH = \Abf \BH \}}$, so that $Z=1$ when the constraint is satisfied, and $0$ otherwise.
By the law of total variance \parencite{Weiss2005ACI}, for any $j=1,\dots,m$, we have
\begin{equation}
\label{eq: law of total variance}
\Var\big(\Bhat_j\big) = \E\big[\Var\big(\Bhat_j|Z\big)\big] + \Var\big(\E\big[\Bhat_j|Z\big]\big).
\end{equation}
%
Since 
\begin{equation*}
\E\big[\Bhat_j | Z\big] = \begin{cases}
\E\big[\Bhat_j | \UH = \Abf \BH\big] \quad \text{if } Z = 1 \\
\E\big[\Bhat_j | \UH \neq \Abf \BH\big] \quad \text{if } Z = 0,
\end{cases}
\end{equation*}
%
we have that $\E[\Bhat_j | Z] = a + (b-a) \text{Ber}$, where $\text{Ber} \sim \text{Bernoulli}(p_c)$;
we recall that $a := \E[B_j | \UH \neq \Abf \BH]$, $b := \E[B_j | \UH = \Abf \BH]$, and $p_c := \prob(\UH = \Abf \BH)$.
%
Hence
\begin{equation}
\label{eq: var(E)}
\Var\big(\E\big[\Bhat_j|Z\big]\big) = (b-a)^2 p_c (1-p_c).
\end{equation}
%
Moreover, since 
\begin{equation*}
\Var\big[\Bhat_j | Z\big] = \begin{cases}
\Var\big[\Bhat_j | \UH = \Abf \BH\big] \quad \text{if } Z = 1 \\
\Var\big[\Bhat_j | \UH \neq \Abf \BH\big] \quad \text{if } Z = 0,
\end{cases}
\end{equation*}
we have 
\begin{equation}
\label{eq: E(var)}
\E\big[\Var\big(\Bhat_j|Z\big)\big] = p_c \, \Var[\Bhat_j | \UH = \Abf \BH] + (1-p_c) \, \Var\big[\Bhat_j | \UH \neq \Abf \BH\big].    
\end{equation}
%
From \eqref{eq: law of total variance}, \eqref{eq: var(E)}, and \eqref{eq: E(var)}, we have
\begin{align*}
\Var\big(\Bhat_j\big) &= p_c \, \Var\big[\Bhat_j | \UH = \Abf \BH\big] + (1-p_c) \, \Var\big[\Bhat_j | \UH \neq \Abf \BH\big] \nonumber \\
& \, + p_c (1-p_c) \, (a-b)^2,
\end{align*}
from which 
\begin{align*}
\Var\big[\Btil_j\big]
&= \Var\big[\Bhat_j | \UH = \Abf \BH\big] \\
&= \frac{\Var\big(\Bhat_j\big) - (1-p_c) \, \Var\big[\Bhat_j | \UH \neq \Abf \BH \big] - p_c (1-p_c) \, (a-b)^2}{p_c}.
\end{align*}


\section{Bernoulli example}
\label{app: example bernoulli}

Let us denote by $\pihat_1$ and $\pihat_2$ the probability mass functions of $\Bhat_1$ and $\Bhat_2$, so that $\pihat_1(0) = 1 - \phat_1$, $\pihat_1(1) = \phat_1$, and $\pihat_1(k)=0$ for any $k \neq 0,1$.
The probability mass function $\pi_U$ of $\Uhat$ is defined as $\pihat_U(0) = \qhat_0$, $\pihat_U(1) = \qhat_1$, $\pihat_U(2) = \qhat_2$, and $\pihat_U(k) = 0$ for any $k \neq 0,1,2$.

Since, from \eqref{eq: reconc density}, the reconciled probability mass function $\pitil_B$ is given by 
\begin{equation*}
\pitil_B(b_1,b_2) \propto \pihat_{1}(b_1) \pihat_{2}(b_2) \pihat_U(b_1+b_2),
\end{equation*}
the reconciled bottom distribution can be expressed as
\begin{equation*}
(\widetilde{B}_1, \widetilde{B}_2) = \begin{cases}
(0,0) \qquad \text{prob} = (1-\phat_1)(1-\phat_2)\qhat_0 / S \\
(1,0) \qquad \text{prob} = \phat_1(1-\phat_2)\qhat_1 / S \\
(0,1) \qquad \text{prob} = (1-\phat_1)\phat_2\qhat_1 / S \\
(1,1) \qquad \text{prob} = \phat_1\phat_2\qhat_2 / S,
\end{cases}
\end{equation*}
where $S := (1-\phat_1)(1-\phat_2)\qhat_0 + \phat_1(1-\phat_2)\qhat_1 + (1-\phat_1)\phat_2\qhat_1 + \phat_1\phat_2\qhat_2$ is the normalizing constant.
%
Hence
\begin{equation*}
\widetilde{B}_1 \sim \textit{Bernoulli}\,(\ptil_1), \quad
\widetilde{B}_2 \sim \textit{Bernoulli}\,(\ptil_2),
\end{equation*}
%
with
\begin{align}
\ptil_1 &= \frac{[(1-\phat_2)\qhat_1 + \phat_2 \qhat_2] \phat_1}
{S}, \nonumber \\
\ptil_2 &= \frac{[(1-\phat_1)\qhat_1 + \phat_1 \qhat_2] \phat_2}
{S}. \label{eq: ptil bernoulli}
\end{align}
%
Finally,
\begin{equation}
\label{eq: qtil bernoulli}
\Util = \begin{cases}
0 \qquad \text{prob} = (1-\phat_1)(1-\phat_2)\qhat_0 / S \\
1 \qquad \text{prob} = (\phat_1+\phat_2-2\phat_1\phat_2)\qhat_1 / S \\
2 \qquad \text{prob} = \phat_1\phat_2\qhat_2 / S.
\end{cases}
\end{equation}


\end{document}
