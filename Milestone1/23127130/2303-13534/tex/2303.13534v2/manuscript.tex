%% This is file `sample-acmsmall-submission.tex',
%
%% Commands for TeXCount
%TC:macro \cite [option:text,text]
%TC:macro \citep [option:text,text]
%TC:macro \citet [option:text,text]
%TC:envir table 0 1
%TC:envir table* 0 1
%TC:envir tabular [ignore] word
%TC:envir displaymath 0 word
%TC:envir math 0 word
%TC:envir comment 0 0
%%
%%
% https://chi2023.acm.org/submission-guides/chi-publication-formats/
% for submission: manuscript,review,anonymous
\documentclass[manuscript,nonacm]{acmart}%
% \documentclass[acmsmall]{acmart}%
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%TC:ignore
\usepackage{afterpage}%
\usepackage{tabularx}%
\usepackage{color}%
\usepackage{soul}%
\usepackage[normalem]{ulem}%
%     \definecolor{HLColor}{rgb}{1,0.94,0.72}
%     \sethlcolor{HLColor}%
\newcommand{\todo}[1]{{\leavevmode\color{black}#1}}
\newcommand{\ok}[1]{{\leavevmode\color{black}#1}}
\newcommand{\revision}[1]{{\leavevmode\color{black}#1}}
\usepackage{multirow}%
\usepackage{makecell}%
\usepackage[export]{adjustbox}% 
\usepackage{graphicx}%
\usepackage{subcaption}%
\usepackage{booktabs}%
\usepackage{framed}%
%%%
% \usepackage{sparklines}
% \def\sparkrectangleh #1 #2 {%
%   \ifdim #1pt > #2pt
%         \errmessage{The left corner #1 of rectangle cannot be lower than #2}%
%   \fi
%   {\pgfmoveto{\pgforigin}\color{sparkrectanglecolor}%
%   \pgfrect[fill]{\pgfxy(#1, 0)}{\pgfxy(#2-#1,1)}}}%
% \setlength\sparklinethickness{0.5pt}
% \def\sparklineheight ex{7pt}
%TC:endignore
%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% \usepackage{setspace}%
% \setstretch{1.5}%

%TC:ignore
\setcopyright{acmlicensed}%
\copyrightyear{2023}%
\acmYear{2023}%
% \acmDOI{XXXXXXX.XXXXXXX}%
% \acmJournal{JACM}%
% \acmVolume{37}%
% \acmNumber{4}%
% \acmPrice{}%
% \acmArticle{}%
% \acmMonth{7}%
%TC:endignore

%TC:ignore
% remove acm reference format for review - add again later!
% \settopmatter{printacmref=false}
% \setcopyright{none} 
%TC:endignore

% \usepackage{lscape}%

% ----------------------
% deactivate the revision highlighting to produce cam-ready version
\renewcommand{\hl}[1]{#1}
% ----------------------

%% Submission ID.
%% Use this when submitting an article to a sponsored event. You'll
%% receive a unique submission ID from the organizers
%% of the event, and this ID should be used as the parameter to this command.
%%\acmSubmissionID{123-A56-BU3}

%%
%% For managing citations, it is recommended to use bibliography
%% files in BibTeX format.
%%
%% You can then either use BibTeX with the ACM-Reference-Format style,
%% or BibLaTeX with the acmnumeric or acmauthoryear sytles, that include
%% support for advanced citation of software artefact from the
%% biblatex-software package, also separately available on CTAN.
%%
%% Look at the sample-*-biblatex.tex files for templates showcasing
%% the biblatex styles.
%%

%%
%% The majority of ACM publications use numbered citations and
%% references.  The command \citestyle{authoryear} switches to the
%% "author year" style.
%%
%% If you are preparing content for an event
%% sponsored by ACM SIGGRAPH, you must use the "author year" style of
%% citations and references.
%% Uncommenting
%% the next command will enable that style.
%%\citestyle{acmauthoryear}

% ====================
%TC:ignore
\input{prompts.tex}
%TC:endignore
% ====================

%% end of the preamble, start of the body of the document source.
\begin{document}

%% The "title" command has an optional parameter,
%% allowing the author to define a "short title" to be used in page headers.
%TC:ignore
\title[%
    % Prompting for Text-to-Image Generation: An Investigation into a Novel Creative Skill
]{%
    Prompting AI Art: An Investigation into the Creative Skill of Prompt Engineering
}%
%TC:endignore

%TC:ignore
%% The "author" command and its associated commands are used to define
%% the authors and their affiliations.
%% Of note is the shared affiliation of the first two authors, and the
%% "authornote" and "authornotemark" commands
%% used to denote shared contribution to the research.
\author{Jonas Oppenlaender} % (\href{https://orcid.org/0000-0002-2342-1540}{0000-0002-2342-1540})}
% % \authornote{Both authors contributed equally to this research.}
\email{jonas.oppenlander@elisa.fi}
\orcid{0000-0002-2342-1540}
% \authornotemark[1]
\affiliation{%
  \institution{Elisa Corporation}
  \city{Helsinki}
  \country{Finland}
  \streetaddress{Ratavartijankatu 5}%
  \postcode{00520}
}

\author{Rhema Linder}
\email{rlinder@utk.edu}
\orcid{0000-0003-4720-6818}
\affiliation{%
  \institution{University of Tennessee}
  \city{Knoxville}
  \state{Tennessee}
  \country{United States}
  % \streetaddress{Seminaarinkatu 15}%
  % \postcode{40014}
}

\author{Johanna Silvennoinen}
\email{johanna.silvennoinen@jyu.fi}
\orcid{0000-0002-0763-0297}
\affiliation{%
  \institution{University of Jyväskylä}
  \city{Jyväskylä}
  \country{Finland}
  % \streetaddress{Seminaarinkatu 15}%
  % \postcode{40014}
}
%TC:endignore

% \makeatletter
% \let\@authorsaddresses\@empty
% \makeatother


% \author{Lars Th{\o}rv{\"a}ld}
% \affiliation{%
%   \institution{The Th{\o}rv{\"a}ld Group}
%   \streetaddress{1 Th{\o}rv{\"a}ld Circle}
%   \city{Hekla}
%   \country{Iceland}}
% \email{larst@affiliation.org}

%% By default, the full list of authors will be used in the page
%% headers. Often, this list is too long, and will overlap
%% other information printed in the page headers. This command allows
%% the author to define a more concise list
%% of authors' names for this purpose.
% \renewcommand{\shortauthors}{Trovato et al.}

%%
%% The abstract is a short summary of the work to be presented in the
%% article.
\begin{abstract}%
%%% ORIGINAL
% Humankind is entering a novel era of creativity --- an era in which anybody can synthesize digital content. The paradigm under which this revolution takes place is prompt-based learning (or in-context learning). This paradigm has found fruitful application in text-to-image generation where it is being used to synthesize digital images from zero-shot text prompts in natural language for the purpose of creating AI art. This activity is referred to as prompt engineering --- the practice of iteratively crafting prompts to generate and improve images.
% In this paper, we investigate prompt engineering as a novel creative skill for creating prompt-based art. In three studies with participants recruited from a crowdsourcing platform, we explore whether untrained participants could 1) recognize the quality of prompts, 2) write prompts, and 3) improve their prompts. Our results indicate that participants could assess the quality of prompts and respective images. This ability increased with the participants' experience and interest in art. Participants further were able to write prompts in rich descriptive language. However, even though participants were specifically instructed to generate artworks, participants' prompts were missing the specific vocabulary needed to apply a certain style to the generated images. Our results suggest that prompt engineering is a learned skill that requires expertise and practice. Based on our findings and experience with running our studies with participants recruited from a crowdsourcing platform, we provide nine recommendations for conducting experimental research on text-to-image generation and prompt engineering with a paid crowd. Our studies offer a deeper understanding of prompt engineering thereby opening up avenues for research on the future of prompt engineering. We conclude by speculating on four possible futures of prompt engineering.%
%%% 150 words summary
We are witnessing a novel era of creativity where anyone can create digital content via prompt-based learning \hl{(known as prompt engineering)}.
% particularly in text-to-image \hl{generation} for AI art.
This paper delves into prompt engineering as a novel creative \hl{skill} for 
\hl{%
% producing prompt-based art.
creating AI art with text-to-image generation.}
\hl{In a pilot study, we find that many crowdsourced participants have knowledge about art which could be used for writing effective prompts.
In three subsequent studies, we explore whether crowdsourced participants can put this knowledge into practice.} We examine if \hl{participants} can 1) discern prompt quality, 2) \hl{write} prompts, and 3) refine prompts. \hl{We find that participants could evaluate prompt quality and crafted descriptive prompts, but they lacked style-specific vocabulary} necessary for effective \hl{prompting.}
\hl{
This is in line with our hypothesis that prompt engineering is a new type of skill that is non-intuitive and must first be acquired (e.g., through means of practice and learning) before it can be used.
% not an innate skill that can be applied without skill acquisition (e.g., through practice and learning).
% We posit that prompt engineering is a not an innate skill that can be applied without skill acquisition (e.g., through practice and learning).
}
Our studies deepen our understanding of prompt engineering and chart future research directions.
We offer nine guidelines for conducting research on text-to-image \hl{generation} and prompt engineering with paid crowds. We conclude by envisioning four potential futures for prompt engineering.
\end{abstract}%

%TC:ignore
%% http://dl.acm.org/ccs.cfm
\begin{CCSXML}
<ccs2012>
  <concept>
      <concept_id>10010405.10010469.10010470</concept_id>
      <concept_desc>Applied computing~Fine arts</concept_desc>
      <concept_significance>300</concept_significance>
  </concept>
  <concept>
      <concept_id>10003120.10003121.10003124</concept_id>
      <concept_desc>Human-centered computing~Interaction paradigms</concept_desc>
      <concept_significance>500</concept_significance>
  </concept>
</ccs2012>
\end{CCSXML}
\ccsdesc[300]{Applied computing~Fine arts}
\ccsdesc[500]{Human-centered computing~Interaction paradigms}
%
%% Keywords. The author(s) should pick words that accurately describe
%% the work being presented. Separate the keywords with commas.
\keywords{prompt engineering, prompting, text-to-image generation, AI art, creativity}%
%TC:endignore
%
\maketitle%
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% Front page
%
% ====================
\section{Introduction}%
% ====================
%
We are entering an era in which anybody can generate digital images from text --- a democratization of art and creative production.
In this novel creative era, humans work\hl{ }% on ``prompt-based engineering''
within a human-computer co-creative framework \cite{062-iccc20.pdf}\hl{. }
% Within this unfolding digital revolution, emerging digital technologies will co-evolve with humans. This requires the renewal of human capabilities and competences.
Emerging digital technologies will co-evolve with humans in this digital revolution, which requires the renewal of human capabilities and competences \cite{GenZ}.
One increasingly important human skill is \textit{\hl{prompting}} due to it providing an intuitive language-based interface to artificial intelligence (AI).
Prompting (or ``prompt engineering'') is the skill and practice of writing inputs (``prompts'') for generative models \cite{guidelines,aiartcreativity}.
Prompt engineering is iterative and interactive~--- a dialogue between humans and AI in an act of co-creation.
As generative models become more widespread, prompt engineering has become an important research area on how humans interact with AI \cite{prompt-programming,2209.01390.pdf,2212.07476.pdf,2022.acl-demo.9.pdf,3491101.3503564.pdf,guidelines,2209.11486.pdf,3544548.3580969.pdf}.

One area where prompt engineering has been particularly useful is the field of digital visual art. State-of-the-art image generation systems, such as OpenAI's DALL-E \cite{DALLE2}, Midjourney \cite{midjourney}, and Google's Imagen~\cite{2205.11487.pdf}, have been trained on large collections of text and images collected from the World Wide Web. These systems can synthesize high-quality images in a wide range of artistic styles from textual input prompts \cite{guidelines,artiststudies,modifiers,aiartcreativity}.
Practitioners of text-to-image generation often use prompt engineering to improve the quality of their digital artworks \cite{guidelines}. Within the community of practitioners, certain keywords and phrases have been identified that act as ``prompt modifiers'' \cite{modifiers}. These keywords can, if included in a prompt, improve the quality of the generative model's output or make images appear in a specific artistic style \cite{guidelines,artiststudies,aiartcreativity}.
While a short prompt may already produce impressive results with these generative systems, the use of prompt modifiers can help practitioners unlock the systems' full potential \cite{modifiers,aiartcreativity,2303.04587.pdf}. The skillful application of prompt modifiers can distinguish expert practitioners of text-to-image generation from novices.

%%% MOTIVATION FOR STUDY
% Text-to-image generation is a novel application area of prompt engineering.
% There is a strong and growing community of users who actively use generative systems to produce digital imagery and artwork.
% Several experiments successfully involved a crowd of people in text-to-image generation. Midjourney, for instance, is a community in which users collaboratively write prompts for generating digital artworks in Discord-based chat rooms~\cite{midjourney}.
% Collected with a similar chat-based technology, \citeauthor{pressmancrowson2022} recently presented Simulacra Aesthetic Captions, a dataset of crowdsourced prompts and imagery~\cite{pressmancrowson2022}.
% Contributions involving a human crowd could enable research into human aesthetic preferences and judgments as well as
% computational assessment of aesthetics, but also training machine learning models to generate prompts or detecting key phrases in prompts, and better aligning AI with human intent and values, for instance with the aim of guiding and training AI to follow human instructions~\cite{pressmancrowson2022,2203.02155.pdf}.

%%%%%
%%%%% MOTIVATE THE STUDY
%%%%%

\hl{Whether prompt engineering is an} \todo{intuitive}
\hl{skill or whether this skill must be acquired (e.g., through means of practice and learning) % there is a learning curve to it
has, so far, not been investigated.}
There are a number of reasons why such an investigation is important.
%
A look at StableDiffusion's Discord channel\footnote{https://discord.com/channels/1002292111942635562/} shows preliminary evidence that some prompts and keywords combinations % --- and especially some popular combinations of prompt modifiers ---
circulating in the community of practitioners are not intuitive\hl{.}
    Such keywords include, for instance, the modifier \textit{``by Greg Rutkowski''} or other popular modifiers, such as % \textit{``highly detailed,''} \textit{intricate,''} 
    \textit{``smooth,''} \textit{``elegant,''} \textit{``luxury,''} \textit{``octane render,''} and \textit{``artstation''}. \hl{These keywords are often used in combination with each other to boost the quality of generated images~\mbox{\cite{modifiers}}, resulting in} unintuitive keyword combinations that a human user would likely never have chosen \hl{to describe the image to another human}.
    These modifiers are \hl{commonly} % intuitively
    applied by practitioners in the AI art community, but may confront \hl{laypeople} with challenges of understanding the \hl{effect of modifiers} on the resulting image.
% The CLIP Interrogator\footnote{https://github.com/pharmapsychotic/clip-interrogator} % and BLIP \cite{2201.12086.pdf} tools,
% image captioning tool gives us a glimpse into what a text-to-image model ``sees'' in an image. % can identify objects and styles in images and outputs them as image caption. % the text-to-image generation system recognizes in a given image (a process called image captioning).
% Using this tool on any given image will result in unintuitive keyword combinations that a human user would likely never have chosen.
Further confounding the problem is that keywords in prompts can affect both the subject and style of a generated image \hl{simultaneously.}

Whether prompt engineering is a skill that humans apply intuitively or whether it is \hl{a new type of skill that needs to be acquired} is important not only for the field of AI art, but also for research on human-AI interaction and the future of work in general.
A source for unintuitiveness of the current practice of prompt engineering is a \hl{potential} misalignment between the written human prompt and the way in which text-to-image models interpret prompts.
    Compared to how we humans understand a prompt and its constituents, text-to-image \hl{generative} models may attach very different meanings to some keywords in the prompt.
\hl{Further,} many \hl{AI-generated} images are shared on social media, often with stunning results.
\hl{However,} if what we see on social media is the result of the application of prompt engineering by skilled experts, then the generative content that we encounter on social media could be skewed by a small group of highly skilled practitioners.
\hl{Or from another perspective,} if prompt engineering is an \hl{acquired} skill that requires expertise and training, this could give rise to novel creative professions with implications for the future of work.
% From a systemic perspective,
\hl{On the other hand,} we run the risk of assigning too much importance to prompting as a method for interacting with generative models \hl{if prompt engineering is an innate ability and an intuitive artistic skill that is acquired quickly~\mbox{\cite{GenAI,PERCEPTIONS}}.}

%%% METHOD
In this paper, we explore the creative skill of prompt engineering in \hl{four studies with a total of 355~participants} recruited from Amazon Mechanical Turk (MTurk), a popular microtask crowdsourcing platform.
The studies are summarized in \autoref{tab:studiesoverview}.

\input{TAB/TAB-STUDIES}

\hl{In a \textbf{pilot study}, we investigate whether crowd workers are able to recognize artists and styles of different artworks.
In practice, artist names and artwork styles are frequently used by expert practitioners of AI generated art to modify the style or improve the quality of digital artworks generated with text-to-image generation systems} \cite{guidelines,modifiers,artiststudies,aiartcreativity}.
\hl{Knowledge about artists and art styles is, therefore, an important pre-requisite for prompt engineering of AI generated art.
Having this expertise could enable participants to create digital artworks by writing effective prompts for deep learning based text-to-image generation systems.}

\textbf{\hl{Key insights from the pilot study:}}
\hl{We find that crowd workers recruited from a paid crowdsourcing platform are surprisingly good at recognizing the style and names of artists. Many workers were able to assign the correct artistic style or artist name to a given painting. This, in theory, would enable crowd workers to use artist styles and artist names as keywords when prompting text-to-image generation systems, potentially making them experts in prompt writing for AI art.}

In \textbf{Study~1}, we explore participants' understanding of how a text-to-image \hl{generation} system produces images of varying quality depending on the phrasing of input prompts.
A feeling of what contributes to the quality of a prompt could enable participants to write prompts and create high-quality images.
In our within-subject experiment, participants separately rated the aesthetic appeal of textual prompts and matching images generated with a text-to-image \hl{generation} system.
We hypothesize that a high degree of consistency within the participants' two ratings may point toward there being a strong understanding of what makes a ``good'' prompt.

\textbf{\hl{Key insights from Study 1:}}
\hl{We find crowd workers are able to grasp what makes a ``good'' prompt. Being able to discern good from bad prompts would, in theory, allow workers to write effective prompts. }
% This ability increased with the participants' experience and interest in art.

\hl{In \mbox{\textbf{Study~2}}, we test the insights from the above two studies in practice. We invite participants to apply their knowledge and expertise} by writing three input prompts for a text-to-image \hl{generation} system with the specific aim of creating a digital artwork \hl{(and without seeing the generated images)}.
We analyze participants' use of descriptive language and the use of prompt modifiers that could influence the quality and style of the resulting artworks.
%
In \textbf{Study~3}, we then invite the same participants who participated in the \hl{Study~2} to review the images generated from their own prompts. Each participant \hl{was asked to improve} their prompts with the specific task of creating an artwork of high visual quality.
% \todo{With this study, we investigate whether expertise in writing prompts is an intuitive skill or whether it is an expert skill, learned through repeated interactions with the generative system. % and practice.}
\hl{%
% If participants fail in their first attempt of writing a prompt, how quickly can they become proficient in using these systems? Would one repeated interaction with the text-to-image generation system be enough?
Our hypothesis is that if prompt engineering is an intuitive skill innate to humans, participants will be able to apply it immediately. On the other hand, if participants are not able to significantly improve their images due to few interactions with the text-to-image generation system within our studies, then this may indicate that the skill of prompt engineering needs to be acquired before it can be applied in practice.}

\textbf{\hl{Key insights from studies 2 and 3:}}
\hl{We find that while participants were able to describe artworks in rich descriptive language, almost none of the participants used specific keywords to adapt the style of their artworks or modify the images in other ways.
Moreover, participants were not able to significantly improve the quality of the artworks in the follow-up study.
This points to prompt engineering being a non-intuitive skill that people first need to acquire before it can be applied in meaningful ways.}


%%% KEY RESULTS

\hl{In summary, our four studies find that while participants had the prerequisites to write prompts for AI art and were good at crafting descriptive prompts, they lacked style-specific vocabulary necessary for effective prompt engineering.}
Due to our decision to recruit crowd workers as participants, our paper is the first to provide insights on how well paid crowd workers perform in experiments on prompt engineering and text-to-image generation.
Based on our findings and experience, we provide recommendations for conducting experiments on text-to-image generation and prompt engineering with an extrinsically motivated crowd in Section~\ref{sec:implications}.
We conclude by speculating on four potential futures for prompt engineering. % in Section~\ref{sec:creativeeconomy}.


% ====================
\section{Related Work}%
\label{sec:related-work}%
% ====================
%
%
% ====================
\subsection{Text-to-Image Generation with Deep Learning}%
\label{sec:background}%
% ====================
%
Text-to-image generation is a type of generative deep learning technology that allows users to create images from text descriptions. This technology has gained significant interest since early 2021, when OpenAI published the results of DALL-E \cite{DALL-E} and the weights of their CLIP model \cite{CLIP}. CLIP is a multi-modal model trained on over 400 million text and image pairs from the Web. The model can be used in text-to-image generation systems to guide the generation of high-fidelity images.
Many approaches and architectures for image generation with deep learning have since been developed, such as diffusion models \cite{2105.05233.pdf}. These approaches typically use machine learning models trained with contrastive language-image techniques using training data scraped from the Web. These systems are text-conditional, meaning they use text as input for image synthesis. This input, known as ``prompt,'' describes the image to the system, which then generates one or more images without further input.

% --------------
\subsection{Prompt Engineering for AI Art}%
\label{sec:promptengineering}%
% --------------
The practice of crafting input prompts is referred to as prompt engineering (or prompting for short).
% The main aim of prompt engineering is to control the output of generative systems with written input prompts. % in natural language.
% The aim of prompt engineering is to develop results that draw on a deep learning model's latent space.
\hl{%
In this section, we explain the `engineering' character of prompt engineering and how prompt engineering is applied for generating AI art.%
}%
%  and highlight the difference to automated approaches of prompt optimization.
%
%
\subsubsection{The engineering character of prompting}
The term prompt engineering was originally coined by % the Reddit user
Gwern Branwen in the context of writing textual inputs for OpenAI's GPT-3 language model~\cite{guidelines}.
`Engineering,' in this case, does not refer to a hard science as found in science, technology, engineering, and mathematics (STEM) disciplines.
    Prompt engineering is a term that originates from within the online community of practitioners.
    Practitioners include artists and creative professionals, but also novices, amateurs, and more serious ``Pro-Ams''~\cite{2556288.2557298.pdf} aiming, for instance, to sell their creations as digital art based on non-fungible tokens (NFTs) \cite{nft}.
    Not every member of this online community may identify as a prompt engineer. An alternative self-understanding could be % that of a
    ``promptist'' \cite{promptism} or ``AI artist'' \cite{Zylinska_2020_AI-Art.pdf}.
%
One aspect of prompt engineering that relates to its engineering character is that it often involves systematic experimentation through trial and error~\cite{guidelines}. The challenge for the prompt engineer is not only to find the right terms to describe an intended output and the right m, but also to anticipate how other people would have described and reacted to the output on the World Wide Web.
% This is because the training data for the generative system typically consists of scraped data from the World Wide Web.

% \subsubsection{Difference to soft prompting techniques}
% Prompt engineering is a language-based practice conducted by humans who write prompts in discrete tokens.
% Thus, it differs from ``soft prompting'' and prompt tuning approaches that aim to automatically optimize input for machine learning models.
% For example, prefix tuning \cite{2021.acl-long.353.pdf} optimizes continuous vectors to fine-tune pre-trained language models for downstream tasks, but it operates in vector space with ``virtual tokens'' rather than discrete tokens. Likewise, prompt tuning \cite{prompt-tuning} uses backpropagation to learn input prompts for a frozen language model to perform downstream tasks. Prompt optimization \cite{2205.12548.pdf} uses reinforcement learning to optimize prompts. In contrast to the above, prompt engineering involves manually writing and rephrasing prompts. % in natural language.
% As such, prompt engineering is closer to human-centered fields, such as Human-Computer Interaction, Computer-Supported Cooperative Work, Human-AI Interaction, and conversational AI than to the field of machine learning.

% --------------
% \subsection{Prompt Engineering for AI Art as a Skill}%
\subsubsection{Prompt engineering for creating AI art}
\label{sec:promptmodifiers}%
% --------------
\hl{Art generated by artificial intelligence, or ``AI art'' \mbox{\cite{Zylinska_2020_AI-Art.pdf}},}
has become a popular application for prompt engineering \cite{aiartcreativity}.
An online community \hl{around AI art} has formed, sharing images and prompts on various platforms. Within this community, certain practices for writing prompts have emerged. For example, prompts often follow a specific pattern, such as the following template \cite{travelersguide}:
\begin{quote}
  \textit{[Medium] [Subject] [Artist(s)] [Details] [Image repository support]}
\end{quote}
A typical prompt could be \cite{zippy}:
\begin{quote}
    \textit{A beautiful painting of a singular lighthouse, shining its light across a tumultuous sea of blood \underline{by greg rutkowski} \underline{and thomas kinkade}, \underline{trending on artstation}.}%
\end{quote}%

Prompt modifiers, such as the underlined terms above, are added to a prompt to influence the resulting image in a specific way \cite{guidelines,modifiers,aiartcreativity}. Prompt modifiers are an important technique in prompt engineering for AI art because they allow the prompt engineer to control the output of the text-to-image generation system. Prompt modifiers may make the resulting images subjectively more aesthetic and attractive~\cite{guidelines,aiartcreativity}.

Different types of prompt modifiers are used in the AI art community \cite{modifiers}, but the two most common types of modifiers affect the style and quality of images. These prompt modifiers consist of specific keywords and phrases that have been found to modify the style or quality of an image (or both).
Modifiers that affect the quality of images \hl{can be referred to as} `quality boosters' \cite{modifiers}, and include phrases such as \textit{``trending on artstation,'' ``unreal engine,'' ``CGSociety,'' ``8k,'' and ``postprocessing.''}
Style modifiers affect the style of an image and can include a wide variety of open domain keywords and phrases, such as \textit{``oil painting,'' % `\textit{`cubism,''}
``in the style of surrealism,''  or ``by James Gurney''}~\cite{modifiers}.

Human-centered research on prompt engineering for text-to-image synthesis \hl{in the field of Human-Computer Interaction (HCI)} is still in its early stages. %, with only a few papers published on the topic in the field of Human-Computer Interaction (HCI).
\citeauthor{guidelines}'s study on subject and style keywords in textual input prompts mentioned that without knowledge of prompt modifiers, users must engage in ``brute-force trial and error'' \cite{guidelines}. The authors presented design guidelines to help people produce better results with text-to-image generative models. \citeauthor{3527927.3532792.pdf} conducted an experiment on using images as visual input prompts, resulting in design guidelines for improving subject representations in AI art \cite{3527927.3532792.pdf}. Besides these guidelines, there are also many community-created resources that offer guidance for novices and practitioners of AI art, such as Ethan Smith's ``Traveler's Guide to the Latent Space''~\cite{travelersguide}, Zippy's ``Disco Diffusion Cheatsheet''~\cite{zippy}, and Harmeet Gabha's ``Disco Diffusion Artist Studies'' \cite{artiststudies}.
%
These resources provide a wealth of information about prompt modifiers  for producing high-quality visual artifacts.


% In the following section, we review studies and experiments that involved a crowd in the production of artworks or investigated the art expertise of crowd workers.


% \input{CROWD-ART-EXPERIENCE}


% --------------
\subsection{Prompt Engineering as a Skill}
\label{sec:skill}%
% --------------

\hl{%
Merriam-Webster defines `skill' as ``the ability to use one's knowledge effectively and readily in execution or performance'' and ``a learned power of doing something competently: a developed aptitude or ability'' \mbox{\cite{skill}}.
This definition captures the essence of skill as not only having knowledge, but also the capability to apply the knowledge effectively in practical situations and in real-world contexts.
}%

% 1. Although the related work discusses important concepts of prompt engineering skills, the manuscript lacks clear definitions of what constitutes a learned skill in prompt engineering and how prompt modifiers relate to learned skills.
\hl{%
In our work, we define `skill in prompt engineering' as the ability to effectively utilize language and % conceptual understanding
prior knowledge to craft prompts that % efficiently
guide generative models towards desired outputs. This encompasses not only the basic knowledge of the relevant language syntax but also the strategic use of prompt modifiers --- elements that refine or alter the direction of generative outputs. Our study aims to investigate whether individuals, particularly participants recruited from a crowdsourcing platform, possess the necessary foundational knowledge to write effective prompts. More importantly, we assess if these participants can translate this knowledge into practical application, demonstrating a skilled use of prompt modifiers in the context of text-to-image generation for AI art. The inability to effectively apply knowledge in practice may suggest a lack of skill, underscoring the need for knowledge acquisition and refinement through learning and training. This approach aligns with our objective to elucidate % the nature and scope of skills in prompt engineering.
whether prompt engineering is an innate ability or whether it must be acquired (e.g., through practice and learning).
}%

% We adopt this definition of skill in our work.
% The purpose of our work is to test whether novices recruited from crowdsourcing platforms have the knowledge to write effective prompts, and whether these participants can apply their knowledge in practice.
% Failure to apply the knowledge in practice may indicate an absence of skill, which would first require the skill to be learned and trained to be effective.


% --------------
\subsection{Prior Work on Applying Skill in Practice}%
% --------------

\hl{Our research is related to prior work focusing on how individuals acquire % and enhance their
skills and how they apply them in practice. In particular, learning how to use web search is a related area of work. This section is structured around three main themes identified in the literature: the divergence between search and domain expertise, the strategies involved in how people rewrite a query after a failed attempt, and teaching how to improve query authorship.}

\subsubsection*{Divergence Between Search and Domain Expertise}
\hl{%
The first theme addresses the relationship between domain expertise and search expertise. \mbox{\citeauthor{EXT-White}} provide an insightful analysis of how domain knowledge influences web search behavior \mbox{\cite{EXT-White}}. Their work demonstrates that domain experts and novices exhibit markedly different search strategies and outcomes.
This finding is supported and extended by \mbox{\citeauthor{EXT-Wood}}, who explore the relative contributions of domain knowledge and search expertise in conducting effective internet searches \mbox{\cite{EXT-Wood}}. These studies underscore the distinct nature of search expertise, separate from domain-specific knowledge, highlighting its importance in effective information retrieval.}

\subsubsection*{Query Reformulation Strategies}
\hl{%
The second theme revolves around how individuals modify their search queries following unsuccessful attempts. \mbox{\citeauthor{EXT-Huang}} offer a comprehensive examination of query reformulation strategies in web search logs \mbox{\cite{EXT-Huang}}. Their analysis reveals the common patterns and tactics users employ when their initial search queries fail to yield desired results. This research is crucial in understanding the adaptive behaviors of users in response to the challenges they encounter during web searches.
}%

\subsubsection*{Educational Approaches to Query Authorship}
\hl{%
The third theme relates to methods for teaching effective query formulation. \mbox{\citeauthor{EXT-Bateman}} contribute significantly to this area through their development of the Search Dashboard tool \mbox{\cite{EXT-Bateman}}. Their study examines how tools that facilitate reflection and comparison can impact users' search behaviors, leading to more effective search strategies. This work is particularly relevant for designing educational interventions and tools aimed at enhancing the search skills of users.
}%

\hl{%
Our studies draw upon and contribute to these existing bodies of work. We extend the understanding of how individuals apply their knowledge in writing prompts, and how they reformulate a prompt in an attempt to improve upon a first prompt.
Both are crucial aspects of information literacy in the digital age.
}%
% The insights from our research aims to provide new perspectives and tools for enhancing web search proficiency.





% ====================
\section{Pilot Study: Crowd Worker's Knowledge of Art}
\label{sec:pilot}
% ====================
\hl{The aim of this study is to explore the Fine Art knowledge of crowd workers. Knowledge of artist names and art styles are essential for writing effective prompts for the purpose of AI art generation} \cite{modifiers}.
% to selecting the right modifiers to add to textual prompts for generating digital artworks with text-to-image generation systems.
% We further test the hypothesis that MTurk workers are better at recognizing American artists than European artists. MTurk is a US-based crowdsourcing platform and empirical research on the demographics of MTurk workers has shown that US-based workers constitute the largest group of crowd workers (e.g.,~\cite{p135-difallah.pdf}).%

% --------------------
\subsection{Research Materials}%
% --------------------
\hl{
We collected 25 artworks by popular artists by searching
% \subsubsection{Artist and artwork selection}
for ``artists of the 20th century'' on Google Search.
% The search results page (SERP) for this query contains a scrollable carousel at the top of the page.
% From this carousel, we hand-selected 25 artists --
% This information was taken from the artist's knowledge panel on Google Search.
The set of artists included eleven artists from North America, eleven from Europe, and three with mixed or other background --- e.g., Belarusian--French and Russian.
% We collected information about the artists from their respective knowledge panel.
From each artist's knowledge panel on Google Search, we collected one artwork and researched the periods the artist is commonly associated with (e.g., Surrealism).
    % explain knowledge panel
    % Knowledge panels are boxes on the SERP that provide condensed information from Google's knowledge graph including an extract from a Wikipedia article.
    % To mitigate bias in the selection of artworks, we collected the first artwork listed in the artist's knowledge panel, with four exceptions.
        % For Frida Kahlo, we selected a different artwork due to the first result depicting close-up nudity (to avoid exposing workers to sensitive content). For Japser Johns, we selected ``Three Flags'' over ``Flags'' and for Edvard Munch we selected ``Anxiety'' over ``Vampire'' because these artworks may be more recognizable. Last, for Joan Miró, we selected ``The Harlequin's Carnival'' because the knowledge panel for this artwork contains information on the period of the artwork.
% For each artwork, we collected information on its style from the knowledge panel on the artwork's SERP. If this information was not available, we substituted with the periods associated with the artist.
The resulting dataset contains 25 artists and 25 artworks listed in \mbox{\autoref{tab:artworks}}.%
% The artworks and their styles are listed in \autoref{tab:artworks}.
}%

% --------------------
\subsection{Task Design and Recruitment}%
% --------------------
\hl{%
We designed a simple crowdsourcing task depicting one of the 25 artworks at a time.
Workers were instructed to identify the artist's name, the artwork's style, and the artwork's title.
The instructions included an explanation of the concept ``style'' taken from a Wikipedia page on style in the visual arts:}
    \textit{``The style of an artwork relates its visual appearance to other works by the same artist or one from the same period, training, location, `school', art movement or culture''} \cite{Wikipedia_Style}.
\hl{%
Workers were instructed to make a guess if they did not know an answer, but not to enter irrelevant information.
Workers were specifically instructed not to use a search engine to complete the task.
% The filenames of the images were anonymized to not allow easy reverse look-up of the information on a search engine.
We recruited US-based workers from Amazon Mechanical Turk (MTurk) with a task approval rate greater than 95\% and at least 1000 completed tasks. This combination of qualification criteria is common in crowdsourcing research (e.g., \mbox{\citet{3491102.3517434}}).
Workers were paid US\$0.05 per task (aiming for an average pay above the minimum wage in the United States).
The task pricing was calculated based on the average completion times in a short pilot ($N=26$, US\$0.02 per task).%
}%

\hl{%
The task was designed as a microtask. Therefore, no demographic data on the workers was collected in this study.
Three tasks were rejected due to no visible effort being made to answer the task with relevant information. The three tasks were republished for other workers to complete.
In total, the pilot study comprised 750 tasks (25 artworks $\times$ 30 tasks) completed by 128 unique workers.%
}%

% ------
\input{TAB/TABLE-STUDY1-ARTWORKS}%
% ------

% --------------------
% \subsection{Participant Recruitment}%
% --------------------
% Participants were recruited from two crowdsourcing platforms: Amazon Mechanical Turk and Prolific.
% In prior research, these two platforms have proven to be a fertile complement to one another in the context of creativity-related studies \cite{CHI20}.
% We found very few signs of insincere answers.
% One worker copied and pasted parts of the instructions and one entered numbers. The two workers were excluded from the data analysis and future studies.
%
% --------------------
\subsection{Analysis}%
% --------------------
\hl{%
We analyzed the data as follows.
For the two answers provided in each task (artist name and artwork style), we manually decided whether the answer was correct. This evaluation was straight-forward \mbox{\cite{McDonald_Reliability_CSCW19.pdf}} and did not require expert knowledge since it only consisted of matching the worker-provided answer with the list of artist names and artwork styles collected from the knowledge panel.
% No inter-rater agreement was calculated \cite{McDonald_Reliability_CSCW19.pdf}.
However, crowdsourced data is noisy \mbox{\cite{crowdsourcing-userstudies,3491102.3517434}} and our data makes no exception.
We decided not to penalize typographic mistakes and partially incorrect answers.
    For instance, we allowed Pollack as correct answer for the artist Jackson Pollock and Wood --- but not Grant --- as correct answer for Grant Wood.
    % Similarly for artwork styles, ``expressionism'' and ``abstract'' was counted as correct for Abstract Expressionism.
For Grant Wood, Edward Hopper, and Jasper Johns we additionally accepted Americana Art as style.
If the answer was correct but in the wrong input field, we counted the answer as correct, since minor improvements in the task design would likely correct this issue.%
}%
%
% Some workers entered the artist name and artwork style in the wrong fields.
% Some workers correctly entered the title of the artwork in the style field. Even though the worker demonstrated knowledge in this case, we did .....


% --------------------
\subsection{Results}%
% --------------------
\hl{
Workers were able to correctly identify at least one of the two answers in 538 cases (71.73\%)
and workers correctly identified both the artist and the artwork's style in almost 30\% of the tasks (% $n=216$;
28.80\%).%
Overall, the workers in our sample were able to correctly identify the artist in slightly more than half of the cases (%$n=395$;
52.67\%). The artwork's style was correctly identified 358 times (48.05\%).
% In about 70\% of the tasks (534; 71.20\%), workers were not able to guess either the artist name or artwork style correctly.
}%

\input{FIG/FIG-SCATTERPLOT}%

\hl{%
Some artists and artwork styles were recognized more frequently than others (see \mbox{\autoref{tab:artworks}}).
% While on the whole, detection rates were fairly low (about 50\%), some artists and artistic styles were recognized more often.
The top five correctly recognized artists were Jackson Pollock, Salvador Dali, Claude Monet, Frida Kahlo, and Pablo Picasso.
The least recognized artists were Georges Braque, Paul C\'ezanne, Egon Schiele, Joan Mir\'o, and Wassily Kandinsky.
The three styles recognized most often include
    Abstract expressionism (Jackson Pollock), Surrealism (Salvador Dal\'i), and Impressionism (Claude Monet).
The three styles recognized least often include
    the styles associated with Georgia O'Keeffe (see \mbox{\autoref{tab:artworks}}),
    Fauvism/Modernism (Henri Matisse),
    % and Surrealism/Dada (Man Ray).
    and Pop art/Contemporary art (Jeff Koons).
}%

\hl{%
Common mistakes made by workers include
% \begin{itemize}
    misspellings of artist names (e.g., Pollack instead of Pollock),
    attributing a painting to the wrong artist (e.g., Warhol instead of Lichtenstein), 
    entering artwork titles instead of artwork styles, and
    attributing an artist name based on information in the image.
% \end{itemize}
%
The latter was the case for Marcel Duchamp's ``Fountain'' which contains a legible inscription (``R. Mutt''). A high number of workers ($n=17$; 56.67\% of the whole response set for this artwork) entered the wrong artist name in this case.
%
Among the workers who entered the correct title of the artwork in one of the input fields ($n=81$; 10.8\% of all responses), some artwork titles were entered more frequently than others (c.f. \mbox{\autoref{tab:artworks}}).
The most recognized titles include
    Jasper John's ``Three Flags,''
    Grant Wood's ``American Gothic,''
    % Frida Kahlo	self-portrait,
    Pablo Picasso's	``Guernica,'' and
    Paul Klee's	``Castle and Sun.''%
% The high number of entries for Frida Kahlo can be explained by the title containing the generic keyword ``self-portrait''.
}%

\hl{%
We noticed differences in the workers' knowledge of art. Some workers were noticeably better at solving the given task than other workers (see 
\mbox{\autoref{fig:scatterplot}}).
A linear regression indicates that there is a positive relationship between the number of correct responses for artwork styles and artist names, $F(1,126)=138.1$, $p<0.001$, $R^2=0.5228$, $R^2_{adjusted}=0.519$.
% The regression coefficient (B=X, 95\% CI [x, x] indicated that xxx
%
% lm(formula = data$Artist.correct ~ data$Style.correct)
% Residuals:
%     Min      1Q  Median      3Q     Max 
% -6.7025 -1.5911 -0.7628  0.3027 18.4390 
% Coefficients:
%                    Estimate Std. Error t value Pr(>|t|)    
% (Intercept)          0.7628     0.3662   2.083   0.0393 *  
% data$Style.correct   0.8283     0.0705  11.750   <2e-16 ***
% ---
% Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1
% Residual standard error: 3.487 on 126 degrees of freedom
% Multiple R-squared:  0.5228,	Adjusted R-squared:  0.519 
% F-statistic: 138.1 on 1 and 126 DF,  p-value: < 2.2e-16
%
% Describe the best detection / worst detection rate
When ordered by their success rate in detecting artist names and artwork styles, the top 50\% of crowd workers account for over nine out of ten (90.63\%) correct responses of artist names and 93.87\% of correct style responses.
% Nine crowd workers were able to correctly identify both the artist name and artwork style. These workers, however, completed only one or two tasks overall.
    The top 20\% of well-performing workers account for about 40\% of the correct responses.
% This is due to the top workers having made only one or two guesses.
% Detection rates dropped the more guesses a worker made.
% The best performing worker was able to correctly guess xxx
% The distribution of responses is long-tailed. Few crowd workers completed many tasks, many completed few. Similar participation dynamics can be found on the World Wide Web, for instance on image rating websites~\mbox{\cite{joshi_1.pdf,icwsm15beauty.pdf}}.
The ratio of correct responses to the number of all responses left by each worker is two-tailed (see \mbox{\autoref{fig:scatterplot}}).
Almost one third of the crowd workers correctly identified both the artist and the style of the artwork in all of their completed tasks.%
}%

% Further, one in ten workers correctly entered the artwork title.
% We believe that detection rates could be improved if workers were allowed to use a search engine.

% To test our hypothesis that workers were better at recognizing the works of American artists compared to works from European artists, we compared the results for two equal sized groups of artists (eleven artists from North America and eleven from Europe).
% Workers had a significantly better ability to identify the artist name correctly if the artist was from North America as compared to Europe
%     (Fisher's Exact Test,
%     $p=0.01$, % 0.009791
%     $OR=0.66$, % 0.659071
%     95\% CI [0.48, 0.91]). % 0.478127, 0.907186
% We found no significant difference in the workers' ability to recognize the artwork's style
%     (Fisher's Exact Test, % for Count Data
%     $p=0.31$, % 0.3112
%     $OR = 1.19$, % 1.185023
%     95\% CI [0.86, 1.63]).% 0.863013, 1.628166


\hl{%
After this first pilot study, we were cautiously optimistic that many crowd workers would -- in theory -- have the knowledge to write effective prompts for generating digital artworks with text-to-image generation systems.
In the following studies, we explored whether crowd workers had an understanding of the mechanics of prompt engineering (Section \mbox{\ref{sec:study2}}), and explored whether workers could apply their knowledge in practice (sections \mbox{\ref{sec:study3}} -- \mbox{\ref{sec:study4}}).
}%



% ====================
\section{Study 1: Understanding Prompt Engineering}%
\label{sec:study2}%
% ====================
We conducted a \hl{within-subject} experiment to study participants' understanding of prompt engineering. Participants were asked to rate \hl{the textual prompts and the corresponding AI-generated images}. We hypothesize that participants with a strong understanding of prompt engineering would exhibit a high consistency between the ratings in the two modalities.
In other words, if someone can predict the aesthetic appeal of an image from its textual prompt, they likely have a good sense of how prompt engineering works.
The study design reflects the knowledge that prompt engineers would \hl{draw on} in practice: \hl{first write a prompt with the intention to produce a high quality image, then observe and assess the quality of the resulting image.} A good understanding of textual prompts is crucial for predicting how well a prompt will perform.
% For instance, \hl{prompts with} descriptive language and many \hl{prompt} modifiers are likely to produce higher quality artworks \hl{than prompts without these characteristics.}%
\hl{For instance, prompts incorporating rich descriptive language and multiple prompt modifiers tend to yield artworks of higher quality compared to prompts lacking these attributes.}
\footnote{Note that some state-of-the-art image generation systems, like Midjourney, are ``greedy'' and will try to turn any input into an aesthetic artwork, even if the prompt is short or non-descriptive. See Section \ref{sec:limitations} for more on this issue.}
%
In the following section, \hl{we describe the study design in detail.}

% --------------------
\subsection{Method}%
% --------------------

% --------------------
\subsubsection{Research materials}%
% --------------------
We curated a set of prompts and images created with Midjourney,\footnote{https://www.midjourney.com} a text-to-image generation system and community of AI art practitioners. Using purposeful sampling, we selected 111 images from the corpus of % over 3500
thousands of Midjourney images generated by the first author. Our choice to use the author's image corpus has several advantages. The corpus includes images with a range of different prompt modifiers commonly used on Midjourney and we avoid intruding on others' intellectual property rights. Further, the author has experience with text-to-image generation and can distinguish failed attempts from successful ones. This allowed us to purposefully sample images with varying levels of subjective quality. Specifically, we selected 59~images judged as failed attempts and 52 images of high aesthetic quality.
We kept the format of four images per prompt, as it resembles the output a prompt engineer would typically receive on Midjourney.

To assess the aesthetic quality of the 111 images in the dataset, we recruited ten volunteer raters from two academic institutions. The raters had diverse backgrounds in Computer Science, Information Sciences, Human-Computer Interaction, Cognitive Science, Electrical Engineering, and Design. They consisted of 2 Professors, 3 PostDocs, 3 PhD students, 1 Master student, and 1 project engineer (5 men and 5 women, age range 24–48 years). Raters completed a simple binary classification task to classify the images as high or low quality based on their aesthetic appeal. Raters were informed that there was an unequal number of images in each category.
% This was intentional to make participants consider each image individually.
The inter-rater agreement over all images, as measured by Fleiss' kappa, was fair, $\kappa = 0.34$, $z = 23.9$, $p<0.00$, 95\%~CI [0.31, 0.37].%
%
We discussed the ratings and selected images for further study. From the set of images with perfect agreement among \hl{the ten} raters, we selected ten high- and ten low-quality images.

\hl{We contend that high-quality images can be determined by aesthetic quality ratings from 10 volunteers, without disclosing the prompts to them, for the following reasons.
Classifying images with high and low aesthetics is a relatively easy task~\mbox{\cite{1606.01621.pdf}}.
The evaluation of `high-quality' and `low-quality' images by a limited number of volunteers %(10~in our study) 
is not aimed at establishing a universal standard of image quality.
We contend that understanding the subjective quality perception of AI-generated images is as important as their fidelity to the prompts.
While we recognize the importance of the alignment between the image and its corresponding prompt in determining quality, our primary focus in this study is on the aesthetic appeal as perceived by individuals without the context of the prompts.
Inspired by the ``wisdom of the crowd'' \mbox{\cite{surowiecki}}, our objective is to leverage perceptual differences in aesthetic appreciation among multiple individuals in order to devise two distinct sets of images, without the influence of the original prompts used to create the images.
Future studies could integrate prompt fidelity as an additional dimension of image quality.}

The final set contains 20~images and respective prompts of varying quality (see \autoref{fig:images} and \autoref{appendix:images}).

% ------
\input{TAB/TABLE-STUDY2-IMAGES}
% ------

% --------------------
\subsubsection{Study design}%
% --------------------
We conducted a within-subject experiment with two \hl{distinct} conditions.
\hl{The first condition required participants to rate} 20~AI-generated images on a 5-point Absolute Category Rating (ACR) scale \cite{06982326.pdf,06286980.pdf} (\hl{refer to} \autoref{fig:promptrating}a). The ACR is \hl{an established scale for producing reliable judgments \mbox{\cite{06982326.pdf}},  noted for its insensitivity to variables such as lighting, monitor calibration, language, and country} \cite{06286980.pdf}.
In the second condition,
\hl{participants were presented with 20~textual prompts and asked to imagine and rate the images they believed these prompts would generate, using the same ACR scale.
% participants were asked to imagine the images that would result from 20 textual prompts used to generate the images in the previous condition, and rate them using the same ACR scale.
% In this condition, participants were only shown the prompts, not the images.
Here, participants were shown only the prompts, not the actual images.
Each prompt was introduced with ``Imagine the image generated from the prompt: \ldots'' accompanied by a descriptive task reminder (see \mbox{\autoref{fig:promptrating}b}).}
% This part of the study was designed to make participants focus on the resulting images, rather than the prompts themselves. To help with this, each prompt was prefaced with ``Imagine the image generated from the prompt: \ldots'' and a descriptive line was added as a reminder of the task (see \autoref{fig:promptrating}b).
%
% -----
\input{FIG/FIG-TASK-SURVEY2}%
% -----

The instructions were carefully designed to avoid confounding factors. For instance, we used neutral wording and avoided referring to the images as artworks to prevent higher positive aesthetic ratings~\cite{10.1163@22134913-00002052.pdf,auteursversie_art_photo_final.pdf,gerger2014.pdf,fpsyg-08-01729.pdf}.
Note that our aim was not to measure exact ground-truth ratings for aesthetic appeal, but to study differences in ratings within participants \hl{in a within-subject design.}


After the two conditions, we collected basic demographics including participants' self-rated interest in art and experience with practicing art.
We also included an optional open-ended item for participants to elaborate on their experience with text-to-image generation. Experience with art and text-to-image generation were measured on 5-point Likert scales, and \hl{self-rated} experience with art was measured as a binary variable.


% --------------------
\subsubsection{Participant recruitment and procedure}%
% --------------------
We recruited US-based participants from Amazon Mechanical Turk (MTurk) with a task approval rate greater than 95\% and at least 1000 completed tasks. This combination of qualification criteria is common in crowdsourcing research (e.g., \citet{3491102.3517434}).
The experiment was implemented as a survey task and hosted on Google Forms.
Participants were paid US\$1.50 for completing the survey. The price was determined from the average completion times in a small-scale pilot study ($N=9$, US\$1 per task).%

The task consisted of 31 items in total, including a consent form, an introduction to the study, 20 ratings of prompts, 20 ratings of images, ten demographic items, and one consistency check.
Participants underwent the two conditions (rating of prompt and rating of images) in balanced order. Half of the participants first rated the prompts, then the images, and the other half vice versa.

To prevent bias, we anonymized the filenames of the images and assigned a random numeric code to each image to make it harder to associate the images with the prompts from the previous survey section. 
As a check for consistency, we duplicated one image and collected a rating for this image (L1, see Appendix~\ref{appendix:a2}). Participants who differed in their rating \hl{for this duplicated image} by greater than one category on the ordinal ACR scale were excluded from analysis.
We excluded four participants for failing this consistency check and another two participants for having completed the survey without completing the task on MTurk.
The final sample included 52 participants.


% --------------------
\subsubsection{Analysis}
% --------------------
% We compared the distributions of ratings using a Kruskal-Wallis rank sum test followed by posthoc Dunn tests where applicable.
% This analysis showed whether the type (Prompt or Visual Artwork) and quality (High or Low) had different ratings.
% We performed a correlation test using Pearson's product-moment correlation to look at the relationship between paired scores for each type.
We hypothesize that participants can detect a relationship between the quality of a prompt, in terms of its ability to depict visual art through human imagination, and the quality of the visual artwork generated by the text-to-image generation system\hl{. }
% Further, we hypothesize that art experience, as measured by self reports, will have an impact on the consistency of the participants' ratings.
To test this hypothesis, we 
\hl{performed a correlation test using Pearson's product-moment correlation to look at the relationship between paired scores for each type.}
We investigated the correlation between art experience and average error for each participant.
% We calculated the participants' art experience by summing the self-reported ``practice of art'' (1 to 5) and ``museum visitation'' (1 to 5), but not text-to-image generation experience (because only one participant had strong experience, which they obtained from taking an MTurk survey).
Average error per participant was calculated by taking the average absolute difference between each pair of prompt and artwork rating.
For example, if all prompts were rated as 2 and all artworks as 5, the average error would be 3.


% --------------------
\subsection{Results}
% --------------------
%
%
% --------------------
\subsubsection{Participants}%  Recruitment and Demographics
% --------------------
Participants ($N=52$) were between 24 and 67 years of age ($M=38.2$~years, $SD=12.98$~years) and included 31~men and 21~women (no non-binary) from diverse educational backgrounds (27~Bachelor's degrees, 10~Master's degrees, among others).
A sizable fraction of the participants (46\%) reported having an educational background in the arts. Twenty-nine participants agreed and nine strongly agreed that they had visited many museums and art galleries  ($M=3.60$, $SD=1.18$). However, participants did not practice art often ($M=3.08$, $SD=1.28$). 
Overall, participants were interested in AI generated art ($M=3.69$, $SD=0.83$), but had little experience with text-to-image generation ($M=2.58$, $SD=1.43$).
Only three participants mentioned having used text-to-image generation (DALL-E mini/Craiyon) before.


\subsubsection{Visual and prompt ratings}
Our study design asked participants to rate both Prompts and Visual artwork. As these crowd workers did not receive special training for prompt engineering or text-based AI art, our goal was to understand the quality of our participants' perceptions. We show the histogram of scores broken into groups for each Art Type (Prompt and Artwork) in \autoref{fig:study2:hist} and \autoref{table:study-2-means} and the Quality (high or low) as described previously. Visually, these show differences across groups, with the distributions of Artworks leaning towards higher quality than prompts.

We used a Kruskal-Wallis rank sum test \hl{on} these four unique groups, \hl{finding a significant difference between the study conditions} ($\chi^2=231.4$, $p<10^{15}$, $df=3$).
Following this significant result, we performed post-hoc Dunn's test pairwise across each group with Bonferroni correction for p-values.
Each of these pairs had significant results with a p-value of less than $10^{-4}$, except for Artwork--High versus Prompt--High, in which $p<.004$.
This implies that the median values among all comparisons of groups (i.e. Artwork--High, Artwork--Low, Prompt--High, Prompt--Low) are significantly different from each other.

Participants were able to differentiate images with low visual aesthetic quality from high quality images.
Artwork--High has a higher mean rating ($\mu=3.70$) compared to Prompt--Low ($\mu=3.39$).
Likewise, participants were able to distinguish between high and low quality  by imaging what would be produced based on textual prompts.
Prompt--High has a higher mean rating ($\mu=3.87$) compared to Prompt--Low ($\mu=2.78$).
The overall span between the Artwork High and Low is larger for Prompts ($3.87 - 2.78=1.09$) than for Artworks ($3.70 - 3.39=.31$).
Both High and Low quality Artworks had distributions that favored a rating of 4, while Prompt--Low has a relatively flat distribution across values of 1 to 4 (see \autoref{fig:study2:hist}).%
%
% A Kruskal-Wallis test showed a significant difference between the ratings for low and high quality images,
% $H(1)=278$, $p<0.00$ with a small effect size of 0.28, 95\% CI [0.24, 1.00].
% The difference was also significant for the imagined outcomes of textual prompts $H(1)=21.1$, $p<0.00$ (\autoref{fig:study2:boxplots}b), but with a negligible effect size of 0.02, 95\% CI [0.01, 1.00].
%
% We conducted a
% posthoc Dunn's test of multiple comparisons
% to examine differences between the high and low quality ratings from workers with and without self-declared experience in art.
%
%
%%%% omnibus for one way Kruskal-Wallis
% 	Kruskal-Wallis rank sum test
%
% data:  Rating by ATQ
% Kruskal-Wallis chi-squared = 231.42, df = 3, p-value < 2.2e-16
%
%
%%%%comparing each group.
%   Kruskal-Wallis rank sum test
%
% data: x and group
% Kruskal-Wallis chi-squared = 231.4204, df = 3, p-value = 0
%
%
%                           Comparison of x by group       %                    
%                                 (No adjustment)          %                      
% Col Mean-|
% Row Mean |   Artwork_   Artwork_   Prompt_H
% ---------+---------------------------------
% Artwork_ |   4.038908
%          |    0.0000*
%          |
% Prompt_H |  -2.666342  -6.705251
%          |    0.0038*    0.0000*
%          |
% Prompt_L |   11.58802   7.549111   14.25436
%          |    0.0000*    0.0000*    0.0000*
%
%
%A Kruskal-Wallis test showed a significant difference between the ratings for low and high quality images,
% $H(1)=278$, $p<0.00$ with a small effect size of 0.28, 95\% CI [0.24, 1.00].
%
% ---------
% \input{FIG/FIG-BOXPLOTS}
\input{FIG/FIG-STUDY2-HIST}%
% ---------
%
%
\input{TAB/TABLE-ARTTYPEQUALITY}%
%
%
\subsubsection{Connection between visual image and prompt quality}
While in theory, prompts that can help readers conjure (i.e. visualize or imagine) more aesthetically appealing mental images will also generate better Artwork, it is not clear whether this would be the case for crowd workers.
While our participants were not able to directly associate Prompts to Artworks, each Artwork had a matching Prompt.
We used a Person's product-moment correlation test to measure whether ratings for the Prompt and Artwork are correlated.
The test shows a weak ($r=.29$, 95\% CI [.23, .34]) but significant ($p<10^{-15}$) positive correlation between ratings from Artworks and Prompts.
This indicates that when a Prompt is seen as having a higher quality, that it is also more likely that the Artwork will appear as having a high quality.

% 	Pearson's product-moment correlation
% data:  rates[rates$ArtType == "Artwork", ]$Rating and rates[rates$ArtType == "Prompt", ]$Rating
% t = 9.8489, df = 1038, p-value < 2.2e-16
% alternative hypothesis: true correlation is not equal to 0
% 95 percent confidence interval:
%  0.2357419 0.3469640
% sample estimates:
%       cor 
% 0.2923412 

% \input{FIG/FIG-STUDY2-EXP}


% \subsubsection{Art familiarity and consistency}
% We hypothesize that when participants are more familiar with art, they will be more effective at rating art.
% One way to measure this is to take the average absolute difference between each Prompt and visual Artwork pair for each participant.
% Being able to visualize  and rate Prompts and Artworks with equal values indicates a skill for predicting the quality of a generated visual artwork.
% This number represents the error in consistency. For example, if each of the rating pairs (Prompt vs Artwork) had the same rating, this error would be equal to~0. If each were pair was different by 1 for each rating, this error would be equal to 1, because it is an average.
% We asked participants to self-report their experience with art by asking them to self-rate their familiarity and recent visits to art shows or museums.
% Thus, we take the sum of both questions (which each range from 1 to 5) and test for correlation to the participants' rating scores.
% The test shows a weak ($r=-.31$, 95\% CI [-.54, -.04]) but significant ($p=.02$) negative correlation between the average error in consistency and art experience.
% This indicates that with more reported art experience, the difference in ratings of Prompts and visual Artworks may decrease (see \autoref{fig:study2:error}).


% Pearson's product-moment correlation
% data:  extable$ABSRatingDiff and extable$ArtExperience
% t = -2.3113, df = 50, p-value = 0.02498
% alternative hypothesis: true correlation is not equal to 0
% 95 percent confidence interval:
%  -0.53798040 -0.04129583
% sample estimates:
%       cor 
% -0.3106947 

% ---------
% \input{FIG/FIG-CORRELATION}
% ---------


% ====================
\section{Study 2: Writing Prompts}%
\label{sec:study3}%
% ====================
Our aim with this study is to probe \hl{whether laypeople recruited from a crowdsourcing platform have the ability} to come up with effective input prompts for text-to-image generation systems with a specific focus on generating digital artworks.
The presence of style modifiers in an input prompt may indicate an understanding of text-to-image generation and how effective prompts can be formulated.

% --------------------
\subsection{Method}%
% --------------------

% --------------------
\subsubsection{Study design}%
% --------------------
We designed a creative crowdsourcing task eliciting three textual prompts from each participant.
The task included a short introduction and the following instructions:
\begin{quote}
    \textit{%(Instructions:)
    Imagine an artificial intelligence that turns textual input prompts into digital artworks.
    Your task is to produce three artworks.
    To this end, you will write three different input prompts for the artificial intelligence.
    You should aim to maximize the visual attractiveness and aesthetic qualities of the digital artworks generated from your input prompts.}
\end{quote}

\hl{Participants were asked to make their artworks as visually attractive and high-quality as possible.
We did not mention that prompt modifiers could be used in the prompt and wrote the instructions to avoid priming participants with a specific style (i.e., we told participants to produce `artworks' rather than `paintings').}
Note, however, that we did not aim to precisely measure attractiveness and quality, but wanted participants to think about the overall visual and aesthetic quality of the images. Participants were told that there was no right or wrong answer, but tasks would be rejected if they didn't follow the instructions.
\hl{To avoid influencing participants prompt modifications in our follow-up study (Study 3), we merely collected prompts from participants in this study --- the outcome of the image generation was not shown to participants in this study.}

As additional questions in the task, we asked whether the participant had experience with text-to-image generation and we collected basic demographics.
Participants were paid US\$0.16 per completed task. The pricing was estimated from the average task completion times in a pilot study ($N=10$, US\$0.12 \hl{per} task).
%
In this pilot study, we noticed some participants wrote a series of consecutive instructions for the AI. The task design and instructions were subsequently adjusted to elicit complete prompts.


% --------------------
\subsubsection{Participant recruitment}%
% --------------------
We recruited 137~unique participants from Amazon Mechanical Turk using the same qualification criteria as in Study~1.
Ten tasks had to be rejected due to clearly no attempt being made to answer the task with relevant information. The ten tasks were republished for other participants. After collecting the data, we manually reviewed the results and removed a further twelve responses from participants who obviously tried to game the task.
The final set includes 375~prompts written by 125~unique participants (three prompts per participant).

% --------------------
\subsubsection{Analysis}%
% --------------------
The analysis of the prompts was conducted with mixed methods.
For each prompt, we qualitatively and quantitatively analyzed the prompts, as follows.

\paragraph{Prompt modifiers}%
\hl{%
Our analysis focused on identifying the presence of specific keywords and phrases frequently used within the AI art community to influence the style and quality of AI-generated images \mbox{\cite{modifiers,aiartcreativity}}. We opted for manual analysis in this case. An initial screening indicated that a very small portion of the prompts utilized prompt modifiers, making automated methods unnecessary for this specific task.

The analysis process involved the first author of this paper who systematically reviewed each prompt, with focus on identifying specific language patterns and stylistic phrases known to impact the AI's generative capabilities. This included looking for explicit instructions or adjectives that might alter the style or quality of the generated images.
Given the clear and specific nature of these prompt modifiers, the coding focused on the presence or absence of these elements in each prompt.
We determined that the coding process was straightforward -- i.e., it
did not require complex categorization or subjective interpretation -- and, thus, did not necessitate validation via inter-rater agreement \mbox{\cite{McDonald_Reliability_CSCW19.pdf}}.
}%




We analyzed whether the prompts contained certain keywords and phrases commonly used in the AI art community to modify the style and quality of AI generated images \cite{modifiers,aiartcreativity}.
We decided on manual analysis because a preliminary screening revealed that very few prompts contained prompt modifiers.
Each prompt was analyzed by an author of this paper. We coded the presence of prompt modifiers and report on their nature and use.
We did not calculate inter-rater agreement because the coding was straight-forward~\mbox{\cite{McDonald_Reliability_CSCW19.pdf}}.%



% --------------------
\paragraph{Descriptive language}%
% --------------------
A prompt written in descriptive language is likely to generate images of high quality.
We quantitatively assessed whether the prompts contained descriptive language by calculating a number of statistical indices for each prompt:
\begin{itemize}
    \item The % characters,
    number of words (tokens) and unique words (types) in the prompt.
    % vocabulary size
        In general, longer prompts are more likely to include certain keywords
        (whether on purpose or by accident)
        that may trigger the image generation system to generate images with high quality or in a certain style.
    \item The Type-Token Ratio (TTR) \cite{TTR}, a standard measure for lexical diversity defined as the number of types divided by the number of tokens in the prompt.\footnote{We also experimented with other indices of lexical diversity, such as the Moving-Average Type-Token Ratio (MATTR) \cite{covington2010.pdf} and
    the Measure of Textual Lexical Diversity (MTLD) \cite{MTLD_MA_Wrap}.
    % Note that measures for lexical diversity have their limitations.
    However, these measures highly depend on the text length \cite{30200474.pdf}.
    Only a small fraction of the prompts in our sample meet the recommended minimum number of tokens for applying lexical diversity measures~\cite{1-s2.0-S1075293520300660-main.pdf}.
    The use of lexical diversity indices, such as the TTR, for comparing texts of different size is not recommended~\cite{30200474.pdf}.
    In our study, we do not use the TTR for comparing the lexical diversity of prompts, but to assess the amount of repetition of tokens in the prompt.}
    A token, in this case, is a discrete word whereas a type is a unique token in the prompt.
    For calculating the TTR, we used Kristopher Kyle's lexical-diversity Python package \cite{pythonpackage}.%
    % \item MATTR
    % \item MTLD\_MA\_Wrap, an adaption of the MTLD (measure of lexical textual diversity) \cite{MTLD_MA_Wrap}. The adapted MTLD uses a wrapping moving window approach that makes the measure better applicable to texts of short length \cite{1-s2.0-S1075293520300660-main.pdf}.
    % \item Shannon's entropy \cite{entropy} as a measure for the amount of information in the prompt.
\end{itemize}%
% as recommended in the guidelines for lexical diversity indices with short texts
    % MATTR 50 tokens
    % MTLD\_MA\_Wrap 100 tokens
    % These guidelines recommend  a minimum text length of 50 tokens for MATTR and 100 tokens for MTLD\_MA\_Wrap.
% Our dataset includes only two prompts of greater than 100 tokens, and seven prompts of greater or equal to 50 tokens.
% TTR and MTLD have been applied to short texts (e.g. for classifying Twitter posts \cite{information-13-00093.pdf})
% We plot the lexical diversity measures in this paper primarily to give a qualitative impression of the prompts, not to compare different prompts.
    % Repetitions are one category of prompt modifiers \cite{modifiers}.
% "it is extremely hazardous to use lexical ‘constants’ to compare texts of different length" \cite{30200474.pdf}

We further used tokenization and parts-of-speech tagging (with the Natural Language Toolkit \cite{P04-3031.pdf}) to calculate the amount of:
\begin{itemize}
    \item % absolute and relative number of 
        Nouns
        (\texttt{NN})
        % (including singular, plural, and proper nouns)
    and verbs (\texttt{VB}) as an indicator of the subjects in the prompt.
        % was included to analyze the topics of the prompts.
    \item % The % absolute and relative number of
        Adjectives
        (\texttt{JJ})
        as an indicator of descriptive
        % (or ``flowery'')
        % embellished
        language.
        % which may produce images of higher quality \cite{guidelines,aiartcreativity}.
    \item % The % absolute and relative number of 
        Prepositions
        (\texttt{IN})
    as an indicator of specific language.
    \item % The % absolute and relative number of 
        Cardinal numbers
        (\texttt{CD}) as information on the number of subjects.%
\end{itemize}%
Each prompt typically contains at least one noun as the main subject. Using descriptive and specific language is likely to improve the outcome of image generation. However, overusing prepositions could result in low fidelity of the image to the prompt. Cardinal numbers are important for prompt writing because a determinate number of subjects (e.g. \textit{``two horses''}) is likely to provide higher-quality images than an indeterminate number (e.g. \textit{``horses''}).

% Descriptive language was detected automatically with methods from natural language processing (NLP). More specifically, we used the spaCy library to apply parts-of-speech tagging and entity recognition.

% With exception of the wordclouds in \autoref{fig:study34:wordclouds}, we did not apply lemmatization and stopword removal (two common steps in natural language processing) in this analysis. The reason is that we are working with literal strings that are used as input for AI image generation systems. These systems are susceptible to tiny changes in their input \cite{x} and changing the prompt could result in vastly different images.


% --------------------
\subsection{Results}%
\label{sec:study2:results}
% --------------------
% In this section, we report on the workers and their use of prompt modifiers and descriptive language in the prompts.
% ---
\subsubsection{Participants}%
% ---
The 125 participants in our sample included 55 men, 67 women, 1 non-binary, and two participants who did not to disclose their gender identity.
The age of participants ranged from 19 to 71 years ($M=41.08$ years, $SD = 13.44$ years).
The majority of participants (98.40\%) reported English being their first language.
%
Thirty-seven participants (30.33\%) responded positively to the question that they had \textit{``experience with text-based image generation systems.''} We had no explanation for this surprisingly high number at this point, but inquired more about the participants' background in our follow-up study in Section~\ref{sec:study4}.
%
Median completion times were higher than estimated in the pilot study, reaching 197 seconds. It is possible that completion times are skewed due to participants reserving tasks in bulk.
% workers spent on average about  on completing the task ($400 seconds$, MIN=30$ s, $MAX=2868$ s, $Median=197$ s).

% -----
% \input{FIG/FIG-WORDCLOUDS}%
% -----

% ---
\subsubsection{On the use of descriptive language}
\label{sec:study3:descriptivelanguage}
% ---
The prompts were of varying length, ranging from 1 to 134 tokens with an average of 12.54~tokens per prompt ($SD=14.65$ tokens).
    % $M=70.77$ characters, $Median=50$ characters).
Overall, the length of prompts was appropriate for text-to-image generation with only four participants producing overly long prompts.
%
% types
% M=10.66
% SD=9.19
%
On average, participants used 3.27 nouns to describe the subjects in their prompt ($SD=3.36$).
%
Participants used verbs only sparingly in their prompts ($M=0.36$, $SD=1.02$).
%
The average number of prepositions ($M=1.78$, $SD=2.27$) was higher than the average number of adjectives ($M=1.65$, $SD=1.95$). However, this number is skewed by four participants who provided long prompts.
These participants were very specific in what their images should contain, with many prepositions being used to denote the relative positions of subjects in the artwork ($Max=21$ prepositions per prompt).

Overall, participants used rich descriptive language.
The participants were creative and often described beautiful natural scenery.
The main topics in the participants' prompts were landscapes, sunsets, and animals. % (see \autoref{fig:study34:wordclouds}).
We note that the richness of the language in the prompts primarily is a result of the use of adjectives.
% (see \autoref{fig:study34:wordclouds}b).
On average, participants used 1.65~adjectives in their prompt ($SD=1.95$).
Colors, in particular, were popular among participants to describe the subjects in their artworks.
The following prompts exemplify the creativity and the use of descriptive language among participants:
\begin{itemize}%
    \item
    % Alpine trees against a backdrop of snowy mountains and blue skies.
    \textit{%
    beautiful landscape with majestic mountains and a bright blue lake
    }
    % \vspace{.2\baselineskip}
    \item
    % A brilliant sunset with shades of orange and pink over the ocean
    \textit{%
    bright yellow sun against a blue sky with puffy clouds
    }
    \item
    \textit{%
    A fruit bowl with vibrant colored fruits in it and a contrasting
    background
    }
    \item
    \textit{%
    Dragon on the tower of a castle in a storm.
    }
    % \\
    \item
    \textit{%
    Knight holding a sword that shines in the sunlight
    }
    % \vspace{.2\baselineskip}
    % \\
    \item
    \textit{%
    A white fluffy puppy is playing in the grass with a large blue ball that is twice his size.
    }
    % \\
    % pretty woman wearing bright red dress and smiling
    \item
    \textit{%
    A shiny black horse with eyes like coal run in a lush green grassy field
    }
    \item
    \textit{%
    There should be a beautiful green forest, full of leaves, with dark brown earth beneath, and a girl in a dress sitting on the ground holding a book.%
    }%
\end{itemize}%

More than half of the prompts (58.13\%) did not repeat any tokens (that is, they had a TTR of 1; $M=0.94$, $SD=0.10$). Most of the repetitions in prompts stem from the participants' need to identify the relative positions of subjects in the image (e.g., \textit{``[...] Touching the black line and going all the way across the top of the black line should be a dark green line. Above the dark green line should be a medium green line. [...]''}).
Repetitions, as a stylistic element in prompts \cite{modifiers}, were not being used.
Only 27~prompts (7.2\% of all prompts) contained cardinal numbers ($M=0.07$, $SD=0.29$).
    % The cardinal numbers are depicted in \autoref{fig:study34:wordclouds}e.
    Two cardinal numbers referred to a period in time which could potentially trigger the image generation system to produce images in a certain style.

Even though we tried to mitigate it in the task design and the instructions, we noticed 18~participants (14.4\%) still provided direct instructions to the AI instead of prompts describing the image content.
    These participants either wrote three separate instructions to the AI (e.g., \textit{``Generate a white 250 ml tea glass [...],''} \textit{``Draw three separate triangles [...],''} and \textit{``Show me some digital artwork from a brand new artist.''}) or they wrote three consecutive instructions as we had observed in our pilot study. The latter may not include nouns as subject terms and could thus result in images with an undetermined subject (e.g., \textit{``sharpen image''}).
Two participants thought they could chat with the AI, asking it, for instance, \textit{``Which do you prefer: starry night sky or blue sea at dawn?,''} \textit{``Enter your favorite geometric shape,''} and \textit{``Can you paint me a rendition of the Monalisa?''}.%


% <TRANSITION?>
% Workers however exercised no control over the style of their artworks.


% ---
\subsubsection{On the use of prompt modifiers}%
\label{sec:study3:modifiers}%
% ---
Even though participants were specifically instructed to create a digital artwork, we found only very few participants included style information in their prompts.
Many participants described a scene in rich descriptive language, but neither mentioned artistic styles, artist names, genres, art media, nor specific artistic techniques.
The participants' prompts may have described an artwork, but without style information, the style of the generated image is left to chance and the resulting images may not match the participants' intent and expectations.

Overall, the prompts did not follow the prompt template mentioned in Section~\ref{sec:promptmodifiers} and best practices common in the AI art community were not followed.
Only one participant made purposeful use of a prompt modifier commonly used in the AI art community. This prompt modifier is \textit{``unreal engine.''}\footnote{The long-form of this modifier is \textit{``rendered in UnrealEngine,''} a computer graphics game engine. Images generated with this prompt modifier may exhibit increased quality due to photo-realistic rendering.}
The participant used this modifier in all her three prompts by concatenating it to the prompt with a plus sign, e.g.%
% \begin{quote}%
    \textit{``rainbow tyrannosaurus rex + unreal engine.''}
% \end{quote}%
A small minority of participants used generic keywords that could trigger a specific style in text-to-image \hl{generation} systems.
    For instance, the generic term \textit{``artwork''} was used in 16~prompts (4.3\%).
The following list of examples reflects almost the entire set of prompts containing explicit style information among the 375 prompts written by participants (with style modifiers underlined):%
\begin{itemize}%
\item
        \underline{Cubism portrait} of a Labrador Retriever using reds and oranges
        % \vspace{.2\baselineskip}
\item
        \underline{Paint} a \underline{portrait} of an old man in a park.
        % \vspace{.2\baselineskip}
\item
        \underline{Draw} a \underline{sketch} of an airplane.
        \vspace{.2\baselineskip}
\item
        \underline{Abstract} trippy colorful background
        \vspace{.2\baselineskip}
\item
        \underline{surreal} sky castle
    % Wildflowers growing along a mountain road. The road is gravel and inclined. Along each side of the road there are golden St. John's Wort, red Indian \hl{Paintbrush}, white Yarrow, and pink wild roses.
\item
        Can you \underline{paint} me a rendition of the \underline{Monalisa}?
        \vspace{.2\baselineskip}
\item
        \underline{Bob Ross}, \underline{Claude Monet}, \underline{Vincent Van Gogh}
        \vspace{.2\baselineskip}
\item
        Are you able to produce any of \underline{rodans work}.
        \vspace{.2\baselineskip}
\item
        what can you do, can you make \underline{pointillism artwork}?
        % \\
        % A black cat sits in the front window of a pale blue \hl{Victorian style} house
\end{itemize}%
%
Besides this sparse --- and sometimes accidental --- addition of style information, we find that overall, participants did not control the style of their creations. % Not including the above short list of prompt modifiers in prompts,
% As can be seen in the above list, the sparse set of prompt modifiers was sometimes only a by-product of the user trying to accomplish a task (e.g., ``draw a sketch''.
\hl{Instead of prompt modifiers, the participants' artwork} styles were mainly determined by the participants' use of descriptive language.
%
%
% ---------
% \input{TAB/TABLE-METRICS-STUDY3-4}
% ---------
%
% -----
% \input{TAB/TABLE-METRICS-STUDY3-4}
% \input{FIG/FIG-INDICES-GRAPHS}
% -----
%
% \subsubsection{Resulting images}
% We generated five images for each of the worker-provided prompts in preparation for our our final study. The image generation process is described in detail in \autoref{sec:study4-materials}. In this section, we qualitatively describe the resulting images.
% Because almost none of the workers provided specific style information, the set of generated images


% ====================
\section{Study 3: Improving Prompts}%
\label{sec:study4}%
% ====================
In a follow-up study, we investigated whether participants could improve their artworks.
% Our last study invited workers to review the images generated from their prompts.
This study aimed to answer the question of whether prompt engineering is an \hl{innate} skill that we humans apply intuitively or whether it is  \hl{an acquired} skill that \hl{requires expertise and practice (e.g., via learning to write prompts from repeated interactions with the text-to-image generation system)} and knowledge of certain keywords and key phrases (prompt modifiers), as discussed in Section \ref{sec:promptmodifiers} and Section \ref{sec:study3:modifiers}.
%
% The crowd workers were invited to review images generated from their previous prompts and asked to re-write the prompts to improve the results.
We hypothesize that if prompt engineering is a learned skill, participants will not be able to significantly improve their artworks after only one iteration.
% In this case, workers will also not be able to apply specific keywords for modifying prompts.%
%
%
% --------------------
\subsection{Method}%
% --------------------

% --------------------
\subsubsection{Study design}%
% --------------------
We invited the same participants who participated in Study~2 to review images generated from their own prompts.
    Participants were then asked to improve their three prompts.
%
To this end, we designed a task that introduced the participant to the study's purpose, using the same instructions as in the previous study.
We additionally highlighted that if the images presented to the participant did not look like artworks, the prompt should be adjusted. % (but without specifying how).
Like in the previous study, we avoided to mention that prompt modifiers could be used to achieve this aim.

Participants were given five images for each of the three prompts they wrote in Study 2. We used the workerId variable on MTurk to load the participant's previous prompts and images. Participants were then asked to rewrite and improve their three prompts. The task included two input fields, one pre-filled with their previous prompt and one for optional negative terms. In practice, negative terms \hl{are an important part of the toolbox of prompt engineers, primarily used for controlling the subject and quality of the image generation \mbox{\cite{modifiers}}.} For example, \hl{defining} \textit{``watermark''} and \textit{``shutterstock''} \hl{as negative terms} can reduce the occurrence of text and watermarks in the \hl{resulting image}.
\hl{Given the task of improving their previously generated artworks, we introduced negative terms as a possible tool for making improvements in Study 3.}
We studied this by incorporating it into our study design. Participants were introduced to the potentially surprising effects of negative terms with \hl{the help of} an example. The example explained that adding \textit{``zebra''} as a negative term to a prompt for a pedestrian crossing could potentially result in an image of a plain road (due to stripes being removed).

For each prompt, we also collected information on whether the images matched the participant's original expectations (given the previous prompt)
and whether the participant thought the prompt needed improvement (both on a Likert-scale from 1~--  Strongly Disagree to 5~-- Strongly Agree).
    The latter was added to identify cases in which participants thought that no further improvement of the prompt was necessary.
We also asked participants to rate their confidence that the new prompt would result in a better artwork  (on a Likert scale from 1~-- Not At All Confident to 5~-- Highly Confident).
%
The task concluded with demographic questions, including
the participant's experience with text-based image generation and interest in viewing and practicing art.
The task design was tested and improved in a small-scale pilot study ($N=8$; US\$1 per task).
    The payment was set to US\$1.75, aiming for an hourly pay of above minimum wage in the United States.

% --------------------
\subsubsection{Research materials}%
\label{sec:study4-materials}%
% --------------------
In this section, we describe how we selected an image generation system and how we generated images from the participants' prompts.%
% ------
\paragraph{System selection}%
% ------
We experimented with different text-to-image generation systems, including
    CLIP Guided Diffusion (512x512, Secondary Model)\footnote{https://colab.research.google.com/drive/1mpkrhOjoyzPeSWy2r7T8EYRaU7amYOOi},
    CLIP Guided Diffusion (HQ 512x512 Uncond)\footnote{https://colab.research.google.com/drive/1QBsaDAZv8np29FPbvjffbE1eytoJcsgA},
    DALLE-E mini\footnote{https://github.com/borisdayma/dalle-mini},
    Disco Diffusion 5.3 and 5.4\footnote{https://github.com/alembics/disco-diffusion},
    Latent Diffusion\footnote{https://github.com/CompVis/latent-diffusion},
    and Majesty Diffusion 1.3\footnote{https://github.com/multimodalart/majesty-diffusion}.
In the end, we selected Latent Diffusion for two main reasons. Latent Diffusion is the foundation for many of the community-driven adaptations and modifications.
More importantly, the system is deterministic and leads to reproducible outcomes. Consecutive runs with the same seed value will generate the same images. This is a crucial requirement since we aim to compare images in between studies.
%
%
\paragraph{Image generation}%
We generated images for the participants' prompts with Latent Diffusion using the following configuration settings:
    text2img-large model (1.4B parameters),
    % DiffusionWrapper has 872.30 M params
    seed value 1040790415,
    eta 1.0,
    ddim steps 100, and
    % n\_samples 1,
    % n\_iter 5,
    scale 5.0.
    % width 256, and
    % height 256.
%
Even though the system is capable of generating images at higher resolutions, we decided to generate images of $256\times256$ pixels to avoid the quirks that often occur when generating images in resolutions that the model was not trained on. 
%
The image generation job yielded 1875 images (125 participants $\times$ 3 prompts per participant $\times$ 5 images per prompt).
%
After collecting the revised prompts from participants, we generated another set of 1875 images using the same seed value and configuration settings as before. Negative terms were used in this second set, if provided by the participant.%

Some hand-selected images generated from the prompts are depicted in \autoref{fig:study34-examples}.
Many images were of photo-realistic quality, depicting landscapes, sunsets, beaches, and animals. Besides photographs, artistic styles included paintings, graphic designs, abstract artworks, as well as magazine and book covers.
Some images contained text and many images contained watermarks.%
% ----
\input{FIG/FIG-IMAGES-STUDY34}%
% ----


% --------------------
\subsubsection{Analysis}%
\label{sec:study4:analysis}%
% --------------------
We analyzed the two sets of prompts and images written in studies~2 and~3 as follows.

\paragraph{Analysis of prompts}
To measure the amount of changes in the prompts,
we calculated the number of tokens added and removed using parts-of-speech tagging as well as the Levenshtein distance~\cite{Levenshtein1966a.pdf}, a measure of lexical similarity denoting the minimum number of edits needed to change one string into another.
To understand the nature of the changes, the first author inductively developed a coding scheme \cite{hsieh2005three} with eight categories: adjectives/adverbs, subjects, prepositions, paraphrasing/synonyms, reordering, cardinal numbers, simplification, and presence of prompt modifiers.
After discussing the codes among all authors and revising the codes, the first author coded all prompts and generated a co-occurrence matrix of changes made by participants. Note that we understand ``subjects'' in the sense of subject terms~\cite{modifiers} for image generation (e.g. ``a woman holding a phone'' would have two subjects (woman and phone). Synonyms were analyzed at the level of individual words and parts of sentences.

\paragraph{Analysis of the revised images}%
\hl{%
We evaluated the images according to the following process, developed collaboratively by the authors.
Initially, a spreadsheet was created with the two sets of prompts and their respective five images from Studies~2 and~3. Through a detailed discussion of 30~image-text pairs, the authors developed a set of evaluation criteria grounded in both the study's objectives and relevant literature. This approach ensured a balance between the specificity of the study and established methodologies in image evaluation. The criteria were designed to encompass both objective and subjective aspects of the images, including binary categories for failed image generations, the extent of style and subject change, and improvements in consistency. Additionally, we included ratings for details, contrast, color, distortions, watermarks, and an overall subjective impression of quality.
We acknowledge that while some elements of the evaluation were inherently subjective, they were rooted
% in a thorough literature review and
in discussions among the authors to ensure they were relevant and appropriate for the context of this study. We aimed to create a comprehensive evaluation framework that not only aligns with existing standards but also caters to the unique aspects of AI-generated imagery.}%


% One author first created a spreadsheet with the two prompts and respective sets of five images from studies 2 and 3. The authors then discussed 30 of these image-text pairs and developed a set of evaluation criteria.
Using the evaluation criteria, each author \hl{then} individually rated 50 pairs of images along these criteria.
After \hl{this} initial round of coding, the authors discussed the results and decided to add four more criteria to the coding scheme. \hl{The final set of criteria included binary categories for failed generations, amount of style and subject change, and whether consistency improved, as well as ratings for details, contrast, color, distortions, watermarks, and overall subjective impression of quality.} After a second round of coding, the authors cross-checked their evaluations and resolved differences through discussion.


% --------------------
\subsection{Results}
% --------------------
% -----
\subsubsection{Participants}
% -----
The sample consisted of 50~crowd workers (40\% of the participants who participated in Study~2).
Participants included 25~men, 24~women, and 1~person who preferred not to disclose the gender identity, aged 20 to 71 years ($M=42.76$, $SD=14.63$).
Participants came from varied educational backgrounds, including some completed college courses (17 participants), Bachelor's degrees (22 participants), Master's degrees (4 participants), and doctorate degrees (2 participants).
%
% ----
\input{FIG/FIG-STUDY34-LIKERTS}%
% ----
%
Seven out of ten participants had an educational background in the arts. Some participants were interested in visiting museums and AI-generated imagery, but most did not practice art themselves and 80\% had little or no experience with text-to-image generation.

Approximately 40\% of participants were disappointed with the generated images, while 55\% of participants' expectations were met. Around 60\% of participants believed the images needed improvement, and a similar percentage of participants were confident that their revised prompts would improve the generated images.


% -----
\subsubsection{Participants' revised prompts}%
% -----
The average Levenshtein distance between the participants' two prompts (not including negative terms) was 28.1 ($SD=25.0$).
    % $Q0=0$, $Q25=10.2$, $Q50=22$, $Q75=36.8$, $Q100=113$
A computational analysis of the changes with parts-of-speech tagging shows that participants added over twice as many tokens as they removed~--- 538 added tokens versus 243 removed tokens (see \autoref{fig:study4:changes:histograms}).
Nouns were added most often (29.55\% of added tokens), followed by adjectives (22.12\%), prepositions (17.84\%) and determiners (8.55\%).
% added-NN	159	29.55\%
% added-JJ	119	22.12\%
% added-IN	96	17.84\%
% added-DT	46	8.55\%
% removed-NN	70	28.81\%
% removed-IN	41	16.87\%
% removed-JJ	32	13.17\%
% removed-DT	21	8.64\%
% kept-NN	449	29.50\%
% kept-DT	316	20.76\%
% kept-IN	258	16.95\%
% kept-JJ	218	14.32\%
The same types of tokens were also most often removed (28.81\% of removed tokens were nouns, 16.87\% prepositions, 13.17\% adjectives, and 8.64\% determiners).
%
In 11~prompts (7.33\%), the participant neither changed the prompt nor provided a negative term.
% 23 workers made no changes at all to their prompts, but 12 of these workers added a negative term.
    Six of these instances consisted of participants pasting random snippets of text.
    % (e.g., ``Less than a decade after breaking the Nazi encryption machine Enigma [...]'').
    % Some of these instances were relevant to text-to-image generation (e.g., ``A new AI program is wowing the public with its ability to draw realistic and creative pictures [...]'').

% ----
\input{TAB/TABLE-IMAGEEVALUATION}%
% ----

% ----
% \afterpage{%
\input{FIG/FIG-IMAGEEXAMPLES}%
% }%
% ----

% -----
% \input{TAB/TABLE-METRICS-STUDY3-4}
% \input{FIG/FIG-INDICES-GRAPHS}
\input{FIG/FIG-STUDY34-CHANGES}%
% -----

Our coding showed that the main strategy used by participants was modifying (i.e., adding, removing, or switching) adjectives in their prompts (see \autoref{fig:study4:changes:cooccurence}).
For example, a participant changed the prompt \textit{``flowers in winter''} to \textit{``purple flowers in winter.''}
This was often combined with changes to the subject of the prompt (cf. \autoref{fig:study4:images}), such as changing \textit{``sweeping arcs''} to \textit{``deep and broad, sweeping arcs in landscapes.''}
Some participants also adapted their prompts based on what they saw in the images, though this often resulted in only minor changes to the revised images.
    For instance, in the case of the above participant, the two images of mountainous landscapes were almost identical.
Another common approach was changing prepositions in the prompts. Few participants attempted to simplify their prompts, and relatively few made changes to cardinal numbers. For instance, one participant changed \textit{``draw a bunch of circles''} to \textit{``draw at least 15 circles,''} and another participant wanted to see \textit{``lots of puffy clouds''} without specifying the exact number.

We found that only one participant (the same as in Section \ref{sec:study3:modifiers}) demonstrated knowledge of prompt modifiers in all three of her prompts. An example written by this participants is
    \textit{``rainbow tyrannosaurus rex, \ul{prehistoric landscape}, \ul{studio ghibli}, \ul{trending on artstation}}.''
This participant used the underlined prompt modifiers which are commonly used in the AI art community.
Only one other participant used a style modifier (\textit{``real photos of [...]''}) in one prompt.
This shows a very small increase in the use of prompt modifiers among participants in between Study 2 and Study 3, even though participants were specifically instructed to improve their artworks.


% -----
\subsubsection{Participants' revised images}%
% -----
We compared the two sets of images generated from each participant's prompts and found that over half of the revised sets showed no improvement in image quality (in terms of details, contrast, color, distortions, watermarks, and consistency).
Selected changes in the prompts and the resulting images are depicted in \autoref{fig:study4:images}.
About half of the sets remained the same, 15\% were worse, and a third were better compared to the previous set.

Some participants were able to make improvements to the generated images, mainly by adding more details. Since participants added more tokens than they removed, the prompts were longer and resulted in about a third of the images having more details. Some participants also improved the images' colors and contrast by adding adjectives to the prompts. For instance, one participant improved the amount of details by adding \textit{``coral reef''} to the end of the prompt \textit{``scuba diver exploring unknown ocean.''} This change resulted in less blur and more details in the coral reef. However, strong changes in the style of the images were rare, with about 70\% of the revised sets being in the same or very similar style. Because participants did not use style modifiers, the revised images often resembled the initial images.

About 15\% of the images were of low aesthetic quality, often consisting of text with no discernible subject (see \autoref{fig:study34-examples:b}).
These images were rarely improved between the studies, and when they were, it was often due to chance.
For instance, the subject was completely changed in about 10\% of the images. This was often a result of participants trying to have a conversation with the AI and entering a completely different prompt as input  (see \hl{\mbox{\autoref{fig:study4:examples:b}} and \mbox{\autoref{fig:study4:examples:e}}}).


% -----
\subsubsection{Participants' use of negative terms}%
% -----
Nineteen participants used negative terms, with eight using them in all three prompts. In total, we collected 39~negative terms.
%
Many negative terms ($n=19$) aimed at removing or modifying the subject in various ways, such as removing \textit{``rocks''} from a beach, trying to correct a \textit{``weird face,''}  avoiding a \textit{``Nude, Naked, White, Man,''} removing the color \textit{``Green.''} in the image of a red star, or attempting to change the subject entirely (\textit{``ballroom''}).
Participants tried to change the style of the images in eight cases, using terms such as \textit{``Black, White, Colorless, Monochromatic,''} \textit{``opaque, solid,''} and \textit{``unfocused.''}
Four out of the 50~participants tried to remove text in the images, using negative terms such as \textit{``letters,''} \textit{``captions,''} and \textit{``text.''}
Only one participant attempted to remove watermarks, using the negative term \textit{``remove watermark.''}
As can be seen in this prompt, some participants did not understand the concept of negative terms, even though we explained it to them.
A few examples of failed and successful attempts are depicted in \autoref{fig:study4:negativeterms}.
Some of the image generations failed, because the participant did not use the negative term correctly. For instance, the prompt on the bottom right of \autoref{fig:study4:negativeterms} contains a monarch butterfly both in the prompt and negative term. The resulting image is sub-par compared to the image generated from the participant's original prompt.%
% ----
\input{FIG/FIG-NEGATIVETERMS}%
% ----



% ====================
\section{Discussion}%
\label{sec:discussion}%
% ====================

In \hl{four} studies, we explored the skill of prompt engineering with \hl{355} participants recruited from a crowdsourcing platform.
\hl{Our pilot study found that participants recruited from a crowdsourcing platform had knowledge of art that could enable them to write effective AI art prompts in practice.}
Our subsequent study shed light on whether people have an understanding of what makes a ``good'' prompt.
Interestingly, participants were better able to identify the quality of prompts than the quality of images.
This is surprising, given the difficulty of the task of imagining the visual outcome of a prompt.
The presence of style modifiers may have influenced participants to rate these prompts higher than prompts without style modifiers.
%
Our findings nevertheless indicate that participants can assess the quality of prompts and respective images. This ability increased with the participants' experience and interest in art.

In the subsequent two studies on writing and improving prompts, \hl{we found that participants wrote creative prompts in rich descriptive language} which may result in beautiful digital artworks.
However, only a negligible amount of \hl{participants applied their knowledge of art in practice and failed to use terms commonly applied in communities of text-to-image art,} such as Midjourney and Stable Diffusion. With the exception of one participant, these specific keywords were not (yet) part of the vocabulary of crowd workers on MTurk \hl{at the time of our study}. While the prompts written by participants were very descriptive and, in some cases, resulted in beautiful and interesting images, participants left the style of their image to chance. The prompts were missing modifiers that would more tightly control the style and quality of the image generation.
This applies to both the initial study on writing prompts and the study on revising and improving prompts.
In the latter, only a minority of the participants were able to improve their revised images, while most of the images remained about the same quality.
An overwhelming majority of participants left the style of the generated images to chance, even though they were instructed to create ``artworks.''

\hl{Reflecting on these findings, it becomes evident that while crowd engagement with AI-driven art generation shows promise, there is a noticeable gap in the effective use of prompt engineering. This leads us to consider the broader implications and questions within the AI and Human-AI Interaction research fields. Particularly, it raises the question of whether prompt engineering is an intuitive skill that can be easily acquired or if it requires more specialized training and understanding.}%
%
%
% --------------------
\subsection{Prompt Engineering as a Non-Intuitive Skill}%
\label{sec:learnedskill}%
% --------------------
Our work adds to the discussion of broader research questions in the AI and Human-AI Interaction research communities:
    Can anyone become an artist with prompt engineering?
    Is prompt engineering a skill that is \hl{innate} to us humans or is it \hl{a skill that needs to be acquired through practice and learning?}
    If prompt engineering is an intuitive skill, how intuitive is it? \hl{Or in other words, how steep is the learning curve to prompt engineering?}
%
These research questions have implications for the future of work and human-computer co-creativity~\cite{062-iccc20.pdf}.
If prompt engineering is an intuitive human skill \hl{that we humans can apply effortlessly}, then we can look forward to a bright future where \hl{anyone} can work in creative professions without having to develop special creative skills.
But if prompt engineering is a \hl{non-intuitive} skill, its application could become limited to highly trained and skilled class of creative professionals who have mastered to speak the language of the generative model \hl{through extensive training.}
The latter case could clearly negatively impact creative production and stifle innovation.


Prompting is a language-based practice and the use of language is intuitive to us humans. Therefore, one could assume that prompting is an intuitive skill.
It is easy to get started with writing prompts and prompting has a large potential in different fields and for many application domains.
%
However, our study found that effective prompt writing requires knowledge of keywords and key phrases. These prompt modifiers are an essential part of the skill of prompt engineering for AI generated art.
Typically, these keywords and key phrases are acquired through iterative experimentation and by learning from prompts shared in dedicated resources, on social media, or in online communities~\cite{aiartcreativity}.
Our studies empirically confirm that style modifiers are unknown to participants recruited on Amazon Mechanical Turk. Prompt modifiers that are being used profusely in the AI art community have not found their way into the collective vocabulary of workers on MTurk.
Participants in our study struggled to write and improve their prompts for the specific task of creating digital artworks.
This points towards prompt engineering being a \hl{non-intuitive} skill or perhaps even a specialist skill, as we discuss in the following section.%
%
%
% --------------------
\subsection{On the Future of % Crowdsourced
Creative Production with Prompt Engineering}%
\label{sec:creativeeconomy}%
% --------------------
%
Text-to-image generation opens new opportunities for creative production of digital images and artworks.
Whether prompt engineering will become an expert skill or even a novel profession is still open.
%
In this section, we speculate on four possible futures of prompt engineering.%
%
% -----
\subsubsection{Prompt engineering as an expert skill}%
% -----
In the future, prompt engineering could become an expert skill that requires deep subject-matter expertise (e.g., knowledge of \hl{subject-specific keywords, prompt modifiers, and their combinations,} but also of the idiosyncrasies of the training data and system configuration \hl{parameters}) to effectively control the output of generative systems. This is similar to the move in the field of machine learning towards ``foundation models'' \cite{FoundationModels}.
Foundation models are very large and costly to train, operate, and maintain. As a result, \hl{the creation of these models as well as} research on these models is limited to a small number of well-financed research institutes that employ highly-skilled professionals \hl{working in well-funded research institutes and organizations}. If prompt engineering becomes a highly skilled profession, it may become exclusive to a narrow group of privileged individuals who have undergone extensive training.%
%
% -----
\subsubsection{Prompt engineering as an everyday skill}%
% -----
In the future, prompt engineering could become a common practice.
In this scenario, people would adapt their creative practices and language to facilitate effective interaction with AI because it is a skill that is needed in everyday life.
People have a need for visual content, and AI-generated content could satisfy this need, from internet memes to the design of greeting cards, logos, and artworks. Prompt engineering could also be used for self-actualization, creativity, and therapy to improve mental health and well-being ~\cite{BURLESON2005436}.
In this scenario, people would expand their \hl{existing} vocabulary to include terms used in prompt engineering in order to produce meaningful outcomes with generative systems. Learning prompt engineering would be similar to learning a new language. \hl{This skill would be acquired at an early age, just like internet literacy is today acquired effortlessly by the generation born after the internet found widespread use.
The skill could also} become part of media literacy education in schools \hl{to elevate the common skill-level to a professional level}.

% -----
\subsubsection{Prompt engineering as an obsolete skill}%
% -----
In the future, prompt engineering could become irrelevant.
% Karpathy called prompt engineering ``Software 3.0''.
% Sam Altman called prompt engineering a bug than a feature that relies on % arcane non-intuitive conjurations of prompt modifiers.
\hl{How users interact with AI models is closely coupled with the advances of AI technology and what the AI models are capable of.}
Prompt engineering can be seen as the smell of a half-baked product that does not solve its users' needs.
As generative systems improve their ability to understand the intent of users, prompt engineering could be a short-lived trend.
The problem of aligning AI with human intent is known as AI~alignment in the scholarly literature \cite{Gabriel2020_Article_ArtificialIntelligenceValuesAn.pdf}.
% is already being addressed by state-of-the-art image generation systems like Google's Imagen~\cite{2205.11487.pdf} and OpenAI's DALL-E 2 \cite{DALLE2}.
State-of-the-art systems, such as ChatGPT~\cite{chatgpt} and % DALL-E 2 \cite{DALLE2} and
\hl{DALL-E 3 \mbox{\cite{dalle-3}}}, demonstrate impressive performance in understanding textual input prompts and aligning with user intent. With these systems, users of all skill levels can generate content from textual prompts.
\hl{As generative systems become better at understanding user intent, prompt engineering could become obsolete, similar to how we no longer use block printing.}
Prompt engineering could become unnecessary --- an archaic skill that does not require expert training and that only few people exercise for nostalgic reasons.
In this scenario, the machine would adapt to humans, rather than the other way around.
The generative machine would develop an ``intimate'' relationship \cite{p75-weiser.pdf} with its users and a perfect understanding of user intent.

\hl{Future work could envision how the machine can adapt to humans and possibly provide guidelines for AI researchers, so they can develop AI models that understand user intent and foster a more ``intimate'' relationship with users.}




% -----
\subsubsection{Prompt engineering as personal signature or curation skill}%
% -----
In the future, our human senses could become better at distinguishing hand-crafted art from AI-generated digital art.
AI artist Mario Klingemann speculated that with the influx of AI-generated images, this skill would help us notice subtle nuances, details, and imperfections in AI-generated art, which could become more important in determining the aesthetic quality of an art piece.\footnote{https://twitter.com/quasimondo/status/1512769106717593610}
In this scenario, anyone could write prompts for generative systems with good results, but only a few would become masters of prompt engineering. The practice of prompt engineering would remain a necessary skill for applying finishing touches and optimizing generative results, as well as imbuing an artwork with a personal style to distinguish it from bland ``off-the-shelf'' generations. Alternatively \hl{or in parallel}, prompt engineering could evolve into a curation skill~--- a personal practice in which everyone has their own curated sets of textual and visual \hl{inputs} used to fine-tune generative models for different purposes.
Current \hl{approaches} that cater towards this future are
\hl{Low-Rank Adaptation (LoRA) \mbox{\cite{hu2022lora}},
Dreambooth \mbox{\cite{dreambooth}} and textual inversion \mbox{\cite{textualinversion}}}.

\hl{The rapidly evolving landscape of prompt engineering, characterized by these innovations, underscores the dynamic nature of human-AI interaction in generative model applications. As we navigate these developments, it becomes increasingly vital to understand how diverse groups, including paid crowd workers, engage with and contribute to generative systems. This understanding not only enriches our perspective on prompt engineering but also informs how we can optimize collaborative and co-creative efforts in this domain. With this context in mind, we now turn our attention to practical recommendations for involving paid crowd workers in research on text-to-image generation systems.
}
% In the following, we formulate recommendations for involving paid crowd workers in research on text-to-image generation systems.

% --------------------
% \subsection{Implications for the Design of Text-to-image Systems}%
\subsection{Recommendations for Conducting Text-to-Image Experiments with Crowd Workers}%
\label{sec:implications}%
% --------------------
In this section, we reflect on our experiences with conducting experiments on prompt engineering with a paid crowd on Amazon Mechanical Turk.
Our research has found that crowd workers are capable of coming up with creative prompts written in descriptive language. However, it is important for workers to receive training in prompt engineering in order to gain a better understanding of how to use prompt modifiers. We have put together nine recommendations for conducting experiments on text-to-imagine generation with a paid online crowd.
% These recommendations can help ensure that an experiment is successful and produces the desired results.

\subsubsection{Recruit the right crowd}%
Some workers on crowdsourcing platforms enjoy and even seek out creative crowdsourcing tasks \cite{CHI20,CHI2020workshop}.
Workers who intrinsically enjoy writing stories or other types of creative crowdsourcing tasks are likely to produce better results with text-to-image generation systems. We see that in some of the workers in our sample who invested effort into writing detailed and descriptive prompts, while others consistently produced images of low aesthetic quality.
It is therefore important to recruit the right crowd, for instance by using qualification tasks.

\subsubsection{Provide guidance and clear task instructions}%
Crowd workers in our studies left the style of the generated images to chance, even though they were instructed to create ``artworks.''
Therefore,
% workers will need to be carefully guided in the task instructions.
task instructions should include clear and detailed explanations of what is expected from the worker, and how the worker can achieve these goals.
This includes explaining what prompts are, how they should be phrased, and how to correctly include prompt modifiers in prompts.
% Whether workers can come up with their own modifiers is still open to future work. Our study found that workers do not seem to apply them on their own.

\subsubsection{Explain keywords and key phrases to workers}%
Only a few workers in our studies intentionally used prompt modifiers. Keywords and key phrases used in the AI art community were virtually unknown to workers (with one exception).
While we expect this situation to improve as the popularity of text-to-image generation systems increases, workers will need to receive explanations and examples of prompt modifiers for workers to be able to use them.
Task instructions should be illustrated with exemplars of prompts and resulting images to educate workers before they write their first prompt.

\subsubsection{Be mindful of lengthy task instructions}%
While detailed instructions are needed, long instructions may slow down workers. Some workers may even ignore long instructions.
An alternative to lengthy instructions is to design instructions as a playful tutorial to train workers.
Another approach would be to use the idle time between image generations for providing useful information to workers.
    OpenAI's DALL-E interface, for instance, provides helpful tips while one waits for the image generation to finish. This principle could be applied to crowdsourcing tasks as well.


\subsubsection{Explain negative prompts}%
Some workers in our sample did not understand the concept of negative terms, even though we explained it to them and provided an illustrative example.
Crowdsourcing tasks should include a carefully designed explanation of how negative terms work, ideally with example images that demonstrate the effect of negative terms.

\subsubsection{Provide dedicated user interfaces}%
Instead of training workers in the use of prompt modifiers, it may be more effective to provide them with user interfaces that hide the complexity of open-domain prompting. One example of this is MindsEye\footnote{https://multimodal.art/mindseye}, which is built on top of Colab notebooks and is designed with usability in mind. With this interface, even novice users can easily change image styles by simply clicking buttons, without needing to understand the underlying mechanics of prompt modifiers. This can help make the process of generating images from text more accessible and user-friendly.

\subsubsection{Provide means for fast and iterative image generation}%
In order to effectively facilitate the process of text-to-image generation, it is important to carefully design the workflow for the workers. One way to do this is by using techniques like remixing and forking, which can help simplify the process of prompt engineering for the workers. For instance, the AI art tool Artbreeder\footnote{https://www.artbreeder.com} makes it easy for users to adjust image generation settings and to create variations and fork other users' images. This can help streamline the process and make it more efficient for workers.

\subsubsection{Provide visual feedback to workers}%
Workers in our studies wrote the first set of prompts without \hl{seeing the resulting images.} Immediate feedback would likely improve the workers' performance in subsequent image generations.
% The quicker feedback can be provided, the better for the worker.

\subsubsection{Allow a training period in which workers can experiment with the text-to-image generation system}%
When workers are assigned batches of microtasks, they may need some time to understand what they need to do. This can lead to a ``warming up'' period and a training effect, where the worker's initial attempts at generating images are not as good as their later attempts.
To account for this, the design of the crowdsourcing campaign should include measures to exclude the workers' first attempts from further analysis. This can help ensure that the results of the experiment are not biased by the workers' learning process.


% --------------------
\subsection{Limitations and Future Work}%
\label{sec:limitations}%
% --------------------
%
We acknowledge a number of risks to the validity of our exploratory studies.
% \subsubsubsection{Threats to the validity}
% \subsubsection{Study 1}%
% Crowd workers have been shown to have developed specific expertise in correctly answering tasks~\cite{Hauser-Schwarz2016_Article_AttentiveTurkersMTurkParticipa.pdf}. It is possible that workers in our pilot study may have used a reverse image search to complete the task despite our instructions not to use a search engine.
% One reason for this potential behavior are the power dynamics prevalent on MTurk. Workers are under pressure to perform well and need to avoid punitive measures from requesters to not curb future work opportunities.
% Further, workers may have understood our task as a pre-qualification for future tasks. Performing well on this task may, in the mind of the worker, lead to invitations in future art-related studies.
% Prior work has shown that there is a subset of workers who enjoy and seek out creative tasks \cite{CHI20} and our results may, therefore, potentially be biased towards workers who self-select to complete such tasks.
% As is often the case in crowdsourcing research, our results may be biased towards those workers who seek out survey tasks and creative tasks.
% Due to the self-selective nature of crowdsourcing platforms, we do not claim that our findings generalize to all crowd workers on Amazon Mechanical Turk.

% \hl{Aesthetic quality assessment is a highly subjective task \mbox{\cite{joshi_1.pdf}}.
% Many factors can affect ratings of aesthetic quality, such as personal values, personal background, the interestingness and content of the image, contrast, proportion, number of elements, novelty, and appropriateness~\mbox{\cite{joshi_1.pdf,1606.01621.pdf,DS77_158.pdf}}.
% We acknowledge that the task given to workers was hard, especially when it comes to imagining the visual outcome of a written prompt in Study 2.
% We did, however, find that workers were able to tell the difference between low quality and high quality prompts.}%

\subsubsection{Limitations}

\hl{%
Aesthetic quality assessment, as explored in Study 1, inherently carries a high degree of subjectivity \mbox{\cite{joshi_1.pdf}}. Various factors influence such ratings, including personal values, background, image content and its interestingness, contrast, proportion, number of elements, novelty, and appropriateness \mbox{\cite{joshi_1.pdf,1606.01621.pdf,DS77_158.pdf}}. We acknowledge that the task we set for workers was challenging, particularly in terms of visualizing and evaluating the potential outcome of a written prompt. While our findings indicate that workers could discern between low and high-quality prompts, we must also recognize a key limitation: the absence of direct measurements for the actual quality of the generated artworks themselves. This gap points to the difficulty in quantitatively assessing artistic creations, a challenge further compounded by the subjective nature of aesthetic evaluation. Future research could benefit from developing more concrete and objective criteria or methods to assess the quality of prompt--artwork pairs, providing a more comprehensive understanding of the relationship between prompt quality and the resulting art.
}%

% Selecting a sample of prompts from the set of Midjourney images for Study~2 --- and especially failed image generation attempts --- was a challenge.
% State-of-the-art image generation systems are ``greedy'' --- they will try to turn any given textual input into aesthetically pleasing images~\cite{aiartcreativity}. Midjourney, in particular, has been created to make it easy for novices to generate stunning images, even from short words and emojis.
% % Some of the prompts included in Study~2 produced interesting results without the (excessive) use of prompt modifiers.
% % We acknowledge that our selection of prompts and images for this study may not be ideal. The first author is not an artist, and there are members on Midjourney who are far better at prompt engineering than the author.
% We acknowledge that selection of stimuli affects the results obtained.
%     % (also exacerbated by the chat-based linear content presentation in the Midjourney community).
% % While the first author is by no means an artist and there are members on the Midjourney community who are far better prompt engineers, 
% % We do not claim our images are representative of the prompts written on Midjourney. % , nor do we claim that our results transfer to images created with other systems.
% % We do not claim that the selected images are representative of the wide range of textual inputs that can be formulated in natural language.
% % However, we evaluated the images with 10 raters and our choice of prompts and images was therefore independently validated.
% However, we believe the prompts and images demonstrate a range of aesthetic quality that is measurable and we confirmed this with ten independent raters who helped us select images for our study.%


% \subsubsection{Studies 3 and 4}%
% Several weeks passed between Study 3 and 4 and we cannot control whether workers became interested in text-based image generation in between the studies.
% % Another factor affecting this is the growing popularity of image generation systems with recent months seeing the release of openly available advanced systems (e.g., DALLE-E mini).
% The growing popularity of openly available image generation systems, such as DALL-E mini, could have led the worker to try out image generation systems and even develop expertise in writing prompts in between our studies.
% However, our data indicates that this was not the case.
% %  learning or habituation from repeated exposure
We further acknowledge limitations in our choice of text-to-image generation system.
Our %overriding
main motivation for selecting Latent Diffusion was to select a deterministic system that produces reproducible results. This allowed us to assess the effect of changes in the prompt on the images.
Note, however, that while Latent Diffusion is a powerful image generation system, it may respond differently to style keywords than CLIP-guided systems.
% suffers from an issue that is present in many of the current image generation systems. The system produces images that contain text and watermarks. This may have biased our results.
% we see no signs of that in our data. Only few workers attempted to remove the text and watermarks.
% We acknowledge that especially the CLIP-guided systems may have been a better playground for workers to try out their skills of prompt engineering. However, while there are systems that are less affected by the duplication issue (e.g., DALL-E mini),
However, only one participant used specific keywords (prompt modifiers) in our study. The second round of images was also never shown to participants. Therefore, we can safely assert that the choice of system had no effect on \hl{how participants adapted to the generative system and how they} wrote and revised their prompts.%
%
% Further, the field of text-to-image generation is evolving extremely quickly.

\hl{Regarding the observation about participants making greater improvements in prompts but not in modifiers in Study~3, we realize that our initial instructions did not explicitly guide participants to modify the stylistic elements or modifiers of the prompts. This oversight might have led to less emphasis on altering these aspects, thus skewing the results in favor of more noticeable changes in the prompts' substantive content.
However, the overall tasks given to the participant was to improve the artwork, and the instructions were thus implicitly part of the given task.
As an alternative approach, a more directed instruction could have provided insights into how novice users interact with and perceive the importance of prompt modifiers in text-to-image generation. This realization could be a valuable consideration for the design of future studies.}


\hl{Last, we acknowledge that to explore the dynamics of prompting skill learning, a long-term field study would need to be conducted. However, our one time modification experiment provides a first indication indicating that the skill of prompting is not an innate skill that users can apply without learning about it first.
Recent work by \mbox{\citeauthor{donyehiya2023human}} provides intriguing insights on the dynamics of prompt learning on Midjourney that future work could build on \mbox{\cite{donyehiya2023human}}.%
}%
%
%
\subsubsection{Future work}%

%\input{FUTUREWORK}

% --------------------
% \subsubsection{Directions for Future Work}%
% \label{sec:futurework}%
% --------------------

% There are still many directions for future work.
\hl{In our paper, we investigated text-based prompt engineering, but there are more facets to text-to-image generation in practice. Future work could, for instance, investigate not just prompting, but the understanding of system configuration parameters and visual inputs.}

\hl{Future work could further investigate specific ways for crowd workers to contribute to prompt engineering, besides writing prompts.
For instance, the crowd's ability to use inpainting and outpainting (that is, completing parts of an image with the text-to-image generation system) could be studied.
% Research into aesthetic judgments
% enabling computational aesthetics and neural image assessment
Further, crowd workers could be recruited to rate the aesthetic quality of AI generated images.
% Our study showed that workers' ratings discriminated between high and low quality images and prompts.
Our study showed that workers' ratings discriminated between high and low quality AI generated images. Workers were also able to distinguish between the quality of  imagined mental images from prompts.
This could potentially enable workers to annotate textual prompts with the aim of identifying prompt modifiers.
This data could be used as input for training and fine-tuning models.
Crowdsourced approaches have been vastly successful in helping realize gains in the emergent properties of language models in the field of Natural Language Processing (NLP).
For text-to-image generation, crowdsourced ratings equally hold value, and companies, such as Midjourney, are exploring adapting their models to user preferences.
% , for instance to extract key phrases and prompt modifiers from prompts with the aim of providing support and guidance to novices.
    % to classify and analyze input prompts.
Such fine-tuned models would not only be valuable for improving the user experience of prompt engineering and text-to-image generation, but also for better aligning generative models with user intent.
% general research to improve systems' understanding of user intent.
As an input for research on guiding machine learning systems, prompts written by humans could contribute toward better alignment of AI with human intent and values~\mbox{\cite{3491102.3501965.pdf}}.
% Language models need to become better aligned with humans to produce outputs that are truthful and not toxic and the recent trend toward scaling language models does not make the models better at understanding the intent of users~\mbox{\cite{2203.02155.pdf}}.
Prompt engineering is an exciting research area that can contribute towards better human-AI alignment.}


% ====================
\section{Conclusion}%
\label{sec:conclusion}%
% ====================
The past few years have seen the rise of generative models.
It is too early to tell whether this development will give birth to new professions, such as ``prompt engineer.''
However, generative AI will deeply affect and reconfigure the fabric of our society.
This opens exciting opportunities for research \hl{in the field of HCI.}

This article investigated \hl{prompt engineering, as a new type of skill,  in the context of AI art.}
In \hl{four} studies, we investigated whether \hl{crowdsourced participants
had knowledge of art, whether they
could recognize the quality of prompts and their resulting images, and whether participants could write and improve prompts.}
% We found participants recruited from Amazon Mechanical Turk are creative and able to write prompts for text-to-image systems in rich descriptive language. 
We found participants recruited from Amazon Mechanical Turk are creative and able to write prompts for text-to-image generation systems in rich descriptive language, but lacked the special vocabulary found in AI art communities. The use of prompt modifiers was not intuitive to participants, pointing towards prompt engineering being \hl{a non-intuitive skill.}
% for the generation of digital artworks.
We provided recommendations for conducting scientific experiments on prompt engineering and text-to-image generation with participants recruited from crowdsourcing platforms.
    % Based on our findings, we provided recommendations for conducting research on text-to-image generation with participants recruited from crowdsourcing platforms
% This article investigated prompt engineering for AI art with participants recruited from a paid crowdsourcing platform.
%
% In this paper, we explored whether crowd workers have the potential to participate in text-to-image generation.
% For crowd workers on crowdsourcing platforms to play a meaningful role in the emerging AI-driven creative ecosystem, workers must be able to participate in the novel text-based mode of creative production.
% Our % extensive
% four studies point toward crowd workers having the potential of contributing to prompt engineering.
    % Particiapnts in our studies overall lacked the special vocabulary used in the online communities for the generation of digital artworks.
    % % Workers were not controlling the style and quality of the image generation. Keywords and phrases that are being profusely used in the AI art community were virtually unknown to workers.
    % Participants in our study were also not able to significantly improve their written prompts.
% Based on our findings, we provided recommendations for conducting research on text-to-image generation with participants recruited from crowdsourcing platforms, and discussed four possible futures of prompt engineering.
% We conclude that prompt engineering is a learned skill that is learned with practice.
% Economic growth is driven  ``from  people  creating  ideas''
% "Economic  growth  arises  from  people  creating  ideas." 
% \cite{IdeaPF.pdf}.
We speculated on four possible futures for prompt engineering.
We hope that whatever the landscape of creative production will turn out to be in the future, it will be an inclusive creative economy in which everyone % crowd workers
can participate in meaningful ways.%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \newpage
% \pagestyle{empty}
% \section*{Disclosure of Interest} 
% The authors report there are no competing interests to declare.
% \section*{Funding} 
% This work was not supported by funding or grant-awarding bodies.
% \section*{Data availability statement}
% Data related to the study is available at\\ 
% %NON-ANON:
% \href{https://osf.io/bjwf4/?view_only=caf73282354643e9bfb34b3b05ef4b62}{https://osf.io/bjwf4/?view\_only=caf73282354643e9bfb34b3b05ef4b62}
% %ANON:
% % \href{https://osf.io/bjwf4/?view_only=d478a65d003544d581bb45bca3b14657}{https://osf.io/bjwf4/?view\_only=d478a65d003544d581bb45bca3b14657}
% \newpage
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% \begin{figure}[h]
%   \centering
%   \includegraphics[width=\linewidth]{sample-franklin}
%   \caption{1907 Franklin Model D roadster. Photograph by Harris \&
%     Ewing, Inc. [Public domain], via Wikimedia
%     Commons. (\url{https://goo.gl/VLCRBB}).}
%   \Description{A woman and a girl in white dresses sit in an open car.}
% \end{figure}

% \begin{table}
%   \caption{Frequency of Special Characters}
%   \label{tab:freq}
%   \begin{tabular}{ccl}
%     \toprule
%     Non-English or Math&Frequency&Comments\\
%     \midrule
%     \O & 1 in 1,000& For Swedish names\\
%     $\pi$ & 1 in 5& Common in math\\
%     \$ & 4 in 5 & Used in business\\
%     $\Psi^2_1$ & 1 in 40,000& Unexplained usage\\
%   \bottomrule
% \end{tabular}
% \end{table}

% \begin{table*}
%   \caption{Some Typical}
%   \label{tab:example}
%   \begin{tabular}{ccl}
%     \toprule
%     Command &A Number & Comments\\
%     \midrule
%     \texttt{{\char'134}author} & 100& Author \\
%     \texttt{{\char'134}table}& 300 & For tables\\
%     \texttt{{\char'134}table*}& 400& For wider tables\\
%     \bottomrule
%   \end{tabular}
% \end{table*}

% \begin{verbatim}
%   \begin{teaserfigure}
%     \includegraphics[width=\textwidth]{sampleteaser}
%     \caption{figure caption}
%     \Description{figure description}
%   \end{teaserfigure}
% \end{verbatim}

%% The acknowledgments section is defined using the "acks" environment
%% (and NOT an unnumbered section). This ensures the proper
%% identification of the section in the article metadata, and the
%% consistent spelling of the heading.
% \begin{acks}
% To Robert, for the bagels and explaining CMYK and color spaces.
% \end{acks}

%% The next two lines define the bibliography style to be used, and
%% the bibliography file.
%TC:ignore
\bibliographystyle{ACM-Reference-Format}
\bibliography{manuscript}
%TC:endignore

%TC:ignore
\appendix
\newpage
\section{Set of images used in Study 1}
\label{appendix:images}
%TC:endignore

% ------------
%TC:ignore
\newlength{\imageheight}
\setlength{\imageheight}{4.5cm}
\newlength{\skipspace}
\setlength{\skipspace}{.1cm}
%TC:endignore
% ------------

%TC:ignore
\subsection{Images with High Aesthetic Appeal}
\label{appendix:a1}
% --------------------
\input{TAB/TABLE-HIGH}
% --------------------
%TC:endignore

%TC:ignore
\newpage
\subsection{Images with Low Aesthetic Appeal}
\label{appendix:a2}
% --------------------
\input{TAB/TABLE-LOW}
% --------------------
%TC:endignore


% \newpage
% \section{Configuration Settings used in Studies 3 and 4.}
% \label{appendix:settings}
% % --------------------
% \input{TAB/TABLE-SETTINGS}
% % --------------------


\end{document}
\endinput
%% End of file `sample-acmsmall-submission.tex'.
