% --------------------
\subsection{Studies on the Art Expertise of Crowd Workers}%
% --------------------
Crowd workers have been engaged in a wide variety of studies and experiments that involve creativity \cite{CHI20}.
%
Already from the very early days of crowdsourcing research, the crowd participated, for instance, in the ludic assessment of images in the ESP Game \cite{ESPGame} or TagATune, a game for music and sound annotation \cite{TagATune}.
Aaron Koblin's crowdsourcing experiments, such as the Johnny Cash project \cite{johnnycash}, 10,000 cents \cite{cents}, and the Sheep Market \cite{sheepmarket}, are instances of widely popular experiments that successfully involved a crowd in the production of art.
Reddit's /r/place is another example of an experiment that was run in 2017 (and repeated in 2022) involving a crowd in collaboratively producing an artwork \cite{redditplace}.%

% \cite{Cohen2012_Chapter_EvaluationOfCreativeAesthetics.pdf}.

The crowd has also been found to be able to provide useful
% One active area of research is the crowdsourcing of
feedback on visual designs~\cite{luther,Voyant,dc2s2chi2019submission}. % Ngoon
    The feedback provided by crowd workers was found to be akin to expert feedback~\cite{Posts_paper_3.pdf}.
Besides providing feedback on visual designs, crowd workers have been engaged in assessing visualizations~\cite{2010-MTurk-CHI.pdf,Information_Visualization_Evaluation_Using_BORGO_Acc6May2018Epub10Jul2018_GREEN_AAM.pdf}.
    \citeauthor{Information_Visualization_Evaluation_Using_BORGO_Acc6May2018Epub10Jul2018_GREEN_AAM.pdf}
    evaluated information visualizations \cite{Information_Visualization_Evaluation_Using_BORGO_Acc6May2018Epub10Jul2018_GREEN_AAM.pdf} and \citeauthor{2010-MTurk-CHI.pdf} studied the crowdsourced graphical perception of visualization design~\cite{2010-MTurk-CHI.pdf}.
    The latter established that crowdsourced perception experiments are a viable method for evaluating visualization designs.
\citeauthor{2506364.2506368.pdf} studied the crowd's ability to assess the aesthetic appeal of images 
% and recognize content
in the context of Quality of Experience (QoE) assessments \cite{2506364.2506368.pdf}.
The authors found that while crowdsourcing can be used for subjective assessments of aesthetic appeal, care needs to be taken in the design of tasks to avoid misinterpretations. % \cite{2506364.2506368.pdf}.
Besides feedback on visual designs and visualizations, participatory design feedback has found application in a wide range of domains, such as architecture \cite{ecaade2020_037.pdf} and the cultural heritage domain \cite{00408.pdf,10.3390.info12020064.pdf}.%

While the above experiments and applications demonstrate that an untrained crowd can contribute to the production of art and provide meaningful feedback, only few studies focus specifically on the crowd workers' knowledge of art and creative abilities.
%
\citeauthor{18944-Article_Text-22710-1-2-20211004.pdf} studied the music perception skills of crowd workers on Prolific.
The authors found that ``untrained crowd workers can possess high perception skills on the music elements of melody, tuning, accent
and tempo'' \cite{18944-Article_Text-22710-1-2-20211004.pdf}.
%
\citeauthor{1866696.1866720.pdf} studied the ability of untrained crowd workers to label statements of commonsense knowledge  \cite{1866696.1866720.pdf}. The workers' judgments correlated with that of experts.
\citeauthor{Ramirez2022_Chapter_CrowdsourcingSyntacticallyDive.pdf} evaluated the crowd's ability to provide paraphrases
\cite{Ramirez2022_Chapter_CrowdsourcingSyntacticallyDive.pdf}.
The authors find that the crowd can be guided to provide syntactically diverse and novel paraphrases.
\citeauthor{PDF.pdf} compared natural language responses from the crowd with responses collected in a laboratory setting \cite{PDF.pdf}.
The study found crowdsourcing is a cost-effective and time-efficient way of collecting natural language data, but the vocabulary size of the workers on Amazon Mechanical Turk was ``less varied'' compared to the vocabulary of a laboratory sample.
A high amount of common knowledge, as well as large vocabulary size, could potentially prove to be useful attributes for prompt engineering.%

% Crowdworkersâ€™ Challenges and Motivations for an Image Description Task \cite{3415176.pdf}


The four studies in this paper connect to the above in multiple ways. We study the Fine Art knowledge of crowd workers on Amazon Mechanical Turk as a prerequisite for generating artworks from textual prompts. We then study whether workers are able to recognize high-quality prompts. We then study the workers' ability of putting their knowledge and understanding into practice.
In two consecutive studies, crowd workers first wrote input prompts in natural language for a text-to-image system and later revised their prompts.

% TRANSITION
