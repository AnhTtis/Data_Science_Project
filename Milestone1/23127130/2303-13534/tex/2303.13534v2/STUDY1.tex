% ====================
\section{Study 1: Crowd Worker's Knowledge of Art}
\label{sec:study1}
% ====================
The aim of this study is to explore the Fine Art knowledge of crowd workers.
Knowledge of artist names and art styles are essential to selecting the right modifiers to add to textual prompts for generating digital artworks with text-to-image systems.
% 
We further % Based on the above observations, we
test the hypothesis that MTurk workers are better at recognizing American artists than European artists. MTurk is a US-based crowdsourcing platform and empirical research on the demographics of MTurk workers has shown that US-based workers constitute the largest group of crowd workers (e.g.,~\cite{p135-difallah.pdf}).%

% --------------------
\subsection{Research Materials}%
% --------------------
We collected 25 artworks by popular artists as follows.
%
% \subsubsection{Artist and artwork selection}
We first searched for ``artists of the 20th century'' on Google Search.
The search results page (SERP) for this query contains a scrollable carousel at the top of the page.
From this carousel, we hand-selected 25 artists -- eleven artists from North America, eleven from Europe, and three with mixed or other background
    (e.g., Belarussian-French and Russian).
This information was taken from the artist's knowledge panel on Google Search.
    % explain knowledge panel
    Knowledge panels are boxes on the SERP that provide condensed information from Google's knowledge graph including an extract from a Wikipedia article.
%
% We collected information about the artists from their respective knowledge panel.
From each artist's knowledge panel, we collected the periods the artist is associated with (e.g. Surrealism) and one artwork.
    To mitigate bias in the selection of artworks, we collected the first artwork listed in the artist's knowledge panel, with four exceptions.
        For Frida Kahlo, we selected a different artwork due to the first result depicting close-up nudity (to avoid exposing workers to sensitive content). For Japser Johns, we selected ``Three Flags'' over ``Flags'' and for Edvard Munch we selected ``Anxiety'' over ``Vampire'' because these artworks may be more recognizable. Last, for Joan Miró, we selected ``The Harlequin's Carnival'' because the knowledge panel for this artwork contains information on the period of the artwork.
For each artwork, we collected information on its style from the knowledge panel on the artwork's SERP. If this information was not available, we substituted with the periods associated with the artist.
The resulting dataset contains 25 artists and 25 artworks listed in \autoref{tab:artworks}.%
% The artworks and their styles are listed in \autoref{tab:artworks}.
%
% --------------------
\subsection{Task Design}%
% --------------------
We designed a simple crowdsourcing task depicting one of the 25 artworks at a time.
Workers were instructed to identify the artist's name and the artwork's style in two mandatory input fields.
    Workers were instructed to make a guess if they did not know an answer, but also instructed not to enter irrelevant information.
The instructions included an explanation of the concept ``style'' taken from a Wikipedia page on style in the visual arts:
    \textit{``The style of an artwork relates its visual appearance to other works by the same artist or one from the same period, training, location, `school', art movement or culture''} \cite{Wikipedia_Style}.
Workers were specifically instructed not to use a search engine to complete the task.
For the study, the filenames of the images were anonymized to not allow easy look-up of information.
%
Workers were paid US\$0.05 per task (aiming for an average pay above the minimum wage in the United States). The task pricing was calculated based on the average completion times in a pilot study ($N=26$, US\$0.02 per task).%

% ------
\input{TABLE-STUDY1-ARTWORKS}%
% ------

% --------------------
\subsection{Participant Recruitment}%
% --------------------
% Participants were recruited from two crowdsourcing platforms: Amazon Mechanical Turk and Prolific.
% In prior research, these two platforms have proven to be a fertile complement to one another in the context of creativity-related studies \cite{CHI20}.
\ok{%
We recruited US-based workers from Amazon Mechanical Turk (MTurk) with a task approval rate greater than 95\% and at least 1000 completed tasks. This combination of qualification criteria is common in crowdsourcing research (e.g., \citet{3491102.3517434}).
The task was designed as a microtask. Therefore, no demographic data on the workers was collected in this study.
Three tasks were rejected due to no visible effort being made to answer the task with relevant information. The three tasks were republished for other workers to complete.
In total, our study comprised 750 tasks (25 artworks $\times$ 30 tasks) completed by 128 unique workers.%
}%
% We found very few signs of insincere answers.
% One worker copied and pasted parts of the instructions and one entered numbers. The two workers were excluded from the data analysis and future studies.
%
% --------------------
\subsection{Data Analysis}%
% --------------------
% We analyzed the data as follows.
For the two answers provided in each task (artist name and artwork style), we manually decided whether the answer was correct. This evaluation was straight-forward and did not require expert knowledge since it only consisted of matching the worker-provided answer with the list of artist names and artwork styles collected from the knowledge panel.
% No inter-rater agreement was calculated \cite{McDonald_Reliability_CSCW19.pdf}.
However, crowdsourced data is noisy \cite{crowdsourcing-userstudies,3491102.3517434} and our data makes no exception.
We decided not to penalize typographic mistakes and partially incorrect answers.
    For instance, we allowed Pollack as correct answer for the artist Jackson Pollock and Wood -- but not Grant -- as correct answer for Grant Wood.
    % Similarly for artwork styles, ``expressionism'' and ``abstract'' was counted as correct for Abstract Expressionism.
For Grant Wood, Edward Hopper, and Jasper Johns we additionally accepted Americana Art as style even though it was not listed in the knowledge panel.
If the answer was correct but in the wrong input field, we counted the answer as correct, since minor improvements in the task design would likely correct this issue.%
% Some workers entered the artist name and artwork style in the wrong fields.
% Some workers correctly entered the title of the artwork in the style field. Even though the worker demonstrated knowledge in this case, we did .....


% --------------------
\subsection{Results}%
% --------------------
Overall, the workers in our sample were able to correctly identify the artist in slightly more than half of the cases (%$n=395$;
52.67\%). The artwork's style was correctly identified 358 times (48.05\%).
Workers were able to correctly identify at least one of the two answers in 538 cases (71.73\%)
and workers correctly identified both the artist and the artwork's style in almost 30\% of the tasks (% $n=216$;
28.80\%).%
% In about 70\% of the tasks (534; 71.20\%), workers were not able to guess either the artist name or artwork style correctly.

Some artists and artwork styles were recognized more frequently than others (see \autoref{tab:artworks}).
% While on the whole, detection rates were fairly low (about 50\%), some artists and artistic styles were recognized more often.
The top five correctly recognized artists were Jackson Pollock, Salvador Dali, Claude Monet, Frida Kahlo, and Pablo Picasso.
The least recognized artists were Georges Braque, Paul C\'ezanne, Egon Schiele, Joan Mir\'o, and Wassily Kandinsky.
The three styles recognized most often include
    Abstract expressionism (Jackson Pollock), Surrealism (Salvador Dalí), and Impressionism (Claude Monet).
The three styles recognized least often include
    the styles associated with Georgia O'Keeffe (see \autoref{tab:artworks}),
    Fauvism/Modernism (Henri Matisse),
    % and Surrealism/Dada (Man Ray).
    and Pop art/Contemporary art (Jeff Koons).

Common mistakes made by workers include
% \begin{itemize}
    misspellings of artist names (e.g., Pollack instead of Pollock),
    attributing a painting to the wrong artist (e.g., Warhol instead of Lichtenstein), 
    entering artwork titles instead of artwork styles, and
    attributing an artist name based on information in the image.
% \end{itemize}
%
The latter was the case for Marcel Duchamp's ``Fountain'' which contains a legible inscription (``R. Mutt''). A high number of workers ($n=17$; 56.67\% of the whole response set for this artwork) entered the wrong artist name in this case.
%
Among the workers who entered the correct title of the artwork in one of the input fields ($n=81$; 10.8\% of all responses), some artwork titles were entered more frequently than others (see \autoref{tab:artworks}).
The most recognized titles include
    Jasper John's ``Three Flags,''
    Grant Wood's ``American Gothic,''
    % Frida Kahlo	self-portrait,
    Pablo Picasso's	``Guernica,'' and
    Paul Klee's	``Castle and Sun.''%
% The high number of entries for Frida Kahlo can be explained by the title containing the generic keyword ``self-portrait''.

\input{FIG-SCATTERPLOT}

We noticed differences in the workers' knowledge of art. Some workers were noticeably better at solving the given task than other workers (see 
\autoref{fig:scatterplot}).
A linear regression indicates that there is a positive relationship between the number of correct responses for artwork styles and artist names, $F(1,126)=138.1$, $p<0.001$, $R^2=0.5228$, $R^2_{adjusted}=0.519$.
% The regression coefficient (B=X, 95\% CI [x, x] indicated that xxx
%
% lm(formula = data$Artist.correct ~ data$Style.correct)
% Residuals:
%     Min      1Q  Median      3Q     Max 
% -6.7025 -1.5911 -0.7628  0.3027 18.4390 
% Coefficients:
%                    Estimate Std. Error t value Pr(>|t|)    
% (Intercept)          0.7628     0.3662   2.083   0.0393 *  
% data$Style.correct   0.8283     0.0705  11.750   <2e-16 ***
% ---
% Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1
% Residual standard error: 3.487 on 126 degrees of freedom
% Multiple R-squared:  0.5228,	Adjusted R-squared:  0.519 
% F-statistic: 138.1 on 1 and 126 DF,  p-value: < 2.2e-16
%
% Describe the best detection / worst detection rate
When ordered by their success rate in detecting artist names and artwork styles, the top 50\% of crowd workers account for over nine out of ten (90.63\%) correct responses of artist names and 93.87\% of correct style responses.
% Nine crowd workers were able to correctly identify both the artist name and artwork style. These workers, however, completed only one or two tasks overall.
    The top 20\% of well-performing workers account for about 40\% of the correct responses.
% This is due to the top workers having made only one or two guesses.
% Detection rates dropped the more guesses a worker made.
% The best performing worker was able to correctly guess xxx
The distribution of responses is long-tailed. Few crowd workers completed many tasks, many completed few. Similar participation dynamics can be found on the World Wide Web, for instance on image rating websites~\cite{joshi_1.pdf,icwsm15beauty.pdf}.
However, the ratio of correct responses to the number of all responses left by each worker is two-tailed (see \autoref{fig:scatterplot}).
Almost one third of the crowd workers correctly identified both the artist and the style of the artwork in all of their completed tasks.%

% Further, one in ten workers correctly entered the artwork title.
% We believe that detection rates could be improved if workers were allowed to use a search engine.

To test our hypothesis that workers were better at recognizing the works of American artists compared to works from European artists, we compared the results for two equal sized groups of artists (eleven artists from North America and eleven from Europe).
Workers had a significantly better ability to identify the artist name correctly if the artist was from North America as compared to Europe
    (Fisher's Exact Test,
    $p=0.01$, % 0.009791
    $OR=0.66$, % 0.659071
    95\% CI [0.48, 0.91]). % 0.478127, 0.907186
We found no significant difference in the workers' ability to recognize the artwork's style
    (Fisher's Exact Test, % for Count Data
    $p=0.31$, % 0.3112
    $OR = 1.19$, % 1.185023
    95\% CI [0.86, 1.63]).% 0.863013, 1.628166


After this first study, we were cautiously optimistic that about a third of the workers would -- in theory -- have the knowledge to write effective prompts for generating digital artworks.
In the following study, we explored whether crowd workers had an understanding of the mechanics of prompt prompt engineering.