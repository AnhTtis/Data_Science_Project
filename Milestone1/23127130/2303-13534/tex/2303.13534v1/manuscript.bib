@article{hsieh2005three,
  title={Three approaches to qualitative content analysis},
  author={Hsieh, Hsiu-Fang and Shannon, Sarah E},
  journal={Qualitative health research},
  volume={15},
  number={9},
  pages={1277--1288},
  year={2005},
  publisher={Sage Publications Sage CA},
  address={Thousand Oaks, CA},
}

@inproceedings{GenAI,
title={Text-to-Image Generation: Perceptions and Realities},
author={Jonas Oppenlaender and Aku Visuri and Ville Paananen and Rhema Linder and Johanna Silvennoinen},
year={2023},
booktitle={Workshop on Generative AI in HCI},
series={CHI '23'},
numpages={5},
}

@misc{modifiers,
  doi = {10.48550/ARXIV.2204.13988},
  author = {Oppenlaender, Jonas},
  keywords = {Multimedia (cs.MM), Computation and Language (cs.CL), Human-Computer Interaction (cs.HC), FOS: Computer and information sciences, FOS: Computer and information sciences, H.5; H.m; J.5},
  title = {A Taxonomy of Prompt Modifiers for Text-To-Image Generation},
  publisher = {arXiv},
  year = {2022},
}

@misc{dreambooth,
  doi = {10.48550/ARXIV.2208.12242},
  author = {Ruiz, Nataniel and Li, Yuanzhen and Jampani, Varun and Pritch, Yael and Rubinstein, Michael and Aberman, Kfir},
  
  keywords = {Computer Vision and Pattern Recognition (cs.CV), Graphics (cs.GR), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {DreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven Generation},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {Creative Commons Attribution 4.0 International}
}


@misc{chatgpt,
title={ChatGPT: Optimizing
Language Models
for Dialogue},
author={{OpenAI}},
year={2022},
url={https://openai.com/blog/chatgpt/},
}


@inproceedings{2303.04587.pdf,
title={A Prompt Log Analysis of Text-to-Image Generation Systems},
author={Yutong Xie and Zhaoying Pan and Jinge Ma and Luo Jie and Qiaozhu Mei},
year={2023},
booktitle={Proceedings of the ACM Web Conference},
series={WWW '23},
}


@misc{textualinversion,
  doi = {10.48550/ARXIV.2208.01618},
  author = {Gal, Rinon and Alaluf, Yuval and Atzmon, Yuval and Patashnik, Or and Bermano, Amit H. and Chechik, Gal and Cohen-Or, Daniel},
  
  keywords = {Computer Vision and Pattern Recognition (cs.CV), Computation and Language (cs.CL), Graphics (cs.GR), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {An Image is Worth One Word: Personalizing Text-to-Image Generation using Textual Inversion},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}




@inproceedings{2556288.2557298.pdf,
author = {Hoare, Michaela and Benford, Steve and Jones, Rachel and Milic-Frayling, Natasa},
title = {Coming in from the Margins: Amateur Musicians in the Online Age},
year = {2014},
isbn = {9781450324731},
publisher = {Association for Computing Machinery},
address = {New York, NY},
doi = {10.1145/2556288.2557298},
abstract = {HCI is increasingly interested in amateurism, but the wider literature suggests that the amateur is a complex and distinctive phenomenon. An interview study reveals the nature of the amateur in the digital age. Even though operating non-professionally at a micro-scale, amateur musicians employ a plethora of online services to sustain local fanbases, reach out to new fans, collaborate internationally, and actively promote both digital and material products. Our findings lead to recommendations for event-oriented promotion tools; community-oriented analytics; tangible and embedded products; and limited-edition digital experiences. We conclude that HCI needs to recognise the amateur as an important class of user, one who is serious about their leisure, and who is also distinct from the professional as from the novice and hobbyist.},
booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
pages = {1295–1304},
numpages = {10},
keywords = {diy, music, amateur, social media, craft, distribution, sharing, promotion, community, tangible},
series = {CHI '14}
}
location = {Toronto, Ontario, Canada},
, USA

@article{BURLESON2005436,
title = {Developing creativity, motivation, and self-actualization with learning systems},
journal = {International Journal of Human-Computer Studies},
volume = {63},
number = {4},
pages = {436-451},
year = {2005},
note = {Computer support for creativity},
issn = {1071-5819},
doi = {https://doi.org/10.1016/j.ijhcs.2005.04.007},
url = {https://www.sciencedirect.com/science/article/pii/S1071581905000467},
author = {Winslow Burleson},
keywords = {Creativity, Learning systems, Psychology, Failure, Motivation},
abstract = {Developing learning experiences that facilitate self-actualization and creativity is among the most important goals of our society in preparation for the future. To facilitate deep understanding of a new concept, to facilitate learning, learners must have the opportunity to develop multiple and flexible perspectives. The process of becoming an expert involves failure, as well as the ability to understand failure and the motivation to move onward. Meta-cognitive awareness and personal strategies can play a role in developing an individual's ability to persevere through failure, and combat other diluting influences. Awareness and reflective technologies can be instrumental in developing a meta-cognitive ability to make conscious and unconscious decisions about engagement that will ultimately enhance learning, expertise, creativity, and self-actualization. This paper will review diverse perspectives from psychology, engineering, education, and computer science to present opportunities to enhance creativity, motivation, and self-actualization in learning systems.}
}


@article{p75-weiser.pdf,
author = {Weiser, Mark},
title = {Some Computer Science Issues in Ubiquitous Computing},
year = {1993},
issue_date = {July 1993},
publisher = {Association for Computing Machinery},
address = {New York, NY},
volume = {36},
number = {7},
issn = {0001-0782},
doi = {10.1145/159544.159617},
journal = {Commun. ACM},
month = {jul},
pages = {75–84},
numpages = {10},
keywords = {ubiquitous computing}
}
, USA

@misc{promptism,
    author={``Johannezz''},
    title={The Promptist Manifesto},
    url={https://deeplearn.art/the-promptist-manifesto/},
    year={2022},
}


@misc{DALLE2,
  doi = {10.48550/ARXIV.2204.06125},
    author = {Ramesh, Aditya and Dhariwal, Prafulla and Nichol, Alex and Chu, Casey and Chen, Mark},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Hierarchical Text-Conditional Image Generation with CLIP Latents},
  publisher = {arXiv},
  year = {2022},
}


@inproceedings{3491102.3501965.pdf,
author = {Boggust, Angie and Hoover, Benjamin and Satyanarayan, Arvind and Strobelt, Hendrik},
title = {Shared Interest: Measuring Human-AI Alignment to Identify Recurring Patterns in Model Behavior},
year = {2022},
isbn = {9781450391573},
publisher = {Association for Computing Machinery},
address = {New York, NY},
doi = {10.1145/3491102.3501965},
abstract = {Saliency methods — techniques to identify the importance of input features on a model’s output — are a common step in understanding neural network behavior. However, interpreting saliency requires tedious manual inspection to identify and aggregate patterns in model behavior, resulting in ad hoc or cherry-picked analysis. To address these concerns, we present Shared Interest: metrics for comparing model reasoning (via saliency) to human reasoning (via ground truth annotations). By providing quantitative descriptors, Shared Interest enables ranking, sorting, and aggregating inputs, thereby facilitating large-scale systematic analysis of model behavior. We use Shared Interest to identify eight recurring patterns in model behavior, such as cases where contextual features or a subset of ground truth features are most important to the model. Working with representative real-world users, we show how Shared Interest can be used to decide if a model is trustworthy, uncover issues missed in manual analyses, and enable interactive probing.},
booktitle = {Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems},
articleno = {10},
numpages = {17},
keywords = {human-computer interaction, saliency methods, interpretability, machine learning},
series = {CHI '22}
}
location = {New Orleans, LA, USA},
, USA

@article{AlignmentHardStart.pdf,
	author = {Eliezer Yudkowsky},
	type = {PDF},
	title = {The AI alignment problem: why it is hard, and where to start},
	journal = {Symbolic Systems Distinguished Speaker},
	publisher = {intelligence.org},
	url = {https://intelligence.org/files/AlignmentHardStart.pdf},
	year = {2016},
	abstract = {… This is where we are on most of the AI alignment problems, like if I ask you, “How do you build a friendly AI?” What stops you is not that you don’t have enough computing power. What …},
}

@article{Gabriel2020_Article_ArtificialIntelligenceValuesAn.pdf,
author={Gabriel, Iason},
year={2020},
title={Artificial Intelligence, Values, and Alignment},
journal={Minds and Machines},
pages={411-437},
volume={30},
number={3},
doi={10.1007/s11023-020-09539-2},
}



@misc{EZCharts,
title={EZ Charts -- Diffusion Parameter Studies},
author={{EnzymeZoo and contributors}},
year={2022},
url={https://docs.google.com/document/d/1ORymHm0Te18qKiHnhcdgGp-WSt8ZkLZvow3raiu2DVU/edit?usp=sharing},
}



@misc{midjourney,
    title={Midjourney.com},
    author={{Midjourney}},
    year={2022},
    url={https://www.midjourney.com},
}


@misc{2205.11487.pdf,
  doi = {10.48550/ARXIV.2205.11487},
  author = {Saharia, Chitwan and Chan, William and Saxena, Saurabh and Li, Lala and Whang, Jay and Denton, Emily and Ghasemipour, Seyed Kamyar Seyed and Ayan, Burcu Karagol and Mahdavi, S. Sara and Lopes, Rapha Gontijo and Salimans, Tim and Ho, Jonathan and Fleet, David J and Norouzi, Mohammad},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding},
  publisher = {arXiv},
  year = {2022},
}


@inproceedings{1908.07125.pdf,
    title = "Universal Adversarial Triggers for Attacking and Analyzing {NLP}",
    author = "Wallace, Eric  and
      Feng, Shi  and
      Kandpal, Nikhil  and
      Gardner, Matt  and
      Singh, Sameer",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    doi = "10.18653/v1/D19-1221",
    pages = "2153--2162",
    abstract = "Adversarial examples highlight model vulnerabilities and are useful for evaluation and interpretation. We define universal adversarial triggers: input-agnostic sequences of tokens that trigger a model to produce a specific prediction when concatenated to any input from a dataset. We propose a gradient-guided search over tokens which finds short trigger sequences (e.g., one word for classification and four words for language modeling) that successfully trigger the target prediction. For example, triggers cause SNLI entailment accuracy to drop from 89.94{\%} to 0.55{\%}, 72{\%} of {``}why{''} questions in SQuAD to be answered {``}to kill american people{''}, and the GPT-2 language model to spew racist output even when conditioned on non-racial contexts. Furthermore, although the triggers are optimized using white-box access to a specific model, they transfer to other models for all tasks we consider. Finally, since triggers are input-agnostic, they provide an analysis of global model behavior. For instance, they confirm that SNLI models exploit dataset biases and help to diagnose heuristics learned by reading comprehension models.",
}

@book{entropy,
author={Shannon, Claude E. and Weaver, Warren},
title={The mathematical theory of communication},
year={1949},
publisher={University of Illinois Press},
address={Urbana, Illinois},
}

@misc{pythonpackage,
    author={Kristopher Kyle},
    year={2018},
    title={lexical-diversity Python package},
    url={https://github.com/kristopherkyle/lexical_diversity},
}
    publisher={GitHub},


@inproceedings{P04-3031.pdf,
    title = "{NLTK}: The Natural Language Toolkit",
    author = "Bird, Steven  and
      Loper, Edward",
    booktitle = "Proceedings of the {ACL} Interactive Poster and Demonstration Sessions",
    month = jul,
    year = "2004",
    address = "Barcelona, Spain",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P04-3031",
    pages = "214--217",
}


@inproceedings{DS77_158.pdf,
title={Measuring Aesthetic in Design},
booktitle={Proceedings of the {DESIGN} 2014 13th International Design Conference},
year={2014},
publisher={The Design Society},
author={Khalighy, Shahabeddin and Green, G. and Scheepers, Christoph and Whittet, Craig},
pages={2083-2094},
}


@inproceedings{icwsm15beauty.pdf,
   author = {Schifanella, Rossano and Redi, Miriam and Aiello, Luca Maria},
   title = {An Image is Worth More than a Thousand Favorites: Surfacing the Hidden Beauty of Flickr Pictures},
   booktitle = {Proceedings of the 9th AAAI International Conference on Weblogs and Social Media},
   series={ICWSM '15},
   year = {2015},
   publisher = {AAAI}
}
   location = {Oxford, UK},


@ARTICLE{06286980.pdf,
  author={Pinson, Margaret H. and Janowski, Lucjan and Pepion, Romuald and Huynh-Thu, Quan and Schmidmer, Christian and Corriveau, Phillip and Younkin, Audrey and Le Callet, Patrick and Barkowsky, Marcus and Ingram, William},
  journal={IEEE Journal of Selected Topics in Signal Processing}, 
  title={The Influence of Subjects and Environment on Audiovisual Subjective Tests: An International Study}, 
  year={2012},
  volume={6},
  number={6},
  pages={640-651},
  doi={10.1109/JSTSP.2012.2215306},
}


@article{Hauser-Schwarz2016_Article_AttentiveTurkersMTurkParticipa.pdf,
author={Hauser, David J. and Schwarz, Norbert},
year={2016},
title={Attentive Turkers: MTurk participants perform better on online attention checks than do subject pool participants},
journal={Behavior Research Methods},
pages={400-407},
volume={48},
number={1},
doi={10.3758/s13428-015-0578-z},
}


@INPROCEEDINGS{06065690.pdf,
  author={Hoßfeld, Tobias and Schatz, Raimund and Egger, Sebastian},
  booktitle={2011 Third International Workshop on Quality of Multimedia Experience}, 
  title={SOS: The MOS is not enough!}, 
  year={2011},
  volume={},
  number={},
  pages={131-136},
  doi={10.1109/QoMEX.2011.6065690},
}


@INPROCEEDINGS{06982326.pdf,
  author={Siahaan, Ernestasia and Redi, Judith A. and Hanjalic, Alan},
  booktitle={2014 Sixth International Workshop on Quality of Multimedia Experience (QoMEX)}, 
  title={Beauty is in the scale of the beholder: Comparison of methodologies for the subjective assessment of image aesthetic appeal}, 
  year={2014},
  volume={},
  number={},
  pages={245-250},
  doi={10.1109/QoMEX.2014.6982326},
}
  

@inproceedings{crowdsourcing-userstudies,
author = {Kittur, Aniket and Chi, Ed H. and Suh, Bongwon},
title = {Crowdsourcing User Studies with Mechanical Turk},
year = {2008},
isbn = {9781605580111},
publisher = {Association for Computing Machinery},
address = {New York, NY},
doi = {10.1145/1357054.1357127},
abstract = {User studies are important for many aspects of the design process and involve techniques ranging from informal surveys to rigorous laboratory studies. However, the costs involved in engaging users often requires practitioners to trade off between sample size, time requirements, and monetary costs. Micro-task markets, such as Amazon's Mechanical Turk, offer a potential paradigm for engaging a large number of users for low time and monetary costs. Here we investigate the utility of a micro-task market for collecting user measurements, and discuss design considerations for developing remote micro user evaluation tasks. Although micro-task markets have great potential for rapidly collecting user measurements at low costs, we found that special care is needed in formulating tasks in order to harness the capabilities of the approach.},
booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
pages = {453–456},
numpages = {4},
keywords = {micro task, remote user study, Mechanical Turk, Wikipedia},
series = {CHI '08}
}
location = {Florence, Italy},
, USA

@inproceedings{p135-difallah.pdf,
author = {Difallah, Djellel and Filatova, Elena and Ipeirotis, Panos},
title = {Demographics and Dynamics of Mechanical Turk Workers},
year = {2018},
isbn = {9781450355810},
publisher = {Association for Computing Machinery},
address = {New York, NY},
doi = {10.1145/3159652.3159661},
abstract = {We present an analysis of the population dynamics and demographics of Amazon Mechanical Turk workers based on the results of the survey that we conducted over a period of 28 months, with more than 85K responses from 40K unique participants. The demographics survey is ongoing (as of November 2017), and the results are available at http://demographics.mturk-tracker.com: we provide an API for researchers to download the survey data. We use techniques from the field of ecology, in particular, the capture-recapture technique, to understand the size and dynamics of the underlying population. We also demonstrate how to model and account for the inherent selection biases in such surveys. Our results indicate that there are more than 100K workers available in Amazon»s crowdsourcing platform, the participation of the workers in the platform follows a heavy-tailed distribution, and at any given time there are more than 2K active workers. We also show that the half-life of a worker on the platform is around 12-18 months and that the rate of arrival of new workers balances the rate of departures, keeping the overall worker population relatively stable. Finally, we demonstrate how we can estimate the biases of different demographics to participate in the survey tasks, and show how to correct such biases. Our methodology is generic and can be applied to any platform where we are interested in understanding the dynamics and demographics of the underlying user population.},
booktitle = {Proceedings of the Eleventh ACM International Conference on Web Search and Data Mining},
pages = {135–143},
numpages = {9},
keywords = {demographics, crowdsourcing, amazon mechanical turk, surveys, capture-recapture, dynamics, selection bias},
series = {WSDM '18}
}
    location = {Marina Del Rey, CA, USA},
, USA

@misc{Wikipedia_Style,
title={Style (visual arts)},
author={Wikipedia},
year={2022},
url={https://en.wikipedia.org/wiki/Style\_(visual\_arts)},
}


@inproceedings{prompt-tuning,
    title = "The Power of Scale for Parameter-Efficient Prompt Tuning",
    author = "Lester, Brian  and
      Al-Rfou, Rami  and
      Constant, Noah",
    booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2021",
    address = "Online and Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    doi = "10.18653/v1/2021.emnlp-main.243",
    pages = "3045--3059",
    abstract = "In this work, we explore {``}prompt tuning,{''} a simple yet effective mechanism for learning {``}soft prompts{''} to condition frozen language models to perform specific downstream tasks. Unlike the discrete text prompts used by GPT-3, soft prompts are learned through backpropagation and can be tuned to incorporate signals from any number of labeled examples. Our end-to-end learned approach outperforms GPT-3{'}s few-shot learning by a large margin. More remarkably, through ablations on model size using T5, we show that prompt tuning becomes more competitive with scale: as models exceed billions of parameters, our method {``}closes the gap{''} and matches the strong performance of model tuning (where all model weights are tuned). This finding is especially relevant because large models are costly to share and serve and the ability to reuse one frozen model for multiple downstream tasks can ease this burden. Our method can be seen as a simplification of the recently proposed {``}prefix tuning{''} of Li and Liang (2021) and we provide a comparison to this and other similar approaches. Finally, we show that conditioning a frozen model with soft prompts confers benefits in robustness to domain transfer and enables efficient {``}prompt ensembling.{''} We release code and model checkpoints to reproduce our experiments.",
}


@misc{2021.acl-long.353.pdf,
      title={Prefix-Tuning: Optimizing Continuous Prompts for Generation}, 
      author={Xiang Lisa Li and Percy Liang},
      year={2021},
      eprint={2101.00190},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}


@misc{2205.12548.pdf,
  doi = {10.48550/ARXIV.2205.12548},
  author = {Deng, Mingkai and Wang, Jianyu and Hsieh, Cheng-Ping and Wang, Yihan and Guo, Han and Shu, Tianmin and Song, Meng and Xing, Eric P. and Hu, Zhiting},
  title = {RLPrompt: Optimizing Discrete Text Prompts With Reinforcement Learning},
  publisher = {arXiv},
  year = {2022},
}
  keywords = {Computation and Language (cs.CL), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  


@article{10.1163@22134913-00002052.pdf,
      author = "Shihoko Arai and Hideaki Kawabata",
      title = "Appreciation Contexts Modulate Aesthetic Evaluation and Perceived Duration of Pictures",
      journal = "Art \& Perception",
      year = "2016",
      publisher = "Brill",
      address = "Leiden, The Netherlands",
      volume = "4",
      number = "3",
      doi = "10.1163/22134913-00002052",
      pages=      "225 - 239",
}


@article{gerger2014.pdf,
title = {Context effects on emotional and aesthetic evaluations of artworks and IAPS pictures},
journal = {Acta Psychologica},
volume = {151},
pages = {174-183},
year = {2014},
issn = {0001-6918},
doi = {10.1016/j.actpsy.2014.06.008},
author = {Gernot Gerger and Helmut Leder and Alexandra Kremer},
keywords = {Art, IAPS, Facial EMG, Context effects, Aesthetic, Emotion},
abstract = {In the arts emotionally negative objects sometimes can be positively judged. Defining an object as art possibly yields specific changes in how perceivers emotionally experience and aesthetically judge a stimulus. To study how emotional experiences (joy, anger, disgust, fear, sadness, and shame ratings, plus facial EMG) and aesthetic judgements (liking ratings) are modulated by an art context (“This is an artwork”) as compared to non-art reality context (“This is a press photograph”) participants evaluated IAPS pictures and veridical artworks depicting emotionally positive and negative content. In line with the assumption that emotional distancing is an essential feature of the art experience we found that positive emotional reactions were attenuated (joy, M. zygomaticus activation) in an art compared to non-art context. However, context had little influence on negative emotional reactions (anger, disgust, fear, sadness, shame, and M. corrugator activation) suggesting that these are similar in art and non-art. Importantly, only artworks of emotionally negative content were judged more positively in an art context — thus liked more. This study, in accordance with the assumption of a distanced aesthetic mode, shows that an art context fosters appraisal processes that influence emotional experiences, allowing to judge negative stimuli aesthetically more positively — thus suppressing the immediacy of emotional stimulus content.}
}


@article{2515245919847202.pdf,
author = {David C. Funder and Daniel J. Ozer},
title ={Evaluating Effect Size in Psychological Research: Sense and Nonsense},
journal = {Advances in Methods and Practices in Psychological Science},
volume = {2},
number = {2},
pages = {156-168},
year = {2019},
doi = {10.1177/2515245919847202},
    abstract = { Effect sizes are underappreciated and often misinterpreted—the most common mistakes being to describe them in ways that are uninformative (e.g., using arbitrary standards) or misleading (e.g., squaring effect-size rs). We propose that effect sizes can be usefully evaluated by comparing them with well-understood benchmarks or by considering them in terms of concrete consequences. In that light, we conclude that when reliably estimated (a critical consideration), an effect-size r of .05 indicates an effect that is very small for the explanation of single events but potentially consequential in the not-very-long run, an effect-size r of .10 indicates an effect that is still small at the level of single events but potentially more ultimately consequential, an effect-size r of .20 indicates a medium effect that is of some explanatory and practical use even in the short run and therefore even more important, and an effect-size r of .30 indicates a large effect that is potentially powerful in both the short and the long run. A very large effect size (r = .40 or greater) in the context of psychological research is likely to be a gross overestimate that will rarely be found in a large sample or in a replication. Our goal is to help advance the treatment of effect sizes so that rather than being numbers that are ignored, reported without interpretation, or interpreted superficially or incorrectly, they become aspects of research reports that can better inform the application and theoretical development of psychological research. }
}



@ARTICLE{fpsyg-08-01729.pdf,
AUTHOR={Pelowski, Matthew and Gerger, Gernot and Chetouani, Yasmine and Markey, Patrick S. and Leder, Helmut},   
TITLE={But Is It really Art? The Classification of Images as “Art”/“Not Art” and Correlation with Appraisal and Viewer Interpersonal Differences}, JOURNAL={Frontiers in Psychology},      
VOLUME={8},      
YEAR={2017},   
articleno={1729},
DOI={10.3389/fpsyg.2017.01729},      
ISSN={1664-1078},   
ABSTRACT={When an individual participates in empirical studies involving the visual arts, they most often are presented with a stream of images, shown on a computer, depicting reproductions of artworks by respected artists but which are often not known to the viewer. While art can of course be shown in presentia actuale—e.g., in the museum—this laboratory paradigm has become our go-to basis for assessing interaction, and, often in conjunction with some means of rating, for assessing evaluative, emotional, cognitive, and even neurophysiological response. However, the question is rarely asked: Do participants actually believe that every image that they are viewing is indeed “Art”? Relatedly, how does this evaluation relate to aesthetic appreciation, and do the answers to these questions vary in accordance with different strategies and interpersonal differences? In this paper, we consider the spontaneous classification of digital reproductions as art or not art. Participants viewed a range of image types—Abstract, Hyperrealistic, Poorly Executed paintings, Readymade sculptures, as well as Renaissance and Baroque paintings. They classified these as “art” or “not art” using both binary and analog scales, and also assessed for liking. Almost universally, individuals did not find all items within a class to be “art,” nor did all participants agree on the arthood status for any one item. Art classification in turn showed a significant positive correlation with liking. Whether an object was classified as art moreover correlated with specific personality variables, tastes, and decision strategies. The impact of these findings is discussed for selection/assessment of participants and for better understanding the basis of findings in past and future empirical art research.},
numpages={21},
}



@article{06eeaa0949554069958cd9e9e18641e5,
title = "The aesthetic pleasure in design scale: The development of a scale to measure aesthetic pleasure for designed artifacts",
abstract = "There is a lack of consistency regarding the scales used to measure aesthetic pleasure within design. They are often chosen ad hoc or adopted from other research fields without being validated for designed artifacts. Moreover, many scales do not measure aesthetic pleasure in isolation, but instead include its determinants (e.g., novelty). Therefore, we developed a new scale to measure aesthetic pleasure and included scales to measure its known determinants for discriminant validity purposes, which automatically led to validating these determinants as well. In the exploratory phase, we identified highly reliable items representative of aesthetic pleasure and its determinants across product categories. In the validation phase, we confirmed these findings across different countries (Australia, the Netherlands, and Taiwan). The final scale consists of 5 items, {"}beautiful,{"} {"}attractive,{"} {"}pleasing to see,{"} {"}nice to see,{"} and {"}like to look at,{"} that together reliably capture the construct of aesthetic pleasure. Several recommendations are formulated regarding the application of this scale in design studies and beyond.",
keywords = "Aesthetic pleasure, Design, Determinants of aesthetic pleasure, Scale development",
author = "Janneke Blijlevens and Paul Hekkert and Clementine Thurgood and Lin-Lin Chen and {Allen Whitfield}, T.W.",
year = "2017",
doi = "10.1037/aca0000098",
language = "English",
volume = "11",
pages = "86--98",
journal = "Psychology of Aesthetics, Creativity, and the Arts",
issn = "1931-3896",
publisher = "American Psychological Association Inc.",
number = "1",
}


@inproceedings{aiartcreativity,
  author = {Oppenlaender, Jonas},
  title = {The Creativity of Text-to-Image Generation},
  year = {2022},
  isbn = {9781450399555},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3569219.3569352},
  booktitle = {25th International Academic Mindtrek Conference},
  pages = {192–202},
  numpages = {11},
  series = {Academic Mindtrek 2022},
  doi = {10.1145/3569219.3569352},
}
  preprint = {https://arxiv.org/abs/2206.02904},
  location = {Tampere, Finland},
  url = {https://dl.acm.org/doi/10.1145/3569219.3569352},
  doi = {10.48550/ARXIV.2206.02904},



@article{auteursversie_art_photo_final.pdf,
title = {Implicit emotion regulation in the context of viewing artworks: ERP evidence in response to pleasant and unpleasant pictures},
journal = {Brain and Cognition},
volume = {107},
pages = {48-54},
year = {2016},
issn = {0278-2626},
doi = {10.1016/j.bandc.2016.06.003},
author = {Noah N.N. {Van Dongen} and Jan W. {Van Strien} and Katinka Dijkstra},
keywords = {Implicit emotion regulation, Affective pictures, Art context, ERP, Late Positive Potential},
abstract = {Presenting affective pictures as a work of art could change perceivers’ judgment and strength in emotional reactions. Aesthetic theory states that perceivers of art emotionally distance themselves, allowing them to appreciate works of art depicting gruesome events. To examine whether implicit emotion regulation is induced by an art context, we assessed whether presenting pleasant and unpleasant IAPS pictures as either “works of art comprising paintings, digital renderings, and photographs of staged scenes” or “photographs depicting real events” modulated perceivers’ Late Positive Potentials (LPP) and likability ratings. In line with previous research and aesthetic theory, participants evaluated the IAPS pictures as more likable when they were presented as works of art than when they were presented as photographs. Moreover, participants’ late LPP amplitudes (600–900ms post picture onset) in response to the pictures were attenuated in the art context condition. These results provide evidence for an implicit emotion regulation induced by the art context.}
}




@article{18944-Article_Text-22710-1-2-20211004.pdf,
title={Exploring the Music Perception Skills of Crowd Workers},
volume={9}, 
number={1},
journal={Proceedings of the AAAI Conference on Human Computation and Crowdsourcing},
author={Samiotis, Ioannis Petros and Qiu, Sihang and Lofi, Christoph and Yang, Jie and Gadiraju, Ujwal and Bozzon, Alessandro},
year={2021},
month={Oct.},
pages={108-119},
}


@incollection{Cohen2012_Chapter_EvaluationOfCreativeAesthetics.pdf,
author="Cohen, Harold
and Nake, Frieder
and Brown, David C.
and Brown, Paul
and Galanter, Philip
and McCormack, Jon
and d'Inverno, Mark",
editor="McCormack, Jon
and d'Inverno, Mark",
title="Evaluation of Creative Aesthetics",
bookTitle="Computers and Creativity",
year="2012",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="95--111",
abstract="This chapter is an edited conversation on the topic of computational evaluation of artistic artefacts. The participants were Harold Cohen, Frieder Nake, David Brown, Jon McCormack, Paul Brown and Philip Galanter. It began at the Dagstuhl seminar on computers and creativity, held in Germany in 2009 and continued over a period of several months via email. The participants discuss their views on the prospects for computational evaluation of both the artistic process and the made artefact.",
isbn="978-3-642-31727-9",
doi="10.1007/978-3-642-31727-9\_4",
}


@inproceedings{2506364.2506368.pdf,
author = {Redi, Judith Alice and Ho\ss{}feld, Tobias and Korshunov, Pavel and Mazza, Filippo and Povoa, Isabel and Keimel, Christian},
title = {Crowdsourcing-Based Multimedia Subjective Evaluations: A Case Study on Image Recognizability and Aesthetic Appeal},
year = {2013},
publisher = {Association for Computing Machinery},
address = {New York, NY},
doi = {10.1145/2506364.2506368},
abstract = {Research on Quality of Experience (QoE) heavily relies on subjective evaluations of media. An important aspect of QoE concerns modeling and quantifying the subjective notions of 'beauty' (aesthetic appeal) and 'something well-known' (content recognizability), which are both subject to cultural and social effects. Crowdsourcing, which allows employing people worldwide to perform short and simple tasks via online platforms, can be a great tool for performing subjective studies in a time and cost-effective way. On the other hand, the crowdsourcing environment does not allow for the degree of experimental control which is necessary to guarantee reliable subjective data. To validate the use of crowdsourcing for QoE assessments, in this paper, we evaluate aesthetic appeal and recognizability of images using the Microworkers crowdsourcing platform and compare the outcomes with more conventional evaluations conducted in a controlled lab environment. We find high correlation between crowdsourcing and lab scores for recognizability but not for aesthetic appeal, indicating that crowdsourcing can be used for QoE subjective assessments as long as the workers' tasks are designed with extreme care to avoid misinterpretations.},
booktitle = {Proceedings of the 2nd ACM International Workshop on Crowdsourcing for Multimedia},
pages = {29–34},
numpages = {6},
keywords = {qoe, aesthetics, crowdsourcing, subjective evaluations},
series = {CrowdMM '13}
}
    location = {Barcelona, Spain},
, USA

@article{nihms-538401.pdf,
title={Purposeful Sampling for Qualitative Data Collection and Analysis in Mixed Method Implementation Research},
author={Palinkas, Lawrence A. and
Horwitz, Sarah M. and
Green, Carla A. and
Wisdom, Jennifer P. and
Duan, Naihua and 
Hoagwood, Kimberly},
year={2015},
journal={Administration and policy in mental health},
volume={42},
number={5},
pages={533-544},
doi={10.1007/s10488-013-0528-y},
}



@book{Zylinska_2020_AI-Art.pdf,
title={AI Art: Machine Visions and Warped Dreams},
author={Joanna Zylinska},
year={2020},
publisher={Open Humanities Press},
address={London, UK},
}


@article{nft,
author = {Kugler, Logan},
title = {Non-Fungible Tokens and the Future of Art},
year = {2021},
issue_date = {September 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {64},
number = {9},
issn = {0001-0782},
doi = {10.1145/3474355},
abstract = {A new blockchain-based technology is changing how the art world works, and changing how we think about asset ownership in the process.},
journal = {Commun. ACM},
month = {aug},
pages = {19–20},
numpages = {2}
}


@misc{DALL-E,
  doi = {10.48550/ARXIV.2102.12092},
  url = {https://arxiv.org/abs/2102.12092},
  author = {Ramesh, Aditya and Pavlov, Mikhail and Goh, Gabriel and Gray, Scott and Voss, Chelsea and Radford, Alec and Chen, Mark and Sutskever, Ilya},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Zero-Shot Text-to-Image Generation},
  publisher = {arXiv},
  year = {2021},
  copyright = {Creative Commons Attribution 4.0 International}
}

@misc{latent-diffusion,
      title={High-Resolution Image Synthesis with Latent Diffusion Models}, 
      author={Robin Rombach and Andreas Blattmann and Dominik Lorenz and Patrick Esser and Björn Ommer},
      year={2021},
      eprint={2112.10752},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}


@inproceedings{dc2s2chi2019submission,
  title = {Supporting Creative Work with Crowd Feedback Systems},
  author = {Jonas Oppenlaender and Simo Hosio},
  year = {2019},
  month = {May},
  booktitle = {Proceedings of the {$DC^2S^2$} Workshop on Designing Crowd-powered Creativity Support Systems},
  numpages = {5 pages},
  venue = {Glasgow, Scotland, UK},
  url = {http://arxiv.org/abs/2004.09204},
}
  location = {Glasgow, Scotland, UK},


@article{FoundationModels,
title={On the Opportunities and Risks of Foundation Models},
author={Rishi Bommasani and Drew A. Hudson and Ehsan Adeli and Russ Altman and Simran Arora and Sydney von Arx and Michael S. Bernstein and Jeannette Bohg and Antoine Bosselut and Emma Brunskill and Erik Brynjolfsson and S. Buch and Dallas Card and Rodrigo Castellon and Niladri S. Chatterji and Annie S. Chen and Kathleen A. Creel and Jared Davis and Dora Demszky and Chris Donahue and Moussa Doumbouya and Esin Durmus and Stefano Ermon and John Etchemendy and Kawin Ethayarajh and Li Fei-Fei and Chelsea Finn and Trevor Gale and Lauren E. Gillespie and Karan Goel and Noah D. Goodman and Shelby Grossman and Neel Guha and Tatsunori Hashimoto and Peter Henderson and John Hewitt and Daniel E. Ho and Jenny Hong and Kyle Hsu and Jing Huang and Thomas F. Icard and Saahil Jain and Dan Jurafsky and Pratyusha Kalluri and Siddharth Karamcheti and Geoff Keeling and Fereshte Khani and O. Khattab and Pang Wei Koh and Mark S. Krass and Ranjay Krishna and Rohith Kuditipudi and Ananya Kumar and Faisal Ladhak and Mina Lee and Tony Lee and Jure Leskovec and Isabelle Levent and Xiang Lisa Li and Xuechen Li and Tengyu Ma and Ali Malik and Christopher D. Manning and Suvir P. Mirchandani and Eric Mitchell and Zanele Munyikwa and Suraj Nair and Avanika Narayan and Deepak Narayanan and Benjamin Newman and Allen Nie and Juan Carlos Niebles and Hamed Nilforoshan and J. F. Nyarko and Giray Ogut and Laurel Orr and Isabel Papadimitriou and Joon Sung Park and Chris Piech and Eva Portelance and Christopher Potts and Aditi Raghunathan and Robert Reich and Hongyu Ren and Frieda Rong and Yusuf H. Roohani and Camilo Ruiz and Jack Ryan and Christopher R'e and Dorsa Sadigh and Shiori Sagawa and Keshav Santhanam and Andy Shih and Krishna Parasuram Srinivasan and Alex Tamkin and Rohan Taori and Armin W. Thomas and Florian Tram{\`e}r and Rose E. Wang and William Wang and Bohan Wu and Jiajun Wu and Yuhuai Wu and Sang Michael Xie and Michihiro Yasunaga and Jiaxuan You and Matei A. Zaharia and Michael Zhang and Tianyi Zhang and Xikun Zhang and Yuhui Zhang and Lucia Zheng and Kaitlyn Zhou and Percy Liang},
journal={ArXiv},
year={2021},
url={https://crfm.stanford.edu/assets/report.pdf}
} 



@report{GenZ,
title={GenZ White Paper: Strengthening Human Competences in the Emerging Digital Era},
author={
Haddington, Pentti and
Hirvonen, Noora and
Hosio, Simo and
Kinnula, Marianne and
Malmberg, Jonna and
Seyfi, Siamak and
Simonen, Jaakko and
Ahola, Sara  and
Cortés Orduna, Marta and
Enwald, Heidi and
Haukipuro, Lotta and
Heikkinen, Mervi and
Hermes, Jan and
Huikari, Sanna and
Iivari, Netta and
Järvelä, Sanna and
Kanste, Outi and
Kokkola, Lydia and
Kunnari, Sari and
Zabolotna, Kateryna
},
year={2021},
publisher={University of Oulu},
isbn={978-952-62-3147-1},
}


@inproceedings{luther,
author = {Luther, Kurt and Tolentino, Jari-Lee and Wu, Wei and Pavel, Amy and Bailey, Brian P. and Agrawala, Maneesh and Hartmann, Bj\"{o}rn and Dow, Steven P.},
title = {Structuring, Aggregating, and Evaluating Crowdsourced Design Critique},
year = {2015},
isbn = {9781450329224},
publisher = {Association for Computing Machinery},
address = {New York, NY},
url = {https://doi.org/10.1145/2675133.2675283},
doi = {10.1145/2675133.2675283},
abstract = {Feedback is an important component of the design process, but gaining access to high-quality critique outside a classroom or firm is challenging. We present CrowdCrit, a web-based system that allows designers to receive design critiques from non-expert crowd workers. We evaluated CrowdCrit in three studies focusing on the designer's experience and benefits of the critiques. In the first study, we compared crowd and expert critiques and found evidence that aggregated crowd critique approaches expert critique. In a second study, we found that designers who got crowd feedback perceived that it improved their design process. The third study showed that designers were enthusiastic about crowd critiques and used them to change their designs. We conclude with implications for the design of crowd feedback services.},
booktitle = {Proceedings of the 18th ACM Conference on Computer Supported Cooperative Work \& Social Computing},
pages = {473–485},
numpages = {13},
keywords = {feedback, social computing, critique, crowdsourcing, design},
series = {CSCW '15}
}
    location = {Vancouver, BC, Canada},
, USA

@inproceedings{sheepmarket,
author = {Koblin, Aaron},
title = {The Sheep Market},
year = {2009},
isbn = {9781605588650},
publisher = {Association for Computing Machinery},
address = {New York, NY},
doi = {10.1145/1640233.1640348},
abstract = {The Sheep Market, is a web-based artwork that appropriates Amazon's Mechanical Turk system to implicate thousands of workers in the creation of a massive database of drawings. From one simple request, submitted to the MTurk system as a 'HIT' or Human Intelligence Task, workers create their version of "a sheep facing to the left" using simple drawing tools. The artist responsible for each drawing receives a payment of two cents for their labor.The inspiration for The Sheep Market project stems from the urge to cast a light on the human role of creativity expressed by workers in the system, while explicitly calling attention to the massive and insignificant role each plays as part of a whole.The project will be presented at the Creativity and Cognition Conference as a large projection of each sheep's animated creation process as drawn by the original artist.},
booktitle = {Proceedings of the Seventh ACM Conference on Creativity and Cognition},
pages = {451–452},
numpages = {2},
keywords = {data visualization, crowdsourcing, design, drawing, art, media art},
series = {C\&C '09}
}
    location = {Berkeley, California, USA},
, USA
Michael


@inproceedings{2010-MTurk-CHI.pdf,
author = {Heer, Jeffrey and Bostock, Michael},
title = {Crowdsourcing Graphical Perception: Using Mechanical Turk to Assess Visualization Design},
year = {2010},
isbn = {9781605589299},
publisher = {Association for Computing Machinery},
address = {New York, NY},
doi = {10.1145/1753326.1753357},
abstract = {Understanding perception is critical to effective visualization design. With its low cost and scalability, crowdsourcing presents an attractive option for evaluating the large design space of visualizations; however, it first requires validation. In this paper, we assess the viability of Amazon's Mechanical Turk as a platform for graphical perception experiments. We replicate previous studies of spatial encoding and luminance contrast and compare our results. We also conduct new experiments on rectangular area perception (as in treemaps or cartograms) and on chart size and gridline spacing. Our results demonstrate that crowdsourced perception experiments are viable and contribute new insights for visualization design. Lastly, we report cost and performance data from our experiments and distill recommendations for the design of crowdsourced studies.},
booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
pages = {203–212},
numpages = {10},
keywords = {mechanical turk, evaluation, user study, graphical perception, experimentation, information visualization, crowdsourcing},
series = {CHI '10}
}
    location = {Atlanta, Georgia, USA},
, USA


@inproceedings{00408.pdf,
author = {Oomen, Johan and Aroyo, Lora},
title = {Crowdsourcing in the Cultural Heritage Domain: Opportunities and Challenges},
year = {2011},
isbn = {9781450308243},
publisher = {Association for Computing Machinery},
address = {New York, NY},
doi = {10.1145/2103354.2103373},
abstract = {Galleries, Libraries, Archives and Museums (short: GLAMs) around the globe are beginning to explore the potential of crowdsourcing, i. e. outsourcing specific activities to a community though an open call. In this paper, we propose a typology of these activities, based on an empirical study of a substantial amount of projects initiated by relevant cultural heritage institutions. We use the Digital Content Life Cycle model to study the relation between the different types of crowdsourcing and the core activities of heritage organizations. Finally, we focus on two critical challenges that will define the success of these collaborations between amateurs and professionals: (1) finding sufficient knowledgeable, and loyal users; (2) maintaining a reasonable level of quality. We thus show the path towards a more open, connected and smart cultural heritage: open (the data is open, shared and accessible), connected (the use of linked data allows for interoperable infrastructures, with users and providers getting more and more connected), and smart (the use of knowledge and web technologies allows us to provide interesting data to the right users, in the right context, anytime, anywhere -- both with involved users/consumers and providers). It leads to a future cultural heritage that is open, has intelligent infrastructures and has involved users, consumers and providers.},
booktitle = {Proceedings of the 5th International Conference on Communities and Technologies},
pages = {138–149},
numpages = {12},
keywords = {heritage, tagging, lifecycle model, crowdsourcing, metadata},
series = {C\&T '11}
}
, USA
    location = {Brisbane, Australia},


@Article{10.3390.info12020064.pdf,
AUTHOR = {Kaldeli, Eirini and Menis-Mastromichalakis, Orfeas and Bekiaris, Spyros and Ralli, Maria and Tzouvaras, Vassilis and Stamou, Giorgos},
TITLE = {CrowdHeritage: Crowdsourcing for Improving the Quality of Cultural Heritage Metadata},
JOURNAL = {Information},
VOLUME = {12},
YEAR = {2021},
NUMBER = {2},
ARTICLE-NUMBER = {64},
ISSN = {2078-2489},
ABSTRACT = {The lack of granular and rich descriptive metadata highly affects the discoverability and usability of cultural heritage collections aggregated and served through digital platforms, such as Europeana, thus compromising the user experience. In this context, metadata enrichment services through automated analysis and feature extraction along with crowdsourcing annotation services can offer a great opportunity for improving the metadata quality of digital cultural content in a scalable way, while at the same time engaging different user communities and raising awareness about cultural heritage assets. To address this need, we propose the CrowdHeritage open end-to-end enrichment and crowdsourcing ecosystem, which supports an end-to-end workflow for the improvement of cultural heritage metadata by employing crowdsourcing and by combining machine and human intelligence to serve the particular requirements of the cultural heritage domain. The proposed solution repurposes, extends, and combines in an innovative way general-purpose state-of-the-art AI tools, semantic technologies, and aggregation mechanisms with a novel crowdsourcing platform, so as to support seamless enrichment workflows for improving the quality of CH metadata in a scalable, cost-effective, and amusing way.},
DOI = {10.3390/info12020064},
}
    URL = {https://www.mdpi.com/2078-2489/12/2/64},



@article{3415176.pdf,
author = {Simons, Rachel N. and Gurari, Danna and Fleischmann, Kenneth R.},
title = {"I Hope This Is Helpful": Understanding Crowdworkers' Challenges and Motivations for an Image Description Task},
year = {2020},
issue_date = {October 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY},
volume = {4},
number = {CSCW2},
doi = {10.1145/3415176},
abstract = {AI image captioning challenges encourage broad participation in designing algorithms that automatically create captions for a variety of images and users. To create large datasets necessary for these challenges, researchers typically employ a shared crowdsourcing task design for image captioning. This paper discusses findings from our thematic analysis of 1,064 comments left by Amazon Mechanical Turk workers using this task design to create captions for images taken by people who are blind. Workers discussed difficulties in understanding how to complete this task, provided suggestions of how to improve the task, gave explanations or clarifications about their work, and described why they found this particular task rewarding or interesting. Our analysis provides insights both into this particular genre of task as well as broader considerations for how to employ crowdsourcing to generate large datasets for developing AI algorithms.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = {oct},
articleno = {105},
numpages = {26},
keywords = {accessibility, crowdsourcing, computer vision, artificial intelligence, amazon mechanical turk, image captioning}
}
, USA



@article{Information_Visualization_Evaluation_Using_BORGO_Acc6May2018Epub10Jul2018_GREEN_AAM.pdf,
author = {Borgo, R. and Micallef, L. and Bach, B. and McGee, F. and Lee, B.},
title = {Information Visualization Evaluation Using Crowdsourcing},
journal = {Computer Graphics Forum},
volume = {37},
number = {3},
pages = {573-595},
keywords = {Categories and Subject Descriptors (according to ACM CCS), I.3.3 Computer Graphics: Picture/Image Generation—Line and curve generation},
doi = {10.1111/cgf.13444},
abstract = {Abstract Visualization researchers have been increasingly leveraging crowdsourcing approaches to overcome a number of limitations of controlled laboratory experiments, including small participant sample sizes and narrow demographic backgrounds of study participants. However, as a community, we have little understanding on when, where, and how researchers use crowdsourcing approaches for visualization research. In this paper, we review the use of crowdsourcing for evaluation in visualization research. We analyzed 190 crowdsourcing experiments, reported in 82 papers that were published in major visualization conferences and journals between 2006 and 2017. We tagged each experiment along 36 dimensions that we identified for crowdsourcing experiments. We grouped our dimensions into six important aspects: study design \& procedure, task type, participants, measures \& metrics, quality assurance, and reproducibility. We report on the main findings of our review and discuss challenges and opportunities for improvements in conducting crowdsourcing studies for visualization research.},
year = {2018}
}


@inproceedings{ecaade2020_037.pdf,
title = "A Novel Crowdsourcing-based Approach for Collaborative Architectural Design",
abstract = "This paper provides an overview of ``Architasker'', a large-scale crowdsourcing approach, platform, and method that enables a collaborative professional architectural design process in collaboration with a community of stakeholders. The platform includes communicating complex architectural project requirements; solution space exploration using different micro-tasks like sketching, 2D and 3D CAD; design selection; and design review as an evolutionary process. The architectural crowdsourcing model underlying the platform is contextualized in the state-of-the-art research on creative crowdsourcing methods and is supported by relevant evidence from empirical experiments. Experimental results validate the effectiveness of the method to generate architectural artifacts by harnessing the skills, talents, and experience of architects and the opinions and values of the stakeholders.",
keywords = "Co-Design, Collective Intelligence, Creative Crowdsourcing, Crowdsourcing, Human Computation, Participatory Design",
author = "Jonathan Dortheimer and Eran Neuman and Tova Milo",
year = "2020",
isbn = "9789491207211",
publisher = "Education and research in Computer Aided Architectural Design in Europe",
pages = "155--164",
editor = "Werner, {Liss C.} and Dietmar Koering",
booktitle = "Anthropologic -- Architecture and Fabrication in the cognitive age",
}
note = "Publisher Copyright: {\textcopyright} 2020, Education and research in Computer Aided Architectural Design in Europe. All rights reserved.; null ; Conference date: 16-09-2020 Through 17-09-2020",
series = "Proceedings of the International Conference on Education and Research in Computer Aided Architectural Design in Europe",


@inproceedings{Posts_paper_3.pdf,
author = {Yuan, Alvin and Luther, Kurt and Krause, Markus and Vennix, Sophie Isabel and Dow, Steven P and Hartmann, Bjorn},
title = {Almost an Expert: The Effects of Rubrics and Expertise on Perceived Value of Crowdsourced Design Critiques},
year = {2016},
isbn = {9781450335928},
publisher = {Association for Computing Machinery},
address = {New York, NY},
doi = {10.1145/2818048.2819953},
abstract = {Expert feedback is valuable but hard to obtain for many designers. Online crowds can provide fast and affordable feedback, but workers may lack relevant domain knowledge and experience. Can expert rubrics address this issue and help novices provide expert-level feedback? To evaluate this, we conducted an experiment with a 2x2 factorial design. Student designers received feedback on a visual design from both experts and novices, who produced feedback using either an expert rubric or no rubric. We found that rubrics helped novice workers provide feedback that was rated nearly as valuable as expert feedback. A follow-up analysis on writing style showed that student designers found feedback most helpful when it was emotionally positive and specific, and that a rubric increased the occurrence of these characteristics in feedback. The analysis also found that expertise correlated with longer critiques, but not the other favorable characteristics. An informal evaluation indicates that experts may instead have produced value by providing clearer justifications.},
booktitle = {Proceedings of the 19th ACM Conference on Computer-Supported Cooperative Work \& Social Computing},
pages = {1005–1017},
numpages = {13},
keywords = {crowdsourcing, rubrics, Design, feedback, expertise, critique},
series = {CSCW '16}
}
    location = {San Francisco, California, USA},
, USA

@InProceedings{1606.01621.pdf,
author="Kong, Shu
and Shen, Xiaohui
and Lin, Zhe
and Mech, Radomir
and Fowlkes, Charless",
editor="Leibe, Bastian
and Matas, Jiri
and Sebe, Nicu
and Welling, Max",
title="Photo Aesthetics Ranking Network with Attributes and Content Adaptation",
booktitle="Computer Vision -- ECCV 2016",
year="2016",
publisher="Springer International Publishing",
address="Cham",
pages="662--679",
abstract="Real-world applications could benefit from the ability to automatically generate a fine-grained ranking of photo aesthetics. However, previous methods for image aesthetics analysis have primarily focused on the coarse, binary categorization of images into high- or low-aesthetic categories. In this work, we propose to learn a deep convolutional neural network to rank photo aesthetics in which the relative ranking of photo aesthetics are directly modeled in the loss function. Our model incorporates joint learning of meaningful photographic attributes and image content information which can help regularize the complicated photo aesthetics rating problem.",
isbn="978-3-319-46448-0"
}



@inproceedings{062-iccc20.pdf,
title = "Modalities, Styles and Strategies: An Interaction Framework for Human–Computer Co-Creativity",
author = "Anna Kantosalo and {Thattai Ravikumar}, Prashanth and Kazjon Grace and Tapio Takala",
year = "2020",
month = sep,
day = "7",
language = "English",
pages = "57--64",
editor = "Am{\'i}lcar Cardoso and Penousal Machado and Tony Veale and Cunha, {Jo{\~a}o Miguel}",
booktitle = "Proceedings of the Eleventh International Conference on Computational Creativity",
publisher = "Association for Computational Creativity",
address = "Portugal",
}




@article{McDonald_Reliability_CSCW19.pdf,
author = {McDonald, Nora and Schoenebeck, Sarita and Forte, Andrea},
title = {Reliability and Inter-Rater Reliability in Qualitative Research: Norms and Guidelines for CSCW and HCI Practice},
year = {2019},
issue_date = {November 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY},
volume = {3},
number = {CSCW},
doi = {10.1145/3359174},
abstract = {What does reliability mean for building a grounded theory? What about when writing an auto-ethnography? When is it appropriate to use measures like inter-rater reliability (IRR)? Reliability is a familiar concept in traditional scientific practice, but how, and even whether to establish reliability in qualitative research is an oft-debated question. For researchers in highly interdisciplinary fields like computer-supported cooperative work (CSCW) and human-computer interaction (HCI), the question is particularly complex as collaborators bring diverse epistemologies and training to their research. In this article, we use two approaches to understand reliability in qualitative research. We first investigate and describe local norms in the CSCW and HCI literature, then we combine examples from these findings with guidelines from methods literature to help researchers answer questions like: "should I calculate IRR?" Drawing on a meta-analysis of a representative sample of CSCW and HCI papers from 2016-2018, we find that authors use a variety of approaches to communicate reliability; notably, IRR is rare, occurring in around 1/9 of qualitative papers. We reflect on current practices and propose guidelines for reporting on reliability in qualitative research using IRR as a central example of a form of agreement. The guidelines are designed to generate discussion and orient new CSCW and HCI scholars and reviewers to reliability in qualitative research.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = {nov},
articleno = {72},
numpages = {23},
keywords = {content analysis, interviews, IRR, qualitative methods, inter-rater reliability}
}
, USA

@misc{2105.05233.pdf,
  doi = {10.48550/ARXIV.2105.05233},
  url = {https://arxiv.org/abs/2105.05233},
  author = {Dhariwal, Prafulla and Nichol, Alex},
  keywords = {Machine Learning (cs.LG), Artificial Intelligence (cs.AI), Computer Vision and Pattern Recognition (cs.CV), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Diffusion Models Beat GANs on Image Synthesis},
  publisher = {arXiv},
  year = {2021},
}



@misc{2204.08583.pdf,
  doi = {10.48550/ARXIV.2204.08583},
  url = {https://arxiv.org/abs/2204.08583},
  author = {Crowson, Katherine and Biderman, Stella and Kornis, Daniel and Stander, Dashiell and Hallahan, Eric and Castricato, Louis and Raff, Edward},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {VQGAN-CLIP: Open Domain Image Generation and Editing with Natural Language Guidance},
  publisher = {arXiv},
  year = {2022},
}


@inproceedings{ICCC_2021_paper_50.pdf,
author={Simon Colton and Amy Smith and Sebastian Berns and Ryan Murdock},
title={Generative Search Engines: First Experiments},
booktitle={Proceedings of the 12th International
Conference on Computational Creativity},
series={ICCC '21},
year={2021},
pages={237-246},
}

@misc{travelersguide,
author={Ethan Smith},
title={A Traveler's Guide to the Latent Space},
year={2022},
url={https://sweet-hall-e72.notion.site/A-Traveler-s-Guide-to-the-Latent-Space-85efba7e5e6a40e5bd3cae980f30235f},
}

@InProceedings{Ramirez2022_Chapter_CrowdsourcingSyntacticallyDive.pdf,
author="Ram{\'i}rez, Jorge
and Baez, Marcos
and Berro, Auday
and Benatallah, Boualem
and Casati, Fabio",
editor="Franch, Xavier
and Poels, Geert
and Gailly, Frederik
and Snoeck, Monique",
title="Crowdsourcing Syntactically Diverse Paraphrases with Diversity-Aware Prompts and Workflows",
booktitle="Advanced Information Systems Engineering",
year="2022",
publisher="Springer International Publishing",
address="Cham",
pages="253--269",
isbn="978-3-031-07472-1",
}



@misc{artiststudies,
title={Disco Diffusion 70+ Artist Studies},
author={Harmeet Gabha},
year={2022},
url={https://weirdwonderfulai.art/resources/disco-diffusion-70-plus-artist-studies/},
}


@article{Levenshtein1966a.pdf,
  title={Binary codes capable of correcting deletions, insertions, and reversals},
  author={Vladimir I. Levenshtein},
  journal={Soviet physics. Doklady},
  year={1965},
  volume={10},
  pages={707-710}
}


@techreport{pressmancrowson2022,
author = { John David Pressman and Katherine Crowson and Simulacra Captions Contributors } ,
year = 2022 ,
title = { Simulacra Aesthetic Captions } ,
institution = { Stability AI } ,
type = {} ,
number = { Version 1.0 } ,
note = {\ url { https://github.com/JD-P/simulacra-aesthetic-captions }}
}

@misc{2203.02155.pdf,
  doi = {10.48550/ARXIV.2203.02155},
  url = {https://arxiv.org/abs/2203.02155},
  author = {Ouyang, Long and Wu, Jeff and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll L. and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and Schulman, John and Hilton, Jacob and Kelton, Fraser and Miller, Luke and Simens, Maddie and Askell, Amanda and Welinder, Peter and Christiano, Paul and Leike, Jan and Lowe, Ryan},
  keywords = {Computation and Language (cs.CL), Artificial Intelligence (cs.AI), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Training language models to follow instructions with human feedback},
  publisher = {arXiv},
  year = {2022},
}



@inproceedings{3527927.3532792.pdf,
author = {Qiao, Han and Liu, Vivian and Chilton, Lydia},
title = {Initial Images: Using Image Prompts to Improve Subject Representation in Multimodal AI Generated Art},
year = {2022},
isbn = {9781450393270},
publisher = {Association for Computing Machinery},
address = {New York, NY},
doi = {10.1145/3527927.3532792},
abstract = {Advances in text-to-image generative models have made it easier for people to create art by just prompting models with text. However, creating through text leaves users with limited control over the final composition or the way the subject is represented. A potential solution is to use image prompts alongside text prompts to condition the model. To better understand how and when image prompts can improve subject representation in generations, we conduct an annotation experiment to quantify their effect on generations of abstract, concrete plural, and concrete singular subjects. We find that initial images improved subject representation across all subject types, with the most noticeable improvement in concrete singular subjects. In an analysis of different types of initial images, we find that icons and photos produced high quality generations of different aesthetics. We conclude with design guidelines for how initial images can improve subject representation in AI art.},
booktitle = {Creativity and Cognition},
pages = {15–28},
numpages = {14},
keywords = {text-to-image, computational creativity, prompt engineering, multimodal generative models, AI co-creation, design guidelines},
series = {C\&C '22}
}
    location = {Venice, Italy},
, USA


@misc{zippy,
title={Zippy's Disco Diffusion Cheatsheet v0.3},
author={Chris Allen},
url={https://docs.google.com/document/d/1l8s7uS2dGqjztYSjPpzlmXLjl5PM3IGkRWI3IiCuK7g/edit?usp=sharing},
year={2022},
}


@article{Shrout1979.pdf,
  title={Intraclass correlations: uses in assessing rater reliability},
  author={Patrick E. Shrout and Joseph L. Fleiss},
  journal={Psychological bulletin},
  year={1979},
  volume={86},
  number={2},
  pages={420-428},
}


@article{IdeaPF.pdf,
Author = {Bloom, Nicholas and Jones, Charles I. and Van Reenen, John and Webb, Michael},
Title = {Are Ideas Getting Harder to Find?},
Journal = {American Economic Review},
Volume = {110},
Number = {4},
Year = {2020},
Month = {April},
Pages = {1104-44},
DOI = {10.1257/aer.20180338},
}
    URL = {https://www.aeaweb.org/articles?id=10.1257/aer.20180338},


@article{PDF.pdf,
author={Saunders, Daniel R. and Bex, Peter J. and Woods, Russell L.},
title={Crowdsourcing a Normative Natural Language Dataset: A Comparison of Amazon Mechanical Turk and In-Lab Data Collection},
year={2013},
journal={Journal of Medical Internet Research},
volume={15},
number={5},
numpages={14},
doi={10.2196/jmir.2620},
}
    URL: https://www.jmir.org/2013/5/e100



@InProceedings{Ramírez2022_Chapter_CrowdsourcingSyntacticallyDive.pdf,
author="Ram{\'i}rez, Jorge
and Baez, Marcos
and Berro, Auday
and Benatallah, Boualem
and Casati, Fabio",
editor="Franch, Xavier
and Poels, Geert
and Gailly, Frederik
and Snoeck, Monique",
title="Crowdsourcing Syntactically Diverse Paraphrases with Diversity-Aware Prompts and Workflows",
booktitle="Advanced Information Systems Engineering",
year="2022",
publisher="Springer International Publishing",
address="Cham",
pages="253--269",
abstract="Task-oriented bots (or simply bots) enable humans to perform tasks in natural language. For example, to book a restaurant or check the weather. Crowdsourcing has become a prominent approach to build datasets for training and evaluating task-oriented bots, where the crowd grows an initial seed of utterances through paraphrasing, i.e., reformulating a given seed into semantically equivalent sentences. In this context, the resulting diversity is a relevant dimension of high-quality datasets, as diverse paraphrases capture the many ways users may express an intent. Current techniques, however, are either based on the assumption that crowd-powered paraphrases are naturally diverse or focus only on lexical diversity. In this paper, we address an overlooked aspect of diversity and introduce an approach for guiding the crowdsourcing process towards paraphrases that are syntactically diverse. We introduce a workflow and novel prompts that are informed by syntax patterns to elicit paraphrases avoiding or incorporating desired syntax. Our empirical analysis indicates that our approach yields higher syntactic diversity, syntactic novelty and more uniform pattern distribution than state-of-the-art baselines, albeit incurring on higher task effort.",
isbn="978-3-031-07472-1"
}




@misc{johnnycash,
title={The Johnny Cash Project},
author={Aaron Koblin},
year={2010},
url={http://www.aaronkoblin.com/project/johnny-cash-project/},
}

@inproceedings{1866696.1866720.pdf,
    title = "Evaluation of Commonsense Knowledge with {M}echanical {T}urk",
    author = "Gordon, Jonathan  and
      Van Durme, Benjamin  and
      Schubert, Lenhart",
    booktitle = "Proceedings of the {NAACL} {HLT} 2010 Workshop on Creating Speech and Language Data with {A}mazon{'}s Mechanical Turk",
    month = jun,
    year = "2010",
    address = "Los Angeles",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W10-0724",
    pages = "159--162",
}


@misc{redditplace,
author={Reddit},
publisher={Reddit},
title={/r/place},
url={https://www.reddit.com/r/place/},
year={2017},
}


@inproceedings{cents,
author = {Kawashima, Takashi and Koblin, Aaron},
title = {Ten Thousand Cents},
year = {2008},
isbn = {9781605583747},
publisher = {Association for Computing Machinery},
address = {New York, NY},
doi = {10.1145/1504229.1504239},
abstract = {Ten Thousand Cents is a digital artwork that creates a representation of a US \$100 bill. Using a custom drawing tool, thousands of individuals working in isolation from one another painted tiny parts of the bill with no knowledge of the overall task. Workers were paid one cent each via Amazon's Mechanical Turk distributed labor tool. The total labor cost to create the bill, the artwork created, and the reproductions available for purchase are all \$100. The work is presented as a video piece with all 10,000 parts being drawn simultaneously. The project explores the circumstances we live in, a new and uncharted combination of digital labor markets, "crowdsourcing," "virtual economies," and digital reproduction.},
booktitle = {ACM SIGGRAPH ASIA 2008 Artgallery: Emerging Technologies},
pages = {18},
numpages = {1},
series = {SIGGRAPH Asia '08}
}
    location = {Singapore},
, USA

@inproceedings{Voyant,
author = {Xu, Anbang and Huang, Shih-Wen and Bailey, Brian},
title = {Voyant: Generating Structured Feedback on Visual Designs Using a Crowd of Non-Experts},
year = {2014},
isbn = {9781450325400},
publisher = {Association for Computing Machinery},
address = {New York, NY},
doi = {10.1145/2531602.2531604},
abstract = {Feedback on designs is critical for helping users iterate toward effective solutions. This paper presents Voyant, a novel system giving users access to a non-expert crowd to receive perception-oriented feedback on their designs from a selected audience. Based on a formative study, the system generates the elements seen in a design, the order in which elements are noticed, impressions formed when the design is first viewed, and interpretation of the design relative to guidelines in the domain and the user's stated goals. An evaluation of the system was conducted with users and their designs. Users reported the feedback about impressions and interpretation of their goals was most helpful, though the other feedback types were also valued. Users found the coordinated views in Voyant useful for analyzing relations between the crowd's perception of a design and the visual elements within it. The cost of generating the feedback was considered a reasonable tradeoff for not having to organize critiques or interrupt peers.},
booktitle = {Proceedings of the 17th ACM Conference on Computer Supported Cooperative Work \& Social Computing},
pages = {1433–1444},
numpages = {12},
keywords = {crowdsourcing, design, feedback, critique, creativity},
series = {CSCW '14}
}
    location = {Baltimore, Maryland, USA},
, USA

@inproceedings{Ngoon,
author = {Ngoon, Tricia J. and Fraser, C. Ailie and Weingarten, Ariel S. and Dontcheva, Mira and Klemmer, Scott},
title = {Interactive Guidance Techniques for Improving Creative Feedback},
year = {2018},
isbn = {9781450356206},
publisher = {Association for Computing Machinery},
address = {New York, NY},
doi = {10.1145/3173574.3173629},
abstract = {Good feedback is critical to creativity and learning, yet rare. Many people do not know how to actually provide effective feedback. There is increasing demand for quality feedback -- and thus feedback givers -- in learning and professional settings. This paper contributes empirical evidence that two interactive techniques -- reusable suggestions and adaptive guidance -- can improve feedback on creative work. We present these techniques embodied in the CritiqueKit system to help reviewers give specific, actionable, and justified feedback. Two real-world deployment studies and two controlled experiments with CritiqueKit found that adaptively-presented suggestions improve the quality of feedback from novice reviewers. Reviewers also reported that suggestions and guidance helped them describe their thoughts and reminded them to provide effective feedback.},
booktitle = {Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems},
pages = {1–11},
numpages = {11},
keywords = {creativity, educational technology, critique, feedback},
series = {CHI '18}
}
location = {Montreal QC, Canada},
url = {https://doi.org/10.1145/3173574.3173629},
, USA

@inproceedings{TagATune,
  author    = {Edith L. M. Law and
               Luis von Ahn and
               Roger B. Dannenberg and
               Mike Crawford},
  editor    = {Simon Dixon and
               David Bainbridge and
               Rainer Typke},
  title     = {TagATune: {A} Game for Music and Sound Annotation},
  booktitle = {Proceedings of the 8th International Conference on Music Information Retrieval},
  pages     = {361--364},
  series={{ISMIR} 2007},
  publisher = {Austrian Computer Society},
  year      = {2007},
  timestamp = {Thu, 12 Mar 2020 11:32:52 +0100},
}
  url       = {http://ismir2007.ismir.net/proceedings/ISMIR2007\_p361\_law.pdf},
  Vienna, Austria, September 23-27, 2007

@inproceedings{ESPGame,
author = {von Ahn, Luis and Dabbish, Laura},
title = {Labeling Images with a Computer Game},
year = {2004},
publisher = {Association for Computing Machinery},
address = {New York, NY},
doi = {10.1145/985692.985733},
abstract = {We introduce a new interactive system: a game that is fun and can be used to create valuable output. When people play the game they help determine the contents of images by providing meaningful labels for them. If the game is played as much as popular online games, we estimate that most images on the Web can be labeled in a few months. Having proper labels associated with each image on the Web would allow for more accurate image search, improve the accessibility of sites (by providing descriptions of images to visually impaired individuals), and help users block inappropriate images. Our system makes a significant contribution because of its valuable output and because of the way it addresses the image-labeling problem. Rather than using computer vision techniques, which don't work well enough, we encourage people to do the work by taking advantage of their desire to be entertained.},
booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
pages = {319–326},
numpages = {8},
keywords = {online games, image labeling, World Wide Web, distributed knowledge acquisition},
series = {CHI '04}
}
    location = {Vienna, Austria},
, USA

@article{vonAhn2006GamesWA,
  title={Games with a purpose},
  author={Luis von Ahn},
  journal={Computer},
  year={2006},
  volume={39},
  pages={92-94}
}


@inproceedings{CHI20,
  title = {Creativity on Paid Crowdsourcing Platforms},
  author = {Oppenlaender, Jonas and Milland, Kristy and Visuri, Aku and Ipeirotis, Panos and Hosio, Simo},
  booktitle = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems},
  series = {CHI ’20},
  year = {2020},
  doi = {10.1145/3313831.3376677},
  publisher = {ACM},
  pages = {1–14},
  articleno = {548},
  numpages = {14},
  address = {New York, NY},
}
  location = {Honolulu, HI, USA},
, USA

@inproceedings{guidelines,
author = {Liu, Vivian and Chilton, Lydia B.},
title = {Design Guidelines for Prompt Engineering Text-to-Image Generative Models},
year = {2022},
publisher = {Association for Computing Machinery},
address = {New York, NY},
doi = {10.1145/3491102.3501825},
abstract = { Text-to-image generative models are a new and powerful way to generate visual artwork. However, the open-ended nature of text as interaction is double-edged; while users can input anything and have access to an infinite range of generations, they also must engage in brute-force trial and error with the text prompt when the result quality is poor. We conduct a study exploring what prompt keywords and model hyperparameters can help produce coherent outputs. In particular, we study prompts structured to include subject and style keywords and investigate success and failure modes of these prompts. Our evaluation of 5493 generations over the course of five experiments spans 51 abstract and concrete subjects as well as 51 abstract and figurative styles. From this evaluation, we present design guidelines that can help people produce better outcomes from text-to-image generative models.},
booktitle = {CHI Conference on Human Factors in Computing Systems},
articleno = {384},
numpages = {23},
keywords = {computational creativity, multimodal generative models, AI co-creation, design guidelines, prompt engineering., text-to-image},
series = {CHI '22},
}
location = {New Orleans, LA, USA},
, USA

@inbook{an-introduction-to-the-cambridge-handbook-of-expertise-and-exper.pdf,
place={Cambridge},
series={Cambridge Handbooks in Psychology},
title={An Introduction to The Cambridge Handbook of Expertise and Expert Performance: Its Development, Organization, and Content},
DOI={10.1017/CBO9780511816796.001},
booktitle={The Cambridge Handbook of Expertise and Expert Performance},
publisher={Cambridge University Press},
address={Cambridge, UK},
author={Ericsson, K. Anders},
editor={Ericsson, K. Anders and Charness, Neil and Feltovich, Paul J. and Hoffman, Robert R.Editors},
year={2006},
pages={3–20},
collection={Cambridge Handbooks in Psychology},
}


@inproceedings{3491102.3517434,
author = {Hope, Tom and Tamari, Ronen and Hershcovich, Daniel and Kang, Hyeonsu B. and Chan, Joel and Kittur, Aniket and Shahaf, Dafna},
title = {Scaling Creative Inspiration with Fine-Grained Functional Aspects of Ideas},
year = {2022},
publisher = {Association for Computing Machinery},
address = {New York, NY},
doi = {10.1145/3491102.3517434},
abstract = {Large repositories of products, patents and scientific papers offer an opportunity for building systems that scour millions of ideas and help users discover inspirations. However, idea descriptions are typically in the form of unstructured text, lacking key structure that is required for supporting creative innovation interactions. Prior work has explored idea representations that were either limited in expressivity, required significant manual effort from users, or dependent on curated knowledge bases with poor coverage. We explore a novel representation that automatically breaks up products into fine-grained functional aspects capturing the purposes and mechanisms of ideas, and use it to support important creative innovation interactions: functional search for ideas, and exploration of the design space around a focal problem by viewing related problem perspectives pooled from across many products. In user studies, our approach boosts the quality of creative search and inspirations, substantially outperforming strong baselines by 50-60%. },
booktitle = {CHI Conference on Human Factors in Computing Systems},
articleno = {12},
numpages = {15},
series = {CHI '22}
}
location = {New Orleans, LA, USA},
, USA

@misc{prompt-programming,
  doi = {10.48550/ARXIV.2102.07350},
  url = {https://arxiv.org/abs/2102.07350},
  author = {Reynolds, Laria and McDonell, Kyle},
  keywords = {Computation and Language (cs.CL), Artificial Intelligence (cs.AI), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Prompt Programming for Large Language Models: Beyond the Few-Shot Paradigm},
  publisher = {arXiv},
  year = {2021},
}



@misc{1810.04805.pdf,
  doi = {10.48550/ARXIV.1810.04805},
  url = {https://arxiv.org/abs/1810.04805},
  author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  publisher = {arXiv},
  year = {2018},
}


@ARTICLE{joshi_1.pdf,
  author={Joshi, Dhiraj and Datta, Ritendra and Fedorovskaya, Elena and Luong, Quang-Tuan and Wang, James Z. and Li, Jia and Luo, Jiebo},
  journal={{IEEE} Signal Processing Magazine}, 
  title={Aesthetics and Emotions in Images}, 
  year={2011},
  volume={28},
  number={5},
  pages={94-115},
  doi={10.1109/MSP.2011.941851},
}


@inproceedings{1291233.1291364.pdf,
author = {Datta, Ritendra and Li, Jia and Wang, James Z.},
title = {Learning the Consensus on Visual Quality for Next-Generation Image Management},
year = {2007},
isbn = {9781595937025},
publisher = {Association for Computing Machinery},
address = {New York, NY},
doi = {10.1145/1291233.1291364},
abstract = {While personal and community-based image collections grow by the day, the demand for novel photo management capabilities grows with it. Recent research has shown that it is possible to learn the consensus on visual quality measures such as aesthetics with a moderate degree of success. Here, we seek to push this performance to more realistic levels and use it to (a) help select high-quality pictures from collections, and (b) eliminate low-quality ones, introducing appropriate performance metrics in each case. To achieve this, we propose a sequential arrangement of a weighted linear least squares regressor and a naive Bayes' classifier, applied to a set of visual features previously found useful for quality prediction. Experiments on real-world data for these tasks show promising performance, with significant improvements over a previously proposed SVM-based method.},
booktitle = {Proceedings of the 15th ACM International Conference on Multimedia},
pages = {533–536},
numpages = {4},
series = {MM '07}
}
location = {Augsburg, Germany},
, USA

@INPROCEEDINGS{AVA_A_large-scale_database_for_aesthetic_visual_analysis.pdf,
  author={Murray, Naila and Marchesotti, Luca and Perronnin, Florent},
  booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, 
  title={AVA: A large-scale database for aesthetic visual analysis}, 
  year={2012},
  volume={},
  number={},
  pages={2408-2415},
  doi={10.1109/CVPR.2012.6247954},
}


@inproceedings{crowdpoweredCST,
  author = {Oppenlaender, Jonas},
  title = {Crowd-Powered Creativity Support Systems},
  year = {2020},
  isbn = {9781450379847},
  publisher = {ACM},
  address = {New York, NY},
  doi = {10.1145/3393672.3398646},
  booktitle = {Companion Proceedings of the 12th ACM SIGCHI Symposium on Engineering Interactive Computing Systems},
  articleno = {15},
  numpages = {4},
  series = {EICS ’20 Companion},
}
  location = {Sophia Antipolis, France},
, USA





@InProceedings{CLIP,
  title = 	 {Learning Transferable Visual Models From Natural Language Supervision},
  author =       {Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and Krueger, Gretchen and Sutskever, Ilya},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {8748--8763},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  abstract = 	 {State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on.}
}
  url = 	 {https://proceedings.mlr.press/v139/radford21a.html},



@thesis{out.pdf,
author={McCarthy, Philip M.},
year={2005},
title={An assessment of the range and usefulness of lexical diversity measures and the potential of the measure of textual, lexical diversity (MTLD)
},
publisher={The University of Memphis},
}

@article{MTLD_MA_Wrap,
    author={McCarthy, Philip M. and Jarvis, Scott},
    year={2010},
    title={MTLD, vocd-D, and HD-D: A validation study of sophisticated approaches to lexical diversity assessment},
    journal={Behavior Research Methods},
    volume={42},
    number={2},
    pages={381–392},
    doi={10.3758/brm.42.2.381},
}


@article{TTR,
title={Studies in language behavior 1: A program of research},
author={Johnson, Wendell},
journal={Psychological Monographs},
volume={56},
number={},
pages={1–15},
year={1944},
}


@article{1-s2.0-S1075293520300660-main.pdf,
title = {Investigating minimum text lengths for lexical diversity indices},
journal = {Assessing Writing},
volume = {47},
numpages = {15},
year = {2021},
doi = {10.1016/j.asw.2020.100505},
author = {Fred Zenker and Kristopher Kyle},
keywords = {Lexical diversity, Text length, SLA, Learner corpus research},
}
issn = {1075-2935},



@article{30200474.pdf,
 ISSN = {00104817},
 abstract = {A well-known problem in the domain of quantitative linguistics and stylistics concerns the evaluation of the lexical richness of texts. Since the most obvious measure of lexical richness, the vocabulary size (the number of different word types), depends heavily on the text length (measured in word tokens), a variety of alternative measures has been proposed which are claimed to be independent of the text length. This paper has a threefold aim. Firstly, we have investigated to what extent these alternative measures are truly textual constants. We have observed that in practice all measures vary substantially and systematically with the text length. We also show that in theory, only three of these measures are truly constant or nearly constant. Secondly, we have studied the extent to which these measures tap into different aspects of lexical structure. We have found that there are two main families of constants, one measuring lexical richness and one measuring lexical repetition. Thirdly, we have considered to what extent these measures can be used to investigate questions of textual similarity between and within authors. We propose to carry out such comparisons by means of the empirical trajectories of texts in the plane spanned by the dimensions of lexical richness and lexical repetition, and we provide a statistical technique for constructing confidence intervals around the empirical trajectories of texts. Our results suggest that the trajectories tap into a considerable amount of authorial structure without, however, guaranteeing that spatial separation implies a difference in authorship.},
 author = {Fiona J. Tweedie and R. Harald Baayen},
 journal = {Computers and the Humanities},
 number = {5},
 pages = {323--352},
 publisher = {Springer},
 title = {How Variable May a Constant Be? Measures of Lexical Richness in Perspective},
 urldate = {2022-06-14},
 volume = {32},
 year = {1998}
}




@article{cvpr11_WLuo_XWang_XTang.pdf,
author = {Tang, Xiaoou and Luo, Wei and Wang, Xiaogang},
title = {Content-Based Photo Quality Assessment},
year = {2013},
issue_date = {December 2013},
publisher = {IEEE Press},
volume = {15},
number = {8},
issn = {1520-9210},
doi = {10.1109/TMM.2013.2269899},
abstract = {Automatically assessing photo quality from the perspective of visual aesthetics is of great interest in high-level vision research and has drawn much attention in recent years. In this paper, we propose content-based photo quality assessment using both regional and global features. Under this framework, subject areas, which draw the most attentions of human eyes, are first extracted. Then regional features extracted from both subject areas and background regions are combined with global features to assess photo quality. Since professional photographers adopt different photographic techniques and have different aesthetic criteria in mind when taking different types of photos (e.g., landscape versus portrait), we propose to segment subject areas and extract visual features in different ways according to the variety of photo content. We divide the photos into seven categories based on their visual content and develop a set of new subject area extraction methods and new visual features specially designed for different categories. The effectiveness of this framework is supported by extensive experimental comparisons of existing photo quality assessment approaches as well as our new features on different categories of photos. In addition, we propose an approach of online training an adaptive classifier to combine the proposed features according to the visual content of a test photo without knowing its category. Another contribution of this work is to construct a large and diversified benchmark dataset for the research of photo quality assessment. It includes 17,673 photos with manually labeled ground truth. This new benchmark dataset can be down loaded at http://mmlab.ie.cuhk.edu.hk/CUHKPQ/Dataset.htm.},
journal = {Trans. Multi.},
month = {dec},
pages = {1930–1943},
numpages = {14}
}


@ARTICLE{lu.pdf,
  author={Lu, Xin and Lin, Zhe and Jin, Hailin and Yang, Jianchao and Wang, James. Z.},
  journal={IEEE Transactions on Multimedia}, 
  title={Rating Image Aesthetics Using Deep Learning}, 
  year={2015},
  volume={17},
  number={11},
  pages={2021-2034},
  doi={10.1109/TMM.2015.2477040},
}


@inproceedings{2005.14165.pdf,
 author = {Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel and Wu, Jeffrey and Winter, Clemens and Hesse, Chris and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {1877--1901},
 publisher = {Curran Associates, Inc.},
 title = {Language Models are Few-Shot Learners},
 volume = {33},
 year = {2020}
}
 url = {https://proceedings.neurips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf},




@article{covington2010.pdf,
author = { Michael A.   Covington  and  Joe D.   McFall },
title = {Cutting the Gordian Knot: The Moving-Average Type–Token Ratio (MATTR)},
journal = {Journal of Quantitative Linguistics},
volume = {17},
number = {2},
pages = {94-100},
year  = {2010},
publisher = {Routledge},
doi = {10.1080/09296171003643098},
}




@misc{2209.01390.pdf,
  doi = {10.48550/ARXIV.2209.01390},
  author = {Dang, Hai and Mecke, Lukas and Lehmann, Florian and Goller, Sven and Buschek, Daniel},
  keywords = {Human-Computer Interaction (cs.HC), Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences, H.5.2; I.2.7},
  title = {How to Prompt? Opportunities and Challenges of Zero- and Few-Shot Learning for Human-AI Interaction in Creative Applications of Generative Models},
  publisher = {arXiv},
  year = {2022},
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@misc{2212.07476.pdf,
  doi = {10.48550/ARXIV.2212.07476},
  author = {Deckers, Niklas and Fröbe, Maik and Kiesel, Johannes and Pandolfo, Gianluca and Schröder, Christopher and Stein, Benno and Potthast, Martin},
  
  keywords = {Information Retrieval (cs.IR), Computation and Language (cs.CL), Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {The Infinite Index: Information Retrieval on Generative Text-To-Image Models},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@inproceedings{3491101.3503564.pdf,
author = {Jiang, Ellen and Olson, Kristen and Toh, Edwin and Molina, Alejandra and Donsbach, Aaron and Terry, Michael and Cai, Carrie J},
title = {PromptMaker: Prompt-Based Prototyping with Large Language Models},
year = {2022},
isbn = {9781450391566},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
doi = {10.1145/3491101.3503564},
abstract = {Prototyping is notoriously difficult to do with machine learning (ML), but recent advances in large language models may lower the barriers to people prototyping with ML, through the use of natural language prompts. This case study reports on the real-world experiences of industry professionals (e.g. designers, program managers, front-end developers) prototyping new ML-powered feature ideas via prompt-based prototyping. Through interviews with eleven practitioners during a three-week sprint and a workshop, we find that prompt-based prototyping reduced barriers of access by substantially broadening who can prototype with ML, sped up the prototyping process, and grounded communication between collaborators. Yet, it also introduced new challenges, such as the need to reverse-engineer prompt designs, source example data, and debug and evaluate prompt effectiveness. Taken together, this case study provides important implications that lay the groundwork toward a new future of prototyping with ML.},
booktitle = {Extended Abstracts of the 2022 CHI Conference on Human Factors in Computing Systems},
articleno = {35},
numpages = {8},
location = {New Orleans, LA, USA},
series = {CHI EA '22}
}



@misc{2022.acl-demo.9.pdf,
  doi = {10.48550/ARXIV.2202.01279},
  author = {Bach, Stephen H. and Sanh, Victor and Yong, Zheng-Xin and Webson, Albert and Raffel, Colin and Nayak, Nihal V. and Sharma, Abheesht and Kim, Taewoon and Bari, M Saiful and Fevry, Thibault and Alyafeai, Zaid and Dey, Manan and Santilli, Andrea and Sun, Zhiqing and Ben-David, Srulik and Xu, Canwen and Chhablani, Gunjan and Wang, Han and Fries, Jason Alan and Al-shaibani, Maged S. and Sharma, Shanya and Thakker, Urmish and Almubarak, Khalid and Tang, Xiangru and Radev, Dragomir and Jiang, Mike Tian-Jian and Rush, Alexander M.},
  
  keywords = {Machine Learning (cs.LG), Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {PromptSource: An Integrated Development Environment and Repository for Natural Language Prompts},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}



@inproceedings{2209.11486.pdf,
    title = "{M}eta{P}rompting: Learning to Learn Better Prompts",
    author = "Hou, Yutai  and
      Dong, Hongyuan  and
      Wang, Xinghao  and
      Li, Bohan  and
      Che, Wanxiang",
    booktitle = "Proceedings of the 29th International Conference on Computational Linguistics",
    month = oct,
    year = "2022",
    address = "Gyeongju, Republic of Korea",
    publisher = "International Committee on Computational Linguistics",
    url = "https://aclanthology.org/2022.coling-1.287",
    pages = "3251--3262",
    abstract = "Prompting method is regarded as one of the crucial progress for few-shot nature language processing. Recent research on prompting moves from discrete tokens based {``}hard prompts{''} to continuous {``}soft prompts{''}, which employ learnable vectors as pseudo prompt tokens and achieve better performance. Though showing promising prospects, these soft-prompting methods are observed to rely heavily on good initialization to take effect. Unfortunately, obtaining a perfect initialization for soft prompts requires understanding of inner language models working and elaborate design, which is no easy task and has to restart from scratch for each new task. To remedy this, we propose a generalized soft prompting method called MetaPrompting, which adopts the well-recognized model-agnostic meta-learning algorithm to automatically find better prompt initialization that facilitates fast adaptation to new prompting tasks. Extensive experiments show MetaPrompting tackles soft prompt initialization problem and brings significant improvement on three different datasets (over 6 points improvement in accuracy for 1-shot setting), achieving new state-of-the-art performance.",
}


@inproceedings{2201.12086.pdf,
      title={BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation}, 
      author={Junnan Li and Dongxu Li and Caiming Xiong and Steven Hoi},
      year={2022},
      booktitle={ICML},
}

