%% This is file `sample-acmsmall-submission.tex',
%
%% Commands for TeXCount
%TC:macro \cite [option:text,text]
%TC:macro \citep [option:text,text]
%TC:macro \citet [option:text,text]
%TC:envir table 0 1
%TC:envir table* 0 1
%TC:envir tabular [ignore] word
%TC:envir displaymath 0 word
%TC:envir math 0 word
%TC:envir comment 0 0
%%
%%
%% The first command in your LaTeX source must be the \documentclass command.
\documentclass[acmsmall,screen,authorversion,nonacm]{acmart}%
% \documentclass[acmsmall]{acmart}%
%%% sigchi
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%TC:ignore
\usepackage{afterpage}%
\usepackage{tabularx}%
\usepackage{color}%
\usepackage{soul}%
%     \definecolor{HLColor}{rgb}{1,0.94,0.72}
%     \sethlcolor{HLColor}%
\newcommand{\todo}[1]{{\leavevmode\color{red}#1}}
\newcommand{\ok}[1]{{\leavevmode\color{green}#1}}
\usepackage{multirow}%
\usepackage{makecell}%
\usepackage[export]{adjustbox}% 
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{booktabs}
%%%
% \usepackage{sparklines}
% \def\sparkrectangleh #1 #2 {%
%   \ifdim #1pt > #2pt
%         \errmessage{The left corner #1 of rectangle cannot be lower than #2}%
%   \fi
%   {\pgfmoveto{\pgforigin}\color{sparkrectanglecolor}%
%   \pgfrect[fill]{\pgfxy(#1, 0)}{\pgfxy(#2-#1,1)}}}%
% \setlength\sparklinethickness{0.5pt}
% \def\sparklineheight ex{7pt}
%TC:endignore
%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% \usepackage{setspace}%
% \setstretch{1.5}%

%TC:ignore
\setcopyright{acmcopyright}%
\copyrightyear{2022}%
\acmYear{2022}%
\acmDOI{XXXXXXX.XXXXXXX}%
\acmJournal{JACM}%
\acmVolume{37}%
\acmNumber{4}%
\acmPrice{}%
\acmArticle{}%
\acmMonth{7}%
%TC:endignore

%TC:ignore
% remove acm reference format for review - add again later!
\settopmatter{printacmref=false}
\setcopyright{none} 
%TC:endignore

% \usepackage{lscape}%

%% Submission ID.
%% Use this when submitting an article to a sponsored event. You'll
%% receive a unique submission ID from the organizers
%% of the event, and this ID should be used as the parameter to this command.
%%\acmSubmissionID{123-A56-BU3}

%%
%% For managing citations, it is recommended to use bibliography
%% files in BibTeX format.
%%
%% You can then either use BibTeX with the ACM-Reference-Format style,
%% or BibLaTeX with the acmnumeric or acmauthoryear sytles, that include
%% support for advanced citation of software artefact from the
%% biblatex-software package, also separately available on CTAN.
%%
%% Look at the sample-*-biblatex.tex files for templates showcasing
%% the biblatex styles.
%%

%%
%% The majority of ACM publications use numbered citations and
%% references.  The command \citestyle{authoryear} switches to the
%% "author year" style.
%%
%% If you are preparing content for an event
%% sponsored by ACM SIGGRAPH, you must use the "author year" style of
%% citations and references.
%% Uncommenting
%% the next command will enable that style.
%%\citestyle{acmauthoryear}

% ====================
%TC:ignore
\input{prompts.tex}
%TC:endignore
% ====================

%% end of the preamble, start of the body of the document source.
\begin{document}

%% The "title" command has an optional parameter,
%% allowing the author to define a "short title" to be used in page headers.
%TC:ignore
\title[%
    % Prompting for Text-to-Image Generation: An Investigation into a Novel Creative Skill
]{%
    Prompting AI Art: An Investigation into the Creative Skill of Prompt Engineering
}%
%TC:endignore

%TC:ignore
%% The "author" command and its associated commands are used to define
%% the authors and their affiliations.
%% Of note is the shared affiliation of the first two authors, and the
%% "authornote" and "authornotemark" commands
%% used to denote shared contribution to the research.
\author{Jonas Oppenlaender} % (\href{https://orcid.org/0000-0002-2342-1540}{0000-0002-2342-1540})}
% % \authornote{Both authors contributed equally to this research.}
\email{jonas.x1.oppenlander@jyu.fi}
\orcid{0000-0002-2342-1540}
% \authornotemark[1]
\affiliation{%
  \institution{University of Jyväskylä}
  \city{Jyväskylä}
  \country{Finland}
  \streetaddress{Seminaarinkatu 15}%
  \postcode{40014}
}

\author{Rhema Linder} % (\href{https://orcid.org/0000-0003-4720-6818}{0000-0003-4720-6818})}
\email{rlinder@utk.edu}
\orcid{0000-0003-4720-6818}
\affiliation{%
  \institution{University of Tennessee}
  \city{Knoxville}
  \state{Tennessee}
  \country{United States}
  % \streetaddress{Seminaarinkatu 15}%
  % \postcode{40014}
}

\author{Johanna Silvennoinen} % (\href{https://orcid.org/0000-0002-0763-0297}{0000-0002-0763-0297})}
\email{johanna.silvennoinen@jyu.fi}
\orcid{0000-0002-0763-0297}
\affiliation{%
  \institution{University of Jyväskylä}
  \city{Jyväskylä}
  \country{Finland}
  % \streetaddress{Seminaarinkatu 15}%
  % \postcode{40014}
}
%TC:endignore

\makeatletter
\let\@authorsaddresses\@empty
\makeatother


% \author{Lars Th{\o}rv{\"a}ld}
% \affiliation{%
%   \institution{The Th{\o}rv{\"a}ld Group}
%   \streetaddress{1 Th{\o}rv{\"a}ld Circle}
%   \city{Hekla}
%   \country{Iceland}}
% \email{larst@affiliation.org}

%% By default, the full list of authors will be used in the page
%% headers. Often, this list is too long, and will overlap
%% other information printed in the page headers. This command allows
%% the author to define a more concise list
%% of authors' names for this purpose.
\renewcommand{\shortauthors}{J. Oppenlaender et al.}

%%
%% The abstract is a short summary of the work to be presented in the
%% article.
\begin{abstract}%
Humankind is entering a novel era of creativity --- an era in which anybody can synthesize digital content. The paradigm under which this revolution takes place is prompt-based learning (or in-context learning). This paradigm has found fruitful application in text-to-image generation where it is being used to synthesize digital images from zero-shot text prompts in natural language for the purpose of creating AI art. This activity is referred to as prompt engineering --- the practice of iteratively crafting prompts to generate and improve images.
In this paper, we investigate prompt engineering as a novel creative skill for creating prompt-based art. In three studies with participants recruited from a crowdsourcing platform, we explore whether untrained participants could 1) recognize the quality of prompts, 2) write prompts, and 3) improve their prompts. Our results indicate that participants could assess the quality of prompts and respective images. This ability increased with the participants' experience and interest in art. Participants further were able to write prompts in rich descriptive language. However, even though participants were specifically instructed to generate artworks, participants' prompts were missing the specific vocabulary needed to apply a certain style to the generated images. Our results suggest that prompt engineering is a learned skill that requires expertise and practice. Based on our findings and experience with running our studies with participants recruited from a crowdsourcing platform, we provide ten recommendations for conducting experimental research on text-to-image generation and prompt engineering with a paid crowd. Our studies offer a deeper understanding of prompt engineering thereby opening up avenues for research on the future of prompt engineering. We conclude by speculating on four possible futures of prompt engineering.

% Humankind is entering a novel era of creativity --- an era in which anybody can synthesize digital content.
% The % interactional
% paradigm under which this revolution takes place is prompt-based learning (or in-context learning).
% This paradigm was first applied to adapt large language models (LLMs) to diverse downstream tasks without the need for retraining the language model. % in the field of Natural Language Processing (NLP).
% The paradigm also found fruitful application in text-to-image generation where it is being used to synthesize digital images from zero-shot text prompts in natural language for the purpose of creating AI art.
% % Practitioners of prompting for text-to-image generation sometimes call themselves `prompt engineers.'
% % In this context,
% This activity is referred to as prompt engineering --- the practice of iteratively crafting prompts
% % using specific keywords
% to generate and improve images.
% %
% In this paper, we investigate prompt engineering as a novel creative skill for creating prompt-based art.
% In three studies with participants recruited from a crowdsourcing platform, we explore whether untrained participants could 1) recognize the quality of prompts, 2) write prompts, and 3) improve their prompts.
% % We investigate whether prompt engineering is a skill that participants apply intuitively or whether it is a learned skill that requires expertise.
% % We investigate whether laypeople apply the skill intuitively or whether it is a learned skill that is acquired from experimentation and practice.
% % We investigate this research question in four studies with participants ($n=355$) recruited from a crowdsourcing platform.
% % In three studies, we investigated whether participants could recognize the quality of prompts (Study 1), write prompts (Study 2), and improve prompts (Study 3).
% Our results indicate that
%     % Our first study shed light on whether people have an understanding of what makes a ``good'' prompt.
%     % Interestingly, participants were better able to identify the quality of prompts than images.
%     % This is surprising, given the difficulty of the task of imagining the visual outcome of a prompt.
%     % % The presence of style modifiers may have been an indicator for workers to rate these prompts higher than prompts without prompt modifiers.
%     % %%%ChatGPT:
%     % The presence of style modifiers may have influenced participants to rate these prompts higher than prompts without style modifiers.
%     % %
%     % Our findings nevertheless indicate that
% participants could assess the quality of prompts and respective images. This ability increased with the participants' experience and interest in art.
% %
% Participants further were able to write prompts in rich descriptive language.
% % , but did not apply the expert vocabulary needed for producing images of a certain style of quality.
%     % and some participants had a good grasp of what makes a prompt successful.
% However, even though participants were specifically instructed to generate artworks, participants' prompts were missing the specific vocabulary needed to
% % turn images into artworks of
% apply a certain style to the generated images.
% Our results suggest that prompt engineering is a % non-intuitive
% learned skill that requires expertise and practice.
% Based on our findings and experience with running our studies with participants recruited from a crowdsourcing platform, we provide ten recommendations for conducting experimental research on text-to-image generation and prompt engineering with a paid crowd.
% Our studies offer a deeper understanding of prompt engineering thereby opening up avenues for research on the future of prompt engineering.
% We conclude by speculating on four possible futures of prompt engineering.
\end{abstract}%

%TC:ignore
%% http://dl.acm.org/ccs.cfm
\begin{CCSXML}
<ccs2012>
  <concept>
      <concept_id>10010405.10010469.10010470</concept_id>
      <concept_desc>Applied computing~Fine arts</concept_desc>
      <concept_significance>300</concept_significance>
  </concept>
  <concept>
      <concept_id>10003120.10003121.10003124</concept_id>
      <concept_desc>Human-centered computing~Interaction paradigms</concept_desc>
      <concept_significance>500</concept_significance>
  </concept>
</ccs2012>
\end{CCSXML}
\ccsdesc[300]{Applied computing~Fine arts}
\ccsdesc[500]{Human-centered computing~Interaction paradigms}
%
%% Keywords. The author(s) should pick words that accurately describe
%% the work being presented. Separate the keywords with commas.
\keywords{prompt engineering, prompting, text-to-image generation, AI art, creativity}%
%TC:endignore
%
\maketitle%
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% Front page

\begin{comment}

\noindent
\textbf{Anonymous Authors} 


\noindent
\textbf{Jonas Oppenlaender} (\href{https://orcid.org/0000-0002-2342-1540}{0000-0002-2342-1540})
\\
% \authornote{Both authors contributed equally to this research.}
% jonas.x1.oppenlander@jyu.fi
joppenlu@jyu.fi
\\
% \authornotemark[1]
  \institution{University of Jyväskylä}
  \city{Jyväskylä}
  \country{Finland}
  % \streetaddress{Seminaarinkatu 15}%
  % \postcode{40014}
\\


\noindent
\textbf{Rhema Linder} (\href{https://orcid.org/0000-0003-4720-6818}{0000-0003-4720-6818})
\\
rlinder@utk.edu
\\
  \institution{University of Tennessee}
  \city{Knoxville}
  \state{Tennessee}
  \country{United States}
  % \streetaddress{Seminaarinkatu 15}%
  % \postcode{40014}
\\

\noindent
\textbf{Johanna Silvennoinen} (\href{https://orcid.org/0000-0002-0763-0297}{0000-0002-0763-0297})
\\
johanna.silvennoinen@jyu.fi
\\
  \institution{University of Jyväskylä}
  \city{Jyväskylä}
  \country{Finland}
  % \streetaddress{Seminaarinkatu 15}%
  % \postcode{40014}


\vspace{3\baselineskip}
\noindent
\textbf{Corresponding author:}\\
Jonas Oppenlaender\\
joppenlu@jyu.fi\\[.2\baselineskip]
Seminaarinkatu 15, 40014 Jyväskylä, Finland


% \vspace{3\baselineskip}
% \noindent
% Colors should be used for any figures in print.

% \vspace{2\baselineskip}
% \noindent
% Word count: $\approx$10,648 words

%%%

\newpage%

% ====================
\section*{Abstract}%
% ====================
%
Humankind is entering a novel era of creativity --- an era in which anybody can synthesize digital content.
The % interactional
paradigm under which this revolution takes place is prompt-based learning (or in-context learning).
This paradigm was first applied to adapt large language models (LLMs) to diverse downstream tasks without the need for retraining the language model. % in the field of Natural Language Processing (NLP).
The paradigm also found fruitful application in text-to-image generation where it is being used to synthesize digital images from zero-shot text prompts in natural language for the purpose of creating AI art.
% Practitioners of prompting for text-to-image generation sometimes call themselves `prompt engineers.'
% In this context,
This activity is referred to as prompt engineering --- the practice of iteratively crafting prompts
% using specific keywords
to generate and improve images.
%
In this paper, we investigate prompt engineering as a novel creative skill for creating prompt-based art.
In three studies with participants recruited from a crowdsourcing platform, we explore whether untrained participants could 1) recognize the quality of prompts, 2) write prompts, and 3) improve their prompts.
% We investigate whether prompt engineering is a skill that participants apply intuitively or whether it is a learned skill that requires expertise.
% We investigate whether laypeople apply the skill intuitively or whether it is a learned skill that is acquired from experimentation and practice.
% We investigate this research question in four studies with participants ($n=355$) recruited from a crowdsourcing platform.
% In three studies, we investigated whether participants could recognize the quality of prompts (Study 1), write prompts (Study 2), and improve prompts (Study 3).
Our results indicate that
    % Our first study shed light on whether people have an understanding of what makes a ``good'' prompt.
    % Interestingly, participants were better able to identify the quality of prompts than images.
    % This is surprising, given the difficulty of the task of imagining the visual outcome of a prompt.
    % % The presence of style modifiers may have been an indicator for workers to rate these prompts higher than prompts without prompt modifiers.
    % %%%ChatGPT:
    % The presence of style modifiers may have influenced participants to rate these prompts higher than prompts without style modifiers.
    % %
    % Our findings nevertheless indicate that
participants could assess the quality of prompts and respective images. This ability increased with the participants' experience and interest in art.
%
Participants further were able to write prompts in rich descriptive language.
% , but did not apply the expert vocabulary needed for producing images of a certain style of quality.
    % and some participants had a good grasp of what makes a prompt successful.
However, even though participants were specifically instructed to generate artworks, participants' prompts were missing the specific vocabulary needed to
% turn images into artworks of
apply a certain style to the generated images.
Our results suggest that prompt engineering is a % non-intuitive
learned skill that requires expertise and practice.
Based on our findings and experience with running our studies with participants recruited from a crowdsourcing platform, we provide ten recommendations for conducting experimental research on text-to-image generation and prompt engineering with a paid crowd.
Our studies offer a deeper understanding of prompt engineering thereby opening up avenues for research on the future of prompt engineering.
We conclude by speculating on four possible futures of prompt engineering.
% and how crowd workers would be affected.
% In this paper, we present an exploration of the potential for crowd workers to contribute to this novel creative economy.
% In four studies with 355 crowd workers, we explore workers' knowledge of Fine Art, understanding of prompt quality, and practical ability to write and improve prompts for text-to-image systems with a specific focus on generating digital artworks.
% Our results indicate that crowd workers can write prompts in rich descriptive language and some workers have a good grasp of what makes a prompt successful.
% We also investigate whether prompt engineering is a skill that we humans apply intuitively or whether it is a learned skill that requires expertise. We conclude by discussing four possible futures of prompt engineering and how crowd workers would be affected.
%
\\[2\baselineskip]
\textbf{Keywords:} prompt engineering, prompting, text-to-image generation, AI art, creativity
\newpage%
\end{comment}
%
% ====================
\section{Introduction}%
% ====================
%
% Prompt engineering (% also called prompt programming \cite{prompt-programming}
% or prompting for short)
% refers to the % human-centered
% practice and skill of phrasing input prompts in natural language for % large-scale
% generative models.
%%% Inspired by ChatGPT:
% Prompt engineering is the practice of creating input prompts in natural language for generative models.
% Prompt engineering is interactive and iterative~--
% a dialogue between humans and artificial intelligence (AI) in an act of co-creation.
% explain prompt
% prompt engineering is bound to become a prominent buzzword when it comes to the effective interaction of humans with AI.
% The aim of this practice is to coax the intended results out of the deep learning model's latent space.
% With the rise of deep learning and the growing ubiquity of generative models, prompt engineering is an important research area as an interface for using AI.

% Visual art has emerged as a strong application area of prompt engineering.
% State-of-the-art image generation systems -- such as OpenAI's DALL-E~2~\cite{DALLE2}, Midjourney \cite{midjourney}, and Google's Imagen~\cite{2205.11487.pdf}~-- have been trained on Web-scale multi-modal data (i.e., text and images).
% These systems are able to synthesize images with high fidelity and in a vast variety of different artistic styles~\cite{guidelines,artiststudies} based on textual inputs % written in natural language
% (called ``prompts'').
% Given a short text as input (``prompt'') in natural language, these systems are able to produce digital images and artworks of stunning quality.
% Practitioners of text-to-image generation use prompt engineering to polish their digital artworks.
% For instance, autoregressive language models, such as GPT-3 \cite{2005.14165.pdf}, are typically given a limited set of examples as context for the system to generate relevant text as output.
% Following this few-shot paradigm,
% Certain keywords and phrases in prompts have emerged from within the community of practitioners. These ``prompt modifiers'' have been found to make the model produce outputs of higher quality or in a certain style~\cite{guidelines,artiststudies,modifiers}.
% While a short textual prompt may already lead to stunning results with state-of-the-art text-to-image systems, it is the enrichment of prompts with prompt modifiers that enables practitioners to realize the full potential of the text-to-image generative system~\cite{modifiers,aiartcreativity}.
% The conscious and skilled application of prompt modifiers 
%     % and the hunch about the training data
% distinguishes expert practitioners from novices.
%     % prompt engineers.

We are entering an era in which anybody can generate digital images from text --- a democratization of art and creative production.
In this novel creative era, humans work on ``prompt-based engineering'' within a human-computer co-creative framework \cite{062-iccc20.pdf}.
% Within this unfolding digital revolution, emerging digital technologies will co-evolve with humans.
% This requires the renewal of human capabilities and competences.
%%% ChatGPT (one sentence)
Emerging digital technologies will co-evolve with humans in this digital revolution, which requires the renewal of human capabilities and competences \cite{GenZ}.
One increasingly important human skill is ``prompting'' due to it providing an intuitive interface to AI.
%%% ChatGPT:
Prompting (or ``prompt engineering'') is the skill and practice of writing inputs (``prompts'')
% in natural language
for generative models \cite{guidelines,aiartcreativity}.
Prompt engineering is iterative and interactive~--- a dialogue between humans and artificial intelligence (AI) in an act of co-creation.
As % deep learning and
generative models become more widespread, prompt engineering has become an important research area on how humans interface with AI \cite{prompt-programming,2209.01390.pdf,2212.07476.pdf,2022.acl-demo.9.pdf,3491101.3503564.pdf,guidelines,2209.11486.pdf}.

One area where prompt engineering has been particularly useful is the field of digital visual art. State-of-the-art image generation systems, such as OpenAI's DALL-E \cite{DALLE2}, Midjourney \cite{midjourney}, and Google's Imagen~\cite{2205.11487.pdf}, have been trained on large collections of text and images collected from the World Wide Web. These systems can synthesize high-quality images in a wide range of artistic styles \cite{guidelines,artiststudies} from textual input prompts.
Practitioners of text-to-image generation often use prompt engineering to improve the quality of their digital artworks \cite{guidelines}. Within the community of practitioners, certain keywords and phrases have been identified that act as ``prompt modifiers'' \cite{modifiers} \cite{modifiers}. These keywords can, if included in a prompt, improve the quality of the generative model's output or make images appear in a specific artistic style \cite{guidelines,artiststudies,aiartcreativity}.
While a short prompt may already produce impressive results with these generative systems, the use of prompt modifiers can help practitioners unlock the systems' full potential \cite{modifiers,aiartcreativity,2303.04587.pdf}. The skillful application of prompt modifiers can distinguish expert practitioners of text-to-image generation from novices.





%%% MOTIVATION FOR STUDY
% Text-to-image generation is a novel application area of prompt engineering.
% There is a strong and growing community of users who actively use text-to-image systems to produce digital imagery and artwork.
% Several experiments successfully involved a crowd of people in text-to-image generation. Midjourney, for instance, is a community in which users collaboratively write prompts for generating digital artworks in Discord-based chat rooms~\cite{midjourney}.
% Collected with a similar chat-based technology, \citeauthor{pressmancrowson2022} recently presented Simulacra Aesthetic Captions, a dataset of crowdsourced prompts and imagery~\cite{pressmancrowson2022}.
% Contributions involving a human crowd could enable research into human aesthetic preferences and judgments as well as
% computational assessment of aesthetics, but also training machine learning models to generate prompts or detecting key phrases in prompts, and better aligning AI with human intent and values, for instance with the aim of guiding and training AI to follow human instructions~\cite{pressmancrowson2022,2203.02155.pdf}.

%%%%%
%%%%% MOTIVATE THE STUDY
%%%%%

Whether prompt engineering is an intuitive skill or whether there is a learning curve to it has, so far, not been investigated.
There are a number of reasons why such an investigation is important.
%
A look at StableDiffusion's Discord channel\footnote{https://discord.com/channels/1002292111942635562/} shows preliminary evidence that some prompts and keywords combinations % --- and especially some popular combinations of prompt modifiers ---
circulating in the community of practitioners do not seem intuitive to novice users.
    Such keywords include, for instance, the modifier \textit{``by Greg Rutkowski''} \cite{modifiers} or other popular modifiers, such as % \textit{``highly detailed,''} \textit{intricate,''} 
    \textit{``smooth,''} \textit{``elegant,''} \textit{``luxury,''} \textit{``octane render,''} and \textit{``artstation''} to boost the quality of an image \cite{modifiers}.
    These modifiers are intuitively applied by practitioners in the AI art community, but may confront novices with challenges of understanding their effect on % prompt and
    the resulting image.
A further source for unintuitiveness of the current practice of prompt engineering is a misalignment between the written human prompt and the way in which the text-to-image models interpret the prompt.
Compared to how we humans understand a prompt and its constituents, the text-to-image model may attach very different meanings to some keywords in the prompt.
The CLIP Interrogator\footnote{https://github.com/pharmapsychotic/clip-interrogator} % and BLIP \cite{2201.12086.pdf} tools,
image captioning tool gives us a glimpse into what a text-to-image model ``sees'' in an image. % can identify objects and styles in images and outputs them as image caption. % the text-to-image system recognizes in a given image (a process called image captioning).
Using this tool on any given image will result in unintuitive keyword combinations that a human user would likely never have chosen.
Further confounding the problem is that keywords in prompts can affect both the subject and style of a generated image.

Whether prompt engineering is a skill that humans apply intuitively or whether it is a learned skill is important not only for the field of AI art, but also for research on human-AI interaction and the future of work in general.
Today, many images are shared on social media, often with stunning results.
If what we see on social media is the result of the application of prompt engineering by experts, then the generative content that we encounter on social media could be skewed by a small group of highly skilled practitioners.
From a systemic perspective, we run the risk of assigning too much importance to prompting as a method for interacting with generative models if prompt engineering is an intuitive skill \cite{GenAI}.
On the other hand, if prompt engineering is a learned skill that requires expertise and training, this could give rise to novel creative professions with implications for the future of work.



%%% CLIP Interrogator result:
%%% a man wearing glasses and a blue shirt, by Paul Kelpe, discord profile picture, economist, without eyebrows, nika maisuradze


%%% RESULTS SUMMARY

% In a pilot study, we first investigated whether participants had the theoretical knowledge to generate digital artworks with prompting.
% We find that participants had an excellent knowledge of Fine Art which could be applied in prompting for generating artworks.

% We then investigated whether participants could tell high quality prompts from low quality prompts.




% The above crowdsourced experiments and datasets involve humans who are intrinsically motivated to use text-to-image systems.
% It is not clear if % and to what extent
% a % extrinsically motivated
% paid crowd could contribute to text-to-image generation.
% This contribution could, for instance, consist of crowd workers writing, assessing, annotating, or improving prompts, or --- more generally --- participating in ``distributed prompt engineering'' in crowd-powered creativity support tools~\cite{crowdpoweredCST}.


%%% METHOD

In this paper, we explore the creative skill of prompt engineering in three studies with a total of 227~participants recruited from Amazon Mechanical Turk (MTurk), a popular microtask crowdsourcing platform.
The studies are summarized in \autoref{tab:studiesoverview}.
% We explore the potential of an extrinsically motivated crowd to participate in prompt engineering with specific focus on AI generated art.
% In four extensive studies with a total of 355 crowd workers recruited from % Amazon Mechanical Turk (MTurk), a popular microtask crowdsourcing platform,
% we explore
% potential of an extrinsically motivated crowd to contribute to prompt engineering in the context of AI generated art. % and its novel creative economy.
% We explore the expertise and potential of crowd workers to contribute to the novel creative economy.
%
% Our underlying research question is
% whether crowd workers have the expertise to write effective prompts with a particular focus on creating digital artworks from textual input prompts.
%
% We explore the crowd workers' expertise in four online studies.

% Expertise is defined as ``the characteristics, skills, and knowledge that distinguish experts from novices and less experienced people'' \cite{an-introduction-to-the-cambridge-handbook-of-expertise-and-exper.pdf}.
% An overview of the three studies is given in \autoref{tab:studiesoverview}.



%
%TC:ignore
\begin{table*}[htb]%
\caption{Overview of the three studies.}%
\label{tab:studiesoverview}%
\small%
\begin{tabularx}{\textwidth}{
    c
    c
    X % >{\hsize=.65\hsize}X
    X
}%
\toprule%
    \makecell{\textbf{ Study }  \\ \textbf{No.} }
    &
    \makecell{\textbf{No. of} \\ \textbf{participants}}
    &
    \makecell[l]{\textbf{Study purpose}}
    &
    \makecell[l]{\textbf{Research question}}
\\%
\midrule%
     % Pilot
    %  &
    %      128
    %  &
    %      Explore the Fine Art knowledge of participants on crowdsourcing platform
    %  &
    %      Are crowd workers able to identify popular artists and artwork styles?
% \\
       1
   &
         52
     & 
        Test participants' understanding of prompt quality 
     &
         Are participants able to tell the quality of an image from the textual input prompt?
\\
       2
   &
         125
     &
         Test participants' ability to write prompts
     &
         Can participants effectively write prompts to create digital artworks?
\\
       3
     &
         50
     &
         Test participants' ability to revise their own prompts
     &
         Can participants improve their prompts to generate better digital artworks?
\\%
\bottomrule%
\end{tabularx}%
\end{table*}%
%TC:endignore
%


% Study 1 ===
% In Study~1, we investigate whether crowd workers are able to recognize artists and styles of different artworks.
% % have knowledge of Fine Art.
% In practice, artist names and artwork styles are frequently used by expert practitioners of AI generated art to modify the style or improve the quality of digital artworks generated with text-to-image generation systems \cite{guidelines,modifiers,artiststudies}.
% Knowledge about artists and art styles is, therefore, an important expertise for prompt engineering of AI generated art.
% Having this expertise could enable workers to create digital artworks by writing effective prompts for deep learning based text-to-image systems.

% Expertise is defined as ``the characteristics, skills, and knowledge that distinguish experts from novices and less experienced people'' \cite{an-introduction-to-the-cambridge-handbook-of-expertise-and-exper.pdf}.

In Study~1, we explore participants' understanding of how a text-to-image system produces images of varying quality depending on the phrasing of input prompts.
A feeling of what contributes to the quality of a prompt
% a ``good'' prompt
could enable participants to write prompts and create high-quality images on their own.
% to assess the quality of prompts or participate in distributed prompt engineering in other meaningful ways (e.g., assessing, annotating, or rephrasing prompts).
    % (e.g., by assessing, annotating, or improving prompts in crowd-powered creativity support tools \cite{crowdpoweredCST}).
In our within-subject experiment, participants separately rated the aesthetic appeal of textual prompts and matching images generated with a text-to-image system.
We hypothesize that a high degree of consistency within the participants' two ratings may point toward there being a strong understanding of what makes a ``good'' prompt.
    % and how to phrase input prompts for a text-to-image system.

In Study~2, we invite participants to put their knowledge and expertise into practice by writing three input prompts for a text-to-image system with the specific aim of creating a digital artwork. % in a creative crowdsourcing task.
We analyze participants' use of descriptive language and the use of prompt modifiers that could influence the quality and style of the resulting artworks.
% We thematically analyze the crowdsourced prompts with specific focus on whether the prompts contain different types of prompt modifiers.
%
In Study~3, we then invite the same participants who participated in the previous study to review the images generated from their own prompts. Each participant improved the prompts with a specific task of creating an artwork of high visual quality.
With this study, we investigate whether expertise in writing prompts emerges intuitively or whether it is an expert skill, learned through iteration and practice. Our hypothesis is that if prompt engineering is a learned skill, participants will not be able to significantly improve their images due to few interactions with the text-to-image system within our studies.
% If the latter is the case, crowd workers would potentially risk of being excluded from creative production if a new creative economy based on prompt engineering came to pass.


%%% KEY RESULTS

% We report on two subsequent studies which tested whether participants would apply their theoretical knowledge in practice.

We find that while participants were able to describe artworks in rich descriptive language, almost none of the participants used specific keywords to adapt the style of their artworks or modify the images in other ways.
Moreover, participants were not able to significantly improve the quality of the artworks in the follow-up study.
This points to prompt engineering being a non-intuitive skill that laypeople first need to learn before it can be applied in meaningful ways.

Due to our decision to recruit crowd workers as participants, our paper is the first to provide insights on how well paid crowd workers perform in experiments on prompt engineering and text-to-image generation. % invite a paid crowd to participate in prompt engineering with a specific focus on synthesis of digital images and artworks.
Based on our findings and experience, we provide recommendations for conducting experiments on text-to-image generation and prompt engineering with an extrinsically motivated crowd. % in Section~\ref{sec:implications}.
We conclude by speculating on four potential futures for prompt engineering. % in Section~\ref{sec:creativeeconomy}.
% and how crowd workers would be affected in each of these four scenarios.


% The remainder of the paper is structured as follows.
% We provide a brief introduction to image generation with deep learning and review related work % on the artistic abilities of crowd workers
% in Section \ref{sec:related-work}.
% Sections~\ref{sec:study1}--\ref{sec:study4} present the results of our four studies.
% In Section \ref{sec:discussion}, we reflect on the outcomes of our studies and broaden our discussion to speculate on the future of prompt engineering for AI art and provide topics for future research.
% We conclude in Section \ref{sec:conclusion}.%



% ====================
\section{Related Work}%
\label{sec:related-work}%
% ====================
%
%
% ====================
\subsection{Text-to-Image Generation with Deep Learning}%
\label{sec:background}%
% ====================
%
% Interest in text-to-image synthesis with deep learning has exploded since early 2021 when OpenAI published the results of DALL-E \cite{DALL-E} and the weights of the CLIP model \cite{CLIP}. CLIP is a multi-modal model that has been trained on more than 400 million text and image pairings from the World Wide Web. When used as discriminator network in a Generative Adversarial Network (GAN) architecture, CLIP ``guides'' the generator network to produce images of high fidelity \cite{2204.08583.pdf}. % that match the concepts encoded in its latent space.
% Different approaches and architectures for image generation with deep learning have since been developed, such as diffusion models \cite{2105.05233.pdf}.
% Common to these approaches is that they employ machine learning models trained with contrastive language-image techniques with training data scraped from the World Wide Web.
% % Latent Diffusion \cite{latent-diffusion}.
% % The two main architectures today are GAN-based architectures and diffusion-based approaches, such as Latent Diffusion \cite{latent-diffusion}. Both architectures are still under active development. For instance, Google recently published research on Parti, an autoregressive text-to-image model \cite{Parti}.
% % Common to all recent approaches is that they are
% The systems are text-conditional: They use text in natural language as input for the synthesis of images.
% This textual input is often referred to as a prompt.
% The prompt describes the image to the text-to-image system, which in turn generates one or more images without further input. % and, therefore, is an expression of intent.
%%% ChatGPT
Text-to-image generation is a type of deep learning technology that allows users to create images from text descriptions. This technology has gained significant interest since early 2021, when OpenAI published the results of DALL-E \cite{DALL-E} and the weights of their CLIP model \cite{CLIP}. CLIP is a multi-modal model trained on over 400 million text and image pairs from the Web. The model can be used in text-to-image systems to produce high-fidelity images.
Many approaches and architectures for image generation with deep learning have since been developed, such as diffusion models \cite{2105.05233.pdf}. These approaches typically use machine learning models trained with contrastive language-image techniques using training data scraped from the Web. These systems are text-conditional, meaning they use % natural language
text as input for image synthesis. This input, known as ``prompt,'' describes the image to the system, which then generates one or more images without further input.

% --------------
\subsection{Prompt Engineering}%
\label{sec:promptengineering}%
% --------------
The practice of crafting input prompts
% in natural language
is referred to as prompt engineering (or prompting for short).
% The main aim of prompt engineering is to control the output of generative systems with written input prompts. % in natural language.
% The aim of prompt engineering is to develop results that draw on a deep learning model's latent space.
In this section, we explain the `engineering' character of prompt engineering and highlight the difference to automated approaches of prompt optimization.


\subsubsection{The engineering character of prompting}
The term prompt engineering was originally coined by % the Reddit user
Gwern Branwen in the context of writing textual inputs for OpenAI's GPT-3 language model~\cite{guidelines}.
`Engineering,' in this case, does not refer to a hard science as found in science, technology, engineering, and mathematics (STEM) disciplines.
    Prompt engineering is a term that originates from within the online community of practitioners.
    Practitioners include artists and creative professionals, but also novices, amateurs, and more serious ``Pro-Ams''~\cite{2556288.2557298.pdf} aiming, for instance, to sell their creations as digital art based on non-fungible tokens (NFTs) \cite{nft}.
    Not every member of this online community may identify as a prompt engineer. An alternative self-understanding could be % that of a
    ``promptist'' \cite{promptism} or ``AI artist'' \cite{Zylinska_2020_AI-Art.pdf}.
%
%One aspect of prompt engineering that relates to its engineering character is that prompt engineering involves (often systematic) experimentation with trail and error~\cite{guidelines}. The challenge for the prompt engineer is not only to find the right terms to describe an intended output, but also to anticipate how other people would have described and reacted to the output (due to the training data for the generative system consisting of scraped data from the Web).
%%% ChatGPT:
One aspect of prompt engineering that relates to its engineering character is that it often involves systematic experimentation through trial and error~\cite{guidelines}. The challenge for the prompt engineer is not only to find the right terms to describe an intended output, but also to anticipate how other people would have described and reacted to the output on the World Wide Web.
% This is because the training data for the generative system typically consists of scraped data from the World Wide Web.

\subsubsection{Difference to soft prompting techniques}
Prompt engineering is a language-based practice conducted by humans who write prompts in discrete tokens.
Thus, it differs from so-called ``soft prompting'' approaches that aim to automatically optimize input for machine learning models.
%     For instance, prefix tuning \cite{2021.acl-long.353.pdf} optimizes continuous task-specific vectors to fine-tune pre-trained language models for downstream tasks.
%     While this approach is inspired by prompting for language models, it operates in vector space with ``virtual tokens,'' not on the level of natural language.
%     Likewise, prompt tuning \cite{prompt-tuning} --- a simplification of prefix tuning --- uses backpropagation to learn input prompts for a frozen language model to perform downstream tasks.
%     Prompt optimization can also be performed with reinforcement learning, as demonstrated in \cite{2205.12548.pdf}.
% What sets prompt engineering apart from the above approaches is that it is the manual practice of writing and re-phrasing prompts in natural language. As such, the practice and emerging research area of prompt engineering is closer to human-centered research fields (such as Human-Computer Interaction (HCI), Computer-Supported Cooperative Work (CSCW), Human-AI Interaction (HAI), and conversational AI) than it is to machine learning.%
%%% ChatGPT:
% Prompt engineering is a language-based practice conducted by humans, unlike ``soft prompting'' approaches that aim to automatically optimize input for machine learning models.
For example, prefix tuning \cite{2021.acl-long.353.pdf} optimizes continuous vectors to fine-tune pre-trained language models for downstream tasks, but it operates in vector space with ``virtual tokens'' rather than discrete tokens. Likewise, prompt tuning \cite{prompt-tuning} uses backpropagation to learn input prompts for a frozen language model to perform downstream tasks. Prompt optimization \cite{2205.12548.pdf} uses reinforcement learning to optimize prompts. In contrast to the above, prompt engineering involves manually writing and rephrasing prompts. % in natural language.
As such, prompt engineering is closer to human-centered fields, such as Human-Computer Interaction, Computer-Supported Cooperative Work, Human-AI Interaction, and conversational AI than to the field of machine learning.

% --------------
\subsection{Prompt Engineering for AI Art}%
\label{sec:promptmodifiers}%
% --------------
% Besides text generation with language models, AI generated art has emerged as a strong application ground for the practice of prompt engineering.
% An online community has formed around prompt engineering for
% AI generated art (often referred to as ``AI art'' within the community)~\cite{aiartcreativity}.
% While the self-understanding of members in this community is a question for future research, one could refer to them as art practitioners, prompt engineers. or ``promptists.''
% Promptism has been called ``a movement rooted in the technological advances of the 21st century that holds the potential to completely transform artistic practices'' \cite{promptism}.
% Images and prompts are being shared on Reddit, Twitter, and dedicated websites.
% Within these communities, practices on writing prompts have emerged.
% For instance, prompts often follow certain patterns and a prompt template has been suggested in \cite{travelersguide}:
% \begin{quote}
%   \textit{[Medium] [Subject] [Artist(s)] [Details] [Image repository support]}
% \end{quote}
% A typical prompt could adhere to this template as follows \cite{zippy}:
% \begin{quote}
%     \textit{A beautiful painting of a singular lighthouse, shining its light across a tumultuous sea of blood \ul{by greg rutkowski and thomas kinkade}, \ul{Trending on artstation}.}%
% \end{quote}%

% \textit{Prompt modifiers}
%     % or ``vitamin phrases~\cite{pressmancrowson2022}
% (underlined above) are pieces of text that are added to a textual prompt with the intention of making the resulting image appear in a certain way \cite{guidelines,modifiers}.
% Prompt modifiers are an important technique of prompt engineering for AI art. Prompt modifiers enable the prompt engineer to control the output of the text-to-image system~\cite{aiartcreativity,guidelines}.
% The presence of prompt modifiers in prompts may result in images that are subjectively perceived as being more aesthetic and attractive~\cite{guidelines,aiartcreativity}.
% \citeauthor{modifiers} identified different types of prompt modifiers being used in the AI art community~\cite{modifiers}, the two most common ones being modifiers that affect the style and quality of images.
% The two most common types of prompt modifiers are modifiers that affect the style and quality of images.
% These prompt modifiers consist of specific keywords and key phrases that have been found to modify the style or quality of an image (or both simultaneously).
    % Note that any term could potentially be a modifier that activates parts of the model's latent space resulting in a specific style being generated by the generative model. It is the practices that arose in the communities of practitioners that determine the intention of a specific prompt modifier.
% Modifiers that affect the quality of images have been referred to as \textit{quality boosters} \cite{modifiers}.
% For instance, keywords and key phrases such as \textit{``trending on artstation,'' ``unreal engine,'' ``CGSociety,'' ``8k,'' ``postprocessing,''} and similar key phrases are often used within the community to improve the quality of generated images.
% Style modifiers affect the style of an image and include a great number of open domain keywords and phrases, such as  \textit{``in the style of surrealism,'' ``oil painting,''
% % ``pencil sketch,''
% or ``by James Gurney''}~\cite{modifiers}.%

%%% ChatGPT
``AI art'' \cite{Zylinska_2020_AI-Art.pdf} --- or art generated by artificial intelligence --- has become a popular application for prompt engineering \cite{aiartcreativity}.
An online community has formed, %  around this practice,
sharing images and prompts on various platforms. Within this community, certain practices for writing prompts have emerged. For example, prompts often follow a specific pattern, such as the following template \cite{travelersguide}:
\begin{quote}
  \textit{[Medium] [Subject] [Artist(s)] [Details] [Image repository support]}
\end{quote}
A typical prompt could be \cite{zippy}:
\begin{quote}
    \textit{A beautiful painting of a singular lighthouse, shining its light across a tumultuous sea of blood \ul{by greg rutkowski and thomas kinkade}, \ul{Trending on artstation}.}%
\end{quote}%

Prompt modifiers, such as the underlined words above, are added to a prompt to influence the resulting image in a specific way \cite{guidelines,modifiers,aiartcreativity}. Prompt modifiers are an important technique in prompt engineering for AI art because they allow the prompt engineer to control the output of the text-to-image system. Prompt modifiers may make the resulting images subjectively more aesthetic and attractive~\cite{guidelines,aiartcreativity}.

Different types of prompt modifiers are used in the AI art community \cite{modifiers}, but the two most common types of modifiers affect the style and quality of images. These prompt modifiers consist of specific keywords and phrases that have been found to modify the style or quality of an image (or both).
Modifiers that affect the quality of images are known as quality boosters \cite{modifiers}, and can include phrases such as \textit{``trending on artstation,'' ``unreal engine,'' ``CGSociety,'' ``8k,'' and ``postprocessing.''}
Style modifiers affect the style of an image and can include a wide variety of open domain keywords and phrases, such as \textit{``oil painting,'', % `\textit{`cubism,''}
``in the style of surrealism,''  or ``by James Gurney''}~\cite{modifiers}.

% Research on the human-centered practice of prompt engineering for text-to-image synthesis is still in its infancy and only few papers have been published specifically on this topic in the field of HCI.
% \citeauthor{guidelines} presented a study on subject and style keywords in textual input prompts~\cite{guidelines}.
%     The authors mention that without knowledge of prompt modifiers, users must engage in ``brute-force trial and error.'' The authors present design guidelines aiming to help people produce better outcomes with text-to-image generative models.
% \citeauthor{3527927.3532792.pdf} recently published an experiment on using images as visual input prompts \cite{3527927.3532792.pdf} resulting in design guidelines for improving subject representations in AI art.
% %
% Guidance for novices and practitioners of AI art
% is also available in many community-provided resources, such as Ethan Smith's ``Traveler’s Guide to the Latent Space''~\cite{travelersguide},
% ``Zippy's Disco Diffusion Cheatsheet''~\cite{zippy},
% % EZcharts (a visual reference guide for CLIP-Guided diffusion) \cite{EZCharts},
% and Harmeet Gabha's ``Disco Diffusion Artist Studies''~\cite{artiststudies}.
% The knowledge bound in the above resources and the expertise with prompt modifiers are decisive factors in producing high-quality artifacts.
    % \cite{Cohen2012_Chapter_EvaluationOfCreativeAesthetics.pdf}.
%%% ChatGPT:
Human-centered research on prompt engineering for text-to-image synthesis is still in its early stages, with only a few papers published on the topic in the field of Human-Computer Interaction (HCI). \citeauthor{guidelines}'s study on subject and style keywords in textual input prompts mentioned that without knowledge of prompt modifiers, users must engage in ``brute-force trial and error'' \cite{guidelines}. The authors presented design guidelines to help people produce better results with text-to-image generative models. \citeauthor{3527927.3532792.pdf} conducted an experiment on using images as visual input prompts, resulting in design guidelines for improving subject representations in AI art \cite{3527927.3532792.pdf}. Besides these guidelines, there are also many community-provided resources that offer guidance for novices and practitioners of AI art, such as Ethan Smith's ``Traveler's Guide to the Latent Space''~\cite{travelersguide}, Zippy's ``Disco Diffusion Cheatsheet''~\cite{zippy}, and Harmeet Gabha's ``Disco Diffusion Artist Studies'' \cite{artiststudies}.
%
These resources provide a wealth of information about prompt modifiers  for producing high-quality visual artifacts.


% In the following section, we review studies and experiments that involved a crowd in the production of artworks or investigated the art expertise of crowd workers.


% \input{CROWD-ART-EXPERIENCE}




% ===============
% \input{STUDY1}
% ===============



% ====================
\section{Study 1: Understanding Prompt Engineering}%
\label{sec:study2}%
% ====================
% To explore crowd workers' understanding of prompt engineering, we designed an experiment in which participants were asked to separately rate textual prompts and the respective AI generated images.
% Our hypothesis is that people who have a strong understanding of prompt engineering will exhibit a high consistency between the ratings in the two modalities (images and textual prompts).
% % We hypothesize that a high intra-rater agreement between the two bimodal ratings indicates a strong understanding of prompt engineering.
% In other words, a participant who can predict the aesthetic appeal of an artwork from its textual prompt is likely to have a sense of
%     % the mechanics of
%     how prompt engineering works.
% The study design mimics the knowledge that prompt engineers would apply in practice.
%     An understanding of textual prompts is essential for knowing whether a prompt will perform well or not.
%     Long prompts that include descriptive language and many prompt modifiers will likely produce artworks of higher aesthetic quality than short prompts\footnote{Note, however, that state-of-the-art image generation systems, such as Midjourney version 4, are ``greedy'' in the sense that they aim to turn any input --- no matter how short or non-descriptive --- into aesthetic artworks~\cite{aiartcreativity}. See Section \ref{sec:limitations} for further discussion on this issue.}.


%%% ChatGPT:
We conducted an experiment to study participants' understanding of prompt engineering. Participants were asked to rate both the AI-generated images and the corresponding textual prompts. We hypothesized that participants with a strong understanding of prompt engineering would exhibit a high consistency between the ratings in the two modalities. In other words, if someone can predict the aesthetic appeal of an image from its textual prompt, they likely have a good sense of how prompt engineering works. The study design reflects the knowledge that prompt engineers would use in practice. A good understanding of textual prompts is crucial for predicting how well a prompt will perform. Longer prompts that include descriptive language and many modifiers are likely to produce higher quality artworks than short ones.%
% \footnote{Note, however, that state-of-the-art image generation systems, such as Midjourney version 4, are ``greedy'' in the sense that they aim to turn any input --- no matter how short or non-descriptive --- into aesthetic artworks~\cite{aiartcreativity}. See Section \ref{sec:limitations} for further discussion on this issue.}
%%% ChatGPT
\footnote{Note that some state-of-the-art image generation systems, like Midjourney version 4, are ``greedy'' and will try to turn any input into an aesthetic artwork, even if the prompt is short or non-descriptive. See Section \ref{sec:limitations} for more on this issue.}
%
In the following section, we describe how we selected a set of prompts and images for this study.

% --------------------
\subsection{Method}%
% --------------------

% --------------------
\subsubsection{Research Materials}%
% --------------------
% We curated a set of prompts and images created with Midjourney~\cite{midjourney}.
%     Midjourney is a text-to-image generation system and a community of AI art practitioners. On Midjourney, prompts (and the resulting digital artworks) are shared in chat rooms.
% The first author used purposeful sampling to select 111 images from the corpus of over 5000 images generated by the author on Midjourney.
%     Purposeful sampling is a sampling strategy with aim to identify and select information-rich cases~\cite{nihms-538401.pdf}.
% Our choice to select images from the authors' corpus has several advantages.
%     First, the author has experimented with text-to-image generation since October 2021. The corpus includes images generated with a range of different prompt modifiers commonly used on Midjourney.
%     Second, by selecting from our own creations, we avoid intruding on the intellectual property rights of others.
%     Last, the author can tell their failed attempts from successful ones. This allowed the author to select images with varying level of subjective quality.
%         More specifically, the author selected 59 images that the author judged as being failed attempts and 52 images of high aesthetic quality.

%%% ChatGPT
We curated a set of prompts and images created with Midjourney, a text-to-image generation system and community of AI art practitioners. Using purposeful sampling, we selected 111 images from the corpus of over 3500 images generated by the first author on Midjourney. Our choice to use the author's corpus has several advantages. The corpus includes images with a range of different prompt modifiers commonly used on Midjourney and we avoid intruding on others' intellectual property rights. Further, the author has experience with text-to-image generation and can distinguish failed attempts from successful ones. This allowed us to create a corpus of images with varying levels of subjective quality. Specifically, we selected 59~images judged as failed attempts and 52 images of high aesthetic quality.
We kept the format of four images per prompt, as it resembles the output a prompt engineer would typically receive on Midjourney.

    % While the latter include images that demonstrate expertise in prompt engineering, we acknowledge that aesthetic assessment of images is a highly qualitative task \cite{x}.

% The set of prompts and images was collected from the community's main image generation channel (``general-1'') with purposeful sampling~\cite{nihms-538401.pdf}.
% Purposeful sampling is a sampling strategy with aim to identify and select information-rich cases.
% Our aim with this sampling strategy is to include prompts and images of varying quality and to cover the two most common types of prompt modifiers from the taxonomy by \citet{modifiers}: subject terms and style modifiers.
%     Subject terms denote the subject being generated (e.g., ``a car''). Style modifiers can be added to a prompt to change the style of the resulting image (e.g., ``by Gustav Klimt'').

% Subject terms and style modifiers by far are more common than repetitions and magic terms.
% Prompt weights were not included in our sample, since weights can theoretically be applied to all other types of prompt modifiers.



    % to be representative of common results produced by Midjourney's CLIP-guided diffusion system.
    % at least three instances for each of the four types of prompt modifiers.
% Out of respect for the intellectual property of the creators in the Midjourney community, we do neither report the full prompts nor the respective images in this paper.
% to respect the intellectual property of the members of the Midjourney community.
% Instead, we provide a general description of the images and the prompts.

% The prompts included the following modifiers:
% style terms included:
% quality terms included: ``trending on artstation''
% magic terms included: ``feel the XXX'', ``control the soul'', ``mind the ''

% To assess the aesthetic quality of the curated set of 111 images, we recruited ten raters from two academic institutions.
%     Raters had diverse educational backgrounds (Computer Science, Information Sciences, % Architecture,
%     Human-Computer Interaction, Cognitive Science, Electrical Engineering, and Design)
%     and consisted of
%     2 Professors, 3 PostDocs, 3 PhD students, 1 Master student, and 1 project engineer
%     (5 men, 5 women, age range 24--48 years).
%     Raters completed a simple binary classification task to classify the images by their aesthetic appeal into high and low quality.
% Raters were informed that there was an unequal number of images in the two categories.
%     The unequal number of images was selected on purpose to make participants reflect on each image individually.

%%% ChatGPT
To assess the aesthetic quality of the 111 images in the dataset, we recruited ten raters from two academic institutions. The raters had diverse backgrounds in Computer Science, Information Sciences, Human-Computer Interaction, Cognitive Science, Electrical Engineering, and Design. They consisted of 2 Professors, 3 PostDocs, 3 PhD students, 1 Master student, and 1 project engineer (5 men and 5 women, age range 24–48 years). Raters completed a simple binary classification task to classify the images as high or low quality based on their aesthetic appeal. Raters were informed that there was an unequal number of images in each category.
% This was intentional to make participants consider each image individually.
The inter-rater agreement over all images, as measured by Fleiss' kappa, was fair, $\kappa = 0.34$, $z = 23.9$, $p<0.00$, 95\%~CI [0.31, 0.37].%

% The full set of images is depicted in Appendix \ref{appendix:images}.
    % is available for future research on prompt engineering \textbf{[Dataset-DOI]}.

% We discussed the ratings and selected images for further study. From the set of images with perfect agreement among raters, we selected ten images with high aesthetic appeal and ten with low aesthetic appeal. The final set contains 20 images and respective prompts of varying quality (see \autoref{fig:images} and \autoref{appendix:images}). Note that Midjourney generates images in batches of four images by default. We decided to keep this format in our study as it resembles the output that a prompt engineer would typically receive after running a prompt.
%%%ChatGPT
We discussed the ratings and selected images for further study. From the set of images with perfect agreement among raters, we selected ten high- and ten low-quality images. The final set contains 20 images and respective prompts of varying quality (see \autoref{fig:images} and \autoref{appendix:images}).


% ------
%TC:ignore
\begin{figure*}[h]
	\footnotesize
	\centering
	\begin{tabularx}{\textwidth}{XXXX}
		\includegraphics[width=\linewidth]{figures/midjourney/255acf24-263a-4625-93c1-b5757b30e702x.jpg}
		& 
		\includegraphics[width=\linewidth]{figures/midjourney/22a357e6-2425-42b7-8475-dd05c419ecc9x.jpg}
		&
		\includegraphics[width=\linewidth]{figures/midjourney/66b13a9f-9c0c-4c9f-aa31-d9c0f2aa8324x.jpg}
		&
		\includegraphics[width=\linewidth]{figures/midjourney/8d91ae8d-673b-40e0-abed-4ce965353c16x.jpg}
		\\
		\multicolumn{2}{c}{\ref{fig:images}a) High aesthetic appeal}
		&
		\multicolumn{2}{c}{\ref{fig:images}b) Low aesthetic appeal}
	\end{tabularx}
	\caption{Exemplars of images used in Study 2. The full set of images and prompts is listed in Appendix~\ref{appendix:images}.}
	\Description{Exemplars of images used in Study 2. The full set of images and prompts is listed in Appendix~\ref{appendix:images}.}
	\label{fig:images}
\end{figure*}
%TC:endignore
% ------

% --------------------
\subsubsection{Study Design}%
% --------------------
% We designed a within-subject experiment with two conditions.
% In the first condition, workers rated the 20 AI generated images from our research material (see \autoref{fig:promptrating}a) on the 5-point Absolute Category Rating (ACR) scale \cite{06982326.pdf,06286980.pdf}.
    % The ACR is a commonly used scale from the Quality of Experience (QoE) domain \cite{06065690.pdf}.
% The ACR was shown to produce high agreement and repeatable judgments \cite{06982326.pdf}. Further, the ACR is little impacted by environmental factors, such as lighting, monitor calibration, background noise, language, and country \cite{06286980.pdf}.
% The second condition  contrasted the participant's image ratings with ratings for textual prompts.
% In the second condition, we asked participants to imagine the image resulting from 20 textual prompts used to generated the 20 images from the previous condition. The same rating scale was used as before.
% Note that in this condition, participants were only shown the textual prompts, not the images. This part of the study was carefully designed to make workers reflect about the resulting image, not the prompt itself. To this end, we prefixed each item with ``Imagine the image generated from the prompt: \ldots'' and added another descriptive line as a reminder of the task (see \autoref{fig:promptrating}b).%
%%% ChatGPT
We conducted a within-subject experiment with two conditions.
In the first condition, participants rated 20 AI-generated images (see \autoref{fig:promptrating}a) on a 5-point Absolute Category Rating (ACR) scale \cite{06982326.pdf,06286980.pdf}. The ACR is known to produce reliable judgments \cite{06982326.pdf} that are relatively insensitive to environmental factors, such as lighting, monitor calibration, language, and country \cite{06286980.pdf}.
In the second condition, participants were asked to imagine the images that would result from 20 textual prompts used to generate the images in the previous condition, and rate them using the same scale.
In this condition, participants were only shown the prompts, not the images.
This part of the study was designed to make participants focus on the resulting images, rather than the prompts themselves. To help with this, each prompt was prefaced with ``Imagine the image generated from the prompt: \ldots'' and a descriptive line was added as a reminder of the task (see \autoref{fig:promptrating}b).
%
% -----
%TC:ignore
\begin{figure*}[thb]%
\centering%
\begin{tabularx}{\textwidth}{XX}
    \includegraphics[width=\linewidth]{figures/task/rating-image-reduced.jpg}
&
    \includegraphics[width=\linewidth]{figures/task/rating-prompt.png}
\\
     \multicolumn{1}{c}{a)} 
&
     \multicolumn{1}{c}{b)} 
\\
\end{tabularx}
  \caption{Example of items used in Study 2 for a) rating the performance of prompts in generating aesthetic images and b) aesthetic appeal of AI-generated artworks.}%
  \Description{Example of survey items.}
  \label{fig:promptrating}%
\end{figure*}%
%TC:endignore
% -----

% The instructions were carefully designed to avoid confounding factors.
% For instance, whether an artifact is classified as art can impact its appraisal~\cite{fpsyg-08-01729.pdf}.
% For this reason, we chose neutral wording throughout this study.
% We specifically avoided referring to the images as artworks as this may result in higher positive aesthetic ratings~\cite{10.1163@22134913-00002052.pdf,auteursversie_art_photo_final.pdf,gerger2014.pdf}.
% Note that our aim is not to measure exact ground-truth ratings for aesthetic appeal. Instead, we are interested in the differences in ratings within workers.%
%%% ChatGPT
The instructions were carefully designed to avoid confounding factors. For instance, we used neutral wording and avoided referring to the images as artworks to prevent higher positive aesthetic ratings~\cite{10.1163@22134913-00002052.pdf,auteursversie_art_photo_final.pdf,gerger2014.pdf,fpsyg-08-01729.pdf}. Our aim was not to measure exact ground-truth ratings for aesthetic appeal, but to study differences in ratings within participants.


% After the two conditions, we collected basic demographics including the workers' experience with art
%     (\textit{``Do you have an educational background in the arts?''}),
% measured as a binary variable (yes/no),
% \textit{``Do you practice art yourself?''}, measured on a 5-point Likert-scale (from 1~--~Never to 5~--~Very often), and 
% the self-rated level of experience with text-based image generation, measured on a 5-point Likert scale (1~--~Not at all experienced to 5~--~Extremely experienced).
    % To further explore the experience of workers, we included an optional open-ended item asking the worker to elaborate on how the experience with text-to-image generation was obtained.
%%% ChatGPT
After the two conditions, we collected basic demographics including participants' experience with art, art practice, and text-based image generation.
    % The survey items were (\textit{``Do you have an educational background in the arts?''}), measured as a binary variable (yes/no), \textit{``Do you practice art yourself?''}, measured on a Likert-scale (from 1~--~Never to 5~--~Very often), and the self-rated level of experience with text-based image generation, measured on a Likert scale from 1~--~Not at all experienced to 5~--~Extremely experienced.
We also included an optional open-ended item for participants to elaborate on their experience with text-to-image generation. Experience with art and text-based image generation were measured on 5-point Likert scales, and experience with art was measured as a binary variable.

% Note that aesthetic preference is typically measured on a multiple-item scale (e.g., \cite{06eeaa0949554069958cd9e9e18641e5}). Our 1-item scale may not be valid as a measure for aesthetic pleasure. However, we decided on a 1-item measure because we are only interested in differences between the two ratings of a worker, not the measure itself. % + to keep the survey short.
% "we use the aesthetics score as a measure of quality" \cite{1291233.1291364.pdf}



% --------------------
\subsubsection{Participant Recruitment and Procedure}%
% --------------------
We recruited US-based participants from Amazon Mechanical Turk (MTurk) with a task approval rate greater than 95\% and at least 1000 completed tasks. This combination of qualification criteria is common in crowdsourcing research (e.g., \citet{3491102.3517434}).
The experiment was implemented as a survey task and hosted on Google Forms.
% The format of a survey task was selected over microtasks because we wanted to compare the two ratings within subjects.
Participants were paid US\$1.50 for completing the survey. The price was determined from the average completion times in a small-scale pilot study ($N=9$, US\$1 per task).%


% The questionnaire consisted of four sections.
% Participants first provided consent and were introduced to the study.
% The participants then underwent the two conditions.
% The order of the two conditions was balanced: One half of the crowd workers first rated the textual prompts, then the images --- and vice versa.
%     % The other half underwent the two conditions in reverse order.
% The order of items within all sections was randomized.



% Participants were presented with four sections. The first section included consent and an introduction to the study. The next two sections were the two conditions, presented in a balanced order. Half of the participants first rated the textual prompts, then the images, and the other half vice versa.
% Overall, the task consisted of 31 items (20 ratings of prompts, 20 ratings of images, ten demographic items, and one consistency check).
% To prevent bias, we anonymized the filenames of the images and assigned a random numeric code to each image to make it harder to associate the images with the prompts from the previous survey section.
% As a check for consistency, we duplicated one image and collected a rating for this image (L1, see Appendix~\ref{appendix:a2}).
% Participants who differed in their rating by greater than one category on the ordinal ACR scale were excluded from analysis. We excluded four participants for failing this consistency check and another two participants for having completed the survey without completing the task on Amazon Mechanical Turk.
% The sample included 52 participants.

%%%ChatGPT
The task consisted of 31 items in total, including a consent form, an introduction to the study, 20 ratings of prompts, 20 ratings of images, ten demographic items, and one consistency check.
Participants underwent the two conditions (rating of prompt and rating of images) in balanced order. Half of the participants first rated the prompts, then the images, and the other half vice versa.

To prevent bias, we anonymized the filenames of the images and assigned a random numeric code to each image to make it harder to associate the images with the prompts from the previous survey section. 
As a check for consistency, we duplicated one image and collected a rating for this image (L1, see Appendix~\ref{appendix:a2}). Participants who differed in their rating by greater than one category on the ordinal ACR scale were excluded from analysis.
We excluded four participants for failing this consistency check and another two participants for having completed the survey without completing the task on MTurk.
The final sample included 52 participants.


% --------------------
\subsubsection{Analysis}
% --------------------

% Our analysis compared the distributions of ratings using Kruskal-Wallis rank sum test, followed up by posthoc Dunn tests where applicable.
% This analysis showed whether groups for the Type (i.e., Prompt or visual Artwork) and Quality (High or Low) do indeed have different ratings.
% Following this, we performed a correlation test using Pearson's product-moment correlation, looking at the relationship between the paired scores for each type.
% We hypothesize that crowd workers are able to detect a relationship between the quality of a prompt, in terms of its ability to depict visual art through human imagination, and the quality of the visual artwork generated by the text-to-image system.
% Further, we hypothesize that art experience, as measured by self reports, will have an impact on the consistency of the workers' ratings.
% To test this hypothesis, we created another test for correlation using art experience and average error for each participant.
% To calculate the workers' art experience, we summed the self reported number for the level of the participants' ``practice of art'' (1 to 5) and museum visitation (1 to 5). We did not add experience in text-to-image generation, because only one participant reported strong experience and this worker reported having obtained the experience from taking a survey.
% The average error was calculated per participant by taking the average of the absolute difference between each pair of prompt and artwork rating. For example, if all prompts were rated as a 2 and all artworks as a 5, the average error would be 3.

%%% ChatGPT
We compared the distributions of ratings using a Kruskal-Wallis rank sum test followed by posthoc Dunn tests where applicable.
This analysis showed whether the type (Prompt or Visual Artwork) and quality (High or Low) had different ratings.
Then we performed a correlation test using Pearson's product-moment correlation to look at the relationship between paired scores for each type.
We hypothesize that participants can detect a relationship between the quality of a prompt, in terms of its ability to depict visual art through human imagination, and the quality of the visual artwork generated by the text-to-image system.
Further, we hypothesize that art experience, as measured by self reports, will have an impact on the consistency of the participants' ratings.
To test this hypothesis, we tested the correlation between art experience and average error for each participant.
We calculated the participants' art experience by summing the self-reported ``practice of art'' (1 to 5) and ``museum visitation'' (1 to 5), but not text-to-image generation experience (because only one participant had strong experience, which they obtained from taking an MTurk survey).
Average error per participant was calculated by taking the average absolute difference between each pair of prompt and artwork rating.
For example, if all prompts were rated as 2 and all artworks as 5, the average error would be 3.

% --------------------
% A high correlation for any two methods designed to measure the same property could  thus  in  itself  just  be  a  sign  that  one  has  chosen  a  widespread  sample.
% A  high  correlation  does  not necessarily imply that there is good agreement between the two methods.
% " (Bland–Altman plot - Wikipedia.pdf)
% two-way mixed-effects model is appropriate for testing intra-rater reliability with multiple scores from the same rater

% ICC is for continuous data
% intra-class correlation estimates and their 95\% confidence intervals were calculated using the `icc` R package based on a 2-way mixed-effects model (ICC 3.1).
% \cite{Shrout1979.pdf}
% standard error of measurement,

% %%% ordinal data
% We report percentage of agreement,
% Spearman correlations,
% Cohen's Kappa ($\kappa$), 
% prevalence-adjusted bias-adjusted kappa, and
% weighted kappa
% for the ordinal scale ratings.
% % s12891-018-2290-5.pdf

% For each rater, we calculated both exact agreement and agreement with 1 point difference in between the two ratings.
% Because the task of rating imaginary outcomes of a prompt is difficult, we expect exact agreement to be low, but an understanding of prompt engineering can be measured with the 1-point relaxed agreement.

% Effect sizes are reported following recommendations by \citeauthor{2515245919847202.pdf} \cite{2515245919847202.pdf}.




% --------------------
\subsection{Results}
% --------------------
%
%
% --------------------
\subsubsection{Participants}%  Recruitment and Demographics
% --------------------
Participants ($N=52$) were between 24 and 67 years of age ($M=38.2$~years, $SD=12.98$~years) and included 31~men and 21~women (no non-binary) from diverse educational backgrounds (27~Bachelor's degrees, 10~Master's degrees, among others).
A sizable fraction of the participants (46\%) reported having an educational background in the arts. Twenty-nine participants agreed and nine strongly agreed that they had visited many museums and art galleries  ($M=3.60$, $SD=1.18$). However, participants did not practice art often ($M=3.08$, $SD=1.28$). 
Overall, participants were interested in AI generated art ($M=3.69$, $SD=0.83$), but had little experience with text-to-image generation ($M=2.58$, $SD=1.43$).
Only three participants mentioned having used text-to-image generation (DALL-E mini/Craiyon) before.


\subsubsection{Visual and prompt ratings}
Our study design asked participants to rate both Prompts and Visual artwork. As these crowd workers did not receive special training for prompt engineering or text-based AI art, our goal was to understand the quality of our participants' perceptions. We show the histogram of scores broken into groups for each Art Type (Prompt and Artwork) in \autoref{fig:study2:hist} and \autoref{table:study-2-means} and the Quality (high or low) as described previously. Visually, these show differences across groups, with the distributions of Artworks leaning towards higher quality than prompts.

We used a Kruskal-Wallis rank sum test across these four unique groups, finding a significant difference ($\chi^2=231.4$, $p<10^{15}$, $df=3$).
Following this significant result, we performed post-hoc Dunn's test pairwise across each group with Bonferroni correction for p-values.
Each of these pairs had significant results with a p-value of less than $10^{-4}$, except for Artwork--High versus Prompt--High, in which $p<.004$.
This implies that the median values among all comparisons of groups (i.e. Artwork--High, Artwork--Low, Prompt--High, Prompt--Low) are significantly different from each other.

Participants were able to differentiate images with low visual aesthetic quality from high quality images.
Artwork--High has a higher mean rating ($\mu=3.70$) compared to Prompt--Low ($\mu=3.39$).
Likewise, participants were able to distinguish between high and low quality  by imaging what would be produced based on textual prompts.
Prompt--High has a higher mean rating ($\mu=3.87$) compared to Prompt--Low ($\mu=2.78$).
The overall span between the Artwork High and Low is larger for Prompts ($3.87 - 2.78=1.09$) than for Artworks ($3.70 - 3.39=.31$).
Both High and Low quality Artworks had distributions that favored a rating of 4, while Prompt--Low has a relatively flat distribution across values of 1 to 4 (see \autoref{fig:study2:hist}).%
%%% what does this imply??
%
% A Kruskal-Wallis test showed a significant difference between the ratings for low and high quality images,
% $H(1)=278$, $p<0.00$ with a small effect size of 0.28, 95\% CI [0.24, 1.00].
% The difference was also significant for the imagined outcomes of textual prompts $H(1)=21.1$, $p<0.00$ (\autoref{fig:study2:boxplots}b), but with a negligible effect size of 0.02, 95\% CI [0.01, 1.00].
%
% We conducted a
% posthoc Dunn's test of multiple comparisons
% to examine differences between the high and low quality ratings from workers with and without self-declared experience in art.
%
%
%%%% omnibus for one way Kruskal-Wallis
% 	Kruskal-Wallis rank sum test
%
% data:  Rating by ATQ
% Kruskal-Wallis chi-squared = 231.42, df = 3, p-value < 2.2e-16
%
%
%%%%comparing each group.
%   Kruskal-Wallis rank sum test
%
% data: x and group
% Kruskal-Wallis chi-squared = 231.4204, df = 3, p-value = 0
%
%
%                           Comparison of x by group       %                    
%                                 (No adjustment)          %                      
% Col Mean-|
% Row Mean |   Artwork_   Artwork_   Prompt_H
% ---------+---------------------------------
% Artwork_ |   4.038908
%          |    0.0000*
%          |
% Prompt_H |  -2.666342  -6.705251
%          |    0.0038*    0.0000*
%          |
% Prompt_L |   11.58802   7.549111   14.25436
%          |    0.0000*    0.0000*    0.0000*
%
%
%A Kruskal-Wallis test showed a significant difference between the ratings for low and high quality images,
% $H(1)=278$, $p<0.00$ with a small effect size of 0.28, 95\% CI [0.24, 1.00].
%
% ---------
%TC:ignore
\noindent%
\begin{figure}[h]%
\centering%
\begin{tabular}{cc}%
  \includegraphics[width=.8\linewidth]{figures/study2/study-2-hist.pdf}
% \vspace{10pt}
% \\
\end{tabular}
\caption{Scores by participants ($N=52$) for images (a) and prompts (b) with high aesthetic quality (H1--H10) and low aesthetic quality (L1--L10) in Study 2.}
\Description{}
\label{fig:study2:hist}
\end{figure}
%TC:endignore
% ---------
%
%
%
%TC:ignore
\noindent%
\begin{table*}%
	\caption{Average rating across Art Type and Quality.}%
	\label{table:study-2-means}%
	% \scriptsize
	\small
	% \begin{tabularx}{\textwidth}{l*{4}{|>{\raggedright\arraybackslash}X}}
		\begin{tabular}{llcc}
			\toprule
			\textbf{Art Type}	&	\textbf{Quality}	&	\textbf{Mean}	&	\textbf{Std Dev}
			\\	%\noalign{\hrule height 1pt}
			\midrule
			Artwork	&	High	&	3.70	&	1.04	\\
			Artwork	&	Low	&	3.39	&	1.15	\\
			Prompt	&	High	&	3.87	&	1.07	\\
			Prompt	&	Low	&	2.78	&	1.28 \\%
			\bottomrule%
		\end{tabular}%
\end{table*}%
%TC:endignore
%
%
%
\subsubsection{Connection between visual image and prompt quality}
While in theory, prompts that can help readers conjure (i.e. visualize or imagine) more aesthetically appealing mental images will also generate better Artwork, it is not clear whether this would be the case for untrained crowd workers.
While our participants were not able to directly associate Prompts to Artworks, each Artwork had a matching Prompt.
We used a Person's product-moment correlation test to measure whether ratings for the Prompt and Artwork are correlated.
The test shows a weak ($r=.29$, 95\% CI [.23, .34]) but significant ($p<10^{-15}$) positive correlation between ratings from Artworks and Prompts.
This indicates that when a Prompt is seen as having a higher quality, that it is also more likely that the Artwork will appear as having a high quality.

% 	Pearson's product-moment correlation

% data:  rates[rates$ArtType == "Artwork", ]$Rating and rates[rates$ArtType == "Prompt", ]$Rating
% t = 9.8489, df = 1038, p-value < 2.2e-16
% alternative hypothesis: true correlation is not equal to 0
% 95 percent confidence interval:
%  0.2357419 0.3469640
% sample estimates:
%       cor 
% 0.2923412 

%TC:ignore
\noindent%
\begin{figure}[h]%
\centering%
\begin{tabular}{cc}%
  \includegraphics[width=.7\linewidth]{figures/study2/study-2-error.pdf}
% \vspace{10pt}
% \\
\end{tabular}
\caption{As art experience increases, the average error in differences between the textual Prompt and visual Artwork decreases.}
\Description{}
\label{fig:study2:error}
\end{figure}
%TC:endignore


\subsubsection{Art familiarity and consistency}
We hypothesized that when participants are more familiar with art, they will be more effective at rating art.
One way to measure this is to take the average absolute difference between each Prompt and visual Artwork pair for each participant.
Being able to visualize  and rate Prompts and Artworks with equal values indicates a skill for predicting the quality of a generated visual artwork.
This number represents the error in consistency. For example, if each of the rating pairs (Prompt vs Artwork) had the same rating, this error would be equal to~0. If each were pair was different by 1 for each rating, this error would be equal to 1, because it is an average.
We asked participants to self-report their familiarity with art by asking them to self-rate their familiarity and recent visits to art shows or museums.
Thus, we take the sum of both questions (which each range from 1 to 5) and test for correlation to the participants' rating scores.
The test shows a weak ($r=-.31$, 95\% CI [-.54, -.04]) but significant ($p=.02$) negative correlation between the average error in consistency and art experience.
This indicates that with more reported art experience, the difference in ratings of Prompts and visual Artworks may decrease (see \autoref{fig:study2:error}).



% Pearson's product-moment correlation

% data:  extable$ABSRatingDiff and extable$ArtExperience
% t = -2.3113, df = 50, p-value = 0.02498
% alternative hypothesis: true correlation is not equal to 0
% 95 percent confidence interval:
%  -0.53798040 -0.04129583
% sample estimates:
%       cor 
% -0.3106947 




% % --------------------
% \subsection{Summary and Discussion}
% % --------------------
% \noindent

% In the following study, we explored whether crowd workers could apply their knowledge and understanding of prompt engineering in practice.




% ====================
\section{Study 2: Writing Prompts}%
\label{sec:study3}%
% ====================
% The aim of this study was to explore whether participants could put their knowledge into practice.
% Given that some workers had an excellent knowledge of art styles and artist names and a good understanding of prompt engineering,
% we were interested to see if crowd workers are able to write prompts for image generation systems.
Our aim with this study was to probe laypeople' ability to come up with effective input prompts for text-to-image systems with a specific focus on generating digital artworks.
The presence of style modifiers in an input prompt may indicate an understanding of text-to-image generation and how effective prompts can be formulated.

% --------------------
\subsection{Method}%
% --------------------

% --------------------
\subsubsection{Study Design}%
% --------------------
We designed a creative crowdsourcing task eliciting three textual prompts from each participant.
The task included a short introduction and the following instructions:
\begin{quote}
    \textit{%(Instructions:)
    Imagine an artificial intelligence that turns textual input prompts into digital artworks.
    Your task is to produce three artworks.
    To this end, you will write three different input prompts for the artificial intelligence.
    You should aim to maximize the visual attractiveness and aesthetic qualities of the digital artworks generated from your input prompts.}
\end{quote}

% One worker in our pilot study considered the three input fields as a series of instructions to the AI. Following the pilot study, we made improvements to the task design to alleviate this issue.
% We intentionally did not mention that style modifiers could be used in the prompt and we carefully wrote the instructions not to prime workers with a style (i.e., we instructed the workers to produce `artworks' instead of `paintings').
%%% ChatGPT:
We did not mention that prompt modifiers could be used in the prompt and wrote the instructions to avoid priming participants with a specific style (i.e., we told participants to produce `artworks' rather than `paintings').
%
% Participants were asked to maximize the visual attractiveness and aesthetic quality of their artworks. Our intention with these two criteria was not to precisely measure attractiveness and quality. Instead, we wanted to make participants reflect about the overall visual and aesthetic quality of the resulting images.
% Participants were told that there was no right or wrong answer, but tasks would be rejected if no effort was made to follow instructions.
%%% ChatGPT
Participants were asked to make their artworks as visually attractive and high-quality as possible. Note, however, that we did not aim to precisely measure attractiveness and quality, but wanted participants to think about the overall visual and aesthetic quality of the images. Participants were told that there was no right or wrong answer, but tasks would be rejected if they didn't follow the instructions.

% The study was pilot tested ($N=10$, US\$0.12/task). In this pilot study, we noticed some workers wrote consecutive instructions for the AI. The task instructions and task design were subsequently adjusted to elicit complete prompts.

As additional questions in this task, we asked whether the participant had experience with text-to-image generation and we collected basic demographics. % (gender, age, and native language).
% and subsequently improved due to some workers considering the three input fields as a series of instructions to the AI.
Participants were paid US\$0.16 per completed task. The pricing was estimated from the average task completion times in a pilot study ($N=10$, US\$0.12/task).
%
In this pilot study, we noticed some participants wrote a series of consecutive instructions for the AI. The task design and instructions waswere subsequently adjusted to elicit complete prompts.


% --------------------
\subsubsection{Participant Recruitment}%
% --------------------
We recruited 137 unique participants from Amazon Mechanical Turk using the same qualification criteria as in Study~1.
    % The sample size was estimated with Slovin's formula with a margin of error of 0.05. The estimated population size is based on Difallah's estimate that there are 2000 active workers present on MTurk at any time \cite{p135-difallah.pdf}.
Ten tasks had to be rejected due to clearly no attempt being made to answer the task with relevant information. The ten tasks were republished for other participants. After collecting the data, we manually reviewed the results and removed a further twelve responses from participants who obviously tried to game the task.
% The final sample consists of 125 unique crowd workers.
The final set includes 375~prompts written by 125~unique participants (three prompts per participant).

% --------------------
\subsubsection{Analysis}%
% --------------------
The analysis of the prompts was conducted with mixed methods.
For each prompt, we qualitatively and quantitatively analyzed the prompts, as follows.

\paragraph{Prompt modifiers}%
% whether and how prompt modifiers were being used
We analyzed whether the prompts contained certain keywords and phrases commonly used in the AI art community to modify the style and quality of AI generated images \cite{modifiers,aiartcreativity}.
We decided on manual analysis % of the data
because a preliminary screening revealed that very few prompts contained prompt modifiers.
% Manual inspection is therefore an appropriate research method.
Each prompt was analyzed by an author of this paper. We coded the presence of prompt modifiers and report on their nature and use.
We did not calculate inter-rater agreement because the coding was straight-forward~\cite{McDonald_Reliability_CSCW19.pdf}.%

% Keywords and key phrases were identified manually by the first author who has been experimenting with text-based image generation systems since October 2021. A preliminary inspection of the prompts confirmed our suspicion that prompt modifiers were rare in the dataset.


% --------------------
\paragraph{Descriptive language}%
% --------------------
A prompt written in descriptive language is likely to generate images of high quality.
We quantitatively assessed whether the prompts contained descriptive language by calculating a number of statistical indices for each prompt:
% Besides prompt modifiers, we analyzed whether a prompt was written in a way that is likely to generate images of high quality.
% To this end, we calculated a number of key statistical indices for each prompt:
\begin{itemize}
    \item The % characters,
    number of words (tokens) and unique words (types) in the prompt.
    % vocabulary size
        In general, longer prompts are more likely to include certain keywords
        (whether on purpose or by accident)
        that may trigger the image generation system to generate images with high quality or in a certain style.
    \item The Type-Token Ratio (TTR) \cite{TTR}, a standard measure for lexical diversity defined as the number of types divided by the number of tokens in the prompt.\footnote{We also experimented with other indices of lexical diversity, such as the Moving-Average Type-Token Ratio (MATTR) \cite{covington2010.pdf} and
    the Measure of Textual Lexical Diversity (MTLD) \cite{MTLD_MA_Wrap}.
    % Note that measures for lexical diversity have their limitations.
    However, these measures highly depend on the text length \cite{30200474.pdf}.
    Only a small fraction of the prompts in our sample meet the recommended minimum number of tokens for applying lexical diversity measures~\cite{1-s2.0-S1075293520300660-main.pdf}.
    The use of lexical diversity indices, such as the TTR, for comparing texts of different size is not recommended~\cite{30200474.pdf}.
    In our study, we do not use the TTR for comparing the lexical diversity of prompts, but to assess the amount of repetition of tokens in the prompt.}
    A token, in this case, is a discrete word whereas a type is a unique token in the prompt.
    For calculating the TTR, we used Kristopher Kyle's lexical-diversity Python package \cite{pythonpackage}.%
    % \item MATTR
    % \item MTLD\_MA\_Wrap, an adaption of the MTLD (measure of lexical textual diversity) \cite{MTLD_MA_Wrap}. The adapted MTLD uses a wrapping moving window approach that makes the measure better applicable to texts of short length \cite{1-s2.0-S1075293520300660-main.pdf}.
    % \item Shannon's entropy \cite{entropy} as a measure for the amount of information in the prompt.
\end{itemize}%
% as recommended in the guidelines for lexical diversity indices with short texts
    % MATTR 50 tokens
    % MTLD\_MA\_Wrap 100 tokens
    % These guidelines recommend  a minimum text length of 50 tokens for MATTR and 100 tokens for MTLD\_MA\_Wrap.
% Our dataset includes only two prompts of greater than 100 tokens, and seven prompts of greater or equal to 50 tokens.
% TTR and MTLD have been applied to short texts (e.g. for classifying Twitter posts \cite{information-13-00093.pdf})
% We plot the lexical diversity measures in this paper primarily to give a qualitative impression of the prompts, not to compare different prompts.
    % Repetitions are one category of prompt modifiers \cite{modifiers}.
% "it is extremely hazardous to use lexical ‘constants’ to compare texts of different length" \cite{30200474.pdf}

We further used tokenization and parts-of-speech tagging (with the Natural Language Toolkit \cite{P04-3031.pdf}) to calculate the amount of:
\begin{itemize}
    \item % absolute and relative number of 
        Nouns
        (\texttt{NN})
        % (including singular, plural, and proper nouns)
    and verbs (\texttt{VB}) as an indicator of the subjects in the prompt.
        % was included to analyze the topics of the prompts.
    \item % The % absolute and relative number of
        Adjectives
        (\texttt{JJ})
        as an indicator of descriptive
        % (or ``flowery'')
        % embellished
        language.
        % which may produce images of higher quality \cite{guidelines,aiartcreativity}.
    \item % The % absolute and relative number of 
        Prepositions
        (\texttt{IN})
    as an indicator of specific language.
    \item % The % absolute and relative number of 
        Cardinal numbers
        (\texttt{CD}) as information on the number of subjects.%
\end{itemize}%
% Reifying the amount of subjects in a prompt via the use of cardinal numbers is important for prompt writing.
% Each prompt typically contains at least one noun as main subject.
% The use of descriptive and specific language is likely to improve the outcome of the image generation.
% The overuse of prepositions could, however, result in a low fidelity of the image to the prompt.
% Cardinal numbers are important for prompt writing, because a determinate number of subjects (``two horses'') in a prompt is likely to provide images of higher quality than an indeterminate number of subjects (``horses'').
%%% ChatGPT:
Each prompt typically contains at least one noun as the main subject. Using descriptive and specific language is likely to improve the outcome of image generation. However, overusing prepositions could result in low fidelity of the image to the prompt. Cardinal numbers are important for prompt writing because a determinate number of subjects (e.g. \textit{``two horses''}) is likely to provide higher-quality images than an indeterminate number (e.g. \textit{``horses''}).

% Descriptive language was detected automatically with methods from natural language processing (NLP). More specifically, we used the spaCy library to apply parts-of-speech tagging and entity recognition.

% With exception of the wordclouds in \autoref{fig:study34:wordclouds}, we did not apply lemmatization and stopword removal (two common steps in natural language processing) in this analysis. The reason is that we are working with literal strings that are used as input for AI image generation systems. These systems are susceptible to tiny changes in their input \cite{x} and changing the prompt could result in vastly different images.


% --------------------
\subsection{Results}%
\label{sec:study2:results}
% --------------------
% In this section, we report on the workers and their use of prompt modifiers and descriptive language in the prompts.
% ---
\subsubsection{Participants}%
% ---
The 125 participants in our sample included 55 men, 67 women, 1 non-binary, and two participants who did not to disclose their gender identity.
The age of participants ranged from 19 to 71 years ($M=41.08$ years, $SD = 13.44$ years).
The majority of participants (98.40\%) reported English being their first language.
%
Thirty-seven participants (30.33\%) responded positively to the question that they had \textit{``experience with text-based image generation systems.''} We had no explanation for this surprisingly high number at this point, but inquired more about the participants' background in our follow-up study in Section~\ref{sec:study4}.
%
Median completion times were higher than estimated in the pilot study, reaching 197 seconds. It is possible that completion times are skewed due to participants reserving tasks in bulk.
% workers spent on average about  on completing the task ($400 seconds$, MIN=30$ s, $MAX=2868$ s, $Median=197$ s).

% -----
%TC:ignore
\begin{figure}[ht]%
\centering%
% \raggedright
\begin{subfigure}[b]{0.3\textwidth}%
    \includegraphics[width=\linewidth]{figures/wordclouds/study3-wordcloud-NN.pdf}
    \subcaption{%
        \centering
        % Nouns
        \texttt{NN}
        % {\small $M=3.27$,~$SD=3.36$}
    } % (\texttt{NN})
    \label{fig:study2:graphs:a}
\end{subfigure}
\begin{subfigure}[b]{0.205\textwidth}
    \includegraphics[width=\linewidth]{figures/wordclouds/study3-wordcloud-JJ.pdf}
    \subcaption{
        \centering
        % Adjectives
        \texttt{JJ}        
        % \\
        % {\small $M=1.65$,~$SD=1.95$}
    } % (\texttt{JJ})
    \label{fig:study2:graphs:b}
\end{subfigure}
\begin{subfigure}[b]{0.205\textwidth}
    \includegraphics[width=\linewidth]{figures/wordclouds/study3-wordcloud-VB.pdf}
    \subcaption{
        \centering
        % Verbs
        \texttt{VB}        
        % {\small $M=0.36$,~$SD=1.02$}
    } % (\texttt{VB})
    \label{fig:study2:graphs:c}
\end{subfigure}
\begin{subfigure}[b]{0.127\textwidth}
    \includegraphics[width=\linewidth]{figures/wordclouds/study3-wordcloud-IN.pdf}
    \subcaption{
        \centering
        % Prepositions
        \texttt{IN}
        % {\small $M=1.78$,~$SD=2.27$}
    } % (\texttt{IN})
    \label{fig:study2:graphs:d}
\end{subfigure}
\begin{subfigure}[b]{0.091\textwidth}
    \includegraphics[width=\linewidth]{figures/wordclouds/study3-wordcloud-CD.pdf}
    \subcaption{
        \centering
        % Cardinals
        \texttt{CD}
        % \\
        % {\small $M=0.07$,~$SD=0.29$}
    } % 
    \label{fig:study2:graphs:e}
\end{subfigure}
\caption{Wordclouds created from the 375 prompts provided by 125 workers in Study 3, split into nouns (\texttt{NN}), adjectives (\texttt{JJ}), verbs (\texttt{VB}), prepositions (\texttt{IN}), and cardinal numbers (\texttt{CD}). English stop words have been removed in this figure.
}
\Description{Wordclouds of tokens from 375 crowdsourced prompts.}%
\label{fig:study34:wordclouds}%
\end{figure}%
%TC:endignore
% -----

% ---
\subsubsection{On the use of descriptive language}
\label{sec:study3:descriptivelanguage}
% ---
The prompts were of varying length, ranging from 1 to 134 tokens with an average of 12.54~tokens per prompt ($SD=14.65$ tokens).
    % $M=70.77$ characters, $Median=50$ characters).
Overall, the length of prompts was appropriate for text-to-image generation with only four participants producing overly long prompts.
%
% types
% M=10.66
% SD=9.19
%
On average, participants used 3.27 nouns to describe the subjects in their prompt ($SD=3.36$).
%
Participants used verbs only sparingly in their prompts ($M=0.36$, $SD=1.02$).
%
The average number of prepositions ($M=1.78$, $SD=2.27$) was higher than the average number of adjectives ($M=1.65$, $SD=1.95$). However, this number is skewed by four participants who provided long prompts.
These participants were very specific in what their images should contain, with many prepositions being used to denote the relative positions of subjects in the artwork ($Max=21$ prepositions per prompt).

Overall, participants used rich descriptive language.
The participants were creative and often described beautiful natural scenery.
The main topics in the participants' prompts were landscapes, sunsets, and animals (see \autoref{fig:study34:wordclouds}).
We note that the richness of the language in the prompts primarily is a result of the use of adjectives (see \autoref{fig:study34:wordclouds}b).
On average, participants used 1.65~adjectives in their prompt ($SD=1.95$).
Colors, in particular, were popular among participants to describe the subjects in their artworks.
The following prompts exemplify the creativity and the use of descriptive language among participants:
\begin{quote}%
    % Alpine trees against a backdrop of snowy mountains and blue skies.
    \textit{%
    beautiful landscape with majestic mountains and a bright blue lake
    \vspace{.2\baselineskip}
    \\
    % A brilliant sunset with shades of orange and pink over the ocean
    bright yellow sun against a blue sky with puffy clouds
    \vspace{.2\baselineskip}
    \\
    A fruit bowl with vibrant colored fruits in it and a contrasting
    background
    \vspace{.2\baselineskip}
    \\
    % Dragon on the tower of a castle in a storm.
    % \\
    % Knight holding a sword that shines in the sunlight
    % \vspace{.2\baselineskip}
    % \\
    A white fluffy puppy is playing in the grass with a large blue ball that is twice his size.
    % \\
    % pretty woman wearing bright red dress and smiling
    \vspace{.2\baselineskip}
    \\
    A shiny black horse with eyes like coal run in a lush green grassy field
    \vspace{.2\baselineskip}
    \\
    There should be a beautiful green forest, full of leaves, with dark brown earth beneath, and a girl in a dress sitting on the ground holding a book.%
    }%
\end{quote}%

More than half of the prompts (58.13\%) did not repeat any tokens (that is, they had a TTR of 1; $M=0.94$, $SD=0.10$). Most of the repetitions in prompts stem from the participants' need to identify the relative positions of subjects in the image (e.g., \textit{``[...] Touching the black line and going all the way across the top of the black line should be a dark green line. Above the dark green line should be a medium green line. [...]''}).
Repetitions, as a stylistic element in prompts \cite{modifiers}, were not being used.
Only 27~prompts (7.2\% of all prompts) contained cardinal numbers ($M=0.07$, $SD=0.29$).
    The cardinal numbers are depicted in \autoref{fig:study34:wordclouds}e.
    Two of the cardinal numbers refer to a period in time which could potentially trigger the image generation system to produce images in a certain style.

Even though we tried to mitigate it in the task design and the instructions, we noticed 18~participants (14.4\%) still provided direct instructions to the AI instead of prompts describing the image content.
    These participants either wrote three separate instructions to the AI (e.g., \textit{``Generate a white 250 ml tea glass [...],''} \textit{``Draw three separate triangles [...],''} and \textit{``Show me some digital artwork from a brand new artist.''}) or they wrote three consecutive instructions as we had observed in our pilot study. The latter may not include nouns as subject terms and could thus result in images with an undetermined subject (e.g., \textit{``sharpen image''}).
Two participants thought they could chat with the AI, asking it, for instance, \textit{``Which do you prefer: starry night sky or blue sea at dawn?,''} \textit{``Enter your favorite geometric shape,''} and \textit{``Can you paint me a rendition of the Monalisa?''}.%


% <TRANSITION?>
% Workers however exercised no control over the style of their artworks.


% ---
\subsubsection{On the use of prompt modifiers}%
\label{sec:study3:modifiers}%
% ---
Even though participants were specifically instructed to create a digital artwork, we found only very few participants included style information in their prompts.
Many participants described a scene in rich descriptive language, but neither mentioned artistic styles, artist names, genres, art media, nor specific artistic techniques.
The participants' prompts may have described an artwork, but without style information, the style of the generated image is left to chance and the resulting images may not match the participants' intent and expectations.

Overall, the prompts did not follow the prompt template mentioned in Section~\ref{sec:promptmodifiers} and best practices common in the AI art community were not followed.
Only one participant made purposeful use of a prompt modifier commonly used in the AI art community. This prompt modifier is \textit{``unreal engine.''}\footnote{The long-form of this modifier is \textit{``rendered in UnrealEngine,''} a computer graphics game engine. Images generated with this prompt modifier may exhibit increased quality due to photo-realistic rendering.}
The participant used this modifier in all her three prompts by concatenating it to the prompt with a plus sign, e.g.%
% \begin{quote}%
    \textit{``rainbow tyrannosaurus rex + unreal engine.''}
% \end{quote}%
A small minority of participants used generic keywords that could trigger a specific style in text-to-image systems.
    For instance, the generic term \textit{``artwork''} was used in 16~prompts (4.3\%).
The following list of examples reflects almost the entire set of prompts containing explicit style information among the 375 prompts written by participants (with style modifiers underlined):
\begin{quote}%
    \textit{%
    \ul{Cubism portrait} of a Labrador Retriever using reds and oranges
    \vspace{.2\baselineskip}
    \\
    \ul{Paint} a \ul{portrait} of an old man in a park.
    \vspace{.2\baselineskip}
    \\
    \ul{Draw} a \ul{sketch} of an airplane.
    \vspace{.2\baselineskip}
    \\
    \ul{Abstract} trippy colorful background
    \vspace{.2\baselineskip}
    \\
    \ul{surreal} sky castle
    \vspace{.2\baselineskip}
    \\
    % Wildflowers growing along a mountain road. The road is gravel and inclined. Along each side of the road there are golden St. John's Wort, red Indian \hl{Paintbrush}, white Yarrow, and pink wild roses.
    % \\
    Can you \ul{paint} me a rendition of the \ul{Monalisa}?
    \vspace{.2\baselineskip}
    \\
    \ul{Bob Ross}, \ul{Claude Monet}, \ul{Vincent Van Gogh}
    \vspace{.2\baselineskip}
    \\
    Are you able to produce any of \ul{rodans work}.
    \vspace{.2\baselineskip}
    \\
    what can you do, can you make \ul{pointillism artwork}?
    % \\
    % A black cat sits in the front window of a pale blue \hl{Victorian style} house
    }%
\end{quote}%
%
Besides this sparse --- and sometimes accidental --- addition of style information, we find that overall, participants did not control the style of their creations. % Not including the above short list of prompt modifiers in prompts,
Output styles were mainly determined by the participants' use of descriptive language.
%
%
% ---------
% \input{TAB/TABLE-METRICS-STUDY3-4}
% ---------
%
% -----
% \input{TAB/TABLE-METRICS-STUDY3-4}
% -----
%
% \subsubsection{Resulting images}
% We generated five images for each of the worker-provided prompts in preparation for our our final study. The image generation process is described in detail in \autoref{sec:study4-materials}. In this section, we qualitatively describe the resulting images.
% Because almost none of the workers provided specific style information, the set of generated images


% ====================
\section{Study 3: Improving Prompts}%
\label{sec:study4}%
% ====================
In a follow-up study, we investigated whether participants could improve their artworks.
% Our last study invited workers to review the images generated from their prompts.
This study aimed to answer the question of whether prompt engineering is a skill that we humans apply intuitively or whether it is a learned skill that requires expertise (e.g., by learning to write prompts from repeated interactions with the text-to-image system) and knowledge of certain keywords and key phrases (prompt modifiers), as discussed in Section \ref{sec:promptmodifiers} and Section \ref{sec:study3:modifiers}.
%
% The crowd workers were invited to review images generated from their previous prompts and asked to re-write the prompts to improve the results.
We hypothesize that if prompt engineering is a learned skill, participants will not be able to significantly improve their artworks after only one iteration.
% In this case, workers will also not be able to apply specific keywords for modifying prompts.%
%
%
% --------------------
\subsection{Method}%
% --------------------

% --------------------
\subsubsection{Study Design}%
% --------------------
We invited the same participants who participated in Study~2 to review images generated from their own prompts.
    Participants were then asked to improve their three prompts.
%
To this end, we designed a task that introduced the participant to the study's purpose, using the same instructions as in the previous study.
We additionally highlighted that if the images presented to the participant did not look like artworks, the prompt should be adjusted accordingly. % (but without specifying how).
Like in the previous study, we avoided to mention that prompt modifiers could be used to achieve this aim.

% The task presented each participant with five images for each of the three prompts written by the participant in Study~2.
% We used the workerId variable to look up the worker's previous prompts and respective images.
% Participants were then instructed to rewrite and improve each of their three prompts. The task included two input fields which were explained to the participant. One field was pre-filled with the participant's previous prompt and one field was used for optionally entering negative terms.
%     In practice, these negative terms are part of the prompt engineer's toolbox to control the outputs of image generation. A prompt engineer could, for instance, seek to reduce the occurrence of text and watermarks by adding \textit{``watermark''} or \textit{``shutterstock''} as negative terms to a prompt.
% We investigated this practice by incorporating it into our study design.
%     Participants were introduced to the potentially surprising effects of the negative terms with an example.
%         This example explained that if the negative term \textit{``zebra''} is added to a prompt of a \textit{``pedestrian crossing,''} the resulting image could, potentially, be of a plain road (due to stripes being removed from the output).
%%% ChatGPT:
Participants were given five images for each of the three prompts they wrote in Study 2. We used the workerId variable on MTurk to load the participant's previous prompts and images. Participants were then asked to rewrite and improve their three prompts. The task included two input fields, one pre-filled with their previous prompt and one for optional negative terms. In practice, negative terms are used by prompt engineers to control image generation. For example, adding \textit{``watermark''} or \textit{``shutterstock''} to a prompt can reduce the occurrence of text and watermarks in the output. We studied this by incorporating it into our study design. Participants were introduced to the potentially surprising effects of negative terms with an example. The example explained that adding \textit{``zebra''} as a negative term to a prompt for a pedestrian crossing could potentially result in an image of a plain road (due to stripes being removed).

For each prompt, we also collected information on whether the images matched the participant's original expectations (given the previous prompt)
and whether the participant thought the prompt needed improvement (both on a Likert-scale from 1~--  Strongly Disagree to 5~-- Strongly Agree).
    The latter was added to identify cases in which participants thought that no further improvement of the prompt was necessary.
% We also elicited the participant's confidence on whether the new prompt would result in an improved artwork (on a Likert scale from 1~-- Not At All Confident to 5~-- Highly Confident).
%%% ChatGPT:
We also asked participants to rate their confidence that the new prompt would result in a better artwork  (on a Likert scale from 1~-- Not At All Confident to 5~-- Highly Confident).
%
The task concluded with demographic questions, including
the participant's experience with text-based image generation and interest in viewing and practicing art.
The task design was tested and improved in a small-scale pilot study ($N=8$; US\$1 per task).
    % The estimated completion time calculated in this pilot study was 6 min 45 s on average.
    The payment was set to US\$1.75, % to incentivize participation, 
    aiming for an hourly pay of above minimum wage in the United States.

% --------------------
\subsubsection{Research Materials}%
\label{sec:study4-materials}%
% --------------------
In this section, we describe how we selected an image generation system and how we generated images from the participants' prompts.%
% ------
\paragraph{System selection}%
% ------
% We generated images from the prompts provided by workers in Study~3.
We experimented with different text-to-image generation systems, including
    CLIP Guided Diffusion (512x512, Secondary Model)\footnote{https://colab.research.google.com/drive/1mpkrhOjoyzPeSWy2r7T8EYRaU7amYOOi},
    CLIP Guided Diffusion (HQ 512x512 Uncond)\footnote{https://colab.research.google.com/drive/1QBsaDAZv8np29FPbvjffbE1eytoJcsgA},
    DALLE-E mini\footnote{https://github.com/borisdayma/dalle-mini},
    Disco Diffusion 5.3 and 5.4\footnote{https://github.com/alembics/disco-diffusion},
    Latent Diffusion\footnote{https://github.com/CompVis/latent-diffusion},
    and Majesty Diffusion 1.3\footnote{https://github.com/multimodalart/majesty-diffusion}.
% The four systems were selected for further experimentation because they represent the state-of-the-art of openly available image generation systems.
% For each system, we generated images from the prompts collected in Study 3.
% We qualitatively examined and compared the sets of images.
In the end, we selected Latent Diffusion for two main reasons. Latent Diffusion is the foundation for many of the community-driven adaptations and modifications. %, such as Majesty Diffusion.
More importantly, the system is deterministic and leads to reproducible outcomes. Consecutive runs with the same seed value will generate the same images. This is a crucial requirement since we aim to compare images in between studies.
%
% \item The results from \textit{CLIP-guided Diffusion (cc12m)} were often grainy and abstract even after 500 generation steps. While this makes CLIP-guided Diffusion an interesting fit for studying prompt engineering, we decided not to proceed with this system because of the long time to generate images (about 8 minutes per batch of five images).
%     \item \textit{CLIP-guided Diffusion (cc12m-1-cfg PLMS)} ...
%     \item \textit{Latent Diffusion} xxxxxxx. The process is quick (about 20 seconds per image). Potential disadvantages are that typically phrases that work on CLIP (such as \textit{``trending on artstation''}) may not work as well in Latent Diffusion. In our batch, Latent Diffusion produced many images containing text.
    % \item \textit{v-majesty-diffusion} Many images generated by this system were photo-realistic. Photographs have a high visual quality out-of-the-box. For studying prompt engineering and the visual quality of images, V-majesty-diffusion was therefore not deemed the right fit for our study.
    % CLIP-guided diffusion is an interesting choice for image generation studies due to CLIP responding well to prompt modifiers \cite{guidelines}.
% Latent Majesty Diffusion represents the state-of-the-art of openly available systems. The system is based on latent diffusion which was trained on LAION and contains many community-provided optimizations.
% However, we found the two systems to be non-deterministic: consecutive runs with the same prompt would produce different images.
% To assess the changes in 
    % We decided to proceed with this system because it produced images of high visual quality with out-of-the-box only few images that contained text.
%    %
% We found the majority of the systems to be non-deterministic % Other systems (e.g., Disco Diffusion) required a long time to generate images.
%
%
\paragraph{Image generation}%
We generated images for the participants' prompts with Latent Diffusion using the following configuration settings:
    text2img-large model (1.4B parameters),
    % DiffusionWrapper has 872.30 M params
    seed value 1040790415,
    eta 1.0,
    ddim steps 100, and
    % n\_samples 1,
    % n\_iter 5,
    scale 5.0.
    % width 256, and
    % height 256.
%
% The configuration settings used to generate the images are listed in Table~\ref{tab:settings} in Appendix \ref{appendix:settings}.
% \begin{table}[h]%
% \centering%
% \begin{tabular}{lc}%
% \toprule%
%     Configuration variable & Value \\
% \midrule%
%     model & text2img-large \\
%     seed & 1040790415 \\
%     % ddim
%         eta & 1.0 \\
%     ddim\ steps & 100 \\
%     n\_samples & 1 \\
%     n\_iter & 5 \\
%     scale & 5.0 \\
%     width & 256 \\
%     height & 256 \\
% \bottomrule%
% \end{tabular}%
% \caption{Image generation settings used in studies 3 and 4.}%
% \label{tab:settings}%
% \end{table}%
%
% A fixed seed was used for all generation runs which makes the image generation reproducible.
Even though the system is capable of generating images at higher resolutions, we decided to generate images of $256\times256$ pixels to avoid the quirks that often occur when generating images in resolutions that the model was not trained on. %, such as the duplication of subjects in the image.
%
% If the source originated from a Colab Notebook (as was the case with CLIP-guided Diffusion and V-majesty-diffusion), we adjusted the code to run on the local server.
% The default settings were used for all systems, with two adjustments.
    % First, we configured each system to sample a batch of five images.
    % Second, we set a fixed seed value for all runs.
        % The fixed seed makes the sampling of images deterministic and reproducible.
        % That is, given the same seed and same prompt, the system will generate the same image.
% With each system, we then generated five images for each worker-provided prompt. This resulted in 5715 images in total (375 prompts $\times$ 5 images $\times$ X systems).
%
% \subsubsection{Final set of material}
The image generation job yielded 1875 images (125 participants $\times$ 3 prompts per participant $\times$ 5 images per prompt).
%
% \subsubsection{Generation of second set of images}
After collecting the revised prompts from participants, we generated another set of 1875 images using the same seed value and configuration settings as before. Negative terms were used in this second set, if provided by the participant.%

% \subsubsection{Set of generated images}%
Some hand-selected images generated from the prompts are depicted in \autoref{fig:study34-examples}.
% Because only few workers provided specific style information in their prompts or provided this information only accidentally,
% The set of images contained a wide variety of different styles.
% As a result of the sparse use of prompt modifiers, the set of images that we generated from the worker-provided prompts contained photographic images.
Many images were of photo-realistic quality, depicting landscapes, sunsets, beaches, and animals. Besides photographs, artistic styles included paintings, graphic designs, abstract artworks, as well as magazine and book covers.
Some images contained text and many images contained watermarks.%
% ----
%
%TC:ignore
\noindent%
{%
\setlength\tabcolsep{2pt}%
\begin{figure}[thb]%
\centering%
\begin{subfigure}[b]{\textwidth}
\begin{tabularx}{\textwidth}{XXXXXXX}%
  \includegraphics[width=\linewidth]{figures/ldm-examples/0000.jpg}
&
  \includegraphics[width=\linewidth]{figures/ldm-examples/0022.jpg}
&
  \includegraphics[width=\linewidth]{figures/ldm-examples/0003.jpg}
&
  \includegraphics[width=\linewidth]{figures/ldm-examples/0023.jpg}
&
  \includegraphics[width=\linewidth]{figures/ldm-examples/0005.jpg}
&
  \includegraphics[width=\linewidth]{figures/ldm-examples/lion.jpg}
&
  \includegraphics[width=\linewidth]{figures/ldm-examples/0007.jpg}
\\
  \includegraphics[width=\linewidth]{figures/ldm-examples/0025.jpg}
&
  \includegraphics[width=\linewidth]{figures/ldm-examples/0024.jpg}
&
  \includegraphics[width=\linewidth]{figures/ldm-examples/0021.jpg}
&
  \includegraphics[width=\linewidth]{figures/ldm-examples/0010.jpg}
&
  \includegraphics[width=\linewidth]{figures/ldm-examples/0011.jpg}
&
  \includegraphics[width=\linewidth]{figures/ldm-examples/landscape.jpg}
  &
  \includegraphics[width=\linewidth]{figures/ldm-examples/0027.jpg}
\\
  \includegraphics[width=\linewidth]{figures/ldm-examples/0013.jpg}
&
  \includegraphics[width=\linewidth]{figures/ldm-examples/0019.jpg}
&
  \includegraphics[width=\linewidth]{figures/ldm-examples/0015.jpg}
&
  \includegraphics[width=\linewidth]{figures/ldm-examples/0016.jpg}
&
  \includegraphics[width=\linewidth]{figures/ldm-examples/0020.jpg}
&
  \includegraphics[width=\linewidth]{figures/ldm-examples/0018.jpg}
&
  \includegraphics[width=\linewidth]{figures/ldm-examples/0009.jpg}
\\
%   \includegraphics[width=\linewidth]{figures/ldm-examples/0017.jpg}
%   \includegraphics[width=\linewidth]{figures/ldm-examples/0002.jpg}
%   \includegraphics[width=\linewidth]{figures/ldm-examples/0026.jpg}
%   \includegraphics[width=\linewidth]{figures/ldm-examples/0004.jpg}
%   \includegraphics[width=\linewidth]{figures/ldm-examples/0014.jpg}
%   \includegraphics[width=\linewidth]{figures/ldm-examples/0008.jpg}
% &
% \\
\end{tabularx}%
    \subcaption{}
    \label{fig:study34-examples:a}%
\end{subfigure}%
\vspace{.5\baselineskip}
\\
\begin{subfigure}[b]{\textwidth}%
\begin{tabularx}{\textwidth}{XXXXXXX}%
  \includegraphics[width=\linewidth]{figures/failed-images/1.jpg}
&
  \includegraphics[width=\linewidth]{figures/failed-images/2.jpg}
&
  \includegraphics[width=\linewidth]{figures/failed-images/3.jpg}
&
  \includegraphics[width=\linewidth]{figures/failed-images/11.jpg}
&
  \includegraphics[width=\linewidth]{figures/failed-images/15.jpg}
&
  \includegraphics[width=\linewidth]{figures/failed-images/4.jpg}
&
  \includegraphics[width=\linewidth]{figures/failed-images/6.jpg}
\\
  \includegraphics[width=\linewidth]{figures/failed-images/8.jpg}
&
  \includegraphics[width=\linewidth]{figures/failed-images/10.jpg}
&
  \includegraphics[width=\linewidth]{figures/failed-images/25.jpg}
&
  \includegraphics[width=\linewidth]{figures/failed-images/21.jpg}
&
  \includegraphics[width=\linewidth]{figures/failed-images/9.jpg}
&
  \includegraphics[width=\linewidth]{figures/failed-images/18.jpg}
  &
  \includegraphics[width=\linewidth]{figures/failed-images/12.jpg}
\\
\end{tabularx}%
    \subcaption{}
    \label{fig:study34-examples:b}%
\end{subfigure}%
\caption{Selected exemplars of a) successful and b) failed image generations from worker-provided prompts. The images in \autoref{fig:study34-examples:a} were selected to represent a variety of different styles and are not representative of the whole set of images. The images in \autoref{fig:study34-examples:b} depict some of the recurring issues in images generated from worker-provided prompts.}%
\Description{Selected images generated from worker-provided prompts.}%
\label{fig:study34-examples}%
\end{figure}%
}%
%TC:endignore
% ----


% --------------------
\subsubsection{Analysis}%
\label{sec:study4:analysis}%
% --------------------
We analyzed the two sets of prompts and images written in studies~2 and~3 as follows.

\paragraph{Analysis of prompts}
% To get a quantitative impression of the amount of changes in the prompts,
% we calculated the number of tokens added and removed between the workers' two prompts using parts-of-speech tagging and calculated the Levenshtein distance~\cite{Levenshtein1966a.pdf}, a measure of lexical similarity denoting the minimum number of edits needed to change one string into another.
% To assess the nature of the changes made by workers, the first author qualitatively identified changes in the prompts between the two studies.
% To this end, the first author familiarized himself with the data and inductively developed a coding scheme \cite{hsieh2005three}.
% The coding scheme was discussed among the authors and subsequently revised.
% The final coding scheme comprised changes made to the prompts along eight categories: adjectives/adverbs, subjects, prepositions, paraphrasing/synonyms, reordering,	cardinal numbers, simplification, presence of prompt modifiers, and whether a prompt was completely different.
%     For instance, if a worker added, removed, or switched adjectives, the prompt was assigned the respective code.
% Note that we understand ``subjects'' in the sense of subject terms~\cite{modifiers} for image generation (e.g., ``a boy holding a ball'' would contain two subjects (boy and ball)). Synonyms were analyzed on the level of individual words, but also parts of sentences --- that is, if a worker expressed something in different terms, we coded it as synonym. After discussing and revising the codes, the first author coded all prompts. The outcome of this coding activity is a co-occurrence matrix of changes made to prompts by workers.
% ChatGPT:
To measure the amount of changes in the prompts,
we calculated the number of tokens added and removed using parts-of-speech tagging as well as the Levenshtein distance~\cite{Levenshtein1966a.pdf}, a measure of lexical similarity denoting the minimum number of edits needed to change one string into another.
To understand the nature of the changes, the first author inductively developed a coding scheme \cite{hsieh2005three} with eight categories: adjectives/adverbs, subjects, prepositions, paraphrasing/synonyms, reordering, cardinal numbers, simplification, and presence of prompt modifiers.
After discussing the codes among all authors and revising the codes, the first author coded all prompts and generated a co-occurrence matrix of changes made by participants. Note that we understand ``subjects'' in the sense of subject terms~\cite{modifiers} for image generation (e.g. ``a woman holding a phone'' would have two subjects (woman and phone). Synonyms were analyzed at the level of individual words and parts of sentences.

\paragraph{Analysis of the revised images}%
% We evaluated the images as follows.
% One author first created a spreadsheet that displayed the two prompts written by workers in studies 3 and 4 as well as the respective sets of five images. The authors then reviewed 30 of these pairs of prompts and images and discussed the changes that had taken place in the images between Study 3 and 4. From this discussion, we developed a set of evaluation criteria. Each author then individually rated 50~pairs of images along the criteria. After this initial round of coding, the authors met again to discuss the results. The authors reviewed each others ratings and discussed and made revisions to ratings. From this discussion, we decided to extend the coding scheme with four more criteria.
% The full set of criteria used to evaluate the images included
%     a binary category to denote failed image generations,
%     the amount of style change,
%     the amount of subject change,
%     whether the consistency of images improved between the studies,
%     as well as the
%     details,
%     contrast,
%     color,
%     distortions,
%     presence of watermarks,
%     and the overall subjective impression of which image set was of higher quality.
% After a second round of coding, the authors cross-checked their evaluations another time and resolved differences through discussion.
%ChatGPT:
We evaluated the images as follows. One author first created a spreadsheet with the two prompts and respective sets of five images from studies 2 and 3. The authors then discussed 30 of these image-text pairs and developed a set of evaluation criteria.
Each author individually rated 50 pairs of images along these criteria.
After the initial round of coding, the authors discussed the results and decided to add four more criteria to the coding scheme. The final set of criteria included binary categories for failed generations, amount of style and subject change, and whether consistency improved, as well as ratings for details, contrast, color, distortions, watermarks, and overall subjective impression of quality. After a second round of coding, the authors cross-checked their evaluations and resolved differences through discussion.


% --------------------
\subsection{Results}
% --------------------
%
% -----
\subsubsection{Participants}
% -----
The sample consisted of 50~crowd workers (40\% of the participants who participated in Study~2).
Participants included 25~men, 24~women, and 1~person who preferred not to disclose the gender identity, aged 20 to 71 years ($M=42.76$, $SD=14.63$).
Participants came from varied educational backgrounds, including some completed college courses (17 participants), Bachelor's degrees (22 participants), Master's degrees (4 participants), and doctorate degrees (2 participants).
%
% ----
%TC:ignore
\noindent%
\begin{figure}[htb]%
\centering%
  % \includegraphics[width=.9\linewidth]{figures/study34/study4-likerts.pdf}%
\includegraphics[width=.9\linewidth]{figures/study34/study4-demographics-short.pdf}%
\caption{Background of the crowd workers participating in Study 4.}%
%  including whether their expectations were met, an assessment of the need for improvement in the prompts, and the workers' self-rated confidence in their ability to improve the image with their revised prompt
\Description{Background of the crowd workers participating in Study 4.}%
\label{fig:study4:likerts}%
\end{figure}%
%TC:endignore
% ----
%
% Seven in ten workers had a self-declared educational background in the arts. There was some interest among workers in visiting museums and in AI generated imagery (see \autoref{fig:study4:likerts}).
% However, workers were overall not likely to practice art themselves and 80\% of workers had no or little experience with text-to-image generation.
%%% ChatGPT
Seven out of ten participants had an educational background in the arts. Some participants were interested in visiting museums and AI-generated imagery, but most did not practice art themselves and 80\% had little or no experience with text-to-image generation.

% When it comes to whether participants' expectations were met by the generated images, there were two groups: participants who were disappointed ($\approx$40\%) and participants whose expectations were met (55\%).
% About 60\% of the participants thought the images needed improvement and about the same percentage of participants was confident (or highly confident) that their revised prompts would improve the generated images.
%%% ChatGPT
Approximately 40\% of participants were disappointed with the generated images, while 55\% of participants' expectations were met. Around 60\% of participants believed the images needed improvement, and a similar percentage of participants were confident that their revised prompts would improve the generated images.


% In the following section, we review the changes made to the prompts by workers.


% -----
\subsubsection{Participants' revised prompts}%
% -----
% We find that workers added more tokens than they removed (see \autoref{fig:study4:changes:histograms}).
The average Levenshtein distance between the participants' two prompts (not including negative terms) was 28.1 ($SD=25.0$).
    % $Q0=0$, $Q25=10.2$, $Q50=22$, $Q75=36.8$, $Q100=113$
A computational analysis of the changes with parts-of-speech tagging shows that participants added over twice as many tokens as they removed~--- 538 added tokens versus 243 removed tokens (see \autoref{fig:study4:changes:histograms}).
Nouns were added most often (29.55\% of added tokens), followed by adjectives (22.12\%), prepositions (17.84\%) and determiners (8.55\%).
% added-NN	159	29.55\%
% added-JJ	119	22.12\%
% added-IN	96	17.84\%
% added-DT	46	8.55\%
% removed-NN	70	28.81\%
% removed-IN	41	16.87\%
% removed-JJ	32	13.17\%
% removed-DT	21	8.64\%
% kept-NN	449	29.50\%
% kept-DT	316	20.76\%
% kept-IN	258	16.95\%
% kept-JJ	218	14.32\%
The same types of tokens were also most often removed (28.81\% of removed tokens were nouns, 16.87\% prepositions, 13.17\% adjectives, and 8.64\% determiners).
%
In 11~prompts (7.33\%), the participant neither changed the prompt nor provided a negative term.
% 23 workers made no changes at all to their prompts, but 12 of these workers added a negative term.
    Six of these instances consisted of participants pasting random snippets of text.
    % (e.g., ``Less than a decade after breaking the Nazi encryption machine Enigma [...]'').
    % Some of these instances were relevant to text-to-image generation (e.g., ``A new AI program is wowing the public with its ability to draw realistic and creative pictures [...]'').

% ----
%
%
\begin{table*}[thb]%
	\small%
	\caption{Evaluation of changes in the two sets of images generated from worker-provided prompts.}%
	\label{tab:imageevaluation}%
	\begin{tabular}{rccccccc}%
		\toprule%
		& details &	contrast & color &	distortions &	watermarks &			consistency &  overall
		\\%
		\midrule%
		worse  & 17 (11.3\%) & 17 (11.3\%) & 12 (8.0\%) & 32 (21.3\%) & 31 (20.7\%) & 23 (15.3\%) & 23 (15.3\%) \\
		same   & 81 (54.0\%) & 85 (56.7\%) & 88 (58.7\%) & 99 (66.0\%) & 85 (56.7\%) & 95 (63.3\%) & 77 (51.3\%) \\
		better & 52 (34.7\%) & 48 (32.0\%) & 50 (33.3\%) & 19 (12.7\%) & 34 (22.7\%) & 32 (21.3\%) & 50 (33.3\%) \\
		\bottomrule%
	\end{tabular}%
\end{table*}%
\vspace{0pt}%
%
%
% ----

% ----
% \afterpage{%
%
%TC:ignore
\newlength{\thumbnailwidth}%
\setlength{\thumbnailwidth}{1.5cm}%
\newlength{\skiplength}%
\setlength{\skiplength}{4pt}%
%
\noindent%
{%
\begin{figure}[thb]%
% \renewcommand{\arraystretch}{1.5}%
\centering%
\begin{subfigure}[b]{0.32\textwidth}
\noindent
    \begin{tabularx}{\textwidth}{cX}%
      \includegraphics[width=\thumbnailwidth,valign=t]{figures/study4/A33B85TN97HQ33/before/0000.jpg}%
        \vspace{\skiplength}
        &
        \scriptsize
        bright full moon just rising over the desert  
        \\
      \includegraphics[width=\thumbnailwidth,valign=t]{figures/study4/A33B85TN97HQ33/after/0000.jpg}%
        &
        \scriptsize
        bright full \textbf{amber colored} moon just rising over the desert 
    \end{tabularx}%
    \subcaption{}%
    \label{fig:study4:examples:a}
\end{subfigure}
\begin{subfigure}[b]{0.32\textwidth}
    \begin{tabularx}{\textwidth}{cX}%
      \includegraphics[width=\thumbnailwidth,valign=t]{figures/study4/A2N93IVSZXSB73/before/0003.jpg}%
        \vspace{\skiplength}
        &
        \scriptsize
        A famers market in Nebraska in the early fall.  
        \\
      \includegraphics[width=\thumbnailwidth,valign=t]{figures/study4/A2N93IVSZXSB73/after/0003.jpg}%
        &
        \scriptsize
        An \textbf{outdoor} famers market in Nebraska in the early fall.  
    \end{tabularx}%
    \subcaption{}%
    \label{fig:study4:examples:b}
\end{subfigure}
\begin{subfigure}[b]{0.32\textwidth}
    \begin{tabularx}{\textwidth}{cX}%
      \includegraphics[width=\thumbnailwidth,valign=t]{figures/study4/AOWW3URQNRJ6U/before/0002.jpg}%
        \vspace{\skiplength}
        &
        \scriptsize
        A forest with scary trees all around
        \\
      \includegraphics[width=\thumbnailwidth,valign=t]{figures/study4/AOWW3URQNRJ6U/after/0002.jpg}%
        &
        \scriptsize
        A forest with \textbf{vibrant green} trees all around.
    \end{tabularx}%
    \subcaption{}%
    \label{fig:study4:examples:c}
\end{subfigure}
\\[.25\baselineskip]
% \midrule
% \\[.25\baselineskip]
%
\begin{subfigure}[b]{0.32\textwidth}
\noindent
    \begin{tabularx}{\textwidth}{cX}%
      \includegraphics[width=\thumbnailwidth,valign=t]{figures/study4/A1OC6IOF1JHM2F/before/0001.jpg}%
        \vspace{\skiplength}
        &
        \scriptsize
        A comfortable warm blanket resting on an antique rocking chair.
        \\
      \includegraphics[width=\thumbnailwidth,valign=t]{figures/study4/A1OC6IOF1JHM2F/after/0001.jpg}%
        &
        \scriptsize
        A comfortable warm blanket resting on an antique rocking chair \textbf{in front of a fireplace}.  
    \end{tabularx}%
    \subcaption{}%
    \label{fig:study4:examples:d}
\end{subfigure}
\begin{subfigure}[b]{0.32\textwidth}
    \begin{tabularx}{\textwidth}{cX}%
      \includegraphics[width=\thumbnailwidth,valign=t]{figures/study4/A2MOKIEQZ0OF2M/before/0000.jpg}%
        \vspace{\skiplength}
        &
        \scriptsize
        Are you able to produce any of rodans work. 
        \\
      \includegraphics[width=\thumbnailwidth,valign=t]{figures/study4/A2MOKIEQZ0OF2M/after/0000.jpg}%
        &
        \scriptsize
        \textbf{Will you please correct} Rodans \textbf{art}work \textbf{for me}.
    \end{tabularx}%
    \subcaption{}%
    \label{fig:study4:examples:e}
\end{subfigure}
\begin{subfigure}[b]{0.32\textwidth}
    \begin{tabularx}{\textwidth}{cX}%
      \includegraphics[width=\thumbnailwidth,valign=t]{figures/study4/A2IMAGGCST8170/before/0004.jpg}%
        \vspace{\skiplength}
        &
        \scriptsize
        miracle of creation 
        \\
      \includegraphics[width=\thumbnailwidth,valign=t]{figures/study4/A2IMAGGCST8170/after/0004.jpg}%
        &
        \scriptsize
        \textbf{explosion of creativity}  
    \end{tabularx}%
    \subcaption{}%
    \label{fig:study4:examples:f}
\end{subfigure}
\\[.25\baselineskip]
%\midrule
%\\[.25\baselineskip]
\begin{subfigure}[b]{0.32\textwidth}
\noindent
    \begin{tabularx}{\textwidth}{cX}%
      \includegraphics[width=\thumbnailwidth,valign=t]{figures/study4/AYIFHDQSXQJ6B/before/0004.jpg}%
        \vspace{\skiplength}
        &
        \tiny
        A fruit bowl with vibrant colored fruits in it and a contrasting background 
        % Draw a bunch of circles that are different sizes. Make some of them overlap. Fill half of them with a pale yellow color. Fill the other half with lilac color.  
        \\
      \includegraphics[width=\thumbnailwidth,valign=t]{figures/study4/AYIFHDQSXQJ6B/after/0004.jpg}%
        &
        \tiny
        A \textbf{neutral colored} bowl with \textbf{a variety of several brightly} colored \textbf{and} vibrant fruits in it, and a background \textbf{that is darker to contrast with the fruit}.
        % Draw at least 15 circles that are different sizes. Make some of them overlap. Fill half of them with a pale yellow color. Fill the other half with lilac color. 
    \end{tabularx}%
    \subcaption{}%
    \label{fig:study4:examples:g}
\end{subfigure}
\begin{subfigure}[b]{0.32\textwidth}
    \begin{tabularx}{\textwidth}{cX}%
      \includegraphics[width=\thumbnailwidth,valign=t]{figures/study4/A2FQYBUXX97P13/before/0003.jpg}%
        \vspace{\skiplength}
        &
        \scriptsize
        A man in a blue business suit sitting on a bench. He holds a briefcase.
        \\
      \includegraphics[width=\thumbnailwidth,valign=t]{figures/study4/A2FQYBUXX97P13/after/0003.jpg}%
        &
        \scriptsize
        A man \textbf{seated on a park} bench. He \textbf{is in a} blue business suit. \textbf{A} briefcase \textbf{is beside him on the bench.}
    \end{tabularx}%
    \subcaption{}%
    \label{fig:study4:examples:h}
\end{subfigure}
\begin{subfigure}[b]{0.32\textwidth}
    \begin{tabularx}{\textwidth}{cX}%
      \includegraphics[width=\thumbnailwidth,valign=t]{figures/study4/A1T4GVQH4P0BO/before/0004.jpg}%
        \vspace{\skiplength}
        &
        \scriptsize
        A wild cat sitting on a brightly-painted fence. 
        \\
      \includegraphics[width=\thumbnailwidth,valign=t]{figures/study4/A1T4GVQH4P0BO/after/0004.jpg}%
        &
        \scriptsize
        A \textbf{tiger stands on top of a} fence \textbf{that has been }painted \textbf{with vivid primary colors.}  
    \end{tabularx}%
    \subcaption{}%
    \label{fig:study4:examples:i}
\end{subfigure}
\caption{Examples of changes (highlighted in bold) in adjectives and adverbs (a--c), subjects (d--f) and multiple changes at once (g--i) made by crowd workers to their own prompts in Study~4.}
\Description{Histograms of changes in tokens and co-occurrence matrix of changes made by crowd workers to their prompts in Study 4. The graphs show that more tokens were added than removed and most workers made changes to adjectives.}
\label{fig:study4:images}%
\end{figure}%
}%
%TC:endignore
%
% }%
% ----

% -----
% \input{TAB/TABLE-METRICS-STUDY3-4}
%TC:ignore
\noindent%
\begin{figure}[h]%
\centering%
  % \includegraphics[width=\linewidth]{figures/boxplots/study34-comparison-boxplots.pdf}
  % \includegraphics[width=\linewidth]{figures/study34/changes.pdf}
%   \\
   % \includegraphics[width=\linewidth]{figures/boxplots/study34-boxplots.pdf}
\begin{subfigure}[b]{0.45\textwidth}
    % \includegraphics[width=\linewidth]{figures/study34/changes.pdf}
    \includegraphics[width=.905\linewidth]{figures/study34/tokens_added_removed.pdf}%
    \subcaption{%
        \centering
        Histogram of changes in tokens
    }%
    \label{fig:study4:changes:histograms}%
\end{subfigure}%
\begin{subfigure}[b]{0.45\textwidth}%
    \includegraphics[width=\linewidth]{figures/study34/study34-co-occurrence-matrix.pdf}%
    \subcaption{%
        \centering
        Co-occurrence matrix of changes
    }%
    \label{fig:study4:changes:cooccurence}%
\end{subfigure}%
\caption{Histograms of changes in prompt tokens and co-occurence matrix of changes made by crowd workers to their prompts in Study~4.}
\Description{Histograms of changes in tokens and co-occurence matrix of changes made by crowd workers to their prompts in Study 4. The graphs show that more tokens were added than removed and most workers made changes to adjectives.}
\label{fig:study34:changes}%
\end{figure}%
%TC:endignore
% -----
% Our qualitative coding of the main changes in each prompt found that modifying adjectives (i.e., adding, removing, or switching adjectives) was the main strategy used by workers (see \autoref{fig:study4:changes:cooccurence}).
% For instance, a worker changed the prompt \textit{``flowers in winter''} to \textit{``purple flowers in winter.''}
% These changes were often combined with changes in the subject (cf. \autoref{fig:study4:images}).
% For instance, a worker improved the prompt \textit{``sweeping arcs''} to \textit{``deep and broad, sweeping arcs in landscapes.''}
% A few workers, such as the one above, adapted their prompts to what they saw in the pictures.
%     This change often resulted in only minute changes in the resulting image. For instance, in the case of the above worker, the two images of mountainous landscapes were almost identical.
% Another fairly common approach among participants were changes in prepositions. Only few participants made changes with an intent of simplifying their prompts and
% relatively few participants made improvements to the cardinal numbers.
%     One worker, for instance, changed \textit{``draw a bunch of circles''} to \textit{``draw at least 15 circles.''}
%     Similarly, one worker wanted to see \textit{``lots of puffy clouds,''} leaving the exact number still undetermined.
%%% ChatGPT
Our coding showed that the main strategy used by participants was modifying (i.e., adding, removing, or switching) adjectives in their prompts (see \autoref{fig:study4:changes:cooccurence}).
For example, a participant changed the prompt \textit{``flowers in winter''} to \textit{``purple flowers in winter.''}
This was often combined with changes to the subject of the prompt (cf. \autoref{fig:study4:images}), such as changing \textit{``sweeping arcs''} to \textit{``deep and broad, sweeping arcs in landscapes.''}
Some participants also adapted their prompts based on what they saw in the images, though this often resulted in only minor changes to the revised images.
    For instance, in the case of the above participant, the two images of mountainous landscapes were almost identical.
Another common approach was changing prepositions in the prompts. Few participants attempted to simplify their prompts, and relatively few made changes to cardinal numbers. For instance, one participant changed \textit{``draw a bunch of circles''} to \textit{``draw at least 15 circles,''} and another participant wanted to see \textit{``lots of puffy clouds''} without specifying the exact number.

% More examples of prompts and images are depicted in \autoref{fig:study4:images}.

% Reordering

% When it comes to the workers' use of prompt modifiers, we found only a small increase in the prevalence of certain keywords and key phrases in the revised prompts.
% The one crowd worker who demonstrated knowledge of prompt modifiers in Study~3 demonstrated her prompt writing skills by adding a number of modifiers commonly used in the AI art community to her prompts:
% \begin{quote}
%     \textit{rainbow tyrannosaurus rex, \ul{prehistoric landscape}, \ul{studio ghibli}, \ul{trending on artstation}}
% \end{quote}
% Besides this one worker who used prompt modifiers in all her three prompts, we found only one other worker who added a style modifier (\textit{``real photos of [...]''}) in one prompt.
%%% ChatGPT:
%v1
% We found only a small increase in the use of prompt modifiers. One participant demonstrated their knowledge of prompt modifiers in all three prompts, while another used a style modifier ('real photos of') in one prompt.
%v2
We found that only one participant (the same as in Section \ref{sec:study3:modifiers}) demonstrated knowledge of prompt modifiers in all three of her prompts. An example written by this participants is
    \textit{``rainbow tyrannosaurus rex, \ul{prehistoric landscape}, \ul{studio ghibli}, \ul{trending on artstation}}.''
This participant used the underlined prompt modifiers which are commonly used in the AI art community.
Only one other participant used a style modifier (\textit{``real photos of [...]''}) in one prompt.
This shows a very small increase in the use of prompt modifiers among participants in between Study 2 and Study 3, even though participants were specifically instructed to improve their artworks.


% -----
\subsubsection{Participants' revised images}%
% -----
% We compared the two sets of images from each worker's prompts and analyzed whether there was an improvement in image quality.
% Selected changes in the prompts and the resulting images are depicted in \autoref{fig:study4:images}.
% %
% In our qualitative comparison of the two sets of images per worker, we find over half the sets of revised images showed no improvement in the level of details, contrast, color, distortions, watermarks, and consistency, compared to the set of images in Study 3 (see \autoref{tab:imageevaluation}).
% % About 15\% of the sets of images decreased in aesthetic quality.
% Considering all criteria, about half the sets of images stayed the same aesthetic quality, about 15\% were worse, and a third were better as compared to the previous set of images.
%%% ChatGPT:
We compared the two sets of images generated from each participant's prompts and found that over half of the revised sets showed no improvement in image quality (in terms of details, contrast, color, distortions, watermarks, and consistency).
Selected changes in the prompts and the resulting images are depicted in \autoref{fig:study4:images}.
About half of the sets remained the same, 15\% were worse, and a third were better compared to the previous set.

%Some workers were able to make improvements to the generated images.
% The main improvement was the amount of details in the images.
%    Since workers added more tokens than they removed, the prompts were longer and resulted in about a third of the images having more details.
% Some workers were able to make improvements to the images' colors and contrast due to the addition of adjectives to the prompts.
%    For instance, one worker improved the amount of details by adding \textit{``coral reef''} to the end of the prompt \textit{``scuba diver exploring unknown ocean.''} This change resulted in less blur and more details in the coral reef. The overall style and content of this image was, however, unchanged.
% About 55\% of the images had no change in subject
% In general, strong changes in the style of images were rare, with about 70\% of the revised sets of images being in the same or very similar style.
% Because workers did not use style modifiers, the revised images often resembled the initial images.
% Notably, only few workers managed to make improvements to the distortions in the image, which, however, is  <-- that's a technical issue that the worker could not solve anyway!
%%% ChatGPT:
Some participants were able to make improvements to the generated images, mainly by adding more details. Since participants added more tokens than they removed, the prompts were longer and resulted in about a third of the images having more details. Some participants also improved the images' colors and contrast by adding adjectives to the prompts. For instance, one participant improved the amount of details by adding \textit{``coral reef''} to the end of the prompt \textit{``scuba diver exploring unknown ocean.''} This change resulted in less blur and more details in the coral reef. However, strong changes in the style of the images were rare, with about 70\% of the revised sets being in the same or very similar style. Because participants did not use style modifiers, the revised images often resembled the initial images.


% About 15\% of the images were of low aesthetic quality, often consisting of text with no discernible subject (see \autoref{fig:study34-examples:b}).
% We find it was rare that these images were improved between the studies. If an improvement took place, it was often due to chance.
% For instance, the subject was completely changed in about 10\% of the images. This was often a result of the few workers who tried to have a conversation with the AI and entered a completely different prompt as input (see \autoref{fig:study34-examples:b} and \autoref{fig:study4:examples:e}).
%%% ChatGPT:
About 15\% of the images were of low aesthetic quality, often consisting of text with no discernible subject (see \autoref{fig:study34-examples:b}).
These images were rarely improved between the studies, and when they were, it was often due to chance.
For instance, the subject was completely changed in about 10\% of the images. This was often a result of participants trying to have a conversation with the AI and entering a completely different prompt as input  (see \autoref{fig:study34-examples:b} and \autoref{fig:study4:examples:e}).


% -----
\subsubsection{Participants' use of negative terms}%
% -----
% Nineteen participants made use of negative terms of which eight participants used negative terms in all three prompts.
% In total, we collected 39~negative terms.
%%% ChatGPT:
Nineteen participants used negative terms, with eight using them in all three prompts. In total, we collected 39~negative terms.
%
Many negative terms ($n=19$) aimed at removing or modifying the subject in various ways, such as removing \textit{``rocks''} from a beach, trying to correct a \textit{``weird face,''}  avoiding a \textit{``Nude, Naked, White, Man,''} removing the color \textit{``Green.''} in the image of a red star, or attempting to change the subject entirely (\textit{``ballroom''}).
Participants tried to change the style of the images in eight cases, using terms such as \textit{``Black, White, Colorless, Monochromatic,''} \textit{``opaque, solid,''} and \textit{``unfocused.''}
Four out of the 50~participants tried to remove text in the images, using negative terms such as \textit{``letters,''} \textit{``captions,''} and \textit{``text.''}
Only one participant attempted to remove watermarks, using the negative term \textit{``remove watermark.''}
As can be seen in this prompt, some participants did not understand the concept of negative terms, even though we explained it to them.
A few examples of failed and successful attempts are depicted in \autoref{fig:study4:negativeterms}.
Some of the image generations failed, because the participant did not use the negative term correctly. For instance, the prompt on the bottom right of \autoref{fig:study4:negativeterms} contains a monarch butterfly both in the prompt and negative term. The resulting image is sub-par compared to the image generated from the participant's original prompt.%
% ----
%TC:ignore
\newlength{\negwidth}%
\setlength{\negwidth}{1.8cm}%
%
\noindent%
\begin{figure}[bht]%
\centering%
\begin{tabularx}{\textwidth}{ccXccX}%
    \small before & \small after & \small revised prompt & \small before & \small after & \small revised prompt
\\
  \includegraphics[width=\negwidth,valign=t]{figures/study4/A1OC6IOF1JHM2F/before/0000.jpg}%
    &
  \includegraphics[width=\negwidth,valign=t]{figures/study4/A1OC6IOF1JHM2F/after/0000.jpg}%
    &
    \scriptsize
    % A very bright, slowly dying star. 
    % \newline
    An explosively bright, dying star.
    \newline
    [Green]
&
  \includegraphics[width=\negwidth,valign=t]{figures/study4/A3JHXEH79DJ83L/before/prompt2/0000.jpg}%
    &
  \includegraphics[width=\negwidth,valign=t]{figures/study4/A3JHXEH79DJ83L/after/prompt2/0000.jpg}%
    &
    \scriptsize
    Hummingbird over red flower with a out of focus background of green shrubs  
    \newline
    [Hummingbird hovering over red flower with short focal point.]
    
\\[.25\baselineskip]

  \includegraphics[width=\negwidth,valign=t]{figures/study4/A3CMBECZOKDVO7/before/0000.jpg}%
    &
  \includegraphics[width=\negwidth,valign=t]{figures/study4/A3CMBECZOKDVO7/after/0000.jpg}%
    &
    \scriptsize
    medium sized metallic sphere slightly above and left of the center of the image
    \newline
    [remove watermark]
&
  \includegraphics[width=\negwidth,valign=t]{figures/study4/A3JHXEH79DJ83L/prompt3/before/0003.jpg}%
    &
  \includegraphics[width=\negwidth,valign=t]{figures/study4/A3JHXEH79DJ83L/prompt3/after/0003.jpg}%
    &
    \scriptsize
    Rainbow colored unicorn with huge mane jumpin over the full moon  
    \newline
    [Shimmering hairy unicorn jumping over the full moon]
    
\\[.25\baselineskip]

  \includegraphics[width=\negwidth,valign=t]{figures/study4/A28HB7240OFGEW/before/0001.jpg}%
    &
  \includegraphics[width=\negwidth,valign=t]{figures/study4/A28HB7240OFGEW/after/0001.jpg}%
    &
    \scriptsize
    An autumn day in a colorful forest  
    \newline
    [Dark Circle]
&
  \includegraphics[width=\negwidth,valign=t]{figures/study4/A3JHXEH79DJ83L/before/0002.jpg}%
    &
  \includegraphics[width=\negwidth,valign=t]{figures/study4/A3JHXEH79DJ83L/after/0002.jpg}%
    &
    \scriptsize
    Butterfly Monarch on a flower 
    \newline
    [Monarch butterfly alit on a wildflower]
\\
\end{tabularx}%
\caption{Examples of successful (left) and failed (right) attempts of participants' using negative terms (in square brackets).}
\Description{Examples of successful and failed attempts of participants' use of negative terms (in square brackets).}
\label{fig:study4:negativeterms}%
\end{figure}%
%TC:endignore
% ----





% ====================
\section{Discussion}%
\label{sec:discussion}%
% ====================

% Our paper contributes an investigation on the potential of crowd workers to contribute to this novel creative economy.

In three studies, we explored the skill of prompt engineering with participants recruited from a crowdsourcing platform.
%%% Study 1
% Our pilot study probed the workers' knowledge of Fine Art. We found about 30\% of the workers had excellent knowledge of artists and artwork styles. This knowledge could, in theory, enable the workers to write effective prompts for generating digital artworks.
%
%%% Study 2
Our first study shed light on whether people have an understanding of what makes a ``good'' prompt.
Interestingly, participants were better able to identify the quality of prompts than images.
This is surprising, given the difficulty of the task of imagining the visual outcome of a prompt.
% The presence of style modifiers may have been an indicator for workers to rate these prompts higher than prompts without prompt modifiers.
%%%ChatGPT:
The presence of style modifiers may have influenced participants to rate these prompts higher than prompts without style modifiers.
%
Our findings nevertheless indicate that participants can assess the quality of prompts and respective images. This ability increased with the participants' experience and interest in art.

%%% Study 3
In the subsequent two studies on writing and improving prompts, we found that many participants were creative and described artworks in rich descriptive language which may result in beautiful digital artworks.
However, only a negligible amount of participants intentionally used terms commonly applied in communities of text-to-image art, such as Midjourney and Stable Diffusion. With the exception of one participant, these specific keywords are not (yet) part of the vocabulary of crowd workers on MTurk. While the prompts written by participants were very descriptive and, in some cases, resulted in beautiful and interesting images, participants left the style of their image to chance. The prompts were missing modifiers that would more tightly control the style and quality of the image generation.
%%% Study 4
This applies to both the initial study on writing prompts and the study on revising and improving prompts.
In the latter, only a minority of the participants were able to improve their revised images, while most of the images remained about the same quality.
% None of the participants intentionally aimed to produce artworks with their revised prompts.
An overwhelming majority of participants left the style of the generated images to chance, even though they were instructed to create ``artworks.''


% --------------------
\subsection{Prompt Engineering as a Learned Skill}%
\label{sec:learnedskill}%
% --------------------

Our work adds to the discussion of broader research questions in the AI research community:
    Can anyone become an artist with prompt engineering?
    Is prompt engineering a skill that is intuitive to us humans or is it a learned skill?
    How steep is the learning curve to prompt engineering?
    If prompt engineering is an intuitive skill, how intuitive is it?
%
These research questions have implications for the future of work and human-computer co-creativity~\cite{062-iccc20.pdf}.
If prompt engineering is an intuitive human skill, then we can look forward to a bright future where anybody can work in creative professions without having to develop special skills.
But if prompt engineering is a learned skill, its application could become limited to highly trained and skilled class of professionals who have mastered to speak the language of the generative model.
The latter case would clearly negatively impact creative production and stifle innovation.
% , and could even lead to % civil unrest if traditional creative professions are disrupted by generative systems.
% polarization in society if parts of the population have no means and access to creative production.}

% Considering future complex scenarios beyond the generation of images, prompt engineering could turn out either way.


% On the one hand,
Prompting is a language-based practice and the use of language is intuitive to us humans. Therefore, one could assume that prompting is an intuitive skill.
It is easy to get started with writing prompts and prompting has a large potential in different fields and for many application domains.
%
However, our study found that effective prompt writing requires knowledge of keywords and key phrases. These prompt modifiers are an essential part of the skill of prompt engineering for AI generated art.
Typically, these keywords and key phrases are acquired through iterative experimentation and by learning from prompts shared in dedicated resources, on social media, or in online communities~\cite{aiartcreativity}.
Our studies empirically confirm that style modifiers are unknown to participants recruited 
% the majority of workers on
on Amazon Mechanical Turk. Prompt modifiers that are being used profusely in the AI art community have not found their way into the collective vocabulary of workers on MTurk.
Participants in our study struggled to write and improve their prompts for the specific task of creating digital artworks.
This points towards prompt engineering being a learned skill or perhaps even a specialist skill, as we discuss in the following section.



% --------------------
% \subsection{Crowd Workers as Participants in Prompt Engineering Experiments}%
% \label{sec:potential}%
% --------------------

% If style information was present in the prompt, it was often accidental (e.g., as part of an instruction to the AI) or not used in a controlled way.



% --------------------
\subsection{On the Future of % Crowdsourced
Creative Production with Prompt Engineering}%
\label{sec:creativeeconomy}%
% --------------------
%
% Much attention in the scholarly debate is being paid to novel computational ways of generating images from text.
%
Text-to-image generation opens new opportunities for creative production of digital images and artworks.
Whether prompt engineering will become an expert skill or even a novel profession is still open.
%
In this section, we speculate on four possible futures
% for creative production with
of prompt engineering.
% For each of the four scenarios, we describe how our findings xxxxxxxxxxxxxx.
% % crowd workers would potentially be affected.%
%
%%% 1) expert skill
% -----
\subsubsection{Prompt engineering as an expert skill}%
% -----
% Prompt engineering could become an expert skill that requires deep subject-matter expertise (e.g. knowledge of keywords and prompt modifiers, but also of the training data and system configuration settings) to effectively control the output of the generative system.
% This skill could become exclusive to a narrow caste of highly trained individuals.
    % An analogical development can be seen today in the area of machine learning (ML) which, in recent years, moved towards ``foundation models'' \cite{FoundationModels}. Foundation models are very large models that are costly to train, operate, and maintain. Consequently, research on cutting-edge foundation models is limited to a small number of well-financed research institutes in academia and industry who employ highly-skilled professionals.
    % For less well equipped institutions and less well-trained researchers and professionals, access to cutting-edge research may become more and more restricted.
    %
% If prompt engineering becomes a highly skilled profession in the future, it may become a skill that is exclusive to a narrow set of privileged individuals who had to undergo extensive training to acquire their expertise.
%%% ChatGPT:
In the future, prompt engineering could become an expert skill that requires deep subject-matter expertise (e.g. knowledge of keywords and prompt modifiers, but also of the training data and system configuration settings) to effectively control the output of generative systems. This is similar to the move in the field of machine learning towards ``foundation models'' \cite{FoundationModels}.
Foundation models are very large and costly to train, operate, and maintain. As a result, research on these models is limited to a small number of well-financed research institutes that employ highly-skilled professionals. If prompt engineering becomes a highly skilled profession, it may become exclusive to a narrow group of privileged individuals who have undergone extensive training.

% In this scenario, laypeople would be at risk of being excluded from using generative technologies unless they develop the expertise and skills necessary for prompt engineering.
% % <RELATE THIS TO THE FINDINGS OF THIS PAPER>
% However, our paper finds that people are creative and able to use rich descriptive language. While participants in our study were missing the specialist vocabulary of a prompt engineer, they could still operate text-to-image generation systems, even though the results were often left to chance.
% % contribute to prompt engineering in other non-expert ways --- for instance, by providing creativity support or evaluating generative designs.

%%% 2) everyday skill
% -----
\subsubsection{Prompt engineering as an everyday skill}%
% -----
% Prompt engineering could become an everyday practice.
% In this scenario, humans would adapt their practices and language to facilitate effective interaction with the AI, simply because it is a skill that is needed in everyday life.
%
% One main application area for text-to-image generation in this scenario is creativity support and generative design.
% Humans have a need for visual content and AI generated content may satisfy this need, from internet memes to the design of birthday cakes, personalized greeting cards, and logos to the creation of beautiful and personally meaningful artworks.
%
% Prompt engineering could also be practiced for self-actualization, creativity, and art therapy for reflection and improving mental health and subjective well-being~\cite{BURLESON2005436}.
%
% In this scenario, humans would expand their vocabulary to include terms used in prompt engineering to be able to produce meaningful outcomes with generative systems. Learning prompt engineering would, in this case, be akin to learning a new language. Prompting could even become a part of media literacy in the school curriculum.
    % One could, for instance, think of a scenario in which users write prompts for generative search engines \cite{ICCC_2021_paper_50.pdf}.
    % In recent weeks, there have been users seeking inspiration from DALL-E for birthday cake designs \footnote{https://twitter.com/entaq/status/1541060553124225024}, producing ideas\footnote{https://twitter.com/PaulYacoubian/status/1514955904659173387}. In the future, one could imagine more complex scenarios in which users conjure entire virtual worlds to explore.

%%% ChatGPT
In the future, prompt engineering could become a common practice.
In this scenario, people would adapt their creative practices and language to facilitate effective interaction with AI because it is a skill that is needed in everyday life.
% One main application of text-to-image generation in this scenario would be creativity support and generative design.
People have a need for visual content, and AI-generated content could satisfy this need, from internet memes to the design of greeting cards, logos, and artworks. Prompt engineering could also be used for self-actualization, creativity, and art therapy to improve mental health and well-being ~\cite{BURLESON2005436}.
In this scenario, people would expand their vocabulary to include terms used in prompt engineering in order to produce meaningful outcomes with generative systems. Learning prompt engineering would be similar to learning a new language, and it could even become part of media literacy education in schools.

%     As in the previous scenario, crowd workers would need to adapt and learn this language if they wanted to participate in creative crowdsourcing tasks.
%     However, with prompt engineering being an everyday practice, workers would likely apply this practice in their own everyday live, and would have little struggle to apply it in online crowdsourcing tasks.

%%% 3) irrelevant skill
% -----
\subsubsection{Prompt engineering as an obsolete skill}%
% -----
% Prompt engineering could lose its relevance as generative systems become better and better at understanding the intent of users.
% In the scholarly literature, this problem is known as AI alignment~\cite{Gabriel2020_Article_ArtificialIntelligenceValuesAn.pdf}. The state-of-the-art of image generation systems, such as Google's Imagen~\cite{2205.11487.pdf} and OpenAI's DALL-E~2 \cite{DALLE2}, demonstrate an impressive performance in understanding the meaning of textual input prompts and the user's intent. With these systems, novices and experts alike can generate photo-realistic images and beautiful digital artworks from textual prompts.
% As generative systems become better at understanding input prompts,  prompt engineering could become irrelevant --- a skill that does not require expert skills and training.
% Unlike in the above scenario in which humans adapt their practices to the machine, the machine would adapt to humans in this scenario.
% As a result, the generative machine would develop ubiquity, with an
% ``intimate'' relationship to its users \cite{p75-weiser.pdf} and a perfect understanding of user intent.
%%% ChatGPT:
In the future, prompt engineering could become irrelevant.
% Karpathy called prompt engineering ``Software 3.0''.
% Sam Altman called prompt engineering a bug than a feature that relies on % arcane non-intuitive conjurations of prompt modifiers.
Prompt engineering can be seen as the smell of a half-baked product that does not solve its users' needs.
As generative systems improve their ability to understand the intent of users, prompt engineering could be a short-lived trend.
The problem of aligning AI with human intent is known as AI alignment in the scholarly literature \cite{Gabriel2020_Article_ArtificialIntelligenceValuesAn.pdf}.
% is already being addressed by state-of-the-art image generation systems like Google's Imagen~\cite{2205.11487.pdf} and OpenAI's DALL-E 2 \cite{DALLE2}.
State-of-the-art systems, such as ChatGPT \cite{chatgpt} and DALL-E 2 \cite{DALLE2}, demonstrate impressive performance in understanding textual input prompts and user intent. With these systems, users of all skill levels can generate % realistic images and digital artworks
content from textual prompts. As generative systems become better at understanding input prompts, prompt engineering could become unnecessary --- an archaic skill that does not require expert training and that only few people exercise for nostalgic reasons. In this scenario, the machine would adapt to humans, rather than the other way around.
The generative machine would develop an ``intimate'' relationship \cite{p75-weiser.pdf} with its users and a perfect understanding of user intent.

% meta models in the background anticipating the intent of users
% relate this to crowd workers
% % In this case, crowd workers could participate in the creative production of digital artifacts just like anybody else without any barriers of entry.
% Our study showed that certain keywords will likely always be needed to control the style of images generated with text-to-image systems.
% % relate this to our study
% These style modifiers had not entered the conscious practice of our studies' participants, yet.

%%% 4) finetuning skill
% -----
\subsubsection{Prompt engineering as personal signature or curation skill}%
% -----
    % AI artist Mario Klingemann has speculated that with the impending flood of AI generated images, our human senses will --- over time --- become sharpened in distinguishing hand-crafted pieces of art from AI generated digital art\footnote{https://twitter.com/quasimondo/status/1512769106717593610}.
    % This skill would help us humans to notice fine nuances, details, and imperfections in AI generated art. These nuances might not matter as much today,
    %     % as we are entering an age of ubiquitous AI generated media,
    % but, in the future, could be a decisive factor of whether a piece of art is considered to be of high aesthetic quality.
    % In this scenario, everybody could write prompts for generative systems with good results, but only few would develop mastery in prompt engineering. The practice of prompt engineering would remain a necessary skill for applying finishing touches and optimizing generative results with the additional aim of imbuing an artwork with a personal style and therefore distinguishing the generated results from bland ``off-the-shelf'' generations.
    % Classic fine art education first start out with mediums and developing a good eye. After that, focus on developing own style.
    % Internet is more fast-paced and styles can be applied effortlessly by including style modifiers in prompts.
    %     PRO-AM
    %     NFT
% -----
% \subsubsection{Prompt engineering as a curation skill}%
% -----
    % Or, perhaps, prompt engineering could evolve into a curation skill --- a personal practice in which everyone has their own sets of curated textual and visual input prompts used to fine-tune generative models for different downstream purposes.
    % <RELATE THIS TO WORKERS>
    % If crowd workers were to participate in the creative production with prompt engineering in these scenarios, they would need to learn the relevant skills.
    % In particular the keywords and prompt modifiers would need to be learned.
    % <RELATE THIS MORE TO THE FINDINGS OF THIS PAPER>

%%% ChatGPT:
In the future, our human senses could become better at distinguishing hand-crafted art from AI-generated digital art.
AI artist Mario Klingemann speculated that with the influx of AI-generated images, this skill would help us notice subtle nuances, details, and imperfections in AI-generated art, which could become more important in determining the aesthetic quality of an art piece.\footnote{https://twitter.com/quasimondo/status/1512769106717593610}
In this scenario, anyone could write prompts for generative systems with good results, but only a few would become masters of prompt engineering. The practice of prompt engineering would remain a necessary skill for applying finishing touches and optimizing generative results, as well as imbuing an artwork with a personal style to distinguish it from bland ``off-the-shelf'' generations. Alternatively, prompt engineering could evolve into a curation skill~--- a personal practice in which everyone has their own curated sets of textual and visual input prompts used to fine-tune generative models for different purposes.
Current systems that cater towards this future are Dreambooth \cite{dreambooth} and the method of textual inversion \cite{textualinversion}.

% In this case, it is not clear if one would want to engage crowd workers in prompt engineering.
% Each worker would have their own signature or curation practice that would result in vastly different results. Perhaps,
%  % in this scenario. crowd workers could still be hired if they followed the signature and curation practices of the ``employer'' or they could
% crowd workers could still be hired to sample different personally curated styles for inspiration and creativity support.



In the following, we formulate recommendations for involving paid crowd workers in research on text-to-image systems.

% --------------------
% \subsection{Implications for the Design of Text-to-image Systems}%
\subsection{Recommendations for Conducting Text-to-Image Experiments with Crowd Workers}%
\label{sec:implications}%
% --------------------
%
% Our findings show that crowd workers are creative and can write prompts in rich descriptive language. However, workers need to be trained in prompt engineering and need to develop knowledge of prompt modifiers. In this section, we provide ten recommendations for conducting experiments on text-to-imagine generation with an online crowd.
%%% ChatGPT:
In this section, we reflect on our experiences with conducting experiments on prompt engineering with a paid crowd on Amazon Mechanical Turk.
Our research has found that crowd workers are capable of coming up with creative prompts written in descriptive language. However, it is important for workers to receive training in prompt engineering in order to gain a better understanding of how to use prompt modifiers. We have put together ten recommendations for conducting experiments on text-to-imagine generation with a paid online crowd.
% These recommendations can help ensure that an experiment is successful and produces the desired results.

\subsubsection{Recruit the right crowd}%
Some workers on crowdsourcing platforms enjoy and even seek out creative crowdsourcing tasks \cite{CHI20}.
Workers who intrinsically enjoy writing stories or other types of creative crowdsourcing tasks are likely to produce better results with text-to-image generation systems. We see that in some of the workers in our sample who invested effort into writing detailed and descriptive prompts, while others consistently produced images of low aesthetic quality.
It is therefore important to recruit the right crowd, for instance by using qualification tasks.

\subsubsection{Provide guidance and clear task instructions}%
Workers in our studies left the style of the generated images to chance, even though they were instructed to create ``artworks.''
Therefore,
% workers will need to be carefully guided in the task instructions.
the task instructions should include clear and detailed explanations of what is expected from the worker, and how the worker can achieve these goals.
This includes explaining what prompts are, how they should be phrased, and how to correctly include prompt modifiers in prompts.
% Whether workers can come up with their own modifiers is still open to future work. Our study found that workers do not seem to apply them on their own.

\subsubsection{Explain keywords and key phrases to workers}%
Only a few workers in our studies intentionally used prompt modifiers. Keywords and key phrases used in the AI art community were virtually unknown to workers (with one exception).
While we expect this situation to improve as the popularity of text-to-image systems increases, workers will need to receive explanations and examples of prompt modifiers for workers to be able to use them.
Task instructions should be illustrated with exemplars of prompts and resulting images to educate workers before they write their first prompt.

% \subsubsection{Provide example images in the task instructions}%
% Prompt engineering is a skill that is learned from repetition and seeing the effect of prompts on images.
% Task instructions should be illustrated with exemplars of prompts and resulting images to educate workers before they write their first prompt.

\subsubsection{Be mindful of lengthy task instructions}%
While detailed instructions are needed, long instructions may slow down workers. Some workers may even ignore long instructions.
An alternative to lengthy instructions is to design instructions as a playful tutorial to train workers.
Another approach would be to use the idle time between image generations for providing useful information to workers.
    OpenAI's DALL-E interface, for instance, provides helpful tips while one waits for the image generation to finish. This principle could be applied to crowdsourcing tasks as well.


\subsubsection{Explain negative prompts}%
Some workers in our sample did not understand the concept of negative terms, even though we explained it to them and provided an illustrative example.
Crowdsourcing tasks should include a carefully designed explanation of how negative terms work, ideally with example images that demonstrate the effect of negative terms.

\subsubsection{Create dedicated user interfaces}%
% As an alternative to teaching workers prompt modifiers, workers could be provided user interfaces that abstract away from the complexity of the open-domain nature of prompting. An example of such an interface is MindsEye\footnote{https://multimodal.art/mindseye}, an interface built on top of Colab notebooks. This interface was created specifically for novices with usability in mind. MindsEye users can change image styles with this user interface by simply clicking buttons.
%%% ChatGPT
Instead of training workers in the use of prompt modifiers, it may be more effective to provide them with user interfaces that hide the complexity of open-domain prompting. One example of this is MindsEye\footnote{https://multimodal.art/mindseye}, which is built on top of Colab notebooks and is designed with usability in mind. With this interface, even novice users can easily change image styles by simply clicking buttons, without needing to understand the underlying mechanics of prompt modifiers. This can help make the process of generating images from text more accessible and user-friendly.

\subsubsection{Provide means for fast and iterative image generation}%
% The workers' workflow should be carefully designed to facilitate text-to-image generation. Remixing and forking could be suitable means to simplify prompt engineering for workers.
% For instance, Artbreeder\footnote{https://www.artbreeder.com} makes it easy for its users to adjust image generation settings and to create variations and fork other users' images.
%%% ChatGPT
In order to effectively facilitate the process of text-to-image generation, it is important to carefully design the workflow for the workers. One way to do this is by using techniques like remixing and forking, which can help simplify the process of prompt engineering for the workers. For instance, the AI art tool Artbreeder\footnote{https://www.artbreeder.com} makes it easy for users to adjust image generation settings and to create variations and fork other users' images. This can help streamline the process and make it more efficient for workers.

\subsubsection{Provide visual feedback to workers}%
Workers in our studies wrote the first set of prompts without feedback. Immediate feedback would likely improve the workers' performance in subsequent image generations.
% The quicker feedback can be provided, the better for the worker.

\subsubsection{Allow a training period in which workers can experiment with the text-to-image system}%
% If batches of microtasks are assigned to workers, workers will likely need some time to understand the task.
% This could result in a warming up period and a training effect that could negatively affect the first image generations of the worker.
% The design of the crowdsourcing campaign should account for these failed attempts, for instance by excluding the workers' first attempts from further analysis.
%%% ChatGPT:
When workers are assigned batches of microtasks, they may need some time to understand what they need to do. This can lead to a ``warming up'' period and a training effect, where the worker's initial attempts at generating images are not as good as their later attempts.
To account for this, the design of the crowdsourcing campaign should include measures to exclude the workers' first attempts from further analysis. This can help ensure that the results of the experiment are not biased by the workers' learning process.


% Some people are better at searching, adding keywords to queries, such as Reddit to find conversations on a topic --- analogy to prompt engineering.



% --------------------
\subsection{Limitations}%
\label{sec:limitations}%
% --------------------
%
We acknowledge a number of risks to the validity of our exploratory studies.
% \subsubsection{Study 1}%
% Crowd workers have been shown to have developed specific expertise in correctly answering tasks~\cite{Hauser-Schwarz2016_Article_AttentiveTurkersMTurkParticipa.pdf}. It is possible that workers in our pilot study may have used a reverse image search to complete the task despite our instructions not to use a search engine.
% One reason for this potential behavior are the power dynamics prevalent on MTurk. Workers are under pressure to perform well and need to avoid punitive measures from requesters to not curb future work opportunities.
% Further, workers may have understood our task as a pre-qualification for future tasks. Performing well on this task may, in the mind of the worker, lead to invitations in future art-related studies.
% Prior work has shown that there is a subset of workers who enjoy and seek out creative tasks \cite{CHI20} and our results may, therefore, potentially be biased towards workers who self-select to complete such tasks.
% As is often the case in crowdsourcing research, our results may be biased towards those workers who seek out survey tasks and creative tasks.
% Due to the self-selective nature of crowdsourcing platforms, we do not claim that our findings generalize to all crowd workers on Amazon Mechanical Turk.

% \subsubsection{Study 2}%
% % challenges of the assessment images \cite{joshi_1.pdf}
Aesthetic quality assessment is a highly subjective task \cite{joshi_1.pdf}.
Many factors can affect ratings of aesthetic quality, such as personal values, personal background, the interestingness and content of the image, contrast, proportion, number of elements, novelty, and ap\-pro\-pri\-ate\-ness~\cite{joshi_1.pdf,1606.01621.pdf,DS77_158.pdf}.
We acknowledge that the task given to workers was hard, especially when it comes to imagining the visual outcome of a written prompt.
% % "the contribution of particular photographic attributes to making an image aesthetically pleasing depends on the thematic content" \cite{1606.01621.pdf}
% % However, classifying images with high and low aesthetics is a relatively easy task~\cite{1606.01621.pdf},
% %     % "images with obviously visible high- or low- aesthetics are relatively easy to classify" \cite{1606.01621.pdf}
% % and we utilized this in our study.
We did, however, find that workers were able to tell the difference between low quality and high quality prompts. %, and classifying images with high and low aesthetics is a relatively easy task~\cite{1606.01621.pdf}.

% Selecting a sample of prompts from the set of Midjourney images for Study~2 --- and especially failed image generation attempts --- was a challenge.
% State-of-the-art image generation systems are ``greedy'' --- they will try to turn any given textual input into aesthetically pleasing images~\cite{aiartcreativity}. Midjourney, in particular, has been created to make it easy for novices to generate stunning images, even from short words and emojis.
% % Some of the prompts included in Study~2 produced interesting results without the (excessive) use of prompt modifiers.
% % We acknowledge that our selection of prompts and images for this study may not be ideal. The first author is not an artist, and there are members on Midjourney who are far better at prompt engineering than the author.
% We acknowledge that selection of stimuli affects the results obtained.
%     % (also exacerbated by the chat-based linear content presentation in the Midjourney community).
% % While the first author is by no means an artist and there are members on the Midjourney community who are far better prompt engineers, 
% % We do not claim our images are representative of the prompts written on Midjourney. % , nor do we claim that our results transfer to images created with other systems.
% % We do not claim that the selected images are representative of the wide range of textual inputs that can be formulated in natural language.
% % However, we evaluated the images with 10 raters and our choice of prompts and images was therefore independently validated.
% However, we believe the prompts and images demonstrate a range of aesthetic quality that is measurable and we confirmed this with ten independent raters who helped us select images for our study.%


% \subsubsection{Studies 3 and 4}%
% Several weeks passed between Study 3 and 4 and we cannot control whether workers became interested in text-based image generation in between the studies.
% % Another factor affecting this is the growing popularity of image generation systems with recent months seeing the release of openly available advanced systems (e.g., DALLE-E mini).
% The growing popularity of openly available image generation systems, such as DALL-E mini, could have led the worker to try out image generation systems and even develop expertise in writing prompts in between our studies.
% However, our data indicates that this was not the case.
% %  learning or habituation from repeated exposure
We further acknowledge limitations in our choice of text-to-image system.
Our %overriding
main motivation for selecting Latent Diffusion was to select a deterministic system that produces reproducible results. This allowed us to assess the effect of changes in the prompt on the images.
Note, however, that while Latent Diffusion is a powerful image generation system, it may respond differently to style keywords than CLIP-guided systems.
% suffers from an issue that is present in many of the current image generation systems. The system produces images that contain text and watermarks. This may have biased our results.
% we see no signs of that in our data. Only few workers attempted to remove the text and watermarks.
% We acknowledge that especially the CLIP-guided systems may have been a better playground for workers to try out their skills of prompt engineering. However, while there are systems that are less affected by the duplication issue (e.g., DALL-E mini),
However, only one participant used specific keywords (prompt modifiers) in our study. The second round of images was also never shown to participants. Therefore, we can safely assert that the choice of system had no effect on how participants wrote and revised their prompts.%
%


% \input{FUTUREWORK}


% ====================
\section{Conclusion}%
\label{sec:conclusion}%
% ====================


The past few years have seen the rise of generative models.
It is too early to tell whether this development will give birth to new professions, such as ``prompt engineer.''
However, generative AI will deeply affect and reconfigure the fabric of our society.
This opens exciting opportunities for research in HCI.

This article investigated prompt engineering for AI art.
In three studies, we investigated whether novice participants could recognize the quality of prompts and their resulting images and whether participants could write and improve prompts.
% We found participants recruited from Amazon Mechanical Turk are creative and able to write prompts for text-to-image systems in rich descriptive language. 
We found participants recruited from Amazon Mechanical Turk are creative and able to write prompts for text-to-image systems in rich descriptive language, but lacked the special vocabulary found in AI art communities. The use of prompt modifiers was not intuitive to participants, pointing towards prompt engineering being a learned skill.
% for the generation of digital artworks.
We provided recommendations for conducting scientific experiments on prompt engineering and text-to-image generation with participants recruited from crowdsourcing platforms.
    % Based on our findings, we provided recommendations for conducting research on text-to-image generation with participants recruited from crowdsourcing platforms
% This article investigated prompt engineering for AI art with participants recruited from a paid crowdsourcing platform.
%
% In this paper, we explored whether crowd workers have the potential to participate in text-to-image generation.
% For crowd workers on crowdsourcing platforms to play a meaningful role in the emerging AI-driven creative ecosystem, workers must be able to participate in the novel text-based mode of creative production.
% Our % extensive
% four studies point toward crowd workers having the potential of contributing to prompt engineering.
    % Particiapnts in our studies overall lacked the special vocabulary used in the online communities for the generation of digital artworks.
    % % Workers were not controlling the style and quality of the image generation. Keywords and phrases that are being profusely used in the AI art community were virtually unknown to workers.
    % Participants in our study were also not able to significantly improve their written prompts.
% Based on our findings, we provided recommendations for conducting research on text-to-image generation with participants recruited from crowdsourcing platforms, and discussed four possible futures of prompt engineering.
% We conclude that prompt engineering is a learned skill that is learned with practice.
% Economic growth is driven  ``from  people  creating  ideas''
% "Economic  growth  arises  from  people  creating  ideas." 
% \cite{IdeaPF.pdf}.
We speculated on four possible futures for prompt engineering.
We hope that whatever the landscape of creative production will turn out to be in the future, it will be an inclusive creative economy in which everyone % crowd workers
can participate in meaningful ways.%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% \newpage

% \pagestyle{empty}

% \section*{Disclosure of Interest} 
% The authors report there are no competing interests to declare.
%%%chatGPT:
% The authors have no competing interests.
% The research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest.

% \section*{Funding} 
% This work was not supported by funding or grant-awarding bodies.

\section*{Data availability statement}
Data related to the study is available at\\ 
%NON-ANON:
\href{https://osf.io/bjwf4/?view_only=caf73282354643e9bfb34b3b05ef4b62}{https://osf.io/bjwf4/?view\_only=caf73282354643e9bfb34b3b05ef4b62}
%ANON:
% \href{https://osf.io/bjwf4/?view_only=d478a65d003544d581bb45bca3b14657}{https://osf.io/bjwf4/?view\_only=d478a65d003544d581bb45bca3b14657}

% \newpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% \begin{figure}[h]
%   \centering
%   \includegraphics[width=\linewidth]{sample-franklin}
%   \caption{1907 Franklin Model D roadster. Photograph by Harris \&
%     Ewing, Inc. [Public domain], via Wikimedia
%     Commons. (\url{https://goo.gl/VLCRBB}).}
%   \Description{A woman and a girl in white dresses sit in an open car.}
% \end{figure}

% \begin{table}
%   \caption{Frequency of Special Characters}
%   \label{tab:freq}
%   \begin{tabular}{ccl}
%     \toprule
%     Non-English or Math&Frequency&Comments\\
%     \midrule
%     \O & 1 in 1,000& For Swedish names\\
%     $\pi$ & 1 in 5& Common in math\\
%     \$ & 4 in 5 & Used in business\\
%     $\Psi^2_1$ & 1 in 40,000& Unexplained usage\\
%   \bottomrule
% \end{tabular}
% \end{table}

% \begin{table*}
%   \caption{Some Typical}
%   \label{tab:example}
%   \begin{tabular}{ccl}
%     \toprule
%     Command &A Number & Comments\\
%     \midrule
%     \texttt{{\char'134}author} & 100& Author \\
%     \texttt{{\char'134}table}& 300 & For tables\\
%     \texttt{{\char'134}table*}& 400& For wider tables\\
%     \bottomrule
%   \end{tabular}
% \end{table*}

% \begin{verbatim}
%   \begin{teaserfigure}
%     \includegraphics[width=\textwidth]{sampleteaser}
%     \caption{figure caption}
%     \Description{figure description}
%   \end{teaserfigure}
% \end{verbatim}

%% The acknowledgments section is defined using the "acks" environment
%% (and NOT an unnumbered section). This ensures the proper
%% identification of the section in the article metadata, and the
%% consistent spelling of the heading.
% \begin{acks}
% To Robert, for the bagels and explaining CMYK and color spaces.
% \end{acks}

%% The next two lines define the bibliography style to be used, and
%% the bibliography file.
%TC:ignore
\bibliographystyle{ACM-Reference-Format}
\bibliography{manuscript}
%TC:endignore

%TC:ignore
\appendix
\newpage
\section{Set of images used in Study 1}
\label{appendix:images}
%TC:endignore

% ------------
%TC:ignore
\newlength{\imageheight}
\setlength{\imageheight}{4.5cm}
\newlength{\skipspace}
\setlength{\skipspace}{.1cm}
%TC:endignore
% ------------

%TC:ignore
\subsection{Images with High Aesthetic Appeal}
\label{appendix:a1}
% --------------------

%TC:ignore
\noindent
\begin{tabularx}{\textwidth}{
		>{\hsize=.245\hsize}X
		>{\hsize=.245\hsize}X
		>{\hsize=.245\hsize}X
		>{\hsize=.245\hsize}X
	}
	% H1
	\vspace{0pt} \includegraphics[width=.24\textwidth,valign=t]{figures/midjourney/25a8c8c9-55c2-48ac-af30-3e1333eb6b76x.jpg}
	&
	% H4
	\vspace{0pt} \includegraphics[width=.24\textwidth,valign=t]{figures/midjourney/255acf24-263a-4625-93c1-b5757b30e702x.jpg}
	&
	% H5
	\vspace{0pt} \includegraphics[width=.24\textwidth,valign=t]{figures/midjourney/0f0c96fc-2160-4e59-8331-1bd1bd619ad5x.jpg}
	&
	% H6
	\vspace{0pt} \includegraphics[width=.24\textwidth,valign=t]{figures/midjourney/2be06581-4f12-4b8b-87a4-f77b9e4b375cx.jpg}
	\\
	\footnotesize
	\textbf{H1:} \prompt{25a8c8c9-55c2-48ac-af30-3e1333eb6b76x.jpg}
	&
	\footnotesize
	\textbf{H4:} \prompt{255acf24-263a-4625-93c1-b5757b30e702x.jpg}
	&
	\footnotesize
	\textbf{H5:} \prompt{0f0c96fc-2160-4e59-8331-1bd1bd619ad5x.jpg}
	&
	\footnotesize
	\textbf{H6:} \prompt{2be06581-4f12-4b8b-87a4-f77b9e4b375cx.jpg}
	\\
\end{tabularx}

\noindent
\begin{tabularx}{\textwidth}{
		>{\hsize=.33\hsize}X
		>{\hsize=.33\hsize}X
		>{\hsize=.33\hsize}X
	}
	% H2
	\vspace{0pt} \includegraphics[width=.325\textwidth,valign=t]{figures/midjourney/a7cbe696-4af4-4af6-9224-c4447c29831cx.jpg}
	&
	% H7
	\vspace{0pt} \includegraphics[width=.325\textwidth,valign=t]{figures/midjourney/7ca2d0bd-6df7-49c1-91c3-916726216dd8x.jpg}
	&
	% H9
	\vspace{0pt} \includegraphics[width=.325\textwidth,valign=t]{figures/midjourney/896c8280-187b-4b55-a640-9157a9af9a6dx.jpg}
	\\
	\footnotesize
	\textbf{H2:} \prompt{a7cbe696-4af4-4af6-9224-c4447c29831cx.jpg}
	&
	\footnotesize
	\textbf{H7:} \prompt{7ca2d0bd-6df7-49c1-91c3-916726216dd8x.jpg}
	&
	\footnotesize
	\textbf{H9:} \prompt{896c8280-187b-4b55-a640-9157a9af9a6dx.jpg}
	\\
\end{tabularx}


\noindent
\begin{tabularx}{\textwidth}{
		>{\hsize=.245\hsize}X
		>{\hsize=.245\hsize}X
		>{\hsize=.49\hsize}X
	}
	% H8
	\vspace{0pt} \includegraphics[width=.24\textwidth,valign=t]{figures/midjourney/5f98723a-488e-493e-954b-9cbf38baa69dx.jpg}
	&
	% H10
	\vspace{0pt} \includegraphics[width=.24\textwidth,valign=t]{figures/midjourney/22a357e6-2425-42b7-8475-dd05c419ecc9x.jpg}
	&
	% H3
	\vspace{0pt} \includegraphics[width=.485\textwidth,valign=t]{figures/midjourney/fb2621f1-b3f0-4cd5-8f93-774836ff1ecex.jpg}
	\\
	\footnotesize
	\textbf{H8:} \prompt{5f98723a-488e-493e-954b-9cbf38baa69dx.jpg}
	&
	\footnotesize
	\textbf{H10:} \prompt{22a357e6-2425-42b7-8475-dd05c419ecc9x.jpg}
	&
	\footnotesize
	\textbf{H3:} \prompt{fb2621f1-b3f0-4cd5-8f93-774836ff1ecex.jpg}    
\end{tabularx}
%TC:endignore
% --------------------
%TC:endignore

%TC:ignore
\newpage
\subsection{Images with Low Aesthetic Appeal}
\label{appendix:a2}
% --------------------

%TC:ignore
\noindent
\begin{tabularx}{\textwidth}{
		>{\hsize=.245\hsize}X
		>{\hsize=.245\hsize}X
		>{\hsize=.245\hsize}X
		>{\hsize=.245\hsize}X
	}
	% L1
	\vspace{0pt} \includegraphics[width=.24\textwidth,valign=t]{figures/midjourney/66b13a9f-9c0c-4c9f-aa31-d9c0f2aa8324x.jpg}
	& 
	% L2
	\vspace{0pt} \includegraphics[width=.24\textwidth,valign=t]{figures/midjourney/6c0066ee-61d7-44a1-bef8-24908db1af47x.jpg}
	& 
	% L3
	\vspace{0pt} \includegraphics[width=.24\textwidth,valign=t]{figures/midjourney/f41ba10e-05f8-458a-8709-22d1060ea5b2x.jpg}
	&
	% L4
	\vspace{0pt} \includegraphics[width=.24\textwidth,valign=t]{figures/midjourney/8d91ae8d-673b-40e0-abed-4ce965353c16x.jpg}
	\\
	\footnotesize
	\textbf{L1:}
	\prompt{66b13a9f-9c0c-4c9f-aa31-d9c0f2aa8324x.jpg}
	& 
	\footnotesize
	\textbf{L2:}
	\prompt{6c0066ee-61d7-44a1-bef8-24908db1af47x.jpg}
	& 
	\footnotesize
	\textbf{L3:}
	\prompt{f41ba10e-05f8-458a-8709-22d1060ea5b2x.jpg}
	&
	\footnotesize
	\textbf{L4:} 
	\prompt{8d91ae8d-673b-40e0-abed-4ce965353c16x.jpg}
	\\
\end{tabularx}

\noindent
\begin{tabularx}{\textwidth}{
		>{\hsize=.245\hsize}X
		>{\hsize=.245\hsize}X
		>{\hsize=.245\hsize}X
		>{\hsize=.245\hsize}X
	}
	% L5
	\vspace{0pt} \includegraphics[width=.24\textwidth,valign=t]{figures/midjourney/71dfc24b-d290-4500-8047-7a0da7bd917bx.jpg}
	&
	% L6
	\vspace{0pt} \includegraphics[width=.24\textwidth,valign=t]{figures/midjourney/1c67fdcd-b16b-44ba-8f82-16f5f443f7c9x.jpg}
	&
	% L7
	\vspace{0pt} \includegraphics[width=.24\textwidth,valign=t]{figures/midjourney/83328d03-585a-4351-b48f-0bfa1cbe72c0x.jpg}
	& 
	% L8
	\vspace{0pt} \includegraphics[width=.24\textwidth,valign=t]{figures/midjourney/6d8ace44-d363-4654-8e95-0f266bf5886dx.jpg}
	\\
	\footnotesize
	\textbf{L5:} 
	\prompt{71dfc24b-d290-4500-8047-7a0da7bd917bx.jpg}
	&
	\footnotesize
	\textbf{L6:}
	\prompt{1c67fdcd-b16b-44ba-8f82-16f5f443f7c9x.jpg}
	&
	\footnotesize
	\textbf{L7:}
	\prompt{83328d03-585a-4351-b48f-0bfa1cbe72c0x.jpg}
	&
	\footnotesize
	\textbf{L8:}
	\prompt{6d8ace44-d363-4654-8e95-0f266bf5886dx.jpg}
	\\
\end{tabularx}

\noindent
\begin{tabularx}{\textwidth}{
		>{\hsize=.245\hsize}X
		>{\hsize=.245\hsize}X
		>{\hsize=.245\hsize}X
		>{\hsize=.245\hsize}X
	}
	% L9
	\vspace{0pt} \includegraphics[width=.24\textwidth,valign=t]{figures/midjourney/46ae15d3-20b6-4cf5-914e-db537e760788x.jpg}
	&
	% L10
	\vspace{0pt} \includegraphics[width=.24\textwidth,valign=t]{figures/midjourney/0da6b738-eb79-4b41-b212-2c3a97ada099x.jpg}
	&
	&
	\\
	\footnotesize
	\textbf{L9:} 
	\prompt{46ae15d3-20b6-4cf5-914e-db537e760788x.jpg}
	&
	\footnotesize
	\textbf{L10:}
	\prompt{0da6b738-eb79-4b41-b212-2c3a97ada099x.jpg}
	&
	&
\end{tabularx}
%TC:endignore
% --------------------
%TC:endignore


% \newpage
% \section{Configuration Settings used in Studies 3 and 4.}
% \label{appendix:settings}
% % --------------------
% \input{TAB/TABLE-SETTINGS}
% % --------------------


\end{document}
\endinput
%% End of file `sample-acmsmall-submission.tex'.
