@String(PAMI = {IEEE Trans. Pattern Anal. Mach. Intell.})
@String(IJCV = {Int. J. Comput. Vis.})
@String(CVPR= {IEEE Conf. Comput. Vis. Pattern Recog.})
@String(ICCV= {Int. Conf. Comput. Vis.})
@String(ECCV= {Eur. Conf. Comput. Vis.})
@String(NIPS= {Adv. Neural Inform. Process. Syst.})
@String(ICPR = {Int. Conf. Pattern Recog.})
@String(BMVC= {Brit. Mach. Vis. Conf.})
@String(TOG= {ACM Trans. Graph.})
@String(TIP  = {IEEE Trans. Image Process.})
@String(TVCG  = {IEEE Trans. Vis. Comput. Graph.})
@String(TMM  = {IEEE Trans. Multimedia})
@String(ACMMM= {ACM Int. Conf. Multimedia})
@String(ICME = {Int. Conf. Multimedia and Expo})
@String(ICASSP=	{ICASSP})
@String(ICIP = {IEEE Int. Conf. Image Process.})
@String(ACCV  = {ACCV})
@String(ICLR = {Int. Conf. Learn. Represent.})
@String(IJCAI = {IJCAI})
@String(PR   = {Pattern Recognition})
@String(AAAI = {AAAI})
@String(CVPRW= {IEEE Conf. Comput. Vis. Pattern Recog. Worksh.})
@String(CSVT = {IEEE Trans. Circuit Syst. Video Technol.})

@String(SPL	= {IEEE Sign. Process. Letters})
@String(VR   = {Vis. Res.})
@String(JOV	 = {J. Vis.})
@String(TVC  = {The Vis. Comput.})
@String(JCST  = {J. Comput. Sci. Tech.})
@String(CGF  = {Comput. Graph. Forum})
@String(CVM = {Computational Visual Media})


@String(PAMI  = {IEEE TPAMI})
@String(IJCV  = {IJCV})
@String(CVPR  = {CVPR})
@String(ICCV  = {ICCV})
@String(ECCV  = {ECCV})
@String(NIPS  = {NeurIPS})
@String(ICPR  = {ICPR})
@String(BMVC  =	{BMVC})
@String(TOG   = {ACM TOG})
@String(TIP   = {IEEE TIP})
@String(TVCG  = {IEEE TVCG})
@String(TCSVT = {IEEE TCSVT})
@String(TMM   =	{IEEE TMM})
@String(ACMMM = {ACM MM})
@String(ICME  =	{ICME})
@String(ICASSP=	{ICASSP})
@String(ICIP  = {ICIP})
@String(ACCV  = {ACCV})
@String(ICLR  = {ICLR})
@String(IJCAI = {IJCAI})
@String(PR = {PR})
@String(AAAI = {AAAI})
@String(CVPRW= {CVPRW})
@String(CSVT = {IEEE TCSVT})


 @article{liu2020energy,
      title={Energy-based Out-of-distribution Detection},
      author={Liu, Weitang and Wang, Xiaoyun and Owens, John and Li, Yixuan},
      journal={Advances in Neural Information Processing Systems},
      year={2020}
 } 

@inproceedings{Fort2021ExploringTL,
  title={Exploring the Limits of Out-of-Distribution Detection},
  author={Stanislav Fort and Jie Ren and Balaji Lakshminarayanan},
  booktitle={NeurIPS},
  year={2021}
}

@article{Huang2021MOSTS,
  title={MOS: Towards Scaling Out-of-distribution Detection for Large Semantic Space},
  author={Rui Huang and Yixuan Li},
  journal={2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2021},
  pages={8706-8715}
}

@article{hendrycks17baseline,
  author    = {Dan Hendrycks and Kevin Gimpel},
  title     = {A Baseline for Detecting Misclassified and Out-of-Distribution Examples in Neural Networks},
  journal = {Proceedings of International Conference on Learning Representations},
  year = {2017},
}

@article{Hendrycks2021NaturalAE,
  title={Natural Adversarial Examples},
  author={Dan Hendrycks and Kevin Zhao and Steven Basart and Jacob Steinhardt and Dawn Xiaodong Song},
  journal={2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2021},
  pages={15257-15266}
}
@article{hendrycks2019robustness,
  title={Benchmarking Neural Network Robustness to Common Corruptions and Perturbations},
  author={Dan Hendrycks and Thomas Dietterich},
  journal={Proceedings of the International Conference on Learning Representations},
  year={2019}
}

@inproceedings{Malinin2018PredictiveUE,
  title={Predictive Uncertainty Estimation via Prior Networks},
  author={Andrey Malinin and Mark John Francis Gales},
  booktitle={NeurIPS},
  year={2018}
}

@article{Malinin2020EnsembleDD,
  title={Ensemble Distribution Distillation},
  author={Andrey Malinin and Bruno Mlodozeniec and Mark John Francis Gales},
  journal={Proceedings of the International Conference on Learning Representations},
  year={2020},
}

@inproceedings{Ryabinin2021ScalingED,
  title={Scaling Ensemble Distribution Distillation to Many Classes with Proxy Targets},
  author={Max Ryabinin and Andrey Malinin and Mark John Francis Gales},
  booktitle={NeurIPS},
  year={2021}
}

@article{Mukhoti2021DeterministicNN,
  title={Deterministic Neural Networks with Appropriate Inductive Biases Capture Epistemic and Aleatoric Uncertainty},
  author={Jishnu Mukhoti and Andreas Kirsch and Joost R. van Amersfoort and Philip H. S. Torr and Yarin Gal},
  journal={ArXiv},
  year={2021},
  volume={abs/2102.11582}
}


@inproceedings{Liu2020SimpleAP,
author = {Liu, Jeremiah Zhe and Lin, Zi and Padhy, Shreyas and Tran, Dustin and Bedrax-Weiss, Tania and Lakshminarayanan, Balaji},
title = {Simple and Principled Uncertainty Estimation with Deterministic Deep Learning via Distance Awareness},
year = {2020},
isbn = {9781713829546},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Bayesian neural networks and deep ensembles are principled approaches to estimate the predictive uncertainty of a deep learning model. However their practicality in real-time, industrial-scale applications are limited due to their heavy memory and inference cost. This motivates us to study principled approaches to high-quality uncertainty estimation that require only a single deep neural network (DNN). By formalizing the uncertainty quantification as a minimax learning problem, we first identify distance awareness, i.e., the model's ability to properly quantify the distance of a testing example from the training data manifold, as a necessary condition for a DNN to achieve high-quality (i.e., minimax optimal) uncertainty estimation. We then propose Spectral-normalized Neural Gaussian Process (SNGP), a simple method that improves the distance-awareness ability of modern DNNs, by adding a weight normalization step during training and replacing the output layer with a Gaussian Process. On a suite of vision and language understanding tasks and on modern architectures (Wide-ResNet and BERT), SNGP is competitive with deep ensembles in prediction, calibration and out-of-domain detection, and outperforms the other single-model approaches.},
booktitle = {Proceedings of the 34th International Conference on Neural Information Processing Systems},
articleno = {629},
numpages = {15},
location = {Vancouver, BC, Canada},
series = {NIPS'20}
}


@inproceedings{Lakshminarayanan2017SimpleAS,
  title={Simple and Scalable Predictive Uncertainty Estimation using Deep Ensembles},
  author={Balaji Lakshminarayanan and Alexander Pritzel and Charles Blundell},
  booktitle={NeurIPS},
  year={2020}
}

@inproceedings{Pearce2020UncertaintyIN,
  title={Uncertainty in Neural Networks: Approximately Bayesian Ensembling},
  author={Tim Pearce and Felix Leibfried and Alexandra Brintrup and Mohamed Zaki and A. D. Neely},
  booktitle={AISTATS},
  year={2020}
}

@article{Pearce2021UnderstandingSC,
  title={Understanding Softmax Confidence and Uncertainty},
  author={Tim Pearce and Alexandra Brintrup and Jun Zhu},
  journal={ArXiv},
  year={2021},
  volume={abs/2106.04972}
}


@InProceedings{pmlr-v70-guo17a,
  title = 	 {On Calibration of Modern Neural Networks},
  author =       {Chuan Guo and Geoff Pleiss and Yu Sun and Kilian Q. Weinberger},
  booktitle = 	 {Proceedings of the 34th International Conference on Machine Learning},
  pages = 	 {1321--1330},
  year = 	 {2017},
  editor = 	 {Precup, Doina and Teh, Yee Whye},
  volume = 	 {70},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {06--11 Aug},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v70/guo17a/guo17a.pdf},
  url = 	 {https://proceedings.mlr.press/v70/guo17a.html},
  abstract = 	 {Confidence calibration – the problem of predicting probability estimates representative of the true correctness likelihood – is important for classification models in many applications. We discover that modern neural networks, unlike those from a decade ago, are poorly calibrated. Through extensive experiments, we observe that depth, width, weight decay, and Batch Normalization are important factors influencing calibration. We evaluate the performance of various post-processing calibration methods on state-of-the-art architectures with image and document classification datasets. Our analysis and experiments not only offer insights into neural network learning, but also provide a simple and straightforward recipe for practical settings: on most datasets, temperature scaling – a single-parameter variant of Platt Scaling – is surprisingly effective at calibrating predictions.}
}


@article{Kim2021AUB,
  title={A Unified Benchmark for the Unknown Detection Capability of Deep Neural Networks},
  author={Jihyo Kim and Jiin Koo and Sangheum Hwang},
  journal={ArXiv},
  year={2021},
  volume={abs/2112.00337}
}

@inproceedings{haoqi2022vim,
title = {ViM: Out-Of-Distribution with Virtual-logit Matching},
author = {Wang, Haoqi and Li, Zhizhong and Feng, Litong and Zhang, Wayne},
booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
year = {2022}
}

@article{Hsu2020GeneralizedOD,
  title={Generalized ODIN: Detecting Out-of-Distribution Image Without Learning From Out-of-Distribution Data},
  author={Yen-Chang Hsu and Yilin Shen and Hongxia Jin and Zsolt Kira},
  journal={2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2020},
  pages={10948-10957}
}

@article{Liang2018EnhancingTR,
  title={Enhancing The Reliability of Out-of-distribution Image Detection in Neural Networks},
  author={Shiyu Liang and Yixuan Li and Rayadurgam Srikant},
  booktitle={ICLR},
  year={2018}
}

@article{He2016DeepRL,
  title={Deep Residual Learning for Image Recognition},
  author={Kaiming He and X. Zhang and Shaoqing Ren and Jian Sun},
  journal={2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2016},
  pages={770-778}
}

@article{Sandler2018MobileNetV2IR,
  title={MobileNetV2: Inverted Residuals and Linear Bottlenecks},
  author={Mark Sandler and Andrew G. Howard and Menglong Zhu and Andrey Zhmoginov and Liang-Chieh Chen},
  journal={2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  year={2018},
  pages={4510-4520}
}

@article{Huang2017DenselyCC,
  title={Densely Connected Convolutional Networks},
  author={Gao Huang and Zhuang Liu and Kilian Q. Weinberger},
  journal={2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2017},
  pages={2261-2269}
}

@inproceedings{Lee2018ASU,
  title={A Simple Unified Framework for Detecting Out-of-Distribution Samples and Adversarial Attacks},
  author={Kimin Lee and Kibok Lee and Honglak Lee and Jinwoo Shin},
  booktitle={NeurIPS},
  year={2018}
}

@inproceedings{Moon2020ConfidenceAwareLF,
  title={Confidence-Aware Learning for Deep Neural Networks},
  author={Jooyoung Moon and Jihyo Kim and Younghak Shin and Sangheum Hwang},
  booktitle={ICML},
  year={2020}
}

@inproceedings{Geifman2017SelectiveCF,
  title={Selective Classification for Deep Neural Networks},
  author={Yonatan Geifman and Ran El-Yaniv},
  booktitle={NIPS},
  year={2017}
}

@article{ElYaniv2010OnTF,
  title={On the Foundations of Noise-free Selective Classification},
  author={Ran El-Yaniv and Yair Wiener},
  journal={J. Mach. Learn. Res.},
  year={2010},
  volume={11},
  pages={1605-1641}
}
@article{Hendrycks2021TheMF,
  title={The Many Faces of Robustness: A Critical Analysis of Out-of-Distribution Generalization},
  author={Dan Hendrycks and Steven Basart and Norman Mu and Saurav Kadavath and Frank Wang and Evan Dorundo and Rahul Desai and Tyler Lixuan Zhu and Samyak Parajuli and Mike Guo and Dawn Xiaodong Song and Jacob Steinhardt and Justin Gilmer},
  journal={2021 IEEE/CVF International Conference on Computer Vision (ICCV)},
  year={2021},
  pages={8320-8329}
}

@article{hendrycks2019oe,
  title={Deep Anomaly Detection with Outlier Exposure},
  author={Hendrycks, Dan and Mazeika, Mantas and Dietterich, Thomas},
  journal={Proceedings of the International Conference on Learning Representations},
  year={2019}
}

@article{Russakovsky2015ImageNetLS,
  title={ImageNet Large Scale Visual Recognition Challenge},
  author={Olga Russakovsky and Jia Deng and Hao Su and Jonathan Krause and Sanjeev Satheesh and Sean Ma and Zhiheng Huang and Andrej Karpathy and Aditya Khosla and Michael S. Bernstein and Alexander C. Berg and Li Fei-Fei},
  journal={International Journal of Computer Vision},
  year={2015},
  volume={115},
  pages={211-252}
}

@inproceedings{Sun2021ReActOD,
  title={ReAct: Out-of-distribution Detection With Rectified Activations},
  author={Yiyou Sun and Chuan Guo and Yixuan Li},
  booktitle={NeurIPS},
  year={2021}
}

@article{yang2021oodsurvey,
  title={Generalized Out-of-Distribution Detection: A Survey},
  author={Yang, Jingkang and Zhou, Kaiyang and Li, Yixuan and Liu, Ziwei},
  journal={arXiv preprint arXiv:2110.11334},
  year={2021}
}

@inproceedings{Granese2021DOCTORAS,
  title={DOCTOR: A Simple Method for Detecting Misclassification Errors},
  author={Federica Granese and Marco Romanelli and Daniele Gorla and Catuscia Palamidessi and Pablo Piantanida},
  booktitle={NeurIPS},
  year={2021}
}

@article{du2022vos,
      title={VOS: Learning What You Don’t Know by Virtual Outlier Synthesis}, 
      author={Du, Xuefeng and Wang, Zhaoning and Cai, Mu and Li, Yixuan},
      journal={Proceedings of the International Conference on Learning Representations},
      year={2022}
}

@inproceedings{Kamath2020SelectiveQA,
  title={Selective Question Answering under Domain Shift},
  author={Amita Kamath and Robin Jia and Percy Liang},
  booktitle={ACL},
  year={2020}
}

@article{https://doi.org/10.1111/biom.13612,
author = {Feng, Jean and Sondhi, Arjun and Perry, Jessica and Simon, Noah},
title = {Selective prediction-set models with coverage rate guarantees},
journal = {Biometrics},
volume = {n/a},
number = {n/a},
pages = {},
keywords = {abstaining prediction models, cross-validation, ensemble methods, neural networks, prediction sets},
doi = {https://doi.org/10.1111/biom.13612},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/biom.13612},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1111/biom.13612},
abstract = {Abstract The current approach to using machine learning (ML) algorithms in healthcare is to either require clinician oversight for every use case or use their predictions without any human oversight. We explore a middle ground that lets ML algorithms abstain from making a prediction to simultaneously improve their reliability and reduce the burden placed on human experts. To this end, we present a general penalized loss minimization framework for training selective prediction-set (SPS) models, which choose to either output a prediction set or abstain. The resulting models abstain when the outcome is difficult to predict accurately, such as on subjects who are too different from the training data, and achieve higher accuracy on those they do give predictions for. We then introduce a model-agnostic, statistical inference procedure for the coverage rate of an SPS model that ensembles individual models trained using K-fold cross-validation. We find that SPS ensembles attain prediction-set coverage rates closer to the nominal level and have narrower confidence intervals for its marginal coverage rate. We apply our method to train neural networks that abstain more for out-of-sample images on the MNIST digit prediction task and achieve higher predictive accuracy for ICU patients compared to existing approaches.}
}

@inproceedings{geifman2019selectivenet,
  title={Selectivenet: A deep neural network with an integrated reject option},
  author={Geifman, Yonatan and El-Yaniv, Ran},
  booktitle={International Conference on Machine Learning},
  pages={2151--2159},
  year={2019},
  organization={PMLR}
}

@inproceedings{Huang2021OnTI,
  title={On the Importance of Gradients for Detecting Distributional Shifts in the Wild},
  author={Rui Huang and Andrew Geng and Yixuan Li},
  booktitle={NeurIPS},
  year={2021}
}

@inproceedings{
geifman2018biasreduced,
title={Bias-Reduced Uncertainty Estimation for Deep Neural Classifiers},
author={Yonatan Geifman and Guy Uziel and Ran El-Yaniv},
booktitle={International Conference on Learning Representations},
year={2019},
url={https://openreview.net/forum?id=SJfb5jCqKm},
}

@inproceedings{Griffin2007Caltech256OC,
  title={Caltech-256 Object Category Dataset},
  author={Gregory Griffin and Alex Holub and Pietro Perona},
  year={2007}
}

@article{openimages,
  title={OpenImages: A public dataset for large-scale multi-label and multi-class image classification.},
  author={Krasin, Ivan and Duerig, Tom and Alldrin, Neil and Ferrari, Vittorio and Abu-El-Haija, Sami and Kuznetsova, Alina and Rom, Hassan and Uijlings, Jasper and Popov, Stefan and Veit, Andreas and Belongie, Serge and Gomes, Victor and Gupta, Abhinav and Sun, Chen and Chechik, Gal and Cai, David and Feng, Zheyun and Narayanan, Dhyanesh and Murphy, Kevin},
  journal={Dataset available from https://github.com/openimages},
  year={2017}
}
@misc{inat,
  doi = {10.48550/ARXIV.1707.06642},
  
  url = {https://arxiv.org/abs/1707.06642},
  
  author = {Van Horn, Grant and Mac Aodha, Oisin and Song, Yang and Cui, Yin and Sun, Chen and Shepard, Alex and Adam, Hartwig and Perona, Pietro and Belongie, Serge},
  
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {The iNaturalist Species Classification and Detection Dataset},
  
  publisher = {arXiv},
  
  year = {2017},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

	    @InProceedings{cimpoi14describing,
	      Author    = {M. Cimpoi and S. Maji and I. Kokkinos and S. Mohamed and and A. Vedaldi},
	      Title     = {Describing Textures in the Wild},
	      Booktitle = {Proceedings of the {IEEE} Conf. on Computer Vision and Pattern Recognition ({CVPR})},
	      Year      = {2014}
}

@article{Kather2016MulticlassTA,
  title={Multi-class texture analysis in colorectal cancer histology},
  author={Jakob Nikolas Kather and Cleo-Aron Weis and Francesco Bianconi and Susanne Maria Melchers and Lothar Rudi Schad and Timo Gaiser and Alexander Marx and Frank G. Z{\"o}llner},
  journal={Scientific Reports},
  year={2016},
  volume={6}
}

@article{Mesejo2016ComputerAidedCO,
  title={Computer-Aided Classification of Gastrointestinal Lesions in Regular Colonoscopy.},
  author={Pablo Mesejo and Daniel Pizarro and Armand Abergel and Olivier Y. Rouquette and Sylvain B{\'e}orchia and Laurent Poincloux and Adrien Bartoli},
  journal={IEEE transactions on medical imaging},
  year={2016}
}


@InProceedings{pmlr-v48-gal16,
  title = 	 {Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning},
  author = 	 {Gal, Yarin and Ghahramani, Zoubin},
  booktitle = 	 {Proceedings of The 33rd International Conference on Machine Learning},
  pages = 	 {1050--1059},
  year = 	 {2016},
  editor = 	 {Balcan, Maria Florina and Weinberger, Kilian Q.},
  volume = 	 {48},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {New York, New York, USA},
  month = 	 {20--22 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v48/gal16.pdf},
  url = 	 {https://proceedings.mlr.press/v48/gal16.html},
  abstract = 	 {Deep learning tools have gained tremendous attention in applied machine learning. However such tools for regression and classification do not capture model uncertainty. In comparison, Bayesian models offer a mathematically grounded framework to reason about model uncertainty, but usually come with a prohibitive computational cost. In this paper we develop a new theoretical framework casting dropout training in deep neural networks (NNs) as approximate Bayesian inference in deep Gaussian processes. A direct result of this theory gives us tools to model uncertainty with dropout NNs – extracting information from existing models that has been thrown away so far. This mitigates the problem of representing uncertainty in deep learning without sacrificing either computational complexity or test accuracy. We perform an extensive study of the properties of dropout’s uncertainty. Various network architectures and non-linearities are assessed on tasks of regression and classification, using MNIST as an example. We show a considerable improvement in predictive log-likelihood and RMSE compared to existing state-of-the-art methods, and finish by using dropout’s uncertainty in deep reinforcement learning.}
}

@inproceedings{kendalunc,
author = {Kendall, Alex and Gal, Yarin},
title = {What Uncertainties Do We Need in Bayesian Deep Learning for Computer Vision?},
year = {2017},
isbn = {9781510860964},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {There are two major types of uncertainty one can model. Aleatoric uncertainty captures noise inherent in the observations. On the other hand, epistemic uncertainty accounts for uncertainty in the model - uncertainty which can be explained away given enough data. Traditionally it has been difficult to model epistemic uncertainty in computer vision, but with new Bayesian deep learning tools this is now possible. We study the benefits of modeling epistemic vs. aleatoric uncertainty in Bayesian deep learning models for vision tasks. For this we present a Bayesian deep learning framework combining input-dependent aleatoric uncertainty together with epistemic uncertainty. We study models under the framework with per-pixel semantic segmentation and depth regression tasks. Further, our explicit uncertainty formulation leads to new loss functions for these tasks, which can be interpreted as learned attenuation. This makes the loss more robust to noisy data, also giving new state-of-the-art results on segmentation and depth regression benchmarks.},
booktitle = {Proceedings of the 31st International Conference on Neural Information Processing Systems},
pages = {5580–5590},
numpages = {11},
location = {Long Beach, California, USA},
series = {NIPS'17}
}

@inproceedings{Panagiotopoulos2022LeveragingSP,
  title={Leveraging Selective Prediction for Reliable Image Geolocation},
  author={Apostolos Panagiotopoulos and Giorgos Kordopatis-Zilos and Symeon Papadopoulos},
  booktitle={MMM},
  year={2022}
}

@incollection{torch,
title = {PyTorch: An Imperative Style, High-Performance Deep Learning Library},
author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
booktitle = {Advances in Neural Information Processing Systems 32},
editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
pages = {8024--8035},
year = {2019},
publisher = {Curran Associates, Inc.}
}

@InProceedings{Techapanurak_2020_ACCV,
    author    = {Techapanurak, Engkarat and Suganuma, Masanori and Okatani, Takayuki},
    title     = {Hyperparameter-Free Out-of-Distribution Detection Using Cosine Similarity},
    booktitle = {Proceedings of the Asian Conference on Computer Vision (ACCV)},
    month     = {November},
    year      = {2020}
}

@article{Nalisnick2019DoDG,
  title={Do Deep Generative Models Know What They Don't Know?},
  author={Eric T. Nalisnick and Akihiro Matsukawa and Yee Whye Teh and Dilan G{\"o}r{\"u}r and Balaji Lakshminarayanan},
  booktitle={International Conference on Learning Representations},
  year={2019}
}

@article{Caterini2021EntropicII,
  title={Entropic Issues in Likelihood-Based OOD Detection},
  author={Anthony L. Caterini and Gabriel Loaiza-Ganem},
  journal={ArXiv},
  year={2021},
  volume={abs/2109.10794}
}

@inproceedings{likelihoodratio,
 author = {Ren, Jie and Liu, Peter J. and Fertig, Emily and Snoek, Jasper and Poplin, Ryan and Depristo, Mark and Dillon, Joshua and Lakshminarayanan, Balaji},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Likelihood Ratios for Out-of-Distribution Detection},
 url = {https://proceedings.neurips.cc/paper/2019/file/1e79596878b2320cac26dd792a6c51c9-Paper.pdf},
 volume = {32},
 year = {2019}
}

@inproceedings{Zhang2021OnTO,
  title={On the Out-of-distribution Generalization of Probabilistic Image Modelling},
  author={Mingtian Zhang and Andi Zhang and Steven G. McDonagh},
  booktitle={NeurIPS},
  year={2021}
}

@ARTICLE{bnn,
  author={Jospin, Laurent Valentin and Laga, Hamid and Boussaid, Farid and Buntine, Wray and Bennamoun, Mohammed},
  journal={IEEE Computational Intelligence Magazine}, 
  title={Hands-On Bayesian Neural Networks—A Tutorial for Deep Learning Users}, 
  year={2022},
  volume={17},
  number={2},
  pages={29-48},
  doi={10.1109/MCI.2022.3155327}}
  
  
 @incollection{failure,
   title = {Addressing Failure Prediction by Learning Model Confidence},
   author = {Corbi\`{e}re, Charles and THOME, Nicolas and Bar-Hen, Avner and Cord, Matthieu and P\'{e}rez, Patrick},
   booktitle = {Advances in Neural Information Processing Systems 32},
   editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
   pages = {2902--2913},
   year = {2019},
   publisher = {Curran Associates, Inc.},
   url = {http://papers.nips.cc/paper/8556-addressing-failure-prediction-by-learning-model-confidence.pdf}
}
@inproceedings{Jiang2018ToTO,
  title={To Trust Or Not To Trust A Classifier},
  author={Heinrich Jiang and Been Kim and Maya R. Gupta},
  booktitle={NeurIPS},
  year={2018}
}

@inproceedings{Kolesnikov2020BigT,
  title={Big Transfer (BiT): General Visual Representation Learning},
  author={Alexander Kolesnikov and Lucas Beyer and Xiaohua Zhai and Joan Puigcerver and Jessica Yung and Sylvain Gelly and Neil Houlsby},
  booktitle={ECCV},
  year={2020}
}

@article{cifar,
title= {CIFAR-10 (Canadian Institute for Advanced Research)},
author= {Alex Krizhevsky and Vinod Nair and Geoffrey Hinton},
year= {2009},
url= {http://www.cs.toronto.edu/~kriz/cifar.html},
abstract= {The CIFAR-10 dataset consists of 60000 32x32 colour images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images. 

The dataset is divided into five training batches and one test batch, each with 10000 images. The test batch contains exactly 1000 randomly-selected images from each class. The training batches contain the remaining images in random order, but some training batches may contain more images from one class than another. Between them, the training batches contain exactly 5000 images from each class. },
keywords= {Dataset},
terms= {}
}

@inproceedings{CSI,
 author = {Tack, Jihoon and Mo, Sangwoo and Jeong, Jongheon and Shin, Jinwoo},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {11839--11852},
 publisher = {Curran Associates, Inc.},
 title = {CSI: Novelty Detection via Contrastive Learning on Distributionally Shifted Instances},
 url = {https://proceedings.neurips.cc/paper/2020/file/8965f76632d7672e7d3cf29c87ecaa0c-Paper.pdf},
 volume = {33},
 year = {2020}
}

@misc{svhn,
  doi = {10.48550/ARXIV.1312.6082},
  
  url = {https://arxiv.org/abs/1312.6082},
  
  author = {Goodfellow, Ian J. and Bulatov, Yaroslav and Ibarz, Julian and Arnoud, Sacha and Shet, Vinay},
  
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Multi-digit Number Recognition from Street View Imagery using Deep Convolutional Neural Networks},
  
  publisher = {arXiv},
  
  year = {2013},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{Xiao2017FashionMNISTAN,
  title={Fashion-MNIST: a Novel Image Dataset for Benchmarking Machine Learning Algorithms},
  author={Han Xiao and Kashif Rasul and Roland Vollgraf},
  journal={ArXiv},
  year={2017},
  volume={abs/1708.07747}
}

@article{yu15lsun,
    Author = {Yu, Fisher and Zhang, Yinda and Song, Shuran and Seff, Ari and Xiao, Jianxiong},
    Title = {LSUN: Construction of a Large-scale Image Dataset using Deep Learning with Humans in the Loop},
    Journal = {arXiv preprint arXiv:1506.03365},
    Year = {2015}
}

@InProceedings{pmlr-v162-hendrycks22a,
  title = 	 {Scaling Out-of-Distribution Detection for Real-World Settings},
  author =       {Hendrycks, Dan and Basart, Steven and Mazeika, Mantas and Zou, Andy and Kwon, Joseph and Mostajabi, Mohammadreza and Steinhardt, Jacob and Song, Dawn},
  booktitle = 	 {Proceedings of the 39th International Conference on Machine Learning},
  pages = 	 {8759--8773},
  year = 	 {2022},
  editor = 	 {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
  volume = 	 {162},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {17--23 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v162/hendrycks22a/hendrycks22a.pdf},
  url = 	 {https://proceedings.mlr.press/v162/hendrycks22a.html},
  abstract = 	 {Detecting out-of-distribution examples is important for safety-critical machine learning applications such as detecting novel biological phenomena and self-driving cars. However, existing research mainly focuses on simple small-scale settings. To set the stage for more realistic out-of-distribution detection, we depart from small-scale settings and explore large-scale multiclass and multi-label settings with high-resolution images and thousands of classes. To make future work in real-world settings possible, we create new benchmarks for three large-scale settings. To test ImageNet multiclass anomaly detectors, we introduce the Species dataset containing over 700,000 images and over a thousand anomalous species. We leverage ImageNet-21K to evaluate PASCAL VOC and COCO multilabel anomaly detectors. Third, we introduce a new benchmark for anomaly segmentation by introducing a segmentation benchmark with road anomalies. We conduct extensive experiments in these more realistic settings for out-of-distribution detection and find that a surprisingly simple detector based on the maximum logit outperforms prior methods in all the large-scale multi-class, multi-label, and segmentation tasks, establishing a simple new baseline for future work.}
}

@inproceedings{izmailov2021bayesian,
  title={What are Bayesian neural network posteriors really like?},
  author={Izmailov, Pavel and Vikram, Sharad and Hoffman, Matthew D and Wilson, Andrew Gordon Gordon},
  booktitle={International conference on machine learning},
  pages={4629--4640},
  year={2021},
  organization={PMLR}
}

@article{Fort2019DeepEA,
  title={Deep Ensembles: A Loss Landscape Perspective},
  author={Stanislav Fort and Huiyi Hu and Balaji Lakshminarayanan},
  journal={ArXiv},
  year={2019},
  volume={abs/1912.02757}
}
@article{ovadia2019can,
  title={Can you trust your model's uncertainty? evaluating predictive uncertainty under dataset shift},
  author={Ovadia, Yaniv and Fertig, Emily and Ren, Jie and Nado, Zachary and Sculley, David and Nowozin, Sebastian and Dillon, Joshua and Lakshminarayanan, Balaji and Snoek, Jasper},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}
@inproceedings{Malinin2021UncertaintyEI,
  title={Uncertainty Estimation in Autoregressive Structured Prediction},
  author={Andrey Malinin and Mark John Francis Gales},
  booktitle={ICLR},
  year={2021}
}


@inproceedings{EnDD,
 author = {Ryabinin, Max and Malinin, Andrey and Gales, Mark},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
 pages = {6023--6035},
 publisher = {Curran Associates, Inc.},
 title = {Scaling Ensemble Distribution Distillation to Many Classes with Proxy Targets},
 url = {https://proceedings.neurips.cc/paper/2021/file/2f4ccb0f7a84f335affb418aee08a6df-Paper.pdf},
 volume = {34},
 year = {2021}
}


@inproceedings{Depeweg2018DecompositionOU,
  title={Decomposition of Uncertainty in Bayesian Deep Learning for Efficient and Risk-sensitive Learning},
  author={Stefan Depeweg and Jos{\'e} Miguel Hern{\'a}ndez-Lobato and Finale Doshi-Velez and Steffen Udluft},
  booktitle={ICML},
  year={2018}
}

@article{Hllermeier2021AleatoricAE,
  title={Aleatoric and epistemic uncertainty in machine learning: an introduction to concepts and methods},
  author={Eyke H{\"u}llermeier and Willem Waegeman},
  journal={Mach. Learn.},
  year={2021},
  volume={110},
  pages={457-506}
}
@article{Abe2022DeepEW,
  title={Deep Ensembles Work, But Are They Necessary?},
  author={Taiga Abe and E. Kelly Buchanan and Geoff Pleiss and Richard S. Zemel and John P. Cunningham},
  journal={ArXiv},
  year={2022},
  volume={abs/2202.06985}
}

@article{medunc,
title	= {Second Opinion Needed: Communicating Uncertainty in Medical Artificial Intelligence},
author	= {Andrew Beam and Ben Kompa},
year	= {2021},
journal	= {NPJ Digital Medicine},
volume	= {4}
}

@inproceedings{ijcai2017-661,
  author    = {Rowan McAllister and Yarin Gal and Alex Kendall and Mark van der Wilk and Amar Shah and Roberto Cipolla and Adrian Weller},
  title     = {Concrete Problems for Autonomous Vehicle Safety: Advantages of Bayesian Deep Learning},
  booktitle = {Proceedings of the Twenty-Sixth International Joint Conference on
               Artificial Intelligence, {IJCAI-17}},
  pages     = {4745--4753},
  year      = {2017},
  doi       = {10.24963/ijcai.2017/661},
  url       = {https://doi.org/10.24963/ijcai.2017/661},
}

@inproceedings{avunc,
 author = {Kendall, Alex and Gal, Yarin},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {What Uncertainties Do We Need in Bayesian Deep Learning for Computer Vision?},
 url = {https://proceedings.neurips.cc/paper/2017/file/2650d6089a6d640c5e85b2b88265dc2b-Paper.pdf},
 volume = {30},
 year = {2017}
}

@article{Howard2017MobileNetsEC,
  title={MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications},
  author={Andrew G. Howard and Menglong Zhu and Bo Chen and Dmitry Kalenichenko and Weijun Wang and Tobias Weyand and Marco Andreetto and Hartwig Adam},
  journal={ArXiv},
  year={2017},
  volume={abs/1704.04861}
}

@article{Howard2019SearchingFM,
  title={Searching for MobileNetV3},
  author={Andrew G. Howard and Mark Sandler and Grace Chu and Liang-Chieh Chen and Bo Chen and Mingxing Tan and Weijun Wang and Yukun Zhu and Ruoming Pang and Vijay Vasudevan and Quoc V. Le and Hartwig Adam},
  journal={2019 IEEE/CVF International Conference on Computer Vision (ICCV)},
  year={2019},
  pages={1314-1324}
}

@article{dosovitskiy2020vit,
  title={An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},
  author={Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and  Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and Uszkoreit, Jakob and Houlsby, Neil},
  journal={ICLR},
  year={2021}
}

@inproceedings{wang2021not,
        title = {Not All Images are Worth 16x16 Words: Dynamic Transformers for Efficient Image Recognition},
       author = {Wang, Yulin and Huang, Rui and Song, Shiji and Huang, Zeyi and Huang, Gao},
    booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
         year = {2021}
}

@inproceedings{vision-transformer,
title = {MobileViT: Light-weight, General-purpose, and Mobile-friendly Vision Transformer},
author = {Sachin Mehta and Mohammad Rastegari},
year = {2022},
booktitle = {ICLR},
URL = {https://arxiv.org/abs/2110.02178}
}
@inproceedings{pan2022edgevits,
  title={EdgeViTs: Competing Light-weight CNNs on Mobile Devices with Vision Transformers},
  author={Pan, Junting and Bulat, Adrian and Tan, Fuwen and Zhu, Xiatian and Dudziak, Lukasz and Li, Hongsheng and Tzimiropoulos, Georgios and Martinez, Brais},
  booktitle={European Conference on Computer Vision},
  year={2022}
}

@InProceedings{Chen_2022_CVPR,
    author    = {Chen, Yinpeng and Dai, Xiyang and Chen, Dongdong and Liu, Mengchen and Dong, Xiaoyi and Yuan, Lu and Liu, Zicheng},
    title     = {Mobile-Former: Bridging MobileNet and Transformer},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2022},
    pages     = {5270-5279}
}

@inproceedings{
zoph2017neural,
title={Neural Architecture Search with Reinforcement Learning},
author={Barret Zoph and Quoc Le},
booktitle={International Conference on Learning Representations},
year={2017},
url={https://openreview.net/forum?id=r1Ue8Hcxg}
}

@inproceedings{
liu2018darts,
title={{DARTS}: Differentiable Architecture Search},
author={Hanxiao Liu and Karen Simonyan and Yiming Yang},
booktitle={International Conference on Learning Representations},
year={2019},
url={https://openreview.net/forum?id=S1eYHoC5FX},
}

@InProceedings{Tan_2019_CVPR,
author = {Tan, Mingxing and Chen, Bo and Pang, Ruoming and Vasudevan, Vijay and Sandler, Mark and Howard, Andrew and Le, Quoc V.},
title = {MnasNet: Platform-Aware Neural Architecture Search for Mobile},
booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2019}
}


@InProceedings{pmlr-v97-tan19a,
  title = 	 {{E}fficient{N}et: Rethinking Model Scaling for Convolutional Neural Networks},
  author =       {Tan, Mingxing and Le, Quoc},
  booktitle = 	 {Proceedings of the 36th International Conference on Machine Learning},
  pages = 	 {6105--6114},
  year = 	 {2019},
  editor = 	 {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  volume = 	 {97},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {09--15 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v97/tan19a/tan19a.pdf},
  url = 	 {https://proceedings.mlr.press/v97/tan19a.html},
  abstract = 	 {Convolutional Neural Networks (ConvNets) are commonly developed at a fixed resource budget, and then scaled up for better accuracy if more resources are given. In this paper, we systematically study model scaling and identify that carefully balancing network depth, width, and resolution can lead to better performance. Based on this observation, we propose a new scaling method that uniformly scales all dimensions of depth/width/resolution using a simple yet highly effective compound coefficient. We demonstrate the effectiveness of this method on MobileNets and ResNet. To go even further, we use neural architecture search to design a new baseline network and scale it up to obtain a family of models, called EfficientNets, which achieve much better accuracy and efficiency than previous ConvNets. In particular, our EfficientNet-B7 achieves stateof-the-art 84.4% top-1 / 97.1% top-5 accuracy on ImageNet, while being 8.4x smaller and 6.1x faster on inference than the best existing ConvNet (Huang et al., 2018). Our EfficientNets also transfer well and achieve state-of-the-art accuracy on CIFAR-100 (91.7%), Flower (98.8%), and 3 other transfer learning datasets, with an order of magnitude fewer parameters.}
}

@article{Wu2019FBNetHE,
  title={FBNet: Hardware-Aware Efficient ConvNet Design via Differentiable Neural Architecture Search},
  author={Bichen Wu and Xiaoliang Dai and Peizhao Zhang and Yanghan Wang and Fei Sun and Yiming Wu and Yuandong Tian and P{\'e}ter Vajda and Yangqing Jia and Kurt Keutzer},
  journal={2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2019},
  pages={10726-10734}
}

@InProceedings{Wang_2022_CVPR,
    author    = {Wang, Linnan and Yu, Chenhan and Salian, Satish and Kierat, Slawomir and Migacz, Szymon and Florea, Alex Fit},
    title     = {Searching the Deployable Convolution Neural Networks for GPUs},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2022},
    pages     = {12227-12236}
}

@inproceedings{hinton2015,
title	= {Distilling the Knowledge in a Neural Network},
author	= {Geoffrey Hinton and Oriol Vinyals and Jeffrey Dean},
year	= {2015},
URL	= {http://arxiv.org/abs/1503.02531},
booktitle	= {NIPS Deep Learning and Representation Learning Workshop}
}
@article{Tang2019DistillingTK,
  title={Distilling Task-Specific Knowledge from BERT into Simple Neural Networks},
  author={Raphael Tang and Yao Lu and Linqing Liu and Lili Mou and Olga Vechtomova and Jimmy J. Lin},
  journal={ArXiv},
  year={2019},
  volume={abs/1903.12136}
}
@inproceedings{Tang2019NaturalLG,
  title={Natural Language Generation for Effective Knowledge Distillation},
  author={Raphael Tang and Yao Lu and Jimmy J. Lin},
  booktitle={EMNLP},
  year={2019}
}
@article{Gou2021KnowledgeDA,
  title={Knowledge Distillation: A Survey},
  author={Jianping Gou and B. Yu and Stephen J. Maybank and Dacheng Tao},
  journal={ArXiv},
  year={2021},
  volume={abs/2006.05525}
}

@inproceedings{Touvron2021TrainingDI,
  title={Training data-efficient image transformers \& distillation through attention},
  author={Hugo Touvron and Matthieu Cord and Matthijs Douze and Francisco Massa and Alexandre Sablayrolles and Herv'e J'egou},
  booktitle={ICML},
  year={2021}
}

@misc{
prunegupta,
title={To Prune, or Not to Prune: Exploring the Efficacy of Pruning for Model Compression},
author={Michael H. Zhu, Suyog Gupta},
year={2018},
url={https://openreview.net/forum?id=S1lN69AT-},
}

@INPROCEEDINGS{gaternet,
  author={Chen, Zhourong and Li, Yang and Bengio, Samy and Si, Si},
  booktitle={2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={You Look Twice: GaterNet for Dynamic Filter Selection in CNNs}, 
  year={2019},
  volume={},
  number={},
  pages={9164-9172},
  doi={10.1109/CVPR.2019.00939}}
  
 @InProceedings{Rajagopal_2020_CVPR_Workshops,
author = {Rajagopal, Aditya and Bouganis, Christos-Savvas},
title = {Now That I Can See, I Can Improve: Enabling Data-Driven Finetuning of CNNs on the Edge},
booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops},
month = {June},
year = {2020}
}

@inproceedings{braindamage,
 author = {LeCun, Yann and Denker, John and Solla, Sara},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky},
 pages = {},
 publisher = {Morgan-Kaufmann},
 title = {Optimal Brain Damage},
 url = {https://proceedings.neurips.cc/paper/1989/file/6c9882bbac1c7093bd25041881277658-Paper.pdf},
 volume = {2},
 year = {1989}
}

@INPROCEEDINGS{sparsefpga,
  author={Huang, Sitao and Pearson, Carl and Nagi, Rakesh and Xiong, Jinjun and Chen, Deming and Hwu, Wen-mei},
  booktitle={2019 IEEE High Performance Extreme Computing Conference (HPEC)}, 
  title={Accelerating Sparse Deep Neural Networks on FPGAs}, 
  year={2019},
  volume={},
  number={},
  pages={1-7},
  doi={10.1109/HPEC.2019.8916419}}
  
  @article{Mishra2021AcceleratingSD,
  title={Accelerating Sparse Deep Neural Networks},
  author={Asit K. Mishra and Jorge Albericio Latorre and Jeff Pool and Darko Stosic and Dusan Stosic and Ganesh Venkatesh and Chong Yu and Paulius Micikevicius},
  journal={ArXiv},
  year={2021},
  volume={abs/2104.08378}
}

@inproceedings{
frankle2018the,
title={The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks},
author={Jonathan Frankle and Michael Carbin},
booktitle={International Conference on Learning Representations},
year={2019},
url={https://openreview.net/forum?id=rJl-b3RcF7},
}

@inproceedings{
lee2018snip,
title={{SNIP}: {SINGLE}-{SHOT} {NETWORK} {PRUNING} {BASED} {ON} {CONNECTION} {SENSITIVITY}},
author={Namhoon Lee and Thalaiyasingam Ajanthan and Philip Torr},
booktitle={International Conference on Learning Representations},
year={2019},
url={https://openreview.net/forum?id=B1VZqjAcYX},
}
@inproceedings{
rao2021dynamicvit,
title={DynamicViT: Efficient Vision Transformers with Dynamic Token Sparsification},
author={Yongming Rao and Wenliang Zhao and Benlin Liu and Jiwen Lu and Jie Zhou and Cho-Jui Hsieh},
booktitle={Advances in Neural Information Processing Systems},
editor={A. Beygelzimer and Y. Dauphin and P. Liang and J. Wortman Vaughan},
year={2021},
url={https://openreview.net/forum?id=jB0Nlbwlybm}
}

    @inproceedings{yin2022avit,
        title={{A}-{V}i{T}: {A}daptive Tokens for Efficient Vision Transformer},
        author={Yin, Hongxu and Vahdat, Arash and Alvarez, Jose and Mallya, Arun and Kautz, Jan and Molchanov, Pavlo},
        booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
        year={2022}
    }
    
    @inproceedings{
li2017pruning,
title={Pruning Filters for Efficient ConvNets},
author={Hao Li and Asim Kadav and Igor Durdanovic and Hanan Samet and Hans Peter Graf},
booktitle={International Conference on Learning Representations},
year={2017},
url={https://openreview.net/forum?id=rJqFGTslg}
}

@article{MACKAY199573,
title = {Bayesian neural networks and density networks},
journal = {Nuclear Instruments and Methods in Physics Research Section A: Accelerators, Spectrometers, Detectors and Associated Equipment},
volume = {354},
number = {1},
pages = {73-80},
year = {1995},
note = {Proceedings of the Third Workshop on Neutron Scattering Data Analysis},
issn = {0168-9002},
doi = {https://doi.org/10.1016/0168-9002(94)00931-7},
url = {https://www.sciencedirect.com/science/article/pii/0168900294009317},
author = {David J.C MacKay},
abstract = {This paper reviews the Bayesian approach to learning in neural networks, then introduces a new adaptive model, the density network. This is a neural network for which target outputs are provided, but the inputs are unspecified. When a probability distribution is placed on the unknown inputs, a latent variable model is defined that is capable of discovering the underlying dimensionality of a data set. A Bayesian learning algorithm for these networks is derived and demonstrated.}
}

@article{mackay,
    author = {MacKay, David J. C.},
    title = "{A Practical Bayesian Framework for Backpropagation Networks}",
    journal = {Neural Computation},
    volume = {4},
    number = {3},
    pages = {448-472},
    year = {1992},
    month = {05},
    abstract = "{A quantitative and practical Bayesian framework is described for learning of mappings in feedforward networks. The framework makes possible (1) objective comparisons between solutions using alternative network architectures, (2) objective stopping rules for network pruning or growing procedures, (3) objective choice of magnitude and type of weight decay terms or additive regularizers (for penalizing large weights, etc.), (4) a measure of the effective number of well-determined parameters in a model, (5) quantified estimates of the error bars on network parameters and on network output, and (6) objective comparisons with alternative learning and interpolation models such as splines and radial basis functions. The Bayesian "evidence" automatically embodies "Occam's razor," penalizing overflexible and overcomplex models. The Bayesian approach helps detect poor underlying assumptions in learning models. For learning models well matched to a problem, a good correlation between generalization ability and the Bayesian evidence is obtained.}",
    issn = {0899-7667},
    doi = {10.1162/neco.1992.4.3.448},
    url = {https://doi.org/10.1162/neco.1992.4.3.448},
    eprint = {https://direct.mit.edu/neco/article-pdf/4/3/448/812348/neco.1992.4.3.448.pdf},
}

@inproceedings{bbb,
author = {Blundell, Charles and Cornebise, Julien and Kavukcuoglu, Koray and Wierstra, Daan},
title = {Weight Uncertainty in Neural Networks},
year = {2015},
publisher = {JMLR.org},
abstract = {We introduce a new, efficient, principled and backpropagation-compatible algorithm for learning a probability distribution on the weights of a neural network, called Bayes by Backprop. It regularises the weights by minimising a compression cost, known as the variational free energy or the expected lower bound on the marginal likelihood. We show that this principled kind of regularisation yields comparable performance to dropout on MNIST classification. We then demonstrate how the learnt uncertainty in the weights can be used to improve generalisation in non-linear regression problems, and how this weight uncertainty can be used to drive the exploration-exploitation trade-off in reinforcement learning.},
booktitle = {Proceedings of the 32nd International Conference on International Conference on Machine Learning - Volume 37},
pages = {1613–1622},
numpages = {10},
location = {Lille, France},
series = {ICML'15}
}


@inproceedings{caruana,
 author = {Ba, Jimmy and Caruana, Rich},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Do Deep Nets Really Need to be Deep?},
 url = {https://proceedings.neurips.cc/paper/2014/file/ea8fcd92d59581717e06eb187f10666d-Paper.pdf},
 volume = {27},
 year = {2014}
}

@misc{
malinin2021regression,
title={Regression Prior Networks},
author={Andrey Malinin and Sergey Chervontsev and Ivan Provilkov and Mark Gales},
year={2021},
url={https://openreview.net/forum?id=ygWoT6hOc28}
}

@article{van2020uncertainty,
  title={Uncertainty Estimation Using a Single Deep Deterministic Neural Network},
  author={van Amersfoort, Joost and Smith, Lewis and Teh, Yee Whye and Gal, Yarin},
  booktitle={International Conference on Machine Learning},
  year={2020}
}

@inproceedings{
charpentier2022natural,
title={Natural Posterior Network: Deep Bayesian Predictive Uncertainty for Exponential Family Distributions},
author={Bertrand Charpentier and Oliver Borchert and Daniel Z{\"u}gner and Simon Geisler and Stephan G{\"u}nnemann},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=tV3N0DWMxCg}
}


@inproceedings{
malinin2021shifts,
title={Shifts: A Dataset of Real Distributional Shift Across Multiple Large-Scale Tasks},
author={Andrey Malinin and Neil Band and Yarin Gal and Mark Gales and Alexander Ganshin and German Chesnokov and Alexey Noskov and Andrey Ploskonosov and Liudmila Prokhorenkova and Ivan Provilkov and Vatsal Raina and Vyas Raina and Denis Roginskiy and Mariya Shmatova and Panagiotis Tigas and Boris Yangel},
booktitle={Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2)},
year={2021},
url={https://openreview.net/forum?id=qM45LHaWM6E}
}

@article{hendrycks2019anomalyseg,
  title={Scaling Out-of-Distribution Detection for Real-World Settings},
  author={Hendrycks, Dan and Basart, Steven and Mazeika, Mantas and Zou, Andy and Kwon, Joe and Mostajabi, Mohammadreza and Steinhardt, Jacob and Song, Dawn},
  journal={ICML},
  year={2022}
}

@inproceedings{
karandikar2021soft,
title={Soft Calibration Objectives for Neural Networks},
author={Archit Karandikar and Nicholas Cain and Dustin Tran and Balaji Lakshminarayanan and Jonathon Shlens and Michael Curtis Mozer and Rebecca Roelofs},
booktitle={Advances in Neural Information Processing Systems},
editor={A. Beygelzimer and Y. Dauphin and P. Liang and J. Wortman Vaughan},
year={2021},
url={https://openreview.net/forum?id=-tVD13hOsQ3}
}

@inproceedings{focal,
 author = {Mukhoti, Jishnu and Kulharia, Viveka and Sanyal, Amartya and Golodetz, Stuart and Torr, Philip and Dokania, Puneet},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {15288--15299},
 publisher = {Curran Associates, Inc.},
 title = {Calibrating Deep Neural Networks using Focal Loss},
 url = {https://proceedings.neurips.cc/paper/2020/file/aeb7b30ef1d024a76f21a1d40e30c302-Paper.pdf},
 volume = {33},
 year = {2020}
}


@article{krishnan2020improving,
  author={Ranganath Krishnan and Omesh Tickoo},
  title={Improving model calibration with accuracy versus uncertainty optimization},
  journal={Advances in Neural Information Processing Systems},
  year={2020}
}

@inproceedings{revisiting,
 author = {Minderer, Matthias and Djolonga, Josip and Romijnders, Rob and Hubis, Frances and Zhai, Xiaohua and Houlsby, Neil and Tran, Dustin and Lucic, Mario},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
 pages = {15682--15694},
 publisher = {Curran Associates, Inc.},
 title = {Revisiting the Calibration of Modern Neural Networks},
 url = {https://proceedings.neurips.cc/paper/2021/file/8420d359404024567b5aefda1231af24-Paper.pdf},
 volume = {34},
 year = {2021}
}

@InProceedings{Jacob_2018_CVPR,
author = {Jacob, Benoit and Kligys, Skirmantas and Chen, Bo and Zhu, Menglong and Tang, Matthew and Howard, Andrew and Adam, Hartwig and Kalenichenko, Dmitry},
title = {Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference},
booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2018}
}

@misc{whitequant,
  doi = {10.48550/ARXIV.2106.08295},
  
  url = {https://arxiv.org/abs/2106.08295},
  
  author = {Nagel, Markus and Fournarakis, Marios and Amjad, Rana Ali and Bondarenko, Yelysei and van Baalen, Mart and Blankevoort, Tijmen},
  
  keywords = {Machine Learning (cs.LG), Artificial Intelligence (cs.AI), Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {A White Paper on Neural Network Quantization},
  
  publisher = {arXiv},
  
  year = {2021},
  
  copyright = {Creative Commons Attribution 4.0 International}
}
@misc{whitecnn,
  doi = {10.48550/ARXIV.1806.08342},
  
  url = {https://arxiv.org/abs/1806.08342},
  
  author = {Krishnamoorthi, Raghuraman},
  
  keywords = {Machine Learning (cs.LG), Computer Vision and Pattern Recognition (cs.CV), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Quantizing deep convolutional networks for efficient inference: A whitepaper},
  
  publisher = {arXiv},
  
  year = {2018},
  
  copyright = {Creative Commons Zero v1.0 Universal}
}


@inproceedings{hfp8,
 author = {Sun, Xiao and Choi, Jungwook and Chen, Chia-Yu and Wang, Naigang and Venkataramani, Swagath and Srinivasan, Vijayalakshmi (Viji) and Cui, Xiaodong and Zhang, Wei and Gopalakrishnan, Kailash},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Hybrid 8-bit Floating Point (HFP8) Training and Inference for Deep Neural Networks},
 url = {https://proceedings.neurips.cc/paper/2019/file/65fc9fb4897a89789352e211ca2d398f-Paper.pdf},
 volume = {32},
 year = {2019}
}

@inproceedings{4bittrain,
 author = {Sun, Xiao and Wang, Naigang and Chen, Chia-Yu and Ni, Jiamin and Agrawal, Ankur and Cui, Xiaodong and Venkataramani, Swagath and El Maghraoui, Kaoutar and Srinivasan, Vijayalakshmi (Viji) and Gopalakrishnan, Kailash},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {1796--1807},
 publisher = {Curran Associates, Inc.},
 title = {Ultra-Low Precision 4-bit Training of Deep Neural Networks},
 url = {https://proceedings.neurips.cc/paper/2020/file/13b919438259814cd5be8cb45877d577-Paper.pdf},
 volume = {33},
 year = {2020}
}

@article{Gholami2022ASO,
  title={A Survey of Quantization Methods for Efficient Neural Network Inference},
  author={Amir Gholami and Sehoon Kim and Zhen Dong and Zhewei Yao and Michael W. Mahoney and Kurt Keutzer},
  journal={ArXiv},
  year={2022},
  volume={abs/2103.13630}
}

@article{Teerapittayanon2016BranchyNetFI,
  title={BranchyNet: Fast inference via early exiting from deep neural networks},
  author={Surat Teerapittayanon and Bradley McDanel and H. T. Kung},
  journal={2016 23rd International Conference on Pattern Recognition (ICPR)},
  year={2016},
  pages={2464-2469}
}

@inproceedings{icml19shallowdeepnetworks,
  Title = {{Shallow-Deep Networks}: Understanding and Mitigating Network Overthinking},
  Author = {{Yigitcan Kaya and Sanghyun Hong and Tudor Dumitras}},
  Booktitle = {Proceedings of the 2019 International Conference on Machine Learning (ICML)},
  Address = {Long Beach, CA},
  Month = {Jun},
  year = {2019}
}

@inproceedings{Huang2018MultiScaleDN,
  title={Multi-Scale Dense Networks for Resource Efficient Image Classification},
  author={Gao Huang and Danlu Chen and Tianhong Li and Felix Wu and Laurens van der Maaten and Kilian Q. Weinberger},
  booktitle={ICLR},
  year={2018}
}
     
@inproceedings{xin-etal-2020-deebert,
    title = "{D}ee{BERT}: Dynamic Early Exiting for Accelerating {BERT} Inference",
    author = "Xin, Ji  and
      Tang, Raphael  and
      Lee, Jaejun  and
      Yu, Yaoliang  and
      Lin, Jimmy",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.acl-main.204",
    pages = "2246--2251",
}


@InProceedings{Kouris2021MultiExitSS,
author="Kouris, Alexandros
and Venieris, Stylianos I.
and Laskaridis, Stefanos
and Lane, Nicholas",
editor="Avidan, Shai
and Brostow, Gabriel
and Ciss{\'e}, Moustapha
and Farinella, Giovanni Maria
and Hassner, Tal",
title="Multi-Exit Semantic Segmentation Networks",
booktitle="Computer Vision -- ECCV 2022",
year="2022",
publisher="Springer Nature Switzerland",
address="Cham",
pages="330--349",
abstract="Semantic segmentation arises as the backbone of many vision systems, spanning from self-driving cars and robot navigation to augmented reality and teleconferencing. Frequently operating under stringent latency constraints within a limited resource envelope, optimising for efficient execution becomes important. At the same time, the heterogeneous capabilities of the target platforms and the diverse constraints of different applications require the design and training of multiple target-specific segmentation models, leading to excessive maintenance costs. To this end, we propose a framework for converting state-of-the-art segmentation CNNs to Multi-Exit Semantic Segmentation (MESS) networks: specially trained models that employ parametrised early exits along their depth to i) dynamically save computation during inference on easier samples and ii) save training and maintenance cost by offering a post-training customisable speed-accuracy trade-off. Designing and training such networks naively can hurt performance. Thus, we propose a novel two-staged training scheme for multi-exit networks. Furthermore, the parametrisation of MESS enables co-optimising the number, placement and architecture of the attached segmentation heads along with the exit policy, upon deployment via exhaustive search in <1 GPUh. This allows MESS to rapidly adapt to the device capabilities and application requirements for each target use-case, offering a train-once-deploy-everywhere solution. MESS variants achieve latency gains of up to 2.83{\$}{\$}{\backslash}times {\$}{\$}{\texttimes}with the same accuracy, or 5.33 pp higher accuracy for the same computational budget, compared to the original backbone network. Lastly, MESS delivers orders of magnitude faster architectural customisation, compared to state-of-the-art techniques.",
isbn="978-3-031-19803-8"
}



@inproceedings{zhou2020bert,
 author = {Zhou, Wangchunshu and Xu, Canwen and Ge, Tao and McAuley, Julian and Xu, Ke and Wei, Furu},
 booktitle = {Advances in Neural Information Processing Systems},
 pages = {18330--18341},
 publisher = {Curran Associates, Inc.},
 title = {BERT Loses Patience: Fast and Robust Inference with Early Exit},
 url = {https://proceedings.neurips.cc/paper/2020/file/d4dd111a4fd973394238aca5c05bebe3-Paper.pdf},
 volume = {33},
 year = {2020}
}

@article{Bakhtiarnia2021MultiExitVT,
  title={Multi-Exit Vision Transformer for Dynamic Inference},
  author={Arian Bakhtiarnia and Qi Zhang and Alexandros Iosifidis},
  journal={ArXiv},
  year={2021},
  volume={abs/2106.15183}
}

@article{Bakhtiarnia2022SingleLayerVT,
  title={Single-Layer Vision Transformers for More Accurate Early Exits with Less Overhead},
  author={Arian Bakhtiarnia and Qi Zhang and Alexandros Iosifidis},
  journal={Neural networks : the official journal of the International Neural Network Society},
  year={2022},
  volume={153},
  pages={
          461-473
        }
}

@article{Wang_Li_2021, title={Harmonized Dense Knowledge Distillation Training for Multi-Exit Architectures}, volume={35}, url={https://ojs.aaai.org/index.php/AAAI/article/view/17225}, abstractNote={Multi-exit architectures, in which a sequence of intermediate classifiers are introduced at different depths of the feature layers, perform adaptive computation by early exiting ``easy&quot; samples to speed up the inference. In this paper, a novel Harmonized Dense Knowledge Distillation (HDKD) training method for multi-exit architecture is designed to encourage each exit to flexibly learn from all its later exits. In particular, a general dense knowledge distillation training objective is proposed to incorporate all possible beneficial supervision information for multi-exit learning, where a harmonized weighting scheme is designed for the multi-objective optimization problem consisting of multi-exit classification loss and dense distillation loss. A bilevel optimization algorithm is introduced for alternatively updating the weights of multiple objectives and the multi-exit network parameters. Specifically, the loss weighting parameters are optimized with respect to its performance on validation set by gradient descent. Experiments on CIFAR100 and ImageNet show that the HDKD strategy harmoniously improves the performance of the state-of-the-art multi-exit neural networks. Moreover, this method does not require within architecture modifications and can be effectively combined with other previously-proposed training techniques and further boosts the performance.}, number={11}, journal={Proceedings of the AAAI Conference on Artificial Intelligence}, author={Wang, Xinglu and Li, Yingming}, year={2021}, month={May}, pages={10218-10226} }


@INPROCEEDINGS{phuong,
  author={Phuong, Mary and Lampert, Christoph},
  booktitle={2019 IEEE/CVF International Conference on Computer Vision (ICCV)}, 
  title={Distillation-Based Training for Multi-Exit Architectures}, 
  year={2019},
  volume={},
  number={},
  pages={1355-1364},
  doi={10.1109/ICCV.2019.00144}}
  
@article{Li2019ImprovedTF,
  title={Improved Techniques for Training Adaptive Deep Networks},
  author={Hao Li and Hong Zhang and Xiaojuan Qi and Ruigang Yang and Gao Huang},
  journal={2019 IEEE/CVF International Conference on Computer Vision (ICCV)},
  year={2019},
  pages={1891-1900}
}

@article{Kouris2022AdaptableMV,
  title={Adaptable mobile vision systems through multi-exit neural networks},
  author={Alexandros Kouris and Stylianos I. Venieris and Stefanos Laskaridis and Nicholas D. Lane},
  journal={Proceedings of the 20th Annual International Conference on Mobile Systems, Applications and Services},
  year={2022}
}

@article{Xia2021AnUD,
  title={An Underexplored Dilemma between Confidence and Calibration in Quantized Neural Networks},
  author={Guoxuan Xia and Sangwon Ha and Tiago Azevedo and Partha P. Maji},
  booktitle={ICBINB@NeurIPS 2021},
  year={2021},
}

@inproceedings{bayesianbits,
 author = {van Baalen, Mart and Louizos, Christos and Nagel, Markus and Amjad, Rana Ali and Wang, Ying and Blankevoort, Tijmen and Welling, Max},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {5741--5752},
 publisher = {Curran Associates, Inc.},
 title = {Bayesian Bits: Unifying Quantization and Pruning},
 url = {https://proceedings.neurips.cc/paper/2020/file/3f13cf4ddf6fc50c0d39a1d5aeb57dd8-Paper.pdf},
 volume = {33},
 year = {2020}
}

@inproceedings{Neklyudov2017StructuredBP,
  title={Structured Bayesian Pruning via Log-Normal Multiplicative Noise},
  author={Kirill Neklyudov and Dmitry Molchanov and Arsenii Ashukha and Dmitry P. Vetrov},
  booktitle={NIPS},
  year={2017}
}


@InProceedings{pmlr-v158-qendro21a,
  title = 	 {Early Exit Ensembles for Uncertainty Quantification},
  author =       {Qendro, Lorena and Campbell, Alexander and Lio, Pietro and Mascolo, Cecilia},
  booktitle = 	 {Proceedings of Machine Learning for Health},
  pages = 	 {181--195},
  year = 	 {2021},
  editor = 	 {Roy, Subhrajit and Pfohl, Stephen and Rocheteau, Emma and Tadesse, Girmaw Abebe and Oala, Luis and Falck, Fabian and Zhou, Yuyin and Shen, Liyue and Zamzmi, Ghada and Mugambi, Purity and Zirikly, Ayah and McDermott, Matthew B. A. and Alsentzer, Emily},
  volume = 	 {158},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {04 Dec},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v158/qendro21a/qendro21a.pdf},
  url = 	 {https://proceedings.mlr.press/v158/qendro21a.html},
  abstract = 	 {Deep learning is increasingly used for decision-making in health applications. However, commonly used deep learning models are deterministic and are unable to provide any estimate of predictive uncertainty. Quantifying model uncertainty is crucial for reducing the risk of misdiagnosis by informing practitioners of low-confident predictions. To address this issue, we propose early exit ensembles, a novel framework capable of capturing predictive uncertainty via an implicit ensemble of early exits. We evaluate our approach on the task of classification using three state-of-the-art deep learning architectures applied to three medical imaging datasets. Our experiments show that early exit ensembles provide better-calibrated uncertainty compared to Monte Carlo dropout and deep ensembles using just a single forward-pass of the model. Depending on the dataset and baseline, early exit ensembles can improve uncertainty metrics up to 2x, while increasing accuracy by up to 2% over its single model counterpart. Finally, our results suggest that by providing well-calibrated predictive uncertainty for both in- and out-of-distribution inputs, early exit ensembles have the potential to improve trustworthiness of models in high-risk medical decision-making.}
}


@inproceedings{
havasi2021training,
title={Training independent subnetworks for robust prediction},
author={Marton Havasi and Rodolphe Jenatton and Stanislav Fort and Jeremiah Zhe Liu and Jasper Snoek and Balaji Lakshminarayanan and Andrew Mingbo Dai and Dustin Tran},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=OGg9XnKxFAH}
}

@inproceedings{
Wen2020BatchEnsemble:,
title={BatchEnsemble: an Alternative Approach to Efficient Ensemble and Lifelong Learning},
author={Yeming Wen and Dustin Tran and Jimmy Ba},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=Sklf1yrYDr}
}

@InProceedings{Xia_2022_ACCV,
    author    = {Xia, Guoxuan and Bouganis, Christos-Savvas},
    title     = {Augmenting Softmax Information for Selective Classification with Out-of-Distribution Data},
    booktitle = {Proceedings of the Asian Conference on Computer Vision (ACCV)},
    month     = {December},
    year      = {2022},
    pages     = {1995-2012}
}

@article{Xia2022OnTU,
  title={On the Usefulness of Deep Ensemble Diversity for Out-of-Distribution Detection},
  author={Guoxuan Xia and Christos-Savvas Bouganis},
  journal={ArXiv},
  year={2022},
  volume={abs/2207.07517}
}

@inproceedings{
yang2021knowledge,
title={Knowledge distillation via softmax regression representation learning},
author={Jing Yang and Brais Martinez and Adrian Bulat and Georgios Tzimiropoulos},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=ZzwDy_wiWv}
}


@InProceedings{Lin_2021_CVPR,
    author    = {Lin, Ziqian and Roy, Sreya Dutta and Li, Yixuan},
    title     = {MOOD: Multi-Level Out-of-Distribution Detection},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2021},
    pages     = {15313-15323}
}

@inproceedings{Zhang2019SCANAS,
  title={SCAN: A Scalable Neural Networks Framework Towards Compact and Efficient Models},
  author={Linfeng Zhang and Zhanhong Tan and Jiebo Song and Jingwei Chen and Chenglong Bao and Kaisheng Ma},
  booktitle={NeurIPS},
  year={2019}
}


@InProceedings{pmlr-v161-ferianc21a,
  title = 	 {On the effects of quantisation on model uncertainty in Bayesian neural networks},
  author =       {Ferianc, Martin and Maji, Partha and Mattina, Matthew and Rodrigues, Miguel},
  booktitle = 	 {Proceedings of the Thirty-Seventh Conference on Uncertainty in Artificial Intelligence},
  pages = 	 {929--938},
  year = 	 {2021},
  editor = 	 {de Campos, Cassio and Maathuis, Marloes H.},
  volume = 	 {161},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {27--30 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v161/ferianc21a/ferianc21a.pdf},
  url = 	 {https://proceedings.mlr.press/v161/ferianc21a.html},
  abstract = 	 {Bayesian neural networks (BNNs) are making significant progress in many research areas where decision-making needs to be accompanied by uncertainty estimation. Being able to quantify uncertainty while making decisions is essential for understanding when the model is over-/under-confident, and hence BNNs are attracting interest in safety-critical applications, such as autonomous driving, healthcare, and robotics. Nevertheless, BNNs have not been as widely used in industrial practice, mainly because of their increased memory and compute costs. In this work, we investigate quantisation of BNNs by compressing 32-bit floating-point weights and activations to their integer counterparts, that has already been successful in reducing the compute demand in standard pointwise neural networks. We study three types of quantised BNNs, we evaluate them under a wide range of different settings, and we empirically demonstrate that a uniform quantisation scheme applied to BNNs does not substantially decrease their quality of uncertainty estimation.}
}

@article{Tan_Li_Wang_Huang_Xu_2021, title={Empowering Adaptive Early-Exit Inference with Latency Awareness}, volume={35}, url={https://ojs.aaai.org/index.php/AAAI/article/view/17181}, DOI={10.1609/aaai.v35i11.17181}, abstractNote={With the capability of trading accuracy for latency on-the-fly, the technique of adaptive early-exit inference has emerged as a promising line of research to accelerate the deep learning inference. However, studies in this line of research commonly use a group of thresholds to control the accuracy-latency trade-off, where a thorough and general methodology on how to determine these thresholds has not been conducted yet, especially with regard to the common requirements of average inference latency. To address this issue and enable latency-aware adaptive early-exit inference, in the present paper, we approximately formulate the threshold determination problem of finding the accuracy-maximum threshold setting that meets a given average latency requirement, and then propose a threshold determination method to tackle our formulated non-convex problem. Theoretically, we prove that, for certain parameter settings, our method finds an approximate stationary point of the formulated problem. Empirically, on top of various models across multiple datasets (CIFAR-10, CIFAR-100, ImageNet and two time-series datasets), we show that our method can well handle the average latency requirements, and consistently finds good threshold settings in negligible time.}, number={11}, journal={Proceedings of the AAAI Conference on Artificial Intelligence}, author={Tan, Xinrui and Li, Hongjia and Wang, Liming and Huang, Xueqing and Xu, Zhen}, year={2021}, month={May}, pages={9825-9833} }


@article{Yang2020ResolutionAN,
  title={Resolution Adaptive Networks for Efficient Inference},
  author={Le Yang and Yizeng Han and Xi Chen and Shiji Song and Jifeng Dai and Gao Huang},
  journal={2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2020},
  pages={2366-2375}
}

@inproceedings{Sun2022OutofdistributionDW,
  title={Out-of-distribution Detection with Deep Nearest Neighbors},
  author={Yiyou Sun and Yifei Ming and Xiaojin Zhu and Yixuan Li},
  booktitle={ICML},
  year={2022}
}

@inproceedings{
sehwag2021ssd,
title={{\{}SSD{\}}: A Unified Framework for Self-Supervised Outlier Detection},
author={Vikash Sehwag and Mung Chiang and Prateek Mittal},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=v5gjXpmR8J}
}

@inproceedings{
jaeger2023a,
title={A Call to Reflect on Evaluation Practices for Failure Detection in Image Classification},
author={Paul F Jaeger and Carsten Tim L{\"u}th and Lukas Klein and Till J. Bungert},
booktitle={International Conference on Learning Representations},
year={2023},
url={https://openreview.net/forum?id=YnkGMIh0gvX}
}

@inproceedings{gustafsson2020evaluating,
  title={Evaluating scalable Bayesian deep learning methods for robust computer vision},
  author={Gustafsson, Fredrik K and Danelljan, Martin and Sch{\"o}n, Thomas B},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops},
  year={2020}
}

@InProceedings{LDU,
author="Franchi, Gianni
and Yu, Xuanlong
and Bursuc, Andrei
and Aldea, Emanuel
and Dubuisson, Severine
and Filliat, David",
title="Latent Discriminant Deterministic Uncertainty",
booktitle="Computer Vision -- ECCV 2022",
year="2022",

address="Cham",

abstract="Predictive uncertainty estimation is essential for deploying Deep Neural Networks in real-world autonomous systems. However, most successful approaches are computationally intensive. In this work, we attempt to address these challenges in the context of autonomous driving perception tasks. Recently proposed Deterministic Uncertainty Methods (DUM) can only partially meet such requirements as their scalability to complex computer vision tasks is not obvious. In this work we advance a scalable and effective DUM for high-resolution semantic segmentation, that relaxes the Lipschitz constraint typically hindering practicality of such architectures. We learn a discriminant latent space by leveraging a distinction maximization layer over an arbitrarily-sized set of trainable prototypes. Our approach achieves competitive results over Deep Ensembles, the state of the art for uncertainty prediction, on image classification, segmentation and monocular depth estimation tasks. Our code is available at https://github.com/ENSTA-U2IS/LDU.",
isbn="978-3-031-19775-8"
}

@article{Beluch2018ThePO,
  title={The Power of Ensembles for Active Learning in Image Classification},
  author={William H. Beluch and Tim Genewein and A. N{\"u}rnberger and Jan M. K{\"o}hler},
  journal={2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  year={2018},
  pages={9368-9377}
}

@inproceedings{Wu2020EnsembleAF,
  title={Ensemble Approaches for Uncertainty in Spoken Language Assessment},
  author={Xixin Wu and Kate Knill and Mark John Francis Gales and Andrey Malinin},
  booktitle={Interspeech},
  year={2020}
}

@article{Fathullah2020EnsembleDA,
  title={Ensemble Distillation Approaches for Grammatical Error Correction},
  author={Yassir Fathullah and Mark John Francis Gales and Andrey Malinin},
  journal={ICASSP 2021 - 2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  year={2020},
  pages={2745-2749}
}

@article{Kondratyuk2020WhenES,
  title={When Ensembling Smaller Models is More Efficient than Single Large Models},
  author={D. Kondratyuk and Mingxing Tan and Matthew A. Brown and Boqing Gong},
  journal={ArXiv},
  year={2020},
  volume={abs/2005.00570}
}

@inproceedings{powerlaw,
 author = {Lobacheva, Ekaterina and Chirkova, Nadezhda and Kodryan, Maxim and Vetrov, Dmitry P},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {2375--2385},
 publisher = {Curran Associates, Inc.},
 title = {On Power Laws in Deep Ensembles},
 url = {https://proceedings.neurips.cc/paper/2020/file/191595dc11b4d6e54f01504e3aa92f96-Paper.pdf},
 volume = {33},
 year = {2020}
}

@inproceedings{
wang2022wisdom,
title={Wisdom of Committees: An Overlooked Approach To Faster and More Accurate Models},
author={Xiaofang Wang and Dan Kondratyuk and Eric Christiansen and Kris M. Kitani and Yair Movshovitz-Attias and Elad Eban},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=MvO2t0vbs4-}
}

@article{Wang2017IDKCF,
  title={IDK Cascades: Fast Deep Learning by Learning not to Overthink},
  author={Xin Wang and Yujia Luo and Daniel Crankshaw and Alexey Tumanov and Joseph Gonzalez},
  booktitle={Proceedings of The 34th Uncertainty in Artificial Intelligence Conference},
  year={2018}
}

@article{Laurent2022PackedEnsemblesFE,
  title={Packed-Ensembles for Efficient Uncertainty Estimation},
  author={Olivier Laurent and Adrien Lafage and Enzo Tartaglione and Geoffrey Daniel and Jean-Marc Martinez and Andrei Bursuc and Gianni Franchi},
  journal={ArXiv},
  year={2022},
  volume={abs/2210.09184}
}

@article{Zhao2020TowardBA,
  title={Toward Better Accuracy-Efficiency Trade-Offs: Divide and Co-Training},
  author={Shuai Zhao and Liguang Zhou and Wenxiao Wang and D. Cai and Tin Lun Lam and Yangsheng Xu},
  journal={IEEE Transactions on Image Processing},
  year={2020},
  volume={31},
  pages={5869-5880}
}


@InProceedings{pmlr-v157-deng21a,
  title = 	 {Ensembling With a Fixed Parameter Budget: When Does It Help and Why?},
  author =       {Deng, Didan and Shi, Emil Bertram},
  booktitle = 	 {Proceedings of The 13th Asian Conference on Machine Learning},
  pages = 	 {1176--1191},
  year = 	 {2021},
  editor = 	 {Balasubramanian, Vineeth N. and Tsang, Ivor},
  volume = 	 {157},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {17--19 Nov},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v157/deng21a/deng21a.pdf},
  url = 	 {https://proceedings.mlr.press/v157/deng21a.html},
  abstract = 	 {Given a fixed parameter budget, one can build a single large neural network or create a memory-split ensemble: a pool of several smaller networks with the same total parameter count as the single network. A memory-split ensemble can outperform its single model counterpart (Lobacheva et al., 2020): a phenomenon known as the memory-split advantage (MSA). The reasons for MSA are still not yet fully understood. In particular, it is difficult in practice to predict when it will exist. This paper sheds light on the reasons underlying MSA using random feature theory. We study the dependence of the MSA on several factors: the parameter budget, the training set size, the L2 regularization and the Stochastic Gradient Descent (SGD) hyper-parameters. Using the bias-variance decomposition, we show that MSA exists when the reduction in variance due to the ensemble (\ie, \textit{ensemble gain}) exceeds the increase in squared bias due to the smaller size of the individual networks (\ie, \textit{shrinkage cost}). Taken together, our theoretical analysis demonstrates that the MSA mainly exists for the small parameter budgets relative to the training set size, and that memory-splitting can be understood as a type of regularization. Adding other forms of regularization, \eg L2 regularization, reduces the MSA. Thus, the potential benefit of memory-splitting lies primarily in the possibility of speed-up via parallel computation. Our empirical experiments with deep neural networks and large image datasets show that MSA is not a general phenomenon, but mainly exists when the number of training iterations is small.}
}

@InProceedings{SC_VQA,
author="Whitehead, Spencer
and Petryk, Suzanne
and Shakib, Vedaad
and Gonzalez, Joseph
and Darrell, Trevor
and Rohrbach, Anna
and Rohrbach, Marcus",
editor="Avidan, Shai
and Brostow, Gabriel
and Ciss{\'e}, Moustapha
and Farinella, Giovanni Maria
and Hassner, Tal",
title="Reliable Visual Question Answering: Abstain Rather Than Answer Incorrectly",
booktitle="Computer Vision -- ECCV 2022",
year="2022",
publisher="Springer Nature Switzerland",
address="Cham",
pages="148--166",
abstract="Machine learning has advanced dramatically, narrowing the accuracy gap to humans in multimodal tasks like visual question answering (VQA). However, while humans can say ``I don't know'' when they are uncertain (i.e., abstain from answering a question), such ability has been largely neglected in multimodal research, despite the importance of this problem to the usage of VQA in real settings. In this work, we promote a problem formulation for reliable VQA, where we prefer abstention over providing an incorrect answer. We first enable abstention capabilities for several VQA models, and analyze both their coverage, the portion of questions answered, and risk, the error on that portion. For that, we explore several abstention approaches. We find that although the best performing models achieve over 71{\%} accuracy on the VQA v2 dataset, introducing the option to abstain by directly using a model's softmax scores limits them to answering less than 8{\%} of the questions to achieve a low risk of error (i.e., 1{\%}). This motivates us to utilize a multimodal selection function to directly estimate the correctness of the predicted answers, which we show can increase the coverage by, for example, {\$}{\$}2.4{\backslash}times {\$}{\$}2.4{\texttimes}from 6.8{\%} to 16.3{\%} at 1{\%} risk. While it is important to analyze both coverage and risk, these metrics have a trade-off which makes comparing VQA models challenging. To address this, we also propose an Effective Reliability metric for VQA that places a larger cost on incorrect answers compared to abstentions. This new problem formulation, metric, and analysis for VQA provide the groundwork for building effective and reliable VQA models that have the self-awareness to abstain if and only if they don't know the answer. Code and Models: https://github.com/facebookresearch/reliable{\_}vqa.",
isbn="978-3-031-20059-5"
}

@inproceedings{
cen2023the,
title={The Devil is in the Wrongly-classified Samples: Towards Unified Open-set Recognition},
author={Jun CEN and Di Luan and Shiwei Zhang and Yixuan Pei and Yingya Zhang and Deli Zhao and Shaojie Shen and Qifeng Chen},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=xLr0I_xYGAs}
}

@inproceedings{adaptive,
author = {Laskaridis, Stefanos and Kouris, Alexandros and Lane, Nicholas D.},
title = {Adaptive Inference through Early-Exit Networks: Design, Challenges and Directions},
year = {2021},
isbn = {9781450385978},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3469116.3470012},
doi = {10.1145/3469116.3470012},
abstract = {DNNs are becoming less and less over-parametrised due to recent advances in efficient model design, through careful hand-crafted or NAS-based methods. Relying on the fact that not all inputs require the same amount of computation to yield a confident prediction, adaptive inference is gaining attention as a prominent approach for pushing the limits of efficient deployment. Particularly, early-exit networks comprise an emerging direction for tailoring the computation depth of each input sample at runtime, offering complementary performance gains to other efficiency optimisations. In this paper, we decompose the design methodology of early-exit networks to its key components and survey the recent advances in each one of them. We also position early-exiting against other efficient inference solutions and provide our insights on the current challenges and most promising future directions for research in the field.},
booktitle = {Proceedings of the 5th International Workshop on Embedded and Mobile Deep Learning},
pages = {1–6},
numpages = {6},
location = {Virtual, WI, USA},
series = {EMDL'21}
}

@ARTICLE {dynamicsurvey,
author = {Y. Han and G. Huang and S. Song and L. Yang and H. Wang and Y. Wang},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
title = {Dynamic Neural Networks: A Survey},
year = {2022},
volume = {44},
number = {11},
issn = {1939-3539},
pages = {7436-7456},
doi = {10.1109/TPAMI.2021.3117837},
publisher = {IEEE Computer Society},
address = {Los Alamitos, CA, USA},
month = {nov}
}

@inproceedings{chen2020learning,
  title={Learning to stop while learning to predict},
  author={Chen, Xinshi and Dai, Hanjun and Li, Yu and Gao, Xin and Song, Le},
  booktitle={International Conference on Machine Learning},
  pages={1520--1530},
  year={2020},
  organization={PMLR}
}

@INPROCEEDINGS{learnexit,
  author={Scardapane, Simone and Comminiello, Danilo and Scarpiniti, Michele and Baccarelli, Enzo and Uncini, Aurelio},
  booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, 
  title={Differentiable Branching In Deep Networks for Fast Inference}, 
  year={2020},
  volume={},
  number={},
  pages={4167-4171},
  doi={10.1109/ICASSP40776.2020.9054209}}


@inproceedings{
pinto2022using,
title={Using Mixup as a Regularizer Can Surprisingly Improve Accuracy \& Out-of-Distribution Robustness},
author={Francesco Pinto and Harry Yang and Ser-Nam Lim and Philip Torr and Puneet K. Dokania},
booktitle={Advances in Neural Information Processing Systems},
editor={Alice H. Oh and Alekh Agarwal and Danielle Belgrave and Kyunghyun Cho},
year={2022},
url={https://openreview.net/forum?id=5j6fWcPccO}
}

@inproceedings{
pinto2021mixmaxent,
title={Mix-MaxEnt: Improving Accuracy and Uncertainty Estimates of Deterministic Neural Networks},
author={Francesco Pinto and Harry Yang and Ser-Nam Lim and Philip Torr and Puneet K. Dokania},
booktitle={NeurIPS 2021 Workshop on Distribution Shifts: Connecting Methods and Applications},
year={2021},
url={https://openreview.net/forum?id=hlVgM8XcssV}
}

@article{uncrev,
title = {A review of uncertainty quantification in deep learning: Techniques, applications and challenges},
journal = {Information Fusion},
volume = {76},
pages = {243-297},
year = {2021},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2021.05.008},
url = {https://www.sciencedirect.com/science/article/pii/S1566253521001081},
author = {Moloud Abdar and Farhad Pourpanah and Sadiq Hussain and Dana Rezazadegan and Li Liu and Mohammad Ghavamzadeh and Paul Fieguth and Xiaochun Cao and Abbas Khosravi and U. Rajendra Acharya and Vladimir Makarenkov and Saeid Nahavandi},
keywords = {Artificial intelligence, Uncertainty quantification, Deep learning, Machine learning, Bayesian statistics, Ensemble learning},
abstract = {Uncertainty quantification (UQ) methods play a pivotal role in reducing the impact of uncertainties during both optimization and decision making processes. They have been applied to solve a variety of real-world problems in science and engineering. Bayesian approximation and ensemble learning techniques are two widely-used types of uncertainty quantification (UQ) methods. In this regard, researchers have proposed different UQ methods and examined their performance in a variety of applications such as computer vision (e.g., self-driving cars and object detection), image processing (e.g., image restoration), medical image analysis (e.g., medical image classification and segmentation), natural language processing (e.g., text classification, social media texts and recidivism risk-scoring), bioinformatics, etc. This study reviews recent advances in UQ methods used in deep learning, investigates the application of these methods in reinforcement learning, and highlights fundamental research challenges and directions associated with UQ.}
}

@inproceedings{
vaze2022openset,
title={Open-Set Recognition: A Good Closed-Set Classifier is All You Need},
author={Sagar Vaze and Kai Han and Andrea Vedaldi and Andrew Zisserman},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=5hLP5JY9S2d}
}


@InProceedings{pmlr-v162-ming22a,
  title = 	 {{POEM}: Out-of-Distribution Detection with Posterior Sampling},
  author =       {Ming, Yifei and Fan, Ying and Li, Yixuan},
  booktitle = 	 {Proceedings of the 39th International Conference on Machine Learning},
  pages = 	 {15650--15665},
  year = 	 {2022},
  editor = 	 {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
  volume = 	 {162},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {17--23 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v162/ming22a/ming22a.pdf},
  url = 	 {https://proceedings.mlr.press/v162/ming22a.html},
  abstract = 	 {Out-of-distribution (OOD) detection is indispensable for machine learning models deployed in the open world. Recently, the use of an auxiliary outlier dataset during training (also known as outlier exposure) has shown promising performance. As the sample space for potential OOD data can be prohibitively large, sampling informative outliers is essential. In this work, we propose a novel posterior sampling based outlier mining framework, POEM, which facilitates efficient use of outlier data and promotes learning a compact decision boundary between ID and OOD data for improved detection. We show that POEM establishes state-of-the-art performance on common benchmarks. Compared to the current best method that uses a greedy sampling strategy, POEM improves the relative performance by 42.0% and 24.2% (FPR95) on CIFAR-10 and CIFAR-100, respectively. We further provide theoretical insights on the effectiveness of POEM for OOD detection.}
}

@inproceedings{
zhu2022boosting,
title={Boosting Out-of-distribution Detection with Typical Features},
author={Yao Zhu and YueFeng Chen and Chuanlong Xie and Xiaodan Li and Rong Zhang and Hui Xue' and Xiang Tian and bolun zheng and Yaowu Chen},
booktitle={Advances in Neural Information Processing Systems},
editor={Alice H. Oh and Alekh Agarwal and Danielle Belgrave and Kyunghyun Cho},
year={2022},
url={https://openreview.net/forum?id=4maAiUt0A4}
}

@inproceedings{sun2022dice,
  title={DICE: Leveraging Sparsification for Out-of-Distribution Detection},
  author={Sun, Yiyou and Li, Yixuan},
  booktitle={European Conference on Computer Vision},
  year={2022}
}


@InProceedings{pmlr-v180-fathullah22a,
  title = 	 {Self-distribution distillation: efficient uncertainty estimation},
  author =       {Fathullah, Yassir and Gales, Mark J. F.},
  booktitle = 	 {Proceedings of the Thirty-Eighth Conference on Uncertainty in Artificial Intelligence},
  pages = 	 {663--673},
  year = 	 {2022},
  editor = 	 {Cussens, James and Zhang, Kun},
  volume = 	 {180},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {01--05 Aug},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v180/fathullah22a/fathullah22a.pdf},
  url = 	 {https://proceedings.mlr.press/v180/fathullah22a.html},
  abstract = 	 {Deep learning is increasingly being applied in safety-critical domains. For these scenarios it is important to know the level of uncertainty in a model’s prediction to ensure appropriate decisions are made by the system. Deep ensembles are the de-facto standard approach to obtaining various measures of uncertainty. However, ensembles often significantly increase the resources required in the training and/or deployment phases. Approaches have been developed that typically address the costs in one of these phases. In this work we propose a novel training approach, self-distribution distillation (S2D), which is able to efficiently train a single model that can estimate uncertainties. Furthermore it is possible to build ensembles of these models and apply hierarchical ensemble distillation approaches. Experiments on CIFAR-100 showed that S2D models outperformed standard models and Monte-Carlo dropout. Additional out-of-distribution detection experiments on LSUN, Tiny ImageNet, SVHN showed that even a standard deep ensemble can be outperformed using S2D based ensembles and novel distilled models.}
}

@book{PNG,
author = {Roelofs, Greg and Koman, Richard},
title = {PNG: The Definitive Guide},
year = {1999},
isbn = {1565925424},
publisher = {O'Reilly and Associates, Inc.},
address = {USA},
abstract = {From the Publisher:PNG (Portable Network Graphics) is the next-generation graphics file format for the Web. Designed as an open-source format to replace GIF (which uses a proprietary compression scheme, for which software makers must pay a licensing fee), PNG is better, smaller, more extensible, and FREE. Already supported by major software like Macromedia Fireworks, Adobe Photoshop, and Microsoft Office 98, as well as Netscape Navigator and Microsoft Internet Explorer, PNG is the next big thing in computer graphics. Similar to the GIF and TIFF formats, but far better, PNG will soon become the preferred file format for color-critical web images and high-quality graphics interchange. Even for noncritical images, PNG will likely replace GIF as the standard web format. PNG supports lossless compression, variable transparency information, and a range of color depths. PNG also provides direct support for gamma correction, the cross-platform control of image "brightness," and transparency. PNG: The Definitive Guide addresses the needs of graphic designers who want to get the most out of the format and programmers who want to add full PNG-support to their own applications. Specifically, Roelofs covers implementing PNG with the libpng C library. The book includes explanations of important improvements with PNG, such as gamma correction and the standard color spaces for precise reproduction of image colors on a wide range of systems. PNG: The Definitive Guide is the first book devoted exclusively to teaching and documenting the important new format, PNG. It is an indispensable compendium for Web content developers and is chock full of examples, sample code, and practical hands-on advice.}
}

@article{Goyal2017AccurateLM,
  title={Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour},
  author={Priya Goyal and Piotr Doll{\'a}r and Ross B. Girshick and Pieter Noordhuis and Lukasz Wesolowski and Aapo Kyrola and Andrew Tulloch and Yangqing Jia and Kaiming He},
  journal={ArXiv},
  year={2017},
  volume={abs/1706.02677}
}

@software{Falcon_PyTorch_Lightning_2019,
author = {Falcon, William and {The PyTorch Lightning team}},
doi = {10.5281/zenodo.3828935},
license = {Apache-2.0},
month = {3},
title = {{PyTorch Lightning}},
url = {https://github.com/Lightning-AI/lightning},
version = {1.4},
year = {2019}
}

@article{NAS,
  author  = {Thomas Elsken and Jan Hendrik Metzen and Frank Hutter},
  title   = {Neural Architecture Search: A Survey},
  journal = {Journal of Machine Learning Research},
  year    = {2019},
  volume  = {20},
  number  = {55},
  pages   = {1--21},
  url     = {http://jmlr.org/papers/v20/18-598.html}
}
@article{NAS2,
  title={A Comprehensive Survey of Neural Architecture Search},
  author={Pengzhen Ren and Yun Xiao and Xiaojun Chang and Po-yao Huang and Zhihui Li and Xiaojiang Chen and Xin Wang},
  journal={ACM Computing Surveys (CSUR)},
  year={2021},
  volume={54},
  pages={1 - 34}
}

@inproceedings{
laurent2023packed,
title={Packed Ensembles for efficient uncertainty estimation},

  author={Olivier Laurent and Adrien Lafage and Enzo Tartaglione and Geoffrey Daniel and Jean-Marc Martinez and Andrei Bursuc and Gianni Franchi},
booktitle={ICLR},
year={2023},
}

@inproceedings{gfnet,
        title = {Glance and Focus: a Dynamic Approach to Reducing Spatial Redundancy in Image Classification},
       author = {Wang, Yulin and others},
    booktitle = {NeurIPS},
         year = {2020},
}
@inproceedings{wang2021not,
        title = {Not All Images are Worth 16x16 Words: Dynamic Transformers for Efficient Image Recognition},
       author = {Wang, Yulin and others},
    booktitle = {NeurIPS},
         year = {2021}
}


@InProceedings{logit-based-edd,
  title = 	 {Logit-based ensemble distribution distillation for robust autoregressive sequence uncertainties},
  author =       {Fathullah, Yassir and Xia, Guoxuan and J. F. Gales, Mark},
  booktitle = 	 {Proceedings of the Thirty-Ninth Conference on Uncertainty in Artificial Intelligence},
  pages = 	 {582--591},
  year = 	 {2023},
  editor = 	 {Evans, Robin J. and Shpitser, Ilya},
  volume = 	 {216},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {31 Jul--04 Aug},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v216/fathullah23a/fathullah23a.pdf},
  url = 	 {https://proceedings.mlr.press/v216/fathullah23a.html},
  abstract = 	 {Efficiently and reliably estimating uncertainty is an important objective in deep learning. It is especially pertinent to autoregressive sequence tasks, where training and inference costs are typically very high. However, existing research has predominantly focused on tasks with static data such as image classification. In this work, we investigate Ensemble Distribution Distillation (EDD) applied to large-scale natural language sequence-to-sequence data. EDD aims to compress the superior uncertainty performance of an expensive (teacher) ensemble into a cheaper (student) single model. Importantly, the ability to separate knowledge (epistemic) and data (aleatoric) uncertainty is retained. Existing probability-space approaches to EDD, however, are difficult to scale to large vocabularies. We show, for modern transformer architectures on large-scale translation tasks, that modelling the ensemble <em>logits</em>, instead of softmax probabilities, leads to significantly better students. Moreover, the students surprisingly even <em>outperform Deep Ensembles</em> by up to $\sim$10% AUROC on out-of-distribution detection, whilst matching them at in-distribution translation.}
}


@inproceedings{
bitterwolf2023ninco,
title={In or Out? Fixing ImageNet Out-of-Distribution Detection Evaluation},
author={Julian Bitterwolf and Maximilian Mueller and Matthias Hein},
booktitle={ICML},
year={2023},
url={https://openreview.net/forum?id=ChniRIfpRR}
}