% CVPR 2023 Paper Template
% based on the CVPR template provided by Ming-Ming Cheng (https://github.com/MCG-NKU/CVPR_Template)
% modified and extended by Stefan Roth (stefan.roth@NOSPAMtu-darmstadt.de)

\documentclass[10pt,twocolumn,letterpaper]{article}

%%%%%%%%% PAPER TYPE  - PLEASE UPDATE FOR FINAL VERSION
\usepackage[review]{cvpr}      % To produce the REVIEW version
%\usepackage{cvpr}              % To produce the CAMERA-READY version
%\usepackage[pagenumbers]{cvpr} % To force page numbers, e.g. for an arXiv version

% Include other packages here, before hyperref.
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{makecell}
\usepackage{floatrow}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{subfig}
\usepackage{pgfplots}
% \usepackage{subtable}
% \floatsetup[table]{capposition=bottom,style = Plaintop}
% \floatsetup{heightadjust=object}
\usepackage{stfloats}


\usepackage{adjustbox}
\usepackage{colortbl}
\usepackage{xcolor}
\usepackage[table,dvipsnames, svgnames, x11names]{xcolor}

\makeatletter
  \newcommand\figcaption{\def\@captype{figure}\caption}
  \newcommand\tabcaption{\def\@captype{table}\caption}
\makeatother
% It is strongly recommended to use hyperref, especially for the review version.
% hyperref with option pagebackref eases the reviewers' job.
% Please disable hyperref *only* if you encounter grave issues, e.g. with the
% file validation for the camera-ready version.
%
% If you comment hyperref and then uncomment it, you should delete
% ReviewTempalte.aux before re-running LaTeX.
% (Or just hit 'q' on the first LaTeX run, let it finish, and you
%  should be clear).
\usepackage[pagebackref,breaklinks,colorlinks]{hyperref}


% Support for easy cross-referencing
\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}


%%%%%%%%% PAPER ID  - PLEASE UPDATE
\def\cvprPaperID{1503} % *** Enter the CVPR Paper ID here
\def\confName{CVPR}
\def\confYear{2023}


\begin{document}

%%%%%%%%% TITLE - PLEASE UPDATE
\title{Supplementary Material}

\author{First Author\\
Institution1\\
Institution1 address\\
{\tt\small firstauthor@i1.org}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
\and
Second Author\\
Institution2\\
First line of institution2 address\\
{\tt\small secondauthor@i2.org}
}
\maketitle

%%%%%%%%% Supp

%%%%%%%%% BODY TEXT
\appendix

\section{Related Work}
\label{s1}

\paragraph{3D Point Cloud Analysis.}
As the main data form in 3D, point clouds have stimulated a range of challenging tasks, including shape classification~\cite{qi2017pointnet,qi2017pointnet++,pointmlp,zhang2022pointclip}, scene segmentation~\cite{lai2022stratified, dai20183dmv, zhao2019dar}, and 3D object detection~\cite{3detr,votenet,shi2020pv, he2020structure}. Existing solutions can be categorized into two groups: projection-based and point-based methods.  To handle the irregularity and sparsity of point clouds, projection-based methods convert them into grid-like data, such as tangent
planes~\cite{tatarchenko2018tangent}, multi-view depth maps~\cite{hamdi2021mvtn,simpleview,zhang2022pointclip,su2015multi}, and 3D voxels~\cite{pvcnn,riegler2017octnet, meng2019vv}.  By doing this, the efficient 2D networks~\cite{resnet} and 3D convolutions~\cite{pvcnn} can be adopted for robust point cloud understanding. However, the projection process inevitably causes geometric information loss and quantization error. Point-based methods directly extract 3D patterns upon the unstructured input points to alleviate this loss of information. The seminal PointNet~\cite{qi2017pointnet} utilizes shared MLP layers to independently extract point features and aggregate the global representation via a max pooling. PointNet++~\cite{qi2017pointnet++} further constructs a multi-stage hierarchy to encode local spatial geometries progressively. Since then, the follow-up methods introduce advanced yet complicated local operators~\cite{xu2021paconv,pointmlp} and network architectures~\cite{guo2021pct,dgcnn} for spatial geometry learning. In this paper, we follow the paradigm of more popular point-based methods, and propose a pure non-parametric network, Point-NN, with its two promising applications. For the first time, we verify the effectiveness of non-parametric components for 3D point cloud analysis.

\paragraph{Local Geometry Operators.}
Referring to the inductive bias of locality~\cite{resnet,krizhevsky2017imagenet}, most existing 3D models adopt delicate 3D operators to iteratively aggregate neighborhood features. Following PointNet++~\cite{qi2017pointnet++}, a series of methods utilize shared MLP layers with learnable relation modules for local pattern encoding, e.g., fully-linked webs~\cite{zhao2019pointweb}, structural relation network~\cite{duan2019structural}, and geometric affine module~\cite{pointmlp}. Some methods define irregular spatial kernels and introduce point-wise convolutions by relation mapping~\cite{rscnn}, Monte Carlo estimation~\cite{wu2019pointconv,hermosilla2018monte}, and dynamic kernel assembling~\cite{xu2021paconv}. Inspired by graph networks, DGCNN~\cite{dgcnn} and others~\cite{landrieu2018large,te2018rgcnn} regard points as vertices and interact local geometry through edges. Transformers~\cite{lai2022stratified,zhao2021point} are also introduced in 3D for attention-based feature communication. CurveNet~\cite{curvenet} proposes generating hypothetical curves for point grouping and feature aggregation. Unlike all previous methods with learnable parameters, Point-NN adopts non-parametric trigonometric functions to reveal the spatial geometry within local regions. Point-PN further appends simple linear layers on top and achieves a high performance-parameter trade-off without complicated operations.

\paragraph{Positional Encodings.}
Transformers~\cite{transformer} represent input signals as an orderless sequence and implicitly utilize positional encodings (PE) to inject positional information. Typically, trigonometric functions are adopted as the non-learnable PE~\cite{gehring2017convolutional} to encode both absolute and relative positions, each dimension corresponding to a sinusoid. For vision, PE can also be learnable during training~\cite{dosovitskiy2020image} or online predicted by neural networks~\cite{zhao2021point,chu2021conditional}. Another work~\cite{rahaman2019spectral} indicates that deep networks can learn better high-frequency variation given a higher dimensional input. NerF~\cite{mildenhall2021nerf} utilizes trigonometric PE to enhance the MLPs for better neural scene representations, but in a different formulation from the Transformer's. Tancik \etal~\cite{tancik2020fourier} further interpret it as Fourier transform to learn high-frequency functions in low dimensional problems. In contrast, we extend the non-learnable trigonometric PE of Transformer for specialized raw-point embedding and local geometry extraction, other than serving as an accessory in previous learnable networks. By doing this, the non-parametric encoder of Point-NN can effectively capture low-level spatial patterns complementary to the already trained 3D models.

\section{Implementation Details}
\label{s2}

\paragraph{Point-NN.}
The non-parametric encoder of Point-NN contains 4 stages. Each stage reduces the point number by half via FPS, and doubles the feature dimension during feature expansion.
The initial feature dimension $C_I$ is set to 72, and the final dimension $C_G$ of global representation is 1,152. The neighbor number $k$ of $k$-NN is 84 for all stages. We set the two hyperparameters $\alpha, \beta$ in $\operatorname{PosE}(\cdot)$ as 500 and 100, respectively, referring to Equation (6) and (7) in the main paper. For part segmentation, the appended non-parametric decoder also conducts feature expansion in each stage by skip connections referring to PointNet++~\cite{qi2017pointnet++}, which concatenate the propagated point features in the decoder with the corresponding ones from the encoder. As shown in Figure~\ref{supp_f1}, we construct the segmentation point-memory bank by storing the part-wise features and labels from the training set, which largely saves the GPU memory. During inference, each point-wise feature of the test point cloud conducts similarity matching with the part-wise feature memory for segmentation.

\begin{figure*}[t!]
  \centering
  % \vspace{0.1cm}
    \includegraphics[width=1\textwidth]{cvpr2023-author_kit-v1_1-1/latex/figs/sup-fig1.pdf}
% \vspace{0.05cm}
   \caption{\textbf{Point-Memory Bank for Part Segmentation.} We first utilize the non-parametric encoder and decoder to extract the point-wise features of the point cloud in the training set. Then, we average the point features with the same part label to obtain the part-wise features, and construct them as the feature memory.}
    \label{supp_f1}
    \vspace{0.1cm}
\end{figure*}

\begin{table}[t]
\small
\tabcaption{\textbf{Ablation Study of Non-Parametric Encoder.} We ablate different designs at every stage of the encoder: the grouping method for local neighbors, feature expansion by concatenation, and pooling operation for feature aggregation. We report the classification accuracy (\%) on ModelNet40~\cite{modelnet40}.}
% \vspace{0.3cm}
% \hspace{-0.3cm}
\begin{minipage}[t!]{0.32\linewidth}
\centering
\begin{adjustbox}{width=\linewidth}
	\begin{tabular}{cc}
	\toprule
	\makecell*[c]{Grouping\\Method} &Acc.\\
        \midrule
        Ball Query &67.1 \\
        \textbf{$k$-NN} &\textbf{76.9} \\
	\bottomrule
	\end{tabular}
\end{adjustbox}
\end{minipage}\,\,
\begin{minipage}[t!]{0.27\linewidth}
\centering
\begin{adjustbox}{width=\linewidth}
	\begin{tabular}{cc}
	\toprule
	\makecell*[c]{Feature\\Expand} &Acc.\\
        \midrule
        w/o &71.6 \\
        \textbf{w} &\textbf{76.9} \\
	\bottomrule
	\end{tabular}
\end{adjustbox}
\end{minipage}\,\,
\begin{minipage}[t!]{0.27\linewidth}
\centering
\begin{adjustbox}{width=\linewidth}
	\begin{tabular}{cc}
	\toprule
	\makecell*[c]{Pooling} &Acc.\\
        \midrule
        Max &72.2 \\
        Ave &65.7 \\
        \textbf{Both} &\textbf{76.9} \\
	\bottomrule
	\end{tabular}
\end{adjustbox}
\end{minipage}
% \vspace*{-1pt}
\label{supp_t1}
\end{table}

\paragraph{Point-PN.}
For the parametric variant, we decrease $C_I$ to 36 and neighbors number $k$ to 40 to achieve lightweight parameters and efficient inference. We adopt the bottleneck architecture with a ratio 0.5 for the two cascaded linear layers after the local geometry aggregation module. The initial parametric raw-point embedding consists of only one linear layer, and the final classifier contains three linear layers. For shape classification, we train Point-PN for 300 epochs with a batch size 32 on a single RTX 3090 GPU, and utilize the same data augmentation with 1,024 input points as previous works~\cite{pointmlp,qian2022pointnext}. On ModelNet40~\cite{modelnet40}, we adopt SGD optimizer with a weight decay 0.0002, and cosine scheduler with an initial learning rate 0.1. On ScanObjectNN~\cite{scanobjectnn}, we adopt AdamW optimizer~\cite{kingma2014adam} with a weight decay 0.05, and cosine scheduler with an initial learning rate 0.002. For part segmentation, we utilize the same decoder architecture and training settings as CurveNet~\cite{curvenet} for a fair comparison.

\begin{table}[t]
\centering
\caption{\textbf{Magnitude $\alpha$ in Trigonometric Functions.} We report the classification accuracy of Point-NN on ModelNet40~\cite{modelnet40}.}
\begin{adjustbox}{width=0.96\linewidth}
	\begin{tabular}{lcccccc}
	\toprule
	\makecell*[c]{Magnitude $\alpha$} &\ 1\ &10 &50 &\textbf{100} &200 &500\\
        \cmidrule(lr){1-1} \cmidrule(lr){2-7}
        \specialrule{0em}{1pt}{1pt}
		 Acc. (\%) &\ 52.7\ &65.3  & 74.2  &\textbf{76.9} &75.4 &74.8\ \\ 
		 % \specialrule{0em}{1pt}{1pt}
   %       GPU & & &  & & &\\ 
		 \specialrule{0em}{1pt}{1pt}
	\bottomrule
	\end{tabular}
\end{adjustbox}
\label{supp_t2}
\end{table}\vspace{0.1cm}

\begin{table}[t]
\centering
\caption{\textbf{Wavelength $\beta$ in Trigonometric Functions.} We report the classification accuracy of Point-NN on ModelNet40~\cite{modelnet40}.}
\begin{adjustbox}{width=\linewidth}
	\begin{tabular}{lcccccc}
	\toprule
	\makecell*[c]{Wavelength $\beta$} &\ 10\ &100 &\textbf{500} &1000 &2000 &3000\ \\
        \cmidrule(lr){1-1} \cmidrule(lr){2-7}
        \specialrule{0em}{1pt}{1pt}
		 Acc. (\%) &\ 41.5\  &74.2 &\textbf{76.9} &74.5 &73.1 &72.9\ \\ 
		 % \specialrule{0em}{1pt}{1pt}
   %       GPU & & &  & & &\\ 
		 \specialrule{0em}{1pt}{1pt}
	\bottomrule
	\end{tabular}
\end{adjustbox}
\label{supp_t3}
\vspace{0.1cm}
\end{table}

\section{Additional Ablation Study}
\label{s3}

\paragraph{Non-Parametric Encoder.}
In Table~\ref{supp_t1}, we further investigate other designs at every stage of Point-NN's non-parametric encoder. As shown, $k$-NN performs better than ball query~\cite{qi2017pointnet++} for grouping the neighbors of each center point since the ball query would fail to aggregate valid geometry in some sparse regions with only a few neighboring points.
Expanding the feature dimension by concatenating the center and neighboring points can improve the performance by +5.3\%. This is because each point obtains larger receptive fields as the network stage goes deeper and requires higher-dimensional vectors to encode more spatial semantics.  For the pooling operation after geometry extraction, we observe applying both max and average pooling achieves the highest accuracy, which can summarize the local patterns from two different aspects.



\begin{table}[t]
\centering
\begin{adjustbox}{width=\linewidth}
	\begin{tabular}{c c c cc c}
	\toprule
		\makecell*[c]{Raw\\Embed.} &\makecell*[c]{Linear\\Layers} &\makecell*[c]{Classifier} &\makecell*[c]{ModelNet40} &\makecell*[c]{ScanObjNN}
		&\makecell*[c]{Param.}\\
		 \cmidrule(lr){1-3} \cmidrule(lr){4-4} \cmidrule(lr){5-5} \cmidrule(lr){6-6}
	   N &-  &N & 76.9 & 61.5 &0.0 M\\
	     N &-  &P & 90.3 &68.5 &0.2 M\\
	     P &-  &P & 90.8 &73.7 &0.2 M\\
	     P &0+1  &P & 93.4 & 86.0 &0.5 M\\
          P &1+0  &P & 93.0 & 83.8 &0.5 M\\
          P &1+1  &P & 93.2 & 86.5 &0.7 M\\
          P &0+2  &P & 93.0 & 84.6 &0.7 M\\
          P &2+0  &P & 92.7 & 84.9 &0.7 M\\
	     P &2+1  &P & 92.7 & 83.6 &0.8 M\\
          P &2+2  &P & 92.9 & 82.7 &1.0 M\\
          \bf P &\bf 1+2  &\bf P & \bf 93.8 &\bf 87.1 &\bf 0.8 M\\
	\bottomrule
	\end{tabular}
\end{adjustbox}
\caption{\textbf{Detailed Step-by-step Construction of Point-PN} on ModelNet40~\cite{modelnet40} and ScanObjectNN~\cite{scanobjectnn}. We report the classification accuracy (\%) on the PB-T50-RS split of ScanObjectNN. `N' or `P' denotes the non-parametric modules or parametric linear layers. `Linear Layers' denotes the number of linear layers inserted into each network stage.}
\label{supp_t5}
% \vspace*{2pt}
\end{table}

\paragraph{Hyperparameters in Trigonometric Functions.}
In Table~\ref{supp_t2} and \ref{supp_t3}, we show the influence of two hyperparameters in trigonometric functions of Point-NN. We fix one of them to be the best-performing value ($\alpha$ as 100, $\beta$ as 500), and vary the other one for ablation.
The combination of the magnitude $\alpha$ and wavelength $\beta$ control the frequency of the channel-wise sinusoid, and thus determine the raw point encoding for different classification accuracy.

\paragraph{Point-Memory Bank with Different Sizes.}
As default, we construct the feature memory by the entire training-set point clouds. In Table~\ref{supp_t4}, we report how Point-NN performs when partial training samples are utilized for the point-memory bank. As shown, Point-NN can attain 60.1\% classification accuracy with only 10\% of the training data, and further achieves 70.1\% with 40\% data, which is comparable to the performance of 100\% ratio but consumes less GPU memory. This indicates Point-NN is not sensitive to the memory bank size and can perform favorably with partial training-set data.

\paragraph{Point-PN.}
In Table~\ref{supp_t5}, we present the detailed results of constructing Point-PN on ModelNet40~\cite{modelnet40} and ScanObjectNN~\cite{scanobjectnn}. As shown, the linear layers after the geometry aggregation module (`0+1', `0+2', `1+2') are more important than the previous ones (`1+0', `2+0', `2+1').
Also, inserting layers into both positions (`1+1') performs better than only into one position (`0+1', `1+0').

\paragraph{Memory Cost by Plug-and-play.}
In Table~\ref{supp_t44}, we present the GPU memory cost brought by the ensemble of Point-NN. As shown, utilizing Point-NN as a plug-and-play module brings no extra learnable parameters and causes marginal memory consumption, indicating its efficiency for training-free performance enhancement.


\begin{table}[t]
\centering
\caption{\textbf{Point-Memory Bank with Different Sizes.} We randomly sample different ratios of ModelNet40~\cite{modelnet40} to construct the point-memory bank and report the classification accuracy with GPU memory consumption.}
\begin{adjustbox}{width=\linewidth}
	\begin{tabular}{lccccccc}
	\toprule
	\makecell*[c]{Ratio (\%)} &1 &5 &10 &20 &40 &80 &100\\
        \cmidrule(lr){1-1} \cmidrule(lr){2-8}
        \specialrule{0em}{1pt}{1pt}
		 Acc. (\%)&37.9 &55.6  &60.1  &65.5 &70.1 &74.2 &\textbf{76.9}\\ 
		 \specialrule{0em}{1pt}{1pt}
         Mem. (G) &3.84 &3.87 &3.93  &4.05 &4.26 &4.82 &5.21\\ 
		 \specialrule{0em}{1pt}{1pt}
	\bottomrule
	\end{tabular}
\end{adjustbox}
\label{supp_t4}
\end{table}

\begin{table}[t]
\centering
\caption{\textbf{Memory Cost by Plug-and-play.} We respectively report the classification accuracy (\%) on ModelNet40~\cite{modelnet40}, learnable parameters, and GPU memory (G). All compared methods are conducted with batch size 32 on a single RTX 3090 GPU.}
\begin{adjustbox}{width=\linewidth}
	\begin{tabular}{lcccc}
	\toprule
	\makecell*[c]{Method} &+NN &Acc. (\%) &Param. &Mem. (G)\\
        \cmidrule(lr){1-1} \cmidrule(lr){2-5}
        \specialrule{0em}{1pt}{1pt}
		 Point-NN&- &76.9  &0.0 M  &5.21 \\ 
		 \specialrule{0em}{1pt}{1pt}
         Point-PN &- &93.8 &0.8 M  &6.63\\ 
         \midrule\specialrule{0em}{1pt}{3pt}
        PointNet++~\cite{qi2017pointnet++} &- &92.6 &1.7 M  &7.46\\
         &\checkmark &\textcolor{blue}{+0.5} &\textcolor{blue}{+0 M}  &\textcolor{blue}{+2.45} \\
         \specialrule{0em}{1pt}{3pt}
        PCT~\cite{guo2021pct} &- &93.2 &-  &5.59\\
         &\checkmark &\textcolor{blue}{+0.2} &\textcolor{blue}{+0 M}  &\textcolor{blue}{+3.25} \\
        \specialrule{0em}{1pt}{1pt}
        PointMLP~\cite{pointmlp} &- &94.1 &12.6 M  &19.07 \\
         &\checkmark &\textcolor{blue}{+0.3} &\textcolor{blue}{+0 M}  &\textcolor{blue}{+0.07} \\
	\bottomrule
	\end{tabular}
\end{adjustbox}
\label{supp_t44}
\end{table}

\begin{figure*}[t!]
  \centering
  % \vspace{0.1cm}
    \includegraphics[width=1\textwidth]{cvpr2023-author_kit-v1_1-1/latex/figs/supp_f1.pdf}
% \vspace{0.05cm}
   \caption{\textbf{Why Do Trigonometric Functions Work?} For an input point cloud, we visualize its low-frequency and high-frequency geometries referring to~\cite{xu2021learning}, and compare with the feature responses of Point-NN, where darker colors indicate higher responses. As shown, Point-NN can focus on the high-frequency 3D structures with sharp variations of the point cloud.}
    \label{supp_f1}
    % \vspace{0.1cm}
\end{figure*}

\section{Discussion}
\label{s4}

\subsection{Why Do Trigonometric Functions Work?}

We leverage the trigonometric function to conduct non-parametric raw-point embedding and geometry extraction. It can reveal the 3D spatial patterns benefited from the following three properties.

\paragraph{Capturing High-frequency 3D Structures.}
As discussed in Tancik et al.~\cite{tancik2020fourier}, transforming low-dimensional input by sinusoidal mapping helps MLPs to learn the high-frequency content during training. Similarly to our non-parametric encoding, Point-NN utilizes trigonometric functions to capture the high-frequency spatial structures of 3D point clouds, and then recognize them from these distinctive characteristics by the point-memory bank. In Figure~\ref{supp_f2}, we visualize the low-frequency (Top) and high-frequency (Middle) geometry of the input point cloud, and compare them with the feature responses of Point-NN (Bottom). The high-frequency geometries denotes the spatial regions of edges, corners, and other fine-grained details, where the local 3D coordinates vary dramatically, while the low-frequency structure normally includes some flat and smooth object surfaces with gentle variations. As shown, aided by trigonometric functions, our Point-NN can concentrate well on these high-frequency 3D patterns.

\paragraph{Encoding Absolute and Relative Positions.}
Benefited from the nature of sinusoid, the trigonometric functions can not only represent the absolute position in the embedding space, but also implicitly encode the relative positional information between two 3D points. For two points, $p_i=(x_i, y_i, z_i)$ and $p_j=(x_j, y_j, z_j)$, we first obtain their $C$-dimensional embeddings referring to Equation (5$\sim$7) in the main paper, formulated as
\begin{align}
\operatorname{PosE}(p_i) &= \operatorname{Concat}(f^{x}_i,\ f^{y}_i,\ f^{z}_i),\\
\operatorname{PosE}(p_j) &= \operatorname{Concat}(f^{x}_j,\ f^{y}_j,\ f^{z}_j),
\end{align}
where $\operatorname{PosE}(\cdot)$ denotes the positional encoding by trigonometric functions, and  $f^x_{i/j},\ f^y_{i/j},\ f^z_{i/j} \in \mathbb{R}^{1\times \frac{C}{3}}$ denote the embeddings of three axes.
Then, their spatial relative relation can be revealed by the dot production between the two embeddings, formulated as
\begin{align}
f^{x}_i f^{x}_j^T + f^{y}_i f^{y}_j^T + f^{z}_i f^{z}_j^T = \operatorname{PosE}(p_i) \operatorname{PosE}(p_j)^T,\nonumber
\end{align}
Taking the x axis as an example,
\begin{align}
\sum_{m=0}^{\frac{C}{6}-1}\mathrm{cosine}(\alpha (x_i - x_j)/{\beta^{\frac{6m}{C}}}) = f^{x}_i f^{x}_j^T,
\end{align}
which indicates the relative x-axis distance of two points, in a similar way to the other two axes. Therefore, the trigonometric function is capable of encoding both absolute and relative 3D positional information for point cloud analysis.

\paragraph{Local Geometry Extraction.}

With the analysis above, we can reinterpret the Equation (9) of reweighing in the main paper from a mathematical perspective, which can be formulated as
\begin{align}
\label{weigh}
    f^w_{cj} &= \big(f_{cj} + \operatorname{PosE}(\Delta p_j)\big) \odot \operatorname{PosE}(\Delta p_j)\\
    &= f_{cj} \odot \operatorname{PosE}(\Delta p_j) + \operatorname{PosE}(\Delta p_j)\big \odot \operatorname{PosE}(\Delta p_j),\nonumber
\end{align}
where $\odot$ denotes element-wise multiplication, and $\Delta p_j, f_{cj}$ denote the relative coordinates and feature of neighbor point $j$ to the center point $c$. Now, there are two terms in Equation~\ref{weigh}, both of which reveal the relative geometry between point $i$ and $j$. The former implicitly captures the relative positional information from point $j$ to $i$, since the summation of its elements corresponds to the dot production between embeddings. The latter also indicates the embedding-space distance between the two points, which can be reformulated as the vector length of $\operatorname{PosE}(\Delta p_j)$. In this way, Point-NN can effectively extract local 3D patterns and aggregate them by pooling operations.

\begin{figure*}[t!]
  \centering
  \vspace{0.1cm}
    \includegraphics[width=0.94\textwidth]{cvpr2023-author_kit-v1_1-1/latex/figs/supp_f2.pdf}
% \vspace{0.05cm}
   \caption{\textbf{Can Point-NN Improve Point-PN by Plug-and-play?} We visualize the feature responses for Point-NN, the trained PointNet++~\cite{qi2017pointnet++} and Point-PN, where darker colors indicate higher responses. As shown, Point-PN captures similar 3D patterns to Point-NN, which harms their complementarity.}
    \label{supp_f2}
    \vspace{0.1cm}
\end{figure*}



\subsection{Point-Memory Bank vs. $k$-NN?}
Based on the already extracted point cloud features, our point-memory bank and $k$-NN algorithm both leverage the inter-sample feature similarity for classification, and require no training or learnable parameters, but are different from the following two aspects.

\paragraph{Soft Integration vs. Hard Assignment.}
As illustrated in Section (2.3) of the main paper, our point-memory bank regards the similarities $S_{cos}$ between the test point cloud feature and the feature memory, $F_{mem}$, as weights, which are adopted for weighted summation of the one-hot label memory, $T_{mem}$. This can be viewed as a soft label integration. Instead, $k$-NN utilizes $S_{cos}$ to search the $k$ nearest neighbors from the training set, and directly outputs the category label with the maximum number of samples within the $k$ neighbors. Hence, $k$-NN conducts a hard label assignment, which is less adaptive than the soft integration. Additionally, our point-memory bank can be accomplished simply by two matrix multiplications, which is more efficient for the hardware.
% In Table~\ref{}, we also implement the `hard' variant of point-memory bank that assigns labels by the maximum number

\begin{table}[t]
\centering
\caption{\textbf{Point-Memory Bank vs. $k$-NN.} `Top-$k$ PoM' denotes the point-memory bank with top-$k$ similarities. We utilize our non-parametric encoder to extract features amd report the classification accuracy (\%) on ModelNet40~\cite{modelnet40}, where `All' denotes 9,840 training samples.}
\begin{adjustbox}{width=\linewidth}
	\begin{tabular}{cccccccc}
	\toprule
	\makecell*[c]{$k$} &1 &10 &100 &500 &1000 &5000 &All\\
        \cmidrule(lr){1-1} \cmidrule(lr){2-8}
        \specialrule{0em}{1pt}{1pt}
		 Top-$k$ PoM &71.6 &75.3  &76.1  &76.5 &76.7 &76.8 &\textbf{76.9}\\
		 \specialrule{0em}{1pt}{1pt}
         $k$-NN &71.8 &71.2 &62.0  &41.7 &34.5 &9.2 &-\\ 
		 \specialrule{0em}{1pt}{1pt}
	\bottomrule
	\end{tabular}
\end{adjustbox}
\label{supp_t7}
\end{table}

\paragraph{All Samples vs. $k$ Neighbors.}
Our point-memory bank softly integrates the entire label memory, which takes the semantics of all training samples into account. In contrast, $k$-NN only involves the nearest $k$ neighbors and discards the category knowledge of other training samples. 

\begin{table}[t]
\centering
\caption{\textbf{Can Point-NN Improve Point-PN by Plug-and-play?} We utilize Point-NN as an inference-time enhancement module to improve the trained Point-PN by interpolating their predictions. We report the accuracy (\%) on the PB-T50-RS split of ScanObjectNN~\cite{scanobjectnn}, ModelNet40~\cite{modelnet40}, and ShapeNetPart~\cite{shapenetpart}.}
\begin{adjustbox}{width=0.9\linewidth}
	\begin{tabular}{cccc}
	\toprule
	\makecell*[c]{Dataset} &ScanObjectNN &ModelNet40 &ShapeNetPart\\
        \cmidrule(lr){1-1} \cmidrule(lr){2-4}
        \specialrule{0em}{1pt}{1pt}
		 Point-PN &87.1 &93.8  &86.6 \\
		 \specialrule{0em}{1pt}{1pt}
         +NN &\textcolor{blue}{+0.1} &\textcolor{blue}{+0.2} &\textcolor{blue}{+0.0}\\ 
		 \specialrule{0em}{1pt}{1pt}
	\bottomrule
	\end{tabular}
\end{adjustbox}
\label{supp_t8}
\end{table}

\paragraph{Performance Comparison.}
In Table~\ref{supp_t7}, based on the point cloud features ectracted by our non-parametric encoder, we implement the top-$k$ version of point-memory bank for comparison with $k$-NN, which only aggregates the label memory of the training samples with top-$k$ similarities. 
As the neighbor number $k$ increases, $k$-NN's performance is severely harmed due to its hard label assignment, while our point-memory bank attains the highest accuracy by utilizing all 9,840 samples for classification, indicating their different characters.


\subsection{Can Point-NN Improve Point-PN by Plug-and-play?}
Point-NN can provide complementary geometric knowledge and serve as a plug-and-play module to boost the existing learnable 3D models after training. Although Point-PN is also a learnable 3D network, the enhanced performance brought by Point-NN is marginal as reported in Table~\ref{supp_t8}. By visualizing feature responses in Figure~\ref{supp_f2}, we observe that the complementarity between Point-NN and Point-PN is much weaker than that between Point-NN and PointNet++~\cite{qi2017pointnet++}. This is because the non-parametric framework of Point-PN is mostly inherited from Point-NN, and can also capture high-frequency 3D geometries via trigonometric functions. Therefore, the learnable Point-PN extracts similar 3D patterns to Point-NN, which harms its plug-and-play performance.


\section{Anonymous Code Release}

For reproducibility, we anonymously release our codes in \url{https://anonymous.4open.science/r/Non-Parametric_Networks_for_3D_Analysis-F246}.


% \clearpage
%%%%%%%%% REFERENCES
{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\end{document}
