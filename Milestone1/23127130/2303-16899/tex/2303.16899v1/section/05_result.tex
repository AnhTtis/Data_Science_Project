



\subsection{Experiments on Movie Audio Descriptions}
\label{exp:video}

\input{table/context}
\input{table/mad_cleaning_quant}
\input{table/other_works}
\input{table/lsmdc_challenge}
\input{table/length}
\input{table/qualitative}


\paragraph{Effect of Temporal Context.}
In Table~\ref{table:context} we show that visual context from multiple frames brings a clear gain for the AD task (C 6.7 vs 4.0).
AD context provides a consistent performance improvement under both oracle (C 17.8 vs 6.7) and recurrent settings (C 12.6 vs 6.7).
Note that we find feeding AD context as text tokens works better than
training a textual feature mapping network, we conjecture the ADs in their original text form carry the most key information like the names and places.
However, subtitle context provides no gain for our model (C 13.3 vs 14.3) under the recurrent setting, 
which we attribute to the very weak correspondence between the visual elements in the scene and the character dialogue. 
When the subtitles are filtered and contain only character names (denoted as `SubN'),
they provide a slight performance gain (C 14.2 vs 13.3).
Since the subtitles used are without speaker identities, the model may struggle to know which character in the frame spoke each subtitle. Overcoming these challenges will be considered in future work.

\vspace{-2mm}
\paragraph{Effect of MAD data cleaning.}
Table~\ref{table:cleaning} demonstrates the benefit of our  MAD v2 annotations over v1, 
confirming the qualitative findings. 
Training the AD model with context on v2 outperforms training on v1 under all settings (both named and unnamed) by a significant margin.
Since the v2 annotations are fewer in number than MAD-v1, 
this suggests they are indeed less noisy and result in AD captioning models with improved performance. 
% \\
\vspace{-2mm}
\paragraph{Effect of Pretraining with Partial Data.}
In Table~\ref{table:context},
we find that \textbf{visual-only pretraining} on open-domain vision-text data provides clear gains 
(CIDEr 8.4 vs 6.7 for CC3M, and 10.0 vs 6.7 for WebVid).
But considering the size of visual samples, the improvement is not data-efficient.
We attribute this to the large domain gap between movie AD and classical visual caption annotations like CC3M or WebVid2M.
{
The \textbf{text-only pretraining} of our model also improves performance. 
For the recurrent AD context model, 
AudioVault-AD pretraining increases CIDEr from 12.6 to 14.1, which indicates the great importance of adapting to the text style and context.}
The combination of the visual module after visual-only pretraining (WebVid) and the textual modules after text-only pretraining (AV-AD) gives a further performance gain 
(C 21.9 vs 19.0 for the oracle setting, and 14.3 vs 14.1 for recurrent). \\
\vspace{-8mm}
\paragraph{Length of Context.}
{In Figure~\ref{fig:length} we show the effect of varying the number of context ADs given to the model. 
Longer AD context improves performance almost consistently across all settings, but it brings extra computational cost due to the quadratic complexity of the attention operation in GPT-2. Note that we experiment with at most 6 contextual AD sentences, which is equivalent to about 70-word embeddings in Eq.~\ref{eq:context_ad}.
The trend for the recurrent setting flattens when the context ADs are longer than 3 sentences, 
which is probably due to the limited power of processing long context for the GPT2 model.
}


\vspace{-3mm}
\subsubsection{Qualitative Results}
Fig.~\ref{fig:mad} shows qualitative examples of our model.
Under the oracle setting, the model can use the character identities easily from the past ground-truth AD (\eg~``master-at-arms'').
Whereas under the recurrent setting,
the model can only learn names from the subtitles but names appear very sparsely in subtitles,
therefore the model mostly predicts pronouns (\eg~``he'', ``they'') but still gets the actions (``looks'') or objects (``necklace'') correct.


\subsection{Comparison with Other Works}
In Table~\ref{tab:self},
we compare our method with previous visual captioning methods. Note that since the MAD dataset only releases the CLIP visual features, rather than the movie frames,
our comparison is limited to methods that build on frozen CLIP features.
We show a clear performance improvement compared to ClipCap~\cite{mokady2021clipcap} and CapDec~\cite{nukrai2022text}, for the latter the language model is also adapted to the movie AD domain by text-only pretraining. The results highlight the importance of context for movie AD.

In Table~\ref{table:sota},
we adapt our method to the Multi-Sentence Description task on LSMDC, in which the model takes five consecutive clips and generates five corresponding descriptions. Since the task is performed on the \emph{unnamed} annotations, we finetune our best model in Table~\ref{tab:self} 
with varying 0-4 context ADs as input on MAD-v2-\texttt{Unnamed} dataset and test with the \emph{recurrent} setting.
To make minimal changes,
our model still takes a single clip feature at each step,
whereas previous methods take all five clips together for movie description.
Despite this disadvantage, we obtain competitive results on this task
even without using the \emph{manually-cleaned} LSMDC training set (C 16.7 vs 15.4), effectively \emph{zero-shot}.
The performance of the model can be further improved by additionally training on LSMDC data.