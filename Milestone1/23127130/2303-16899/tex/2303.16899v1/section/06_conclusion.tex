\section{Conclusion and Future Work}


This paper focuses on the automatic generation of movie AD for a given time interval, and has made significant progress.
We propose an AutoAD pipeline that incorporates contextual information. Additionally, we demonstrate the effectiveness of partial-data pretraining, a technique that could be widely applicable when full data is difficult to obtain.
Further, we clean up the previous MAD dataset and collect a new text-only movie AD dataset as a pretraining resource.
{
However, a clear limitation of this AutoAD pipeline is character naming -- referencing \textit{who} is doing \textit{what}, a necessary ingredient for story-coherent movie AD. Additionally,
future work could tackle the problem of \textit{when} to generate AD,
instead of relying on the annotated AD timestamps.
}
\label{sec:conclusion}


\vspace{2mm}
{
\noindent\textbf{Acknowledgements.}
We thank Mattia Soldan for helping with the MAD dataset,
Anna Rohrbach for the LSMDC dataset,
and the AudioVault team for their priceless contribution to the visually impaired.
This research is funded by EPSRC PG VisualAI EP/T028572/1,
a Google-Deepmind Scholarship, and
ANR-21-CE23-0003-01 CorVis.
\par
}
\clearpage