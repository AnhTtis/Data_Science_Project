
\section{Denoising MAD Dataset}
\vspace{-1mm}
\label{sec:mad}

% \maxb{**Some can be moved to supp if lack of space.**}
% \subsection{Movie Audio Description Dataset v2 (MAD)}
Our main objective is to generate movie audio descriptions.
% To train the model, we use the MAD training set~\cite{soldan2022mad},
For this goal, the model is trained on the MAD training set~\cite{soldan2022mad},
a dataset of AD caption-video clip pairs from 488 movies. MAD provides the video data in the form of CLIP visual features in order to avoid copyright restrictions.
The AD annotations for each movie are automatically collected from AudioVault\footnote{\url{https://audiovault.net}\label{fn:audiovault}}, 
a large open-source database of audio files containing the full-length original movie track mixed with the AD narrator's voice. The MAD authors transcribe a subset of this data using ASR, and also have access to the official DVD subtitles. 
Their automated method then uses \textit{text-based} speaker separation of the transcribed audio by using subtitles to know when dialogue is present, and assuming all other speech is AD.

This however introduces \emph{significant noise} because 
(i) the outdated ASR model results in erroneous transcriptions; and (ii) official DVD subtitles are not exhaustive of all speech in the movie and thus such a method frequently misidentifies character dialogue as AD narration
% , thereby introducing substantial noise to the annotations 
(an example is provided in Fig.~\ref{fig:mad-anno}). Further, obtaining official subtitles from DVDs presents additional challenges when collecting this data at scale.

We propose an improved automated data collection method for AD, requiring only the audio track as input (no DVD subtitles), that tackles both issues by using \textit{audio-based} speaker separation and an improved ASR model.
%Our method takes only the audio track as input.
We then use this method to collect improved annotations for the MAD dataset.
Briefly, taking the mixed audio containing both AD narrations and original movie sound track as input,
our automated AD collection pipeline contains five stages:
(1) speech recognition using WhisperX~\cite{bain2022whisperx} resulting in punctuated transcriptions with word-level timestamps;
(2) sentence tokenization using nltk~\cite{bird2006nltk} to provide sentence-level segmentation;
(3) speaker diarization~\cite{Bredin2020,Bredin2021} to assign speaker labels to each sentence, where the sentence timestamps are used as oracle voice-activity-detection (VAD);
(4) labelling the speaker ID of the AD narrator by selecting the cluster with the lowest proportion of first-person pronouns (\eg~`I' and `we'); and finally
(5) synchronization of the segment timestamps with the visual features by comparing audio.
Further details are in the~\app.

Henceforth we refer to the original MAD annotation~\cite{soldan2022mad} as \textbf{MAD-v1} 
and our new denoised annotations as \textbf{MAD-v2}.
%In total MAD-v2 consists of 264K AD narrations over 488 movies, 
%6\% fewer than MAD-v1 which we attribute to the reduction of character dialogue being incorrectly identified as AD.
A qualitative comparison is shown in Fig.~\ref{fig:mad}, 
we find that our MAD-v2 is much more robust and contains less errors and less character dialogue leakage. % More examples are provided in the~\app.
% \az{Describe both versions here, i.e.\ named and someone versions. We want to refer to both versions in the experiments.}
%For evaluation MAD~\cite{soldan2022mad} uses annotated videos from LSMDC as val/test splits in order to minimise noise,
%since LSMDC annotations are sourced from AD with additional manual refinement of grammar and temporal boundaries. 
%For the same reason, we leave the MAD evaluation annotations unchanged.
% since they have been manually verified and contain minimal amounts of noise.
Both LSMDC and MAD-v1 post-process their annotations by replacing character names 
in the annotations with `someone' via entity recognition, 
and release both variants of annotations which we refer to as \textbf{\texttt{Named}} and \textbf{\texttt{Unnamed}}. 
Similarly, we propose two variants of our denoised annotations: \\
\noindent\textbf{MAD-v2-\texttt{Named}:} It contains the raw collected AD narrations \emph{without} any post-processing on the character names. \\
% \maxb{Since we have MAD-original named, we no longer need to describe unnamed versions, we compare MAD-original vs. MAD-v2 on named setting only (avoids confusion, and additional data collection difference of NER). Unnamed is simply an implementation detail to evaluate on LSMDC state-of-the-art in which character names are replaced with -SOMEONE-. }
\noindent\textbf{MAD-v2-\texttt{Unnamed}:} Following the character name anonymisation performed in earlier works, 
we identify character names using a Named Entity Recognition (NER) model~\cite{ner_jeanbaptiste} and replace them with `someone'.
% \arsha{Shall we promise here that we will release MAD-V2?}
\input{table/dataset_stats}

\vspace{-1mm}
\section{Partial Pretraining with AudioVault Dataset}
\label{sec:audiovault_textonly}
\vspace{-1mm}
% \az{Make this a new section about the need for pre-training and the datasets we collect and use for this}

Paired AD and corresponding visual data are difficult to obtain 
especially due to movie copyrights,
whereas a large number of movie ADs audio tracks are available online for free (\eg~AudioVault).
To demonstrate the effect of partial pretraining in Sec.~\ref{sec:ad-pretrain},
we collect a large-scale \emph{text-only} movie AD dataset from AudioVault.
In detail, we source mixed audio files from over 7,000 movies from AudioVault that are not included in MAD-v1,
and use a denoising pipeline similar to that described in Sec.~\ref{sec:mad}
to obtain the movie ADs (detailed in~\app).
Additionally we obtain a proxy for the movie subtitles by assuming the ASR from all the non-AD speakers are the characters' dialogues.
To ensure no test-time leakage, we remove all movies present in either LSMDC or MAD from the dataset. 

Overall, our AudioVault dataset is an order of magnitude larger 
than prior AD datasets (see Table~\ref{tab:dataset-compare}),
from which we provide two sets of data:

\noindent\textbf{AudioVault-AD.} 
The AD narrations from AudioVault and their corresponding timestamps within each movie, totalling 3.3 million AD utterances.

\noindent\textbf{AudioVault-Sub.} 
The subtitles data from AudioVault and their corresponding timestamps within each movie, totalling 8.7 million subtitle utterances.

% Both our MAD-v2 and AudioVault datasets will be released publicy.
%\az{Do we also contribute anything new for LSMDC?}



%%%%%%%%%%%% OLD %%%%%%%%%%%
% Our proposed AD collection method requires only the mixed audio track, unlike MAD-v1 which additionally requires movie subtitles and the original movie audio track (without AD).
% This enables collection of AD narrations at scale. In doing so, we facilitate large scale text-only model pretraining described in Section~\ref{sec:method:textpt}, we expand the original MAD dataset and further collect unpaired AD and subtitles for over 7,000 movies sourced from AudioVault. Our AudioVault dataset is an an order of magnitude greater than prior AD datasets (see Table~\ref{tab:dataset-compare}), afforded by the scalability of audio-only data since no visual frames for the movie need to be collected and aligned. AudioVault contains user-uploaded mixed audio tracks (containing original film audio mixed with AD narrations) for full-length films. We extract the AD automatically following steps 1-3 of the MADv2 data collection pipeline. 
% To automatically extract a proxy for the subtitles by assuming any speaker segment not assigned to the AD cluster is the dialogue from the movie. 