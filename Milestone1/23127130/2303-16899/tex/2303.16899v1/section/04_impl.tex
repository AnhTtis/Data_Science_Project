\vspace{-2mm}
\section{Experiments}\vspace{-1mm}
\label{sec:exp}
In this section we first outline the experimental details for the AD task, the datasets used for training \& testing, the architectural details, and the evaluation metrics (Sec.~\ref{sec:impl}). We then report results and discuss the findings, 
perform ablations on our model, and compare to prior works (Sec.~\ref{exp:video}).

\vspace{-1mm}
\subsection{Implementation Details}
\label{sec:impl}

\subsubsection{Datasets}
\label{sec:impt:dataset}
\paragraph{Training Datasets.}
\textbf{CC3M}~(Conceptual Caption)~\cite{sharma2018cc}
is a large image alt-text dataset that contains 3.3M web images.
\textbf{WebVid}~\cite{Bain21} is
a large video-caption dataset that contains 2.5M short stock footage videos.
We use them for the partial-data pretraining for visual modules.
Additionally, we use our \textbf{AudioVault-AD} to pretrain the textual modules, 
as described in Sec.~\ref{sec:ad-pretrain}.
For the main Movie AD task, 
we train with original \textbf{MAD-v1} and our cleaned version \textbf{MAD-v2}, detailed in Sec.~\ref{sec:mad}. \\
\noindent\textbf{Test Datasets.}
\textbf{LSMDC}~\cite{rohrbach2015lsmdc}
contains 118K short video clips with descriptions from 202 movies, of which 182 of them are public.
The original {MAD}-val\&test split inherits LSMDC annotations 
after filtering out 20 lower-quality movies, resulting in 162 movies from all the LSMDC-train/val/test splits.
{
We propose an evaluation split named \textbf{MAD-eval} by further excluding LSMDC train\&test movies from these 162 movies, which gives a subset consisting of 10 movies.
The reason is twofold: 
(i) LSMDC-train is commonly used by other works as training data, and 
(ii) the character names of LSMDC-test are not public.
}
Similarly, we use both
\textbf{MAD-eval-\texttt{Named}} and \textbf{MAD-eval-\texttt{Unnamed}} versions.
The `Unnamed' version corresponds to the standard LSMDC annotation style -- where the characters' titles and names in the descriptions are replaced by the word `someone'; 
the `Named' version is constructed from the original character names provided by LSMDC.
Additionally, subtitles are not provided with MAD-val/test or LSMDC, 
so we transcribe them from the full-length audio tracks using WhisperX~\cite{bain2022whisperx}.


\vspace{-3mm}
\subsubsection{Architecture}\label{sec:exp:arch}

For \textbf{visual features}, 
we use the CLIP ViT-B-32 model~\cite{clip2021},
which is a 12-layer transformer encoder that outputs
$1\times512$ feature vectors for each input frame.
These features are provided by the MAD dataset.
For the \textbf{visual mapping network},
we use a 2-layer transformer encoder with 
8 attention heads and 512 hidden dimensions,
followed by a linear projection layer that projects $512$-d features into $768$-d.
We use ten prompt vectors.
For the \textbf{language model}, 
we use GPT-2~\cite{gpt2019}, specifically the version from HuggingFace.
The GPT-2 model takes as input $768$-d token embeddings,
passes through a 12-layer transformer with a causal attention map,
and outputs the next token embedding for every input token.
We limit the generated number of tokens to 36, since most movie ADs are less than 36 tokens.
% Short sentences are padded at the end up to 36 tokens. 
The GPT-2 is frozen in most of our experiments unless otherwise stated.
Each special token (\eg~$\texttt{B}_{\text{AD}}$)
is a learnable $768$-d vector.
We take at most 64 past AD tokens and 32 subtitle tokens, and short text samples are padded.
Specifically for subtitles, we take the most recent four dialogues within a one-minute time window.

\vspace{-4mm}
\subsubsection{Training and Inference Details}
On the MAD-v1 and MAD-v2 datasets,
we use a batch size of 8 sequences,
each of which contains 16 consecutive video-AD pairs from a movie.
Overall that gives $8\times 16$ video-AD pairs for every batch.
From each video clip, 8 frame features are uniformly sampled.
By default, the model is trained for 10 epochs.
One epoch means the model has seen \emph{all} the audio descriptions once.
Additional implementation details are in the~\app.

We use the AdamW optimizer~\cite{loshchilov2017adamw} and a cosine-decay learning rate schedule with a linear warm-up.
The starting learning rate is $10^{-4}$ and is decayed to $0$.
For each experiment, we use a single Nvidia A-40 for training.
For text generation, greedy search and beam search are commonly used sampling methods. 
We stop the text generation when a full stop mark is predicted,
otherwise we limit the sequence length to 67 tokens.
We use beam search with a beam size of $5$
and mainly report results by the top-1 beam-searched outputs,
since beam search performs slightly better than greedy search on multiple scenarios.
Note that under the `recurrent' setting,
we feed the past greedy-searched text outputs to the model to generate the current AD,
which we find gives more stable results.

\vspace{-4mm}
\subsubsection{Evaluation Metrics}
To evaluate the quality of text compared with the ground-truth,
we use classic metrics including 
ROUGE-L~\cite{lin2004rouge}~(\textbf{R-L}), 
CIDEr~\cite{vedantam2015cider}~(\textbf{C}) and 
SPICE~\cite{anderson2016spice}~(\textbf{S}).
We also report BertScore~\cite{zhang2019bertscore}~(\textbf{BertS}),
which evaluates word matching between a candidate sentence and reference sentence with pretrained BERT embeddings.
A higher value indicates better text generation compared with the ground-truth.

