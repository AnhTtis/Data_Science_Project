\section{Related Works}
\label{sec:related_work}

% \az{Look at the paper ``QUERYD: A VIDEO DATASET WITH HIGH-QUALITY TEXT AND AUDIO NARRATIONS" for references, e.g.\ other datasets that have used AD. 
% Also, if you have time, watch some of the tutorials at \url{https://www.youtube.com/playlist?list=PLNJrbI_nyy9uzywoJfyDRoeKA1SaIEFJ7} on how to provide AD}
% \tengda{to AZ: haha interesting videos, I watched them all.}

\noindent \textbf{Image Captioning.}
Image captioning is a long-standing problem in computer vision~\cite{Chen2014LearningAR,donahue2015long,karpathy2015deep,kiros2014unifying,Lu2018NeuralBT,anderson2018bottomup,chen2015cococap}.
Early pioneering works learn to associate images and words within a limited vocabulary and a set of images~\cite{barnard2001semantics,barnard2003matching,lavrenko2003model}.
Large-scale image captioning datasets have been collected by scraping images from the internet and their corresponding alt-texts with quality filters as a post-processing~\cite{sharma2018cc}. In doing so, strong joint image-text representations can be learned~\cite{clip2021}, and image captioning from raw pixels, with impressive results~\cite{yu2022coca,li2022blip}.
Recent work~\cite{mokady2021clipcap,nukrai2022text} learns a bridge between strong joint image-text representations (CLIP) and the natural language representation (GPT-2) for image captioning, obtaining promising results that generalise well across domains. In this work, we extend this approach to perform automatic AD
from videos.

\vspace{2pt}
\noindent \textbf{Video Captioning.}
Video captioning presents additional challenges due to the lack of quality large-scale video-text data and increased complexity from the temporal axis. Early video caption datasets~\cite{chen2011msvd,xu2016msrvtt} adopt manual annotations, a far from scalable collection method. ASR (automated speech recognition) from YouTube instructional videos is collected at scale for video-language datasets~\cite{miech2019howto100m}, but contains high levels of noise due to the weak correspondence between the narration and visual content. VideoCC~\cite{nagrani2022learning} transfers captions from images to videos, but this method is still limited by the existing seed image captioning dataset used.
Earlier video captioning models lack generalisation capabilities 
due to limited training data~\cite{Venugopalan2015SequenceTS,Park2019AdversarialIF}. Some recent methods~\cite{seo2022end,huang2020multimodal,luo2020univl} train on ASR from the HowTo100M dataset, while others expand image-text representations~\cite{tang2021clip4caption} to multiple frames.

A task more related to AD is that of dense video captioning~\cite{krishna2017dense}, which involves producing a number of captions and their corresponding grounded timestamps in the video. To enrich inter-task interactions, recent works for this task~\cite{chadha2020iperceive, chen2021towards, deng2021sketch, li2018jointly, mun2019streamlined, rahman2019watch, shen2017weakly, shi2019dense, wang2018bidirectional, wang2021end, zhou2018end} jointly train both a captioning and localization module. Our task differs in that the captions are: made with the intent to aid storytelling; specific to the movie domain; and complementary to the audio track.

\noindent \textbf{Visual Storytelling.} Most similar in vein to the AD task is visual storytelling~\cite{huang2016visual,Li2020VideoST,ravi2021aesop}, in which the goal is to generate coherent sentences for a sequence of video clips or images. LSMDC~\cite{rohrbach2017movie} proposes the multi-description task of generating captions for a set of clips from a movie, with character names anonymized. In contrast, movie AD takes as input a continuous long video and describes the visual happenings complementary to the story, characters, dialogue and audio. Most similar to our model is TPAM~\cite{yu2021transitional} which prompts a frozen GPT-2 with local visual features. Ours differs in that: (i) it is not restricted to local visual context but rather global by recurrently conditioning on previous outputs; and (ii) we additionally pretrain GPT on in-domain text-only AD data.

\noindent \textbf{Movie Understanding.} Previous works investigate storyline understanding by aligning movies to additional data sources such as plots~\cite{xiong2019graph,sun2022synopses}, books ~\cite{tapaswi2015book2movie,Zhu2015AligningBA},
scripts~\cite{papalampidi2019movie}, and YouTube summaries~\cite{bain2020cmd}. However, these sources are limited in number and often do not closely relate to the visual elements in the frame.
Using existing movie AD as the data source for videos is an emergent direction for movie understanding. LSMDC~\cite{rohrbach2015lsmdc}, M-VAD dataset~\cite{torabi2015mvad} and MPII-MD~\cite{rohrbach2015dataset}, gather AD and scripts from movies to provide captions for short video clips, several seconds in duration. QuerYD~\cite{oncescu2021queryd} provides high-quality textual descriptions for longer videos by scraping AD from YouDescribe~\cite{youdescribe}, an online community of AD contributors. Recently, the MAD dataset~\cite{soldan2022mad} collects movie AD at scale to provide dense textual annotations for movies with a focus on visual grounding task.


\vspace{2pt}
\noindent \textbf{Prompt Tuning and Adapters.}
Originally for language modelling,
prompt tuning is a lightweight approach to adapt a pretrained model
to perform a downstream task.
Early works~\cite{Brown2020,Lester2021,LiLiang2021,ju2022prompting}
learn prompt vectors that are shared within the targeted dataset and task.
A similar line of works to ours is 
\emph{visual-conditioned} prompt tuning,
in which the prompt vectors are conditioned on the visual inputs.
Visual-conditioned prompts are used for adapting
pretrained image-language models~\cite{Bahng2022,Jia2022},
and for few-shot learning~\cite{tsimpoukelli2021frozen,alayrac2022flamingo}.
Training lightweight feature adapters between pretrained vision and text encoders
is another approach to adapt pretrained models~\cite{gao2021clip, zhang2021tip}.
The adapter layers can also be inserted into the pretrained language model in 
an interleaved way~\cite{yang2022zero}. Our work adopts prompt tuning in order to condition a language generation model on visual information (frames), and textual context (subtitles and previous AD).




\begin{figure*}[t]
    \centering
    \includegraphics[width=0.99\textwidth]{img/arch4.pdf}
    \vspace{-10pt}
    \caption{
    \textbf{(a) Overview of AutoAD:}
    AutoAD consists of a \textit{frozen} visual encoder (CLIP) and a \textit{frozen} LLM (GPT) for generating captions. We introduce a lightweight mapping network to map CLIP features into visual tokens, which are then combined with previous AD context and subtitle context, before being fed into the GPT model. 
    $\mathcal{M}_\text{V}$ refers to the visual mapping network,
    $[\tt{B_*}]$
    and $[\tt{E_*}]$ 
    denote the learnable special tokens for contextual AD and subtitle sequences.
    \textbf{(b) Detail of the {visual mapping network}:}
    A transformer encoder takes as input multiple frame features and outputs 
    a few visual tokens which are further fed to a text generation model.
    }
    \vspace{-5mm}
    \label{fig:arch_context}
\end{figure*}
