\vspace{-8mm}
\section{Introduction}
\label{sec:intro}
\renewcommand{\epigraphflush}{flushleft}
\renewcommand{\epigraphsize}{\small}
\setlength{\epigraphwidth}{0.43\textwidth}
{\scriptsize
\epigraph{\textit{That of all the arts, the most important for us is the cinema.}\\\hspace{155pt}\textit{Vladimir Lenin}}
{}
}
\vspace{-3mm}


\begin{figure}[t!]
    \centering
    \includegraphics[width=0.49\textwidth]{img/outofsight.pdf}
    \vspace{-6mm}
    \caption{\textbf{Movie audio description (AD)} consists of sentences describing movies for the visually impaired. Note how it is heavily influenced by various types of context -- the visual frames, the previous AD, and the subtitles of the movie. 
    }
    \vspace{-5mm}
    \label{fig:teaser}
\end{figure}

One of the long-term aims of computer vision is to understand long-form feature films. There has been steady progress towards this aim with the identification of characters by their face and voice~\cite{bojanowski2013act,Brown21,Everingham06a,Tapaswi12,PersonSearch18}, the recognition of their actions and inter-actions~\cite{Laptev08,marszalek09,PatronPerez10,vondrick2016anticipating}, of their relationships~\cite{Kukleva_2020_CVPR}, and 3D pose~\cite{Angjoo22}. However, this is still a long way away from story understanding.
Movie {\em Audio Description (AD)}, 
the narration describing visual elements in movies, 
provides a means to evaluate current movie understanding capabilities.
AD was developed to aid visually impaired audiences, and is typically generated by experienced annotators. 
The amount of AD on the internet is growing due to more societal support for visually impaired communities and its inclusion is becoming an emerging legal requirement.

AD differs from image or video captioning in several significant respects~\cite{youdescribe}, bringing its own challenges.
First, AD provides dense descriptions of important visual elements {\em over time}.
Second, AD is always provided on a separate soundtrack to the original audio track and is highly {\em complementary} to it.
It is complementary in two ways: it does not need to provide descriptions of events that can be understood from the soundtrack alone (such as dialogue and ambient sounds), and it is constrained in time to intervals that do not overlap with the dialogue.
Third, unlike dense video captioning, AD aims at {\em storytelling};
therefore, it typically includes factors like a character's name, emotion, and action descriptions.

In this work, our objective is automatic AD generation -- a model that takes continuous movie frames as input and outputs AD in text form. 
Specifically, we generate text given a temporal interval of an AD, and evaluate its quality by comparing with the ground-truth AD.
This is a relatively unexplored task in the vision community with previous work targeting ActivityNet videos~\cite{wang2021toward}, a very different domain to long-term feature films with storylines, and the LSMDC challenge~\cite{rohrbach2015lsmdc}, where the descriptions and character names are treated separately.

As usual, one of the challenges holding back progress is the lack of suitable training data.
Paired image-text or video-text data that is available at scale, such as alt-text~\cite{clip2021,sharma2018cc} or stock footage with captions~\cite{Bain21}, 
does not generalize well to the movie domain~\cite{bain2022clip}.
However, collecting high-quality data for movie understanding is also difficult.
Researchers have tried to hire human annotators to describe video clips~\cite{chen2015cococap,xu2016msrvtt,krishna2017anetcaption} but this does not scale well.
Movie scripts, books and plots have also been used as learning signals~\cite{bojanowski2013act,sigurdsson2016charades,Zhu2015AligningBA} but they do not ground on vision closely and are limited in number.

In this paper we address the AD and training data challenges by -- Spoiler Alert -- developing a model that uses temporal context
together with a visually conditioned generative language model, while providing new and cleaner sources of training data.
To achieve this, we leverage the strength of large-scale language models (LLMs), like GPT~\cite{gpt2019}, and vision-language models, like CLIP~\cite{clip2021},
and integrate them into a video captioning pipeline that can be effectively trained with AD data.

Our contributions are the following: 
(i) inspired by ClipCap~\cite{mokady2021clipcap} we propose a model that is effectively able to leverage both temporal context (from previously generated AD) and dialogue context (in particular the names of characters) to improve AD generation. This is done by bridging foundation models with lightweight adapters to integrate both types of context; 
(ii) we address the lack of large-scale training data for AD by pretraining components of our model on partially missing data which are typically available in large quantities e.g.\ text-only AD without movie frames, or visual captioning datasets without multiple sentences as context; 
(iii) we propose an automatic pipeline for collecting AD narrations at scale using speaker-based separation; and finally 
(iv) we show promising results on automatic AD, as seen from both qualitative and quantitative evaluations, 
and also achieve impressive \emph{zero-shot} results on the LSMDC multi-description benchmark comparable to the finetuned state-of-the-art.

