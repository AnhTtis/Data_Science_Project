\begin{table}
\centering
\scriptsize
\resizebox{0.8\columnwidth}{!}{%
\begin{tabular}{@{}ll|ll@{}}
\toprule
Methods & \multicolumn{1}{c}{Paired Training Data} & \multicolumn{1}{|l}{C} & \multicolumn{1}{l}{M} \\ \midrule
Baseline~\cite{Park2019AdversarialIF} & LSMDC   & 11.9 & 8.3  \\
TAPM~\cite{yu2021transitional}        & LSMDC   & 15.4 & \textbf{8.4}  \\ \midrule
AutoAD (ours)  & MAD-v2-\texttt{Unnamed} & 16.7 & 7.4  \\
AutoAD (ours)  & MAD-v2-\texttt{Unnamed} \& LSMDC & \textbf{17.5} & 7.5 \\ 
\bottomrule
\end{tabular}
}
\vspace{-2mm}
\caption{
\textbf{Results on the LSMDC 2019 Multi-Sentence Description public test set.} We report our method with different amounts of training data and without subtitles for comparison under similar settings.
Official challenge metrics (CIDEr and METEOR) are reported with the `sentence' setting as described in~\cite{rohrbach2015lsmdc,yu2021transitional}. %\gul{mention `official metrics' if removing R-L}
}
\vspace{-2mm}
\label{table:sota}
\end{table}

