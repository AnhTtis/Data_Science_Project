% Please download the latest anthology.bib from
%
% http://aclweb.org/anthology/anthology.bib.gz
@inproceedings{lin-etal-2019-kagnet,
    title = "{K}ag{N}et: Knowledge-Aware Graph Networks for Commonsense Reasoning",
    author = "Lin, Bill Yuchen  and
      Chen, Xinyue  and
      Chen, Jamin  and
      Ren, Xiang",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D19-1282",
    doi = "10.18653/v1/D19-1282",
    pages = "2829--2839",
    abstract = "Commonsense reasoning aims to empower machines with the human ability to make presumptions about ordinary situations in our daily life. In this paper, we propose a textual inference framework for answering commonsense questions, which effectively utilizes external, structured commonsense knowledge graphs to perform explainable inferences. The framework first grounds a question-answer pair from the semantic space to the knowledge-based symbolic space as a schema graph, a related sub-graph of external knowledge graphs. It represents schema graphs with a novel knowledge-aware graph network module named KagNet, and finally scores answers with graph representations. Our model is based on graph convolutional networks and LSTMs, with a hierarchical path-based attention mechanism. The intermediate attention scores make it transparent and interpretable, which thus produce trustworthy inferences. Using ConceptNet as the only external resource for Bert-based models, we achieved state-of-the-art performance on the CommonsenseQA, a large-scale dataset for commonsense reasoning.",
}

@inproceedings{feng-etal-2020-scalable,
    title = "Scalable Multi-Hop Relational Reasoning for Knowledge-Aware Question Answering",
    author = "Feng, Yanlin  and
      Chen, Xinyue  and
      Lin, Bill Yuchen  and
      Wang, Peifeng  and
      Yan, Jun  and
      Ren, Xiang",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-main.99",
    doi = "10.18653/v1/2020.emnlp-main.99",
    pages = "1295--1309",
    abstract = "Existing work on augmenting question answering (QA) models with external knowledge (e.g., knowledge graphs) either struggle to model multi-hop relations efficiently, or lack transparency into the model{'}s prediction rationale. In this paper, we propose a novel knowledge-aware approach that equips pre-trained language models (PTLMs) has with a multi-hop relational reasoning module, named multi-hop graph relation network (MHGRN). It performs multi-hop, multi-relational reasoning over subgraphs extracted from external knowledge graphs. The proposed reasoning module unifies path-based reasoning methods and graph neural networks to achieve better interpretability and scalability. We also empirically show its effectiveness and scalability on CommonsenseQA and OpenbookQA datasets, and interpret its behaviors with case studies, with the code for experiments released.",
}

@inproceedings{khashabi-etal-2020-unifiedqa,
    title = "{UNIFIEDQA}: Crossing Format Boundaries with a Single {QA} System",
    author = "Khashabi, Daniel  and
      Min, Sewon  and
      Khot, Tushar  and
      Sabharwal, Ashish  and
      Tafjord, Oyvind  and
      Clark, Peter  and
      Hajishirzi, Hannaneh",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2020",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.findings-emnlp.171",
    doi = "10.18653/v1/2020.findings-emnlp.171",
    pages = "1896--1907",
    abstract = "Question answering (QA) tasks have been posed using a variety of formats, such as extractive span selection, multiple choice, etc. This has led to format-specialized models, and even to an implicit division in the QA community. We argue that such boundaries are artificial and perhaps unnecessary, given the reasoning abilities we seek to teach are not governed by the format. As evidence, we use the latest advances in language modeling to build a single pre-trained QA model, UNIFIEDQA, that performs well across 19 QA datasets spanning 4 diverse formats. UNIFIEDQA performs on par with 8 different models that were trained on individual datasets themselves. Even when faced with 12 unseen datasets of observed formats, UNIFIEDQA performs surprisingly well, showing strong generalization from its outof-format training data. Finally, simply finetuning this pre trained QA model into specialized models results in a new state of the art on 10 factoid and commonsense question answering datasets, establishing UNIFIEDQA as a strong starting point for building QA systems.",
}

@inproceedings{karpukhin-etal-2020-dense,
    title = "Dense Passage Retrieval for Open-Domain Question Answering",
    author = "Karpukhin, Vladimir  and
      Oguz, Barlas  and
      Min, Sewon  and
      Lewis, Patrick  and
      Wu, Ledell  and
      Edunov, Sergey  and
      Chen, Danqi  and
      Yih, Wen-tau",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-main.550",
    doi = "10.18653/v1/2020.emnlp-main.550",
    pages = "6769--6781",
    abstract = "Open-domain question answering relies on efficient passage retrieval to select candidate contexts, where traditional sparse vector space models, such as TF-IDF or BM25, are the de facto method. In this work, we show that retrieval can be practically implemented using dense representations alone, where embeddings are learned from a small number of questions and passages by a simple dual-encoder framework. When evaluated on a wide range of open-domain QA datasets, our dense retriever outperforms a strong Lucene-BM25 system greatly by 9{\%}-19{\%} absolute in terms of top-20 passage retrieval accuracy, and helps our end-to-end QA system establish new state-of-the-art on multiple open-domain QA benchmarks.",
}

@inproceedings{sun2020predicate,
  title={A Predicate-Function-Argument Annotation of Natural Language for Open-Domain Information eXpression},
  author={Sun, Mingming and Hua, Wenyue and Liu, Zoey and Wang, Xin and Zheng, Kangjie and Li, Ping},
  booktitle={Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  pages={2140--2150},
  year={2020}
}
@article{karpukhin2020dense,
  title={Dense passage retrieval for open-domain question answering},
  author={Karpukhin, Vladimir and O{\u{g}}uz, Barlas and Min, Sewon and Lewis, Patrick and Wu, Ledell and Edunov, Sergey and Chen, Danqi and Yih, Wen-tau},
  journal={arXiv preprint arXiv:2004.04906},
  year={2020}
}
@article{han2020xerte,
  title={xerte: Explainable reasoning on temporal knowledge graphs for forecasting future links},
  author={Han, Zhen and Chen, Peng and Ma, Yunpu and Tresp, Volker},
  journal={arXiv preprint arXiv:2012.15537},
  year={2020}
}
@article{devlin2018bert,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018}
}
@article{lin2020differentiable,
  title={Differentiable open-ended commonsense reasoning},
  author={Lin, Bill Yuchen and Sun, Haitian and Dhingra, Bhuwan and Zaheer, Manzil and Ren, Xiang and Cohen, William W},
  journal={arXiv preprint arXiv:2010.14439},
  year={2020}
}
@article{dhingra2020differentiable,
  title={Differentiable reasoning over a virtual knowledge base},
  author={Dhingra, Bhuwan and Zaheer, Manzil and Balachandran, Vidhisha and Neubig, Graham and Salakhutdinov, Ruslan and Cohen, William W},
  journal={arXiv preprint arXiv:2002.10640},
  year={2020}
}

@article{alsentzer2020subgraph,
  title={Subgraph neural networks},
  author={Alsentzer, Emily and Finlayson, Samuel and Li, Michelle and Zitnik, Marinka},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={8017--8029},
  year={2020}
}

@inproceedings{teru2020inductive,
  title={Inductive relation prediction by subgraph reasoning},
  author={Teru, Komal and Denis, Etienne and Hamilton, Will},
  booktitle={International Conference on Machine Learning},
  pages={9448--9457},
  year={2020},
  organization={PMLR}
}

@inproceedings{han2020explainable,
  title={Explainable subgraph reasoning for forecasting on temporal knowledge graphs},
  author={Han, Zhen and Chen, Peng and Ma, Yunpu and Tresp, Volker},
  booktitle={International Conference on Learning Representations},
  year={2020}
}
