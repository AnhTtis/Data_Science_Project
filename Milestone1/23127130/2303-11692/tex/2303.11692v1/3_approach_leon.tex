\vspace{-0.2cm}
\section{ByteCover3}
\label{sec:approach}
\vspace{-0.8em}
The overall architecture of \ourname{} is illustrated in Fig. \ref{fig:overall}. \ourname{} is inherited from ByteCover and ByteCover2 and adopts a multi-loss learning paradigm for CSI. One of the major settings that differs it from previous works lies in the use of local features. In this section, we first describe the extraction of local features and then introduce our main contributions, i.e., the LAL loss for local feature matching and the two-stage feature retrieval pipeline. 
\vspace{-1em}
\subsection{Local Feature Extraction}
To extract local features from each recording, we first resample the audio to $22,050$ Hz and split it into $N$ short chunks with a length of $20$ seconds and a overlap of $10$ seconds. For each chunk, we calculate a constant-Q transform (CQT) spectrogram with the number of bins per octave and the hop size set to $12$ and $512$ respectively, using Hann window as the window function. The CQT spectrograms are then downsampled with an averaging factor of $100$ along the time axis to reduce the computation cost. Therefore, the input audio is processed into a compressed $N$-chunk CQTs $\mathbf{S} \in \mathbb{R}^{N \times F \times T}$, where $N$ is the number of chunks, $F$ is the number of CQT bins ($84$ in our setting), and $T$ is the number of frames in each chunk.

The ResNet-IBN model \cite{du2021bytecover}, which replaces the residual connection blocks of ResNet50~\cite{he2016deep} with the instance-batch normalization (IBN) blocks, is then applied as the backbone to extract local embeddings from the input CQTs. In ByteCover3, our ResNet-IBN follows the original ByteCover setting, except that a 3-D input $\mathbf{S} \in \mathbb{R}^{N \times F \times T}$ is taken instead of the original 2-D input. The output of ResNet-IBN before the global generalized mean (GeM) pooling layer is hence a 4-D embedding ${\mathbf{Z}} \in \mathbb{R}^{N \times C \times H \times W}$, where $C$ is the number of output channels, $H$ and $W$ are the spatial sizes along the frequency and time axes respectively. In practice, we set $C = 2048$, $H = 6$ and $W=\nicefrac{T}{8}$. Finally, the temporal and frequency axes of $\mathbf{Z}$ are integrated by the GeM pooling operation and a dimensionality reduction module, i.e., PCA-FC \cite{du2022bytecover2}, is utilized on the channel dimension to obtain the compacted final embedding ${\mathbf{X}} \in \mathbb{R}^{N\times 512}$, which contains $N$ local features from the original audio, as opposed to ByteCover and ByteCover2 that only adopted a single global embedding.
\vspace{-1em}
\subsection{The Local Alignment Loss}
Existing CSI methods generally employ either a classification loss (e.g., softmax loss) or a metric learning loss (e.g., triplet loss) or a combination of them as the optimization objective during training. Nevertheless, as these methods only rely on a single global vector for each music track, their loss functions are limited to measuring similarities between two vectors (e.g. dot product or cosine similarity). Whereas in our case, we wish to compare two sequences of vectors that contain different number of local features, which requires a new similarity measure. To address this problem, we propose a novel loss design called Local Alignment Loss (LAL) $\mathcal{L}_{\operatorname{lal}}$ consisting of a classification loss $\mathcal{L}_{\operatorname{lac}}$ and a triplet loss $\mathcal{L}_{\operatorname{lat}}$.


% two loss objectives for training: the classification approach that classifies different versions of a music into the same class, and the metric learning approach that minimizes the distance between versions while maximizing the distance between different songs. The previous ByteCover~\cite{du2021bytecover} took a multi-loss training approach that combined both classification and triplet loss, which significantly improved the performance in CSI. 

We first introduce a similarity measure termed \emph{MaxMean} inspired by~\cite{cai2016two}: let $\mathbf{X} \in \mathbb{R}^{M\times C}$ and $\mathbf{Y} \in \mathbb{R}^{N\times C}$ denote two $C$-dimensional feature sequences that each contain $M$ and $N$ local features ($M$ and $N$ could be highly different). For each local feature $\{\mathbf{x}_i\}_{i=1}^M \in \mathbb{R}^{1\times C}$ in $\mathbf{X}$, we calculate the cosine similarity between $\mathbf{x}_i$ and all the local features $\{\mathbf{y}_i\}_{j=1}^N \in \mathbb{R}^{1\times C}$ in $\mathbf{Y}$, and regard the maximal value as the similarity measure $s_i$ for $\mathbf{x}_i$:
\begin{equation}
s_i = \max(\cos(\mathbf{x}_i, \mathbf{y}_j)), ~j=1,\dots,N,
\end{equation}
and the final similarity score is obtained by taking average over all the similarity measures, i.e., $\operatorname{MaxMean}(\mathbf{X}, \mathbf{Y})=\frac{1}{M}\sum_{i=1}^M s_i$. The shorter local feature is always regraded as the first operand, because the \textit{MaxMean} operator is non-commutative. Since only the maximal value of all the matching scores from $\mathbf{x}_i$ to $\mathbf{Y}$ is considered, we can avoid the distractions of local features in $\mathbf{Y}$ that are irrelevant to $\mathbf{x}_i$. 

With the \emph{MaxMean} measure described above, we then illustrate how the original classification loss in previous ByteCover~\cite{du2021bytecover} is transformed to the novel LAL. Recall in ByteCover, the classification loss $\mathcal{L}_{cls}$ is defined as:
\begin{align}
\mathcal{L}_{cls} = \operatorname{CE}(\sigma(\mathbf{W}\mathbf{f}^\mathrm{T}),~ y) = \operatorname{CE}(\sigma(\{\mathbf{w}_k\mathbf{f}^\mathrm{T}\}_{k=1}^{K}),~ y),
\end{align}
where $\operatorname{CE}(\cdot,\cdot)$ is the cross entropy and $\sigma(\cdot)$ is the softmax function. We denote $y$ as the ground-truth label, $\mathbf{f} \in \mathbb{R}^{1\times C}$ as the global feature extracted from ResNet-IBN, and $\mathbf{W}\in\mathbb{R}^{K\times C}$ as the weight matrix in the linear layer before softmax that contains $K$ weight vectors $\{{\bf w}_k\}_{k=1}^K$ for classification.

To adapt $\mathcal{L}_{cls}$ to the novel \emph{MaxMean} measure with the local features, we draw inspiration from \cite{sun2020circle} and consider $\mathbf{w}_k$ as a proxy feature representation of the $k^\mathrm{th}$ class. In this sense, the result of $\mathbf{w}_k\mathbf{f}^\mathrm{T}$ can be interpreted as the similarity score between two features $\mathbf{w}_k$ and $\mathbf{f}$ based on dot product, which we argue can be replaced by the \emph{MaxMean} metric. Specifically, our new local alignment classification loss $\mathcal{L}_{lac}$ is written as:
\begin{align}
\mathcal{L}_{lac}  & = \operatorname{CE}(\sigma(\{\operatorname{logit}_k\}_{k=1}^K), y),\\
\operatorname{logit}_k  & = \operatorname{MaxMean}(\mathbf{X}, \mathbf{W}_k),
\end{align}
where ${\bf X} \in \mathbb{R}^{N \times C}$ is the final embedding with $N$ local features extracted by ResNet-IBN, $\mathbf{W} \in \mathbb{R}^{K \times L \times C}$ is a trainable weight matrix in the linear layer before softmax and ${\bf W}_k \in \mathbb{R}^{L \times C}$ denotes the proxy representation for class $k$.

In addition to the classification loss, a triplet loss was also used in ByteCover, which is simply modified in ByteCover3 by replacing the Euclidean distance with \emph{MaxMean} metric:
\begin{equation}
\mathcal{L}_{lat} = [\operatorname{MaxMean}({\bf X}_n,{\bf X}) - \operatorname{MaxMean}({\bf X}_p,{\bf X})]_{+}.
\end{equation}
Finally, our overall loss $\mathcal{L}_{lal}$ is given by $\mathcal{L}_{lal} = \mathcal{L}_{lac} + \mathcal{L}_{lat}$.
\vspace{-0.4em}
\subsection{Two-Stage Feature Retrieval}
\vspace{-0.2em}
An efficient feature retrieval pipeline is also critical for constructing a practical industrial-strength CSI system. Previous methods usually use an all-pairs strategy that includes computing the similarity between the query sample and each item in the database, which is time consuming. Moreover, there is a significant leap in complexity from the vector similarity measure ($\mathcal{O}(1)$) to the local alignment measure \textit{MaxMean} ($\mathcal{O}(n^2)$)~\cite{cai2016two}, which makes the all-pairs strategy even worse for ByteCover3. To solve this problem, we propose a two-stage pipeline with a hierarchical searching strategy for the retrieval of deep local embeddings. 

Given a query sample with $M$ local features, the first stage is to eliminate the database recordings that are highly unlikely to be a match. Specifically, for each local feature in the query, we search for its Top-$K$ nearest neighbors in the gallery of local features extracted in advance from all the database recordings, using the hierarchical navigable small world (HNSW) graphs \cite{malkov2018efficient}, with $K$ set to $50$. This results in a candidate set of $M\times K$ local matches for the given query, based on which our second stage of feature retrieval is further performed. Suppose that the $M \times K$ local matches originate from $D$ database recordings ($D \leq M\times K$ since some local matches may original from the same recordings), and thus our second stage is to compare the given query with each of the $D$ candidate recordings, based on the \emph{MaxMean} measure introduced above. The candidate recordings with the highest \emph{MaxMean} similarities are finally outputted as the retrieval results.

In practical use, the query is typically less than 60s, and thus we have $M \leq 5$ as our local features are extracted every 20s with overlap of 10 seconds. Therefore, in the second stage we only need to calculate the \emph{MaxMean} similarity for $M \times K \leq 250$ times, which is significantly less then the calculation needed in the all-pairs strategy.

% The first stage of recalling is conducted by vector similarity search that aims to shrink the size of candidates sets. In detail, every vector in the local embedding sequence with length $n$ of query sample searches for its Top-$K$ nearest vector in the database. These $n \times K$ results yield a candidate set which has $n \times K$ recording at the most, because the different results may belong to the same recording. In practice, the $K$ is set to $50$ and the $n$ ranges from $1$~$6$ while the duration query is shorter than 1 minute.  Moreover, the implementation of vector similarity search can be accelerated dramatically with ANN search methods, as the error of Top-$K$ caused by ANN decreases when $K$ increases from $1$ ~\cite{malkov2018efficient}. Thus, the size of candidate set is shrunk from millions to $50~300$ by the first recall layer. Further, the second layer conducts fine-grained ranking under MinMax measure between the query sample and other samples in this compact candidate set to get the final most matched recording. As shown in Tab. \ref{tab:time}, by incorporating the two-layer searching pipeline and ANN method, the new ByteCover3 system with local embeddings achieves a similar speed with the previous ByteCover2 system that uses global embedding.