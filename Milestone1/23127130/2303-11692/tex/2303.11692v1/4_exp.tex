\vspace{-1em}
\section{Experiments}
\label{sec:exp}


\vspace{-0.5em}





\subsection{Evaluation Settings and Training Details} 
\label{subsec:dataset}



\vspace{-0.2em}
The evaluation of ByteCover3 was conducted based on three public datasets: (1) \emph{SHS100K}~\cite{xu2018key}, which is collected from the \textit{Second Hand Songs} dataset, and consists of 8,858 cover groups and 108,523 recordings; (2) \emph{Covers80} \cite{ellis2007identifying}, which contains 160 recordings of 80 songs, with 2 covers per song; and (3) \emph{Da-TACOS} \cite{yesiler2020accurate}, which consists of 1000 cliques and 15,000~music performances. 

More specifically, the training subset of SHS100K was used to train ByteCover3, and to obtain short music clips for training, we randomly cut a segment from each training recording, where the segment duration is uniformly sampled between $6$s and $60$s. These short music clips were then mixed with the original full-track training samples to form the final training set. The testing of ByteCover3 was performed in a query-retrieval mode using the testing subset of SHS100K, Cover80 and Da-TACOS. For each query, we constructed a query set consisting of the original full-track recording, and $9$ music clips randomly cut from it, with the duration being $6$, $10$, $15$, $20$, $25$, $30$, $40$, $50$ and $60$ seconds respectively.


For the training of \ourname{}, the weights of the trained ByteCover2 model were used to initialize the ResNet-IBN module. Similar to ByteCover2, we implemented ByteCover3 in Pytorch framework and trained it using the Adam Optimizer. The learning rate and the batch size were set to $0.001$ and $128$ respectively. Every training batch contained synthetic short samples and full-length samples mixed in a $1:1$ ratio to improve the stability of training process.

During testing, the mean average precision (mAP) and the mean rank of the first correctly identified cover (MR1) were used as evaluation metrics. In our calculation of mAP and MR1, we set the similarity values to 0 for the database items that are blocked in the first retrieval stage. 

Moreover, since the queries were derived from snippets of some recordings in the result list, we ignored these recordings when calculating the evaluation metrics, to ensure that there is no leakage in the final detection performance.



\begin{figure}[t]



	\resizebox{0.95 \columnwidth}{!}{

	\begin{tikzpicture}
        \node (image) at (0,0) {

            \includegraphics[width=.7\textwidth]{figures/curve.pdf}
            };
        \node[latex-] at (11em,7.5em){ByteCover2};
        \node[latex-] at (0em,10.1em){\textbf{\ourname}};
        \node[latex-] at (11em,5.2em){Re-MOVE};

        

        \node[latex-] at (0em,-13.7em){Length};
        \node[latex-] at (-18.4em,0em){\rotatebox{90}{mAP on SHS100K}};
        
        \node[latex-] at (7.8em,-6.5em) {\resizebox{0.6\columnwidth}{!}{%\tablestyle{2pt}{1}
		\begin{tabular}[b]{l|cr}
		    \hline
			& mAP & Length  \\
			\hline

			Re-MOVE\cite{yesiler2020less} & 2.34\%  & 6 Secs \\
			ByteCover2\cite{du2022bytecover2} & 1.59\%  & 6 Secs \\
			\bf \ourname & \bf 8.40\%  & \bf 6 Secs \\
			\hline

			Re-MOVE\cite{yesiler2020less} & 50.5\% & 60 Secs  \\
			ByteCover2\cite{du2022bytecover2} & 68.4\%  & 60 Secs \\
			\bf \ourname & \bf 73.2\%  & \bf 60 Secs \\
			\hline
    	\end{tabular}}};
	
    \end{tikzpicture}
    }
	\hspace{-44mm}
        \vspace{-1em}
	\caption{Length of Queries vs. Performance.}
        \vspace{-1.5em}
	\label{fig:lengperf}
\end{figure}



\vspace{-0.6em}
\subsection{Comparison on Performance and Efficiency}
\label{ssec:comp}
\vspace{-0.5em}

Fig.~\ref{fig:lengperf} displays the mAP results of \ourname{} for different query lengths on the synthetic SHS100K test set, using Re-MOVE \cite{yesiler2020less} and ByteCover2 \cite{du2022bytecover2} as compared methods. As illustrated in the figure, our \ourname{} model achieves the best mAPs for all the query lengths. This clearly indicates the effectiveness of ByteCover3 for identifying short music queries. As for the task of identifying full-length recordings, our ByteCover3 is also competitive. As presented in Table~1 (the first three parts), the performances of ByteCover3 are comparable with the state-of-the-art method ByteCover2~\cite{du2022bytecover2} on all the three test sets in the full-length settings, even with an embedding size of $512$, which is smaller than those of ByteCover and ByteCover2.

To measure the effect of our proposed LAL loss on the performance of CSI, we conducted an ablation study on SHS100K-TEST for query length of 30s, by comparing \ourname{} with ByteCover2 and \emph{ByteCover2 + Local}. \emph{ByteCover2 + Local} modifies ByteCover2 with local features as done in \ourname{}, and it differs from \ourname{} in that it does not use the LAL loss. The lower part of Table~1 gives the comparison results. As shown in the table, \ourname{} achieves the highest mAP and lowest MR1 among the methods, which obviously proves the importance of LAL for short-query CSI.

Our last experiment is to test the time consumption of CSI, using the same setting as in \cite{du2022bytecover2}. As shown in Table~2, even equipped with local features, the retrieval speed of \ourname{} is still at the same scale with ByteCover2 using an embedding size of $128$. We owe this to the use of our two-stage feature retrieval pipeline. Please note that the inference time of ByteCover3 is two times longer than ByteCover2, because we split the input CQT spectrogram into chunks and the complexity of feature extraction is higher. However, the total time consumption of ByteCover3 is sitll similar with that of ByteCover2-128.

\input{tables/2_ablation}
\vspace{-1em}

\input{tables/time_table}

