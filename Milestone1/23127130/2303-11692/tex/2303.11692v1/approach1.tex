\section{Approach}

\subsection{Problem Formulation}
% a training set
% find a function for feature extraction toward this task
\begin{figure}
	\centering
	\includegraphics[width=3.2in]{pipe.pdf}
	\caption{Training procedure and retrieval procedure.}
	\label{fig:pipe}
\end{figure}
As shown in \figref{fig:pipe}, we have a training dataset $D = \{(x_n, t_n)\}$, where $x_n$ is a recording and $t_n$ is a one-hot vector denoting to which song (or class) the recording belongs. Different versions of the same song are viewed as the samples from the same class, and different songs are regarded as the different classes. We aim to train a classification network model parameterized as $\{\theta,\lambda\}$ from $D$. As shown in \figref{fig:net}, $\theta$ is the parameter of all convolutions and FC$0$; $\lambda$ is the parameter of FC$1$; $f_\theta$ is the output of the FC$0$ layer. Then, this model could be used for cover song retrieval. More specifically, after the training, given a query $Q$ and references ${R_n}$ in the dataset, we extract latent features $f_\theta(Q),  {f_\theta(R_n)}$ which we call as music representations obtained by the network, and then we use a metric $s$ to measure the similarity between them. In the following sections, we will discuss the low-level representation used, the design of network structure and a robust trained model against key transposition and tempo change. We use lowercase for audio and uppercase for the CQT to discriminate.



\subsection{Low-level Representation}
The CQT, mapping frequency energy into musical notes, is extracted by \textit{Librosa} \cite{mcfee2015librosa} for our experiment. The audio is resampled to $22050$ Hz, the number of bins per octave is set as $12$ and Hann window is used for extraction with a hop size of $512$. Finally, a $20$-point mean down-sampling in the time direction is applied to the CQT, and the resulting feature is a sequence with a feature rate of about $2$ Hz. It could also be viewed as an $84 \times T$ matrix where $T$ depends on the duration of input audio.
%An visualization of CQT is shown in \figref{fig:cqt}, where the x-axis and y-axis represent time and frequency dimension respectively. Some segmentations displayed in the figure represent notes and their harmonic series in music. Compared to chroma feautres extensively used in CSI systems \cite{ellis2007identifyingcover,serra2008chroma}, CQT is a low-level feature as it does not involve projecting frequency components into twelve pitch classes. It remains more information and helps neural networks to learn a better representation for CSI. 

\subsection{Convolutional Neural Network for Key Transposition}
\begin{figure}
	\centering
	\includegraphics[width=3.2in]{cqt.pdf}
	\caption{Visualization of CQTs extracted from two versions of \textit{Twinkle Little Star}. Note that log operation is applied to make the visualization more remarkable.}
	\label{fig:cqt}
\end{figure}

Key transposition refers to that a collection of notes is moved up or down in pitch by a constant interval. \figref{fig:cqt} shows an example of key transposition. The readers could find the figures look very similar. If one shifts vertically, it becomes another one. However matching the figures on the original feature space (without consideration of transposition) fails to measure their similarity properly. To handle key transposition, straight-forward approaches test all possible transpositions or some potential transpositions \cite{ellis2007identifyingcover}:
\begin{equation}
  \label{equ:key}
  \max_{i} s(f(Q), f(R^{(i)}))
\end{equation}
in which $Q, R$ are CQTs extracted from query and reference respectively and $R^{(i)}$ represents shifting $R$ vertically for $i$ elements. Note that we have $R^{(0)} = R$. $f$ is a function indicating some post-processing to $Q$ and $R^{(i)}$, and $s$ represents some algorithm or metric to measure the similarity between $Q$ and $R^{(i)}$. Moreover, some researchers try to extract key-invariant representations; i.e. whatever the key is, the extracted features remain unchanged exactly or approximately \cite{bertin2012large,Seetharaman2017CoverSI,xu2018key}. They extract an intermediate feature satisfying that $f(R^{(i)}) = f(R^{(j)}), i \neq j$, and with this property, \equref{equ:key} becomes $s(f(Q), f(R))$. 


However previous methods use hand-crafted features for $??f$ \cite{martin2012blast,Seetharaman2017CoverSI}. In our approach, CNN is used to learn key-invariant representation. Visualizing CQTs as images, one may find that key-invariant representation is very related to shift-invariant representation in image classification. However, instead of recognizing objects in images, CNN is used to capture chord patterns or melodic structures from music. It could be used to learn a representation invariant to translations approximately \cite{goodfellow2016deep}, and thus \equref{equ:key} becomes $f(R^{(i)}) \approx f(R^{(j)}), i \neq j$ when using CNN ($\theta$ is the parameter of network). Still we find that this approximation work well in the experiments. Note that \cite{xu2018key} also use CNN to learn a key-invariant representation but their approach uses chroma sequences and aims at learning a recurrent-invariant feature. Our approach is more straight-forward and achieves much higher precision than theirs (see Section \ref{subsec:com}). 

However previous methods use hand-crafted features for $f$ \cite{martin2012blast,Seetharaman2017CoverSI}. In our approach, CNN is used to learn key-invariant representation. Ideally, given musical descriptor $Q \in R^{84 \times T}$, we assume that $Q$ is a relatively sparse matrix and most of the energy lies in a sub-matrix $Q_ \in R^{M \times T}$ ($M < 84$). $Q$ could be written as below:
 \begin{equation}
    \label{equ:key}
   \max_{i} s(f(Q), f(R^{(i)}))
 \end{equation}


\subsection{Network Structure}



\subsection{Training Scheme}


\subsection{Retrieval}
After the training, the network is used to extract music representations. As shown in \figref{fig:pipe}, given a query $q$ and a reference $r$, we first extract their CQT descriptors $Q$ and $R$ respectively, which are fed into the network to obtain music representations $f_\theta(Q)$ and $f_\theta(R))$, and then the similarity $s$, defined as their cosine similarity, are measured.
\begin{equation}
s(f_\theta(Q), f_\theta(R)) = \frac{f_\theta(Q)^Tf_\theta(R)}{|f_\theta(Q)||f_\theta(R)|}
\end{equation}
After computing the pair-wise similarity between the query and references in the dataset, a ranking list is returned for evaluation.