\section{Introduction}
% \begin{figure}[t]
% \centering
% \includegraphics[width=0.98\columnwidth]{figures/intro.jpg} 
% \caption{Illustration .} 
% \label{fig intro}
% \end{figure}

Recent years have witnessed a successful use of deep learning methods in the task of cover song identification (CSI), i.e., finding cover versions of a given music track in a music database. These methods generally formulate CSI as either a classification problem~\cite{yu2019temporal, yu2020learning, xu2018key} or a metric learning problem~\cite{yesiler2020accurate}, or a combination of both~\cite{du2021bytecover,du2022bytecover2,hu2022wideresnet}, and then train deep neural networks to learn low-dimensional features from different representations of audio. The features are then indexed and retrieved, where the distances between features are used to measure the similarities of songs. Deep learning models have proved their capability in learning discriminative and robust features, boosting the accuracy of CSI by a large margin compared to traditional methods based on handcrafted features.

% different audio representations such as constant-Q transform (CQT)~\cite{brown1991calculation}, melodic line~\cite{doras2020combining} and the harmonic structure~\cite{yesiler2019datacos}

 % that are invariant to irrelevant changes in timbre, tempo, structure and lyrics among various music versions

Despite of the promising progress above, one challenge remains for CSI in real-world applications, which is that most existing methods only consider situations when the query and the database item are both full-length music tracks with a typical duration of several minutes. Nevertheless, in many real-world scenarios, the task is to identify short music queries which are, for example, tens of seconds long, against full-length database songs. For instance, massive short videos with less-than-one-minute length have been created and uploaded to short video platforms such as TikTok in the past years, where a large proportion of these videos are accompanied by a carefully-selected music track that may be a remixed or cover fragment of an original music. For copyright management and reporting purpose, platforms need to identify these cover fragments. Unfortunately, as shown in section~\ref{sec:exp}, most existing CSI systems failed in the experiments of identifying short queries.  
An explanation for such incapability is that current CSI methods are mostly equipped with a global pooling layer to aggregate the information from all time sections, and then generate a global embedding for each song as the CSI feature. However, when matching a short music clip to a full-length recording, there will be some irrelevant sections in the full-length recording that may create noises in the embedding, which pose a negative impact on the similarity measurement between features. \par
% To solve this problem, an intuitive idea is to extract local features from both queries and database items, and calculate the similarity between two songs based only on the local features that have been matched, thus avoiding the interference from irrelevant sections. This idea has been considered in a very few traditional CSI works~\cite{rafii2014audio, cai2016two}, which, however, all have their limitations. For example, although capable of dealing with short queries, \cite{cai2016two} can only identify live versions of songs from known artists. \cite{cai2016two} performed music segmentation first and used segment-wise features for similarity measurement, but its performance has a noticeable gap with respect to SOTA CSI methods. Moreover, there is no report in~\cite{cai2016two} about the adaption of the method to short queries.
To solve this problem, an intuitive idea is to extract local features from both queries and database items, and calculate the similarity between two songs based only on the local features that have high matching score, thus avoiding the interference from irrelevant sections. This idea has been considered in a few traditional CSI works~\cite{muller2005audio, casey2008analysis, grosche2012toward, rafii2014audio, cai2016two}. However, due to the limited discriminative capacity and robustness of handcrafted features, as well as the high complexity in performing efficient indexing and retrieval, traditional CSI methods are generally only applicable for small databases (e.g., the database used in \cite{grosche2012toward} contains only 2,484 recordings) or simplified scenarios (e.g., \cite{rafii2014audio} can only identify live versions of songs from known artists), while their performances for the general CSI task against large databases are poor (or have not been reported). In \cite{zalkow2021efficient}, Zalkow \emph{et al.} made the first attempt to employ deep learning for short-query CSI and used convolutional neural networks (CNNs) to compress the input features. However, the databases used in \cite{zalkow2021efficient} are still limited to thousands of audio recordings or even less, and its usability in real-world applications is unverified.



% , which, however, all have their limitations. For example, although capable of dealing with short queries, \cite{cai2016two} can only identify live versions of songs from known artists. \cite{cai2016two} performed music segmentation first and used segment-wise features for similarity measurement, but its performance has a noticeable gap with respect to SOTA CSI methods. Moreover, there is no report in~\cite{cai2016two} about the adaption of the method to short queries.

In this paper, we extend our previous works of ByteCover~\cite{du2021bytecover} and ByteCover2~\cite{du2022bytecover2}, and present the new version of our CSI system, \emph{ByteCover3}, to solve the problem of identifying short music queries against a industry-scale database of full-length recordings. Different from existing works that rely on global features~\cite{yesiler2020accurate,yu2020learning,doras2019cover,du2021bytecover,du2022bytecover2}, ByteCover3 is designed to learn a set of deep local embeddings (or features) from each audio and uses the matching of local embeddings to accomplish the identification of short queries against full songs. To optimize the matching of local features, we propose a new loss termed \emph{local alignment loss} (LAL) and apply it in our multi-loss paradigm first introduced in ByteCover \cite{du2021bytecover}. LAL constitutes one of the major contributions of ByteCover3, and by using the LAL loss, the performance of CSI can be significantly improved. Moreover, to improve the efficiency of feature matching, a two-stage feature retrieval pipeline, which consists of an approximate nearest neighbor (ANN) stage and a re-ranking stage, is also designed. This consists of our second contribution.



% . LAL is specifically designed to optimize the similarity measurement between local features of music tracks with different length, and by using LAL, the performance of CSI can be significantly improved. To optimize feature indexing and retrieval, we also build for ByteCover3 a two-stage feature retrieval pipeline, which consists an approximate nearest neighbor search stage and a re-ranking stage to further improves the efficiency and effectiveness of ByteCover3.