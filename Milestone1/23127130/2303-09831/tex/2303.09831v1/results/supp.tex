% Template for ICME 2022 paper; to be used with:
%          spconf.sty  - ICASSP/ICIP/ICME LaTeX style file, and
%          IEEEbib.bst - IEEE bibliography style file.
% --------------------------------------------------------------------------
\documentclass{article}
\usepackage{spconf,amsmath,epsfig}
\usepackage{times}  % DO NOT CHANGE THIS
\usepackage{helvet}  % DO NOT CHANGE THIS
\usepackage{courier}  % DO NOT CHANGE THIS
\usepackage[hyphens]{url}  % DO NOT CHANGE THIS
\usepackage{graphicx} % DO NOT CHANGE THIS
\urlstyle{rm} % DO NOT CHANGE THIS
\def\UrlFont{\rm}  % DO NOT CHANGE THIS
\usepackage{natbib}  % DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT
\usepackage{caption} % DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT
\DeclareCaptionStyle{ruled}{labelfont=normalfont,labelsep=colon,strut=off} % DO NOT CHANGE THIS
\frenchspacing  % DO NOT CHANGE THIS
\setlength{\pdfpagewidth}{8.5in}  % DO NOT CHANGE THIS
\setlength{\pdfpageheight}{11in}  % DO NOT CHANGE THIS
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{makecell}
\usepackage{array}
\usepackage{color}
\newcommand{\tim}[1]{\textcolor{red}{[LJ: #1]}}
\usepackage{flushend}
\usepackage{amsmath}
\usepackage{amssymb}
\definecolor{dark}{rgb}{0.2, 0.2, 0.5}
\usepackage[pagebackref=false,colorlinks=true,bookmarks=false,linkcolor=red,citecolor=dark,urlcolor=black]{hyperref}
\usepackage{newfloat}
\usepackage{listings}
\let\OLDthebibliography\thebibliography
\renewcommand\thebibliography[1]{
  \OLDthebibliography{#1}
  \setlength{\parskip}{0pt}
  \setlength{\itemsep}{0pt plus 0.3ex}
}

\pagestyle{empty}

\begin{document}\sloppy

% Example definitions.
% --------------------
\def\x{{\mathbf x}}
\def\L{{\cal L}}


% Title.
% ------
\title{Model-driven Face Stylization: A privacy-preserving Approach\\
Appendix}
%
% Single address.
% ---------------
\name{Anonymous ICME submission}
%Address and e-mail should NOT be added in the submission paper. They should be present only in the camera ready paper. 
\address{}

\maketitle


\section{Discriminator}
We build our model upon the official Pytorch implementation of PSP~\cite{psp}, from which we inherit most of the training details, including the progressive discriminator proposed by Karras \textit{et al.}~\cite{PG-GAN}.

\begin{figure}
	\begin{center}
		\includegraphics[scale=0.25]{D.pdf} 
	\end{center}
	\caption{The architecture of the discriminator.}
	\label{moti}
\end{figure}

\begin{figure}
	% \small
	\centering
		\includegraphics[width=0.8\linewidth]{uky-sty.jpg} 
	\caption{Results generated from the style-preserving model trained on Ukiyoe~\cite{pinkney2020ukiyoe} dataset with different noises.} 
	\label{uky}
\end{figure}
\section{More Visualized Results}
We provide more testing results of the comparisons with the state-of-the-art methods in Fig.~\ref{SOTA}, and results generated from the style-preserving model trained on Ukiyoe, CUFS and LAMP-HQ in Fig.~\ref{uky}, Fig.~\ref{sketch} and Fig.~\ref{NIR}, respectively.
Fig.~\ref{rand-noise} shows the noise-guided results of MODIFY, we are able to generate diverse results from random-selected noise vectors.

\begin{figure*}[!htbp]
	\small
	\centering
	\setlength\tabcolsep{0.1mm}{
		\renewcommand\arraystretch{1.5}
		\begin{tabular}{p{2cm}<{\centering}p{1.8cm}<{\centering}p{1.8cm}<{\centering}p{1.8cm}<{\centering}p{1.8cm}<{\centering}p{1.8cm}<{\centering}p{1.8cm}<{\centering}p{1.8cm}<{\centering}p{1.8cm}<{\centering}}
			% \hline
			(a) Input & (b) OST & (c) StarGAN v2 & (d) StarGAN v2 &(e) Unpaired PSP & (f) Unpaired PSP & (g) MODIFY & (h) MODIFY & (i) MODIFY \\
			\hline
			\# Samples in $\mathcal{X}$ & 1 & 1 & All & 1 & All & 1 & All & All\\
			\hline
			Data-driven &  True & True & True & True & True & False & False & False\\
			\hline
			Training mode & Offline & Offline & Offline & Offline & Offline & Test-time & Online & Offline\\
			%			\hline
			\includegraphics[height=0.8\textheight]{app-sota-a.jpg} &
			\includegraphics[height=0.8\textheight]{app-sota-b.jpg} &
			\includegraphics[height=0.8\textheight]{app-sota-c.jpg} &
			\includegraphics[height=0.8\textheight]{app-sota-d.jpg} &
			\includegraphics[height=0.8\textheight]{app-sota-e.jpg} &
			\includegraphics[height=0.8\textheight]{app-sota-f.jpg} &
			\includegraphics[height=0.8\textheight]{app-sota-g.jpg} &
			\includegraphics[height=0.8\textheight]{app-sota-h.jpg} &
			\includegraphics[height=0.8\textheight]{app-sota-i.jpg} \\
			% 	\hline
	\end{tabular}}
	\caption{Results of online, offline and test-time training versions of MODIFY compared to other three state-of-the-art approaches (\textit{i.e.}, OST~\cite{OST}, StarGAN v2~\cite{starganv2}, and PSP~\cite{psp}). Fig.~\ref{SOTA} (d), (f), (h) and (i) use the entire source dataset, and Fig.~\ref{SOTA} (b), (c), (e) and (g) use one sample in source dataset. Note that  (b), (c), (e) use a random-selected source image for training, and our method shown in (i) adaptively updates the model for the input test image without separate training.} 
	\label{SOTA}
\end{figure*} 
\begin{figure*}
\begin{center}
	\includegraphics[scale=0.2]{noise-guided.pdf} 
\end{center}
\caption{Results generated from different noise vector. Each column generated from the same one. MODIFY can generate multi-style images from different noises.}
\label{rand-noise}
\end{figure*}
\begin{figure}
	% \small
	\centering
	\setlength\tabcolsep{0mm}{
		\renewcommand\arraystretch{1.2}
		\scalebox{0.85}{
			\begin{tabular}{p{2cm}<{\centering}p{2cm}<{\centering}p{2cm}<{\centering}p{2cm}<{\centering}}
				% 	\hline
				(a) Input & (b) Style 1&(c) Style 2& (d) Style 3\\
				\includegraphics[width=1\linewidth]{app-s-a.jpg} &
				\includegraphics[width=1\linewidth]{app-s-b.jpg} &
				\includegraphics[width=1\linewidth]{app-s-c.jpg} &
				\includegraphics[width=1\linewidth]{app-s-d.jpg} \\
				% 	\hline
	\end{tabular}}}
	\caption{Results generated from the style-preserving model trained on CUFS~\cite{cufs} dataset with different noises.} 
	\label{sketch}
\end{figure}
\begin{figure}
	% \small
	\centering
	\setlength\tabcolsep{0mm}{
		\renewcommand\arraystretch{1.2}
		\scalebox{0.85}{
			\begin{tabular}{p{2cm}<{\centering}p{2cm}<{\centering}p{2cm}<{\centering}p{2cm}<{\centering}}
				% 	\hline
				(a) Input & (b) Style 1&(c) Style 2& (d) Style 3\\
				\includegraphics[width=1\linewidth]{app-n-a.jpg} &
				\includegraphics[width=1\linewidth]{app-n-b.jpg} &
				\includegraphics[width=1\linewidth]{app-n-c.jpg} &
				\includegraphics[width=1\linewidth]{app-n-d.jpg} \\
				% 	\hline
	\end{tabular}}}
	\caption{Results generated from the style-preserving model trained on LAMP-HQ~\cite{lamp-hq} dataset with different noises.} 
	\label{NIR}
\end{figure}



% -------------------------------------------------------------------------
\bibliographystyle{main}
\bibliography{main}

\end{document}
