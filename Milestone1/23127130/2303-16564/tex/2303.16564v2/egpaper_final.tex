\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{iccv}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage[T1]{fontenc}
\usepackage{arydshln}
\usepackage{algorithm}
\usepackage{algpseudocode}
% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[breaklinks=true,bookmarks=false]{hyperref}

\iccvfinalcopy % *** Uncomment this line for the final submission

% \def\iccvPaperID{8875} % *** Enter the ICCV Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
% \ificcvfinal\pagestyle{}\fi

\begin{document}

%%%%%%%%% TITLE
\title{Implicit Visual Bias Mitigation by Posterior Estimate Sharpening of a Bayesian Neural Network}

\author{Rebecca S Stone, Nishant Ravikumar, Andrew J Bulpitt, David C Hogg \\
School of Computing, University of Leeds \\
{\tt\small \{sc16rsmy, n.ravikumar, a.j.bulpitt, d.c.hogg\}@leeds.ac.uk}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
}

\maketitle
% Remove page # from the first page of camera-ready.

%%%%%%%%% ABSTRACT
\begin{abstract}
The fairness of a deep neural network is strongly affected by dataset bias and spurious correlations, both of which are usually present in modern feature-rich and complex visual datasets. Due to the difficulty and variability of the task, no single de-biasing method has been universally successful. In particular, implicit methods not requiring explicit knowledge of bias variables are especially relevant for real-world applications. We propose a novel implicit mitigation method using a Bayesian neural network, allowing us to leverage the relationship between epistemic uncertainties and the presence of bias or spurious correlations in a sample. Our proposed posterior estimate sharpening procedure encourages the network to focus on core features that do not contribute to high uncertainties. Experimental results on three benchmark datasets demonstrate that Bayesian networks with sharpened posterior estimates perform comparably to prior existing methods and show potential worthy of further exploration.
\end{abstract}

%%%%%%%%% BODY TEXT
\section{Introduction}

In a perfectly unbiased world, every possible combination of features would be equally present in every sample set. Unfortunately, it has become increasingly apparent that societal and historical biases are prevalent in every domain. Furthermore, even if it were always feasible to obtain a balanced dataset, this in itself does not ensure a fair model, as relative quantities are not the sole contributor towards bias~\cite{wang2020mitigating, mehrabi2021survey}. Not only are biased datasets pervasive, but algorithms themselves can even amplify bias~\cite{zhao2017men, hall2022systematic}.

Particularly in the visual domain, it is not realistic to assume a comprehensive knowledge of all bias variables in a dataset, let alone know where they occur across the training data. In high-impact domains highly susceptible to dataset bias, including medicine, finance, welfare, and crime, confidentiality and privacy requirements prohibit general use of metadata which might specify presence of bias variables in training sets. Furthermore, in most cases population bias makes balanced sampling impossible. The implications and consequences~\cite{chouldechova2018case, challen2019artificial, noseworthy2020assessing, buolamwini2018gender}~\footnote{https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing} of unfair models built using such biased datasets are disheartening and intolerable.

Recently, various studies propose implicit mitigation methods for neural networks~\cite{wang2020towards, nam2020learning, xu2021consistent, amini2019uncovering}. Yet many challenges still remain. Sample imbalances between minority and majority groups can be too steep to overcome without overfitting~\cite{weiss2007cost, amini2019uncovering, stone2022epistemic}, learning spurious correlations is often much easier than learning the desired core features~\cite{hall2022systematic}, and some bias-conflicting samples are simply more difficult to learn than others~\cite{wang2020mitigating}. Additionally, studies comparing results for multiple methods on more than one visual bias benchmark dataset indicate that \textit{no single mitigation method is universally effective}~\cite{shrestha2022investigation, wang2020towards, hall2022systematic}.

We contribute towards a better understanding and exploration of visual bias mitigation by proposing a novel implicit mitigation method which leverages the correlation between uncertainties and bias-conflicting samples. Epistemic uncertainties in the Bayesian paradigm arise from lack of knowledge -- missing information or data -- and decrease given a more comprehensive distribution of data. This contrasts with aleatoric uncertainty, or data ``noise" uncertainty which is irreducible since probabilistic variations are present irrespective of the dataset size. Thus, a more balanced distribution of data (in both quantity and ease of learning) will reduce epistemic uncertainties. We can directly leverage this for implicit bias mitigation.

A few recent works have also used this correlation. Branchaud-Charron et al.~\cite{branchaud2021can} show Bayesian Active Learning by Disagreement (BALD)~\cite{gal2017deep} can help mitigate bias against relying on spurious correlations through re-balancing the dataset via an acquisition scheme which greedily reduces epistemic uncertainty. Khan et al.~\cite{khan2019striking} use Bayesian uncertainty estimates to deal with class imbalance by weighting the loss function to move learned class boundaries away from more uncertain classes. This focuses primarily on class-level bias in the class imbalance scenario, with some consideration for sample-level bias; our proposed method focuses solely on sample-level bias as we do not expect bias to be constrained to classes. Epistemic uncertainties are also used in a previous study~\cite{stone2022epistemic} (EpiWt) to dynamically weight samples during training, but this approach suffers from the same overfitting problem as both explicit and implicit up-weighting methods.

Contributing to this body of literature, we propose a novel posterior estimate sharpening approach, which focuses on uncertainty-guided implicit feature re-weighting in the classification layers of 
a Bayesian neural network. In addition to showing potential compared against other methods, our sharpening approach also provides post-inference uncertainty estimates which can be useful for a variety of purposes.

\section{Related Work}

Prior mitigation methods can be categorized as \textit{explicit} or \textit{implicit}. Explicit approaches require knowing the bias variables and which training samples are bias-aligned. Various works have experimented with re-weighting or re-sampling, the intuitive approach by which the minority samples are made to appear more often, thus encouraging the model not to leverage undesired correlations. This can be done via sampling probability~\cite{chawla2002smote, kamiran2012data}, loss weighting~\cite{cui2019class}, or synthetically generating or augmenting data to balance out or encourage learning of certain features~\cite{he2008adasyn, geirhos2018imagenet}.

Other methods focus on discouraging the representation portion of the network from encoding the biases. This can be accomplished through adversarial learning~\cite{zhang2018mitigating, wadsworth2018achieving}, or by manipulating the feature representations in latent space to disentangle the spurious and core features~\cite{thong2021feature, tartaglione2021end}.

Another category of works explicitly learn the bias variable, in order to then unlearn it or adjust the model decisions based on this knowledge~\cite{dwork2012fairness, wang2020towards}. Alvi et al.~\cite{alvi2018turning} propose a variant of this through ``fairness through blindness", simultaneously learning the target task while learning to ignore the bias variables. However, these methods are vulnerable to redundant encoding~\cite{mehrabi2021survey}, or learning combinations of other perhaps unlabeled bias variables as a proxy for the chosen bias variables. 

The knowledge of bias variables can also be used to group the training data into subgroups for training fairer models. Group distributionally robust optimization (gDRO) suggests sub-grouping data according to their bias and class variables, and optimizes such that the model generalizes best over all groups~\cite{sagawa2019distributionally}. Predictive group invariance (PGI)~\cite{ahmed2021systematic} also groups data according to bias variables and encourages matched predictive distributions across the groups, penalizing learning spurious features that do not appear across all groups. Both~\cite{sagawa2019distributionally, ahmed2021systematic} assume bias variables are specified in the training set, but also propose ways to infer groupings implicitly from the data. Similarly, but with a reinforcement learning setup, \cite{wang2020mitigating} propose an adaptive margin loss function which computes ideal margins between groups.

In contrast, implicit methods do not assume any prior knowledge of bias variables in the data. One such category of implicit methods leverages the observation that networks themselves can sometimes identify when they have learned spurious features as they are more easily learned, but, relying on them still results in misclassifications for a minority. Learning from Failure (LfF)~\cite{nam2020learning}, Learning with Biased Committee (LWBC)~\cite{kim2022learning}, and approaches by Bahng et al.~\cite{bahng2020learning} and Sanh et al.~\cite{sanh2020learning} exploit the failures of one or more normal or bias-amplifying networks to inform a de-biased network. These methods attempt to minimise the effect of bias-aligned training samples, some by actually removing samples from the training set shown to the target network.

Other methods explore implicit versions of up-weighting or re-sampling, by discovering sparse areas of the feature space~\cite{amini2019uncovering} or dynamically identifying samples more likely to be bias-conflicting~\cite{stone2022epistemic}. Du et al.~\cite{du2021fairness} de-bias the classification head of the network by training on neutralized representations of training data. This requires knowing when bias variables appear in the training samples (or at least being able to predict using some other model) in order to adjust the representations.

Another category of implicit methods change the loss function to reward learning a fairer model; Xu et al.~\cite{xu2021consistent} implement this via a false positive rate penalty loss, and Pezeshki et al.~\cite{pezeshki2021gradient} through Spectral Decoupling (SD), an adjusted L2 loss encouraging feature decoupling and discouraging learning features at the expense of learning another. SD allows the learning of simple correlations but not unilaterally, resulting in the preservation of minority core features.

Recently, Shrestha et al.~\cite{shrestha2022occamnets} open a promising new direction of work, leveraging architectural inductive biases with an adaptable architecture (OccamNets) which allows the network to favor simpler solutions when needed. The resulting model prefers focusing on smaller visual regions for predictions yet does not constrain all samples to rely on the same complexity of hypotheses, as optimal ``exit" locations in the architecture are learned per sample.

Our proposed implicit method shares intuition and motivation with many of these approaches, as we discuss further in Section~\ref{subsec: observations}.

\section{Methodology}


\subsection{Premise}

For a Bayesian neural network with parameters $\boldsymbol{\theta}$, posterior $p(\boldsymbol{\theta}~\mid D, \boldsymbol{x})$, class-labelled training data \(D = (X, Y)\), and test sample $\boldsymbol{x}_i$, the predictive posterior distribution for a given target class $\boldsymbol{y}_i$ is as follows:

\begin{equation}
    p(\boldsymbol{y}_i \mid D, \boldsymbol{x}_i) = \int_{}^{} p(\boldsymbol{y}_i \mid \boldsymbol{\theta}, \boldsymbol{x}_i) p(\boldsymbol{\theta} \mid D) d\boldsymbol{\theta}
\label{eq:posterior}
\end{equation}

While this itself is intractable, stochastic gradient MCMC (SG-MCMC~\cite{welling2011bayesian}) is a tractable variation of the gold standard Markov chain Monte Carlo (MCMC) algorithm, and uses the stochastic gradient over mini-batches with an additional Gaussian noise bound instead of the gradient over the whole training distribution. Furthermore, for faster convergence and a more thorough exploration of the multimodal distributions prevalent in deep neural networks, we adopt the cyclical learning rate schedule introduced in~\cite{zhang2019cyclical} known as cyclical SG-MCMC, or cSG-MCMC. Larger learning step phases provide a warm restart to the subsequent smaller steps constituting the sampling phase.

The final estimated posterior of the Bayesian network consists of \(\boldsymbol{\Theta} = \{\boldsymbol{\theta_1}, ... \boldsymbol{\theta_M}\}\), \(M\) moments sampled from the posterior over all trainable parameters, taken during the sampling phases of each learning cycle. With functional model \(\boldsymbol{\Phi}\) representing the neural network, the approximate predictive mean $\boldsymbol{\mu}_i$ and output class prediction $\boldsymbol{\hat{y}}_i$ for sample \(\boldsymbol{x}_i\) are:

\begin{equation}
    \boldsymbol{\mu}_i \approx \frac{1}{M} \sum_{m=1}^{M} \boldsymbol{\Phi}_{\theta_m}(\boldsymbol{x}_i)
    \label{eq:predictive_mean}
\end{equation}
\begin{equation}
    \boldsymbol{\hat{y}}_i = \textup{argmax}\left ( \boldsymbol{\mu}_i \right )
    \label{eq:argmax_pred_mean}
\end{equation}

The epistemic uncertainty corresponding to this prediction is:

\begin{equation}
    \boldsymbol{\sigma}_i \approx \frac{1}{M} \sqrt{\sum_{m=1}^{M} \left(\boldsymbol{\Phi}_{\boldsymbol{\theta}_m}(\boldsymbol{x}_i) - \boldsymbol{\mu}_i \right)^2}
\label{eq:sigma}
\end{equation}

which for any given sample is an indicator of model confidence in its prediction based on the information given. We refer readers to~\cite{jospin2022hands} for a more in-depth presentation of Bayesian neural networks and their implementation.

\subsection{Bias and Uncertainties}

Bias-conflicting samples tend to have higher epistemic uncertainties than their bias-aligned counterparts~\cite{ghandeharioun2019characterizing, ali2021accounting, stone2022epistemic}. These epistemic uncertainties arise from the classification, not representation, portions of the network despite the learned representations being arguably ``biased representations"~\cite{du2021fairness}. We can briefly explore these two claims on a toy synthetic dataset which we call Biased Synbols. We choose Synbols~\cite{lacoste2020synbols} as the tool to build Biased Synbols as we can easily control the resolution and features in each generated image. The foreground of each $224$ x $224$ image is a character displayed in a font chosen randomly from a large selection of fonts, and the texture of the character is a random natural scene, cropped to fit the character mask. The background behind the character is a 2-color gradient. We introduce four types of bias as shown in Figure~\ref{fig:synbols}.

\begin{figure}[th]%
\begin{center}
\includegraphics[width=0.23\linewidth]{figures/s_black.png}
\includegraphics[width=0.23\linewidth]{figures/s_square.png}
\includegraphics[width=0.23\linewidth]{figures/s_blur.png}
\includegraphics[width=0.23\linewidth]{figures/s_gray.png}
\end{center}
\caption{One sample from class ``s" of our toy dataset Biased-Synbols with controllable bias variables \textit{(left to right)}: texture of letter, spurious square placed in random corner, resolution of whole image, and gray-scale.}
\label{fig:synbols}%
\end{figure}

\begin{figure}[]%
\begin{center}
\includegraphics[width=1.0\linewidth]{figures/unit0.jpg}
\includegraphics[width=1.0\linewidth]{figures/unit10.jpg}
\end{center}
\caption{The six training images for which kernel 0, convolutional layer 4 \textit{(top)} and kernel 10, convolution layer 2 \textit{(bottom)} from AlexNet most strongly activate. The first kernel has identified a core feature for class ``s", whereas the second corresponds to the spurious or bias feature rather than the character.}
\label{fig:units}%
\end{figure}

We generate a two letter (``s" and ``t") binary classification task with train/val/test splits of 20k/5k/5k and train Bayesian AlexNet on four versions of Biased Synbols, each with a different bias variable and \(p_{bias} = 0.95\) for the ``s" class and \(p_{bias} = 0.05\) for the ``t". In each case, the bias-conflicting samples have higher mean group uncertainties than the bias-aligned (a mean increase by 0.15, 0.16, 0.09, and 0.20 for the four variables shown in Figure~\ref{fig:synbols} respectively, for normalized uncertainties between 0.0-1.0). 

Next, we identify where the discrepancies in uncertainties arise from in the network. We find indiscernible difference in uncertainties by averaging the uncertainties across the features extracted directly after the final convolutional layer. To further confirm, we perform network dissection~\cite{bau2017network}. on Biased Synbols with the spurious square in a random corner. Network dissection measures the alignment between convolutional kernel activations and any concept, which can be defined by segmentation masks on the dataset. As the square bias variable is generally spatially distinct from the core feature, i.e. the character shape, we can dissect the network to locate which convolutional kernels are most strongly activated by the former (Figure~\ref{fig:units}). For every convolutional layer in AlexNet, we compute the intersection-over-union (IoU) of the square with the thresholded kernel activation heatmaps, and measure correlation between these IoUs and the kernel uncertainties. The Pearson correlations per layer are -0.01, -0.03, 0.13, -0.07, and 0.17, strongly indicating no clear correlation; thus, simply learning features associated with bias variables \textit{does not create discrepancies in uncertainties}.

These observations motivate our focus on the classification head and the uncertainties induced by learning a biased feature weighting in the classification layers of the network.

\subsection{Posterior Estimate Sharpening}

We propose a \textit{sharpening} procedure (Algorithm \ref{alg:sharpening_procedure}) which further fine-tunes the classification layers of each Monte-Carlo sample from the Bayesian posterior using a multi-component loss function. We freeze the representation portion of the network during this procedure because (1) we aim to learn re-weighting of learned features in the classification head only, (2) updating the representation network can change the feature representations, thus complicating the re-weighting goal, and (3) experimental evidence backs these intuitions, showing a drastic drop in both performance and model fairness when the whole network is fine-tuned. Thus, when we refer to posterior estimates for sharpening, we are referring to $\boldsymbol{\theta}_c$, where each posterior sample \(\boldsymbol{\theta}_m = \{\boldsymbol{\theta}_{r, m}, \boldsymbol{\theta}_{c, m}\}\) includes the parameters from the representation and classification portions of the network respectively. The validity of operating on $\boldsymbol{\theta}_c$ alone can be supported by literature on ``Bayesian last layer" networks~\cite{kristiadi2020being}.

Our novel loss, a \textit{sharpening loss}, penalizes the posterior of a Bayesian neural network for having a large variance. This can also be compared to artificially sharpening or tempering the posterior (see discussion in Section~\ref{subsec: observations}). Consider a regular negative log likelihood loss measuring, for a sample $\boldsymbol{x}_i$, the divergence between the true class and the predicted likelihood of each class. For one-hot encoded true target $\boldsymbol{y}_i$ and model prediction $\boldsymbol{\Phi}(\boldsymbol{x}_i)$, negative log likelihood loss computes the sum over all possible classes:

\begin{equation}
    L_{nll} = L(\boldsymbol{y}_i, \boldsymbol{\Phi}(\boldsymbol{x}_i)) = - \sum \boldsymbol{y}_i \cdot \textup{log}(\boldsymbol{\Phi}(\boldsymbol{x}_i)) 
    \label{eq:nll_loss}
\end{equation}

% Where \(\hat{y}_i\) is the output prediction from one single posterior sample \(\theta_i\), 
With our Bayesian posterior estimate, our proposed loss $L_s$ must be computed separately for each posterior sample $\boldsymbol{\theta}_m$. We replace the true target $\boldsymbol{y}_i$ with one-hot encoded $\boldsymbol{\hat{y}}_i$ (Equation~\ref{eq:argmax_pred_mean}) derived from the predictive mean, capturing the divergence between the prediction of \textit{a single posterior estimate} and the mean prediction of the whole posterior:

\begin{equation}
    L_s = L(\hat{\boldsymbol{y}_i}, \boldsymbol{\Phi}_{\theta_m}(\boldsymbol{x}_i)) = - \sum \boldsymbol{\hat{y}}_i \cdot \textup{log}(\boldsymbol{\Phi}_{\theta_m}(\boldsymbol{x}_i)) 
    \label{eq:nll_loss}
\end{equation}

This minimizes $\boldsymbol{\sigma}$ in Equation~\ref{eq:sigma}. Equivalently, $L_s$ encourages the classification layers of the network to learn a feature weighting which minimizes uncertainties for high uncertainty samples, by diminishing the importance of features contributing to high uncertainties. A bias-conflicting sample class pair $(\boldsymbol{x}_i, \boldsymbol{y}_i)$ may have high uncertainty because it does not contain bias feature $b$ present in the majority of class $\boldsymbol{y}_i$. One or more features $F$ capture the relationship between $b$ and $\boldsymbol{y}_i$ and the variance of the distributions over the weights of $F$ are responsible for the high class uncertainty for $\boldsymbol{x}_i$. The sharpening loss rewards down-weighting $F$, hence discouraging the $(b, \boldsymbol{y}_i)$ correlation.

Not all samples are equally important. The bias-conflicting sample with high uncertainty, for example, should have larger gradients which shift the distribution. In contrast, there is no need to adjust the weights for a bias-aligned sample with low uncertainty; to do so would strengthen the existing undesired correlation. To reflect this, we weight each sample by \(\boldsymbol{w}_i\) as a function of its epistemic uncertainty as proposed in~\cite{stone2022epistemic} and shown in Equation~\ref{eq:weighting_function} for a batch of size \(N\). The distribution is shifted by 1.0 and scaling constant \(\kappa\) controls the steepness of the function such that low uncertainty samples are never completely discounted, but only minimally shift the distribution.

\begin{equation}
\boldsymbol{w}_i = (1.0 + \boldsymbol{\sigma}_{i, y_i})^\kappa
\label{eq:weighting_function}
\end{equation}

Backpropagating with this loss sharpens the estimated posterior, and has the potential to also shift the predictive mean away from the true target. Thus, we find it is not beneficial to completely discard the original loss that directs the network towards true targets. 

\begin{algorithm}
\caption{Sharpening procedure on Monte-Carlo estimate of posterior \(\boldsymbol{\Theta} =\{\boldsymbol{\theta}_m, ... \boldsymbol{\theta}_M\}\)}
\label{alg:sharpening_procedure}
\begin{algorithmic}[1]
\Require Training data ${X, Y}$, neural network $\boldsymbol{\Phi}$, update step size $\epsilon$, posterior \(\boldsymbol{\Theta}\)
\For{each iteration of sharpening}
\State Given \(\boldsymbol{\Theta}\), compute $\boldsymbol{\mu}$ and $\boldsymbol{\sigma}$, for all $X$
    \For{each $\boldsymbol{\theta}_m = \{\boldsymbol{\theta}_{r, m}, \boldsymbol{\theta}_{c, m}\} \in \boldsymbol{\Theta}$}
        \For{each batch $\boldsymbol{x}_b$, $\boldsymbol{y}_b$ of size $|b|$}
            % \State $\hat{y}_{b, m} = $
            \State $\boldsymbol{w}_b = (1.0 + \boldsymbol{\sigma}_{b, y_b})^\kappa$
            \State $\boldsymbol{\hat{y}}_b = \textup{argmax}\left ( \boldsymbol{\mu}_b \right )$
            \State $L_{nll} = L(\boldsymbol{y}_b, \boldsymbol{\Phi}_{\theta_m}(\boldsymbol{x}_b), \boldsymbol{w}_b) / ~|b|$
            \State  $L_s = L(\boldsymbol{\hat{y}}_b, \boldsymbol{\Phi}_{\theta_m}(\boldsymbol{x}_b), \boldsymbol{w}_b) / ~|b|$
            \State $\textup{Update} \left[ \boldsymbol{\theta}_{c, m} \gets \boldsymbol{\theta}_{c, m} - \epsilon  \textup{PCGrad}(L_s, L_{nll}) \right ]$
        \EndFor
    \EndFor
\EndFor
\end{algorithmic}
\end{algorithm}

\begin{figure*}[!ht]%
\begin{center}
\includegraphics[width=0.9\linewidth]{figures/iccv_paper.pdf}
\end{center}
\caption{Datasets used for evaluation: Biased MNIST \textit{(left)}, COCO-on-Places \textit{(center)}, Biased Action Recognition (BAR) \textit{(right)}.}
\label{fig:datasets}%
\end{figure*}

As the two loss gradients may conflict, we use a form of gradient surgery used in multi-task learning called \textit{projecting conflicting gradients} (PCGrad)~\cite{yu2020gradient}, whereby the gradient of one loss is projected onto the normal plane of the other when the gradients have negative cosine similarly. For each sample in each mini-batch, gradients for losses \((L_s, L)\) are computed. If there is destructive interference, one gradient is selected at random and projected onto the normal plane of the other. In the case of constructive interference, both gradients are combined as usual. The cumulative update to the weights is then averaged over the batch. An ablation study showing experimental support for the use of gradient surgery is shown in Table~\ref{tab:ablation_loss}.

\subsection{Related Observations}
\label{subsec: observations}
 Sharpening posterior estimates allows the network to first learn all features unhindered, then implicitly adjusts the weighting of those features with respect to the final task. The motivation for this novel approach stems from two main observations. 
 
 \textbf{Addressing simplicity bias.} Firstly, deep neural networks trained with empirical risk minimization (ERM) prefer simple solutions, tending to rely on spurious correlations especially when they are easier to learn than the core features. This has been referred to as \textit{simplicity bias}~\cite{shah2020pitfalls}. Every bias mitigation method attempts to inform the model when it should rely on more complex predictive features versus when a simpler explanation is preferable. In literature, this has been addressed by either (1) selectively modifying the learned representations post-training~\cite{santurkar2021editing, du2021fairness}, or (2) guiding the network to selectively learn un-biased representations to begin with~\cite{nam2020learning, kim2022learning, pezeshki2021gradient, shrestha2022occamnets, stone2022epistemic}.

Posterior estimate sharpening falls in the first category, an implicitly-guided re-weighting of the classification head. It constrains the complexity of the model, not unlike OccamNets~\cite{shrestha2022occamnets} which does so architecturally. Furthermore, sharpening is less prone to overfitting to minority samples as unlike~\cite{stone2022epistemic, chawla2002smote, kamiran2012data, cui2019class}, it is not directly equivalent to up-weighting samples, focusing rather on feature weighting. The sharpening procedure addresses simplicity bias by relying on uncertainty estimates to inform when simpler solutions are not preferable, because they result in high uncertainties for bias-conflicting samples.

\textbf{Adjusting Bayesian posteriors.} Secondly, artificially tampering with posteriors of Bayesian deep neural networks is not a foreign concept. As presented by Wenzel et al.~\cite{wenzel2020good}, the true Bayesian posteriors of deep neural networks are rarely used in practice. Rather, \textit{tempered} or \textit{cold posteriors} are widely found to perform better. Tempered posteriors are equivalent to over-counting the data by a factor of \(1/T\) for temperature scalar \(T\) and re-scaling the prior to \(p(\boldsymbol{\theta})^{1/T}\). In contrast, the true Bayes posterior corresponding to \(T = 1\) usually gives sub-optimal performance.

We note, however, that our proposed sharpening procedure is applied only to the \textit{estimated} posterior derived from MC sampling, not to the true Bayes posterior. The MC samples provide a finite, numerical handle on model inference. Furthermore, running the sharpening procedure with a sampling size \(M\)  of up to 20 indicates that increasing the MC sample size for a more accurate posterior estimate has negligible impact on the de-biasing.

\section{Experiments}
\subsection{Datasets}

Following the benchmarks established by Shreshtha et al. in ~\cite{shrestha2022occamnets}, we experiment on three datasets for visual bias mitigation shown in Figure~\ref{fig:datasets}. \textbf{Biased MNIST}~\cite{shrestha2022investigation} introduces color, texture, scale, and contextual biases into a 5 x 5 grid of cells, one or more of which contain one of the target MNIST 10 digits. With \textit{\(p_{bias} = 0.95\)}, each digit co-occurs 95\% with each bias source. The test set is unbiased with a 50K/10K/10K train/val/test split. \textbf{COCO-on-Places}~\cite{ahmed2021systematic} spuriously correlates Places backgrounds with COCO object foregrounds. Most zebras, for example, appear in front of a desert, and most horses in front of a snowy landscape. Three test sets are provided: 1) biased backgrounds, matching the correlations present in the training set, 2) unseen backgrounds, including seen objects placed on new backgrounds unseen during training, and 3) seen but unbiased backgrounds. COCO-on-Places has a 7200/900/900 train/val/test split and 9 classes. Finally, \textbf{Biased Action Recognition (BAR)}~\cite{nam2020learning} includes manually selected real-world images of action-background pairs, where the backgrounds are correlated to the action; e.g., most characters in the ``climbing" class are climbing snowy mountains, and more rarely climb other objects such as buildings, structures, etc. BAR is the smallest dataset with a 1641/300/654 train/val/test split~\footnote{We randomly split the published training set into training and validation sets as no split is consistently used in literature.} and 6 target classes. 

\begin{figure}[]%
\begin{center}
\includegraphics[width=0.37\linewidth]{figures/bmnist_epis.png}
\includegraphics[width=0.5\linewidth]{figures/coco_epis.png}
\includegraphics[width=1.0\linewidth]{figures/bar_epis.png}
\end{center}
\caption{Mean uncertainties for subgroups of each test dataset, showing bias-induced discrepancies, but also increased uncertainties arising from a variety of other sources, some of which may be unintentional bias present in the data.}
\label{fig:group_epis}%
\end{figure}

\subsection{Group Epistemic Uncertainties}

Unlike Biased-Synbols where the bias variable and its effect on model uncertainty can be observed and controlled in isolation, our benchmark datasets are significantly more complex. Bias-inducing features co-occur, but not always on the same samples (Biased MNIST). Classes can be severely imbalanced (BAR contains only 163 fishing images compared to its largest class of 520 images). Furthermore, both COCO and BAR exhibit intra-class diversity; the ``racing" category includes bikes, motorcycles, runners, and a wide variety of vehicles, and COCO objects occur with different angles and coloring. This diversity also introduces bias into the dataset, albeit unintentional and unaccounted for. The mean group sample uncertainties extracted from Bayesian ResNet18 models trained on each dataset reflect this as well, as shown in Figure~\ref{fig:group_epis}. 

While this entangled experimental setup makes evaluation of the impact of de-biasing methods more difficult, the scenario is certainly realistic given the state of real-world visual datasets. Instead of explicitly disentangling the bias sources, sharpening trusts that the posterior adequately expresses its own shortages in information. Thus, sharpening considers class imbalance, intra-class diversity, unintentional biases, and known biases simultaneously.

\subsection{Training and Inference}

 Optimal parameters for cSG-MCMC are determined via grid search for initial step sizes \{0.01 0.05, 0.1, 0.5\}, cycle lengths \{150, 200, 300, 400, 500, 600, 700\}, and Gaussian noise control parameter $\alpha \in$ \{0.1, 0.3, 0.5, 0.7\}. We fixed each schedule to 2 cycles and 3 moments sampled per sampling phase. Batch sizes for the baseline Bayesian models were fixed at 128 for training, and 64 for the sharpening procedure due to memory constraints (32 for BAR due to larger image size). Validation accuracy was used to determine stopping points and optimal hyper-parameters for both baseline Bayesian models and the sharpening procedure. Training took place on several IBM Power 9 dual-CPU nodes with 4 NVIDIA V100 GPUs. We refer readers to the Appendix for the optimal parameters for all experiments.

 Following methodology reported in~\cite{shrestha2022occamnets}, we reduce the kernel size of the first convolutional layer of ResNet18 from 7 to 3 for COCO due to the small image size (64 x 64). We also initialize the priors for the BAR Bayesian ResNet18 models using the weights from a deterministic ResNet18 trained on an ImageNet subset of 100 classes.

The sharpening procedure requires keeping a handle on each Monte Carlo sample from $p(\boldsymbol{\theta} \mid D, \boldsymbol{x})$, and back-propagating on each of these samples for every iteration. Thus, the procedure has time complexity of $O(M \cdot N \cdot K)$ for $M$ posterior samples, $K$ multiplication operations as required for the network architecture, and $N$ iterations, versus $O(N \cdot K)$ for a deterministic network.


\begin{table}[h]
\centering
\footnotesize
\caption{Unbiased test set accuracies comparing Bay-ResNet against current debiasing methods. \textbf{\underline{First}}, \textbf{second} and \underline{third} best results are formatted.}
\label{tab:overall}
% \addtolength\tabcolsep{1pt}
\begin{tabular}{lccc}
\hline
 
Architecture+Method &
  \begin{tabular}[c]{@{}l@{}}Biased MNIST\end{tabular}  &
  \begin{tabular}[c]{@{}l@{}}COCO\end{tabular} &
  \begin{tabular}[c]{@{}l@{}}BAR \end{tabular} \\ \hline
  \multicolumn{4}{c}{\textit{Explicit method results}} \\
ResNet+UpWt & 37.7 $\pm$ 1.6 & 35.2 $\pm$ 0.4 & 51.1 $\pm$ 1.9   \\
ResNet+gDRO~\cite{sagawa2019distributionally}  & 19.2 $\pm$ 0.9 & 35.3 $\pm 0.1$ & 38.7 $\pm$2.2  \\
ResNet+PGI~\cite{ahmed2021systematic}  & \textbf{48.6} $\pm$ 0.7 & \textbf{42.7} $\pm$ 0.6 & \textbf{53.6} $\pm$0.9 \\ 
\hline
\multicolumn{4}{c}{\textit{Implicit method results}} \\
ResNet+ERM  & 36.8~$\pm$0.7 & \underline{35.6}~$\pm$ 1.0 & 51.3~$\pm$1.9 \\
ResNet+SD~\cite{pezeshki2021gradient} & 37.1 $\pm$ 1.0 & 35.4 $\pm$ 0.5  & 51.3~$\pm$2.3  \\ 
OccamResNet~\cite{shrestha2022occamnets} & \textbf{\underline{65.0}} $\pm 1.0$ & \textbf{\underline{43.4}} $\pm$ 1.0 & \underline{52.6}~ $\pm$1.9 \\
% \multicolumn{4}{c}{\textit{BayResNet results}} \\
% Bay-ResNet & 32.0~$\pm$ 1.2 & 33.0~$\pm$ 0.2 & \underline{52.7}~$\pm$2.6 \\
BayResNet+EpiWt \cite{stone2022epistemic} & 34.4~$\pm 1.1$ & 34.4~$\pm$ 0.8 & 52.1~$\pm$1.5 \\
\hdashline
BayResNet+Sharpen & \underline{38.6}~$\pm$0.6 & 34.9~$\pm$0.6 & \textbf{\underline{53.9}}~$\pm$0.7 \\
\hline
\end{tabular}%
\end{table}

\begin{table*}[h]
\centering
\footnotesize
\caption{Overall and per-class accuracies on BAR, showing large discrepancies in performance across methods and classes.}
\label{tab:BAR-all-methods}
%\addtolength\tabcolsep{1pt}
\resizebox{1.0\textwidth}{!}{%
\begin{tabular}{llllllll}
 \hline
Methods &
  \begin{tabular}[c]{@{}c@{}}Overall\end{tabular} &
  \begin{tabular}[c]{@{}c@{}}Climbing\end{tabular} &
  \begin{tabular}[c]{@{}c@{}}Diving\end{tabular} &
  \begin{tabular}[c]{@{}c@{}}Fishing\end{tabular} &
  \begin{tabular}[c]{@{}c@{}}Pole\\Vaulting\end{tabular} & \begin{tabular}[c]{@{}c@{}}Racing\end{tabular} & \begin{tabular}[c]{@{}c@{}}Throwing\end{tabular}
   \\ \hline
\multicolumn{8}{c}{\textit{Explicit method results}} \\
ResNet+UpWt & 51.1~$\pm1.9$ & 61.7~$\pm13.2$ & \textbf{43.9}~$\pm5.8$ & 42.3~$\pm8.3$ & 52.3~$\pm7.4$ & 67.9~$\pm6.7$ & 28.2~$\pm12.8$ \\
ResNet+gDRO~\cite{sagawa2019distributionally} & 38.7~$\pm2.2$ & 49.5~$\pm8.5$ & 40.3~$\pm8.4$ & 44.0~$\pm10.4$ & 39.9~$\pm7.1$ & 41.7~$\pm4.0$ & 13.5~$\pm5.9$ \\
ResNet+PGI~\cite{ahmed2021systematic} & \underline{53.6}~$\pm0.9$ & 61.2~$\pm10.4$ & 38.4~$\pm4.1$ & 42.9~$\pm8.4$ & \textbf{\underline{73.3}}~$\pm3.7$ & 68.9~$\pm5.9$ & 23.5~$\pm1.9$ \\ 
OccamResNet+UpWt & 52.2~$\pm1.4$ & 57.9~$\pm1.8$ & 35.7~$\pm7.5$ & 51.8~$\pm11.2$ & 64.3~$\pm8.8$ & 71.8~$\pm3.8$ & 27.4~$\pm3.5$ \\
OccamResNet+gDRO~\cite{sagawa2019distributionally} & 52.9~$\pm0.8$ & 51.2~$\pm9.6$ & \underline{42.8}~$\pm8.2$ & 52.3~$\pm5.1$ & 63.5~$\pm7.3$ & 74.2~$\pm5.2$ & 25.3~$\pm4.5$ \\
OccamResNet+PGI~\cite{ahmed2021systematic} & \textbf{\underline{55.9}}~$\pm0.7$ & 64.2~$\pm5.1$ & \textbf{\underline{52.3}}~$\pm6.4$ & 51.4~$\pm8.3$ & \underline{64.4}~$\pm4.1$ & 70.9~$\pm8.1$ & 18.6~$\pm6.8$ \\ 
\hline
\multicolumn{8}{c}{\textit{Implicit method results}} \\
ResNet+ERM & 51.3~$\pm1.9$ & \underline{69.5}~$\pm7.5$  & 29.2~$\pm1.8$ & 39.9~$\pm16.2$ & 55.5~$\pm6.4$ & 75.6~$\pm5.6$ & \textbf{31.8}~$\pm4.3$ \\
ResNet+SD~\cite{pezeshki2021gradient} & 51.3~$\pm2.3$ & 62.1~$\pm7.5$ & 35.8~$\pm2.0$ & 51.2~$\pm6.4$ & 62.4~$\pm9.2$ & 71.6~$\pm10.0$ & 18.5~$\pm6.7$ \\
OccamResNet~\cite{sagawa2019distributionally} & 52.6~$\pm1.9$ & 59.3~$\pm3.8$ & 42.3~$\pm7.5$ & 44.6~$\pm14.9$ & 60.5~$\pm8.6$ & 74.1~$\pm7.2$ & 22.1~$\pm3.9$  \\
OccamResNet+SD~\cite{pezeshki2021gradient} & 52.3~$\pm2.4$ & 56.4~$\pm6.8$ & 34.3~$\pm5.8$ & \textbf{55.4}~$\pm7.4$ & \textbf{69.1}~$\pm4.9$ & 72.9~$\pm4.2$ & 21.8~$\pm2.1$ \\
% \multicolumn{8}{c}{\textit{BayResNet results}} \\
BayResNet & 52.7~$\pm2.6$ & \textbf{71.1}~$\pm2.7$  & 28.3~$\pm10.3$ & \underline{54.8}~$\pm8.2$ & 54.2~$\pm3.3$ & \textbf{78.0}~$\pm3.9$ & \textbf{31.8}~$\pm2.6$ \\
BayResNet+EpiWt~\cite{stone2022epistemic} & 52.1~$\pm1.6$  & 67.9~$\pm2.2$  & 27.9~$\pm5.1$  & 51.6~$\pm5.4$ & 59.8~$\pm3.5$  & \underline{76.2}~$\pm0.4$  & \underline{30.0}~$\pm2.9$ \\
\hdashline
BayResNet+Sharpen & \textbf{53.9}~$\pm0.7$ & \textbf{\underline{72.1}}~$\pm2.0$  & 32.1~$\pm3.2$ & \textbf{\underline{59.5}}~$\pm2.3$ & 52.7~$\pm2.7$ & \textbf{\underline{79.6}}~$\pm0.7$ & \textbf{\underline{32.6}}~$\pm2.9$ \\ \hline
\end{tabular}%
}
\end{table*}

\section{Results and Discussion}

\begin{table}[h]
\centering
\footnotesize
\caption{Accuracies on all three test splits of COCO-on-Places: biased backgrounds (Bgs), unseen backgrounds, and seen but unbiased backgrounds.} 
\label{tab:coco-on-places}
\addtolength\tabcolsep{1pt}
\begin{tabular}{lccc}
 \hline
Architecture+Method &
  \begin{tabular}[c]{@{}c@{}}Biased\\ Bgs\end{tabular} &
  \begin{tabular}[c]{@{}c@{}}Unseen \\ Bgs\end{tabular} &
  \begin{tabular}[c]{@{}c@{}}Seen, but \\ Unbiased \\ Bgs\end{tabular} \\ \hline
\multicolumn{4}{c}{\textit{Explicit method results}} \\
ResNet+PGI~\cite{ahmed2021systematic} & 77.5 $\pm$0.6 & 52.8 $\pm$0.7 & \underline{42.7}~$\pm$0.6 \\ 
OccamResNet+PGI~\cite{ahmed2021systematic} & 82.8~$\pm$0.6 & \textbf{55.3}~$\pm$1.3 & \textbf{\underline{43.6}}~$\pm$0.6 \\
\hline
\multicolumn{4}{c}{\textit{Implicit method results}} \\
ResNet+ERM  & \underline{84.9}~$\pm$ 0.5 & \underline{53.2}~$\pm$0.7 & 35.6~$\pm$1.0  \\
OccamResNet~\cite{shrestha2022occamnets} & 84.0~$\pm$1.0 & \textbf{\underline{55.8}}~$\pm$1.2 & \textbf{43.4}~$\pm$1.0  \\
% \multicolumn{4}{c}{\textit{BayResNet}-18 results}  \\
BayResNet & 84.3~$\pm$0.4 & 49.7~$\pm$1.3 & 33.0~$\pm$0.2 \\
BayResNet+EpiWt~\cite{stone2022epistemic} & \textbf{\underline{85.8}}~$\pm$0.1 & 50.3~$\pm$1.1 & 34.4~$\pm$0.8 \\
\hdashline
BayResNet+Sharpen & \textbf{85.6}~$\pm$0.6 & 51.2~$\pm$0.2 & 34.9~$\pm$0.6 \\ \hline
\end{tabular}%
\end{table}

\begin{table*}[]
\centering
\footnotesize
\caption{Accuracies on majority (maj)/minority (min) groups and bias-induced accuracy gaps for each bias variable in BiasedMNIST ($p_{bias}=0.95$). Smallest accuracy gaps for each bias variable are in \textbf{bold}.}
\label{tab:biased-mnist}
\addtolength\tabcolsep{0.9pt}
% \resizebox{\textwidth}{!}{
\begin{tabular}{lccccccccccccc} \hline
\multicolumn{1}{l}{\begin{tabular}[c]{@{}l@{}} Architecture+Method \end{tabular}} &
  \multicolumn{1}{l}{} &
  \multicolumn{2}{c}{\begin{tabular}[c]{@{}c@{}}Digit \\ Scale\end{tabular}} &
  \multicolumn{2}{c}{\begin{tabular}[c]{@{}c@{}}Digit\\ Color\end{tabular}} &
  \multicolumn{2}{c}{\begin{tabular}[c]{@{}c@{}}Texture\end{tabular}} &
  \multicolumn{2}{c}{\begin{tabular}[c]{@{}c@{}}Texture \\Color\end{tabular}} &
  \multicolumn{2}{c}{\begin{tabular}[c]{@{}c@{}}Letter\end{tabular}} &
  \multicolumn{2}{c}{\begin{tabular}[c]{@{}c@{}}Letter \\ Color\end{tabular}} \\
\multicolumn{1}{l}{} &
  \multicolumn{1}{l}{\begin{tabular}[c]{@{}l@{}}Test \\ Acc.\end{tabular}} &
  \multicolumn{2}{c}{maj/min~($\Delta$)} &
  \multicolumn{2}{c}{maj/min~($\Delta$)} &
  \multicolumn{2}{c}{maj/min~($\Delta$)} &
  \multicolumn{2}{c}{maj/min~($\Delta$)} &
  \multicolumn{2}{c}{maj/min~($\Delta$)} &
  \multicolumn{2}{c}{maj/min~($\Delta$)}
  \\ \hline
 \multicolumn{1}{l}{ResNet+ERM} &
  \multicolumn{1}{l}{36.8} &
  \multicolumn{2}{c}{\textbf{87.2/31.3 (55.9)}} &
  \multicolumn{2}{c}{\textbf{78.5/32.1 (46.4)}} &
  \multicolumn{2}{c}{76.1/32.4 (43.7)} &
  \multicolumn{2}{c}{41.9/36.3 (5.6)} &
  \multicolumn{2}{c}{46.7/35.7 (11.0)} &
  \multicolumn{2}{c}{45.7/35.9 (9.8)} \\
   \multicolumn{1}{l}{BayResNet+Sharpen} &
  \multicolumn{1}{l}{38.6} &
  \multicolumn{2}{c}{89.9/32.2 (57.7)} &
  \multicolumn{2}{c}{82.6/32.9 (49.7)} &
  \multicolumn{2}{c}{\textbf{61.3/35.3 (26.0)}} &
  \multicolumn{2}{c}{\textbf{40.0/37.8 (2.2)}} &
  \multicolumn{2}{c}{\textbf{44.2/37.3 (6.9)}} &
  \multicolumn{2}{c}{\textbf{45.2/37.2 (8.0)}} \\
  \hline 
\end{tabular}
% }
\end{table*}

\begin{table}[]
\centering
\footnotesize
\caption{Ablation study on sharpening procedure loss functions and their performance on the most difficult test sets for each dataset. Validation accuracy was used for optimization in each case.}
\label{tab:ablation_loss}
\addtolength\tabcolsep{2.5pt}
\begin{tabular}{lccc}
 \hline
Sharpening Loss &
  \begin{tabular}[c]{@{}l@{}}Biased MNIST\end{tabular} &
  \begin{tabular}[c]{@{}l@{}}COCO\end{tabular} &
  \begin{tabular}[c]{@{}l@{}}BAR \end{tabular} \\ \hline
\(L_s\) & 34.8~$\pm$ 2.1 & 33.3~$\pm$ 1.4  & 53.5~$\pm$1.0   \\
\(L + k \cdot L_s\) & 33.4~$\pm$0.2 & 32.3~$\pm$ 0.3  & 53.2~$\pm$0.4  \\
\(\textup{PCGrad}(L_s, L)\) & \textbf{38.6}~$\pm$0.5 & \textbf{34.9}~$\pm$1.2 & \textbf{53.9}~$\pm$0.7 \\
\hline
\end{tabular}%
\end{table}

For a fair comparison against prior reported results on these datasets, we use a ResNet18 throughout the experiments. We compare against several explicit and implicit mitigation methods, separated in each table: UpWt, gDRO, and PGI for explicit approaches, and the baseline ERM, SD and OccamNet for implicits. We also include EpiWt, another implicit method, due to its similarity to our approach in leveraging the epistemic uncertainties of a Bayesian neural network. We consider two baselines: (1) empirical risk minimization (ERM) of a deterministic ResNet18, and (2) a Bayesian ResNet18, both trained with normal cross entropy loss and no bias mitigation. The sharpening procedure gives competitive results on BAR, and comparable performance on Biased MNIST (Table~\ref{tab:overall}). 

We observe poorer performance on the COCO dataset. Posterior estimate sharpening always increases model fairness when compared to its baseline Bayesian starting point. As demonstrated in Table~\ref{tab:coco-on-places}, the Bayesian ResNet struggles with both the out-of-distribution and unbiased background test sets, with differences of -3.5\% and -2.6\% respectively compared to the deterministic ERM baseline. Building on the BayResNet, the sharpening thus starts with a \textit{more unfair model} than methods based on a deterministic ERM model, giving it a distinct disadvantage. Assuming the discrepancy in performance is somewhat due to the stochastic nature of the network, BayResNet+EpiWt would also have a disadvantage but does better on the biased test set than BayResNet+Sharpen; nonetheless, as is consistent across datasets, BayResNet+Sharpen still does better than BayResNet+EpiWt on the two unbiased test sets.

Regardless, this highlights a weakness of the sharpening approach. We consider the vast topic of comparing the predictive performances of deterministic and Bayesian neural networks out of the scope of this paper, but acknowledge that Bayesian neural networks can struggle to match deterministic benchmarks, which directly affects the sharpening method. We weigh these sacrifices in performance against the added benefit of quantified prediction uncertainty estimates, which deterministic models fail to communicate altogether.

BAR is a counterexample, where the Bayesian baseline outperforms or does at least as well as the deterministic one across class groups. The most realistic and complex dataset of the three, BAR is neither texturally simple like Biased MNIST nor synthetically created and low-resolution like COCO-on-Places. The posterior estimate sharpening method performs well on BAR (Table~\ref{tab:BAR-all-methods}), especially considering the large discrepancy in performances across classes and methods. Notably, while various methods do well in two or three classes, BayResNet+Sharpen has good performance across the majority of classes (5/7 classes). For this dataset, perhaps because of its ability to regularize despite the small dataset size and substantial intra-class diversity, the baseline BayResNet provides a strong starting point for the sharpening method.

Table~\ref{tab:biased-mnist} shows the majority/minority group accuracies and bias-induced accuracy gap for the six bias variables in Biased MNIST. Compared to the baseline ERM, the sharpened model decreases the accuracy gaps between majority and minority groups across the dataset for 4 out of 6 of the biases (texture, texture color, letter, and letter color). While BayResNet+Sharpen results on Biased MNIST are not as competitive, results demonstrate that the sharpening objective aims at increasing model fairness rather than simply rewarding accuracy gains on the majority groups.

We consider the impact of PCGrad on the multi-task loss in the ablation study shown in Table~\ref{tab:ablation_loss}. For weighted loss $L + k \cdot L_s$, scalar weight $k$ is chosen using a grid search in range \(k = 2\) to \(k = 10\) with optimal \(k\) being in range 3 to 5 for all datasets with negligible impact for \(k < 3\) and declining accuracy (biased and unbiased sets) for values \(k > 5\); this loss simply adds the two losses together regardless of conflict, with a fixed weighting term across all samples.

% \subsection{Uncertainties}

% Maybe here (or in supplementary material) show graphs of group average uncertainty trends under different loss types - show steady decrease under $L_s$, no change under $L$ alone, and varying under PCGrad.
% Also for supplementary material: present GradCAM samples on minority groups showing effect of sharpening on attention on toy Biased Synbols dataset.

Finally, uncertainty distributions across training sets do not collapse under PCGrad sharpening, so sample uncertainty estimates extracted at inference time may still be useful for triage or other purposes in high-risk scenarios.

\section{Conclusion}


The visual datasets heavily relied on by today's deep neural networks are complex, feature-dense, and full of unidentified biases and spurious correlations. While no single bias mitigation method has thus far been unilaterally successful, a wide variety of ideas in recent literature make promising advances in both explicit and implicit mitigation.

Our posterior estimate sharpening method adds to this body of work by implicitly exploiting the relationship between minority samples and epistemic uncertainties. We have demonstrated competitive performance on one benchmark, and comparable results on another. Further exploration is required to better understand when and why Bayesian neural networks struggle with respect to deterministic networks within the context of bias mitigation, and why the performances of mitigation methods vary across datasets. This perhaps calls for a better disentangling of features and correlations within training and test datasets used for evaluation, which we leave for future consideration.

\section{Acknowledgements}

The first author is a recipient of an Ezra Rabin Scholarship. Training was carried out on Bede, a facility of the N8 Centre of Excellence in Computationally Intensive Research (N8 CIR)~\footnote{https://n8cir.org.uk/bede/}. We would like to acknowledge the respective authors for their publicly available code for cSG-MCMC~\footnote{https://github.com/ruqizhang/csgmcmc}, network dissection~\footnote{https://github.com/davidbau/dissect}, PCGrad~\cite{Pytorch-PCGrad}, and OccamNets~\footnote{https://github.com/erobic/occam-nets-v1} from which we have modified code for our work. We thank Robik Shrestha for kindly providing the checkpoint for the ResNet18 trained on the ImageNet-100 subset, which was used as the prior for the BAR models. Finally, special thanks to Jose Sosa and Mohammed Alghamdi from the School of Computing's Computer Vision Group for critical feedback and many great discussions.

{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\end{document}
