{
    "arxiv_id": "2303.11909",
    "paper_title": "The Multiscale Surface Vision Transformer",
    "authors": [
        "Simon Dahan",
        "Logan Z. J. Williams",
        "Daniel Rueckert",
        "Emma C. Robinson"
    ],
    "submission_date": "2023-03-21",
    "revised_dates": [
        "2024-06-04"
    ],
    "latest_version": 2,
    "categories": [
        "eess.IV",
        "cs.CV",
        "q-bio.NC"
    ],
    "abstract": "Surface meshes are a favoured domain for representing structural and functional information on the human cortex, but their complex topology and geometry pose significant challenges for deep learning analysis. While Transformers have excelled as domain-agnostic architectures for sequence-to-sequence learning, the quadratic cost of the self-attention operation remains an obstacle for many dense prediction tasks. Inspired by some of the latest advances in hierarchical modelling with vision transformers, we introduce the Multiscale Surface Vision Transformer (MS-SiT) as a backbone architecture for surface deep learning. The self-attention mechanism is applied within local-mesh-windows to allow for high-resolution sampling of the underlying data, while a shifted-window strategy improves the sharing of information between windows. Neighbouring patches are successively merged, allowing the MS-SiT to learn hierarchical representations suitable for any prediction task. Results demonstrate that the MS-SiT outperforms existing surface deep learning methods for neonatal phenotyping prediction tasks using the Developing Human Connectome Project (dHCP) dataset. Furthermore, building the MS-SiT backbone into a U-shaped architecture for surface segmentation demonstrates competitive results on cortical parcellation using the UK Biobank (UKB) and manually-annotated MindBoggle datasets. Code and trained models are publicly available at https://github.com/metrics-lab/surface-vision-transformers.",
    "pdf_urls": [
        "http://arxiv.org/pdf/2303.11909v1",
        "http://arxiv.org/pdf/2303.11909v2"
    ],
    "publication_venue": "Accepted for publication at MIDL 2024, 17 pages, 6 figures"
}