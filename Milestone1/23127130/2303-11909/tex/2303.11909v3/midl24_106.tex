\documentclass{midl} % Include author names

% The following packages will be automatically loaded:
% jmlr, amsmath, amssymb, natbib, graphicx, url, algorithm2e
% ifoddpage, relsize and probably more
% make sure they are installed with your latex distribution

\usepackage{mwe} % to get dummy images
\jmlryear{2024}
\jmlrworkshop{Full Paper -- MIDL 2024}
\jmlrvolume{106}
\editors{Accepted for publication at MIDL 2024}

%%%%%%%%%%%%%%%%%%%%%%%%

\usepackage{graphicx}
\usepackage{url} 
\usepackage{array}
\usepackage{multirow}%\usepackage{natbib}
%\bibliographystyle{plainnat}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{multirow}
\usepackage{pifont}
\newcommand{\xmark}{\ding{55}}%
\newcommand{\cmark}{\ding{51}}%
\usepackage{diagbox}
\usepackage{tablefootnote}
\newcolumntype{P}[1]{>{\centering\arraybackslash}p{#1}}
\usepackage{xcolor} % Add the xcolor package


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\title[MS-SiT]{The Multiscale Surface Vision Transformer}

 % Use \Name{Author Name} to specify the name.
 % If the surname contains spaces, enclose the surname
 % in braces, e.g. \Name{John {Smith Jones}} similarly
 % if the name has a "von" part, e.g \Name{Jane {de Winter}}.
 % If the first letter in the forenames is a diacritic
 % enclose the diacritic in braces, e.g. \Name{{\'E}louise Smith}

 % Two authors with the same address
 % \midlauthor{\Name{Author Name1} \Email{abc@sample.edu}\and
 %  \Name{Author Name2} \Email{xyz@sample.edu}\\
 %  \addr Address}

 % Three or more authors with the same address:
 % \midlauthor{\Name{Author Name1} \Email{an1@sample.edu}\\
 %  \Name{Author Name2} \Email{an2@sample.edu}\\
 %  \Name{Author Name3} \Email{an3@sample.edu}\\
 %  \addr Address}


% Authors with different addresses:
% \midlauthor{\Name{Author Name1} \Email{abc@sample.edu}\\
% \addr Address 1
% \AND
% \Name{Author Name2} \Email{xyz@sample.edu}\\
% \addr Address 2
% }

%\footnotetext[1]{Contributed equally}

% More complicate cases, e.g. with dual affiliations and joint authorship
\midlauthor{\Name{Simon Dahan\nametag{$^{1}$}} \Email{simon.dahan@kcl.ac.uk}\\
\Name{Logan Z. J. Williams\nametag{$^{1,2}$}}  \Email{logan.williams@kcl.ac.uk}\\ 
\Name{Daniel Rueckert\nametag{$^{3}$}}\Email{daniel.rueckert@tum.de}\\
\Name{Emma C. Robinson\nametag{$^{1,2}$}} \Email{emma.robinson@kcl.ac.uk}\\
\addr $^{1}$ Department of Biomedical Engineering \& Imaging Science, King’s College London\\
\addr $^{2}$ Centre for the Developing Brain, King’s College London\\
\addr $^{3}$ Institute for AI in Medicine, Technical University of Munich}


\begin{document}

\maketitle

\begin{abstract}
Surface meshes are a favoured domain for representing structural and functional information on the human cortex, but their complex topology and geometry pose significant challenges for deep learning analysis. While Transformers have excelled as domain-agnostic architectures for sequence-to-sequence learning, the quadratic cost of the self-attention operation remains an obstacle for many dense prediction tasks. Inspired by some of the latest advances in hierarchical modelling with vision transformers, we introduce the Multiscale Surface Vision Transformer (MS-SiT) as a backbone architecture for surface deep learning. The self-attention mechanism is applied within local-mesh-windows to allow for high-resolution sampling of the underlying data, while a shifted-window strategy improves the sharing of information between windows. Neighbouring patches are successively merged, allowing the MS-SiT to learn hierarchical representations suitable for any prediction task. Results demonstrate that the MS-SiT outperforms existing surface deep learning methods for neonatal phenotyping prediction tasks using the Developing Human Connectome Project (dHCP) dataset. Furthermore, building the MS-SiT backbone into a U-shaped architecture for surface segmentation demonstrates competitive results on cortical parcellation using the UK Biobank (UKB) and manually-annotated MindBoggle datasets. Code and trained models are publicly available at \url{https://github.com/metrics-lab/surface-vision-transformers}.
\end{abstract}

\begin{keywords}
Vision Transformers, Cortical Imaging, Geometric Deep Learning, Segmentation, Neurodevelopment
\end{keywords}

\section{Introduction}
In recent years, there has been an increasing interest in using attention-based learning methodologies  in the medical imaging community, with the Vision Transformer (ViT) \cite{dosovitskiy2020} emerging as a particularly promising alternative to convolutional methods. The ViT circumvents the need for convolutions by translating image analysis to a sequence-to-sequence learning problem, using self-attention mechanisms to improve the modelling of long-range dependencies. This has led to significant improvements in many medical imaging tasks, where global context is crucial, such as tumour or multi-organ segmentation \cite{Y.Tang2021,J.Yuanfeng2021, A.Hatamizadeh2022}. At the same time, there has been a growing enthusiasm for adapting attention-based mechanisms to irregular geometries where the translation of the convolution operation is not trivial, but the representation of the data as sequences can be straightforward, for instance for protein modelling \cite{K.Atz2021, J.Jumper2021, M.Baek2021} or functional connectomes \cite{B.Kim2021}. Similarly, vision transformers (ViTs) have been recently translated to the study of cortical surfaces \cite{Dahan2022}, by re-framing the problem of surface analysis on sphericalised meshes as a sequence-to-sequence learning task and by doing so improving the modelling of long-range dependencies in cortical surfaces. Transformer models have also emerged as a promising tool for modelling various cognitive processes, such as language and speech \cite{J.Millet2022, Defossez2023Decoding}, vision \cite{tang2023brain}, and spatial encoding in the hippocampus \cite{J.Whittington2021}. 

Despite promising results on high-level prediction tasks, one of the main limitations of the ViT remains the computational cost of the global self-attention operation, which scales quadratically with sequence length. This limits the ability of the ViT to capture fine-grained details and to be used directly for dense prediction tasks.  Various strategies have been developed to overcome this limitation, including restricting the computation of self-attention to local windows \cite{F.Haoqi2021,liu2021swin} or implementing linear approximations \cite{S.Wang2020, X.Yunyang2021}. Among these, the hierarchical architecture of the Swin Transformer \cite{liu2021swin} has emerged as a particularly favoured candidate. This implements windowed local self-attention, alongside a shifted window strategy that allows cross-window connections. Neighbouring patch tokens are progressively merged across the network, producing a hierarchical representation of image features. This hierarchical strategy has shown to improve performance over the global-attention approach of the standard ViT, and has already found applications within the medical imaging domain \cite{A.Hatamizadeh2022}. \citet{Cheng2022SphericalTransformer} attempted to adapt such windowed local attention to the study of cortical meshes. However, attention windows were defined as the vertices forming  the hexagonal patches of a low-resolution grid, but not the patch features. This restricts the feature extraction with self-attention to a small number of vertices on the mesh and greatly limits the local feature extraction capabilities of the model.

In this paper, we therefore introduce the Multiscale Surface Vision Transformer (MS-SiT) as a novel backbone architecture for surface deep learning. The MS-SiT takes inspiration from the Swin Transformers model and extends the Surface Vision Transformers (SiT) \cite{Dahan2022} to a hierarchical version that can serve for any high-level or dense prediction task on sphericalised meshes. First, the MS-SiT introduces a local-attention operation between surface patches and within local attention-windows defined by the subdivisions of a high-resolution sampling grid. This allows for the modelling of fine-grained details of cortical features (with sequences of up to 20,480 patches). Moreover, to preserve the modelling of long-range dependencies between distant regions of the input surface, the MS-SiT adapts the shifted local-attention approach, introduced in \cite{liu2021swin}, by shifting the sampling grid across the input surface. This allows propagation of information between neighbouring attention-windows, achieving global attention at a reduced computational cost; however, it is challenging to implement due to the irregular spacing and sampling of vertices on native surface meshes. We evaluate our approach on neonatal phenotype prediction tasks derived from the Developing Human Connectome Project (dHCP), as well as on cortical parcellation for both UK Biobank (UKB) and manually-annotated MindBoggle datasets. Our proposed MS-SiT architecture strongly surpasses existing surface deep learning methods for predictions of cortical phenotypes and achieves competitive performance on cortical parcellation tasks, highlighting its potential as a holistic deep learning backbone and a powerful tool for clinical applications.


\section{Methods}


\paragraph{Backbone} The proposed MS-SiT adapts the Swin Transformer architecture \cite{liu2021swin} to the case of cortical surface analysis, as illustrated in Figure \ref{fig:swin_model}. Here, input data $X \in \mathbb{R}^{|V_6| \times C}$ ($C$ channels) is represented on a 6th-order icospheric (ico6) tessellation: $I_6=(V_6,F_6)$, with $|V_6|=40962$ vertices and $|F_6|=40962$ faces. This data is first partitioned into a sequence of $|F_5|=20480$ non-overlapping triangular patches: $T_5=\{t^1_5,t^2_5,..t_5^{|F_5|}\}$ (with $t^i_5 \subset V_6, |t^i_5|=|t_5|=6$), by patching the data with ico5: $I_{5}=(V_5, F_5), |V_5|=10242, |F_5|= 20480$ (Figure \ref{fig:swin_model} A.2). Imaging features for each patch are then concatenated across channels, and flattened to produce an initial sequence: $ X^{0} = \left [ X^{0}_1, ..., X^{0}_{|F_5|} \right] \in \mathbb{R}^{|F_5|\times (C|t_5|)}$ (Figure  \ref{fig:swin_model}A.3). Trainable positional embeddings, LayerNorm (LN) and a dropout layer are then applied, before passing it to the MS-SiT encoder, organised into $l=\{1,2,3,4\}$ levels.%  (Figure \ref{fig:swin_model} A.4).

At each level of the encoder, a linear layer projects the input sequence $X^{l}$ to a $2^{(l-1)}\times D$-dimensional embedding space: $X_{emb}^{l} \in \mathbb{R}^{|F_{6-l}|\times 2^{(l-1)}D}$. Local multi-head self-attention blocks (local-MHSA), described in section \ref{section:mhsa}, are then applied, outputting a transformed sequence of the same resolution ($X_{MHSA}^{l} \in \mathbb{R}^{|F_{6-l}|\times 2^{(l-1)}D}$). This is subsequently downsampled through a patch merging layer, which follows the regular downsampling of the icosphere, to merge clusters of 4 neighbouring triangles together (Figure \ref{fig:swin_model}B), generating output: $X_{out}^{l} \in \mathbb{R}^{|F_{6-l-1}|\times 2^{(l+1)}D}$. 

This process is repeated across several layers, with the spatial resolution of patches progressively downsampled from $I_5 \rightarrow I_4 \rightarrow I_3 \rightarrow I_2$, but the channel dimension doubling each time. In doing so, the MS-SiT architecture produces a hierarchical representation of patch features, with respectively $|F_5|=20480$, $|F_4|=5120$, $|F_3|=1280$, and $|F_2|=320$ patches. In the last level, the patch merging layer is omitted (see Figure \ref{fig:swin_model}) and the sequence of patches is averaged into a single token, and input to a final linear layer, for classification or regression  (Figure \ref{fig:swin_model}A.5). Inspired by previous work \cite{cao2021swinunet}, the segmentation pipeline employs a UNet-like architecture, with skip-connections between encoder and decoder layers, and patch partition instead of patch merging applied during decoding. An illustration of the pipeline is provided in Figure \ref{fig:swin_model_segmentation}, Appendix \ref{appendix:segmentation-pipeline}.
 
\begin{figure}[t!]
    \centering
    \includegraphics[scale=0.3]{data/ms_sit_2.jpeg}
    \caption{[A] MS-SiT pipeline. The input cortical surface is resampled from native resolution (1) to an ico6 input mesh and partitioned (2). The sequence is then flattened (3) and passed to the MS-SiT encoder layers (4). The head (5) can be adapted for classification or regression tasks. [B] illustrates the patch merging operation (here from $I_4$ to $I_3$ grid). High-resolution patches are grouped by 4 to form patches of lower-resolution sampling grid [C] A Local-MHSA block is composed of two attention blocks: \textbf{W}indow-MHSA and \textbf{S}hifted \textbf{W}indow-MHSA.}
    \label{fig:swin_model}
\end{figure}


\paragraph{Local Multi-Head Self-Attention blocks }
\label{section:mhsa}
are defined similarly to ViT blocks \cite{dosovitskiy2020}:  as successive multi-head self-attention (MHSA) and feed-forward (FFN) layers, with LayerNorm (LN) and residual layers in between (Figure \ref{fig:swin_model}C). Here, a \textbf{W}indow-MHSA (\textbf{W-MHSA}) replaces the global MHSA of standard vision transformers, applying self-attention between patches within non-overlapping local mesh-windows. To provide the model with sufficient contextual information, this attention window is defined by an icosahedral tessellation three levels down from the resolution used to represent the feature sequence. This means that at level $l$, while the sequence is represented by $I_{6-l}$, the attention windows correspond to the non-overlapping faces $F_{6-(l+3)}$ defined by $I_{6-(l+3)}$. For example, at level 1 the features are input at ico5, and local attention is calculated between the subset of 64 triangular patches that overlap with each face of ico2 ($F_2$), see Figure \ref{fig:swin_model}.B.1. Only in the last layer, is attention not restricted to local windows but applied globally to the $I_2$ grid, allowing for global sharing of information across the entire sequence. More details of the parameterisation of window attention grids is provided in the Appendix \ref{appendix:segmentation-pipeline}, Table \ref{tab:mssit-grid-resolution}. This use of local self-attention significantly reduces the computational cost of attention at level $l$, from $\mathcal{O}(|F_{6-l}|^2)$ to $\mathcal{O}(w_{l}|F_{6-l}|)$ with $w_{l} << |F_{6-l}|$.

\paragraph{Self-Attention with Shifted Windows}
Cross-window connections are introduced thro-\\ugh \textbf{S}hifted \textbf{W}indow MHSA (\textbf{SW-MHSA}) modules, to improve the modelling power of the local self-attention operations. These alternate with the W-MHSA, and are implemented by shifting all the patches in the sequence $I_{6-l}$, at level $l$ by $w_s$ positions, where $w_s$ is a fraction of the window size $w_l$ (typically $w_l=64$). In this way, a fraction of the patches of each attention window now falls within an adjacent window (see Figure  \ref{fig:shifted_attention}). This preserves the cost of applying self-attention in a windowed fashion, whilst increasing the models representational power by sharing information between non-overlapping attention windows.

The W-MHSA and SW-MHSA implementation can be summarised as follows: 
\begin{equation}
\begin{aligned}
\hat{\mathbf{X}}^{{l}}  & = \emph{W-MSA}(\mathbf{X}_{emb}^{l})+ \mathbf{X}_{emb}^{l}\\
 \mathbf{Z}^{l}& = \emph{FFN}(\hat{\mathbf{X}}^{{l}} ) + \hat{\mathbf{X}}^{{l}} \\
\hat{\mathbf{Z}}^{{l}}  & = \emph{SW-MSA}( \mathbf{Z}^{l}) +  \mathbf{Z}^{l}\\
 \mathbf{X}_{MHSA}^{l} & = \emph{FFN}(\hat{\mathbf{Z}}^{{l}} ) + \hat{\mathbf{Z}}^{{l}} \\
\end{aligned}
\label{eq:transformer}
\end{equation}
Here $\mathbf{X}_{emb}^{l}$ and $\mathbf{X}_{MHSA}^{l}$ correspond to input and output sequences of the local-MHSA block at level $l$. Residual connections are referred to by the $+$ symbol. 



\paragraph{Training details}
Augmentation strategies were introduced to improve regularisation and increase transformation invariance. This included implementing random rotational transforms, where the degree of rotation about each axis was randomly sampled in the range $\in [-30^{\circ}, +30^{\circ}]$ (for the regression tasks) and $\in [-15^{\circ}, +15^{\circ}]$ (for the segmentation tasks). In addition, elastic deformations were simulated by randomly displacing the vertices of a coarse ico2 grid to a maximum of 1/8th of the distance between neighbouring points (to enforce diffeomorphisms \cite{Fawaz2021}). These deformations were interpolated to the high-resolution grid of the image domain, online, during training. The effect of tuning the parameters of the SW-MHSA modules is presented in Table \ref{table:ablation_shifted_window} and reveals that the best results are obtained while shifting half of the patches.

\section{Experiments \& Results}

All experiments were run on a single RTX 3090 24GB GPU. The AdamW optimiser \cite{I.Loshchilov2017} with Cosine Decay scheduler was used as the default optimisation scheme, more details about optimisation and hyper-parameters tuning in Appendix \ref{appendix:hparams}. A combination of Dice Loss and CrossEntropyLoss was used for the segmentation tasks and MSE loss was used for the regression tasks. Surface data augmentation was randomly applied with a probability of $80\%$. If selected, one random transformation is applied: either rotation ($50\%$) or non-linear warping ($50\%$). For all regression tasks, a custom balancing sampling strategy was applied to address the imbalance of the data distribution.

\subsection{Phenotyping predictions on dHCP data}

\paragraph{Data} from the dHCP comes from the publicly available third release\footnote{ \url{http://www.developingconnectome.org}.} \cite{edwards2022developing} and consists of cortical surface meshes and metrics (sulcal depth, curvature, cortical thickness and T1w/T2w myelination) derived from T1- and T2-weighted magnetic resonance images (MRI), using the dHCP structural pipeline, described by \cite{A.Makropoulos2018} and references therein \cite{MKuklisova-Murgasova2012, A.Schuh2017, E.Hughes2017, L.Cordero-Grande2018,A.Makropoulos2018}. In total 580 scans were used from 419 term neonates (born after $37$ weeks gestation) and 111 preterm  neonates (born prior to $37$ weeks gestation). 95 preterm neonates were scanned twice, once shortly after birth, and once at term-equivalent age. 

\paragraph{Tasks and experimental set up:} Phenotype regression was benchmarked on two tasks:  prediction of postmenstrual age (PMA) at scan, and gestational age (GA) at birth. Here, PMA was seen as a model of `healthy' neurodevelopment, since training data was drawn from the scans of term-born neonates and preterm neonates' first scans: covering brain ages from 26.71 to 44.71 weeks PMA. By contrast, the objective of the GA model was to predict the degree of prematurity (birth age) from the participants' term-age scans, thus the model was trained on scans from term neonates and preterm neonates' second scans. Experiments were run on both registered (\textbf{\emph{template}} space) and unregistered (\emph{\textbf{native}} space) data to evaluate the generalisability of MS-SiT compared to surface convolutional approaches (Spherical UNet (SUNet) \cite{F.Zhao2019} and MoNet \cite{F.Monti2017}). The four aforementioned cortical metrics were used as input data. Training test and validation sets were allocated in the ratio of 423:53:54 examples (for PMA) and 411:51:52 (for GA) with a balanced distribution of examples from each age bin. 

\begin{table}[t!]
    %\hspace{-0.7cm}
    \setlength{\tabcolsep}{5.5pt}	  
    \begin{tabular}{l c c c c c c}
    \hline
    \rule{0pt}{3ex} 
    \textbf{Model} & \textbf{Aug.}  &  \multicolumn{1}{c}{\begin{tabular}[c]{@{}c@{}} \textbf{Shifted}\\ \textbf{Attention}\end{tabular}} &  \multicolumn{1}{c}{\begin{tabular}[c]{@{}c@{}} \textbf{PMA}\\ \textbf{Template}\end{tabular}} &  \multicolumn{1}{c}{\begin{tabular}[c]{@{}c@{}} \textbf{PMA}\\ \textbf{Native}\end{tabular}} &  \multicolumn{1}{c}{\begin{tabular}[c]{@{}c@{}} \textbf{GA}\\ \textbf{Template}\end{tabular}}  & \multicolumn{1}{c}{\begin{tabular}[c]{@{}c@{}} \textbf{GA}\\ \textbf{Native}\end{tabular}}\\
    \hline
    %\rule{0pt}{2ex}  
    %Spherical UNet  & \cmark &  n/a & 0.57$\pm$0.18 & 0.87$\pm$0.50 & \textbf{0.85$\pm$0.17} & 2.16$\pm$0.57 \\
    %MoNet  & \cmark &  n/a & 0.57$\pm$0.02 & \textbf{0.57$\pm$0.02} & 1.33$\pm$0.21 & 1.52$\pm$0.22 \\
    SUNet \tablefootnote{\cite{F.Zhao2019}} & \cmark &  n/a & 0.75$\pm$0.18 & 1.63$\pm$0.51 & 1.14$\pm$0.17 & 2.41$\pm$0.68 \\
    MoNet \tablefootnote{\cite{F.Monti2017}}  & \cmark &  n/a & 0.61$\pm$0.04 & 0.63$\pm$0.05 & 1.50$\pm$0.08 & 1.68$\pm$0.06 \\
    %\hline
    SiT-T (ico2)  & \cmark &  n/a & 0.58$\pm$0.02 & 0.66$\pm$0.01 & 1.04$\pm$0.04 & 1.28$\pm$0.06 \\
    SiT-T (ico3)  & \cmark &  n/a & 0.54$\pm$0.05 & 0.68$\pm$0.01 & 1.03$\pm$0.06 & 1.27$\pm$0.05\\
    SiT-T (ico4)  & \cmark &  n/a & 0.57$\pm$0.03& 0.83$\pm$0.04 & 1.41$\pm$0.09 & 1.49$\pm$0.10\\
    \hline
    %\rule{0pt}{2ex} 
    MS-SiT & \cmark & \xmark & \textbf{0.49$\pm$0.01}& \textbf{0.59$\pm$0.01}& 1.00$\pm$0.04& 1.17$\pm$0.04\\
    MS-SiT & \cmark & \cmark & \textbf{0.49$\pm$0.01}& \textbf{0.59$\pm$0.01} & \textbf{0.88$\pm$0.02} & \textbf{0.93$\pm$0.05}\\
    \hline
    \end{tabular}
    \rule{0pt}{2ex}
    \caption{Test results for the dHCP phenotype prediction tasks: PMA and GA. \textbf{M}ean \textbf{A}bsolute \textbf{E}rror (MAE) and std are averaged across three training runs for all experiments.}
    \label{table:results-dhcp}
\end{table}


\textbf{Results} from the phenotyping prediction experiments are presented in Table \ref{table:results-dhcp}, where the MS-SiT models were compared against several surface 
convolutional approaches and three versions of the Surface Vision Transformer (SiT) using different grid sampling resolutions. 
The MS-SiT model consistently outperformed all other models across all prediction tasks (PMA and GA) and data configurations (template and native). Specifically, on the PMA task,
the MS-SiT model outperformed other models by over 54\% compared to Spherical UNet \cite{F.Zhao2019}, 13\% to MoNet \cite{F.Monti2017}, and 12\% to the SiT (ico3)
(average over both data-configurations), achieving a prediction error of 0.49 MAE on template data, which is within the margin of error of age estimation in routine ultrasound
(typically, 5 to 7 days on average). On the GA task, the MS-SiT model achieved an even larger improvement with 49\%, 43\%, and 21\% reduction in MAE relative to Spherical UNet,
MoNet, and SiT (ico3), respectively. Importantly, the model demonstrated much greater transformation invariance, with only a 5\% drop in performance between the template and
native configurations, compared to 53\% for Spherical UNet, and 10\% for MoNet. Results also revealed a significant benefit to using the SW-MHSA with a 16\% improvement over
the vanilla version on GA predictions. 

\begin{figure}[t]
    \centering
    \includegraphics[scale=0.25]{data/seg.001.jpeg}
    \caption{Top: Inflated surface showing mean dice scores shown for each of the DKT regions, for both MoNet and the pre-trained MS-SIT. Bottom: Boxplots comparing regional parcellation results. Asterisks denote statistical significance for one-sided paired t-test (pink: MS-SiT $>$ MoNet; green MoNet $>$ MS-SIT; ****:  $p<0.0001$, **: $p<0.01$, *: $p<0.05$ ).}
    \label{fig:seg_dice}
\end{figure}

\subsection{Cortical parcellation on UKB \& MindBoggle}

\textbf{Data \& Tasks} Cortical segmentation was performed using 88 manually labelled adult brains from the MindBoggle-101 dataset
 \cite{A.Klein2012}\footnote{\url{https://MindBoggle.info/data}}, annotated into 31 regions using a modified version of the Desikan–Killiany (DK) 
 atlas \cite{Desikan2006AutomatedLabeling}, which delineates regions according to features of cortical shape. Surface files were processed with the Ciftify 
 pipeline \cite{E.Dickie2019}, which implements HCP-style post-processing including file conversion to GIFTI and CIFTI formats, and MSM Sulc alignment 
 \cite{E.Robinson2014,E.Robinson2018} \footnote{13 datasets failed due to missing files}. Separately, FreeSurfer annotation parcellations (based on a 
 standard version of the DK atlas with 35 regions) were available for 4000 UK Biobank subjects, processed according to \cite{F.Alfaro2018}. These were used for 
 pretraining. In both cases, datasets were split into 80\%/10\%/10\% sets. As the annotations characterise folding patters, we used shape-based cortical metrics as 
 input features: sulcal depth and curvature maps. 

\textbf{Results} are presented in Table \ref{table:results-mindboggle}. The MS-SiT was compared against three other gDL approaches for cortical segmentation: 
Adv-GCN, a graph-based method optimized for alignment invariance \cite{K.Gopinath2020}, SPHARM-net \cite{HaLyu2022SPHARMNet}, a spherical harmonic-based CNN method, 
and MoNet, which learns filters by fitting mixtures of Gaussians on the surface \cite{F.Monti2017}. MoNet achieved the best dice results overall, while MS-SiT superforms 
the two other gDL networks. However, a per region box plot (Fig \ref{fig:seg_dice}) of its performance relative to the MS-SIT shows this is largely driven by improvements 
to two large regions. Overall, MoNet and the MS-SIT differ significantly for 10 out of 32 regions, with MS-SIT outperforming MoNet for 6 of these. We also evaluated the performance of the MS-SiT model by providing it with more inductive biases, via transfer learning from a model first trained on the larger UKB dataset (achieving 0.94 dice for cortical parcellation), increasing slightly the final performance.
 
\begin{table}[t!]
    \centering
    \setlength{\tabcolsep}{25pt}
    \begin{tabular}{l  c c }
    \hline
    \rule{0pt}{3ex} 
    \textbf{Methods} & \textbf{Augmentation} & \textbf{Dice overlap} \\
    \hline
    \rule{0pt}{2ex}  
    Adv-GCN\tablefootnote{Run on a different train/test split, \cite{K.Gopinath2020}}   & n/a & 0.857$\pm$0.04\\
    \rule{0pt}{2ex}
    SPHARM-Net  \tablefootnote{\cite{HaLyu2022SPHARMNet}} & n/a & 0.887$\pm$0.06\\
    \rule{0pt}{2ex}
    MoNet \tablefootnote{\cite{F.Monti2017}} & \cmark & \textbf{0.910$\pm$0.01}\\
    \hline
    \rule{0pt}{2ex}
    MS-SiT & \cmark & 0.897$\pm$0.01\\
    \rule{0pt}{2ex}
    MS-SiT (UKB) & \cmark & 0.901$\pm$0.01\\
    \hline
    \end{tabular}
    \caption{Overall mean and standard deviation of Dice scores (across all regions).}
    \label{table:results-mindboggle}
\end{table}


\section{Discussion}

The novel MS-SiT network presents an efficient and reliable framework, based purely on self-attention, 
for any biomedical surface learning task where data can be represented on sphericalised meshes. Unlike 
existing convolution-based methodologies translated to study general manifolds, MS-SiT does not compromise 
on filter expressivity, computational complexity, or transformation  equivariance \cite{Fawaz2021}. Instead, 
with the use of local and shifted attention, the model is able to effectively reduce the computational cost 
of applying attention on larger sampling grids, relative to \cite{Dahan2022}, improving phenotyping performance, 
and performing competitively on cortical segmentation. Compared to convolution-based approach, the use of attention 
allows for the retrieval of attention maps, providing interpretable insights into the most attended cortical regions
 (Fig \ref{fig:attention}), and the methodology's robustness to transformations enables it to perform well on 
 both registered and native space data, removing the need for spatial normalisation using image registration. 

\clearpage  % Acknowledgements, references, and appendix do not count toward the page limit (if any)
% Acknowledgments---Will not appear in anonymized version
\midlacknowledgments{We would like to acknowledge funding from the EPSRC Centre for Doctoral Training in Smart Medical Imaging (EP/S022104/1).}


\bibliography{midl24_106}
\appendix
%\iffalse

\newpage
\section{Methods}
\label{appendix:segmentation-pipeline}

\subsection{Network architecture details}
The details of the MS-SiT architecture are provided in Table \ref{tab:mssit-grid-resolution}.

\begin{table}[h]
  \hspace{-0.3cm}
  \setlength{\tabcolsep}{6.0pt}
  \begin{tabular}{l  c  c  c  c }
  \hline
     \multicolumn{1}{c}{\begin{tabular}[c]{@{}c@{}} \textbf{MS-SiT} \\ \textbf{levels} \end{tabular}}  & \multicolumn{1}{c}{\begin{tabular}[c]{@{}c@{}} \textbf{Sequence} \\ \textbf{length} $|F_{6-l}|$ \end{tabular}} &\multicolumn{1}{c}{\begin{tabular}[c]{@{}c@{}}\textbf{Attention} \\ \textbf{windows} \end{tabular}} &\multicolumn{1}{c}{\begin{tabular}[c]{@{}c@{}} \textbf{Window size} \\  \textbf{$w_l$} \end{tabular}} & \textbf{Level architecture}
    %& MS-SiT small
    \\
    \hline
    %\addlinespace[0.2cm]
    $l=1$ & 20480 & 320  & 64 & \multicolumn{1}{c}{\begin{tabular}[c]{@{}c@{}} 
linear 96-d - LN \\ 
$\left[ w_{l} \text{ 64, dim 96, head 3} \right] \times 2$ \\ 
merging - LN 
\end{tabular}} \\
    \hline
    %\addlinespace[0.2cm]
    $l=2$ & 5120 & 80 & 64 &\multicolumn{1}{c}{\begin{tabular}[c]{@{}c@{}} 
    linear 192-d - LN \\ 
    $\left[ w_{l} \text{ 64, dim 192, head 6} \right] \times 2$ \\ 
    merging - LN 
    \end{tabular}}
    
    \\
    \hline
    %\addlinespace[0.2cm]
    $l=3$ & 1280 & 20 & 64  &\multicolumn{1}{c}{\begin{tabular}[c]{@{}c@{}} 
linear 384-d - LN \\ 
$\left[ w_{l} \text{ 64, dim 384, head 12} \right] \times 6$ \\ merging - LN \end{tabular}}
    \\
    \hline
    %\addlinespace[0.2cm]
    $l=4$ & 320 & 1 & 320 &\multicolumn{1}{c}{\begin{tabular}[c]{@{}c@{}} 
linear 768-d - LN \\ 
$\left[ w_{l} \text{ 320, dim 768, head 24} \right] \times 2$ \\
merging - LN 
\end{tabular}}

    \\
    %\addlinespace[0.2cm]
    \hline
  \end{tabular}
  \rule{0pt}{3ex}
  \caption{MS-SiT detailed architecture. The MS-SiT model adapts the Swin-T architecture \cite{liu2021swin} into  a 4-level network with $\{2,2,6,2 \}$ local-MHSA blocks and $\{3,6,12,24\}$ attention heads per level. As in \cite{liu2021swin}, the initial embedding dimension is $D=96$. Thus, the MS-SiT encoder has 27.5 M trainable parameters. 
  }
  \label{tab:mssit-grid-resolution}
\end{table}

\subsection{Positional Embeddings}

The MS-SiT model implements positional encodings in the form of 1D learnable weights, added to each token of the input sequence  $X^{0}$. This follows the implementations in \cite{dosovitskiy2020, Dahan2022}.

\subsection{Segmentation pipeline}

The MS-SiT architecture can be turned into a U-shape network for segmentation for segmentation tasks Figure \ref{fig:swin_model_segmentation}.
\begin{figure}[t!]
    \centering
     \includegraphics[scale=0.45]{data/seg_3.001.jpeg}
     \caption{MS-SiT segmentation pipeline. Input data is resampled and partitioned as in Figure \ref{fig:swin_model}. The  $l=\{1,2,3,4\}$ levels of the segmentation pipeline are similar to the MS-SiT encoder levels (Figure \ref{fig:swin_model}). The \emph{patch partition} layers reverse the patch merging procedure of the MS-SiT encoder, upsampling the  spatial resolution of patches from $I_2 \rightarrow I_3 \rightarrow I_4 \rightarrow I_5$. Skip connections between levels are used. Finally, a \emph{spherical resampling} layer resamples the final embeddings to an ico6 tessellation (40962 vertices), before the final segmentation prediction.}
     \label{fig:swin_model_segmentation}
\end{figure}


\subsection{Shifted-attention}

An illustration of the shifted-attention mechanism is presented in Figure \ref{fig:shifted_attention}.

%\begin{figure}[h!]
%    \centering
%    \includegraphics[scale=0.15]{data/ms_sit_shift.jpeg}
%    \caption{[1] W-MHSA applies self-attention within a local window (in white), defined by a regular icosahedral partitioning grid. [2] SW-MHSA shifts patches such that local attention is computed between patches originally in different local windows.}
%    \label{fig:shifted_attention}
%\end{figure}

\begin{figure}[h!]
    \centering
    \includegraphics[scale=0.20]{data/shift_new.001.jpeg}
    \caption{[1] W-MHSA applies self-attention within a local window, defined by a fixed regular icosahedral partitioning grid. Two local windows are show here, delimited by the yellow and blue colours here. [2] SW-MHSA shifts patches such that local attention is computed between patches originally in different local windows.}
    \label{fig:shifted_attention}
\end{figure}

\newpage
\section{Results}

\subsection{Attention weights}

Attention weights from the last layer of the MS-SiT can be extracted and visualised on the input surface, see Figure \ref{fig:attention}. They are compared to the attention weights extracted from a SiT model on the same GA prediction task. 

\begin{figure}[h!]
     \centering
     \includegraphics[scale=0.20]{data/attention.001.jpeg}
     \caption{Comparison of normalised attention maps from the last layers of a SiT model \cite{Fawaz2021}  (applying global attention for all layers) and an MS-SiT model, both trained for GA-template prediction. MS-SiT maps display highly specific attention patterns, compared to the SiT counterparts, focusing on characteristic landmarks of cortical development such as the sensorimotor cortex with low myelination in preterm (pink arrows) and high myelination in term (blue arrows).}
     \label{fig:attention}
\end{figure}

\subsection{Optimisation and hyperparameter search}
\label{appendix:hparams}

\subsubsection{Hyperparameter search for optimal shifting factor}
In table \ref{table:ablation_shifted_window}, we evaluate the impact of the shifting factor $w_{s}$ on the prediction performance.

\begin{table}[h!]
    \centering
    %\setlength{\tabcolsep}{7pt}
    \begin{tabular}{c  c  c  c}
    \hline
    \rule{0pt}{3ex} 
    \textbf{Shifted attention} & \textbf{Shift factor $w_s$}& \textbf{MAE}& \textbf{MAE (augmentation)} \\
    \hline
    \xmark & \xmark & 1.68 $\pm$ 0.13 & 1.24 $\pm$ 0.04\\
    \cmark & 1/2 & 1.55 $\pm$ 0.03 &  \textbf{1.16 $\pm$ 0.02}\\
    \cmark & 1/4 & \textbf{1.54 $\pm$ 0.08} & 1.20 $\pm$ 0.03\\
    %\cmark & 1/8 & 1.60 & \textbf{1.08}\\
    \cmark & 1/16 & 1.56 $\pm$ 0.09 & 1.26 $\pm$ 0.04\\
    \hline
    \end{tabular}
    \rule{0pt}{3ex}
    \caption{Hyper-parameter tuning of the shift factor $w_s$ in the \textbf{SW-MHSA} module. The use of $w_s \in \{\frac{1}{2},\frac{1}{4}, \frac{1}{16}\}$ is compared to no shift for MS-SiT models trained to predict GA from template-aligned dHCP data. Models were trained for 25k iterations ($\sim$ 50\% typical training runtime). We report MAE and std for the validation dataset, averaged over 3 runs are reported. Shifting the sequence of half the length of the attention windows, i.e. $w_s=\frac{1}{2}$, provides the best results overall and is used in all the following.}
    \label{table:ablation_shifted_window}
\end{table}

\subsubsection{Optimisation and scheduling}

The training strategy used for each task is summarised in \tableref{tab:hps-start}. Extensive experiments showed that AdamW \cite{kingma2017adam} with linear warm-up and cosine decay scheduler was the best optimisation strategy overall (\href{https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.CosineAnnealingLR.html}{PyTorch Library - CosineAnnealingLR} and \href{https://github.com/ildoonet/pytorch-gradual-warmup-lr}{GitHub - PyTorch Gradual Warmup}). This follows standard practices \cite{gotmare2018closer} and training results from similar transformer models \cite{liu2021swin}. Of note, SGD with momentum and small learning rate ($LR = 1e^{-5}$) also achieved good performances on the phenotype prediction tasks but could not converge on the cortical parcellation. Mean Square Error loss (MSE) was used to optimise models on the regression tasks and an unweighted combination of DiceLoss and CELoss (MONAI implementation \href{https://docs.monai.io/en/stable/losses.html#diceceloss}{MONAI - DiceCELoss}) is used for optimisation of the segmentation task. We used a batch size of 16 for the phenotyping prediction experiments, and a batch size of 1 for the segmentation experiment (as it led to better results). In Figure \ref{fig:training-losses}, we compare the training and validation losses between our MS-SiT methodology and the SiT methodology. 


\begin{table}[h]
  \footnotesize
  \centering
  \setlength{\tabcolsep}{4pt}
  \begin{tabular}{p{1.5cm} P{3.0cm} P{2.5cm} c c c}
    \hline
    \multicolumn{1}{c}{\textbf{Model}} & \multicolumn{1}{c}{\textbf{Task}} &\multicolumn{1}{c}{\textbf{Optimiser}} &\multicolumn{1}{c}{\textbf{Warm-up it}}   &\multicolumn{1}{c}{\textbf{Learning Rate}} & \multicolumn{1}{c}{\textbf{Training epochs}}\\
    \hline
    MS-SiT & PMA & AdamW/SGD & 1000 & $1e^{-5}$  & 1000 \\
    MS-SiT & GA & AdamW/SGD & 1000 &  $1e^{-5}$  &  1000 \\
    MS-SiT & Cortical Parcellation & AdamW & 100 &  $3e^{-4}$  & 200 \\
    \hline
  \end{tabular}
  \caption{Training strategies for all tasks. Overall, AdamW with linear warp-up and cosine decay is selected as the default optimisation startegy.}
  \label{tab:hps-start}
\end{table}


\begin{figure}[h!]
     \centering
     \includegraphics[scale=0.25]{data/losses.png}
     \caption{Comparison of training and validation losses between MS-SiT and SiT-tiny (ico2) models trained on PMA. Plots seem to indicate a faster convergence of the MS-SiT model.}
     \label{fig:training-losses}
\end{figure}


\end{document}
