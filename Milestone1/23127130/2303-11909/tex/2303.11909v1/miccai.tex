% This is samplepaper.tex, a sample chapter demonstrating the
% LLNCS macro package for Springer Computer Science proceedings;
% Version 2.20 of 2017/10/04
%
\documentclass[runningheads]{llncs}
%
\usepackage{graphicx}
\usepackage{appendix}
\usepackage{cite}


% additional imports
% additional imports
\usepackage{subfiles} 
\usepackage{booktabs}
% for check mark
\usepackage{pifont}
\newcommand{\xmark}{\ding{55}}%
\newcommand{\cmark}{\ding{51}}%
\usepackage{multirow}

\usepackage{xltabular}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{footnote}
\usepackage{tablefootnote}
\usepackage{float}
 \usepackage{hyperref} 
 \usepackage{tablefootnote}

 \let\labelitemi\labelitemii




% Used for displaying a sample figure. If possible, figure files should
% be included in EPS format.
%
% If you use the hyperref package, please uncomment the following line
% to display URLs in blue roman font according to Springer's eBook style:
% \renewcommand\UrlFont{\color{blue}\rmfamily}

\begin{document}
%
\title{The Multiscale Surface Vision Transformer}
%
%\titlerunning{Abbreviated paper title}
% If the paper title is too long for the running head, you can set
% an abbreviated paper title here
%
\author{Simon Dahan \inst{1,3} \and Abdulah Fawaz \inst{1} \and Mohamed A. Suliman \inst{1} \and Mariana da Silva \inst{1} \and Logan Z. J. Williams \inst{1,2}
Daniel Rueckert \inst{4}\and
Emma C. Robinson \inst{1,2}}
%
\authorrunning{Dahan et al.}
% First names are abbreviated in the running head.
% If there are more than two authors, 'et al.' is used.
%
\institute{Department of Biomedical Engineering, King's College London 
 \and Centre for the Developing Brain, King’s College London \and The Alan Turing Institute, London, United Kingdom  \and Department of Computing, Imperial College London 
 %\and Artificial Intelligence in Healthcare and Medicine, Technical University of Munich
 \\ \email{\{simon.dahan, emma.robinson\}@kcl.ac.uk}
}

%
\maketitle              % typeset the header of the contribution
%
\begin{abstract}
Surface meshes are a favoured domain for representing structural and functional information on the human cortex, but their complex topology and geometry pose significant challenges for deep learning analysis. While Transformers have excelled as domain-agnostic architectures for sequence-to-sequence learning, notably for structures where the translation of the convolution operation is non-trivial, the quadratic cost of the self-attention operation remains an obstacle for many dense prediction tasks. 
Inspired by some of the latest advances in hierarchical modelling with vision transformers, we introduce the Multiscale Surface Vision Transformer (MS-SiT) as a backbone architecture for surface deep learning. The self-attention mechanism is applied within local-mesh-windows to allow for high-resolution sampling of the underlying data, while a shifted-window strategy improves the sharing of information between windows. Neighbouring patches are successively merged, allowing the MS-SiT to learn hierarchical representations suitable for any prediction task. Results demonstrate that the MS-SiT outperforms existing surface deep learning methods for neonatal phenotyping prediction tasks using the Developing Human Connectome Project (dHCP) dataset. Furthermore, building the MS-SiT backbone into a U-shaped architecture for surface segmentation demonstrates competitive results on cortical parcellation using the UK Biobank (UKB) and manually-annotated MindBoggle datasets. Code and trained models are publicly available at \url{https://github.com/metrics-lab/surface-vision-transformers}.


\keywords{Deep Learning  \and Vision Transformers \and Neuroimaging }
\end{abstract}
%
%
%
\section{Introduction}

In recent years, there has been an increasing interest in using attention-based learning methodologies in the medical imaging community, with the Vision Transformer (ViT) \cite{dosovitskiy2020} emerging as a particularly promising alternative to convolutional methods. The ViT circumvents the need for convolutions by translating image analysis to a sequence-to-sequence learning problem, using self-attention mechanisms to improve the modelling of long-range dependencies. This has led to significant improvements in many medical imaging tasks, where global context is crucial, such as tumour or multi-organ segmentation \cite{Y.Tang2021,J.Yuanfeng2021, A.Hatamizadeh2022}. At the same time, there has been a growing enthusiasm for adapting attention-based mechanisms to irregular geometries where the translation of the convolution operation is not trivial \cite{Fawaz2021}, but the representation of the data as sequences can be straightforward. For instance, transformer-based methodologies have been used in the context of molecular representations for modelling protein structure and interaction \cite{K.Atz2021, J.Jumper2021, M.Baek2021}, or in the context of learning the spatio-temporal dynamics of functional connectomes \cite{B.Kim2021}.   
Moreover, transformer models have emerged as a promising tool for modelling various cognitive processes. In particular, they have successfully captured the representation of spatial encoding in the hippocampus \cite{J.Whittington2021}, as well as exhibiting brain-like representations of language and speech processing in transformer models trained on audiobooks \cite{C.Caucheteux2021, J.Millet2022}. This highlights the potential of transformer models to bridge the gap between neural and computational models of cognition.

Despite promising results on high-level prediction tasks, one of the main limitations of the ViT remains the computational cost of the global self-attention operation, which scales quadratically with sequence length. This issue is particularly pronounced for high-resolution medical data. Various strategies have been developed to overcome this limitation, including restricting the computation of self-attention to local windows \cite{F.Haoqi2021,Ze.Liu2021} or implementing linear approximations \cite{S.Wang2020, X.Yunyang2021}. Among these, the hierarchical architecture of the Swin Transformer \cite{Ze.Liu2021} has emerged as a particularly promising candidate. This implements windowed-self-attention, 
alongside a shifted window strategy that allows cross-window connections. Neighbouring patch tokens are progressively merged across the network, producing a hierarchical representation of image features. This hierarchical strategy has shown to improve performance over the global-attention approach of the standard ViT, and has already found applications within the medical imaging domain \cite{A.Hatamizadeh2022}. 

In this paper, we therefore introduce the Multiscale Surface Vision Transformer (MS-SiT) as a novel backbone architecture for surface deep learning. The MS-SiT extends the global attention-based approach of the Surface Vision Transformer (SiT) \cite{Dahan2022}, to a hierarchical version that can serve as a backbone for any high-level or dense prediction task on sphericalised meshes. 
We evaluate our approach on neonatal phenotype prediction tasks derived from the Developing Human Connectome Project (dHCP), as well as on cortical parcellation for both UK Biobank (UKB) and manually-annotated MindBoggle datasets. Key contributions are summarised as follows: 
\begin{itemize}
    \item We introduce the Multiscale Surface Vision Transformer (MS-SiT), a fully-attention-based framework for surface deep learning, implementing hierarchical feature aggregation on sphericalised meshes. 
    \item Our proposed MS-SiT architecture surpasses existing surface deep learning methods for predictions of cortical phenotypes and achieves competitive performance on cortical parcellation tasks, highlighting its potential as a powerful tool for clinical applications.
    \item Performance is further enhanced through the translation of the Shifted Window Multi-Head Self-Attention mechanism \cite{Ze.Liu2021} to genus-zero surface domains.
\end{itemize}

\section{Methods}

\subsection{Architecture overview}
% need to explain
% how the local attention works
% how the patch merging works 
% explain the complexity cost of the model compared to the SiT. 

\subsubsection{Backbone} The proposed MS-SiT adapts the Swin Transformer architecture\cite{Ze.Liu2021} to the case of cortical surface analysis, as illustrated in Figure \ref{fig:swin_model}. 
Here, input data $X \in \mathbb{R}^{|V_6| \times C}$ ($C$ channels) is represented on a 6th-order icospheric (ico6) tessellation: $I_6=(V_6,F_6)$, with $|V_6|=40962$ vertices and $|F_6|=40962$ faces. This data is first partitioned into a sequence of $|F_5|=20480$ non-overlapping triangular patches: $T_5=\{t^1_5,t^2_5,..t_5^{|F_5|}\}$ (with $t^i_5 \subset V_6, |t^i_5|=6$), by patching the data with ico5: $I_{5}=(V_5, F_5), |V_5|=10242, |F_5|= 20480$ (Figure \ref{fig:swin_model} A.2). Imaging features for each patch are then concatenated across channels, and flattened to produce an initial sequence: $ X^{0} = \left [ X^{0}_1, ..., X^{0}_{|F_5|} \right] \in \mathbb{R}^{|F_5|\times (|t_5|C)}$ (Figure  \ref{fig:swin_model}A.3). Positional embeddings, LayerNorm (LN) and a dropout layer are then applied, before passing it to the MS-SiT encoder, organised into $l=\{1,2,3,4\}$ levels.%  (Figure \ref{fig:swin_model} A.4).

At each level of the encoder, a linear layer projects the input sequence $X^{l}$ to a $2^{(l-1)}\times D$-dimensional embedding space: $X_{emb}^{l} \in \mathbb{R}^{|F_{6-l}|\times 2^{(l-1)}D}$. Local multi-head self-attention blocks (local-MHSA), described in detail below, are then applied, % learning the shared relationship between patch features, and 
outputting a transformed sequence of the same resolution ($X_{MHSA}^{l} \in \mathbb{R}^{|F_{6-l}|\times 2^{(l-1)}D}$). This is subsequently downsampled through a patch merging layer, which follows the regular downsampling of the icosphere, to merge clusters of 4 neighbouring triangles together (Figure \ref{fig:swin_model}B.), generating output: $X_{out}^{l} \in \mathbb{R}^{|F_{6-l-1}|\times 2^{(l+1)}D}$. %which similarly to convolution networks, has the overall effect of compressing the spatial resolution across levels, as the channel dimension is increased.

%which combines outputs from 4 neighbouring patches into a sequence of length $4lD$, according to the regular downsampling of the icosphere (Figure \ref{fig:swin_model}B.). Thus for the first level, this reduces the spatial resolution from $I_5$ to $I_4$, resulting in an input to layer 2 of shape $X^{2} \in \mathbb{R}^{|F_4|\times 4D}$

This process is repeated across several layers, with the spatial resolution of patches progressively downsampled from $I_5 \rightarrow I_4 \rightarrow I_3 \rightarrow I_2$, but the channel dimension doubling each time. In doing so, the MS-SiT architecture produces a hierarchical representation of patch features, with respectively $|F_5|=20480$, $|F_4|=5120$, $|F_3|=1280$, and $|F_2|=320$ patches. In the last MS-SiT level, the patch merging layer is omitted and the sequence of patches is averaged into a single token, and input to a final linear layer, for classification or regression  (Figure \ref{fig:swin_model}A.5). The segmentation pipeline employs a UNet-like architecture, with skip-connections between encoder and decoder layers, and patch partition instead of patch merging applied during decoding. An illustration of the segmentation architecture is provided in the Supplementary %\ref{section:segmentation-pipeline} 
(Figure S.1).
 
\begin{figure}[t!]
    \hspace{-0.1cm}
    \includegraphics[scale=0.23]{data/ms_sit_5.001.jpeg}
    \caption{[A] MS-SiT pipeline. The input cortical surface is resampled from native resolution to an ico6 input mesh (1) and partitioned, using an ico5 grid, into a sequence of 20480 non-overlapping
triangular patches (2). The sequence is then flattened (3) and passed to the MS-SiT encoder layers (4). At the end of the encoder, (5) the head can be adapted for classification or regression tasks. [B] An example of the patch merging process from $I_4$ to $I_3$ grid. [C] Two local-MHSA blocks applying successively with W-MHSA and SW-MHSA.}
    \label{fig:swin_model}
\end{figure}

\subsubsection{Local Multi-Head Self-Attention blocks } are defined similarly to ViT blocks \cite{dosovitskiy2020}:  as successive multi-head self-attention (MHSA) and feed-forward (FFN) layers, with LayerNorm (LN) and residual layers in between (Figure \ref{fig:swin_model}C). Here, a \textbf{W}indow-MHSA (\textbf{W-MHSA}) replaces the global MHSA of standard vision transformers, applying self-attention between patches within non-overlapping local mesh-windows. To provide the model with sufficient contextual information, this attention window is defined by an icosahedral tessellation two levels down from the resolution used to represent the feature sequence. This means that at level $l$, while the sequence is represented by $I_{6-l}$, the attention windows correspond to the non-overlapping faces $F_{6-(l+2)}$ defined by $I_{6-(l+2)}$. For example, at level 1 the features are input at ico5, and local attention is calculated between the subset of 64 triangular patches that overlap with each face of ico3 ($F_3$). 
%the attention windows correspond to the non-overlapping triangles $T_{k_{l+2}}$.
Only in the last layer, is attention not restricted to local windows but applied globally to the $I_2$ grid, allowing for global sharing of information across the entire sequence. More details of the parameterisation of window attention grids is provided in Supplementary Table S.1.
This use of local self-attention significantly reduces the computational cost of attention at level $l$, from $\mathcal{O}(|F_{6-l}|^2)$ to $\mathcal{O}(w_{l}|F_{6-l}|)$ with $w_{l} << |F_{6-l}|$.

\subsubsection{Self-Attention with Shifted Windows}
Cross-window connections are introduced through \textbf{S}hifted \textbf{W}indow MHSA (\textbf{SW-MHSA}) modules, in order to improve the modelling power of the local self-attention operations. These alternate with the W-MHSA, and are implemented by shifting all the patches in the sequence $I_{6-l}$, at level $l$, by $w_s$ positions, where $w_s$ is a fraction of the window size $w_l$ (typically $w_l=64$). In this way, a fraction of the patches of each attention window %$F^{i}$ 
now falls within an adjacent window. %$F^{j \in N_i}$. 
This preserves the cost of applying self-attention in a windowed fashion, whilst increasing the models representational power by sharing information between non-overlapping attention windows. An illustration of two successive local-MHSA blocks implementing W-MHSA and SW-MHSA is provided in Figure \ref{fig:swin_model}C. Implementation can be summarised as follows: 
\begin{equation}
\begin{aligned}
\hat{\mathbf{X}}^{{l}}  & = \emph{W-MSA}(\mathbf{X}_{emb}^{l})+ \mathbf{X}_{emb}^{l}\\
 \mathbf{Z}^{l}& = \emph{FFN}(\hat{\mathbf{X}}^{{l}} ) + \hat{\mathbf{X}}^{{l}} \\
\hat{\mathbf{Z}}^{{l}}  & = \emph{SW-MSA}( \mathbf{Z}^{l}) +  \mathbf{Z}^{l}\\
 \mathbf{X}_{MHSA}^{l} & = \emph{FFN}(\hat{\mathbf{Z}}^{{l}} ) + \hat{\mathbf{Z}}^{{l}} \\
\end{aligned}
\label{eq:transformer}
\end{equation}
Here $\mathbf{X}_{emb}^{l}$ and $\mathbf{X}_{MHSA}^{l}$ correspond to input and output sequences of the local-MHSA block at level $l$. Residual connections are referred to by the $+$ symbol. 
%\subsection{Training details}

\subsubsection{Training details}
Augmentation strategies were introduced to improve regularisation and increase transformation invariance. This included implementing random rotational transforms, where the degree of rotation about each axis was randomly sampled in the range $\in [-30^{\circ}, +30^{\circ}]$ (for the regression tasks) and $\in [-15^{\circ}, +15^{\circ}]$ (for the segmentation tasks). In addition, elastic deformations were simulated by randomly displacing the vertices of a coarse ico2 grid to a maximum of 1/8th of the distance between neighbouring points (to enforce diffeomorphisms \cite{Fawaz2021}). These deformations were interpolated to the high-resolution grid of the image domain, online, during training. The effect of tuning the parameters of the SW-MHSA modules is presented in the Supplementary Table S.2 and reveals that the best results are obtained while shifting half of the patches.

\section{Experiments \& Results}

All experiments were run on a single RTX 3090 24GB GPU. The AdamW optimiser \cite{I.Loshchilov2017} with Cosine Decay scheduler was used as the default optimisation scheme. A combination of Dice Loss and CrossEntropyLoss was used for the segmentation tasks and MSE loss was used for the regression tasks. Surface data augmentation was randomly applied with a probability of $80\%$. If selected, one random transformation is applied: rotation ($50\%$) or non-linear warping ($50\%$). For all regression tasks, a custom balancing sampling strategy was applied to address the imbalance of the data distribution.

\subsection{Phenotyping predictions on dHCP data}

\textbf{Data} from the dHCP comes from the publicly available third release\footnote{ \url{http://www.developingconnectome.org}.} \cite{edwards2022developing} and consists of cortical surface meshes and metrics (sulcal depth, curvature, cortical thickness and T1w/T2w myelination) derived from T1- and T2-weighted magnetic resonance images (MRI), using the dHCP structural pipeline, described by \cite{A.Makropoulos2018} and references therein \cite{MKuklisova-Murgasova2012, A.Schuh2017, E.Hughes2017, L.Cordero-Grande2018,A.Makropoulos2018}. In total 580 scans were used from 419 term neonates (born after $37$ weeks gestation) and 111 preterm  neonates (born prior to $37$ weeks gestation). 95 preterm neonates were scanned twice, once shortly after birth, and once at term-equivalent age. 

\textbf{Tasks and experimental set up:} Phenotype regression was benchmarked on two tasks:  prediction of postmenstrual age (PMA) at scan, and gestational age (GA) at birth.  Here, PMA was seen as a model of `healthy' neurodevelopment, since training data was drawn from the scans of term-born neonates and preterm neonates' first scans: covering brain ages from 26.71 to 44.71 weeks PMA. By contrast, the objective of the GA model was to predict the degree of prematurity (birth age) from the participants' term-age scans, thus the model was trained on scans from term neonates and preterm neonates' second scans. Experiments were run on both \textbf{\emph{template}}-aligned data and unregistered (\emph{\textbf{native}}) to evaluate the robustness of surface deep learning methods on non-aligned data and compared against surface convolutional approaches (Spherical UNet (SUNet) \cite{F.Zhao2019} and MoNet \cite{F.Monti2017}). Training test and validation sets were allocated in the ratio of 423:53:54 examples (for PMA) and 411:51:52 (for GA) with a balanced distribution of examples, within each partition, from each age bin. 


\begin{table}[b!]
    \setlength{\tabcolsep}{3.5pt}	  
    \begin{tabular}{l| c c c c c c}
    \hline
    \rule{0pt}{3ex} 
    \textbf{Model} & \textbf{Aug.}  &  \multicolumn{1}{c}{\begin{tabular}[c]{@{}c@{}} \textbf{Shifted}\\ \textbf{Attention}\end{tabular}} &  \multicolumn{1}{c}{\begin{tabular}[c]{@{}c@{}} \textbf{PMA}\\ \textbf{Template}\end{tabular}} &  \multicolumn{1}{c}{\begin{tabular}[c]{@{}c@{}} \textbf{PMA}\\ \textbf{Native}\end{tabular}} &  \multicolumn{1}{c}{\begin{tabular}[c]{@{}c@{}} \textbf{GA}\\ \textbf{Template}\end{tabular}}  & \multicolumn{1}{c}{\begin{tabular}[c]{@{}c@{}} \textbf{GA}\\ \textbf{Native}\end{tabular}}\\
    \hline
    %\rule{0pt}{2ex}  
    %Spherical UNet  & \cmark &  n/a & 0.57$\pm$0.18 & 0.87$\pm$0.50 & \textbf{0.85$\pm$0.17} & 2.16$\pm$0.57 \\
    %MoNet  & \cmark &  n/a & 0.57$\pm$0.02 & \textbf{0.57$\pm$0.02} & 1.33$\pm$0.21 & 1.52$\pm$0.22 \\
    SUNet \cite{F.Zhao2019} & \cmark &  n/a & 0.75$\pm$0.18 & 1.63$\pm$0.51 & 1.14$\pm$0.17 & 2.41$\pm$0.68 \\
    MoNet \cite{F.Monti2017}  & \cmark &  n/a & 0.61$\pm$0.04 & 0.63$\pm$0.05 & 1.50$\pm$0.08 & 1.68$\pm$0.06 \\
    \hline
    SiT-T (ico2)  & \cmark &  n/a & 0.58$\pm$0.02 & 0.66$\pm$0.01 & 1.04$\pm$0.04 & 1.28$\pm$0.06 \\
    SiT-T (ico3)  & \cmark &  n/a & 0.54$\pm$0.05 & 0.68$\pm$0.01 & 1.03$\pm$0.06 & 1.27$\pm$0.05\\
    SiT-T (ico4)  & \cmark &  n/a & 0.57$\pm$0.03& 0.83$\pm$0.04 & 1.41$\pm$0.09 & 1.49$\pm$0.10\\
    \hline
    %\rule{0pt}{2ex} 
    MS-SiT & \cmark & \xmark & \textbf{0.49$\pm$0.01}& \textbf{0.59$\pm$0.01}& 1.00$\pm$0.04& 1.17$\pm$0.04\\
    MS-SiT & \cmark & \cmark & \textbf{0.49$\pm$0.01}& \textbf{0.59$\pm$0.01} & \textbf{0.88$\pm$0.02} & \textbf{0.93$\pm$0.05}\\
    \hline
    \end{tabular}
    \rule{0pt}{2ex}
    \caption{Test results for the dHCP phenotype prediction tasks: PMA and GA. \textbf{M}ean \textbf{A}bsolute \textbf{E}rror (MAE) and std are averaged across three training runs for all experiments.}
    \label{table:results-dhcp}
\end{table}


\textbf{Results} from the phenotyping prediction experiments are presented in Table \ref{table:results-dhcp}, where the MS-SiT models were compared against several surface convolutional approaches and three versions of the Surface Vision Transformer (SiT) using different grid sampling resolutions. 
 The MS-SiT model consistently outperformed all other models across all prediction tasks (PMA and GA) and data configurations (template and native). Specifically, on the PMA task, the MS-SiT model outperformed other models by over 54\% compared to Spherical UNet \cite{F.Zhao2019}, 13\% to MoNet \cite{F.Monti2017}, and 12\% to the SiT (ico3) (average over both data-configurations), achieving a prediction error of 0.49 MAE on template data, which is within the margin of error of age estimation in routine ultrasound (typically, 5 to 7 days on average). On the GA task, the MS-SiT model achieved an even larger improvement with 49\%, 43\%, and 21\% reduction in MAE relative to Spherical UNet, MoNet, and SiT (ico3), respectively. Importantly, the model demonstrated much greater transformation invariance, with only a 5\% drop in performance between the template and native configurations, compared to 53\% for Spherical UNet, and 10\% for MoNet. Results also revealed a significant benefit to using the SW-MHSA with a 16\% improvement over the vanilla version on GA predictions.  

\begin{figure}[t!]
    \hspace{-0.2cm}
    \includegraphics[scale=0.19]{data/seg.001.jpeg}
    \caption{Top: Inflated surface showing mean dice scores shown for each of the DKT regions, for both MoNet and the pre-trained MS-SIT. Bottom: Boxplots comparing regional parcellation results. Asterisks denote statistical significance for one-sided paired t-test (pink: MS-SiT $>$ MoNet; green MoNet $>$ MS-SIT; ****:  $p<0.0001$, **: $p<0.01$, *: $p<0.05$ ).}
    \label{fig:seg_dice}
\end{figure}

\subsection{Cortical parcellation on UKB \& MindBoggle}

\textbf{Data \& Tasks} Cortical segmentation was performed using 88 manually labelled adult brains from the MindBoggle-101 dataset \cite{A.Klein2012}%\footnote{\url{https://MindBoggle.info/data}}
, annotated into 32 regions following a modified version of the Desikan–Killiany–Tourville (DKT)\cite{A.Klein2012} pipeline, which delineates regions according to features of cortical shape. Surface files were processed with the CIFTIFY pipeline  \cite{E.Dickie2019}, which implements HCP-style post-processing including file conversion to GIFTI and CIFTI formats, and MSM Sulc alignment \cite{E.Robinson2014,E.Robinson2018} \footnote{13 datasets failed due to missing files}. Separately, FreeSurfer annotation parcellations (based on a standard version of the DK atlas with 35 regions) were available for 4000 UK Biobank subjects, processed according to \cite{F.Alfaro2018}. These were used for pretraining. In both cases, datasets were split into 80/10/10 sets. Sulcal depth and curvature maps were used as input features. 

\textbf{Results} are presented in Table \ref{table:results-mindboggle}. The Multiscale Surface Vision Transformer (MS-SiT) was compared against two other geometric deep learning approaches for cortical segmentation: Adv-GCN, a graph-based method optimized for alignment invariance \cite{K.Gopinath2020}, and MoNet, which learns filters by fitting mixtures of Gaussians on the surface \cite{F.Monti2017}. MoNet achieved the best dice results overall. %, followed by the MS-SIT pre-trained on Biobank which slightly outperformed the result trained from scratch. 
 However, a per region box plot comparison (Fig \ref{fig:seg_dice}) of its performance relative to the MS-SIT (pre-trained with Biobank) shows this is largely driven by improvements to two large regions. Overall, MoNet and the MS-SIT differ significantly for 10 out of 32 regions, with MS-SIT outperforming MoNet for 6 of these. 

\begin{table}[t!]
    \centering
    \setlength{\tabcolsep}{10pt}
    \begin{tabular}{l | c c c }
    \hline
    \rule{0pt}{3ex} 
    \textbf{Methods} & \textbf{Aug.} & \multicolumn{1}{c}{\begin{tabular}[c]{@{}c@{}} \textbf{Shifted}\\ \textbf{Attention}\end{tabular}} & \textbf{Dice} \\
    \hline
    %Spherical UNet & \xmark & n/a & 0.898 $\pm$ 0.018\\
    %\rule{0pt}{2ex}
    %Monet &\xmark & n/a & 0.919 $\pm$ 0.012\\
    %\rule{0pt}{2ex}  
    %Spherical UNet &\cmark & n/a & 0.914 $\pm$ 0.010\\
    %Freesurfer & n/a & n/a & xx$\pm$xx\\
    \rule{0pt}{2ex}  
    Adv-GCN\tablefootnote{run on a different train/test split}  \cite{K.Gopinath2020} & n/a & n/a & 0.857$\pm$0.035\\
    \hline
    \rule{0pt}{2ex}
    MoNet \cite{F.Monti2017} & \cmark & n/a & \textbf{0.910$\pm$0.01}\\
    \rule{0pt}{2ex}
    %MS-SiT T & \cmark & \xmark & 0.869$\pm$0.01\\
   %MS-SiT T & \cmark & \xmark & 0.886$\pm$0.01\\
    %\rule{0pt}{2ex}
    %MS-SiT T & \cmark & \cmark & 0.880$\pm$0.01\\
    MS-SiT & \cmark & \cmark & 0.897$\pm$0.01\\
    \rule{0pt}{2ex}
    %MS-SiT T (UKB) & \cmark & \cmark & 0.885$\pm$0.01\\
    MS-SiT (UKB) & \cmark & \cmark & 0.901$\pm$0.01\\
    \hline
    \end{tabular}
    \caption{Overall mean and standard deviation of Dice scores (across all regions).}
    \label{table:results-mindboggle}
\end{table}


\section{Discussion}

The novel MS-SiT network presents an efficient and reliable framework, based purely on self-attention, for any biomedical surface learning task where data can be represented on sphericalised meshes. Unlike existing convolution-based methodologies translated to study general manifolds, MS-SiT does not compromise on filter expressivity, computational complexity, or rotational equivariance \cite{Fawaz2021}. Instead, with the use of local and shifted attention, the model is able to effectively reduce the computational cost of applying attention on larger sampling grids, relative to \cite{Dahan2022}, improving phenotyping performance, and performing competitively on cortical segmentation. Compared to convolution-based approach, the use of attention allows for the retrieval of attention maps, providing interpretable insights into the most attended cortical regions (Supplementary S.2), and the methodology's invariance to transformations enables it to perform well on both aligned and non-aligned data, removing the need for pre-alignment. %erfect alignment of structures.  %while providing interpretable attention maps

\section{Acknowledgements}

We would like to acknowledge funding from the EPSRC Centre for Doctoral Training in Smart Medical Imaging (EP/S022104/1). This work was also supported by The Alan Turing Institute’s Enrichment Scheme. The dHCP project was funded by the European Research Council (ERC) under the European Union Seventh Framework Programme [FR/2007–2013]/ERC Grant Agreement no. 319,456. UKB data were accessed through application number 53775, under PI Emma C. Robinson  The UKB was approved by the National Information Governance Board for Health and Social Care and the National Health Service North West Centre for Research Ethics Committee (Ref: 11/NW/0382). The MindBoggle project was funded by the NIMHR01 grant MH084029. 

%
% ---- Bibliography ----
%
% BibTeX users should specify bibliography style 'splncs04'.
% References will then be sorted and formatted in the correct style.
%
\bibliographystyle{splncs04}
\bibliography{miccai}
%

\appendix



%\iffalse
\newpage
\section{Supplementary material}
\label{section:segmentation-pipeline}

      
\begin{table}[h]
  \hspace{-1.0cm}
  \renewcommand\thetable{S.1} 
  \setlength{\tabcolsep}{7pt}
  \begin{tabular}{l | c | c | c | c }
     \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}} MS-SiT \\ levels \end{tabular}}  & \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}} Sequence length \\ $|F_{6-l}|$ \end{tabular}} &\multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}Nbr attention \\ windows \end{tabular}} &\multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}} Window size \\  $w_l$ \end{tabular}} & MS-SiT
    %& MS-SiT small
    \\
    \hline
    \hline
    %\addlinespace[0.2cm]
    $l=1$ & 20480 & 320  & 64 & \begin{tabular}{c}
                linear $96$-d - LN\\  
                 $\Bigl[\begin{tabular}{c}
                            $w_{l}$ $64$ \\
                            dim $96$, head $3$ \\
                \end{tabular} \Bigr]  \times 2$\\
                merging - LN
                \end{tabular}
    \\ 
    \hline
    %\addlinespace[0.2cm]
    $l=2$ & 5120 & 80 & 64 & \begin{tabular}{c}
                linear $192$-d - LN\\  
                 $\Bigl[\begin{tabular}{c}
                            $w_{l}$ $64$ \\
                            dim $192$, head $6$ \\
                \end{tabular} \Bigr]  \times 2$\\
                merging - LN
                \end{tabular}
    \\ 
    \hline
    %\addlinespace[0.2cm]
    $l=3$ & 1280 & 20 & 64  & \begin{tabular}{c}
                linear $384$-d - LN\\  
                 $\Bigl[\begin{tabular}{c}
                            $w_{l}$ $64$ \\
                            dim $384$, head $12$ \\
                \end{tabular} \Bigr]  \times 6$\\
                merging - LN
                \end{tabular}
    \\ 
    \hline
    %\addlinespace[0.2cm]
    $l=4$ & 320 & 1 & 320 & \begin{tabular}{c}
                linear $768$-d - LN\\  
                 $\Bigl[\begin{tabular}{c}
                            $w_{l}$ $64$ \\
                            dim $768$, head $24$ \\
                \end{tabular} \Bigr]  \times 2$\\
                merging - LN
                \end{tabular}
    \\
    %\addlinespace[0.2cm]
  \end{tabular}
  \rule{0pt}{3ex}
  \caption{MS-SiT detailed architecture. The MS-SiT model adapts the Swin-T architecture \cite{Ze.Liu2021}, into  a 4-level network with $\{2,2,6,2 \}$ local-MHSA blocks and $\{3,6,12,24\}$ attention heads per level. As in \cite{Ze.Liu2021}, the initial embedding dimension is $D=96$. Thus, the MS-SiT encoder has 29.5 M trainable parameters. 
  }
  \label{tab:mssit-grid-resolution}
\end{table}

\begin{table}[h!]
    \centering
    \renewcommand\thetable{S.2} 
    \setlength{\tabcolsep}{7pt}
    \begin{tabular}{|c | c | c |c|}
    \hline
    \rule{0pt}{3ex} 
    \textbf{Shifted attention} & \textbf{Shift factor $w_s$}& \textbf{MAE}& \textbf{MAE (augmentation)} \\
    \hline
    \xmark & \xmark & 1.68 $\pm$ 0.13 & 1.24 $\pm$ 0.04\\
    \cmark & 1/2 & 1.55 $\pm$ 0.03 &  \textbf{1.16 $\pm$ 0.02}\\
    \cmark & 1/4 & \textbf{1.54 $\pm$ 0.08} & 1.20 $\pm$ 0.03\\
    %\cmark & 1/8 & 1.60 & \textbf{1.08}\\
    \cmark & 1/16 & 1.56 $\pm$ 0.09 & 1.26 $\pm$ 0.04\\
    \hline
    \end{tabular}
    \rule{0pt}{3ex}
    \caption{Hyper-parameter tuning of the shift factor $w_s$ in the \textbf{SW-MHSA} module. The use of $w_s \in \{\frac{1}{2},\frac{1}{4}, \frac{1}{16}\}$ is compared to no shift for MS-SiT models trained on GA-template and for 25k iterations ($\sim$ 50\% typical training runtime).  MAE and std on validation data, averaged over 3 runs are reported. Shifting the sequence of half the length of the attention windows, i.e. $w_s=\frac{1}{2}$, provides the best results overall and is used in all the following.}
    \label{table:ablation_shifted_window}
\end{table}




\begin{figure}[t!]
    \centering
     \renewcommand\thefigure{S.1} 
     \includegraphics[scale=0.38]{data/seg_3.001.jpeg}
     \caption{MS-SiT segmentation pipeline. Input data is resampled and partitioned as in Figure \ref{fig:swin_model}. The  $l=\{1,2,3,4\}$ levels of the segmentation pipeline are similar to the MS-SiT encoder levels (Figure \ref{fig:swin_model}). The \emph{patch partition} layers reverse the patch merging procedure of the MS-SiT encoder, upsampling the  spatial resolution of patches from $I_2 \rightarrow I_3 \rightarrow I_4 \rightarrow I_5$. Skip connections between levels are used. Finally, a \emph{spherical resampling} layer resamples the final embeddings to an ico6 tessellation (40962 vertices), before the final segmentation prediction.}
     \label{fig:swin_model_segmentation}
\end{figure}



\begin{figure}[h!]
     \renewcommand\thefigure{S.2} 
     \includegraphics[scale=0.18]{data/attention.001.jpeg}
     \caption{Comparison of normalised attention maps from the last layers of a SiT model \cite{Fawaz2021}  (applying global attention for all layers) and an MS-SiT model, both trained for GA-template prediction. MS-SiT maps display highly specific attention patterns, compared to the SiT counterparts, focusing on characteristic landmarks of cortical development such as the sensorimotor cortex with low myelination in preterm (pink arrows) and high myelination in term (blue arrows).}
     \label{fig:swin_model_segmentation}
\end{figure}


%\fi
\end{document}


