\begin{abstract}
\vspace{-5mm}

We tackle the task of text-to-3D creation with pre-trained latent-based NeRFs (NeRFs that generate 3D objects given input latent code). Recent works such as DreamFusion and Magic3D have shown great success in generating 3D content using NeRFs and text prompts, but the current approach of optimizing a NeRF for every text prompt is 1) extremely time-consuming and 2) often leads to low-resolution outputs. To address these challenges, we propose a novel method named 3D-CLFusion which leverages the pre-trained latent-based NeRFs and performs fast 3D content creation in less than a minute. In particular, we introduce a latent diffusion prior network for learning the w latent from the input CLIP text/image embeddings. This pipeline allows us to produce the w latent without further optimization during inference and the pre-trained NeRF is able to perform multi-view high-resolution 3D synthesis based on the latent. We note that the novelty of our model lies in that we introduce contrastive learning during training the diffusion prior which enables the generation of the valid view-invariant latent code. We demonstrate through experiments the effectiveness of our proposed view-invariant diffusion process for fast text-to-3D creation, e.g., 100 times faster than DreamFusion. We note that our model is able to serve as the role of a plug-and-play tool for text-to-3D with pre-trained NeRFs.

% We propose a novel method for text-to-3D content creation, named 3D-CLFusion, that leverages pre-trained latent-based Neural Radiance Fields (NeRFs) to generate 3D content quickly and efficiently. Recent works such as DreamFusion have demonstrated the potential of using NeRFs and text prompts to create 3D content. However, these approaches have limitations, such as inefficiency during inference and low-resolution outputs due to the need to optimize a NeRF for every text prompt. To address these challenges, our method introduces a latent diffusion prior network that learns the w latent from input CLIP text/image embeddings, allowing us to produce the w latent without further optimization during inference. This pipeline enables the pre-trained NeRF to perform multi-view synthesis based on the latent. Furthermore, we incorporate contrastive learning during training of the diffusion prior to ensure that the latent is view-invariant. As a result of using pre-trained NeRFs, which are capable of synthesizing realistic images, the rendered 3D content is high-resolution. We demonstrate through experiments that our proposed view-invariant diffusion process is capable of performing text-to-3D rendering with latent-based NeRFs. However, it is worth noting that our plug-and-play model is currently limited to the category of pre-trained NeRFs and we will provide more results with a broader range of 3D objects in future work.

% We propose a novel approach for fast text-to-3D content creation leveraging pre-trained latent-based Neural Radiance Fields (NeRFs). Recent works such as DreamFusion and Magic3D have shown great success in generating 3D content using NeRFs and text prompts, but the current approach of optimizing a NeRF for every text prompt is extremely time-consuming and often leads to low-resolution outputs. Our method, named 3D-CLFusion, addresses these limitations by leveraging pre-trained NeRFs, introducing a latent diffusion prior network, and performing fast 3D content creation in under a minute. The latent diffusion prior network learns the w latent from input CLIP text/image embeddings, allowing us to produce the w latent without further optimization during inference. This pipeline enables the pre-trained NeRF to perform multi-view synthesis of high-resolution 3D content. We note that the core novelty lies in incorporating contrastive learning during training of the diffusion prior to ensuring that the latent is view-invariant. As a result of using pre-trained NeRFs, which are capable of synthesizing realistic images, the rendered 3D content is high-resolution. We demonstrate through experiments the effectiveness of our proposed view-invariant diffusion process for fast text-to-3D creation, e.g., 100 times faster than DreamFusion.

% We believe that this method can significantly reduce the time and resources needed to generate 3D content.

% We present a novel method, 3D-CLFusion, for Text-to-3D content creation using pre-trained latent-based Neural Radiance Fields (NeRFs). Current approaches to 3D content creation with NeRFs and text prompts involve optimizing a NeRF for each prompt, which can be time-consuming and lead to low-resolution outputs. Our method addresses these limitations by leveraging pre-trained NeRFs and introducing a latent diffusion prior network to quickly generate 3D content in under a minute. By incorporating contrastive learning during training, our approach generates view-invariant latents and produces high-resolution 3D renders. Our plug-and-play model generates fast inference but is currently limited to the category of pre-trained NeRFs. Future work will include evaluating the method with a broader range of 3D objects.
\end{abstract}