\section{Experiment}
\label{sec:EXP}

\begin{figure*}[t!]
  \centering
  \includegraphics[width=\linewidth]{fig/stylenerf.pdf}
  \vspace{-7mm}
  \caption{\textbf{Qualitative comparisons on text-to-3D with pre-trained latent-based NeRF: StyleNeRF~\cite{gu2021stylenerf}.} All of the output images are rendered using the same camera poses and the checkpoints from FFHQ and CompCars datasets.}
  \label{fig:stylenerf}
%   \vspace{-4mm}
\end{figure*}
% \input{exp/quant_metric}

\begin{figure*}[t!]
  \centering
  \includegraphics[width=\linewidth]{fig/EG3D.pdf}
  \vspace{-7mm}
  \caption{\textbf{Qualitative comparisons on text-to-3D with pre-trained latent-based NeRF: EG3D~\cite{chan2022efficient}.} All of the output images are rendered using the same camera poses and the checkpoints from FFHQ and AFHQ (Cat class) datasets.}
  \label{fig:EG3D}
  % \vspace{-3mm}
\end{figure*}
% \input{exp/quant_novel}
\subsection{Datasets}

% \paragraph{Datasets.}
Since our pipeline relies on the pre-trained latent-based NeRF, we train the latent-based NeRF on some datasets including FFHQ~\cite{karras2019style}, AFHQ~\cite{choi2020starganv2}, and CompCar~\cite{yang2015large}.
\newline

\noindent\textbf{FFHQ}~\cite{karras2019style} id a face dataset which contains 70,000 face images. We assume the captured faces are mostly in the center. Though the size of the images is provided as 1024 $\times$ 1024 in the dataset, all of the images are resized into 256 $\times$ 256 for training the checkpoints.
\newline

\noindent\textbf{AFHQ}~\cite{choi2020starganv2}  is an animal face dataset that contains 15,000 high-quality images at 512Ã—512 resolution and includes three categories of animals which are cats, dogs, and wildlife. Each category has about 5000 images. For each category, the dataset split around 500 images as a test set and provide all remaining images as a training set. Only the training images are used in the experiments.
\newline

\noindent\textbf{CompCars}~\cite{yang2015large} is a car dataset that contains 136,726 images capturing the different vehicles with various styles. The original dataset contains images with different aspect ratios. All of the images in the dataset are center-cropped and resized into 256 $\times$ 256.


\subsection{Experimental Settings}
\paragraph{Pre-trained latent-based NeRFs.}
To train our diffusion prior in our 3D-CLFusion, we use the pre-trained StyleNeRF~\cite{gu2021stylenerf} and EG3D~\cite{chan2022efficient} models as the generators trained on FFHQ, AFHQ, and CompCars. To have a fair comparison, we use the pre-trained models that are provided by original papers online. Specifically, StyleNeRF~\cite{gu2021stylenerf} provides checkpoints on FFHQ in 256, 512, and 1024 dimensions. EG3D~\cite{chan2022efficient} provides models trained on FFHQ and AFHQ (only cat category). Since StyleNeRF and EG3D do not provide the checkpoints for realistic cars such as the images in CompCars, we additionally train the checkpoint of the generator on cars using StyleNeRF for evaluation.

\paragraph{Baselines.}
Since our 3D-CLFusion is the first diffusion prior to leveraging latent-based NeRFs for text-to-3D, we compare it with the most similar baseline: clip2latent~\cite{pinkney2022clip2latent}. clip2latent is also a framework for $w$ latent diffusion while the main difference is their design model is for 2D StyleGAN and does not have the constraints on view-invariant learning for NeRFs. To further compare with the direct optimization method, we compare our model with latent vector optimization in $\mathcal{W}$~\cite{Karras2019stylegan2}.

% To compare with encoder-based methods, we compare our proposed method with current state-of-the-art encoders which include pSp~\cite{richardson2021encoding} in $\mathcal{W}+$ space and ReStyle~\cite{alaluf2021restyle} from 2D GAN inversion. We also compare with the baseline encoder for $\mathcal{W}$ space inversion. For fair comparison, all of the encoder-based competitors are trained on both real and synthesized images.
% To compare with online optimization methods, we compare our model with latent vector optimization to $\mathcal{W}+$~\cite{Karras2019stylegan2}  and PTI~\cite{roich2021pivotal}.
\input{exp/quant_clip}
\paragraph{Evaluation settings.}

\begin{figure*}[t!]
  \centering
  \includegraphics[width=0.95\linewidth]{fig/abl_noise.pdf}
  % \vspace{-5mm}
  \caption{\textbf{Ablation studies on the learning of $\epsilon_\theta$ and learning of $f_\theta$ in the diffusion prior of 3D-CLFusion.} The results are produced from the generators of both StyleNeRF and EG3D, which are trained on the FFHQ dataset.}
  \label{fig:abl}
  \vspace{-4mm}
\end{figure*}

For qualitative comparison, we compare the input text prompt and the multi-view images generated from its latent code. We would like to note that, the generators trained on FFHQ and AFHQ are trained using the frontal head yaw angle roughly ranging between $-45^{\circ}$ to $45^{\circ}$ degrees for StyleNeRF and EG3D.
Only the checkpoints on CompCars can have 360-degree rendered images. For quantitative results, we use the CLIP similarity score following clip2latent~\cite{pinkney2022clip2latent} and only measure the frontal view of the latent-based NeRFs generated by the models trained on FFHQ.


\subsection{Implementation Details}

To train our diffusion priors (either $\epsilon_\theta$ or $f_\theta$), we leverage the generated images from the generators to generate the ground-truth paired data. Specifically, we sample a latent $w \in \mathbb{R}^{512}$ from the generator and generate $k=8$ views by manipulating camera poses for one $w$. Each generated image will be resized to $224 \times 224$ for the CLIP image encoder, \textit{i.e.,} ViT-B/32 we use in the experiments, to generate the CLIP embeddings $e \in \mathbb{R}^{512}$. To have the same comparison with clip2latent~\cite{pinkney2022clip2latent}, we also choose the same architecture as the diffusion prior in DALLE-2~\cite{ramesh2022hierarchical} where the $\epsilon_\theta$ or $f_\theta$ is implemented with causal transformer~\cite{melnychuk2022causal}. Following the training strategy in clip2latent, we also apply classifier-free guidance~\cite{ho2022classifier} and pseudo-text embeddings~\cite{zhou2021lafite} to enhance the diversity of the generated image and prevent overfitting on the image embeddings. The number of timesteps for the diffusion process is set as 1000. For the hyperparameter for all of the loss functions, all losses are equally weighted ($\lambda_{diff}=1.0$ and $\lambda_{contrast}=1.0$) for all the experiments. The batch size is set as $512$ where we ensure there are 64 ground-truth $w$ latent each with 8 different CLIP image embeddings from different views for optimizing the loss.  We optimize the network using Adam optimizer with a learning rate of $0.0001$ and train the model for 1,000,000 iterations. Each experiment is conducted on 1 Nvidia GPU 3090 (24G) and implemented in PyTorch.


\subsection{Results and Comparisons.}

In this section, we compare our proposed model with the baseline approach: clip2latent~\cite{pinkney2022clip2latent} and the online optimization method~\cite{Karras2019stylegan2} qualitatively (in Figure~\ref{fig:stylenerf} and Figure~\ref{fig:EG3D}) and quantitatively (in Table~\ref{table:abl_clip}). 
% 
\paragraph{Qualitative results.}
As shown in Figure~\ref{fig:stylenerf}, we compare the quality of the generated latent $w$ among all of the models on the generated images from StyleNeRF. The left part displays the results on the FFHQ dataset while the right part displays the results on CompCars. There are some phenomena that can be summarized. First, although clip2latent is able to generate a latent code that can generate close images, semantically the output images are not well matched. For example, the gender on the FFHQ and the color of the vehicle on CompCars are not well matched with the input text prompts. This infers that without the constraints to ensuring the latent $w$ to be invariant, the generated latent code $w$ from the text CLIP embeddings may not share close latent $\mathcal{W}$ space. Second, the optimization is able to generate satisfactory results on FFHQ while failing to optimize the entire color of the vehicle. We credit the reason to that because the 360-degree vehicle is difficult to manipulate compared with faces in FFHQ. Thus, directly optimizing CLIP loss will be easy to bring artifacts and generate unnatural results (see the rightmost column of the blue SUV with green background).

We can also observe similar observations on EG3D~\cite{chan2022efficient} shown in Figure~\ref{fig:EG3D}. First, clip2latent fails to generate the well-matched latent for EG3D to render (see the sunglasses in the second big column and the color of the cat in the third big column). The optimization method is also able to generate well-matched results while they are unnatural (see the yellow eyes in the rightmost column). Our proposed methods exhibit comparable results as direct optimization.

\paragraph{Quantitative results.}

As shown in the first three rows of Table~\ref{table:abl_clip}, we compare the two methods with the CLIP similarity score using the 64 text prompts provided by clip2latent~\cite{pinkney2022clip2latent}. To measure the inference time and have a fair comparison, we train the optimization~\cite{Karras2019stylegan2} for 200 iterations for each text prompt. Our proposed model achieves a superior score compared with clip2latent with a similar inference time. On the other hand, although the optimization method has artifacts in qualitative results, it has the best result in CLIP score. This is basically because the objective of the optimization is sorely CLIP similarity. Thus, online optimization is supposed to have the best score. 

\begin{figure}[t!]
  \centering
  \includegraphics[width=\linewidth]{fig/abl.pdf}
  % \vspace{-8mm}
  \caption{\textbf{Ablation studies on the proposed losses.} The results are from the StyleNeRF generator trained on FFHQ.}
  \label{fig:abl_loss}
  \vspace{-4mm}
\end{figure}

\subsection{Ablation studies}
\paragraph{Learning objectives: $\epsilon_\theta$ vs $f_\theta$.}
To further support the claim in Section~\ref{sec:method} that learning $f_\theta$ is more stable than $\epsilon_\theta$ when applying contrastive loss, we compare the models learning two different objectives qualitatively in Figure~\ref{fig:abl} and quantitatively in Table~\ref{table:abl_clip}. As shown in Figure~\ref{fig:abl}, it is obvious that learning $\epsilon_\theta$ would lead to inferior results. For example, learning $\epsilon_\theta$ can not generate good latent when feeding the input text prompt as "A vampire's face" in either StyleNeRF or EG3D. Table~\ref{table:abl_clip} also shows that learning $\epsilon_\theta$ exhibits a worse CLIP score compared with $f_\theta$. We credit the reason that by producing the $\Bar{w}_0$ from $\epsilon_\theta$, the $\Bar{w}_0$ will be far from consistency since it also depends on $w_t$ in each step. The view-invariant learning will not work easily on the network in $\epsilon_\theta$.


\paragraph{Contrative loss function: $\mathcal{L}_{contrast}$.}
To further analyze the effectiveness of essential contrastive losses of our proposed 3D-CLFusion, we conduct the experiments with one of them excluded and present the qualitative result in Figure~\ref{fig:abl_loss} and quantitatively in Table~\ref{table:abl_clip} as well. First, apparently without L2 loss $\mathcal{L}_{2}$, the diffusion model is not able to well derive view-invariant latent code in each diffusion process while it can still ensure the multi-view CLIP embeddings will produce close $w_0$ with the remaining triple loss. Second, we can observe that in the model with the triplet loss $\mathcal{L}_{tri}$ in our $\mathcal{L}_{contrast}$ excluded, the produced latent code $w$ starts to semantically deviate the input text prompt, which infers that the inter-class distance learning is also important for learning the view-invariant latent code in $\mathcal{W}$ latent space. Third, with the entire contrastive loss $\mathcal{L}_{contrast}$ excluded, the model is not able to have any guidance on view-invariant learning and would produce similar results as clip2latent~\cite{pinkney2022clip2latent}.






