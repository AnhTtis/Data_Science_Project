\section{The Proposed Method}
\label{sec:method}



\subsection{Problem Formulation and Overview}

Given the input text prompt, our goal is to generate the conditioned multi-view images with the pre-trained latent-based NeRF generator as our text-to-3D task. Specifically, given the input text embedding with CLIP, denoted as $e_t$, we aim to produce the corresponding $w$ latent\footnote{latent $w \in \mathcal{W}$~\cite{shen2020interpreting} or the extended latent $w \in \mathcal{W}+$~\cite{Karras2019stylegan2}} as output. The pre-trained NeRF generator denoted as $G$, is able to synthesize images $x$ given different camera poses $p$: $x = G(w,p)$. 

\begin{figure*}[t!]
  \centering
  \includegraphics[width=\linewidth]{fig/model.pdf}
  \vspace{-6mm}
  \caption{\textbf{Overview of our proposed 3D-CLFusion.} It consists of three modules: CLIP text/image encoders, contrastive latent diffusion prior, and pre-trained latent-based NeRF. More details can be referred to the section~\ref{sec:method}.}
%   Feature loss can only be applied to multi-view images (synthesized images in this work). 
  \label{fig:model}
  % \vspace{-6mm}
\end{figure*}

In order to achieve text-to-3D creation by producing the $w$ latent for fast NeRF rendering, we propose a generative framework named 3D-CLFusion, which is presented in Figure~\ref{fig:model}. First, in order to produce the latent $w$ for the generator to render from the input text prompt, we introduce the diffusion prior network ($f_{\theta}$ where $\theta$ denotes the network parameters) which is able to take the CLIP text embedding $e_{txt}$ as an input condition and produces $w_0$ latent in the last diffusion step. More details of the diffusion/denoising process will be presented later. Since we do not have labeled text embeddings for the ground-truth latent $w_0$, we leverage the image CLIP embeddings for training the diffusion prior. We assume CLIP text and image embeddings share the closed latent space. Second, in order to learn the view-invariant latent $w$ in $\mathcal{W}$ space, we leverage the multi-view CLIP image embeddings from the images generated by the model and apply the contrastive loss to ensure the produced latent in $\mathcal{W}$ are view-invariant given different CLIP embeddings from a different view. After the training stage, we can sample text CLIP embedding from a given text input prompt and generate the corresponding 3D multi-view images with the pre-trained NeRF.

% We leverage synthesized images (\textit{i.e.}, using the multi-view images synthesized by the generator, shown in Figure~\ref{fig:syn}) and the real images along with the off-the-shelf pose estimator~\cite{ruiz2018fine} for measuring the camera pose.
% The NeRF-3DE involves two stages: the base stage and the refining stage. 1) In base stage, the introduced base encoder $E_{base}$ takes an image $x$ as input and produces the style latent code $w_{base}$. 
% In order to learn the view-invariant latent code, we leverage the multi-view images synthesized by the generator, shown in Figure~\ref{fig:syn}. This latent code $w_{base}$ is optimized to roughly reconstruct the 2D input image $\hat{x}_{base}$ and enable 3D-consistent novel-view rendering. 
% 2) To further minimize the identity gap between the output and the input images, a refining encoder $E_{ref}$ is introduced to refine the latent code $w_{base}$. It first takes the concatenation of the input image $x$ and the generated image $\hat{x}_{base}$ from the previous stage as input and learns a residue $\Delta_{w^+}$. Then we can obtain the output style latent code $\hat{w} = w_{ref}^+$ by adding the residue to $w^{+}_{base}$. 

\subsection{Pre-trained Latent-based NeRF}
\paragraph{Latent-based Neural Implicit Representation.} Following StyleGANs~\cite{karras2019style,Karras2019stylegan2}, Latent-basde NeRFs~\cite{gu2021stylenerf} also introduce the mapping network $f$ which maps noise vectors from a spherical Gaussian space $\mathcal{Z}$ to the style space $\mathcal{W}$. $f$ consists of several MLP layers and the input style vector $w \in \mathcal{W}$ can be derived by $w = f(z), z \in \mathcal{Z}$. Following the neural rendering mechanism in NeRF~\cite{mildenhall2020nerf}, all of our pre-trained latent-based NeRF also takes the position $u\in \mathbb{R}^3$ and viewing direction $d \in \mathbb{S}^2$ as inputs, and predicts the density $\sigma(u)\in \mathbb{R}$ and view-dependent color $c(u,d) \in \mathbb{R}^3$.

% In order to render the color and density for each coordinate in 3D space with high-frequency details, latent-based NeRF also uses positional embedding with Fourier series: $\gamma(p) = (\sin(2^0\pi p), \cos(2^0\pi p), ... , \sin(2^{L-1}\pi p), \cos(2^{L-1}\pi p))$,
% % \begin{equation}
% % \begin{split}
% % \gamma(p) = (\sin(2^0\pi p), \cos(2^0\pi p), ... , \sin(2^{L-1}\pi p), \cos(2^{L-1}\pi p)),
% % \end{split}
% % \end{equation}
% where the function $\gamma(.)$ is applied to each of the three coordinates in $u$ and to the three coordinates of the view direction $d$. Let us denote the rendering network by $\phi^n_{w}$ where $n$ indicates the number of MLP layers within and $w$ indicates the style feature. Each MLP weight matrix is modulated by the latent code $w$ independently. Both the density and the color can be rendered respectively with:
% \begin{equation}\small
% \begin{split}
% \sigma_{w}(x) = h_{\sigma}(\phi^n_{w}(\gamma(u)))\\
% c_{w}(u,d) = h_{c}(\phi^n_{w}(\gamma(u)),\gamma(d)),
% \end{split}
% \end{equation}
% where $h_{\sigma}(\cdot)$ and $h_{c}(\cdot)$ are projection layers.

\paragraph{Volume Rendering with Radiance Fields.}
Once we have the color and density for each coordinate and view direction, we render the color $C(r)$ for each pixel along that camera ray $r(t)=o+td$ passing through the camera center $o$ with volume rendering~\cite{kajiya1984ray}:
\begin{equation}\small
\begin{split}
C_{w}(r) = \int_{t_n}^{t_f} T(t)\sigma_{w}(r(t))c_{w}(r(t),d)dt,\\
\text{where}\quad T(t) = \exp(-\int_{t_n}^t \sigma_{w}(r(s))ds).
\end{split}
\end{equation}
The function $T(t)$ denotes the accumulated transmittance along the ray from $t_n$ to $t$. In practice, the continuous integration is discretized by accumulating sampled points along the ray. More details can be obtained in \cite{mildenhall2020nerf,gu2021stylenerf,chan2022efficient}.

\subsection{Latent Diffusion with CLIP embedding}
In order to produce the corresponding $w$ from the input text prompt $e_{txt}$ or image prompt $e_{img}$, we choose to employ a latent diffusion process to learn the mapping given the success of latent diffusion models in \cite{rombach2022high,pinkney2022clip2latent,ramesh2022hierarchical}. Now we present the overview of the latent diffusion model which contains: forward (diffusion) and backward (denoising) processing.

As stated in DDPM~\cite{ho2020denoising} and Latent Diffusion~\cite{rombach2022high}, we can formulate the diffusion process of $w$ latent diffusion for our task as:
% 
\vspace{-3mm}
\begin{equation}\label{eq:forward}
\begin{split}
q(w_{1:T}|w_0) & =  \prod_{t=1}^{T} q(w_t|w_{t-1}),\\
 q(w_t|w_{t-1}) & = \mathcal{N}(w_t; \sqrt{1-\beta_t}w_{t-1},\beta_t \mathbf{I}),
\end{split}
\end{equation}
where $w_1$ ... $w_T$ are the latent of the same dimensionality of $w_0$ and $\beta_1<$ ... $<\beta_T$ are variance schedule. The reverse process can also be formulated as:
\begin{equation}\label{eq:backward}
\begin{split}
p_{\theta}(w_{0:T}) & = p(x_T) \prod_{t=1}^{T} p_{\theta}(w_{t-1}|w_{t}),\\
 p_{\theta}(w_{t-1}|w_{t}) & = \mathcal{N}(w_{t-1}; \mu_{\theta}(w_t,t),\Sigma_{\theta} (w_t,t)),
\end{split}
\end{equation}
where $\theta$ is the learnable parameters. If we set $\Sigma_{\theta} (x_t,t) = \sigma_t^2 \mathbf{I}$ as fixed variance, we only need to learn $\mu_{\theta}(w_t,t)$. Since we can denote $\alpha_t = 1-\beta_t$ and $\Bar{\alpha}_t = \prod_{s-1}^t \alpha_s$, we can estimate the forward process posteriors conditioned on $w_0$ as:
\begin{equation}\label{eq:posterior}
\begin{split}
 q(w_{t-1}|w_{t},w_0) = \mathcal{N}(w_{t-1}; \Tilde{\mu}_t (w_t,w_0), \Tilde{\beta}_t \mathbf{I}),\\
 \Tilde{\mu}_t (w_t,w_0) = \frac{\sqrt{\Bar{\alpha}_{t-1}}\beta_t}{1 - \Bar{\alpha}_{t}} w_0 + \frac{\sqrt{\alpha_t}(1 - \Bar{\alpha}_{t-1})}{1 - \Bar{\alpha}_{t}}w_t,\\
\Tilde{\beta}_t = \frac{1 - \Bar{\alpha}_{t-1}}{1 - \Bar{\alpha}_{t}}\beta_t.
\end{split}
\end{equation}
% \begin{equation}\label{eq:posterior}
% \begin{split}
%  \text{where} q(w_{t-1}|w_{t},w_0) = \mathcal{N}(w_{t-1}; \Tilde{\mu}_t (w_t,w_0) \Tilde{\beta}_t \mathbf{I}),
% \end{split}
% \end{equation}
As stated in DDPM~\cite{ho2020denoising}, we can choose to learn the $\mu_{\theta}(w_t,t)$ in Equ.~\ref{eq:backward} and reparameterize it as:
\begin{equation}\label{eq:mu1}
\begin{split}
\mu_{\theta}(w_t,t) & = \Tilde{\mu}_t (w_t,w_0) = \frac{1}{\sqrt{\alpha_t}} (w_t - \frac{\beta_t}{\sqrt{1-\Bar{\alpha_t}}} \epsilon_\theta (w_t,t,e)),\\
\text{and}\quad w_0 & = \frac{1}{\sqrt{\alpha_t}} (w_t- \sqrt{1-\Bar{\alpha_t}}\epsilon_\theta (w_t,t,e))
\end{split}
\end{equation}
where $\epsilon_\theta$ is a function approximator intended to predict the noise $\epsilon$ from $w_t$, timestep embeddings $t$, and the CLIP embeddings $e_{txt}$ or $e_{img}$. Or we can reparameterize $\mu_{\theta}(w_t,t)$ as:
% 
\begin{equation}
\begin{split}
& \mu_{\theta}(w_t,t) = \Tilde{\mu}_t (w_t,w_0) \\ & = \frac{\sqrt{\Bar{\alpha}_{t-1}}\beta_t}{1 - \Bar{\alpha}_{t}} f_\theta (w_t,t,e) + \frac{\sqrt{\alpha_t}(1 - \Bar{\alpha}_{t-1})}{1 - \Bar{\alpha}_{t}}w_t,
\end{split}
\label{eq:mu2}
\end{equation}
% 
where $f_\theta$ is a function approximator intended to predict the $w_0$ from $w_t$, timestep embeddings $t$, and the CLIP embeddings $e_{txt}$ or $e_{img}$. Later we will show learning $f_\theta$ in Equ.~\ref{eq:mu2} is preferable for contrastive learning compared with learning $\epsilon_\theta$ in Equ.~\ref{eq:mu1}. The diffusion prior loss can be defined as:
\begin{equation}\label{eq:loss_diff}
\begin{split}
\mathcal{L}_{diff} = \mathbb{E}[\|\epsilon - \epsilon_\theta (w_t,t,e))\|^2]\\ \text{or} \quad \mathcal{L}_{diff} = \mathbb{E}[\|w_0 - f_\theta (w_t,t,e)\|^2]
\end{split}
\end{equation}

\subsection{View-invariant Latent Diffusion}

For 3D latent-based NeRF diffusion, the latent $w$ not only needs to match the input CLIP embedding $e$ but is also assumed to be view-invariant. As stated in NeRF-3DE~\cite{li20223d}, only the latent $w$ inside the valid manifold in the latent space $\mathcal{W}$ for latent-based NeRF is capable to produce reasonable view-invariant 3D objects, while the latent outside the manifold would lead to severe 3D distortion.

Since the CLIP image embeddings generated from the same $w$ with different camera poses $p_i$, $e_{img}^i$ representing CLIP image embeddings from different views are also different. Hence, the produced $w_t$ using Equ.~\ref{eq:backward} would not be view-invariant. We need to ensure the latent produce from the diffusion prior in each step are view-invariant as:
\begin{equation}\label{eq:ideal}
\begin{split}
f_\theta(w_t,t,E_{clip}^{img}(x_i) & = f_\theta(w_t,t,E_{clip}^{img}(x_j)\\
x_i & = G(w_0, p_i), i\neq j
\end{split}
\end{equation}
where $p_i$, $i=0,1,..,n$ represent camera poses. 
Minimizing the objective allows the model to learn the view-invariant latent code $\hat{w}$ since it maps multi-view embeddings $e_i = E_{clip}^{img}(x_i)$ (controlled by the pose $p_i$) to the same latent code $w$ for each set of the training sample. During inference, a single-view CLIP embedding (either text or image embeddings) is mapped to the latent code $\hat{w}$ which can produce multi-view images of the same identity by changing the poses.


To ensure this, we can use contrastive learning to train the diffusion prior by applying constraints on $\Tilde{w}_0 = f_{\theta} (w_t, t, e)$ in Equ.~\ref{eq:mu2}. 
Specifically, we perform contrastive learning with L2 loss $\mathcal{L}_{2}$ and triplet loss $\mathcal{L}_{tri}$ on the produced $w_0$ in each diffusion step. We can formulate $\mathcal{L}_{2}$ loss as:
\begin{equation}\label{eq:l2}
\begin{split}
\mathcal{L}_{2} & = \|f_{\theta} (w_t, t, e^i)- f_{\theta} (w_t, t, e^j)\|^2,
\end{split}
\end{equation}
where $e_i$ and $e_j$ are produced by the same ground-truth $w$. To maximize the inter-class discrepancy while minimizing intra-class distinctness, we introduce $\mathcal{L}_{tri}$. Specifically, for each input image embedding $e$, we sample a positive image embedding $e_\mathrm{pos}$ with the same identity label and a negative image $e_\mathrm{neg}$ with different identity labels to form a triplet tuple. Then, the following equations compute the distances between $e$ and $e_\mathrm{pos}$/$e_\mathrm{neg}$:

% \vspace{-0.2cm}
\begin{equation}
  \begin{aligned}
  d_\mathrm{pos} = \| f_{\theta} (w_t, t, e) - f_{\theta} (w_t, t, e^{pos})\|_2, \\
  d_\mathrm{neg} = \|f_{\theta} (w_t, t, e) - f_{\theta} (w_t, t, e^{neg})\|_2,
  \end{aligned}
  \label{eq:d-pos}
  % \vspace{-0.1cm}
\end{equation}
% \vspace{-0.1cm}
% \begin{equation}
%   \begin{aligned}
%   d_\mathrm{neg} = \|{w}_{base} - {w}_{{base}_\mathrm{neg}}\|_2,
%   \end{aligned}
%   \label{eq:d-neg}
%   \vspace{-0.1cm}
% \end{equation}
With the above definitions, we have the triplet loss $\mathcal{L}_{tri}$ defined as:
% \vspace{-0.2cm}
\begin{equation}
  % \small
  \begin{aligned}
  \mathcal{L}_{tri} &~ 
  = \max(0, m + d_\mathrm{pos} - d_\mathrm{neg}),
  \end{aligned}
  \label{eq:tri}
%   \vspace{-0.1cm}
\end{equation}
%
where $m > 0$ is the margin used to define the distance difference between the positive image pair $d_\mathrm{pos}$ and the negative image pair $d_\mathrm{neg}$. The contrastive loss can be summed up as:
% The feature-level loss for $n$ multi-view synthesized images can be summed up as:
\begin{equation}
  \begin{aligned}
  \mathcal{L}_{contrast} = \mathcal{L}_{2} + \mathcal{L}_{tri}
  \end{aligned}
  \label{eq:feat}
%   \vspace{-0.1cm}
\end{equation}

We would like to note that, the contrastive loss can still be applied using Equ.~\ref{eq:mu1} on the predicted $\Tilde{w}_0 = \frac{1}{\sqrt{\alpha_t}} (w_t, \sqrt{1-\Bar{\alpha}_t}\epsilon_\theta (w_t,t,e))$. However, applying constraints on this $\Tilde{w}_0$ would not be stable since it depends on both predicted $\epsilon$ and $w_t$ in each step, where $w_t$ is varying and sampled from Gaussian in each step.

% \subsection{Full objective}
The total loss $\mathcal{L}$ for training our proposed NeRF-3DE is summarized as follows:
%
\begin{equation}
  \begin{split}
  \mathcal{L}_{total} & = \lambda_{diff}\cdot\mathcal{L}_{diff} + \lambda_{contrast}\cdot\mathcal{L}_{contrast},
  \end{split}
  \label{eq:fullobj}
\end{equation}
where $\lambda_{diff}$ and $\lambda_{contrast}$ are the hyper-parameters used to control the weighting of the corresponding losses.

