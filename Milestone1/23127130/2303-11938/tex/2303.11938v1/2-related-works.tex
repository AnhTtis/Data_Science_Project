\section{Related Works}


% \paragraph{Multi-view synthesis with Neural Radiance Fields.}

% \paragraph{Sparse-view synthesis.}


\paragraph{Text-to-image generation.}

There are several text-to-image generative models that have been proposed in recent months. The majority of these models are trained using large amounts of text-image paired data where image generation is conditioned on the text input. To achieve high-quality and accurate image generation from text, several of the existing approaches make use of pre-trained CLIP to produce text embedding. One line of research is to leverage diffusion models in \cite{ramesh2022hierarchical,rombach2022high,saharia2022photorealistic}, where the models directly learn the mapping between text and image with diffusion priors (\textit{i.e.}, Dalle 2~\cite{ramesh2022hierarchical} and Glide\cite{nichol2021glide}) or sample from a low-resolution latent space and decode the latent into high-resolution images (\textit{i.e.}, Stable Diffusion~\cite{rombach2022high}). Another line of work~\cite{patashnik2021styleclip,gal2022stylegan,abdal2022clip2stylegan,kocasari2022stylemc} is to perform text-guided generation or editing relying on CLIP and the pre-trained StyleGANs~\cite{karras2019style,Karras2019stylegan2}. StyleGANs have achieved high image quality and support different levels of semantic manipulation in the $w \in \mathcal{W}$ latent space.
Recently, clip2latent~\cite{pinkney2022clip2latent} has been proposed to produce the $w$ latent in StyleGAN from the input text prompt with the diffusion model.
However, most of these works focus on the rendering of 2D images
and are not capable of manipulating camera poses easily as NeRF~\cite{mildenhall2020nerf}. 

% With improvements in modeling and data curation, diffusion models can compose complex semantic concepts from text descriptions (nouns, adjectives, artistic styles, etc.) to generate high-quality images of objects and scenes~\cite{ramesh2022hierarchical,rombach2022high,saharia2022photorealistic}. Sampling images from diffusion models is time consuming. To generate high-resolution images, these models either utilize a cascade of super-resolution mod- els (\textit{i.e.}, Dalle 2~\cite{ramesh2022hierarchical} and Glide\cite{nichol2021glide}) or sample from a lower-resolution latent space and decode latents into high-resolution images (\textit{i.e.}, Stable Diffusion~\cite{rombach2022high}). Despite the advances in high-resolution image generation, using language to describe and control 3D properties (e.g. camera viewpoints) while maintaining coherency in 3D remains an open, challenging problem.



% GANs~\cite{goodfellow2014generative} have demonstrated success in image synthesis and have been extended to a number of works~\cite{zhang2019self,brock2018large,karras2017progressive}.
% StyleGANs~\cite{karras2019style,Karras2019stylegan2} achieve
% state-of-the-art image quality and support different levels of semantic manipulation.

% In particular, many methods have been proposed for finding these semantic latent space manipulation using varying levels of supervision. These include full-supervision in the form of semantic
% labels~\cite{abdal2021styleflow,shen2020interpreting,goetschalckx2019ganalyze} and unsupervised approaches~\cite{wang2021geometry,voynov2020unsupervised}. Some methods~\cite{harkonen2020ganspace,tewari2020stylerig,abdal2020image2stylegan++,shoshan2021gan} also leverage disentangled properties in the latent space to enable 3D controls. However, most of these works focus on the rendering of 2D images with 3D controls
% and are not capable of manipulating camera poses easily as volumetric rendering (NeRF~\cite{mildenhall2020nerf}). 

\paragraph{3D image synthesis with NeRFs.}

Methods built on implicit
functions, e.g., NeRF~\cite{mildenhall2020nerf}, have been proposed in \cite{chan2021pi,schwarz2020graf,pan2021shading,niemeyer2021giraffe}.
% However, their costly querying and sampling operations limit training efficiency and image resolutions. 
To generate high-resolution images conditioned on the input style latent code, EG3D~\cite{chan2022efficient}, StyleNeRF~\cite{gu2021stylenerf}, VolumeGAN~\cite{xu20223d}, StyleSDF~\cite{or2022stylesdf}, and GMPI~\cite{zhao2022generative} have been developed. In addition, some works such as Sofgan~\cite{chen2022sofgan} and Sem2NeRF~\cite{chen2022sem2nerf} are able to perform multi-view synthesis with NeRF by taking into multi-view or single-view semantic masks.
However, most of these works are not capable of generating 3D objects given purely input text. We will demonstrate the effectiveness of our proposed diffusion prior to serving as a text-to-3D plug-and-play tool into EG3D~\cite{chan2022efficient} and StyleNeRF~\cite{gu2021stylenerf} in this work.

\paragraph{Text-to-3D generation with NeRFs.}

In recent years, several models~\cite{jain2022zero,poole2022dreamfusion,lin2022magic3d} for the task of text-to-3D generation have been proposed using NeRFs~\cite{mildenhall2020nerf}. Dream Fields~\cite{jain2022zero} leverage the pre-trained image-text encoder (\textit{i.e., CLIP}~\cite{radford2021learning}) as the image guidance to optimize the neural implicit scene representations (\textit{i.e., NeRFs}) through online training. Since the pre-trained CLIP models may not be effective image-level generation objectives, some works such as DreamFusion~\cite{poole2022dreamfusion} and Magic3D~\cite{lin2022magic3d} turn to train the NeRF using pre-trained diffusion prior~\cite{saharia2022photorealistic} instead of CLIP.
Though DreamFusion~\cite{poole2022dreamfusion} and Magic3D~\cite{lin2022magic3d} are capable of performing satisfactory 3D content creation and rendering with an open-vocabulary input text prompt, the inference takes 1.5 hours and 40 minutes for each model to train a NeRF from scratch. In this work, we aim to resolve the issue by leveraging the pre-trained NeRFs and diffusion prior, and producing the latent in less than a minute.

% \paragraph{Inversion.}

% GAN inversion~\cite{zhu2016generative} is the process of obtaining a latent code that can allow the generator to reconstruct the given image. Generally, inversion methods either directly optimize the latent feature to minimize the loss for a given image~\cite{abdal2019image2stylegan,abdal2020image2stylegan++,bau2020semantic,gu2020image}, train an encoder on a large number of images to learn a mapping from an image to a style latent~\cite{alaluf2021restyle,guan2020collaborative,kang2021gan,kim2021exploiting,pidhorskyi2020adversarial,richardson2021encoding,tov2021designing,wang2022high}, or use a hybrid approach leveraging both methods~\cite{zhu2016generative,zhu2020indomain}. For the encoder-based methods, pSp~\cite{richardson2021encoding} proposes a feature pyramid encoder into $\mathcal{W}+$ space. ReStyle~\cite{abdal2019image2stylegan} iteratively refines the predicted style latent through a few forward passes. However, these effective approaches are designed for 2D StyleGAN. Recently, IDE-3D~\cite{sun2022ide} propose an inversion approach for a 3D neural renderer with semantic masks yet is not able to generalize to several pre-trained style-based NeRFs. In this work, we would like to design a NeRF inversion model which is generalizable for most style-based NeRFs.