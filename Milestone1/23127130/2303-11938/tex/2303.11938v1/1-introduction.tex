\section{Introduction}





%  Tackle text-to-3D. existing works text-to-3d and challenges. 

We aim the tackling the task of text-to-3D domain-specific content creation. 3D content can be represented with neural radiance field (NeRF)~\cite{mildenhall2020nerf} in a photorealistic way. Currently, text-to-3D with NeRFs have been explored in DreamField~\cite{jain2022zero}, DreamFusion~\cite{poole2022dreamfusion}, or Magic3D~\cite{lin2022magic3d}. By leveraging the pre-trained models from CLIP~\cite{radford2021learning} or diffusion priors~\cite{saharia2022photorealistic} as the learning objective, these works are capable of producing 3D content given the input text prompt through training a NeRF from scratch. However, training one individual model for each text prompt would cause the first issue: slow inference (\textit{i.e., around one hour}) for the above models. Due to no real image guidance during updating, the model would lead to the second issue: low-resolution multi-view rendering. 



% NeRF inversion of 3D style-based generative radiance fields, which typically combine neural radiance field (NeRF)~\cite{mildenhall2020nerf} with the generative adversarial network (GAN)~\cite{goodfellow2014generative}. NeRF inversion is similar to GAN inversion (\cite{zhu2016generative}) which learns a mapping function to project an image into the GAN's latent space. Currently, GAN inversion has been successfully explored in StyleGANs~\cite{karras2019style,Karras2019stylegan2} (\textit{e.g.,} StyleGANv2) which has been used for image synthesis, and enables flexible control of the latent space. Several approaches of GAN inversion are capable of inverting the input image into the latent space (\textit{i.e.}, $\mathcal{W}$ space)~\cite{jahanian2019steerability,shen2020interpreting,tewari2020stylerig,harkonen2020ganspace} or extended latent space (\textit{i.e.}, $\mathcal{W+}$ space: concatenation of all $\mathcal{W}$ latent code from each layer)~\cite{abdal2019image2stylegan,abdal2020image2stylegan++,zhu2020indomain,abdal2021styleflow} for image editing. 
% % These methods involve a process commonly referred as GAN inversion~\cite{zhu2016generative}, where one first inverts an image into StyleGANâ€™s latent space and then obtain a new code that is then used by StyleGAN to reconstruct or generate the output image. 
% Recently, IDE-3D~\cite{sun2022ide} directly employ a basic $\mathcal{W+}$ encoder for the inversion of their proposed 3D neural renderer with semantic masks. 
% However, the exploration of generalized inversion approaches for style-based NeRFs is still limited. 

% Multi-view synthesis from single-view synthesis.

\begin{figure}[t!]
  \centering
  \includegraphics[width=\linewidth]{fig/teasor.pdf}
  % \vspace{-6mm}
  \caption{\textbf{Comparison of text to content creation with pre-trained GANs}. Compared with existing effective text-to-2D with StyleGANs~\cite{pinkney2022clip2latent}, it is more challenging to perform text-to-3D with latent-based NeRFs via diffusion process since the denoised latent is assumed to be view-invariant. }
  \label{fig:teasor}
  \vspace{-4mm}
\end{figure}

Recently, 3D latent-based models using radiance fields (\textit{i.e.}, NeRFs~\cite{mildenhall2020nerf}), such as EG3D~\cite{chan2022efficient} or StyleNeRF~\cite{gu2021stylenerf}, have been proposed for unsupervised generation of multi-view consistent images. Similar to StyleGANs~\cite{karras2019style,Karras2019stylegan2}, these NeRFs learn a controllable $w \in \mathcal{W}$ space and enable explicit 3D camera control, using only single-view training images. 
To achieve fast text-to-3D creation, one straightforward way is to produce the $w$ latent from the input text prompt without further updating the NeRF model. This idea has been explored in clip2latent~\cite{pinkney2022clip2latent} for text-to-2D creation, which takes less than one minute to generate high-resolution realistic images from the input text prompt with a trained diffusion prior. However, there are two main challenges for directly applying the clip2latent on 3D latent-based NeRF. First, the latent space $w \in \mathcal{W}$ is assumed to be view-invariant for 3D NeRFs which is different from 2D StyleGANs. That is, if only single-view prompts (image CLIP embeddings) are used to train the diffusion prior, the produced latent code may be only valid when generating images of the same view (camera pose) where this issue is also noted in NeRF-3DE~\cite{li20223d}. To address this issue, we can use multi-view CLIP prompts from the same latent $w$ to train the diffusion prior. However, this leads to our second challenge: how to ensure the diffusion process to produce view-invariant latent from multi-view CLIP embeddings, as shown in Figure~\ref{fig:teasor}.


To properly achieve fast and realistic text-to-3D creation with pre-trained NeRFs, we propose a framework named 3D-CLFusion to produce the view-invariant latent code from an input text prompt. Our 3D-CLFusion is composed of a diffusion prior network, a pre-trained clip model, and a pre-trained generator (\textit{i.e., NeRF}). First, in order to produce the latent code $w$ for the generator to render from the input text prompt, we introduce the diffusion prior which takes the CLIP text embedding as a condition and produces $w$ latent in each diffusion step. Since we do not have labeled text embeddings, we leverage the image CLIP embeddings for training the diffusion prior. This training strategy is inspired by clip2latent~\cite{pinkney2022clip2latent} where we believe CLIP text and image embeddings share the closed latent space. Second, in order to learn the view-invariant latent in $\mathcal{W}$ space, we leverage the multi-view images generated by the model itself and contrastive learning to ensure the produced latent code in $\mathcal{W}$ are the same given different CLIP embeddings from a different view.
% It is important to note, the novelty of this model lies in the design of the view-invariant diffusion process with the proper loss function on each diffusion step.
Later in the experiments, we will show the significance of our introduced contrastive learning in the diffusion process. We have demonstrated the effectiveness of the proposed framework for the fast text-to-3D using StyleNeRF~\cite{gu2021stylenerf} and EG3D~\cite{chan2022efficient} as the pre-trained generators in Figure~\ref{fig:show} and the experiments. Compared with DreamFusion~\cite{poole2022dreamfusion} and Magic3D~\cite{lin2022magic3d} which take around one hour for inference with NeRF, our model only takes less than 30 seconds for each 3D object. The contributions of this paper are summarized as follows:

\vspace{-3mm}
\begin{itemize}
\itemsep -1mm
\item We demonstrate the challenges of the task text-to-3D directly using latent-based NeRFs and the limitations of the current models for this task.
\item We propose a framework named 3D-CLFusion, which aims to produce view-invariant latent for rendering 3D objects with NeRFs from the input text prompt.
\item Compared with the existing models leveraging diffusion priors for text-to-2D with pre-trained StyleGANs, our designed model achieves more effective text-to-3D with pre-trained NeRFs.
\item Though the 3D object created by our model is limited to the class of pre-trained model, the inference time is at least 100 times faster than the existing text-to-3D creation from neural rendering.
% \item Our proposed framework can also be applied to online optimization (see appendix), \textit{e.g.}, combining with PTI~\cite{roich2021pivotal}, or on a different NeRF generator (\textit{e.g.}, EG3D~\cite{chan2022efficient}, see appendix)
% generate near perfect view-consistent images.
% \item We also demonstrate that the style latent code generated by our proposed EncSNeRF is able to serve as a good initial point for online optimization.

\end{itemize}
%


