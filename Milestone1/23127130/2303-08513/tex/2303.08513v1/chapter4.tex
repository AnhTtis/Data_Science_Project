
Despite the technical differences illustrated in the previous section, both finite-element and finite-volume solvers tackle nonlinear problems by use of an internal iteration loop that repeats the assembly and (partial) solution of a linearized equation system for each iteration.
%
For the sake of a simpler nomenclature, in the following the term \textit{\subproblemIter s} will be used to refer to either Newton iterations or fixed-point iterations.

This section discusses the influence of these \subproblemIter s on the computational cost of partitioned schemes for fluid-structure interaction.
Moreover, it proposes a new criterion to evaluate the coupling loop's convergence solely based on the subproblem residuals.\\

\mypara For a clear notation, 
this work uses an uppercase $N$ for all iteration counts that refer to the whole simulation, like the total number of coupling iterations
$\totalCoupleIter$.
% 
% like $\totalCoupleIter$, $\totalProblemIter^f$, or $\totalProblemIter^s$.
In contrast, a lowercase $n$ stands for the number of \subproblemIter s within a flow or solid solver call,
$n^f$ and $n^s$.
%as well as their imposed upper limits
% $\iterPerCall^f$ and $\iterPerCall^s$.
% 
The \subproblemIter\ index $i$ restarts from $1$ for each solver call,
the coupling iteration index $k$ for each time step.
% 
% 
% Occasionally, 
% Additionally,
Additionally,
for a simpler derivation
\Sec{costMeasure} uses
the index $\kbar$ to iterate
over all $\totalCoupleIter$ coupling steps 
of the full simulation.

% coupling iteration will also be 

% Additionally, the index $\kbar$
% will be used to identify
% but $\kbar$
% 

\subsection{Cost measure} \label{sec:costMeasure}

It is common practice in partitioned fluid-structure interaction simulations to treat the two solvers as black boxes
of which only the in- and output are known, but not their interior properties and functionalities.
The computational cost of a specific scheme is then quantified by the number of coupling iterations required for convergence.
Although this measure is motivated by the assumption that solving the subproblems is much more expensive than the data exchange or any other step of the coupling,
it in fact also presumes the cost of one solver call to be constant.
%

This may not be in line with empirical observations,
% indicating that the s
but 
following the black-box perspective in the strictest sense possible, i.e., 
% Following the black-box idea in the strictest sense possible,
if the interior workings of the solvers are completely unknown,
this is indeed the best guess.
%
In practice, however,
it is almost always known 
whether the subproblems are nonlinear and whether
the solvers use any standard discretization technique
like finite volumes or elements.
% without compromising the black-box concept.
% In practice, however, this is typically not the case



As discussed in \Sec{subproblems}, both FV and FE solvers 
% use an internal iteration loop that 
repeat the most expensive steps, the assembly and (partial) solution of the linearized system, for every \subproblemIter.
% 
It is therefore expected that a significant part of the cost of a solver call scales with the number of \subproblemIter s it performs.
Consequently, this work proposes to estimate the computational cost of calling a subproblem $p$ in coupling iteration $\kbar$ by
\begin{align}
    % \leftIdx{k}{\costFac{SolverCall}}^p \approx \costFac{Const}^p + \leftIdx{k}{n^p} \cdot \costFac{Iteration}^p ~,
    \costSolver^p(\kbar) \approx \costFix^p + {n^p}(\kbar) \cdot \costIter^p ~, \label{equ:CostSolverCall}
    % \costFac{SolverCall}(k) \approx \costFac{Const}(p) + {n}(k,p) \cdot \costFac{Iteration}(p)~,
\end{align}
where 
% $\costFix^p$ is the constant cost contribution,
$n^p(\kbar)$ is the number of \subproblemIter s run for this solver call.
% and $\costIter^p$ the cost for one of these iterations.
The constant cost $\costFix^p$ accounts for all operations that are executed
once per solver call, 
which can for example include updating the mesh or computing the fluid loads.
% 
In contrast, $\costIter^p$ represents all cost contributions that are incurred for each subproblem iteration,
i.e., in particular assembling and solving the linear system.
% 
% 
% The cost factors $\costFix^p$ and $\costIter^p$
% account for all 
% that are perform
% 
Note that the costs factors $\costFix^p$ and $\costIter^p$ are assumed independent from the iteration or time step.
The validity of this assumption will be confirmed by the results in \Sec{results}.

Concerning the full run time of the simulation $\timeSimulation$,
% 
the cost measure $\costSimulation$ is obtained by summing up the cost of solver calls and data transfer over the total number of coupling iterations $\totalCoupleIter$, yielding
\begin{align}
    % \costFac{Simulation} \approx \totalCoupleIter \cdot \costFac{DataTransfer} + \sum_{k=1}^{\totalCoupleIter} \sum_{p=f,s}  \costFac{SolverCall}^p(k)
    \timeSimulation \approx \costSimulation = \sum_{\kbar=1}^{\totalCoupleIter} \left[ \costCouple + \sum_{p=f,s}  \costSolver^p(\kbar) \right] = \totalCoupleIter \cdot \costCouple + \sum_{\kbar=1}^{\totalCoupleIter} \sum_{p=f,s}  \costSolver^p(\kbar) ~, \label{equ:CostSimulation}
\end{align}
% 
where the cost $\costCouple$
of the data transfer and update techniques,
like relaxation or IQN, per coupling iteration
was considered 
constant. 
Note that the index $\kbar$ running over all
coupling iterations is used to avoid
% Note that in \Equ{CostSimulation} the coupling iteration index $k$ runs over all $\totalCoupleIter$ coupling iterations of the whole simulation to avoid 
introducing an additional time step sum. 
% In the remainder of the work, however, it starts from $1$ for every new time step.
% \footnote{Note that while the coupling iteration index $k$ normally starts from $1$ within every new time step, the sum in \Equ{CostSimulation} is running over all $\totalCoupleIter$ iterations of the whole simulation to avoid introducing an additional time step sum.}.



The cost measure used in this work is obtained by inserting \Equ{CostSolverCall} into \Equ{CostSimulation} and regrouping the terms based on whether they are scaling with the number of coupling iterations or the number of \subproblemIter s:
% \todo{Check whether directly starting from total numbers is better.}
% NOTE: Let's see which steps we can skip...
\begin{alignat}{2} \label{equ:equivalentTime}
    \costSimulation & = 
    % Step 0:
    % \totalCoupleIter \cdot \costFac{DataTransfer} + \sum_{k=1}^{\totalCoupleIter} \sum_{p=f,s}  \costFac{SolverCall}^p(k) ~,
    % Step 1:
    \totalCoupleIter \cdot \costCouple ~~+~ \sumCplIter \sumProblems \big[ \costFix^p && ~+~ n^p(\kbar)  \cdot \costIter^p \big] 
    \\ \nonumber
    % Step 2:
    % & = \totalCoupleIter \cdot \costFac{DataTransfer} ~~+~ \sumCplIter \sumProblems \costFac{Const}^p &&~+~ \sumCplIter \sumProblems  n^p(k) \cdot \costFac{Iteration}^p \\ \nonumber
    % Step 3:
    % & = \totalCoupleIter \cdot \costFac{DataTransfer} &&+ \sumProblems \costFac{Const}^p \sumCplIter 1 &&+ \sumProblems \sumCplIter n^p(k) \cdot \costFac{Iteration}^p \\ \nonumber
    % Step 4:
    % & = \totalCoupleIter \cdot \costFac{DataTransfer} ~~+~ \totalCoupleIter \sumProblems \costFac{Const}^p &&~+~ \sumProblems \costFac{Iteration}^p \sumCplIter n^p(k) \\ \nonumber
    % Step 5:
    & = \totalCoupleIter \cdot \underbrace{\big( \costCouple  ~+~ \sumProblems \costFix^p \big)}_{=: \,\costCoupleSum} &&~+~ \sumProblems \costIter^p \underbrace{\sumCplIter n^p(\kbar)}_{=: \, \totalProblemIter^p} \\ \nonumber
    % Step 6:
    & = \totalCoupleIter \cdot \costCoupleSum ~+~ \sumProblems \totalProblemIter^p \cdot \costIter^p       
\end{alignat}
% 
The newly introduced variables $\totalProblemIter^f$, $\totalProblemIter^s$, and $\costCoupleSum=\costCouple+\costFix^f+\costFix^s$ represent the total number of \subproblemIter s of problem $f$ or $s$ 
and all costs occurring once per coupling iteration, respectively.

The cost factors $\costIter^p$ and $\costCoupleSum$
are prescribed by the simulation framework, computer architecture, problem sizes, etc. and therefore considered
as given constants,
so that the cost measure boils down to a weighted sum of the iteration counts $\totalCoupleIter$, $\totalProblemIter^f$, and $\totalProblemIter^s$.
% As the data transfer is usually cheaper than the \subproblemIter s, 
% As often $\costFac{Coupling}<<\costFac{Iteration}^i$ holds,
% 
An efficient partitioned scheme therefore not only has to reduce the total number of coupling iterations, but in particular also the
total number of \subproblemIter s. \\

\mypara In partitioned FSI, the mesh update of the flow problem is sometimes interpreted as a third problem besides fluid and structure.
While this viewpoint could easily be integrated into \Equ{CostSimulation} by adding the mesh update to the sum over the subproblems,
it will be included in the flow solver's cost within this work, for the sake of a cleaner notation.
This is equivalent as long as the number of mesh updates scales with $\totalCoupleIter$ (or $\totalProblemIter^f$).

\subsection{Iterations per solver call} \label{sec:IterationsPerCall}

A straightforward and simple way to influence the total number of \subproblemIter s $\totalProblemIter^f$ and $\totalProblemIter^s$ performed throughout the whole simulation
is to limit the number of \subproblemIter s per solver call.
% 
More precisely, each solver call may only perform up to $\iterPerCall^f$ or $\iterPerCall^s$  \subproblemIter s,
% 
rather than always iterating until full convergence is reached.
% \todo{maybe introduce notation for iteration per solver call here?}

This naturally raises the central research question investigated in this work:
% In a partitioned algorithm (for FSI),
% how many \subproblemIter s are best run per solver call
% to minimize computational cost?
how do the maximum numbers of \subproblemIter s per solver call, $\iterPerCall^f$ and $\iterPerCall^s$,
% And how does the number
% of a partitioned algorithm?
% How does the number of \subproblemIter s run per solver call
influence the total computational cost of a partitioned algorithm?
% And is there an optimal number to ensure an efficient simulation?
% 
% While it might sound like a quite simple question at first,
The question might sound simple at first, but the relation between the \subproblemIter s per solver call and the computational cost is non-trivial:
% 
\begin{itemize}
    \item If fewer \subproblemIter s are performed per solver call,
    the solvers exchange data more frequently, so that
    % RHS vector stays up to date,
    the boundary conditions at the FSI interface and with it 
    the RHS $\bv$ stay up to date,
    improving the individual quality of each \subproblemIter.
    Aside from the higher communication cost,
    however, this also brings the risk of feeding back inaccurate data into the coupling loop and, if applicable, in the quasi-Newton Jacobian approximation, 
    % which uses differences between interface data, 
    which may result in slower convergence, i.e., a higher $\totalCoupleIter$, or even divergence.
    
    \item Running more \subproblemIter s per solver call, on the other hand,
    increases the accuracy of the exchanged data fields
    and the IQN method's input-output pairs,
    typically resulting in a reduced number of coupling steps $\totalCoupleIter$.
    On the downside, computational time is misspend on polishing a solution for which the boundary conditions are still incorrect, so that it will be overwritten in the next coupling step anyway.
\end{itemize}
% 
Good choices of the \subproblemIter s per flow and solid solver call, $\iterPerCall^f$ and $\iterPerCall^s$,
should balance these opposing trends to reduce the overall computational cost. \\

% \mypara For a clear notation, 
% this work uses an uppercase $N$ for all iteration counts that refer to the whole simulation, like $\totalCoupleIter$, $\totalProblemIter^f$, or $\totalProblemIter^s$.
% In contrast, a lowercase $\iterPerCall$ denotes the maximum number of \subproblemIter s within a solver call,
% %$n^f$ and $n^s$,
% %as well as their imposed upper limits
% $\iterPerCall^f$ and $\iterPerCall^s$.
% % 
% The \subproblemIter\ index $i$ restarts from $1$ for each solver call, the coupling iteration index $k$ restarts for each time step.

\subsection{Convergence criterion}
\label{sec:convergence_criterion}

It is common in partitioned FSI simulations to determine the convergence of the coupling loop by comparing a norm of the fixed-point residual $\Rk{k}$
to some tolerance $\varepsilon^c$, see for example
\cite{Kuttler2008,Degroote2013b,scheufele2017robust,Schussnig_2022}.
% 
Although 
both \subproblemIter\ loops have to converge as well
for accurate results,
% 
% both \subproblemIter\ loops to satisfy
% their convergence bounds $\varepsilon^f$ and $\varepsilon^s$,
% 
% the associated residual bounds $\varepsilon^f$ and $\varepsilon^s$ are rarely included explicitly into the 
% convergence criterion,
these conditions are rarely accounted for explicitly,
as they are inherently fulfilled in case all solver calls
iterate to full convergence.
%
Limiting the \subproblemIter s per solver call
to $\iterPerCall^f$ or $\iterPerCall^s$,
however, requires a method to make sure that both subproblems satisfy their residual tolerances $\varepsilon^f$ and $\varepsilon^s$
before going to the next time step.
% 
Unfortunately, the influence of the fixed-point tolerance $\varepsilon^c$ is rarely discussed in literature,
let alone its interplay with the subproblem residual tolerances $\varepsilon^f$ and $\varepsilon^s$.

%To tackle t2his problem
To tackle this issue, this work proposes a novel
convergence criterion for partitioned algorithms that does not
introduce any coupling tolerance $\varepsilon^c$.
Instead, it relies solely on the subproblem residuals.
%
As an added benefit, the new criterion allows for a fair comparison of convergence rates,
independent from the number of \subproblemIter s per solver call.
The remainder of this section derives the proposed convergence criterion.\\


% % 
% % This subproblem convergence often is not checked for
% While the \subproblemIter  convergence is inherently satisfied in case all solver calls iterate to full convergence,
% running only $\iterPerCall^f$ or $\iterPerCall^s$ \subproblemIter s requires to explicitly 
% check for this to allow for a fair comparison of runs.
%  This new convergence criterion has two main advantages:
% (1) It does not introduce any bound for the relative change of the interface data as it is typically used in the partitioned FSI community. Instead, it relies solely on the convergence criteria the two subproblems have anyway.
% This way, the issue of how to relate these bounds is avoided. % the issue of an appropriate choice for this bound is resolved
% (2) It allows for a fair comparison of simulation results produced for different numbers of \subproblemIter s per solver call,
% like in the parameter study of \Sec{results}. \\ \todo{More scientific explanation of advantages?}





% \vspace{1cm}
%To ensure the comparibility of results for 
% As discussed in Section \ref{Sec:Chapter3}, both subsolvers of the partitioned FSI scheme solve a nonlinear problem of the form $A(u) u = b$,

% Both subsolvers of the partitioned FSI scheme, no matter whether FV- or FE-based,
As explained in \Sec{subproblems}, both finite-element and finite-volume solvers
iteratively solve a nonlinear problem of the form $\Am(\uv)~\uv = \bv$. 
In other words, while they account for a nonlinearity of the system matrix $\Am$, the right-hand side vector $\bv$ is assumed to stay unchanged throughout the \subproblemIter s.
% 
In a coupled FSI problem, however, the right-hand sides of both subproblems in fact also depend on $\uv$, even if only in an implicit manner.
% 
For example, the traction and pressure forces exerted by the fluid onto the solid change with the deformation state.
Analogically, updating the flow field alters the solid deformation and with it the fluid's boundary position and velocity.
% Due to the implicit nature of this nonlinearity,
In practice, 
this implicit nonlinearity of the RHS is impossible to explicitly account for when coupling two black-box
solvers in a partitioned algorithm.

An interesting consequence of this realization is that only the first \subproblemIter\ of a solver call uses the correct RHS vector $\bv$, because
% 
in the first iteration, both the system matrix $\Am$ and RHS $\bv$ refer to the current solution $\uv^0$, i.e., the initial value for the subproblem solve.
The subsequent \subproblemIter s, on the other hand, inevitably lack any contribution of the other problem to $\bv(\uv)$
that would
follow from the change of $\uv$ within the current solver call, as only the system matrices are updated to the new solution $\uv^i$, while $\bv=\bv(\uv^0)$ stays unchanged\footnote{As mentioned in \Fnt{implicit} (page~\pageref{fnt:implicit}), it is assumed that all dependence on $\uv$ inherent to the subproblem is treated implicitly, i.e., within the system matrix $\Am(\uv)$.}.
%
% To illustrate this effect, note that for {\subproblemIter}$i$, we solve 
% This effect can be illustrated via the linearized systems solved in the \subproblemIter s: \todo{Check whether too much and clean indices.}
The effect is sketched in \Tab{NonlinearRHS}. 

\input{Tables/Table_NonlinearRHS}

As a result, only the residual of a solver call's first \subproblemIter, 
$\vekt{r}_p^0 = 
\bv (\uv^0) - \Am (\uv^0) \uv^0$, allows to draw conclusions about the convergence of the coupling loop, because it quantifies to which extent the subproblem at hand already balances the current coupling data, i.e.,
the coupling data that resulted from the current subproblem solution.
% already balances the coupling data that resulted from its current solution.
% 
For example, the solid solver's first residual is the difference between the Cauchy stresses and the
external loads that the flow solver determined with the current deformation state as boundary condition.
% 
The \subproblemIter s $i=2,3,\cdots$, in contrast, merely attempt to find the converged solution for a defective right-hand side vector $\bv = \bv(\uv^0) \neq \bv(\uv^{i-1})$.
% by mimizing $\vect{r} = \bv - \Am (\uv^i) \uv^i$$

This motivates the idea to trigger the convergence of the coupled problem via the subproblem residuals of the first \subproblemIter:
%
the coupling iteration is considered converged, if for all subproblems the residual of the first \subproblemIter\ satisfies the respective tolerance $\varepsilon^f$ or $\varepsilon^s$.
% 
The different iteration loops and the new convergence criteria are illustrated by \Fig{IterationLoops}.

\begin{figure}[ht]
	\centering
	\resizebox{0.99\textwidth}{!}{
		\input{Figure_IterationLoops}
	}
	\caption{Illustration of the different iteration loops considered in this work. While there is only one coupling loop managing the solver calls,
	each subproblem has its own internal iterations. 
 Note that the pseudo code $\mathbf{u}^i \,= \mathbf{A}^{i-1} \, \backslash \, \mathbf{b}$ 
 represents solving either \Equ{linearizedFE} for a Newton iteration
 or \Equ{linearizedFV} for a fixed-point iteration. Similarly, 
 $\norm{\resFlow^{i-1}} < \varepsilon ^{f}$ is merely a symbolic notation
 for the convergence checks discussed in \Sec{subproblems}. 
 % Lastly, the exchanged interface data may or may not be modified by an interface quasi-Newton update.
 Lastly, the indicated quasi-Newton update is of course optional.
 } \label{fig:IterationLoops} 
\end{figure}

To integrate the new criterion into an existing black-box FSI framework,
it is typically simplest to check after each solver call whether (1) the subproblem has converged
and (2) only one \subproblemIter\ was run. Only if both conditions are fulfilled for flow and solid solver in the current coupling iteration,
the time step has converged. \\

% This new convergence criterion has two main advantages:
% (1) It does not introduce any bound for the relative change of the interface data as it is typically used in the partitioned FSI community. Instead, it relies solely on the convergence criteria the two subproblems have anyway.
% This way, the issue of how to relate these bounds is avoided. % the issue of an appropriate choice for this bound is resolved
% (2) It allows for a fair comparison of simulation results produced for different numbers of \subproblemIter s per solver call,
% like in the parameter study of \Sec{results}. \\ \todo{More scientific explanation of advantages?}

\mypara Note that in \Fig{IterationLoops} the solution of the subproblem is updated after the calculation of the residual, but before convergence is checked.
As a consequence, one more update is performed even though the \subproblemIter\ converged.
It would also be possible to exit the \subproblemIter\ upon convergence.
However, this is often impossible for black-box solvers and, 
moreover, the accuracy of the coupled solution would then be prescribed by
the subproblem convergence tolerance that is satisfied first, rather than the one satisfied last. \\


\mypara The interpretation of the first subproblem residual within a solver call as a measure for the coupling loop's
convergence is in fact only exact if the exchanged data fields are not modified in between the solvers.
% 
%i.e., no update step is applied.
% i.e., no update step like $\vect{d}^{k+1}=\vect{\tilde{d}}^k + \Delta \vect{d}$ is applied.
% 
Although this modification is common in practice
and for example part of relaxation or IQN methods (\Sec{IQN}), 
%which apply an update step, see for example \Sec{2.5}. 
%In practice, however, the update increment is typically proportional to the fixed-point residual $\vect{R}^k$, 
%e.g. for relaxation or IQN methods,
the update increment is proportional to the fixed-point residual $\Rk{k}$ in these approaches
and therefore
vanishes upon convergence, 
as will be confirmed in \Sec{results}.
Consequently, it is safe to neglect the effect of an update step within the proposed convergence criterion. \label{Remark:Increment}
