{
    "arxiv_id": "2303.08789",
    "paper_title": "PLEX: Making the Most of the Available Data for Robotic Manipulation Pretraining",
    "authors": [
        "Garrett Thomas",
        "Ching-An Cheng",
        "Ricky Loynd",
        "Vibhav Vineet",
        "Mihai Jalobeanu",
        "Andrey Kolobov"
    ],
    "submission_date": "2023-03-15",
    "revised_dates": [
        "2023-03-16"
    ],
    "latest_version": 1,
    "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG"
    ],
    "abstract": "A rich representation is key to general robotic manipulation, but existing model architectures require a lot of data to learn it. Unfortunately, ideal robotic manipulation training data, which comes in the form of expert visuomotor demonstrations for a variety of annotated tasks, is scarce. In this work we propose PLEX, a transformer-based architecture that learns from task-agnostic visuomotor trajectories accompanied by a much larger amount of task-conditioned object manipulation videos -- a type of robotics-relevant data available in quantity. The key insight behind PLEX is that the trajectories with observations and actions help induce a latent feature space and train a robot to execute task-agnostic manipulation routines, while a diverse set of video-only demonstrations can efficiently teach the robot how to plan in this feature space for a wide variety of tasks. In contrast to most works on robotic manipulation pretraining, PLEX learns a generalizable sensorimotor multi-task policy, not just an observational representation. We also show that using relative positional encoding in PLEX's transformers further increases its data efficiency when learning from human-collected demonstrations. Experiments showcase \\appr's generalization on Meta-World-v2 benchmark and establish state-of-the-art performance in challenging Robosuite environments.",
    "pdf_urls": [
        "http://arxiv.org/pdf/2303.08789v1"
    ],
    "publication_venue": null
}