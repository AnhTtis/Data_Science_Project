\begin{abstract}
A rich representation is key to general robotic manipulation, but existing model architectures require a lot of data to learn it. Unfortunately, ideal  robotic manipulation training data, which comes in the form of expert visuomotor demonstrations for a variety of annotated tasks, is scarce. In this work we propose \appr, a transformer-based architecture that learns from task-agnostic visuomotor trajectories accompanied by a much larger amount of task-conditioned object manipulation \emph{videos} -- a type of robotics-relevant data available in quantity. The key insight behind \appr\ is that the trajectories with observations and actions help induce a latent feature space and train a robot to execute task-agnostic manipulation routines, while a diverse set of video-only demonstrations can efficiently teach the robot how to plan in this feature space for a wide variety of tasks. In contrast to most works on robotic manipulation pretraining, \appr\ learns a generalizable sensorimotor multi-task policy, not just an observational representation. We also show that using relative positional encoding in \appr's transformers further increases its data efficiency when learning from human-collected demonstrations. Experiments showcase \appr's generalization on \mw\ benchmark and establish state-of-the-art performance in challenging \rsrw\ environments.
\end{abstract}