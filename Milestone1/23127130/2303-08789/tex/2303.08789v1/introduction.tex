\section{Introduction}

The use of transformers~\cite{vaswani2017attention} has lead to breakthroughs in training large-scale general-purpose representations for computer vision (CV) and natural language processing (NLP). In NLP, it has yielded models with strong zero-shot performance on unseen tasks, which can be further improved by finetuning only a tiny fraction of these models' weights on a small number of examples~\cite{brown2020gpt3}. At the same time, despite impressive progress, transformer-based representations haven't shown the same versatility in decision-making for embodied agents such as robotic manipulators. 

While some attribute this gap to the lack of suitable training data for these scenarios~\citep{fm2021}, we argue instead that data relevant to training robotic manipulation models is copious. Nonetheless, existing data has important structures ignored by most training methods, especially by certain design decisions that have been carried over from transformer-based sequence models for NLP. As a result, existing methods utilize only a subset of the robotic manipulation data, hurting data efficiency. These insights lead us to propose a novel transformer-based model architecture called \emph{\appr}, tailored to the realities of robotic manipulation training data and capable of using this data for attaining strong zero-shot performance and effective finetuning. 

We observe that robotics-relevant data falls into three categories. \textbf{(1)} The most plentiful category is comprised of ``in-the-wild'' video datasets \cite{youtue8m,ho2022imagenvideo, grauman2022ego4d}. Some of them, e.g., Epic Kitchens~\cite{Damen2018EPICKITCHENS,Damen2022RESCALING}, focus on object manipulation. In aggregation, these datasets cover an immense variety of tasks and are typically annotated with descriptions of the activities depicted in the videos but contain no explicit action information that a robotic arm could use to mimic them. \textbf{(2)} The second category consists of matching sequences of percepts and actions. In some datasets of this type, these sequences don't correspond to meaningful tasks, having been generated by a scripted exploration policy~\cite{robonetv1}. In others, they come from well-defined tasks, but even in the largest such datasets, e.g., ~\citet{robonetv2}, the task coverage is modest compared to video-only data such as~\cite{grauman2022ego4d}. Nonetheless, these sensorimotor sequences capture valuable correlations between a robot's actions and changes in the environment. \textbf{(3)} The third data category, the scarcest one, consists of high-quality demonstrations gathered on-demand for a target task in a target environment. Thus, a scalable model architecture for robotic manipulation must be able to learn \emph{primarily} from videos, while being extra data-efficient on sensorimotor training sequences and the available demonstrations from the target environment.

\appr, the \textit{\textbf{pl}}anning-\textit{\textbf{ex}}ecution architecture we propose, achieves exactly that. A \appr\ model has two major transformer-based components: \textbf{(I)} a task-conditioned {observational} planner that, given a task specification and an estimate of the current world state, determines the next state to which the robot should attempt to transition, and \textbf{(II)} an executor that, having received the desired next state from the planner, produces an action that should lead there from the current state. The training process is asymmetric: the executor is trained by optimizing an inverse dynamics loss over exploratory sensorimotor data of the aforementioned category \textbf{(2)}, while the planner is trained by minimizing a loss of its autoregressive predictions computed with respect to video-only trajectories of category \textbf{(1)}. The target-task data of category \textbf{(3)} can be optionally used to efficiently finetune the planner, the executor, or both.

We make three design choices that greatly help the data efficiency of \appr's training:

\emph{Learning to plan in the observation embedding space.} Rather than generating videos of proposed task execution using, e.g., stable diffusion as in~\citet{ho2022imagenvideo,du2023unipi}, \appr\ learns to plan in the low-dimensional space of observation embeddings. The executor learns its inverse dynamics model in this latent space as well.

\emph{Asymmetric learning of the embedding space.} The space in which the executor and the planner operate is induced either by a pretrained frozen feature-rich observation encoder, such as DINO~\cite{caron2021emerging}, or by the executor's loss \emph{only} --- the planner's training doesn't shape the latent space in any way. Besides preventing a potential latent space collapse, keeping the planner's gradients from entering observation encoders makes training significantly cheaper.

\emph{Relative positional encodings.} To indicate the ordering of elements in an input sequence, many transformers use \emph{positional encodings}~\cite{vaswani2017attention}, with \emph{absolute} positional encodings being the most common. NLP researchers also proposed a so-called \emph{relative} encoding scheme but have found it to give little advantage over the absolute one in large language models~\cite{orig}. In this work, we empirically show that in robotic manipulation, relative positional encoding significantly improves training efficiency from human-collected data, and we adopt it as part of \appr.

Among prior work, most approaches that use video-only demonstrations for pretraining in robotic manipulation produces purely visual representations~\cite{yen2020see,chen2021learning,nair2022r3m,Radosavovic2022RWRL}. The majority of those that produce sensorimotor models need most or all of the video demonstrations to be accompanied by action sequences that generated the videos, which is the case for only a small fraction of the available manipulation-relevant video data~\cite{mcil21rss,jang2021bc,mandi2021generalizable,gato,robonetv2,nasiriany2022learning,rt2022}. Few approaches for robotic manipulation have a dedicated planning component trainable separately from a controller. One exception are approaches that plan in a skill space~\cite{hakhamaneshi2021hierarchical,ren2021generalization,xihan2022skill,nasiriany2022learning,mees2022matters}, which \appr\ can be modified to do as well. Conceptually, \appr\ falls under the paradigm of learning from observations (LfO), but existing LfO approaches don't have multitask zero-shot planning capability~\cite{nair2017knot,rados2021soil,pathak2018zs,vpt2022} or demostrate it only in low-dimensional environments across similar tasks~\cite{xu2022pgiorl}. Of the works that have used transformers for robotic manipulation~\cite{dasari2020transformers,kim2021transformer,mees2022matters,gato,rt2022}, only \citet{rt2022} have analyzed their data efficiency, and none have looked at positional embeddings as a way to improve it. Overall, the closest approach to \appr\ is the concurrently proposed UniPi~\cite{du2023unipi}. It also has counterparts of \appr's planner and executor, but its planner operates in the image space using  diffusion~\cite{ho2022imagenvideo}, which is expensive in terms of compute and data and may fail to model manipulation-relevant object 3D structure consistently~\cite{ho2022imagenvideo}.

In summary, our work makes the following contributions:
\begin{itemize}[leftmargin=*]
\item We introduce the transformer-based \appr\ architecture, which exploits the structure of realistically available robotic manipulation-relevant data for data-efficient training of sensorimotor representations for this domain.
%
\item We empirically show on the challenging Robosuite/Robomimic~\cite{robosuite,robomimic2021} and  benchmark that, contrary to conclusions from NLP, the use of relative positional encodings significantly improves the effectiveness of \appr\ learning from human-collected demonstrations.
%
\item We demonstrate on the multi-task \mw~\cite{mw2019} benchmark that despite pretraining mostly on video-only data, \appr\ exhibits strong zero-shot performance on unseen tasks, which can be further improved by a small amount of finetuning on video demonstrations. 
\end{itemize}
