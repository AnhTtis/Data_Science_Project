\section{Experiments}


\begin{comment}{
\andrey{
To do:
\begin{itemize}
    \item Rel vs abs encoding on Robosuite, 10 seeds: point out that rel encoding helps on human-collected data (possibly because of policy and/or dynamics non-Markovianness). Maybe also mention that it doesn't make much diff. on MW, where data collection policy is scripted?
    \item Try pretraining on RS? Unlikely to help, but maybe worth a try... Using Robomimic video data from real robots too?
    \item Pretraining on MW, 10 seeds: pretrain using 50 0.5-noise trajs as exploratory data (\textbf{done}), maybe try lookaheads of 1 and 3 as well? (\textbf{done}, l=3 didn't work very well),  for K=5 and K=30 (\textbf{done, K=5 works slightly worse than K=30, sticking to the latter}). \textbf{Make sure to check the pretraining loss weights! -- done} For each, \textbf{(1)} evaluate *just the inv.d. part* with a zero vector as the next-state pred., to make sure that inv.d. alone doesn't "memorize" policy (\textbf{done}). \textbf{(2)} Finetune each pretrained model for 10 trajs for 10 seeds using (a) just the video part (\textbf{done}) and (b) video and inv.d. part., last block each.(Make sure to check the pretraining loss weights! (\textbf{done}). 
\end{itemize}
}
\end{comment}

We conduct two sets of experiments that aim to answer the following questions:
\begin{enumerate}[leftmargin=*]
\item[\textbf{(i)}] Does \appr\ pretrained on task-agnostic sensorimotor data and task-annotated video data generalize well to downstream robotic manipulation tasks?
\item[\textbf{(ii)}] How does the use of relative positional encodings affect \appr's policy quality?
\end{enumerate}

\subsection{\appr\ implementation details}

Both sets of experiments use the same \appr\ implementation, whose planning and execution components are based on the GPT-2 flavor of the Decision Transformer (DT) codebase~\cite{chen2021decision}. However, except for the experiment involving DT itself, we don't use rewards or condition on returns to generate a trajectory, unlike DT. \Cref{app:impl} gives more details about our \appr\ implementation. %combined with an $h$-dimensional embedding of proprioceptive state at that is fed into the planner's transformer. is concatenated with the 

%that maps them to $\mathbb{R}^h$, the transformer's $h$-dimensional input space

\subsection{Benchmarks and training data}
\noindent
\textbf{\mw:} \mw~\cite{mw2019} is a collection of 50 tasks, each set in its own environment involving a Sawyer arm. \mw\ provides several task splits. We consider the ML50 split, which consists of 45 training tasks and 5 target tasks:  \emph{\textbf{door-lock}}, \emph{\textbf{door-unlock}},  \emph{\textbf{hand-insert}},  \emph{\textbf{bin-picking}}, and  \emph{\textbf{box-close}}. These 5 target tasks serve for evaluating various \appr\ models. We use \mw's v2 version with image observations, which is described in more detail in \Cref{app:benchmarks}.

\mw\ comes with high-quality scripted policies for all tasks. To get \emph{\textbf{video demonstration data}} ($\mtvd$), we use these scripted policies to generate 100 successful video-only demonstrations for each of the 45 pretraining tasks. Therefore, $|\mtvd|= 4500$ trajectories.  %Thus, in all experiments, $|\mtvd| \leq 45 \cdot 75 = 3375$ trajectories. Most demonstrations have $< 150$ steps. 

To generate \emph{\textbf{visuomotor trajectories}} ($\vmt$) for pretraining \appr's executor, we modify the scripted policies for the \emph{door-lock}, \emph{door-unlock},  \emph{hand-insert},  \emph{bin-picking}, and  \emph{box-close} tasks by adding zero-mean Gaussian noise with standard deviation $\sigma=0.5$ to their recommended actions and recording the altered actions into 50 training trajectories per task, creating $|\vmt| = 250$ noisy trajectories.
%and recording the altered actions into 100 training trajectories per task. This data isn't reflective of a high-quality behavior but is exploratory and well-suited for training an inverse dynamics model. In any given experiment run, we sample at most 50 out of the resulting episodes, so $|\vmt| \leq 50$ noisy trajectories.

Last but not least, for \mw's \emph{\textbf{target-task demonstrations}} ($\ttd$), we employ the original scripted policies to produce 75 demonstrations per target task and sample 10 of them in a finetuning experiment run, i.e., $|\ttd| = 10$ trajectories for each downstream task in every finetuning experiment. Typical demonstration trajectories have 50 --- 150 time steps, depending on the task. \\

%For the 5 target tasks, we also generate 50 trajectories per task using an scripted expert policy with Gaussian noise ($\sigma = 0.5$), which yields a very poor-quality policy for these tasks (this is intentional). 

%After generating 50 trajectories for the target tasks using this noisy policy, we dissociate task specifications (images showing goal scenes) from them. This data serves as our \textbf{\upm} data during pretraining as an sample of dynamics in the 5 target environments but not as a sample of high-quality trajectories for them 

%For the remaining 45 tasks' generated trajectories, we remove the actions and rewards from them, turning them into \textbf{\tapo} pretraining data. Finally, for \emph{expert} (i.e., non-noisy) trajectories for the 5 target tasks, we remove the action data and rewards, thus turning them into \textbf{\tapo} data as well.

\textbf{\rsrw:} The \rsrw\ benchmark~\cite{robosuite}, like \mw, consists of several environments with robotic manipulation tasks but has a significantly more complicated dynamics and action space. We use 9 of its tasks involving a single robot arm (Panda): \emph{\textbf{Lift}}, \emph{\textbf{Stack}}, \emph{\textbf{Door}}, \emph{\textbf{NutAssemblyRound}}, \emph{\textbf{NutAssemblySquare}}, \emph{\textbf{PickPlaceBread}} \emph{\textbf{PickPlaceCan}}, \emph{\textbf{PickPlaceMilk}}, and \emph{\textbf{PickPlaceCereal}}. \rsrw's details are provided in \Cref{app:benchmarks}.



%We explore the role of \appr's relative positional encodings on the \rsrw\ benchmark ~\cite{robomimic2021}. It consists of several challenging manipulation tasks, out of which we experiment with 9 (see \Cref{fig:rsrw}). The remaining two, \emph{Wipe} and \emph{Toolhang}, appear to have issues with dynamics that have prevented us from running data collection on them.

Importantly, training data for \rsrw\ was collected by hand, not generated by scripted policies. \rsrw\ provides a keyboard and SpaceMouse interfaces for controlling the Panda arm in its environments, and Robomimic supplies datasets of 200 expert (``professional-human'') trajectories collected using the SpaceMouse interface for the \emph{NutAssemblySquare}, \emph{PickPlaceCan}, and \emph{Lift} tasks. %Our experiments use at most 75 of these demonstrations per task, sampling them randomly without replacement.
For each of the remaining tasks, we gather 75 high-quality trajectories via the keyboard interface ourselves. We employ \rsrw\ tasks only for experiments that involve training single-task policies from scratch, so all of these trajectories are used as \emph{\textbf{target-task demonstration data ($\ttd$)}}. Typical demonstration trajectory lengths vary between 50 and 300 time steps.

%We employ \rsrw\ tasks only for experiments that involve training single-task policies from scratch and hence need only \emph{\textbf{target-task demonstrations $(\ttd)$}}. Thus, in each such experiment $|\ttd| \leq 75$ per task.




\subsection{Generalization experiments \label{sec:gen_exp}}

\begin{figure*}[t!]
    %\hspace{-0.8in}
    %\centering
    \hspace{0.15in}
    \includegraphics[width=1.0\linewidth]{5_MW_probs_w_zs_w_goal_emb.png}
    \vspace{-0.3in}
    
    \caption{\appr's generalization experiments.}
    \label{fig:mw}
\end{figure*}


In these experiments, we focus on pretraining \appr\ on multi-task \mw\ data. We show that \appr\ pretrained on as few as 4500 video demonstrations and a small amount of exploratory dynamics data from the target environments exhibits good downstream performance on downstream tasks \emph{zero-shot}. Moreover, finetuning only 5\% of \appr's parameters on just 10 \emph{video-only} demonstrations for a given task significantly boosts \appr's success rate there. Last but not least, we show that \emph{video-only demonstrations is all \appr\ needs during finetuning}: full demonstrations, with both video and action sequences, don't increase \appr's performance beyond video-only ones if the dynamics data used during pretraining has sufficient coverage. In summary, \appr\ can perform well without seeing a single sensorimotor expert demonstration during pretraining and finetuning. The results are in \Cref{fig:mw}.


\textbf{Training and evaluation protocol.} Our procedure trains a 16,639,149-parameter \appr\ instance, \emph{including the ResNet-18-based image encoder}, from scratch, beginning with a randomly initialized model. We evaluate \appr's performance in terms of its success rate on downstream tasks. All success rate evaluations are done on 50 measurements starting from initial states sampled from the \mw\ target task's \emph{test} distribution. All training hyperparameters are listed in \Cref{tab:training_params} in \Cref{app:hyp}.


First, we pretrain \appr's \emph{executor} on the dynamics dataset $\vmt$ of 250 exploratory trajectories from the 5 target environments and evaluate the zero-shot success rate of the resulting model, \emph{EX} in \Cref{fig:mw}, on each of the 5 target tasks. Since during evaluation EX needs predictions of future observation embeddings but the planner that would normally provide them hasn't been pretrained yet, we feed the embedding of the task's \emph{goal} image as the predictions.%$\vec{0}$ to EX as the predictions.

Next, we freeze the pretrained executor and observations encoder, and pretrain \appr's planner on the $\mtvd$ dataset of 4500 video-only expert demonstrations from 45 tasks. Note that, with observation encoders frozen, the planner learns to operate in the embedding space induced by the executor. This step yields a fully pretrained \appr\ model, which we evaluate zero-shot on 5 target tasks.

Afterwards, we finetune \emph{the last transformer layer} of the pretrained \appr's planner independently for each target task on $\ttd$ of 10 video-only demonstrations from that task. This amounts to training 926,733 parameters -- $\approx5\%$ of \appr. For comparison, we also finetune \appr\ on 10 \emph{full} (sensorimotor) demonstrations for each task, by finetuning the last transformer layer of \appr's planner, the last layer of its executor, and the action head, which adds up to 1,783,053 parameters ($\approx11\%$ of \appr).  To evaluate the success rate of the finetuned model, we adopt the procedure from \citet{robomimic2021}: after each of 15 finetuning epochs, we measure the average success rate of the resulting model across 50 500-step rollouts, and record the maximum success rate across the 15 epochs. 


The above steps are repeated on 10 seeds and the averaged results along with the corresponding confidence intervals are plotted in \Cref{fig:mw}.\\

\textbf{Results.} Since the exploratory dynamics data $\vmt$ for pretraining \appr\ came from the target task environments, a natural question is: could the executor EX have inadvertently learned to solve the target tasks by exploiting biases in the data collection procedure? The performance of pure pretrained EX in \Cref{fig:mw} (\emph{Pretr. EX only, zero-shot} in \Cref{fig:mw}) shows the level of task knowledge EX gained with the exploratory data and serves as the baseline for the other \appr\ models in this experiment.

The zero-shot downstream performance of fully trained \appr\ (\emph{Pretr. \appr, zero-shot} in \Cref{fig:mw}) showcases the merit of both video demonstration pretraining and \appr's data efficiency. Despite using only 4500 trajectories, \appr\ learns a planning policy that generalizes well to tasks quite dissimilar to those that generated the pretraining videos. For all 5 downstream tasks, this policy outperforms the EX baseline by $\geq 2\times$.

At the cost of finetuning just 5\% of \appr's parameters on 10 video-only target-task demonstrations, the performance of the pretrained policy can see further improvement; in the case of \emph{hand-insert-v2}, \emph{bin-picking-v2}, and \emph{box-close-v2}, a drastic one.

Interestingly, finetuning on 10 \emph{full} trajectories doesn't give additional benefits (compare the last two bars for each target task in \Cref{fig:mw}). We attribute this to two reasons. First, \appr\ learned the inverse dynamics in the observation space region covered by the pretraining dataset $\vmt$ well, and seeing the inverse dynamics from the target tasks ($\ttd$) doesn't improve EX \emph{over the observation space region covered by $\vmt$}. Second, since the image encoder was pretrained only on observations from $\vmt$ and frozen during finetuning, finetuning can't help the encoders learn any extra features for modeling inverse dynamics \emph{over the observation space region covered by $\ttd$}, even if such features would improve \appr's performance. This shouldn't be a problem if the observation encoder's output feature space is rich to begin with, e.g., if it comes from a large encoder such as DINO~\cite{caron2021emerging} pretrained on an internet-scale image dataset.



\subsection{Positional encoding experiments \label{sec:enc_exps}}


\begin{figure*}[t!]
    %\hspace{-0.8in}
    %\centering
    \includegraphics[width=1.0\linewidth]{rs_rel_vs_abs.png}
    \vspace{-0.3in}
    \caption{Data efficiency of \appr's relative positional encoding in single-task mode on \rsrw's single-arm tasks with $|\ttd|$ varying from 5 to 75.  \textbf{\appr} (with relative encodings) in most cases significantly outperforms and at worst matches the performance of its version \textbf{\appr-abs} with absolute positional encodings. Both versions significantly outperform the Decision Transformer~\cite{chen2021decision}, which uses ``global'' positional encodings.}
    \label{fig:rsrw}
\end{figure*}

In the \mw\ experiments, all training data was generated by scripted policies. In real settings, most such data is generated by people teleoperating robots or performing various tasks themselves. A hallmark of human-generated datasets compared to script-generated ones is the demonstration variability in the former: even trajectories for the same task originating in the same state tend to be different. In this section's experiments, we show that in low-data regimes typical of finetuning, using \appr\ with relative positional encoding on human-generated demonstrations yields superior policies for a given amount of training data than using absolute encoding. The results are in \Cref{fig:rsrw}.


\textbf{Training and evaluation protocol.} For each of the 9 \rsrw\ tasks, we train 36,242,769-parameter \appr\ instances with absolute and relative positional encodings. These models' only difference from \appr\ instances for \mw\ is the presence of two ResNet-18 networks in the observation encoder, one for each camera. As for \mw, the encoder is trained from scratch, in order to make our results comparable to Robomimic's~\cite{robomimic2021}, where models use an identical encoder and also train it tabula-rasa. All training hyperparameters are listed in \Cref{tab:training_params} in \Cref{app:hyp}.

For the same reason of comparing our results to Robomimic's, we forego pretraining \appr\ in this experiment. Instead, we train a separate \appr\ model in behavior cloning (BC) mode for a single task at a time using that task's $\ttd$ dataset of full sensorimotor demonstrations. In the BC mode, \appr\ is optimized solely w.r.t. its action predictions' MSE loss, whose gradients backpropagate though the whole network. 

Recall that for most tasks, we have 75 sensorimotor demonstrations. For them, to show \appr's scalability in the amount of training data, we train \appr\ for $|\ttd| =5,10,25, 50,$ and $75$, sampling $\ttd$'s from the set of 75 demonstrations without replacement. 

For \emph{Lift}, \emph{PickPlaceCan}, and \emph{NutAssemblySquare}, Robomimic provides 200 high-quality demonstrations each and results of BC-RNN on subsets of these datasets with $|\ttd| =40, 100,$ and $200$. Therefore, for these problems we train \appr\ for $|\ttd| =5,10,25, 50, 75$, as well as $40, 100,$ and $200$.

As baselines, for each task and dataset sizes from 5 to 75 we also train two flavors of the Decision Transformer (DT)~\cite{chen2021decision}. DT uses what we informally call a \emph{global} positional embedding, which assigns a separate embedding to every timestep up to the problem's maximum time horizon $T$, rather than to every time step in its context (up to size $K \ll T$). In our case, $T = 2000$. This type of positional embedding should be even less data-efficient than absolute, and our experiments confirm this (see the \emph{DT-global} plots in \Cref{fig:rsrw}). We note, however, that \citet{chen2021decision} used rewards and returns when training and evaluating DT. While our \emph{DT-global} instances have the rewards and returns masked out (\appr\ assumes that rewards/returns are unavailable and doesn't use them either), we also train a DT version with \rsrw's rewards. During evaluation, this version's trajectory generation is conditioned not only on the goal image, but also on a return uniformly sampled from the range of returns in $\ttd$. This baseline is called \emph{DT-global(+rew)} in \Cref{fig:rsrw}.

The evaluation protocol is the same as in \mw\ experiments and Robomimic: we train each method for 10 epochs, after each epoch compute the success rate across 50 trajectories (with 700-step horizon) and record the best success rate across the 10 epochs.

For each task, dataset size, and model type, we repeat training on 10 seeds. \\


\begin{table*}[t]
\begin{center}
  \begin{tabular}{|c|c|c|c|c|c|c|}
    %\hline
    \cline{2-7}
    %\multirow{2}{*}{Dataset} &
    \multicolumn{1}{c|}{} &
      \multicolumn{2}{c|}{\emph{Lift}} &
      \multicolumn{2}{c|}{\emph{PickPlaceCan}} &
      \multicolumn{2}{c|}{\emph{NutAssemblySquare}} \\
      %\hline
      \cline{1-7}
    $|\ttd|$ & 75  & 200 & 75 & 200  & 75 & 200 \\
    \hline
   \appr & $100 \pm 0$ & $100 \pm 0$ & $80.4 \pm 5.7$ & $96.6 \pm 4.1$ & $64.0 \pm 4.6$ & $86.0 \pm 6.1$ \\
    \hline
    \appr-abs & $100 \pm 0$ & $100 \pm 0$ & $72.8 \pm 8.0$ & $93.0 \pm 4.7$ & $45.2 \pm 5.7$ & $76.8 \pm 4.9$ \\
    \hline
  \end{tabular}
  \end{center}
  \caption{Performance of \appr\ and \appr-abs as the amount of training data $|\ttd|$ increases from 75 to 200 trajectories. The performance gap between the two is narrower on the larger dataset. For \emph{Lift} and several other \rsrw\ tasks, this trend becomes visible for datasets smaller than 200 (see \Cref{fig:rsrw}.}
  \label{tab:rs_large}
\end{table*}

\begin{table*}[t]
\setlength{\tabcolsep}{0.5em}
\begin{center}
  \begin{tabular}{|c|c|c|c|c|c|c|c|c|c|}
    %\hline
    \cline{2-10}
    %\multirow{2}{*}{Dataset} &
    \multicolumn{1}{c|}{} &
      \multicolumn{3}{c|}{\emph{Lift}} &
      \multicolumn{3}{c|}{\emph{PickPlaceCan}} &
      \multicolumn{3}{c|}{\emph{NutAssemblySquare}} \\
      %\hline
      \cline{1-10}
    $|\ttd|$ & 40 & 100 & 200 & 40 & 100 & 200 & 40 & 100 & 200 \\
    \hline
   \appr & $100 \pm 0$ & $100 \pm 0$ & $100 \pm 0$ & $82.8 \pm 8.9$ & $95.8 \pm 2.8$ & $96.6 \pm 4.1$ & $40.4 \pm 6.9$ & $69.6 \pm 4.1$ & $86.0 \pm 3.1$ \\
    \hline
    BC-RNN & $100 \pm 0$ & $100 \pm 0$ & $100 \pm 0$ & $83.3 \pm 1.9$ & $97.3 \pm 0.9$ & $98.0 \pm 0.9$ & $29.3 \pm 4.1$ & $64.7 \pm 4.1$ & $82.0 \pm 0.0$\\
    \hline
  \end{tabular}
  \end{center}
  \caption{Performance of \appr\ and BC-RNN on three \rsrw\ tasks from ~\citet{robomimic2021} on $|\ttd|=40,100,$ and $200$ demonstrations. BC-RNN's results come from Figure 3b and Table 27 in \citet{robomimic2021}). On the easier \emph{Lift} and \emph{PickPlaceCan}, \appr\ and BC-RNN are at par, but on the harder \emph{NutAssemblySquare} \appr\ performs better. \textbf{On the remaining 6 problems for which we have gathered the demonstration data, BC-RNN's success rate is 0.}}
  \label{tab:plex_vs_bcrnn}
\end{table*}


\textbf{Results.} As \Cref{fig:rsrw} shows, \appr\ learns strong policies using at most 75 demonstrations, despite having to train a 36M-parameter model including randomly initialized vision models for tasks, most of which have complex dynamics and broad initial state distributions. Moreover, \appr\ with relative positional encoding (denoted simply as \emph{\appr} in the legend) outperforms the alternatives by as much as 20 percentage points (pp) on \rsrw's human-generated demonstration data while never losing to them. In particular, the Decision Transformer without return conditioning (\emph{DT-global}) performs very poorly, and even with return conditioning (\emph{DT-global(+rew)}) falls far short of both \emph{\appr} and \emph{\appr-abs}. Since all models share most of the implementation and are trained similarly when \emph{\appr} and \emph{\appr-abs} run in BC mode, we attribute \appr's advantage only to the combined effect of using human-generated training data and positional encodings. We have also trained \appr\ and \appr-abs for \mw's 5 target tasks from the previous experiment for various amounts of the available -- scripted -- demonstrations for these tasks and noticed no significant performance difference between \appr\ and \appr-abs on any task. This provides additional evidence that the utility of relative positional enconding manifests itself specifically on human-generated demonstration data.

Given \Cref{fig:rsrw}, one may wonder: does \appr-abs's performance plateau at a lower level than \appr's with relative positional encoding, or does \appr-abs catch up on datasets with $|\ttd| > 75$? Although for most problems we don't have enough data to determine this, Table \ref{tab:rs_large} provides an insight using the tasks for which we have Robomimic-provided 200 training demonstrations. Comparing the performance gaps between \appr\ and \appr-abs on 75-trajectory and 200-trajectory datasets reveals that the gap tends to become smaller. The same can be seen for \emph{Stack}, \emph{PickPlaceCereal}, \emph{NutAssemblyRound} in \Cref{fig:rsrw}, suggesting that with sufficient data \appr-abs may perform as well as \appr. However, the amount of data for which this happens may not be feasible to collect in practice.


In fact, relying on relative positional encoding allows \appr\ to achieve \emph{state-of-the art} performance on all \rsrw\ tasks in this experiment. To establish this, in addition to the baselines in  \Cref{fig:rsrw}, we compare to the results of a BC-RNN implementation from the work that introduced some of these \rsrw\ problems~\cite{robomimic2021}. Interestingly, while running that implementation on the tasks for which we have gathered the data ourselves resulted in 0 success rate, while running it on tasks with Robomimic-supplied 200 trajectories (\emph{Lift}, \emph{PickPlaceCan}, and \emph{NutAssemblySquare}) worked well and allowed us to reproduce \citet{robomimic2021}'s results. \appr's comparison to BC-RNN's results on those problems are in \Cref{tab:plex_vs_bcrnn}. \appr\ and BC-RNN are at par on the easier problems but \appr\ performs better on the harder \emph{NutAssemblySquare}.






%The results of training \appr\ with relative and absolute positional encodings are presented in \Cref{fig:rsrw} and its caption. \appr\ with relative positional encodings often outperforms the alternatives by as much as 20 pp while never losing to them, clearly demonstrating the merit of using relative positional encodings when learning from small robotic manipulation datasets.




%\subsection{Pretraining \appr}


%In this experiment, \appr\ has available to it during pretraining only the task-annotated video-only data from 45 tasks and dynamics sampled from poor-quality policies from the 5 target environments. We pretrain 2 versions of \appr: \appr-full and \appr-no-video. The latter uses only the \upm\ dynamics data. The former uses both, sampling video-only batches with probability 0.5. Afterwards, both models are evaluated \emph{zero-shot} on the door-lock (DL), door-unlock (DU), hand-insert (HI), bin-picking (BP), and box-close (BC) tasks. Then both have \emph{only the last block of their observation transformer} fine-tuned on 75 videos from the expert policy.


%The zero-shot success rates of the models are as follows. \appr-no-video achieved 98\% success rate on the DU task and 0 on all others. \appr\ achieved 18\% on HI, 100\% on DU, 30\% on DL, 0\% on BP, and 44\% on BC. This suggests that \appr-no-video inferred some of the downstream task policy from the \upm\ data but since it didn't use the video data during pretraining, it never learned to plan, yielding poor downstream performance. On the other hand, \appr's planner did get pretrained, and, even though the pretraining video data came from the 45 tasks that were quite distinct from the 5 downstream ones, managed to generalize its learnings sufficiently to yield non-trivial \emph{zero-shot} performance on 4 out of 5 target tasks.


%After finetuning on 75 video-only trajectories per task, both \appr\ and \appr-no-video yielded nearly identical success rates -- HI:44\%, DU: 100\%, DL: 98\%, BP: 100\%, BC: 62\%. Note that throughout pretraining and finetuning, neither \appr\ nor \appr-no-video have seen \emph{any} complete expert trajectories from the target tasks, and only a small fraction of these models' parameters -- the observation transformer's last block -- was altered during finetuning. Thus, these models' performance is due to learning dynamics at pretraining time and learning to plan successfully at finetuning.



% using no noise to generate the fine-tuning data and adding Gaussian noise ($\sigma = 0.5$) to generate the pretraining data. We then produce several datasets for each task: containing 5, 10, 25, 50, and 75 shortest trajectories out of the 75.



%===================================================== NOTES/THOUGHTS START =========================================================================
\begin{comment}
In our experiments, we evaluate the ability of our model to learn control policies from a combination of data sources and a small number of demonstrations.

Potential baselines:

\begin{itemize}
    \item The same approach as ours, but with an MLP instead of DT.
    \item The same approach as ours (including SimCLR) but without the video prediction loss.
    \item For each task, TD3+BC or IQL from robomimic vs pretrained DT, for different amounts of demonstration data. %COG \cite{singh2020cog} -- offline RL (CQL) with sparse rewards and off-task data to improve the target-task policy by generalizing it to initial states not seen in the target-task demos.
    %\item IQL? Maybe COG with IQL instead of CQL?
    \item Ablation: architecture (DT vs MLP), Loss (video loss or not), Horizon randomization, treatment of missing variables, etc.
\end{itemize}



Experiments:

\begin{itemize}
    \item Comparisons to the above baselines, \textbf{in sparse-reward settings.}
    \item Showing the scalability properties of the transformer: for a fixed amount of data, increasing the number of transformer heads and layers, and showing that the advantage over baselines increases (hopefully!) in the following sense: \textbf{the number of target-task demos DT requires is far smaller than the amount that baselines require}.
    \item Vary the quality of off-task trajectories (and videos), and see how the DT perf varies for the target task for a fixed set of target task demos. E.g., in COG off-task demos have 30-50\% success rate \cite{singh2020cog}.
\end{itemize}

TO THINK ABOUT: is our initial state distribution broad enough?

Varying the planning horizon: we want the pretrained model to be agnostic to test-time eval horizon.

NOTE: let's not portray this work as offline RL.

Should we train on all the data, or pick random subsets of tasks to be used for video-only and off-task data? I feel we should at least vary the number of
\end{comment}
%===================================================== NOTES/THOUGHTS END =========================================================================