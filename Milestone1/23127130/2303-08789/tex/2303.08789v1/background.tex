\section{Problem statement and relevant concepts}

\subsection{Problem statement}

We consider the problem of learning a generalist task- and embodiment-conditioned policy for goal-directed object manipulation. Namely, assuming our training data has sufficient coverage of tasks and dynamics, we would like to learn a policy that can drive a robotic manipulator to successfully accomplish a task, where this target task and robotic manipulator may not be known during the policy training process. 

Formally, this problem can be described as a partially observable Markov decision process (POMDP) $\langle \GG, \EE, \SS, \OO, z, \AA, p, r\rangle$ with a special structure. Here, $\GG$ is the space of possible manipulation tasks and $\EE$ the space of possible embodiments (i.e., robots) that we may want to carry out the tasks in $\GG$. $\SS = \PP \times \WW$ is a state space consisting of a space $\PP$ of robots' proprioceptive states (e.g., poses, joint speeds, etc.) and a space $\WW$ of world states. A state $s$'s proprioceptive part $p \in \PP$ is known at execution time and in some of the training data, whereas the world state $w \in \WW$ is never observable directly. A latent state $s$ can be probabilistically inferred from its observations $o\in \OO$ and a state- and embodiment-conditioned distribution $z: \EE \times \SS \rightarrow \Delta(\OO)$ that describes how latent states in $\SS$ manifest themselves through observations, where $\Delta$ denotes the space of distributions. For robotic manipulation, each observation can consist of several \emph{modalities}: camera images (possibly from several cameras at each time step), depth maps, tactile sensor readings, etc. The distribution $z$ is unknown and needs to be learned. $\AA$ is an action space, e.g., the space of all pose changes the robotic manipulator can achieve in 1 time step, and $p: \EE \times \SS \times \AA \rightarrow \Delta (\SS)$ is a transition function describing how executing an action affects a current state, which potentially is stochastic. A reward function $r: \GG \times \SS \times \AA \times \SS  \rightarrow \mathbb{R}$ can provide additional detail about task execution by assigning a numeric reward to each state transition, e.g., 0 for transitions to a task's goal state and -1 otherwise. Our objective is to learn a policy $\pi: \GG \times \EE \times \OO_{|H} \rightarrow A$ that maps a robot embodiment $e \in \EE$'s history of observations $\OO_{|H}$ over the previous $H$ steps to an action so as to lead the robot to accomplish a task $g \in \GG$.

In practice, learning a generalist policy that performs well on a broad distribution of tasks zero-shot is very challenging, as  the coverage and amount of publicly available training data are limited. Thus, in this work we consider a two-phased learning process: (1) pretraining, during which the generalist policy per se is trained, and (2) finetuning, during which the generalist policy is adapted to a target task. Next, we describe the data suitable for each stage.


\subsection{Data for training robotic manipulation models}

\begin{comment}
%
Training data we consider in this work is in the form of \emph{trajectories} -- sequences of timestamped datapoints reflecting an agent's interaction with an environment. Formally, we assume some universe of data modalities $\MM = \MM_s \cup \MM_m \cup \MM_g$ subdivided into sensory ($\MM_s$), motor ($\MM_m$), and task specification ($\MM_g$) ones. In this paper, $\MM_s = \{\text{camera images}, \text{proprioception}\}$, $\MM_m =\{\text{control inputs}\}$, and $\MM_g = \{\text{goal-scene camera images}\}$. A trajectory is $\tau = g, o_0, a_0, \ldots, o_{N_{\tau}-1}, a_{N_{\tau}-1}, o_{N_{\tau}}$. %$\MM = \{\text{task specification}, \text{camera image}, \text{proprioception}, \text{action}\}$, where but our Every training trajectory
\end{comment}

We categorize the available datasets relevant to training robotic manipulation systems into several broad groups: \footnote{Static image datasets, e.g., ImageNet, aren't treated by \appr\ in a special way and we don't discuss it here, but can be used to pretrain \appr's image encoders.}

\textbf{Multi-task video demonstrations ($\mtvd$).}
The most abundant category, it comprises data collections ranging from general YouTube videos to curated benchmarks such as Ego4D~\cite{grauman2022ego4d}, Epic Kitchens~\cite{Damen2018EPICKITCHENS,Damen2022RESCALING}, and YouTube-8M~\cite{youtue8m} showing \emph{an} agent -- either a robot or a person -- performing a meaningful object manipulation task with an end-effector. The common denominator of this data is that it contains demonstration-quality sequences of observations annotated with descriptions of tasks they accomplish, but not the action sequences whose execution generated these videos.
 
\textbf{Visuomotor trajectories ($\vmt$).} These trajectories consist of paired sequences of observations and robots' actions. In general, this data may be generated by activities that most people will not find meaningful, e.g., grabbing random objects in a tray as in the RoboNet~\cite{robonetv1}, although some of it may consist of high-quality demonstrations of specific tasks, e.g., as in the Bridge Dataset~\cite{robonetv2}. For \appr, the main role of these datasets is showing how various embodiments can \emph{execute} observable changes in their environment. Since we don't impose strong quality, quantity, or task association requirements on $\vmt$\ data, we expect it to be relatively easy to collect for any target embodiment and environment for which it isn't publicly available already.

\textbf{Target-task demonstrations ($\ttd$)}. 
This is the most scarce but also most desirable data category, since it encompasses high-quality trajectories for a specific task in question, ideally collected on the target embodiment (robot). Note, however that we don't require that these demonstrations be visuomotor. In fact, our experiments show that \appr\ needs only video demonstrations for finetuning to learn a high-quality policy for a target task.

\textbf{A key data assumption} we make in this work is that $|\ttd | \ll |\vmt| \ll |\mtvd|$.



\subsection{Transformers and positional encodings}

A transformer-based architecture consists of several specially structured \emph{self-attention layers} and, in general, maps an input \emph{set} (often called a \emph{context}) of $K$ elements called \emph{tokens} to an output of the same size $K$~\cite{vaswani2017attention}. In most applications, transformers need to map \emph{ordered} sets -- sequences -- to other ordered sets, e.g. translate sentences from one language to another. In order to give a transformer the position of each token within the input context, special vectors called \emph{positional encodings} are added to each input element. These vectors can be learned as part of training a transformer architecture or be hand-crafted and fixed.

The original transformer architecture \cite{vaswani2017attention} and most of its variations that have since been applied to language modeling, e.g., \citet{brown2020gpt3} employ \emph{absolute positional encoding} of input tokens. Under this scheme, each token’s \textit{absolute} position within a context selects a per-position vector (either trainable or fixed) that is added to the token’s embedding vector. Some transformers, e.g., \citet{chen2021decision}, use what we call a \emph{global positional encoding}. It is similar to the absolute one, but assigns a separate vector to all positions in an input sequence up to some maximum length $T$, rather than just to the positions in the (much shorter) context $K$. Finally, models based on Transformer-XL~\cite{orig,gato,rt2022}, instead condition the attention computation itself on the \textit{relative} positions between different pairs of input tokens. A work on natural language tasks, where training data is plentiful, reports this \emph{relative positional encoding} to provide only modest improvements there~\cite{orig}. The works on decision-making simply ``inherit'' relative positional encoding from Transformer-XL that they build on~\cite{gato} without analyzing its pros and cons.

In this work, we make a case that on datasets typical of robotic manipulation, which consist of small numbers of human-gathered demonstrations, relative positional encoding tends to yield significantly better performance than absolute or global one.
