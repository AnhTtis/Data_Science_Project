\newpage
\onecolumn
\begin{appendices}

\section{\appr\ implementation details \label[appendix]{app:impl}}

 We condition \appr's planner on embeddings of goal images. Low-dimensional inputs (actions and proprioceptive states) are mapped to $\mathbb{R}^h$, the transformer's $h$-dimensional input space, using a 1-layer linear neural network. High-dimensional inputs -- videoframes from one or several cameras at each time step as well as goal images -- are processed using a ResNet-18-based~\cite{resnet} encoder from Robomimic~\cite{robomimic2021}. It applies a random crop augmentation to each camera's image, passes it through a separate ResNet18 instance associated with that camera, then passes the result through a spatial softmax layer \cite{levine2016end}, and finally through a small MLP. The resulting embedding is fed into \appr's planner. If the robot has several cameras, the encoder has a separate ResNet instance for each. For each time step, \appr's planner outputs an $h$-dimensional latent state representing the \emph{predicted} embedding of \appr's visual observations $k$ time steps into the future, where $k$ is a tunable parameter. These latents are then fed directly into the planner as predictions of future observation embeddings. The output latents from the planner transformer are fed through a $\tanh$ non-linearity, which outputs action vectors in the $[-1, 1]$ range. \appr\ architecture hyperparameters used in each experiment are in \Cref{tab:arch_params} in \Cref{app:hyp}. 

Our \appr\ implementation is available at {\color{red} $\langle$ TO BE RELEASED $\rangle$}.
 
\section{Hyperparameters \label[appendix]{app:hyp}}

\begin{table*}[h]
\begin{center}
  \begin{tabular}{|c|c|c|}
    \hline
    \emph{Parameter name} & \thead{\mw \\  (\emph{PLanner}/\emph{EXecutor})} & \thead{\rsrw \\ (\emph{PLanner}/\emph{EXecutor})} \\
    \hline
    \# layers & 2/2 & 3/3 \\
    context size $K$ & 30/30 time steps & 30/30 time steps \\
    hidden dimension & 256/256 & 256/256 \\
    \# transformer heads & 4/4 & 4/4 \\
    \# evaluation episodes & 50 & 50 \\
    \# max. evaluation episode length & 500 & 700 \\
    \hline
  \end{tabular}
  \end{center}
  \caption{Hyperparameters of \appr's transformer-based planner and executor components for the \mw\ and \rsrw\ benchmarks. In each case, the planner and executor use the same parameters, but for most problems the executor's context length $K$ can be much smaller than the planner's without loss of performance, e.g., $K_{EX}=10$. For the Decision Transformer on \rsrw, we use 4 transformer layers and otherwise the same hyperparameters as for \appr.}
  \label{tab:arch_params}
\end{table*}


\begin{table*}[h]
\begin{center}
  \begin{tabular}{|c|c|c|c|}
    \cline{2-4}
    \multicolumn{1}{c|}{} &
      \multicolumn{2}{c|}{\thead{\mw}} & \thead{\rsrw} \\
    \hline
    \emph{Parameter name} & \thead{pretraining \\  (\emph{PLanner}/\emph{EXecutor})} & \thead{last-layer finetuning \\ (\emph{PLanner}/\emph{EXecutor})} & \thead{behavior cloning \\ (\emph{PLanner}/\emph{EXecutor})} \\
    \hline
    \# observation prediction lookahead steps & 1/ -- & 1/ -- & 1/ -- \\
    learning rate & $5\cdot 10^{-4}$ & $5\cdot 10^{-4}$ & $5\cdot 10^{-4}$ \\
    batch size & 256 & 256 & 256 \\
    weight decay & $10^{-5}$ & $10^{-5}$ & $10^{-5}$ \\
    \# training epochs & 10/30 & 10/10(?) & 10 \\
    \# training steps per epoch & 250/250 & 250/250(?) & 500 \\
    \hline
  \end{tabular}
  \end{center}
  \caption{Hyperparameters of \appr\ training for the generalization experiments on \mw\ and positional encoding experiments on \rsrw. The former use \appr\ in pretraining and finetuning modes; the latter only in behavior cloning mode (training the entire model from scratch for a single target task). In finetuning mode, we adapt only the last transformer layer of the planner and, in one experiment, of the executor as well. The (?) next to the executor's hyperparameters indicate that they were used only in the experiment where the executor was actually finetuned. For the Decision Transformer on \rsrw\, we use the same hyperparameters as for \appr.}
  \label{tab:training_params}
\end{table*}


\section{\mw\ and \rsrw\ details \label[appendix]{app:benchmarks}}

\textbf{\mw.} In our \mw-v2 setup, at each time step the agent receives an $84 \times 84$ image from the environment's \emph{corner} camera and the Sawyer arm's 18D proprioceptive state. The agent's actions have 4 dimensions, each scaled to the $[-1, 1]$ range. Although \mw\ also provides privileged information about the state of the environment, including the poses of all relevant objects, our \appr\ agent doesn't access it.

\textbf{\rsrw.} The observation and action space in our experiments is exactly as in the best-performing high-dimensional setup from the Robomimic paper~\cite{robomimic2021}. Namely, actions are 7-dimensional: 6 dimensions for the gripper's pose control (OSC\_POSE) and 1 for opening/closing it. Visual observations are a pair of $84 \times 84$ images from \emph{agentview} (frontal) and \emph{eye-in-hand} (wrist) cameras at each step. Proprioceptive states consist of a 3D gripper position, a 4D quaternion for its orientation, and 2D gripper fingers' position.

\end{appendices}