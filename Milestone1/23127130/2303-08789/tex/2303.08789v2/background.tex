\vspace{-3mm}
\section{Problem statement and relevant concepts}
\vspace{-1mm}

\subsection{Problem statement}
\vspace{-1mm}

We consider the problem of learning a generalist  task-conditioned policy for goal-directed object manipulation. Namely, we seek a policy that can control a robotic manipulator to successfully accomplish tasks that the robot may not have encountered during the policy training process; such a policy formally can be viewed a solution to a task-conditioned partially observable Markov decision process (POMDP) described in \Cref{app:pomdp}. In practice, learning a generalist policy that performs well on a broad distribution of tasks zero-shot is very challenging, as the coverage and amount of publicly available training data are limited. Therefore, in this work we consider a two-phased learning process: (1) pretraining, during which a generalist policy is trained, and (2) finetuning, during which this policy is adapted to a target task.

\vspace{-1mm}
\subsection{Data for training robotic manipulation models}
\vspace{-1mm}
 
We consider three broad groups of datasets relevant to training robotic manipulation systems:\footnote{Static image datasets, e.g., ImageNet, aren't treated by \appr\ in a special way and we don't discuss it here, but can be used to pretrain \appr's image encoders.}

\textbf{Multi-task video demonstrations ($\mtvd$).}
Being the most abundant category, it comprises data collections ranging from general YouTube videos to curated benchmarks such as Ego4D~\cite{grauman2022ego4d}, Epic Kitchens~\cite{Damen2018EPICKITCHENS,Damen2022RESCALING}, and YouTube-8M~\cite{youtue8m} showing \emph{an} agent -- either a robot or a person -- performing a meaningful object manipulation task with an end-effector. This data contains demonstration-quality sequences of video observations and descriptions of tasks they accomplish, but not the action sequences whose execution generated these videos.
 
\textbf{Visuomotor trajectories ($\vmt$).} These trajectories consist of paired sequences of observations and robots' actions. Although some of them may be high-quality demonstrations of specific tasks, e.g., as in the Bridge Dataset~\cite{robonetv2}, many of these trajectories are generated by activities that most people will not find meaningful, e.g., grabbing random objects in a tray, as in the RoboNet~\cite{robonetv1}. Since no strong quality, quantity, or task association requirements are imposed on $\vmt$\ data, it is relatively easy to collect for any target embodiment and environment.

\textbf{Target-task demonstrations ($\ttd$)}. 
This is the most scarce but also most desirable data category, since it encompasses high-quality trajectories for a specific task in question, ideally collected on the target embodiment (robot). Note, however that we don't require that these demonstrations be visuomotor. In fact, our experiments show that \appr\ needs only video demonstrations for finetuning to learn a high-quality policy for a target task.

\textbf{A key data assumption} we make in this work is that $|\ttd | \ll |\vmt| \ll |\mtvd|$.


\vspace{-1mm}
\subsection{Transformers and positional encodings}
\vspace{-1mm}
A transformer-based architecture consists of several specially structured \emph{self-attention layers} and, in general, maps an input \emph{set} (often called a \emph{context}) of $K$ elements (called \emph{tokens}) to an output of the same size $K$~\cite{vaswani2017attention}. In most applications, such as language translation, transformers need to map \emph{ordered} sets (i.e. sequences) to other ordered sets, and therefore add special vectors called \emph{positional encodings} to each input element to identify its position in a sequence. These encodings can be learned as part of transformer's training or be hand-crafted.

The most common scheme is the \emph{absolute positional encoding}, where each position in the transformer's $K$-sized context gets a positional vector~\cite{vaswani2017attention}. Some transformers, e.g., \citet{chen2021decision}, use what we call a \emph{global positional encoding}. It is similar to the absolute one, but assigns a separate vector to each position \emph{in the entire input sequence} rather than just the $K$-sized context, up to some maximum length $T \gg K$. Finally, models based on Transformer-XL~\cite{orig,gato,rt2022}, instead condition the attention computation on the \textit{relative} positions between different pairs of input tokens  within a context. 
In this work, we argue that on robotic manipulation finetuning datasets that consist of small numbers of human-gathered demonstrations, relative positional encoding is significantly more data-efficient than absolute or global one.
