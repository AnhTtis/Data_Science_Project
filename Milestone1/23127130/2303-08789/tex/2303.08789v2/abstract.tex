\begin{abstract}
A rich representation is key to general robotic manipulation, but existing approaches to representation learning require large amounts of multimodal demonstrations.
In this work we propose \appr\footnote{The paper's accompanying code and data are available at \href{https://microsoft.github.io/PLEX}{https://microsoft.github.io/PLEX}.}, a transformer-based architecture that learns from a small amount of \emph{task-agnostic visuomotor} trajectories and a much larger amount of \emph{task-conditioned} object manipulation \emph{videos} --- a type of data available in quantity. 
\appr\ uses visuomotor trajectories to induce a latent feature space and to learn task-agnostic manipulation routines, while diverse video-only demonstrations teach \appr\ how to plan in the induced latent feature space for a wide variety of tasks. Experiments showcase \appr's generalization on \mw\ and SOTA performance in challenging \rsrw\ environments. In particular, using relative positional encoding in \appr's transformers greatly helps in low-data regimes of learning from human-collected demonstrations.
\end{abstract}