\newpage
\onecolumn
\begin{appendices}

\section{Related work \label[appendix]{app:rel_work}}
\input{related_work}

\section{Problem formalization \label[appendix]{app:pomdp}}

Formally, the problem \appr\ aims to solve can be described as a partially observable Markov decision process (POMDP) $\langle \GG, \SS, \OO, z, \AA, p, r\rangle$ with a special structure. Here, $\GG$ is the space of possible manipulation tasks that we may want to carry out the tasks in $\GG$. $\SS = \PP \times \WW$ is a state space consisting of a space $\PP$ of robots' proprioceptive states (e.g., poses, joint speeds, etc.) and a space $\WW$ of world states. A state $s$'s proprioceptive part $p \in \PP$ is known at execution time and in some of the training data, whereas the world state $w \in \WW$ is never observable directly. A latent state $s$ can be probabilistically inferred from its observations $o\in \OO$ and a state-conditioned distribution $z: \SS \rightarrow \Delta(\OO)$ that describes how latent states in $\SS$ manifest themselves through observations, where $\Delta$ denotes the space of distributions. For robotic manipulation, each observation can consist of several \emph{modalities}: camera images (possibly from several cameras at each time step), depth maps, tactile sensor readings, etc. The distribution $z$ is unknown and needs to be learned. $\AA$ is an action space, e.g., the space of all pose changes the robotic manipulator can achieve in 1 time step, and $p: \SS \times \AA \rightarrow \Delta (\SS)$ is a transition function describing how executing an action affects a current state, which potentially is stochastic. A reward function $r: \GG \times \SS \times \AA \times \SS  \rightarrow \mathbb{R}$ can provide additional detail about task execution by assigning a numeric reward to each state transition, e.g., 0 for transitions to a task's goal state and -1 otherwise. Our objective is to learn a policy $\pi: \GG \times \OO_{|H} \rightarrow A$ that maps a history of observations $\OO_{|H}$ over the previous $H$ steps to an action so as to lead the robot to accomplish a task $g \in \GG$.

\section{\appr\ implementation details \label[appendix]{app:impl}}


The transformers \appr\ uses as its planner and executor are derived from the GPT-2-based version of the Decision Transformer (DT)~\cite{chen2021decision}. Like in DT, we feed inputs into \appr\ by embedding each modality instance (e.g., an image or an action) as a single unit. This is different to the way, e.g., Gato~\cite{gato} and Trajectory Transformer~\cite{janner2021reinforcement} do it, by splitting each input into fragments such as image patches and embedding each fragment separately.

 We condition \appr's planner on embeddings of goal images. Low-dimensional inputs (actions and proprioceptive states) are mapped to $\mathbb{R}^h$, the transformer's $h$-dimensional input space, using a 1-layer linear neural network. High-dimensional inputs -- videoframes from one or several cameras at each time step as well as goal images -- are processed using a ResNet-18-based~\cite{resnet} encoder from Robomimic~\cite{robomimic2021}. It applies a random crop augmentation to each camera's image, passes it through a separate ResNet18 instance associated with that camera, then passes the result through a spatial softmax layer \cite{levine2016end}, and finally through a small MLP. The resulting embedding is fed into \appr's planner. If the robot has several cameras, the encoder has a separate ResNet instance for each. For each time step, \appr's planner outputs an $h$-dimensional latent state representing the \emph{predicted} embedding of \appr's visual observations $k$ time steps into the future, where $k$ is a tunable parameter. These latents are then fed directly into the planner as predictions of future observation embeddings. The output latents from the planner transformer are fed through a $\tanh$ non-linearity, which outputs action vectors in the $[-1, 1]$ range. The hyperparameters can be found in \Cref{tab:arch_params,tab:training_params}.

Our \appr\ implementation is available at \href{https://microsoft.github.io/PLEX}{https://microsoft.github.io/PLEX}.



\section{Additional details about the experiments \label[appendix]{app:exp_details}}

\subsection{\mw\ and \rsrw\ details \label[appendix]{app:benchmarks}}
\textbf{\mw.} In our \mw-v2 setup, at each time step the agent receives an $84 \times 84$ image from the environment's \emph{corner} camera and the Sawyer arm's 18D proprioceptive state. The agent's actions have 4 dimensions, each scaled to the $[-1, 1]$ range. Although \mw\ also provides privileged information about the state of the environment, including the poses of all relevant objects, our \appr\ agent doesn't access it.

\textbf{\rsrw.} The observation and action space in our experiments is exactly as in the best-performing high-dimensional setup from the Robomimic paper~\cite{robomimic2021}. Namely, actions are 7-dimensional: 6 dimensions for the gripper's pose control (OSC\_POSE) and 1 for opening/closing it. Visual observations are a pair of $84 \times 84$ images from \emph{agentview} (frontal) and \emph{eye-in-hand} (wrist) cameras at each step. Proprioceptive states consist of a 3D gripper position, a 4D quaternion for its orientation, and 2D gripper fingers' position.


\subsection{Details of the baselines from prior work \label[appendix]{app:baseline_details}}

\textbf{\appr+R3M~\cite{nair2022r3m}.} We experiment with two combinations of \appr\ with a frozen ResNet-50-based R3M~\cite{nair2022r3m}, an observational representation pretrained on the large Ego4D dataset~\cite{grauman2022ego4d} In these experiments, R3M replaces Robomimic's ResNet-18, and we use versions of our \mw\ $\vmt$, $\mtvd$, and $\ttd$ datasets with 224x224 image observations instead of the 84x84 ones.

One combination, \emph{\appr+R3M, BC} in \Cref{fig:mw}, learns a single-task policy on 10 full sensorimotor demonstrations for each \mw\ target task. It operates in behavior cloning (BC) mode, whereby \appr\ is optimized solely w.r.t. its action predictions' MSE loss, whose gradients backpropagate though the whole network (except the frozen R3M). The other combination, \emph{pretr. \appr+R3M} in \Cref{fig:mw}, follows the same \appr\ pretraining and finetuning process as described previously, except the R3M encoder stays frozen throughout.

\textbf{Learning from Play~\cite{mcil21rss}.} Our final baseline is an adapted \emph{Learning from Play} (LfP) approach~\cite{mcil21rss}. As in \citet{mcil21rss}, LfP doesn't use video-only $\mtvd$ data or target-task demonstrations $\ttd$; it trains one model for all target tasks from the ``play'' dataset $\vmt$ only. Instead of using language annotations to separate ``meaningful'' subsequences in $\vmt$, we give LfP the ground-truth knowledge of where trajectories sampled from different tasks begin and end. Accordingly, we don't use language during training either. As n the case of \appr, We train \emph{Learning from Play} to plan conditioned only on goal images and present it with goal images from successful trajectories of the target tasks during evaluation.

\subsection{Proprioceptive states in Meta-World pretraining}

While \appr\ in general takes proprioceptive states as input (\Cref{fig:transformer}), we found that in the case of Meta-World, using them actually hurts \appr's performance. Our hypothesis is that they serve as salient features that ``distract" the $\mathcal{L}_{EX}$ loss from training the visual observation encoder and hence inducing a good latent space. The latent space, in turn, is crucial for the subsequent pretraining of \appr's planner.

Therefore, we mask out the proprioceptive states when pretraining \appr\ for Meta-World. Doing so forces the executor to learn to use the features of visual observations, not proprioceptive information, for predicting actions, shaping a better latent visual observation space for the planner.



\subsection{Success rate evaluation protocol \label[appendix]{app:srate_details}}

\textbf{In the generalization experiments on \mw}, all success rate evaluations are done on 50 500-step rollouts starting from initial states sampled from the \emph{test} distributions of \mw's ML\replaced{50}{45} target tasks (\emph{door-lock}, \emph{door-unlock}, \emph{hand-insert},  \emph{bin-picking}, and  \emph{box-close}).

To evaluate the zero-shot success rate of the pretrained EX and \appr\ models, we compute the average across 50 rollouts generated by these models on each of the 5 target tasks \emph{at the end of pretraining}.

To evaluate the success rate of the finetuned models, we adopt the procedure from \citet{robomimic2021}. The finetuning lasts for $N$ epochs (see \Cref{tab:training_params}). After each epoch, we measure the average success rate of the resulting model across 50 rollouts, and record the maximum average success rate across all finetuning epochs. 

\textbf{In the positional encoding experiments on \rsrw}, the evaluation protocol is the same as in \mw\ finetuning and in Robomimic~\cite{robomimic2021}: we train each model for $N$ epochs (see \Cref{tab:training_params}), after each epoch compute the success rate across 50 trajectories (with 700-step horizon), and record the best average success rate across all epochs.



\subsection{\rsrw\ datasets and model training \label[appendix]{app:rs_data}}




Training data for \rsrw\ was collected from human demonstrations, not generated by scripted policies. \rsrw\ provides a keyboard and SpaceMouse interfaces for controlling the Panda arm in its environments, and Robomimic supplies datasets of 200 expert (``professional-human'') trajectories collected using the SpaceMouse interface for the \emph{NutAssemblySquare}, \emph{PickPlaceCan}, and \emph{Lift} tasks. For each of the tasks without pre-collected Robomimic datasets, we gather 75 high-quality trajectories via \rsrw's keyboard interface ourselves. We employ \rsrw\ tasks only for experiments that involve training single-task policies from scratch, so all of these trajectories are used as \emph{\textbf{target-task demonstration data ($\ttd$)}}. Typical demonstration trajectory lengths vary between 50 and 300 time steps.

Accordingly, to show the difference between relative and absolute positional encodings' data efficiency, we train \appr\ for $|\ttd| =5,10,25, 50,$ and $75$, sampling $\ttd$'s from the set of 75 demonstrations without replacement. The results are presented in the main paper in \cref{fig:rsrw}.
For \emph{Lift}, \emph{PickPlaceCan}, and \emph{NutAssemblySquare}, Robomimic~\cite{robomimic2021} similarly provides 200 high-quality human-collected demonstrations each, as well as the results of BC-RNN on subsets of these datasets with $|\ttd| =40, 100,$ and $200$. Therefore, for these problems we train \appr\ for $|\ttd| =5,10,25, 50, 75$, as well as $40, 100,$ and $200$. The results are shown in \cref{tab:rs_large} and \cref{tab:plex_vs_bcrnn}.


The only difference of \appr\ model instances for \rsrw\ from those for \mw\ is the former having \emph{two} ResNet-18s in the observation encoder, one for the eye-in-hand and one for the agentview camera. As for \mw, the encoder in the  \rsrw\  is trained from scratch, in order to make our results comparable to Robomimic's~\cite{robomimic2021}, where models use an identical encoder and also train it tabula-rasa. In this experiment, we train \appr\ in behavior cloning (BC) mode, like \mw's single-task \appr+R3M, whereby \appr\ is optimized solely w.r.t. its action predictions' MSE loss, whose gradients backpropagate though the whole network. All hyperparameters are in \Cref{tab:training_params} in \Cref{app:hyp}.



We compare \appr\ with relative positional encoding to \appr\ with absolute one and to two flavors of the Decision Transformer (DT)~\cite{chen2021decision}, which use global positional embedding. One flavor (\emph{DT-global} in \Cref{fig:rsrw}) is trained to condition only on task specification (i.e., goal images), like \appr. We note, however, that \citet{chen2021decision} used rewards and returns when training and evaluating DT. Therefore, we also train a return-conditioned version of DT (\emph{DT-global(+rew)} in \Cref{fig:rsrw}), with returns uniformly sampled from the range of returns in $\ttd$ during evaluation.


\subsection{Additional \rsrw\ results \label[appendix]{app:rs}}



\textbf{Comparison to BC-RNN.} Relying on relative positional encoding allows \appr\ to achieve \emph{state-of-the art} performance on all \rsrw\ tasks in our experiments. To establish this, in addition to the baselines in  \Cref{fig:rsrw}, we compare to the results of a BC-RNN implementation from the work that introduced some of these \rsrw\ problems~\cite{robomimic2021}. Interestingly, running BC-RNN on the tasks for which we have collected demonstrations ourselves resulted in 0 success rate \replaced{}{(\Cref{tab:plex_vs_bcrnn_other})}, while running it on tasks with Robomimic-supplied 200 trajectories (\emph{Lift}, \emph{PickPlaceCan}, and \emph{NutAssemblySquare}) reproduced \citet{robomimic2021}'s results. \appr's comparison to BC-RNN's results on those problems are in \Cref{tab:plex_vs_bcrnn} in \Cref{app:rs_data}. \appr\ and BC-RNN are at par on the easier problems but \appr\ performs better on the harder \emph{NutAssemblySquare}.

\begin{table*}[h]
\setlength{\tabcolsep}{0.2em}
\begin{footnotesize}
    \centering
      \begin{tabular}{|c|c|c|c|c|c|c|c|c|c|}
        \cline{2-10}
        \multicolumn{1}{c|}{} &
          \multicolumn{3}{c|}{\emph{Lift}} &
          \multicolumn{3}{c|}{\emph{PickPlaceCan}} &
          \multicolumn{3}{c|}{\emph{NutAssemblySquare}} \\
          \cline{1-10}
        $|\ttd|$ & 40 & 100 & 200 & 40 & 100 & 200 & 40 & 100 & 200 \\
        \hline
       \appr & $100 \pm 0$ & $100 \pm 0$ & $100 \pm 0$ & $82.8 \pm 8.9$ & $95.8 \pm 2.8$ & $96.6 \pm 4.1$ & $40.4 \pm 6.9$ & $69.6 \pm 4.1$ & $86.0 \pm 3.1$ \\
        \hline
        BC-RNN & $100 \pm 0$ & $100 \pm 0$ & $100 \pm 0$ & $83.3 \pm 1.9$ & $97.3 \pm 0.9$ & $98.0 \pm 0.9$ & $29.3 \pm 4.1$ & $64.7 \pm 4.1$ & $82.0 \pm 0.0$\\
        \hline
      \end{tabular}
\caption{Performance of \appr\ and BC-RNN on three \rsrw\ tasks from ~\citet{robomimic2021} on $|\ttd|=40,100,$ and $200$ demonstrations. BC-RNN's results come from Figure 3b and Table 27 in \citet{robomimic2021}). On the easier \emph{Lift} and \emph{PickPlaceCan}, \appr\ and BC-RNN are at par, but on the harder \emph{NutAssemblySquare} \appr\ performs better. On the remaining 6 problems for which we have gathered the demonstration data, BC-RNN's success rate is 0\replaced{.}{ --- see \Cref{tab:plex_vs_bcrnn_other}.}}
\label{tab:plex_vs_bcrnn}
\end{footnotesize}  
\end{table*}

\replaced{}{\begin{table*}[h]
\setlength{\tabcolsep}{0.2em}
\begin{footnotesize}
    \centering
      \begin{tabular}{|c|c|c|c|c|c|c|}
        \cline{2-7}
        \multicolumn{1}{c|}{} &
          \multicolumn{1}{c|}{\emph{Door}} &
          \multicolumn{1}{c|}{\emph{Stack}} &
          \multicolumn{1}{c|}{\emph{PickPlaceBread}} &
          \multicolumn{1}{c|}{\emph{PickPlaceMilk}} &
          \multicolumn{1}{c|}{\emph{PickPlaceCereal}} &
          \multicolumn{1}{c|}{\emph{NutAssemblyRound}} \\
          \cline{1-7}
        $|\ttd|$ & 75 & 75 & 75 & 75 & 75 & 75  \\
        \hline
       \appr & $78.4 \pm 9.2$ & $97.3 \pm 2.9$ & $92.0 \pm 4.65$ & $65.6 \pm 4.6$ & $72.2 \pm 4.4$ & $49.8 \pm 5.5$ \\
        \hline
        BC-RNN & $0 \pm 0$ & $0 \pm 0$ & $0 \pm 0$ & $0 \pm 0$ & $0 \pm 0$ & $0 \pm 0$ \\
        \hline
      \end{tabular}
\caption{Performance of \appr\ and BC-RNN on the remaining 6 Robotsuite/Robomimic tasks from \Cref{fig:rsrw}. \appr's numbers are copied from that Figure.}
\label{tab:plex_vs_bcrnn_other}
\end{footnotesize}  
\end{table*}}


\textbf{Better data efficiency or higher performance?}  Given \Cref{fig:rsrw}, one may wonder: does \appr-abs's performance plateau at a lower level than \appr's with relative positional encoding, or does \appr-abs catch up on datasets with $|\ttd| > 75$? For most tasks we don't have enough training data to determine this, but \Cref{tab:rs_large} in \Cref{app:rs_data} provides an insight for the tasks with Robomimic-supplied 200 training demonstrations. Comparing the performance gaps between \appr\ and \appr-abs on 75-trajectory and 200-trajectory datasets reveals that the gap tends to become smaller. The same can be seen for \emph{Stack}, \emph{PickPlaceCereal}, \emph{NutAssemblyRound} already at $|\ttd| = 75$ in \Cref{fig:rsrw}, suggesting that with sufficient data \appr-abs may perform as well as \appr. However, the amount of data for which this happens may not be feasible to collect in practice.

\begin{table*}[h]
\begin{center}
  \begin{tabular}{|c|c|c|c|c|c|c|}
    \cline{2-7}
    \multicolumn{1}{c|}{} &
      \multicolumn{2}{c|}{\emph{Lift}} &
      \multicolumn{2}{c|}{\emph{PickPlaceCan}} &
      \multicolumn{2}{c|}{\emph{NutAssemblySquare}} \\
      \cline{1-7}
    $|\ttd|$ & 75  & 200 & 75 & 200  & 75 & 200 \\
    \hline
   \appr & $100 \pm 0$ & $100 \pm 0$ & $80.4 \pm 5.7$ & $96.6 \pm 4.1$ & $64.0 \pm 4.6$ & $86.0 \pm 6.1$ \\
    \hline
    \appr-abs & $100 \pm 0$ & $100 \pm 0$ & $72.8 \pm 8.0$ & $93.0 \pm 4.7$ & $45.2 \pm 5.7$ & $76.8 \pm 4.9$ \\
    \hline
  \end{tabular}
  \end{center}
  \caption{Performance of \appr\ and \appr-abs as the amount of training data $|\ttd|$ increases from 75 to 200 trajectories. The performance gap between the two is narrower on the larger dataset. For \emph{Lift} and several other \rsrw\ tasks, this trend becomes visible for datasets smaller than 200 (see \Cref{fig:rsrw}.}
  \label{tab:rs_large}
\end{table*}





\section{Hyperparameters \label[appendix]{app:hyp}}


\begin{table*}[h]
\begin{center}
  \begin{tabular}{|c|c|c|}
    \hline
    \emph{Parameter name} & \thead{\mw \\  (\emph{PLanner}/\emph{EXecutor})} & \thead{\rsrw \\ (\emph{PLanner}/\emph{EXecutor})} \\
    \hline
    \# layers & 3/3 & 3/3 \\
    context size $K$ & 30/30 time steps & 30/30 time steps \\
    hidden dimension & 256/256 & 256/256 \\
    \# transformer heads & 4/4 & 4/4 \\
    \# evaluation episodes & 50 & 50 \\
    max. evaluation episode length & 500 & 700 \\
    \hline
  \end{tabular}
  \end{center}
  \caption{Hyperparameters of \appr's transformer-based planner and executor components for the \mw\ and \rsrw\ benchmarks. In each case, the planner and executor use the same parameters, but for most problems the executor's context length $K$ can be much smaller than the planner's without loss of performance, e.g., $K_{EX}=10$. For the Decision Transformer on \rsrw, we use 4 transformer layers and otherwise the same hyperparameters as for \appr.}
  \label{tab:arch_params}
\end{table*}


\begin{table*}[h]
\begin{center}
  \begin{tabular}{|c|c|c|c|}
    \cline{2-4}
    \multicolumn{1}{c|}{} &
      \multicolumn{2}{c|}{\thead{\mw}} & \thead{\rsrw} \\
    \hline
    \emph{Parameter name} & \thead{pretraining \\  (\emph{PLanner}/\emph{EXecutor})} & \thead{last-layer finetuning \\ (\emph{PLanner}/\emph{EXecutor})} & \thead{behavior cloning \\ (\emph{PLanner}/\emph{EXecutor})} \\
    \hline
     lookahead steps & 1/ -- & 1/ -- & 1/ -- \\
    learning rate & $5\cdot 10^{-4}$ & $5\cdot 10^{-4}$ & $5\cdot 10^{-4}$ \\
    batch size & 256 & 256 & 256 \\
    weight decay & $10^{-5}$ & $10^{-5}$ & $10^{-5}$ \\
    \# training epochs & 10/10 & 10/10(?) & 10 \\
    \# training steps per epoch & 250/250 & 250/250(?) & 500 \\
    \hline
  \end{tabular}
  \end{center}
  \caption{Hyperparameters of \appr\ training for the generalization experiments on \mw\ and positional encoding experiments on \rsrw. The former use \appr\ in pretraining and finetuning modes; the latter only in behavior cloning mode (training the entire model from scratch for a single target task). In finetuning mode, we adapt only the last transformer layer of the planner and, in one experiment, of the executor as well. The (?) next to the executor's hyperparameters indicate that they were used only in the experiment where the executor was actually finetuned. For the Decision Transformer on \rsrw\, we use the same hyperparameters as for \appr.}
  \label{tab:training_params}
\end{table*}

\end{appendices}