\vspace{-1mm}
\section{Experiments}
\vspace{-1mm}

We conduct two sets of experiments to answer the following questions: \emph{\textbf{(i)} Does \appr\ pretrained on task-agnostic sensorimotor data and task-annotated video data generalize well to downstream tasks?
\textbf{(ii)} How does the use of relative positional encodings affect \appr's policy quality?} \Cref{app:impl} provides the details about our \appr\ implementation.\footnote{We implement \appr using the GPT-2 of the DT codebase~\cite{chen2021decision} but without return conditioning.} 


\vspace{-1mm}
\subsection{Benchmarks and training data}
\vspace{-1mm}

\noindent
\textbf{\mw:} \mw~\cite{mw2019} is a collection of 50 tasks featuring a Sawyer arm. We use \mw-v2 with image observations (see details in \Cref{app:benchmarks}). We consider the ML\replaced{50}{45} split consisting of 45 training and 5 target tasks (\emph{\textbf{door-lock}}, \emph{\textbf{door-unlock}},  \emph{\textbf{hand-insert}},  \emph{\textbf{bin-picking}}, and  \emph{\textbf{box-close}}). We use these 5 target tasks for evaluation. \mw\ comes with high-quality scripted policies for all tasks. To get \emph{\textbf{video demonstration data}} ($\mtvd$), we use these scripted policies to generate 100 successful video-only demonstrations for each of the 45 training tasks, i.e., $|\mtvd|= 4500$.
%
To generate \emph{\textbf{visuomotor trajectories}} ($\vmt$), for the 5 target tasks' environments, we add zero-mean Gaussian noise with standard deviation $0.5$ to the actions of the scripted policies and record the altered actions. We collect 75 trajectories per task, i.e., $|\vmt| = 375$.
%
Finally, for \emph{\textbf{target-task demonstrations}} ($\ttd$), we employ the original scripted policies to produce 75 demonstrations per target task and sample 10 of them in a finetuning experiment run, i.e., $|\ttd| = 10$. 


\textbf{\rsrw:}  \rsrw\ benchmark~\cite{robosuite}, compared  \mw, has robotic manipulation tasks with a significantly more complicated dynamics and action space. We use 9 of its tasks involving a single robot arm (Panda) (\emph{\textbf{Lift}}, \emph{\textbf{Stack}}, \emph{\textbf{Door}}, \emph{\textbf{NutAssemblyRound}}, \emph{\textbf{NutAssemblySquare}}, \emph{\textbf{PickPlaceBread}} \emph{\textbf{PickPlaceCan}}, \emph{\textbf{PickPlaceMilk}}, and \emph{\textbf{PickPlaceCereal}}). \rsrw's details are provided in \Cref{app:benchmarks}.
%
Importantly, the training data for \rsrw\ was collected from human demonstrations, \emph{not} generated by scripted policies as in \mw. See \Cref{app:rs_data} for details.

\vspace{-1mm}
\subsection{Generalization experiments \label{sec:gen_exp}}
\vspace{-1mm}


\begin{figure}[t!]
    \hspace{-0.25in}
    \centering
    \includegraphics[width=1\linewidth]{5_MW_probs_w_zs_w_goal_emb+R3M+LfP__WIDE.png}
    \vspace{-0.1in}
    \caption{\appr's generalization experiments. The confidence intervals are computed with 10 seeds.}
    \label{fig:mw}
    \vspace{-0.2in}
\end{figure}

Here we focus on pretraining \appr\ with multi-task \mw\ data. The results are shown in \Cref{fig:mw}. We train a 16,639,149-parameter \appr\ instance (including the ResNet-18-based image encoder) from scratch with random initialization. We use the success rate on the 5 target tasks as the performance metric. For baselines, we experiment with \appr\ with a frozen ResNet-50-based R3M~\cite{nair2022r3m}, an observational representation pretrained on the large Ego4D dataset~\cite{grauman2022ego4d}. We denote it as \emph{\appr{}+R3M}\replaced{.}{; in \Cref{fig:mw}, \emph{Pretr. \appr{}+R3M} was first pretrained on multitask data and then finetuned on a target task, while \emph{\appr{}+R3M, BC} was trained only on a single target task's data from the start.} In addition, we use an adapted \emph{Learning from Play} (LfP) approach~\cite{mcil21rss}. The hyperparameters and details can found \cref{app:impl,app:exp_details}. 
In summary, the experimental results show that \appr\ can perform well without seeing a single sensorimotor expert demonstration.

\vspace{-2mm}
\paragraph{\appr demonstrates \replaced{strong zero-shot performance}{zero-shot generalization capabilities}}
\Cref{fig:mw} shows that \appr\ pretrained on as few as 4500 video demonstrations ($\mtvd$) from the training environments and 250 dynamics trajectories ($\vmt$) from the target environments (denoted as \emph{Pretr. \appr, zero-shot} in \Cref{fig:mw}) exhibits good downstream performance \emph{zero-shot}. 
%
To demonstrate that this performance is really due to planning learned from video-only data as opposed to the executor inadvertently exploiting biases in the data, we consider a \appr variation (denoted as \emph{Pretr. EX only, zero-shot}) where we only pretrain the \emph{executor} (on $\vmt$), not the planner.\footnote{At run time we feed the embedding of the task's \emph{goal} image as the predictions that the \emph{executor} conditions on (since no planner is trained).} 
The results of \emph{Pretr. EX only, zero-shot} \replaced{shows the baseline}{reflect a level of} performance one can get with knowledge \replaced{}{contained} in the dynamics data $\vmt$ alone. \emph{Pretr. EX only, zero-shot} underperforms  \emph{Pretr. \appr, zero-shot}, which shows the importance of learning from $\mtvd$ via \appr's planner. 


\replaced{As a baseline, we also report \emph{Learning from Play} performance, which}{Our main baseline for zero-shot generalization is \emph{Learning from Play} (LfP)~\cite{mcil21rss}, one of the few existing methods able to generalize zero-shot from data as low-quality as $\mtvd$.} LfP has planning capability but doesn't have a way to use either the video-only data $\mtvd$ or the target-task demonstrations $\ttd$. The latter two give \appr\ a large advantage.






\vspace{-2mm}
\paragraph{\replaced{\appr can fast adapt to samll video-only demonstrations}{\appr can be finetuned effectively using only a few video-only demonstrations}}
We further show that finetuning only 5\% of \appr's parameters (the last transformer layer of the planner) on just 10 \emph{video-only} demonstrations for a given task significantly boosts \appr's success rate there. For all 5 downstream tasks, this policy outperforms \emph{Pretr. EX only, zero-shot} by $\geq 2\times$. The improvement is drastic especially in the case of \emph{hand-insert-v2}, \emph{bin-picking-v2}, and \emph{box-close-v2}.

\vspace{-2mm}
\paragraph{Video-only demonstrations is all \appr\ needs during finetuning}
Interestingly, we find that full demonstrations (with both video and action sequences) don't increase \appr's performance beyond video-only ones. This can seen from the experimental results of \emph{Pretr. \appr, ft. on 10 full demos}, where we finetune \appr\ (the action head and last transformer layer of \appr's planner, executor; $\approx11\%$ of \appr) on 10 \emph{full} (sensorimotor) demonstrations for each task. We think this is due to \appr's image encoder being pretrained only on observations from $\vmt$ and frozen during finetuning. Because of this, finetuning couldn't help the encoder learn any extra features for modeling inverse dynamics \emph{over the observation space region covered by $\ttd$}, even if such features would improve \appr's performance.

The issue of impoverished observation coverage in $\vmt$ dataset can be addressed by using a frozen encoder pretrained on an independent large dataset, as the results of \replaced{single-task}{} \emph{\appr{}+R3M, BC} and of \emph{pretrained \appr{}+R3M} in \Cref{fig:mw} suggest. There, \appr's R3M encoder was never trained on \emph{any} \mw\ observations but enables \appr\ to perform reasonably well.

\replaced{ This experiment also shows}{%
%
The results of  \emph{Pretr. \appr{}+R3M} and \emph{\appr{}+R3M, BC} in \Cref{fig:mw} illuminate} two other aspects of using observation-only representations like R3M: (1) The sensorimotor representation that \appr\ learns \emph{on top of} R3M clearly helps generalization -- \emph{pretrained \appr{}+R3M} performs much better than \replaced{the signle-task one}{\emph{\appr{}+R3M, BC}, which was trained only on a single task's data}, despite \replaced{}{\emph{pretrained \appr{}+R3M}} seeing just video-only demonstrations at finetuning. (2) Fully frozen \replaced{somewhat R3M}{R3M somewhat} limits \appr's performance -- \appr\ \replaced{}{variants} that pretrained \replaced{its}{their} own encoder outperform\replaced{s}{} \appr{}+R3M on 4 of 5 tasks and match it on the remaining one.



\vspace{-1mm}
\subsection{Positional encoding experiments \label{sec:enc_exps}}
\vspace{-1mm}

\begin{figure}[t!]
    \vspace{-0.2in}
    \hspace{-0.25in}
    \includegraphics[width=1.05\linewidth]{rs_rel_vs_abs_9L.png}
    \vspace{-0.2in}
    \caption{Data efficiency of \appr's relative positional encoding in single-task mode on \rsrw's single-arm tasks with $|\ttd|$ varying from 5 to 75.  \textbf{\appr} (with relative encodings) in most cases significantly outperforms and at worst matches the performance of its version \textbf{\appr-abs} with absolute positional encodings. Both versions significantly outperform DT.}
    \label{fig:rsrw}
    \vspace{-0.2in}
\end{figure}


In the \mw\ experiments, all training data was generated by scripted policies. In real settings, most such data is generated by people teleoperating robots or performing various tasks themselves. A hallmark of human-generated datasets compared to script-generated ones is the demonstration variability in the former: even trajectories for the same task originating in the same state tend to be different. In this section, we show that in low-data regimes typical of finetuning on human-generated demonstrations, \appr\ with relative positional encoding  yields superior policies for a given amount of training data than using absolute encoding. The results are in \Cref{fig:rsrw}.


\textbf{Baselines, training and evaluation protocol.}  To analyze data efficiency and compare to prior results on \rsrw, we focus on an extreme variant of finetuning -- training from scratch. For each of the 9 \rsrw\ tasks and each of the evaluated encodings, we train a separate 36,242,769-parameter \appr\ instance using only that task's $\ttd$ dataset of full sensorimotor human-generated demonstrations. We compare \appr\ with relative positional encoding to \appr\ with absolute one and to two flavors of the Decision Transformer (DT)~\cite{chen2021decision}, which use global positional embedding.
\Cref{app:rs} and \Cref{fig:rsrw} provide more details about  model training dataset collection, and the baselines. For each task/dataset size/approach, we train on 10 seeds. 



\textbf{Results.} As \Cref{fig:rsrw} shows, \appr\ learns strong policies using at most 75 demonstrations, despite having to train a 36M-parameter model including randomly initialized vision models for tasks, most of which have complex dynamics and broad initial state distributions. Moreover, \appr\ with relative positional encoding (denoted simply as \emph{\appr} in the legend) outperforms the alternatives by as much as 20 percentage points (pp) on \rsrw's human-generated demonstration data while never losing to them. In particular, \emph{DT-global(+rew)} and, especially,
\emph{DT-global} perform far worse of both \emph{\appr} and \emph{\appr-abs}. Since all models share most of the implementation and are trained similarly when \emph{\appr} and \emph{\appr-abs} run in BC mode, we attribute \appr's advantage only to the combined effect of using human-generated training data and positional encodings. We have also trained \appr\ and \appr-abs for \mw's 5 target tasks from the previous experiment for various amounts of the available -- scripted -- demonstrations for these tasks and noticed no significant performance difference between \appr\ and \appr-abs on any task. This provides additional evidence that the utility of relative positional enconding manifests itself specifically on human-generated demonstration data.

In fact, relying on relative positional encoding allows \appr\ to achieve state-of-the art performance on all \rsrw\ tasks in this experiment, as we show and analyze empirically in \Cref{app:rs_data}.
