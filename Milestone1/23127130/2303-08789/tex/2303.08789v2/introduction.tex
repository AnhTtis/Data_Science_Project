\section{Introduction}
\vspace{-1mm}

Transformers~\cite{vaswani2017attention} have lead to breakthroughs in training large-scale general representations for computer vision (CV) and natural language processing (NLP)~\cite{brown2020gpt3}, enabling zero-shot adaptation and fast finetuning~\citep{fm2021}. At the same time, despite impressive progress, transformer-based representations haven't shown the same versatility for robotic manipulation. Some attribute this gap to the lack of suitable training data for robotics~\citep{fm2021}. We argue instead that data relevant to training robotic manipulation models is copious but has important structure that most existing training methods ignore and fail to leverage. These insights lead us to propose a novel transformer-based  \replaced{}{architecture}, called \emph{\appr}, that is capable of \replaced{zero-shot adaptation and effective finetuning thanks to being tailored to the realities of robotic manipulation data}{effective learning from realistically available robotic manipulation datasets}. 

\replaced{We observe that robotics-relevant data falls into three major categories. \textbf{(1)} The most plentiful category is comprised of ``in-the-wild'' video datasets~\cite{youtue8m}~\cite{ho2022imagenvideo}~\cite{grauman2022ego4d}. Some of them, e.g., Epic Kitchens~\cite{Damen2018EPICKITCHENS}~\cite{Damen2022RESCALING}, focus on object manipulation. In aggregation, these datasets cover an immense variety of tasks and are typically annotated with activity descriptions but contain no explicit action information for a robotic to mimic. \textbf{(2)} The second category consists of matching sequences of percepts and actions. In some datasets of this type, these sequences don't correspond to meaningful task and are generated by a scripted exploration policy~\cite{robonetv1}. In others, they come from well-defined tasks, but even in the largest such datasets, e.g.,~\citep{robonetv2}, the task coverage is modest compared to video-only data such as~\cite{grauman2022ego4d}. Nonetheless, these sensorimotor sequences capture valuable correlations between a robot's actions and changes in the environment. \textbf{(3)} The third data category, the scarcest one, consists of high-quality demonstrations for a target task in a target environment. Thus, a scalable model architecture for robotic manipulation must be able to learn {primarily} from videos, while being extra data-efficient on sensorimotor training sequences and the available demonstrations from the target environment.}{We observe that robotics-relevant data falls into three major categories: \textbf{(1)} Video-only data, which contain high-quality and potentially description-annotated demonstrations for an immense variety of tasks but have no explicit action information for a robot to mimic; \textbf{(2)} Data containing matching sequences of percepts \emph{and actions}, which are less plentiful than pure videos and don't necessarily correspond to meaningful tasks~\cite{robonetv1}, but capture valuable correlations between a robot's actions and changes in the environment and are easy to collect on a given robot; \textbf{(3)} Small sets of high-quality sensorimotor demonstrations for a target task in a target environment. Thus, a scalable model architecture for robotic manipulation must be able to learn {primarily} from videos, while being extra data-efficient on sensorimotor training sequences and the small amount target demonstrations.}

\appr, the \textit{\textbf{PL}}anning-\textit{\textbf{EX}}ecution architecture we propose, is designed to take advantage of data sources of these types. A \appr\ model has two major transformer-based components: \textbf{(I)} a task-conditioned {observational} \emph{planner} that, given a task specification and an estimate of the current world state, determines the next state to which the robot should attempt to transition, and \textbf{(II)} an \emph{executor} that, having received the desired next state from the planner, produces an action that should lead there from the current state. The executor is trained by optimizing an inverse dynamics loss over exploratory sensorimotor data of the aforementioned category \textbf{(2)}, while the planner is trained by minimizing a loss of its autoregressive predictions computed with respect to video-only trajectories of category \textbf{(1)}. The target-task data of category \textbf{(3)} can be optionally used to efficiently finetune the planner, the executor, or both.

We make three design choices that greatly help the data efficiency of \appr's training:

\vspace{-1mm}
\begin{itemize}[leftmargin=*]    
\item \emph{Learning to plan in the observation embedding space.} Rather than generating videos of proposed task execution using, e.g., stable diffusion as in~\citet{du2023unipi}, \appr\ learns to plan and execute in the low-dimensional space of observation embeddings.

\item\emph{Asymmetric learning of the embedding space.} The \replaced{}{observation embedding} space in which the executor and the planner operate is induced \replaced{either by a pretrained frozen feature-rich observation encoder, such as R3M~\cite{nair2022r3m}, or by the executor's training loss \emph{only}. Keeping the planner's gradients from entering observation encoders makes training significantly cheaper and removes the risk of latent space collapse.}{by training the observation encoder using the executor's loss \emph{only} (or even by employing a frozen feature-rich encoder such as R3M~\cite{nair2022r3m}). The planner's gradients don't affect the encoder, which reduces the cost of \appr\ training.}

\item \emph{Relative positional encodings.} 
We adopt the relative positional encodings \cite{orig} in \appr. We empirically show that in robotic manipulation the relative positional encodings significantly improve training efficiency from human-collected data compared with the \emph{absolute} positional encodings~\cite{vaswani2017attention} commonly used in the literature on transformers.
\end{itemize}
\vspace{-1mm}
Most approaches that use video-only demonstrations for pretraining in robotic manipulation produce purely visual representations (see, e.g., \cite{yen2020see,chen2021learning,nair2022r3m,Radosavovic2022RWRL}). The majority of algorithms that produce sensorimotor models need most or all of the video demonstrations to be accompanied by action sequences that generated the videos, a requirement that holds only for a small fraction available manipulation data
~\cite{mcil21rss,jang2021bc,mandi2021generalizable,gato,robonetv2,nasiriany2022learning,rt2022}. Few approaches have a dedicated trainable planning component; e.g., 
\cite{hakhamaneshi2021hierarchical,ren2021generalization,xihan2022skill,nasiriany2022learning,mees2022matters} plan in a skill space, which \appr\ can be modified to do as well. Conceptually, \appr\ falls under the paradigm of learning from observations (LfO), but existing LfO approaches don't have multitask zero-shot planning capability~\cite{nair2017knot,rados2021soil,pathak2018zs,vpt2022} or demostrate it only in low-dimensional environments across similar tasks~\cite{xu2022pgiorl}. Of the works that have used transformers for robotic manipulation~\cite{dasari2020transformers,kim2021transformer,mees2022matters,gato,rt2022}, only \citet{rt2022} have analyzed their data efficiency, and none have looked at positional embeddings as a way to improve it. Overall, the closest approach to \appr\ is the concurrently proposed UniPi~\cite{du2023unipi}. It also has counterparts of \appr's planner and executor, but its planner operates using  diffusion \emph{in the image space}~\cite{ho2022imagenvideo}, which is expensive both datawise and computationally, and may fail to model manipulation-relevant 3D object structure consistently~\cite{ho2022imagenvideo}. A more extensive discussion of prior work is provided in \Cref{app:rel_work}.


We experimentally show that \appr's planner-executor design can effectively exploit the structure of realistically available robotic manipulation data to achieve efficient learning. 
On the multi-task \mw~\cite{mw2019} benchmark, despite pretraining mostly on video data, \appr\ exhibits strong zero-shot performance on unseen tasks and can be further improved by finetuning on a small amount of video-only demonstrations. We empirically show on the challenging Robosuite/Robomimic~\cite{robosuite,robomimic2021} benchmark that, contrary to conclusions from NLP~\cite{orig}, the use of relative positional encodings significantly improves the data efficiency of \appr\ learning from human-collected demonstrations. 
