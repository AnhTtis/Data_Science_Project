
Our work lies at the intersection of scalable multi-task representation learning for robotic manipulation, learning from observations, and decision-making using transformers.

\textbf{Representation learning for robotic manipulation.} Most approaches of this kind focus on pretraining purely \emph{non-motor}, usually visual, representation models (see, e.g., \cite{yen2020see,chen2021learning,nair2022r3m,chane2023learning,karamcheti2023language}, and references therein). These models don't output actions; they are meant to be foundations on top of which a policy network is to be learned. Thus, in contrast to \appr, by themselves they can't enable zero-shot generalization to unseen tasks even in the limit of pretraining data coverage and amount. However, they are synergistic with \appr: \appr\ can use them as frozen observation encoders, as we show in \Cref{sec:gen_exp} on the example of R3M~\cite{nair2022r3m}.

Techniques that train sensorimotor models -- i.e., full-fledged generalist policies, like \appr\ -- have also been rising in prominence. Some of them~\cite{yu2018daml,zhou2020watch,li2022meta} are based on \emph{meta learning}~\cite{finn2017os}. However, \citet{mandi2022effectiveness} have shown multi-task pretraining followed by finetuning to be more effective when the task distribution is broad, and several approaches~\cite{mcil21rss,jang2021bc,mandi2021generalizable,gato,robonetv2,nasiriany2022learning,rt2022} follow this training paradigm as does \appr. At the same time, most of them need pretraining data consisting of high-quality demonstrations in the form of matching videos \emph{and} action sequences. While the quality requirement can be relaxed using offline RL, as, e.g., in \citet{singh2020cog}, in order to enable generalization across broad task distributions these sensorimotor training demonstrations need correspondingly broad task coverage. This assumption is presently unrealistic and ignores the vast potential of the available video-only data --- the weakness \appr\ aims to address. 

Among the sensorimotor representation learning methods that, like \appr, try to learn from both video-only and sensorimotor data are \citet{schmeckpeper2020reinforcement}, \citet{mcil21rss}, and \citet{mees2022matters}. \citet{schmeckpeper2020reinforcement} consider single-task settings only and require the video-only and sensorimotor data to provide demonstrations for the same tasks. \citet{mcil21rss} and \citet{mees2022matters} allow the sensorimotor data to come from exploratory policies rather than task demonstrations but insist that this data must be generated from meaningful \emph{skills}, a strong assumption that \appr\ avoids. 

Architecturally, most aforementioned approaches use monolithic models that don't have separate components for planning and execution like \appr. Notable exceptions are methods that mine skills from pretraining data, embed them into a latent space, and use the latent skill space for accelerated policy learning of new tasks after pretraining~\cite{hakhamaneshi2021hierarchical,ren2021generalization,xihan2022skill,nasiriany2022learning,mees2022matters}. This is akin to planning in the skill space. 
\appr\ can accommodate this approach hierarchically by having, e.g., a CVAE-based high-level planning model~\cite{lynch2019play} produce a task-conditioned sequence of skill latents and feeding them into a skill-conditioned planning model that will plan in the observation embedding space. However, in this work's experiments, for simplicity \appr\ plans in the observation embedding space directly.

\textbf{Learning and imitation from observations (I/LfO)} I/LfO has been used in robotic manipulation both for single-task tabula-rasa policy learning~\cite{nair2017knot,rados2021soil} and pretraining~\cite{pathak2018zs}. \citet{pathak2018zs} is related to \appr\ in spirit but lacks a counterpart of \appr's planner. As a result, it can't complete an unseen task based on the task's goal description alone: it needs either a sequence of subgoal images starting at the robot's initial state or a sequence of landmarks common to all initial states of a given task. Beyond robotics, a type of LfO was also employed by~\citet{vpt2022} and \citet{venuto2022multi} to pretrain a large sensorimotor model for Minecraft and Atari, respectively. This model, like \citet{pathak2018zs}'s, doesn't have a task-conditioned planning capability and is meant to serve only as a finetunable behavioral prior. \citet{xu2022pgiorl} investigate an LfO method akin to \appr\ in low-dimensional environments, where it side-steps the question of choosing an appropriate representation for planning, the associated efficiency tradeoffs, and pretraining a generalizable planning policy.


Overall, the closest approach to \appr\ is the concurrently proposed UniPi~\cite{du2023unipi}. It also has a universal planner meant to be pretrained on a large collection of available videos, as well as an executor that captures inverse dynamics. However, UniPi ignores the issue of data efficiency and plans in the space of images (observations), using diffusion~\cite{ho2022imagenvideo}, rather than in the latent space of their embeddings. This is expensive to learn and potentially detrimental to plan quality.  Latents even from statically pretrained image encoders are sufficient to capture object manipulation-relevant details from videoframes~\cite{yen2020see}, whereas diffusion models can easily miss these details or model their 3D structure inconsistently~\cite{ho2022imagenvideo}. Indeed, despite being conceptually capable of closed-loop control, for computational efficiency reasons UniPi generates open-loop plans, while \appr\ interleaves planning and execution in a closed loop. 

\textbf{Transformers for decision making and their data efficiency.} After emerging as the dominant paradigm in NLP~\cite{brown2020gpt3} and CV~\cite{dosovitskiy2021image}, transformers have
been recently applied to solving general long-horizon decision-making problems by imitation and reinforcement learning~\cite{chen2021decision,janner2021reinforcement,shang2021starformer,WMG,GTrXL}, including multi-task settings~\cite{dance2021conditioned} and robotic manipulation~\cite{dasari2020transformers,kim2021transformer,mees2022matters,gato,rt2022}. \citet{mees2022matters} provide evidence that in robotic manipulation transformers perform better than RNNs~\cite{mcil21rss} while having many fewer parameters. Of all these works, only \citet{gato} uses relative positional encoding, and only by ``inheriting'' it with the overall Transformer-XL architecture~\cite{orig}, without motivating its effectiveness for decision-making.


\textbf{Task specification formats.} Task specification modality can significantly influence the generalization power of models pretrained on multi-task data. Common task conditioning choices are images of a task's goal~\cite{robonetv2}, videos of a task demonstration by a person~\cite{yu2018daml,jang2021bc} or by a robot~\cite{finn2017os,mandi2021generalizable}, and language descriptions~\cite{mcil21rss,jang2021bc,mees2022matters,ahn2022can,rt2022}. \appr\ is compatible with any of these formats; in the experiments, we use goal images.