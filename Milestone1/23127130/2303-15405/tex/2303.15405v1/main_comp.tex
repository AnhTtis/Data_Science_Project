\documentclass[a4paper,twocolumn,11pt]{quantumarticle}
\pdfoutput=1
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage[T1]{fontenc}
\usepackage{mathtools}
\usepackage{algorithmcx}
\usepackage{algpseudocodex}
\usepackage{braket}
\usepackage[colorlinks=true,linkcolor=blue,urlcolor=blue,citecolor=blue,pdfusetitle]{hyperref}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{amssymb}
\usepackage{comment}

\bibliographystyle{quantum}

\usepackage{tikz}
\usepackage{lipsum}

\newtheorem{theorem}{Theorem}

\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}
\algnewcommand\algorithmicprecompute{\textbf{Precompute:}}
\newcommand{\tr}{\mathrm{tr}}
\DeclareMathOperator{\vectorize}{Vec}
\DeclareMathOperator{\unvectorize}{Unvec}

\begin{document}

\title{A Gillespie algorithm for efficient simulation of quantum jump trajectories}

\author{Marco Radaelli}
\affiliation{School of Physics, Trinity College Dublin, Dublin 2, Ireland}
\email{radaellm@tcd.ie}
\orcid{0000-0002-9210-5779}
\author{Gabriel T. Landi}
\affiliation{Department of Physics and Astronomy, University of Rochester, Rochester, New York 14627, USA}
\email{gabriel.landi@rochester.edu}
\author{Felix C. Binder}
\affiliation{School of Physics, Trinity College Dublin, Dublin 2, Ireland}
\email{quantum@felix-binder.net}
\maketitle

\begin{abstract}
The  jump unravelling of a quantum master equation decomposes the dynamics of an open quantum system into abrupt jumps, interspersed by periods of coherent dynamics where no jumps occur. 
Simulating these jump trajectories is computationally expensive, as it requires very small time steps to ensure convergence. 
This computational challenge is aggravated in regimes where the coherent, Hamiltonian dynamics are fast compared to the dissipative dynamics responsible for the jumps. 
Here, we present a quantum version of the Gillespie algorithm that bypasses this issue by directly constructing the waiting time distribution for the next jump to occur.
In effect, this avoids the need for timestep discretisation altogether, instead evolving the system continuously from one jump to the next.
We describe the algorithm in detail and discuss relevant limiting cases. To illustrate it we include four example applications of increasing physical complexity. These additionally serve to compare the performance of the algorithm to alternative approaches -- namely, the widely-used routines contained in the powerful Python library QuTip. We find significant gains in efficiency for our algorithm and discuss in which regimes these are most pronounced. 
Publicly available implementations of our code are provided in Julia and Mathematica. 


\end{abstract}

\section{Introduction}

Quantum master equations have become an absolutely essential methodology for most areas of quantum physics. They are used to describe experiments in a wide variety of platforms, from quantum optics to mesoscopic electronics. 
The quintessential Gorini–Kossakowski–Sudarshan–Lindblad (GKSL) master equation has the form~\cite{Lindblad1976,Gorini1976,breuer2007,schaller2014}
\begin{equation}\label{M}
    \frac{d\rho}{dt} = \mathcal{L}\rho = -i [H,\rho] + \sum_{k=1}^r \mathcal{D}[L_k] \rho,
\end{equation}
where $\mathcal{L}$ is the Liouvillian, 
$\rho$ the state of the system, 
$H$ the Hamiltonian, and $\mathcal{D}[L]\bullet = L \bullet L^\dagger - \frac{1}{2} \{L^\dagger L, \bullet\}$ a Lindblad dissipator.

While a master equation describes the ensemble-averaged evolution of the system's density matrix $\rho(t)$, one can also \emph{unravel} it in terms of specific quantum trajectories~\cite{Wiseman_2009,Carmichael2000}. 
The quantum jump unravelling (QJU), in particular, separates the dynamics into stochastic process, consisting of abrupt jumps occurring at random times, and unitary no-jump evolution in between jumps. 
The jumps are associated with the terms~$L_k\rho L_k^\dagger$, each representing a possible ``jump channel''. 
The QJU method has been extensively employed in various contexts, for many decades.
Its main motivation lies in the fact that, in many experimentally relevant systems, the quantum jumps are directly associated with clicks in specific detectors, as is the case, for instance, for photon-detection in optical systems. Here, the stochastic dynamics can be used to obtain the full counting statistics of photo-detection~\cite{Kewming2022}.
A beautiful recent example is the experiment reported in Ref.~\cite{Fink_2018}, which used photo-detection stochastic trajectories to build up the statistics necessary to demonstrate a driven-dissipative phase transition. 
Similarly, in mesoscopic physics, the QJU can be used to model coherent electron tunneling from quantum dots to metallic leads~\cite{Goan_2001_1,schaller2014,Landi2022}. 
In this case, the  jump channels correspond to the injection/extraction of an electron onto/from the quantum dot.

Sometimes, the QJU can also be used as a method to simulate the solution $e^{\mathcal{L}t} \rho(0)$ of Eq.~\eqref{M}. Such a stochastic simulation is formally equivalent to what is often referred to as \emph{Monte Carlo Wavefunction} (MCW).
This is because the QJU allows one to work with pure states which are numerically much easier to handle than mixed states in systems with high-dimensional Hilbert spaces due to correspondingly lower memory requirements~\cite{gardiner2004}.

The main shortcoming of the QJU/MCW is that it may require very small time steps for the integration of the stochastic master equation to converge. 
This happens for the following reason. 
In quantum coherent systems, the dynamics is always a mixture of the unitary dynamics, which cause no jumps, and dissipative terms responsible for jumps.
Generically, these two processes may occur on very different timescales. 
In simulations, small time steps may therefore be necessary, for instance, to account for the unitary dynamics, even though the jumps occur on a much longer timescale. 

A similar problem is also found in classical master equations evolving under different timescales. 
And, in that community, the Gillespie algorithm \cite{Gillespie_1976, Gillespie_1977} stands out as an extremely efficient alternative method for simulating the behaviour of such dissipative dynamics. 
The algorithm is based on the Waiting-Time Distribution (WTD) $W(t)$, which describes the time it takes until the next jump occurs~\cite{brandes_2008,Landi2023} . 
Instead of integrating an equation over small time steps, the Gillespie algorithm samples a random time $T$ from $W(t)$. This represents the time the next jump will occur. 
Next, it  randomly selects one of the possible jump channels. Hereby, both samples are picked with the correct probability.
In this fashion, the system is directly propagated forward in time, to the configuration after the jump. 
A major advantage of classical master equations is that $W(t)$ is always exponentially distributed, making it very easy to sample the random times $T$. 
In addition, classical master equations belong to the class of \emph{renewal processes}: after the jump the state of the system is completely reset, losing any possible memory of previous configurations. As a consequence, $W(t)$ only depends on the current configuration of the system, and not on its history. 

In quantum systems, these two features are generally absent: the unitary dynamics produces WTDs which are not exponentially distributed, and jumps do not fully reset the state of the system in general. 
Nonetheless, as we show in this paper, a quantum version of the Gillespie algorithm can be derived and implemented, and indeed results in an efficient simulation method.
In particular, the quantum Gillespie algorithm we implement has a moderate overhead in terms of quantities that must be pre-computed. Once this is done, the simulation of the quantum trajectories becomes comparatively fast. And, more importantly, the algorithm does not require small timestep discretisation to ensure convergence. 
Thus, the algorithm is particularly suited for simulating long-time trajectories. 

This paper is structured as follows.
After a brief overview of the QJU method in Sec.~\ref{sec:QJU}, we detail our algorithm in Sec.~\ref{sec:alg}, and then compare it to alternative approaches in Sec.~\ref{sec:bench}. Finally, illustrative examples are provided in Sec.~\ref{sec:examples}. 
For the latter, we compare our method with the standard QJU solvers from QuTip. 
An implementation of our algorithm in Mathematica is publicly available in the Melt library~\cite{implementation_Melt}, and an implementation in Julia in Ref.~\cite{implementation_Julia}.


\section{Quantum Jump Unravelling}
\label{sec:QJU}

In this section, we briefly review and motivate the QJU. 
For an infinitesimal time $dt$, the evolution of Eq.~\eqref{M} can be decomposed as 
\begin{equation}\label{evo_dt}
    \rho(t+dt) = e^{\mathcal{L} dt} \rho(t) = \sum_{k=0}^r M_k \rho M_k^\dagger,
\end{equation}
where $M_k$ are Kraus operators: for $k=1,\ldots,r$ they read $M_k = \sqrt{dt} L_k$, while for $k=0$ we have $M_0 = 1- i dt H_e$, where 
\begin{align}\label{He}
    H_e =& H - \frac{i}{2}  J\text{, with} \\\label{J}J :=& \sum_k L_k^\dagger L_k,
\end{align}
is a non-Hermitian Hamiltonian. 
The Kraus operators satisfy 
$M_0^\dagger M_0 + \sum_{k=1}^r M_k^\dagger M_k = \mathbb{I} + \mathcal{O}(dt)^2$.

The decomposition in~\eqref{evo_dt} motivates the interpretation in terms of quantum jumps.
For each timestep, one $M_k$ is chosen with probability 
$p_k = \tr(M_k \rho M_k^\dagger)$, and the system is updated to
\begin{equation}\label{update_rule}
    \rho \to \frac{M_k \rho M_k^\dagger}{\tr(M_k \rho M_k^\dagger)}.
\end{equation}
If $k=1,\ldots,r$, we say a jump occurred in ``channel'' $k$. 
Otherwise, if $k=0$, no jump occurred. 
The latter is much more likely since $p_0 \sim \mathcal{O}(1)$ while $p_k \sim \mathcal{O}(dt)$.
This therefore yields a \emph{quantum trajectory}; i.e., a stochastic evolution of the system consisting of a few abrupt jumps, connected by a no-jump (smooth) evolution described by $M_0$~\cite{Wiseman_2009,Rouchon_2022,Qutip}.

The quantum trajectory is described by a set of outcomes $(k_1,k_2,\ldots)$, corresponding to the randomly chosen operators $M_k$ at each time step. 
One can attribute the outcomes with $k=1,\ldots,r$ to a ``click'' in a detector, while $k=0$ represents no click. 
The solution of Eq.~\eqref{M}, $\rho(t) = e^{\mathcal{L} t} \rho(0)$, is called the \emph{unconditional evolution}, while the stochastic trajectory generated by the update rule~\eqref{update_rule} is said to be \emph{conditional}, since it is conditioned on the specific set of outcomes $(k_1,k_2,\ldots)$.
The unconditional evolution is  recovered by averaging the conditional evolution over multiple trajectories.




\section{The Gillespie algorithm}
\label{sec:alg}

A typical quantum trajectory will  have the form 
\begin{equation*}
    000000001000000000200000000100000001\ldots,
\end{equation*}
consisting of many $0$s, interspersed by rare jumps (in this case in channels labelled as 1 and 2). 
Clearly, it is simpler to just label the trajectories by the random times between jumps $T_i$, and the channel $k_i$ that each jump went into. That is, the quantum trajectory can instead be described by the set of outcomes
\begin{equation}\label{gillespie_trajectories}
    (T_1,k_1), (T_2,k_2), (T_3,k_3),\ldots.
\end{equation}
The Gillespie algorithm~\cite{Gillespie_1976, Gillespie_1977} avoids integrating the update rule (Eq.~\eqref{update_rule}) over infinitesimal timesteps, by instead sampling the pairs $(T_n,k_n)$ with correct probabilities.
This is done using the waiting-time distribution (WTD) which can be built as follows. 
If no jump occurs for a time $T$, at which point a jump into channel $k$ happens, then the state of the system is updated, up to a normalisation, to: 
\begin{equation}\label{gillespie_update}
    \rho \to L_k e^{-i H_e T} \rho e^{i H_e^\dagger T} L_k^\dagger.
\end{equation}
Note here that the term $e^{-i H_e T}$ results from the sequential application of the no-jump Kraus operator, in the infinitesimal limit: 
\begin{align}
\lim\limits_{n\to \infty} (\mathbb{I}-i H_e t/n)^n = e^{-i H_e t}. 
\end{align}
The normalisation factor for the rhs of Eq.~\eqref{gillespie_update} yields precisely the WTD~\cite{brandes_2008,Landi2023} 
\begin{equation}\label{WTD_basic}
    W(t,k|\rho) = \tr\big\{ L_k^\dagger L_k e^{-i H_e t} \rho e^{i H_e^\dagger t}\big\}.
\end{equation}
If we can  sample over the WTD, then we may evolve the system directly from one jump to the next, rather than having to integrate the dynamics step-by-step. 

In order to arrive at a sampling procedure for Eq.~\eqref{WTD_basic} we first decompose it into two parts as $W(t,k|\rho) = P(k|t,\rho) W(t|\rho)$, where 
\begin{align}\label{WTD_J}
    \begin{split}
        W(t|\rho) =& \sum_k W(t,k|\rho)\\ =& \tr\big\{ J e^{-i H_e t} \rho e^{i H_e^\dagger t}\big\},
    \end{split}
\end{align}
with $J = \sum_k L_k^\dagger L_k$, as in Eq.~\eqref{J}. The normalisation condition for $W(t|\rho)$ takes the form
\begin{equation}
    \int_0^\infty dt ~W(t|\rho) = 1,\qquad  \forall\rho.
    \label{eq:normalisation_wtd}
\end{equation}
Physically, this implies that a jump to some channel has to eventually occur for any initial state, i.e., dark subspaces are not allowed. 
Letting $\tilde{\rho}=e^{-i H_e t} \rho e^{i H_e^\dagger t}$, the remaining factor is given by
\begin{equation}\label{P_k_t_rho}
    P(k|t,\rho) = \frac{\tr\big\{ L_k^\dagger L_k \tilde{\rho}\big\}}{\tr\big\{ J \tilde{\rho}\big\}}.
\end{equation}
The sampling is thus split in two: first, we sample a time $T$ using Eq.~\eqref{WTD_J},  evolving the system from $\rho\to \tilde{\rho}$. 
Next, we choose which channel the system will jump to by sampling over Eq.~\eqref{P_k_t_rho}.
This last part is comparatively straightforward, as there are only $r$ options. 
The most challenging technical issue is how to sample from Eq.~\eqref{WTD_J}.

To overcome this challenge, we write 
\begin{align}
    W(t|\rho) =& \tr\{Q(t) \rho\big\}\text{, with}\\
    \label{W_t_Q_t_basic}
    Q(t) =& e^{i H_e^\dagger t} J e^{-i H_e t}.
\end{align}
We can pre-compute $Q(t)$ for a set of times ${\tt ts}$. 
This set should be sufficiently fine-grained to resolve the fine structure in $W(t|\rho)$, and should go up to a sufficiently large time to ensure that all possible jump times are taken into account. 
For each state $\rho$, we then compute a list $W(t|\rho) = \tr\{Q(t) \rho\big\}$ for $t\in {\tt ts}$, and sample one element $T\in {\tt ts}$ from it.
A pseudocode implementation of our algorithm is provided at the end, labelled Algorithm~\ref{alg:Gillespie_core}. 

{\bf Pure states:} Initial pure states remain pure throughout the evolution along  any trajectory, provided that all the existing jump channels are monitored. This results in a significant advantage in memory requirements, and in speed. All relevant formulas above continue to hold provided we replace $\rho$ with $|\psi\rangle\langle \psi|$:
\begin{align}
    W(t|\psi) &= \langle \psi| Q(t) |\psi\rangle, 
    \\[0.4cm]
    P(k|t,\psi) &= \frac{\langle \tilde{\psi} | L_k^\dagger L_k |\tilde{\psi}\rangle}{\langle \tilde{\psi} | J |\tilde{\psi}\rangle},
    \\[0.4cm]
    |\psi\rangle &\to L_k e^{-i H_e T} |\psi\rangle,
\end{align}
where $|\tilde{\psi}\rangle = e^{-i H_e T}|\psi\rangle$. \\
An implementation of the Gillespie algorithm for the simpler case of pure states is available in Julia~\cite{implementation_Julia}. The pure states implementation also allows for smaller memory consumption when simulating the (full-monitoring) evolution of ensembles of pure states. With Algorithm~\ref{alg:Gillespie_core} below we only report the more general mixed-state implementation explicitely.

{\bf Renewal processes:} For this important subclass we have~\cite{brandes_2008}
\begin{equation}\label{renewal}
   \frac{L_k \rho L_k^\dagger}{\tr(L_k \rho L_k^\dagger)} =  \sigma_k,
\end{equation}
for some state $\sigma_k$. 
Hence, the state after the jump is always fully reset to a specific $\sigma_k$, depending only on the jump channel.
In this case, there are only a finite number of WTDs $W(t|\sigma_k)$.
They can therefore all be pre-computed, greatly speeding up the entire process. 
Simulating renewal processes is therefore rather straightforward with the proposed method. 
The true power of the here-proposed Gillespie algorithm, however, emerges for non-renewal processes, where the state $\rho$ is only partially affected by each jump. 

{\bf Sampling in-between jumps:} The Gillespie algorithm yields the state of the system at random times after each jump, according to the update rule~\eqref{gillespie_update}.
For many applications, this suffices. 
In other cases, however, one also requires the state at arbitrary times in between jumps. Only a slight additional step is required to obtain such intermediate states.
Consider the update map~\eqref{gillespie_update}, for a certain time $T$ between jumps. 
The state at any intermediate time $t<T$ before the next jump is then simply
\begin{equation}
    \frac{e^{-i H_e t} \rho e^{i H_e^\dagger t}}{\tr\big\{ e^{-i H_e t} \rho e^{i H_e^\dagger t}\big\}}, \qquad t < T. 
\end{equation}
Computing this Hamiltonian evolution does not require infinitesimal timesteps. 
Instead, one can proceed as in the usual Trotter-decomposition for unitary dynamics \cite{Nielsen2002, Georgescu2014}. 
Suppose one wishes to obtain the state at steps $\Delta t$, which does not have to be small. Define $V = e^{-i H_e \Delta t}$, then simply iterate $\rho \to V\rho V^\dagger$.
A pseudocode implementation is presented as Algorithm \ref{alg:Gillespie_filling} below.

{\bf Partial monitoring:} We can generalise the above to the case where only a subset $\mathcal{M}$ of the jump operators $L_1, \ldots, L_r$ are monitored. 
Define the jump super-operators
\begin{equation}
    \mathcal{J}_k \rho = L_k \rho L_k^\dagger, 
    \qquad k \in \mathcal{M},
\end{equation}
as well as the no-jump super-operator $\mathcal{L}_0 = \mathcal{L} - \sum_{k\in\mathcal{M}} \mathcal{J}_k$ (If all channels are monitored then $\mathcal{L}_0 \rho = -i (H_e \rho - \rho H_e^\dagger)$, with $H_e$ defined in Eq.~\eqref{He}). 
Eq.~\eqref{WTD_J} is then replaced by 
\begin{align}\label{WTD_L0}
    W(t|\rho) =& \tr\big\{ J e^{\mathcal{L}_0 t} (\rho)\big\}\text{, with}\\ J =& \sum_{k\in \mathcal{M}} L_k^\dagger L_k.
\end{align}
The normalisation condition \eqref{eq:normalisation_wtd} is here tantamount to $\mathcal{L}_0$ being invertible. \\
Similarly, Eq.~\eqref{P_k_t_rho} becomes
\begin{align}
    P(k|t,\rho) =& \frac{\tr\big\{ L_k^\dagger L_k \tilde{\rho}\big\}}{\sum_q \tr\big\{ L_q^\dagger L_q\tilde{\rho}\big\}}\text{, with}\\ \tilde{\rho} =& e^{\mathcal{L}_0 t} \rho.
\end{align}
In this case, one must pre-compute $Q(t) = e^{\mathcal{L}_0^\dagger t} (J) $, with $\mathcal{L}_0^\dagger$ the adjoint super-operator of $\mathcal{L}_0$.

{\bf Classical master equation:} The original Gillespie algorithm~\cite{Gillespie_1976,Gillespie_1977} was developed for classical (Pauli) rate equations. This is actually a particular case of the present quantum version, corresponding to Eq.~\eqref{M} with $H=0$ and jump operators $L_{ij} = \sqrt{W_{ij}} |i\rangle\langle j|$, describing jumps between (orthonormal) basis states $|i\rangle$, and $|j\neq i\rangle$, with transition rate $W_{ij}$.
For this case, Eq.~\eqref{J} reduces to 
$H_e = - \tfrac{i}{2} J$, with 
\begin{equation}
\begin{split}
    J =& \sum_{i\neq j} L_{ij}^\dagger L_{ij} = \sum_{i\neq j} W_{ij} |j\rangle\langle j|\\ =& \sum_j R_j |j\rangle\langle j|,
    \end{split}
\end{equation}
where $R_j = \sum_{i\neq j} W_{ij}$ is the \emph{escape rate} for the system to leave state $|j\rangle$. 
As $[H_e, J]=0$, now Eq.~\eqref{WTD_J} simplifies to 
\begin{equation}
\begin{split}
    W(t|\rho) =& \tr\big\{ J e^{-J t} \rho\big\} \\=& \sum_j R_j e^{-R_j t} \langle j|\rho|j\rangle.
    \end{split}
\end{equation}
The process is renewal (Eq.~\eqref{renewal} is satisfied).  
Hence, $\rho$ -- the state at the previous jump -- will actually be $\rho = |i\rangle\langle i|$, for some state $|i\rangle$. 
Hence, the WTD becomes 
\begin{equation}
    W(t|i) = R_i e^{- R_i t},
\end{equation}
which is an exponential distribution with rate $R_i$. 
Sampling it is thus trivial\footnote{If $r$ is uniform between 0 and 1, then $-\frac{1}{R_i} \ln r$ is exponentially distributed with rate $R_i$.}, which is the reason why the Gillespie algorithm is so efficient in the classical context.



\section{Comparison with current methods}
\label{sec:bench}
In this section, we compare our Quantum Gillespie algorithm to other methods currently in use.

The most naive way to simulate a jump process is using a direct discretisation of time: at each time step, the probability of jump (or absence thereof) is determined, and the state is evolved accordingly. This method gives a faithful representation of the process, for a small enough time increment. 
Very often, however, the timescale of the jumps (i.e. the average time difference between two subsequent jumps) is very different from the typical timescale of the Hamiltonian evolution. Therefore, the vast majority of time steps involve no jump, while nonetheless requiring significant computational power.

A more efficient method is the Monte Carlo Wavefunction (MCW) method, used in QuTip's \verb"mcsolve" or QuantumOptics.jl \verb"mcwf" function \cite{Qutip, QuantumOptics}, which involves numerical inverse-function sampling of the waiting time distribution. This method is significantly faster than the previous one, but still involves the solution of a non-Hermitian Schr{\"o}dinger equation until a random time\footnote{For details about the implementation of the function, see QuTip documentation \href{https://QuTip.org/docs/latest/guide/dynamics/dynamics-monte.html}{https://QuTip.org/docs/latest/guide/dynamics/dynamics-monte.html}}. Since it involves stopping this evolution at random times, dependent on the jump probability, this method's speed depends critically on the jump rates: higher jump rates give rise to much quicker simulations. 

Despite the improvements, however, MCW still suffers from a high sensitivity to the discretisation time $dt$. 
This becomes particularly problematic when one wishes to carry out the simulations for very long times, as the errors tend to accumulate (unless the process is renewal). A comparison between the long-time results of QuTip and Gillespie for the example system presented in Sect.~\ref{sect:Kerr_model} is shown in Fig.~\ref{fig:Kerr_model}.

Our method is particularly suitable for all cases in which the difference in the timescales between Hamiltonian dynamics and jumps is evident. In fact, it allows leaping directly from a jump to the following one, without wasting time doing calculations in between these time steps. 
Additionally, it does not suffer from any discretisation time issues, and errors do not accumulate over the long-run. 

The only main cost in our algorithm is in the pre-computation of the quantities $Q(t)$ in Eq.~\eqref{W_t_Q_t_basic}. 
This can be expensive, especially when the dimensionality of the system is large. 
We have found that this essentially accounts for most of the computational time: once these quantities are pre-computed, generating the dynamics becomes extremely fast, as will be illustrated through specific examples in the next section. 
This, together with the fact that errors do not accumulate, means that our algorithm is particularly suited for simulating long-time behaviour. 

This concludes our presentation of the algorithm. In the following section, we showcase its use for the simulation of different dynamical models with increasing physical complexity.


\section{Example applications}\label{sec:examples}
To better illustrate the advantages and shortcomings of the Gillespie algorithm, in this section we present a range of applications for various systems of physical interest.

In all simulations below, we used the Julia implementation of the Gillespie algorithm, available at \cite{implementation_Julia}. They were performed on an Apple MacBook Pro with Apple M1 processor and 8 GB of RAM. We compared the performances of our algorithm with QuTip. A direct comparison with another well-known library, QuantumOptics.jl, was not directly feasible because, to the best of our knowledge, it does not give access to the jump times for the evolution of mixed states.

\subsection{Single-qubit resonant fluorescence}
We considered first a very well-studied quantum optics problem, one-qubit resonant fluorescence \cite{Wiseman_2009, Carmichael_1989}. A single qubit evolves with Hamiltonian
\begin{equation}
    H = \Delta \sigma^z + \Omega \sigma^x
\end{equation}
(here: $\hbar = 1$), where $\Delta$ represents a detuning term and $\Omega$ is the Rabi frequency due to coupling with an external electromagnetic field. Given a leak rate $\gamma$ for the qubit, its time evolution is described by the Lindbladian super-operator
\begin{equation}
\begin{split}
    \mathcal{L}\rho =& -i[H, \rho] \\
    & + \frac{\gamma}{2} \left(2 \sigma^- \rho \sigma^+ - \{\sigma^+\sigma^-, \rho\}\right),
\end{split}
\end{equation}
where $\sigma^\pm = \sigma^x \pm i \sigma^y$. The process is renewal, and the form of the waiting time distribution is analytically known \cite{Carmichael_1989}. We simulated the process both with QuTip and Gillespie, and we obtained the waiting time distributions shown in Fig.~\ref{fig:resonant_fluorescence}. To simulate 1000 trajectories, Gillespie needed approximately 2 minutes, QuTip 10. 
\begin{figure}[htbp]
    \includegraphics[width=\columnwidth]{figures/FIG_both_resonant_fluorescence.pdf}  
    \caption{Single-qubit resonant fluorescence: simulated waiting time distribution with 1000 trajectories, with QuTip and with Gillespie. Parameters: $\Delta = 0$, $\Omega = \gamma = 0.5$. }\label{fig:resonant_fluorescence}
\end{figure}

\subsection{Double qubit non-renewal process}
In order to discuss the behaviour of the Gillespie algorithm when relaxing the renewal hypothesis, we considered a slightly more complex physical system. Two qubits are coupled with an exchange interaction (coupling intensity $g$); one of them is driven by an external electromagnetic field (Rabi frequency $\Omega$), while the other can decay with rate $\gamma$. The Hamiltonian and the Lindbladian are:
\begin{equation}
    H = \Omega \sigma_1^x + g \left(\sigma_1^+ \sigma_2^- + \sigma_1^- \sigma_2^+\right);
\end{equation}
\begin{equation}
\begin{split}
    \mathcal{L}\rho = &  -i[H,\rho] \\
    & + \frac{\gamma}{2} \left(2 \sigma^-_2\rho\sigma^+_2 - \{\sigma^+_2 \sigma^-_2, \rho\}\right).
\end{split}
\end{equation}
The waiting time distributions are shown in Fig.~\ref{fig:double_qubit}. In this case as well, Gillespie allows for a very significant speedup. We fixed the time for the calculation at 5 minutes, and Gillespie was able to simulate approximately 3000 trajectories, while QuTip produced about 400. 

\begin{figure}[htbp]
    \includegraphics[width=\columnwidth]{figures/FIG_both_double_qubit.pdf}
    \caption{Double-qubit system: simulated waiting time distribution. The time allowed for the simulation was 5 minutes, during which QuTip simulated approximately 400 trajectories, Gillespie ca. 3000. The two waiting time distributions do not match very precisely, due to the limited number of trajectories simulated by QuTip. Of course, if enough computation time had been allowed, the two curves would have overlapped much more. Parameters: $\Omega = 3$, $\gamma = 0.1$, $g=0.3$.}
    \label{fig:double_qubit}
\end{figure}

\subsection{Mesoscopic charge qubit under continuous quantum measurement}
As a further application, we considered the model of mesoscopic charge qubit presented in Ref.~\cite{Goan_2001_1, Wiseman_2009, Goan_2001}. The system is given by two coupled quantum dots, with Hamiltonian
\begin{equation}
    H = \sum_{j=1}^2 \omega_j \sigma_j^+\sigma_j^- + \Omega(\sigma_1^+ \sigma_2^- + \sigma_1^-\sigma_2^+),
\end{equation}
where $\omega_1$ and $\omega_2$ represent the electron annihilation/creation energy for the respective dot, and $\Omega$ sets the coupling strength. The Lindbladian is given by
\begin{equation}\label{Example_3_Liouvillian}
    \mathcal{L}\rho = - i [H,\rho] + \mathcal{D}[\mathcal{T} + \chi n_1]\rho,
\end{equation}
where $\mathcal{T}$ and $\chi$ represent the tunnelling amplitudes between the two dots.

We reproduced the stochastic trajectory for the expectation value of the population-difference operator $Z_c$ (Fig.~2 in Ref.~\cite{Goan_2001}) using the Gillespie algorithm in Fig.~\ref{fig:Goan_Milburn}, together with the stochastic sequence of jump times. This simulation shows the possibility to interpolate the states between the jump times computed using the Gillespie algorithm. 
\begin{figure}[htbp]
    \includegraphics[width=\columnwidth]{figures/FIG_both_Goan_Milburn.pdf}
    \caption{Single-trajectory simulated expectation value of the population difference for the mesoscopic charge model, computed both with QuTip (dotted blue line) and Gillespie (solid orange line). For Gillespie, the expectation value of the observable $Z_c$ on the state right after the jump is represented with a solid circle. In the lower panels, the corresponding jump times are shown, as computed by the two methods. Parameters: $\omega_1 = \omega_2 = \Omega = \mathcal{T}=\chi = 1$.}
    \label{fig:Goan_Milburn}
\end{figure}

Note that the Gillespie algorithm does not give a relevant advantage when only a small number of trajectories is considered. In some cases, due to the pre-computation stage, it can actually take longer than time-discretisation-based algorithms. Instead, the power of the Gillespie algorithm we propose comes to bear when a large number of trajectories is taken into account.
For this specific model, this can be quite advantageous because quite often one has $\mathcal{T} \gg \chi$ in Eq.~\eqref{Example_3_Liouvillian}. This means that many jumps will occur, but most of these will be uninformative. 
It therefore becomes relevant to acquire the statistics over a very large number of jumps, which is where the Gillespie method becomes particularly powerful. 


\subsection{Kerr model}
\label{sect:Kerr_model}
We considered finally the application of the Gillespie method to the simulation of the famous Kerr model~\cite{Drummond1980} described by the Hamiltonian
\begin{equation}
    H = \Delta a^\dagger a + \frac{U}{2} a^\dagger a^\dagger a a + F^* a^\dagger + F a,
\end{equation}
where $a$ represents a bosonic annihilation operator, $\Delta$ is a detuning parameter, $U$ represents the intensity of the non-linear coupling, and $F$ is the driving strength of an external laser. The system undergoes a jump-like evolution, described by the Lindbladian super-operator
\begin{equation}
    \mathcal{L}\rho = -i \left[H, \rho\right] + \frac{\gamma}{2}\left(2 a \rho a^\dagger - a^\dagger a \rho - \rho a^\dagger a\right),
\end{equation}
where $\gamma$ is the decay rate. 
Quantum trajectories for this model have recently been obtained experimentally in~\cite{Fink_2018}, and our approach can be used to directly compare with that experiment. 
In particular, we simulated the behaviour of the expectation value of the number operator $a^\dagger a$ as a function of time, both with QuTip and our Gillespie algorithm. The results for a single trajectory are shown in Fig.~\ref{fig:Kerr_model}. Contrary to the previous example, in this case we did not fill the gaps between the jumps in the case of Gillespie: the expectation value of the number operator is then known only for the state right after the jump.  This is compensated by a significant gain in performances on the side of Gillespie: due to pre-computation of the heaviest parts of the simulation, obtaining data for a large number of trajectories can be much faster than with other methods.
\begin{figure}[htbp]
    \includegraphics[width=\columnwidth]{figures/FIG_both_Fink.pdf}
    \caption{Single-trajectory simulated expectation value of the number operator for the Kerr model presented in the main text, computed with both QuTip (dotted blue line) and Gillespie (solid orange circles). The expectation value of $N$ on the steady state is represented by the red dashed line. The lower panels show the jump times obtained for the trajectories at the top, corresponding to each method, respectively. Note how, for long times, QuTip does not converge due to the error accumulation effect detailed in the main text, while Gillespie does. Parameters: $\Delta=1.5$, $U=0.05$, $F=3.27$, $\gamma=1$.}
    \label{fig:Kerr_model}
\end{figure}

\section*{Conclusions}
In this work, we have presented a newly developed algorithm to efficiently simulate quantum jump trajectories: the Gillespie algorithm, named after the classical algorithm that inspired it. It allows leaping directly from one quantum jump to the next, without having to go through many steps during which no jumps take place. The algorithm is therefore particularly suitable for those cases in which there is a significant difference in timescales between very fast Hamiltonian dynamics and much slower jump dynamics. We presented examples of applications of the Gillespie algorithm to different systems of interest, highlighting its strong points with respect to more traditional simulation techniques, based on time discretisation.

\section*{Acknowledgements}
The research conducted in this publication was funded by the Irish Research Council under grant numbers IRCLA/2022/3922 and GOIPG/2022/2321. The authors thank Michael Kewming for helpful comments on QuTiP.

\begin{algorithm*}
\caption{Gillespie evolution for mixed states and/or partial monitoring}\label{alg:Gillespie_core}
\begin{algorithmic}
    \Require Hamiltonian $H$, \\
            list of monitored jumps Kraus operators [$L$], \\
            list of unmonitored jumps Kraus operators [$S$] \\
            initial state density matrix $\rho_0$, \\
            final time $t_f$, \\
            time step $dt$, \\
            number\_trajectories \\
    \item[]        
    \algorithmicprecompute{} $J$ = $\sum_k L_k^\dagger L_k$, $\tilde{J} = \vectorize(J)$
    \Comment{Vectorized quantities: $\tilde{\bullet}$ superscript} \\
    $\tilde{\mathcal{L}}_0 = -i \mathbb{I}\otimes H + i H^T \otimes \mathbb{I}$ \\
    $\qquad + \sum_l \left(S^*_l \otimes S_l - \frac{1}{2} \mathbb{I}\otimes S_l^\dagger S_l - \frac{1}{2} (S^\dagger_l S_l)^T \otimes \mathbb{I})\right)$ \\
    $\qquad + \sum_k \left(-\frac{1}{2} \mathbb{I}\otimes L_k^\dagger L_k - \frac{1}{2} (L_k^\dagger L_k)^T \otimes \mathbb{I}\right)$ \\

    $\tilde{\mathcal{L}}_0^\dagger = i \mathbb{I}\otimes H - i H^T \otimes \mathbb{I}$ \\
    $\qquad + \sum_l \left(S_l^T\otimes S_l^\dagger - \frac{1}{2} (S^\dagger_l S_l)^T \otimes \mathbb{I} - \frac{1}{2} \mathbb{I} \otimes (S^\dagger_l S_l)\right)$ \\
    $\qquad + \sum_k \left(-\frac{1}{2} (L_k^\dagger L_k)^T \otimes \mathbb{I} - \frac{1}{2} \mathbb{I}\otimes L_k^\dagger L_k\right)$ \\

    Let $[V]$ be the list of the no-jumps evolution superoperators \\
    Let $Q_s[t]$ be the list of the non-state-dependent parts of the WTD \\
    \For{all times $t<t_f$}
        \State $V[t] = \exp\left(\tilde{\mathcal{L}}_0 t\right)$ \\
        \State{$Q_s[t] = \unvectorize\left(\exp\left(\tilde{\mathcal{L}}_0^\dagger t\right)\tilde{J}\right)$}
        \Comment{$\unvectorize$ function: inverse of the vectorisation}
    \EndFor

    \For{all trajectories}
        \State{Fix initial state $\rho = \rho_0$, $\tau=0$}
        \While{$\tau < t_f$}
            \State{Let $[W]$ be the list to contain the WTD.}
            \For{every value $Q$ in $[Q_s]$}
                \State{$W = \tr\left[Q \rho\right]$}
            \EndFor
            \State{Sample a time $T$ from $W$}
            \State{$\tau += T$}
            \State{$\rho = \unvectorize\left( V[T] \vectorize(\rho)\right)$}
            \State{Let $[p]$ be the list for weights of the different (monitored) jump channels}
            \For{$L$ in $[L]$}
                \State{$p[L] = \tr\left[L^\dagger L\rho\right]$}
            \EndFor
            \State{Sample a jump channel $k$ from the weights distribution $[p]$}
            \State{$\rho = \frac{L_k \rho L_k^\dagger}{\tr\left[L_k \rho L_k^\dagger\right]}$}
            \Comment{State update after jump}
        \EndWhile
    \EndFor 
\Ensure [$(t_j, k_j, \rho_j$] vectors of results and states after jumps for all trajectories, [$V$], [t\_range] list of times at which $V$ is known.
\end{algorithmic}
\end{algorithm*}

\begin{algorithm*}
\caption{Finding the state at fixed times along a trajectory}\label{alg:Gillespie_filling}
\begin{algorithmic}
    \Require $[t^*]$ list of times at which $V$ is known \\
    $[t]$ list of times at which the state has to be computed \\
    $[V]$ list of evolution superoperators at times specified in $[t^*]$ \\
    $[(t_j, k_j, \rho_j)]$ vector of results and states for a single trajectory \\

    \State{Set to simplify the notation a final jump at $+\infty$ time.}
    \For{each jump $(t_j, k_j, \rho_j)$ along the trajectory}
        \State{Determine the set $[\bar{t}]$ of times such that $t_j\leq \bar{t} < t_{j+1}$}
        \For{each $\bar{t}$ in $[\bar{t}]$}
            \State{Find the closest time $t_{cl}$ in $[t^*]$ to $\bar{t}-t_j$}
            \State{$\rho = \frac{\unvectorize\left(V[t_{cl}] \vectorize(\rho)\right)}{\tr\left[\unvectorize\left(V[t_{cl}] \vectorize(\rho)\right)\right]}$}
        \EndFor
    \EndFor
    \Ensure the vector of $\rho$ states at all times in $[t]$
\end{algorithmic}
\end{algorithm*}
\bibliography{bibliography}

\end{document}
