\section{Introduction}

The evolution of our universe can be uniquely determined by its initial conditions and the laws of physics governing its dynamics. To understand this cosmic history, %\st{the evolution of universe}, 
astrophysicists use a large number of surveys \cite{Amendola_2018, Eisenstein_2011, survey_spergel} and simulations \cite{Springel_2001, Bagla_2002, villaescusa}. 
%to extract meaningful information. 
These simulations %\st{try to predict the nonlinear structure} \drew{
compute the gravitational evolution of a system of N-body particles given %\st{the} \drew{
a set of nearly uniform and typically Gaussian initial conditions, representing the early universe. 
%\st{of these particles}. 
These forward simulations, however, are computationally expensive and require a large amount of time and computational resources.

In recent years, deep-learning techniques have been shown to be extremely helpful in accelerating the forward modeling process \cite{he2019, jamieson}. These deep-learning models learn the mapping between pairs of inputs and outputs from numerical N-body simulations and act as fast and accurate approximators for these simulators. These deep-learning surrogates speed up the forward modeling process by several orders of magnitude. Since neural networks are theoretically proven to be universal function approximators \cite{HORNIK1989359}, the forward modeling of N-body simulations fits perfectly into the regime of neural networks.

% \begin{figure}
%     \centering
%     \begin{tikzpicture}
%   % Set X oval
%   \draw (-2,0) ellipse (1 and 2);
%   \draw (-2,-2) node[below] {X};
%   % Set X elements
%   \draw[gray, fill=gray] (-2,0.9) circle (0.2) node (x1) {};
%   \draw[gray, fill=gray] (-2,0.3) circle (0.2) node (x2) {};
%   \draw[gray, fill=gray] (-2,-0.3) circle (0.2) node (x3) {};
%   \draw[gray, fill=gray] (-2,-0.9) circle (0.2) node (x4) {};
  
%   % Set Y oval
%   \draw (2,0) ellipse (1 and 2);
%   \draw (2,-2) node[below] {Y};
%   % Set Y elements
%   \draw[gray, fill=gray] (2,0.9) circle (0.2) node (y1) {};
%   \draw[gray, fill=gray] (2,0.3) circle (0.2) node (y2) {};
%   \draw[gray, fill=gray] (2,-0.3) circle (0.2) node (y3) {};
%   \draw[gray, fill=gray] (2,-0.9) circle (0.2) node (y4) {};
  
%   % Arrows
%   \draw[-latex] (x1) -- (y1);
%   \draw[-latex] (x2) -- (y1);
%   \draw[-latex] (x3) -- (y2);
%   \draw[-latex] (x4) -- (y3);
% \end{tikzpicture}
%     \caption{Example of a many-to-one function between two sets X and Y. For our purpose, X corresponds to the set of all possible initial conditions of an N-body simulation and Y corresponds to the set of final conditions.}
%     \label{manytoone}
% \end{figure}

\begin{figure*}[t!]
\begin{center}
\centerline{\includegraphics[width=0.8\textwidth]{images/N128/slices_lin.png}}
\caption{Qualitative comparison of the $x,y, \text{and } z$ displacements for a $128 \times 128$ slice of particles from a linear field sampled using the training parameters and the 
corresponding linear field predicted by our inverse model.}
\label{slices}
\end{center}
\end{figure*}

The problem of inferring the initial state of the universe, or the input to an N-body simulation that generates a specific redshift zero (current time) nonlinear displacement field, is an inverse problem and poses a completely different challenge. Inverse problems are hard as they require a search over a large space of input configurations (the potential initial conditions of the universe), and typically involve one-to-many mapping if learned as a reverse mapping. Standard neural networks are one-to-one mappings, and not expected to work well in these problems. Sampling approaches such as Hamiltonian Monte Carlo \cite{Radford} based on Bayesian priors are computationally very expensive. One could resort to more complex generative neural networks such as generative adversarial networks (GANs) \cite{gan}, normalizing flows \cite{nf}, diffusion models \cite{pmlr-v37-sohl-dickstein15, df2}, 
etc. but they %have their own challenges as well \citep[cf.][]{mode_collapse, chen2023learning, salimans2022progressive}.  
 often either fail to converge, are unstable,  or require excessive computational time \citep[cf.][]{mode_collapse, chen2023learning, salimans2022progressive}. 
%\drew{maybe we should be specific about what the challenges are for these alternative approaches: is it that they require more training data? more computational resources/time to train? Do they require very finely tuned hyperparameters? ``harder to train'' seems too vague}. 
%\sh{we also know that the diffusion models should work quite well... so }
%\sh{I rewrote the above sentence to make it a bit less strong}
%This is because multiple different initial states could lead to the same final state, i.e., it is a many-to-one function. Thus, the mapping from final states to the initial states is not a function, but rather a one-to-many relation. 
%\aarti{One cannot simply run the N-body simulations to go back in time to the initial states. - True, but is the reason many-to-one mapping or that the equations don't work backwards in time? If latter, maybe just say "Moreover, one cannot simply ..."} \drew{The equations are time reversible, but we cannot solve them that way on a computer. Additionally, our discretization scheme (representing chunks of the matter field by discrete particles) leads to problems of shell-crossing. The actual continuous limit of the matter field that we are modeling with point-like particles will stream through itself (multi-streaming), so that the association of a region of the matter field with one or another N-body particle becomes ambiguous, or ill-defined. Then if we run a sequence of simulations in increasing resolution, we will find that some high-resolution particles end up far away from the lower resolution particles we would naively assign them to based on starting location.} 
%Therefore, there is no direct way of determining the initial conditions, and, we must resort to progressive sampling based methods \aarti{mention some refs? BORG?} to infer the possible initial states. 

 Importantly, simulations in scientific fields such as cosmology are often deterministic, and therefore they are reversible in principle. The one-to-many backward problem arises primarily due to numerical and computational errors which get exacerbated only at small scales which are dominated by nonlinear effects. This can cause the backward trajectories to be highly divergent. This motivates the approach we demonstrate here: training a standard deterministic neural network to learn the reverse map and output the initial states of cosmological N-body simulations using the final state of displacements as input.
We show that despite the one-to-many nature of the reverse mapping at small scales, a simple neural network can do an excellent job of predicting the initial states not only at large scales but even down to relatively small scales ($k>0.1\ \mathrm{Mpc}^{-1}\,h$) where the nonlinear dynamics of gravitational clustering become important. Our model continues to have $<1-2\%$ error down to $k=0.8$--$0.9\ \mathrm{Mpc}^{-1}\,h$.
The inverse model we train is only slightly less accurate than the forward model based on the same architecture \cite{jamieson}. Our results empirically motivate the use of neural networks as approximate inverse-mapping black boxes that could generate reliable initial states for a given output state, which could then be used to speed up the more fine-grained sampling-based inverse modeling methods.
