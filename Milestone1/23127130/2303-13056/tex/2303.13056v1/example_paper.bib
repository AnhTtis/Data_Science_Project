 @article{he2019, title={Learning to predict the cosmological structure formation}, volume={116}, DOI={10.1073/pnas.1821458116}, number={28}, journal={Proceedings of the National Academy of Sciences}, author={He, Siyu and Li, Yin and Feng, Yu and Ho, Shirley and Ravanbakhsh, Siamak and Chen, Wei and Póczos, Barnabás}, year={2019}, pages={13825–13832}} 


 @misc{jamieson,
  doi = {10.48550/ARXIV.2206.04594},
  url = {https://arxiv.org/abs/2206.04594},
  author = {Jamieson, Drew and Li, Yin and de Oliveira, Renan Alves and Villaescusa-Navarro, Francisco and Ho, Shirley and Spergel, David N.},
  keywords = {Cosmology and Nongalactic Astrophysics (astro-ph.CO), Machine Learning (cs.LG), FOS: Physical sciences, FOS: Physical sciences, FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Field Level Neural Network Emulator for Cosmological N-body Simulations},
  publisher = {arXiv},
  year = {2022},
  copyright = {Creative Commons Attribution 4.0 International}
}


 @article{villaescusa, 
 title={The Quijote simulations}, 
 volume={250}, 
 DOI={10.3847/1538-4365/ab9d82}, 
 number={1}, 
 journal={The Astrophysical Journal Supplement Series}, author={Villaescusa-Navarro, Francisco and others}, 
 year={2020}, 
 pages={2}
 } 

 @article{Springel_2001,
	doi = {10.1016/s1384-1076(01)00042-2},
	url = {https://doi.org/10.1016%2Fs1384-1076%2801%2900042-2},
	year = 2001,
	month = {apr},
	publisher = {Elsevier {BV}
},
	volume = {6},
	number = {2},
	pages = {79--117},
	author = {Volker Springel and Naoki Yoshida and Simon D.M. White},
	title = {{GADGET}: a code for collisionless and gasdynamical cosmological simulations},
	journal = {New Astronomy}
}
}


@article{Bagla_2002,
	doi = {10.1007/bf02702282},
	url = {https://doi.org/10.1007%2Fbf02702282},
	year = 2002,
	month = {dec},
	publisher = {Springer Science and Business Media {LLC}
},
	volume = {23},
	number = {3-4},
	pages = {185--196},
	author = {J. S. Bagla},
	title = {{TreePM}: A code for cosmological N-body simulations},
	journal = {Journal of Astrophysics and Astronomy}
}
}

@article{Amendola_2018,
	doi = {10.1007/s41114-017-0010-3},
	url = {https://doi.org/10.1007%2Fs41114-017-0010-3},
	year = 2018,
	month = {apr},
	publisher = {Springer Science and Business Media {LLC}
},
	volume = {21},
	number = {1},
	author = {Luca Amendola and others},
	title = {Cosmology and fundamental physics with the Euclid satellite},
	journal = {Living Reviews in Relativity}
}}


@misc{survey_spergel,
  doi = {10.48550/ARXIV.1503.03757},
  url = {https://arxiv.org/abs/1503.03757},
  author = {Spergel, D. and others},
  keywords = {Instrumentation and Methods for Astrophysics (astro-ph.IM), FOS: Physical sciences, FOS: Physical sciences},
  title = {Wide-Field InfrarRed Survey Telescope-Astrophysics Focused Telescope Assets WFIRST-AFTA 2015 Report},
  publisher = {arXiv},
  year = {2015},
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@article{Eisenstein_2011,
	doi = {10.1088/0004-6256/142/3/72},
	url = {https://doi.org/10.1088%2F0004-6256%2F142%2F3%2F72},
	year = 2011,
	month = {aug},
	publisher = {American Astronomical Society},
	volume = {142},
	number = {3},
	pages = {72},
	author = {Daniel J. Eisenstein and others},
	title = {{SDSS}-{III}: {MASSIVE} {SPECTROSCOPIC} {SURVEYS} {OF} {THE} {DISTANT} {UNIVERSE},
   {THE} {MILKY} {WAY},
   {AND} {EXTRA}-{SOLAR} {PLANETARY} {SYSTEMS}},
	journal = {The Astronomical Journal}
}
}

@article{HORNIK1989359,
title = {Multilayer feedforward networks are universal approximators},
journal = {Neural Networks},
volume = {2},
number = {5},
pages = {359-366},
year = {1989},
issn = {0893-6080},
doi = {https://doi.org/10.1016/0893-6080(89)90020-8},
url = {https://www.sciencedirect.com/science/article/pii/0893608089900208},
author = {Kurt Hornik and Maxwell Stinchcombe and Halbert White},
keywords = {Feedforward networks, Universal approximation, Mapping networks, Network representation capability, Stone-Weierstrass Theorem, Squashing functions, Sigma-Pi networks, Back-propagation networks},
abstract = {This paper rigorously establishes that standard multilayer feedforward networks with as few as one hidden layer using arbitrary squashing functions are capable of approximating any Borel measurable function from one finite dimensional space to another to any desired degree of accuracy, provided sufficiently many hidden units are available. In this sense, multilayer feedforward networks are a class of universal approximators.}
}

@article{Radford,
author = {Neal Radford},
year = {2012},
month = {06},
pages = {},
title = {MCMC using Hamiltonian dynamics},
journal = {Handbook of Markov Chain Monte Carlo},
doi = {10.1201/b10905-6}
}

@misc{gan,
  doi = {10.48550/ARXIV.1406.2661},
  url = {https://arxiv.org/abs/1406.2661},
  author = {Goodfellow, Ian J. and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
  keywords = {Machine Learning (stat.ML), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Generative Adversarial Networks},
  publisher = {arXiv},
  year = {2014},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{nf,
  doi = {10.48550/ARXIV.1505.05770},
  url = {https://arxiv.org/abs/1505.05770},
  author = {Rezende, Danilo Jimenez and Mohamed, Shakir},
  keywords = {Machine Learning (stat.ML), Artificial Intelligence (cs.AI), Machine Learning (cs.LG), Computation (stat.CO), Methodology (stat.ME), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Variational Inference with Normalizing Flows},
  publisher = {arXiv},
  year = {2015},
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@InProceedings{pmlr-v37-sohl-dickstein15,
  title = 	 {Deep Unsupervised Learning using Nonequilibrium Thermodynamics},
  author = 	 {Sohl-Dickstein, Jascha and Weiss, Eric and Maheswaranathan, Niru and Ganguli, Surya},
  booktitle = 	 {Proceedings of the 32nd International Conference on Machine Learning},
  pages = 	 {2256--2265},
  year = 	 {2015},
  editor = 	 {Bach, Francis and Blei, David},
  volume = 	 {37},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Lille, France},
  month = 	 {07--09 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v37/sohl-dickstein15.pdf},
  url = 	 {https://proceedings.mlr.press/v37/sohl-dickstein15.html},
  abstract = 	 {A central problem in machine learning involves modeling complex data-sets using highly flexible families of probability distributions in which learning, sampling, inference, and evaluation are still analytically or computationally tractable. Here, we develop an approach that simultaneously achieves both flexibility and tractability. The essential idea, inspired by non-equilibrium statistical physics, is to systematically and slowly destroy structure in a data distribution through an iterative forward diffusion process. We then learn a reverse diffusion process that restores structure in data, yielding a highly flexible and tractable generative model of the data. This approach allows us to rapidly learn, sample from, and evaluate probabilities in deep generative models with thousands of layers or time steps, as well as to compute conditional and posterior probabilities under the learned model. We additionally release an open source reference implementation of the algorithm.}
}

@misc{df2,
  doi = {10.48550/ARXIV.2006.11239},
  url = {https://arxiv.org/abs/2006.11239},
  author = {Ho, Jonathan and Jain, Ajay and Abbeel, Pieter},
  keywords = {Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Denoising Diffusion Probabilistic Models},
  publisher = {arXiv},
  year = {2020},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{mode_collapse,
  doi = {10.48550/ARXIV.1612.02136},
  url = {https://arxiv.org/abs/1612.02136},
  author = {Che, Tong and Li, Yanran and Jacob, Athul Paul and Bengio, Yoshua and Li, Wenjie},
  keywords = {Machine Learning (cs.LG), Artificial Intelligence (cs.AI), Computer Vision and Pattern Recognition (cs.CV), Neural and Evolutionary Computing (cs.NE), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Mode Regularized Generative Adversarial Networks},
  publisher = {arXiv},
  year = {2016},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@inproceedings{chen2023learning,
title={Learning Continuous Normalizing Flows For Faster Convergence To Target Distribution via Ascent Regularizations},
author={Shuangshuang Chen and Sihao Ding and Yiannis Karayiannidis and M{\r{a}}rten Bj{\"o}rkman},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=6iEoTr-jeB7}
}


@inproceedings{salimans2022progressive,
title={Progressive Distillation for Fast Sampling of Diffusion Models},
author={Tim Salimans and Jonathan Ho},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=TIdIXIpzhoI}
}