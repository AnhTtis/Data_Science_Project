    \subsection{Synthetic distribution} \label{sec:synth_distribution}

    \begin{figure}[!htb]
    \centering
    \includegraphics{./figures/example_structures.pdf}
    \caption{a) Some randomly picked examples of ICSD crystals. b) Some randomly
    picked examples of synthetically generated crystals. While coordination and
    distances are not chemically correct for the synthetic crystals, crystal
    symmetries are reproduced correctly.}
    \label{fig:example_structures}
    \end{figure}

    \begin{figure}[!htb]
    \centering
    \includegraphics{./figures/histograms/histograms.pdf} 
    \caption{Histograms comparing the distributions of descriptors of the
    synthetically generated crystals with the ICSD distribution in the test
    dataset. a) density factor $ \frac{V_\text{unit cell}}{\sum_i 4/3 \pi
    \left(\frac{r_{i\text{;cov}} + r_{i\text{;VdW}}}{2}\right)^3} =
    \frac{V_\text{unit cell}}{V_\text{atomic}}$, b) crystallite sizes, c) unit cell
    volume (conventional cell settings), d) number of atoms in the asymmetric
    unit. The probability density of the ICSD is visualized by a stacked bar
    histogram, where the green portion of the bar was correctly classified and
    the red portion was incorrectly classified. The probability density of the
    synthetic crystals is visualized by the dark blue line. The portion between the
    dark blue line and the light blue line was correctly classified, the portion below the
    light blue line was incorrectly classified. The reported classification
    performance is based on the ResNet-101 model trained on diffractograms from
    synthetic crystals.}
    \label{fig:histograms}
    \end{figure}

    We first present an analysis of the generated synthetic crystals.
    Figure~\ref{fig:example_structures} shows some randomly selected ICSD and
    synthetic crystal structures side-by-side. Visually, the crystals appear very
    similar. However, no physical or chemical considerations regarding stability, clashing atoms, and element combinations are taken into account in the generation of synthetic crystals. As discussed earlier, our goal is to demonstrate that this is
    not problematic when using these crystals for the extraction of structural
    information from powder diffractograms. On the contrary, we expect the synthetic crystals to be a better basis for generalization to fundamentally new crystal structures than existing finite databases.

    To compare the distribution of ICSD crystals with the synthetic
    distribution, we evaluated structural descriptors, i.e. density factors, crystallite sizes, unit cell volumes, and numbers of atoms in the asymmetric unit, and compare their histograms
    in Figure~\ref{fig:histograms}.
    One can see that the overall distributions of
    the synthetic and ICSD crystals are very similar for all four descriptors.
    This shows that our chosen generation algorithm reproduces crystals that are
    similar to ICSD crystals in terms of these more general descriptors.

    \subsection{Classification results} \label{sec:classification_results} 
    
    The main results of our experiments (see Section~\ref{sec:models_and_experiments}) to
    classify the space group of powder diffractograms can be
    found in Table~\ref{tab:results}. The goal of our experiments is to systematically analyse and quantify the changes in classification accuracy introduced by our two main contributions: A more challenging dataset split, and training on continuously generated synthetic data.

    We started by repeating previously reported experiments trained directly using ICSD crystals
    with a random train-test split instead of the split based on structure types
    (similar to the original approach by
    \citeauthor{parkClassificationCrystalStructure2017}
    \supercite{parkClassificationCrystalStructure2017}). This model achieved a
    very high test accuracy of 83.2\%.
    We note that
    \citeauthor{parkClassificationCrystalStructure2017} removed data from the training
    dataset, ``[...] heavily
    duplicated data [...]''\supercite{parkClassificationCrystalStructure2017}, but did not specify the exact criterions used. In
    contrast, we did not exclude any duplicates in this experiment based on a
    random train-test split.
    Furthermore, as discussed in Section~\ref{sec:generate_crystals}, we excluded crystals with a very high unit cell volume and a very high number of atoms in the asymmetric unit.
    This is
    likely the reason for the slightly higher classification accuracy that we
    observed, compared to the originally reported 81.1\%. 
    
    When splitting randomly, the model merely needs to recognize
    structures or structure types and assign the correct space group. This task
    is much easier than actually extracting the space group using more general
    patterns.
    When going from random splits to structure type-based splits (see Section~\ref{sec:dataset_split}), it becomes obvious that both the ``parkCNN big'' as well as the ``parkCNN medium'' models overfit the training data and do not generalize well to unseen structure types in the test set.

    Training the same model, in particular the ``parkCNN big'' model, on synthetic crystals leads to a 1.6 percentage points higher test
    accuracy than the ``parkCNN big'' model trained on ICSD diffractograms. At the same time, the training accuracy drops from 87.2\% to 74.2\% indicating that the model is now limited more by missing capacity rather than by overfitting, which is why we explored larger models, which will be discussed later.
    The gap between
    training and test accuracy is $31.1$ percentage points when training on ICSD data, while for training
    using synthetic crystals, the gap is only $16.5$
    percentage points. We note that this gap between training using
    synthetic crystals and testing using ICSD crystals cannot stem from
    overfitting, since no diffractograms are repeated for the synthetic
    training. The difference rather stems from the differences between
    the synthetic distribution and the ICSD distribution of crystals.

    \begin{table}[!htb]
        \begin{center}
        \caption{Results of training on diffractograms simulated from ICSD crystals (random splits as well as structure type-based splits) compared to when training on diffractograms from synthetic
        crystals. Test accuracy
        in all cases refers to the accuracy when testing on the ICSD test
        dataset. The training accuracies are averaged over the last 10 epochs of
        the respective run.
        Experiments trained directly on ICSD data
        overfitted to the training data. Training longer would have further
        increased the training accuracy, while not increasing the test
        accuracy.}
        \vspace*{2mm}
        
        \begin{tabular}{ccccccc} 
            \toprule
            Split & \begin{tabular}{@{}c@{}}Training \\ dataset\end{tabular} & \begin{tabular}{@{}c@{}}Testing \\ dataset\end{tabular} & Model & \begin{tabular}{@{}c@{}}Number of \\ parameters\end{tabular}& \begin{tabular}{@{}c@{}}Training acc. \\ / \%\end{tabular} & \begin{tabular}{@{}c@{}}Test acc. \\ / \%\end{tabular} \\
            \midrule
            Random & ICSD & ICSD & parkCNN medium & 4\,246\,797 & 88.4 & 83.2 \\
            \midrule
            Structure& \multirow{2}{*}{ICSD} & \multirow{2}{*}{ICSD}& parkCNN big & 4\,959\,585 & 87.2 & 56.1 \\
            type&&& parkCNN medium & 4\,246\,797 & 90.9 & 55.9 \\
            \midrule
            & \multirow{4}{*}{synthetic} & \multirow{4}{*}{ICSD} &parkCNN big & 4\,959\,585 & 74.2 & 57.7 \\
            Structure&&& ResNet-10 & 9\,395\,025 & 87.2 & 73.4 \\
            type$^{1}$ &&& ResNet-50 & 41\,362\,385 & 91.8 & 79.3 \\
            &&& ResNet-101 & 60\,354\,513 & 92.2 & \textbf{79.9} \\
            \bottomrule
        \end{tabular}\label{tab:results}
        \end{center} 
        \footnotesize{$^{1}$Here, the split type refers to the statistics and the test dataset, rather than the training and the test dataset.}
    \end{table}

    While the ``parkCNN big'' model trained on synthetic crystals outperforms
    the approach of training directly on ICSD crystals by only 1.6 percentage
    points, the advantage of training on an infinite stream of synthetic data increases when using models with more parameters and thus higher capacity. In
    contrast to training directly on a finite set of ICSD crystals, it is possible to train
    very large models using the infinite synthetic data stream without the
    potential of overfitting. As found in the last lines of Table~\ref{tab:results}, ResNet-10, ResNet-50, and ResNet-101 based models achieve ICSD test accuracies of 73.4\%,
    79.3\%, and 79.9\%. This is a significant increase from the 57.7\% achieved
    by the ``parkCNN big'' model. Figure~S4 in the SI further shows the top-$k$ accuracy over $k$ for the ResNet-101 model. With increasing $k$ the accuracy exceeds 95\% at $k=5$.
    This means that our model can not only determine the correct space group with a high probability but can also generate an almost complete list of possible space group candidates.
    
    Figure~\ref{fig:training_curve} shows the ICSD test accuracy, the training
    accuracy (on synthetic data), and the ICSD top-5 test accuracy for all three ResNet variants
    as a function of epochs trained. For all three metrics, the difference
    between ResNet-50 and ResNet-101 is comparably small, while the step from
    ResNet-10 to ResNet-50 is substantial (5.9 percentage points in ICSD test
    accuracy, see Table~\ref{tab:results}). This shows that going beyond the
    model size of the ResNet-101 will likely not yield a big improvement in
    accuracy. In contrast to the 79.9\% accuracy reached in the top-1 ICSD test accuracy, the top-5 ICSD test accuracy of the ResNet-101 model reaches
    96\%. However, for all three ResNet variants, a gap between training using synthetic
    crystals and testing using the ICSD remains (12.3 percentage points for
    ResNet-101). As also shown in Figure~S5 in the SI, the accuracy convergence can be approximately described by a power law, indicating that exponentially more training time will substantially reduce classification errors and thus potentially lead to top-1 accuracies of 90\% and above, at the cost of a 100-fold increase in training times, which is currently infeasible.

    \begin{figure}[!htbp] 
    \centering
    \includegraphics{figures/training_curves/training_curve_main.pdf}
    \caption{Test accuracy (ICSD), training accuracy (synthetic crystals), and
    test top-5 accuracy (ICSD) as a function of epochs. We show all three metrics for the
    models ResNet-101, ResNet-50, and ResNet-10. To better show the scaling
    behavior, both axes use logarithmic scaling. To better see the exponential behaviour, see Figure~S5 in the SI.
    }
    \label{fig:training_curve} 
    \end{figure}

    The histograms in Figure~\ref{fig:histograms} show, next to the overall
    distribution, also the fraction of diffractograms classified wrongly for
    testing on the ICSD (red bar) and on the synthetic data (below the light blue line)
    for the ResNet-101 model. First, one can see that throughout almost all
    regions of the distributions, the accuracy on the synthetic data is slightly
    higher than that on the ICSD. This is related to the aforementioned gap of 12.3
    percentage points between train and test accuracy and can be attributed to
    differences between the synthetic and ICSD distribution of crystals. This
    will be discussed in detail in the next section. It is surprising to see that the
    dependence on crystallite sizes is rather weak, as smaller crystallite sizes
    result in broader peaks (see Scherrer equation, Eq.~\ref{eq:scherrer}), potentially making the
    classification harder due to more peak overlaps.

    In summary, the maximum ICSD test accuracy of 79.9\% that we achieved using the
    ResNet-101 model almost reaches the 81.14\% that
    \citeauthor{parkClassificationCrystalStructure2017}
    \supercite{parkClassificationCrystalStructure2017} reported for the space
    group classification. However, our accuracy is based on a train-test split
    based on structure types, in contrast to the random split used by
    \citeauthor{parkClassificationCrystalStructure2017}. This creates a much
    harder but also realistic task to solve since the model needs to generalize to other structure
    types without merely recognizing diffractograms or structure types that it
    has already seen during training. This becomes especially apparent from our
    experiment directly trained on diffractograms from ICSD crystals with the
    split based on structure types, which reached only 56.1\% instead of the
    81.14\% reported by
    \citeauthor{parkClassificationCrystalStructure2017}\supercite{parkClassificationCrystalStructure2017}.

    \subsubsection*{Experimental results}
    To go beyond simulated diffractograms, we trained ResNet-50 models on calculated diffractograms with background, noise, and impurities and applied the trained models to the RRUFF mineral database. Our results (see Figure~S3 in the SI) show that it is essential to include impurity phases in the training data. By doing so, we obtain a top-1 accuracy of 25.2\% and a top-10 accuracy of over 60\%. 
    This is of high practical relevance since having a short list of potential space groups is often sufficient as a first step to further refinement and analysis.

    \citeauthor{vecseiNeuralNetworkBased2019} performed similar experiments of space group classification on the same database.
    Using an ensemble of 10 fully connected neural networks, they reached a classification accuracy of 54\%\supercite{vecseiNeuralNetworkBased2019}.
    While our obtained accuracy is significantly lower than the one reported by \citeauthor{vecseiNeuralNetworkBased2019}, our approach is much more general:
    The training dataset used by \citeauthor{vecseiNeuralNetworkBased2019} was based on simulated diffractograms of structures of the ICSD, which contains almost all RRUFF structures, leading to high similarities of training and testing data.
    Therefore, the model needed to simply recognize the minerals, instead of directly inferring the space group using the symmetry elements - as our method needs to do.

    We want to emphasize that our efforts to apply the methodology to experimental data are only preliminary. We expect improved results with an improved data generation protocol since the procedure contains many parameters to be tuned. Ideally, one would use a generative machine learning approach to add the experimental effects (noise, background, impurities) to the pure diffractograms. We also want to point out that the noise level and quality of data in the RRUFF dataset are limited. Application of the presented methodology on other experimental datasets is desirable.
    We will address these points in future work, where we focus on improved ways of modeling experimental imperfections. 
    
    As discussed above, for the classification of pure diffractograms we observed the ResNet-50 to have the best cost-benefit ratio, since the ResNet-101 yielded only slight improvements. For the more complicated problem of classifying diffractograms with experimental imperfections, bigger models and longer training times might be necessary.

    \subsection{Differences between synthetic crystals and ICSD crystals}

    We showed that training directly on crystals from the ICSD yields a gap
    between the training and test accuracy due to overfitting. The training on
    the synthetic dataset also shows a gap between the training and test
    accuracy (see Table~\ref{tab:results}), but it is smaller than when training
    directly on ICSD crystals. Furthermore, this gap is not due to overfitting,
    since overfitting to singular diffractograms is not possible when the model
    is trained using an infinite stream of generated synthetic crystals. The gap
    rather stems from systematic differences between the synthetic and ICSD
    distribution of crystals.

    To analyze those differences, we created three modifications of the ICSD
    test dataset. In the first modification, the fractional coordinates of the atoms in
    the asymmetric unit of the crystals of the ICSD test dataset were randomly
    uniformly resampled (as in the synthetic crystal generation algorithm). In
    the second modification, the lattice parameters were randomized following the KDE
    used in the synthetic generation algorithm. The third modification combines both previous modifications, i.e. both the
    coordinates and the lattice parameters were resampled. These three modified 
    test datasets bring the ICSD test dataset closer to the distribution used 
    for training and let us quantify which factors contribute to the gap between
    training and testing.

    We evaluated the test accuracies on these datasets for the experiment using
    the ResNet-101 model trained using synthetic crystals. We found that
    randomizing the coordinates yields an increase in test accuracy of $5.96$
    percentage points. Randomizing the lattice parameters results in an increase
    of $1.31$ percentage points. Randomizing both the coordinates and the
    lattice parameters leads to an increase of $6.32$ percentage
    points, explaining approximately half of the gap of 12.3 percentage points between synthetic training and ICSD test accuracy.

    We also tested the ResNet-101 model on diffractograms simulated based on the
    statistics dataset from which the statistics for our generation algorithm
    have been extracted. This returns a $4.33$ percentage points higher accuracy
    than on the test dataset. This is due to the different structure types
    present in the test and statistics dataset yielding different occupation
    probabilities (potentially also 0) for Wyckoff positions and overall slightly different
    statistics for the synthetic generation algorithm.

    To summarize, the contributions to the gap stem from differences in the
    distribution of lattice parameters and coordinates with $6.32$ percentage
    points and from the differences between the statistics dataset and test
    dataset with $4.33$ percentage points. Added together (assuming the effects of geometry and Wyckoff position occupation probabilities are unrelated and thus additive), the contributions
    almost completely explain the gap of $12.3$ percentage points between
    training and test accuracy. The remaining part is likely due to our
    algorithm that places atoms on Wyckoff positions not reproducing the ICSD
    distribution exactly. However, the remaining difference is remarkably small.

    \begin{figure}[!htb]
    \centering
    \includegraphics{./figures/histograms/histograms_rel.pdf}
    \caption{Classification performance for each bin of a) the unit cell volume
    (conventional cell settings) and b) the number of atoms in the asymmetric
    unit. The classification performance when testing on the ICSD is shown using
    a stacked bar plot, where the red bar indicates the classification error and
    the green bar indicates the correctly classified portion of each bin. When
    testing on the synthetic distribution, the classification error is given by
    the light blue line, the rest (between dark blue and light blue line) was correctly
    classified. The reported classification performance is based on the
    ResNet-101 model trained on diffractograms from synthetic crystals. This
    visualization clearly shows the error rate within each bin, in contrast to
    Figure~\ref{fig:histograms}, which additionally includes the relative
    proportion of the crystals of the respective bin to the total amount of
    crystals.}
    \label{fig:histograms_rel}
    \end{figure}

    In Figure~\ref{fig:histograms_rel} we show the test classification error in
    each bin for the unit cell volume and the number of atoms in the asymmetric
    unit using the ResNet-101 model trained on diffractograms of synthetic
    crystals. The classification error is shown both for testing on
    diffractograms from synthetic crystals and on ICSD diffractograms. One can
    see that for small volumes and a small number of atoms in the asymmetric
    unit, the difference between classifying ICSD diffractograms and
    diffractograms from synthetic crystals is relatively small. As the volume
    and number of atoms in the asymmetric unit increase, the gap between the two
    errors increases, too. We already identified the uniformly sampled atom
    coordinates in the synthetic distribution as the main contributor to the gap
    in accuracy between the synthetic crystals and ICSD crystals. Therefore, it
    seems that the uniform sampling of atom coordinates works well for small 
    number of atoms in the asymmetric unit and small volumes, while the error 
    due to this sampling strategy increases slightly for higher volumes and higher
    number of atoms in the asymmetric unit.
    
    When looking at the distribution of crystals in the ICSD, the number of
    atoms in the asymmetric unit tends to be larger for lower-symmetry space
    groups (for example, in the triclinic crystal system) than for
    higher-symmetry space groups such as those from the cubic crystal system.
    Therefore, the increasing test error on diffractograms from ICSD crystals
    with a higher number of atoms in the asymmetric unit is especially relevant
    for these lower-symmetry space groups.
    It might be possible that a different scheme of generating atom positions in
    the unit cell (compared to the independent uniform sampling that we used)
    works better for a high number of atoms in the asymmetric unit.

    Overall, it is important to note that the distribution of ICSD crystals is (apart from a few Wyckoff position occupation probabilities which are exactly zero in the statistics dataset\footnote{Setting them to small non-zero values typically leads to the generation of rather large unit cells, as the general Wyckoff positions have high multiplicities.}) almost completely encompassed by the much larger distribution of snynthetic crystals that we used for training.
    However, due to finite training times and model capacity, a performance gap remains. This gap can be improved by using (substantially) more computing power or by narrowing the very general synthetic distribution, e.g., by using a different algorithm to generate atom positions.
    This indicates an inherent challenge in XRD classification but more generally in materials property prediction:
    Machine learning models are ultimately trained to be employed in real-world tasks, which are typically related to novel, i.e. yet unseen materials and structures.
    At the same time, the machine learning models are tested based on an IID assumption, i.e. the assumption that the distribution of training and testing data is the same.
    While not being a contradiction in the limit of infinite training data and model capacity, this becomes an (unsolvable) challenge in reality, when facing finite datasets and models.
    In our case, our model trained on a large distribution of synthetic crystal structures will likely generalize better to completely new crystal structures different from any crystal structure contained in the ICSD database.
    At the same time, it suffers from smaller ICSD test set errors, even though the ICSD distribution is contained in the synthetic data generation distribution.