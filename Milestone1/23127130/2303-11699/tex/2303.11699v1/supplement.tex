\documentclass[a4paper]{article}

\usepackage[utf8]{inputenc}

\usepackage[
backend=biber,
%style=numeric,
style=nature,
sorting=none,
doi=true,
maxbibnames=99,
date=iso,
seconds=true,
urldate=iso
%citestyle=numeric-comp
]{biblatex}

%% Language and font encodings
\usepackage[english]{babel}
\usepackage[T1]{fontenc}
\usepackage{csquotes} 

%% Sets page size and margins
\usepackage[a4paper,top=3cm,bottom=2cm,left=3cm,right=3cm,marginparwidth=1.75cm]{geometry}

%% Useful packages
\usepackage{amsmath}
\usepackage{graphicx}
%\usepackage[colorinlistoftodos]{todonotes}
\usepackage{hyperref}
\hypersetup{colorlinks=true, allcolors=blue}
\usepackage{authblk}
%\usepackage[comma,super,sort&compress]{natbib}
\usepackage{multirow}


\newcommand{\TheBibliography}
{
    \clearpage
    %\thispagestyle{plain}
    %\lhead[\thepage]{Bibliography}
}

\usepackage{nomencl}
\usepackage{booktabs}
\usepackage{tabularx}
\usepackage{textcomp}
\usepackage{amsfonts}
\usepackage{comment} %%temporary, for organization purposes
\usepackage{outlines} %%same
\usepackage{subfig}
\usepackage[version=4]{mhchem}
\usepackage{siunitx}
\usepackage{bm}
\usepackage[normalem]{ulem} %%temporary, for organization purposes
\date{}
\newcommand{\red}[1]{\textcolor{red}{#1}}
\DeclareSIUnit\angstrom{\text {Ã…}}

\setcounter{biburllcpenalty}{7000}
\setcounter{biburlucpenalty}{8000}

%\renewcommand{\figurename}{Supplementary Figure}
%\addto\captionsenglish{\renewcommand{\figurename}{Supplementary Figure}}
%\addto\captionsenglish{\renewcommand{\tablename}{Supplementary Table}}

\renewcommand\thesection{S\arabic{section}}
\renewcommand\thesubsection{\thesection.\arabic{subsection}}
\renewcommand{\thefigure}{S\arabic{figure}}
\renewcommand{\thetable}{S\arabic{table}}


\interfootnotelinepenalty=10000

\addbibresource{bib.bib}
 
\title{Supplementary Information\\to\\Neural networks trained on synthetically generated crystals can extract structural information from ICSD powder X-ray diffractograms}

\author[1,2]{Henrik Schopmans}
\author[1,2]{Patrick Reiser}
\author[1,2]{Pascal Friederich*}


\begin{document}

\maketitle

\section{Generating synthetic crystals} \label{sec:generating_crystals} 

Here we describe the algorithm to generate synthetic crystals in more detail.
To generate a single crystal, the following steps are executed:

\begin{enumerate}
    \itemsep0em
    \item Random selection of a space group. We follow the space group distribution of the ICSD to allow comparison with previous work (see Section~2.3 of the paper).
    \item The number of unique elements in the crystal is drawn from a discrete
    distribution extracted from the crystals in the ICSD belonging to the
    specified space group.
    \item The unique elements are drawn, also from a discrete probability
    distribution from the crystals in the ICSD belonging to the specified space
    group.
    \item For each of the unique elements, the number of repetitions in the
    asymmetric unit is chosen, and for each repetition, a Wyckoff position is
    randomly selected. Again, both the probability of the number of repetitions and the Wyckoff
    occupation probabilities are extracted from the ICSD for the specified space
    group. We do not place more than one atom on a Wyckoff position that does not
    have a degree of freedom.
    \item For each atom placed on a Wyckoff position, uniformly distributed random fractional
    coordinates are drawn.
    \item Lattice parameters (normalized to unity volume) of the crystal system
    that the specified space group belongs to are drawn from a kernel density
    estimate (KDE)
    of the ICSD. The bandwidth is chosen based on Scott's rule
    (see the \emph{SciPy} \supercite{virtanenSciPyFundamentalAlgorithms2020}
    implementation of KDE). 
    \item We generated a KDE of the volume conditioned\footnote{For this
        conditional kernel density estimate, we used the implementation of
        \emph{statsmodels} \supercite{seaboldStatsmodelsEconometricStatistical2010}
        with the normal reference rule of thumb to estimate the bandwidth.} on $
        \sum_i 4/3 \pi \left(\frac{r_{i\text{;cov}} +
        r_{i\text{;VdW}}}{2}\right)^3 = V_\text{atomic} $, where the sum covers
        all atoms in the conventional unit cell, $r_{i\text{;cov}}$ is the
        atomic covalent radius, and $r_{i\text{;VdW}}$ is the atomic van der
        Waals radius. The KDE was generated from all crystals of the ICSD
        belonging to the specified space group. Then, $ V_\text{atomic} $ is
        calculated for the chosen atoms in the conventional unit cell and the volume is
        drawn based on the KDE conditioned on $V_\text{atomic}$. The lattice parameters (chosen in
        the previous step) are further scaled by the cube root of the chosen volume.
    \item Space group symmetry operations are applied using \emph{Python}
    library \emph{PyXtal} \supercite{fredericksPyXtalPythonLibrary2021}.
\end{enumerate} 



When generating a crystal of a specific space group without placing an atom
on the general Wyckoff position, it is not always the case that the crystal
belongs to that space group. To prevent wrong space group assignments, we use the \emph{Pymatgen}
\supercite{ongPythonMaterialsGenomics2013} interface to \emph{Spglib}
\supercite{togoSpglibSoftwareLibrary2018} to check the space group of each
crystal after its generation. 
If the space group deviates, we generate a new
crystal with the same number of unique elements as before, in order to not 
distort the distribution of number of unique elements extracted from the ICSD.
If the generation
fails 20 times in total, we start from the beginning with a new number of
unique elements. 
    
\section{Machine learning models} \label{sec:ml_methods} 
    
    We now want to describe the machine learning models that we used for the
    classification of space groups in more detail. Powder diffractograms include
    similar features (peaks) at different locations and the position of a
    feature in the diffractogram has a spatial meaning. This suggests that the
    properties of the convolution operation, namely the parameter sharing (with
    sparse connectivity) and equivariance\supercite{goodfellowDeepLearning2016},
    might be beneficial when processing powder diffractograms.

    As a baseline, we first used the two CNN architectures used by
    \citeauthor{parkClassificationCrystalStructure2017}
    \supercite{parkClassificationCrystalStructure2017} for the classification of
    extinction groups and space groups. Since our training dataset is an
    infinite stream of diffractograms and we do not have to worry about
    overfitting, we further used the deeper architectures ResNet-10, ResNet-50, and
    ResNet-101\supercite{heDeepResidualLearning2016}. All architectures are now
    described in detail.

    \subsection{Architectures by \citeauthor{parkClassificationCrystalStructure2017}}

    \citeauthor{parkClassificationCrystalStructure2017}\supercite{parkClassificationCrystalStructure2017}
    introduced three models, one for the classification of the crystal system,
    one for extinction groups, and one for space groups. We used only the last
    two models and call them ``parkCNN medium'' and ``parkCNN big'',
    respectively.

    ``parkCNN big'' consists of three convolution layers with average pooling,
    two hidden fully connected layers with 2300 and 1150 nodes, and a
    145-dimensional softmax output. The architecture is summarized in
    Figure~\ref{fig:park_architecture}. The ``parkCNN medium'' model has fewer
    parameters than ``parkCNN big'' since the two hidden fully connected layers
    have 4040 and 202 nodes.

    \begin{figure}[!htb]
    \centering
    \includegraphics{figures/Park_models.pdf}
    \caption{The CNN architecture for space group classification (``parkCNN
    big'') as introduced by \citeauthor{parkClassificationCrystalStructure2017}
    \supercite{parkClassificationCrystalStructure2017}. Each convolution or
    fully connected layer is implicitly followed by a ReLU activation, the
    output uses a softmax activation. We used only 145 target space groups
    instead of 230, since the remaining space groups did not have enough entries
    present in the ICSD to extract enough statistics for the synthetic
    generation of crystals. Furthermore,
    \citeauthor{parkClassificationCrystalStructure2017} used an input length of
    10001 instead of our input length of 8501. A dropout rate of 30\% is used
    after the activations of the convolution blocks. Dropout with a rate of 50\%
    is used after the activations of the fully connected layers. However,
    dropout is only used if the model is directly trained on ICSD
    diffractograms, not when using the synthetic data.}
    \label{fig:park_architecture}
    \end{figure}

    \subsection{ResNet architecture}
    With increasing size of the training dataset and increasing difficulty
    of the chosen task, the number of model parameters needs to be
    increased, too.

    In principle, a deeper model with additional layers should always be able to
    express the same solution of a shallower model by simply ``learning'' an
    identity map in addition to the shallower model. In practice, however, a
    degradation problem for CNNs with increasing depth has been observed and
    very deep models can perform worse than their shallow counterpart
    \supercite{heDeepResidualLearning2016}. Therefore, the ResNet architecture
    developed by \citeauthor{heDeepResidualLearning2016}
    \supercite{heDeepResidualLearning2016} at Microsoft in 2015 introduced additional
    skip connections, where information is able to simply flow past the
    convolution layer and is added to its output. This makes it possible for
    needed information of the input or earlier layers to flow further into the
    model without degradation.

    \begin{figure}[!htb]
    \centering
    \includegraphics{figures/resnet_blocks.pdf}
    \caption{a) ResNet residual block, the main building block of the shallower
    variants of the ResNet architecture. b) ResNet bottleneck block, the main
    building block of the deeper variants of the ResNet architecture. All
    convolution operations are implicitly followed by a batch normalization
    layer. In both cases, a skip connection allows information to directly flow
    past the convolution operations. (Illustration based on
    \supercite{heDeepResidualLearning2016})} 
    \label{fig:resnet_bottleneck}
    \end{figure}

    Figure \ref{fig:resnet_bottleneck}a visualizes the residual block used for
    the shallower versions of the ResNet architecture (up to 34 layers). Figure
    \ref{fig:resnet_bottleneck}b visualizes the bottleneck block used for the
    deeper variants (50 and more layers). This type of building block is called
    a bottleneck block since it first reduces the number of channels using a 1x1
    convolution operation with N filters. Then, the main convolution with a
    3x3 kernel is performed, followed by a third 1x1 convolution that upscales
    to 4N channels. This down- and upscaling of the number of channels is
    performed to increase the performance of the model. All convolution
    operations of both types of building blocks are followed by batch
    normalization implicitly.
    
    In the simplest case, the skip connection of the residual and bottleneck
    block is simply an identity mapping and added to the output of the block.
    However, if the number of input channels and dimensions of a block are
    different from those in the output, a projection in the form of a 1x1
    convolution with the necessary number of filters and stride (usually 2) is
    used instead of the identity.

    \begin{table}[!htb]
        \centering
        \caption{Composition of the ResNet-10, ResNet-50, and ResNet-101
        architectures \supercite{heDeepResidualLearning2016}. The architectures
        are to be read from top to bottom. Square brackets indicate a residual
        or bottleneck building block with the respective number of filters for
        each convolution.}
        \vspace*{2mm}
        %\resizebox{\textwidth}{!}{%
        \begin{tabular}{ccccc}
            output size & layer name & ResNet-10 & ResNet-50 & ResNet-101 \\
            \toprule
            112x112 & initial conv. & \multicolumn{3}{c}{$ 7 \times 7 $ conv., 64 channels, stride 2} \\
            \midrule
            56x56 & initial pool & \multicolumn{3}{c}{$3 \times 3$ max pool, stride 2} \\
            \midrule
            56x56 & block group 1 & $ \left[\begin{array}{c} 3 \times 3,64 \\ 3 \times 3,64 \end{array}\right] $ $\times 1$ & $ \left[\begin{array}{c} 1 \times 1,64 \\ 3 \times 3,64 \\ 1 \times 1,256 \end{array}\right] $ $\times 3$ & $ \left[\begin{array}{c} 1 \times 1,64 \\ 3 \times 3,64 \\ 1 \times 1,256 \end{array}\right] $ $\times 3$ \\[5mm]
            \midrule
            28x28 & block group 2 & $ \left[\begin{array}{c} 3 \times 3,128 \\ 3 \times 3,128 \end{array}\right] $ $\times 1$ & $ \left[\begin{array}{c} 1 \times 1,128 \\ 3 \times 3,128 \\ 1 \times 1,512 \end{array}\right] $ $\times 4$ & $ \left[\begin{array}{c} 1 \times 1,128 \\ 3 \times 3,128 \\ 1 \times 1,512 \end{array}\right] $ $\times 4$ \\[5mm]
            \midrule
            14x14 & block group 3 & $ \left[\begin{array}{c} 3 \times 3,256 \\ 3 \times 3,256 \end{array}\right] $ $\times 1$ & $ \left[\begin{array}{c} 1 \times 1,256 \\ 3 \times 3,256 \\ 1 \times 1,1024 \end{array}\right] $ $\times 6$ & $ \left[\begin{array}{c} 1 \times 1,256 \\ 3 \times 3,256 \\ 1 \times 1,1024 \end{array}\right] $ $\times 23$\\[5mm]
            \midrule
            7x7 & block group 4 & $ \left[\begin{array}{c} 3 \times 3,512 \\ 3 \times 3,512 \end{array}\right] $ $\times 1$ & $ \left[\begin{array}{c} 1 \times 1,512 \\ 3 \times 3,512 \\ 1 \times 1,2048 \end{array}\right] $ $\times 3$ & $ \left[\begin{array}{c} 1 \times 1,512 \\ 3 \times 3,512 \\ 1 \times 1,2048 \end{array}\right] $ $\times 3$\\[5mm]
        \end{tabular}
        %}
        \label{tab:resnet_architecture}
    \end{table}

    Table \ref{tab:resnet_architecture} summarizes the ResNet-10, ResNet-50, and
    ResNet-101 models. Each architecture is to be read from top to bottom. The
    square brackets indicate a residual or bottleneck block with their two or
    three convolution operations and respective number of filters. The 
    number after the square brackets ``$\times N$'' indicates how often 
    the building block is to be repeated in the respective block group.

    If the output dimension changes from one block group to the next, the first
    building block of the next block group downsamples the dimensions by using a
    stride of 2 for the first 3x3 convolution in the case of a residual block
    and for the middle 3x3 convolution in the case of a bottleneck block. All
    other convolution operations of the building blocks are performed with
    stride 1. 

    We used the ResNet implementation as found in the \emph{TensorFlow} model
    garden \supercite{yuTensorFlowModelGarden}. Since our data is
    one-dimensional, we used an adapted 1D version. We replaced all 2D
    convolutions and pooling operations with their 1D equivalent ($N \times N
    \rightarrow N $). Furthermore, we used a kernel size of 9 in place of the
    3x3 kernels and stride 4 instead of stride 2 in the bottleneck blocks and
    projection skip connections ($N \times N \rightarrow N^2$). This squaring
    was performed to obtain a better distribution of the number of weights
    throughout the architecture (similar to the original 2D case). As discussed
    in our paper, we used group
    normalization\supercite{wuGroupNormalization2018} (with 32 groups) instead
    of batch normalization. We further added an additional fully connected layer
    with 256 nodes after the flatten layer in the end of the ResNet models,
    followed by the output layer.

    \section{Application to experimental diffractograms}
    \subsection{Dataset}
    To test the performance on experimental diffractograms, we used 
    the publicly available RRUFF mineral database \supercite{lafuentePowerDatabasesRRUFF2015}. It contains 5829 mineral samples with multiple types of
    measured spectra and, most important for us, powder XRD measurements with
    K$_\alpha$ radiation for 2952 samples. Of these 2952 samples, 2875 samples
    provide the output of a Rietveld refinement, including the space group label. We further
    removed by hand some samples that had excessive amounts of noise and
    were of bad quality. Many samples further only provide a simulated
    diffractogram without noise or background. They were also excluded. This left
    942 diffractograms for our analysis.

    \subsection{Data generation}
    To be able to apply our models to experimental diffractograms, we
    added an additional background function and noise to the generated
    diffractograms to make them more similar to experimental data. 
    We used a Gaussian process to generate
    random background functions and added additive and multiplicative Gaussian noise.
    All
    diffractograms were generated in the range $ 2 \theta \in \left[ 5, 90
    \right] \si{\degree} $ with step size \SI{0.01}{\degree}.

    The generation protocol is as follows:

    \begin{enumerate}
        \item Sample the background function from a Gaussian
        process with radial basis function kernel without any
        conditioning:
        \begin{equation}
            k(x_i, x_j) = a \exp\left(- \frac{|x_i - x_j|^2}{2l^2} \right)
        \end{equation}
        We chose $ a = 1 $ and sampled $ l $ uniformly in 
        $[7,40]$ for each diffractogram.
        \item Subtract the minimum intensity from the background function obtained from the Gaussian process.
        \item Divide it by the sum of the 8501 ($2\theta$-range) entries.
        \item Multiply it by a scaling factor drawn from a truncated normal
        distribution in the range $[0,150]$ with mean $0$ and standard deviation
        $75$.
        \item Add the pure diffractogram (intensity-range $[0,1.0]$).
        \item Add Gaussian additive noise with mean $0$ and standard deviation uniformly drawn in $[0,0.02]$.
        \item Multiply with multiplicative Gaussian noise with mean $1$ and standard deviation uniformly drawn in $[0,0.16]$.
    \end{enumerate}

    To resemble a more realistic peak profile, we used the pseudo-Voigt profile instead of the Gaussian profile
    that we used for the classification of pure diffractograms.
    The pseudo-Voigt profile uses the full width at half maximum (FWHM) $ \Gamma_\text{G} $ of
    the Gaussian $G$, the FWHM $\Gamma_\text{L}$ of the Lorentzian $L$, and a mixing parameter $\eta$ as parameters \supercite{gilmoreInternationalTablesCrystallography2019}:

    \begin{equation}
        \text{PV}\left(\theta-\theta_{0} ; \Gamma_\text{L}, \Gamma_\text{G} \right)=\eta G\left(\theta-\theta_{0} ; \Gamma_\text{G}\right)+(1-\eta) L\left(\theta-\theta_{0} ; \Gamma_\text{L}\right)
        \label{eq:pseudo_voigt}
    \end{equation}

    with

    \begin{equation}
        G(\theta-\theta_0;\sigma=\frac{\Gamma_\text{G}}{2\sqrt{2 \ln{2}}})=\frac{1}{\sigma \sqrt{2 \pi}} e^{-\frac{1}{2}\left(\frac{\theta-\theta_0}{\sigma}\right)^{2}}
    \end{equation}

    and

    \begin{equation}
        L(\theta-\theta_0;\gamma=\frac{1}{2} \Gamma_\text{L})=\frac{1}{\pi \gamma}\left[\frac{\gamma^{2}}{\left(\theta-\theta_{0}\right)^{2}+\gamma^{2}}\right] \, \text{.}
    \end{equation}
    
    The FWHM $\Gamma_\text{G}$ of the Gaussian is typically 
    parametrized using the Caglioti equation \supercite{gilmoreInternationalTablesCrystallography2019} as
    
    \begin{equation}
        \Gamma_\text{G}^{2}=U \tan ^{2} \theta+V \tan \theta+W \, \text{.}
        \label{eq:caglioti}
    \end{equation}
    
    Following the suggestions for typical Rietveld parameter ranges by
    \citeauthor{kadukTypicalValuesRietveld2011} \supercite{kadukTypicalValuesRietveld2011}
    and comparing the resulting peaks with the ones from the RRUFF dataset, we decided to sample
    the Caglioti parameters uniformly in the following ranges: $ [0,0.01] $ for
    U, $ [0,0.01] $ for W, and V was fixed at $V=0.0$. We further used a single
    mixing parameter $ \eta $ uniformly sampled in $ [0.0, 1.0] $ for the full
    $2\theta$-range. For simplicity, we used
    the same FWHM from the Caglioti equation for both the Gaussian and
    Lorentzian of the pseudo-Voigt. We further considered the K$_{\alpha_1}$ /
    K$_{\alpha_2}$ splitting of the K$_\alpha$ line since for some
    diffractograms in the RRUFF dataset, this splitting is visible.

    We further implemented the option to add impurity phases to the training
    (and simulated ICSD test) data. The minerals of the RRUFF database are not all made up
    of one phase, but most of them contain small amounts of one or more
    impurity phases. To model this, for each training diffractogram, we used a
    superposition of the main phase to be classified and an impurity phase of a
    random space group ($a$ is uniformly sampled in $[0,0.05]$):

    \begin{equation}
        I(\theta) = (1-a) I_\text{pure} + a I_\text{impurity}
    \end{equation}

    \subsection{Experiments}
    For our experiments on experimental data, we used the same
    split based on structure types as we used for pure diffractograms.
    We performed two experiments using the ResNet-50 architecture, one with impurity phases 
    and one without. For both, a learning rate of $0.0001$,
    a batch size of 145, and 1000 epochs where used.
    
    \begin{figure}[!htb] 
    \centering
    \includegraphics{./figures/rruff_acc.pdf}
    \caption{Top-$k$ accuracy as a function of $k$ tested on RRUFF dataset for ResNet-50 model trained with added noise and
    background. The experiment corresponding to the blue curve additionally contained one added impurity phase for
    each diffractogram in the training data.}
    \label{fig:rruff_top_k}
    \end{figure}

\newpage
\section{Additional figures}

\begin{figure}[!htb] 
\centering
\includegraphics{./figures/top_k.pdf}
\caption{Top-$k$ accuracy as a function of $k$ tested on ICSD test dataset and the synthetic training data for ResNet-101 model trained on synthetic data.}
\label{fig:resnet_101_top_k}
\end{figure}
 
\begin{figure}[!htb] 
\centering
\includegraphics{./figures/training_curve_main_1_minus_acc.pdf}
\caption{$1-\text{acc.}$ for test accuracy (ICSD), training accuracy (synthetic crystals), and
test top-5 accuracy (ICSD) as a function of epochs for ResNet-101 model trained on synthetic data. To better show the scaling
behavior, both axes use logarithmic scaling.}
\label{fig:resnet_101_1_minus_acc}
\end{figure}

\printnomenclature

\clearpage

%\nocite{*}
\TheBibliography
\printbibliography[heading=bibintoc]

\end{document} 