\section{Baseline Models}

Participants had access to the code base from our pilot study that is described in more details in \cite{EMNLP}. Most participants built upon this by implementing their own methods on the provided code base.

\subsection{Sub-task 1}
The starter kit for sub-task 1 can be found in the \textsc{NL4Opt} repository\footnote{Sub-task 1 baseline is available at: \href{https://github.com/nl4opt/nl4opt-subtask1-baseline}{\textcolor{blue}{https://github.com/nl4opt/nl4opt-subtask1-baseline}}}. The baseline model, \textsc{XLM-RoBERTa-base} (\textsc{XLM-R-base}) \citep{XLM-R}, was trained and fine-tuned by minimizing the log-likelihood loss. As part of the pilot study, we reported\footnote{Stratified performance: \href{https://github.com/nl4opt/nl4opt-subtask1-baseline/tree/main/baseline\#results}{\textcolor{blue}{https://github.com/nl4opt/nl4opt-subtask1-baseline/tree/main/baseline\#results}}} the baseline model's performance on the test set when evaluated on the source domain, target domain, and entire test set for all entity types (i.e., constraint direction, limit, etc.). Based on this preliminary analysis, the objective name was the most difficult to identify potentially due to its ambiguity. We expect the greatest improvements would arise from methods that are capable of accurately recognizing the objective names and their spans. \textbf{Evaluation:} \textit{This baseline achieved an F1 score of \textbf{0.906} on the test split.}

\subsection{Sub-task 2}
The starter kit for sub-task 2 can be found in the \textsc{NL4Opt} repository\footnote{\href{https://github.com/nl4opt/nl4opt-subtask2-baseline}{Sub-task 2 baseline is available at: \textcolor{blue}{ https://github.com/nl4opt/nl4opt-subtask2-baseline}}}. The starter kit for sub-task 2 contains code to parse the XML-like intermediate representations and annotated examples into our Problem Formulation dataclass and code to score the submission. Additional information regarding the canonical representation, parsing, and scoring can be found in this \href{https://github.com/nl4opt/nl4opt-subtask2-baseline/blob/main/notebooks/demo.ipynb}{\textcolor{blue}{notebook}}. For the generation sub-task, the baseline model is a \textsc{BART} encoder-decoder \citep{BART} that leverages a prompt-guided generation and a copy mechanism to generate a meaning representation of the optimization formulation. \textbf{Evaluation:} \textit{This baseline achieved an accuracy of \textbf{0.610} on the test set. }

