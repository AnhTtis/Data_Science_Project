\section{The \textsc{NL4Opt Competition}}

\begin{figure*}
\centering\includegraphics[width=0.6\columnwidth]{figures/subtask-1-revision1.png}
\caption{Description of dataset for sub-task 1: ``recognizing the problem entities"}
\label{fig:subtask-1}
\end{figure*}

The \textsc{NL4Opt Competition} explores the design of natural language interfaces for optimization solvers. The results of this competition forward the accessibility and usability of these solvers and allow non-experts to solve important problems from various industries. Specifically, we used this competition to explore methods of converting a natural language description of an optimization problem into a mathematical formulation. This goal was separated into two inter-related sub-tasks:
\begin{enumerate}
    \item Recognition of optimization problem entities,
    \item Generation of problem formulation.
\end{enumerate}

The first sub-task was to recognize optimization model entity types (i.e., constraint direction, constraint limit, objective direction, objective name, parameter, variable) from the problem description. In the first sub-task, the goal was to detect text spans from the problem description that represent semantic entities of the optimization problem and to tag them according to the listed entity types. This sub-task aimed to reduce the ambiguity by identifying important components of the optimization model. An illustration of sub-task 1 is provided in Figure \ref{fig:subtask-1}. 

The second sub-task was to generate a precise meaning representation of the optimization formulation. This sub-task was simplified using the ground truth information of the problem entities from the first sub-task. An illustration of sub-task 2 is provided in Figure \ref{fig:subtask-2}. \\

\begin{figure*}
\centering\includegraphics[width=0.8\columnwidth]{figures/subtask-2-revision2.png}
\caption{Dataset annotation and evaluation protocol for sub-task 2: "generating the problem formulation"}
\label{fig:subtask-2}
\end{figure*}


\noindent The proposed sub-tasks are characterized by the following challenges: 

\begin{enumerate}
\item \textbf{Unstructured multi-sentence input}. 
An optimization description is the input document that describes the decision variables, objective, and a set of constraints. In addition, the structure of the input varies depending on the structure of the optimization problem and the linguistic style. Thus, the multi-sentence input exhibits a high level of compositionality and ambiguity due to the variability of the linguistic patterns, of the problem domains, and of the problem structures. 
\item \textbf{Mismatched inputs and outputs}. The contextual information from the input description is abstracted away in the target formulation. Therefore, the absence of contextual clues in the output makes it difficult to align the input-output pair. Thus, the meaning representation of the problem formulation (i.e. the output of generation model) is important as it bridges the problem description and the mathematical formulation. In fact, semantically equivalent representations may have syntactically different forms and can lead to different performance \citep{guo-etal-2020-benchmarking-meaning}. 
\item \textbf{Low-resource learning constraint}. Specialized knowledge is required to create a dataset thereby drastically increasing the cost of dataset creation. The design of machine learning models for this task is challenging as they must learn from a small number of expert-annotated examples.
\item \textbf{Domain-agnostic parsing}. Finally, OR tools are applied to disparate problem domains (e.g. forestry, transportation or medicine) \citep{williams2013model}. As a result, the learning-based solution must generalize not only to new problem instances but also to new application domains. 
\end{enumerate}

\subsection{Evaluation}

\paragraph{NER sub-task:} we evaluated the models based on their achieved micro-averaged F1 score given by:
\vspace{-2pt}
\begin{equation}
\text{{F1}}=\frac{2\times P\times R}{P+R},
\end{equation}
where $P$ and $R$ are the average precision and average recall further averaged over all entity types, respectively.

\paragraph{Generation sub-task:} we used an application-specific metric since the task was motivated by the need to precisely formulate the optimization problem. We have benchmarked the models based on the declaration-level mapping accuracy given by:
\vspace{-2pt}
\begin{equation}\label{equ:accuracy}
% \text{{Acc}}=1-\frac{\sum_{i=1}^{N}\left(\text{FP}_{i}+\text{FN}_{i}\right)}{\sum_{i=1}^{N}D_{i}},
\text{Acc} = 1 - \frac{\sum_{i=1}^N \min\left\{\text{FP}_i + \text{FN}_i, D_i\right\}}{\sum_{i=1}^N D_i},
\end{equation}
where $N$ is the number of linear programming word problems (LPWPs) in the test set. For each LPWP $i$, $D_i$ is the number of ground-truth declarations. The false positive $\text{FP}_{i}$ is the number of non-matched predicted declarations whereas the false negative $\text{FN}_{i}$ denotes the number of ground-truth declarations without a match. To clarify the evaluation protocol, we have emphasized that the canonical representation, as described in Figure \ref{fig:subtask-2}, would be used to compare the ground-truth and predicted formulations.

\subsection{Competition statistics}
Over 150 teams registered for this competition combining for a total of more than 300 valid independent submissions. The demographics of registered participants were affiliated as follows: 30\% post-secondary, 30\% independent, 30\% unspecified, and 10\% industry.  There were 19 teams with valid submissions to sub-task 1 and 9 to sub-task 2. These teams reported the following affiliations: 60\% industry, 25\% post-secondary, and 15\% independent.

\subsection{Additional Competition Details}

All the details and relevant information of the competition are made accessible at the competition website (\href{https://nl4opt.github.io/}{\textcolor{blue}{https://nl4opt.github.io/}}). This website contains the rules, a FAQ/Tutorial section, access to the starter kit, and final results of the competition. We have released the test set and encourage all new and returning participants to leverage this competition as a benchmark for new methods. We especially welcome those that are interested in tackling the challenges listed above (i.e., unstructured input, misaligned input-output pair, low-resource learning constraint, generalizability).