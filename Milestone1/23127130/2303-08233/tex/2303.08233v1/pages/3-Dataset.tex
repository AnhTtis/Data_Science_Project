\section{The \textsc{NL4Opt} Dataset}

\paragraph{Dataset description.} A total of 1101 annotated LPWPs from 6 different domains were created for the \textsc{NL4Opt Competition}. We separated the dataset into 713, 99, and 289 samples for training, development, and testing, respectively. The data samples were distributed identically for both sub-tasks. It is important to evaluate submissions for generalizability towards unseen problem domains. Therefore, we include LPWPs from the three similar (source) domains of sales, advertising, and investment in both training, development and test splits. However, problems from three other (target) domains (production, transportation, and sciences) have been reserved for the development and test splits. To ensure that the development set was never used for training, we reviewed the final submitted code and re-trained all submissions prior to announcing the winning teams. Table \ref{tab:data-distribution} presents the number of samples and the ratio between the source and target domains for the three splits of data. An example of data and its annotations for the two sub-tasks is illustrated in Figures \ref{fig:subtask-1} and \ref{fig:subtask-2}. 

\begin{table}[ht]
\centering
\caption{\label{tab:data-distribution}Data Distribution.}
\begin{tabular}[t]{lcc}
\toprule
Split & \#/samples & source:target\\
\midrule
Train&713&1:0\\
Dev&99&1:3\\
Test&289&1:3\\
\bottomrule
\end{tabular}
\end{table}

For the first sub-task, the input is the problem description and the output is the set of entities that correspond to the components of the problem (Figure \ref{fig:subtask-1}). The entities are labelled according to predefined entity types. The labels were provided in Spacy format and in BIO tagging format.

For the second sub-task, the inputs are the problem description, its set of problem entities, and the order mapping of variable mentions. The ground-truth label annotations consist of the objective declaration and the constraints declarations as shown in Figure \ref{fig:subtask-2}. The output of the semantic parser is the meaning representation of those declarations. As shown in Figure \ref{fig:subtask-2}, the meaning representation should be converted to a canonical form for evaluation. Participants were encouraged to either design their own meaning representation or use the representation and conversion scripts from our pilot study. 

\paragraph{Dataset creation.} A team of 20 AI engineers and OR experts spent three months to create our preliminary LPWP dataset. This team used the Prodigy tool \citep{Prodigy} to manually create and annotate this preliminary dataset containing 600 problems. Within the team, five were tasked with verifying that each problem adhered to specific guidelines to ensure diversity in problem types and language patterns. Throughout the process of creating the remaining 501 samples, suggested annotations were generated using a preliminary NER model trained on the preliminary dataset.  For the second sub-task, we created a custom Prodigy recipe and a Python script to efficiently annotate the ground-truth declarations of the objective and constraints. All of the new problems and annotations for both sub-tasks were verified and corrected by at least two experts. \cite{EMNLP} describes in more details the data creation process (e.g., exclusion criteria, inter-annotator agreement, correction process, average duration of each step, etc.).

Note that we did not use existing datasets from third parties and have released the dataset\footnote{All data are available at: \href{https://github.com/nl4opt/nl4opt-competition}{\textcolor{blue}{https://github.com/nl4opt/nl4opt-competition}}} under the MIT License to benefit the research community.


