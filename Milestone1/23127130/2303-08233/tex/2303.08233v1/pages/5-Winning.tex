\section{Solutions}
\begin{table}[h]
  \begin{minipage}{.5\linewidth}
    \centering
    \caption{\label{tab:ST1-Results}Sub-task 1 winning results.}
    \small
    \begin{tabular}{ c l c }
    \\
      \toprule
      Rank & Team & F1 score \\
      \midrule
      1 & Infrrd AI Lab & 0.939 \\
      2 & mcmc & 0.933 \\
      3 & PingAn-zhiniao & 0.932 \\
      4 & Long & 0.931 \\
      5 & VTCC-NLP & 0.929 \\
      \hdashline
      - & Baseline & 0.906 \\
      \bottomrule
    \end{tabular}
  \end{minipage}%
  \begin{minipage}{.5\linewidth}
    \centering
    \caption{\label{tab:ST2-Results}Sub-task 2 winning results.}
    \small
    \begin{tabular}{ c l c }
    \\
      \toprule
      Rank & Team & F1 score \\
      \midrule
      1 & UIUC-NLP & 0.899 \\
      2 & Sjang & 0.878 \\
      3 & Long & 0.867 \\
      4 & PingAn-zhiniao & 0.866 \\
      5 & Infrrd AI Lab & 0.780 \\
      \hdashline
      - & Baseline & 0.610 \\
      \bottomrule
    \end{tabular}
  \end{minipage}
\end{table}
\subsection{Sub-task 1}
\subsubsection{First Place: Team Infrrd AI Lab}
\textsc{Team Infrrd AI Lab} (JiangLong He, Mamatha N., Shiv Vignesh, Deepak Kumar, Akshay Uppal) leveraged \textbf{ensemble learning} with \textbf{augmentation} to achieve an F1 score of \textbf{0.939} on the test set. Their base model consists of text embedding, BiLSTM, and CRF layers. Through ablations studies, they found that the \textit{majority voting} of an ensemble of 5 different models that were designed in a combination of \textsc{XLM-R-base} and \textsc{RoBERTa-base} transformers for text embeddings, BiLSTM layers, and CRF layers performed the best on the test set. They also implemented 4 types of data augmentation techniques during training. Namely, label-wise token replacement, synonym replacement, mention replacement, and shuffle within segments. For more details, refer to \citep{Infrrd}.

\subsubsection{Second Place: Team mcmc}
\textsc{Team mcmc} (Kangxu Wang, Ze Chen, Jiewen Zheng) trained models for \textbf{ensemble learning} with \textbf{adversarial attacks} to achieve an F1 score of \textbf{0.933} on the test set. They found that implementing adversarial attack using the FGM proposed by \cite{Adversarial} on the \textsc{DeBERTa-large} transformer \citep{DeBERTa-v3} with a CRF layer performed the best on the development set. They trained 9 variations of this model using different random initializations to form their ensemble and leveraged \textit{majority voting} for the final prediction. For more details, refer to \citep{mcmc}.

\subsubsection{Third Place: Team PingAn-zhiniao}
\textsc{Team PingAn-zhiniao} (Qi Zeng, Xiuyuan Yang, Yixiu Wang, Chang Shu) augmented the fine-tuning process of the \textsc{XLM-R-large} transformer by implementing a \textbf{global pointer decoder} followed by a \textbf{multi-head decoder} to achieve an F1 score of \textbf{0.932} on the test set. This was the only sub-task 1 winning submission that did not use ensemble learning. Initially, they fine-tuned using the \textsc{XLM-R-large} encoder to produce the embeddings which was fed into both the global pointer decoder and multi-head decoder. Upon reaching an F1 score of 0.9 on the development set, the global pointer decoder was removed while the encoder with multi-head decoder model continued training.

\subsubsection{Fourth Place: Team Long}
\textsc{Team Long} (Yuting Ning, Jiayu Liu, Longhu Qin, Tong Xiao, Shangzi Xue, Zhenya Huang, Qi Liu, Enhong Chen, Jinze Wu) leveraged \textbf{ensemble learning}, \textbf{adversarial training}, and some \textbf{post-processing} techniques to achieve an F1 score of \textbf{0.931} on the test set. They used \textsc{XLM-R} as the base model and leverage projected gradient descent method \citep{PGD} and FGM for adversarial training. Augmentations included variables swapping, synonym replacement in objective names, and randomizing of numbers. They also implemented some quick-check rules to enforce consistency in tagging entity spans. Four models (XLM-R-base and XLM-R-large) were optimized for specific entity types and the final prediction was obtained through an emsemble learning framework. For more details, refer to \citep{Long} or their code\footnote{\label{long}\href{https://github.com/bigdata-ustc/nl4opt}{Team Long code: \textcolor{blue}{https://github.com/bigdata-ustc/nl4opt}}}.

\subsubsection{Fifth Place: Team VTCC-NLP}
\textsc{Team VTCC-NLP} (Xuan-Dung Doan) proposed \textbf{ensemble learning} to achieve an F1 score of \textbf{0.929} on the test set. They also explored the use of ELMo embedding \citep{ELMO} and GCN models \citep{GCN} and found that both improved the performance of the baseline model accuracy, but negatively impacted the performance when included for ensemble learning. The final ensemble consisted of XLMR, DeBERTaV3, and BART. For more details, refer to \citep{VTCC}.

\subsection{Sub-task 2}
\subsubsection{First Place: Team UIUC-NLP}
\textsc{Team UIUC-NLP} (Neeraj Gangwar, Nickvash Kani) \textbf{tagged the input} and implemented a \textbf{``decode all-at-once" strategy} to achieve an accuracy of \textbf{0.899} on the reserved test set. They used the \textsc{BART-large} encoder-decoder model and enriched the input by surrounding entities with XML-like tagging. Through ablation studies, they found the best performance when combining this input tagging strategy with generating all objective and constraint declarations at once. This team also reports higher sensitivity to hyperparameters and initial seeds when using the large version of \textsc{BART} compared to the base version. For more details, refer to \citep{UIUC} or their code\footnote{\href{https://github.com/mlpgroup/nl4opt-eq-generation}{Team UIUC code: \textcolor{blue}{https://github.com/mlpgroup/nl4opt-eq-generation}}}.

\subsubsection{Second Place: Team Sjang}
\textsc{Team Sjang} (Sanghwan Jang) used a \textbf{scaling hyperparameter} to introduce \textbf{entity tag embeddings} and they implement simple \textbf{data augmentation} to achieve an accuracy of \textbf{0.878} on the reserved test set. Compared to the baseline, they report a 16\% increase in accuracy by implementing the \textsc{BART-large} model, a further 10\% improvement by scaling the tag embedding, and another 1.5\% through simple augmentations to the constraints by reversing the constraint direction. For more details, refer to \citep{Sjang} or their code\footnote{\href{https://github.com/jsh710101/nl4opt}{Team Sjang code: \textcolor{blue}{https://github.com/jsh710101/nl4opt}}}.

\subsubsection{Third Place: Team Long}
\textsc{Team Long} \textbf{redesigned the prompt}, implemented \textbf{data augmentation}, and leveraged \textbf{adversarial training} to achieve an accuracy of \textbf{0.867} on the reserved test set. They used the baseline \textsc{BART-base} with copy mechanism as the generator and leverage adversarial training during fine-tuning by using FGM. They enhance the entities by inserting XML-like tags, and alter the location of constraint and objective direction entities to where they occur in the original input description. For more details, refer to \citep{Long} or their code\textsuperscript{\ref{long}}.

\subsubsection{Fourth Place: Team PingAn-zhiniao}
\textsc{Team PingAn-zhiniao} primarily leveraged \textbf{data preprocessing} and \textbf{hyperparameter tuning} to achieve an accuracy of \textbf{0.866} on the reserved test set. Data preprocessing included wrapping entity types with tags and they report that the most improvement was brought when the bert\_dropout hyperparameter was set to 0.5.

\subsubsection{Fifth Place: Team Infrrd AI Lab}
\textsc{Team Infrrd AI Lab} \textbf{preprocessed} the input and utilized \textbf{multitask training} to achieve an accuracy of \textbf{0.780} on the reserved test set. They used the text-to-text transfer transformer (T5) \citep{T5} and processed the input by wrapping entities with the markup of entity types. They also reported an increase in performance when they separated each sample into multiple samples, each corresponding to one declaration. Multitask learning was leveraged to train the model to generate text when given different prompts. For more details, refer to \citep{Infrrd}.


\subsection{Experiments with large language models}
After the competition ended, we wanted to compare the performance of black-box large language models. In particular, we conducted some experiments with ChatGPT to see how it would perform in our competition. For these experiments, we combined the two sub-tasks and directly asked ChatGPT to generate a problem formulation from a given LPWP (problem description). We evaluated the performance of ChatGPT on both the test and development datasets using the declaration-level mapping accuracy defined in Equation (\ref{equ:accuracy}). To ensure consistent output from ChatGPT, we structured our prompts as follows:

``
$<$\textit{Problem description}$>$\
\textit{Use the above problem description and write the optimization formulation of the problem. Please only give me the model with just one-line explanations for each model element. I don't need the solution. Remove all non-essential spaces. Don't simplify the expressions and don't use LaTeX code or any code in your responses. Use ``x", ``y", and ``z" as variables name.}"

For these experiments, we used the \textit{gpt-3.5-turbo} model trained on data up to September 1st, 2021. To evaluate the performance of ChatGPT, we asked OR experts to manually verify the correctness of the generated models by ChatGPT and measured the  per-declaration accuracy. ChatGPT achieved an accuracy of \textbf{0.927} on the reserved test set for this combined task. 