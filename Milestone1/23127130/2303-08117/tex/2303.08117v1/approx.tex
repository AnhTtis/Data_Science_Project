\section{Omitted Details in \Cref{sec:approx-overview}}\label{sec:approx-detailed}
In \Cref{sec:approx-overview}, we claim that it is possible to approximately execute the Inside-Outside algorithm for PCFG learned on \dataset{PTB} dataset, and can drastically reduce the size of our constructed model with minimal impact on the 1-masking predictions and parsing performance (\Cref{thm:approx-few-nt-informal,thm:approx-low-rank-informal}). We overview the techniques in \Cref{sec:approx-overview}, and we show the details in this section.

\subsection{Finding important non-terminals}\label{sec:approx-few-nt}

Note that in both \Cref{thm:hard_attnt} and \Cref{thm:soft_attnt}, in every layer $1 \le \ell \le L-1$, we use one attention head with parameters $\mK_A^{(\ell)}, \mQ_A^{(\ell)}, \mV_A^{(\ell)}$ to compute all the inside probabilities $\alpha(A, i, j)$ for all spans with length $\ell+1$, i.e. $j-i = \ell$. For layer $L+1 \le \ell \le 2L-1$, the model constructed in \Cref{thm:hard_attnt} uses two attention heads to compute the outside probabilities $\beta(A,i,j)$ for a specific non-terminal $A$ for spans with length $2L - \ell$, and the model constructed in \Cref{thm:soft_attnt} uses one attention heads to compute the outside probabilities $\beta(A,i,j)$ for a specific non-terminal $A$ for spans with length $2L - \ell$. Now to show how restricting the computations to certain non-terminals $\tilde\gI\cup\tilde\gP$ can reduce the size of the constructed models in \Cref{thm:hard_attnt,thm:soft_attnt} we classify the inside and outside probabilities into four categories: (1) the inside probabilities for pre-terminals, $\alpha(A,i,i)$ for $A\in\gP$; (2) the inside probabilities for in-terminals, $\alpha(A,i,j)$ for $A\in\gI$; (3) the outside probabilities for in-terminals, $\beta(A,i,j)$ for $A\in\gI$; and (4) the outside probabilities for pre-terminals, $\beta(A,i,i)$ for $A\in \gP$.

\paragraph{Category (1): the inside probabilities for pre-terminals} Recall that in the constructed model in \Cref{thm:hard_attnt,thm:soft_attnt}, the inside probabilities for pre-terminals $\alpha(A,i,i)$ for $A\in\gP$ is directly initialized from the PCFG rules, and thus do not need attention heads to compute. Thus, we can just use $O(|\gP|)$ dimensions to store all the inside probabilities for pre-terminals $\alpha(A,i,i)$ for $A\in\gP$. Although we can also only initialize the inside probabilities only for the pre-terminals $\tilde\gP$, i.e. initialize $\alpha(A,i,i)$ for $A\in\tilde\gP$ and use less embedding dimensions, empirically the performance will drop and thus we initialize all the probabilities $\alpha(A,i,i)$ for $A\in\gP$. Although we should store the probabilities for pre-terminals larger than the set $\tilde\gP$, there is indeed another technique to reduce the embedding dimensions. Note that since in the future computations, we only compute the probabilities for the in-terminals $\tilde\gI$, and not every pre-terminal $A\in\gP$ can be produced by in-terminals $B\in\tilde\gI$. Thus, we only need to store the pre-terminals $\gP_{\tilde\gI}$ that can be produced from $\tilde\gI$. Empirically, for PCFG learned on \dataset{PTB} dataset, $|\gP| = 720$, but if we choose $|\tilde\gI| = 20$, the number of pre-terminals that can be produced from $\tilde\gI$ drops to $|\gP_{\tilde\gI}| = 268 < 270$.
Specifically for the model in \Cref{thm:soft_attnt}, we need $|\gP_{\tilde\gI}|$ coordinates at each position to store these inside probabilities.

\paragraph{Category (2): the inside probabilities for in-terminals} The computation of the inside probabilities for in-terminals, $\alpha(A,i,j)$ for $A\in\gI$ happens from layer $1$ to layer $L-1$ in the constructed model in \Cref{thm:hard_attnt,thm:soft_attnt}. Note that from layer $1$ to layer $L-1$, the model only computes the probabilities for the in-terminals, since a span with a length larger than 1 cannot be labeled by a pre-terminal. Thus, if we only compute the inside probabilities for in-terminals $|\tilde\gI|$, we can reduce the number of attention heads in layer $1$ to layer $L-1$ from $O(|\gI|)$ to $O(|\tilde\gI|)$ since in \Cref{thm:hard_attnt,thm:soft_attnt} we use a constant number of attention heads to compute the probabilities for a single in-terminal. Specifically for the model in \Cref{thm:soft_attnt}, we only need $|\tilde\gI|$ attention heads from layer $1$ to layer $L-1$; besides, we need $(L-1)|\tilde\gI|$ coordinates at each position to store these inside probabilities.

\paragraph{Category (3): the outside probabilities for in-terminals} The computation of the outside probabilities for in-terminals, $\beta(A,i,j)$ for $A\in\gI$ happens from layer $L$ to layer $L-2$ in the constructed model in \Cref{thm:hard_attnt,thm:soft_attnt}. Note that in layer $L$, we only need to initialize the outside probabilities $\beta(A,1,L)$ for $A\in\gI$, thus do not need attention heads for computation (however we need attention heads to move the inside and outside probabilities in this layer, which cost 2 attention heads). Then from layer $L+1$ to layer $L-2$, the model computes the outside probabilities for the in-terminals $\beta(A,i,j)$ for $A\in\tilde\gI$. Thus if we only compute the outside probabilities for in-terminals $|\tilde\gI|$, we can reduce the number of attention heads in layer $1$ to layer $L-1$ from $O(|\gI|)$ to $O(|\tilde\gI|)$. Specifically for the model in \Cref{thm:soft_attnt}, we only need $|\tilde\gI|$ attention heads from layer $L$ to layer $L-2$; besides, we need $(L-1)|\tilde\gI|$ coordinates at each position to store these outside probabilities for in-terminals $\tilde\gI$.

\paragraph{Category (4): the outside probabilities for pre-terminals} The outside probabilities for pre-terminals $\beta(A,i,i)$ for $A\in\gP$ is only computed in the final layer in \Cref{thm:hard_attnt,thm:soft_attnt}. Thus if we choose to compute the probabilities for only $\tilde\gP$, we can reduce the number of attention heads in layer $2L-1$ from $O(|\gI|)$ to $O(|\tilde\gI|)$. Specifically for the model in \Cref{thm:soft_attnt}, we only need $|\tilde\gP|$ attention heads in layer $L-1$; besides, we need $|\tilde\gP|$ coordinates at each position to store these outside probabilities for in-terminals $\tilde\gP$. Also as mentioned in \Cref{sec:approx-overview}, if $|\tilde\gP| < c|\tilde\gI|$ for some constant $c$, we can also simulate the computations in the last layer with $|\tilde\gP|$ heads by $c$ layers with $|\tilde\gI|$ heads. In particular, if we choose $|\tilde\gP| = 45, |\tilde\gI|=20$, we can use 3 layers with $20$ attention heads in each layer to simulate the last layer with $45$ attention heads in the original construction.

\paragraph{Put everything together: proof of \Cref{thm:approx-few-nt-informal}} We choose $|\tilde\gP| = 45, |\tilde\gI|=20$. We can use $20$ attention heads in each layer, and we now count the number of layers and the embedding dimension we need. The number of layers is easy to compute, since we just need to use $3$ layers with $20$ attention heads to simulate the original $1$ layer with $45$ attention heads, thus the total number of layers is $2L-1 + (3-1) = 2L+1$. As for the embedding dimension, we need
\[d = |\gP_{\tilde\gI}| + (L-1)|\tilde\gI| + (L-1)|\tilde\gI| + |\tilde\gP| \le 270 + (2L-2)|\tilde\gI| + |\tilde\gP| = 275 + 2L|\tilde\gI| = 275 + 40L.\]

%For layer $L+1 \le \ell \le 2L-1$, the model constructed in \Cref{thm:hard_attnt} uses two attention heads to compute the outside probabilities $\beta(A,i,j)$ for a specific non-terminal $A$ for spans with length $2L - \ell$, and the model constructed in \Cref{thm:soft_attnt} uses one attention heads to compute the outside probabilities $\beta(A,i,j)$ for a specific non-terminal $A$ for spans with length $2L - \ell$. Thus, it is clear that if we only compute the inside and outside probabilities for a small set of non-terminals $\tilde\gN$, the dependency on $|\gN|$ reduces to $|\tilde\gN|$ for the number of attention heads in both constructions, i.e., for the construction in \Cref{thm:hard_attnt}, the number of attention heads drops from $4|\gN|$ to $4|\tilde\gN|$, and for the construction in \Cref{thm:soft_attnt}, the number of attention heads drops from $|\gN|$ to $|\tilde\gN|$.

%Besides reducing the number of attention heads, computing only the probabilities for important non-terminals can also reduce the number of embedding dimensions at each layer. Note that in the construction in \Cref{thm:hard_attnt} and \Cref{thm:soft_attnt}, we need to store all the inside and outside probabilities, and thus the embedding size has the term $|\gN| L$, where $L$ is the length of the sentence. The number of embedding dimensions for different models may have different coefficients before $|\gN| L$. Now if we only compute the probabilities for non-terminals $\tilde\gN$, we don't need to store the probabilities

\subsection{Utilizing structures across non-terminals}\label{sec:approx-low-rank}

In this section, we show the details of how to further reduce the number of attention heads using structures across non-terminals, and add more discussion on how we learn the transformation matrices $\{\mW^{(\ell)}\}_{\ell \le L}$

\paragraph{Reducing the number of attention heads} We focus on reducing the number of attention heads to compute the inside and outside probabilities for the in-terminals $\tilde\gI$, since the computation for the outside probabilities for pre-terminals $\tilde\gP$ only happens in the final layer of the constructed model, and thus can use multiple layers to compute as long as $\tilde\gP$ is not too large.

For simplicity, we only show the details of how to reduce the number of attention heads to compute the inside probabilities for in-terminals $\tilde\gI$ in \Cref{thm:soft_attnt}, and the technique can be easily applied to the computation of outside probabilities for in-terminals $\tilde\gI$ in \Cref{thm:soft_attnt}, and the inside and outside probabilities for $\tilde\gI$ in \Cref{thm:hard_attnt}.

Recall from the proof of \Cref{thm:soft_attnt} that we at each layer $\ell$, we use a single attention head $\mK^{(\ell)}_A, \mQ^{(\ell)}_A$ to compute the inside probabilities $\alpha(A,i,j)$ for spans with length $\ell+1$, i.e., $j-i = \ell$. Specifically, for the attention head $\mK^{(\ell)}_A, \mQ^{(\ell)}_A$ at layer $\ell$, we want to compute and store the probability $\alpha(A, i-\ell, i)$ at position $i$. Thus we construct $\mK^{(\ell)}_A, \mQ^{(\ell)}_A$ such that the attention score $a_{i,j}^{A,(\ell)}$ when the position $i$ attends to position $j$ satisfies
\begin{align*}
     a_{i,j}^{A,(\ell)} & =\text{ReLU}(\mK_{A}^{(\ell)} \ve_j^{(\ell-1)} + p_{j-i} - b_{j-i, \ell})^{\top}  \mQ_{A}^{(\ell)} \ve_i^{(\ell-1)} \\
     &= \sum_{B, C \in \gN} \Pr[A \to B C] \cdot \alpha(B, j+1, i) \cdot  \alpha (C, i-\ell, j),
\end{align*}
if $i - \ell \le j \le i-1$ and $0$ otherwise. Then, summing over all locations $j$ gives us $\alpha(A, i-\ell, i)$. Also, a key property of $\mK_{A}^{(\ell)}$ is that this key matrix does not depend on the non-terminal $A$, but only depends on $\ell$. Thus, if we have a set of coefficients $\{\omega_A^{(\ell)}\}_{A\in\gI}$, we can compute the linear combination of the inside probability $\sum_{A\in\tilde\gI} \omega_A^{(\ell)}\alpha(A, i-\ell, i)$ using one attention head, since if we choose
\[\mQ^{(\ell)} = \sum_{A\in\tilde\gI} \omega_A^{(\ell)}\mQ_{A}^{(\ell)}, \quad\mK^{(\ell)} = \mK_A^{(\ell)}, \forall A\in\tilde\gI,\]
we have the attention score
\begin{align*}
     a_{i,j}^{(\ell)} & =\text{ReLU}(\mK^{(\ell)} \ve_j^{(\ell-1)} + p_{j-i} - b_{j-i, \ell})^{\top}  \mQ^{(\ell)} \ve_i^{(\ell-1)} \\
     & =\text{ReLU}(\mK^{(\ell)} \ve_j^{(\ell-1)} + p_{j-i} - b_{j-i, \ell})^{\top}  \left(\sum_{A\in\tilde\gI} \omega_A^{(\ell)}\mQ_{A}^{(\ell)}\right) \ve_i^{(\ell-1)} \\
     & =\sum_{A\in\tilde\gI} \omega_A^{(\ell)} \cdot \text{ReLU}(\mK^{(\ell)} \ve_j^{(\ell-1)} + p_{j-i} - b_{j-i, \ell})^{\top}  \mQ_{A}^{(\ell)} \ve_i^{(\ell-1)} \\
     & =\sum_{A\in\tilde\gI} \omega_A^{(\ell)} \cdot \text{ReLU}(\mK_A^{(\ell)} \ve_j^{(\ell-1)} + p_{j-i} - b_{j-i, \ell})^{\top}  \mQ_{A}^{(\ell)} \ve_i^{(\ell-1)} \\
     &= \sum_{A\in\tilde\gI} \omega_A^{(\ell)}\left(\sum_{B, C \in \gN} \Pr[A \to B C] \cdot \alpha(B, j+1, i) \cdot  \alpha (C, i-\ell, j)\right),
\end{align*}
if $i - \ell \le j \le i-1$ and $0$ otherwise. Then, summing over all locations $j$ gives us $\sum_{A\in\tilde\gI} \omega_A^{(\ell)}\alpha(A, i-\ell, i)$. Then if we have a transformation matrix $\mW^{(\ell)}\in\R^{k^{(\ell)}\times |\tilde\gI|}$, we can use $k^{(\ell)}$ attention heads to compute $\mW^{(\ell)}\bm\alpha(i-\ell, i)$, where $\bm\alpha(i-\ell, i)\in\R^{|\tilde\gI|}$ is the vector that contains $\alpha(A, i-\ell, i)$ for all $A\in\tilde\gI$. Then after we use $k^{(\ell)}$ attention heads to compute the probabilities $\mW^{(\ell)}\bm\alpha(i-\ell, i)$ and stored them in position $i$'s embeddings, we can then use linear layer on position $i$ to recover the original probabilities by $\bm{\tilde\alpha}(i-\ell, i) = (\mW^{(\ell)})^{\dagger} \mW^{(\ell)}\bm\alpha(i-\ell, i)$, and use $\tilde\alpha(A, i-\ell, i)$ for $A\in\tilde\gI$ for the future computations.

\paragraph{More discussions on the transformation matrix $\mW^{(\ell)}$} First, we can observe that by introducing the transformation matrix $\mW^{(\ell)}$ generalized the previous method that only computes a small set of in-terminals $\tilde\gI$ and pre-terminals $\tilde\gP$, and in theory we can directly learn the transformation matrix $\mW^{(\ell)}$ from the original PCFG without reducing the size at first, i.e., $\mW^{(\ell)}\in\R^{k^{(\ell)}\times |\gI|}$. However empirically, if we directly learn $\mW^{(\ell)}$ from all the in-terminals $\gI$ but not from the top-20 frequent in-terminals $\tilde\gI$, the performance drops. Thus, we choose to learn the matrix $\mW^{(\ell)}$ starting from the most frequent in-terminals $\tilde\gI$. One possible explanation is that the learning procedure is also heuristic, and certainly may not learn the best transformation matrix.

Besides, we use the same transformation matrix $\mW^{(\ell)}$ when computing the inside and outside probabilities, and it is also natural to use different transformation matrices when computing the inside and outside probabilities. Recall that we learn the transformation $\mW^{(\ell)}$ by the Eigenvalue decomposition on matrix $\mX^{(\ell)}$, where $\mX^{(\ell)} = \sum_{s} \mX_s^{(\ell)} / \norm{\mX_s^{(\ell)}}_{\text{F}}$ and $\mX_s^{(\ell)} = \sum_{i,j:j-i=\ell} \vmu_s^{i,j}(\vmu_s^{i,j})^\top$. Then, we can also learn two matrices $\mW^{(\ell)}_{\text{inside}}$ and $\mW^{(\ell)}_{\text{outside}}$ through the Eigenvalue decomposition on matrices $\mX^{(\ell)}_{\text{inside}}$ and $\mX^{(\ell)}_{\text{outside}}$ respectively, where
\begin{align*}
    \mX^{(\ell)}_{\text{inside}} =& \sum_{s} \mX_{s,\text{inside}}^{(\ell)} / \norm{\mX_{s,\text{inside}}^{(\ell)}}_{\text{F}}, \quad \mX_{s,\text{inside}}^{(\ell)} = \sum_{i,j:j-i=\ell} \bm\alpha_s^{i,j}(\bm\alpha_s^{i,j})^\top, \\
    \mX^{(\ell)}_{\text{outside}} =& \sum_{s} \mX_{s,\text{outside}}^{(\ell)} / \norm{\mX_{s,\text{outside}}^{(\ell)}}_{\text{F}}, \quad \mX_{s,\text{outside}}^{(\ell)} = \sum_{i,j:j-i=\ell} \bm\beta_s^{i,j}(\bm\beta_s^{i,j})^\top.
\end{align*}
However empirically, we also find that the performance drops by using different transformation matrices for inside and outside probabilities computation, which may also be attributed to the non-optimality of our method to learn the transformation matrix.

\paragraph{Put everything together: proof of \Cref{thm:approx-low-rank-informal}} We choose $k^{(\ell)} = 15,|\tilde\gP| = 45, |\tilde\gI|=20$. Note that the embedding dimension doesn't change if we apply the approximation technique, and only the number of attention heads reduces from $20$ to $15$. Thus, the embedding dimension is still
\[d = |\gP_{\tilde\gI}| + (L-1)|\tilde\gI| + (L-1)|\tilde\gI| + |\tilde\gP| \le 270 + (2L-2)|\tilde\gI| + |\tilde\gP| = 275 + 2L|\tilde\gI| = 275 + 40L.\]
Also note that $|\tilde\gP| = 45 = 3\times 15$, and thus we can compute all the outside probabilities for pre-terminals $\tilde\gP$ by $3$ layers where each layer has $15$ attention heads.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%