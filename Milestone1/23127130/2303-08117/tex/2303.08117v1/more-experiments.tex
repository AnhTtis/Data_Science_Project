\section{More Experiment Results}
In this section, we provide more experiment results for RoBERTa pre-trained on PCFG-generated data. In \Cref{sec:more-parsing-result}, we show more structural probing results related to the experiments in \Cref{sec:parse}. In \Cref{sec:attn-patterns}, we do some simple analysis on the attention patterns for RoBERTa pre-trained on PCFG-generated data, trying to gain more understanding of the mechanism beneath large language models.

\subsection{More results on constituency parsing}\label{sec:more-parsing-result}

\paragraph{Probing on embeddings from different layers} In \Cref{sec:parse}, we show the probing results on the embeddings either from $0$-th layer or from the best layer (the layer that achieves the highest F1 score) of different pre-trained models. In this section, we show how the F1 score changes with different layers.

\Cref{fig:f1-using-different-layers} shows sentence F1 scores for linear probes $f(\cdot)$ trained on different layers' embeddings for different pre-trained models. We show the results under the \dataset{PCFG} and \dataset{PTB} settings. From \Cref{fig:f1-using-different-layers}, we observe that using the embeddings from the $0$-th layer can only get sentence F1 scores close to (or even worse than) the naive Right-branching baseline for all the pre-trained models. However, except for model A3L12, the linear probe can get at least $60\%$ sentence F1 using the embeddings from layer 1. Then, the sentence F1 score increases as the layer increases, and gets nearly saturated at layer 3 or 4. The F1 score for the latter layers may be better than the F1 score at layer 3 or 4, but the improvement is not significant. The observations still hold if we change the linear probe to a neural network, consider the OOD setting instead of \dataset{PCFG} and \dataset{PTB}, or change the measurement from sentence F1 to corpus F1.

Our observations suggest that most of the constituency parse tree information can be encoded in the lower layers, and a lot of the parse tree information can be captured even in the first layer. Although our constructions (\Cref{thm:hard_attnt,thm:soft_attnt}) and approximations (\Cref{thm:approx-few-nt-informal,thm:approx-low-rank-informal}) try to reduce the number of attention heads and the number of embedding dimensions close to the real language models, we don't know how to reduce the number of layers close to BERT or RoBERTa (although our number is acceptable since GPT-3 has 96 layers). More understanding of how language models can process such information in such a small number of layers is needed.

%\haoyu{need a final ``conclusion'' sentence, like we need more understanding on how transformers process information in such a small number of layers.}

\begin{figure*}
\begin{subfigure}[t]{0.48\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figs/parsing-pcfg-diff-layers-linear.pdf}
    \caption{Comparison under \dataset{PCFG} setting. We compare the models with different number of layers.}
    \label{fig:parsing-pcfg-diff-layers-linear}
\end{subfigure}
\hfill
\begin{subfigure}[t]{0.48\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figs/parsing-pcfg-diff-attn-linear.pdf}
    \caption{Comparison under \dataset{PCFG} setting. We compare the models with different number of attention heads.}
    \label{fig:parsing-pcfg-diff-attn-linear}
\end{subfigure}

\begin{subfigure}[t]{0.48\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figs/parsing-ptb-diff-layers-linear.pdf}
    \caption{Comparison under \dataset{PTB} setting. We compare the models with different number of layers.}
    \label{fig:parsing-ptb-diff-layers-linear}
\end{subfigure}
\hfill
\begin{subfigure}[t]{0.48\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figs/parsing-ptb-diff-attn-linear.pdf}
    \caption{Comparison under \dataset{PTB} setting. We compare the models with different number of attention heads.}
    \label{fig:parsing-ptb-diff-attn-linear}
\end{subfigure}
    \caption{Sentence F1 for linear probes $f(\cdot)$ trained on different layers' embeddings for different pre-trained models. We show the results under \dataset{PCFG} and \dataset{PTB} settings. A$i$L$j$ denotes the pre-trained model with $i$ attention heads and $j$ layers.}
    \label{fig:f1-using-different-layers}
\end{figure*}

\paragraph{Comparison with probes using other input structures} In \Cref{sec:parse}, we train a probe $f(\cdot)$ to predict the relative depth $\text{tar}(i) = \text{depth}(i,i+1) - \text{depth}(i-1,i)$, and the input to the probe $f$ is the concatenation of the embedding $\ve^{(\ell)}_i$ at position $i$ and the embedding $\ve^{(\ell)}_{\text{EOS}}$ for the EOS token at some layer $\ell$. Besides taking the concatenation $[\ve^{(\ell)}_i; \ve^{(\ell)}_{\text{EOS}}]$ as the input structure of the probe, it is also natural to use the concatenation $[\ve^{(\ell)}_{i-1}; \ve^{(\ell)}_i; \ve^{(\ell)}_{i+1}]$ to predict the relative depth $\text{tar}(i)$. In this part, we compare the performances of probes with different input structures. We use EOS to denote the probe that takes $[\ve^{(\ell)}_i; \ve^{(\ell)}_{\text{EOS}}]$ as the input and predicts the relative depth, while ADJ (Adjacent embeddings) to denote the probe the takes $[\ve^{(\ell)}_{i-1}; \ve^{(\ell)}_i; \ve^{(\ell)}_{i+1}]$ as input.

\Cref{fig:parsing-diff-inputs} shows the probing results on A12L12, the model with 12 attention heads and 12 layers. We compare the probes with different inputs structure (EOS or ADJ), and the input embeddings come from different layers (the $0$-th layer or the layer that achieves the best F1 score). We observe that: (1) the probes using ADJ input structure have better parsing scores than the probes using EOS input structure, and (2) the sentence F1 for the probes using the ADJ input structure is high even if the input comes from layer 0 of the model ($>55\%$ for linear $f(\cdot)$ and $>60\%$ for neural network $f(\cdot)$). Although the probe using ADJ has better parsing scores than the probe using EOS, it is harder to test whether it is a good probe, since the concatenation of adjacent embeddings $[\ve^{(0)}_{i-1}; \ve^{(0)}_i; \ve^{(0)}_{i+1}]$ from layer $0$ is already contextualized, and it is hard to find a good baseline to show that the probe is \emph{sensitive} to the information we want to test. Thus, we choose to follow~\citet{vilares2020parsing,arps2022probing} and use the probe with input structure $[\ve^{(\ell)}_i; \ve^{(\ell)}_{\text{EOS}}]$ in \Cref{sec:parse}.

Nonetheless, the experiment results for probes taking $[\ve^{(0)}_{i-1}; \ve^{(0)}_i; \ve^{(0)}_{i+1}]$ as input are already surprising: by knowing three adjacent word identities and their position (the token embedding $\ve^{(0)}_i$ contains both the word embedding and the positional embedding) and train a 2-layer neural network on top of that, we can get $62.67\%, 63.91\%, 57.02\%$ sentence F1 scores under \dataset{PCFG}, \dataset{PTB}, and OOD settings respectively. As a comparison, the probe taking $[\ve^{(\ell)}_i; \ve^{(\ell)}_{\text{EOS}}]$ as input~\citep{vilares2020parsing,arps2022probing} only get $39.06\%, 39.31\%, 33.33\%$ sentence F1 under \dataset{PCFG}, \dataset{PTB}, and OOD settings respectively. It shows that lots of syntactic information (useful for parsing) can be captured by just using adjacent words without more context.


\begin{figure*}
    \begin{subfigure}[t]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/parsing-diff-inputs-linear.pdf}
        \caption{Comparison of different inputs under different settings when the probe $f(\cdot)$ is linear.}
        \label{fig:parsing-diff-inputs-linear}
    \end{subfigure}
    \begin{subfigure}[t]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/parsing-diff-inputs-nn.pdf}
        \caption{Comparison of different inputs under different settings when the probe $f(\cdot)$ is a 2-layer neural network.}
        \label{fig:parsing-diff-inputs-nn}
    \end{subfigure}
    \caption{Comparison of the probes with different inputs under different settings. We probe the model with 12 attention heads and 12 layers, and report the scores with $f(\cdot)$ taking embeddings from layer 0 or the embeddings from the best layer. EOS denotes the probe that takes $[\ve^{(\ell)}_i; \ve^{(\ell)}_{\text{EOS}}]$ as input and predicts the relative depth $\text{tar}(i)$, and ADJ (Adjacent embeddings) denotes the probe that takes $[\ve^{(\ell)}_{i-1}; \ve^{(\ell)}_i; \ve^{(\ell)}_{i+1}]$ as input.}
    \label{fig:parsing-diff-inputs}
\end{figure*}


\paragraph{More discussion on probing measurement} (Unlabelled) F1 score is the default performance measurement in the constituency parsing and syntactic probing literature. However, we would like to point out that only focusing on the F1 score may cause some bias. Because all the spans have equal weight when computing the F1 score, and most of the spans in a tree have a short length (if the parse tree is perfectly balanced, then length 2 spans consist of half of the spans in the parse tree), one can get a decently well F1 score by only getting correct on short spans. Besides, we also show that by taking the inputs $[\ve^{(0)}_{i-1}; \ve^{(0)}_i; \ve^{(0)}_{i+1}]$ from layer 0 of the model (12 attention heads and 12 layers), we can already capture a lot of the syntactic information useful to recover the constituency parse tree (get a decently well F1 score). Thus, the F1 score for the whole parse tree may cause people to focus less on the long-range dependencies or long-range structures, and focus more on the short-range dependencies or structures.

To mitigate this problem, \citet{vilares2020parsing} computed the F1 score not only for the whole parse tree, but also for each length of spans. \citet{vilares2020parsing} showed that BERT trained on natural language can get a very good F1 score when the spans are short (for length 2 spans, the probing F1 is over $80\%$), but when the span becomes longer, the F1 score quickly drops. Even for spans with length 5, the F1 score is less than $70\%$, and for spans with length 10, the F1 score is less than $60\%$. Our experiments that probe the marginal probabilities for different lengths of spans (\Cref{sec:probe-marginal-probs}) can also be viewed as an approach to mitigate the problem.



\subsection{Analysis of attention patterns}\label{sec:attn-patterns}
In \Cref{sec:parse}, we probe the embeddings of the models pre-trained on synthetic data generated from PCFG and show that model training on MLM indeed \emph{captures} syntactic information that can recover the constituency parse tree, but we don't know how the models capture that information. \Cref{thm:io-optimal-mlm} builds the connection between MLM and the Inside-Outside algorithm, and the connection is also verified in \Cref{sec:probe-marginal-probs}, which shows that the embeddings also contain the marginal probability information computed by the Inside-Outside algorithm. However, we only build up the correlation between the Inside-Outside algorithm and the attention models, and we still don't know the mechanism inside the language models: the model may be executing the Inside-Outside algorithm (or some approximations of the Inside-Outside algorithm), but it may also use some mechanism far from the Inside-Outside algorithm but happens to contain the marginal probability information. To understand more about the mechanism of language models, we need to \emph{open up the black box} and go further than probing, and this section serves as one step to do so.

\paragraph{General idea} The key ingredient that distinguishes current large language models and the fully-connected neural networks is the self-attention module. Thus besides probing for certain information, we can also look at the attention score matrix and discover some patterns. In particular, we are interested in how far an attention head looks at, which we called the "averaged attended distance".

\paragraph{Averaged attended distance} For a model and a particular attention head, given a sentence $s$ with length $L_s$, the head will generate an $L_s \times L_s$ matrix $\mA$ containing the pair-wise attention score, where each row of $\mA$ sums to 1. Then we compute the following quantity ``Averaged attended distance''
\[\text{AD}_s = \frac{1}{L_s}\sum_{1\le i,j \le L_s} |i-j|\cdot \mA_{i,j},\]
which can be intuitively interpreted as ``the average distance this attention head is looking at''. We then take the average of the quantity for all sentences. We compute ``Averaged attended distance'' for three models on the synthetic \dataset{PCFG} dataset and \dataset{PTB} dataset. The models all have 12 attention heads in each layer but have 12, 6, 3 layers respectively.

\paragraph{Experiment results} \Cref{fig:attn-dist} shows the results of the ``Averaged attented distance'' for each attention head in different models. \cref{fig:attn-dist-a12l12-pcfg,fig:attn-dist-a12l6-pcfg,fig:attn-dist-a12l3-pcfg} show the results on the synthetic \dataset{PCFG} dataset, and \cref{fig:attn-dist-a12l12-ptb,fig:attn-dist-a12l6-ptb,fig:attn-dist-a12l3-ptb} show the results on the \dataset{PTB} dataset. We sort the attention heads in each layer according to the ``Averaged attended distance''.

From \cref{fig:attn-dist-a12l12-pcfg,fig:attn-dist-a12l6-pcfg,fig:attn-dist-a12l3-pcfg}, we can find that for all models, there are several attention heads in the first layer that look at very close tokens (``Averaged attended distance'' less than $3$). Then as the layer increases, the ``Averaged attended distance'' also increases in general, meaning that the attention heads are looking at further tokens. Then at some layer, there are some attention heads looking at very far tokens (``Averaged attended distance'' larger than 12).\footnote{Note that the average length of the sentences in the synthetic \dataset{PCFG} dataset is around 24, if the attention head gives 0.5 attention score to the first and the last token for every token, the ``Averaged attended distance'' will be 12.} This finding also gives some implication that the model is doing something that correlates with our construction: it looks longer spans as the layer increases. However, different from our construction that the attention head only looks at a fixed length span, models trained using MLM look at different lengths of spans at each layer, which cannot be explained by our current construction, and suggests a further understanding of the mechanism of large language models.

Besides, we can find that the patterns are nearly the same for the synthetic \dataset{PCFG} dataset and \dataset{PTB} dataset, and thus the previous finding can also be transferred to the \dataset{PTB} dataset.


\begin{figure}[!t]
\centering
\begin{subfigure}[b]{0.48\textwidth}
\centering
\includegraphics[width=\textwidth]{figs/pcfg-dist-patterns-a12l12.pdf}
\caption{12 attention heads and 12 layers, \dataset{PCFG} dataset.}
\label{fig:attn-dist-a12l12-pcfg}
\end{subfigure}
\hfill
\begin{subfigure}[b]{0.48\textwidth}
\centering
\includegraphics[width=\textwidth]{figs/ptb-dist-patterns-a12l12.pdf}
\caption{12 attention heads and 12 layers, \dataset{PTB} dataset.}
\label{fig:attn-dist-a12l12-ptb}
\end{subfigure}

\begin{subfigure}[b]{0.48\textwidth}
\centering
\includegraphics[width=\textwidth]{figs/pcfg-dist-patterns-a12l6.pdf}
\caption{12 attention heads and 6 layers, \dataset{PCFG} dataset.}
\label{fig:attn-dist-a12l6-pcfg}
\end{subfigure}
\hfill
\begin{subfigure}[b]{0.48\textwidth}
\centering
\includegraphics[width=\textwidth]{figs/ptb-dist-patterns-a12l6.pdf}
\caption{12 attention heads and 6 layers, \dataset{PTB} dataset.}
\label{fig:attn-dist-a12l6-ptb}
\end{subfigure}

\begin{subfigure}[b]{0.48\textwidth}
\centering
\includegraphics[width=\textwidth]{figs/pcfg-dist-patterns-a12l3.pdf}
\caption{12 attention heads and 3 layers, \dataset{PCFG} dataset.}
\label{fig:attn-dist-a12l3-pcfg}
\end{subfigure}
\hfill
\begin{subfigure}[b]{0.48\textwidth}
\centering
\includegraphics[width=\textwidth]{figs/ptb-dist-patterns-a12l3.pdf}
\caption{12 attention heads and 3 layers, \dataset{PTB} dataset.}
\label{fig:attn-dist-a12l3-ptb}
\end{subfigure}


\caption{``Averaged attented distance'' of each attention heads for different models on \dataset{PCFG} and \dataset{PTB} datasets. \cref{fig:attn-dist-a12l12-pcfg,fig:attn-dist-a12l6-pcfg,fig:attn-dist-a12l3-pcfg} show the results on the synthetic \dataset{PCFG} dataset, and \cref{fig:attn-dist-a12l12-ptb,fig:attn-dist-a12l6-ptb,fig:attn-dist-a12l3-ptb} show the results on the \dataset{PTB} dataset.}
\label{fig:attn-dist}
\end{figure}