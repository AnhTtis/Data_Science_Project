\section{Conclusion and Further Discussion}

In this work, we show that masked language models with moderate size have the capacity to parse decently well. Besides, we probe BERT-like models pre-trained (with MLM loss) on synthetic text generated using PCFGs and empirically verify that these models capture syntactic information that helps reconstruct (partially) a parse tree.
%with pre-training under MLM loss. 
Furthermore, we show that the models contain the marginal span probabilities computed by the Inside-Outside algorithm, thus connecting masked language pre-training and parsing. We hope our findings may yield new insights into large language models and masked language modeling. 


One limitation of our paper is that we use probing experiments to show the existence of Inside-Outside probabilities inside the contextualized embeddings. However, we don't have definitive experiments  to show whether the learned model actually simulates the Inside-Outside algorithm on the input.   We leave for future work the design of  experiments to interpret the content of the contextualized embeddings and thus ``reverse-engineer'' the algorithms used by the learned model. Other interesting directions include the convergence analysis of attention models on different generative models, the importance of model scale during pre-training under different generative models, and the differences between parsing with shallow and deep models. 



%One future direction is to understand how language models can process the parsing information using very few layers.