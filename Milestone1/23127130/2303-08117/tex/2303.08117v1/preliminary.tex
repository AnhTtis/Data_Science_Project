\section{Preliminaries}\label{sec:preliminary}

\subsection{Attention}
%\abhishek{Need to have some primer on attention here}
%\haoyu{will also compress this section later}

For the rest of the paper, we will focus on encoder-only models like BERT and RoBERTa \citep{devlin2019bert,liu2019roberta}. An encoder-only model stacks multiple identical layers, where each layer contains an attention module, followed by a feed-forward module. We focus our interest on the attention module. Each attention module is comprised of multiple heads, where the computation of each head $h$ is represented by three matrices $\mQ_{h}, \mK_{h}, \mV_{h} \in \mathbb{R}^{d \times d}$.

Given an input sequence $[w_1, \cdots, w_L]$ of length $L$, we will denote the contextual embeddings of the sequence after layer $\ell$'s computations using $\mE^{(\ell)} \in \R^{L \times d},$ where $\ve_i^{(\ell)}$ denotes the contextual embedding of $i^{th}$ token. The computation of an attention head $h$ at layer $\ell$ is given by
\begin{align*}
    \vv^{(\ell)}_{i, h} = \sum_{j \in [L]} a^h_{i, j} \mV_h \ve^{(\ell)}, 
\end{align*}
where $a^h_{i, j}$ is defined as attention score between $\ve_i$ and $\ve_j$ with head $h$ and is given by
\begin{align}
    a^h_{i, j} = f_{\text{attn}} ( \mE^{(\ell)} \mK_h^{\top}, \mQ_h \ve_i^{(\ell)} )_j \label{def:attention}.
\end{align}
$f_{attn}$ is a non-linear function and is generally used as softmax activation on $\mE^{(\ell)} \mK_h^{\top} \mQ_h \ve_i^{(\ell)}$. Finally, the output of the attention module is given by $\sum_h \vv^{(\ell)}_{i, h}.$ Note that the above definition of attention captures the attention module used in practice, where the contextual embeddings are split equally among the different attention heads, and the outputs of each head are simply merged after individual head computations. For ease of presentation, we will use this general definition henceforth.
%However, for simplicity of constructions 





%We denote $\ve_i^{(\ell)}\in\R^d$ the embedding for position $i$ at layer $\ell$ in the attention model and $\mE^{(\ell)} = [\ve_1^{(\ell)},\dots,\ve_L^{(\ell)}]\in\R^{d\times L}$ the matrix form. 

%Then a single attention head in layer $\ell$ consists of three matrices $\mK\in\R^{d\times k}$: the key matrix, $\mQ\in\R^{d\times k}$ the query matrix, and $\mV^{d\times d}$ the value matrix. Then for a fixed position $i$, we can compute the attention scores of each position $\bm \alpha = h((\mE^{(\ell)})^\top \mK \mQ^\top \ve_i^{(\ell)})$, where $h$ is some activation function, e.g., the softmax. Then, the embedding after this attention head is computed as $\ve_i = \sum_j \alpha_j \mV \ve_i^{\ell}$ In this paper, to make our construction easier, we consider two forms of activation function $h$, the first is similar to the ``hard attention'' where we only keep the largest entry and set others to $0$, formally $h_i(\vx) = \vx_i$ if $i = \arg\max_i \vx_i$ and $0$ otherwise. The second function $h$ is more related to the ``soft attention'' where $h_i(\vx) = \text{ReLU}(\vx_i)$ (we change the softmax to ReLU).

\subsection{PCFG and parsing}
\paragraph{PCFG model} A probabilistic context-free grammar (PCFG) is a formal language generative model. It is defined as a 5-tuple $\gG = (\gN, \gI, \gP, n, p)$, where
\begin{itemize}
    \item $\gN$ is the set of non-terminal symbols in the grammar. $\gI\subset\gN$ is a finite set of \emph{in-terminals}. $\gP\subset\gN$ is a finite set of \emph{pre-terminals}. We assume that $\gN = \gI\cup\gP$, and $\gI\cap\gP = \phi$.
    \item $[n]$ is the set of all possible words.
    \item For all $A\in\gI, B\in\gN, C\in\gN$, there is a context-free rule $A\to BC$.
    \item For rule $A\to BC$ where $A\in\gI, B\in\gN, C\in\gN$, there is a probability $\Pr[A\to BC]$. The probabilities satisfy for all $A$, $\sum_{B,C}\Pr[A\to BC] = 1$.
    \item For all $A\in\gP, w\in [n]$, there is a context-free rule $A\to w$.
    \item For each rule $A\to w$ where $A\in\gP, w\in [n]$, there is a probability $\Pr[A\to w]$. The probabilities satisfy for all $A$, $\sum_{w}\Pr[A\to w] = 1$.
    \item A non-terminal $\text{Root}\in\gI$. %A non-terminal $\text{Root}$ and transitions $\text{Root} \to A$ for all non-terminals $A$.
\end{itemize}

Strings are generated from the PCFG as follows: We always maintain a string $s_t \in ([n]\cup \gN)^*$ at step $t$. The initial string $s_1 = \text{ROOT}$. At step $t$, if all characters in $s_t$ belong to $[n]$, the generation process ends, and $s_t$ is the resulting string. Otherwise, we pick a character $A\in s_t$ such that $A\in\gN$. If $A\in\gP$, we replace the character $A$ to $w$ with probability $\Pr[A\to w]$. If $A\in\gI$, we replace the character $A$ to two characters $BC$ with probability $\Pr[A\to BC]$.

% we reach a string $s \in [n]^*$, using the derivation $s_1, s_2, \cdots,  s_t$, where $s_t=s$. For all $1 < i < t$, $s_i \in (\gN \cup [n])^{*}$ is derived from $s_{i-1}$ by choosing a non-terminal $A \in s_{i-1}$ and replacing it with a rule $\beta \in \{A \to BC\}_{B, C \in \gN}.$ 


\paragraph{Parse trees and parsing} For a sentence $s = w_1w_2\dots w_L$ with length $L$, a labeled parse tree represents the most probable list of derivations that lead to the generation of the sentence under the PCFG $\gG$. It is defined as a list of spans with non-terminals $\{(A, i, j)\}$ that forms a tree involving all the spans $\{i, j\}_{i, j \in [L]}$. An unlabelled parse tree is a list of spans (without the non-terminals) such that it forms a tree. We also call the tree binary if the resulting tree only has at most two children for each node.

Given a sentence $s$ and the PCFG model, one famous procedure to find the (unlabelled) parse tree is to use the Labelled-Recall algorithm~\citep{goodman1996parsing}, where the algorithm finds the tree $T = \{(i,j)\}$ such that $\sum_{(i,j)\in T} \mathrm{score}(i,j)$ is maximized, where $\mathrm{score}(i,j) = \max_{A\in \gN} \Pr[A\Rightarrow w_iw_{i+1}\cdots w_j, \text{Root}\Rightarrow s|\gG]$. $\Pr[A\Rightarrow w_iw_{i+1}\cdots w_j, \text{Root}\Rightarrow s|\gG]$ is called the marginal probability of the span $w_iw_{i+1}\cdots w_j$ under the non-terminal $A$ and is denoted by $\mu(A, i, j)$. 
%is called the marginal probabilities.

We use the Inside-Outside algorithm \citep{baker1979trainable,manning1999foundations} to compute the marginal probabilities. The Inside-Outside algorithm uses dynamic programming to compute two probability terms, the inside probabilities $\alpha(A, i,j) = \Pr[A\to w_iw_{i+1}\dots w_j|\gG]$ and the outside probabilities $\beta(A, i, j)=\Pr[\text{Root}\rightarrow w_1w_2\dots w_{i-1} A w_{j+1}\dots w_L|\gG]$ for all non-terminals and all spans $(i, j)$ with $1 \le i \le j \le L$. Specifically, the recursive relation is given by
{\small
\begin{align}
    & \alpha(A,i,j) \label{eq:inside_probability} = \sum_{B,C}\sum_{k=i}^{j-1} \Pr[A\to BC]\alpha(B,i,k)\alpha(C,k+1,j), 
\end{align}
}

{\small
\begin{align}
     \beta(A,i,j)  \label{eq:outside_probability}
    &= \sum_{B, C} \sum_{k=1}^{i-1}\Pr[B \to C A] \alpha(C, k, i-1) \beta(B, k, j)  + \sum_{B, C} \sum_{k=j+1}^{L}\Pr[B \to A C] \alpha(C, j+1, k) \beta(B, i, k)
\end{align}
}

with the base cases $\alpha(A,i,i) = \Pr[A\to w_i]$ for all $A,i$ and $\beta(\text{Root},1,L)=1$ for all $A$. The marginal probabilities are then given by
\begin{align}
    \mu(A,i,j) \label{eq:marginal_probability} 
    & = \Pr[A\Rightarrow w_iw_{i+1}\dots w_j, \text{Root}\Rightarrow s|\gG] \\
    & = \Pr[\text{Root}\rightarrow w_1w_2\dots w_{i-1} A w_{j+1}\dots w_L|\gG] \cdot \Pr[A\to w_iw_{i+1}\dots w_j|\gG] \nonumber \\
    & = \alpha(A,i,j)\cdot\beta(A,i,j). \nonumber
\end{align}




The performance of parsing is measured by the unlabelled F1 score, which is the F1 score on the prediction of spans. However there are two different F1 scores depending on the average: Sentence F1, which is the average of F1 of each sentence, and Corpus F1, which is computed by the total true positives, false positives, and false negatives.

\subsection{Probing}
%\haoyu{TBD. need to introduce the probing technique.}

A probe $f(\cdot)$ is just a supervised model, where there is some input $\vx$ to the probe $f(\cdot)$ and we want the probe $f(\vx)$ to predict some target $\text{tar}(\vx)$. For example, \citet{hewitt2019structural} used the probe $f(\cdot)$ to predict the tree distance $\text{tar}(i,j) = d_{\gT}(i,j)$ between words, where the tree distance $d_{\gT}(i,j)$ is the number of edges between word $i$ and $j$ in the dependency parse tree $\gT$. The input of $f(\cdot)$ is $\ve_i^{(\ell)} - \ve_j^{(\ell)}$, which is the difference between the embeddings at layer $\ell$ of words at position $i$ and $j$, and
\[f(\ve_i^{(\ell)} - \ve_j^{(\ell)}) = \norm{\mB\left(\ve_i^{(\ell)} - \ve_j^{(\ell)}\right)}^2,\]
is the square of a linear function and $\mB$ is the trainable parameter. Besides the tree distance, one can change the probe to predict other information (i.e. the target), change the input, or even change the function class of the probe $f$.

Despite the mathematical equivalence of a probe and a supervised model (or a parser in the parsing setting), the goals of a probe and a supervised model (or a parser) are different. The goal of a model (or a parser) is to get a high prediction score, while the goal of a probe is \emph{to identify the existence of certain information intrinsically stored in the embeddings}~\citep{maudslay2020tale, chen2021probing}. Thus, we would like to restrict the power of probes such that it is ``sensitive'' to the information we want to probe. For example in the parsing setting, ideally, the probing performance should be low on un-contextualized embeddings (e.g., embeddings at the 0th layer of our models) and high on contextualized ones.



\iffalse
Given an input sentence, the first step is to feed the sentence into the BERT model, and get the contextualized embeddings for each word; the second step is to apply a fixed (over different positions in the sentence and different sentences) linear transformation on the contextualized embeddings and map the contextualized embeddings to a low dimensional subspace, say 40 dimensions; the third step is to compute the minimum spanning tree of the points in the low dimensional subspace, where the distance between points is the Euclidean distance; finally, regarding the minimum spanning tree as the undirected dependency parse tree. This procedure is simple, but it works surprisingly well. The UUAS score of linear probing on BERT is over 80\%, which is a very decent score. 
\fi