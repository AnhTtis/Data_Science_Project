\section{Probing Masked Language Models for Parsing Information}\label{sec:mlmtoparsing}
%\rong{It's not very clear to me what this title is trying to say; maybe something like ``Probing Masked Language Model for Parsing Information''?}

In \cref{sec:construction}, we showed that attention models are expressive enough to execute the Inside-Outside algorithm and the intermediate states of the constructed model contain the syntactic information such as the probability of different labels for every span. However, these results are only existential, and it remains to see whether models trained with masked language modeling loss contain similar information, such as information about the spans in the syntactic parsing tree and the marginal probabilities computed by the Inside-Outside algorithm. %what remains to be answered are: (1) whether the models trained with masked language modeling loss contain syntactic information, and (2) whether the models learn to ``execute'' the algorithm when trained with masked language modeling loss. 

One difficulty in answering this question, as suggested by \citet{maudslay2021syntactic}, is that syntactic probes on BERT-like models may leverage semantic cues to do parsing. To avoid this issue, we pre-train multiple RoBERTa models on synthetic datasets generated from English PCFG (\Cref{sec:pretrain-pcfg}), which eliminates semantic relations among tokens. We then probe the trained model for building parse trees (\Cref{sec:parse}). We consider three settings for probing: train and test the probe on synthetic PCFG data (\dataset{PCFG}); train and test on \dataset{PTB} dataset (\dataset{PTB}); and train on the synthetic PCFG data while test on \dataset{PTB} (out of distribution, OOD). Note that in the OOD setting, semantic relations neither appear in the pre-trained model nor the probe. Hence the decision of the probe must come entirely from syntactic relations. This serves as a baseline for syntactic probe on \dataset{PTB}. To verify if the models indeed capture the information computed by the Inside-Outside algorithm, we further probe for marginal probabilities in the pre-trained models (\Cref{sec:probe-marginal-probs}).
%Interestingly, we find probing patterns that indicate the existence of these span probabilities inside the contextual embeddings.
%, and thus do well on parsing and MLM (\Cref{thm:hard_attnt,thm:soft_attnt,thm:io-optimal-mlm}). The remaining questions are: Do attention models trained using masked language modeling really contain syntactic information? Do these models contain information computed by the Inside-Outside algorithm as shown by \Cref{thm:io-optimal-mlm}?


\subsection{Pre-training on PCFG}\label{sec:pretrain-pcfg}
%\haoyu{maybe pertaining details can be put into the appendix, and only briefly mention it here?}
\paragraph{Experiment setup}  We generate $10^7$ sentences for the training set from the PCFG, with an average length of $25$ words. The training set is roughly $10\%$ in size compared to the training set of the original RoBERTa which was trained on a combination of Wikipedia (2500M words) plus BookCorpus (800M words). We also keep a small validation set of $5 \times 10^4$ sentences generated from the PCFG to track the MLM loss. We follow \cite{izsak-etal-2021-train,wettig2022should} to pre-train all our models within a single day on a cluster of 8 RTX 2080 GPUs. Specifically, we train our models with AdamW \cite{loshchilov2017decoupled} optimization, using $4096$ sequences in a batch and hyperparameters $(\beta_1, \beta_2, \epsilon) = (0.9, 0.98, 10^{-6}).$ We follow a linear warmup schedule for $1380$ training steps with the peak learning rate of $2 \times 10^{-3}$, after which the learning rate drops linearly to $0$ (with the max-possible training step being $2.3 \times 10^{4}$). We report the performance of all our models at step $5 \times 10^{3}$ where the loss seems to converge for all the models.


\paragraph{Architecture} To understand the impact of different components in the encoder model, we pre-train different models by varying the number of attention heads and layers in the model. To understand the role of the number of layers in the model, we start from the RoBERTa-base architecture, which has 12 layers and 12 attention heads, and vary the number of layers to 1,3,6 to obtain $3$ different architectures. Similarily, to understand the role of the number of attention heads in the model, we start from the RoBERTa-base architecture and vary the number of attention heads to 3 and 24 to obtain $2$ different architectures. For simplicity, we use A$i$L$j$ to denote the model with $i$ attention heads in each layer and $j$ layers in this section (\Cref{sec:mlmtoparsing})
%we take the original RoBERTa architecture as a baseline ($12$ layers, $12$ attention heads, and $768$ embedding dimension) and alter one component of the architecture at a time to get newer architectures, i.e. we either change the number of layers, attention heads or embedding size of the model to get newer models while keeping the other components fixed. 



\paragraph{Experiment results} \Cref{tab:pretraining-ppl} shows the training and validation perplexity of different models. From the results, we first find that the trained models have small training and validation perplexity gap, implying that these models don't overfit the training set. We find that except for models with too few layers (A12L1) and too few attention heads (A3L12), all the other models have nearly the same train and test perplexity. Further increasing the depth and the number of attention heads does not seem to enhance the perplexity.

\begin{table}[!t]
    \centering
    \begin{tabular}{|c|c|c|}
        \hline
        Model & Training ppl. & Validation ppl. \\
        \hline
        A12L12 & 106.16 & 106.68 \\
        A12L1 & 111.8 & 110.57 \\
        A12L3 & 108.09 & 105.79 \\
        A12L6 & 105.78 & 104.58 \\
        A3L12 & 120.52 & 117.39 \\
        A24L12 & 106.28 & 104.5 \\
        \hline
    \end{tabular}
    \caption{The perplexity of different RoBERTa models pre-trained on synthetic PCFG data. A$i$L$j$ denotes the model with $i$ attention heads and $j$ layers.}
    \label{tab:pretraining-ppl}
\end{table}



\subsection{Probing for constituency parse trees}\label{sec:parse}

We probe the language models pre-trained on synthetic PCFG data and show that these models indeed capture the ``syntactic information'', in particular, they capture the structure of the constituency parse trees underlying the input sentences.

%In this part, we probe the pre-trained language models trained on PCFG data and show that simple probes can parse decently well, verifying that pre-training on mask language modeling captures the synthetic information.

\paragraph{Experiment setup} We mostly follow the probing procedure in \citet{vilares2020parsing,arps2022probing} for constituency parsing that predicts the relative depth of the common ancestors. Given a sentence $w_1w_2\dots w_L$ with parse tree $T$ (not necessarily binary), we denote $\text{depth}(i,i+1)$ the depth of the least common ancestor of $w_i,w_{i+1}$ in the parse tree $T$, we want to find a probe $f^{(\ell)}$ to predict the relative depth $\text{tar}(i) = \text{depth}(i,i+1) - \text{depth}(i-1,i)$ for position $i$. In \citet{vilares2020parsing}, the probe $f^{(\ell)}$ is linear, and the input to the probe $f^{(\ell)}$ at position $i$ is the concatenation of the embeddings at position $i$ and the BOS (or EOS) token. Besides the linear probe $f^{(\ell)}$, we also experiment with the probe where $f^{(\ell)}$ is a 2-layer neural network with 16 hidden neurons.

As discussed before, we consider three settings: \dataset{PCFG}, \dataset{PTB}, and the OOD setting. In \dataset{PCFG} setting, we train and test the probe $f^{(\ell)}$ on the synthetic PCFG data we generated, and in \dataset{PTB} setting we train on \dataset{PTB} training set and test on \dataset{PTB} development set~\citep{marcus1993building} without removing the punctuations. In OOD setting we train the probe on the synthetic \dataset{PCFG} dataset, but test on the \dataset{PTB} development set, excluding nearly all the semantic information contained in the probe itself.

\begin{figure}[!t]
    \centering
    \includegraphics[width=0.6\textwidth]{figs/probe_parsing.pdf}
    \caption{Comparison between different probes under different settings. The probe is set to be linear or a 2-layer neural net, and the input to the probe is layer 0's embedding from A12L12 or the embeddings from the layer that achieves the highest F1 score.}
    \label{fig:probe-parsing-comparison}
\end{figure}

\begin{table*}[]
    \centering
    \small
    \begin{tabular}{|c|c|c|ccccccc|}
    \hline
         & & & IO & A12L12 & A12L1 & A12L3 & A12L6 & A3L12 & A24L12 \\
    \hline
        \multirow{6}{*}{\rotatebox[origin=c]{90}{Linear}}& \multirow{2}{*}{\begin{turn}{90} \dataset{PCFG} \end{turn}} & Sent. F1 & 81.61 &  \textbf{71.34} & 63.16 & 69.96 & \textbf{71.23} & 64.71 & \textbf{70.76} \\
        & & Corpus F1 & 71.65 & \textbf{63.01} & 54.24 & 61.54 & \textbf{62.57} & 55.36 & \textbf{62.56} \\
        \cline{2-10}
        & \multirow{2}{*}{\begin{turn}{90} \dataset{PTB} \end{turn}} & Sent. F1 & 78.77 &  \textbf{69.31} & 62.99 & 68.22 & 68.13 & 61.56 & \textbf{68.79} \\
        & & Corpus F1 & 75.90 & \textbf{65.01} & 59.96 & \textbf{65.21} & \textbf{65.01} & 58.31 & \textbf{65.97} \\
        \cline{2-10}
        & \multirow{2}{*}{\begin{turn}{90} OOD \end{turn}} & Sent. F1 & 81.61 & \textbf{64.26} & 57.96 & 63.22 & \textbf{63.89} & 58.00 & \textbf{63.88} \\
        & & Corpus F1 & 71.65 & \textbf{60.98} & 54.29 & 59.79 & \textbf{60.58} & 54.39 & \textbf{60.62} \\
        \hline
        \multirow{6}{*}{\rotatebox[origin=c]{90}{2-layer NN}}& \multirow{2}{*}{\begin{turn}{90} \dataset{PCFG} \end{turn}} & Sent. F1 & 81.61 &  \textbf{73.71} & 64.80 & 72.62 & \textbf{73.60} & 62.55 & \textbf{73.27} \\
        & & Corpus F1 & 71.65 & \textbf{66.18} & 57.16 & \textbf{65.36} & \textbf{66.01} & 53.36 & \textbf{65.92} \\
        \cline{2-10}
        & \multirow{2}{*}{\begin{turn}{90} \dataset{PTB} \end{turn}} & Sent. F1 & 78.77 & \textbf{71.32} & 64.89 & 70.15 & \textbf{70.33} & 63.23 & \textbf{70.59} \\
        & & Corpus F1 & 75.90 & \textbf{68.07} & 62.09 & \textbf{67.25} & \textbf{67.31} & 60.59 & \textbf{67.93} \\
        \cline{2-10}
        & \multirow{2}{*}{\begin{turn}{90} OOD \end{turn}} & Sent. F1 & 81.61 & \textbf{66.99} & 59.89 & \textbf{66.21} & \textbf{66.56} & 57.60 & \textbf{67.18} \\
        & & Corpus F1 & 71.65 & \textbf{63.89} & 56.74 & 63.30 & 63.81 & 54.60 & \textbf{64.54} \\
        \hline
    \end{tabular}
    \caption{The parsing results (unlabelled F1 score) for different models under different settings. Linear and 2-layer NN denote the classifier for the probes respectively. Each entry denotes the best F1 score achieved using one of the layer's (contextualized) embeddings. The IO column denotes the results for parsing using the Inside-Outside algorithm. A$i$L$j$ denotes the model with $i$ attention heads and $j$ layers. We highlight the scores that are within 1\% to the max (except the IO) in each row.}
    \label{tab:parsing-results}
\end{table*}

\iffalse
\begin{table*}[]
    \centering
    \scriptsize
    \begin{tabular}{|c|c|c|ccccccc|}
    \hline
         & & & IO & A12L12 & A12L1 & A12L3 & A12L6 & A3L12 & A24L12 \\
    \hline
        \multirow{6}{*}{\rotatebox[origin=c]{90}{Linear}}& \multirow{2}{*}{\begin{turn}{90} \dataset{PCFG} \end{turn}} & Sent. F1 & 81.61 &  30.50 / \textbf{71.34} & 30.69 / 63.16 & 31.04 / 69.96 & 31.01 / \textbf{71.23} & 30.80 / 64.71 & 31.09 / \textbf{70.76} \\
        & & Corpus F1 & 71.65 &  22.27 / \textbf{63.01} &  22.19 / 54.24 & 22.37 / 61.54 & 22.79 / \textbf{62.57} & 22.37 / 55.36 & 22.64 / \textbf{62.56} \\
        \cline{2-10}
        & \multirow{2}{*}{\begin{turn}{90} \dataset{PTB} \end{turn}} & Sent. F1 & 78.77 &  30.54 / \textbf{69.31} & 30.58 / 62.99 & 30.33 / 68.22 & 30.34 / 68.13 & 30.16 / 61.56 & 31.09 / \textbf{68.79} \\
        & & Corpus F1 & 75.90 & 27.01 / \textbf{65.01} &  27.33 / 59.96 & 27.06 / \textbf{65.21} & 27.01 / \textbf{65.01} & 26.94 / 58.31 & 27.70 / \textbf{65.97} \\
        \cline{2-10}
        & \multirow{2}{*}{\begin{turn}{90} OOD \end{turn}} & Sent. F1 & 81.61 & 25.55 / \textbf{64.26} & 25.74 / 57.96 & 25.47 / 63.22 & 25.57 / \textbf{63.89} & 25.33 / 58.00 & 25.44 / \textbf{63.88} \\
        & & Corpus F1 & 71.65 & 21.77 / \textbf{60.98} & 21.91 / 54.29 & 21.69 / 59.79 & 21.73 / \textbf{60.58} & 21.54 / 54.39 & 21.74 / \textbf{60.62} \\
        \hline
        \multirow{6}{*}{\rotatebox[origin=c]{90}{2-layer NN}}& \multirow{2}{*}{\begin{turn}{90} \dataset{PCFG} \end{turn}} & Sent. F1 & 81.61 &  39.06 / \textbf{73.71} & 41.11 / 64.80 & 36.93 / 72.62 & 40.65 / \textbf{73.60} & 32.06 / 62.55 & 40.88 / \textbf{73.27} \\
        & & Corpus F1 & 71.65 & 29.53 / \textbf{66.18}  & 30.89 / 57.16 & 28.06 / \textbf{65.36} & 30.43 / \textbf{66.01} & 24.25 / 53.36 & 30.70 / \textbf{65.92} \\
        \cline{2-10}
        & \multirow{2}{*}{\begin{turn}{90} \dataset{PTB} \end{turn}} & Sent. F1 & 78.77 & 39.31 / \textbf{71.32} & 40.48 / 64.89 & 38.37 / 70.15 & 38.88 / \textbf{70.33} & 38.14 / 63.23 & 38.74 / \textbf{70.59} \\
        & & Corpus F1 & 75.90 & 36.50 / \textbf{68.07} & 37.62 / 62.09 & 35.28 / \textbf{67.25} & 36.08 / \textbf{67.31} & 35.35 / 60.59 & 36.04 / \textbf{67.93} \\
        \cline{2-10}
        & \multirow{2}{*}{\begin{turn}{90} OOD \end{turn}} & Sent. F1 & 81.61 & 33.33 / \textbf{66.99} & 38.19 / 59.89 & 34.00 / \textbf{66.21} & 37.21 / \textbf{66.56} & 33.95 / 57.60 & 29.83 / \textbf{67.18} \\
        & & Corpus F1 & 71.65 & 29.27 / \textbf{63.89} & 33.82 / 56.74 & 30.31 / 63.30 & 32.88 / 63.81 & 29.49 / 54.60 & 26.69 / \textbf{64.54} \\
        \hline
    \end{tabular}
    \caption{The parsing results (unlabelled F1 score) for different models under different settings. Linear and 2-layer NN denote the classifier for the probes respectively. The 2 scores in each entry denote the F1 score using the (un-contextualized) embeddings of the zeroth layer ($\ell = 0$) and the best F1 score achieved using one of the layer's (contextualized) embeddings. The IO column denotes the results for parsing using the Inside-Outside algorithm. A$i$L$j$ denotes the model with $i$ attention heads and $j$ layers. We highlight the scores that are within 1\% to the max in each row.}
    \label{tab:parsing-results}
\end{table*}
\fi


\paragraph{Experiment results} 

From \Cref{fig:probe-parsing-comparison}, we first observe that in all settings there is a huge gap between the probe trained on layer 0's embeddings and the best layer's embeddings. We view the performance on layer 0's embedding as a baseline, and this shows that both probing methods benefit significantly from the representations of later layers. %bes are both sensitive to syntactic information we want to test.

\Cref{tab:parsing-results} shows detailed probing results for different settings (\dataset{PCFG}, \dataset{PTB}, and OOD), different probes (linear or a 2-layer neural net) on different models. 
%\haoyu{change the following numbers accordingly after the experiment results are out}
Except for models with A12L1 and A3L12, the linear and neural net probes give decent parsing scores (> 70\% sentence F1 for neural net probes) in both \dataset{PCFG} and \dataset{PTB} settings. As for the OOD setting, the performances achieved by the best layer drop by about 5\% compared with \dataset{PCFG} and \dataset{PTB}, but they are still much better than the performance achieved by the $0$-th layer embeddings. In this setting, there is no semantic information even in the probe itself and thus gives a baseline for the probes on \dataset{PTB} dataset that only uses syntactic data.
As a comparison, the naive baseline, Right-branching (RB), reaches $<40\%$ for both sentence and corpus F1 score~\citep{li2020empirical} on \dataset{PTB} dataset, and if we use layer 0's embeddings to probe, the sentence F1 is  $<41\%$ in all settings for all models.
Our positive results on syntactic parsing support the claim that pre-training language models using masked language modeling loss can indeed capture the structural information of the underlying constituency parse tree.

Compared with probing results on BERT-like models pre-trained on natural language, whose labeled sentence F1 is 78.2\% for BERT~\citep{vilares2020parsing} and 80.4\% for RoBERTa~\citep{arps2022probing} on \dataset{PTB} dataset, A12L12 model achieves 69.31\% unlabelled F1 for linear probes and 71.32\% for NN probe in the \dataset{PTB} setting. The performance gap with previous literature suggests that BERT-like models trained on natural language indeed contain semantic cues in their embeddings that help to parse.

%, since in the \dataset{PTB} setting, the probes may also leverage the semantic information, and the only difference comes from the embeddings used to probe.

%As for the OOD setting, the performances achieved by the best layer drop by about 5\% compared with \dataset{PCFG} and \dataset{PTB}, but they are still much better than the performance achieved by the $0$-th layer embeddings. In this setting, there is no semantic information even in the probe itself, and thus gives a baseline for the probes on \dataset{PTB} dataset that only uses syntactic data. Our positive results on syntactic parsing support the claim that pretraining language models on masked language models can indeed capture syntactic information.

%\haoyu{currently I think the following discussion can be put into the appendix. then in the appendix, i will show more plots and results on the probes with 3 adjacent tokens as input.}

%\haoyu{we may discuss some potential problems for probes here, e.g., one can get high F1 by getting correct on very short spans.}

%\haoyu{third exp: TBA. compare the parsing results using the embeddings from different layers}


\subsection{Probing for the marginal probabilities}\label{sec:probe-marginal-probs}
%In the previous part, we show that pre-training using MLM can capture syntactic data. In this part, we want to understand more about how pre-training on MLM can capture syntactic information. We probe the pre-trained language models trained on PCFG data and show that the models also contain syntactic information like the marginal probabilities computed by the Inside-Outside algorithm, thus empirically verifying \Cref{thm:io-optimal-mlm} that pre-training on MLM may implicitly execute some approximated version of Inside-Outside algorithm, and thus can do well on parsing task.

\Cref{sec:parse} verifies that language models can capture structure information of the constituency parse trees, but we still don't understand how this information is represented. \Cref{sec:construct-io,sec:mlmandio} suggest that a possible mechanism for transformers to capture the syntactic information is to execute the Inside-Outside algorithm. In this subsection, we test if the intermediate-layer representations of the transformer can be used to predict marginal probabilities computed in the Inside-Outside algorithm. 

\paragraph{Experiment setup} To test if the model contains information computed by the Inside-Outside algorithm, we train a probe to predict the normalized marginal probabilities for spans with a specific length. Fix the span length $\ell$, for each sentence $w_1w_2\dots w_L$, denote $\ve_1, \ve_2,\dots,\ve_L$ the embeddings from the last layer of the pre-trained language model. We want to find a probe $f^{(\ell)}$ such that for each span $[i,i+\ell-1]$ with length $\ell$, the probe $f^{(\ell)}([\ve_i;\ve_{i+\ell-1}])$ predicts the normalized marginal probability of span $[i,i+\ell-1]$. Formally,
\[\text{tar}(i,i+\ell-1) = s(i,i+\ell-1) / \max_{j,j'}s(j,j'),\]
where $s(i,j) = \max_A \mu(A,i,j)$ is the marginal probability of span $[i,j]$ and $\mu(A,i,j)$ is given by eq.~\ref{eq:marginal_probability}.
Here, the input to the probe $[\ve_i;\ve_{i+\ell-1}]\in\R^{2d}$ is the concatenation of $\ve_i$ and $\ve_{i+\ell-1}$. To test the sensitivity of our probe, we also take the embeddings from the $0^{th}$ layer as input to the probe $f^{(\ell)}$.

We give two choices for the probe $f^{(\ell)}$: (1) linear, and (2) a 2-layer neural network with 16 hidden neurons, since the relation between the embeddings and the target may not be a simple linear function. Similar to the \Cref{sec:parse}, we also consider three settings: \dataset{PCFG}, \dataset{PTB}, and OOD.
%(1) \dataset{PCFG}, where we train and test $f^{(\ell)}$ on the synthetic \dataset{PCFG} dataset, (2) \dataset{PTB}, where we train and test $f^{(\ell)}$ on the \dataset{PTB} dataset, and (3) OOD, where we train $f^{(\ell)}$ on the synthetic \dataset{PCFG} dataset and test on the \dataset{PTB} dataset.

\paragraph{Experiment results} 

\begin{figure}
\begin{subfigure}[t]{0.49\textwidth}
    \centering
    \includegraphics[width=0.9\linewidth]{figs/probe_prob_comparison.pdf}
    \caption{We compare 4 probes under \dataset{PTB} setting: the input comes from the $0$-th layer or the $12$-th layer, and the probe is linear or NN.}
    \label{fig:probe-prob-comparison}
\end{subfigure}
\hfill
\begin{subfigure}[t]{0.49\textwidth}
    \centering
    \includegraphics[width=0.9\linewidth]{figs/probe_prob_setting_comparison.pdf}
    \caption{We compare the NN probe using the $12$-th layer embeddings from the A12L12 model under different settings.}
    \label{fig:probe-prob-setting-comparison}
\end{subfigure}
\caption{Comparison between different probes for marginal probabilities under different settings on the pre-trained model with 12 attention heads and 12 layers. The y-axis denotes the correlation between the probe output and the target, and the x-axis denotes probes for different lengths.}
\end{figure}

\Cref{fig:probe-prob-comparison} compares the 4 different probes for models with 12 attention heads and 12 layers: the input comes from layer 0 ( which is un-contextualized) and layer 12 (which is contextualized), and the probe is linear or is a 2-layer neural network. We observe that for both linear and neural net, changing the input from layer 0 to layer 12 drastically increases the predicted correlation, which again suggests that using just the token embeddings (the word embeddings and the positional embeddings) does not contain enough information about the marginal probabilities. % showing that the probes we design are sensitive to the information we want to test.
Besides, the neural net can predict better on layer 12 embeddings, but performs nearly the same on layer 0, suggesting that the neural network is a better probe in this setting. % (more sensitive to the information).

\Cref{fig:probe-prob-setting-comparison} compares the probing results under three different settings. We observe that the probe can get a high correlation with the real marginal probabilities under all settings. Besides, it is surprising that there is nearly no performance drop if we change the testing dataset from \dataset{PCFG} to \dataset{PTB} (\dataset{PCFG} setting and OOD setting), implying that the probe (together with the embeddings) really contains the syntactic information computed by Inside-Outside algorithm instead of overfitting to the training dataset itself.

\Cref{tab:probe-probs-ptb} shows the probing results on different pre-trained models. We can observe that the neural network probe is highly correlated with the target for pre-trained models except for models with 12 heads 1 layer and 3 heads 12 layers. When the length of spans increases, the probing correlation becomes worse, which means that the syntactic information for longer ranges is harder to be captured by the pre-trained language model. However surprisingly, even for length $10$ spans, the NN probe can reach 78\% F1 for the best model. The high correlation between probes and the target gives strong hint that the pre-trained models contain certain syntactic information computed by the Inside-Outside algorithm. All the results in this experiment show that training on MLM may incentivize the model to do some approximation of the Inside-Outside algorithm, verifying our constructions in \Cref{sec:construction}.

\iffalse
\begin{table*}
\begin{subtable}[h]{\textwidth}
    \centering
    \scriptsize
    \begin{tabular}{|c|c|c|c|c|c|c|}
    \hline
    \makecell{Length\\ of span} & \dataset{PTB} & \makecell{\dataset{PTB}\\ sent length $\le 10$} & \makecell{\dataset{PTB}\\ $11 \le$ sent length $\le 20$} & \makecell{\dataset{PTB}\\ $21 \le$ sent length $\le 30$} & \makecell{\dataset{PTB}\\ $31 \le$ sent length $\le 40$} \\
    \hline 
    2 & .86 / .88 / .87 & .88 / .88 / .88 & .87 / .88 / .87 & .86 / .88 / .87 & .87 / .89 / .87 \\
    3 & .77 / .79 / .79 & .84 / .83 / .85 & .78 / .79 / .79 & .78 / .79 / .80 & .78 / .79 / .80 \\
    4 & .66 / .69 / .69 & .77 / .76 / .75 & .68 / .70 / .70 & .67 / .69 / .69 & .67 / .70 / .71 \\
    5 & .58 / .62 / .62 & .77 / .77 / .74 & .60 / .64 / .63 & .58 / .62 / .61 & .60 / .63 / .63 \\
    \hline
    \end{tabular}
    \caption{Linear probing for the ``normalized'' marginal probabilities at different lengths. The three numbers in each entry denote the results of models with 3, 12, and 24 attention heads respectively. All models have 12 layers and 768 embedding dimensions.}
    \label{tab:lp-probs-ptb-attn}
\end{subtable}

\begin{subtable}[h]{\textwidth}
    \centering
    \scriptsize
    \begin{tabular}{|c|c|c|c|c|c|c|}
    \hline
    \makecell{Length\\ of span} & \dataset{PTB} & \makecell{\dataset{PTB}\\ sent length $\le 10$} & \makecell{\dataset{PTB}\\ $11 \le$ sent length $\le 20$} & \makecell{\dataset{PTB}\\ $21 \le$ sent length $\le 30$} & \makecell{\dataset{PTB}\\ $31 \le$ sent length $\le 40$} \\
    \hline 
    2 & .83 / .88 / .88 / .88 & .84 / .89 / .88 / .88 & .84 / .89 / .89 / .88 & .82 / .88 / .88 / .88 & .83 / .89 / .88 / .89 \\
    3 & .74 / .80 / .79 / .79 & .83 / .84 / .83 / .83 & .75 / .80 / .79 / .79 & .75 / .80 / .79 / .79 & .75 / .81 / .79 / .79 \\
    4 & .65 / .69 / .69 / .69 & .73 / .76 / .77 / .76 & .67 / .70 / .70 / .70 & .65 / .69 / .70 / .69 & .65 / .71 / .70 / .70 \\
    5 & .57 / .62 / .61 / .62 & .76 / .76 / .75 / .77 & .60 / .63 / .63 / .64 & .58 / .61 / .62 / .62 & .58 / .63 / .62 / .63 \\
    \hline
    \end{tabular}
    \caption{Linear probing for the ``normalized'' marginal probabilities at different lengths. The four numbers in each entry denote the results of models with 1, 3, 6, and 12 layers respectively. All models have 12 attention heads and 768 embedding dimensions.}
    \label{tab:lp-probs-ptb-layer}
\end{subtable}

\begin{subtable}[h]{\textwidth}
    \centering
    \scriptsize
    \begin{tabular}{|c|c|c|c|c|c|c|}
    \hline
    \makecell{Length\\ of span} & \dataset{PTB} & \makecell{\dataset{PTB}\\ sent length $\le 10$} & \makecell{\dataset{PTB}\\ $11 \le$ sent length $\le 20$} & \makecell{\dataset{PTB}\\ $21 \le$ sent length $\le 30$} & \makecell{\dataset{PTB}\\ $31 \le$ sent length $\le 40$} \\
    \hline 
    2 & .88 / .93 / .92 & .74 / .88 / .88 & .88 / .94 / .93 & .90 / .93 / .93 & .89 / .93 / .93 \\
    3 & .84 / .90 / .89 & .49 / .84 / .86 & .82 / .90 / .90 & .86 / .90 / .90 & .86 / .91 / .90 \\
    4 & .78 / .86 / .85 & .09 / .81 / .83 & .79 / .85 / .82 & .82 / .85 / .85 & .74 / .88 / .86 \\
    5 & .69 / .79 / .79 & .10 / .76 / .89 & .67 / .84 / .80 & .66 / .80 / .77 & .74 / .83 / .78 \\
    \hline
    \end{tabular}
    \caption{Probing for the ``normalized'' marginal probabilities at different lengths with a 2-layer neural net. The three numbers in each entry denote the results of models with 3, 12, and 24 attention heads respectively. All models have 12 layers and 768 embedding dimensions.}
    \label{tab:nnp-probs-ptb-attn}
\end{subtable}

\begin{subtable}[h]{\textwidth}
    \centering
    \scriptsize
    \begin{tabular}{|c|c|c|c|c|c|c|}
    \hline
    \makecell{Length\\ of span} & \dataset{PTB} & \makecell{\dataset{PTB}\\ sent length $\le 10$} & \makecell{\dataset{PTB}\\ $11 \le$ sent length $\le 20$} & \makecell{\dataset{PTB}\\ $21 \le$ sent length $\le 30$} & \makecell{\dataset{PTB}\\ $31 \le$ sent length $\le 40$} \\
    \hline 
    2 & .88 / .91 / .92 / .93 & .89 / .91 / .90 / .88 & .89 / .93 / .93 / .94 & .88 / .91 / .93 / .93 & .89 / .92 / .93 / .93 \\
    3 & .84 / .88 / .89 / .90 & .86 / .88 / .84 / .84 & .84 / .85 / .90 / .90 & .85 / .90 / .90 / .90 & .86 / .88 / .90 / .91 \\
    4 & .77 / .82 / .84 / .86 & .80 / .90 / .84 / .81 & .80 / .86 / .87 / .85 & .80 / .84 / .84 / .85 & .78 / .84 / .84 / .88 \\
    5 & .70 / .77 / .81 / .79 & .87 / .88 / .80 / .76 & .74 / .83 / .83 / .84 & .73 / .80 / .81 / .80 & .73 / .79 / .79 / .83 \\
    \hline
    \end{tabular}
    \caption{Probing for the ``normalized'' marginal probabilities at different lengths with a 2-layer neural net. The four numbers in each entry denote the results of models with 1, 3, 6, and 12 layers respectively. All models have 12 attention heads and 768 embedding dimensions.}
    \label{tab:nnp-probs-ptb-layer}
\end{subtable}
\caption{Probing for the ``normalized'' marginal probabilities at different lengths with a linear model or a 2-layer neural net. We show the Pearson correlation between the predicted probabilities and the probabilities computed by the Inside-Outside algorithm on \dataset{PTB} datasets and its subsets partitioned by different lengths of sentences.}
\label{tab:probe-probs-ptb}
\end{table*}
\fi

\begin{table*}[]
    \centering
    \begin{tabular}{|c|cccccc|}
    \hline
         \makecell{Span \\Length} & A12L12 & A12L1 & A12L3 & A12L6 & A3L12 & A24L12 \\
    \hline
        $\ell = 2$ &  .88 / \textbf{.93} & .83 / .88 &  .88 / .91  &  .88 / \textbf{.92}  &  .86 / .88 & .87 / \textbf{.92} \\
        $\ell = 3$ &  .79 / \textbf{.90} & .74 / .84 &  .80 / .88  &  .79 / \textbf{.89}  &  .77 / .84 & .79 / \textbf{.89}  \\
        $\ell = 4$ &  .69 / \textbf{.86} & .65 / .77 &  .69 / .82  &  .69 / .84  &  .66 / .78 & .69 / \textbf{.85}  \\
        $\ell = 5$ &  .62 / .79 & .57 / .70 &  .62 / .77   &  .61 / \textbf{.81} &  .58 / .69 & .62 / .79  \\
        $\ell = 10$ & .51 / \textbf{.77} & .48 / .68 & .51 / .75 & .51 / \textbf{.78} & .51 / .61 & .51 / .73 \\
        \hline
    \end{tabular}
    \caption{Probing for the ``normalized'' marginal probabilities at different lengths with a linear model or a 2-layer neural net. We show the Pearson correlation between the predicted probabilities and the probabilities computed by the Inside-Outside algorithm on \dataset{PTB} datasets. Two numbers in each entry denote the correlations gotten by a linear probe and a 2-layer neural net probe respectively whose inputs come from the final layer of the model. A$i$L$j$ denotes the model with $i$ attention heads and $j$ layers.}
    \label{tab:probe-probs-ptb}
\end{table*}

