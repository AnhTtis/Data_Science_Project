\section{Related Works}

\paragraph{Probing for transformers}
There has been an emerging interest in understanding the information that BERT-like models encode implicitly in their embeddings~\citep{rogers2020primer}, e.g. the syntactic (structural) information~\citep{hewitt2019structural,reif2019visualizing,manning2020emergent,vilares2020parsing,maudslay2020tale,maudslay2021syntactic,chen2021probing,arps2022probing}. However as mentioned in \citet{maudslay2021syntactic}, the probing success of the previous works on syntax emerged from the model using semantic cues to parse. The probed pre-trained models had been pre-trained on natural language datasets, where the semantic structures are most likely correlated with the syntactic ones. Indeed, \citet{arps2022probing} tried to separate the semantics from syntax by training the probe on a mixture of natural language and manipulated data, however, the authors acknowledged the possibility of semantics still affecting the decision of the probe
 trained on manipulated data. Besides syntax, researchers have also performed probing experiments for other linguistic structures like semantics, sentiment, etc.~\citep{belinkov2017neural,reif2019visualizing,kim2020pre,richardson2020probing}.

\paragraph{Expressive power of transformers}
 \citet{Yun2020Are,yun2020n} showed that transformers are universal sequence-to-sequence function approximators, while \citet{perez2021attention,bhattamishra2020computational} showed that attention models can simulate Turing machines. Attention models with bounded size have been shown capable of recognizing deterministic context-free languages such as bounded-depth Dyke-k~\citep{yao2021self}. The size of the constructed models, however, depends on the complexity of the target function and often requires arbitrary precision to encode the target function. \citet{wei2022statistically} proposed statistically meaningful approximations of Turing machines using attention models, that also exhibit good statistical learnability. \citet{liu2022transformers} constructed (by hand) transformers that can efficiently simulate  automata for inputs of a small range of lengths. 
 %and empirically showed the existence of such solutions in pre-trained transformers. 
 %However, their short-cut solution is not robust to OOD generalization, while our probes are all OOD generalizable since we pre-train on synthetic PCFG data while probing on the \dataset{PTB} dataset.
 %However, as evident from the name, shortcut solutions aren't robust to OOD generalization.
 Interestingly, we observe that language models pre-trained on PCFG-generated data encode relevant information from the Inside-Outside algorithm for sentences from both natural language and PCFG-generated data, which suggests an implicit bias toward learning the general algorithm (generalizing to all input lengths). 
 %This may suggest that pre-training on purely synthetic data and data closer to natural language have different regimes. Besides, these results do not shed light on the expressivity of transformers needed to encode relevant syntactic and semantic information in languages. 
 A careful study is left for future work.
 

 


%\paragraph{Grammar induction}