

%\iffalse
Pre-trained language models have been shown to encode linguistic structures, e.g. dependency and constituency parse trees, in their embeddings while being trained on unsupervised loss functions like masked language modeling. Some doubts have been raised whether the models actually are doing parsing or only some computation weakly correlated with it. We study questions: (a) Is it possible to explicitly describe transformers with realistic embedding dimension, number of heads, etc. that are {\em capable} of doing parsing ---or even approximate parsing? (b) Why do  pre-trained models capture parsing structure? 
This paper takes a step toward answering these questions in the context of generative modeling with PCFGs. 
%Under the assumption of English language generation under a PCFG, 
We show that masked language models like BERT or RoBERTa of moderate sizes can %encode 
approximately execute the Inside-Outside algorithm for the English PCFG~\citep{marcus1993building}.
We also show that the Inside-Outside algorithm is optimal for masked language modeling loss on the PCFG-generated data. We also give a construction of transformers with $50$ layers, $15$ attention heads, and $1275$ dimensional embeddings in average 
%that can achieve a 1-mask perplexity of $50$ on sentences of length at most $25$ in the PTB dataset. 
%Furthermore,
such that 
%probing 
using its embeddings it is possible to do constituency parsing with $>70\%$ F1 score  %on the same dataset.
on PTB dataset. We conduct probing experiments on models pre-trained on PCFG-generated data to show that this not only allows recovery of approximate parse tree, but also recovers marginal span probabilities computed by the Inside-Outside algorithm, which suggests an implicit bias of masked language modeling towards this algorithm.
%Inside-Outside algorithm.  


%\fi

\iffalse
Pre-trained language models have been shown to encode linguistic structures in their embeddings while being trained on unsupervised loss functions like masked language modeling. This raises two questions: (a) how much does the model architecture and size affect its ability to pick the linguistic structures during pre-training, and (b) why do these models capture the linguistic structure while  trained with unsupervised pre-training losses? We answer these questions in the context of generative modeling with PCFGs. We show that masked language models like BERT and RoBERTa with moderate sizes can encode the Inside-Outside algorithm for English PCFG \cite{marcus1993building}, which is also the optimal algorithm for masked language modeling on the PCFG-generated data. Probing experiments on models pre-trained on PCFG-generated data show that the contextual embeddings encode the Inside-Outside probabilities, which can then be extracted using linear probes to perform syntax parsing.  
\fi
%How much does the architecture and model size matter 



%A popular method to show the existence of such structures in the embeddings is lin