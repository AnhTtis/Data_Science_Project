\section{Introduction}
%\haoyu{need to change the story, since there is a NAACL paper says that BERT/GPT-2 may use semantic to parse, and question if BERT/GPT-2 really contains synthetic data (proposed that when making those claims, need to separate the synthetic and semantic)}
One of the surprising discoveries about transformer-based language models like BERT~\citep{devlin2019bert} and RoBERTa~\citep{liu2019roberta} was that contextual word embeddings encode information about parsing, which can be extracted using a simple ``linear probing''  to yield approximately correct  dependency parse trees for the text~\citep{hewitt2019structural,manning2020emergent}. %Later \citet{hewitt2019structural} gave a very simple procedure, called , to extract the dependency parsing information using pre-trained BERT. 
Subsequently, \citet{vilares2020parsing,wu2020perturbed,arps2022probing} used linear probing also to recover information about constituency parse trees. The current paper focuses on the ability of BERT-style transformers to do constituency parsing, specifically for Probabilistic Context-Free Grammars (PCFGs). As noted in~\citep{bhattamishra2020computational,perez2021attention}, transformers are Turing complete and thus certainly capable of parsing. But do they parse while trying to do masked-word prediction?  
%from BERT-style models. 
%All these works show that these pre-trained language models contain information that can recover the syntactic parse trees. 


One reason to be cautiously skeptical is that naive translation of constituency parsing algorithms into a transformer results in  transformers with a number of heads that scales with the size of the grammar (\Cref{sec:construct-io}), whereas BERT-like models have around a dozen heads. This leads to the following question.  %are much smaller compared to key parameters of commonly used grammars. Our first question considers the capability of BERT-like models in parsing:%thus certainly capable of parsing sentences with a model whose size scales with the sentence size and grammar, we still don't know why BERT-size model has the capacity to ``parse''. Thus we have the following question,


\begin{quote}
    %{\em Are BERT-like models actually doing parsing or just something that correlates with parsing?}
    %{\em How do the size and architecture of BERT-like models relate with their capacity to do parsing?}
    \centering
    {\em (Qs 1): Are BERT-like models capable of parsing with a realistic number of heads?}%with their capacity to do parsing?}
\end{quote}
%Later in this paper we will show that moderately sized attention networks can already perform approximate inference on complicated PCFG grammars.

This is not an idle question: recently \citet{maudslay2021syntactic} suggested that the linear probing was using semantic cues to do parsing. They constructed  semantically meaningless but syntactically correct sentences and observed a large drop in parsing performance via linear probes compared to earlier papers.  %Thus, the second question is
\begin{quote}
\centering
    {\em (Qs 2): Do BERT-like models trained for masked language modeling (MLM) encode syntax,\\ and if so, how and why?}
\end{quote}

\subsection{This paper}
To understand Qs 1, we first construct an attention model that executes the Inside-outside algorithm for probabilistic context-free grammar (PCFG) (\Cref{sec:construct-io}). If the PCFG has $N$ non-terminals and the length of the sentence is $L$, our constructed attention model has $2L$ layers in total, $N$ attention heads in each layer, and $2NL$ embedding dimensions in each layer. However, this is massive compared to BERT.
For PCFG learned on Penn Treebank (\dataset{PTB})~\citep{marcus1993building}, $N=1600$, average $L \approx 25$, which leads to an attention model with $80$k embedding dimension, depth $50$, and $1.6$k attention heads per layer. By contrast, BERT has $768$ embedding dimensions, $12$ layers, and $12$ attention heads per layer! 

One potential explanation could be that BERT does not do exact parsing; merely that it computes {\em some} of the information relevant to parsing. After all, linear probing didn't recover complete parse trees. It recovered trees with modest F1 score, such as $78.2\%$ for BERT~\citep{vilares2020parsing} and $82.6\%$ for RoBERTa~\citep{arps2022probing}. %as compared to the optimal parse tree.
To the best of our knowledge, there has been no study of parsing methods that strategically discard information to do approximate parsing in a more resource-efficient manner. Toward this goal, we design an approximate version of the  Inside-Outside algorithm (\Cref{sec:approx-overview}), which is executable by an attention model with $2L$ layers, $15$ attention heads, and $40L$ embedding dimensions, while still achieving $>70\%$ F1 score for constituency parsing on \dataset{PTB} dataset~\citep{marcus1993building}. 
\iffalse
\haoyu{I feel like the following contains too many details for the intro} After getting the inside and outside probabilities, we can apply the Labelled Recall algorithm to get the parse trees and reach a decent constituency parsing score. Besides using the Labelled Recall algorithm to get the parse tree from the inside and outside probabilities, we also show that a simple greedy selection of the spans can also get a rough parse tree, which can be implemented by a simple head and illustrate more on why linear probing works.
\fi

The above construction shows that realistic architectures are capable of capturing a fair bit of parsing information. But this still begs the question of whether or not they need to do so for masked language modeling. After all, \citet{maudslay2021syntactic} suggested that  linear probing of MLMs is picking up on semantic information that simply happens to correlate with parse trees.
To better understand this, we tried to take semantics out of the picture as follows: Generate synthetic text according to a PCFG that was fitted to English text; then train a (masked) language model on the synthetic text.
This is a more rigorous way to focus  on separating syntax from semantics than the more {\em ad hoc}  method of \citet{maudslay2021syntactic}.
%By modifying our earlier construction, 
 \Cref{sec:mlmandio} notes that given such synthetic text, the Inside-Outside algorithm  will minimize MLM loss. 
 %{\sc do we know that some other parsing methods would not minimize MLM loss, or at least not be easily doable with attention at least as far as we know? if so note it here. Something like: "Note that parsing could in principle  be done by other algorithms, but.." }   
 Note that parsing could in principle be done by other algorithms like CYK~\citep{kasami1966efficient}, but there is no explicit connection between CYK and the masked language modeling.
 Our experiments show that the language models pre-trained from the synthetic data still contain  syntactic information --- simple probing methods recover reasonable parse tree structure (\Cref{sec:parse}), though interestingly, the quality is better when probing using a 2-layer net instead of linear probes. More interestingly, probes of contextualized embeddings show (\Cref{sec:probe-marginal-probs}) that they  contain information correlated with the information computed in the Inside-Outside algorithm.  This %is aligned with our construction and suggests that% which supports our construction that 
suggests that attention models do indeed implicitly do some form of approximate parsing ---in particular a process related to the Inside-Outside algorithm---  to achieve low MLM loss.
%may implicitly use some mechanisms correlated to the Inside-Outside algorithm.

\iffalse
{\sc guys, don't duplicate information; just put links to the sections in the paras above}

\rong{The paragraphs below seems a bit repetitive.}
Our contribution can be summarized as follow:
\begin{enumerate}
    \item Given any PCFG with $N$ non-terminals, we can construct an attention model with $2L$ layers in total, $N$ attention heads in each layer, and $2NL$ embedding dimensions such that for any sentence with length at most $L$ generated by the PCFG, the attention model can execute the Inside-outside algorithm (\Cref{sec:construction}). We show that Inside-outside algorithm is optimal for masked language modeling on synthetic PCFG data (\Cref{sec:mlmandio}), and conjecture that pre-training using masked language modeling has the implicit bias to embed the Inside-outside algorithms or the syntactic information. For the PCFG learned from \dataset{PTB} dataset, we can compress the attention model to $15$ attention heads in each layer and $40L$ dimensional embeddings without losing much on the parsing performance (\Cref{sec:approx-overview}).
    \item We also pre-train the RoBERTa model on the synthetic PCFG data (\Cref{sec:pretrain-pcfg}) and empirically show that probing on the model pre-trained on synthetic PCFG data can still recover the constituency parse tree decently well (\Cref{sec:parse}), verifying that pre-training using MLM indeed contains structural information. Furthermore, we probe the pre-trained model and find that the model contains the information computed by the Inside-outside algorithm, suggesting that the algorithm used by the trained model has a correlation with the Inside-outside algorithm (\Cref{sec:probe-marginal-probs}).
\end{enumerate}
\fi

\iffalse
Our paper is organized as follows: in Section \ref{}, we give the background knowledge and preliminaries; in Section \ref{}, we show that there exists an attention model that can execute the Inside-outside algorithm given the PCFG parameters, and thus can parse consequently; in Section \ref{}, we apply different approximation techniques on the execution of Inside-outside algorithms on PCFG learn from English corpus, and drastically reduce the size of constructed attention model showed in Section \ref{}; finally in Section \ref{}, we bridge the parsing with the pre-training from the view of PCFG. We use linear probing to show that the language models pre-trained from PCFG data indeed contain more syntactic information belonged the mask predictions. 
\fi