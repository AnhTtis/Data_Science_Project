\section{Parsing using Attention Models}\label{sec:construction}
In this section, we construct attention models with a moderate number of layers and heads that can perform parsing tasks and optimize masked language model losses. In \Cref{sec:construct-io}, we show that given any PCFG, there exist attention models that can execute the Inside-Outside algorithm using this PCFG for bounded-length sentences. Then in \Cref{sec:mlmandio}, we connect our construction with masked language modeling by showing Inside-Outside algorithm is optimal for masked language modeling on data generated from the PCFG model. Finally, in Section~\ref{sec:approx-overview}, we show that the size of these constructions can be made even smaller while maintaining most of their performances in parsing.

\subsection{Attention model can execute Inside-Outside algorithm}\label{sec:construct-io}
%\haoyu{this part will also simplify a lot, e.g. only keep theorems (or even informal) and a brief overview of the construction}
In this section, we give several constructions that show even moderate-sized attention models % state our theoretical results that show that attention models 
are expressive enough to represent the Inside-Outside algorithm. 

We first give a construction (Theorem~\ref{thm:hard_attnt}) that relies on {\em hard attention}, where only one of the attended positions will have positive attention score. For this construction,
%In the first theorem, 
we define $f_{\text{attn}}: \mathbb{R}^{L \times d} \times \mathbb{R}^{d}$ such that the attention scores in eq.~\ref{def:attention} are given by
\begin{align}
    a^h_{i, j} = \text{ReLU} (  (\mK_h \ve_j^{(\ell)})^{\top} \mQ_h \ve_i^{(\ell)} ). \label{eq:hard_attention} 
\end{align}
This is similar to softmax attention used in practice, with softmax replaced by $\text{ReLU}$ activation. %If only a single score in $\{a^h_{i, j}\}_{j \in [L]}$ is positive, we get an attention module that maintains the attention score only at the highest attended position. This is the intended application for $f_{\text{attn}}$ and hence, we call it as hard attention.

%given a set of scores $\{s_1, s_2, \cdots, s_L\}$, the hard attention module will return $\max(s_1, s_2, \cdots, s_L)$ at index $\arg\max(s_1, s_2, \cdots, s_L)$ and $0$ elsewhere. For our construction, an attention head that takes scores $\{s_1, \cdots, s_L\}$ and returns $\{ReLU(s_1), \cdots, ReLU(s_L)\}$ suffices.
%We assume that our sentences are of length at most $L$.

\begin{theorem}[Hard attention]\label{thm:hard_attnt}
    Given a PCFG $\gG = (\gN, \gI, \gP, n, p)$, there exists an attention model with hard attention modules (\ref{eq:hard_attention}), embeddings of size $(4|\gN| + 1)L$, $2L-1$ layers, and $4|\gN|$ attention heads in each layer, that can simulate the Inside-Outside algorithm on all sentences of length at most $L$ generated from $\gG$, and embed all the inside and outside probabilities. 
\end{theorem}

%\haoyu{AP and I both feel like we may only keep the very high-level idea here, and the proof will go to the appendix.}

\begin{proof} 
The first $L-1$ layers simulate the recursive formulation of the Inside probabilities from eq.~\ref{eq:inside_probability}, and the last $L-1$ layers simulate the recursive formulation of the outside probabilities from  eq.~\ref{eq:outside_probability}. The model uses embeddings of size $4|\gN| L + L$, where the last $L$ coordinates serve as one-hot positional embeddings and are kept unchanged throughout the model. 

\paragraph{Notations:} For typographical simplicity, we will divide our embeddings into 5 sub-parts. We will use the first $2 |\gN| L$ coordinates to store the inside probabilities, the second $2 |\gN| L$ coordinates to store the outside probabilities, and the final $L$ coordinates to store the one-hot positional encodings. For every position $i$ and span length $\ell+1$, we store the inside probabilities $\{\alpha(A, i, i+\ell)\}_{A \in \gN}$ after computation in its embedding at coordinates $[|\gN|\ell, |\gN|(\ell+1))$. Similarly
we store $\{\alpha(A, i-\ell, i)\}_{A \in \gN}$ at $[|\gN|(L+\ell), |\gN|(L+\ell+1))$, $\{\beta(A, i, i+\ell)\}_{A \in \gN}$ at $[|\gN|(2L+\ell), |\gN|(2L+\ell+1))$, and $\{\beta(A, i-\ell, i)\}_{A \in \gN}$ at $[|\gN|(3L+\ell), |\gN|(3L+\ell+1))$ respectively. For simplicity of presentation, we won't handle cases where $i+\ell$ or $i-\ell$ is outside the range of $1$ to $L$ - those coordinates will be fixed to 0. % we cross the input boundaries in the computations. We presume that all such computations are set as $0$.

%Wherever applicable, for every $A \in \gN$ and $k \le L$, we will use $(k, A), (k, A) + |\gN| L, (k, A) + 2|\gN| L, (k, A) + 3|\gN| L$ to denote the coordinates that store $\alpha(A, i, i + k - 1), \alpha(A, i - k + 1, i), \beta(A, i, i + k - 1),$ and $\beta(A, i - k + 1, i)$ respectively. 



\paragraph{Token Embeddings:} The initial embeddings for each token $w$ will contain $\Pr[A \rightarrow w]$ for all $A \in \gP$. This is to initiate the inside probabilities of all spans of length $1$. 
Furthermore, the tokens will have a one-hot encoding of their positions in the input in the last $L$ coordinates. 

%For token at position $1$, $(L, \text{ROOT}) + 2|\gN| L$ coordinate will store $1$, and for token at position $L$, $(L, \text{ROOT}) + 3|\gN| L$ coordinate will store $1$, to initiate $\beta( \text{Root}, 1, L ) = 1$.
%The rest of the coordinates, except the position coordinates, are set as $0$.


\paragraph{Inside probabilities:} The contextual embeddings at position $i$ after the computations of any layer $\ell < L$ contains the inside probabilities of all spans of length at most $\ell + 1$ starting and ending at position $i$, i.e.  $\alpha(A, i, i + k)$ and $\alpha(A, i-k, i)$ for all $A \in \gN$ and $k \le \ell$. The rest of the coordinates, except the position coordinates, contain $0$.




%Generally speaking, the first $|\gN| L$ coordinates record the inside probabilities for spans starting at position $i$, and $(|\gN| L+1)$-th to $2|\gN| L$-th record the inside probabilities for spans ending at position $i$. For layer $\ell \le L$, $(2|\gN| L+1)$-th to $4|\gN| L$-th are $0$. 




%As for the attention score between $(\ve_i^{(\ell)}$ and $\ve_j^{(\ell)}$, we make it
%\begin{align*}
%    & \text{Attn}_{\mK_e^{(\ell)}, \mQ_e^{(\ell)}, \mK_p^{(\ell)}, \mQ_p^{(\ell)}}(i,j) \\
%    =&  (\vv_i^{(\ell)})^\top (\mK_e^{(\ell)})^\top \mQ_e^{(\ell)} \vv_j^{(\ell)} \\
%    & \quad + \vp_i^\top (\mK_p^{(\ell)})^\top \mQ_p^{(\ell)} \vp_j \\
%    =&  (\vv_i^{(\ell)})^\top (\mK_e^{(\ell)})^\top \mQ_e^{(\ell)} \vv_j^{(\ell)} + |p_i^{(\ell)} - p_j^{(\ell)}|.
%\end{align*}

%\paragraph{First layer}
%At the first layer $\ell = 1$, for the embedding at $i$-th position, the coordinate $(1,A)$ and $(1,A)+|\gN|L$ denote the inside probability $\alpha(A,i,i)$, which can be initialized from the word embedding matrix. All other coordinates except the positional embeddings are set to $0$.

\paragraph{Layer $1 \le \ell < L$: }
At each position $i$, this layer computes the inside probabilities of spans of length $\ell+1$ starting and ending at $i$, using the recursive formulation from eq.~\ref{eq:inside_probability}. 

%Recall that the recursive definition of the inside probabilities is given by
%We use $\gN$ attention heads 
For every non-terminal $A \in \gN$, we will use a unique attention head to compute $\alpha(A, i, i + \ell)$ at each token $i$. Specifically, the attention head representing non-terminal $A \in \gN$ will represent the following operation at each position $i$:  
\begin{align}
    \alpha(A, i, j) =&  \sum_{B, C \in \gN} \sum_{k=i}^{j-1}\Pr[A \rightarrow B C]  \cdot \alpha(B, i, k) \cdot \alpha(C, k+1, j) \nonumber \\
    =& \sum_{B, C \in \gN} \sum_{ \substack{\ell_1, \ell_2 \ge 0\\ \ell_1 + \ell_2 = \ell-1}
     }\Pr[A \rightarrow B C]  \cdot \alpha(B, i, i+\ell_1) \cdot \alpha(C, j-\ell_2, j) \label{eq:construction-inside-computation},
\end{align}
where $j = i+\ell$. In the final step, we modified the formulation to represent the interaction of spans of different lengths starting at $i$ and ending at $j$. We represent this computation as the attention score $a_{i, j}$ using a key matrix $\mK_{A}^{(\ell)}$ and query matrix $\mQ_{A}^{(\ell)}$.
%We want the $A$-th attention head in layer $\ell$ compute the score
%\begin{align*}
%& \sum_{A \rightarrow BC \in R}\sum_{k=i+1}^{i+\ell-2} (P(A \rightarrow B C) \\
%& \quad \times \alpha(B, i, k) \times \alpha(C, k+1, i+\ell-1)),
%\end{align*}
%which is exactly the inside probability $\alpha(A,i,j)$ for all $j = i+\ell-1$.
%Note that there exists a matrix $\mM_{A}^{(\ell)}$ such that
%{\small
%\begin{align*}
%    & \dotp{\vv_i^{(\ell-1)}}{\mM_{A}^{(\ell)} \cdot \vv_{i+\ell-1}^{(\ell-1)}} \\
%    =& \sum_{A \rightarrow BC }\sum_{k=i+1}^{i+\ell-2} P(A \rightarrow B C) \alpha(B, i, k) \alpha(C, k+1, j)).
%\end{align*}
%}

%$\{(L + k, \ell - k)\}_{1 \le k \le \ell}$ 
\paragraph{Computing Eq.~\ref{eq:construction-inside-computation}} We set the Key matrix $\mK_{A}^{(\ell)}$ as $\mI$. The Query matrix  $\mQ_{A}^{(\ell)}$ is set such that if we define $\mP_A\in \R^{|\gN|\times |\gN|}$ that contains $\{\Pr[A \to BC]\}_{B,C \in \gN},$ $\mP_A$ appears at positions $(|\gN| (L + \ell_2), |\gN|  \ell_1 )$ for all $\ell_1, \ell_2 \ge 0$ with $\ell_1 + \ell_2 = \ell - 1$. Finally, $\mQ_{A}^{(\ell)}$ contains $\mQ_p\in \R^{L \times L} $ at position $(4|\gN|L, 4|\gN|L)$, such that $\mQ_p[i,i+\ell] = 0$ for $0 \le i < L$, with the rest set to $-\zeta$ for some large constant $\zeta$. The rest of the blocks are set as $0$. We give an intuition behind the structure of $\mQ_{A}^{(\ell)}$ below.

\paragraph{Intuition behind $\mQ_{A}^{(\ell)}$:} For any position $i$ and range $\ell_1 \le \ell$,  $\ve_i^{(\ell-1)}$ contains the inside probabilities $\{ \alpha(C, i - \ell_1, i) \}_{C \in \gN}$ in the coordinates $[|\gN| (L+\ell_1), |\gN| (L+\ell_1+1) )$, while it contains the inside probabilities $\{ \alpha(B, i, i + \ell_1) \}_{B \in \gN}$ in the coordinates $[|\gN| \ell_1, |\gN| (\ell_1+1) ).$ Hence, if we set 
%all blocks at positions $\{(|\gN| (L + \ell_1), |\gN| \ell_2)\}_{\ell_1, \ell_2 \le \ell}$ 
the block  at position $(|\gN| (L + \ell_2), |\gN| \ell_1)$ in $\mQ_{A}^{(\ell)}$
to $\mP_A$ for some $0 \le \ell_1, \ell_2 \le \ell$, with the rest set to $0$, we can get for any two positions $i, j$,
\begin{align*}
    & (\mK_{A}^{(\ell)} \ve_j^{(\ell-1)})^{\top}  \mQ_{A}^{(\ell)} \ve_i^{(\ell-1)} = \sum_{B, C \in \gN}   \Pr[A \to B C] \cdot \alpha(B, i, i+\ell_1) \cdot  \alpha (C, j -\ell_2, j) .
\end{align*}

Because we want to involve the sum over all $\ell_1, \ell_2$ pairs with $\ell_1 + \ell_2 = \ell - 1$, we will set blocks at positions $\{(|\gN| (L + \ell_2), |\gN| \ell_1 )\}_{\ell_1, \ell_2 : \ell_1 + \ell_2 = \ell-1}$ to $\mP_A$, while setting the rest to $0$. This gives us
\begin{align*}
    &(\mK_{A}^{(\ell)} \ve_j^{(\ell-1)})^{\top} \mQ_{A}^{(\ell)} \ve_i^{(\ell-1)} = \sum_{B, C \in \gN} \sum_{\substack{\ell_1, \ell_2 \ge 0\\ \ell_1 + \ell_2 = \ell-1}}  \Pr[A \to B C]  \cdot \alpha(B, i, i+\ell_1) \cdot  \alpha (C, j -\ell_2, j) .
\end{align*}


However, we want $(\mK_{A}^{(\ell)} \ve_j^{(\ell-1)})^{\top} \mQ_{A}^{(\ell)} \ve_i^{(\ell-1)}$ to compute $\alpha(A, i, j)$ iff $j = i + \ell$ and $0$ otherwise, so we will use the final block in $\mQ_{A}^{(\ell)}$ that focuses on the one-hot position encodings of $i$ and $j$ to differentiate the different location pairs. Specifically, the final block $\mQ_p$ will return $0$ if $j = i + \ell$, while it returns $-\zeta$ for some large constant $\zeta$ if $j \ne i + \ell$. This gives us
{\small
\begin{align}
    &(\mK_{A}^{(\ell)} \ve_j^{(\ell-1)})^{\top} \mQ_{A}^{(\ell)} \ve_i^{(\ell-1)} = \zeta(\mathbb{I}[j - i = \ell] - 1) + \sum_{B, C \in \gN} \sum_{\substack{\ell_1, \ell_2 \ge 0\\ \ell_1 + \ell_2 = \ell-1}}  \Pr[A \to B C] \cdot \alpha(B, i, i+\ell_1) \cdot  \alpha (C, j -\ell_2, j). \label{eq:attnt_head_inside_construct}
\end{align}
}
With the inclusion of the term $\zeta(\mathbb{I}[j - i = \ell ] - 1)$, we make $(\mK_{A}^{(\ell)} \ve_j^{(\ell-1)})^{\top} \mQ_{A}^{(\ell)} \ve_i^{(\ell-1)}$ positive if $j - i = \ell$, and negative if $j - i \ne \ell$. Applying a ReLU activation on top will zero out the unnecessary terms, leaving us with $\alpha(A, i, i+\ell)$ at each location $i$.


 




%Specifically, denote $\mP_A\in \R^{|\gN|\times |\gN|}$ the probabilities of rules splitted from non-terminal $A$. $\mM_{A}^{(\ell)}$ is a $4L\times 4L$ block matrix where each block has size $|\gN|\times |\gN|$. All the blocks except $(|\gN|L+k_1, k_2)$ where $k_1+k_2 = \ell$ in $\mM_{A,\ell}$ are zero, and the blocks at position $(|\gN|L+k_1, k_2)$ are $\mP_A$. To implement this as self-attention, we can set $\mK_A^{(\ell)} = \mI$ the identity matrix, and $\mQ_A^{(\ell)} = \mM_A^{(\ell)}$.

%We also need to make sure that the attention score between $i$ and $i+\ell'-1$ for $\ell'\neq \ell$ is smaller than the attention score between $i$ and $i+\ell-1$ (and thus the hard attention head will compute the probability between $i$ and $i+\ell-1$ by taking the max). We can achieve this goal by using positional embeddings. Specifically, if we use the one-hot positional embeddings, i.e., $\vp_i[i] = 1$ if the position is $i$ and $0$ otherwise. Then, we can set $\mK_p^{(\ell)} = \mI$, and $\mQ_p^{(\ell)}[i,i+\ell-1] = 0$ and all other entries to be $-1$. Then, $\vp_i^\top \mK_p^\top \mQ_p \vp_{i+\ell-1} = 0$ and $\vp_i^\top \mK_p^\top \mQ_p \vp_{j} = -1$ for all $j \neq i+\ell-1$, and using $|N|$ hard attentions together we can compute $\alpha(A,i,i+\ell-1)$.

Similarly, we use another $|\gN|$ attention heads to compute $\alpha(A,i-\ell, i)$. In the end, we use the residual connections to copy the previously computed inside probabilities $\alpha(A,i-\ell', i)$ and $\alpha(A,i, i+\ell')$ for $\ell' < \ell$.

%in order to compute $\sum_{\ell_1, \ell_2 \le \ell} \sum_{B, C \in \gN} \Pr[A \to B C] \cdot \alpha(B, i, i+\ell_1) \cdot  \alpha (C, j -\ell_2, j)$, 

%we can set all the first $4L$ blocks to $\mP_A\in \R^{|\gN|\times |\gN|}$ that contains $\{\Pr[A \to BC]\}_{B,C \in \gN}.$


%$\mQ_{A, \ell}$ is defined as a block-diagonal matrix, where all $\{(L + k, \ell - k)\}_{1 \le k \le \ell}$ blocks store $\mP_A\in \R^{|\gN|\times |\gN|}$ that contains $\{\Pr[A \to BC]\}_{B,C \in \gN}.$ The last block is a $L \times L$ matrix $\mQ_p$ such that $\mQ_p[i,i+\ell-1] = 0$ for $0 \le i < L$, with the rest set to $-1$. The rest of the blocks are set as $0$. The explanation for this structure of $\mQ_{A, \ell}$ is as follows.



%\haoyu{revise to here, following also needs to be revised.}

\paragraph{Outside probabilities:}

%For layer $\ell > L$, the $(k,A)$-th coordinate denotes the inside probability $\alpha(A, i+1, i+k)$ and the $(k,A)+|\gN|L$-th coordinate denotes the inside probability $\alpha(A, i-k, i-1)$ for all $k$. (Note that now position $i$ stores the inside probabilities for spans starting at $i+1$ and ending at $i-1$.) The $(k,A) + 2|\gN| L$-th coordinate denotes the outside probability $\beta(A, i, i+k-1)$, and the $(k,A) + 3|\gN| L$-th coordinate denotes the outside probability $\beta(A, i-k+1, i)$ at the current step ($\beta(A, i, i+k-1) = \beta(A, i-k+1, i) = 0$ for all $k > \ell - L$. The last $L$ coordinates denote the positional embeddings. We denote the first $4|\gN| L$ coordinates of $\ve_i^{(\ell)}$ to be $\vv_i^{(\ell)}$, and the positional embedding in $\ve_i^{(\ell)}$ as $\vp_i$. Thus, $\ve_i^{(\ell)} = (\vv_i^{(\ell)}, \vp_i$

In addition to all the inside probabilities, the contextual embeddings at position $i$ after the computations of any layer $(2L - 1) - \ell$ ($\ge L$) contain the outside probabilities of all spans of length at least $\ell + 1$ starting and ending at position $i$, i.e.  $\beta(A, i, i + k)$ and $\beta(A, i - k, i)$ for all $A \in \gN$ and $k \ge \ell $. The rest of the coordinates, except the position coordinates, contain $0$.

\paragraph{Layer $L$}
In this layer, we initialize the outside probabilities $\beta(\text{ROOT}, 1, L) = 1$ and $\beta(A, 1, L) = 0$ for $A\neq \text{ROOT}$. Furthermore, we move the inside probabilities $\alpha(A,i+1,i+k)$ from position $i+1$ to position $i$, and $\alpha(A,i-k,i-1)$ from position $i-1$ to position $i$ using 2 attention heads. 

\paragraph{Layer $L + 1 \le \tilde{\ell} := (2L - 1) - \ell \le 2L - 1$:} 
At each position $i$, this layer computes the outside probabilities of spans of length $\ell + 1$ starting and ending at $i$, using the recursive formulation from eq.~\ref{eq:outside_probability}. The recursive formulation for $\beta(A, i, i + \ell)$ for a non-terminal $A \in \gN$ has two terms, given by
\begin{align}
    & \beta(A,i,j)  = \beta_1(A,i,j) + \beta_2(A, i, j), \text{ with   } \nonumber \\&
    \beta_1(A,i,j) = \sum_{C,B \in \gN} \sum_{k=1}^{i-1} \Pr[B \to C A] \alpha(C, k, i-1) \beta(B, k, j), \text{ and } \label{eq:beta1} \\&
    \beta_2(A, i, j) = \sum_{B,C \in \gN} \sum_{k=j+1}^{L} \Pr[B \to A C] \alpha(C, j+1, k) \beta(B, i, k), \label{eq:beta2}
\end{align}
where $j = i + \ell.$ For each non-terminal $A \in \gN$, we will use two unique heads to compute $\beta(A, i, i+\ell)$
, each representing one of the two terms in the above formulation. We outline the construction for $\beta_1$; the construction for $\beta_2$ follows similarly.




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\paragraph{Computing Eq.~\ref{eq:beta1}} We build the attention head in the same way we built the attention head to represent the inside probabilities in eq.~\ref{eq:attnt_head_inside_construct}. Similar to \ref{eq:attnt_head_inside_construct}, we modify the formulation of $\beta_1$ to highlight the interaction of spans of different lengths.
\begin{align}
    \beta_1(A,i,j) = \sum_{B, C \in \gN} \sum_{\substack{\ell_1, \ell_2 \ge 0\\ \ell_2 - \ell_1 = \ell }} \Pr[B \to C A] \alpha(C, i-\ell_1, i-1) \beta(B, j-\ell_2, j), \label{eq:construction-beta1}
\end{align}
where $j = i+\ell$. We represent this computation as the attention score $a_{i, i+\ell}$ using a key matrix $\mK_{A, 1}^{(\tilde{\ell})}$ and query matrix $\mQ_{A, 1}^{(\tilde{\ell})}$. 
%We want the $A$-th attention head in layer $\ell$ compute the score
%\begin{align*}
%& \sum_{A \rightarrow BC \in R}\sum_{k=i+1}^{i+\ell-2} (P(A \rightarrow B C) \\
%& \quad \times \alpha(B, i, k) \times \alpha(C, k+1, i+\ell-1)),
%\end{align*}
%which is exactly the inside probability $\alpha(A,i,j)$ for all $j = i+\ell-1$.
%Note that there exists a matrix $\mM_{A}^{(\ell)}$ such that
%{\small
%\begin{align*}
%    & \dotp{\vv_i^{(\ell-1)}}{\mM_{A}^{(\ell)} \cdot \vv_{i+\ell-1}^{(\ell-1)}} \\
%    =& \sum_{A \rightarrow BC }\sum_{k=i+1}^{i+\ell-2} P(A \rightarrow B C) \alpha(B, i, k) \alpha(C, k+1, j)).
%\end{align*}
%}
%$\{(L + k, \ell - k)\}_{1 \le k \le \ell}$ 
First, we set the Key matrix $\mK_{A, 1}^{(\tilde{\ell})}$ as $\mI$. If we define $\mP_{A, r} \in \R^{|\gN|\times |\gN|}$ as a matrix that contains $\{\Pr[B \to CA]\}_{B,C \in \gN},$ which is the set of all rules where $A$ appears as the right child, $\mQ_{A, 1}^{(\tilde{\ell})}$ is set such that $\mP_{A, r}$ appears at positions $[|\gN| (3L + \ell_2), |\gN| (L + \ell_1))$ for all $0 \le \ell_1, \ell_2 \le L$ that satisfy $\ell_2 - \ell_1 = \ell$. Finally, $\mQ_{A, 1}^{(\tilde{\ell})}$ contains $\mQ_p\in \R^{L \times L} $ at position $(4|\gN|L, 4|\gN|L)$, such that $\mQ_p[i,i+\ell] = 0$ for $0 \le i < L$, with the rest set to $-\zeta$ for some large constant $\zeta$. The rest of the blocks are set as $0$. We give an intuition behind the structure of $\mQ_{A,1}^{(\tilde{\ell})}$ below.

\paragraph{Intuition for $\mQ_{A, 1}^{(\tilde{\ell})}$:}  For position $i$ and any ranges $1 \le \ell_1 < L$, $\ell+1 \le \ell_2 \le L$, $\ve_i^{( \tilde{\ell} - 1 )}$ contains the inside probabilities $\{ \alpha(C, i - \ell_1, i-1) \}_{C \in \gN}$ in the coordinates $[ |\gN| (L+\ell_1), |\gN| (L+\ell_1+1) )$, while it contains the outside probabilities $\{ \beta(B, i - \ell_2, i) \}_{B \in \gN}$ in the coordinates $[|\gN| (3L+\ell_2), |\gN| (3L+\ell_2+1) ).$ Hence, if we set the block at position $(|\gN| (3L + \ell_2), |\gN| (L + \ell_1))$
to $\mP_A$ for some $0 \le \ell_1 \le L, \ell+1 \le \ell_2 \le L$, with the rest set to $0$, we can get for any two positions $i, j$,
\begin{align*}
    & (\mK_{A}^{(\tilde{\ell})} \ve_j^{(\tilde{\ell}-1)})^{\top}  \mQ_{A}^{(\tilde{\ell})} \ve_i^{(\tilde{\ell}-1)} = \sum_{B, C \in \gN}   \Pr[B \to CA] \cdot \alpha(C, i-\ell_1, i-1) \cdot  \beta (B, j -\ell_2, j) .
\end{align*}


%Hence, if we set all blocks at positions $\{(|\gN| (2L + \ell_1), |\gN| \ell_2)\}_{\ell_1 \le \ell, \ell_2 \ge L-\ell}$ to $\mP_A$, with the rest set to $0$, we can get for any two positions $i, j$,
%\begin{align*}
%    & (\mK_{A}^{(\ell)} \ve_j^{(\ell-1)})^{\top}  \mQ_{A}^{(\ell)} \ve_i^{(\ell-1)} = \sum_{B, C \in \gN} \sum_{\ell_1, \ell_2 \le \ell}   \Pr[A \to B C] \cdot \alpha(B, i, i+\ell_1) \cdot  \alpha (C, j -\ell_2, j) .
%\end{align*}

Because we want to include the sum over $\ell_1, \ell_2$ pairs with $ \ell_2 - \ell_1 = \ell$, we will only set blocks at positions $[|\gN| (3L + \ell_2), |\gN| (L + \ell_1))$ for all $0 \le \ell_1, \ell_2 \le L$ that satisfy $\ell_2 - \ell_1 = \ell$ to $\mP_{A, r}$, while setting the rest to $0$. This gives us
\begin{align*}
    &(\mK_{A}^{(\tilde{\ell})} \ve_j^{(\tilde{\ell}-1)})^{\top}  \mQ_{A}^{(\tilde{\ell})} \ve_i^{(\tilde{\ell}-1)} = \sum_{B, C \in \gN}  \sum_{\substack{\ell_1, \ell_2 \ge 0 \\ \ell_2 - \ell_1 = \ell}}  \Pr[B \to CA] \cdot \alpha(C, i-\ell_1, i-1) \cdot  \beta (B, j -\ell_2, j).
\end{align*}


Because we want $(\mK_{A}^{(\tilde{\ell})} \ve_j^{(\tilde{\ell}-1)})^{\top}  \mQ_{A}^{(\tilde{\ell})} \ve_i^{(\tilde{\ell}-1)}$ to compute $\beta_1(A, i, j)$ with $j = i + \ell$ and $0$ otherwise, we will use the final block in $\mQ_{A}^{(\ell)}$ that focuses on the one-hot position encodings of $i$ and $j$ to differentiate the different location pairs. Specifically, the final block $\mQ_p$ will return $0$ if $j = i + \ell$, while it returns $-\zeta$ for some large constant $\zeta$, if $j \ne i + \ell$. This gives us
\begin{align*}
    &
    (\mK_{A}^{(\tilde{\ell})} \ve_j^{(\tilde{\ell}-1)})^{\top}  \mQ_{A}^{(\tilde{\ell})} \ve_i^{(\tilde{\ell}-1)} =\zeta (\mathbb{I}[j - i = \ell ] - 1) + \sum_{B, C \in \gN}  \sum_{\substack{\ell_1, \ell_2 \ge 0\\ \ell_2 - \ell_1 = \ell} } \Pr[B \to CA] \cdot \alpha(C, i-\ell_1, i-1) \cdot  \beta (B, j -\ell_2, j) 
\end{align*}
With the inclusion of the term $\zeta(\mathbb{I}[j - i = \ell ] - 1)$, we make $(\mK_{A}^{(\tilde{\ell})} \ve_j^{(\tilde{\ell}-1)})^{\top}  \mQ_{A}^{(\tilde{\ell})} \ve_i^{(\tilde{\ell}-1)}$ positive if $j - i = \ell$, and negative if $j - i \ne \ell$. Applying a ReLU activation on top will zero out the unnecessary terms, leaving us with $\beta_1(A, i, i+\ell)$ at each location $i$.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%






\iffalse
Now at layer $\ell + L$ where $1 < \ell \le L$, we compute $\beta(A,i,i+L-\ell)$ for $i \le \ell$ and $\beta(A,i-L+\ell,i)$ for $i \ge \ell$ at position $i$. We first compute $\beta(A,i,i+L-\ell)$, which needs $2$ attention heads together. First note that there exist two matrices $\mM_{A,1}^{(\ell+L)}$ and $\mM_{A,2}^{(\ell+L)}$ such that
{\small
\begin{align*}
    &\dotp{\vv_i^{(\ell-1+L)}}{\mM_{A,1}^{(\ell+L)} \cdot \vv_{i+L-\ell}^{(\ell-1+L)}}
    = \sum_{B \rightarrow CA} \sum_{k=1}^{i-1}P[B \rightarrow C A]
    \cdot\alpha(C, k, i-1) \beta(B, k, i+L-\ell) \\
    &\dotp{\vv_i^{(\ell-1+L)}}{\mM_{A,2}^{(\ell+L)} \cdot \vv_{i+L-\ell}^{(\ell-1+L)}}
    = \sum_{B \rightarrow A C} \sum_{k=i+L-\ell+1}^{L}P[B \rightarrow A C] \cdot\alpha(C, i+L-\ell+1, k) \beta(B, i, k).
\end{align*}
}
Specifically, we denote $\mP_{A,r} \in \R^{|\gN|\times |\gN|}$ the probabilities of rules that $A$ is the right child (the different columns of $\mP_{A,r}$ denote different parent non-terminals $B$ and different rows denote different left child $C$). We denote $\mP_{A,l} \in \R^{|\gN|\times |\gN|}$ the probabilities of rules that $A$ is the left child (the different rows of $\mP_{A,l}$ denote different parent non-terminals $B$ and different columns denote different right child $C$).

Then, $\mM_{A,1}^{(\ell+L)}$ is a $4L\times 4L$ block matrix where each block has size $|\gN|\times |\gN|$. All the blocks except $(|\gN|L+k_1, 3|\gN|L+k_2)$ where $k_2-k_1 = L - \ell+1$ in $\mM_{A,1}^{(\ell+L)}$ are zero, and the blocks at position $(|\gN|L+k_1, 3|\gN|L+k_2)$ are $\mP_{A,r}$. Similarly, $\mM_{A,2}^{(\ell+L)}$ is a $4L\times 4L$ block matrix where each block has size $|\gN|\times |\gN|$. All the blocks except $(2|\gN|L+k_1,k_2)$ where $k_1 - k_2 = L - \ell+1$ in $\mM_{A,2}^{(\ell+L)}$ are zero, and the blocks at position $(2|\gN|L+k_1,k_2)$ are $\mP_{A,l}$.
\fi 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Besides, we also need $2|\gN|$ additional heads for the outside probabilities $\beta(A,i-\ell,i)$. In the end, we use the residual connections to copy the previously computed inside probabilities $\beta(A, i-\ell', i)$ and $\alpha(A, i, i+\ell')$ for $\ell' > \ell$.
%and similar positional embeddings to make sure that $i$ is only attended to $i+L-\ell$ by using the ``hard attention''
\end{proof}

The use of hard attention simplifies the intuition of construction, but it doesn't fully leverage the power of attention models. Next, we show that it is possible to reduce the embedding size and the number of attention heads by introducing %we modify eq.~\ref{def:attention} to include 
relative positions and use soft attention (where multiple attention positions can have a nonzero attention score). We introduce $2L + 1$ relative position vectors $ \{ p_{ t } \in \mathbb{R}^d \}_{-L \le t \le L},$ and relative position biases $\{ b_{t, \ell} \in \mathbb{R} \}_{-L \le t \le L, 1 \le \ell \le 2L-1}$ that modify the key vectors depending on the relative position of the query and key tokens. For an attention head $h$ in layer $\ell$, the attention score $a_{i, j}^h$ is given by
\begin{equation}
    a_{i, j}^h =  \text{ReLU}( \mK_h \ve_j^{(\ell)} + p_{j - i} - b_{j-i, \ell} )^\top \mQ_h \ve_i^{(\ell)} \label{eq:soft_attention}
\end{equation}


\begin{theorem}[Relative positional embeddings] \label{thm:soft_attnt}
    Given a PCFG $\gG = (\gN, \gI, \gP, n, p)$, there exists an attention model with soft relative attention modules (\ref{eq:soft_attention}),  with embeddings of size $2|\gN| L + 1$, $2L$ layers, and $|\gN|$ attention heads in each layer, that can simulate the Inside-Outside algorithm on all sentences of length at most $L$ generated from $\gG$, and embed all the inside and outside probabilities. 
\end{theorem}
\iffalse
\haoyu{Rong's construction is actually another approximated version of the Inside-Outside algorithm, but we do not have time to test the performance empirically in these days. I think we should not put this in the first arxiv version, but we will do the related experiments before submission to ARR and add the related results then.}
Note that the dynamic programming for the inside and outside probabilities are computed for different length of spans (eq.~\ref{eq:inside_probability},\ref{eq:outside_probability}), and thus the previous constructions has $2L$ layers in total, where $L$ is the length of the sentence. However most of the time, the depth of the parse tree $d$ is much smaller than the length of the sentence $L$, and if we can execute the Inside-Outside algorithm in terms of depth, we should be able to reduce the number of layers from $2L$ to $2d$. The following theorem shows that there exists a construction that can compute the ``modified version'' of inside and outside probabilities 
\begin{theorem}[Informal]
    Given a PCFG $\gG = (\gN, \gI, \gP, n, p)$, there exists an attention model with soft relative attention modules (\ref{eq:soft_attention}), with embeddings of size $2|\gN| L + L$, $2L$ layers, and $2|\gN|$ attention heads in each layer, that can simulate the Inside-Outside algorithm on all sentences of length at most $L$ generated from $\gG$, and embed all the inside and outside probabilities.
\end{theorem}
\fi

The proof of the above theorem is in \cref{sec:soft_attnt_proof}. After we execute the Inside-Outside algorithms and get the inside and outside probabilities for spans, one can directly build the parse tree by applying the Labelled-Recall algorithm~\citep{goodman1996parsing}, which can be viewed as a sort of ``probe''.


\subsection{Masked language modeling for PCFG}\label{sec:mlmandio}

The Inside-Outside algorithm not only can perform parsing but also has a connection with masked language modeling, the pre-training loss used by BERT. The following theorem shows that, if the language is generated from a PCFG, then the Inside-Outside algorithm is optimal to predict the masked tokens.

\begin{theorem}\label{thm:io-optimal-mlm}
    Assuming that the language is generated from a PCFG, the Inside-Outside algorithm reaches the optimal masked language modeling loss.
\end{theorem}

Because the Inside-Outside algorithm is optimal for masked language modeling loss on synthetic PCFG data, we conjecture that if the model is pre-trained on synthetic PCFG data, it will implicitly embed the Inside-Outside algorithm (or the quantities computed by the Inside-Outside algorithm) in the model itself. This allows its intermediate layers to encode syntactic information useful for parsing. % thus containing the syntactic information. 
We verify this conjecture in \Cref{sec:probe-marginal-probs}. This conjecture may also explain why large language models pre-trained on natural language contain structural information~\citep{hewitt2019structural,vilares2020parsing,arps2022probing}

\begin{proof}[Proof of \Cref{thm:io-optimal-mlm}]
We first focus on 1-mask predictions, where given an input of tokens $w_1, w_2, \cdots, w_L$, and a randomly selected index $i$, we need to predict the token at position $i$ given the rest of the tokens, i.e. $\Pr\{w | w_{-i}\}$. Under the generative rules of the PCFG model, we have
\begin{align}
    \Pr[w | w_{-i}] & = \sum_{A}\Pr[A\to w] \cdot \Pr[A\text{ generates word at pos }i | w_{-i}] \nonumber\\
    & = \sum_{A}\Pr[A\to w]\cdot \frac{\beta(A,i,i)}{\sum_{B}\beta(B,i,i)} \label{eq:1mask_pcfg}
    %\\&
    %= \alpha(A, i, i) \cdot \frac{\beta(A,i,i)}{\sum_{B \in \gN}\beta(B,i,i)}.
\end{align}
Note that $\Pr[A\to w]$ can be extracted from the PCFG and $\{\beta(B, i, i)\}_{B \in \gN}$ can be computed by the Inside-outside algorithm. Thus, Inside-outside can solve the 1-masking problem optimally.

Now we consider the case where we randomly mask $m\%$ (e.g., 15\%) of the tokens and predict these tokens given the rest. In this setting, if the original sentence is generated from PCFG $\gG = (\gN, \gI, \gP, n, p)$, one can modify the PCFG to get $\gG' = (\gN, \gI, \gP, n+1, p')$ with $n+1$ denote the mask token $text{[MASK]}$ and for each preterminal $A\in\gP$, $p'(A\to \text{[MASK]}) = m\%$ and $p'(A\to w) = (1-m\%)p(A\to w),$ for all $w\neq \text{[MASK]}$. Then, the distribution of the randomly masked sentences follows the distribution of sentences generated from the modified PCFG $\gG'$. Similar to the 1-masking setting, we can use the Inside-outside algorithm to compute the optimal token distribution at a masked position.

\end{proof}


\subsection{Towards realistic size}\label{sec:approx-overview}
%\haoyu{currently AP and I both feel like we should put the details approximation into the appendix, and only give brief theorem, exp results and intuition here. In this version, we keep the minimal math here.}

In \Cref{sec:construct-io}, we show the construction of an attention model to execute the Inside-Outside algorithm for any PCFG. However, for a PCFG learned on the \dataset{PTB} dataset which contains sentences of average length $25$, we need an attention model with $~1600$ attention heads, $~3200L$ embedding dimension, and $2L$ layers to simulate the Inside-Outside algorithm on sentences of length $L$.
%, assuming $L\approx 25$ is the average length of the sentences. 
The constructed model is extremely large compared with BERT, which raises the question of whether our construction sheds any light on real-world architectures. In this section, we give positive evidence in this regard and show that we can utilize different approximation techniques to reduce the number of attention heads and the width of the embeddings in the constructed model, while still maintaining reasonable parsing performance. % taking us closer to real-world architectures. 
We heavily utilize the underlying sparsity in the structure of the English PCFG, which we point out below. The details have been deferred to \Cref{sec:approx-detailed}.

%We give intuitions on the approximation methods and show the size of the resulting language models, with the details deferred to \Cref{sec:approx-detailed}.

\paragraph{First ingredient: finding important non-terminals}
%In the constructions of \cref{thm:hard_attnt,thm:soft_attnt}, in all attention layers, we need a unique attention head for each non-terminal to represent the computation of an inside or outside probability corresponding to the non-terminal. However, if we only compute the probabilities of a specific set of non-terminals $\tilde\gI$ and $\tilde\gP$ in eq.~\ref{eq:inside_probability} and~\ref{eq:outside_probability} and set all others to 0, we can reduce the number of attention heads from $|\gN|$ to $\max\{|\tilde\gI|,|\tilde\gP|\}$, because the last layer of the model computes the outside probabilities for pre-terminals (need $|\tilde\gP|$ heads) and all middle layer computes the probabilities for in-terminals (need $|\tilde\gI|$ heads). If $|\tilde\gP| < c|\tilde\gI|$ for some constant $c$, we can also simulate the computations in the last layer with $|\tilde\gP|$ heads by $c$ layers with $|\tilde\gI|$ heads. Besides, we can also reduce the size of the embeddings since storing probabilities for the irrelevant non-terminals becomes unnecessary. Our hypothesis is that we can indeed focus only on a few non-terminals.
In the constructions of \cref{thm:hard_attnt,thm:soft_attnt}, the number of attention heads and embedding dimensions depend on the number of non-terminals of the PCFG. Thus if we can find a smaller PCFG, we can make the model much smaller. Specifically, if we only compute the probabilities of a specific set of in-terminals $\tilde\gI$ and pre-terminals $\tilde\gP$ in eq.~\ref{eq:inside_probability} and~\ref{eq:outside_probability}, we can reduce the number of attention heads from $|\gN|$ to $\max\{|\tilde\gI|,|\tilde\gP|\}$.\footnote{If $|\tilde\gP| < c|\tilde\gI|$ for some constant $c$, we can also simulate the computations in the last layer with $|\tilde\gP|$ heads by $c$ layers with $|\tilde\gI|$ heads. Besides, we can also reduce the size of the embeddings since storing probabilities for the irrelevant non-terminals becomes unnecessary.} Our hypothesis is that we can indeed focus only on a few non-terminals while retaining most of the performance.

\begin{hypothesis}
     For the PCFG $\gG = (\gN, \gI, \gP, n, p)$ learned on the English corpus, there exists $\tilde\gI\subset\gI,\tilde\gP\subset\gP$ with $|\tilde\gI|\ll |\gI|, |\tilde\gP|\ll |\gP|$, such that simulating Inside-Outside algorithm with $\tilde\gI \cup \tilde\gP$ non-terminals introduces \underline{small} error in the 1-mask perplexity and has \underline{minimal} impact on the parsing performance of the Labeled-Recall algorithm.
\end{hypothesis}
%We hypothesize that computing a small set of $|\tilde\gN$, i.e. $|\tilde\gN| \ll |\gN|$, can approximate the Inside-Outside algorithm without losing much on the parsing performance.
%We haven't made the error bounds explicit in the above hypothesis. We will perform experiments on the English PCFG to verify our hypothesis. 
We empirically verify our hypothesis through experiments. To find candidate sets $\tilde\gI,\tilde\gP$ for our hypothesis, we check the frequency of different non-terminals appearing at the head of spans in the parse trees of the \dataset{PTB}~\citep{marcus1993building} training set. We consider the Chomsky-transformed (binarized) parse trees for sentences in the \dataset{PTB} training set, and collect the labeled spans $\{(A, i, j)\}$ from the parse trees of all sentences. For all non-terminals $A$, we compute $\text{freq}(A)$, which denotes the number of times non-terminal $A$ appears at the head of a span. %Formally, 
%\[\text{freq}(A,\ell) := \sum_{(B,j,j')\in \{T_i\}_i}\mathbb I\{A = B, j'-j+1 = \ell\}.\]
\Cref{fig:freq-dist} shows the plot of $\text{freq}(A)$ for in-terminals and pre-terminals, with the order of the non-terminals sorted by the magnitude of $\text{freq}(\cdot)$. We observe that an extremely small subset of non-terminals have high frequency, which allows us to restrict our computation for the inside and outside probabilities to the few top non-terminals sorted by their $\text{freq}$ scores. We select the top frequent non-terminals as possible candidates for forming the set $\tilde\gN$.

%appears with high frequency in a specific length, and thus computing only the top-frequent non-terminals should not affect the computation a lot intuitively.

\begin{figure}
     \centering
     \iffalse
     \begin{subfigure}[b]{0.325\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figs/nt-freq-len2.png}
         %\caption{Freq. plot for spans with length $2$}
         %\label{fig:len-2-spans}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.325\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figs/nt-freq-len5.png}
         %\caption{Freq. plot for spans with length $5$}
         %\label{fig:len-5-spans}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.325\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figs/nt-freq-len10.png}
         %\caption{Freq. plot for spans with length $10$}
         %\label{fig:len-10-spans}
    \end{subfigure}
    \fi
    \begin{subfigure}[t]{0.48\textwidth}
        \includegraphics[width=\linewidth]{figs/fig-in-global.png}
    \end{subfigure}
    \begin{subfigure}[t]{0.48\textwidth}
        \includegraphics[width=\linewidth]{figs/fig-pre-global.png}
    \end{subfigure}
    \iffalse
    \begin{subfigure}[t]{0.48\textwidth}
        \includegraphics[width=\linewidth]{figs/nts-frequency.pdf}
    \end{subfigure}
    \fi
        \caption{Plot for the frequency distribution of in-terminals ($\gI$) and pre-terminals ($\gP$). We compute the number of times a specific non-terminal appears in a span of a parse tree in the \dataset{PTB} training set. We then sort the non-terminals according to their normalized frequency and then show the frequency vs. index plot.}
        \label{fig:freq-dist}
\end{figure}




%To further verify that with the approximated computation, we select the non-terminals $\gN^{(\ell)}$ that will be computed for spans with length $\ell$ greedily from $\text{freq}(A,\ell)$ (i.e., select the non-terminals with the highest frequency). Then we execute the approximated version of the Inside-Outside algorithm and compute the unlabelled F1 score.
\iffalse
\begin{table}[]
    \centering
    \begin{tabular}{|c|c|c|c|}
    \hline
         & Corpus F1 & Sent F1 & TV \\
         \hline
         \makecell{No approx.} & 75.90 & 78.77 & 0 \\
         \hline
         $|\gN^{(\ell)}| = 20$ & 62.49 & 61.17 & 0.054 \\
         $|\gN^{(\ell)}| = 30$ & 70.29 & 70.92 & 0.045 \\
         $|\gN^{(\ell)}| = 40$ & 72.41 & 72.97 & 0.029\\
         \hline
    \end{tabular}
    \caption{Unlabelled F1 scores for approximate Inside-Outside algorithm with very few non-terminals to compute in each layer on \dataset{PTB} development set. $\gN^{(\ell)}$ denotes the set of non-terminals to compute for layer $\ell$ and is selected to be the non-terminals with top frequency for spans with length $\ell$. The PCFG is learned on \dataset{PTB} training dataset. Besides the parsing F1 results, we also show the TV distance between the exact computation and the approximated computation for 1-masking prediction.}
    \label{tab:few-nt-pcfg}
\end{table}
\fi

\begin{table}[]
    \centering
    \begin{tabular}{|c|c|c|c|}
    \hline
         Approximation & Corpus F1 & Sent F1 & ppl. \\
         \hline
         \makecell{No approx.} & 75.90 & 78.77 & 50.80 \\
         \hline
         $|\tilde\gI| = 10,|\tilde\gP|=45$ & 57.14 & 60.32 & 59.57 \\
         $|\tilde\gI| = 20,|\tilde\gP|=45$ & 68.41 & 71.91 & 55.16 \\
         $|\tilde\gI| = 40,|\tilde\gP|=45$ & 72.45 & 75.43 & 54.09 \\
         \hline
    \end{tabular}
    \iffalse
    \begin{tabular}{|c|c|c|c|c|}
    \hline
         & Corpus F1 & Sent F1 & TV & ppl. \\
         \hline
         \makecell{No approx.} & 75.90 & 78.77 & 0 & 50.80 \\
         \hline
         $|\tilde\gN| = 10$ & 57.14 & 60.32 & 0.114 & 58.11 \\
         $|\tilde\gN| = 20$ & 68.41 & 71.91 & 0.073 & 55.16 \\
         $|\tilde\gN| = 40$ & 72.45 & 75.43 & 0.050 & 54.09 \\
         \hline
    \end{tabular}
    \fi
    \iffalse
    \begin{tabular}{|c|c|c|}
    \hline
         & Corpus F1 & Sent F1 \\
         \hline
         \makecell{No approx.} & 75.90 & 78.77 \\
         \hline
         $|\tilde\gN| = 10$ & 57.14 & 60.32  \\
         $|\tilde\gN| = 20$ & 68.41 & 71.91  \\
         $|\tilde\gN| = 40$ & 72.45 & 75.43  \\
         \hline
    \end{tabular}
    \fi
    \caption{Experiment results by approximately computing the Inside-Outside algorithm with very few non-terminals. We show the unlabelled F1 scores on \dataset{PTB} development set as well as the 1-masking perplexity. $\tilde\gI$ ($\tilde\gP$) denotes the set of in(pre)-terminals to compute and are selected to be the in(pre)-terminals with top frequency. The PCFG is learned on \dataset{PTB} training dataset. The ppl. column denote the 1-masking perplexity on 200 sentences generated from the learned PCFG. %Besides the parsing F1 results, we also show the TV distance between the exact computation and the approximated computation for 1-masking prediction.
    }
    \label{tab:few-nt-pcfg-global}
\end{table}


We verify the effect of restricting our computation to the frequent non-terminals on the 1-mask perplexity and the unlabeled F1 score of the approximate Inside-Outside algorithm in \Cref{tab:few-nt-pcfg-global}. Recall from \Cref{thm:io-optimal-mlm}, the 1-mask probability distribution for a given sentence $w_1, \cdots, w_L$ at any index $i$ is given by \cref{eq:1mask_pcfg}, and thus we can use \cref{eq:1mask_pcfg} to compute the 1-mask perplexity on the corpus. To measure the impact on 1-mask language modeling, we report the perplexity of the original and the approximate Inside-Outside algorithm on 200 sentences generated from PCFG. 

%Besides the F1 score, we also compute the total variation distance between the 1-masking distribution computed by the Inside-Outside algorithm and the approximated version. 
We observe that restricting the computation to the top-$40$ and $45$ frequent in-terminals and pre-terminals leads to $<6.5\%$ increase in average 1-mask perplexity.% with the average TV distance of the masked token predictions between the original and the approximate Inside-Outside algorithm being only $0.05$. 
Furthermore, the Labeled-Recall algorithm observes at most $4.24\%$ drop from the F1 performance of the original PCFG. %and the total variation between distributions is $0.05$. \abhishek{What is TV between distributions here? Please explain} 
If we further restrict the computation to the top-$20$ and $45$ in-terminals and pre-terminals, we can still get $71.91\%$ sentence F1 score, and the increase in average 1-mask perplexity is less than $8.6\%$. However, restricting the computation to $10$ in-terminals leads to at least $15\%$ drop in parsing performance. Thus combining \Cref{thm:soft_attnt} and \Cref{tab:few-nt-pcfg-global}, we have the following informal theorem.


\begin{theorem}[Informal]\label{thm:approx-few-nt-informal}
    Given the PCFG $\gG = (\gN, \gI, \gP, n, p)$ learned on the English corpus, there exists an attention model with soft relative attention modules (\ref{eq:soft_attention}),  with embeddings of size $275+40L$, $2L+1$ layers, and $20$ attention heads in each layer, that can simulate an approximate Inside-Outside algorithm on all sentences of length at most $L$ generated from $\gG$, introducing $9.29\%$ increase in average 1-mask perplexity and resulting in at most $8.71\%$ drop in the parsing performance of the Labeled-Recall algorithm. 
    %Parsing using this approximated Inside-Outside algorithm gives $68.41\%$ corpus F1 $71.91\%$ sentence F1 on \dataset{PTB} dataset.
    %There exists an attention model
    %By computing the inside and outside probabilities for only the top-$20$ non-terminals ($|\tilde\gN| = 20$), we can reduce the size of the model in \Cref{thm:soft_attnt} to $20$ attention heads in each layer, $540+40L$ embedding dimension, and $2L$ layers to approximately execute the Inside-Outside algorithm on PCFG learned on English corpus. Parsing using this approximated Inside-Outside algorithm gives us $68.41\%$ corpus F1 $71.91\%$ sentence F1 on \dataset{PTB} dataset.
\end{theorem}

If we plug in the average length $L\approx 25$ for sentences in \dataset{PTB}, we can get a model with $20$ attention heads, $1275$ hidden dimension, and $51$ layers. Compared with the construction in \Cref{thm:soft_attnt}, the size of the model is much closer to reality. Besides, using this approximation doesn't affect the parsing performance a lot: compared with parsing using the Inside-Outside algorithm that achieves $75.90\%$ corpus F1 and $78.77\%$ sentence F1 on \dataset{PTB} dataset, the approximated computation shows a drop by $8.71\%$. The parsing score is still highly non-trivial, since the naive baseline, Right Branching (RB), can only get $<40\%$ sentence and corpus F1 scores on \dataset{PTB} dataset\footnote{The naive RB works well under the \dataset{PTB} after removing the punctuations. To get such a score on \dataset{PTB} with the punctuations, one needs to modify the RB baseline~\citep{li2020empirical}. Directly applying RB on \dataset{PTB} with punctuations will lead to <1\% for both sentence and corpus F1 scores.}.



\paragraph{Second ingredient: utilizing structures across non-terminals}

In both \Cref{thm:soft_attnt} and \Cref{thm:approx-few-nt-informal}, we still assign one attention head to represent the computation for a specific non-terminal, which does not utilize possible underlying structures between different non-terminals. Next, we give the intuition of the second approximation method that utilizes the possible hidden structure between non-terminals.

Recall that in the proof of \Cref{thm:hard_attnt}, we use one attention head at layer $\ell$ to compute the inside probabilities $\alpha(A,i,j)$ with $j-i = \ell$ (see eq.~\ref{eq:attnt_head_inside_construct}). Note that if the inside probabilties $\alpha(A,i,j)$ for different non-terminals $A\in\tilde\gI$ lie in a $k^{(\ell)}$-dimensional subspace with $k^{(\ell)} < |\tilde\gI|$, we can compute all of the inside probabilities $\alpha(A,i,j)$ using only $k^{(\ell)}$ attention heads instead of $|\tilde\gN|$ by computing the vector $\mW^{(\ell)}\bm{\alpha}(i,j)$, where $\mW^{(\ell)}\in\R^{k^{(\ell)}\times |\tilde\gI|}$ is the transformation matrix and $\bm{\alpha}(i,j)\in\R^{|\tilde\gI|}$ is the concatenation of all inside probabilties $\{\alpha(A,i,j)\}_{A\in\tilde\gI}$.\footnote{Here for simplicity, we only consider how to further reduce the number of attention heads for the in-terminals $\tilde\gI$, since the computation of outside probabilities for $A\in\tilde\gP$ needs at most $|\tilde\gP|$ attention heads in the last layer and can be simulated by several layers with less attention heads as long as $|\tilde\gP$ is not too large.} Although the probabilities should not lie in a low dimensional subspace in reality, we can still try to learn a transformation matrix $\mW^{(\ell)}\in\R^{k^{(\ell)}\times |\tilde\gI|}$ and approximately compute the inside probabilities by $\bm\alpha(i,j) = (\mW^{(\ell)})^{\dagger}\mW^{(\ell)}\bm\alpha^*(i,j)$ for $j-i = \ell$, where $\bm\alpha^*(i,j)$ is computed using eq.~\ref{eq:attnt_head_inside_construct}. The same procedure can also be applied to the computation of outside probabilities. Please refer to \Cref{sec:approx-low-rank} for more details on how we perform the approximated computations. We hypothesize that we can indeed find such transformation matrices $\{\mW^{(\ell)}\}_{\ell\le L}$ that can reduce the computations while retaining most of the performance.
%The following informal theorem show the effectiveness of using this approximation, and please refer to \Cref{sec:approx-low-rank} for more details.

\begin{hypothesis}
     For the PCFG $\gG = (\gN, \gI, \gP, n, p)$ learned on the English corpus, there exists transformation matrices $\mW^{(\ell)}\in\R^{k^{(\ell)}\times |\tilde\gI|}$ for every $\ell \le L$, such that approximately simulating the Inside-Outside algorithm with $\{\mW^{(\ell)}\}_{\ell\le L}$ introduces \underline{small} error in the 1-mask perplexity and has \underline{minimal} impact on the parsing performance of the Labeled-Recall algorithm.
\end{hypothesis}

We verify our hypothesis through experiments. We learn the matrix $\mW^{(\ell)}$ that captures the correlation of the non-terminals in $\tilde\gI$ for spans with length $\ell+1$. For a single sentence $s$ and a specific span with length $\ell+1$, we can compute the marginal probability of this span of each non-terminal $\mu(A,i,j) = \alpha(A,i,j)\times \beta(A,i,j)$ for all $A\in\tilde\gI$. Then denote $\vmu_s^{i,j}\in\R^{|\tilde\gI|}$ as the vector representation of the marginal probabilities for this span. We can compute $\mX_s^{(\ell)} = \sum_{i,j:j-i=\ell} \vmu_s^{i,j}(\vmu_s^{i,j})^\top$ as a matrix to capture the correlation of in-terminals $\tilde\gI$ for spans with length $\ell+1$ given a sentence $s$.
Then, we sum over the sentences in the \dataset{PTB} training set and get the normalized correlations
$\mX^{(\ell)} = \sum_{s} \mX_s^{(\ell)} / \norm{\mX_s^{(\ell)}}_{\text{F}}$.
Finally, we apply the Eigen-decomposition on $\mX_\ell$ and set $\mW^{(\ell)}$ to contain the Eigen-vectors with top $k^{(\ell)}$ Eigen-values. Please refer to \Cref{sec:approx-low-rank} for more discussions on finding the transformation matrices $\{\mW^{(\ell)}\}_{\ell\le L}$.

\Cref{tab:learned-transformation-global} shows the parsing results and the 1-masking perplexity that utilizes the hidden structure $\{\mW^{(\ell)}\}_{\ell\le L}$ with different $k^{(\ell)}$. Without utilizing hidden structure, if we only compute the probabilities for top-$10$ in-terminals, we can only get $60.32\%$ sentence F1 on \dataset{PTB}. After utilizing the hidden structures, we can get $71.33\%$ sentence F1 on \dataset{PTB} with a model that has only 15 attention heads, and $65.31\%$ sentence F1 with a model that only has 10 attention heads. The following informal theorem summarizes the results.

%It is also worth noting that empirically, directly learning the transformation matrix $\mW^{(\ell)}$ by the Eigen-decomposition of $\mX^{(\ell)}$ computed on all non-terminals (thus with size $~1600\times 1600$) performs worse than learning $\mW^{(\ell)}$ by the Eigen-decomposition of $\mX^{(\ell)}$ computed on top 40 non-terminals ($|\gN^{(\ell)}| = 40$).

\iffalse
\begin{table}[!t]
    \centering
    \begin{tabular}{|c|c|c|c|}
        \hline
        $k^{(\ell)}$ & Corpus F1 & Sent F1 & TV \\ 
        \hline
        \makecell{Baseline \\ $|\gN^{(\ell)}| = 20$} & 62.49 & 61.17 & 0.054 \\
        \hline
        \makecell{$k^{(\ell)} = 10$} & 57.90 & 61.71 & 0.107 \\
        %\hline
        \makecell{$k^{(\ell)} = 15$} & 64.90 & 68.36 & 0.082 \\
        %\hline
        \makecell{$k^{(\ell)} = 20$} & 68.03 & 69.17 & 0.046 \\
        \hline
    \end{tabular}
    \caption{Experiment results using learned transformations. For the Baseline, there is no ``sparse-coding'' style approximation, we only compute the probabilities for the top 20 non-terminals. We show the parsing F1 results and the TV distance for 1-masking prediction.}
    \label{tab:learned-transformation}
\end{table}
\fi

\begin{table}[!t]
    \centering
    \begin{tabular}{|c|c|c|c|}
        \hline
        Approximation & Corpus F1 & Sent F1 & ppl. \\ 
        \hline
        \makecell{$|\tilde\gI| = 10,|\tilde\gP|=45$} & 57.14 & 60.32 & 59.57 \\
        $|\tilde\gI| = 20,|\tilde\gP|=45$ & 68.41 & 71.91 & 55.16 \\
        \hline
        \makecell{$k^{(\ell)} = 10,|\tilde\gI| = 20,|\tilde\gP|=45$} & 61.72 & 65.31 & 57.05 \\
        %\hline
        \makecell{$k^{(\ell)} = 15,|\tilde\gI| = 20,|\tilde\gP|=45$} & 68.20 & 71.33 & 55.52 \\
        %\hline
        %\makecell{$k^{(\ell)} = 20$} & & & \\
        \hline
    \end{tabular}
    \iffalse
    \begin{tabular}{|c|c|c|c|c|}
        \hline
        $k^{(\ell)}$ & Corpus F1 & Sent F1 & TV & ppl. \\ 
        \hline
        \makecell{$|\tilde\gI| = 10,|\tilde\gP|=45$} & 57.14 & 60.32 & 0.114 & 58.11 \\
        $|\tilde\gI| = 20,|\tilde\gP|=45$ & 68.41 & 71.91 & 0.073 & 54.07 \\
        \hline
        \makecell{$k^{(\ell)} = 10,|\tilde\gI| = 20,|\tilde\gP|=45$} & 61.72 & 65.31 & 0.108 & 55.64 \\
        %\hline
        \makecell{$k^{(\ell)} = 15,|\tilde\gI| = 20,|\tilde\gP|=45$} & 68.20 & 71.33 & 0.084 & 54.36 \\
        %\hline
        %\makecell{$k^{(\ell)} = 20$} & & & \\
        \hline
    \end{tabular}
    \fi
    \iffalse
    \begin{tabular}{|c|c|c|}
        \hline
        $k^{(\ell)}$ & Corpus F1 & Sent F1 \\ 
        \hline
        \makecell{$|\tilde\gN| = 10$} & 57.14 & 60.32 \\
        $|\tilde\gN| = 20$ & 68.41 & 71.91 \\
        \hline
        \makecell{$k^{(\ell)} = 10$} & 61.72 & 65.31 \\
        %\hline
        \makecell{$k^{(\ell)} = 15$} & 68.20 & 71.33 \\
        %\hline
        %\makecell{$k^{(\ell)} = 20$} & & & \\
        \hline
    \end{tabular}
    \fi
    \caption{Experiment results using learned transformations $\mW^{(\ell)}$. For the baselines, we only compute the probabilities for the important non-terminals. We show the parsing F1 results on \dataset{PTB} development set as well as the 1-masking perplexity on 200 sentences generated from the PCFG. % and the TV distance for 1-masking prediction.
    }
    \label{tab:learned-transformation-global}
\end{table}


\begin{theorem}[Informal]\label{thm:approx-low-rank-informal}
    Given the PCFG $\gG = (\gN, \gI, \gP, n, p)$ learned on the English corpus, there exists an attention model with soft relative attention modules (\ref{eq:soft_attention}),  with embeddings of size $275+40L$, $2L+1$ layers, and $15$ attention heads in each layer, that can simulate an approximate Inside-Outside algorithm on all sentences of length at most $L$ generated from $\gG$, introducing $8.6\%$ increase in average 1-mask perplexity (eq.~\ref{eq:1mask_pcfg}) and resulting in at most $9.45\%$ drop in the parsing performance of the Labeled-Recall algorithm.
    %By utilizing the hidden structure between non-terminals, we can reduce the size of model in \Cref{thm:soft_attnt} to $15$ attention heads in each layer, $540+40L$ embedding dimension, and $2L$ layers to approximately execute the Inside-Outside algorithm on PCFG learned on English corpus. Parsing using this approximated Inside-Outside algorithm gives us $68.20\%$ corpus F1 $71.33\%$ sentence F1 on \dataset{PTB} dataset.
\end{theorem}

Compared with the parsing results from \Cref{thm:approx-few-nt-informal}, the corpus and sentence F1 scores are nearly the same, and we further reduce the number of attention heads in each layer from $20$ to $15$. If we only use $10$ attention heads to approximately execute the Inside-Outside algorithm, we can still get $61.72\%$ corpus F1 and $65.31\%$ sentence F1 on \dataset{PTB} dataset, which is still much better than the Right-branching baseline. \Cref{thm:approx-low-rank-informal} shows that attention models with a size much closer to the real models (like BERT or RoBERTa) still have enough capacity to parse decently well (>70\% sentence F1 on \dataset{PTB}).

It is also worth noting that approximately executing the Inside-Outside algorithm using the transformation matrices $\{\mW^{(\ell)}\}_{\ell\le L}$ is very different from reducing the size of the PCFG grammar, since we use different matrix $\mW^{(\ell)}$ when computing the probabilities for spans with different length. If we choose to learn the same transformation matrix $\mW$ for all the layers $\ell$, the performance drops.
%Although the number of layers of our constructed models ($50$ to process average length sentence in \dataset{PTB}) are still relatively large compared to the models in reality ($12$ for BERT and RoBERTa), our approximations in \Cref{thm:approx-few-nt-informathm:approx-low-rank-informal} show strong hint that the PCFG learned on English corpus are ``highly compressible''.




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\iffalse
\subsection{Overview of our construction}\label{sec:construct-overview}
\haoyu{this will be very simple, only mentioning that there exists an ``linear'' attention head to compute the probabilities. All the missing proofs will go to appendix. The construction overview may place after \Cref{thm:hard_attnt} or \Cref{thm:soft_attnt}. Feel like going after \Cref{thm:hard_attnt} makes most sense.}
Our construction adds attention layers to simulate the recursive definitions of the inside and outside probabilities in \cref{eq:inside_probability,eq:outside_probability}. 
%at any position, $i$, for a set of scores ${a_{i, 1}, a_{i, 2}, \cdots, a_{i, L}},$ the hard attention module returns $\max(a_{i, 1}, a_{i, 2}, \cdots, a_{i, L}).$ 


Embeddings of a word $w$ will contain $\Pr[A \rightarrow w]$ for all $A \in \gP$. Given an input sentence $[w_1, w_2, \cdots, w_L]$, we concatenate the embeddings of each word with a one-hot position encoding representing its position in the sentence.
The first $L$ layers compute the inside probabilities, while the next $L$ layers compute the outside probabilities for a given sentence. 

\paragraph{Inside probabilities:} After $\ell \le L$ attention layers, the contextual embedding at each position $i$ contains the inside probabilities of all spans of length at most $\ell$ starting and ending at position $i$, i.e.  $\alpha(A, i, i + k)$ and $\alpha(A, i-k, i)$ for all $A \in \gN$ and $k \le \ell$. In layer $\ell$, for a non-terminal $A \in \gN$, we use one attention head to compute $\alpha(A, i, i+\ell)$ using eq.~\ref{eq:inside_probability}. To do so, we set the Key matrix $\mK_{A, \ell}$ to $\mI$, while the Query matrix $\mQ_{A, \ell}$ is defined as a block-diagonal matrix, with $4L$ blocks of size $|\gN| \times |\gN|$ and a single block of size $L \times L$, such that for any position $j$, 


\begin{align*}
    &(\mK_{A, \ell} \ve_i^{(\ell-1)})^{\top} \mQ_{A, \ell} \ve_j^{(\ell-1)} \\=& (\mathbb{I}[j-i = \ell] - 1) + \sum_{B, C \in \gN} \sum_{\ell_1, \ell_2: \ell_1 + \ell_2 = \ell}  \Pr[A \to B C] \\& \cdot \alpha(B, i, i+\ell_1) \cdot  \alpha (C, j -\ell_2, j) .
\end{align*}



The component $\mathbb{I} [ j - i = \ell ]$ is used by the attention model to pick only the relevant position $j$ (with its attention score intact) such that $j - i = \ell$. This component comes from the interaction of the one-hot position embeddings at the two positions. If $j - i \ne \ell$, the above score is negative and will be zeroed out by the $ReLU$ activation. The value matrix $\mV_{A, \ell}$ is set such that the final computation is stored in the relevant position in the contextual embedding. Similarly, we use another attention head to compute $\alpha(A, i-\ell, i)$ for non-terminal $A$.
%Similarly, we use another attention head to compute 


%where the $(\ell_1, \ell_2)$ block-diagonal matrix contains the $|\gN| \times |\gN|$ matrix $\{ \Pr[A \to BC] \} _{B, C \in \gN}$, iff $\ell_1 + \ell_2 - |\gN| = \ell$ and $0$ otheriwse. In simple terms, $\mQ_{A, \ell}$ only focuses on the weighted inner product of the contextual sub-embeddings at positions $i$ and $j$ that contain $\alpha(*, i, i+\ell_1)$ and $\alpha(*, i, i+\ell_2)$, with $\ell_1 + \ell_2 = \ell$. In order to 

%Each attention head in layer $\ell$ will correspond to the computation of inside probabilities w.r.t. a particular non-terminal in 


\paragraph{Outside probabilities:} After $\ell \ge L$ attention layers, in addition to all inside probabilities, the embedding at each position $i$ will contain the outside probabilities of all spans of lengths at least $2L - \ell$ starting and ending at position $i$, i.e.  $\beta(A, i, i + k)$ and $\beta(A, i-k, i)$ for all $A \in \gN$ and $k \ge 2L - \ell$. At layer $\ell$, for any non-terminal $A \in \gN$, we require 2 attention heads to compute each of the 2 terms for $\beta(A, i, i + 2L - \ell)$ in eq.~\ref{eq:outside_probability}. Furthermore, we require 2 additional heads to compute $\beta(A, i - 2L +\ell, i)$. The construction of the attention heads is similar to the construction of the attention heads that compute the inside probabilities.  



\paragraph{Soft attention with relative positions:}
The constructed attention model has redundancies that can be removed with relative position embeddings. We outline the change for the computation of the inside probabilities. Similar modifications are applied for the computation of the outside probabilities.

At layer $\ell \le L$ and position $i$, for the attention head that represents the computation of $\alpha(A, i, i+\ell)$ for a non-terminal $A \in \gN$, rather than restricting its  attention to a single position $i + \ell$ as before, we define $\mQ_{A, \ell}$, $\mK_{A, \ell}$, and the position vectors $\{p_{t, 0}\}_{-L \le t \le  L}$ such that, 
\begin{align}
    &( \mK_{A, \ell} \ve_j^{(\ell-1)} \odot p_{j - i, 0 } )^\top  \mQ_{A, \ell} \ve_i^{(\ell-1)} = \mathbb{I}[ j \ge i ] \cdot \nonumber\\
    & \sum_{B, C} \Pr[ A \to BC ] \cdot  \alpha(B, i, j) \cdot \alpha(C, j, i + \ell) . \label{eq:intend_position}
\end{align}
$\mQ_{A, \ell}$ is set as a $2L \times 2L$ block-diagonal matrix, where each block is a $|\gN| \times |\gN|$ matrix  that contains $\{\Pr[A \to BC]\}_{B, C \in \gN}.$ $\mK_{A, \ell}$ is set to $\mI$. $p_{j - i, 0}$ is set in $\{0, 1\}^{d}$ such that if $j \ge i$, we attend only to the portion of $\ve_j^{(\ell-1)}$ containing $\{ \alpha(C, j, j + \ell') \} _{C \in \gN}$ with $\ell' = \ell - (j - i)$. $p_{j-i, 0}$ set to $0$ if $j < i$. Given that the coordinates of our embeddings lie in the range $[0, 1]$, $\mK_{A, \ell} \ve_j^{(\ell-1)} \odot p_{j - i, 0 }$ can be achieved by $ReLU(\mK_{A, \ell} \ve_j^{(\ell-1)} + p_{j - i, 0 } - 1).$

\fi












