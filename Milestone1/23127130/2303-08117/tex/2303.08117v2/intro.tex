\section{Introduction}
\looseness=-1One of the surprising discoveries about transformer-based language models like BERT~\citep{devlin2019bert} and RoBERTa~\citep{liu2019roberta} was that contextual word embeddings encode information about parsing, which can be extracted using a simple ``linear probing''  to yield approximately correct  dependency parse trees for the text~\citep{hewitt2019structural,manning2020emergent}.
Subsequently, \citet{vilares2020parsing,wu2020perturbed,arps2022probing} employed linear probing also to recover information about constituency parse trees.
Investigating the parsing capability of transformers is of significant interest, as incorporating (the awareness of) syntax in large language models has been shown to enhance the final performance on various downstream tasks~\citep{xu2021syntax,bai2021syntax}. Additionally, it can contribute to the ongoing exploration of the ``mechanistic interpretability'' for reverse engineering the inner workings of pre-trained large language models~\citep{elhage2021mathematical,olsson2022context,nanda2023progress}.
%\haoyu{i revise the intro to this: i think this is more related to our ``motivation''.}

%But how should we formally understand the ability to parse and reason in the framework of pre-training?
%A fundamental understanding of the properties of a pre-trained model and the importance of architecture and pre-training requires assumptions on the pre-training data distribution. 
%One possible direction can be to 
%Talk something mechanistic here?
%We follow the recent line of works on the mechanistic interpretability of transformers to reverse engineer the inner workings of the pre-trained model\cite{elhage2021mathematical,olsson2022context,nanda2023progress}. 
%However, in order to have a fundamental argument about connections between pre-training and parsing, we make assumptions about the data distribution so that we can connect the pre-trained model to an optimal algorithm that we expect the model to learn during pre-training. One of the generative models to model language that have been extensively studied both theoretically and empirically is Probabilistic Context-Free Grammar (PCFG) \cite{manning1999foundations} (see \cref{sec:pcfg_def} for a formal definition). 
\looseness=-1The current paper focuses on the ability of BERT-style transformers to do constituency parsing, specifically for PCFGs. Prior studies~\citep{bhattamishra2020computational,perez2021attention} established that transformers are Turing complete, suggesting their potential for parsing. But do they actually parse while trying to do masked-word prediction?  
One reason to be cautiously skeptical is that naive translation of constituency parsing algorithms into a transformer results in transformers with number of heads that scales with the size of the grammar (\Cref{sec:construct-io}), whereas BERT-like models have around a dozen heads. This leads to the following question.  %are much smaller compared to key parameters of commonly used grammars. Our first question considers the capability of BERT-like models in parsing:%thus certainly capable of parsing sentences with a model whose size scales with the sentence size and grammar, we still don't know why BERT-size model has the capacity to ``parse''. Thus we have the following question,


\begin{quote}
    %{\em Are BERT-like models actually doing parsing or just something that correlates with parsing?}
    %{\em How do the size and architecture of BERT-like models relate with their capacity to do parsing?}
    \centering
    {\em (Qs 1): Are BERT-like models capable of parsing with realistic number of heads?}%with their capacity to do parsing?}
\end{quote}
%Later in this paper we will show that moderately sized attention networks can already perform approximate inference on complicated PCFG grammars.

\looseness=-1 This is not an idle question as \citet{maudslay2021syntactic} suggested that linear probing relies on semantic cues for parsing. They created syntactically correct but semantically meaningless sentences and found a significant drop in parsing performance compared to previous studies.  %Thus, the second question is
\begin{quote}
\centering
    {\em (Qs 2): Do BERT-like models trained for masked language modeling (MLM) encode syntax, and if so, how and why?}
\end{quote}

\subsection{This paper}
%Write about the dependence between data distribution and the model's performance. PCFG is one distribution to look at!
%To understand the properties of a pre-trained model, we . However, in order to understand the relation between pre-training loss and 

\looseness=-1To address Qs 1, we construct a transformer that executes the Inside-outside algorithm for PCFG (\Cref{sec:construct-io}). If the PCFG has $N$ non-terminals and the length of the sentence is $L$, our constructed transformer has $2L$ layers in total, $N$ attention heads, and $2NL$ embedding dimensions in each layer. However, this is massive compared to BERT.
For PCFG learned on Penn Treebank (\dataset{PTB})~\citep{marcus1993building}, $N=1600$, average $L \approx 25$, which leads to a transformer with $80$k embedding dimension, depth $50$, and $1.6$k attention heads per layer. By contrast, BERT has $768$ embedding dimensions, $12$ layers, and $12$ attention heads per layer! 

\looseness=-1 One potential explanation could be that BERT does not do exact parsing but merely computes {\em some} information related to parsing. After all, linear probing didn't recover complete parse trees. It recovered trees with modest F1 score, such as $78.2\%$ for BERT~\citep{vilares2020parsing} and $82.6\%$ for RoBERTa~\citep{arps2022probing}. %as compared to the optimal parse tree.
To the best of our knowledge, no study has investigated parsing methods that  strategically discard information to do more efficient approximate parsing. Toward this goal, we design an approximate version of the Inside-Outside algorithm (\Cref{sec:approx-overview}), executable by a transformer with $2L$ layers, $15$ attention heads, and $40L$ embedding dimensions, while still achieving $>70\%$ F1 score for constituency parsing on \dataset{PTB} dataset~\citep{marcus1993building}. 
\iffalse
\haoyu{I feel like the following contains too many details for the intro} After getting the inside and outside probabilities, we can apply the Labelled Recall algorithm to get the parse trees and reach a decent constituency parsing score. Besides using the Labelled Recall algorithm to get the parse tree from the inside and outside probabilities, we also show that a simple greedy selection of the spans can also get a rough parse tree, which can be implemented by a simple head and illustrate more on why linear probing works.
\fi

\looseness=-1 Although realistic models can capture a fair amount of parsing information, it is unclear whether they need to do so for masked language modeling (MLM). After all, \citet{maudslay2021syntactic} suggested that linear probing picks up on semantic information that happens to correlate with parse trees.
To further explore this, we trained a (masked) language model on the synthetic text generated from a PCFG tailored to English text, separating syntax from semantics in a more rigorous manner than \citet{maudslay2021syntactic}.
%To better understand this, we tried to take semantics out of the picture as follows: Generate synthetic text according to a PCFG that was fitted to English text; then train a (masked) language model on the synthetic text.
%This is a more rigorous way to focus  on separating syntax from semantics than the more {\em ad hoc}  method of \citet{maudslay2021syntactic}.
%By modifying our earlier construction, 
\Cref{sec:mlmandio} notes that given such synthetic text, the Inside-Outside algorithm will minimize MLM loss. 
%{\sc do we know that some other parsing methods would not minimize MLM loss, or at least not be easily doable with attention at least as far as we know? if so note it here. Something like: "Note that parsing could in principle  be done by other algorithms, but.." }   
Note that parsing algorithms like CYK~\citep{kasami1966efficient} could be used instead of Inside-Outside, but they do not have an explicit connection to MLM (\Cref{sec:mlmandio}).
Experiments with pre-trained models on synthetic PCFG data (\Cref{sec:pretrain-pcfg}) reveal the existence of syntactic information inside the models: simple probing methods recover reasonable parse tree structure (\Cref{sec:parse}).
%, though interestingly, the quality is better when probing using a 2-layer net instead of linear probes. 
Additionally, probes of contextualized embeddings reveal correlations with the information computed by the Inside-Outside algorithm (\Cref{sec:probe-marginal-probs}).  This %is aligned with our construction and suggests that% which supports our construction that 
suggests transformers implicitly engage in a form of approximate parsing, in particular a process related to the Inside-Outside algorithm, to achieve low MLM loss.
%may implicitly use some mechanisms correlated to the Inside-Outside algorithm.

\iffalse
{\sc guys, don't duplicate information; just put links to the sections in the paras above}

\rong{The paragraphs below seems a bit repetitive.}
Our contribution can be summarized as follow:
\begin{enumerate}
    \item Given any PCFG with $N$ non-terminals, we can construct an attention model with $2L$ layers in total, $N$ attention heads in each layer, and $2NL$ embedding dimensions such that for any sentence with length at most $L$ generated by the PCFG, the attention model can execute the Inside-outside algorithm (\Cref{sec:construction}). We show that Inside-outside algorithm is optimal for masked language modeling on synthetic PCFG data (\Cref{sec:mlmandio}), and conjecture that pre-training using masked language modeling has the implicit bias to embed the Inside-outside algorithms or the syntactic information. For the PCFG learned from \dataset{PTB} dataset, we can compress the attention model to $15$ attention heads in each layer and $40L$ dimensional embeddings without losing much on the parsing performance (\Cref{sec:approx-overview}).
    \item We also pre-train the RoBERTa model on the synthetic PCFG data (\Cref{sec:pretrain-pcfg}) and empirically show that probing on the model pre-trained on synthetic PCFG data can still recover the constituency parse tree decently well (\Cref{sec:parse}), verifying that pre-training using MLM indeed contains structural information. Furthermore, we probe the pre-trained model and find that the model contains the information computed by the Inside-outside algorithm, suggesting that the algorithm used by the trained model has a correlation with the Inside-outside algorithm (\Cref{sec:probe-marginal-probs}).
\end{enumerate}
\fi

\iffalse
Our paper is organized as follows: in Section \ref{}, we give the background knowledge and preliminaries; in Section \ref{}, we show that there exists an attention model that can execute the Inside-outside algorithm given the PCFG parameters, and thus can parse consequently; in Section \ref{}, we apply different approximation techniques on the execution of Inside-outside algorithms on PCFG learn from English corpus, and drastically reduce the size of constructed attention model showed in Section \ref{}; finally in Section \ref{}, we bridge the parsing with the pre-training from the view of PCFG. We use linear probing to show that the language models pre-trained from PCFG data indeed contain more syntactic information belonged the mask predictions. 
\fi