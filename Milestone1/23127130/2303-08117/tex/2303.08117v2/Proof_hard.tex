\begin{proof} 
The first $L-1$ layers simulate the recursive formulation of the Inside probabilities from eq.~\ref{eq:inside_probability}, and the last $L-1$ layers simulate the recursive formulation of the outside probabilities from  eq.~\ref{eq:outside_probability}. The model uses embeddings of size $4|\gN| L + L$, where the last $L$ coordinates serve as one-hot positional embeddings and are kept unchanged throughout the model. 

\paragraph{Notations:} For typographical simplicity, we will divide our embeddings into 5 sub-parts. We will use the first $2 |\gN| L$ coordinates to store the inside probabilities, the second $2 |\gN| L$ coordinates to store the outside probabilities, and the final $L$ coordinates to store the one-hot positional encodings. For every position $i$ and span length $\ell+1$, we store the inside probabilities $\{\alpha(A, i, i+\ell)\}_{A \in \gN}$ after computation in its embedding at coordinates $[|\gN|\ell, |\gN|(\ell+1))$. Similarly
we store $\{\alpha(A, i-\ell, i)\}_{A \in \gN}$ at $[|\gN|(L+\ell), |\gN|(L+\ell+1))$, $\{\beta(A, i, i+\ell)\}_{A \in \gN}$ at $[|\gN|(2L+\ell), |\gN|(2L+\ell+1))$, and $\{\beta(A, i-\ell, i)\}_{A \in \gN}$ at $[|\gN|(3L+\ell), |\gN|(3L+\ell+1))$ respectively. For simplicity of presentation, we won't handle cases where $i+\ell$ or $i-\ell$ is outside the range of $1$ to $L$ - those coordinates will be fixed to 0. % we cross the input boundaries in the computations. We presume that all such computations are set as $0$.

%Wherever applicable, for every $A \in \gN$ and $k \le L$, we will use $(k, A), (k, A) + |\gN| L, (k, A) + 2|\gN| L, (k, A) + 3|\gN| L$ to denote the coordinates that store $\alpha(A, i, i + k - 1), \alpha(A, i - k + 1, i), \beta(A, i, i + k - 1),$ and $\beta(A, i - k + 1, i)$ respectively. 



\paragraph{Token Embeddings:} The initial embeddings for each token $w$ will contain $\Pr[A \rightarrow w]$ for all $A \in \gP$. This is to initiate the inside probabilities of all spans of length $1$. 
Furthermore, the tokens will have a one-hot encoding of their positions in the input in the last $L$ coordinates. 

%For token at position $1$, $(L, \text{ROOT}) + 2|\gN| L$ coordinate will store $1$, and for token at position $L$, $(L, \text{ROOT}) + 3|\gN| L$ coordinate will store $1$, to initiate $\beta( \text{Root}, 1, L ) = 1$.
%The rest of the coordinates, except the position coordinates, are set as $0$.


\paragraph{Inside probabilities:} The contextual embeddings at position $i$ after the computations of any layer $\ell < L$ contains the inside probabilities of all spans of length at most $\ell + 1$ starting and ending at position $i$, i.e.  $\alpha(A, i, i + k)$ and $\alpha(A, i-k, i)$ for all $A \in \gN$ and $k \le \ell$. The rest of the coordinates, except the position coordinates, contain $0$.




%Generally speaking, the first $|\gN| L$ coordinates record the inside probabilities for spans starting at position $i$, and $(|\gN| L+1)$-th to $2|\gN| L$-th record the inside probabilities for spans ending at position $i$. For layer $\ell \le L$, $(2|\gN| L+1)$-th to $4|\gN| L$-th are $0$. 




%As for the attention score between $(\ve_i^{(\ell)}$ and $\ve_j^{(\ell)}$, we make it
%\begin{align*}
%    & \text{Attn}_{\mK_e^{(\ell)}, \mQ_e^{(\ell)}, \mK_p^{(\ell)}, \mQ_p^{(\ell)}}(i,j) \\
%    =&  (\vv_i^{(\ell)})^\top (\mK_e^{(\ell)})^\top \mQ_e^{(\ell)} \vv_j^{(\ell)} \\
%    & \quad + \vp_i^\top (\mK_p^{(\ell)})^\top \mQ_p^{(\ell)} \vp_j \\
%    =&  (\vv_i^{(\ell)})^\top (\mK_e^{(\ell)})^\top \mQ_e^{(\ell)} \vv_j^{(\ell)} + |p_i^{(\ell)} - p_j^{(\ell)}|.
%\end{align*}

%\paragraph{First layer}
%At the first layer $\ell = 1$, for the embedding at $i$-th position, the coordinate $(1,A)$ and $(1,A)+|\gN|L$ denote the inside probability $\alpha(A,i,i)$, which can be initialized from the word embedding matrix. All other coordinates except the positional embeddings are set to $0$.

\paragraph{Layer $1 \le \ell < L$: }
At each position $i$, this layer computes the inside probabilities of spans of length $\ell+1$ starting and ending at $i$, using the recursive formulation from eq.~\ref{eq:inside_probability}. 

%Recall that the recursive definition of the inside probabilities is given by
%We use $\gN$ attention heads 
For every non-terminal $A \in \gN$, we will use a unique attention head to compute $\alpha(A, i, i + \ell)$ at each token $i$. Specifically, the attention head representing non-terminal $A \in \gN$ will represent the following operation at each position $i$:  

{\small
\begin{align}
    &\alpha(A, i, j) \nonumber \\
    =&  \sum_{B, C \in \gN} \sum_{k=i}^{j-1}\Pr[A \rightarrow B C]  \cdot \alpha(B, i, k) \cdot \alpha(C, k+1, j) \nonumber \\
    =& \sum_{B, C \in \gN} \sum_{ \substack{\ell_1, \ell_2 \ge 0\\ \ell_1 + \ell_2 = \ell-1}
     }\Pr[A \rightarrow B C] \nonumber \\
     &\quad \cdot \alpha(B, i, i+\ell_1) \cdot \alpha(C, j-\ell_2, j) \label{eq:construction-inside-computation},
\end{align}
}

where $j = i+\ell$. In the final step, we modified the formulation to represent the interaction of spans of different lengths starting at $i$ and ending at $j$. We represent this computation as the attention score $a_{i, j}$ using a key matrix $\mK_{A}^{(\ell)}$ and query matrix $\mQ_{A}^{(\ell)}$.
%We want the $A$-th attention head in layer $\ell$ compute the score
%\begin{align*}
%& \sum_{A \rightarrow BC \in R}\sum_{k=i+1}^{i+\ell-2} (P(A \rightarrow B C) \\
%& \quad \times \alpha(B, i, k) \times \alpha(C, k+1, i+\ell-1)),
%\end{align*}
%which is exactly the inside probability $\alpha(A,i,j)$ for all $j = i+\ell-1$.
%Note that there exists a matrix $\mM_{A}^{(\ell)}$ such that
%{\small
%\begin{align*}
%    & \dotp{\vv_i^{(\ell-1)}}{\mM_{A}^{(\ell)} \cdot \vv_{i+\ell-1}^{(\ell-1)}} \\
%    =& \sum_{A \rightarrow BC }\sum_{k=i+1}^{i+\ell-2} P(A \rightarrow B C) \alpha(B, i, k) \alpha(C, k+1, j)).
%\end{align*}
%}

%$\{(L + k, \ell - k)\}_{1 \le k \le \ell}$ 
\paragraph{Computing Eq.~\ref{eq:construction-inside-computation}} We set the Key matrix $\mK_{A}^{(\ell)}$ as $\mI$. The Query matrix  $\mQ_{A}^{(\ell)}$ is set such that if we define $\mP_A\in \R^{|\gN|\times |\gN|}$ that contains $\{\Pr[A \to BC]\}_{B,C \in \gN},$ $\mP_A$ appears at positions $(|\gN| (L + \ell_2), |\gN|  \ell_1 )$ for all $\ell_1, \ell_2 \ge 0$ with $\ell_1 + \ell_2 = \ell - 1$. Finally, $\mQ_{A}^{(\ell)}$ contains $\mQ_p\in \R^{L \times L} $ at position $(4|\gN|L, 4|\gN|L)$, such that $\mQ_p[i,i+\ell] = 0$ for $0 \le i < L$, with the rest set to $-\zeta$ for some large constant $\zeta$. The rest of the blocks are set as $0$. We give an intuition behind the structure of $\mQ_{A}^{(\ell)}$ below.

\paragraph{Intuition behind $\mQ_{A}^{(\ell)}$:} For any position $i$ and range $\ell_1 \le \ell$,  $\ve_i^{(\ell-1)}$ contains the inside probabilities $\{ \alpha(C, i - \ell_1, i) \}_{C \in \gN}$ in the coordinates $[|\gN| (L+\ell_1), |\gN| (L+\ell_1+1) )$, while it contains the inside probabilities $\{ \alpha(B, i, i + \ell_1) \}_{B \in \gN}$ in the coordinates $[|\gN| \ell_1, |\gN| (\ell_1+1) ).$ Hence, if we set 
%all blocks at positions $\{(|\gN| (L + \ell_1), |\gN| \ell_2)\}_{\ell_1, \ell_2 \le \ell}$ 
the block  at position $(|\gN| (L + \ell_2), |\gN| \ell_1)$ in $\mQ_{A}^{(\ell)}$
to $\mP_A$ for some $0 \le \ell_1, \ell_2 \le \ell$, with the rest set to $0$, we can get for any two positions $i, j$,

{\small
\begin{align*}
    & (\mK_{A}^{(\ell)} \ve_j^{(\ell-1)})^{\top}  \mQ_{A}^{(\ell)} \ve_i^{(\ell-1)}\\
    =&  \sum_{B, C \in \gN}   \Pr[A \to B C] \cdot \alpha(B, i, i+\ell_1) \cdot  \alpha (C, j -\ell_2, j) .
\end{align*}
}

Because we want to involve the sum over all $\ell_1, \ell_2$ pairs with $\ell_1 + \ell_2 = \ell - 1$, we will set blocks at positions $\{(|\gN| (L + \ell_2), |\gN| \ell_1 )\}_{\ell_1, \ell_2 : \ell_1 + \ell_2 = \ell-1}$ to $\mP_A$, while setting the rest to $0$. This gives us

{\small
\begin{align*}
    &(\mK_{A}^{(\ell)} \ve_j^{(\ell-1)})^{\top} \mQ_{A}^{(\ell)} \ve_i^{(\ell-1)} \\
    =& \sum_{B, C \in \gN} \sum_{\substack{\ell_1, \ell_2 \ge 0\\ \ell_1 + \ell_2 = \ell-1}}  \Pr[A \to B C] \cdot \alpha(B, i, i+\ell_1) \\
    &\quad \cdot  \alpha (C, j -\ell_2, j) .
\end{align*}
}

However, we want $(\mK_{A}^{(\ell)} \ve_j^{(\ell-1)})^{\top} \mQ_{A}^{(\ell)} \ve_i^{(\ell-1)}$ to compute $\alpha(A, i, j)$ iff $j = i + \ell$ and $0$ otherwise, so we will use the final block in $\mQ_{A}^{(\ell)}$ that focuses on the one-hot position encodings of $i$ and $j$ to differentiate the different location pairs. Specifically, the final block $\mQ_p$ will return $0$ if $j = i + \ell$, while it returns $-\zeta$ for some large constant $\zeta$ if $j \ne i + \ell$. This gives us

{\small
\begin{align}
    &(\mK_{A}^{(\ell)} \ve_j^{(\ell-1)})^{\top} \mQ_{A}^{(\ell)} \ve_i^{(\ell-1)} \nonumber \\
    =&  \zeta(\mathbb{I}[j - i = \ell] - 1) + \sum_{B, C \in \gN} \sum_{\substack{\ell_1, \ell_2 \ge 0\\ \ell_1 + \ell_2 = \ell-1}}  \Pr[A \to B C] \nonumber \\
    &\quad \cdot \alpha(B, i, i+\ell_1) \cdot  \alpha (C, j -\ell_2, j). \label{eq:attnt_head_inside_construct}
\end{align}
}
With the inclusion of the term $\zeta(\mathbb{I}[j - i = \ell ] - 1)$, we make $(\mK_{A}^{(\ell)} \ve_j^{(\ell-1)})^{\top} \mQ_{A}^{(\ell)} \ve_i^{(\ell-1)}$ positive if $j - i = \ell$, and negative if $j - i \ne \ell$. Applying a ReLU activation on top will zero out the unnecessary terms, leaving us with $\alpha(A, i, i+\ell)$ at each location $i$.


 




%Specifically, denote $\mP_A\in \R^{|\gN|\times |\gN|}$ the probabilities of rules splitted from non-terminal $A$. $\mM_{A}^{(\ell)}$ is a $4L\times 4L$ block matrix where each block has size $|\gN|\times |\gN|$. All the blocks except $(|\gN|L+k_1, k_2)$ where $k_1+k_2 = \ell$ in $\mM_{A,\ell}$ are zero, and the blocks at position $(|\gN|L+k_1, k_2)$ are $\mP_A$. To implement this as self-attention, we can set $\mK_A^{(\ell)} = \mI$ the identity matrix, and $\mQ_A^{(\ell)} = \mM_A^{(\ell)}$.

%We also need to make sure that the attention score between $i$ and $i+\ell'-1$ for $\ell'\neq \ell$ is smaller than the attention score between $i$ and $i+\ell-1$ (and thus the hard attention head will compute the probability between $i$ and $i+\ell-1$ by taking the max). We can achieve this goal by using positional embeddings. Specifically, if we use the one-hot positional embeddings, i.e., $\vp_i[i] = 1$ if the position is $i$ and $0$ otherwise. Then, we can set $\mK_p^{(\ell)} = \mI$, and $\mQ_p^{(\ell)}[i,i+\ell-1] = 0$ and all other entries to be $-1$. Then, $\vp_i^\top \mK_p^\top \mQ_p \vp_{i+\ell-1} = 0$ and $\vp_i^\top \mK_p^\top \mQ_p \vp_{j} = -1$ for all $j \neq i+\ell-1$, and using $|N|$ hard attentions together we can compute $\alpha(A,i,i+\ell-1)$.

Similarly, we use another $|\gN|$ attention heads to compute $\alpha(A,i-\ell, i)$. In the end, we use the residual connections to copy the previously computed inside probabilities $\alpha(A,i-\ell', i)$ and $\alpha(A,i, i+\ell')$ for $\ell' < \ell$.

%in order to compute $\sum_{\ell_1, \ell_2 \le \ell} \sum_{B, C \in \gN} \Pr[A \to B C] \cdot \alpha(B, i, i+\ell_1) \cdot  \alpha (C, j -\ell_2, j)$, 

%we can set all the first $4L$ blocks to $\mP_A\in \R^{|\gN|\times |\gN|}$ that contains $\{\Pr[A \to BC]\}_{B,C \in \gN}.$


%$\mQ_{A, \ell}$ is defined as a block-diagonal matrix, where all $\{(L + k, \ell - k)\}_{1 \le k \le \ell}$ blocks store $\mP_A\in \R^{|\gN|\times |\gN|}$ that contains $\{\Pr[A \to BC]\}_{B,C \in \gN}.$ The last block is a $L \times L$ matrix $\mQ_p$ such that $\mQ_p[i,i+\ell-1] = 0$ for $0 \le i < L$, with the rest set to $-1$. The rest of the blocks are set as $0$. The explanation for this structure of $\mQ_{A, \ell}$ is as follows.



%\haoyu{revise to here, following also needs to be revised.}

\paragraph{Outside probabilities:}

%For layer $\ell > L$, the $(k,A)$-th coordinate denotes the inside probability $\alpha(A, i+1, i+k)$ and the $(k,A)+|\gN|L$-th coordinate denotes the inside probability $\alpha(A, i-k, i-1)$ for all $k$. (Note that now position $i$ stores the inside probabilities for spans starting at $i+1$ and ending at $i-1$.) The $(k,A) + 2|\gN| L$-th coordinate denotes the outside probability $\beta(A, i, i+k-1)$, and the $(k,A) + 3|\gN| L$-th coordinate denotes the outside probability $\beta(A, i-k+1, i)$ at the current step ($\beta(A, i, i+k-1) = \beta(A, i-k+1, i) = 0$ for all $k > \ell - L$. The last $L$ coordinates denote the positional embeddings. We denote the first $4|\gN| L$ coordinates of $\ve_i^{(\ell)}$ to be $\vv_i^{(\ell)}$, and the positional embedding in $\ve_i^{(\ell)}$ as $\vp_i$. Thus, $\ve_i^{(\ell)} = (\vv_i^{(\ell)}, \vp_i$

In addition to all the inside probabilities, the contextual embeddings at position $i$ after the computations of any layer $(2L - 1) - \ell$ ($\ge L$) contain the outside probabilities of all spans of length at least $\ell + 1$ starting and ending at position $i$, i.e.  $\beta(A, i, i + k)$ and $\beta(A, i - k, i)$ for all $A \in \gN$ and $k \ge \ell $. The rest of the coordinates, except the position coordinates, contain $0$.

\paragraph{Layer $L$}
In this layer, we initialize the outside probabilities $\beta(\text{ROOT}, 1, L) = 1$ and $\beta(A, 1, L) = 0$ for $A\neq \text{ROOT}$. Furthermore, we move the inside probabilities $\alpha(A,i+1,i+k)$ from position $i+1$ to position $i$, and $\alpha(A,i-k,i-1)$ from position $i-1$ to position $i$ using 2 attention heads. 

\paragraph{Layer $L + 1 \le \tilde{\ell} := (2L - 1) - \ell \le 2L - 1$:} 
At each position $i$, this layer computes the outside probabilities of spans of length $\ell + 1$ starting and ending at $i$, using the recursive formulation from eq.~\ref{eq:outside_probability}. The recursive formulation for $\beta(A, i, i + \ell)$ for a non-terminal $A \in \gN$ has two terms, given by

{\small
\begin{align}
     \beta(A,i,j) =& \beta_1(A,i,j) + \beta_2(A, i, j), \text{ with   } \nonumber \\
    \beta_1(A,i,j) =& \sum_{C,B \in \gN} \sum_{k=1}^{i-1} \Pr[B \to C A] 
    \nonumber \\
    &\quad\cdot \alpha(C, k, i-1) \beta(B, k, j), \text{ and } \label{eq:beta1} \\
    \beta_2(A, i, j) =& \sum_{B,C \in \gN} \sum_{k=j+1}^{L} \Pr[B \to A C] \nonumber \\
    &\quad \cdot \alpha(C, j+1, k) \beta(B, i, k), \label{eq:beta2}
\end{align}
}

where $j = i + \ell.$ For each non-terminal $A \in \gN$, we will use two unique heads to compute $\beta(A, i, i+\ell)$
, each representing one of the two terms in the above formulation. We outline the construction for $\beta_1$; the construction for $\beta_2$ follows similarly.




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\paragraph{Computing Eq.~\ref{eq:beta1}} We build the attention head in the same way we built the attention head to represent the inside probabilities in eq.~\ref{eq:attnt_head_inside_construct}. Similar to \ref{eq:attnt_head_inside_construct}, we modify the formulation of $\beta_1$ to highlight the interaction of spans of different lengths.

{\small
\begin{align}
    \beta_1(A,i,j) =& \sum_{B, C \in \gN} \sum_{\substack{\ell_1, \ell_2 \ge 0\\ \ell_2 - \ell_1 = \ell }} \Pr[B \to C A] \nonumber \\
    & \quad \cdot \alpha(C, i-\ell_1, i-1) \beta(B, j-\ell_2, j), \label{eq:construction-beta1}
\end{align}
}

where $j = i+\ell$. We represent this computation as the attention score $a_{i, i+\ell}$ using a key matrix $\mK_{A, 1}^{(\tilde{\ell})}$ and query matrix $\mQ_{A, 1}^{(\tilde{\ell})}$. 
%We want the $A$-th attention head in layer $\ell$ compute the score
%\begin{align*}
%& \sum_{A \rightarrow BC \in R}\sum_{k=i+1}^{i+\ell-2} (P(A \rightarrow B C) \\
%& \quad \times \alpha(B, i, k) \times \alpha(C, k+1, i+\ell-1)),
%\end{align*}
%which is exactly the inside probability $\alpha(A,i,j)$ for all $j = i+\ell-1$.
%Note that there exists a matrix $\mM_{A}^{(\ell)}$ such that
%{\small
%\begin{align*}
%    & \dotp{\vv_i^{(\ell-1)}}{\mM_{A}^{(\ell)} \cdot \vv_{i+\ell-1}^{(\ell-1)}} \\
%    =& \sum_{A \rightarrow BC }\sum_{k=i+1}^{i+\ell-2} P(A \rightarrow B C) \alpha(B, i, k) \alpha(C, k+1, j)).
%\end{align*}
%}
%$\{(L + k, \ell - k)\}_{1 \le k \le \ell}$ 
First, we set the Key matrix $\mK_{A, 1}^{(\tilde{\ell})}$ as $\mI$. If we define $\mP_{A, r} \in \R^{|\gN|\times |\gN|}$ as a matrix that contains $\{\Pr[B \to CA]\}_{B,C \in \gN},$ which is the set of all rules where $A$ appears as the right child, $\mQ_{A, 1}^{(\tilde{\ell})}$ is set such that $\mP_{A, r}$ appears at positions $[|\gN| (3L + \ell_2), |\gN| (L + \ell_1))$ for all $0 \le \ell_1, \ell_2 \le L$ that satisfy $\ell_2 - \ell_1 = \ell$. Finally, $\mQ_{A, 1}^{(\tilde{\ell})}$ contains $\mQ_p\in \R^{L \times L} $ at position $(4|\gN|L, 4|\gN|L)$, such that $\mQ_p[i,i+\ell] = 0$ for $0 \le i < L$, with the rest set to $-\zeta$ for some large constant $\zeta$. The rest of the blocks are set as $0$. We give an intuition behind the structure of $\mQ_{A,1}^{(\tilde{\ell})}$ below.

\paragraph{Intuition for $\mQ_{A, 1}^{(\tilde{\ell})}$:}  For position $i$ and any ranges $1 \le \ell_1 < L$, $\ell+1 \le \ell_2 \le L$, $\ve_i^{( \tilde{\ell} - 1 )}$ contains the inside probabilities $\{ \alpha(C, i - \ell_1, i-1) \}_{C \in \gN}$ in the coordinates $[ |\gN| (L+\ell_1), |\gN| (L+\ell_1+1) )$, while it contains the outside probabilities $\{ \beta(B, i - \ell_2, i) \}_{B \in \gN}$ in the coordinates $[|\gN| (3L+\ell_2), |\gN| (3L+\ell_2+1) ).$ Hence, if we set the block at position $(|\gN| (3L + \ell_2), |\gN| (L + \ell_1))$
to $\mP_A$ for some $0 \le \ell_1 \le L, \ell+1 \le \ell_2 \le L$, with the rest set to $0$, we can get for any two positions $i, j$,

{\small
\begin{align*}
    & (\mK_{A}^{(\tilde{\ell})} \ve_j^{(\tilde{\ell}-1)})^{\top}  \mQ_{A}^{(\tilde{\ell})} \ve_i^{(\tilde{\ell}-1)} \\
    =&  \sum_{B, C \in \gN}   \Pr[B \to CA] \cdot \alpha(C, i-\ell_1, i-1) \cdot  \beta (B, j -\ell_2, j) .
\end{align*}
}


%Hence, if we set all blocks at positions $\{(|\gN| (2L + \ell_1), |\gN| \ell_2)\}_{\ell_1 \le \ell, \ell_2 \ge L-\ell}$ to $\mP_A$, with the rest set to $0$, we can get for any two positions $i, j$,
%\begin{align*}
%    & (\mK_{A}^{(\ell)} \ve_j^{(\ell-1)})^{\top}  \mQ_{A}^{(\ell)} \ve_i^{(\ell-1)} = \sum_{B, C \in \gN} \sum_{\ell_1, \ell_2 \le \ell}   \Pr[A \to B C] \cdot \alpha(B, i, i+\ell_1) \cdot  \alpha (C, j -\ell_2, j) .
%\end{align*}

Because we want to include the sum over $\ell_1, \ell_2$ pairs with $ \ell_2 - \ell_1 = \ell$, we will only set blocks at positions $[|\gN| (3L + \ell_2), |\gN| (L + \ell_1))$ for all $0 \le \ell_1, \ell_2 \le L$ that satisfy $\ell_2 - \ell_1 = \ell$ to $\mP_{A, r}$, while setting the rest to $0$. This gives us


\begin{align*}
    &(\mK_{A}^{(\tilde{\ell})} \ve_j^{(\tilde{\ell}-1)})^{\top}  \mQ_{A}^{(\tilde{\ell})} \ve_i^{(\tilde{\ell}-1)} \\
    =& \sum_{B, C \in \gN}  \sum_{\substack{\ell_1, \ell_2 \ge 0 \\ \ell_2 - \ell_1 = \ell}}  \Pr[B \to CA] \\
    &\quad\cdot \alpha(C, i-\ell_1, i-1) \cdot  \beta (B, j -\ell_2, j).
\end{align*}


Because we want $(\mK_{A}^{(\tilde{\ell})} \ve_j^{(\tilde{\ell}-1)})^{\top}  \mQ_{A}^{(\tilde{\ell})} \ve_i^{(\tilde{\ell}-1)}$ to compute $\beta_1(A, i, j)$ with $j = i + \ell$ and $0$ otherwise, we will use the final block in $\mQ_{A}^{(\ell)}$ that focuses on the one-hot position encodings of $i$ and $j$ to differentiate the different location pairs. Specifically, the final block $\mQ_p$ will return $0$ if $j = i + \ell$, while it returns $-\zeta$ for some large constant $\zeta$, if $j \ne i + \ell$. This gives us

{\small
\begin{align*}
    & (\mK_{A}^{(\tilde{\ell})} \ve_j^{(\tilde{\ell}-1)})^{\top}  \mQ_{A}^{(\tilde{\ell})} \ve_i^{(\tilde{\ell}-1)} \\
    =& \zeta (\mathbb{I}[j - i = \ell ] - 1) + \sum_{B, C \in \gN}  \sum_{\substack{\ell_1, \ell_2 \ge 0\\ \ell_2 - \ell_1 = \ell} } \Pr[B \to CA] \\
    &\quad \cdot \alpha(C, i-\ell_1, i-1) \cdot  \beta (B, j -\ell_2, j) 
\end{align*}
}

With the inclusion of the term $\zeta(\mathbb{I}[j - i = \ell ] - 1)$, we make $(\mK_{A}^{(\tilde{\ell})} \ve_j^{(\tilde{\ell}-1)})^{\top}  \mQ_{A}^{(\tilde{\ell})} \ve_i^{(\tilde{\ell}-1)}$ positive if $j - i = \ell$, and negative if $j - i \ne \ell$. Applying a ReLU activation on top will zero out the unnecessary terms, leaving us with $\beta_1(A, i, i+\ell)$ at each location $i$.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%






\iffalse
Now at layer $\ell + L$ where $1 < \ell \le L$, we compute $\beta(A,i,i+L-\ell)$ for $i \le \ell$ and $\beta(A,i-L+\ell,i)$ for $i \ge \ell$ at position $i$. We first compute $\beta(A,i,i+L-\ell)$, which needs $2$ attention heads together. First note that there exist two matrices $\mM_{A,1}^{(\ell+L)}$ and $\mM_{A,2}^{(\ell+L)}$ such that
{\small
\begin{align*}
    &\dotp{\vv_i^{(\ell-1+L)}}{\mM_{A,1}^{(\ell+L)} \cdot \vv_{i+L-\ell}^{(\ell-1+L)}}
    = \sum_{B \rightarrow CA} \sum_{k=1}^{i-1}P[B \rightarrow C A]
    \cdot\alpha(C, k, i-1) \beta(B, k, i+L-\ell) \\
    &\dotp{\vv_i^{(\ell-1+L)}}{\mM_{A,2}^{(\ell+L)} \cdot \vv_{i+L-\ell}^{(\ell-1+L)}}
    = \sum_{B \rightarrow A C} \sum_{k=i+L-\ell+1}^{L}P[B \rightarrow A C] \cdot\alpha(C, i+L-\ell+1, k) \beta(B, i, k).
\end{align*}
}
Specifically, we denote $\mP_{A,r} \in \R^{|\gN|\times |\gN|}$ the probabilities of rules that $A$ is the right child (the different columns of $\mP_{A,r}$ denote different parent non-terminals $B$ and different rows denote different left child $C$). We denote $\mP_{A,l} \in \R^{|\gN|\times |\gN|}$ the probabilities of rules that $A$ is the left child (the different rows of $\mP_{A,l}$ denote different parent non-terminals $B$ and different columns denote different right child $C$).

Then, $\mM_{A,1}^{(\ell+L)}$ is a $4L\times 4L$ block matrix where each block has size $|\gN|\times |\gN|$. All the blocks except $(|\gN|L+k_1, 3|\gN|L+k_2)$ where $k_2-k_1 = L - \ell+1$ in $\mM_{A,1}^{(\ell+L)}$ are zero, and the blocks at position $(|\gN|L+k_1, 3|\gN|L+k_2)$ are $\mP_{A,r}$. Similarly, $\mM_{A,2}^{(\ell+L)}$ is a $4L\times 4L$ block matrix where each block has size $|\gN|\times |\gN|$. All the blocks except $(2|\gN|L+k_1,k_2)$ where $k_1 - k_2 = L - \ell+1$ in $\mM_{A,2}^{(\ell+L)}$ are zero, and the blocks at position $(2|\gN|L+k_1,k_2)$ are $\mP_{A,l}$.
\fi 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Besides, we also need $2|\gN|$ additional heads for the outside probabilities $\beta(A,i-\ell,i)$. In the end, we use the residual connections to copy the previously computed inside probabilities $\beta(A, i-\ell', i)$ and $\alpha(A, i, i+\ell')$ for $\ell' > \ell$.
%and similar positional embeddings to make sure that $i$ is only attended to $i+L-\ell$ by using the ``hard attention''
\end{proof}

