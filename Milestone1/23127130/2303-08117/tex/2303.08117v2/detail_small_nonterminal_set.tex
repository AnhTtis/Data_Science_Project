We hypothesize that we can focus only on a few non-terminals while retaining most of the performance.

\begin{hypothesis}\label{hyp:small_nonterminal_subset}
     For the PCFG $\gG = (\gN, \gI, \gP, n, p)$ learned on the English corpus, there exists $\tilde\gI\subset\gI,\tilde\gP\subset\gP$ with $|\tilde\gI|\ll |\gI|, |\tilde\gP|\ll |\gP|$, such that simulating Inside-Outside algorithm with $\tilde\gI \cup \tilde\gP$ non-terminals introduces \underline{small} error in the 1-mask perplexity and has \underline{minimal} impact on the parsing performance of the Labeled-Recall algorithm.
\end{hypothesis}


To find candidate sets $\tilde\gI,\tilde\gP$ for our hypothesis, we check the frequency of different non-terminals appearing at the head of spans in the parse trees of the \dataset{PTB}~\citep{marcus1993building} training set. We consider the Chomsky-transformed (binarized) parse trees for sentences in the \dataset{PTB} training set, and collect the labeled spans $\{(A, i, j)\}$ from the parse trees of all sentences. For all non-terminals $A$, we compute $\text{freq}(A)$, which denotes the number of times non-terminal $A$ appears at the head of a span. %Formally, 
%\[\text{freq}(A,\ell) := \sum_{(B,j,j')\in \{T_i\}_i}\mathbb I\{A = B, j'-j+1 = \ell\}.\]
\Cref{fig:freq-dist} shows the plot of $\text{freq}(A)$ for in-terminals and pre-terminals, with the order of the non-terminals sorted by the magnitude of $\text{freq}(\cdot)$. We observe that an extremely small subset of non-terminals have high frequency, which allows us to restrict our computation for the inside and outside probabilities to the few top non-terminals sorted by their $\text{freq}$ scores. We select the top frequent non-terminals as possible candidates for forming the set $\tilde\gN$.

%appears with high frequency in a specific length, and thus computing only the top-frequent non-terminals should not affect the computation a lot intuitively.

\begin{figure}[!t]
     \centering
     \iffalse
    \begin{subfigure}[t]{0.4\textwidth}
        \includegraphics[width=\linewidth]{figs/fig-in-global.png}
    \end{subfigure}
    \begin{subfigure}[t]{0.4\textwidth}
        \includegraphics[width=\linewidth]{figs/fig-pre-global.png}
    \end{subfigure}
    \fi
    \includegraphics[width=0.8\linewidth]{figs/fig-nt-global.png}
    \iffalse
    \begin{subfigure}[t]{0.48\textwidth}
        \includegraphics[width=\linewidth]{figs/nts-frequency.pdf}
    \end{subfigure}
    \fi
    
        \caption{Plot for the frequency distribution of in-terminals ($\gI$) and pre-terminals ($\gP$). We compute the number of times a specific non-terminal appears in a span of a parse tree in the \dataset{PTB} training set. We then sort the non-terminals according to their normalized frequency and then show the frequency vs. index plot.}
        \label{fig:freq-dist}
\end{figure}




%To further verify that with the approximated computation, we select the non-terminals $\gN^{(\ell)}$ that will be computed for spans with length $\ell$ greedily from $\text{freq}(A,\ell)$ (i.e., select the non-terminals with the highest frequency). Then we execute the approximated version of the Inside-Outside algorithm and compute the unlabelled F1 score.
\iffalse
\begin{table}[]
    \centering
    \begin{tabular}{|c|c|c|c|}
    \hline
         & Corpus F1 & Sent F1 & TV \\
         \hline
         \makecell{No approx.} & 75.90 & 78.77 & 0 \\
         \hline
         $|\gN^{(\ell)}| = 20$ & 62.49 & 61.17 & 0.054 \\
         $|\gN^{(\ell)}| = 30$ & 70.29 & 70.92 & 0.045 \\
         $|\gN^{(\ell)}| = 40$ & 72.41 & 72.97 & 0.029\\
         \hline
    \end{tabular}
    \caption{Unlabelled F1 scores for approximate Inside-Outside algorithm with very few non-terminals to compute in each layer on \dataset{PTB} development set. $\gN^{(\ell)}$ denotes the set of non-terminals to compute for layer $\ell$ and is selected to be the non-terminals with top frequency for spans with length $\ell$. The PCFG is learned on \dataset{PTB} training dataset. Besides the parsing F1 results, we also show the TV distance between the exact computation and the approximated computation for 1-masking prediction.}
    \label{tab:few-nt-pcfg}
\end{table}
\fi



We verify the effect of restricting our computation to the frequent non-terminals on the 1-mask perplexity and the unlabeled F1 score of the approximate Inside-Outside algorithm in \Cref{tab:few-nt-pcfg-global}. Recall from \Cref{thm:io-optimal-mlm}, the 1-mask probability distribution for a given sentence $w_1, \cdots, w_L$ at any index $i$ is given by \cref{eq:1mask_pcfg}, and thus we can use \cref{eq:1mask_pcfg} to compute the 1-mask perplexity on the corpus. To measure the impact on 1-mask language modeling, we report the perplexity of the original and the approximate Inside-Outside algorithm on 200 sentences generated from PCFG. 

%Besides the F1 score, we also compute the total variation distance between the 1-masking distribution computed by the Inside-Outside algorithm and the approximated version. 
We observe that restricting the computation to the top-$40$ and $45$ frequent in-terminals and pre-terminals leads to $<6.5\%$ increase in average 1-mask perplexity. % with the average TV distance of the masked token predictions between the original and the approximate Inside-Outside algorithm being only $0.05$. 
Furthermore, the Labeled-Recall algorithm observes at most $4.24\%$ drop from the F1 performance of the original PCFG. %and the total variation between distributions is $0.05$. \abhishek{What is TV between distributions here? Please explain} 
If we further restrict the computation to the top-$20$ and $45$ in-terminals and pre-terminals, we can still get $71.91\%$ sentence F1 score, and the increase in average 1-mask perplexity is less than $8.6\%$. However, restricting the computation to $10$ in-terminals leads to at least $15\%$ drop in parsing performance.

Thus combining \Cref{thm:soft_attnt} and \Cref{tab:few-nt-pcfg-global}, we have the following informal theorem.


\begin{theorem}[Informal]\label{thm:approx-few-nt-informal}
    Given the PCFG $\gG = (\gN, \gI, \gP, n, p)$ learned on the English corpus, there exist  subsets $\tilde\gI\subset\gI,\tilde\gP\subset\gP$ with $|\tilde\gI| = 20, |\tilde\gP| = 45$, and an attention model with soft relative attention modules (\ref{eq:soft_attention}) with embeddings of size $275+40L$, $2L+1$ layers, and $20$ attention heads in each layer, that can simulate the Inside-Outside algorithm restricted to $\tilde\gI,\tilde\gP$
    on all sentences of length at most $L$ generated from $\gG$. The restriction introduces a $9.29\%$ increase in average 1-mask perplexity and $8.71\%$ drop in the parsing performance of the Labeled-Recall algorithm. 
    %Parsing using this approximated Inside-Outside algorithm gives $68.41\%$ corpus F1 $71.91\%$ sentence F1 on \dataset{PTB} dataset.
    %There exists an attention model
    %By computing the inside and outside probabilities for only the top-$20$ non-terminals ($|\tilde\gN| = 20$), we can reduce the size of the model in \Cref{thm:soft_attnt} to $20$ attention heads in each layer, $540+40L$ embedding dimension, and $2L$ layers to approximately execute the Inside-Outside algorithm on PCFG learned on English corpus. Parsing using this approximated Inside-Outside algorithm gives us $68.41\%$ corpus F1 $71.91\%$ sentence F1 on \dataset{PTB} dataset.
\end{theorem}

\iffalse
\begin{table}[!t]
    \centering
    \footnotesize
    \begin{tabular}{|c|c|c|c|}
    \hline
         Approximation & Corpus F1 & Sent F1 & ppl. \\
         \hline
         \makecell{No approx.} & 75.90 & 78.77 & 50.80 \\
         \hline
         $|\tilde\gI| = 10,|\tilde\gP|=45$ & 57.14 & 60.32 & 59.57 \\
         $|\tilde\gI| = 20,|\tilde\gP|=45$ & 68.41 & 71.91 & 55.16 \\
         $|\tilde\gI| = 40,|\tilde\gP|=45$ & 72.45 & 75.43 & 54.09 \\
         \hline
    \end{tabular}
    \iffalse
    \begin{tabular}{|c|c|c|c|c|}
    \hline
         & Corpus F1 & Sent F1 & TV & ppl. \\
         \hline
         \makecell{No approx.} & 75.90 & 78.77 & 0 & 50.80 \\
         \hline
         $|\tilde\gN| = 10$ & 57.14 & 60.32 & 0.114 & 58.11 \\
         $|\tilde\gN| = 20$ & 68.41 & 71.91 & 0.073 & 55.16 \\
         $|\tilde\gN| = 40$ & 72.45 & 75.43 & 0.050 & 54.09 \\
         \hline
    \end{tabular}
    \fi
    \iffalse
    \begin{tabular}{|c|c|c|}
    \hline
         & Corpus F1 & Sent F1 \\
         \hline
         \makecell{No approx.} & 75.90 & 78.77 \\
         \hline
         $|\tilde\gN| = 10$ & 57.14 & 60.32  \\
         $|\tilde\gN| = 20$ & 68.41 & 71.91  \\
         $|\tilde\gN| = 40$ & 72.45 & 75.43  \\
         \hline
    \end{tabular}
    \fi
    \caption{Experiment results by approximately computing the Inside-Outside algorithm with very few non-terminals. We show the unlabelled F1 scores on \dataset{PTB} development set as well as the 1-masking perplexity. $\tilde\gI$ ($\tilde\gP$) denotes the set of in(pre)-terminals to compute and are selected to be the in(pre)-terminals with top frequency. The PCFG is learned on \dataset{PTB} training dataset. The ppl. column denote the 1-masking perplexity on 200 sentences generated from the learned PCFG. %Besides the parsing F1 results, we also show the TV distance between the exact computation and the approximated computation for 1-masking prediction.
    }
    \label{tab:few-nt-pcfg-global}
\end{table}
\fi

If we plug in the average length $L\approx 25$ for sentences in \dataset{PTB}, we can get a model with $20$ attention heads, $1275$ hidden dimension, and $51$ layers. Compared with the construction in \Cref{thm:soft_attnt}, the size of the model is much closer to reality. The proof of \Cref{thm:approx-few-nt-informal} is shown in \Cref{sec:approx-few-nt}.
%Besides, this approximation doesn't affect the parsing performance much: compared with parsing using the Inside-Outside algorithm that achieves $75.90\%$ corpus F1 and $78.77\%$ sentence F1 on \dataset{PTB} dataset, the approximated computation shows a drop by $8.71\%$. 