We hypothesize that we can  find linear transformation matrices $\{\mW^{(\ell)}\}_{\ell\le L}$ that can reduce the computations while retaining most of the performance, and our hypothesis is formalized as follow:
%The following informal theorem show the effectiveness of using this approximation, and please refer to \Cref{sec:approx-low-rank} for more details.

\begin{hypothesis}
     For the PCFG $\gG = (\gN, \gI, \gP, n, p)$ learned on the English corpus, there exists transformation matrices $\mW^{(\ell)}\in\R^{k^{(\ell)}\times |\tilde\gI|}$ for every $\ell \le L$, such that approximately simulating the Inside-Outside algorithm with $\{\mW^{(\ell)}\}_{\ell\le L}$ introduces \underline{small} error in the 1-mask perplexity and has \underline{minimal} impact on the parsing performance of the Labeled-Recall algorithm.
\end{hypothesis}



\Cref{tab:learned-transformation-global} verifies our hypothesis, and lead to \Cref{thm:approx-low-rank-informal}. Compared with the parsing results from \Cref{thm:approx-few-nt-informal}, the corpus and sentence F1 scores are nearly the same, and we further reduce the number of attention heads in each layer from $20$ to $15$. If we only use $10$ attention heads to approximately execute the Inside-Outside algorithm, we can still get $61.72\%$ corpus F1 and $65.31\%$ sentence F1 on \dataset{PTB} dataset, which is still much better than the Right-branching baseline. \Cref{thm:approx-low-rank-informal} shows that attention models with a size much closer to the real models (like BERT or RoBERTa) still have enough capacity to parse decently well (>70\% sentence F1 on \dataset{PTB}).

It is also worth noting that approximately executing the Inside-Outside algorithm using the transformation matrices $\{\mW^{(\ell)}\}_{\ell\le L}$ is very different from reducing the size of the PCFG grammar, since we use different matrix $\mW^{(\ell)}$ when computing the probabilities for spans with different length. If we choose to learn the same transformation matrix $\mW$ for all the layers $\ell$, the performance drops.
%Although the number of layers of our constructed models ($50$ to process average length sentence in \dataset{PTB}) are still relatively large compared to the models in reality ($12$ for BERT and RoBERTa), our approximations in \Cref{thm:approx-few-nt-informathm:approx-low-rank-informal} show strong hint that the PCFG learned on English corpus are ``highly compressible''.

\paragraph{More discussions on the transformation matrix $\mW^{(\ell)}$} We can observe that by introducing the transformation matrix $\mW^{(\ell)}$ generalized the first ingredient that only computes a small set of in-terminals $\tilde\gI$ and pre-terminals $\tilde\gP$, and in theory we can directly learn the transformation matrix $\mW^{(\ell)}$ from the original PCFG without reducing the size at first, i.e., $\mW^{(\ell)}\in\R^{k^{(\ell)}\times |\gI|}$. However empirically, if we directly learn $\mW^{(\ell)}$ from all the in-terminals $\gI$ but not from the top-20 frequent in-terminals $\tilde\gI$, the performance drops. Thus, we choose to learn the matrix $\mW^{(\ell)}$ starting from the most frequent in-terminals $\tilde\gI$. One possible explanation is that the learning procedure is also heuristic, and certainly may not learn the best transformation matrix.

Besides, we use the same transformation matrix $\mW^{(\ell)}$ when computing the inside and outside probabilities, and it is also natural to use different transformation matrices when computing the inside and outside probabilities. Recall that we learn the transformation $\mW^{(\ell)}$ by the Eigenvalue decomposition on matrix $\mX^{(\ell)}$, where $\mX^{(\ell)} = \sum_{s} \mX_s^{(\ell)} / \norm{\mX_s^{(\ell)}}_{\text{F}}$ and $\mX_s^{(\ell)} = \sum_{i,j:j-i=\ell} \vmu_s^{i,j}(\vmu_s^{i,j})^\top$. Then, we can also learn two matrices $\mW^{(\ell)}_{\text{inside}}$ and $\mW^{(\ell)}_{\text{outside}}$ through the Eigenvalue decomposition on matrices $\mX^{(\ell)}_{\text{inside}}$ and $\mX^{(\ell)}_{\text{outside}}$ respectively, where
\begin{align*}
    \mX^{(\ell)}_{\text{inside}} =& \sum_{s} \mX_{s,\text{inside}}^{(\ell)} / \norm{\mX_{s,\text{inside}}^{(\ell)}}_{\text{F}}, \\
    \mX_{s,\text{inside}}^{(\ell)} =& \sum_{i,j:j-i=\ell} \bm\alpha_s^{i,j}(\bm\alpha_s^{i,j})^\top, \\
    \mX^{(\ell)}_{\text{outside}} =& \sum_{s} \mX_{s,\text{outside}}^{(\ell)} / \norm{\mX_{s,\text{outside}}^{(\ell)}}_{\text{F}}, \\
    \mX_{s,\text{outside}}^{(\ell)} =& \sum_{i,j:j-i=\ell} \bm\beta_s^{i,j}(\bm\beta_s^{i,j})^\top.
\end{align*}
However empirically, we also find that the performance drops by using different transformation matrices for inside and outside probabilities computation, which may also be attributed to the non-optimality of our method to learn the transformation matrix.
