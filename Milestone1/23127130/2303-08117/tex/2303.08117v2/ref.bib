@inproceedings{devlin2019bert,
  title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  booktitle={Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)},
  pages={4171--4186},
  year={2019}
}

@article{liu2019roberta,
  title={Roberta: A robustly optimized bert pretraining approach},
  author={Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
  journal={arXiv preprint arXiv:1907.11692},
  year={2019}
}

@inproceedings{hewitt2019structural,
  title={A structural probe for finding syntax in word representations},
  author={Hewitt, John and Manning, Christopher D},
  booktitle={Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)},
  pages={4129--4138},
  year={2019}
}

@article{manning2020emergent,
  title={Emergent linguistic structure in artificial neural networks trained by self-supervision},
  author={Manning, Christopher D and Clark, Kevin and Hewitt, John and Khandelwal, Urvashi and Levy, Omer},
  journal={Proceedings of the National Academy of Sciences},
  volume={117},
  number={48},
  pages={30046--30054},
  year={2020},
  publisher={National Acad Sciences}
}

@article{arps2022probing,
  title={Probing for Constituency Structure in Neural Language Models},
  author={Arps, David and Samih, Younes and Kallmeyer, Laura and Sajjad, Hassan},
  journal={arXiv preprint arXiv:2204.06201},
  year={2022}
}

@inproceedings{wu2020perturbed,
  title={Perturbed Masking: Parameter-free Probing for Analyzing and Interpreting BERT},
  author={Wu, Zhiyong and Chen, Yun and Kao, Ben and Liu, Qun},
  booktitle={Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
  pages={4166--4176},
  year={2020}
}

@inproceedings{yao2021self,
  title={Self-Attention Networks Can Process Bounded Hierarchical Languages},
  author={Yao, Shunyu and Peng, Binghui and Papadimitriou, Christos and Narasimhan, Karthik},
  booktitle={Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)},
  pages={3770--3785},
  year={2021}
}

@inproceedings{bhattamishra2020computational,
  title={On the Computational Power of Transformers and Its Implications in Sequence Modeling},
  author={Bhattamishra, Satwik and Patel, Arkil and Goyal, Navin},
  booktitle={Proceedings of the 24th Conference on Computational Natural Language Learning},
  pages={455--475},
  year={2020}
}

@article{perez2021attention,
  title={Attention is Turing-Complete.},
  author={P{\'e}rez, Jorge and Barcel{\'o}, Pablo and Marinkovic, Javier},
  journal={J. Mach. Learn. Res.},
  volume={22},
  number={75},
  pages={1--35},
  year={2021}
}

@article{liu2022transformers,
  title={Transformers learn shortcuts to automata},
  author={Liu, Bingbin and Ash, Jordan T and Goel, Surbhi and Krishnamurthy, Akshay and Zhang, Cyril},
  journal={arXiv preprint arXiv:2210.10749},
  year={2022}
}

@inproceedings{cohen2012spectral,
  title={Spectral learning of latent-variable PCFGs},
  author={Cohen, Shay B and Stratos, Karl and Collins, Michael and Foster, Dean and Ungar, Lyle},
  booktitle={Proceedings of the 50th annual meeting of the association for computational linguistics (Volume 1: Long papers)},
  pages={223--231},
  year={2012}
}

@article{cohen2014spectral,
  title={Spectral learning of latent-variable PCFGs: Algorithms and sample complexity},
  author={Cohen, Shay B and Stratos, Karl and Collins, Michael and Foster, Dean P and Ungar, Lyle},
  journal={The Journal of Machine Learning Research},
  volume={15},
  number={1},
  pages={2399--2449},
  year={2014},
  publisher={JMLR. org}
}



@book{manning1999foundations,
  title={Foundations of statistical natural language processing},
  author={Manning, Christopher and Schutze, Hinrich},
  year={1999},
  publisher={MIT press}
}

@article{marcus1993building,
  title={Building a Large Annotated Corpus of English: The Penn Treebank},
  author={Marcus, Mitch and Santorini, Beatrice and Marcinkiewicz, Mary Ann},
  journal={Computational Linguistics},
  volume={19},
  number={2},
  pages={313--330},
  year={1993}
}

@inproceedings{goodman1996parsing,
  title={Parsing Algorithms and Metrics},
  author={Goodman, Joshua},
  booktitle={34th Annual Meeting of the Association for Computational Linguistics},
  pages={177--183},
  year={1996}
}

@article{baker1979trainable,
  title={Trainable grammars for speech recognition},
  author={Baker, James K},
  journal={The Journal of the Acoustical Society of America},
  volume={65},
  number={S1},
  pages={S132--S132},
  year={1979},
  publisher={Acoustical Society of America}
}

@inproceedings{maudslay2021syntactic,
  title={Do Syntactic Probes Probe Syntax? Experiments with Jabberwocky Probing},
  author={Maudslay, Rowan Hall and Cotterell, Ryan},
  booktitle={Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
  pages={124--131},
  year={2021}
}

@inproceedings{maudslay2020tale,
  title={A Tale of a Probe and a Parser},
  author={Maudslay, Rowan Hall and Valvoda, Josef and Pimentel, Tiago and Williams, Adina and Cotterell, Ryan},
  booktitle={Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
  pages={7389--7395},
  year={2020}
}

@inproceedings{chen2021probing,
  title={Probing BERT in Hyperbolic Spaces},
  author={Chen, Boli and Fu, Yao and Xu, Guangwei and Xie, Pengjun and Tan, Chuanqi and Chen, Mosha and Jing, Liping},
  booktitle={International Conference on Learning Representations},
  year={2021}
}


@inproceedings{
Yun2020Are,
title={Are Transformers universal approximators of sequence-to-sequence functions?},
author={Chulhee Yun and Srinadh Bhojanapalli and Ankit Singh Rawat and Sashank Reddi and Sanjiv Kumar},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=ByxRM0Ntvr}
}

@article{yun2020n,
  title={O (n) connections are expressive enough: Universal approximability of sparse transformers},
  author={Yun, Chulhee and Chang, Yin-Wen and Bhojanapalli, Srinadh and Rawat, Ankit Singh and Reddi, Sashank and Kumar, Sanjiv},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={13783--13794},
  year={2020}
}


@inproceedings{
wei2022statistically,
title={Statistically Meaningful Approximation: a Case Study on Approximating Turing Machines with Transformers},
author={Colin Wei and Yining Chen and Tengyu Ma},
booktitle={Advances in Neural Information Processing Systems},
editor={Alice H. Oh and Alekh Agarwal and Danielle Belgrave and Kyunghyun Cho},
year={2022},
url={https://openreview.net/forum?id=VOyYhoN_yg}
}

@inproceedings{vilares2020parsing,
  title={Parsing as pretraining},
  author={Vilares, David and Strzyz, Michalina and S{\o}gaard, Anders and G{\'o}mez-Rodr{\'\i}guez, Carlos},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={34},
  number={05},
  pages={9114--9121},
  year={2020}
}

@article{rogers2020primer,
  title={A Primer in BERTology: What We Know About How BERT Works},
  author={Rogers, Anna and Kovaleva, Olga and Rumshisky, Anna},
  journal={Transactions of the Association for Computational Linguistics},
  volume={8},
  pages={842--866},
  year={2020}
}

@article{reif2019visualizing,
  title={Visualizing and measuring the geometry of BERT},
  author={Reif, Emily and Yuan, Ann and Wattenberg, Martin and Viegas, Fernanda B and Coenen, Andy and Pearce, Adam and Kim, Been},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}

@article{belinkov2017neural,
  title={What do neural machine translation models learn about morphology?},
  author={Belinkov, Yonatan and Durrani, Nadir and Dalvi, Fahim and Sajjad, Hassan and Glass, James},
  journal={arXiv preprint arXiv:1704.03471},
  year={2017}
}

@inproceedings{kim2020pre,
  title={Are Pre-trained Language Models Aware of Phrases? Simple but Strong Baselines for Grammar Induction},
  author={Kim, Taeuk and Choi, Jihun and Edmiston, Daniel and Lee, Sang-goo},
  booktitle={International Conference on Learning Representations},
  year={2020}
}

@inproceedings{richardson2020probing,
  title={Probing natural language inference models through semantic fragments},
  author={Richardson, Kyle and Hu, Hai and Moss, Lawrence and Sabharwal, Ashish},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={34},
  number={05},
  pages={8713--8721},
  year={2020}
}


@inproceedings{izsak-etal-2021-train,
    title = "How to Train {BERT} with an Academic Budget",
    author = "Izsak, Peter  and
      Berchansky, Moshe  and
      Levy, Omer",
    booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2021",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.emnlp-main.831",
}



@article{loshchilov2017decoupled,
  title={Decoupled weight decay regularization},
  author={Loshchilov, Ilya and Hutter, Frank},
  journal={arXiv preprint arXiv:1711.05101},
  year={2017}
}

@inproceedings{li2020empirical,
  title={An empirical comparison of unsupervised constituency parsing methods},
  author={Li, Jun and Cao, Yifan and Cai, Jiong and Jiang, Yong and Tu, Kewei},
  booktitle={Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
  pages={3278--3283},
  year={2020}
}


@article{wettig2022should,
  title={Should you mask 15\% in masked language modeling?},
  author={Wettig, Alexander and Gao, Tianyu and Zhong, Zexuan and Chen, Danqi},
  journal={arXiv preprint arXiv:2202.08005},
  year={2022}
}

@article{kasami1966efficient,
  title={An efficient recognition and syntax-analysis algorithm for context-free languages},
  author={Kasami, Tadao},
  journal={Coordinated Science Laboratory Report no. R-257},
  year={1966},
  publisher={Coordinated Science Laboratory, University of Illinois at Urbana-Champaign}
}

@Misc{Spectral-Parser,
    author = {Haoran Peng},
    title = {Spectral Learning of Latent-Variable PCFGs: High-Performance Implementation},
    year = {2021},
    url = "https://github.com/GavinPHR/Spectral-Parser"
}

@article{scikit-learn,
 title={Scikit-learn: Machine Learning in {P}ython},
 author={Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V.
         and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P.
         and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and
         Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},
 journal={Journal of Machine Learning Research},
 volume={12},
 pages={2825--2830},
 year={2011}
}

@inproceedings{hewitt2019designing,
  title={Designing and Interpreting Probes with Control Tasks},
  author={Hewitt, John and Liang, Percy},
  booktitle={Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)},
  pages={2733--2743},
  year={2019}
}


@article{kim2017structured,
  title={Structured attention networks},
  author={Kim, Yoon and Denton, Carl and Hoang, Luong and Rush, Alexander M},
  journal={arXiv preprint arXiv:1702.00887},
  year={2017}
}


@article{shen2020structformer,
  title={Structformer: Joint unsupervised induction of dependency and constituency structure from masked language modeling},
  author={Shen, Yikang and Tay, Yi and Zheng, Che and Bahri, Dara and Metzler, Donald and Courville, Aaron},
  journal={arXiv preprint arXiv:2012.00857},
  year={2020}
}


@article{merrill2022saturated,
  title={Saturated transformers are constant-depth threshold circuits},
  author={Merrill, William and Sabharwal, Ashish and Smith, Noah A},
  journal={Transactions of the Association for Computational Linguistics},
  volume={10},
  pages={843--856},
  year={2022},
  publisher={MIT Press}
}



@article{anil2022exploring,
  title={Exploring length generalization in large language models},
  author={Anil, Cem and Wu, Yuhuai and Andreassen, Anders and Lewkowycz, Aitor and Misra, Vedant and Ramasesh, Vinay and Slone, Ambrose and Gur-Ari, Guy and Dyer, Ethan and Neyshabur, Behnam},
  journal={arXiv preprint arXiv:2207.04901},
  year={2022}
}


@article{nogueira2021investigating,
  title={Investigating the limitations of transformers with simple arithmetic tasks},
  author={Nogueira, Rodrigo and Jiang, Zhiying and Lin, Jimmy},
  journal={arXiv preprint arXiv:2102.13019},
  year={2021}
}


@article{bhattamishra2020ability,
  title={On the ability and limitations of transformers to recognize formal languages},
  author={Bhattamishra, Satwik and Ahuja, Kabir and Goyal, Navin},
  journal={arXiv preprint arXiv:2009.11264},
  year={2020}
}


@inproceedings{edelman2022inductive,
  title={Inductive biases and variable creation in self-attention mechanisms},
  author={Edelman, Benjamin L and Goel, Surbhi and Kakade, Sham and Zhang, Cyril},
  booktitle={International Conference on Machine Learning},
  pages={5793--5831},
  year={2022},
  organization={PMLR}
}


@article{nanda2023progress,
  title={Progress measures for grokking via mechanistic interpretability},
  author={Nanda, Neel and Chan, Lawrence and Liberum, Tom and Smith, Jess and Steinhardt, Jacob},
  journal={arXiv preprint arXiv:2301.05217},
  year={2023}
}



@inproceedings{lakretz-etal-2019-emergence,
    title = "The emergence of number and syntax units in {LSTM} language models",
    author = "Lakretz, Yair  and
      Kruszewski, German  and
      Desbordes, Theo  and
      Hupkes, Dieuwke  and
      Dehaene, Stanislas  and
      Baroni, Marco",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N19-1002",
    doi = "10.18653/v1/N19-1002",
    pages = "11--20",
    abstract = "Recent work has shown that LSTMs trained on a generic language modeling objective capture syntax-sensitive generalizations such as long-distance number agreement. We have however no mechanistic understanding of how they accomplish this remarkable feat. Some have conjectured it depends on heuristics that do not truly take hierarchical structure into account. We present here a detailed study of the inner mechanics of number tracking in LSTMs at the single neuron level. We discover that long-distance number information is largely managed by two {``}number units{''}. Importantly, the behaviour of these units is partially controlled by other units independently shown to track syntactic structure. We conclude that LSTMs are, to some extent, implementing genuinely syntactic processing mechanisms, paving the way to a more general understanding of grammatical encoding in LSTMs.",
}



@article{murty2022characterizing,
  title={Characterizing Intrinsic Compositionality In Transformers With Tree Projections},
  author={Murty, Shikhar and Sharma, Pratyusha and Andreas, Jacob and Manning, Christopher D},
  journal={arXiv preprint arXiv:2211.01288},
  year={2022}
}



@inproceedings{conia-navigli-2022-probing,
    title = "Probing for Predicate Argument Structures in Pretrained Language Models",
    author = "Conia, Simone  and
      Navigli, Roberto",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.acl-long.316",
    doi = "10.18653/v1/2022.acl-long.316",
    pages = "4622--4632",
    abstract = "Thanks to the effectiveness and wide availability of modern pretrained language models (PLMs), recently proposed approaches have achieved remarkable results in dependency- and span-based, multilingual and cross-lingual Semantic Role Labeling (SRL). These results have prompted researchers to investigate the inner workings of modern PLMs with the aim of understanding how, where, and to what extent they encode information about SRL. In this paper, we follow this line of research and probe for predicate argument structures in PLMs. Our study shows that PLMs do encode semantic structures directly into the contextualized representation of a predicate, and also provides insights into the correlation between predicate senses and their structures, the degree of transferability between nominal and verbal structures, and how such structures are encoded across languages. Finally, we look at the practical implications of such insights and demonstrate the benefits of embedding predicate argument structure information into an SRL model.",
}


@article{vulic2020probing,
  title={Probing pretrained language models for lexical semantics},
  author={Vuli{\'c}, Ivan and Ponti, Edoardo Maria and Litschko, Robert and Glava{\v{s}}, Goran and Korhonen, Anna},
  journal={arXiv preprint arXiv:2010.05731},
  year={2020}
}





@inproceedings{jawahar2019does,
  title={What does BERT learn about the structure of language?},
  author={Jawahar, Ganesh and Sagot, Beno{\^\i}t and Seddah, Djam{\'e}},
  booktitle={ACL 2019-57th Annual Meeting of the Association for Computational Linguistics},
  year={2019}
}



@misc{
alain2017understanding,
title={Understanding intermediate layers using linear classifier probes},
author={Guillaume Alain and Yoshua Bengio},
year={2017},
url={https://openreview.net/forum?id=ryF7rTqgl}
}



@article{hupkes2018visualisation,
  title={Visualisation and'diagnostic classifiers' reveal how recurrent and recursive neural networks process hierarchical structure},
  author={Hupkes, Dieuwke and Veldhoen, Sara and Zuidema, Willem},
  journal={Journal of Artificial Intelligence Research},
  volume={61},
  pages={907--926},
  year={2018}
}


@inproceedings{conneau-etal-2018-cram,
    title = "What you can cram into a single {\$}{\&}!{\#}* vector: Probing sentence embeddings for linguistic properties",
    author = {Conneau, Alexis  and
      Kruszewski, German  and
      Lample, Guillaume  and
      Barrault, Lo{\"\i}c  and
      Baroni, Marco},
    booktitle = "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2018",
    address = "Melbourne, Australia",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P18-1198",
    doi = "10.18653/v1/P18-1198",
    pages = "2126--2136",
    abstract = "Although much effort has recently been devoted to training high-quality sentence embeddings, we still have a poor understanding of what they are capturing. {``}Downstream{''} tasks, often based on sentence classification, are commonly used to evaluate the quality of sentence representations. The complexity of the tasks makes it however difficult to infer what kind of information is present in the representations. We introduce here 10 probing tasks designed to capture simple linguistic features of sentences, and we use them to study embeddings generated by three different encoders trained in eight distinct ways, uncovering intriguing properties of both encoders and training methods.",
}



@article{elhage2021mathematical,
   title={A Mathematical Framework for Transformer Circuits},
   author={Elhage, Nelson and Nanda, Neel and Olsson, Catherine and Henighan, Tom and Joseph, Nicholas and Mann, Ben and Askell, Amanda and Bai, Yuntao and Chen, Anna and Conerly, Tom and DasSarma, Nova and Drain, Dawn and Ganguli, Deep and Hatfield-Dodds, Zac and Hernandez, Danny and Jones, Andy and Kernion, Jackson and Lovitt, Liane and Ndousse, Kamal and Amodei, Dario and Brown, Tom and Clark, Jack and Kaplan, Jared and McCandlish, Sam and Olah, Chris},
   year={2021},
   journal={Transformer Circuits Thread},
   note={https://transformer-circuits.pub/2021/framework/index.html}
}


@article{olsson2022context,
   title={In-context Learning and Induction Heads},
   author={Olsson, Catherine and Elhage, Nelson and Nanda, Neel and Joseph, Nicholas and DasSarma, Nova and Henighan, Tom and Mann, Ben and Askell, Amanda and Bai, Yuntao and Chen, Anna and Conerly, Tom and Drain, Dawn and Ganguli, Deep and Hatfield-Dodds, Zac and Hernandez, Danny and Johnston, Scott and Jones, Andy and Kernion, Jackson and Lovitt, Liane and Ndousse, Kamal and Amodei, Dario and Brown, Tom and Clark, Jack and Kaplan, Jared and McCandlish, Sam and Olah, Chris},
   year={2022},
   journal={Transformer Circuits Thread},
   note={https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html}
}

@inproceedings{bai2021syntax,
  title={Syntax-BERT: Improving Pre-trained Transformers with Syntax Trees},
  author={Bai, Jiangang and Wang, Yujing and Chen, Yiren and Yang, Yaming and Bai, Jing and Yu, Jing and Tong, Yunhai},
  booktitle={Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume},
  pages={3011--3020},
  year={2021}
}

@inproceedings{xu2021syntax,
  title={Syntax-Enhanced Pre-trained Model},
  author={Xu, Zenan and Guo, Daya and Tang, Duyu and Su, Qinliang and Shou, Linjun and Gong, Ming and Zhong, Wanjun and Quan, Xiaojun and Jiang, Daxin and Duan, Nan},
  booktitle={Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)},
  pages={5412--5422},
  year={2021}
}

@inproceedings{strubell2018linguistically,
    title = "Linguistically-Informed Self-Attention for Semantic Role Labeling",
    author = "Strubell, Emma  and
      Verga, Patrick  and
      Andor, Daniel  and
      Weiss, David  and
      McCallum, Andrew",
    booktitle = "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
    month = oct # "-" # nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D18-1548",
    doi = "10.18653/v1/D18-1548",
    pages = "5027--5038",
    abstract = "Current state-of-the-art semantic role labeling (SRL) uses a deep neural network with no explicit linguistic features. However, prior work has shown that gold syntax trees can dramatically improve SRL decoding, suggesting the possibility of increased accuracy from explicit modeling of syntax. In this work, we present linguistically-informed self-attention (LISA): a neural network model that combines multi-head self-attention with multi-task learning across dependency parsing, part-of-speech tagging, predicate detection and SRL. Unlike previous models which require significant pre-processing to prepare linguistic features, LISA can incorporate syntax using merely raw tokens as input, encoding the sequence only once to simultaneously perform parsing, predicate detection and role labeling for all predicates. Syntax is incorporated by training one attention head to attend to syntactic parents for each token. Moreover, if a high-quality syntactic parse is already available, it can be beneficially injected at test time without re-training our SRL model. In experiments on CoNLL-2005 SRL, LISA achieves new state-of-the-art performance for a model using predicted predicates and standard word embeddings, attaining 2.5 F1 absolute higher than the previous state-of-the-art on newswire and more than 3.5 F1 on out-of-domain data, nearly 10{\%} reduction in error. On ConLL-2012 English SRL we also show an improvement of more than 2.5 F1. LISA also out-performs the state-of-the-art with contextually-encoded (ELMo) word representations, by nearly 1.0 F1 on news and more than 2.0 F1 on out-of-domain text.",
}

