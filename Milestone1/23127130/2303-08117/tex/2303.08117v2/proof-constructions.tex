\section{Missing Proofs in \Cref{sec:construction}}
In this section, we show the detailed proof for \Cref{thm:hard_attnt}, \Cref{thm:soft_attnt}, and \Cref{thm:io-optimal-mlm}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\iffalse
\subsection{Proof of \Cref{thm:hard_attnt}}

\paragraph{Construction}
Next, we show the construction of this attention model.

\paragraph{Data structure} For the embedding at position $i$ at layer $\ell \le L$, the $(k,A)$-th coordinate denotes the inside probability $\alpha(A, i, i+k-1)$ at the current step $\ell$. the $(k,A)+|N|L$-th coordinate denotes the inside probability $\alpha(A, i-k+1, i)$. The $(k,A) + 2|N| L$-th coordinate denotes the outside probability $\beta(A, i, i+k-1)$, and the $(k,A) + 3|N| L$-th coordinate denotes the outside probability $\beta(A, i-k+1, i)$. The last $d_p$ coordinates denote the positional embeddings.

As for the attention score between $(e_i, p_i)$ and $(e_j, p_j)$, we make it
\[\text{Attn}_{\mK_e, \mQ_e, \mK_p, \mQ_p}(i,j) = e_i^\top \mK_e^\top \mQ_e e_j + p_i^\top \mK_p^\top \mQ_p p_j.\]


\[\text{Attn}_{\mK_e, \mQ_e, \mK_p, \mQ_p}(i,j) = e_i^\top \mK_e^\top \mQ_e e_j +  |p_i - p_j|.\]

\paragraph{First layer}
At the first layer $\ell = 1$, for the embedding at $i$-th position, the coordinate $(1,A)$ and $(1,A)+|N|L$ denote the inside probability $\alpha(A,i,i)$, which can be initialized from the word embedding matrix. All other coordinates except the positional embeddings are set to $0$.

\paragraph{Layer $1 < \ell \le L$}
These layers compute the inside probabilities $\alpha(A,i,j)$ for all $A \in N, i,j\le L$ and $j-i=\ell-1$, i.e., the span with length $\ell$. Only the first $2|N|L$ coordinates and the positional embeddings are non-zero.

The recursive definition of the inside probabilities are given by
\[\alpha(A, i, j)=\sum_{A \rightarrow BC \in R} \sum_{k=i+1}^{j-1}(P(A \rightarrow B C) \times \alpha(B, i, k) \times \alpha(C, k+1, j)).\]

We want the $A$-th attention head in layer $\ell$ compute the score
\[\sum_{A \rightarrow BC \in R}\sum_{k=i+1}^{i+\ell-2} (P(A \rightarrow B C) \times \alpha(B, i, k) \times \alpha(C, k+1, i+\ell-1)),\]
which is exactly the inside probability $\alpha(A,i,j)$ for all $j = i+\ell-1$.

Note that there exists a matrix $\mM_{A,\ell}$ such that
\[ \dotp{e_i^{(\ell-1)}}{\mM_{A,\ell} \cdot e_{i+\ell-1}^{(\ell-1)}} = \sum_{A \rightarrow BC \in R}\sum_{k=i+1}^{i+\ell-2} (P(A \rightarrow B C) \times \alpha(B, i, k) \times \alpha(C, k+1, j)),\]
where the embeddings $e_i^{(\ell-1)}$ and $e_{i+\ell-1}^{(\ell-1)}$ are the embeddings without positional embeddings at $i$-th and $i+\ell-1$-th positions at $\ell-1$ layer. Specifically, denote $P_A\in \R^{|N|\times |N|}$ the probabilities of rules splitted from non-terminal $A$. $\mM_{A,\ell}$ is a $4L\times 4L$ block matrix where each block has size $|N|\times |N|$. All the blocks except $(|N|L+k_1, k_2)$ where $k_1+k_2 = \ell$ in $\mM_{A,\ell}$ are zero, and the blocks at position $(|N|L+k_1, k_2)$ are $P_A$. To implement this as an attention, we can set $\mK = \mI$ the identity matrix, and $\mQ = \mM_{A,\ell}$.

We also need to make sure that the attention score between $i$ and $i+\ell'-1$ for $\ell'\neq \ell$ is smaller than the attention score between $i$ and $i+\ell-1$ (and thus the hard attention head will compute the probability between $i$ and $i+\ell-1$ by taking the max). We can achieve this goal by using the positional embeddings. Specifically, if we use the one-hot positional embeddings, i.e., $e_p[i] = 1$ if the position is $i$ and $0$ otherwise. Then, we can set $\mK_p = \mI$, and $\mQ_p[i,i+\ell-1] = 0$ and all other entries to be $-1$. Then, $p_i^\top \mK_p^\top \mQ_p p_{i+\ell-1} = 0$ and $p_i^\top \mK_p^\top \mQ_p p_{j} = -1$ for all $j \neq i+\ell-1$, and using $|N|$ hard attentions together we can compute $\alpha(A,i,i+\ell-1)$.

Similarly, we also needs another $|N|$ attention heads to compute $\alpha(A,i-\ell+1, i)$.

In the end, we use the residual connection to copy all $\alpha(A,i-\ell'+1, i)$ and $\alpha(A,i, i+\ell'-1)$ for $\ell' < \ell$ to this layer.

\paragraph{Layer $L+1$}
Similar to Layer 1, this layer will setup all the outside probabilities $\beta(S,1,L) = 1$. Besides, this layer copies all the inside probabilities from the Layer $L$.

\paragraph{Layer $L+1 < \ell \le 2L$} These layers are similar to the layers in Layer $1 < \ell \le L$. The difference is that these layers need to compute the outside probabilities, and each outside probability needs $2|N|$ attention heads (as opposed to $|N|$ for the inside probabilities). Besides computing the outside probabilities for span starting at $i$, we also need to store the outside probabilities ending at $i$, and thus need $4|N|$ attention heads in total.

Now we show the construction in more detail.

First, recall that the recursive definition for the outside probabilities is
\begin{equation*}
    \begin{aligned}
        \beta(A, i, j)=& \sum_{B \rightarrow CA \in R} \sum_{k=1}^{i-1}(P(B \rightarrow C A) \times \alpha(C, k, i-1) \times \beta(B, k, j)) \\
        &+\sum_{B \rightarrow A C \in R} \sum_{k=j+1}^{L}(P(B \rightarrow A C) \times \alpha(C, j+1, k) \times \beta(B, i, k)).
    \end{aligned}
\end{equation*}

Now at layer $\ell + L$ where $1 < \ell \le L$, we compute $\beta(A,i,i+L-\ell)$ for $i \le \ell$ and $\beta(A,i-L+\ell,i)$ for $i \ge \ell$ at position $i$. We first compute $\beta(A,i,i+L-\ell)$, which needs $2$ attention heads together. First note that there exist two matrices $\mM_{A,\ell}^{(1)}$ and $\mM_{A,\ell}^{(2)}$ such that
\begin{align*}
    \dotp{e_i^{(\ell-1+L)}}{\mM_{A,\ell}^{(1)} \cdot e_{i+L-\ell}^{(\ell-1+L)}} =& \sum_{B \rightarrow CA \in R} \sum_{k=1}^{i-1}(P(B \rightarrow C A) \times \alpha(C, k, i-1) \times \beta(B, k, i+L-\ell)) \\
    \dotp{e_i^{(\ell-1+L)}}{\mM_{A,\ell}^{(2)} \cdot e_{i+L-\ell}^{(\ell-1+L)}} =& \sum_{B \rightarrow A C \in R} \sum_{k=i+L-\ell+1}^{L}(P(B \rightarrow A C) \times \alpha(C, i+L-\ell+1, k) \times \beta(B, i, k)),
\end{align*}
where the embeddings $e_i^{(\ell-1+L)}$ and $e_{i+L-\ell}^{(\ell-1+L)}$ are the embeddings without positional embeddings at $i$-th and $i+L-\ell$-th positions at $\ell-1 + L$ layer. Specifically, we denote $P_A^{(r)} \in \R^{|N|\times |N|}$ the probabilities of rules that $A$ is the right child (the different columns of $P_A^{(r)}$ denote different parent non-terminals $B$ and different rows denote different left child $C$). We denote $P_A^{(l)} \in \R^{|N|\times |N|}$ the probabilities of rules that $A$ is the left child (the different rows of $P_A^{(l)}$ denote different parent non-terminals $B$ and different columns denote different right child $C$).

$\mM_{A,\ell}^{(1)}$ is a $4L\times 4L$ block matrix where each block has size $|N|\times |N|$. All the blocks except $(|N|L+k_1, 3|N|L+k_2)$ where $k_2-k_1 = L - \ell$ in $\mM_{A,\ell}^{(1)}$ are zero, and the blocks at position $(|N|L+k_1, 3|N|L+k_2)$ are $P_A^{(r)}$. Similarly, $\mM_{A,\ell}^{(2)}$ is a $4L\times 4L$ block matrix where each block has size $|N|\times |N|$. All the blocks except $(2|N|L+k_1,k_2)$ where $k_1 - k_2 = L - \ell$ in $\mM_{A,\ell^{(2)}}$ are zero, and the blocks at position $(2|N|L+k_1,k_2)$ are $P_A^{(l)}$.

Besides, we also need $2|N|$ additional heads for the outside probabilities $\beta(A,i-L+\ell,i)$ and similar positional embeddings to make sure that $i$ is only attended to $i+L-\ell$.
\fi
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Proof of \Cref{thm:hard_attnt}}\label{sec:hard_attnt_proof}
\input{Proof_hard}



\subsection{Proof of \Cref{thm:soft_attnt}}\label{sec:soft_attnt_proof}






%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\iffalse
\paragraph{Construction}
Next, we show the construction of this attention model.

\paragraph{Data structure} The storage in the embeddings will slightly differ in layers $1 \le \ell \le L$ and layers $L + 1 \le \ell \le 2L$

\begin{itemize}
    \item For the embedding at position $i$ at layer $\ell \le L$, the $(k,A)$-th coordinate denotes the inside probability $\alpha(A, i-k+1, i)$ at the current step $\ell$. Other coordinates store $0$.
    
    \item For the embedding at position $i$ at layer $L+1 \le \ell \le 2L$, the $(k,A)$-th coordinate denotes the inside probability $\alpha(A, i-k, i-1)$ at the current step $\ell$. The $(k,A) + 2|N| L$-th coordinate denotes the outside probability $\beta(A, i, i+k-1)$. 
\end{itemize}

 %The $(k,A)+|N|L$-th coordinate denotes the inside probability $\alpha(A, i-k, i-1)$. The $(k,A) + 2|N| L$-th coordinate denotes the outside probability $\beta(A, i, i+k-1)$. 
 We also have $4L + 2$ relative position vectors $p_{-L}, p_{-(L-1)}, \cdots, p_0,  p_1 \cdots, p_{L}, \Tilde{p}_1, \cdots, \Tilde{p}_L$, with

\begin{enumerate}
    \item For any $i \in [1, L]$, $p_{i}[|N| * (i - 1) +  j] = 0$ for all $0 \le j < |N|$, and $-1$ otherwise.
    
    \item  For any $i \in [1, L]$, $\Tilde{p}_{i} [|N| * (L + i - 2) +j] = 0$, for all $0 \le j < |N|$, and $-1$ otherwise.
    
    \item For any $i > 0$, $p_{-i}[|N| * (i - 1) +  j] = 0$ for all $0 \le j < |N|$, and $-1$ otherwise.
    
    \item For any $i > 0$, $\Tilde{p}_{i} [j] = -1$ for all $j$.
    
    \item $\Tilde{p}_{0} [j] = p_{0} [j] = -1$ for all $j$.
\end{enumerate}
We will use the position embeddings $p_{-L}, \cdots, p_L$ in layers $1, \cdots, L$ and $\Tilde{p}_{-L}, \cdots, \Tilde{p}_L$ in layers $L+1, \cdots, 2L$.


%The last $d_p$ coordinates denote the positional embeddings.

As for the attention score between $e_i$ and $e_j$, we make it
\[\text{Attn}_{\mK_e, \mQ_e, \mK_p, \mQ_p}(i,j) = e_i^\top \mQ_e^\top  \sigma_{relu}(\mK_e e_j + p_{j-i} ) ,\] where $\sigma_{relu}$ denotes the ReLU activation function.


\paragraph{First layer}
At the first layer $\ell = 1$, for the embedding at $i$-th position, the coordinate $(1,A)$ denotes the inside probability $\alpha(A,i,i)$, which can be initialized from the word embedding matrix. All other coordinates are set to $0$.
%except the positional embeddings are set to $0$.

\paragraph{Layer $1 < \ell \le L$}
These layers compute the inside probabilities $\alpha(A, j, i)$ for all $A \in N, i,j\le L$ and $i-j=\ell-1$, i.e., the span with length $\ell$. Only the first $2|N|L$ coordinates are non-zero.

The recursive definition of the inside probabilities is given by
\[\alpha(A, j, i)=\sum_{A \rightarrow BC \in R} \sum_{k=j+1}^{i-1}(P(A \rightarrow B C) \times \alpha(B, j, k) \times \alpha(C, k+1, i)).\]

We want the $A$-th attention head in layer $\ell$ compute the score
\[\sum_{A \rightarrow BC \in R}\sum_{k=i-\ell+1}^{i-1} (P(A \rightarrow B C) \times \alpha(B, i-\ell+1, k) \times \alpha(C, k+1, i)),\]
which is exactly the inside probability $\alpha(A, i-\ell+1, i)$ for all $A$.

We can represent this computation as an attention head, i.e.:
\begin{align*}
    \sum_{A \rightarrow BC \in R}\sum_{k=i-\ell+1}^{i-1} (P(A \rightarrow B C) \times \alpha(B, i-\ell+1, k) \times \alpha(C, k+1, i)) = \sum_{k=0}^{L-1} \dotp{\mQ_{A, \ell} e_i^{(\ell-1)}}{ \sigma_{relu} ( \mK_{\ell}  e_{k}^{(\ell-1)} + p_{j-i}  ) }.
\end{align*}
Here the matrix $\mQ_{A, \ell}$ represents a block-diagonal matrix, containing $\ell$ repetitive blocks  along the diagonal starting from $(0, 0)$, with the rest of the blocks set to $0$. $\mK_{\ell}$ is a rotation matrix such that for any vector $v$, $(\mK_{\ell} v)[|N| * a + b] = v [ |N| * (\ell - 1 - a) + b ]$ for any $0 \le a \leq \ell - 1, 0 \le b < |N|$, and is $0$ otherwise.


%Similarly, we also need another $|N|$ attention heads to compute $\alpha(A,i-\ell+1, i)$. 
In the end, we use the residual connection to copy all $\alpha(A,i-\ell'+1, i)$ for $\ell' < \ell$ to this layer.


\paragraph{Layer $L+1$}
Similar to Layer 1, this layer will setup all the outside probabilities $\beta(S,1,L) = 1$. 
\todo{Think about copying alpha from previous layer and previous position for back ending alphas.}

%Moreover, at each position $i$, we simply copy the inside probabilities at position $i-i$
Besides, this layer copies all the inside probabilities from the Layer $L$. 

\paragraph{Layer $L+1 < \bar{\ell} \le 2L$} These layers are similar to the layers in Layer $1 < \ell \le L$. Let $\ell = 2L - \bar{\ell}.$
%The difference is that these layers need to compute the outside probabilities, and each outside probability needs $2|N|$ attention heads (as opposed to $|N|$ for the inside probabilities). Besides computing the outside probabilities for span starting at $i$, we also need to store the outside probabilities ending at $i$, and thus need $4|N|$ attention heads in total. 

Now we show the construction in more detail.

First, recall that the recursive definition for the outside probabilities is
\begin{equation*}
    \begin{aligned}
        \beta(A, i, j)=& \sum_{B \rightarrow CA \in R} \sum_{k=1}^{i-1}(P(B \rightarrow C A) \times \alpha(C, k, i-1) \times \beta(B, k, j)) \\
        &+\sum_{B \rightarrow A C \in R} \sum_{k=j+1}^{L}(P(B \rightarrow A C) \times \alpha(C, j+1, k) \times \beta(B, i, k)).
    \end{aligned}
\end{equation*}


We can represent this computation as an attention head, i.e.:
\begin{align*}
     \beta(A, i, j) = &\sum_{k=0}^{L-1} \dotp{\mQ_{A, \ell} e_i^{(\ell-1)}}{\mK_{\ell}  e_{k}^{(\ell-1)} \odot \Tilde{p}_{j-i} },
\end{align*}
where $\mQ_{A, \ell}$ is a block diagonal matrix, with
\begin{itemize}
    \item $\mQ_{A, \ell}$ contains $L$ repetitive  blocks of $P(\cdot \to \cdot A)$ along its diagonal starting from $(0, 0)$.
    
    \item $\mQ_{A, \ell}$ contains $\ell$ repetitive blocks of $P(\cdot \to A \cdot)$ along its diagonal starting from $(|N|*(L + \ell), |N|*(L + \ell))$.
    
    \item Rest of the elements in $\mQ_{A, \ell}$ are $0$.
\end{itemize}

$\mK_{\ell}$ is a rotation matrix with the following structure: for any vector $v$,
\begin{itemize}
    \item $(\mK_{\ell} v)[|N|*a + b] = v [  |N| * (L + \ell + a ) + b ]$ for any $0 \le a \leq L - \ell + 1, 0 \le b < |N|$.
     
    \item $(\mK_{\ell} v)[|N|*(L + a) + b] = v [  |N| * a + b ]$ for any $0 \le a \leq L, 0 \le b < |N|$.
    
    \item Other coordinates $\mK_{\ell} v$ turn to $0$.
\end{itemize}
\fi




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%\subsection{Proof}





Similar to the proof of \cref{thm:hard_attnt}, the first $L-1$ layers simulate the recursive formulation of the Inside probabilities from eq.~\ref{eq:inside_probability}, and the last $L-1$ layers simulate the recursive formulation of the  outside probabilities from  eq.~\ref{eq:outside_probability}. The model uses embeddings of size $2|\gN| L$ and uses $4L+2$ relative position embeddings.

\paragraph{Notations:} For typographical simplicity, we will divide our embeddings into 2 sub-parts. We will use the first $|\gN| L$ coordinates to store the inside probabilities, and the second $|\gN| L$ coordinates to store the outside probabilities. For every position $i$ and span length $\ell+1$, we store the inside probabilities $\{\alpha(A, i-\ell, i)\}_{A \in \gN}$ after computation in its embedding at coordinates $[|\gN|\ell, |\gN|(\ell+1))$, where the coordinates for embeddings start from $0$. Similarly
we store $\{\beta(A, i, i+\ell)\}_{A \in \gN}$ at $[|\gN|(L+\ell), |\gN|(L+\ell+1))$. For simplicity of presentation, we won't handle cases where $i+\ell$ or $i-\ell$ is outside the range of $1$ to $L$ - those coordinates will be fixed to 0. % we cross the input boundaries in the computations. We presume that all such computations are set as $0$.

%Wherever applicable, for every $A \in \gN$ and $k \le L$, we will use $(k, A), (k, A) + |\gN| L, (k, A) + 2|\gN| L, (k, A) + 3|\gN| L$ to denote the coordinates that store $\alpha(A, i, i + k - 1), \alpha(A, i - k + 1, i), \beta(A, i, i + k - 1),$ and $\beta(A, i - k + 1, i)$ respectively. 



\paragraph{Token Embeddings:} The initial embeddings for each token $w$ will contain $\Pr[A \rightarrow w]$ for all $A \in \gP$. This is to initiate the inside probabilities of all spans of length $1$. 
%Furthermore, the tokens will have a one-hot encoding of their positions in the input in the last $L$ coordinates. 


\paragraph{Relative position embeddings:} We introduce $2L + 1$ relative position vectors $ \{ p_{ t } \in \mathbb{R}^{2 |\gN| L}  \}_{-L \le t \le L},$ that modify the key vectors depending on the relative position of the query and key tokens. Furthermore, we introduce $(2L-1)L$ relative position-dependent biases $ \{ b_{ t, \ell } \in \mathbb{R}  \}_{-L \le t \le L, 1 \le \ell \le 2L-1 }.$ We introduce the structures of the biases in the contexts of their intended uses.

\paragraph{Structure of $\{ p_{ t } \}_{-L \le t \le L}$:} 
For $t < 0$, we define $p_{t}$ such that all coordinates in $[|\gN| (-t-1), |\gN| (-t) )$ are set to $1$, with the rest set to $0$. For $t > 0$, we define $p_{t}$ such that all coordinates in $[|\gN| (L+t-1), |\gN| (L + t) )$ are set to $1$, with the rest set to $0$. $p_0$ is set as all $0s$.



\paragraph{Attention formulation:} At any layer $1 \le \ell \le 2L-1$ except $L$, we define the attention score  $a_{i, j}^h$  between $\ve_i^{(\ell-1)}$ and $\ve_j^{(\ell-1)}$ for any head $h$ with Key and Query matrices $\mK^{(\ell)}_h$ and $\mQ^{(\ell)}_h$ as

{\small
\begin{equation}
    a_{i, j}^h =  \text{ReLU}( \mK^{(\ell)}_h \ve_j^{(\ell-1)} + p_{j - i} - b_{j - i, \ell} )^\top \mQ^{(\ell)}_h \ve_i^{(\ell-1)} \label{eq:soft_attention_appnd}.
\end{equation}
}

For layer $L$, we do not use the relative position embeddings, i.e. we define the attention score  $a_{i, j}^h$  between $\ve_i^{(L-1)}$ and $\ve_j^{(L-1)}$ for any head $h$ with Key and Query matrices $\mK^{(L)}_h$ and $\mQ^{(L)}_h$ as

{\small
\begin{equation}
    a_{i, j}^h =  \text{ReLU}( \mK^{(L-1)}_h \ve_j^{(L-1)} - b_{j - i, L} )^\top \mQ^{(\ell)}_h \ve_i^{(L-1)} \label{eq:soft_attention_appnd_L}.
\end{equation}
}
%For token at position $1$, $(L, \text{ROOT}) + 2|\gN| L$ coordinate will store $1$, and for token at position $L$, $(L, \text{ROOT}) + 3|\gN| L$ coordinate will store $1$, to initiate $\beta( \text{Root}, 1, L ) = 1$.
%The rest of the coordinates, except the position coordinates, are set as $0$.


\paragraph{Inside probabilities:} The contextual embeddings at position $i$ after the computations of any layer $\ell < L$ contains the inside probabilities of all spans of length at most $\ell + 1$ ending at position $i$, i.e.  $\alpha(A, i-k, i)$ for all $A \in \gN$ and $k \le \ell$. The rest of the coordinates contain $0$.




\paragraph{Structure of $\{ b_{ t, \ell } \}_{-L \le t \le L, 1 \le \ell \le L-1}$:} For any $1 \le \ell \le L-1$, for all $t \ge 0$ and $t < -\ell$, we set $b_{t, \ell}$ as $\zeta$ for some large constant $\zeta$. All other biases are set as $1$.
%For all $t$, we set $b_{ t, \ell } = 1 - \mathbb{I}[-\ell \le t \le 0]$

%Generally speaking, the first $|\gN| L$ coordinates record the inside probabilities for spans starting at position $i$, and $(|\gN| L+1)$-th to $2|\gN| L$-th record the inside probabilities for spans ending at position $i$. For layer $\ell \le L$, $(2|\gN| L+1)$-th to $4|\gN| L$-th are $0$. 




%As for the attention score between $(\ve_i^{(\ell)}$ and $\ve_j^{(\ell)}$, we make it
%\begin{align*}
%    & \text{Attn}_{\mK_e^{(\ell)}, \mQ_e^{(\ell)}, \mK_p^{(\ell)}, \mQ_p^{(\ell)}}(i,j) \\
%    =&  (\vv_i^{(\ell)})^\top (\mK_e^{(\ell)})^\top \mQ_e^{(\ell)} \vv_j^{(\ell)} \\
%    & \quad + \vp_i^\top (\mK_p^{(\ell)})^\top \mQ_p^{(\ell)} \vp_j \\
%    =&  (\vv_i^{(\ell)})^\top (\mK_e^{(\ell)})^\top \mQ_e^{(\ell)} \vv_j^{(\ell)} + |p_i^{(\ell)} - p_j^{(\ell)}|.
%\end{align*}

%\paragraph{First layer}
%At the first layer $\ell = 1$, for the embedding at $i$-th position, the coordinate $(1,A)$ and $(1,A)+|\gN|L$ denote the inside probability $\alpha(A,i,i)$, which can be initialized from the word embedding matrix. All other coordinates except the positional embeddings are set to $0$.

\paragraph{Layer $1 \le \ell < L$: }
At each position $i$, this layer computes the inside probabilities of spans of length $\ell+1$ ending at $i$, using the recursive formulation from eq.~\ref{eq:inside_probability}. 

%Recall that the recursive definition of the inside probabilities is given by
%We use $\gN$ attention heads 
For every non-terminal $A \in \gN$, we will use a unique attention head to compute $\alpha(A, i - \ell, i)$ at each token $i$. Specifically, the attention head representing non-terminal $A \in \gN$ will represent the following operation at each position $i$:  

{\small
\begin{align}
    &\alpha(A, i-\ell, i) \nonumber \\
    =&  \sum_{B, C \in \gN} \sum_{j=i-\ell}^{i-1} \Pr[A \rightarrow B C] \alpha(B, i-\ell, j) \alpha(C, j+1, i) \nonumber \\
    =& \sum_{j=i-\ell}^{i-1} \sum_{B, C \in \gN}  \Pr[A \rightarrow B C]   \alpha(B, i-\ell, j) \alpha(C, j+1, i). \label{eq:construction-inside-computation-soft-attn}
    %\\
    %=& \sum_{B, C \in \gN} \sum_{k=j}^{i-1} \Pr[A \rightarrow B C]  \cdot \alpha(B, i-\ell_2, i-\ell_2-1) \cdot \alpha(C, , i) \label{eq:construction-inside-computation_soft},
\end{align}
}

 In the final step, we swapped the order of the summations to observe that the desired computation can be represented as a sum over individual computations at locations $j < i$.  That is, we represent $\sum_{B, C \in \gN}  \Pr[A \rightarrow B C]  \cdot \alpha(B, i-\ell, j) \cdot \alpha(C, j+1, i)$ as the attention score $a_{i,  j}$ for all $i-\ell \le j \le i$, while $\alpha(A, i-\ell, i)$ will be represented as $\sum_{i-\ell \le j < i-1} a_{i,  j}.$


%In the final step, we modified the formulation to represent the interaction of spans of different lengths starting at $i$ and ending at $j$. 

%We represent this computation as the attention score $a_{i, j}$ using a key matrix $\mK_{A}^{(\ell)}$ and query matrix $\mQ_{A}^{(\ell)}$.
%We want the $A$-th attention head in layer $\ell$ compute the score
%\begin{align*}
%& \sum_{A \rightarrow BC \in R}\sum_{k=i+1}^{i+\ell-2} (P(A \rightarrow B C) \\
%& \quad \times \alpha(B, i, k) \times \alpha(C, k+1, i+\ell-1)),
%\end{align*}
%which is exactly the inside probability $\alpha(A,i,j)$ for all $j = i+\ell-1$.
%Note that there exists a matrix $\mM_{A}^{(\ell)}$ such that
%{\small
%\begin{align*}
%    & \dotp{\vv_i^{(\ell-1)}}{\mM_{A}^{(\ell)} \cdot \vv_{i+\ell-1}^{(\ell-1)}} \\
%    =& \sum_{A \rightarrow BC }\sum_{k=i+1}^{i+\ell-2} P(A \rightarrow B C) \alpha(B, i, k) \alpha(C, k+1, j)).
%\end{align*}
%}

%$\{(L + k, \ell - k)\}_{1 \le k \le \ell}$ 
\paragraph{Structure of $\mQ_{A}^{(\ell)}$ and $\mK_A^{(\ell)}$ to compute Eq.~\ref{eq:construction-inside-computation-soft-attn}:} 
\begin{enumerate}
    \item $\mK_{A}^{(\ell)}$ is a rotation matrix such that in $\mK_{A}^{(\ell)} \ve_i^{(\ell)}$, for all $\ell_1 \le \ell$, the inside probabilities $ \{ \alpha(B, i-\ell_1, i) \}_{B \in \gN}$ appears in the coordinates $[ |\gN| (\ell - \ell_1),  |\gN| (\ell - \ell_1+1) )$. Note that $\mK_A^{(\ell)}$ are the same for different $A$, and only depend on $\ell$.
    \item The Query matrix  $\mQ_{A}^{(\ell)}$ is a block diagonal matrix,  such that if we define $\mP_A\in \R^{|\gN|\times |\gN|}$ that contains $\{\Pr[A \to BC]\}_{B, C \in \gN},$ $\mP_A$ appears in the first $\ell$ blocks along the diagonal, i.e. it occurs at all positions starting at $( |\gN| \ell_1,  |\gN| \ell_1 )$ for all $\ell_1 < \ell$. The rest of the blocks are set as $0$s.
\end{enumerate}

%The Query matrix  $\mQ_{A}^{(\ell)}$ is set such that if we define $\mP_A\in \R^{|\gN|\times |\gN|}$ that contains $\{\Pr[A \to BC]\}_{B,C \in \gN},$ $\mP_A$ appears at positions $(|\gN| (L + \ell_2), |\gN|  \ell_1 )$ for all $\ell_1, \ell_2 \ge 0$ with $\ell_1 + \ell_2 = \ell - 1$. Finally, $\mQ_{A}^{(\ell)}$ contains $\mQ_p\in \R^{L \times L} $ at position $(4|\gN|L, 4|\gN|L)$, such that $\mQ_p[i,i+\ell] = 0$ for $0 \le i < L$, with the rest set to $-\zeta$ for some large constant $\zeta$. The rest of the blocks are set as $0$. We give an intuition behind the structure of $\mQ_{A}^{(\ell)}$ below.

\paragraph{Intuition behind $\mQ_{A}^{(\ell)}$, $\mK_{A}^{(\ell)}$, the relative position embeddings and the biases:} For any position $i$ and range $\ell_1 < \ell$,  $\ve_i^{(\ell-1)}$ contains the inside probabilities $\{ \alpha(C, i - \ell_1, i) \}_{C \in \gN}$ in the coordinates $[|\gN| \ell_1, |\gN| (\ell_1+1) )$. With the application of $\mK_{A}^{(\ell)}$, $\mK_{A}^{(\ell)} \ve_i^{(\ell-1)}$ contains the inside probabilities $\{ \alpha(C, i - \ell_1, i) \}_{C \in \gN}$ in the coordinates $[|\gN| (\ell - 1 - \ell_1), |\gN| (\ell - \ell_1) ).$
Hence, if we set 
%all blocks at positions $\{(|\gN| (L + \ell_1), |\gN| \ell_2)\}_{\ell_1, \ell_2 \le \ell}$ 
the block  at position $(|\gN| \ell_1, |\gN| \ell_1)$ in $\mQ_{A}^{(\ell)}$
to $\mP_A$ for some $0 \le \ell_1 < \ell$, with the rest set to $0$, we can get for any two positions $i, j$,

{\small
\begin{align*}
    & (\mK_{A}^{(\ell)} \ve_j^{(\ell-1)})^{\top}  \mQ_{A}^{(\ell)} \ve_i^{(\ell-1)} \\
    = & \sum_{B, C \in \gN}   \Pr[A \to B C] \cdot \alpha(B, i-\ell_1, i) \\
    &\quad \cdot  \alpha (C, j - (\ell - 1 - \ell_1), j).
\end{align*}
}



Setting the first $\ell$ diagonal blocks in $\mQ_{A}^{(\ell)}$ to $\mP_A$ can get for any two positions $i, j$,

{\small
\begin{align*}
    & (\mK_{A}^{(\ell)} \ve_j^{(\ell-1)})^{\top}  \mQ_{A}^{(\ell)} \ve_i^{(\ell-1)} \\
    =& \sum_{\ell_1 \le \ell-1} \sum_{B, C \in \gN}   \Pr[A \to B C] \cdot \alpha(B, i-\ell_1, i) \\
    &\quad \cdot  \alpha (C, j - (\ell - \ell_1 - 1), j).
\end{align*}
}

However, for $\alpha(A, i-\ell, i)$, the attention score above should only contribute with  $\ell_1 = i - j - 1$. Moreover, we also want the above sum to be $0$ if $j \ge i$ or $j \le i - \ell - 1$. Hence, we will use the relative position vector $p_{j - i}$, bias $b_{j-i, \ell}$ and the ReLU activation to satisfy the following conditions:
\begin{enumerate}
    \item $i - \ell \le j \le i-1$. 
    \item The portion containing $\{\alpha (C, j - (\ell - \ell_1 - 1), j)\}_{C \in \gN}$ in $\mK_{A}^{(\ell)} \ve_j^{(\ell-1)}$ is activated only if $\ell_1 = i - j - 1$.
\end{enumerate}

 %Thus, for any $\ell_1 \le \ell$, $\mK_{A}^{(\ell)} \ve_j^{(\ell-1)} + p_{j-i} - b_{j-i, \ell}$ will contain $\{ \alpha (C, j - (\ell-\ell_1), j) + \mathbb{I} [\ell_1 == i - j + 1]  - \mathbb{I} [  ] \}_{C \in \gN}$ in coordinates $[|\gN| \ell_1, |\gN| (\ell_1 + 1) )$, which will return

 For any positions $i, j$ and $\ell_1 < \ell$, $\mK_{A}^{(\ell)} \ve_j^{(\ell-1)} + p_{j-i} - b_{j-i, \ell}$ will contain $\{ \alpha (C, j - (\ell-\ell_1-1), j) + \mathbb{I} [\ell_1 = i - j - 1]  - 1 - \zeta \mathbb{I} [  j < i - \ell  \text{ or } j > i - 1 ] \}_{C \in \gN}$ in coordinates $[|\gN| \ell_1, |\gN| (\ell_1 + 1) )$, which will give us

 {\small
\begin{align*}
     & \text{ReLU}(\mK_{A}^{(\ell)} \ve_j^{(\ell-1)} + p_{j-i} - b_{j-i, \ell})^{\top}  \mQ_{A}^{(\ell)} \ve_i^{(\ell-1)} \\
     =& \sum_{B, C \in \gN} \Pr[A \to B C] \cdot \alpha(B, j+1, i) \cdot  \alpha (C, i-\ell, j),
\end{align*}
}
if $i - \ell \le j \le i-1$ and $0$ otherwise. Summing over all locations $j$ gives us $\alpha(A, i-\ell, i)$.




%we will set blocks at positions $\{(|\gN| (L + \ell_2), |\gN| \ell_1 )\}_{\ell_1, \ell_2 : \ell_1 + \ell_2 = \ell-1}$ to $\mP_A$, while setting the rest to $0$. This gives us
%\begin{align*}
%    &(\mK_{A}^{(\ell)} \ve_j^{(\ell-1)})^{\top} \mQ_{A}^{(\ell)} \ve_i^{(\ell-1)} = \sum_{B, C \in \gN} \sum_{\substack{\ell_1, \ell_2 \ge 0\\ \ell_1 + \ell_2 = \ell-1}}  \Pr[A \to B C]  \cdot \alpha(B, i, i+\ell_1) \cdot  \alpha (C, j -\ell_2, j) .
%\end{align*}


%However, we want $(\mK_{A}^{(\ell)} \ve_j^{(\ell-1)})^{\top} \mQ_{A}^{(\ell)} \ve_i^{(\ell-1)}$ to compute $\alpha(A, i, j)$ iff $j = i + \ell$ and $0$ otherwise, so we will use the final block in $\mQ_{A}^{(\ell)}$ that focuses on the one-hot position encodings of $i$ and $j$ to differentiate the different location pairs. Specifically, the final block $\mQ_p$ will return $0$ if $j = i + \ell$, while it returns $-\zeta$ for some large constant $\zeta$ if $j \ne i + \ell$. This gives us
%{\small
%\begin{align}
%    &(\mK_{A}^{(\ell)} \ve_j^{(\ell-1)})^{\top} \mQ_{A}^{(\ell)} \ve_i^{(\ell-1)} = \zeta(\mathbb{I}[j - i = \ell] - 1) + \sum_{B, C \in \gN} \sum_{\substack{\ell_1, \ell_2 \ge 0\\ \ell_1 + \ell_2 = \ell-1}}  \Pr[A \to B C] \cdot \alpha(B, i, i+\ell_1) \cdot  \alpha (C, j -\ell_2, j). \label{eq:attnt_head_inside_construct}
%\end{align}
%}
%With the inclusion of the term $\zeta(\mathbb{I}[j - i = \ell ] - 1)$, we make $(\mK_{A}^{(\ell)} \ve_j^{(\ell-1)})^{\top} \mQ_{A}^{(\ell)} \ve_i^{(\ell-1)}$ positive if $j - i = \ell$, and negative if $j - i \ne \ell$. Applying a ReLU activation on top will zero out the unnecessary terms, leaving us with $\alpha(A, i, i+\ell)$ at each location $i$.


 




%Specifically, denote $\mP_A\in \R^{|\gN|\times |\gN|}$ the probabilities of rules splitted from non-terminal $A$. $\mM_{A}^{(\ell)}$ is a $4L\times 4L$ block matrix where each block has size $|\gN|\times |\gN|$. All the blocks except $(|\gN|L+k_1, k_2)$ where $k_1+k_2 = \ell$ in $\mM_{A,\ell}$ are zero, and the blocks at position $(|\gN|L+k_1, k_2)$ are $\mP_A$. To implement this as self-attention, we can set $\mK_A^{(\ell)} = \mI$ the identity matrix, and $\mQ_A^{(\ell)} = \mM_A^{(\ell)}$.

%We also need to make sure that the attention score between $i$ and $i+\ell'-1$ for $\ell'\neq \ell$ is smaller than the attention score between $i$ and $i+\ell-1$ (and thus the hard attention head will compute the probability between $i$ and $i+\ell-1$ by taking the max). We can achieve this goal by using positional embeddings. Specifically, if we use the one-hot positional embeddings, i.e., $\vp_i[i] = 1$ if the position is $i$ and $0$ otherwise. Then, we can set $\mK_p^{(\ell)} = \mI$, and $\mQ_p^{(\ell)}[i,i+\ell-1] = 0$ and all other entries to be $-1$. Then, $\vp_i^\top \mK_p^\top \mQ_p \vp_{i+\ell-1} = 0$ and $\vp_i^\top \mK_p^\top \mQ_p \vp_{j} = -1$ for all $j \neq i+\ell-1$, and using $|N|$ hard attentions together we can compute $\alpha(A,i,i+\ell-1)$.

%Similarly, we use another $|\gN|$ attention heads to compute $\alpha(A,i-\ell, i)$. In the end, we use the residual connections to copy the previously computed inside probabilities $\alpha(A,i-\ell', i)$ and $\alpha(A,i, i+\ell')$ for $\ell' < \ell$.

%in order to compute $\sum_{\ell_1, \ell_2 \le \ell} \sum_{B, C \in \gN} \Pr[A \to B C] \cdot \alpha(B, i, i+\ell_1) \cdot  \alpha (C, j -\ell_2, j)$, 

%we can set all the first $4L$ blocks to $\mP_A\in \R^{|\gN|\times |\gN|}$ that contains $\{\Pr[A \to BC]\}_{B,C \in \gN}.$


%$\mQ_{A, \ell}$ is defined as a block-diagonal matrix, where all $\{(L + k, \ell - k)\}_{1 \le k \le \ell}$ blocks store $\mP_A\in \R^{|\gN|\times |\gN|}$ that contains $\{\Pr[A \to BC]\}_{B,C \in \gN}.$ The last block is a $L \times L$ matrix $\mQ_p$ such that $\mQ_p[i,i+\ell-1] = 0$ for $0 \le i < L$, with the rest set to $-1$. The rest of the blocks are set as $0$. The explanation for this structure of $\mQ_{A, \ell}$ is as follows.



%\haoyu{revise to here, following also needs to be revised.}

\paragraph{Outside probabilities:}

%For layer $\ell > L$, the $(k,A)$-th coordinate denotes the inside probability $\alpha(A, i+1, i+k)$ and the $(k,A)+|\gN|L$-th coordinate denotes the inside probability $\alpha(A, i-k, i-1)$ for all $k$. (Note that now position $i$ stores the inside probabilities for spans starting at $i+1$ and ending at $i-1$.) The $(k,A) + 2|\gN| L$-th coordinate denotes the outside probability $\beta(A, i, i+k-1)$, and the $(k,A) + 3|\gN| L$-th coordinate denotes the outside probability $\beta(A, i-k+1, i)$ at the current step ($\beta(A, i, i+k-1) = \beta(A, i-k+1, i) = 0$ for all $k > \ell - L$. The last $L$ coordinates denote the positional embeddings. We denote the first $4|\gN| L$ coordinates of $\ve_i^{(\ell)}$ to be $\vv_i^{(\ell)}$, and the positional embedding in $\ve_i^{(\ell)}$ as $\vp_i$. Thus, $\ve_i^{(\ell)} = (\vv_i^{(\ell)}, \vp_i$

In addition to all the inside probabilities, the contextual embeddings at position $i$ after the computations of any layer $(2L - 1) - \ell$ ($\ge L$) contain the outside probabilities of all spans of length at least $\ell + 1$ starting at position $i$, i.e.  $\beta(A, i, i + k)$ for all $A \in \gN$ and $k \ge \ell $. The rest of the coordinates contain $0$.

\paragraph{Layer $L$}
In this layer, we initialize the outside probabilities $\beta(\text{ROOT}, 1, L) = 1$ and $\beta(A, 1, L) = 0$ for $A\neq \text{ROOT}$. Furthermore, we move the inside probabilities $\alpha(A,i-k,i-1)$ from position $i-1$ to position $i$ using 1 attention head. For the attention head, $b_{-1, L}$ is set as $0$, while the rest are set as $\zeta$ for some large constant $\zeta$ so that the attention heads only attend to position $i-1$ at any position $i$.


\paragraph{Layer $L + 1 \le \tilde{\ell} := (2L - 1) - \ell \le 2L - 1$:} 
At each position $i$, this layer computes the outside probabilities of spans of length $\ell + 1$ starting at $i$, using the recursive formulation from eq.~\ref{eq:outside_probability}. The recursive formulation for $\beta(A, i, i + \ell)$ for a non-terminal $A \in \gN$ has two terms, given by

{\small
\begin{align}
     \beta(A,i,i+\ell)  =& \beta_1(A,i,i+\ell) + \beta_2(A, i, i+\ell), \text{ with   } \label{eq:beta1_beta2_soft}\\
    \beta_1(A,i,i+\ell) =& \sum_{j=1}^{i-1}  \sum_{C,B \in \gN} \Pr[B \to C A] \nonumber \\
    &\quad\cdot \alpha(C, j, i-1) \beta(B, j, i+\ell), \text{ and } \label{eq:beta1_soft} \\
    \beta_2(A, i, i+\ell) =& \sum_{j=i+\ell+1}^{L} \sum_{B,C \in \gN}  \Pr[B \to A C] \nonumber \\
    &\quad \cdot \alpha(C, i+\ell+1, j) \beta(B, i, j). \label{eq:beta2_soft}
\end{align}
}

For each non-terminal $A \in \gN$, we will use a single unique head to compute $\beta(A, i, i+\ell)$ with query matrix $\mQ_{A}^{(\Tilde{\ell})}$ and key matrix $\mK_{A}^{(\Tilde{\ell})}$. Combining the operations of both $\beta_1$ and $\beta_2$ in a single attention head is the main reason behind the decrease in the number of necessary attention heads, compared to \cref{thm:hard_attnt}.
%We outline the construction for $\beta_1$; the construction for $\beta_2$ follows similarly.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\paragraph{Computing Eq.~\ref{eq:beta1_beta2_soft}} 

\paragraph{Structure of $\{ b_{ t, \ell } \}_{-L \le t \le L, L+1 \le \ell \le 2L-1}$:} For any $L+1 \le \ell \le 2L-1$, for $0 \le t \le \ell + 1$, $b_{t, \ell}$ is set as $\zeta$ for some large constant $\zeta$. 
%for all $t \ne 0$, we set $b_{t, \ell}$ as $1$ for some large constant $\zeta$. 
All other biases are set as $1$.

\paragraph{Structure of Query and key matrices:}    
\begin{enumerate}
    \item $\mK_{A}^{(\tilde\ell)}$ is a rotation matrix such that in $\mK_{A}^{(\tilde\ell)} \ve_i^{(\ell)}$, for all $L > \ell_1 > \ell$, the outside probabilities $ \{ \beta(B, i, i+\ell_1) \}_{B \in \gN}$ appears in the coordinates $[ |\gN| (\ell_1 -\ell - 1),  |\gN| (\ell_1 -\ell) )$.  Furthermore, for all $0 \le \ell_1 \le L - \ell - 2$, the inside probabilities $ \{ \alpha(C, i-1-\ell_1, i-1) \}_{C \in \gN}$ appears in the coordinates $[ |\gN| (L + \ell + \ell_1 + 1 ),  |\gN| (L + \ell + \ell_1 + 2) )$. Note that $\mK_A^{(\tilde\ell)}$ is same for all $A$, and only depends on $\ell$.
    
    \item The Query matrix  $\mQ_{A}^{(\tilde\ell)}$ is a block diagonal matrix. If we define $\mP_{A, r} \in \R^{|\gN|\times |\gN|}$ as a matrix that contains $\{\Pr[B \to CA]\}_{B,C \in \gN},$ which is the set of all rules where $A$ appears as the right child, $\mP_{A, r}$ appears at positions $(|\gN| \ell_1, |\gN| \ell_1)$ for all $\ell_1 < L$, which is the set of the first $L$ blocks along the diagonal. Furthermore, if we define $\mP_{A, l} \in \R^{|\gN|\times |\gN|}$ as a matrix that contains $\{\Pr[B \to AC]\}_{B,C \in \gN},$ which is the set of all rules where $A$ appears as the left child, $\mP_{A, l}^{\top}$ appears at positions $(|\gN| \ell_1, |\gN| \ell_1)$ for all $\ell_1 \ge L+\ell+1$, which is a set of $L-\ell-2$ blocks along the diagonal located towards the end.
    
    %,  such that if we define $\mP_A\in \R^{|\gN|\times |\gN|}$ that contains $\{\Pr[A \to BC]\}_{B, C \in \gN},$ $\mP_A$ appears in the first $\ell+1$ blocks along the diagonal, i.e. it occurs at all positions starting at $( |\gN| \ell_1,  |\gN| \ell_1 )$ for all $\ell_1 \le \ell$. The rest of the blocks are set as $0$s.
\end{enumerate}
%We build the attention head in the same way we built the attention head to represent the inside probabilities in eq.~\ref{eq:attnt_head_inside_construct}. Similar to \ref{eq:attnt_head_inside_construct}, we modify the formulation of $\beta_1$ to highlight the interaction of spans of different lengths.
%\begin{align}
%    \beta_1(A,i,j) = \sum_{B, C \in \gN} \sum_{\substack{\ell_1, \ell_2 \ge 0\\ \ell_2 - \ell_1 = \ell }} \Pr[B \to C A] \alpha(C, i-\ell_1, i-1) \beta(B, j-\ell_2, j), \label{eq:construction-beta1}
%\end{align}
%where $j = i+\ell$. We represent this computation as the attention score $a_{i, i+\ell}$ using a key matrix $\mK_{A, 1}^{(\tilde{\ell})}$ and query matrix $\mQ_{A, 1}^{(\tilde{\ell})}$. 
%We want the $A$-th attention head in layer $\ell$ compute the score
%\begin{align*}
%& \sum_{A \rightarrow BC \in R}\sum_{k=i+1}^{i+\ell-2} (P(A \rightarrow B C) \\
%& \quad \times \alpha(B, i, k) \times \alpha(C, k+1, i+\ell-1)),
%\end{align*}
%which is exactly the inside probability $\alpha(A,i,j)$ for all $j = i+\ell-1$.
%Note that there exists a matrix $\mM_{A}^{(\ell)}$ such that
%{\small
%\begin{align*}
%    & \dotp{\vv_i^{(\ell-1)}}{\mM_{A}^{(\ell)} \cdot \vv_{i+\ell-1}^{(\ell-1)}} \\
%    =& \sum_{A \rightarrow BC }\sum_{k=i+1}^{i+\ell-2} P(A \rightarrow B C) \alpha(B, i, k) \alpha(C, k+1, j)).
%\end{align*}
%}
%$\{(L + k, \ell - k)\}_{1 \le k \le \ell}$ 
%First, we set the Key matrix $\mK_{A, 1}^{(\tilde{\ell})}$ as $\mI$. If we define $\mP_{A, r} \in \R^{|\gN|\times |\gN|}$ as a matrix that contains $\{\Pr[B \to CA]\}_{B,C \in \gN},$ which is the set of all rules where $A$ appears as the right child, $\mQ_{A, 1}^{(\tilde{\ell})}$ is set such that $\mP_{A, r}$ appears at positions $[|\gN| (3L + \ell_2), |\gN| (L + \ell_1))$ for all $0 \le \ell_1, \ell_2 \le L$ that satisfy $\ell_2 - \ell_1 = \ell$. Finally, $\mQ_{A, 1}^{(\tilde{\ell})}$ contains $\mQ_p\in \R^{L \times L} $ at position $(4|\gN|L, 4|\gN|L)$, such that $\mQ_p[i,i+\ell] = 0$ for $0 \le i < L$, with the rest set to $-\zeta$ for some large constant $\zeta$. The rest of the blocks are set as $0$. We give an intuition behind the structure of $\mQ_{A,1}^{(\tilde{\ell})}$ below.

\paragraph{Intuition behind $\mQ_{A}^{(\tilde\ell)}$, $\mK_{A}^{(\tilde\ell)}$, the relative position embeddings and the biases:} Considering any location $i$, we split the computation of $\beta(A, i, i+\ell)$ with the attention head into the computation of $\beta_1$ (eq.~\ref{eq:beta1_soft}) and $\beta_2$ (eq.~\ref{eq:beta2_soft}). For $\beta_1$, we express each term $\sum_{C,B \in \gN} \Pr[B \to C A] \alpha(C, j, i-1) \beta(B, j, i+\ell)$ as the attention score $a_{i, j}$ and then express $\beta_1$ as $\sum_{j \le i-1} a_{i, j}$. Similarly, for $\beta_2$, we express each term $\sum_{B,C \in \gN}  \Pr[B \to A C] \alpha(C, i+\ell+1, j) \beta(B, i, j)$ as the attention score $a_{i, j}$ and then express $\beta_1$ as $\sum_{j \ge i+\ell+1} a_{i, j}$. The relative position vectors and biases help to differentiate the operations on the left and right-hand sides of $i$, as we showcase below.

\subparagraph{Computing $\beta_1$ (eq.~\ref{eq:beta1_soft}):}
For any position $i$ and  $\ell_1 \ge 0$,  $\ve_i^{(\tilde\ell-1)}$ contains the inside probabilities $\{ \alpha(C, i - 1 - \ell_1, i-1) \}_{C \in \gN}$ in the coordinates $[|\gN|\ell_1 , |\gN| (\ell_1 + 1) )$. With the application of $\mK_{A}^{(\tilde\ell)}$, for $\ell_1 > \ell$, $\mK_{A}^{(\tilde\ell)} \ve_i^{(\tilde\ell-1)}$  contains the outside probabilities $\{ \beta(B, i,  i + \ell_1) \}_{B \in \gN}$ in the coordinates $[|\gN| (\ell_1 - \ell - 1), |\gN| (\ell_1 - \ell) ).$
Hence, if we set 
%all blocks at positions $\{(|\gN| (L + \ell_1), |\gN| \ell_2)\}_{\ell_1, \ell_2 \le \ell}$ 
the block  at position $(|\gN| \ell_1, |\gN| \ell_1)$ in $\mQ_{A}^{(\ell)}$
to $\mP_{A, r}$ for some $L > \ell_1 \ge 0$, with the rest set to $0$, we can get for any two positions $i, j$,

{\small
\begin{align*}
    & (\mK_{A}^{(\tilde\ell)} \ve_j^{(\tilde\ell-1)})^{\top}  \mQ_{A}^{(\tilde\ell)} \ve_i^{(\tilde\ell-1)} \\
    =& \sum_{B, C \in \gN}   \Pr[B \to CA] \cdot \alpha(C, i-1-\ell_1, i-1) \\
    &\quad \cdot  \beta (B, j, j + \ell + \ell_1 + 1).
\end{align*}
}


Setting the first $L$ diagonal blocks in $\mQ_{A}^{(\tilde\ell)}$ to $\mP_{A, r}$ can get for any two positions $i, j$,

{\small
\begin{align*}
    & (\mK_{A}^{(\tilde\ell)} \ve_j^{(\tilde\ell-1)})^{\top}  \mQ_{A}^{(\tilde\ell)} \ve_i^{(\tilde\ell-1)} \\
    =& \sum_{\ell_1 \ge 0} \sum_{B, C \in \gN}   \Pr[B \to CA] \cdot \alpha(C, i-1-\ell_1, i-1)\\
    &\quad \cdot  \beta (B, j, j + \ell + \ell_1 + 1).
\end{align*}
}

However, for $\beta_1(A, i, i+\ell)$, the attention score above should only contribute with  $\ell_1 = i - j - 1$. Moreover, we also want the above sum to be $0$ if $j \ge i$. Hence, we will use the relative position vector $p_{j - i}$, bias $b_{j-i, \tilde\ell}$ and the ReLU activation to satisfy the following conditions:
\begin{enumerate}
    \item $j < i$. 
    \item The portion containing $\{\beta (B, j, j + \ell + \ell_1 + 1)\}_{C \in \gN}$ in $\mK_{A}^{(\tilde\ell)} \ve_j^{(\tilde\ell-1)}$ is activated only if $\ell_1 = i - j - 1$.
\end{enumerate}

 %Thus, for any $\ell_1 \le \ell$, $\mK_{A}^{(\ell)} \ve_j^{(\ell-1)} + p_{j-i} - b_{j-i, \ell}$ will contain $\{ \alpha (C, j - (\ell-\ell_1), j) + \mathbb{I} [\ell_1 == i - j + 1]  - \mathbb{I} [  ] \}_{C \in \gN}$ in coordinates $[|\gN| \ell_1, |\gN| (\ell_1 + 1) )$, which will return

 For any positions $i, j$ and $0 \le \ell_1 \le L$, $\mK_{A}^{(\tilde\ell)} \ve_j^{(\tilde\ell-1)} + p_{j-i} - b_{j-i, \tilde\ell}$ will contain $\{ \beta (B, j, j + \ell + \ell_1 + 1) + \mathbb{I} [\ell_1 = i - j - 1]  - 1 - \zeta \mathbb{I} [  i \le j \le i + \ell ] \}_{B \in \gN}$ in coordinates $[|\gN| \ell_1, |\gN| (\ell_1 + 1) )$, which will give us

{\small
\begin{align*}
     &\text{ReLU}(\mK_{A}^{(\tilde\ell)} \ve_j^{(\tilde\ell-1)} + p_{j-i} - b_{j-i, \tilde\ell})^{\top}  \mQ_{A}^{(\tilde\ell)} \ve_i^{(\tilde\ell-1)} \\ =& \sum_{C,B \in \gN} \Pr[B \to C A] \alpha(C, j, i-1) \beta(B, j, i+\ell),
\end{align*}
}

iff $j < i$ and $0$ otherwise. Summing over all locations gives us $\beta_1(A, i, i+\ell)$.





\subparagraph{Computing $\beta_2$ (eq.~\ref{eq:beta2_soft}):}
For any position $i$ and  $L > \ell_1 > \ell$,  $\ve_i^{(\tilde\ell-1)}$ contains the outside probabilities $\{ \beta(B, i, i+\ell_1) \}_{B \in \gN}$ in the coordinates $[|\gN| (L+\ell_1) , |\gN| (L + \ell_1 + 1) )$. With the application of $\mK_{A}^{(\tilde\ell)}$, for $L > \ell_1 > \ell$, $\mK_{A}^{(\tilde\ell)} \ve_i^{(\tilde\ell-1)}$  contains the inside probabilities $\{ \alpha(C, i-1-\ell_1,  i-1) \}_{C \in \gN}$ in the coordinates $[|\gN| (L + \ell + \ell_1 + 1), |\gN| (L + \ell + \ell_1 + 2) ).$
Hence, if we set 
%all blocks at positions $\{(|\gN| (L + \ell_1), |\gN| \ell_2)\}_{\ell_1, \ell_2 \le \ell}$ 
the block  at position $(|\gN| \ell_1, |\gN| \ell_1)$ in $\mQ_{A}^{(\tilde\ell)}$
to $\mP_{A, l}^\top$ for some $\ell_1 \ge L + \ell + 1$, with the rest set to $0$, we can get for any two positions $i, j$,

{\small
\begin{align*}
    & (\mK_{A}^{(\tilde\ell)} \ve_j^{(\tilde\ell-1)})^{\top}  \mQ_{A}^{(\tilde\ell)} \ve_i^{(\tilde\ell-1)} \\
    =& \sum_{B, C \in \gN}   \Pr[B \to AC] \cdot \alpha(C,  j-\ell_1+\ell+L, j-1) \\
    &\quad \cdot  \beta (B, i, i + \ell_1 - L).
\end{align*}
}



Setting diagonal blocks at positions $\{ (|\gN|\ell_1, |\gN|\ell_1) \}_{\ell_1 \ge L+\ell+1}$ in $\mQ_{A}^{(\tilde\ell)}$ to $\mP_{A, l}^\top$ can get for any two positions $i, j$,

{\small
\begin{align*}
    & (\mK_{A}^{(\tilde\ell)} \ve_j^{(\tilde\ell-1)})^{\top}  \mQ_{A}^{(\tilde\ell)} \ve_i^{(\tilde\ell-1)}\\
    =& \sum_{ \ell_1 \ge \ell + 1 } \sum_{B, C \in \gN}   \Pr[B \to AC] \cdot \alpha(C, j-\ell_1+\ell, j-1) \\
    &\quad \cdot  \beta (B, i, i + \ell_1).
\end{align*}
}

However, for $\beta_1(A, i, i+\ell)$, the attention score above should only contribute with  $\ell_1 = j - i - 1$. Moreover, we also want the above sum to be $0$ if $j \le i+\ell$. 
%First of all, we observe that the contribution from $\ell_1 \le \ell$ is $0$, since all $\beta(A, i, i+\ell_1)$ for $\ell_1 \le \ell$ haven't been computed yet. 
We will use the relative position vector $p_{j-i}$, bias $b_{j-i, \tilde\ell}$ and the ReLU activation to satisfy the following conditions:
\begin{enumerate}
    \item $j > i+\ell$. 
    \item The portion containing $\{ \alpha(C, j-\ell_1+\ell, j-1) \}_{C \in \gN}$ in $\mK_{A}^{(\tilde\ell)} \ve_j^{(\tilde\ell-1)}$ is activated only if $\ell_1 = j - i - 1$.
\end{enumerate}
%Because of the way we activate the necessary portion in $\mK_{A}^{(\tilde\ell)} \ve_j^{(\tilde\ell-1)}$ using the relative position vectors and biases, for $i \le j < i + \ell$ the above dot product is $0$ as all $\beta(A, i, i+\ell_1)$ for $\ell_1 \le \ell$ haven't been computed yet and are currently $0$s. 


 %Thus, for any $\ell_1 \le \ell$, $\mK_{A}^{(\ell)} \ve_j^{(\ell-1)} + p_{j-i} - b_{j-i, \ell}$ will contain $\{ \alpha (C, j - (\ell-\ell_1), j) + \mathbb{I} [\ell_1 == i - j + 1]  - \mathbb{I} [  ] \}_{C \in \gN}$ in coordinates $[|\gN| \ell_1, |\gN| (\ell_1 + 1) )$, which will return

 Thus, for any positions $i, j$ and $0 \le \ell_1 \le L$, $\mK_{A}^{(\tilde\ell)} \ve_j^{(\tilde\ell-1)} + p_{j-i} - b_{j-i, \tilde\ell}$ will contain $\{  \alpha(C, j-\ell_1+\ell, j-1) + \mathbb{I} [\ell_1 = i - j - 1]  - 1 - \zeta \mathbb{I} [ i \le j \le i+\ell ] \}_{C \in \gN}$ in coordinates $[|\gN| \ell_1, |\gN| (\ell_1 + 1) )$, which will give us

 {\small
\begin{align*}
     &\text{ReLU}(\mK_{A}^{(\tilde\ell)} \ve_j^{(\tilde\ell-1)} + p_{j-i} - b_{j-i, \tilde\ell})^{\top}  \mQ_{A}^{(\tilde\ell)} \ve_i^{(\tilde\ell-1)}\\
     =& \sum_{j=i+\ell+1}^{L} \sum_{B,C \in \gN}  \Pr[B \to A C] \alpha(C, i+\ell+1, j) \beta(B, i, j),
\end{align*}
}

iff $j > i + \ell + 1$ and $0$ otherwise. Summing over all locations gives us $\beta_2(A, i, i+\ell)$.

\subparagraph{Computing $\beta_1 + \beta_2$ (eq.~\ref{eq:beta1_beta2_soft}):} From our construction, $\beta_1$ requires the dot product of the inside probabilities stored at the query vector and the outside probabilities stored at the key vector. However,  $\beta_2$ requires the dot product of the outside probabilities stored at the query vector and the inside probabilities stored at the key vector. Since $\beta_1$ and $\beta_2$ are computed on the left and the right-hand side of the query respectively, we use the relative position embeddings to separate the two operations. The vector $p_{j-i}$ activates only the outside probabilities in the key vector when $j > i$ and activates only the inside probabilities in the key vector when $j < i$. Thus, we can compute $\beta_1+\beta_2$ as the sum of the attention scores of a single head, where the computation of $\beta_1$ and $\beta_2$ have been restricted to the left and the right-hand side of the query respectively.


%For position $i$ and any ranges $1 \le \ell_1 < L$, $\ell+1 \le \ell_2 \le L$, $\ve_i^{( \tilde{\ell} - 1 )}$ contains the inside probabilities $\{ \alpha(C, i - \ell_1, i-1) \}_{C \in \gN}$ in the coordinates $[ |\gN| (L+\ell_1), |\gN| (L+\ell_1+1) )$, while it contains the outside probabilities $\{ \beta(B, i - \ell_2, i) \}_{B \in \gN}$ in the coordinates $[|\gN| (3L+\ell_2), |\gN| (3L+\ell_2+1) ).$ Hence, if we set the block at position $(|\gN| (3L + \ell_2), |\gN| (L + \ell_1))$
%to $\mP_A$ for some $0 \le \ell_1 \le L, \ell+1 \le \ell_2 \le L$, with the rest set to $0$, we can get for any two positions $i, j$,
%\begin{align*}
%    & (\mK_{A}^{(\tilde{\ell})} \ve_j^{(\tilde{\ell}-1)})^{\top}  \mQ_{A}^{(\tilde{\ell})} \ve_i^{(\tilde{\ell}-1)} = \sum_{B, C \in \gN}   \Pr[B \to CA] \cdot \alpha(C, i-\ell_1, i-1) \cdot  \beta (B, j -\ell_2, j) .
%\end{align*}


%Hence, if we set all blocks at positions $\{(|\gN| (2L + \ell_1), |\gN| \ell_2)\}_{\ell_1 \le \ell, \ell_2 \ge L-\ell}$ to $\mP_A$, with the rest set to $0$, we can get for any two positions $i, j$,
%\begin{align*}
%    & (\mK_{A}^{(\ell)} \ve_j^{(\ell-1)})^{\top}  \mQ_{A}^{(\ell)} \ve_i^{(\ell-1)} = \sum_{B, C \in \gN} \sum_{\ell_1, \ell_2 \le \ell}   \Pr[A \to B C] \cdot \alpha(B, i, i+\ell_1) \cdot  \alpha (C, j -\ell_2, j) .
%\end{align*}

%Because we want to include the sum over $\ell_1, \ell_2$ pairs with $ \ell_2 - \ell_1 = \ell$, we will only set blocks at positions $[|\gN| (3L + \ell_2), |\gN| (L + \ell_1))$ for all $0 \le \ell_1, \ell_2 \le L$ that satisfy $\ell_2 - \ell_1 = \ell$ to $\mP_{A, r}$, while setting the rest to $0$. This gives us
%\begin{align*}
%    &(\mK_{A}^{(\tilde{\ell})} \ve_j^{(\tilde{\ell}-1)})^{\top}  \mQ_{A}^{(\tilde{\ell})} \ve_i^{(\tilde{\ell}-1)} = \sum_{B, C \in \gN}  \sum_{\substack{\ell_1, \ell_2 \ge 0 \\ \ell_2 - \ell_1 = \ell}}  \Pr[B \to CA] \cdot \alpha(C, i-\ell_1, i-1) \cdot  \beta (B, j -\ell_2, j).
%\end{align*}


%Because we want $(\mK_{A}^{(\tilde{\ell})} \ve_j^{(\tilde{\ell}-1)})^{\top}  \mQ_{A}^{(\tilde{\ell})} \ve_i^{(\tilde{\ell}-1)}$ to compute $\beta_1(A, i, j)$ with $j = i + \ell$ and $0$ otherwise, we will use the final block in $\mQ_{A}^{(\ell)}$ that focuses on the one-hot position encodings of $i$ and $j$ to differentiate the different location pairs. Specifically, the final block $\mQ_p$ will return $0$ if $j = i + \ell$, while it returns $-\zeta$ for some large constant $\zeta$, if $j \ne i + \ell$. This gives us
%\begin{align*}
%    &
%    (\mK_{A}^{(\tilde{\ell})} \ve_j^{(\tilde{\ell}-1)})^{\top}  \mQ_{A}^{(\tilde{\ell})} \ve_i^{(\tilde{\ell}-1)} =\zeta (\mathbb{I}[j - i = \ell ] - 1) + \sum_{B, C \in \gN}  \sum_{\substack{\ell_1, \ell_2 \ge 0\\ \ell_2 - \ell_1 = \ell} } \Pr[B \to CA] \cdot \alpha(C, i-\ell_1, i-1) \cdot  \beta (B, j -\ell_2, j) 
%\end{align*}
%With the inclusion of the term $\zeta(\mathbb{I}[j - i = \ell ] - 1)$, we make $(\mK_{A}^{(\tilde{\ell})} \ve_j^{(\tilde{\ell}-1)})^{\top}  \mQ_{A}^{(\tilde{\ell})} \ve_i^{(\tilde{\ell}-1)}$ positive if $j - i = \ell$, and negative if $j - i \ne \ell$. Applying a ReLU activation on top will zero out the unnecessary terms, leaving us with $\beta_1(A, i, i+\ell)$ at each location $i$.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%






\iffalse
Now at layer $\ell + L$ where $1 < \ell \le L$, we compute $\beta(A,i,i+L-\ell)$ for $i \le \ell$ and $\beta(A,i-L+\ell,i)$ for $i \ge \ell$ at position $i$. We first compute $\beta(A,i,i+L-\ell)$, which needs $2$ attention heads together. First note that there exist two matrices $\mM_{A,1}^{(\ell+L)}$ and $\mM_{A,2}^{(\ell+L)}$ such that
{\small
\begin{align*}
    &\dotp{\vv_i^{(\ell-1+L)}}{\mM_{A,1}^{(\ell+L)} \cdot \vv_{i+L-\ell}^{(\ell-1+L)}}
    = \sum_{B \rightarrow CA} \sum_{k=1}^{i-1}P[B \rightarrow C A]
    \cdot\alpha(C, k, i-1) \beta(B, k, i+L-\ell) \\
    &\dotp{\vv_i^{(\ell-1+L)}}{\mM_{A,2}^{(\ell+L)} \cdot \vv_{i+L-\ell}^{(\ell-1+L)}}
    = \sum_{B \rightarrow A C} \sum_{k=i+L-\ell+1}^{L}P[B \rightarrow A C] \cdot\alpha(C, i+L-\ell+1, k) \beta(B, i, k).
\end{align*}
}
Specifically, we denote $\mP_{A,r} \in \R^{|\gN|\times |\gN|}$ the probabilities of rules that $A$ is the right child (the different columns of $\mP_{A,r}$ denote different parent non-terminals $B$ and different rows denote different left child $C$). We denote $\mP_{A,l} \in \R^{|\gN|\times |\gN|}$ the probabilities of rules that $A$ is the left child (the different rows of $\mP_{A,l}$ denote different parent non-terminals $B$ and different columns denote different right child $C$).

Then, $\mM_{A,1}^{(\ell+L)}$ is a $4L\times 4L$ block matrix where each block has size $|\gN|\times |\gN|$. All the blocks except $(|\gN|L+k_1, 3|\gN|L+k_2)$ where $k_2-k_1 = L - \ell+1$ in $\mM_{A,1}^{(\ell+L)}$ are zero, and the blocks at position $(|\gN|L+k_1, 3|\gN|L+k_2)$ are $\mP_{A,r}$. Similarly, $\mM_{A,2}^{(\ell+L)}$ is a $4L\times 4L$ block matrix where each block has size $|\gN|\times |\gN|$. All the blocks except $(2|\gN|L+k_1,k_2)$ where $k_1 - k_2 = L - \ell+1$ in $\mM_{A,2}^{(\ell+L)}$ are zero, and the blocks at position $(2|\gN|L+k_1,k_2)$ are $\mP_{A,l}$.
\fi 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%Besides, we also need $2|\gN|$ additional heads for the outside probabilities $\beta(A,i-\ell,i)$. In the end, we use the residual connections to copy the previously computed inside probabilities $\beta(A, i-\ell', i)$ and $\alpha(A, i, i+\ell')$ for $\ell' > \ell$.
%and similar positional embeddings to make sure that $i$ is only attended to $i+L-\ell$ by using the ``hard attention''



\subsection{Proof of \cref{thm:io-optimal-mlm}} \label{sec:proof_io-optimal-mlm}
\input{mlm_proof}

