\section{More Experiment Results}
In this section, we provide more experiment results for RoBERTa pre-trained on PCFG-generated data. In \Cref{sec:more-parsing-result}, we show more structural probing results related to the experiments in \Cref{sec:parse}. In \Cref{sec:attn-patterns}, we do some simple analysis on the attention patterns for RoBERTa pre-trained on PCFG-generated data, trying to gain more understanding of the mechanism beneath large language models.

\subsection{Details for pre-training}\label{sec:pretraining-details}
\paragraph{Experiment setup}  We generate $10^7$ sentences for the training set from the PCFG, with an average length of $25$ words. The training set is roughly $10\%$ in size compared to the training set of the original RoBERTa which was trained on a combination of Wikipedia (2500M words) plus BookCorpus (800M words). We also keep a small validation set of $5 \times 10^4$ sentences generated from the PCFG to track the MLM loss. We follow \cite{izsak-etal-2021-train,wettig2022should} to pre-train all our models within a single day on a cluster of 8 RTX 2080 GPUs. Specifically, we train our models with AdamW \cite{loshchilov2017decoupled} optimization, using $4096$ sequences in a batch and hyperparameters $(\beta_1, \beta_2, \epsilon) = (0.9, 0.98, 10^{-6}).$ We follow a linear warmup schedule for $1380$ training steps with the peak learning rate of $2 \times 10^{-3}$, after which the learning rate drops linearly to $0$ (with the max-possible training step being $2.3 \times 10^{4}$). We report the performance of all our models at step $5 \times 10^{3}$ where the loss seems to converge for all the models.


\paragraph{Architecture} To understand the impact of different components in the encoder model, we pre-train different models by varying the number of attention heads and layers in the model. To understand the role of the number of layers in the model, we start from the RoBERTa-base architecture, which has 12 layers and 12 attention heads, and vary the number of layers to 1,3,6 to obtain $3$ different architectures. Similarily, to understand the role of the number of attention heads in the model, we start from the RoBERTa-base architecture and vary the number of attention heads to 3 and 24 to obtain $2$ different architectures.

\paragraph{Data generation from PCFG} Strings are generated from the PCFG $\gG = (\gN, \gI, \gP, n, p)$ as follows: We always maintain a string $s_t \in ([n]\cup \gN)^*$ at step $t$. The initial string $s_1 = \text{ROOT}$. At step $t$, if all characters in $s_t$ belong to $[n]$, the generation process ends, and $s_t$ is the resulting string. Otherwise, we pick a character $A\in s_t$ such that $A\in\gN$. If $A\in\gP$, we replace the character $A$ to $w$ with probability $\Pr[A\to w]$. If $A\in\gI$, we replace the character $A$ to two characters $BC$ with probability $\Pr[A\to BC]$.

%we take the original RoBERTa architecture as a baseline ($12$ layers, $12$ attention heads, and $768$ embedding dimension) and alter one component of the architecture at a time to get newer architectures, i.e. we either change the number of layers, attention heads or embedding size of the model to get newer models while keeping the other components fixed. 

\subsection{More results on constituency parsing}\label{sec:more-parsing-result}

\paragraph{More details on probing experiments}
In \Cref{sec:parse}, we mention that there are three settings: \dataset{PCFG}, \dataset{PTB}, and OOD. We generate two synthetic PCFG datasets according to the PCFG generation process: the first contains 10,000 sentences, which serves as the training set for probes, and the second contains 2,000 sentences, which serves as the test set for probes. As for the \dataset{PTB}, the training set for the probes consists of the first 10,000 sentences from sections 02-21, and we use \dataset{PTB} section 22 as the test set for the probes. In the \dataset{PCFG} setting, we train on the PCFG training set we generated, and test on the PCFG test set. In the \dataset{PTB} setting, we train on the \dataset{PTB} training set (10,000 sentences in sections 02-21) and test on the \dataset{PTB} test set (section 22). In the OOD setting, we train on the \dataset{PCFG} training set, while test on the \dataset{PTB} test set (section 22).

For the linear probe, we directly use Scikit-learn~\citep{scikit-learn}. For the 2-layer NN probe, we train the neural net with Adam optimizer with learning rate $1e-3$. We optimize for 800 epochs, and we apply a multi-step learning rate schedule with milestones $200,400,600$ and decreasing factor $0.1$. The batch size for Adam is chosen to be 4096.

\paragraph{Probing on embeddings from different layers} In \Cref{sec:parse}, we show the probing results on the embeddings either from $0$-th layer or from the best layer (the layer that achieves the highest F1 score) of different pre-trained models. In this section, we show how the F1 score changes with different layers.

\Cref{fig:f1-using-different-layers} shows sentence F1 scores for linear probes $f(\cdot)$ trained on different layers' embeddings for different pre-trained models. We show the results under the \dataset{PCFG} and \dataset{PTB} settings. From \Cref{fig:f1-using-different-layers}, we observe that using the embeddings from the $0$-th layer can only get sentence F1 scores close to (or even worse than) the naive Right-branching baseline for all the pre-trained models. However, except for model A3L12, the linear probe can get at least $60\%$ sentence F1 using the embeddings from layer 1. Then, the sentence F1 score increases as the layer increases, and gets nearly saturated at layer 3 or 4. The F1 score for the latter layers may be better than the F1 score at layer 3 or 4, but the improvement is not significant. The observations still hold if we change the linear probe to a neural network, consider the OOD setting instead of \dataset{PCFG} and \dataset{PTB}, or change the measurement from sentence F1 to corpus F1.

Our observations suggest that most of the constituency parse tree information can be encoded in the lower layers, and a lot of the parse tree information can be captured even in the first layer. Although our constructions (\Cref{thm:hard_attnt,thm:soft_attnt}) and approximations (\Cref{thm:approx-few-nt-informal,thm:approx-low-rank-informal}) try to reduce the number of attention heads and the number of embedding dimensions close to the real language models, we don't know how to reduce the number of layers close to BERT or RoBERTa (although our number is acceptable since GPT-3 has 96 layers). More understanding of how language models can process such information in such a small number of layers is needed.

%\haoyu{need a final ``conclusion'' sentence, like we need more understanding on how transformers process information in such a small number of layers.}

\begin{figure*}[!th]
\begin{subfigure}[t]{0.48\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figs/parsing-pcfg-diff-layers-linear.pdf}
    \caption{Comparison under \dataset{PCFG} setting. We compare the models with different number of layers.}
    \label{fig:parsing-pcfg-diff-layers-linear}
\end{subfigure}
\hfill
\begin{subfigure}[t]{0.48\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figs/parsing-pcfg-diff-attn-linear.pdf}
    \caption{Comparison under \dataset{PCFG} setting. We compare the models with different number of attention heads.}
    \label{fig:parsing-pcfg-diff-attn-linear}
\end{subfigure}

\begin{subfigure}[t]{0.48\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figs/parsing-ptb-diff-layers-linear.pdf}
    \caption{Comparison under \dataset{PTB} setting. We compare the models with different number of layers.}
    \label{fig:parsing-ptb-diff-layers-linear}
\end{subfigure}
\hfill
\begin{subfigure}[t]{0.48\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figs/parsing-ptb-diff-attn-linear.pdf}
    \caption{Comparison under \dataset{PTB} setting. We compare the models with different number of attention heads.}
    \label{fig:parsing-ptb-diff-attn-linear}
\end{subfigure}
    \caption{Sentence F1 for linear probes $f(\cdot)$ trained on different layers' embeddings for different pre-trained models. We show the results under \dataset{PCFG} and \dataset{PTB} settings. A$i$L$j$ denotes the pre-trained model with $i$ attention heads and $j$ layers.}
    \label{fig:f1-using-different-layers}
\end{figure*}

\paragraph{Comparison with probes using other input structures} In \Cref{sec:parse}, we train a probe $f(\cdot)$ to predict the relative depth $\text{tar}(i) = \text{depth}(i,i+1) - \text{depth}(i-1,i)$, and the input to the probe $f$ is the concatenation of the embedding $\ve^{(\ell)}_i$ at position $i$ and the embedding $\ve^{(\ell)}_{\text{EOS}}$ for the EOS token at some layer $\ell$. Besides taking the concatenation $[\ve^{(\ell)}_i; \ve^{(\ell)}_{\text{EOS}}]$ as the input structure of the probe, it is also natural to use the concatenation $[\ve^{(\ell)}_{i-1}; \ve^{(\ell)}_i; \ve^{(\ell)}_{i+1}]$ to predict the relative depth $\text{tar}(i)$. In this part, we compare the performances of probes with different input structures. We use EOS to denote the probe that takes $[\ve^{(\ell)}_i; \ve^{(\ell)}_{\text{EOS}}]$ as the input and predicts the relative depth, while ADJ (Adjacent embeddings) to denote the probe the takes $[\ve^{(\ell)}_{i-1}; \ve^{(\ell)}_i; \ve^{(\ell)}_{i+1}]$ as input.

\Cref{fig:parsing-diff-inputs} shows the probing results on A12L12, the model with 12 attention heads and 12 layers. We compare the probes with different inputs structure (EOS or ADJ), and the input embeddings come from different layers (the $0$-th layer or the layer that achieves the best F1 score). We observe that: (1) the probes using ADJ input structure have better parsing scores than the probes using EOS input structure, and (2) the sentence F1 for the probes using the ADJ input structure is high even if the input comes from layer 0 of the model ($>55\%$ for linear $f(\cdot)$ and $>60\%$ for neural network $f(\cdot)$). Although the probe using ADJ has better parsing scores than the probe using EOS, it is harder to test whether it is a good probe, since the concatenation of adjacent embeddings $[\ve^{(0)}_{i-1}; \ve^{(0)}_i; \ve^{(0)}_{i+1}]$ from layer $0$ is already contextualized, and it is hard to find a good baseline to show that the probe is \emph{sensitive} to the information we want to test. Thus, we choose to follow~\citet{vilares2020parsing,arps2022probing} and use the probe with input structure $[\ve^{(\ell)}_i; \ve^{(\ell)}_{\text{EOS}}]$ in \Cref{sec:parse}.

Nonetheless, the experiment results for probes taking $[\ve^{(0)}_{i-1}; \ve^{(0)}_i; \ve^{(0)}_{i+1}]$ as input are already surprising: by knowing three adjacent word identities and their position (the token embedding $\ve^{(0)}_i$ contains both the word embedding and the positional embedding) and train a 2-layer neural network on top of that, we can get $62.67\%, 63.91\%, 57.02\%$ sentence F1 scores under \dataset{PCFG}, \dataset{PTB}, and OOD settings respectively. As a comparison, the probe taking $[\ve^{(\ell)}_i; \ve^{(\ell)}_{\text{EOS}}]$ as input~\citep{vilares2020parsing,arps2022probing} only get $39.06\%, 39.31\%, 33.33\%$ sentence F1 under \dataset{PCFG}, \dataset{PTB}, and OOD settings respectively. It shows that lots of syntactic information (useful for parsing) can be captured by just using adjacent words without more context.


\begin{figure*}
    \begin{subfigure}[t]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/parsing-diff-inputs-linear.pdf}
        \caption{Comparison of different inputs under different settings when the probe $f(\cdot)$ is linear.}
        \label{fig:parsing-diff-inputs-linear}
    \end{subfigure}
    \begin{subfigure}[t]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/parsing-diff-inputs-nn.pdf}
        \caption{Comparison of different inputs under different settings when the probe $f(\cdot)$ is a 2-layer neural network.}
        \label{fig:parsing-diff-inputs-nn}
    \end{subfigure}
    \caption{Comparison of the probes with different inputs under different settings. We probe the model with 12 attention heads and 12 layers, and report the scores with $f(\cdot)$ taking embeddings from layer 0 or the embeddings from the best layer. EOS denotes the probe that takes $[\ve^{(\ell)}_i; \ve^{(\ell)}_{\text{EOS}}]$ as input and predicts the relative depth $\text{tar}(i)$, and ADJ (Adjacent embeddings) denotes the probe that takes $[\ve^{(\ell)}_{i-1}; \ve^{(\ell)}_i; \ve^{(\ell)}_{i+1}]$ as input.}
    \label{fig:parsing-diff-inputs}
\end{figure*}


\paragraph{More discussion on probing measurement} (Unlabelled) F1 score is the default performance measurement in the constituency parsing and syntactic probing literature. However, we would like to point out that only focusing on the F1 score may cause some bias. Because all the spans have equal weight when computing the F1 score, and most of the spans in a tree have a short length (if the parse tree is perfectly balanced, then length 2 spans consist of half of the spans in the parse tree), one can get a decently well F1 score by only getting correct on short spans. Besides, we also show that by taking the inputs $[\ve^{(0)}_{i-1}; \ve^{(0)}_i; \ve^{(0)}_{i+1}]$ from layer 0 of the model (12 attention heads and 12 layers), we can already capture a lot of the syntactic information useful to recover the constituency parse tree (get a decently well F1 score). Thus, the F1 score for the whole parse tree may cause people to focus less on the long-range dependencies or long-range structures, and focus more on the short-range dependencies or structures.

To mitigate this problem, \citet{vilares2020parsing} computed the F1 score not only for the whole parse tree, but also for each length of spans. \citet{vilares2020parsing} showed that BERT trained on natural language can get a very good F1 score when the spans are short (for length 2 spans, the probing F1 is over $80\%$), but when the span becomes longer, the F1 score quickly drops. Even for spans with length 5, the F1 score is less than $70\%$, and for spans with length 10, the F1 score is less than $60\%$. Our experiments that probe the marginal probabilities for different lengths of spans (\Cref{sec:probe-marginal-probs}) can also be viewed as an approach to mitigate the problem.

\subsection{More results on probing marginal probabilities}

In \cref{sec:probe-marginal-probs}, we conduct probing experiments to demonstrate the predictability of the "normalized marginal probabilities" computed by the Inside-Outside algorithm using transformer representations. Our objective is to establish a strong correlation, measured through the Pearson correlation coefficient. However, we have not provided a comprehensive explanation for our preference for Pearson correlation over alternative metrics such as Spearman correlation. In the following section, we show the experiment results measured by the Spearman correlation, and give an explanation of why we prefer the Pearson correlation over the Spearman correlation.



\begin{table*}
    \centering
    \footnotesize
    \begin{tabular}{|c|cccccc|}
    \hline
         \makecell{Span \\Length} & A12L12 & A12L1 & A12L3 & A12L6 & A3L12 & A24L12 \\
    \hline
        $\ell = 2$ &  .71 / \textbf{.93} & .69 / .88 &  .75 / \textbf{.93}  &  .71 / \textbf{.93}  &  .76 / .86 & .75 / \textbf{.92} \\
        $\ell = 5$ &  .59 / \textbf{.82} & .54 / .64 &  .47 / .79  &  .49 / .79  &  .54 / .71 & .48 / .79 \\
        $\ell = 10$ & .43 / \textbf{.78} & .48 / .68 & .59 / .73 & .45 / .75 & .33 / .62 & .39 / .72 \\
        \hline
    \end{tabular}
    \caption{Probing for the ``normalized'' marginal probabilities of spans at different lengths on different pre-trained models. We report the Spearman and Pearson correlations (separated by /) between the predicted probabilities and the span marginal probabilities computed by the Inside-Outside algorithm on \dataset{PTB} datasets for the 2-linear net probe. 
    %Two numbers in each entry denote the correlations with a linear probe and a 2-layer neural net probe respectively on the final layer of the model. 
    %The high correlation indicates that the MLM pre-trained models approximately encode the marginal span probabilities of the Inside-Outside algorithm during pre-training. 
    %The numbers come from a single run since the probe is stable across multiple runs.
    }
    \label{tab:probe-probs-ptb-spearman}
\end{table*}

\begin{figure*}
    \begin{subfigure}[t]{0.32\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figs/probe-prob-l2.png}
    \caption{Span length to probe: $\ell = 2$.}
    \label{fig:probe-prob-l2}
\end{subfigure}
\hfill
\begin{subfigure}[t]{0.32\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figs/probe-prob-l5.png}
    \caption{Span length to probe: $\ell = 5$.}
    \label{probe-prob-l5}
\end{subfigure}
\hfill
\begin{subfigure}[t]{0.32\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figs/probe-prob-l10.png}
    \caption{Span length to probe: $\ell = 10$.}
    \label{fig:probe-prob-l10}
\end{subfigure}
\caption{The predicted probability versus true normalized marginal probability plot for different span lengths $\ell$ using 2-layer NN probe with the 12-th layer's representations from A12L12 model. In each figure, we sample 200 points (each point corresponds to a span) to plot from the test set. The y-axis denotes the predicted probabilities and the x-axis denotes the true normalized marginal probabilities. The line shows the best linear fit for all the spans in the test set. We can observe that there are lots of points that have very small normalized marginal probabilities, and it is very hard to predict their rank correctly, thus resulting in a low Spearman correlation.}
\label{fig:probe-probe-pred-vs-true}
\end{figure*}

\paragraph{Measure with Spearman correlation} \cref{tab:probe-probs-ptb-spearman} summarizes the correlations between the predicted probabilities and the span marginal probabilities computed by the Inside-Outside algorithm on \dataset{PTB} datasets for the 2-linear net probes. It is evident that the Spearman correlation is significantly lower than the Pearson correlation, indicating that the probe primarily captures "linear" correlations rather than rank-based relationships.

In order to investigate the underlying cause of this phenomenon, we plot the predicted probabilities against the true normalized marginal probabilities, as shown in \cref{fig:probe-probe-pred-vs-true}. Numerous points have extremely small normalized marginal probabilities, particularly when the probe length $\ell$ is large (e.g., $\ell = 5, 10$). This observation aligns with the intuition that the probability of a randomly selected span existing in the constituency parse tree is low.

However, accurately predicting the exact rank for the points clustered near the origin proves to be extremely challenging, leading to a relatively low Spearman correlation. In contrast, when considering the Pearson correlation, the noise associated with predicting spans having low normalized marginal probabilities is relatively small compared to the overall "variance" of the data points. Furthermore, it is evident that the probe exhibits greater efficacy in capturing the "influential spans" characterized by large normalized marginal probabilities. Achieving relatively accurate predictions for these influential spans accounts for a significant portion of the observed variation, leading to a relatively high Pearson correlation.


\subsection{More details on control tasks}\label{sec:control-task}
%In probing experiments, it is crucial to ensure that the probing performance accurately reflects the presence of the specific information we intend to test. Consequently, it is undesirable for the probe to possess excessive power and be capable of learning all aspects (see \cref{sec:preliminary} for further discussions). \citet{chen2021probing} utilize the concept of ``sensitivity'' to assess the extent to which the probe captures the targeted information. The ``sensitivity'' of a probe is defined as the difference in probing performance between the layer of interest and the 0-th layer (see \cref{sec:parse} and \cref{sec:probe-marginal-probs} for further details). Intuitively, a large gap indicates that the probe fails to perform adequately using representations from the 0-th layer but achieves better performance when utilizing representations from a later layer, thus confirming the presence of the targeted information. In situations where there are two probe choices (e.g., a linear classifier or a 2-layer neural network), the option exhibiting greater ``sensitivity'' should be selected as it captures a relatively higher amount of the targeted information.

%\citet{hewitt2019designing} introduced another metric, known as ``selectivity'', to assess the degree to which the probe captures the targeted information. Broadly speaking, \citet{hewitt2019designing} devised a specific task referred to as the ``control task'' to evaluate the probe's capability to align with specific types of random labels. Subsequently, ``selectivity'' is defined as the difference in performance between the probe for the original task, utilizing the layer of interest, and the probe for the control task, also utilizing the layer of interest. Intuitively, a large gap suggests that the probe lacks sufficient expressive power, resulting in the performance boost originating from the representations of the layer being probed, thus confirming the presence of specific information. Similarly, in scenarios involving two probe choices (e.g., a linear classifier or a 2-layer neural network), the option exhibiting greater ``selectivity'' should be preferred as it captures a relatively higher amount of the targeted information.

%It is important to note that a probe with higher ``sensitivity'' does not necessarily imply larger ``selectivity''. Nevertheless, as demonstrated in the subsequent parts, the metrics of ``sensitivity'' and ``selectivity'' align for both the constituency parsing probes (\cref{sec:parse}) and the marginal probability probes (\cref{sec:probe-marginal-probs}).

In this section, we present more details for the design of the control task in~\citep{hewitt2019designing}, and also show the control task experiment for the marginal probability probes (\cref{sec:probe-marginal-probs}).

\paragraph{Control task} \citet{hewitt2019designing} considered control task for sequence labeling problems: Given a sentence $x_{1:T}$, the goal is to label each word $y_{1:T}$. For example, the Part-of-speech tagging problem and the dependency parsing all belong to the sequence labeling category, since for Part-of-speech tagging, $y_i$ is the POS tag of $x_i$, and for dependency parsing, $y_i$ is the parent of $x_i$ in the parse tree. For a sequence labeling problem, the control task for this sequence labeling problem consists of two key components:
\begin{enumerate}
    \item Structure: the output $\hat y_i$ of a word $x_i$ is a deterministic function of $x_i$, i.e., $\hat y_i = \phi(x_i)$.
    \item Randomness: The output $\hat y_i$ for each word $x_i$ is sampled independently at random.
\end{enumerate}

Then, the goal of the control task is to fit the labels $\hat y_{1:T}$ using the probe with the input $h_{1:T}$ where $h_{1:T}$ denote the hidden representations of the specific layer of the transformer. Please refer to Section 2 of \citet{hewitt2019designing} for more details and examples on control task.

%\paragraph{Control task for constituency parsing probe} For the constituency parsing probe in \cref{sec:parse}, it is easy to design a control task since in \cref{sec:parse} we reduce the constituency parsing problem to a sequence labeling problem that predicts the relative depth of the common ancestors between words. Specifically, we have $y_i = \text{tar}(i) = \text{depth}(i,i+1) - \text{depth}(i-1,i)$ for position $i$. Then for the control task, for each word $w$, we uniformly sample $\phi(w) \in \{-1,0,1\}$, and then define the labels for the control task as $\hat y_{1:T} = [\phi(x_1), \phi(x_2),\dots,\phi(x_T)]$.

\paragraph{Control task for marginal probability probe} For the marginal probability probe in \cref{sec:probe-marginal-probs}, we need to generalize the original control task from sequence labeling problem to span labeling problem. Given a span $x_{i:j}$, the original goal is to predict the normalized marginal probability $y_{i,j} = \text{tar}(i,j) = s(i,j) / \max_{j_1,j_2}s(j_1,j_2)$ where $s(i,j)$ is the marginal probability for span $i:j$ computed by the Inside-Outside algorithm. Now for each pair of words $w_1, w_2$, we uniformly sample $\phi(w_1,w_2)\in [0,1]$. Then for the sequence $x_{1:T}$, we have the label for the control task $\hat y_{i,j} = \phi(x_i, x_j)$.

\paragraph{\emph{Selectivity} is aligned with \emph{Sensitivity} for marginal probability probes}

In \cref{sec:control-task-main}, we design the control task for constituency parsing probes and show that \emph{selectivity} is aligned with \emph{sensitivity} (\cref{tab:parsing-control-task}). In this part, we show that \emph{selectivity} is aligned with \emph{sensitivity} for the marginal probability probes.
\cref{tab:marginal-prob-control-task} provides a summary of the performance of the constituency parsing probe and the marginal probability probes, employing different architectures (linear classifier and a 2-layer neural network with 16 hidden neurons), on the original task, control task, as well as the selectivity.

%Based on the results presented in \cref{tab:parsing-control-task}, it is observed that the probe with a 2-layer neural network achieves slightly higher accuracy in predicting the relative depth of common ancestors, leading to a higher F1 score in constituency parsing. However, its performance on the control task surpasses that of the probe with a linear classifier by a significant margin. This suggests that when using the ``selectivity'' metric, the linear probe outperforms the 2-layer neural network probe in recovering the constituency parse tree, aligning with the conclusions drawn using the ``sensitivity metric'' (see Figure \ref{fig:probe-parsing-comparison}, where the sensitivity of the linear probe is greater than that of the 2-layer neural network probe).

Based on the information presented in \cref{tab:marginal-prob-control-task}, it is evident that the probe utilizing a 2-layer neural network demonstrates superior performance in predicting span probabilities for the control task. Nonetheless, compared to the linear probe, the 2-layer neural network probe achieves significantly better results on the original task, resulting in a larger ``selectivity''. Analyzing \cref{fig:probe-prob-comparison}, we observe that the 2-layer NN probe exhibits significantly stronger predictive correlation than the linear probe at the 12-th layer of A12L12, while displaying similar performance at the 0-th layer, which contributes to a higher ``sensitivity''. Consequently, the ``selectivity'' metric aligns with the ``sensitivity'' metric for marginal probability probes, indicating that 2-layer NN probes capture a relatively greater amount of syntactic information.

\begin{table*}
    \footnotesize
    \centering
    \begin{tabular}{|c|c|ccccc|}
    \hline
         & Probe span length & 2 & 3 & 4 & 5 & 10 \\
         \hline
        \multirow{3}{*}{\rotatebox[origin=c]{90}{Linear}} & pred. marginal prob. & .88 & .79 & .69 & .62 & .51 \\
         & control task & .62 & .55 & .53 & .60 & .58 \\
         & selectivity & .26 & .24 & .16 & .02 & -.07 \\
         \hline
        \multirow{3}{*}{\rotatebox[origin=c]{90}{NN}} & pred. marginal prob. & .93 & .90 & .86 & .79 & .77 \\
         & control task & .66 & .66 & .69 & .66 & .68 \\
         & selectivity & .27 & .24 & .17 & .13 & .09 \\ 
         \hline
    \end{tabular}
    \caption{Computing the selectivity of marginal probability probes with linear and 2-layer NN architectures (see \cref{sec:probe-marginal-probs} and \cref{sec:control-task}). The ``pred. marginal prob.'' rows denote the probing results for the ``normalized'' marginal probabilities of spans at different lengths using the 12-th layer of A12L12. We report the Pearson correlation between the predicted probabilities and the span marginal probabilities computed by the Inside-Outside algorithm on \dataset{PTB} dataset. The ``control task'' rows denote the Pearson correlation between the predicted probabilities and the probabilities generated from the control task on \dataset{PTB} dataset using the 12-th layer of A12L12. The selectivity is the difference between the original task performance and the control task performance. We can observe that for spans with all lengths tested, the probe with 2-layer NN has a larger selectivity, especially when the probe length is large.}
    \label{tab:marginal-prob-control-task}
\end{table*}

\subsection{Analysis of attention patterns}\label{sec:attn-patterns}
In \Cref{sec:parse}, we probe the embeddings of the models pre-trained on synthetic data generated from PCFG and show that model training on MLM indeed \emph{captures} syntactic information that can recover the constituency parse tree. \Cref{thm:io-optimal-mlm} builds the connection between MLM and the Inside-Outside algorithm, and the connection is also verified in \Cref{sec:probe-marginal-probs}, which shows that the embeddings also contain the marginal probability information computed by the Inside-Outside algorithm. However, we only build up the correlation between the Inside-Outside algorithm and the attention models, and we still don't know the mechanism inside the language models: the model may be executing the Inside-Outside algorithm (or some approximations of the Inside-Outside algorithm), but it may also use some mechanism far from the Inside-Outside algorithm but happens to contain the marginal probability information. We leave for future work the design of experiments to interpret the content of the contextualized embeddings and thus ``reverse-engineer'' the learned model. In this section, we take a small step to understand more about the mechanism of language models: we need to \emph{open up the black box} and go further than probing, and this section serves as one step to do so.

\paragraph{General idea} The key ingredient that distinguishes current large language models and the fully-connected neural networks is the self-attention module. Thus besides probing for certain information, we can also look at the attention score matrix and discover some patterns. In particular, we are interested in how far an attention head looks at, which we called the "averaged attended distance".

\paragraph{Averaged attended distance} For a model and a particular attention head, given a sentence $s$ with length $L_s$, the head will generate an $L_s \times L_s$ matrix $\mA$ containing the pair-wise attention score, where each row of $\mA$ sums to 1. Then we compute the following quantity ``Averaged attended distance''
\[\text{AD}_s = \frac{1}{L_s}\sum_{1\le i,j \le L_s} |i-j|\cdot \mA_{i,j},\]
which can be intuitively interpreted as ``the average distance this attention head is looking at''. We then take the average of the quantity for all sentences. We compute ``Averaged attended distance'' for three models on the synthetic \dataset{PCFG} dataset and \dataset{PTB} dataset. The models all have 12 attention heads in each layer but have 12, 6, 3 layers respectively.

\paragraph{Experiment results} \Cref{fig:attn-dist} shows the results of the ``Averaged attented distance'' for each attention head in different models. \cref{fig:attn-dist-a12l12-pcfg,fig:attn-dist-a12l6-pcfg,fig:attn-dist-a12l3-pcfg} show the results on the synthetic \dataset{PCFG} dataset, and \cref{fig:attn-dist-a12l12-ptb,fig:attn-dist-a12l6-ptb,fig:attn-dist-a12l3-ptb} show the results on the \dataset{PTB} dataset. We sort the attention heads in each layer according to the ``Averaged attended distance''.

From \cref{fig:attn-dist-a12l12-pcfg,fig:attn-dist-a12l6-pcfg,fig:attn-dist-a12l3-pcfg}, we can find that for all models, there are several attention heads in the first layer that look at very close tokens (``Averaged attended distance'' less than $3$). Then as the layer increases, the ``Averaged attended distance'' also increases in general, meaning that the attention heads are looking at further tokens. Then at some layer, there are some attention heads looking at very far tokens (``Averaged attended distance'' larger than 12).\footnote{Note that the average length of the sentences in the synthetic \dataset{PCFG} dataset is around 24, if the attention head gives 0.5 attention score to the first and the last token for every token, the ``Averaged attended distance'' will be 12.} This finding also gives some implication that the model is doing something that correlates with our construction: it looks longer spans as the layer increases. However, different from our construction that the attention head only looks at a fixed length span, models trained using MLM look at different lengths of spans at each layer, which cannot be explained by our current construction, and suggests a further understanding of the mechanism of large language models.

Besides, we can find that the patterns are nearly the same for the synthetic \dataset{PCFG} dataset and \dataset{PTB} dataset, and thus the previous finding can also be transferred to the \dataset{PTB} dataset.


\begin{figure*}[!t]
\centering
\begin{subfigure}[b]{0.48\textwidth}
\centering
\includegraphics[width=\textwidth]{figs/pcfg-dist-patterns-a12l12.pdf}
\caption{12 attention heads and 12 layers, \dataset{PCFG} dataset.}
\label{fig:attn-dist-a12l12-pcfg}
\end{subfigure}
\hfill
\begin{subfigure}[b]{0.48\textwidth}
\centering
\includegraphics[width=\textwidth]{figs/ptb-dist-patterns-a12l12.pdf}
\caption{12 attention heads and 12 layers, \dataset{PTB} dataset.}
\label{fig:attn-dist-a12l12-ptb}
\end{subfigure}

\begin{subfigure}[b]{0.48\textwidth}
\centering
\includegraphics[width=\textwidth]{figs/pcfg-dist-patterns-a12l6.pdf}
\caption{12 attention heads and 6 layers, \dataset{PCFG} dataset.}
\label{fig:attn-dist-a12l6-pcfg}
\end{subfigure}
\hfill
\begin{subfigure}[b]{0.48\textwidth}
\centering
\includegraphics[width=\textwidth]{figs/ptb-dist-patterns-a12l6.pdf}
\caption{12 attention heads and 6 layers, \dataset{PTB} dataset.}
\label{fig:attn-dist-a12l6-ptb}
\end{subfigure}

\begin{subfigure}[b]{0.48\textwidth}
\centering
\includegraphics[width=\textwidth]{figs/pcfg-dist-patterns-a12l3.pdf}
\caption{12 attention heads and 3 layers, \dataset{PCFG} dataset.}
\label{fig:attn-dist-a12l3-pcfg}
\end{subfigure}
\hfill
\begin{subfigure}[b]{0.48\textwidth}
\centering
\includegraphics[width=\textwidth]{figs/ptb-dist-patterns-a12l3.pdf}
\caption{12 attention heads and 3 layers, \dataset{PTB} dataset.}
\label{fig:attn-dist-a12l3-ptb}
\end{subfigure}


\caption{``Averaged attented distance'' of each attention heads for different models on \dataset{PCFG} and \dataset{PTB} datasets. \cref{fig:attn-dist-a12l12-pcfg,fig:attn-dist-a12l6-pcfg,fig:attn-dist-a12l3-pcfg} show the results on the synthetic \dataset{PCFG} dataset, and \cref{fig:attn-dist-a12l12-ptb,fig:attn-dist-a12l6-ptb,fig:attn-dist-a12l3-ptb} show the results on the \dataset{PTB} dataset.}
\label{fig:attn-dist}
\end{figure*}