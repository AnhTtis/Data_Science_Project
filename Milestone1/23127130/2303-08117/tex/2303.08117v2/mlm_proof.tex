\begin{proof}[Proof of \Cref{thm:io-optimal-mlm}]
We first focus on 1-mask predictions, where given an input of tokens $w_1, w_2, \cdots, w_L$, and a randomly selected index $i$, we need to predict the token at position $i$ given the rest of the tokens, i.e. $\Pr\{w | w_{-i}\}$. Under the generative rules of the PCFG model, we have

{\small
\begin{align}
    &\Pr[w | w_{-i}] \nonumber \\
    = & \sum_{A}\Pr[A\to w] \cdot \Pr[A\text{ generates word at pos }i | w_{-i}] \nonumber\\
    = & \sum_{A}\Pr[A\to w]\cdot \frac{\beta(A,i,i)}{\sum_{B}\beta(B,i,i)}. \label{eq:1mask_pcfg}
    %\\&
    %= \alpha(A, i, i) \cdot \frac{\beta(A,i,i)}{\sum_{B \in \gN}\beta(B,i,i)}.
\end{align}
}
Note that $\Pr[A\to w]$ can be extracted from the PCFG and $\{\beta(B, i, i)\}_{B \in \gN}$ can be computed by the Inside-outside algorithm. Thus, Inside-outside can solve the 1-masking problem optimally.

Now we consider the case where we randomly mask $m\%$ (e.g., 15\%) of the tokens and predict these tokens given the rest. In this setting, if the original sentence is generated from PCFG $\gG = (\gN, \gI, \gP, n, p)$, one can modify the PCFG to get $\gG' = (\gN, \gI, \gP, n+1, p')$ with $n+1$ denote the mask token $text{[MASK]}$ and for each preterminal $A\in\gP$, $p'(A\to \text{[MASK]}) = m\%$ and $p'(A\to w) = (1-m\%)p(A\to w),$ for all $w\neq \text{[MASK]}$. Then, the distribution of the randomly masked sentences follows the distribution of sentences generated from the modified PCFG $\gG'$. Similar to the 1-masking setting, we can use the Inside-outside algorithm to compute the optimal token distribution at a masked position.

\end{proof}
