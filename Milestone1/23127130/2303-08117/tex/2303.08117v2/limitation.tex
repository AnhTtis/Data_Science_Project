\section*{Limitation}

We believe that the main limitations of our study are the transformer architecture and size. 

Due to limitations imposed by GPU resources, we assess encoder-only models with specific limitations: a maximum of 12 layers, 24 attention heads per layer, and 768 embedding dimensions.  Nevertheless, all the experiment results begin to stabilize for smaller models and generalize to the largest model we investigate. Hence, we believe that the results can generalize to even larger models. 

Our central theoretical discovery (Theorem 3.3) establishes a connection between the masked language modeling (MLM) loss and the Inside-outside algorithm.  Extending to auto-regressive models like GPT is an important theoretical question and is kept for future study.