\section{Parsing using Transformers}\label{sec:construction}
\looseness=-1We design transformers with moderate layers and heads for parsing and masked language modeling. In \Cref{sec:construct-io}, we prove that transformers can execute the Inside-Outside algorithm for bounded-length sentences with any PCFG. In \Cref{sec:mlmandio}, we connect our construction with masked language modeling and demonstrate the optimality of the Inside-Outside algorithm for MLM on PCFG-generated data. Finally, in \cref{sec:approx-overview}, we demonstrate the ability to reduce the size of these constructions while retaining their parsing performance.

\subsection{Transformers can execute Inside-Outside algorithm}\label{sec:construct-io}
%\haoyu{this part will also simplify a lot, e.g. only keep theorems (or even informal) and a brief overview of the construction}
%In this section, we give constructions that show attention models % state our theoretical results that show that attention models 
%are expressive enough to represent the Inside-Outside algorithm. 

\looseness=-1 We first give a construction (\cref{thm:hard_attnt}) that relies on {\em hard attention}, where only one of the attended positions will have positive attention score. For this construction,
%In the first theorem, 
we define $f_{\text{attn}}: \mathbb{R}^{L \times d} \times \mathbb{R}^{d}$ such that the attention scores in eq.~\ref{def:attention} are given by

{
\small
\begin{align}
    a^h_{i, j} = \text{ReLU} (  (\mK_h \ve_j^{(\ell)})^{\top} \mQ_h \ve_i^{(\ell)} ). \label{eq:hard_attention} 
\end{align}
}

This is similar to softmax attention used in practice, with softmax replaced by $\text{ReLU}$ activation. %If only a single score in $\{a^h_{i, j}\}_{j \in [L]}$ is positive, we get an attention module that maintains the attention score only at the highest attended position. This is the intended application for $f_{\text{attn}}$ and hence, we call it as hard attention.

%given a set of scores $\{s_1, s_2, \cdots, s_L\}$, the hard attention module will return $\max(s_1, s_2, \cdots, s_L)$ at index $\arg\max(s_1, s_2, \cdots, s_L)$ and $0$ elsewhere. For our construction, an attention head that takes scores $\{s_1, \cdots, s_L\}$ and returns $\{ReLU(s_1), \cdots, ReLU(s_L)\}$ suffices.
%We assume that our sentences are of length at most $L$.

\begin{theorem}[Hard attention]\label{thm:hard_attnt}
    There exists a model with hard attention modules (\ref{eq:hard_attention}), $(4|\gN| + 1)L$ embeddings, $2L-1$ layers, and $4|\gN|$ attention heads in each layer that simulates the Inside-Outside algorithm on all sentences with length at most $L$ generated by PCFG $\gG = (\gN, \gI, \gP, n, p)$ and embed all inside and outside probabilities.
\end{theorem}

%\haoyu{AP and I both feel like we may only keep the very high-level idea here, and the proof will go to the appendix.}
\looseness=-1 \begin{proof}[Proof sketch] We give the proof sketch and defer details to \cref{sec:hard_attnt_proof}. The core idea is to use the first $L$ layers to compute the inside probabilities with the recursive eq. \ref{eq:inside_probability}. Each layer $\ell \le L$ computes $\alpha(A,i,j)$ for all position pairs $(i, j)$ with $j-i = \ell$ and all non-terminals $A$. The next $L$ layers compute the outside probabilities with the recursive eq. \ref{eq:outside_probability}. Each layer $L+\ell > L$ computes $\beta(A,i,j)$ for all position pairs $(i, j)$ with $j-i=L-\ell$ and all non-terminals $A$. 

\looseness=-1 At any position $i$ in a layer $\ell \le L$, the input token embeds inside probabilities of all spans with a maximum length of $\ell$, starting and ending at $i$: $\alpha(A, i, j)$ and $\alpha(A, k, i)$ for all non-terminals $A$ and position tuples $(i, j, k)$ where $j-i<\ell$, $i-k<\ell$. To compute $\alpha(A, i, i+\ell)$ at each position $i$ for each non-terminal $A$, we use an attention head that calculates an inner product between the embeddings at positions $i$ and $i+\ell$, weighted by the matrix containing ${\Pr[A \to BC]}_{B, C \in \gN}$. The token at position $i$ attends only to the token at $i+\ell$ thanks to the position embeddings and hard attention. We use another attention head to compute $\alpha(A, i-\ell, i)$, and store the new inside probability terms along with the previous ones in the embeddings. We use a similar technique to compute the outside probabilities in the next $L$ layers. In layer $L+\ell$, we use two attention heads to compute $\beta(A, i, i+L-\ell)$ for each non-terminal $A$ and position $i$, as there are two terms to compute in \ref{eq:outside_probability}. We use two additional attention heads to compute $\beta(A, i-L+\ell, i)$, resulting in four attention heads for each non-terminal.
\end{proof}
%and we can compute all the outside probabilities using $L-1$ layers.
%Hard attention simplifies the construction, but doesn't fully utilize the power of attention models. 

\looseness=-1 To further reduce embedding size and attention heads, we introduce relative positions and use soft attention. We introduce $2L + 1$ relative position vectors $ \{ p_{ t } \in \mathbb{R}^d \}_{-L \le t \le L},$ and relative position biases $\{ b_{t, \ell} \in \mathbb{R} \}_{-L \le t \le L, 1 \le \ell \le 2L-1}$ that modify the key vectors depending on the relative position of the query and key tokens. For an attention head $h$ in layer $\ell$, the attention score $a_{i, j}^h$ is given by

{
\small
\begin{equation}
    a_{i, j}^h =  \text{ReLU}( \mK_h \ve_j^{(\ell)} + p_{j - i} - b_{j-i, \ell} )^\top \mQ_h \ve_i^{(\ell)}. \label{eq:soft_attention}
\end{equation}
}


\begin{theorem}[Relative positional embeddings] \label{thm:soft_attnt}
    There exists a model with attention module (\ref{eq:soft_attention}), $2|\gN| L + 1$ embeddings, $2L-1$ layers, and $|\gN|$ attention heads in each layer that simulate the Inside-Outside algorithm on all sentences with length at most $L$ generated by PCFG $\gG = (\gN, \gI, \gP, n, p)$ and embed all inside and outside probabilities.
\end{theorem}
\iffalse
\haoyu{Rong's construction is actually another approximated version of the Inside-Outside algorithm, but we do not have time to test the performance empirically in these days. I think we should not put this in the first arxiv version, but we will do the related experiments before submission to ARR and add the related results then.}
Note that the dynamic programming for the inside and outside probabilities are computed for different length of spans (eq.~\ref{eq:inside_probability},\ref{eq:outside_probability}), and thus the previous constructions has $2L$ layers in total, where $L$ is the length of the sentence. However most of the time, the depth of the parse tree $d$ is much smaller than the length of the sentence $L$, and if we can execute the Inside-Outside algorithm in terms of depth, we should be able to reduce the number of layers from $2L$ to $2d$. The following theorem shows that there exists a construction that can compute the ``modified version'' of inside and outside probabilities 
\begin{theorem}[Informal]
    Given a PCFG $\gG = (\gN, \gI, \gP, n, p)$, there exists an attention model with soft relative attention modules (\ref{eq:soft_attention}), with embeddings of size $2|\gN| L + L$, $2L$ layers, and $2|\gN|$ attention heads in each layer, that can simulate the Inside-Outside algorithm on all sentences of length at most $L$ generated from $\gG$, and embed all the inside and outside probabilities.
\end{theorem}
\fi

\looseness=-1 The proof is deferred to \cref{sec:soft_attnt_proof}. \Cref{thm:soft_attnt} uses one attention head to compute layer-wise inside/outside probabilities per non-terminal, and only requires $|\gN|$ heads in each layer. 
Once we have the inside and outside probabilities for spans, we can directly build the parse tree using the Labelled-Recall algorithm, which acts as a ``probe'' on the contextual representations of the model.


\subsection{Masked language modeling for PCFG}\label{sec:mlmandio}

\looseness=-1 The Inside-Outside algorithm not only can parse but also has a connection to masked language modeling (MLM), the pre-training loss used by BERT. The following theorem shows that, if the language is generated from a PCFG, then the Inside-Outside algorithm achieves the optimal MLM loss.

\begin{theorem}\label{thm:io-optimal-mlm}
    Assuming language is generated from a PCFG, the Inside-Outside algorithm reaches the optimal MLM loss.
\end{theorem}

\looseness=-1The Inside-Outside algorithm optimizes MLM loss on PCFG data, suggesting that pre-training on such data enables implicit learning of the algorithm  or its computed quantities. Consequently, intermediate layers can capture syntactic information for parsing, potentially explaining the presence of structural information in language models~\citep{hewitt2019structural,vilares2020parsing,arps2022probing}. We validate this conjecture in \Cref{sec:probe-marginal-probs}.



\subsection{Towards realistic size}\label{sec:approx-overview}
%\haoyu{currently AP and I both feel like we should put the details approximation into the appendix, and only give brief theorem, exp results and intuition here. In this version, we keep the minimal math here.}

\looseness=-1 For PCFG learned on the \dataset{PTB} training set (\dataset{PTB} sections 02-21) with an average sentence length of 25~\citep{Spectral-Parser}, \Cref{sec:construct-io} requires $~1600$ attention heads, $~3200L$ embedding dimensions, and $2L$ layers to simulate the Inside-Outside algorithm for sentences of length $L$, which is much larger than BERT.
%, assuming $L\approx 25$ is the average length of the sentences. 
%This is extremely large compared to BERT, which raises the question of whether our construction sheds any light on real-world architectures. 
However, by utilizing the inherent sparsity in the English PCFG, we can reduce the number of attention heads and the width of the embeddings while maintaining decent parsing performance. The details are deferred to \Cref{sec:approx-detailed}.

%We give intuitions on the approximation methods and show the size of the resulting language models, with the details deferred to \Cref{sec:approx-detailed}.

\paragraph{First ingredient: finding important non-terminals}
%In the constructions of \cref{thm:hard_attnt,thm:soft_attnt}, in all attention layers, we need a unique attention head for each non-terminal to represent the computation of an inside or outside probability corresponding to the non-terminal. However, if we only compute the probabilities of a specific set of non-terminals $\tilde\gI$ and $\tilde\gP$ in eq.~\ref{eq:inside_probability} and~\ref{eq:outside_probability} and set all others to 0, we can reduce the number of attention heads from $|\gN|$ to $\max\{|\tilde\gI|,|\tilde\gP|\}$, because the last layer of the model computes the outside probabilities for pre-terminals (need $|\tilde\gP|$ heads) and all middle layer computes the probabilities for in-terminals (need $|\tilde\gI|$ heads). If $|\tilde\gP| < c|\tilde\gI|$ for some constant $c$, we can also simulate the computations in the last layer with $|\tilde\gP|$ heads by $c$ layers with $|\tilde\gI|$ heads. Besides, we can also reduce the size of the embeddings since storing probabilities for the irrelevant non-terminals becomes unnecessary. Our hypothesis is that we can indeed focus only on a few non-terminals.
\looseness=-1 In the constructions of \cref{thm:hard_attnt,thm:soft_attnt}, the number of attention heads and embedding dimensions depend on the number of non-terminals of the PCFG. Thus if we can find a smaller PCFG, we can make the model much smaller. Specifically, if we only compute the probabilities of a specific set of in-terminals $\tilde\gI$ and pre-terminals $\tilde\gP$ in eq.~\ref{eq:inside_probability} and~\ref{eq:outside_probability}, we can reduce the number of attention heads from $|\gN|$ to $\max\{|\tilde\gI|,|\tilde\gP|\}$.\footnote{When $|\tilde\gP| < c|\tilde\gI|$, we can simulate the computations in the final layer using $c$ layers with $|\tilde\gI|$ heads instead of $|\tilde\gP|$ heads. Additionally, we can decrease the embedding size by only storing probabilities for relevant non-terminals.} 


\begin{table}[!t]
    \centering
    \footnotesize
    \begin{tabular}{|c|c|c|c|}
    \hline
         Approximation & Corpus F1 & Sent F1 & ppl. \\
         \hline
         \makecell{No approx.} & 75.90 & 78.77 & 50.80 \\
         \hline
         $|\tilde\gI| = 10,|\tilde\gP|=45$ & 57.14 & 60.32 & 59.57 \\
         $|\tilde\gI| = 20,|\tilde\gP|=45$ & 68.41 & 71.91 & 55.16 \\
         $|\tilde\gI| = 40,|\tilde\gP|=45$ & 72.45 & 75.43 & 54.09 \\
         \hline
    \end{tabular}
    \iffalse
    \begin{tabular}{|c|c|c|c|c|}
    \hline
         & Corpus F1 & Sent F1 & TV & ppl. \\
         \hline
         \makecell{No approx.} & 75.90 & 78.77 & 0 & 50.80 \\
         \hline
         $|\tilde\gN| = 10$ & 57.14 & 60.32 & 0.114 & 58.11 \\
         $|\tilde\gN| = 20$ & 68.41 & 71.91 & 0.073 & 55.16 \\
         $|\tilde\gN| = 40$ & 72.45 & 75.43 & 0.050 & 54.09 \\
         \hline
    \end{tabular}
    \fi
    \iffalse
    \begin{tabular}{|c|c|c|}
    \hline
         & Corpus F1 & Sent F1 \\
         \hline
         \makecell{No approx.} & 75.90 & 78.77 \\
         \hline
         $|\tilde\gN| = 10$ & 57.14 & 60.32  \\
         $|\tilde\gN| = 20$ & 68.41 & 71.91  \\
         $|\tilde\gN| = 40$ & 72.45 & 75.43  \\
         \hline
    \end{tabular}
    \fi
    \caption{Restricting computations of the Inside-Outside algorithm to the most frequent in(pre)-terminal subsets $\tilde\gI$ ($\tilde\gP$) in the \dataset{PTB} sections 02-21. We report the unlabelled F1 scores on \dataset{PTB} section 22 and the 1-masking perplexity on 200 sentences generated from the PCFG. %The PCFG is learned on \dataset{PTB} training dataset. 
    %The perplexity of the approximate algorithm was computed on 200 sentences generated from the English PCFG.
    $|\tilde \gI|=20, |\tilde \gP|=45$ resulted in a $8.58\%$ increase in perplexity and $8.71\%$ decrease in parsing F1 scores.
    %Besides the parsing F1 results, we also show the TV distance between the exact computation and the approximated computation for 1-masking prediction.
    }
    \label{tab:few-nt-pcfg-global}
\end{table}



%We hypothesize that computing a small set of $|\tilde\gN$, i.e. $|\tilde\gN| \ll |\gN|$, can approximate the Inside-Outside algorithm without losing much on the parsing performance.
%We haven't made the error bounds explicit in the above hypothesis. We will perform experiments on the English PCFG to verify our hypothesis. 
%We empirically verify our hypothesis through experiments, with details in \Cref{appendix:small_nonterminal_subset}. In summary, 
\looseness=-1 We sort the non-terminals in terms of their frequency of occurrence in the \dataset{PTB} training set and show that restricting the Inside-Outside computation to a few frequent non-terminals has a negligible drop in performance (\cref{tab:few-nt-pcfg-global}). The parsing score is still highly non-trivial, since the naive baseline, Right Branching (RB), can only get $<40\%$ sentence and corpus F1 scores on \dataset{PTB} dataset.
%\footnote{The naive RB works well under the \dataset{PTB} after removing the punctuations. To get such a score on \dataset{PTB} with the punctuations, one needs to modify the RB baseline~\citep{li2020empirical}. Directly applying RB on \dataset{PTB} with punctuations will lead to <1\% for both sentence and corpus F1 scores.}.



\paragraph{Second ingredient: utilizing structures across non-terminals}
\looseness=-1 We still use one attention head to represent the computation for a specific non-terminal, which does not utilize possible underlying correlations between different non-terminals.
%In \Cref{thm:hard_attnt,thm:soft_attnt}, we assign one attention head to represent the computation for a specific non-terminal, which does not utilize possible underlying correlations between different non-terminals. The second ingredient will be to use such structures.
%Next, we give the intuition of the second approximation method that utilizes the possible hidden structure between non-terminals.
Specifically, for \cref{thm:soft_attnt}, we use one attention head at layer $\ell < L$ to compute the inside probabilities $\alpha(A, i,j)$ with $j-i = \ell$. 
%If $\alpha(A,i,j)$ for different non-terminals $A\in\tilde\gI$ lie in a $k^{(\ell)}$-dimensional subspace with $k^{(\ell)} < |\tilde\gI|$, we can compute all of the inside probabilities $\alpha(A,i,j)$ using only $k^{(\ell)}$ attention heads instead of $|\tilde\gN|$ by computing the vector $\mW^{(\ell)}\bm{\alpha}(i,j)$, where $\mW^{(\ell)}\in\R^{k^{(\ell)}\times |\tilde\gI|}$ is the transformation matrix and $\bm{\alpha}(i,j)\in\R^{|\tilde\gI|}$ is the concatenation of all inside probabilties $\{\alpha(A,i,j)\}_{A\in\tilde\gI}$.
If $\alpha(A,i,j)$ for different non-terminals $A\in\tilde\gI$ lie in a $k^{(\ell)}$-dimensional subspace with $k^{(\ell)} < |\tilde\gI|$, we can compute all of the inside probabilities using only $k^{(\ell)}$ attention heads by computing the vector $\mW^{(\ell)}\bm{\alpha}(i,j)$, where $\mW^{(\ell)}\in\R^{k^{(\ell)}\times |\tilde\gI|}$ is the transformation matrix and $\bm{\alpha}(i,j)\in\R^{|\tilde\gI|}$ is the concatenation of all inside probabilties ${\alpha(A,i,j)}_{A\in\tilde\gI}$. The same procedure can also be applied to the computation of outside probabilities.
\footnote{The computation for $A\in\tilde\gP$ needs $|\tilde\gP|$ heads in the last layer and can be simulated by several layers with fewer heads.} 
Although the probabilities should not lie in a low dimensional subspace in reality, we can still try to learn a transformation matrix $\mW^{(\ell)}\in\R^{k^{(\ell)}\times |\tilde\gI|}$ and approximately compute the inside probabilities by $\bm\alpha(i,j) = (\mW^{(\ell)})^{\dagger}\mW^{(\ell)}\bm\alpha^*(i,j)$ for $j-i = \ell$, where $\bm\alpha^*(i,j)$ denotes the Inside probabilities for non-terminals in $\tilde\gI$. 
%The same procedure can also be applied to the computation of outside probabilities. 
Please refer to \Cref{sec:approx-low-rank} for more details.
%on how we perform the approximated computations. 
%We hypothesize that we can indeed find such transformation matrices $\{\mW^{(\ell)}\}_{\ell\le L}$ that can reduce the computations while retaining most of the performance.
%The following informal theorem show the effectiveness of using this approximation, and please refer to \Cref{sec:approx-low-rank} for more details.

%\begin{hypothesis}
%     For the PCFG $\gG = (\gN, \gI, \gP, n, p)$ learned on the English corpus, there exists transformation matrices $\mW^{(\ell)}\in\R^{k^{(\ell)}\times |\tilde\gI|}$ for every $\ell \le L$, such that approximately simulating the Inside-Outside algorithm with $\{\mW^{(\ell)}\}_{\ell\le L}$ introduces \underline{small} error in the 1-mask perplexity and has \underline{minimal} impact on the parsing performance of the Labeled-Recall algorithm.
%\end{hypothesis}
 \looseness=-1 \subparagraph{Learning the transformations} %$\mW^{(\ell)}$ captures the correlation of in-terminals $\tilde\gI$ for spans with length $\ell+1$. 
 For sentence $s$ and a span with length $\ell+1$, we compute the marginal probabilities of this span $\vmu_s^{i,j}\in\R^{|\tilde\gI|}$, that contains $\mu(A,i,j)$ for each non-terminal $A\in\tilde\gI$. 
 %Compute $\mX_s^{(\ell)} = \sum_{i,j:j-i=\ell} \vmu_s^{i,j}(\vmu_s^{i,j})^\top$ as a matrix to capture the correlation of in-terminals $\tilde\gI$ for spans with length $\ell+1$ given a sentence $s$.
%We sum over the sentences in the \dataset{PTB} training set and get 
We then compute the normalized correlation matrix
$\mX^{(\ell)} = \sum_{s} \mX_s^{(\ell)} / \|\mX_s^{(\ell)}\|_{\text{F}}$, where $\mX_s^{(\ell)} = \sum_{i,j:j-i=\ell} \vmu_s^{i,j}(\vmu_s^{i,j})^\top$, which captures the correlation of $\tilde\gI$ for spans with length $\ell+1$ in the entire corpus.
We apply the Eigen-decomposition on $\mX_\ell$ and set $\mW^{(\ell)}$ as the top $k^{(\ell)}$ Eigen-vectors. 
%Please refer to \Cref{sec:approx-low-rank} for more discussions on the transformation matrices.

\looseness=-1 The parsing results and 1-masking perplexity using $\{\mW^{(\ell)}\}_{\ell\le L}$ with different $k^{(\ell)}$ are shown in \Cref{tab:learned-transformation-global}. Utilizing the linear transformations, we obtain $71.33\%$ and $65.31\%$ sentence F1 on \dataset{PTB} with only 15 and 10 attention heads respectively, whereas only computing probabilities for top-$10$ in-terminals gives $60.32\%$ sentence F1 on \dataset{PTB}. The following theorem summarizes the results.
%It is also worth noting that empirically, directly learning the transformation matrix $\mW^{(\ell)}$ by the Eigen-decomposition of $\mX^{(\ell)}$ computed on all non-terminals (thus with size $~1600\times 1600$) performs worse than learning $\mW^{(\ell)}$ by the Eigen-decomposition of $\mX^{(\ell)}$ computed on top 40 non-terminals ($|\gN^{(\ell)}| = 40$).

\iffalse
\begin{table}[!t]
    \centering
    \begin{tabular}{|c|c|c|c|}
        \hline
        $k^{(\ell)}$ & Corpus F1 & Sent F1 & TV \\ 
        \hline
        \makecell{Baseline \\ $|\gN^{(\ell)}| = 20$} & 62.49 & 61.17 & 0.054 \\
        \hline
        \makecell{$k^{(\ell)} = 10$} & 57.90 & 61.71 & 0.107 \\
        %\hline
        \makecell{$k^{(\ell)} = 15$} & 64.90 & 68.36 & 0.082 \\
        %\hline
        \makecell{$k^{(\ell)} = 20$} & 68.03 & 69.17 & 0.046 \\
        \hline
    \end{tabular}
    \caption{Experiment results using learned transformations. For the Baseline, there is no ``sparse-coding'' style approximation, we only compute the probabilities for the top 20 non-terminals. We show the parsing F1 results and the TV distance for 1-masking prediction.}
    \label{tab:learned-transformation}
\end{table}
\fi

\begin{table}[!t]
    \centering
    \scriptsize
    \begin{tabular}{|c|c|c|c|}
        \hline
        Approximation & Corpus F1 & Sent F1 & ppl. \\ 
        \hline
        \makecell{$|\tilde\gI| = 10,|\tilde\gP|=45$} & 57.14 & 60.32 & 59.57 \\
        $|\tilde\gI| = 20,|\tilde\gP|=45$ & 68.41 & 71.91 & 55.16 \\
        \hline
        \makecell{$k^{(\ell)} = 10,|\tilde\gI| = 20,|\tilde\gP|=45$} & 61.72 & 65.31 & 57.05 \\
        %\hline
        \makecell{$k^{(\ell)} = 15,|\tilde\gI| = 20,|\tilde\gP|=45$} & 68.20 & 71.33 & 55.52 \\
        %\hline
        %\makecell{$k^{(\ell)} = 20$} & & & \\
        \hline
    \end{tabular}
    \iffalse
    \begin{tabular}{|c|c|c|c|c|}
        \hline
        $k^{(\ell)}$ & Corpus F1 & Sent F1 & TV & ppl. \\ 
        \hline
        \makecell{$|\tilde\gI| = 10,|\tilde\gP|=45$} & 57.14 & 60.32 & 0.114 & 58.11 \\
        $|\tilde\gI| = 20,|\tilde\gP|=45$ & 68.41 & 71.91 & 0.073 & 54.07 \\
        \hline
        \makecell{$k^{(\ell)} = 10,|\tilde\gI| = 20,|\tilde\gP|=45$} & 61.72 & 65.31 & 0.108 & 55.64 \\
        %\hline
        \makecell{$k^{(\ell)} = 15,|\tilde\gI| = 20,|\tilde\gP|=45$} & 68.20 & 71.33 & 0.084 & 54.36 \\
        %\hline
        %\makecell{$k^{(\ell)} = 20$} & & & \\
        \hline
    \end{tabular}
    \fi
    \iffalse
    \begin{tabular}{|c|c|c|}
        \hline
        $k^{(\ell)}$ & Corpus F1 & Sent F1 \\ 
        \hline
        \makecell{$|\tilde\gN| = 10$} & 57.14 & 60.32 \\
        $|\tilde\gN| = 20$ & 68.41 & 71.91 \\
        \hline
        \makecell{$k^{(\ell)} = 10$} & 61.72 & 65.31 \\
        %\hline
        \makecell{$k^{(\ell)} = 15$} & 68.20 & 71.33 \\
        %\hline
        %\makecell{$k^{(\ell)} = 20$} & & & \\
        \hline
    \end{tabular}
    \fi
    \caption{Approximate Inside-Outside algorithm using linear transformations $\{\mW^{(\ell)} \in \mathbb{R}^{k^{(\ell)} \times |\tilde\gI|} \}$ on the inside/outside probabilities of the selected subset $\tilde{\gI}$. We report the F1 scores on \dataset{PTB} section 22 and the 1-masking perplexity on 200 sentences generated from the PCFG. Applying linear transformations can further reduce the number of attention heads in the constructed model to $15$ starting from $20$ frequent non-terminals subset $\tilde\gI$, while only changing the performance by at most $1\%$. 
    % and the TV distance for 1-masking prediction.
    }
    \label{tab:learned-transformation-global}
\end{table}

%\paragraph{Combining both the ingredients:} Hence, combining the two ingredients, we have the following theorem.

\begin{theorem}[Informal]\label{thm:approx-low-rank-informal}
    There exists a model with attention module (\ref{eq:soft_attention}), $275+40L$ embeddings, $2L+1$ layers, and $15$ attention heads in each layer that can approximately execute  Inside-Outside algorithm on all sentences with length at most $L$ generated by English PCFG, introducing $8.6\%$ increase in average 1-mask perplexity and resulting in at most $9.45\%$ drop in the parsing performance of Labeled-Recall algorithm.
    %By utilizing the hidden structure between non-terminals, we can reduce the size of model in \Cref{thm:soft_attnt} to $15$ attention heads in each layer, $540+40L$ embedding dimension, and $2L$ layers to approximately execute the Inside-Outside algorithm on PCFG learned on English corpus. Parsing using this approximated Inside-Outside algorithm gives us $68.20\%$ corpus F1 $71.33\%$ sentence F1 on \dataset{PTB} dataset.
\end{theorem}





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\iffalse
\subsection{Overview of our construction}\label{sec:construct-overview}
\haoyu{this will be very simple, only mentioning that there exists an ``linear'' attention head to compute the probabilities. All the missing proofs will go to appendix. The construction overview may place after \Cref{thm:hard_attnt} or \Cref{thm:soft_attnt}. Feel like going after \Cref{thm:hard_attnt} makes most sense.}
Our construction adds attention layers to simulate the recursive definitions of the inside and outside probabilities in \cref{eq:inside_probability,eq:outside_probability}. 
%at any position, $i$, for a set of scores ${a_{i, 1}, a_{i, 2}, \cdots, a_{i, L}},$ the hard attention module returns $\max(a_{i, 1}, a_{i, 2}, \cdots, a_{i, L}).$ 


Embeddings of a word $w$ will contain $\Pr[A \rightarrow w]$ for all $A \in \gP$. Given an input sentence $[w_1, w_2, \cdots, w_L]$, we concatenate the embeddings of each word with a one-hot position encoding representing its position in the sentence.
The first $L$ layers compute the inside probabilities, while the next $L$ layers compute the outside probabilities for a given sentence. 

\paragraph{Inside probabilities:} After $\ell \le L$ attention layers, the contextual embedding at each position $i$ contains the inside probabilities of all spans of length at most $\ell$ starting and ending at position $i$, i.e.  $\alpha(A, i, i + k)$ and $\alpha(A, i-k, i)$ for all $A \in \gN$ and $k \le \ell$. In layer $\ell$, for a non-terminal $A \in \gN$, we use one attention head to compute $\alpha(A, i, i+\ell)$ using eq.~\ref{eq:inside_probability}. To do so, we set the Key matrix $\mK_{A, \ell}$ to $\mI$, while the Query matrix $\mQ_{A, \ell}$ is defined as a block-diagonal matrix, with $4L$ blocks of size $|\gN| \times |\gN|$ and a single block of size $L \times L$, such that for any position $j$, 


\begin{align*}
    &(\mK_{A, \ell} \ve_i^{(\ell-1)})^{\top} \mQ_{A, \ell} \ve_j^{(\ell-1)} \\=& (\mathbb{I}[j-i = \ell] - 1) + \sum_{B, C \in \gN} \sum_{\ell_1, \ell_2: \ell_1 + \ell_2 = \ell}  \Pr[A \to B C] \\& \cdot \alpha(B, i, i+\ell_1) \cdot  \alpha (C, j -\ell_2, j) .
\end{align*}



The component $\mathbb{I} [ j - i = \ell ]$ is used by the attention model to pick only the relevant position $j$ (with its attention score intact) such that $j - i = \ell$. This component comes from the interaction of the one-hot position embeddings at the two positions. If $j - i \ne \ell$, the above score is negative and will be zeroed out by the $ReLU$ activation. The value matrix $\mV_{A, \ell}$ is set such that the final computation is stored in the relevant position in the contextual embedding. Similarly, we use another attention head to compute $\alpha(A, i-\ell, i)$ for non-terminal $A$.
%Similarly, we use another attention head to compute 


%where the $(\ell_1, \ell_2)$ block-diagonal matrix contains the $|\gN| \times |\gN|$ matrix $\{ \Pr[A \to BC] \} _{B, C \in \gN}$, iff $\ell_1 + \ell_2 - |\gN| = \ell$ and $0$ otheriwse. In simple terms, $\mQ_{A, \ell}$ only focuses on the weighted inner product of the contextual sub-embeddings at positions $i$ and $j$ that contain $\alpha(*, i, i+\ell_1)$ and $\alpha(*, i, i+\ell_2)$, with $\ell_1 + \ell_2 = \ell$. In order to 

%Each attention head in layer $\ell$ will correspond to the computation of inside probabilities w.r.t. a particular non-terminal in 


\paragraph{Outside probabilities:} After $\ell \ge L$ attention layers, in addition to all inside probabilities, the embedding at each position $i$ will contain the outside probabilities of all spans of lengths at least $2L - \ell$ starting and ending at position $i$, i.e.  $\beta(A, i, i + k)$ and $\beta(A, i-k, i)$ for all $A \in \gN$ and $k \ge 2L - \ell$. At layer $\ell$, for any non-terminal $A \in \gN$, we require 2 attention heads to compute each of the 2 terms for $\beta(A, i, i + 2L - \ell)$ in eq.~\ref{eq:outside_probability}. Furthermore, we require 2 additional heads to compute $\beta(A, i - 2L +\ell, i)$. The construction of the attention heads is similar to the construction of the attention heads that compute the inside probabilities.  



\paragraph{Soft attention with relative positions:}
The constructed attention model has redundancies that can be removed with relative position embeddings. We outline the change for the computation of the inside probabilities. Similar modifications are applied for the computation of the outside probabilities.

At layer $\ell \le L$ and position $i$, for the attention head that represents the computation of $\alpha(A, i, i+\ell)$ for a non-terminal $A \in \gN$, rather than restricting its  attention to a single position $i + \ell$ as before, we define $\mQ_{A, \ell}$, $\mK_{A, \ell}$, and the position vectors $\{p_{t, 0}\}_{-L \le t \le  L}$ such that, 
\begin{align}
    &( \mK_{A, \ell} \ve_j^{(\ell-1)} \odot p_{j - i, 0 } )^\top  \mQ_{A, \ell} \ve_i^{(\ell-1)} = \mathbb{I}[ j \ge i ] \cdot \nonumber\\
    & \sum_{B, C} \Pr[ A \to BC ] \cdot  \alpha(B, i, j) \cdot \alpha(C, j, i + \ell) . \label{eq:intend_position}
\end{align}
$\mQ_{A, \ell}$ is set as a $2L \times 2L$ block-diagonal matrix, where each block is a $|\gN| \times |\gN|$ matrix  that contains $\{\Pr[A \to BC]\}_{B, C \in \gN}.$ $\mK_{A, \ell}$ is set to $\mI$. $p_{j - i, 0}$ is set in $\{0, 1\}^{d}$ such that if $j \ge i$, we attend only to the portion of $\ve_j^{(\ell-1)}$ containing $\{ \alpha(C, j, j + \ell') \} _{C \in \gN}$ with $\ell' = \ell - (j - i)$. $p_{j-i, 0}$ set to $0$ if $j < i$. Given that the coordinates of our embeddings lie in the range $[0, 1]$, $\mK_{A, \ell} \ve_j^{(\ell-1)} \odot p_{j - i, 0 }$ can be achieved by $ReLU(\mK_{A, \ell} \ve_j^{(\ell-1)} + p_{j - i, 0 } - 1).$

\fi












