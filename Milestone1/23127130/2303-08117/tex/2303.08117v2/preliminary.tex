
\section{Preliminaries}\label{sec:preliminary}

\subsection{Attention}
%\abhishek{Need to have some primer on attention here}
%\haoyu{will also compress this section later}

\looseness=-1 We focus on encoder-only transformers like BERT and RoBERTa~\citep{devlin2019bert,liu2019roberta}, which stack identical layers with an attention module followed by a feed-forward module. Each attention module has multiple heads, represented by three matrices $\mQ_{h}, \mK_{h}, \mV_{h} \in \mathbb{R}^{d \times d}$.
For an input sequence of length $L$, we use $\mE^{(\ell)} \in \R^{L \times d}$ to denote contextual embeddings after layer $\ell$'s computations, where $\ve_i^{(\ell)}$ is the embedding of the $i^{th}$ token. The output of the attention head $h$ at layer $\ell$ is $\vv^{(\ell)}_{i, h} = \sum_{j \in [L]} a^h_{i, j} \mV_h \ve^{(\ell)}$, where $a^h_{i, j}$ is the attention score between $\ve_i$ and $\ve_j$ for head $h$:

{\small
\vspace{-1mm}
\begin{align}
    a^h_{i, j} = f_{\text{attn}} ( \mE^{(\ell)} \mK_h^{\top}, \mQ_h \ve_i^{(\ell)} )_j \label{def:attention}.
\end{align}
}

\looseness=-1 $f_{attn}$ is a non-linear function and is generally used as softmax on $\mE^{(\ell)} \mK_h^{\top} \mQ_h \ve_i^{(\ell)}$. Finally, the output of the attention module is given by $\sum_h \vv^{(\ell)}_{i, h}.$ This is a general definition of the attention module and captures the split and merge of the embeddings across the attention heads used in practice.
%Note that the above definition of attention captures the attention module used in practice, where the contextual embeddings are split equally among the different attention heads, and the outputs of each head are simply merged after individual head computations. For ease of presentation, we will use this general definition henceforth.
%However, for simplicity of constructions 





%We denote $\ve_i^{(\ell)}\in\R^d$ the embedding for position $i$ at layer $\ell$ in the attention model and $\mE^{(\ell)} = [\ve_1^{(\ell)},\dots,\ve_L^{(\ell)}]\in\R^{d\times L}$ the matrix form. 

%Then a single attention head in layer $\ell$ consists of three matrices $\mK\in\R^{d\times k}$: the key matrix, $\mQ\in\R^{d\times k}$ the query matrix, and $\mV^{d\times d}$ the value matrix. Then for a fixed position $i$, we can compute the attention scores of each position $\bm \alpha = h((\mE^{(\ell)})^\top \mK \mQ^\top \ve_i^{(\ell)})$, where $h$ is some activation function, e.g., the softmax. Then, the embedding after this attention head is computed as $\ve_i = \sum_j \alpha_j \mV \ve_i^{\ell}$ In this paper, to make our construction easier, we consider two forms of activation function $h$, the first is similar to the ``hard attention'' where we only keep the largest entry and set others to $0$, formally $h_i(\vx) = \vx_i$ if $i = \arg\max_i \vx_i$ and $0$ otherwise. The second function $h$ is more related to the ``soft attention'' where $h_i(\vx) = \text{ReLU}(\vx_i)$ (we change the softmax to ReLU).

\subsection{PCFG and parsing} \label{sec:pcfg_def}
\looseness=-1 \paragraph{PCFG model} A probabilistic context-free grammar (PCFG) is a language generative model. It is defined as a 5-tuple $\gG = (\gN, \gI, \gP, n, p)$, where
\begin{itemize}
[leftmargin=*]
\setlength\itemsep{0.05em}
    \item $\gN$ is the set of non-terminal. $\gI,\gP\subset\gN$ are sets of \emph{in-terminals} and \emph{pre-terminals} respectively. $\gN = \gI\cup\gP$, and $\gI\cap\gP = \phi$.
    \item $[n]$ is the set of all possible words.
    \item $\forall A\in\gI, B,C\in\gN$, there is a rule $A\to BC$.
    \item For rule $A\to BC$ where $A\in\gI, B, C\in\gN$, there is a probability $\Pr[A\to BC]$ satisfying for all $A$, $\sum_{B,C}\Pr[A\to BC] = 1$.
    \item For all $A\in\gP, w\in [n]$, a rule $A\to w$.
    \item For each rule $A\to w$ where $A\in\gP, w\in [n]$, a probability $\Pr[A\to w]$, which satisfies for all $A$, $\sum_{w}\Pr[A\to w] = 1$.
    \item A non-terminal $\text{Root}\in\gI$. %A non-terminal $\text{Root}$ and transitions $\text{Root} \to A$ for all non-terminals $A$.
\end{itemize}

\looseness=-1 \paragraph{Data generation from PCFG} Strings are generated from the PCFG $\gG = (\gN, \gI, \gP, n, p)$ as follows: we maintain a string $s_t \in ([n]\cup\gN)^*$ at step $t$ with $s_1 = \text{ROOT}$. At step $t$, if all characters in $s_t$ belong to $[n]$, the generation process ends, and $s_t$ is the resulting string. Otherwise, we pick a character $A\in s_t$ such that $A\in\gN$. If $A\in\gP$, we replace the character $A$ to $w$ with probability $\Pr[A\to w]$. If $A\in\gI$, we replace the character $A$ to two characters $B, C$ with probability $\Pr[A\to BC]$.
% we reach a string $s \in [n]^*$, using the derivation $s_1, s_2, \cdots,  s_t$, where $s_t=s$. For all $1 < i < t$, $s_i \in (\gN \cup [n])^{*}$ is derived from $s_{i-1}$ by choosing a non-terminal $A \in s_{i-1}$ and replacing it with a rule $\beta \in \{A \to BC\}_{B, C \in \gN}.$ 


\looseness=-1 \paragraph{Parse trees and parsing} For a sentence $s = w_1\dots w_L$ with length $L$, a labeled parse tree represents the likely derivations of a sentence under PCFG $\gG$. It is defined as a list of spans with non-terminals $\{(A, i, j)\}$ that forms a tree. An unlabelled parse tree is a list of spans that forms a tree. %We also call the tree binary if the resulting tree only has at most two children for each node.

\looseness=-1To find the unlabelled parse tree for a sentence $s$ under the PCFG model, the Labelled-Recall algorithm~\citep{goodman1996parsing} is commonly used. This algorithm searches for the tree $T = \{(i,j)\}$ that maximizes $\sum_{(i,j)\in T} \mathrm{score}(i,j)$, where $\mathrm{score}(i,j)=\max_{A\in \gN} \Pr[A\Rightarrow w_iw_{i+1} \cdots w_j, \text{Root}\Rightarrow s|\gG]:=\max_{A\in \gN}\mu(A, i, j)$ is the marginal probability of span $w_iw_{i+1} \cdots w_j$ under non-terminal $A$.

%is called the marginal probabilities.
\looseness=-1Marginal probabilities are computed by Inside-Outside algorithm~\citep{baker1979trainable}, with the inside probabilities $\alpha(A, i,j)$
%$ = \Pr[A\to w_iw_{i+1} \dots w_j | \gG]$ 
and the outside probabilities $\beta(A, i, j)$
%$=\Pr[\text{Root}\rightarrow w_1w_2 \dots w_{i-1} A w_{j+1} \dots w_L | \gG]$
computed by the following recursion

\iffalse
{\scriptsize
\begin{align}
    \alpha(A,i,j) 
    & = \sum_{B,C}\sum_{k=i}^{j-1} \Pr[A\to BC]\alpha(B,i,k)\alpha(C,k+1,j), \label{eq:inside_probability} \\
     \beta(A,i,j) 
    &= \sum_{B, C} \sum_{k=1}^{i-1}\Pr[B \to C A] \alpha(C, k, i-1) \beta(B, k, j) \label{eq:outside_probability} \\  & \quad + \sum_{B, C} \sum_{k=j+1}^{L}\Pr[B \to A C] \alpha(C, j+1, k) \beta(B, i, k) \nonumber
\end{align}
}
\fi

{\small
\begin{align}
    & \alpha(A,i,j) \nonumber \\
    & = \sum_{B,C}\sum_{k=i}^{j-1} \Pr[A\to BC]\alpha(B,i,k)\alpha(C,k+1,j), \label{eq:inside_probability} \\
    & \beta(A,i,j) \nonumber \\ 
    &= \sum_{B, C} \sum_{k=1}^{i-1}\Pr[B \to C A] \alpha(C, k, i-1) \beta(B, k, j) \label{eq:outside_probability} \\  & \quad + \sum_{B, C} \sum_{k=j+1}^{L}\Pr[B \to A C] \alpha(C, j+1, k) \beta(B, i, k) \nonumber
\end{align}
}


\looseness=-1 with the base cases $\alpha(A,i,i) = \Pr[A\to w_i]$ for all $A,i$ and $\beta(\text{Root},1,L)=1$ for all $A$. The marginal probabilities are then computed as
\iffalse
\begin{align}
    &\mu(A,i,j) \nonumber \\
    & = \Pr[A\Rightarrow w_iw_{i+1}\dots w_j, \text{Root}\Rightarrow s|\gG] \label{eq:marginal_probability} \\
    & = \Pr[\text{Root}\rightarrow w_1w_2\dots w_{i-1} A w_{j+1}\dots w_L|\gG] \nonumber \\
    &\quad \cdot \Pr[A\to w_iw_{i+1}\dots w_j|\gG] \nonumber \\
    & = \alpha(A,i,j)\cdot\beta(A,i,j). \nonumber
\end{align}
\fi
{\small
\begin{equation}\label{eq:marginal_probability}
   \mu(A,i,j) = \alpha(A,i,j)\times\beta(A,i,j). 
\end{equation}
}

\looseness=-1 Parsing performance is evaluated by two types of unlabelled F1 scores, which depend on the average method: Sentence F1 (average of F1 scores for each sentence) and Corpus F1 (considers total true positives, false positives, and false negatives).

\subsection{Probing}
%\haoyu{TBD. need to introduce the probing technique.}

\looseness=-1 A probe $f(\cdot)$ is a supervised model that predicts a target $\text{tar}(\vx)$ for a given input $\vx$ \cite{alain2017understanding,hupkes2018visualisation,conneau-etal-2018-cram}. As an example, \citet{hewitt2019structural} used a probe $f(\cdot)$ to predict the tree distance $\text{tar}(i,j) = d_{\gT}(i,j)$ between words in a dependency parse tree $\gT$. 
%The input of $f(\cdot)$ is $\ve_i^{(\ell)} - \ve_j^{(\ell)}$, the difference between the embeddings at layer $\ell$ of the two words, and the probe is given by $f(\ve_i^{(\ell)} - \ve_j^{(\ell)}) = \norm{\mB\left(\ve_i^{(\ell)} - \ve_j^{(\ell)}\right)}^2$, with $\mB$ as the trainable parameter. %Besides the tree distance, one can change the probe to predict other information (i.e. the target), change the input, or even change the function class of the probe $f$.
Although mathematically equivalent, probes and supervised models have different goals. The latter aims for high prediction scores, while the former seeks to identify certain intrinsic information in embeddings~\citep{maudslay2020tale,chen2021probing}. Probes should be limited to only detect the desired information, with low performance on uncontextualized embeddings and high performance on contextualized ones.% in the parsing setting.



\iffalse
Given an input sentence, the first step is to feed the sentence into the BERT model, and get the contextualized embeddings for each word; the second step is to apply a fixed (over different positions in the sentence and different sentences) linear transformation on the contextualized embeddings and map the contextualized embeddings to a low dimensional subspace, say 40 dimensions; the third step is to compute the minimum spanning tree of the points in the low dimensional subspace, where the distance between points is the Euclidean distance; finally, regarding the minimum spanning tree as the undirected dependency parse tree. This procedure is simple, but it works surprisingly well. The UUAS score of linear probing on BERT is over 80\%, which is a very decent score. 
\fi