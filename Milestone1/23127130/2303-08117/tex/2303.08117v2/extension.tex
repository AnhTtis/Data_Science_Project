% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Remove the "review" option to generate the final version.
\usepackage[review]{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

\newcommand{\theHalgorithm}{\arabic{algorithm}}
% Recommended, but optional, packages for figures and better typesetting:
\usepackage{graphicx}
\usepackage{algorithm,algorithmic}
\usepackage{booktabs} % for professional tables
 
\usepackage{hyperref}

\usepackage{natbib}
 

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\allowdisplaybreaks

\usepackage{mathrsfs}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{hypothesis}[theorem]{Hypothesis}

\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}

%\usepackage[utf8]{inputenc} % allow utf-8 input
%\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{cleveref}       % \Cref
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
%\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
%\usepackage{fullpage}

\usepackage{microtype}
\usepackage{graphicx}
\usepackage{makecell}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{multirow}
\usepackage{rotating}

\usepackage{colortbl}
\definecolor{bgcolor}{rgb}{0.66,0.88,1.00}

\usepackage{tablefootnote}
\usepackage{threeparttable}


\usepackage{xspace}

\xspaceaddexceptions{[]\{\}}


\newcommand{\dataset}[1]{{\tt #1}\xspace}

\usepackage{thm-restate}

\newcommand{\squeeze}{\textstyle}

\usepackage{eqparbox}
\renewcommand{\algorithmiccomment}[1]{\hfill\eqparbox{COMMENT}{// #1}}

\usepackage{etoolbox}
\usepackage{enumitem}
\patchcmd{\quote}{\rightmargin}{\leftmargin 1em \rightmargin}{}{}
%============
\newcommand{\algname}[1]{{\sf\small#1}\xspace}
 
\makeatletter
\allowdisplaybreaks


\newcommand{\dotp}[2]{\left\langle {#1}, {#2}, \right\rangle}
\newcommand{\norm}[1]{\left\|{#1}\right\|}
\input{math_commands}

\newcommand{\haoyu}[1]{{\color{orange}[HZ: #1]}}
\newcommand{\abhishek}[1]{{\color{blue}[AP: #1]}}
\newcommand{\rong}[1]{{\color{green}[RG: #1]}}


\title{Extensions: Do Transformers Parse while Predicting the Masked Word?}
\author{
}
\date{}

\begin{document}

\maketitle

\section{Limitation}

We believe that the main limitations of our study are the transformer architecture and size. 

Due to limitations imposed by GPU resources, we assess encoder-only models with specific limitations: maximum of 12 layers, 24 attention heads per layer, and 768 embedding dimensions.  Nevertheless, all the experiment results begin to stabilize for smaller models and generalize to the largest model we investigate. Hence, we believe that the results can  generalize to even larger models. 

Our central theoretical discovery (Theorem 3.3) establishes a connection between the masked language modeling (MLM) loss and the Inside-outside algorithm.  Extending to auto-regressive models like GPT is an important theoretical question and is kept for future study.

\end{document}