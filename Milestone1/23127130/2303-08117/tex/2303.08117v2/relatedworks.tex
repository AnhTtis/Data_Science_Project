\section{Related Works}

%\looseness=-1\paragraph{Probing for transformers}
\paragraph{(Structural) probing}
\looseness=-1Several recent works on probing have aimed to study the encoded information in BERT-like models~\citep{rogers2020primer}. \citet{hewitt2019structural,reif2019visualizing,manning2020emergent,vilares2020parsing,maudslay2020tale,maudslay2021syntactic,chen2021probing,arps2022probing,jawahar2019does} have demonstrated that it is possible to predict various syntactic information present in the input sequence, including parse trees or POS tags, from internal states of BERT. 
%However, in contrast to these existing approaches that typically utilize a pre-trained model as-is, we adopt a close environment approach to understand the relationship between the data distribution, the masked language model objective, and the architecture to its ability to do syntactic parsing. We show for one particular pre-training data distribution, the pre-trained model's representation captures quantities correlated with the quantities of an optimal algorithm. We hope that our work can motivate future work in this direction.
In contrast to existing approaches that commonly employ a model pre-trained on natural language, we pre-train our model under PCFG-generated data to investigate the interplay between the data, the MLM objective, and the architecture's capacity for parsing. 
Besides syntax, probing has also been used to test other linguistic structures like semantics, sentiment, etc.~\citep{belinkov2017neural,reif2019visualizing,kim2020pre,richardson2020probing,vulic2020probing,conia-navigli-2022-probing}.
%e.g. the syntactic (structural) information~\citep{hewitt2019structural,reif2019visualizing,manning2020emergent,vilares2020parsing,maudslay2020tale,maudslay2021syntactic,chen2021probing,arps2022probing,jawahar2019does}. 


%However as mentioned in \citet{maudslay2021syntactic}, the probing success of the previous works on syntax emerged from the model using semantic cues to parse. 
%The probed pre-trained models had been pre-trained on natural language datasets, where the semantic structures are most likely correlated with the syntactic ones. Indeed, \citet{arps2022probing} tried to separate the semantics from syntax by training the probe on a mixture of natural language and manipulated data, however, the authors acknowledged the possibility of semantics still affecting the decision of the probe trained on manipulated data. 
%\paragraph{Other probings} Besides syntax, probing has been used for other linguistic structures like semantics, sentiment, etc.~\citep{belinkov2017neural,reif2019visualizing,kim2020pre,richardson2020probing,vulic2020probing,conia-navigli-2022-probing}.

\paragraph{Expressive power of transformers}
\looseness=-1\citet{Yun2020Are,yun2020n} show that transformers are universal sequence-to-sequence function approximators. Later, \citet{perez2021attention,bhattamishra2020computational} show that attention models can simulate Turing machines, with \citet{wei2022statistically} proposing statistically meaningful approximations of Turing machines. 
%Attention models with bounded size have been shown capable of recognizing deterministic context-free languages such as bounded-depth Dyck-k~\citep{yao2021self}. 
%The size of the constructed models, however, depends on the complexity of the target function and often requires arbitrary precision to encode the target function. 
%, that also exhibit good statistical learnability. 
%\citet{liu2022transformers} constructed (by hand) transformers that can efficiently simulate automata. 
To understand the behavior of moderate-size transformer architectures, many works have investigated specific classes of languages, e.g. bounded-depth Dyck languages~\citep{yao2021self}, modular prefix sums~\citep{anil2022exploring}, adders~\citep{nanda2023progress}, regular languages~\citep{bhattamishra2020ability}, and sparse logical predicates~\citep{edelman2022inductive}. \citet{merrill2022saturated} relate saturated transformers with constant depth threshold circuits, and \citet{liu2022transformers} provide a unified theory on understanding automata within transformers.
%Compared to the existing literature, our expressiveness power results are more related to the real natural language, since the PCFG we study is learned from natural language and the transformer we construct is of moderate size. \haoyu{is previous sentence OK? Or the following sentence?} 
These works study expressive power under a class of synthetic language. Compared to the prior works, our results are more related to the natural language, as we consider not only a class of synthetic language (PCFG), but also a specific PCFG tailored to the natural language.

% for inputs of a small range of lengths. 
 %and empirically showed the existence of such solutions in pre-trained transformers. 
 %However, their short-cut solution is not robust to OOD generalization, while our probes are all OOD generalizable since we pre-train on synthetic PCFG data while probing on the \dataset{PTB} dataset.
 %However, as evident from the name, shortcut solutions aren't robust to OOD generalization.
 %Interestingly, we observe that language models pre-trained on PCFG-generated data encode relevant information from the Inside-Outside algorithm for sentences from both natural language and PCFG-generated data, which suggests an implicit bias toward learning the general algorithm (generalizing to all input lengths). 
 %This may suggest that pre-training on purely synthetic data and data closer to natural language have different regimes. Besides, these results do not shed light on the expressivity of transformers needed to encode relevant syntactic and semantic information in languages. 
 %A careful study is left for future work.
 

 


%\paragraph{Grammar induction}