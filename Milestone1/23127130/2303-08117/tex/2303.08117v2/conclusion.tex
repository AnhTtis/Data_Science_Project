\section{Conclusion}

\looseness=-1 In this work, we show that MLM with moderate size has the capacity to parse decently well. We probe BERT-like models pre-trained (with MLM loss) on the synthetic text generated using PCFGs to verify that these models capture syntactic information.
%that helps reconstruct (partially) a parse tree.
%with pre-training under MLM loss. 
Furthermore, we show that the models contain the marginal span probabilities computed by the Inside-Outside algorithm, thus connecting MLM and parsing. We hope our findings may yield new insights into large language models and MLM. 

%One limitation of our paper is that we show the existence of Inside-Outside probabilities inside the embeddings, but we don't have definitive experiments to show the model actually simulates the Inside-Outside algorithm. We leave for future work the design of experiments to interpret the content of the contextualized embeddings and thus ``reverse-engineer'' the learned model. 
%Other interesting directions include the convergence analysis of attention models on different generative models, the importance of model scale during pre-training under different generative models, and the differences between parsing with shallow and deep models. 



%One future direction is to understand how language models can process the parsing information using very few layers.