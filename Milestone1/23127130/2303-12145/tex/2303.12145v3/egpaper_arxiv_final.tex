\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{iccv}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{authblk}
\newcommand*\samethanks[1][\value{footnote}]{\footnotemark[#1]}
\usepackage{pifont}% http://ctan.org/pkg/pifont
\usepackage{multirow}
\usepackage{xcolor}
\newcommand{\xmark}{\ding{55}}%
% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
% \usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}

% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[breaklinks=true,bookmarks=false]{hyperref}

\iccvfinalcopy % *** Uncomment this line for the final submission

\def\iccvPaperID{****} % *** Enter the ICCV Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
\ificcvfinal\pagestyle{empty}\fi

\begin{document}

%%%%%%%%% TITLE
\title{Efficient Feature Distillation for Zero-shot Detection}

% \author{Zhuoming Liu\thanks{Equal contribution.}  Xuefeng Hu\thanks{Equal contribution.} \\
% Institution1\\
% Institution1 address\\
% {\tt\small firstauthor@i1.org}
% }
\vspace{-0.8cm}
\author[]{Zhuoming Liu\thanks{Equal contribution.}}
\author[]{Xuefeng Hu\samethanks[1]}
\author[]{Ram Nevatia\thanks{Corresponding author}}
\affil[]{University Southern California}
\affil[]{\tt\small \{liuzhuom, xuefengh, nevatia\}@usc.edu}
\vspace{-0.8cm}


% \author{Foo\thanks{University of Podunk, Timbuktoo}
% \and Bar\samethanks
% \and Baz\thanks{Somewhere Else}
% \and Bof\samethanks[1]
% \and Hmm\samethanks}


% \author{Author1\thanks{equal contribution} \and Author2\printfnsymbol{1} \and Author3}
% \institute{\email{email1, email2, email3} \\ Institute}
\maketitle

% Remove page # from the first page of camera-ready.
\ificcvfinal\thispagestyle{empty}\fi

%%%%%%%%% ABSTRACT
\begin{abstract}
The large-scale vision-language models (e.g., CLIP) are leveraged by different methods to detect unseen objects. 
%Many recent methods have leveraged large-scale vision-language models (e.g., CLIP) for unseen object detection.
However, most of these works require additional captions or images for training, which is not feasible in the context of zero-shot detection. 
In contrast, the distillation-based method is an extra-data-free method, but it has its limitations.
%Specifically, distillation regions used by existing work  bias to the base categories, limiting the novel category information and harming the distillation efficiency.
%Among all, the distillation-based method is the most promising one as it requires the least amount of additional data to perform zero-shot detection. However, the distillation-based method also has its limitation.
Specifically, existing work creates distillation regions that are biased to the base categories, which limits the distillation of novel category information and harms the distillation efficiency.
Furthermore, directly using the raw feature from CLIP for distillation neglects the domain gap between the training data of CLIP and the detection datasets, which makes it difficult to learn the mapping from the image region to the vision-language feature space - an essential component for detecting unseen objects. 
As a result, existing distillation-based methods require an excessively long training schedule.
To solve these problems, we propose Efficient feature distillation for Zero-Shot Detection (EZSD). 
% To bridge the domain gap, EZSD adapts the CLIP's feature space to the detection dataset's domain by renormalizing the CLIP. 
% EZSD generates the CLIP Proposals, which are the distillation regions that CLIP believes novel instances exist in, obtaining more unseen category information.
% To further improve the model performance, EZSD takes advantage of semantic meaning for regression.
Firstly, EZSD adapts the CLIP's feature space to the target detection domain by re-normalizing CLIP to bridge the domain gap;
Secondly, EZSD uses CLIP to generate distillation proposals with potential novel instances, to avoid the distillation being overly biased to the base categories.
Finally, EZSD takes advantage of semantic meaning for regression to further improve the model performance. 
As a result, EZSD achieves state-of-the-art performance in the COCO zero-shot benchmark with a much shorter training schedule and outperforms previous work by 4\% in LVIS overall setting with 1/10 training time.
\end{abstract}

\begin{figure}[!t]
   \centering
   \includegraphics[width=1\linewidth]{clip_adaptaton_and_mapping_v5.pdf} 
   %\vskip -0.1in
   \caption{Overview of our method. 1. We adapt the CLIP feature space to the detection dataset's domain.
   2. We use  CLIP to select the meaningful distillation regions named "CLIP Proposals".
   3. We map the image features of our model into the adapted CLIP feature space by optimizing the L1 loss between the image feature of the CLIP Proposals from CLIP and the one from our model. 
   4. We detect  novel objects by using the novel category text embeddings which have a high cosine similarity score with the image feature of the novel instance.} 
   \label{clip_adaptaton_and_mapping} 
   \vskip -0.1in
 \end{figure}

%%%%%%%%% BODY TEXT
\section{Introduction}\label{sec:intro}
%1. why zero-shot detection is important

Object detection is a fundamental task in computer vision.  
Typically, an object detector is trained on a dataset with specific categories and extending to new categories requires annotated instances of the new category and retraining of the detector. Collecting enough new data and annotating it correctly, is expensive and time-consuming. This attracts researchers to train a model with less supervision, which leads to few-shot detection and zero-shot detection.
In this paper, we focus on real-life zero-shot detection, in which the model is trained on limited base categories, and the trained model can detect novel objects given only their category names.

To recognize novel instances when only their name is available, it is natural to consider a vision-language feature space.
In recent years, some large-scale vision-language classification models pretrained on millions of image-text pairs, for instance, CLIP\cite{radford2021learning}, ALIGN\cite{jia2021scaling} have become available.
Different solutions are proposed to leverage these models to enable a detector that can detect novel objects.
Some researchers\cite{zareian2021open, gao2021towards, zhou2022detecting, zhong2022regionclip} use these models to generate pseudo-label and train their own model with pseudo-label and the image-caption pair, while other trains a prompt\cite{feng2022promptdet} to turn the CLIP into a detector. However, all these methods need additional caption and image data, 
which may not available in the real-life zero-shot detection setting.
In contrast, ViLD\cite{gu2021open} learns a mapping from image regions to CLIP feature space by distillation for detecting the novel object, which is applicable when no additional data is provided.

To know how good the features are for distillation, we first apply CLIP to classify the instances in the COCO\cite{lin2014microsoft} dataset. 
We found that the classification accuracy (ACC) is only 46\% which is much lower than the ACC of the classifier in Faster R-CNN\cite{ren2015faster} (about 90\%). This indicates the domain gap between the training data of CLIP and the detection dataset, making the mapping from the image region to the vision-language feature space harder to learn. 
In addition, since the distillation is conducted on some specific image regions, how to select such a region is an important question.
ViLD uses the proposals from an RPN trained with base category annotations as distillation regions.
These proposals bias toward the region with base categories instances, which limits the novel information obtained by the detector and harms the distillation efficiency. Therefore, ViLD suffers from an extremely long training schedule. 

To address these problems, we propose \textbf{E}fficient Feature Distillation for \textbf{Z}ero-\textbf{S}hot \textbf{D}etection (\textbf{EZSD}).
For bridging the domain gap, we find that simply finetuning the layer normalization layers in the CLIP with the base category instances significantly improves the ACC on both base and novel (Fig~\ref{clip_adaptaton_and_mapping}, 1).
For the distillation regions, we expect these regions could contain novel objects so that some novel category information can be introduced into the detector.
To make the best use of the only information we have in the zero-shot detection setting, the name of novel categories, we decide to use CLIP to select these regions with the help of the novel category names. 
The selected regions are named as CLIP Proposals, in which CLIP believes there is a novel category instance(Fig~\ref{clip_adaptaton_and_mapping}, 2).

After adapting the feature space and generating the CLIP Proposals, EZSD learns a mapping from the image regions to the vision-language feature space by distillation, which is achieved by minimizing the L1 loss between the features of the CLIP Proposals from the CLIP, and the one from our model (Fig~\ref{clip_adaptaton_and_mapping}, 3).
Once the model is trained, in all potential regions given by the RPN, EZSD recognizes the novel objects by using the novel category name's text embedding, which has a high cosine similarity score with the image feature of the novel objects (Fig~\ref{clip_adaptaton_and_mapping}, 4).
To further improve the model performance, we introduce a semantic-based regressor, which takes the text embedding as additional information for regression.

By only providing the name of the novel categories to EZSD, EZSD can outperform previous open-vocabulary work by 4\% in novel categories and achieves state-of-the-art performance with a much shorter training schedule in COCO zero-shot benchmark. On the LVIS dataset, our method achieves a 4\% improvement over ViLD in overall performance with 1/10 training time. This indicates that the adapted feature and CLIP Proposals benefit both the distillation quality and the training efficiency. The contributions of this paper are summarized as follows:
 \begin{enumerate}
    \item
    We find a simple but effective way to bridge the domain gap between the training data of CLIP and the detection datasets, which improves the classification accuracy and makes the feature more discriminating, further benefiting the detection task.
    \item
    We design a method to select the meaningful distillation regions, named CLIP Proposals, which introduce more information about the unseen object in the distillation, improving the model performance on the novel and enhancing the training efficiency.
    To further improve the model performance, we also introduce the semantic-based regressor.

    \item
    %ISAL achieves state-of-the-art performance in both the commonly-used active learning setting for image classification and object detection and a newly-designed large-scale setting for object detection. 
    EZSD achieves state-of-the-art performance in novel categories with a much shorter training schedule in COCO zero-shot benchmark and outperforms ViLD in LVIS overall setting by 4\% with 1/10 training time.
 \end{enumerate}

%-------------------------------------------------------------------------
\begin{figure*}[!t]
   \centering
   \includegraphics[width=1\linewidth]{figure_for_clip_proposal.pdf} 
   %\vskip -0.1in
   \caption{The pipeline of CLIP Proposals generation. The CLIP Proposals and its features are pregenerated before the detector training.} 
   \label{clip_proposal} 
   \vskip -0.1in
 \end{figure*}

%-------------------------------------------------------------------------
\section{Related Work}

\textbf{The Large-scale Vision-language Pretraining.}
The large-scale pretraining already exist in Vision (MOCO\cite{he2020momentum}, MOCOv2\cite{chen2020improved}, etc.) and Language (BERT\cite{devlin2018bert}, GPT\cite{brown2020language}, etc.) for a long time.
Recently, there is a trend to use free-form supervision (raw text) to train the vision model which has evolved to large-scale vision-language pretrained models. 
For instance, the CLIP\cite{radford2021learning} and ALIGN\cite{jia2021scaling} are trained with a large-scale dataset with hundreds of millions of image-text pairs using contrastive learning.
GLIP\cite{li2022grounded} is pretrained in an object-level with a large-scale grounding dataset.
These multi-modal models' feature space and their knowledge are useful and can be applied to many other tasks, such as zero-shot classification and zero-shot detection.
However, the training data of these models are usually noisy and there is a domain gap between these data and the datasets of downstream tasks. 


\textbf{Domain Adaptation.}
Domain adaptation is necessary when we apply a pretrained model to other datasets.
In computer vision, the most common method for bridging the domain gap is to finetune the whole network on the new dataset or add one extra MLP layer at the end.
In the natural language processing community, Prompt tuning\cite{li2021prefix, lester2021power} surfaced as an important tool for domain adaptation in recent years.
Besides, the simple renormalization method is found to be effective by Perez\cite{perez2018film} and Lu\cite{lu2021pretrained}.
For adapting the large-scale multi-modal model, Kim et al.\cite{kim2022how} discusses the effectiveness of different ways in adapting the CLIP\cite{radford2021learning} to new classification datasets.

\textbf{Zero-shot Detection and Open Vocabulary Detection.}
Zero-shot detection aims to learn knowledge from the base categories and to generalize the knowledge to the novel categories, enabling the model to detect novel objects.
Bansal et al.\cite{bansal2018zero} propose to use the max-margin loss to match the visual features of cropped image regions with word embeddings.
Zhu et al.\cite{zhu2020don} synthesize the visual features with a generative model to improve localization performance.
Rahman et al.\cite{rahman2020improved} models background categories and cluster categories with similar semantics by using polarity loss. 
Zhong et al.\cite{zhong2022regionclip}, Zhou et al.\cite{zhou2022detecting} and Zareian et al\cite{zareian2021open} pretrain their model with image-caption pair to learn the vision-language feature space for open vocabulary detection. 
Gao et al.\cite{gao2021towards} train its model with pseudo-label which is generated by the ALBEF\cite{li2021align} and map the image region feature to CLIP's\cite{radford2021learning} feature space.
Feng et al.\cite{feng2022promptdet} design a prompt to make use of CLIP for detection.
Ma et al.\cite{Ma_2022_CVPR} design a open vocabulary one-stage detector trained with distillation.
Gu et al.\cite{gu2021open} propose ViLD which distills visual features from CLIP.

% Few-shot detection\cite{chen2018lstd, li2021few, li2021transformation, zhang2021hallucination, sun2021fsce, zhu2021semantic, xiao2020few, wu2020multi, qiao2021defrcn} is similar to the zero-shot detection, but for each novel categories, it has a few support images.
% The problem becomes how to make the best use of these few-shot samples and boost the model performance on the novel as much as possible.
% Wang et al\cite{wang2020frustratingly} propose TFA, a simple but effective method to finetune the model with few-shot samples.
% \cite{zhang2021meta, fan2020few, yan2019meta, han2022meta} use meta-learning to train the detector and let the model quickly adapt to the novel categories.
% Meta Faster R-CNN\cite{han2022meta} considers the relationship between different image regions, helping the model achieve state-of-the-art performance.


%-------------------------------------------------------------------------
\begin{figure*}[!t]
   \centering
   \includegraphics[width=1\linewidth]{figure_for_distillation_v3.pdf} 
   \vskip -0.1in
   \caption{Overview of our model. The figure presents the model structures in the RoI head of the two-stage detector. We use the per CLIP Proposal distillation weight and semantic-based regressor to further improve the model performance.} 
   \label{method_overview} 
   \vskip -0.1in
 \end{figure*}


\section{Method}
Our method aims to handle the zero-shot detection problem.
In zero-shot detection, the model is trained on a set of base classes $C_{b}$ and tested on novel classes $C_{n}$. 
The name of the novel categories is known before model training. 

\textbf{Method Overview.}
Inspired by the previous paper ViLD~\cite{gu2021open}, we map the image feature to the CLIP's multi-modal feature space. 
The \textbf{\textit{mapping}} is learned by distilling the knowledge from CLIP in some selected \textbf{\textit{distillation regions}} of each training image. 
The knowledge distillation is conducted by optimizing the L1 loss between the \textbf{\textit{feature from CLIP}} and the feature from our model in the selected regions.
After learning the mapping, given the proposal from the RPN, our model can recognize and detect novel objects by using the text embedding of the novel categories.
%Fig \ref{clip_adaptaton_and_mapping} shows the method's overview.

In Section~\ref{finetune_feature}, we describe how to adapt the \textbf{\textit{feature from CLIP}} to the domain of detection datasets.
In Section~\ref{generate_clip_proposal}, we demonstrate how to select the \textbf{\textit{distillation regions}} on each image.
In Section~\ref{model_training}, we discuss our model structure and how to train our model and learn the \textbf{\textit{mapping}} with the adapted feature from the selected regions.



\subsection{Adapt Vision-language Model Feature} \label{finetune_feature}
To understand how good the image feature from CLIP is, we first apply the CLIP to classify the instances in COCO~\cite{lin2014microsoft} dataset.
We first extract the feature for each instance from the vision encoder of the CLIP. 
The feature of the instance $i$ can be expressed as: $ins_{i} = V(Crop(I, $ $GT_{i(1.2x)}))$, where $V$ is the vision encoder of the CLIP and $Crop(I, GT_{i(1.2x)})$ means cropping the region from Image $I$ base on 1.2x enlarged GT bboxes $GT_{i(1.2x)}$.
We use the 1.2x enlarged bboxes since the enlarged bboxes help CLIP yield the best classification accuracy.
We present more details in the supplementary material.
We generate the text embedding for each COCO category from the text encoder of the CLIP.
We calculate the cosine similarity between the image feature and text embeddings, and select the category with the highest cosine score as the predicted category.

We notice that when directly applying the CLIP to classify the COCO instances, the classification accuracy (ACC) is only about 50\% which is much lower than the ACC of the classifier in a well-trained detector, indicating that there is a huge distribution gap between the training data of the CLIP and detection datasets.
To bridge the gap, inspired by\cite{kim2022how}, we simply fine-tune the CLIP's layer of normalization layers by minimizing the cross-entropy loss, using all base categories instances in the detection dataset.
This simple method boosts the ACC on COCO to about 80\%.
Also, using the adapted CLIP feature for distillation helps improving detection results.

\subsection{Generate CLIP Proposals} \label{generate_clip_proposal}
To obtain useful information for the novel categories, we need to select some meaningful image regions for distillation.
we expect these regions to contain some novel category objects, 
and introduce information of the novel categories.
ViLD trains an RPN with base category annotations and uses the proposals from this RPN as the distillation regions.
However, since the RPN is learned to predict base categories, which makes the proposals bias toward the areas which contain the base categories and ignore the areas which potentially have the novel category instance.
Instead of using the RPN to determine where to distill, we decide to use CLIP~\cite{radford2021learning}.
Trained with 400 million image and text pairs collected from the internet, CLIP is learned to discriminate a large number of categories. 
Therefore, if the region contains a novel object, the CLIP should yield high confidence in it.
We name these regions CLIP Proposals.
Fig~\ref{clip_proposal} demonstrates how to generate CLIP Proposal.


To select image regions as the CLIP Proposals, we first generate anchors over the image $I$.
Then we crop the image base on the anchors and extract the feature for these anchors from CLIP's vision encoder.
We generate the text embeddings $T_{i}$ for a given dictionary from the CLIP's text encoder.
In zero-shot detection setting, in addition to the information of the novel categories, we also need the knowledge of the base categories. Thus, we use all category names as the dictionary.
We classify each anchor by calculating the cosine similarity score between its image feature and all text embeddings.
We use the score after the softmax of the predicted category as the objectness score $o_{i}$ of the anchor.
We finally select the anchors with high objectness scores after the Non-Maximum Suppression (NMS) as CLIP Proposals. 

All the CLIP Proposals $C_{i}$ and their feature from CLIP $c_{i}$ is generated offline.
The $c_{i}$ can be expressed like this: $V(Crop(I, C_{i}))$, where $V$ and $Crop$  are the same as those in $ins_{i}$ definition.
We also add 1.2x enlarged base categories' GT bbox as part of the CLIP Proposals.
Our experiments show that even though these CLIP Proposals are noisy, they are still meaningful regions for distillation and help our detector performs better on novel categories.


 \begin{table*}[!t]
    \centering
    \begin{tabular}{c|c|c|c|c|ccc}
      \hline % horizontal line

       \hline
       \multirow{2}{*}{Method} & \multirow{2}{*}{Pretrain} & \multirow{2}{*}{Mask} & \multirow{2}{*}{Epoch} & \multirow{2}{*}{Novel Category Knowledge Source} & \multicolumn{3}{c}{AP50} \\ \cline{6-8} 
            & & & & & Base & Novel & Overall \\ 
       \hline
       OVR-CNN    &\checkmark &\xmark     & -    & COCO Captions              &46.0 & 22.8 & 39.9 \\
       PBBL       &\xmark     &\checkmark & -    & ALBEF\cite{li2021align}, CLIP, Pseudo-label  &46.1 & 30.8 & 42.1 \\
       OVOS       &\xmark     &\xmark     & 36   & CLIP                       &51.3 & 20.3 & 43.2 \\ 
       Detic      &\checkmark &\checkmark & -    & Conceptual Captions\cite{sharma-etal-2018-conceptual}, CLIP  &47.1 & 27.8 & 45.0 \\
       RegionCLIP &\checkmark &\xmark     & -    & COCO Captions\cite{lin2014microsoft}, CLIP        &54.8 & 26.8 & 47.5 \\
       PromptDet  &\xmark     &\checkmark & -    & CLIP, External Images      &- & 22.6 & 50.6 \\ 
       ViLD       &\xmark     &\checkmark & 107  & CLIP                       &59.5 & 27.6 & 51.3 \\ 
       \hline 
       %Ours       &\xmark     &\checkmark & 12   & CLIP, Novel Category Name &55.7 &  30.4 & 49.0 \\ 
       Ours       &\xmark     &\checkmark & 36   & CLIP, Novel Category Name & \textbf{59.9} & \textbf{31.6} &  \textbf{52.1} \\ 
       %\hline 
       %coreset&45.52&67.66&79.93&85.36&88.61\\
       %random&45.52&67.55&77.77&83.09&86.50\\
      \hline     
    \end{tabular}
    \centering
    \vspace{+1mm}    
    \caption{Evaluation results on COCO zero-shot benchmark. All the models are trained with the ResNet50 backbone. Mask indicates whether the model is trained with Mask annotations. Our model achieves state-of-the-art performance with 1/3 training time of ViLD.}
    \label{table:zeroshot}   
 \end{table*}



\subsection{Model Structure} \label{model_training}
Compared with the traditional two-stage detector, our model structure has three main differences: CLIP Proposals' feature distillation, cosine-similarity-based classifier, and semantic-based regressor. 
The model structure overview is shown in Fig \ref{method_overview}.

\textbf{Proposals' Feature Distillation.}
To obtain knowledge from CLIP and map the image feature from our model into the CLIP's feature space, we distill the knowledge from CLIP by minimizing the L1 loss between the CLIP Proposals' feature from the CLIP's vision encoder $c_{i}$ and the one from our model $c^{'}_{i}$. 
The $c^{'}_{i}$ can be expressed as: $Conv_{c}(Align(Bbone(I), C_{i}))$, where $Bbone(I)$ means getting the feature map by passing the image $I$ through the backbone $Bbone$, and $Align$ means doing the RoIAlign base on the CLIP Proposal $C_{i}$ on feature map. 
$Conv_{c}$ means passing the feature after the RoIAlign through the convolution and linear layers in the classification branch.

% mention the clip proposal filtering in the implementation details.
In the CLIP Proposal generation, we know the objectness score of each proposal.
For the proposal with a higher objectness score, it has a higher probability of having an object in it.
Therefore, we should assign a higher weight to these proposals. 
We directly use the objectness score as the weight, the distillation loss is formulated like this:
\begin{equation}
   \label{eq:gradsimi}
   \begin{aligned}
   L_{dist} = \frac1M\sum_{i=1}^Mo_i\vert c_i- c^{'}_i\vert_1
   \end{aligned}
\end{equation}
where $o_i$ is the objectness score of the CLIP Proposal $C_i$ and $M$ is the total number of the CLIP Proposal on one image.

\textbf{Cosine-similarity-based Classifier.}
By distilling the knowledge from CLIP, we are able to map the image feature of our model to CLIP feature space.
Instead of using a learnable linear layer as the classifier, we use text embedding generated from CLIP's text encoder. In the training phase, we only need the name of the base categories, which are then converted into text embedding $B_{i}$.
For each proposal $P_{i}$ given by the Region Proposal Network(RPN), we generate its feature $p_{i}$. The $p_{i}$ can be expressed as: $p_{i} = Conv_{c}(Align(Bbone(I), P_{i}))$, where $Conv_{c}$, $Align$, and $Bbone$ are the same as those in $c^{'}_{i}$ definition. The classification loss is given by:
\begin{equation}
   \label{eq:gradsimi}
   \begin{aligned}
   L_{cls} = \frac1N\sum_{i=1}^N L_{CE}(softmax(\boldsymbol{cos_{i}}) , y_{i})
   \end{aligned}
\end{equation}
where N is total number of proposals, $y_{i}$ is the assigned label for the proposal $P_{i}$.
The vector $\boldsymbol{cos_{i}}$ for the proposal $P_{i}$ is defined as $[cos(p_{i}, B_1),\dots, cos(p_{i}, B_n), cos(p_{i}, BG)]$ in which $n$ is number of base categories, $cos$ is the cosine similarity score, and $BG$ is a learnable vector for background.

At inference time, we also need to detect the novel categories. 
We generate the text embedding for both the base $B_{i}$ and the novel $N_{i}$.
The vector $\boldsymbol{cos_{i}}$ become $[cos(p_{i}, B_1), \dots, cos(p_{i}, B_n),cos(p_{i}, N_1), \dots, cos(p_{i}, N_k),$ $cos(p_{i}, BG)]$ where k is the number of the novel categories.

\textbf{Semantic-based Regressor.}
To improve the performance of the regression module, we add the semantic information of each category into consideration.
For each foreground proposal $P_{i}$ given by the Region Proposal Network(RPN), we generate its feature for regression $r_{i}$. The $r_{i}$ can be expressed as $Conv_{r}(Align(Bbone(I), P_{i}))$, where $Align$ and $Bbone$ are the same as those in $c^{'}_{i}$ definition, and $Conv_{r}$ means passing the feature after the RoIAlign through the convolution layers and linear layers in the regression branch. The regression loss is defined as:
\begin{equation}
   \label{eq:gradsimi}
   \begin{aligned}
   L_{reg} = \frac1K\sum_{i=1}^K L_{1}(Linear(Cat(r_{i}, B_{y_{i}})) , a_{i})
   \end{aligned}
\end{equation}
where $K$ is the total number of the foreground proposals, $Linear$ is the linear layer for bbox prediction, $Cat$ mean concatenation, $B_{y_{i}}$ means the text embedding of the assigned GT label $y_{i}$ for the proposal $i$, and $a_{i}$ is the GT bbox for the proposal $P_{i}$.
At inference time, since we no longer have the GT label. We concatenate the $r_{i}$ with the text embedding $B_{pred_{i}}$ or $N_{pred_{i}}$ of the predicted category of the proposal $P_{i}$, where $pred_{i} = \arg\max(\boldsymbol{cos_{i}})$.

Finally, our overall loss function is given by:
\begin{equation}
   \label{eq:gradsimi}
   \begin{aligned}
   L = L_{dist} + L_{cls} + L_{reg}
   \end{aligned}
\end{equation}

 \begin{table*}[!t]
    \centering
    \begin{tabular}{c|c|c|c|cccc}
      \hline % horizontal line

       \hline
       \multirow{2}{*}{Method} & \multirow{2}{*}{Pretrain} & \multirow{2}{*}{Epoch} & \multirow{2}{*}{Splits} & \multicolumn{4}{c}{AP} \\ \cline{5-8} 
            & & & & Freq & Comm & Rare & All \\ 
       \hline
       Detic      &\checkmark& -    & F+C (base) / R (novel, OVD)     & 31.6 & 26.3 & 17.8 & 26.8 \\
       RegionCLIP &\checkmark& -    & F+C (base) / R (novel, OVD)     & 34.0 & 27.4 & 17.1 & 28.2 \\
       PromptDet  &\xmark    & -    & F+C (base) / R (novel, OVD)     & 29.3 & 23.3 & 21.4 & 25.3 \\ 
       ViLD       &\xmark    & 468  & F+C (base) / R (novel, OVD)     & 28.3 & 20.0 & 16.1 & 22.5 \\ 
       ViLD*      &\xmark    & 48   & F+C (base) / R (novel, OVD)     & 26.4 & 17.5 & 11.8 & 20.0\\ 
       Ours       &\xmark    & 48   & F+C (base) / R (novel, ZSD)     & 31.7 & 25.6 & 15.8 & 26.3\\ 
       \hline 
       %ViLD*      &\xmark    & 24   &  freq(base) / comm(novel, OVD) / rare(novel, OVD) &24.9 & 12.2 & 11.2 & 17.5 \\
       %Ours       &\xmark    & 24   &  freq(base) / comm(novel, ZSD) / rare(novel, OVD) &\textbf{30.9}& \textbf{14.3} & \textbf{12.5} & \textbf{20.5}\\ 
       ViLD*      &\xmark    & 48   &  F (base) / C (novel, OVD) / R (novel, OVD) &26.4 & 13.2 & 11.3 & 18.5\\ 
       Ours       &\xmark    & 48   &  F (base) / C (novel, ZSD) / R (novel, OVD) &\textbf{31.9} & \textbf{15.2} & \textbf{13.1} & \textbf{21.3}\\ 
       
      \hline     
    \end{tabular}
    \centering
    \vspace{+1mm}    
    \caption{Evaluation results on LVIS benchmark. The ViLD* is our reproduced result of ViLD with a shorter training schedule. In [F+C/R] split, EZSD performs 4\% better than ViLD in All with only 1/10 training time. In [F/C/R] split, EZSD outperforms ViLD* in all 4 settings.}
    \label{table:lvis_zeroshot}   
 \end{table*}

\begin{table}[!t]
    \centering
    \begin{tabular}{c|c|cccc}
      \hline % horizontal line

       \hline
       \multirow{2}{*}{Method} & \multirow{2}{*}{Epoch} & \multicolumn{3}{c}{AP50} \\ \cline{3-5} 
            & & Base & Novel & Overall \\ 
       \hline
       ViLD*      & 12   &48.3 & 17.2 & 40.2  \\
       Ours       & 12   &\textbf{55.7} & \textbf{30.4} & \textbf{49.0} \\ 
       ViLD*      & 36   &56.0 & 24.2 & 48.5\\ 
       Ours       & 36   &\textbf{59.9} & \textbf{31.6} & \textbf{52.1}\\ 
       
      \hline     
    \end{tabular}
    \centering
    \vspace{+1mm}    
    \caption{Evaluation results on COCO zero-shot benchmark. EZSD outperforms ViLD in both 1x and 3x settings, showing our distillation is more efficient.}
    \label{table:coco_efficiency}   
 \end{table}

 
\begin{table}[!t]
    \centering
    \begin{tabular}{c|c|cccc}
      \hline % horizontal line

       \hline
       \multirow{2}{*}{Method} & \multirow{2}{*}{Epoch} & \multicolumn{4}{c}{AP} \\ \cline{3-6} 
            & & Freq & Comm & Rare & All \\ 
       \hline
       ViLD*      & 24   &24.9 & 12.2 & 11.2 & 17.5 \\
       Ours       & 24   &\textbf{30.9} & \textbf{14.3} & \textbf{12.5} & \textbf{20.5}\\ 
       ViLD*      & 48   &26.4          & 13.2 & 11.3 & 18.5\\ 
       Ours       & 48   &\textbf{31.9} & \textbf{15.2} & \textbf{13.1} & \textbf{21.3}\\ 
       
      \hline     
    \end{tabular}
    \centering
    \vspace{+1mm}    
    \caption{Evaluation results on LVIS [F/C/R] split. EZSD outperforms ViLD in both 2x and 4x settings due to the efficient feature distillation.}
    \label{table:lvis_efficiency}   
 \end{table}

 \begin{table*}[!t]
    \centering
    \begin{tabular}{c|cccc|cccc|cccc}
      \hline % horizontal line 
      \multirow{2}{*}{Method} & \multicolumn{4}{c|}{Base} & \multicolumn{4}{c|}{Novel}  & \multicolumn{4}{c}{General} \\ \cline{2-13} 
                            & L             & M             & S    & Avg  & L    & M    & S    & Avg  & L    & M    & S    & Avg  \\ \hline 
       w/o Adaptation       & 69.9          & 70.2          & 46.6 & 66.8 & 90.7 & \textbf{82.3} & 48.5 & 77.7 & 61.3 & 62.2 & 36.9 & 53.9 \\
       w/ Adaptation        & \textbf{92.5} & \textbf{89.5} & \textbf{80.3} & \textbf{87.5} & \textbf{91.1} & 81.6 & \textbf{66.3} & \textbf{81.9} & \textbf{84.8} & \textbf{80.7} & \textbf{68.8} & \textbf{78.3} \\ 
      \hline     
    \end{tabular}
    \centering
    \vspace{+1mm}    
    \caption{Adapting CLIP to the detection dataset's domain. The table presents the classification accuracy (ACC) of CLIP(w/ or w/o adaptation) when it is applied to classify the COCO dataset's instance. The ACC is aggregated based on the size of the instances. After the adaptation, the ACC is improved by a huge margin in three different settings, especially for small objects.}
    \label{table:adapt_clip}   
 \end{table*}

 
\section{Experiments} \label{experiment}
We first present our model result on COCO\cite{lin2014microsoft} and LVIS\cite{gupta2019lvis} zero-shot detection benchmark in section~\ref{zeroshot}.
In section~\ref{effiency} we compare our method with ViLD to show the efficiency of our method.
%In section~\ref{fewshot}, we evaluate our model performance on both PASCAL VOC~\cite{pascal-voc-2007} and COCO few-shot detection benchmark to show that our model has comparable performance as the few-shot detector even though we do not make use of additional image and annotation information.
Finally, we conduct the ablation study with visualization analysis.

\textbf{Implementation Details.}
We use the publicly available pretrained CLIP model ViT-B/32 as the open-vocabulary classification model, with an input size of 224$\times$224.

We finetune the layer normalization layers in the CLIP with base categories instances in COCO or LVIS base on the setting and maintain all other parameters fixed.
All the instances are cropped by 1.2x enlarged GT bboxes.
We use AdamW optimizer with learning rate of 0.0001 and clip the L2 norm of the gradients when larger than 0.1. 
We finetune the model for 12 epochs.

For CLIP Proposal generation, we first resize the image with the image ratio maintained. 
%For an image in PASCAL VOC, its long edge will be resized into 1000 as width or 600 as height.
%For an image in COCO, its long edge will be resized into 1333 as width or 800 as height.
We generate the anchors on each image with a stride of 32 pixels and with 5 different sizes(32, 64, 128, 256, 512), and 3 different ratios(1:1, 2:1, 1:2).
We select the top 1000 anchors after NMS as CLIP Proposals on each image.
If we will use the adapted CLIP features to train our detector we use the adapted CLIP to generate the CLIP Proposals.
Otherwise, we use the unadapted CLIP for CLIP Proposal generation.
In model training, we randomly select a fixed subset with 200 CLIP Proposals on each image for training.
We provide more implementation details in the supplementary material.

\subsection{Comparison with Zero-shot Detection Models} \label{zeroshot}
In this section, we evaluate EZSD in the zero-shot detection benchmark.

\textbf{Datasets and Evaluation Metrics.}
We evaluate EZSD on COCO and LVIS(v1). For the COCO dataset, we use train2017 for training and use val2017 for validation. 
The COCO benchmark is divided into 48 base categories and 17 novel categories.
%15 categories without a synset in the WordNet hierarchy are removed.
For the LVIS dataset, we use the training/validation images for training/evaluation.
We evaluate our model in two different splits of the LVIS dataset: 1.\textbf{[F+C/R]} uses \textbf{F}requent and \textbf{C}ommon categories as the base (866 categories), and \textbf{R}are categories as the novel (337 categories). We argue that in this split, the rare category objects are so sparse (less than 0.5\% annotations in the validation set) that the model's performance on it is not representative. Therefore, we propose a second split. 2.\textbf{[F/C/R]} uses the frequent categories as the base (405 categories), common and rare categories as the novel(common has 461 categories, rare has 405 categories). In the CLIP proposals generation, we only use frequent and common object names to generate proposals. Therefore, our model's performance in common categories is Zero-Shot Detection performance and the one in rare categories is Open Vocabulary Detection performance.
%Following the setting in \cite{zareian2021open}, we filter out the images that have neither the base category instances nor the novel category instances in the validation set. We evaluate the model in generalized setting, which evaluates the base and novel categories at the same time.
On COCO, AP50 is used as the evaluation metric, while on LVIS the AP is used.

\textbf{Model.}
We train a Mask R-CNN\cite{he2017mask} model with ResNet-50\cite{he2016deep} FPN\cite{lin2017feature} backbone. The backbone is pretrained on ImageNet\cite{deng2009imagenet}. We use SGD as the optimizer with batch size 4, learning rate 0.005, momentum 0.9, and weight decay 0.0001. We adopt linear warmup for the first 500 iterations, with a warm up ratio is 0.001. 
On COCO, We train our model with 36 epochs and divide the learning rate by 10 at epoch 27 and epoch 33.
On LVIS, We train our model with 48 epochs and divide the learning rate by 10 at epoch 32 and epoch 44.
We train our model with multi-scale train-time augmentation.

\textbf{Baselines.}
Since the performance of the existing zero-shot detection (ZSD) models are much worse than the performance of the open vocabulary detection (OVD) models,
we decide to compare EZSD with OVD models, OVR-CNN\cite{zareian2021open}, PBBL\cite{gao2021towards}, OVOS\cite{Ma_2022_CVPR}, Detic\cite{zhou2022detecting}, RegionCLIP\cite{zhong2022regionclip}, PromptDet\cite{feng2022promptdet}, ViLD\cite{gu2021open}.
The difference between the ZSD and OVD is that ZSD knows the name of the novel categories before training, and does not require any additional data.
%The OVR-CNN and RegionCLIP need to make use of the caption of the COCO dataset in their training, which introduces some additional costs in the zero-shot detection setting.
ViLD uses distillation to obtain information on the novel categories from CLIP. It has the best overall performance on COCO dataset. However, it uses the data augmentation of large-scale jittering\cite{ghiasi2021simple} with an extremely long training schedule.

\textbf{Results.}
Table~\ref{table:zeroshot} shows the results of EZSD in the COCO zero-shot detection benchmark. 
EZSD achieves 59.9\% and 31.6\% on base and novel, respectively, outperforming PromptDet by 9\% on novel. Its performance is 4\% better than ViLD and RegionCLIP in the novel and has a better performance in base and overall settings, with the use of 1/3 training time of ViLD. 

Table~\ref{table:lvis_zeroshot} shows the results of EZSD in the LVIS benchmark.
In [F+C/R] split, The Detic, RegionCLIP and PromptDet have a better performance than our model since they train their model with the additional captions and images, which may not available in real-life zero-shot detection setting. 
With 1/10 training time of ViLD, our method is 4\% higher in overall performance than ViLD and has comparable performance in the rare categories. 
In [F/C/R] split, our method shows 5\% improvement on Freq, and 2\% improvement in both Comm and Rare over the ViLD with the same training schedule. 
Since our method uses CLIP text embedding as the classifier, which is an open vocabulary classifier, EZSD can detect the objects in both common and rare categories, although it is only trained with frequent categories annotations.
EZSD only uses the name of the base and common categories in proposal generation. 
Therefore, EZSD's performance on common is the zero-shot detection performance, while its performance on rare categories is the open vocabulary detection performance.
We believe that the enhancement in performance for rare categories can be attributed to the elimination of the domain gap, whereas the improvement in common categories results from both the CLIP proposals and domain adaptation. Additionally, we note a significant boost in the model's performance on the base categories, which we believe is due to the elimination of the domain gap, making the mapping from the image to the CLIP feature space easier to learn.


 \begin{table}[!t]
    \centering
    \begin{tabular}{c|c|ccc}
      \hline % horizontal line

       \hline
       CLIP Feature & Distill Region & Base & Novel & Overall \\
       \hline
       Raw          & RPN Proposal  & 48.8 & 17.5 & 40.6 \\
       Adapted      & RPN Proposal  & 56.9 & 24.6 & 48.5 \\ 
       Raw          & CLIP Proposal & 48.7 & 19.3 & 41.7 \\ 
       Adapted      & CLIP Proposal & 55.7 &  \textbf{30.4} & 49.0 \\ 
      \hline     
    \end{tabular}
    \centering
    \vspace{+0.5mm}    
    \caption{Ablation study on CLIP's feature adaptation and CLIP Proposal using COCO zero-shot detection benchmark.}
    \label{table:ablation_feat_and_clip_proposal}   
 \end{table}


 \begin{table}[!t]
    \centering
    \begin{tabular}{c|c|ccc}
      \hline % horizontal line

       \hline
       SB Reg & PPDW & Base & Novel & Overall \\
       \hline
                   &            & 55.5 & 28.2 & 48.3 \\
        \checkmark &            & 55.8 & 29.8 & 48.5 \\ 
        \checkmark & \checkmark & 55.7 & \textbf{30.4} & 49.0 \\ 
      \hline     
    \end{tabular}
    \centering
    \vspace{+1mm}    
    \caption{Ablation study on Semantic-based regressor and per CLIP Proposal distillation weight using COCO zero-shot detection benchmark. SB Reg means the Semantic-based regressor, and PPDW means the per CLIP Proposal distillation weight.}
    \label{table:albation_rwe_and_ppdw}   
 \end{table}

 \begin{table}[!t]
    \centering
    \begin{tabular}{c|ccc}
      \hline % horizontal line

       \hline
       Proposal & IoGT & \#(IoGT$\ge$0.8) & \#(IoGT$\ge$0.5) \\
       \hline
         RPN & 0.340 & 362818 (9\%)  & 610157 (15\%) \\
         CLIP(Ours) & \textbf{0.365} & \textbf{563799 (14\%)} & \textbf{870830 (21\%)} \\ 
         
      \hline     
    \end{tabular}
    \centering
    \vspace{+1mm}    
    \caption{The effective distillation region of different proposals. The table presents the \textbf{I}ntersection between the proposal and the novel GT bboxes \textbf{o}ver novel \textbf{GT} bboxes (IoGT), the number of proposals that have high IoGT (IoGT$\ge$0.8, IoGT$\ge$0.5), and the percentage of these proposals in all proposals. Our CLIP proposals can cover more novel categories instances thus improving the distillation efficiency.}
    \label{table:iou_between_proposal_and_gtbbox}   
 \end{table}

\begin{figure*}[!t]
   \centering
   \includegraphics[width=1\linewidth]{detection_visualization_v2.pdf} 
   %\vskip -0.1in
   \caption{The visualization result on the COCO zero-shot setting. The first row presents the results of the model trained with the adapted CLIP features from CLIP Proposals. The second row presents the results of the model trained with raw CLIP features from RPN proposals.} 
   \label{detection_visualization} 
   \vskip -0.1in
 \end{figure*}
\subsection{Efficiency Evaluation} \label{effiency}
In this section, we compare EZSD with our reproduced ViLD to show the efficiency of our method. In Table~\ref{table:coco_efficiency}, we present our model and our reproduced ViLD on COCO with 1x and 3x training schedules. Our method is consistently better than ViLD in two different settings. Table \ref{table:lvis_efficiency} shows EZSD and our reproduced ViLD on LVIS dataset with 2x and 4x training schedules. Our method shows substantial improvement over the ViLD with the same training schedule. 
These results suggest that the adapted feature space and the CLIP Proposals improve the distillation quality and efficiency. Thus, the model performance is improved.


\subsection{Ablation Study and Visualization}
In this section, we conduct ablation studies using COCO zero-shot detection benchmark. All the experiment details are the same as mentioned in section~\ref{zeroshot}. We train our detector for 12 epochs in all experiments of this section.

%\textbf{Adapting CLIP to detection dataset's Domain}

\textbf{CLIP's Feature Adaptation and CLIP Proposal.} 
Table~\ref{table:adapt_clip} presents the classification result of adapting the CLIP to the COCO dataset's domain. 
We evaluate the classification accuracy(ACC) on the instances in the COCO validation set. 
We follow the zero-shot detection setting in Section~\ref{zeroshot}. 
For the base/novel setting, we only use the text embeddings of the base/novel categories as the classifier to classify the base/novel instances. 
For the general setting, we generate the text embeddings of both base and novel categories and classify the instance of all 65 categories at the same time.
Since in the novel setting the classifier just needs to classify the instance into 17 categories which is much easier than the other two settings, the ACC in the novel setting is much higher than the one in the other two.

Before the adaptation, the ACC in the general setting is only about 53.9\%, which is much lower than the ACC of the classifier in a well-trained detector. 
This phenomenon indicates that there is a huge domain gap between the training data of CLIP and the detection dataset.
While the objects in CLIP's training image are clear and large and at the center of the image, the objects in the detection dataset might be small and occluded.
Although we only fine-tuned the CLIP on base categories instances, we observe a huge improvement in all three settings, especially for the small objects.
The average ACC reaches 87.5\%, 81.9\%, and 78.3\% in the Base, Novel, and General settings, respectively. 
The ACC for the small objects improved by 33.7\%, 17.8\%, and 31.9\% in three different settings and reaches 80.3\%, 66.3\%, and 68.8\% after adaptation. This indicates that the simple fine-tuning method can effectively bridge the domain gap and make the feature more discriminating.


Table~\ref{table:ablation_feat_and_clip_proposal} shows the effectiveness of the CLIP Proposals and the CLIP's feature adaptation in the detection setting.
All the models are trained with semantic-based regressor and per CLIP Proposal distillation weight. 
Following the method used in~\cite{gu2021open}, we generate the RPN proposals by using the RPN of Mask R-CNN trained on the base category annotations. 
%The model trained with raw CLIP features and RPN proposal can be regarded as the implementation of ViLD with a much shorter training schedule and without heavy augmentation.
Using the adapted CLIP's feature for distillation can consistently improve both base and novel categories performance, no matter which kinds of distillation regions we use. 
Our model performance on novel benefits from using CLIP Proposals as distillation regions (30.4\% vs. 24.6\% with adapted CLIP features and 19.3\% vs 17.5\% with raw CLIP features).
We believe that the adaptation of the CLIP and CLIP Proposal are complementary to each other, which makes the improvement given by CLIP Proposals with adapted CLIP (5.8\%) larger than CLIP Proposals with raw CLIP (1.8\%).

The performance on base categories of models trained with CLIP Proposals is slightly worse than those trained with RPN proposals since the CLIP Proposals focus more on the regions with novel categories.
%Similar trade-off between the base and novel is also mentioned in \cite{gu2021open}.
Our experiment results show that with a longer training schedule, the performance gap on the base categories can be eliminated while the advantage on the novel will be maintained. We will provide these results in the supplementary material.


\textbf{Semantic-based Regressor and Per CLIP Proposal Distillation Weight.}
Table~\ref{table:albation_rwe_and_ppdw} shows the effects of semantic-base regressor and per CLIP Proposal distillation weight. 
All the experiments use adapted CLIP's features, and CLIP Proposals as distillation regions.
The semantic-base regressor helps the model performs better on both base and novel categories, showing the semantic meaning of the categories does provide useful information to the regressor and improves its performance.
Combining the semantic-base regressor and per CLIP Proposal distillation weight, the AP50 on novel reaches 30.4\%. 
This indicates the reweighting of different distillation boxes further improves the distillation quality.


\textbf{Statistic and Visualization Analysis.}
To compare which proposals can provide more meaningful novel categories information, we compare the effective distillation region of different proposals in an 8000 images subset of COCO training set in Table~\ref{table:iou_between_proposal_and_gtbbox}. 
We calculate the \textbf{I}ntersection between the proposal and the novel GT bboxes \textbf{o}ver novel \textbf{GT} bboxes (IoGT) and present the number and the percentage of the proposals that have high IoGT in all proposals.
Our CLIP proposals are 6\% higher than the RPN proposal in the percentage of the high IoGT proposals, meaning that the CLIP proposal can cover more potential novel instances and improve the distillation efficiency.

Figure~\ref{detection_visualization} provides some visualizations of the detected novel objects on the COCO zero-shot benchmark. 
The first row presents the results from the model trained with the adapted CLIP features from CLIP Proposals (AFCP). 
The second row presents the results from the model trained with raw CLIP features from the RPN proposal (RFRP).
The results show that though two models can localize the object correctly, the AFCP model has a higher classification accuracy than the RFRP model thanks to the adapted features and the more meaningful distillation regions.

\section{Conclusion}
We have proposed Efficient Feature Distillation for Zero-shot Detection (EZSD) in this paper. 
EZSD successfully solves two problems that are critical in transferring knowledge from large-scale multi-modal models to zero-shot detectors. 1. bridging the domain gap between the classification dataset and detection dataset, and 2. selecting the meaningful distillation regions (CLIP Proposals) for obtaining knowledge from CLIP.
Benefiting from solving these two problems, EZSD achieves state-of-the-art performance with a much shorter training schedule in the COCO zero-shot setting and outperforms previous work by 4\% in LVIS overall setting with 1/10 training time.
We believe our work provides a solid solution for applying zero-shot detection in real life, and hope our method can inspire other works in the future.
{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\clearpage
\appendix

\begin{figure*}[!t]
   \centering
   \includegraphics[width=1\linewidth]{clip_proposal_visualization.pdf} 
   %\vskip -0.1in
   \caption{Visualization of using CLIP Proposals or RPN proposals as distillation regions in COCO zero-shot setting. The blue boxes and green boxes represent the GT bboxes of the novel and base categories. The red boxes represent the CLIP proposals or the RPN proposals with the highest IoU with the novel GT bboxes. The visualization shows the CLIP proposals can cover more novel objects even though the box may not accurate.} 
   \label{clip_proposal_visualization} 
   \vskip -0.1in
 \end{figure*}
 
 
%%%%%%%%% BODY TEXT
\section{Implementation Details}\label{sec:implementation details}
We include all implementation details in this section. 
%We conduct all our experiments on two GPUs.
\subsection{Adapt Image-language Model Feature} \label{sec:Adapt}
% model we use
We use the publicly available pretrained CLIP\cite{radford2021learning} model ViT-B/32 as the open-vocabulary classification model, with an input size of 224$\times$224.

% data pipeline
Based on the detection setting we use for training and evaluating our zero-shot detector, we adapt the CLIP to two detection domains: COCO\cite{lin2014microsoft} zero-shot domain, LVIS zero-shot domain\cite{gupta2019lvis}.
We finetune the layer normalization layers in the CLIP with base category instances in COCO or LVIS based on the detection setting we use and maintain all other parameters fixed.
All base category instances are cropped by 1.2x enlarged GT bboxes.
We conduct the zero padding to convert each cropped region to the square and apply the default preprocessing pipeline of the CLIP.

% model training/ training schedule
We use CLIP to predict the category of each cropped region and calculate the cross-entropy loss with the GT label of each region.
We finetune the model by optimizing the Cross-Entropy Loss.
We use AdamW optimizer with a learning rate of 0.0001, batch size 4 and clip the L2 norm of the gradients when larger than 0.1. 
We finetune the model for 12 epochs.



\subsection{Generate CLIP Proposals}
% model we use
When generating the CLIP Proposals, we still use the CLIP model we mentioned in section~\ref{sec:Adapt} as a classifier to select the distillation regions. If we will use the adapted CLIP's feature to train the zero-shot detector, we will use the adapted CLIP to generate the CLIP Proposals. Otherwise, we use the unadapted CLIP to generate CLIP Proposals.

% data pipeline
We generate the CLIP proposals on all the training images of the detection dataset base on the detection setting we use.
We first resize the image with the image ratio maintained. 
The long edge of the image will be resized into 1333 as width or 800 as height.


% model training/ training schedule
We generate the anchors on each image with a stride of 32 pixels and with 5 different sizes(32, 64, 128, 256, 512), and 3 different ratios(1:1, 2:1, 1:2).
We select the top 1000 anchors after NMS as CLIP Proposals on each image.
We filter out the anchors which have high IoU with the base category GT bboxes to reduce the redundancy since we will add 1.2x enlarged base category GT bbox as part of the CLIP Proposals.
In model training, we randomly select a fixed subset with 200 CLIP Proposals on each image for training.

\subsection{Zero-shot Detection Setting}
In COCO zero-shot detection setting, the dataset is divided into 48 base categories and 17 novel categories.
15 categories without a synset in the WordNet hierarchy are removed. 

We filter out the training images which do not have base category annotation.
Following the setting in \cite{zareian2021open}, we filter out the images that have neither the base category instances nor the novel category instances in the validation set. 
The training set contains 107761 images and 665387 base category instances.
The validation set contains 4836 images and 28538 base category instances and 33152 novel category instances.
We evaluate the model in a generalized setting, which evaluates the base and novel categories at the same time.
AP50 is used as the evaluation metric.

In LVIS zero-shot detection setting, the dataset is divided into 866 base categories (containing 405 frequent categories and 461 common categories) and 337 novel categories (337 rare categories).
This split is noted as \textbf{[F+C/R]}.
The training set contains 98531 images and 1264884 base category instances.
The validation set contains 19442 images and 243507 base category instances and 1200 novel category instances.
However, we argue that in this split, the rare category objects are so sparse (less than 0.5\% annotations in the validation set) that the model's performance on it is not representative. 
Therefore, we propose another split. \textbf{[F/C/R]} uses the frequent categories as the base(405 categories), common and rare categories as the novel(common has 461 categories, rare has 405 categories).
The training set contains 98531 images and 1200258 base category instances.
The validation set contains 19442 images and 230427 base category instances and 14280 novel category instances.
We aggregate the model performance in frequent, common, and rare categories separately.
AP is used as the evaluation metric.

\begin{figure*}[!t]
   \centering
   \includegraphics[width=1\linewidth]{tsne_map.pdf} 
   %\vskip -0.1in
   \caption{The tSNE embeddings of the COCO GT instance feature from the unadapted CLIP and adapted CLIP. The GT features from the adapted CLIP form more dense clusters, indicating that the features become more discriminating and the CLIP is adapted into the detection dataset domain.} 
   \label{tsne_map} 
   \vskip -0.1in
 \end{figure*} 

\section{Experiments in Few-shot Detection Settings}\label{sec:fewshot}

Few-shot detection setting is similar to the one in zero-shot detection. 
The model is trained on the base category's annotations and evaluated on novel categories. 
The only difference is that in few-shot detection, each novel category has the same number of annotated objects(i.e, K-shot), which can be used to improve the model performance on the novel before the model is evaluated.
We directly evaluate our model in the few-shot benchmark, without using this K-shot additional information.


\textbf{Datasets and Evaluation Metrics.} 
We evaluate our approach on PASCAL VOC 2007+2012 and COCO.
For the few-shot PASCAL VOC dataset, we combine the trainval set of 2007 with the one of 2012 as training data. 
PASCAL VOC 2007 test set is used for evaluation. 
The 20 classes are divided into 15 base classes and 5 novel classes. 
We evaluate our model in three different base/novel splits used in \cite{wang2020frustratingly}.
Split 1 has 14631 training images with 41084 base category instances, and the validation set has 4952 images, 10552 base category instances, and 1480 novel instances.
Split 2 has 14779 training images with 40397 base category instances, and the validation set has 4952 images, 10447 base category instances, and 1585 novel instances.
Split 3 has 14318 training images with 40511 base category instances, and the validation set has 4952 images, 10605 base category instances, and 1427 novel instances.


For the few-shot COCO dataset, we use the COCO train2017 as training data and evaluate our model on the COCO val2017.
The 20 categories that exist in PASCAL VOC are used as the novel categories, while the rest of the 60 categories are used as the base categories.
The training set has 98459 images and 367189 base category instances.
The validation set has 5000 images and 15831 base category instances and 36781 novel category instances.

AP50 is used as the evaluation metric in PASCAL VOC, while AP and AP50 are used in COCO.


\textbf{Model.}
Following previous work in few-shot detection, we train a Faster R-CNN\cite{ren2015faster} model with ResNet-101 FPN backbone. 
The backbone is pretrained on ImageNet. 
We use SGD as the optimizer with batch size 4, learning rate 0.005, momentum 0.9, and weight decay 0.0001. 
We also adopt linear warmup for the first 500 iterations, with a warm up ratio is 0.001. 
We apply multi-scale train-time augmentation.
For the PASCAL VOC dataset, we train the model for 21 epochs and divide the learning rate by 10 at epoch 15 and epoch 18.
For the COCO dataset, we train the model for 18 epochs and divide the learning rate by 10 at epoch 14 and epoch 16.

\textbf{Baselines.}
We compare EZSD's performance with two few-shot detection models, TFA\cite{wang2020frustratingly} and Meta Faster R-CNN \cite{han2022meta} as the baselines. The TFA model with linear layer as the classifier is noted as \textit{TFA w/fc}, while the model with cosine classifier is noted as \textit{TFA w/cos}.

\textbf{Results.}
Table \ref{table:voc_fewshot} shows the results on the PASCAL dataset. 
EZSD achieves 40.9\% in novel AP50 averaged over three different splits.
EZSD's performance matches the TFA 3-shot performance in split1 and split2 and is 4.7\% higher than TFA in split3. 
Compared with the TFA's performance on base, EZSD is 1.8\% higher.
For Meta Faster R-CNN, it generates proposals for each category on each image, which needs multiple forward passes. 
Its inference time will be much slower if the dataset has a large number of novel categories.
Compared with the Meta Faster R-CNN, EZSD outperforms it without using any additional annotations by a 1.6\%, 3\%, and 6.9\% in three different splits, respectively.
Table \ref{table:coco_fewshot} shows the results on the COCO dataset.
EZSD achieves 10.2\% and 22.2\% in AP and AP50, respectively, matching TFA's 10-shot performance and 2.6\% and 5.9\% higher than the Meta Faster R-CNN's 2-shot performance in AP and AP50, respectively.
Our model zero-shot performance on the few-shot setting shows the power of adapted multi-modal feature space and validates the effectiveness of using CLIP Proposals as distillation regions.

 \begin{table}[!t]
    \centering
    \begin{tabular}{c|c|cccc}
      \hline % horizontal line

       \hline
       \multirow{2}{*}{Method} & \multirow{2}{*}{Shot} & \multicolumn{4}{c}{Novel AP50}  \\ \cline{3-6} 
       
        & &Split1         & Split2        & Split 3       & Avg \\
       \hline
       TFA w/fc          & 1    & 36.8          & 18.2          & 27.7          & 27.6 \\
       TFA w/fc          & 2    & 29.1          & 29.0          & 33.6          & 30.6 \\ 
       TFA w/fc          & 3    & 43.6          & 33.4          & 42.5          & 39.8 \\ 
       TFA w/cos         & 1    & 39.8          & 23.5          & 30.8          & 31.4 \\
       TFA w/cos         & 2    & 36.1          & 26.9          & 34.8          & 32.6 \\ 
       TFA w/cos         & 3    & \textbf{44.7} & \textbf{34.1} & 42.8          & 40.5 \\ 
       MF R-CNN          & 1    & 43.0          & 27.7          & 40.6          & 37.1\\
       \hline 
       Ours              & 0    & 44.6          & 30.7          & \textbf{47.5} & \textbf{40.9} \\ 
      \hline  
       \multicolumn{6}{c}{\textbf{Split1 Base(AP50): TFA (3-Shot)=79.1, Ours=80.8}} \\
       \hline
    \end{tabular}
    \centering
    \vspace{+1mm}    
    \caption{Evaluation results on the novel categories of PASCAL VOC few-shot benchmark. MF R-CNN means Meta Faster R-CNN. Our model zero-shot performance on the novel match the TFA's performance in its 3-shot setting. Our model also has a better performance on base.}
    \label{table:voc_fewshot}   
 \end{table}
 
 \begin{table}[!t]
    \centering
    \begin{tabular}{c|c|c c}
      \hline % horizontal line

       \hline
       Method            & Shot & AP & AP50        \\
       \hline
       TFA w/fc          & 10    & 10.0  & 19.2      \\
       %TFA w/fc          & 30    & 13.4  & 24.7      \\ 
       TFA w/cos         & 10    & 10.0  & 19.1      \\ 
       %TFA w/cos         & 30    & \textbf{13.7}  & 24.9     \\
       MF R-CNN          & 2     & 7.6   & 16.3     \\
       %MF R-CNN          & 10    & 12.7  & \textbf{25.7}     \\
       \hline 
       Ours              & 0     & \textbf{11.0}  & \textbf{23.5}         \\ 
      \hline  
    \end{tabular}
    \centering
    \vspace{+1mm}    
    \caption{Evaluation results on novel categories of COCO few-shot benchmark. MF R-CNN means Meta Faster R-CNN. Our model zero-shot performance on the novel match the TFA's performance in its 10-shot setting.}
    \label{table:coco_fewshot}   
 \end{table} 
 

\section{Additional Ablation Study}\label{sec:ablation}
Table \ref{table:bbox_size} presents the experimental results on how the size of the bounding box (bbox) that we use to crop the instances in the COCO~\cite{lin2014microsoft} dataset affects the classification accuracy (ACC) of the unadapted CLIP~\cite{radford2021learning}.
For the large objects, the more accurate bbox provided the higher ACC CLIP can achieve. For the small objects, CLIP needs more background information to be correctly classified. In all settings, the average ACC over all three sizes of the bbox is still much lower than the classifier of the well-trained detector, indicating the domain gap between the training data of the CLIP and the detection dataset exists. We use the 1.2x GT bbox to crop the base GT instance since it has the highest average ACC.

We provide an additional ablation study in Table \ref{table:ablation_feat_and_clip_proposal_36e}. We train all models with the adapted CLIP features. For the models trained with 12 epochs, the performance on novel categories of the model trained with the RPN proposals is 5.8\% lower than the one of the model trained with the CLIP proposals, though the former has slightly better performance on base categories. For the models trained with 36 epochs, two models (RPN proposal and CLIP proposal) has similar performance on base categories, and the model trained with the CLIP proposal features still have much better novel category performance. This indicates that the negative effect on model performance on base categories caused by the CLIP proposal is negligible and can be alleviated by a longer training schedule.
It also shows that the information of base categories provided by the distillation has redundancy, which may accelerate the model convergence on base, but may not improve the model performance.

 \begin{table}[!t]
    \centering
    \begin{tabular}{c|cccc}
      \hline % horizontal line 
      \multirow{2}{*}{Bbox Size} & \multicolumn{4}{c}{General} \\ \cline{2-5} 
                            & L    & M    & S    & Avg  \\ \hline 
       0.8x GT       & 62.3          & 54.0          & 23.2          & 47.1 \\
       1.0x GT       & \textbf{64.0} & 61.9          & 32.9          & 53.4 \\
       1.2x GT       & 61.3          & \textbf{62.2} & 36.9          & \textbf{53.9} \\
       1.5x GT       & 56.7          & 59.5          & 40.6          & 52.6 \\
       2.0x GT       & 50.5          & 52.6          & \textbf{42.9} & 48.9 \\
      \hline     
    \end{tabular}
    \centering
    \vspace{+1mm}    
    \caption{The classification accuracy (ACC) of the unadapted CLIP on COCO instances with different sizes of the GT bboxes to crop the instances. We decide to use the 1.2x enlarged GT bbox to crop the instance since it has the best average ACC.}
    \label{table:bbox_size}   
 \end{table}

 \begin{table}[!t]
    \centering
    \begin{tabular}{c|c|ccc}
      \hline % horizontal line

       \hline
       Epoch & Distill Region & Base & Novel & Overall \\
       \hline

       12      & RPN Proposal  & 56.9          & 24.6          & 48.5 \\ 
       12      & CLIP Proposal & 55.7          & 30.4 & 49.0 \\ 
       36      & RPN Proposal  & \textbf{60.2} & 24.3          & 50.8 \\ 
       36      & CLIP Proposal & 59.9          & \textbf{31.6} & \textbf{52.1} \\ 
      \hline     
    \end{tabular}
    \centering
    \vspace{+1mm}    
    \caption{Ablation study on using CLIP Proposals as distillation in COCO zero-shot benchmark. The model trained with CLIP Proposals has much better performance on novel categories.}
    \label{table:ablation_feat_and_clip_proposal_36e}   
 \end{table}




\section{Additional Visualizations}\label{sec:visualization}


 
Fig~\ref{clip_proposal_visualization} shows the visualization of using CLIP Proposals and RPN proposals as distillation regions in the COCO zero-shot setting. 
The blue boxes and green boxes represent the GT bboxes of the novel and base categories. 
The red boxes represent the CLIP Proposals or the RPN proposals with the highest IoU with the novel GT bboxes. 
The three images on the left show that the CLIP Proposals can cover most of the novel category objects although the boxes may not accurate, while the RPN regards some of the novel objects as background and just ignores them. 
Although the CLIP proposals are not accurate, the features extracted from these boxes are accurate and meaningful. 
This phenomenon is also proved by the experiments in \cite{gu2021open}.
Therefore, using the CLIP Proposals as distillation regions provides more novel category information and improve the detector's performance on the novel.

Fig~\ref{tsne_map} shows the tSNE embeddings of the COCO instance features of the unadapted CLIP and the adapted CLIP. 
We collect 20 GT instances for each base and novel category in COCO zero-shot setting and extract their features from unadapted CLIP or adapted CLIP, and then generate the tSNE embeddings with these features.
The GT instances in the adapted CLIP feature space form some dense clusters. This indicates that the CLIP's feature space has been adapted in the COCO dataset domain and the features become more discriminating after adaptation, improving the classification accuracy. 
The dots do not form a dense cluster mostly come from the "person" category. Since the instances of the person usually show up with other categories instances and occluded by other objects, therefore the person categories features are more scattered.



\end{document}
