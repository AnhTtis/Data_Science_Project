\section{Conclusions}
Traditionally, the problem of detecting model shift behaviour has relied on measurements for detecting shifts in the input or output data distributions. In this paper, we have provided theoretical and experimental evidence that explanation shift can be a more suitable indicator to detect and identify the shift in the behaviour of machine learning models. We have provided mathematical analysis examples, synthetic data, and real data experimental evaluation. We found that measures of explanation shift can provide more insights than measures of the input distribution and prediction shift when monitoring machine learning models. 



\subsection*{Reproducibility Statement}\label{sec:reproducibility}
To ensure reproducibility, we make the data, code repositories, and experiments publicly available
\footnote{\url{https://anonymous.4open.science/r/ExplanationShift-icml/README.md}}. Also, an open source Python package is released with the methods used (to be released upon acceptance). 
For our experiments, we used default \texttt{scikit-learn} parameters \cite{pedregosa2011scikit}. We describe the system requirements and software dependencies of our experiments. Experiments were run on a 4 vCPU server with 32 GB RAM.

\begin{comment}
\subsection*{Acknowledgments}
This work has received funding by the European Union’s Horizon 2020 research and innovation programme under the Marie
Skłodowska-Curie Actions (grant agreement number 860630) for
the project : \enquote{NoBIAS - Artificial Intelligence without Bias}. Furthermore, this work reflects only the authors’ view and the European Research Executive Agency (REA) is not responsible for any
use that may be made of the information it contains. 
\end{comment}