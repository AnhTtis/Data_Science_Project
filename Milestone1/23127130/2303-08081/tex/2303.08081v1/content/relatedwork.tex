\section{Foundations and Related Work}\label{sec:foundations}
%\gjergji{This comes quite unexpectedly. I think we should start with explanations shift detection methods and their shortcomings; we should then hint at the advantages of analyzing explanation shifts and then motivate why we choose Shapley values as a representative explanation method for our approach.}\carlos{I agree, but for the next resubmission}
\subsection{Explainable AI}
Explainability has become an important concept in legal and ethical data processing guidelines and machine learning applications ~\cite{selbst2018intuitive}. A wide variety of methods have been developed to account for the decision of algorithmic systems ~\cite{guidotti_survey,DBLP:conf/fat/MittelstadtRW19,DBLP:journals/inffus/ArrietaRSBTBGGM20}. One of the most popular approaches to explaining machine learning models has been the use of Shapley values to attribute relevance to features used by the model~\cite{lundberg2020local2global,lundberg2017unified}. The Shapley value is a concept from coalition game theory that aims to allocate the surplus generated by the grand coalition in a game to each of its players~\cite{shapley}. The Shapley value $\mathcal{S}_j$ for the $j$'th player can be defined via a value function $\mathrm{val}:2^N \to \mathbb{R}$ of players in $T$:

\begin{small}
\begin{gather}
\mathcal{S}_j(\mathrm{val}) = \sum_{T\subseteq N\setminus \{j\}} \frac{|T|!(p-|T|-1)!}{p!}(\mathrm{val}(T\cup \{j\}) - \mathrm{val}(T))
\end{gather}
\end{small}

In machine learning, $N=\{1,\ldots,p\}$ is the set of features  occurring in the training data, and $T$ is used to denote a subset of $N$. Given that
 $x$ is the feature vector of the instance to be explained, and the term $\mathrm{val}_x(T)$ represents the prediction for the feature values in $T$ that are marginalized over features that are not included in $T$:
% \begin{gather}
% \mathrm{val}_{\hat{f},x}(T) = E_{X_{N\setminus T}}[\hat{f}(X)|X_T=x_T]-E_X[\hat{f}(X)]
% \end{gather}
\begin{gather}
\mathrm{val}_{f,x}(T) = E_{X|X_T=x_T}[f(X)]-E_X[f(X)]
\end{gather}

The Shapley value framework satisfies several theoretical properties~\cite{molnar2019,shapley,WINTER20022025,aumann1974cooperative}, and our approach is based on the efficiency property. Our approach works with explanation techniques that fulfill efficiency  and uninformative properties, and we use Shapley values as an example.


\textbf{Efficiency.} Feature contributions add up to the difference of prediction for $x^{\star}$ and the expected value of $f$:
\begin{gather}
    \sum_{j \in N} \Ss_j(f, x^{\star}) = f(x^{\star}) - E[f(X)])
\end{gather}

%\klaus{I would indicate, that $\mathrm{val}$ also depends on $\hat{f}$: $\mathrm{val}_{\hat{f},x}(T)$ }
%\steffen{The first expected value does not make sense as the index in the E does have a different dimensionality than the X. And this seems to be only one bug}\klaus[inline]{I still think this equation is valid: we condition on the features in $T$, so the expectation has to be done overall feature, not in $T$. So $E_{X_{N\setminus T}}$ is the expectation we want to build. If we ignore the ordering, we could write $\hat{f}(X_T,X_{N\setminus T})$ instead of $\hat{f}(X)$, which might make it clearer, but formally less correct.}

It is important to differentiate between the theoretical Shapley values and the different implementations that approximate them. We use  TreeSHAP as an efficient implementation of an approach for tree-based models of Shapley values~\cite{lundberg2020local2global,molnar2019,Zern2023Interventional}, particularly we use the observational (or path-dependent) estimation  ~\cite{DBLP:journals/corr/abs-2207-07605,DBLP:conf/nips/FryeRF20,DBLP:journals/corr/ShapTrueModelTrueData} and for linear models we use the correlation dependent implementation that takes into account feature dependencies \cite{DBLP:journals/ai/AasJL21}.


%In this work, we study changes in the Shapley values (explanation shift), which are related to changes in the model predictive performance and fairness.


%\gjergji{Here, we need to state again why the above discussions are important for our work and what we are aiming for. As discussed in our meeting, I think only focusing on SHAP might be too restrictive.} \carlos{I agree, but Shapley values have some nice theoretical properties that allow to proof the theorems. Let's leave this for further work?}\steffen{I agree with Gjergji: This should be handled now. NeurIPS expects that such issues are handled in one publication and not in a string of publications.}\carlos{This would take me in the best case scenario one month of work. Also, LIME has one crucial hyperparameter. The size of the neighborhood sampling, without tuning this the results might be random.}

%There are two ways we might want to compute SHAP values: the full conditional SHAPvalues or the interventional SHAP values. For interventional SHAP values, we break any dependence structure between features in the model and uncover how the model would behave if we intervened and changed some of the inputs. We respect the correlations among the input features for the total conditional SHAP values, so if the model depends on one input. However, if that input is correlated with another input, both get some credit for the model's behavior. The interventional option stays "true to the model," meaning it will only give credit to the model's features. In contrast, the correlation option stays "true to the data" because it only considers how the model would behave when respecting the correlations in the input data. For sparse cases, the only interventional option is supported.



\subsection{Related Work on Tabular Data}
Evaluating how two distributions differ has been a widely studied topic in the statistics and statistical learning literature~\cite{statisticallearning,datasetShift}, that have advanced recently in last years ~\cite{DBLP:conf/nips/ParkAKP21,DBLP:conf/nips/LeeLLS18,DBLP:conf/icml/ZhangSMW13}.~\cite{DBLP:conf/nips/RabanserGL19} provide a comprehensive empirical investigation, examining how dimensionality reduction and two-sample testing might be combined to produce a practical pipeline for detecting distribution shifts in real-life machine learning systems. 

Some techniques detect that new data is out-of-distribution data when using neural networks based on the prediction space~\cite{fort2021exploring,NEURIPS2020_219e0524}. They use the maximum softmax probabilities/likelihood as a confidence score~\cite{DBLP:conf/iclr/HendrycksG17}, temperature or energy based scores ~\cite{DBLP:conf/nips/RenLFSPDDL19,DBLP:conf/nips/LiuWOL20,DBLP:conf/nips/WangLBL21}, they extract information from the gradient space~\cite{DBLP:journals/corr/GradientShift}, they fit a Gaussian distribution to the embedding or they use the Mahalanobis distance for out-of-distribution detection~\cite{DBLP:conf/nips/LeeLLS18,DBLP:journals/corr/reliableShift}. 

Many of these methods are  developed specifically for neural networks that operate on image and text data, and often they can not be directly applied to traditional machine learning techniques. 
For image and text data, one may build on the assumption that the relationships between relevant predictor variables ($X$) and
response variables ($Y$) remains unchanged, i.e.\ that no \emph{concept shift} occurs. For instance, the essence of how a dog looks like remains unchanged over different data sets, even if contexts may change. Thus, one can define  invariances on the latent spaces of deep neural models, which are not applicable to tabular data in a likewise manner. For example, predicting buying behavior before, during, and after the COVID-19 pandemic constitutes a concept shift that is not amenable to such methods.
 We  focus on such tabular data where techniques such as gradient boosting decision trees achieve state-of-the-art model performance~\cite{grinsztajn2022why,DBLP:journals/corr/abs-2101-02118,BorisovNNtabular}. 
 %The explanation space is a projection of a machine-learning model to a distribution space that has more dimensions than the output space and has theoretical properties that we exploit in contrast to latent spaces.


%\steffen{The following is rather unclear and lacks citation.}Other research lines, such as membership inference (or membership classifiers) aims to detect that given a data record and a machine learning model, detect if the record was in the modelâ€™s training dataset or not ~\cite{DBLP:conf/sp/ShokriSSS17}. Our approach is slightly different, as we don't attempt to predict if a certain instance belongs or not to the training data, but if its distribution is as the training dataset.  \steffen{I do not understand why the topic of discovering that an instance was part of the training data is even an issue that is worth discussing.}

% For tabular data libraries such as \cite{alibi-detect} recolect the work 
% Shap original paper suggestion

The first approach of using explainability to detect changes in the model was suggested by~\cite{lundberg2020local2global} who monitored the SHAP value contribution in order to identify possible bugs in the pipeline. A similar approach by~\cite{li2022enabling} allows for tracking distributional shifts and their impact among input variables using slidSHAP a novel method for unlabelled data streams. 
%Other works use explainable AI towards distribution shifts are 
\cite{DBLP:conf/aistats/BudhathokiJBN21}  identifies the drivers of distribution changes using graphical causal models and feature attributions using Shapley values. % The key idea is that, given a causal graph, they factorize the joint distribution into independent causal conditionals. Then, changes in the joint distribution can  be attributed to changes in some of the causal conditionals.
  In our approach we do not rely on additional information, such as a causal graph~\cite{schrouff2022diagnosing}. We will analyse later why monitoring changes in the input data distributions and the prediction distributions are not sufficient to monitor for change.
%\steffen{One question that was not answered in the past (and I have not read the complete paper yet) was: what are the best competing methods that will be compared to empirically. This seems like a worthy candidate to compare to.}\carlos{It's on the synthetic data experiments part.}\steffen{How was your AAAI2023 paper related?}


%Other lines have aimed to predict or find proxies for OOD model performance using agreement-on-the-line \cite{baek2022agreementontheline}, accuracy on the line~\cite{DBLP:conf/icml/MillerTRSKSLCS21}, explainable uncertainty\cite{mougan2022monitoring} or, distribution-free uncertainty quantification under covariate shift \cite{prinster2022jaws}\steffen{how are they related to our approach? Terms like `agreement-on-the-line' do not explain anything. Our paper must be self-contained and not require to read other papers.}

%\carlos{not sure how to fit with the next paragraph.} % Then we provide an experiment where we can see that changes on the explanations serve as a better model degradation proxy than experiments on input data or model predictions.

Recent work confirms these principal limitations by theorems about the impossibility to predict model degradation~\cite{garg2022leveraging,chen2022estimating} or the impossibility to detect that new data is out-of-distribution~\cite{fang2022is,DBLP:conf/icml/ZhangGR21,guerin2022outofdistribution}.
We do not overcome such limitations, however, our approach provides for hints that allows the machine learning engineer to better understand the change of interactions resulting from shifting data distributions and learned models.

%focused on delimiting the results that can be obtained in the field, from stating impossibility theorems delimiting the situations in which we can predict model performance~\cite{garg2022leveraging,chen2022estimating} or out-of-distribution data detection~\cite{fang2022is,DBLP:conf/icml/ZhangGR21,guerin2022outofdistribution}. Our work is limited to recognizing that changes in the explanations are key indicators for changes in the model behaviour in situations where we have unlabeled OOD data.

%\subsection{Related Work on Images and Text}
%
%We study the problems possibly incurred by distribution shifts on tabular data, which still constitutes a major field of application for machine learning. In contrast, most recent research on model monitoring proposes methods for modalities such as texts or images. These recent methods build on the assumption that the relationships between relevant predictor variables ($X$) and response variables ($Y$) remains unchanged, i.e.\ that no \emph{concept shift} occurs. For instance, their assumption is that the essence of how a dog looks like remains unchanged over different contexts. Thus, they can define  invariances on the latent spaces of deep neural models, which are not applicable to tabular data in a likewise manner. For example, predicting buying behavior before, during, and after the COVID-19 pandemic constitutes a concept shift that is not amenable to such methods.