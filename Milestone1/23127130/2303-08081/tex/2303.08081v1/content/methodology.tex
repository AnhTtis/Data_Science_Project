\section{Methodology}
\subsection{Key Terminology}

The objective of supervised learning is to induce a function $f_\theta:\mbox{dom}(X) \to \mbox{dom}(Y)$, where $f_\theta$ is from a family of functions $f_\theta \in F$, from training set  $\Dd{tr}=\{(x_0^{tr},y_0^{tr})\ldots, (x_n^{tr},y_n^{tr})\} \subseteq \mbox{dom}({X} \times {Y})$
with  predictor variables $X$ and target variable $Y$, respectively. The estimated hypothesis $f_\theta$ is expected to generalize well on  new, previously unseen data $\Dd{new}=\{x_0^{new}, \ldots, x_k^{new} \} \subseteq \mbox{dom}(X)$
, for which the target labels are unknown. The traditional machine learning assumption is that training data $\Dd{tr}$ and novel data  $\Dd{new}$ are sampled from the same underlying distribution $\mathbf{P}(X\times Y)$.

%\gjergji{You first use $X$ and $Y$ as the domain and range of $f$. In the previous sentence you use them as variables in $P(X,Y)$. You can use $\mathcal{X}, \mathcal{Y}$ as the domain and range of $f$ instead. For a distribution, I would use lower-case $p$, otherwise (with $P$ as a probability measure conventionally) it is just a probability.}

%Then we call \textit{model behaviour} to the procedure of how the input data contributes to the prediction with respect to the model $f_\theta$. Then, out-of-distribution model behaviour detection is a binary classification problem that relies on a score to differentiate between in- and out-of-distribution samples that affect the model predictions.

%If we have a hold-out test data set $\Dd{te}=\{(x_0^{te},y_0^{te})\ldots, (x_m^{te},y_m^{te})\} \subseteq \mathcal{X} \times \mathcal{Y}$ disjoint from $\Dd{tr}$, but also sampled from $P(X,Y)$, one may use $\Dd{te}$ to estimate performance indicators for $\Dd{new}$. Commonly, novel data is sampled from a  distribution $P'(X,Y)$ that is different from $P(X,Y)$.% and there is no hold-out data set sampled from $P'(X,Y)$ to estimate model performance. We use $\Dd{ood} \sim P'(X,Y)$ to refer to such novel, out-of-distribution data.

\begin{definition}[Out-of-distribution data]
Given a  training data set  $\Dd{tr}=\{(x_0^{tr},y_0^{tr})\ldots, (x_n^{tr},y_n^{tr})\}\sim \mathbf{P}(X\times Y)$
and $\Dd{new}=\{x_0^{new},\ldots, x_k^{new}\}\sim \mathbf{P}(X')$, we say that $\Dd{new}$ is out-of-distribution if $\mathbf{P}(X)$ and $\mathbf{P}(X')$ are different distributions.
%\gjergji{Careful: A probability cannot be generated. There is a probability of an event under a given distribution over events. If $P$ is supposed to represent a distribution with a prior distribution, you should use $p$ instead. But in general, *data* is generated by some distribution.}
\end{definition}

\begin{definition}[Out-of-distribution predictions]
Given a model $f_\theta:\mbox{dom}(X) \to \mbox{dom}(Y)$ with parameters $\theta$ learned from training set  $\Dd{tr}=\{(x_0^{tr},y_0^{tr})\ldots, (x_n^{tr},y_n^{tr})\}$, we say that
$\Dd{new}=\{x_0^{new},\ldots, x_k^{new}\}$
 has out-of-distribution predictions with respect to model $f_\theta$ if $f_\theta(\Dd{tr}_{X})$ is sampled from a  distribution different than $f_\theta(\Dd{new})$.
\end{definition}

\begin{definition}[Explanation Space]
An explanation function $\mathcal{S}:F\times \mbox{dom}(X)\to \mathbb{R}^p$ maps a model $f_\theta$  and data of interest $x\in \mathbb{R}^p$ to a vector of contributions $\mathcal{S}(f_\theta, x)\in \mathbb{R}^p$. Given a dataset $\mathcal{D}$, its explanation space is the matrix with rows $\Ss(f_\theta, x_i)^\top$ for $x_i \in \mathcal{D}$. 

%\klaus{I would distinguish between \emph{explanation function} and \emph{SHAP Values}, which is a specific explanation function (Sabaas would be another). Note, that these are not Shapley Values, but SHAP Values with this signature.}
\end{definition}\label{def:explanationSpace}

We use  Shapley values to define the explanation function $\Ss$. 
%\steffen{What is the signature?}\carlos{can i say $dim(x_i) \equiv dim(\mathcal{S}(f_\theta,x_i))$} \klaus{You could write something like $\mathcal{S}:F\times \mathcal{X}\to \mathbb{R}^p$} \klaus[inline]{Might be better to define it for a single feature vector $x$. Batch computation is an implementation detail.}\klaus{Be nice to the reader. $S$ (the subsets) and $\mathcal{S}$ (the explanation function) look quite similar. Consider to use e.g. $T$ for the former one.}
\begin{definition}[Out-of-distribution explanations]
Given a model $f_\theta:\mbox{dom}(X) \to \mbox{dom}(Y)$ with parameters $\theta$ learned from training set  $\Dd{tr}=\{(x_0^{tr},y_0^{tr})\ldots, (x_n^{tr},y_n^{tr})\}$, we say that $\Dd{new}=\{x_0^{new},\ldots, x_k^{new}\}$ has \emph{out-of-distribution explanations} with respect to the model $f_\theta$ if $\Ss(f_\theta,\Dd{new})$ is sampled from a different distribution than $\Ss(f_\theta,\Dd{tr}_X)$.
\end{definition}

\begin{definition}[Explanation Shift]
Given a measure of statistical distance $d$,
we measure \emph{explanation shift} as the distance between two explanations of the model $f_\theta$ 
by  $d(\mathcal{S}(f_\theta, \Dd{tr}_X),\mathcal{S}(f_\theta, \Dd{new}))$.
\end{definition}

%\steffen{should explanation shift be defined on distributions or on data instances?}\carlos{since ood data is on dist, then ood exp is on dist too}



\subsection{Explanation Shift Detector: Quantifying and interpreting OOD explanations}\label{sec:Detector}

%We start by fitting a model $f_{\theta}$ to the training data, $X^{\text{tr}}$ to predict an  outcome $Y^{\text{tr}}$, and calculate the SHAP values on a hold out set $\Ss(f_\theta,X^{\text{val}})$ and in the out-of-distribution data $\Ss(f_\theta,X^{\text{ood}})$.
%\steffen{problem definitions should always be independent of the solution. Your problem definition (which should be placed at the beginning of section 3) should not even mention explanation. What should be described here is the method, i.e. that g is learned. Note that this section still misses the explanation part}\carlos{there is a paragraph there, but dont know exactly how to formalize this}\klaus{Two reasons: 1. as Steffen said: don't mix the solution into the problem. The problem here is to detect OOD Explanations. Whether you do it with a classifier $g_\psi$ or with another approach is not part of the problem. 2. I wonder if detecting ood explanations is not itself part of the solution and detecting ood data / predictions is the real problem.}\carlos{Now?}

Given a training dataset, a model, and a new dataset sampled from an unknown distribution.  The problem we are trying to solve is measuring and inspecting out-of-distribution explanations on this new dataset. Our proposed method is the \enquote{Explanation Shift Detector}:

\begin{definition}[Explanation Shift Detector]
Given training data $\Dd{tr}=\{(x_0^{tr},y_0^{tr})\ldots, (x_n^{tr},y_n^{tr})\} \sim \mathbf{P}({X} \times {Y})$  and a classifier $f_\theta$ the \emph{Explanation shift detector} returns ID, if $S(f_\theta,X)$ and
$S(f_\theta,X^{new})$ are sampled from the same distribution and OOD otherwise.
%
%aim of \enquote{Explanation OOD detection} is to train a classifier $g_\psi$ such that for any explanation of test data $\Ss(f_\theta,x)$ drawn from the mixed marginal distribution of $\Dd{te}$ and $\Dd{ood}$,$\quad g_\psi$ can correctly classify $\Ss(f_\theta,x)$ to be a realization of $\Ss(f_\theta,X)$ if ID model behaviour or $\Ss(f_\theta,X')$ if OOD model behaviour, we denote this label as $A = \{a_x | x\in X\cup X^{new}, a_x\in \{0,1\}$. The function $g:\Ss(f_\theta,\mathcal{X})\rightarrow \{ID,OOD\}$.
\end{definition}


To implement this approach we start by fitting a model $f_\theta$ to the training data, $X^{\text{tr}}$ to predict an  outcome $Y^{\text{tr}}$, and compute explanations on an in-distribution validation data set $X^{\text{val}}$:  $\Ss(f_\theta,X^{\text{val}})$. We also compute explanations on $X^{new}$, which may be in-distribution or out-of-distribution. We construct a new dataset $E=\{(S(f_\theta,x),a_x)|x\in X^{\text{val}},a_x=0\}\cup \{(S(f_\theta ,x),a_x)|x\in X^{\text{new}},a_x=1\}$ and  we train a discrimination model $g_\psi$  on the explanation space of the two distributions $E$, to predict if a certain explanation should be classified as ID or OOD.  
If the discriminator $g_\psi$  cannot distinguish the two distributions in $E$, i.e.\ its AUC is approximately $0.5$, then $X^{new}$ as a whole is classified as showing in-distribution behavior: its features interact with $f_\theta$ in the same way as with validation data. 

%\steffen{I rewrote this part. check it}

\begin{gather}\label{eq:explanationShift}
\psi = \argmin_{\tilde{\psi}} \sum_{x\in X^{\text{val}}\cup X^{\text{new}}} \ell( g_{\tilde{\psi}}(\Ss(f_\theta,x)) , 
a_x )
\end{gather}

Where $\ell$ is any given classification loss function (eq.~\ref{eq:explanationShift}). We call the model $g_\psi$ \enquote{Explanation Shift Detector}. One of the benefits of this approach is that it allows for \enquote{explaining} the \enquote{Explanation Shift Detector} at both global and individual instances levels. In this work, we use feature attribution explanations for the model $g_\psi$, whose intuition is to respond to the question of: \textit{What are the features driving the OOD model behaviour?}. For conceptual simplicity, in this work, our model $g_\psi$ is restricted to linear models and  we will use the coefficients as feature attribution methods. Future work can be envisioned in this section by applying different explainable AI techniques to the \enquote{Explanation Shift Detector}.

%Some of the main questions that arise from the previous section are \textit{What happens with multidimensional statistical testing?} or \textit{Does the explanation space work in hybrid/simultaneous shifts?}, and further more \textit{Can we interpret the feature and features interaction that drive distribution shift?}

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.8\columnwidth]{images/diagram.png}
    \caption{\enquote{Explanation Shift Detector} workflow diagram. The model $f_\theta$ is trained on $(X,Y)$, and outputs explanations for both  distributions. Then the \enquote{Explanation Shift detector} $g_\psi$ receives the explanations and aims to predict if the model behaviour is different between them. If the $AUC \neq 0.5$ then there is a change in the data that implies a change in the model behaviour. Finally, we can proceed to account $g_\psi$ with explainable AI techniques and identify the roots of the change.}
    \label{fig:workflow}
\end{figure}

