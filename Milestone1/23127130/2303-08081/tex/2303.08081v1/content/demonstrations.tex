\section{Analytical examples}
This section covers the analytical examples demonstrations presented in the Section \ref{subsec:explanationShiftMethods} of the main body of the paper.
\subsection{Explanation vs Prediction}

\begin{prop}
    Given a model $f_\theta:X \to Y$. If $f_\theta(x^{'})\neq f_\theta(x)$, then $\Ss(f_\theta,x^{'}) \neq \Ss(f_\theta,x)$.
\end{prop}
\begin{gather}
\texttt{Given}\quad f_\theta(x)\neq f_\theta(x')\\
\sum_{j=1}^p \Ss_j(f_\theta,x) = f_\theta(x) - E_X[f_\theta(X)]\\
\texttt{then}\quad \Ss(f,x)\ \neq \Ss(f,x')
\end{gather}

\begin{example}[Explanation shift that does not affect the prediction distribution] Given $\mathcal{D}^{tr}$ is generated from $(X_1,X_2,Y), X_1 \sim U(0,1), X_2 \sim U(1,2), Y = X_1+X_2+\epsilon$ and thus the model is $f(x)=x_1+x_2$. If $\mathcal{D}^{new}$ is generated from $X_1^{new}\sim U(1,2), X_2^{new}\sim U(0,1)$, the prediction distributions are identical $f_\theta(\mathcal{D}^{tr}),f_\theta(\mathcal{D}^{new})\sim U(0,3)$, but explanation distributions are different $S(f_\theta,\mathcal{D}^{tr}_X)\neq S(f_\theta,\mathcal{D}^{new})$
\begin{gather}
    \forall i \in  \{1,2\} \quad \Ss_i(f_\theta,x) = \alpha_i \cdot x_i  \\
   \forall i \in  \{1,2\} \Rightarrow  \Ss_i(f_\theta,X))\neq \Ss_i(f_\theta,X^{new})\\
    \Rightarrow f_\theta(X)=f_\theta(X^{new})
\end{gather}
\end{example}
\subsection{Explanation shifts vs input data distribution shifts}
\subsubsection{Multivariate shift}
\textbf{Example 1: \textit{Multivariate Shift}}\textit{
Let $X = (X_1,X_2) \sim  N\left(\begin{bmatrix}\mu_{1}  \\ \mu_{2} \end{bmatrix},\begin{bmatrix}\sigma^2_{x_1} & 0 \\0 & \sigma^2_{x_2} \end{bmatrix}\right)$
and $X^{ood} = (X^{ood}_1,X^{ood}_2) \sim  N\left(\begin{bmatrix}\mu_{1}  \\ \mu_{2} \end{bmatrix},\begin{bmatrix} \sigma^2_{x_1} & \rho\sigma_{x_1}\sigma_{x_2}  \\ \rho\sigma_{x_1}\sigma_{x_2} & \sigma^2_{x_2}\end{bmatrix}\right)$ and target $Y = X_1 + X_2 + \epsilon$. We fit a linear model 
$f_\theta(X_1,X_2) = \gamma + a\cdot X_1 + b \cdot X_2.\hspace{0.5cm}$  $X_1$ and $X_2$ are identically distributed with $X_1^{ood}$ and $X_2^{ood}$, respectively, while this does not hold for the corresponding SHAP values $\Ss_j(f_\theta,X)$ and $\Ss_j(f_\theta,X^{ood})$.}
\begin{gather}
\Ss_1(f_\theta,x) = a(x_1 - \mu_1)\\
\Ss_1(f_\theta,x^{ood}) =\\
=\frac{1}{2}[\mathrm{val}(\{1,2\}) - \mathrm{val}(\{2\})] + \frac{1}{2}[\mathrm{val}(\{1\}) - \mathrm{val}(\emptyset)] \\
\mathrm{val}(\{1,2\}) = E[f_\theta|X_1=x_1, X_2=x_2] = a x_1 + b x_2\\
\mathrm{val}(\emptyset) = E[f_\theta]= a \mu_1 + b  \mu_2 \\
\mathrm{val}(\{1\}) = E[f_\theta(x) | X_1 = x_1] +b\mu_2 \\
\mathrm{val}(\{1\}) = \mu_1 +\rho \frac{\rho_{x_1}}{\sigma_{x_2}}(x_1-\sigma_1)+b \mu_2\\
\mathrm{val}(\{2\}) = \mu_2 +\rho \frac{\sigma_{x_2}}{\sigma_{x_1}}(x_2-\mu_2)+a\mu_1 \\
\Rightarrow \Ss_1(f_\theta,x^{ood})\neq a(x_1 - \mu_1)
\end{gather}
%\klaus{I just realized that this example requires the knowledge of the ood distribution. I need some time to rethink this.}\carlos{Its okey, we have access to $X^{ood}$ but not $y^{ood}$}

\begin{comment}
\subsubsection{Concept Shift}
\textbf{Example 2: \textit{Concept shift}}\textit{
Let $X = (X_1,X_2) \sim N(\mu,I)$, and $X^{ood}= (X^{ood}_1,X^{ood}_2) \sim N(\mu,I)$, where $I$ is an identity matrix of order two and $\mu = (\mu_1,\mu_2)$. We now create two synthetic targets $Y=a + \alpha \cdot X_1 + \beta \cdot X_2 + \epsilon$ and $Y^{ood}=a + \beta \cdot X_1 + \alpha \cdot X_2 + \epsilon$. Let $f_\theta$ be a linear regression model trained on $f(X,Y)$ and $h_\phi$ another linear model trained on $(X^{ood},Y^{ood})$. Then $P(f_\theta(X)) = P(h_\phi(X^{ood}))$, $P(X) = P(X^{ood})$ but $\Ss(f_\theta,X)\neq \Ss(h_\phi, X)$}. 
\begin{gather}
X  \sim N(\mu,\sigma^2\cdot I), X^{ood}\sim N(\mu,\sigma^2\cdot I)\\
\rightarrow P(X) = P(X^{ood})\\
Y \sim a + \alpha N(\mu, \sigma^2) + \beta N(\mu, \sigma^2) + N(0, \sigma^{'2})\\
Y^{ood} \sim a + \beta N(\mu, \sigma^2) + \alpha N(\mu, \sigma^2) + N(0, \sigma^{'2})\\
\rightarrow P(Y) = P(Y^{ood})\\
\Ss(f_\theta,X) = \left( \begin{matrix}\alpha(X_1 - \mu_1)  \\\beta(X_2-\mu_2) \end{matrix}\right) \sim \left(\begin{matrix}N(\mu_1,\alpha^2 \sigma^2)  \\N(\mu_2,\beta^2 \sigma^2) \end{matrix}\right)\\
\Ss(h_\phi,X) =  \left( \begin{matrix}\beta(X_1 - \mu_1)  \\\alpha(X_2-\mu_2) \end{matrix}\right)\sim \left(\begin{matrix}N(\mu_1,\beta^2 \sigma^2)  \\N(\mu_2,\alpha^2 \sigma^2) \end{matrix}\right)\\
\mathrm{If} \quad \alpha \neq \beta \rightarrow \Ss(f_\theta,X)\neq \Ss(h_\phi,X)
\end{gather}
\end{comment}
\subsubsection{Uninformative Features}

\textbf{Example 2: \textit{Unused features}}\textit{
Let $X = (X_1,X_2,X_3) \sim N(\mu,\mathrm{diag}(c))$, and $X^{ood}= (X^{ood}_1,X^{ood}_2,X^{ood}_3) \sim N(\mu,\mathrm{diag}(c'))$, where $\mathrm{c}$ and $\mathrm{c'}$ are an identity matrix of order three and $\mu = (\mu_1,\mu_2,\mu_3)$. We now create a synthetic target $Y=a_0 + a_1 \cdot X_1 + a_2 \cdot X_2 + \epsilon$ that is independent of $X_3$. We train a linear regression $f_\theta$ on $(X,Y)$, with coefficients $a_0,a_1,a_2,a_3$. Then if $\mu_3'\neq \mu_3$ or $c_3' \neq c_3$, then $P(X_3)$ can be different from $P(X_3^{ood})$ but $\Ss_3(f_\theta, X) = \Ss_3(f_\theta,X^{ood})$}
\begin{gather}
X_3\sim N(\mu_3,c_3),X_3^{ood} \sim N(\mu_3^{'}, c_3^{'})\\
\mathrm{If} \quad  \mu_3^{'}\neq \mu_3 \quad\mathrm{or} \quad c_3^{'}\neq c_3 \rightarrow P(X_3)\neq P(X_3^{ood})\\
\Ss(f_\theta,X) = \left(\begin{bmatrix} a_1(X_1 - \mu_1)  \\a_2(X_2 - \mu_2)  \\a_3(X_3 - \mu_3)   \end{bmatrix} \right) = \left(\begin{bmatrix} a_1(X_1 - \mu_1)  \\a_2(X_2 - \mu_2)  \\0   \end{bmatrix} \right)\\
\Ss_3(f_\theta,X)= \Ss_3(f_\theta, X^{ood})
\end{gather}

%\section{Synthetic data experiments}
%This section covers the last experiment of uninformative features on synthetic data that aims at providing empirical evidence about using the explanation space as (cf. Section \ref{sec:experiments})  
