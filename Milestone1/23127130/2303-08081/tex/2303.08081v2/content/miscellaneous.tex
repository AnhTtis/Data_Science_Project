
\subsection{Detecting fairness changes under covariate subpopulation shift}

A common type of distribution shift that can affect fairness is subpopulation shift. In this type of shift, data changes from one particular group can go unnoticed due to being statistically unrepresented. Calculating group statistics or group comparison can be misrepresentative as small groups tend to have an \textit{reducible induced bias}~\cite{FairEncoder}.

For this experiment we make use of the Adult credit scoring dataset. The available data is split into a 50/50 stratified train/test split, maintaining the ratio of each category between train and test set. The trainning predictions are extracted out of a 10 fold cross validation evaluation. From the train dataset we remove the highly educated black people  $X_{tr} \setminus X_{Black}^{Race} \cap X_{High}^{Education}$ and add it to the test set $X_{te} + X_{Black}^{Race} \cap X_{High}^{Education}$. The dataset contains $48,842$ samples from which $4,685$ are black and  $60$ highly educated black people. With this evaluation method, we are able to replicate a synthetic subpopulation shift that has effect on fairness metrics.

As a measure of unfairness, we use the notion of equal opportunity, which is estimated using the disparate treatment metric~\cite{zafat2017fairnessDisparate} ---hence, for simplicity, we refer to the interplay of these concepts as the \textit{equal opportunity fairness (EOF)} metric. The value is the difference in the True Positive Rate (TPR) between the protected group and the reference group:

\begin{equation}\label{eq:TPR}
\text{TPR} = \frac{\text{TP}}{\text{TP + FN}} \hspace{2cm} \text{Equal Opportunity Fairness} = \text{TPR}_\text{White} - \text{TPR}_{\text{Black}}
\end{equation}

A negative value in (\ref{eq:TPR}) is due to the worse ability of a Machine Learning model to find $+50k$ salaries for the protected group (\textit{Black}) in comparison with the reference group (\textit{White}).



\begin{table}[ht]
\centering
\begin{tabular}{c|c}
Comparison                                              & \textbf{Conclusions} \\ \hline
$P(X^{tr})$, $P(X^{te})$                                        & Equal                \\
$P(X^{tr}_{Black})$, $P(X^{te}_{Black})$                        & Equal                \\
$\LL(X^{tr})$, $\LL(X^{te})$                                    & Equal                \\
$\Ss(f_\theta,X^{tr})$, $\Ss'(f_\theta,X^{te})$                 & Equal                \\
$\Ss(f_\theta,X^{tr}_{Black})$, $\Ss'(f_\theta,X^{te}_{Black})$ & Distinct             \\
$EOF(X^{tr})$, $EOF(X^{te})$                                      & Distinct             \\
\end{tabular}
\caption{Statitistical distribution both group and general are equivalent (Komorov-Smirnov test). Group Shapley values indicate a change between train and test data that is reflected in a change of Equal Oportunity Fairness (EOF)}\label{table:fairnesDetection}
\end{table}


\subsubsection{Explainable AI error boosting: A granular approach}

In the first approach we dealt with distributions comparisons between in distribution  and out-of-distributions data. In this next approach we aim at quantifying model error at the individual instance level. For this we build a estimator using a gradient boosting decision tree $f_\theta(X_{tr},y_{tr})$ and get the explanations for the data $\mathcal{S}(f_\theta,X_{tr})$. After, we build another model $(g_\theta)$  on three different sets of data: $(i)$ normal input data $(X_{tr})$, $(ii)$ explanation values $S(f_\theta,X_{tr})$ and $(iii)$ a combination of both $\{S(f_\theta,X_{tr}),X_{tr}\}$. As the target variable we use $Y'= \texttt{error} = |Y - f_\theta(X)|$, predicting the error of the model. 

This explainable AI boosting technique can be seen as a type of traditional ensemble boosting with new aggregated features. This method even if provides a boost on the model performance is at the moment not very practical as it relies on the calculation of Shapley values that is computationally expensive, increasing the training time of the learning task.

\subsubsection{Posterior Distribution Shift}
Coming back to the previous problem of detecting posterior distribution shift, our model $f_\theta(X)$ is a random forest regressor, and the train test split is based on an equal random and uniform partition of training and test dataset with equal proportions. The metric used for this case is the mean squared error.

\begin{table}[ht]
\centering
\begin{tabular}{cc|cc}
                    & Estimator $(g_\theta)$               & \textbf{$X\_{te}$}             & \textbf{$X'\_{te}$}          \\ \hline
                    &  Dummy Mean Regressor                & 0.109                          & 0.126                        \\ \hline
$X$                 & \multirow{3}{*}{Logistic Regression} & 0.109                          & 0.126                        \\
$S(f_\theta,X)$     &                                      & 0.109                          & 0.126                        \\
$\{S(f_\theta),X\}$ &                                      & 0.109                          & 0.126                        \\ \hline
$X$                 & \multirow{3}{*}{Gradient Boosting}   & 0.213                          & 0.154                        \\
$S(f_\theta,X)$     &                                      & 0.153                          & 0.152                        \\
$\{S(f_\theta),X\}$ &                                      & \textbf{0.134}                 & \textbf{0.128}               \\ \hline
$X$                 & \multirow{3}{*}{MLP}                 & 0.125                          & 0.118                        \\
$S(f_\theta,X)$     &                                      & 0.109                          & 0.141                        \\
$\{S(f_\theta),X\}$ &                                      & \textbf{0.031}                 & \textbf{0.039}               \\ \hline
\end{tabular}
\caption{Boosting the original model $f_\theta$ with another model predicting the error. The evaluated metric in this case is mean squared error (lower is better). Improvements of providing data and explanations to a neural network help the model to generalize very well to out-of-distribution data.}
\end{table}


\subsubsection{US census data}
Making use of the US census data from the folktables package~\cite{ding2021retiring}. we aim to predict if the income of a certain individual is above a threshold (binary decision), we train a classifier ($f_\theta$) using data from the state of California and then evaluate it on data from Michigan state. The result of train are extracted out of 5-fold crossvalidation. 
\begin{table}[ht]
\centering
\begin{tabular}{cc|cc}
                    & Estimator $(g_\theta)$               & \textbf{Train\_{California}} & \textbf{Test\_{Michigan}} \\ \hline
$X$                 & \multirow{3}{*}{Logistic Regression} & 0.618                          & 0.653                        \\
$S(f_\theta,X)$     &                                      & 0.607                          & 0.677                        \\
$\{S(f_\theta),X\}$ &                                      & \textbf{0.629}                 & \textbf{0.681}               \\ \hline
$X$                 & \multirow{3}{*}{Random Forest}       & 0.748                          & 0.723                        \\
$S(f_\theta,X)$     &                                      & 0.759                          & 0.745                        \\
$\{S(f_\theta),X\}$ &                                      & \textbf{0.765}                 & \textbf{0.750}               \\ \hline
$X$                 & \multirow{3}{*}{Gradient Boosting}   & 0.767                          & 0.757                        \\
$S(f_\theta,X)$     &                                      & 0.766                          & 0.756                        \\
$\{S(f_\theta),X\}$ &                                      & 0.768                         & 0.756                       \\ \hline
$X$                 & \multirow{3}{*}{MLP}                 & 0.558                          & 0.580                        \\
$S(f_\theta,X)$     &                                      & \textbf{0.781}                 & 0.776                        \\
$\{S(f_\theta),X\}$ &                                      & 0.623                          & \textbf{0.787}               \\ \hline
\end{tabular}
\caption{Boosting the original model $f_\theta$ with another model predicting the error, the used metric is AUC (higher is better). The combination of both input data and explanations data achieves better score in unseen iid data and ood data.}
\end{table}


%Group fairness is based on calculating performance metrics in a desired dataset. A common situation in real life tasks is the absence of the deployment label. 
%Deploying an underperforming machine learning model is a bad business decision. Deploying a highly discriminative model is unethical and possibly illegal.
%\todo{@Thanassis: I suggest you give some idea on how you go about this first; e.g. "using a number of theoretical examples and experimentation on US census data we demonstrate that change in the input data distribution alone..."}.
%Data is not static, it evolves and so does the explanation. While changes on the explanations indicate a change of the behaviour of the model, changes on input distributions are independent of model performance or behaviour. In this work we monitor changes on the explanation as a way to improve  meaningful distribution shift detection, model degradation quantification and monitoring group fairness. 

%Monitoring explanations can be particularly helpful for monitoring changes on the model performance and dealing with fairness. Detecting group fairness changes in unlabelled data scenarios is especially challenging since it is not possible calculate evaluation metrics and group fairness work relies on calculating them. In this work we advocate for monitoring explanations as a better indicative than monitoring input data, improving in distribution shift detection and model degradation quantification. Furthermore we provide an approach that allows to monitor changes that have an impact on group fairness.
%In this work, we motivate the problem of detecting fairness changes in unlabeled data scenarios. Furthermore, we tackle the problem with the use of explainable AI as a method to understand variation in the model behavior that imply fairness changes.

%\steffen{How does our work compare to recent work on test-time training, e.g. Test-Time Training with Self-Supervision for Generalization under Distribution Shifts Yu Sun, Xiaolong Wang, Zhuang Liu, John Miller, Alexei Efros, Moritz Hardt Proceedings of the 37th International Conference on Machine Learning, PMLR 119:9229-9248, 2020. \url{http://proceedings.mlr.press/v119/sun20b.html}.  The specific paper is not a really competitor, however, other test-time training approaches might be.}\carlos{I have added in the related work section}

% See if one of the properties of Shapley values can be used
% Compare with multivariate testing
% Consider doing a figure to explain the model degradation quantification

% Intro to model Monitoring
%
%'\textit{Everything changes and nothing stands still}' \footnote{{\textgreekfont Tά Πάντα ῥεῖ}} – Heraclitus of Ephesus, 500 B.C.E. ~\cite{wheelwright1974heraclitus,sedley2003plato}. The world is constantly changing, and so does the data. 
%\steffen{I am not a fan of adding unnecessary bells and whistle to a story line that needs to be concise and convincing.}

\steffen{Let's discuss which terminology to use: performance vs. quality. vs. key performance indicators vs. predictive quality etc.} - 

%Forecasting the quality of models on out-of-distribution data, however, remains an unsolved challenge.
%Detecting the impact of data changes on already deployed Machine Learning (ML) models in data-driven AI systems is a challenging task. The ML paradigm allows us to evaluate the predictive performance of models on unseen data coming from the same distribution as the training data. However, the performance of the models on out-of-distribution data remains unknown. 

 \begin{gather}
     \frac{\sum_i \mathrm{NE}(f(x_i)) > \mathrm{Threshold}}{\mathrm{Total Number of samples}} = \mathrm{Accuracy}\\
     f(x) = \Ss(f,X) + E[f(X)]\\
     \frac{\sum_i \mathrm{NE}(\Ss(f,X)) > \mathrm{Threshold}}{\mathrm{Total Number of samples}} = \mathrm{Accuracy}
 \end{gather}
 
%Methods to detect changes in the distribution either on input data or on the explanations are designed to detect if distributions differ. However, they can fail to detect or quantify the change in the model performance since they are not explicitly designed for it. Theoretically, without any further assumptions, distribution shifts can cause degradation arbitrarily in model performance; quantifying this impact a priori is an extremely challenging task.




%\gjergji{Here, we are quite general and not specific about the explanation technique. This is fine, but should be supported by experiments.}

% http://proceedings.mlr.press/v28/zhang13d.pdf
% https://mingming-gong.github.io/papers/AAAI_MULTI.pdf

%%% Are Labels Always Necessary for Classifier Accuracy Evaluation? - https://arxiv.org/pdf/2007.02915.pdf
% https://arxiv.org/pdf/2011.03156.pdf
% https://arxiv.org/pdf/1810.11953.pdf

%% Review more on NIPS 2021 and NIPS 2020
%\steffen{The following section is not about `quantifying model degradation'. It is about experiments on that. What I also do not get is why A.2.1 is at the same level as A.2.2. A.2.1 should be at a different level than A.2.2 But A.2.2 should be at the same level as A.3.Let's assume A.2.1 becomes a standalone section A.2 and a new section A.3 presents all the different experiments on the different datasets in A.3.1 to A.3.n, then in A3(.0) one could make generic statements that are the same for all experiments. This would be way better readable and also easier to refer to from the main text.}\steffen{The structure of the appendix does not mirror what is told. Assuming that the reader might not read the appendix but only skip it, the appendix needs to have a clean structure that the reader can easily dip into what he is interested in.}

\section{Experiments on the distance used for quantifying model degradation}
\subsection{Experiments on the distance used for quantifying model degradation on synthetic data}
Coming back to the previous problem of detecting multivariate distribution shift (cf. section \ref{subsec:explanationShiftMethods}), our model $f_\theta (X)$ is a gradient boosting decision tree~\cite{xgboost,catboost}, and the train test split is based on an equally random and uniform partition of training and test dataset with equal proportions. The out-of-distribution data is $X^{ood}$ (cf. section~\ref{sec:experiments}). 

The input data can either be a distribution shift, explanation shift, or a combination of both. The loss used for this case is the mean absolute error between the training performance and the performance of $g_\theta$ on each $X_b$,  $MAE( MSE(f_\theta(X^{tr}),y^{tr}) , MSE(f_\theta(X^{b}),y^b) )$. As estimators we use a Linear Regression, using $B=2,000$ bootstraps of $1,000$ samples.


\begin{table}[ht]
\centering
\caption{Mean Absolute Error of model $g_\theta$. Results are the average error between the results on $X_{te}$ data and the subsample $X_b \in X_{ood}$ with different input data for the synthetic multivariate distribution shift problem (cf. Section \ref{subsec:explanationShiftMethods})}\label{table:quantificationSynthetic} %\steffen{where is the discussion which distance to use and why?}
\begin{tabular}{cc|c}
Input data          & Distribution comparison              & MAE                  \\ \hline
                    & Dummy Mean Regressor                 & 0.262                \\ \hline
Distribution Shit   & \multirow{3}{*}{Wasserstein}         & 0.262                \\
Explanation Shift   &                                      & \textbf{0.099}       \\
Exp+Dist Shift      &                                      & \textbf{0.099}       \\ \hline
Distribution Shit   & \multirow{3}{*}{Kolmogorov-Smirnov}  & 0.262                \\
Explanation Shift   &                                      & 0.257                \\
Exp+Dist Shift      &                                      & 0.257                \\ \hline
Distribution Shit   & \multirow{3}{*}{Population Stability}& 0.259                \\
Explanation Shift   &                                      & 0.242                \\
Exp+Dist Shift      &                                      & 0.240                 \\ \hline
\end{tabular}
\end{table}

In Table~\ref{table:quantificationSynthetic},  we can see  different statistical differences on both distribution shift and explanation shift. Explanations shift provide a consistent improvement over distribution differences in the input data. The best distance metric for this use case is the Wasserstein distance. 



\subsection{Experiments on the distance used for quantifying model degradation on the US income dataset}

Coming back to the data used on the experimental section of the paper (cf. Section \ref{exp:quantifying}), we use the learning approach introduced in Section \ref{subsec:explanationShiftMethods} on the US census Income data by training a classifier,$f_\theta$, using data from the state of California in 2014, learning statistical differences on bootstrap samples from  California state in 2015, and evaluating the method on the rest of US states in 2018. For simplicity the model degradation quantification estimator is a linear regression and we compare three different statistical differences: Wasserstein distance, Kolgomorov-Smirnov and Population Stability Index.

\begin{table}[ht]
\centering
\caption{Degradation estimation of predictive performance on the US census data with different statistical distance metrics (lower is better). Results are the average over all US states. A low value error indicates a well model degradation prediction on out-of-distribution data.}\label{table:quantificationDistances}
\begin{tabular}{ll|c}
Distance         & Input               & Predictive Performance  \\ \hline
Mean Baseline     &                     & $0.023\pm0.0003$        \\ \hline
Wasserstein         & Distribution Shift  & $0.026\pm0.009$         \\
Wasserstein         & Explanation Shift   & $0.018\pm0.002$         \\
Wasserstein         & Exp + Dist Shift    & $0.018\pm0.008$         \\ \hline
Kolmogorov-Smirnov  & Distribution Shift  & $0.041\pm0.003$        \\
Kolmogorov-Smirnov  & Explanation Shift   & $0.018\pm0.002$        \\
Kolmogorov-Smirnov  & Exp + Dist Shift    & $0.038\pm0.003$        \\ \hline
Population Stability& Distribution Shift  & $0.035\pm0.006$      \\
Population Stability& Explanation Shift   & $0.014\pm0.004$      \\
Population Stability& Exp + Dist Shift    &$0.027\pm0.004$
\end{tabular}
\end{table}

In tables \ref{table:quantificationSynthetic} and \ref{table:quantificationDistances} we have used a linear model $g_\theta$ and different measures for statistical differences between distribution shift $d(X^{tr},X^{ood})$ or explanation shift $d(\mathcal{S}(f_\theta,X^{tr}),\mathcal{S}(f_\theta,X^{ood})$. In this work, we have selected the Wasserstein distance as is the one that achieves best average result with synthetic data and in the US census income data. 


\section{Experiments based on the US Census dataset}
%% EMPLOYMENT DATA
%%%%%%%%%%
\subsection{Employment data: Detecting and quantifying model degradation}
The goal of this dataset from the folktables package \cite{ding2021retiring} is to predict whether an individual is employed including individuals between the ages of 16 and 90. Following the same methodology as in cf. Section \ref{exp:quantifying}. We train a gradient boosting decision tree classifier $f_\theta$ using data from California in 2014 and then evaluate it on data from other states in 2018.

\begin{table}[ht]
\centering
\begin{tabular}{l|c|c|c|c}
\textbf{Data}         & \textbf{AUC} & \textbf{EOF} & Distribution Shifts& Explanation Shifts \\ \hline
California ($X^{te}$) & 0.810        & 0.020        & 0      & 0       \\
Michigan              & 0.824        & 0.023        & 14     & 15      \\
Kansas                & 0.780        & 0.070        & 14     & 14      \\
New York              & 0.822        & 0.017        & 14     & 10      \\
Hawaii                & 0.820        & -0.021       & 11     & 15      \\
Puerto Rico           & 0.757        & 0.003        & 16     & 15     
\end{tabular}
\caption{Model performance and equal opportunity fairness metrics across the US employment dataset, using a California-2014 hold-out set as test data and the rest of states in 2018 as out-of-distribution data. Distribution shift, represents the number of distinct input features (based on a one-tailed p-value (0.1 threshold) of a Kolmogorov-Smirnov test, and Explanation shifts, the number of different SHAP features }\label{table:employment}
\end{table}

\begin{table}[ht]
\centering
\begin{tabular}{ll|c|cc}

Estimator & Input              & Predictive Performance  & \multicolumn{2}{c}{Fairness}  \\ \hline
          &                    & Overall                 & $TPR_{white}$             & $TPR_{black}$            \\ \hline
Mean Baseline     &            & $0.043\pm0.005$         & $0.089\pm0.003$          & $0.079\pm0.005$  \\ \hline
Linear    & Distribution Shift & $0.059\pm0.005$         & $0.128\pm0.004$          & $0.030\pm0.005$           \\
Linear    & Explanation Shift  & $0.031\pm0.005$         & \textbf{0.030}$\pm0.002$ &$0.094\pm0.007$            \\
Linear    & Exp + Dist Shift   & $0.040\pm0.005$         & $0.074\pm0.003$          &\textbf{0.028}$\pm0.004$   \\ \hline
XGBoost   & Distribution Shift & $0.049\pm0.003$         & $0.075\pm0.003$          &$0.068\pm0.005$            \\
XGBoost   & Explanation Shift  & $0.042\pm0.004$         & $0.071\pm0.003$          & $0.040\pm0.004$           \\
XGBoost   & Exp + Dist Shift   & \textbf{0.027}$\pm0.004$& $0.072\pm0.003$          &$0.038\pm0.005$       
\end{tabular}
\caption{Predictive performance and fairness metrics of model $g_\theta$ on the US employment data (lower is better).Adding distribution changes on the explanation space provides better results in the three evaluated performance indicators}\label{table:quantificationEmployment}
\end{table}

In table \ref{table:employment}, for some states in 2018 the performance is sometimes even better than in trainning data. Implying that not all of the distribution or explanation shift are malign. In table \ref{table:quantificationEmployment} the linear model trained with explanation shift differences improves both the baseline and the linear model with distribution shift.

%%%%%%
%% TRAVEL TIME
%%%%%%%%
\subsection{Travel Time: Detecting and quantifying model degradation}

The goal of this dataset from the folktables package \cite{ding2021retiring} predict whether an individual has a commute to work that is longer than 20 minutes. 

In table \ref{table:travelTime}, for some states in 2018 the equal opportunity fairness is sometimes even better than in training data. Implying that not all of the distribution or explanation shift are malign also for fairness. In table \ref{table:quantificationTravelTime} the linear model trained with explanation shift differences improves both the baseline and the linear model with distribution shift, except when monitoring the true positive rate of the protected group.

\begin{table}[ht]
\centering
\begin{tabular}{l|c|c|c|c}
\textbf{Data}         & \textbf{AUC} & \textbf{EOF} & Distribution Shifts& Explanation Shifts \\ \hline
California ($X^{te}$) & 0.676        & -0.138        & 0      & 0       \\
Michigan              & 0.597        & -0.009        & 15     & 13      \\
Kansas                & 0.640        & 0.003         & 12     & 13     \\
New York              & 0.753        & -0.324        & 14     & 19      \\
Hawaii                & 0.626        & -0.044        & 13     & 12      \\
Puerto Rico           & 0.596        & -0.025        & 14     & 13     
\end{tabular}
\caption{Model performance and equal opportunity fairness metrics across the US travel time dataset, using a California-2014 hold-out set as test data and the rest of states in 2018 as out-of-distribution data.}\label{table:travelTime}
\end{table}


\begin{table}[ht]
\centering
\begin{tabular}{ll|c|cc}

Estimator & Input               & Predictive Performance  & \multicolumn{2}{c}{Fairness}  \\ \hline
          &                     & Overall                 & $TPR_{white}$            & $TPR_{black}$             \\ \hline
Mean Baseline     &             & $0.016\pm0.003$         & $0.017\pm0.002$          & \textbf{0.054}$\pm0.001$  \\ \hline
Linear    & Distribution Shift  & $0.015\pm0.002$         & $0.019\pm0.003$          & $0.059\pm0.001$           \\
Linear    & Prediction Shift    & $0.016\pm0.003$         & $0.017\pm0.002$          & $0.056\pm0.001$           \\
Linear    & Explanation Shift   & \textbf{0.014}$\pm0.003$& \textbf{0.014}$\pm0.001$ & $0.103\pm0.001$           \\
Linear    & Exp + Dist Shift    & $0.027\pm0.003$         & $0.019\pm0.003$          & $0.754\pm0.006$           \\ \hline
XGBoost   & Distribution Shift  & $0.024\pm0.004$         & $0.018\pm0.002$          & $0.074\pm0.004$           \\
XGBoost   & Prediction Shift    & $0.017\pm0.004$         & $0.017\pm0.002$          & $0.057\pm0.004$           \\
XGBoost   & Explanation Shift   & $0.052\pm0.004$         & $0.019\pm0.003$          & $0.073\pm0.004$           \\
XGBoost   & Exp + Dist Shift    & $0.039\pm0.004$         & $0.016\pm0.002$          & $0.066\pm0.002$       
\end{tabular}
\caption{Predictive performance and fairness metrics of model $g_\theta$ on the US travel time dataset (lower is better). A low value error indicates a well model degradation prediction. The linea model with explanation shift provides the best results. For the True Positive Rate degradation of the protected group none of the models are able to improve the baseline}\label{table:quantificationTravelTime}
\end{table}



%%%%%%%%%%
%% MOBILITY
%%%%%%%%%
\subsection{Mobility: Detecting and quantifying model degradation}
The goal of this dataset from the folktables package \cite{ding2021retiring} is to predict whether an individual had the same residential address one year ago, after filtering the ACS PUMS data sample to only include individuals between the ages of 18 and 35. This filtering increases the difficulty of the prediction task, as the base rate of staying at the same address is above $90\%$ for the general population.

Following the same methodology as in section \ref{exp:quantifying}, we train a gradient boosting decision tree classifier $f_\theta$ using data from California in 2014 and then evaluate it on data from other states in 2018. For the quantification of model decay, we train on the statistical differences with California in 2015 and evaluate it in the rest of US states in 2018.

\begin{table}[ht]
\centering
\begin{tabular}{l|c|c|c|c}
\textbf{Data}         & \textbf{AUC} & \textbf{EOF} & Distribution Shifts& Explanation Shifts \\ \hline
California ($X^{te}$) & 0.706        & 0.008        & 0      & 0       \\
Michigan              & 0.686        & 0.021        & 16     & 20      \\
Kansas                & 0.673        & 0.073        & 13     & 20     \\
New York              & 0.668        & -0.013        & 15     & 19      \\
Hawaii                & 0.634        & -0.108       & 12     & 20      \\
Puerto Rico           & 0.699        & 0.010        & 15     & 20     
\end{tabular}
\caption{Model performance and equal opportunity fairness metrics across the US mobility dataset, using a California-2014 hold-out set as test data and the rest of states in 2018 as out-of-distribution data.}\label{table:mobility}
\end{table}


\begin{table}[ht]
\centering
\begin{tabular}{ll|c|cc}

Estimator & Input              & Predictive Performance & \multicolumn{2}{c}{Fairness}  \\ \hline
          &                    & Overall                & $TPR_{white}$             & $TPR_{black}$            \\ \hline
Mean Baseline     &            & $0.023\pm0.0003$        & $0.043\pm0.004$          & \textbf{0.085}$\pm0.001$  \\ \hline
Linear    & Distribution Shift & $0.026\pm0.009$         & $0.052\pm0.005$          & $0.131\pm0.002$           \\
Linear    & Explanation Shift  & $0.018\pm0.002$         & \textbf{0.029}$\pm0.004$ &$0.121\pm0.002$            \\
Linear    & Exp + Dist Shift   & $0.018\pm0.008$         & $0.033\pm0.004$          & $0.094\pm0.002$           \\ \hline
XGBoost   & Distribution Shift & $0.019\pm0.006$         & $0.048\pm0.005$          &$0.096\pm0.002$            \\
XGBoost   & Explanation Shift  & $0.017\pm0.004$         & $0.036\pm0.004$          & $0.092\pm0.001$           \\
XGBoost   & Exp + Dist Shift   & \textbf{0.016}$\pm0.004$& $0.031\pm0.004$          &$0.109\pm0.002$       
\end{tabular}
\caption{Predictive performance and fairness metrics of model $g_\psi$ on the US employment data (lower is better).}\label{table:quantificationMobility}
\end{table}


\newpage







%$\Dd{tr}=\{(x_0^{tr},y_0^{tr})\ldots, (x_n^{tr},y_n^{tr})\} \subseteq \mathcal{X} \times \mathcal{Y}$ where $\mathcal{X} \times \mathcal{Y}$ denote the domain of predictors $X$ and target $Y$ respectively. Unsupervised model monitoring refers to having and indicator of $X^{te}$ in absence of $Y^{te}$
\begin{comment}
\begin{gather}
X^{tr},Y^{tr} \rightarrow f_\theta(X^{tr},Y^{tr})\\
f_\theta \quad \mathrm{performance} \rightarrow X^{te}??
\end{gather}
\[
P(X^{tr})\neq P(X^{te})
\]
\[
P(Y^{tr})\neq P(Y^{te})
\]
\[
P(X^{tr}|Y^{tr}) \neq  P(X^{te}|Y^{te})
\]
\begin{gather}
\mathcal{S}( \textcolor{red}{f_\theta},X) \neq \mathcal{S}( \textcolor{red}{f_{\theta'}},X)\\
\mathcal{S}( f_\theta,\textcolor{red}{X}) \neq \mathcal{S}(f_{\theta},\textcolor{red}{X'})\\
\end{gather}
\newpage
%%%%%%%
%% PUBLIC COVERAGE
%%%%%%%
\subsection{Public Coverage}

\begin{table}[ht]
\centering
\begin{tabular}{l|c|c|c|c}
\textbf{Data}         & \textbf{AUC} & \textbf{EOF} & $\#$ X & $\# \Ss(f_\theta,X)$ \\ \hline
California ($X^{te}$) & 0.757        & -0.205        & 0      & 0       \\
Michigan              & 0.788        & -0.116        & 16     & 18      \\
Kansas                & 0.688        & -0.194         & 13     & 16     \\
New York              & 0.757        & -0.095        & 14     & 18      \\
Hawaii                & 0.775        & -0.326        & 11     & 16      \\
Puerto Rico           & 0.674        & -0.024        & 17     & 18     
\end{tabular}
\caption{Model performance and equal opportunity fairness metrics across the US travel time dataset, using a California-2014 hold-out set as test data and the rest of states in 2018 as out-of-distribution data. $\# X$, represents the number of distinct input features (based on a one-tailed p-value (0.1 threshold) of a Kolmogorov-Smirnov test, and $\# \Ss(f_\theta,X)$, the number of different features in the explanation space. Most of the features are flagged as distinct in input and explanation space, even though in same cases the drop on performance and equal opportunity fairness is minimal.}\label{table:publiccoverage}
\end{table}

\begin{table}[ht]
\centering
\begin{tabular}{ll|c|cc}

Estimator & Input       & Predictive Performance & \multicolumn{2}{c}{Fairness}  \\ \hline
          &             & Overall                & $TPR_{white}$             & $TPR_{black}$            \\ \hline
Mean Baseline     &     & $0.0\pm0.00$         & $0.060\pm0.008$          & \textbf{0.10}$\pm0.001$  \\ \hline
Linear    & Data        & $0.0\pm0.00$         & \textbf{0.042}$\pm0.008$ & \textbf{0.10}$\pm0.001$  \\
Linear    & Shap        & $0.0\pm0.00$         & $0.065\pm0.01$           & $0.12\pm0.001$            \\
Linear    & Shap + Data & $0.0\pm0.00$         & $0.047\pm0.01$           & $0.12\pm0.001$           \\ \hline
XGBoost   & Data        & $0.0\pm0.00$         & $0.052\pm0.008$          & $0.12\pm0.002$            \\
XGBoost   & Shap        & $0.0\pm0.00$         & $0.044\pm0.007$          & $0.14\pm0.002$           \\
XGBoost   & Shap + Data & $00\pm0.00$          & $0.046\pm0.008$          & $0.18\pm0.002$       
\end{tabular}
\caption{\carlos{Yet to fill the results}Predictive performance and fairness metrics of model $g_\theta$ on the US public coverage (lower is better). Results are the average out-of-distribution error between the model performance on $X^{te}$ and the performance on the subsample $X^b \in X^{ood}$ with different input data over the US. A low value error indicates a well model degradation prediction on  $X^b$ }\label{table:quantificationPublicCoverage}
\end{table}

\begin{sidewaystable}[ht] 
\centering
\begin{tabular}{l|l|lllll}
        & Input       & AA     & AK     & HI     & PR     & VI     \\ \hline
Dummy   &             & 0.0244 & 0.0243 & 0.0244 & 0.0241 & 0.0232 \\
Linear  & Data        & 0.0171 & 0.0391 & 0.0356 & 0.0131 & 0.0252 \\
Linear  & Shap        & 0.0183 & 0.022  & 0.0167 & 0.0251 & 0.0163 \\
Linear  & Shap + Data & 0.0104 & 0.0309 & 0.022  & 0.014  & 0.0193 \\
XGBoost & Data        & 0.0292 & 0.0185 & 0.0339 & 0.0181 & 0.0122 \\
XGBoost & Shap        & 0.019  & 0.0136 & 0.0148 & 0.0201 & 0.0157 \\
XGBoost & Shap + Data & 0.0334 & 0.0108 & 0.018  & 0.0219 & 0.011 
\end{tabular} \caption{Performance}
\end{sidewaystable}

 
\begin{sidewaystable}[ht] 
\centering
\begin{tabular}{l|l|lllll}
        & Input       & AA     & AK     & HI     & PR     & VI     \\ \hline
Dummy   &             & 0.0244 & 0.0243 & 0.0244 & 0.0241 & 0.0232 \\
Linear  & Data        & 0.0171 & 0.0391 & 0.0356 & 0.0131 & 0.0252 \\
Linear  & Shap        & 0.0183 & 0.022  & 0.0167 & 0.0251 & 0.0163 \\
Linear  & Shap + Data & 0.0104 & 0.0309 & 0.022  & 0.014  & 0.0193 \\
XGBoost & Data        & 0.0292 & 0.0185 & 0.0339 & 0.0181 & 0.0122 \\
XGBoost & Shap        & 0.019  & 0.0136 & 0.0148 & 0.0201 & 0.0157 \\
XGBoost & Shap + Data & 0.0334 & 0.0108 & 0.018  & 0.0219 & 0.011 
\end{tabular} \caption{Fairness}
\end{sidewaystable}

\section{Shapley Demonstration}
For the case of linear regression with independent variables, the Shapley values can be computed precisely.

\subsection{Posterior distribution shift}
We create a bi-variate normal distributions $X = (X_1,X_2) \sim N(\mu,\sigma^2\cdot I_2)\sim (X^{ood}_1,X^{ood}_2)$, where $I_2$ is an identity matrix of order two. We now create two synthetic targets $Y=a + \alpha X_1 + \beta X_2 + \epsilon $ and $Y^{ood}=Y=a + \beta X_1 + \alpha X_2 + \epsilon$ and fit two linear regressions $f_\theta(X,Y)$ and $g_\theta(X,Y^{ood})$. 


The interventional SHAP calculate the Shapley values as $\Ss(f_\theta,x) = [x_i - \bar{X}]\times [a,b] = \beta_i(x_i - \mu_i)=\beta_i(x_i - \bar{X}_i)$. The interventional SHAP value assumes there is no interdependency between features.


Computing the Shapley values for the \textbf{correlation dependent or conditional observational expectations} is substantially more complex. 
We have that $P(Y)= P(Y^{ood}), P(X)= P(X^{ood})$.

$Y\sim a + \alpha N(\mu,\sigma^2) + \beta  N(\mu,\sigma^2) + N(0,\sigma^2) $ and $Y\sim a + \beta N(\mu,\sigma^2) + \alpha N(\mu,\sigma^2) + N(0,\sigma^2) $

Since the features are independent we can calculate the  interventional conditional expectation Shapley values $\Ss(f_\theta, x)) = \beta_i(x_i-\mu_i)$ \cite{DBLP:journals/corr/ShapTrueModelTrueData}. The coefficients for the first model $\beta^{f_\theta}_1=2,\beta^{f_\theta}_2=1$ and for the second, $\beta^{g_\theta}_1=1,\beta^{g_\theta}_2=2$, thus $\Ss(f_\theta,X)\neq \Ss(g_\theta,X)$ while $P(X^{tr})=P(X^{ood})$, and $P(Y^{tr})=P(Y^{ood})$.
\end{comment}

\begin{table*}
\centering
\begin{tabular}{ll|c|cc}
Estimator & Input       & Predictive Performance & Equal Opportunity Fairness  \\ \hline
\multicolumn{4}{c}{Mobility Dataset} \\ \hline
Linear            & Prediction Shift    & $0.57\pm0.07$           & $0.58\pm0.06$\\
Linear            & Distribution Shift  & $0.60\pm0.05$           & \textbf{0.90}$\pm0.05$\\
Linear            & Explanation Shift   & \textbf{0.62}$\pm0.03$  & \textbf{0.90}$\pm0.02$\\
Linear            & ATC                 & $0.51\pm0.01$           & -              \\ \hline
Gradient Boosting & Prediction Shift    & \textbf{0.76}$\pm0.09$  & $0.50\pm0.02$\\
Gradient Boosting & Distribution Shift  & $0.54\pm0.16$           & $0.58\pm0.06$\\
Gradient Boosting & Explanation Shift   & $0.67\pm0.14$           & \textbf{0.64}$\pm0.02$\\
Gradient Boosting & ATC                 & $0.50\pm0.02$           & -              \\ \hline
\multicolumn{4}{c}{Income Dataset} \\ \hline
Linear            & Prediction Shift    & \textbf{0.64}$\pm0.07$  & $0.52\pm0.02$\\
Linear            & Distribution Shift  & \textbf{0.64}$\pm0.07$  & \textbf{0.78}$\pm0.02$\\
Linear            & Explanation Shift   & \textbf{0.64}$\pm0.07$  & 0.77$\pm0.02$\\
Linear            & ATC                 & $0.56\pm0.03$           & -              \\ \hline
Gradient Boosting & Prediction Shift    & $0.58\pm0.06$           & $0.50\pm0.02$\\
Gradient Boosting & Distribution Shift  & $0.62\pm0.05$           & $0.58\pm0.06$\\
Gradient Boosting & Explanation Shift   & \textbf{0.64}$\pm0.04$  & \textbf{0.64}$\pm0.02$\\
Gradient Boosting & ATC                 & $0.53\pm0.03$           & -              \\ \hline
\multicolumn{4}{c}{Employment Dataset} \\ \hline
Linear            & Prediction Shift    & $0.53\pm0.03$           & $0.56\pm0.03$\\
Linear            & Distribution Shift  & \textbf{0.61}$\pm0.02$  & \textbf{0.77}$\pm0.04$\\
Linear            & Explanation Shift   & \textbf{0.61}$\pm0.02$  & $0.71\pm0.02$\\
Linear            & ATC                 & $0.51\pm0.02$           & -              \\ \hline
Gradient Boosting & Prediction Shift    & $0.53\pm0.04$           & $0.57\pm0.02$\\
Gradient Boosting & Distribution Shift  & $0.61\pm0.04$           & $0.64\pm0.06$\\
Gradient Boosting & Explanation Shift   & \textbf{0.63}$\pm0.03$  & \textbf{0.70}$\pm0.04$\\
Gradient Boosting & ATC                 & $0.52\pm0.05$           & -              \\ \hline
\end{tabular}

\end{table*}




\begin{comment}
\steffen{There are too many details for defining fairness in this introduction. I would put these details into the related work section and aim at a definition of fairness that consumes only one third of its current length. This simplification makes it easier for the reader to understand the narrative and the details are not lost, but in the related work section.}
In efforts to mitigate these concerns, researchers have proposed diverse metrics aiming to quantify deviations from various statistical parities that we might expect to observe in a fair world and offered a variety of algorithms in attempts to improve these parities ~\cite{wachter2020bias,zafar2017parity,DBLP:conf/www/ZafarVGG17}. The notion of group fairness aims to establish some form of parity between groups of individuals based on protected or sensitive attributes like gender or race. Various forms of parity between groups have been proposed in the literature ~\cite{wachter2020bias}: statistical parity ~\cite{DBLP:conf/kdd/Corbett-DaviesP17}, equal opportunity ~\cite{DBLP:conf/nips/HardtPNS16}, and equal misclassification rates ~\cite{DBLP:conf/www/ZafarVGG17}. The research community has made an enormous effort to design algorithmic methods to improve this for fairness and discrimination metrics. Various publications reconcile the information of relevant papers, create taxonomies, define future opportunities and current gaps~\cite{barocas-hardt-narayanan,DBLP:journals/widm/NtoutsiFGINVRTP20,DBLP:journals/corr/Aequitas}. All this body of methodological work on group fairness accountability heavily relies on the assumption that is possible to evaluate the model predictions. 


Detecting changes in the distribution in absence of labeled data is a challenging question both in theory and practice ~\cite{DBLP:conf/aaai/RamdasRPSW15,DBLP:conf/nips/RabanserGL19}.  Traditionally some of the simplest and most scalable approaches are based on statistical distances between source and test distributions, such as the Population Stability Index (PSI), or the Kolmogorov-Smirnov statistic~\cite{continual_learning,clouderaff}. These statistical tests correctly detect univariate changes in the distribution but can be independent of the model performance and can therefore be too sensitive, indicating a change in the covariates but without any degradation in the model performance.
\steffen{The following sentence is not clearly delineating}
Another strategy is to build an indicator that behaves similar to the model performance ~\cite{mougan2022monitoring,garg2022leveraging}. Overall changes in model performance do not necessarily imply changes in the fairness metrics. 
\steffen{here the message gets blurred: what is the paper supposed to achieve? monitor fairness? predict change of model accuracy? Both? These objectives need to be established firmly in the introduciton before such statements can be made}

% Relating unsupervised model monitoring to fairness
From a fairness perspective, the problem relies on detecting distribution changes that have a fairness impact. Even if a model has achieved fair results in train data it can exhibit discriminant treatment on out-of-distribution data. The risk of these silent machine learning failures relies on the introduction of a bias or discrimination in the production system . 


% xAI
In recent years, the field of explainable AI has emerged as a way to understand model decisions ~\cite{molnar2019} and interpret the inner works of the so-called black box models\cite{guidotti_survey}. Recent advances in explainable AI have developed new algorithmic procedures that enable to account for the outputs of a machine learning model beyond performance metrics calculation ~\cite{doshivelez2017rigorous}. According to the literature, papers in explainble AI methods might have been suffering from two issues: $(i)$ a lack of a rigorous definition and evaluation methods for explainability~\cite{doshivelez2017rigorous,mythos_interpretability} and $(ii)$ explainability methods are introduced as general-purpose solutions and do not directly address real use cases or a specific user audience~\cite{desiderataECB,explainabe_public_policy,bewareInmates}. In this work, we redefine the usage of explainability techniques aiming to detect and quantify changes in the explanations that impact model performance and impact fairness in unlabeled data cases when is non-viable to calculate group performance metrics.
%\steffen{We have a terminological issue here. In model development one uses training data, validation data and test data. Once the model is deployed, and unseen data arrives we should not call this unseen data `test data' again, because this will be confused with test data as used to judge the quality of the model during development. I would call the unseen data `application data'. `unseen data' would also work. In the following sentence, this terminological unclarity arises: `test distributions'.}
%and most scalable approaches \steffen{why is the emphasis here on `scalable'? Are there approaches that show good quality, but do not scale well? Maybe then one could make these approaches scalable? Or does this really refer to quality? I hit on this because SHAP values are not that scalable either. Indeed the method we use only approximates shap values, which are not scalable by themselves.}
%Statistical tests on the
%\steffen{`input data' is yet another term. Creating new terms hurts the understanding of the reader and is to be avoided. Reuse terms that were introduced before.}
%input data can detect changes in the distributions but can be unrelated to the model performance; also,  differences in the prediction space %might not have enough information. 

%Recent developments in ML have brought to attention further KPIs such as explanation quality and fairness (according to pre-specified definitions). 
%This work studies how changes in the explanation space, i.e., explanation shifts, are better indicators for quality of deployed ML models, both wrt.\ quality of prediction and fairness, than approaches based on distribution or target shifts. 
% Unsupervised model monitoring

%\steffen{As always, I am very negative about creating an artificial chasm between research and practice. At least here I find it unwarranted. Every ML person wants to deploy models and predict when they are not good enough anymore. This is not specific to practitioners.}


\end{comment}
\begin{comment}
\subsection{Monitoring the explanation space towards fairness changes detection}\label{sec:methodology.fairness}

A simple way to see if there is disparate treatment of the model behavior between two protected groups is seen if the protected attribute is used at the prediction stage. Using Shapley values, we can detect differences in the disparate treatment of protected groups.
 
If the protected attribute, $A$, is encoded \cite{FairEncoder}, present within the training data, a practice that may be necessary for avoiding discrimination in data-driven decision models \cite{RomeiR2014_DiscSurvey,DBLP:journals/ail/ZliobaiteC16} and features are independent and identically distributed. By the \enquote{Dummy} property of the Shapley values, if the feature does not contribute to the prediction, then, the Shapley value is zero $\Ss_A(f_\theta,X)=0$. If this happens, the predicted values of the model are the same independently of whether an element belongs to a group of the protected attribute $f_\theta \perp A$, achieving demographic parity $P(f_\theta(X)|A) = P(f_\theta(X))$.

On the contrary, if the Shapley value of the protected attribute is $\Ss_A(f_\theta,X)\neq 0 $, demographic parity does not hold anymore. Monitoring changes in the Shapley explanations can provide information about fairness violations. 

In the absence of the protected attribute on the training data, non iid features, the problem is not as trivial. In this work for simplicity, we consider that the protected attribute is within the training data.
\end{comment} 

\begin{comment}
\subsubsection{Detecting fairness changes on synthetic data}\label{sec:experiment.synthetic.fairness}
%\carlos{change this. call y_}
To have an case example of how the SHAP values of the protected attributes help to detect fairness changes, we create a protected attribute  $A\in \{0,1\}$ and sample two normal distributions $X_1 = X_{\{1,A\}}$,$X_2 = X_{\{2,A\}}$ where $(X_1, X_2) \sim N(0,1)$ and we then create the target $Y = Y_A$ and then generate the  target by $Y_1 = 1 \quad\mathrm{if}\quad X_1 + X_2 > 0 \quad\mathrm{else}\quad 0$, $Y_0 = 1 \quad\mathrm{if}\quad \gamma + X1 + X2 > 0 \quad\mathrm{else}\quad 0$. The model $f_\theta(X_1,X_2,A)$ to estimate the target $Y$. We vary the parameter $ \gamma \in [0,5]$ and plot the changes in figure \ref{fig:fairSyn}. For this experiment our data is drawn out of 5,000 samples from the previously indicated distributions. Our model $f_\theta$ is a Logistic regression. The SHAP values are estimated using correlation dependent algorithm \cite{DBLP:journals/corr/ShapTrueModelTrueData}.

\begin{figure}[ht]
\centering
\includegraphics[width=1.1\linewidth]{images/syntheticFairnes.png}
\caption{Equal opportunity fairness, Demographic Parity and SHAP values of the protected attribute when modifying the gamma hyperparameter of the dataset \ref{sec:experiment.synthetic.fairness}. As the disparate treatment increases among the classes, the SHAP value of the protected attribute increases.}\label{fig:fairSyn}
\end{figure}

In figure \ref{fig:fairSyn}, we can see how the SHAP contribution varies along the demographic parity and equal opportunity fairness when changing the gamma parameter. Monitoring the SHAP value can serve as a proxy of fairness degradation.
\end{comment}


\subsubsection{Model Monitoring via OOD Explanations}
One of the main purposes of detecting out-of-distributions data/predictions/explanations is monitoring machine learning models in unseen data. In this section, we compare which explanation space provides the best signaling for model monitoring: we select the most OOD (data, predictions, and explanations) $100$ instances and the most ID $20,000$ instances out-of of the three approaches and evaluate what signaling space would be more relevant for detecting differences in the area under the curve $\texttt{AUC}_{\texttt{dif}}=\texttt{AUC}^{tr}-\texttt{AUC}^{ood}$ of the original estimator $f_\theta$.

\begin{table}[ht]
\centering
\caption{AUC decay on the $@20.000$ ID instances (\textit{Lower is better}) on the three spaces over a temporal and political geography shift. We expect that the model in ID data has the most similar performance to the ID holdout set. Out-of-distribution explanations achieve fewer false-positive model degradation indications for $66\%$ of the experiments.}
\begin{tabular}{c|ccc}
\textit{}          & \textbf{Explanations} & \textbf{Input}   & \textbf{Predictions} \\ \hline
Employment         & $\mathbf{-0.013}$     & $0.017$           & $0.167$               \\
Income             & $\mathbf{0.008}$      & $0.021$           & $0.048$               \\
Mobility           & $\mathbf{0.095}$      & $0.097$           & $0.096$               \\
Public Coverage    & $0.034$               & $\mathbf{0.028}$  & $0.033$               \\
Travel Time        & $\mathbf{0.010}$       & $0.054$            & $0.015$              
\end{tabular}
\end{table}

\begin{table}[ht]
\centering
\caption{AUC decay on the $@100$ OOD instances \textit{Higher is better} over the Travel Time dataset. We expect that the model in OOD data has a performance degradation w.r.t. the ID holdout set.}
\begin{tabular}{c|ccc}
              & \textbf{Explanations} & \textbf{Input}  & \textbf{Predictions} \\ \hline
\textit{FL-18} & $\mathbf{0.27}$       & $-0.01$         & $0.09$              \\
\textit{MI-18} & $0.15$                & $0.08$          & $\mathbf{0.21}$     \\
\textit{MN-18} & $\mathbf{0.19}$       & $0.03$          & $0.11$              \\
\textit{NY-18} & $\mathbf{0.23}$       & $-0.02$          & $-0.02$            \\
\textit{TX-18} & $\mathbf{0.20}$       & $0.07$          & $0.17$              \\
\textit{WI-18} & $\mathbf{0.12}$       & $0.10$          & $0.05$              
\end{tabular}
\end{table}

Out-of-distribution explanations provide more relevant model monitoring signaling for $66\%$ of the ID cases and for $50\%$ of the OOD cases with respect to the methods that rely on the input and prediction spaces.

This experimental part is limited by two factors, first, the difference between the calculated Shapley value and the real one, we have approximated values by interventional TreeExplainer~\cite{lundberg2020local2global}, still active areas of research are progressing on this field. Secondly, the simplicity of our method, our method is limited to a binary classification using a linear regression, the effects of more sophisticated ad-hoc classification method could potentiate the results.


\subsection{Explanation Shift Detector}
\carlos{Not sure if we have a case for this section.}
\steffen{the following theorem says nothing because `equivalent' is not defined. The Theorem is not a theorem, rather a proposition or corollary.}
\begin{theorem}
If $g_\psi$ is the Bayes optimal classifier trained on $\{\Ss(f_\theta,X)),A\}$, and, its predictive performance to distinguish whether a data instance belongs to in-distribution or out-of-distribution data is zero, i.e.\ $AUC(g_\psi(\Ss(f_\theta,X)),A) \approx 0.5$, then the behaviour of $f_\theta(X)$ is equivalent between in-distribution and out-of-distribution data  $P(\Ss(f_\theta(X), X))\equiv P(\Ss(f_\theta(X),X^{ood}))$
\end{theorem}
\begin{gather}
\mathrm{If} \quad \mathrm{AUC}(g_\psi(\Ss(f_\theta, X)),A)=0.5 \\
\Rightarrow P(\Ss(f_\theta, X^{ood})) \equiv P(\Ss(f_\theta, X))\\
\end{gather}
\steffen{the following arguments do not work. I don't know how to formalize this precisely. One could downgrade the theorem to running text and only argue the case, but do not claim that it is a proven statement.}
\carlos{not sure about this}
\klaus{I agree with Steffen. It starts with the fact, that I am not sure what behaviour exactly means. Do you mean the distribution of the predictions? Then write this!}
The above theorems hold because we are using the Bayesian optimal classifier~\cite{DBLP:conf/nips/HardtPNS16}, which implies that there are no out-of-distribution and in-distribution differences, nor in the distributions of the individual features nor on covariates of Explanation Space composed by Shapley values, then we can conclude that the model behaviour is not different between the two distributions. 


\subsubsection{Model Monitoring via OOD Explanations}
One of the main purposes of detecting out-of-distributions data/predictions/explanations is monitoring machine learning models in unseen data. As several authors have stated, quantifying model performance in OOD data is an extremely challenging task where no method will perform best for all types of distribution shifts. 


For this experiment, given a geopolitical an temporal shift (a new unseen state on a different date), we compare which explanation space provides the best signaling for model monitoring: we rank the most ID $1,000$ instances($\sim 10\%$) out-of of the three approaches and evaluate what signaling space would be more relevant for detecting drops in the AUC.

\begin{table}[ht]
\centering
\caption{\carlos{not sure what is better AUC decay or AUC normal}AUC decay on the $@1.000$ ID instances (\textit{Lower is better}) on the three spaces over a temporal and political geography shift. We expect that the model in ID data has the most similar performance to the ID holdout set. Out-of-distribution explanations achieve fewer false-positive model degradation indications.}
\begin{tabular}{c|ccc}
\textit{}          & \textbf{Explanations} & \textbf{Input}   & \textbf{Predictions} \\ \hline
Employment         & $\mathbf{-0.013}$     & $0.017$           & $0.167$               \\
Income             & $\mathbf{0.008}$      & $0.021$           & $0.048$               \\
Mobility           & $\mathbf{0.095}$      & $0.097$           & $0.096$               \\
Public Coverage    & $0.034$               & $\mathbf{0.028}$  & $0.033$               \\
Travel Time        & $\mathbf{0.010}$       & $0.054$            & $0.015$              
\end{tabular}
\end{table}

Out-of-distribution explanations provide more relevant model monitoring signaling for $66\%$ of the ID cases and for with respect to the methods that rely on the input and prediction spaces.



%Two factors limit this experimental part, first, the difference between the calculated Shapley value and the real one, we have approximated values by interventional TreeExplainer~\cite{lundberg2020local2global}, still active areas of research are progressing in this field. Secondly, the simplicity of our method, our method is limited to a binary classification using linear regression, the effects of a more sophisticated ad-hoc classification method could potentiate the results.
