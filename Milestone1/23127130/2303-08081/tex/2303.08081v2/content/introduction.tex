\section{Introduction}
Machine learning theory gives us the means to forecast the quality of ML models on unseen data, provided that this data is sampled from the same distribution as the data used to train and evaluate the model. If unseen data is sampled from a different distribution, model quality may deteriorate.

Model monitoring tries to signal and possibly even quantify such decay of trained models. Such monitoring is challenging because only in a few applications do unseen data come with labels that allow for direct monitoring of model quality. Much more often,  deployed ML models encounter unseen data for which target labels are lacking or biased~\cite{DBLP:conf/nips/RabanserGL19,DesignMLSystems,DBLP:conf/icml/ZhangGR21}.   

%the deployment data has accessible labels, and predictive quality measures can be calculated. There are many use cases where there are no labels for the data used in deployment, or the labels are difficult to obtain.
%\steffen{Every time you say `performance' I think of run time, never of quality. Maybe this is just me. At least `performance' is ambiguous.}


%\todo{@Gjergji: Add some real-world examples here: Real-time advertisement ranking on the web or mobile apps (labels exist only partially and may be biased, i.e., the observed clicks), credit scoring, fraud detection (no labels after deployment; labels are only collected in batches and serve for the development of the next model). Also, state that most of these use cases are based on heterogeneous tabular data.}

%Hence, typically, the only data available in ML applications are labeled source data and unlabeled deployment data~\cite{garg2022leveraging}.
Detecting changes in the quality of deployed ML models in the absence of labeled data remains a challenge~\cite{DBLP:conf/aaai/RamdasRPSW15,DBLP:conf/nips/RabanserGL19}. State-of-the-art techniques model statistical distances between training and unseen data distributions~\cite{continual_learning,clouderaff} or statistical distances between distributions of  model predictions~\cite{garg2022leveraging,garg2021ratt}. The shortcomings of these measures of \emph{distribution shifts} is that they do not relate changes of distributions to how they interact with trained models.  Often, there is the need to go beyond detecting changes and understand how feature attribution changes~\cite{10.1145/3534678.3542617,mougan2022monitoring,continual_learning}.

% xAI
The field of explainable AI has emerged as a way to understand model decisions ~\cite{xai_concepts,molnar2019} and interpret the inner workings of black box models~\cite{guidotti_survey}. The core idea of this paper is to go beyond the modeling of distribution shifts and monitor for \emph{explanation shifts} to signal change of interactions between learned models and dataset features in tabular data. 
We newly define explanation shift to be constituted by the statistical comparison between how predictions from training data are explained and how predictions on new data are explained.  %Explanation shift goes beyond the mere recognition of changes in data distributions towards the recognition of changes of how data distributions relate to the models' inner workings.
 
In summary, our contributions are:
\begin{itemize}
    \item We propose measures of explanation shifts as a key indicator for investigating the interaction between distribution shift and learned models.

    
    \item We define an \textit{Explanation Shift Detector} that operates on the explanation space allowing for more sensitive and explainable changes of interactions between distribution shifts and learned models.
    
    \item We compare our monitoring method that is based on explanation shifts with methods that are based on other kinds of distribution shifts. We find that monitoring for explanation shifts results in better indicators for varying model behaviour.
    
 %   provide a mathematical comparison between changes on the explanation versus input and versus predictions that shows how simple, but key types of distribution shift interact with linear models such that measures of explanation shift become much better indicators of model behaviour changes than measures of distribution shift or prediction shift. 


    \item We release an open-source Python package\footnote{to be released upon acceptance}, which implements our \enquote{\textit{Explanation Shift Detector}} that is \texttt{scikit-learn} compatible~\cite{pedregosa2011scikit}, along the code and usage tutorials for further reproducibility.
    
    %\item \textcolor{red}{We compare how measures of shifts of distributions, predictions, and explanations affect the task of quantifying model degradation in terms of predictive performance. }
    
    %\item \textcolor{red}{We provide comprehensive experimental evaluations and comparisons on real-world data concerned with  detecting and quantifying model decay.}
\end{itemize}

