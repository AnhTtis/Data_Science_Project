\section{Experiments}\label{sec:experiments}

The first experimental section explores the detection of distribution shift on the previoust synthetic examples. The second part explores the utility of explanation shift on real data applications.

\subsection{Explanation Shift Detection}
\subsubsection{Detecting multivariate shift}\label{sec:multivariate}

Given two bivariate normal distributions $X = (X_1,X_2) \sim  N\left(0,\begin{bmatrix}1 & 0 \\0& 1 \end{bmatrix}\right)$ and $X^{new} = (X^{new}_1,X^{new}_2) \sim  N \left( 0,\begin{bmatrix}1 & 0.2 \\0.2 & 1 \end{bmatrix}\right)$, then, for each feature $j$ the underlying distribution is equally distributed between $X$ and $X^{new}$, $\forall j \in \{1,2\}: P(X_j) = P(X^{new}_j)$, and what is different are the interaction terms between them. We now create a synthetic target $Y=X_1\cdot X_2 + \epsilon$ with $\epsilon \sim N(0,0.1)$ and fit a gradient boosting decision tree  $f_\theta(X)$. Then we compute the SHAP explanation values for $\mathcal{S}(f_\theta,X)$ and $\mathcal{S}(f_\theta,X^{new})$

\begin{table}[ht]
\centering
\caption{Displayed results are the one-tailed p-values of the Kolmogorov-Smirnov test comparison between two underlying distributions. Small p-values indicates that compared distributions would be very unlikely  to be equally distributed. SHAP values correctly indicate the interaction changes that individual distribution comparisons cannot detect}\label{table:multivariate}
\begin{tabular}{c|cc}
Comparison                                 & \textbf{p-value} & \textbf{Conclusions} \\ \hline
$P(X_1)$, $P(X^{new}_1)$                        & 0.33                        & Not Distinct                         \\
$P(X_2)$, $P(X^{new}_2)$                        & 0.60                        & Not Distinct                          \\
$\Ss_1(f_\theta,X)$, $\Ss_1(f_\theta,X^{new})$ & $3.9\mathrm{e}{-153}$        & Distinct                              \\
$\Ss_2(f_\theta,X)$, $\Ss_2(f_\theta,X^{new})$ & $2.9\mathrm{e}{-148}$        & Distinct   
\end{tabular}
\end{table}

Having drawn $50,000$ samples from both $X$ and $X^{new}$, in Table~\ref{table:multivariate}, we evaluate whether changes on the input data distribution or on the explanations are able to detect changes on covariate distribution.
%\steffen{... complete... What are your hypotheses/null-hypotheses? You jump over too many steps, which makes your proposition become awkwardly ambiguous.}
For this, we compare the one-tailed p-values of the Kolmogorov-Smirnov test between the input data distribution, and the explanations space.  Explanation shift correctly detects the multivariate distribution change that univariate statistical testing can not detect.

\begin{comment}
\subsubsection{Detecting concept shift}

As mentioned before, concept shift cannot be detected if new data comes without target labels. If new data is labelled, the explanation space can be useful to detect concept shift.

Given a bivariate normal distribution  $X = (X_1,X_2) \sim  N(1,I)$ where $I$ is an identity matrix of order two. We now create two synthetic targets $Y= X_1^2 \cdot X_2 + \epsilon$ and $Y^{new}=X_1 \cdot X_2^2 + \epsilon$ and fit two machine learning models $f_\theta$ on $(X,Y)$ and $h_\Upsilon$ on $(X,Y^{new})$. Now we compute the SHAP values for $\mathcal{S}(f_\theta,X)$ and $\mathcal{S}(h_\Upsilon,X)$
%\steffen{If I understand it correctly, the use of $f$ and $g$ is here completely wrong. First, you train a fuction $f_\theta(X)=Y$ (and not $f_\theta(X,Y)$), second, $g$ was previously used to run on distances, now it would run on X --- don't do such a sudden change of semantics for your symbols. Rather say: $f_\phi(X)=Y^{new}$. Table 2 needs update, too. }\carlos{Yes, you are right, its not G, but another model $h_\Upsilon$}

\begin{table}[ht]
\centering
\caption{Distribution comparison for synthetic concept shift. Displayed results are the one-tailed p-values of the Kolmogorov-Smirnov test comparison between two underlying distributions }\label{table:concept}
\begin{tabular}{c|c}
Comparison                                              & \textbf{Conclusions} \\ \hline
$P(X)$, $P(X^{new})$                                    & Not Distinct         \\
$P(Y)$, $P(Y^{new})$                                    & Not Distinct         \\
$P(f_\theta(X))$, $P(h_\Upsilon(X^{new}))$                  & Not Distinct         \\
$P(\Ss(f_\theta,X)$), $P(\Ss(h_\Upsilon,X))$                    & Distinct             \\
\end{tabular}
\end{table}

In Table~\ref{table:concept}, we see how the distribution shifts are not able to capture the change in the model behavior while the SHAP values are different. The \enquote{Distinct/Not distinct} conclusion is based on the one-tailed p-value of the Kolmogorov-Smirnov test with a $0.05$ threshold drawn out of $50,000$ samples for both distributions. As in the synthetic example, in table \ref{table:concept} SHAP values can detect a relational change between $X$ and $Y$, even if both distributions remain equivalent.
\end{comment}
\subsubsection{Uninformative features on synthetic data}

To have an applied use case of the synthetic example from the methodology section, we create a three-variate normal distribution $X = (X_1,X_2,X_3) \sim N(0,I_3)$, where $I_3$ is an identity matrix of order three. The target variable is generated  $Y=X_1\cdot X_2 + \epsilon$ being independent of $X_3$. For both, training and test data, $50,000$ samples are drawn. Then out-of-distribution data is created by shifting $X_3$, which is independent of the target, on test data $X^{new}_3= X^{te}_3+1$.

\begin{table}[ht]
\centering
\caption{Distribution comparison when modifying a random noise variable on test data. The input data shifts while explanations and predictions do not.}\label{table:unused}
\begin{tabular}{c|c}
Comparison                                              & \textbf{Conclusions} \\ \hline
$P(X^{te}_3)$, $P(X^{new}_3)$                                       & Distinct                \\
$f_\theta(X^{te})$, $f_\theta(X^{new})$                     & Not Distinct            \\
$\Ss(f_\theta,X^{te})$, $\Ss(f_\theta,X^{new})$                    & Not Distinct            \\
\end{tabular}
\end{table}

In Table~\ref{table:unused}, we see how an unused feature has changed the input distribution, but the explanation space and performance evaluation metrics remain the same. The \enquote{Distinct/Not Distinct} conclusion is based on the one-tailed p-value of the Kolmogorov-Smirnov test drawn out of $50,000$ samples for both distributions.


\subsubsection{Explanation shift that does not affect the prediction}

In this case we provide a situation when we have changes in the input data distributions that affect the model explanations but do not affect the model predictions due to positive and negative associations between the model predictions and the distributions cancel out producing a vanishing correlation in the mixture of the distribution (Yule's effect~\ref{sec:exp.vs.pred}).  

We create a train and test data by drawing $50,000$ samples from a bi-uniform distribution  $X_1 \sim U(0,1), \quad X_2 \sim U(1,2)$ the target variable is generated  by $Y = X_1+X_2$ where we train our model $f_\theta$. Then if out-of-distribution data is sampled from $X_1^{new}\sim U(1,2)$, $X_2^{new}\sim U(0,1)$

\begin{table}[ht]
\centering
\caption{Distribution comparison over how the change on the contributions of each feature can cancel out to produce an equal prediction (cf. Section \ref{sec:exp.vs.pred}), while explanation shift will detect this behaviour changes on the predictions will not.}\label{table:predShift}
\begin{tabular}{c|c}
Comparison                                              & \textbf{Conclusions} \\ \hline
$f(X^{te})$, $f(X^{new})$                                  & Not Distinct            \\
$\Ss(f_\theta,X^{te}_2)$, $\Ss(f_\theta,X^{new}_2)$                    & Distinct            \\
$\Ss(f_\theta,X^{te}_1)$, $\Ss(f_\theta,X^{new}_1)$                    & Distinct            \\
\end{tabular}
\end{table}


In Table~\ref{table:predShift}, we see how an unused feature has changed the input distribution, but the explanation space and performance evaluation metrics remain the same. The \enquote{Distinct/Not Distinct} conclusion is based on the one-tailed p-value of the Kolmogorov-Smirnov test drawn out of $50,000$ samples for both distributions.


\subsection{Explanation Shift Detector: Measurements on synthetic data}

In this work, we are proposing explanation shifts as an indicator of out-of-distribution model behavior. The \textit{Explanation Shift Detector} (cf. Section \ref{sec:Detector}), aims to detect if the behaviour of a machine learning model is different between unseen data and training data.


As ablation studies, in this work, we compare our method that learns on the explanations space (eq. \ref{eq:explanationShift}) against learning on different spaces: on input data space, that detects out-of-distribution data, but is independent of the model:
%\gjergji{The two equations are not well explained. What is $n$? How do you know that argmin of the average loss has a unique solution $\psi$? By the way, I see the same problems in Equation (4).}\carlos{We discused this with Klaus. The idea is that  argmin refers to the optimal paramer/arg. When do I say that it has an unique solution?} \gjergji{Note that in general argmin(...) can have multiple solutions (in this case multiple optimal parameters), unless you can prove some kind of convexity; it is better to write just min(...) instead (i.e., not an equation) and say that we are interested in parameters $\psi$ that minimize ... }
%\steffen{I think that it is not a good idea to give these different functions the same name. In Table 5 it becomes visible that different functions are learned in (11), in (12) and in definition 3.6, but this is far from clear when reading this text. If different names were given, they would be clearly distinguished in Table 5.}\carlos{I have used different params}

\begin{gather}
\phi = \argmin_{\tilde{\phi}} \sum_{x\in X\cup X^{new}} \ell( g_{\tilde{\phi}}(\textcolor{blue}{x})), a_x )
\end{gather}
on output space, that detects out-of-distribution predictions, but can suffer from Yule's effect of distribution shift (cf. section~\ref{sec:exp.vs.pred}):
\begin{gather}
\Upsilon = \argmin_{\tilde{\Upsilon}} \sum_{x\in X\cup X^{new}} \ell( g_{\tilde{\Upsilon}}(\textcolor{blue}{f_\theta(x)}), a_x )
\end{gather}

Our first experiment showcases the two main contributions of our method: $(i)$ being more sensitive than output spaces and input spaces to changes in the model behaviour and, $(ii)$ accounting for its drivers. 

In order to do this, we first generate a synthetic dataset with a shift similar to the multivariate shift one (cf. Section \ref{sec:multivariate}), but we add an extra variable $X_3 = N(0,1)$ and generate our target $Y=X_1 \cdot X_2 + X_3$, and parametrize the multivariate shift between $\rho = r(X_1,X_2)$. For the model $f_\theta$ we use a gradient boosting decision tree, while for $g_\psi$ we use a logistic regression. For model performance metrics, as we are in binary classification scenarios, we use the Area Under the Curve (AUC)---a  metric that illustrates the diagnostic ability of a binary classifier system as its discrimination threshold is varied~\cite{statisticallearning}.

\begin{table}[ht]
\caption{Conceptual comparison table over different detection methods over the examples discussed above. The \enquote{Explanation Shift Detector}, learning $g_\psi$ over the explanation space is the only method that achieves the desired results and is accountable. We evaluate accountability by checking if the feature attributions of the detection method correspond with the synthetic shift generated in both scenarios}\label{tab:ExplanationShiftDetector}
\begin{tabular}{c|cccc}
\textbf{Detection Method} & \textbf{Multiv.}        & \textbf{Uninf.} & \textbf{Accountability} \\ \hline
Explanation sp. ($g_\psi$)& \cmark & \cmark & \cmark \\
Input space($g_\phi$)     & \cmark & \xmark  & \xmark \\
Prediction sp.($g_\Upsilon$)  & \cmark & \cmark &  \xmark \\
Input Shift(Univ)         & \xmark & \xmark & \xmark \\
Input Shift(Mult.)        & \cmark & \xmark & \xmark \\
Output Shift              & \cmark & \cmark & \xmark \\
Uncertainty           & $\sim$ & \cmark & \cmark 
\end{tabular}
\end{table}

\begin{figure*}[ht]
\centering
\includegraphics[width=.49\textwidth]{images/sensivity.png}\hfill
\includegraphics[width=.49\textwidth]{images/SOTAsensitivity.png}
\caption{In the left figure, comparison of the performance of \textit{Explanation Shift Detector}, based on learning on  different spaces. Learning on the explanation space shows more sensitivity than learning in the output space. In the right figure, related work comparison of SOTA methods \cite{alibi-detect,mougan2022monitoring,DBLP:conf/nips/RabanserGL19}, good indicators should follow a progressive steady positive slope.}
\label{fig:sensitivity}
\end{figure*}

In Table \ref{tab:ExplanationShiftDetector} and Figure \ref{fig:sensitivity}, we show the effectos of our algorithmic approach when learning on different spaces. In the sensitivity experiment, we see that the Explanation Space offers a higher sensitivity towards distribution shift detection. This can be explained using the additive property of the Shapley values. What the explanation space is representing is a higher dimensional space than the output space that contains the model behavior. On the other hand, the input space, even if it has the same dimensional, it does not contain the projection of the model behaviour. Furthermore, we can proceed and explain what are the reasons driving the drift, by extracting the coefficients of $g_\psi$ 
 of the $\rho = 1$ case, $\beta_1 \neq 0, \beta_2\neq 0, \beta_3\sim 0 $, providing global explainability about the features that are shifting, the \textit{Explanation Shift Detector} correctly detects the features that are causing the drift.

In the right image of Figure \ref{fig:sensitivity} the comparison\footnote{The metric for the \enquote{Explanation Shift Detector} is $2\cdot(AUC-0.5)$, in order to scale respect to other metrics.} against other state-of-the-art techniques: statistical differences on the input data (Input KS, Classifier Drift)~\cite{alibi-detect,continual_learning}, that are independent of the model; uncertainty estimation ~\cite{DBLP:conf/nips/KimXB20,mougan2022monitoring,romano2021pmlb}, whose properties under specific types of shift remains unknown, or statistical changes on the output data~\cite{fort2021exploring,NEURIPS2020_219e0524} (K-S and Wasserstein Distance), which correctly detect that the model behaviour is changing, but lacks the sensitivity of the explanation space.

%has, and measures on p-values are over-saturated. \gjergji{What do you mean by this? How can a measure be over-saturated?} %In the Appendix (cf. Section XX) we provide other synthetic data experiment with Unused Features. \carlos{Yet to do.}

\subsection{Experiments on real data: Inspecting out-of-distribution explanations}
\begin{figure*}[ht]
\centering
\includegraphics[width=.49\textwidth]{images/AUC_OOD_ACSTravelTime.png}\hfill
\includegraphics[width=.49\textwidth]{images/feature_importance_ACSTravelTime.png}
\caption{In the left figure, comparison of the performance of \textit{Explanation Shift Detector}, in different states. Learning in the explanation space shows more sensitivity than learning in the output space. In the right figure, strength analysis of features driving the change in model behaviour.}
\label{fig:xai}
\end{figure*}


The need for explainable model monitoring has been expressed by several authors~\cite{https://doi.org/10.48550/arxiv.2007.06299,continual_learning,desiderataECB,DBLP:conf/aistats/BudhathokiJBN21}, understanding the effects of the distribution shift on the behaviour can provide algorithmic transparency to stakeholders and to the ML engineering team.

After providing analytical examples and experiments with synthetic data, still there is the challenge of operationalizing the \textit{Explanation Shift Detector} to real-world data.  In this section, we provide experiments on the ACSTravelTime task, whose goal is to predict whether an individual has a commute to work that is longer than 20 minutes. To create the distribution shift we train the model $f_\theta$ in California in 2014 and evaluating in the rest of the states in 2018, creating a geopolitical and temporal shift. The model $g_\theta$ is trained each time on each state using only the $X^{new}$ in the absence of the label, and its performance is evaluated by a 50/50 random train-test split. As models we use a gradient boosting decision tree\cite{xgboost,catboost} as estimator $f_\theta$, approximating the Shapley values by TreeExplainer \cite{lundberg2020local2global}, and using logistic regression for the \textit{Explanation Shift Detector}.


Our hypothesis is that the AUC in OOD data of the \enquote{Explanation Shift Detector} $g_\psi$ will be higher than ID data due to OOD model behaviour. In Figure \ref{fig:xai}, we can see the performance of the \enquote{Explanation Shift Detector} over different 
 data distributions. The baseline is a hold-out set of $ID-CA14$, and the closest AUC is for $CA18$, where there is just a temporal shift, then the OOD detection performance over the rest of states. The most OOD state is Puerto Rico (PR18).

The next question that we aim to answer is what are the features where the model behaviour is different. For this, we do a distribution comparison between the linear coefficients of the \enquote{Explanation Shift Detector} in ID and in OOD. As a distance measure, we use the Wasserstein distance between 1000 in-distribution bootstraps using a $63.2\%$ sampling fraction~\cite{statisticallearning} from California-14 and 1000 OOD bootstraps from other states in 2018 (see \ref{fig:xai}). In the right image  \ref{fig:xai}, for PR18, we see that the most important feature is the citizenship status\footnote{See the ACS PUMS data dictionary for the full list of variables available \url{https://www.census.gov/programs-surveys/acs/microdata/documentation.html}}. 

We also perform an across-task evaluation, by comparing it with the other prediction task in the appendix. We can see how, even if some of the features are in present in different prediction tasks, the weight and importance order assigned by the \enquote{Explanation Shift Detector} is different. One of the main contributions of this method is that is not just how distribution differs, but how they differ with respect to the model.

\section{Discussion}

In this work, we have proposed explanation shifts as a key indicator for investigating the interaction between distribution shifts and the learned model. Finding that monitoring explanations shift is a better indicator than model varying behaviour.

Our approach is not able to detect concept shifts, as concept shift requires understanding the interaction between prediction and response variables. By the very nature of concept shifts, such changes can only be understood if new data comes with labeled responses. We work under the assumption that such labels are not available for new data and, therefore, our method is not able to predict the degradation of prediction performance under distribution shifts. All papers such as \cite{garg2022leveraging,mougan2022monitoring,baek2022agreementontheline,chen2022estimating,fang2022is,baek2022agreementontheline,DBLP:conf/icml/MillerTRSKSLCS21} that address the monitoring of prediction performance have the same limitation. Only under specific assumptions, e.g., no occurrence of concept shift, performance degradation can be predicted with reasonable reliability.

%Without any assumptions on the type of shift, estimating model performance in out-of-distribution data is a challenging task, where no estimator will be the best under all the types of shift \cite{garg2022leveraging}, in this work we have  shifted the focus on detecting out-of-distribution model behaviour instead of out-of-distribution model performance or out-of-distribution data. We compared how well measures of explanation shift would perform relative to distribution shift and found encouraging results. 
The  potential utility of explanation shifts as distribution shift indicators that affect the model in computer vision or natural language processing tasks remains an open question. We have used Shapley values to derive indications of explanation shifts, but we believe that other AI explanation techniques may be applicable and come with their own advantages.

%As explainability technique we have used Shapley values, there is further explainable AI techniques such as more feature attribution methods or contrafactual explanations that can be applied. 

%We can see how for CA14 and CA18 the statistical differences are lower, indicating that the coefficients are more likely to be from the same distribution and that there are no out-of-distribution model behaviour drivers. On the Appendix (cf. Section XX) we provide experiments on 5 more datasets based on the US Income data. \carlos{Yet to do.}

%\\[5cm]
%\steffen{What I have been preaching for a while, but which you seem to ignore is:
%\\
%1. You need to compare to state-of-the-art methods. Pull out 2-3 methods published in the last 2-3 years and compare your approach.
%\\
%2. You need to go beyond the census dataset. 
%\\
%I believe that the paper will be rejected if you do not do these two steps. They seem unnecessary to you, but these are standard check boxes that reviewers want to have ticked. When they feel even slightly uneasy about your approach, e.g. because it is so simple as it is, they will give the lack of 1. and 2. as reasons that you did not do your job fully}
