\section{Mathematical Analysis}
In this section, we provide a mathematical analysis on how changes in the distributions impact the explanation space and compare to the input data space and the prediction space. 


\subsection{Explanation vs Prediction spaces}\label{sec:exp.vs.pred}

%\steffen{I do not understand the following sentence. I do not even understand the preceding sentence.}\carlos{check}
This section exploits the benefits of the explanation space with respect to the prediction space when detecting distribution shifts. We will provide a theorem proving that OOD predictions imply OOD Explanations, and a counterexample for the opposite case, OOD Explanations do not necessarily imply OOD predictions. 
\begin{prop}
    Given a model $f_\theta:X \to Y$. If $f_\theta(x^{'})\neq f_\theta(x)$, then $\Ss(f_\theta,x^{'}) \neq \Ss(f_\theta,x)$.
\end{prop}
\begin{gather}
\texttt{Given}\quad f_\theta(x)\neq f_\theta(x')\\
\sum_{j=1}^p \Ss_j(f_\theta,x) = f_\theta(x) - E_X[f_\theta(X)]\\
\texttt{then}\quad \Ss(f,x)\ \neq \Ss(f,x')
\end{gather}



By additivity/efficiency property of the Shapley values~\cite{DBLP:journals/ai/AasJL21} (equation (3)), if  the prediction between two instances is different, then they differ in at least one component of their explanation vectors. 

%both terms of the equation, if the distribution of the prediction space changes, then at least one of the values of the sum has to be different, so the explanation is distinct. \gjergji{Not quite sure what you mean, because two summands can change in such a way that the sum remains unchanged and (8) could still be equality, although I understand that because of additivity/efficiency of Shapley values (8) cannot be an equality.}\carlos{If this happens the explanation space still will be different $\Ss(f,X)\neq \Ss(f,X')$}

The opposite direction does not hold, we can have out-of-distributions explanations but in-distribution predictions, as we can see in the following counter-example:
\begin{example}[Explanation shift that does not affect the prediction distribution] Given $\mathcal{D}^{tr}$ is generated from $(X_1,X_2,Y), X_1 \sim U(0,1), X_2 \sim U(1,2), Y = X_1+X_2+\epsilon$ and thus the model is $f(x)=x_1+x_2$. If $\mathcal{D}^{new}$ is generated from $X_1^{new}\sim U(1,2), X_2^{new}\sim U(0,1)$, the prediction distributions are identical $f_\theta(\mathcal{D}^{tr}),f_\theta(\mathcal{D}^{new})\sim U(0,3)$, but explanation distributions are different $S(f_\theta,\mathcal{D}^{tr}_X)\neq S(f_\theta,\mathcal{D}^{new})$
\begin{gather}
    \forall i \in  \{1,2\} \quad \Ss_i(f_\theta,x) = \alpha_i \cdot x_i  \\
   \forall i \in  \{1,2\} \Rightarrow  \Ss_i(f_\theta,X))\neq \Ss_i(f_\theta,X^{new})\\
    \Rightarrow f_\theta(X)=f_\theta(X^{new})
\end{gather}
\end{example}

%\klaus{Equation \eqref{eq:dummy_label_1}: on the left there is a vector for each sample (i.e. a matrix in total). On the right, there is a scalar for each sample (i.e. a vector in total). The equality does not make sense if the dimensions don't match!}\carlos{check?}
In this example, we can calculate the IID linear interventional Shapley values~\cite{DBLP:journals/ai/AasJL21}. Then the Shapley values for each feature will have different distributions between train and out-of-distribution explanations, but the prediction space will remain unaltered.

%We call this the Yule's effect~\cite{yule1900vii,simpson1951interpretation} of distribution shift on the predictions, where the differences between feature contribution can cancel out between themselves and result in an equal prediction, even though the model behaviour is distinct. \carlos{Klaus, can you check if this makes sense?}

 
%\steffen{sometimes you say `output space', here you say `prediction space'. I get confused here. Also the but sentence is unclear. Same as what?}\carlos{Let's use \enquote{Prediction Space} all the time.}


\subsection{Explanation shifts vs input data distribution shifts}\label{subsec:explanationShiftMethods}

This section provides two different examples of situations where changes in the explanation space can correctly account for model behavior changes, but where statistical checks on the input data either (1) cannot detect changes, 
%(2) require sophisticated methods to detect these changes,
or (2) detect changes that do not affect model behavior. For simplicity the model used  in the analytical examples is a linear regression where, if the features are independent, the Shapley value can be estimated by $\Ss(f_\theta, x_i) = a_i(x_i-\mu_i)$, where $a_i$ are the coefficients of the linear model and $\mu_i$ the mean of the features \cite{DBLP:journals/corr/ShapTrueModelTrueData}. 

\subsubsection{Detecting multivariate shift}
One  type of distribution shift that is challenging to detect comprises cases where the univariate distributions for each feature $j$ are equal between the source $\Dd{tr}$ and the unseen dataset $\Dd{new}$, but where  interdependencies among different features change. 
%\steffen{I do not understand the following sentence. You say in the following sentence that univariate tests are sufficient, though in the preceding sentence  you basically said that they are not sufficient, and then you say that your univariate distribution is on high dimensional space, which brings another aspect in the discussion. This later aspect needs to be shaved off into a different sentence and anyway there is not always a high dimensional space.}\carlos{check?}
On the other hand, multi-covariance statistical testing is not an easy task that has high sensitivity easily leading to false positives. The following  examples demonstrate that Shapley values account for co-variate interaction changes while a univariate statistical test will provide false negatives. \\
\textbf{Example 1: \textit{Multivariate Shift}}\\ 
\textit{Let $X = (X_1,X_2) \sim {\tiny  N\left(\begin{bmatrix}\mu_{1}  \\ \mu_{2} \end{bmatrix},\begin{bmatrix}\sigma^2_{x_1} & 0 \\0 & \sigma^2_{x_2} \end{bmatrix}\right),}$\\
 $X^{new} = (X^{new}_1,X^{new}_2) \sim {\tiny N\left(\begin{bmatrix}\mu_{1}  \\ \mu_{2} \end{bmatrix},\begin{bmatrix} \sigma^2_{x_1} & \rho\sigma_{x_1}\sigma_{x_2}  \\ \rho\sigma_{x_1}\sigma_{x_2} & \sigma^2_{x_2}\end{bmatrix}\right)}$. We fit a linear model 
$f_\theta(X_1,X_2) = \gamma + a\cdot X_1 + b \cdot X_2.\hspace{0.5cm}$  $X_1$ and $X_2$ are identically distributed with $X_1^{new}$ and $X_2^{new}$, respectively, while this does not hold for the corresponding SHAP values $\Ss_j(f_\theta,X)$ and $\Ss_j(f_\theta,X^{new})$.}

The  detailed analysis is given in the Appendix.


%The above theorem works under the assumption of linear regression and that the covariate term $\rho=1$, is a not-so-common situation in ML applications.
\begin{comment}
\subsubsection{Detecting concept distribution shift}
One  type of distribution shift that is most challenging to detect comprises cases where distributions are equal between source and unseen data-set $P(X^{tr}) = P(X^{new})$ and the target variable  $P(Y^{tr}) = P(Y^{new})$ and what changes are the relationships that features have with the target $P(Y^{tr}|X^{tr}) \neq  P(Y^{new}|X^{new})$. This kind of distribution shift is also known as concept drift or posterior shift~\cite{DesignMLSystems} and is especially difficult to notice, as it requires labeled data to detect in general. The following example compares how  explanations may change for two models fed with the same input data and different target relations.

\textbf{Example 2: \textit{Concept shift}}\textit{
Let $X = (X_1,X_2) \sim N(\mu,I)$, and $X^{new}= (X^{new}_1,X^{new}_2) \sim N(\mu,I)$, where $I$ is an identity matrix of order two and $\mu = (\mu_1,\mu_2)$. We now create two synthetic targets $Y=a + \alpha \cdot X_1 + \beta \cdot X_2 + \epsilon$ and $Y^{new}=a + \beta \cdot X_1 + \alpha \cdot X_2 + \epsilon$. Let $f_\theta$ be a linear regression model trained on $(X,Y)$ and $h_\phi$ another linear model trained on $(X^{new},Y^{new})$. Then $P(f_\theta(X)) = P(h_\phi(X^{new}))$, $P(X) = P(X^{new})$ but $\Ss(f_\theta,X)\neq \Ss(h_\phi, X)$}. 
\end{comment}
\subsubsection{Shifts on uninformative features by the model}

Another typical problem is false positives when a statistical test recognizes differences between a source distribution and a new distribution, though the differences do not affect the model behavior\cite{grinsztajn2022why}. One of the intrinsic properties that Shapley values satisfy is the \enquote{Dummy}, where a feature $j$ that does not change the predicted value, regardless of which coalition the feature is added, should have a Shapley value of $0$. If $\mathrm{val}(S\cup \{j\}) = \mathrm{val}(S)$ for all $S\subseteq \{1,\ldots, p\}$ then $\Ss_j(f_\theta, x_i)=0$.

\textbf{Example 2: \textit{Unused features}}\textit{
Let $X = (X_1,X_2,X_3) \sim N(\mu,\mathrm{diag}(c))$, and $X^{new}= (X^{new}_1,X^{new}_2,X^{new}_3) \sim N(\mu,\mathrm{diag}(c'))$, where $\mathrm{c}$ and $\mathrm{c'}$ are an identity matrix of order three and $\mu = (\mu_1,\mu_2,\mu_3)$. We now create a synthetic target $Y=a_0 + a_1 \cdot X_1 + a_2 \cdot X_2 + \epsilon$ that is independent of $X_3$. We train a linear regression $f_\theta$ on $(X,Y)$, with coefficients $a_0,a_1,a_2,a_3$. Then if $\mu_3'\neq \mu_3$ or $c_3' \neq c_3$, then $P(X_3)$ can be different from $P(X_3^{new})$ but $\Ss_3(f_\theta, X) = \Ss_3(f_\theta,X^{new})$}
%\klaus{If $c$ and $c'$ are vectors, then $c\cdot I$ is also a vector, but you need a covariance \textbf{matrics}. Write $\mathrm{diag}(c)$ instead.}\carlos{does not it seem weird?}\klaus{We can make it less weird by (a) using \texttt{\textbackslash mathrm} instead of \texttt{\textbackslash texttt} and (b) omitting $\cdot I$: $\mathrm{diag}(c)$ is already a matrics}
