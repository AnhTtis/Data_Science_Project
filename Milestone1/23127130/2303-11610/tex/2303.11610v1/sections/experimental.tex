%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Experimental results}\label{sec:experiments}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Experimental setup}

\noindent \textbf{Datasets.} 
We evaluate our approach on SemanticKITTI~\cite{behley2019semantickitti, geiger2012cvpr, behley2021ijrr} and SemanticPOSS~\cite{pan2020semanticposs}.
SemanticKITTI~\cite{behley2019semantickitti} consists of 43,552 point cloud acquisitions with point-level annotations of 19 semantic classes.
Based on the official benchmark guidelines~\cite{behley2019semantickitti}, we use sequence $08$ for validation and the other sequences for training.
SemanticPOSS~\cite{pan2020semanticposs} consists of 2,988 real-world point cloud acquisitions with point-level annotations of 13 semantic classes.
Based on the official benchmark guidelines~\cite{pan2020semanticposs}, we use sequence $03$ for validation and the other sequences for training.

\noindent \textbf{Experimental protocol for 3D NCD.}
Similarly to what proposed by \cite{zhao2022novel} in the 2D domain, we create different splits of each dataset to validate the NCD performance.
We create four splits for SemanticKITTI and SemanticPOSS.
We refer to these splits as SemanticKITTI-$n^i$ and SemanticPOSS-$n^i$, where $i$ indexes the split.
In each set, the novel classes and the base classes correspond to unlabelled and labelled points, respectively.
Tabs.~\ref{tab:KITTI_folds} \& \ref{tab:POSS_folds} detail the splits of our datasets.
These splits are selected based on their class distribution in the dataset and on the semantic relationship between novel and base classes, e.g.~in KITTI-$4^3$ the base class \textit{motorcycle} can be helpful to discover the novel class \textit{motorcyclist}.
We report additional details about the selection process in the Supplementary Material.

We quantify the performance by using the mean Intersection over Union (mIoU), which is defined as the average IoU across the considered classes \cite{behley2019semantickitti}.
We provide separate mIoU values for the base and novel classes.
We also report the overall mIoU computed across all the classes in the dataset for completeness.

%+++++++++++++++++++++++++++++++++++
\begin{table}[t]
    \centering
    \caption{SemanticKITTI splits, is defined as KITTI-$n^i$, where $n$ is the number of novel classes and $i$ is the split index.}
    \label{tab:KITTI_folds}
    \vspace{-.2cm}
    \resizebox{\columnwidth}{!}{
    \begin{tabular}{ll}
        \toprule
        Split & Novel Classes \\
        \midrule
        KITTI-$5^0$ & \textit{building}, \textit{road}, \textit{sidewalk}, \textit{terrain}, \textit{vegetation} \\
        KITTI-$5^1$ & \textit{car}, \textit{fence}, \textit{other-ground}, \textit{parking}, \textit{trunk} \\
        KITTI-$5^2$ & \textit{motorcycle}, \textit{other-vehicle}, \textit{pole}, \textit{traffic-sign}, \textit{truck} \\
        KITTI-$4^3$ & \textit{bicycle}, \textit{bicyclist}, \textit{motorcyclist}, \textit{person} \\
        \bottomrule
    \end{tabular}
    }
\end{table}
%+++++++++++++++++++++++++++++++++++
%+++++++++++++++++++++++++++++++++++
\begin{table}[t]
    \centering
    \caption{SemanticPOSS splits, defined as POSS-$n^i$, where $n$ is the number of novel classes and $i$ is the split index.}
    \label{tab:POSS_folds}
    \vspace{-.2cm}
    \resizebox{.7\columnwidth}{!}{
    \begin{tabular}{ll}
        \toprule
        Split & Novel Classes  \\
        \midrule
        POSS-$4^0$ & \textit{building}, \textit{car}, \textit{ground}, \textit{plants} \\
        POSS-$3^1$ & \textit{bike}, \textit{fence}, \textit{person} \\
        POSS-$3^2$ & \textit{pole}, \textit{traffic-sign}, \textit{trunk} \\
        POSS-$3^3$ & \textit{cone-stone}, \textit{rider}, \textit{trashcan} \\
        \bottomrule
    \end{tabular}
    }
\end{table}
%+++++++++++++++++++++++++++++++++++




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\noindent \textbf{Implementation Details.}
We implement our network based on a MinkowskiUNet-34C network~\cite{choy20194d}.
Point-level features are extracted from the penultimate layer.
The segmentation heads are implemented as linear layers, producing output logits for each point in the batched point clouds.
We train our network for 10 epochs.
We use the SGD optimizer, with momentum 0.9 and weight decay 0.0001. 
Our learning rate scheduler consists of linear warm-up and cosine annealing, with $lr_{max} = 10^{-2}$ and $lr_{min} = 10^{-5}$. 
We train with batch size equal to 4. 
We employ 5 segmentation heads, that are used in synergy with an equal number of over-clustering heads, with $o = 3$.
In $\phi$, we set $p=0.5$ for SemanticKITTI-$n^i$ and $p=0.3$ for SemanticPOSS-$n^i$.
We adapted the implementation of the Sinkhorn-Knopp algorithm~\cite{cuturi2013sinkhorn} from the code provided by \cite{caron2020unsupervised}, with the introduction of the queue and an in-place normalisation steps.
Similarly to \cite{caron2020unsupervised}, we set $n_{sk\_iters}=3$, while we adopt a linear decay for $\epsilon$, with $\epsilon_{start}=0.3, \epsilon_{end}=0.05$.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Quantitative analysis}

\noindent \textbf{Segmentation quality.}
Tabs.~\ref{tab:results_poss} \& \ref{tab:results_semantickitti} report the quantitative results on SemanticPOSS and SemanticKITTI, respectively.
We report the \textit{Full supervision} setting as our upper bound.

On SemanticPOSS, we outperform EUMS$^\dag$ on three out of four splits with an improvement of $18.3$ mIoU on POSS-$4^0$, $~9.0$ mIoU on POSS-$3^1$ and $0.6$ on POSS-$3^2$.
In these splits, \ourmethod shows a large improvement on all the novel classes, with the exception of the class \textit{fence} in POSS-$3^1$ and \textit{traffic-sign} in POSS-$3^3$.
Differently, we deem the lower performance in POSS-$3^3$ is due to the difficulty and scarce presence of these novel classes.
The advantage of EUMS$^\dag$ is the clustering on the whole dataset that enables a complete visibility of all the novel classes.
On average, \ourmethod achieves $21.40$ mIoU, improving over EUMS$^\dag$ of $6.5$ mIoU.

On SemanticKITTI, we outperform EUMS$^\dag$ on all the four splits, improving by $12.6$ mIoU on KITTI-$5^0$, $1.2$ mIoU on KITTI-$5^1$, $4.2$mIoU on KITTI-$5^2$ and $5.3$ mIoU on KITTI-$4^3$.
\ourmethod outperforms EUMS$^\dag$ by a large margin on the majority of the novel classes. 
Exceptions are the class \textit{sidewalk} in KITTI-$5^0$, \textit{car} in KITTI-$5^1$, \textit{motorcycle} in KITTI-$5^2$ and \textit{motorcyclist} in KITTI-$4^3$.
On average, \ourmethod achieves $22.84$ mIoU, improving over EUMS$^\dag$ of $5.8$ mIoU.
Interestingly, \ourmethod outperforms also the supervised upper bound on the class \textit{trunk} in KITTI-$5^1$.




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\noindent \textbf{Computational time.}
\ourmethod outperforms EUMS$^\dag$ in terms of computational time.
Firstly, EUMS$^\dag$ requires a pre-training step and a fine-tuning step, i.e.~30 training epochs in total. 
Then, EUMS$^\dag$ requires a large amount of memory (up to 200 GB memory for KITTI-$5^0$) to store the data required for clustering, taking several hours (50 hrs) to complete the training procedure. 
Differently, \ourmethod achieves superior performance with 10 training epochs, by using less memory (10 GB max) and a lower computational time (up to 25 hrs for KITTI-$5^0$).
We run these tests using one GPU Tesla A40-48GB.



\input{tabs/poss}
\input{tabs/kitti}





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Qualitative analysis}

Fig.~\ref{fig:qualitative} shows some segmentation results of \ourmethod and EUMS$^\dag$ on SemanticPOSS and SemaniticKITTI.
We can observe that the predictions of the base classes in the two datasets are correct for both the models, with just minor errors at the edges of the objects.
\ourmethod shows better segmentation capabilities also when dealing with the novel classes, being able to properly segment the correct class for the unknown objects. 
On the larger objects, such as buildings in the scenes, some mixing of labels can be observed.
Differently, EUMS nearly fails in correctly recognising the novel objects, resulting in the inclusion of different classes (either novel or base) into the same object, e.g.~the facade of the building in POSS-$3^1$.


\input{images/qualitative}