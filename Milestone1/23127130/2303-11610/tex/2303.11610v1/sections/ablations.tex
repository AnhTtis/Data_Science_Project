%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Ablation studies}\label{sec:ablation}
We study the main components of \ourmethod and its behaviour when varying its parameters on SemanticPOSS.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\noindent\textbf{Method components.}
Fig.~\ref{fig:ablation_components} shows the performance on novel and base classes of seven versions of \ourmethod.
The first three versions use the pre-trained model on the base classes, while the last four versions use the model trained from scratch. 
Each version is defined as follows:

\begin{itemize}
    \item $\mathsf{P}$: we use a pre-trained model, and we remove $Z_q$, $\tau_c$ and the over-clustering heads.
    \item $\mathsf{OC}$: $\mathsf{P}$ + over-clustering heads.
    \item $\mathsf{Q}$: $\mathsf{OC}$ + $Z_q$, i.e.~our queue without class-balancing.
    \item $\mathsf{NP}$: $\mathsf{Q}$ without pre-training.
    \item $\mathsf{NP}$+: $\mathsf{NP}$ + our selection function $\phi$ on the queue.
    \item $\mathsf{NP}$++: $\mathsf{NP}$ + $\tau_c$ on the features used to derive the pseudo-labels.
    \item $\mathsf{Full}$: \ourmethod with all the components activated.
\end{itemize}

Pre-trained approaches generally underperform their trained-from-scratch counterparts on the novel classes.
This is visible in the low performance of $\mathsf{P}$, $\mathsf{OC}$ and $\mathsf{Q}$.
We have a significant improvement when pre-training is not used ($\mathsf{NP}$), i.e.~we achieve $20.26$ mIoU.
We can see that the queue both with and without pre-training is helpful.
When we add the feature selection for the queue and for the training, i.e.~$\mathsf{NP}$+ and $\mathsf{NP}$++, we have improvements, i.e.~$20.63$ mIoU and $20.90$ mIoU, respectively.
The best performance is achieved with $\mathsf{Full}$.
Although we can observe variations on the performance of the base classes, their information is retained by the network when we discover the novel ones.

%-------------------------------
\begin{figure}[t]
\centering
    \begin{tabular}{@{}c@{}c}
        \begin{overpic}[width=0.48\columnwidth]{images/ablations/components/components_novel.pdf}
        \end{overpic}& 
        \begin{overpic}[width=0.52\columnwidth]{images/ablations/components/components_base.pdf}
        \end{overpic}
    \end{tabular}
    \vspace{-.3cm}
    \caption{Ablation study with different components and initialisation strategies on SemanticPOSS. 
    In $\mathsf{P}$, $\mathsf{OC}$ and $\mathsf{Q}$, we initialise the model after base pre-training, and use different configurations of the over-clustering heads and of our queue balancing. 
    In $\mathsf{NP}$ $\mathsf{NP}$+, $\mathsf{NP}$++ and $\mathsf{Full}$, we begin with $\mathsf{Q}$, we avoid pre-training, and we use $\phi$ and $\tau_c$ incrementally. 
    See Sec.~\ref{sec:ablation} for definition of methods.}
    \label{fig:ablation_components}
\end{figure}
%-------------------------------







%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\noindent \textbf{Parameter analysis.}
We study the behaviour of the percentile $p$ in our selection function $\phi$, when we apply it to the features both for pseudo-labelling and for the class-balanced queue $\mathtt{Z}_q$.
Tab.~\ref{tab:components_ablations} reports the results on each split of SemanticPOSS.
For each split, we can observe that the performance depends on the number of points and difficulty of the novel classes. In POSS$-4^0$ and POSS-$3^1$, lower values of $p$ result in less severe selection. 
We believe this is related to the class distribution within these splits.
This is in line with what observed in Tab.~\ref{tab:results_poss}.
In POSS$-3^2$ and POSS-$3^3$, we notice a different behaviour, a higher value of $p$ provides better results.
We relate this to the difficulty of the novel classes in these splits whose noisy pseudo-labels can benefit from a more rigorous selection of the features.

%-------------------------------
\begin{table}[t]
    \centering
    \caption{Ablation study showing how different values of $p$ affect the performance on SemanticPOSS. 
    The lower $p$ is, the less severe the selection of the features, resulting in better performances for POSS-$4^0$. Differently, POSS-$3^3$ benefits from an higher value of $p$, which leads to a more vigorous filtering of the features. POSS-$3^1$ and POSS-$3^2$ show the best performances with $p=0.5$}
    \label{tab:components_ablations}
    \vspace{-.2cm}
    \resizebox{.85\columnwidth}{!}{
    \begin{tabular}{lccccc}
        \toprule
        \multirow{2}{*}{Split} & \multicolumn{5}{c}{Percentile $p$}\\
        & 0.1 & 0.3 & 0.5 & 0.7 & 0.9 \\
        \midrule
        POSS-$4^0$ & 30.81 & \textbf{35.70} & 28.77 & 30.93 & 26.69 \\
        POSS-$3^1$ & 28.33 & 30.02 & \textbf{30.43} & 23.32 & 18.91 \\
        POSS-$3^2$ & 8.07 & 8.95 & \textbf{10.32} & 10.25 & 7.76 \\
        POSS-$3^3$ & 10.55 & 10.94 & 11.69 & \textbf{14.38} & 13.42 \\
        \midrule
        Avg. & 19.44 & \textbf{21.40} & 20.30 & 19.72 & 16.70 \\
        \bottomrule
    \end{tabular}
    }
\end{table}
%-------------------------------
