%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Adapting NCD for 2D images to 3D} \label{sec:adaptation_Zhao}

Another contribution of this work is to adapt the method proposed by Zhao et al.~\cite{zhao2022novel} for NCD for 2D semantic segmentation (EUMS) to 3D data. 
Our empirical evaluation (see Sec.~\ref{sec:experiments}) shows that the transposition of EUMS to the 3D domain has some limitations.
In particular, as described in Sec.~\ref{sec:intro}, EUMS uses two assumptions: \textbf{I}) the novel classes belong to the foreground and \textbf{II}) each image can contain at most one novel class.
This allows EUMS to leverage a saliency detection model to produce a foreground mask and a segmentation model pre-trained on the base classes to determine which portion of the image is background.
The portion of the image that belongs to both the foreground mask and the background mask is where features are then pooled.
EUMS computes a feature representation for each image by average pooling the features of the pixels belonging the unknown portion.
The feature representations of all the images in the dataset are clustered with K-Means by using the number of classes to discover as the target number of clusters.
EUMS shows that overclustering and entropy-based modelling can be exploited to improve the results.
The affiliation of a point to its cluster is used to produce hard pseudo-labels that are in turn used along with the ground-truth labels to fine-tune the pre-trained model.

With 3D point clouds, there is no concept of foreground and background (in contrast with \textbf{I}). Our adaptation is designed to discover the classes of all the unlabelled points (in contrast with \textbf{II}).
Therefore, given the unlabelled points of each point cloud, we randomly extract a subset of these by setting a ratio (e.g.~30\%) with upper bound (e.g.~1K) on the number of points to select.
We compute and collect their features for all the point clouds in the dataset and apply K-Means on the whole set of features.
Note that this clustering step is computationally expensive, and we had to use High Performance Computing to execute it.
The subsampling of the points was necessary to fit the data in the RAM (see Sec.~\ref{sec:experiments} for a detailed analysis).
Once the cluster prototypes are computed, we produce the hard pseudo-labels.
To enrich the set of pseudo-labels, we propagate the pseudo-label of each point to its nearest neighbour in the coordinate space.
This allows us to expand the subset of pseudo-labelled randomly selected points.
We also implement the other steps of overclustering and entropy-based modelling to boost the results.
Lastly, we fine-tune our model with these pseudo-labels.
We name our transposition of EUMS as EUMS$^\dag$ and report its block diagram in the Supplementary Material.














