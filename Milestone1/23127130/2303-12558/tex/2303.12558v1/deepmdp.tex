From now on, fix MDPs $\mdp = \mdptuple$ and $\latentmdp = \latentmdptuple$ such that $\latentstates$ is equipped with metric $\distance_{\latentstates}$.
Let $\embed\colon \states \to \latentstates$ and $\embeda \colon \states \times \latentactions \to \actions$ be respectively state and action \emph{embedding functions}.
Intuitively, $\embed$ and $\embeda$ connect the state and action spaces of the two MDPs.
We refer to $\tuple{\latentmdp, \embed, \embeda}$ as a \emph{latent space model} of $\mdp$ and $\latentmdp$ as its \emph{latent MDP}.
We write $\latentmpolicies = \mpolicies{\latentmdp}$, and ${\latentvaluessymbol{\latentpolicy}{}}$ (resp. ${\latentqvaluessymbol{\latentpolicy}{}}$) for the value (resp. action-value) function of a policy $\latentpolicy \in \latentmpolicies$ in $\latentmdp$.
We can also consider $\latentpolicy$ as a policy in $\mdp$, where states passed to $\latentpolicy$ are embedded with $\embed$. Then, actions executed are embedded with $\embeda$.
Let $\latentpolicy \in \latentmpolicies$ and $\state \in \states$, we write $\latentaction \sim \latentpolicy\fun{\sampledot \mid \state}$ for $ \latentaction \sim \latentpolicy\fun{\sampledot \mid \embed\fun{\state}}$.
We further write $\qvalues{\latentpolicy}{}{\state}{\latentaction}$ as shorthand for $\qvalues{\latentpolicy}{}{\state}{\embeda\fun{\state, \latentaction}}$.
%$\action \sim \latentpolicy\fun{\sampledot \mid \state} \equiv \latentaction \sim \latentpolicy\fun{\sampledot \mid \state}, \,\action = \latentaction$
%Note that we can similarly define latent space models where $\mdp$ and $\latentmdp$ share the same action space $\actions$. In that case, the embedding action function $\embeda\fun{\state, \sampledot}$ is the identity function for all $\state \in \states$.

A particular point of interest is to focus on discrete latent models when $\mdp$ is continuous.
In that case, we assume $\distance_{\latentstates}$ to be the discrete metric, i.e., $\condition{\neq}$.
%This enables the use of model checking to verify properties in $\latentmdp$.
% and synthesis to construct policies in $\latentpolicies$.
%
% \subsection{Definitions}
%
In the following, we adopt the latent space model formalism of
\citet{DBLP:conf/icml/GeladaKBNB19}.
%\todo{F: I cut here. We already mentioned that in the intro}
% They refer to their models as \emph{DeepMDPs}, i.e., latent space models where $\embed$ is learned through the minimization of tractable losses (usually via neural networks). %, coming with theoretical guarantees.
% Note that they focus on the continuous setting and they do not consider action embedding functions.
%\FD{I revised this sub-section. Smooth-valuations and Lipschitzness definitions have been moved to the Appendix.}

\smallparagraph{Notations.}~Let $\latentpolicy \in \latentmpolicies$, we write $\Rmax{\latentpolicy}$ for $\sup_{\latentstate \in \latentstates} \left| \latentrewards_{\latentpolicy}\fun{\latentstate} \right|$.
We say that $\latentmdp$ is \emph{$\langle \KR{\latentpolicy}, \KP{\latentpolicy}\rangle$-Lipschitz} if for all $\latentstate_1, \latentstate_2 \in \latentstates$,
\begin{align*}
    \left|\latentrewards_{\latentpolicy}\fun{\latentstate_1} - \latentrewards_{\latentpolicy}\fun{\latentstate_2}\right| &\leq \KR{\latentpolicy} \distance_{\latentstates}\fun{\latentstate_1, \latentstate_2},\\
    \wassersteindist{\distance_{\latentstates}}{\latentprobtransitions_{\latentpolicy}\fun{\sampledot \mid \latentstate_1}}{ \latentprobtransitions_{\latentpolicy}\fun{\sampledot \mid \latentstate_2}} &\leq \KP{\latentpolicy} \distance_{\latentstates}\fun{\latentstate_1, \latentstate_2}.
\end{align*}
% In the discrete setting, we omit the $\distance_{\latentstates}$ terms.

%\smallparagraph{Local losses.}~%
%While global losses introduced by \cite{DBLP:conf/icml/GeladaKBNB19} are theoretically interesting, they depend on operations over the whole state-action space of $\mdp$, making them infeasible to approximate (and further be optimized).
%Moreover, when focusing on the behavior of $\mdp$ under a given a policy, it is rather interesting to focus on parts of the state space reachable when executing this policy, instead of considering the whole, possibly irrelevant, state space of $\mdp$.
\smallparagraph{Local losses.}~%
% \emph{Local losses} are measured under an expectation over a
% state-action distribution $\stationary{}$ and are defined as:% follows:
Let $\stationary{} \in \distributions{\states \times \actions}$,
\emph{local losses} are defined as:% follows:
\begin{align*}
    \localrewardloss{\stationary{}} &= \expectedsymbol{\state, \latentaction \sim \stationary{}}\left| \rewards\fun{\state, \latentaction} - \latentrewards\fun{\embed\fun{\state}, \latentaction} \right|, \\
    \localtransitionloss{\stationary{}} &= \expectedsymbol{\state, \latentaction \sim \stationary{}} \wassersteindist{\distance_{\latentstates}}{\embed\probtransitions\fun{\sampledot \mid \state, \latentaction}}{\latentprobtransitions\fun{\sampledot \mid \embed\fun{\state}, \latentaction}}
\end{align*}
where (i) $\rewards\fun{\state, \latentaction}$, (ii)
$\probtransitions\fun{\sampledot \mid \state, \latentaction}$, and (iii)
$\embed\probtransitions\fun{\sampledot \mid \state, \latentaction}$ are
shorthand for (i) $\rewards\fun{\state, \embeda\fun{\state, \latentaction}}$,
(ii) $\probtransitions\fun{\sampledot \mid \state, \embeda\fun{\state,
\latentaction}}$, and (iii) the distribution over $\latentstates$ of sampling
$\state' \sim \probtransitions\fun{\sampledot \mid \state, \latentaction}$ and
then embedding $\latentstate' = \embed\fun{\state}$. %
% where $\rewards(\state, \latentaction) = \rewards(\state, \embeda(\state, \latentaction))$,
% $\probtransitions(\sampledot \mid \state, \latentaction) = \probtransitions(\sampledot \mid \state, \embeda(\state,
% \latentaction))$, and
% $\embed\probtransitions(\latentstate' \mid \state, \latentaction) = \condition{=}(\embed(\state'), \latentstate') \cdot \probtransitions(\state' \mid \state, \latentaction)$ for all $\latentstate' \in \latentstates$.%
% The expectation term enables the use of stochastic gradient descent methods to optimize the two losses, which make them particularly interesting in practice.
%These losses allow bounding the difference between values of the original MDP $\mdp$ and those of its latent abstraction $\latentmdp$.

Henceforth, we make the following assumptions.
%From now on, we restrict our attention to the following setting.
\begin{assumption}\label{assumption:ergodicity}
MDP $\mdp$ is ergodic.
\end{assumption}
\begin{assumption}\label{assumption:rewards}
Rewards of $\mdp$ are scaled in the interval $\mathopen[ -\frac{1}{2}, \frac{1}{2}\mathclose]$, i.e., $\rewards \colon \states \times \actions \to \mathopen[ -\frac{1}{2}, \frac{1}{2}\mathclose]$.
\end{assumption}
\begin{assumption}\label{assumption:labeling}
The embedding function preserves the labels, i.e., $\embed\fun{\state} = \latentstate \implies \labels\fun{\state} = \latentlabels\fun{\latentstate}$ for $\state \in \states$, $\latentstate \in \latentstates$. 
\end{assumption}
The first assumption amounts to considering a reset of the environment is part of $\mdp$ (see \citealt{DBLP:conf/nips/Huang20} for details).
Assuming $\latentmdp$ is discrete, all $\wassersteinsymbol{\distance_{\latentstates}}$ terms can be replaced by $\dtvsymbol$ %we have $\localtransitionloss{\stationary{}} = \dtv{\embed\probtransitions\fun{\sampledot \mid \state, \latentaction}}{\latentprobtransitions\fun{\sampledot \mid \embed\fun{\state}, \latentaction}}$
since Wasserstein coincides with TV when using the discrete metric.
In that case, (optimal) constants $\KR{\latentpolicy}$ and
$\KP{\latentpolicy}$ can be computed in polynomial time in $\latentmdp$ for any $\latentpolicy \in \latentmpolicies$.

Proofs of the claims made in the following two subsections are provided in supplementary material (Appendix~\ref{appendix:section:latent-space-model}).
% Moreover, w.l.o.g., $\KR{}$ allows us to scale the rewards of $\latentmdp$ in any desired interval.


\subsection{Bisimulation and Value Difference Bounds}

%With the end-goal of enabling the model checking
%and synthesis
%of $\mdp$ given a discrete latent space model $\tuple{\latentmdp, \embed,
%\embeda}$ of $\mdp$, 
We aim now at formally checking
%and 
%quantifying 
whether
the latent space model offers a good abstraction of the original MDP $\mdp$.
%or not.
To do so, we present bounds that link the two MDPs.
We extend the bounds from \citet{DBLP:conf/icml/GeladaKBNB19} to discrete spaces while additionally taking into account state
labels and discounted reachability events.
Moreover, we present new \emph{bisimulation} bounds in the local setting.
%We start with the notion of \emph{bisimulation}.

\smallparagraph{Bisimulation.}
A \emph{(probabilistic) bisimulation}
is a behavioral equivalence between states. 
Formally, a bisimulation on $\mdp$ is an equivalence relation $B_{\Phi}$ such
that for all $\state_1, \state_2 \in \states$ and $\Phi \subseteq
\set{\rewards, \labels}$, $\state_1 B_{\Phi} \state_2$ iff
$\probtransitions\fun{T \mid \state_1, \action} = \probtransitions\fun{T \mid
\state_2, \action}$,
$\labels\fun{\state_1} = \labels\fun{\state_2}$ if  $\labels
    \in \Phi$,  and $\rewards\fun{\state_1, \action} = \rewards\fun{\state_2, \action}$ if $\rewards \in \Phi$,
% $\labels\fun{\state_1} = \labels\fun{\state_2}$, $\rewards\fun{\state_1, \action} = \rewards\fun{\state_2, \action}$ and $\probtransitions\fun{T \mid \state_1, \action} = \probtransitions\fun{T \mid \state_2, \action}$
for each action $\action \in \actions$, and (Borel measurable) equivalence class $T \in \states / B_{\Phi}$.
Properties of bisimulation include trace, trajectory, and value equivalence \citep{DBLP:conf/popl/LarsenS89,DBLP:journals/ai/GivanDG03}.
%Of particular interest is the maximal bisimulation relation which defines the partition with the fewest elements.
The relation can be extended to compare two MDPs (in our case $\mdp$ and $\latentmdp$) by considering the disjoint union of their state space.
We denote the largest bisimulation relation by $\sim_{\Phi}$.

\smallparagraph{Pseudometrics.}
%Bisimulation relations are often too conservative when applied over uncountable state spaces.
%Intuitively, this is due to their ``all-or-nothing'' nature: two states having nearly identical but slight numerical difference in their rewards or probability transitions are treated as completely different. %, leading to different bisimulation partitions.
\citet{DBLP:journals/tcs/DesharnaisGJP04} introduced \emph{bisimulation pseudometrics} for continuous Markov processes, generalizing the notion of bisimilariy by assigning a \emph{bisimilarity distance} between states. % to quantify their behavioral similarity.
A \emph{pseudometric} $\bidistance$ satisfies symmetry % ($\bidistance\fun{x, y} = \bidistance\fun{y, x}$)
and the triangle inequality. %($\bidistance\fun{x, y} + \bidistance\fun{y, z} \geq \bidistance\fun{x, z}$)
%but not necessarily the identity of indiscernibles.
%, meaning that a zero distance does not imply that elements are the same ($\bidistance\fun{x, y} = 0 \centernot\iff x = y$).
%In the context of bisimulation pseudometrics, the zero distance characterizes the probabilistic bisimulation, i.e., $\bidistance\fun{x, y} \iff x \sim y$.

Probabilistic bisimilarity can be characterized by a logical family of functional expressions %\citep{DBLP:journals/tcs/DesharnaisGJP04}
derived from a logic $\logic$.
More specifically, given a policy $\policy \in \mpolicies{\mdp}$, we consider a family $\functionalexpr\fun{\policy}$ of real-valued functions $f$, parameterized by the discount factor $\discount$ and defining the semantics of $\logic$ in $\mdp_\policy$.
%Such a logic allows formalizing discounted properties, including reachability (e.g., \citealt{DBLP:conf/fsttcs/ChatterjeeAMR08}).
The related pseudometric $\bidistance_{\policy}$ is defined as: $\bidistance_\policy\fun{\state_1, \state_2} = \sup_{f \in \functionalexpr\fun{\policy}} \left| f\fun{\state_1} - f\fun{\state_2} \right|$, for all $\state_1,\state_2 \in \states$.
\iffalse
\begin{align*}
    && \bidistance_\policy\fun{\state_1, \state_2} =& \sup_{f \in \functionalexpr\fun{\policy}} \left| f\fun{\state_1} - f\fun{\state_2} \right|, & \forall \state_1, \state_2 \in \states.
\end{align*}
\fi
We distinguish between pseudometrics $\bidistance^{\rewards}_\policy$, characterized by functional expressions including rewards,
%(e.g., \citealt{DBLP:conf/birthday/FernsPK14}), 
and $\bidistance^{\labels}_\policy$, whose functional expressions are based on state labels.
%(e.g., \citealt{DBLP:conf/fossacs/ChenBW12}).
Let $\tilde{P}$ be the space of pseudometrics on $\states$ and $\varphi \in \set{\rewards, \labels}$.
Define $\Delta\colon \tilde{P} \to \tilde{P}$ so that $\Delta\fun{\distance^{\varphi}_\policy}\fun{\state_1, \state_2}$ is $\fun{1 - \discount} \left| \rewards_\policy\fun{\state_1} - \rewards_\policy\fun{\state_2} \right| \cdot \condition{\set{\rewards}}\fun{\varphi} + M$ where:
\[
M = \max \begin{cases}
\discount
    \wassersteindist{\distance_{\policy}^{\varphi}}{\probtransitions_\policy\fun{\sampledot
  \mid \state_1}}{\probtransitions_\policy\fun{\sampledot \mid \state_2}},\\
  \condition{\neq}\fun{\labels\fun{\state_1},
  \labels\fun{\state_2}} \cdot \condition{\set{\labels}}\fun{\varphi},
\end{cases}
\]
\iffalse
\begin{align*}
    \Delta\fun{\distance^{\varphi}_\policy}\fun{\state_1, \state_2} =& \fun{1 - \discount} \left| \rewards_\policy\fun{\state_1} - \rewards_\policy\fun{\state_2} \right| \cdot \condition{\set{\rewards}}\fun{\varphi} \\
    &+ \max \{ \discount
    \wassersteindist{\distance_{\policy}^{\varphi}}{\probtransitions_\policy\fun{\sampledot
  \mid \state_1}}{\probtransitions_\policy\fun{\sampledot \mid \state_2}},\\
    &\phantom{+ \max \{\} }\condition{\neq}\fun{\labels\fun{\state_1},
  \labels\fun{\state_2}} \cdot \condition{\set{\labels}}\fun{\varphi}\}
\end{align*}
\fi
then $\bidistance^{\varphi}_\policy$ is its unique fixed point whose kernel is $\sim_{\{\varphi\}}$, i.e., $\bidistance^{\varphi}_{\latentpolicy}(\state_1, \state_2) = 0$ iff
$\state_1 \!\! \sim_{\{\varphi\}} \! \! \state_2$
%$\state_1~\sim_{\{\varphi\}}~\state_2$
\citep{DBLP:conf/icalp/BreugelW01,DBLP:conf/birthday/FernsPK14},
% When considering rewards, \citet{DBLP:conf/uai/FernsPP05} showed that
% $ | \values{\policy}{}{\state_1} - \values{\policy}{}{\state_2} | \leq \nicefrac{\bidistance^{\rewards}_{\policy}\fun{\state_1, \state_2}}{1 - \discount}.
% $
and
% \begin{equation*}
%     \left| \values{\policy}{}{\state_1} - \values{\policy}{}{\state_2} \right| \leq \frac{\bidistance^{\rewards}_{\policy}\fun{\state_1, \state_2}}{1 - \discount}. \label{eq:reward-bisimulation}
% \end{equation*}
$ | \values{\policy}{}{\state_1} - \values{\policy}{}{\state_2} | \leq \nicefrac{\bidistance^{\rewards}_{\policy}\fun{\state_1, \state_2}}{1 - \discount}
$ \cite{DBLP:conf/uai/FernsPP05}.


%based on the modal logic of \cite{DBLP:conf/popl/LarsenS89}.
% More specifically, given a policy $\policy \in \mpolicies{\mdp}$, we consider a family $\functionalexpr\fun{\policy}$ of real-valued functions $f \colon \states \to [0, 1]$ which define the semantic of $\logic$ in $\mdp_\policy$, parameterized by the discount factor $\discount$.
% Such a logic allows formalizing discounted properties, including reachability.
% For any pair of states $\state_1, \state_2 \in \states$ and policy $\policy \in \mpolicies{\mdp}$, the related pseudometric $\logicbidistance_{\policy} \colon \states \times \states \to \mathopen[0, 1\mathclose]$ is defined as
% \[
%     \logicbidistance_\policy\fun{\state_1, \state_2} = \sup_{f \in \functionalexpr\fun{\policy}} \left| % f\fun{\state_1} - f\fun{\state_2} \right|.
% \]
% When focusing on discounted return values involving general measurable reward function $\rewards$, we further consider the pseudometric $\bidistance_\policy \colon \states \times \states \to \R$ introduced by \cite{DBLP:journals/siamcomp/FernsPP11}, with a characterization based on the value difference: % for any pair of states $\state_1, \state_2 \in \states$ and policy $\policy \in \mpolicies{\mdp}$,
% \[
%     \left| \values{\policy}{}{\state_1} - \values{\policy}{}{\state_2} \right| \leq \frac{\bidistance_{\policy}\fun{\state_1, \state_2}}{1 - \discount}.
% \]
%
% In this work, we distinguish two bisimulation pseudometrics.
% First, we consider the pseudometric $\bidistance \colon \states \times \states \to \R$ introduced by \cite{DBLP:conf/uai/FernsPP05}, with a characterization based on the discounted return value difference: for any pair of states $\state_1, \state_2 \in \states$ and policy $\policy \in \mpolicies{\mdp}$,
% \[
% \left| \values{\policy}{}{\state_1} - \values{\policy}{}{\state_2} \right| \leq \frac{\bidistance_{\policy}\fun{\state_1, \state_2}}{1 - \discount}.
% \]
% Second, we consider the logical characterization introduced by \cite{DBLP:journals/tcs/DesharnaisGJP04}, based on a family of functional expressions derived from the modal logic $\logic$ of \cite{DBLP:conf/popl/LarsenS89}.
% More specifically, given a policy $\policy \in \mpolicies{\mdp}$, we consider a family $\functionalexpr\fun{\policy}$ of real-valued functions $f \colon \states \to [0, 1]$ which define the semantic of $\logic$ in $\mdp_\policy$, parameterized by the discount factor $\discount$.
% Such a logic allows formalizing discounted properties, including reachability (e.g., \citealt{DBLP:conf/fsttcs/ChatterjeeAMR08}).
%  the related pseudometric $\logicbidistance_{\policy} \colon \states \times \states \to \mathopen[0, 1\mathclose]$ is defined as
% \[
%     \logicbidistance_\policy\fun{\state_1, \state_2} = \sup_{f \in \functionalexpr\fun{\policy}} \left| f\fun{\state_1} - f\fun{\state_2} \right|.
% \]
% for any pair of states $\state_1, \state_2 \in \states$ and policy $\policy \in \mpolicies{\mdp}$.
%usually expressed with the notion of trajectory, execution trace or value equality.

%connect bisimulation metrics with latent space models by showing 
%We now show %that 
\smallparagraph{Bisimulation distance bounds.}~We state that
the expected bisimulation distance between states and their latent abstraction is bounded by local losses.
%
%Let $\latentpolicy \in \latentmpolicies$, assume $\latentmdp$ is discrete and $\langle\KR{\latentpolicy}, \KP{\latentpolicy}\rangle$-Lipschitz.
%Let $\varphi = \set{\rewards, \labels}$, then given the induced stationary distribution $\stationary{\latentpolicy}$ in $\mdp$, let
Fix
$\latentpolicy \in \latentmpolicies$ and assume $\latentmdp$ is discrete and $\langle\KR{\latentpolicy}, \KP{\latentpolicy}\rangle$-Lipschitz.
%then
Given the induced stationary distribution $\stationary{\latentpolicy}$ in $\mdp$,
\begin{align}
    % \expectedsymbol{\state \sim \stationary{\latentpolicy}} \bidistance^{\varphi}_{\latentpolicy}\fun{\state, \embed\fun{\state}} \leq  \localrewardloss{\stationary{\latentpolicy}} \cdot \condition{\set{\rewards}}\fun{\varphi} + \discount \localtransitionloss{\stationary{\latentpolicy}} \frac{\fun{\KR{\latentpolicy} \cdot  \condition{\set{\rewards}}\fun{\varphi} + \discount \cdot \condition{\set{\labels}}\fun{\varphi}}}{1 - \discount \fun{\KP{\latentpolicy}\condition{\set{\rewards}}\fun{\varphi} + \condition{\set{\labels}}\fun{\varphi}}}.
    \expectedsymbol{\state \sim \stationary{\latentpolicy}} \bidistance^{\rewards}_{\latentpolicy}\fun{\state, \embed\fun{\state}} & \leq  \localrewardloss{\stationary{\latentpolicy}} + \discount \localtransitionloss{\stationary{\latentpolicy}} \frac{\KR{\latentpolicy}}{1 - \discount \KP{\latentpolicy}}, \notag \\
    \expectedsymbol{\state \sim \stationary{\latentpolicy}} \bidistance^{\labels}_{\latentpolicy}\fun{\state, \embed\fun{\state}} & \leq \frac{\discount \localtransitionloss{\stationary{\latentpolicy}}}{1 - \discount}. \label{eq:bidistance-bound}
\end{align}
%
The result provides us a general way to asses the quality of our latent
abstraction: the bisimulation distance between states and their embedding is
guaranteed to be small in average whenever local losses are small.
% Notice that bounds of Lemma~\ref{lemma:qvalues-diff-bound} and Lemma~\ref{lemma:bidistance-embedding} are further the same (modulo the discount scaling of Eq.~\ref{eq:reward-bisimulation}).
% This justifies the choice of using local losses to evaluate the quality of the latent model as an abstraction of $\mdp$.
% In addition, it also means that this bound on the expected bisimulation distance can be checked from samples using Theorem~\ref{thm:pac-qvaluedif}.
%
% We additionally provide abstraction quality bounds in terms of bisimulation metrics.
% This allows formalizing the quality of the latent space model as an abstraction of $\mdp$ as follows.
%
%We further formalize the quality of the abstraction by showing that the bisimulation distance between states with same representation can be upper-bounded by local losses:
%
This allows us formalizing the quality of the abstraction by bounding the bisimulation distance between states with same representation by local losses:
%
%\begin{lemma}\label{lemma:bidistance-abstraction-quality}
% Let $\latentpolicy \in \latentmpolicies$, assume $\latentmdp$ is discrete and $\langle\KR{\latentpolicy}, \KP{\latentpolicy}\rangle$-Lipschitz.
% Let $\varphi = \set{\rewards, \labels}$ and $\stationary{\latentpolicy}$ be the stationary distribution of $\mdp$ induced by $\latentpolicy$.
%for any pair of states $\state_1, \state_2 \in \states$ such that $\embed\fun{\state_1} = \embed\fun{\state_2}$,
for any states $\state_1, \state_2 \in \states$ with $\embed\fun{\state_1} = \embed\fun{\state_2}$,
\begin{align}
    \bidistance^{\rewards}_{\latentpolicy}\fun{\state_1, \state_2} \leq& \left[\localrewardloss{\stationary{\latentpolicy}} + \frac{
    \discount \localtransitionloss{\stationary{\latentpolicy}} \KR{\latentpolicy}}{1 - \discount \KP{\latentpolicy}}\right] \fun{\stationary{\latentpolicy}^{-1}\fun{\state_1} + \stationary{\latentpolicy}^{-1}\fun{\state_2}}, \notag \\
    \bidistance^{\labels}_{\latentpolicy}\fun{\state_1, \state_2} \leq&  \frac{\discount \localtransitionloss{\stationary{\latentpolicy}}}{1 - \discount} \fun{\stationary{\latentpolicy}^{-1}\fun{\state_1} + \stationary{\latentpolicy}^{-1}\fun{\state_2}}. \label{eq:bidistance-abstraction-quality}
\end{align}
%\end{lemma}


\smallparagraph{Value difference bounds.}~%
Considering discounted returns or a specific event, the quality of the latent abstraction induced by $\embed$ and $\embeda$ can be in particular formalized by means of \emph{value difference bounds}.
%as demonstrated by the theoretical bounds from \cite{DBLP:conf/icml/GeladaKBNB19}.
% Given any policy in $\latentmpolicies$, such bounds link the values of the original MDP $\mdp$ and those of its latent abstraction $\latentmdp$.
% In the following, we show that we can check them to any degree of accuracy with high probability by means of simulations.
%We adapt these bounds \cite{DBLP:conf/icml/GeladaKBNB19} in the discrete setting for discounted return, and we derive new ones in the case of discounted reachability.
These bounds can be intuitively derived by taking the value function as a
real-valued function from $\functionalexpr\fun{\policy}$.
% \begin{lemma}\label{lemma:qvalues-diff-bound}
Let $\labelset{C}, \labelset{T} \subseteq \atomicprops$, $\varphi \in \set{\until{\labelset{C}}{\labelset{T}}, \eventually \labelset{T}}$ and $\KV = \min\fun{\nicefrac{\Rmax{\latentpolicy}}{1 - \discount}, \nicefrac{\KR{\latentpolicy}}{1 - \discount \KP{\latentpolicy}}}$,
then, %given the induced stationary distribution $\stationary{\latentpolicy}$ in $\mdp$,
\begin{align}
    \expectedsymbol{\state, \latentaction \sim \stationary{\latentpolicy}} \left| \qvalues{\latentpolicy}{}{\state}{\latentaction} - \latentqvalues{\latentpolicy}{}{\embed\fun{\state}}{\latentaction} \right| & \leq \frac{\localrewardloss{\stationary{\latentpolicy}} + \discount \KV \localtransitionloss{\stationary{\latentpolicy}}}{1 - \gamma}, \notag \\ %\tag{\citealt{DBLP:conf/icml/GeladaKBNB19}} \\
    \expectedsymbol{\state, \latentaction \sim \stationary{\latentpolicy}} \left| \qvalues{\latentpolicy}{\varphi}{\state}{\latentaction} - \latentqvalues{\latentpolicy}{\varphi}{\embed\fun{\state}}{\latentaction} \right| & \leq \frac{\discount \localtransitionloss{\stationary{\latentpolicy}}}{1 - \discount}. \label{eq:value-dif-bound}
\end{align}
% \end{lemma}
%
% \begin{lemma}\label{lem:quality-latent-space}
    % Let $\latentpolicy \in \latentmpolicies$ and assume $\latentmdp$ is discrete and $\langle \KR{\latentpolicy}, \KP{\latentpolicy} \rangle$-Lipschitz.
    % Let $\labelset{C}, \labelset{T} \subseteq \atomicprops$, $\varphi = \set{\until{\labelset{C}}{\labelset{T}}, \eventually \labelset{T}}$, $\KV = \min\fun{\nicefrac{\Rmax{\latentpolicy}}{1 - \discount}, \nicefrac{\KR{\latentpolicy}}{1 - \discount \KP{\latentpolicy}}}$, and $\stationary{\latentpolicy}$ be the induced stationary distribution in $\mdp$.
    Moreover, for any states $\state_1, \state_2 \in \states$ with $\embed\fun{\state_1} = \embed\fun{\state_2}$,
    \begin{align}
      &\left| \values{\latentpolicy}{}{\state_1} {-}
      \values{\latentpolicy}{}{\state_2} \right| \leq
      \frac{\localrewardloss{\stationary{\latentpolicy}} {+} \discount \KV
      \localtransitionloss{\stationary{\stationary{\latentpolicy}}}}{1 {-}
      \discount} \fun{\stationary{\latentpolicy}^{-1}\fun{\state_1} {+}
      \stationary{\latentpolicy}^{-1}\fun{\state_2}}, \notag\\
      %
      &\left| \values{\latentpolicy}{\varphi}{\state_1} {-}
      \values{\latentpolicy}{\varphi}{\state_2} \right| \leq \frac{\discount
      \localtransitionloss{\stationary{\latentpolicy}}}{1 {-} \discount}
      \fun{\stationary{\latentpolicy}^{-1}\fun{\state_1} {+}
      \stationary{\latentpolicy}^{-1}\fun{\state_2}}. \label{eq:value-diff-abstraction-quality}
    \end{align}
% \end{lemma}
%
Intuitively, when local losses are sufficiently small, then (i) the expected value difference of states and their embeddings that are likely to be seen under a latent policy is also small, and (ii) states with the same embedding have close values. 

\subsection{Checking the Quality of the Abstraction}
While bounding the difference between values offered by $\latentpolicy \in \latentmpolicies$ in $\mdp$ and $\latentmdp$ is theoretically possible using $\localrewardloss{\stationary{\latentpolicy}}$ and $\localtransitionloss{\stationary{\latentpolicy}}$, we need to accurately approximate these losses from samples to further offer practical guarantees (recall that $\mdp$ is unknown).
%
Although the agent is able to produce execution traces by
interacting with $\mdp$,
%In a BSCC of $\mdp_{\latentpolicy}$, this means the agent produces $\state, \latentaction \sim \stationary{\latentpolicy}$ at each step in the limit,
%as well as a reward $\reward$ resulting from the transition to the next state.
%================================================================
%Two issues arise: First, the reward obtained does not necessarily come from a function defined over state-action pairs.
%Second,
%================================================================
estimating the expectation over the Wasserstein is intractable.
Intuitively, even if approximating Wasserstein from samples is possible (e.g., \citealp{DBLP:conf/aistats/GenevayCBCP19}), 
this would require access to a generative model for $\probtransitions\fun{\sampledot \mid \state, \action}$ (e.g., \citealp{DBLP:journals/ml/KearnsMN02}) from which we would have to draw a sufficient number of samples for each $\state, \action$ drawn from $\stationary{\latentpolicy}$ to then be able to estimate the expectation. %of $\localtransitionloss{\stationary{\latentpolicy}}$.
\citet{DBLP:conf/icml/GeladaKBNB19} overcome this issue by assuming a deterministic MDP%
% (i.e., $\probtransitions({\sampledot \mid \state, \action})$ is Dirac)
, which allows optimizing an approximation of $\localtransitionloss{\stationary{\latentpolicy}}$ through gradient descent.
%
To deal with general MDPs, we study
%lower and
an upper bound on $\localtransitionloss{\stationary{\latentpolicy}}$ that can be efficiently approximated from samples:
%\begin{lemma}\label{lemma:local-loss-upper-bounds}
%Let $\stationary{\latentpolicy}$ be a stationary distribution of $\mdp$ under $\latentpolicy \in \latentmpolicies$, then
\begin{align*}
    %&\localrewardloss{\stationary{\latentpolicy}} \leq \expectedsymbol{\state, \latentaction, \state' \sim \stationary{\latentpolicy}}\left| \rewards\fun{\state, \latentaction, \state'} - \latentrewards\fun{\embed\fun{\state}, \latentaction} \right| = \localrewardlossupper{\stationary{\latentpolicy}}, \\ %\label{eq:rewardloss-upperbound} \\
    %\wassersteindist{\distance_{\latentstates}}{\embed\stationary{\latentpolicy}}{\latentprobtransitions_{\embed}\stationary{\latentpolicy}} \leq \,
    &\localtransitionloss{\stationary{\latentpolicy}} \leq \expectedsymbol{\state, \latentaction, \state' \sim \stationary{\latentpolicy}} \wassersteindist{\distance_{\latentstates}}{\embed\fun{\sampledot \mid \state'}}{\latentprobtransitions\fun{\sampledot \mid \embed\fun{\state}, \latentaction}} = \localtransitionlossupper{\stationary{\latentpolicy}}, \label{eq:transitionloss-bounds}
\end{align*}
where $\stationary{\latentpolicy}(\state, \latentaction, \state')=  \stationary{\latentpolicy}(\state, \latentaction) \cdot \probtransitions(\state' \mid \state, \latentaction)$
and $\embed(\latentstate \mid \state) = \condition{=}(\embed\fun{\state}, \latentstate)$. %$\embed\stationary{\latentpolicy}\fun{\latentstate} = \expectedsymbol{\state \sim \stationary{\latentpolicy}}{\embed\fun{\latentstate \mid \state}}$, and
% $\latentprobtransitions_\embed\steadystate{\policy}\fun{\latentstate} = \expectedsymbol{\state, \latentaction \sim \stationary{\latentpolicy}}{\latentprobtransitions\fun{\latentstate \mid \embed\fun{\state}, \latentaction}}$.
%\end{lemma}
%
We now provide \emph{probably approximately correct} (PAC) guarantees for estimating local losses for discrete latent models, derived from 
%the Hoeffding's inequalities \citep{10.2307/2282952:hoeffding}.
the \citeauthor{10.2307/2282952:hoeffding}'s inequalities. %(\citeyear{10.2307/2282952:hoeffding}).
%
\begin{lemma}\label{lemma:pac-loss}
Suppose $\latentmdp$ is discrete and the agent interacts with $\mdp$ by executing $\latentpolicy \in \latentpolicies$, thus producing $ \tuple{\seq{\state}{T}, \seq{\latentaction}{T - 1}, \seq{\reward}{T - 1}} \sim \stationary{\latentpolicy}$.
Let $\error, \proberror \in \mathopen]0, 1\mathclose[$ and denote by
\begin{align*}
    \localrewardlossapprox{\stationary{\latentpolicy}} &= \frac{1}{T} \sum_{t = 0}^{T - 1} \left| \reward_t - \latentrewards\fun{\embed\fun{\state_t}, \latentaction_t}  \right|, \\
    % \localtransitionlossapprox{\stationary{\latentpolicy}} &= \frac{1}{2T} \sum_{t = 0}^{T - 1} \sum_{\latentstate' \in \latentstates} \left| \embed\fun{\latentstate' \mid \state_{t + 1}} - \latentprobtransitions\fun{\latentstate' \mid \embed\fun{\state_t}, \latentaction_t} \right|.
    \localtransitionlossapprox{\stationary{\latentpolicy}} &= \frac{1}{T} \sum_{t = 0}^{T - 1} \left[ 1 - \latentprobtransitions\fun{\embed\fun{\state_{t + 1}} \mid \embed\fun{\state_t}, \latentaction_t} \right].
\end{align*}
Then after $T \geq \left\lceil \nicefrac{- \log\fun{\frac{\proberror}{4}}}{2
  \error^2} \right\rceil$ steps,
    %$\left| \localrewardlossupper{\stationary{\latentpolicy}} -
    $\left| \localrewardloss{\stationary{\latentpolicy}} -
    \localrewardlossapprox{\stationary{\latentpolicy}}  \right| \leq \error$,
    $\left| \localtransitionlossupper{\stationary{\latentpolicy}} -
    \localtransitionlossapprox{\stationary{\latentpolicy}} \right| \leq
    \error$
with probability at least $1 - \proberror$.
\end{lemma}
%
%
%As a direct consequence of Lem.~\ref{lemma:local-loss-upper-bounds} and Lem.~\ref{lemma:pac-loss}, 
%$\localrewardloss{\stationary{\latentpolicy}} \leq \localrewardlossapprox{\stationary{\latentpolicy}} + \error$  and
%$\localtransitionloss{\stationary{\latentpolicy}} \leq \localtransitionlossapprox{\stationary{\latentpolicy}} + \error$ with probability at least $1 - \proberror$ when a sufficient number of simulation steps has been performed.
%The conjunction of Lemma~\ref{lemma:qvalues-diff-bound} and Lemma~\ref{lemma:pac-loss}
This yields the following Theorem, finally allowing to check the  abstraction quality as a bounded value difference.
\begin{theorem}\label{thm:pac-qvaluedif}
    Let $\latentpolicy \in \latentmpolicies$ and assume $\latentmdp$ is discrete and $\langle \KR{\latentpolicy}, \KP{\latentpolicy} \rangle$-Lipschitz.
    Let $\labelset{C}, \labelset{T} \subseteq \atomicprops$, $\varphi \in \set{\until{\labelset{C}}{\labelset{T}}, \eventually \labelset{T}}$ and $\KV = \min\fun{\nicefrac{\Rmax{\latentpolicy}}{1 - \discount}, \nicefrac{\KR{\latentpolicy}}{1 - \discount \KP{\latentpolicy}}}$.
    Let $\error, \proberror \in \mathopen]0, 1\mathclose[$ and $\stationary{\latentpolicy}$ be the stationary distribution of $\mdp_{\latentpolicy}$.
    %Assume the agent executes $\latentpolicy$ in $\mdp$, then
    Then, after $T \geq \left\lceil \nicefrac{- \log\fun{\frac{\proberror}{4}} \fun{1 +
    \discount \KV}^2}{2\error^2\fun{1 - \discount}^2}\right\rceil$ interaction steps through $\stationary{\latentpolicy}$,
    \begin{align*}
        \expectedsymbol{\state, \latentaction \sim \stationary{\latentpolicy}}
        \left| \qvalues{\latentpolicy}{}{\state}{\latentaction} -
        \latentqvalues{\latentpolicy}{}{\embed\fun{\state}}{\latentaction}
        \right| & \leq
        \frac{\localrewardlossapprox{\stationary{\latentpolicy}} + \discount
        \KV \localtransitionlossapprox{\stationary{\latentpolicy}}}{1 -
      \gamma} + \error, \\
        \expectedsymbol{\state, \latentaction \sim \stationary{\latentpolicy}} \left| \qvalues{\latentpolicy}{\varphi}{\state}{\latentaction} - \latentqvalues{\latentpolicy}{\varphi}{\embed\fun{\state}}{\latentaction} \right| & \leq \frac{\discount \localtransitionlossapprox{\stationary{\latentpolicy}}}{1 - \discount} + \frac{\discount \error}{1 + \discount \KV}
    \end{align*}
    with probability at least $1 - \proberror$.
\end{theorem}
%
