\section{Policy variant (no action encoder)}
We learn the latent space model $\tuple{\latentmdp_{\decoderparameter}, \embed_{\encoderparameter}, \embeda_{\encoderparameter, \decoderparameter}}$
%by adapting the \emph{Wasserstein autoencoder} (WAE) framework introduced by \cite{DBLP:conf/iclr/TolstikhinBGS18}.
by minimizing $\wassersteindist{\transitiondistance}{\stationary{\policy}}{\stationarydecoder}$ such that
% \begin{align*}
%     \stationarydecoder\fun{\sampledot, \sampledot, \reward, \state'} = \stationarydecoder\fun{\reward, \state'} =  \int_{\latentstates} \decoder\fun{\reward, \state' \mid \latentstate'} \latentstationaryprior\fun{\latentstate'} \, d\latentstate' %= \expectedsymbol{\state, \action \sim \stationary{\policy}} \expectedsymbol{\latentstate \sim \embed_{\encoderparameter}\fun{\sampledot \mid \state}}\expectedsymbol{\latentstate' \sim \latentprobtransitions_{\decoderparameter}\fun{\sampledot \mid \latentstate, \action}} \decoder\fun{\reward, \state' \mid \latentstate'}
% \end{align*}
% for some latent stationary distribution $\latentstationaryprior$.
% Notice that we are here again omitting the label terms since we assume a perfect reconstruction of labels by encoding them directly into the latent space.
% Moreover, we do not desire to learn an action embedding function since we assume the original and latent space models sharing the same action space, we thus make $\decoder$ and $\latentstationaryprior$ dependent on actions generated by $\stationary{\policy}$ as follows:
% \begin{align*}
%     \decoder\fun{\reward, \state' \mid \latentstate'} = \expectedsymbol{\state, \action \sim \stationary{\policy}}\expectedsymbol{\latentstate \sim \embed_{\encoderparameter}\fun{\sampledot \mid \state}}{\decoder\fun{\reward, \state' \mid \latentstate, \action, \latentstate'}} &&\text{ and }&&
%     \stationarydecoder\fun{\latentstate'} = \expectedsymbol{\state, \action \sim \stationary{\policy}} \expectedsymbol{\latentstate \sim \embed_{\encoderparameter}\fun{\sampledot \mid \state}}{\latentprobtransitions_{\decoderparameter}\fun{\latentstate' \mid \latentstate, \action}}.
% \end{align*}
\begin{align}
    \stationarydecoder\fun{\state, \action, \reward, \state'} =  \int_{\latentstates \times \latentstates} \decoder\fun{\state, \action, \reward, \state' \mid \latentstate, \latentstate'} \, d\latentstationaryprior\fun{\latentstate, \latentstate'} \label{eq:policy-stationary-decoder}
    %= \expectedsymbol{\state, \action \sim \stationary{\policy}} \expectedsymbol{\latentstate \sim \embed_{\encoderparameter}\fun{\sampledot \mid \state}}\expectedsymbol{\latentstate' \sim \latentprobtransitions_{\decoderparameter}\fun{\sampledot \mid \latentstate, \action}} \decoder\fun{\reward, \state' \mid \latentstate'}
\end{align}
where $\latentstationaryprior$ denotes the stationary distribution of the latent model $\latentmdp_{\decoderparameter}$ and $\latentstate, \latentstate'$ both follow this distribution with $\latentstationaryprior\fun{\latentstate, \latentstate'} = \latentstationaryprior\fun{\latentstate} \cdot \expectedsymbol{\latentaction \sim \latentpolicy_{\decoderparameter}\fun{\cdot \mid \latentstate}} \latentprobtransitions_{\decoderparameter}\fun{\latentstate' \mid \latentstate, \latentaction}$.

Given the definition of $\stationarydecoder$, it is sufficient to find an encoder $\embed$ whose marginal is given by $Q\fun{\latentstate, \latentstate'} = \expectedsymbol{\state, \state' \sim \stationary{\policy}} \embed\fun{\latentstate, \latentstate' \mid \state, \state'}$ % = \embed_{\encoderparameter}\stationary{\policy}$,
and identical to the prior distribution $\latentstationaryprior$ instead of finding the optimal coupling in the primal definition of \wassersteindist{\transitiondistance}{\stationary{\policy}}{\stationarydecoder} \citep{Bousquet2017FromOT,DBLP:conf/iclr/TolstikhinBGS18}.
\begin{theorem}
Let $\mdp$ be an MDP with state-action space $\states \times \actions$, $\latentmdp_{\decoderparameter}
$ be an ergodic latent MDP of $\mdp$ with state-action space $\latentstates \times \latentactions$ and reward function $\latentrewards_{\decoderparameter}$,
%= \tuple{\latentstates, \latentactions, \latentprobtransitions_{\decoderparameter}, \latentrewards_{\decoderparameter}, \latentlabels, \atomicprops, \zinit}$ be a latent MDP of $\mdp$,
$\latentpolicy_{\decoderparameter}$ be a policy for $\latentmdp_{\decoderparameter}$, $\stationarydecoder \in \distributions{\states \times \actions \times \R \times \states}$ and $\decoder \colon \latentstates^2 \to \distributions{\states \times \actions \times \R \times \states}$ be respectively a generator and a decoder as defined in Eq.~\ref{eq:policy-stationary-decoder}, $\generative_{\decoderparameter}\colon \latentstates \to \states$, $\embeda_{\decoderparameter} \colon \latentstates \times \latentactions \to \actions$, and
\begin{equation*}
    G_{\decoderparameter} \colon \latentstates^2 \to \states \times \actions \times \R \times \states, \; \tuple{\latentstate, \latentstate'} \mapsto \tuple{\generative_{\decoderparameter}\fun{\latentstate}, \expectedsymbol{\latentaction \sim \latentpolicy_{\decoderparameter}\fun{\cdot \mid \latentstate}}\embeda_{\decoderparameter}\fun{\latentstate, \latentaction}, \expectedsymbol{\latentaction \sim \latentpolicy_{\decoderparameter}\fun{\cdot \mid \latentstate}}{\latentrewards_{\decoderparameter}\fun{\latentstate, \latentaction}}, \generative_{\decoderparameter}\fun{\latentstate'}}
\end{equation*}
\begin{enumerate}[(i)]
\iffalse
\item If $\latentpolicy_{\decoderparameter}$ is deterministic (write $\latentpolicy_{\decoderparameter}\colon \latentstates \to \latentactions$), then so is $\decoder$ when the distribution is characterized by $G_\decoderparameter$, and
%     \begin{align*}
%         \inf_{\coupling \in \couplings{\stationary{\policy}}{\stationarydecoder}} \, \expectedsymbol{\tau, \tau' \sim \coupling} \transitiondistance\fun{\tau, \tau'} = \inf_{Q: Q = \latentstationaryprior} \, \expectedsymbol{\state, \action, \reward, \state' \sim \stationary{\policy}} \, \expectedsymbol{\latentstate, \latentstate' \sim \embed\fun{\sampledot \mid \state, \state'}} \transitiondistance\fun{\tuple{\state, \action, \reward, \state', \labels\fun{\state'}}, G_{\decoderparameter}\fun{\latentstate, \latentstate'}}
% \end{align*}
\begin{align*}
    \wassersteindist{\tracedistance}{\stationary{\policy}}{\stationarydecoder} = \inf_{\embed:\, Q = \latentstationaryprior} \, \expectedsymbol{\state, \action, \reward, \state' \sim \stationary{\policy}} \, \expected{\latentstate, \latentstate' \sim \embed\fun{\sampledot \mid \state, \state'}}{\distance_{\actions}\fun{\action, \embeda_{\decoderparameter}\fun{\latentstate, \latentpolicy_{\decoderparameter}\fun{\latentstate}}} + \left| r - \rewards_{\decoderparameter}\fun{\latentstate, \latentpolicy_{\decoderparameter}\fun{\latentstate}} \right| + \distance_{\states}\fun{\state', \generative_{\decoderparameter}\fun{\latentstate'}}}
\end{align*}
\item Otherwise, assume $\states = \R^{m}$, $\actions = \R^{n}$, and $\decoder\fun{\cdot \mid \latentstate, \latentstate'}$ have mean values $G_{\decoderparameter}\fun{\latentstate, \latentstate'} \in \R^{k}$ and marginal variances $\sigma_1^2, \dots, \sigma^2_{k} \geq 0$ for all $\latentstate, \latentstate' \in \latentstates$ with $k = 2 m + n + 1$. Take $\norm{.}^2_2$ as $\distance_{\states}$, $\distance_{\actions}$, and reward distance metric, then
\begin{align*}
    &\wassersteindist{\tracedistance}{\stationary{\policy}}{\stationarydecoder} \\
    \leq& \sum_{i=m+1}^{k} \sigma_i^2 + \inf_{\embed: \, Q = \latentstationaryprior} \, \expectedsymbol{\state, \action, \reward, \state' \sim \stationary{\policy}} \, \expected{\latentstate, \latentstate' \sim \embed\fun{\sampledot \mid \state, \state'}}{\norm{\action - \expectedsymbol{\latentaction \sim \latentpolicy_{\decoderparameter}\fun{\cdot \mid \latentstate}}\embeda_{\decoderparameter}\fun{\latentstate, \latentaction}} + \left| r - \expectedsymbol{\latentaction \sim \latentpolicy_{\decoderparameter}\fun{\cdot \mid \latentstate}}{\latentrewards_{\decoderparameter}\fun{\latentstate, \latentaction}} \right| + \norm{\state' - \generative_{\decoderparameter}\fun{\latentstate'}}}^2
\end{align*}
\fi
\item If $\latentpolicy_{\decoderparameter}$ is deterministic, then so is $\decoder$ when $G_\decoderparameter$ is its Dirac function, and
\begin{align*}
    &\wassersteindist{\tracedistance}{\stationary{\policy}}{\stationarydecoder} \\ =& \inf_{\embed: \, Q = \latentstationaryprior} \, \expectedsymbol{\state, \action, \reward, \state' \sim \stationary{\policy}} \, \expectedsymbol{\latentstate, \latentstate' \sim \embed\fun{\sampledot \mid \state, \state'}}\expected{\latentaction \sim \latentpolicy_{\decoderparameter}\fun{\sampledot\mid \latentstate}}{\distance_\states\fun{\state, \generative_{\decoderparameter}\fun{\latentstate}} + \distance_{\actions}\fun{\action, \embeda_{\decoderparameter}\fun{\latentstate, \latentaction}} + \left| r - \rewards_{\decoderparameter}\fun{\latentstate, \latentaction} \right| + \distance_{\states}\fun{\state', \generative_{\decoderparameter}\fun{\latentstate'}}}
\end{align*}
\item Otherwise, assume $\states = \R^{m}$, $\actions = \R^{n}$, and $\decoder\fun{\cdot \mid \latentstate, \latentstate'}$ have mean values $G_{\decoderparameter}\fun{\latentstate, \latentstate'} \in \R^{k}$ and marginal variances $\sigma_1^2, \dots, \sigma^2_{k} \geq 0$ for all $\latentstate, \latentstate' \in \latentstates$ with $k = 2 m + n + 1$.
%Take $\norm{.}^2_2$ as $\distance_{\states}$, $\distance_{\actions}$, and reward distance metric, then
Take the $\ell_2$-distance for $\distance_{\states}$, $\distance_{\actions}$,
%and let $c\fun{\vect{x}, \vect{y}} = \norm{\vect{x} - \vect{y}}^2$, 
then we bound the squared $2$-Wasserstein distance as:
\begin{align*}
    &\wassersteinsymbol{2}^2\fun{\stationary{\policy},{\stationarydecoder}} \\
    \leq& \sum_{i=1}^{k} \sigma_i^2 + \inf_{\embed: \, Q = \latentstationaryprior} \, \expectedsymbol{\state, \action, \reward, \state' \sim \stationary{\policy}} \, \expectedsymbol{\latentstate, \latentstate' \sim \embed\fun{\sampledot \mid \state, \state'}}\expected{\latentaction \sim \latentpolicy_{\decoderparameter}\fun{\cdot \mid \latentstate}}{
    %    \norm{\state - \generative_{\decoderparameter}\fun{\latentstate}}^2 + \norm{\action - \embeda_{\decoderparameter}\fun{\latentstate, \latentaction}}^2 + \left| r - {\latentrewards_{\decoderparameter}\fun{\latentstate, \latentaction}} \right|^2 + \norm{\state' - \generative_{\decoderparameter}\fun{\latentstate'}}^2
        \distance_{\states}^2\fun{\state, \generative_{\decoderparameter}\fun{\latentstate}} + \distance_{\actions}^2\fun{\action, \embeda_{\decoderparameter}\fun{\latentstate, \latentaction}} + \left| r - {\latentrewards_{\decoderparameter}\fun{\latentstate, \latentaction}} \right|^2 + \distance_{\states}^2\fun{\state', \generative_{\decoderparameter}\fun{\latentstate'}}
    }
\end{align*}
\end{enumerate}
\end{theorem}
\begin{proof}
%The results follow directly from \citet[Theorem~1 and Corollary~1]{DBLP:conf/iclr/TolstikhinBGS18}.
First, assume $\latentpolicy_{\decoderparameter}$ is deterministic (write $\latentpolicy_{\decoderparameter}\colon \latentstates \to \latentactions$) and assume that $\decoder$ is deterministic with $G_{\decoderparameter}$ as its Dirac function.
In that case, we have
\begin{align*}
    \wassersteindist{\tracedistance}{\stationary{\policy}}{\stationarydecoder} &=
    \inf_{\coupling \in \couplings{\stationary{\policy}}{\stationarydecoder}} \, \expectedsymbol{\tau, \tau' \sim \coupling} \transitiondistance\fun{\tau, \tau'}\\
    &=\inf_{\embed: \, Q = \latentstationaryprior} \, \expectedsymbol{\state, \action, \reward, \state' \sim \stationary{\policy}} \, \expected{\latentstate, \latentstate' \sim \embed\fun{\sampledot \mid \state, \state'}}{\distance_{\states}\fun{\state, \generative_{\decoderparameter}\fun{\latentstate}}+\distance_{\actions}\fun{\action, \embeda_{\decoderparameter}\fun{\latentstate, \latentpolicy_{\decoderparameter}\fun{\latentstate}}} + \left| r - \rewards_{\decoderparameter}\fun{\latentstate, \latentpolicy_{\decoderparameter}\fun{\latentstate}} \right| + \distance_{\states}\fun{\state', \generative_{\decoderparameter}\fun{\latentstate'}}} \tag{\citet[Theorem~1]{DBLP:conf/iclr/TolstikhinBGS18}}\\
    &= \inf_{\embed: \, Q = \latentstationaryprior} \, \expectedsymbol{\state, \action, \reward, \state' \sim \stationary{\policy}} \, \expectedsymbol{\latentstate, \latentstate' \sim \embed\fun{\sampledot \mid \state, \state'}}\expected{\latentaction \sim \latentpolicy_{\decoderparameter}\fun{\sampledot\mid \latentstate}}{\distance_{\states}\fun{\state, \generative_{\decoderparameter}\fun{\latentstate}} + \distance_{\actions}\fun{\action, \embeda_{\decoderparameter}\fun{\latentstate, \latentaction}} + \left| r - \rewards_{\decoderparameter}\fun{\latentstate, \latentaction} \right| + \distance_{\states}\fun{\state', \generative_{\decoderparameter}\fun{\latentstate'}}} \tag{since $\latentpolicy_{\decoderparameter}$ is deterministic}
\end{align*}

Second, assume $\states = \R^{m}$, $\actions = \R^{n}$, and $\decoder\fun{\cdot \mid \latentstate, \latentstate'}$ have mean values $G_{\decoderparameter}\fun{\latentstate, \latentstate'} \in \R^{k}$ and marginal variances $\sigma_1^2, \dots, \sigma^2_{k} \geq 0$ for all $\latentstate, \latentstate' \in \latentstates$ with $k = 2 m + n + 1$. Take $\norm{.}^2_2$ as $\distance_{\states}$, $\distance_{\actions}$, and reward distance metric, then %let $g_{\decoderparameter} \colon \latentstates^2 \to \states \times \actions \times \R \times \states , \; \tuple{\latentstate, \latentstate'} \mapsto \tuple{\generative_{\decoderparameter}\fun{\latentstate},  \expectedsymbol{\latentaction \sim \latentpolicy_{\decoderparameter}\fun{\cdot \mid \latentstate}}\embeda_{\decoderparameter}\fun{\latentstate, \latentaction}, \expectedsymbol{\latentaction \sim \latentpolicy_{\decoderparameter}\fun{\cdot \mid \latentstate}}{\latentrewards_{\decoderparameter}\fun{\latentstate, \latentaction}}, \generative_{\decoderparameter}\fun{\latentstate'}} $, we have
we have
\begin{align*}
    &\wassersteindist{\tracedistance}{\stationary{\policy}}{\stationarydecoder} \\
    \leq& \sum_{i=m+1}^{k} \sigma_i^2 + \inf_{\coupling \in \couplings{\stationary{\policy}}{\latentstationaryprior}} \,
    \expected{\tuple{\state, \action, \reward, \state'}, \tuple{\latentstate', \latentstate'}\sim \coupling}{\norm{\tuple{\state, \action, \reward, \state'} - G_{\decoderparameter}\fun{\latentstate, \latentstate'}}^2} \tag{\citet[Corollary~1]{DBLP:conf/iclr/TolstikhinBGS18}}\\
    %\expected{\trace, \tuple{\latentstate', \latentstate'}\sim \coupling}{\norm{\trace - g_{\decoderparameter}\fun{\latentstate, \latentstate'}}^2} \tag{\citet[Corollary~1]{DBLP:conf/iclr/TolstikhinBGS18}}\\
    =& \sum_{i=m+1}^{k} \sigma_i^2 + \wassersteindist{c}{\stationary{\policy}}{\latentstationaryprior}
    %\leq& \sum_{i=m+1}^{k} \sigma_i^2 + \inf_{\embed: \, Q = \latentstationaryprior} \, \expectedsymbol{\state, \action, \reward, \state' \sim \stationary{\policy}} \, \expected{\latentstate, \latentstate' \sim \embed\fun{\sampledot \mid \state, \state'}}{\norm{\action - \expectedsymbol{\latentaction \sim \latentpolicy_{\decoderparameter}\fun{\cdot \mid \latentstate}}\embeda_{\decoderparameter}\fun{\latentstate, \latentaction}} + \left| r - \expectedsymbol{\latentaction \sim \latentpolicy_{\decoderparameter}\fun{\cdot \mid \latentstate}}{\latentrewards_{\decoderparameter}\fun{\latentstate, \latentaction}} \right| + \norm{\state' - \generative_{\decoderparameter}\fun{\latentstate'}}}^2
\end{align*}
for $c \colon \fun{\states \times \actions \times \R \times \states} \times \latentstates^2 \to \left[0, + \infty \right[, \; {\tuple{\state, \action, \reward, \state'}, \tuple{\latentstate, \latentstate'}} \mapsto \norm{\tuple{\state, \action, \reward, \state'} - g_{\decoderparameter}\fun{\latentstate, \latentstate'}}^2$.
Observe that $\latentstationaryprior\fun{\latentstate, \latentstate'} = \int_{\latentvariables \times \latentvariables} \latentstationaryprior\fun{\latentstate, \latentstate' \mid \latentvariable, \latentvariable'} \,d\Xi\fun{\latentvariable, \latentvariable'}$ for any latent space $\latentvariables$ and latent prior distribution $\Xi \in \distributions{\latentvariables^2}$.
When $\latentvariables = \latentstates$, one can set $\Xi$ to $\latentstationaryprior$ to obtain
$\latentstationaryprior\fun{\latentstate_1, \latentstate_1'} = \int_{\latentstates \times \latentstates} \condition{=}\fun{\tuple{\latentstate_1, \latentstate_1'}, \tuple{\latentstate_2, \latentstate_2'}} \,d\latentstationaryprior\fun{\latentstate_2, \latentstate_2'}$.
Take $G' \colon \latentstates^2 \to \latentstates^2$ be the identity function, then we can apply again \citet[Theorem~1]{DBLP:conf/iclr/TolstikhinBGS18}, which gives us:
\begin{align*}
    &\wassersteindist{\tracedistance}{\stationary{\policy}}{\stationarydecoder}\\
    \leq& \sum_{i=m+1}^{k} \sigma_i^2 + \wassersteindist{c}{\stationary{\policy}}{\latentstationaryprior} \\
    =& \sum_{i=m+1}^{k} \sigma_i^2 + \inf_{\embed:\, Q=\Xi} \expectedsymbol{\state, \action, \reward, \state' \sim \stationary{\policy}} \expectedsymbol{\latentvariable, \latentvariable' \sim \embed\fun{\sampledot \mid \state, \state'}} c\fun{\tuple{\state, \action, \reward, \state'}, G'\fun{\latentvariable, \latentvariable'}} \tag{\citet[Theorem~1]{DBLP:conf/iclr/TolstikhinBGS18}}\\
    =& \sum_{i=m+1}^{k} \sigma_i^2 + \inf_{\embed:\, Q=\latentstationaryprior} \expectedsymbol{\state, \action, \reward, \state' \sim \stationary{\policy}} \expectedsymbol{\latentstate, \latentstate' \sim \embed\fun{\sampledot \mid \state, \state'}} c\fun{\tuple{\state, \action, \reward, \state'}, \tuple{{\latentstate}, {\latentstate'}}}\\
    =& \sum_{i=m+1}^{k} \sigma_i^2 + \inf_{\embed: \, Q = \latentstationaryprior} \, \expectedsymbol{\state, \action, \reward, \state' \sim \stationary{\policy}} \, \expectedsymbol{\latentstate, \latentstate' \sim \embed\fun{\sampledot \mid \state, \state'}}\Big[\norm{\state - \generative_{\decoderparameter}\fun{\latentstate}}^2 + \norm{\action - \expectedsymbol{\latentaction \sim \latentpolicy_{\decoderparameter}\fun{\cdot \mid \latentstate}}\embeda_{\decoderparameter}\fun{\latentstate, \latentaction}}^2 + \\
        &  \omit$\hfill \left| r - \expectedsymbol{\latentaction \sim \latentpolicy_{\decoderparameter}\fun{\cdot \mid \latentstate}}{\latentrewards_{\decoderparameter}\fun{\latentstate, \latentaction}} \right|^2 + \norm{\state' - \generative_{\decoderparameter}\fun{\latentstate'}}^2\Big]$\\
    \leq& \sum_{i=m+1}^{k} \sigma_i^2 + \inf_{\embed: \, Q = \latentstationaryprior} \, \expectedsymbol{\state, \action, \reward, \state' \sim \stationary{\policy}} \, \expectedsymbol{\latentstate, \latentstate' \sim \embed\fun{\sampledot \mid \state, \state'}}\expected{\latentaction \sim \latentpolicy_{\decoderparameter}\fun{\cdot \mid \latentstate}}{\norm{\state - \generative_{\decoderparameter}\fun{\latentstate}}^2 + \norm{\action - \embeda_{\decoderparameter}\fun{\latentstate, \latentaction}}^2 + \left| r - {\latentrewards_{\decoderparameter}\fun{\latentstate, \latentaction}} \right|^2 + \norm{\state' - \generative_{\decoderparameter}\fun{\latentstate'}}^2} \tag{Jensen's inequality}
\end{align*}
\end{proof}
%
From now on, fix $\embed_\encoderparameter$ as a parameterized state embedding function such that $\embed_\encoderparameter\fun{\latentstate, \latentstate' \mid \state, \state'} = \embed_{\encoderparameter}\fun{\latentstate \mid \state} \cdot \embed_{\encoderparameter}\fun{\latentstate' \mid \state'}$ for any $\state, \state' \in \states$, $\latentstate, \latentstate' \in \latentstates$, and $Q_\encoderparameter = \expectedsymbol{\state, \state' \sim \stationary{\policy}} \embed_{\encoderparameter}\fun{\cdot \mid \state, \state'}$.
Write $\Delta$ for set of deterministic distributions, then let
$\distance_{\states}$ and $\distance_\actions$ be the usual $\ell_2$ distance,
 $\Sigma_{\encoderparameter, \decoderparameter} = \sum_{i = m + 1}^{k} \sigma_i^2 \cdot \condition{\Delta}\fun{\latentpolicy_{\decoderparameter}}$, and
 %$c = \tracedistance^{\; 2 - \condition{\Delta}\fun{\latentpolicy_{\decoderparameter}}}$.
$c\fun{\tuple{\state, \action, \reward, \state'}, \tuple{\latentstate, \latentaction, \latentstate'}} = \distance_{\states}^{\; 2 - \condition{\Delta}\fun{\latentpolicy_{\decoderparameter}}}\fun{\state, \generative_{\decoderparameter}\fun{\latentstate}} + \distance_{\actions}^{\; 2 - \condition{\Delta}\fun{\latentpolicy_{\decoderparameter}}}\fun{\action, \embeda_{\decoderparameter}\fun{\latentstate, \latentaction}} + \left| r - {\latentrewards_{\decoderparameter}\fun{\latentstate, \latentaction}} \right|^{\; 2 - \condition{\Delta}\fun{\latentpolicy_{\decoderparameter}}} + \distance_{\states}^{\; 2 - \condition{\Delta}\fun{\latentpolicy_{\decoderparameter}}}\fun{\state', \generative_{\decoderparameter}\fun{\latentstate'}}$.

We define the WAE-MDP objective as:
\begin{align*}
    \min_{\encoderparameter, \decoderparameter} \, \Sigma_{\encoderparameter, \decoderparameter} + \expectedsymbol{\state, \action, \reward, \state' \sim \stationary{\policy}} \, \expectedsymbol{\latentstate, \latentstate' \sim \embed_{\encoderparameter}\fun{\sampledot \mid \state, \state'}} \expectedsymbol{\latentaction \sim \latentpolicy_{\decoderparameter}\fun{\sampledot \mid \latentstate}} c\fun{\tuple{\state, \action, \reward, \state'}, \tuple{\latentstate, \latentaction, \latentstate'}} + \beta \cdot \divergencesymbol\fun{\encoder, \latentstationaryprior},
\end{align*}
where $\divergencesymbol$ is an arbitrary discrepancy metric and $\beta > 0$ a hyperparameter.
% Take $G_{\decoderparameter}\fun{\latentstate, \action, \latentstate'} = \tuple{\generative_{\decoderparameter}\fun{\latentstate}, \action, \latentrewards_{\decoderparameter}\fun{\latentstate, \action}, \generative_{\decoderparameter}\fun{\latentstate'}, \latentlabels_{\decoderparameter}\fun{\latentstate'}}$ such that we assume a perfect reconstruction of labels through $\latentlabels$ and $\generative_{\decoderparameter} \colon \latentstates \to \states$ being a generative function mapping latent to grounded states.
% Notice that $G_{\decoderparameter}$ maps actions through the identity function since assuming the original and latent space models share the same action space is equivalent to having an action embedding function being the identity function.
% Consequently, the distance term $\transitiondistance\fun{\tuple{\state, \action, \reward, \state', \labels\fun{\state'}}, G_{\decoderparameter}\fun{\latentstate, \action, \latentstate'}}$ becomes simply $\left| \reward - \latentrewards_{\decoderparameter}\fun{\latentstate, \action} \right| + \distance_{\states}\fun{\state', \generative_{\decoderparameter}\fun{\latentstate'}}$.
%
In the same spirit than the work of \citet{DBLP:journals/corr/abs-1902-09323/zhang19}, one can set $\divergencesymbol$ to $\wassersteinsymbol{\distance_{\latentstates}}$, which yields the following Lemmae.
\begin{lemma}\label{lem:policy-regularizer-upper-bound}
Let $\originaltolatentstationary{} \in \distributions{\latentstates \times \latentstates}, \; \tuple{\latentstate, \latentstate'} \mapsto \expected{\state \sim \stationary{\policy}}{\embed_{\encoderparameter}\fun{\latentstate \mid \state} \cdot \expectedsymbol{\latentaction \sim \latentpolicy_{\decoderparameter}\fun{\sampledot \mid \latentstate}}{ \latentprobtransitions_{\decoderparameter}\fun{\latentstate' \mid \latentstate, \latentaction}}}$, then
\begin{align*}
    \wassersteindist{\distance_{\latentstates}}{\encoder}{ \latentstationaryprior}
    \leq \wassersteindist{\distance_{\latentstates}}{\encoder}{\originaltolatentstationary{}} + \expectedsymbol{\state, \action \sim \stationary{\policy}}\expectedsymbol{\latentstate \sim \embed_{\encoderparameter}\fun{\sampledot \mid \state}}\expectedsymbol{\latentaction \sim \latentpolicy_{\decoderparameter}\fun{\sampledot \mid \latentstate}} \wassersteindist{\distance_{\latentstates}}{\embed_{\encoderparameter}\probtransitions\fun{\sampledot \mid \state, \action}}{\latentprobtransitions_{\decoderparameter}\fun{\sampledot \mid \latentstate, \latentaction}}
\end{align*}
\end{lemma}
\begin{proof}
Wasserstein is compliant with the triangular inequality \cite{Villani2009}, which gives us:
\begin{align*}
    \wassersteindist{\distance_{\latentstates}}{\encoder}{ \latentstationaryprior}
    \leq \wassersteindist{\distance_{\latentstates}}{\encoder}{\originaltolatentstationary{}} + \wassersteindist{\distance_{\latentstates}}{\originaltolatentstationary{}}{ \latentstationaryprior},
\end{align*}
where
\begin{align}
    \wassersteindist{\distance_{\latentstates}}{\originaltolatentstationary{}}{ \latentstationaryprior} &= \sup_{f \in \Lipschf{\distance_{\latentstates}}} \expectedsymbol{\state \sim \stationary{\policy}}\expectedsymbol{\latentstate \sim \embed_{\encoderparameter}\fun{\sampledot \mid \state}}\expectedsymbol{\latentaction \sim \latentpolicy\fun{\sampledot \mid \latentstate}}\expectedsymbol{\latentstate' \sim \latentprobtransitions_{\decoderparameter}\fun{\sampledot \mid \latentstate, \latentaction}} f\fun{\latentstate, \latentstate'} - \expectedsymbol{\latentstate \sim \latentstationaryprior}\expectedsymbol{\latentaction \sim \latentpolicy\fun{\sampledot \mid \latentstate}}\expectedsymbol{\latentstate' \sim \latentprobtransitions_{\decoderparameter}\fun{\sampledot \mid \latentstate, \latentaction}} f\fun{\latentstate, \latentstate'}\text{, and} \notag\\
    \wassersteindist{\distance_{\latentstates}}{\encoder}{\originaltolatentstationary{}} &=
    \sup_{f \in \Lipschf{\distance_{\latentstates}}} \expectedsymbol{\state, \action, \state' \sim \stationary{\policy}}\expectedsymbol{\latentstate, \latentstate' \sim \embed_{\encoderparameter}\fun{\sampledot \mid \state, \state'}} f\fun{\latentstate, \latentstate'} - \expectedsymbol{\state \sim \stationary{\policy}}\expectedsymbol{\latentstate \sim \embed_{\encoderparameter}\fun{\sampledot \mid \state}}\expectedsymbol{\latentaction \sim \latentpolicy\fun{\sampledot \mid \latentstate}}\expectedsymbol{\latentstate' \sim \latentprobtransitions_{\decoderparameter}\fun{\sampledot \mid \latentstate, \latentaction}} f\fun{\latentstate, \latentstate'} \label{eq:proof-lemma-wdist-triangular-inequality-4} \\
    &\leq \expectedsymbol{\state, \action \sim \stationary{\policy}}\expectedsymbol{\latentstate \sim \embed_{\encoderparameter}\fun{\sampledot \mid \state}}\expectedsymbol{\latentaction \sim \latentpolicy_{\decoderparameter}\fun{\sampledot \mid \latentstate}} \; \sup_{f \in \Lipschf{\distance_{\latentstates}}} \expectedsymbol{\state' \sim \probtransitions\fun{\sampledot \mid \state, \action}} \expectedsymbol{\latentstate' \sim \embed_{\encoderparameter}\fun{\sampledot \mid \state'}} f\fun{\latentstate, \latentstate'} - \expectedsymbol{\latentstate' \sim \latentprobtransitions_{\decoderparameter}\fun{\sampledot \mid \latentstate, \latentaction}} f\fun{\latentstate, \latentstate'} \label{eq:proof-lemma-wdist-triangular-inequality-5} \\
    &= \expectedsymbol{\state, \action \sim \stationary{\policy}}\expectedsymbol{\latentstate \sim \embed_{\encoderparameter}\fun{\sampledot \mid \state}}\expectedsymbol{\latentaction \sim \latentpolicy_{\decoderparameter}\fun{\sampledot \mid \latentstate}} \; \sup_{f \in \Lipschf{\distance_{\latentstates}}} \expectedsymbol{\state' \sim \probtransitions\fun{\sampledot \mid \state, \action}} \expectedsymbol{\latentstate' \sim \embed_{\encoderparameter}\fun{\sampledot \mid \state'}} f\fun{\latentstate'} - \expectedsymbol{\latentstate' \sim \latentprobtransitions_{\decoderparameter}\fun{\sampledot \mid \latentstate, \latentaction}} f\fun{\latentstate'} \label{eq:proof-lemma-wdist-triangular-inequality-6} \\
    & = \expectedsymbol{\state, \action \sim \stationary{\policy}}\expectedsymbol{\latentstate \sim \embed_{\encoderparameter}\fun{\sampledot \mid \state}}\expectedsymbol{\latentaction \sim \latentpolicy_{\decoderparameter}\fun{\sampledot \mid \latentstate}} \wassersteindist{\distance_{\latentstates}}{\embed_{\encoderparameter}\probtransitions\fun{\sampledot \mid \state, \action}}{\latentprobtransitions_{\decoderparameter}\fun{\sampledot \mid \latentstate, \latentaction}}. \notag
\end{align}
We pass from Eq.~\ref{eq:proof-lemma-wdist-triangular-inequality-4} to Eq.~\ref{eq:proof-lemma-wdist-triangular-inequality-5} by the Jensen's inequality.
To see how we pass from Eq.~\ref{eq:proof-lemma-wdist-triangular-inequality-5} to Eq.~\ref{eq:proof-lemma-wdist-triangular-inequality-6}, notice in the supremum over $\Lipschf{\distance_{\latentstates}}$ of Eq.~\ref{eq:proof-lemma-wdist-triangular-inequality-6} that
\begin{align*}
    \Lipschf{\distance_{\latentstates}} &= \set{f \colon f\fun{\latentstate_1, \latentstate_1'} - f\fun{\latentstate_2, \latentstate_2'} \leq \distance_{\latentstates}\fun{\latentstate_1, \latentstate_2} + \distance_{\latentstates}\fun{\latentstate_1', \latentstate_2'}} \\
    &= \set{f \colon f\fun{\latentstate, \latentstate_1'} - f\fun{\latentstate, \latentstate_2'} \leq \distance_{\latentstates}\fun{\latentstate, \latentstate} + \distance_{\latentstates}\fun{\latentstate_1', \latentstate_2'}} \tag{where $\latentstate$ is drawn from $\embed_{\encoderparameter}$} \\
    &= \set{f \colon f\fun{\latentstate_1'} - f\fun{\latentstate_2'} \leq \distance_{\latentstates}\fun{\latentstate_1', \latentstate_2'}}.
\end{align*}
\end{proof}
\begin{lemma}
% If $\embed_{\encoderparameter}$ is deterministic and traces are generated by running $\latentpolicy_{\decoderparameter}$ in the environment, then
Assume traces are generated by running $\latentpolicy_{\decoderparameter}$ in the environment, then
\begin{align*}
    %\wassersteindist{\distance_{\latentstates}}{\encoder}{ \latentstationaryprior}\leq
    \wassersteindist{\distance_{\latentstates}}{\encoder}{\originaltolatentstationary{}} + \wassersteindist{\distance_{\latentstates}}{\originaltolatentstationary{}}{ \latentstationaryprior} = \localtransitionloss{\stationary{\latentpolicy_{\decoderparameter}}}.
\end{align*}
\end{lemma}
\begin{proof}
Recall that we defined $\latentpolicy_{\decoderparameter}\fun{\sampledot \mid \state} = \expectedsymbol{\latentstate \sim \embed\fun{\sampledot \mid \state}}\latentpolicy_{\decoderparameter}\fun{\sampledot \mid \latentaction}$ for any $\state \in \states$.
Observe that $\wassersteindist{\distance_{\latentstates}}{\latentprobtransitions_{\latentpolicy_{\decoderparameter}}\stationary{\latentpolicy_{\decoderparameter}}}{ \latentstationaryprior} = 0$ (both distributions match).
%
Then, notice that 
\begin{align*}
&\wassersteindist{\distance_{\latentstates}}{\encoder}{\latentprobtransitions_{\latentpolicy_{\decoderparameter}}\stationary{\latentpolicy_{\decoderparameter}}} \\
=& \wassersteindist{\distance_{\latentstates}}{\encoder}{\latentstationaryprior}\\
=& \sup_{f \in \Lipschf{\distance_{\latentstates}}} \expectedsymbol{\state, \state' \sim \stationary{\latentpolicy_{\decoderparameter}}}\expectedsymbol{\latentstate, \latentstate' \sim \embed_{\encoderparameter}\fun{\sampledot \mid \state, \state'}} f\fun{\latentstate, \latentstate'} - \expectedsymbol{\state \sim \stationary{\latentpolicy_{\decoderparameter}}}\expectedsymbol{\latentstate \sim \embed_{\encoderparameter}\fun{\sampledot \mid \state}}\expectedsymbol{\latentaction \sim \latentpolicy_{\decoderparameter}\fun{\sampledot \mid \latentstate}}\expectedsymbol{\latentstate' \sim \latentprobtransitions_{\decoderparameter}\fun{\sampledot \mid \latentstate, \latentaction}} f\fun{\latentstate, \latentstate'}\\
\leq& \expectedsymbol{\state \sim \stationary{\policy}}\expectedsymbol{\latentstate \sim \embed_{\encoderparameter}\fun{\sampledot \mid \state}}\expectedsymbol{\latentaction \sim \latentpolicy_{\decoderparameter}\fun{\sampledot \mid \latentstate}} \; \sup_{f \in \Lipschf{\distance_{\latentstates}}} \expectedsymbol{\state' \sim \probtransitions\fun{\sampledot \mid \state, \latentaction}} \expectedsymbol{\latentstate' \sim \embed_{\encoderparameter}\fun{\sampledot \mid \state'}} f\fun{\latentstate'} - \expectedsymbol{\latentstate' \sim \latentprobtransitions_{\decoderparameter}\fun{\sampledot \mid \latentstate, \latentaction}} f\fun{\latentstate'} \tag{by the Jensen's inequality and the development of $\Lipschf{\distance_{\latentstates}}$ in the proof of Lemma~\ref{lem:regularizer-upper-bound}.} \\
=& \expectedsymbol{\state, \latentaction \sim \stationary{\latentpolicy_{\decoderparameter}}} \wassersteindist{\distance_{\latentstates}}{\embed_{\encoderparameter}\probtransitions\fun{\sampledot \mid \state, \latentaction}}{\latentprobtransitions\fun{\sampledot \mid \embed_{\encoderparameter}\fun{\state}, \latentaction}} \\
=& \localtransitionloss{\stationary{\latentpolicy_{\decoderparameter}}}.
\end{align*}
\end{proof}

We choose to minimize the upper bound induced by Lemma~\ref{lem:regularizer-upper-bound} to recover the local transition loss when the latent policy matches the original one, which yields the following W$^2$AE-MDP (\emph{Wasserstein-Wasserstein auto-encoded MDP}) objective:
\begin{multline*}
    \min_{\encoderparameter, \decoderparameter} \, \Sigma_{\decoderparameter} + \expectedsymbol{\state, \action, \reward, \state' \sim \stationary{\policy}} \, \expectedsymbol{\latentstate, \latentstate' \sim \embed_{\encoderparameter}\fun{\sampledot \mid \state, \state'}} \expectedsymbol{\latentaction \sim \latentpolicy_{\decoderparameter}\fun{\sampledot \mid \latentstate}}{ c\fun{\tuple{\state, \action, \reward, \state'}, \tuple{\latentstate, \latentaction, \latentstate'}}} \\
    + \beta \cdot \fun{ \wassersteindist{\distance_{\latentstates}}{\originaltolatentstationary{}}{ \latentstationaryprior} + \expectedsymbol{\state, \action \sim \stationary{\policy}}\expectedsymbol{\latentstate \sim \embed_{\encoderparameter}\fun{\sampledot \mid \state}} \expectedsymbol{\latentaction \sim \latentpolicy_{\decoderparameter}\fun{\sampledot \mid \latentstate}}\wassersteindist{\distance_{\latentstates}}{\embed_{\encoderparameter}\probtransitions\fun{\sampledot \mid \state, \action}}{\latentprobtransitions_{\decoderparameter}\fun{\sampledot \mid \latentstate, \latentaction}}}
\end{multline*}
