\todo[inline]{F: {\color{red} DEPRECATED}}
\smallparagraph{Partially observable MDPs.} A \emph{partially observable MDP} (POMDP) is a tuple $\pomdp = \pomdptuple$ where $\mdp$ is the underlying MDP modeling the environment, $\observations$ is a set of \emph{observations}, and $\observe \colon \actions \times \states \to \distributions{\observations}$ is an \emph{observation function}.
Intuitively, $\observe(\observation \mid \action, \state')$ is the probability of observing $\observation$ when $\action$ is chosen and the resulted state in $\mdp$ is $s'$.

In POMDP $\pomdp$, the agent does not directly perceive states from $\mdp$ but perceives observations of the environment instead.
Therefore, a \emph{history} of $\pomdp$ is a sequence of actions and observations $\historytuple{\action}{\observation}{T}$ such that $\exists \trajectorytuple{\state}{\action}{T} \in \trajectories{\mdp}$ with $\observe\left(\observation_{t+1} \mid \action_t, \state_{t+1}\right) > 0$ for $t \in \set{0, \dots, T-1}$, $T \in \N$. The set of histories of $\pomdp$ is $\histories{\pomdp}$.
According to an observed history, a \emph{belief} $\belief \in \distributions{\states}$ can be maintained, giving the probability for the agent of being in each particular state of $\mdp$ \citep{DBLP:conf/aaai/CassandraKL94}.
Agent policies do not depend on trajectories of $\mdp$ since they are not observable, but rather on observed histories in $\pomdp$ or (sequences of) such beliefs.

The initial belief $\belief_I(\state)$ is equal to $1$ if $\state = \sinit$, and $0$ otherwise. Then, the belief can be updated
%via the \emph{state estimator distribution}
according to $\stateestimator\colon \distributions{\states} \times \actions \times \observations \to \distributions{\states}$, $\stateestimator\fun{\state' \mid \belief, \action, \observation}$ being the probability of transitioning to state $\state'\in \states$ when the current belief is $\belief$, the action $\action \in \actions$ is executed, and $\observation \in \observations$ is perceived afterwards.
By Bayes' rule, we have
\begin{equation*}
    \stateestimator\left(\state' \mid \belief, \action, \observation\right) = \frac{\stateestimator(\observation \mid \belief, \action, \state') \cdot \stateestimator(\state'\mid\belief, \action)}{\stateestimator(\observation \mid \belief, \action)}
\end{equation*}
with
$\stateestimator\left(\observation \mid \belief, \action, \state'\right) = \observe\left(\observation \mid \action, \state'\right)$,
$\stateestimator(\state'\mid\belief, \action) = \sum_{\state \in \states} \transitions\left(\state, \action, \state'\right) \cdot \belief(\state)$,
and $\stateestimator\left(\observation \mid \belief, \action\right) = \sum_{\state' \in \states} \stateestimator\left(\observation \mid \belief, \action, \state'\right) \cdot \stateestimator(\state'\mid\belief, \action)$.
Therefore, the belief update for state $\state'$ is given by
\begin{equation*}
    \belief^{\action, \observation}(\state') = \stateestimator\left(\state' \mid \belief, \action, \observation\right) = \frac{\observe\left(\observation \mid \action, \state'\right) \cdot \sum_{\state \in \states} \transitions\left(\state, \action, \state'\right) \cdot \belief(\state)}{\sum_{\state' \in \states} \observe\left(\observation \mid \action, \state'\right) \cdot \sum_{\state \in \states} \transitions\left(\state, \action, \state'\right) \cdot \belief\fun{\state}}
\end{equation*}
%\todo{G: I don't understand this part, it is looks very complicated whereas belief updates are very intuitive, maybe it's $\sigma$ that has to be better explained?}
The set of reachable beliefs $\beliefs$ constructed this way allows $\pomdp$ to be formulated as an equivalent (infinite) MDP $\beliefmdp$ with state space $\beliefs$, same action space $\actions$, transition probability distribution $\probtransitions{\beliefs}$ such that $\probtransitions{\beliefs}\left(\belief' \mid \belief, \action \right)$ is equal to $\stateestimator\left(\observation \mid \belief, \action \right)$ if $\belief'=\belief^{\action, \observation}$ and $0$ otherwise, and reward function $\beliefrewards{\beliefs}\left(\belief, \action\right) = \sum_{\state \in \states} \belief\left(\state\right) \cdot \rewards(\state, \action)$.
%\todo{G: continuous is a vague word here, in fact a countably infinite state space is enough. No need for real numbers here right? Or did you start with a partially observable continuous space MDP?}

We assume the environment being modeled by an unknown continuous-space MDP $\mdp_\states = \statesmdptuple{\states}{\state}$, where $\states$ is a continuous state space.
In order to recover a policy with formal guarantees in $\mdp_\states$, we learn a finite abstraction of the environment in which model-checking techniques can be applied.
Specifically, we learn a new MDP $\mdp_\latentstates = \statesmdptuple{\latentstates}{\latentstate_I}$ with finite state space $\latentstates$ via a stochastic encoder $\encoder \colon \states \times \actions \times \rewards \times \states \times 2^{\atomicprops} \to \distributions{\latentstates}$.
We assume $\encoder\fun{\latentstate' \mid \state, \action, \reward, \state', \labels_{\scriptscriptstyle\states}\fun{\state'}} > 0 \iff \labels_{\scriptscriptstyle\states}\fun{\state'} = \labels_{\scriptscriptstyle\latentstates}\fun{\latentstate'}$.
Intuitively, $\encoder\fun{\latentstate' \mid \state, \action, \reward, \state', \labeling'}$ gives the probability of being in state $\latentstate$ in $\mdp_\latentstates$ after observing transition $\tuple{\state, \action, \reward, \state', \labeling'}$ in $\mdp_\states$.
Such an encoder allows producing discrete states that share similar labels, transition dynamics and rewards.

\smallparagraph{POMDP formulation.} Let $\encoder\fun{\latentstate' \mid \state, \action, \state'} = \encoder\fun{\latentstate' \mid \state, \action, \rewards_{\scriptscriptstyle \states}\fun{\state, \action, \state'}, \state', \labels_{\scriptscriptstyle \states}\fun{\state'}}$.
From $\tracetuple{\state}{\action}{\reward}{\labeling}{T} \in \traces{\mdp_\states}$, one can recover $\tracetuple{\latentstate}{\action}{\reward}{\labeling}{T} \in \traces{\mdp_\latentstates}$ by sampling $\sample{\latentstate_{t+1}}{\encoder}\fun{\sampledot \mid \state_t, \action_t, \state_{t+1}}$ for $t \in \set{0, \dots, T - 1 }$.
Precisely, after observing sequence $\historytuple{\action}{\state}{t}$, the agent executes an action ${\action}_{t} \in \act{\state_t}$, observes ${\state}_{t+1} \in \states$ and goes to state $\latentstate_{t + 1} \in \latentstates$ with probability $\encoder\fun{\latentstate_{t + 1} \mid {\state}_{t}, {\action}_{t}, {\state}_{t+1}}$. 
In the following, we show that an agent acting this way is equivalent to acting in a POMDP where the belief state over $\latentstates$ is induced by $\encoder$.

Let POMDP $\pomdp_\latentstates = \tuple{\mdp_\latentstates, \states, \observe}$ with $\observe\colon \states \times \actions \times \latentstates \to \distributions{\states}$ such that $\observe\fun{\state' \mid \state, \action, \latentstate'}$ is the probability of observing $\state'$ when the last observation was $\state$, then $\action$ is executed and the resulted state in $\mdp_\latentstates$ is $\latentstate'$.
% Notice that $\observe$ is slightly different from the general definition. In the following, we will see this is not a problem due to the belief space being induced by $\encoder$.
By Bayes' rule, we have
$\observe\fun{\state' \mid \state, \action, \latentstate'} = {\encoder\fun{\latentstate' \mid \state, \action, \state'} \cdot \probtransitions{\states}\fun{\state' \mid \state, \action}}/{\transitiontolatent\fun{\latentstate' \mid \state, \action}}$.
Intuitively, $\transitiontolatent\fun{\latentstate' \mid \state, \action}$ is the probability of transitioning to $\latentstate'$ in $\mdp_\latentstates$ when $\action \in \actions$ is chosen.
By construction, $\transitiontolatent\fun{\latentstate' \mid \state, \action} = \sum_{\state' \in \states} \encoder\fun{\latentstate' \mid \state, \action, \state'} \cdot \probtransitions{\states}\fun{\state' \mid \state, \action}$.
\begin{lemma}\label{belief-independent}
Let $\tuple{\action_{\scriptscriptstyle 0:t}, \state_{\scriptscriptstyle 1:t+1}} \in \histories{\pomdp_\latentstate}$ and $\belief_t$ be the belief of $\pomdp_\latentstates$ after observing $\historytuple{\action}{\state}{t}$,
then (i) $\stateestimator\fun{\state_{t + 1} \mid \belief_t, \action_t} = \probtransitions{\states}\fun{\state_{t + 1} \mid \state_t, \action_t}$, , (ii) $\indexof{\rewards}{\distributions{\latentstates}}\fun{\belief_t, \action_t} = \indexof{\rewards}{\states}\fun{\state_t, \action_t}$, and (iii) for all $\latentstate' \in \latentstates$, $\stateestimator\fun{\latentstate' \mid \belief_t, \action_t} = \probtransitions{\states, \latentstates}\fun{\latentstate' \mid \state_t, \action_t}$.
\end{lemma}
\begin{proof}
The belief space of any POMDP is a \emph{sufficient information state space process} \citep{DBLP:journals/jair/Hauskrecht00} that preserves the probability of being in each particular state of the underlying MDP and the probability of the new observation when an action in executed.
In $\pomdp_\latentstates$, that means $\stateestimator\fun{\state_{t + 1} \mid \tuple{\action_{\scriptscriptstyle 0:t-1}, \state_{\scriptscriptstyle 1:t}}, \action_t} = \stateestimator\fun{\state_{t + 1} \mid \belief_t, \action_t}$.
The results follow then from the fact that the history $\historytuple{\action}{\state}{t}$ associated with $\belief_t$ corresponds to the trajectory $\tuple{\sinit, \state_{\scriptscriptstyle 1:t}, \action_{\scriptscriptstyle 0:t-1}}$ in $\mdp_\states$.
By the Markov property, the probability distribution and reward obtained by choosing $\action_t$ in $\state_t$ do not depend on the past history, so we have (i) $\stateestimator\fun{\state_{t + 1} \mid \belief_t, \action_t} = \stateestimator\fun{\state_{t + 1} \mid \tuple{\action_{\scriptscriptstyle 0:t-1}, \state_{\scriptscriptstyle 1:t}}, \action_t} = \probtransitions{\states}\fun{\state_{t + 1 } \mid \sinit, \state_{\scriptscriptstyle 1:t}, \seq{\action}{t}} = \probtransitions{\states}\fun{{\state_{t+1} \mid \state_t, \action_t}}$, (ii) $\indexof{\rewards}{\distributions{\latentstates}}\fun{\belief_t, \action_t} = \indexof{\rewards}{\states}\fun{\tuple{\sinit, \indexof{\state}{1:t-1}, \indexof{\action}{0:t-1}}, \state_t, \action_t} = \indexof{\rewards}{\states}\fun{\state_t, \action_t}$, and (iii) $\stateestimator\fun{\latentstate' \mid \belief_t, \action_t} =
\sum_{\state' \in \states} \encoder\fun{\latentstate' \mid \state_t, \action_t, \state'} \cdot \stateestimator\fun{\state' \mid \tuple{\action_{\scriptscriptstyle 0:t-1}, \state_{\scriptscriptstyle 1:t}}, \action_t} = 
\sum_{\state' \in \states} \encoder\fun{\latentstate' \mid \state_t, \action_t, \state'} \cdot \probtransitions{\states}\fun{\state' \mid \state_t, \action_t} = \transitiontolatent\fun{\latentstate' \mid \state_t, \action_t}$.
\end{proof}
%
As a consequence, for any belief $\belief$ of $\pomdp_\latentstates$ and $\action \in \actions$, $\stateestimator\fun{\sampledot \mid \belief, \action}$ can be made independent of $\belief$ provided that the last observation is known.
%
\begin{corollary}
The belief space of $\pomdp_\latentstates$ is $\latentbeliefspace = \set{\encoder\fun{\sampledot \mid \state, \action, \state'} \mid \state, \state' \in \states, \, \action \in \act{\state} } \cup \set{\encoder^I}$ with $\encoder^I \in \distributions{\latentstates}$ such that $\encoder^I\fun{\latentstate} \text{ is equal to } 1 \text{ if } \latentstate = \latentstate_I, \, 0 \text{ else}$.
\end{corollary}
%
\begin{proof}
Let $\tuple{\action_{\scriptscriptstyle 0:t}, \state_{\scriptscriptstyle 1:t+1}} \in \histories{\pomdp_\latentstate}$ and $\belief_t$ be the belief of $\pomdp_\latentstates$ after observing $\historytuple{\action}{\state}{t}$.
Assume $\belief_t \in \set{\encoder\fun{\sampledot \mid \state, \action, \state'} \mid \state, \state' \in \states, \, \action \in \act{\state} } \cup \set{\encoder^I}$.
By definition, the next belief $\belief_{t + 1}$ after executing $\action_t$ and observing $\state_{t + 1}$ thereafter is given by $\belief_{t}^{\action_t, \state_{t + 1}}$ where for all $\latentstate_{t + 1} \in \latentstates$,
\begin{align*}
    \belief_{t}^{\action_t, \state_{t + 1}}&\fun{\latentstate_{t + 1}} = \stateestimator\fun{\latentstate_{t + 1} \mid \encoder\fun{\sampledot \mid \state_{t - 1}, \action_{t - 1}, \state_{t}}, \action_t, \state_{t + 1}}\\
    &= \frac{\stateestimator(\state_{t + 1} \mid \encoder\fun{\sampledot \mid \state_{t - 1}, \action_{t - 1}, \state_{t}}, \action_t, \latentstate_{t + 1}) \cdot \stateestimator(\latentstate_{t + 1} \mid \encoder\fun{\sampledot \mid \state_{t - 1}, \action_{t - 1}, \state_{t}}, \action_t)}{\stateestimator(\state_{t + 1} \mid \encoder\fun{\sampledot \mid \state_{t - 1}, \action_{t - 1}, \state_{t}}, \action_t)}\\
    &= \frac{\stateestimator(\state_{t + 1} \mid \encoder\fun{\sampledot \mid \state_{t - 1}, \action_{t - 1}, \state_{t}}, \action_t, \latentstate_{t + 1}) \cdot \probtransitions{\states, \latentstates}\fun{\latentstate_{t + 1} \mid \state_t, \action_t}}{\probtransitions{\states}\fun{\state_{t + 1} \mid \state_t, \action_t}} \tag{by Lemma~\ref{belief-independent}}\\
    &= \frac{\fun{\sum_{\state \in \states} \stateestimator\fun{\state \mid \encoder\fun{\sampledot \mid \state_{t - 1}, \action_{t - 1}, \state_{t}}} \cdot \observe\fun{\state_{t + 1} \mid \state, \action_t, \latentstate_{t + 1}}} \cdot \probtransitions{\states, \latentstates}\fun{\latentstate_{t + 1} \mid \state_t, \action_t}}{\probtransitions{\states}\fun{\state_{t + 1} \mid \state_t, \action_t}}\\
    &= \frac{\observe\fun{\state_{t + 1} \mid \state_{t}, \action_t, \latentstate_{t + 1}} \cdot \probtransitions{\states, \latentstates}\fun{\latentstate_{t + 1} \mid \state_t, \action_t}}{\probtransitions{\states}\fun{\state_{t + 1} \mid \state_t, \action_t}}\tag{$\stateestimator\fun{\state \mid \encoder\fun{\sampledot \mid \state_{t - 1}, \action_{t - 1}, \state_{t}}}$ is $1$ if $\state = \state_t$, $0$ else}\\
    &= \frac{\encoder\fun{\latentstate_{t + 1} \mid \state_t, \action_t, \state_{t + 1}} \cdot \probtransitions{\states}\fun{\state_{t + 1} \mid \state_t, \action_t} \cdot \probtransitions{\states, \latentstates}\fun{\latentstate_{t + 1} \mid \state_t, \action_t}}{\transitiontolatent\fun{\latentstate_{t + 1} \mid \state_t, \action_t} \cdot \probtransitions{\states}\fun{\state_{t + 1} \mid \state_t, \action_t}}\\
    &= \encoder\fun{\latentstate_{t + 1} \mid \state_t, \action_t, \state_{t + 1}}
\end{align*}
The result is then obtained by induction over histories of $\pomdp_\latentstates$.
\end{proof}
%
Contrarily to general POMDPs, the beliefs over the states of the underlying MDP thus only depend on a limited history of $\pomdp_\latentstates$,  namely the last observation, the last action executed and the new observation.
%In the following, we show that there is an equivalence between $\mdp_\states$ and $\pomdp_\latentstates$ that preserves the probability of trajectories and their return.
We formally define the belief MDP as follows.

\smallparagraph{Belief MDP.} The \emph{belief MDP} of $\pomdp_\latentstates$ is $\mdp_\latentbeliefspace = \statesmdptuple{\latentbeliefspace}{\encoder^I}$.
Let $\previousstate, \state \in \states \cup \set{{\scriptstyle\top}}$, $\previousaction \in \act{\previousstate}$, $\action \in \act{\state}$ and $\belief' \in \latentbeliefspace$.
The components of $\mdp_\latentbeliefspace$ are defined as follows.
From the initial belief, we assume that $\mdp_\states$ is in $\state = \sinit \in \states$.
Therefore, we formulate the initial belief as $\encoder^I = \encoder\fun{\sampledot \mid {\scriptstyle\top}, \indexof{\action}{\top}, \sinit}$ for some dummy state $\scriptstyle\top$ and action $\indexof{\action}{\top} \in \act{{\scriptstyle\top}}$.
Then, for belief $\encoder\fun{\sampledot \mid \previousstate, \previousaction, \state} \in \latentbeliefspace$, $\act{\encoder\fun{\sampledot \mid \previousstate, \previousaction, \state}} = \act{\state}$, and
$\probtransitions{\latentbeliefspace}\fun{\belief' \mid \encoder\fun{\sampledot \mid \previousstate, \previousaction, \state}, \action} = 0$ if $\belief' \neq \encoder\fun{\sampledot \mid \state, \action, \state'}$ for all $\state' \in \states$.
Otherwise,
\begin{align*}
    \probtransitions{\latentbeliefspace}&\fun{\encoder\fun{\sampledot\mid\state, \action, \state'} \mid \encoder\fun{\sampledot \mid \previousstate, \previousaction, \state}, \action} = \stateestimator\fun{\state'\mid \encoder\fun{\sampledot \mid \previousstate, \previousaction, \state}, \action} = \probtransitions{\states}\fun{\state'\mid \state, \action}%\tag{by Lemma~\ref{belief-independent}}
%    &= \probtransitions{\states}\fun{\state'\mid \state, \action} \cdot \sum_{\latentstate'\in\latentstates} \encoder\fun{\latentstate' \mid \state, \action, \state'} = \probtransitions{\states}\fun{\state'\mid \state, \action}
\end{align*}
The state-action reward function is given by $\indexof{\rewards}{\latentbeliefspace}\fun{\encoder(\sampledot \mid \previousstate, \previousaction, \state), \action} = \indexof{\rewards}{\states}\fun{\state, \action}$.
%
Finally, $\indexof{\labels}{\latentbeliefspace}\fun{\encoder\fun{\sampledot \mid \state, \action, \state'}} = \indexof{\labels}{\states}\fun{\state'}$ since for all $\latentstate' \in \latentstates$, $\encoder\fun{\latentstate' \mid \state, \action, \state'} > 0 \iff \labels_{\scriptscriptstyle\states}\fun{\state'} = \labels_{\scriptscriptstyle\latentstates}\fun{\latentstate'}$.

One can easily show there is a bisimulation \citep{DBLP:journals/ai/GivanDG03} between $\mdp_\states$ and $\mdp_\latentbeliefspace$ (and consequently between $\mdp_\states$ and $\pomdp_\latentstates$):
%via the equivalence relation $\set{\tuple{\state, \belief} \in \states \times \latentbeliefspace \mid \belief = \encoder\fun{\sampledot \mid \previousstate, \previousaction, \state} \text{ for } \previousstate \in \states, \, \previousaction \in \act{\previousstate}}$.
intuitively, $\mdp_\latentbeliefspace$ refines $\mdp_\states$ by recording the last state and action visited.
In the following we show that we can benefits from this link between $\pomdp_\latentstates$ and $\mdp_\states$ to recover some guarantees in $\mdp_\states$ using the underlying MDP $\mdp_\latentstates$ of $\pomdp_\latentstates$.

\smallparagraph{Upper bound guarantees.} Given $\policy \in \mpolicies{\mdp_\latentstates}$, one can recover a finite-memory policy $\policy'\in\policies{\mdp_\states}$ in a natural way using $\encoder$:
for all $\previousstate, \state \in \states$, $\previousaction, \action \in \actions$, $\policy'\fun{\action \mid \previousstate, \previousaction, \state} = \sum_{\latentstate \in \latentstates} \policy\fun{\action \mid \latentstate} \cdot \encoder\fun{\latentstate \mid \previousstate, \previousaction, \state}$.
We say that two such policies are equivalent under relation $\policyequiv$.\todo{G: is this a definition or a claim?}

\begin{theorem}
Let $T \subseteq \atomicprops$, $\discount \in [0, 1]$, $\policy \in \mpolicies{\mdp_\latentstates}$ and $\policy' \in \policies{\mdp_\states}$ such that $\policy \policyequiv \policy'$,
then $\Prob^{\mdp_\latentstates}_{\pi}\fun{\eventually T} \geq \Prob^{\mdp_\states}_{\policy'}\fun{\eventually T}$, and
$\expectmdp{\policy}{\mdp_\latentstates}{\discountedreturn{\discount}} \geq \expectmdp{\policy'}{\mdp_\states}{\discountedreturn{\discount}}$.
\end{theorem}
%
\begin{proof}
The result follows from the fact that the \emph{value function} (giving the optimal expected return) of the underlying MDP of a POMDP is an upper bound on the value function of this POMDP \citep[Theorems~6 and 9]{DBLP:journals/jair/Hauskrecht00}.
Precisely, we fix $\policy$ in $\mdp_\latentstates$.
From policy $\policy$ in $\mdp_\latentstates$, we recover policy $\indexof{\policy}{\latentbeliefspace} \colon \latentbeliefspace \to \distributions{\actions}$ in $\pomdp_\latentstates$ such that $\indexof{\policy}{\latentbeliefspace}\fun{\action \mid \belief} = \sum_{\latentstate \in \latentstates} \policy\fun{\action \mid \latentstate} \cdot \belief\fun{\latentstate}$.
The POMDP $\pomdp_\latentstates$ under $\indexof{\policy}{\latentbeliefspace}$ is itself a POMDP such that the underlying MDP is the Markov Chain induced by $\mdp_\latentstates$ and $\policy$\todo{F: prove that? {\color{red}wrong}},
and the belief MDP is defined by the Markov Chain $\mdp_{\latentbeliefspace, \policy} = \tuple{\latentbeliefspace, \indexof{\transitions}{\latentbeliefspace, {\scriptstyle \policy}}, \indexof{\rewards}{\latentbeliefspace, {\scriptstyle \policy}}, \indexof{\labels}{\latentbeliefspace}, \atomicprops, \encoder^I}$ (we ignore the action space since there is only one enabled action in each state).
The probability transition distribution is defined as follows.
Let $\previousstate, \state, \state' \in \states$, $\previousaction\in\act{\previousstate}, \action \in \act{\state}$,
\begin{align*}
&\probtransitions{\latentbeliefspace, {\scriptstyle\policy}}\fun{\encoder\fun{\sampledot \mid \state, \action, \state'} \mid \encoder\fun{\sampledot \mid \previousstate, \previousaction, \state}}\\
& \quad = \sum_{\action^\star \in \act{\state}} \indexof{\policy}{\latentbeliefspace}\fun{\action^\star \mid \encoder\fun{\sampledot \mid \previousstate, \previousaction, \state}} \cdot \probtransitions{\latentbeliefspace}\fun{\encoder\fun{\sampledot \mid \state, \action, \state'} \mid \encoder\fun{\sampledot \mid \previousstate, \previousaction, \state}, \action^\star}\\
& \quad = \sum_{\action^\star \in \act{\state}} \sum_{\latentstate \in \latentstates} \policy\fun{\action^\star \mid \latentstate} \cdot \encoder\fun{\latentstate \mid \previousstate, \previousaction, \state} \cdot \probtransitions{\latentbeliefspace}\fun{\encoder\fun{\sampledot \mid \state, \action, \state'} \mid \encoder\fun{\sampledot \mid \previousstate, \previousaction, \state}, \action^\star}.
\end{align*}
By definition, $\probtransitions{\latentbeliefspace}\fun{\encoder\fun{\sampledot \mid \state, \action, \state'} \mid \encoder\fun{\sampledot \mid \previousstate, \previousaction, \state}, \action^\star} = 0$ if $\action^\star \neq \action$. Therefore,
\begin{align*}
\probtransitions{\latentbeliefspace, {\scriptstyle\policy}}&\fun{\encoder\fun{\sampledot \mid \state, \action, \state'} \mid \encoder\fun{\sampledot \mid \previousstate, \previousaction, \state}}\\
&= \sum_{\latentstate \in \latentstates} \policy\fun{\action \mid \latentstate} \cdot  \encoder\fun{\latentstate \mid \previousstate, \previousaction, \state} \cdot \probtransitions{\latentbeliefspace}\fun{\encoder\fun{\sampledot \mid \state, \action, \state'} \mid \encoder\fun{\sampledot \mid \previousstate, \previousaction, \state}, \action}\\
&= \sum_{\latentstate \in \latentstates} \policy\fun{\action \mid \latentstate} \cdot  \encoder\fun{\latentstate \mid \previousstate, \previousaction, \state} \cdot \probtransitions{\states}\fun{\state' \mid \state, \action}.
\end{align*}
We define the reward function as $\indexof{\rewards}{\latentbeliefspace, {\scriptstyle \policy}} \colon \latentbeliefspace \to \R$ such that \todo{G: are you redefining it or recalling its definition?}
\begin{align*}
\indexof{\rewards}{\latentbeliefspace, {\scriptstyle \policy}}&\fun{\encoder\fun{\sampledot \mid \previousstate, \previousaction, \state}}\\
&= \sum_{\action \in \act{\state}} \indexof{\policy}{\latentbeliefspace}\fun{\action \mid \encoder\fun{\sampledot \mid \previousstate, \previousaction, \state}} \cdot \indexof{\rewards}{\latentbeliefspace}\fun{\encoder\fun{\sampledot\mid\previousstate, \previousaction, \state}, \action}\\
&= \sum_{\action \in \act{\state}} \sum_{\latentstate \in \latentstates} \policy\fun{\action \mid \latentstate} \cdot \encoder\fun{\latentstate \mid \previousstate, \previousaction, \state} \cdot \indexof{\rewards}{\states}\fun{\state, \action}.
\end{align*}
%
On the other hand, in $\mdp_\states$, one can see $\policy'$ as a finite memory policy encoded by a Mealy machine with memory $\states \times \actions$, recording the last $\previousstate \in \states$ and $\previousaction \in \act{\previousstate}$ visited.
By construction, since $\policy \policyequiv \policy'$, the Markov chain induced by $\mdp_\states$ and $\policy'$ is exactly the Markov chain $\mdp_{\latentbeliefspace, \policy}$ modulo the state equivalence relation
$\set{\tuple{\belief, \rightarrow} \in \latentbeliefspace \times \states \times \actions \times \states \mid \belief = \encoder\fun{\sampledot \mid \state, \action, \state'} \text{ and } \rightarrow = \tuple{\state, \action, \state'} \text{ for } \state, \state' \in \states, \action \in \act{\state}}$.

By \cite{DBLP:journals/jair/Hauskrecht00}, we thus have 
$\expectmdp{\policy}{\mdp_\latentstates}{\discountedreturn{\discount}} \geq \expectmdp{\indexof{\policy}{\latentbeliefspace}}{\pomdp_{\latentstates}}{\discountedreturn{\discount}} = \expectmdp{\indexof{\policy}{\latentbeliefspace}}{\mdp_\latentbeliefspace}{\discountedreturn{\discount}} = \expectmdp{\policy'}{\mdp_\states}{\discountedreturn{\discount}}$.
Moreover, one can reduce a reachability objective of the form $\Prob^{\mdp_\latentstates}_{\policy}\fun{\eventually T}$ to an expected (un)discounted return objective of the form $\expectmdp{\policy}{\mdp'_\latentstates}{\discountedreturn{\discount}}$ with $\discount = 1$, in a modified MDP $\mdp'_\latentstates$ where states $t \in \set{\latentstate \in \latentstates \mid \indexof{\labels}{\latentstates}\fun{\latentstate} \cap T \neq \emptyset}$ are made absorbing (i.e., $\transitions'_\latentstates\fun{t, \action, t} = 1$ for all $\action \in \act{t}$), and where each transition has $0$ reward except for transitions entering a state $t$, giving a reward of $1$ \citep{DBLP:journals/lmcs/EtessamiKVY08}.
This yields the result for the reachability objective.
\end{proof}