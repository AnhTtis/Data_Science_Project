\emph{Reinforcement learning} (RL) is emerging as a solution of choice to address challenging real-word scenarios such as epidemic mitigation and prevention strategies 
%\citep{DBLP:conf/pkdd/LibinMVPHLN20,DBLP:journals/corr/abs-2204-05027},
\citep{DBLP:conf/pkdd/LibinMVPHLN20},
multi-energy management \citep{CEUSTERS2021117634}, or effective canal control  \citep{REN2021103049}.
% (e.g., \citep{CEUSTERS2021117634,DBLP:conf/pkdd/LibinMVPHLN20,REN2021103049,DBLP:journals/corr/abs-2204-05027}).
RL enables learning high performance controllers by introducing general nonlinear function approximators (such as neural networks) to scale with high-dimensional and continuous state-action spaces.
This introduction, termed \emph{deep-RL}, causes the loss of the conventional convergence guarantees of RL \citep{DBLP:journals/ml/Tsitsiklis94} as well as those obtained in some continuous settings \citep{phdthesis:Nowe94}, and hinders their wide roll-out in critical settings.
% This work \emph{enables} the \emph{formal verification} of \emph{any} such policies, learned for \emph{unknown} continuous environments modeled as \emph{Markov decision processes} (MDPs), through their distillation over smaller and explicit \emph{discrete latent space models}.
% new
This work \emph{enables} the \emph{formal verification} of \emph{any} such policies, learned by agents interacting with {unknown}, {continuous} environments modeled as \emph{Markov decision processes} (MDPs). % through their distillation over discrete representations, 
% through smaller, explicit, and accurate \emph{discrete latent space models} that are tractable for \emph{model checking} \citep{DBLP:BK08}.
Specifically, we learn a \emph{discrete} representation of the state-action space of the MDP, which yield both a (smaller, explicit) \emph{latent space model} and a distilled version of the RL policy, that are tractable for \emph{model checking} \citep{DBLP:BK08}.
The latter are supported by \emph{bisimulation guarantees}: intuitively, the agent behaves similarly in the original and latent models.
% The resulting \emph{latent space models} are tractable for \emph{model checking} \citep{DBLP:BK08}.
% that approximates the unknown dynamics of the real environment.
% The strength of this approach lies in the fact that we do not simply verify that the RL agent meets a \emph{predefined} set of properties, but rather provide a latent environment model on which the user can reason and check \emph{any} desired property.
The strength of our approach is not simply that we verify that the RL agent meets a \emph{predefined} set of specifications, but rather provide an abstract model on which the user can reason and check \emph{any} desired agent property.

\emph{Variational MDPs} (VAE-MDPs, \citealt{DBLP:journals/corr/abs-2112-09655}) offer a valuable framework for doing so.
The distillation is provided with PAC-verifiable {bisimulation} bounds guaranteeing
% The distillation relies on learning an abstraction of the original environment, the latter being eventually provided with PAC verifiable \emph{bisimulation} bounds:
% formalized as (i) \emph{abstraction quality}: the agent executing the distilled policy in the original and latent model has close behaviors; (ii) \emph{representation quality}: the agent behaves similarly from all states embedded to the same discrete state.
that the agent behaves similarly
(i) in the original and latent model (\emph{abstraction quality});
(ii) from all original states embedded to the same discrete state (\emph{representation quality}).
%Along with these bounds, the latent model is provided with embedding functions allowing to execute the distilled policy on the original spaces and lift the guarantees obtained by formally checking the latent model back to the real environment.
Whilst the bounds offer a confidence metric that enables the verification
%of discounted expected return and (safety-constrained) reachability properties,
of performance and safety properties,
VAE-MDPs suffer from several learning flaws.
First, training a VAE-MDP relies on variational proxies to the bisimulation bounds, meaning there is no learning guarantee on the quality of the latent model via its optimization.
%
Second, \emph{variational autoencoders} (VAEs) \citep{DBLP:journals/corr/KingmaW13, DBLP:journals/jmlr/HoffmanBWP13}
% are known to suffer from \emph{mode collapse}, resulting in a degenerate local optimum where the model learns to ignore the latent space (e.g., \citep{DBLP:conf/icml/AlemiPFDS018}).
% VAE-MDPs are no exception, and this phenomenon results in a deterministic mapping to a single discrete latent state.
are known to suffer from \emph{posterior collapse} (e.g., \citealt{DBLP:conf/icml/AlemiPFDS018})
resulting in a deterministic mapping to a unique latent state in VAE-MDPs.
Most of the training process focuses on handling this phenomenon and setting up the stage for the concrete distillation and abstraction, finally taking place in a second training phase.
This requires extra regularizers, setting up annealing schemes
%, training the model in different learning phases,
and learning phases,
and defining prioritized replay buffers to store transitions.
% where transitions are stored.
% derived from the bucketization of the state space.
% with importance sampling patterns for the stored transitions.
Distillation through VAE-MDPs is thus a meticulous task, requiring a large step budget and tuning many hyperparameters.
% Being part of its incentive, all these make yet the combination of VAE-MDPs with safe RL methods all the most arduous since critical scenarios often require achieving safety standards quickly. 
% While this is part of its incentives, all these make the combination of VAE-MDPs with safe RL methods all the more arduous as critical scenarios require rapid safety compliance.

Building upon \emph{Wasserstein} autoencoders %(WAE, \citealt{DBLP:conf/iclr/TolstikhinBGS18})
\citep{DBLP:conf/iclr/TolstikhinBGS18} instead of VAEs, we introduce \emph{Wasserstein auto-encoded MDPs} (WAE-MDPs), which overcome those limitations.
Our WAE relies on the \emph{optimal transport} (OT) from trace distributions resulting from the execution of the RL policy in the real environment to that reconstructed from the latent model operating under the distilled policy.
% Concretely, V- and WAEs minimize two terms: a reconstruction cost plus a regularizer that penalizes the discrepancy between the latent prior distribution and the distribution induced by the encoder.
% WAEs are more flexible than VAEs: this regularization is not limited to the \emph{Kullback-Leibler divergence}, which allows us deriving a new objective function directly incorporating the bisimulation bounds, instead of relying on variational proxies.
% In contrast to VAEs, the WAE regularizer is not limited to the \emph{Kullback-Leibler divergence}, which allows us to derive a novel objective function incorporating the bisimulation bounds instead of relying on variational proxies.
%
% This further provides WAEs with a wider range of architectural network possibilities: while VAEs require to learn stochastic mappings and determinize or reconstruct some of them afterwards 
% to obtain the bisimulation guarantees, WAEs have no such requirements meaning \emph{all the necessary components to obtain the guarantees are the same during learning and deployment}.
% This further makes it possible to link the guarantees directly to the WAE objective. 
%We develop a masked autoregressive flow that allows to learn continuous relaxation of multivariate Bernoulli distributions, to model the dynamics of the latent model.
In contrast to VAEs which rely on variational proxies, we derive a novel objective that directly incorporate the bisimulation bounds.
Furthermore, while VAEs learn stochastic mappings to the latent space which need be determinized or even entirely reconstructed from data at the deployment time to obtain the guarantees, our WAE has no such requirements, and learn \emph{all the necessary components to obtain the guarantees during learning}, and does not require such post-processing operations.

Those theoretical claims are reflected in our experiments: policies are distilled up to $10$ times faster through WAE- than VAE-MDPs and provide better abstraction quality and performance in general, without the need for setting up annealing schemes and training phases, nor prioritized buffer and extra regularizer.
%In addition to recovering the original policy performance, some of our distilled policies even outperform them,
Our distilled policies are able to recover (and sometimes even outperform) the original policy performance,
highlighting the representation quality offered by our new framework: the distillation is able to remove some non-robustness of the input RL policy.
Finally, we formally verified \emph{time-to-failure} properties (e.g., \citealt{DBLP:conf/focs/Pnueli77}) to emphasize the applicability of our approach.

\smallparagraph{Other Related Work.}~%
Complementary works approach safe RL via formal methods \citep{DBLP:conf/tacas/Junges0DTK16,DBLP:conf/aaai/AlshiekhBEKNT18,jansen_et_al:LIPIcs:2020:12815,DBLP:conf/atal/SimaoJS21}, aimed at formally ensuring safety \emph{during RL}, all of which require providing an abstract model of the safety aspects of the environment.
They also include the work of \citet{DBLP:conf/fmcad/AlamdariAHL20}, applying synthesis and model checking on
%deep-RL distillations,
policies distilled from RL,
without quality guarantees.
%
Other frameworks share our goal of verifying deep-RL policies \citep{DBLP:conf/formats/Bacci020, DBLP:conf/ijcai/CarrJT20} but rely on a known environment model, among other assumptions (e.g., deterministic or discrete environment).
Finally, \emph{DeepSynth} \citep{DBLP:conf/aaai/HasanbeigJAMK21} allows learning a formal model from execution traces, with the different purpose of guiding the agent towards sparse and non-Markovian rewards.
%They also encompass \citet{DBLP:conf/fmcad/AlamdariAHL20} focus on %model-checking and synthesizing controllers based on 
%tree-based policies distilled from deep-RL, without considering abstraction quality guarantees.


%
% For learning the latent space,
On the latent space training side, WWAEs \citep{DBLP:journals/corr/abs-1902-09323/zhang19} reuse OT as latent regularizer discrepancy (in Gaussian closed form), whereas we derive two regularizers involving OT.
These two are, in contrast, optimized via the dual formulation of Wasserstein, as in \emph{Wassertein-GANs} \citep{DBLP:conf/icml/ArjovskyCB17}.
Similarly to \emph{VQ-VAEs} \citep{DBLP:conf/nips/OordVK17} and \emph{Latent Bernoulli AEs} \citep{DBLP:conf/icml/FajtlAMR20}, our latent space model learns discrete spaces via deterministic encoders, but relies on a smooth approximation instead of using the straight-through gradient estimator.

Works on \emph{representation learning} for RL \citep{DBLP:conf/icml/GeladaKBNB19,DBLP:conf/nips/CastroKPR21,DBLP:conf/iclr/0001MCGL21,DBLP:journals/corr/abs-2112-15303} consider bisimulation metrics to optimize the representation quality,
%aim at providing the same kind of representation quality that we have for discrete spaces: % for learning from unstructured observations (e.g., images).
% Derived from this metric, auxiliary tasks are optimized to learn a representation on which RL can directly be applied. 
and aim at learning (continuous) representations which capture bisimulation, so that two states close in the representation are guaranteed to provide close and relevant information to optimize the performance of the controller.
In particular, as in our work, \emph{DeepMDPs} \citep{DBLP:conf/icml/GeladaKBNB19} are learned by optimizing \emph{local losses}, by assuming a deterministic MDP and without verifiable confidence measurement.
