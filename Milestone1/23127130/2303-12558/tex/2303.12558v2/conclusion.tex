% In this work, we presented VAE-MDPs, a framework for learning discrete latent models of unknown, continuous-spaces environments with bisimulation guarantees.
% We detailed how such latent models can be learned by executing an RL policy in the environment and showed that the procedure yields a distilled version of the latter as a side effect.
% To provide the guarantees, we introduced new local losses bounds aimed at discrete latent spaces with their PAC-efficient approximation algorithm derived from the execution of the distilled policy.
% All this enables the verification of RL policies for unknown continuous MDPs.

% Experimental results demonstrate the feasibility of our approach through the PAC bounds and the performance of the distilled policy achieved for various environments.
% Our tool can also be used to highlight the lack of robustness of input policies when the distillation fails.

% Complementary to safe-RL approaches addressed via formal methods, we emphasize the ability of our tool to be coupled with such algorithms when no model of the environment is known a priori.
% The applicability of our method enables its use in future work for real-world case studies and
% %(e.g., watershed management, \citealt{REN2021103049}) and
% more complex settings, such as multi-agent systems.
% \begin{itemize}
%     \item Recall our two main contributions: (i) the new local-loss bounds aimed at discrete latent spaces and their PAC-efficient approximation algorithm with (ii) policy distillation
%     \item Highlight the fact that our framework now enables verification of RL policies for continuous MDPs
%     \item Highlight the amazing results we obtained experimentally and that our tool can also be used to find when distilled policies are not so good (indicating that the RL policy is perhaps not so robust)
%     \item Point to some interesting future directions or open questions we did not address
% \end{itemize}

We presented  WAE-MDPs, a framework for learning formally verifiable distillations of RL policies with bisimulation guarantees.
% Formal verification is enabled by abstracting the unknown continuous environment to a discrete model, learned from the execution of the RL policy.
The latter, along with the learned abstraction of the unknown continuous environment to a discrete model, %learned from the execution of the RL policy,
enables the verification.
% The resulting learning algorithm is supported by guarantees that ensure the model quality.
Our method overcomes the limitations of VAE-MDPs and our results show that it outperforms the latter in terms of learning speed, model quality, and performance, in addition to being supported by stronger learning guarantees.
As mentioned by \citet{DBLP:journals/corr/abs-2112-09655}, distillation failure reveals the lack of robustness of original RL policies.
In particular, we found that distilling highly noise-sensitive RL policies (such as robotics simulations, e.g., \citealt{todorov2012mujoco}) is laborious, even though the result remains formally verifiable.

We demonstrated the feasibility of our approach through the verification of reachability objectives, which are building blocks for stochastic model-checking \citep{DBLP:BK08}.
Besides the scope of this work, the verification of general discounted $\omega$-regular properties is theoretically allowed in our model via the rechability to components of standard constructions based on automata products (e.g., \citealt{%DBLP:BK08,
DBLP:conf/cav/BaierK0K0W16,DBLP:conf/cav/SickertEJK16}), and discounted games algorithms \citep{DBLP:conf/fsttcs/ChatterjeeAMR08}.
%
Beyond distillation, our results, supported by Thm.~\ref{thm:latent-execution-objective}, suggest that our WAE-MDP can be used as a \emph{general latent space learner} for RL, further opening possibilities to combine RL and formal methods \emph{online} when no formal model is a priori known, and  address this way safety in  RL with guarantees. % to provide formal guarantees on the behaviors of the agent to guide .
%As mentioned by \citet{DBLP:journals/corr/abs-2112-09655}, distillation failure highlights the lack of robustness of original RL policies.
%In particular, we found that distilling highly noise-sensitive RL policies (such as robotics simulations, e.g., \citealt{todorov2012mujoco}) can turn out to be laborious, even though the result remains formally verifiable.
% In particular, policies distilled from robotics simulations (e.g., \citealt{todorov2012mujoco}), although formally verifiable, resulted in inferior performance policies. 