%\subsubsection{The Objective Function: action encoder variant}
Given Assumption~\ref{assumption:vae-mdp}, we consider the OT between \emph{local} distributions,
where traces are drawn from episodic RL processes or infinite interactions
%\citep{DBLP:conf/nips/Huang20}
(we show in Appendix~\ref{appendix:discrepancy-measure} that considering the OT between trace-based distributions in the limit amounts to reasoning about stationary distributions).
% We learn the latent space model by minimizing
Our goal is to minimize
$\wassersteindist{\transitiondistance}{\stationary{\policy}}{\stationarydecoder}$ so that
%
\begin{equation}
    \stationarydecoder\fun{\state, \action, \reward, \state'} =  \int_{\latentstates \times \latentactions \times \latentstates} \decoder\fun{\state, \action, \reward, \state' \mid \latentstate, \latentaction, \latentstate'} \, d\latentstationaryprior\fun{\latentstate, \latentaction, \latentstate'}, \label{eq:stationary-decoder}
    %= \expectedsymbol{\state, \action \sim \stationary{\policy}} \expectedsymbol{\latentstate \sim \embed_{\encoderparameter}\fun{\sampledot \mid \state}}\expectedsymbol{\latentstate' \sim \latentprobtransitions_{\decoderparameter}\fun{\sampledot \mid \latentstate, \action}} \decoder\fun{\reward, \state' \mid \latentstate'}
\end{equation}
%
where $\decoder$ is a transition decoder and $\latentstationaryprior$ denotes the stationary distribution of the latent model $\latentmdp_{\decoderparameter}$.
% so that $\latentstationaryprior\fun{\latentstate, \latentaction, \latentstate'} = \latentstationaryprior\fun{\latentstate} \cdot { \latentpolicy_{\decoderparameter}\fun{\latentaction \mid \latentstate}} \cdot \latentprobtransitions_{\decoderparameter}\fun{\latentstate' \mid \latentstate, \latentaction}$.
%
% The \emph{Wasserstein autoencoder} (WAE) \citep{DBLP:conf/iclr/TolstikhinBGS18} framework allows us to exploit this model to simplify 
As proved by \citet{Bousquet2017FromOT}, this model allows to derive a simpler form of the OT: % between $\stationary{\policy}$ and $\stationary{\decoderparameter}$
instead of finding the optimal coupling
of (i) the stationary distribution $\stationary{\policy}$ of $\mdp_\policy$ 
and (ii) the behavioral model $\stationary{\decoderparameter}$, in the primal definition of $\wassersteindist{\tracedistance}{\stationary{\policy}}{\stationarydecoder}$,
%from $\couplings{\stationary{\policy}}{\stationary{\decoderparameter}}$,
it is sufficient to find an encoder $\transitionencoder$ whose marginal is given by $Q\fun{\latentstate, \latentaction, \latentstate'} = \expectedsymbol{\state, \action, \state' \sim \stationary{\policy}} \transitionencoder\fun{\latentstate, \latentaction, \latentstate' \mid \state, \action, \state'}$ % = \embed_{\encoderparameter}\stationary{\policy}$,
and identical to $\stationary{\policy}$. % $\latentstationaryprior$ instead of finding the optimal coupling in the primal definition of \wassersteindist{\transitiondistance}{\stationary{\policy}}{\stationarydecoder} \citep{Bousquet2017FromOT,DBLP:conf/iclr/TolstikhinBGS18}.
This is summarized in the following Theorem, yielding a particular case of \emph{Wasserstein-autoencoder} \cite{DBLP:conf/iclr/TolstikhinBGS18}:
\begin{theorem}
%Let $\mdp$ be an MDP with state-action space $\states \times \actions$, $\latentmdp_{\decoderparameter}
%$ be an ergodic latent MDP of $\mdp$ with state-action space $\latentstates \times \latentactions$ and reward function $\latentrewards_{\decoderparameter}$,
%= \tuple{\latentstates, \latentactions, \latentprobtransitions_{\decoderparameter}, \latentrewards_{\decoderparameter}, \latentlabels, \atomicprops, \zinit}$ be a latent MDP of $\mdp$,
%$\policy$ and $\latentpolicy_{\decoderparameter}$ be respectively policies for $\mdp$ and $\latentmdp_{\decoderparameter}$,
% $\stationarydecoder \in \distributions{\states \times \actions \times \R \times \states}$ and $\decoder \colon \latentstates \times \latentactions \times \latentstates \to \distributions{\states \times \actions \times \R \times \states}$ be respectively a generator and a decoder as defined in Eq.~\ref{eq:stationary-decoder},
Let $\stationarydecoder$ and $\decoder$ be respectively a behavioral model and transition decoder as defined in Eq.~\ref{eq:stationary-decoder},
$\generative_{\decoderparameter}\colon \latentstates \to \states$ be a state-wise decoder, and 
% $\embeda_{\decoderparameter} \colon \latentstates \times \latentactions \to \actions$ be an action embedding function.
$\embeda_{\decoderparameter}$ be an action embedding function.
% \begin{equation*}
%     G_{\decoderparameter} \colon \latentstates \times \latentactions \times \latentstates \to \states \times \actions \times \R \times \states, \; \tuple{\latentstate,  \latentaction, \latentstate'} \mapsto \tuple{\generative_{\decoderparameter}\fun{\latentstate}, \embeda_{\decoderparameter}\fun{\latentstate, \latentaction}, {\latentrewards_{\decoderparameter}\fun{\latentstate, \latentaction}}, \generative_{\decoderparameter}\fun{\latentstate'}}.
% \end{equation*}
% Then, $\decoder$ is deterministic when $G_\decoderparameter$ is its Dirac function. In that case,
Assume $\decoder$ is deterministic with Dirac function
$G_{\decoderparameter}\fun{\latentstate, \latentaction, \latentstate'} = \tuple{\generative_{\decoderparameter}\fun{\latentstate}, \embeda_{\decoderparameter}\fun{\latentstate, \latentaction}, {\latentrewards_{\decoderparameter}\fun{\latentstate, \latentaction}}, \generative_{\decoderparameter}\fun{\latentstate'}}$, then
%Then,
%\begin{multline*}
\begin{equation*}
    \wassersteindist{\tracedistance}{\stationary{\policy}}{\stationarydecoder}% =& \inf_{\coupling \in \couplings{\stationary{\policy}}{\stationarydecoder}} \, \expectedsymbol{\tau, \tau' \sim \coupling} \transitiondistance\fun{\tau, \tau'}\\
    = \inf_{\transitionencoder: \, Q = \latentstationaryprior} \, \expectedsymbol{\state, \action, \reward, \state' \sim \stationary{\policy}} \, \expectedsymbol{\latentstate, \latentaction, \latentstate' \sim \transitionencoder\fun{\sampledot \mid \state, \action, \state'}} %\\
    \tracedistance\fun{\tuple{\state, \action, \reward, \state'}, G_{\decoderparameter}\fun{\latentstate, \latentaction, \latentstate'}}.
    % \Big[\distance_\states\fun{\state, \generative_{\decoderparameter}\fun{\latentstate}} + \distance_{\actions}\fun{\action, \embeda_{\decoderparameter}\fun{\latentstate, \latentaction}} + \left| r - \rewards_{\decoderparameter}\fun{\latentstate, \latentaction} \right| + \distance_{\states}\fun{\state', \generative_{\decoderparameter}\fun{\latentstate'}}\Big].
\end{equation*}
%\end{multline*}
\end{theorem}
%\begin{proof}
%Assume that $\decoder$ is deterministic with $G_{\decoderparameter}$ as its Dirac function.
%Then, the results follow directly from \citet[Theorem~1 and Corollary~1]{DBLP:conf/iclr/TolstikhinBGS18}:
%The result follows directly from
%\citet[Theorem~1 and Corollary~1]{DBLP:conf/iclr/TolstikhinBGS18}.
%\cite[Theorem~1 and Corollary~1]{DBLP:conf/iclr/TolstikhinBGS18}.
% \begin{align*}
%     \wassersteindist{\tracedistance}{\stationary{\policy}}{\stationarydecoder} &=
%     \inf_{\coupling \in \couplings{\stationary{\policy}}{\stationarydecoder}} \, \expectedsymbol{\tau, \tau' \sim \coupling} \transitiondistance\fun{\tau, \tau'}\\
%     &=\inf_{\embed: \, Q = \latentstationaryprior} \, \expectedsymbol{\state, \action, \reward, \state' \sim \stationary{\policy}} \, \expected{\latentstate, \latentaction, \latentstate' \sim \embed\fun{\sampledot \mid \state, \action, \state'}}{\distance_\states\fun{\state, \generative_{\decoderparameter}\fun{\latentstate}} + \distance_{\actions}\fun{\action, \embeda_{\decoderparameter}\fun{\latentstate, \latentaction}} + \left| r - \rewards_{\decoderparameter}\fun{\latentstate, \latentaction} \right| + \distance_{\states}\fun{\state', \generative_{\decoderparameter}\fun{\latentstate'}}}
% \end{align*}
%\end{proof}
%
Henceforth, fix
%$\embed_\encoderparameter \colon \states \to \distributions{\latentstates}$
$\embed_\encoderparameter \colon \states \to {\latentstates}$
and $\embed_{\encoderparameter}^{\scriptscriptstyle\actions} \colon \latentstates \times \actions \to \distributions{\latentactions}$ as parameterized
%state embedding function
state
and action encoders with
%$\embed_\encoderparameter\fun{\latentstate, \latentaction, \latentstate' \mid \state, \action, \state'} = \embed_{\encoderparameter}\fun{\latentstate \mid \state} \cdot \embed_{\encoderparameter}^{\scriptscriptstyle \actions}\fun{\latentaction \mid \latentstate, \action} \cdot \embed_{\encoderparameter}\fun{\latentstate' \mid \state'}$% for any $\state, \state' \in \states$, $\latentstate, \latentstate' \in \latentstates$
$\embed_\encoderparameter\fun{\latentstate, \latentaction, \latentstate' \mid \state, \action, \state'} = \condition{\embed_{\encoderparameter}\fun{\state}=\latentstate} \cdot \embed_{\encoderparameter}^{\scriptscriptstyle \actions}\fun{\latentaction \mid \latentstate, \action} \cdot \condition{\embed_{\encoderparameter}\fun{\state'} =\latentstate'}$%
% $\embed_\encoderparameter\fun{\latentstate, \latentaction, \latentstate' \mid \state, \action, \state'} = \condition{=}\fun{\tuple{\embed_{\encoderparameter}\fun{\state}, \embed_{\encoderparameter}\fun{\state'}}, \tuple{\latentstate, \latentstate'}} \cdot \embed_{\encoderparameter}^{\scriptscriptstyle \actions}\fun{\latentaction \mid \latentstate, \action}$%
, and define the marginal encoder as $Q_\encoderparameter = \expectedsymbol{\state, \action, \state' \sim \stationary{\policy}} \embed_{\encoderparameter}\fun{\cdot \mid \state, \action, \state'}$.
% The action embedding function $\embeda_{\encoderparameter, \decoderparameter}$ can be retrieved via $\embeda_{\encoderparameter, \decoderparameter}\fun{\state, \latentaction} = \expectedsymbol{\latentstate \sim \embed_{\encoderparameter}\fun{\sampledot \mid \state}}{\embeda_{\decoderparameter}\fun{\latentstate, \latentaction}}$.
%
Training the model components can be achieved via the objective:
%We name this latent space model \emph{Wasserstein auto-encoded MDP} (WAE-MDP), with the objective:
\begin{align*}
    \min_{\encoderparameter, \decoderparameter} \, \expectedsymbol{\state, \action, \reward, \state' \sim \stationary{\policy}} \, \expectedsymbol{\latentstate, \latentaction, \latentstate' \sim \embed_{\encoderparameter}\fun{\sampledot \mid \state, \action, \state'}}  \tracedistance\fun{\tuple{\state, \action, \reward, \state'}, G_{\decoderparameter}\fun{\latentstate, \latentaction, \latentstate'}} + \beta \cdot \divergencesymbol\fun{\encoder, \latentstationaryprior},
\end{align*}
where $\divergencesymbol$ is an arbitrary discrepancy metric and $\beta > 0$ a hyperparameter.
Intuitively, the encoder $\embed_{\encoderparameter}$ can be learned by enforcing its marginal distribution $\encoder$ to match $\latentstationaryprior$ through this discrepancy.
% \begin{remark}[VAEs vs. WAEs]
% VAE- and WAE-MDP both involve a transition reconstruction term and a regularization term penalizing the discrepancy between the distributions over the latent variables produced respectively from the encoder and in the latent model.
% % Given $\state \in \states$, VAEs require the encoder $\embed_{\encoderparameter}$ being stochastic and enforce $\actionencoder\fun{\sampledot \mid \latentstate, \action}$ and $\embed_{\encoderparameter}\fun{\sampledot \mid \state'}$ to respectively match $\latentpolicy_{\decoderparameter}\fun{\sampledot \mid \latentstate}$ $\latentprobtransitions_{\decoderparameter}\fun{\sampledot \mid \latentstate, \latentaction}$ for all the different actions produced from $\policy\fun{\sampledot \mid \action}$ and 
% Given $\state \in \states$ and its embedding $\latentstate \in \latentstates$, VAE requires the encoder $\embed_{\encoderparameter}$ being stochastic and enforces $\embed_{\encoderparameter}\fun{\sampledot \mid \state'}$ to match  $\latentprobtransitions_{\latentpolicy_{\decoderparameter}}\fun{\sampledot \mid \latentstate}$ for all $\state' \sim \probtransitions_{\policy}\fun{\sampledot \mid \state}$ produced in the real environment.
% This eventually breaks the dependency of $\embed_{\encoderparameter}$ on $\state'$ and leads to \emph{mode collapse}. % and $\latentstate \sim \embed_{\encoderparameter}\fun{\sampledot \mid \state}$ produced.
% On the other hand, WAE has no such requirement on the encoding function and intuitively forces the mixture $\encoder$
% %$\expected{\state, \action, \state' \sim \stationary{\policy}}{\embed_{\encoderparameter}\fun{\sampledot \mid \state, \action, \state'}}$
% to match $\latentstationaryprior$ which has not impact on any dependency and naturally allows to avoid collapsing issues \citep{DBLP:conf/iclr/TolstikhinBGS18}.
% \end{remark}
\begin{remark}%
% When $\policy$ already produces discrete actions, i.e., when $\policy$ is a latent policy (cf. Rem~\ref{rmk:latent-policy-execution}), or $\mdp$ has a discrete action space,
If $\mdp$ has a discrete action space, then learning $\latentactions$ is not necessary. We can set $\latentactions = \actions$ using identity functions for the action encoder and decoder
% ignoring learning $\latentactions$ can be achieved by respectively setting
% % $\actionencoder\fun{\sampledot \mid \latentstate, \cdot}$ or both $\actionencoder\fun{\sampledot \mid \latentstate, \cdot}$ and $\embeda_{\decoderparameter}\fun{\latentstate, \sampledot}$ to the identity
% %
% the action encoder and the action encoder/decoder pair to the indentity function
% This typically occurs when $\policy$ is a latent policy (cf. Rem.~\ref{rmk:latent-policy-execution}) or when $\mdp$ has already a discrete action space.
% To ignore learning the action space, it suffices to take $\actionencoder\fun{\sampledot \mid \latentstate, \cdot}$ and $\embeda_{\decoderparameter}\fun{\latentstate, \sampledot}$ as the identity function.
(details in Appendix~\ref{appendix:discrete-action-space}).
\end{remark}
% \smallparagraph{Dealing with discrete actions.}~When the policy $\policy$ executed in $\mdp$ already produces discrete actions, learning $\latentactions$ is, in many cases, not necessary.
% This typically occurs when $\policy$ is a latent policy (cf. Rem.~\ref{rmk:latent-policy-execution}) or when $\mdp$ has already a discrete action space.
% To ignore learning the action space, it suffices to take $\actionencoder\fun{\sampledot \mid \latentstate, \cdot}$ and $\embeda_{\decoderparameter}\fun{\latentstate, \sampledot}$ as the identity function.
% More details are given in Appendix~\ref{sec:discrete-latent-spaces}.

% (notice that the premise of Assumption~\ref{assumption:action-decoder} implies the one of Assumption~\ref{assumption:action-encoder}).
%\begin{remark} \label{rmk:zero-action-distance}
%When the original MDP $\mdp$ with action space $\actions$ operates under a latent policy $\latentpolicy \colon \latentstates \to \distributions{\latentactions}$ while its original action space is continuous (i.e., $\actions \neq \latentactions$; Assumption~\ref{assumption:action-encoder} holds while Assumption~\ref{assumption:action-decoder} does not), then each latent action produced is embedded back to the original action space via $\embeda_{\decoderparameter}$, which allows to execute it in $\mdp$ (see Remark~\ref{rmk:latent-policy-execution}).
%Therefore, the projection of $G_{\decoderparameter}$ on the action space is given by $\distance_{\actions}\fun{\embeda_{ \decoderparameter}\fun{\latentstate, \latentaction}, \embeda_{\decoderparameter}\fun{\latentstate, \latentaction}}$.
% When the state encoder $\embed_{\encoderparameter}$ is deterministic, this distance thus equals zero.
%\end{remark}
%
\iffalse
\begin{algorithm}%[H]
% \setstretch{1.35}
\caption{Wasserstein$^2$ Auto-Encoded MDP}\label{alg:wwae-mdp}
\DontPrintSemicolon
\KwIn{batch size $N$, max. step $T$, no. of regularizer updates $\ncritic$, penalty coefficient $\delta$}
%\SetNoFillComment
\SetKwComment{Comment}{$\triangleright$\ }{}
\SetCommentSty{textnormal}
\LinesNotNumbered 
\SetKwBlock{Begin}{function}{end function}
%test \;
\For{$t = 1$ to $T$}{
    \For{$i = 1$ to $N$}{
        Draw $\tuple{\state^i, \action^i, \reward^i, \state^{\prime\, i}}$ from $\stationary{\policy}$,
        $\;\tuple{\latentstate_{\embed}^{\,i}, \latentaction_{\embed}^{\,i}, \latentstate_\embed^{\prime\, i}}$ from $\embed_{\encoderparameter}({\sampledot \mid \state^i, \action^i, \state^{\prime\, i}})$, 
        $\;\latentstate^{\prime\, i}_{\scriptscriptstyle \latentprobtransitions}$ from $\latentprobtransitions_{\decoderparameter}({\sampledot \mid \latentstate_{\embed}^{\,i}, \latentaction_{\embed}^{\, i}})$, and
        $\tuple{\latentstate^{\, i}_{\stationary{}}, \latentaction^{\, i}_{\stationary{}}, \latentstate^{\prime\, i}_{\stationary{}}}$ resp. from $\latentstationaryprior$, $\latentpolicy_{\decoderparameter}({\sampledot \mid \latentstate^{\, i}_{\stationary{}}})$, and $\latentprobtransitions_{\decoderparameter}({\sampledot \mid \latentstate^{\, i}_{\stationary{}}, \latentaction^{\, i}_{\stationary{}}})$\;
    }
    $
    %\begin{aligned}
    	\mathcal{W} \gets \textstyle \sum_{i = 1}^{N} \varphi_{\wassersteinparameter}^{\stationary{}}({\latentstate_{\embed}^{\,i}, \latentaction_{\embed}^{\,i}, \latentstate^{\prime\, i}_{\scriptscriptstyle \latentprobtransitions}})
    	- \varphi_{\wassersteinparameter}^{\stationary{}}({\latentstate^{\, i}_{\stationary{}}, \latentaction^{\, i}_{\stationary{}}, \latentstate^{\prime\, i}_{\stationary{}}}) +
    	\varphi_{\wassersteinparameter}^{\probtransitions}({\state^{i} \!, \action^{i} \!, \latentstate_{\embed}^{\, i}, \latentaction_{\embed}^{\, i}, \latentstate^{\prime\, i}_{\embed}}) - \varphi_{\wassersteinparameter}^{\probtransitions}({\state^{i}\!, \action^{i}\!, \latentstate_{\embed}^{\, i}, \latentaction_{\embed}^{\, i}, \latentstate^{\prime\, i}_{\scriptscriptstyle \latentprobtransitions}})$\;
    %\end{aligned}
    %$\; %$\!\!\!\!$ \Comment*[r]{$\steadystateregularizer{\policy}, \localtransitionloss{\stationary{\policy}}$ ($\max_{\wassersteinparameter}$)}
    %$%\begin{aligned}
    	$P \gets \textstyle \sum_{i = 1}^{N} \textsc{Gp}\big({\varphi_{\wassersteinparameter}^{\stationary{}}, \tuple{\latentstate_{\embed}^{\,i}, \latentaction_{\embed}^{\,i}, \latentstate^{\prime\, i}_{\scriptscriptstyle \latentprobtransitions}}, \tuple{\latentstate^{\, i}_{\stationary{}}, \latentaction^{\, i}_{\stationary{}}, \latentstate^{\prime\, i}_{\stationary{}}}}\big) + \textsc{Gp}\big({\latentstate' \mapsto \varphi_{\wassersteinparameter}^{\probtransitions}\fun{\state^i, \action^i, \latentstate^{\, i}, \latentaction^{\, i}, \latentstate'}, \latentstate^{\prime \, i}_{\embed}, \latentstate^{\prime\, i}_{\scriptscriptstyle \latentprobtransitions}}\big)
    %\end{aligned}
    $\;
    Update $\wassersteinparameter$ by ascending $\nicefrac{1}{N} \cdot \fun{\beta  \,\mathcal{W} - \delta \, P}$\;
    \If{$t \bmod \ncritic = 0$
    %\Comment*[r]{Auto-Encoder loss ($\min_{\encoderparameter, \decoderparameter}$)}
    }{
     $\mathcal{L} \gets \sum_{i = 1}^{N} \norm{\state^i- \generative_{\decoderparameter}\fun{\latentstate_{\embed}^{\, i}}} + \norm{\action^i - \embeda_{\decoderparameter}\fun{\latentstate_{\embed}^{\, i}, \latentaction_{\embed}^{\, i}}} + \left| \reward^i - \latentrewards_{\decoderparameter}\fun{\latentstate_{\embed}^{\, i}, \latentaction_{\embed}^{\, i}} \right| + \norm{\state^{\prime\, i} - \generative_{\decoderparameter}\fun{\latentstate_{\embed}^{\prime\, i}}}$\;
     Update $\tuple{\encoderparameter, \decoderparameter}$ by descending $\nicefrac{1}{N}\cdot \fun{\mathcal{L} + {\beta} \, \mathcal{W}}$\;
    }
}
\Begin($\textsc{Gp}\fun{\varphi_{\wassersteinparameter},\vect{x}, \vect{y}}$ \hfill $\triangleright\ $ Gradient penalty \citep{DBLP:conf/nips/GulrajaniAADC17} for $\varphi_{\wassersteinparameter} \colon \R^n \to \R$ and $\vect{x}, \vect{y} \in \R^n$){
%\Comment*[r]{$\varphi_{\wassersteinparameter} \colon \R^n \to \R$, $\vect{x}, \vect{y} \in \R^n$})
%{
$\epsilon \gets \mathit{U}\fun{0, 1}$; $\vect{z} \gets \epsilon \vect{x} + (1 - \epsilon) \vect{y}$ \Comment*[r]{random noise; straight lines between $\vect{x}$ and $\vect{y}$}
\Return{$\fun{\norm{\gradient_{\vect{z}} \, \varphi_{\wassersteinparameter}\fun{\vect{z}}} - 1}^2$}
}
\end{algorithm}
\fi
\begin{algorithm}%[H]
% \setstretch{1.35}
\caption{Wasserstein$^2$ Auto-Encoded MDP}\label{alg:wwae-mdp}
\DontPrintSemicolon
\KwIn{batch size $N$, max. step $T$, no. of regularizer updates $\ncritic$, penalty coefficient $\delta > 0$}
%\SetNoFillComment
\SetKwComment{Comment}{$\triangleright$\ }{}
\SetCommentSty{textnormal}
\LinesNotNumbered 
\SetKwBlock{Begin}{function}{end function}
%test \;
\For{$t = 1$ to $T$}{
    \For{$i = 1$ to $N$}{
        Sample a transition ${\state_i, \action_i, \reward_i, \state^{\prime}_i}$ from the original environment via $\stationary{\policy}$\;
        Embed the transition into the latent space by drawing ${\latentstate_{i}, \latentaction_{i}, \latentstate^{\prime}_{i}}$ from $\embed_{\encoderparameter}({\sampledot \mid \state_i, \action_i, \state^{\prime}_{i}})$\;
        Make the latent space model transition to the next latent state:  $\latentstate^{\star}_{i} \sim \latentprobtransitions_{\decoderparameter}({\sampledot \mid \latentstate_{i}, \latentaction_{i}})$\;
        Sample a latent transition from $\latentstationaryprior$:
        $\latentvariable_i \sim \latentstationaryprior$, $\latentaction'_i \sim \latentpolicy_{\decoderparameter}\fun{\sampledot \mid \latentvariable_i}$, and $\latentvariable_i^{\prime} \sim \latentprobtransitions_{\decoderparameter}\fun{\sampledot \mid \latentvariable_i, \latentaction^{\prime}_i}$\;
    }
    $
    %\begin{aligned}
    	\mathcal{W} \gets \textstyle \sum_{i = 1}^{N} \steadystatenetwork({\latentstate_{i}, \latentaction_{i}, \latentstate^{\star}_{i}})
    	- \steadystatenetwork({\latentvariable_{i}, \latentaction^{\prime}_{i}, \latentvariable^{\prime}_{i}}) +
    	\transitionlossnetwork({\state_{i}, \action_{i}, \latentstate_{i}, \latentaction_{i}, \latentstate^{\prime}_{i}}) -  \transitionlossnetwork({\state_{i}, \action_{i}, \latentstate_{i}, \latentaction_{i}, \latentstate^{\star}_{i}})$\;
    %\end{aligned}
    %$\; %$\!\!\!\!$ \Comment*[r]{$\steadystateregularizer{\policy}, \localtransitionloss{\stationary{\policy}}$ ($\max_{\wassersteinparameter}$)}
    %$%\begin{aligned}
    	$P \gets \textstyle \sum_{i = 1}^{N} \textsc{Gp}\big({\steadystatenetwork, \tuple{\latentstate_{i}, \latentaction_{i}, \latentstate^{\star}_{i}}, \tuple{\latentvariable_{i}, \latentaction^{\prime}_{i}, \latentvariable^{\prime}_i}}\big) + \textsc{Gp}\big({\vx \mapsto \transitionlossnetwork\fun{\state_i, \action_i, \latentstate_{i}, \latentaction_{i}, \vx}, \latentstate^{\prime}_{i}, \latentstate^{\star}_{i}}\big)
    %\end{aligned}
    $\;
    Update the Lipschitz networks parameters $\wassersteinparameter$ by ascending $\nicefrac{1}{N} \cdot \fun{\beta  \,\mathcal{W} - \delta \, P}$\;
    \If{$t \bmod \ncritic = 0$
    %\Comment*[r]{Auto-Encoder loss ($\min_{\encoderparameter, \decoderparameter}$)}
    }{
     $\mathcal{L} \gets \sum_{i = 1}^{N} \distance_{\states}\fun{\state_i,  \generative_{\decoderparameter}\fun{\latentstate_{i}}} + \distance_{\actions}\fun{\action_i, \embeda_{\decoderparameter}\fun{\latentstate_{i}, \latentaction_{i}}} + \distance_{\rewards}\fun{\reward_i,  \latentrewards_{\decoderparameter}\fun{\latentstate_{i}, \latentaction_{i}}} + \distance_{\states}\fun{\state^{\prime}_i, \generative_{\decoderparameter}\fun{\latentstate^{\prime}_{i}}}$\;
     Update the latent space model parameters $\tuple{\encoderparameter, \decoderparameter}$ by descending $\nicefrac{1}{N}\cdot \fun{\mathcal{L} + {\beta} \, \mathcal{W}}$\;
    }
}
\Begin($\textsc{Gp}\fun{\varphi_{\omega},\vect{x}, \vect{y}}$ \hfill $\triangleright\ $ \textbf{Gradient penalty} for $\varphi_{\wassersteinparameter} \colon \R^n \to \R$ and $\vect{x}, \vect{y} \in \R^n$){
\ifarxiv
$\epsilon \sim \mathit{U}\fun{0, 1}$ \Comment*[r]{random noise}
$\tilde{\vect{x}} \gets \epsilon \vect{x} + (1 - \epsilon) \vect{y}$ \Comment*[r]{straight lines between $\vect{x}$ and $\vect{y}$}
\else
$\epsilon \sim \mathit{U}\fun{0, 1}$; $\tilde{\vect{x}} \gets \epsilon \vect{x} + (1 - \epsilon) \vect{y}$ \Comment*[r]{random noise; straight lines between $\vect{x}$ and $\vect{y}$}
\fi
\Return{$\fun{\norm{\gradient_{\tilde{\vect{x}}} \, \varphi_{\wassersteinparameter}\fun{\tilde{\vect{x}}}} - 1}^2$}
}
\end{algorithm}
% \subsection{Guaranteed Abstraction Quality and Distillation via Wasserstein Regularization}
%In the same spirit than \emph{Wasserstein-Wasserstein autoencoders} \citep{DBLP:journals/corr/abs-1902-09323/zhang19},
When $\policy$ is executed in $\mdp$, %the original model,
observe that
% the action encoder $\actionencoder$ enables its \emph{parallel execution} in the latent model:
its \emph{parallel execution} in $\latentmdp_{\decoderparameter}$ % the latent model 
is enabled by the action encoder $\actionencoder$:
%
% given an original state $\state \in \states$, then the action $\action \sim \policy\fun{\sampledot \mid \state}$ is prescribed by $\policy$, which is then embedded in the latent space via $\latentaction \sim \actionencoder\fun{\sampledot \mid \embed_{\encoderparameter}\fun{\state}, \action}$ (cf. Fig.~\ref{subfig:latent-fow-distillation}).
given an original state $\state \in \states$, $\policy$ first prescribes the action $\action \sim \policy\fun{\sampledot \mid \state}$, which is then embedded in the latent space via $\latentaction \sim \actionencoder\fun{\sampledot \mid \embed_{\encoderparameter}\fun{\state}, \action}$ (cf. Fig.~\ref{subfig:latent-fow-distillation}).
%
% The local transition loss resulting from this interaction is
% $\localtransitionloss{\stationary{\policy}} = \expectedsymbol{\state, \tuple{\action, \latentaction} \sim \stationary{\policy}} \wassersteindist{\distance_{\latentstates}}{\embed_{\encoderparameter}\probtransitions\fun{\sampledot \mid \state, \action}}{\latentprobtransitions\fun{\sampledot \mid \embed_{\encoderparameter}\fun{\state}, \latentaction}}$
This parallel execution, along with setting  $\divergencesymbol$ to $\wassersteinsymbol{\transitiondistance}$, yield an upper bound on the latent regularization, compliant with the bisimulation bounds. %`, as claimed in the following.
%guarantees of Eq.~\ref{eq:bidistance-bound}.
A two-fold regularizer is obtained thereby, defining the foundations of our objective function:%
% The following Lemma leads a which are the building blocks for defining our objective function.
\begin{restatable}{lemma}{regularizerlemma}\label{lem:regularizer-upper-bound}
%Let $\originaltolatentstationary{} \in \distributions{\latentstates \times \latentactions \times \latentstates}, \; \tuple{\latentstate, \latentaction, \latentstate'} \mapsto \expectedsymbol{\state, \action \sim \stationary{\policy}}[\embed_{\encoderparameter}(\latentstate \mid \state) \cdot \embed_{\encoderparameter}^{\scriptscriptstyle \actions}(\latentaction \mid \latentstate, \action) \cdot \latentprobtransitions_{\decoderparameter}(\latentstate' \mid \latentstate, \latentaction)]$ denote the distribution over the state-action pairs drawn from 
%the steady-state of $\mdp_{\policy}$
% Let $\originaltolatentstationary{} \in \distributions{\latentstates \times \latentactions \times \latentstates}, \; \tuple{\latentstate, \latentaction, \latentstate'} \mapsto \expectedsymbol{\state, \action \sim \stationary{\policy}}[\condition{\embed_{\encoderparameter}(\state)=\latentstate} \cdot \embed_{\encoderparameter}^{\scriptscriptstyle \actions}(\latentaction \mid \latentstate, \action) \cdot \latentprobtransitions_{\decoderparameter}(\latentstate' \mid \latentstate, \latentaction)]$ denote the distribution of drawing state-action pairs from the
Define $\originaltolatentstationary{}\fun{\latentstate, \latentaction, \latentstate'} = \expectedsymbol{\state, \action \sim \stationary{\policy}}[\condition{\embed_{\encoderparameter}(\state)=\latentstate} \cdot \embed_{\encoderparameter}^{\scriptscriptstyle \actions}(\latentaction \mid \latentstate, \action) \cdot \latentprobtransitions_{\decoderparameter}(\latentstate' \mid \latentstate, \latentaction)]$ as the distribution of drawing state-action pairs from interacting with $\mdp$, embedding them to the latent spaces, and finally letting them transition to their successor state in $\latentmdp_{\decoderparameter}$. Then,
$
%\begin{align*}
    \wassersteindist{\transitiondistance}{\encoder}{ \latentstationaryprior}
    \leq \wassersteindist{\transitiondistance}{\latentstationaryprior}{\originaltolatentstationary{}} + \localtransitionloss{\stationary{\policy}}.
    % \leq \wassersteindist{\transitiondistance}{\latentstationaryprior}{\originaltolatentstationary{}} + \expectedsymbol{\state, \action \sim \stationary{\policy}}\expectedsymbol{\latentaction \sim \embed_{\encoderparameter}^{\actions}\fun{\sampledot \mid \embed_{\encoderparameter}\fun{\state}, \action}} \wassersteindist{\distance_{\latentstates}}{\embed_{\encoderparameter}\probtransitions\fun{\sampledot \mid \state, \action}}{\latentprobtransitions_{\decoderparameter}\fun{\sampledot \mid \embed_{\encoderparameter}\fun{\state}, \latentaction}}.
%\end{align*}
$
\end{restatable}
%
We therefore define the \waemdp (\emph{Wasserstein-Wasserstein auto-encoded MDP}) objective as:
\begin{equation*}
    %\min_{\encoderparameter, \decoderparameter} \expectedsymbol{\state,  \action, \state' \sim \stationary{\latentpolicy}} \, 
    %\expectedsymbol{\latentstate, \latentaction, \latentstate' \sim \embed_{\encoderparameter}\fun{\sampledot \mid \state, \action, \state'}}
    %\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!
    \min_{\encoderparameter, \decoderparameter} \! \! \! \expectedsymbol{}_{\substack{\state, \action, \state' \sim \stationary{\policy} \\ \latentstate, \latentaction, \latentstate' \sim \embed_{\encoderparameter}({\sampledot \mid \state, \action, \state'})}} \! \! \! \!
    \left[{\distance_{\states}\fun{\state, \generative_{\decoderparameter}\fun{\latentstate}} + 
    \distance_{\actions}\fun{\action, \embeda_{\decoderparameter}\fun{\latentstate, \latentaction}} + 
    \distance_{\states}\fun{\state', \generative_{\decoderparameter}\fun{\latentstate'}}}\right] + \localrewardloss{\stationary{\policy}} + \beta \cdot ({ \steadystateregularizer{\policy} + \localtransitionloss{\stationary{\policy}}}),
\end{equation*}
% \begin{aligned}
%     \min_{\encoderparameter, \decoderparameter} \expectedsymbol{}_{\substack{\state, \action, \reward, \state' \sim \stationary{\policy} \\ \latentstate, \latentaction, \latentstate' \sim \embed_{\encoderparameter}\fun{\sampledot \mid \state, \action, \state'}}}
% \end{aligned}
% \begin{aligned}
%     &\big[\tracedistance\fun{\tuple{\state, \action, \reward, \state'}, G_{\decoderparameter}\fun{\latentstate, \latentaction, \latentstate'}}\\
%     & \quad\quad\quad\quad + \beta \cdot \fun{ \wassersteindist{\distance_{\latentstates}}{\originaltolatentstationary{}}{ \latentstationaryprior} +
%     \wassersteindist{\distance_{\latentstates}}{\embed_{\encoderparameter}\probtransitions\fun{\sampledot \mid \state, \action}}{\latentprobtransitions_{\decoderparameter}\fun{\sampledot \mid \latentstate, \latentaction}}}\big]
% \end{aligned}
%
%
%\begin{equation*}
% \min_{\encoderparameter, \decoderparameter} \! \! \! \expectedsymbol{}_{\substack{\state, \action, \reward, \state' \sim \stationary{\policy} \\ \latentstate, \latentaction, \latentstate' \sim \embed_{\encoderparameter}\fun{\sampledot \mid \state, \action, \state'}}} \! \! \! \!
% \left[
% \tracedistance\fun{\tuple{\state, \action, \reward, \state'}, G_{\decoderparameter}\fun{\latentstate, \latentaction, \latentstate'}} +
% \beta \cdot \fun{ \steadystateregularizer{\policy} + \wassersteindist{\distance_{\latentstates}}{\embed_{\encoderparameter}\probtransitions\fun{\sampledot \mid \state, \action}}{\latentprobtransitions_{\decoderparameter}\fun{\sampledot \mid \latentstate, \latentaction}}}
% \right],
% \end{equation*}
where $\steadystateregularizer{\policy} = \wassersteindist{\transitiondistance}{\originaltolatentstationary{}}{\latentstationaryprior}$ and
$\localtransitionloss{\stationary{\policy}}$ are respectively called \emph{steady-state} and \emph{transition} regularizers.
The former allows to quantify the distance between the stationary distributions respectively induced by $\policy$ in $\mdp$ and $\latentpolicy_{\decoderparameter}$ in $\latentmdp_{\decoderparameter}$, further enabling the distillation. % of $\policy$ into $\latentpolicy_{\decoderparameter}$.
The latter allows to learn the latent dynamics.
%and is actually closely related to the local transition loss:
%
%
%\smallparagraph{Distillation.}~
Note that $\localrewardloss{\stationary{\policy}}$ and $\localtransitionloss{\stationary{\policy}}$ --- set over $\stationary{\policy}$ instead of $\stationary{\latentpolicy_{\decoderparameter}}$ --- are not sufficient to ensure the bisimulation bounds (Eq.~\ref{eq:bidistance-bound}): running $\policy$ in $\latentmdp_{\decoderparameter}$ depends on the parallel execution of $\policy$ in the original model, which does not permit its (conventional) verification.  % reasoning about the latent model alone.
Breaking this dependency is enabled by learning the distillation $\latentpolicy_{\decoderparameter}$ through $\steadystateregularizer{\policy}$, as shown in Fig.~\ref{subfig:latent-fow-distillation}:
%
%The distillation of the original policy $\policy$ into $\latentpolicy_{\decoderparameter}$ is enabled through the latent flow shown in Fig.~\ref{subfig:latent-fow-distillation}.
% At each distillation step, a state $\state$ is recovered from $\stationary{\policy}$.
% Given $\state$, the execution of $\policy$ in $\latentmdp$ is made possible via the action encoder $\actionencoder$:
%For each state $\state$ produced from $\stationary{\policy}$, the execution of $\policy$ in $\latentmdp$ is made possible via the action encoder $\actionencoder$:
%we write $\latentaction \sim \policy\fun{\sampledot \mid \state}$ for drawing $\action \sim \policy\fun{\sampledot \mid \state}$, followed by $\latentaction \sim \actionencoder\fun{\sampledot \mid \embed_{\encoderparameter}\fun{\state}, \action}$.
%Given this, we can rewrite the transition regularizer as $\expectedsymbol{\state, \tuple{\action, \latentaction} \sim \stationary{\policy}} \wassersteindist{\distance_{\latentstates}}{\embed_{\encoderparameter}\probtransitions\fun{\sampledot \mid \state, \action}}{\latentprobtransitions\fun{\sampledot \mid \embed_{\encoderparameter}\fun{\state}, \latentaction}} = \localtransitionloss{\stationary{\policy}}$.
% This means that minimizing the transition regularizer by executing the original policy $\policy$ is equivalent to minimizing $\localtransitionloss{\stationary{\policy}}$, further yielding bisimulation guarantees derived from the execution \emph{of the original policy $\policy$} (replacing $\latentpolicy$ in Eq.~\ref{eq:bidistance-bound}),
minimizing
%the steady-state regularizer
$\steadystateregularizer{\policy}$
allows to make $\stationary{\policy}$ and $\latentstationaryprior$ closer together, further bridging the gap of the discrepancy between $\policy$ and $\latentpolicy_{\decoderparameter}$.
%Note that the guarantees linked to $\localtransitionloss{\stationary{\policy}}$ are impractical alone due to the dependency of the latent execution of $\policy$ on original states $\state \in \states$; minimizing $\steadystateregularizer{\policy}$ to recover $\latentpolicy_{\decoderparameter}$ and the guarantees linked to $\localtransitionloss{\stationary{\latentpolicy_{\decoderparameter}}}$ breaks this dependency.
%At any time, considering the latent policy resulting from this distillation allows recovering the local losses along with the linked the bisimulation bounds in the objective function of the \waemdp: %, as stated in the following Theorem: 
At any time, recovering the local losses along with the linked bisimulation bounds in the objective function of the \waemdp is allowed by considering the latent policy resulting from this distillation:
\begin{restatable}{theorem}{latentexecutionobjective}\label{thm:latent-execution-objective}
Assume that traces are generated by running a latent policy $\latentpolicy \in \latentpolicies$ in the original environment and let $\distance_{\rewards}$ be the usual Euclidean distance, then the \waemdp objective is
\begin{equation*}
%\[
    \min_{\encoderparameter, \decoderparameter} \expected{\state, \state' \sim \stationary{\latentpolicy}}{\distance_{\states}\fun{\state, \generative_{\decoderparameter}\fun{\embed_{\encoderparameter}\fun{\state}}} + \distance_{\states}\fun{\state', \generative_{\decoderparameter}\fun{\embed_{\encoderparameter}\fun{\state'}}}} + \localrewardloss{\stationary{\latentpolicy}} + \beta \cdot ( \steadystateregularizer{\latentpolicy} + \localtransitionloss{\stationary{\latentpolicy}}).
%\]
\end{equation*}
\end{restatable}%