%We consider the agent interacting with an \emph{unknown} environment modeled by a continuous MDP $\mdp = \mdptuple$.
% The environment being unknown precisely means that $\transitions$, $\rewards$ and $\labels$ are unknown, preventing us to use them to compute a policy for the agent.
% However, by interacting with the environment, the latter is able to observe the current state $\state$ of the environment and its label $\labeling = \labels\fun{\state}$, as well as the outcome of executing an action $\action$ in $\state$, i.e., $\state' \sim \probtransitions\fun{\sampledot \mid \state, \action}$, and the reward $\reward = \rewards\fun{\state, \action, \state'}$ of the resulted transition.
% Furthermore, if we fix a policy $\policy \in \policies{\mdp}$ for the agent, it produces execution traces $\defaulttrace \sim \mdp_\policy$ by interacting with $\mdp$ according to $\policy$.
%in $\mdp$ with $\action_{t+1} \sim \policy\fun{\sampledot \mid \indexof{\state}{0:t}, \indexof{\action}{0:t}}$ for $t < T - 1$.
%In order to use model checking as a mean to provide a control policy for the agent, the model to be verified is required to be finite.
%Therefore, we seek for a discrete abstraction $\latentmdp$ with finite state space $\latentstates$ and action space $\latentactions$ where the agent would be able to produce and recover the set of outputs (i.e., execution traces) of $\mdp$ with same probability when a policy is fixed in $\latentmdp$.
% In the following, we describe how we tackle this problem by learning the abstraction via parameterized function approximators.
%
%
%
We now provide a framework based on \emph{variational autoencoders} \citep{DBLP:journals/corr/KingmaW13} that allows us learning a (discrete) latent space model of $\mdp$ through the interaction of the agent executing a pre-learned RL policy $\policy \in \mpolicies{\mdp}$ in the environment.
Concretely, we seek a latent space model $\tuple{\latentmdp_{\decoderparameter}, \embed_{\encoderparameter}, \embeda_{\encoderparameter}}$ such that $ \latentmdp_{\decoderparameter} = \tuple{\latentstates, \latentactions, \latentprobtransitions_{\decoderparameter}, \latentrewards_{\decoderparameter}, \latentlabels_{\decoderparameter}, \atomicprops, \zinit}$.
We propose to learn the parameters $\tuple{\encoderparameter, \decoderparameter}$ of a \emph{behavioral model} $\decoder$ from which we can retrieve $\latentprobtransitions_{\decoderparameter}$, $\latentrewards_{\decoderparameter}$, and $\latentlabels_{\decoderparameter}$ through the following optimization:
\begin{align}\label{eq:mdp-discrepancy}
    \min_{\decoderparameter} \divergence{\mdp_\policy}{\decoder},
\end{align}
where $\divergencesymbol$ denotes a discrepancy measure. Intuitively, the end goal is to allow $\decoder$ to learn to mimic the behavior of the original MDP by making use of an \emph{encoder} $\encoder$ (including the state embedding function), thus yielding a latent MDP bisimilar to $\mdp$.
In the following, we present two approaches that are distinguished by the discrepancy $\divergencesymbol$ chosen.

\subsection{The variational approach}
\subsubsection{State space abstraction}
We first focus on learning an abstraction of $\states$ through a latent space model $\tuple{\latentmdp_{\decoderparameter}, \embed_{\encoderparameter}, \sampledot}$ that shares its action space with $\mdp$ (i.e., $\latentactions = \actions$).
% equipped with a reconstruction model $\decoder$ with parameters $\decoderparameter$ from which we can retrieve models for $\transitions_\decoderparameter$, $\rewards_\decoderparameter$, and $\labels_\decoderparameter$.
Fix any policy $\policy \in \mpolicies{\mdp}$, we dedicate this Subsection to the optimization of Eq.~\ref{eq:mdp-discrepancy} through KL divergence, i.e., $\min_{\decoderparameter} \dkl{\mdp_{\policy}}{\decoder}$, or equivalently the maximization of the marginal \emph{log-likelihood of traces} of the original MDP, given by
\begin{equation}
    \expected{\trace \sim \mdp_\policy}{\log \decoder\fun{\seq{\state}{T}, \seq{\reward}{T - 1}, \seq{\labeling}{T} \mid \seq{\action}{T-1}}} \label{eq:maximum-likelihood}
\end{equation}
where we make the dependence of $\tuple{\seq{\state}{T}, \seq{\reward}{T - 1}, \seq{\labeling}{T}}$ on $\latentstates$ explicit by the law of total probability:
\begin{align}
    & \decoder\fun{\seq{\state}{T}, \seq{\reward}{T - 1}, \seq{\labeling}{T} \mid \seq{\action}{T - 1}} \notag \\
    =&  \int_{\inftrajectories{\latentmdp_{\decoderparameter}}} \decoder\fun{\seq{\state}{T}, \seq{\reward}{T - 1}, \seq{\labeling}{T} \mid \seq{\latentstate}{T}, \seq{\action}{T - 1}} \cdot \condition{=}\fun{\seq{\action}{T - 1}, \seq{\action'}{T - 1}} \, d\latentprobtransitions_{\decoderparameter}\fun{\seq{\latentstate}{T} \mid \seq{\action'}{T - 1}} \label{eq:intractable-latent}
\end{align}
with $\trace = \defaulttrace$ and $\latentprobtransitions_{\decoderparameter}\fun{\seq{\latentstate}{T} \mid \seq{\action}{T - 1}} = \prod_{t = 0}^{T - 1} \latentprobtransitions_{\decoderparameter}\fun{\latentstate_{t + 1} \mid \latentstate_t, \action_t}$. %, and $\tuple{\seq{\latentstate}{T}, \seq{\action}{T - 1}} \in \trajectories{{\latentmdp, \decoder}}$.
Notice that we consider the likelihood conditioned by actions chosen by $\policy$ since we do not desire to learn an MC, but rather an MDP.
Optimizing Eq.~\ref{eq:maximum-likelihood} through Eq.~\ref{eq:intractable-latent} is typically intractable.
Intuitively, given an execution trace $\trace$, one may want to make use of \emph{Monte-Carlo sampling methods} to approximate the likelihood of $\trace$ through Eq.~\ref{eq:intractable-latent}
by generating a finite number of latent sequences $\set{\seq{\latentstate^{\,i}}{T}}_{i=1}^{N}$ from $ \latentprobtransitions_{\decoderparameter}\fun{\sampledot \mid \seq{\action}{T - 1}}$, and then averaging $\frac{1}{N}\sum_{i = 1}^{N}\decoder\fun{\seq{\state}{T}, \seq{\reward}{T - 1}, \seq{\labeling}{T} \mid \seq{\latentstate^{\,i}}{T}, \seq{\action}{T - 1}}$ to approximate $\decoder\fun{\seq{\state}{T}, \seq{\reward}{T - 1}, \seq{\labeling}{T} \mid  \seq{\action}{T - 1}}$.
Using such methods amounts to trying to approximate an integration over elements from an infinitely uncountable space with a sum over a finite set of elements from this space, meaning that for most $\seq{\latentstate^{\, i}}{T}$, $\decoder\fun{\seq{\state}{T}, \seq{\reward}{T - 1}, \seq{\labeling}{T} \mid \seq{\latentstate^{\, i}}{T}, \seq{\action}{T - 1}}$ will be close to zero thus contributing almost nothing to our estimates.
To overcome this, we introduce a new model $\encoder\fun{\sampledot \mid \seq{\state}{T}, \seq{\action}{T-1}, \seq{\reward}{T-1}, \seq{\labeling}{T}}$ that we consider here as a {probabilistic encoder} giving a distribution over sequences $\seq{\latentstate}{T}$ that are likely to produce $\tuple{\seq{\state}{T}, \seq{\reward}{T-1}, \seq{\labeling}{T}}$ according to any policy sequentially producing $\seq{\action}{T-1}$.
We learn such an encoder through the following optimization:
\begin{equation}
    %\min_{\encoderparameter, \decoderparameter} \expected{\trace \sim \mdp_\policy}{\divergence{\encoder\fun{\sampledot \mid \seq{\state}{T}, \seq{\action}{T-1}, \seq{\reward}{T-1}, \seq{\labeling}{T}}}{\decoder\fun{\sampledot \mid \seq{\state}{T}, \seq{\action}{T-1}, \seq{\reward}{T-1}, \seq{\labeling}{T}}}}
    \min_{\encoderparameter, \decoderparameter} \; \expectedsymbol{\trace \sim \mdp_\policy}{\,\divergence{\encoder\fun{\sampledot \mid \trace}}{{\decoder\fun{\sampledot \mid \trace}}}\,}
\end{equation}
where $\divergencesymbol$ denotes a discrepancy measure.
If we let once again $\divergencesymbol$ be the KL divergence, one can set a \emph{lower bound} on the log-likelihood of produced traces,
%on the $\log$-values of Eq.~\ref{eq:maximum-likelihood}
often referred to as \emph{evidence lower bound} (ELBO, \citealp{DBLP:journals/jmlr/HoffmanBWP13}) as follows:
\begin{align*}
    &&&\dkl{\encoder\fun{\sampledot \mid \seq{\state}{T}, \seq{\action}{T-1}, \seq{\reward}{T-1}, \seq{\labeling}{T}}}{\decoder\fun{\sampledot \mid \seq{\state}{T}, \seq{\action}{T-1}, \seq{\reward}{T-1}, \seq{\labeling}{T}}} \\
    &&=&\expectedsymbol{\seq{\latentstate}{T} \sim \encoder\fun{\sampledot \mid \trace}}\left[\log \encoder\fun{\seq{\latentstate}{T} \mid \seq{\state}{T}, \seq{\action}{T - 1}, \seq{\reward}{T - 1}, \seq{\labeling}{T}} - \log \decoder\fun{\seq{\latentstate}{T} \mid \seq{\state}{T}, \seq{\action}{T-1}, \seq{\reward}{T-1}, \seq{\labeling}{T}}\right] \\
    &&\begin{aligned}
        = \\ \, 
    \end{aligned}&\begin{aligned}
        \; \expectedsymbol{\seq{\latentstate}{T} \sim \encoder\fun{\sampledot \mid \trace}}&[\log \encoder\fun{\seq{\latentstate}{T} \mid \seq{\state}{T}, \seq{\action}{T - 1}, \seq{\reward}{T - 1}, \seq{\labeling}{T}} - \log \decoder\fun{\seq{\state}{T}, \seq{\reward}{T}, \seq{\labeling}{T} \mid \seq{\latentstate}{T}, \seq{\action}{T - 1}} \\
        &\,-\log \latentprobtransitions_{\decoderparameter}\fun{\seq{\latentstate}{T} \mid \seq{\action}{T -1}}] + \log \decoder\fun{\seq{\state}{T}, \seq{\reward}{T - 1}, \seq{\labeling}{T} \mid \seq{\action}{T-1}}
    \end{aligned} \tag{Bayes' rule}\\
    &\equiv&&\log \decoder\fun{\seq{\state}{T}, \seq{\reward}{T - 1}, \seq{\labeling}{T} \mid \seq{\action}{T-1}} - \dkl{\encoder\fun{\sampledot \mid \trace}}{{\decoder\fun{ \sampledot \mid \trace}}} \\
    && =&\expectedsymbol{\seq{\latentstate}{T} \sim \encoder\fun{\sampledot \mid \trace}}[{\log \decoder\fun{\seq{\state}{T}, \seq{\reward}{T - 1}, \seq{\labeling}{T} \mid \seq{\latentstate}{T}, \seq{\action}{T-1}}}] \\
    &&&- \dkl{\encoder\fun{\sampledot \mid \seq{\state}{T}, \seq{\action}{T - 1}, \seq{\reward}{T-1} \seq{\labeling}{T}}}{\latentprobtransitions_{\decoderparameter}\fun{\sampledot \mid \seq{\action}{T -1}}}
    %&& =& \; \elbo\fun{\latentmdp_{\decoderparameter}, \embed_{\encoderparameter}}.
\end{align*}
The purpose of optimizing the ELBO is twofold.
First, it allows us learning $\embed_{\encoderparameter}$ through $\encoder$ with $\encoder\fun{\seq{\latentstate}{T} \mid \seq{\state}{T}, \seq{\action}{T - 1}, \seq{\reward}{T-1} \seq{\labeling}{T}} = \prod_{t = 0}^{T - 1} \embed_{\encoderparameter}\fun{\latentstate_{t + 1} \mid \state_t}$.
Second, this allows us learning transition, reward and label models for $\latentmdp$.
We thus argue that sequences $\seq{\state}{T}$, $\seq{\reward}{T - 1}$, and $\seq{\labeling}{T}$ can be made independent given $\tuple{\seq{\latentstate}{T}, \seq{\action}{T - 1}}$.
Moreover, we assume that a \emph{generative model} $\decoder^{\generative}$ enabling the reconstruction from latent states in $\latentstates$ to grounded states in $\states$ exists, which gives us
% log version
% \begin{align*}
%     & \log \decoder\fun{\seq{\state}{T}, \seq{\reward}{T - 1}, \seq{\labeling}{T} \mid \seq{\latentstate}{T}, \seq{\action}{T - 1}}\\
%     =\, & \log \decoder^{\generative}\fun{\seq{\state}{T} \mid \seq{\latentstate}{T}} + \log \decoder^{\rewards}\fun{\seq{\reward}{T - 1} \mid \seq{\latentstate}{T}, \seq{\action}{T - 1}} + \log \decoder^{\labels}\fun{\seq{\labeling}{T} \mid \seq{\latentstate}{T}}
% \end{align*}
\begin{align}
    \decoder\fun{\seq{\state}{T}, \seq{\reward}{T - 1}, \seq{\labeling}{T} \mid \seq{\latentstate}{T}, \seq{\action}{T - 1}}
    = \decoder^{\generative}\fun{\seq{\state}{T} \mid \seq{\latentstate}{T}} \cdot \decoder^{\rewards}\fun{\seq{\reward}{T - 1} \mid \seq{\latentstate}{T}, \seq{\action}{T - 1}} \cdot \decoder^{\labels}\fun{\seq{\labeling}{T} \mid \seq{\latentstate}{T}} %\label{eq:behavioral-model-chain}
\end{align}
$\textstyle \text{with } \allowbreak \decoder^{\generative}(\seq{\state}{T} \mid \seq{\latentstate}{T}) = \prod_{t = 0}^{T} \decoder^{\generative}(\state_t \mid \latentstate_t), \, \decoder^{\rewards}(\seq{\reward}{T - 1} \mid \seq{\latentstate}{T}, \seq{\action}{T - 1}) = \prod_{t = 0}^{T - 1} \decoder^{\rewards}(\reward_{t} \mid \latentstate_t, \action_t) \text{, and } 
\decoder^{\labels}(\seq{\labeling}{T} \mid \seq{\latentstate}{T}) = \prod_{t = 0}^{T}\decoder^{\labels}(\labeling_t \mid {\latentstate}_{t}).$
We also emphasize that a \emph{perfect reconstruction} of labels is possible, i.e., $\log \decoder^{\labels}\fun{\seq{\labeling}{T} \mid \seq{\latentstate}{T}} = 0$.
Intuitively, this can be done by encoding labels directly into the latent space. Details are given below.
From now on, we omit the label term.

Taking Assumption~\ref{assumption:ergodicity} into account, drawing multiple finite traces $\trace \sim \mdp_{\policy}$ can be seen as a continuous interaction with $\mdp$ along an infinite trace.
This observation allows us to formulate the ELBO in the local setting and connect to local losses introduced in Section~\ref{section:deepmdp}.
%
%
\begin{lemma}\label{lemma:trace-to-stationary}
Let  $\stationary{\policy}$ be the stationary distribution of $\mdp_\policy$ and $f \colon \states \times \actions \times \rewards \times \states \to \R$ be a continuous function.
Assume $\states, \actions$ are compact, then % induced by $\policy$, then 
\begin{align*}
    \lim_{T \to \infty} \frac{1}{T} \expected{\trace \sim \mdp_{\policy}[T]}{\sum_{t = 0}^{T - 1} f\fun{\state_t, \action_t, \reward_t, \state_{t + 1}}} = \expectedsymbol{\state, \action, \reward, \state' \sim \stationary{\policy}} f\fun{\state, \action, \reward, \state'}
\end{align*}
where $\mdp_{\policy}[T]$ denotes the distribution over $\traces{\mdp_\policy}\fun{T}$ and $ \stationary{\policy}(\state, \action, \reward, \state') = \stationary{\policy}(\state, \action, \state') \cdot \condition{=}(\reward,  \rewards\fun{\state, \action, \state'})$.
\end{lemma}
\begin{proof}
Let $\mu_T: \distributions{\states}, \state \mapsto \Prob_\policy\fun{\set{\tuple{\seq{\state}{\infty}, \seq{\action}{\infty}} \in \inftrajectories{\mdp_{\policy}} \mid \state_T = \state}}$ be the distribution giving the probability for the agent of being in each particular state of $\mdp_{\policy}$ after exactly $T$ steps. %, and $f \colon \states \times \actions \times \rewards \times \states \to \R$ be a continuous function.
First, observe that
\begin{align*}
    & \expected{\trace \sim \mdp_{\policy}[T]}{\sum_{t = 0}^{T - 1} f\fun{\state_t, \action_t, \reward_t, \state_{t + 1}}} \\
    =&  \expectedsymbol{\action_0 \sim \policy\fun{\sampledot \mid \sinit}}\expectedsymbol{\state_1 \sim \probtransitions\fun{\sampledot \mid \sinit, \action_0}} \expectedsymbol{\action_1 \sim \policy\fun{\sampledot \mid \state_1}} \cdots \expectedsymbol{\state_T \sim \probtransitions\fun{\sampledot \mid \state_{T - 1}, \action_{T - 1}}} \left[ \sum_{t = 0}^{T - 1} f\fun{\state_t, \action_t, \rewards\fun{\state_t, \action_t, \state_{t + 1}}, \state_{t + 1}}\right]\\
    =& \sum_{t = 0}^{T - 1} \expectedsymbol{\action_0 \sim \policy\fun{\sampledot \mid \sinit}}\expectedsymbol{\state_1 \sim \probtransitions\fun{\sampledot \mid \sinit, \action_0}} \expectedsymbol{\action_1 \sim \policy\fun{\sampledot \mid \state_1}} \cdots \expectedsymbol{\state_T \sim \probtransitions\fun{\sampledot \mid \state_{T - 1}, \action_{T - 1}}} f\fun{\state_t, \action_t, \rewards\fun{\state_t, \action_t, \state_{t + 1}}, \state_{t + 1}}\\
    =& \sum_{t = 0}^{T - 1} \expectedsymbol{\trace \sim \mdp_{\policy}[t + 1]} f\fun{\state_t, \action_t, \reward_t, \state_{t + 1}}.
\end{align*}
Therefore, we have
\begin{align*}
    & \lim_{T \to \infty} \frac{1}{T} \expected{\trace \sim \mdp_{\policy}[T]}{\sum_{t = 0}^{T - 1} f\fun{\state_t, \action_t, \reward_t, \state_{t + 1}}} \\
    =& \lim_{T \to \infty} \frac{1}{T} \sum_{t = 0}^{T - 1} \expectedsymbol{\trace \sim \mdp_{\policy}[t + 1]} f\fun{\state_t, \action_t, \reward_t, \state_{t + 1}} \\
    =& \lim_{T \to \infty} \frac{1}{T} \sum_{t = 0}^{T - 1} \int_{\traces{\mdp_{\policy}}\fun{t + 1}} f\fun{\state_t, \action_t, \rewards\fun{\state_t, \action_t, \state_{t + 1}}, \state_{t + 1}} \, d\Prob_\policy\fun{Cyl\fun{\seq{\state}{t + 1}, \seq{\action}{t}}} \tag{where $Cyl\fun{\seq{\state}{t + 1}, \seq{\action}{t}} = \set{\tuple{\seq{\state^{\star}}{\infty}, \seq{\action^{\star}}{\infty}} \in \inftrajectories{\mdp_{\policy}} \mid \state^{\star}_i = \state_i \quad \forall i \leq t + 1}$} \\
    =& \lim_{T \to \infty} \frac{1}{T} \sum_{t = 0}^{T - 1} \int_{\states} \int_{\actions} \int_{\states} f\fun{\state, \action, \rewards\fun{\state, \action, \state'}, \state'} \, d\mu_t\fun{\state} \, d\policy\fun{\action \mid \state} \, d\probtransitions\fun{\state' \mid \state, \action} \\
    =& \lim_{T \to \infty} \int_{\states}  \frac{1}{T} \sum_{t = 0}^{T - 1} \mu_t\fun{d\state}  \int_{\actions} \int_{\states} f\fun{\state, \action, \rewards\fun{\state, \action, \state'}, \state'} \, d\policy\fun{\action \mid \state} \, d\probtransitions\fun{\state' \mid \state, \action}.
    %=& \lim_{T \to \infty} \frac{1}{T} \sum_{t = 0}^{T - 1} \expectedsymbol{\state \sim \mu_{t}} \expectedsymbol{\action \sim \policy\fun{\sampledot \mid \state}} \expectedsymbol{\state' \sim \probtransitions\fun{\sampledot \mid \state, \action}} f\fun{\state, \action, \rewards\fun{\state, \action, \state'}, \state'} \, d\state
\end{align*}
By definition of $\stationary{\policy}$, $\stationary{\policy}\fun{A} = \lim_{T \to \infty} \frac{1}{T} \sum_{t = 0}^{T - 1} \mu_{t}\fun{A}$ for all $A \in \borel{\states}$, i.e., $\mu_t$ weakly converges to $\stationary{\policy}$.
By Assumption~\ref{assumption:rewards}, the set of images of $\rewards$ is a compact space.
Therefore, $f$ has compact support and by the Portmanteau's Theorem, 
\begin{align*}
    & \lim_{T \to \infty} \int_{\states}  \frac{1}{T} \sum_{t = 0}^{T - 1} \mu_t\fun{d\state} \int_{\actions} \int_{\states} f\fun{\state, \action, \rewards\fun{\state, \action, \state'}, \state'} \, d\policy\fun{\action \mid \state} \, d\probtransitions\fun{\state' \mid \state, \action} \\
    =& \int_{\states} \stationary{\policy}\fun{d\state} \int_{\actions} \int_{\states} f\fun{\state, \action, \rewards\fun{\state, \action, \state'}, \state'} \, d\policy\fun{\action \mid \state} \, d\probtransitions\fun{\state' \mid \state, \action} \\
    =& \expectedsymbol{\state, \action, \reward, \state' \sim \stationary{\policy}} f\fun{\state, \action, \reward, \state'}.
\end{align*}
\end{proof}
%
\begin{corollary}
Let $ \embed\fun{\latentstate, \latentstate' \mid \state, \state'} = \embed\fun{\latentstate \mid \state} \embed\fun{\latentstate' \mid \state'}$, then
\begin{align*}
    & \lim_{T \to \infty} \frac{1}{T} \expectedsymbol{\trace \sim \mdp_{\policy}[T]}\left[\log \decoder\fun{\seq{\state}{T}, \seq{\reward}{T - 1}, \seq{\labeling}{T} \mid \seq{\action}{T-1}} - \dkl{\encoder\fun{\sampledot \mid \trace}}{{\decoder\fun{ \sampledot \mid \trace}}} \right] \\
    % \begin{aligned}
    % = \\ \, \\ \, \\
    % \end{aligned} & \begin{aligned} \;
    % \lim_{T \to \infty} \frac{1}{T} \expectedsymbol{\trace \sim \mdp_{\policy}[T]} \Bigg[ \sum_{t = 0}^{T - 1} \expectedsymbol{\latentstate_t, \latentstate_{t + 1} \sim \embed_{\encoderparameter}\fun{\sampledot \mid \state_t, \state_{t + 1}}} \big[&{\log \decoder^{\generative}\fun{\state_{t+1} \mid \latentstate_{t + 1}} + \log \decoder^{\rewards}\fun{\reward_{t + 1} \mid \latentstate_t, \action_t}} \\
    % & - \dkl{\embed_{\encoderparameter}\fun{\sampledot \mid \state_{t + 1}}}{\latentprobtransitions_{\decoderparameter}\fun{\sampledot \mid \latentstate_t, \action_t}}\big] \Bigg]
    % \end{aligned} \\
    =& \expectedsymbol{}_{\substack{\state, \action, \reward, \state' \sim \stationary{\policy}\\ \latentstate, \latentstate' \sim \embed_{\encoderparameter}\fun{\sampledot \mid \state, \state'}}}\left[{\log \decoder^{\generative}\fun{\state' \mid \latentstate'} + \log \decoder^{\rewards}\fun{\reward \mid \latentstate, \action}} - \dkl{\embed_{\encoderparameter}\fun{\sampledot \mid \state'}}{\latentprobtransitions_{\decoderparameter}\fun{\sampledot\mid \latentstate, \action}}\right].
\end{align*}
\end{corollary}
\begin{proof}
Taking 
\[f\fun{\state, \action, \reward, \state'} = \expectedsymbol{\latentstate, \latentstate' \sim \embed_{\encoderparameter}\fun{\sampledot \mid \state, \state'}}\big[\log \decoder^{\generative}\fun{\state' \mid \latentstate'} + \log \decoder^{\rewards}\fun{\reward \mid \latentstate, \action} - \dkl{\embed_{\encoderparameter}\fun{\sampledot \mid \state'}}{\latentprobtransitions_{\decoderparameter}\fun{\sampledot\mid \latentstate, \action}}\big]\]
yields the result.
\end{proof}
%
Finally, our objective becomes the maximization of the following ELBO:
\begin{align*}
    \max_{\encoderparameter, \decoderparameter}\; \elbo\fun{\latentmdp_{\decoderparameter}, \embed_{\encoderparameter}} = - \min_{\encoderparameter, \decoderparameter} \left\{\distortion_{\encoderparameter, \decoderparameter} + \rate_{\encoderparameter, \decoderparameter} \right\}
    %= \max_{\encoderparameter, \decoderparameter} & \; \expectedsymbol{}_{\trace \sim \mdp_\policy}[\;\log \decoder\fun{\seq{\state}{T}, \seq{\reward}{T - 1}, \seq{\labeling}{T} \mid \seq{\action}{T-1}} - \dkl{\encoder\fun{\seq{\latentstate}{T} \mid \trace}}{{\decoder\fun{\seq{\latentstate}{T} \mid \trace}}}\;] \notag \\
    %= \max_{\encoderparameter, \decoderparameter} & \; \expectedsymbol{}_{\trace \sim \mdp_\policy} \bigg[\, \sum_{t = 0}^{T - 1} \expected{z_t, z_{t + 1} \sim \encoder^\sim}{\log \decoder^{\generative}\fun{\state_t \mid \latentstate_t} + \log \decoder^{\rewards}\fun{\reward_{t + 1} \mid \latentstate_t, \action_t, \latentstate_{t + 1}} + \log \decoder^{\labels}\fun{\labeling_t \mid {\latentstate}_{t}}} \notag \\
    %& - \dkl{\encoder^\transitions\fun{\latentstate_{t + 1} \mid \state_t, \action_t}}{\latentprobtransitions_{\decoderparameter}\fun{\latentstate_{t + 1} \mid \latentstate_t, \action_t}}  \,\bigg]
    %= \min_{\encoderparameter, \decoderparameter} & \; \distortion_{\encoderparameter, \decoderparameter} + \rate_{\encoderparameter, \decoderparameter}
\end{align*}
where $\distortion$ and $\rate$ denote respectively the \emph{distortion} and \emph{rate} of the variational model  \citep{DBLP:conf/icml/AlemiPFDS018} such that
\begin{align*}
    \distortion_{\encoderparameter, \decoderparameter} &= - \expectedsymbol{\state, \action, \reward, \state' \sim \stationary{\policy}} \, \expected{\latentstate, \latentstate' \sim \embed_{\encoderparameter}\fun{\sampledot \mid \state, \state'}}{\log \decoder^{\generative}\fun{\state' \mid \latentstate'} + \log \decoder^{\rewards}\fun{\reward \mid \latentstate, \action}} \\
    %\rate_{\encoderparameter, \decoderparameter} &= \expected{\trace \sim \mdp_\policy}{\sum_{t=0}^{T - 1}\dkl{\encoder^\transitions\fun{\latentstate_{t + 1} \mid \state_t, \action_t}}{\latentprobtransitions_{\decoderparameter}\fun{\latentstate_{t + 1} \mid \latentstate_t, \action_t}}}
    \rate_{\encoderparameter, \decoderparameter} &=  \expectedsymbol{\state, \action, \state' \sim \stationary{\policy}} \, \expectedsymbol{\latentstate \sim \embed_{\encoderparameter}\fun{\sampledot \mid \state}}\dkl{\embed_{\encoderparameter}\fun{\sampledot \mid \state'}}{\latentprobtransitions_{\decoderparameter}\fun{\sampledot \mid \latentstate, \action}}.
\end{align*}
We omit the subscripts when the context is clear.
One can see the optimization of $\elbo\fun{\latentmdp_{\decoderparameter}, \embed_{\encoderparameter}}$ as an indirect optimization of (i) $\localrewardloss{\stationary{\latentpolicy}}$ via the log-likelihood of rewards produced by taking $\latentrewards_{\decoderparameter}\fun{\latentstate, \action} = \expectedsymbol{\reward \sim \decoder^{\rewards}\fun{\sampledot \mid \latentstate, \action}} \reward$, and (ii) $\localtransitionlossupper{\stationary{\latentpolicy}}$ where we change the Wasserstein distance term to KL divergence.
This change has a cost though, introducing new potential problems such as \emph{posterior collapse} with the loss of theoretical guarantees on the quality of the abstraction (c.f. Lemma~\ref{lemma:bidistance-abstraction-quality}).
%

\smallparagraph{Choosing $\policy$.} Learning a discrete abstraction of the full environment model describing all the possible interactions that the agent can have with it is hardly tractable and scalable.
Consequently, $\policy$ must be wisely chosen: $\latentstates$ is learned based on sequences generated from $\mdp_\policy$, meaning ${\latentmdp_{\decoderparameter}}$ is constrained to reachable parts of the system by executing $\policy$.
The key is to make $\policy$ permissive enough to constrain the abstraction to \emph{relevant} parts of the environment, by executing \emph{promising} actions to achieve the objective.
In practice, we compute a policy $\policy$ via an RL algorithm so that (i) the set of promising actions is determined by information gathered during the learning process, and (ii) the permissiveness of $\policy$ is expressed through its entropy. 
Intuitively, the higher the entropy of $\policy$, the more permissive $\policy$ is.

\smallparagraph{Discrete latent distributions.} We aim at learning a \emph{discrete} latent space $\latentstates$, $\embed_{\encoderparameter}$ and $\latentprobtransitions_{\decoderparameter}$ are thus supposed to be \emph{discrete distributions}.
W.l.o.g., we assume latent states to have a binary representation, we further see each bit as a Bernoulli random variable.
Two main challenges arise by learning the parameters of discrete latent distributions via stochastic computation graphs through optimization techniques such as \emph{gradient descent} and Monte-Carlo sampling methods:
(i) gradient descent is not applicable to learn $\encoderparameter$ and $\decoderparameter$ due to the discontinuity of $\latentstates$, and (ii) sampling $\latentstate$ from $\embed_{\encoderparameter}$ must be a \emph{derivable operation}.
We overcome both problems by making use of \emph{continuous relaxation of Bernoulli distributions} \citep{DBLP:conf/iclr/JangGP17,DBLP:conf/iclr/MaddisonMT17}.

Concretely, let $\logit \in \R$ and $\temperature \in \mathopen]0, 1 \mathclose]$, $\latentstate_\temperature \in \mathopen[0, 1\mathclose]$ has a \emph{relaxed Bernoulli distribution} $\latentstate_\temperature \sim \relaxedbernoulli{\logit}{\temperature}$ with logit $\logit$, temperature parameter $\temperature$ iff
(a) if $l$ is a logistic sample with location parameter $\logit\temperature^{-1}$ and scale parameter $\temperature^{-1}$, i.e., $l \sim \logistic{\logit\temperature^{-1}}{\temperature^{-1}}$, then $\latentstate_\temperature = \sigmoid\fun{l}$, $\sigmoid$ being the \emph{sigmoid} function,
(b) $\lim_{\temperature \to 0} \relaxedbernoulli{\logit}{\temperature} = \bernoulli{\logit}$, meaning $\Prob(\textstyle\lim_{\temperature \to 0} \latentstate_\temperature = 1) = \sigmoid\fun{\logit}$, and
(c) let $p_{\logit, \temperature}$ be the density of $\relaxedbernoulli{\logit}{\temperature}$, then $p_{\logit, \temperature}\fun{\latentstate_\temperature}$ is log-convex in $\latentstate_\temperature$.
Since the logistic distribution belongs to the location-scale family, it is fully reparameterizable, i.e., $l \sim \logistic{\logit\temperature^{-1}}{\temperature^{-1}} \equiv x \sim \logistic{0}{1} \text{ and } l = \frac{\logit + x}{\temperature}$.
In practice, we train $\embed_{\encoderparameter}$ and $\latentprobtransitions_{\decoderparameter}$ to infer and generate $\log_2 |\latentstates|$ logits, and we anneal $\temperature$ from $1$ to $0$ during training while using Bernoulli distributions for evaluation.
As prescribed in \cite{DBLP:conf/iclr/MaddisonMT17}, we use two different temperature parameters, one for $\embed_{\encoderparameter}$ and another for $\latentprobtransitions_{\decoderparameter}$.

In order to enable $\log \decoder^\labels\fun{\seq{\labeling}{T} \mid \seq{\latentstate}{T}} = 0$, we linearly encode $\labeling_t$ into each $\latentstate_t$ via $\embed_{\encoderparameter}$.
Recall that each label is given in binary, we thus allocate $|\atomicprops|$ bits in $\latentstates$ for the labels.
This enforces the following property of the $\embed_{\encoderparameter}$:
\[
\forall \state \in \states, \, \forall \latentstate \in \latentstates,\, \embed_{\encoderparameter}\fun{\latentstate \mid \state} > 0 \implies \labels\fun{\state} = \latentlabels_\decoderparameter\fun{\latentstate},
\]
which allows satisfying Assumption~\ref{assumption:labeling} when considering a deterministic encoder.

\smallparagraph{Posterior collapse.} A common issue encountered while optimizing variational models by maximizing the ELBO is the so called \emph{posterior collapse} problem.
Intuitively, this results in a degenerated local optimum where the model learns to ignore the latent space while the reconstruction model alone is used to optimize the ELBO.
With a discrete encoder, this translates into a deterministic mapping to a single latent state, regardless of the input.

From an information-theoretic point of view, optimizing an ELBO gives way to a trade-off between the minimization of the rate and the distortion, where the feasible region is a convex set \citep{DBLP:conf/icml/AlemiPFDS018}.
Posterior collapse happens when the rate is close to zero (referred to as the \emph{auto-decoding limit} of the variational model). %, which can be achieved by the optimizer by merging all states to the same embedding.
%$\expected{\trace \sim \mdp_\policy}{\sum_{t=0}^{T - 1}\dkl{\encoder^\transitions\fun{\latentstate_{t + 1} \mid \state_t, \action_t}}{\latentprobtransitions_{\decoderparameter}\fun{\latentstate_{t + 1} \mid \latentstate_t, \action_t}}} \approx 0$, meaning
% $\encoder^\transitions\fun{\seq{\latentstate}{T} \mid \seq{\state}{T}, \seq{\action}{T - 1}} \approx \latentprobtransitions_{\decoderparameter}\fun{\seq{\latentstate}{T} \mid \seq{\action}{T - 1}}$.
%$\log \embed_{\encoderparameter}\fun{\latentstate' \mid \state'} \approx \expected{\latentstate \sim \embed_{\encoderparameter}\fun{\sampledot \mid \state}}{\log \latentprobtransitions_{\decoderparameter}\fun{{\latentstate}' \mid \latentstate, {\action}}}$ for any $\latentstate' \in \latentstates$ by definition of the KL divergence.
%This means (i) the embedding distribution is nearly independent of ${\state' \in \states}$, and (ii) the latent  transition distribution tends to ignore $\action \in \actions$.
% The latent representation is thus not encoding information about input transitions, meaning that it fails to create a useful representation.
On the other hand, one can achieve a distortion close to zero (referred to as the \emph{auto-encoding limit}) at the price of a higher rate.

As already mentioned, posterior collapse is a consequence of the KL divergence here replacing the Wasserstein in the rate minimization: when the latter is used, Lemma~\ref{lemma:local-loss-upper-bounds} and Lemma~\ref{lemma:bidistance-abstraction-quality} ensure that a value close to zero implies a bisimilarity between states embedded to the same representation.

Different solutions have been proposed in the literature to prevent posterior collapse.
These include \emph{KL-scaling} and \emph{KL-annealing}, that consists in changing the ELBO to $\beta\text{-}\elbo = -(\distortion + \beta \cdot \rate)$, with $\beta \in \mathopen[0, 1\mathclose]$.
This allows to interpolate between auto-encoding and auto-decoding behavior which is not possible with the standard ELBO objective.

Another method consists in adding a cross-entropy regularizer $\alpha \in \mathopen[1, \infty\mathclose[$ to the ELBO, giving
% \begin{equation*}
%     \alpha\text{-}\elbo_{\latentstates}(\encoderparameter, \decoderparameter) = \expectedsymbol{\trace \sim \mdp_\policy}[\expected{\seq{\latentstate}{T} \sim \encoder}{\log \decoder\fun{\seq{\state}{T}, \seq{\latentstate}{T}, \seq{\reward}{T-1} \mid \seq{\action}{T-1}}} + \alpha \cdot \entropy{\encoder(\sampledot \mid \trace)}]
% \end{equation*}
\begin{align*}
    %& \alpha\text{-}\elbo(\latentmdp_{\decoderparameter}, \embed_{\encoderparameter}) \\
    \expectedsymbol{}_{\substack{\state, \action, \reward, \state' \sim \stationary{\policy} \\ \latentstate, \latentstate' \sim \embed\fun{\sampledot \mid \state, \state'}}}\big[{{\log \decoder^{\generative}\fun{\state' \mid \latentstate'} + \log \decoder^{\rewards}\fun{\reward \mid \latentstate, \action}} + \log \latentprobtransitions_{\decoderparameter}\fun{\latentstate' \mid \latentstate, \action}} + \alpha \cdot \entropy{\embed_{\encoderparameter}(\sampledot \mid \state')}\big]
\end{align*}
and anneal $\alpha$ to 1 during training \citep{DBLP:conf/icml/HaarnojaZAL18,DBLP:conf/corl/BurkeHR19}, where 
%$\decoder\fun{\seq{\state}{T}, \seq{\latentstate}{T}, \seq{\reward}{T-1} \mid \seq{\action}{T-1}} = \prod_{t = 0}^{T - 1} \decoder^{\generative}(\state_{t+1} \mid \latentstate_{t + 1}) \cdot \decoder^{\rewards}(\reward_{t + 1} \mid \latentstate_t, \action_t, \latentstate_{t + 1}) \cdot \latentprobtransitions_{\decoderparameter}(\latentstate_{t + 1} \mid \latentstate_t, \action_t)$, and
$\entropy{P}$ denotes the entropy of $P \in \distributions{\measurableset}$ for measurable space $\measurableset$.
Intuitively, this allows encouraging the encoder to learn to make plenty use of the latent space to represent the input data.

A drawback of these methods is that we are no longer optimizing a lower bound on the log-likelihood of the input sequences while optimizing
\begin{equation*}
\tuple{\alpha, \beta}\text{-}\elbo(\latentmdp_{\decoderparameter}, \embed_{\encoderparameter}) = - (\distortion_{\encoderparameter, \decoderparameter} + \beta \cdot \rate_{\encoderparameter, \decoderparameter}) + (\alpha - 1) \cdot \entropy{\embed_{\encoderparameter}}
\end{equation*}
In practice, setting up annealing schemes for $\alpha$ and $\beta$ allows to eventually recover the ELBO and avoid posterior collapse ($\alpha=1$ and $\beta=1$ matches the ELBO).
\cite{DBLP:conf/iclr/HeSNB19} introduced a method only based on the training process of the variational model, which allows to overcome this problem.
It consists in an aggressive training schedule which iterates between multiple updates of $\encoderparameter$ and one update of $\decoderparameter$, based on the observation that the inference model often lags behind the reconstruction model during training when posterior collapse occurs.

In our experiments, we also noticed that posterior collapse is even more difficult to handle when the entropy of the produced traces is low, in particular when rewards and states produced by executing $\policy$ have low variance.
This can be explained by an auto-encoding behavior: \cite{DBLP:conf/icml/AlemiPFDS018} showed that $\dataentropy - \rate$ is an upper bound on the distortion $\distortion$ that can be achieved, with $\dataentropy$ being the entropy of the input distribution (in our case $\dataentropy = \entropy{\stationary{\policy}}$).
When $\entropy{\stationary{\policy}}$ is low, the optimizer may tend to minimize the distortion by pushing the rate close to zero and achieve this way the low entropy of the input distribution.

\subsubsection{Action space abstraction}
Assuming the action space $\actions$ is large or continuous, we additionally now consider learning a discrete space $\latentactions$ through the latent space model $\tuple{\latentmdp_{\decoderparameter}, \embed_{\encoderparameter}, \embeda_{\encoderparameter, \decoderparameter}}$.
Intuitively, the trick here is to let the dynamics of $\policy$ be part of the abstraction.
Then, the log-likelihood of execution traces produced when executing $\policy$ in $\mdp$ %by making use of latent spaces $\latentstates$ and $\latentactions$
is given by
\begin{equation*}
    \expected{\trace \sim \mdp_\policy}{\log \decoder\fun{\trace}} \label{eq:maximum-likelihood-with-actions}
    \text{ such that }
    \decoder\fun{\trace} = \int_{\inftrajectories{\latentmdp_{\decoderparameter}}} \decoder\fun{\trace \mid \seq{\latentstate}{T}, \seq{\latentaction}{T - 1}} \, d\latentprobtransitions_{\latentpolicy, \decoderparameter}\fun{\seq{\latentstate}{T}, \seq{\latentaction}{T - 1}} 
\end{equation*}
with $\latentprobtransitions_{\latentpolicy, \decoderparameter}\fun{\seq{\latentstate}{T}, \seq{\latentaction}{T-1}} = \prod_{t = 0}^{T - 1} {\latentpolicy_{\decoderparameter}}\fun{\latentaction_t \mid \latentstate_t} \cdot \latentprobtransitions_{\decoderparameter}\fun{\latentstate_{t + 1} \mid \latentstate_t, \latentaction_t}$.
Note that $\latentpolicy_{\decoderparameter}$ can be seen as a \emph{simplified policy} embedding dynamics of $\policy$ defined over the latent spaces.
As for the state space abstraction case, we set up a lower bound on the log-likelihood of produced traces as follows:
\begin{align*}
    &&& {\dkl{\encoder\fun{\sampledot \mid \trace}}{{\decoder\fun{\sampledot \mid \trace}}}} \notag \\
    &&= \,& {\expected{\seq{\latentstate}{T}, \seq{\latentaction}{T - 1} \sim \encoder\fun{\sampledot \mid \trace}}{\log \encoder\fun{\seq{\latentstate}{T}, \seq{\latentaction}{T-1} \mid \trace} - {\decoder\fun{\seq{\latentstate}{T}, \seq{\latentaction}{T - 1} \mid \trace}}}} \notag \\
    && \begin{aligned}
       = \\ \, \\
    \end{aligned} \;& \begin{aligned}
        \expectedsymbol{\seq{\latentstate}{T}, \seq{\latentaction}{T - 1} \sim \encoder\fun{\sampledot \mid \trace}}[&\log \encoder\fun{\seq{\latentstate}{T}, \seq{\latentaction}{T-1} \mid \trace} - \log \decoder\fun{\seq{\state}{T}, \seq{\action}{T - 1}, \seq{\reward}{T - 1}\mid \seq{\latentstate}{T}, \seq{\latentaction}{T - 1}}\\
        & - \log \latentprobtransitions_{\latentpolicy, \decoderparameter}\fun{\seq{\latentstate}{T}, \seq{\latentaction}{T - 1}}] + \log \decoder\fun{\trace}
    \end{aligned} \tag{Bayes' rule} \\
    &\equiv&& {\log \decoder\fun{\trace} - \dkl{\encoder\fun{\sampledot \mid \trace}}{{\decoder\fun{\sampledot \mid \trace}}}} \notag \\
    %&&= & \expectedsymbol{\trace \sim \mdp_\policy}\bigg[\expectedsymbol{\seq{\latentstate}{T}, \seq{\latentaction}{T - 1} \sim \encoder}[\log \decoder\fun{\seq{\state}{T}, \seq{\action}{T - 1}, \seq{\reward}{T - 1}\mid \seq{\latentstate}{T}, \seq{\latentaction}{T - 1}}]  \notag \\
    %&&& \quad\quad\quad\quad - \dkl{\encoder\fun{\seq{\latentstate}{T}, \seq{\latentaction}{T-1} \mid \trace}}{\latentprobtransitions_{\decoderparameter}\fun{\seq{\latentstate}{T}, \seq{\latentaction}{T - 1}}}\bigg] \label{eq:ground-action-elbo}
    && \begin{aligned}
        = \\ \, \\
    \end{aligned} \:& \begin{aligned}
        \expectedsymbol{\seq{\latentstate}{T}, \seq{\latentaction}{T - 1} \sim \encoder\fun{\sampledot \mid \trace}}[&\log \decoder\fun{\seq{\state}{T}, \seq{\action}{T - 1}, \seq{\reward}{T - 1}\mid \seq{\latentstate}{T}, \seq{\latentaction}{T - 1}}  + \log \latentprobtransitions_{\latentpolicy, \decoderparameter}\fun{\seq{\latentstate}{T}, \seq{\latentaction}{T - 1}} \\
        & - \log \encoder\fun{\seq{\latentstate}{T}, \seq{\latentaction}{T-1} \mid \trace}]
    \end{aligned} \label{eq:ground-action-elbo}
\end{align*}
We assume the existence of a decoder enabling the reconstruction of actions in $\actions$, based on the current state of $\latentmdp_{\decoderparameter}$ and the discrete action chosen in that state.
Then, we have
\begin{align*}
    & \log \decoder\fun{\seq{\state}{T}, \seq{\action}{T - 1}, \seq{\reward}{T - 1}\mid \seq{\latentstate}{T}, \seq{\latentaction}{T - 1}} \\
    =\, & \log \decoder^{\generative}\fun{\seq{\state}{T} \mid \seq{\latentstate}{T}} + \log \decoder^{\generative}\fun{\seq{\action}{T - 1} \mid \seq{\latentstate}{T - 1}, \seq{\latentaction}{T - 1}} + \log \decoder^\rewards\fun{\seq{\reward}{T - 1} \mid \seq{\latentstate}{T}, \seq{\latentaction}{T - 1}}
\end{align*}
with $\decoder^{\generative}(\seq{\action}{T - 1} \mid \seq{\latentstate}{T - 1}, \seq{\latentaction}{T - 1}) = \prod_{t = 0}^{T - 1} \embeda_{\decoderparameter}(\action_t \mid \latentstate_t, \latentaction_t)$, and $\decoder^\rewards(\seq{\reward}{T - 1} \mid \seq{\latentstate}{T}, \seq{\latentaction}{T - 1}) = \prod_{t = 0}^{T - 1} \decoder^\rewards(\reward_t \mid \latentstate_t, \latentaction_t)$.
This allows us learning our action embedding function through $\embeda_{\encoderparameter, \decoderparameter}\fun{\action \mid \state, \latentaction} = \expectedsymbol{\latentstate \sim \embed_{\encoderparameter}\fun{\sampledot \mid \state}}\embeda_{\decoderparameter}\fun{\action \mid \latentstate, \latentaction}$ for all $\state \in \states, \action \in \actions, \latentaction \in \latentactions$.
Regarding the encoder, the chain rule gives us $\encoder\fun{\seq{\latentstate}{T}, \seq{\latentaction}{T-1} \mid \trace} = \encoder\fun{\seq{\latentaction}{T-1} \mid \seq{\latentstate}{T}, \trace} \cdot \encoder\fun{\seq{\latentstate}{T} \mid \trace}$.
We further learn an action encoder $\latentembeda_{\encoderparameter}$ through $\encoder\fun{\seq{\latentaction}{T-1} \mid \seq{\latentstate}{T}, \trace} = \prod_{t = 0}^{T - 1} \latentembeda_{\encoderparameter}\fun{\latentaction_t \mid \latentstate_t, \action_t}$.
Putting all together, our objective involves the following brand new ELBO measured in the local setting using Lemma~\ref{lemma:trace-to-stationary}:
\begin{align*}
    \max_{\encoderparameter, \decoderparameter}\, \elbo\fun{\latentmdp_{\decoderparameter}, \embed_{\encoderparameter}, \embeda_{\encoderparameter, \decoderparameter}} = - \min_{\encoderparameter, \decoderparameter} \left\{ \distortion_{\encoderparameter, \decoderparameter} + \rate_{\encoderparameter, \decoderparameter} \right\},
\end{align*}
such that
\begin{align*}
    \distortion_{\encoderparameter, \decoderparameter} =& - \expectedsymbol{\state, \action, \reward, \state' \sim \stationary{\policy}} \, \expectedsymbol{}_{\substack{\latentstate, \latentstate' \sim \embed_{\encoderparameter}\fun{\sampledot \mid \state, \state'} \\ \latentaction \sim \latentembeda_{\encoderparameter}\fun{\sampledot \mid \latentstate, \action}}}\left[{\log \embeda_{\decoderparameter}\fun{\action \mid \latentstate, \latentaction} + \log \decoder^{\generative}\fun{\state' \mid \latentstate'} + \log \decoder^{\rewards}\fun{\reward \mid \latentstate, \latentaction}}\right] \\
    % \rate_{\encoderparameter, \decoderparameter} =& \expectedsymbol{\trace \sim \mdp_\policy}\Big[\sum_{t=0}^{T - 1}\dkl{\encoder^\transitions\fun{\latentstate_{t + 1} \mid \state_t, \action_t}}{\latentprobtransitions_{\decoderparameter}\fun{\latentstate_{t + 1} \mid \latentstate_t, \latentaction_t}} \notag \\
    % & \quad\quad\quad\quad\quad\;\, + \dkl{\embeda_{\encoderparameter}\fun{\latentaction_t \mid \latentstate_t, \action_t}}{\latentpolicy_{\decoderparameter}\fun{\latentaction_t \mid \latentstate_t}}\Big]
    % \begin{aligned}
    %     \rate_{\encoderparameter, \decoderparameter} = \\ \phantom{\rate_{\encoderparameter, \decoderparameter} = }
    % \end{aligned}& \;
    % \begin{aligned}
    %     \expectedsymbol{\trace \sim \mdp_\policy}\Big[\sum_{t=0}^{T - 1} \expectedsymbol{\latentstate_t \sim \encoder^\transitions} \big[&\dkl{\encoder^\transitions\fun{\latentstate_{t + 1} \mid \state_t, \action_t}}{\latentprobtransitions_{\decoderparameter}\fun{\latentstate_{t + 1} \mid \latentstate_t, \latentaction_t}} \\
    %     + & \dkl{\embeda_{\encoderparameter}\fun{\latentaction_t \mid \latentstate_t, \action_t}}{\latentpolicy_{\decoderparameter}\fun{\latentaction_t \mid \latentstate_t}}\big]\Big]
    % \end{aligned}
    \rate_{\encoderparameter, \decoderparameter} =& \expectedsymbol{\state, \action, \state' \sim \stationary{\policy}} \, \expectedsymbol{}_{\substack{\latentstate \sim \embed_{\encoderparameter}\fun{\sampledot \mid \state} \\ \latentaction \sim \latentembeda_{\encoderparameter}\fun{\sampledot \mid \latentstate, \action}}}\left[ \dkl{\embed_{\encoderparameter}\fun{\sampledot \mid \state'}}{\latentprobtransitions_{\decoderparameter}\fun{\sampledot \mid \latentstate, \latentaction}} + \dkl{\latentembeda_{\encoderparameter}\fun{\sampledot\mid\latentstate, \action}}{\latentpolicy_{\decoderparameter}\fun{\sampledot \mid \latentstate}} \right].
\end{align*}
 %where we consider the local setting under $\stationary{\policy}$ using Lemma~\ref{lemma:trace-to-stationary}.
%and executing $\latentaction$ in state $\latentstate$ can be seen as executing $\action \sim \decoder^{\generative}\fun{\sampledot \mid \latentstate, \latentaction}$ in the real environment.
%

\smallparagraph{Two-phase learning.} Optimizing $\elbo\fun{\latentmdp_{\decoderparameter}, \embed_{\encoderparameter}, \embeda_{\encoderparameter}}$ and learning both latent spaces at once might be a difficult task.
One can split this ELBO in two to better control the optimization and prevent this way (i) high-variance loss, (ii) the optimizer to fall in bad local optima, and (iii) posterior collapse.
Therefore, we re-write the ELBO as follows.
\begin{proposition}
Let $\tuple{\encoderparameter_1, \encoderparameter_2}$, $\tuple{\decoderparameter_1, \decoderparameter_2}$ be respectively encoder and behavior model parameters such that $\encoderparameter = \encoderparameter_1 \cup \encoderparameter_2$, $\decoderparameter = \decoderparameter_1 \cup \decoderparameter_2$, and $\tuple{\latentmdp_{\decoderparameter_1}, \embed_{\encoderparameter_1}}, \tuple{\latentmdp_{\decoderparameter_2}, \embed_{\encoderparameter_2}, \embeda_{\encoderparameter_{1}, \decoderparameter_2}}$ be latent space models of $\mdp$ such that $\latentmdp_{\decoderparameter_1}$ shares its action space with $\mdp$, then
\begin{align*}
    \elbo\fun{\latentmdp_{\decoderparameter}, \embed_{\encoderparameter}, \embeda_{\encoderparameter, \decoderparameter}} =& \,\elbo\fun{\latentmdp_{\decoderparameter_1}, \embed_{\encoderparameter_1}} + \elbo\fun{\latentmdp_{\decoderparameter_2}, \embeda_{\encoderparameter_2, \decoderparameter_2}},
\end{align*}
if and only if
\begin{align*}
    & \elbo\fun{\latentmdp_{\decoderparameter_2}, \embeda_{\encoderparameter_2, \decoderparameter_2}} \\
    =& \lim_{T \to \infty} \frac{1}{T} \expected{\trace \sim \mdp_\policy[T]}{\log \decodersymbol_{\decoderparameter_2}^{\policy}\fun{\seq{\action}{T - 1} \mid \seq{\state}{T - 1}}  - \!\!\!\! \expectedsymbol{\seq{\latentstate}{T} \sim \encodersymbol_{\encoderparameter_{1}}\fun{\sampledot \mid \trace}} \! \dkl{\encodersymbol_{\encoderparameter_2}\fun{\sampledot \mid \seq{\latentstate}{T}, \trace}}{\decoder\fun{\sampledot \mid \seq{\latentstate}{T}, \trace}}} \\
    =& - \fun{\distortion_{\encoderparameter_2, \decoderparameter_2} + \rate_{\encoderparameter_2, \decoderparameter_2} }
\end{align*}
where $\expected{\trace \sim \mdp_\policy[T]}{\log \decodersymbol_{ \decoderparameter_2}^\policy\fun{\seq{\action}{T - 1} \mid \seq{\state}{T - 1}}}$ denotes the log-likelihood of $\policy$, and
\begin{align*}
    \distortion_{\encoderparameter_2, \decoderparameter_2} &= - \expectedsymbol{\state, \action \sim \stationary{\policy}}\expectedsymbol{}_{\substack{\latentstate \sim \embed_{\encoderparameter_1}\fun{\sampledot \mid \state} \\ \latentaction \sim \latentembeda_{\encoderparameter_2}\fun{\sampledot \mid \latentstate, \action}}}\left[\log \embeda_{\decoderparameter_2}\fun{\action \mid \latentstate, \latentaction}\right] - \Delta \latentprobtransitions_{\decoderparameter} - \Delta\decodersymbol_{\decoderparameter}^\rewards, \\
    \Delta\latentprobtransitions_{\decoderparameter} &= \expectedsymbol{\state, \action, \state' \sim \stationary{\policy}}\, \expectedsymbol{}_{\substack{\latentstate, \latentstate' \sim \embed_{\encoderparameter_1}\fun{\sampledot \mid \state, \state'} \\ \latentaction \sim \latentembeda_{\encoderparameter_2}\fun{\sampledot \mid \latentstate, \action}}}\left[\log \latentprobtransitions_{\decoderparameter_2}\fun{\latentstate' \mid \latentstate, \latentaction} - \log \latentprobtransitions_{\decoderparameter_1}\fun{\latentstate' \mid \latentstate, \action}\right],\\
    \Delta\decoder^\rewards &= \expectedsymbol{\state, \action, \reward, \sampledot \sim \stationary{\policy}}\expectedsymbol{}_{\substack{\latentstate \sim \embed_{\encoderparameter_1}\fun{\sampledot \mid \state} \\ \latentaction \sim \latentembeda_{\encoderparameter_2}\fun{\sampledot \mid \latentstate, \action}}} \left[ \log \decodersymbol_{\decoderparameter_2}^\rewards\fun{\reward \mid \latentstate, \latentaction} - \log \decodersymbol_{\decoderparameter_1}^\rewards\fun{\reward \mid \latentstate, \action} \right], \\
    \rate_{\encoderparameter_2, \decoderparameter_2} &= \expectedsymbol{\state, \action \sim \stationary{\policy}} \, \expectedsymbol{\latentstate \sim \embed_{\encoderparameter_1}\fun{\sampledot \mid \state}} \dkl{\latentembeda_{\encoderparameter_2}\fun{\sampledot \mid \latentstate, \action}}{\latentpolicy_{\decoderparameter_2}\fun{\sampledot \mid \latentstate}}.
\end{align*}
\end{proposition}
\begin{proof} First, observe that we can re-write the log-likelihood of traces as
\begin{align*}
    \log \decoder\fun{\seq{\state}{T}, \seq{\action}{T-1}, \seq{\reward}{T-1}, \seq{\labeling}{T}}
    &= \log \fun{\decodersymbol_{\decoderparameter_1}\fun{\seq{\state}{T}, \seq{\reward}{T-1}, \seq{\labeling}{T} \mid \seq{\action}{T-1}} \prod_{t = 0}^{T - 1} \decodersymbol^\policy_{\decoderparameter_2}\fun{\action_t \mid \state_t}}\\
    &= \log \decodersymbol_{\decoderparameter_1}\fun{\seq{\state}{T}, \seq{\reward}{T-1}, \seq{\labeling}{T} \mid \seq{\action}{T-1}} + \log \decodersymbol_{\decoderparameter_2}^{\policy}\fun{\seq{\action}{T - 1} \mid \seq{\state}{T - 1}}
    \end{align*}
     and the KL divergence between encoder and behavioral distributions over latent sequences as
    \begin{align*}
    &\dkl{\encoder\fun{\sampledot \mid \trace}}{\decoder\fun{\sampledot \mid \trace}} \\
    \begin{aligned}
    = \\ \, \\ \,
    \end{aligned}& \; \begin{aligned}
        \expectedsymbol{\seq{\latentstate}{T}, \seq{\latentaction}{T - 1} \sim \encoder\fun{\sampledot \mid \trace}}[&\log \encodersymbol_{\encoderparameter_1}\fun{\seq{\latentstate}{T} \mid \trace} + \log \encodersymbol_{\encoderparameter_2}\fun{\seq{\latentaction}{T - 1} \mid \seq{\latentstate}{T}, \trace}\\
        &- \log \decodersymbol_{\decoderparameter_1}\fun{\seq{\latentstate}{T} \mid \trace} - \log \decodersymbol_{\decoderparameter}\fun{\seq{\latentaction}{T - 1} \mid \seq{\latentstate}{T}, \trace}]
    \end{aligned} \tag{by definition of KL divergence and chain rule} \\
    \begin{aligned}
    = \\ \, \\ \,
    \end{aligned}& \; \begin{aligned}
        \expectedsymbol{\seq{\latentstate}{T} \sim \encodersymbol_{\encoderparameter_1}\fun{\sampledot \mid \trace}}\Big[& \log \encodersymbol_{\encoderparameter_1}\fun{\seq{\latentstate}{T} \mid \trace} - \log \decodersymbol_{\decoderparameter_1}\fun{\seq{\latentstate}{T} \mid \trace} \\
        &+ \expected{\seq{\latentaction}{T - 1} \sim \encodersymbol_{\encoderparameter_2}\fun{\sampledot \mid \seq{\latentstate}{T}, \trace}}{ \log \encodersymbol_{\encoderparameter_2}\fun{\seq{\latentaction}{T - 1} \mid \seq{\latentstate}{T}, \trace} - \decodersymbol_{\decoderparameter}\fun{\seq{\latentaction}{T - 1} \mid \seq{\latentstate}{T}, \trace}}\Big]
    \end{aligned}\\
    =& \dkl{\encodersymbol_{\encoderparameter_1}\fun{\sampledot \mid \trace}}{\decodersymbol_{\decoderparameter_1}\fun{\sampledot \mid \trace}} + \expectedsymbol{\seq{\latentstate}{T} \sim \encodersymbol_{\encoderparameter_1}} \dkl{\encodersymbol_{\encoderparameter_2}\fun{\sampledot \mid \seq{\latentstate}{T}, \trace}}{\decoder\fun{\sampledot \mid \seq{\latentstate}{T}, \trace}}.
\end{align*}
This gives us
\begin{align*}
     &\log \decoder\fun{\seq{\state}{T}, \seq{\action}{T-1}, \seq{\reward}{T-1}, \seq{\labeling}{T}} - \dkl{\encoder\fun{\sampledot \mid \trace}}{\decoder\fun{\sampledot \mid \trace}} \\
     =& \log \decodersymbol_{\decoderparameter_1}\fun{\seq{\state}{T}, \seq{\reward}{T-1}, \seq{\labeling}{T} \mid \seq{\action}{T-1}} + \log \decodersymbol_{\decoderparameter_2}^{\policy}\fun{\seq{\action}{T - 1} \mid \seq{\state}{T - 1}} \\
     &- \dkl{\encodersymbol_{\encoderparameter_1}\fun{\sampledot \mid \trace}}{\decodersymbol_{\decoderparameter_1}\fun{\sampledot \mid \trace}} - \expectedsymbol{\seq{\latentstate}{T} \sim \encodersymbol_{\encoderparameter_1}\fun{\sampledot \mid \trace}} \dkl{\encodersymbol_{\encoderparameter_2}\fun{\sampledot \mid \seq{\latentstate}{T}, \trace}}{\decoder\fun{\sampledot \mid \seq{\latentstate}{T}, \trace}}
\end{align*}
which yields the first equality.
Then, by repeatedly applying Bayes' and chain rules, we have
\begin{align*}
    & \log \decoder\fun{\seq{\latentaction}{T - 1} \mid \seq{\latentstate}{T}, \trace} \\
    = & \log \decodersymbol_{\decoderparameter_2}\fun{\seq{\latentstate}{T}, \trace \mid \seq{\latentaction}{T - 1}} + \log \decodersymbol_{\decoderparameter_2}\fun{\seq{\latentaction}{T - 1}} - \log \decodersymbol_{\decoderparameter}\fun{\seq{\latentstate}{T}, \trace}\\
    = & \log \decodersymbol_{\decoderparameter_2}\fun{\seq{\state}{T}, \seq{\action}{T - 1}, \seq{\reward}{T - 1} \mid \seq{\latentstate}{T}, \seq{\latentaction}{T - 1}} + \log \latentprobtransitions_{\decoderparameter_2}\fun{\seq{\latentstate}{T} \mid \seq{\latentaction}{T - 1}} + \log \decodersymbol_{\decoderparameter_2}\fun{\seq{\latentaction}{T - 1}} \\
    & - \log \decodersymbol_{\decoderparameter}\fun{\seq{\state}{T}, \seq{\reward}{T - 1}, \seq{\latentstate}{T}, \seq{\action}{T - 1}} \\
    = & \log \decodersymbol_{\decoderparameter_2}\fun{\seq{\state}{T}, \seq{\action}{T - 1}, \seq{\reward}{T - 1} \mid \seq{\latentstate}{T}, \seq{\latentaction}{T - 1}} + \log \latentprobtransitions_{\latentpolicy, \decoderparameter_2}\fun{\seq{\latentstate}{T}, \seq{\latentaction}{T - 1}}\\
    & - \log \big(\prod_{t = 0}^{T - 1} \decodersymbol^\policy_{\decoderparameter_2}\fun{\action_t \mid \state_t} \cdot \latentprobtransitions_{\decoderparameter_1}\fun{\latentstate_{t + 1} \mid \latentstate_{t}, \action_t} \cdot \decodersymbol_{\decoderparameter_1}^{\rewards}\fun{\reward_t \mid \latentstate_t, \action_t, \latentstate_{t + 1}} \cdot \decodersymbol_{\decoderparameter_1}^{\generative}\fun{\state_{t + 1} \mid \latentstate_{t+1}}\big)\\
    = & \log \decodersymbol_{\decoderparameter_1}^{\generative}\fun{\seq{\state}{T} \mid \seq{\latentstate}{T}} +  \log \decodersymbol_{\decoderparameter_2}^{\generative}\fun{\seq{\action}{T - 1} \mid \seq{\latentstate}{T - 1}, \seq{\latentaction}{T - 1}} + \log  \decodersymbol_{\decoderparameter_2}^\rewards\fun{\seq{\reward}{T - 1} \mid \seq{\latentstate}{T}, \seq{\latentaction}{T - 1}} \\
    & + \log \latentpolicy_{\decoderparameter_2}\fun{\seq{\latentaction}{T - 1} \mid \seq{\latentstate}{T - 1}} + \log \latentprobtransitions_{\decoderparameter_2}\fun{\seq{\latentstate}{T} \mid \seq{\latentaction}{T - 1}} \\
    & - \log \decodersymbol_{\decoderparameter_2}^{\policy}\fun{\seq{\action}{T - 1} \mid \seq{\state}{T - 1}} - \log \latentprobtransitions_{\decoderparameter_1}\fun{\seq{\latentstate}{T} \mid \seq{\action}{T - 1}} - \log  \decodersymbol_{\decoderparameter_1}^\rewards\fun{\seq{\reward}{T - 1} \mid \seq{\latentstate}{T}, \seq{\action}{T - 1}} \\
    &  - \log \decodersymbol_{\decoderparameter_1}^{\generative}\fun{\seq{\state}{T} \mid \seq{\latentstate}{T}},
\end{align*}
meaning 
\begin{align*}
    & \expected{\trace \sim \mdp_{\policy}}{\log \decodersymbol^\policy_{\decoderparameter_2}\fun{\seq{\action}{T - 1} \mid \seq{\state}{T - 1}} - \expectedsymbol{\seq{\latentstate}{T} \sim \encodersymbol_{\encoderparameter_1}\fun{\sampledot \mid \trace}} \dkl{\encodersymbol_{\encoderparameter_2}\fun{\sampledot \mid \seq{\latentstate}{T}, \trace}}{\decodersymbol_{\decoderparameter_2}\fun{\sampledot \mid \seq{\latentstate}{T}, \trace}}} \\
    =& \expected{\trace \sim \mdp_{\policy}}{\log \decodersymbol^\policy_{\decoderparameter_2}\fun{\seq{\action}{T - 1} \mid \seq{\state}{T - 1}} - \expectedsymbol{}_{\substack{\seq{\latentstate}{T} \sim \encodersymbol_{\encoderparameter_1}\fun{\sampledot \mid \trace} \\ \seq{\latentaction}{T - 1} \sim \encodersymbol_{\encoderparameter_2}\fun{\sampledot \mid \seq{\latentstate}{T}, \trace}}}  \log \fun{\frac{\encodersymbol_{\encoderparameter_2}\fun{\seq{\latentaction}{T - 1} \mid \seq{\latentstate}{T}, \trace}}{\decodersymbol_{\decoderparameter_2}\fun{\seq{\latentaction}{T - 1} \mid \seq{\latentstate}{T}, \trace}}}} \\
    \begin{aligned}
    = \\ \, \\ \, \\ \, \\ \, \\ \,
    \end{aligned}& \; \begin{aligned}
        \expectedsymbol{}_{\substack{\trace \sim \mdp_{\policy} \\ \seq{\latentstate}{T} \sim \encodersymbol_{\encoderparameter_1}\fun{\sampledot \mid \trace} \\ \seq{\latentaction}{T - 1} \sim \encodersymbol_{\encoderparameter_2}\fun{\sampledot \mid \seq{\latentstate}{T}, \seq{\action}{T - 1}}}} \Bigg[&  \log \decodersymbol_{\decoderparameter_2}^{\generative}\fun{\seq{\action}{T - 1} \mid \seq{\latentstate}{T - 1}, \seq{\latentaction}{T - 1}} \\
        &+ \log \latentprobtransitions_{\decoderparameter_2}\fun{\seq{\latentstate}{T} \mid \seq{\latentaction}{T - 1}} - \log \latentprobtransitions_{\decoderparameter_1}\fun{\seq{\latentstate}{T} \mid \seq{\action}{T - 1}} \\
        & + \log  \decodersymbol_{\decoderparameter_2}^\rewards\fun{\seq{\reward}{T - 1} \mid \seq{\latentstate}{T}, \seq{\latentaction}{T - 1}} - \log  \decodersymbol_{\decoderparameter_1}^\rewards\fun{\seq{\reward}{T - 1} \mid \seq{\latentstate}{T}, \seq{\action}{T - 1}} \\
        & - \log \encodersymbol_{\encoderparameter_2}\fun{\seq{\latentaction}{T - 1} \mid \seq{\latentstate}{T}, \seq{\action}{T - 1}} + \log \latentpolicy_{\decoderparameter_2}\fun{\seq{\latentaction}{T - 1} \mid \seq{\latentstate}{T - 1}}\Bigg],
    \end{aligned}
\end{align*}
which yields the distortion and rate values when applying Lemma~\ref{lemma:trace-to-stationary}.
\end{proof}
%

\smallparagraph{Discrete latent distributions.} Similarly to the latent state space abstraction, we learn discrete latent distributions for $\embeda_{\encoderparameter}$ and $\latentpolicy_{\decoderparameter}$ by learning logits of a continuous relaxation of discrete distributions, through the \emph{Gumbel softmax trick} \citep{DBLP:conf/iclr/JangGP17,DBLP:conf/iclr/MaddisonMT17}.
Let $n = |\latentactions|$, and $\latentactions = \set{\latentaction_1, \dots, \latentaction_n}$.
Drawing $\latentaction_i \sim \categorical{\logits}$  from a discrete (categorical) distribution in $\distributions{\latentactions}$ with logits parameter $\logits \in \R^n$ is equivalent to applying an $\argmax$ operator to the sum of \emph{Gumble samples} and these logits, i.e., $i = \argmax_{k} \logits_k - \log\fun{- \log \vect{\epsilon}_k}$, where $\vect{\epsilon} \in \mathopen[0, 1\mathclose]^n$ is a uniform noise.
The sampling operator being fully reparameterizable here, $\argmax$ is however not derivable, preventing the operation to be optimized through gradient descent.
The proposed solution is to replace the $\argmax$ operator by a softmax function with temperature parameter $\temperature \in \mathopen]0, \left(n - 1\right)^{-1}\mathclose]$.

Concretely, $\vect{x} \in \mathopen[0, 1\mathclose]^n$ has a \emph{relaxed discrete distribution} $\vect{x} \sim \relaxedcategorical{\logits}{\temperature}$ with logits $\logits$, temperature parameter $\temperature$ iff
(a) let $\vect{\epsilon} \in [0, 1]^n$ be a uniform noise and $\vect{G}_k = {- \log \fun{- \log \vect{\epsilon}_k}}$, then $\vect{x}_k = {\frac{\exp{\fun{\fun{\logits_k + \vect{G}_k} \temperature^{-1}}}}{\sum_{i = 1}^{n} \exp{\fun{\fun{\logits_i + \vect{G}_i} \lambda^{-1}}}}}$ for each $k \in [n]$,
(b) $\lim_{\temperature \to 0} \relaxedcategorical{\logits}{\temperature} = \categorical{\logits}$, meaning $\Prob(\textstyle\lim_{\temperature \to 0} \vect{x}_k = 1) = \frac{\exp{\logits_k}}{\sum_{i = 1}^{n}\exp{\logits_i}}$,
and (c) let $p_{\logits, \temperature}$ be the density of $\relaxedcategorical{\logits}{\temperature}$, then $p_{\logits, \temperature}\fun{\vx}$ is log-convex in $\vx$.
In practice, we train $\embeda_{\encoderparameter}$ and $\latentpolicy_{\decoderparameter}$ to infer and generate logits parameter $\logits$, and we anneal $\temperature$ from $(n - 1)^{-1}$ to $0$ during training while using discrete distributions for evaluation.
Again, as prescribed in \cite{DBLP:conf/iclr/MaddisonMT17}, we use two different temperature parameters, one for $\embeda_{\encoderparameter}$ and another for $\latentpolicy_{\decoderparameter}$.