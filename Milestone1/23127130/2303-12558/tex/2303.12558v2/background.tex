% We write
% $[T] = \{n \in \N \mid n \leq T\}$,
% $\sigma(x) = \nicefrac{1}{1+e^{-x}}$ for the sigmoid function, and
% $\condition{A} \colon X \to [1]$ for the indicator function: $\condition{A}\fun{a} = 1$ iff $a \in A \subseteq X$.
% Let $\measurableset$ be a complete and separable space and $\borel{\measurableset}$ be the set of all Borel subsets of $\measurableset$.
% We write $\distributions{\measurableset}$ for the set of measures $P$
% defined on $\measurableset$.
%
In the following, we write $\distributions{\measurableset}$ for the set of measures over (complete, separable metric space) $\measurableset$.
\ifarxiv
The index of all the notations introduced along the paper is available at the end of the Appendix.
\fi
% The index of all notations is available in Appendix~\ref{appendix:notations}.

\textbf{Markov decision processes}~%
%A \emph{Markov decision process}
(MDPs) are tuples $\mdp = \mdptuple$ where
$\states$ is a set of \emph{states}; $\actions$, a set of \emph{actions}; $\probtransitions \colon \states \times \actions \to \distributions{\states}$, a \emph{probability transition function} that maps the current state and action to a \emph{distribution} over the next states; $\rewards \colon \states \times \actions \to \R$, a \emph{reward function}; $\labels \colon \states \to 2^\atomicprops$, a \emph{labeling function} over a set of atomic propositions $\atomicprops$; and $\sinit \in \states$, the \emph{initial state}.
% The set of \emph{enabled actions} of $\state \in \states$ is $\act{\state} \subseteq \actions$.
%We assume  $\act{\state} \neq \emptyset$ for all $\state \in \states$. 
If $|\actions| = 1$, $\mdp$ is a fully stochastic process called a \emph{Markov chain} (MC).
We write $\mdp_\state$ for the MDP obtained when replacing the initial state of $\mdp$ by $\state \in \states$.
%\newline\smallparagraph{Trajectories}~
An agent interacting in $\mdp$ produces \emph{trajectories}, i.e., sequences of states and actions $\trajectory = \trajectorytuple{\state}{\action}{T}$
where $\state_0 = \sinit$ and $\state_{t + 1} \sim \probtransitions\fun{\sampledot \mid \state_t, \action_t}$ for $t < T$.
The set of infinite trajectories of $\mdp$ is \inftrajectories{\mdp}.
%An \emph{execution trace} $\trace$ of $\mdp$ is a trajectory that additionally records labels and rewards encountered.
%The set of execution traces of $\mdp$ is $\traces{\mdp}$.
% \emph{Execution traces} are trajectories that additionally record rewards encountered.
%
%Let $\labelset{T} \subseteq \atomicprops$, we write $\getstates{\labelset{T}} = \set{\state \mid \labels\fun{\state}\cap \labelset{T} \neq \emptyset} \subseteq \states$ and $\getstates{\neg \labelset{T}} = \states \setminus \getstates{\labelset{T}}$.
We assume $\atomicprops$ and labels being respectively {one-hot} and binary encoded.
Given $\labelset{T} \subseteq \atomicprops$, we write $\state \models \labelset{T}$ if $\state$ is labeled with $\labelset{T}$, i.e., $\labels\fun{\state}\cap \labelset{T} \neq \emptyset$, and $\state \models \neg \labelset{T}$ for $\state \not\models \labelset{T}$.
%We write $\mdp_\state$ for the MDP obtained when replacing the initial state of $\mdp$ by $\state \in \states$, $\mdp \oplus \rewards'$ for replacing the reward function by $\rewards'$,
%and $\selfloop{\mdp}{B}$ for making states from $B \subseteq \states$ absorbing. %, i.e., by changing $\probtransitions\fun{\sampledot \mid \state, \action}$ to $\probtransitions'\fun{\sampledot \mid \state, \action}$ such that $\probtransitions'\fun{B \mid \state, \action} = 1$ for all $\state \in B$, $\action \in \act{\state}$.
We refer to MDPs with continuous state or action spaces as \emph{continuous MDPs}. 
In that case, we assume $\states$ and $\actions$ are complete separable metric spaces equipped with a Borel $\sigma$-algebra,
and $\labels^{-1}\fun{\labelset{T}}$ is Borel-measurable for any $\labelset{T} \subseteq \atomicprops$.
%The space of all MDPs is $\mdpspace$.

\smallparagraph{Policies and stationary distributions.}~A \emph{(memoryless) policy} $\policy \colon \states \to \distributions{\actions}$ % is a  stochastic mapping from states to actions
prescribes which action to choose at each step of the interaction.
% such that $\support{\policy\fun{\sampledot \mid \state}} \subseteq \act{\state}$.
The set of memoryless policies of $\mdp$ is $\mpolicies{\mdp}$.
The MDP $\mdp$ and $\policy \in \mpolicies{\mdp}$
induce an MC $\mdp_\policy$
% with transition function $\probtransitions_{\policy}\fun{\sampledot \mid \state}$
%$\probtransitions_{\policy}\fun{\sampledot \mid \state} = \expectedsymbol{\action \sim \policy\fun{\sampledot \mid \state}} \probtransitions\fun{\sampledot \mid \state, \action}$
with unique probability measure $\Prob^\mdp_\policy$ on the Borel $\sigma$-algebra over measurable subsets $\varphi \subseteq \inftrajectories{\mdp}$~\citep{DBLP:books/wi/Puterman94}.
We drop the superscript when the context is clear.
%For $\policy \in \mpolicies{\mdp}$, we denote by $\probtransitions_\policy \colon \states \to \distributions{\states}$ the probability transition distribution of $\mdp_\policy$. % and by $\rewards_\policy \colon \states \to \R$ its reward function.
%We write $\trace = \defaulttrace \sim \mdp_{\policy}$ for $\trajectorytuple{\state}{\action}{T} \sim \Prob^{\mdp}_{\policy}$ with $\trace \in \traces{\mdp_{\policy}}$.
%
%\smallparagraph{Stationary distributions.}~
%Let $\policy \in \mpolicies{\mdp}$, $\stationary{\policy}^{t}: \states \to \distributions{\states}$ with $
Define $\stationary{\policy}^{t}\fun{\state' \mid \state} =  \Prob^{\mdp_\state}_\policy(\{{\seq{\state}{\infty}, \seq{\action}{\infty}}
\mid \state_t = \state'\})$ as the distribution giving the probability of being in each state of $\mdp_\state$ after $t$ steps.
% $B \subseteq \states$ is a \emph{strongly connected component} (SCC)
% of $\mdp_\policy$ if for any pair of states $\state, \state' \in B$, $\stationary{\policy}^t\fun{\state' \mid \state} > 0$ for some $t \in \N$. 
% It is a \emph{bottom SCC} (BSCC) if (i) $B$ is a maximal SCC, and (ii) for each $\state \in B$, $\probtransitions_\policy\fun{B \mid \state} = 1$.
%$B\subseteq \states$ is a \emph{bottom strongly connected component} (BSCC) if given any  states $\state, \state' \in B$, (i) $B$ is a maximal subset satisfying $\stationary{\policy}^t\fun{\state'\mid \state} > 0$  for some $t \geq 0$, and (ii)  
$B\subseteq \states$ is a \emph{bottom strongly connected component} (BSCC) of $\mdp_\policy$ if (i) $B$ is a maximal subset satisfying $\stationary{\policy}^t\fun{\state'\mid \state} > 0$  for any  $\state, \state' \in B$ and some $t \geq 0$, and (ii)  
$\expectedsymbol{\action \sim \policy\fun{\sampledot \mid \state}} \probtransitions\fun{B \mid \state, \action}=1$ for all $\state \in \states$.
The unique stationary distribution of $B$ is
$\stationary{\policy} \in \distributions{B}$.
We write $\state, \action \sim \stationary{\policy}$ for sampling $\state$ from $\stationary{\policy}$ then $\action$ from $\policy$.
An MDP $\mdp$ is \emph{ergodic} if for all $\policy \in \mpolicies{\mdp}$, the state space of $\mdp_\policy$ consists of a unique aperiodic BSCC %
with $\stationary{\policy} = \lim_{t \to \infty} \stationary{\policy}^t\fun{\sampledot \mid \state}$ for all $\state \in \states$.


%\smallparagraph{Events and value functions.}~%
% \smallparagraph{Value functions.}~%
% Let $\labelset{C}, \labelset{T} \subseteq \atomicprops$, the \emph{(constrained) reachability} %(resp. \emph{reachability})
% event is $\until{\labelset{C}}{\labelset{T}} =
% \{\, \seq{\state}{\infty}, \seq{\action}{\infty} \,|\, \exists i \in \N, \forall j < i, \state_j \in \getstates{\labelset{C}} \wedge \state_i \in \getstates{\labelset{T}}\, \} \in \borel{\inftrajectories{\mdp}}$.
% %(resp. $\eventually \labelset{T} = \until{\neg \emptyset}{\labelset{T}}$).
% Safety w.r.t. failure states $\labelset{T}$ can be expressed as the safety-constrained reachability %event
% to a destination $\labelset{C}$
% %(resp. safety event)
% through $\until{\neg \labelset{T}}{\labelset{C}}$.
% %(resp. $\always \neg \labelset{T} = \inftrajectories{\mdp} \setminus \eventually \labelset{T}$).
% Let $\discount \in \mathopen[0, 1\mathclose[,$
% % $\varphi \in \set{\epsilon, \until{\labelset{C}}{\labelset{T}}, \eventually \labelset{T}}$ where $\epsilon$ is the empty symbol, and
% % $\varphi \in \set{\epsilon, \until{\labelset{C}}{\labelset{T}}}$ where $\epsilon$ is the empty symbol, and
% $\epsilon$ be the empty symbol,
% $\varphi \in \borel{\inftrajectories{\mdp}} \cup \set{\epsilon}$,
% and goal-oriented MDP $\objective \colon \states \times \borel{\inftrajectories{\mdp}} \cup \set{\epsilon} \to \mdpspace$,
% the \emph{value} obtained by running
% $\policy \in \mpolicies{\mdp}$
% from state $\state$ in $\mdp$ is
% $\values{\policy}{\varphi}{\state} = \expectedsymbol{\policy}^{\objective\fun{\state, \varphi}}\left[{\sum_{t=0}^{\infty} \discount^t \rewards(\state_t, \action_t)}\right]$ with 
% $\lim_{\discount \to 1}\values{\policy}{\varphi}{\state} = \Prob_{\policy}^{\mdp_\state}\fun{\varphi}$ when $\varphi \neq \epsilon$.
% %$\values{\policy}{\varphi}{\state}$
% It
% corresponds to the \emph{expected discounted}
% (i) \emph{return} with $\objective\fun{\state, \epsilon} = \mdp_\state$,
% and (ii) \emph{reachability} with $\objective\fun{\state, \until{\labelset{C}}{\labelset{T}}} = \selfloop{\mdp_\state}{\getstates{\neg \labelset{C}} \cup \getstates{\labelset{T}}} \oplus \rewards^\labelset{T}$ so that $\rewards^{\labelset{T}} = \fun{1 - \discount}\condition{\getstates{\labelset{T}} \times \actions}$. 
% %, (iii) \emph{reachability} when $\varphi = \eventually \labelset{T}$ with $\mdp[\state, \eventually \labelset{T}] = \selfloop{\mdp_\state}{\getstates{\labelset{T}}} \oplus \rewards^\labelset{T}$.
% % When $\varphi \in \set{\until{\labelset{C}}{\labelset{T}}, \eventually\labelset{T}}$,
% The typical goal of an RL agent is to learn a policy $\policy^{\star}$ that maximizes $\values{\policy^{\star}}{}{\sinit}$.
%for $\varphi = \until{\labelset{c}}{\labelset{t}}$,
% Observe that
%$\values{\policy}{\varphi}{t}$ = 1 for $t \in \getstates{\labelset{t}}$ and
%$\lim_{\discount \to 1}\values{\policy}{\varphi}{\state} = \prob_\policy^{\mdp_\state}\fun{\varphi}$ for $\state \in \states$.
% $\lim_{\discount \to 1}\values{\policy}{\until{\labelset{C}}{\labelset{T}}}{\state} = \Prob_{\policy}^{\mdp_\state}\fun{\until{\labelset{C}}{\labelset{T}}}$ for all $\state \in \states$.
% The \emph{action-value function} is $\qvalues{\policy}{\varphi}{\state}{\action} = \rewards'\fun{\state, \action} + \expected{\state' \sim \probtransitions\fun{\sampledot \mid \state, \action}}{\discount \values{\policy}{\varphi}{\state'}}$, with $\rewards' = \rewards$ if $\varphi = \epsilon$ and $\rewards' = \rewards^\labelset{T}$ otherwise.
%
%\todo{F: I added this, because otherwise I feel that $\stationary{}^t$ is not used}.
% \subsection{Latent Space Models and Bisimulation}
%
%\smallparagraph{Values.}~%
%Let $\labelset{C}, \labelset{T} \subseteq \atomicprops$, the \emph{(constrained) reachability} 
%event is $\until{\labelset{C}}{\labelset{T}} =
%\{\, \seq{\state}{\infty}, \seq{\action}{\infty} \,|\, \exists i \in \N, \forall j < i, \state_j \models {\labelset{C}} \wedge \state_i \models {\labelset{T}}\, \} \subseteq \inftrajectories{\mdp}$.
%Safety w.r.t. failure states $\labelset{T}$ can be expressed as the safety-constrained reachability
%to a destination $\labelset{C}$
%through $\until{\neg \labelset{T}}{\labelset{C}}$.
%Given $\policy \in \mpolicies{\mdp}$, the \emph{value} of a state $\state \in \states$ is the expected value of a random variable
%obtained by running $\policy$ from $\state$.
%For a discount factor $\discount \in \mathopen[0, 1\mathclose]$, we write $\values{\policy}{}{\state} = \expectedsymbol{\policy}^{\mdp_{\state}}\left[ \sum_{t = 0}^{\infty} \discount^t \rewards\fun{\state_t, \action_t} \right]$ for the \emph{discounted return} and 
%$\values{\policy}{\varphi}{\state} = \expectedsymbol{\policy}^{\mdp_{\state}}\left[ \discount^{t^{\star}} \condition{\tuple{\seq{\state}{\infty}, \seq{\action}{\infty}} \, \in \, \varphi} \right]$ for the \emph{discounted probability of satisfying} $\varphi = \until{\labelset{C}}{\labelset{T}}$,
%where $t^{\star}$ is the length of the shortest trajectory prefix that allows satisfying $\varphi$.
%Notice that $\values{\policy}{\varphi}{\state} = \Prob_{\policy}^{\mdp_{\state}}\fun{\varphi}$ when $\discount = 1$.
%The typical goal of an RL agent is to learn a policy $\policy^{\star}$ that maximizes $\values{\policy^{\star}}{}{\sinit}$ through interactions with an (unknown) MDP.

\smallparagraph{Value objectives.}~%
Given $\policy \in \mpolicies{\mdp}$, the \emph{value} of a state $\state \in \states$ is the expected value of a random variable obtained by running $\policy$ from $\state$.
For a discount factor $\discount \in \mathopen[0, 1\mathclose]$, we consider the following objectives.
(i) \emph{Discounted return}:~we write $\values{\policy}{}{\state} = \expectedsymbol{\policy}^{\mdp_{\state}}\left[ \sum_{t = 0}^{\infty} \discount^t \rewards\fun{\state_t, \action_t} \right]$ for the expected discounted rewards accumulated along trajectories.
	The typical goal of an RL agent is to learn a policy $\policy^{\star}$ that maximizes $\values{\policy^{\star}}{}{\sinit}$ through interactions with the (unknown) MDP; %$\mdp$, being possibly unknown; % with an (unknown) MDP.
(ii) \emph{Reachability}:~let $\labelset{C}, \labelset{T} \subseteq \atomicprops$, the \emph{(constrained) reachability} 
	event is $\until{\labelset{C}}{\labelset{T}} =
	\{\, \seq{\state}{\infty}, \seq{\action}{\infty} \,|\, \exists i \in \N, \forall j < i, \state_j \models {\labelset{C}} \wedge \state_i \models {\labelset{T}}\, \} \subseteq \inftrajectories{\mdp}$.
	%\{\, \seq{\state}{\infty}, \seq{\action}{\infty} \in \inftrajectories{\mdp} \mid \exists i \in \N, \forall j < i, \state_j \models {\labelset{C}} \wedge \state_i \models {\labelset{T}}\, \} $;
	% Intuitively, this is the set of trajectories where the agent hits a goal state labeled with $\labelset{T}$ while 
	We write $\values{\policy}{\varphi}{\state} = \expectedsymbol{\policy}^{\mdp_{\state}}\left[ \discount^{t^{\star}} \condition{\tuple{\seq{\state}{\infty}, \seq{\action}{\infty}} \, \in \, \varphi} \right]$ for the \emph{discounted probability of satisfying} $\varphi = \until{\labelset{C}}{\labelset{T}}$,
	where $t^{\star}$ is the length of the shortest trajectory prefix that allows satisfying $\varphi$.
	Intuitively, %$\values{\policy}{\varphi}{\state}$
	this denotes the discounted return of remaining in a region of the MDP where states are labeled with $\labelset{C}$, until visiting \emph{for the first time} a \emph{goal state} labeled with $\labelset{T}$, and the return is the binary reward signal capturing this event.
	\emph{Safety} w.r.t. failure states $\labelset{C}$ can be expressed as the safety-constrained reachability to a destination $\labelset{T}$ through $\until{\neg \labelset{C}}{\labelset{T}}$.
	Notice that $\values{\policy}{\varphi}{\state} = \Prob_{\policy}^{\mdp_{\state}}\fun{\varphi}$ when $\discount = 1$.
%\end{enumerate}

\smallparagraph{Latent MDP.}~Given the original (continuous, possibly unknown) environment model $\mdp$, a \emph{latent space model} is another (smaller, explicit) MDP $\latentmdp = \latentmdptuple$ with state-action space linked to the original one via state and action \emph{embedding functions}:  $\embed\colon \states \to \latentstates$ and $\embeda\colon \latentstates \times \latentactions \to \actions$.
%Through auto-encoding procedures, 
%The latter can be learned to optimize an \emph{equivalence criterion} between the two models \citep{DBLP:conf/icml/GeladaKBNB19,DBLP:journals/corr/abs-2112-09655}.
%Fix MDPs $\mdp = \mdptuple$ and $\latentmdp = \latentmdptuple$
%such that $\latentstates$ is equipped with metric $\distance_{\latentstates}$.
% with $\distance_{\latentstates}$ being a metric on $\latentstates$.
%Let $\embed\colon \states \to \distributions{\latentstates}$ and $\embeda \colon \latentstates \times \latentactions \to \actions$ be respectively state and action \emph{embedding functions}.
%Let $\embed\colon \states \to \latentstates$ and $\embeda \colon \latentstates \times \latentactions \to \actions$ be state and action \emph{embedding functions}, connecting the spaces of the two MDPs.
We refer to $\tuple{\latentmdp, \embed, \embeda}$ as a \emph{latent space model} of $\mdp$ and $\latentmdp$ as its \emph{latent MDP}.
Our goal is to learn $\tuple{\latentmdp, \embed, \embeda}$ by optimizing an \emph{equivalence criterion} between the two models.
% We assume that $\latentstates$ is equipped with metric $\distance_{\latentstates}$, and
We assume that $\distance_{\latentstates}$ is a metric on $\latentstates$, and
% write $\latentmpolicies = \mpolicies{\latentmdp}$
write  $\latentmpolicies$ for the set of policies of $\latentmdp$
and ${\latentvaluessymbol{\latentpolicy}{}}$ for the values of running $\latentpolicy \in \latentmpolicies$ in $\latentmdp$. 
% and ${\latentvaluessymbol{\latentpolicy}{}}$ % (resp. ${\latentqvaluessymbol{\latentpolicy}{}}$)
% for the value
%(resp. action-value)
% function of a policy $\latentpolicy \in \latentmpolicies$ in $\latentmdp$.
\begin{remark}[Latent flow] \label{rmk:latent-policy-execution}
The latent policy $\latentpolicy$ can be seen as a policy in $\mdp$ (cf. Fig.~\ref{subfig:latent-flow-guarantees}): states passed to $\latentpolicy$ are first embedded with $\embed$ to the latent space, then the actions produced by $\latentpolicy$ are executed via $\embeda$ in the original environment.
Let $\state \in \states$, we write 
$\latentaction \sim \latentpolicy\fun{\sampledot \mid \state}$ for %first sampling $\latentstate \sim \embed\fun{\sampledot \mid \state}$ and then executing $\latentaction \sim \latentpolicy\fun{\sampledot \mid \latentaction}$ via $\embeda$:
$\latentpolicy\fun{\sampledot \mid \embed\fun{\state}}$, then
the reward and next state are respectively given by $\rewards\fun{\state, \latentaction} = \rewards\fun{\state, \embeda\fun{\embed\fun{\state}, \latentaction}}$ and $ \state' \sim \probtransitions\fun{\sampledot \mid \state, \latentaction} = \probtransitions\fun{\sampledot \mid \state, \embeda\fun{\embed\fun{\state}, \latentaction}}$.
%given $\latentstate$, we further write
%$\rewards\fun{\state, \latentaction}$ for $\rewards\fun{\state, \embeda\fun{\latentstate, \latentaction}}$
%and $\probtransitions\fun{\sampledot \mid \state, \latentaction}$ for $\probtransitions\fun{\sampledot \mid \state, \embeda\fun{\latentstate, \latentaction}}$.
% Moreover, we naturally write $\tuple{\state, \latentstate}, \tuple{\action, \latentaction} \sim \stationary{\latentpolicy}$ for sampling $\state \sim \stationary{\latentpolicy}$, then embedding $\latentstate \sim \embed\fun{\sampledot \mid \state}$, and finally executing $\action = \embeda\fun{\latentstate, \latentaction}$ with $\latentaction \sim \latentpolicy\fun{\sampledot \mid \latentstate}$.
%We further write $\qvalues{\latentpolicy}{}{\state}{\latentaction}$ as shorthand for $\qvalues{\latentpolicy}{}{\state}{\embeda\fun{\state, \latentaction}}$.
\end{remark}

\begin{figure}
\begin{subfigure}{.375\textwidth}
    \centering
    \includegraphics[width=.95\textwidth]{ressources/latent_flow.pdf}
    \caption{Execution of the latent policy $\latentpolicy$ in the original and latent MDPs, and local losses.}
    \label{subfig:latent-flow-guarantees}
\end{subfigure}
\hspace{.025\textwidth}
\begin{subfigure}{.575\textwidth}
    \centering
    \includegraphics[width=.925\textwidth]{ressources/latent_flow_distillation.pdf}
    \caption{Parallel execution of the original RL policy $\policy$ in the original and latent MDPs, local losses, and steady-state regularizer.}
    \label{subfig:latent-fow-distillation}
\end{subfigure}
\caption{Latent flows: arrows represent (stochastic) mappings, the original (resp. latent) state-action space is spread along the blue (resp. green) area, and distances are depicted in red.
Distilling $\policy$ into $\latentpolicy$ via flow~(b) by minimizing 
%the steady-state regularizer 
$\steadystateregularizer{\policy}$ 
allows closing the gap between flows~(a) and~(b).}
\label{fig:latent-flow}
\end{figure}

%\smallparagraph{Local losses.}~%
% \emph{Local losses} are measured under an expectation over a
% state-action distribution $\stationary{}$ and are defined as:% follows:
% Let $\stationary{} \in \distributions{\states \times \latentstates \times \actions \times \latentactions}$,
% \emph{local losses} are defined as:% follows:
% \begin{align*}
%     \localrewardloss{\stationary{}} &= \expectedsymbol{\tuple{\state, \latentstate}, \tuple{\action, \latentaction} \sim \stationary{}}\left| \rewards\fun{\state, \action} - \latentrewards\fun{\latentstate, \latentaction} \right|, %\\
%     &
%     \localtransitionloss{\stationary{}} &= \expectedsymbol{\tuple{\state, \latentstate}, \tuple{\action, \latentaction} \sim \stationary{}} \wassersteindist{\distance_{\latentstates}}{\embed\probtransitions\fun{\sampledot \mid \state, \action}}{\latentprobtransitions\fun{\sampledot \mid \latentstate, \latentaction}}
% \end{align*}
% where $\embed\probtransitions\fun{\sampledot \mid \state, \action}$ is
% a shorthand for the distribution over $\latentstates$ of sampling
% $\state' \sim \probtransitions\fun{\sampledot \mid \state, \action}$ and
% then embedding $\latentstate' \sim \embed\fun{\sampledot \mid \state}$. %
%Let $\stationary{} \in \distributions{\states \times \latentactions}$,
\textbf{Local losses} allow quantifying the distance between the original and latent reward/transition functions \emph{in the local setting}, i.e., under a given state-action distribution $\stationary{} \in \distributions{\states \times \latentactions}$:
%They are defined as:% follows:
\begin{align*}
    \localrewardloss{\stationary{}} &= \expectedsymbol{\state,  \latentaction \sim \stationary{}}\left| \rewards\fun{\state, \latentaction} - \latentrewards\fun{\embed\fun{\state}, \latentaction} \right|, %\\
    &
    \localtransitionloss{\stationary{}} &= \expectedsymbol{\state, \latentaction \sim \stationary{}} 
    %\wassersteindist{\distance_{\latentstates}}{\embed\probtransitions\fun{\sampledot \mid \state, \latentaction}}{\latentprobtransitions\fun{\sampledot \mid \embed\fun{\state}, \latentaction}}
    D\fun{\embed\probtransitions\fun{\sampledot \mid \state, \latentaction}, \latentprobtransitions\fun{\sampledot \mid \embed\fun{\state}, \latentaction}}
\end{align*}
where $\embed\probtransitions\fun{\sampledot \mid \state, \latentaction}$ is
the distribution %over $\latentstates$
of drawing
$\state' \sim \probtransitions\fun{\sampledot \mid \state, \latentaction}$ then
% then embedding $\latentstate' \sim \embed\fun{\sampledot \mid \state}$. %
embedding $\latentstate' = \embed\fun{\state'}$, and $D$ is a discrepancy measure. %
Fig~\ref{subfig:latent-flow-guarantees} depicts the losses when states and actions are drawn from a stationary distribution $\stationary{\latentpolicy}$
% induced by the execution of $\latentpolicy \in \latentmpolicies$ in $\mdp$.
resulting from running $\latentpolicy \in \latentmpolicies$ in $\mdp$.
% To enable model checking the latent model and upcoming guarantees, we work under the following assumptions:
% We work under the following assumptions, ensuring the upcoming guarantees:
In this work, we focus on the case where $D$ is the \emph{Wasserstein distance} $\wassersteinsymbol{\distance_{\latentstates}}$:
%\smallparagraph{Wasserstein distance.}~Let
given two distributions $P, Q$ over a measurable set $\measurableset$ %and assume $\measurableset$ is equipped with a distance metric $\distance$.
equipped with a metric $\distance$,
%$\distance \colon \measurableset \times \measurableset \to \mathopen[0, +\infty \mathclose[$. %such that $\distance\fun{x, y} = 0 \iff x = y$.
$\wassersteinsymbol{\distance}$ is the solution of the \emph{optimal transport} (OT) from $P$ to $Q$, i.e.,
%for $c \colon \measurableset^2 \to \mathopen[0, +\infty \mathclose[$
the minimum cost of changing $P$ into $Q$ \citep{Villani2009}: 
$
\wassersteindist{\distance}{P}{Q} = \inf_{\coupling \in \couplings{P}{Q}} \expectedsymbol{x, y \sim \coupling} \distance\fun{x, y},
$
% \begin{equation}
%     \wassersteindist{c}{P}{Q} = \inf_{\coupling \in \couplings{P}{Q}} \int_{\measurableset \times \measurableset} c\fun{x, y} \coupling\fun{x, y} \, dx \, dy
% \end{equation}
$\couplings{P}{Q}$ being the set of all \emph{couplings} of $P$ and $Q$. %, i.e., the set of joint distributions whose marginals are $P$ and $Q$.
% When $c = \distance$, the solution of the OT is the \emph{Wasserstein distance}.
%In that case, the Kantorovich-Rubinstein duality allows to formulate the Wasserstein distance between $P$ and $Q$ as
The \emph{Kantorovich duality} yields
%\begin{equation}
$
    %\wassersteindist{\distance}{P}{Q} = \sup_{f \in \Lipschf{\distance}} \int f\fun{x} \, dP\fun{x} - \int f\fun{y} \, dQ\fun{y},
    \wassersteindist{\distance}{P}{Q} = \sup_{f \in \Lipschf{\distance}} \expectedsymbol{x \sim P} f\fun{x}  - \expectedsymbol{x \sim Q} f\fun{y}
    %\wassersteindist{\distance}{P}{Q} = \sup_{f \in \Lipschf{\distance}}\, \expectedsymbol{x \sim P} f\fun{x} - \expectedsymbol{y \sim Q} f\fun{y} %\wassersteindist{\distance}{P}{Q} = \sup_{f \in \Lipschf{\distance}}\; \expectedsymbol{x \sim P} f\fun{x}  - \expectedsymbol{x \sim Q} f\fun{y}
    \label{eq:wasserstein-dual}
$
%\end{equation}
where $\Lipschf{\distance}$ is the set of 1-Lipschiz functions.
%functions w.r.t. $\distance$.
%, i.e., $\Lipschf{\distance} = \set{f \colon f\fun{x} - f\fun{y} \leq \distance\fun{x, y}}$.
Local losses are related to a well-established \emph{behavioral} equivalence between transition systems, called \emph{bisimulation}.

\smallparagraph{Bisimulation.}
A \emph{bisimulation} $\bisimulation$ %_{\Phi}$
on $\mdp$ is a behavioral equivalence between states $\state_1, \state_2 \in \states$ 
%such that, $\state_1 B_{\Phi} \state_2$ iff
so that, $\state_1 \, \bisimulation \, \state_2$ iff
(i) $\probtransitions(T \mid \state_1, \action) = \probtransitions(T \mid \state_2, \action)$,
(ii) $\labels(\state_1) = \labels(\state_2)$,
%if  $\labels \in \Phi$, 
and (iii) $\rewards(\state_1, \action) = \rewards(\state_2, \action)$
%if $\rewards \in \Phi$,
for each action $\action \in \actions$ and (Borel measurable) equivalence class $T \in \states / \bisimulation$.
Properties of bisimulation include trajectory and value equivalence \citep{DBLP:conf/popl/LarsenS89,DBLP:journals/ai/GivanDG03}.
Requirements (ii) and (iii) can be respectively relaxed depending on whether we focus only on behaviors formalized through $\atomicprops$ or rewards.
The relation can be extended to compare two MDPs (e.g., $\mdp$ and $\latentmdp$) by considering the disjoint union of their state space.
We denote the largest bisimulation relation by $\sim$.

%\smallparagraph{Distance.}
Characterized by a logical family of functional expressions
% %\citep{DBLP:journals/tcs/DesharnaisGJP04}
derived from a logic $\logic$,
\emph{bisimulation pseudometrics}~\citep{DBLP:journals/tcs/DesharnaisGJP04} generalize the notion of bisimilariy. %by assigning a \emph{bisimilarity distance} between states, 
% Desharnais et al.~\citep{DBLP:journals/tcs/DesharnaisGJP04} introduced \emph{bisimulation pseudometrics} for continuous Markov processes, generalizing the notion of bisimilariy by assigning a \emph{bisimilarity distance} between states. 
% A \emph{pseudometric} $\bidistance$ satisfies symmetry 
%and the triangle inequality. %, but not the identity of indiscernible.
% characterized by a logical family of functional expressions
% %\citep{DBLP:journals/tcs/DesharnaisGJP04}
% derived from a logic $\logic$.
More specifically, given a policy $\policy \in \mpolicies{\mdp}$, we consider a family $\functionalexpr$ of real-valued functions parameterized by a discount factor $\discount$ and defining the semantics of $\logic$ in $\mdp_\policy$.
Such functional expressions allow to formalize discounted properties such as reachability, safety, as well as general $\omega$-regular specifications
%\citep{DBLP:conf/fsttcs/ChatterjeeAMR08, DBLP:conf/fossacs/ChenBW12}
\citep{DBLP:conf/fsttcs/ChatterjeeAMR08}
and may include rewards as well \citep{DBLP:conf/birthday/FernsPK14}.
% further allowing to formalize (discounted) properties. %, including reachability \citep{DBLP:conf/fsttcs/ChatterjeeAMR08}.
The pseudometric $\bidistance_{\policy} $
is defined as \emph{the largest behavioral difference} $\bidistance_\policy\fun{\state_1, \state_2} = \sup_{f \in \functionalexpr} \left| f\fun{\state_1} - f\fun{\state_2} \right|$,
% for all $\state_1,\state_2 \in \states$.
%Let $\bidistance^{{}_{\scriptstyle\rewards}}_\policy$ and $\bidistance^{{}_{\scriptstyle\labels}}_\policy$ be respectively pseudometrics characterized by functional expressions including rewards \citep{DBLP:conf/birthday/FernsPK14} and based on state labels \citep{DBLP:conf/fossacs/ChenBW12}, 
%\emph{The kernel of $\bidistance_{\policy}$ is bisimilarity}: $\bidistance_{\policy}\fun{\state_1, \state_2} = 0$ iff $\state_1 \sim \state_2$.
and \emph{its kernel is bisimilarity}: $\bidistance_{\policy}\fun{\state_1, \state_2} = 0$ iff $\state_1 \sim \state_2$.
%We distinguish between pseudometrics characterized by functional expressions including rewards   $\bidistance^{\rewards}_\policy$ \citep{DBLP:conf/birthday/FernsPK14},  and those based on state labels $\bidistance^{\labels}_\policy$ \citep{DBLP:conf/fossacs/ChenBW12}.
In particular, \emph{value functions are Lipschitz-continuous w.r.t.~$\bidistance_{\policy}$}:
%, i.e., given any event $E \in \borel{\inftrajectories{}}$ semantically resulting from a formula in $\logic$,
%set $\rho(\varphi)$ to $E$ if $\varphi = \labels$ and to $\epsilon$ otherwise,
%then
% $| \values{\policy}{\rho(\varphi)}{\state_1} - \values{\policy}{\rho(\varphi)}{\state_2}| \leq \fun{1 - \discount}^{-\condition{\{\rewards\}}\fun{\varphi}} \bidistance^{\varphi}_{\policy}\fun{\state_1, \state_2}$.
$| \values{\policy}{\scalebox{.95}{$\cdot$}}{\state_1} - \values{\policy}{\scalebox{.95}{$\cdot$}}{\state_2}| \leq 
%\nicefrac{1}{(1-\discount)^{K_{\varphi}}} \cdot \bidistance^{\varphi}_{\policy}\fun{\state_1, \state_2}$,
K \bidistance_{\policy}\fun{\state_1, \state_2}$,
% $| \values{\policy}{\rho(\varphi)}{\state_1} - \values{\policy}{\rho(\varphi)}{\state_2}| \leq  K\, \bidistance^{\varphi}_{\policy}\fun{\state_1, \state_2}$,
% with $K = \fun{1 - \discount}^{-\condition{\{\rewards\}}\fun{\varphi}}$.
% with $K_{\varphi} = \condition{\{\rewards\}}\fun{\varphi}$ .
where $K$ is $\nicefrac{1}{\fun{1 - \discount}}$ if rewards are included in $\functionalexpr$ and $1$ otherwise.
%
To ensure the upcoming bisimulation guarantees, we make the following assumptions:
\begin{assumption}\label{assumption:vae-mdp}
MDP $\mdp$ is ergodic, $\images{\rewards}$ is a bounded space scaled in $\left[\nicefrac{-1}{2}, \nicefrac{1}{2} \right]$, and
the embedding function preserves the labels, i.e., $\embed\fun{\state} = \latentstate \implies \labels\fun{\state} = \latentlabels\fun{\latentstate}$ for $\state \in \states$, $\latentstate \in \latentstates$. 
\end{assumption}
Note that the ergodicity assumption is compliant with episodic RL and a wide range of continuous learning tasks (see \citealt{DBLP:conf/nips/Huang20,DBLP:journals/corr/abs-2112-09655} for detailed discussions on this setting).
% We kindly refer the reader to the works of \citet{DBLP:conf/nips/Huang20,DBLP:journals/corr/abs-2112-09655} for detailed discussions on this setting.

\smallparagraph{Bisimulation bounds \citep{DBLP:journals/corr/abs-2112-09655}.}~%
%Let $\bidistance^{\labels}_{\policy}$ and $\bidistance^{\rewards}_{\policy}$ be respectively the pseudometric presented by \cite{DBLP:conf/fossacs/ChenBW12} and \cite{DBLP:conf/uai/FernsPP05}.
%The kernel of those pseudometrics is the bisimilarity, defined under a given policy $\policy$. 
%The expected \emph{bisimulation distance} between states and their embedding %latent abstraction is bounded by local losses:
%assume $\mdp$ is ergodic, rewards are scaled in $\mathopen[ -\nicefrac{1}{2}, \nicefrac{1}{2} \mathclose]$, and the embedding function preserves the labels (see \citealt{DBLP:journals/corr/abs-2112-09655} for a detailed discussion of these assumptions),
%Let $\labelset{C}, \labelset{T} \subseteq \atomicprops$, $\varphi \in \set{\until{\labelset{C}}{\labelset{T}}, \eventually \labelset{T}}$,
$\mdp$ being set over continuous spaces with possibly unknown dynamics, evaluating $\bidistance$ can turn out to be particularly arduous, if not intractable.
A solution is to evaluate the original and latent model bisimilarity via local losses:
fix
$\latentpolicy \in \latentmpolicies$, assume $\latentmdp$ is discrete, then given the induced stationary distribution $\stationary{\latentpolicy}$ in $\mdp$, let $\state_1, \state_2 \in \states$ with $\embed\fun{\state_1} = \embed\fun{\state_2}$:
%$K_{\varphi} = \condition{\set{\rewards}}\fun{\varphi}$,
%  \begin{align}
%      \expectedsymbol{\state \sim \stationary{\latentpolicy}} \expectedsymbol{\latentstate \sim \embed\fun{\sampledot \mid \state}} \bidistance^{\rewards}_{\latentpolicy}\fun{\state, \latentstate} & \leq  \localrewardloss{\stationary{\latentpolicy}} + \discount \localtransitionloss{\stationary{\latentpolicy}} \KV, %\frac{\KR{\latentpolicy}}{1 - \discount \KP{\latentpolicy}},
%      & \expectedsymbol{\state, \latentaction \sim \stationary{\latentpolicy}} \left| \qvalues{\latentpolicy}{}{\state}{\latentaction} - \latentqvalues{\latentpolicy}{}{\embed\fun{\state}}{\latentaction} \right| & \leq \frac{\localrewardloss{\stationary{\latentpolicy}} + \discount \KV \localtransitionloss{\stationary{\latentpolicy}}}{1 - \gamma}, \notag \\
%      \expectedsymbol{\state \sim \stationary{\latentpolicy}} \bidistance^{\labels}_{\latentpolicy}\fun{\state, \embed\fun{\state}} & \leq \frac{\discount \localtransitionloss{\stationary{\latentpolicy}}}{1 - \discount},\, \text{and} & \expectedsymbol{\state, \latentaction \sim \stationary{\latentpolicy}} \left| \qvalues{\latentpolicy}{\varphi}{\state}{\latentaction} - \latentqvalues{\latentpolicy}{\varphi}{\embed\fun{\state}}{\latentaction} \right| & \leq \frac{\discount \localtransitionloss{\stationary{\latentpolicy}}}{1 - \discount}. \label{eq:bidistance-bound}
%  \end{align}
% \begin{align}
%     &\expectedsymbol{\state \sim \stationary{\latentpolicy}} \bidistance^{\rewards}_{\latentpolicy}\fun{\state, \embed\fun{\state}} \leq  \localrewardloss{\stationary{\latentpolicy}} + \discount \localtransitionloss{\stationary{\latentpolicy}} \KV,& %\frac{\KR{\latentpolicy}}{1 - \discount \KP{\latentpolicy}},
%     &\bidistance^{\rewards}_{\latentpolicy}\fun{\state_1, \state_2} \leq \left[\localrewardloss{\stationary{\latentpolicy}} +
%     \discount \localtransitionloss{\stationary{\latentpolicy}} \KV\right] \fun{\stationary{\latentpolicy}^{-1}\fun{\state_1} + \stationary{\latentpolicy}^{-1}\fun{\state_2}} \notag\\
%     %& \expectedsymbol{\state, \latentaction \sim \stationary{\latentpolicy}} \left| \qvalues{\latentpolicy}{}{\state}{\latentaction} - \latentqvalues{\latentpolicy}{}{\embed\fun{\state}}{\latentaction} \right| & \leq \frac{\localrewardloss{\stationary{\latentpolicy}} + \discount \KV \localtransitionloss{\stationary{\latentpolicy}}}{1 - \gamma}, \notag \\
%     &\expectedsymbol{\state \sim \stationary{\latentpolicy}} \bidistance^{\labels}_{\latentpolicy}\fun{\state, \embed\fun{\state}} \leq \frac{\discount \localtransitionloss{\stationary{\latentpolicy}}}{1 - \discount} & % ,\, \text{and} & \expectedsymbol{\state, \latentaction \sim \stationary{\latentpolicy}} \left| \qvalues{\latentpolicy}{\varphi}{\state}{\latentaction} - \latentqvalues{\latentpolicy}{\varphi}{\embed\fun{\state}}{\latentaction} \right| & \leq \frac{\discount \localtransitionloss{\stationary{\latentpolicy}}}{1 - \discount}.
%     &\bidistance^{\labels}_{\latentpolicy}\fun{\state_1, \state_2} \leq  \frac{\discount \localtransitionloss{\stationary{\latentpolicy}}}{1 - \discount} \fun{\stationary{\latentpolicy}^{-1}\fun{\state_1} + \stationary{\latentpolicy}^{-1}\fun{\state_2}}
%     \label{eq:bidistance-bound}
% \end{align}
% \begin{equation}
% &\expectedsymbol{\state \sim \stationary{\latentpolicy}} \bidistance^{\varphi}_{{\latentpolicy}}\fun{\state, \embed\fun{\state}} \leq \localrewardloss{\stationary{\latentpolicy}} K_{\varphi} + \frac{\discount \localtransitionloss{\stationary{\latentpolicy}}}{1 - \discount},&
% &\bidistance^{\varphi}_{\latentpolicy}\fun{\state_1, \state_2} \leq \Big(\localrewardloss{\stationary{\latentpolicy}} K_{\varphi}+ \frac{\discount \localtransitionloss{\stationary{\latentpolicy}}}{1 - \discount}\Big) \fun{\stationary{\latentpolicy}^{-1}\fun{\state_1} + \stationary{\latentpolicy}^{-1}\fun{\state_2}}
%\label{eq:bidistance-bound}
% \end{equation}
\begin{align}
\expectedsymbol{\state \sim \stationary{\latentpolicy}} \bidistance_{{\latentpolicy}}\fun{\state, \embed\fun{\state}} &\leq %K_{\varphi}
\frac{\localrewardloss{\stationary{\latentpolicy}} + \discount \localtransitionloss{\stationary{\latentpolicy}}}{1 - \discount}, &
\bidistance_{\latentpolicy}\fun{\state_1, \state_2} &\leq \Big( \frac{\localrewardloss{\stationary{\latentpolicy}} +\discount \localtransitionloss{\stationary{\latentpolicy}}}{1 - \discount}\Big) \fun{\stationary{\latentpolicy}^{-1}\fun{\state_1} + \stationary{\latentpolicy}^{-1}\fun{\state_2}}.
\label{eq:bidistance-bound}
\end{align}
%for some Lipschitz constants $\langle\KR{\latentpolicy}, \KP{\latentpolicy}\rangle$ over $\latentrewards_{\latentpolicy}$ and $\latentprobtransitions_{\latentpolicy}$, and $\KV = \min\set{\frac{\sup_{\latentstate \in \latentstates} \left| \latentrewards_{\latentpolicy}\fun{\latentstate} \right|}{1 - \discount}, \frac{\KR{\latentpolicy}}{1 - \discount \KP{\latentpolicy}}}$.
The two inequalities guarantee respectively the \emph{quality of the abstraction} and \emph{representation}: when local losses are small, (i) states and their embedding are bisimilarly close in average, and (ii) all states sharing the same discrete representation are bisimilarly close.
The local losses and related bounds can be efficiently PAC-estimated. %\citep{DBLP:journals/corr/abs-2112-09655}.
% Our goal is to formally check properties in the tractable latent model $\latentmdp$ operating under a latent policy $\latentpolicy$.
Our goal is to learn a latent model where the behaviors of the agent executing $\latentpolicy$ can be formally verified, and the bounds offer a confidence metric allowing to lift the guarantees obtained this way back to the original model $\mdp$, when the latter operates under $\latentpolicy$. We show in the following how to learn a latent space model by optimizing the aforementioned bounds, and distill policies $\policy \in \mpolicies{\mdp}$ obtained via \emph{any} RL technique to a latent policy $\latentpolicy \in \latentmpolicies$.