\smallparagraph{Other related work.}
%~We already mentioned the {DeepMDP} framework \citep{DBLP:conf/icml/GeladaKBNB19} 
%which introduced abstraction quality guarantees for representation learning purposes.
%
Frameworks providing formal guarantees \emph{during the RL process} include the work of \citet{DBLP:conf/tacas/Junges0DTK16}, \emph{Shielded-RL} \citep{DBLP:conf/aaai/AlshiekhBEKNT18,jansen_et_al:LIPIcs:2020:12815}, and \emph{AlwaysSafe} \citep{DBLP:conf/atal/SimaoJS21}.
These all require an abstract model of the safety aspect of the environment. Our approach is complementary in that we assume no prior knowledge and \emph{learn an abstraction}.
Notably, our goal is not the same: they aim at verifying whether the exploration is safe while our goal is to verify policies learned via \emph{any} RL technique.
The \textsc{Mosaic} approach \citep{DBLP:conf/formats/Bacci020} shares ours
% \citet{DBLP:conf/formats/Bacci020} share
%this purpose with ours
in the particular case of verifying deep-RL policies.
%They repeatedly refine an abstraction by partitioning the state space into hyperrectangles, and derive upper probability bounds on failure states.
However, they require (i) the neural network specifying the policy, (ii) the environment to be known and deterministic, and (iii) a formal description of the probability with which faults occur when attempting to execute particular actions.
%
%\todo{F: RNN paper here}
Finally, \citet{DBLP:conf/ijcai/CarrJT20} verify policies represented as recurrent neural networks (RNNs).
Although they require the environment to be discrete as well as a formal model of the environment. The authors discretize the RNN hidden states by using \emph{quantized} autoencoders%
% (another approach to learning discrete latent spaces)
, in the same spirit as our policy distillation.
%, and (iii) the hidden states of the RNN to be discretized (here, via a quantized autoencoder, another approach to learning discrete latent spaces).

VAEs have been used in the context of (model-based) RL to learn latent representations of the unknown environment and train
simpler policies
from the features extracted
%defined over the induced latent observation space
(e.g., \citealt{DBLP:conf/icml/CorneilGB18, DBLP:conf/nips/FreemanHM19, DBLP:conf/nips/LeeNAL20,ALA2021:Burden-et-al}).
In particular, \citet{DBLP:conf/icml/CorneilGB18} focused on learning discrete latent MDPs from continuous-state environments with discrete actions (without guarantees nor distilled policies) to plan via \emph{prioritized sweeping}. % \citep{DBLP:books/lib/SuttonB98}.