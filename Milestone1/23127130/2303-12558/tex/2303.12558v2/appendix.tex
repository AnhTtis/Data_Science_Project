\section{Theoretical Details on WAE-MDPs} \label{appendix:wae-mdp}
\subsection{The Discrepancy Measure}\label{appendix:discrepancy-measure}
We show that reasoning about discrepancy measures between stationary distributions is sound in the context of infinite interaction and episodic RL processes.
Let $\decoder$ be a parameterized behavioral model that generate finite traces from the original environment (i.e., finite sequences of state, actions, and rewards of the form $\tuple{\seq{\state}{T}, \seq{\action}{T - 1}, \seq{\reward}{T - 1}}$), our goal is to find the best parameter $\decoderparameter$ which offers the most accurate reconstruction of the original traces issued from the original model $\mdp$ operating under $\policy$. %, according to the raw transition distance $\tracedistance$.
We demonstrate that, in the limit, considering the OT between trace-based distributions is equivalent to considering the OT between the stationary distribution of $\mdp_{\policy}$ and the one of the behavioral model.
% To do so, we start from trace-based distributions, i.e., distributions 
%Using $\tracedistance$, one can also measure the distance between transitions by considering traces of unit size.
% \begin{equation*}
%     \tracedistance\fun{\trace, \trace'} = \sum_{t = 0}^{T - 1} \distance_\states\fun{\state_{t + 1}, \state'_{t + 1}} + \distance_{\actions}\fun{\action_t, \action'_t} + \left| \reward_t - \reward'_t \right| + \distance_{\labels}\fun{\labeling_{t + 1}, \labeling'_{t + 1}}.
% \end{equation*}
% Then, %given $T \in \N$,
% we are considering optimizing
% \begin{equation*}
%     \wassersteindist{\tracedistance}{\mdp_{\policy}}{\decoder} = \! \sup_{f \in \Lipschf{\tracedistance}} \expectedsymbol{\trace \sim \mdp_{\policy}} f\fun{\seq{\state}{T}, \seq{\action}{T - 1}, \seq{\reward}{T - 1}, \seq{\labeling}{T}} - \!\! \expectedsymbol{\trace \sim \decoder} f\fun{\seq{\state}{T}, \seq{\action}{T - 1}, \seq{\reward}{T - 1}, \seq{\labeling}{T}}. \label{eq:trace-wasserstein}
%     %    \wassersteindist{\tracedistance}{\mdp_{\policy}[T]}{\decoder} = \! \sup_{f \in \Lipschf{\tracedistance}} \expectedsymbol{\trace \sim \mdp_{\policy}[T]} f\fun{\seq{\state}{T}, \seq{\action}{T - 1}, \seq{\reward}{T - 1}, \seq{\labeling}{T}} - \!\! \expectedsymbol{\trace \sim \decoder} f\fun{\seq{\state}{T}, \seq{\action}{T - 1}, \seq{\reward}{T - 1}, \seq{\labeling}{T}}. \label{eq:trace-wasserstein}
% \end{equation*}
% Alternatively, the following Lemmae enable optimizing the Wasserstein distance between the original MDP and the behavioral model %Eq.~\ref{eq:trace-wasserstein}
% when traces are drawn from episodic RL processes or infinite interactions \cite{DBLP:conf/nips/Huang20}.

%When traces are drawn from infi e consider optimizing the distance between transition distributions
Let us first formally recall the definition of the metric on the \emph{transitions} of the MDP.

\smallparagraph{Raw transition distance.}~%
Assume that $\states$, $\actions$, and $\images{\rewards}$ are respectively equipped with metric $\distance_{\states}$, $\distance_{\actions}$, and $\distance_{\rewards}$,
let us define the \emph{raw transition distance metric} over \emph{transitions} of $\mdp$, i.e., tuples of the form $\tuple{\state, \action, \reward, \state'}$, as $\transitiondistance \colon \states \times \actions \times \images{\rewards} \times \states$,
\begin{equation}
    \tracedistance\fun{\tuple{\state_1, \action_1, \reward_1, \state'_1}, \tuple{\state_2, \action_2, \reward_2, \state'_2}} = \distance_\states\fun{\state_1, \state_2} + \distance_{\actions}\fun{\action_1, \action_2} + \distance_{\rewards}\fun{\reward_1, \reward_2} + \distance_{\states}\fun{\state_1', \state'_2}.\notag
\end{equation}
In a nutshell, $\transitiondistance$ consists of the sum of the distance of all the transition components.
Note that it is a well defined distance metric since the sum of distances preserves the identity of indiscernible, symmetry, and triangle inequality. 

\smallparagraph{Trace-based distributions.}~%Instead of reasoning over full traces generated from $\mdp_\policy$, we enable a transition-based optimization by considering the distribution over transitions which may occur along traces of size $T$:
The raw distance $\tracedistance$ allows to reason about \emph{transitions}, we thus consider the distribution over \emph{transitions which occur along traces of length $T$} to compare the dynamics of the original and behavioral models:
%We consider the following distributions over transitions which may occur along trajectories of length $T$:
\begin{align*}
    \mathcal{D}_\policy\left[ T \right]\fun{\state, \action, \reward, \state'} &= \frac{1}{T} \sum_{t = 1}^{T} \stationary{\policy}^t\fun{\state \mid \sinit} \cdot \policy\fun{\action \mid \state} \cdot \probtransitions\fun{\state' \mid \state, \action} \cdot \condition{\reward=\rewards\fun{\state, \action}}, \text{ and} \\
    \mathcal{P}_\decoderparameter[T]\fun{\state, \action, \reward, \state'} &= \frac{1}{T} \sum_{t = 1}^{T} \expectedsymbol{\seq{\state}{t}, \seq{\action}{t - 1}, \seq{\reward}{t - 1} \sim \decoder[t]}{\condition{\tuple{\indexof{\state}{t - 1}, \indexof{\action}{t - 1}\indexof{\reward}{t - 1}, \indexof{\state}{t}}= \tuple{\state, \action, \reward, \state'}}},
\end{align*} 
where $\decoder[T]$ denotes the distribution over traces of length $T$, generated from $\decoder$.
Intuitively, $\nicefrac{1}{T} \cdot \sum_{t = 1}^{T} \stationary{\policy}^t\fun{\state \mid \sinit}$
%gives the probability of visiting each particular state $\state$ of $\mdp_\policy$
can be seen as the fraction of the time spent in $\state$
along traces of length $T$, starting from the initial state \citeAR{10.5555/280952}.
Therefore, drawing $\tuple{\state, \action, \reward, \state'} \sim \mathcal{D}_\policy\left[ T \right]$ trivially follows: % from the transition function of $\mdp_\policy$:
it is equivalent to drawing $\state$ from $\nicefrac{1}{T} \cdot \sum_{t = 1}^{T} \stationary{\policy}^t\fun{\cdot \mid \sinit}$, then respectively $\action$ and $\state'$ from $\policy\fun{\cdot \mid \state}$ and $\probtransitions\fun{\cdot \mid \state, \action}$, to finally obtain $\reward = \rewards\fun{\state, \action}$. % and $\labeling' = \labels\fun{\state'}$.
% We similarly define the transition distribution of the behavioral model as:
Given $T \in \N$, our objective is to minimize the Wasserstein distance between those distributions:
%\begin{equation}
%    \min_{\decoderparameter} \, 
$\wassersteindist{\tracedistance}{\mathcal{D}_{\policy}[T]}{\mathcal{P}_{\decoderparameter}[T]}$.
%\end{equation}
%
The following Lemma enables optimizing the Wasserstein distance between the original MDP and the behavioral model %Eq.~\ref{eq:trace-wasserstein}
when traces are drawn from episodic RL processes or infinite interactions \citep{DBLP:conf/nips/Huang20}.
% (see \citealt{DBLP:conf/nips/Huang20} for a discussion of this RL setting).
% (see \cite{DBLP:conf/nips/Huang20} for a discussion of this RL setting).

\begin{lemma}\label{lemma:wasserstein-transition-limit}
Assume the existence of a stationary behavioral model $\stationarydecoder = \lim_{T \to \infty} \mathcal{P}_{\decoderparameter}[T]$, then
\begin{equation*}
    \lim_{T \to \infty} \wassersteindist{\tracedistance}{\mathcal{D}_{\policy}[T]}{\mathcal{P}_{\decoderparameter}[T]} = \wassersteindist{\tracedistance}{\stationary{\policy}}{\stationarydecoder}.
\end{equation*}
\end{lemma}
\begin{proof}
First, note that $\nicefrac{1}{T} \cdot \sum_{t = 1}^T \stationary{\policy}^t\fun{\cdot \mid \sinit}$ weakly converges to $ \stationary{\policy}$ as $T$  goes to $\infty$ \citeAR{10.5555/280952}. The result follows then from \citep[Corollary~6.9]{Villani2009}.
\end{proof}
% \begin{proof}
%     The result follows from (i) $\lim_{T \to \infty}\nicefrac{1}{T} \cdot \sum_{t = 1}^T \stationary{\policy}^t\fun{\cdot \mid \sinit} = \stationary{\policy}$ \citep{10.5555/280952} and (ii) \citep[Corollary~6.9]{Villani2009}.
% \end{proof}

\subsection{Dealing with Discrete Actions}\label{appendix:discrete-action-space}
When the policy $\policy$ executed in $\mdp$ already produces discrete actions, learning a latent action space is, in many cases, not necessary.
We thus make the following assumptions:
%
%\begin{remark}
% Let $\policy \colon \states \to \distributions{\actions^{\star}}$ be the policy executed in the environment and assume that $\actions^{\star}$ is a finite set.
% Then, one can set $\latentactions = \actions^{\star}$.
% In that case, learning $\embed^{\actions}_{\encoderparameter}$ and $\embeda_{\decoderparameter}$ is not necessary and setting them to the identity function is sufficient.
% Concretely, %$\policy$ produces discrete actions
% this situation occurs
% when (i) $\actions^{\star}$ is the action space of $\mdp$, or (ii) $\policy$ is a latent policy (e.g., when $\policy = \latentpolicy_{\decoderparameter}$).
%\end{remark}
%
\begin{assumption} \label{assumption:action-encoder}
Let $\policy \colon \states \to \distributions{\actions^{\star}}$ be the policy executed in $\mdp$ and assume that $\actions^{\star}$ is a (tractable) finite set.
Then, we take $\latentactions=\actions^{\star}$ and $\actionencoder$ as the 
%deterministic distribution with identity Dirac function, 
identity function,
i.e., $\actionencoder \colon \latentstates \times \actions^{\star} \to \actions^{\star}, \, \tuple{\latentstate, \action^{\star}} \mapsto \action^{\star}$.
\end{assumption}
\begin{assumption} \label{assumption:action-decoder}
Assume that the action space of the original environment $\mdp$ is a (tractable) finite set.
Then, we take $\embeda_{\decoderparameter}$ as the identity function, i.e., $\embeda_{\decoderparameter} = \actionencoder$.
\end{assumption}
Concretely, the premise of Assumption~\ref{assumption:action-encoder} typically occurs when $\policy$ is a latent policy (see Rem.~\ref{rmk:latent-policy-execution}) \emph{or} when $\mdp$ has already a discrete action space.
In the latter case, Assumption~\ref{assumption:action-encoder} and~\ref{assumption:action-decoder} amount to setting $\latentactions = \actions$ and ignoring the action encoder and embedding function.
Note that if a discrete action space is too large, or if the user explicitly aims for a coarser space, then the former is not considered as tractable, these assumptions do not hold, and the action space is abstracted to a smaller set of discrete actions.
%\begin{remark}
	%Note that abstracting a (too large) discrete set of actions is still feasible to obtain a tractable latent action space, by ignoring Assumption~\ref{}
%\end{remark}

\subsection{Proof of Lemma~\ref{lem:regularizer-upper-bound}}
\smallparagraph{Notation.}~%
From now on, we write $\embed_{\encoderparameter}\fun{\latentstate, \latentaction \mid \state, \action} = \condition{\embed_\encoderparameter\fun{\state}=\latentstate} \cdot \actionencoder\fun{\latentaction \mid \latentstate, \action}$.

%Let us recall the Lemma:

\regularizerlemma*
\begin{proof}
Wasserstein is compliant with the triangular inequality \citep{Villani2009}, which gives us:
\begin{align*}
    \wassersteindist{\transitiondistance}{\encoder}{ \latentstationaryprior}
    \leq \wassersteindist{\transitiondistance}{\encoder}{\originaltolatentstationary{}} + \wassersteindist{\distance_{\latentstates}}{\originaltolatentstationary{}}{ \latentstationaryprior},
\end{align*}
where
\begin{align}
&\wassersteindist{\transitiondistance}{\originaltolatentstationary{}}{ \latentstationaryprior} \tag{note that $\wassersteinsymbol{\tracedistance}$ is reflexive \citep{Villani2009}} \\
=& \sup_{f \in \Lipschf{\transitiondistance}} \expectedsymbol{\state, \action \sim \stationary{\policy}}\expectedsymbol{\latentstate, \latentaction \sim \embed_{\encoderparameter}\fun{\sampledot \mid \state, \action}}\expectedsymbol{\latentstate' \sim \latentprobtransitions_{\decoderparameter}\fun{\sampledot \mid \latentstate, \latentaction}} f\fun{\latentstate, \latentaction, \latentstate'}
- \expectedsymbol{\latentstate \sim \latentstationaryprior}\expectedsymbol{\latentaction \sim \latentpolicy_{\decoderparameter}\fun{\sampledot \mid \latentstate}}\expectedsymbol{\latentstate' \sim \latentprobtransitions_{\decoderparameter}\fun{\sampledot \mid \latentstate, \latentaction}} f\fun{\latentstate, \latentaction, \latentstate'}\text{, and} \notag \\[.5em]
& \wassersteindist{\transitiondistance}{\encoder}{\originaltolatentstationary{}} \notag \\
=&
    \sup_{f \in \Lipschf{\transitiondistance}} \expectedsymbol{\state, \action, \state' \sim \stationary{\policy}}\expectedsymbol{\latentstate, \latentaction, \latentstate' \sim \embed_{\encoderparameter}\fun{\sampledot \mid \state, \action, \state'}} f\fun{\latentstate, \latentaction, \latentstate'} - \expectedsymbol{\state, \action \sim \stationary{\policy}}\expectedsymbol{\latentstate, \latentaction \sim \embed_{\encoderparameter}\fun{\sampledot \mid \state, \action}}\expectedsymbol{\latentstate' \sim \latentprobtransitions_{\decoderparameter}\fun{\sampledot \mid \latentstate, \latentaction}} f\fun{\latentstate, \latentaction, \latentstate'} \label{eq:proof-lemma-wdist-triangular-inequality-1} \\
    \leq& \expectedsymbol{\state, \action \sim \stationary{\policy}}\expectedsymbol{\latentstate, \latentaction \sim \embed_{\encoderparameter}\fun{\sampledot \mid \state, \action}} \; \sup_{f \in \Lipschf{\transitiondistance}} \expectedsymbol{\state' \sim \probtransitions\fun{\sampledot \mid \state, \action}} f\fun{\latentstate, \latentaction, \embed_{\encoderparameter}\fun{\state'}} - \expectedsymbol{\latentstate' \sim \latentprobtransitions_{\decoderparameter}\fun{\sampledot \mid \latentstate, \latentaction}} f\fun{\latentstate, \latentaction, \latentstate'} \label{eq:proof-lemma-wdist-triangular-inequality-2} \\
    =& \expectedsymbol{\state, \action \sim \stationary{\policy}}\expectedsymbol{\latentaction \sim \embed_{\encoderparameter}^{\scriptscriptstyle \actions}\fun{\sampledot \mid \embed_{\encoderparameter}\fun{\state}, \action}} \; \sup_{f \in \Lipschf{\distance_{\latentstates}}} \expectedsymbol{\latentstate' \sim \embed_{\encoderparameter}\probtransitions\fun{\sampledot \mid \state, \action}} f\fun{\latentstate'} - \expectedsymbol{\latentstate' \sim \latentprobtransitions_{\decoderparameter}\fun{\sampledot \mid \embed_{\encoderparameter}\fun{\state}, \latentaction}} f\fun{\latentstate'} \label{eq:proof-lemma-wdist-triangular-inequality-3} \\
    =& \expectedsymbol{\state, \action \sim \stationary{\policy}}\expectedsymbol{\latentaction \sim \embed_{\encoderparameter}^{\scriptscriptstyle \actions}\fun{\sampledot \mid \embed_{\encoderparameter}\fun{\state}, \action}} \wassersteindist{\distance_{\latentstates}}{\embed_{\encoderparameter}\probtransitions\fun{\sampledot \mid \state, \action}}{\latentprobtransitions_{\decoderparameter}\fun{\sampledot \mid \embed_{\encoderparameter}\fun{\state}, \latentaction}}. \notag
\end{align}
We pass from Eq.~\ref{eq:proof-lemma-wdist-triangular-inequality-1} to Eq.~\ref{eq:proof-lemma-wdist-triangular-inequality-2} by the Jensen's inequality.
To see how we pass from Eq.~\ref{eq:proof-lemma-wdist-triangular-inequality-2} to Eq.~\ref{eq:proof-lemma-wdist-triangular-inequality-3}, notice
%in the supremum over $\Lipschf{\distance_{\latentstates}}$ in Eq.~\ref{eq:proof-lemma-wdist-triangular-inequality-2}
that
\begin{align*}
    \Lipschf{\transitiondistance} &= \left\{f \colon f\fun{\latentstate_1, \latentaction_1, \latentstate_1'} - f\fun{\latentstate_2, \latentaction_2, \latentstate_2'} \leq \transitiondistance\fun{\tuple{\latentstate_1, \latentaction_1, \latentstate_1'}, \tuple{\latentstate_2, \latentaction_2, \latentstate_2'}}\right\} \\
    \Lipschf{\transitiondistance} &= \set{f \colon f\fun{\latentstate_1, \latentaction_1, \latentstate_1'} - f\fun{\latentstate_2, \latentaction_2, \latentstate_2'} \leq \distance_{\latentstates}\fun{\latentstate_1, \latentstate_2} + \distance_{\latentactions}\fun{\latentaction_1, \latentaction_2} + \distance_{\latentstates}\fun{\latentstate_1', \latentstate_2'}}
\end{align*}
Observe now that $\latentstate$ and $\latentaction$ are fixed in the supremum computation of Eq.~\ref{eq:proof-lemma-wdist-triangular-inequality-2}: all functions $f$ considered and taken from $\Lipschf{\transitiondistance}\,$ are of the form $f\fun{\latentstate, \latentaction, \sampledot}$.
It is thus sufficient to consider the supremum over functions from the following subset of $\Lipschf{\transitiondistance}\,$:
\begin{align*}
    &\set{f \colon f\fun{\latentstate, \latentaction, \latentstate_1'} - f\fun{\latentstate, \latentaction, \latentstate_2'} \leq \distance_{\latentstates}\fun{\latentstate, \latentstate} + \distance_{\latentactions}\fun{\latentaction, \latentaction} +  \distance_{\latentstates}\fun{\latentstate_1', \latentstate_2'}} \\ \tag{for $\latentstate$, $\latentaction$ drawn from $\embed_{\encoderparameter}$} \\
    =&\set{f \colon f\fun{\latentstate, \latentaction, \latentstate_1'} - f\fun{\latentstate, \latentaction, \latentstate_2'} \leq \distance_{\latentstates}\fun{\latentstate_1', \latentstate_2'}}\\
    =&\set{f \colon f\fun{\latentstate_1'} - f\fun{\latentstate_2'} \leq \distance_{\latentstates}\fun{\latentstate_1', \latentstate_2'}} \\
    =& \Lipschf{\distance_{\latentstates}}.
\end{align*}
Given a state $\state \in \states$ in the original model, the (parallel) execution of $\policy$ in $\latentmdp_{\decoderparameter}$ is enabled through $\policy\fun{\action, \latentaction \mid \state} = \policy\fun{\action \mid \state} \cdot \actionencoder\fun{\latentaction \mid \embed_{\encoderparameter}\fun{\state}, \action}$ (cf. Fig.~\ref{subfig:latent-fow-distillation}).
The local transition loss resulting from this interaction is:
\begin{align*}
\localtransitionloss{\stationary{\policy}} &= \expectedsymbol{\state, \tuple{\action, \latentaction} \sim \stationary{\policy}} \wassersteindist{\distance_{\latentstates}}{\embed_{\encoderparameter}\probtransitions\fun{\sampledot \mid \state, \action}}{\latentprobtransitions\fun{\sampledot \mid \embed_{\encoderparameter}\fun{\state}, \latentaction}}\\
&= \expectedsymbol{\state, \action \sim \stationary{\policy}}\expectedsymbol{\latentaction \sim \embed_{\encoderparameter}^{\scriptscriptstyle \actions}\fun{\sampledot \mid \embed_{\encoderparameter}\fun{\state}, \action}} \wassersteindist{\distance_{\latentstates}}{\embed_{\encoderparameter}\probtransitions\fun{\sampledot \mid \state, \action}}{\latentprobtransitions_{\decoderparameter}\fun{\sampledot \mid \embed_{\encoderparameter}\fun{\state}, \latentaction}},
\end{align*}
which finally yields the result.
\end{proof}

\subsection{Proof of Theorem~\ref{thm:latent-execution-objective}}
Before proving Theorem~\ref{thm:latent-execution-objective}, let us introduce the following Lemma, that explicitly demonstrates the link between the transition regularizer of the \waemdp objective and the local transition loss required to obtain the guarantees related to the bisimulation bounds of Eq.~\ref{eq:bidistance-bound}.

\begin{lemma} \label{lem:regularizer-to-local-transition-loss}
% If $\embed_{\encoderparameter}$ is deterministic and traces are generated by running $\latentpolicy_{\decoderparameter}$ in the environment, then
Assume that traces are generated by running $\latentpolicy \in \latentpolicies$ in the original environment, then
\begin{align*}
     \expectedsymbol{\state, \action^\star \sim \stationary{\latentpolicy}}\expectedsymbol{\latentaction \sim \embed_{\encoderparameter}^{\actions}\fun{\sampledot \mid \embed_{\encoderparameter}\fun{\state}, \action^{\star}}} \wassersteindist{\distance_{\latentstates}}{\embed_{\encoderparameter}\probtransitions\fun{\sampledot \mid \state, \action^{\star}}}{\latentprobtransitions_{\decoderparameter}\fun{\sampledot \mid \embed_{\encoderparameter}\fun{\state}, \latentaction}} = \localtransitionloss{\stationary{\latentpolicy}}.
\end{align*}
\end{lemma}
\begin{proof}
Since the latent policy $\latentpolicy$ generates latent actions, Assumption~\ref{assumption:action-encoder} holds, which means:
\begin{align*}
     & \expectedsymbol{\state, \action^{\star} \sim \stationary{\latentpolicy}}\expectedsymbol{\latentaction \sim \embed_{\encoderparameter}^{\actions}\fun{\sampledot \mid \embed_{\encoderparameter}\fun{\state}, \action^{\star}}} \wassersteindist{\distance_{\latentstates}}{\embed_{\encoderparameter}\probtransitions\fun{\sampledot \mid \state, \action^{\star}}}{\latentprobtransitions_{\decoderparameter}\fun{\sampledot \mid \embed_{\encoderparameter}\fun{\state}, \latentaction}}\\
     =& \expectedsymbol{\state, \latentaction \sim \stationary{\latentpolicy}} \wassersteindist{\distance_{\latentstates}}{\embed_{\encoderparameter}\probtransitions\fun{\sampledot \mid \state, \latentaction}}{\latentprobtransitions_{\decoderparameter}\fun{\sampledot \mid \embed_{\encoderparameter}\fun{\state}, \latentaction}}\\
     =& \, \localtransitionloss{\stationary{\latentpolicy}}.
\end{align*}
\end{proof}

\latentexecutionobjective*

\begin{proof}
%When the original MDP $\mdp$ with action space $\actions$ operates under a latent policy $\latentpolicy \colon \latentstates \to \distributions{\latentactions}$ while its original action space is continuous (i.e., $\actions \neq \latentactions$; Assumption~\ref{assumption:action-encoder} holds while Assumption~\ref{assumption:action-decoder} does not), then each latent action produced is embedded back to the original action space via $\embeda_{\decoderparameter}$, which allows to execute it in $\mdp$ (see Remark~\ref{rmk:latent-policy-execution}).
%Therefore, the projection of $G_{\decoderparameter}$ on the action space is given by $\distance_{\actions}\fun{\embeda_{ \decoderparameter}\fun{\latentstate, \latentaction}, \embeda_{\decoderparameter}\fun{\latentstate, \latentaction}}$.
We distinguish two cases: (i) the case where the original and latent models share the same discrete action space, i.e., $\actions = \latentactions$, and (ii) the case where the two have a different action space (e.g., when the original action space is continuous), i.e., $\actions \neq \latentactions$.
In both cases, the local losses term follows by definition of $\localrewardloss{\stationary{\latentpolicy}}$ and Lemma~\ref{lem:regularizer-to-local-transition-loss}.
When $\distance_{\rewards}$ is the Euclidean distance (or even the $L_1$ distance since rewards are scalar values), the expected reward distance occurring in the expected trace-distance term $\tracedistance$ in the \waemdp objective directly translates to the local loss $\localrewardloss{\stationary{\latentpolicy}}$.
%\begin{align*}
% &\expectedsymbol{}_{\substack{\state, \action, \reward, \state' \sim \stationary{\policy} \\ \latentstate, \latentaction, \latentstate' \sim \embed_{\encoderparameter}\fun{\sampledot \mid \state, \action, \state'}}}
%\tracedistance\fun{\tuple{\state, \action, \reward, \state'}, G_{\decoderparameter}\fun{\latentstate, \latentaction, \latentstate'}} \\
%&= \expectedsymbol{}_{\substack{\state, \action, \reward, \state' \sim \stationary{\policy} \\ \latentstate, \latentaction, \latentstate' \sim \embed_{\encoderparameter}\fun{\sampledot \mid \state, \action, \state'}}}
%\left[
%\distance_\states\fun{\state_1, \state_2} + \distance_{\actions}\fun{\action_1, \action_2} + \distance_{\rewards}\fun{\reward_1, \reward_2} + \distance_{\states}\fun{\state_1', \state'_2}
%\right]
%\end{align*}
Concerning the local transition loss, in case~(i), the result naturally follows from  Assumption~\ref{assumption:action-encoder} and~\ref{assumption:action-decoder}.
In case~(ii), only Assumption~\ref{assumption:action-encoder} holds, meaning the action encoder term of the \waemdp objective is ignored, but not the action embedding term appearing in $G_{\decoderparameter}$.
Given $\state \sim \stationary{\latentpolicy}$, recall that executing $\latentpolicy$ in $\mdp$ amounts to embedding the produced latent actions $\latentaction \sim \latentpolicy\fun{\sampledot \mid \embed_{\encoderparameter}\fun{\state}}$ back to the original environment via $\action = \embeda_{\decoderparameter}\fun{\embed_{\encoderparameter}\fun{\state}, \latentaction}$ (cf. Rem.~\ref{rmk:latent-policy-execution} and Fig.~\ref{subfig:latent-flow-guarantees}).
Therefore, the projection of $\tracedistance\fun{\tuple{\state, \action, \reward, \state'}, G_{\decoderparameter}\fun{\embed_{\encoderparameter}\fun{\state}, \latentaction, \embed_{\encoderparameter}\fun{\state'}}}$ on the action space $\actions$ is $\distance_{\actions}\fun{\embeda_{\decoderparameter}\fun{\embed_{\encoderparameter}\fun{\state}, \latentaction}, \embeda_{\decoderparameter}\fun{\embed_{\encoderparameter}\fun{\state}, \latentaction}} = 0$, for $\reward = \rewards\fun{\state, \action}$ and $\state' \sim \probtransitions\fun{\sampledot \mid \state, \action}$.
\end{proof}

\subsection{Optimizing the Transition Regularizer}\label{appendix:tractable-transition-regularizer}
In the following, we detail how we derive a tractable form of %the expectation over the OT dual form of
our transition regularizer $\localtransitionloss{\stationary{\policy}}\fun{\wassersteinparameter}$.
%The optimization of the transition regularizer $\localtransitionloss{\stationary{\policy}}$ is enabled through
Optimizing the ground Kantorovich-Rubinstein duality is enabled via the introduction of a parameterized, $1$-Lipschitz network 
$\transitionlossnetwork$, that need to be trained to attain the supremum of the dual:
\begin{align*}
\localtransitionloss{\stationary{\policy}}\fun{\wassersteinparameter} &= \expectedsymbol{\state, \action \sim \stationary{\policy}}\expectedsymbol{\latentstate, \latentaction \sim \embed_{\encoderparameter}\fun{\sampledot \mid \state, \action}} \; \max_{\wassersteinparameter \colon \transitionlossnetwork \in \Lipschf{\distance_{\latentstates}}} \; \expectedsymbol{\latentstate' \sim \embed_{\encoderparameter}\probtransitions\fun{\sampledot \mid \state, \action}} \transitionlossnetwork\fun{\latentstate'} - \expectedsymbol{\latentstate' \sim \latentprobtransitions_{\decoderparameter}\fun{\sampledot \mid \latentstate, \latentaction}} \transitionlossnetwork\fun{\latentstate'}. %\label{eq:transition-regularizer}
%=& \expectedsymbol{\state, \action \sim \stationary{\policy}}\expectedsymbol{\latentaction \sim \embed_{\encoderparameter}^{\scriptscriptstyle \actions}\fun{\sampledot \mid \embed_{\encoderparameter}\fun{\state}, \action}} \wassersteindist{\distance_{\latentstates}}{\embed_{\encoderparameter}\probtransitions\fun{\sampledot \mid \state, \action}}{\latentprobtransitions_{\decoderparameter}\fun{\sampledot \mid \embed_{\encoderparameter}\fun{\state}, \latentaction}}. \notag
\end{align*}
Under this form, optimizing $\localtransitionloss{\stationary{\policy}}\fun{\wassersteinparameter}$ is intractable due to the expectation over the maximum. 
%We rewrite the objective to make the optimization tractable through Monte Carlo estimation thanks to the following Lemma.
The following Lemma allows us rewriting $\localtransitionloss{\stationary{\policy}}$ to make the optimization tractable through Monte Carlo estimation.
\begin{lemma}
Let $\measurableset, \varmeasurableset$ be two measurable sets, $\stationary{} \in \distributions{\measurableset}$, $P \colon \measurableset \to \distributions{\varmeasurableset}, Q \colon \measurableset \to \distributions{\varmeasurableset}$, and $\distance \colon \varmeasurableset \times \varmeasurableset \to \mathopen[ 0, + \infty \mathclose[$ be a metric on $\varmeasurableset$. Then,
\begin{align*}
    &\expectedsymbol{x \sim \stationary{}}{\wassersteindist{\distance}{P\fun{\sampledot \mid x}}{Q\fun{\sampledot \mid x}}} = \sup_{\varphi \colon \measurableset \to \Lipschf{\distance}} \expected{x \sim \stationary{}}{\expectedsymbol{y_1 \sim P\fun{\sampledot \mid x}} \varphi\fun{x}\fun{y_1} - \expectedsymbol{y_2 \sim Q\fun{\sampledot \mid x}} \varphi\fun{x}\fun{y_2}}
\end{align*}
\begin{proof}
Our objective is to show that 
\begin{align}
    &\expected{x \sim \stationary{}}{\sup_{f \in \Lipschf{\distance}} \expectedsymbol{y_1 \sim P\fun{\sampledot \mid x}}\varphi\fun{y_1}\fun{x} - \expectedsymbol{y_2 \sim Q\fun{\sampledot \mid x}} \varphi\fun{y_2}\fun{x}} \label{eq:expected-cond-wasserstein}\\
    =& \sup_{\varphi \colon \measurableset \to \Lipschf{\distance}} \expected{x \sim \stationary{}}{\expectedsymbol{y_1 \sim P\fun{\sampledot \mid x}} \varphi\fun{x}\fun{y_1} - \expectedsymbol{y_2 \sim Q\fun{\sampledot \mid x}} \varphi\fun{x}\fun{y_2}} \label{eq:sup-expected-cond-wasserstein}
\end{align}
We start with (\ref{eq:expected-cond-wasserstein})~$\leq$~(\ref{eq:sup-expected-cond-wasserstein}). Construct $\varphi^\star \colon \measurableset \to \Lipschf{\distance}$ by setting for all $x \in \measurableset$
\begin{equation*}
    \varphi^\star\fun{x} = \arg\sup_{f \in \Lipschf{\distance}} \expectedsymbol{y_1 \sim P\fun{\sampledot \mid x}} f\fun{y_1} - \expectedsymbol{y_2 \sim Q\fun{\sampledot \mid x}} f\fun{y_2}.
\end{equation*}
This gives us
\begin{align*}
    &\expected{x \sim \stationary{}}{\sup_{f \in \Lipschf{\distance}} \expectedsymbol{y_1 \sim P\fun{\sampledot \mid x}} f\fun{y_1} - \expectedsymbol{y_2 \sim Q\fun{\sampledot \mid x}} f\fun{y_2}} \\
    =& \expected{x \sim \stationary{}}{\expectedsymbol{y_1 \sim P\fun{\sampledot \mid x}} \varphi^{\star}\fun{x}\fun{y_1} - \expectedsymbol{y_2 \sim Q\fun{\sampledot \mid x}} \varphi^{\star}\fun{x}\fun{y_2}} \\
    \leq& \sup_{\varphi \colon \measurableset \to \Lipschf{\distance}} \expected{x \sim \stationary{}}{\expectedsymbol{y_1 \sim P\fun{\sampledot \mid x}} \varphi\fun{x}\fun{y_1} - \expectedsymbol{y_2 \sim Q\fun{\sampledot \mid x}} \varphi\fun{x}\fun{y_2}}.
\end{align*}
It remains to show that (\ref{eq:expected-cond-wasserstein})~$\geq$~(\ref{eq:sup-expected-cond-wasserstein}).
Take
\[
\varphi^{\star} = \arg\sup_{\varphi \colon \measurableset \to \Lipschf{\distance}} \expected{x \sim \stationary{}}{\expectedsymbol{y_1 \sim P\fun{\sampledot \mid x}} \varphi\fun{x}\fun{y_1} - \expectedsymbol{y_2 \sim Q\fun{\sampledot \mid x}} \varphi\fun{x}\fun{y_2}}.
\]
Then, for all $x \in \measurableset$, we have $\varphi^{\star}\fun{x} \in \Lipschf{\distance}$ which means:
\begin{align*}
    & \expectedsymbol{y_1 \sim P\fun{\sampledot \mid x}} \varphi^{\star}\fun{x}\fun{y_1} - \expectedsymbol{y_2 \sim Q\fun{\sampledot \mid x}} \varphi^{\star}\fun{x}\fun{y_2} \\
    \leq & \sup_{f \in \Lipschf{\distance}} \expectedsymbol{y_1 \sim P\fun{\sampledot \mid x}} f\fun{y_1} - \expectedsymbol{y_2 \sim Q\fun{\sampledot \mid x}} f\fun{y_2}
\end{align*}
This finally yields
\begin{align*}
    &\expected{x \sim \stationary{}}{\expectedsymbol{y_1 \sim P\fun{\sampledot \mid x}} \varphi^{\star}\fun{x}\fun{y_1} - \expectedsymbol{y_2 \sim Q\fun{\sampledot \mid x}} \varphi^{\star}\fun{x}\fun{y_2}} \\
    \leq& \expected{x \sim \stationary{}}{\sup_{f \in \Lipschf{\distance}} \expectedsymbol{y_1 \sim P\fun{\sampledot \mid x}} f\fun{y_1} - \expectedsymbol{y_2 \sim Q\fun{\sampledot \mid x}} f\fun{y_2}}.
\end{align*}
\end{proof}
\end{lemma}
\begin{corollary}\label{cor:min-sup-regularizer}
Let  $\stationary{\policy}$ be a stationary distribution of $\mdp_\policy$ and $\measurableset = \states \times \actions \times \latentstates \times \latentactions$, then
%   \begin{align*} 
%       & \expectedsymbol{\state, \action \sim \stationary{\policy}} \expectedsymbol{\latentstate \sim \embed_{\encoderparameter}\fun{\sampledot \mid \state}} \expectedsymbol{\latentaction \sim \embed^{\actions}_{\encoderparameter}\fun{\sampledot \mid \latentstate, \action}} \wassersteindist{\distance_{\latentstates}}{\embed_{\encoderparameter}\probtransitions\fun{\sampledot \mid \state, \action}}{\latentprobtransitions_{\decoderparameter}\fun{\sampledot \mid \latentstate, \latentaction}} \\ 
%       =& \sup_{\varphi \colon \measurableset \to \Lipschf{\distance_{\latentstates}}} \, \expectedsymbol{\state, \action, \state' \sim \stationary{\policy}}\expectedsymbol{\latentstate \sim \embed_{\encoderparameter}\fun{\sampledot \mid \state}} \expected{\latentaction \sim \embed^{\actions}_{\encoderparameter}\fun{\sampledot \mid \latentstate, \action}}{\expectedsymbol{\latentstate' \sim \embed_{\encoderparameter}\fun{\sampledot \mid \state'}}\varphi\fun{\state, \action, \latentstate, \latentaction}\fun{\latentstate'} - \expectedsymbol{\latentstate' \sim \latentprobtransitions_{\decoderparameter}\fun{\sampledot \mid \latentstate, \action}} \varphi\fun{\state, \action, \latentstate, \latentaction}\fun{\latentstate'}}
%   \end{align*}
\begin{align*}
\localtransitionloss{\stationary{\policy}} = \sup_{\varphi \colon \measurableset \to \scriptscriptstyle \Lipschf{\distance_{\latentstates}}} \expectedsymbol{\state, \action, \state' \sim \stationary{\policy}} \expected{\latentstate, \latentaction \sim \embed_{\encoderparameter}\fun{\sampledot \mid \state, \action}}{\varphi\fun{\state, \action, \latentstate, \latentaction}\fun{\embed_{\encoderparameter}\fun{\state'}} - \expectedsymbol{\latentstate' \sim \latentprobtransitions_{\decoderparameter}\fun{\sampledot \mid \latentstate, \action}}{\varphi\fun{\state, \action, \latentstate, \latentaction}}\fun{\latentstate'}}
\end{align*}
\end{corollary}
\iffalse
\begin{proof}
Notice that 
\begin{align*}
    & \expectedsymbol{\state, \action \sim \stationary{\policy}} \wassersteindist{\distance_{\latentstates}}{\embed_{\encoderparameter}\probtransitions\fun{\sampledot \mid \state, \action}}{\latentprobtransitions_{\decoderparameter}\fun{\sampledot \mid \latentstate, \latentaction}} \\ 
    =& \expectedsymbol{\state, \action \sim \stationary{\policy}}{\sup_{f \in \Lipschf{\distance_{\latentstates}}} \left\{  \expectedsymbol{\state ' \sim \probtransitions\fun{\sampledot \mid \state, \action}}\expectedsymbol{\latentstate' \sim \embed_{\encoderparameter}\fun{\sampledot \mid \state'}} f\fun{\latentstate'} - \expectedsymbol{\latentstate \sim \embed_{\encoderparameter}\fun{\sampledot\mid \state}}  \expectedsymbol{\latentstate' \sim \latentprobtransitions_{\decoderparameter}\fun{\sampledot \mid \latentstate, \action}} f\fun{\latentstate'}\right\}}, \tag{e.g., \citealt{Villani2009}}
\end{align*}
our objective is therefore to show that
\begin{align}
    & \expectedsymbol{\state, \action \sim \stationary{\policy}}{\sup_{f \in \Lipschf{\distance_{\latentstates}}} \left\{  \expectedsymbol{\state ' \sim \probtransitions\fun{\sampledot \mid \state, \action}}\expectedsymbol{\latentstate' \sim \embed_{\encoderparameter}\fun{\sampledot \mid \state'}} f\fun{\latentstate'} - \expectedsymbol{\latentstate \sim \embed_{\encoderparameter}\fun{\sampledot\mid \state}}  \expectedsymbol{\latentstate' \sim \latentprobtransitions_{\decoderparameter}\fun{\sampledot \mid \latentstate, \action}} f\fun{\latentstate'}\right\}} \label{eq:wae-mdp-loss-proof-1} \\
    =& \sup_{\varphi \colon \states \times \actions \to \Lipschf{\distance_{\latentstates}}} \, \expected{\state, \action, \state' \sim \stationary{\policy}}{\expectedsymbol{\latentstate' \sim \embed_{\encoderparameter}\fun{\sampledot \mid \state'}}\varphi\tuple{\state, \action}\fun{\latentstate'} - \expectedsymbol{\latentstate \sim \embed_{\encoderparameter}\fun{\sampledot\mid \state}}  \expectedsymbol{\latentstate' \sim \latentprobtransitions_{\decoderparameter}\fun{\sampledot \mid \latentstate, \action}} \varphi\tuple{\state, \action}\fun{\latentstate'}}. \label{eq:wae-mdp-loss-proof-2}
\end{align}
We start with the $(\ref{eq:wae-mdp-loss-proof-1}) \leq (\ref{eq:wae-mdp-loss-proof-2})$ direction. For all $\state \in \states, \action \in \actions$, construct $\varphi^{\star} \colon \states \times \actions \to \Lipschf{\distance_{\latentstates}}$ by setting 
\begin{align*}
    \varphi^\star{\tuple{\state, \action}} = \arg\sup_{f \in \Lipschf{\distance_{\latentstates}}} \left[  \expectedsymbol{\state ' \sim \probtransitions\fun{\sampledot \mid \state, \action}}\expectedsymbol{\latentstate' \sim \embed_{\encoderparameter}\fun{\sampledot \mid \state'}} f\fun{\latentstate'} - \expectedsymbol{\latentstate \sim \embed_{\encoderparameter}\fun{\sampledot\mid \state}}  \expectedsymbol{\latentstate' \sim \latentprobtransitions_{\decoderparameter}\fun{\sampledot \mid \latentstate, \action}} f\fun{\latentstate'}\right].
\end{align*}
This gives us
\begin{align*}
    & \expectedsymbol{\state, \action \sim \stationary{\policy}}{\sup_{f \in \Lipschf{\distance_{\latentstates}}} \left\{  \expectedsymbol{\state ' \sim \probtransitions\fun{\sampledot \mid \state, \action}}\expectedsymbol{\latentstate' \sim \embed_{\encoderparameter}\fun{\sampledot \mid \state'}} f\fun{\latentstate'} - \expectedsymbol{\latentstate \sim \embed_{\encoderparameter}\fun{\sampledot\mid \state}}  \expectedsymbol{\latentstate' \sim \latentprobtransitions_{\decoderparameter}\fun{\sampledot \mid \latentstate, \action}} f\fun{\latentstate'}\right\}}\\
    =& \expected{\state, \action \sim \stationary{\policy}}{\expectedsymbol{\state' \sim \probtransitions\fun{\sampledot \mid \state, \action}}\expectedsymbol{\latentstate \sim \embed_{\encoderparameter}\fun{\sampledot \mid \state'}}\varphi^\star\tuple{\state, \action}\fun{\latentstate'} - \expectedsymbol{\latentstate \sim \embed_{\encoderparameter}\fun{\sampledot\mid \state}}  \expectedsymbol{\latentstate' \sim \latentprobtransitions_{\decoderparameter}\fun{\sampledot \mid \latentstate, \action}} \varphi^\star\tuple{\state, \action}\fun{\latentstate'}} \\
    \leq&  \sup_{\varphi \colon \states \times \actions \to \Lipschf{\distance_{\latentstates}}} \, \expected{\state, \action \sim \stationary{\policy}}{\expectedsymbol{\state' \sim \probtransitions\fun{\sampledot \mid \state, \action}}\expectedsymbol{\latentstate \sim \embed_{\encoderparameter}\fun{\sampledot \mid \state'}}\varphi\tuple{\state, \action}\fun{\latentstate'} - \expectedsymbol{\latentstate \sim \embed_{\encoderparameter}\fun{\sampledot\mid \state}}  \expectedsymbol{\latentstate' \sim \latentprobtransitions_{\decoderparameter}\fun{\sampledot \mid \latentstate, \action}} \varphi\tuple{\state, \action}\fun{\latentstate'}}\\
    =& \sup_{\varphi \colon \states \times \actions \to \Lipschf{\distance_{\latentstates}}} \, \expected{\state, \action, \state' \sim \stationary{\policy}}{\expectedsymbol{\latentstate' \sim \embed_{\encoderparameter}\fun{\sampledot \mid \state'}}\varphi\tuple{\state, \action}\fun{\latentstate'} - \expectedsymbol{\latentstate \sim \embed_{\encoderparameter}\fun{\sampledot\mid \state}}  \expectedsymbol{\latentstate' \sim \latentprobtransitions_{\decoderparameter}\fun{\sampledot \mid \latentstate, \action}} \varphi\tuple{\state, \action}\fun{\latentstate'}}.
\end{align*}
It remains to prove the other direction, i.e., $(\ref{eq:wae-mdp-loss-proof-1}) \geq (\ref{eq:wae-mdp-loss-proof-2})$.
Take
\begin{align*}
	\varphi^{\star} = \arg\sup_{\varphi \colon \states \times \actions \to \Lipschf{\distance_{\latentstates}}} \, \expected{\state, \action, \state' \sim \stationary{\policy}}{\expectedsymbol{\latentstate \sim \embed_{\encoderparameter}\fun{\sampledot \mid \state'}}\varphi\tuple{\state, \action}\fun{\latentstate'} - \expectedsymbol{\latentstate \sim \embed_{\encoderparameter}\fun{\sampledot\mid \state}}  \expectedsymbol{\latentstate' \sim \latentprobtransitions_{\decoderparameter}\fun{\sampledot \mid \latentstate, \action}} \varphi\tuple{\state, \action}\fun{\latentstate'}},
\end{align*}
then for all $\state \in \states$, $\action \in \actions$, $\varphi^\star\tuple{\state, \action} \in \Lipschf{\distance_{\latentstates}}$ and
\begin{align*}
	&& &{\expectedsymbol{\state' \sim \probtransitions\fun{\sampledot \mid \state, \action}}\expectedsymbol{\latentstate \sim \embed_{\encoderparameter}\fun{\sampledot \mid \state'}}\varphi^\star\tuple{\state, \action}\fun{\latentstate'} - \expectedsymbol{\latentstate \sim \embed_{\encoderparameter}\fun{\sampledot\mid \state}}  \expectedsymbol{\latentstate' \sim \latentprobtransitions_{\decoderparameter}\fun{\sampledot \mid \latentstate, \action}} \varphi^\star\tuple{\state, \action}\fun{\latentstate'}} \\
	&&\leq& {\sup_{f \in \Lipschf{\distance_{\latentstates}}} \left\{  \expectedsymbol{\state ' \sim \probtransitions\fun{\sampledot \mid \state, \action}}\expectedsymbol{\latentstate' \sim \embed_{\encoderparameter}\fun{\sampledot \mid \state'}} f\fun{\latentstate'} - \expectedsymbol{\latentstate \sim \embed_{\encoderparameter}\fun{\sampledot\mid \state}}  \expectedsymbol{\latentstate' \sim \latentprobtransitions_{\decoderparameter}\fun{\sampledot \mid \latentstate, \action}} f\fun{\latentstate'}\right\}} \\
	&\equiv&&  \expected{\state, \action \sim \stationary{\policy}}{\expectedsymbol{\state' \sim \probtransitions\fun{\sampledot \mid \state, \action}}\expectedsymbol{\latentstate \sim \embed_{\encoderparameter}\fun{\sampledot \mid \state'}}\varphi^\star\tuple{\state, \action}\fun{\latentstate'} - \expectedsymbol{\latentstate \sim \embed_{\encoderparameter}\fun{\sampledot\mid \state}}  \expectedsymbol{\latentstate' \sim \latentprobtransitions_{\decoderparameter}\fun{\sampledot \mid \latentstate, \action}} \varphi^\star\tuple{\state, \action}\fun{\latentstate'}} \\
	&&\leq& \expectedsymbol{\state, \action \sim \stationary{\policy}}{\sup_{f \in \Lipschf{\distance_{\latentstates}}} \left\{  \expectedsymbol{\state ' \sim \probtransitions\fun{\sampledot \mid \state, \action}}\expectedsymbol{\latentstate' \sim \embed_{\encoderparameter}\fun{\sampledot \mid \state'}} f\fun{\latentstate'} - \expectedsymbol{\latentstate \sim \embed_{\encoderparameter}\fun{\sampledot\mid \state}}  \expectedsymbol{\latentstate' \sim \latentprobtransitions_{\decoderparameter}\fun{\sampledot \mid \latentstate, \action}} f\fun{\latentstate'}\right\}},
\end{align*}
which yields the result.
\end{proof}
\fi
\iffalse
Corollary~\ref{cor:min-sup-regularizer} allows us re-writing our objective as
\begin{multline*}
    \min_{\encoderparameter, \decoderparameter}\max_{\omega} \, \expectedsymbol{\state, \action, \reward, \state' \sim \stationary{\policy}} \, \expectedsymbol{\latentstate, \latentaction_1, \latentstate' \sim \embed_{\encoderparameter}\fun{\sampledot \mid \state, \action, \state'}} \expectedsymbol{\latentstate_1' \sim \latentprobtransitions_{\decoderparameter}\fun{\sampledot \mid \latentstate, \latentaction}} \expectedsymbol{\latentstate_2 \sim \stationary{\latentpolicy_{\decoderparameter}}} \expectedsymbol{\latentaction_2 \sim \latentpolicy_{\decoderparameter}\fun{\sampledot \mid \latentstate_2}}\expectedsymbol{\latentstate_2' \sim \latentprobtransitions_{\decoderparameter}\fun{\sampledot \mid \latentstate_2, \latentaction_2}} \\
    \left[\tracedistance\fun{\tuple{\state, \action, \reward, \state'}, G_{\decoderparameter}\fun{\latentstate, \latentaction_1, \latentstate'}} + \beta \cdot \fun{ \varphi_{\omega}^{\stationary{}}\fun{\latentstate, \latentaction_1, \latentstate_1'} - \varphi^{\stationary{}}_{\omega}\fun{\latentstate_2, \latentaction_2, \latentstate_2'} + \varphi^{\probtransitions}_{\omega}\fun{\state, \action, \latentstate, \latentaction, \latentstate'} - \varphi^{\probtransitions}_{\omega}\fun{\state, \action, \latentstate, \latentaction, \latentstate_1'} } \right]
\end{multline*}
where we assume that $\varphi^{\stationary{}}_{\wassersteinparameter}$ and $\varphi^{\latentprobtransitions}_{\wassersteinparameter}$ are functions parameterized by $\wassersteinparameter$ compliant with the 1-Lipschitz constraint.
\fi
Consequently, we rewrite $\localtransitionloss{\stationary{\policy}}\fun{\wassersteinparameter}$ as a tractable maximization:
\begin{equation*}
    \localtransitionloss{\stationary{\policy}}\fun{\wassersteinparameter} = \max_{\wassersteinparameter \colon \transitionlossnetwork \in \Lipschf{\distance_{\scriptscriptstyle \latentstates}}} \; \expectedsymbol{\state, \action, \state' \sim \stationary{\policy}} \expected{\latentstate, \latentaction \sim \embed_{\encoderparameter}\fun{\sampledot \mid \state, \action}}{\transitionlossnetwork\fun{\state, \action, \latentstate, \latentaction, \embed_{\encoderparameter}\fun{\state'}} - \expectedsymbol{\latentstate' \sim \latentprobtransitions_{\decoderparameter}\fun{\sampledot \mid \latentstate, \latentaction}} \transitionlossnetwork\fun{\state, \action, \latentstate, \latentaction, \latentstate'}}.
\end{equation*}

\subsection{The Latent Metric}\label{appendix:latent-metric}%: Proof of Theorem~\ref{thm:lipsch-equiv}}
%\smallparagraph{Latent metric.}~%As stated above, continuous relaxations relying on a temperature parameter $\temperature$ are used during training to allow the objective function to be optimized via gradient descent in the backward pass, while discrete random variables are used in the forward pass, as $\temperature$ goes to $0$.
In the following, we show that considering the Euclidean distance for $\tracedistance$ and $\distance_{\latentstates}$ in the latent space for optimizing  the regularizers $\steadystateregularizer{{\policy}}$ and $\localtransitionloss{\stationary{\policy}}$ is Lipschitz equivalent to considering a continuous $\temperature$-relaxation of the \emph{discrete metric} $\condition{\neq}\fun{\vx, \vy} = \condition{\vx \neq \vy}$.
Consequently, this also means it is consistently sufficient to enforce $1$-Lispchitzness via the gradient penalty approach of \citet{DBLP:conf/nips/GulrajaniAADC17} during training to maintain the guarantees linked to the regularizers in the zero-temperature limit, when the spaces are discrete.

% When latent states are assumed to be one-hot encoded, we use relaxed categorical distributions to represent latent space distributions, and the metric space $\fun{[0, 1]^n, \frac{1}{2}\norm{.}}$ converges to $\fun{\latentstates, \condition{\neq}}$ whenever the temperature parameter $\temperature$ of the relaxation goes to $0$, where $n = {|\latentstates|}$.
% However, since the state representation is assumed binary, we use relaxed Bernoulli latent variables and we need use a distance metric $\distance_{\latentstates}$ different from $\norm{.}$ which satisfies $\fun{[0, 1]^n, \distance_{\latentstates}} \rightarrow \fun{\latentstates, \condition{\neq}}$ as $\temperature \to 0$, where $n = \log_{2} |\latentstates|$.
% The following proposition shows that enforcing $\varphi_{\wassersteinparameter}$ to be 1-Lipschitz for $\norm{.}$ is still sufficient to learn to minimize (an upper bound on) $\wassersteinsymbol{\distance_{\latentstates}}$ for a suitable choice of $\distance_{\latentstates}$ in the latent binary representation case.
\begin{lemma}
Let $\distance$ be the usual Euclidean distance and $\distance_{\temperature}\colon \mathopen[0, 1 \mathclose]^n \times \mathopen[0, 1 \mathclose]^n \to \mathopen[0, 1\mathclose[, \, \tuple{\vx, \vy} \mapsto \frac{\distance\fun{\vx, \vy}}{\temperature + \distance\fun{\vx, \vy}}$ for $\temperature \in \mathopen]0, 1\mathclose]$ and $n \in \N$, then $\distance_{\temperature}$ is a distance metric.
\end{lemma}
\begin{proof}
The function $\distance_{\temperature}$ is a metric iff it satisfies the following axioms:
\begin{enumerate}
    \item \emph{Identity of indiscernibles}:
    If $\vx=\vy$, then $\distance_{\lambda}\fun{\vx, \vy} = \frac{\distance\fun{\vx, \vy}}{\temperature + \distance\fun{\vx, \vy}} = \frac{0}{\temperature + 0} = 0$ since $\distance$ is a distance metric.
    Assume now that $\distance_\temperature\fun{\vx, \vy} = 0$ and take $\alpha = \distance\fun{\vx, \vy}$, for any $\vx, \vy$. Thus, $\alpha \in \left[0, +\infty\right[$ and $0 = \frac{\alpha}{\temperature + \alpha}$ is only achieved in $\alpha = 0$, which only occurs whenever $\vx = \vy$ since $\distance$ is a distance metric.
    \item \emph{Symmetry}:
    \begin{align*}
        \distance_\temperature\fun{\vx, \vy} &= \frac{\distance\fun{\vx, \vy}}{\temperature + \distance\fun{\vx, \vy}}\\
        &= \frac{\distance\fun{\vy, \vx}}{\temperature + \distance\fun{\vy, \vx}} \tag{$\distance$ is a distance metric} \\
        &= \distance_\temperature\fun{\vy, \vx}
    \end{align*}
    \item \emph{Triangle inequality}: 
    Let $\vx, \vy, \vz \in \mathopen[0, 1\mathclose]^n$, the triangle inequality holds iff
    \begin{align}
        &&\distance_\temperature\fun{\vx, \vy} + \distance_{\temperature}\fun{\vy, \vz} &\geq \distance_{\temperature}\fun{\vx, \vz}& \label{eq:triangle-ineq-d-temperature} \\
        &\equiv& \frac{\distance\fun{\vx, \vy}}{\temperature + \distance\fun{\vx, \vy}} + \frac{\distance\fun{\vy, \vz}}{\temperature + \distance\fun{\vy, \vz}} & \geq \frac{\distance\fun{\vx, \vz}}{\temperature + \distance\fun{\vx, \vz}}& \notag \\
        &\equiv& \frac{\temperature \distance\fun{\vx, \vy} + \temperature \distance\fun{\vy, \vz} + 2\distance\fun{\vx, \vy} \distance\fun{\vy, \vz}}{\temperature^2 + \temperature \distance\fun{\vx, \vy} + \temperature \distance\fun{\vy, \vz} + \distance\fun{\vx, \vy}\distance\fun{\vy, \vz}} & \geq \frac{\distance\fun{\vx, \vz}}{\temperature + \distance\fun{\vx, \vz}}& \notag
    \end{align}
    \begin{align}
        &\equiv& &\temperature^2 \distance\fun{\vx, \vy} + \temperature^2 \distance\fun{\vy, \vz} + 2 \temperature \distance\fun{\vx, \vy}\distance\fun{\vy, \vz} + \notag \\
        &&&\temperature\distance\fun{\vx, \vy} \distance\fun{\vx, \vz} + \temperature\distance\fun{\vy, \vz} \distance\fun{\vx, \vz} + 2 \distance\fun{\vx, \vy} \distance\fun{\vy, \vz}\distance\fun{\vx, \vz} & \notag \\
        && \geq& \temperature^2 \distance\fun{\vx, \vz} + \temperature\distance\fun{\vx, \vy} \distance\fun{\vx, \vz} + \temperature\distance\fun{\vy, \vz} \distance\fun{\vx, \vz} + \distance\fun{\vx, \vy} \distance\fun{\vy, \vz} \distance\fun{\vx, \vz} \tag{cross-product, with $\temperature > 0$ and $\images{\distance} \in \mathopen[0, \infty\mathclose[$} \\
        &\equiv& &\temperature^2 \distance\fun{\vx, \vy} + \temperature^2 \distance\fun{\vy, \vz} + 2 \temperature \distance\fun{\vx, \vy}\distance\fun{\vy, \vz} + 
        \distance\fun{\vx, \vy} \distance\fun{\vy, \vz}\distance\fun{\vx, \vz}
        \geq \temperature^2 \distance\fun{\vx, \vz} \label{eq:triangle-ineq-relaxation}
    \end{align}
    Since $\distance$ is a distance metric, we have 
    \begin{equation}
        \temperature^2 \distance\fun{\vx, \vy} + 
        \temperature^2 \distance\fun{\vy, \vz} \geq 
        \temperature^2 \distance\fun{\vx, \vz} \label{eq:triangle-ineq-scaled}
    \end{equation}
    and $\images{\distance}\in \mathopen[0, \infty\mathclose[$, meaning
    \begin{equation}
        2 \temperature \distance\fun{\vx, \vy}\distance\fun{\vy, \vz} + 
        \distance\fun{\vx, \vy} \distance\fun{\vy, \vz}\distance\fun{\vx, \vz} \geq 0 \label{eq:triangle-ineq-upper-bound}
    \end{equation}
\end{enumerate}
By Eq.~\ref{eq:triangle-ineq-scaled} and~\ref{eq:triangle-ineq-upper-bound}, the inequality of Eq.~\ref{eq:triangle-ineq-relaxation} holds.
Furthermore, the fact that Eq.~\ref{eq:triangle-ineq-d-temperature} and~\ref{eq:triangle-ineq-relaxation} are equivalent yields the result.
\end{proof}
\begin{lemma}
%Let $\distance$, $\distance_{\temperature}$ as defined above, then (i) $\fun{[0, 1]^n, \distance_{\temperature}} \xrightarrow[\temperature \to 0]{} \fun{\latentstates, \condition{\neq}}$ when $n = \log_2 \left|\latentstates\right|$,  $\fun{[0, 1]^n, \distance_{\temperature}} \xrightarrow[\temperature \to 0]{} \fun{\latentstates \times \latentactions \times \latentstates, \condition{\neq}}$ when $n = 2 \cdot \log_2 \left|\latentstates\right|  + \left|\latentactions\right|$, and (ii) $\distance$, $\distance_\temperature$ are Lipschitz-equivalent.
Let $\distance$, $\distance_{\temperature}$ as defined above, then
(i) $ \distance_{\temperature} \xrightarrow[\temperature \to 0]{} \condition{\neq}$
% (i) $ \lim_{\temperature \to 0}\distance_{\temperature} = \condition{\neq}$
and (ii) $\distance, \distance_\temperature$ are Lipschitz-equivalent.
\end{lemma}
\begin{proof}
Part (i) is straightforward by definition of $\distance_{\temperature}$.
Distances $\distance$ and $\distance_{\temperature}$ are Lispchitz equivalent if and only if $\exists a, b > 0$ such that $\forall \vx, \vy \in \mathopen[ 0, 1 \mathclose]^n$,
\begin{alignat*}{3}
    &\,& a \cdot \distance\fun{\vx, \vy} &\leq &\distance_\temperature\fun{\vx, \vy} \,\,\,\,\,\, &\leq  b \cdot \distance\fun{\vx, \vy} \\
    & \equiv & a \cdot \distance\fun{\vx, \vy} &\leq &\frac{\distance\fun{\vx, \vy}}{\temperature + \distance\fun{\vx, \vy}} &\leq  b \cdot \distance\fun{\vx, \vy} \\
    & \equiv & a & \leq & \frac{1}{\temperature + \distance\fun{\vx, \vy}} &\leq b
\end{alignat*}
Taking $a = \frac{1}{\temperature + \sqrt{n}}$ and $b = \frac{1}{\temperature}$ yields the result.
\end{proof}
\begin{corollary}
For all $\beta \geq \nicefrac{1}{\temperature}$, $\state \in \states$, $\action \in \actions$, $\latentstate \in \latentstates$, and $\latentaction \in \latentactions$, we have
\begin{enumerate}
    \item $\wassersteindist{\distance_{\temperature}}{\originaltolatentstationary{}}{\latentstationaryprior} \leq \beta \cdot \wassersteindist{\distance}{\originaltolatentstationary{}}{\latentstationaryprior}$ \label{enum:steady-state-regularizer-distance}
    \item $\wassersteindist{\distance_{\temperature}}{\embed_{\encoderparameter}\probtransitions\fun{\sampledot\mid \state, \action}}{\latentprobtransitions_{\decoderparameter}\fun{\sampledot \mid \latentstate, \latentaction}} \leq \beta \cdot \wassersteindist{\distance}{\embed_{\encoderparameter}\probtransitions\fun{\sampledot\mid \state, \action}}{\latentprobtransitions_{\decoderparameter}\fun{\sampledot \mid \latentstate, \latentaction}}$\label{enum:transition-regularizer-distance}
\end{enumerate}
\end{corollary}
\begin{proof}
By Lipschitz equivalence, taking $\beta \geq \nicefrac{1}{\temperature}$ ensures that $\forall n \in \N$, $\forall \vx, \vy \in \mathopen[0, 1\mathclose]^n, \, \distance_{\temperature}\fun{\vx, \vy} \leq \beta \cdot \distance\fun{\vx, \vy}$.
Moreover, for any distributions $P, Q$, $\wassersteindist{\distance_{\temperature}}{P}{Q} \leq \beta \cdot \wassersteindist{\distance}{P}{Q}$
% (cf., e.g., \citealt[Lemma~A.4]{DBLP:conf/icml/GeladaKBNB19} for details).
(cf., e.g., \citealt[Lemma~A.4]{DBLP:conf/icml/GeladaKBNB19} for details).
% Therefore,
%for $n_1 = 2 \cdot \log_2 \left|\latentstates\right| + \left| \latentactions \right|$ and $n_2 = \log_2 \left|\latentstates\right|$,
%there are $\beta_1, \beta_2 \geq \nicefrac{1}{\temperature}$ respectively satisfying Eq.~\ref{enum:steady-state-regularizer-distance} and~\ref{enum:transition-regularizer-distance}.
% Taking $\beta = \max \set{\beta_1, \beta_2}$ yields the result.
\end{proof}
In practice, taking the hyperparameter $\beta \geq \nicefrac{1}{\temperature}$ in the \waemdp ensures that minimizing the $\beta$-scaled regularizers w.r.t$.$ $\distance$ also minimizes the regularizers w.r.t$.$ the $\temperature$-relaxation $\distance_\temperature$, being the discrete distribution in the zero-temperature limit.
% objective should thus be set with a higher value than $\temperature^{-1}$ when relaxations relying on $\temperature$ are used.
Note that optimizing over two different $\beta_1, \beta_2$ instead of a unique scale factor $\beta$ is also a good practice to interpolate between the two regularizers.


\section{Experiment Details} \label{appendix:experiments}
The code for conducting and replicating our experiments is available at \url{https://github.com/florentdelgrange/wae_mdp}.
\subsection{Setup}\label{appendix:setup}
We used \textsc{TensorFlow} \texttt{2.7.0} \citepAR{tensorflow2015-whitepaper} to implement the neural network architecture of our \waemdp, \textsc{TensorFlow Probability} \texttt{0.15.0} \citepAR{dillon2017tensorflow} to handle the probabilistic components of the latent model (e.g., latent distributions with reparameterization tricks, masked autoregressive flows, etc.), as well as \textsc{TF-Agents} \texttt{0.11.0} \citepAR{TFAgents} to handle the RL parts of the framework.

Models have been trained on a cluster running under \texttt{CentOS Linux 7 (Core)} composed of a mix of nodes containing Intel processors with the following CPU microarchitectures:
(i) \texttt{10-core INTEL E5-2680v2}, (ii) \texttt{14-core INTEL E5-2680v4}, and (iii) \texttt{20-core INTEL Xeon Gold 6148}.
We used $8$ cores and $32$ GB of memory for each run.

\subsection{Stationary Distribution}\label{appendix:experiments:stationary-distribution}
%\smallparagraph{Stationary distribution.}~%
To sample from the stationary distribution $\stationary{\policy}$ of episodic learning environments operating under $\policy \in \mpolicies{\mdp}$, we implemented the \emph{recursive $\epsilon$-perturbation trick} of \cite{DBLP:conf/nips/Huang20}.
In a nutshell, the reset of the environment is explicitly added to the state space of $\mdp$, which is entered at the end of each episode and left with probability $1 - \epsilon$ to start a new one.
We also added a special atomic proposition $\labelset{reset}$ into $\atomicprops$ to label this reset state and reason about episodic behaviors.
For instance, this allows verifying whether the agent behaves safely during the entire episode, or if it is able to reach a goal before the end of the episode.
% We labeled this state with $\labelset{Reset}$

\subsection{Environments with initial distribution}
Many environments do not necessarily have a single initial state, but rather an initial distribution over states $d_I \in \distributions{\states}$.
In that case, the results presented in this paper remain unchanged: it suffices to add a dummy state $\state^{\star}$ to the state space $\states \cup \set{\state^{\star}}$ so that $\sinit = \state^{\star}$ with the transition dynamics $\probtransitions\fun{\state' \mid \state^{\star}, \action} = d_I\fun{\state'}$ for any action $\action \in \actions$.
Therefore, each time the reset of the environment is triggered, we make the MDP entering the initial state $\state^{\star}$, then transitioning to $\state'$ according to $d_I$.


\subsection{Latent space distribution}
\begin{figure}
    \centering
    \includegraphics[width=0.32\textwidth]{ressources/CartPole_histogram.pdf}
    \includegraphics[width=0.32\textwidth]{ressources/MountainCar_histogram.pdf}
    \includegraphics[width=0.32\textwidth]{ressources/Acrobot_histogram.pdf}
    \\
    \includegraphics[width=0.32\textwidth]{ressources/LunarLander_histogram.pdf}
    \includegraphics[width=0.31\textwidth]{ressources/Pendulum_histogram.pdf}
    \caption{Latent space distribution along training steps. The intensity of the blue hue corresponds to the frequency of latent states produced by $\embed_{\encoderparameter}$ during training.}
    \label{fig:histograms}
\end{figure}
As pointed out in Sect.~\ref{sec:experiments}, posterior collapse is naturally avoided when optimizing \waemdp.
To illustrate that, we report the distribution of latent states produced by $\embed_{\encoderparameter}$ during training (Fig.~\ref{fig:histograms}).
The plots reveal that the latent space generated by mapping original states drawn from $\stationary{\policy}$ during training to $\latentstates$ via $\embed_{\encoderparameter}$ is fairly distributed, for each environment.

\subsection{Distance Metrics: state, action, and reward reconstruction}

The choice of the distance functions $\distance_{\states}$, $\distance_{\actions}$, and $\distance_{\rewards}$, plays a role in the success of our approach.
The usual Euclidean distance is often a good choice for all the transition components, but the scale, dimensionality, and nature of the inputs sometimes require using scaled, normalized, or other kinds of distances to allow the network to reconstruct each component. While we did not observe such requirements in our experiments (where we simply used the Euclidean distance), high dimensional observations (e.g., images) are an example of data which could require tuning the state-distance function in such a way, to make sure that the optimization of the reward or action reconstruction will not be disfavored compared to that of the states.

\subsection{Value difference}
In addition to reporting the quality guarantees of the model along training steps through local losses (cf. Figure~\ref{subfig:pac-losses}), our experiments revealed that the absolute value difference $\norm{V_{\latentpolicy_{\decoderparameter}}}$ between the original and latent models operating under the latent policy  quickly decreases and tends to converge to values in the same range (Figure \ref{fig:value-diff}). 
\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{ressources/value_diff.png}
    \caption{Absolute value difference $\norm{V_{\latentpolicy_{\decoderparameter}}}$ reported along training steps.}
    \label{fig:value-diff}
\end{figure}
This is consistent with the fact that minimizing local losses lead to close behaviors (cf. Eq.~\ref{eq:bidistance-bound}) and that the value function is Lipschitz-continuous w.r.t. $\bidistance_{\latentpolicy_{\decoderparameter}}$ (cf. Section~\ref{sec:background}).

\subsection{Remark on formal verification}\label{rmk:reachability}
Recall that \emph{our bisimulation guarantees come by construction of the latent space.}
Essentially, our learning algorithm spits out a distilled policy and a latent state space which already yields a guaranteed bisimulation distance between the original MDP and the latent MDP.
This is the crux of how we enable verification techniques like model checking. % even if we do not study them or benchmark them directly.
In particular, bisimulation guarantees mean that \emph{reachability probabilities in the latent MDP compared to those in the original one are close}.
Furthermore, the value difference of (omega-regular) properties (formulated through mu-calculus) obtained in the two models is bounded by this distance (cf. Sect.~\ref{sec:background} and \citealt{DBLP:conf/fsttcs/ChatterjeeAMR08}). 

%\smallparagraph{Reachability is all you need:}~{this is the key ingredient to model-check MDPs}.
\smallparagraph{Reachability is the key ingredient} to model-check MDPs.
Model-checking properties is in most cases performed by reduction to the reachability of components or regions of the MDP: it either consists of (i) iteratively checking the reachability of the parts of the state space satisfying path formulae that comprise the specification, through a tree-like decomposition of the latter (e.g., for (P,R-)CTL properties, cf. \citealt{DBLP:BK08}), or (ii) checking the reachability to the part of the state space of a product of the MDP with a memory structure or an automaton that embeds the omega-regular property --- e.g., for LTL \citep{DBLP:conf/cav/BaierK0K0W16,DBLP:conf/cav/SickertEJK16}, LTLf \citep{DBLP:journals/corr/abs-2009-10883}, or GLTL \citep{DBLP:journals/corr/LittmanTFIWM17}, among other specification formalisms. 
The choice of specification formalism is up to the user and depends on the case study. {The scope of this work is focusing on learning to distill RL policies with bisimulation guarantees \emph{so that model checking can be applied}, in order to reason about the behaviors of the agent}. That being said, \emph{reachability is all we need} to show that model checking can be applied.
%\fi


%\subsection{Additional experiments on the formal verification of temporal events}~\label{appendix:ltl}
%As mentioned in Section~\ref{sec:background}, the value function of events formalizing $\omega$-regular specifications is Lipschitz-continuous w.r.t. the bisimulation pseudometrics.
%In the following, we provide additional experiments for verifying general $\omega$-regular specifications.
%LTL \cite{DBLP:BK08} is a temporal logic allowing to formalize a subset of the $\omega$-regular language.
%The formulae of LTL are built over the set of atomic propositions $\atomicprops$ and are defined according to the following grammar:
%\begin{equation*}
%    \varphi \Coloneqq a \mid \neg \varphi \mid \varphi \wedge \varphi \mid \ltlnext \varphi \mid \until{\varphi}{\varphi}
%\end{equation*}
%where $a \in \atomicprops$. 
%Intuitively, $\ltlnext \varphi$ means that $\varphi$ holds at the next time step,
%while $\until{\varphi}{\psi}$ means that $\varphi$ holds in the current step and \emph{will} hold in the next steps until $\psi$ holds.
%Standard abbreviations are $\texttt{true} \equiv a \vee \neg a$; $\varphi \wedge \psi \equiv  \neg (\neg \varphi \vee \neg \psi)$; $\eventually \varphi \equiv \until{\texttt{true}}{\varphi}$ ($\varphi$ eventually holds); and $\always \varphi \equiv \neg \eventually \neg \varphi$ ($\varphi$ always holds).
%We write $\trajectory \models \varphi$ if the trajectory $\trajectory = \tuple{\seq{\state}{\infty}, \seq{\action}{\infty}} \in \inftrajectories{\mdp}$ satisfies the LTL formula $\varphi$.
%The satisfaction relation is defined inductievly as follows:
%\begin{itemize}
%    %\renewcommand{\labelitemi}{$\bullet$}
%    \item $\trajectory \models a$ iff $a \in \labels\fun{\state_t}$,
%    \item $\trajectory \models \neg \varphi$ iff $\trajectory \not\models \varphi$,
%    \item $\trajectory \models \varphi \vee \psi$ iff $\trajectory \models \varphi$ or $\trajectory \models \psi$,
%    \item $\trajectory  \models \ltlnext \varphi$ iff $\tuple{\state_{\scriptscriptstyle 1: \infty}, \action_{\scriptscriptstyle 1: \infty}} \models \varphi$, and
%    \item $\trajectory \models \until{\varphi}{\psi}$ iff $\exists i \in \N$ such that $\tuple{\state_{\scriptscriptstyle i: \infty}, \action_{\scriptscriptstyle i: \infty}} \models \psi$ and $\forall j < i$, $\tuple{\state_{\scriptscriptstyle j: \infty}, \action_{\scriptscriptstyle j: \infty}} \models \varphi$.
%\end{itemize}
%The set of trajectories satisfying any LTL formula $\varphi$ is Borel-measurable in $\mdp$ \cite{DBLP:BK08}, meaning that the probabilities and values of LTL formulae can be formally computed.
%
%Properties of interest formalized via $\omega$-regular specifications include (i) \emph{liveness} $\always \eventually \varphi$: the property is satisfied infinitely often, and (ii) \emph{persistence} $\eventually \always \varphi$: the system reaches a point from which $\varphi$ is always satisfied.
%Model checking such properties relies on detecting, analyzing, and classifying the BSCCs of the induced Markov Chain \cite{DBLP:BK08}.
%By ergodicity of Assumption~\ref{assumption:vae-mdp}, this intuitively boils down to checking whether $\varphi$ is reachable from the initial state, and (ii) that all states reachable from the initial state satisfy $\varphi$.
%Therefore, reachability events play an essential role here for computing the values of such LTL formulae.
%Note that when $\discount = 1$, the problem reduces to a simple graph analysis of the underlying structure of $\mdp$ since all formulae are almost surely, infinitely often, satisfied in a BSCC \cite{DBLP:BK08}.
%We verified additional LTL properties for the same OpenAI environments and distilled policies as considered in Sect.~\ref{sec:experiments}:
%\begin{enumerate}
%    \item \emph{CartPole}: the system is \emph{always safe}, i.e., the pole angle remains lower than $15$ degrees and the cart position remains lower than $1.5$ during all episodes,
%    \item \emph{MountainCar}: the car reaches the flag at the top of the mountain infinitely often,
%    \item \emph{Pendulum}: there is an episode where the pendulum straightens and then remains straight until the end of the episode,
%    \item \emph{Pendulum}: along all episodes, the pendulum always gets up straight and then remains upright until the end of the current episode
%    \item \emph{LunarLander}: the lander never crashes: it never passes from an unsafe landing state to the reset state,
%    \item \emph{LunarLander}: there is an episode where the angle of the lander is unsafe during the landing (more than $22$ degrees),
%    \item \emph{LunarLander}: in all episodes, the angle of the lander is safe until it lands.
%\end{enumerate}
%The results are summarized in Table~\ref{table:evaluation-ltl}.
%\input{table_evaluation_2}

\subsection{Hyperparameters}\label{appendix:hyperparams}

\smallparagraph{\waemdp parameters.}~All components (e.g., functions or distribution locations and scales, see Fig.~\ref{fig:wae-architecture}) are represented and inferred by neural networks (multilayer perceptrons).
All the networks share the same architecture (i.e., number of layers and neurons per layer).
We use a simple uniform experience replay of size $10^{6}$ to store the transitions and sample them.
The training starts when the agent has collected $10^{4}$ transitions in $\mdp$.
We used minibatches of size $128$ to optimize the objective and we applied a minibatch update every time the agent executing $\policy$ has performed $16$ steps in $\mdp$.
We use the recursive $\epsilon$-perturbation trick of \cite{DBLP:conf/nips/Huang20} with $\epsilon = \nicefrac{3}{4}$: when an episode ends, it restarts from the initial state with probability $\nicefrac{1}{4}$; before re-starting an episode, the time spent in the reset state labeled with $\labelset{reset}$ follows then the geometric distribution with expectation $\nicefrac{\epsilon}{1 - \epsilon} = 3$.
We chose the same latent state-action space size than \cite{DBLP:journals/corr/abs-2112-09655}, except for LunarLander that we decreased to $\log_2 \left| \latentstates \right| = 14$ and $\left|\latentactions\right| = 3$ to improve the scalability of the verification.

\smallparagraph{VAE-MDPs parameters.}~For the comparison of Sect.~\ref{sec:experiments}, we used the exact same VAE-MDP hyperparameter set as prescribed by \cite{DBLP:journals/corr/abs-2112-09655}, except for the state-action space of LunarLander that we also changed for scalability and fair comparison purpose.%
\footnote{The code for conducting the VAE-MDPs experiments is available at \url{https://github.com/florentdelgrange/vae_mdp} (GNU General Public License v3.0).}

\smallparagraph{Hyperparameter search.}~To evaluate our \waemdp, we realized a search in the parameter space defined in Table~\ref{table:hyperparameter-search}.
The best parameters found (in terms of trade-off between performance and latent quality) are reported in Table~\ref{table:hyperparameters}.
% Note that all the networks used to represent the \waemdp parameterized functions (see Fig.~\ref{fig:wae-architecture}) use the same number of layers and neurons per layer derived from this search.
We used two different optimizers for minimizing the loss (referred to as the minimizer) and computing the Wasserstein terms (reffered to as the maximizer).
We used \textsc{Adam} \citepAR{DBLP:journals/corr/KingmaB14} for the two, but we allow for different learning rates $\textsc{Adam}_{\alpha}$ and exponential decays $\textsc{Adam}_{\beta_1}, \textsc{Adam}_{\beta_2}$.
We also found that polynomial decay for $\textsc{Adam}_{\alpha}$ (e.g., to $10^{-5}$ for $4\cdot 10^{5}$ steps) is a good practice to stabilize the experiment learning curves, but is not necessary to obtain high-quality and performing distillation.
Concerning the continuous relaxation of discrete distributions,
we used a different temperature for each distribution, as \cite{DBLP:conf/iclr/MaddisonMT17} pointed out that doing so is valuable to improve the results.
We further followed the guidelines of \citet{DBLP:conf/iclr/MaddisonMT17} to choose the interval of temperatures and did not schedule any annealing scheme (in contrast to VAE-MDPs). 
Essentially, the search reveals that the regularizer scale factors $\beta_{\scalebox{1.1}{$\cdot$}}$ (defining the optimization direction) as well as the encoder and latent transition temperatures are important to improve the performance of distilled policies.
For the encoder temperature, we found a nice spot in $\temperature_{\scriptscriptstyle \embed_{\encoderparameter}} = \nicefrac{2}{3}$, which provides the best performance in general, whereas the choice of $\temperature_{\scriptscriptstyle \latentprobtransitions_{\decoderparameter}}$ and $\beta_{\scalebox{1.1}{$\cdot$}}$ are (latent-) environment dependent.
The importance of the temperature parameters for the continuous relaxation of discrete distributions is consistent with the results of \citep{DBLP:conf/iclr/MaddisonMT17}, revealing that the success of the relaxation depends on the choice of the temperature for the different latent space sizes. % and on the specific application.
%that our MAF, requiring $\log_2 \left| \latentstates \right|$ passes through the network to sample a latent successor state, is sensitive to the temperature that controls the continuous relaxation.

% Please add the following required packages to your document preamble:
% \usepackage{booktabs}
% \usepackage{graphicx}
\begin{table}
\centering
\caption{Hyperparameter search. $\temperature_X$ refers to the temperature used for \waemdp component $X$.}
\label{table:hyperparameter-search}
\resizebox{\columnwidth}{!}{%
\begin{tabular}{@{}ll@{}}
\toprule
Parameter &
  Range \\ \midrule
$\textsc{Adam}_{\alpha}$ (minimizer) &
  $\set{0.0001, 0.0002, 0.0003, 0.001}$ \\
$\textsc{Adam}_{\alpha}$ (maximizer) &
  $\set{0.0001, 0.0002, 0.0003, 0.001}$ \\
$\textsc{Adam}_{\beta_1}$ &
  $\set{0, 0.5, 0.9}$ \\
$\textsc{Adam}_{\beta_2}$ &
  $\set{0.9, 0.999}$ \\
neurons per layer &
  $\set{64, 128, 256, 512}$ \\
number of hidden layers &
  $\set{1, 2, 3}$ \\
activation &
  $\set{\text{ReLU}, \text{Leaky ReLU}, \text{tanh}, \frac{\text{softplus}\fun{2x + 2}}{2} - 1 \, \textit{(smooth ELU)}}$ \\
$\beta_{\steadystateregularizer{\policy}}$ &
  $\set{10, 25, 50, 75, 100}$ \\
$\beta_{\localtransitionloss{\stationary{\policy}}}$ &
  $\set{10, 25, 50, 75, 100}$ \\
$\ncritic$ &
  $\set{5, 10, 15, 20}$ \\
$\delta$ &
  $\set{10, 20}$ \\
use $\varepsilon$-mimic (cf. \citealt{DBLP:journals/corr/abs-2112-09655}) &
  $\set{\text{True}, \text{False}}$ (if True, a decay rate of $10^{-5}$ is used) \\
$\temperature_{\scriptscriptstyle\latentprobtransitions_{\decoderparameter}}$ &
  $\set{0.1, \nicefrac{1}{3}, \nicefrac{1}{2}, \nicefrac{2}{3}, \nicefrac{3}{5}, 0.99}$ \\
$\temperature_{\scriptscriptstyle\embed_{\encoderparameter}}$ &
  $\set{0.1, \nicefrac{1}{3}, \nicefrac{1}{2}, \nicefrac{2}{3}, \nicefrac{3}{5}, 0.99}$ \\
$\temperature_{\scriptscriptstyle\latentpolicy_{\decoderparameter}}$ &
  $\set{\nicefrac{1}{\left| \latentactions \right| - 1}, \nicefrac{1}{\fun{\left| \latentactions \right| - 1} \cdot 1.5}}$ \\
$\temperature_{\scriptscriptstyle \actionencoder}$ &
 $\set{\nicefrac{1}{\left| \latentactions \right| - 1}, \nicefrac{1}{\fun{\left| \latentactions \right| - 1} \cdot 1.5}}$ \\ \bottomrule
\end{tabular}%
}
\end{table}

\begin{table}
\centering
\caption{Final hyperparameters used to evaluate \waemdps in Sect.~\ref{sec:experiments}}
\label{table:hyperparameters}
\resizebox{\columnwidth}{!}{%
\begin{tabular}{llllll}
\toprule
{} &          CartPole &       MountainCar &           Acrobot &       LunarLander &          Pendulum \\
\midrule
$\log_2 \left| \latentstates \right|$                                         &  9 &  10 &  13 &  14 &  13 \\
$\left| \latentactions \right|$                                               &  2 $=\left|\actions\right|$ & $2=\left|\actions\right|$  & $3=\left|\actions\right|$ &  $3$ &  $3$ \\
activation                                                                    &  tanh &  ReLU &  Leaky Relu &  ReLU &  ReLU \\
layers                                                                        &  $[64, 64, 64]$ &  $[512, 512]$ &  $[512, 512]$ &  $[256]$ &  $[256, 256, 256]$ \\
$\textsc{Adam}_{\alpha}$ (minimizer)                                          & $0.0002$ & $0.0001$ & $0.0002$ & $0.0003$ & $0.0003$ \\
$\textsc{Adam}_{\alpha}$ (maximizer)                                          & $0.0002$ & $0.0001$ & $0.0001$ & $0.0003$ & $0.0003$ \\
$\textsc{Adam}_{\beta_1}$                                                     & $0.5$ & $0$ & $0$ & $0$ & $0.5$ \\
$\textsc{Adam}_{\beta_2}$                                                     & $0.999$ & $0.999$ & $0.999$ & $0.999$ & $0.999$ \\
$\beta_{\localtransitionloss{\stationary{\policy}}}$                          & $10$ & $25$ & $10$ & $50$ & $25$ \\
$\beta_{\steadystateregularizer{\policy}}$                                    & $75$ & $100$ & $10$ & $100$ & $25$ \\
$\ncritic$                                          &  5 &  20 &  20 &  15 &  5 \\
$\delta$                                                                      & $20$ & $10$ & $20$ & $20$ & $10$ \\
$\varepsilon$                                                                 & $0$ & $0$ & $0$ & $0$ & $0.5$ \\
$\temperature_{\scriptscriptstyle\latentprobtransitions_{\decoderparameter}}$ & $\nicefrac{1}{3}$ & $\nicefrac{1}{3}$ & $0.1$ & $0.75$ & $\nicefrac{2}{3}$ \\
$\temperature_{\scriptscriptstyle\embed_{\encoderparameter}}$                 & $\nicefrac{1}{3}$ & $\nicefrac{2}{3}$ & $\nicefrac{2}{3}$ & $\nicefrac{2}{3}$ & $\nicefrac{2}{3}$ \\
$\temperature_{\scriptscriptstyle\latentpolicy_{\decoderparameter}}$          & $\nicefrac{2}{3}$ & $\nicefrac{1}{3}$ & $0.5$ & $0.5$ & $0.5$ \\
$\temperature_{\scriptscriptstyle \actionencoder}$                            & / & / & / & $\nicefrac{1}{3}$ & $\nicefrac{1}{3}$ \\
\bottomrule
\end{tabular}
}
\end{table}

\smallparagraph{Labeling functions.}~We used the same labeling functions as those described by \citet{DBLP:journals/corr/abs-2112-09655}.
For completeness, we recall the labeling function used for each environment in Table~\ref{appendix:table:labels}.

\smallparagraph{Time to failure properties.}~%
Based on the labeling described in Table~\ref{appendix:table:labels}, we formally detail the time to failure properties checked in Sect.~\ref{sec:experiments} whose results are listed in Table~\ref{table:evaluation} for each environment.
Let $\labelset{Reset} = \set{\mathsf{reset}} = \tuple{0, \dots, 1}$ (we assume here that the last bit indicates whether the current state is a reset state or not)
and define $\state \models \labelset{L}_1 \wedge \labelset{L}_2$ iff $s \models \labelset{L}_1$ and $s \models \labelset{L}_2$ for any $\state \in \states$, then
\begin{itemize}
    \item \emph{CartPole}: $\varphi = \until{\neg \labelset{Reset}}{\labelset{Unsafe}}$, where $\labelset{Unsafe} = \tuple{1, 1, 0}$
    \item \emph{MountainCar}: $\varphi = \until{\neg \labelset{Goal}}{\labelset{Reset}}$, where $\labelset{Goal} = \tuple{1, 0, 0, 0}$
    \item \emph{Acrobot}: $\varphi = \until{\neg \labelset{Goal}}{\labelset{Reset}}$, where $\labelset{Goal} = \tuple{1, 0, \dots, 0}$
    \item \emph{LunarLander}: $\varphi = \until{\neg \labelset{SafeLanding}}{\labelset{Reset}}$, where $\labelset{SafeLanding} = \labelset{GroundContact} \wedge \labelset{MotorsOff}$, $\labelset{GroundContact} = \tuple{0, 1, 0, 0, 0, 0, 0}$, and $ \labelset{MotorsOff} = \tuple{0, 0, 0, 0, 0, 1, 0}$
    \item \emph{Pendulum}: $\varphi = \eventually\fun{\neg\mathsf{Safe} \wedge \ltlnext \mathsf{Reset}}$,
    where $\labelset{Safe} = \tuple{1, 0, 0, 0, 0}$,
    $\eventually \labelset{T} = \neg \until{\emptyset}{\labelset{T}}$, and
    $\state_{i} \models \ltlnext \labelset{T}$ iff $\state_{i + 1} \models \labelset{T}$,
    for any $\labelset{T} \subseteq \atomicprops, {{\state}_{i : \infty}, {\action}_{i : \infty} }\in \inftrajectories{\mdp}$.
    Intuitively, $\varphi$ denotes the event of ending an episode in an unsafe state, just before resetting the environment, which means that either the agent never reached the safe region or it reached and left it at some point. 
    Formally, $\varphi =
	\set{\seq{\state}{\infty}, \seq{\action}{\infty} \, | \, \exists i \in \N, \state_i \not\models \labelset{Safe}\, \wedge \, \state_{i + 1} \models \labelset{Reset} } \subseteq \inftrajectories{\mdp}$.
\end{itemize}

\begin{table}
\centering
\resizebox{\columnwidth}{!}{%
\begin{tabular}{lllll}
\toprule
Environment &
  $\states \subseteq$ &
  Description, for $\vect{\state} \in \states$ &
  $\labels\fun{\vect{s}} = \tuple{p_1, \dots, p_n, p_{\mathsf{reset}}}$ \\
 \midrule
CartPole &
  $\R^4$ &
  \begin{tabular}[c]{@{}l@{}}\tabitem $\vect{\state}_1$: cart position\\ \tabitem $\vect{\state}_2$: cart velocity\\ \tabitem $\vect{\state}_3$: pole angle (rad)\\ \tabitem $\vect{\state}_4$: pole velocity at tip\end{tabular} &
  \begin{tabular}[c]{@{}l@{}}\tabitem $p_1 = \condition{\vect{\state}_1 \geq 1.5}$: unsafe cart position\\
  \tabitem $p_2 = \condition{\vect{\state}_3 \geq  0.15} $: unsafe pole angle\end{tabular} \\ \midrule
MountainCar &
  $\R^2$ &
  \begin{tabular}[c]{@{}l@{}}\tabitem $\vect{\state}_1$: position\\ \tabitem $\vect{\state}_2$: velocity\end{tabular} &
  %$\begin{aligned}\langle &\condition{\geq}\fun{\vect{\state}_1, \nicefrac{1}{2}}, \\ & \condition{\geq}\fun{\vect{\state}_1, \nicefrac{-1}{2}}, \\ & \condition{\geq}\fun{\vect{\state}_2, 0}\rangle \end{aligned}$ &
  \begin{tabular}[c]{@{}l@{}}\tabitem $p_1 = \condition{\vect{\state}_1 > 1.5}$: target position\\ \tabitem $p_2 = \condition{\vect{\state}_1 \geq \nicefrac{-1}{2}}$: right-hand side of the mountain\\ \tabitem $p_3 = \condition{\vect{\state}_2 \geq 0}$: car going forward\end{tabular} \\ \midrule
Acrobot &
  $\R^6$ &
  \begin{tabular}[c]{@{}l@{}}Let $\theta_1, \theta_2 \in \mathopen[0, 2\pi \mathclose]$ be the angles\\ of the two rotational joints,\\ \tabitem $\vect{\state}_1 = \cos\fun{\theta_1}$\\ \tabitem $\vect{\state}_2 = \sin\fun{\theta_1}$\\ \tabitem $\vect{\state}_3 = \cos\fun{\theta_2}$\\ \tabitem $\vect{\state}_4 = \sin\fun{\theta_2}$\\ \tabitem $\vect{\state}_5$: angular velocity 1\\ \tabitem $\vect{\state}_6$: angular velocity 2\end{tabular} &
  %$\begin{aligned}\langle &\condition{>}\fun{-\vect{\state}_1 -\vect{\state}_3 \cdot \vect{\state}_1 + \vect{\state}_4 \cdot \vect{\state}_2, 1}, \\ & \condition{\geq}\fun{\vect{\state}_1, 0}, \\ & \condition{\geq}\fun{\vect{\state}_2, 0} \\ & \condition{\geq}\fun{\vect{\state}_3, 0} \\ & \condition{\geq}\fun{\vect{\state}_4, 0}\\ & \condition{\geq}\fun{\vect{\state}_5, 0}\\ & \condition{\geq}\fun{\vect{\state}_6, 0}\rangle \end{aligned}$ &
  \begin{tabular}[c]{@{}l@{}}\tabitem $p_1= \condition{-\vect{\state}_1 -\vect{\state}_3 \cdot \vect{\state}_1 + \vect{\state}_4 \cdot \vect{\state}_2 > 1}$: RL agent target \\\tabitem $p_2 = \condition{\vect{\state}_1 \geq 0}$: $\theta_1 \in [0, \nicefrac{\pi}{2}] \cup [\nicefrac{3\pi}{2}, 2\pi]$ \\
  \tabitem $p_3 = \condition{\vect{\state}_2 \geq 0} $: $\theta_1 \in [0, \pi]$\\
  \tabitem $p_4 = \condition{\vect{\state}_3 \geq 0}$: $\theta_2 \in [0, \nicefrac{\pi}{2}] \cup [\nicefrac{3\pi}{2}, 2\pi]$\\
  \tabitem $p_5 = \condition{\vect{\state}_4 \geq 0}$: $\theta_2 \in [0, \pi]$\\
  \tabitem $p_6 = \condition{\vect{\state}_5 \geq 0}$: positive angular velocity (1)\\
  \tabitem $p_7 = \condition{\vect{\state}_6 \geq 0}$: positive angular velocity (2)\\
  \end{tabular} \\\midrule
Pendulum &
  $\R^3$ &
  \begin{tabular}[c]{@{}l@{}}Let $\theta \in \mathopen[0, 2\pi \mathclose]$ be the joint angle\\ \tabitem $\vect{\state}_1 = \cos\fun{\theta}$\\ \tabitem $\vect{\state}_2 = \sin\fun{\theta}$\\ \tabitem $\vect{\state}_3$: angular velocity\end{tabular} &
  %$\begin{aligned}\langle &\condition{\geq}\fun{\vect{\state}_1, \cos\fun{\nicefrac{\pi}{3}}}, \\ & \condition{\geq}\fun{\vect{\state}_1, 0}, \\ & \condition{\geq}\fun{\vect{\state}_2, 0} \\ & \condition{\geq}\fun{\vect{\state}_3, 0} \rangle \end{aligned}$ &
    \begin{tabular}[c]{@{}l@{}}
    \tabitem $p_1 = \condition{\vect{\state}_1 \geq \cos\fun{\nicefrac{\pi}{3}}}$: safe joint angle \\
    \tabitem $p_2 = \condition{\vect{\state}_1 \geq 0}$: $\theta \in [0, \nicefrac{\pi}{2}] \cup [\nicefrac{3\pi}{2}, 2\pi]$ \\
  \tabitem $p_3 = \condition{\vect{\state}_2 \geq 0}$: $\theta \in [0, \pi]$\\
  \tabitem $p_4 = \condition{\vect{\state}_3 \geq 0}$: positive angular velocity\\
  \end{tabular} \\\midrule
LunarLander &
  $\R^8$ &
  \begin{tabular}[c]{@{}l@{}}\tabitem $\vect{\state}_1$: horizontal coordinates\\ \tabitem $\vect{\state}_2$: vertical coordinates\\ \tabitem $\vect{\state}_3$: horizontal speed\\ \tabitem $\vect{\state}_4$: vertical speed\\ \tabitem $\vect{\state}_5$: ship angle\\ \tabitem $\vect{\state}_6$: angular speed\\ \tabitem $\vect{\state}_7$: left leg contact\\ \tabitem $\vect{\state}_8$: right leg contact\end{tabular} &
  % based on the heuristic function &
  \begin{tabular}[c]{@{}l@{}}
  \tabitem $p_1$: unsafe angle\\
  \tabitem $p_2$: leg ground contact\\
  \tabitem $p_3$: lands rapidly\\
  \tabitem $p_4$: left inclination\\
  \tabitem $p_5$: right inclination\\
  \tabitem $p_6$: motors shut down 
  \end{tabular}\\
\bottomrule
\end{tabular}
}
\caption{
Labeling functions for the OpenAI environments considered in our experiments \citep{DBLP:journals/corr/abs-2112-09655}.
We provide a short description of the state space and the meaning of each atomic proposition.
Recall that labels are binary encoded, for
$n = |\atomicprops| - 1$ (one bit is reserved for $\mathsf{reset}$) and $p_{\mathsf{reset}} = 1$ iff $\vect{s}$ is a reset state (cf. Appendix~\ref{appendix:experiments:stationary-distribution}). % with $\atomicprops = \set{p_1, \dots, p_n}$, we write $p_{i}$ for the one-hot vector of size $n$ with the $i^{\text{th}}$ entry set to $1$, with $i \in \set{1, \dots n}$.
%The LunarLander labeling function is based on the heuristic function of the OpenAI's implementation \citep{DBLP:journals/corr/BrockmanCPSSTZ16}.
}
\label{appendix:table:labels}
\end{table}

\section{On the curse of Variational Modeling}\label{appendix:posterior-collapse}

\emph{Posterior collapse} is a well known issue occurring in variational models (see, e.g., \citealtAR{DBLP:conf/icml/AlemiPFDS018,DBLP:conf/iclr/TolstikhinBGS18,DBLP:conf/iclr/HeSNB19,DBLP:conf/icml/DongS0B20}) which intuitively results in a degenerate local optimum where the model learns to ignore the latent space and use only the reconstruction functions (i.e., the decoding distribution) to optimize the objective.
VAE-MDPs are no exception, as pointed out in the original paper (\citealp[Section 4.3 and Appendix C.2]{DBLP:journals/corr/abs-2112-09655}).

Formally, VAE- and WAE-MDPs optimize their objective by minimizing two losses: a \emph{reconstruction cost} plus a \emph{regularizer term} which penalizes a discrepancy between the encoding distribution and the dynamics of the latent space model. 
%In our WAE-MDPs, the former corresponds to the minimization of the raw transition distance and the later to the minimization of the steady-state and transition regularizers.
In VAE-MDPs, the former corresponds to the the \emph{distortion}, and the later to the \emph{rate} of the variational model (further details are given in \citealt{DBLP:conf/icml/AlemiPFDS018,DBLP:journals/corr/abs-2112-09655}), while in our WAE-MDPs, the former corresponds to the raw transition distance and the later to both the steady-state and transition regularizers.
Notably, the rate minimization of VAE-MDPs involves regularizing a \emph{stochastic} {embedding function} $\embed_{\encoderparameter}\fun{\sampledot \mid \state}$ \emph{point-wise}, i.e., for all different input states $\state \in \states$ drawn from the interaction with the original environment.
In contrast, the latent space regularization of the WAE-MDP involves the marginal embedding distribution $\encodersymbol_{\encoderparameter}$ where the embedding function $\embed_{\encoderparameter}$ is not required to be stochastic.
% VAE- have notable differences from WAE-MDPs regarding this procedure: (i) the embedding function is \emph{stochastic}, i.e., for any input state $\state \in \states$, it is a distribution $\embed_{\encoderparameter}\fun{\sampledot \mid \state}$, and (ii) the latent space regularization is applied for all states $\state \in \states$ \emph{point-wise} $\embed_{\encoderparameter}\fun{\sampledot \mid \state}$ to match the latent dynamics --- in contrast to WAEs, which rather the \emph{marginal} distribution $\encodersymbol_{\encoderparameter}$.
\cite{DBLP:conf/icml/AlemiPFDS018} showed that \emph{posterior collapse occurs in VAEs when the rate of the variational model is close to zero,} leading to low-quality representation.

\begin{figure*}
    \begin{subfigure}{.6\textwidth}
        \includegraphics[width=.495\textwidth]{ressources/cartpole-hist-mode-collapse.pdf}
        \hfill
        \includegraphics[width=.495\textwidth]{ressources/cartpole-hist.pdf}
        \caption{Latent space distribution along training steps. The intensity of the blue hue corresponds to the frequency of latent states produced from $\embed_{\encoderparameter}$ during training.
        The vanilla model collapses to a single state.}
        \label{subfig:mode-collapse-state-frequency}
    \end{subfigure}
    \begin{subfigure}{.4\textwidth}
        \includegraphics[width=\textwidth]{ressources/vae_rate.pdf}
        \caption{Rate of the variational model.}
        \label{subfig:rate}
    \end{subfigure}
    \begin{subfigure}{0.32\textwidth}
        \includegraphics[width=\textwidth]{ressources/vae_disto.pdf}
        \caption{Distortion of the variational model.\\ $\,$\\ $\;$}
        \label{subfig:distortion}
    \end{subfigure}
    \hspace{.01333\textwidth}
    \begin{subfigure}{0.32\textwidth}
        \includegraphics[width=\textwidth]{ressources/vae_encoder_entropy.pdf}
        \caption{Average point-wise entropy of $\embed_{\encoderparameter}\fun{\sampledot \mid \state}$, for $\state \in \states$ drawn from the interaction with the original environment.}
        \label{subfig:encoder-entropy}
    \end{subfigure}
    \hspace{.01333\textwidth}
    \begin{subfigure}{0.31\textwidth}
        \includegraphics[width=\textwidth]{ressources/vae_policy_perf.pdf}
        \caption{Performance of the resulting distilled policy $\latentpolicy_{\decoderparameter}$ when deployed in the original environment (averaged over 30 episodes).}
        \label{subfig:vae-distillation}
    \end{subfigure}
    \caption{
        Comparison of the VAE-MDP in the CartPole environment (i) when the distortion and the rate are minimized as is (\emph{vanilla model}) and (ii) when it makes use of annealing schemes, entropy regularization, and prioritized experience replay to avoid posterior collapse (cf.~\citealt{DBLP:journals/corr/abs-2112-09655}).
        While the former clearly fails to learn a useful latent representation, the later does so meticulously and smoothly in two distinguishable phases:
        first, $\embed_{\encoderparameter}$ focuses on fairly distributing the latent space, setting up the stage to the concrete optimization occurring from step $4 \cdot 10^5$, where the entropy of $\embed_{\encoderparameter}$ is lowered, which allows to get the rate of the variational model away from zero.
        Five instances of the models are trained with different random seeds, with the same hyperparameters than in Sect.~\ref{sec:experiments}.
    }
    \label{fig:vae-mode-collapse}
\end{figure*}


\smallparagraph{Posterior collapse in VAE-MDPs.}~We illustrate the sensitivity of VAE-MDPs to the posterior collapse problem in Fig.~\ref{fig:vae-mode-collapse}, through the CartPole environment\footnote{
    In fact, the phenomenon of collapsing to few state occurs for all the environments considered in this paper when their prioritized experience replay is not used, as illustrated in \citealp[Appendix~C.2]{DBLP:journals/corr/abs-2112-09655}.
}: minimizing the distortion and the rate as is yields an embedding function which maps deterministically every input state to the same \emph{sink} latent state (cf. Fig.~\ref{subfig:mode-collapse-state-frequency}).
Precisely, there is a latent state $\latentstate \in \latentstates$ so that $\embed_{\encoderparameter}\fun{\latentstate \mid \state} \approx 1$ and $\latentprobtransitions_{\decoderparameter}\fun{\latentstate \mid \latentstate, \latentaction} \approx 1$ whatever the state $\state \in \states$ and action $\latentaction \in \latentactions$.
This is a form of posterior collapse, the resulting rate quickly drops to zero (cf. Fig~\ref{subfig:rate}), and the resulting latent representation yields no information at all.
This phenomenon is handled in VAE-MDPs by using (i) prioritized replay buffers that allow to focus on inputs that led to bad representation, and (ii) modifying the objective function for learning the latent space model --- the so-called evidence lower bound \citepAR{DBLP:journals/jmlr/HoffmanBWP13,DBLP:journals/corr/KingmaW13}, or \textsc{ELBO} for short --- and set up annealing schemes to eventually recover the \textsc{ELBO} at the end of the training process.
Consequently, the resulting learning procedure focuses primarily on fairly distributing the latent space, to avoid it to collapse to a single latent state,
% minimizing the distortion of the input transition distribution by ignoring the latent space (referred to as the \emph{auto-decoding phenomenon}),
to the detriment of learning the dynamics of the environment and the distillation of the RL policy.
Then, the annealing scheme allows to make the model learn to finally smoothly use the latent space to maximize the \textsc{ELBO}, and achieve consequently a lower distortion at the ``price'' of a higher rate.  

\smallparagraph{Impact of the resulting learning procedure.}~The aforementioned annealing process, used to avoid that every state collapses to the same representation, possibly induces a high entropy embedding function (Fig.~\ref{subfig:encoder-entropy}), which further complicates the learning of the model dynamics and the distillation in the first stage of the training process.
In fact, in this particular case, one can observe that the entropy reaches its maximal value, which yields a fully random state embedding function.
Recall that the VAE-MDP latent space is learned through \emph{independent} Bernoulli distributions.
Fig.~\ref{subfig:encoder-entropy} reports values centered around $4.188$ in the first training phase, which corresponds to the entropy of the state embedding function when $\embed_{\encoderparameter}\fun{\sampledot \mid \state}$ is uniformly distributed over $\latentstates$ for any state $\state \in \states$: $H\fun{\embed_{\encoderparameter}\fun{\sampledot \mid \state}} = \sum_{i=0}^{\log_2 \left| \latentstates \right| - \left| \atomicprops \right| = 6} - p_i\log~p_i - \fun{1 - p_i} \log\fun{1 - p_i} = 4.188 $, where $p_i = \nicefrac{1}{2}$ for all $i$.
The rate (Fig.~\ref{subfig:rate}) drops to zero since the divergence pulls the latent dynamics towards this high entropy (yet another form of posterior collapse), which hinders the latent space model to learn a useful representation.
However, the annealing scheme increases the rate importance along training steps, which enables the optimization to eventually leave this local optimum (here around $4\cdot10^5$ training steps).
This allows the learning procedure to leave the zero-rate spot, reduce the distortion (Fig.~\ref{subfig:distortion}), and finally distill the original policy (Fig.~\ref{subfig:vae-distillation}).

As a result, the whole engineering required to mitigate posterior collapse slows down the training procedure.
This phenomenon is reflected in Fig.~\ref{fig:eval-plots}: VAE-MDPs need several steps to stabilize and set up the stage to the concrete optimization, whereas WAE-MDPs have no such requirements since they naturally do not suffer from collapsing issues (cf. Fig.~\ref{fig:histograms}), and are consequently faster to train.

\smallparagraph{Lack of representation guarantees.}~On the theoretical side, since VAE-MDPs are optimized via the ELBO and the local losses via the related variational proxies, VAE-MDPs \emph{do not leverage the representation quality guarantees} induced by local losses (Eq.~\ref{eq:bidistance-bound}) during the learning procedure (as explicitly pointed out by \citealp[Sect.~4.1]{DBLP:journals/corr/abs-2112-09655}.): in contrast to WAE-MDPs, when two original states are embedded to the same latent, abstract state, the former are not guaranteed to be bisimilarly close (i.e., the agent is not guaranteed to behave the same way from those two states by executing the policy), meaning those proxies do not prevent original states having distant values collapsing together to the same latent representation.

    \nomenclature[m0]{$\mdp = \mdptuple$}{MDP $\mdp$ with state space $\states$, action space $\actions$, transition function $\probtransitions$, labeling function $\labels$, atomic proposition space $\atomicprops$, and initial state $\sinit$.}%
    \nomenclature[msinit]{$\mdp_\state$}{MDP obtained by replacing the initial state of $\mdp$ by $\state \in \states$}
    \nomenclature[mtrajectory]{$\trajectory = \trajectorytuple{\state}{\action}{T}$}{Trajectory}
    \nomenclature[mtrajectories]{$\inftrajectories{\mdp}$}{Set of infinite trajectories of $\mdp$}
    \nomenclature[mpolicy]{$\policy$}{Memoryless policy $\policy \colon \states \to \distributions{\actions} $}
    \nomenclature[pdist]{$\distributions{\measurableset}$}{Set of measures over a complete, separable metric space $\measurableset$}
    \nomenclature{$\condition{[\textit{cond}]}$}{indicator function: $1$ if the statement [\textit{cond}] is true, and $0$ otherwise}
    \nomenclature{$f_{\decoderparameter}$}{A function $f_{\decoderparameter} \colon \measurableset \to \R$ modeled by a neural network, parameterized by $\decoderparameter$, where $\measurableset$ is any measurable set}
    \nomenclature[mprobmeasure]{$\Prob_{\policy}^{\mdp}$}{Unique probability measure induced by the policy $\policy$ in $\mdp$ on the Borel $\sigma$-algebra over measurable subsets of $\inftrajectories{\mdp}$}
    \nomenclature[mpolicies]{$\mpolicies{\mdp}$}{Set of memoryless policies of $\mdp$}
    \nomenclature{}{}
    \nomenclature[mstationary]{$\stationary{\policy}$}{Stationary distribution of $\mdp$ induced by the policy $\policy$}
    \nomenclature[mdiscount]{$\discount$}{Discount factor in $\mathopen[0, 1\mathclose]$}
    \nomenclature[mvalues]{}{}
    \nomenclature[mstate]{$\state$}{State in $\states$}
    \nomenclature[maction]{$\action$}{Action in $\actions$}
    \nomenclature[mlimitingdistr]{$\stationary{\policy}^{t}$}{Limiting distribution of the MDP defined as  $\stationary{\policy}^{t}\fun{\state' \mid \state} = \Prob_{\policy}^{\mdp_{\state}}\fun{\set{\seq{\state}{\infty}, \seq{\action}{\infty} \mid \state_t = \state'}}$, for any source state $\state \in \states$}
    \nomenclature[mvalues]{$\valuessymbol{\policy}{\sampledot}$}{Value function for the policy $\policy$}
    \nomenclature[mreachability]{$\until{\labelset{C}}{\labelset{T}}$}{Constrained reachability event}
    \nomenclature[l1]{$\latentmdp = \latentmdptuple$}{Latent MDP with state space $\latentstates$, action space $\latentactions$, reward function $\latentrewards$, labeling function $\latentlabels$, atomic proposition space $\atomicprops$, and initial state $\zinit$.}
    \nomenclature[lstate]{$\latentstate$}{Latent state in $\latentstates$}
    \nomenclature[laction]{$\latentaction$}{Latent action in $\latentactions$}
    \nomenclature[lembedding]{$\embed$}{State embedding function, from $\states$ to $\latentstates$}
    \nomenclature[lembeddingaction]{$\embeda$}{Action embedding function, from $\latentstates \times \latentactions$ to $\actions$}
    \nomenclature[l2latentspacemodel]{$\tuple{\latentmdp, \embed, \embeda}$}{Latent space model of $\mdp$}
    \nomenclature[ldistance]{$\distance_{\latentstates}$}{Distance metric over $\latentstates$}
    \nomenclature[lantentpolicy]{$\latentpolicy$}{Latent policy $\latentpolicy\colon \latentstates \to \actions$; can be executed in $\mdp$ via $\embed$: $\latentpolicy\fun{\sampledot \mid \embed\fun{\state}}$}
    \nomenclature[localtransitionloss]{$\localtransitionloss{\stationary{}}$}{Local transition loss under distribution $\stationary{}$}
    \nomenclature[localrewardloss]{$\localrewardloss{\stationary{}}$}{Local reward loss under distribution $\stationary{}$}
    \nomenclature[lembedprob]{$\embed\probtransitions$}{Distribution of drawing $\state' \sim \probtransitions\fun{\sampledot \mid \state, \action}$, then embedding $\latentstate' = \embed\fun{\state'}$, for any state $\state \in \states$ and action $\action \in \actions$}
    \nomenclature[mbidistance]{$\bidistance_{\policy}$}{Bisimulation pseudometric}
    \nomenclature[pdiscrepancy]{$D$}{Discrepancy measure; $D\fun{P, Q}$ is the discrepancy between distributions $P, Q \in \distributions{\measurableset}$}
    \nomenclature[pwasserstein]{$\wassersteinsymbol{\distance}$}{Wasserstein distance w.r.t. the metric $\distance$; $\wassersteindist{\distance}{P}{Q}$ is the Wasserstein distance between distributions $P, Q \in \distributions{\measurableset}$}
    \nomenclature{$\Lipschf{\distance}$}{Set of $1$-Lipschitz functions w.r.t. the distance metric $\distance$}
    \nomenclature[wbehavioral]{$\stationary{\decoderparameter}$}{Behavioral model: distribution over $\states \times \actions \times \images{\rewards} \times \states$}
    \nomenclature[mtracedistance]{$\tracedistance$}{Raw transition distance, i.e., metric over $\states \times \actions \times \images{\rewards} \times \states$}
    \nomenclature[wstationary]{$\latentstationaryprior$}{Stationary distribution of the latent model $\latentmdp_{\decoderparameter}$, parameterized by $\decoderparameter$}
    \nomenclature[wmarginalencoder]{$Q_{\encoderparameter}$}{Marginal encoding distribution over $\latentstates \times \latentactions \times \latentstates: \expectedsymbol{\state, \action, \state' \sim \stationary{\policy}} \embed_{\encoderparameter}\fun{\sampledot \mid \state, \action, \state'}$}
    \nomenclature[wembeddinga]{$\embed^{\actions}_{\encoderparameter}$}{Action encoder mapping $\latentstates \times \actions$ to $\distributions{\latentactions}$}
    \nomenclature[wtransition]{$\originaltolatentstationary{}$}{Distribution of drawing state-action pairs from interacting with $\mdp$, embedding them to the latent spaces, and finally letting them transition to their successor state in $\latentmdp_{\decoderparameter}$, in $\distributions{\latentstates \times \latentactions \times \latentstates}$}
    \nomenclature[mdistancestate]{$\distance_{\states}$}{Metric over the state space}
    \nomenclature[mdistanceaction]{$\distance_{\actions}$}{Metric over the action space}
    \nomenclature[mdistancerewards]{$\distance_{\rewards}$}{Metric over $\images{\rewards}$}
    \nomenclature[wgenerate]{$\generative_{\decoderparameter}$}{State-wise decoder, from $\latentstates$ to $\states$}
    \nomenclature[wdirac]{$G_{\decoderparameter}$}{Mapping $\tuple{\latentstate, \latentaction, \latentstate'} \mapsto \tuple{\generative_{\decoderparameter}\fun{\latentstate}, \embeda_{\decoderparameter}\fun{\latentstate, \latentaction}, \latentrewards_{\decoderparameter}\fun{\latentstate, \latentaction}, \generative_{\decoderparameter}\fun{\latentstate'}}$}
    \nomenclature[wsteadystate]{\steadystateregularizer{\latentpolicy}}{Steady-state regularizer}
    \nomenclature[w]{}{}
    \nomenclature[wtransitionlossnet]{\transitionlossnetwork}{Transition Lipschitz network}
    \nomenclature[wsteadystatenet]{\steadystatenetwork}{Steady-state Lipschitz network}
    \nomenclature{$\sigmoid$}{Sigmoid function, with $\sigmoid\fun{x} = \nicefrac{1}{1 + \exp\fun{-x}}$}
    \nomenclature[lvalue]{$\latentvaluessymbol{\latentpolicy}{\sampledot}$}{Latent value function}
    \nomenclature[lpolicies]{$\latentpolicies$}{Set of (memoryless) latent policies}
    \nomenclature[wtemperarue]{$\temperature$}{Temperature parameter}
    \nomenclature[pLogistic]{$\logistic{\mu}{s}$}{Logistic distribution with location parameter $\mu$ and scale parameter $s$}
    %\onehalfspacing
    %
% \section{Index of Notations}\label{appendix:notations}
\printnomenclature
%
% \clearpage
%
\bibliographyAR{references}
\bibliographystyleAR{iclr2023_conference}