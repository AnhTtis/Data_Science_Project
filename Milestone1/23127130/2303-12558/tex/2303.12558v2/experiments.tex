% \begin{figure}
%     \centering
%     \includegraphics[width=0.9\textwidth]{ressources/loss.pdf}
%     \includegraphics[width=\textwidth]{ressources/local_losses.pdf}
%     \includegraphics[width=\textwidth]{ressources/eval_policy.pdf}
%     \caption{Plots reporting (i) the \waemdp objective, (ii) PAC local losses approximation for an error of at most $10^{-2}$ and probability confidence $0.955$, and (iii)
%     the episode return $\episodereturn{\latentpolicy_{\decoderparameter}}$ obtained when executing the distilled policy in the original MDP 
%     (approximated by averaging over $30$ episodes).
%     For each environment, we trained five different instances of the models with different random seeds: the solid line is the median and the shaded interval the interquartile range.}
%     \label{fig:eval-plots}
% \end{figure}
\begin{figure*}
\begin{subfigure}{\textwidth}
    \centering
    \includegraphics[width=\textwidth]{ressources/loss.pdf}
    \caption{\waemdp objective: reconstruction loss, transition and steady-state regularizers}
    \label{subfig:objective}
\end{subfigure}
\begin{subfigure}{\textwidth}
    \centering
    \includegraphics[width=\textwidth]{ressources/local_losses.pdf}
    \caption{PAC local losses approximation for an error of at most $10^{-2}$ and probability confidence $0.955$}
    \label{subfig:pac-losses}
\end{subfigure}
\begin{subfigure}{\textwidth}
    \centering
    \includegraphics[width=\textwidth]{ressources/eval_policy.pdf}
    \caption{Episode return obtained when executing the distilled policy in the original MDP 
    (averaged over $30$ episodes)}
    \label{subfig:policy-performance}
\end{subfigure}
    \caption{
    For each environment, we trained five different instances of the models with different random seeds: the solid line is the median and the shaded interval the interquartile range.}
    \label{fig:eval-plots}
\end{figure*}
%
We evaluate the quality of latent space models learned and policies distilled through \waemdps.
To do so, we first trained deep-RL policies (DQN, \citealt{DBLP:journals/nature/MnihKSRVBGRFOPB15} on discrete, and SAC, \citealt{DBLP:conf/icml/HaarnojaZAL18} on continuous action spaces) for various OpenAI benchmarks \citep{DBLP:journals/corr/BrockmanCPSSTZ16}, which we then distill via our approach (Figure~\ref{fig:eval-plots}).
%
We thus evaluate
%This evaluation consists of:
    %\item 
    (a) the \waemdp training metrics, %via our variational objective, i.e., the local ELBO,
    %\item 
    (b) the abstraction and representation quality via \emph{PAC local losses upper bounds} \citep{DBLP:journals/corr/abs-2112-09655},
    %\item 
    and (c) the distilled policy performance when deployed in the original environment. %, and %in $\mdp$
    %This allows to assess if the latent model learned yields a sound compression of the state-action space that retains the necessary information to optimize the return.
    %(iv) the formal verification of \emph{time-to-failure} properties in the latent model.
%\end{itemize}
% The results are summarized in Fig.~\ref{fig:eval-plots} and Table~\ref{table:evaluation}.
%
% We trained deep-RL policies \emph{beforehand} (DQN \citep{DBLP:journals/nature/MnihKSRVBGRFOPB15} on discrete and SAC \citep{DBLP:conf/icml/HaarnojaZAL18} on continuous action spaces) for various OpenAI benchmarks \citep{DBLP:journals/corr/BrockmanCPSSTZ16} (see Fig.~\ref{fig:eval-plots}) that we distill via our approach.
%We distill policies learned via deep-RL (DQN \citep{DBLP:journals/nature/MnihKSRVBGRFOPB15} on discrete and SAC \citep{DBLP:conf/icml/HaarnojaZAL18} on continuous action spaces) for various OpenAI benchmarks \citep{DBLP:journals/corr/BrockmanCPSSTZ16} (see Fig.~\ref{fig:eval-plots}).
The confidence metrics and performance are compared with those of VAE-MDPs.
Finally, we formally verify properties in the latent model. %, which highlights the applicability of our approach. 
The exact setting to reproduce our results is in Appendix~\ref{appendix:experiments}. 

%\smallparagraph{Stationary distribution.}~%
%To sample from the stationary distribution of episodic learning environments, we implemented the \emph{recursive $\epsilon$-perturbation trick} of \cite{DBLP:conf/nips/Huang20}.
%In a nutshell, the reset of the environment is explicitly added to the state space of $\mdp$, which is entered at the end of each episode and left with probability $1 - \epsilon$ to start a new one.
%We also add $\labelset{Reset}$ into $\atomicprops$ to label this reset state.
%% We labeled this state with $\labelset{Reset}$

\smallparagraph{Learning metrics.}~The objective (Fig.~\ref{subfig:objective}) is a weighted sum of the reconstruction loss and the two Wasserstein regularizers.
%, i.e., $\steadystateregularizer{\policy}(\wassersteinparameter)$ and $\localtransitionloss{\stationary{\policy}}(\wassersteinparameter)$.
The choice of $\beta$ defines the optimization direction.
% In practice, we observed that setting $\beta = 25$ already produced good results.
% Splitting $\beta$ in two independent scaling factors $\beta_1, \beta_2$ is a wise fine-tuning strategy to control the possible trade-offs between $\steadystateregularizer{\policy}(\wassersteinparameter)$ and $\localtransitionloss{\stationary{\policy}}(\wassersteinparameter)$.
%, although we did not observe any pronounced conflicts between the losses in our experiments.
\ifarxiv
Posterior collapse is not observed, naturally avoided in WAEs \citep{DBLP:conf/iclr/TolstikhinBGS18},
which reflects
that the latent space is consistently distributed (see Appendix~\ref{appendix:posterior-collapse} for a discussion and a concrete illustration of collapsing issues occurring in VAE-MDPs).
\else
In contrast to VAEs (cf. Appendix~\ref{appendix:posterior-collapse}), WAEs indeed naturally avoid posterior collapse \citep{DBLP:conf/iclr/TolstikhinBGS18}, indicating that the latent space is consistently distributed.
\fi
% the consistently distributed latent space.
Optimizing the objective (Fig.~\ref{subfig:objective}) effectively allows minimizing the local losses (Fig.~\ref{subfig:pac-losses}) and recovering the performance of the original policy (Fig.~\ref{subfig:policy-performance}).

\smallparagraph{Local losses.}~%
%We compare the quality of our latent models with that of VAE-MDPs.
%
%To allow for a fair comparison, we use the same discrete space size for W- and VAEs.
%
%At the end of training, VAE-MDPs reconstruct a latent transition function $\hat{\probtransitions}$ via frequency estimation of the latent transition dynamics (e.g., \cite{DBLP:conf/cav/BazilleGJS20,DBLP:conf/icml/CorneilGB18}),
%at the end of training,
For V- and WAEs, we formally evaluate PAC upper bounds on $\localrewardloss{\stationary{\latentpolicy_{\decoderparameter}}}$ and $\localtransitionloss{\stationary{\latentpolicy_{\decoderparameter}}}$ via the algorithm of \cite{DBLP:journals/corr/abs-2112-09655} (Fig~\ref{subfig:pac-losses}). %for the transition function $\latentprobtransitions_{\decoderparameter}$ learned through the optimizaion of the objective (Fig.~\ref{subfig:objective}).
The lower the local losses, the closer $\mdp$ and $\latentmdp_{\decoderparameter}$ are in terms of behaviors induced by $\latentpolicy_{\decoderparameter}$ (cf. Eq.~\ref{eq:bidistance-bound}).
In VAEs, the losses are evaluated on a transition function $\hat{\probtransitions}$ obtained via frequency estimation of the latent transition dynamics \citep{DBLP:journals/corr/abs-2112-09655},
by reconstructing the transition model a posteriori and collecting data to estimate the transition probabilities (e.g., \citealt{DBLP:conf/cav/BazilleGJS20,DBLP:conf/icml/CorneilGB18}).
We thus also report the metrics for $\hat{\probtransitions}$.
%We thus report the metrics for the learned transition function $\latentprobtransitions_{\decoderparameter}$ and frequency estimated $\hat{\probtransitions}$.
Our bounds quickly converge to close values in general for $\latentprobtransitions_{\decoderparameter}$  and $\hat{\probtransitions}$, whereas for VAEs, the convergence is slow and unstable, with $\hat{\probtransitions}$ offering better bounds.
We emphasize that WAEs do not require this additional reconstruction step to obtain losses that can be leveraged to assess the quality of the model, in contrast to VAEs, where learning $\latentprobtransitions_{\decoderparameter}$ was performed via overly restrictive distributions, leading to poor estimation in general (cf. Ex.~\ref{ex:independent-ber-not-sufficient}).
%The explanation is twofold: in contrast to our approach, VAE-MDPs learn (i) $\latentprobtransitions_{\decoderparameter}$ restricted to independent Bernoullis, yielding poor estimations in general (cf. Sec.~\ref{sec:discrete-latent-spaces}), (ii) a stochastic embedding function $\embed_{\encoderparameter}$, which completely changes the dynamics when determinizing it for deployment.
% The frequency estimation and encoder determinization are not necessary in our approach, which makes its combination with 
%Second, we observe that the regularizer $\localtransitionloss{\stationary{\policy}}(\wassersteinparameter)$ learned is close to the 
%bound on $\localtransitionloss{\stationary{\latentpolicy_{\decoderparameter}}}\!\!$,
% transition bound,
%which shows the ability of our model to efficiently estimate the transition loss. %(Wasserstein term in the ) transition loss. 
Finally, \emph{when the distilled policies offer comparable performance} (Fig.~\ref{subfig:policy-performance}), our bounds are either close to or better than those of VAEs.

\smallparagraph{Distillation.}~%
The bisimulation guarantees (Eq.~\ref{eq:bidistance-bound}) are only valid for $\latentpolicy_{\decoderparameter}$, the policy under which formal properties can be verified.
It is crucial that $\latentpolicy_{\decoderparameter}$ achieves performance close to $\policy$, the original one, when deployed in the RL environment. 
We evaluate the performance of $\latentpolicy_{\decoderparameter}$ via the undiscounted episode return $\episodereturn{\latentpolicy_{\decoderparameter}}$ obtained by running $\latentpolicy_{\decoderparameter}$ in the original model $\mdp$.
%: $\episodereturn{\latentpolicy_{\decoderparameter}} = \expectedsymbol{\latentpolicy_{\decoderparameter}}^{\selfloop{\mdp}{\getstates{\labelset{Reset}}}}\left[ \sum_{t = 0}^{\infty} \rewards(\state_t, \action_t) \right]$.
We observe that $\episodereturn{\latentpolicy_{\decoderparameter}}$ approaches faster the original performance $\episodereturn{\policy}$ for
%W${}^2\!$AE- than for VAE-MDPs:
W- than VAEs:
WAEs converge in a few steps
%(from $2\cdot 10^4$ to $2\cdot 10^5$ steps)
for all environments, whereas the full learning budget %of $10^6$ steps
is sometimes necessary with VAEs.
% Interestingly, $\episodereturn{\latentpolicy_{\decoderparameter}}$ even sometime outperform $\episodereturn{\policy}$.
The success in recovering the original performance emphasizes the representation quality guarantees (Eq.~\ref{eq:bidistance-bound}) induced by WAEs:
%\waemdps:
when local losses are minimized, all original states that are embedded to the same representation are bisimilarly close. 
Distilling the policy over the new representation, albeit discrete and hence coarser, still achieves effective performance since
%$\latentpolicy_{\decoderparameter}$
$\embed_{\encoderparameter}$
keeps only what is important to preserve behaviors, and thus values. 
% only preserves the necessary information for the behaviors, and thus values. 
% keeps the necessary information to preserve the behaviors, and thus values.
Furthermore, the distillation can remove some non-robustness obtained during RL:
%possibly resulting from overfitting:
%for (bisimilarly close) $\state_1, \state_2 \in \states$ with $\embed_{\encoderparameter}\fun{\state_1} = \embed_{\encoderparameter}\fun{\state_2}$,
%$\latentpolicy_{\decoderparameter}\fun{\sampledot \mid \embed_{\encoderparameter}\fun{\state_1}} = \latentpolicy_{\decoderparameter}\fun{\sampledot \mid \embed_{\encoderparameter}\fun{\state_2}}$, whereas $\policy\fun{\sampledot \mid \state_1}$ and $\policy\fun{\sampledot \mid \state_2}$ might be completely different.
$\latentpolicy_{\decoderparameter}$ prescribes the same actions for bisimilarly close states, whereas this is not necessarily the case for $\policy$.
\input{table_evaluation}

\smallparagraph{Formal verification.}~%
To formally verify $\latentmdp_\decoderparameter$, we implemented a \emph{value iteration} (VI) engine, handling the neural network encoding of the latent space for discounted properties, which is one of the most popular algorithms for checking property probabilities in MDPs (e.g., \citealt{DBLP:BK08, stt:Hensel2021, doi:10.1146/annurev-control-042820-010947}).
We verify \emph{time-to-failure} properties $\varphi$, often used to check the failure rate of a system \citep{DBLP:conf/focs/Pnueli77} by measuring whether the agent fails \emph{before the end of the episode}. %, often used to check the failure rate of a system \citep{DBLP:conf/focs/Pnueli77}.
Although simple, such properties highlight the applicability of our approach on reachability events, which are building blocks to verify MDPs
\ifarxiv
(\citealt{DBLP:BK08}; cf. Appendix~\ref{rmk:reachability} for a discussion).
\else
(\citealt{DBLP:BK08}; cf. Appendix~\ref{rmk:reachability}).
\fi
In particular, we checked whether the agent reaches an unsafe position or angle ({CartPole}, {LunarLander}), does not reach its goal position (MountainCar, Acrobot), and does not reach and stay in a safe region of the system (Pendulum).
%given $\labelset{Failure}, \labelset{Goal} \subseteq \atomicprops$, such properties have either the form $\until{\neg\labelset{Reset}}{\labelset{Failure}}$ or $\until{\neg \labelset{Goal}}{\labelset{Reset}}$.
%
%\smallparagraph{Formal verification.}~%
%We implemented a value iteration algorithm to formally verify properties in the latent MDP $\latentmdp_{\decoderparameter}$ and highlight the applicability of our approach.
%We verify \emph{time-to-failure} properties $\varphi$, often used to assess the failure rate of a system (e.g., \cite{DBLP:conf/focs/Pnueli77}):
%given $\labelset{Failure}, \labelset{Goal} \subseteq \atomicprops$, such properties have either the form $\until{\neg\labelset{Reset}}{\labelset{Failure}}$ or $\until{\neg \labelset{Goal}}{\labelset{Reset}}$.
Results are in Table~\ref{table:evaluation}: for each environment, we select the distilled policy which gives the best trade-off between performance (episode return) and abstraction quality (local losses). % (the step is specified).
% We write ${\latentvaluessymbol{\latentpolicy_{\decoderparameter}}{}}$ for the value function in $\latentmdp_{\decoderparameter}$. 
As extra confidence metric, we report the value difference 
$\norm{V_{\latentpolicy_{\decoderparameter}}} = |\values{\latentpolicy_{\decoderparameter}}{}{\sinit} - \latentvalues{\latentpolicy_{\decoderparameter}}{}{\zinit}|$ obtained by executing $\latentpolicy_{\decoderparameter}$ in $\mdp$ and $\latentmdp_{\decoderparameter}$ %.
% Values obtained in $\mdp$ are averaged over $30$ episodes, while those obtained in $\latentmdp_{\decoderparameter}$ are formally computed.
($\values{\latentpolicy_{\decoderparameter}}{}{\sampledot}$ is averaged while $\latentvalues{\latentpolicy_{\decoderparameter}}{}{\sampledot}$ is formally computed).
% We present additional experiments for other types of temporal properties in Appendix~\ref{appendix:ltl}.

% \smallparagraph{Limitations.}~%
% As pointed out in \cite{DBLP:journals/corr/abs-2112-09655}, distillation failure can reveal the lack of robustness of input policies.
% We observed that recovering the performance of highly noise-sensitive RL policies is laborious.
% In particular, (vanilla) SAC policies distilled via our approach for robotics environments (such as \texttt{MuJoCo} \citep{todorov2012mujoco}), although formally verifiable, resulted in inferior performance policies.
% \FD{Should we add something to this limitation? (explicitly state the limitations is recommended in the NeurIPS guidelines).
% I am tempted to say that using our latent representation as input of deep-RL algorithms (replacing the distillation process) and let the agent learn directly on it is a promising direction to address this limitation --- Theorem \ref{thm:latent-execution-objective} is a perfect support for that.
% Moreover, in that case, the discrete representation allows for online verification algorithms to ensure safety guarantees during the learning process, such as shielding.
% Perhaps this can be addressed in the conclusion?}

\iffalse

\begin{remark}[Reachability is all you need] \label{rmk:reachability}
Recall that \emph{our bisimulation guarantees come by construction of the latent space.}
Essentially, our learning algorithm spits out a distilled policy and a latent state space which already yields a guaranteed bisimulation distance between the original MDP and the latent MDP.
This is the crux of how we enable verification techniques like model checking. % even if we do not study them or benchmark them directly.
In particular, bisimulation guarantees mean that \emph{reachability probabilities in the latent MDP compared to those in the original one are close}.
Furthermore, the value difference of (omega-regular) properties (formulated through mu-calculus) obtained in the two models is bounded by this distance (cf. Sect.~\ref{sec:background} and \citealt{DBLP:conf/fsttcs/ChatterjeeAMR08}). 
%
\emph{Reachability is the key ingredient to model-check MDPs}. Model-checking properties is in most cases performed by reduction to the reachability of components or regions of the MDP: it either consists of (i) iteratively checking the reachability of the parts of the state space satisfying path formulae that comprise the specification, through a tree-like decomposition of the latter (e.g., for (P,R-)CTL properties, cf. \citealt{DBLP:BK08}), or (ii) checking the reachability to the part of the state space of a product of the MDP with a memory structure or an automaton that embeds the omega-regular property --- e.g., for LTL \citep{DBLP:conf/cav/BaierK0K0W16,DBLP:conf/cav/SickertEJK16}, LTLf \citep{DBLP:journals/corr/abs-2009-10883}, or GLTL \citep{DBLP:journals/corr/LittmanTFIWM17}, among other specification formalisms. 
The choice of specification formalism is up to the user and depends on the case study. {The scope of this work is focusing on learning to distill RL policies with bisimulation guarantees \emph{so that model checking can be applied}, in order to reason about the behaviors of the agent}. That being said, reachability is all we need to show that model checking can be applied.
\end{remark}
\fi