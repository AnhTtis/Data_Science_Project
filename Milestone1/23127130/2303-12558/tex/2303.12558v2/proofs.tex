% \todo[inline]{Make sure to reshuffle sections to have the right ordering}
\section{Details on Discrepancy Measures}
Let $P, Q \in
\distributions{\measurableset}$ be probability distributions over $\measurableset$ with density functions $p$ and $q$.

\smallparagraph{Wasserstein.}~The Kantorovich-Rubinstein duality allows formulating the Wasserstein distance between $P$ and $Q$ as
    \[
        %\wassersteindist{\distance}{P}{Q} = \sup_{\tuple{f_1, f_2} \in \Lipschf{\distance}^2} \int_X f_1\fun{x} \, dP\fun{x} + \int_X f_2\fun{y} \, dQ\fun{y}
        \wassersteindist{\distance}{P}{Q} = \sup_{f \in \Lipschf{\distance}} \left|{\expectedsymbol{x \sim P} f\fun{x} - \expectedsymbol{y \sim Q} f\fun{y}}\right|,
    \]
    % where $\Lipschf{\distance}^2 = \set{\tuple{f_1, f_2} \mid f_1\fun{x} + f_2\fun{y} \leq \distance\fun{x, y}}$.
    where $\Lipschf{\distance}$ is the set of 1-Lipschitz functions, i.e.,
    $\Lipschf{\distance} = \set{f : \left|f\fun{x} - f\fun{y}\right| \leq
    \distance\fun{x, y}}$.
    %

\smallparagraph{Total Variation.}~One can reformulate the total variation distance between $P$ and $Q$ as
\begin{align*}
\dtv{P}{Q} ={} & \sup_{A \in \borel{\measurableset}} |P\fun{A} - Q\fun{A}|\\
{}={} & \frac{1}{2} \int_{\measurableset} |p\fun{x} - q\fun{x}|\, dx.
\end{align*}
When $\measurableset$ is continuous, TV might thus give an overly strong measure of the numerical differences across probability distributions to all measurable sets since the distance between two point masses $dx, dy$ is $1$ unless $x = y$ \citep{DBLP:conf/uai/FernsPP05}.

\section{Latent Space Models}\label{appendix:section:latent-space-model}
From now on, fix MDP $\mdp = \mdptuple$ with latent space model $\tuple{\latentmdp, \embed, \embeda}, \, \latentmdp = \latentmdptuple$.
\subsection{DeepMDPs}
In the following, we recall DeepMDPs notions (from \citealt{DBLP:conf/icml/GeladaKBNB19}) that are useful to prove bisimumation distance (Lem.~\ref{lemma:bidistance-embedding}, Lem.~\ref{lemma:bidistance-abstraction-quality}) and value difference bounds (Lem.~\ref{lemma:qvalues-diff-bound}, Lem.~\ref{lem:quality-latent-space}).  

\smallparagraph{Smooth-valuations.}
A policy $\latentpolicy \in \latentmpolicies$ is said to be \emph{$\KV$-($\dtvsymbol$-)smooth-valued} if
$\sup_{\latentstate \in \latentstates} \left| \latentvalues{\latentpolicy}{}{\latentstate}\right| \leq \KV$ and if for all $\latentaction \in \latentactions$, $\sup_{\latentstate \in \latentstates} \left| \latentqvalues{\latentpolicy}{}{\latentstate}{\latentaction} \right| \leq \KV.$
Observe that all policies $\latentpolicy \in \latentmpolicies$ are
$(\nicefrac{\Rmax{\latentpolicy}}{1 - \discount})$-smooth-valued, where $\Rmax{\latentpolicy} = \sup_{\latentstate \in \latentstates} \left| \latentrewards_{\latentpolicy}\fun{\latentstate} \right|$.

\smallparagraph{Lipschitzness.}
%Let $\latentmdp$ be a latent MDP with metric $\distance_\latentstates$,
A policy $\latentpolicy \in \latentmpolicies$ is $\KV$-Lipschitz-valued if for
all $\latentstate_1, \latentstate_2 \in \states$,
\(
    \left|\latentvalues{\latentpolicy}{}{\latentstate_1} - \latentvalues{\latentpolicy}{}{\latentstate_2} \right| \leq \KV\distance_{\latentstates}\fun{\latentstate_1, \latentstate_2}
\)
and for all $\latentaction \in \latentactions$,
\(
    \left|\latentqvalues{\latentpolicy}{}{\latentstate_1}{\latentaction} - \latentqvalues{\latentpolicy}{}{\latentstate_2}{\latentaction} \right| \leq \KV\distance_{\latentstates}\fun{\latentstate_1, \latentstate_2}.
\)
The MDP $\latentmdp$ is \emph{$\langle \KR{\latentpolicy}, \KP{\latentpolicy}\rangle$-Lipschitz} if for all $\latentstate_1, \latentstate_2 \in \latentstates$,
\begin{align*}
    \left|\latentrewards_{\latentpolicy}\fun{\latentstate_1} - \latentrewards_{\latentpolicy}\fun{\latentstate_2}\right| &\leq \KR{\latentpolicy} \distance_{\latentstates}\fun{\latentstate_1, \latentstate_2},\\
    \wassersteindist{\distance_{\latentstates}}{\latentprobtransitions_{\latentpolicy}\fun{\sampledot \mid \latentstate_1}}{ \latentprobtransitions_{\latentpolicy}\fun{\sampledot \mid \latentstate_2}} &\leq \KP{\latentpolicy} \distance_{\latentstates}\fun{\latentstate_1, \latentstate_2}.
\end{align*}
Moreover, all latent policies $\latentpolicy \in \latentmpolicies$ with $\KP{\latentpolicy} \leq \frac{1}{\discount}$ are $(\nicefrac{\KR{\latentpolicy}}{1 - \discount
\KP{\latentpolicy}})$-Lipschitz-valued \citep[Lem.~1]{DBLP:conf/icml/GeladaKBNB19}.
In the discrete setting, we replace the Wasserstein term by TV and we omit the $\distance_{\latentstates}$ terms.

\subsection{Bounded Bisimulation Distance}
\begin{lemma}\label{lemma:bidistance-embedding}
Let $\latentpolicy \in \latentmpolicies$, assume $\latentmdp$ is discrete and $\langle\KR{\latentpolicy}, \KP{\latentpolicy}\rangle$-Lipschitz.
Then, given the induced stationary distribution $\stationary{\latentpolicy}$ in $\mdp$,
\begin{align*}
    % \expectedsymbol{\state \sim \stationary{\latentpolicy}} \bidistance^{\varphi}_{\latentpolicy}\fun{\state, \embed\fun{\state}} \leq  \localrewardloss{\stationary{\latentpolicy}} \cdot \condition{\set{\rewards}}\fun{\varphi} + \discount \localtransitionloss{\stationary{\latentpolicy}} \frac{\fun{\KR{\latentpolicy} \cdot  \condition{\set{\rewards}}\fun{\varphi} + \discount \cdot \condition{\set{\labels}}\fun{\varphi}}}{1 - \discount \fun{\KP{\latentpolicy}\condition{\set{\rewards}}\fun{\varphi} + \condition{\set{\labels}}\fun{\varphi}}}.
    \expectedsymbol{\state \sim \stationary{\latentpolicy}} \bidistance^{\rewards}_{\latentpolicy}\fun{\state, \embed\fun{\state}} & \leq  \localrewardloss{\stationary{\latentpolicy}} + \discount \localtransitionloss{\stationary{\latentpolicy}} \frac{\KR{\latentpolicy}}{1 - \discount \KP{\latentpolicy}}, \\
    \expectedsymbol{\state \sim \stationary{\latentpolicy}} \bidistance^{\labels}_{\latentpolicy}\fun{\state, \embed\fun{\state}} & \leq \frac{\discount \localtransitionloss{\stationary{\latentpolicy}}}{1 - \discount}.
\end{align*}
\end{lemma}
\begin{proof}
% Let us first state a property derived from Wasserstein: let $\distance_1, \distance_2 \in \tilde{P}_\measurableset$ such that $\distance_1\fun{x, y} \leq K \distance_2\fun{x, y}$ for all $x, y \in \measurableset$ and some $K \in \R_{\geq 0}$, then for all $P, Q \in \distributions{\measurableset}$, $\wassersteindist{\distance_1}{P}{Q} \leq K \wassersteindist{\distance_2}{P}{Q}$.
% A proof of the statement is provided by \citealp[Lemma~A.4]{DBLP:conf/icml/GeladaKBNB19}, where they showed that a Lipschitz norm of $\bidistance_{\latentpolicy}^{\rewards}$ in $\latentstates$ is given by $\frac{(1 - \discount) \KR{\latentpolicy}}{1 - \discount \KP{\latentpolicy}}$.
% Let $=_{\labels}$ be the relation $\set{\latentstate_1, \latentstate_2 \mid \labels\fun{\latentstate_1} = \labels\fun{\latentstate_2}} \subseteq \latentstates^2$, we similarly show that $\bidistance^{\labels}_{\latentpolicy}\fun{\latentstate_1, \latentstate_2} \leq \discount \KP{\latentpolicy}$ whenever $\latentstate_1 =_{\labels} \latentstate_2$.
% \cite{DBLP:conf/fossacs/ChenBW12} showed that the sequence given by $\bidistance^{\labels}_{\latentpolicy, 0}\fun{\latentstate_1, \latentstate_2} = \condition{\neq_{\labels}}$ and $\bidistance^{\labels}_{\latentpolicy, n + 1} = \Delta(\bidistance^{\labels}_{\latentpolicy, n})$ converges to $\bidistance^{\labels}_{\latentpolicy}$ as $n \to \infty$.
% Let $K_n = \sup_{\latentstate_1 =_{\labels} \latentstate_2} \bidistance^{\labels}_{\latentpolicy, n}$ for $n \geq 1$.
% When $n = 1$, we have
% \begin{align*}
%     K_1 &= \sup_{\latentstate_1 =_{\labels} \latentstate_2} \discount \wassersteindist{\condition{\neq_{\labels}}}{\latentprobtransitions_{\latentpolicy}\fun{\sampledot \mid \latentstate_1}}{\latentprobtransitions_{\latentpolicy}\fun{\sampledot \mid \latentstate_2}} \\
%     & \leq \discount \sup_{\latentstate_1 =_{\labels} \latentstate_2} \wassersteindist{\condition{\neq}}{\latentprobtransitions_{\latentpolicy}\fun{\sampledot \mid \latentstate_1}}{\latentprobtransitions_{\latentpolicy}\fun{\sampledot \mid \latentstate_2}} \tag{by the property stated above} \\ % \tag{$\forall \latentstate_1, \latentstate_2 \in \latentstates$, $\condition{\neq_{\labels}}\fun{\latentstate_1, \latentstate_2} \leq \condition{\neq}\fun{\latentstate_1, \latentstate_2}$} \\
%     & \leq \discount \sup_{\latentstate_1 \neq \latentstate_2} \dtv{\latentprobtransitions_{\latentpolicy}\fun{\sampledot \mid \latentstate_1}}{\latentprobtransitions_{\latentpolicy}\fun{\sampledot \mid \latentstate_2}} \\
%     & \leq \discount \KP{\latentpolicy}
% \end{align*}
% Assume $K_n \leq \discount \KP{\latentpolicy}$ for $n \geq 1$, then
% \begin{align*}
%     K_{n + 1} &= \discount \sup_{\latentstate_1 =_{\labels} \latentstate_2} \wassersteindist{\bidistance^{\labels}_{\latentpolicy, n}}{\latentprobtransitions_{\latentpolicy}\fun{\sampledot \mid \latentstate_1}}{\latentprobtransitions_{\latentpolicy}\fun{\sampledot \mid \latentstate_2}} \\
%     &\leq \discount \sup_{\latentstate_1 \neq \latentstate_2} \discount \KP{\latentpolicy} \wassersteindist{\condition{\neq}}{\latentprobtransitions_{\latentpolicy}\fun{\sampledot \mid \latentstate_1}}{\latentprobtransitions_{\latentpolicy}\fun{\sampledot \mid \latentstate_2}} \tag{by the property stated above} \\
%     &= \discount^2 \KP{\latentpolicy} \sup_{\latentstate_1 \neq \latentstate_2} \dtv{\latentprobtransitions_{\latentpolicy}\fun{\sampledot \mid \latentstate_1}}{\latentprobtransitions_{\latentpolicy}\fun{\sampledot \mid \latentstate_2}}
%     \leq (\discount \KP{\latentpolicy})^2
%     \leq \discount \KP{\latentpolicy},
% \end{align*}
% which yields the desired upper bound.
% Let $K^{\rewards} = \frac{(1 - \discount) \KR{\latentpolicy}}{1 - \discount \KP{\latentpolicy}}$ and $K^{\labels} = \discount \KP{\latentpolicy}$, the Wasserstein property stated above allows us to derive
% \begin{align*}
%     &\expectedsymbol{\state, \latentaction \sim \stationary{\latentpolicy}}{\wassersteindist{\bidistance^{\varphi}_{\latentpolicy}}{\embed\probtransitions\fun{\sampledot \mid \state, \latentaction}}{\latentprobtransitions\fun{\sampledot \mid \embed\fun{\state}, \latentaction}}}\\
%     \leq& \expected{\state, \latentaction \sim \stationary{\latentpolicy}}{K^{\varphi} \wassersteindist{\condition{\neq}}{\embed\probtransitions\fun{\sampledot \mid \state, \latentaction}}{\latentprobtransitions\fun{\sampledot \mid \embed\fun{\state}, \latentaction}}} \\
%     \leq& K^{\varphi} \expectedsymbol{\state, \latentaction \sim \stationary{\latentpolicy}}\dtv{\embed\probtransitions\fun{\sampledot \mid \state, \latentaction}}{\latentprobtransitions\fun{\sampledot \mid \embed\fun{\state}, \latentaction}} \\
%     \leq& K^{\varphi} \localtransitionloss{\stationary{\latentpolicy}}.
% \end{align*}
\citet[Lemma~A.4]{DBLP:conf/icml/GeladaKBNB19} showed that $\bidistance_{\latentpolicy}^{\rewards}$ is $\frac{(1 - \discount) \KR{\latentpolicy}}{1 - \discount \KP{\latentpolicy}}$-Lipschitz in $\latentstates$, whereas $\bidistance_{\latentpolicy}^{\labels}$ is $1$-Lipschitz since it assigns a maximal distance of one whenever states have different labels.
Let $K_{\rewards} = \frac{(1 - \discount) \KR{\latentpolicy}}{1 - \discount \KP{\latentpolicy}}$, $K_{\labels} = 1$ and $\varphi \in \set{\rewards, \labels}$, this allows us deriving the following inequality:
\begin{align*}
    &\expectedsymbol{\state \sim \stationary{\latentpolicy}}{\wassersteindist{\bidistance^{\varphi}_{\latentpolicy}}{\embed\probtransitions_{\latentpolicy}\fun{\sampledot \mid \state}}{\latentprobtransitions_{\latentpolicy}\fun{\sampledot \mid \embed\fun{\state}}}} \\
    =& \expected{\state \sim \stationary{\latentpolicy}}{\inf_{\coupling \in \couplings{\embed\probtransitions_{\latentpolicy}\fun{\sampledot \mid \state}}{\latentprobtransitions_{\latentpolicy}\fun{\sampledot \mid \embed\fun{\state}}}} \sum_{\latentstate_1, \latentstate_2 \in \states} \bidistance^{\varphi}_{\latentpolicy}\fun{\latentstate_1, \latentstate_2} \coupling\fun{\latentstate_1, \latentstate_2}} \\
    \leq& \expected{\state \sim \stationary{\latentpolicy}}{ \inf_{\coupling \in \couplings{\embed\probtransitions_{\latentpolicy}\fun{\sampledot \mid \state}}{\latentprobtransitions_{\latentpolicy}\fun{\sampledot \mid \embed\fun{\state}}}} \sum_{\latentstate_1, \latentstate_2 \in \states}  K_{\varphi} \condition{\neq}\fun{\latentstate_1, \latentstate_2} \coupling\fun{\latentstate_1, \latentstate_2}} \tag{$\bidistance^{\varphi}_{\latentpolicy}$ is $K_{\varphi}$-Lipschitz in $\latentstates$} \\
    =& K_{\varphi} \expected{\state \sim \stationary{\latentpolicy}}{ \inf_{\coupling \in \couplings{\embed\probtransitions_{\latentpolicy}\fun{\sampledot \mid \state}}{\latentprobtransitions_{\latentpolicy}\fun{\sampledot \mid \embed\fun{\state}}}} \sum_{\latentstate_1 \neq \latentstate_2} \coupling\fun{\latentstate_1, \latentstate_2}} \\
    =&  K_{\varphi} \expectedsymbol{\state \sim \stationary{\latentpolicy}}{\dtv{\embed\probtransitions_{\latentpolicy}\fun{\sampledot \mid \state}}{\latentprobtransitions_{\latentpolicy}\fun{\sampledot \mid \embed\fun{\state}}}} \tag{by definition of TV; TV coincides with $\wassersteinsymbol{\condition{\neq}}$} \\
    =&  K_{\varphi} \expected{\state \sim \stationary{\latentpolicy}}{\frac{1}{2} \sum_{\latentstate' \in \latentstates} \left|\embed\probtransitions_{\latentpolicy}\fun{\latentstate' \mid \state} - \latentprobtransitions_{\latentpolicy}\fun{\latentstate' \mid \embed\fun{\state}} \right|} \\
     =&  K_{\varphi} \expected{\state \sim \stationary{\latentpolicy}}{\frac{1}{2} \sum_{\latentstate' \in \latentstates} \left|\expectedsymbol{\latentaction \sim \latentpolicy\fun{\sampledot \mid \state}}\embed\probtransitions\fun{\latentstate' \mid \state, \latentaction} - \expectedsymbol{\latentaction \sim \latentpolicy\fun{\sampledot \mid \state}}\latentprobtransitions\fun{\latentstate' \mid \embed\fun{\state}, \latentaction} \right|} \\
     =&  K_{\varphi} \expected{\state \sim \stationary{\latentpolicy}}{\frac{1}{2} \sum_{\latentstate' \in \latentstates} \left|\expectedsymbol{\latentaction \sim \latentpolicy\fun{\sampledot \mid \state}} \fun{\embed\probtransitions\fun{\latentstate' \mid \state, \latentaction} - \latentprobtransitions\fun{\latentstate' \mid \embed\fun{\state}, \latentaction}} \right|} \\
     \leq& K_{\varphi} \expectedsymbol{\state \sim \stationary{\latentpolicy}}\expected{\latentaction \sim \latentpolicy\fun{\sampledot \mid \state}}{\frac{1}{2} \sum_{\latentstate' \in \latentstates}  \left| \embed\probtransitions\fun{\latentstate' \mid \state, \latentaction} - \latentprobtransitions\fun{\latentstate' \mid \embed\fun{\state}, \latentaction} \right|} \tag{Jensen's  inequality} \\
     \leq& K_\varphi \localtransitionloss{\stationary{\latentpolicy}}.
\end{align*}
Then,
\begin{align*}
    &\expectedsymbol{\state \sim \stationary{\latentpolicy}} \bidistance^{\varphi}_{\latentpolicy}\fun{\state, \embed\fun{\state}} \\
    =& \expected{\state \sim \stationary{\latentpolicy}}{\fun{1 - \discount} \left| \rewards_{\latentpolicy}\fun{\state} - \latentrewards_{\latentpolicy}\fun{\embed\fun{\state}} \right| \cdot \condition{\set{\rewards}}\fun{\varphi} + \discount \wassersteindist{\bidistance_{\latentpolicy}^{\varphi}}{\probtransitions_{\latentpolicy}\fun{\sampledot \mid \state}}{\latentprobtransitions_{\latentpolicy}\fun{\sampledot \mid \embed\fun{\state}}} } \tag{$\labels\fun{\state} = \labels\fun{\embed\fun{\state}}$ by Assumption~\ref{assumption:labeling}} \\
    \leq& \fun{1 - \discount} \localrewardloss{\stationary{\latentpolicy}} \cdot \condition{\set{\rewards}}\fun{\varphi} + \discount \expected{\state \sim \stationary{\latentpolicy}}{ \wassersteindist{\bidistance_{\latentpolicy}^{\varphi}}{\probtransitions_{\latentpolicy}\fun{\sampledot \mid \state}}{\latentprobtransitions_{\latentpolicy}\fun{\sampledot \mid \embed\fun{\state}}} } \\
    \leq& \fun{1 - \discount} \localrewardloss{\stationary{\latentpolicy}} \cdot \condition{\set{\rewards}}\fun{\varphi} \\
    &+ \discount \expected{\state \sim \stationary{\latentpolicy}}{ \wassersteindist{\bidistance_{\latentpolicy}^{\varphi}}{\probtransitions_{\latentpolicy}\fun{\sampledot \mid \state}}{\embed\probtransitions_{\latentpolicy}\fun{\sampledot \mid {\state}}} + \wassersteindist{\bidistance_{\latentpolicy}^{\varphi}}{\embed\probtransitions_{\latentpolicy}\fun{\sampledot \mid \state}}{\latentprobtransitions_{\latentpolicy}\fun{\sampledot \mid \embed\fun{\state}}} } \tag{triangular inequality} \\
    \leq& \fun{1 - \discount} \localrewardloss{\stationary{\latentpolicy}} \cdot \condition{\set{\rewards}}\fun{\varphi} + \discount K_\varphi \localtransitionloss{\stationary{\latentpolicy}} + \discount \expectedsymbol{\state \sim \stationary{\latentpolicy}}{ \wassersteindist{\bidistance_{\latentpolicy}^{\varphi}}{\probtransitions_{\latentpolicy}\fun{\sampledot \mid \state}}{\embed\probtransitions_{\latentpolicy}\fun{\sampledot \mid {\state}}} } \tag{by the inequality derived above} \\
    \leq& \fun{1 - \discount} \localrewardloss{\stationary{\latentpolicy}} \cdot \condition{\set{\rewards}}\fun{\varphi} + \discount K_\varphi \localtransitionloss{\stationary{\latentpolicy}} + \discount \expectedsymbol{\state \sim \stationary{\latentpolicy}}{ \bidistance_{\latentpolicy}^{\varphi}\fun{\state, \embed\fun{\state}} }.
\end{align*}
To see how we pass from the penultimate to the last line, recall that 
\[\expectedsymbol{\state \sim \stationary{\latentpolicy}}\wassersteindist{\bidistance_{\latentpolicy}^{\varphi}}{\probtransitions_{\latentpolicy}\fun{\sampledot \mid \state}}{\embed\probtransitions_{\latentpolicy}\fun{\sampledot \mid {\state}}} = \expectedsymbol{\state \sim \stationary{\latentpolicy}} \sup_{f \in \Lipschf{\bidistance^{\varphi}_{\latentpolicy}}} \left| \expectedsymbol{\state' \sim \probtransitions_{\latentpolicy}\fun{\sampledot \mid \state}}{f\fun{\state'}} - \expectedsymbol{\latentstate' \sim \embed\probtransitions_{\latentpolicy}\fun{\sampledot \mid \state}}{f\fun{\latentstate'}} \right|,\]
where \[\Lipschf{\bidistance^{\varphi}_{\latentpolicy}} = \set{f \colon \states \uplus \latentstates \to \R : \left| f\fun{\state_1^\star} - f\fun{\state_2^{\star}} \right| \leq  \bidistance^{\varphi}_{\latentpolicy}\fun{\state_1^{\star}, \state_2^{\star}}}.\]
%Take $f^{\star}_s = \sup_{f \in \Lipschf{\bidistance^{\varphi}_{\latentpolicy}}} \left| \expectedsymbol{\state' \sim \probtransitions_{\latentpolicy}\fun{\sampledot \mid \state}}{f\fun{\state'}} - \expectedsymbol{\latentstate' \sim \embed\probtransitions_{\latentpolicy}\fun{\sampledot \mid \state}}{f\fun{\latentstate'}} \right|$ for $\state \in \states$, then we have
Then, we have
\begin{align*}
    &\expectedsymbol{\state \sim \stationary{\latentpolicy}} \sup_{f \in \Lipschf{\bidistance^{\varphi}_{\latentpolicy}}} \left| \expectedsymbol{\state' \sim \probtransitions_{\latentpolicy}\fun{\sampledot \mid \state}}{f\fun{\state'}} - \expectedsymbol{\latentstate' \sim \embed\probtransitions_{\latentpolicy}\fun{\sampledot \mid \state}}{f\fun{\latentstate'}} \right| \\
    =& \expectedsymbol{\state \sim \stationary{\latentpolicy}} \sup_{f \in \Lipschf{\bidistance^{\varphi}_{\latentpolicy}}} \left| \expectedsymbol{\state' \sim \probtransitions_{\latentpolicy}\fun{\sampledot \mid \state}}{f\fun{\state'}} - \expectedsymbol{\state' \sim \probtransitions_{\latentpolicy}\fun{\sampledot \mid \state}}{f\fun{\embed\fun{\state'}}} \right| \\
    =& \expectedsymbol{\state \sim \stationary{\latentpolicy}} \sup_{f \in \Lipschf{\bidistance^{\varphi}_{\latentpolicy}}} \left| \expectedsymbol{\state' \sim \probtransitions_{\latentpolicy}\fun{\sampledot \mid \state}}\fun{f\fun{\state'} - f\fun{\embed\fun{\state'}}} \right| \\
    \leq& \expectedsymbol{\state \sim \stationary{\latentpolicy}} \expectedsymbol{\state' \sim \probtransitions_{\latentpolicy}\fun{\sampledot \mid \state}} \sup_{f \in \Lipschf{\bidistance^{\varphi}_{\latentpolicy}}} \left| {f\fun{\state'} - f\fun{\embed\fun{\state'}}} \right| \\
    =& \expectedsymbol{\state \sim \stationary{\latentpolicy}} \sup_{f \in \Lipschf{\bidistance^{\varphi}_{\latentpolicy}}} \left| {f\fun{\state} - f\fun{\embed\fun{\state}}} \right| \tag{by the stationary property} \\
    \leq& \expectedsymbol{\state \sim \stationary{\latentpolicy}} \bidistance^{\varphi}_{\latentpolicy}\fun{\state, \embed\fun{\state}} \tag{since $f \in \Lipschf{\bidistance^{\varphi}_{\latentpolicy}}$}.
\end{align*}
This finally yields 
\begin{align*}
     (1 - \discount) {\expectedsymbol{\state \sim \stationary{\latentpolicy}} \bidistance^{\varphi}_{\latentpolicy}\fun{\state, \embed\fun{\state}}} &\leq \fun{1 - \discount} \localrewardloss{\stationary{\latentpolicy}} \cdot \condition{\set{\rewards}}\fun{\varphi} + \discount \localtransitionloss{\stationary{\latentpolicy}} K_\varphi \\
    \equiv  \expectedsymbol{\state \sim \stationary{\latentpolicy}} \bidistance^{\varphi}_{\latentpolicy}\fun{\state, \embed\fun{\state}} &\leq \localrewardloss{\stationary{\latentpolicy}} \cdot \condition{\set{\rewards}}\fun{\varphi} + \discount \localtransitionloss{\stationary{\latentpolicy}} \frac{ K_\varphi }{1 - \discount}.
\end{align*}
\end{proof}

\subsection{Abstraction Quality as Bounded Bisimulation Distance}
\begin{lemma}\label{lemma:bidistance-abstraction-quality}
Let $\latentpolicy \in \latentmpolicies$, assume $\latentmdp$ is discrete and $\langle\KR{\latentpolicy}, \KP{\latentpolicy}\rangle$-Lipschitz.
Let $\stationary{\latentpolicy}$ be the stationary distribution of $\mdp$ induced by $\latentpolicy$.
Then, for any pair of states $\state_1, \state_2 \in \states$ such that $\embed\fun{\state_1} = \embed\fun{\state_2}$,
\begin{align*}
    \bidistance^{\rewards}_{\latentpolicy}\fun{\state_1, \state_2} \leq& \left[\localrewardloss{\stationary{\latentpolicy}} + \frac{
    \discount \localtransitionloss{\stationary{\latentpolicy}} \KR{\latentpolicy}}{1 - \discount \KP{\latentpolicy}}\right] \fun{\stationary{\latentpolicy}^{-1}\fun{\state_1} + \stationary{\latentpolicy}^{-1}\fun{\state_2}}, \\
    \bidistance^{\labels}_{\latentpolicy}\fun{\state_1, \state_2} \leq&  \frac{\discount \localtransitionloss{\stationary{\latentpolicy}}}{1 - \discount} \fun{\stationary{\latentpolicy}^{-1}\fun{\state_1} + \stationary{\latentpolicy}^{-1}\fun{\state_2}}.
\end{align*}
\end{lemma}
\begin{proof}
Let $K_{\rewards} = \frac{(1 - \discount) \KR{\latentpolicy}}{1 - \discount \KP{\latentpolicy}}$, $K_{\labels} = 1$, and $\varphi \in \set{\rewards, \labels}$.
We use the fact that $\bidistance^{\varphi}_{\latentpolicy}\fun{\state, \embed\fun{\state}} \leq \stationary{\latentpolicy}^{-1}\fun{\state} \expectedsymbol{\state \sim \stationary{\latentpolicy}}{\bidistance^{\varphi}_{\latentpolicy}\fun{\state, \embed\fun{\state}}}$ for all $\state \in \states$.
Then, we have
\begin{align*}
    &\bidistance^{\varphi}_{\latentpolicy}\fun{\state_1, \state_2}\\
    \leq& \bidistance^{\varphi}_{\latentpolicy}\fun{\state_1, \embed\fun{\state_1}} + \bidistance^{\varphi}_{\latentpolicy}\fun{\embed\fun{\state_1}, \embed\fun{\state_2}} + \bidistance^{\varphi}_{\latentpolicy}\fun{\embed\fun{\state_2}, \state_2} \tag{triangular inequality} \\
    \leq& \stationary{\latentpolicy}^{-1}\fun{\state_1} \expectedsymbol{\state \sim \stationary{\latentpolicy}}{\bidistance^{\varphi}_{\latentpolicy}\fun{\state_1, \embed\fun{\state_1}}} + \stationary{\latentpolicy}^{-1}\fun{\state_2} \expectedsymbol{\state \sim \stationary{\latentpolicy}}{\bidistance^{\varphi}_{\latentpolicy}\fun{\state_2, \embed\fun{\state_2}}} + \bidistance^{\varphi}_{\latentpolicy}\fun{\embed\fun{\state_1}, \embed\fun{\state_2}} \\
    \leq& \fun{\stationary{\latentpolicy}^{-1}\fun{\state_1} + \stationary{\latentpolicy}^{-1}\fun{\state_2}} \fun{\localrewardloss{\stationary{\latentpolicy}} \cdot \condition{\set{\rewards}}\fun{\varphi} + \discount \localtransitionloss{\stationary{\latentpolicy}} \frac{ K_\varphi }{1 - \discount}} + \bidistance^{\varphi}_{\latentpolicy}\fun{\embed\fun{\state_1}, \embed\fun{\state_2}} \tag{Lemma~\ref{lemma:bidistance-embedding}} \\
    =& \fun{\stationary{\latentpolicy}^{-1}\fun{\state_1} + \stationary{\latentpolicy}^{-1}\fun{\state_2}} \fun{\localrewardloss{\stationary{\latentpolicy}} \cdot \condition{\set{\rewards}}\fun{\varphi} + \discount \localtransitionloss{\stationary{\latentpolicy}} \frac{ K_\varphi }{1 - \discount}}. \tag{$\embed\fun{\state_1} = \embed\fun{\state_2}$ by assumption}
\end{align*}
\end{proof}

\subsection{Value Difference Bound}
\begin{lemma}\label{lemma:qvalues-diff-bound}
    Let $\latentpolicy \in \latentmpolicies$ and assume $\latentmdp$ is discrete and $\langle \KR{\latentpolicy}, \KP{\latentpolicy} \rangle$-Lipschitz.
    Let $\labelset{C}, \labelset{T} \subseteq \atomicprops$, $\varphi \in \set{\until{\labelset{C}}{\labelset{T}}, \eventually \labelset{T}}$ and $\KV = \min\fun{\nicefrac{\Rmax{\latentpolicy}}{1 - \discount}, \nicefrac{\KR{\latentpolicy}}{1 - \discount \KP{\latentpolicy}}}$.
    Then, given the induced stationary distribution $\stationary{\latentpolicy}$ in $\mdp$,
    \begin{align*}
        \expectedsymbol{\state, \latentaction \sim \stationary{\latentpolicy}} \left| \qvalues{\latentpolicy}{}{\state}{\latentaction} - \latentqvalues{\latentpolicy}{}{\embed\fun{\state}}{\latentaction} \right| & \leq \frac{\localrewardloss{\stationary{\latentpolicy}} + \discount \KV \localtransitionloss{\stationary{\latentpolicy}}}{1 - \gamma}, \\ %\tag{\citet{DBLP:conf/icml/GeladaKBNB19}} \\
        \expectedsymbol{\state, \latentaction \sim \stationary{\latentpolicy}} \left| \qvalues{\latentpolicy}{\varphi}{\state}{\latentaction} - \latentqvalues{\latentpolicy}{\varphi}{\embed\fun{\state}}{\latentaction} \right| & \leq \frac{\discount \localtransitionloss{\stationary{\latentpolicy}}}{1 - \discount}.
    \end{align*}
\end{lemma}
%
\begin{proof}
The first result for discounted return follows directly from \citep{DBLP:conf/icml/GeladaKBNB19}.
The choice of $\KV$ is derived from the fact that $\KP{\latentpolicy} \leq \frac{1}{\discount}$ since $\latentmdp$ is discrete:
$\dtvsymbol$ is bounded by one as well as the diameter of $\latentstates$, equipped with the discrete metric.
For the reachability cases, first observe that $\qvalues{\latentpolicy}{\varphi}{\state}{\sampledot} = 1$ when $\state \in \getstates{\labelset{T}} \subseteq \states$ and $\latentqvalues{\latentpolicy}{\varphi}{\latentstate}{\sampledot} = 1$ when $\latentstate \in \getstates{\labelset{T}} \subseteq \latentstates$.
Moreover, if $\varphi = \until{\labelset{C}}{\labelset{T}}$, $\qvalues{\latentpolicy}{\varphi}{\state}{\sampledot} = 0$ when $\state \in \getstates{\labelset{\neg C}} \subseteq \states$ and $\latentqvalues{\latentpolicy}{\varphi}{\latentstate}{\sampledot} = 0$ when $\latentstate \in \getstates{\labelset{\neg C}} \subseteq \latentstates$.
By Assumption~\ref{assumption:labeling}, $\state \in \getstates{\neg \labelset{C}} \cup \getstates{\labelset{T}}$ implies that the value difference is zero since the labels of each state and those of its latent abstraction are the same. 
Therefore, let $\mathbb{I}$ be $\condition{\getstates{\labelset{C}} \cap \getstates{\neg \labelset{T}}}$ if $\varphi = \until{\labelset{C}}{\labelset{T}}$ and $\condition{\getstates{\neg \labelset{T}}}$ if $\varphi = \eventually \labelset{T}$,
\begin{align*}
    & \expectedsymbol{\state, \latentaction \sim \stationary{\latentpolicy}} \left| \qvalues{\latentpolicy}{\varphi}{\state}{\latentaction} - \latentqvalues{\latentpolicy}{\varphi}{\embed\fun{\state}}{\latentaction} \right| \\
    =& \expected{\state, \latentaction \sim \stationary{\latentpolicy}}{ \mathbb{I}\fun{\state} \left| \expected{\state' \sim \probtransitions\fun{\sampledot \mid \state, \latentaction}}{\discount \values{\latentpolicy}{\varphi}{\state'}} - \expected{\latentstate' \sim \latentprobtransitions\fun{\sampledot \mid \embed\fun{\state}, \latentaction}}{\discount \latentvalues{\latentpolicy}{\varphi}{\latentstate'}} \right|} \\
    \leq& \discount \expectedsymbol{\state, \latentaction \sim \stationary{\latentpolicy}} \left| \expectedsymbol{\state' \sim \probtransitions\fun{\sampledot \mid \state, \latentaction}}{\values{\latentpolicy}{\varphi}{\state'}} - \expectedsymbol{\latentstate' \sim \latentprobtransitions\fun{\sampledot \mid \embed\fun{\state}, \latentaction}}{\latentvalues{\latentpolicy}{\varphi}{\latentstate'}}  \right| \\
    \leq& \discount \expectedsymbol{\state, \latentaction \sim \stationary{\latentpolicy}} \left| \expected{\state' \sim \probtransitions\fun{\sampledot \mid \state, \latentaction}}{\values{\latentpolicy}{\varphi}{\state'} - \latentvalues{\latentpolicy}{\varphi}{\embed\fun{\state'}}} + \expectedsymbol{}_{\substack{\latentstate' \sim \latentprobtransitions\fun{\sampledot \mid \embed\fun{\state}, \latentaction} \\ \state' \sim \probtransitions\fun{\sampledot \mid \state, \action}}}\left[{\latentvalues{\latentpolicy}{\varphi}{\embed\fun{\state'}} - \latentvalues{\latentpolicy}{\varphi}{\latentstate'}}\right] \right| \\
    \leq& \discount \expectedsymbol{\state, \latentaction \sim \stationary{\latentpolicy}} \left| \expected{\state' \sim \probtransitions\fun{\sampledot \mid \state, \latentaction}}{\values{\latentpolicy}{\varphi}{\state'} - \latentvalues{\latentpolicy}{\varphi}{\embed\fun{\state'}}} \right| + \discount  \expectedsymbol{\state, \latentaction \sim \stationary{\latentpolicy}} \left|\expectedsymbol{}_{\substack{\latentstate' \sim \latentprobtransitions\fun{\sampledot \mid \embed\fun{\state}, \latentaction} \\ \state' \sim \probtransitions\fun{\sampledot \mid \state, \action}}}\left[{\latentvalues{\latentpolicy}{\varphi}{\embed\fun{\state'}} - \latentvalues{\latentpolicy}{\varphi}{\latentstate'}}\right] \right| \tag{triangular inequality} \\
    \leq& \discount \expectedsymbol{\state, \latentaction \sim \stationary{\latentpolicy}} \left| \expected{\state' \sim \probtransitions\fun{\sampledot \mid \state, \latentaction}}{\values{\latentpolicy}{\varphi}{\state'} - \latentvalues{\latentpolicy}{\varphi}{\embed\fun{\state'}}} \right| \\
    & + \discount  \expectedsymbol{\state, \latentaction \sim \stationary{\latentpolicy}} \left|\expectedsymbol{\state' \sim \probtransitions\fun{\sampledot \mid \state, \latentaction}}{\latentvalues{\latentpolicy}{\varphi}{\embed\fun{\state'}} - \expectedsymbol{\latentstate' \sim \latentprobtransitions\fun{\sampledot \mid \embed\fun{\state}, \latentaction}}\latentvalues{\latentpolicy}{\varphi}{\latentstate'}} \right| \\
    \leq& \discount \expectedsymbol{\state, \latentaction \sim \stationary{\latentpolicy}} \left| \expected{\state' \sim \probtransitions\fun{\sampledot \mid \state, \latentaction}}{\values{\latentpolicy}{\varphi}{\state'} - \latentvalues{\latentpolicy}{\varphi}{\embed\fun{\state'}}} \right| + \discount  \expectedsymbol{\state, \latentaction \sim \stationary{\latentpolicy}} \wassersteindist{\condition{\neq}}{\embed\probtransitions\fun{\sampledot\mid \state, \latentaction}}{\latentprobtransitions\fun{\sampledot \mid \embed\fun{\state}, \latentaction}} \tag{by definition of the Wasserstein dual} \\
    \leq& \discount \expectedsymbol{\state, \latentaction \sim \stationary{\latentpolicy}} \left| \expected{\state' \sim \probtransitions\fun{\sampledot \mid \state, \latentaction}}{\values{\latentpolicy}{\varphi}{\state'} - \latentvalues{\latentpolicy}{\varphi}{\embed\fun{\state'}}} \right| + \discount \localtransitionloss{\stationary{\latentpolicy}} \\
    \leq& \discount \expectedsymbol{\state, \latentaction \sim \stationary{\latentpolicy}} \expectedsymbol{\state' \sim \probtransitions\fun{\sampledot \mid \state, \latentaction}} \left|\values{\latentpolicy}{\varphi}{\state'} - \latentvalues{\latentpolicy}{\varphi}{\embed\fun{\state'}} \right| + \discount \localtransitionloss{\stationary{\latentpolicy}} \tag{Jensen's inequality} \\
    \leq& \discount \expectedsymbol{\state \sim \stationary{\latentpolicy}} \left|\values{\latentpolicy}{\varphi}{\state} - \latentvalues{\latentpolicy}{\varphi}{\embed\fun{\state}} \right| + \discount \localtransitionloss{\stationary{\latentpolicy}} \tag{by the stationnary property} \\
    =& \discount \expectedsymbol{\state, \latentaction \sim \stationary{\latentpolicy}} \left|\qvalues{\latentpolicy}{\varphi}{\state}{\latentaction} - \latentqvalues{\latentpolicy}{\varphi}{\embed\fun{\state}}{\latentaction} \right| + \discount \localtransitionloss{\stationary{\latentpolicy}}
\end{align*}
Therefore,
\begin{align*}
    (1 - \discount) \expectedsymbol{\state, \latentaction \sim \stationary{\latentpolicy}} \left|\qvalues{\latentpolicy}{\varphi}{\state}{\latentaction} - \latentqvalues{\latentpolicy}{\varphi}{\embed\fun{\state}}{\latentaction} \right| &\leq \discount \localtransitionloss{\stationary{\latentpolicy}}, \text{ i.e.},\\
    \expectedsymbol{\state, \latentaction \sim \stationary{\latentpolicy}} \left|\qvalues{\latentpolicy}{\varphi}{\state}{\latentaction} - \latentqvalues{\latentpolicy}{\varphi}{\embed\fun{\state}}{\latentaction} \right| &\leq \frac{\discount \localtransitionloss{\stationary{\latentpolicy}}}{1 - \discount}
\end{align*}
\end{proof}

\subsection{Abstraction Quality as Value Difference Bound}
\begin{lemma}\label{lem:quality-latent-space}
    Let $\latentpolicy \in \latentmpolicies$ and assume $\latentmdp$ is discrete and $\langle \KR{\latentpolicy}, \KP{\latentpolicy} \rangle$-Lipschitz.
    Let $\labelset{C}, \labelset{T} \subseteq \atomicprops$, $\varphi \in \set{\until{\labelset{C}}{\labelset{T}}, \eventually \labelset{T}}$, $\KV = \min\fun{\nicefrac{\Rmax{\latentpolicy}}{1 - \discount}, \nicefrac{\KR{\latentpolicy}}{1 - \discount \KP{\latentpolicy}}}$, and $\stationary{\latentpolicy}$ be the induced stationary distribution in $\mdp$.
    Then, for any pair of states $\state_1, \state_2 \in \states$ such that $\embed\fun{\state_1} = \embed\fun{\state_2}$,
    \begin{align*}
      \left| \values{\latentpolicy}{}{\state_1} {-}
      \values{\latentpolicy}{}{\state_2} \right| &\leq
      \frac{\localrewardloss{\stationary{\latentpolicy}} {+} \discount \KV
      \localtransitionloss{\stationary{\stationary{\latentpolicy}}}}{1 {-}
      \discount} \fun{\stationary{\latentpolicy}^{-1}\fun{\state_1} {+}
      \stationary{\latentpolicy}^{-1}\fun{\state_2}},\\
      %
      \left| \values{\latentpolicy}{\varphi}{\state_1} {-}
      \values{\latentpolicy}{\varphi}{\state_2} \right| &\leq \frac{\discount
      \localtransitionloss{\stationary{\latentpolicy}}}{1 {-} \discount}
      \fun{\stationary{\latentpolicy}^{-1}\fun{\state_1} {+}
      \stationary{\latentpolicy}^{-1}\fun{\state_2}}.
    \end{align*}
\end{lemma}
\begin{proof}
    The discounted return case follows directly from \citet{DBLP:conf/icml/GeladaKBNB19} and Lemma~\ref{lemma:qvalues-diff-bound}.
    For the reachability cases, we similarly use the fact that $\left| \values{\latentpolicy}{\varphi}{\state} -  \latentvalues{\latentpolicy}{\varphi}{\embed\fun{\state}} \right| \leq \stationary{\latentpolicy}^{-1}\fun{\state} \expectedsymbol{\state \sim \stationary{\latentpolicy}}\left| \values{\latentpolicy}{\varphi}{\state} -  \latentvalues{\latentpolicy}{\varphi}{\embed\fun{\state}} \right|$, which gives us
    \begin{align*}
        & \left| \values{\latentpolicy}{\varphi}{\state_1} -  \values{\latentpolicy}{\varphi}{\state_2} \right| \\
        =& \left| \values{\latentpolicy}{\varphi}{\state_1} -  \latentvalues{\latentpolicy}{\varphi}{\embed\fun{\state_1}}  +  \latentvalues{\latentpolicy}{\varphi}{\embed\fun{\state_1}} -  \latentvalues{\latentpolicy}{\varphi}{\embed\fun{\state_2}}  +  \latentvalues{\latentpolicy}{\varphi}{\embed\fun{\state_2}} - \values{\latentpolicy}{\varphi}{\state_2} \right|\\
        \leq& \left| \values{\latentpolicy}{\varphi}{\state_1} -  \latentvalues{\latentpolicy}{\varphi}{\embed\fun{\state_1}} \right| + \left| \latentvalues{\latentpolicy}{\varphi}{\embed\fun{\state_1}} -  \latentvalues{\latentpolicy}{\varphi}{\embed\fun{\state_2}} \right| + \left| \latentvalues{\latentpolicy}{\varphi}{\embed\fun{\state_2}} - \values{\latentpolicy}{\varphi}{\state_2} \right| \tag{triangular inequality} \\
        \leq& \stationary{\latentpolicy}^{-1}\fun{\state_1} \expectedsymbol{\state \sim \stationary{\latentpolicy}} \left| \values{\latentpolicy}{\varphi}{\state_1} -  \latentvalues{\latentpolicy}{\varphi}{\embed\fun{\state_1}} \right| + \left| \latentvalues{\latentpolicy}{\varphi}{\embed\fun{\state_1}} -  \latentvalues{\latentpolicy}{\varphi}{\embed\fun{\state_2}} \right| \\
        & + \stationary{\latentpolicy}^{-1}\fun{\state_2} \expectedsymbol{\state \sim \stationary{\latentpolicy}} \left| \latentvalues{\latentpolicy}{\varphi}{\embed\fun{\state_2}} - \values{\latentpolicy}{\varphi}{\state_2} \right| \\
        =& \stationary{\latentpolicy}^{-1}\fun{\state_1} \expectedsymbol{\state \sim \stationary{\latentpolicy}} \left| \values{\latentpolicy}{\varphi}{\state_1} -  \latentvalues{\latentpolicy}{\varphi}{\embed\fun{\state_1}} \right| + \stationary{\latentpolicy}^{-1}\fun{\state_2} \expectedsymbol{\state \sim \stationary{\latentpolicy}} \left| \values{\latentpolicy}{\varphi}{\state_2} -  \latentvalues{\latentpolicy}{\varphi}{\embed\fun{\state_2}} \right| \tag{$\embed\fun{\state_1} = \embed\fun{\state_2}$ by assumption} \\
        \leq& \frac{\discount \localtransitionloss{\stationary{\latentpolicy}}}{1 - \discount} \fun{\stationary{\latentpolicy}^{-1}\fun{\state_1} + \stationary{\latentpolicy}^{-1}\fun{\state_2}} \tag{by Lemma~\ref{lemma:qvalues-diff-bound}}
    \end{align*}
\end{proof}
\subsection{Transition-reward function}
In general RL environments, it is quite common for rewards to be generated from a reward function defined over transitions rather than over state-action pairs., i.e., $\rewards \colon \states \times \actions \times \states$.
In that case, we have $\rewards\fun{\state, \action} = \expectedsymbol{\state' \sim \probtransitions\fun{\sampledot \mid \state, \action}}{\rewards\fun{\state, \action, \state'}}$.
Taking transition-rewards into account, we set up an upper bound on $\localrewardloss{\stationary{\latentpolicy}}$ that can be efficiently estimated from samples (using Lemma~\ref{lemma:pac-loss}).
\begin{lemma}
Let $\stationary{\latentpolicy}$ be a stationary distribution of $\mdp$ under $\latentpolicy \in \latentmpolicies$, then
\begin{align*}
    &\localrewardloss{\stationary{\latentpolicy}} \leq \expectedsymbol{\state, \latentaction, \state' \sim \stationary{\latentpolicy}}\left| \rewards\fun{\state, \latentaction, \state'} - \latentrewards\fun{\embed\fun{\state}, \latentaction} \right| = \localrewardlossupper{\stationary{\latentpolicy}}.
\end{align*}
\end{lemma}
\begin{proof}
The upper bound $\localrewardlossupper{\stationary{\latentpolicy}}$ on the local reward loss is given as follows:
\begin{align*}
    & \expectedsymbol{\state, \latentaction \sim \stationary{\latentpolicy}}\left| \rewards\fun{\state, \latentaction} - \latentrewards\fun{\embed\fun{\state}, \latentaction} \right| \\
    =& \expectedsymbol{\state, \latentaction \sim \stationary{\latentpolicy}}\left| \expectedsymbol{\state' \sim \probtransitions\fun{\sampledot \mid \state, \latentaction}}\rewards\fun{\state, \latentaction, \state'} - \latentrewards\fun{\embed\fun{\state}, \latentaction} \right| \\
    \leq& \expectedsymbol{\state, \latentaction \sim \stationary{\latentpolicy}} \expectedsymbol{\state' \sim \probtransitions\fun{\sampledot \mid \state, \latentaction}} \left|\rewards\fun{\state, \latentaction, \state'} - \latentrewards\fun{\embed\fun{\state}, \latentaction} \right| \tag{Jensen's inequality}\\
    =& \expectedsymbol{\state, \latentaction, \state' \sim \stationary{\latentpolicy}} \left|\rewards\fun{\state, \latentaction, \state'} - \latentrewards\fun{\embed\fun{\state}, \latentaction} \right|
\end{align*}
\end{proof}
This ensures Lem.~\ref{lemma:pac-loss} and Thm.~\ref{thm:pac-qvaluedif} to remain valid by approximating $\localrewardlossupper{\stationary{\latentpolicy}}$ instead of $\localrewardloss{\stationary{\latentpolicy}}$.

\subsection{Local Transition Loss Upper Bound}
Recall that to deal with general MDPs, we study
an upper bound on $\localtransitionloss{\stationary{\latentpolicy}}$ (namely, $\localtransitionlossupper{\stationary{\latentpolicy}}$) that can be efficiently approximated from samples.
We derive it in the following Lemma.
\begin{lemma}\label{lemma:local-loss-upper-bounds}
Let $\stationary{\latentpolicy}$ be a stationary distribution of $\mdp$ under $\latentpolicy \in \latentmpolicies$, then
\begin{align*}
    %&\localrewardloss{\stationary{\latentpolicy}} \leq \expectedsymbol{\state, \latentaction, \state' \sim \stationary{\latentpolicy}}\left| \rewards\fun{\state, \latentaction, \state'} - \latentrewards\fun{\embed\fun{\state}, \latentaction} \right| = \localrewardlossupper{\stationary{\latentpolicy}}, \\ %\label{eq:rewardloss-upperbound} \\
    %\wassersteindist{\distance_{\latentstates}}{\embed\stationary{\latentpolicy}}{\latentprobtransitions_{\embed}\stationary{\latentpolicy}} \leq \,
    &\localtransitionloss{\stationary{\latentpolicy}} \leq \expectedsymbol{\state, \latentaction, \state' \sim \stationary{\latentpolicy}} \wassersteindist{\distance_{\latentstates}}{\embed\fun{\sampledot \mid \state'}}{\latentprobtransitions\fun{\sampledot \mid \embed\fun{\state}, \latentaction}} = \localtransitionlossupper{\stationary{\latentpolicy}} %\label{eq:transitionloss-bounds}
\end{align*}
where we write $\stationary{\latentpolicy}\fun{\state, \latentaction, \state'}  =  \stationary{\latentpolicy}\fun{\state, \latentaction} \cdot \probtransitions\fun{\state' \mid \state, \latentaction}$
and $\embed\fun{\latentstate \mid \state} = \condition{=}\fun{\embed\fun{\state}, \latentstate}$. %$\embed\stationary{\latentpolicy}\fun{\latentstate} = \expectedsymbol{\state \sim \stationary{\latentpolicy}}{\embed\fun{\latentstate \mid \state}}$, and
% $\latentprobtransitions_\embed\steadystate{\policy}\fun{\latentstate} = \expectedsymbol{\state, \latentaction \sim \stationary{\latentpolicy}}{\latentprobtransitions\fun{\latentstate \mid \embed\fun{\state}, \latentaction}}$.
\end{lemma}
\begin{proof}
% We then prove the LHS of the inequality~\ref{eq:transitionloss-bounds}.
% We have 
% \begin{align*}
    % & \expectedsymbol{\state, \latentaction \sim \stationary{\latentpolicy}} \wassersteindist{\distance_{\latentstates}}{\embed\probtransitions\fun{\sampledot \mid \state, \latentaction}}{\latentprobtransitions\fun{\sampledot \mid \embed\fun{\state}, \latentaction}} \\
%     =& \expectedsymbol{\state, \latentaction \sim \stationary{\latentpolicy}}{\sup_{f \in \Lipschf{\distance_{\latentstates}}} \left| \expectedsymbol{\latentstate_1 \sim \embed\probtransitions\fun{\sampledot \mid \state, \latentaction}} f\fun{\latentstate_1} - \expectedsymbol{\latentstate_2 \sim \latentprobtransitions\fun{\sampledot \mid \embed\fun{\state}, \latentaction}} f\fun{\latentstate_2} \right|} \\
%     \geq & {\sup_{f \in \Lipschf{\distance_{\latentstates}}} \expectedsymbol{\state, \latentaction \sim \stationary{\latentpolicy}} \left| \expectedsymbol{\latentstate_1 \sim \embed\probtransitions\fun{\sampledot \mid \state, \latentaction}} f\fun{\latentstate_1} - \expectedsymbol{\latentstate_2 \sim \latentprobtransitions\fun{\sampledot \mid \embed\fun{\state}, \latentaction}} f\fun{\latentstate_2} \right|} \\
%     \geq & {\sup_{f \in \Lipschf{\distance_{\latentstates}}} \left| \expected{\state, \latentaction \sim \stationary{\latentpolicy}}{\expectedsymbol{\latentstate_1 \sim \embed\probtransitions\fun{\sampledot \mid \state, \latentaction}} f\fun{\latentstate_1} - \expectedsymbol{\latentstate_2 \sim  \latentprobtransitions\fun{\sampledot \mid \embed\fun{\state}, \latentaction}} f\fun{\latentstate_2}} \right|} \\
%     =& {\sup_{f \in \Lipschf{\distance_{\latentstates}}} \left|  \expectedsymbol{\state, \latentaction \sim \stationary{\latentpolicy}}\, \expectedsymbol{\state' \sim \probtransitions\fun{\sampledot \mid \state, \latentaction}} f\fun{\embed\fun{\state'}} - \expectedsymbol{\state, \latentaction \sim \stationary{\latentpolicy}}\,\expectedsymbol{\latentstate_2 \sim  \latentprobtransitions\fun{\sampledot \mid \embed\fun{\state}, \latentaction}} f\fun{\latentstate_2} \right|}\\
%     =& {\sup_{f \in \Lipschf{\distance_{\latentstates}}} \left|  \expectedsymbol{\state \sim \stationary{\latentpolicy}} f\fun{\embed\fun{\state}} - \expectedsymbol{\latentstate_2 \sim \latentprobtransitions_{\embed}\stationary{\latentpolicy}} f\fun{\latentstate_2} \right|} \tag{by the stationary property}\\
%     =& {\sup_{f \in \Lipschf{\distance_{\latentstates}}} \left| \expectedsymbol{\latentstate_1 \sim \embed\stationary{\latentpolicy}} f\fun{\latentstate_1} - \expectedsymbol{\latentstate_2 \sim \latentprobtransitions_{\embed}\stationary{\latentpolicy}} f\fun{\latentstate_2} \right|} \\
%     =& \wassersteindist{\distance_{\latentstates}}{\embed\stationary{\latentpolicy}}{\latentprobtransitions_{\embed}\stationary{\latentpolicy}}
% \end{align*}
%The RHS inequality, i.e.,
The upper bound $\localtransitionlossupper{\stationary{\latentpolicy}}$ on the local transition loss is obtained as follows.
\begin{align*}
    & \expectedsymbol{\state, \latentaction \sim \stationary{\latentpolicy}}{\sup_{f \in \Lipschf{\distance_{\latentstates}}} \left| \expectedsymbol{\latentstate_1 \sim \embed\probtransitions\fun{\sampledot \mid \state, \latentaction}} f\fun{\latentstate_1} - \expectedsymbol{\latentstate_2 \sim \latentprobtransitions\fun{\sampledot \mid \embed\fun{\state}, \latentaction}} f\fun{\latentstate_2} \right|} \\
    =& \expectedsymbol{\state, \latentaction \sim \stationary{\latentpolicy}}{\sup_{f \in \Lipschf{\distance_{\latentstates}}} \left| \expectedsymbol{\state' \sim \probtransitions\fun{\sampledot \mid \state, \latentaction}} f\fun{\embed\fun{\state'}} - \expectedsymbol{\latentstate_2 \sim \latentprobtransitions\fun{\sampledot \mid \embed\fun{\state}, \latentaction}} f\fun{\latentstate_2} \right|} \\
    =& \expectedsymbol{\state, \latentaction \sim \stationary{\latentpolicy}}{\sup_{f \in \Lipschf{\distance_{\latentstates}}} \left| \expected{\state' \sim \probtransitions\fun{\sampledot \mid \state, \latentaction}}{f\fun{\embed\fun{\state'}} - \expectedsymbol{\latentstate_2 \sim \latentprobtransitions\fun{\sampledot \mid \embed\fun{\state}, \latentaction}} f\fun{\latentstate_2}} \right|} \\
    \leq& \expectedsymbol{\state, \latentaction \sim \stationary{\latentpolicy}}\sup_{f \in \Lipschf{\distance_{\latentstates}}} \expectedsymbol{\state' \sim \probtransitions\fun{\sampledot \mid \state, \latentaction}} \left|f\fun{\embed\fun{\state'}} - \expectedsymbol{\latentstate_2 \sim \latentprobtransitions\fun{\sampledot \mid \embed\fun{\state}, \latentaction}} f\fun{\latentstate_2} \right| \tag{Jensen's inequality}\\
    \leq& \expectedsymbol{\state, \latentaction \sim \stationary{\latentpolicy}} \expectedsymbol{\state' \sim \probtransitions\fun{\sampledot \mid \state, \latentaction}} \sup_{f \in \Lipschf{\distance_{\latentstates}}} \left|f\fun{\embed\fun{\state'}} - \expectedsymbol{\latentstate_2 \sim \latentprobtransitions\fun{\sampledot \mid \embed\fun{\state}, \latentaction}} f\fun{\latentstate_2} \right| \\
    =& \expectedsymbol{\state, \latentaction, \state' \sim \stationary{\latentpolicy}} \sup_{f \in \Lipschf{\distance_{\latentstates}}} \left|\expectedsymbol{\latentstate_1 \sim {\embed\fun{\sampledot \mid \state'}}} f\fun{{\latentstate_1}} - \expectedsymbol{\latentstate_2 \sim \latentprobtransitions\fun{\sampledot \mid \embed\fun{\state}, \latentaction}} f\fun{\latentstate_2} \right| \\
    =& \expectedsymbol{\state, \latentaction, \state' \sim \stationary{\latentpolicy}} \wassersteindist{\distance_{\latentstates}}{\embed\fun{\sampledot \mid \state'} }{\latentprobtransitions\fun{\sampledot \mid \embed\fun{\state}, \latentaction}}
\end{align*}
\end{proof}

\subsection{Proof of Lemma~\ref{lemma:pac-loss}}
\begin{proof}
Since $\mdp$ is ergodic, the agent acts in the BSCC induced by $\latentpolicy$ and each tuple $\tuple{\state_t, \action_t, \reward_t, \state_{t + 1}}$ can be considered to be drawn independently from the stationary distribution $\stationary{\latentpolicy}$ of the BSCC in the limit.
The result follows then from the Hoeffding's bound \citep{10.2307/2282952:hoeffding}:
\begin{itemize}
    \item consider the sequence of independent random variables $\seq{X^\rewards}{T - 1}$ such that \[X^{\rewards}_t = |\reward_t - \latentrewards\fun{\embed\fun{\state_t}, \latentaction_t}| \in \mathopen[0, 1\mathclose],\] then
    $\localrewardlossapprox{\stationary{\latentpolicy}} = \frac{1}{T} \sum_{t=0}^{T-1} X^\rewards_t$ and
    $\Pr\fun{ \left|\localrewardloss{\star} - \localrewardlossapprox{\stationary{\latentpolicy}} \right| \geq \error} \leq \proberror_\rewards$, with $\proberror_\rewards = 2 e^{-2T\error^2}$ and $\localrewardloss{\star} = \localrewardloss{\stationary{\latentpolicy}}$ if $\rewards$ is defined over state-action pairs and $\localrewardloss{\star} = \localrewardlossupper{\stationary{\latentpolicy}}$ if it is defined over transitions.
    \item consider the sequence of random variables $\seq{X^{\probtransitions}}{T - 1}$ such that
    \[
        X^{\probtransitions}_t =  1 - \latentprobtransitions\fun{\embed\fun{\state_{t + 1}} \mid \embed\fun{\state_t}, \latentaction_t} \in \mathopen[0, 1 \mathclose],
    \]
    The latent MDP $\latentmdp$ being discrete, we have
    \begin{align*}
        \localtransitionlossupper{\stationary{\latentpolicy}} & = \expectedsymbol{\state, \latentaction, \state' \sim \stationary{\latentpolicy}} \dtv{\embed\fun{\sampledot \mid \state'}}{\latentprobtransitions\fun{\sampledot \mid \embed\fun{\state}, \action}} \\
        & = \expected{\state, \latentaction, \state' \sim \stationary{\latentpolicy}}{ \frac{1}{2} \sum_{\latentstate' \in \states} \left| \embed\fun{\latentstate' \mid \state'} - \latentprobtransitions\fun{\latentstate' \mid \embed\fun{\state}, \latentaction} \right|} \\
        & = \expected{\state, \latentaction, \state' \sim \stationary{\latentpolicy}}{ \frac{1}{2} \cdot \fun{\fun{1 - \latentprobtransitions\fun{\embed\fun{\state'} \mid \embed\fun{\state}, \latentaction}} + \sum_{\latentstate' \in \states \setminus\set{\embed\fun{\state'}}} \left| 0 - \latentprobtransitions\fun{\latentstate' \mid \embed\fun{\state}, \latentaction} \right|}}
        % \tag{because $\embed\fun{\latentstate' \mid \state'} = \condition{=}\fun{\embed\fun{\state'}, \latentstate'}$}
        \tag{because $\embed\fun{\latentstate' \mid \state'} = 1$ if $ \embed\fun{\state'} = \latentstate'$ and $0$ otherwise.} \\
        & = \expected{\state, \latentaction, \state' \sim \stationary{\latentpolicy}}{ \frac{1}{2} \cdot \fun{\fun{1 - \latentprobtransitions\fun{\embed\fun{\state'} \mid \embed\fun{\state}, \latentaction}} + \sum_{\latentstate' \in \states \setminus\set{\embed\fun{\state'}}} \latentprobtransitions\fun{\latentstate' \mid \embed\fun{\state}, \latentaction}}} \\
        & = \expected{\state, \latentaction, \state' \sim \stationary{\latentpolicy}}{\frac{1}{2} \cdot 2 \cdot \fun{1 - \latentprobtransitions\fun{\embed\fun{\state'} \mid \embed\fun{\state}, \latentaction}}} \\
        & = \expected{\state, \latentaction, \state' \sim \stationary{\latentpolicy}}{1 - \latentprobtransitions\fun{\embed\fun{\state'} \mid \embed\fun{\state}, \latentaction}}
    \end{align*}
    then $\localtransitionlossapprox{\stationary{\latentpolicy}} = \frac{1}{T}\sum_{t = 0}^{T - 1} X^{\probtransitions}_{t}$ and $\Pr\fun{ \left| \localtransitionlossupper{\stationary{\latentpolicy}} - \localtransitionlossapprox{\stationary{\latentpolicy}} \right| \geq \error} \leq \proberror_\probtransitions$, with $\proberror_\probtransitions = 2e^{-2T\error^2}$.
\end{itemize}
Therefore,
\begin{align*}
    & \Pr\fun{\left|\localrewardloss{\star} - \localrewardlossapprox{\stationary{\latentpolicy}} \right| \geq \error \vee \left| \localtransitionlossupper{\stationary{\latentpolicy}} - \localtransitionlossapprox{\stationary{\latentpolicy}} \right| \geq \error} \\
    & \leq \proberror_{\rewards} + \proberror_{\probtransitions} \\
    & = 2 e^{-2 T \error^2} + 2e^{-2 T \error^2} \\
    & = 4 e^{-2T\error^2}.
\end{align*}
We thus consider $\proberror \geq 4e^{-2T\error^2}$, i.e., $\log\fun{\frac{\proberror}{4}} \geq -2T\error^2$ and $T \geq \frac{- \log\fun{\frac{\proberror}{4}}}{2 \error^2}$, which yields
\[ \Pr\fun{\left|\localrewardloss{\star} - \localrewardlossapprox{\stationary{\latentpolicy}} \right| \leq \error \wedge \left| \localtransitionlossupper{\stationary{\latentpolicy}} - \localtransitionlossapprox{\stationary{\latentpolicy}} \right| \leq \error} \geq 1 - \proberror.\]
\end{proof}

\subsection{Proof of Theorem~\ref{thm:pac-qvaluedif}}

\begin{proof}
Let $\error' \in \mathopen]0, 1\mathclose[$ and $T \geq \Big\lceil \frac{- \log\fun{\frac{\proberror}{4}}}{2 \error'^2} \Big\rceil$, then by applying Lemma~\ref{lemma:qvalues-diff-bound} and Lemma~\ref{lemma:pac-loss}, we have
\begin{align*}
    \expectedsymbol{\state, \latentaction \sim \stationary{\latentpolicy}} \left| \qvalues{\latentpolicy}{}{\state}{\latentaction} - \latentqvalues{\latentpolicy}{}{\embed\fun{\state}}{\latentaction} \right| & \leq & \frac{\localrewardloss{\stationary{\latentpolicy}} + \discount \KV \localtransitionloss{\stationary{\latentpolicy}}}{1 - \gamma} &\leq \frac{\fun{\localrewardlossapprox{\stationary{\latentpolicy}} + \error'} + \discount \KV \fun{\localtransitionlossapprox{\stationary{\latentpolicy}} + \error'}}{1 - \gamma}, \text{ and} \\ 
    \expectedsymbol{\state, \latentaction \sim \stationary{\latentpolicy}} \left| \qvalues{\latentpolicy}{\varphi}{\state}{\latentaction} - \latentqvalues{\latentpolicy}{\varphi}{\embed\fun{\state}}{\latentaction} \right| & \leq & \frac{\discount \localtransitionloss{\stationary{\latentpolicy}}}{1 - \discount} \quad \; \quad &\leq \frac{\discount \fun{\localtransitionlossapprox{\stationary{\latentpolicy}} + \error'}}{1 - \discount}
\end{align*}
with probability at least $1 - \proberror$.
To offer an error of at most $\error$, we need
\begin{align*}
    && \frac{\error' + \discount \KV \error'}{1 - \discount} &\leq \error& \\
    &\equiv& \error' + \discount \KV \error' &\leq \error (1 - \discount)& \\
    &\equiv& \error'(1 + \discount \KV) &\leq \error (1 - \discount)& \\
    &\equiv& \error' &\leq \frac{\error (1 - \discount)}{1 + \discount \KV}&
\end{align*}
which yields the result.
\end{proof}

\section{Details on Variational MDPs}
\smallparagraph{ELBO.} In this work, we focus on the case where
$\dklsymbol$ is used as divergence measure.
The goal is thus to optimize $\min_{\decoderparameter}
\dkl{\mdp_{\policy}}{\decoder}$, i.e., maximizing the marginal
log-likelihood of traces of the original MDP $\expected{\trace \sim \mdp_\policy}{\log \decoder\fun{\trace}}$.
Recall that
% \begin{equation}
%     \expected{\trace \sim \mdp_\policy}{\log \decoder\fun{\trace}} 
%     \text{ s.t. }
%     \decoder\fun{\trace} =
%         \begin{cases}
%             \int_{\inftrajectories{\latentmdp_{\decoderparameter}}} \decoder\fun{\trace \mid \seq{\latentstate}{T}} \, d\latentprobtransitions_{\latentpolicy_{ \decoderparameter}}\fun{\seq{\latentstate}{T}} & \text{if } \latentactions = \actions, \\ 
%             \int_{\inftrajectories{\latentmdp_{\decoderparameter}}} \decoder\fun{\trace \mid \seq{\latentstate}{T}, \seq{\latentaction}{T - 1}} \, d\latentprobtransitions_{\latentpolicy_{ \decoderparameter}}\fun{\seq{\latentstate}{T}, \seq{\latentaction}{T - 1}} & \text{else.} 
%         \end{cases} \label{eq:maximum-likelihood}
% \end{equation}
\begin{equation}
    \decoder\fun{\trace} =
            \int_{\inftrajectories{\latentmdp_{{\decoderparameter}}}} \decoder\fun{\trace \mid \seq{\latentvariable}{T}} \, d\latentprobtransitions_{\latentpolicy_{ \decoderparameter}}\fun{\seq{\latentvariable}{T}} \label{eq:maximum-likelihood2}
\end{equation}
where $\trace = \defaulttrace$, $\latentvariable \in \latentvariables$ such that $\latentvariables = \latentstates$ if $\latentactions = \actions$ and $\latentvariables = \latentstates \times \latentactions$ otherwise,
$\latentprobtransitions_{\latentpolicy_{\decoderparameter}}\fun{\seq{\latentstate}{T}} = \prod_{t = 0}^{T - 1} \latentprobtransitions_{\latentpolicy_{\decoderparameter}}\fun{\latentstate_{t + 1} \mid \latentstate_t}$, and $\latentprobtransitions_{\latentpolicy_{ \decoderparameter}}\fun{\seq{\latentstate}{T}, \seq{\latentaction}{T-1}} = \prod_{t = 0}^{T - 1} {\latentpolicy_{\decoderparameter}}\fun{\latentaction_t \mid \latentstate_t} \cdot \latentprobtransitions_{\decoderparameter}\fun{\latentstate_{t + 1} \mid \latentstate_t, \latentaction_t}$.
%Note that
The dependency of $\trace$ on $\latentvariables$ in Eq.~\ref{eq:maximum-likelihood2}
is made explicit by the law of total probability.

Optimizing $\expected{\trace \sim \mdp_\policy}{\log \decoder\fun{\trace}}$ through Eq.~\ref{eq:maximum-likelihood2} is typically intractable \citep{DBLP:journals/corr/KingmaW13}.
% Intuitively, one may want to make use of \emph{Monte-Carlo sampling methods} to approximate the likelihood of $\trace$ through Eq.~\ref{eq:intractable-latent}
% by generating a finite number of latent sequences $\set{\seq{\latentstate^{\,i}}{T}}_{i=1}^{N}$ from $ \latentprobtransitions_{\decoderparameter}\fun{\sampledot \mid \seq{\action}{T - 1}}$, and then averaging $\frac{1}{N}\sum_{i = 1}^{N}\decoder\fun{\seq{\state}{T}, \seq{\reward}{T - 1}, \seq{\labeling}{T} \mid \seq{\latentstate^{\,i}}{T}, \seq{\action}{T - 1}}$ to approximate $\decoder\fun{\seq{\state}{T}, \seq{\reward}{T - 1}, \seq{\labeling}{T} \mid  \seq{\action}{T - 1}}$.
% Using such methods amounts to trying to approximate an integration over elements from an infinitely uncountable space with a sum over a finite set of elements from this space, meaning that for most $\seq{\latentstate^{\, i}}{T}$, $\decoder\fun{\seq{\state}{T}, \seq{\reward}{T - 1}, \seq{\labeling}{T} \mid \seq{\latentstate^{\, i}}{T}, \seq{\action}{T - 1}}$ will be close to zero thus contributing almost nothing to our estimates.
To overcome this, we use an encoder $\encoder\fun{\seq{\latentvariable}{T} \mid \trace}$ to approximate the intractable true posterior
%\begin{itemize}
    %\item
    $\decoder\fun{\seq{\latentvariable}{T} \mid \trace} = \decoder\fun{\trace \mid \seq{\latentvariable}{T}} \cdot \nicefrac{{}\latentprobtransitions_{\latentpolicy_{ \decoderparameter}}\fun{\seq{\latentvariable}{T}}}{\decoder\fun{\trace}}$. %if $\latentactions = \actions$, and
    %\item
    % $\decoder\fun{\seq{\latentstate}{T}, \seq{\latentaction}{T - 1} \mid \trace} = \decoder\fun{\trace \mid \seq{\latentstate}{T}, \seq{\latentaction}{T - 1}} \cdot \latentprobtransitions_{\latentpolicy_{ \decoderparameter}}\fun{\seq{\latentstate}{T}, \seq{\latentaction}{T - 1}} / \decoder\fun{\trace}$ otherwise.
%\end{itemize}
% giving a distribution over sequences $\seq{\latentstate}{T}$ that are likely to produce $\trace$ to estimate the integration of Eq.~\ref{eq:maximum-likelihood}.
Such an encoder can be learned through the following optimization:
\begin{equation}
    %\min_{\encoderparameter, \decoderparameter} \expected{\trace \sim \mdp_\policy}{\divergence{\encoder\fun{\sampledot \mid \seq{\state}{T}, \seq{\action}{T-1}, \seq{\reward}{T-1}, \seq{\labeling}{T}}}{\decoder\fun{\sampledot \mid \seq{\state}{T}, \seq{\action}{T-1}, \seq{\reward}{T-1}, \seq{\labeling}{T}}}}
    \min_{\encoderparameter, \decoderparameter} \; \expectedsymbol{\trace \sim \mdp_\policy}{\,\divergence{\encoder\fun{\sampledot \mid \trace}}{{\decoder\fun{\sampledot \mid \trace}}},}
\end{equation}
where $\divergencesymbol$ denotes a discrepancy measure.
If we let once again $\divergencesymbol$ be $\dklsymbol$, one can set a \emph{lower bound} on the log-likelihood of produced traces,
%on the $\log$-values of Eq.~\ref{eq:maximum-likelihood}
often referred to as \emph{evidence lower bound} (ELBO, \citealp{DBLP:journals/jmlr/HoffmanBWP13}) as follows:
\begin{align*}
    &&& {\dkl{\encoder\fun{\sampledot \mid \trace}}{{\decoder\fun{\sampledot \mid \trace}}}} \notag \\
    &&= \,& {\expected{\seq{\latentvariable}{T} \sim \encoder\fun{\sampledot \mid \trace}}{\log \encoder\fun{\seq{\latentvariable}{T} \mid \trace} - {\decoder\fun{\seq{\latentvariable}{T} \mid \trace}}}} \notag \\
    && \begin{aligned}
       = \\ \, \\
    \end{aligned} \;& \begin{aligned}
        \expectedsymbol{\seq{\latentvariable}{T} \sim \encoder\fun{\sampledot \mid \trace}}[&\log \encoder\fun{\seq{\latentvariable}{T} \mid \trace} - \log \decoder\fun{\trace \mid \seq{\latentvariable}{T}} - \log \latentprobtransitions_{\latentpolicy_{\decoderparameter}}\fun{\seq{\latentvariable}{T}}] + \log \decoder\fun{\trace}
    \end{aligned} %\tag{Bayes' rule}
    \\
    &\iff&& {\log \decoder\fun{\trace} - \dkl{\encoder\fun{\sampledot \mid \trace}}{{\decoder\fun{\sampledot \mid \trace}}}} \notag \\
    %&&= & \expectedsymbol{\trace \sim \mdp_\policy}\bigg[\expectedsymbol{\seq{\latentstate}{T}, \seq{\latentaction}{T - 1} \sim \encoder}[\log \decoder\fun{\seq{\state}{T}, \seq{\action}{T - 1}, \seq{\reward}{T - 1}\mid \seq{\latentstate}{T}, \seq{\latentaction}{T - 1}}]  \notag \\
    %&&& \quad\quad\quad\quad - \dkl{\encoder\fun{\seq{\latentstate}{T}, \seq{\latentaction}{T-1} \mid \trace}}{\latentprobtransitions_{\decoderparameter}\fun{\seq{\latentstate}{T}, \seq{\latentaction}{T - 1}}}\bigg] \label{eq:ground-action-elbo}
    && \begin{aligned}
        = 
    \end{aligned} \:& \begin{aligned}
        \expectedsymbol{\seq{\latentvariable}{T} \sim \encoder\fun{\sampledot \mid \trace}}[&\log \decoder\fun{\trace \mid \seq{\latentvariable}{T}}  + \log \latentprobtransitions_{\latentpolicy_{ \decoderparameter}}\fun{\seq{\latentvariable}{T}}
        - \log \encoder\fun{\seq{\latentvariable}{T} \mid \trace}]
    \end{aligned} \label{eq:ground-action-elbo}
\end{align*}

\smallparagraph{Behavioral model.} We assume that the behavioral model $\decoder$ allows recovering the latent MDP parameters (i.e., $\latentprobtransitions_\decoderparameter$  $\latentrewards_{\decoderparameter}$ and $\latentlabels_{\decoderparameter}$), by decomposing it into reward and label models, the distilled policy, as well as a generative model $\decoder^{\generative}$ allowing the reconstruction of states and actions.
%, as well as a distillation $\latentpolicy_{\decoderparameter}$ of the policy $\policy$ that allows producing the traces in $\mdp$.
Therefore, states, actions, rewards, and labels are independent given the latent sequence:
\begin{align*}
    &\decoder\fun{\seq{\state}{T}, \seq{\action}{T - 1}, \seq{\reward}{T - 1}, \seq{\labeling}{T} \mid \seq{\latentstate}{T}}\\
    = & \decoder^{\generative}\fun{\seq{\state}{T} \mid \seq{\latentstate}{T}} \cdot \decoder\fun{\seq{\action}{T - 1}, \seq{\reward}{T - 1} \mid \seq{\latentstate}{T}} \cdot \decoder^{\labels}\fun{\seq{\labeling}{T} \mid \seq{\latentstate}{T}} \\
    = & \decoder^{\generative}\fun{\seq{\state}{T} \mid \seq{\latentstate}{T}} \cdot 
    \latentpolicy_{\decoderparameter}\fun{\seq{\action}{T - 1} \mid \seq{\latentstate}{T}} \cdot
    \decoder^{\rewards}\fun{\seq{\reward}{T - 1} \mid \seq{\latentstate}{T}, \seq{\action}{T - 1}} \cdot  \decoder^{\labels}\fun{\seq{\labeling}{T} \mid \seq{\latentstate}{T}}, \tag{by the chain rule, $\decoder\fun{\seq{\action}{T - 1}, \seq{\reward}{T - 1} \mid \seq{\latentstate}{T}} = \latentpolicy_{\decoderparameter}\fun{\seq{\action}{T - 1} \mid \seq{\latentstate}{T}} \cdot
    \decoder^{\rewards}\fun{\seq{\reward}{T - 1} \mid \seq{\latentstate}{T}, \seq{\action}{T - 1}}$}  \\
    & \text{and} \\
    &\decoder\fun{\seq{\state}{T}, \seq{\action}{T - 1}, \seq{\reward}{T - 1}, \seq{\labeling}{T} \mid \seq{\latentstate}{T}, \seq{\latentaction}{T - 1}}\\
    = & \decoder^{\generative}\fun{\seq{\state}{T} \mid \seq{\latentstate}{T}} \cdot \decoder^{\generative}\fun{\seq{\action}{T - 1} \mid \seq{\latentstate}{T},  \seq{\latentaction}{T - 1}} \cdot \decoder^{\rewards}\fun{\seq{\reward}{T - 1} \mid \seq{\latentstate}{T}, \seq{\latentaction}{T - 1}} \cdot \decoder^{\labels}\fun{\seq{\labeling}{T} \mid \seq{\latentstate}{T}}
\end{align*}

\smallparagraph{Local setting.} We formulate the ELBO in the local setting thanks to the following Lemma.
\begin{lemma}\label{lemma:trace-to-stationary}
Let  $\stationary{\policy}$ be the stationary distribution of $\mdp_\policy$ and $f \colon \states \times \actions \times \images{\rewards} \times \states \to \R$ be a continuous function.
Assume $\states, \actions$ are compact, then % induced by $\policy$, then 
\begin{align*}
    \lim_{T \to \infty} \frac{1}{T} \expected{\trace \sim \mdp_{\policy}[T]}{\sum_{t = 0}^{T - 1} f\fun{\state_t, \action_t, \reward_t, \state_{t + 1}}} = \expectedsymbol{\state, \action, \reward, \state' \sim \stationary{\policy}} f\fun{\state, \action, \reward, \state'}
\end{align*}
where $\traces{\mdp}\fun{T}$ denotes the set of traces in $\mdp_{\policy}$ of size $T$, $\mdp_{\policy}[T]$ denotes the distribution over $\traces{\mdp_\policy}\fun{T}$, and $ \stationary{\policy}(\state, \action, \reward, \state') = \stationary{\policy}(\state, \action, \state') \cdot \condition{=}(\reward,  \rewards\fun{\state, \action, \state'})$.
\end{lemma}
\begin{proof}
%Let $\mu_T: \distributions{\states}$, $\mu_T\fun{\state} = \Prob_\policy\fun{\set{\tuple{\seq{\state}{\infty}, \seq{\action}{\infty}} \in \inftrajectories{\mdp_{\policy}} \mid \state_T = \state}}$ be the distribution giving the probability for the agent of being in each particular state of $\mdp_{\policy}$ after exactly $T$ steps. %, and $f \colon \states \times \actions \times \rewards \times \states \to \R$ be a continuous function.
First, observe that
\begin{align*}
    & \expected{\trace \sim \mdp_{\policy}[T]}{\sum_{t = 0}^{T - 1} f\fun{\state_t, \action_t, \reward_t, \state_{t + 1}}} \\
    =&  \expectedsymbol{\action_0 \sim \policy\fun{\sampledot \mid \sinit}}\expectedsymbol{\state_1 \sim \probtransitions\fun{\sampledot \mid \sinit, \action_0}} \expectedsymbol{\action_1 \sim \policy\fun{\sampledot \mid \state_1}} \cdots \expectedsymbol{\state_T \sim \probtransitions\fun{\sampledot \mid \state_{T - 1}, \action_{T - 1}}} \left[ \sum_{t = 0}^{T - 1} f\fun{\state_t, \action_t, \rewards\fun{\state_t, \action_t, \state_{t + 1}}, \state_{t + 1}}\right]\\
    =& \sum_{t = 0}^{T - 1} \expectedsymbol{\action_0 \sim \policy\fun{\sampledot \mid \sinit}}\expectedsymbol{\state_1 \sim \probtransitions\fun{\sampledot \mid \sinit, \action_0}} \expectedsymbol{\action_1 \sim \policy\fun{\sampledot \mid \state_1}} \cdots \expectedsymbol{\state_T \sim \probtransitions\fun{\sampledot \mid \state_{T - 1}, \action_{T - 1}}} f\fun{\state_t, \action_t, \rewards\fun{\state_t, \action_t, \state_{t + 1}}, \state_{t + 1}}\\
    =& \sum_{t = 0}^{T - 1} \expectedsymbol{\trace \sim \mdp_{\policy}[t + 1]} f\fun{\state_t, \action_t, \reward_t, \state_{t + 1}}.
\end{align*}
Second, let $t \in \N, \state, \state' \in \states, \action \in \act{\state},$ and $\reward = \rewards\fun{\state, \action, \state'}$, also observe that
\begin{align*}
    \stationary{\policy}^t\fun{\state \mid \sinit} &= \int_{\traces{\mdp_{\policy}}\fun{t}} \condition{=}\fun{\state_t, \state} \, d\Prob_{\policy}\fun{Cyl\fun{\seq{\state}{t},\seq{\action}{t - 1}}}
    \tag{where $Cyl\fun{\seq{\state}{t}, \seq{\action}{t}} = \set{\tuple{\seq{\state^{\star}}{\infty}, \seq{\action^{\star}}{\infty}} \in \inftrajectories{\mdp_{\policy}} \mid \state^{\star}_{i + 1} = \state_{i + 1}, \action^{\star}_{i} = \action_i \; \forall i \leq t - 1}$} \\
    f\fun{\state, \action, \reward, \state'} \cdot \stationary{\policy}^t\fun{\state \mid \sinit} &=  f\fun{\state, \action, \reward, \state'} \cdot \int_{\traces{\mdp_{\policy}}\fun{t}} \condition{=}\fun{\state_t, \state} \, d\Prob_{\policy}\fun{Cyl\fun{\seq{\state}{t}, \seq{\action}{t - 1}}} \\
    \int_{\states} f\fun{\state, \action, \reward, \state'} \,d\stationary{\policy}^t\fun{\state \mid \sinit} & =  \int_{\states} \int_{\traces{\mdp_{\policy}}\fun{t}} f\fun{\state, \action, \reward, \state'} \cdot \condition{=}\fun{\state_t, \state} \, d\Prob_{\policy}\fun{Cyl\fun{\seq{\state}{t}, \seq{\action}{t - 1}}} \, d\state \\
    & = \int_{\traces{\mdp_{\policy}}\fun{t}} f\fun{\state_t, \action, \reward, \state'} \, d\Prob_{\policy}\fun{Cyl\fun{\seq{\state}{t}, \seq{\action}{t - 1}}}.
\end{align*}
Therefore, we have
\begin{align*}
    & \lim_{T \to \infty} \frac{1}{T} \expected{\trace \sim \mdp_{\policy}[T]}{\sum_{t = 0}^{T - 1} f\fun{\state_t, \action_t, \reward_t, \state_{t + 1}}} \\
    =& \lim_{T \to \infty} \frac{1}{T} \sum_{t = 0}^{T - 1} \expectedsymbol{\trace \sim \mdp_{\policy}[t + 1]} f\fun{\state_t, \action_t, \reward_t, \state_{t + 1}} \\
    =& \lim_{T \to \infty} \frac{1}{T} \sum_{t = 0}^{T - 1} \int_{\traces{\mdp_{\policy}}\fun{t + 1}} f\fun{\state_t, \action_t, \reward_t, \state_{t + 1}} \, d\Prob_\policy\fun{Cyl\fun{\seq{\state}{t + 1}, \seq{\action}{t}}} \\
    =& \lim_{T \to \infty} \frac{1}{T} \sum_{t = 0}^{T - 1} \int_{\states} \int_{\actions} \int_{\states} f\fun{\state, \action, \rewards\fun{\state, \action, \state'}, \state'} \, d\stationary{\policy}^t\fun{\state\mid\sinit} \, d\policy\fun{\action \mid \state} \, d\probtransitions\fun{\state' \mid \state, \action} \\
    =& \lim_{T \to \infty} \int_{\states}  \frac{1}{T} \sum_{t = 0}^{T - 1} \stationary{\policy}^t\fun{d\state \mid \sinit}  \int_{\actions} \int_{\states} f\fun{\state, \action, \rewards\fun{\state, \action, \state'}, \state'} \, d\policy\fun{\action \mid \state} \, d\probtransitions\fun{\state' \mid \state, \action}.
    %=& \lim_{T \to \infty} \frac{1}{T} \sum_{t = 0}^{T - 1} \expectedsymbol{\state \sim \mu_{t}} \expectedsymbol{\action \sim \policy\fun{\sampledot \mid \state}} \expectedsymbol{\state' \sim \probtransitions\fun{\sampledot \mid \state, \action}} f\fun{\state, \action, \rewards\fun{\state, \action, \state'}, \state'} \, d\state
\end{align*}
By definition of $\stationary{\policy}$ and Assumption~\ref{assumption:ergodicity}, $\stationary{\policy}\fun{A} = \lim_{T \to \infty} \frac{1}{T} \sum_{t = 0}^{T - 1} \stationary{\policy}^{t}\fun{A \mid \state}$ for all $\state \in \states$, $A \in \borel{\states}$, i.e., $\frac{1}{T} \sum_{t = 0}^{T - 1} \stationary{\policy}^{t}\fun{A \mid \state}$ weakly converges to $\stationary{\policy}$ (e.g., \citealtAR{DBLP:books/daglib/0020348}).
By Assumption~\ref{assumption:rewards}, the set of images of $\rewards$ is a compact space.
Therefore, $f$ has compact support and by the Portmanteau's Theorem, 
\begin{align*}
    & \lim_{T \to \infty} \int_{\states}  \frac{1}{T} \sum_{t = 0}^{T - 1} \stationary{\policy}^t\fun{d\state \mid \sinit} \int_{\actions} \int_{\states} f\fun{\state, \action, \rewards\fun{\state, \action, \state'}, \state'} \, d\policy\fun{\action \mid \state} \, d\probtransitions\fun{\state' \mid \state, \action} \\
    =& \int_{\states} \stationary{\policy}\fun{d\state} \int_{\actions} \int_{\states} f\fun{\state, \action, \rewards\fun{\state, \action, \state'}, \state'} \, d\policy\fun{\action \mid \state} \, d\probtransitions\fun{\state' \mid \state, \action} \\
    =& \expectedsymbol{\state, \action, \reward, \state' \sim \stationary{\policy}} f\fun{\state, \action, \reward, \state'}.
\end{align*}
\end{proof}

%
\begin{corollary}\label{cor:elbo-local}
Let $ \embed\fun{\latentstate, \latentstate' \mid \state, \state'} = \embed\fun{\latentstate \mid \state} \embed\fun{\latentstate' \mid \state'}$, then
\begin{align*}
    & \lim_{T \to \infty} \frac{1}{T} \expectedsymbol{}_{\substack{\trace \sim \mdp_{\policy}[T] \\ \seq{\latentvariable}{T} \sim \encoder\fun{\sampledot \mid \trace}}} \left[\log \decoder\fun{\trace \mid \seq{\latentvariable}{T}} - \dkl{\encoder\fun{\sampledot \mid \trace}}{{\decoder\fun{ \sampledot \mid \trace}}} \right] \\
    % \begin{aligned}
    % = \\ \, \\ \, \\
    % \end{aligned} & \begin{aligned} \;
    % \lim_{T \to \infty} \frac{1}{T} \expectedsymbol{\trace \sim \mdp_{\policy}[T]} \Bigg[ \sum_{t = 0}^{T - 1} \expectedsymbol{\latentstate_t, \latentstate_{t + 1} \sim \embed_{\encoderparameter}\fun{\sampledot \mid \state_t, \state_{t + 1}}} \big[&{\log \decoder^{\generative}\fun{\state_{t+1} \mid \latentstate_{t + 1}} + \log \decoder^{\rewards}\fun{\reward_{t + 1} \mid \latentstate_t, \action_t}} \\
    % & - \dkl{\embed_{\encoderparameter}\fun{\sampledot \mid \state_{t + 1}}}{\latentprobtransitions_{\decoderparameter}\fun{\sampledot \mid \latentstate_t, \action_t}}\big] \Bigg]
    % \end{aligned} \\
    % --------------------------------------------------------
    % =& \expectedsymbol{}_{\substack{\state, \action, \reward, \state' \sim \stationary{\policy}\\ \latentstate, \latentstate' \sim \embed_{\encoderparameter}\fun{\sampledot \mid \state, \state'}}}\left[{\log \decoder^{\generative}\fun{\state' \mid \latentstate'} + \log \decoder^{\rewards}\fun{\reward \mid \latentstate, \action}} - \dkl{\embed_{\encoderparameter}\fun{\sampledot \mid \state'}}{\latentprobtransitions_{\decoderparameter}\fun{\sampledot\mid \latentstate, \action}}\right].
    % --------------------------------------------------------
    =&
    \left\lbrace\begin{array}{@{}r@{\quad}l@{}l@{}}
    \displaystyle \expectedsymbol{}_{\substack{\state, \action, \reward, \state' \sim \stationary{\policy}\\ \latentstate, \latentstate' \sim \embed_{\encoderparameter}\fun{\sampledot \mid \state, \state'}}} \big[ \log \decoder^{\generative}\fun{\state' \mid \latentstate'} + \log \latentpolicy_{\decoderparameter}\fun{\action \mid \latentstate} + \log \decoder^{\rewards}\fun{\reward \mid \latentstate, \action}& \\[-12pt]
    \hfill - \dkl{\embed_{\encoderparameter}\fun{\sampledot \mid \state'}}{\latentprobtransitions_{\latentpolicy_{\decoderparameter}}\fun{\sampledot\mid \latentstate}} \big]
        & \text{if } \latentactions = \actions, \text{ and} \\[10pt]
    \displaystyle \expectedsymbol{}_{\substack{\state, \action, \reward, \state' \sim \stationary{\policy}\\ \latentstate, \latentstate' \sim \embed_{\encoderparameter}\fun{\sampledot \mid \state, \state'} \\ \latentaction \sim \latentembeda_{\encoderparameter}\fun{\sampledot \mid \latentstate, \action}}} \big[ \log \decoder^{\generative}\fun{\state' \mid \latentstate'} + \log\embeda_{\decoderparameter}\fun{\action \mid \latentstate, \latentaction} + \log \decoder^{\rewards}\fun{\reward \mid \latentstate, \latentaction} & \\[-25pt]
    \hfill - \dkl{\embed_{\encoderparameter}\fun{\sampledot \mid \state'}}{\latentprobtransitions_{\decoderparameter}\fun{\sampledot\mid \latentstate}} \quad \;\, & \\[5pt]
    \hfill - \dkl{\latentembeda_{\encoderparameter}\fun{\sampledot\mid\latentstate, \action}}{\latentpolicy_{\decoderparameter}\fun{\sampledot \mid \latentstate}} \big]
        & \text{otherwise},
    \end{array}
    \right.
\end{align*}
where $\trace = \defaulttrace$, $\seq{\latentvariable}{T} = \seq{\latentstate}{T}$ if $\latentactions = \actions$ and $\seq{\latentvariable}{T} = \tuple{\seq{\latentstate}{T}, \seq{\latentaction}{T - 1}}$ otherwise.
\end{corollary}
\begin{proof}
Taking 
\[f\fun{\state, \action, \reward, \state'} = \expectedsymbol{\latentstate, \latentstate' \sim \embed_{\encoderparameter}\fun{\sampledot \mid \state, \state'}}\big[ g\fun{\tuple{\state, \action, \reward, \state'}, \tuple{\latentstate, \latentstate'}} \big]\]
with
\begin{align*}
    & g\fun{\tuple{\state, \action, \reward, \state'}, \tuple{\latentstate, \latentstate'}}\\
    =& 
    \left\lbrace\begin{array}{@{}r@{\quad}l@{}l@{}}
    \log \fun{\decoder^{\generative}\fun{\state' \mid \latentstate'} \cdot \decoder^{\rewards}\fun{\reward \mid \latentstate, \action} \cdot \latentpolicy_{\decoderparameter}\fun{\action \mid \latentstate}} - \dkl{\embed_{\encoderparameter}\fun{\sampledot \mid \state'}}{\latentprobtransitions_{\latentpolicy_{\decoderparameter}}\fun{\sampledot\mid \latentstate}} & \text{if } \latentactions = \actions, \text{ and} \\[5pt]
    \expectedsymbol{\latentaction \sim \encoder^\actions\fun{\sampledot \mid \latentstate, \action}} \Big[ \log \fun{\decoder^{\generative}\fun{\state' \mid \latentstate'} \cdot \embeda_{\decoderparameter}\fun{\action \mid \latentstate, \latentaction} \cdot \decoder^{\rewards}\fun{\reward \mid \latentstate, \latentaction}} \hfill & \\[3pt]
    - \dkl{\embed_{\encoderparameter}\fun{\sampledot \mid \state'}}{\latentprobtransitions_{\decoderparameter}\fun{\sampledot\mid \latentstate, \latentaction}} \Big] - \dkl{\latentembeda_{\encoderparameter}\fun{\sampledot\mid\latentstate, \action}}{\latentpolicy_{\decoderparameter}\fun{\sampledot \mid \latentstate}} & \text{otherwise}
    \end{array}
    \right.
\end{align*}
yields the result.
\end{proof}

\subsection{Latent Distributions}
\smallparagraph{Discrete latent state space.} W.l.o.g., we assume latent states to have a binary representation, we further see each bit as a Bernoulli random variable.
Let $\logit \in \R$ and $\temperature \in \mathopen]0, 1 \mathclose]$, $\latentstate_\temperature \in \mathopen[0, 1\mathclose]$ has a \emph{relaxed Bernoulli distribution} $\latentstate_\temperature \sim \relaxedbernoulli{\logit}{\temperature}$ with logit $\logit$ and temperature parameter $\temperature$ iff
(a) if $l$ is a logistic sample with location parameter $\logit\temperature^{-1}$ and scale parameter $\temperature^{-1}$, i.e., $l \sim \logistic{\logit\temperature^{-1}}{\temperature^{-1}}$, then $\latentstate_\temperature = \sigmoid\fun{l}$, $\sigmoid$ being the \emph{sigmoid} function,
(b) $\lim_{\temperature \to 0} \relaxedbernoulli{\logit}{\temperature} = \bernoulli{\logit}$, meaning $\Prob(\textstyle\lim_{\temperature \to 0} \latentstate_\temperature = 1) = \sigmoid\fun{\logit}$, and
(c) let $p_{\logit, \temperature}$ be the density of $\relaxedbernoulli{\logit}{\temperature}$, then $p_{\logit, \temperature}\fun{\latentstate_\temperature}$ is log-convex in $\latentstate_\temperature$.
Since the logistic distribution belongs to the location-scale family, it is fully reparameterizable, i.e., $l \sim \logistic{\logit\temperature^{-1}}{\temperature^{-1}} \equiv x \sim \logistic{0}{1} \text{ and } l = \frac{\logit + x}{\temperature}$.
In practice, we train $\embed_{\encoderparameter}$ and $\latentprobtransitions_{\decoderparameter}$ to infer and generate $\log_2 |\latentstates|$ logits, and we anneal $\temperature$ to $0$ during training while using Bernoulli distributions for evaluation.
As suggested by \citet{DBLP:conf/iclr/MaddisonMT17}, we use two different temperature parameters and we fixed their initial values to $\temperature_{\embed_{\encoderparameter}} = \nicefrac{2}{3}$ for the encoder distribution and $\temperature_{\probtransitions_{\decoderparameter}} = \nicefrac{1}{2}$ for the latent transition function.

\smallparagraph{Discrete latent action space.} We learn discrete latent distributions for $\encoder^{\actions}$ and $\latentpolicy_{\decoderparameter}$ by learning the logits of continuous relaxation of discrete distributions, through the \emph{Gumbel softmax trick}.
Let $n = |\latentactions|$, and $\latentactions = \set{\latentaction_1, \dots, \latentaction_n}$.
Drawing $\latentaction_i \sim \categorical{\logits}$  from a discrete (categorical) distribution in $\distributions{\latentactions}$ with logits parameter $\logits \in \R^n$ is equivalent to applying an $\argmax$ operator to the sum of \emph{Gumble samples} and these logits, i.e., $i = \argmax_{k} \logits_k - \log\fun{- \log \vect{\epsilon}_k}$, where $\vect{\epsilon} \in \mathopen[0, 1\mathclose]^n$ is a uniform noise.
The sampling operator being fully reparameterizable here, $\argmax$ is however not derivable, preventing the operation to be optimized through gradient descent.
The proposed solution is to replace the $\argmax$ operator by a softmax function with temperature parameter $\temperature \in \mathopen]0, \left(n - 1\right)^{-1}\mathclose]$.

Concretely, $\vect{x} \in \mathopen[0, 1\mathclose]^n$ has a \emph{relaxed discrete distribution} $\vect{x} \sim \relaxedcategorical{\logits}{\temperature}$ with logits $\logits$, temperature parameter $\temperature$ iff
(a) let $\vect{\epsilon} \in [0, 1]^n$ be a uniform noise and $\vect{G}_k = {- \log \fun{- \log \vect{\epsilon}_k}}$, then $\vect{x}_k = {\frac{\exp{\fun{\fun{\logits_k + \vect{G}_k} \temperature^{-1}}}}{\sum_{i = 1}^{n} \exp{\fun{\fun{\logits_i + \vect{G}_i} \lambda^{-1}}}}}$ for each $k \in [n]$,
(b) $\lim_{\temperature \to 0} \relaxedcategorical{\logits}{\temperature} = \categorical{\logits}$, meaning $\Prob(\textstyle\lim_{\temperature \to 0} \vect{x}_k = 1) = \frac{\exp{\logits_k}}{\sum_{i = 1}^{n}\exp{\logits_i}}$,
and (c) let $p_{\logits, \temperature}$ be the density of $\relaxedcategorical{\logits}{\temperature}$, then $p_{\logits, \temperature}\fun{\vx}$ is log-convex in $\vx$.
In practice, we train $\encoder^{\actions}$ and $\latentpolicy_{\decoderparameter}$ to infer and generate logits parameter $\logits$, and we anneal $\temperature$ from $(n - 1)^{-1}$ to $0$ during training while using discrete distributions for evaluation.
Again, as prescribed by \citet{DBLP:conf/iclr/MaddisonMT17}, we use two different temperature parameters, one for $\encoder^{\actions}$ and another one for $\latentpolicy_{\decoderparameter}$.

\subsection{Prioritized Experience Replay}
%\smallparagraph{Experience replay.}~%
Modern RL techniques allow to learn the parameters $\decoderparameter$ of a policy, model, or value function, by processing encountered experiences $\experience$ 
%encountered in the interaction loop 
via a loss function $\loss\fun{\experience, \decoderparameter}$.
If these
%such experiences 
are processed sequentially and discarded after all parameter updates, the latter are strongly correlated and rare events are quickly forgotten.
\emph{Prioritized experience replay buffers} \citep{DBLP:journals/corr/SchaulQAS15} are finite data structures $\replaybuffer$ that allow to overcome both issues by
(i) storing experiences $\experience$ with some assigned priority $p_{\experience}$, and
(ii) producing $\experience \sim \replaybuffer$ according to $P\fun{\experience} = p_\experience^\varsigma / \sum_{\experience' \in \replaybuffer} p_{\experience'}^\varsigma$, $\varsigma \in \mathopen[0, 1\mathclose]$, which allows minimizing $\expected{\experience \sim \replaybuffer}{w_{\experience} \cdot \loss\fun{\experience, \decoderparameter}}$ for \emph{importance sampling weight} $w_\experience > 0$.
%, and
%(iii) removing $\experience$ from $\replaybuffer$ (when full) according to a distribution $Q$.
%In their simpler form, $P = Q$ with $\alpha = 0$ ($P$ and $Q$ are uniform).
%However, experiences drawn from $\replaybuffer$ can be in that case more or less surprising, redundant, or not being useful for the learning process.
The replay buffer $\replaybuffer$ is \emph{uniform} if $\varsigma = 0$.
Otherwise, when $\varsigma > 0$, a bias is introduced because priorities change the real distribution of experiences and consequently the estimates of the expectation of $\loss$.
This bias is alleviated via $w_{\experience} = \fun{|\replaybuffer| \cdot P\fun{\experience}}^{-\omega}$ for each $\experience \in \replaybuffer$, $\omega \in \mathopen[0, 1\mathclose]$, where $\omega = 1$ fully corrects it.

We use prioritized replay buffers to store transitions and sample them when optimizing our loss function, i.e., $\elbo$.
This allows to alleviate the posterior collapse problem, as shown in Figure~\ref{appendix:fig:prioritized-replay}.
We introduced two priority functions, yielding a bucket-based and a loss-based prioritized replay buffer.
In the following, we elaborate how precisely we assign priorities via the loss-based priority function.

\smallparagraph{Loss-based priorities.}~Draw $\tuple{\state, \action, \reward, \state'} \sim \stationary{\policy}$, we aim at assigning $p_{\tuple{\state, \action, \reward, \state'}}$ to its \emph{individual transition loss}, i.e.,
$\loss_{\elbo}$:
for $\latentstate, \latentstate' \sim \embed_{\encoderparameter}\fun{\sampledot \mid \state, \state'}$, $\latentaction \sim \encoder^\actions\fun{\sampledot \mid \latentstate, \action}$ if $\latentactions \neq \actions$ and $\latentaction = \action$ else,
    \begin{align*}
        & \loss_{\elbo}\fun{\tuple{\state, \action, \reward, \state'}, \tuple{\latentstate, \latentaction, \latentstate'}} \\
        =&  - \log \decoder^\generative\fun{\state' \mid \latentstate'} - \log \decoder^{\rewards}\fun{\reward \mid \latentstate, \latentaction} - \log \latentpolicy_{\decoderparameter}\fun{\latentaction \mid \latentstate} + \log \embed_{\encoderparameter}\fun{\latentstate' \mid \state'} \\
        & - \condition{=}\fun{\latentactions, \actions} \cdot \log \latentprobtransitions_{\latentpolicy_{\decoderparameter}}\fun{\latentstate' \mid \latentstate} - \condition{\neq}\fun{\latentactions, \actions} \cdot \fun{\log \embeda_{\decoderparameter}\fun{\action \mid \latentstate, \latentaction} + \log \latentprobtransitions_{\decoderparameter}\fun{\latentstate' \mid \latentstate, \latentaction} - \log \encoder^{\actions}\fun{\latentaction \mid \latentstate, \action}}.
    \end{align*}
Notice that we cannot assign directly $p_{\tuple{\state, \action, \reward, \state'}}$ to $\loss_{\elbo}$ since priorities require to be strictly positive to compute the replay buffer distribution.
A solution is to pass $\loss_{\elbo}\fun{\tuple{\state, \action, \reward, \state'}, \tuple{\latentstate, \latentaction, \latentstate'}}$ to the logistic function, i.e.,
\[
    p_{\tuple{\state, \action, \reward, \state'}} = x^{\star} \cdot \sigma\fun{k \cdot \fun{\loss_{\elbo}\fun{\tuple{\state, \action, \reward, \state'}, \tuple{\latentstate, \latentaction, \latentstate'}} - x_0}}
\]
where $x^{\star} > 0$ is the scale, $k > 0$ is the growth rate, and $x_0$ is the location of the logistic.
In practice, we set $x^{\star}$ to the desired maximum priority and we maintain upper ($L^{\max}$) and lower bounds ($L^{\min}$) on the loss during learning.
Along training steps, we tune these values according to the current loss and we set $x_0 = \frac{L^{\max} - L^{\min}}{2}$ and $k = \frac{x^{\star}}{L^{\max} - L^{\min}}$.

\begin{figure}
    \begin{subfigure}{0.495\textwidth}
        \centering
        \includegraphics[width=\textwidth]{ressources/cartpole_histogram.pdf} \\
        \caption{CartPole}
    \end{subfigure}
    \begin{subfigure}{0.495\textwidth}
        \centering
        \includegraphics[width=\textwidth]{ressources/mountain_car_histogram.pdf} \\
        \caption{MountainCar}
    \end{subfigure}\\
    \begin{subfigure}{0.495\textwidth}
        \centering
        \includegraphics[width=\textwidth]{ressources/acrobot_histogram.pdf} \\
        \caption{Acrobot}
    \end{subfigure}
    \begin{subfigure}{0.495\textwidth}
        \centering
        \includegraphics[width=\textwidth]{ressources/pendulum_histogram.pdf} \\
        \caption{Pendulum}
    \end{subfigure}
    \caption{Latent space distribution along training steps. The intensity of the blue hue corresponds to the frequency of latent states produced by $\embed$ during training. We compare a bucket-based prioritized against a simple uniform experience replay. The latent space learned with transitions sampled from the uniform replay buffer collapses to (a) two, (b) a single, (c)~and~(d) few latent state(s).}
    \label{appendix:fig:prioritized-replay}
\end{figure}

\section{Details on RL Policy Distillation}
% One can see $\latentpolicy_{\decoderparameter}$ as a \emph{compression} of the input RL policy $\policy$ over $\latentstates$ and $\latentactions$.
Concretely, the goal of $\latentpolicy_{\decoderparameter}$ is to mimic the behavior of $\policy$ through the latent spaces.
However, executing $\latentpolicy_{\decoderparameter}$ in $\mdp$ via $\embed_{\encoderparameter}$ and $\embeda_{\encoderparameter, \decoderparameter}$ as described in Sect.~\ref{section:deepmdp} to evaluate our latent space model (e.g., Lem.~\ref{lemma:pac-loss}) may
% lead the agent to parts of the state space that are not reachable when we execute the original policy $\policy$. %,
result in performance loss compared to that offered under $\policy$.
%with
%$\mdp_{\policy}$ and $\mdp_{\latentpolicy}$ not behaving exactly the same.
% This is due to our abstraction approach: we do not expect a perfect bisimulation between the two models but a suitable bisimulation distance.
This is due to the abstraction learned via the ELBO: we do not expect a zero-distortion by encoding the state-action space through the VAE (and $\mdp_{\latentpolicy_{\decoderparameter}}$ to behave exactly like $\mdp_{\policy}$), but to minimize it.
Due to this remaining distortion, decisions taken according to $\latentpolicy_{\decoderparameter}$ may result in unreachable states under $\policy$, causing $\latentpolicy_{\decoderparameter}$ to behave poorly in that particular states since the learning process only allows to learn from transitions produced via $\policy$.
We propose two approaches that can be used alone or together to alleviate this problem.

\smallparagraph{Globally-robust RL policy.}~%
In general, policies $\policy$ learned through deep-RL provide \emph{local} performance, meaning they provide good performance in states $\state \in \states$ that are likely to be reached from $\sinit$ by executing $\policy$. % (i.e., $\expectedsymbol{\state \sim \stationary{\policy}}{\values{\policy}{}{\state}}$ is good).
It is, however, often suitable to distil and verify \emph{globally-robust} RL policies, i.e., policies trained to provide good \emph{global} performance, in a wider range of states, in particular those that are likely to be reached through the distillation.
%Indeed, policies under consideration should be learned to optimize $\values{\policy}{}{\state}$ for a wider range of states $\state \in \states$ than those reachable from $\sinit$ under $\policy$ in $\mdp$, in particular those that are likely to be reached after the distillation. %perturbations (e.g., those linked to the distortion of $\latentpolicy_{\decoderparameter}$),
Training globally robust RL policies can be achieved by allowing the agent to learn (or pursuing its training) in a modified version of $\mdp$, where $\sinit$ is picked at random from $\states$.
Training $\policy$ this way will usually take longer since this may require more exploration.
The degree of randomness can be decided according to the exploration/exploitation trade-off and the additional training time allowed. %or set uniform if the environment allows it.
Moreover, we argue that this additional time is acceptable in our context since enabling and applying model checking are time-consuming tasks by nature.

\smallparagraph{$\varepsilon$-mimic.}~Assuming $\policy$ is sufficiently robust for the distillation, we allow the encoder to process states reachable under both $\latentpolicy_{\decoderparameter}$ and $\policy$ by learning from a mixture $\hat{\policy}^{\varepsilon}_{\decoderparameter}$ of the two, defined as $\hat{\policy}^{\varepsilon}_{\decoderparameter}\fun{\action \mid \state} = \policy\fun{\action \mid \state} \cdot (1 - \varepsilon) + \latentpolicy_{\decoderparameter}\fun{\action \mid \state} \cdot \varepsilon$ for all $\state \in \states$, $\action \in \actions$, given $\varepsilon \in \mathopen[0, 1 \mathclose]$.
Given the robustness assumption, states entered according to a decision $\action \sim \latentpolicy_{\decoderparameter}\fun{\sampledot \mid \state}$ (produced with probability $\varepsilon$) should be efficiently processed
by $\policy$.
In practice, we start with a high value of $\varepsilon$ (e.g., $\varepsilon=\nicefrac{1}{2}$) to encourage exploration of states possibly reachable under $\latentpolicy_{\decoderparameter}$ and we anneal it to $0$ during training.

\section{Experiments}\label{appendix:experiments}
%
\subsection{Setup}
We used \textsc{TensorFlow} \texttt{2.4.1} \citepAR{tensorflow2015-whitepaper} to implement the neural networks modeling our parameterized distributions constituting our variational model, optimize the loss function ($\elbo$), and running our PAC approximation schemes.
Precisely, we used \textsc{TensorFlow Probability} \texttt{0.12.2} \citepAR{dillon2017tensorflow} to handle the probabilistic components of the VAE (e.g., distributions, reparameterization tricks, etc.), as well as \textsc{TF-Agents} \texttt{0.7.1} \citepAR{TFAgents} to handle the RL parts of the framework, coupled with \textsc{Reverb} replay buffers \citepAR{cassirer2021reverb}.

The experiments have been driven on a cluster running under \texttt{CentOS Linux 7 (Core)} composed of a mix of nodes containing Intel processors with the following CPU microarchitectures:
(i) \texttt{10-core INTEL E5-2680v2}, (ii) \texttt{14-core INTEL E5-2680v4}, and (iii) \texttt{20-core INTEL Xeon Gold 6148}.
We used $8$ cores and $128$ GB of memory for each of our experiments.

\subsection{Hyper-parameters}
Each distribution parameters (locations, scales, logits) are inferred by neural networks (multilayer perceptrons) composed of two dense hidden layers and $256$ cells by layer.
For all our experiments, we used the \textsc{TensorFlow}'s implementation of the Adam optimizer \citepAR{DBLP:journals/corr/KingmaB14} to learn the neural networks weights.
We fixed the size of the prioritized experience replay to $10^{6}$.
The training starts when the agent has collected $10^{4}$ transitions in $\mdp$.
Every time a transition is added to the replay buffer, its priority is set to the maximum value.
We used minibatches of size $128$ to optimize the loss function (i.e., $\elbo$) and we applied a minibatch update every time the agent executing $\policy$ performed $16$ steps in $\mdp$.
This allows the VAE to process at least $\nicefrac{1}{8}$ of the new transitions collected by sampling from the replay buffer (more details in \citealt{DBLP:journals/corr/SchaulQAS15}).
We fixed $\omega$ to $0.4$ as suggested by \citet{DBLP:journals/corr/SchaulQAS15}.

\smallparagraph{Annealing schemes.}~During training, we anneal different hyper-parameters to $0$ or $1$ according to the following annealing schemes:
(i) we start annealing the parameters at step $t_0 = 10^{4}$, 
(ii) let $\zeta \in \mathopen[ 0, \infty \mathclose]$, we anneal $\zeta$ to $0$ via $\zeta_t = \zeta \cdot (1 - \tau_{\zeta})^{t - t_0}$, or
(iii) let $\zeta \in \mathopen[ 0, 1]$, we anneal $\zeta$ to $1$ via $\zeta_{t} = \zeta + (1 - \zeta)\cdot (1 - (1 - \tau_{\zeta})^{t - t_0})$
for step $t > t_0$ and annealing term $\tau_{\zeta}$.

\smallparagraph{Unshared parameters.}~The list of single hyper-parameters used for the different environments is given in Table~\ref{table:hyperparameters}.
The activation function of all cells of the neural networks was chosen between relu and leaky relu, the learning rate of the optimizer in $\mathopen[10^{-4}, 10^{-3}\mathclose]$, the regularizer scale factor $\alpha$ from $\set{0.1, 1, 10, 100}$, $\varsigma$ in $\mathopen[0, 1\mathclose]$, the values of each annealing term in $\mathopen[ 10^{-6}, 10^{-4} \mathclose]$, and we tested both loss- and bucket-based experience replay buffers.

\begin{table}
    \centering
    \input{hyperparameters}
    \caption{Hyper-parameter choices for the different environments.
    We write
    $\temperature^X_Y$ for the temperature parameter used for the concrete relaxation of Bernoulli distributions when $X = \latentstates$ (resp. Categorical distributions when $X = \latentactions$).
    This is the encoder distribution if $Y=1$ and the one of the latent transition function (resp. distilled policy) else, if $Y=2$. 
    Moreover, we write $\alpha_{\scriptscriptstyle \actions}$ for the scale factor of the action entropy regularizer where the final entropy regularization term is $\alpha \cdot \fun{\entropy{\encoder} + \alpha_{\scriptscriptstyle \actions} \cdot \entropy{\encoder^{\actions}}}$.}
    \label{table:hyperparameters}
\end{table}

\subsection{Labeling functions}
The labeling functions $\labels$ we used for each environment are detailed in Table~\ref{appendix:table:labels}.
%
\input{labels}
%
%
\section{Reproducibility Checklist}
\smallparagraph{Seeding.}~The code we provide allows for setting random seeds to run the experiments, making the overall shape of the Figures we present in this paper reproducible.
However, the exact values obtained in each particular instance are not: our prioritized experience replays (and those of \textsc{TF-Agents}) rely on \textsc{Reverb} replay buffers, handling randomness via the \textsc{Abseil} \texttt{C++} library{\footnote{\texttt{https://abseil.io}}} which does not allow for manual seeding.
All other libraries using randomness are seeded properly.

\clearpage
\bibliographystyleAR{aaai22}
\bibliographyAR{references}