\section{Discussion and Conclusion}
\label{sec:discussion}
In this section, we summarize our results, and discuss high-level recommendations. First, we proposed a novel quantitative evaluation setting, \difull, to disentangle the behaviour of the model from that of the attribution method. This allowed us to evaluate for model-faithfulness by partitioning inputs into regions that could and could not influence the model's decision. Using this, we showed that (\cref{fig:main}) some popularly used attribution methods, such as \gradcam, can provide model-unfaithful attributions. On the other hand, while noisy, backpropagation-based methods like \intgrad and \ixg localize perfectly under this setting. We note, however, that our setting cannot evaluate the correctness of attributions within the target grid cells, and as such a high localization performance on \difull is a necessary condition for a good attribution method, but not a sufficient condition. In other words, \difull can be viewed as a coarse sanity check that should be passed by any model-faithful attribution method, but our results show that several do not do so. This could be of practical importance in use cases where models learn to focus on a fixed local region in an image to reach their decisions.

Second, we observed that different attribution methods are typically evaluated at different depths, which leads to them being compared unfairly. To address this, we proposed a multi-layer evaluation scheme, \mlatt, through which we compared each attribution method at identical model depths (\cref{fig:main,fig:scatter:selected}). We found that surprisingly, a diverse set of methods perform very similarly and localize well, particularly at the final layer. This includes backpropagation-based methods like \ixg and \intgrad, which have often been criticized for providing highly noisy and hard to interpret attributions. Combined with their perfect localization on \difull, this shows that \ixg and \intgrad at the final layer can be used as an alternative to \gradcam, when coarse localization is desired. Quantitative (\cref{fig:main,fig:scatter:selected}) and qualitative (\cref{fig:vis:pg,fig:vis:disc}) results at intermediate layers also point to the existence of a trade-off between faithfulness and coarseness of attributions, particularly for methods like \ixg and \intgrad. While attributions computed closer to the input explain a larger fraction of the network and provides more fine-grained attributions, such attributions often localize poorly and are not very helpful to end users. On the other hand, attributions computed closer to the final layer explain only a small part of the network, but are coarser, localize better and highlight the object features more clearly. As a result, the choice of layer to compute attributions would depend on the user's preference in the presence of this trade-off.

Third, we proposed an aggregate attribution evaluation scheme, \aggatt, to holistically visualize the performance of an attribution method. Unlike evaluation on a small subset of examples, this shows the full range of localizations across the dataset and eliminates any inadvertent biases from the choice of examples. Furthermore, it allows one to easily visualize the performance at the best and worst localized examples, and could help identify cases when an attribution method unexpectedly fails.

Fourth, we showed that a simple post-hoc Gaussian smoothing step can significantly improve localization (\cref{fig:smooth,fig:vis:ixgsmoothnegative}) for some attribution methods (\intgrad, \ixg). Unlike commonly used smoothing techniques like \smoothgrad, this requires no additional passes through the network and no selection of hyperparameters. As we show in the supplement, it also results in better localized attributions. This shows that while originally noisy, obtaining a local summary of attribution maps from these methods could provide maps that are useful for humans in practice. However, we find that the effectiveness of smoothing is influenced by the network architecture, in particular the presence of batch normalization layers, which suggests that architectural considerations must be taken into account when using attribution methods.

Finally, we find that certain configurations of layer-wise relevance propagation (LRP) consistently perform the best quantitatively and qualitatively across network depths. However, by interpolating between different LRP configurations (see \cref{subsec:lrp}), we find that this is likely due to the fact that the well-performing LRP-configurations maintain the sign of the attributions to the final layer in the backpropagation process. As such, some aspects of the model computations are not reflected in the final attribution maps (negative contributions at intermediate layers are neglected) and the final attributions are largely dependent on the localisation performance at the final layer. How to better reflect those negative contributions in the backpropagation process  is thus an interesting direction for future work.

While we focus on CNNs in our work, performing a comprehensive evaluation for attribution methods on the recently proposed state-of-the-art image classification architectures such as vision transformers (ViTs) \cite{dosovitskiy2021an} is another interesting direction for future work.

Overall, we find that fair comparisons, holistic evaluations (\difull, \gridpg, \aggatt, \mlatt), and careful disentanglement of model behaviour from the explanations provide better insights in the performance of attribution methods.