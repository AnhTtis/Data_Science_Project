% CVPR 2022 Paper Template
% based on the CVPR template provided by Ming-Ming Cheng (https://github.com/MCG-NKU/CVPR_Template)
% modified and extended by Stefan Roth (stefan.roth@NOSPAMtu-darmstadt.de)

\documentclass[10pt,letterpaper]{article}

%%%%%%%%% PAPER TYPE  - PLEASE UPDATE FOR FINAL VERSION
% \usepackage[review]{cvpr}      % To produce the REVIEW version
\usepackage{cvpr}              % To produce the CAMERA-READY version
%\usepackage[pagenumbers]{cvpr} % To force page numbers, e.g. for an arXiv version

% Include other packages here, before hyperref.
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage[accsupp]{axessibility}  % Improves PDF readability for those with disabilities.

% It is strongly recommended to use hyperref, especially for the review version.
% hyperref with option pagebackref eases the reviewers' job.
% Please disable hyperref *only* if you encounter grave issues, e.g. with the
% file validation for the camera-ready version.
%
% If you comment hyperref and then uncomment it, you should delete
% ReviewTempalte.aux before re-running LaTeX.
% (Or just hit 'q' on the first LaTeX run, let it finish, and you
%  should be clear).
\usepackage[pagebackref,breaklinks,colorlinks]{hyperref}


% Support for easy cross-referencing
\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}

% If you wish to avoid re-using figure, table, and equation numbers from
% the main paper, please uncomment the following and change the numbers
% appropriately.
\setcounter{figure}{10}
%\setcounter{table}{1}
\setcounter{equation}{1}

% If you wish to avoid re-using reference numbers from the main paper,
% please uncomment the following and change the counter for `enumiv' to
% the number of references you have in the main paper (here, 6).
\let\oldthebibliography=\thebibliography
\let\oldendthebibliography=\endthebibliography
\renewenvironment{thebibliography}[1]{%
    \oldthebibliography{#1}%
    \setcounter{enumiv}{34}%
}{\oldendthebibliography}


%%%%%%%%% PAPER ID  - PLEASE UPDATE
\def\cvprPaperID{6357} % *** Enter the CVPR Paper ID here
\def\confName{CVPR}
\def\confYear{2022}

\input{commands}
\usepackage{tocloft}
\setcounter{tocdepth}{1}
\addtolength{\cftsecnumwidth}{2pt}
% \addtolength{\cftsubsecnumwidth}{7pt}
% \renewcommand{\thesection}{A\arabic{section}}
\usepackage[square,sort,comma,numbers]{natbib}
\renewcommand*{\citenumfont}[1]{S#1}
\renewcommand*{\bibnumfmt}[1]{[S#1]}

\renewcommand\thesection{\Alph{section}}
\setcounter{page}{13}

\begin{document}

%%%%%%%%% TITLE - PLEASE UPDATE
\title{Better Understanding Differences in Attribution
Methods via Systematic Evaluations\\Supplementary Material}

\author{Sukrut Rao, Moritz Böhle, Bernt Schiele\\
Max Planck Institute for Informatics, Saarland Informatics Campus, Saarbrücken, Germany\\
{\tt\small \{sukrut.rao,mboehle,schiele\}@mpi-inf.mpg.de}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
% \and
% Second Author\\
% Institution2\\
% First line of institution2 address\\
% {\tt\small secondauthor@i2.org}
}
\maketitle

\tableofcontents
~\\
In this supplement, we provide additional details and results of our evaluation on our proposed settings. First, we provide quantitative results on ImageNet \cite{imagenet} (\cref{sec:additional:dipart}) on all seven models we evaluate on, followed by complete qualitative results using \aggatt (\cref{sec:additional:aggatt}) with examples. Then, we investigate the correlation between the localization scores of attribution methods (\cref{sec:additional:correlation}) and compare it with the trends seen from the qualitative and quantitative results. This is followed by a discussion of the impact of smoothing attributions (\cref{sec:additional:smoothing}), including a comparison of the similarity of \gradcam\cite{selvaraju2017grad} with \sintgrad and \sixg on the same set of examples across \aggatt bins. In \cref{sec:alllayers}, we provide quantitative results at all spatial layers for all models, and show that it reflects the trends observed from the three chosen layers in our experiments. This is followed by an analysis of the computational costs of each evaluation setting (\cref{sec:cost}), and a comparison of our proposed smoothing technique with SmoothGrad \cite{smilkov2017smoothgrad} (\cref{sec:smoothgrad}). Next, we provide an illustration on the potential instability in LRP \cite{bach2015pixel} attributions due to the lack of implementation invariance in certain LRP propagation rules (\cref{sec:lrpimplinv}). Then, we describe further implementation details (\cref{sec:impldetails}) of our experiments. Finally, we provide results of our approach on CIFAR10 \cite{krizhevsky2009learning} (\cref{sec:cifar}), and find similar trends as on ImageNet across our settings.

\newpage

%%%%%%%%% BODY TEXT

\section{Quantitative Results on All Models}\label{sec:additional:dipart}
We provide the full results of our quantitative evaluation on the Grid Pointing Game \cite{boehle2021convolutional} (\gridpg), \difull, and \dipart using the backpropagation-based (\cref{fig:imagenet:quantitative}, left), activation-based (\cref{fig:imagenet:quantitative}, middle), and perturbation-based (\cref{fig:imagenet:quantitative}, right) methods on VGG19 \cite{simonyan2014very}, Resnet152 \cite{he2016deep}, VGG11 \cite{simonyan2014very}, Resnet18 \cite{he2016deep}, Wide Resnet \cite{zagoruyko2016wide}, Resnext \cite{xie2017aggregated}, and Googlenet \cite{szegedy2015going}.

It can be seen that the performance on \difull and \dipart is very similar across all three evaluation settings and the three layers. The most significant difference between the two can be seen among the backpropagation-based methods and \layercam\cite{jiang2021layercam}. On \difull, these methods show near-perfect localization, since the gradients of the outputs from each classification head that are used to assign importance are zero with respect to weights and activations of all grid cells disconnected from that head. On the other hand, the receptive field of the convolutional layers can overlap adjacent grid cells in \dipart, and the gradients of the outputs from the classification heads can thus have non-zero values with respect to inputs and activations from these adjacent grid regions. This also results in decreasing localization scores when moving backwards from the classifier.

Furthermore, the localization scores for \grad\cite{simonyan2013deep} and \gb\cite{springenberg2014striving} are constant at the final layer for most models, except VGG19 and VGG11. This is because in all these models, this layer is immediately followed by a global average pooling layer, due to which all activations at this layer get an equal share of the gradients.

\input{fig_imagenet_quantitative}

\newpage
\section{Qualitative Results using \aggatt}\label{sec:additional:aggatt}
In this section, we present additional qualitative results using our \aggatt evaluation along with examples of attributions from each bin, for each of \gridpg \cite{boehle2021convolutional} (\cref{sec:qual:gridpg}), \difull (\cref{sec:qual:difull}), and \dipart (\cref{sec:qual:dipart}).
\subsection{\gridpg}\label{sec:qual:gridpg}
\cref{fig:vis:examples:gridpg:vgg11:input} and \cref{fig:vis:examples:gridpg:vgg11:final} show examples from the median position of each \aggatt bin for each attribution method at the input and final layers, respectively, evaluated on \gridpg at the top-left grid cell using VGG19 \cite{simonyan2014very}. At the input layer (\cref{fig:vis:examples:gridpg:vgg11:input}), we observe that the \textbf{backpropagation-based methods} show noisy attributions that do not strongly localize to the top-left grid cell. This corroborates the poor quantitative performance of these methods at the input layer (\cref{fig:imagenet:quantitative}, top). With the exception of \layercam \cite{jiang2021layercam}, the \textbf{activation-based methods}, on the other hand, show strong attributions across all four grid cells, and localize very poorly. They appear to highlight the edges across the input irrespective of the class of each grid cell. This also agrees with the quantitative results (\cref{fig:imagenet:quantitative}, middle), where the median localization score of these methods is below the uniform attribution baseline. \layercam, being similar to \ixg \cite{shrikumar2017learning}, lies at the interface between activation and backpropagation-based methods, and also shows weak and noisy attributions. The \textbf{perturbation-based methods} visually show a high variance in attributions. While they localize well for about half the dataset (first three bins), the bottom half (last three bins) shows noisy and poorly localized attributions, which again agrees with the quantitative results (\cref{fig:imagenet:quantitative}, bottom). This further shows how evaluating on individual inputs can be misleading, and the utility of \aggatt for obtaining a holistic view across the dataset.

\input{fig_vis_gridpg_examples_vgg11}
Finally, we show the \aggatt bins for all methods at all three layers using both VGG19 and Resnet152 \cite{he2016deep} in \cref{fig:vis:bins:gridpg}. We see that the \aggatt bins reflect the trends observed in the examples in each bin, and serve as a useful tool for visualization.
\input{fig_vis_gridpg_bins}

\subsection{\difull}\label{sec:qual:difull}
\cref{fig:vis:examples:difull:vgg11:input} and \cref{fig:vis:examples:difull:vgg11:final} show examples from the median position of each \aggatt bin for each attribution method at the input and final layers, respectively, evaluated on \difull at the top-left grid cell using VGG19. At the input layer (\cref{fig:vis:examples:difull:vgg11:input}), the backpropagation-based methods and \layercam show perfect localization across the dataset. This is explained by the disconnected construction of \difull, and agrees with the quantitative results shown in \cref{fig:imagenet:quantitative}). The activation-based methods show very poor localization that appear visually similar to the attributions observed on \gridpg (\cref{sec:qual:gridpg}). \occ shows near-perfect localization, since the placement of the classification kernel at any location not overlapping with the top-left grid cell does not influence the output in the \difull setting. \rise still produces noisy attributions across the dataset. While only the top-left grid cell influences the output, the use of random masks causes input regions that share masks with inputs in the top-left cell to also get attributed.

\input{fig_vis_difull_examples_vgg11}
Finally, we show the \aggatt bins for all methods at all three layers using both VGG19 and Resnet152 in \cref{fig:vis:bins:difull}, and see that they reflect the trends observed in the individual examples seen from each bin.
\input{fig_vis_difull_bins}

\subsection{\dipart}\label{sec:qual:dipart}
\cref{fig:vis:examples:dipart:vgg11:input} and \cref{fig:vis:examples:dipart:vgg11:final} show examples from the median position of each \aggatt bin for each attribution method at the input and final layers, respectively, evaluated on \dipart at the top-left grid cell using VGG19. In addition, \cref{fig:vis:bins:dipart} shows the \aggatt bins for all methods at all three layers using both VGG19 and Resnet152. As observed with the quantitative results (\cref{sec:additional:dipart}, the performance seen visually on \dipart across the three layers is very similar to that on \difull (\cref{sec:qual:difull}). However, they slightly differ in the case of the backpropagation-based methods and \layercam, particularly at the input layer (\cref{fig:vis:examples:dipart:vgg11:input}). This is because unlike in \difull, the grid cells are only partially disconnected, and the receptive field of the convolutional layers can overlap adjacent grid cells to some extent. Nevertheless, as can be seen here, only a small boundary region around the top-left grid cell receives attributions, and the difference is not visually very perceivable. This further shows that the \dipart setting can be thought of as a natural extension for \difull, that mostly shares the requisite property without being an entirely constructed setting.
\input{fig_vis_dipart_examples_vgg11}
\input{fig_vis_dipart_bins}

\newpage
\section{Correlation between Attributions}\label{sec:additional:correlation}
From the quantitative (\cref{fig:imagenet:quantitative}) and qualitative (\cref{fig:vis:bins:gridpg}) results, we observed that diverse methods perform similarly on \gridpg \cite{boehle2021convolutional} both in terms of localization score and through \aggatt visualizations when evaluated fairly. This was particularly the case with \intgrad \cite{sundararajan2017axiomatic}, \ixg \cite{shrikumar2017learning}, \gradcam \cite{selvaraju2017grad}, and \occ \cite{zeiler2013visualizing}, when evaluated at the final layer. We also found (Sec 5.2 in the paper) that smoothing \intgrad and \ixg (the result of which we call \sintgrad and \sixg) attributions evaluated at the \emph{input layer} leads to visually and quantitatively similar performance as \gradcam evaluated at the \emph{final layer}. In this section, we investigate this further, and study the correlation of these methods at the level of individual attributions. In particular, we compute the Spearman rank correlation coefficient of the localization scores using VGG19 \cite{simonyan2014very} of every pair of methods from each of the three layers. The results are shown in \cref{fig:correlation}.

We observe that at the input layer (\cref{fig:correlation}, top-left corner), the activation-based methods are poorly correlated with each other and with the backpropagation and perturbation-based methods. This also agrees with the poor localization of these methods seen previously (\cref{fig:imagenet:quantitative}, \cref{fig:vis:examples:gridpg:vgg11:input}). The backpropagation-based and perturbation-based methods, on the other hand, show moderate to strong correlation amongst and with each other. Similar results can be seen when comparing methods at the middle layer with the input layer and the final layer (\cref{fig:correlation}, edge centres).  However, when compared at the middle layer (\cref{fig:correlation}, middle), the activation-based methods still correlate poorly with other methods, but the strength of the correlation improves in general.

Further, when compared at the final layer (\cref{fig:correlation}, bottom-right corner), all methods show moderate to strong correlations with each other. This could be because generating explanations at the final layer is a significantly easier task as compared to doing so at the input, since the activations are used as is and only the classification layers' outputs are explained. The pairs with very strong positive correlation also show that attribution methods with diverse mechanisms can perform similarly when evaluated fairly. Finally, we observe that the activation-based methods at the final layer, instead of the input layer, correlate much better with the other methods at the input layer (\cref{fig:correlation}, top-right, bottom-left).

We also observe that \sintgrad and \sixg at the input layer correlate well with the best-performing methods (\intgrad, \ixg, \gradcam, \occ) at the final layer. Further, this marks a significant improvement when compared with \intgrad and \ixg at the input layer. For example, \intgrad at the input layer compared with \gradcam at the final layer results in a correlation coefficient of $0.34$, while \sintgrad results in a correlation coefficient of $0.80$.

We further study the effect of smoothing in \cref{tab:corr:smooth:IxG,tab:corr:smooth:IntGrad}. We observe that the correlation between \sintgrad and \sixg improves significantly over \intgrad and \ixg for VGG19 when using large kernels. However, for Resnet152 \cite{he2016deep}, the improvement for \sixg is very small. This agrees with the quantitative localization performance of these methods (Sec 5.2 in the paper). This shows that beyond aggregate visual similarity and quantitative performance, smoothing \intgrad and \ixg can produce explanations at the input layer that are individually similar to \gradcam at the final layer, while also explaining the full network and performing significantly better on \difull. We further visually compare the impact of smoothing in \cref{sec:additional:smoothing}.

\input{table_correlation_smooth}
\input{fig_correlations}

\newpage
\section{Impact of Smoothing Attributions}\label{sec:additional:smoothing}
In this section, we explore the impact of smoothing attributions. First, we briefly discuss a possible reason for the improvement in localization of attributions after smoothing (\cref{sec:smooth:explain}. Then, we visualize the impact of smoothing through examples and \aggatt visualizations (\cref{sec:smooth:aggatt}). Further, we also compare the performance of \gradcam \cite{selvaraju2017grad} at the final layer with \sintgrad and \sixg at the input layer across the same examples from each bin and show their similarities across bins (\cref{sec:smooth:compare}).

\subsection{Effect of Smoothing}\label{sec:smooth:explain}
We believe that our smoothing results highlight an interesting aspect of piece-wise linear models (PLMs), which goes beyond mere practical improvements. For PLMs (such as the models used here), \ixg \cite{shrikumar2017learning} yields the exact pixel contributions according to the linear mapping given by the PLM. In other words, the sum of \ixg attributions over all pixels yields exactly (ignoring biases) the model output. If the effective receptive field of the model is small (cf. \cite{luo2016understanding}), sum pooling \ixg with a kernel of the same size accurately computes the model’s local output (apart from the influence of bias terms). Our method of smoothing \ixg with a Gaussian kernel performs a weighted average pooling of attributions in the local region around each pixel, which produces a similar effect and appears to summarize the effect of the pixels in the local region to the model's output, which leads to less noisy attributions and better localization.


\subsection{\aggatt Evaluation after Smoothing}\label{sec:smooth:aggatt}
In \cref{fig:vis:examples:smooth:gridpg:vgg11:intgrad} and \cref{fig:vis:examples:smooth:gridpg:vgg11:ixg}, we show examples from each \aggatt bin for \sintgrad and \sixg at the input layer for two different kernel sizes, and compare with \intgrad \cite{sundararajan2017axiomatic} and \ixg at the input layer respectively. We observe that the localization performance significantly improves with increasing kernel size, and produces much stronger attributions for the target grid cell. In \cref{fig:vis:bins:smooth:gridpg}, we show the \aggatt bins for these methods on both VGG19 \cite{simonyan2014very} and Resnet152 \cite{he2016deep}. We see that this reflects the trends seen from the examples, and also clearly shows the relative ineffectiveness of smoothing \ixg for Resnet152 (\cref{fig:vis:bins:smooth:gridpg} bottom right and \cref{tab:corr:smooth:IxG}).
\input{fig_vis_smooth_examples}
\input{fig_vis_smooth_bins}
\subsection{Comparing \gradcam with \sintgrad and \sixg}\label{sec:smooth:compare}
We now compare \gradcam at the final layer with \sintgrad and \sixg at the input layer with $K=129$ on the same set of examples (\cref{fig:vis:examples:smooth:compare}). We pick an example from each \aggatt bin of \gradcam, and evaluate all three methods on them. From \cref{fig:vis:examples:smooth:compare}, we observe that the three methods produce visually similar attributions across the \aggatt bins. While the attributions of \sintgrad and \sixg are somewhat coarser than \gradcam, particularly for the examples in the first few bins, they still concentrate around similar regions in the images. Interestingly, they perform similarly even for examples where \gradcam does not localize well, \ie, in the last two bins. Finally, we again see that \sixg using Resnet152 performs relatively worse as compared to the other methods (as also seen in \cref{tab:corr:smooth:IxG}).
\input{fig_vis_compare}

\newpage
\section{Quantitative Evaluation on All Layers}\label{sec:alllayers}
As discussed in the paper (Section 5.2, Figure 4), we evaluate trends in localization performance across the full range of network depths across all seven models. Full results on all attribution methods can be found in \cref{fig:alllayers:line}.
\input{fig_alllayers_line}
Additionally, \cref{fig:alllayers:vgg11,fig:alllayers:resnet18} show the results on evaluating at each convolutional layer of VGG11 \cite{simonyan2014very} and each layer block of Resnet18 \cite{he2016deep}, visualized via boxplots to show the performance across the dataset. We find that the performance on the remaining layers is consistent with the trend observed from the three chosen layers in our experiments.


\input{fig_alllayers_vgg11}
\input{fig_alllayers_resnet18}


\newpage
\section{Computational Cost}\label{sec:cost}
Unlike \gridpg \cite{boehle2021convolutional}, the \difull setting involves passing each grid cell separately through the network. In this section, we compare the computational costs of \gridpg, \difull, and \dipart, and show that it is similar across the three settings. Let the input be in the form of a $n\times n$ grid. Each setting consists of a CNN module, which obtains features from the input, and a classifier module, which provides logits for each cell in the grid using the obtained features. We analyze each of these modules one by one. 

\paragraph{CNN Module:} In \gridpg and \dipart, the entire grid is passed through the CNN module as a single input. On the other hand, in \difull, each grid cell is passed separately. This can be alternatively viewed as stacking each of the $n^2$ grid cells along the batch dimension before passing them through the network. Consequently, the inputs in the \difull setting have their widths and heights scaled by a factor of $\frac{1}{n}$, and the batch size scaled by a factor of $n^2$. Since the operations within the CNN module scale linearly with input size, the computational cost for each grid cell in \difull is $\frac{1}{n^2}$ times the cost for the full grid in \gridpg and \dipart. Since there are $n^2$ such grid cells, the total computational cost for the CNN module of \difull equals that of \gridpg and \dipart.

\paragraph{Classifier Module:} The classifier module in the \difull and \dipart settings consists of $n^2$ classification heads, each of which receives features corresponding to a single grid cell. On the other hand, the \gridpg setting uses a classifier kernel over the composite feature map for the full grid. Let the dimensions of the feature map for a single grid cell be $d\times d$. This implies that in \gridpg, using a stride of 1, the classification kernel slides over $((n-1)d+1)^2$ windows of the input, each of which results in a call to the classifier module. In contrast, in \difull and \dipart, the classifier module is called only $n^2$ times, one for each head. This shows that the computational cost of \difull and \dipart for the classifier module and the pipeline as a whole is at most as much as of \gridpg.


\newpage
\section{Comparison with \smoothgrad}\label{sec:smoothgrad}
In our work, we find that smoothing \intgrad \cite{sundararajan2017axiomatic} and \ixg \cite{shrikumar2017learning} attributions with a Gaussian kernel can lead to significantly improved localization, particularly for networks without batch normalization layers \cite{ioffe2015batch}. As discussed in \cref{sec:smooth:explain}, we believe this to be because smoothing summarizes the effect of inputs in a local window around each pixel to the output logit, and reduces noisiness of attributions. Prior approaches to address noise in attributions include SmoothGrad \cite{smilkov2017smoothgrad}, which involves adding Gaussian noise to an input and averaging over attributions from several samples. Here, we compare our smoothing with that of SmoothGrad. \cref{fig:smoothgrad} shows that our methods (\sintgrad, \sixg) show significantly better \gridpg \cite{boehle2021convolutional} localization than SmoothGrad on \intgrad and \ixg, except in the case of \ixg with Resnet18 \cite{he2016deep}, where our smoothing does not improve localization likely due to the presence of batch normalization layers. The scores on \difull decrease to an extent since our Gaussian smoothing allows attributions to ``leak'' to neighbouring grid cells. These results are corroborated by \aggatt visualizations in \cref{fig:vis:bins:smoothgrad:gridpg}. We also note that \smoothgrad requires significantly higher computational cost than our approach, as attributions need to be generated for several noisy samples of each input, and is also sensitive to the choice of hyperparameters such as the noise percentage and the number of samples.

\input{fig_smoothgrad}
\input{fig_vis_smoothgrad}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\section{LRP: Lack of Implementation Invariance}\label{sec:lrpimplinv}
In this section, we provide an illustration on the impact of the lack of implementation invariance \cite{sundararajan2017axiomatic} in the $ z^+$ propagation rule of LRP. Using our \difull setting, we provide a general construction of a transform that results in a functionally equivalent model but causes LRP with the $ z^+$ rule to shift nearly all attributions to the disconnected regions.

The transform is illustrated in \cref{fig:vis:lrp:implinv:transform}. In the \difull setting, each classification head is connected to its corresponding grid cell's feature map with the weights from the pre-trained model, and is disconnected (alternatively, connected with zero weights) to all other grid cells (\cref{fig:vis:lrp:implinv:transform:before}). For the transform, we introduce an additional linear layer $l_{c'}$ between the final feature activations $f$ and the first classification layer $l_c$, as shown in \cref{fig:vis:lrp:implinv:transform:after}.

Let the number of neurons in $l_c$ be $k$. We construct $l_{c'}$ with $k+2$ neurons, as shown. We use $k$ neurons to map the original linear transformation back to $l_c$, by using an identity transform between the connected regions in $f$ and $l_{c'}$, and using zero weights for the disconnected regions. The two additional neurons are used to aggregate the activations from all the disconnected regions of $f$, one each with positive and negative sign. These neurons are then connected with weight $1$ to $l_c$. As a result, the contributions from the disconnected regions cancel out in the output of $l_c$, and the model remains functionally equivalent.

However, when applying LRP with the $z^+$ propagation rule, relevance is distributed only among neurons that contribute positively to the output. Consequently, when propagating from $l_{c'}$ to $l_c$, most of the relevance is transferred to the new neuron that aggregates positive attributions from the disconnected regions. Since the regions are fully disconnected for all further propagation, this causes most attributions to now be given to the disconnected regions, as shown in \cref{fig:vis:lrp:implinv:example}.

\input{fig_vis_lrpimplinv}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\section{Implementation Details}\label{sec:impldetails}
\subsection{Dataset}\label{sec:impldetails:dataset}
As described in the paper (Sec. 4), we obtain 2,000 attributions for each attribution method on each of \gridpg \cite{boehle2021convolutional}, \difull, and \dipart, using inputs consisting of four subimages arranged in $2\times2$ grids. For \gridpg, since we evaluate on all four subimages, we do this by constructing 500 grid images after randomly sampling 2,000 images from the validation set. Each grid image contains subimages from four distinct classes. On the other hand, for \difull and \dipart, we place images of the same class at the top-left and bottom-right corners to test whether an attribution  method  simply  highlights  class-related  features, irrespective of them being used by the model. Therefore, we evaluate only on these two grid locations. In order to obtain 2,000 attributions as with \gridpg, for these two settings, we construct 1,000 grid images by randomly sampling 4,000 images from the validation set.


\subsection{Models and Attribution Methods}
We implement our settings using PyTorch \cite{pytorch}, and use pretrained models from Torchvision \cite{pytorch}. We use implementations from the Captum library \cite{kokhlikyan2020captum} for \grad \cite{simonyan2013deep}, \gb \cite{springenberg2014striving}, \intgrad \cite{sundararajan2017axiomatic}, and \ixg \cite{shrikumar2017learning}, from the Zennit library \cite{anders2021software} for LRP \cite{bach2015pixel}, and from \cite{boehle2021convolutional} for \occ \cite{zeiler2013visualizing} and \rise \cite{petsiuk2018rise}. For \grad and \gb, the absolute value of the attributions are used. All attributions across methods are summed along the channel dimensions before evaluation.

\occ involves sliding an occlusion kernel of size $K$ with stride $s$ over the image. As the spatial dimensions of the feature maps decreases from the input to the final layer, we select different values of $K$ and $s$ for each layer. In our experiments, we use $K=16,s=8$ for the input, and $K=5,s=2$ for the middle and final layers.

\rise generates attributions by occluding the image using several randomly generated masks and weighing them based on the change in the output class confidence. In our experiments, we use $M=1000$ masks. We use fewer masks than \cite{petsiuk2018rise} to offset the increased computational cost from using $448\times448$ images, but found similar results from a subset of experiments with $M=6000$.

The $\gamma$-rule for LRP-Composite as originally defined expects positive inputs. So, to enable its use with negative inputs that may result from using Resnet model, we use the generalized version of the $\gamma$-rule (proposed in \cite{andeol2021learning}) that is implemented in the Zennit library \cite{anders2021software}.


\subsection{Localization Metric}
In our quantitative evaluation, use the same formulation for the localization score as proposed in \gridpg (Sec 3.1.1 in the paper).
Let $A^+(p)$ refer to the positive attribution given to the $p^{th}$ pixel. The localization score for the subimage $x_i$ is given by:
\begin{equation}\label{eq:localization}
    L_i = \frac{\sum_{p\in x_i} A^+(p)}{\sum_{j=1}^{n^2}\sum_{p\in x_j} A^+(p)} 
\end{equation}
However, $L_i$ is undefined when the denominator in \cref{eq:localization} is zero, \ie, $\sum_{j=1}^{n^2}\sum_{p\in x_j} A^+(p)=0$. This can happen, for instance, when all attributions for an input are negative. To handle such cases, we set $L_i=0$ in our evaluation whenever the denominator is zero.

\subsection{\aggatt Visualizations}
To generate our \aggatt visualizations, we sort attribution maps in the descending order of the localization score and bin them into percentile ranges to obtain aggregate attribution maps (Sec 3.2 in the paper). However, we observe that when evaluating on \difull, the backpropagation-based attribution methods show perfect localization (Sec 5.1 in the paper), and all attributions share the same localization score. In this scenario, and in all other instances when two attributions have the same localization score, we break the tie by favouring maps that have stronger attributions in the target grid cell. We do this by ordering attributions with the same localization score in the descending order of the sum of attributions within the target grid cell, \ie, the numerator in \cref{eq:localization}.

Further, when producing the aggregate maps, we normalize the aggregate attributions using a common normalizing factor for each method. This is done to accurately reflect the strength of the average attributions across bins for a particular method.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newpage
\section{Evaluation on CIFAR10}\label{sec:cifar}
In addition to ImageNet \cite{imagenet}, we also evaluate using our settings on CIFAR10 \cite{krizhevsky2009learning}. In this section, we present these results, and find similar trends in performance as on ImageNet. We first describe the experimental setup (\cref{sec:cifar:setup}) used, and then show the quantitative results on \gridpg \cite{boehle2021convolutional}, \difull, and \dipart (\cref{sec:cifar:quantitative}) and some qualitative results using \aggatt (\cref{sec:cifar:qualitative}).

\subsection{Experimental Setup}\label{sec:cifar:setup}
\myparagraph{Network Architecture:}
We use a modified version of the VGG11 \cite{simonyan2014very} architecture, with the last two convolutional layers removed. Since the CIFAR10 inputs have smaller dimensions ($32\times32$) than Imagenet ($224\times224$), using all the convolutional layers results in activations with very small spatial dimensions, which makes it difficult to apply attribution methods at the final layer. After removing the last two convolutional layers, we obtain activations at the new final layer with dimensions $4\times4$ before pooling. We then perform our evaluation at the input (Inp), middle layer (Conv3) and the final layer (Conv6).

\myparagraph{Data:}
We construct grid datasets consisting of $2\times2$ and $3\times3$ grids using images from the validation set classified correctly by the network with a confidence of at least $0.99$. We obtain 4,000 (resp. 4,500) attributions for each method from the $2\times2$ (resp. $3\times3$) grid datasets respectively. As with ImageNet (\cref{sec:impldetails:dataset}), we evaluate on all grid cells for \gridpg and only at the top-left and bottom-right corners on \difull and \dipart. To obtain an equivalent 4,000 (resp. 4,500) attributions from using just the corners on \difull and \dipart, we randomly sample 8,000 (resp. 20,250) images for the $2\times2$ ($3\times3$) grid datasets, and construct 2,000 (2,250) composite images. Note that the CIFAR10 validation set only has a total of 10,000 images. Since we only evaluate at the two corners, we allow subimages at other grid cells to repeat across multiple composite images. However, no two subimages are identical within the same composite image.

\subsection{Quantitative Evaluation on \gridpg, \difull, and \dipart}\label{sec:cifar:quantitative}
The results of the quantitative evaluation can be found in \cref{fig:cifar:quantitative} for both $2\times2$ grids (left) and $3\times3$ grids (right). We observe that all methods perform similarly as on ImageNet (\cref{fig:imagenet:quantitative}). Since localizing on $3\times3$ grids poses a more challenging task, we observe generally poorer performance across all methods on that setting.
\input{fig_cifar_quantitative}



\subsection{Qualitative Results using \aggatt}\label{sec:cifar:qualitative}
In \cref{fig:vis:cifar:bins}, we show \aggatt evaluations on $3\times3$ grids for a method each from the set of backpropagation-based (\ixg \cite{shrikumar2017learning}), activation-based (\gradcam \cite{selvaraju2017grad}), and perturbation-based (\occ \cite{zeiler2013visualizing}) methods. Further, we show examples of attributions at the input and final layer on \gridpg for these methods (\cref{fig:vis:cifar:examples:gridpg:vgg11:input,fig:vis:cifar:examples:gridpg:vgg11:final}). We see that these show similar trends in their performance as on ImageNet (\cref{sec:additional:aggatt}). 
\input{fig_vis_cifar_gridpg_bins}
\input{fig_vis_cifar_gridpg_examples_3x3}
\newpage



%%%%%%%%% REFERENCES
{\small
\bibliographystyle{ieee_fullname}
\bibliography{supp_references}
}

\end{document}
