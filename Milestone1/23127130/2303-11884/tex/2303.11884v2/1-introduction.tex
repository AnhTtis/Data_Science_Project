\IEEEraisesectionheading{\section{Introduction}}
\label{sec:intro}

\begin{figure*}[t]
\centering
\input{fig_teaser}
\end{figure*}

\IEEEPARstart{D}{eep} neural networks (DNNs) are highly successful on many computer vision tasks.
However, their black box nature makes it hard to interpret and thus trust their decisions. 
To shed light on the models' decision-making process, several methods have been proposed that aim to attribute importance values to individual input features (see \cref{sec:related}).
However, given the lack of ground truth importance values, it has proven difficult to compare and evaluate these \emph{attribution methods} in a holistic and systematic manner.

In this work that extends \cite{rao2022towards}, we take a three-pronged approach towards addressing this issue. In particular, we focus on three important components for such evaluations: reliably measuring the methods' \emph{model-faithfulness}, ensuring a \emph{fair comparison} between methods, and providing a framework that allows for \emph{systematic} visual inspections of their attributions. 

First, we propose an evaluation scheme (\textbf{DiFull}), which allows distinguishing possible from impossible importance attributions. This effectively provides ground truth annotations for whether or not an input feature can possibly have influenced the model output. As such, it can highlight distinct failure modes of attribution methods (\cref{fig:teaser}, left). 

Second, a fair evaluation requires attribution methods to be compared on equal footing. However, we observe that different methods explain DNNs to different depths (e.g., full network or classification head only). 
Thus, some methods in fact solve a much easier problem (i.e., explain a much shallower network). To even the playing field, we propose a multi-layer evaluation scheme for attributions ({\bf ML-Att}) and 
thoroughly evaluate
commonly used methods across multiple layers and models (\cref{fig:teaser}, left). 
When compared on the same level, we find that performance differences between some methods essentially vanish.

Third, relying on individual examples for a qualitative comparison is prone to skew the comparison and cannot fully represent the evaluated attribution methods. To overcome this, we propose a qualitative evaluation scheme for which we aggregate attribution maps ({\bf AggAtt}) across many input samples. This allows us to observe trends in the performance of attribution methods across complete datasets, in addition to looking at individual examples (\cref{fig:teaser}, right).

\input{1.1-contribs}