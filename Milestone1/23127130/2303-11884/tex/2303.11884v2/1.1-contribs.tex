\myparagraph{Contributions.}
\textbf{(1)} We propose a novel evaluation setting, {\bf\disc}, 
in which we control which regions \emph{cannot possibly} influence a model's output, which allows us to highlight definite failure modes of attribution methods.
\textbf{(2)} 
We argue that methods can only be compared fairly when evaluated \emph{on the same layer}. To do this, we introduce {\bf ML-Att} and evaluate all attribution methods at multiple layers. 
We show that, when compared fairly, apparent performance differences between some methods effectively vanish. 
\textbf{(3)} We propose a novel aggregation method, {\bf\aggatt}, to qualitatively evaluate attribution methods across all images in a dataset. This allows to qualitatively assess a method's performance across many samples (\cref{fig:teaser}, right), which complements the evaluation on individual samples.
\textbf{(4)} We propose a post-processing smoothing step that significantly improves localization performance on some attribution methods. 
We observe significant differences when evaluating these smoothed attributions on different architectures, which highlights how architectural design choices can influence an attribution method's applicability.

In this extended version of \cite{rao2022towards}, we additionally provide the following:
\textbf{(1)} We evaluate on a wider variety of network architectures, in particular deeper networks with higher classification accuracies, including VGG19 \cite{simonyan2014very}, ResNet152 \cite{he2016deep}, ResNeXt \cite{xie2017aggregated}, Wide ResNet \cite{zagoruyko2016wide}, and GoogLeNet \cite{szegedy2015going}. We show that the results and trends discussed in \cite{rao2022towards} generalize well to diverse CNN architectures.
\textbf{(2)} We evaluate our settings on multiple configurations of the layer-wise relevance propagation (LRP) \cite{bach2015pixel} family of attribution methods, that modify the gradient flow during backpropagation to identify regions in the image important to the model. We show that while LRP can outperform all other methods, achieving good localization requires carefully choosing propagation rules and their parameters, and is sensitive to the model formulation and architecture.
\textbf{(3)} We show that the trends in performance of attribution methods at multiple layers (\mlatt), which was visualized at a subset of layers (input, middle, and final) in \cite{rao2022towards}, generalizes across layers and architectures for each method.
Our code is available at \href{https://github.com/sukrutrao/Attribution-Evaluation}{https://github.com/sukrutrao/Attribution-Evaluation}.



