\section{Experimental Results and Discussion}
\label{sec:results}

\input{fig_box_main}
\input{fig_box_smoothing}
\input{fig_vis_gridpg}
\input{fig_vis_sixg_negative}
\input{fig_vis_difull}

In this section, we first present the quantitative results for all attribution methods on \pg, \cs, and \disc and compare their performance at multiple layers (\ref{subsec:layer_results}).
Further, we present a simple smoothing mechanism that provides highly performant attributions on all three settings, and discuss architectural considerations that impact its effectiveness (\ref{subsec:smoothing}). Finally, we present qualitative results using \aggatt, and show its use in highlighting strengths and deficiencies of attribution methods (\ref{subsec:evalqualitative}).

\subsection{Evaluation on \pg, \disc, and \cs}
\label{subsec:layer_results}
We perform \mlatt evaluation using the the input (Inp), and the activations at a middle layer (Mid) and final convolutional layer (Fin) before the classification head
(x-ticks in \cref{fig:main}) for all three quantitative evaluation settings (\pg, \disc, \cs, minor columns in \cref{fig:main}) discussed in \cref{sec:method}. In the following, we discuss the methods' results, grouped by their `method family': backpropagation-based, activation-based, and perturbation-based methods (major columns in \cref{fig:main}).

\myparagraph{Backpropagation-based methods:}
We observe that all methods except \lrp perform poorly at the initial layer on \pg (\cref{fig:main}, left). 
Specifically, we observe that they yield noisy attributions that do not seem to reflect the grid structure of the images; i.e., positive attributions are nearly as likely to be found outside of a subimage for a specific class as they are to be found inside. 

However, they improve on later layers. At the final layer, \intgrad and \ixg show very good localization (comparable to \gradcam), which suggests that the methods may have similar explanatory power when compared on an equal footing. We note that \ixg at the final layer has been previously proposed under the name \detgradcam\cite{saha2020role}.

\lrp, on the other hand, performs strongly at all three layers.
We believe that this is likely because the $z^+$ rule used in the convolutional layers propagates relevance backwards in a manner that favours activations that contribute positively to the final output. As the localization metric only considers positive attributions, such a propagation scheme would result in a high localization score. Note that this only evaluates a single LRP configuration, as we discuss in \cref{subsec:lrp}, we find that the performance can significantly vary based on the propagation rules used.

On \disc, all methods show near-perfect localization across layers (\cref{fig:vis:disc}). No attribution is given to disconnected subimages since the gradients with respect to them are zero (after all, they are \emph{fully disconnected});
degradations for other layers can be attributed to the applied upsampling. However, the lack of implementation invariance \cite{sundararajan2017axiomatic} in \lrp implies that relevance could be made to effectively propagate through disconnected regions by constructing an appropriate functionally equivalent model, as we discuss in \cref{subsec:lrp} and the supplement.

Similar results are seen in \cs, but with decreasing localization when moving backwards from the classifier, which can be attributed to the fact that the receptive field can overlap with other subimages in this setting. Overall, we find that similar performance is obtained on \disc and \cs across all methods.

\myparagraph{Activation-based methods:}
We see that all methods with the exception of \layercam improve in localization performance from input to final layer on all three settings. Since attributions are computed using a scalar weighted sum of attribution maps, this improvement could be explained by improved localization of activations from later layers. In particular, localization is very poor at early layers, which is a well-known limitation of \gradcam \cite{jiang2021layercam}. The weighting scheme also causes final layer attributions for all methods except \layercam to perform worse on \disc than on \pg, since these methods attribute importance to both instances of the repeated class (\cref{fig:vis:disc}). This issue is absent in \layercam as it does not apply a pooling operation.

\myparagraph{Perturbation-based methods:}
We observe (\cref{fig:main}, right) \occ to perform well across layers on \disc, since occluding disconnected subimages cannot affect the model outputs and are thus not attributed importance. 
However, the localization drops slightly for later layers. This is due to the fact that the relative size (w.r.t.~activation map) of the overlap regions between occlusion kernels and adjacent subimages increases.
This highlights the sensitivity of performance to the choice of hyperparameters, and the tradeoff between computational cost and performance. 

On \pg, \occ performance improves with layers.
On the other hand, \rise performs poorly across all settings and layers. Since it uses random masks, pixels outside a target grid cell that share a mask with pixels within get attributed equally. So while attributions tend to concentrate more in the target grid cell, the performance can be inconsistent (\cref{fig:vis:disc}).

\subsection{Localization across network depths}
In this section, we evaluate the trends in localization performance across the full range of network depths for the seven models we evaluate on (VGG19, VGG11 \cite{simonyan2014very}, Resnet152, Resnet18 \cite{he2016deep}, ResNeXt \cite{xie2017aggregated}, Wide ResNet \cite{zagoruyko2016wide}, GoogLeNet \cite{szegedy2015going}). Our quantitative evaluation using our proposed \mlatt scheme so far (\cref{fig:main}) focused on three representative network depths -- at the input, a middle layer, and the final layer of each model. %network architecture.
We found that several methods (\eg \ixg, \intgrad, \gradcam, \lrp) localize well at the final layer. Here, we evaluate whether the performance on these three layers is representative of the general trend across all layers, and whether the trends for each attribution methods generalize across diverse network architectures.

The quantitative results for a subset of attribution methods can be found in \cref{fig:scatter:selected}; for the remaining methods, see supplement.
We pick four methods, two backpropagation-based (\intgrad, \ixg) and two activation-based (\gradcam, \ablationcam), whose performance increases most prominently from the input to the final layer in \cref{fig:main}. In addition, we show results on \lrp, the best performing method overall. Full results on all methods can be found in the supplement. For each attribution method, we plot the mean localization score on each model across all network depths. The x-axis shows the fraction of the model depth, where 0 refers to the input layer and 1 refers to the final convolutional layer, and the y-axis shows the localization score. Each line plots the mean localization score across all possible depths for a single model.

We find that the trends in performance at the chosen three layers in \cref{fig:main} generalize to all layers, with the localization performance improving at deeper layers for all the chosen methods (except \lrp). Furthermore, we find that these trends also generalize across network architectures, and demonstrates the utility of \mlatt in finding similar performance across diverse attribution methods when compared fairly at identical depths. We find that the performance of \intgrad and \ixg steadily improves from the input to the final layer, while that of \gradcam and \ablationcam is poor except near the final layer. \lrp, on the other hand, scores highly throughout the network.

\input{fig_line_selected}

\subsection{Smoothing Attributions}\label{subsec:smoothing}
From \cref{subsec:layer_results}, we see that \gradcam localizes well at the final layer in \pg, but performs poorly on all the other settings as a consequence of global pooling of gradients (for \disc) and poor localization of early layer features (for \pg early layers). 
Since \ixg, in contrast, does not use a pooling operation, it performs well on \disc at all layers and on \pg at the final layer.
However, it performs poorly at the input and middle layers on \pg due to the noisiness of gradients; \intgrad shows similar results. 

Devising an approach to eliminate this noise would provide an attribution method that performs well across settings and layers. 
Previous approaches to reduce noise include averaging attribution maps over many perturbed samples (SmoothGrad\cite{smilkov2017smoothgrad}, see supplement for a comparison) or adding a gradient penalty during training\cite{kiritoshi2019l1}. However, \smoothgrad is computationally expensive as it requires several passes on the network to obtain attributions, and is sensitive to the chosen perturbations. 
Similarly, adding a penalty term during training requires retraining the network. 

Here, we propose to simply apply a Gaussian smoothing kernel on existing \intgrad and \ixg attributions. We evaluate on \disc and \pg using several kernel sizes, using standard deviation $K/4$ for kernels of size $K$. We refer to the smooth versions as \sintgrad and \sixg respectively.

On VGG19 (\cref{fig:smooth}, top), we find that \sintgrad and \sixg localize significantly better than \intgrad and \ixg, and the performance improves with increasing kernel size. In detail, \sintgrad \emph{on the input layer} with $K\myeq257$ outperforms \gradcam \emph{on the final layer}, despite explaining the full network. While performance on \disc drops slightly as smoothing leaks attributions across grid boundaries, both \sintgrad and \sixg localize well across settings and layers. However, on Resnet18 (\cref{fig:smooth}, bottom), while \sintgrad improves similarly, \sixg does not, which we discuss next.


\myparagraph{Impact of Network Architecture:}\label{subsec:batchnorm}
A key difference between the VGG19 and Resnet152 architectures used in our experiments is that VGG19 does not have batch normalization (BatchNorm) layers.
We note that batch norm effectively randomizes the sign of the input vectors to the subsequent layer, by centering those inputs around the origin (cf.\cite{ioffe2015batch,kiritoshi2019l1}). Since the sign of the input determines whether a contribution (weighted input) is \emph{positive} or \emph{negative}, a BatchNorm layer will randomize the sign of the contribution and the `valence' of the contributions will be encoded in the BatchNorm biases.
To test our hypothesis, we evaluate \sixg on a VGG19 with BatchNorm layers (\cref{fig:smooth}, middle), and observe results similar to Resnet152: i.e., we observe no systematic improvement by increasing the kernel size of the Gaussian smoothing operation. This shows that the architectural choices of a model can have a significant impact on the performance of attribution methods.

\subsection{Qualitative Evaluation using \aggatt}\label{subsec:evalqualitative}
In this section, we present qualitative results using \aggatt for select attributions evaluated on \pg and \disc and multiple layers. 
First, to investigate the qualitative impact of smoothing, we use \aggatt to compare \ixg, \sixg, and \gradcam attributions on \pg on multiple layers.
We employ \aggatt on \disc to highlight specific characteristics and failure cases of some attribution methods.

\myparagraph{\aggatt on \pg:}
We show \aggatt results for \ixg, \sixg, \gradcam, and \lrp at three layers on \pg using VGG19 on the images at the top-left corner (\cref{fig:vis:pg}).
For each method, a set of three rows corresponds to the attributions at input, middle, and final layers. For \sixg, we set $K$ to $129$, $17$, and $9$ respectively.
We further show individual samples (median bin) of the first and last bins per method. 

We observe that the aggregate visualizations are consistent with the quantitative results (\cref{fig:main,,fig:smooth}) and the individual examples shown for each bin. 
The performance improves for \ixg and \gradcam from input to final layer, while \sixg localizes well across three layers. Attributions from \lrp are generally visually pleasing and localize well across layers. Finally, the last two columns show that all the attribution methods perform `poorly' for some inputs; e.g., we find that \ixg and \gradcam on the final layer attribute importance to other subimages if they exhibit features that are consistent with the class in the top-left subimage. 
While the attributions might be conceived as incorrect, we find that many `failure cases' on \pg highlight features that the underlying model might in fact use, even if they are in another subimage. 
Given the lack of ground truth, it is difficult to assess whether these attributions faithfully reflect model behaviour or deficiencies of the attribution methods.
 
Despite explaining significantly more layers, \sintgrad and \sixg at the input layer not only match \gradcam at the final layer quantitatively (\cref{fig:smooth}) and qualitatively (\cref{fig:vis:pg}), but are also highly consistent with it for individual explanations. Specifically, the Spearman rank correlation between the localization scores of \gradcam (final layer) and \sintgrad (input layer) increases significantly as compared to \intgrad (input layer)  (e.g., $0.3$$\rightarrow$$0.78$ on VGG19), implying that their attributions for any input tend to lie in the same \aggatt bins (see supplement). 

To further understand the effect of smoothing, we visualize \sixg with varying kernel sizes while including negative attributions (\cref{fig:vis:ixgsmoothnegative}). The top row shows aggregate attributions across the dataset, while the middle and bottom rows show an example under the \pg and standard localization settings respectively. We observe that while \ixg attributions appear noisy (column 2), smoothing causes positive and negative attributions to cleanly separate out, with the positive attributions concentrating around the object. For instance, in the second row, \ixg attributions concentrate around both the dog and the wolf, but \sixg with $K\myeq129$ correctly attributes only the dog positively. This could indicate a limited effective receptive field (RF) \cite{luo2016understanding} of the models. Specifically, note that for piece-wise linear models, summing the contributions (given by \ixg) over all input dimensions within the RF exactly yields the output logit (disregarding biases). Models with a small RF would thus be well summarised by \sixg for an adequately sized kernel; we elaborate on this in the supplement.

\myparagraph{\aggatt on \disc:}
We visually evaluate attributions on \disc for one method per method family, i.e., from backpropagation-based (\ixg, input layer), activation-based (\gradcam, final layer), and perturbation-based (\rise, input layer) methods at their standard layers (\cref{fig:vis:disc}). The top row corroborates the near-perfect localization shown by the backpropagation-based methods on \disc. The middle row shows that \gradcam attributions concentrate at the top-left and bottom-right corners, which contain images of the same class, since global pooling of gradients makes it unable to distinguish between the two even though only the top-left instance (here) influences classification. Finally, for \rise, we observe that while attributions localize well for around half the images, the use of random masks results in noisy attributions for the bottom half.

\subsection{Evaluation using various LRP Configurations}
\label{subsec:lrp}
\input{fig_box_lrp}
From the previous sections, we saw that \lrp using the configuration by \cite{arias2021explains} outperformed all other attribution methods at all layers. More generally, \lrp \cite{bach2015pixel} is a paradigm that encompasses a family of attribution methods that modify the gradients during backpropagation. The mechanism of relevance propagation is specified by a set of propagation rules used across the network. Rules are selected for each layer usually based on the type of layer and its position in the network, and a mapping of layers to rules constitutes a unique \lrp configuration. Some of the existing backpropagation-based methods that were proposed independently, such as \ixg \cite{shrikumar2017learning} and Excitation Backprop \cite{zhang2018top}, can be viewed as specific configurations of \lrp \cite{montavon2019layer}.

In this section, we study the impact of the choice of rules and their hyperparameters in attribution performance of \lrp.
Specifically, following prior work \cite{montavon2019layer}, we consider a composite configuration (hereafter referred to as LRP-Composite), that applies the $\epsilon$-rule on fully connected layers, the $\gamma$-rule on convolutional layers except the first layer, and the $z^{\mathcal{B}}$-rule on the first convolutional layer. In contrast to the $\epsilon$-rule that weighs positive and negative contributions equally when propagating relevance, the $\gamma$-rule uses a hyperparameter $\gamma$ that increases the weight given to positive contributions. As $\gamma\to\infty$, relevance is propagated only based on positive contributions, and the configuration is identical to the one used in \cite{arias2021explains} and the previous sections (hereafter referred to as LRP-Focus). In our experiments, we investigate the impact of $\gamma$ on performance of \lrp, and evaluate LRP-Composite using values of $\gamma$ in $\{0,0.001,0.01,0.1,0.25\}$. $\gamma=0$ corresponds to using the $\epsilon$-rule where no additional weight is given to positive contributions, and $\gamma=0.25$ is the value that is commonly used (\eg \cite{montavon2019layer}). We also evaluate the setting when $\gamma\to\infty$, \ie using LRP-Focus. Quantitative results for both models on \gridpg can be found in \cref{fig:lrpbox}.

We find that the performance is highly sensitive to the choice of $\gamma$. Low values of $\gamma$ (up to 0.01) localize poorly, particularly at the input layer. For higher values of $\gamma$, including LRP-Focus where $\gamma\to\infty$, the localization performance is high across layers for both models on \gridpg.
We attribute this to the following: if only positive contributions are considered at intermediate layers, the \emph{sign} of the attributions to the last layers will be maintained throughout the backpropagation process. In particular, the distribution of positive and negative attributions at the input layer will be largely dependent on the attributions at the final layer. Hence, since the $\epsilon$-rule performs well at the final layer (similar to \ixg and \intgrad), maintaining the sign of the attributions will lead to good results at the input layer, which the $\gamma$-rule achieves by suppressing negative contributions. We believe that understanding how to better integrate the negative contributions in the backward pass to reflect all model computations is thus an interesting direction to explore in future work.

\myparagraph{Lack of Implementation Invariance:}
As discussed in \cite{sundararajan2017axiomatic}, \lrp in general is not implementation invariant, \ie, functionally equivalent models could be assigned highly dissimilar attribution maps for the same input. In particular, this also holds for the $z^+$-rule, which is used in the best-performing LRP-Focus configuration. This leads to the possibility of controlling which pixels get attributed by appropriately formulating an equivalent model. Importantly, as we show in the supplement, this can also lead to pixels that have no influence on the output logit to get high attributions. This shows that while LRP can be highly performant, one must carefully consider the parameters used and the properties of the setting before using it in practice.