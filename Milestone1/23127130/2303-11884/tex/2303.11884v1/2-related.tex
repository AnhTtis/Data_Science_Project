\section{Related Work}
\label{sec:related}

\myparagraph{Post-hoc attribution methods} 
broadly use one of three main mechanisms. 
\emph{Backpropagation-based} methods \cite{simonyan2013deep,springenberg2014striving,shrikumar2017learning,sundararajan2017axiomatic,srinivas2019full,zhang2018top,bach2015pixel,montavon2019layer} typically rely on the gradients with respect to the input~\cite{simonyan2013deep,shrikumar2017learning,sundararajan2017axiomatic,springenberg2014striving} or with respect to intermediate layers\cite{rebuffi2020there,zhang2018top}.
\emph{Activation-based} methods \cite{zhou2016learning,selvaraju2017grad,chattopadhyay2018grad,ramaswamy2020ablation,wang2020score,jiang2021layercam} weigh activation maps to assign importance, typically of the final convolutional layer.
The activations may be weighted by their gradients\cite{zhou2016learning,selvaraju2017grad,chattopadhyay2018grad,jiang2021layercam} or by estimating their importance to the classification score\cite{ramaswamy2020ablation,wang2020score}. 
\emph{Perturbation-based} methods \cite{ribeiro2016should,petsiuk2018rise,zeiler2013visualizing,fong2017interpretable,dabkowski2017real} treat the network as a black-box and assign importance by observing the change in output on perturbing the input. This is done by occluding parts of the image \cite{ribeiro2016should,petsiuk2018rise,zeiler2013visualizing} or 
optimizing for a mask that maximizes/minimizes class confidence\cite{fong2017interpretable,dabkowski2017real}.

In this work, we evaluate on a diverse set of attribution methods spanning all three categories.

\myparagraph{Evaluation Metrics:}
Several metrics have been proposed to evaluate attribution methods, and can broadly be categorised into \emph{Sanity checks}, 
\emph{localization-}, and \emph{perturbation-based metrics}. 
Sanity checks \cite{adebayo2018sanity,rebuffi2020there,ancona2017towards} test for basic properties attributions must satisfy (e.g., explanations should depend on the model parameters).
Localization-based metrics evaluate how well attributions localize class discriminative features of the input. 
Typically, this is done by measuring how well attributions coincide with object bounding boxes or image grid cells (see below)~\cite{zhang2018top,schulz2020restricting,cao2015look,fong2017interpretable,boehle2021convolutional}.
Perturbation-based metrics measure model behaviour under input perturbation guided by attributions. 
Examples include removing the most\cite{samek2016evaluating} or least\cite{srinivas2019full} salient pixels, or using the attributions to scale input features and measuring changes in confidence\cite{chattopadhyay2018grad}.
Our work combines aspects from localization metrics and sanity checks to evaluate the model-faithfulness of an attribution method.

\myparagraph{Localization on Grids:}
Relying on object bounding boxes for localization assumes that the model only relies on information \emph{within} those bounding boxes.
However, neural networks are known to also rely on context information for their decisions, cf.~\cite{rakshith}.
Therefore, recent work \cite{boehle2021convolutional,shah2021input,arias2021explains} proposes creating a grid of inputs from distinct classes and measuring localization to the entire grid cell, which allows evaluation on datasets where bounding boxes are not available.
However, this does not \emph{guarantee} that the model only uses information from within the grid cell, and may fail for similar looking features (\cref{fig:vis:examplesmethods}, right). In our work, we propose a metric that controls the flow of information and guarantees that grid cells are classified independently.