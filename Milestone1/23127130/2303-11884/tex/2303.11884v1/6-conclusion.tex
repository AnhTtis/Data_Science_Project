\section{Discussion and Conclusion}
\label{sec:discussion}
In this section, we summarize our results, and discuss high-level recommendations. First, we proposed a novel quantitative evaluation setting, \difull, to disentangle the behaviour of the model from that of the attribution method. This allowed us to evaluate for model-faithfulness by partitioning inputs into regions that could and could not influence the model's decision. Using this, we showed that (\cref{fig:main}) some popularly used attribution methods, such as \gradcam, can provide model-unfaithful attributions. On the other hand, while noisy, backpropagation-based methods like \intgrad and \ixg localize perfectly under this setting. We note, however, that our setting cannot evaluate the correctness of attributions within the target grid cells, and as such a high localization performance on \difull is a necessary condition for a good attribution method, but not a sufficient condition. In other words, \difull can be viewed as a coarse sanity check that should be passed by any model-faithful attribution method, but our results show that several do not do so. This could be of practical importance in use cases where models learn to focus on a fixed local region in an image to reach their decisions.

Second, we observed that different attribution methods are typically evaluated at different depths, which leads to them being compared unfairly. To address this, we proposed a multi-layer evaluation scheme, \mlatt, through which we compared each attribution method at identical model depths (\cref{fig:main,fig:scatter:selected}). We found that surprisingly, a diverse set of methods perform very similarly and localize well, particularly at the final layer. This includes backpropagation-based methods like \ixg and \intgrad, which have often been criticized for providing highly noisy and hard to interpret attributions. Combined with their perfect localization on \difull, this shows that \ixg and \intgrad at the final layer can be used as an alternative to \gradcam, when coarse localization is desired.

Third, we proposed an aggregate attribution evaluation scheme, \aggatt, to holistically visualize the performance of an attribution method. Unlike evaluation on a small subset of examples, this shows the full range of localizations across the dataset and eliminates any inadvertent biases from the choice of examples. Furthermore, it allows one to easily visualize the performance at the best and worst localized examples, and could help identify cases when an attribution method unexpectedly fails.

Fourth, we showed that a simple post-hoc Gaussian smoothing step can significantly improve localization (\cref{fig:smooth,fig:vis:ixgsmoothnegative}) for some attribution methods (\intgrad, \ixg). Unlike commonly used smoothing techniques like \smoothgrad, this requires no additional passes through the network and no selection of hyperparameters. As we show in the supplement, it also results in better localized attributions. This shows that while originally noisy, obtaining a local summary of attribution maps from these methods could provide maps that are useful for humans in practice. However, we find that the effectiveness of smoothing is influenced by the network architecture, in particular the presence of batch normalization layers, which suggests that architectural considerations must be taken into account when using attribution methods.

Finally, we find that a configuration of layer-wise relevance propagation (LRP) overall performs the best quantitatively and qualitatively. However, we show that the performance is sensitive to the model architecture, choice of propagation rules at each layer, and even the model formulation. This shows that while LRP can potentially outperform all methods, one needs to carefully
adapt the configuration to the desired use case, and applying it out-of-the-box can lead to unpredictable results.

Overall, we find that fair comparisons, holistic evaluations (\difull, \gridpg, \aggatt, \mlatt), and careful disentanglement of model behaviour from the explanations provide better insights in the performance of attribution methods.