\section{Experimental Setup}
\label{sec:experiments}

\input{fig_attributions_examples}

\myparagraph{Dataset and Architectures:}
We run our experiments on VGG19\cite{simonyan2014very} and Resnet152\cite{he2016deep} trained on Imagenet\cite{imagenet}; similar results on other architectures and on CIFAR10\cite{krizhevsky2009learning} can be found in the supplement.
For each model, we separately select images from the validation set that were classified with a confidence score of at least 0.99. By only using highly confidently classified images\cite{boehle2021convolutional,arias2021explains}, we ensure that the features within each grid cell constitute positive evidence of its class for the model, and features outside it contain low positive evidence since they get confidently classified to a different class.

\myparagraph{Evaluation on \pg, \disc, and \cs:}
We evaluate on $2\mytimes2$ grids constructed by randomly sampling images from the set of confidently classified images (see above). Specifically, we generate 2000 attributions per method for each of \pg, \disc, and \cs. For \pg, we use images from distinct classes, while for \disc and \cs we use distinct classes except in the bottom right corner, where we use the same class as the top left. By repeating the same class twice, we can test whether an attribution method simply highlights class-related features, irrespective of them being used by the model.
Since subimages are disconnected from the classification heads of other locations in \disc and \cs, the use of repeating classes does not change which regions should be attributed (\cref{sec:method:difull}).

\myparagraph{Evaluation at Intermediate Layers:}
We evaluate each method at the input (image), middle\footnote{We show a single intermediate layer to visualize trends from the input to the final layer; for results on all layers, see supplement.
} (Conv9 for VGG19, Conv3\_x for Resnet152), and final spatial layer (Conv16 for VGG19, Conv5\_x for Resnet152) of each network, see \cref{sec:method:intermediate}. Evaluating beyond the input layer leads to lower dimensional attribution maps, given by the dimensions of the activation maps at those layers. 
Thus, as is common practice~\cite{selvaraju2017grad}, we upsample those maps to the dimensions of the image ($448\times 448$) using bilinear interpolation.

\myparagraph{Qualitative Evaluation on \aggatt:}
As discussed, for \aggatt we use bins of unequal sizes (\cref{sec:method:aggatt}). In particular, we bin the attribution maps into the following percentile ranges: 0--2\%, 2--5\%, 5--50\%, 50--95\%, 95--98\%, and 98--100\%; cf.~\cref{fig:teaser}. Further, in our experiments we evaluate the attributions for classes at the top-left grid location.

\myparagraph{Attribution Methods:}
We evaluate a diverse set of attribution methods, for an overview see \cref{sec:related}. As discussed in \cref{sec:method:intermediate}, to apply those methods to intermediate network layers, we divide the full model into two virtual parts $\mathbf f_\text{pre}$ and $\mathbf f_\text{explain}$ and treat the output of  $\mathbf f_\text{pre}$ as the input to $\mathbf f_\text{explain}$ to obtain importance attributions for those `pre-processed' inputs. In particular, we evaluate the following methods. 
From the set of \textbf{backpropagation-based methods}, we evaluate on \gb\cite{springenberg2014striving}, \grad\cite{simonyan2013deep}, \intgrad\cite{sundararajan2017axiomatic}, \ixg\cite{shrikumar2017learning}, and \lrp\cite{bach2015pixel}. 
From the set of \textbf{activation-based methods}, we evaluate on \gradcam\cite{selvaraju2017grad}, \gradcampp\cite{chattopadhyay2018grad}, \ablationcam\cite{ramaswamy2020ablation}, \scorecam\cite{wang2020score}, and \layercam\cite{jiang2021layercam}. 
Note that in our framework, these methods can be regarded as using the classification head only (except \cite{jiang2021layercam}) for $\mathbf f_\text{explain}$, see \cref{sec:method:intermediate}. In order to evaluate them at earlier layers, we simply expand $\mathbf f_\text{explain}$ accordingly to include more network layers.
From the set of \textbf{perturbation-based methods}, we evaluate \occ\cite{zeiler2013visualizing} and \rise\cite{petsiuk2018rise}. These are typically evaluated on the input layer, and measure output changes when perturbing (occluding) the input (\cref{fig:vis:examplesmethods}, left). 
Note that \occ involves sliding an occlusion kernel of size $K$ with stride $s$ over the input.
We use $K\myeq16,s\myeq8$ for the input, and $K\myeq5,s\myeq2$ at the middle and final layers to account for the lower dimensionality of the feature maps.
For \rise, we use $M\myeq1000$ random masks, generated separately for evaluations at different network layers.

For \lrp, following \cite{arias2021explains,montavon2017explaining}, 
we primarily use a configuration that applies the $\epsilon$-rule with $\epsilon\myeq0.25$ for the fully connected layers in the network, the $z^+$-rule for the convolutional layers except the first convolutional layer, and the $z^{\mathcal{B}}$-rule for the first convolutional layer. We discuss the performance across other configurations, including the composite configuration proposed by \cite{montavon2019layer}, in \cref{subsec:lrp}. Note that since certain \lrp rules, such as the $z^+$-rule, are not implementation invariant (\cite{sundararajan2017axiomatic}), relevance may be distributed differently for functionally equivalent models. In particular, relevance propagation through batch normalization layers can be handled in multiple ways, such as by replacing them with $1\mytimes1$ convolutions or by merging them with adjacent linear layers. In our experiments, as in
\cite{montavon2019layer}, 
batch normalization layers are handled by merging them with adjacent convolutional or fully connected layers. We further discuss some ramifications of the lack of implementation invariance to attribution localization in \cref{subsec:lrp} and the supplement.
