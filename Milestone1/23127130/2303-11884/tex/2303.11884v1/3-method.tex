\section{Evaluating Attribution Methods}
\label{sec:method}

\input{fig_arch}


We present our evaluation settings for better understanding the strengths and shortcomings of attribution methods. Similar to the Grid Pointing Game (\gridpg)\cite{boehle2021convolutional}, these metrics evaluate attribution methods on image grids with multiple classes. In particular, we propose a novel quantitative metric, \difull, and an extension to it, \dipart (\ref{sec:method:quantitative}), as stricter tests of model faithfulness than \gridpg. Further, we present a qualitative metric, \aggatt (\ref{sec:method:aggatt}) and an evaluation setting that compares methods at identical layers, \mlatt (\ref{sec:method:intermediate}).

\subsection{Quantitative Evaluation: Disconnecting Inputs}\label{sec:method:quantitative}
In the following, we introduce the quantitative metrics that we use to compare attribution methods. For this, we first describe \pg and the grid dataset construction it uses\cite{boehle2021convolutional}. We then devise a novel setting, in which we carefully control which features can influence the model output. 
By construction, this provides ground truth annotations for image regions that can or cannot possibly have influenced the model output. While \pg evaluates how well the methods localize class discriminative features, our metrics complement it by evaluating their \emph{model-faithfulness}.

\mysubsub{Grid Data and \pg}\label{sec:method:gridpg}
For \pg~\cite{boehle2021convolutional}, the attribution methods are evaluated on a synthetic grid of $n\times n$ images in which each class may occur at most once. In particular, for each of the occurring classes, \pg measures the fraction of positive attribution assigned to the respective grid cell versus the overall amount of positive attribution. Specifically, let $A^+(p)$ refer to the positive attribution given to the $p^{th}$ pixel. The localization score for the subimage $x_i$ is given by:
\begin{equation}
    \label{eq:localisation}
    L_i = \frac{\sum_{p\in x_i} A^+(p)}{\sum_{j=1}^{n^2}\sum_{p\in x_j} A^+(p)} 
\end{equation}
An `optimal' attribution map would thus yield $L_i\myeq1$, while uniformly distributing attributions would yield $L_i\myeq\frac{1}{n^2}$.
 
 By only using confidently classified images from distinct classes, \pg aims to ensure that the model does not find `positive evidence' for any of the occurring classes in the grid cells of other classes. 
 However, specifically for class-combinations that share low-level features, this assumption might not hold, see \cref{fig:vis:examplesmethods} (right): despite the two dogs (upper left and lower right) being classified correctly as single images, the output for the logit of the dog in the upper left is influenced by the features of the dog in the lower right in the grid image.
 Since all images in the grid can indeed influence the model output in \pg\footnote{As shown in \cref{fig:arch:pg}, the convolutional layers of the model under consideration process the entire grid to obtain feature maps, which are then classified point-wise. Finally, a \emph{single output per class} is obtained by globally pooling all point-wise classification scores. As such, the class logits can, of course, be influenced by all images in the grid.}, it is unclear whether such an attribution is in fact not \emph{model-faithful}.
 
\mysubsub{Proposed Metric: \disc}\label{sec:method:difull}
As discussed, the assumption in \pg that no feature outside the subimage of a given class should positively influence the respective class logit might not hold. Hence, we propose to \underline{full}y \underline{di}sconnect (\disc) the individual subimages from the model outputs for other classes. 
For this, we introduce two modifications. First, after removing the GAP operation, we use $n \times n$ classification heads, one for each subimage, and only \emph{locally} pool those outputs that have their receptive field center \emph{above the same subimage}. Second, we ensure that their receptive field {does not overlap} with other subimages by zeroing out the respective connections. 

In particular, we implement \disc by passing the subimages separately through the CNN backbone of the model under consideration\footnote{
This is equivalent to setting the respective weights of a convolutional kernel to zero every time it overlaps with another subimage.}, see \cref{fig:arch:disc}. Then, we apply the classification head separately to the feature maps of each subimage. As we discuss in the supplement, \difull has similar computational requirements as \gridpg.

As a result, we can \emph{guarantee} that no feature outside the subimage of a given class can possibly have influenced the respective class logit---they are indeed \emph{fully disconnected}.

\mysubsub{Natural Extension: \cs}\label{sec:method:dipart}
At one end, \pg allows any subimage to influence the output for any other class, while at the other, \disc completely disconnects the subimages. In contrast to \pg, \disc might be seen as a constructed setting not seen in typical networks. 
As a more natural setting, we therefore propose \cs, for which we only \underline{part}ially \underline{di}sconnect the subimages from the outputs for other classes, see \cref{fig:arch:cs}.  Specifically, we do not zero out all connections (\cref{sec:method:difull}), but instead only apply the local pooling operation from \disc and thus obtain local classification heads for each subimage (as in \disc). 
However, in this setting, the classification head for a specific subimage can be influenced by features in other subimages that lie within the head's receptive field. For models with a small receptive field, this yields very similar results as \disc (\cref{sec:results} and Supplement).

\subsection{Qualitative Evaluation: \aggatt}\label{sec:method:aggatt}
In addition to quantitative metrics, attribution methods are often compared qualitatively on individual examples for a visual assessment. However, this is sensitive to the choice of examples and does not provide a holistic view of the method's performance.
By constructing standardized grids, in which `good' and `bad' (\pg) or \emph{possible} and \emph{impossible} (\disc) attributions are always located in the same regions, we can instead construct \emph{aggregate attribution maps}. 

Thus, we propose a new qualitative evaluation scheme, \mbox{\bf\aggatt}, for which we generate a set of aggregate maps for each method that progressively show the performance of the methods from the best to the worst localized attributions. 

For this, we first select a grid location and then sort all corresponding attribution maps in descending order of the localization score, see \cref{eq:localisation}. Then, we bin the maps into percentile ranges and, finally, obtain an aggregate map per bin by averaging all maps within a single bin. In our experiments, we observed that attribution methods typically performed consistently over a wide range of inputs, but showed significant deviations in the tails of the distributions (best and worst case examples). Thus, 
to obtain a succinct visualization that highlights both distinct failure cases as well as the best possible results, we use bins of unequal sizes. Specifically, we use smaller bins for the top and bottom percentiles. For an example of \aggatt, see \cref{fig:teaser}.

As a result, \aggatt allows for a \emph{systematic} qualitative evaluation and provides a holistic view of the performance of attribution methods across many samples.

\subsection{Attributions Across Network Layers: ML-Att}\label{sec:method:intermediate}
Attribution methods often vary significantly in the degree to which they explain a model. Activation-based attribution methods like \gradcam\cite{selvaraju2017grad}, e.g., are typically applied on the last spatial layer, and thus only explain a fraction of the full network.
This is a significantly easier task as compared to explaining the entire network, as is done by typical backpropagation-based methods. Activations from deeper layers of the network would also be expected to localize better, since they would represent the detection of higher level features by the network (\cref{fig:teaser}, left).

For a \emph{fair comparison} between methods, we thus propose a multi-layer evaluation scheme for attributions ({\bf ML-Att}). Specifically, we evaluate methods at various network layers and compare their performance \emph{on the same layers}. For this, we evaluate all methods at the input, an intermediate, and the final spatial layer of multiple network architectures, see \cref{sec:experiments} for details.
Importantly, we find that apparent differences found between some attribution methods vanish when compared \emph{fairly}, i.e., on the same layer (\cref{subsec:layer_results}).

Lastly, we note that most attribution methods have been designed to assign importance values to \emph{input features} of the model, not \emph{intermediate} network activations. The generalisation to intermediate layers, however, is straightforward. For this, we simply divide the full model $\mathbf f_\text{full}$ into two virtual parts: $\mathbf f_\text{full}\myeq\mathbf f_\text{explain}\circ \mathbf f_\text{pre}$. Specifically, we treat $\mathbf f_\text{pre}$ as a pre-processing step and use the attribution methods to explain the outputs of $\mathbf f_\text{explain}$ with respect to the inputs $\mathbf f_\text{pre}(\mathbf x)$. Note that in its standard use case, in \gradcam $\mathbf f_\text{pre}(\mathbf x)$ is given by all convolutional layers of the model, whereas for most gradient-based methods  $\mathbf f_\text{pre}(\mathbf x)$ is the identity.