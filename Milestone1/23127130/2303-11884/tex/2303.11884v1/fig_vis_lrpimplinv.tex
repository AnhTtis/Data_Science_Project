\begin{figure}[H]
	\centering
	\begin{subfigure}{0.346\linewidth}
	    \centering
	    \includegraphics[width=0.5\linewidth]{figures/ZPlusSplitMatrix-orig.png}
	    \caption{Original}
		\label{fig:vis:lrp:implinv:transform:before}
	\end{subfigure}
	\hfill
	\begin{subfigure}{0.64\linewidth}
	    \centering
	    \includegraphics[width=0.5\linewidth]{figures/ZPlusSplitMatrix-split.png}
	    \caption{Transformed}
		\label{fig:vis:lrp:implinv:transform:after}
	\end{subfigure}
	\caption{\textbf{Transform that inserts a new layer between the final feature layer and the first classification layer in the \difull setting.} The resultant model is functionally equivalent to the original model. We add a new layer with a neuron that each aggregate all activations from all regions with zero connections with positive and negative sign respectively. The contributions from these two neurons cancel out in the next layer, keeping the model functionally equivalent. However, the $z^+$ rule only considers neurons that contribute positively to the output, causing almost all relevance to flow through the new positive neuron and to the disconnected regions (as shown in \cref{fig:vis:lrp:implinv:example}).}
	\label{fig:vis:lrp:implinv:transform}
\end{figure}
\begin{figure}[H]
	\centering
		\includegraphics[width=0.9\linewidth]{figures/vis_lrp_implinv.png}
		\caption{\textbf{LRP Attributions for VGG19 in the \difull setting before and after the transform.} As the $z^+$ propagation rule is not implementation invariant, appropriately transforming the model to a functionally equivalent model can result in vastly different attribution maps.}
		\label{fig:vis:lrp:implinv:example}
\end{figure}