\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{iccv}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage[table]{xcolor}
\usepackage{booktabs}
\usepackage{tabularx}
\usepackage{gensymb}
\usepackage{enumitem}
\usepackage{multirow}
\usepackage{makecell}
\usepackage{caption}
\usepackage{subcaption}
\captionsetup{belowskip=0pt,aboveskip=2pt}
%\usepackage[subtle]{savetrees}
% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}

\iccvfinalcopy % *** Uncomment this line for the final submission

\def\iccvPaperID{12192} % *** Enter the ICCV Paper ID here

% Pages are numbered in submission mode, and unnumbered in camera-ready
%\ificcvfinal\pagestyle{empty}\fi

\begin{document}

%%%%%%%%% TITLE
\title{Forecasting localized weather impacts on vegetation as seen from space with meteo-guided video prediction}

\author{Vitus Benson\textsuperscript{1,2,3,*} \and Christian Requena-Mesa\textsuperscript{1,2} \and Claire Robin\textsuperscript{1,2} \and Lazaro Alonso\textsuperscript{1} \and José Cortés\textsuperscript{1} \and Zhihan Gao\textsuperscript{4} \and Nora Linscheid\textsuperscript{1} \and Mélanie Weynants\textsuperscript{1} \and Markus Reichstein\textsuperscript{1,2} \\
\textsuperscript{1} Max-Planck-Institute for Biogeochemistry  \quad \textsuperscript{2} ELLIS Unit Jena  \quad \textsuperscript{3} ETH Zürich  \\ \textsuperscript{4} Hong Kong University of Science and Technology  \quad \textsuperscript{*} {\tt vbenson@bgc-jena.mpg.de}
}

\maketitle
% Remove page # from the first page of camera-ready.
\ificcvfinal\thispagestyle{empty}\fi


%%%%%%%%% ABSTRACT
\begin{abstract}
   We present a novel approach for modeling vegetation response to weather in Europe as measured by the Sentinel 2 satellite. Existing satellite imagery forecasting approaches focus on photorealistic quality of the multispectral images, while derived vegetation dynamics have not yet received as much attention. We leverage both spatial and temporal context by extending state-of-the-art video prediction methods with weather guidance. We extend the EarthNet2021 dataset to be suitable for vegetation modeling by introducing a learned cloud mask and an appropriate evaluation scheme. Qualitative and quantitative experiments demonstrate superior performance of our approach over a wide variety of baseline methods, including leading approaches to satellite imagery forecasting. Additionally, we show how our modeled vegetation dynamics can be leveraged in a downstream task: inferring gross primary productivity for carbon monitoring. To the best of our knowledge, this work presents the first models for continental-scale vegetation modeling at fine resolution able to capture anomalies beyond the seasonal cycle, thereby paving the way for predictive assessments of vegetation status.
\end{abstract}


\begin{figure}[t]
    \centering
    \includegraphics[width=\columnwidth]{figures/fig1draft-0.4}
    \caption{In this work, future vegetation status $\hat{V}$ is predicted with deep learning models $f$ from past satellite imagery $X$, past and future weather $C$ and elevation $E$. The underlying dataset spans across Europe with minicubes split into train (red dots), temporal OOD test (ood-t, orange dots) and spatio-temporal OOD test (ood-st, blue dots) subsets.}
    \label{fig:task}
\end{figure}

\section{Introduction}
Optical satellite images have been proven to be useful for monitoring vegetation status. This is necessary for a variety of applications in agriculture, forestry, humanitarian aid or carbon accounting. In all these cases, prognostic information is relevant: Farmers want to know how their farmland may react to a given weather scenario \cite{wolanin.etal_2019}. Humanitarian organisations need to understand the localized impact of droughts on pastoral communities for mitigation of famine with anticipatory action \cite{meshesha.etal_2020}. Afforestation efforts need to consider how their forests react to future climate \cite{sturm.etal_2022}.

However, such prognostic information is often not available at fine resolution. Even near-realtime data, is often still lacking. For instance in cloudy regions multiple weeks may pass before a clear-sky observation is available.


Forecasting optical satellite imagery as a way to tackle both issues has recently been investigated with video prediction methods \cite{diaconu.etal_2022, kladny.etal_2022, requena-mesa.etal_2021, robin.etal_2022} on the EarthNet2021 dataset \cite{requena-mesa.etal_2021}. These models are able to forecast satellite imagery of high perceptual quality. However, their skill at modeling vegetation dynamics is harder to assess: despite EarthNet2021 being the largest such dataset for high resolution landscape forecasting \cite{xiong.etal_2022}, a faulty cloud mask, insufficient baselines and a poorly interpretable evaluation protocol still limit suitability of the EarthNet2021 dataset for vegetation prediction.

In this paper, we approach continental-scale modeling of vegetation dynamics. To achieve this, we predict remotely sensed vegetation greenness at 20m conditioned on coarse-scale weather. We extend the EarthNet2021 dataset \cite{requena-mesa.etal_2021} to be suitable for the task by improving cloud mask and spatio-temporal test sets. We then extend state-of-the-art approaches for video prediction with weather conditioning. Fig.~\ref{fig:task} presents a sketch of our approach: Future vegetation state ($\hat{V}$) is predicted from satellite image spectra ($X$), past and future weather data ($C$), and elevation information ($E$) via deep learning ($f$).


Our major \textbf{contributions} can be summarized as follows. \textbf{(1)} We expand the EarthNet2021 dataset with a learned cloud mask and a new evaluation scheme to be suitable for vegetation prediction. \textbf{(2)}  We present State-of-the-Art models for vegetation prediction, outperforming the top-3 approaches on the EarthNet2021 challenge and multiple other strong baselines both qualitatively and quantitatively. \textbf{(3)} We show how our results can be applied for prognostic carbon monitoring in a real-world use case.  

Find our source code at \url{https://github.com/earthnet2021/earthnet-models-pytorch}.

\section{Related Work}
\textbf{Vegetation Modeling} %MODIS scale NDVI.
Vegetation modeling from remote sensing has a long tradition at coarse resolution, e.g. from the AVHRR or MODIS satellites \cite{ji.peters_2004,kraft.etal_2019, lees.etal_2022, zeng.etal_2022}. Since 2015, the Sentinel 2 satellites deliver imagery at high resolution (up to $10$m). Several studies have used this data for regional crop yield modeling \cite{schwalbert.etal_2018, engen.etal_2021} and regional vegetation forecasting \cite{ferchichi.etal_2022, yu.etal_2022}. With EarthNet2021 \cite{requena-mesa.etal_2021}, the first dataset for continental-scale satellite imagery forecasting was introduced. Subsequent works leveraged the ConvLSTM model \cite{shi.etal_2015} for satellite imagery prediction \cite{diaconu.etal_2022, kladny.etal_2022} and for vegetation prediction in Africa \cite{robin.etal_2022}. Another line of work focuses on imputing cloudy time steps \cite{meraner.etal_2020, yang.etal_2022}, yet often with a focus on historical gapfilling instead of near-realtime information. 

\textbf{Spatio-temporal learning}
The ConvLSTM \cite{shi.etal_2015} was first introduced for precipitation nowcasting. Subsequently, spatio-temporal forecasting of the Earth system has gained traction, with strong results not only on precipitation nowcasting \cite{ravuri.etal_2021a, shi.etal_2017}, but also on weather forecasting \cite{bi.etal_2022, lam.etal_2022, pathak.etal_2022}, climate projection \cite{nguyen.etal_2023} and wildfire modeling \cite{kondylatos.etal_2022}. Beyond the Earth system, video prediction is spatio-temporal learning. State-of-the-art video prediction models use ConvNets \cite{babaeizadeh.etal_2021, gao.etal_2022a}, ConvLSTM successors \cite{wang.etal_2023, wu.etal_2021} or Transformers \cite{gupta.etal_2022a, nash.etal_2022a}. A sub-area of video prediction uses action conditioning: predicting future frames by giving a future action in video games \cite{oh.etal_2015} or robot experiments \cite{babaeizadeh.etal_2018, finn.etal_2016}.


\section{Methods}

\subsection{Task}
We predict the future NDVI, a remote sensing proxy of vegetation state ($V^t \in \mathbb{R}^{H\times W}, t \in [T+1, T+K]$) conditioned on past satellite imagery ($X^t \in \mathbb{R}^{H\times W}, t \in [1, T]$), past and future weather ($C^t \in \mathbb{R}, t \in [1, T+K]$) and static elevation maps ($E \in \mathbb{R}^{H\times W}$). Hence, denoting a model $f(.;\theta)$ with parameters $\theta$, we obtain vegetation predictions as:
\begin{align}
    \hat{V}^{T+1:T+K} = f(X^{1:T}, C^{1:T+K}, E; \theta)
\end{align}
In this paper most models are deep neural networks, trained with stochastic gradient descent to maximize a Gaussian Likelihood. More specifically, the optimal parameters $\theta^{*}$ are obtained by minimizing the mean squared error over valid pixels $V_{*}^t = V^t \odot M_Q^t \odot M_L$, where $M_Q \in \{0,1\}^{H\times W}$ masks pixels that are cloudy, cloud shadow or snow, $M_L \in \{0,1\}^{H\times W}$ masks pixels that are not cropland, forest, grassland or shrubland and $\odot$ denotes elementwise multiplication. Hence the training objective (leaving out dimensions for simplicity) is
\begin{align}
    \theta^{*} = \underset{\theta}{arg min} \frac{\sum (V - \hat{V})^2 \odot M_Q \odot M_L}{\sum M_Q \odot M_L}
\end{align}
In this work $H=W=128\text{px}, T=10$ and $K=20$.

 \begin{figure*}[t]
    \centering
    \includegraphics[width=\textwidth]{figures/fig2draft-0.2}
    \caption{Simplified view of evaluated models. Baselines (a,b), weather-guided deep learning (c,d,e,f).}
    \label{fig:models}
\end{figure*}

\subsection{Models}\label{sec:models}
This study focusses modeling around meteo-guided deep learning. We study in-depth four models which are representative for their respective model class. Two models perform next-frame prediction and leverage internal memory (ConvLSTM and PredRNN) to perform iterative roll-out. Two models perform next-cuboid prediction (SimVP and Earthformer), thereby modeling the full target period temporal dynamics at once. PredRNN, SimVP and Earthformer follow an \emph{encode-process-decode} \cite{battaglia.etal_2018a} configuration. Encoders and decoders operate in the spatial domain without any temporal fusion, while the processor translates latent features spatio-temporally. For encoding and decoding we leverage ConvNets, which are the standard in the domain of satellite remote sensing. The models are sketched in fig.~\ref{fig:models} and described below.

\textbf{ConvLSTM-meteo} We follow the original ConvLSTM work \cite{shi.etal_2015} and use an encoding-forecasting setup (fig.~\ref{fig:models}c). It consists of two networks, each containing two ConvLSTM cells, without parameter sharing: One for the context period which works with past satellite imagery and past weather, and one for the target period, only using future weather as input. This is in contrast to the ConvLSTM flavors previously studied on EarthNet2021 \cite{diaconu.etal_2022, kladny.etal_2022}, but has been shown to work better on a similar problem in Africa \cite{robin.etal_2022}. 

\textbf{PredRNN-meteo} The PredRNN video prediction model \cite{wang.etal_2017, wang.etal_2023} is a ConvLSTM with two memory states: one for longer-term dynamics that passes information at the same depth level over time, and one for more complex short-term dynamics, with an information flow that zigzags through levels over time (called ST-LSTM). We propose a similar network to the action-conditioned PredRNN \cite{wang.etal_2023}: using ConvNet encoder and decoder and conditioning in each memory cell. We generalize their action-conditioning by using feature-wise linear modulation \cite{perez.etal_2018} for weather conditioning on the inputs (fig.~\ref{fig:models}e).

\textbf{SimVP-meteo} The SimVP video prediction model \cite{tan.etal_2023} is an encode-process-decode model with a ConvNet processor called Gated Spatiotemporal Attention Translator. It achieves temporal modeling by stacking the features of all time steps along the channel dimension. Each block then processes first with depth-wise convolution in the spatial domain, then with channel-wise convolutions in the temporal domain and finally gates with an attention layer. We achieve weather conditioning by feature-wise linear modulation \cite{perez.etal_2018} on the latent embeddings at each stage of the processor (fig.~\ref{fig:models}d).

\textbf{Earthformer-meteo} The Earthformer Earth system forecasting model \cite{gao.etal_2022} uses cuboid attention to process spatio-temporal chunks of information. It uses different cuboids of each input tensor as tokens for self- and cross-attention. Multiple cuboid attention modules are composed in a UNet like architecture. To tame the memory usage of the attention mechanisms, ConvNet encoder and decoder are used. Weather conditioning is achieved with early fusion during context steps and latent fusion during target steps (fig.~\ref{fig:models}f).


\subsection{Baselines}\label{sec:baselines}
We compare against several baselines:

\textbf{Non-ML baselines} We build three non-ML baselines. The persistence baseline, as in EarthNet2021 \cite{requena-mesa.etal_2021}, constantly predicts the last valid NDVI observation from the context period. The previous year baseline \cite{robin.etal_2022} predicts the NDVI that was observed one year ago, obtained by interpolating last years observations linearly. The climatology baseline is produced by interpolating the NDVI timeseries leaving out the desired target year, then taking the mean over years, and then smoothing with a one month box-filter (fig.~\ref{fig:models}a). 

\textbf{Local timeseries models} We compare against three commonly used time series models: Kalman filter, LightGBM \cite{ke.etal_2017} and Prophet \cite{taylor.letham_2018} from the Python library darts \cite{herzen.etal_2022}. These are trained on timeseries from a single pixel and applied to forecast this pixel, given future weather as covariates. Since they are fit for every pixel separately, running such timeseries models is expensive. Predicting a single minicube takes $\sim3$h on an 8-CPU machine, which is $\mathcal{O}(10^4)$ slower than the deep learning approaches. 


\textbf{LSTM} A much faster timeseries model is the LSTM when trained globally. We implement a pixelwise LSTM as a ConvLSTM with 1x1 kernel size. It can not make use of spatial context, but does use temporal memory.

\textbf{Next-frame UNet} The next-frame UNet predicts autoregressively without memory the vegetation of the next time step. This is a common baseline for weather prediction, a task with insignificant memory effects \cite{rasp.etal_2020}.

\textbf{Next-cuboid UNet} The next-cuboid UNet works in chunks: It stacks all context time steps along the channel dimension and outputs all target time steps at once. It is similar to SimVP, but does early spatio-temporal fusion.

\subsection{Implementation details} We build all of our ConvNets with a PatchMerge-style architecture similar to the one in Earthformer \cite{gao.etal_2022}. For SimVP and PredRNN, such encoders and decoders are more powerful, but also slightly more parameter-intensive, than the variants used in the original papers. We use GroupNorm \cite{wu.he_2018} and LeakyReLU activation \cite{xu.etal_2015b}. Skip connections preserve high-fidelity content between encoders and decoders. Our framework is implemented in PyTorch, and models are trained on Nvidia A40 and A100 GPUs. We use the AdamW \cite{loshchilov.hutter_2022} optimizer and tune the learning rate per model. More implementation details can be found in the supplementary materials.

\begin{table}[]
    \centering
    \begin{tabularx}{\columnwidth}{Xcccc}
    \toprule
    Algorithm & Works on EN21 & Prec & Rec & F1 \\
    \midrule
    Sen2Cor & Yes	& 0.83 & 0.60 & 0.70 \\
    FMask & No & 0.85 & 0.85 & 0.85\\
    KappaMask & No & 0.74 & 0.88 & 0.81\\
    UNet RGBNir & Yes & \underline{0.91} & 0.90 & \underline{0.90}\\
    \qquad /w SCL & Yes & 0.83 & \textbf{0.93} & 0.88\\
    UNet 13Bands & No & \textbf{0.94} & \underline{0.92} & \textbf{0.93}\\
    \bottomrule
    \end{tabularx}
    \caption{Precision, recall and F1-score of different Sentinel~2 cloud masking algorithms.}
    \label{tab:cloudmask}
\end{table}



\begin{table*}[t]
    \centering
    \begin{tabularx}{\textwidth}{>{\footnotesize\scshape}cXccccccc}
    \toprule
    & Model & $R^2$ $\uparrow$ & RMSE $\downarrow$ & NSE $\uparrow$ &   $|\text{bias}|$ $\downarrow$ & $\underset{\text{Climatology}}{\text{Outperform}}$ $\uparrow$& $\underset{25 \text{ days}}{\text{RMSE}}$ $\downarrow$& \#Params\\
    \midrule
    \parbox[t]{1mm}{\multirow{3}{*}{\rotatebox[origin=c]{90}{non-ML}}}& Persistence & 0.00 &      0.23 &      -1.28 &      0.17 &      21.8\% &      \textbf{0.09}  & 0 \\
    & Previous year &  0.56 &      0.20 &      -0.40 &      0.14 &      19.3\% &      0.18  & 0 \\
    & Climatology & 0.58 &      0.18 &      -0.34 &      0.13 &       0.0\% &      0.16 & 0 \\
    \arrayrulecolor{black!30}\midrule
    \parbox[t]{1mm}{\multirow{3}{*}{\rotatebox[origin=c]{90}{local TS}}} & Kalman filter &      0.41 &      0.19 &      -0.57 &      0.13 &      27.0\% &      0.16  & $\mathcal{O}$(10)\\
    & LightGBM &      0.51 &      0.17 &      -0.22 &      0.12 &      42.2\% &      0.11  & n.a.\\
    & Prophet & 0.57 &      0.16 &      -0.05 &      0.11 &      60.6\% &      0.13  & $\mathcal{O}$(10) \\
    \arrayrulecolor{black!30}\midrule
    \parbox[t]{1mm}{\multirow{3}{*}{\rotatebox[origin=c]{90}{EN21}}} & ConvLSTM \cite{diaconu.etal_2022} & 0.51 &      0.18 &      -0.37 &      0.12 &      43.9\% &      0.12  & 0.2M \\
    & SG-ConvLSTM \cite{kladny.etal_2022} & 0.53 &      0.19 &      -0.33 &      0.14 &      45.8\% &      0.11  & 0.7M \\
    & Earthformer \cite{gao.etal_2022} &  0.49 &      0.17 &      -0.27 &      0.12 &      47.2\% &      0.11  &  60.6M\\
    \arrayrulecolor{black!30}\midrule
    \parbox[t]{1mm}{\multirow{4}{*}{\rotatebox[origin=c]{90}{This Study}}} & ConvLSTM-meteo & \textbf{0.62} \footnotesize{$\pm$0.01} & \textbf{0.14} \footnotesize{$\pm$0.00} &  \textbf{0.11} \footnotesize{$\pm$0.03} & 0.10 \footnotesize{$\pm$0.00} & \textbf{68.2\%} \footnotesize{$\pm$1.8\%} & 0.10 \footnotesize{$\pm$0.00} & 1.0M \\
    & PredRNN-meteo & \textbf{0.62} \footnotesize{$\pm$0.00} & 0.15 \footnotesize{$\pm$0.00} &  0.03 \footnotesize{$\pm$0.00} & 0.10 \footnotesize{$\pm$0.00} & 64.7\% \footnotesize{$\pm$1.2\%} & 0.10 \footnotesize{$\pm$0.00} & 1.4M \\
    & SimVP-meteo & 0.60 \footnotesize{$\pm$0.00} & 0.15 \footnotesize{$\pm$0.00} &  0.03 \footnotesize{$\pm$0.01} & \textbf{0.09} \footnotesize{$\pm$0.00} & 64.1\% \footnotesize{$\pm$1.0\%} & 0.10 \footnotesize{$\pm$0.00} & 6.6M \\
    & Earthformer-meteo & 0.52 &      0.16 &      -0.13 &      0.10 &      56.5\% & \textbf{0.09} & 60.6M\\
    %\quad no weather & 0.48& 0.49 & -8.14 &  0.46 & 0.0\% & 34.1M & $\sim$0.05s\\
    \arrayrulecolor{black}\bottomrule
    \end{tabularx}
    \caption{Quantitative Results. For ConvLSTM-meteo, PredRNN-meteo and SimVP-meteo, we report the mean ($\pm$std. dev.) from three different random seeds.}
    \label{tab:quantitative}
\end{table*}


\subsection{Data}
EarthNet2021 \cite{requena-mesa.etal_2021} is a dataset for Earth surface forecasting, that is weather-conditioned satellite image prediction. It contains spatio-temporal minicubes, that are a collection of $30$ 5-daily satellite images ($10$ context, $20$ target), $150$ daily meteorological observations and an elevation map. Spatial dimensions are $128\times 128$px ($2.56\times 2.56$km). We make the dataset suitable for vegetation modeling:

\textbf{Cloud mask} Vegetation proxies derived from optical satellite imagery are only meaningful if observations with clouds, shadows and snow are excluded. The cloudmask in EarthNet2021 is faulty. We train a UNet with Mobilenetv2 encoder \cite{sandler.etal_2018} on the CloudSEN12 dataset \cite{aybar.etal_2022} to detect clouds and cloud shadows from RGB and Nir bands. Tab.~\ref{tab:cloudmask} compares precision, recall and F1 scores of detecting faulty pixels. Our approach outperforms Sen2Cor \cite{louis.etal_2016} (used in EarthNet2021), FMask \cite{qiu.etal_2019} and KappaMask \cite{domnich.etal_2021} baselines by a large margin. If using the Sentinel 2 SCL band in addition, to allow for snow masking, precision drops, but recall increases: i.e. the cloud mask gets more conservative. Using all 13 Sentinel 2 L2A bands is better than just using 4 bands, however such a model would not be directly applicable on EarthNet2021 data.

\textbf{Test sets} EarthNet2021 comes with four test sets. Yet, all of them contain data during the same period as training data, only at different locations (with varying degree of separation). Since weather has high spatial correlation lengths, model performance might be overestimated by evaluating at similar times but different locations.  To tackle this, we introduce four new test sets and a new validation set:
\begin{itemize}[noitemsep,topsep=0pt]
    \item \emph{OOD-t} contains 245 minicubes from the EarthNet2021 IID testset, stratified by Sentinel 2 tile, years 2021-2022
    \item \emph{val} contains 245 minicubes from the EarthNet2021 IID testset, stratified by Sentinel 2 tile, year 2020
    %\item \emph{IID} containes 245 minicubes from the EarthNet2021 IID testset, stratified by Sentinel 2 tile, year 2020
    \item \emph{OOD-s} contains 800 minicubes stratified over $1\degree \times 1\degree$ lat-lon grid cells outside EarthNet2021 train regions, years 2017-2019
    \item \emph{OOD-st} contains 800 minicubes stratified over $1\degree \times 1\degree$ lat-lon grid cells outside EarthNet2021 train regions, for the years 2021-2022
\end{itemize}
\emph{OOD-t} is the main test set used throughout this study. It tests the models' ability to extrapolate in time: i.e. we allow it to learn from past information about a location and want to know how it would perform in the future. \emph{val} follows the same reasoning and hence allows for early stopping of models according to their temporal extrapolation skill. \emph{OOD-s} and \emph{OOD-st} test spatial extrapolation, as well as spatio-temporal extrapolation. For all test sets, we create minicubes over four periods during the European growing season \cite{rotzer.chmielewski_2001} each year: Predicting March-May (MAM), May-July (MJJ), July-September (JAS) and September-November (SON).

\textbf{Additional Layers} We add the ESA Worldcover Landcover map \cite{zanagadaniele.etal_2021} for selecting only vegetated pixels during evaluation, the Geomorpho90m Geomorphons map \cite{amatulli.etal_2020} for further evaluation and the ALOS \cite{tadono.etal_2016}, Copernicus \cite{esa_2021} and NASA \cite{crippen.etal_2016} DEMs, to provide uncertainty in the elevation maps. Furthermore, we update meteorology to a newer version of E-OBS \cite{cornes.etal_2018}, now containing the additional meteorological drivers wind speed, relative Humidity and shortwave downwelling radiation alongside the previously existing rainfall, sea-level pressure and temperature (daily mean, min \& max). In contrast to EarthNet2021, we only provide one vector instead of a 3D tensor of meteorology, dropping the meso-scale surrounding of each minicube. This reduced the memory footprint of each minicube by $>5x$ and makes the task easier. Finally, we provide proper georeferencing, which was missing in EarthNet2021.



\subsection{Evaluation}
We resort to traditional metrics in environmental modeling:
\begin{itemize}[noitemsep,topsep=0pt]
    \item $R^2$ squared pearson correlation coefficient
    \item $\text{RMSE}$ root mean squared error
    \item $\text{NSE} = 1 - \dfrac{MSE(V, \hat{V})}{Var[V]}$, the nash-sutcliffe efficiency \cite{nash.sutcliffe_1970}, a measure of relative variability
    \item $|\text{bias}| = | \overline{V} - \overline{\hat{V}} |$, the absolute bias
\end{itemize}
In addition, we propose to measure if a model is better than the NDVI climatology, by computing the Outperformance score: The percentage of minicubes, for which the model is better in at least 3 out of the 4 metrics. Here, better means their score difference (ordering s.t. higher=better) exceeds $0.01$ for RMSE and $|\text{bias}|$ and $0.05$ for NSE and $R^2$. We also report the RMSE over only the first 25 days (5 time steps) of the target period. 

We compute all metrics per pixel over clear-sky timesteps. We then consider only pixels with vegetated landcover (cropland, grassland, forest, shrubland), no seasonal flooding (minimum NDVI $>0$), enough observations ($\geq10$ during target period, $\geq3$ during context period) and considerable variation (NDVI std. dev $>0.1$). All these pixelwise scores are grouped by minicube and landcover, and then aggregated to account for class imbalance. Finally, the macro-average of the scores per landcover class is computed. In this way, the scores represent a conservative estimate of the expected performance of dynamic vegetation modeling during a new year or at a new location. 


\section{Experiments}


\begin{figure*}[t]
    \centering
    \includegraphics[width=\textwidth]{figures/vitus_figure.pdf}
    \caption{Qualitative Results. We plot results of PredRNN-meteo for one OOD-t minicube located near Oradea, Romania. The top-left shows timeseries for all pixels (mean and std. dev.) and for a single pixel (green square on top right). The right side shows image timeseries of cloud-masked target and predicted NDVI alongside their difference.}
    \label{fig:qualitative}
\end{figure*}

\subsection{Baseline comparison}
This work is the first to systematically evaluate vegetation prediction models at $20m$ resolution in Europe. However, previous work on satellite imagery forecasting is applicable, since the NDVI, our vegetation proxy, can be derived from the red and near-infrared channels. Hence, we evaluate the Top-3 models from the EarthNet2021 challenge leaderboard\footnote{\url{https://web.archive.org/web/20230228215255/https://www.earthnet.tech/en21/ch-leaderboard/}} using their trained weights: a regular ConvLSTM \cite{diaconu.etal_2022}, an encode-process-decode ConvLSTM called SGED-ConvLSTM \cite{kladny.etal_2022} and the Earthformer \cite{gao.etal_2022}.

We compare these against three Non-ML baselines: persistence, previous year and climatology. Note, the climatology uses a lot more information than our models (6 years vs. 50 days). Additionally, we compare with Kalman filter, LightGBM \cite{ke.etal_2017} and Prophet \cite{taylor.letham_2018}, local timeseries forecasting models, which also work with the full timeseries instead of just 50 context days.

We introduce four new model variants: ConvLSTM-meteo, PredRNN-meteo, SimVP-meteo and Earthformer-meteo (see sec.~\ref{sec:models}). These are weather-guided extensions of four state-of-the-art approaches to video prediction, each belonging to a different model class. For ConvLSTM-meteo, PredRNN-meteo and SimVP-meteo, we report the mean ($\pm$std. dev.) from three different random seeds. Earthformer-meteo has an order of magnitude more parameters, making training more expensive, which is why we only report one random seed.

The quantitative results are shown in table~\ref{tab:quantitative}. Both the climatology and Prophet are strong baselines, which outperform all of the top-3 models from the EarthNet2021 challenge. SimVP, PredRNN and ConvLSTM outperform all baselines on all metrics except for the 25-day RMSE, where a persistence baseline is slightly stronger. For all three models and metrics, differences to the climatology are highly significant when tested for all pixels (with Wilcoxon signed-rank test, $\alpha = 0.001$), but also for each land cover or for smaller subsets of $100$ minicubes. Earthformer-meteo, has overall lower skill. It mostly excels at RMSE and $|\text{bias}|$, where it can perform similar to other methods, yet has way lower performance for NSE and $R^2$. Here, NSE may be weak and RMSE good because we aggregate over the full dataset, hence indicating spatial patterns of model skill. %When not using the weather, SimVP and PredRNN exhibit large drops in performance, especially for $R^2$ and $NSE$.

Qualitative results of the PredRNN model for one of the minicubes from the OOD-t test set with the highest scores are reported in fig.~\ref{fig:qualitative}. The model clearly learns the complex dynamics of vegetation, with a strong seasonal evolution of the crop fields. It interpolates faithfully those pixels, which are masked in the target, and contains a strong temporal consistency. However, as the prediction horizon increases, predictions become more blurry, even obscuring field boundaries, which should stay consistent over time.

\subsection{Weather guidance}
Our meteo-guided models benefit from the weather conditioning. Fig.~\ref{fig:horizon} compares each one of the four models (blue) against a variant without weather conditioning (orange). For all metrics (except Earthformer-meteo $R^2$), using weather outperforms not using it. The SimVP has the largest performance gain due to meteo-guidance. This could possibly be since it does not explicitly model memory effects, but rather learns to disentangle the temporal evolution in one piece. The ConvLSTM without weather has only slightly lower skill than the SimVP-meteo.  

For PredRNN and SimVP, we perform an extended ablation study regarding weather guidance, which we present in the supplementary material. The different weather conditioning approaches concatenation, feature-wise linear modulation (FiLM, \cite{perez.etal_2018}) and cross-attention \cite{rombach.etal_2022} have only a small influence on performance scores, if applied at the right location: cross-attention favors latent fusion, FiLM generally outperforms concatenation and is suitable for early fusion.

\begin{figure}
    \centering
    \includegraphics[width=\columnwidth]{figures/fig4}
    \caption{Model performance comparing meteo-guided models (blue) with the same those not using weather (black bar is std. dev. from three random seeds).}
    \label{fig:horizon}
\end{figure}




\begin{table}
\centering
\begin{tabularx}{\columnwidth}{Xcccc}
\toprule
Model & $R^2$ $\uparrow$ & Diff $\uparrow$ & RMSE $\downarrow$ & Diff $\downarrow$ \\
\midrule
Climatology & 0.58 & & 0.18 & \\
\arrayrulecolor{black!30}\midrule
1x1 LSTM & 0.57 & & 0.17 &\\
\quad spatial shuffle & 0.57 & 0.00 & 0.17 & 0.00 \\
Next-frame UNet & 0.51 & & 0.19 &  \\
\quad spatial shuffle & 0.48 & -0.03 & 0.21 & +0.02\\
Next-cuboid UNet & 0.56 & & 0.16 & \\
\quad spatial shuffle & 0.43 & -0.13 & 0.21 & +0.05\\
\arrayrulecolor{black!30}\midrule
ConvLSTM & 0.62 & & 0.14 &\\
\quad spatial shuffle & 0.60 & -0.02 & 0.16 & +0.02\\
PredRNN & 0.62 & & 0.15 & \\
\quad spatial shuffle & 0.45 & -0.17 & 0.22 & +0.07 \\
SimVP & 0.60 & & 0.15 & \\
\quad spatial shuffle & 0.49 & -0.11 & 0.22 & +0.07 \\
\bottomrule
\end{tabularx}
\caption{Model skill when spatial interactions are broken through shuffling.}
\label{tab:spatiotemp}
\end{table}



\subsection{The role of spatial interactions}
In contrast to video prediction, there is relatively little spatial movement across frames of satellite images. Field boundaries are mostly fixed in space as are forest limits. The largest variations appear within these edges in the temporal domain. Hence, it is not clear a-priori, that video prediction models, which take into account spatio-temporal interactions, are a good choice for modeling vegetation dynamics. However, at $20m$ resolution, lateral processes might appear, which cannot be captured by predictor variables. For instance, a grassland might react differently to a meteorological drought if it is closer to a river or lays on a north-facing slope. Also, trees at the forest edge are differently affected by weather than those at the center of a forest plot.

We approach studying the role of spatial interactions by comparing model performance against models trained with spatially shuffled input, i.e. explicitly breaking spatial interactions \cite{requena-mesa.etal_2019}. We perform the shuffling across spatial dimensions and across the batch, to also destroy image statistics, which may already give information on the local neighborhood of a pixel. We evaluate three of our four models: ConvLSTM, PredRNN and SimVP. We skip the Earthformer for this experiment since it is very expensive to train. In addition we also study three baselines: a pixelwise (1x1) LSTM, the next-frame UNet and the next-cuboid UNet (see sec.~\ref{sec:baselines}). The pixelwise LSTM is a global timeseries model unable to capture spatial interactions. The next-frame UNet models spatial interactions, but does not consider temporal memory. All other models can leverage spatio-temporal dependencies, though the ConvLSTM only has a small local receptive field ($\sim100$m around each pixel).

The results are reported in tab.~\ref{tab:spatiotemp}. As can be expected, the pixelwise LSTM can be trained with spatial shuffled pixels without performance loss. All other models, though, exhibit a drop in performance under pixel shuffling. For PredRNN, SimVP and Next-cuboid UNet it can be very large, as they have large receptive fields. For the next-frame UNet it is smaller, as it itself is not a very skillful model. The Conv\-LSTM also exhibits only a small performance drop, which may be due to its local receptive field. In turn, this may indicate that spatial interactions relevant for vegetation response to weather are of rather local nature and long-range interactions seldom important.




\begin{figure}
    \centering
    \includegraphics[width=\columnwidth]{figures/R2_over_predtime.pdf}
    \caption{Model skill over different seasons for the ConvLSTM-meteo on the OOD-t test set.}
    \label{fig:seasons}
\end{figure}



\begin{table}
\centering
\begin{tabularx}{\columnwidth}{Xcccc}
\toprule
& \multicolumn{2}{c}{OOD-s} & \multicolumn{2}{c}{OOD-st} \\
Model & $R^2$ $\uparrow$ & RMSE $\downarrow$ & $R^2$ $\uparrow$ & RMSE $\downarrow$ \\
\midrule
Climatology & 0.50 & 0.15 & 0.56 & 0.19 \\
ConvLSTM & \textbf{0.55} & \textbf{0.14} & \textbf{0.58} & \textbf{0.15} \\
PredRNN & 0.54 & 0.15 & \textbf{0.58} & \textbf{0.15} \\
SimVP & 0.50 & 0.15 & 0.54 & \textbf{0.15} \\
Earthformer & 0.47 &      0.15 &      0.47 &      0.16 \\
\bottomrule
\end{tabularx}
\caption{Model skill at spatial (OOD-s) and spatio-temporal (OOD-st) extrapolation.}
\label{tab:extrapol}
\end{table}

\subsection{Strengths and Limitations of SOTA model}

The OOD-t test set contains minicubes from four different 3-month periods each over two years. Fig.~\ref{fig:seasons} dissects the model skill of our best model: ConvLSTM-meteo (one run). There are large differences between the years. Until September, the growing season was better predicted in 2022. Afterwards it flips, and 2021 takes the lead. The first half of the growing season is usually better predicted than the second half. This is probably due to anthropogenic influences, especially harvest, mowing, cutting and forest fires, being more prevalent in the second half. Such events are particularly hard to predict from the given weather covariates, and may be interpreted as random noise.

We assess the performance at spatio-(temporal) extrapolation of all four meteo-guided models on the OOD-s and OOD-st test sets and report in tab.~\ref{tab:extrapol}. The SimVP and the PredRNN can extrapolate in space and time. However, the margin to the climatology does shrink. Here, more training data might help: spatial extrapolation is theoretically not necessary for modeling vegetation dynamics (only temporal extrapolation is). Practically speaking, however, it does help to increase inference speed and enable potential applicability over large areas.

Reassured by spatial extrapolation capabilities, we present a map of $R^2$ for the ConvLSTM-meteo in fig.~\ref{fig:mapR2}a. Cropland regions on the Iberian peninsula and in northern France, as well as forests in the Balkans are regions with great applicability of the model. For the former two, this may be explained by many training samples in those regions, for the last, it cannot. Grasslands and forests in Poland and highly heterogenous regions (mountains, near cities, near coasts) are more challenging for the model.

Geomorphons capture local terrain features, derived from first and second spatial derivatives of elevation. Fig.~\ref{fig:mapR2}b shows densities of RMSE of the ConvLSTM-meteo for different geomorphons from  the Geomorpho90m map \cite{amatulli.etal_2020}. Generally, the model performs well across all classes. Summits and Depressions, two rather extreme types, seem to be slightly easier to predict. Homogeneous terrain (red: flat, shoulder, footslope) has a larger tail towards high error. This may be as those regions are typically where there is a lot of anthropogenic activity, possibly leading to dynamics less covered by the predictors (harvest, clear-cut, etc.). 

\begin{figure}
    \centering
    \begin{subfigure}[c]{0.49\columnwidth}
    \includegraphics[width=\textwidth]{figures/fig6a.pdf}
    \end{subfigure}
    \begin{subfigure}[c]{0.49\columnwidth}
    \includegraphics[width=\textwidth]{figures/fig6b.pdf}
    \end{subfigure}
    
    \caption{Panel a) shows a map of $R^2$ on OOD-t and OOD-st test sets and panel b) shows probability densities of RMSE per geomorphon. Both use ConvLSTM-meteo.}
    \label{fig:mapR2}
\end{figure}


\subsection{Downstream task: carbon monitoring}

Carbon monitoring is of great importance for climate change mitigation, especially in relation to nature-based solutions. The gross primary productivity (GPP) represents the amount of carbon that is taken up by plants through photosynthesis and subsequently stored. It is not directly observable. At a few hundred research stations around the world with eddy covariance measurement technology, it can be indirectly measured. For carbon monitoring, it would be beneficial to measure this quantity everywhere on the globe. It has been shown \cite{pabon-moreno.etal_2022} that Sentinel 2 NDVI is correlated to GPP measured with eddy covariance. We build on this correlation to show how our models could potentially be leveraged to give near real-time estimates of GPP and to study weather scenarios.

Fig.~\ref{fig:gpp} compares modeled with observed GPP at the Fluxnet site Grillenburg (identifier DE-Gri) in eastern Germany distributed by ICOS \cite{degri_icos}. First, we fit a linear model between observed NDVI and GPP for the years 2017-2019. Here, interpolated grassland NDVI pixels (fig.~\ref{fig:gpp}b, inside red boundaries) are used. Next, we perform an out-of-sample analysis and find an $R^2 = 0.53$ for 2020-01 to 2021-04 (fig.~\ref{fig:gpp}a, blue line). Finally, we forecast GPP with our PredRNN-meteo model from May to July 2021(fig.~\ref{fig:gpp}a, orange line). The resulting forecast has decent quality at short prediction horizons, but low skill after 75 days (fig.~\ref{fig:gpp}c). These results show a way to leverage models from this paper for near real-time carbon monitoring. However, for application at scale, it is likely beneficial to use a more powerful GPP model (e.g. random forest \cite{pabon-moreno.etal_2022} or light-use efficiency \cite{bao.etal_2022}), fitted across many Fluxnet sites.

\begin{figure}
    \centering
    \includegraphics[width=0.95\columnwidth]{figures/figgpp}
    \caption{Panel a) shows timeseries of observed (green) and modeled GPP (blue from NDVI observations, orange from NDVI prediction). Panel b) shows a satellite image of the Grillenburg Fluxnet site with grassland boundaries in red. Panel c) shows the RMSE over prediction horizons.}
    \label{fig:gpp}
\end{figure}



\section{Conclusion}
We proposed a novel approach for modeling vegetation response to weather in Europe. In particular, we presented four meteo-guided video prediction methods, taking past satellite imagery and future weather as input to produce future vegetation dynamics at $20$m resolution. Our experiments demonstrate that our models outperform existing state-of-the-art satellite imagery forecasting methods and a wide variety of strong baselines. To the best of our knowledge, we present the first study considering a climatology baseline and outperforming it with models, which, given the strong seasonality of vegetation dynamics, indicates real-world usefulness of our models in impactful usecases such as humanitarian anticipatory action or carbon monitoring. 

{%\small
\paragraph{Author contributions.} VB experiments, figures, writing. CRM supervision, figures, writing. CR, ZG experiments. LA figures. JC, NL, MW writing. MR funding, supervision, writing. All authors contributed to discussing the results. \\
\textbf{Acknowledgments.} We are thankful for invaluable help, comments and discussions to Nuno Carvalhais, Reda ElGhawi, Christian Reimers, Annu Panwar and Xingjian Shi. MW thanks the European Space Agency for funding the project DeepExtremes (AI4Science ITT). CRM and LA are thankful to the European Union’s DeepCube Horizon 2020 (research and innovation programme grant agreement No 101004188). NL and JC acknowledge funding from the European Union’s Horizon 2020 research and innovation programme under grant agreement No 101003469. 
}

%\clearpage
\iffalse
{\small
\bibliographystyle{ieee_fullname}
\bibliography{earthnet}
}

\newpage
\fi
\appendix
\section{Model details}
\subsection{Cloud masking}
\paragraph{Baselines (Table 1)}
The baselines reported in table 1 are taken from CloudSEN12 \cite{aybar.etal_2022}. Sen2Cor \cite{louis.etal_2016} is the processing software from ESA used to produce the Scene Classification Layer (SCL) mask, which was also introduced in EarthNet2021 \cite{requena-mesa.etal_2021}. FMask \cite{qiu.etal_2019} is a processing software originally designed for NASA Landsat imagery, but now repurposed to also work with Sentinel 2 imagery. It requires L1C top-of-atmosphere reflectance from all bands to be produced (EarthNet2021 only containes L2A bottom-of-atmosphere reflectance from four bands). KappaMask \cite{domnich.etal_2021} is a cloud mask based on deep learning, in table 1 we reported scores from the L2A version, which uses all 13 L2A bands as input.

\paragraph{UNet Mobilenetv2 (Table 1)}
Our UNet with Mobilenetv2 encoder \cite{sandler.etal_2018} was trained in two variants, one with RGB and near-infrared bands of L2A imagery (i.e. works with EarthNet2021) and one with all 13 bands of L2A imagery. We adopted the exact same implementation that was benchmarked in the CloudSEN12 paper \cite{aybar.etal_2022}, with the only difference being that in the paper, L1C imagery was used (which is often not useful in practical use-cases). In detail, this means we trained the UNet with Mobilenetv2 encoder using the Segmentation Models PyTorch Python library\footnote{\url{https://segmentation-models-pytorch.readthedocs.io/en/latest/}}. We used a batch size of 32, random horizontal and vertical flipping, random 90 degree rotations, random mirroring, unweighted cross entropy loss, early stopping with a patience of 10 epochs, AdamW optimizer, learning rate of $1e^{-3}$, and a learning rate schedule reducing the learning rate by a factor of 10 if validation loss did not decrease for 4 epochs.


\subsection{Vegetation modeling}
\paragraph{Local timeseries models (Table 2)}
We train the local timeseries models (table 2) at each pixel. For a given pixel we extract the full timeseries of NDVI and weather variables at 5-daily resolution. All variables are linearly gapfilled and weather is aggregated with min, mean, max, and std to 5-daily. The whole timeseries before each target period is used to train a timeseries model, for the target period the model only receives weather. The Kalman Filter runs with default parameters from darts \cite{herzen.etal_2022}. The LightGBM model gets lagged variables from the last 10 time steps and predicts a full 20 time step chunk at once. For Prophet we again use default parameters.

\paragraph{EarthNet models (Table 2)}
For running the leading models from EarthNet2021 we utilize the code from the respective github repositories: ConvLSTM \cite{diaconu.etal_2022}\footnote{\url{https://github.com/dcodrut/weather2land}}, SGED-ConvLSTM \cite{kladny.etal_2022}\footnote{\url{https://github.com/rudolfwilliam/satellite_image_forecasting}} and Earthformer \cite{gao.etal_2022} \footnote{\url{https://github.com/amazon-science/earth-forecasting-transformer/tree/main/scripts/cuboid_transformer/earthnet_w_meso}}. We derive the NDVI from the predicted satellite bands red and near-infrared:
\begin{align}
    NDVI = \frac{NIR - Red}{NIR + Red + 1e^{-8}}
\end{align}

\paragraph{ConvLSTM-meteo (Table 2,3,4, Figure 4,5,6)}
Our ConvLSTM-meteo contains four ConvLSTM-cells \cite{shi.etal_2015} in total, two for processing context frames and two for processing target frames. Each has convolution kernels with bias, hidden dimension of 64 and kernel size of 3. We train for 100 epochs with a batch size of 32, a learning rate of $4e^{-5}$ and with AdamW optimizer. We train three models from the random seeds 42, 97 and 27.

\paragraph{PredRNN-meteo (Table 2,3,4, Figure 3,4,7)}
Our PredRNN-meteo contains two ST-ConvLSTM-cells \cite{wang.etal_2017} Each has convolution kernels with bias, hidden dimension of 64 and kernel size of 3 and residual connections. We use a PatchMerge encoder decoder with GroupNorm (16 groups), convolutions with kernel size of 3 and hidden dimension of 64, LeakyReLU activation and downsampling rate of 4x. We train for 100 epochs with a batch size of 32, a learning rate of $3e^{-4}$ and with AdamW optimizer. We use a spatio-temporal memory decoupling loss term with weight 0.1 and reverse exponential scheduling of true vs. predicted images (as in the PredRNN journal version \cite{wang.etal_2023}). We train three models from the random seeds 42, 97 and 27.

\paragraph{SimVP-meteo (Table 2,3,4, Figure 4)}
Our SimVP-meteo has a PatchMerge encoder decoder with GroupNorm (16 groups), convolutions with kernel size of 3 and hidden dimension of 64, LeakyReLU activation and downsampling rate of 4x. The encoder processes all 10 context time steps at once (stacked along the channel dimension). The decoder processes 1 target time step at a time. The gated spatio-temporal attention processor \cite{tan.etal_2023} translates between both in the latent space, we use two layers and 64 hidden channels. We train for 100 epochs with a batch size of 64, a learning rate of $6e^{-4}$ and with AdamW optimizer. We train three models from the random seeds 42, 97 and 27.

\paragraph{Earthformer-meteo (Table 2,4, Figure 4)}
Our Earthformer-meteo is a transformer combined with an initial PatchMerge encoder (and a final decoder) to reduce the dimensionality. The encoder and decoder use LeakyReLU activation, hidden size of 64 and 256 and downsample 2x. In between, the transformer processor has a UNet-type architecture, with cross-attention to merge context frame information with target frame embeddings. GeLU activation and LayerNorm, axial self-attention, 0.1 dropout and 4 attention heads are used. Weather information is regridded to match the spatial resolution of satellite imagery and used as input during context and target period. We train for 100 epochs with a batch size of 32, a maximum learning rate of $1e^{-4}$, linear learning rate warm up, cosine learning rate shedule and with AdamW optimizer.

\paragraph{1x1 LSTM (Table 4)}
Our 1x1 LSTM is implemented as a ConvLSTM-meteo with kernel size of 1. We train for 100 epochs with a batch size of 32, a learning rate of $4e^{-5}$ and with AdamW optimizer.

\paragraph{Next-frame UNet (Table 4)}
Our next-frame UNet has a depth of 5, latent weather conditioning with FiLM, a hidden size 128, kernel size 3, LeakyReLU activation, GroupNorm (16 groups), PatchMerge downsampling and nearest upsampling. We train for 100 epochs with a batch size of 64, a learning rate of $6e^{-4}$ and with AdamW optimizer.

\paragraph{Next-cuboid UNet (Table 4)}
Our next-cuboid UNet has a depth of 5, latent weather conditioning with FiLM, a hidden size 256, kernel size 3, LeakyReLU activation, GroupNorm (16 groups), PatchMerge downsampling and nearest upsampling. We train for 100 epochs with a batch size of 64, a learning rate of $6e^{-4}$ and with AdamW optimizer.


\section{Weather ablations}
\subsection{Methods}
Most of our baseline approaches have been originally proposed to handle only past covariates. Here, we condition forecasts on future weather. A-priori it is not known how to best achieve this weather conditioning. For PredRNN-meteo and SimVP-meteo, we compare three approaches, each fused at three different locations. The approaches operate pixelwise, taking features $x_{in} \in \mathbb{R}^d$ and conditioning input $c_{i} \in \mathbb{R}^{n}$ for weather variable $i$. The conditioning layers $g(\cdot, \cdot; \phi)$ with parameters $\phi$ then operate as
\begin{align}
    x_{out} = g(x_{in}, c; \phi) \in \mathbb{R}^d
\end{align}
We parameterize $g$ with neural networks. %Fig.~\ref{fig:weathercond} provides an overview of the three approaches:

\paragraph{CAT} First concatenates $x_{in}$ and a flattened $c$ along the channel dimension, and then performs a linear projection to obtain $x_{out}$ of same dimensionality as $x_{in}$. In practice we implement this with a 1x1 Conv layer.

\paragraph{FiLM} Feature-wise linear modulation \cite{perez.etal_2018} generalizes the concatenation layer before. It produces $x_{out}$ with linear modulation:
\begin{align}
  x_{out} = x_{in} + \sigma(\gamma(c; \phi_{\gamma})\odot N(f(x_{in}; \phi_{f})) + \beta(c; \phi_{\beta}))
\end{align}
Here, $f$ is a linear layer, $\gamma$ and $\beta$ are MLPs, $N$ is a normalization layer and $\sigma$ is a pointwise non-linear activation function.

\paragraph{xAttn} Cross-attention is an operation commonly found in the Transformers architecture. In recent works on image generation with diffusion models it is used to condition the generative process on a text embedding \cite{rombach.etal_2022}. Inspired from this, we propose a pixelwise conditioning layer based on multi-head cross-attention. The input $x_{in}$ is treated as a single token query $Q$. Each weather variable $c_{i}$ is treated as individual tokens, from which we derive keys $K$ and values $V$. The result is then just regular multi-head attention $MHA$ in a residual block:
\begin{align}
    x_{out} &= x_{in} \\
    &+ f(N(MHA(Q(x_{in}; \phi_{Q}), K(c; \phi_{K}), V(c; \phi_{V}))); \phi_{f})
\end{align}
Here, $f$ is either a linear projection or a MLP and $N$ is a normalization layer.

Each of the three approaches we apply at three locations throughout the network:

\paragraph{Early fusion} Just fusing all data modalities before passing it to a model. This Early CAT has been previously used for weather conditioning in satellite imagery forecasting %\cite{diaconu.etal_2022, kladny.etal_2022}.

\paragraph{Latent fusion} In the encode-process-decode framework, encoders are meant to capture spatial, and not temporal, relationships. Hence, latent fusion conditions the encoded spatial inputs twice: right after leaving the encoder and before entering the decoder.

\paragraph{All (fusion everywhere)} In addition, we compare against conditioning at every stage of the encoders, processors and decoders. All CAT has been applied to condition stochastic video predictions on random latent codes \cite{lee.etal_2018}.

\subsection{Results}
Fig.~\ref{fig:horizon2} summarizes the findings by looking at the RMSE over the prediction horizon. For the first 50 days, most models are better than the climatology, afterwards, most are worse. If using early fusion, FiLM is the best conditioning method. For latent fusion and fusion everywhere (all), xAttn is a consistent choice, but FiLM may sometimes be better (and sometimes a lot worse). CAT in general should be avoided, which is consistent with the theoretical observation, that CAT is a special case of FiLM.

For SimVP, the best weather guiding method is latent fusion with FiLM. For PredRNN, the best method is early fusion with FiLM. This is likely due to the difference in treatment of the temporal axis. For SimVP, early fusion would merge all time steps, hence, latent fusion is a better choice. For PredRNN on the other hand, early fusion handles only a single timestep. 

\begin{figure}
    \centering
    \includegraphics[width=\columnwidth]{figures/predhorizon.pdf}
    \caption{Model performance (RMSE) when using different ways of weather conditioning over varying prediction horizons.}
    \label{fig:horizon2}
\end{figure}



\section{Performance per landcover type}
Fig.~\ref{fig:map2} shows the model performance per landcover type.

\begin{figure}
    \centering
    \includegraphics[width=\columnwidth]{figures/map_R2_predrnn.pdf}
    \caption{Model performance per landcover. Maps represent $R^2$ on OOD-t and OOD-st test sets of PredRNN-meteo.}
    \label{fig:map2}
\end{figure}


{\small
\bibliographystyle{ieee_fullname}
\bibliography{earthnet}
}


\end{document}
\iffalse

%%%%%%%%% BODY TEXT
\section{Introduction}

Please follow the steps outlined below when submitting your manuscript to
the IEEE Computer Society Press.  This style guide now has several
important modifications (for example, you are no longer warned against the
use of sticky tape to attach your artwork to the paper), so all authors
should read this new version.

%-------------------------------------------------------------------------
\subsection{Language}

All manuscripts must be in English.

\subsection{Dual submission}

Please refer to the author guidelines on the ICCV 2023 web page for a
discussion of the policy on dual submissions.

\subsection{Paper length}
Papers, excluding the references section,
must be no longer than eight pages in length. The references section
will not be included in the page count, and there is no limit on the
length of the references section. For example, a paper of eight pages
with two pages of references would have a total length of 10 pages.
{\bf There will be no extra page charges for ICCV 2023.}

Overlength papers will simply not be reviewed.  This includes papers
where the margins and formatting are deemed to have been significantly
altered from those laid down by this style guide.  Note that this
\LaTeX\ guide already sets figure captions and references in a smaller font.
The reason such papers will not be reviewed is that there is no provision for
supervised revisions of manuscripts.  The reviewing process cannot determine
the suitability of the paper for presentation in eight pages if it is
reviewed in eleven.  

%-------------------------------------------------------------------------
\subsection{The ruler}
The \LaTeX\ style defines a printed ruler which should be present in the
version submitted for review.  The ruler is provided in order that
reviewers may comment on particular lines in the paper without
circumlocution.  If you are preparing a document using a non-\LaTeX\
document preparation system, please arrange for an equivalent ruler to
appear on the final output pages.  The presence or absence of the ruler
should not change the appearance of any other content on the page.  The
camera-ready copy should not contain a ruler. (\LaTeX\ users may uncomment
the \verb'\iccvfinalcopy' command in the document preamble.)  Reviewers:
note that the ruler measurements do not align well with the lines in the paper
--- this turns out to be very difficult to do well when the paper contains
many figures and equations, and, when done, looks ugly.  Just use fractional
references (e.g.\ this line is $095.5$), although in most cases one would
expect that the approximate location will be adequate.

\subsection{Mathematics}

Please number all of your sections and displayed equations.  It is
important for readers to be able to refer to any particular equation.  Just
because you didn't refer to it in the text doesn't mean some future readers
might not need to refer to it.  It is cumbersome to have to use
circumlocutions like ``the equation second from the top of page 3 column
1''.  (Note that the ruler will not be present in the final copy, so is not
an alternative to equation numbers).  All authors will benefit from reading
Mermin's description of how to write mathematics:
\url{http://www.pamitc.org/documents/mermin.pdf}.

\subsection{Blind review}

Many authors misunderstand the concept of anonymizing for blind
review.  Blind review does not mean that one must remove
citations to one's own work --- in fact, it is often impossible to
review a paper unless the previous citations are known and
available.

Blind review means that you do not use the words ``my'' or ``our''
when citing previous work.  That is all.  (But see below for
tech reports.)

Saying ``this builds on the work of Lucy Smith [1]'' does not say
that you are Lucy Smith; it says that you are building on her
work.  If you are Smith and Jones, do not say ``as we show in
[7]'', say ``as Smith and Jones show in [7]'' and at the end of the
paper, include reference 7 as you would any other cited work.

An example of a bad paper just asking to be rejected:
\begin{quote}
\begin{center}
    An analysis of the frobnicatable foo filter.
\end{center}

   In this paper, we present a performance analysis of our
   previous paper [1] and show it to be inferior to all
   previously known methods.  Why the previous paper was
   accepted without this analysis is beyond me.

   [1] Removed for blind review
\end{quote}

An example of an acceptable paper:

\begin{quote}
\begin{center}
     An analysis of the frobnicatable foo filter.
\end{center}

   In this paper, we present a performance analysis of the
   paper of Smith \etal [1] and show it to be inferior to
   all previously known methods.  Why the previous paper
   was accepted without this analysis is beyond me.

   [1] Smith, L and Jones, C. ``The frobnicatable foo
   filter, a fundamental contribution to human knowledge''.
   Nature 381(12), 1-213.
\end{quote}

If you are making a submission to another conference at the same time,
which covers similar or overlapping material, you may need to refer to that
submission in order to explain the differences, just as you would if you
had previously published related work.  In such cases, include the
anonymized parallel submission~\cite{Authors14} as additional material and
cite it as
\begin{quote}
[1] Authors. ``The frobnicatable foo filter'', F\&G 2014 Submission ID 324,
Supplied as additional material {\tt fg324.pdf}.
\end{quote}

Finally, you may feel you need to tell the reader that more details can be
found elsewhere, and refer them to a technical report.  For conference
submissions, the paper must stand on its own, and not {\em require} the
reviewer to go to a tech report for further details.  Thus, you may say in
the body of the paper ``further details may be found
in~\cite{Authors14b}''.  Then submit the tech report as additional material.
Again, you may not assume the reviewers will read this material.

Sometimes your paper is about a problem that you tested using a tool that
is widely known to be restricted to a single institution.  For example,
let's say it's 1969, you have solved a key problem on the Apollo lander,
and you believe that the ICCV70 audience would like to hear about your
solution.  The work is a development of your celebrated 1968 paper entitled
``Zero-g frobnication: How being the only people in the world with access to
the Apollo lander source code makes us a wow at parties'', by Zeus \etal.

You can handle this paper like any other.  Don't write ``We show how to
improve our previous work [Anonymous, 1968].  This time we tested the
algorithm on a lunar lander [name of lander removed for blind review]''.
That would be silly, and would immediately identify the authors. Instead,
write the following:
\begin{quotation}
\noindent
   We describe a system for zero-g frobnication.  This
   system is new because it handles the following cases:
   A, B.  Previous systems [Zeus et al. 1968] didn't
   handle case B properly.  Ours handles it by including
   a foo term in the bar integral.

   ...

   The proposed system was integrated with the Apollo
   lunar lander, and went all the way to the moon, don't
   you know.  It displayed the following behaviors
   which shows how well we solved cases A and B: ...
\end{quotation}
As you can see, the above text follows standard scientific conventions,
reads better than the first version and does not explicitly name you as
the authors.  A reviewer might think it likely that the new paper was
written by Zeus \etal, but cannot make any decision based on that guess.
He or she would have to be sure that no other authors could have been
contracted to solve problem B.
\medskip

\noindent
FAQ\medskip\\
{\bf Q:} Are acknowledgements OK?\\
{\bf A:} No.  Leave them for the final copy.\medskip\\
{\bf Q:} How do I cite my results reported in open challenges?
{\bf A:} To conform with the double-blind review policy, you can report the results of other challenge participants together with your results in your paper. For your results, however, you should not identify yourself and should not mention your participation in the challenge. Instead, present your results referring to the method proposed in your paper and draw conclusions based on the experimental comparison to other results.\medskip\\

\begin{figure}[t]
\begin{center}
\fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
   %\includegraphics[width=0.8\linewidth]{egfigure.eps}
\end{center}
   \caption{Example of a caption.  It is set in Roman so mathematics
   (always set in Roman: $B \sin A = A \sin B$) may be included without an
   ugly clash.}
\label{fig:long}
\label{fig:onecol}
\end{figure}

\subsection{Miscellaneous}

\noindent
Compare the following:\\
\begin{tabular}{ll}
 \verb'$conf_a$' &  $conf_a$ \\
 \verb'$\mathit{conf}_a$' & $\mathit{conf}_a$
\end{tabular}\\
See The \TeX book, p165.

The space after \eg, meaning ``for example'', should not be a
sentence-ending space. So \eg is correct, {\em e.g.} is not.  The provided
\verb'\eg' macro takes care of this.

When citing a multi-author paper, you may save space by using ``et alia'',
shortened to ``\etal'' (not ``{\em et.\ al.}'' as ``{\em et}'' is a complete word.)
However, use it only when there are three or more authors.  Thus, the
following is correct: ``
   Frobnication has been trendy lately.
   It was introduced by Alpher~\cite{Alpher02}, and subsequently developed by
   Alpher and Fotheringham-Smythe~\cite{Alpher03}, and Alpher \etal~\cite{Alpher04}.''

This is incorrect: ``... subsequently developed by Alpher \etal~\cite{Alpher03} ...''
because reference~\cite{Alpher03} has just two authors.  If you use the
\verb'\etal' macro provided, then you need not worry about double periods
when used at the end of a sentence as in Alpher \etal.

For this citation style, keep multiple citations in numerical (not
chronological) order, so prefer \cite{Alpher03,Alpher02,Authors14} to
\cite{Alpher02,Alpher03,Authors14}.

\begin{figure*}
\begin{center}
\fbox{\rule{0pt}{2in} \rule{.9\linewidth}{0pt}}
\end{center}
   \caption{Example of a short caption, which should be centered.}
\label{fig:short}
\end{figure*}

%------------------------------------------------------------------------
\section{Formatting your paper}

All text must be in a two-column format. The total allowable width of the
text area is $6\frac78$ inches (17.5 cm) wide by $8\frac78$ inches (22.54
cm) high. Columns are to be $3\frac14$ inches (8.25 cm) wide, with a
$\frac{5}{16}$ inch (0.8 cm) space between them. The main title (on the
first page) should begin 1.0 inch (2.54 cm) from the top edge of the
page. The second and following pages should begin 1.0 inch (2.54 cm) from
the top edge. On all pages, the bottom margin should be 1-1/8 inches (2.86
cm) from the bottom edge of the page for $8.5 \times 11$-inch paper; for A4
paper, approximately 1-5/8 inches (4.13 cm) from the bottom edge of the
page.

%-------------------------------------------------------------------------
\subsection{Margins and page numbering}

All printed material, including text, illustrations, and charts, must be kept
within a print area 6-7/8 inches (17.5 cm) wide by 8-7/8 inches (22.54 cm)
high.

Page numbers should be included for review submissions but not for the 
final paper. Review submissions papers should have page numbers in the 
footer with numbers centered and .75 inches (1.905 cm) from the bottom 
of the page and start on the first page with the number 1.

Page numbers will be added by the publisher to all camera-ready papers 
prior to including them in the proceedings and before submitting the 
papers to IEEE Xplore. As such, your camera-ready submission should 
not include any page numbers. Page numbers should automatically be 
removed by uncommenting (if it's not already) the line
\begin{verbatim}
% \iccvfinalcopy
\end{verbatim}
near the beginning of the .tex file.

%-------------------------------------------------------------------------
\subsection{Type-style and fonts}

Wherever Times is specified, Times Roman may also be used. If neither is
available on your word processor, please use the font closest in
appearance to Times to which you have access.

MAIN TITLE. Center the title 1-3/8 inches (3.49 cm) from the top edge of
the first page. The title should be in Times 14-point, boldface type.
Capitalize the first letter of nouns, pronouns, verbs, adjectives, and
adverbs; do not capitalize articles, coordinate conjunctions, or
prepositions (unless the title begins with such a word). Leave two blank
lines after the title.

AUTHOR NAME(s) and AFFILIATION(s) are to be centered beneath the title
and printed in Times 12-point, non-boldface type. This information is to
be followed by two blank lines.

The ABSTRACT and MAIN TEXT are to be in a two-column format.

MAIN TEXT. Type main text in 10-point Times, single-spaced. Do NOT use
double-spacing. All paragraphs should be indented 1 pica (approx. 1/6
inch or 0.422 cm). Make sure your text is fully justified---that is,
flush left and flush right. Please do not place any additional blank
lines between paragraphs.

Figure and table captions should be 9-point Roman type as in
Figures~\ref{fig:onecol} and~\ref{fig:short}.  Short captions should be centered.

\noindent Callouts should be 9-point Helvetica, non-boldface type.
Initially capitalize only the first word of section titles and first-,
second-, and third-order headings.

FIRST-ORDER HEADINGS. (For example, {\large \bf 1. Introduction})
should be Times 12-point boldface, initially capitalized, flush left,
with one blank line before, and one blank line after.

SECOND-ORDER HEADINGS. (For example, { \bf 1.1. Database elements})
should be Times 11-point boldface, initially capitalized, flush left,
with one blank line before, and one after. If you require a third-order
heading (we discourage it), use 10-point Times, boldface, initially
capitalized, flush left, preceded by one blank line, followed by a period
and your text on the same line.

%-------------------------------------------------------------------------
\subsection{Footnotes}

Please use footnotes\footnote {This is what a footnote looks like.  It
often distracts the reader from the main flow of the argument.} sparingly.
Indeed, try to avoid footnotes altogether and include necessary peripheral
observations in
the text (within parentheses, if you prefer, as in this sentence).  If you
wish to use a footnote, place it at the bottom of the column on the page on
which it is referenced. Use Times 8-point type, single-spaced.

%-------------------------------------------------------------------------
\subsection{References}

List and number all bibliographical references in 9-point Times,
single-spaced, at the end of your paper. When referenced in the text,
enclose the citation number in square brackets, for
example~\cite{Authors14}.  Where appropriate, include the name(s) of
editors of referenced books.

\begin{table}
\begin{center}
\begin{tabular}{|l|c|}
\hline
Method & Frobnability \\
\hline\hline
Theirs & Frumpy \\
Yours & Frobbly \\
Ours & Makes one's heart Frob\\
\hline
\end{tabular}
\end{center}
\caption{Results.   Ours is better.}
\end{table}

%-------------------------------------------------------------------------
\subsection{Illustrations, graphs, and photographs}

All graphics should be centered.  Please ensure that any point you wish to
make is resolvable in a printed copy of the paper.  Resize fonts in figures
to match the font in the body text, and choose line widths that render
effectively in print.  Many readers (and reviewers), even of an electronic
copy, will choose to print your paper in order to read it.  You cannot
insist that they do otherwise, and therefore must not assume that they can
zoom in to see tiny details on a graphic.

When placing figures in \LaTeX, it's almost always best to use
\verb+\includegraphics+, and to specify the  figure width as a multiple of
the line width as in the example below
{\small\begin{verbatim}
   \usepackage[dvips]{graphicx} ...
   \includegraphics[width=0.8\linewidth]
                   {myfile.eps}
\end{verbatim}
}

%-------------------------------------------------------------------------
\subsection{Color}

Please refer to the author guidelines on the ICCV 2023 web page for a discussion
of the use of color in your document.

%------------------------------------------------------------------------
\section{Final copy}

You must include your signed IEEE copyright release form when you submit
your finished paper. We MUST have this form before your paper can be
published in the proceedings.

\fi
