@article{entwistleElectronicExcitedStates2022,
  title = {Electronic Excited States in Deep Variational {{Monte Carlo}}},
  author = {Entwistle, Mike and Sch{\"a}tzle, Zeno and Erdman, Paolo A. and Hermann, Jan and No{\'e}, Frank},
  year = {2022},
  month = mar,
  doi = {10.48550/arXiv.2203.09472},
  abstract = {Obtaining accurate ground and low-lying excited states of electronic systems is crucial in a multitude of important applications. One ab initio method for solving the Schr\textbackslash "odinger equation that scales favorably for large systems is variational quantum Monte Carlo (QMC). The recently introduced deep QMC approach uses ansatzes represented by deep neural networks and generates nearly exact ground-state solutions for molecules containing up to a few dozen electrons, with the potential to scale to much larger systems where other highly accurate methods are not feasible. In this paper, we extend one such ansatz (PauliNet) to compute electronic excited states. We demonstrate our method on various small atoms and molecules and consistently achieve high accuracy for low-lying states. To highlight the method's potential, we compute the first excited state of the much larger benzene molecule, as well as the conical intersection of ethylene, with PauliNet matching results of more expensive high-level methods.},
  langid = {english}
}

@article{gaoAbInitioPotentialEnergy2021,
  title = {Ab-{{Initio Potential Energy Surfaces}} by {{Pairing GNNs}} with {{Neural Wave Functions}}},
  author = {Gao, Nicholas and G{\"u}nnemann, Stephan},
  year = {2021},
  month = nov,
  journal = {arXiv:2110.05064 [physics]},
  eprint = {2110.05064},
  eprinttype = {arxiv},
  primaryclass = {physics},
  abstract = {Solving the Schr\textbackslash "odinger equation is key to many quantum mechanical properties. However, an analytical solution is only tractable for single-electron systems. Recently, neural networks succeeded at modeling wave functions of many-electron systems. Together with the variational Monte-Carlo (VMC) framework, this led to solutions on par with the best known classical methods. Still, these neural methods require tremendous amounts of computational resources as one has to train a separate model for each molecular geometry. In this work, we combine a Graph Neural Network (GNN) with a neural wave function to simultaneously solve the Schr\textbackslash "odinger equation for multiple geometries via VMC. This enables us to model continuous subsets of the potential energy surface with a single training pass. Compared to existing state-of-the-art networks, our Potential Energy Surface Network PESNet speeds up training for multiple geometries by up to 40 times while matching or surpassing their accuracy. This may open the path to accurate and orders of magnitude cheaper quantum mechanical calculations.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Physics - Chemical Physics,Physics - Computational Physics}
}

@misc{gaoGeneralizingNeuralWave2023,
  title = {Generalizing {{Neural Wave Functions}}},
  author = {Gao, Nicholas and G{\"u}nnemann, Stephan},
  year = {2023},
  month = feb,
  journal = {arXiv.org},
  doi = {10.48550/arXiv.2302.04168},
  abstract = {Recent neural network-based wave functions have achieved state-of-the-art accuracies in modeling ab-initio ground-state potential energy surface. However, these networks can only solve different spatial arrangements of the same set of atoms. To overcome this limitation, we present Graph-learned Orbital Embeddings (Globe), a neural network-based reparametrization method that can adapt neural wave functions to different molecules. We achieve this by combining a localization method for molecular orbitals with spatial message-passing networks. Further, we propose a locality-driven wave function, the Molecular Oribtal Network (Moon), tailored to solving Schr\textbackslash "odinger equations of different molecules jointly. In our experiments, we find Moon requiring 8 times fewer steps to converge to similar accuracies as previous methods when trained on different molecules jointly while Globe enabling the transfer from smaller to larger molecules. Further, our analysis shows that Moon converges similarly to recent transformer-based wave functions on larger molecules. In both the computational chemistry and machine learning literature, we are the first to demonstrate that a single wave function can solve the Schr\textbackslash "odinger equation of molecules with different atoms jointly.},
  howpublished = {https://arxiv.org/abs/2302.04168v1},
  langid = {english}
}

@article{gaoSamplingfreeInferenceAbInitio2022,
  title = {Sampling-Free {{Inference}} for {{Ab-Initio Potential Energy Surface Networks}}},
  author = {Gao, Nicholas and G{\"u}nnemann, Stephan},
  year = {2022},
  month = may,
  doi = {10.48550/arXiv.2205.14962},
  abstract = {Recently, it has been shown that neural networks not only approximate the ground-state wave functions of a single molecular system well but can also generalize to multiple geometries. While such generalization significantly speeds up training, each energy evaluation still requires Monte Carlo integration which limits the evaluation to a few geometries. In this work, we address the inference shortcomings by proposing the Potential learning from ab-initio Networks (PlaNet) framework, in which we simultaneously train a surrogate model in addition to the neural wave function. At inference time, the surrogate avoids expensive Monte-Carlo integration by directly estimating the energy, accelerating the process from hours to milliseconds. In this way, we can accurately model high-resolution multi-dimensional energy surfaces for larger systems that previously were unobtainable via neural wave functions. Finally, we explore an additional inductive bias by introducing physically-motivated restricted neural wave function models. We implement such a function with several additional improvements in the new PESNet++ model. In our experimental evaluation, PlaNet accelerates inference by 7 orders of magnitude for larger molecules like ethanol while preserving accuracy. Compared to previous energy surface networks, PESNet++ reduces energy errors by up to 74\%.},
  langid = {english}
}

@inproceedings{gerardGoldstandardSolutionsSchrodinger2022,
  title = {Gold-Standard Solutions to the {{Schr\"odinger}} Equation Using Deep Learning: {{How}} Much Physics Do We Need?},
  shorttitle = {Gold-Standard Solutions to the {{Schr\"odinger}} Equation Using Deep Learning},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Gerard, Leon and Scherbela, Michael and Marquetand, Philipp and Grohs, Philipp},
  year = {2022},
  month = oct,
  abstract = {Finding accurate solutions to the Schr\"odinger equation is the key unsolved challenge of computational chemistry. Given its importance for the development of new chemical compounds, decades of research have been dedicated to this problem, but due to the large dimensionality even the best available methods do not yet reach the desired accuracy. Recently the combination of deep learning with Monte Carlo methods has emerged as a promising way to obtain highly accurate energies and moderate scaling of computational cost. In this paper we significantly contribute towards this goal by introducing a novel deep-learning architecture that achieves 40-70\% lower energy error at 6x lower computational cost compared to previous approaches. Using our method we establish a new benchmark by calculating the most accurate variational ground state energies ever published for a number of different atoms and molecules. We systematically break down and measure our improvements, focusing in particular on the effect of increasing physical prior knowledge. We surprisingly find that increasing the prior knowledge given to the architecture can actually decrease accuracy.},
  langid = {english}
}

@misc{hermannAbinitioQuantumChemistry2022,
  title = {Ab-Initio Quantum Chemistry with Neural-Network Wavefunctions},
  author = {Hermann, Jan and Spencer, James and Choo, Kenny and Mezzacapo, Antonio and Foulkes, W. M. C. and Pfau, David and Carleo, Giuseppe and No{\'e}, Frank},
  year = {2022},
  month = aug,
  number = {arXiv:2208.12590},
  eprint = {2208.12590},
  eprinttype = {arxiv},
  primaryclass = {physics, stat},
  publisher = {{arXiv}},
  abstract = {Machine learning and specifically deep-learning methods have outperformed human capabilities in many pattern recognition and data processing problems, in game playing, and now also play an increasingly important role in scientific discovery. A key application of machine learning in the molecular sciences is to learn potential energy surfaces or force fields from ab-initio solutions of the electronic Schr\"odinger equation using datasets obtained with density functional theory, coupled cluster, or other quantum chemistry methods. Here we review a recent and complementary approach: using machine learning to aid the direct solution of quantum chemistry problems from first principles. Specifically, we focus on quantum Monte Carlo (QMC) methods that use neural network ansatz functions in order to solve the electronic Schr\"odinger equation, both in first and second quantization, computing ground and excited states, and generalizing over multiple nuclear configurations. Compared to existing quantum chemistry methods, these new deep QMC methods have the potential to generate highly accurate solutions of the Schr\"odinger equation at relatively modest computational cost.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Physics - Chemical Physics,Physics - Computational Physics,Statistics - Machine Learning}
}

@article{hermannDeepneuralnetworkSolutionElectronic2020,
  title = {Deep-Neural-Network Solution of the Electronic {{Schr\"odinger}} Equation},
  author = {Hermann, Jan and Sch{\"a}tzle, Zeno and No{\'e}, Frank},
  year = {2020},
  month = oct,
  journal = {Nature Chemistry},
  volume = {12},
  number = {10},
  pages = {891--897},
  publisher = {{Nature Publishing Group}},
  issn = {1755-4349},
  doi = {10.1038/s41557-020-0544-y},
  abstract = {The electronic Schr\"odinger equation can only be solved analytically for the hydrogen atom, and the numerically exact full configuration-interaction method is exponentially expensive in the number of electrons. Quantum Monte Carlo methods are a possible way out: they scale well for large molecules, they can be parallelized and their accuracy has, as yet, been only limited by the flexibility of the wavefunction ansatz used. Here we propose PauliNet, a deep-learning wavefunction ansatz that achieves nearly exact solutions of the electronic Schr\"odinger equation for molecules with up to 30 electrons. PauliNet has a multireference Hartree\textendash Fock solution built in as a baseline, incorporates the physics of valid wavefunctions and is trained using variational quantum Monte Carlo. PauliNet outperforms previous state-of-the-art variational ansatzes for atoms, diatomic molecules and a strongly correlated linear H10, and matches the accuracy of highly specialized quantum chemistry methods on the transition-state energy of cyclobutadiene, while being computationally efficient.},
  copyright = {2020 The Author(s), under exclusive licence to Springer Nature Limited},
  langid = {english},
  keywords = {Computational chemistry,Method development,Physical chemistry,Quantum chemistry,Theoretical chemistry},
  annotation = {Bandiera\_abtest: a Cg\_type: Nature Research Journals Primary\_atype: Research Subject\_term: Computational chemistry;Method development;Physical chemistry;Quantum chemistry;Theoretical chemistry Subject\_term\_id: computational-chemistry;method-development;physical-chemistry;quantum-chemistry;theoretical-chemistry}
}

@article{liFermionicNeuralNetwork2022,
  title = {Fermionic Neural Network with Ef fective Core Potential},
  author = {Li, Xiang and Fan, Cunwei and Ren, Weiluo and Chen, Ji},
  year = {2022},
  month = jan,
  journal = {Physical Review Research},
  volume = {4},
  number = {1},
  pages = {013021},
  publisher = {{American Physical Society}},
  doi = {10.1103/PhysRevResearch.4.013021},
  abstract = {Deep learning techniques have opened a new venue for electronic structure theory in recent years. In contrast to traditional methods, deep neural networks provide much more expressive and flexible wave function Ans\"atze, resulting in better accuracy and timescale behavior. In order to study larger systems while retaining sufficient accuracy, we integrate a powerful neural-network-based model (FermiNet) with the effective core potential method, which helps to reduce the complexity of the problem by replacing inner core electrons with additional semilocal potential terms in the Hamiltonian. In this work, we calculate the ground-state energy of 3d transition metal atoms and their monoxides, which is quite challenging for the original FermiNet work, and the results are consistent with both experimental data and other state-of-the-art computational methods. Our work is an important step for a broader application of deep learning in the electronic structure calculation of molecules and materials.}
}

@article{pfauInitioSolutionManyelectron2020,
  title = {Ab Initio Solution of the Many-Electron {{Schr\"odinger}} Equation with Deep Neural Networks},
  author = {Pfau, David and Spencer, James S. and Matthews, Alexander G. D. G. and Foulkes, W. M. C.},
  year = {2020},
  month = sep,
  journal = {Phys. Rev. Res.},
  volume = {2},
  number = {3},
  pages = {033429},
  publisher = {{American Physical Society}},
  doi = {10.1103/PhysRevResearch.2.033429}
}

@article{qianInteratomicForceNeural2022,
  title = {Interatomic Force from Neural Network Based Variational Quantum {{Monte Carlo}}},
  author = {Qian, Yubing and Fu, Weizhong and Ren, Weiluo and Chen, Ji},
  year = {2022},
  month = oct,
  journal = {The Journal of Chemical Physics},
  volume = {157},
  number = {16},
  pages = {164104},
  publisher = {{American Institute of Physics}},
  issn = {0021-9606},
  doi = {10.1063/5.0112344},
  abstract = {Accurate ab initio calculations are of fundamental importance in physics, chemistry, biology, and materials science, which have witnessed rapid development in the last couple of years with the help of machine learning computational techniques such as neural networks. Most of the recent efforts applying neural networks to ab initio calculation have been focusing on the energy of the system. In this study, we take a step forward and look at the interatomic force obtained with neural network wavefunction methods by implementing and testing several commonly used force estimators in variational quantum Monte Carlo (VMC). Our results show that neural network ansatz can improve the calculation of interatomic force upon traditional VMC. The relationship between the force error and the quality of the neural network, the contribution of different force terms, and the computational cost of each term is also discussed to provide guidelines for future applications. Our work demonstrates that it is promising to apply neural network wavefunction methods in simulating structures/dynamics of molecules/materials and provide training data for developing accurate force fields.}
}

@article{renGroundStateMolecules2022,
  title = {Towards the Ground State of Molecules via Diffusion {{Monte Carlo}} on Neural Networks},
  author = {Ren, Weiluo and Fu, Weizhong and Chen, Ji},
  year = {2022},
  month = apr,
  journal = {arXiv:2204.13903 [physics]},
  eprint = {2204.13903},
  eprinttype = {arxiv},
  primaryclass = {physics},
  abstract = {Diffusion Monte Carlo (DMC) based on fixed-node approximation has enjoyed significant developments in the past decades and become one of the go-to methods when accurate ground state energy of molecules and materials is needed. The remaining bottleneck is the limitations of the inaccurate nodal structure, prohibiting more challenging electron correlation problems to be tackled with DMC. In this work, we apply the neural-network based trial wavefunction in fixed-node DMC, which allows accurate calculation of a broad range of atomic and molecular systems of different electronic characteristics. Our method is superior in both accuracy and efficiency compared to state-of-the-art neural network methods using variational Monte Carlo. Overall, this computational framework provides a new benchmark for accurate solution of correlated electronic wavefunction and also shed light on the chemical understanding of molecules.},
  archiveprefix = {arXiv},
  keywords = {Physics - Chemical Physics,Physics - Computational Physics}
}

@article{schatzleConvergenceFixednodeLimit2021,
  title = {Convergence to the Fixed-Node Limit in Deep Variational {{Monte Carlo}}},
  author = {Sch{\"a}tzle, Z. and Hermann, J. and No{\'e}, F.},
  year = {2021},
  month = mar,
  journal = {The Journal of Chemical Physics},
  volume = {154},
  number = {12},
  pages = {124108},
  publisher = {{American Institute of Physics}},
  issn = {0021-9606},
  doi = {10.1063/5.0032836},
  abstract = {Variational quantum Monte Carlo (QMC) is an ab initio method for solving the electronic Schr\"odinger equation that is exact in principle, but limited by the flexibility of the available Ans\"atze in practice. The recently introduced deep QMC approach, specifically two deep-neural-network Ans\"atze PauliNet and FermiNet, allows variational QMC to reach the accuracy of diffusion QMC, but little is understood about the convergence behavior of such Ans\"atze. Here, we analyze how deep variational QMC approaches the fixed-node limit with increasing network size. First, we demonstrate that a deep neural network can overcome the limitations of a small basis set and reach the mean-field (MF) complete-basis-set limit. Moving to electron correlation, we then perform an extensive hyperparameter scan of a deep Jastrow factor for LiH and H4 and find that variational energies at the fixed-node limit can be obtained with a sufficiently large network. Finally, we benchmark MF and many-body Ans\"atze on H2O, increasing the fraction of recovered fixed-node correlation energy of single-determinant Slater\textendash Jastrow-type Ans\"atze by half an order of magnitude compared to previous variational QMC results, and demonstrate that a single-determinant Slater\textendash Jastrow-backflow version of the Ansatz overcomes the fixed-node limitations. This analysis helps understand the superb accuracy of deep variational Ans\"atze in comparison to the traditional trial wavefunctions at the respective level of theory and will guide future improvements of the neural-network architectures in deep QMC.}
}

@article{scherbelaSolvingElectronicSchrodinger2022,
  title = {Solving the Electronic {{Schr\"odinger}} Equation for Multiple Nuclear Geometries with Weight-Sharing Deep Neural Networks},
  author = {Scherbela, Michael and Reisenhofer, Rafael and Gerard, Leon and Marquetand, Philipp and Grohs, Philipp},
  year = {2022},
  month = may,
  journal = {Nature Computational Science},
  volume = {2},
  number = {5},
  pages = {331--341},
  publisher = {{Nature Publishing Group}},
  issn = {2662-8457},
  doi = {10.1038/s43588-022-00228-x},
  abstract = {The Schr\"odinger equation describes the quantum-mechanical behaviour of particles, making it the most fundamental equation in chemistry. A solution for a given molecule allows computation of any of its properties. Finding accurate solutions for many different molecules and geometries is thus crucial to the discovery of new materials such as drugs or catalysts. Despite its importance, the Schr\"odinger equation is notoriously difficult to solve even for single molecules, as established methods scale exponentially with the number of particles. Combining Monte Carlo techniques with unsupervised optimization of neural networks was recently discovered as a promising approach to overcome this curse of dimensionality, but the corresponding methods do not exploit synergies that arise when considering multiple geometries. Here we show that sharing the vast majority of weights across neural network models for different geometries substantially accelerates optimization. Furthermore, weight-sharing yields pretrained models that require only a small number of additional optimization steps to obtain high-accuracy solutions for new geometries.},
  copyright = {2022 The Author(s), under exclusive licence to Springer Nature America, Inc.},
  langid = {english},
  keywords = {Communicating chemistry,Computational methods,Computational science,Physical chemistry,Quantum chemistry}
}

@article{schuttSchNetContinuousfilterConvolutional2017,
  title = {{{SchNet}}: {{A}} Continuous-Filter Convolutional Neural Network for Modeling Quantum Interactions},
  shorttitle = {{{SchNet}}},
  author = {Sch{\"u}tt, Kristof T. and Kindermans, Pieter-Jan and Sauceda, Huziel E. and Chmiela, Stefan and Tkatchenko, Alexandre and M{\"u}ller, Klaus-Robert},
  year = {2017},
  month = jun,
  doi = {10.48550/arXiv.1706.08566},
  abstract = {Deep learning has the potential to revolutionize quantum chemistry as it is ideally suited to learn representations for structured data and speed up the exploration of chemical space. While convolutional neural networks have proven to be the first choice for images, audio and video data, the atoms in molecules are not restricted to a grid. Instead, their precise locations contain essential physical information, that would get lost if discretized. Thus, we propose to use continuous-filter convolutional layers to be able to model local correlations without requiring the data to lie on a grid. We apply those layers in SchNet: a novel deep learning architecture modeling quantum interactions in molecules. We obtain a joint model for the total energy and interatomic forces that follows fundamental quantum-chemical principles. This includes rotationally invariant energy predictions and a smooth, differentiable potential energy surface. Our architecture achieves state-of-the-art performance for benchmarks of equilibrium molecules and molecular dynamics trajectories. Finally, we introduce a more challenging benchmark with chemical and structural variations that suggests the path for further work.},
  langid = {english}
}

@article{spencerBetterFasterFermionic2020,
  title = {Better, {{Faster Fermionic Neural Networks}}},
  author = {Spencer, James S. and Pfau, David and Botev, Aleksandar and Foulkes, W. M. C.},
  year = {2020},
  annotation = {\_eprint: 2011.07125}
}

@inproceedings{unkeSEEquivariantPrediction2021,
  title = {{{SE}}(3)-Equivariant Prediction of Molecular Wavefunctions and Electronic Densities},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Unke, Oliver and Bogojeski, Mihail and Gastegger, Michael and Geiger, Mario and Smidt, Tess and M{\"u}ller, Klaus-Robert},
  year = {2021},
  volume = {34},
  pages = {14434--14447},
  publisher = {{Curran Associates, Inc.}},
  abstract = {Machine learning has enabled the prediction of quantum chemical properties with high accuracy and efficiency, allowing to bypass computationally costly ab initio calculations. Instead of training on a fixed set of properties, more recent approaches attempt to learn the electronic wavefunction (or density) as a central quantity of atomistic systems, from which all other observables can be derived. This is complicated by the fact that wavefunctions transform non-trivially under molecular rotations, which makes them a challenging prediction target. To solve this issue, we introduce general SE(3)-equivariant operations and building blocks for constructing deep learning architectures for geometric point cloud data and apply them to reconstruct wavefunctions of atomistic systems with unprecedented accuracy. Our model achieves speedups of over three orders of magnitude compared to ab initio methods and reduces prediction errors by up to two orders of magnitude compared to the previous state-of-the-art. This accuracy makes it possible to derive properties such as energies and forces directly from the wavefunction in an end-to-end manner. We demonstrate the potential of our approach in a transfer learning application, where a model trained on low accuracy reference wavefunctions implicitly learns to correct for electronic many-body interactions from observables computed at a higher level of theory. Such machine-learned wavefunction surrogates pave the way towards novel semi-empirical methods, offering resolution at an electronic level while drastically decreasing computational cost. Additionally, the predicted wavefunctions can serve as initial guess in conventional ab initio methods, decreasing the number of iterations required to arrive at a converged solution, thus leading to significant speedups without any loss of accuracy or robustness. While we focus on physics applications in this contribution, the proposed equivariant framework for deep learning on point clouds is promising also beyond, say, in computer vision or graphics.}
}

@article{wilsonSimulationsStateoftheartFermionic2021a,
  title = {Simulations of State-of-the-Art Fermionic Neural Network Wave Functions with Diffusion {{Monte Carlo}}},
  author = {Wilson, Max and Gao, Nicholas and Wudarski, Filip and Rieffel, Eleanor and Tubman, Norm M.},
  year = {2021},
  month = mar,
  journal = {arXiv:2103.12570 [physics, physics:quant-ph]},
  eprint = {2103.12570},
  eprinttype = {arxiv},
  primaryclass = {physics, physics:quant-ph},
  abstract = {Recently developed neural network-based \textbackslash emph\{ab-initio\} solutions (Pfau et. al arxiv:1909.02487v2) for finding ground states of fermionic systems can generate state-of-the-art results on a broad class of systems. In this work, we improve the results for this Ansatz with Diffusion Monte Carlo. Additionally, we introduce several modifications to the network (Fermi Net) and optimization method (Kronecker Factored Approximate Curvature) that reduce the number of required resources while maintaining or improving the modelling performance. In terms of the model, we remove redundant computations and alter the way data is handled in the permutation equivariant function. The Diffusion Monte Carlo results exceed or match state-of-the-art performance for all systems investigated: atomic systems Be-Ne, and the carbon cation C\$\^+\$.},
  archiveprefix = {arXiv},
  keywords = {Physics - Chemical Physics,Physics - Computational Physics,Quantum Physics}
}

@article{fosterCanonicalConfigurationalInteraction1960,
  title = {Canonical {{Configurational Interaction Procedure}}},
  author = {Foster, J. M. and Boys, S. F.},
  year = {1960},
  month = apr,
  journal = {Reviews of Modern Physics},
  volume = {32},
  number = {2},
  pages = {300--302},
  publisher = {{American Physical Society}},
  doi = {10.1103/RevModPhys.32.300},
  abstract = {DOI:https://doi.org/10.1103/RevModPhys.32.300}
}

@article{pipekFastIntrinsicLocalization1989,
  title = {A Fast Intrinsic Localization Procedure Applicable for Ab Initio and Semiempirical Linear Combination of Atomic Orbital Wave Functions},
  author = {Pipek, J{\'a}nos and Mezey, Paul G.},
  year = {1989},
  month = may,
  journal = {The Journal of Chemical Physics},
  volume = {90},
  number = {9},
  pages = {4916--4926},
  publisher = {{American Institute of Physics}},
  issn = {0021-9606},
  doi = {10.1063/1.456588}
}

@article{lehtolaPipekMezeyOrbital2014,
  title = {Pipek\textendash{{Mezey Orbital Localization Using Various Partial Charge Estimates}}},
  author = {Lehtola, Susi and J{\'o}nsson, Hannes},
  year = {2014},
  month = feb,
  journal = {Journal of Chemical Theory and Computation},
  volume = {10},
  number = {2},
  pages = {642--649},
  publisher = {{American Chemical Society}},
  issn = {1549-9618},
  doi = {10.1021/ct401016x},
  abstract = {The Pipek\textendash Mezey scheme for generating chemically intuitive, localized molecular orbitals is generalized to incorporate various ways of estimating the atomic charges, instead of the ill-defined Mulliken charges used in the original formulation, or L\"owdin charges, which have also been used. Calculations based on Bader, Becke, Voronoi, Hirshfeld, and Stockholder partial charges, as well as intrinsic atomic orbital charges, are applied to orbital localization for a variety of molecules. While the charges obtained with these various estimates differ greatly, the resulting localized orbitals are found to be quite similar and properly separate {$\sigma$} and {$\pi$} orbitals, as well as core and valence orbitals. The calculated results are only weakly dependent on the basis set, unlike those based on Mulliken or L\"owdin charges. The effect of varying the penalty exponent on the charge in the objective function was studied briefly and was found to lead to some changes in the localized orbitals when degeneracies are present. The various localization methods have been implemented in ERKALE, an open source program for electronic structure calculations.}
}



@article{sunRecentDevelopmentsPySCF2020,
  title = {Recent Developments in the {{PySCF}} Program Package},
  author = {Sun, Qiming and Zhang, Xing and Banerjee, Samragni and Bao, Peng and Barbry, Marc and Blunt, Nick S. and Bogdanov, Nikolay A. and Booth, George H. and Chen, Jia and Cui, Zhi-Hao and Eriksen, Janus J. and Gao, Yang and Guo, Sheng and Hermann, Jan and Hermes, Matthew R. and Koh, Kevin and Koval, Peter and Lehtola, Susi and Li, Zhendong and Liu, Junzi and Mardirossian, Narbe and McClain, James D. and Motta, Mario and Mussard, Bastien and Pham, Hung Q. and Pulkin, Artem and Purwanto, Wirawan and Robinson, Paul J. and Ronca, Enrico and Sayfutyarova, Elvira R. and Scheurer, Maximilian and Schurkus, Henry F. and Smith, James E. T. and Sun, Chong and Sun, Shi-Ning and Upadhyay, Shiv and Wagner, Lucas K. and Wang, Xiao and White, Alec and Whitfield, James Daniel and Williamson, Mark J. and Wouters, Sebastian and Yang, Jun and Yu, Jason M. and Zhu, Tianyu and Berkelbach, Timothy C. and Sharma, Sandeep and Sokolov, Alexander Yu. and Chan, Garnet Kin-Lic},
  year = {2020},
  month = jul,
  journal = {The Journal of Chemical Physics},
  volume = {153},
  number = {2},
  pages = {024109},
  publisher = {{American Institute of Physics}},
  issn = {0021-9606},
  doi = {10.1063/5.0006074}
}

@article{simonscollaborationonthemany-electronproblemSolutionManyElectronProblem2017,
  title = {Towards the {{Solution}} of the {{Many-Electron Problem}} in {{Real Materials}}: {{Equation}} of {{State}} of the {{Hydrogen Chain}} with {{State-of-the-Art Many-Body Methods}}},
  shorttitle = {Towards the {{Solution}} of the {{Many-Electron Problem}} in {{Real Materials}}},
  author = {{Simons Collaboration on the Many-Electron Problem} and Motta, Mario and Ceperley, David M. and Chan, Garnet Kin-Lic and Gomez, John A. and Gull, Emanuel and Guo, Sheng and {Jim{\'e}nez-Hoyos}, Carlos A. and Lan, Tran Nguyen and Li, Jia and Ma, Fengjie and Millis, Andrew J. and Prokof'ev, Nikolay V. and Ray, Ushnish and Scuseria, Gustavo E. and Sorella, Sandro and Stoudenmire, Edwin M. and Sun, Qiming and Tupitsyn, Igor S. and White, Steven R. and Zgid, Dominika and Zhang, Shiwei},
  year = {2017},
  month = sep,
  journal = {Physical Review X},
  volume = {7},
  number = {3},
  pages = {031059},
  publisher = {{American Physical Society}},
  doi = {10.1103/PhysRevX.7.031059},
  abstract = {We present numerical results for the equation of state of an infinite chain of hydrogen atoms. A variety of modern many-body methods are employed, with exhaustive cross-checks and validation. Approaches for reaching the continuous space limit and the thermodynamic limit are investigated, proposed, and tested. The detailed comparisons provide a benchmark for assessing the current state of the art in many-body computation, and for the development of new methods. The ground-state energy per atom in the linear chain is accurately determined versus bond length, with a confidence bound given on all uncertainties.}
}

@misc{RDKit,
  author = {Landrum, Greg},
  title = {RDKit: Open-source cheminformatics},
  year = {2009},
  howpublished = {\url{https://github.com/rdkit/rdkit}},
  note = {GitHub repository},
}

@article{Han_2019,
	author = {Han, Jiequn and Zhang, Linfeng and E, Weinan},
	doi = {10.1016/j.jcp.2019.108929},
	issn = {0021-9991},
	journal = {J. Comput. Phys.},
	month = dec,
	pages = {108929},
	publisher = {Elsevier BV},
	title = {Solving many-electron {S}chrödinger equation using deep neural networks},
	url = {http://dx.doi.org/10.1016/j.jcp.2019.108929},
	volume = {399},
	year = {2019}
}


@misc{ferminet_dmc_bytedance,
  doi = {10.48550/ARXIV.2204.13903},
  
  url = {https://arxiv.org/abs/2204.13903},
  
  author = {Ren, Weiluo and Fu, Weizhong and Chen, Ji},
  
  keywords = {Chemical Physics (physics.chem-ph), Computational Physics (physics.comp-ph), FOS: Physical sciences, FOS: Physical sciences},
  
  title = {Towards the ground state of molecules via diffusion Monte Carlo on neural networks},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{FermiNet_DMC,
	archiveprefix = {arXiv},
	doi = {10.48550/ARXIV.2103.12570},
	author = {Wilson, Max and Gao, Nicholas and Wudarski, Filip and Rieffel, Eleanor and Tubman, Norm M.},
	eprint = {2103.12570},
	primaryclass = {physics.chem-ph},
	title = {Simulations of state-of-the-art fermionic neural network wave functions with diffusion {M}onte {C}arlo},
	year = {2021}
}



@article{qianYubing_dl_forces_2022,
author = {Qian,Yubing  and Fu,Weizhong  and Ren,Weiluo  and Chen,Ji },
title = {Interatomic force from neural network based variational quantum Monte Carlo},
journal = {The Journal of Chemical Physics},
volume = {157},
number = {16},
pages = {164104},
year = {2022},
doi = {10.1063/5.0112344},

URL = { 
        https://doi.org/10.1063/5.0112344
    
},
eprint = { 
        https://doi.org/10.1063/5.0112344
    
}

}

@article{entwistle_electronic_2023,
	title = {Electronic excited states in deep variational {Monte} {Carlo}},
	volume = {14},
	issn = {2041-1723},
	url = {https://doi.org/10.1038/s41467-022-35534-5},
	doi = {10.1038/s41467-022-35534-5},
	abstract = {Obtaining accurate ground and low-lying excited states of electronic systems is crucial in a multitude of important applications. One ab initio method for solving the Schrödinger equation that scales favorably for large systems is variational quantum Monte Carlo (QMC). The recently introduced deep QMC approach uses ansatzes represented by deep neural networks and generates nearly exact ground-state solutions for molecules containing up to a few dozen electrons, with the potential to scale to much larger systems where other highly accurate methods are not feasible. In this paper, we extend one such ansatz (PauliNet) to compute electronic excited states. We demonstrate our method on various small atoms and molecules and consistently achieve high accuracy for low-lying states. To highlight the method’s potential, we compute the first excited state of the much larger benzene molecule, as well as the conical intersection of ethylene, with PauliNet matching results of more expensive high-level methods.},
	number = {1},
	journal = {Nature Communications},
	author = {Entwistle, M. T. and Schätzle, Z. and Erdman, P. A. and Hermann, J. and Noé, F.},
	month = jan,
	year = {2023},
	pages = {274},
}

@article{li_ab_solids_2022,
	title = {Ab initio calculation of real solids via neural network ansatz},
	volume = {13},
	issn = {2041-1723},
	url = {https://doi.org/10.1038/s41467-022-35627-1},
	doi = {10.1038/s41467-022-35627-1},
	abstract = {Neural networks have been applied to tackle many-body electron correlations for small molecules and physical models in recent years. Here we propose an architecture that extends molecular neural networks with the inclusion of periodic boundary conditions to enable ab initio calculation of real solids. The accuracy of our approach is demonstrated in four different types of systems, namely the one-dimensional periodic hydrogen chain, the two-dimensional graphene, the three-dimensional lithium hydride crystal, and the homogeneous electron gas, where the obtained results, e.g. total energies, dissociation curves, and cohesive energies, reach a competitive level with many traditional ab initio methods. Moreover, electron densities of typical systems are also calculated to provide physical intuition of various solids. Our method of extending a molecular neural network to periodic systems can be easily integrated into other neural network structures, highlighting a promising future of ab initio solution of more complex solid systems using neural network ansatz, and more generally endorsing the application of machine learning in materials simulation and condensed matter physics.},
	number = {1},
	journal = {Nature Communications},
	author = {Li, Xiang and Li, Zhe and Chen, Ji},
	month = dec,
	year = {2022},
	pages = {7895},
}

@article{cassella_model_solids_physrevlett_2023,
  title = {Discovering Quantum Phase Transitions with Fermionic Neural Networks},
  author = {Cassella, Gino and Sutterud, Halvard and Azadi, Sam and Drummond, N. D. and Pfau, David and Spencer, James S. and Foulkes, W. M. C.},
  journal = {Phys. Rev. Lett.},
  volume = {130},
  issue = {3},
  pages = {036401},
  numpages = {6},
  year = {2023},
  month = jan,
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevLett.130.036401},
  url = {https://link.aps.org/doi/10.1103/PhysRevLett.130.036401}
}


@misc{Wilson_HEG2022,
  doi = {10.48550/ARXIV.2202.04622},
  
  url = {https://arxiv.org/abs/2202.04622},
  
  author = {Wilson, Max and Moroni, Saverio and Holzmann, Markus and Gao, Nicholas and Wudarski, Filip and Vegge, Tejs and Bhowmik, Arghya},
  
  keywords = {Chemical Physics (physics.chem-ph), Materials Science (cond-mat.mtrl-sci), Quantum Physics (quant-ph), FOS: Physical sciences, FOS: Physical sciences},
  
  title = {Wave function Ansatz (but Periodic) Networks and the Homogeneous Electron Gas},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}




@misc{gao-pesnet++_2022,
  doi = {10.48550/ARXIV.2205.14962},
  
  url = {https://arxiv.org/abs/2205.14962},
  
  author = {Gao, Nicholas and Günnemann, Stephan},
  
  keywords = {Machine Learning (cs.LG), Chemical Physics (physics.chem-ph), Computational Physics (physics.comp-ph), FOS: Computer and information sciences, FOS: Computer and information sciences, FOS: Physical sciences, FOS: Physical sciences},
  
  title = {Sampling-free Inference for Ab-Initio Potential Energy Surface Networks},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {Creative Commons Attribution 4.0 International}
}

@misc{gpt3_2020,
  doi = {10.48550/ARXIV.2005.14165},
  
  url = {https://arxiv.org/abs/2005.14165},
  
  author = {Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel M. and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
  
  keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Language Models are Few-Shot Learners},
  
  publisher = {arXiv},
  
  year = {2020},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@misc{batatia_ilyes_mace2022,
  doi = {10.48550/ARXIV.2206.07697},
  
  url = {https://arxiv.org/abs/2206.07697},
  
  author = {Batatia, Ilyes and Kovács, Dávid Péter and Simm, Gregor N. C. and Ortner, Christoph and Csányi, Gábor},
  
  keywords = {Machine Learning (stat.ML), Materials Science (cond-mat.mtrl-sci), Machine Learning (cs.LG), Chemical Physics (physics.chem-ph), FOS: Computer and information sciences, FOS: Computer and information sciences, FOS: Physical sciences, FOS: Physical sciences},
  
  title = {MACE: Higher Order Equivariant Message Passing Neural Networks for Fast and Accurate Force Fields},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{gasteggerDeepNeuralNetwork2020,
  title = {A Deep Neural Network for Molecular Wave Functions in Quasi-Atomic Minimal Basis Representation},
  author = {Gastegger, M. and McSloy, A. and Luya, M. and Sch{\"u}tt, K. T. and Maurer, R. J.},
  year = {2020},
  month = jul,
  journal = {The Journal of Chemical Physics},
  volume = {153},
  number = {4},
  eprint = {2005.06979},
  primaryclass = {physics, stat},
  pages = {044123},
  issn = {0021-9606, 1089-7690},
  doi = {10.1063/5.0012911},
  abstract = {The emergence of machine learning methods in quantum chemistry provides new methods to revisit an old problem: Can the predictive accuracy of electronic structure calculations be decoupled from their numerical bottlenecks? Previous attempts to answer this question have, among other methods, given rise to semi-empirical quantum chemistry in minimal basis representation. We present an adaptation of the recently proposed SchNet for Orbitals (SchNOrb) deep convolutional neural network model [Nature Commun. 10, 5024 (2019)] for electronic wave functions in an optimised quasi-atomic minimal basis representation. For five organic molecules ranging from 5 to 13 heavy atoms, the model accurately predicts molecular orbital energies and wavefunctions and provides access to derived properties for chemical bonding analysis. Particularly for larger molecules, the model outperforms the original atomic-orbital-based SchNOrb method in terms of accuracy and scaling. We conclude by discussing the future potential of this approach in quantum chemical workflows.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Physics - Computational Physics,Statistics - Machine Learning}
}

@inproceedings{martens2015optimizing,
  title={Optimizing neural networks with kronecker-factored approximate curvature},
  author={Martens, James and Grosse, Roger},
  booktitle={International conference on machine learning},
  pages={2408--2417},
  year={2015},
  organization={PMLR}
}

@software{kfac_repo_jax,
  author = {Aleksandar Botev and James Martens},
  title = {{KFAC-JAX}},
  url = {http://github.com/deepmind/kfac-jax},
  version = {0.0.1},
  year = {2022},
}

@misc{vonglehnSelfAttentionAnsatzAbinitio2022,
  title = {A {{Self-Attention Ansatz}} for {{Ab-initio Quantum Chemistry}}},
  author = {{von Glehn}, Ingrid and Spencer, James S. and Pfau, David},
  year = {2022},
  month = nov,
  journal = {arXiv.org},
  doi = {10.48550/arXiv.2211.13672},
  abstract = {We present a novel neural network architecture using self-attention, the Wavefunction Transformer (Psiformer), which can be used as an approximation (or Ansatz) for solving the many-electron Schr\textbackslash "odinger equation, the fundamental equation for quantum chemistry and material science. This equation can be solved from first principles, requiring no external training data. In recent years, deep neural networks like the FermiNet and PauliNet have been used to significantly improve the accuracy of these first-principle calculations, but they lack an attention-like mechanism for gating interactions between electrons. Here we show that the Psiformer can be used as a drop-in replacement for these other neural networks, often dramatically improving the accuracy of the calculations. On larger molecules especially, the ground state energy can be improved by dozens of kcal/mol, a qualitative leap over previous methods. This demonstrates that self-attention networks can learn complex quantum mechanical correlations between electrons, and are a promising route to reaching unprecedented accuracy in chemical calculations on larger systems.},
  url = {https://arxiv.org/abs/2211.13672v1},
  langid = {english}
}


@article{carleoSolvingQuantumManybody2017,
  title = {Solving the Quantum Many-Body Problem with Artificial Neural Networks},
  author = {Carleo, Giuseppe and Troyer, Matthias},
  year = {2017},
  month = feb,
  journal = {Science},
  volume = {355},
  number = {6325},
  pages = {602--606},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.aag2302},
  langid = {english}
}

@misc{kochkovVariationalOptimizationAI2018,
  title = {Variational Optimization in the {{AI}} Era: {{Computational Graph States}} and {{Supervised Wave-function Optimization}}},
  shorttitle = {Variational Optimization in the {{AI}} Era},
  author = {Kochkov, Dmitrii and Clark, Bryan K.},
  year = {2018},
  month = nov,
  journal = {arXiv.org},
  doi = {10.48550/arXiv.1811.12423},
  abstract = {Representing a target quantum state by a compact, efficient variational wave-function is an important approach to the quantum many-body problem. In this approach, the main challenges include the design of a suitable variational ansatz and optimization of its parameters. In this work, we address both of these challenges. First, we define the variational class of Computational Graph States (CGS) which gives a uniform framework for describing all computable variational ansatz. Secondly, we develop a novel optimization scheme, supervised wave-function optimization (SWO), which systematically improves the optimized wave-function by drawing on ideas from supervised learning. While SWO can be used independently of CGS, utilizing them together provides a flexible framework for the rapid design, prototyping and optimization of variational wave-functions. We demonstrate CGS and SWO by optimizing for the ground state wave-function of 1D and 2D Heisenberg models on nine different variational architectures including architectures not previously used to represent quantum many-body wave-functions and find they are energetically competitive to other approaches. One interesting application of this architectural exploration is that we show that fully convolution neural network wave-functions can be optimized for one system size and, using identical parameters, produce accurate energies for a range of system sizes. We expect these methods to increase the rate of discovery of novel variational ansatz and bring further insights to the quantum many body problem.},
  howpublished = {https://arxiv.org/abs/1811.12423v1},
  langid = {english}
}

@misc{radfordLearningTransferableVisual2021,
  title = {Learning {{Transferable Visual Models From Natural Language Supervision}}},
  author = {Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and Krueger, Gretchen and Sutskever, Ilya},
  year = {2021},
  month = feb,
  journal = {arXiv.org},
  doi = {10.48550/arXiv.2103.00020},
  abstract = {State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on. We release our code and pre-trained model weights at https://github.com/OpenAI/CLIP.},
  url = {https://arxiv.org/abs/2103.00020v1},
  langid = {english}
}

@misc{yuanFlorenceNewFoundation2021,
  title = {Florence: {{A New Foundation Model}} for {{Computer Vision}}},
  shorttitle = {Florence},
  author = {Yuan, Lu and Chen, Dongdong and Chen, Yi-Ling and Codella, Noel and Dai, Xiyang and Gao, Jianfeng and Hu, Houdong and Huang, Xuedong and Li, Boxin and Li, Chunyuan and Liu, Ce and Liu, Mengchen and Liu, Zicheng and Lu, Yumao and Shi, Yu and Wang, Lijuan and Wang, Jianfeng and Xiao, Bin and Xiao, Zhen and Yang, Jianwei and Zeng, Michael and Zhou, Luowei and Zhang, Pengchuan},
  year = {2021},
  month = nov,
  journal = {arXiv.org},
  doi = {10.48550/arXiv.2111.11432},
  abstract = {Automated visual understanding of our diverse and open world demands computer vision models to generalize well with minimal customization for specific tasks, similar to human vision. Computer vision foundation models, which are trained on diverse, large-scale dataset and can be adapted to a wide range of downstream tasks, are critical for this mission to solve real-world computer vision applications. While existing vision foundation models such as CLIP, ALIGN, and Wu Dao 2.0 focus mainly on mapping images and textual representations to a cross-modal shared representation, we introduce a new computer vision foundation model, Florence, to expand the representations from coarse (scene) to fine (object), from static (images) to dynamic (videos), and from RGB to multiple modalities (caption, depth). By incorporating universal visual-language representations from Web-scale image-text data, our Florence model can be easily adapted for various computer vision tasks, such as classification, retrieval, object detection, VQA, image caption, video retrieval and action recognition. Moreover, Florence demonstrates outstanding performance in many types of transfer learning: fully sampled fine-tuning, linear probing, few-shot transfer and zero-shot transfer for novel images and objects. All of these properties are critical for our vision foundation model to serve general purpose vision tasks. Florence achieves new state-of-the-art results in majority of 44 representative benchmarks, e.g., ImageNet-1K zero-shot classification with top-1 accuracy of 83.74 and the top-5 accuracy of 97.18, 62.4 mAP on COCO fine tuning, 80.36 on VQA, and 87.8 on Kinetics-600.},
  howpublished = {https://arxiv.org/abs/2111.11432v1},
  langid = {english}
}

@article{hastingsMonteCarloSampling1970,
  title = {Monte {{Carlo}} Sampling Methods Using {{Markov}} Chains and Their Applications},
  author = {Hastings, W. K.},
  year = {1970},
  month = apr,
  journal = {Biometrika},
  volume = {57},
  number = {1},
  pages = {97--109},
  issn = {0006-3444},
  doi = {10.1093/biomet/57.1.97},
  abstract = {A generalization of the sampling method introduced by Metropolis et al. (1953) is presented along with an exposition of the relevant theory, techniques of application and methods and difficulties of assessing the error in Monte Carlo estimates. Examples of the methods, including the generation of random orthogonal matrices and potential applications of the methods to numerical problems arising in statistics, are discussed.}
}

@article{hoffmannTrainingComputeOptimalLarge2022,
  title = {Training {{Compute-Optimal Large Language Models}}},
  author = {Hoffmann, Jordan and Borgeaud, Sebastian and Mensch, Arthur and Buchatskaya, Elena and Cai, Trevor and Rutherford, Eliza and Casas, Diego de Las and Hendricks, Lisa Anne and Welbl, Johannes and Clark, Aidan and Hennigan, Tom and Noland, Eric and Millican, Katie and van den Driessche, George and Damoc, Bogdan and Guy, Aurelia and Osindero, Simon and Simonyan, Karen and Elsen, Erich and Rae, Jack W. and Vinyals, Oriol and Sifre, Laurent},
  year = {2022},
  month = mar,
  doi = {10.48550/arXiv.2203.15556},
  abstract = {We investigate the optimal model size and number of tokens for training a transformer language model under a given compute budget. We find that current large language models are significantly undertrained, a consequence of the recent focus on scaling language models whilst keeping the amount of training data constant. By training over 400 language models ranging from 70 million to over 16 billion parameters on 5 to 500 billion tokens, we find that for compute-optimal training, the model size and the number of training tokens should be scaled equally: for every doubling of model size the number of training tokens should also be doubled. We test this hypothesis by training a predicted compute-optimal model, Chinchilla, that uses the same compute budget as Gopher but with 70B parameters and 4\$\textbackslash times\$ more more data. Chinchilla uniformly and significantly outperforms Gopher (280B), GPT-3 (175B), Jurassic-1 (178B), and Megatron-Turing NLG (530B) on a large range of downstream evaluation tasks. This also means that Chinchilla uses substantially less compute for fine-tuning and inference, greatly facilitating downstream usage. As a highlight, Chinchilla reaches a state-of-the-art average accuracy of 67.5\% on the MMLU benchmark, greater than a 7\% improvement over Gopher.},
  langid = {english}
}

@article{schuttUnifyingMachineLearning2019,
  title = {Unifying Machine Learning and Quantum Chemistry with a Deep Neural Network for Molecular Wavefunctions},
  author = {Sch{\"u}tt, K. T. and Gastegger, M. and Tkatchenko, A. and M{\"u}ller, K.-R. and Maurer, R. J.},
  year = {2019},
  month = nov,
  journal = {Nat. Commun.},
  volume = {10},
  number = {1},
  pages = {5024},
  issn = {2041-1723},
  doi = {10.1038/s41467-019-12875-2},
  abstract = {Machine learning advances chemistry and materials science by enabling large-scale exploration of chemical space based on quantum chemical calculations. While these models supply fast and accurate predictions of atomistic chemical properties, they do not explicitly capture the electronic degrees of freedom of a molecule, which limits their applicability for reactive chemistry and chemical analysis. Here we present a deep learning framework for the prediction of the quantum mechanical wavefunction in a local basis of atomic orbitals from which all other ground-state properties can be derived. This approach retains full access to the electronic structure via the wavefunction at force-field-like efficiency and captures quantum mechanics in an analytically differentiable representation. On several examples, we demonstrate that this opens promising avenues to perform inverse design of molecular structures for targeting electronic property optimisation and a clear path towards increased synergy of machine learning and quantum chemistry.}
}

@article{batatiaDesignSpaceEquivariant2022,
  title = {The {{Design Space}} of {{E}}(3)-{{Equivariant Atom-Centered Interatomic Potentials}}},
  author = {Batatia, Ilyes and Batzner, Simon and Kov{\'a}cs, D{\'a}vid P{\'e}ter and Musaelian, Albert and Simm, Gregor N. C. and Drautz, Ralf and Ortner, Christoph and Kozinsky, Boris and Cs{\'a}nyi, G{\'a}bor},
  year = {2022},
  month = may,
  doi = {10.48550/arXiv.2205.06643},
  urldate = {2022-10-24},
  abstract = {The rapid progress of machine learning interatomic potentials over the past couple of years produced a number of new architectures. Particularly notable among these are the Atomic Cluster Expansion (ACE), which unified many of the earlier ideas around atom density-based descriptors, and Neural Equivariant Interatomic Potentials (NequIP), a message passing neural network with equivariant features that showed state of the art accuracy. In this work, we construct a mathematical framework that unifies these models: ACE is generalised so that it can be recast as one layer of a multi-layer architecture. From another point of view, the linearised version of NequIP is understood as a particular sparsification of a much larger polynomial model. Our framework also provides a practical tool for systematically probing different choices in the unified design space. We demonstrate this by an ablation study of NequIP via a set of experiments looking at in- and out-of-domain accuracy and smooth extrapolation very far from the training data, and shed some light on which design choices are critical for achieving high accuracy. Finally, we present BOTNet (Body-Ordered-Tensor-Network), a much-simplified version of NequIP, which has an interpretable architecture and maintains accuracy on benchmark datasets.},
  langid = {english}
}


@article{westermayr_machine_2021,
	title = {Machine {Learning} for {Electronically} {Excited} {States} of {Molecules}},
	volume = {121},
	issn = {0009-2665},
	url = {https://doi.org/10.1021/acs.chemrev.0c00749},
	doi = {10.1021/acs.chemrev.0c00749},
	number = {16},
	journal = {Chemical Reviews},
	author = {Westermayr, Julia and Marquetand, Philipp},
	month = aug,
	year = {2021},
	note = {Publisher: American Chemical Society},
	pages = {9873--9926},
	annote = {doi: 10.1021/acs.chemrev.0c00749},
}