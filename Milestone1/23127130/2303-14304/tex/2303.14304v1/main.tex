
\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage[pagenumbers]{cvpr} %

\usepackage{graphicx, nicefrac}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage[table,xcdraw]{xcolor}
\usepackage[margin=0pt,position=bottom,font=footnotesize,labelfont=bf]{caption}

\usepackage[accsupp]{axessibility}  %

\usepackage[pagebackref,breaklinks,colorlinks]{hyperref}

\usepackage{cuted}

\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}


\usepackage{dblfloatfix}
\usepackage{float}
\usepackage{pifont}%
\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%

\DeclareMathOperator*{\argmax}{\bf arg\,max}
\DeclareMathOperator*{\argmin}{\bf arg\,min}
\newcommand{\vw}{\ensuremath{\mathbf w}}
\newcommand{\vv}{\ensuremath{\mathbf v}}
\newcommand{\vx}{\ensuremath{\mathbf x}}

\newcommand{\zikui}[1]{\textcolor{blue}{\textbf{ZC: #1}}}
\newcommand{\ytan}[1]{\textcolor{orange}{\textbf{YT: #1}}}
\newcommand{\Salnote}[1]{\textcolor{red}{\textbf{SA: #1}}}

\newcommand{\squishlist}{
   \begin{list}{$\bullet$}
    { \setlength{\itemsep}{0pt}      \setlength{\parsep}{3pt}
      \setlength{\topsep}{3pt}       \setlength{\partopsep}{0pt}
      \setlength{\leftmargin}{1.0em} \setlength{\labelwidth}{1em}
      \setlength{\labelsep}{0.5em} } }
      
\newcommand{\squishend}{
    \end{list}  }

\setlength{\belowcaptionskip}{0pt}

\def\cvprPaperID{7059} %
\def\confName{CVPR}
\def\confYear{2023}


\begin{document}

\title{
Ensemble-based Blackbox Attacks on Dense Prediction 
}

\author{Zikui Cai$^*$, Yaoteng Tan$^*$, and M. Salman Asif\\
University of California Riverside\\
{\tt\small \{zcai032,ytan073,sasif\}@ucr.edu}
}
\maketitle

\def\thefootnote{*}\footnotetext{Equal contribution}

\begin{abstract}
We propose an approach for adversarial attacks on dense prediction models (such as object detectors and segmentation). It is well known that the attacks generated by a single surrogate model do not transfer to arbitrary (blackbox) victim models. Furthermore, targeted attacks are often more challenging than the untargeted attacks. In this paper, we show that a carefully designed ensemble can create effective attacks for a number of victim models. In particular, we show that normalization of the weights for individual models plays a critical role in the success of the attacks. We then demonstrate that by adjusting the weights of the ensemble according to the victim model can further improve the performance of the attacks. We performed a number of experiments for object detectors and segmentation to highlight the significance of the our proposed methods. Our proposed ensemble-based method outperforms existing blackbox attack methods for object detection and segmentation. Finally we show that our proposed method can also generate a single perturbation that can fool multiple blackbox detection and segmentation models simultaneously. Code is available at \href{https://github.com/CSIPlab/EBAD}{\texttt{https://github.com/CSIPlab/EBAD}}.




\end{abstract}

\section{Introduction}
\label{sec:intro}




Computer vision models (e.g., classification, object detection, segmentation, and depth estimation) are known to be vulnerable to carefully crafted adversarial examples~\cite{szegedy2013intriguing,goodfellow2014explaining,cai2022context,gu2021adversarial,cheng2022physical}. 
Creating such adversarial attacks is easy for whitebox models, where the victim model is completely known~\cite{goodfellow2014explaining,kurakin2016adversarial,madry2017towards,dong2018boosting,xie2019improving}. In contrast, creating adversarial attacks for blackbox models, where the victim model is unknown, remains a challenging task \cite{liu2017delving,xie2017adversarial,arnab2018robustness}. Most of the existing blackbox attack methods have been developed for classification models \cite{lord2022attacking,huang2019black,cheng2019improving,tashiro2020diversity}. Blackbox attacks for dense prediction models such as object detection and segmentation are relatively less studied \cite{cai2022context, gu2021adversarial, liang2022large}, and most of the existing ones mainly focus on untargeted attacks \cite{gu2021adversarial}. 
Furthermore, a vast majority of these methods are based on transfer attacks, in which a surrogate (whitebox) model is used to generate the adversarial example that is tested on the victim model. However, the success rate of such transfer-based attacks is often low, especially for targeted attacks \cite{huang2019black,cheng2019improving,tashiro2020diversity}. 



\begin{figure}[t]
    \centering
    \includegraphics[width=0.45\textwidth]{figures/intro.png}
    \caption{Illustration of the targeted ensemble-based blackbox attack. (Top) Attack generated by a single surrogate model does not transfer on the victim blackbox model (person does not map to car). (Bottom) Attack generated by weight balancing and optimization can transfer on a variety of victim models (person is mapped to car).}\label{fig:intro}
    \vspace{-5mm}
\end{figure}




In this paper, we propose and evaluate an ensemble-based blackbox attack method for objection detection and segmentation. 
Our method is inspired by three key observations: 
1) targeted attacks generated by a single surrogate model are rarely successful; 
2) attacks generated by an ensemble of surrogate models are highly successful if the contribution from all the models is properly normalized; and 
3) attacks generated by an ensemble for a specific victim model can be further improved by adjusting the contributions of different surrogate models. 
The overall idea of the proposed work is illustrated in \cref{fig:intro}. 
Our proposed method can be viewed as a combination of transfer- and query-based attacks, where we can adjust the contribution based on the feedback from the victim model using a small number of queries (5--20 in our experiments).
In contrast, conventional query-based attacks require hundreds or thousands of queries from the victim model \cite{chen2017zoo,ilyas2018black,tu2019autozoom,guo2019simple}.


We conduct comprehensive experiments to validate our proposed method and achieve state-of-the-art performance for both targeted and untargeted blackbox attacks on object detection. 
Specifically, our proposed method attains 29--53\% success rate using only 5 queries for targeted attacks on object detectors, whereas the current state-of-the-art method \cite{cai2022context} achieves 20--39\% success rate with the same number of queries. 
Furthermore, we extend our evaluation to untargeted and targeted attacks on blackbox semantic segmentation models. Our method achieves 0.9--1.55\% mIoU for untargeted and 69--95\% pixel-wise success for targeted attacks. By comparison, the current state-of-the-art method \cite{gu2021adversarial} obtains
 0.6--7.97\% mIoU for untargeted attacks and does not report results for targeted attacks. 
To the best of our knowledge, our work is the first approach for targeted and query-based attacks for semantic segmentation. 


Below we summarize main contributions of this work. 

\squishlist
\item We design a novel framework that can effectively attack blackbox dense prediction models based on an ensemble of surrogate models. %
\item We propose two simple yet highly effective ideas, namely weight balancing and weight optimization, with which we can achieve significantly better attack performance compared to existing methods. 
\item We extensively evaluate our method for targeted and untargeted attacks on object detection and semantic segmentation models and achieve state-of-the-art results. 
\item We demonstrate that our proposed method can generate a single perturbation that can fool multiple blackbox detection and segmentation models simultaneously. 
\squishend


\input{related}
\input{method}
\input{experiment}

\vspace{-5pt}
\section{Conclusion}
We presented a new method to generate targeted attacks for dense prediction task (e.g., object detectors and semantic segmentation) using an ensemble of surrogate models. We demonstrate that (victim model-agnostic) weight balancing and (victim model-specific) weight optimization play a critical role in the success of attacks. We present an extensive set of experiments to demonstrate the performance of our method with different models and datasets. Finally, we show that our approach can create adversarial examples to fool multiple blackbox models and tasks jointly. 

\noindent\textbf{Limitations.}
Our method employs an ensemble of surrogate models to generate attacks, which inevitably incurs higher memory and computational overhead. Moreover, the success of our method hinges on the availability of a diverse set of surrogate models, which could potentially limit its efficacy if such models are not readily obtainable.


\noindent\textbf{Acknowledgments.} 
This material is based upon work supported by AFOSR award FA9550-21-1-0330, NSF award 2046293, UC Regents Faculty Development grant. We acknowledge the computing support from Nautilus PRP.







{\small
\bibliographystyle{ieee_fullname}
\bibliography{refs}
}

\newpage
\input{supp}

\end{document}
