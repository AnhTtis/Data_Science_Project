\section{Method}

\subsection{Preliminaries}

We consider a per-instance attack scenario in which we generate adversarial perturbation $\delta$ for a given image $x$. To keep the perturbation imperceptible, we bound its $\ell_p$ norm as $\|\delta\|_p \leq \varepsilon$. In our experiments, we mainly use $\ell_\infty$ or max norm that limits the maximum level of perturbation. Our goal is to find $\delta$ such that the perturbed image, $x^\star = x + \delta$, can disrupt a victim image recognition system $f_\vv$ to make wrong predictions. Suppose the original prediction for the clean image $x$ is $y = f_\vv(x)$. The attack goal is $f(x^\star) \neq y$ for untargeted attack, and $f(x^\star) = y^\star$ for targeted attack, where $y^\star$ is the desired output (e.g., label or bounding box or segmentation map).

For classification models, the label $y \in \mathbb{R}$ is a scalar. However, dense prediction models can have  more complex output space. For object detection, the variable-length output $y \in \mathbb{R}^{K\times6}$, where $K$ is the number of detected objects, and each object label and position are encoded in a vector of length 6 that include the object category, bounding box coordinates, and confidence score. Some other tasks like keypoint detection and OCR are similar to object detection. For semantic segmentation, the prediction $y \in \mathbb{R}^{H \times W}$ is per-pixel classification, where $H$ and $W$ are the height and width of the input image, respectively. Depth and optical flow estimation tasks have similar output structure. 




The adversarial loss functions for object detection and semantic segmentation can be defined using their respective training or prediction loss functions. 
Let us consider a whitebox model $f$ and an input image $x$ with output $y = f(x)$. For untargeted attack, we can search for the adversarial example $x^\star$ by solving the following maximization problem: 
\begin{equation}\label{eq:untargeted}
    x^\star = \argmax_{x} ~ \mathcal{L}(f(x), y),
\end{equation}
where $\mathcal{L}(f(x),y)$ represents the training loss of the model with input $x$ and output $y$. 
For targeted attacks, with a target output $y^\star$, we solve the following minimization problem: 
\begin{equation}\label{eq:targeted}
    x^\star = \argmin_{x} ~ \mathcal{L}(f(x), y^\star).
\end{equation}


Different from classification, which mostly use cross-entropy loss across different models, dense predictions have different loss functions for different models due to the complexity of the output space and diversity of the architectures. For example, two-stage object detector, including Faster RCNN \cite{ren2015faster}, has losses for object classification, bounding box regression, and losses on the region proposal network (RPN). But for one-stage object detectors like YOLO \cite{Redmon2016YOLO,redmon2018yolov3}, they do not have losses corresponding to RPN. Due to the large variability of the loss functions used in different dense prediction models, we use the corresponding training loss $\mathcal{L}$ for each model as the optimization loss to guide the backpropagation. 


We employ PGD\cite{madry2017towards} to optimize the perturbation as 
\begin{equation}\label{eq:PGD}
    \delta^{t+1} = \Pi_{\varepsilon} \left( \delta^{t} - \lambda~ \mathbf{sign} (\nabla_{\delta}\mathcal{L}(f(x+\delta^t), y^\star)) \right),
\end{equation}
for targeted attack and
\begin{equation}\label{eq:PGD-untargeted}
    \delta^{t+1} = \Pi_{\varepsilon} \left( \delta^{t} + \lambda~ \mathbf{sign} (\nabla_{\delta}\mathcal{L}(f(x+\delta^t), y)) \right),
\end{equation}
for untargeted attack. Here $t$ indicates the attack step, $\lambda$ is the step size, and $\Pi_{\varepsilon}$ projects the perturbation into a $\ell_p$ norm ball with radius $\varepsilon$. In the rest of the paper,  we focus on targeted attacks without loss of generalization.



\subsection{Ensemble-based attacks}

In an ensemble-based transfer attack, we use an ensemble of $N$ surrogate (whitebox) models: $\mathcal{F} = \{f_1,\ldots, f_N\}$ to generate perturbations to attack the victim model $f_\vv$. Note that if the ensemble has a single model, then such an attack becomes a simple transfer attack with a single surrogate model. 
Let us denote the training loss function for $i$th model as $\mathcal{L}_i(f_i(x),y^*)$. A natural approach to combine the loss functions of all surrogate models is to compute an average or weighted average of the individual loss functions. For instance, we can generate the adversarial image by solving the following optimization problem: 
\begin{equation}\label{eq:ensemble-loss}
x^\star(\boldsymbol{\alpha}) = \argmin_{x} \sum_{i=1}^N \alpha_i\mathcal{L}_i(f_i(x),y^\star), 
\end{equation}
where $x^\star(\boldsymbol{\alpha})$ is a function of the weights of the ensemble ${\boldsymbol{\alpha} = \{\alpha_1,\ldots, \alpha_N\}}$. 
One of our key observations is that the choice of weights plays a critical role in the transfer attack success rate of the ensemble models. 





\input{figures/fig-obj-loss-imbalance.tex}

\noindent\textbf{Weight balancing (victim model agnostic).} In ensemble-based transfer attacks, we build on the intuition that if an adversarial example can fool all models simultaneously, it would potentially be more transferable to any unseen victim model. This concept has been empirically corroborated by numerous works \cite{liu2017delving,dong2018boosting}. However, most attack methods have only been verified on classification models, all of which use the same cross-entropy loss and yield similar loss values. In contrast, the loss functions for object detectors in an ensemble can differ significantly and cover a large range of values (as shown in \cref{fig:obj-loss-imbalance}). In such cases, models with large loss terms heavily influence the optimization procedure, reducing the attack success rate for models with small losses (see~\cref{tab:obj-ablation}). To overcome this issue, we propose a simple yet effective solution to balance the weights assigned to each model in the ensemble as follows. 
For each input image $x$ and target output $y^\star$, we adjust the weight for $i$th surrogate model loss as 
\begin{equation}
\alpha_i = \frac{ \sum_{i=1}^N \mathcal{L}_i(f_i(x), y^\star)}{N \mathcal{L}_i(f_i( x), y^\star)}. \label{eq:weight-balancing}
\end{equation}
The weights are adjusted in a whitebox setting as it allows us to measure the loss of each whitebox model accurately. The purpose of weight balancing is to ensure that all surrogate models can be successfully attacked, making the generated example more adversarial for blackbox victim models. 





\noindent\textbf{Weight optimization (victim model specific).} Note that the weight normalization, as discussed above, is agnostic to the victim model. We further observe that such transfer-based attacks can be further improved by optimizing the weights of the ensemble according to the victim model, input image, and target output. In particular, we can change the individual $\alpha_i$ to create the perturbations that reduce the victim model loss $\mathcal{L}_\vv$. To achieve this goal, we need to solve the following optimization problem with respect to $\boldsymbol{\alpha}$: 
\begin{equation}\label{eq:weight-optimization}
    \boldsymbol{\alpha}^\star = \argmin_{\boldsymbol{\alpha}} ~ \mathcal{L}_\vv(f_\vv(x^\star(\boldsymbol{\alpha})), y^\star).
\end{equation}
The optimization problem in \eqref{eq:weight-optimization} is a nested optimization that we can solve as an alternating minimization routine. \\
\textbf{Step 0.} Given input $x$, output $y^\star$, and surrogate ensemble $\mathcal{F}$, we initialize $\boldsymbol{\alpha}$ using \eqref{eq:weight-balancing}. \\
\textbf{Step 1.} Solve \eqref{eq:ensemble-loss} to generate an adversarial example $x^\star(\boldsymbol{\alpha})$. \\
\textbf{Step 2.} Test the victim model. Stop if attack is successful; otherwise, change one of the $\alpha_i$ and repeat Step 1. 

In our experiments, we update the $\alpha_i$ in a cyclic manner (one coordinate at a time) as $\alpha_i \pm \gamma$ in \textbf{Step 2}, where $\gamma$ denotes a step size. In every round, we select the value of $\alpha_i$ that provides smallest value of the victim loss. We count the number of queries as the number of times we test the generated adversarial example on the victim model and denote it as $Q$ in our experiments. 















