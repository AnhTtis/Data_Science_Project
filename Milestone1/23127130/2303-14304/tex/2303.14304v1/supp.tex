








      

\newpage
\clearpage

\begin{strip} 
    \begin{center} 
        {\Large  \textbf{Ensemble-based Blackbox Attacks on Dense Prediction \\ (Supplementary Material)}}
    \end{center} 
\end{strip} 


\newcommand{\beginsupplement}{%
        \setcounter{table}{0}
        \renewcommand{\thetable}{S\arabic{table}}%
        \setcounter{figure}{0}
        \renewcommand{\thefigure}{S\arabic{figure}}%
     }
\makeatletter
\newcommand\refwithdefault[2]{%
  \@ifundefined{r@#1}{%
    #2%
  }{%
    \ref{#1}%
  }%
}
\makeatother
     






\section*{Summary}
In the supplementary material, we provide additional results and analyses on joint attacks for multiple blackbox models of different dense prediction tasks, attacks on object detection, attacks on semantic segmentation, and corresponding visualization of adversarial examples for qualitative evaluation. We also report runtime and resource usage.


\appendix
\beginsupplement




\section{Joint attack for multiple blackbox models}
In this section, we provide additional visualization results
for joint (targeted) blackbox attacks against object detection and semantic segmentation models. 


\input{figures/fig-joint-examples.tex}

\noindent \textbf{More Visualization of adversarial examples.} We visualize some adversarial examples in \cref{fig:obj-joint-examples}. In \cref{fig:joint-example1}, we show an example where our method generates a single perturbed image to map the bicycle on the right-hand-side to train. In object detection results we see the label for the Bicycle bounding box has been changed to Train, and for the segmentation map, the corresponding region has changed to teal color encoding for Train as well. In \cref{fig:joint-example2}, the generated perturbed image maps the car in the middle to traffic light. Note that the bounding box for the Car in the middle changed to Traffic Light for the object detector and the same area in the semantic segmentation map changed the color to orange (corresponding to Traffic Light label). 

\section{Attacks against object detection}

\input{tables/tab-obj-ablation-coco.tex}

\noindent \textbf{Attacks using different surrogate models.} In our previous experiments (\cref{tab:obj-ablation} and \cref{tab:obj-ablation-coco}), we follow the model selection in \cite{cai2022context} for a fair comparison. We can easily replace the surrogate models with different ones and expect its effectiveness across different settings.
For example, we can replace YOLOv3 with Deformable DETR (denoted as Deform) and get similar results, as shown in \cref{tab:obj-replaceYOLO} below. The experiment setup and victim  models are same as reported in \cref{tab:obj-replaceYOLO} for $\ell_\infty = 20$. 

\input{tables/tab-supp-obj-different-surrogates}

\noindent \textbf{Comparisons with zero-query attacks.} 
Zero-Query attack (ZQA) \cite{cai2022zero} does not rely on any feedback from the victim. It assesses the attack success probability on the surrogate model before launching a single and most promising attack against the victim. Due to these differences in problem setting, we do not directly compare with this method in the main paper. Here we compare the numbers reported from corresponding manuscripts in \cref{tab:obj-replaceYOLO}. ZQA uses a single surrogate model without any feedback from the victim model. It performs worse than the few-query attacks \cite{cai2022context} with 3--5 queries, and our method clearly outperforms both of them. 

\noindent \textbf{Comparison with conventional query-based attacks}. Existing query-based methods, including GARSDC \cite{liang2022large} and PRFA \cite{Liang21Parallel}, require thousands of queries (which is prohibitive) and they are only applicable for untargeted attacks.
Furthermore, their perturbations are clearly visible, see Fig. 5 in \cite{liang2022large}, while our perturbations remain imperceptible. 
\input{tables/tab-supp-obj-compare-conventional}
For these key differences, we did not include their comparison in the main paper, but here we provide a mAP score comparison with them. We use 5 surrogate models from~\cref{tab:obj-ensemble-size} and perform vanishing attacks on ATSS \cite{zhang2020bridging} model, we show in \cref{tab:compare-conventional} that our method can achieve a near-zero mAP within just a few queries ($Q$).

\section{Attacks against semantic segmentation}
\noindent \textbf{Attacks on Pascal VOC dataset.} We generate adversarial attacks using different sizes of ensemble and report mIoU scores on the Pascal VOC dataset in \cref{tab:seg-untar-voc}. Similar to the results on the Cityscapes dataset in~Tab.~\cref{tab:seg-untar-cs}, as we increase the number of surrogate models from 2 to 6, the attack performance improves (indicated by smaller mIoU scores). Attack performance of our method further improves with weight optimization (with $Q=20$). These results show that by adjusting the weights of the surrogate ensemble, we can improve the attack performance. Our attack method with $N=6$ surrogate models provides 27--29\% improvement in mIoU scores compared to DS attack for the victim models \texttt{PSPNet-Res50} and \texttt{DeepLabV3-Res50}. Note that DS attack uses these two models as the whitebox surrogates as well victim models.  In contrast, we keep all four victim models \texttt{PSPNet-Res50, DeepLabV3-Res50, PSPNet-Res101, DeepLabV3-Res101} out of our ensemble. 
Our surrogate ensemble consists of \texttt{FCN, UPerNet, PSANet, GCNe, ANN, EncNet} with \texttt{ResNet50} backbones, which reflects a more realistic setting where the victim blackbox model is different from any of the surrogate models.  








\noindent \textbf{Effect of backbones on attack performance.} 
We note that for VOC dataset results in \cref{tab:seg-untar-voc}, our method provides high attack success for blackbox victim models with \texttt{ResNet50} backbone. However, the attack performance on victim models with \texttt{ResNet101} backbone degrades (as reflected by large mIoU values). To further demonstrate the effectiveness of our attack, we replace the backbones of the surrogate models with the \texttt{ResNet101} backbones while keeping the rest of model architectures same as the original ensemble. Results reported in \cref{tab:seg-untar-voc-r101} show that if we replace surrogate models with \texttt{ResNet101} backbones (same backbone as the victim blackbox models), then our attack method provides significantly better results. 

\noindent \textbf{Attack performance on different backbones.} 
We performed additional experiments using \texttt{FCN} and \texttt{PSPNet} methods and \texttt{MobileNetV2} and \texttt{ResNeSt} (denoted as -mv2 and -s101 in \cref{tab:seg-backbones}) backbones for victim models.
The attack setting corresponds to~Tab.~\cref{tab:seg-targeted}. Due to the great difference in backbones across surrogate and victim, the attack performance drops. Nevertheless, the attack performance improves significantly as we increase the ensemble size and optimize ensemble weights. Results are reported in \cref{tab:seg-backbones}.
\input{tables/tab-rebuttal-seg.tex}


\noindent \textbf{Attack performance on surrogate models.} For the sake of completeness, we also report attack performance on the whitebox surrogate models for both untargeted and targeted attacks in \cref{tab:seg-ensemble-size-untar} and \cref{tab:seg-ensemble-size-tar}. We observe that as we increase the number of models in the ensemble from $N = 1$ to $N = 5$, we can achieve better attack performance on all the whitebox and blackbox victim models we tested. Attacks that are successful on blackbox victim models are almost always successful on all surrogate models.








\input{tables/tab-seg-untar-voc.tex}
\input{tables/tab-seg-untar-voc-r101.tex}


\input{tables/tab-seg-untar-wb.tex}
\input{tables/tab-seg-tar-wb.tex}



\noindent \textbf{Visualization of adversarial examples.} We present some visual examples of untargeted attacks in \cref{fig:seg-untar-examples} and targeted attacks in \cref{fig:seg-tar-examples}. We observe that the attacks generated by surrogate model do not transfer to the victim model for untargeted or targeted cases (i.e., $Q=0$). The attacks generated after weight optimization (i.e., $Q=20$) succeed for untargeted and targeted attacks. 
Our targeted attack setup is visually explained in \cref{fig:seg-tgt-explain}. Instead of mapping every pixel prediction to an arbitrary target label, we focus on attacking a single object $y$ in the original prediction (e.g.``road'' in \cref{fig:tgt-original} with white bounding-box). We select the target label  $y^\star$ as the class that appears most frequently as the least-likely label of the pixels in the selected region. For example, \cref{fig:tgt-ll} shows class ``building'' in grey color as the least likely class in the target region. Finally, we generate attack to replace the entire selected region in the original prediction to its target label (\cref{fig:tgt-target}).

\input{figures/fig-seg-vis-untar.tex}
\input{figures/fig-seg-tgt-explain.tex}
\input{figures/fig-seg-vis-tar.tex}
 
\section{Runtime and resource usage}
We performed experiments on a single RTX 3090 GPU. Average time per query to attack an object detector for a $375\times500$  image with ensemble size $N=\{2,5\}$ is $\{0.5,1\}$sec. Average time per query to attack a segmentation model for a $512\times1024$ image with ensemble size $N=\{2,5\}$ is $\{2.5,5.5\}$sec.







