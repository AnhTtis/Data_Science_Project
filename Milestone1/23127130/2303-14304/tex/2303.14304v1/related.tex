\section{Related work}

\noindent \textbf{Blackbox adversarial attacks.} In the context of blackbox attacks, the attacker cannot access the model parameters or compute the gradient via backpropagation. Blackbox attack methods can be broadly divided into two groups: transfer-based \cite{papernot2016transferability,papernot2017practical,liu2017delving,li2020towards} and query-based attacks \cite{chen2017zoo,ilyas2018black,tu2019autozoom}.
Transfer-based attacks rely on the assumption that surrogate models share similarities with the victim model, such that an adversarial example generated for the surrogate model can also fool the victim model.
Query-based methods generate attacks by searching the adversarial examples space based on the feedback obtained from the victim model through queries. They can often achieve higher success rate but may require a large number of queries. 


\noindent \textbf{Ensemble-based attacks. }
Ensemble-based attacks leverage the idea of transfer attack and assume that if an adversarial example can fool multiple models simultaneously, the chances of fooling an unseen model are higher \cite{liu2017delving,yuan2021meta,dong2018boosting}. 
Recently, some methods have combined ensemble-based transfer attacks with limited feedback from the victim models to improve the overall success rate \cite{guo2019simple,huang2019black,tashiro2020diversity,suya2019hybrid, li2021adversarial,lord2022attacking}. 
These methods have mainly focused on classification models, and ensemble attacks on dense prediction tasks such as object detection and semantic segmentation are relatively less studied, especially for targeted attacks \cite{wu2020making}. 









\noindent \textbf{Attacks against object detectors and segmentation.}
Dense (pixel-level) prediction tasks such as object detection and semantic segmentation have higher task complexities\cite{wang2021pyramid} compared to classification tasks.  Existing attacks on object detectors mainly focus on whitebox setting, although there are a few exceptions \cite{wei2019transferable,cai2022blackbox}. A recent study \cite{cai2022context} generates blackbox attacks on object detectors by using a surrogate ensemble and context-aware attack-based queries. Another approach \cite{wei2019transferable} trains a generative model to generate transferable attacks. While some patch-based attacks \cite{Liu2019Dpatch,saha2020role} are effective, the patches are easily noticeable. 
Recent works \cite{gu2022segpgd,gu2021adversarial} have investigated adversarial robustness for semantic segmentation and proposed a transferable untargeted attack using a single surrogate model. 
While most existing methods are based on a single surrogate model, we demonstrate that using multiple surrogates with weight balancing/search in the attack generation process, we can generate more effective adversarial examples for both untargeted and targeted scenarios, as well as for various types of dense prediction tasks.



