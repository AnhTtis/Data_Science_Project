\section{Experiments}

To evaluate the effectiveness of our method, we performed extensive experiments on attacking various object detection and semantic segmentation models. We first show that the attacks generated by a single surrogate model fail to transfer to arbitrary victim models. Then we show that the attack transfer rate can be increased by using an ensemble with weight balancing. Additional optimization of the weights surrogates for each victim model can further improve the attack performance. Finally, we show that we can generate single perturbations to fool object detectors and semantic segmentation models simultaneously.



\subsection{Experiment setup}



\subsubsection{Object detection}

\noindent\textbf{Models and datasets.} We utilize \texttt{MMDetection}~\cite{mmdetection} toolbox to select various model architectures and weights pre-trained on COCO 2017 dataset \cite{lin2014microsoft}. To construct the surrogate ensemble, we start with two widely used models, \texttt{Faster R-CNN}~\cite{ren2015faster} and \texttt{YOLO}~\cite{Redmon2016YOLO,redmon2018yolov3}, and expand the ensemble by appending models with different architectures, including \{\texttt{FCOS}~\cite{tian2019fcos}, \texttt{Grid R-CNN}~\cite{lu2019grid}, \texttt{SSD}~\cite{liu2016ssd}\}. 
 We select different victim models, including \{\texttt{RetinaNet}~\cite{lin2017focal}, \texttt{Libra R-CNN}~\cite{pang2019libra}, \texttt{FoveaBox}~\cite{kong2020foveabox}, \texttt{FreeAnchor}~\cite{zhang2019freeanchor}, \texttt{DETR}~\cite{carion2020end}\}. 
We evaluate attack performance on COCO 2017 \cite{lin2014microsoft} and Pascal VOC 2007 \cite{everingham2010pascal} datasets. Since the models from this repository are trained on COCO, which contains 80 object categories (a superset of VOC dataset's 20 categories), while testing on VOC dataset, we only return the objects that exist in VOC. We follow the setup in \cite{cai2022context} and randomly select 500 images containing multiple (2--6) objects from VOC 2007 test and COCO 2017 validation sets.


\noindent\textbf{Evaluation metrics.} We mainly focus on targeted attacks for object detection since they are more challenging than untargeted or vanishing attacks. We measure the performance of the attack using attack success rate (ASR), which equals the number of successfully attacked images over the total number of attacks. We follow the setting in \cite{cai2022context}, where if the target label is detected within the victim object region with IOU $> 0.3$, the attack is determined a success.

\noindent\textbf{Perturbation and query budget.} We tested different perturbation levels with $\ell_\infty = \{ 10,20,30\}$ out of $255$. We use at most 10 queries for attacking object detectors, and we show the trends of how ASR increases with the number of queries. To align with \cite{cai2022context} that uses 5 attack plans, we set the maximum query budget to $Q = 5$ in \cref{tab:obj-ablation}. 


\noindent\textbf{Comparing methods.} 
We compare with \cite{cai2022context}, which is a state-of-the-art transfer-based approach that leverages context information to design attack plans to iteratively attack the victim object. The method generates different perturbations by iterating over a set of predefined attacks, and the total number of queries is the number of attempted attacks. BASES \cite{cai2022blackbox} is a recent work on ensemble-based blackbox attacks, which mainly focused on classification tasks and did not consider the loss distributions of different surrogate models. In our experiments, the ensemble with weight optimization and without balancing is equivalent to BASES \cite{cai2022blackbox}.



\input{tables/tab-obj-ablation.tex}
\input{figures/fig-obj-trend.tex}

\subsubsection{Semantic segmentation}

\noindent\textbf{Models and datasets.}
We use \texttt{MMSegmentation}~\cite{mmseg2020} toolbox to select different model architectures and weights pre-trained on Cityscapes \cite{cordts2016cityscapes} ($x \in \mathbb{R}^{512\times1024\times3}$)  and Pascal VOC ($x \in \mathbb{R}^{512\times512\times3}$) datasets.
We select \texttt{PSPNet}~\cite{zhao2017pyramid} and \texttt{DeepLabV3}~\cite{chen2017rethinking} with \texttt{ResNet50} and \texttt{ResNet101} \cite{he2016deep} backbones as our blackbox victim models.
For the surrogate ensemble, we start with the primary semantic segmentation model \texttt{FCN}~\cite{long2015fcn}, and expand the ensemble 
with \{\texttt{UPerNet}~\cite{xiao2018upernet}, \texttt{PSANet}~\cite{zhao2018psanet}, \texttt{GCNet}~\cite{cao2019gcnet}, \texttt{ANN}~\cite{zhu2019ann}, \texttt{EncNet}~\cite{Zhang_2018_encnet}\}. All models are built on \texttt{ResNet50}~\cite{he2016deep} backbone trained with the cross-entropy loss. 
The loss values across all surrogate models have similar range; therefore, the effect of weight balancing for semantic segmentation is not as significant as it is for object detection.  
We use validation datasets from Cityscapes \cite{cordts2016cityscapes} and Pascal VOC 2012, which contains 500 and 1499 images with 19 and 21 classes,  respectively. 


\noindent\textbf{Evaluation metrics.}
We use different metrics for untargeted and targeted attack performance evaluation. In untargeted experiments, the attack performance is evaluated using the mIoU score (in percentage \%), the lower mIoU score the better attack performance. For targeted experiments, we report the pixel success ratio (PSR), which indicates the percentage of pixels successfully assigned the desired label in the target region, the higher the better attack performance. %

\noindent\textbf{Perturbation and query budget.}
We use the perturbation budget $\ell_\infty \leq 8$ out of 255 and 
query budget $Q=20$.


\noindent\textbf{Comparing methods.} 
We compare with dynamic scale (DS) attack \cite{gu2021adversarial} which is the most recent method that achieves the highest attack transfer rate on semantic segmentation untargeted attacks. 






\subsection{Attacks against object detection}
Following settings in \cite{cai2022context}, we randomly select one object from the output of victim model as the victim object and perturb it into a target object that does not exist in the original detection. This approach rules out the possibility of mis-counting existing objects as the target object. 

\input{tables/tab-obj-ensemble-size.tex}

\input{tables/tab-seg-untar-cs.tex}
\input{tables/tab-seg-tar.tex}

We report our main results in \cref{tab:obj-ablation}. 
The baseline method uses a surrogate ensemble without weight balancing and models are assigned weight of 1. 
Such a baseline method is same a transfer-based method and results in highly imbalanced success rate for different surrogate models. For instance, at $\ell_\infty \leq 10$, the success rate for \texttt{YOLOv3} is above 90\% while the success rate for \texttt{Faster R-CNN} is less than 30\%. Low success rate on surrogate side translates to low success rate on blackbox victim side. The main reason for such imbalance is that the loss of different object detectors can be highly unbalanced (e.g., the loss value for \texttt{YOLOv3} is nearly 60$\times$ larger than the loss of \texttt{Faster RCNN} for targeted attacks, \cf \cref{fig:obj-loss-imbalance}). 
With weight balancing, the success rate increases for surrogate and blackbox victim models. The success rate is further increased on surrogate and victim blackbox models if we optimize the weights, same as BASES \cite{cai2022blackbox}.
Our method (with weight balancing and optimization) achieves a significantly higher ASR compared to context-aware attack across different datasets and different perturbation budgets. On average, our ASR on blackbox victim models is over $4\times$ better than baseline method and over $1.5\times$ better than context-aware attack. On whitebox surrogate models, weight balancing and optimization also achieves the highest ASR. Context-aware attack fixes weight ratio for surrogate models, $\nicefrac{\alpha_{\text{FRCNN}}}{\alpha_{\text{YOLO}}}  = 4$, which is sub-optimal according to our analyses. Even though it achieves much higher performance than baseline, it still largely under-performs our method. 
Similar trend is observed for COCO dataset (see~\cref{tab:obj-ablation-coco}). 


\cref{fig:obj-trend} shows the effect of the number of queries on the ASR that gradually improves as we optimize the weights. We observe the largest increase in the first two steps and then the improvement plateaus as $Q\rightarrow10$.








We also conducted an experiment to test our method with varying ensemble sizes. The results for $\ell_\infty \le 10$, $Q=5$ are presented in \cref{tab:obj-ensemble-size}. As we increase the number of models in the ensemble from $N=1$ to $N=5$, we observe an increased ASR on all blackbox victim models. 



\subsection{Attacks against semantic segmentation}
We evaluate the effectiveness of our attack on semantic segmentation in both untargeted and targeted settings. 
For the sake of consistency and a fair comparison, we adopt adversarial attack settings in DS attack \cite{gu2021adversarial}. 

\input{figures/fig-seg-trend.tex}

\noindent \textbf{Untargeted attacks.}
We generate adversarial attacks using different ensemble sizes and report mIoU scores on Cityscapes in \cref{tab:seg-untar-cs} and \cref{fig:seg-trend} (and Pascal VOC in supplementary material).
In the untargeted setting, semantic segmentation models are attacked to maximize the loss between clean and modified annotation; hence, the lower mIoU implies better attack performance. All of the victim models achieve high performance on clean images. 
The baseline method (direct transfer attack with one surrogate model using PGD) performs well in the whitebox setting but suffers when the victim uses another backbone. For example, the attacks generated on \texttt{PSPNet-Res50} achieves 3.43\% mIoU on \texttt{PSPNet-Res50} but only attains 24.18\% mIoU on \texttt{PSPNet-Res101}. DS attack achieves better results than the baseline method but still suffers from cross-backbone transfers. 
On the other hand, our method, without weight optimization (\ie, $Q=0$) and using a surrogate ensemble of $N=2$ models, can achieve results comparable to DS attack, particularly for attacks on Res101 models. As we increase the number of surrogate models to 4 or 6, our attack performance further improves. 
Furthermore, when we apply weight optimization (\eg, $Q=20$), the attack improves by updating the weights of the surrogate models, allowing us to outperform DS attack for all victim models. 
\cref{fig:seg-trend} shows how the mIoU changes with the number of queries. We observe that the mIoU gradually reduces as we query the victim model and optimize the weights. The largest decrease happens in the first 3--4 steps and then the reduction plateaus as $Q\rightarrow20$.






\input{figures/fig-joint-curves-supp.tex}
\input{figures/fig-joint-attack.tex}








\noindent \textbf{Targeted attack.}
To evaluate our method in a more challenging setting, we consider a targeted attack scenario, where instead of changing every pixel in the segmentation to some arbitrary label, we focus on attacking a dominant class (\ie, the class occupying the largest area) in the scene to its least likely class $y^\star$. For each clean image, we first select a region with the dominant class $y$ (\eg, ``road'' or ``building'' for most of the Cityscapes images. See~\cref{fig:seg-tgt-explain} as an example). Then based on the least-likely class of each pixel in that region,
we select the class that appears most frequently as the target label $y^\star$ of the entire region.
We use PSR as our evaluation metric, which represents the percentage of pixels in the selected region that are successfully assigned to $y^\star$. The higher percentage indicates more pixels are successfully attacked to the desired class, which indicates better attack performance. %
Our targeted attack results are reported in \cref{tab:seg-targeted}.
Results show that as we increase the number of surrogate models $(N)$, the ASR improves for most instances without any weight optimization step (i.e., $Q=0$). If we perform weight optimization for $Q=20$ steps, then the success rate increases for all the models. For instance, with $N=4$, the ASR for Res101 models increases from 17--26\% to 63-64\%. 




\subsection{Joint attack for multiple models and tasks}
We first show that generally adversarial examples generated for object detection do not transfer to semantic segmentation, and vice versa. Then we show that we can generate single perturbations to fool object detectors and semantic segmentation models simultaneously, by using a surrogate ensemble including both detection and segmentation models. We choose targeted attacks in our experiments because they are more challenging than untargeted attacks. 

\noindent \textbf{Experiment setup.} On the blackbox (victim) side, we tested \texttt{RetinaNet} as the victim object detector and \texttt{PSPNet-Res50} as the victim semantic segmentation model. On the whitebox (surrogate) side, we used \texttt{Faster RCNN, YOLOv3} as the surrogate object detectors and \texttt{FCN, UPerNet} as the surrogate semantic segmentation models. We performed targeted attacks on 500 test images selected from the validation set of CityScapes dataset.

\noindent \textbf{Results.} We present the ASRs for task-specific and joint attacks in \cref{fig:obj-joint-curves}. Green curves denote ASR for object detectors, and blue curves denote PSR for semantic segmentation. 
\cref{fig:sub-joint-det} presents the results when we generate attacks using an object detector surrogate ensemble. Note that success rate for victim object detector (\texttt{RetinaNet}) increases as we optimize the weights but the  success rate for the semantic segmentation model (\texttt{PSPNet}) remains small. 
Similarly, \cref{fig:sub-joint-seg} presents the results when we generate attacks using a segmentation surrogate ensemble. The success rate for the victim semantic segmentation model increases, but the success rate for the object dector remains close to zero. 
\cref{fig:sub-joint-all} presents the results when we perform a joint attack using an ensemble that consists of both object detectors and segmentation models. The blackbox ASR is high on both detection and segmentation \cref{fig:sub-joint-all}, and the attack performance improves as we update the weights of the surrogate models. In \cref{fig:sub-joint-all}, we show the results for different perturbation budgets, with $\ell_\infty \leq 10$, the success rates on detection and segmentation are between $60\%-70\%$, which are close to in-domain detection attacks in \cref{fig:sub-joint-det} and in-domain segmentation attacks \cref{fig:sub-joint-seg}. When we increase the perturbation to $\ell_\infty \leq 20$, the success rate for both detection and segmentation can surpass $80\%$.

\noindent \textbf{Visualization of adversarial examples.} 
In this example, our goal is to perturb the car in the middle to a traffic light. We assign the target label for car region to traffic light. \cref{fig:obj-joint-attack} shows the results where a single adversarial image generated by the surrogate model can successfully fool the blackbox models \texttt{RetinaNet} and \texttt{PSPNet}. 