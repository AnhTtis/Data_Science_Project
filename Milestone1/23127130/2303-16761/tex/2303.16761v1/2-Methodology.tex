\section{Methodology}

\begin{figure}
    \centering
    \includegraphics[width=0.9\linewidth]{model_architecture.pdf}
    \caption{The architecture of our proposed approach.}
    \label{fig:model_architecture}
\end{figure}
In this section, we describe how our dialogue-to-video retrieval system works. Our retrieval system consists of two major components: 1) a \textbf{\textit{temporal-aware video encoder}} responsible for encoding the image frames in video with temporal information. 2) a \textbf{\textit{dialogue-query encoder}} responsible for encoding the dialogue query with conversational information. As shown in Figure~\ref{fig:model_architecture}, our model receives video-query pairs and produces similarity scores. Each video consists of $n$ frames: $V = \{f_{1}, f_{2},......,f_{n}\}$ and each dialogue query is composed of $m$ turns of conversation: $D = \{d_{1},d_{2},......,d_{m}\}$.

In the \textit{video encoder}, we encode each frame $f_{i}$ to its visual representation $f_{i}^{h}$. Then we incorporate temporal information to the corresponding frame representation and feed them into a stacked \textsc{Multi-Head-Attention} module, yielding temporal frame representation $f_{i}^{h^{'}}$. In the \textit{dialogue-query encoder}, we sequentially encode $D$ by letting $d_{i}^{h}=\textsc{Text-Encoder}(d_{i-1}^{h}, d_{i})$ in order to produce a dialogue-history-aware dialogue representation. We then obtain the final dialogue-query representation by fusing all $d_{i}^{h}$: $D^{h} = g(d_{1}^{h},......,d_{m}^{h})$ where $g$ represents our fusion function. After obtaining $D^{h}$, we use it to calculate similarities with each frame $f_{i}^{h^{'}}$, which are then used to obtain a video representation $V^{h}$ based on the weighted summation of all $f_{i}^{h^{'}}$. Finally, we obtain the dialogue-to-video similarity score using the dot-product between $D^{h}$ and $V^{h}$.


\subsection{Temporal-aware Video Encoder}

Our \textit{temporal-aware video encoder}, which is built on Vision Transformer~\cite{dosovitskiy2020image} %based on Transformer~\cite{Vaswani:2017:AYN:3295222.3295349}, 
firstly encodes each frame $f_{i}$ to its visual representation:

\begin{equation}
    f_{i}^{h} = \textsc{Image-Encoder}(f_{i})
\end{equation}

Then we inject the positional information of the corresponding frame in the video to the frame representation and feed it to the \textsc{Multi-Head-Attention} module:

\begin{equation}
    f_{i}^{h'} = \textsc{Multi-Head-Attention}([f_{1}^{p},......,f_{n}^{p}])
\end{equation}
where $f_{i}^{p}$ is the frame representation with positional information $f_{i}^{p}=\psi(f_{i}^{h}, p_{i})$ and $p_{i}$ is the corresponding positional embedding. Practically, we add \textit{absolute} positional embedding vectors to frame representation as in BERT~\cite{bert}: $f_{i}^{p}=f_{i}^{h} + p_{i}$.
Finally, we obtain the temporal-aware video representation $V^{h'} = \{f_{1}^{h'},......,f_{n}^{h'}\}$.


\subsection{Dialogue-query Encoder}

The dialogue-query encoder is responsible for encoding the dialogue-query $D = \{d_{1},d_{2},......,d_{m}\}$:

\begin{equation}
    d_{i}^{h}=\textsc{Text-Encoder}(d_{i-1}^{h}, d_{i})
\end{equation}

where \textsc{Text-Encoder} is a Transformer-based encoder model~\cite{vaswani2017transformer,bert,radford2021learning_clip} in our experiments. Then we fuse all $d_{i}^{h}$ to obtain a dialogue-level representation $D^{h}$ for the dialogue-query:

\begin{equation}
    D^{h} = g(d_{1}^{h},......,d_{m}^{h})
\end{equation}

\subsection{Interaction between Video and Dialogue-query}
To calculate the similarity score between each $V$ and $D$, we firstly compute the similarity scores between dialogue-query $D^{h}$ and each frame $f_{i}^{h'}$. Then we obtain a weighted summation of all frames $f_{i}^{h'}$ as the video representation $V^{h}$:

\begin{equation}
    V^{h} = \sum_{i=1}^{n} c_{i}f_{i}^{h}
\end{equation}

\begin{equation}
    c_{i} = \dfrac{e^{\phi(D^{h}, f_{i}^{h})}}{\sum\limits_{j=1}^{n}e^{\phi(D^{h}, f_{j}^{h})}}
\end{equation}

The final similarity score is obtained by dot-product between $D^{h}$ and $V^{h}$: $s=D^{h}(V^{h})^{T}$
\subsection{Training Objective}

We perform in-batch contrastive learning~\cite{karpukhin2020dense,gao2021simcse}. For a batch of $N$ video-dialogue pairs $\{(V_{1}, D_{1}),......,(V_{N}, D_{N})\}$, the dialogue-to-video and video-to-dialogue match loss are:

\begin{equation}
    L_{d2v} = -\dfrac{1}{N}\sum_{i=1}^{N}\frac{e^{D_{i}^{h}(V_{i}^{h})^{T}}}{\sum\limits_{j=1}^{N}e^{D_{i}^{h}(V_{j}^{h})^{T}}}
\end{equation}

\begin{equation}
    L_{v2d} = -\dfrac{1}{N}\sum_{i=1}^{N}\frac{e^{D_{i}^{h}(V_{i}^{h})^{T}}}{\sum\limits_{j=1}^{N}e^{D_{j}^{h}(V_{i}^{h})^{T}}}
\end{equation}

The overall loss to be minimized during the training process is $L = (L_{d2v}+L_{v2d})/2$.