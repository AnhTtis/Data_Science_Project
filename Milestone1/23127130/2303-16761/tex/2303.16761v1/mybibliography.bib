
@String(PAMI = {IEEE Trans. Pattern Anal. Mach. Intell.})
@String(IJCV = {Int. J. Comput. Vis.})
@String(CVPR= {IEEE Conf. Comput. Vis. Pattern Recog.})
@String(ICCV= {Int. Conf. Comput. Vis.})
@String(ECCV= {Eur. Conf. Comput. Vis.})
@String(NIPS= {Adv. Neural Inform. Process. Syst.})
@String(ICPR = {Int. Conf. Pattern Recog.})
@String(BMVC= {Brit. Mach. Vis. Conf.})
@String(TOG= {ACM Trans. Graph.})
@String(TIP  = {IEEE Trans. Image Process.})
@String(TVCG  = {IEEE Trans. Vis. Comput. Graph.})
@String(TMM  = {IEEE Trans. Multimedia})
@String(ACMMM= {ACM Int. Conf. Multimedia})
@String(ICME = {Int. Conf. Multimedia and Expo})
@String(ICASSP=	{ICASSP})
@String(ICIP = {IEEE Int. Conf. Image Process.})
@String(ACCV  = {ACCV})
@String(ICLR = {Int. Conf. Learn. Represent.})
@String(IJCAI = {IJCAI})
@String(PR   = {Pattern Recognition})
@String(AAAI = {AAAI})
@String(CVPRW= {IEEE Conf. Comput. Vis. Pattern Recog. Worksh.})
@String(CSVT = {IEEE Trans. Circuit Syst. Video Technol.})

@String(SPL	= {IEEE Sign. Process. Letters})
@String(VR   = {Vis. Res.})
@String(JOV	 = {J. Vis.})
@String(TVC  = {The Vis. Comput.})
@String(JCST  = {J. Comput. Sci. Tech.})
@String(CGF  = {Comput. Graph. Forum})
@String(CVM = {Computational Visual Media})


@String(PAMI  = {IEEE TPAMI})
@String(IJCV  = {IJCV})
@String(CVPR  = {CVPR})
@String(ICCV  = {ICCV})
@String(ECCV  = {ECCV})
@String(NIPS  = {NeurIPS})
@String(ICPR  = {ICPR})
@String(BMVC  =	{BMVC})
@String(TOG   = {ACM TOG})
@String(TIP   = {IEEE TIP})
@String(TVCG  = {IEEE TVCG})
@String(TCSVT = {IEEE TCSVT})
@String(TMM   =	{IEEE TMM})
@String(ACMMM = {ACM MM})
@String(ICME  =	{ICME})
@String(ICASSP=	{ICASSP})
@String(ICIP  = {ICIP})
@String(ACCV  = {ACCV})
@String(ICLR  = {ICLR})
@String(IJCAI = {IJCAI})
@String(PR = {PR})
@String(AAAI = {AAAI})
@String(CVPRW= {CVPRW})
@String(CSVT = {IEEE TCSVT})

@inproceedings{bendersky2014up,
  title={Up next: retrieval methods for large scale related video suggestion},
  author={Bendersky, Michael and Garcia-Pueyo, Lluis and Harmsen, Jeremiah and Josifovski, Vanja and Lepikhin, Dima},
  booktitle={Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining},
  pages={1769--1778},
  year={2014}
}

 @inproceedings{alamri2019audiovisual,
             		 title={Audio-Visual Scene-Aware Dialog},
              		author={Huda Alamri and Vincent Cartillier and Abhishek Das and Jue Wang and Anoop Cherian and Irfan Essa and Dhruv Batra and Tim K. Marks and Chiori Hori and Peter Anderson and Stefan Lee and Devi Parikh},
              		booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
              		year={2019}
 			 } 
 			 
@inproceedings{zheng2022MMChat,
  author    = {Zheng, Yinhe and Chen, Guanyi and Liu, Xin and Sun, Jian},
  title     = {MMChat: Multi-Modal Chat Dataset on Social Media},
  booktitle = {Proceedings of The 13th Language Resources and Evaluation Conference},
  year      = {2022},
  publisher = {European Language Resources Association},
}

@inproceedings{he2021improving,
  title={Improving video retrieval by adaptive margin},
  author={He, Feng and Wang, Qi and Feng, Zhifan and Jiang, Wenbin and L{\"u}, Yajuan and Zhu, Yong and Tan, Xiao},
  booktitle={Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval},
  pages={1359--1368},
  year={2021}
}

@article{madasu2022learning,
  title={Learning to Retrieve Videos by Asking Questions},
  author={Madasu, Avinash and Oliva, Junier and Bertasius, Gedas},
  journal={arXiv preprint arXiv:2205.05739},
  year={2022}
}

@inproceedings{maeoki2020interactive,
  title={Interactive video retrieval with dialog},
  author={Maeoki, Sho and Uehara, Kohei and Harada, Tatsuya},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops},
  pages={952--953},
  year={2020}
}

@inproceedings{zhang2021personalized,
  title={Personalized Multi-modal Video Retrieval on Mobile Devices},
  author={Zhang, Haotian and Jepson, Allan D and Mohomed, Iqbal and Derpanis, Konstantinos G and Zhang, Ran and Fazly, Afsaneh},
  booktitle={Proceedings of the 29th ACM International Conference on Multimedia},
  pages={1185--1191},
  year={2021}
}

@inproceedings{covington2016deep,
  title={Deep neural networks for youtube recommendations},
  author={Covington, Paul and Adams, Jay and Sargin, Emre},
  booktitle={Proceedings of the 10th ACM conference on recommender systems},
  pages={191--198},
  year={2016}
}

@inproceedings{mithun2018learning,
  title={Learning joint embedding with multimodal cues for cross-modal video-text retrieval},
  author={Mithun, Niluthpol Chowdhury and Li, Juncheng and Metze, Florian and Roy-Chowdhury, Amit K},
  booktitle={ICMR},
  year={2018}
}

@InProceedings{dialogue_to_video_ecir,
author="Lyu, Chenyang
and Nguyen, Manh-Duy
and Ninh, Van-Tu
and Zhou, Liting
and Gurrin, Cathal
and Foster, Jennifer",
editor="Kamps, Jaap
and Goeuriot, Lorraine
and Crestani, Fabio
and Maistro, Maria
and Joho, Hideo
and Davis, Brian
and Gurrin, Cathal
and Kruschwitz, Udo
and Caputo, Annalina",
title="Dialogue-to-Video Retrieval",
booktitle="Advances in Information Retrieval: 45th European Conference on Information Retrieval, ECIR 2023, Dublin, Ireland, April 2--6, 2023, Proceedings, Part II",
year="2023",
publisher="Springer Nature Switzerland",
address="Cham",
pages="493--501",
abstract="Recent years have witnessed an increasing amount of dialogue/conversation on the web especially on social media. That inspires the development of dialogue-based retrieval, in which retrieving videos based on dialogue is of increasing interest for recommendation systems. Different from other video retrieval tasks, dialogue-to-video retrieval uses structured queries in the form of user-generated dialogue as the search descriptor. We present a novel dialogue-to-video retrieval system, incorporating structured conversational information. Experiments conducted on the AVSD dataset show that our proposed approach using plain-text queries improves over the previous counterpart model by 15.8{\%} on R@1. Furthermore, our approach using dialogue as a query, improves retrieval performance by 4.2{\%}, 6.2{\%}, 8.6{\%} on R@1, R@5 and R@10 and outperforms the state-of-the-art model by 0.7{\%}, 3.6{\%} and 6.0{\%} on R@1, R@5 and R@10 respectively.",
isbn="978-3-031-28238-6"
}



@article{miech2018learning,
  title={Learning a text-video embedding from incomplete and heterogeneous data},
  author={Miech, Antoine and Laptev, Ivan and Sivic, Josef},
  journal={arXiv:1804.02516},
  year={2018}
}

@article{liu2019use,
  title={Use what you have: Video retrieval using representations from collaborative experts},
  author={Liu, Yang and Albanie, Samuel and Nagrani, Arsha and Zisserman, Andrew},
  journal={arXiv:1907.13487},
  year={2019}
}

@inproceedings{gabeur2020multi,
  title={Multi-modal transformer for video retrieval},
  author={Gabeur, Valentin and Sun, Chen and Alahari, Karteek and Schmid, Cordelia},
  booktitle={ECCV},
  year={2020},
}

@inproceedings{wang2021t2vlad,
  title={{T2VLAD}: global-local sequence alignment for text-video retrieval},
  author={Wang, Xiaohan and Zhu, Linchao and Yang, Yi},
  booktitle={CVPR},
  year={2021}
}

@inproceedings{gao2021simcse,
   title={{SimCSE}: Simple Contrastive Learning of Sentence Embeddings},
   author={Gao, Tianyu and Yao, Xingcheng and Chen, Danqi},
   booktitle={Empirical Methods in Natural Language Processing (EMNLP)},
   year={2021}
}

@inproceedings{karpukhin2020dense,
   title={Dense Passage Retrieval for Open-Domain Question Answering},
   author={Karpukhin, Vladimir and OÄŸuz, Barlas and Min, Sewon and Lewis, Patrick and Wu, Ledell and Edunov, Sergey and Chen, Danqi and Yih, Wen-tau},
   booktitle={Empirical Methods in Natural Language Processing (EMNLP)},
   year={2020}
}

@InProceedings{bain2021frozen,
  author       = "Max Bain and Arsha Nagrani and G{\"u}l Varol and Andrew Zisserman",
  title        = "Frozen in Time: A Joint Video and Image Encoder for End-to-End Retrieval",
  booktitle    = "IEEE International Conference on Computer Vision",
  year         = "2021",
}

@article{luo2021clip4clip,
  title={CLIP4Clip: An Empirical Study of CLIP for End to End Video Clip Retrieval and Captioning},
  author={Luo, Huaishao and Ji, Lei and Zhong, Ming and Chen, Yang and Lei, Wen and Duan, Nan and Li, Tianrui},
  journal={Neurocomputing},
  year={2022},
  publisher={Elsevier}
}

@article{fang2021clip2video,
  title={{CLIP2Video}: Mastering Video-Text Retrieval via Image {CLIP}},
  author={Fang, Han and Xiong, Pengfei and Xu, Luhui and Chen, Yu},
  journal={arXiv:2106.11097},
  year={2021}
}

@inproceedings{xu2016msr,
  title={{MSR-VTT}: A large video description dataset for bridging video and language},
  author={Xu, Jun and Mei, Tao and Yao, Ting and Rui, Yong},
  booktitle={CVPR},
  year={2016}
}

@inproceedings{wang2019vatex,
  title={{VATEX}: A large-scale, high-quality multilingual dataset for video-and-language research},
  author={Wang, Xin and Wu, Jiawei and Chen, Junkun and Li, Lei and Wang, Yuan-Fang and Wang, William Yang},
  booktitle={ICCV},
  year={2019}
}

@inproceedings{chen2011collecting,
  title={Collecting highly parallel data for paraphrase evaluation},
  author={Chen, David and Dolan, William B},
  booktitle={ACL},
  year={2011}
}

@inproceedings{zhang2018cross,
  title={Cross-modal and hierarchical modeling of video and text},
  author={Zhang, Bowen and Hu, Hexiang and Sha, Fei},
  booktitle={ECCV},
  year={2018}
}

@inproceedings{croitoru2021teachtext,
  title={{TeachText}: Crossmodal generalized distillation for text-video retrieval},
  author={Croitoru, Ioana and Bogolin, Simion-Vlad and Leordeanu, Marius and Jin, Hailin and Zisserman, Andrew and Albanie, Samuel and Liu, Yang},
  booktitle={ICCV},
  year={2021}
}

@article{radford2021learning,
  title={Learning transferable visual models from natural language supervision},
  author={Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and others},
  journal={arXiv:2103.00020},
  year={2021}
}

@article{krizhevsky2017imagenet-alexnet,
  title={Imagenet classification with deep convolutional neural networks},
  author={Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
  journal={Communications of the ACM},
  volume={60},
  number={6},
  pages={84--90},
  year={2017},
  publisher={AcM New York, NY, USA}
}

@article{lecun1998gradient-lenet,
  title={Gradient-based learning applied to document recognition},
  author={LeCun, Yann and Bottou, L{\'e}on and Bengio, Yoshua and Haffner, Patrick},
  journal={Proceedings of the IEEE},
  volume={86},
  number={11},
  pages={2278--2324},
  year={1998},
  publisher={Ieee}
}

@article{andres2021straightforward,
  title={A Straightforward Framework For Video Retrieval Using {CLIP}},
  author={Andr{\'e}s Portillo-Quintero, Jes{\'u}s and Ortiz-Bayliss, Jos{\'e} Carlos and Terashima-Mar{\'\i}n, Hugo},
  journal={arXiv:2102.12443},
  year={2021}
}

@inproceedings{alayrac2020self,
  title={Self-Supervised MultiModal Versatile Networks.},
  author={Alayrac, Jean-Baptiste and Recasens, Adria and Schneider, Rosalia and Arandjelovic, Relja and Ramapuram, Jason and De Fauw, Jeffrey and Smaira, Lucas and Dieleman, Sander and Zisserman, Andrew},
  booktitle={NeurIPS},
  year={2020}
}

@article{liu2021hit,
  title={{HiT}: Hierarchical transformer with momentum contrast for video-text retrieval},
  author={Liu, Song and Fan, Haoqi and Qian, Shengsheng and Chen, Yiru and Ding, Wenkui and Wang, Zhongyuan},
  journal={arXiv:2103.15049},
  year={2021}
}

@inproceedings{wang2016learning,
  title={Learning deep structure-preserving image-text embeddings},
  author={Wang, Liwei and Li, Yin and Lazebnik, Svetlana},
  booktitle={CVPR},
  year={2016}
}

@article{patrick2020support,
  title={Support-set bottlenecks for video-text representation learning},
  author={Patrick, Mandela and Huang, Po-Yao and Asano, Yuki and Metze, Florian and Hauptmann, Alexander and Henriques, Joao and Vedaldi, Andrea},
  journal={arXiv:2010.02824},
  year={2020}
}


@inproceedings{Arandjelovic2017LookLA,
	title={Look, Listen and Learn},
	author={Relja Arandjelovic and Andrew Zisserman},
	booktitle={ICCV},
	year={2017}
}


@inproceedings{zhu2020actbert,
  title={{ActBERT}: Learning global-local video-text representations},
  author={Zhu, Linchao and Yang, Yi},
  booktitle={CVPR},
  year={2020}
}

@inproceedings{gabeur2022masking,
  title={Masking Modalities for Cross-modal Video Retrieval},
  author={Gabeur, Valentin and Nagrani, Arsha and Sun, Chen and Alahari, Karteek and Schmid, Cordelia},
  booktitle={WACV},
  year={2022}
}

@article{li2021clip,
  title={A {CLIP-Enhanced} Method for Video-Language Understanding},
  author={Li, Guohao and He, Feng and Feng, Zhifan},
  journal={arXiv:2110.07137},
  year={2021}
}

@article{gao2021clip2tv,
  title={{CLIP2TV}: An Empirical Study on Transformer-based Methods for Video-Text Retrieval},
  author={Gao, Zijian and Liu, Jingyu and Chen, Sheng and Chang, Dedan and Zhang, Hao and Yuan, Jinwei},
  journal={arXiv:2111.05610},
  year={2021}
}

@article{cheng2021improving,
  title={Improving Video-Text Retrieval by Multi-Stream Corpus Alignment and Dual Softmax Loss},
  author={Cheng, Xing and Lin, Hezheng and Wu, Xiangyu and Yang, Fan and Shen, Dong},
  journal={arXiv:2109.04290},
  year={2021}
}

@inproceedings{dzabraev2021mdmmt,
  title={{MDMMT}: Multidomain multimodal transformer for video retrieval},
  author={Dzabraev, Maksim and Kalashnikov, Maksim and Komkov, Stepan and Petiushko, Aleksandr},
  booktitle={CVPR},
  year={2021}
}

@inproceedings{kamath2021mdetr,
  title={{MDETR-modulated} detection for end-to-end multi-modal understanding},
  author={Kamath, Aishwarya and Singh, Mannat and LeCun, Yann and Synnaeve, Gabriel and Misra, Ishan and Carion, Nicolas},
  booktitle={ICCV},
  year={2021}
},

@inproceedings{li2020oscar,
  title={Oscar: Object-semantics aligned pre-training for vision-language tasks},
  author={Li, Xiujun and Yin, Xi and Li, Chunyuan and Zhang, Pengchuan and Hu, Xiaowei and Zhang, Lei and Wang, Lijuan and Hu, Houdong and Dong, Li and Wei, Furu and others},
  booktitle={ECCV},
  year={2020},
}

@inproceedings{dong2021dual,
  title={Dual encoding for video retrieval by text},
  author={Dong, Jianfeng and Li, Xirong and Xu, Chaoxi and Yang, Xun and Yang, Gang and Wang, Xun and Wang, Meng},
  booktitle={PAMI},
  year={2021},
}


@inproceedings{yu2018joint,
  title={A joint sequence fusion model for video question answering and retrieval},
  author={Yu, Youngjae and Kim, Jongseok and Kim, Gunhee},
  booktitle={ECCV},
  year={2018}
}

@inproceedings{sun2019videobert,
  title={{VideoBERT}: A joint model for video and language representation learning},
  author={Sun, Chen and Myers, Austin and Vondrick, Carl and Murphy, Kevin and Schmid, Cordelia},
  booktitle={ICCV},
  year={2019}
}

@inproceedings{anne2017localizing,
  title={Localizing moments in video with natural language},
  author={Anne Hendricks, Lisa and Wang, Oliver and Shechtman, Eli and Sivic, Josef and Darrell, Trevor and Russell, Bryan},
  booktitle={ICCV},
  year={2017}
}
@inproceedings{krishna2017dense,
  title={Dense-captioning events in videos},
  author={Krishna, Ranjay and Hata, Kenji and Ren, Frederic and Fei-Fei, Li and Carlos Niebles, Juan},
  booktitle={ICCV},
  year={2017}
}

@inproceedings{rohrbach2015dataset,
  title={A dataset for movie description},
  author={Rohrbach, Anna and Rohrbach, Marcus and Tandon, Niket and Schiele, Bernt},
  booktitle={CVPR},
  year={2015}
}

@inproceedings{wray2021semantic,
  title={On Semantic Similarity in Video Retrieval},
  author={Wray, Michael and Doughty, Hazel and Damen, Dima},
  booktitle={CVPR},
  year={2021}
}

@inproceedings{sharma2018conceptual,
  title={Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning},
  author={Sharma, Piyush and Ding, Nan and Goodman, Sebastian and Soricut, Radu},
  booktitle={ACL},
  year={2018}
}

@article{loshchilov2017decoupled,
  title={Decoupled weight decay regularization},
  author={Loshchilov, Ilya and Hutter, Frank},
  journal={arXiv:1711.05101},
  year={2017}
}

@inproceedings{lecun2015deep,
  title={Deep learning},
  author={LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
  booktitle={Nature},
  year={2015},
}


@inproceedings{krizhevsky2012imagenet,
  title={Imagenet classification with deep convolutional neural networks},
  author={Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
  booktitle={NeurIPS},
  year={2012}
}

@inproceedings{wu2021exploring,
  title={Exploring Heterogeneous Clues for Weakly-Supervised Audio-Visual Video Parsing},
  author={Wu, Yu and Yang, Yi},
  booktitle={CVPR},
  year={2021}
}

@inproceedings{miech2019howto100m,
  title={{Howto100M}: Learning a text-video embedding by watching hundred million narrated video clips},
  author={Miech, Antoine and Zhukov, Dimitri and Alayrac, Jean-Baptiste and Tapaswi, Makarand and Laptev, Ivan and Sivic, Josef},
  booktitle={ICCV},
  year={2019}
}

@inproceedings{lei2021less,
  title={Less is more: {ClipBERT} for video-and-language learning via sparse sampling},
  author={Lei, Jie and Li, Linjie and Zhou, Luowei and Gan, Zhe and Berg, Tamara L and Bansal, Mohit and Liu, Jingjing},
  booktitle={CVPR},
  year={2021}
}

@inproceedings{szegedy2015going,
  title={Going deeper with convolutions},
  author={Szegedy, Christian and Liu, Wei and Jia, Yangqing and Sermanet, Pierre and Reed, Scott and Anguelov, Dragomir and Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich, Andrew},
  booktitle={CVPR},
  year={2015}
}

@inproceedings{he2016deep,
  title={Deep residual learning for image recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={CVPR},
  year={2016}
}

@article{hochreiter1997long,
  title={Long short-term memory},
  author={Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
  journal={Neural computation},
  volume={9},
  number={8},
  pages={1735--1780},
  year={1997},
  publisher={MIT Press}
}

@inproceedings{bahdanau2015neural-attn,
  title={Neural machine translation by jointly learning to align and translate},
  author={Bahdanau, Dzmitry and Cho, Kyung Hyun and Bengio, Yoshua},
  booktitle={3rd International Conference on Learning Representations, ICLR 2015},
  year={2015}
}

@inproceedings{he2016deep-resnet,
  title={Deep residual learning for image recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={770--778},
  year={2016}
}

@inproceedings{simonyan2015very,
  title={Very deep convolutional networks for large-scale image recognition},
  author={Simonyan, Karen and Zisserman, Andrew},
  booktitle={ICLR},
  year={2015}
}

@article{loshchilov2016sgdr,
  title={{SGDR}: Stochastic gradient descent with warm restarts},
  author={Loshchilov, Ilya and Hutter, Frank},
  journal={arXiv:1608.03983},
  year={2016}
}

@article{russakovsky2015imagenet,
  title={Imagenet large scale visual recognition challenge},
  author={Russakovsky, Olga and Deng, Jia and Su, Hao and Krause, Jonathan and Satheesh, Sanjeev and Ma, Sean and Huang, Zhiheng and Karpathy, Andrej and Khosla, Aditya and Bernstein, Michael and others},
  journal={IJCV},
  year={2015}
}

@inproceedings{dosovitskiy2020image,
  title={An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},
  author={Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and others},
  booktitle={International Conference on Learning Representations},
  year={2020}
}

@inproceedings{haa500,
    title={HAA500: Human-Centric Atomic Action Dataset with Curated Videos},
    author={Jihoon Chung and Cheng-hsin Wuu and Hsuan-ru Yang and Yu-Wing Tai and Chi-Keung Tang},
    booktitle = {ICCV},
    year = {2021}
}

@article{wu2022snoc,
  title={Switchable novel object captioner},
  author={Wu, Yu and Jiang, Lu and Yang, Yi},
  journal={TPAMI},
  year={2022}
}

@inproceedings{Agrawal2018DontJA,
  title={Don't Just Assume; Look and Answer: Overcoming Priors for Visual Question Answering},
  author={Aishwarya Agrawal and Dhruv Batra and Devi Parikh and Aniruddha Kembhavi},
  booktitle = {CVPR},
  year={2018},
}


@article{Goyal2021SelfsupervisedPO,
  title={Self-supervised Pretraining of Visual Features in the Wild},
  author={Priya Goyal and Mathilde Caron and Benjamin Lefaudeux and Min Xu and Pengchao Wang and Vivek Mangalore Pai and Mannat Singh and Vitaliy Liptchinsky and Ishan Misra and Armand Joulin and Piotr Bojanowski},
  journal={arXiv:2103.01988},
  year={2021},
}

@article{vural2020deep,
  title={Deep multi query image retrieval},
  author={Vural, Cabir and Akbacak, Enver},
  journal={SPIC},
  year={2020},
}

@inproceedings{fernando2013mining,
  title={Mining multiple queries for image retrieval: On-the-fly learning of an object-specific mid-level representation},
  author={Fernando, Basura and Tuytelaars, Tinne},
  booktitle={CVPR},
  year={2013}
}

@inproceedings{huang2017multi,
  title={Multi-query image retrieval using {CNN} and {SIFT} features},
  author={Huang, Shiuan and Hang, Hsueh-Ming},
  booktitle={APSIPA ASC},
  year={2017},
}

@article{wang2017effective,
  title={Effective multi-query expansions: Collaborative deep networks for robust landmark retrieval},
  author={Wang, Yang and Lin, Xuemin and Wu, Lin and Zhang, Wenjie},
  journal={TIP},
  year={2017},
}

@inproceedings{wang2015effective,
  title={Effective multi-query expansions: Robust landmark retrieval},
  author={Wang, Yang and Lin, Xuemin and Wu, Lin and Zhang, Wenjie},
  booktitle={ACMMM},
  year={2015}
}

@inproceedings{zheng2016mars,
  title={Mars: A video benchmark for large-scale person re-identification},
  author={Zheng, Liang and Bie, Zhi and Sun, Yifan and Wang, Jingdong and Su, Chi and Wang, Shengjin and Tian, Qi},
  booktitle={ECCV},
  year={2016},
}

@article{liu2022unsupervised,
  title={Unsupervised person re-identification with stochastic training strategy},
  author={Liu, Tianyang and Lin, Yutian and Du, Bo},
  journal={TIP},
  year={2022},
}

@article{lin2020bayesian,
  title={Bayesian query expansion for multi-camera person re-identification},
  author={Lin, Yutian and Zheng, Zhedong and Zhang, Hong and Gao, Chenqiang and Yang, Yi},
  journal={Pattern Recognition Letters},
  year={2020},
}

@inproceedings{imani2019deep,
  title={Deep neural networks for query expansion using word embeddings},
  author={Imani, Ayyoob and Vakili, Amir and Montazer, Ali and Shakery, Azadeh},
  booktitle={ECIR},
  year={2019},
}

@inproceedings{arandjelovic2012three,
  title={Three things everyone should know to improve object retrieval},
  author={Arandjelovi{\'c}, Relja and Zisserman, Andrew},
  booktitle={CVPR},
  year={2012},
}

@inproceedings{chum2011total,
  title={Total recall {II}: Query expansion revisited},
  author={Chum, Ond{\v{r}}ej and Mikulik, Andrej and Perdoch, Michal and Matas, Ji{\v{r}}{\'\i}},
  booktitle={CVPR},
  year={2011},
}

@inproceedings{chum2007total,
  title={Total recall: Automatic query expansion with a generative feature model for object retrieval},
  author={Chum, Ondrej and Philbin, James and Sivic, Josef and Isard, Michael and Zisserman, Andrew},
  booktitle={ICCV},
  year={2007},
}

@article{radenovic2018fine,
  title={Fine-tuning {CNN} image retrieval with no human annotation},
  author={Radenovi{\'c}, Filip and Tolias, Giorgos and Chum, Ond{\v{r}}ej},
  journal={TPAMI},
  year={2018},
}

@article{gordo2017end,
  title={End-to-end learning of deep visual representations for image retrieval},
  author={Gordo, Albert and Almazan, Jon and Revaud, Jerome and Larlus, Diane},
  journal={IJCV},
  year={2017},
}

@inproceedings{gordo2020attention,
  title={Attention-based query expansion learning},
  author={Gordo, Albert and Radenovic, Filip and Berg, Tamara},
  booktitle={ECCV},
  year={2020},
}

@inproceedings{radford2021learning_clip,
  title={Learning transferable visual models from natural language supervision},
  author={Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and others},
  booktitle={International Conference on Machine Learning},
  pages={8748--8763},
  year={2021},
  organization={PMLR}
}

% Please download the latest anthology.bib from
%
% http://aclweb.org/anthology/anthology.bib.gz

@inproceedings{DBLP:conf/ecir/LyuJG20,
  author    = {Chenyang Lyu and
               Tianbo Ji and
               Yvette Graham},
  editor    = {Ricardo Campos and
               Al{\'{\i}}pio M{\'{a}}rio Jorge and
               Adam Jatowt and
               Sumit Bhatia},
  title     = {Incorporating Context and Knowledge for Better Sentiment Analysis
               of Narrative Text},
  booktitle = {Proceedings of Text2Story - Third Workshop on Narrative Extraction
               From Texts co-located with 42nd European Conference on Information
               Retrieval, Text2Story@ECIR 2020, Lisbon, Portugal, April 14th, 2020
               [online only]},
  series    = {{CEUR} Workshop Proceedings},
  volume    = {2593},
  pages     = {39--45},
  publisher = {CEUR-WS.org},
  year      = {2020},
  url       = {http://ceur-ws.org/Vol-2593/paper5.pdf},
  timestamp = {Fri, 24 Apr 2020 15:50:00 +0200},
  biburl    = {https://dblp.org/rec/conf/ecir/LyuJG20.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@article{DBLP:journals/entropy/JiLCC21,
  author    = {Tianbo Ji and
               Chenyang Lyu and
               Zhichao Cao and
               Peng Cheng},
  title     = {Multi-Hop Question Generation Using Hierarchical Encoding-Decoding
               and Context Switch Mechanism},
  journal   = {Entropy},
  volume    = {23},
  number    = {11},
  pages     = {1449},
  year      = {2021},
  url       = {https://doi.org/10.3390/e23111449},
  doi       = {10.3390/e23111449},
  timestamp = {Wed, 15 Dec 2021 10:28:32 +0100},
  biburl    = {https://dblp.org/rec/journals/entropy/JiLCC21.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{kocisky-etal-2018-narrativeqa,
    title = "The {N}arrative{QA} Reading Comprehension Challenge",
    author = "Ko{\v{c}}isk{\'y}, Tom{\'a}{\v{s}}  and
      Schwarz, Jonathan  and
      Blunsom, Phil  and
      Dyer, Chris  and
      Hermann, Karl Moritz  and
      Melis, G{\'a}bor  and
      Grefenstette, Edward",
    journal = "Transactions of the Association for Computational Linguistics",
    volume = "6",
    year = "2018",
    address = "Cambridge, MA",
    publisher = "MIT Press",
    url = "https://aclanthology.org/Q18-1023",
    doi = "10.1162/tacl_a_00023",
    pages = "317--328",
    abstract = "Reading comprehension (RC){---}in contrast to information retrieval{---}requires integrating information and reasoning about events, entities, and their relations across a full document. Question answering is conventionally used to assess RC ability, in both artificial agents and children learning to read. However, existing RC datasets and tasks are dominated by questions that can be solved by selecting answers using superficial information (e.g., local context similarity or global term frequency); they thus fail to test for the essential integrative aspect of RC. To encourage progress on deeper comprehension of language, we present a new dataset and set of tasks in which the reader must answer questions about stories by reading entire books or movie scripts. These tasks are designed so that successfully answering their questions requires understanding the underlying narrative rather than relying on shallow pattern matching or salience. We show that although humans solve the tasks easily, standard RC models struggle on the tasks presented here. We provide an analysis of the dataset and the challenges it presents.",
}

@inproceedings{dua-etal-2019-drop,
    title = "{DROP}: A Reading Comprehension Benchmark Requiring Discrete Reasoning Over Paragraphs",
    author = "Dua, Dheeru  and
      Wang, Yizhong  and
      Dasigi, Pradeep  and
      Stanovsky, Gabriel  and
      Singh, Sameer  and
      Gardner, Matt",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N19-1246",
    doi = "10.18653/v1/N19-1246",
    pages = "2368--2378",
    abstract = "Reading comprehension has recently seen rapid progress, with systems matching humans on the most popular datasets for the task. However, a large body of work has highlighted the brittleness of these systems, showing that there is much work left to be done. We introduce a new reading comprehension benchmark, DROP, which requires Discrete Reasoning Over the content of Paragraphs. In this crowdsourced, adversarially-created, 55k-question benchmark, a system must resolve references in a question, perhaps to multiple input positions, and perform discrete operations over them (such as addition, counting, or sorting). These operations require a much more comprehensive understanding of the content of paragraphs, as they remove the paraphrase-and-entity-typing shortcuts available in prior datasets. We apply state-of-the-art methods from both the reading comprehension and semantic parsing literatures on this dataset and show that the best systems only achieve 38.4{\%} F1 on our generalized accuracy metric, while expert human performance is 96{\%}. We additionally present a new model that combines reading comprehension methods with simple numerical reasoning to achieve 51{\%} F1.",
}

@inproceedings{nguyen2016ms_marco,
  title={MS MARCO: A human generated machine reading comprehension dataset},
  author={Nguyen, Tri and Rosenberg, Mir and Song, Xia and Gao, Jianfeng and Tiwary, Saurabh and Majumder, Rangan and Deng, Li},
  booktitle={CoCo@ NIPS},
  year={2016}
}

@inproceedings{zhang2003question,
  title={Question classification using support vector machines},
  author={Zhang, Dell and Lee, Wee Sun},
  booktitle={Proceedings of the 26th annual international ACM SIGIR conference on Research and development in informaion retrieval},
  pages={26--32},
  year={2003}
}
@article{JMLR:v9:vandermaaten08a_tsne,
  author  = {Laurens van der Maaten and Geoffrey Hinton},
  title   = {Visualizing Data using t-SNE},
  journal = {Journal of Machine Learning Research},
  year    = {2008},
  volume  = {9},
  number  = {86},
  pages   = {2579-2605},
  url     = {http://jmlr.org/papers/v9/vandermaaten08a.html}
}

@misc{zhang2020machine,
      title={Machine Reading Comprehension: The Role of Contextualized Language Models and Beyond}, 
      author={Zhuosheng Zhang and Hai Zhao and Rui Wang},
      year={2020},
      eprint={2005.06249},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}


@article{bender-friedman-2018-data_statement,
    title = "Data Statements for Natural Language Processing: Toward Mitigating System Bias and Enabling Better Science",
    author = "Bender, Emily M.  and
      Friedman, Batya",
    journal = "Transactions of the Association for Computational Linguistics",
    volume = "6",
    year = "2018",
    url = "https://aclanthology.org/Q18-1041",
    doi = "10.1162/tacl_a_00041",
    pages = "587--604",
    abstract = "In this paper, we propose data statements as a design solution and professional practice for natural language processing technologists, in both research and development. Through the adoption and widespread use of data statements, the field can begin to address critical scientific and ethical issues that result from the use of data from certain populations in the development of technology for other populations. We present a form that data statements can take and explore the implications of adopting them as part of regular practice. We argue that data statements will help alleviate issues related to exclusion and bias in language technology, lead to better precision in claims about how natural language processing research can generalize and thus better engineering results, protect companies from public embarrassment, and ultimately lead to language technology that meets its users in their own preferred linguistic style and furthermore does not misrepresent them to others.",
}


@inproceedings{gardner2021competency_remove_artifacts_in_data,
    title = "Competency Problems: On Finding and Removing Artifacts in Language Data",
    author = "Gardner, Matt  and
      Merrill, William  and
      Dodge, Jesse  and
      Peters, Matthew  and
      Ross, Alexis  and
      Singh, Sameer  and
      Smith, Noah A.",
    booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2021",
    address = "Online and Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.emnlp-main.135",
    doi = "10.18653/v1/2021.emnlp-main.135",
    pages = "1801--1813",
    abstract = "Much recent work in NLP has documented dataset artifacts, bias, and spurious correlations between input features and output labels. However, how to tell which features have {``}spurious{''} instead of legitimate correlations is typically left unspecified. In this work we argue that for complex language understanding tasks, all simple feature correlations are spurious, and we formalize this notion into a class of problems which we call competency problems. For example, the word {``}amazing{''} on its own should not give information about a sentiment label independent of the context in which it appears, which could include negation, metaphor, sarcasm, etc. We theoretically analyze the difficulty of creating data for competency problems when human bias is taken into account, showing that realistic datasets will increasingly deviate from competency problems as dataset size increases. This analysis gives us a simple statistical test for dataset artifacts, which we use to show more subtle biases than were described in prior work, including demonstrating that models are inappropriately affected by these less extreme biases. Our theoretical treatment of this problem also allows us to analyze proposed solutions, such as making local edits to dataset instances, and to give recommendations for future data collection and model design efforts that target competency problems.",
}

@inproceedings{kaushik-lipton-2018-much,
    title = "How Much Reading Does Reading Comprehension Require? A Critical Investigation of Popular Benchmarks",
    author = "Kaushik, Divyansh  and
      Lipton, Zachary C.",
    booktitle = "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
    month = oct # "-" # nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D18-1546",
    doi = "10.18653/v1/D18-1546",
    pages = "5010--5015",
    abstract = "Many recent papers address reading comprehension, where examples consist of (question, passage, answer) tuples. Presumably, a model must combine information from both questions and passages to predict corresponding answers. However, despite intense interest in the topic, with hundreds of published papers vying for leaderboard dominance, basic questions about the difficulty of many popular benchmarks remain unanswered. In this paper, we establish sensible baselines for the bAbI, SQuAD, CBT, CNN, and Who-did-What datasets, finding that question- and passage-only models often perform surprisingly well. On 14 out of 20 bAbI tasks, passage-only models achieve greater than 50{\%} accuracy, sometimes matching the full model. Interestingly, while CBT provides 20-sentence passages, only the last is needed for accurate prediction. By comparison, SQuAD and CNN appear better-constructed.",
}

@inproceedings{al-negheimish-etal-2021-numerical_reasoning,
    title = "Numerical reasoning in machine reading comprehension tasks: are we there yet?",
    author = "Al-Negheimish, Hadeel  and
      Madhyastha, Pranava  and
      Russo, Alessandra",
    booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2021",
    address = "Online and Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.emnlp-main.759",
    pages = "9643--9649",
    abstract = "Numerical reasoning based machine reading comprehension is a task that involves reading comprehension along with using arithmetic operations such as addition, subtraction, sorting and counting. The DROP benchmark (Dua et al., 2019) is a recent dataset that has inspired the design of NLP models aimed at solving this task. The current standings of these models in the DROP leaderboard, over standard metrics, suggests that the models have achieved near-human performance. However, does this mean that these models have learned to reason? In this paper, we present a controlled study on some of the top-performing model architectures for the task of numerical reasoning. Our observations suggest that the standard metrics are incapable of measuring progress towards such tasks.",
}

@inproceedings{jia-liang-2017-adversarial,
    title = "Adversarial Examples for Evaluating Reading Comprehension Systems",
    author = "Jia, Robin  and
      Liang, Percy",
    booktitle = "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing",
    month = sep,
    year = "2017",
    address = "Copenhagen, Denmark",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D17-1215",
    doi = "10.18653/v1/D17-1215",
    pages = "2021--2031",
    abstract = "Standard accuracy metrics indicate that reading comprehension systems are making rapid progress, but the extent to which these systems truly understand language remains unclear. To reward systems with real language understanding abilities, we propose an adversarial evaluation scheme for the Stanford Question Answering Dataset (SQuAD). Our method tests whether systems can answer questions about paragraphs that contain adversarially inserted sentences, which are automatically generated to distract computer systems without changing the correct answer or misleading humans. In this adversarial setting, the accuracy of sixteen published models drops from an average of 75{\%} F1 score to 36{\%}; when the adversary is allowed to add ungrammatical sequences of words, average accuracy on four models decreases further to 7{\%}. We hope our insights will motivate the development of new models that understand language more precisely.",
}

@inproceedings{weissenborn-etal-2017-making_fastqa,
    title = "Making Neural {QA} as Simple as Possible but not Simpler",
    author = "Weissenborn, Dirk  and
      Wiese, Georg  and
      Seiffe, Laura",
    booktitle = "Proceedings of the 21st Conference on Computational Natural Language Learning ({C}o{NLL} 2017)",
    month = aug,
    year = "2017",
    address = "Vancouver, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/K17-1028",
    doi = "10.18653/v1/K17-1028",
    pages = "271--280",
    abstract = "Recent development of large-scale question answering (QA) datasets triggered a substantial amount of research into end-to-end neural architectures for QA. Increasingly complex systems have been conceived without comparison to simpler neural baseline systems that would justify their complexity. In this work, we propose a simple heuristic that guides the development of neural baseline systems for the extractive QA task. We find that there are two ingredients necessary for building a high-performing neural QA system: first, the awareness of question words while processing the context and second, a composition function that goes beyond simple bag-of-words modeling, such as recurrent neural networks. Our results show that FastQA, a system that meets these two requirements, can achieve very competitive performance compared with existing models. We argue that this surprising finding puts results of previous systems and the complexity of recent QA datasets into perspective.",
}

@inproceedings{chendanqi-etal-2016-thorough_cnn,
    title = "A Thorough Examination of the {CNN}/{D}aily {M}ail Reading Comprehension Task",
    author = "Chen, Danqi  and
      Bolton, Jason  and
      Manning, Christopher D.",
    booktitle = "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2016",
    address = "Berlin, Germany",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P16-1223",
    doi = "10.18653/v1/P16-1223",
    pages = "2358--2367",
}

@inproceedings{geva-etal-2019-modeling_annotator_bias,
    title = "Are We Modeling the Task or the Annotator? An Investigation of Annotator Bias in Natural Language Understanding Datasets",
    author = "Geva, Mor  and
      Goldberg, Yoav  and
      Berant, Jonathan",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D19-1107",
    doi = "10.18653/v1/D19-1107",
    pages = "1161--1166",
    abstract = "Crowdsourcing has been the prevalent paradigm for creating natural language understanding datasets in recent years. A common crowdsourcing practice is to recruit a small number of high-quality workers, and have them massively generate examples. Having only a few workers generate the majority of examples raises concerns about data diversity, especially when workers freely generate sentences. In this paper, we perform a series of experiments showing these concerns are evident in three recent NLP datasets. We show that model performance improves when training with annotator identifiers as features, and that models are able to recognize the most productive annotators. Moreover, we show that often models do not generalize well to examples from annotators that did not contribute to the training set. Our findings suggest that annotator bias should be monitored during dataset creation, and that test set annotators should be disjoint from training set annotators.",
}

@inproceedings{rajpurkar-etal-2018-know_squad2.0,
    title = "Know What You Don{'}t Know: Unanswerable Questions for {SQ}u{AD}",
    author = "Rajpurkar, Pranav  and
      Jia, Robin  and
      Liang, Percy",
    booktitle = "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)",
    month = jul,
    year = "2018",
    address = "Melbourne, Australia",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P18-2124",
    doi = "10.18653/v1/P18-2124",
    pages = "784--789",
    abstract = "Extractive reading comprehension systems can often locate the correct answer to a question in a context document, but they also tend to make unreliable guesses on questions for which the correct answer is not stated in the context. Existing datasets either focus exclusively on answerable questions, or use automatically generated unanswerable questions that are easy to identify. To address these weaknesses, we present SQuADRUn, a new dataset that combines the existing Stanford Question Answering Dataset (SQuAD) with over 50,000 unanswerable questions written adversarially by crowdworkers to look similar to answerable ones. To do well on SQuADRUn, systems must not only answer questions when possible, but also determine when no answer is supported by the paragraph and abstain from answering. SQuADRUn is a challenging natural language understanding task for existing models: a strong neural system that gets 86{\%} F1 on SQuAD achieves only 66{\%} F1 on SQuADRUn. We release SQuADRUn to the community as the successor to SQuAD.",
}


@inproceedings{lyu2021improving,
  title={Improving Unsupervised Question Answering via Summarization-Informed Question Generation},
  author={Lyu, Chenyang and Shang, Lifeng and Graham, Yvette and Foster, Jennifer and Jiang, Xin and Liu, Qun},
  booktitle={Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing},
  pages={4134--4148},
  year={2021}
}

@inproceedings{conneau-etal-2018-xnli,
    title = "{XNLI}: Evaluating Cross-lingual Sentence Representations",
    author = "Conneau, Alexis  and
      Rinott, Ruty  and
      Lample, Guillaume  and
      Williams, Adina  and
      Bowman, Samuel  and
      Schwenk, Holger  and
      Stoyanov, Veselin",
    booktitle = "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
    month = oct # "-" # nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D18-1269",
    doi = "10.18653/v1/D18-1269",
    pages = "2475--2485",
    abstract = "State-of-the-art natural language processing systems rely on supervision in the form of annotated data to learn competent models. These models are generally trained on data in a single language (usually English), and cannot be directly used beyond that language. Since collecting data in every language is not realistic, there has been a growing interest in cross-lingual language understanding (XLU) and low-resource cross-language transfer. In this work, we construct an evaluation set for XLU by extending the development and test sets of the Multi-Genre Natural Language Inference Corpus (MultiNLI) to 14 languages, including low-resource languages such as Swahili and Urdu. We hope that our dataset, dubbed XNLI, will catalyze research in cross-lingual sentence understanding by providing an informative standard evaluation task. In addition, we provide several baselines for multilingual sentence understanding, including two based on machine translation systems, and two that use parallel data to train aligned multilingual bag-of-words and LSTM encoders. We find that XNLI represents a practical and challenging evaluation suite, and that directly translating the test data yields the best performance among available baselines.",
}

@inproceedings{dong-etal-2010-hownet,
    title = "{H}ow{N}et and Its Computation of Meaning",
    author = "Dong, Zhendong  and
      Dong, Qiang  and
      Hao, Changling",
    booktitle = "Coling 2010: Demonstrations",
    month = aug,
    year = "2010",
    address = "Beijing, China",
    publisher = "Coling 2010 Organizing Committee",
    url = "https://aclanthology.org/C10-3014",
    pages = "53--56",
}

@inproceedings{xu2017cn_DBpedia,
  title={CN-DBpedia: A never-ending Chinese knowledge extraction system},
  author={Xu, Bo and Xu, Yong and Liang, Jiaqing and Xie, Chenhao and Liang, Bin and Cui, Wanyun and Xiao, Yanghua},
  booktitle={International Conference on Industrial, Engineering and Other Applications of Applied Intelligent Systems},
  pages={428--438},
  year={2017},
  organization={Springer}
}

@article{sun2019ernie_baidu,
  title={Ernie: Enhanced representation through knowledge integration},
  author={Sun, Yu and Wang, Shuohuan and Li, Yukun and Feng, Shikun and Chen, Xuyi and Zhang, Han and Tian, Xin and Zhu, Danxiang and Tian, Hao and Wu, Hua},
  journal={arXiv preprint arXiv:1904.09223},
  year={2019}
}

@inproceedings{kudo-richardson-2018-sentencepiece,
    title = "{S}entence{P}iece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing",
    author = "Kudo, Taku  and
      Richardson, John",
    booktitle = "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations",
    month = nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D18-2012",
    doi = "10.18653/v1/D18-2012",
    pages = "66--71",
    abstract = "This paper describes SentencePiece, a language-independent subword tokenizer and detokenizer designed for Neural-based text processing, including Neural Machine Translation. It provides open-source C++ and Python implementations for subword units. While existing subword segmentation tools assume that the input is pre-tokenized into word sequences, SentencePiece can train subword models directly from raw sentences, which allows us to make a purely end-to-end and language independent system. We perform a validation experiment of NMT on English-Japanese machine translation, and find that it is possible to achieve comparable accuracy to direct subword training from raw sentences. We also compare the performance of subword training and segmentation with various configurations. SentencePiece is available under the Apache 2 license at \url{https://github.com/google/sentencepiece}.",
}
@article{bordes2013translating_transE,
  title={Translating embeddings for modeling multi-relational data},
  author={Bordes, Antoine and Usunier, Nicolas and Garcia-Duran, Alberto and Weston, Jason and Yakhnenko, Oksana},
  journal={Advances in neural information processing systems},
  volume={26},
  year={2013}
}


@misc{chelba2014billion,
      title={One Billion Word Benchmark for Measuring Progress in Statistical Language Modeling}, 
      author={Ciprian Chelba and Tomas Mikolov and Mike Schuster and Qi Ge and Thorsten Brants and Phillipp Koehn and Tony Robinson},
      year={2014},
      eprint={1312.3005},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{peters2019knowledge_elmo,
  title={Knowledge Enhanced Contextual Word Representations},
  author={Peters, Matthew E and Neumann, Mark and Logan, Robert and Schwartz, Roy and Joshi, Vidur and Singh, Sameer and Smith, Noah A},
  booktitle={Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)},
  pages={43--54},
  year={2019}
}

@inproceedings{brown2020language_gpt3,
 author = {Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel and Wu, Jeffrey and Winter, Clemens and Hesse, Chris and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M. F. Balcan and H. Lin},
 pages = {1877--1901},
 publisher = {Curran Associates, Inc.},
 title = {Language Models are Few-Shot Learners},
 url = {https://proceedings.neurips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf},
 volume = {33},
 year = {2020}
}

@article{raffel2020exploring_t5,
  title={Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer},
  author={Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J},
  journal={Journal of Machine Learning Research},
  volume={21},
  number={140},
  pages={1--67},
  year={2020}
}

@misc{liu2019roberta,
      title={RoBERTa: A Robustly Optimized BERT Pretraining Approach}, 
      author={Yinhan Liu and Myle Ott and Naman Goyal and Jingfei Du and Mandar Joshi and Danqi Chen and Omer Levy and Mike Lewis and Luke Zettlemoyer and Veselin Stoyanov},
      year={2019},
      eprint={1907.11692},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{radford2019language_gpt2,
  title={Language Models are Unsupervised Multitask Learners},
  author={Radford, Alec and Wu, Jeff and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  year={2019}
}

@inproceedings{roy2020incorporating_survey,
  title={Incorporating Extra Knowledge to Enhance Word Embedding.},
  author={Roy, Arpita and Pan, Shimei},
  booktitle={IJCAI},
  pages={4929--4935},
  year={2020}
}


@misc{wei2021knowledge_survey,
      title={Knowledge Enhanced Pretrained Language Models: A Compreshensive Survey}, 
      author={Xiaokai Wei and Shen Wang and Dejiao Zhang and Parminder Bhatia and Andrew Arnold},
      year={2021},
      eprint={2110.08455},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{qiu2020pre_survey,
  title={Pre-trained models for natural language processing: A survey},
  author={Qiu, Xipeng and Sun, Tianxiang and Xu, Yige and Shao, Yunfan and Dai, Ning and Huang, Xuanjing},
  journal={Science China Technological Sciences},
  pages={1--26},
  year={2020},
  publisher={Springer}
}

@inproceedings{wang-etal-2018-glue,
    title = "{GLUE}: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding",
    author = "Wang, Alex  and
      Singh, Amanpreet  and
      Michael, Julian  and
      Hill, Felix  and
      Levy, Omer  and
      Bowman, Samuel",
    booktitle = "Proceedings of the 2018 {EMNLP} Workshop {B}lackbox{NLP}: Analyzing and Interpreting Neural Networks for {NLP}",
    month = nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W18-5446",
    doi = "10.18653/v1/W18-5446",
    pages = "353--355",
    abstract = "Human ability to understand language is \textit{general, flexible, and robust}. In contrast, most NLU models above the word level are designed for a specific task and struggle with out-of-domain data. If we aspire to develop models with understanding beyond the detection of superficial correspondences between inputs and outputs, then it is critical to develop a unified model that can execute a range of linguistic tasks across different domains. To facilitate research in this direction, we present the General Language Understanding Evaluation (GLUE, gluebenchmark.com): a benchmark of nine diverse NLU tasks, an auxiliary dataset for probing models for understanding of specific linguistic phenomena, and an online platform for evaluating and comparing models. For some benchmark tasks, training data is plentiful, but for others it is limited or does not match the genre of the test set. GLUE thus favors models that can represent linguistic knowledge in a way that facilitates sample-efficient learning and effective knowledge-transfer across tasks. While none of the datasets in GLUE were created from scratch for the benchmark, four of them feature privately-held test data, which is used to ensure that the benchmark is used fairly. We evaluate baselines that use ELMo (Peters et al., 2018), a powerful transfer learning technique, as well as state-of-the-art sentence representation models. The best models still achieve fairly low absolute scores. Analysis with our diagnostic dataset yields similarly weak performance over all phenomena tested, with some exceptions.",
}

@inproceedings{zhu2015aligning_bookcorpus,
  title={Aligning books and movies: Towards story-like visual explanations by watching movies and reading books},
  author={Zhu, Yukun and Kiros, Ryan and Zemel, Rich and Salakhutdinov, Ruslan and Urtasun, Raquel and Torralba, Antonio and Fidler, Sanja},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={19--27},
  year={2015}
}

@inproceedings{liu2018generating,
  title={Generating Wikipedia by Summarizing Long Sequences},
  author={Liu, Peter J and Saleh, Mohammad and Pot, Etienne and Goodrich, Ben and Sepassi, Ryan and Kaiser, Lukasz and Shazeer, Noam},
  booktitle={International Conference on Learning Representations},
  year={2018}
}

@article{hochreiter1997long_lstm,
  title={Long short-term memory},
  author={Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
  journal={Neural computation},
  volume={9},
  number={8},
  pages={1735--1780},
  year={1997},
  publisher={MIT Press}
}

@article{deerwester1990indexing_LSA,
  title={Indexing by latent semantic analysis},
  author={Deerwester, Scott and Dumais, Susan T and Furnas, George W and Landauer, Thomas K and Harshman, Richard},
  journal={Journal of the American society for information science},
  volume={41},
  number={6},
  pages={391--407},
  year={1990},
  publisher={Wiley Online Library}
}

@incollection{hinton1986distributed,
  title={Distributed representations},
  author={Hinton, GE and McClelland, JL and Rumelhart, DE},
  booktitle={Parallel distributed processing: explorations in the microstructure of cognition, vol. 1: foundations},
  pages={77--109},
  year={1986}
}

@article{rumelhart1986learning_backpropagation,
  title={Learning representations by back-propagating errors},
  author={Rumelhart, David E and Hinton, Geoffrey E and Williams, Ronald J},
  journal={nature},
  volume={323},
  number={6088},
  pages={533--536},
  year={1986},
  publisher={Nature Publishing Group}
}

@article{elman1990finding,
  title={Finding structure in time},
  author={Elman, Jeffrey L},
  journal={Cognitive science},
  volume={14},
  number={2},
  pages={179--211},
  year={1990},
  publisher={Wiley Online Library}
}

@article{mikolov2013efficient_word2vec_2,
  title={Efficient estimation of word representations in vector space},
  author={Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
  journal={arXiv preprint arXiv:1301.3781},
  year={2013}
}

@inproceedings{mikolov2013distributed_word2vec_1,
  title={Distributed representations of words and phrases and their compositionality},
  author={Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg S and Dean, Jeff},
  booktitle={Advances in neural information processing systems},
  pages={3111--3119},
  year={2013}
}

@article{bengio2003neural_nnlm,
  title={A neural probabilistic language model},
  author={Bengio, Yoshua and Ducharme, R{\'e}jean and Vincent, Pascal and Janvin, Christian},
  journal={The journal of machine learning research},
  volume={3},
  pages={1137--1155},
  year={2003},
  publisher={JMLR. org}
}

@inproceedings{levy-goldberg-2014-dependency_word2vec,
    title = "Dependency-Based Word Embeddings",
    author = "Levy, Omer  and
      Goldberg, Yoav",
    booktitle = "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)",
    month = jun,
    year = "2014",
    address = "Baltimore, Maryland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P14-2050",
    doi = "10.3115/v1/P14-2050",
    pages = "302--308",
}

@inproceedings{lauscher2020specializing,
  title={Specializing Unsupervised Pretraining Models for Word-Level Semantic Similarity},
  author={Lauscher, Anne and Vuli{\'c}, Ivan and Ponti, Edoardo Maria and Korhonen, Anna and Glava{\v{s}}, Goran},
  booktitle={Proceedings of the 28th International Conference on Computational Linguistics},
  pages={1371--1383},
  year={2020}
}

@book{bird2009natural_nltk,
  title={Natural language processing with Python: analyzing text with the natural language toolkit},
  author={Bird, Steven and Klein, Ewan and Loper, Edward},
  year={2009},
  publisher={" O'Reilly Media, Inc."}
}

@inproceedings{bird2006nltk,
  title={NLTK: the natural language toolkit},
  author={Bird, Steven},
  booktitle={Proceedings of the COLING/ACL 2006 Interactive Presentation Sessions},
  pages={69--72},
  year={2006}
}

@inproceedings{li-roth-2002-learning_question_classification,
    title = "Learning Question Classifiers",
    author = "Li, Xin  and
      Roth, Dan",
    booktitle = "{COLING} 2002: The 19th International Conference on Computational Linguistics",
    year = "2002",
    url = "https://aclanthology.org/C02-1150",
}

@inproceedings{northcutt2021pervasive_label_error,
  title={Pervasive Label Errors in Test Sets Destabilize Machine Learning Benchmarks},
  author={Northcutt, Curtis G and Athalye, Anish and Mueller, Jonas},
  booktitle={Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 1)},
  year={2021}
}

@inproceedings{ott2018analyzing_uncertainty_in_nmt,
  title={Analyzing uncertainty in neural machine translation},
  author={Ott, Myle and Auli, Michael and Grangier, David and Ranzato, Marcâ€™Aurelio},
  booktitle={International Conference on Machine Learning},
  pages={3956--3965},
  year={2018},
  organization={PMLR}
}

@inproceedings{wu-etal-2020-perturbed,
    title = "Perturbed Masking: Parameter-free Probing for Analyzing and Interpreting {BERT}",
    author = "Wu, Zhiyong  and
      Chen, Yun  and
      Kao, Ben  and
      Liu, Qun",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.383",
    doi = "10.18653/v1/2020.acl-main.383",
    pages = "4166--4176",
    abstract = "By introducing a small set of additional parameters, a \textit{probe} learns to solve specific linguistic tasks (e.g., dependency parsing) in a supervised manner using feature representations (e.g., contextualized embeddings). The effectiveness of such \textit{probing} tasks is taken as evidence that the pre-trained model encodes linguistic knowledge. However, this approach of evaluating a language model is undermined by the uncertainty of the amount of knowledge that is learned by the probe itself. Complementary to those works, we propose a parameter-free probing technique for analyzing pre-trained language models (e.g., BERT). Our method does not require direct supervision from the probing tasks, nor do we introduce additional parameters to the probing process. Our experiments on BERT show that syntactic trees recovered from BERT using our method are significantly better than linguistically-uninformed baselines. We further feed the empirically induced dependency structures into a downstream sentiment classification task and find its improvement compatible with or even superior to a human-designed dependency schema.",
}

@inproceedings{hewitt-manning-2019-structural,
    title = "{A} Structural Probe for Finding Syntax in Word Representations",
    author = "Hewitt, John  and
      Manning, Christopher D.",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N19-1419",
    doi = "10.18653/v1/N19-1419",
    pages = "4129--4138",
    abstract = "Recent work has improved our ability to detect linguistic knowledge in word representations. However, current methods for detecting syntactic knowledge do not test whether syntax trees are represented in their entirety. In this work, we propose a structural probe, which evaluates whether syntax trees are embedded in a linear transformation of a neural network{'}s word representation space. The probe identifies a linear transformation under which squared L2 distance encodes the distance between words in the parse tree, and one in which squared L2 norm encodes depth in the parse tree. Using our probe, we show that such transformations exist for both ELMo and BERT but not in baselines, providing evidence that entire syntax trees are embedded implicitly in deep models{'} vector geometry.",
}

@inproceedings{tenney-etal-2019-bert_rediscover,
    title = "{BERT} Rediscovers the Classical {NLP} Pipeline",
    author = "Tenney, Ian  and
      Das, Dipanjan  and
      Pavlick, Ellie",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P19-1452",
    doi = "10.18653/v1/P19-1452",
    pages = "4593--4601",
    abstract = "Pre-trained text encoders have rapidly advanced the state of the art on many NLP tasks. We focus on one such model, BERT, and aim to quantify where linguistic information is captured within the network. We find that the model represents the steps of the traditional NLP pipeline in an interpretable and localizable way, and that the regions responsible for each step appear in the expected sequence: POS tagging, parsing, NER, semantic roles, then coreference. Qualitative analysis reveals that the model can and often does adjust this pipeline dynamically, revising lower-level decisions on the basis of disambiguating information from higher-level representations.",
}

@inproceedings{clark-etal-2019-bert_look_at,
    title = "What Does {BERT} Look at? An Analysis of {BERT}{'}s Attention",
    author = "Clark, Kevin  and
      Khandelwal, Urvashi  and
      Levy, Omer  and
      Manning, Christopher D.",
    booktitle = "Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP",
    month = aug,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W19-4828",
    doi = "10.18653/v1/W19-4828",
    pages = "276--286",
    abstract = "Large pre-trained neural networks such as BERT have had great recent success in NLP, motivating a growing body of research investigating what aspects of language they are able to learn from unlabeled data. Most recent analysis has focused on model outputs (e.g., language model surprisal) or internal vector representations (e.g., probing classifiers). Complementary to these works, we propose methods for analyzing the attention mechanisms of pre-trained models and apply them to BERT. BERT{'}s attention heads exhibit patterns such as attending to delimiter tokens, specific positional offsets, or broadly attending over the whole sentence, with heads in the same layer often exhibiting similar behaviors. We further show that certain attention heads correspond well to linguistic notions of syntax and coreference. For example, we find heads that attend to the direct objects of verbs, determiners of nouns, objects of prepositions, and coreferent mentions with remarkably high accuracy. Lastly, we propose an attention-based probing classifier and use it to further demonstrate that substantial syntactic information is captured in BERT{'}s attention.",
}

@inproceedings{rogers-2021-changing_the_world_by_changing_the_data,
    title = "Changing the World by Changing the Data",
    author = "Rogers, Anna",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-long.170",
    doi = "10.18653/v1/2021.acl-long.170",
    pages = "2182--2194",
    abstract = "NLP community is currently investing a lot more research and resources into development of deep learning models than training data. While we have made a lot of progress, it is now clear that our models learn all kinds of spurious patterns, social biases, and annotation artifacts. Algorithmic solutions have so far had limited success. An alternative that is being actively discussed is more careful design of datasets so as to deliver specific signals. This position paper maps out the arguments for and against data curation, and argues that fundamentally the point is moot: curation already is and will be happening, and it is changing the world. The question is only how much thought we want to invest into that process.",
}

@inproceedings{gu2018universal,
  title={Universal Neural Machine Translation for Extremely Low Resource Languages},
  author={Gu, Jiatao and Hassan, Hany and Devlin, Jacob and Li, Victor OK},
  booktitle={NAACL-HLT},
  year={2018}
}

@inproceedings{adams2017cross,
  title={Cross-lingual word embeddings for low-resource language modeling},
  author={Adams, Oliver and Makarucha, Adam and Neubig, Graham and Bird, Steven and Cohn, Trevor},
  booktitle={Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers},
  pages={937--947},
  year={2017}
}

@misc{chen2021making,
      title={Towards Making the Most of Multilingual Pretraining for Zero-Shot Neural Machine Translation}, 
      author={Guanhua Chen and Shuming Ma and Yun Chen and Dongdong Zhang and Jia Pan and Wenping Wang and Furu Wei},
      year={2021},
      eprint={2110.08547},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{liu-etal-2020-multilingual-denoising_mbart,
    title = "Multilingual Denoising Pre-training for Neural Machine Translation",
    author = "Liu, Yinhan  and
      Gu, Jiatao  and
      Goyal, Naman  and
      Li, Xian  and
      Edunov, Sergey  and
      Ghazvininejad, Marjan  and
      Lewis, Mike  and
      Zettlemoyer, Luke",
    journal = "Transactions of the Association for Computational Linguistics",
    volume = "8",
    year = "2020",
    url = "https://aclanthology.org/2020.tacl-1.47",
    doi = "10.1162/tacl_a_00343",
    pages = "726--742",
    abstract = "This paper demonstrates that multilingual denoising pre-training produces significant performance gains across a wide variety of machine translation (MT) tasks. We present mBART{---}a sequence-to-sequence denoising auto-encoder pre-trained on large-scale monolingual corpora in many languages using the BART objective (Lewis et al., 2019). mBART is the first method for pre-training a complete sequence-to-sequence model by denoising full texts in multiple languages, whereas previous approaches have focused only on the encoder, decoder, or reconstructing parts of the text. Pre-training a complete model allows it to be directly fine-tuned for supervised (both sentence-level and document-level) and unsupervised machine translation, with no task- specific modifications. We demonstrate that adding mBART initialization produces performance gains in all but the highest-resource settings, including up to 12 BLEU points for low resource MT and over 5 BLEU points for many document-level and unsupervised models. We also show that it enables transfer to language pairs with no bi-text or that were not in the pre-training corpus, and present extensive analysis of which factors contribute the most to effective pre-training.1",
}
@inproceedings{pires2019multilingual_mbert,
  title={How Multilingual is Multilingual BERT?},
  author={Pires, Telmo and Schlinger, Eva and Garrette, Dan},
  booktitle={Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
  pages={4996--5001},
  year={2019}
}

@article{conneau2019cross_xlm,
  title={Cross-lingual language model pretraining},
  author={Conneau, Alexis and Lample, Guillaume},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  pages={7059--7069},
  year={2019}
}

@inproceedings{kondratyuk-straka-2019-75-universal-depdendency,
    title = "75 Languages, 1 Model: Parsing {U}niversal {D}ependencies Universally",
    author = "Kondratyuk, Dan  and
      Straka, Milan",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D19-1279",
    doi = "10.18653/v1/D19-1279",
    pages = "2779--2795",
    abstract = "We present UDify, a multilingual multi-task model capable of accurately predicting universal part-of-speech, morphological features, lemmas, and dependency trees simultaneously for all 124 Universal Dependencies treebanks across 75 languages. By leveraging a multilingual BERT self-attention model pretrained on 104 languages, we found that fine-tuning it on all datasets concatenated together with simple softmax classifiers for each UD task can meet or exceed state-of-the-art UPOS, UFeats, Lemmas, (and especially) UAS, and LAS scores, without requiring any recurrent or language-specific components. We evaluate UDify for multilingual learning, showing that low-resource languages benefit the most from cross-linguistic annotations. We also evaluate for zero-shot learning, with results suggesting that multilingual training provides strong UD predictions even for languages that neither UDify nor BERT have ever been trained on.",
}

@inproceedings{sennrich-etal-2016-improving,
    title = "Improving Neural Machine Translation Models with Monolingual Data",
    author = "Sennrich, Rico  and
      Haddow, Barry  and
      Birch, Alexandra",
    booktitle = "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2016",
    address = "Berlin, Germany",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P16-1009",
    doi = "10.18653/v1/P16-1009",
    pages = "86--96",
}

@inproceedings{sogaard-etal-2021-need_talk_random_splits,
    title = "We Need To Talk About Random Splits",
    author = "S{\o}gaard, Anders  and
      Ebert, Sebastian  and
      Bastings, Jasmijn  and
      Filippova, Katja",
    booktitle = "Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume",
    month = apr,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.eacl-main.156",
    pages = "1823--1832",
    abstract = "(CITATION) argued for using random splits rather than standard splits in NLP experiments. We argue that random splits, like standard splits, lead to overly optimistic performance estimates. We can also split data in biased or adversarial ways, e.g., training on short sentences and evaluating on long ones. Biased sampling has been used in domain adaptation to simulate real-world drift; this is known as the covariate shift assumption. In NLP, however, even worst-case splits, maximizing bias, often under-estimate the error observed on new samples of in-domain data, i.e., the data that models should minimally generalize to at test time. This invalidates the covariate shift assumption. Instead of using multiple random splits, future benchmarks should ideally include multiple, independent test sets instead; if infeasible, we argue that multiple biased splits leads to more realistic performance estimates than multiple random splits.",
}

@article{liu2021challenges,
  title={Challenges in Generalization in Open Domain Question Answering},
  author={Liu, Linqing and Lewis, Patrick and Riedel, Sebastian and Stenetorp, Pontus},
  journal={arXiv e-prints},
  pages={arXiv--2109},
  year={2021}
}

@inproceedings{lewis-etal-2021-question,
    title = "Question and Answer Test-Train Overlap in Open-Domain Question Answering Datasets",
    author = "Lewis, Patrick  and
      Stenetorp, Pontus  and
      Riedel, Sebastian",
    booktitle = "Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume",
    month = apr,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.eacl-main.86",
    pages = "1000--1008",
    abstract = "Ideally Open-Domain Question Answering models should exhibit a number of competencies, ranging from simply memorizing questions seen at training time, to answering novel question formulations with answers seen during training, to generalizing to completely novel questions with novel answers. However, single aggregated test set scores do not show the full picture of what capabilities models truly have. In this work, we perform a detailed study of the test sets of three popular open-domain benchmark datasets with respect to these competencies. We find that 30{\%} of test-set questions have a near-duplicate paraphrase in their corresponding train sets. In addition, we find that 60-70{\%} of answers in the test sets are also present in the train sets. Using these findings, we evaluate a variety of popular open-domain models to obtain greater insight into what extent they can generalize, and what drives their overall performance. We find that all models perform substantially worse on questions that cannot be memorized from train sets, with a mean absolute performance difference of 61{\%} between repeated and non-repeated data. Finally we show that simple nearest-neighbor models outperform a BART closed-book QA model, further highlighting the role that train set memorization plays in these benchmarks",
}

@misc{bommasani2021opportunities_foundations,
      title={On the Opportunities and Risks of Foundation Models}, 
      author={Rishi Bommasani and Drew A. Hudson and Ehsan Adeli and Russ Altman and Simran Arora and Sydney von Arx and Michael S. Bernstein and Jeannette Bohg and Antoine Bosselut and Emma Brunskill and Erik Brynjolfsson and Shyamal Buch and Dallas Card and Rodrigo Castellon and Niladri Chatterji and Annie Chen and Kathleen Creel and Jared Quincy Davis and Dora Demszky and Chris Donahue and Moussa Doumbouya and Esin Durmus and Stefano Ermon and John Etchemendy and Kawin Ethayarajh and Li Fei-Fei and Chelsea Finn and Trevor Gale and Lauren Gillespie and Karan Goel and Noah Goodman and Shelby Grossman and Neel Guha and Tatsunori Hashimoto and Peter Henderson and John Hewitt and Daniel E. Ho and Jenny Hong and Kyle Hsu and Jing Huang and Thomas Icard and Saahil Jain and Dan Jurafsky and Pratyusha Kalluri and Siddharth Karamcheti and Geoff Keeling and Fereshte Khani and Omar Khattab and Pang Wei Koh and Mark Krass and Ranjay Krishna and Rohith Kuditipudi and Ananya Kumar and Faisal Ladhak and Mina Lee and Tony Lee and Jure Leskovec and Isabelle Levent and Xiang Lisa Li and Xuechen Li and Tengyu Ma and Ali Malik and Christopher D. Manning and Suvir Mirchandani and Eric Mitchell and Zanele Munyikwa and Suraj Nair and Avanika Narayan and Deepak Narayanan and Ben Newman and Allen Nie and Juan Carlos Niebles and Hamed Nilforoshan and Julian Nyarko and Giray Ogut and Laurel Orr and Isabel Papadimitriou and Joon Sung Park and Chris Piech and Eva Portelance and Christopher Potts and Aditi Raghunathan and Rob Reich and Hongyu Ren and Frieda Rong and Yusuf Roohani and Camilo Ruiz and Jack Ryan and Christopher RÃ© and Dorsa Sadigh and Shiori Sagawa and Keshav Santhanam and Andy Shih and Krishnan Srinivasan and Alex Tamkin and Rohan Taori and Armin W. Thomas and Florian TramÃ¨r and Rose E. Wang and William Wang and Bohan Wu and Jiajun Wu and Yuhuai Wu and Sang Michael Xie and Michihiro Yasunaga and Jiaxuan You and Matei Zaharia and Michael Zhang and Tianyi Zhang and Xikun Zhang and Yuhui Zhang and Lucia Zheng and Kaitlyn Zhou and Percy Liang},
      year={2021},
      eprint={2108.07258},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{qiu2020pre,
  title={Pre-trained models for natural language processing: A survey},
  author={Qiu, Xipeng and Sun, Tianxiang and Xu, Yige and Shao, Yunfan and Dai, Ning and Huang, Xuanjing},
  journal={Science China Technological Sciences},
  pages={1--26},
  year={2020},
  publisher={Springer}
}

@article{sejnowski2020unreasonable,
  title={The unreasonable effectiveness of deep learning in artificial intelligence},
  author={Sejnowski, Terrence J},
  journal={Proceedings of the National Academy of Sciences},
  volume={117},
  number={48},
  pages={30033--30038},
  year={2020},
  publisher={National Acad Sciences}
}

@inproceedings{lyu-etal-2020-improving,
    title = "Improving Document-Level Sentiment Analysis with User and Product Context",
    author = "Lyu, Chenyang  and
      Foster, Jennifer  and
      Graham, Yvette",
    booktitle = "Proceedings of the 28th International Conference on Computational Linguistics",
    month = dec,
    year = "2020",
    address = "Barcelona, Spain (Online)",
    publisher = "International Committee on Computational Linguistics",
    url = "https://aclanthology.org/2020.coling-main.590",
    doi = "10.18653/v1/2020.coling-main.590",
    pages = "6724--6729",
    abstract = "Past work that improves document-level sentiment analysis by encoding user and product in- formation has been limited to considering only the text of the current review. We investigate incorporating additional review text available at the time of sentiment prediction that may prove meaningful for guiding prediction. Firstly, we incorporate all available historical review text belonging to the author of the review in question. Secondly, we investigate the inclusion of his- torical reviews associated with the current product (written by other users). We achieve this by explicitly storing representations of reviews written by the same user and about the same product and force the model to memorize all reviews for one particular user and product. Additionally, we drop the hierarchical architecture used in previous work to enable words in the text to directly attend to each other. Experiment results on IMDB, Yelp 2013 and Yelp 2014 datasets show improvement to state-of-the-art of more than 2 percentage points in the best case.",
}

@inproceedings{da2019cracking,
  title={Cracking the Contextual Commonsense Code: Understanding Commonsense Reasoning Aptitude of Deep Contextual Representations},
  author={Da, Jeff and Kasai, Jungo},
  booktitle={Proceedings of the First Workshop on Commonsense Inference in Natural Language Processing},
  pages={1--12},
  year={2019}
}

@inproceedings{liu2020k_kbert,
  title={K-bert: Enabling language representation with knowledge graph},
  author={Liu, Weijie and Zhou, Peng and Zhao, Zhe and Wang, Zhiruo and Ju, Qi and Deng, Haotang and Wang, Ping},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={34},
  number={03},
  pages={2901--2908},
  year={2020}
}

@article{wang2021kepler,
  title={KEPLER: A unified model for knowledge embedding and pre-trained language representation},
  author={Wang, Xiaozhi and Gao, Tianyu and Zhu, Zhaocheng and Zhang, Zhengyan and Liu, Zhiyuan and Li, Juanzi and Tang, Jian},
  journal={Transactions of the Association for Computational Linguistics},
  volume={9},
  pages={176--194},
  year={2021},
  publisher={MIT Press}
}

@article{yu2020jaket_knowledge_graph,
  title={Jaket: Joint pre-training of knowledge graph and language understanding},
  author={Yu, Donghan and Zhu, Chenguang and Yang, Yiming and Zeng, Michael},
  journal={arXiv preprint arXiv:2010.00796},
  year={2020}
}

@article{colon2021combining_structured_knowledge,
  title={Combining pre-trained language models and structured knowledge},
  author={Colon-Hernandez, Pedro and Havasi, Catherine and Alonso, Jason and Huggins, Matthew and Breazeal, Cynthia},
  journal={arXiv preprint arXiv:2101.12294},
  year={2021}
  
  }
  
@inproceedings{he-etal-2020-knowledge_bert,
    title = "{BERT}-{MK}: Integrating Graph Contextualized Knowledge into Pre-trained Language Models",
    author = "He, Bin  and
      Zhou, Di  and
      Xiao, Jinghui  and
      Jiang, Xin  and
      Liu, Qun  and
      Yuan, Nicholas Jing  and
      Xu, Tong",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2020",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.findings-emnlp.207",
    doi = "10.18653/v1/2020.findings-emnlp.207",
    pages = "2281--2290",
    abstract = "Complex node interactions are common in knowledge graphs (KGs), and these interactions can be considered as contextualized knowledge exists in the topological structure of KGs. Traditional knowledge representation learning (KRL) methods usually treat a single triple as a training unit, neglecting the usage of graph contextualized knowledge. To utilize these unexploited graph-level knowledge, we propose an approach to model subgraphs in a medical KG. Then, the learned knowledge is integrated with a pre-trained language model to do the knowledge generalization. Experimental results demonstrate that our model achieves the state-of-the-art performance on several medical NLP tasks, and the improvement above MedERNIE indicates that graph contextualized knowledge is beneficial.",
}

@inproceedings{zhang-etal-2019-ernie,
    title = "{ERNIE}: Enhanced Language Representation with Informative Entities",
    author = "Zhang, Zhengyan  and
      Han, Xu  and
      Liu, Zhiyuan  and
      Jiang, Xin  and
      Sun, Maosong  and
      Liu, Qun",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P19-1139",
    doi = "10.18653/v1/P19-1139",
    pages = "1441--1451",
    abstract = "Neural language representation models such as BERT pre-trained on large-scale corpora can well capture rich semantic patterns from plain text, and be fine-tuned to consistently improve the performance of various NLP tasks. However, the existing pre-trained language models rarely consider incorporating knowledge graphs (KGs), which can provide rich structured knowledge facts for better language understanding. We argue that informative entities in KGs can enhance language representation with external knowledge. In this paper, we utilize both large-scale textual corpora and KGs to train an enhanced language representation model (ERNIE), which can take full advantage of lexical, syntactic, and knowledge information simultaneously. The experimental results have demonstrated that ERNIE achieves significant improvements on various knowledge-driven tasks, and meanwhile is comparable with the state-of-the-art model BERT on other common NLP tasks. The code and datasets will be available in the future.",
}

@inproceedings{wang2018glue_benckmark,
  title={GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding},
  author={Wang, Alex and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel},
  booktitle={Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP},
  pages={353--355},
  year={2018}
}

@article{rogers2020primer_bert,
  title={A primer in bertology: What we know about how bert works},
  author={Rogers, Anna and Kovaleva, Olga and Rumshisky, Anna},
  journal={Transactions of the Association for Computational Linguistics},
  volume={8},
  pages={842--866},
  year={2020},
  publisher={MIT Press}
}

@article{mccann2017learned_covec_from_nmt,
  title={Learned in Translation: Contextualized Word Vectors},
  author={McCann, Bryan and Bradbury, James and Xiong, Caiming and Socher, Richard},
  journal={Advances in Neural Information Processing Systems},
  volume={30},
  year={2017}
}

@inproceedings{bert_structure_of_language,
    title = "What Does {BERT} Learn about the Structure of Language?",
    author = "Jawahar, Ganesh  and
      Sagot, Beno{\^\i}t  and
      Seddah, Djam{\'e}",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P19-1356",
    doi = "10.18653/v1/P19-1356",
    pages = "3651--3657",
    abstract = "BERT is a recent language representation model that has surprisingly performed well in diverse language understanding benchmarks. This result indicates the possibility that BERT networks capture structural information about language. In this work, we provide novel support for this claim by performing a series of experiments to unpack the elements of English language structure learned by BERT. Our findings are fourfold. BERT{'}s phrasal representation captures the phrase-level information in the lower layers. The intermediate layers of BERT compose a rich hierarchy of linguistic information, starting with surface features at the bottom, syntactic features in the middle followed by semantic features at the top. BERT requires deeper layers while tracking subject-verb agreement to handle long-term dependency problem. Finally, the compositional scheme underlying BERT mimics classical, tree-like structures.",
}
@book{Aho:72,
    author  = {Alfred V. Aho and Jeffrey D. Ullman},
    title   = {The Theory of Parsing, Translation and Compiling},
    year    = "1972",
    volume  = "1",
    publisher = {Prentice-Hall},
    address = {Englewood Cliffs, NJ}
}

@book{APA:83,
    author  = {{American Psychological Association}},
    title   = {Publications Manual},
    year    = "1983",
   publisher = {American Psychological Association},
   address = {Washington, DC}
}

@article{Chandra:81,
	author = {Ashok K. Chandra and Dexter C. Kozen and Larry J. Stockmeyer},
	year = "1981",
	title = {Alternation},
	journal = {Journal of the Association for Computing Machinery},
	volume = "28",
	number = "1",
	pages = "114--133",
	doi = "10.1145/322234.322243",
}



@book{Gusfield:97,
    author  = {Dan Gusfield},
    title   = {Algorithms on Strings, Trees and Sequences},
    year    = "1997",
    publisher = {Cambridge University Press},
    address = {Cambridge, UK}
}


@inproceedings{borsch2011,
	title={A particle filter algorithm for {B}ayesian wordsegmentation},
	author={Borschinger, Benjamin and Johnson, Mark and others},
	booktitle = {Proceedings of the Australasian Language Technology Association Workshop 2011},
	year={2011},
	address = {Canberra, Australia},
	pages = {10--18},
}


@article{ACM:83,
	author = {Association for Computing Machinery},
	year = "1983",
	journal = {Computing Reviews},
	volume = "24",
	number = "11",
	pages = "503--512",
}

@inproceedings{tang-etal-2015-document,
    title = "Document Modeling with Gated Recurrent Neural Network for Sentiment Classification",
    author = "Tang, Duyu  and
      Qin, Bing  and
      Liu, Ting",
    booktitle = "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing",
    month = sep,
    year = "2015",
    address = "Lisbon, Portugal",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D15-1167",
    doi = "10.18653/v1/D15-1167",
    pages = "1422--1432",
}

@inproceedings{tang-etal-2015-user-product,
    title = "Learning Semantic Representations of Users and Products for Document Level Sentiment Classification",
    author = "Tang, Duyu  and
      Qin, Bing  and
      Liu, Ting",
    booktitle = "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = jul,
    year = "2015",
    address = "Beijing, China",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P15-1098",
    doi = "10.3115/v1/P15-1098",
    pages = "1014--1023",
}

@inproceedings{ma-etal-2017-cascading,
    title = "Cascading Multiway Attentions for Document-level Sentiment Classification",
    author = "Ma, Dehong  and
      Li, Sujian  and
      Zhang, Xiaodong  and
      Wang, Houfeng  and
      Sun, Xu",
    booktitle = "Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = nov,
    year = "2017",
    address = "Taipei, Taiwan",
    publisher = "Asian Federation of Natural Language Processing",
    url = "https://www.aclweb.org/anthology/I17-1064",
    pages = "634--643",
    abstract = "Document-level sentiment classification aims to assign the user reviews a sentiment polarity. Previous methods either just utilized the document content without consideration of user and product information, or did not comprehensively consider what roles the three kinds of information play in text modeling. In this paper, to reasonably use all the information, we present the idea that user, product and their combination can all influence the generation of attentions to words and sentences, when judging the sentiment of a document. With this idea, we propose a cascading multiway attention (CMA) model, where multiple ways of using user and product information are cascaded to influence the generation of attentions on the word and sentence layers. Then, sentences and documents are well modeled by multiple representation vectors, which provide rich information for sentiment classification. Experiments on IMDB and Yelp datasets demonstrate the effectiveness of our model.",
}
@inproceedings{chen-etal-2016-neural-sentiment,
    title = "Neural Sentiment Classification with User and Product Attention",
    author = "Chen, Huimin  and
      Sun, Maosong  and
      Tu, Cunchao  and
      Lin, Yankai  and
      Liu, Zhiyuan",
    booktitle = "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2016",
    address = "Austin, Texas",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D16-1171",
    doi = "10.18653/v1/D16-1171",
    pages = "1650--1659",
}
@inproceedings{amplayo-2019-rethinking,
    title = "Rethinking Attribute Representation and Injection for Sentiment Classification",
    author = "Amplayo, Reinald Kim",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D19-1562",
    doi = "10.18653/v1/D19-1562",
    pages = "5602--5613",
    abstract = "Text attributes, such as user and product information in product reviews, have been used to improve the performance of sentiment classification models. The de facto standard method is to incorporate them as additional biases in the attention mechanism, and more performance gains are achieved by extending the model architecture. In this paper, we show that the above method is the least effective way to represent and inject attributes. To demonstrate this hypothesis, unlike previous models with complicated architectures, we limit our base model to a simple BiLSTM with attention classifier, and instead focus on how and where the attributes should be incorporated in the model. We propose to represent attributes as chunk-wise importance weight matrices and consider four locations in the model (i.e., embedding, encoding, attention, classifier) to inject attributes. Experiments show that our proposed method achieves significant improvements over the standard approach and that attention mechanism is the worst location to inject attributes, contradicting prior work. We also outperform the state-of-the-art despite our use of a simple base model. Finally, we show that these representations transfer well to other tasks. Model implementation and datasets are released here: https://github.com/rktamplayo/CHIM.",
}

@inproceedings{long-etal-2018-dual,
    title = "Dual Memory Network Model for Biased Product Review Classification",
    author = "Long, Yunfei  and
      Ma, Mingyu  and
      Lu, Qin  and
      Xiang, Rong  and
      Huang, Chu-Ren",
    booktitle = "Proceedings of the 9th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis",
    month = oct,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W18-6220",
    doi = "10.18653/v1/W18-6220",
    pages = "140--148",
    abstract = "In sentiment analysis (SA) of product reviews, both user and product information are proven to be useful. Current tasks handle user profile and product information in a unified model which may not be able to learn salient features of users and products effectively. In this work, we propose a dual user and product memory network (DUPMN) model to learn user profiles and product reviews using separate memory networks. Then, the two representations are used jointly for sentiment prediction. The use of separate models aims to capture user profiles and product information more effectively. Compared to state-of-the-art unified prediction models, the evaluations on three benchmark datasets, IMDB, Yelp13, and Yelp14, show that our dual learning model gives performance gain of 0.6{\%}, 1.2{\%}, and 0.9{\%}, respectively. The improvements are also deemed very significant measured by \textit{p-values}.",
}

@inproceedings{cikm19-memory,
  author    = {Zhigang Yuan and
               Fangzhao Wu and
               Junxin Liu and
               Chuhan Wu and
               Yongfeng Huang and
               Xing Xie},
  title     = {Neural Review Rating Prediction with User and Product Memory},
  booktitle = {Proceedings of the 28th {ACM} International Conference on Information and Knowledge Management, {CIKM} 2019, Beijing, China, November 3-7,2019},
  pages     = {2341--2344},
  year      = {2019}
}

@inproceedings{dou-2017-capturing,
    title = "Capturing User and Product Information for Document Level Sentiment Analysis with Deep Memory Network",
    author = "Dou, Zi-Yi",
    booktitle = "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing",
    month = sep,
    year = "2017",
    address = "Copenhagen, Denmark",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D17-1054",
    doi = "10.18653/v1/D17-1054",
    pages = "521--526",
    abstract = "Document-level sentiment classification is a fundamental problem which aims to predict a user{'}s overall sentiment about a product in a document. Several methods have been proposed to tackle the problem whereas most of them fail to consider the influence of users who express the sentiment and products which are evaluated. To address the issue, we propose a deep memory network for document-level sentiment classification which could capture the user and product information at the same time. To prove the effectiveness of our algorithm, we conduct experiments on IMDB and Yelp datasets and the results indicate that our model can achieve better performance than several existing methods.",
}

@inproceedings{kim-2014-convolutional,
    title = "Convolutional Neural Networks for Sentence Classification",
    author = "Kim, Yoon",
    booktitle = "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ({EMNLP})",
    month = oct,
    year = "2014",
    address = "Doha, Qatar",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D14-1181",
    doi = "10.3115/v1/D14-1181",
    pages = "1746--1751",
}


@article{doc-lvl-senti-analysis,
    author = {Xia, Rui and Xu, Feng and Yu, Jianfei and Qi, Yong and Cambria, Erik},
    year = {2015},
    month = {11},
    pages = {},
    title = {Polarity shift detection, elimination and ensemble: A three-stage model for document-level sentiment analysis},
    volume = {52},
    journal = {Information Processing \& Management},
    doi = {10.1016/j.ipm.2015.04.003}
}

@article{sentiment-survey,
  title={Sentiment analysis and opinion mining: a survey},
  author={Vinodhini, G and Chandrasekaran, RM},
  journal={International Journal},
  volume={2},
  number={6},
  pages={282--292},
  year={2012}
}

@inproceedings{emotions-from-text,
    title = "Emotions from Text: Machine Learning for Text-based Emotion Prediction",
    author = "Alm, Cecilia Ovesdotter  and
      Roth, Dan  and
      Sproat, Richard",
    booktitle = "Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing",
    month = oct,
    year = "2005",
    address = "Vancouver, British Columbia, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/H05-1073",
    pages = "579--586",
}
@article{senti-analysis-jacobs,
  title={Sentiment Analysis for Words and Fiction Characters From the Perspective of Computational (Neuro-)Poetics},
  author={Arthur M. Jacobs},
  journal={Front. Robotics and AI},
  year={2019},
  volume={6},
  pages={53},
  url={https://www.frontiersin.org/article/10.3389/frobt.2019.00053},
  DOI={10.3389/frobt.2019.00053}, 
}

@article{Chinese-SA,
 author = {Zhang, Changli and Zeng, Daniel and Li, Jiexun and Wang, Fei-Yue and Zuo, Wanli},
 title = {Sentiment Analysis of Chinese Documents: From Sentence to Document Level},
 journal = {J. Am. Soc. Inf. Sci. Technol.},
 issue_date = {December 2009},
 volume = {60},
 number = {12},
 month = dec,
 year = {2009},
 issn = {1532-2882},
 pages = {2474--2487},
 numpages = {14},
 url = {http://dx.doi.org/10.1002/asi.v60:12},
 doi = {10.1002/asi.v60:12},
 acmid = {1673010},
 publisher = {John Wiley \& Sons, Inc.},
 address = {New York, NY, USA},
 keywords = {data mining, knowledge bases, machine learning, non English language materials, sentiment analysis},
}


@inproceedings{bert,
    title = "{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    author = "Devlin, Jacob  and
      Chang, Ming-Wei  and
      Lee, Kenton  and
      Toutanova, Kristina",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/N19-1423",
    doi = "10.18653/v1/N19-1423",
    pages = "4171--4186",
    abstract = "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7{\%} (4.6{\%} absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",
}
@incollection{word2vec,
title = {Distributed Representations of Words and Phrases and their Compositionality},
author = {Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg S and Dean, Jeff},
booktitle = {Advances in Neural Information Processing Systems 26},
editor = {C. J. C. Burges and L. Bottou and M. Welling and Z. Ghahramani and K. Q. Weinberger},
pages = {3111--3119},
year = {2013},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf}
}

@inproceedings{ibm_knowledge,
    title = "{SLIDE} - a Sentiment Lexicon of Common Idioms",
    author = "Jochim, Charles  and
      Bonin, Francesca  and
      Bar-Haim, Roy  and
      Slonim, Noam",
    booktitle = "Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018)",
    month = may,
    year = "2018",
    address = "Miyazaki, Japan",
    publisher = "European Language Resources Association (ELRA)",
    url = "https://www.aclweb.org/anthology/L18-1379",
}

@inproceedings{bo_pang-SA,
 author = {Pang, Bo and Lee, Lillian and Vaithyanathan, Shivakumar},
 title = {Thumbs Up?: Sentiment Classification Using Machine Learning Techniques},
 booktitle = {Proceedings of the ACL-02 Conference on Empirical Methods in Natural Language Processing - Volume 10},
 series = {EMNLP '02},
 year = {2002},
 pages = {79--86},
 numpages = {8},
 url = {https://doi.org/10.3115/1118693.1118704},
 doi = {10.3115/1118693.1118704},
 acmid = {1118704},
 publisher = {Association for Computational Linguistics},
 address = {Stroudsburg, PA, USA},
} 

@article{fuzzy_huetter,
  title={Fuzzy typing for document management},
  author={Huettner, Alison and Subasic, Pero},
  journal={ACL 2000 Companion Volume: Tutorial Abstracts and Demonstration Notes},
  pages={26--27},
  year={2000}
}


@article{das_chen,
 ISSN = {00251909, 15265501},
 URL = {http://www.jstor.org/stable/20122297},
 abstract = {Extracting sentiment from text is a hard semantic problem. We develop a methodology for extracting small investor sentiment from stock message boards. The algorithm comprises different classifier algorithms coupled together by a voting scheme. Accuracy levels are similar to widely used Bayes classifiers, but false positives are lower and sentiment accuracy higher. Time series and cross-sectional aggregation of message information improves the quality of the resultant sentiment index, particularly in the presence of slang and ambiguity. Empirical applications evidence a relationship with stock values--tech-sector postings are related to stock index levels, and to volumes and volatility. The algorithms may be used to assess the impact on investor opinion of management announcements, press releases, third-party news, and regulatory changes.},
 author = {Sanjiv R. Das and Mike Y. Chen},
 journal = {Management Science},
 number = {9},
 pages = {1375--1388},
 publisher = {INFORMS},
 title = {Yahoo! For Amazon: Sentiment Extraction from Small Talk on the Web},
 volume = {53},
 year = {2007}
}

@inproceedings{tong2001operational,
  title={An operational system for detecting and tracking opinions in on-line discussion},
  author={Tong, Richard M},
  booktitle={Working Notes of the ACM SIGIR 2001 Workshop on Operational Text Classification},
  volume={1},
  number={6},
  year={2001}
}

@article{wiebe2004learning,
  title={Learning subjective language},
  author={Wiebe, Janyce and Wilson, Theresa and Bruce, Rebecca and Bell, Matthew and Martin, Melanie},
  journal={Computational linguistics},
  volume={30},
  number={3},
  pages={277--308},
  year={2004},
  publisher={MIT Press}
}

@inproceedings{pang_minimum_cuts,
 author = {Pang, Bo and Lee, Lillian},
 title = {A Sentimental Education: Sentiment Analysis Using Subjectivity Summarization Based on Minimum Cuts},
 booktitle = {Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics},
 series = {ACL '04},
 year = {2004},
 location = {Barcelona, Spain},
 articleno = {271},
 url = {https://doi.org/10.3115/1218955.1218990},
 doi = {10.3115/1218955.1218990},
 acmid = {1218990},
 publisher = {Association for Computational Linguistics},
 address = {Stroudsburg, PA, USA},
} 

@inproceedings{nakagawa2010dependency,
  title={Dependency tree-based sentiment classification using CRFs with hidden variables},
  author={Nakagawa, Tetsuji and Inui, Kentaro and Kurohashi, Sadao},
  booktitle={Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics},
  pages={786--794},
  year={2010},
  organization={Association for Computational Linguistics}
}
@inproceedings{crf,
 author = {Lafferty, John D. and McCallum, Andrew and Pereira, Fernando C. N.},
 title = {Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data},
 booktitle = {Proceedings of the Eighteenth International Conference on Machine Learning},
 series = {ICML '01},
 year = {2001},
 isbn = {1-55860-778-1},
 pages = {282--289},
 numpages = {8},
 url = {http://dl.acm.org/citation.cfm?id=645530.655813},
 acmid = {655813},
 publisher = {Morgan Kaufmann Publishers Inc.},
 address = {San Francisco, CA, USA},
} 

@inproceedings{ibm_cnn,
    title = "Deep Convolutional Neural Networks for Sentiment Analysis of Short Texts",
    author = "dos Santos, C{\'\i}cero  and
      Gatti, Ma{\'\i}ra",
    booktitle = "Proceedings of {COLING} 2014, the 25th International Conference on Computational Linguistics: Technical Papers",
    month = aug,
    year = "2014",
    address = "Dublin, Ireland",
    publisher = "Dublin City University and Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/C14-1008",
    pages = "69--78",
}

@inproceedings{socher-etal-2013-recursive,
    title = "Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank",
    author = "Socher, Richard  and
      Perelygin, Alex  and
      Wu, Jean  and
      Chuang, Jason  and
      Manning, Christopher D.  and
      Ng, Andrew  and
      Potts, Christopher",
    booktitle = "Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing",
    month = oct,
    year = "2013",
    address = "Seattle, Washington, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D13-1170",
    pages = "1631--1642",
}
@inproceedings{wang2016attention,
  title={Attention-based LSTM for aspect-level sentiment classification},
  author={Wang, Yequan and Huang, Minlie and Zhao, Li and others},
  booktitle={Proceedings of the 2016 conference on empirical methods in natural language processing},
  pages={606--615},
  year={2016}
}
@inproceedings{zhou2016attention,
  title={Attention-based LSTM network for cross-lingual sentiment classification},
  author={Zhou, Xinjie and Wan, Xiaojun and Xiao, Jianguo},
  booktitle={Proceedings of the 2016 conference on empirical methods in natural language processing},
  pages={247--256},
  year={2016}
}
@inproceedings{elmo,
  title={Deep contextualized word representations},
  author={Peters, Matthew E and Neumann, Mark and Iyyer, Mohit and Gardner, Matt and Clark, Christopher and Lee, Kenton and Zettlemoyer, Luke},
  booktitle={Proceedings of NAACL-HLT},
  pages={2227--2237},
  year={2018}
}

@article{gpt,
  title={Improving language understanding by generative pre-training},
  author={Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya},
  year="2018",
  booktitle=""
}

@inproceedings{jawahar-etal-2019-bert,
    title = "What Does {BERT} Learn about the Structure of Language?",
    author = "Jawahar, Ganesh  and
      Sagot, Beno{\^\i}t  and
      Seddah, Djam{\'e}",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P19-1356",
    doi = "10.18653/v1/P19-1356",
    pages = "3651--3657",
    abstract = "BERT is a recent language representation model that has surprisingly performed well in diverse language understanding benchmarks. This result indicates the possibility that BERT networks capture structural information about language. In this work, we provide novel support for this claim by performing a series of experiments to unpack the elements of English language structure learned by BERT. Our findings are fourfold. BERT{'}s phrasal representation captures the phrase-level information in the lower layers. The intermediate layers of BERT compose a rich hierarchy of linguistic information, starting with surface features at the bottom, syntactic features in the middle followed by semantic features at the top. BERT requires deeper layers while tracking subject-verb agreement to handle long-term dependency problem. Finally, the compositional scheme underlying BERT mimics classical, tree-like structures.",
}

@inproceedings{Wolf2019HuggingFacesTS,
    title = "Transformers: State-of-the-Art Natural Language Processing",
    author = "Wolf, Thomas  and
      Debut, Lysandre  and
      Sanh, Victor  and
      Chaumond, Julien  and
      Delangue, Clement  and
      Moi, Anthony  and
      Cistac, Pierric  and
      Rault, Tim  and
      Louf, Remi  and
      Funtowicz, Morgan  and
      Davison, Joe  and
      Shleifer, Sam  and
      von Platen, Patrick  and
      Ma, Clara  and
      Jernite, Yacine  and
      Plu, Julien  and
      Xu, Canwen  and
      Le Scao, Teven  and
      Gugger, Sylvain  and
      Drame, Mariama  and
      Lhoest, Quentin  and
      Rush, Alexander",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations",
    month = oct,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-demos.6",
    doi = "10.18653/v1/2020.emnlp-demos.6",
    pages = "38--45",
    abstract = "Recent progress in natural language processing has been driven by advances in both model architecture and model pretraining. Transformer architectures have facilitated building higher-capacity models and pretraining has made it possible to effectively utilize this capacity for a wide variety of tasks. Transformers is an open-source library with the goal of opening up these advances to the wider machine learning community. The library consists of carefully engineered state-of-the art Transformer architectures under a unified API. Backing this library is a curated collection of pretrained models made by and available for the community. Transformers is designed to be extensible by researchers, simple for practitioners, and fast and robust in industrial deployments. The library is available at https://github.com/huggingface/transformers.",
}

@article{scikit-learn,
 title={Scikit-learn: Machine Learning in {P}ython},
 author={Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V.
         and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P.
         and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and
         Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},
 journal={Journal of Machine Learning Research},
 volume={12},
 pages={2825--2830},
 year={2011}
}

@Article{Cortes1995_svm,
author="Cortes, Corinna
and Vapnik, Vladimir",
title="Support-vector networks",
journal="Machine Learning",
year="1995",
month="Sep",
day="01",
volume="20",
number="3",
pages="273--297",
abstract="Thesupport-vector network is a new learning machine for two-group classification problems. The machine conceptually implements the following idea: input vectors are non-linearly mapped to a very high-dimension feature space. In this feature space a linear decision surface is constructed. Special properties of the decision surface ensures high generalization ability of the learning machine. The idea behind the support-vector network was previously implemented for the restricted case where the training data can be separated without errors. We here extend this result to non-separable training data.",
issn="1573-0565",
doi="10.1007/BF00994018",
url="https://doi.org/10.1007/BF00994018"
}

@inproceedings{agarwal2011sentiment,
  title={Sentiment analysis of twitter data},
  author={Agarwal, Apoorv and Xie, Boyi and Vovsha, Ilia and Rambow, Owen and Passonneau, Rebecca},
  booktitle={Proceedings of the Workshop on Language in Social Media (LSM 2011)},
  pages={30--38},
  year={2011}
}

@inproceedings{vanzo2014context,
  title={A context-based model for sentiment analysis in twitter},
  author={Vanzo, Andrea and Croce, Danilo and Basili, Roberto},
  booktitle={Proceedings of coling 2014, the 25th international conference on computational linguistics: Technical papers},
  pages={2345--2354},
  year={2014}
}

@inproceedings{cortes2009learning,
  title={Learning non-linear combinations of kernels},
  author={Cortes, Corinna and Mohri, Mehryar and Rostamizadeh, Afshin},
  booktitle={Advances in neural information processing systems},
  pages={396--404},
  year={2009}
}

@inproceedings{kaljahi-jennifer-2016-detecting,
    title = "Detecting Opinion Polarities using Kernel Methods",
    author = "Kaljahi, Rasoul  and
      Foster, Jennifer",
    booktitle = "Proceedings of the Workshop on Computational Modeling of People{'}s Opinions, Personality, and Emotions in Social Media ({PEOPLES})",
    month = dec,
    year = "2016",
    address = "Osaka, Japan",
    publisher = "The COLING 2016 Organizing Committee",
    url = "https://www.aclweb.org/anthology/W16-4307",
    pages = "60--69",
    abstract = "We investigate the application of kernel methods to representing both structural and lexical knowledge for predicting polarity of opinions in consumer product review. We introduce any-gram kernels which model lexical information in a significantly faster way than the traditional n-gram features, while capturing all possible orders of n-grams n in a sequence without the need to explicitly present a pre-specified set of such orders. We also present an effective format to represent constituency and dependency structure together with aspect terms and sentiment polarity scores. Furthermore, we modify the traditional tree kernel function to compute the similarity based on word embedding vectors instead of exact string match and present experiments using the new models.",
}

@book{toolan2012narrative,
  title={Narrative: A critical linguistic introduction},
  author={Toolan, Michael},
  year={2012},
  publisher={Routledge}
}

@phdthesis{alm2008affect,
  title={Affect in Text and Speech},
  author={Alm, Cecilia},
  year={2008},
  school={University of Illinois at Urbana-Champaign}
}


@inproceedings{attention_bert_stanford,
    title = "What Does {BERT} Look at? An Analysis of {BERT}{'}s Attention",
    author = "Clark, Kevin  and
      Khandelwal, Urvashi  and
      Levy, Omer  and
      Manning, Christopher D.",
    booktitle = "Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP",
    month = aug,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W19-4828",
    doi = "10.18653/v1/W19-4828",
    pages = "276--286",
    abstract = "Large pre-trained neural networks such as BERT have had great recent success in NLP, motivating a growing body of research investigating what aspects of language they are able to learn from unlabeled data. Most recent analysis has focused on model outputs (e.g., language model surprisal) or internal vector representations (e.g., probing classifiers). Complementary to these works, we propose methods for analyzing the attention mechanisms of pre-trained models and apply them to BERT. BERT{'}s attention heads exhibit patterns such as attending to delimiter tokens, specific positional offsets, or broadly attending over the whole sentence, with heads in the same layer often exhibiting similar behaviors. We further show that certain attention heads correspond well to linguistic notions of syntax and coreference. For example, we find heads that attend to the direct objects of verbs, determiners of nouns, objects of prepositions, and coreferent mentions with remarkably high accuracy. Lastly, we propose an attention-based probing classifier and use it to further demonstrate that substantial syntactic information is captured in BERT{'}s attention.",
}

@Inbook{bing-liu-book,
author="Zhang, Lei
and Liu, Bing",
title="Sentiment Analysis and Opinion Mining",
bookTitle="Encyclopedia of Machine Learning and Data Mining",
year="2017",
publisher="Springer US",
address="Boston, MA",
pages="1152--1161",
abstract="With the rapid growth of social media, sentiment analysis, also called opinion mining, has become one of the most active research areas in natural language processing. Its application is also widespread, from business services to political campaigns. This article gives an introduction to this important area and presents some recent developments.",
isbn="978-1-4899-7687-1",
doi="10.1007/978-1-4899-7687-1_907",
url="https://doi.org/10.1007/978-1-4899-7687-1_907"
}

@inproceedings{amplayo-etal-2018-cold,
    title = "Cold-Start Aware User and Product Attention for Sentiment Classification",
    author = "Amplayo, Reinald Kim  and
      Kim, Jihyeok  and
      Sung, Sua  and
      Hwang, Seung-won",
    booktitle = "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2018",
    address = "Melbourne, Australia",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P18-1236",
    doi = "10.18653/v1/P18-1236",
    pages = "2535--2544",
    abstract = "The use of user/product information in sentiment analysis is important, especially for cold-start users/products, whose number of reviews are very limited. However, current models do not deal with the cold-start problem which is typical in review websites. In this paper, we present Hybrid Contextualized Sentiment Classifier (HCSC), which contains two modules: (1) a fast word encoder that returns word vectors embedded with short and long range dependency features; and (2) Cold-Start Aware Attention (CSAA), an attention mechanism that considers the existence of cold-start problem when attentively pooling the encoded word vectors. HCSC introduces shared vectors that are constructed from similar users/products, and are used when the original distinct vectors do not have sufficient information (i.e. cold-start). This is decided by a frequency-guided selective gate vector. Our experiments show that in terms of RMSE, HCSC performs significantly better when compared with on famous datasets, despite having less complexity, and thus can be trained much faster. More importantly, our model performs significantly better than previous models when the training data is sparse and has cold-start problems.",
}


@inproceedings{aaai-18-jiajun-chen,
  title={Improving review representations with user attention and product attention for sentiment classification},
  author={Wu, Zhen and Dai, Xin-Yu and Yin, Cunyan and Huang, Shujian and Chen, Jiajun},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={32},
  year={2018}
}

@inproceedings{wang2022effective,
  title={Effective Rating Prediction Using an Attention-Based User Review Sentiment Model},
  author={Wang, Xi and Ounis, Iadh and Macdonald, Craig},
  booktitle={European Conference on Information Retrieval},
  pages={487--501},
  year={2022},
  organization={Springer}
}

@inproceedings{xue-etal-2021-mt5,
    title = "m{T}5: A Massively Multilingual Pre-trained Text-to-Text Transformer",
    author = "Xue, Linting  and
      Constant, Noah  and
      Roberts, Adam  and
      Kale, Mihir  and
      Al-Rfou, Rami  and
      Siddhant, Aditya  and
      Barua, Aditya  and
      Raffel, Colin",
    booktitle = "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.naacl-main.41",
    doi = "10.18653/v1/2021.naacl-main.41",
    pages = "483--498",
    abstract = "The recent {``}Text-to-Text Transfer Transformer{''} (T5) leveraged a unified text-to-text format and scale to attain state-of-the-art results on a wide variety of English-language NLP tasks. In this paper, we introduce mT5, a multilingual variant of T5 that was pre-trained on a new Common Crawl-based dataset covering 101 languages. We detail the design and modified training of mT5 and demonstrate its state-of-the-art performance on many multilingual benchmarks. We also describe a simple technique to prevent {``}accidental translation{''} in the zero-shot setting, where a generative model chooses to (partially) translate its prediction into the wrong language. All of the code and model checkpoints used in this work are publicly available.",
}

@inproceedings{yang-etal-2018-hotpotqa,
    title = "{H}otpot{QA}: A Dataset for Diverse, Explainable Multi-hop Question Answering",
    author = "Yang, Zhilin  and
      Qi, Peng  and
      Zhang, Saizheng  and
      Bengio, Yoshua  and
      Cohen, William  and
      Salakhutdinov, Ruslan  and
      Manning, Christopher D.",
    booktitle = "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
    month = oct # "-" # nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D18-1259",
    doi = "10.18653/v1/D18-1259",
    pages = "2369--2380",
    abstract = "Existing question answering (QA) datasets fail to train QA systems to perform complex reasoning and provide explanations for answers. We introduce HotpotQA, a new dataset with 113k Wikipedia-based question-answer pairs with four key features: (1) the questions require finding and reasoning over multiple supporting documents to answer; (2) the questions are diverse and not constrained to any pre-existing knowledge bases or knowledge schemas; (3) we provide sentence-level supporting facts required for reasoning, allowing QA systems to reason with strong supervision and explain the predictions; (4) we offer a new type of factoid comparison questions to test QA systems{'} ability to extract relevant facts and perform necessary comparison. We show that HotpotQA is challenging for the latest QA systems, and the supporting facts enable models to improve performance and make explainable predictions.",
}

@article{yang2019xlnet,
  title={Xlnet: Generalized autoregressive pretraining for language understanding},
  author={Yang, Zhilin and Dai, Zihang and Yang, Yiming and Carbonell, Jaime and Salakhutdinov, Russ R and Le, Quoc V},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}
@inproceedings{transformer-xl,
    title = "Transformer-{XL}: Attentive Language Models beyond a Fixed-Length Context",
    author = "Dai, Zihang  and
      Yang, Zhilin  and
      Yang, Yiming  and
      Carbonell, Jaime  and
      Le, Quoc  and
      Salakhutdinov, Ruslan",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P19-1285",
    doi = "10.18653/v1/P19-1285",
    pages = "2978--2988",
    abstract = "Transformers have a potential of learning longer-term dependency, but are limited by a fixed-length context in the setting of language modeling. We propose a novel neural architecture Transformer-XL that enables learning dependency beyond a fixed length without disrupting temporal coherence. It consists of a segment-level recurrence mechanism and a novel positional encoding scheme. Our method not only enables capturing longer-term dependency, but also resolves the context fragmentation problem. As a result, Transformer-XL learns dependency that is 80{\%} longer than RNNs and 450{\%} longer than vanilla Transformers, achieves better performance on both short and long sequences, and is up to 1,800+ times faster than vanilla Transformers during evaluation. Notably, we improve the state-of-the-art results of bpc/perplexity to 0.99 on enwiki8, 1.08 on text8, 18.3 on WikiText-103, 21.8 on One Billion Word, and 54.5 on Penn Treebank (without finetuning). When trained only on WikiText-103, Transformer-XL manages to generate reasonably coherent, novel text articles with thousands of tokens. Our code, pretrained models, and hyperparameters are available in both Tensorflow and PyTorch.",
}

@inproceedings{pontiki-etal-2014-semeval,
    title = "{S}em{E}val-2014 Task 4: Aspect Based Sentiment Analysis",
    author = "Pontiki, Maria  and
      Galanis, Dimitris  and
      Pavlopoulos, John  and
      Papageorgiou, Harris  and
      Androutsopoulos, Ion  and
      Manandhar, Suresh",
    booktitle = "Proceedings of the 8th International Workshop on Semantic Evaluation ({S}em{E}val 2014)",
    month = aug,
    year = "2014",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/S14-2004",
    doi = "10.3115/v1/S14-2004",
    pages = "27--35",
}


@software{spacy,
  author = {Honnibal, Matthew and Montani, Ines and Van Landeghem, Sofie and Boyd, Adriane},
  title = {{spaCy: Industrial-strength Natural Language Processing in Python}},
  year = 2020,
  publisher = {Zenodo},
  doi = {10.5281/zenodo.1212303},
  url = {https://doi.org/10.5281/zenodo.1212303}
}
@inproceedings{kang-etal-2020-handling,
    title = "Handling Anomalies of Synthetic Questions in Unsupervised Question Answering",
    author = "Hong, Giwon  and
      Kang, Junmo  and
      Lim, Doyeon  and
      Myaeng, Sung-Hyon",
    booktitle = "Proceedings of the 28th International Conference on Computational Linguistics",
    month = dec,
    year = "2020",
    address = "Barcelona, Spain (Online)",
    publisher = "International Committee on Computational Linguistics",
    url = "https://aclanthology.org/2020.coling-main.306",
    doi = "10.18653/v1/2020.coling-main.306",
    pages = "3441--3448",
    abstract = "Advances in Question Answering (QA) research require additional datasets for new domains, languages, and types of questions, as well as for performance increases. Human creation of a QA dataset like SQuAD, however, is expensive. As an alternative, an unsupervised QA approach has been proposed so that QA training data can be generated automatically. However, the performance of unsupervised QA is much lower than that of supervised QA models. We identify two anomalies in the automatically generated questions and propose how they can be mitigated. We show our approach helps improve unsupervised QA significantly across a number of QA tasks.",
}

@inproceedings{wang2016machine_match-lstm,
  title={Machine comprehension using match-LSTM and answer pointer.(2017)},
  author={Wang, Shuohang and Jiang, Jing},
  booktitle={ICLR 2017: International Conference on Learning Representations, Toulon, France, April 24-26: Proceedings},
  pages={1--15}
}

@inproceedings{dzendzik2021english_qa-dataset,
      title={English Machine Reading Comprehension Datasets: A Survey}, 
      author={Daria Dzendzik and Carl Vogel and Jennifer Foster},
      year={2021},
      booktitle={Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
      publisher={Association for Computational Linguistics}
}

@article{seo2016bidirectional_bidaf,
  title={Bidirectional attention flow for machine comprehension},
  author={Seo, Minjoon and Kembhavi, Aniruddha and Farhadi, Ali and Hajishirzi, Hannaneh},
  journal={ICLR},
  year={2017}
}

@inproceedings{saha-etal-2018-duorc,
    title = "{D}uo{RC}: Towards Complex Language Understanding with Paraphrased Reading Comprehension",
    author = "Saha, Amrita  and
      Aralikatte, Rahul  and
      Khapra, Mitesh M.  and
      Sankaranarayanan, Karthik",
    booktitle = "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2018",
    address = "Melbourne, Australia",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P18-1156",
    doi = "10.18653/v1/P18-1156",
    pages = "1683--1693",
    abstract = "We propose DuoRC, a novel dataset for Reading Comprehension (RC) that motivates several new challenges for neural approaches in language understanding beyond those offered by existing RC datasets. DuoRC contains 186,089 unique question-answer pairs created from a collection of 7680 pairs of movie plots where each pair in the collection reflects two versions of the same movie - one from Wikipedia and the other from IMDb - written by two different authors. We asked crowdsourced workers to create questions from one version of the plot and a different set of workers to extract or synthesize answers from the other version. This unique characteristic of DuoRC where questions and answers are created from different versions of a document narrating the same underlying story, ensures by design, that there is very little lexical overlap between the questions created from one version and the segments containing the answer in the other version. Further, since the two versions have different levels of plot detail, narration style, vocabulary, etc., answering questions from the second version requires deeper language understanding and incorporating external background knowledge. Additionally, the narrative style of passages arising from movie plots (as opposed to typical descriptive passages in existing datasets) exhibits the need to perform complex reasoning over events across multiple sentences. Indeed, we observe that state-of-the-art neural RC models which have achieved near human performance on the SQuAD dataset, even when coupled with traditional NLP techniques to address the challenges presented in DuoRC exhibit very poor performance (F1 score of 37.42{\%} on DuoRC v/s 86{\%} on SQuAD dataset). This opens up several interesting research avenues wherein DuoRC could complement other RC datasets to explore novel neural approaches for studying language understanding.",
}

@article{bio_asq,
  title={An overview of the BIOASQ large-scale biomedical semantic indexing and question answering competition},
  author={Tsatsaronis, George and Balikas, Georgios and Malakasiotis, Prodromos and Partalas, Ioannis and Zschunke, Matthias and Alvers, Michael R and Weissenborn, Dirk and Krithara, Anastasia and Petridis, Sergios and Polychronopoulos, Dimitris and others},
  journal={BMC bioinformatics},
  volume={16},
  number={1},
  pages={1--28},
  year={2015},
  publisher={BioMed Central}
}

@inproceedings{Gardner2017AllenNLP,
    title = "{A}llen{NLP}: A Deep Semantic Natural Language Processing Platform",
    author = "Gardner, Matt  and
      Grus, Joel  and
      Neumann, Mark  and
      Tafjord, Oyvind  and
      Dasigi, Pradeep  and
      Liu, Nelson F.  and
      Peters, Matthew  and
      Schmitz, Michael  and
      Zettlemoyer, Luke",
    booktitle = "Proceedings of Workshop for {NLP} Open Source Software ({NLP}-{OSS})",
    month = jul,
    year = "2018",
    address = "Melbourne, Australia",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W18-2501",
    doi = "10.18653/v1/W18-2501",
    pages = "1--6",
    abstract = "Modern natural language processing (NLP) research requires writing code. Ideally this code would provide a precise definition of the approach, easy repeatability of results, and a basis for extending the research. However, many research codebases bury high-level parameters under implementation details, are challenging to run and debug, and are difficult enough to extend that they are more likely to be rewritten. This paper describes AllenNLP, a library for applying deep learning methods to NLP research that addresses these issues with easy-to-use command-line tools, declarative configuration-driven experiments, and modular NLP abstractions. AllenNLP has already increased the rate of research experimentation and the sharing of NLP components at the Allen Institute for Artificial Intelligence, and we are working to have the same impact across the field.",
}

@inproceedings{chen2020reinforcement,
  title={Reinforcement Learning Based Graph-to-Sequence Model for Natural Question Generation},
  author={Chen, Yu and Wu, Lingfei and Zaki, Mohammed J},
  booktitle={International Conference on Learning Representations},
  year={2019}
}

@inproceedings{fisch2019mrqa,
    title={{MRQA} 2019 Shared Task: Evaluating Generalization in Reading Comprehension},
    author={Adam Fisch and Alon Talmor and Robin Jia and Minjoon Seo and Eunsol Choi and Danqi Chen},
    booktitle={Proceedings of 2nd Machine Reading for Reading Comprehension (MRQA) Workshop at EMNLP},
    year={2019},
}

 @inproceedings{
adamw,
title={Decoupled Weight Decay Regularization},
author={Ilya Loshchilov and Frank Hutter},
booktitle={International Conference on Learning Representations},
year={2019},
url={https://openreview.net/forum?id=Bkg6RiCqY7},
}



@article{kwiatkowski-etal-2019-naturalquestions-nq,
    title = "Natural Questions: A Benchmark for Question Answering Research",
    author = "Kwiatkowski, Tom  and
      Palomaki, Jennimaria  and
      Redfield, Olivia  and
      Collins, Michael  and
      Parikh, Ankur  and
      Alberti, Chris  and
      Epstein, Danielle  and
      Polosukhin, Illia  and
      Devlin, Jacob  and
      Lee, Kenton  and
      Toutanova, Kristina  and
      Jones, Llion  and
      Kelcey, Matthew  and
      Chang, Ming-Wei  and
      Dai, Andrew M.  and
      Uszkoreit, Jakob  and
      Le, Quoc  and
      Petrov, Slav",
    journal = "Transactions of the Association for Computational Linguistics",
    volume = "7",
    month = mar,
    year = "2019",
    url = "https://www.aclweb.org/anthology/Q19-1026",
    doi = "10.1162/tacl_a_00276",
    pages = "452--466",
    abstract = "We present the Natural Questions corpus, a question answering data set. Questions consist of real anonymized, aggregated queries issued to the Google search engine. An annotator is presented with a question along with a Wikipedia page from the top 5 search results, and annotates a long answer (typically a paragraph) and a short answer (one or more entities) if present on the page, or marks null if no long/short answer is present. The public release consists of 307,373 training examples with single annotations; 7,830 examples with 5-way annotations for development data; and a further 7,842 examples with 5-way annotated sequestered as test data. We present experiments validating quality of the data. We also describe analysis of 25-way annotations on 302 examples, giving insights into human variability on the annotation task. We introduce robust metrics for the purposes of evaluating question answering systems; demonstrate high human upper bounds on these metrics; and establish baseline results using competitive methods drawn from related literature.",
}

@article{marquez-etal-2008-special-SRL,
    title = "Special Issue Introduction: Semantic Role Labeling: An Introduction to the Special Issue",
    author = "M{\`a}rquez, Llu{\'\i}s  and
      Carreras, Xavier  and
      Litkowski, Kenneth C.  and
      Stevenson, Suzanne",
    journal = "Computational Linguistics",
    volume = "34",
    number = "2",
    year = "2008",
    url = "https://www.aclweb.org/anthology/J08-2001",
    doi = "10.1162/coli.2008.34.2.145",
    pages = "145--159",
}

@inproceedings{artetxe2018unsupervised-unmt,
  title={Unsupervised Neural Machine Translation},
  author={Artetxe, Mikel and Labaka, Gorka and Agirre, Eneko and Cho, Kyunghyun},
  booktitle={International Conference on Learning Representations},
  year={2018}
}

@inproceedings{banerjee-lavie-2005-meteor,
    title = "{METEOR}: An Automatic Metric for {MT} Evaluation with Improved Correlation with Human Judgments",
    author = "Banerjee, Satanjeev  and
      Lavie, Alon",
    booktitle = "Proceedings of the {ACL} Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization",
    month = jun,
    year = "2005",
    address = "Ann Arbor, Michigan",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W05-0909",
    pages = "65--72",
}

@inproceedings{callison-burch-etal-2006-evaluating,
    title = "Re-evaluating the Role of {B}leu in Machine Translation Research",
    author = "Callison-Burch, Chris  and
      Osborne, Miles  and
      Koehn, Philipp",
    booktitle = "11th Conference of the {E}uropean Chapter of the Association for Computational Linguistics",
    month = apr,
    year = "2006",
    address = "Trento, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/E06-1032",
}

@inproceedings{song2019mass,
  title={MASS: Masked Sequence to Sequence Pre-training for Language Generation},
  author={Song, Kaitao and Tan, Xu and Qin, Tao and Lu, Jianfeng and Liu, Tie-Yan},
  booktitle={International Conference on Machine Learning},
  pages={5926--5936},
  year={2019},
  organization={PMLR}
}

@inproceedings{joshi-etal-2017-triviaqa,
    title = "{T}rivia{QA}: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension",
    author = "Joshi, Mandar  and
      Choi, Eunsol  and
      Weld, Daniel  and
      Zettlemoyer, Luke",
    booktitle = "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2017",
    address = "Vancouver, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P17-1147",
    doi = "10.18653/v1/P17-1147",
    pages = "1601--1611",
    abstract = "We present TriviaQA, a challenging reading comprehension dataset containing over 650K question-answer-evidence triples. TriviaQA includes 95K question-answer pairs authored by trivia enthusiasts and independently gathered evidence documents, six per question on average, that provide high quality distant supervision for answering the questions. We show that, in comparison to other recently introduced large-scale datasets, TriviaQA (1) has relatively complex, compositional questions, (2) has considerable syntactic and lexical variability between questions and corresponding answer-evidence sentences, and (3) requires more cross sentence reasoning to find answers. We also present two baseline algorithms: a feature-based classifier and a state-of-the-art neural network, that performs well on SQuAD reading comprehension. Neither approach comes close to human performance (23{\%} and 40{\%} vs. 80{\%}), suggesting that TriviaQA is a challenging testbed that is worth significant future study.",
    }

@inproceedings{lewis-etal-2020-bart,
    title = "{BART}: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension",
    author = "Lewis, Mike  and
      Liu, Yinhan  and
      Goyal, Naman  and
      Ghazvininejad, Marjan  and
      Mohamed, Abdelrahman  and
      Levy, Omer  and
      Stoyanov, Veselin  and
      Zettlemoyer, Luke",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.acl-main.703",
    doi = "10.18653/v1/2020.acl-main.703",
    pages = "7871--7880",
    abstract = "We present BART, a denoising autoencoder for pretraining sequence-to-sequence models. BART is trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text. It uses a standard Tranformer-based neural machine translation architecture which, despite its simplicity, can be seen as generalizing BERT (due to the bidirectional encoder), GPT (with the left-to-right decoder), and other recent pretraining schemes. We evaluate a number of noising approaches, finding the best performance by both randomly shuffling the order of sentences and using a novel in-filling scheme, where spans of text are replaced with a single mask token. BART is particularly effective when fine tuned for text generation but also works well for comprehension tasks. It matches the performance of RoBERTa on GLUE and SQuAD, and achieves new state-of-the-art results on a range of abstractive dialogue, question answering, and summarization tasks, with gains of up to 3.5 ROUGE. BART also provides a 1.1 BLEU increase over a back-translation system for machine translation, with only target language pretraining. We also replicate other pretraining schemes within the BART framework, to understand their effect on end-task performance.",
}

@inproceedings{tang-etal-2018-learning,
    title = "Learning to Collaborate for Question Answering and Asking",
    author = "Tang, Duyu  and
      Duan, Nan  and
      Yan, Zhao  and
      Zhang, Zhirui  and
      Sun, Yibo  and
      Liu, Shujie  and
      Lv, Yuanhua  and
      Zhou, Ming",
    booktitle = "Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)",
    month = jun,
    year = "2018",
    address = "New Orleans, Louisiana",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/N18-1141",
    doi = "10.18653/v1/N18-1141",
    pages = "1564--1574",
    abstract = "Question answering (QA) and question generation (QG) are closely related tasks that could improve each other; however, the connection of these two tasks is not well explored in literature. In this paper, we give a systematic study that seeks to leverage the connection to improve both QA and QG. We present a training algorithm that generalizes both Generative Adversarial Network (GAN) and Generative Domain-Adaptive Nets (GDAN) under the question answering scenario. The two key ideas are improving the QG model with QA through incorporating additional QA-specific signal as the loss function, and improving the QA model with QG through adding artificially generated training instances. We conduct experiments on both document based and knowledge based question answering tasks. We have two main findings. Firstly, the performance of a QG model (e.g in terms of BLEU score) could be easily improved by a QA model via policy gradient. Secondly, directly applying GAN that regards all the generated questions as negative instances could not improve the accuracy of the QA model. Learning when to regard generated questions as positive instances could bring performance boost.",
}

@inproceedings{duan-etal-2017-qgforqa,
    title = "Question Generation for Question Answering",
    author = "Duan, Nan  and
      Tang, Duyu  and
      Chen, Peng  and
      Zhou, Ming",
    booktitle = "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing",
    month = sep,
    year = "2017",
    address = "Copenhagen, Denmark",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D17-1090",
    doi = "10.18653/v1/D17-1090",
    pages = "866--874",
    abstract = "This paper presents how to generate questions from given passages using neural networks, where large scale QA pairs are automatically crawled and processed from Community-QA website, and used as training data. The contribution of the paper is 2-fold: First, two types of question generation approaches are proposed, one is a retrieval-based method using convolution neural network (CNN), the other is a generation-based method using recurrent neural network (RNN); Second, we show how to leverage the generated questions to improve existing question answering systems. We evaluate our question generation method for the answer sentence selection task on three benchmark datasets, including SQuAD, MS MARCO, and WikiQA. Experimental results show that, by using generated questions as an extra signal, significant QA improvement can be achieved.",
}
@inproceedings{alberti-etal-2019-synthetic,
    title = "Synthetic {QA} Corpora Generation with Roundtrip Consistency",
    author = "Alberti, Chris  and
      Andor, Daniel  and
      Pitler, Emily  and
      Devlin, Jacob  and
      Collins, Michael",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P19-1620",
    doi = "10.18653/v1/P19-1620",
    pages = "6168--6173",
    abstract = "We introduce a novel method of generating synthetic question answering corpora by combining models of question generation and answer extraction, and by filtering the results to ensure roundtrip consistency. By pretraining on the resulting corpora we obtain significant improvements on SQuAD2 and NQ, establishing a new state-of-the-art on the latter. Our synthetic data generation models, for both question generation and answer extraction, can be fully reproduced by finetuning a publicly available BERT model on the extractive subsets of SQuAD2 and NQ. We also describe a more powerful variant that does full sequence-to-sequence pretraining for question generation, obtaining exact match and F1 at less than 0.1{\%} and 0.4{\%} from human performance on SQuAD2.",
}
@inproceedings{shakeri-etal-2020-end-wangzhiguo,
    title = "End-to-End Synthetic Data Generation for Domain Adaptation of Question Answering Systems",
    author = "Shakeri, Siamak  and
      Nogueira dos Santos, Cicero  and
      Zhu, Henghui  and
      Ng, Patrick  and
      Nan, Feng  and
      Wang, Zhiguo  and
      Nallapati, Ramesh  and
      Xiang, Bing",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.emnlp-main.439",
    doi = "10.18653/v1/2020.emnlp-main.439",
    pages = "5445--5460",
    abstract = "We propose an end-to-end approach for synthetic QA data generation. Our model comprises a single transformer-based encoder-decoder network that is trained end-to-end to generate both answers and questions. In a nutshell, we feed a passage to the encoder and ask the decoder to generate a question and an answer token-by-token. The likelihood produced in the generation process is used as a filtering score, which avoids the need for a separate filtering model. Our generator is trained by fine-tuning a pretrained LM using maximum likelihood estimation. The experimental results indicate significant improvements in the domain adaptation of QA models outperforming current state-of-the-art methods.",
}

@misc{narayan2020qurious-maji,
      title={QURIOUS: Question Generation Pretraining for Text Generation}, 
      author={Shashi Narayan and GonÃ§alo Simoes and Ji Ma and Hannah Craighead and Ryan Mcdonald},
      year={2020},
      eprint={2004.11026},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{dong2019unified-unilm,
  title={Unified language model pre-training for natural language understanding and generation},
  author={Dong, Li and Yang, Nan and Wang, Wenhui and Wei, Furu and Liu, Xiaodong and Wang, Yu and Gao, Jianfeng and Zhou, Ming and Hon, Hsiao-Wuen},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}
@inproceedings{cho-etal-2019-mixture,
    title = "Mixture Content Selection for Diverse Sequence Generation",
    author = "Cho, Jaemin  and
      Seo, Minjoon  and
      Hajishirzi, Hannaneh",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D19-1308",
    doi = "10.18653/v1/D19-1308",
    pages = "3121--3131",
    abstract = "Generating diverse sequences is important in many NLP applications such as question generation or summarization that exhibit semantically one-to-many relationships between source and the target sequences. We present a method to explicitly separate diversification from generation using a general plug-and-play module (called SELECTOR) that wraps around and guides an existing encoder-decoder model. The diversification stage uses a mixture of experts to sample different binary masks on the source sequence for diverse content selection. The generation stage uses a standard encoder-decoder model given each selected content from the source sequence. Due to the non-differentiable nature of discrete sampling and the lack of ground truth labels for binary mask, we leverage a proxy for ground truth mask and adopt stochastic hard-EM for training. In question generation (SQuAD) and abstractive summarization (CNN-DM), our method demonstrates significant improvements in accuracy, diversity and training efficiency, including state-of-the-art top-1 accuracy in both datasets, 6{\%} gain in top-5 accuracy, and 3.7 times faster training over a state-of-the-art model. Our code is publicly available at https://github.com/clovaai/FocusSeq2Seq.",
}
@inproceedings{lin-2004-rouge,
    title = "{ROUGE}: A Package for Automatic Evaluation of Summaries",
    author = "Lin, Chin-Yew",
    booktitle = "Text Summarization Branches Out",
    month = jul,
    year = "2004",
    address = "Barcelona, Spain",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W04-1013",
    pages = "74--81",
}

@inproceedings{papineni-etal-2002-bleu,
    title = "{B}leu: a Method for Automatic Evaluation of Machine Translation",
    author = "Papineni, Kishore  and
      Roukos, Salim  and
      Ward, Todd  and
      Zhu, Wei-Jing",
    booktitle = "Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2002",
    address = "Philadelphia, Pennsylvania, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P02-1040",
    doi = "10.3115/1073083.1073135",
    pages = "311--318",
}

@inproceedings{zhang-bansal-2019-addressing,
    title = "Addressing Semantic Drift in Question Generation for Semi-Supervised Question Answering",
    author = "Zhang, Shiyue  and
      Bansal, Mohit",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D19-1253",
    doi = "10.18653/v1/D19-1253",
    pages = "2495--2509",
    abstract = "Text-based Question Generation (QG) aims at generating natural and relevant questions that can be answered by a given answer in some context. Existing QG models suffer from a {``}semantic drift{''} problem, i.e., the semantics of the model-generated question drifts away from the given context and answer. In this paper, we first propose two semantics-enhanced rewards obtained from downstream question paraphrasing and question answering tasks to regularize the QG model to generate semantically valid questions. Second, since the traditional evaluation metrics (e.g., BLEU) often fall short in evaluating the quality of generated questions, we propose a QA-based evaluation method which measures the QG model{'}s ability to mimic human annotators in generating QA training data. Experiments show that our method achieves the new state-of-the-art performance w.r.t. traditional metrics, and also performs best on our QA-based evaluation metrics. Further, we investigate how to use our QG model to augment QA datasets and enable semi-supervised QA. We propose two ways to generate synthetic QA pairs: generate new questions from existing articles or collect QA pairs from new articles. We also propose two empirically effective strategies, a data filter and mixing mini-batch training, to properly use the QG-generated data for QA. Experiments show that our method improves over both BiDAF and BERT QA baselines, even without introducing new articles.",
}
@inproceedings{xie-etal-2020-exploring,
    title = "Exploring Question-Specific Rewards for Generating Deep Questions",
    author = "Xie, Yuxi  and
      Pan, Liangming  and
      Wang, Dongzhe  and
      Kan, Min-Yen  and
      Feng, Yansong",
    booktitle = "Proceedings of the 28th International Conference on Computational Linguistics",
    month = dec,
    year = "2020",
    address = "Barcelona, Spain (Online)",
    publisher = "International Committee on Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.coling-main.228",
    doi = "10.18653/v1/2020.coling-main.228",
    pages = "2534--2546",
    abstract = "Recent question generation (QG) approaches often utilize the sequence-to-sequence framework (Seq2Seq) to optimize the log likelihood of ground-truth questions using teacher forcing. However, this training objective is inconsistent with actual question quality, which is often reflected by certain global properties such as whether the question can be answered by the document. As such, we directly optimize for QG-specific objectives via reinforcement learning to improve question quality. We design three different rewards that target to improve the fluency, relevance, and answerability of generated questions. We conduct both automatic and human evaluations in addition to thorough analysis to explore the effect of each QG-specific reward. We find that optimizing on question-specific rewards generally leads to better performance in automatic evaluation metrics. However, only the rewards that correlate well with human judgement (e.g., relevance) lead to real improvement in question quality. Optimizing for the others, especially answerability, introduces incorrect bias to the model, resulting in poorer question quality. The code is publicly available at https://github.com/YuxiXie/RL-for-Question-Generation.",
}

@inproceedings{ma2020improving,
  title={Improving question generation with sentence-level semantic matching and answer position inferring},
  author={Ma, Xiyao and Zhu, Qile and Zhou, Yanlin and Li, Xiaolin},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={34},
  number={05},
  pages={8464--8471},
  year={2020}
}

@article{lyu2022extending,
  title={Extending the Scope of Out-of-Domain: Examining QA models in multiple subdomains},
  author={Lyu, Chenyang and Foster, Jennifer and Graham, Yvette},
  journal={arXiv preprint arXiv:2204.04534},
  year={2022}
}
@article{ji2022achieving,
  title={Achieving Reliable Human Assessment of Open-Domain Dialogue Systems},
  author={Ji, Tianbo and Graham, Yvette and Jones, Gareth JF and Lyu, Chenyang and Liu, Qun},
  journal={arXiv preprint arXiv:2203.05899},
  year={2022}
}
@inproceedings{sun-etal-2018-answer-yanjunma,
    title = "Answer-focused and Position-aware Neural Question Generation",
    author = "Sun, Xingwu  and
      Liu, Jing  and
      Lyu, Yajuan  and
      He, Wei  and
      Ma, Yanjun  and
      Wang, Shi",
    booktitle = "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
    month = oct # "-" # nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D18-1427",
    doi = "10.18653/v1/D18-1427",
    pages = "3930--3939",
    abstract = "In this paper, we focus on the problem of question generation (QG). Recent neural network-based approaches employ the sequence-to-sequence model which takes an answer and its context as input and generates a relevant question as output. However, we observe two major issues with these approaches: (1) The generated interrogative words (or question words) do not match the answer type. (2) The model copies the context words that are far from and irrelevant to the answer, instead of the words that are close and relevant to the answer. To address these two issues, we propose an answer-focused and position-aware neural question generation model. (1) By answer-focused, we mean that we explicitly model question word generation by incorporating the answer embedding, which can help generate an interrogative word matching the answer type. (2) By position-aware, we mean that we model the relative distance between the context words and the answer. Hence the model can be aware of the position of the context words when copying them to generate a question. We conduct extensive experiments to examine the effectiveness of our model. The experimental results show that our model significantly improves the baseline and outperforms the state-of-the-art system.",
}

@techreport{Heilman2009Question-noah,
  title={Question generation via overgenerating transformations and ranking},
  author={Heilman, Michael and Smith, Noah A},
  year={2009},
  institution={Carnegie-Mellon University}
}

@inproceedings{heilman-smith-2010-good,
    title = "Good Question! Statistical Ranking for Question Generation",
    author = "Heilman, Michael  and
      Smith, Noah A.",
    booktitle = "Human Language Technologies: The 2010 Annual Conference of the North {A}merican Chapter of the Association for Computational Linguistics",
    month = jun,
    year = "2010",
    address = "Los Angeles, California",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/N10-1086",
    pages = "609--617",
}
@inproceedings{rajpurkar-etal-2016-squad1.1,
    title = "{SQ}u{AD}: 100,000+ Questions for Machine Comprehension of Text",
    author = "Rajpurkar, Pranav  and
      Zhang, Jian  and
      Lopyrev, Konstantin  and
      Liang, Percy",
    booktitle = "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2016",
    address = "Austin, Texas",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D16-1264",
    doi = "10.18653/v1/D16-1264",
    pages = "2383--2392",
}
@inproceedings{du-etal-2017-learning,
    title = "Learning to Ask: Neural Question Generation for Reading Comprehension",
    author = "Du, Xinya  and
      Shao, Junru  and
      Cardie, Claire",
    booktitle = "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2017",
    address = "Vancouver, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P17-1123",
    doi = "10.18653/v1/P17-1123",
    pages = "1342--1352",
    abstract = "We study automatic question generation for sentences from text passages in reading comprehension. We introduce an attention-based sequence learning model for the task and investigate the effect of encoding sentence- vs. paragraph-level information. In contrast to all previous work, our model does not rely on hand-crafted rules or a sophisticated NLP pipeline; it is instead trainable end-to-end via sequence-to-sequence learning. Automatic evaluation results show that our system significantly outperforms the state-of-the-art rule-based system. In human evaluations, questions generated by our system are also rated as being more natural (\textit{i.e.,}, grammaticality, fluency) and as more difficult to answer (in terms of syntactic and lexical divergence from the original text and reasoning needed to answer).",
}

@inproceedings{NIPS2014_seq2seq,
 author = {Sutskever, Ilya and Vinyals, Oriol and Le, Quoc V},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K. Q. Weinberger},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Sequence to Sequence Learning with Neural Networks},
 url = {https://proceedings.neurips.cc/paper/2014/file/a14ac55a4f27472c5d894ec1c3c743d2-Paper.pdf},
 volume = {27},
 year = {2014}
}


@inproceedings{puri-etal-2020-training-synthetic,
    title = "Training Question Answering Models From Synthetic Data",
    author = "Puri, Raul  and
      Spring, Ryan  and
      Shoeybi, Mohammad  and
      Patwary, Mostofa  and
      Catanzaro, Bryan",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.emnlp-main.468",
    doi = "10.18653/v1/2020.emnlp-main.468",
    pages = "5811--5826",
    abstract = "Question and answer generation is a data augmentation method that aims to improve question answering (QA) models given the limited amount of human labeled data. However, a considerable gap remains between synthetic and human-generated question-answer pairs. This work aims to narrow this gap by taking advantage of large language models and explores several factors such as model size, quality of pretrained models, scale of data synthesized, and algorithmic choices. On the SQuAD1.1 question answering task, we achieve higher accuracy using solely synthetic questions and answers than when using the SQuAD1.1 training set questions alone. Removing access to real Wikipedia data, we synthesize questions and answers from a synthetic text corpus generated by an 8.3 billion parameter GPT-2 model and achieve 88.4 Exact Match (EM) and 93.9 F1 score on the SQuAD1.1 dev set. We further apply our methodology to SQuAD2.0 and show a 2.8 absolute gain on EM score compared to prior work using synthetic data.",
}

@article{lewis2021paq,
    title = "{PAQ}: 65 Million Probably-Asked Questions and What You Can Do With Them",
    author = {Lewis, Patrick  and
      Wu, Yuxiang  and
      Liu, Linqing  and
      Minervini, Pasquale  and
      K{\"u}ttler, Heinrich  and
      Piktus, Aleksandra  and
      Stenetorp, Pontus  and
      Riedel, Sebastian},
    journal = "Transactions of the Association for Computational Linguistics",
    volume = "9",
    year = "2021",
    address = "Cambridge, MA",
    publisher = "MIT Press",
    url = "https://aclanthology.org/2021.tacl-1.65",
    doi = "10.1162/tacl_a_00415",
    pages = "1098--1115",
    abstract = "Abstract Open-domain Question Answering models that directly leverage question-answer (QA) pairs, such as closed-book QA (CBQA) models and QA-pair retrievers, show promise in terms of speed and memory compared with conventional models which retrieve and read from text corpora. QA-pair retrievers also offer interpretable answers, a high degree of control, and are trivial to update at test time with new knowledge. However, these models fall short of the accuracy of retrieve-and-read systems, as substantially less knowledge is covered by the available QA-pairs relative to text corpora like Wikipedia. To facilitate improved QA-pair models, we introduce Probably Asked Questions (PAQ), a very large resource of 65M automatically generated QA-pairs. We introduce a new QA-pair retriever, RePAQ, to complement PAQ. We find that PAQ preempts and caches test questions, enabling RePAQ to match the accuracy of recent retrieve-and-read models, whilst being significantly faster. Using PAQ, we train CBQA models which outperform comparable baselines by 5{\%}, but trail RePAQ by over 15{\%}, indicating the effectiveness of explicit retrieval. RePAQ can be configured for size (under 500MB) or speed (over 1K questions per second) while retaining high accuracy. Lastly, we demonstrate RePAQ{'}s strength at selective QA, abstaining from answering when it is likely to be incorrect. This enables RePAQ to {``}back-off{''} to a more expensive state-of-the-art model, leading to a combined system which is both more accurate and 2x faster than the state-of-the-art model alone.",
}

@inproceedings{fabbri-etal-2020-template,
    title = "Template-Based Question Generation from Retrieved Sentences for Improved Unsupervised Question Answering",
    author = "Fabbri, Alexander  and
      Ng, Patrick  and
      Wang, Zhiguo  and
      Nallapati, Ramesh  and
      Xiang, Bing",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.acl-main.413",
    doi = "10.18653/v1/2020.acl-main.413",
    pages = "4508--4513",
    abstract = "Question Answering (QA) is in increasing demand as the amount of information available online and the desire for quick access to this content grows. A common approach to QA has been to fine-tune a pretrained language model on a task-specific labeled dataset. This paradigm, however, relies on scarce, and costly to obtain, large-scale human-labeled data. We propose an unsupervised approach to training QA models with generated pseudo-training data. We show that generating questions for QA training by applying a simple template on a related, retrieved sentence rather than the original context sentence improves downstream QA performance by allowing the model to learn more complex context-question relationships. Training a QA model on this data gives a relative improvement over a previous unsupervised model in F1 score on the SQuAD dataset by about 14{\%}, and 20{\%} when the answer is a named entity, achieving state-of-the-art performance on SQuAD for unsupervised QA.",
}
@ARTICLE{education-2005,  author={A. C. {Graesser} and P. {Chipman} and B. C. {Haynes} and A. {Olney}},  journal={IEEE Transactions on Education},   title={AutoTutor: an intelligent tutoring system with mixed-initiative dialogue},   year={2005},  volume={48},  number={4},  pages={612-618},  doi={10.1109/TE.2005.856149}}

@inproceedings{lewis-etal-2019-unsupervised-cloze,
    title = "Unsupervised Question Answering by Cloze Translation",
    author = "Lewis, Patrick  and
      Denoyer, Ludovic  and
      Riedel, Sebastian",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P19-1484",
    doi = "10.18653/v1/P19-1484",
    pages = "4896--4910",
    abstract = "Obtaining training data for Question Answering (QA) is time-consuming and resource-intensive, and existing QA datasets are only available for limited domains and languages. In this work, we explore to what extent high quality training data is actually required for Extractive QA, and investigate the possibility of unsupervised Extractive QA. We approach this problem by first learning to generate context, question and answer triples in an unsupervised manner, which we then use to synthesize Extractive QA training data automatically. To generate such triples, we first sample random context paragraphs from a large corpus of documents and then random noun phrases or Named Entity mentions from these paragraphs as answers. Next we convert answers in context to {``}fill-in-the-blank{''} cloze questions and finally translate them into natural questions. We propose and compare various unsupervised ways to perform cloze-to-natural question translation, including training an unsupervised NMT model using non-aligned corpora of natural questions and cloze questions as well as a rule-based approach. We find that modern QA models can learn to answer human questions surprisingly well using only synthetic training data. We demonstrate that, without using the SQuAD training data at all, our approach achieves 56.4 F1 on SQuAD v1 (64.5 F1 when the answer is a Named Entity mention), outperforming early supervised models.",
}
@inproceedings{li-etal-2020-harvesting,
    title = "Harvesting and Refining Question-Answer Pairs for Unsupervised {QA}",
    author = "Li, Zhongli  and
      Wang, Wenhui  and
      Dong, Li  and
      Wei, Furu  and
      Xu, Ke",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.acl-main.600",
    doi = "10.18653/v1/2020.acl-main.600",
    pages = "6719--6728",
    abstract = "Question Answering (QA) has shown great success thanks to the availability of large-scale datasets and the effectiveness of neural models. Recent research works have attempted to extend these successes to the settings with few or no labeled data available. In this work, we introduce two approaches to improve unsupervised QA. First, we harvest lexically and syntactically divergent questions from Wikipedia to automatically construct a corpus of question-answer pairs (named as RefQA). Second, we take advantage of the QA model to extract more appropriate answers, which iteratively refines data over RefQA. We conduct experiments on SQuAD 1.1, and NewsQA by fine-tuning BERT without access to manually annotated data. Our approach outperforms previous unsupervised approaches by a large margin, and is competitive with early supervised models. We also show the effectiveness of our approach in the few-shot learning setting.",
}
@inproceedings{narayan-etal-2018-xsum,
    title = "Don{'}t Give Me the Details, Just the Summary! Topic-Aware Convolutional Neural Networks for Extreme Summarization",
    author = "Narayan, Shashi  and
      Cohen, Shay B.  and
      Lapata, Mirella",
    booktitle = "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
    month = oct # "-" # nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D18-1206",
    doi = "10.18653/v1/D18-1206",
    pages = "1797--1807",
    abstract = "We introduce {``}extreme summarization{''}, a new single-document summarization task which does not favor extractive strategies and calls for an abstractive modeling approach. The idea is to create a short, one-sentence news summary answering the question {``}What is the article about?{''}. We collect a real-world, large-scale dataset for this task by harvesting online articles from the British Broadcasting Corporation (BBC). We propose a novel abstractive model which is conditioned on the article{'}s topics and based entirely on convolutional neural networks. We demonstrate experimentally that this architecture captures long-range dependencies in a document and recognizes pertinent content, outperforming an oracle extractive system and state-of-the-art abstractive approaches when evaluated automatically and by humans.",
}

@inproceedings{dhole-manning-2020-syn-qg,
    title = "Syn-{QG}: Syntactic and Shallow Semantic Rules for Question Generation",
    author = "Dhole, Kaustubh  and
      Manning, Christopher D.",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.acl-main.69",
    doi = "10.18653/v1/2020.acl-main.69",
    pages = "752--765",
    abstract = "Question Generation (QG) is fundamentally a simple syntactic transformation; however, many aspects of semantics influence what questions are good to form. We implement this observation by developing Syn-QG, a set of transparent syntactic rules leveraging universal dependencies, shallow semantic parsing, lexical resources, and custom rules which transform declarative sentences into question-answer pairs. We utilize PropBank argument descriptions and VerbNet state predicates to incorporate shallow semantic content, which helps generate questions of a descriptive nature and produce inferential and semantically richer questions than existing systems. In order to improve syntactic fluency and eliminate grammatically incorrect questions, we employ back-translation over the output of these syntactic rules. A set of crowd-sourced evaluations shows that our system can generate a larger number of highly grammatical and relevant questions than previous QG systems and that back-translation drastically improves grammaticality at a slight cost of generating irrelevant questions.",
}
@inproceedings{trischler-etal-2017-newsqa,
    title = "{N}ews{QA}: A Machine Comprehension Dataset",
    author = "Trischler, Adam  and
      Wang, Tong  and
      Yuan, Xingdi  and
      Harris, Justin  and
      Sordoni, Alessandro  and
      Bachman, Philip  and
      Suleman, Kaheer",
    booktitle = "Proceedings of the 2nd Workshop on Representation Learning for {NLP}",
    month = aug,
    year = "2017",
    address = "Vancouver, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W17-2623",
    doi = "10.18653/v1/W17-2623",
    pages = "191--200",
    abstract = "We present NewsQA, a challenging machine comprehension dataset of over 100,000 human-generated question-answer pairs. Crowdworkers supply questions and answers based on a set of over 10,000 news articles from CNN, with answers consisting of spans of text in the articles. We collect this dataset through a four-stage process designed to solicit exploratory questions that require reasoning. Analysis confirms that NewsQA demands abilities beyond simple word matching and recognizing textual entailment. We measure human performance on the dataset and compare it to several strong neural models. The performance gap between humans and machines (13.3{\%} F1) indicates that significant progress can be made on NewsQA through future research. The dataset is freely available online.",
}

@article{mcdonald2006discriminative,
  title={Discriminative training and spanning tree algorithms for dependency parsing},
  author={McDonald, Ryan},
  journal={University of Pennsylvania, PhD Thesis},
  year={2006}
}

@inproceedings{mcdonald-pereira-2006-online,
    title = "Online Learning of Approximate Dependency Parsing Algorithms",
    author = "McDonald, Ryan  and
      Pereira, Fernando",
    booktitle = "11th Conference of the {E}uropean Chapter of the Association for Computational Linguistics",
    month = apr,
    year = "2006",
    address = "Trento, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/E06-1011",
}

@inproceedings{bastings-etal-2017-graph,
    title = "Graph Convolutional Encoders for Syntax-aware Neural Machine Translation",
    author = "Bastings, Joost  and
      Titov, Ivan  and
      Aziz, Wilker  and
      Marcheggiani, Diego  and
      Sima{'}an, Khalil",
    booktitle = "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing",
    month = sep,
    year = "2017",
    address = "Copenhagen, Denmark",
    publisher = "Association for Computational Linguistics",
    doi = "10.18653/v1/D17-1209",
    pages = "1957--1967",
    abstract = "We present a simple and effective approach to incorporating syntactic structure into neural attention-based encoder-decoder models for machine translation. We rely on graph-convolutional networks (GCNs), a recent class of neural networks developed for modeling graph-structured data. Our GCNs use predicted syntactic dependency trees of source sentences to produce representations of words (i.e. hidden states of the encoder) that are sensitive to their syntactic neighborhoods. GCNs take word representations as input and produce word representations as output, so they can easily be incorporated as layers into standard encoders (e.g., on top of bidirectional RNNs or convolutional neural networks). We evaluate their effectiveness with English-German and English-Czech translation experiments for different types of encoders and observe substantial improvements over their syntax-agnostic versions in all the considered setups.",
}

@inproceedings{Buchholz:2006:CST:1596276.1596305,
 author = {Buchholz, Sabine and Marsi, Erwin},
 title = {CoNLL-X Shared Task on Multilingual Dependency Parsing},
 booktitle = {Proceedings of the Tenth Conference on Computational Natural Language Learning},
 series = {CoNLL-X '06},
 year = {2006},
 location = {New York City, New York},
 pages = {149--164},
 numpages = {16},
 url = {http://dl.acm.org/citation.cfm?id=1596276.1596305},
 acmid = {1596305},
 publisher = {Association for Computational Linguistics},
 address = {Stroudsburg, PA, USA},
}


@inproceedings{nivre-2009-non,
    title = "Non-Projective Dependency Parsing in Expected Linear Time",
    author = "Nivre, Joakim",
    booktitle = "Proceedings of the Joint Conference of the 47th Annual Meeting of the {ACL} and the 4th International Joint Conference on Natural Language Processing of the {AFNLP}",
    month = aug,
    year = "2009",
    address = "Suntec, Singapore",
    publisher = "Association for Computational Linguistics",
    pages = "351--359",
}

@inproceedings{nivre-etal-2016-universal,
    title = "Universal Dependencies v1: A Multilingual Treebank Collection",
    author = "Nivre, Joakim  and
      de Marneffe, Marie-Catherine  and
      Ginter, Filip  and
      Goldberg, Yoav  and
      Haji{\v{c}}, Jan  and
      Manning, Christopher D.  and
      McDonald, Ryan  and
      Petrov, Slav  and
      Pyysalo, Sampo  and
      Silveira, Natalia  and
      Tsarfaty, Reut  and
      Zeman, Daniel",
    booktitle = "Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC} 2016)",
    month = may,
    year = "2016",
    address = "Portoro{\v{z}}, Slovenia",
    publisher = "European Language Resources Association (ELRA)",
    pages = "1659--1666",
    abstract = "Cross-linguistically consistent annotation is necessary for sound comparative evaluation and cross-lingual learning experiments. It is also useful for multilingual system development and comparative linguistic studies. Universal Dependencies is an open community effort to create cross-linguistically consistent treebank annotation for many languages within a dependency-based lexicalist framework. In this paper, we describe v1 of the universal guidelines, the underlying design principles, and the currently available treebanks for 33 languages.",
}

@article{ammar-etal-2016-many,
    title = "Many Languages, One Parser",
    author = "Ammar, Waleed  and
      Mulcaire, George  and
      Ballesteros, Miguel  and
      Dyer, Chris  and
      Smith, Noah A.",
    journal = "Transactions of the Association for Computational Linguistics",
    volume = "4",
    year = "2016",
    url = "https://www.aclweb.org/anthology/Q16-1031",
    doi = "10.1162/tacl_a_00109",
    pages = "431--444"
}


@inproceedings{mattoni-etal-2017-zero,
    title = {Zero-shot translation for low-resource Indian languages},
    author = {Giulia Mattoni  and  Pat Nagle  and  Carlos Collantes  and  Dimitar Shterionov},
    booktitle = {Proceedings of MT Summit XVI -- Vol.2 Commercial MT Users and Translators Track},
    month = sep,
    address = {Nagoya, Aichi, Japan},
    publisher = {Asia-Pacific Association for Machine Translation},
    editor = {Masaru Yamada  and  Mark Seligman},
    pages = {1--10},
    year = {2017}
}

@inproceedings{firat-etal-2016-zero,
    title = "Zero-Resource Translation with Multi-Lingual Neural Machine Translation",
    author = "Firat, Orhan  and
      Sankaran, Baskaran  and
      Al-Onaizan, Yaser  and
      Yarman Vural, Fatos T.  and
      Cho, Kyunghyun",
    booktitle = "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2016",
    address = "Austin, Texas",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D16-1026",
    doi = "10.18653/v1/D16-1026",
    pages = "268--277",
}

@inproceedings{yamagishi-etal-2016-controlling,
    title = "Controlling the Voice of a Sentence in {J}apanese-to-{E}nglish Neural Machine Translation",
    author = "Yamagishi, Hayahide  and
      Kanouchi, Shin  and
      Sato, Takayuki  and
      Komachi, Mamoru",
    booktitle = "Proceedings of the 3rd Workshop on {A}sian Translation ({WAT}2016)",
    month = dec,
    year = "2016",
    address = "Osaka, Japan",
    publisher = "The COLING 2016 Organizing Committee",
    url = "https://www.aclweb.org/anthology/W16-4620",
    pages = "203--210",
    abstract = "In machine translation, we must consider the difference in expression between languages. For example, the active/passive voice may change in Japanese-English translation. The same verb in Japanese may be translated into different voices at each translation because the voice of a generated sentence cannot be determined using only the information of the Japanese sentence. Machine translation systems should consider the information structure to improve the coherence of the output by using several topicalization techniques such as passivization. Therefore, this paper reports on our attempt to control the voice of the sentence generated by an encoder-decoder model. To control the voice of the generated sentence, we added the voice information of the target sentence to the source sentence during the training. We then generated sentences with a specified voice by appending the voice information to the source sentence. We observed experimentally whether the voice could be controlled. The results showed that, we could control the voice of the generated sentence with 85.0{\%} accuracy on average. In the evaluation of Japanese-English translation, we obtained a 0.73-point improvement in BLEU score by using gold voice labels.",
}
@inproceedings{sennrich-etal-2016-controlling,
    title = "Controlling Politeness in Neural Machine Translation via Side Constraints",
    author = "Sennrich, Rico  and
      Haddow, Barry  and
      Birch, Alexandra",
    booktitle = "Proceedings of the 2016 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2016",
    address = "San Diego, California",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/N16-1005",
    doi = "10.18653/v1/N16-1005",
    pages = "35--40",
}

@inproceedings{yarowsky2001inducing,
  title={Inducing multilingual text analysis tools via robust projection across aligned corpora},
  author={Yarowsky, David and Ngai, Grace and Wicentowski, Richard},
  booktitle={Proceedings of the first international conference on Human language technology research},
  pages={1--8},
  year={2001},
  organization={Association for Computational Linguistics}
}


@inproceedings{plank-agic-2018-distant,
    title = "Distant Supervision from Disparate Sources for Low-Resource Part-of-Speech Tagging",
    author = "Plank, Barbara  and
      Agi{\'c}, {\v{Z}}eljko",
    booktitle = "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
    month = oct # "-" # nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D18-1061",
    pages = "614--620",
    abstract = "a cross-lingual neural part-of-speech tagger that learns from disparate sources of distant supervision, and realistically scales to hundreds of low-resource languages. The model exploits annotation projection, instance selection, tag dictionaries, morphological lexicons, and distributed representations, all in a uniform framework. The approach is simple, yet surprisingly effective, resulting in a new state of the art without access to any gold annotated data.",
}

@article{johnson-etal-2017-googles_gnmt,
    title = "{G}oogle{'}s Multilingual Neural Machine Translation System: Enabling Zero-Shot Translation",
    author = "Johnson, Melvin  and
      Schuster, Mike  and
      Le, Quoc V.  and
      Krikun, Maxim  and
      Wu, Yonghui  and
      Chen, Zhifeng  and
      Thorat, Nikhil  and
      Vi{\'e}gas, Fernanda  and
      Wattenberg, Martin  and
      Corrado, Greg  and
      Hughes, Macduff  and
      Dean, Jeffrey",
    journal = "Transactions of the Association for Computational Linguistics",
    volume = "5",
    month = dec,
    year = "2017",
    url = "https://www.aclweb.org/anthology/Q17-1024",
    doi = "10.1162/tacl_a_00065",
    pages = "339--351",
    abstract = "We propose a simple solution to use a single Neural Machine Translation (NMT) model to translate between multiple languages. Our solution requires no changes to the model architecture from a standard NMT system but instead introduces an artificial token at the beginning of the input sentence to specify the required target language. Using a shared wordpiece vocabulary, our approach enables Multilingual NMT systems using a single model. On the WMT{'}14 benchmarks, a single multilingual model achieves comparable performance for Englishâ†’French and surpasses state-of-theart results for Englishâ†’German. Similarly, a single multilingual model surpasses state-of-the-art results for Frenchâ†’English and Germanâ†’English on WMT{'}14 and WMT{'}15 benchmarks, respectively. On production corpora, multilingual models of up to twelve language pairs allow for better translation of many individual pairs. Our models can also learn to perform implicit bridging between language pairs never seen explicitly during training, showing that transfer learning and zero-shot translation is possible for neural translation. Finally, we show analyses that hints at a universal interlingua representation in our models and also show some interesting examples when mixing languages.",
}

@article{Bloom:1970,
 author = {Bloom, Burton H.},
 title = {Space/Time Trade-offs in Hash Coding with Allowable Errors},
 journal = {Commun. ACM},
 issue_date = {July 1970},
 volume = {13},
 number = {7},
 month = jul,
 year = {1970},
 issn = {0001-0782},
 pages = {422--426},
 numpages = {5},
 url = {http://doi.acm.org/10.1145/362686.362692},
 doi = {10.1145/362686.362692},
 acmid = {362692},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {hash addressing, hash coding, retrieval efficiency, retrieval trade-offs, scatter storage, searching, storage efficiency, storage layout},
} 

@article{blei2003latent,
  title={Latent {D}irichlet allocation},
  author={Blei, David M. and Ng, Andrew Y. and Jordan, Michael I.},
  journal={Journal of Machine Learning Research},
issue_date = {3/1/2003},
 volume = {3},
 month = mar,
 year = {2003},
 issn = {1532-4435},
 pages = {993--1022},
 numpages = {30},
 url = {http://dl.acm.org/citation.cfm?id=944919.944937},
 acmid = {944937},
 publisher = {JMLR.org}
}

@inproceedings{bjorkelund-etal-2017-ims,
    title = "{IMS} at the {C}o{NLL} 2017 {UD} Shared Task: {CRF}s and Perceptrons Meet Neural Networks",
    author = {Bj{\"o}rkelund, Anders  and
      Falenska, Agnieszka  and
      Yu, Xiang  and
      Kuhn, Jonas},
    booktitle = "Proceedings of the {C}o{NLL} 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies",
    month = aug,
    year = "2017",
    address = "Vancouver, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/K17-3004",
    doi = "10.18653/v1/K17-3004",
    pages = "40--51",
    abstract = "This paper presents the IMS contribution to the CoNLL 2017 Shared Task. In the preprocessing step we employed a CRF POS/morphological tagger and a neural tagger predicting supertags. On some languages, we also applied word segmentation with the CRF tagger and sentence segmentation with a perceptron-based parser. For parsing we took an ensemble approach by blending multiple instances of three parsers with very different architectures. Our system achieved the third place overall and the second place for the surprise languages.",
}

@InProceedings{junczysdowmunt-grundkiewicz:2016:WMT,
   author    = {Junczys-Dowmunt, Marcin  and  Grundkiewicz, Roman},
   title     = {Log-linear Combinations of Monolingual and Bilingual Neural Machine Translation Models for Automatic Post-Editing},
   booktitle = {Proceedings of the First Conference on Machine Translation},
   month     = {August},
   year      = {2016},
   address   = {Berlin, Germany},
   publisher = {Association for Computational Linguistics},
   pages     = {751--758},
   url       = {http://www.aclweb.org/anthology/W16-2378}
}

 @misc{11372/LRT-2613,
 title = {{WMT18} {APE} Shared Task: En-{DE} {NMT} Train and Dev Data},
 author = {Turchi, Marco and Negri, Matteo and Chatterjee, Rajen},
 url = {http://hdl.handle.net/11372/LRT-2613},
 note = {{LINDAT}/{CLARIN} digital library at the Institute of Formal and Applied Linguistics ({{\'U}FAL}), Faculty of Mathematics and Physics, Charles University},
 copyright = {{AGREEMENT} {ON} {THE} {USE} {OF} {DATA} {IN} {QT21} {APE} Task},
 year = {2018} }
 
@inproceedings{negri-etal-2018-escape,
    title = "{ESCAPE}: a Large-scale Synthetic Corpus for Automatic Post-Editing",
    author = "Negri, Matteo  and
      Turchi, Marco  and
      Chatterjee, Rajen  and
      Bertoldi, Nicola",
    booktitle = "Proceedings of the 11th Language Resources and Evaluation Conference",
    month = may,
    year = "2018",
    address = "Miyazaki, Japan",
    publisher = "European Language Resource Association",
    url = "https://www.aclweb.org/anthology/L18-1004",
}

@inproceedings{zhou-etal-2018-massively,
    title = "Massively Parallel Cross-Lingual Learning in Low-Resource Target Language Translation",
    author = "Zhou, Zhong  and
      Sperber, Matthias  and
      Waibel, Alexander",
    booktitle = "Proceedings of the Third Conference on Machine Translation: Research Papers",
    month = oct,
    year = "2018",
    address = "Belgium, Brussels",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W18-6324",
    pages = "232--243",
}



@InProceedings{N10:1091,
  author = 	"Surdeanu, Mihai
		and Manning, Christopher D.",
  title = 	"Ensemble Models for Dependency Parsing: Cheap and Good?",
  booktitle = 	"Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics",
  year = 	"2010",
  publisher = 	"Association for Computational Linguistics",
  pages = 	"649--652",
  location = 	"Los Angeles, California",
  url = 	"http://aclweb.org/anthology/N10-1091"
}

@InProceedings{K17:3007,
  author = 	"Sato, Motoki and Manabe, Hitoshi and Noji, Hiroshi and Matsumoto, Yuji",
  title = 	"Adversarial Training for Cross-Domain Universal Dependency Parsing",
  booktitle = 	"Proceedings of the CoNLL 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies",
  year = 	"2017",
  publisher = 	"Association for Computational Linguistics",
  pages = 	"71--79",
  location = 	"Vancouver, Canada",
  doi = 	"10.18653/v1/K17-3007",
  url = 	"http://aclweb.org/anthology/K17-3007"
}

@InProceedings{K17:3003,
  author = 	"Shi, Tianze
		and Wu, Felix G.
		and Chen, Xilun
		and Cheng, Yao",
  title = 	"Combining Global Models for Parsing Universal Dependencies",
  booktitle = 	"Proceedings of the CoNLL 2017 Shared Task: Multilingual Parsing from Raw      Text to Universal Dependencies    ",
  year = 	"2017",
  publisher = 	"Association for Computational Linguistics",
  pages = 	"31--39",
  location = 	"Vancouver, Canada",
  doi = 	"10.18653/v1/K17-3003",
  url = 	"http://aclweb.org/anthology/K17-3003"
}

@InProceedings{K17:3005,
  author = 	"Che, Wanxiang
		and Guo, Jiang
		and Wang, Yuxuan
		and Zheng, Bo
		and Zhao, Huaipeng
		and Liu, Yang
		and Teng, Dechuan
		and Liu, Ting",
  title = 	"The HIT-SCIR System for End-to-End Parsing of Universal Dependencies",
  booktitle = 	"Proceedings of the CoNLL 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies",
  year = 	"2017",
  publisher = 	"Association for Computational Linguistics",
  pages = 	"52--62",
  location = 	"Vancouver, Canada",
  doi = 	"10.18653/v1/K17-3005",
  url = 	"http://aclweb.org/anthology/K17-3005"
}

@inproceedings{che-etal-2018-towards,
    title = "Towards Better {UD} Parsing: Deep Contextualized Word Embeddings, Ensemble, and Treebank Concatenation",
    author = "Che, Wanxiang  and
      Liu, Yijia  and
      Wang, Yuxuan  and
      Zheng, Bo  and
      Liu, Ting",
    booktitle = "Proceedings of the {C}o{NLL} 2018 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies",
    month = oct,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/K18-2005",
    pages = "55--64",
    abstract = "This paper describes our system (HIT-SCIR) submitted to the CoNLL 2018 shared task on Multilingual Parsing from Raw Text to Universal Dependencies. We base our submission on Stanford{'}s winning system for the CoNLL 2017 shared task and make two effective extensions: 1) incorporating deep contextualized word embeddings into both the part of speech tagger and parser; 2) ensembling parsers trained with different initialization. We also explore different ways of concatenating treebanks for further improvements. Experimental results on the development data show the effectiveness of our methods. In the final evaluation, our system was ranked first according to LAS (75.84{\%}) and outperformed the other systems by a large margin.",
}

@InProceedings{delhoneux-stymne-nivre:2017:IWPT,
  author    = {de Lhoneux, Miryam  and  Stymne, Sara  and  Nivre, Joakim},
  title     = {Arc-Hybrid Non-Projective Dependency Parsing with a Static-Dynamic Oracle},
  booktitle = {Proceedings of the 15th International Conference on Parsing Technologies},
  month     = {September},
  year      = {2017},
  address   = {Pisa, Italy},
  publisher = {Association for Computational Linguistics},
  pages     = {99--104},
  url       = {http://www.aclweb.org/anthology/W17-6314}
}

@InProceedings{das-zaffar-sarkar:2017:K17-3,
  author    = {Das, Ayan  and  Zaffar, Affan  and  Sarkar, Sudeshna},
  title     = {Delexicalized transfer parsing for low-resource languages using transformed and combined treebanks},
  booktitle = {Proceedings of the CoNLL 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies},
  month     = {August},
  year      = {2017},
  address   = {Vancouver, Canada},
  publisher = {Association for Computational Linguistics},
  pages     = {182--190},
  abstract  = {This paper describes our dependency parsing system in CoNLL-2017 shared task on
	Multilingual Parsing from Raw Text to Universal Dependencies. We primarily
	focus on the low-resource languages (surprise languages). We have developed a
	framework to combine multiple treebanks to train parsers for low resource
	languages by delexicalization method. We have applied transformation on source
	language treebanks based on syntactic features of the low-resource language to
	improve performance of the parser. In the official evaluation, our system
	achieves an macro-averaged LAS score of 67.61 and 37.16 on the entire blind
	test data and the surprise language test data respectively.},
  url       = {http://www.aclweb.org/anthology/K/K17/K17-3019.pdf}
}

@inproceedings{gardner-etal-2018-allennlp,
    title = "{A}llen{NLP}: A Deep Semantic Natural Language Processing Platform",
    author = "Gardner, Matt  and
      Grus, Joel  and
      Neumann, Mark  and
      Tafjord, Oyvind  and
      Dasigi, Pradeep  and
      Liu, Nelson F.  and
      Peters, Matthew  and
      Schmitz, Michael  and
      Zettlemoyer, Luke",
    booktitle = "Proceedings of Workshop for {NLP} Open Source Software ({NLP}-{OSS})",
    month = jul,
    year = "2018",
    address = "Melbourne, Australia",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W18-2501",
    pages = "1--6",
    abstract = "Modern natural language processing (NLP) research requires writing code. Ideally this code would provide a precise definition of the approach, easy repeatability of results, and a basis for extending the research. However, many research codebases bury high-level parameters under implementation details, are challenging to run and debug, and are difficult enough to extend that they are more likely to be rewritten. This paper describes AllenNLP, a library for applying deep learning methods to NLP research that addresses these issues with easy-to-use command-line tools, declarative configuration-driven experiments, and modular NLP abstractions. AllenNLP has already increased the rate of research experimentation and the sharing of NLP components at the Allen Institute for Artificial Intelligence, and we are working to have the same impact across the field.",
}

@inproceedings{straka-strakova-2017-tokenizing,
    title = "Tokenizing, {POS} Tagging, Lemmatizing and Parsing {UD} 2.0 with {UDP}ipe",
    author = "Straka, Milan  and
      Strakov{\'a}, Jana",
    booktitle = "Proceedings of the {C}o{NLL} 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies",
    month = aug,
    year = "2017",
    address = "Vancouver, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/K17-3009",
    doi = "10.18653/v1/K17-3009",
    pages = "88--99",
    abstract = "Many natural language processing tasks, including the most advanced ones, routinely start by several basic processing steps {--} tokenization and segmentation, most likely also POS tagging and lemmatization, and commonly parsing as well. A multilingual pipeline performing these steps can be trained using the Universal Dependencies project, which contains annotations of the described tasks for 50 languages in the latest release UD 2.0. We present an update to UDPipe, a simple-to-use pipeline processing CoNLL-U version 2.0 files, which performs these tasks for multiple languages without requiring additional external data. We provide models for all 50 languages of UD 2.0, and furthermore, the pipeline can be trained easily using data in CoNLL-U format. UDPipe is a standalone application in C++, with bindings available for Python, Java, C{\#} and Perl. In the CoNLL 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies, UDPipe was the eight best system, while achieving low running times and moderately sized models.",
}

@inproceedings{chen-etal-2018-simple,
    title = "A Simple yet Effective Joint Training Method for Cross-Lingual Universal Dependency Parsing",
    author = "Chen, Danlu  and
      Lin, Mengxiao  and
      Hu, Zhifeng  and
      Qiu, Xipeng",
    booktitle = "Proceedings of the {C}o{NLL} 2018 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies",
    month = oct,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/K18-2026",
    pages = "256--263",
    abstract = "This paper describes Fudan{'}s submission to CoNLL 2018{'}s shared task Universal Dependency Parsing. We jointly train models when two languages are similar according to linguistic typology and then ensemble the models using a simple re-parse algorithm. We outperform the baseline method by 4.4{\%} (2.1{\%}) on average on development (test) set in CoNLL 2018 UD Shared Task.",
}

@inproceedings{schuster-etal-2019-cross,
    title = "Cross-Lingual Alignment of Contextual Word Embeddings, with Applications to Zero-shot Dependency Parsing",
    author = "Schuster, Tal  and
      Ram, Ori  and
      Barzilay, Regina  and
      Globerson, Amir",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/N19-1162",
    pages = "1599--1613",
    abstract = "We introduce a novel method for multilingual transfer that utilizes deep contextual embeddings, pretrained in an unsupervised fashion. While contextual embeddings have been shown to yield richer representations of meaning compared to their static counterparts, aligning them poses a challenge due to their dynamic nature. To this end, we construct context-independent variants of the original monolingual spaces and utilize their mapping to derive an alignment for the context-dependent spaces. This mapping readily supports processing of a target language, improving transfer by context-aware embeddings. Our experimental results demonstrate the effectiveness of this approach for zero-shot and few-shot learning of dependency parsing. Specifically, our method consistently outperforms the previous state-of-the-art on 6 tested languages, yielding an improvement of 6.8 LAS points on average.",
}


@inproceedings{nivre-etal-2007-conll,
    title = "The {C}o{NLL} 2007 Shared Task on Dependency Parsing",
    author = {Nivre, Joakim  and
      Hall, Johan  and
      K{\"u}bler, Sandra  and
      McDonald, Ryan  and
      Nilsson, Jens  and
      Riedel, Sebastian  and
      Yuret, Deniz},
    booktitle = "Proceedings of the {C}o{NLL} Shared Task Session of {EMNLP}-{C}o{NLL} 2007",
    month = jun,
    year = "2007",
    address = "Prague, Czech Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D07-1096",
    pages = "915--932",
}

@InProceedings{P18:2098,
  author = 	{Stymne, Sara
		and de Lhoneux, Miryam
		and Smith, Aaron
		and Nivre, Joakim},
  title = 	{Parser Training with Heterogeneous Treebanks},
  booktitle = 	{Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
  year = 	{2018},
  publisher = 	{Association for Computational Linguistics},
  pages = 	{619--625},
  location = 	{Melbourne, Australia},
  url = 	{http://aclweb.org/anthology/P18-2098}
}

@inproceedings{smith-etal-2018-82,
    title = "82 Treebanks, 34 Models: Universal Dependency Parsing with Multi-Treebank Models",
    author = "Smith, Aaron  and
      Bohnet, Bernd  and
      de Lhoneux, Miryam  and
      Nivre, Joakim  and
      Shao, Yan  and
      Stymne, Sara",
    booktitle = "Proceedings of the {C}o{NLL} 2018 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies",
    month = oct,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/K18-2011",
    pages = "113--123",
    abstract = "We present the Uppsala system for the CoNLL 2018 Shared Task on universal dependency parsing. Our system is a pipeline consisting of three components: the first performs joint word and sentence segmentation; the second predicts part-of-speech tags and morphological features; the third predicts dependency trees from words and tags. Instead of training a single parsing model for each treebank, we trained models with multiple treebanks for one language or closely related languages, greatly reducing the number of models. On the official test run, we ranked 7th of 27 teams for the LAS and MLAS metrics. Our system obtained the best scores overall for word segmentation, universal POS tagging, and morphological features.",
}

@InProceedings{ballesteros-dyer-smith:2015:EMNLP,
  author    = {Ballesteros, Miguel  and  Dyer, Chris  and  Smith, Noah A.},
  title     = {Improved Transition-based Parsing by Modeling Characters instead of Words with LSTMs},
  booktitle = {Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing},
  month     = {September},
  year      = {2015},
  address   = {Lisbon, Portugal},
  publisher = {Association for Computational Linguistics},
  pages     = {349--359},
  url       = {http://aclweb.org/anthology/D15-1041}
}

@InProceedings{D14-1082,
  author = 	"Chen, Danqi
		and Manning, Christopher",
  title = 	"A Fast and Accurate Dependency Parser using Neural Networks",
  booktitle = 	"Proceedings of the 2014 Conference on Empirical Methods in Natural      Language Processing (EMNLP)    ",
  year = 	"2014",
  publisher = 	"Association for Computational Linguistics",
  pages = 	"740--750",
  location = 	"Doha, Qatar",
  doi = 	"10.3115/v1/D14-1082",
  url = 	"http://aclweb.org/anthology/D14-1082"
}


@InProceedings{N07-1049,
  author = 	"Attardi, Giuseppe
		and Ciaramita, Massimiliano",
  title = 	"Tree Revision Learning for Dependency Parsing",
  booktitle = 	"Human Language Technologies 2007: The Conference of the North American Chapter of the Association for Computational Linguistics; Proceedings of the Main Conference",
  year = 	"2007",
  publisher = 	"Association for Computational Linguistics",
  pages = 	"388--395",
  location = 	"Rochester, New York",
  url = 	"http://aclweb.org/anthology/N07-1049"
}


@InProceedings{zeman:EtAl:2017:K17:3,
  author    = {Zeman, Daniel  and  Popel, Martin  and  Straka, Milan  and  Hajic, Jan
     and  Nivre, Joakim  and  Ginter, Filip  and  Luotolahti, Juhani  and  Pyysalo, Sampo
     and  Petrov, Slav  and  Potthast, Martin  and  Tyers, Francis  and  Badmaeva, Elena 
     and  Gokirmak, Memduh  and  Nedoluzhko, Anna  and  Cinkova, Silvie  and  Hajic jr., Jan
     and  Hlavacova, Jaroslava  and  Kettnerov\'{a}, V\'{a}clava  and  Uresova, Zdenka
     and  Kanerva, Jenna  and  Ojala, Stina  and  Missil\"{a}, Anna  and  Manning, Christopher D.
     and  Schuster, Sebastian  and  Reddy, Siva  and  Taji, Dima  and  Habash, Nizar  and  Leung, Herman
     and  de Marneffe, Marie-Catherine  and  Sanguinetti, Manuela  and  Simi, Maria
     and  Kanayama, Hiroshi  and  dePaiva, Valeria  and  Droganova, Kira 
     and  Mart\'{i}nez Alonso, H\'{e}ctor  and  \c{C}\"{o}ltekin, \c{C}a\u{g}r 
     and  Sulubacak, Umut  and  Uszkoreit, Hans  and  Macketanz, Vivien  and  Burchardt, Aljoscha 
     and  Harris, Kim  and  Marheinecke, Katrin  and  Rehm, Georg  and  Kayadelen, Tolga 
     and  Attia, Mohammed  and  Elkahky, Ali  and  Yu, Zhuoran  and  Pitler, Emily 
     and  Lertpradit, Saran  and  Mandl, Michael  and  Kirchner, Jesse  and  Alcalde, Hector Fernandez
     and  Strnadov\'{a}, Jana  and  Banerjee, Esha  and  Manurung, Ruli  and  Stella, Antonio 
     and  Shimada, Atsuko  and  Kwak, Sookyoung  and  Mendonca, Gustavo  and  Lando, Tatiana
     and  Nitisaroj, Rattima  and  Li, Josie},
  title     = {CoNLL 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies},
  booktitle = {Proceedings of the CoNLL 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies},
  month     = {August},
  year      = {2017},
  address   = {Vancouver, Canada},
  publisher = {Association for Computational Linguistics},
  pages     = {1--19},
  url       = {http://www.aclweb.org/anthology/K17-3001}
}

@inproceedings{wan-etal-2018-ibm,
    title = "{IBM} Research at the {C}o{NLL} 2018 Shared Task on Multilingual Parsing",
    author = "Wan, Hui  and
      Naseem, Tahira  and
      Lee, Young-Suk  and
      Castelli, Vittorio  and
      Ballesteros, Miguel",
    booktitle = "Proceedings of the {C}o{NLL} 2018 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies",
    month = oct,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/K18-2009",
    pages = "92--102",
    abstract = "This paper presents the IBM Research AI submission to the CoNLL 2018 Shared Task on Parsing Universal Dependencies. Our system implements a new joint transition-based parser, based on the Stack-LSTM framework and the Arc-Standard algorithm, that handles tokenization, part-of-speech tagging, morphological tagging and dependency parsing in one single model. By leveraging a combination of character-based modeling of words and recursive composition of partially built linguistic structures we qualified 13th overall and 7th in low resource. We also present a new sentence segmentation neural architecture based on Stack-LSTMs that was the 4th best overall.",
}


@InProceedings{zeman:EtAl:2018:K18:2,
  author    = {Zeman, Daniel  and  Haji{\v{c}}, Jan  and  Popel, Martin  and  Potthast, Martin  and  Straka, Milan  and  Ginter, Filip  and  Nivre, Joakim  and  Petrov, Slav},
  title     = {{CoNLL} 2018 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies},
  booktitle = {Proceedings of the {CoNLL} 2018 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies},
  month     = {October},
  year      = {2018},
  address   = {Brussels, Belgium},
  publisher = {Association for Computational Linguistics},
  pages     = {1--21},
  url       = {http://www.aclweb.org/anthology/K18-2001}
}


@InProceedings{dozat-manning,
  author    = {Dozat, Timothy  and  Manning, Christopher D.},
  title     = {Deep biaffine attention for neural dependency parsing},
  booktitle = {Proceedings of the 5th International Conference on Learning Representations (ICLR 2017)},
  url = {https://openreview.net/pdf?id=Hk95PK9le},
  year      = {2017}
}


@InProceedings{attardiEtAl2009,
  author    = {Attardi, Giuseppe  and  Dell'Orletta, Felice},
  title     = {Reverse Revision and Linear Tree Combination for Dependency Parsing},
  booktitle = {Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, Companion Volume: Short Papers},
  month     = {June},
  year      = {2009},
  address   = {Boulder, Colorado},
  publisher = {Association for Computational Linguistics},
  pages     = {261--264},
  url       = {http://www.aclweb.org/anthology/N/N09/N09-2066}
}

 @misc{11234/1-2837,
 title = {Universal Dependencies 2.2},
 author = {Nivre, Joakim and Abrams, Mitchell
     and Agi{\'c}, {\v Z}eljko and Ahrenberg, Lars and Antonsen, Lene and Aranzabe, Maria Jesus and Arutie, Gashaw and Asahara, Masayuki and Ateyah, Luma and Attia, Mohammed and Atutxa, Aitziber and Augustinus, Liesbeth and Badmaeva, Elena and Ballesteros, Miguel and Banerjee, Esha and Bank, Sebastian and Barbu Mititelu, Verginica and Bauer, John and Bellato, Sandra and Bengoetxea, Kepa and Bhat, Riyaz Ahmad and Biagetti, Erica and Bick, Eckhard and Blokland, Rogier and Bobicev, Victoria and B{\"o}rstell, Carl and Bosco, Cristina and Bouma, Gosse and Bowman, Sam and Boyd, Adriane and Burchardt, Aljoscha and Candito, Marie and Caron, Bernard and Caron, Gauthier and Cebiro{\u g}lu Eryi{\u g}it, G{\"u}l{\c s}en and Celano, Giuseppe G. A. and Cetin, Savas and Chalub, Fabricio and Choi, Jinho and Cho, Yongseok and Chun, Jayeol and Cinkov{\'a}, Silvie and Collomb, Aur{\'e}lie and {\c C}{\"o}ltekin, {\c C}a{\u g}r{\i} and Connor, Miriam
     and Courtin, Marine and Davidson, Elizabeth and de Marneffe, Marie-Catherine and de Paiva, Valeria and Diaz de Ilarraza, Arantza and Dickerson, Carly and Dirix, Peter and Dobrovoljc, Kaja and Dozat, Timothy and Droganova, Kira and Dwivedi, Puneet and Eli, Marhaba and Elkahky, Ali and Ephrem, Binyam and Erjavec, Toma{\v z} and Etienne, Aline and Farkas, Rich{\'a}rd and Fernandez Alcalde, Hector and Foster, Jennifer and Freitas, Cl{\'a}udia and Gajdo{\v s}ov{\'a}, Katar{\'{\i}}na and Galbraith, Daniel and Garcia, Marcos and G{\"a}rdenfors, Moa and Gerdes, Kim and Ginter, Filip and Goenaga, Iakes and Gojenola, Koldo and G{\"o}k{\i}rmak, Memduh and Goldberg, Yoav and G{\'o}mez Guinovart, Xavier and Gonz{\'a}les Saavedra, Berta and Grioni,
 Matias and Gr{\=u}z{\={\i}}tis, Normunds and Guillaume, Bruno and Guillot-Barbance, C{\'e}line and Habash, Nizar and Haji{\v c}, Jan and Haji{\v c} jr., Jan and H{\`a} M{\~y}, Linh and Han, Na-Rae and Harris, Kim and Haug, Dag and Hladk{\'a}, Barbora and Hlav{\'a}{\v c}ov{\'a}, Jaroslava and Hociung, Florinel and Hohle, Petter and Hwang, Jena and Ion, Radu and Irimia, Elena and Jel{\'{\i}}nek, Tom{\'a}{\v s} and Johannsen, Anders and J{\o}rgensen, Fredrik and Ka{\c s}{\i}kara, H{\"u}ner and Kahane, Sylvain and Kanayama, Hiroshi and Kanerva, Jenna and Kayadelen, Tolga and Kettnerov{\'a}, V{\'a}clava and Kirchner, Jesse and Kotsyba, Natalia and Krek, Simon and Kwak, Sookyoung and Laippala, Veronika and Lambertino, Lorenzo and Lando, Tatiana and Larasati, Septina Dian and Lavrentiev, Alexei and Lee, John and L{\^e} H{\`{\^o}}ng, PhÆ°Æ¡ng and Lenci, Alessandro and Lertpradit, Saran and Leung, Herman and Li, Cheuk Ying and Li, Josie and Li, Keying and Lim, {KyungTae} and Ljube{\v s}i{\'c}, Nikola and Loginova, Olga and Lyashevskaya, Olga and Lynn, Teresa and Macketanz, Vivien and Makazhanov, Aibek
     and Mandl, Michael and Manning, Christopher and Manurung, Ruli and M{\u a}r{\u a}nduc, C{\u a}t{\u a}lina and Mare{\v c}ek, David and Marheinecke, Katrin and Mart{\'{\i}}nez Alonso, H{\'e}ctor and Martins, Andr{\'e} and Ma{\v s}ek, Jan and Matsumoto, Yuji and {McDonald}, Ryan and Mendon{\c c}a, Gustavo and Miekka, Niko and Missil{\"a}, Anna and Mititelu, C{\u a}t{\u a}lin and Miyao, Yusuke and Montemagni, Simonetta and More, Amir and Moreno Romero, Laura and Mori, Shinsuke and Mortensen, Bjartur and Moskalevskyi, Bohdan and Muischnek, Kadri and Murawaki, Yugo and M{\"u}{\"u}risep, Kaili and Nainwani, Pinkey and Navarro Hor{\~n}iacek, Juan Ignacio and Nedoluzhko,
 Anna and Ne{\v s}pore-B{\=e}rzkalne, Gunta and Nguy{\~{\^e}}n Th{\d i}, LÆ°Æ¡ng and Nguy{\~{\^e}}n Th{\d i} Minh, Huy{\`{\^e}}n and Nikolaev, Vitaly and Nitisaroj, Rattima and Nurmi, Hanna and Ojala, Stina and Ol{\'u}{\`o}kun, Ad{\'e}day{\d o}Ì€ and Omura, Mai and Osenova, Petya and {\"O}stling, Robert and {\O}vrelid, Lilja and Partanen, Niko and Pascual, Elena and Passarotti, Marco and Patejuk, Agnieszka and Peng, Siyao and Perez, Cenel-Augusto and Perrier, Guy and Petrov, Slav and Piitulainen, Jussi and Pitler, Emily and Plank, Barbara and Poibeau, Thierry and Popel, Martin and Pretkalni{\c n}a, Lauma and Pr{\'e}vost, Sophie and Prokopidis, Prokopis and Przepi{\'o}rkowski, Adam and Puolakainen, Tiina and Pyysalo, Sampo and R{\"a}{\"a}bis, Andriela and Rademaker, Alexandre and Ramasamy, Loganathan and Rama, Taraka and Ramisch, Carlos and Ravishankar, Vinit and Real, Livy and Reddy, Siva and Rehm, Georg
     and Rie{\ss}ler, Michael and Rinaldi, Larissa and Rituma, Laura and Rocha, Luisa and Romanenko, Mykhailo and Rosa, Rudolf and Rovati, Davide and RoÈ™ca, Valentin and Rudina, Olga and Sadde, Shoval and Saleh, Shadi and Samard{\v z}i{\'c}, Tanja and Samson, Stephanie and Sanguinetti,
 Manuela and Saul{\={\i}}te, Baiba and Sawanakunanon, Yanin and Schneider, Nathan and Schuster, Sebastian and Seddah, Djam{\'e} and Seeker, Wolfgang and Seraji, Mojgan and Shen, Mo and Shimada, Atsuko and Shohibussirri, Muh and Sichinava, Dmitry and Silveira, Natalia and Simi, Maria and Simionescu, Radu and Simk{\'o}, Katalin and {\v S}imkov{\'a}, M{\'a}ria and Simov, Kiril and Smith, Aaron and Soares-Bastos, Isabela and Stella, Antonio and Straka, Milan and Strnadov{\'a}, Jana and Suhr, Alane and Sulubacak, Umut and Sz{\'a}nt{\'o}, Zsolt and Taji, Dima and Takahashi, Yuta and Tanaka, Takaaki and Tellier, Isabelle and Trosterud, Trond
     and Trukhina, Anna and Tsarfaty, Reut and Tyers, Francis and Uematsu, Sumire and Ure{\v s}ov{\'a}, Zde{\v n}ka and Uria, Larraitz and Uszkoreit, Hans and Vajjala, Sowmya and van Niekerk, Daniel and van Noord, Gertjan and Varga, Viktor and Vincze, Veronika and Wallin, Lars and Washington, Jonathan North and Williams, Seyi and Wir{\'e}n, Mats and Woldemariam, Tsegay and Wong, Tak-sum and Yan, Chunxiao and Yavrumyan, Marat M. and Yu, Zhuoran and {\v Z}abokrtsk{\'y}, Zden{\v e}k and Zeldes, Amir and Zeman, Daniel and Zhang, Manying and Zhu, Hanzhi},
 url = {http://hdl.handle.net/11234/1-2837},
 note = {{LINDAT}/{CLARIN} digital library at the Institute of Formal and Applied Linguistics ({{\'U}FAL}), Faculty of Mathematics and Physics, Charles University},
 copyright = {Licence Universal Dependencies v2.2},
 year = {2018}
 }
 
 @inproceedings{lim-poibeau-2017-system,
    title = "A System for Multilingual Dependency Parsing based on Bidirectional {LSTM} Feature Representations",
    author = "Lim, KyungTae  and
      Poibeau, Thierry",
    booktitle = "Proceedings of the {C}o{NLL} 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies",
    month = aug,
    year = "2017",
    address = "Vancouver, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/K17-3006",
    doi = "10.18653/v1/K17-3006",
    pages = "63--70",
    abstract = "In this paper, we present our multilingual dependency parser developed for the CoNLL 2017 UD Shared Task dealing with {``}Multilingual Parsing from Raw Text to Universal Dependencies{''}. Our parser extends the monolingual BIST-parser as a multi-source multilingual trainable parser. Thanks to multilingual word embeddings and one hot encodings for languages, our system can use both monolingual and multi-source training. We trained 69 monolingual language models and 13 multilingual models for the shared task. Our multilingual approach making use of different resources yield better results than the monolingual approach for 11 languages. Our system ranked 5 th and achieved 70.93 overall LAS score over the 81 test corpora (macro-averaged LAS F1 score).",
}

@inproceedings{lim-etal-2018-sex,
    title = "{SE}x {B}i{ST}: A Multi-Source Trainable Parser with Deep Contextualized Lexical Representations",
    author = "Lim, KyungTae  and
      Park, Cheoneum  and
      Lee, Changki  and
      Poibeau, Thierry",
    booktitle = "Proceedings of the {C}o{NLL} 2018 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies",
    month = oct,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/K18-2014",
    pages = "143--152",
    abstract = "We describe the SEx BiST parser (Semantically EXtended Bi-LSTM parser) developed at Lattice for the CoNLL 2018 Shared Task (Multilingual Parsing from Raw Text to Universal Dependencies). The main characteristic of our work is the encoding of three different modes of contextual information for parsing: (i) Treebank feature representations, (ii) Multilingual word representations, (iii) ELMo representations obtained via unsupervised learning from external resources. Our parser performed well in the official end-to-end evaluation (73.02 LAS {--} 4th/26 teams, and 78.72 UAS {--} 2nd/26); remarkably, we achieved the best UAS scores on all the English corpora by applying the three suggested feature representations. Finally, we were also ranked 1st at the optional event extraction task, part of the 2018 Extrinsic Parser Evaluation campaign.",
}

 @misc{11234/1-2895,
 title = {Universal Dependencies 2.3},
 author = {Nivre, Joakim and Abrams, Mitchell and Agi{\'c}, {\v Z}eljko and Ahrenberg, Lars and Antonsen, Lene and Aplonova, Katya and Aranzabe, Maria Jesus and Arutie, Gashaw and Asahara, Masayuki and Ateyah, Luma and Attia, Mohammed and Atutxa, Aitziber and Augustinus, Liesbeth and Badmaeva, Elena and Ballesteros, Miguel and Banerjee, Esha and Bank, Sebastian and Barbu Mititelu, Verginica and Basmov, Victoria and Bauer, John and Bellato, Sandra and Bengoetxea, Kepa and Berzak, Yevgeni and Bhat, Irshad Ahmad and Bhat, Riyaz Ahmad and Biagetti, Erica and Bick, Eckhard and Blokland, Rogier and Bobicev, Victoria and B{\"o}rstell, Carl and Bosco, Cristina and Bouma, Gosse and Bowman, Sam and Boyd, Adriane and Burchardt, Aljoscha and Candito, Marie and Caron, Bernard and Caron, Gauthier and Cebiro{\u g}lu Eryi{\u g}it, G{\"u}l{\c s}en and Cecchini, Flavio Massimiliano and Celano, Giuseppe G. A. and {\v C}{\'e}pl{\"o}, Slavom{\'{\i}}r and Cetin, Savas and Chalub, Fabricio and Choi, Jinho and Cho, Yongseok and Chun, Jayeol and Cinkov{\'a}, Silvie and Collomb, Aur{\'e}lie and {\c C}{\"o}ltekin, {\c C}a{\u g}r{\i} and Connor, Miriam and Courtin, Marine and Davidson, Elizabeth and de Marneffe, Marie-Catherine and de Paiva, Valeria and Diaz de Ilarraza, Arantza and Dickerson, Carly and Dirix, Peter and Dobrovoljc, Kaja and Dozat, Timothy and Droganova, Kira and Dwivedi, Puneet and Eli, Marhaba and Elkahky, Ali and Ephrem, Binyam and Erjavec, Toma{\v z} and Etienne, Aline and Farkas, Rich{\'a}rd and Fernandez Alcalde, Hector and Foster, Jennifer and Freitas, Cl{\'a}udia and Gajdo{\v s}ov{\'a}, Katar{\'{\i}}na and Galbraith, Daniel and Garcia, Marcos and G{\"a}rdenfors, Moa and Garza, Sebastian and Gerdes, Kim and Ginter, Filip and Goenaga, Iakes and Gojenola, Koldo and G{\"o}k{\i}rmak, Memduh and Goldberg, Yoav and G{\'o}mez Guinovart, Xavier and Gonz{\'a}les Saavedra, Berta and Grioni,
 Matias and Gr{\=
u}z{\={\i}}tis, Normunds and Guillaume, Bruno and Guillot-Barbance, C{\'e}line and Habash, Nizar and Haji{\v c}, Jan and Haji{\v c} jr., Jan and H{\`a} M{\~y}, Linh and Han, Na-Rae and Harris, Kim and Haug, Dag and Hladk{\'a}, Barbora and Hlav{\'a}{\v c}ov{\'a}, Jaroslava and Hociung, Florinel and Hohle, Petter and Hwang, Jena and Ion, Radu and Irimia, Elena and Ishola, {\d O}l{\'a}j{\'{\i}}d{\'e} and Jel{\'{\i}}nek, Tom{\'a}{\v s} and Johannsen, Anders and J{\o}rgensen, Fredrik and Ka{\c s}{\i}kara, H{\"u}ner and Kahane, Sylvain and Kanayama, Hiroshi and Kanerva, Jenna and Katz, Boris and Kayadelen, Tolga and Kenney, Jessica and Kettnerov{\'a}, V{\'a}clava and Kirchner, Jesse and Kopacewicz, Kamil and Kotsyba, Natalia and Krek, Simon and Kwak, Sookyoung and Laippala, Veronika and Lambertino, Lorenzo and Lam, Lucia and Lando, Tatiana and Larasati, Septina Dian and Lavrentiev, Alexei and Lee, John and L{\^e} H{\`{\^o}}ng, PhÆ°Æ¡ng and Lenci, Alessandro and Lertpradit, Saran and Leung, Herman and Li, Cheuk Ying and Li, Josie and Li, Keying and Lim, {KyungTae} and Ljube{\v s}i{\'c}, Nikola and Loginova, Olga and Lyashevskaya, Olga and Lynn, Teresa and Macketanz, Vivien and Makazhanov, Aibek and Mandl, Michael and Manning, Christopher and Manurung, Ruli and M{\u a}r{\u a}nduc, C{\u a}t{\u a}lina and Mare{\v c}ek, David and Marheinecke, Katrin and Mart{\'{\i}}nez Alonso, H{\'e}ctor and Martins, Andr{\'e} and Ma{\v s}ek, Jan and Matsumoto, Yuji and {McDonald}, Ryan and Mendon{\c c}a, Gustavo and Miekka, Niko and Misirpashayeva, Margarita and Missil{\"a}, Anna and Mititelu, C{\u a}t{\u a}lin and Miyao, Yusuke and Montemagni, Simonetta and More, Amir and Moreno Romero, Laura and Mori, Keiko Sophie and Mori, Shinsuke and Mortensen, Bjartur and Moskalevskyi, Bohdan and Muischnek, Kadri and Murawaki, Yugo and M{\"u}{\"u}risep, Kaili and Nainwani, Pinkey and Navarro Hor{\~n}iacek, Juan Ignacio and Nedoluzhko,
 Anna and Ne{\v s}pore-B{\=e}rzkalne, Gunta and Nguy{\~{\^e}}n Th{\d i}, LÆ°Æ¡ng and Nguy{\~{\^e}}n Th{\d i} Minh, Huy{\`{\^e}}n and Nikolaev, Vitaly and Nitisaroj, Rattima and Nurmi, Hanna and Ojala, Stina and Ol{\'u}{\`o}kun, Ad{\'e}day{\d o}Ì€ and Omura, Mai and Osenova, Petya and {\"O}stling, Robert and {\O}vrelid, Lilja and Partanen, Niko and Pascual, Elena and Passarotti, Marco and Patejuk, Agnieszka and Paulino-Passos, Guilherme and Peng, Siyao and Perez, Cenel-Augusto and Perrier, Guy and Petrov, Slav and Piitulainen, Jussi and Pitler, Emily and Plank, Barbara and Poibeau, Thierry and Popel, Martin and Pretkalni{\c n}a, Lauma and Pr{\'e}vost, Sophie and Prokopidis, Prokopis and Przepi{\'o}rkowski, Adam and Puolakainen, Tiina and Pyysalo, Sampo and R{\"a}{\"a}bis, Andriela and Rademaker, Alexandre and Ramasamy, Loganathan and Rama, Taraka and Ramisch, Carlos and Ravishankar, Vinit and Real, Livy and Reddy, Siva and Rehm, Georg and Rie{\ss}ler, Michael and Rinaldi, Larissa and Rituma, Laura and Rocha, Luisa and Romanenko, Mykhailo and Rosa, Rudolf and Rovati, Davide and RoÈ™ca, Valentin and Rudina, Olga and Rueter, Jack and Sadde, Shoval and Sagot, Beno{\^{\i}}t and Saleh, Shadi and Samard{\v z}i{\'c}, Tanja and Samson, Stephanie and Sanguinetti,
 Manuela and Saul{\={\i}}te, Baiba and Sawanakunanon, Yanin and Schneider, Nathan and Schuster, Sebastian and Seddah, Djam{\'e} and Seeker, Wolfgang and Seraji, Mojgan and Shen, Mo and Shimada, Atsuko and Shohibussirri, Muh and Sichinava, Dmitry and Silveira, Natalia and Simi, Maria and Simionescu, Radu and Simk{\'o}, Katalin and {\v S}imkov{\'a}, M{\'a}ria and Simov, Kiril and Smith, Aaron and Soares-Bastos, Isabela and Spadine, Carolyn and Stella, Antonio and Straka, Milan and Strnadov{\'a}, Jana and Suhr, Alane and Sulubacak, Umut and Sz{\'a}nt{\'o}, Zsolt and Taji, Dima and Takahashi, Yuta and Tanaka, Takaaki and Tellier, Isabelle and Trosterud, Trond and Trukhina, Anna and Tsarfaty, Reut and Tyers, Francis and Uematsu, Sumire and Ure{\v s}ov{\'a}, Zde{\v n}ka and Uria, Larraitz and Uszkoreit, Hans and Vajjala, Sowmya and van Niekerk, Daniel and van Noord, Gertjan and Varga, Viktor and Villemonte de la Clergerie, Eric and Vincze, Veronika and Wallin, Lars and Wang, Jing Xian and Washington, Jonathan North and Williams, Seyi and Wir{\'e}n, Mats and Woldemariam, Tsegay and Wong, Tak-sum and Yan, Chunxiao and Yavrumyan, Marat M. and Yu, Zhuoran and {\v Z}abokrtsk{\'y}, Zden{\v e}k and Zeldes, Amir and Zeman, Daniel and Zhang, Manying and Zhu, Hanzhi},
 url = {http://hdl.handle.net/11234/1-2895},
 note = {{LINDAT}/{CLARIN} digital library at the Institute of Formal and Applied Linguistics ({{\'U}FAL}), Faculty of Mathematics and Physics, Charles University},
 copyright = {Licence Universal Dependencies v2.3},
 year = {2018} }
 
 @misc{bies2012english,
  title={English web treebank},
  author={Bies, Ann and Mott, Justin and Warner, Colin and Kulick, Seth},
  note={Linguistic Data Consortium, Philadelphia, PA},
  year={2012}
}
@InProceedings{RKS08,
 author    = {Ravi, Sujith and Kevin Knight and Radu Soricut},
 title     = {Automatic Prediction of Parser Accuracy},
 booktitle = {Proceedings of EMNLP},
 OPTmonth     = {},
 year      = {2008},
 address   = {Hawaii},
 OPTpublisher = {},
 OPTpages     = {},
 url       = {},
 OPTnote      = {}
 }
 
 @inproceedings{guo-etal-2016-universal,
    title = "A Universal Framework for Inductive Transfer Parsing across Multi-typed Treebanks",
    author = "Guo, Jiang  and
      Che, Wanxiang  and
      Wang, Haifeng  and
      Liu, Ting",
    booktitle = "Proceedings of {COLING} 2016, the 26th International Conference on Computational Linguistics: Technical Papers",
    month = dec,
    year = "2016",
    address = "Osaka, Japan",
    publisher = "The COLING 2016 Organizing Committee",
    url = "https://www.aclweb.org/anthology/C16-1002",
    pages = "12--22",
    abstract = "Various treebanks have been released for dependency parsing.Despite that treebanks may belong to different languages or have differentannotation schemes, they contain common syntactic knowledge that is potentialto benefit each other.This paper presents a universal framework for transfer parsing acrossmulti-typed treebanks with deep multi-task learning.We consider two kinds of treebanks as source: the multilingual universaltreebanks and the monolingual heterogeneous treebanks.Knowledge across the source and target treebanks are effectively transferredthrough multi-level parameter sharing.Experiments on several benchmark datasets in various languages demonstrate thatour approach can make effective use of arbitrary source treebanks to improvetarget parsing models.",
}

@article{jiang-etal-2015-automatic,
    title = "Automatic Adaptation of Annotations",
    author = {Jiang, Wenbin  and
      L{\"u}, Yajuan  and
      Huang, Liang  and
      Liu, Qun},
    journal = "American Journal of Computational Linguistics",
    volume = "41",
    number = "1",
    month = mar,
    year = "2015",
    url = "https://www.aclweb.org/anthology/J15-1005",
    doi = "10.1162/COLI_a_00210",
    pages = "119--147",
}

@inproceedings{niu-etal-2009-exploiting,
    title = "Exploiting Heterogeneous Treebanks for Parsing",
    author = "Niu, Zheng-Yu  and
      Wang, Haifeng  and
      Wu, Hua",
    booktitle = "Proceedings of the Joint Conference of the 47th Annual Meeting of the {ACL} and the 4th International Joint Conference on Natural Language Processing of the {AFNLP}",
    month = aug,
    year = "2009",
    address = "Suntec, Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P09-1006",
    pages = "46--54",
}

@inproceedings{johansson-2013-training,
    title = "Training Parsers on Incompatible Treebanks",
    author = "Johansson, Richard",
    booktitle = "Proceedings of the 2013 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2013",
    address = "Atlanta, Georgia",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/N13-1013",
    pages = "127--137",
}

@inproceedings{li-etal-2012-exploiting,
    title = "Exploiting Multiple Treebanks for Parsing with Quasi-synchronous Grammars",
    author = "Li, Zhenghua  and
      Liu, Ting  and
      Che, Wanxiang",
    booktitle = "Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2012",
    address = "Jeju Island, Korea",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P12-1071",
    pages = "675--684",
}

@inproceedings{tyers-etal-2018-multi,
    title = "Multi-source synthetic treebank creation for improved cross-lingual dependency parsing",
    author = "Tyers, Francis  and
      Sheyanova, Mariya  and
      Martynova, Aleksandra  and
      Stepachev, Pavel  and
      Vinogorodskiy, Konstantin",
    booktitle = "Proceedings of the Second Workshop on Universal Dependencies ({UDW} 2018)",
    month = nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W18-6017",
    pages = "144--150",
    abstract = "",
}

@article{agic-etal-2016-multilingual,
    title = "Multilingual Projection for Parsing Truly Low-Resource Languages",
    author = "Agi{\'c}, {\v{Z}}eljko  and
      Johannsen, Anders  and
      Plank, Barbara  and
      Mart{\'\i}nez Alonso, H{\'e}ctor  and
      Schluter, Natalie  and
      S{\o}gaard, Anders",
    journal = "Transactions of the Association for Computational Linguistics",
    volume = "4",
    year = "2016",
    url = "https://www.aclweb.org/anthology/Q16-1022",
    doi = "10.1162/tacl_a_00100",
    pages = "301--312",
    abstract = "We propose a novel approach to cross-lingual part-of-speech tagging and dependency parsing for truly low-resource languages. Our annotation projection-based approach yields tagging and parsing models for over 100 languages. All that is needed are freely available parallel texts, and taggers and parsers for resource-rich languages. The empirical evaluation across 30 test languages shows that our method consistently provides top-level accuracies, close to established upper bounds, and outperforms several competitive baselines.",
}

@article{chu1965shortest,
  title={On the shortest arborescence of a directed graph},
  author={Chu, Yoeng-Jin and Liu, Tseng-Hong},
  journal={Scientia Sinica},
  volume={14},
  pages={1396--1400},
  year={1965}
}

@article{edmonds1967optimum,
  title={Optimum branchings},
  author={Edmonds, Jack},
  journal={Journal of Research of the national Bureau of Standards B},
  volume={71},
  number={4},
  pages={233--240},
  year={1967}
}

@inproceedings{dyer-etal-2013-simple,
    title = "A Simple, Fast, and Effective Reparameterization of {IBM} Model 2",
    author = "Dyer, Chris  and
      Chahuneau, Victor  and
      Smith, Noah A.",
    booktitle = "Proceedings of the 2013 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2013",
    address = "Atlanta, Georgia",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/N13-1073",
    pages = "644--648",
}

@article{hwa2005bootstrapping,
  title={Bootstrapping parsers via syntactic projection across parallel texts},
  author={Hwa, Rebecca and Resnik, Philip and Weinberg, Amy and Cabezas, Clara and Kolak, Okan},
  journal={Natural language engineering},
  volume={11},
  number={3},
  pages={311--325},
  year={2005},
  publisher={Cambridge University Press}
}

@inproceedings{zeman-resnik-2008-cross,
    title = "Cross-Language Parser Adaptation between Related Languages",
    author = "Zeman, Daniel  and
      Resnik, Philip",
    booktitle = "Proceedings of the {IJCNLP}-08 Workshop on {NLP} for Less Privileged Languages",
    year = "2008",
    url = "https://www.aclweb.org/anthology/I08-3008",
}

@inproceedings{mcdonald-etal-2011-multi,
    title = "Multi-Source Transfer of Delexicalized Dependency Parsers",
    author = "McDonald, Ryan  and
      Petrov, Slav  and
      Hall, Keith",
    booktitle = "Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing",
    month = jul,
    year = "2011",
    address = "Edinburgh, Scotland, UK.",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D11-1006",
    pages = "62--72",
}

@inproceedings{sagae-lavie-2006-parser,
    title = "Parser Combination by Reparsing",
    author = "Sagae, Kenji  and
      Lavie, Alon",
    booktitle = "Proceedings of the Human Language Technology Conference of the {NAACL}, Companion Volume: Short Papers",
    month = jun,
    year = "2006",
    address = "New York City, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/N06-2033",
    pages = "129--132",
}

@inproceedings{mcdonald-etal-2005-non,
    title = "Non-Projective Dependency Parsing using Spanning Tree Algorithms",
    author = "McDonald, Ryan  and
      Pereira, Fernando  and
      Ribarov, Kiril  and
      Haji{\v{c}}, Jan",
    booktitle = "Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing",
    month = oct,
    year = "2005",
    address = "Vancouver, British Columbia, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/H05-1066",
    pages = "523--530",
}

@inproceedings{rosa-marecek-2018-cuni,
    title = "{CUNI} x-ling: Parsing Under-Resourced Languages in {C}o{NLL} 2018 {UD} Shared Task",
    author = "Rosa, Rudolf  and
      Mare{\v{c}}ek, David",
    booktitle = "Proceedings of the {C}o{NLL} 2018 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies",
    month = oct,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/K18-2019",
    doi = "10.18653/v1/K18-2019",
    pages = "187--196",
    abstract = "This is a system description paper for the CUNI x-ling submission to the CoNLL 2018 UD Shared Task. We focused on parsing under-resourced languages, with no or little training data available. We employed a wide range of approaches, including simple word-based treebank translation, combination of delexicalized parsers, and exploitation of available morphological dictionaries, with a dedicated setup tailored to each of the languages. In the official evaluation, our submission was identified as the clear winner of the Low-resource languages category.",
}

@inproceedings{al-rfou-etal-2013-polyglot,
    title = "{P}olyglot: Distributed Word Representations for Multilingual {NLP}",
    author = "Al-Rfou{'}, Rami  and
      Perozzi, Bryan  and
      Skiena, Steven",
    booktitle = "Proceedings of the Seventeenth Conference on Computational Natural Language Learning",
    month = aug,
    year = "2013",
    address = "Sofia, Bulgaria",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W13-3520",
    pages = "183--192",
}

@inproceedings{meechanmadden2019,
  title={How to Parse Low-Resource Languages:
Cross-Lingual Parsing, Target Language Annotation, or Both?},
  author={Meechan-Maddon, Ailsa and Joakim Nivre},
  booktitle={Proceedings of DepLing},
  pages={},
  year={2019}
}

@article{graves2005framewise,
  title={Framewise phoneme classification with bidirectional LSTM and other neural network architectures},
  author={Graves, Alex and Schmidhuber, J{\"u}rgen},
  journal={Neural networks},
  volume={18},
  number={5-6},
  pages={602--610},
  year={2005},
  publisher={Elsevier}
}

@inproceedings{peters-etal-2018-deep,
    title = "Deep Contextualized Word Representations",
    author = "Peters, Matthew  and
      Neumann, Mark  and
      Iyyer, Mohit  and
      Gardner, Matt  and
      Clark, Christopher  and
      Lee, Kenton  and
      Zettlemoyer, Luke",
    booktitle = "Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)",
    month = jun,
    year = "2018",
    address = "New Orleans, Louisiana",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/N18-1202",
    doi = "10.18653/v1/N18-1202",
    pages = "2227--2237",
    abstract = "We introduce a new type of deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy). Our word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pre-trained on a large text corpus. We show that these representations can be easily added to existing models and significantly improve the state of the art across six challenging NLP problems, including question answering, textual entailment and sentiment analysis. We also present an analysis showing that exposing the deep internals of the pre-trained network is crucial, allowing downstream models to mix different types of semi-supervision signals.",
}

@inproceedings{devlin-etal-2019-bert,
    title = "{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    author = "Devlin, Jacob  and
      Chang, Ming-Wei  and
      Lee, Kenton  and
      Toutanova, Kristina",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/N19-1423",
    doi = "10.18653/v1/N19-1423",
    pages = "4171--4186",
    abstract = "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7{\%} (4.6{\%} absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",
}

@inproceedings{schlichtkrull-sogaard-2017-cross,
    title = "Cross-Lingual Dependency Parsing with Late Decoding for Truly Low-Resource Languages",
    author = "Schlichtkrull, Michael  and
      S{\o}gaard, Anders",
    booktitle = "Proceedings of the 15th Conference of the {E}uropean Chapter of the Association for Computational Linguistics: Volume 1, Long Papers",
    month = apr,
    year = "2017",
    address = "Valencia, Spain",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/E17-1021",
    pages = "220--229",
    abstract = "In cross-lingual dependency annotation projection, information is often lost during transfer because of early decoding. We present an end-to-end graph-based neural network dependency parser that can be trained to reproduce matrices of edge scores, which can be directly projected across word alignments. We show that our approach to cross-lingual dependency parsing is not only simpler, but also achieves an absolute improvement of 2.25{\%} averaged across 10 languages compared to the previous state of the art.",
}

@article{tiedemann2016opus,
  title={OPUS--Parallel Corpora for Everyone},
  author={Tiedemann, J{\"o}rg},
  journal={Baltic Journal of Modern Computing},
  pages={384},
  year={2016},
  publisher={Latvijas Universit{\=a}te}
}

@inproceedings{guo-etal-2015-cross,
    title = "Cross-lingual Dependency Parsing Based on Distributed Representations",
    author = "Guo, Jiang  and
      Che, Wanxiang  and
      Yarowsky, David  and
      Wang, Haifeng  and
      Liu, Ting",
    booktitle = "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = jul,
    year = "2015",
    address = "Beijing, China",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P15-1119",
    doi = "10.3115/v1/P15-1119",
    pages = "1234--1244",
}


@Article{TiedemannZeljko16,
  author = 	 {Tiedemann, JÃ¶rg and Agi{\'c}, {\v{Z}}eljko},
  title = 	 {Synthetic Treebanking for Cross-Lingual Dependency Parsing},
  journal = 	 {Journal of Artificial Intelligence Research},
  year = 	 {2016},
  OPTkey = 	 {},
  volume = 	 {5},
  number = 	 {},
  pages = 	 {},
  OPTmonth = 	 {},
  OPTnote = 	 {A thorough comparison of pre-neural crosslingual parsing. Various forms of projected annotation methods are compared to delexicalised baselines, and also to the use of SMT instead of parallel corpora to produce synthetic treebanks in the target language. Interesting points to note: syntax-based SMT is more useful than phrase-based SMT, and their back-projection approach is similar to Tyers et al. 2018 except that they perform the translation into the source and projection of the resulting parse during test time instead of using the approach to train a synthetic treebank for the target language.},
  OPTannote = 	 {}
}

@inproceedings{plank-etal-2016-multilingual,
    title = "Multilingual Part-of-Speech Tagging with Bidirectional Long Short-Term Memory Models and Auxiliary Loss",
    author = "Plank, Barbara  and
      S{\o}gaard, Anders  and
      Goldberg, Yoav",
    booktitle = "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)",
    month = aug,
    year = "2016",
    address = "Berlin, Germany",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P16-2067",
    doi = "10.18653/v1/P16-2067",
    pages = "412--418",
}
@inproceedings{DBLP:journals/corr/abs-1904-02099,
    title = "75 Languages, 1 Model: Parsing {U}niversal {D}ependencies Universally",
    author = "Kondratyuk, Dan  and
      Straka, Milan",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D19-1279",
    doi = "10.18653/v1/D19-1279",
    pages = "2779--2795",
    abstract = "We present UDify, a multilingual multi-task model capable of accurately predicting universal part-of-speech, morphological features, lemmas, and dependency trees simultaneously for all 124 Universal Dependencies treebanks across 75 languages. By leveraging a multilingual BERT self-attention model pretrained on 104 languages, we found that fine-tuning it on all datasets concatenated together with simple softmax classifiers for each UD task can meet or exceed state-of-the-art UPOS, UFeats, Lemmas, (and especially) UAS, and LAS scores, without requiring any recurrent or language-specific components. We evaluate UDify for multilingual learning, showing that low-resource languages benefit the most from cross-linguistic annotations. We also evaluate for zero-shot learning, with results suggesting that multilingual training provides strong UD predictions even for languages that neither UDify nor BERT have ever been trained on.",
}

@inproceedings{lynn-etal-2014-cross,
    title = "Cross-lingual Transfer Parsing for Low-Resourced Languages: An {I}rish Case Study",
    author = "Lynn, Teresa  and
      Foster, Jennifer  and
      Dras, Mark  and
      Tounsi, Lamia",
    booktitle = "Proceedings of the First Celtic Language Technology Workshop",
    month = aug,
    year = "2014",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics and Dublin City University",
    url = "https://www.aclweb.org/anthology/W14-4606",
    doi = "10.3115/v1/W14-4606",
    pages = "41--49",
}

@inproceedings{vilares-etal-2016-one,
    title = "One model, two languages: training bilingual parsers with harmonized treebanks",
    author = "Vilares, David  and
      G{\'o}mez-Rodr{\'\i}guez, Carlos  and
      Alonso, Miguel A.",
    booktitle = "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)",
    month = aug,
    year = "2016",
    address = "Berlin, Germany",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P16-2069",
    doi = "10.18653/v1/P16-2069",
    pages = "425--431",
}

@inproceedings{foster-etal-2008-adapting,
    title = "Adapting a {WSJ}-Trained Parser to Grammatically Noisy Text",
    author = "Foster, Jennifer  and
      Wagner, Joachim  and
      van Genabith, Josef",
    booktitle = "Proceedings of ACL-08: HLT, Short Papers",
    month = jun,
    year = "2008",
    address = "Columbus, Ohio",
    publisher = "Association for Computational Linguistics",
    pages = "221--224",
}

@inproceedings{foster:hal-00702445,
  TITLE = {{\#hardtoparse: POS Tagging and Parsing the Twitterverse}},
  AUTHOR = {Foster, Jennifer and {\c C}etinoglu, {\"O}zlem and Wagner, Joachim and Le Roux, Joseph and Hogan, Stephen and Nivre, Joakim and Hogan, Deirdre and Van Genabith, Josef},
  URL = {https://hal.archives-ouvertes.fr/hal-00702445},
  BOOKTITLE = {{AAAI 2011 Workshop On Analyzing Microtext}},
  ADDRESS = {United States},
  PAGES = {20-25},
  YEAR = {2011},
  PDF = {https://hal.archives-ouvertes.fr/hal-00702445/file/aaai_mt_2011.pdf},
  HAL_ID = {hal-00702445},
  HAL_VERSION = {v1},
}

@inproceedings{yamada-matsumoto-2003-statistical,
    title = "Statistical Dependency Analysis with Support Vector Machines",
    author = "Yamada, Hiroyasu  and
      Matsumoto, Yuji",
    booktitle = "Proceedings of the Eighth International Conference on Parsing Technologies",
    month = apr,
    year = "2003",
    address = "Nancy, France",
    pages = "195--206",
    abstract = "In this paper, we propose a method for analyzing word-word dependencies using deterministic bottom-up manner using Support Vector machines. We experimented with dependency trees converted from Penn treebank data, and achieved over 90{\%} accuracy of word-word dependency. Though the result is little worse than the most up-to-date phrase structure based parsers, it looks satisfactorily accurate considering that our parser uses no information from phrase structures.",
}

@inproceedings{nivre-2003-efficient,
    title = "An Efficient Algorithm for Projective Dependency Parsing",
    author = "Nivre, Joakim",
    booktitle = "Proceedings of the Eighth International Conference on Parsing Technologies",
    month = apr,
    year = "2003",
    address = "Nancy, France",
    pages = "149--160",
    abstract = "This paper presents a deterministic parsing algorithm for projective dependency grammar. The running time of the algorithm is linear in the length of the input string, and the dependency graph produced is guaranteed to be projective and acyclic. The algorithm has been experimentally evaluated in parsing unrestricted Swedish text, achieving an accuracy above 85{\%} with a very simple grammar.",
}

@inproceedings{nivre-2004-incrementality,
    title = "Incrementality in Deterministic Dependency Parsing",
    author = "Nivre, Joakim",
    booktitle = "Proceedings of the Workshop on Incremental Parsing: Bringing Engineering and Cognition Together",
    month = jul,
    year = "2004",
    address = "Barcelona, Spain",
    publisher = "Association for Computational Linguistics",
    pages = "50--57",
}

@inproceedings{nivre-nilsson-2005-pseudo,
    title = "Pseudo-Projective Dependency Parsing",
    author = "Nivre, Joakim  and
      Nilsson, Jens",
    booktitle = "Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics ({ACL}{'}05)",
    month = jun,
    year = "2005",
    address = "Ann Arbor, Michigan",
    publisher = "Association for Computational Linguistics",
    doi = "10.3115/1219840.1219853",
    pages = "99--106",
}

@inproceedings{bohnet-2010-top,
    title = "Top Accuracy and Fast Dependency Parsing is not a Contradiction",
    author = "Bohnet, Bernd",
    booktitle = "Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010)",
    month = aug,
    year = "2010",
    address = "Beijing, China",
    publisher = "Coling 2010 Organizing Committee",
    pages = "89--97",
}

@article{kubler2009dependency,
  title={Dependency parsing},
  author={K{\"u}bler, Sandra and McDonald, Ryan and Nivre, Joakim},
  journal={Synthesis Lectures on Human Language Technologies},
  volume={1},
  number={1},
  pages={1--127},
  year={2009},
  publisher={Morgan \& Claypool Publishers}
}

@inproceedings{petrov-etal-2012-universal,
    title = "A Universal Part-of-Speech Tagset",
    author = "Petrov, Slav  and
      Das, Dipanjan  and
      McDonald, Ryan",
    booktitle = "Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12)",
    month = may,
    year = "2012",
    address = "Istanbul, Turkey",
    publisher = "European Language Resources Association (ELRA)",
    url = "http://www.lrec-conf.org/proceedings/lrec2012/pdf/274_Paper.pdf",
    pages = "2089--2096",
}

@inproceedings{dyer-etal-2015-transition,
    title = "Transition-Based Dependency Parsing with Stack Long Short-Term Memory",
    author = "Dyer, Chris  and
      Ballesteros, Miguel  and
      Ling, Wang  and
      Matthews, Austin  and
      Smith, Noah A.",
    booktitle = "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = jul,
    year = "2015",
    address = "Beijing, China",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P15-1033",
    doi = "10.3115/v1/P15-1033",
    pages = "334--343",
}

@article{kiperwasser-goldberg-2016-simple,
    title = "Simple and Accurate Dependency Parsing Using Bidirectional {LSTM} Feature Representations",
    author = "Kiperwasser, Eliyahu  and
      Goldberg, Yoav",
    journal = "Transactions of the Association for Computational Linguistics",
    volume = "4",
    year = "2016",
    url = "https://www.aclweb.org/anthology/Q16-1023",
    doi = "10.1162/tacl_a_00101",
    pages = "313--327",
    abstract = "We present a simple and effective scheme for dependency parsing which is based on bidirectional-LSTMs (BiLSTMs). Each sentence token is associated with a BiLSTM vector representing the token in its sentential context, and feature vectors are constructed by concatenating a few BiLSTM vectors. The BiLSTM is trained jointly with the parser objective, resulting in very effective feature extractors for parsing. We demonstrate the effectiveness of the approach by applying it to a greedy transition-based parser as well as to a globally optimized graph-based parser. The resulting parsers have very simple architectures, and match or surpass the state-of-the-art accuracies on English and Chinese.",
}

@inproceedings{pennington-etal-2014-glove,
    title = "{G}love: Global Vectors for Word Representation",
    author = "Pennington, Jeffrey  and
      Socher, Richard  and
      Manning, Christopher",
    booktitle = "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ({EMNLP})",
    month = oct,
    year = "2014",
    address = "Doha, Qatar",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D14-1162",
    doi = "10.3115/v1/D14-1162",
    pages = "1532--1543",
}

@inproceedings{dozat-etal-2017-stanfords,
    title = "{S}tanford{'}s Graph-based Neural Dependency Parser at the {C}o{NLL} 2017 Shared Task",
    author = "Dozat, Timothy  and
      Qi, Peng  and
      Manning, Christopher D.",
    booktitle = "Proceedings of the {C}o{NLL} 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies",
    month = aug,
    year = "2017",
    address = "Vancouver, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/K17-3002",
    doi = "10.18653/v1/K17-3002",
    pages = "20--30",
    abstract = "This paper describes the neural dependency parser submitted by Stanford to the CoNLL 2017 Shared Task on parsing Universal Dependencies. Our system uses relatively simple LSTM networks to produce part of speech tags and labeled dependency parses from segmented and tokenized sequences of words. In order to address the rare word problem that abounds in languages with complex morphology, we include a character-based word representation that uses an LSTM to produce embeddings from sequences of characters. Our system was ranked first according to all five relevant metrics for the system: UPOS tagging (93.09{\%}), XPOS tagging (82.27{\%}), unlabeled attachment score (81.30{\%}), labeled attachment score (76.30{\%}), and content word labeled attachment score (72.57{\%}).",
}

@inproceedings{zhang-etal-2019-empirical,
    title = "An Empirical Investigation of Structured Output Modeling for Graph-based Neural Dependency Parsing",
    author = "Zhang, Zhisong  and
      Ma, Xuezhe  and
      Hovy, Eduard",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P19-1562",
    doi = "10.18653/v1/P19-1562",
    pages = "5592--5598",
    abstract = "In this paper, we investigate the aspect of structured output modeling for the state-of-the-art graph-based neural dependency parser (Dozat and Manning, 2017). With evaluations on 14 treebanks, we empirically show that global output-structured models can generally obtain better performance, especially on the metric of sentence-level Complete Match. However, probably because neural models already learn good global views of the inputs, the improvement brought by structured output modeling is modest.",
}

@inproceedings{eisner-1996-three,
    title = "Three New Probabilistic Models for Dependency Parsing: An Exploration",
    author = "Eisner, Jason M.",
    booktitle = "{COLING} 1996 Volume 1: The 16th International Conference on Computational Linguistics",
    year = "1996",
    url = "https://www.aclweb.org/anthology/C96-1058",
}

@inproceedings{mcdonald-etal-2005-online,
    title = "Online Large-Margin Training of Dependency Parsers",
    author = "McDonald, Ryan  and
      Crammer, Koby  and
      Pereira, Fernando",
    booktitle = "Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics ({ACL}{'}05)",
    month = jun,
    year = "2005",
    address = "Ann Arbor, Michigan",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P05-1012",
    doi = "10.3115/1219840.1219852",
    pages = "91--98",
}

@inproceedings{vania-etal-2018-character,
    title = "What do character-level models learn about morphology? The case of dependency parsing",
    author = "Vania, Clara  and
      Grivas, Andreas  and
      Lopez, Adam",
    booktitle = "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
    month = oct # "-" # nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D18-1278",
    doi = "10.18653/v1/D18-1278",
    pages = "2573--2583",
    abstract = "When parsing morphologically-rich languages with neural models, it is beneficial to model input at the character level, and it has been claimed that this is because character-level models learn morphology. We test these claims by comparing character-level models to an oracle with access to explicit morphological analysis on twelve languages with varying morphological typologies. Our results highlight many strengths of character-level models, but also show that they are poor at disambiguating some words, particularly in the face of case syncretism. We then demonstrate that explicitly modeling morphological case improves our best model, showing that character-level models can benefit from targeted forms of explicit morphological modeling.",
}

@inproceedings{goldberg-nivre-2012-dynamic,
    title = "A Dynamic Oracle for Arc-Eager Dependency Parsing",
    author = "Goldberg, Yoav  and
      Nivre, Joakim",
    booktitle = "Proceedings of {COLING} 2012",
    month = dec,
    year = "2012",
    address = "Mumbai, India",
    publisher = "The COLING 2012 Organizing Committee",
    url = "https://www.aclweb.org/anthology/C12-1059",
    pages = "959--976",
}

@inproceedings{zhang-clark-2008-tale,
    title = "A Tale of Two Parsers: {I}nvestigating and Combining Graph-based and Transition-based Dependency Parsing",
    author = "Zhang, Yue  and
      Clark, Stephen",
    booktitle = "Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing",
    month = oct,
    year = "2008",
    address = "Honolulu, Hawaii",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D08-1059",
    pages = "562--571",
}

@inproceedings{zhang-etal-2017-dependency-parsing,
    title = "Dependency Parsing as Head Selection",
    author = "Zhang, Xingxing  and
      Cheng, Jianpeng  and
      Lapata, Mirella",
    booktitle = "Proceedings of the 15th Conference of the {E}uropean Chapter of the Association for Computational Linguistics: Volume 1, Long Papers",
    month = apr,
    year = "2017",
    address = "Valencia, Spain",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/E17-1063",
    pages = "665--676",
    abstract = "Conventional graph-based dependency parsers guarantee a tree structure both during training and inference. Instead, we formalize dependency parsing as the problem of independently selecting the head of each word in a sentence. Our model which we call DENSE (as shorthand for \textbf{De}pendency \textbf{N}eural \textbf{Se}lection) produces a distribution over possible heads for each word using features obtained from a bidirectional recurrent neural network. Without enforcing structural constraints during training, DeNSe generates (at inference time) trees for the overwhelming majority of sentences, while non-tree outputs can be adjusted with a maximum spanning tree algorithm. We evaluate DeNSe on four languages (English, Chinese, Czech, and German) with varying degrees of non-projectivity. Despite the simplicity of the approach, our parsers are on par with the state of the art.",
}

@misc{11234/1-2988,
 title = {Universal Dependencies 2.4},
 author = {Nivre, Joakim and Abrams, Mitchell and Agi{\'c}, {\v Z}eljko and Ahrenberg,
 Lars and Aleksandravi{\v c}i{\=u}t{\.e}, Gabriel{\.e} and Antonsen, Lene and Aplonova, Katya and Aranzabe, Maria Jesus and Arutie, Gashaw and Asahara, Masayuki and Ateyah, Luma and Attia, Mohammed and Atutxa, Aitziber and Augustinus, Liesbeth and Badmaeva, Elena and Ballesteros, Miguel and Banerjee, Esha and Bank, Sebastian and Barbu Mititelu, Verginica and Basmov, Victoria and Bauer, John and Bellato, Sandra and Bengoetxea, Kepa and Berzak, Yevgeni and Bhat, Irshad Ahmad and Bhat, Riyaz Ahmad and Biagetti, Erica and Bick, Eckhard and Bielinskien{\.e}, Agn{\.e} and Blokland, Rogier and Bobicev, Victoria and Boizou, Lo{\"{\i}}c and Borges V{\"o}lker, Emanuel and B{\"o}rstell, Carl and Bosco, Cristina and Bouma, Gosse and Bowman, Sam and Boyd, Adriane and Brokait{\.e}, Kristina and Burchardt, Aljoscha and Candito, Marie and Caron, Bernard and Caron, Gauthier and Cebiro{\u g}lu Eryi{\u g}it, G{\"u}l{\c s}en and Cecchini, Flavio Massimiliano and Celano, Giuseppe G. A. and {\v C}{\'e}pl{\"o}, Slavom{\'{\i}}r and Cetin, Savas and Chalub, Fabricio and Choi, Jinho and Cho, Yongseok and Chun, Jayeol and Cinkov{\'a}, Silvie and Collomb, Aur{\'e}lie and {\c C}{\"o}ltekin, {\c C}a{\u g}r{\i} and Connor, Miriam and Courtin, Marine and Davidson, Elizabeth and de Marneffe, Marie-Catherine and de Paiva, Valeria and Diaz de Ilarraza, Arantza and Dickerson, Carly and Dione, Bamba and Dirix, Peter and Dobrovoljc, Kaja and Dozat, Timothy and Droganova, Kira and Dwivedi, Puneet and Eckhoff, Hanne and Eli, Marhaba and Elkahky, Ali and Ephrem, Binyam and Erjavec, Toma{\v z} and Etienne, Aline and Farkas, Rich{\'a}rd and Fernandez Alcalde, Hector and Foster, Jennifer and Freitas, Cl{\'a}udia and Fujita, Kazunori and Gajdo{\v s}ov{\'a}, Katar{\'{\i}}na and Galbraith, Daniel and Garcia, Marcos and G{\"a}rdenfors, Moa and Garza, Sebastian and Gerdes, Kim and Ginter, Filip and Goenaga, Iakes and Gojenola, Koldo and G{\"o}k{\i}rmak, Memduh and Goldberg, Yoav and G{\'o}mez Guinovart, Xavier and Gonz{\'a}lez Saavedra, Berta and Grioni,
 Matias and Gr{\=
u}z{\={\i}}tis, Normunds and Guillaume, Bruno and Guillot-Barbance, C{\'e}line and Habash, Nizar and Haji{\v c}, Jan and Haji{\v c} jr., Jan and H{\`a} M{\~y}, Linh and Han, Na-Rae and Harris, Kim and Haug, Dag and Heinecke, Johannes and Hennig, Felix and Hladk{\'a}, Barbora and Hlav{\'a}{\v c}ov{\'a}, Jaroslava and Hociung, Florinel and Hohle, Petter and Hwang, Jena and Ikeda, Takumi and Ion, Radu and Irimia, Elena and Ishola, {\d O}l{\'a}j{\'{\i}}d{\'e} and Jel{\'{\i}}nek, Tom{\'a}{\v s} and Johannsen, Anders and J{\o}rgensen, Fredrik and Ka{\c s}{\i}kara, H{\"u}ner and Kaasen, Andre and Kahane, Sylvain and Kanayama, Hiroshi and Kanerva, Jenna and Katz, Boris and Kayadelen, Tolga and Kenney, Jessica and Kettnerov{\'a}, V{\'a}clava and Kirchner, Jesse and K{\"o}hn, Arne and Kopacewicz, Kamil and Kotsyba, Natalia and Kovalevskait{\.e}, Jolanta and Krek, Simon and Kwak, Sookyoung and Laippala, Veronika and Lambertino, Lorenzo and Lam, Lucia and Lando, Tatiana and Larasati, Septina Dian and Lavrentiev, Alexei and Lee, John and L{\^e} H{\`{\^o}}ng, PhÆ°Æ¡ng and Lenci, Alessandro and Lertpradit, Saran and Leung, Herman and Li, Cheuk Ying and Li, Josie and Li, Keying and Lim, {KyungTae} and Li, Yuan and Ljube{\v s}i{\'c}, Nikola and Loginova, Olga and Lyashevskaya, Olga and Lynn, Teresa and Macketanz, Vivien and Makazhanov, Aibek and Mandl, Michael and Manning, Christopher and Manurung, Ruli and M{\u a}r{\u a}nduc, C{\u a}t{\u a}lina and Mare{\v c}ek, David and Marheinecke, Katrin and Mart{\'{\i}}nez Alonso, H{\'e}ctor and Martins, Andr{\'e} and Ma{\v s}ek, Jan and Matsumoto, Yuji and {McDonald}, Ryan and {McGuinness}, Sarah and Mendon{\c c}a, Gustavo and Miekka, Niko and Misirpashayeva, Margarita and Missil{\"a}, Anna and Mititelu, C{\u a}t{\u a}lin and Miyao, Yusuke and Montemagni, Simonetta and More, Amir and Moreno Romero, Laura and Mori, Keiko Sophie and Morioka, Tomohiko and Mori, Shinsuke and Moro, Shigeki and Mortensen, Bjartur and Moskalevskyi, Bohdan and Muischnek, Kadri and Murawaki, Yugo and M{\"u}{\"u}risep, Kaili and Nainwani, Pinkey and Navarro Hor{\~n}iacek, Juan Ignacio and Nedoluzhko,
 Anna and Ne{\v s}pore-B{\=e}rzkalne, Gunta and Nguy{\~{\^e}}n Th{\d i}, LÆ°Æ¡ng and Nguy{\~{\^e}}n Th{\d i} Minh, Huy{\`{\^e}}n and Nikaido, Yoshihiro and Nikolaev, Vitaly and Nitisaroj, Rattima and Nurmi, Hanna and Ojala, Stina and Ol{\'u}{\`o}kun, Ad{\'e}day{\d o}Ì€ and Omura, Mai and Osenova, Petya and {\"O}stling, Robert and {\O}vrelid, Lilja and Partanen, Niko and Pascual, Elena and Passarotti, Marco and Patejuk, Agnieszka and Paulino-Passos, Guilherme and Peljak-{\L}api{\'n}ska, Angelika and Peng, Siyao and Perez, Cenel-Augusto and Perrier, Guy and Petrova, Daria and Petrov, Slav and Piitulainen, Jussi and Pirinen, Tommi A and Pitler, Emily and Plank, Barbara and Poibeau, Thierry and Popel, Martin and Pretkalni{\c n}a, Lauma and Pr{\'e}vost, Sophie and Prokopidis, Prokopis and Przepi{\'o}rkowski, Adam and Puolakainen, Tiina and Pyysalo, Sampo and R{\"a}{\"a}bis, Andriela and Rademaker, Alexandre and Ramasamy, Loganathan and Rama, Taraka and Ramisch, Carlos and Ravishankar, Vinit and Real, Livy and Reddy, Siva and Rehm, Georg and Rie{\ss}ler, Michael and Rimkut{\.e}, Erika and Rinaldi, Larissa and Rituma, Laura and Rocha, Luisa and Romanenko, Mykhailo and Rosa, Rudolf and Rovati, Davide and RoÈ™ca, Valentin and Rudina, Olga and Rueter, Jack and Sadde, Shoval and Sagot, Beno{\^{\i}}t and Saleh, Shadi and Salomoni, Alessio and Samard{\v z}i{\'c}, Tanja and Samson, Stephanie and Sanguinetti, Manuela and S{\"a}rg,
 Dage and Saul{\={\i}}te, Baiba and Sawanakunanon, Yanin and Schneider, Nathan and Schuster, Sebastian and Seddah, Djam{\'e} and Seeker, Wolfgang and Seraji, Mojgan and Shen, Mo and Shimada, Atsuko and Shirasu, Hiroyuki and Shohibussirri, Muh and Sichinava, Dmitry and Silveira, Natalia and Simi, Maria and Simionescu, Radu and Simk{\'o}, Katalin and {\v S}imkov{\'a}, M{\'a}ria and Simov, Kiril and Smith, Aaron and Soares-Bastos, Isabela and Spadine, Carolyn and Stella, Antonio and Straka, Milan and Strnadov{\'a}, Jana and Suhr, Alane and Sulubacak, Umut and Suzuki, Shingo and Sz{\'a}nt{\'o}, Zsolt and Taji, Dima and Takahashi, Yuta and Tamburini, Fabio and Tanaka, Takaaki and Tellier, Isabelle and Thomas, Guillaume and Torga, Liisi and Trosterud, Trond and Trukhina, Anna and Tsarfaty, Reut and Tyers, Francis and Uematsu, Sumire and Ure{\v s}ov{\'a}, Zde{\v n}ka and Uria, Larraitz and Uszkoreit, Hans and Vajjala, Sowmya and van Niekerk, Daniel and van Noord, Gertjan and Varga, Viktor and Villemonte de la Clergerie, Eric and Vincze, Veronika and Wallin, Lars and Walsh, Abigail and Wang, Jing Xian and Washington, Jonathan North and Wendt, Maximilan and Williams, Seyi and Wir{\'e}n, Mats and Wittern, Christian and Woldemariam, Tsegay and Wong, Tak-sum and Wr{\'o}blewska, Alina and Yako, Mary and Yamazaki, Naoki and Yan, Chunxiao and Yasuoka, Koichi and Yavrumyan, Marat M. and Yu, Zhuoran and {\v Z}abokrtsk{\'y}, Zden{\v e}k and Zeldes, Amir and Zeman, Daniel and Zhang, Manying and Zhu, Hanzhi},
 url = {http://hdl.handle.net/11234/1-2988},
 note = {{LINDAT}/{CLARIN} digital library at the Institute of Formal and Applied Linguistics ({{\'U}FAL}), Faculty of Mathematics and Physics, Charles University},
 copyright = {Licence Universal Dependencies v2.4},
 year = {2019} }
 
 @phdthesis{delhoneux19,
  Title                    = {{Linguistically Informed Neural Dependency Parsing for Typologically Diverse Languages}},
  Author                   = {Miryam de Lhoneux},
  School                   = {Uppsala University},
  Year                     = {2019},
}

@inproceedings{de-marneffe-etal-2014-universal,
    title = "Universal {S}tanford dependencies: A cross-linguistic typology",
    author = "de Marneffe, Marie-Catherine  and
      Dozat, Timothy  and
      Silveira, Natalia  and
      Haverinen, Katri  and
      Ginter, Filip  and
      Nivre, Joakim  and
      Manning, Christopher D.",
    booktitle = "Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14)",
    month = may,
    year = "2014",
    address = "Reykjavik, Iceland",
    publisher = "European Language Resources Association (ELRA)",
    url = "http://www.lrec-conf.org/proceedings/lrec2014/pdf/1062_Paper.pdf",
    pages = "4585--4592",
}

@inproceedings{mcdonald-etal-2013-universal,
    title = "Universal Dependency Annotation for Multilingual Parsing",
    author = {McDonald, Ryan  and
      Nivre, Joakim  and
      Quirmbach-Brundage, Yvonne  and
      Goldberg, Yoav  and
      Das, Dipanjan  and
      Ganchev, Kuzman  and
      Hall, Keith  and
      Petrov, Slav  and
      Zhang, Hao  and
      T{\"a}ckstr{\"o}m, Oscar  and
      Bedini, Claudia  and
      Bertomeu Castell{\'o}, N{\'u}ria  and
      Lee, Jungmee},
    booktitle = "Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)",
    month = aug,
    year = "2013",
    address = "Sofia, Bulgaria",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P13-2017",
    pages = "92--97",
}

@inproceedings{zeman-2008-reusable,
    title = "Reusable Tagset Conversion Using Tagset Drivers",
    author = "Zeman, Daniel",
    booktitle = "Proceedings of the Sixth International Conference on Language Resources and Evaluation ({LREC}'08)",
    month = may,
    year = "2008",
    address = "Marrakech, Morocco",
    publisher = "European Language Resources Association (ELRA)",
    url = "http://www.lrec-conf.org/proceedings/lrec2008/pdf/66_paper.pdf",
}

@inproceedings{wang-etal-2019-cross,
    title = "Cross-Lingual {BERT} Transformation for Zero-Shot Dependency Parsing",
    author = "Wang, Yuxuan  and
      Che, Wanxiang  and
      Guo, Jiang  and
      Liu, Yijia  and
      Liu, Ting",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D19-1575",
    doi = "10.18653/v1/D19-1575",
    pages = "5725--5731",
    abstract = "This paper investigates the problem of learning cross-lingual representations in a contextual space. We propose Cross-Lingual BERT Transformation (CLBT), a simple and efficient approach to generate cross-lingual contextualized word embeddings based on publicly available pre-trained BERT models (Devlin et al., 2018). In this approach, a linear transformation is learned from contextual word alignments to align the contextualized embeddings independently trained in different languages. We demonstrate the effectiveness of this approach on zero-shot cross-lingual transfer parsing. Experiments show that our embeddings substantially outperform the previous state-of-the-art that uses static embeddings. We further compare our approach with XLM (Lample and Conneau, 2019), a recently proposed cross-lingual language model trained with massive parallel data, and achieve highly competitive results.",
}

@inproceedings{conneau2017word,
  author    = {Guillaume Lample and
               Alexis Conneau and
               Marc'Aurelio Ranzato and
               Ludovic Denoyer and
               Herv{\'{e}} J{\'{e}}gou},
  title     = {Word translation without parallel data},
  booktitle = {6th International Conference on Learning Representations, {ICLR} 2018,
               Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings},
  publisher = {OpenReview.net},
  year      = {2018},
  url       = {https://openreview.net/forum?id=H196sainb},
  timestamp = {Thu, 25 Jul 2019 14:25:59 +0200},
  biburl    = {https://dblp.org/rec/conf/iclr/LampleCRDJ18.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}



@inproceedings{lample2017unsupervised,
  author    = {Guillaume Lample and
               Alexis Conneau and
               Ludovic Denoyer and
               Marc'Aurelio Ranzato},
  title     = {Unsupervised Machine Translation Using Monolingual Corpora Only},
  booktitle = {6th International Conference on Learning Representations, {ICLR} 2018,
               Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings},
  publisher = {OpenReview.net},
  year      = {2018},
  url       = {https://openreview.net/forum?id=rkYTTf-AZ},
  timestamp = {Thu, 25 Jul 2019 14:25:53 +0200},
  biburl    = {https://dblp.org/rec/conf/iclr/LampleCDR18.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{tran-bisazza-2019-zero,
    title = "Zero-shot Dependency Parsing with Pre-trained Multilingual Sentence Representations",
    author = "Tran, Ke  and
      Bisazza, Arianna",
    booktitle = "Proceedings of the 2nd Workshop on Deep Learning Approaches for Low-Resource NLP (DeepLo 2019)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D19-6132",
    doi = "10.18653/v1/D19-6132",
    pages = "281--288",
    abstract = "We investigate whether off-the-shelf deep bidirectional sentence representations (Devlin et al., 2019) trained on a massively multilingual corpus (multilingual BERT) enable the development of an unsupervised universal dependency parser. This approach only leverages a mix of monolingual corpora in many languages and does not require any translation data making it applicable to low-resource languages. In our experiments we outperform the best CoNLL 2018 language-specific systems in all of the shared task{'}s six truly low-resource languages while using a single system. However, we also find that (i) parsing accuracy still varies dramatically when changing the training languages and (ii) in some target languages zero-shot transfer fails under all tested conditions, raising concerns on the {`}universality{'} of the whole approach.",
}

@article{lample2019cross,
  title={Cross-lingual language model pretraining},
  author={Conneau, Alexis and Lample, Guillaume},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@inproceedings{vaswani2017transformer,
 author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, \Lukasz and Polosukhin, Illia},
 title = {Attention is All You Need},
 booktitle = {Proceedings of the 31st International Conference on Neural Information Processing Systems},
 series = {NIPS'17},
 year = {2017},
 isbn = {978-1-5108-6096-4},
 location = {Long Beach, California, USA},
 pages = {6000--6010},
 numpages = {11},
 url = {http://dl.acm.org/citation.cfm?id=3295222.3295349},
 acmid = {3295349},
 publisher = {Curran Associates Inc.},
 address = {USA},
} 

@inproceedings{barry-etal-2019-cross,
    title = "Cross-lingual Parsing with Polyglot Training and Multi-treebank Learning: A Faroese Case Study",
    author = "Barry, James  and
      Wagner, Joachim  and
      Foster, Jennifer",
    booktitle = "Proceedings of the 2nd Workshop on Deep Learning Approaches for Low-Resource NLP (DeepLo 2019)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D19-6118",
    doi = "10.18653/v1/D19-6118",
    pages = "163--174",
    abstract = "Cross-lingual dependency parsing involves transferring syntactic knowledge from one language to another. It is a crucial component for inducing dependency parsers in low-resource scenarios where no training data for a language exists. Using Faroese as the target language, we compare two approaches using annotation projection: first, projecting from multiple monolingual source models; second, projecting from a single polyglot model which is trained on the combination of all source languages. Furthermore, we reproduce multi-source projection (Tyers et al., 2018), in which dependency trees of multiple sources are combined. Finally, we apply multi-treebank modelling to the projected treebanks, in addition to or alternatively to polyglot modelling on the source side. We find that polyglot training on the source languages produces an overall trend of better results on the target language but the single best result for the target language is obtained by projecting from monolingual source parsing models and then training multi-treebank POS tagging and parsing models on the target side.",
}

@misc{11234/1-3105,
 title = {Universal Dependencies 2.5},
 author = {Zeman, Daniel and Nivre, Joakim and Abrams, Mitchell and Aepli, No{\"e}mi and Agi{\'c}, {\v Z}eljko and Ahrenberg,
 Lars and Aleksandravi{\v c}i{\=u}t{\.e}, Gabriel{\.e} and Antonsen, Lene and Aplonova, Katya and Aranzabe, Maria Jesus and Arutie, Gashaw and Asahara, Masayuki and Ateyah, Luma and Attia, Mohammed and Atutxa, Aitziber and Augustinus, Liesbeth and Badmaeva, Elena and Ballesteros, Miguel and Banerjee, Esha and Bank, Sebastian and Barbu Mititelu, Verginica and Basmov, Victoria and Batchelor, Colin and Bauer, John and Bellato, Sandra and Bengoetxea, Kepa and Berzak, Yevgeni and Bhat, Irshad Ahmad and Bhat, Riyaz Ahmad and Biagetti, Erica and Bick, Eckhard and Bielinskien{\.e}, Agn{\.e} and Blokland, Rogier and Bobicev, Victoria and Boizou, Lo{\"{\i}}c and Borges V{\"o}lker, Emanuel and B{\"o}rstell, Carl and Bosco, Cristina and Bouma, Gosse and Bowman, Sam and Boyd, Adriane and Brokait{\.e}, Kristina and Burchardt, Aljoscha and Candito, Marie and Caron, Bernard and Caron, Gauthier and Cavalcanti, Tatiana and Cebiro{\u g}lu Eryi{\u g}it, G{\"u}l{\c s}en and Cecchini, Flavio Massimiliano and Celano, Giuseppe G. A. and {\v C}{\'e}pl{\"o}, Slavom{\'{\i}}r and Cetin, Savas and Chalub, Fabricio and Choi, Jinho and Cho, Yongseok and Chun, Jayeol and Cignarella, Alessandra T. and Cinkov{\'a}, Silvie and Collomb, Aur{\'e}lie and {\c C}{\"o}ltekin, {\c C}a{\u g}r{\i} and Connor, Miriam and Courtin, Marine and Davidson, Elizabeth and de Marneffe, Marie-Catherine and de Paiva, Valeria and de Souza, Elvis and Diaz de Ilarraza, Arantza and Dickerson, Carly and Dione, Bamba and Dirix, Peter and Dobrovoljc, Kaja and Dozat, Timothy and Droganova, Kira and Dwivedi, Puneet and Eckhoff, Hanne and Eli, Marhaba and Elkahky, Ali and Ephrem, Binyam and Erina, Olga and Erjavec, Toma{\v z} and Etienne, Aline and Evelyn, Wograine and Farkas, Rich{\'a}rd and Fernandez Alcalde, Hector and Foster, Jennifer and Freitas, Cl{\'a}udia and Fujita, Kazunori and Gajdo{\v s}ov{\'a}, Katar{\'{\i}}na and Galbraith, Daniel and Garcia, Marcos and G{\"a}rdenfors, Moa and Garza, Sebastian and Gerdes, Kim and Ginter, Filip and Goenaga, Iakes and Gojenola, Koldo and G{\"o}k{\i}rmak, Memduh and Goldberg, Yoav and G{\'o}mez Guinovart, Xavier and Gonz{\'a}lez Saavedra,
 Berta and Grici{\=u}t{\.e}, Bernadeta and Grioni,
 Matias and Gr{\=
u}z{\={\i}}tis, Normunds and Guillaume, Bruno and Guillot-Barbance, C{\'e}line and Habash, Nizar and Haji{\v c}, Jan and Haji{\v c} jr., Jan and H{\"a}m{\"a}l{\"a}inen, Mika and H{\`a} M{\~y}, Linh and Han, Na-Rae and Harris, Kim and Haug, Dag and Heinecke, Johannes and Hennig, Felix and Hladk{\'a}, Barbora and Hlav{\'a}{\v c}ov{\'a}, Jaroslava and Hociung, Florinel and Hohle, Petter and Hwang, Jena and Ikeda, Takumi and Ion, Radu and Irimia, Elena and Ishola, {\d O}l{\'a}j{\'{\i}}d{\'e} and Jel{\'{\i}}nek, Tom{\'a}{\v s} and Johannsen, Anders and J{\o}rgensen, Fredrik and Juutinen, Markus and Ka{\c s}{\i}kara, H{\"u}ner and Kaasen, Andre and Kabaeva, Nadezhda and Kahane, Sylvain and Kanayama, Hiroshi and Kanerva, Jenna and Katz, Boris and Kayadelen, Tolga and Kenney, Jessica and Kettnerov{\'a}, V{\'a}clava and Kirchner, Jesse and Klementieva, Elena and K{\"o}hn, Arne and Kopacewicz, Kamil and Kotsyba, Natalia and Kovalevskait{\.e}, Jolanta and Krek, Simon and Kwak, Sookyoung and Laippala, Veronika and Lambertino, Lorenzo and Lam, Lucia and Lando, Tatiana and Larasati, Septina Dian and Lavrentiev, Alexei and Lee, John and L{\^e} H{\`{\^o}}ng, PhÆ°Æ¡ng and Lenci, Alessandro and Lertpradit, Saran and Leung, Herman and Li, Cheuk Ying and Li, Josie and Li, Keying and Lim, {KyungTae} and Liovina, Maria and Li, Yuan and Ljube{\v s}i{\'c}, Nikola and Loginova, Olga and Lyashevskaya, Olga and Lynn, Teresa and Macketanz, Vivien and Makazhanov, Aibek and Mandl, Michael and Manning, Christopher and Manurung, Ruli and M{\u a}r{\u a}nduc, C{\u a}t{\u a}lina and Mare{\v c}ek, David and Marheinecke, Katrin and Mart{\'{\i}}nez Alonso, H{\'e}ctor and Martins, Andr{\'e} and Ma{\v s}ek, Jan and Matsumoto, Yuji and {McDonald}, Ryan and {McGuinness}, Sarah and Mendon{\c c}a, Gustavo and Miekka, Niko and Misirpashayeva, Margarita and Missil{\"a}, Anna and Mititelu, C{\u a}t{\u a}lin and Mitrofan, Maria and Miyao, Yusuke and Montemagni, Simonetta and More, Amir and Moreno Romero, Laura and Mori, Keiko Sophie and Morioka, Tomohiko and Mori, Shinsuke and Moro, Shigeki and Mortensen, Bjartur and Moskalevskyi, Bohdan and Muischnek, Kadri and Munro, Robert and Murawaki, Yugo and M{\"u}{\"u}risep, Kaili and Nainwani, Pinkey and Navarro Hor{\~n}iacek, Juan Ignacio and Nedoluzhko,
 Anna and Ne{\v s}pore-B{\=e}rzkalne, Gunta and Nguy{\~{\^e}}n Th{\d i}, LÆ°Æ¡ng and Nguy{\~{\^e}}n Th{\d i} Minh, Huy{\`{\^e}}n and Nikaido, Yoshihiro and Nikolaev, Vitaly and Nitisaroj, Rattima and Nurmi, Hanna and Ojala, Stina and Ojha, Atul Kr. and Ol{\'u}{\`o}kun, Ad{\'e}day{\d o}Ì€ and Omura, Mai and Osenova, Petya and {\"O}stling, Robert and {\O}vrelid, Lilja and Partanen, Niko and Pascual, Elena and Passarotti, Marco and Patejuk, Agnieszka and Paulino-Passos, Guilherme and Peljak-{\L}api{\'n}ska, Angelika and Peng, Siyao and Perez, Cenel-Augusto and Perrier, Guy and Petrova, Daria and Petrov, Slav and Phelan, Jason and Piitulainen, Jussi and Pirinen, Tommi A and Pitler, Emily and Plank, Barbara and Poibeau, Thierry and Ponomareva, Larisa and Popel, Martin and Pretkalni{\c n}a, Lauma and Pr{\'e}vost, Sophie and Prokopidis, Prokopis and Przepi{\'o}rkowski, Adam and Puolakainen, Tiina and Pyysalo, Sampo and Qi, Peng and R{\"a}{\"a}bis, Andriela and Rademaker, Alexandre and Ramasamy, Loganathan and Rama, Taraka and Ramisch, Carlos and Ravishankar, Vinit and Real, Livy and Reddy, Siva and Rehm, Georg and Riabov, Ivan and Rie{\ss}ler, Michael and Rimkut{\.e}, Erika and Rinaldi, Larissa and Rituma, Laura and Rocha, Luisa and Romanenko, Mykhailo and Rosa, Rudolf and Rovati, Davide and RoÈ™ca, Valentin and Rudina, Olga and Rueter, Jack and Sadde, Shoval and Sagot, Beno{\^{\i}}t and Saleh, Shadi and Salomoni, Alessio and Samard{\v z}i{\'c}, Tanja and Samson, Stephanie and Sanguinetti, Manuela and S{\"a}rg,
 Dage and Saul{\={\i}}te, Baiba and Sawanakunanon, Yanin and Schneider, Nathan and Schuster, Sebastian and Seddah, Djam{\'e} and Seeker, Wolfgang and Seraji, Mojgan and Shen, Mo and Shimada, Atsuko and Shirasu, Hiroyuki and Shohibussirri, Muh and Sichinava, Dmitry and Silveira, Aline and Silveira, Natalia and Simi, Maria and Simionescu, Radu and Simk{\'o}, Katalin and {\v S}imkov{\'a}, M{\'a}ria and Simov, Kiril and Smith, Aaron and Soares-Bastos, Isabela and Spadine, Carolyn and Stella, Antonio and Straka, Milan and Strnadov{\'a}, Jana and Suhr, Alane and Sulubacak, Umut and Suzuki, Shingo and Sz{\'a}nt{\'o}, Zsolt and Taji, Dima and Takahashi, Yuta and Tamburini, Fabio and Tanaka, Takaaki and Tellier, Isabelle and Thomas, Guillaume and Torga, Liisi and Trosterud, Trond and Trukhina, Anna and Tsarfaty, Reut and Tyers, Francis and Uematsu, Sumire and Ure{\v s}ov{\'a}, Zde{\v n}ka and Uria, Larraitz and Uszkoreit, Hans and Utka, Andrius and Vajjala, Sowmya and van Niekerk, Daniel and van Noord, Gertjan and Varga, Viktor and Villemonte de la Clergerie, Eric and Vincze, Veronika and Wallin, Lars and Walsh, Abigail and Wang, Jing Xian and Washington, Jonathan North and Wendt, Maximilan and Williams, Seyi and Wir{\'e}n, Mats and Wittern, Christian and Woldemariam, Tsegay and Wong, Tak-sum and Wr{\'o}blewska, Alina and Yako, Mary and Yamazaki, Naoki and Yan, Chunxiao and Yasuoka, Koichi and Yavrumyan, Marat M. and Yu, Zhuoran and {\v Z}abokrtsk{\'y}, Zden{\v e}k and Zeldes, Amir and Zhang, Manying and Zhu, Hanzhi},
 url = {http://hdl.handle.net/11234/1-3105},
 note = {{LINDAT}/{CLARIN} digital library at the Institute of Formal and Applied Linguistics ({{\'U}FAL}), Faculty of Mathematics and Physics, Charles University},
 copyright = {Licence Universal Dependencies v2.5},
 year = {2019} }
 
 @inproceedings{straka-etal-2016-udpipe,
    title = "{UDP}ipe: Trainable Pipeline for Processing {C}o{NLL}-U Files Performing Tokenization, Morphological Analysis, {POS} Tagging and Parsing",
    author = "Straka, Milan  and
      Haji{\v{c}}, Jan  and
      Strakov{\'a}, Jana",
    booktitle = "Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16)",
    month = may,
    year = "2016",
    address = "Portoro{\v{z}}, Slovenia",
    publisher = "European Language Resources Association (ELRA)",
    url = "https://www.aclweb.org/anthology/L16-1680",
    pages = "4290--4297",
    abstract = "Automatic natural language processing of large texts often presents recurring challenges in multiple languages: even for most advanced tasks, the texts are first processed by basic processing steps {--} from tokenization to parsing. We present an extremely simple-to-use tool consisting of one binary and one model (per language), which performs these tasks for multiple languages without the need for any other external data. UDPipe, a pipeline processing CoNLL-U-formatted files, performs tokenization, morphological analysis, part-of-speech tagging, lemmatization and dependency parsing for nearly all treebanks of Universal Dependencies 1.2 (namely, the whole pipeline is currently available for 32 out of 37 treebanks). In addition, the pipeline is easily trainable with training data in CoNLL-U format (and in some cases also with additional raw corpora) and requires minimal linguistic knowledge on the users{'} part. The training code is also released.",
}

@InProceedings{pmlr-v9-glorot10a,
  title = 	 {Understanding the difficulty of training deep feedforward neural networks},
  author = 	 {Xavier Glorot and Yoshua Bengio},
  booktitle = 	 {Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics},
  pages = 	 {249--256},
  year = 	 {2010},
  editor = 	 {Yee Whye Teh and Mike Titterington},
  volume = 	 {9},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Chia Laguna Resort, Sardinia, Italy},
  month = 	 {13--15 May},
  publisher = 	 {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf},
  url = 	 {http://proceedings.mlr.press/v9/glorot10a.html},
  abstract = 	 {Whereas before 2006 it appears that deep multi-layer neural networks were not successfully trained, since then several algorithms have been shown to successfully train them, with experimental results showing the superiority of deeper vs less deep architectures. All these experimental results were obtained with new initialization or training mechanisms. Our objective here is to understand better why standard gradient descent from random initialization is doing so poorly with deep neural networks, to better understand these recent relative successes and help design better algorithms in the future.  We first observe the influence of the non-linear activations functions. We find that the logistic sigmoid activation is unsuited for deep networks with random initialization because of its mean value, which can drive especially the top hidden layer into saturation. Surprisingly, we find that saturated units can move out of saturation by themselves, albeit slowly, and explaining the plateaus sometimes seen when training neural networks. We find that a new non-linearity that saturates less can often be beneficial. Finally, we study how activations and gradients vary across layers and during training, with the idea that training may be more difficult when the singular values of the Jacobian associated with each layer are far from 1.  Based on these considerations, we propose a new initialization scheme that brings substantially faster convergence.}
}

@inproceedings{kuncoro-etal-2017-recurrent,
    title = "What Do Recurrent Neural Network Grammars Learn About Syntax?",
    author = "Kuncoro, Adhiguna  and
      Ballesteros, Miguel  and
      Kong, Lingpeng  and
      Dyer, Chris  and
      Neubig, Graham  and
      Smith, Noah A.",
    booktitle = "Proceedings of the 15th Conference of the {E}uropean Chapter of the Association for Computational Linguistics: Volume 1, Long Papers",
    month = apr,
    year = "2017",
    address = "Valencia, Spain",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/E17-1117",
    pages = "1249--1258",
    abstract = "Recurrent neural network grammars (RNNG) are a recently proposed probablistic generative modeling family for natural language. They show state-of-the-art language modeling and parsing performance. We investigate what information they learn, from a linguistic perspective, through various ablations to the model and the data, and by augmenting the model with an attention mechanism (GA-RNNG) to enable closer inspection. We find that explicit modeling of composition is crucial for achieving the best performance. Through the attention mechanism, we find that headedness plays a central role in phrasal representation (with the model{'}s latent attention largely agreeing with predictions made by hand-crafted head rules, albeit with some important differences). By training grammars without nonterminal labels, we find that phrasal representations depend minimally on nonterminals, providing support for the endocentricity hypothesis.",
}

@inproceedings{andor-etal-2016-globally,
    title = "Globally Normalized Transition-Based Neural Networks",
    author = "Andor, Daniel  and
      Alberti, Chris  and
      Weiss, David  and
      Severyn, Aliaksei  and
      Presta, Alessandro  and
      Ganchev, Kuzman  and
      Petrov, Slav  and
      Collins, Michael",
    booktitle = "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2016",
    address = "Berlin, Germany",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P16-1231",
    doi = "10.18653/v1/P16-1231",
    pages = "2442--2452",
}

@inproceedings{tsarfaty-etal-2010-statistical,
    title = "Statistical Parsing of Morphologically Rich Languages ({SPMRL}) What, How and Whither",
    author = "Tsarfaty, Reut  and
      Seddah, Djam{\'e}  and
      Goldberg, Yoav  and
      Kuebler, Sandra  and
      Versley, Yannick  and
      Candito, Marie  and
      Foster, Jennifer  and
      Rehbein, Ines  and
      Tounsi, Lamia",
    booktitle = "Proceedings of the {NAACL} {HLT} 2010 First Workshop on Statistical Parsing of Morphologically-Rich Languages",
    month = jun,
    year = "2010",
    address = "Los Angeles, CA, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W10-1401",
    pages = "1--12",
}

@inproceedings{kondratyuk-etal-2018-lemmatag,
    title = "{L}emma{T}ag: Jointly Tagging and Lemmatizing for Morphologically Rich Languages with {BRNN}s",
    author = "Kondratyuk, Daniel  and
      Gaven{\v{c}}iak, Tom{\'a}{\v{s}}  and
      Straka, Milan  and
      Haji{\v{c}}, Jan",
    booktitle = "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
    month = oct # "-" # nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D18-1532",
    doi = "10.18653/v1/D18-1532",
    pages = "4921--4928",
    abstract = "We present LemmaTag, a featureless neural network architecture that jointly generates part-of-speech tags and lemmas for sentences by using bidirectional RNNs with character-level and word-level embeddings. We demonstrate that both tasks benefit from sharing the encoding part of the network, predicting tag subcategories, and using the tagger output as an input to the lemmatizer. We evaluate our model across several languages with complex morphology, which surpasses state-of-the-art accuracy in both part-of-speech tagging and lemmatization in Czech, German, and Arabic.",
}

@inproceedings{de-lhoneux-etal-2018-parameter,
    title = "Parameter sharing between dependency parsers for related languages",
    author = "de Lhoneux, Miryam  and
      Bjerva, Johannes  and
      Augenstein, Isabelle  and
      S{\o}gaard, Anders",
    booktitle = "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
    month = oct # "-" # nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D18-1543",
    doi = "10.18653/v1/D18-1543",
    pages = "4992--4997",
    abstract = "Previous work has suggested that parameter sharing between transition-based neural dependency parsers for related languages can lead to better performance, but there is no consensus on what parameters to share. We present an evaluation of 27 different parameter sharing strategies across 10 languages, representing five pairs of related languages, each pair from a different language family. We find that sharing transition classifier parameters always helps, whereas the usefulness of sharing word and/or character LSTM parameters varies. Based on this result, we propose an architecture where the transition classifier is shared, and the sharing of word and character parameters is controlled by a parameter that can be tuned on validation data. This model is linguistically motivated and obtains significant improvements over a monolingually trained baseline. We also find that sharing transition classifier parameters helps when training a parser on unrelated language pairs, but we find that, in the case of unrelated languages, sharing too many parameters does not help.",
}

@inproceedings{sagae-tsujii-2007-dependency,
    title = "Dependency Parsing and Domain Adaptation with {LR} Models and Parser Ensembles",
    author = "Sagae, Kenji  and
      Tsujii, Jun{'}ichi",
    booktitle = "Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning ({EMNLP}-{C}o{NLL})",
    month = jun,
    year = "2007",
    address = "Prague, Czech Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D07-1111",
    pages = "1044--1050",
}

@inproceedings{vania-etal-2019-systematic,
    title = "A systematic comparison of methods for low-resource dependency parsing on genuinely low-resource languages",
    author = "Vania, Clara  and
      Kementchedjhieva, Yova  and
      S{\o}gaard, Anders  and
      Lopez, Adam",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D19-1102",
    doi = "10.18653/v1/D19-1102",
    pages = "1105--1116",
    abstract = "Parsers are available for only a handful of the world{'}s languages, since they require lots of training data. How far can we get with just a small amount of training data? We systematically compare a set of simple strategies for improving low-resource parsers: data augmentation, which has not been tested before; cross-lingual training; and transliteration. Experimenting on three typologically diverse low-resource languages{---}North S{\'a}mi, Galician, and Kazah{---}We find that (1) when only the low-resource treebank is available, data augmentation is very helpful; (2) when a related high-resource treebank is available, cross-lingual training is helpful and complements data augmentation; and (3) when the high-resource treebank uses a different writing system, transliteration into a shared orthographic spaces is also very helpful.",
}

@inproceedings{kulmizev-etal-2019-deep,
    title = "Deep Contextualized Word Embeddings in Transition-Based and Graph-Based Dependency Parsing - A Tale of Two Parsers Revisited",
    author = "Kulmizev, Artur  and
      de Lhoneux, Miryam  and
      Gontrum, Johannes  and
      Fano, Elena  and
      Nivre, Joakim",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D19-1277",
    doi = "10.18653/v1/D19-1277",
    pages = "2755--2768",
    abstract = "Transition-based and graph-based dependency parsers have previously been shown to have complementary strengths and weaknesses: transition-based parsers exploit rich structural features but suffer from error propagation, while graph-based parsers benefit from global optimization but have restricted feature scope. In this paper, we show that, even though some details of the picture have changed after the switch to neural networks and continuous representations, the basic trade-off between rich features and global optimization remains essentially the same. Moreover, we show that deep contextualized word embeddings, which allow parsers to pack information about global sentence structure into local feature representations, benefit transition-based parsers more than graph-based parsers, making the two approaches virtually equivalent in terms of both accuracy and error profile. We argue that the reason is that these representations help prevent search errors and thereby allow transition-based parsers to better exploit their inherent strength of making accurate local decisions. We support this explanation by an error analysis of parsing experiments on 13 languages.",
}

@inproceedings{mulcaire-etal-2019-polyglot,
    title = "Polyglot Contextual Representations Improve Crosslingual Transfer",
    author = "Mulcaire, Phoebe  and
      Kasai, Jungo  and
      Smith, Noah A.",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/N19-1392",
    doi = "10.18653/v1/N19-1392",
    pages = "3912--3918",
    abstract = "We introduce Rosita, a method to produce multilingual contextual word representations by training a single language model on text from multiple languages. Our method combines the advantages of contextual word representations with those of multilingual representation learning. We produce language models from dissimilar language pairs (English/Arabic and English/Chinese) and use them in dependency parsing, semantic role labeling, and named entity recognition, with comparisons to monolingual and non-contextual variants. Our results provide further evidence for the benefits of polyglot learning, in which representations are shared across multiple languages.",
}

@inproceedings{hall-etal-2007-single,
    title = "Single Malt or Blended? A Study in Multilingual Parser Optimization",
    author = {Hall, Johan  and
      Nilsson, Jens  and
      Nivre, Joakim  and
      Eryi{\v{g}}it, G{\"u}l{\c{s}}en  and
      Megyesi, Be{\'a}ta  and
      Nilsson, Mattias  and
      Saers, Markus},
    booktitle = "Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning ({EMNLP}-{C}o{NLL})",
    month = jun,
    year = "2007",
    address = "Prague, Czech Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D07-1097",
    pages = "933--939",
}

@article{Nivre:Etal07,
  added-at = {2010-10-26T21:28:56.000+0200},
  author = {Nivre, Joakim and Hall, Johan and Nilsson, Jens and Chanev, Atanas and Eryigit, GÃ¼lsen and K\"ubler, Sandra and Marinov, Svetoslav and Marsi, Erwin},
  biburl = {https://www.bibsonomy.org/bibtex/2219fdcfa3a894d52f158905978aafe09/arnsholt},
  description = {dblp},
  ee = {http://dx.doi.org/10.1017/S1351324906004505},
  interhash = {3e59d56dc8973e4b77aa6854a267ef04},
  intrahash = {219fdcfa3a894d52f158905978aafe09},
  journal = {Natural Language Engineering},
  keywords = {inf5830 nlp},
  number = 2,
  pages = {95--135},
  timestamp = {2011-03-10T16:22:00.000+0100},
  title = {MaltParser: A language-independent system for data-driven dependency parsing.},
  url = {http://dblp.uni-trier.de/db/journals/nle/nle13.html#NivreHNCEKMM07},
  volume = 13,
  year = 2007
}



@article{mcdonald-nivre-2011-analyzing,
    title = "Analyzing and Integrating Dependency Parsers",
    author = "McDonald, Ryan  and
      Nivre, Joakim",
    journal = "Computational Linguistics",
    volume = "37",
    number = "1",
    year = "2011",
    url = "https://www.aclweb.org/anthology/J11-1007",
    doi = "10.1162/coli_a_00039",
    pages = "197--230",
}

@article{hochreiter1997long,
  title={Long short-term memory},
  author={Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
  journal={Neural computation},
  volume={9},
  number={8},
  pages={1735--1780},
  year={1997},
  publisher={MIT Press}
}

@article{le2012dcu,
  title={DCU-Paris13 systems for the SANCL 2012 shared task},
  author={Le Roux, Joseph and Foster, Jennifer and Wagner, Joachim and Kaljahi, Rasul and Bryl, Anton},
  journal={In Notes of the First Workshop on Syntactic Analysis ofNon-Canonical Language (SANCL)},
  year={2012}
}


@inproceedings{conneau2019unsupervised,
    title = "Unsupervised Cross-lingual Representation Learning at Scale",
    author = "Conneau, Alexis  and
      Khandelwal, Kartikay  and
      Goyal, Naman  and
      Chaudhary, Vishrav  and
      Wenzek, Guillaume  and
      Guzm{\'a}n, Francisco  and
      Grave, Edouard  and
      Ott, Myle  and
      Zettlemoyer, Luke  and
      Stoyanov, Veselin",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.747",
    doi = "10.18653/v1/2020.acl-main.747",
    pages = "8440--8451",
    abstract = "This paper shows that pretraining multilingual language models at scale leads to significant performance gains for a wide range of cross-lingual transfer tasks. We train a Transformer-based masked language model on one hundred languages, using more than two terabytes of filtered CommonCrawl data. Our model, dubbed XLM-R, significantly outperforms multilingual BERT (mBERT) on a variety of cross-lingual benchmarks, including +14.6{\%} average accuracy on XNLI, +13{\%} average F1 score on MLQA, and +2.4{\%} F1 score on NER. XLM-R performs particularly well on low-resource languages, improving 15.7{\%} in XNLI accuracy for Swahili and 11.4{\%} for Urdu over previous XLM models. We also present a detailed empirical analysis of the key factors that are required to achieve these gains, including the trade-offs between (1) positive transfer and capacity dilution and (2) the performance of high and low resource languages at scale. Finally, we show, for the first time, the possibility of multilingual modeling without sacrificing per-language performance; XLM-R is very competitive with strong monolingual models on the GLUE and XNLI benchmarks. We will make our code and models publicly available.",
}


@inproceedings{ahmad-etal-2019-difficulties,
    title = "On Difficulties of Cross-Lingual Transfer with Order Differences: A Case Study on Dependency Parsing",
    author = "Ahmad, Wasi  and
      Zhang, Zhisong  and
      Ma, Xuezhe  and
      Hovy, Eduard  and
      Chang, Kai-Wei  and
      Peng, Nanyun",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/N19-1253",
    doi = "10.18653/v1/N19-1253",
    pages = "2440--2452",
    abstract = "Different languages might have different word orders. In this paper, we investigate crosslingual transfer and posit that an orderagnostic model will perform better when transferring to distant foreign languages. To test our hypothesis, we train dependency parsers on an English corpus and evaluate their transfer performance on 30 other languages. Specifically, we compare encoders and decoders based on Recurrent Neural Networks (RNNs) and modified self-attentive architectures. The former relies on sequential information while the latter is more flexible at modeling word order. Rigorous experiments and detailed analysis shows that RNN-based architectures transfer well to languages that are close to English, while self-attentive models have better overall cross-lingual transferability and perform especially well on distant languages.",
}

@article{bojanowski-etal-2017-enriching,
    title = "Enriching Word Vectors with Subword Information",
    author = "Bojanowski, Piotr  and
      Grave, Edouard  and
      Joulin, Armand  and
      Mikolov, Tomas",
    journal = "Transactions of the Association for Computational Linguistics",
    volume = "5",
    year = "2017",
    url = "https://www.aclweb.org/anthology/Q17-1010",
    doi = "10.1162/tacl_a_00051",
    pages = "135--146",
    abstract = "Continuous word representations, trained on large unlabeled corpora are useful for many natural language processing tasks. Popular models that learn such representations ignore the morphology of words, by assigning a distinct vector to each word. This is a limitation, especially for languages with large vocabularies and many rare words. In this paper, we propose a new approach based on the skipgram model, where each word is represented as a bag of character n-grams. A vector representation is associated to each character n-gram; words being represented as the sum of these representations. Our method is fast, allowing to train models on large corpora quickly and allows us to compute word representations for words that did not appear in the training data. We evaluate our word representations on nine different languages, both on word similarity and analogy tasks. By comparing to recently proposed morphological word representations, we show that our vectors achieve state-of-the-art performance on these tasks.",
}

@inproceedings{mulcaire-etal-2019-low,
    title = "Low-Resource Parsing with Crosslingual Contextualized Representations",
    author = "Mulcaire, Phoebe  and
      Kasai, Jungo  and
      Smith, Noah A.",
    booktitle = "Proceedings of the 23rd Conference on Computational Natural Language Learning (CoNLL)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/K19-1029",
    doi = "10.18653/v1/K19-1029",
    pages = "304--315",
    abstract = "Despite advances in dependency parsing, languages with small treebanks still present challenges. We assess recent approaches to multilingual contextual word representations (CWRs), and compare them for crosslingual transfer from a language with a large treebank to a language with a small or nonexistent treebank, by sharing parameters between languages in the parser itself. We experiment with a diverse selection of languages in both simulated and truly low-resource scenarios, and show that multilingual CWRs greatly facilitate low-resource dependency parsing even without crosslingual supervision such as dictionaries or parallel text. Furthermore, we examine the non-contextual part of the learned language models (which we call a {``}decontextual probe{''}) to demonstrate that polyglot language models better encode crosslingual lexical correspondence compared to aligned monolingual language models. This analysis provides further evidence that polyglot training is an effective approach to crosslingual transfer.",
}


@ARTICLE{Rossetto-VBS19,
  author={L. {Rossetto} and R. {Gasser} and J. {Lokoc} and W. {Bailer} and K. {Schoeffmann} and B. {Muenzer} and T. {Soucek} and P. A. {Nguyen} and P. {Bolettieri} and A. {Leibetseder} and S. {Vrochidis}},
  journal={IEEE Transactions on Multimedia}, 
  title={Interactive Video Retrieval in the Age of Deep Learning - Detailed Evaluation of VBS 2019}, 
  year={2020},
  volume={},
  number={},
  pages={1-1}}
  
@inproceedings{tran2021vr,
  title={A VR interface for browsing visual spaces at VBS2021},
  author={Tran, Ly-Duyen and Nguyen, Manh-Duy and Nguyen, Thao-Nhu and Healy, Graham and Caputo, Annalina and Nguyen, Binh T and Gurrin, Cathal},
  booktitle={International Conference on Multimedia Modeling},
  pages={490--495},
  year={2021},
  organization={Springer}
}

@inproceedings{hezel2022efficient,
  title={Efficient Search and Browsing of Large-Scale Video Collections with Vibro},
  author={Hezel, Nico and Schall, Konstantin and Jung, Klaus and Barthel, Kai Uwe},
  booktitle={International Conference on Multimedia Modeling},
  pages={487--492},
  year={2022},
  organization={Springer}
}

@inproceedings{le2022avseeker,
  title={AVSeeker: an active video retrieval engine at VBS2022},
  author={Le, Tu-Khiem and Ninh, Van-Tu and Tran, Mai-Khiem and Healy, Graham and Gurrin, Cathal and Tran, Minh-Triet},
  booktitle={International Conference on Multimedia Modeling},
  pages={537--542},
  year={2022},
  organization={Springer}
}

@inproceedings{lokovc2022video,
  title={Video Search with Context-Aware Ranker and Relevance Feedback},
  author={Loko{\v{c}}, Jakub and Mejzl{\'\i}k, Franti{\v{s}}ek and Sou{\v{c}}ek, Tom{\'a}{\v{s}} and Dokoupil, Patrik and Pe{\v{s}}ka, Ladislav},
  booktitle={International Conference on Multimedia Modeling},
  pages={505--510},
  year={2022},
  organization={Springer}
}

@inproceedings{venugopalan2015sequence,
  title={Sequence to sequence-video to text},
  author={Venugopalan, Subhashini and Rohrbach, Marcus and Donahue, Jeffrey and Mooney, Raymond and Darrell, Trevor and Saenko, Kate},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={4534--4542},
  year={2015}
}

@article{yang2018text2video,
  title={Text2video: An end-to-end learning framework for expressing text with videos},
  author={Yang, Xiaoshan and Zhang, Tianzhu and Xu, Changsheng},
  journal={IEEE Transactions on Multimedia},
  volume={20},
  number={9},
  pages={2360--2370},
  year={2018},
  publisher={IEEE}
}

@inproceedings{song2021towards,
  title={Towards diverse paragraph captioning for untrimmed videos},
  author={Song, Yuqing and Chen, Shizhe and Jin, Qin},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={11245--11254},
  year={2021}
}
