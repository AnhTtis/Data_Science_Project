\section{Experiments}

\subsection{Dataset}
We conduct our experiments on the popular video-dialogue dataset: AVSD~\cite{alamri2019audiovisual}.\footnote{https://video-dialog.com} In AVSD, each video is associated with a 10-round dialogue discussing the content of the corresponding video. We follow the dataset split of AVSD in~\cite{alamri2019audiovisual,maeoki2020interactive}, 7,985 videos for training, 863 videos for validation and 1,000 videos for testing.
\subsection{Training setup}
Our implementation is based on CLIP~\cite{radford2021learning_clip} from Huggingface~\cite{Wolf2019HuggingFacesTS}. CLIP is used to initialize our \textsc{Image-Encoder} and \textsc{Text-Encoder}. For performance and efficiency consideration, we employ ViT-B/16~\cite{radford2021learning_clip} as our image encoder.\footnote{https://openai.com/blog/clip/}. We train our system with a learning rate of $1\times10^{-5}$ for 10 epochs, with a batch size of 16. We use a maximum gradient norm of 1. The optimizer we used is AdamW~\cite{adamw}, for which the $\epsilon$ is set to $1\times10^{-8}$. We perform early stopping when the performance on validation set degrades. We employ R@K, Median Rank and Mean Rank as evaluation metrics ~\cite{alamri2019audiovisual}. Our code is made publicly available.~\footnote{https://github.com/lyuchenyang/Dialogue-to-Video-Retrieval}

\subsection{Results}

\begin{table*}[!htb]
  \centering
  \small
    \caption{Experimental Results on AVSD dataset}
  \input{Table-0-main-results}
  \label{tbl-0-main-results}
\end{table*}

We present our experimental results on the test set of AVSD~\cite{alamri2019audiovisual} in Table~\ref{tbl-0-main-results}, where we also show the results of recent baseline models including: 1) LSTM~\cite{maeoki2020interactive}, an LSTM-based interactive video retrieval model; 2) \textsc{FiT}~\cite{bain2021frozen}, a Transformer-based text-to-video retrieval model using the video summary as the search query; 3)  \textsc{FiT}~\cite{bain2021frozen} + Dialogue, the \textsc{FiT} model with dialogue in AVSD~\cite{alamri2019audiovisual} as the search query~\footnote{We concatenate all the rounds of dialogue as plain text to serve as the search query.}; 4) \textsc{ViReD}~\cite{madasu2022learning}, a video retrieval system based on \textsc{FiT} and CLIP~\cite{radford2021learning_clip} using the dialogue summary as the initial query and model-generated dialogue as an additional query. 
In Table~\ref{tbl-0-main-results}, our model is named \textsc{D2V}~(\textbf{D}ialogue-\textbf{t}o-\textbf{V}ideo). We also include the results of our system using the the video caption~(script in AVSD dataset) -- \textsc{D2V+Script} -- and the dialogue summary~(summary in AVSD dataset) as the search query -- \textsc{D2V+Summary}. 

The results in Table~\ref{tbl-0-main-results} show that our proposed approach, \textsc{D2V}, achieves superior performance compared to previous models. First, \textsc{D2V+Script} with plain-text video caption input outperforms its counterpart \textsc{FiT} by a large margin~(15.8 R@1 improvement) and even obtains significant improvements (by 10.6 R@1) over \textsc{FiT} using dialogue as input. That shows the effectiveness of our proposed model architecture. Second, \textsc{D2V+Dialogue} significantly outperforms \textsc{D2V+Script} and \textsc{D2V+Summary} by 3.2 R@1 and 2.2 R@1 respectively, which demonstrates the benefit of incorporating dialogue as a search query. The results in Table~\ref{tbl-0-main-results} show that the dialogue does indeed contain important information about the video content and demonstrates the plausibility of using dialogue as a search query.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.9\linewidth]{effect_dialogue_rounds_12x4.pdf}
    \caption{Effect of dialogue rounds}
    \label{fig:effect_of_dialogue_rounds}
\end{figure}

\paragraph{\textbf{Effect of Dialogue Rounds}} 
%Furthermore, 
We investigate the effect of dialogue rounds on the retrieval performance. The results on the validation set of AVSD are shown in Figure~\ref{fig:effect_of_dialogue_rounds}, where we use a varying number of dialogue rounds~(from 1 to 10) when  retrieving videos. 
%Based on the results in Figure~\ref{fig:effect_of_dialogue_rounds},
We observe a consistent improvement with an increasing number of dialogue rounds. The results show that with more rounds of dialogue, we can obtain better retrieval performance. The improvement brought by increasing the dialogue rounds is more significant especially in the early stage~(when using 1 round of dialogue versus 3 rounds).

