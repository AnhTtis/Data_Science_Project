\section{Introduction}

The aim of a video retrieval system is to find the best matching videos according to queries provided by the users~\cite{mithun2018learning,miech2018learning,liu2019use,dzabraev2021mdmmt,cheng2021improving}. Video retrieval has significant practical value as the vast volume of videos on the web has triggered the need for efficient and effective video search systems. 

In this paper, we focus on improving the performance of video retrieval systems by combining both textual descriptions of the target video with interactive dialogues between users discussing the content of the target video. 

Previous work on video retrieval applied a CNN-based architecture~\cite{lecun1998gradient-lenet,krizhevsky2017imagenet-alexnet,he2016deep-resnet} combined with an RNN network~\cite{bahdanau2015neural-attn} to handle visual features and their time-series information \cite{venugopalan2015sequence,yang2018text2video,anne2017localizing}. Meanwhile, another RNN model was employed to embed a textual description into the same vector space as the video, so that their similarity could be computed in order 
%they could be measured its similarity score 
to perform retrieval \cite{mithun2018learning,yang2018text2video,anne2017localizing}. Due to the huge impact of the transformer architecture \cite{vaswani2017transformer} in both text and image modalities, this network has also been widely applied in the video retrieval research field, obtaining improvements over previous approaches \cite{gabeur2020multi,bain2021frozen,luo2021clip4clip,le2022avseeker,hezel2022efficient}. 

Current video retrieval research, however, mainly focuses on plain text queries such as video captions or descriptions. The need to search videos using queries with complex structures becomes more important when the initial simple text query is ambiguous or not sufficiently well described to find the correct relevant video. Nevertheless, there are only a few studies that focus on this problem~\cite{maeoki2020interactive,madasu2022learning}. Madusa et al.~\cite{madasu2022learning} 
%had a similar approach that 
used a dialogue, a sequence of questions and answers about a video, as a query to perform the retrieval because this sequential structure contains rich and detailed information. Specifically, starting with a simple initial description, a video retrieval model would return a list of matching videos from which a question and its answer were generated to create an extended dialogue. This iterative process continued until the correct video was found. Unlike the model of Maeoki et al.\cite{maeoki2020interactive} which applied a CNN-based encoder and an LSTM~\cite{hochreiter1997long_lstm} to embed data from each modality and to generate questions and answers, Madusa et al's system, \textsc{ViReD}~\cite{madasu2022learning}, applied Video2Sum~\cite{song2021towards} to convert a video into a textual summary which can be used with the initial query to get the generated dialogue with the help of a BART model~\cite{lewis-etal-2020-bart}.

In this paper, we focus on a less-studied aspect of video retrieval: dialogue-to-video retrieval where the search query is a user-generated dialogue that contains structured information from each turn of the dialogue. The need for dialogue-to-video retrieval derives from the increasing amount of online conversations on social media, which inspires the development of effective dialogue-to-video retrieval systems for many purposes, especially recommendation systems~\cite{alamri2019audiovisual,he2021improving,zheng2022MMChat}. Different from general text-to-video retrieval, dialogue-to-video uses user-generated dialogues as the search query to retrieve videos. The dialogue contains user discussion about a certain video, which provides dramatically different information than a plain-text query. This is because during the interaction between users in the dialogue, a discussion similar to the following could happen ``A: \textit{The main character of that movie was involved in a horrible car accident when he was 13.} B: \textit{No, I think you mean another character.}''. Such discussion contains subtle information about the video of interest and thus cannot be treated as a plain-text query.

Therefore, to incorporate the conversational information from dialogues, we propose a novel dialogue-to-video retrieval approach. In our proposed model, we sequentially encode each turn of the dialogue to obtain a dialogue-aware query representation with the purpose of retaining the dialogue information. 
Then we calculate the similarity between this dialogue-aware query representation and individual frames in the video in order to obtain a weighted video representation. 
Finally, we use the video representation to compute an overall similarity score with the dialogue-aware query. To validate the effectiveness of our approach, we conduct dialogue-to-video experiments on a benchmark dataset AVSD~\cite{alamri2019audiovisual}. Experimental results show that our approach achieves significant improvements over previous state-of-the-art models including \textsc{FiT} and \textsc{ViReD}~\cite{maeoki2020interactive,bain2021frozen,madasu2022learning}.

