%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% LaTeX Template for AAMAS-2023 (based on sample-sigconf.tex)
%%% Prepared by the AAMAS-2023 Program Chairs based on the version from AAMAS-2022. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% Start your document with the \documentclass command.
%%% Use the first variant below for the final paper.
%%% Use the second variant below for submission.
\documentclass[sigconf]{aamas} 
\settopmatter{printacmref=false} % Removes citation information below abstract
\renewcommand\footnotetextcopyrightpermission[1]{} % removes footnote with conference information in first column
\pagestyle{plain} % removes running headers
%%% Load required packages here (note that many are included already).


\usepackage{balance} % for balancing columns on the final page

\usepackage{amsmath}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{graphicx}
%\usepackage{amssymb}
\usepackage{wrapfig} 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% AAMAS-2023 copyright block (do not change!)
%\setcopyright{none} 
\setcopyright{none} 
%\setcopyright{ifaamas}
%\acmConference[AAMAS '23]{Proc.\@ of the 22nd International Conference
%on Autonomous Agents and Multiagent Systems (AAMAS 2023)}{May 29 -- June 2, 2023}
%{London, United Kingdom}{A.~Ricci, W.~Yeoh, N.~Agmon, B.~An (eds.)}
%\copyrightyear{2023}
%\acmYear{2023}
%\acmDOI{}
%\acmPrice{}
%\acmISBN{}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% Use this command to specify your EasyChair submission number.
%%% In anonymous mode, it will be printed on the first page.

%\acmSubmissionID{45}

%%% Use this command to specify the title of your paper.

%\title{\texttt{TA-Explore:} Teacher-Assisted Exploration for Facilitating Fast Reinforcement Learning}
\title{Human-Inspired Framework to Accelerate Reinforcement Learning}

%%% Provide names, affiliations, and email addresses for all authors.

\author{Ali Beikmohammadi}
\affiliation{
  \institution{Department of Computer and Systems Sciences \\ Stockholm University}
  \city{Stockholm}
  \country{Sweden}}
\email{beikmohammadi@dsv.su.se}

\author{Sindri Magn√∫sson}
\affiliation{
  \institution{Department of Computer and Systems Sciences \\ Stockholm University}
  \city{Stockholm}
  \country{Sweden}}
\email{sindri.magnusson@dsv.su.se}

%%% Use this environment to specify a short abstract for your paper.

\begin{abstract}
While deep reinforcement learning (RL) is becoming an integral part of good decision-making in data science, it is still plagued with sample inefficiency. This can be challenging when applying deep-RL in real-world environments where physical interactions are expensive and can risk system safety. To improve the sample efficiency of RL algorithms, this paper proposes a novel human-inspired framework that facilitates fast exploration and learning for difficult RL tasks. The main idea is to first provide the learning agent with simpler but similar tasks that gradually grow in difficulty and progress toward the main task. The proposed method requires no pre-training phase. Specifically, the learning of simpler tasks is only done for one iteration. The generated knowledge could be used by any transfer learning, including value transfer and policy transfer, to reduce the sample complexity while not adding to the computational complexity. So, it can be applied to any goal, environment, and reinforcement learning algorithm - both value-based methods and policy-based methods and both tabular methods and deep-RL methods.   We have evaluated our proposed framework on both a simple Random Walk for illustration purposes and on more challenging optimal control problems with constraint. The experiments show the good performance of our proposed framework in improving the sample efficiency of RL-learning algorithms, especially when the main task is difficult.
\end{abstract}

%%% The code below was generated by the tool at http://dl.acm.org/ccs.cfm.
%%% Please replace this example with code appropriate for your own paper.


%%% Use this command to specify a few keywords describing your work.
%%% Keywords should be separated by commas.

\keywords{Reinforcement Learning; Deep RL; TD Learning; Policy Optimization; PPO; Sample Efficiency; Exploration}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% Include any author-defined commands here.
         
\newcommand{\BibTeX}{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em\TeX}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}

%%% The following commands remove the headers in your paper. For final 
%%% papers, these will be inserted during the pagination process.

\pagestyle{fancy}
\fancyhead{}

%%% The next command prints the information defined in the preamble.

\maketitle 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Introduction}
Intelligent and data-driven decision-making is becoming an increasingly important part of data science. The subfield of AI concerned with how intelligent agents learn to make optimal decisions in an unknown environment from data and experience is RL. The area has flourished recently by building on advancements in deep learning that have given RL agents superb generalization abilities~\cite{mnih2015}. Deep-RL has had huge impact in multiple areas where it has solved challenging problems, e.g., in game playing \cite{silver2016, silver2017}, financial markets \cite{meng2019, fischer2018, 10.1145/3383455.3422540}, robotic control \cite{ kober2013, lillicrap2015}, optimal control \cite{126844, PERRUSQUIA2021145}, healthcare\cite{CORONATO2020101964, 10.1145/3477600}, autonomous driving \cite{shalev2016, ref4}, and recommendation systems~\cite{rec1,rec2, ref3}. 


%In recent years, it is widely acknowledged that RL is the most promising method for solving sequential decision-making problems, especially since deep neural networks for function approximation have been developed \cite{mnih2015}. There has been a lot of success using RL to solve a variety of challenging problems, including playing Go \cite{silver2016, silver2017}, financial markets \cite{meng2019, fischer2018}, robotic control \cite{ kober2013, lillicrap2015}, optimal control \cite{126844, PERRUSQUIA2021145}, healthcare\cite{CORONATO2020101964, 10.1145/3477600}, and autonomous driving \cite{shalev2016}, but far less attention has been paid to its weaknesses and improving its shortcomings appears to be a priority.


 A central challenge hindering RL from achieving its full potential in many application areas is that RL approaches  generally need a lot of samples to learn good decision policies~\cite{10.1007/BF00992698, sutton1999, yu2018towards, sung2017learning}. %In   sample. In fact, in many application it is costly to interact with the environment and we want
 %That is, the agents usually need a lot of samples to learn good decision policies. 
 In practice, this means that the agent needs many interactions with the environment to learn, which is often expensive.  This is especially challenging when the agents do not have access to simulation environment but interact directly with a real-world environment.  For example, in cyber-physical systems, medical applications, or in robotic systems, it is not only costly to spend much time exploring the environment, it can be downright dangerous to learn slowly~\cite{Cheng_Orosz_Murray_Burdick_2019,NEURIPS2020_8df6a659, NEURIPS2020_448d5eda}. Finding approaches to facilitate efficient exploration and learning is a central challenge in RL. 
 
 %sub-optimal actions and in-efficient exploration is not only costly but it 
% In such situations, interactions but unconstrained exploration might
 

 
% it is costly to... Or in recemder systems it is costly to ... Ideally, we would like to learn good policies with from as few samples or interactions with the environment.  \textcolor{green}{Still working on this. Will add a bit more discussion in this paragraph}
%\textcolor{red}{One of the main problems the researchers face in RL, regardless of whether they use value-based methods \cite{tesauro1995, 10.1007/BF00992698} or policy-based methods \cite{ sutton1999}, is the long time required to learn the task from samples. In other words, the agent needs a huge number of samples to learn what actions are known as optimal actions in each state, and that is why it is a sample-inefficient method \cite{BOTVINICK2019408}. Typically, the more difficult and complex the task assigned to the agent, the longer it takes for it to learn the optimal policy.}


Most existing RL algorithms start the learning process from scratch. In particular, the agent starts exploring each environment without using any prior knowledge (there are notable exceptions, see Related Work). This is fundamentally different to how humans explore and learn in new environments~\cite{GOTTLIEB2013585}. When faced with  difficult and nested tasks, humans try to learn them step by step by considering several short-term and auxiliary goals that are easy to achieve but are at the same time aligned with the main goal. To better understand this process, consider a math teacher and student. To facilitate learning, the teacher must aid the student by providing her with tasks of gradually increasing difficulty. For example, the teacher will first teach the student addition, then multiplication before finally teaching her exponents. It would be extremely  difficult for the student to directly learn exponents without the side-steps of learning addition and multiplication first.
Similarly, when an RL agent learns a difficult task it can be challenging and sample inefficient to directly take on the main goal. In the spirit of human learning, it might often be more likely to success to first solve simpler similar tasks that  gradually increase in difficulty and progress towards the main tasks. 

%if the agent gets no direction and prior knowledge to guide the exploration.

%without giving any prior information is exactly like leaving a student with the problem of the powers and roots of numbers without knowing the basic operations of mathematics.

%
%tasks and reward that help

%You can never imagine a teacher teaching the problems of the roots and powers of numbers while the student does not yet know the four basic mathematical operations. What actually happens is that everything starts with teaching one-digit numbers addition and then subtraction. The teacher then acquaints the student with multiplication and division based on addition and subtraction operations so that it is finally time to teach powers and roots of numbers. Learning a difficult task by the agent in a RL-based approach without giving any prior information is exactly like leaving a student with the problem of the powers and roots of numbers without knowing the basic operations of mathematics.

%The most common way to find solutions to problems can come from looking at human behavior when facing the same issue. This inspiration has also widely been embraced by many researchers in the field of machine learning, specifically RL \cite{GOTTLIEB2013585}.

%When faced with a difficult and nested task, human always tries to learn it step by step by considering several short-term and auxiliary goals that are easy to achieve but at the same time in line with the main goal. To better understand this process, consider a math teacher and student. You can never imagine a teacher teaching the problems of the roots and powers of numbers while the student does not yet know the four basic mathematical operations. What actually happens is that everything starts with teaching one-digit numbers addition and then subtraction. The teacher then acquaints the student with multiplication and division based on addition and subtraction operations so that it is finally time to teach powers and roots of numbers. Learning a difficult task by the agent in a RL-based approach without giving any prior information is exactly like leaving a student with the problem of the powers and roots of numbers without knowing the basic operations of mathematics.

\textbf{Contribution:}
 The main goal of this paper is to provide a framework to allow such Teacher-Assisted exploration in reinforcement learning. In particular, we provide a systematic approach for facilitating the learning of an RL agent by providing it with simpler auxiliary goals that gradually become more difficult and converge to the main goal. Thus allowing the agent to learn its main goal faster. Our framework is algorithm-agnostic; it can be used together with any algorithm, both value-based and policy-based methods, and both tabular methods and deep-RL methods since it supports both value transfer and policy transfer. The auxiliary goals are applied by defining one assistant reward alongside the target reward together with an annealing function to make associated Markov decision processes (MDPs) in sequence. In this process, the agent slowly shifts between learning these goals by learning each for just one iteration and uses the knowledge gained by sequences of auxiliary goals when learning the main task. So, in this approach, there is no need to define many MDPs separately that follow the same distribution. We illustrate the promise of the approach in simple intuitive settings using tabular algorithms.
We also illustrate the efficiency of our proposed framework by extensive experiments using deep-RL on two real-world problems (see Section \ref{section5}).
The experiments indicate that our proposed method is effective in speeding up the learning, especially when the main task is difficult, without increasing computational complexity. 
%Our code is available as supplementary material, and we will make it publicly available if the paper is accepted.
The code for the experiments is publicly available in the following link: \url{https://github.com/AliBeikmohammadi/TA-Explore}.
%The code for the experiments is publicly available in the following anonymous link: \url{https://anonymous.4open.science/r/D2A3/}.
%
%In this paper, we propose a human-inspired method. To be more specific, we help the agent by considering an auxiliary goal so that it can learn the main goal faster. The proposed method can be used as a new framework in training an agent for any goal and in any environment regardless of the type of RL algorithm used - both value-based methods and policy-based methods. The auxiliary goal is applied by defining an assistant reward alongside the target reward in Markov decision process (MDP). In this process, the agent slowly shifts between learning these two goals and uses the auxiliary goal as prior knowledge when learning the main task. Experiments indicate that our proposed method is effective at speeding up the learning to agent, especially when the main task is difficult and complicated.
%
\iffalse
Our contributions are summarized as flows:
\begin{itemize}
\item We examine the sample-inefficient problem of reinforcement learning and define the auxiliary goal human-inspired.
\item We introduce a new framework called \texttt{TA-Explore} through the definition of assistant reward based on the auxiliary goal.
\item We clarify the introduced framework with a simple example (see Section \ref{section4}).
\item We illustrate the efficiency of our proposed framework by extensive experiments on an optimal control problem with constraint (see Section \ref{section5}).
\end{itemize}
\fi

\textbf{Limitation:}
We assume that there exists a possibility of defining some auxiliary goals and, consequently, some assistant rewards for the considered tasks. Such goals and rewards might not exist for every task. However, surprisingly often, it is possible to build simple, intuitive assistant rewards that can be used to guide the learning process. For example, in many real-world control problems, one seeks to satisfy a series of constraints while optimizing the main objective. We have shown by providing two examples given in Section \ref{section5} that always learning to satisfy constraints as an auxiliary goal and then learning the main goal is very effective in learning speed and sample efficiency.
Also, we illustrate the process of defining auxiliary goals and reward in detail in Section \ref{section4} in order to diminish our approach limitation by showing the simplicity of defining the auxiliary goals and reward.

The rest of this paper is organized as follows. In Section \ref{section2}, we  review related work. We introduce our proposed framework in Section \ref{section3}. Preliminary and extended experimental results and analyses are reported in Sections \ref{section4} and \ref{section5}, respectively. We conclude  and discuss future works in Section \ref{section6}.

%\textbf{Notation:} \textcolor{red}{It would be good to explain the main notation used...}

\section{Related Work} \label{section2}
To the best of our knowledge, this paper stands out from the rest of the studies because first, we use a sequence of auxiliary goals (i.e., MDPs) by defining only one assistant reward (not many and not following from a particular distribution) and an annealing function. Second, we learn each auxiliary task only one iteration (without necessarily having $\epsilon$-accuracy with $\delta$ probability assumption). And third, our proposed method supports policy and value transfers, which leads our approach to combine with any type of RL-based algorithm to improve sample efficiency. However, the scope of related work can be broadened to include other efforts to help the agent learn its main task more efficiently.
It should be noted that due to the strong assumptions of other studies stated in the following, it is not possible to use them in the examples in the next sections. As a result, we have to compare our method with the original algorithms.
%It is important to note that due to the differences in the initial hypotheses and objectives, some of which were also outlined in conjunction with the related literature, we are not able to compare the results with them.

 Improving the exploration process of a RL agent has been well studied in the literature. 
Many previous efforts are based on the hypothesis that if we can visit as many states as possible with high probability, then the agent's learning will be more successful and sample efficient. In other words, further exploration is in line with achieving the main goal of the RL agent. There are two fundamental questions that emerge in this setting. In the absence of rewards, what are the agents supposed to look for? and at what point should the agent stop exploring and start acting greedily? 
%As RL has evolved, many approaches have been suggested to address these questions.
%
%
%In general, 
Research that seeks to answer these questions can be divided into two general categories.
Firstly, there are single-environment approaches, which include intrinsic motivation with \textit{visitation counts} \cite{NIPS2016_afda3322,STREHL20081309}, \textit{optimism} \cite{NIPS2006_c1b70d96, NIPS2008_e4a6222c, lai1985asymptotically}, \textit{curiosity} \cite{NIPS2016_abd81528, pmlr-v70-pathak17a, pmlr-v100-schultheis20a, stadie2015incentivizing}, and \textit{reward shaping} \cite{Dann_Zambetta_Thangarajah_2019, ijcai2019-324,albe} .
Secondly, there multi-environment approaches that incrementally learn a task cross environments, for example through \textit{transfer learning} \cite{weiss2016survey, parisi2021, 2020sequential, 2018abel, 2021lipschitz}, \textit{continual learning} \cite{Kirkpatrick3521}, \textit{meta-learning} \cite{finn2017model}, and \textit{curriculum learning} \cite{narvekar2020curriculum, luo2020accelerating}. We discuss these approaches below and contrast them to our contributions. 
%We discuss some of the interesting approaches in more detail in the following.

\textbf{Single-environment:} The notion of intrinsic rewards for exploration originated with Schmidhuber \cite{schmidhuber1991}, who proposed encouraging exploration by visiting unpredictable states. 
In recent years, researchers in RL have extensively studied auxiliary rewards to compensate for the lack of external rewards.
Many intrinsic rewards have been proposed, including bonuses based on visitation counts and prediction errors \cite{NIPS2016_afda3322,STREHL20081309}.
For instance, a dynamic model can be learned and used to predict the next state \cite{NIPS2016_abd81528, pmlr-v70-pathak17a, stadie2015incentivizing}.
Here, the agent is incentivized to explore unpredictable states by granting a bonus proportional to its prediction error.
Another idea, by Schultheis et al. \cite{pmlr-v100-schultheis20a}, is to maximize extrinsic rewards by meta-gradient to learn intrinsic rewards. Also, potential-based reward shaping to encourage agents to explore more has been studied as a way of increasing the learning speed \cite{Dann_Zambetta_Thangarajah_2019, ijcai2019-324,albe}.

These methods of exploration are often called agent-centric, since they  are based updating the agent's belief, e.g., through the forward model error. In this sense, these works fall into the same category as ours, both rely on the hypothesis that exploration at the same time with exploitation by agent can be sufficient to achieve a (sub-) optimal policy. 
Another common assumption is that in all the papers following this method, the main goal and consequently the target award, i.e., the extrinsic reward is clear and we have knowledge about it. 
The main difference between our work and others is that their purpose in the first place is only to explore as much as possible, and they are based on the assumption that by exploring more, the agent can learn an optimal policy. 
In contrast, we do not have such an assumption, and we do not seek to visit the states as much as possible. Instead, we focus only on intelligent and auxiliary exploration, which is resulted from transferred knowledge gained on intermediate partially solved tasks, which is in line with the main goal and accelerates the learning of the main task, not a complete acquaintance of the environment.



% In general, these methods of exploration are called agent-centric, which means that these methods are based on the agent's belief, such as the forward model error. In this sense, these works fall into the same category as ours, and both rely on the hypothesis that exploration at the same time with exploitation by agent can be sufficient to achieve a (sub-) optimal policy.  Another common assumption is that in all the papers following this method, the main goal and consequently the target award, i.e., the extrinsic reward is clear and we have knowledge about it. The main difference between our work and others is that their purpose in the first place is only to explore as much as possible, and they are based on the assumption that by exploring more, the agent can learn an optimal policy. 
% In contrast, we do not have such an assumption, and we do not seek to visit the states as much as possible. Instead, we focus only on intelligent and auxiliary exploration, which is in line with the main goal and accelerates the learning of the main task, not a complete acquaintance of the environment.

\textbf{Multi-environment:} 
%
The second set of papers reviewed followed the idea of incrementally learning tasks that has long been known in machine learning \cite{ring1994}.
In RL, although recent methods have mostly focused on policy and feature transfer, some have studied exploration transfer \cite{parisi2021}.
In policy transfer, behaviors are transferred to a new agent (student) by a pre-trained agent (teacher). 
The student is trained to minimize the KullbackLeibler divergence to the teacher using policy distillation, for example \cite{DBLP}.
Other methods, re-use policies from source tasks directly to create a student policy \cite{10.1145/1160633.1160762, NIPS2017_350db081}. On the other hand, a pre-learned state representation used in feature transfer encourages the agent to explore when it is presented with tasks \cite{Hansen2020Fast}.
In exploration transfer, in a task-agnostic and environment-centric scenario, the agent is first attempted by defining an intrinsic reward to visit the states as more as possible especially interesting states in multiple environments instead of learning the main task, and then this prior knowledge is transferred as a bias to the main environment in which the main task is to be trained \cite{parisi2021}.
Studies on continual reinforcement learning seek to determine how learning one or more tasks can accelerate learning of other tasks, as well as avoid ruinous forgetting \cite{Kirkpatrick3521}.
Meta reinforcement learning and curriculum learning, on the other hand, focus on exploiting underlying similarities between tasks to speed up the learning process of new tasks \cite{finn2017model,narvekar2020curriculum, luo2020accelerating}. 
Many studies have been done under the Lifelong RL framework, where the tasks change following a specific distribution \cite{2020sequential, 2018abel, 2021lipschitz}. In \cite{2018abel, 2021lipschitz}, by spending a lot of computational costs, knowledge transfer is done through transferring initializers. Most are only compatible with ($\epsilon$, $\delta$)-PAC algorithms \cite{2020sequential, 2018abel, 2021lipschitz, strehl2009}

%In these multi-environment methods  it is assumed that we can learn across multiple tasks/environments. For this to beneficial, we need to have multiple tasks/environments that are similar to each other. However, 
%
In the mentioned incrementally learning task methods, it is assumed that there are several tasks/environments that, with the help of the experience gained on one or multiple tasks/environments, we seek to accelerate the learning of the same task in the new environment or the new task in the same environment. 
As a result, the most important hypothesis is that tasks/environments are very similar to each other, which is not always consistent with reality and deprives us of the possibility of using these methods on any task and any environment. On the contrary, our proposed framework does not need to have several similar environments/tasks. In particular, by defining only one assistant reward along with a decreasing function, we successfully represent a large number of MDPs, which essentially does not require the assumption of switching tasks over a particular distribution. Therefore, it can be used for any environment and any task.
Another point is that in transfer learning, the learning process consists of two phases of pre-training and the end training phase, which are isolated from each other. More specifically, it is assumed that the learning on the previous tasks has been done thoroughly and with good accuracy. In our proposed framework, however, we are smoothly and consistently shifting between the auxiliary goals and the main goal. Not only do we not assume any tasks are already learned, but we also do not need to learn them completely and accurately, i.e., we only have one iteration of training per auxiliary task. 
In the other studies, like \cite{2020sequential}, which identify the closest task for knowledge transfer, there is a lot of computational cost and limit on the learning transfer method and the algorithms that can be combined with their methods. But our approach does not add computational complexity. Also, it can transfer knowledge through both value and policy transfer and is compatible with any algorithm.
Note that, we have an integrated learning process so that when considering the acceleration of convergence, we do not neglect to consider the time required to learn the auxiliary goals.
The last issue is related to \cite{parisi2021}, where the authors try to explore as much as possible in the pre-training stage. On the contrary, we do not suppose such a hypothesis in our work. As a result, we only focus on defining auxiliary goals for faster learning the task.

%\section{Our Framework: Teaching Assistant}
\section{Our Framework: \texttt{TA-Explore}}
\label{section3}

In this section, we first describe the problem formulation in Section \ref{Sec:RL} and our novel \texttt{TA-Explore} framework in Section \ref{Sec:TA}.

\subsection{Reinforcement Learning}%Problem Formulation: MDP}
\label{Sec:RL}
%\textcolor{red}{It seems that you have been implicitly assuming episodic tasks with finite horizon. But all the discussing is just for the infinite  horizon case. There seems to be a clear miss-alignment between what you have are thinking and what you have written. I think it is easily fixed if we clearly explain the episodic tasks.   }

We consider reinforcement learning in a Markov Decision Process (MDP). Formally, a MDP is characterized by a 5-tuple $(S,A,P,R^T,\gamma)$ where $S$ denotes the set of states; $A$ denotes the set of actions; $P : S \times A \rightarrow \Delta(S)$ denotes the transition probability\footnote{i.e., $P(s'|s,a) = \mathsf{Pr}[s_{t+1} = s' | s_t=s,a_t=a]$.} from state $s \in S$ to state $s' \in S$ when the action $a \in A$ is taken; $R^T : S \times A \times S \rightarrow \mathbb{R}$ is the immediate reward\footnote{Here we use the superscript T on the reward to distinguish it from the assistant reward defined later.} received by the agent after transitioning from $(s,a)$ to $s'$;  $\gamma \in[0,1)$ is the discount factor that trades off the instantaneous and future rewards. The discount factor $\gamma$ controls how much weight is put on old  rewards compared to new ones, i.e., small $\gamma$ means that we put higher priority to recent rewards. %The key assumption of the MDP is the Markov assumption that the probablity $P(s,a)$ 

%It is widely accepted that MDP is a standard model to describe agent decision-making with full observability of the system state $s$.

We consider episodic tasks. % where the episodes are indexed by $e\in \mathbb{N}$. 
 In the beginning of each episode the agent starts in an initial state $s_0\in S$ which is an IID sample from the the distribution $\mu$ on $S$. 
After that, at each time step $t\in \mathbb{N}$, the agent takes an action $a_t$ which leads the system to transfer to $s_{t+1} \sim P(\cdot|s_t,a_t)$. The agent receives an instantaneous reward $R^T(s_t,a_t,s_{t+1})$. The agent makes decision by following a parameterized policy $\pi : S \times \Theta \rightarrow \Delta(A)$, a mapping from the state space $S$ to a distribution over the action space $A$. In particular, we have $a_t \sim \pi(\cdot|s_t; \theta)$ where $\theta \in \Theta$ is an adjustable parameter. The goal of the agent is to find the policy  $\pi(\cdot|s_t; \theta)$ by tuning the parameter $\theta$ that optimizes the cumulative reward
\begin{equation}
  M(\theta):=\mathbb{E}\bigg[\sum_{t= 0}^{H} \gamma^t R^T(s_t,a_t,s_{t+1})\Big|a_t \sim \pi(.|s_t,\theta),s_0\sim \mu\bigg],
\end{equation}
where $H$ is the termination time.\footnote{Note that $H$ is generally a random variable parametrized by $\theta$.}
We use  the notation $M(\theta)$ to indicate that optimizing the above cumulative reward is the main goal of the agent. In particular, the agent seeks to find the optimal solution to the following optimziation problem
\begin{equation} \label{problem:M}
    \max_{\theta\in \Theta} ~M(\theta).
\end{equation}

 The goal of RL is to learn the optimal parameter $\theta$ by trial-and-error where the agent continually interacts with the environment. Learning to optimize the main goal is often difficult, it is not uncommon that it takes thousands or even millions of interactions to find good policies. Next we illustrate our main idea, how we can facilitate more efficient learning by using assistant rewards.  

%The goal of solving the MDP is thus to find a policy $\pi : S\times \Theta \rightarrow \Delta(A)$, a mapping from the state space $S$ to the distribution over the action space $A$, so that $a_t \sim \pi(\cdot|s_t; \theta)$, where $\theta \in \Theta$ are adjustable parameters while training a model, and the return or discounted expected cumulative rewards, which is defined below, is maximized.
%\begin{equation}
%  \mathbb{E}\bigg[\sum_{t\geq 0} \gamma^t R(s_t,a_t,s_{t+1})\Big|a_t \sim \pi(.|s_t,\theta_t),s_0\bigg]
%\end{equation}
%



%%%%%%

%Through interactions with the environment, a RL agent is modeled to be able to make sequential decisions. In general, the environment is defined as a discounted Markov decision process (MDP) with an infinite horizon, henceforward known as the Markov decision process.

%A Markov decision process is characterized by a 5-tuple $(S,A,P,R,\gamma)$, where $S$ is a finite set of states, and $A$ denotes the action spaces; $P : S \times A \rightarrow \Delta(S)$ denotes the transition probability (i.e., $P(s,s') = \mathsf{Pr}[s_{t+1} = s' | s_t=s,a_t=a]$) from any state $s \in S$ at time $t$ to any state $s' \in S$ at time $t+1$ for any given action $a \in A; R : S \times A \times S \rightarrow \mathbb{R}$ is the immediate reward received by the agent after transitioning from $(s,a)$ to $s'$;  $\gamma \in[0,1)$ is the discount factor that trades off the instantaneous and future rewards. When discounting, rewards that are earned earlier are given higher priority, which is desirable in some cases.

%It is widely accepted that MDP is a standard model to describe agent decision-making with full observability of the system state $s$. At each time $t$, the agent takes an action $a_t$ considering the system state $s_t$, which leads the system to transfer to $s_{t+1} \sim P(.|s_t,a_t)$. Additionally, the agent receives an instantaneous reward $R(s_t,a_t,s_{t+1})$. The goal of solving the MDP is thus to find a policy $\pi : S \rightarrow \Delta(A)$, a mapping from the state space $S$ to the distribution over the action space $A$, so that $a_t \sim \pi(.|s_t, \theta_t)$, where $\theta \in \Theta$ are adjustable parameters while training a model, and the return or discounted expected cumulative rewards, which is defined below, is maximized.
%\begin{equation}
%  \mathbb{E}\bigg[\sum_{t\geq 0} \gamma^t R(s_t,a_t,s_{t+1})\Big|a_t \sim \pi(.|s_t,\theta_t),s_0\bigg]
%\end{equation}


\subsection{\texttt{TA-Explore}}
\label{Sec:TA}
%\textcolor{red}{some of the discussion will need to be adjusted}

 Just like with human learning, it is reasonable to first have the agent solve simple tasks that are similar to the difficult main goal $M(\theta)$. Then gradually increase the difficulty of the simple tasks so they eventually converges to the main goal. Formally, we may consider some auxiliary goal
\begin{equation}
    A(\theta) =  \mathbb{E}\bigg[\sum_{t= 0}^{H} \gamma^t R^A(s_t,a_t,s_{t+1})\Big|a_t \sim \pi(.|s_t,\theta),s_0\sim \mu \bigg], 
\end{equation}
 where $R^A$ is an assistant reward.
 Then the auxiliary goal is to optimize the following problem
  \begin{equation}
\max_{\theta \in \Theta} ~A(\theta).
\end{equation}

Ideally, we should choose $R^A$ in a way that a) it results in a simple RL problem that can be solved fast and b) solving it is a side-step towards the main goal. We illustrate examples of such good assistant rewards that can truly make learning faster in the next two sections. 
The goal of the agent is not to learn the auxiliary goal,  but rather to use it to facilitate learning. Thus it is important that the agent can gradually put more effort on the main goal $M(\theta)$. For example, the agent might gradually increase its focus after finishing each episode. To that end, let $\beta(e)$ denote the  parameter that controls how quickly the agents progresses towards the main goal, where  $e\in \mathbb{N}$ is the episode index.
% 
% \footnote{In principle, we can apply the same idea in continuous by}
% 
% The parameter that controls how quickly the agents progresses towards the main goal is $\beta(e).$ In particular, 
 Then, during episode $e$, the agent uses the immediate reward\footnote{We can also apply this idea on continuous tasks with infinite horizon but then $\beta(\cdot)$ should be a function of $t$.} 
\begin{equation} \label{eq}
 R_e(s_t,a_t,s_{t+1}) {=}\beta(e)R^A(s_t,a_t,s_{t+1}) {+} (1{-}\beta(e))R^T(s_t,a_t,s_{t+1}).
 \end{equation}
 %
 %
%Note that, regardless of whether the task is episodic or continuous, during learning the agent we always consider the time horizon (i.e., training episode length) that even if none of the conditions for terminating the episode are met, we reset the environment. So, in both cases, $e$ can be considered as a counter equal with the number of times the environment has been reset.
 %
 %
 
 The agent will then try to optimize the following  teacher assisted goal at episode $e$
 \begin{equation}
     \texttt{TA}(\theta;e)=\mathbb{E}\bigg[\sum_{t= 0}^{H} \gamma^t R_e(s_t,a_t,s_{t+1})\Big|a_t \sim \pi(.|s_t,\theta),s_0\sim \mu \bigg].
 \end{equation}
 
In particular, by finding  the solution to the optimization problem \begin{equation}\label{problem:TA}
\max_{\theta \in \Theta} \texttt{TA}(\theta; e),
\end{equation}
as $e$ goes to infinity. 
By examining $\texttt{TA}(\theta; e)$, it can be found that now, despite having only one assistant reward ($R^A$), thanks to the $\beta(\cdot)$ variable, until the $\beta(\cdot)$ goes zero, several teacher assisted goals are generated, which indicate several artificial tasks (MDPs).
Note that if $\beta(e)$ decays to zero, then $\theta_{M}^{\star}=\theta_{\texttt{TA}}^{\star}$, where $\theta_{M}^{\star}$ and $\theta_{\texttt{TA}}^{\star}$ are, respectively, the solutions to~\eqref{problem:M} and~\eqref{problem:TA} as $e$ goes to infinity.\footnote{Here we assume a unique optimal solution but, in general, the set of optimizers of~\eqref{problem:M} and~\eqref{problem:TA} are equal.} This can be seen by the fact that the affect of the assistant reward disappears as $e$ increases. In other words, solving the \texttt{TA}-goal is the same as solving the main-goal. However, if the assistant reward is designed intelligently then can  improve the sample efficiency of any RL algorithm. We illustrate this in the next two sections. 

Moving to a new MDP in each episode (until $\beta(e)=0$) increases the non-stationarity of the problem.
In this sense, this work is similar to the methods of encouraging the exploration \cite{NIPS2016_afda3322,STREHL20081309}, which at first sight all cause more instability of the problem and should lead to slow learning. However, it has been seen that due to giving more feedback leads to an increase in learning speed. It is also our case, with the difference that here, instead of encouraging blind exploration of all states, we exploit transferred knowledge between tasks before learning each of them thoroughly (which leads to overfitting and being far from the main goal). The art here is to reliably set $\beta(e)$) to transfer the appropriate prior knowledge through the value function or policy, which results in a more sample-efficient algorithm.
 
 Finding a suitable  $\beta(e)$ is clearly an important part of making the learning progress smooth, which also determines the number of MDPs. In general,  $\beta(e)$ should be decreasing in $e$ and should converge to $0$ as $e$ increases, to ensure convergence to the main goal. How fast $\beta(e)$ decreases depends on the task, and hence a specific and unique formula cannot be stated for it.
 %
 %However, since $\beta(e)$ definition is easily possible, this shortcoming cannot be considered considerable. 
% \textcolor{red}{\textbf{This is very complicated discussion. I suggest splitting up the ideas in couple of sentences. We could give also the functional form of $\beta(e)$ that we used for the control problem.   }In general,  the authors suggest that when choosing a $\beta(e)$, it should be noted that the closer the alignment of the auxiliary goal to the main goal, the easier the $\beta(e)$ selection, in which case, for example, the $\beta(e)$ can start from one and slowly tend to zero by half of the total episodes. On the other hand, if the alignment between the objective functions is low, the authors suggest using functions with a faster slope rate to zero, or for example $\beta(e) = \beta(0)\lambda^e$, where $\lambda \in (0,1)$.}
In general, how well the main goal and auxiliary goal align % ass alignment between the main goal and auxiliary goal functions 
can be a good clue for how to select $\beta(e)$. Specifically, if they are well aligned then then $\beta(e)$ can be decreased at a slower rate, e.g., linear rate by setting 
%$\beta(e) = \max[(E-e)\beta(0)/E,0]$
$\beta(e) = [(E-e)\beta(0)/E]_+$. Here $\beta(e)$ starts to gradually decrease from $\beta(0)$ and reaches zero at episode $E$. 
Conversely, if the alignment between these two goals is low then it is better to decrease $\beta(e)$ at a faster rate. For example, in that case we might have  $\beta(e)$ decrease exponentially fast, i.e., $\beta(e) = \beta(0)\lambda^e$, where $\lambda \in (0,1)$.

%We name this new learning framework \texttt{TA-Explore} because it explores intelligently in the direction of the main goal, instead of blindly exploring, and ultimately helps converge faster.
\texttt{TA-Explore} can be used with  any task and environment and is compatible with any RL algorithm. Because our method is not directly focused on tightening the upper band, it is not limited to some algorithms, such as R-MAX \cite{ strehl2009}, which depend highly on the upper bound. It can be integrated with any non-($\epsilon$, $\delta$)-PAC algorithm. \texttt{TA-Explore} may be used with both value-based methods and policy-based methods and both tabular methods and deep-RL methods. In deep-RL methods, prior knowledge improves the initial weighting of neural networks used as approximator functions (instead of random initial weighting).
In particular, tasks with multiple goals or constraints can easily be included in this framework (see Section \ref{section5}). Unlike conventional transfer learning methods, which include two phases of pre-training and end training, it follows unified and integrated learning that is both less complex and requires less time for training. Also, due to not needing several pre-solved tasks, it can be easily applied to any environment (without adding computational complexity).

In the following, we illustrate how to use \texttt{TA-Explore} with a simple example in Section \ref{section4}, and then its effectiveness in solving difficult real-world problems is further analyzed in Section \ref{section5}.
 
 

%learning trajectory
\iffalse 
Now suppose the task $T$ in which our objective is to maximize the main goal function, $M(a_t, s_t, \phi_t)$, where $\phi$ are parameters that may be considered in calculating the main goal function value. To do this by RL algorithms, we require a suitable definition of the reward, which we call the target reward here and represent it with $R^T$, so that maximizing the target reward by finding an optimal policy leads to the main goal function maximization and vice versa.
\begin{equation}
\max_{a \in A} M(a_t, s_t, \phi_t) \Leftrightarrow \max_{a \in A; \theta \in \Theta} \mathbb{E}\bigg[\sum_{t\geq 0} \gamma^t R^T(s_t,a_t,s_{t+1})\Big|a_t \sim \pi(.|s_t,\theta_t),s_0\bigg]
\end{equation}
So in a nutshell, in RL, there is an implicit mapping from the main goal function $M$ to the target reward $R^T$, i.e., $M \Leftrightarrow R^T$.

Now, without diminishing the generality of the problem, it can be assumed that in task $T$, a simpler goal function, which we call the auxiliary goal function $A(a_t, s_t, \omega_t)$, can be defined so that, first, it is (at least intuitively) easier to reach. And secondly, it is in line with the main goal function $M$, so that plays a role of prior knowledge for the agent. Similarly, we can specify a reward for $A$, which we call the assistant reward $R^A$, such that;
\begin{equation}
\max_{a \in A} A(a_t, s_t, \omega_t) \Leftrightarrow \max_{a \in A; \theta \in \Theta} \mathbb{E}\bigg[\sum_{t\geq 0} \gamma^t R^A(s_t,a_t,s_{t+1})\Big|a_t \sim \pi(.|s_t,\theta_t),s_0\bigg]
\end{equation}
and consequently, $A \Leftrightarrow R^A$.

We do not want to first complete the learning to maximizing $A$ through $R^A$ and then transfer learning.
Our main idea, instead, is smoothly and consistently shifting to  the main goal function $M$ optimization through the following equation before completing learning on the maximization of the auxiliary goal function $A$;
\begin{equation}
\max_{a \in A} \beta(e)A(a_t, s_t, \omega_t) + (1-\beta(e))M(a_t, s_t, \phi_t)
\end{equation}
and hence,
\begin{equation} \label{eq:1}
\beta(e)A + (1-\beta(e))M \Leftrightarrow \beta(e)R^A + (1-\beta(e))R^T
\end{equation}
where $\beta(0) \in (0,1]$, and $\lim_{e\rightarrow \infty} \beta(e)=0$.

Note that selecting a suitable $\beta(e)$ is highly dependent on the task, and hence a specific and unique formula cannot be stated for it. However, since $\beta(e)$ definition is easily possible, this shortcoming cannot be considered considerable. In general, the authors suggest that when choosing a $\beta(e)$, it should be noted that the closer the alignment of the auxiliary goal to the main goal, the easier the $\beta(e)$ selection, in which case, for example, the $\beta(e)$ can start from one and slowly tend to zero by half of the total episodes. On the other hand, if the alignment between the objective functions is low, the authors suggesions with a faster slope rate to zero, or for example $\beta(e) = \beta(0)\lambda^{e}$, where $\lambda \in (0,1)$ and $e$ is number of episodes passed so far.

We name this new learning framework \texttt{TA-Explore} because it explores intelligently in the direction of the main goal, instead of blindly exploring, and ultimately helps converge faster.
\texttt{TA-Explore} has generalizability so that it can be used for any task and any environment and is compatible with any RL algorithm -both value-based methods and policy-based methods. 
In particular, tasks with multiple goals or constraints can easily be included in this framework (see Section \ref{section5}). Another point is that unlike conventional transfer learning methods, which include two phases of pre-training and end training, it follows unified and integrated learning that is both less complex and requires less time for training.
In the following, we explore how to use and how to work \texttt{TA-Explore} with a simple example in Section \ref{section4}, and then its effectiveness in solving difficult problems is further analyzed in Section \ref{section5}.
\fi

\section{Random Walk: An Illustration} \label{section4}
% \begin{figure}[t]
%      \centering
%      \begin{subfigure}[b]{0.455\linewidth}
%          \centering
%          \includegraphics[width=\linewidth]{fig1_1.png}
%          \caption{}
%          \label{fig1.a}
%      \end{subfigure}
%      \hfill
%      \begin{subfigure}[b]{0.45\linewidth}
%          \centering
%          \includegraphics[width=\linewidth]{fig1_2.png}
%          \caption{}
%          \label{fig1.b}
%      \end{subfigure}
%         \caption{Random Walk example, inspired by Sutton et al.'s book on Reinforcement Learning \cite{sutton2018}, where (a) describes how to receive the target reward $R^T$ and (b) illustrates how to acquire the auxiliary reward $R^A$. In both cases, the episode terminates by going to A or E states.}
%         \label{fig1}
% \end{figure}

% \begin{figure}[h]
%   \centering
%   \includegraphics[width=0.7 \linewidth]{fig2_10}
%   \caption{To how the considered $\beta(e)$ function behaves for Random Walk example, with different $\lambda$ values. As $\lambda$ increases, the downward slope of the curve decreases, resulting in a slower shifting of the agent from auxiliary goal $A$ learning to main goal $M$ learning.}
%   \label{fig2}
% \end{figure}

\begin{figure}[h]
     \centering
     \begin{subfigure}[b]{\linewidth}
         \centering
         \includegraphics[width=\linewidth]{fig1_1.png}
         \caption{}
         \label{fig1.a}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{\linewidth}
         \centering
         \includegraphics[width=\linewidth]{fig1_2n.png}
         \caption{}
         \label{fig1.b}
     \end{subfigure}
        \caption{Random Walk example \cite{sutton2018}, where (a) describes how to receive the target reward $R^T$ and (b) illustrates how to acquire the assistant reward $R^A$. In both cases, the episode terminates by going to A or E states.}
        \Description{des1}
        \label{fig1}
\end{figure}

In this section, we demonstrate the efficiency and simplicity of \texttt{TA-Explore} by examining a simple  Random Walk example \cite{sutton2018}. This example is a Markov reward process (MRP), a MDP without actions.  This allows us to illustrate the main ideas behind \texttt{TA-Explore} in simple and intuitive manner.  
%In this section, we demonstrate the efficiency and simplicity of \texttt{TA-Explore} by examining a simple  Random Walk example, see, e.g., Chapter 6 in \cite{sutton2018}. This example is a Markov reward process (MRP), a MDP without actions.  This allows us to illustrate the main ideas behind \texttt{TA-Explore} in simple and intuitive manner. 

%and it is a very useful problem to examine the prediction ability of the proposed method. 


Figure \ref{fig1} illustrates the Random Walk environment with 5 states. There are two terminal states A and E  and three non-terminal states B, C, and D. If the agent goes to either terminal state then the episode is terminated. In all episodes, the agent starts in state C. After that, in each time step, the agent takes a step to the left or to the right with an equal probability. The agent gets the reward zero for every transition, except it gets the reward $+1$ if it reaches the state E, the terminal state on the right.  The discount factor is $\gamma = 1$, i.e., this is a undiscounted MDP. 

Generalizing this random walk to multiple states is simple. We just  arrange all the states in a line like in Figure~\ref{fig1} except longer. Similarly, the terminal states are the once on the end of the line to the left and to the right. The initial state is the middle state. We will consider  such random walks with 5, 11, and 33 states.
%Figure \ref{fig1} illustrates the Random Walk environment with 5 states. There are two terminal states A and E  and three non-terminal states B, C, and D. If the agent goes to either terminal state then the episode is terminated. In all episodes, the agent starts in state C. After that, in each time step, the agent takes a step to the left or to the right with an equal probability. The agent gets the reward zero for every transition, except it gets the reward $+1$ if it reaches the state E, the terminal state on the right.  The discount factor is $\gamma = 1$, i.e., this is a undiscounted MDP. Generalizing this random walk to multiple states is simple. We just  arrange all the states in a line like in Figure~\ref{fig1} except longer. Similarly as in the 5 state case, the terminal states are the once on the end of the line to the left and to the right. The initial state is the middle state. At each time step the agent takes a step to the left or to the right with an equal probability. The agent gets the reward zero for every transition, except it gets the reward $+1$ if it reaches the terminal state on the right. We will consider  such random walks with 5, 11, and 33 states.

%In particular, the target reward is $R^T(s,a,s')=0$ for all $(s,a,s')\in S\times A \times S\setminus \{E\}$ and $R^T(s,a,s')=1$ if $s'=E$. The discount factor is $\gamma = 1$, i.e., this is a undiscounted MDP. This process can be generalized to multipel


%
%To be more specific, as shown in Figure \ref{fig1}, an example of this environment with five states,
%including two terminal states and three non-terminal states, the agent in all episodes starts from the middle state, C, and in each time step moves one step left or right with an equal probability. 
%It should be added that if the agent goes to the extreme left, A, or the extreme right, E, the episode terminates. 
%As seen in Figure \ref{fig1.a}, the main goal $M$ is to reach the most right state E.  Consequently, for this objective, the target reward $R^T$ is defined in such a way that the agent will receive a reward of +1 only when it reaches this state. We set $\gamma^t = 1$ (the MDP is undiscounted). 


% This is a difficult sentence to read :)  obviously clear. There is much information, we can move \gamma=1 to the previous paragraph
% intuitively clear

\begin{figure}[h]
  \centering
  \includegraphics[width=1\linewidth]{fig2_10n}
  \caption{The behaviour of the various $\beta(e)$ function considered for Random Walk example, with different $\lambda$ values. As $\lambda$ increases, a slower shifting of the agent from auxiliary goal $A$ learning to main goal $M$ learning happens.}
  %\caption{To how the considered $\beta(e)$ function behaves for Random Walk example, with different $\lambda$ values. As $\lambda$ increases, the downward slope of the curve decreases, resulting in a slower shifting of the agent from auxiliary goal $A$ learning to main goal $M$ learning.}
  \Description{des2}
  \label{fig2}
\end{figure}

Since there is no action here, the main goal of the agent is to learn the value of each state by experience. 
%In particular, the value function is defined as 
%$$a\textcolor{red}{We\_should\_probably\_define\_the\_value\_here.} $$
%\textcolor{blue}{the true value of each state is the probability of terminating on the right if starting from that state.}
%
Learning the value is slow in the initial episodes, it requires a lot of samples. This is because in the beginning  the agent only gets an immediate  feedback (non-zero reward) if and when it reaches the terminal state on the right. 

However, we might facilitate learning  by providing the agent with simpler tasks that provide immediate feedback  more frequently. For example, as shown in Figure \ref{fig1.b}, we consider the assistant reward $R^A$ that provides the immediate reward 0.1 every time the agent goes to the right except when it reaches the terminal state on the right then we give 1 as the immediate reward. It is intuitive that learning this auxiliary goal is faster since the agent gets more frequent reward feedback to update its value function. Also, as illustrated in Figure \ref{fig2}, a simple but intelligent idea for determining the $\beta(e)$ function could be $\beta(e) = \beta(0)\lambda^{e}$, where $\beta(0)=1$, and $\lambda \in \{0.8, 0.9, 0.95\}$. The agent could first start to learn goal $A$ but focus very quickly on learning goal $M$.


To study our proposed method performance, although \texttt{TA-Explore} supports any kind of RL algorithm since Random Walk is an MRP problem with discrete state space and not looking for an optimal action learning, a basic value-based algorithm is our choice. In this paper, we use the temporal-difference learning method (TD(0)) \cite{tesauro1995} with a constant step-size of 0.1. So in this example, transfer learning is taking place through value transfer. This means that the value functions obtained for each task are used as the initial values of the next task.

We report the average results of 100 times tests. It should be noted that at each execution, the state-value functions for the all states are initialized with zero. Since the true value function of this problem can be calculated through the Bellman equation exactly, the performance measurement is the root mean square (RMS) error between the value function learned and the true value function corresponding to the main goal $M$ and the target reward $R^T$.

As shown in Figure \ref{fig3.a}, if we learn only the auxiliary goal $A$ and consequently considering only the assistant reward $R^A$, it is observed that by around episode 38, the agent is surprisingly learning the main goal $M$ as well, as RMS error has been dropping.
But then, as the agent tries to learn the auxiliary goal $A$ more, the error increases. This is intuitive due to the difference between the true value functions of the two goals. But the point that was predictable is that the speed of learning the auxiliary goal $A$ is faster than learning the main goal $M$ (See Only $R^T$ in Figure \ref{fig3.a}). Hence if the agent starts learning the auxiliary goal $A$ first, but before over-fitting on it, starts learning the main goal $M$, it can get the most out of prior information. Then, it learns the main goal $M$ much faster. 

Specifically, Figure \ref{fig3.a} shows that with $\lambda = 0.95$, \texttt{TA-Explore} managed to make the most of learning on the auxiliary goal $A$, so that it was finally able to converge more than about twice as fast on the main goal $M$.
As the number of states increases to 11 in Figure \ref{fig3.b}, still $\lambda = 0.95$ is a good choice. The critical point is that as the number of states increases, the superiority of our proposed method becomes noticeable. By increasing the number of states, the probability of visiting the most right state for the first time decreases. So, the convergence speed of the baseline approach, which is highly dependent on the state's numbers, decreases. 
On the other side, our method still with 50\% of probability will be rewarded even on the first step. Additionally, in Figure \ref{fig3.c}, the results are shown with considering 33 states, where the baseline has not yet been able to converge completely. On the contrary, it happens to \texttt{TA-Explore}. Note that, here, $\lambda = 0.95$ is no longer a proper choice. 

In fact, computing the true values in both cases using the Bellman equation turns out that although the two goals seem largely in line with each other, with an increasing number of states, an increasing difference between the true state-value functions appears. As a result, if the agent spends a lot of time learning the auxiliary goal, not only will it not get acceptable prior information from learning the auxiliary goal $A$, but sometimes it will even lead to a longer learning process of the main goal $M$ due to the need to pass the time to forget those bad experiences. Hence, the agent should shift to learn the main goal $M$ faster but not too hastily, just like $\lambda = 0.9$ in Figure \ref{fig3.c}, where has been able to converge at more than twice the speed.

In conclusion, by examining these results, we can ensure the scalability of the proposed method. By selecting an appropriate $\beta(e)$ function, as the number of states increases, the area enclosed between the baseline and our algorithm increases more and more. It means that on the one hand, the baseline takes a long time to converge, and on the other hand, \texttt{TA-Explore} converges with a high slope, gaining the appropriate prior knowledge.
It can also be expected that as \texttt{TA-Explore} is exceptionally profitable on simple tasks and value-based algorithms that do not leave much room for maneuver, it will have remarkable performance on more complex tasks such as tasks with continuous state and action spaces, and it will outstandingly improve the results of policy-based approaches as well as deep-RL methods. The next section provides evidence for this claim.

\begin{figure}[h] %[H]
     \centering
     \begin{subfigure}{0.9\linewidth}
         \centering
         \includegraphics[width=\linewidth]{fig2_1n.png}
         \caption{5 states}
         \label{fig3.a}
     \end{subfigure}
     \begin{subfigure}{0.9\linewidth}
         \centering
         \includegraphics[width=\linewidth]{fig4_3n.png}
         \caption{11 states}
         \label{fig3.b}
     \end{subfigure}
     \begin{subfigure}{0.9\linewidth}
         \centering
         \includegraphics[width=\linewidth]{fig5_2n.png}
         \caption{33 states}
         \label{fig3.c}
     \end{subfigure}
        \caption{The main goal learning curves in Random Walk example for different $\lambda$ values and the different number of states. The undrawn curves in (b) and (c) have diverged and have been omitted to show the rest of the results in more detail.}
        %\caption{The main goal learning curves in Random Walk example for different $\lambda$ values and the different number of states. The performance measure shown is the root-mean-squared (RMS) error between the value function learned considering the assumed rewards (i.e., only $R^T$, only $R^A$, or \texttt{TA-Explore} with different $\lambda$s) and the true value function, which is averaged over the states and then averaged over 100 runs. 
        %In all cases, the TD(0) algorithm is used as the backbone. 
        %As it turns out, in all cases, by choosing the appropriate $\lambda$, one can be sure that \texttt{TA-Explore} is learning faster. The undrawn curves in (b) and (c) have diverged and have been omitted to show the rest of the results in more detail.}
        \label{fig3}
\end{figure}





%Learning the value is slow in the initial episodes, it requires a lot of samples. This is because in the beginning  the agent only gets an immediate  feedback (non-zero reward) if and when it reaches the terminal state on the right. However, we might facilitate learning  by providing the agent with simpler tasks that provide immediate feedback  more frequently. For example, as shown in Figure \ref{fig1.b}, we consider the assistant reward $R^A$ that provides the immediate reward 0.1 every time the agent goes to the right except when it reaches the terminal state on the right then we give 1 as the immediate reward. It is intuitive that learning this auxiliary goal is faster since the agent gets more frequent reward feedback to update its value function. To understand this better, let us next take a closer look at the true value functions in both cases so that we can use it to guess a proper $\beta(e)$ function, which is the only thing left of our proposed framework.

%\textcolor{red}{It is good if we can relate already}
%It is obviously clear that with these definitions, even in the most optimistic case we assume the task is undiscounted (i.e., $\gamma^t = 1$), the state-value functions will never change from the initial values unless the agent visits the extreme right state for the first time. Therefore, we based on our framework, desire to solve this problem by defining an auxiliary goal $A$ and a suitable assistant reward $R^A$. As shown in Figure \ref{fig1.b}, an auxiliary goal $A$ can be defined seeking to move the agent to the right, and consequently, we consider the auxiliary reward $R^A$ to be such that the agent for each time moves to the right, earning a reward of +1. It is intuitively clear that learning this auxiliary goal is much faster since with a 50\% probability in the first movement it can receive a reward and its state-value functions are updated. But let us take a closer look at the true value state functions in both cases so that we can use it to guess a proper $\beta(e)$function, which is the only thing left of our proposed framework.

%Using the Bellman equation, we can compute the true value of all the states. In particular, for the main goal $M$ with the target reward $R^T$ the values of the states A, B, C, D, and E, are, respectively, $0, 0.25, 0.5, 0.75,$ and $0$. Similarly, for the auxiliary goal $A$ with the assistant reward $R^A$, the values for the states A, B, C, D, and E, are, respectively, $0, 0.4, 0.7, 0.9,$ and $0$. If we repeat these calculations again for the random walk with seven states we get the values $0, 0.167, 0.333, 0.5, 0.667, 0.833,$ and $0$ for the main goal $M$ and $0, 0.417, 0.733, 0.95, 1.067, 1.083,$ and $0$ for the auxiliary goal $A$.


% \begin{wrapfigure}{l}{0.35\textwidth}
% \vspace{-25pt}
%   \begin{center}
%     \includegraphics[width=\linewidth]{fig2_10n}
%   \end{center}
%     \caption{$\beta(e)$ for Random Walk example, with different $\lambda$ values. As $\lambda$ increases, a slower shifting of the agent from auxiliary goal $A$ learning to main goal $M$ learning happens.}
%   %\caption{To how the considered $\beta(e)$ function behaves for Random Walk example, with different $\lambda$ values. As $\lambda$ increases, the downward slope of the curve decreases, resulting in a slower shifting of the agent from auxiliary goal $A$ learning to main goal $M$ learning.}
%   \label{fig2}
% \end{wrapfigure}


% \begin{figure}[h]
%   \centering
%   \includegraphics[width=0.7 \linewidth]{fig2_10}
%   \caption{To how the considered $\beta(e)$ function behaves for Random Walk example, with different $\lambda$ values. As $\lambda$ increases, the downward slope of the curve decreases, resulting in a slower shifting of the agent from auxiliary goal $A$ learning to main goal $M$ learning.}
%   \label{fig2}
% \end{figure}


% For value-based methods, state-value functions contain crucial information. Therefore, if we desire to use the information obtained through full training on auxiliary goal $A$ as prior information for learning the main goal $M$, a high alignment between them seems necessary. However, as it turns out, although at first glance the two goals seem largely in line with each other, with increasing number of states, an increasing difference between the the true state-value functions appears. As a result, if the agent spends a lot of time learning the auxiliary goal, not only will it not get acceptable prior information, but sometimes it will even lead to a longer learning process of the main goal due to the need to pass the time to forget those bad experiences. Thus, as illustrated in Figure \ref{fig2}, a simple but intelligent idea for determining the $\beta(e)$ function could be $\beta(e) = \beta(0)\lambda^{e}$, where $\beta(0)=1$, and $\lambda \in \{0.8, 0.9, 0.95\}$. The agent could first start to learn goal $A$ but focus very quickly on learning goal $M$. Note that, again, it is undeniable that as the number of states increases, the difference between the state-value functions of the two goals increases. Hence, we will need to shift faster to the main goal (for instance, considering $\lambda = 0.8 $).


% After specifying all the parameters of our proposed method, now it is time to study its performance. To do this, although \texttt{TA-Explore} supports any kind of RL algorithm since Random Walk is an MRP problem with discrete state space and not looking for an optimal action learning, a value-based algorithm is the best option. In this paper, we use the temporal-difference learning method (TD(0)) \cite{tesauro1995} with a constant step-size of 0.1. We report the average results of 100 times tests. It should be noted that at each execution, the state-value functions for the all states are initialized with zero. Since the true value function of this problem can be calculated through the Bellman equation exactly, the performance measurement is the root mean square (RMS) error between the value function learned and the true value function corresponding to the main goal $M$ and the target reward $R^T$.


% \begin{figure*}[t] %[h]
%      \centering
%      \begin{subfigure}{0.32\linewidth}
%          \centering
%          \includegraphics[width=\linewidth]{fig2_1n.png}
%          \caption{5 states}
%          \label{fig3.a}
%      \end{subfigure}
%      \begin{subfigure}{0.32\linewidth}
%          \centering
%          \includegraphics[width=\linewidth]{fig4_3n.png}
%          \caption{11 states}
%          \label{fig3.b}
%      \end{subfigure}
%      \begin{subfigure}{0.32\linewidth}
%          \centering
%          \includegraphics[width=\linewidth]{fig5_2n.png}
%          \caption{33 states}
%          \label{fig3.c}
%      \end{subfigure}
%         \caption{The main goal learning curves in Random Walk example for different $\lambda$ values and the different number of states. The undrawn curves in (b) and (c) have diverged and have been omitted to show the rest of the results in more detail.}
%         %\caption{The main goal learning curves in Random Walk example for different $\lambda$ values and the different number of states. The performance measure shown is the root-mean-squared (RMS) error between the value function learned considering the assumed rewards (i.e., only $R^T$, only $R^A$, or \texttt{TA-Explore} with different $\lambda$) and the true value function, which is averaged over the states and then averaged over 100 runs. In all cases, the TD(0) algorithm is used as the backbone. As it turns out, in all cases, by choosing the appropriate $\lambda$, one can be sure that \texttt{TA-Explore} is learning faster. The point is that the undrawn curves in (b) and (c) have diverged and have been omitted to show the rest of the results in more detail.}
%         \label{fig3}
% \end{figure*}


% As shown in Figure \ref{fig3.a}, if we learn only the auxiliary goal $A$ and consequently considering only the assistant reward $R^A$, it is observed that by around episode 38, the agent is surprisingly learning the main goal $M$ as well, as RMS error has been dropping. But then, as the agent tries to learn the auxiliary goal $A$ more, the error increases. This is intuitive due to the difference between the true value functions of the two goals. But the point that was predictable is that the speed of learning the auxiliary goal $A$ is faster than learning the main goal $M$ (See Only $R^T$ in Figure \ref{fig3.a}). Hence if the agent starts learning the auxiliary goal $A$ first, but before over-fitting on it, starts learning the main goal $M$, it can get the most out of prior information. Then, it learns the main goal $M$ much faster. Specifically, Figure \ref{fig3.a} shows that with $\lambda = 0.95$, \texttt{TA-Explore} managed to make the most of learning on the auxiliary goal $A$, so that it was finally able to converge more than about twice as fast on the main goal $M$.

% As the number of states increases to 11 in Figure \ref{fig3.b}, still $\lambda = 0.95$ is a good choice. The critical point is that as the number of states increases, the superiority of our proposed method becomes noticeable. By increasing the number of states, the probability of visiting the most right state for the first time decreases. So, the convergence speed of the baseline approach, which is highly dependent on the state's numbers, decreases. On the other side, our method still with 50\% of probability will be rewarded even on the first step. Additionally, in Figure \ref{fig3.c}, the results are shown with considering 33 states, where the baseline has not yet been able to converge completely. On the contrary, it happens to \texttt{TA-Explore}. Note that, here, $\lambda = 0.95$ is no longer a proper choice. The reason for this, as mentioned earlier, is that as the number of states increases, the difference between the true value functions of the main and auxiliary goals increases. As a result, less useful prior information can be obtained from learning the auxiliary goal $A$. So, the agent should shift to learn the main goal $M$ faster but not too hastily, just like $\lambda = 0.9$, where has been able to converge at more than twice the speed.

% In conclusion, by examining these results, we can ensure the scalability of the proposed method. By selecting an appropriate $\beta(e)$ function, as the number of states increases, the area enclosed between the baseline and our algorithm increases more and more. It means that on the one hand, the baseline takes a long time to converge, and on the other hand, \texttt{TA-Explore} converges with a high slope, gaining the appropriate prior knowledge. It can also be expected that as \texttt{TA-Explore} is exceptionally profitable on simple tasks and value-based algorithms that do not leave much room for maneuver, it will have remarkable performance on more complex tasks such as tasks with continuous state and action spaces, and it will outstandingly improve the results of policy-based approaches as well as deep-RL methods. The next section provides evidence for this claim.

\section{Experiment on Real-World Problems} \label{section5}
 
 We illustrate the promise of the \texttt{TA-Explore} framework on two optimal control problem with constraints. We consider  a dynamical system governed by the difference equation $s_{t+1}=g_t(s_t,a_t,e_t)$, where $s_t$ is the state of the system, $a_t$ is the control action, and $e_t$ is a random disturbance; $g_t$ is the rule that maps the current state, control action, and disturbance at time $t$ to a new state. The goal of the agent is to find the actions that optimize the accumulated immediate rewards  $f_t(s_t,a_t)$. Often there are also constraints on the states. Thus, the agent should not only optimize its rewards, it should also ensure feasibility of the states. This could be, for example, to ensure safety of the system operations. Mathematically, such problems are  formulated as follows
% 
% that at every time, the goal is to maximize $f_t(s_t,a_t)$, while $s_{\min} \leqslant s_t \leqslant s_{\max}$. In terms of mathematical optimization, we aim to solve the problem
% 
% \begin{align}
% \begin{split}\label{eq:10}
%     \text{maximize} f_t(s_t,a_t) \\
%     \text{subject to } s_{t+1}=g_t(s_t,a_t,e_t) \\
%      min \leqslant s_t \leqslant max
% \end{split}
% \end{align}
%
\begin{displaymath}
\begin{split}\label{eq:10}
    \text{maximize} & \sum_{t=0}^{H} \gamma^t f_t(s_t,a_t) \\
    \text{subject to } & s_{t+1}=g_t(s_t,a_t,e_t), \text{and }\\   
   & s_{\min} \leqslant s_t \leqslant s_{\max}.
\end{split}
\end{displaymath}

%The problem is an optimal control problem. However, as
 The constraints on the state variable $s_t$ make the problem a difficult optimal control problem and similar to a real-world problem. Even if the dynamics $g_t(s_t,a_t,e_t)$  were known and linear and the rewards quadratic this problem could not be solved by traditional LQ-regulators due to the state constraint~\cite{aastrom2007feedback}. The problem is further complicated by the fact that the model for the system's dynamics is usually not known~\cite{recht2019}. Learning based methods like RL are usually the only feasible approaches for such problems.


% If system dynamics $g(\cdot)$ are unknown, as is often the case, then learning based approaches like RL are needed to solve the above problem~\cite{recht2019}. 
% This problem is further complicated due to the constraints on the states. Even if the dynamics are known and linear and the rewards quadratic this problem could not be solved by traditional linear-quadratic regulator due to the constraints on the states $s_t$. 
 %The problem is further complicated by the fact that the model for the system's dynamics is usually not known~\cite{recht2019}. Then learning based RL methods are usually the only feasible approaches to solve such problems.
 
% , even if the dynamics were known the problem would still be challenging due to the constraints on the state. 
% The problem is challenging since the model for the system's dynamics is usually not known~\cite{recht2019}. Then learning based RL methods are usually the only feasible approaches to solve such problems. However, even if the
%
%
%However, due to the constraints on the states, this problem cannot be solved with traditional Linear‚Äìquadratic regulator due to the constraints~\cite{aastrom2007feedback}. Therefore, even if the model $g_t(s_t,a_t,e_t)$ would be know, this would be extremely difficult problem. The problem is further complicated by the fact that the model for the system's dynamics is usually not known~\cite{recht2019}. Then learning based RL methods are usually the only feasible approaches to solve such problems.

 In RL, constraints are often included in the reward (i.e., as the main goal $M$) by having a penalty for  violating them.
% 
 This means that the RL agent needs to learn simultaneously how to satisfy the constraint while it is learning to optimize the rewards. However, learning to satisfy the constraints is a much easier task because the agent has more freedom of selection among various actions, which leads to satisfying constraints. Thus it is intuitive to use constraints satisfaction as the auxiliary goal $A$ in our framework.  
%
%In RL-based approaches constraints are usually included in the objective and together with $f_t$ collectively it is considered as the main goal $M$. Consequently, a target reward $R^T$ is assigned to it, which includes a large penalty for not satisfying the constraint as well as a reward commensurate with the $f_t$. The point is that by doing so, the complexity of the problem for the agent increases because the agent is faced with a somehow multi-objective problem. \textcolor{red}{It is complicated because the agent often just gets a high penelty for being infeasible. This is not useful, since the agent does not.}
%
%Instead we can use our framework. To do this, we have a simple mission ahead of us. We consider the auxiliary goal $A$ of imposing constraints, and then we assume a 

We can achieve  this by using a negative assistant reward $R^A$. Since the $R^A$ is actually part of the target reward $R^T$, there is a high alignment between the two rewards. As a result, the $\beta(e)$ function can be selected more easily. Our selection is a descending linear function\footnote{i.e., 
%$\beta(e) = \max[(E-e)\beta(0)/E,0]$
$\beta(e) = [(E-e)\beta(0)/E]_+$
} that starts at $\beta(0)$ and goes to zero by $e=E$, where it leads to a sequence of $E$ MDPs, each of which is solved for one episode.
%
%In the following, given these definitions for $R^A$ and $R^T$, we examine the performance of \texttt{TA-Explore} in solving the problem of optimal temperature control with constraint \cite{recht2019}. Finally, we go one step further and solve a nonlinear control problem \cite{GOUTA2017280}.
%\footnote{The source code is  available at \url{https://anonymous.4open.science/r/D2A3/}}
\footnote{The source code is  available at \url{https://github.com/AliBeikmohammadi/TA-Explore}}
%\footnote{Code is available as supplementary material, and we will make it publicly available if the paper is accepted.}

\subsection{Optimal Temperature Control with Constraint - Linear Dynamics} \label{section5.1}
Consider a data center cooling where three heat sources are coupled to their own cooling devices.
Each component of the state $s$ is the internal temperature of one heat source, which should be maintained in the range of -2 to 2 (i.e., we have constraint on states such that $-2 \leqslant s_t \leqslant 2$).
Under constant load, the sources heat up and radiate heat into the surrounding environment.  
The voltage of each cooler is known as $a$ and the objective is to minimize it while satisfying the constraint. It can be approximated by the linear dynamical system $s_{t+1}=As_t+Ba_t+e_t$ where,
\begin{displaymath}
A=
  \begin{bmatrix}
1.01 & 0.01 & 0\\
0.01 & 1.01 & 0.01\\
0 & 0.01 & 1.01
\end{bmatrix}~~~~
, ~~~~ B=I,
\end{displaymath}
and where $e_t$ is a noise with zero mean with covariance $10^{-4}I$. The voltage of each cooler is  the action and the goal is to minimize the power (i.e., $\omega||a||^2$) while satisfying the constraint \cite{recht2019}. 

To put it in our framework we define the target reward $R^T$  and the assistant reward $R^A$  as follows:
\begin{equation} \label{rt}
  R^T =
    \begin{cases}
      -\omega \Vert a\Vert ^2 & \text{if the constraint is satisfied}\\%\text{if } s\in[-2,2]
      -\omega \Vert a\Vert ^2 -100 & \text{otherwise}
    \end{cases}       
\end{equation}
\begin{equation} \label{ra}
  R^A =
    \begin{cases}
      0 & \text{if the constraint is satisfied}\\%\text{if } s\in[-2,2]
      -100 & \text{otherwise}
    \end{cases}       
\end{equation}
where $\omega$ is the weight the first term, that we examine its impact later.

The time horizon is 100 steps, and the number of training episodes is 8000. To make the problem more challenging, we allow the states to take any initial value according to the standard normal distribution. Also, we do not terminate the episode by not satisfying the constraint (i.e., allow the training to continue for 100 time-steps in each episode). For $\beta(e)$ described at the beginning of this section, we set $E=4000$ and $\beta(0)=1$, which means that after 4,000 episodes, the objective function will be the same as the main goal $M$.

To train the agent, since the state and action spaces are continuous, we select PPO, a deep-RL approach, as the backbone, which is one of the most successful policy-based algorithms \cite{schulman2017proximal}. This algorithm is highly efficient, and therefore improving its results is a challenge that we are looking for. 
As for PPO setup, actor and critic models are updated using mini-batches of 512 samples, 10 training epochs, and the Adam optimizer \cite{kingma2014adam} (learning rate 0.00025). 
In addition, the discount and lambda factors are 0.99, and 0.9, respectively.
For both actor and critic models, the input is fed
into three layers with ReLU activation and 512, 256 and 64 units, respectively. 
The output layer of the actor and critic models have, respectively, 3 and 1 neurons and we use, respectively, the tanh and identity activation functions.

%, which has 3 neurons, use tanh activation function, while the critic model has one neuron with the identity activation function.


In this example, transfer learning is done through policy transfer by using the weights obtained in each episode for each task as the initial weighting of the neural network used as an approximation function of the policy of the next task. 
Figure \ref{fig4} shows the performance of using PPO with and without \texttt{TA-Explore} framework. While using our framework, we still use the same hyperparameters as used for the baseline, which can facilitate employing our method on any algorithm without any tuning need.  We measure the performance by plotting the average reward $R^T$. 
%
%the using our framework compared to 
%The measurement of effectiveness is the average reward $R^T$ gained by the agent, regardless of whether it is trained directly on it (Baseline) or trained according to Equation \ref{eq} (\texttt{TA-Explore}). 
This criterion is natural since  the agent only wants to learn the main goal $M$, the auxiliary goal $A$ is just used to facilitate learning. So learning the auxiliary goal $A$ in itself does not matter to us unless it helps the agent to converge faster by transferring prior knowledge to it.

As shown in Figure \ref{fig4.a}, \texttt{TA-Explore} converges more than twice as fast. The reason for this speed of convergence is simply the learning of the assistant reward $R^A$ considered in our proposed method during initial episodes on the one hand, and on the other hand, the confusion of agent when considering a complicated reward, where the agent does not know whether the reward is due to the violation of the constraints or being far from the main goal. 

\begin{figure}[h] %[H]
     \centering
     \begin{subfigure}{1\linewidth}
         \centering
         \includegraphics[width=\linewidth]{fig10_1.png}
         \caption{}
         \label{fig4.a}
     \end{subfigure}
     \begin{subfigure}{1\linewidth}
         \centering
         \includegraphics[width=\linewidth]{fig10_2.png}
         \caption{}
         \label{fig4.b}
     \end{subfigure}
     \begin{subfigure}{1\linewidth}
         \centering
         \includegraphics[width=\linewidth]{fig10_3.png}
         \caption{}
         \label{fig4.c}
     \end{subfigure}
        \caption{Average performance on optimal temperate control with constraint for different weighting values to the control objective (i.e., (a) $1\Vert a\Vert ^2$ , (b) $10\Vert a\Vert ^2$, (c) $100\Vert a\Vert ^2$). To have a clear illustration, 50 episodes moving average reward are plotted.}
       %\caption{Average performance of \texttt{TA-Explore} on optimal temperate control with constraint for different weighting values to the control objective (i.e., (a) $1\Vert a\Vert ^2$ , (b) $10\Vert a\Vert ^2$, (c) $100\Vert a\Vert ^2$). The performance measure shown is the amount of the target reward $R^T$ achieved by the agent in each episode, regardless of whether the agent is learning according to $R^T$ or a combination of $R^T$ with the assistant reward $R^A$ according to Equation \ref{eq}. In all cases, the PPO algorithm is used as the backbone with the same setup. To have a clear illustration, 50 episodes moving average reward are plotted. As it turns out, \texttt{TA-Explore}, in all cases, converges very fast. But, the baseline converges too late and in (c) does not converge during 8000 episodes.}
        \Description{des4}
        \label{fig4}
\end{figure}

\iffalse
\begin{figure*}[t]
     \centering
     \begin{subfigure}{.30\linewidth}
         \centering
         \includegraphics[width=\linewidth]{fig10_1n.png}
         \caption{}
         \label{fig4.a}
     \end{subfigure}
     \begin{subfigure}{.30\linewidth}
         \centering
         \includegraphics[width=\linewidth]{fig10_2n.png}
         \caption{}
         \label{fig4.b}
     \end{subfigure}
     \begin{subfigure}{.30\linewidth}
         \centering
         \includegraphics[width=\linewidth]{fig10_3n.png}
         \caption{}
         \label{fig4.c}
     \end{subfigure}
        \caption{Average performance on optimal temperate control with constraint for different weighting values to the control objective (i.e., (a) $1\Vert a\Vert ^2$ , (b) $10\Vert a\Vert ^2$, (c) $100\Vert a\Vert ^2$). To have a clear illustration, 50 episodes moving average reward are plotted.}
       % \caption{Average performance of \texttt{TA-Explore} on optimal temperate control with constraint for different weighting values to the control objective (i.e., (a) $1\Vert a\Vert ^2$ , (b) $10\Vert a\Vert ^2$, (c) $100\Vert a\Vert ^2$). The performance measure shown is the amount of the target reward $R^T$ achieved by the agent in each episode, regardless of whether the agent is learning according to $R^T$ or a combination of $R^T$ with the assistant reward $R^A$ according to Equation \ref{eq}. In all cases, the PPO algorithm is used as the backbone with the same setup. To have a clear illustration, 50 episodes moving average reward are plotted. As it turns out, \texttt{TA-Explore}, in all cases, converges very fast. But, the baseline converges too late and in (c) does not converge during 8000 episodes.}
        \label{fig4}
\end{figure*}
\fi

By giving more weight to the first term of $R^T$, we lose more information in $R^A$ than in $R^T$. Hence, the alignment between them are reduced and the problem becomes more challenging.
However, as shown in Figures \ref{fig4.b} and \ref{fig4.c}, \texttt{TA-Explore} converge well over almost the same number of episodes. On the contrary, the baseline is acquired bad results so that it has not converge in 8,000 episodes in Figure \ref{fig4.c}.
So it can be concluded again that the more complex the problem, the more and more the area enclosed between the two curves, and we converge faster than the baseline. Note that, although here we showed that learning the constraints and gradually considering the main problem greatly contribute to the convergence, one can even further improve it by skillfully defining auxiliary goal considering their domain knowledge.

\subsection{A Coupled Four Tank MIMO System - Nonlinear Dynamics}

\begin{figure}[h]
  \centering
  \includegraphics[width=0.9\linewidth]{fig11.png}
  \caption{Coupled four tank MIMO system \cite{GOUTA2017280}; known as a nonlinear optimal control problem with constraints.}
  \Description{des5}
  \label{fig5}
\end{figure}

We also study the performance on a coupled four water tank system with nonlinear dynamics \cite{GOUTA2017280}.
Figure \ref{fig5} illustrates how this system is comprised of a liquid basin, two pumps, four tanks with the same area with orifices. The dynamics of the systems are given by:
\begin{align*}
%
\begin{cases}
s_{t+1}^1 =&-c_1\sqrt{s_t^1}+c_2\sqrt{s_t^3}+c_3\sqrt{s_t^4} \\
s_{t+1}^2 =&-c_4\sqrt{s_t^2}+c_5\sqrt{s_t^3}+c_6\sqrt{s_t^4} \\
s_{t+1}^3 =& -c_7\sqrt{s_t^3}+c_8 a_t^1 \\
s_{t+1}^4 =& -c_9\sqrt{s_t^4}+c_{10} a_t^2
%
\end{cases}   
\end{align*}
where $c_1, ..., c_{10}$ are model parameters that are set as in \cite{GOUTA2017280}. The actions are $a^1$ and $a^2$ which indicate  the voltages applied, respectively, to Pump 1 and Pump 2, which are  bounded between 0 and 12Volts. The states $s^1$, $s^2$, $s^3$, and $s^4$ indicate  the liquid levels in the four tanks. They should always be greater than 3cm and less than 30cm (i.e., we have constraint both on states and actions such that $3 \leqslant s_t \leqslant 30$,  $0 \leqslant a_t \leqslant 12$).

It is clear that this problem can easily be included in our proposed framework with the same rewards mentioned in Equations \ref{rt} and \ref{ra} ($\omega =1$).
 %Since there are many similarities to the previous problem, 
 We use \texttt{TA-Explore} with 
%$\beta(e) = \max[(E-e)\beta(0)/E,0]$
$\beta(e) = [(E-e)\beta(0)/E]_+$
 where $\beta(0)=0.5$ and $E = 3000$. 
%In addition, due to the presence of root in the dynamic of the system, it is not possible to negatively initialize the states. Hence, 
The initial state values are produced in the range of allowed states (i.e., between 3 and 30) with a uniform distribution. We also terminate the episode if the constraints are not met. The rest of the setups are the same as in Section \ref{section5.1}, except that it has assumed to take 30,000 episodes to train because of being very difficult to learn.
In this experiment, PPO with the same settings as in Section \ref{section5.1} is used as the backbone, except the number of actor model output neurons which is 2. To ensure the feasibility of the actions is easy, since the output of the actor model is limited due to the  tanh activation function which can be scaled between 0 to 12.

%The important point is that since the output of the actor model is limited due to the use of the tanh activation function between -1 and 1, by scaling it to 0 to 12, the agent can select any desired allowable voltage. And more importantly, always make us sure that the constraints on the actions are satisfied.
Figure \ref{fig6} shows a comparison between the proposed method and the baseline, where regardless of the complexity of the problem, and the relatively poor $\beta(e)$ selection, \texttt{TA-Explore} has still converged more than $30\%$ faster to the target reward. Considering $\beta(0)=0.5$ means that even in the first episode our method pays attention to minimizing the voltage while satisfying the constraint. But compared to the target reward $R^T$, it gives it a weight equal to half.
%
%This shows that \texttt{TA-Explore}  has performed exceptionally well in a variety of experiments with varying levels of difficulty we have conducted. Hence, it can be of great help to lovers of reinforcement learning by enhancing its minor limitation that is addressed in the next section.

\begin{figure}[h]
  \centering
  \includegraphics[width=1\linewidth]{fig10_4.png}
  \caption{Performance on coupled four tank MIMO system. To have a clear illustration, 50 episodes moving average reward are plotted.}
  %\caption{Average performance of \texttt{TA-Explore} on coupled four tank MIMO system. As shown, \texttt{TA-Explore} converges about $30\%$ faster than baseline. The performance measure shown is the amount of the target reward $R^T$ achieved by the agent in each episode. In both cases, the PPO algorithm is used as the backbone. Note that, 50 episodes moving average reward are plotted to have a clear illustration.}
  \Description{des6}
  \label{fig6}
\end{figure}

\section{Conclusion and Future Work} \label{section6}
Sample inefficiency is a critical weakness of current developments in reinforcement learning. RL algorithms must become more efficient if they are to achieve their full potential in many real-world applications. In this paper, we introduce a new framework \texttt{TA-Explore} for smart exploration. The main idea is to use simpler assistant tasks that are aligned  with the main task that gradually progress towards the main difficult task. Thus helping the agent in finding a more efficient learning trajectory.  
The \texttt{TA-Explore} framework is extremely flexible, it can be applied to any RL task, any type of environment. In addition, any type of RL algorithm can be used as its backbone. 
We conducted comprehensive experiments on a wide range of tasks with different difficulties, with different RL algorithms -both value-based methods and policy-based methods and both tabular methods and deep-RL methods.
The results show the excellent performance of the proposed method in increasing the convergence speed.
Moreover, \texttt{TA-Explore} has no additional computational cost and complexity.
Therefore, our proposed framework, which owes its superiority to the definition of the teacher-assisted goal, can impact the field of RL and make researchers think about a better definition of the reward, relaxing limiting assumptions, and no need to complete learning of the prior tasks before the transfer learning process. 
Proving the sample complexity improvement in such a setting is potentially future work.
%
Also, for future work, adding a self-tuning feature to the $\beta$ function can go a long way in overcoming the only current limitation of the proposed method, which is to select the $\beta$ function experimentally. On the other hand, examining its performance in multi-agent environments under the partially observable Markov decision process (POMDP) assumption, which have higher complexity and suffer from slow learning of the objective function, can be considered a valuable task.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% The acknowledgments section is defined using the "acks" environment
%%% (rather than an unnumbered section). The use of this environment 
%%% ensures the proper identification of the section in the article 
%%% metadata as well as the consistent spelling of the heading.

\begin{acks}
This work was partially supported by the Swedish Research Council through grant 2020-03607 and in part by Digital Futures and the C3.ai Digital Transformation Institute. 
The computations were enabled by resources in project SNIC 2022/22-942 provided by the Swedish National Infrastructure for Computing (SNIC) at Chalmers Centre for Computational Science and Engineering (C3SE) partially funded by the Swedish Research Council through grant agreement no. 2018-05973.
\end{acks}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% The next two lines define, first, the bibliography style to be 
%%% applied, and, second, the bibliography file to be used.

\bibliographystyle{ACM-Reference-Format} 
%%% The following command should be issued somewhere in the first column 
%%% of the final page of your paper.
\balance
\bibliography{sample}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\end{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

