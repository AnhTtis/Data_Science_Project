\documentclass[letterpaper, 10 pt, conference]{ieeeconf}  % Comment this line out if you need a4paper

%\documentclass[a4paper, 10pt, conference]{ieeeconf}      % Use this line for a4 paper

\IEEEoverridecommandlockouts  % This command is only needed if 
               % you want to use the \thanks command

\overrideIEEEmargins % Needed to meet printer requirements.

\usepackage{amsmath,amsfonts}
\usepackage{algorithmic}
\usepackage{array}
\usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
\usepackage{textcomp}
\usepackage{stfloats}
\usepackage{url}
\usepackage{verbatim}
\usepackage{graphicx}
\hyphenation{op-tical net-works semi-conduc-tor IEEE-Xplore}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\usepackage{balance}

\usepackage{cite}
\usepackage{nicefrac}
\usepackage{bm}
%\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{mathtools}
\usepackage[colorlinks=true, allcolors=blue]{hyperref}
\usepackage{bbm}
\usepackage{color}
\usepackage{caption}
\usepackage{algorithm}
\usepackage[pdftex,dvipsnames]{xcolor}  % Coloured text etc.
\usepackage[colorinlistoftodos,prependcaption]{todonotes}
\usepackage{xcolor}
\usepackage{wrapfig} 


\title{ \LARGE \bf Human-Inspired Framework to Accelerate Reinforcement Learning}

\iffalse
\author{Ali Beikmohammadi
%\IEEEmembership{Fellow, IEEE}
and Sindri Magn\'usson 
%\IEEEmembership{Member, IEEE}
\thanks{A. Beikmohammadi and S. Magn\'usson are with the Department of Computer and System Science, Stockholm University, 11419 Stockholm, Sweden (e-mail: beikmohammadi@dsv.su.se; sindri.magnusson@dsv.su.se).}}

\author{Albert Author$^{1}$ and Bernard D. Researcher$^{2}$% <-this % stops a space
\thanks{*This work was not supported by any organization}% <-this % stops a space
\thanks{$^{1}$Albert Author is with Faculty of Electrical Engineering, Mathematics and Computer Science,
        University of Twente, 7500 AE Enschede, The Netherlands
        {\tt\small albert.author@papercept.net}}%
\thanks{$^{2}$Bernard D. Researcheris with the Department of Electrical Engineering, Wright State University,
        Dayton, OH 45435, USA
        {\tt\small b.d.researcher@ieee.org}}%
}
\fi

\author{Ali Beikmohammadi$^{1}$ and Sindri Magn\'usson$^{2}$% <-this % stops a space
\thanks{A. Beikmohammadi and S. Magn\'usson are with the Department of Computer and System Science, Stockholm University, 11419 Stockholm, Sweden
        {\tt\small $^{1}$beikmohammadi@dsv.su.se; $^{2}$sindri.magnusson@dsv.su.se}}%
}

\begin{document}

\maketitle
\thispagestyle{empty}
\pagestyle{empty}

\begin{abstract}
Reinforcement learning (RL) is crucial for data science decision-making but suffers from sample inefficiency, particularly in real-world scenarios with costly physical interactions. This paper introduces a novel human-inspired framework to enhance RL algorithm sample efficiency. It achieves this by initially exposing the learning agent to simpler tasks that progressively increase in complexity, ultimately leading to the main task. This method requires no pre-training and involves learning simpler tasks for just one iteration. The resulting knowledge can facilitate various transfer learning approaches, such as value and policy transfer, without increasing computational complexity. It can be applied across different goals, environments, and RL algorithms, including value-based, policy-based, tabular, and deep RL methods. Experimental evaluations demonstrate the framework's effectiveness in enhancing sample efficiency, especially in challenging main tasks, demonstrated through both a simple Random Walk and more complex optimal control problems with constraints.
%While reinforcement learning (RL) is becoming an integral part of good decision-making in data science, it is still plagued with sample inefficiency. This can be challenging when applying deep RL in real-world environments where physical interactions are expensive.
%%and can risk system safety. 
%To improve the sample efficiency of RL algorithms, this paper proposes a novel human-inspired framework that facilitates fast exploration and learning for difficult RL tasks. The main idea is to first provide the learning agent with simpler but similar tasks that gradually grow in difficulty and progress toward the main task. The proposed method requires no pre-training phase. Specifically, the learning of simpler tasks is only done for one iteration. The generated knowledge could be used by any transfer learning, including value transfer and policy transfer, to reduce the sample complexity while not adding to the computational complexity. So, it can be applied to any goal, environment, and RL algorithm - both value-based methods and policy-based methods and both tabular methods and deep-RL methods.   We have evaluated our proposed framework on both a simple Random Walk for illustration purposes and on more challenging optimal control problems with constraint. The experiments show the good performance of our proposed framework in improving the sample efficiency of RL-learning algorithms, especially when the main task is difficult.
\end{abstract}

\section{Introduction}
Intelligent and data-driven decision-making is becoming an increasingly important part of data science. The subfield of AI concerned with how intelligent agents learn to make optimal decisions in an unknown environment from data and experience is RL. The area has flourished recently by building on advancements in deep learning that have given RL agents superb generalization abilities~\cite{mnih2015}. Deep-RL has had a huge impact in multiple areas where it has solved challenging problems, e.g., in game playing \cite{silver2017}, 
%\cite{silver2016, silver2017}, 
financial markets \cite{meng2019}, 
%\cite{meng2019, fischer2018, 10.1145/3383455.3422540}, 
robotic control \cite{ kober2013}, 
%\cite{ kober2013, lillicrap2015},
optimal control \cite{126844}, 
%\cite{126844, PERRUSQUIA2021145}, 
%healthcare\cite{CORONATO2020101964, 10.1145/3477600}, 
and autonomous driving \cite{shalev2016}.
%\cite{shalev2016, ref4}.
%, and recommendation systems~\cite{rec1,rec2, ref3}. 

%A central challenge hindering RL from achieving its full potential in many application areas is that RL approaches generally need a lot of samples to learn good decision policies~\cite{10.1007/BF00992698, sutton1999, yu2018towards, sung2017learning}.  In practice, this means that the agent needs many interactions with the environment to learn, which is often expensive.  This is especially challenging when the agents 
 %%do not have access to the simulation environment but 
% interact directly with a real-world environment.  For example, in cyber-physical systems, medical applications, or in robotic systems, it is not only costly to spend much time exploring the environment, but it can also be downright dangerous to learn slowly~\cite{Cheng_Orosz_Murray_Burdick_2019,NEURIPS2020_8df6a659, NEURIPS2020_448d5eda}. Finding approaches to facilitate efficient exploration and learning is a central challenge in RL. 

A central challenge hindering RL from achieving its full potential in many application areas is that RL approaches generally need a lot of samples to learn good decision policies~\cite{10.1007/BF00992698, yu2018towards, sung2017learning}.  
%~\cite{10.1007/BF00992698, sutton1999, yu2018towards, sung2017learning}.  
This translates to costly and time-consuming interactions with the environment, particularly in real-world scenarios like cyber-physical systems, medical applications, and robotics~\cite{NEURIPS2020_8df6a659, NEURIPS2020_448d5eda}. %~\cite{Cheng_Orosz_Murray_Burdick_2019,NEURIPS2020_8df6a659, NEURIPS2020_448d5eda}. 
The quest to enhance exploration and learning efficiency remains a key challenge in RL.

Most existing RL algorithms start the learning process from scratch. In particular, the agent starts exploring each environment without using any prior knowledge (there are notable exceptions; see Section \ref{section2}). This is fundamentally different from how humans explore and learn in new environments~\cite{GOTTLIEB2013585}. 
%When faced with difficult and nested tasks, humans try to learn them step by step by considering several short-term and auxiliary goals that are easy to achieve but are at the same time aligned with the main goal. To better understand this process, consider a math teacher and student. To facilitate learning, the teacher must aid the student by providing her with tasks of gradually increasing difficulty. For example, the teacher will first teach the student addition, then multiplication, before finally teaching her exponents. It would be extremely difficult for the student to learn exponents directly without the side steps of learning addition and multiplication first.
%Similarly, when an RL agent learns a difficult task, it can be challenging and sample inefficient to take on the main goal directly. In the spirit of human learning, it might often be more likely to succeed first to solve simpler, similar tasks that gradually increase in difficulty and progress towards the main tasks. 
When tackling complex tasks, humans adopt a step-by-step approach. They break down the overarching goal into manageable short-term objectives that align with the main goal. To illustrate, consider a math teacher guiding a student. The teacher starts with basic addition, then moves to multiplication, and eventually introduces exponents. This incremental approach enhances comprehension. Similarly, RL agents facing tough challenges might benefit from mastering simpler, related tasks before tackling the primary objective, mirroring human learning principles.

\textbf{Contribution:}
% The main goal of this paper is to provide a framework to allow such Teacher-Assisted exploration in RL. In particular, we provide a systematic approach for facilitating the learning of an RL agent by providing it with simpler auxiliary goals that gradually become more difficult and converge to the main goal. Thus allowing the agent to learn its main goal faster. The auxiliary goals are applied by defining one assistant reward alongside the target reward together with an annealing function to make associated Markov decision processes (MDPs) in sequence. In this process, the agent slowly shifts between learning these goals by learning each for just one iteration and uses the knowledge gained by sequences of auxiliary goals when learning the main task. So, in this approach, there is no need to define many MDPs separately that follow the same distribution.  Our framework is algorithm-agnostic; it can be used together with any algorithm, both value-based and policy-based methods, and both tabular methods and deep-RL methods since it supports both value transfer and policy transfer. We illustrate the promise of the approach in simple, intuitive settings using tabular algorithms. We also illustrate the efficiency of our proposed framework by extensive experiments using deep-RL on two real-world problems (see Section \ref{section5}). The experiments indicate that our proposed method is effective in speeding up learning, especially when the main task is difficult, without increasing computational complexity. 
%Our code is available as supplementary material, and we will make it publicly available if the paper is accepted.
%The code for the experiments is publicly available in the following anonymous link: \url{https://anonymous.4open.science/r/TA-Explore2023}.
%The code for the experiments is publicly available in the following link: \url{https://github.com/AliBeikmohammadi/TA-Explore}.
This paper introduces a framework for Teacher-Assisted exploration in RL. It systematically aids the RL agent's learning process by providing progressively challenging auxiliary goals alongside the main goal. These auxiliary goals are incorporated by introducing an assistant reward in conjunction with the target reward, managed through an annealing function, creating a sequential progression of Markov decision processes (MDPs). The agent alternates between learning each auxiliary goal for a single iteration, leveraging the acquired knowledge to expedite mastery of the main goal. This approach eliminates the need for defining numerous separate MDPs with similar distributions. Importantly, our framework is algorithm-agnostic and compatible with various RL methods, including value-based, policy-based, tabular, and deep-RL approaches. We demonstrate its effectiveness in intuitive scenarios using tabular algorithms and validate its efficiency through extensive deep-RL experiments on control problems (refer to Section \ref{section5}).
The results indicate that our method accelerates learning, particularly in challenging tasks, without increasing computational complexity.
Our code is available in \url{https://github.com/AliBeikmohammadi/TA-Explore}.
%Our code is available as supplementary material, and we will make it publicly available if the paper is accepted.
%The code for the experiments is publicly available in the following anonymous link: \url{https://anonymous.4open.science/r/TA-Explore2023}.
%The code for the experiments is publicly available in the following link: \url{https://github.com/AliBeikmohammadi/TA-Explore}.

\textbf{Limitation:}
%We assume that there exists a possibility of defining an auxiliary goal and, consequently, an assistant reward for the considered task. Such an assumption might not exist for every task. However, surprisingly often, it is possible to build simple, intuitive assistant rewards that can be used to guide the learning process. For example, in many real-world control problems, one seeks to satisfy a series of constraints while optimizing the main objective. We have shown by providing two examples given in Section \ref{section5} that always learning to satisfy constraints as an auxiliary goal and then learning the main goal is very effective in learning speed and sample efficiency. Also, we illustrate the process of defining auxiliary goals and rewards in detail in Section \ref{section4} in order to diminish our approach limitation by showing the simplicity of defining the auxiliary goals and rewards.
We assume that it's possible to define auxiliary goals and corresponding assistant rewards for certain tasks. While not applicable to every task, surprisingly often, we can create %straightforward and 
intuitive assistant rewards to guide the learning process. For instance, in many real-world control problems, the objective is to meet various constraints while optimizing the main goal. Our examples in Section \ref{section5} demonstrate that first learning to satisfy constraints as an auxiliary goal before pursuing the main goal is highly effective in terms of learning speed and sample efficiency. Furthermore, we provide a detailed explanation of the process of defining auxiliary goals and rewards in Section \ref{section4} to illustrate the simplicity of our approach and overcome its limitations.

%The rest of this paper is organized as follows. 
%We introduce our proposed framework in Section \ref{section3}. Preliminary and extended experimental results and analyses are reported in Sections \ref{section4} and \ref{section5}, respectively. 
%In Section \ref{section2}, we  review related work. 
%We conclude  and discuss future works in Section \ref{section6}.

\section{Our Framework: \texttt{TA-Explore}}
\label{section3}
%In this section, we first describe the problem formulation in Section \ref{Sec:RL} and our novel \texttt{TA-Explore} framework in Section \ref{Sec:TA}.

\subsection{Reinforcement Learning}%Problem Formulation: MDP}
\label{Sec:RL}
We consider RL in a MDP. Formally, an MDP is characterized by a 5-tuple $(S,A,P,R^T,\gamma)$ where $S$ denotes the set of states; $A$ denotes the set of actions; $P : S \times A \rightarrow \Delta(S)$ denotes the transition probability
%\footnote{i.e., $P(s'|s,a) = \mathsf{Pr}[s_{t+1} = s' | s_t=s,a_t=a]$.} 
from state $s \in S$ to state $s' \in S$ when the action $a \in A$ is taken; $R^T : S \times A \times S \rightarrow \mathbb{R}$ is the immediate reward
%\footnote{Here we use the superscript T on the reward to distinguish it from the assistant reward defined later.} 
received by the agent after transitioning from $(s,a)$ to $s'$;  $\gamma \in[0,1)$ is the discount factor that trades off the instantaneous and future rewards. The $\gamma$ controls how much weight is put on old  rewards compared to new ones, i.e., small $\gamma$ means that we put a higher priority on recent rewards. 

We consider episodic tasks. % where the episodes are indexed by $e\in \mathbb{N}$. 
 At the beginning of each episode, the agent starts in an initial state $s_0\in S$ which is an IID sample from the distribution $\mu$ on $S$. 
After that, at each time step $t\in \mathbb{N}$, the agent takes action $a_t$ which leads the system to transfer to $s_{t+1} \sim P(\cdot|s_t,a_t)$. The agent receives an instantaneous reward $R^T(s_t,a_t,s_{t+1})$. The agent makes the decision by following a parameterized policy $\pi : S \times \Theta \rightarrow \Delta(A)$, a mapping from the state space $S$ to a distribution over the action space $A$. In particular, we have $a_t \sim \pi(\cdot|s_t; \theta)$ where $\theta \in \Theta$ is an adjustable parameter. The goal of the agent is to find the policy  $\pi(\cdot|s_t; \theta)$ by tuning the parameter $\theta$
%that optimizes the cumulative reward and 
to solve the optimization problem
\begin{equation} \label{problem:M}
\small
  \max_{\theta\in \Theta} M(\theta):=\mathbb{E}\bigg[\sum_{t= 0}^{H} \gamma^t R^T(s_t,a_t,s_{t+1})\Big|a_t \sim \pi(.|s_t,\theta),s_0\sim \mu\bigg],
\end{equation}
where $H$ is the termination time.
%\footnote{Note that $H$ is generally a random variable parametrized by $\theta$.}
We use the notation $M(\theta)$ to indicate that optimizing the above cumulative reward is the main goal of the agent. 
%In particular, the agent seeks to find the optimal solution to the following optimization problem
%\begin{equation} \label{problem:M}
%    \max_{\theta\in \Theta} ~M(\theta).
%\end{equation}

%The goal of RL is to learn the optimal parameter $\theta$ by trial and error, where the agent continually interacts with the environment. Learning to optimize the main goal is often difficult. It is not uncommon that it takes thousands or even millions of interactions to find good policies. Next, we illustrate our main idea, how we can facilitate more efficient learning by using assistant rewards.  

RL aims to find the optimal $\theta$ through trial and error, with the agent constantly engaging the environment. Learning to optimize the main goal can be challenging, often requiring thousands or even millions of interactions to discover effective policies. 
%Next, we demonstrate our key concept: enhancing learning efficiency through assistant rewards.

\subsection{\texttt{TA-Explore}}
\label{Sec:TA}
%Just like with human learning, it is reasonable first to have the agent solve simple tasks that are similar to the difficult main goal $M(\theta)$. Then, gradually increase the difficulty of the simple tasks so they eventually converge to the main goal. 
Similar to human learning, the agent should begin by tackling easy tasks resembling the challenging main goal $M(\theta)$. As proficiency grows, progressively intensify the simplicity of these tasks until they align with the main objective.
Formally, we may consider some auxiliary goal
\begin{equation*}
A(\theta) =  \mathbb{E}\bigg[\sum_{t= 0}^{H} \gamma^t R^A(s_t,a_t,s_{t+1})\Big|a_t \sim \pi(.|s_t,\theta),s_0\sim \mu \bigg],
\end{equation*}
 where $R^A$ is an assistant reward.
 %Then the auxiliary goal is to optimize the following problem
%  \begin{equation}
%\max_{\theta \in \Theta} ~A(\theta).
%\end{equation}
Then the auxiliary goal is to optimize $\max_{\theta \in \Theta} ~A(\theta).$
 
Ideally, we should choose $R^A$ in a way that a) it results in a simple RL problem that can be solved fast, and b) solving it is a side-step towards the main goal. We illustrate examples of such assistant rewards
%that can truly make learning faster 
in the next 
%two 
sections. 
%The goal of the agent is not to learn the auxiliary goal but rather to use it to facilitate learning. Thus, it is important that the agent can gradually put more effort into the main goal $M(\theta)$. For example, the agent might gradually increase its focus after finishing each episode. To that end, let $\beta(e)$ denote the parameter that controls how quickly the agents progress towards the main goal, where $e\in \mathbb{N}$ is the episode index. Then, during episode $e$, the agent uses the immediate reward
The agent's primary objective is not mastering these auxiliary goals; rather, it should leverage them to expedite progress towards $M(\theta)$.
%To ensure this, the agent should gradually shift its focus towards $M(\theta)$. 
This transition is controlled by the parameter $\beta(e)$, where $e\in \mathbb{N}$ represents the episode index. During episode $e$, the agent employs the immediate reward
%\footnote{We can also apply this idea on continuous tasks with infinite horizon, but then $\beta(\cdot)$ should be a function of $t$.} 
\begin{align*}\label{eq}
R_e(s_t,a_t,s_{t+1}) \hspace{0.3cm}{=} \hspace{0.3cm} & \beta(e)R^A(s_t,a_t,s_{t+1}) {+} \\
 & (1{-}\beta(e))R^T(s_t,a_t,s_{t+1}).
\end{align*}
 
 The agent then tries to optimize the following teacher-assisted goal at episode $e$
 \begin{displaymath}
     \texttt{TA}(\theta;e)=\mathbb{E}\bigg[\sum_{t= 0}^{H} \gamma^t R_e(s_t,a_t,s_{t+1})\Big|a_t \sim \pi(.|s_t,\theta),s_0\sim \mu \bigg],
 \end{displaymath}
 by finding  the solution to the optimization problem \begin{equation}\label{problem:TA}
\max_{\theta \in \Theta} \texttt{TA}(\theta; e).
\end{equation}
%as $e$ goes to infinity. 
When examining $\texttt{TA}(\theta; e)$, we observe that despite having only one assistant reward, multiple teacher-assisted goals are generated due to the presence of the function $\beta(\cdot)$. These goals correspond to several artificial tasks represented as MDPs.
It is noteworthy that if $\beta(e)$ gradually diminishes to zero, $\theta_{M}^{\star}=\theta_{\texttt{TA}}^{\star}$, where $\theta_{M}^{\star}$ and $\theta_{\texttt{TA}}^{\star}$ are, respectively, the solutions to~\eqref{problem:M} and~\eqref{problem:TA} as $e$ goes to infinity. This convergence occurs because the influence of the assistant reward diminishes as $e$ increases, solving the \texttt{TA}-goal becomes equivalent to solving the main goal.
%However, when the assistant reward is strategically designed, it has the potential to enhance the sample efficiency of any RL algorithm. We elaborate on this enhancement in the following two sections.

%By examining $\texttt{TA}(\theta; e)$, it can be found that now, despite having only one assistant reward, thanks to the $\beta(\cdot)$, until the $\beta(\cdot)$ goes zero, several teacher assisted goals are generated, which indicate several artificial tasks (MDPs).
%Note that if $\beta(e)$ decays to zero, then $\theta_{M}^{\star}=\theta_{\texttt{TA}}^{\star}$, where $\theta_{M}^{\star}$ and $\theta_{\texttt{TA}}^{\star}$ are, respectively, the solutions to~\eqref{problem:M} and~\eqref{problem:TA} as $e$ goes to infinity.
%%\footnote{Here, we assume a unique optimal solution but, in general, the set of optimizers of~\eqref{problem:M} and~\eqref{problem:TA} are equal.} 
%This can be seen by the fact that the effect of the assistant reward disappears as $e$ increases. In other words, solving the \texttt{TA}-goal is the same as solving the main goal. However, if the assistant reward is designed intelligently, then it can improve the sample efficiency of any RL algorithm. We illustrate this in the next two sections. 

Moving to a new MDP in each episode until $\beta(e)=0$ heightens the non-stationarity problem. This aligns with methods resembling exploration encouragement \cite{NIPS2016_afda3322,STREHL20081309}, initially appearing to exacerbate problem instability and slow learning. Nonetheless, it's observed that increased feedback actually accelerates learning. Our approach differs by not advocating blind exploration of all states; instead, we harness transferred knowledge across tasks before deep dives (avoiding overfitting and straying from the main goal). The crucial art lies in setting $\beta(e)$ reliably to transfer suitable prior knowledge via the value function or policy, leading to a more sample-efficient algorithm.

%Moving to a new MDP in each episode (until $\beta(e)=0$) increases the non-stationarity of the problem. In this sense, this work is similar to the methods of encouraging the exploration \cite{NIPS2016_afda3322,STREHL20081309}, which at first sight all cause more instability of the problem and should lead to slow learning. However, it has been seen that due to giving more feedback leads to an increase in learning speed. It is also our case, with the difference that here, instead of encouraging blind exploration of all states, we exploit transferred knowledge between tasks before learning each of them thoroughly (which leads to overfitting and being far from the main goal). The art here is to reliably set $\beta(e)$) to transfer the appropriate prior knowledge through the value function or policy, which results in a more sample-efficient algorithm.

Selecting an appropriate $\beta(e)$ is crucial for smooth learning and influences the number of MDPs. Generally, $\beta(e)$ should decrease with increasing $e$ and approach 0 for convergence to the main goal. The rate of decrease depends on the task, making it challenging to specify a universal formula. The alignment between main and auxiliary goals can guide $\beta(e)$ selection: strong alignment allows for a slower linear decrease, such as $\beta(e) = [(E - e)\beta(0)/E]_+$, starting from $\beta(0)$ and reaching 0 at episode $E$. Conversely, weak alignment suggests a faster exponential decrease, like $\beta(e) = \beta(0)\lambda^e$, with $\lambda \in (0,1)$.

%Finding a suitable  $\beta(e)$ is clearly an important part of making the learning progress smooth, which also determines the number of MDPs. In general,  $\beta(e)$ should be decreasing in $e$ and should converge to $0$ as $e$ increases to ensure convergence to the main goal. How fast $\beta(e)$ decreases depends on the task, and hence, a specific and unique formula cannot be stated for it. In general, how well the main goal and auxiliary goal align 
%% ass alignment between the main goal and auxiliary goal functions 
%can be a good clue for how to select $\beta(e)$. Specifically, if they are well aligned, then $\beta(e)$ can be decreased at a slower rate, e.g., linear rate, by setting $\beta(e) = [(E-e)\beta(0)/E]_+$. Here $\beta(e)$ starts to gradually decrease from $\beta(0)$ and reaches zero at episode $E$. Conversely, if the alignment between these two goals is low, then it is better to decrease $\beta(e)$ at a faster rate. For example, in that case we might have  $\beta(e)$ decrease exponentially fast, i.e., $\beta(e) = \beta(0)\lambda^e$, where $\lambda \in (0,1)$.

\texttt{TA-Explore} is versatile, compatible with any RL algorithm, and not limited to specific algorithms like R-MAX \cite{ strehl2009} that rely on tight upper bounds. It can be integrated with any non-($\epsilon$, $\delta$)-PAC algorithm, whether it's value-based, policy-based, tabular, or deep-RL. In deep-RL, prior knowledge improves initial neural network weightings. It accommodates tasks with multiple goals or constraints (see Section \ref{section5}). Unlike conventional transfer learning methods with pre-training and end-training phases, it offers unified, integrated learning that's simpler and faster. Plus, it does not require pre-solved tasks and easily applies to any environment without added computational complexity.

%\texttt{TA-Explore} can be used with any task and environment and is compatible with any RL algorithm. Because our method is not directly focused on tightening the upper band, it is not limited to some algorithms, such as R-MAX \cite{ strehl2009}, which depend highly on the upper bound. It can be integrated with any non-($\epsilon$, $\delta$)-PAC algorithm. \texttt{TA-Explore} may be used with both value-based methods and policy-based methods and both tabular methods and deep-RL methods. In deep-RL methods, prior knowledge improves the initial weighting of neural networks used as approximator functions (instead of random initial weighting). In particular, tasks with multiple goals or constraints can easily be included in this framework (see Section \ref{section5}). Unlike conventional transfer learning methods, which include two phases of pre-training and end-training, it follows unified and integrated learning that is both less complex and requires less time for training. Also, due to not needing several pre-solved tasks, it can be easily applied to any environment (without adding computational complexity).

%In the following, we illustrate how to use \texttt{TA-Explore} with a simple example in Section \ref{section4}, and then its effectiveness in solving difficult real-world problems is further analyzed in Section \ref{section5}.

\section{Random Walk: An Illustration} \label{section4}

\begin{figure}[t] %[h]
     \centering
     \subfloat[]{
         \includegraphics[width=0.47\linewidth]{figs/fig1_1.png}
         \label{fig1.a}}
         %\hfill
     \subfloat[]{
         \includegraphics[width=0.47\linewidth]{figs/fig1_2n.png}
         \label{fig1.b}}
        \caption{Random Walk example \cite{sutton2018}, where (a) describes how to receive 
        %the target reward 
        $R^T$ and (b) illustrates how to acquire  
        %the assistant reward 
        $R^A$. 
        %In both cases, the episode terminates by going to A or E states.
        }
        \label{fig1}
\end{figure}

In this section, we showcase the efficiency and simplicity of \texttt{TA-Explore} through an examination of a Random Walk example \cite{sutton2018}, a Markov reward process (i.e., an MDP without actions), allowing us to elucidate the core concepts of \texttt{TA-Explore} in an intuitive manner. 
Figure \ref{fig1} depicts the Random Walk environment comprising 5 states. It encompasses two terminal states, denoted as A and E, alongside three non-terminal states, labeled as B, C, and D. If the agent reaches either terminal state, the episode concludes. In all episodes, the agent initiates from state C. Subsequently, at each time step, the agent moves either to the left or right with an equal probability. The agent receives a reward of zero for every transition, except when it reaches state E on the right, where it garners a reward of $+1$. The discount factor is set to $\gamma = 1$, indicating an undiscounted MDP.
Extending this problem to incorporate additional states is straightforward. We simply arrange all the states in a linear fashion, akin to Figure~\ref{fig1} but with a greater number of states. %Likewise, the terminal states remain positioned at the ends of the line, both to the left and right. The initial state remains the middle state. 
We will explore random walks with 5, 11, and 33 states.

%In this section, we demonstrate the efficiency and simplicity of \texttt{TA-Explore} by examining a simple  Random Walk example \cite{sutton2018}. This example is a Markov reward process (MRP), an MDP without actions.  This allows us to illustrate the main ideas behind \texttt{TA-Explore} in a simple and intuitive manner.  

%Figure \ref{fig1} illustrates the Random Walk environment with 5 states. There are two terminal states, A and E,  and three non-terminal states, B, C, and D. If the agent goes to either terminal state, then the episode is terminated. In all episodes, the agent starts in state C. After that, in each time step, the agent takes a step to the left or to the right with an equal probability. The agent gets the reward zero for every transition, except it gets the reward $+1$ if it reaches state E, the terminal state on the right.  The discount factor is $\gamma = 1$, i.e., this is an undiscounted MDP. 

%Generalizing this random walk to multiple states is simple. We just arrange all the states in a line like in Figure~\ref{fig1} except longer. Similarly, the terminal states are the ones on the end of the line to the left and to the right. The initial state is the middle state. We will consider such random walks with 5, 11, and 33 states.

Due to a lack of action here, the agent's main goal is to acquire state values through experience. Initial value learning is slow and necessitates a substantial number of samples. This is because, initially, the agent only receives immediate feedback (a non-zero reward) when it reaches the terminal state on the right. However, we can expedite learning by assigning simpler tasks to the agent that offer more frequent immediate feedback. For instance, in Figure \ref{fig1.b}, we introduce an assistant reward, $R^A$, which provides a reward of 0.1 each time the agent moves to the right, except when it reaches the right terminal state, at which point we award 1. It is evident that learning this auxiliary goal is swifter because the agent receives more frequent reward feedback to update its value function. Additionally, as depicted in Figure \ref{fig2}, a straightforward yet intelligent approach for determining the $\beta(e)$ function might be $\beta(e) = \beta(0)\lambda^{e}$,  where $\beta(0)=1$, and $\lambda \in \{0.8, 0.9, 0.95\}$. The agent could initially begin learning goal $A$ but will quickly shift its focus to mastering goal $M$.

%Since there is no action here, the main goal of the agent is to learn the value of each state by experience. Learning the value is slow in the initial episodes, and it requires a lot of samples. This is because, in the beginning, the agent only gets immediate feedback (non-zero reward) if and when it reaches the terminal state on the right. 

%However, we might facilitate learning by providing the agent with simpler tasks that provide immediate feedback more frequently. For example, as shown in Figure \ref{fig1.b}, we consider the assistant reward $R^A$ that provides the immediate reward 0.1 every time the agent goes to the right except when it reaches the terminal state on the right then we give 1 as the immediate reward. It is intuitive that learning this auxiliary goal is faster since the agent gets more frequent reward feedback to update its value function. Also, as illustrated in Figure \ref{fig2}, a simple but intelligent idea for determining the $\beta(e)$ function could be $\beta(e) = \beta(0)\lambda^{e}$, where $\beta(0)=1$, and $\lambda \in \{0.8, 0.9, 0.95\}$. The agent could first start to learn goal $A$ but focus very quickly on learning goal $M$.

To assess the performance of our proposed method, we opted for a basic value-based algorithm, specifically the temporal-difference learning method (TD(0))  \cite{tesauro1995} with a constant step size of 0.1, despite the compatibility of \texttt{TA-Explore} with various RL algorithms. This choice is due to Random Walk's nature as an MRP problem with a discrete state space. 
In this instance, transfer learning occurs through value transfer, where the value functions derived for each task serve as the initial values for the subsequent task. Our results are an average of 100 test runs, with each execution initializing the state-value functions for all states to zero.
The performance measurement hinges on the root mean square (RMS) error between the learned value function and the true value function corresponding to $M$ and $R^T$. 

%To study our proposed method performance, although \texttt{TA-Explore} supports any kind of RL algorithm since Random Walk is an MRP problem with discrete state space and not looking for optimal action learning, a basic value-based algorithm is our choice. In this paper, we use the temporal-difference learning method (TD(0)) \cite{tesauro1995} with a constant step size of 0.1. So in this example, transfer learning is taking place through value transfer. This means that the value functions obtained for each task are used as the initial values of the next task.

%We report the average results of 100 times tests. It should be noted that at each execution, the state-value functions for all states are initialized with zero. Since the true value function of this problem can be calculated through the Bellman equation exactly, the performance measurement is the root mean square (RMS) error between the value function learned and the true value function corresponding to the main goal $M$ and the target reward $R^T$.

Here's the compressed text:

In Figure \ref{fig3.a}, learning $A$ initially yields positive results, with the agent also unintentionally learning $M$ by episode 38, resulting in a reduction in RMS error. However, as the agent prioritizes learning goal $A$, the error increases, reflecting the divergence in the value functions of the two goals; learning $A$ outpaces learning $M$ (refer to Only $R^T$ in Figure \ref{fig3.a}). Therefore, starting with goal $A$ and subsequently transitioning to goal $M$ before overfitting can maximize prior knowledge utilization. This approach accelerates learning $M$. Specifically, with $\lambda = 0.95$, \texttt{TA-Explore} achieves twice faster convergence on goal $M$.

In Figure \ref{fig3.b}, even with 11 states, $\lambda = 0.95$ remains an effective choice. The critical insight is that as the state count rises, our proposed method's superiority becomes evident. More states decrease the likelihood of initially reaching the far-right state, slowing down baseline convergence, which heavily relies on state numbers. Conversely, our method maintains a $50\%$ chance of receiving rewards from the first step. In Figure \ref{fig3.c}, we consider 33 states where the baseline fails to converge, while \texttt{TA-Explore} succeeds. Notably, $\lambda = 0.95$ ceases to be an appropriate choice. As the number of states grows, disparities in true state-value functions emerge, even though the two goals initially align. Spending too much time on auxiliary goal learning hampers the agent's access to valuable prior information and can lead to extended main goal learning, necessitating time to forget undesirable experiences. Therefore, the agent should transition to main goal learning more quickly, albeit not hastily, as illustrated in Figure \ref{fig3.c} with $\lambda = 0.9$, resulting in more than double the speed of convergence.

%As shown in Figure \ref{fig3.a}, if we learn only the auxiliary goal $A$ and consequently consider only the assistant reward $R^A$, it is observed that by around episode 38, the agent is surprisingly learning the main goal $M$ as well, as RMS error has been dropping. But then, as the agent tries to learn the auxiliary goal $A$ more, the error increases. This is intuitive due to the difference between the true value functions of the two goals. But the point that was predictable is that the speed of learning the auxiliary goal $A$ is faster than learning the main goal $M$ (See Only $R^T$ in Figure \ref{fig3.a}). Hence if the agent starts learning the auxiliary goal $A$ first, but before over-fitting on it, starts learning the main goal $M$, it can get the most out of prior information. Then, it learns the main goal $M$ much faster. 

%Specifically, Figure \ref{fig3.a} shows that with $\lambda = 0.95$, \texttt{TA-Explore} managed to make the most of learning on the auxiliary goal $A$ so that it was finally able to converge more than about twice as fast on the main goal $M$. As the number of states increases to 11 in Figure \ref{fig3.b}; still $\lambda = 0.95$ is a good choice. The critical point is that as the number of states increases, the superiority of our proposed method becomes noticeable. By increasing the number of states, the probability of visiting the most right state for the first time decreases. So, the convergence speed of the baseline approach, which is highly dependent on the state's numbers, decreases. On the other side, our method, still with 50\% probability, will be rewarded even on the first step. Additionally, in Figure \ref{fig3.c}, the results are shown by considering 33 states where the baseline has not yet been able to converge completely. On the contrary, it happens to \texttt{TA-Explore}. Note that, here, $\lambda = 0.95$ is no longer a proper choice. 

%In fact, computing the true values in both cases using the Bellman equation turns out that although the two goals seem largely in line with each other, with an increasing number of states, an increasing difference between the true state-value functions appears. As a result, if the agent spends a lot of time learning the auxiliary goal, not only will it not get acceptable prior information from learning the auxiliary goal $A$, but sometimes it will even lead to a longer learning process of the main goal $M$ due to the need to pass the time to forget those bad experiences. Hence, the agent should shift to learn the main goal $M$ faster but not too hastily, just like $\lambda = 0.9$ in Figure \ref{fig3.c}, where it has been able to converge at more than twice the speed.

%In conclusion, by examining these results, we can ensure the scalability of the proposed method. By selecting an appropriate $\beta(e)$ function, as the number of states increases, the area enclosed between the baseline and our algorithm increases more and more. It means that on the one hand, the baseline takes a long time to converge, and on the other hand, \texttt{TA-Explore} converges with a high slope, gaining the appropriate prior knowledge.
%As \texttt{TA-Explore} is exceptionally profitable on simple tasks and value-based algorithms that do not leave much room for maneuvering, it may have remarkable performance on more complex tasks with continuous state and action spaces. 
%And it may outstandingly improve the results of policy-based approaches as well as deep-RL methods. The next section provides evidence for this claim.
%It can also be expected that as \texttt{TA-Explore} is exceptionally profitable on simple tasks and value-based algorithms that do not leave much room for maneuvering, it will have remarkable performance on more complex tasks such as tasks with continuous state and action spaces, and it will outstandingly improve the results of policy-based approaches as well as deep-RL methods. The next section provides evidence for this claim.


\begin{figure*}[t] %[H]
     \centering
     \subfloat[$\beta(e)$]{
         \includegraphics[width=0.24\textwidth]{figs/fig2_10nn.png}
         \label{fig2}}
     \subfloat[5 states]{
         \includegraphics[width=0.24\textwidth]{figs/fig2_1n.png}
         \label{fig3.a}}
         %\hfill
     \subfloat[11 states]{
         \includegraphics[width=0.23\textwidth]{figs/fig4_3nn.png}
         \label{fig3.b}}
         %\hfill
     \subfloat[33 states]{
         \includegraphics[width=0.23\textwidth]{figs/fig5_2nn.png}
         \label{fig3.c}}
        \caption{(a) The behaviour of the $\beta(e)$ function considered for Random Walk example, with different $\lambda$ values. 
        %As $\lambda$ increases, a slower shifting of the agent from auxiliary goal $A$ learning to main goal $M$ learning happens.
        (b-d) The main goal learning curves in the Random Walk example for different $\lambda$ values and the different number of states. The undrawn curves in (c) and (d) have diverged and have been omitted to show the rest of the results in more detail.}
        %\caption{The main goal learning curves in Random Walk example for different $\lambda$ values and the different number of states. The performance measure shown is the root-mean-squared (RMS) error between the value function learned considering the assumed rewards (i.e., only $R^T$, only $R^A$, or \texttt{TA-Explore} with different $\lambda$s) and the true value function, which is averaged over the states and then averaged over 100 runs. 
        %In all cases, the TD(0) algorithm is used as the backbone. 
        %As it turns out, in all cases, by choosing the appropriate $\lambda$, one can be sure that \texttt{TA-Explore} is learning faster. The undrawn curves in (b) and (c) have diverged and have been omitted to show the rest of the results in more detail.}
        \label{fig3}
\end{figure*}


\section{Experiment on Control Problems} \label{section5}
 
 We illustrate the promise of the \texttt{TA-Explore} framework on two optimal control problems with constraints. We consider  a dynamical system governed by the difference equation $s_{t+1}=g_t(s_t,a_t,e_t)$, where $s_t$ is the state of the system, $a_t$ is the control action, and $e_t$ is a random disturbance; $g_t$ is the rule that maps the current state, control action, and disturbance at time $t$ to a new state. The goal of the agent is to find the actions that optimize the accumulated immediate rewards  $f_t(s_t,a_t)$. Often there are also constraints on the states. Thus, the agent should not only optimize its rewards, but it should also ensure the feasibility of the states. This could be, for example, to ensure the safety of the system operations. Mathematically, such problems are  formulated as follows:
\begin{displaymath}
\begin{split}\label{eq:10}
    \text{maximize} & \sum_{t=0}^{H} \gamma^t f_t(s_t,a_t) \\
    \text{subject to } & s_{t+1}=g_t(s_t,a_t,e_t), \text{and }\\   
   & s_{\min} \leqslant s_t \leqslant s_{\max}.
\end{split}
\end{displaymath}
The constraints on state variable $s_t$ pose challenges for optimal control, resembling real-world issues. Even with known linear dynamics $g_t(s_t,a_t,e_t)$ and quadratic rewards, traditional LQ-regulators cannot handle it due to the state constraint~\cite{aastrom2007feedback}. Furthermore, the problem is exacerbated by the typical lack of knowledge about the system's dynamics~\cite{recht2019}. In such cases, learning-based methods, such as RL, are often the only viable solutions.

%The constraints on the state variable $s_t$ make the problem a difficult optimal control problem and similar to a real-world problem. Even if the dynamics $g_t(s_t,a_t,e_t)$  were known and linear and the rewards quadratic, this problem could not be solved by traditional LQ-regulators due to the state constraint~\cite{aastrom2007feedback}. The problem is further complicated by the fact that the model for the system's dynamics is usually not known~\cite{recht2019}. Learning-based methods like RL are usually the only feasible approaches for such problems.

In RL, constraints are typically incorporated into the reward as the main goal $M$ via a penalty for violations. This means the RL agent must simultaneously learn to satisfy constraints while optimizing rewards. Learning to satisfy constraints is relatively easier because the agent has more action choices, facilitating constraint satisfaction. Hence, it makes sense to use constraint satisfaction as the auxiliary goal $A$ in our framework. We achieve this by employing a negative assistant reward $R^A$. As $R^A$ is part of the target reward $R^T$, there is a strong alignment between them. Consequently, the $\beta(e)$ function can be selected more easily. Our choice is a descending linear function, starting at $\beta(0)$ and reaching zero at $e=E$. This results in a sequence of $E$ MDPs, each solved over one episode.

%In RL, constraints are often included in the reward (i.e., as the main goal $M$) by having a penalty for violating them. This means that the RL agent needs to learn simultaneously how to satisfy the constraint while it is learning to optimize the rewards. However, learning to satisfy the constraints is a much easier task because the agent has more freedom of selection among various actions, which leads to satisfying constraints. Thus, it is intuitive to use constraints satisfaction as the auxiliary goal $A$ in our framework.  

%We can achieve this by using a negative assistant reward $R^A$. Since the $R^A$ is actually part of the target reward $R^T$, there is a high alignment between the two rewards. As a result, the $\beta(e)$ function can be selected more easily. Our selection is a descending linear function
%%\footnote{i.e., $\beta(e) = [(E-e)\beta(0)/E]_+$} 
%that starts at $\beta(0)$ and goes to zero by $e=E$, where it leads to a sequence of $E$ MDPs, each of which is solved for one episode.
%%\footnote{The source code is  available at \url{https://anonymous.4open.science/r/TA-Explore2023}}
%%\footnote{Code is available as supplementary material, and we will make it publicly available if the paper is accepted.}

\subsection{Optimal Temperature Control with Constraint - Linear Dynamics} \label{section5.1}
Consider a data center cooling where three heat sources are coupled to their own cooling devices.
Each component of the state $s$ is the internal temperature of one heat source, which should be maintained in the range of -2 to 2 (i.e., we have constraints on states such that $-2 \leqslant s_t \leqslant 2$).
Under constant load, the sources heat up and radiate heat into the surrounding environment.  
The voltage of each cooler is known as $a$, and the objective is to minimize it while satisfying the constraint. It can be approximated by the linear dynamical system $s_{t+1}=As_t+Ba_t+e_t$ where,
\begin{displaymath}
A=
  \begin{bmatrix}
1.01 & 0.01 & 0\\
0.01 & 1.01 & 0.01\\
0 & 0.01 & 1.01
\end{bmatrix}~~~~
, ~~~~ B=I,
\end{displaymath}
and where $e_t$ is a noise with zero mean with covariance $10^{-4}I$. The voltage of each cooler is the action and the goal is to minimize the power (i.e., $\omega||a||^2$) while satisfying the constraint \cite{recht2019}. To put it in our framework, we define the target reward $R^T$  and the assistant reward $R^A$  as follows:
\begin{equation} \label{rt}
\begin{split}
  &R^T =
    \begin{cases}
      -\omega \Vert a\Vert ^2 & \text{if constraint is satisfied}\\%\text{if } s\in[-2,2]
      -\omega \Vert a\Vert ^2 -100 & \text{otherwise}
    \end{cases} 
    \\
   &R^A =
    \begin{cases}
      0 & \hspace{1.3cm}\text{if constraint is satisfied}\\%\text{if } s\in[-2,2]
      -100 & \hspace{1.3cm}\text{otherwise}
    \end{cases}  
\end{split}
\end{equation}
\iffalse
\begin{equation} \label{rt}
  R^T =
    \begin{cases}
      -\omega \Vert a\Vert^2 & \\ 
      -\omega \Vert a\Vert^2 -100 &
    \end{cases}       
  R^A =
    \begin{cases}
      0 & \text{if constraint is satisfied}\\
      -100 & \text{otherwise}
    \end{cases}       
\end{equation}
\fi
where $\omega$ is the weight of the first term; its impact will be examined later.

The time horizon is 100 steps, and there are 8000 training episodes. To increase the challenge, states can have any initial value following a standard normal distribution. Episodes do not end if the constraint is not satisfied (training continues for 100 time steps in each episode). For $\beta(e)$, we set $E=4000$ and $\beta(0)=1$, so the objective function equals the main goal $M$ after 4,000 episodes.

%The time horizon is 100 steps, and the number of training episodes is 8000. To make the problem more challenging, we allow the states to take any initial value according to the standard normal distribution. Also, we do not terminate the episode by not satisfying the constraint (i.e., allow the training to continue for 100 time steps in each episode). For $\beta(e)$ described at the beginning of this section, we set $E=4000$ and $\beta(0)=1$, which means that after 4,000 episodes, the objective function will be the same as the main goal $M$.

To train the agent, we employ PPO, a deep-RL approach, due to its success as a policy-based algorithm \cite{schulman2017proximal}. PPO is known for its efficiency, presenting a challenge for improvement. In our setup, we update actor and critic models with mini-batches of 512 samples, using 10 training epochs and the Adam optimizer (LR = 0.00025). We set discount and lambda factors at 0.99 and 0.9, respectively.
Both actor and critic models consist of three layers with ReLU activation, with unit sizes of 512, 256, and 64. Actor and critic output layers have 3 and 1 neurons, respectively, utilizing tanh and identity activation functions.
In addition to the original PPO, we compare our performance against four state-of-the-art algorithms: A2C \cite{mnih2016asynchronous}, DDPG \cite{lillicrap2015}, SAC \cite{haarnoja2018soft}, and TD3 \cite{td3}.

%To train the agent, since the state and action spaces are continuous, we select PPO, a deep-RL approach, as the backbone, which is one of the most successful policy-based algorithms \cite{schulman2017proximal}. This algorithm is highly efficient, and therefore, improving its results is a challenge that we are looking for. As for the PPO setup, actor and critic models are updated using mini-batches of 512 samples, 10 training epochs, and the Adam optimizer \cite{kingma2014adam} (learning rate 0.00025). In addition, the discount and lambda factors are 0.99 and 0.9, respectively. For both actor and critic models, the input is fed into three layers with ReLU activation and 512, 256, and 64 units, respectively. The output layer of the actor and critic models have, respectively, 3 and 1 neurons, and we use, respectively, the tanh and identity activation functions. 

In this example, transfer learning is achieved via policy transfer using episode-obtained weights for each task as initial neural network weights for the next task. Figure \ref{fig4} displays \texttt{TA-Explore} framework performance. Our framework maintains the same hyperparameters as PPO, streamlining its applicability across various algorithms without requiring further tuning. Other algorithm implementations are based on the Stable Baselines3 repository \cite{stable-baselines3}. Performance evaluation involves plotting the average reward $R^T$, a pertinent metric as the primary focus of the agent is mastering the main goal $M$; the auxiliary goal $A$ serves solely as a learning facilitator. Consequently, acquiring proficiency in the auxiliary goal $A$ has no intrinsic importance, unless it expedites knowledge transfer for faster convergence.

%In this example, transfer learning is done through policy transfer by using the weights obtained in each episode for each task as the initial weighting of the neural network used as an approximation function of the policy of the next task. Figure \ref{fig4} shows the performance of using PPO with and without \texttt{TA-Explore} framework. While using our framework, we still use the same hyperparameters as used for the baseline, which can facilitate employing our method on any algorithm without any tuning need.  We measure the performance by plotting the average reward $R^T$. This criterion is natural since the agent only wants to learn the main goal $M$, and the auxiliary goal $A$ is just used to facilitate learning. So, learning the auxiliary goal $A$ in itself does not matter to us unless it helps the agent to converge faster by transferring prior knowledge to it.


In Figure \ref{fig4.a}, \texttt{TA-Explore} converges twice as fast because of rapid learning of assistant reward $R^A$ in early episodes. Other algorithms exhibit slower convergence due to the agent's struggle in discerning complex rewards, making it challenging to differentiate constraint violations from deviations from the main goal. 
Prioritizing the initial term of $R^T$ sacrifices more information from $R^A$, diminishing alignment between them, thus heightening the challenge. Nonetheless, as depicted in Figures \ref{fig4.b} and \ref{fig4.c}, \texttt{TA-Explore} converges nearly as swiftly. In contrast, the baselines yield unsatisfactory results, often failing to converge even after 8,000 episodes. Thus, it is evident that for more intricate problems, our convergence outpaces the baselines. It is worth noting that, while we have demonstrated the significance of learning constraints and gradually incorporating the primary problem, further enhancement can be achieved through skillful formulation of auxiliary goals informed by domain knowledge.

%As shown in Figure \ref{fig4.a}, \texttt{TA-Explore} converges more than twice as fast. The reason for this speed of convergence is simply the learning of the assistant reward $R^A$ considered in our proposed method during initial episodes on the one hand, and on the other hand, the confusion of the agent when considering a complicated reward, where the agent does not know whether the reward is due to the violation of the constraints or being far from the main goal. 

%By giving more weight to the first term of $R^T$, we lose more information in $R^A$ than in $R^T$. Hence, the alignment between them is reduced, and the problem becomes more challenging.
%However, as shown in Figures \ref{fig4.b} and \ref{fig4.c}, \texttt{TA-Explore} converge well over almost the same number of episodes. On the contrary, the baseline has acquired bad results so that it has not converged in 8,000 episodes in Figure \ref{fig4.c}.
%So it can be concluded again that the more complex the problem, the more and more the area enclosed between the two curves, and we converge faster than the baseline. Note that, although here we showed that learning the constraints and gradually considering the main problem greatly contribute to the convergence, one can even further improve it by skillfully defining auxiliary goals considering their domain knowledge.

\begin{figure}[ht] %[H]
     \centering
     \subfloat[]{
         \includegraphics[width=0.8\linewidth]{figs/Temperature_Control_e8000_omega1_beta1.0_E4000.png}
         \label{fig4.a}}
         \hfill
     \subfloat[]{
         \includegraphics[width=0.8\linewidth]{figs/Temperature_Control_e8000_omega10_beta1.0_E4000.png}
         \label{fig4.b}}
         \hfill
     \subfloat[]{
         \includegraphics[width=0.8\linewidth]{figs/Temperature_Control_e8000_omega100_beta1.0_E4000.png}
         \label{fig4.c}}
        \caption{Average performance on optimal temperate control problem for different weighting values to the control objective (i.e., (a) $1\Vert a\Vert ^2$, (b) $10\Vert a\Vert ^2$, (c) $100\Vert a\Vert ^2$). To have a clear illustration, 50 episodes moving average reward are plotted.}
       %\caption{Average performance of \texttt{TA-Explore} on optimal temperate control with constraint for different weighting values to the control objective (i.e., (a) $1\Vert a\Vert ^2$ , (b) $10\Vert a\Vert ^2$, (c) $100\Vert a\Vert ^2$). The performance measure shown is the amount of the target reward $R^T$ achieved by the agent in each episode, regardless of whether the agent is learning according to $R^T$ or a combination of $R^T$ with the assistant reward $R^A$ according to Equation \ref{eq}. In all cases, the PPO algorithm is used as the backbone with the same setup. To have a clear illustration, 50 episodes moving average reward are plotted. As it turns out, \texttt{TA-Explore}, in all cases, converges very fast. But, the baseline converges too late and in (c) does not converge during 8000 episodes.}
        %\Description{des4}
        \label{fig4}
\end{figure}


\iffalse
\begin{figure*}[t]
     \centering
     \begin{subfigure}{.30\linewidth}
         \centering
         \includegraphics[width=\linewidth]{figs/fig10_1n.png}
         \caption{}
         \label{fig4.a}
     \end{subfigure}
     \begin{subfigure}{.30\linewidth}
         \centering
         \includegraphics[width=\linewidth]{figs/fig10_2n.png}
         \caption{}
         \label{fig4.b}
     \end{subfigure}
     \begin{subfigure}{.30\linewidth}
         \centering
         \includegraphics[width=\linewidth]{figs/fig10_3n.png}
         \caption{}
         \label{fig4.c}
     \end{subfigure}
        \caption{Average performance on optimal temperate control with constraint for different weighting values to the control objective (i.e., (a) $1\Vert a\Vert ^2$, (b) $10\Vert a\Vert ^2$, (c) $100\Vert a\Vert ^2$). To have a clear illustration, 50 episodes moving average reward are plotted.}
       % \caption{Average performance of \texttt{TA-Explore} on optimal temperate control with constraint for different weighting values to the control objective (i.e., (a) $1\Vert a\Vert ^2$ , (b) $10\Vert a\Vert ^2$, (c) $100\Vert a\Vert ^2$). The performance measure shown is the amount of the target reward $R^T$ achieved by the agent in each episode, regardless of whether the agent is learning according to $R^T$ or a combination of $R^T$ with the assistant reward $R^A$ according to Equation \ref{eq}. In all cases, the PPO algorithm is used as the backbone with the same setup. To have a clear illustration, 50 episodes moving average reward are plotted. As it turns out, \texttt{TA-Explore}, in all cases, converges very fast. But, the baseline converges too late and in (c) does not converge during 8000 episodes.}
        \label{fig4}
\end{figure*}
\fi


\subsection{Coupled Four Tank MIMO System - Nonlinear Dynamics}

\begin{figure}[ht]
  \centering
  \includegraphics[width=0.65\linewidth]{figs/fig11.png}
  \caption{Coupled four tank MIMO system \cite{GOUTA2017280}
  %; known as a nonlinear optimal control problem with constraints.
  .}
  %\Description{des5}
  \label{fig5}
\end{figure}

We also study the performance on a coupled four-water tank system with nonlinear dynamics \cite{GOUTA2017280}.
Figure \ref{fig5} illustrates how this system is comprised of a liquid basin, two pumps, and four tanks with the same area with orifices. The dynamics of the systems are given by:
\begin{align*}
%
\begin{cases}
s_{t+1}^1 =&-c_1\sqrt{s_t^1}+c_2\sqrt{s_t^3}+c_3\sqrt{s_t^4} \\
s_{t+1}^2 =&-c_4\sqrt{s_t^2}+c_5\sqrt{s_t^3}+c_6\sqrt{s_t^4} \\
s_{t+1}^3 =& -c_7\sqrt{s_t^3}+c_8 a_t^1 \\
s_{t+1}^4 =& -c_9\sqrt{s_t^4}+c_{10} a_t^2
%
\end{cases}   
\end{align*}
where $c_1, ..., c_{10}$ are model parameters that are set as in \cite{GOUTA2017280}. The actions are $a^1$ and $a^2$, which indicate  the voltages applied, respectively, to Pump 1 and Pump 2, which are  bounded between 0 and 12Volts. The states $s^1$, $s^2$, $s^3$, and $s^4$ indicate  the liquid levels in the four tanks. They should always be greater than 3cm and less than 30cm (i.e., we have constraints both on states and actions such that $3 \leqslant s_t \leqslant 30$,  $0 \leqslant a_t \leqslant 12$).

The problem can be integrated into our proposed framework using the same rewards from Equation \ref{rt} ($\omega =1$). We employ \texttt{TA-Explore} with $\beta(e) = [(E-e)\beta(0)/E]_+$, where $\beta(0)=0.5$ and $E = 3000$. Initial state values are uniformly distributed between 3 and 30. Episodes are terminated if constraints are not met. All other settings remain consistent with Section \ref{section5.1}, with training requiring 30,000 episodes due to its complexity.
In this experiment, we use PPO with settings identical to those in Section \ref{section5.1}, except the number of actor model output neurons is 2. Ensuring action feasibility is straightforward because the actor model's output, governed by the tanh activation function, scales between 0 and 12.

%It is clear that this problem can easily be included in our proposed framework with the same rewards mentioned in Equation \ref{rt} ($\omega =1$). We use \texttt{TA-Explore} with $\beta(e) = [(E-e)\beta(0)/E]_+$ where $\beta(0)=0.5$ and $E = 3000$. The initial state values are produced in the range of allowed states (i.e., between 3 and 30) with a uniform distribution. We also terminate the episode if the constraints are not met. The rest of the setups are the same as in Section \ref{section5.1}, except that it has assumed to take 30,000 episodes to train because of being very difficult to learn. In this experiment, PPO with the same settings as in Section \ref{section5.1} is used as the backbone, except for the number of actor model output neurons which is 2. To ensure the feasibility of the actions is easy since the output of the actor model is limited due to the tanh activation function, which can be scaled between 0 to 12.

Figure \ref{fig6} demonstrates the performance contrast between our proposed approach and baseline methods. Notably, \texttt{TA-Explore} consistently converges to the target reward over $30\%$ faster than PPO, regardless of problem complexity or suboptimal $\beta(e)$ choices. This remarkable convergence stands in contrast to the other baselines, which struggle to reach convergence. The choice of $\beta(0)=0.5$ signifies that from the very first episode, our method prioritizes minimizing voltage while respecting constraints, albeit with half the weight compared to the target reward $R^T$.

%Figure \ref{fig6} shows a comparison between the proposed method and the baseline, where regardless of the complexity of the problem and the relatively poor $\beta(e)$ selection, \texttt{TA-Explore} has still converged more than $30\%$ faster to the target reward. Considering $\beta(0)=0.5$ means that even in the first episode, our method pays attention to minimizing the voltage while satisfying the constraint. But compared to the target reward $R^T$, it gives it a weight equal to half.


\begin{figure}[ht]
  \centering
  \includegraphics[width=0.8\linewidth]{figs/Four_Tank_e30000_omega1_beta0.5_E3000.png}
  \caption{Performance on coupled four tank MIMO system. To have a clear illustration, 50 episodes moving average reward are plotted.}
  %\caption{Average performance of \texttt{TA-Explore} on coupled four tank MIMO system. As shown, \texttt{TA-Explore} converges about $30\%$ faster than baseline. The performance measure shown is the amount of the target reward $R^T$ achieved by the agent in each episode. In both cases, the PPO algorithm is used as the backbone. Note that, 50 episodes moving average reward are plotted to have a clear illustration.}
  %\Description{des6}
  \label{fig6}
\end{figure}



\section{Related Work} \label{section2}
To the best of our knowledge, this paper stands out from the rest of the studies because first, we use a sequence of auxiliary goals (i.e., MDPs) by defining only one assistant reward (not many and not following from a particular distribution) and an annealing function. Second, we learn each auxiliary task only one iteration (without necessarily having $\epsilon$-accuracy with $\delta$ probability assumption). And third, our proposed method supports policy and value transfers, which leads our approach to combine with any type of RL-based algorithm to improve sample efficiency. However, the scope of related work can be broadened to include other efforts to help the agent learn its main task more efficiently.
It should be noted that due to the strong assumptions of other studies stated in this section, it is not possible to use them in our experiments. That is why we have to limit the comparison of our method with the original algorithms.


 Improving the exploration process of an RL agent has been well-studied in the literature. 
Many previous efforts are based on the hypothesis that if we can visit as many states as possible with high probability, then the agent's learning will be more successful and sample efficient. In other words, further exploration is in line with achieving the main goal of the RL agent. There are two fundamental questions that emerge in this setting. In the absence of rewards, what are the agents supposed to look for? And at what point should the agent stop exploring and start acting greedily?  
Research that seeks to answer these questions can be divided into two general categories.
Firstly, there are single-environment approaches, which include intrinsic motivation with \textit{visitation counts} \cite{NIPS2016_afda3322,STREHL20081309}, \textit{optimism} %\cite{NIPS2006_c1b70d96, NIPS2008_e4a6222c, lai1985asymptotically},
\cite{NIPS2006_c1b70d96, NIPS2008_e4a6222c}, 
\textit{curiosity} 
\cite{NIPS2016_abd81528, pmlr-v70-pathak17a, pmlr-v100-schultheis20a}
%\cite{NIPS2016_abd81528, pmlr-v70-pathak17a, pmlr-v100-schultheis20a, stadie2015incentivizing}
, and \textit{reward shaping} \cite{Dann_Zambetta_Thangarajah_2019}.
%\cite{Dann_Zambetta_Thangarajah_2019, ijcai2019-324,albe}.
Secondly, there are multi-environment approaches that incrementally learn a task across environments, for example, through \textit{transfer learning} \cite{weiss2016survey, parisi2021, 2020sequential, 2018abel, 2021lipschitz}, \textit{continual learning} \cite{Kirkpatrick3521}, \textit{meta-learning} \cite{finn2017model}, and \textit{curriculum learning} \cite{narvekar2020curriculum, luo2020accelerating}. We discuss these approaches below and contrast them to our contributions. 


\textbf{Single-environment:} The notion of intrinsic rewards for exploration originated with Schmidhuber \cite{schmidhuber1991}, who proposed encouraging exploration by visiting unpredictable states. 
In recent years, researchers in RL have extensively studied auxiliary rewards to compensate for the lack of external rewards.
Many intrinsic rewards have been proposed, including bonuses based on visitation counts and prediction errors \cite{NIPS2016_afda3322,STREHL20081309}.
For instance, a dynamic model can be learned %and used 
to predict the next state \cite{NIPS2016_abd81528, pmlr-v70-pathak17a}.
%\cite{NIPS2016_abd81528, pmlr-v70-pathak17a, stadie2015incentivizing}.
Here, the agent is incentivized to explore unpredictable states by granting a bonus proportional to its prediction error.
Another idea, by Schultheis et al. \cite{pmlr-v100-schultheis20a}, is to maximize extrinsic rewards by meta-gradient to learn intrinsic rewards. Also, potential-based reward shaping to encourage agents to explore more has been studied as a way of increasing the learning speed \cite{Dann_Zambetta_Thangarajah_2019}.
%\cite{Dann_Zambetta_Thangarajah_2019, ijcai2019-324,albe}.

These methods of exploration are often called agent-centric since they are based on updating the agent's belief, e.g., through the forward model error. In this sense, these works fall into the same category as ours; both rely on the hypothesis that exploration at the same time with exploitation by agents can be sufficient to achieve a (sub-) optimal policy. 
Another common assumption is that in all the papers following this method, the main goal and, consequently, the target award, i.e., the extrinsic reward is clear, and we have knowledge about it. 
The main difference between our work and others is that their purpose in the first place is only to explore as much as possible, and they are based on the assumption that by exploring more, the agent can learn an optimal policy. 
In contrast, we do not have such an assumption, and we do not seek to visit the states as much as possible. Instead, we focus only on intelligent and auxiliary exploration, which is resulted from transferred knowledge gained on intermediate partially solved tasks, which is in line with the main goal and accelerates the learning of the main task, not a complete acquaintance of the environment.


\textbf{Multi-environment:} 
%
The second set of papers reviewed followed the idea of incrementally learning tasks that have long been known in machine learning \cite{ring1994}.
In RL, although recent methods have mostly focused on policy and feature transfer, some have studied exploration transfer \cite{parisi2021}.
In policy transfer, behaviors are transferred to a new agent (student) by a pre-trained agent (teacher). 
The student is trained to minimize the KullbackLeibler divergence to the teacher using policy distillation, for example, \cite{DBLP}.
Other methods re-use policies from source tasks directly to create a student policy \cite{10.1145/1160633.1160762, NIPS2017_350db081}. On the other hand, a pre-learned state representation used in feature transfer encourages the agent to explore when it is presented with tasks \cite{Hansen2020Fast}.
In exploration transfer, in a task-agnostic and environment-centric scenario, the agent is first attempted by defining an intrinsic reward to visit the states as more as possible especially interesting states in multiple environments, instead of learning the main task, and then this prior knowledge is transferred as a bias to the main environment in which the main task is to be trained \cite{parisi2021}.
Studies on continual RL seek to determine how learning one or more tasks can accelerate learning of other tasks, as well as avoid ruinous forgetting \cite{Kirkpatrick3521}.
Meta RL and curriculum learning, on the other hand, focus on exploiting underlying similarities between tasks to speed up the learning process of new tasks \cite{finn2017model,narvekar2020curriculum, luo2020accelerating}. 
Many studies have been done under the Lifelong RL framework, where the tasks change following a specific distribution \cite{2020sequential, 2018abel, 2021lipschitz}. In \cite{2018abel, 2021lipschitz}, by spending a lot of computational costs, knowledge transfer is done through transferring initializers. Most are only compatible with ($\epsilon$, $\delta$)-PAC algorithms \cite{2020sequential, 2018abel, 2021lipschitz, strehl2009}


In the mentioned incrementally learning task methods, it is assumed that there are several tasks/environments that, with the help of the experience gained on one or multiple tasks/environments, we seek to accelerate the learning of the same task in the new environment or the new task in the same environment. 
As a result, the most important hypothesis is that tasks/environments are very similar to each other, which is not always consistent with reality and deprives us of the possibility of using these methods on any task and any environment. On the contrary, our proposed framework does not need to have several similar environments/tasks. In particular, by defining only one assistant reward along with a decreasing function, we successfully represent a large number of MDPs, which essentially does not require the assumption of switching tasks over a particular distribution. Therefore, it can be used for any environment and any task.
Another point is that in transfer learning, the learning process consists of two phases pre-training and the end-training phase, which are isolated from each other. More specifically, it is assumed that the learning on the previous tasks has been done thoroughly and with good accuracy. In our proposed framework, however, we are smoothly and consistently shifting between the auxiliary goals and the main goal. Not only do we not assume any tasks are already learned, but we also do not need to learn them completely and accurately, i.e., we only have one iteration of training per auxiliary task. 
In the other studies, like \cite{2020sequential}, which identify the closest task for knowledge transfer, there is a lot of computational cost and limit on the learning transfer method and the algorithms that can be combined with their methods. But our approach does not add computational complexity. Also, it can transfer knowledge through both value and policy transfer and is compatible with any algorithm.
Note that we have an integrated learning process so that when considering the acceleration of convergence, we do not neglect to consider the time required to learn the auxiliary goals.
The last issue is related to \cite{parisi2021}, where the authors try to explore as much as possible in the pre-training stage. On the contrary, we do not suppose such a hypothesis in our work. As a result, we only focus on defining auxiliary goals for faster learning of the task.


\section{Conclusion and Future Work} \label{section6}
Sample inefficiency is a critical weakness of current developments in RL. RL algorithms must become more efficient if they are to achieve their full potential in many real-world applications. In this paper, we introduce a new framework \texttt{TA-Explore} for smart exploration. The main idea is to use simpler assistant tasks that are aligned  with the main task that gradually progress toward the main difficult task. Thus helping the agent in finding a more efficient learning trajectory.  
The \texttt{TA-Explore} framework is extremely flexible; it can be applied to any RL task, any type of environment. In addition, any type of RL algorithm can be used as its backbone. 
We conducted comprehensive experiments on a wide range of tasks with different difficulties, with different RL algorithms -both value-based methods and policy-based methods and both tabular methods and deep-RL methods.
The results show the excellent performance of the proposed method in increasing the convergence speed.
Moreover, \texttt{TA-Explore} has no additional computational cost and complexity.
Therefore, our proposed framework, which owes its superiority to the definition of the teacher-assisted goal, can impact the field of RL and make researchers think about a better definition of the reward, relaxing limiting assumptions, and no need to complete learning of the prior tasks before the transfer learning process. 
Proving the sample complexity improvement in such a setting is potentially future work.
%
Also, for future work, adding a self-tuning feature to the $\beta$ function can go a long way in overcoming the only current limitation of the proposed method, which is to select the $\beta$ function experimentally. On the other hand, examining its performance in multi-agent environments under the partially observable Markov decision process (POMDP) assumption, which has higher complexity and suffers from slow learning of the objective function, can be considered a valuable task.

\section*{Acknowledgment}
This work was partially supported by the Swedish Research Council through grant agreement no. 2020-03607 and in part by %Digital Futures, the C3.ai Digital Transformation Institute, and 
Sweden's Innovation Agency (Vinnova). 
The computations were enabled by resources provided by the National Academic Infrastructure for Supercomputing in Sweden (NAISS) %and the Swedish National Infrastructure for Computing (SNIC) 
at Chalmers Centre for Computational Science and Engineering (C3SE) partially funded by the Swedish Research Council through grant agreement no. 2022-06725. % and no. 2018-05973.

\balance
%\addtolength{\textheight}{-12cm}   % This command serves to balance the column lengths
                                  % on the last page of the document manually. It shortens
                                  % the textheight of the last page by a suitable amount.
                                  % This command does not take effect until the next page
                                  % so it should come on the page before the last. Make
                                  % sure that you do not shorten the textheight too much.
\bibliographystyle{IEEEtran}
\bibliography{sample_without_url.bib}
\end{document}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
