%%%%%%%%%%%%%%%Entries
@article{mnih2015,
  title={Human-level control through deep reinforcement learning},
  author={Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A and Veness, Joel and Bellemare, Marc G and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K and Ostrovski, Georg and others},
  journal={nature},
  volume={518},
  number={7540},
  pages={529--533},
  year={2015},
  publisher={Nature Publishing Group}
}
@article{silver2016,
  title={Mastering the game of Go with deep neural networks and tree search},
  author={Silver, David and Huang, Aja and Maddison, Chris J and Guez, Arthur and Sifre, Laurent and Van Den Driessche, George and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc and others},
  journal={nature},
  volume={529},
  number={7587},
  pages={484--489},
  year={2016},
  publisher={Nature Publishing Group}
}
@article{silver2017,
  title={Mastering the game of go without human knowledge},
  author={Silver, David and Schrittwieser, Julian and Simonyan, Karen and Antonoglou, Ioannis and Huang, Aja and Guez, Arthur and Hubert, Thomas and Baker, Lucas and Lai, Matthew and Bolton, Adrian and others},
  journal={nature},
  volume={550},
  number={7676},
  pages={354--359},
  year={2017},
  publisher={Nature Publishing Group}
}

@article{meng2019,
  title={Reinforcement learning in financial markets},
  author={Meng, Terry Lingze and Khushi, Matloob},
  journal={Data},
  volume={4},
  number={3},
  pages={110},
  year={2019},
  publisher={Multidisciplinary Digital Publishing Institute}
}
@techreport{fischer2018,
  title={Reinforcement learning in financial markets-a survey},
  author={Fischer, Thomas G},
  year={2018},
  institution={FAU Discussion Papers in Economics}
}

@article{kober2013,
  title={Reinforcement learning in robotics: A survey},
  author={Kober, Jens and Bagnell, J Andrew and Peters, Jan},
  journal={The International Journal of Robotics Research},
  volume={32},
  number={11},
  pages={1238--1274},
  year={2013},
  publisher={SAGE Publications Sage UK: London, England}
}
@article{lillicrap2015,
  title={Continuous control with deep reinforcement learning},
  author={Lillicrap, Timothy P and Hunt, Jonathan J and Pritzel, Alexander and Heess, Nicolas and Erez, Tom and Tassa, Yuval and Silver, David and Wierstra, Daan},
  journal={arXiv preprint arXiv:1509.02971},
  year={2015}
}
@inproceedings{haarnoja2018soft,
  title={Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor},
  author={Haarnoja, Tuomas and Zhou, Aurick and Abbeel, Pieter and Levine, Sergey},
  booktitle={International conference on machine learning},
  pages={1861--1870},
  year={2018},
  organization={PMLR}
}
@inproceedings{td3,
  title={Addressing function approximation error in actor-critic methods},
  author={Fujimoto, Scott and Hoof, Herke and Meger, David},
  booktitle={International conference on machine learning},
  pages={1587--1596},
  year={2018},
  organization={PMLR}
}
@inproceedings{mnih2016asynchronous,
  title={Asynchronous methods for deep reinforcement learning},
  author={Mnih, Volodymyr and Badia, Adria Puigdomenech and Mirza, Mehdi and Graves, Alex and Lillicrap, Timothy and Harley, Tim and Silver, David and Kavukcuoglu, Koray},
  booktitle={International conference on machine learning},
  pages={1928--1937},
  year={2016},
  organization={PMLR}
}
@article{stable-baselines3,
  author  = {Antonin Raffin and Ashley Hill and Adam Gleave and Anssi Kanervisto and Maximilian Ernestus and Noah Dormann},
  title   = {Stable-Baselines3: Reliable Reinforcement Learning Implementations},
  journal = {Journal of Machine Learning Research},
  year    = {2021},
  volume  = {22},
  number  = {268},
  pages   = {1-8},
  url     = {http://jmlr.org/papers/v22/20-1364.html}
}
@misc{rl-zoo3,
  author = {Raffin, Antonin},
  title = {RL Baselines3 Zoo},
  year = {2020},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/DLR-RM/rl-baselines3-zoo}},
}

@ARTICLE{126844,  author={Sutton, R.S. and Barto, A.G. and Williams, R.J.},  journal={IEEE Control Systems Magazine},   title={Reinforcement learning is direct adaptive optimal control},   year={1992},  volume={12},  number={2},  pages={19-22},  doi={10.1109/37.126844}}

@article{PERRUSQUIA2021145,
title = {Identification and optimal control of nonlinear systems using recurrent neural networks and reinforcement learning: An overview},
journal = {Neurocomputing},
volume = {438},
pages = {145-154},
year = {2021},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2021.01.096},
author = {Adolfo Perrusquía and Wen Yu},
keywords = {Recurrent neural networks, Reinforcement learning, Model matching, Modeling error, Riccati equation, Lyapunov equation},
abstract = {This paper reviews the identification and optimal control problems using recurrent neural networks and reinforcement learning for nonlinear systems both in discrete- and continuous-time. Since neural networks can approximate any nonlinear function, then is shown that it can approximate a dynamical system using some well-identified elements and different neural structures. Existing methods using Lyapunov and Riccati equations to get the neural weights update rules are reviewed. Optimal control using a linear quadratic regulator formulation or reinforcement learning methods are discussed. We discuss the normalized gradient descent algorithm as core algorithm for the optimal control design using reinforcement learning.}
}

@article{CORONATO2020101964,
title = {Reinforcement learning for intelligent healthcare applications: A survey},
journal = {Artificial Intelligence in Medicine},
volume = {109},
pages = {101964},
year = {2020},
issn = {0933-3657},
doi = {https://doi.org/10.1016/j.artmed.2020.101964},
author = {Antonio Coronato and Muddasar Naeem and Giuseppe {De Pietro} and Giovanni Paragliola},
keywords = {Artificial intelligence, Reinforcement learning, Healthcare, Personalized medicine},
abstract = {Discovering new treatments and personalizing existing ones is one of the major goals of modern clinical research. In the last decade, Artificial Intelligence (AI) has enabled the realization of advanced intelligent systems able to learn about clinical treatments and discover new medical knowledge from the huge amount of data collected. Reinforcement Learning (RL), which is a branch of Machine Learning (ML), has received significant attention in the medical community since it has the potentiality to support the development of personalized treatments in accordance with the more general precision medicine vision. This report presents a review of the role of RL in healthcare by investigating past work, and highlighting any limitations and possible future contributions.}
}

@article{10.1145/3477600,
author = {Yu, Chao and Liu, Jiming and Nemati, Shamim and Yin, Guosheng},
title = {Reinforcement Learning in Healthcare: A Survey},
year = {2021},
issue_date = {January 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {55},
number = {1},
issn = {0360-0300},
doi = {10.1145/3477600},
abstract = {As a subfield of machine learning, reinforcement learning (RL) aims at optimizing decision making by using interaction samples of an agent with its environment and the potentially delayed feedbacks. In contrast to traditional supervised learning that typically relies on one-shot, exhaustive, and supervised reward signals, RL tackles sequential decision-making problems with sampled, evaluative, and delayed feedbacks simultaneously. Such a distinctive feature makes RL techniques a suitable candidate for developing powerful solutions in various healthcare domains, where diagnosing decisions or treatment regimes are usually characterized by a prolonged period with delayed feedbacks. By first briefly examining theoretical foundations and key methods in RL research, this survey provides an extensive overview of RL applications in a variety of healthcare domains, ranging from dynamic treatment regimes in chronic diseases and critical care, automated medical diagnosis, and many other control or scheduling problems that have infiltrated every aspect of the healthcare system. In addition, we discuss the challenges and open issues in the current research and highlight some potential solutions and directions for future research.},
journal = {ACM Comput. Surv.},
month = {nov},
articleno = {5},
numpages = {36},
keywords = {healthcare, automated diagnosis, Reinforcement learning, critical care, chronic disease, dynamic treatment regimes}
}

@article{shalev2016,
  title={Safe, multi-agent, reinforcement learning for autonomous driving},
  author={Shalev-Shwartz, Shai and Shammah, Shaked and Shashua, Amnon},
  journal={arXiv preprint arXiv:1610.03295},
  year={2016}
}
@article{tesauro1995,
  title={Temporal difference learning and TD-Gammon},
  author={Tesauro, Gerald and others},
  journal={Communications of the ACM},
  volume={38},
  number={3},
  pages={58--68},
  year={1995}
}
@article{10.1007/BF00992698,
author = {Watkins, Christopher J. C. H. and Dayan, Peter},
title = {Q -Learning},
year = {1992},
issue_date = {May 1992},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {8},
number = {3–4},
issn = {0885-6125},
doi = {10.1007/BF00992698},
abstract = { cal Q -learning (Watkins, 1989) is a simple way for agents to learn how to act optimally in controlled Markovian domains. It amounts to an incremental method for dynamic programming which imposes limited computational demands. It works by successively improving its evaluations of the quality of particular actions at particular states.This paper presents and proves in detail a convergence theorem for cal Q -learning based on that outlined in Watkins (1989). We show that cal Q -learning converges to the optimum action-values with probability 1 so long as all actions are repeatedly sampled in all states and the action-values are represented discretely. We also sketch extensions to the cases of non-discounted, but absorbing, Markov environments, and where many cal Q values can be changed each iteration, rather than just one.},
journal = {Mach. Learn.},
month = {may},
pages = {279–292},
numpages = {14},
keywords = {reinforcement learning, cal Q -learning, asynchronous dynamic programming, temporal differences}
}
@article{sutton1999,
  title={Policy gradient methods for reinforcement learning with function approximation},
  author={Sutton, Richard S and McAllester, David and Singh, Satinder and Mansour, Yishay},
  journal={Advances in neural information processing systems},
  volume={12},
  year={1999}
}
@article{BOTVINICK2019408,
title = {Reinforcement Learning, Fast and Slow},
journal = {Trends in Cognitive Sciences},
volume = {23},
number = {5},
pages = {408-422},
year = {2019},
issn = {1364-6613},
doi = {https://doi.org/10.1016/j.tics.2019.02.006},
author = {Matthew Botvinick and Sam Ritter and Jane X. Wang and Zeb Kurth-Nelson and Charles Blundell and Demis Hassabis},
abstract = {Deep reinforcement learning (RL) methods have driven impressive advances in artificial intelligence in recent years, exceeding human performance in domains ranging from Atari to Go to no-limit poker. This progress has drawn the attention of cognitive scientists interested in understanding human learning. However, the concern has been raised that deep RL may be too sample-inefficient – that is, it may simply be too slow – to provide a plausible model of how humans learn. In the present review, we counter this critique by describing recently developed techniques that allow deep RL to operate more nimbly, solving problems much more quickly than previous methods. Although these techniques were developed in an AI context, we propose that they may have rich implications for psychology and neuroscience. A key insight, arising from these AI methods, concerns the fundamental connection between fast RL and slower, more incremental forms of learning.}
}
@article{GOTTLIEB2013585,
title = {Information-seeking, curiosity, and attention: computational and neural mechanisms},
journal = {Trends in Cognitive Sciences},
volume = {17},
number = {11},
pages = {585-593},
year = {2013},
issn = {1364-6613},
doi = {https://doi.org/10.1016/j.tics.2013.09.001},
author = {Jacqueline Gottlieb and Pierre-Yves Oudeyer and Manuel Lopes and Adrien Baranes},
abstract = {Intelligent animals devote much time and energy to exploring and obtaining information, but the underlying mechanisms are poorly understood. We review recent developments on this topic that have emerged from the traditionally separate fields of machine learning, eye movements in natural behavior, and studies of curiosity in psychology and neuroscience. These studies show that exploration may be guided by a family of mechanisms that range from automatic biases toward novelty or surprise to systematic searches for learning progress and information gain in curiosity-driven behavior. In addition, eye movements reflect visual information searching in multiple conditions and are amenable for cellular-level investigations. This suggests that the oculomotor system is an excellent model system for understanding information-sampling mechanisms.}
}
@inproceedings{NIPS2016_afda3322,
 author = {Bellemare, Marc and Srinivasan, Sriram and Ostrovski, Georg and Schaul, Tom and Saxton, David and Munos, Remi},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Lee and M. Sugiyama and U. Luxburg and I. Guyon and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Unifying Count-Based Exploration and Intrinsic Motivation},
 volume = {29},
 year = {2016}
}

@article{STREHL20081309,
title = {An analysis of model-based Interval Estimation for Markov Decision Processes},
journal = {Journal of Computer and System Sciences},
volume = {74},
number = {8},
pages = {1309-1331},
year = {2008},
note = {Learning Theory 2005},
issn = {0022-0000},
doi = {https://doi.org/10.1016/j.jcss.2007.08.009},
author = {Alexander L. Strehl and Michael L. Littman},
keywords = {Reinforcement learning, Learning theory, Markov Decision Processes},
abstract = {Several algorithms for learning near-optimal policies in Markov Decision Processes have been analyzed and proven efficient. Empirical results have suggested that Model-based Interval Estimation (MBIE) learns efficiently in practice, effectively balancing exploration and exploitation. This paper presents a theoretical analysis of MBIE and a new variation called MBIE-EB, proving their efficiency even under worst-case conditions. The paper also introduces a new performance metric, average loss, and relates it to its less “online” cousins from the literature.}
}
@inproceedings{NIPS2006_c1b70d96,
 author = {Auer, Peter and Ortner, Ronald},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {B. Sch\"{o}lkopf and J. Platt and T. Hoffman},
 pages = {},
 publisher = {MIT Press},
 title = {Logarithmic Online Regret Bounds for Undiscounted Reinforcement Learning},
 volume = {19},
 year = {2007}
}
@inproceedings{NIPS2008_e4a6222c,
 author = {Auer, Peter and Jaksch, Thomas and Ortner, Ronald},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Near-optimal Regret Bounds for Reinforcement Learning},
 volume = {21},
 year = {2009}
}
@article{lai1985asymptotically,
  title={Asymptotically efficient adaptive allocation rules},
  author={Lai, Tze Leung and Robbins, Herbert and others},
  journal={Advances in applied mathematics},
  volume={6},
  number={1},
  pages={4--22},
  year={1985}
}

@inproceedings{NIPS2016_abd81528,
 author = {Houthooft, Rein and Chen, Xi and Chen, Xi and Duan, Yan and Schulman, John and De Turck, Filip and Abbeel, Pieter},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Lee and M. Sugiyama and U. Luxburg and I. Guyon and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {VIME: Variational Information Maximizing Exploration},
 volume = {29},
 year = {2016}
}


@InProceedings{pmlr-v70-pathak17a,
  title = 	 {Curiosity-driven Exploration by Self-supervised Prediction},
  author =       {Deepak Pathak and Pulkit Agrawal and Alexei A. Efros and Trevor Darrell},
  booktitle = 	 {Proceedings of the 34th International Conference on Machine Learning},
  pages = 	 {2778--2787},
  year = 	 {2017},
  editor = 	 {Precup, Doina and Teh, Yee Whye},
  volume = 	 {70},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {06--11 Aug},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v70/pathak17a/pathak17a.pdf},
  abstract = 	 {In many real-world scenarios, rewards extrinsic to the agent are extremely sparse, or absent altogether. In such cases, curiosity can serve as an intrinsic reward signal to enable the agent to explore its environment and learn skills that might be useful later in its life. We formulate curiosity as the error in an agent’s ability to predict the consequence of its own actions in a visual feature space learned by a self-supervised inverse dynamics model. Our formulation scales to high-dimensional continuous state spaces like images, bypasses the difficulties of directly predicting pixels, and, critically, ignores the aspects of the environment that cannot affect the agent. The proposed approach is evaluated in two environments: VizDoom and Super Mario Bros. Three broad settings are investigated: 1) sparse extrinsic reward, where curiosity allows for far fewer interactions with the environment to reach the goal; 2) exploration with no extrinsic reward, where curiosity pushes the agent to explore more efficiently; and 3) generalization to unseen scenarios (e.g. new levels of the same game) where the knowledge gained from earlier experience helps the agent explore new places much faster than starting from scratch.}
}

@InProceedings{pmlr-v100-schultheis20a,
  title = 	 {Receding Horizon Curiosity},
  author =       {Schultheis, Matthias and Belousov, Boris and Abdulsamad, Hany and Peters, Jan},
  booktitle = 	 {Proceedings of the Conference on Robot Learning},
  pages = 	 {1278--1288},
  year = 	 {2020},
  editor = 	 {Kaelbling, Leslie Pack and Kragic, Danica and Sugiura, Komei},
  volume = 	 {100},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {30 Oct--01 Nov},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v100/schultheis20a/schultheis20a.pdf},
  abstract = 	 {Sample-efficient exploration is crucial not only for discovering rewarding experiences but also for adapting to environment changes in a task-agnostic fashion. A principled treatment of the problem of optimal input synthesis for system identification is provided within the framework of sequential Bayesian experimental design. In this paper, we present an effective trajectory-optimization-based approximate solution of this otherwise intractable problem that models optimal exploration in an unknown Markov decision process (MDP). By interleaving episodic exploration with Bayesian nonlinear system identification, our algorithm takes advantage of the inductive bias to explore in a directed manner, without assuming prior knowledge of the MDP. Empirical evaluations indicate a clear advantage of the proposed algorithm in terms of the rate of convergence and the final model fidelity when compared to intrinsic-motivation-based algorithms employing exploration bonuses such as prediction error and information gain. Moreover, our method maintains a computational advantage over a recent model-based active exploration (MAX) algorithm, by focusing on the information gain along trajectories instead of seeking a global exploration policy. A reference implementation of our algorithm and the conducted experiments is publicly available1.}
}
@misc{stadie2015incentivizing,
      title={Incentivizing Exploration In Reinforcement Learning With Deep Predictive Models}, 
      author={Bradly C. Stadie and Sergey Levine and Pieter Abbeel},
      year={2015},
      eprint={1507.00814},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}
@article{weiss2016survey,
  title={A survey of transfer learning},
  author={Weiss, Karl and Khoshgoftaar, Taghi M and Wang, DingDing},
  journal={Journal of Big data},
  volume={3},
  number={1},
  pages={1--40},
  year={2016},
  publisher={SpringerOpen}
}
@article{parisi2021,
  title={Interesting Object, Curious Agent: Learning Task-Agnostic Exploration},
  author={Parisi, Simone and Dean, Victoria and Pathak, Deepak and Gupta, Abhinav},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  year={2021}
}
@article {Kirkpatrick3521,
	author = {Kirkpatrick, James and Pascanu, Razvan and Rabinowitz, Neil and Veness, Joel and Desjardins, Guillaume and Rusu, Andrei A. and Milan, Kieran and Quan, John and Ramalho, Tiago and Grabska-Barwinska, Agnieszka and Hassabis, Demis and Clopath, Claudia and Kumaran, Dharshan and Hadsell, Raia},
	title = {Overcoming catastrophic forgetting in neural networks},
	volume = {114},
	number = {13},
	pages = {3521--3526},
	year = {2017},
	doi = {10.1073/pnas.1611835114},
	publisher = {National Academy of Sciences},
	abstract = {Deep neural networks are currently the most successful machine-learning technique for solving a variety of tasks, including language translation, image classification, and image generation. One weakness of such models is that, unlike humans, they are unable to learn multiple tasks sequentially. In this work we propose a practical solution to train such models sequentially by protecting the weights important for previous tasks. This approach, inspired by synaptic consolidation in neuroscience, enables state of the art results on multiple reinforcement learning problems experienced sequentially.The ability to learn tasks in a sequential fashion is crucial to the development of artificial intelligence. Until now neural networks have not been capable of this and it has been widely thought that catastrophic forgetting is an inevitable feature of connectionist models. We show that it is possible to overcome this limitation and train networks that can maintain expertise on tasks that they have not experienced for a long time. Our approach remembers old tasks by selectively slowing down learning on the weights important for those tasks. We demonstrate our approach is scalable and effective by solving a set of classification tasks based on a hand-written digit dataset and by learning several Atari 2600 games sequentially.},
	issn = {0027-8424},
	eprint = {https://www.pnas.org/content/114/13/3521.full.pdf},
	journal = {Proceedings of the National Academy of Sciences}
}

@InProceedings{pmlr-v97-rakelly19a,
  title = 	 {Efficient Off-Policy Meta-Reinforcement Learning via Probabilistic Context Variables},
  author =       {Rakelly, Kate and Zhou, Aurick and Finn, Chelsea and Levine, Sergey and Quillen, Deirdre},
  booktitle = 	 {Proceedings of the 36th International Conference on Machine Learning},
  pages = 	 {5331--5340},
  year = 	 {2019},
  editor = 	 {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  volume = 	 {97},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {09--15 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v97/rakelly19a/rakelly19a.pdf},
  abstract = 	 {Deep reinforcement learning algorithms require large amounts of experience to learn an individual task. While meta-reinforcement learning (meta-RL) algorithms can enable agents to learn new skills from small amounts of experience, several major challenges preclude their practicality. Current methods rely heavily on on-policy experience, limiting their sample efficiency. They also lack mechanisms to reason about task uncertainty when adapting to new tasks, limiting their effectiveness on sparse reward problems. In this paper, we address these challenges by developing an off-policy meta-RL algorithm that disentangles task inference and control. In our approach, we perform online probabilistic filtering of latent task variables to infer how to solve a new task from small amounts of experience. This probabilistic interpretation enables posterior sampling for structured and efficient exploration. We demonstrate how to integrate these task variables with off-policy RL algorithms to achieve both meta-training and adaptation efficiency. Our method outperforms prior algorithms in sample efficiency by 20-100X as well as in asymptotic performance on several meta-RL benchmarks.}
}
@inproceedings{schmidhuber1991,
  title={A possibility for implementing curiosity and boredom in model-building neural controllers},
  author={Schmidhuber, J{\"u}rgen},
  booktitle={Proc. of the international conference on simulation of adaptive behavior: From animals to animats},
  pages={222--227},
  year={1991}
}
@article{ring1994,
  title={Continual learning in reinforcement environments},
  author={Ring, Mark Bishop and others},
  year={1994},
  publisher={Citeseer},
  journal={PhD thesis, University of Texas at Austin},
}
@inproceedings{DBLP,
  author={Andrei A. Rusu and Sergio Gomez Colmenarejo and Çaglar Gülçehre and Guillaume Desjardins and James Kirkpatrick and Razvan Pascanu and Volodymyr Mnih and Koray Kavukcuoglu and Raia Hadsell},
  title={Policy Distillation},
  year={2016},
  cdate={1451606400000},
  booktitle={ICLR (Poster)},
}
@inproceedings{10.1145/1160633.1160762,
author = {Fern\'{a}ndez, Fernando and Veloso, Manuela},
title = {Probabilistic Policy Reuse in a Reinforcement Learning Agent},
year = {2006},
isbn = {1595933034},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
doi = {10.1145/1160633.1160762},
abstract = {We contribute Policy Reuse as a technique to improve a reinforcement learning agent with guidance from past learned similar policies. Our method relies on using the past policies as a probabilistic bias where the learning agent faces three choices: the exploitation of the ongoing learned policy, the exploration of random unexplored actions, and the exploitation of past policies. We introduce the algorithm and its major components: an exploration strategy to include the new reuse bias, and a similarity function to estimate the similarity of past policies with respect to a new one. We provide empirical results demonstrating that Policy Reuse improves the learning performance over different strategies that learn without reuse. Interestingly and almost as a side effect, Policy Reuse also identifies classes of similar policies revealing a basis of core policies of the domain. We demonstrate that such a basis can be built incrementally, contributing the learning of the structure of a domain.},
booktitle = {Proceedings of the Fifth International Joint Conference on Autonomous Agents and Multiagent Systems},
pages = {720–727},
numpages = {8},
location = {Hakodate, Japan},
series = {AAMAS '06}
}
@inproceedings{NIPS2017_350db081,
 author = {Barreto, Andre and Dabney, Will and Munos, Remi and Hunt, Jonathan J and Schaul, Tom and van Hasselt, Hado P and Silver, David},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {I. Guyon and U. V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Successor Features for Transfer in Reinforcement Learning},
 volume = {30},
 year = {2017}
}
@inproceedings{Hansen2020Fast,
title={Fast Task Inference with Variational Intrinsic Successor Features},
author={Steven Hansen and Will Dabney and Andre Barreto and David Warde-Farley and Tom Van de Wiele and Volodymyr Mnih},
booktitle={International Conference on Learning Representations},
year={2020},
}
@book{sutton2018,
  title={Reinforcement learning: An introduction},
  author={Sutton, Richard S and Barto, Andrew G},
  year={2018},
  publisher={MIT press}
}
@article{GOUTA2017280,
title = {Generalized predictive control for a coupled four tank MIMO system using a continuous-discrete time observer},
journal = {ISA Transactions},
volume = {67},
pages = {280-292},
year = {2017},
issn = {0019-0578},
doi = {https://doi.org/10.1016/j.isatra.2016.11.021},
author = {Houssemeddine Gouta and Salim {Hadj Saïd} and Nabil Barhoumi and Faouzi M’Sahli},
keywords = {Predictive Control, Liquid level Control, Exponential stability, Sampled outputs, Continuous-discrete time observer, Experimental validation},
abstract = {This paper deals with the problem of the observer based control design for a coupled four-tank liquid level system. For this MIMO system's dynamics, motivated by a desire to provide precise and sensorless liquid level control, a nonlinear predictive controller based on a continuous-discrete observer is presented. First, an analytical solution from the model predictive control (MPC) technique is developed for a particular class of nonlinear MIMO systems and its corresponding exponential stability is proven. Then, a high gain observer that runs in continuous-time with an output error correction time that is updated in a mixed continuous-discrete fashion is designed in order to estimate the liquid levels in the two upper tanks. The effectiveness of the designed control schemes are validated by two tests; The first one is maintaining a constant level in the first bottom tank while making the level in the second bottom tank to follow a sinusoidal reference signal. The second test is more difficult and it is made using two trapezoidal reference signals in order to see the decoupling performance of the system’s outputs. Simulation and experimental results validate the objective of the paper.}
}
@article{recht2019,
  title={A tour of reinforcement learning: The view from continuous control},
  author={Recht, Benjamin},
  journal={Annual Review of Control, Robotics, and Autonomous Systems},
  volume={2},
  pages={253--279},
  year={2019},
  publisher={Annual Reviews}
}
@article{schulman2017proximal,
  title={Proximal policy optimization algorithms},
  author={Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
  journal={arXiv preprint arXiv:1707.06347},
  year={2017}
}
@article{kingma2014adam,
  title={Adam: A method for stochastic optimization},
  author={Kingma, Diederik P and Ba, Jimmy},
  journal={arXiv preprint arXiv:1412.6980},
  year={2014}
}
@inproceedings{rec1,
author = {Wang, Lu and Zhang, Wei and He, Xiaofeng and Zha, Hongyuan},
title = {Supervised Reinforcement Learning with Recurrent Neural Network for Dynamic Treatment Recommendation},
year = {2018},
isbn = {9781450355520},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
doi = {10.1145/3219819.3219961},
abstract = {Dynamic treatment recommendation systems based on large-scale electronic health records (EHRs) become a key to successfully improve practical clinical outcomes. Prior relevant studies recommend treatments either use supervised learning (e.g. matching the indicator signal which denotes doctor prescriptions), or reinforcement learning (e.g. maximizing evaluation signal which indicates cumulative reward from survival rates). However, none of these studies have considered to combine the benefits of supervised learning and reinforcement learning. In this paper, we propose Supervised Reinforcement Learning with Recurrent Neural Network (SRL-RNN), which fuses them into a synergistic learning framework. Specifically, SRL-RNN applies an off-policy actor-critic framework to handle complex relations among multiple medications, diseases and individual characteristics. The "actor'' in the framework is adjusted by both the indicator signal and evaluation signal to ensure effective prescription and low mortality. RNN is further utilized to solve the Partially-Observed Markov Decision Process (POMDP) problem due to lack of fully observed states in real world applications. Experiments on the publicly real-world dataset, i.e., MIMIC-3, illustrate that our model can reduce the estimated mortality, while providing promising accuracy in matching doctors' prescriptions.},
booktitle = {Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery \& Data Mining},
pages = {2447–2456},
numpages = {10},
keywords = {supervised reinforcement learning, deep sequential recommendation, dynamic treatment regime},
location = {London, United Kingdom},
series = {KDD '18}
}
@inproceedings{rec2,
author = {Chen, Shi-Yong and Yu, Yang and Da, Qing and Tan, Jun and Huang, Hai-Kuan and Tang, Hai-Hong},
title = {Stabilizing Reinforcement Learning in Dynamic Environment with Application to Online Recommendation},
year = {2018},
isbn = {9781450355520},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
doi = {10.1145/3219819.3220122},
abstract = {Deep reinforcement learning has shown great potential in improving system performance autonomously, by learning from iterations with the environment. However, traditional reinforcement learning approaches are designed to work in static environments. In many real-world problems, the environments are commonly dynamic, in which the performance of reinforcement learning approaches can degrade drastically. A direct cause of the performance degradation is the high-variance and biased estimation of the reward, due to the distribution shifting in dynamic environments. In this paper, we propose two techniques to alleviate the unstable reward estimation problem in dynamic environments, the stratified sampling replay strategy and the approximate regretted reward, which address the problem from the sample aspect and the reward aspect, respectively. Integrating the two techniques with Double DQN, we propose the Robust DQN method. We apply Robust DQN in the tip recommendation system in Taobao online retail trading platform. We firstly disclose the highly dynamic property of the recommendation application. We then carried out online A/B test to examine Robust DQN. The results show that Robust DQN can effectively stabilize the value estimation and, therefore, improves the performance in this real-world dynamic environment.},
booktitle = {Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery \& Data Mining},
pages = {1187–1196},
numpages = {10},
keywords = {dynamic environment, reinforcement learning, recommendation, approximate regretted reward, stratified sampling replay},
location = {London, United Kingdom},
series = {KDD '18}
}
@inproceedings{ref3,
author = {Zhao, Xiangyu and Zhang, Liang and Ding, Zhuoye and Xia, Long and Tang, Jiliang and Yin, Dawei},
title = {Recommendations with Negative Feedback via Pairwise Deep Reinforcement Learning},
year = {2018},
isbn = {9781450355520},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
doi = {10.1145/3219819.3219886},
abstract = {Recommender systems play a crucial role in mitigating the problem of information overload by suggesting users' personalized items or services. The vast majority of traditional recommender systems consider the recommendation procedure as a static process and make recommendations following a fixed strategy. In this paper, we propose a novel recommender system with the capability of continuously improving its strategies during the interactions with users. We model the sequential interactions between users and a recommender system as a Markov Decision Process (MDP) and leverage Reinforcement Learning (RL) to automatically learn the optimal strategies via recommending trial-and-error items and receiving reinforcements of these items from users' feedback. Users' feedback can be positive and negative and both types of feedback have great potentials to boost recommendations. However, the number of negative feedback is much larger than that of positive one; thus incorporating them simultaneously is challenging since positive feedback could be buried by negative one. In this paper, we develop a novel approach to incorporate them into the proposed deep recommender system (DEERS) framework. The experimental results based on real-world e-commerce data demonstrate the effectiveness of the proposed framework. Further experiments have been conducted to understand the importance of both positive and negative feedback in recommendations.},
booktitle = {Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery \& Data Mining},
pages = {1040–1048},
numpages = {9},
keywords = {pairwise deep Q-network, recommender system, deep reinforcement learning},
location = {London, United Kingdom},
series = {KDD '18}
}

@inproceedings{yu2018towards,
  title={Towards Sample Efficient Reinforcement Learning.},
  author={Yu, Yang},
  booktitle={IJCAI},
  pages={5739--5743},
  year={2018}
}

@inbook{ref4,
author = {Xu, Zhi and Liu, Shuncheng and Wu, Ziniu and Chen, Xu and Zeng, Kai and Zheng, Kai and Su, Han},
title = {PATROL: A Velocity Control Framework for Autonomous Vehicle via Spatial-Temporal Reinforcement Learning},
year = {2021},
isbn = {9781450384469},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {The largest portion of urban congestion is caused by 'phantom' traffic jams, causing significant delay travel time, fuel waste, and air pollution. It frequently occurs in high-density traffics without any obvious signs of accidents or roadworks. The root cause of 'phantom' traffic jams in one-lane traffics is the sudden change in velocity of some vehicles (i.e. harsh driving behavior (HDB)), which may generate a chain reaction with accumulated impact throughout the vehicles along the lane. This paper makes the first attempt to address this notorious problem in a one-lane traffic environment through velocity control of autonomous vehicles. Specifically, we propose a velocity control framework, called PATROL (sPAtial-temporal ReinfOrcement Learning). First, we design a spatial-temporal graph inside the reinforcement learning model to process and extract the information (e.g. velocity and distance difference) of multiple vehicles ahead across several historical time steps in the interactive environment. Then, we propose an attention mechanism to characterize the vehicle interactions and an LSTM structure to understand the vehicles' driving patterns through time. At last, we modify the reward function used in previous velocity control works to enable the autonomous driving agent to predict the HDB of preceding vehicles and smoothly adjust its velocity, which could alleviate the chain reaction caused by HDB. We conduct extensive experiments to demonstrate the effectiveness and superiority of PATROL in alleviating the 'phantom' traffic jam in simulation environments. Further, on the real-world velocity control dataset, our method significantly outperforms the existing methods in terms of driving safety, comfortability, and efficiency.},
booktitle = {Proceedings of the 30th ACM International Conference on Information &amp; Knowledge Management},
pages = {2271–2280},
numpages = {10}
}
@inproceedings{10.1145/3383455.3422540,
author = {Yang, Hongyang and Liu, Xiao-Yang and Zhong, Shan and Walid, Anwar},
title = {Deep Reinforcement Learning for Automated Stock Trading: An Ensemble Strategy},
year = {2020},
isbn = {9781450375849},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
doi = {10.1145/3383455.3422540},
abstract = {Stock trading strategies play a critical role in investment. However, it is challenging to design a profitable strategy in a complex and dynamic stock market. In this paper, we propose an ensemble strategy that employs deep reinforcement schemes to learn a stock trading strategy by maximizing investment return. We train a deep reinforcement learning agent and obtain an ensemble trading strategy using three actor-critic based algorithms: Proximal Policy Optimization (PPO), Advantage Actor Critic (A2C), and Deep Deterministic Policy Gradient (DDPG). The ensemble strategy inherits and integrates the best features of the three algorithms, thereby robustly adjusting to different market situations. In order to avoid the large memory consumption in training networks with continuous action space, we employ a load-on-demand technique for processing very large data. We test our algorithms on the 30 Dow Jones stocks that have adequate liquidity. The performance of the trading agent with different reinforcement learning algorithms is evaluated and compared with both the Dow Jones Industrial Average index and the traditional min-variance portfolio allocation strategy. The proposed deep ensemble strategy is shown to outperform the three individual algorithms and two baselines in terms of the risk-adjusted return measured by the Sharpe ratio.},
booktitle = {Proceedings of the First ACM International Conference on AI in Finance},
articleno = {31},
numpages = {8},
keywords = {deep reinforcement learning, markov decision process, ensemble strategy, actor-critic framework, automated stock trading},
location = {New York, New York},
series = {ICAIF '20}
}
@misc{sung2017learning,
      title={Learning to Learn: Meta-Critic Networks for Sample Efficient Learning}, 
      author={Flood Sung and Li Zhang and Tao Xiang and Timothy Hospedales and Yongxin Yang},
      year={2017},
      eprint={1706.09529},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@article{Cheng_Orosz_Murray_Burdick_2019, title={End-to-End Safe Reinforcement Learning through Barrier Functions for Safety-Critical Continuous Control Tasks}, volume={33}, DOI={10.1609/aaai.v33i01.33013387}, abstractNote={&lt;p&gt;Reinforcement Learning (RL) algorithms have found limited success beyond simulated applications, and one main reason is the absence of safety guarantees &lt;em&gt;during&lt;/em&gt; the learning process. Real world systems would realistically fail or break before an optimal controller can be learned. To address this issue, we propose a controller architecture that combines (1) a model-free RL-based controller with (2) model-based controllers utilizing control barrier functions (CBFs) and (3) online learning of the unknown system dynamics, in order to ensure safety during learning. Our general framework leverages the success of RL algorithms to learn high-performance controllers, while the CBF-based controllers both &lt;em&gt;guarantee&lt;/em&gt; safety and &lt;em&gt;guide&lt;/em&gt; the learning process by constraining the set of explorable polices. We utilize Gaussian Processes (GPs) to model the system dynamics and its uncertainties.&lt;/p&gt; &lt;p&gt;Our novel controller synthesis algorithm, RL-CBF, guarantees safety with high probability during the learning process, regardless of the RL algorithm used, and demonstrates greater policy exploration efficiency. We test our algorithm on (1) control of an inverted pendulum and (2) autonomous carfollowing with wireless vehicle-to-vehicle communication, and show that our algorithm attains much greater sample efficiency in learning than other state-of-the-art algorithms &lt;em&gt;and&lt;/em&gt; maintains safety during the entire learning process.&lt;/p&gt;}, number={01}, journal={Proceedings of the AAAI Conference on Artificial Intelligence}, author={Cheng, Richard and Orosz, Gábor and Murray, Richard M. and Burdick, Joel W.}, year={2019}, month={Jul.}, pages={3387-3395} }
@inproceedings{NEURIPS2020_8df6a659,
 author = {Turchetta, Matteo and Kolobov, Andrey and Shah, Shital and Krause, Andreas and Agarwal, Alekh},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M. F. Balcan and H. Lin},
 pages = {12151--12162},
 publisher = {Curran Associates, Inc.},
 title = {Safe Reinforcement Learning via Curriculum Induction},
 volume = {33},
 year = {2020}
}
@inproceedings{NEURIPS2020_448d5eda,
 author = {Anderson, Greg and Verma, Abhinav and Dillig, Isil and Chaudhuri, Swarat},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M. F. Balcan and H. Lin},
 pages = {6172--6183},
 publisher = {Curran Associates, Inc.},
 title = {Neurosymbolic Reinforcement Learning with Formally Verified Exploration},
 volume = {33},
 year = {2020}
}

@article{aastrom2007feedback,
  title={Feedback systems},
  author={{\AA}str{\"o}m, Karl Johan and Murray, Richard M},
  journal={An Introduction for Scientists and Engineers, Karl Johan {\AA}str{\"o}m and Richard M. Murray},
  year={2007},
  publisher={Citeseer}
}

@inproceedings{finn2017model,
  title={Model-agnostic meta-learning for fast adaptation of deep networks},
  author={Finn, Chelsea and Abbeel, Pieter and Levine, Sergey},
  booktitle={International conference on machine learning},
  pages={1126--1135},
  year={2017},
  organization={PMLR}
}
@article{narvekar2020curriculum,
  title={Curriculum learning for reinforcement learning domains: A framework and survey},
  author={Narvekar, Sanmit and Peng, Bei and Leonetti, Matteo and Sinapov, Jivko and Taylor, Matthew E and Stone, Peter},
  journal={arXiv preprint arXiv:2003.04960},
  year={2020}
}
@inproceedings{luo2020accelerating,
  title={Accelerating reinforcement learning for reaching using continuous curriculum learning},
  author={Luo, Sha and Kasaei, Hamidreza and Schomaker, Lambert},
  booktitle={2020 International Joint Conference on Neural Networks (IJCNN)},
  pages={1--8},
  year={2020},
  organization={IEEE}
}
@article{Dann_Zambetta_Thangarajah_2019, title={Deriving Subgoals Autonomously to Accelerate Learning in Sparse Reward Domains}, volume={33}, DOI={10.1609/aaai.v33i01.3301881}, abstractNote={&lt;p&gt;Sparse reward games, such as the infamous &lt;em&gt;Montezuma’s Revenge&lt;/em&gt;, pose a significant challenge for Reinforcement Learning (RL) agents. Hierarchical RL, which promotes efficient exploration via subgoals, has shown promise in these games. However, existing agents rely either on human domain knowledge or slow autonomous methods to derive suitable subgoals. In this work, we describe a new, autonomous approach for deriving subgoals from raw pixels that is more efficient than competing methods. We propose a novel intrinsic reward scheme for exploiting the derived subgoals, applying it to three &lt;em&gt;Atari&lt;/em&gt; games with sparse rewards. Our agent’s performance is comparable to that of state-of-the-art methods, demonstrating the usefulness of the subgoals found.&lt;/p&gt;}, number={01}, journal={Proceedings of the AAAI Conference on Artificial Intelligence}, author={Dann, Michael and Zambetta, Fabio and Thangarajah, John}, year={2019}, month={Jul.}, pages={881-889} }

@inproceedings{ijcai2019-324,
  title     = {Automatic Successive Reinforcement Learning with Multiple Auxiliary Rewards},
  author    = {Fu, Zhao-Yang and Zhan, De-Chuan and Li, Xin-Chun and Lu, Yi-Xing},
  booktitle = {Proceedings of the Twenty-Eighth International Joint Conference on
               Artificial Intelligence, {IJCAI-19}},
  publisher = {International Joint Conferences on Artificial Intelligence Organization},             
  pages     = {2336--2342},
  year      = {2019},
  month     = {7},
  doi       = {10.24963/ijcai.2019/324},
}
@inproceedings{albe,
author = {Liu, Yiming and Hu, Zheng},
title = {The Guiding Role of Reward Based on Phased Goal in Reinforcement Learning},
year = {2020},
isbn = {9781450376426},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
doi = {10.1145/3383972.3384039},
abstract = {Sparse and delayed rewards have greatly hindered the deep reinforcement learning, which is supposed to acquire the optimal policy by learning from trajectories. Reward shaping, which has previously been introduced to accelerate learning, is one of the most effective methods to tackle this crucial yet challenging problem. However, how to reasonably implement reward shaping needs to be explored. Currently, the method of reward shaping usually requires a large number of expert demonstrations, and the environment is poorly explored. In this paper, we proposed a method of reward shaping---Reinforcement learning framework based on phased goal, which will accelerate learning convergence speed with less expert examples and explore better especially for tasks where environment rewards are particularly sparse. The framework consists of reward based on phased goal and policy learning using PPO2. The process of acquiring designed reward is divided into stage classification and calculation of goal proximity. Experiments proved that our method can effectively alleviate the problem of sparse reward and obtain higher scores in Atari game than basic algorithm.},
booktitle = {Proceedings of the 2020 12th International Conference on Machine Learning and Computing},
pages = {535–541},
numpages = {7},
keywords = {Reinforcement learning, Phased goal, Goal proximity, Reward shaping},
location = {Shenzhen, China},
series = {ICMLC 2020}
}

@inproceedings{2021lipschitz,
  title={Lipschitz lifelong reinforcement learning},
  author={Lecarpentier, Erwan and Abel, David and Asadi, Kavosh and Jinnai, Yuu and Rachelson, Emmanuel and Littman, Michael L},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={35},
  pages={8270--8278},
  year={2021}
}
@inproceedings{2018abel,
  title={Policy and value transfer in lifelong reinforcement learning},
  author={Abel, David and Jinnai, Yuu and Guo, Sophie Yue and Konidaris, George and Littman, Michael},
  booktitle={International Conference on Machine Learning},
  pages={20--29},
  year={2018},
  organization={PMLR}
}
@inproceedings{2020sequential,
  title={Sequential transfer in reinforcement learning with a generative model},
  author={Tirinzoni, Andrea and Poiani, Riccardo and Restelli, Marcello},
  booktitle={International Conference on Machine Learning},
  pages={9481--9492},
  year={2020},
  organization={PMLR}
}
@article{strehl2009,
  title={Reinforcement Learning in Finite MDPs: PAC Analysis.},
  author={Strehl, Alexander L and Li, Lihong and Littman, Michael L},
  journal={Journal of Machine Learning Research},
  volume={10},
  number={11},
  year={2009}
}