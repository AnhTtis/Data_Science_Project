\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}

\usepackage{multirow}
\usepackage{adjustbox}
\usepackage{tabularx}
\usepackage{booktabs}
\usepackage{enumitem}


\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

% \title{Identity-Preserving Knowledge Distillation for Cross-resolution Face Recognition\\
\title{Cross-resolution Face Recognition via Identity-Preserving Network and Knowledge Distillation\\
% {\footnotesize \textsuperscript{*}Note: Sub-titles are not captured in Xplore and
% should not be used}
\thanks{Support from the Swiss National Science
Foundation (SNSF) 20CH21\_195532 for XAIface
CHIST-ERA-19-XAI-011 is acknowledged.}
% \thanks{Identify applicable funding agency here. If none, delete this.}
}

\author{\IEEEauthorblockN{Yuhang Lu, Touradj Ebrahimi}
\IEEEauthorblockA{Multimedia Signal Processing Group (MMSPG)\\
\'Ecole Polytechnique F\'ed\'erale de Lausanne (EPFL)
% ~
}}

\maketitle

\begin{abstract}
Cross-resolution face recognition has become a challenging problem for modern deep face recognition systems. It aims at matching a low-resolution probe image with high-resolution gallery images registered in a database. Existing methods mainly leverage prior information from high-resolution images by either reconstructing facial details with super-resolution techniques or learning a unified feature space. 
To address this challenge, this paper proposes a new approach that enforces the network to focus on the discriminative information stored in the low-frequency components of a low-resolution image. 
A cross-resolution knowledge distillation paradigm is first employed as the learning framework. Then, an identity-preserving network, WaveResNet, and a wavelet similarity loss are designed to capture low-frequency details and boost performance. Finally, an image degradation model is conceived to simulate more realistic low-resolution training data. Consequently, extensive experimental results show that the proposed method consistently outperforms the baseline model and other state-of-the-art methods across a variety of image resolutions.

% In this paper, a new approach is proposed to address this issue, which enforces the network to focus on the discriminative information stored in the low-frequency components of the LR image. 
\end{abstract}

\begin{IEEEkeywords}
Low resolution, Face recognition, Knowledge distillation
\end{IEEEkeywords}

\section{Introduction}
\label{sec:intro}
In the past decades, face recognition (FR) has founds its way in many everyday applications. Current state-of-the-art deep learning-based face recognition systems achieve near-perfect performance on well-known public face recognition benchmarks such as LFW \cite{huang2008labeled} and MegaFace \cite{kemelmacher2016megaface}.
However, these face datasets are primarily collected in controlled environments and in high resolution, which quite differ from face images captured by real-world devices. 
In fact, studies \cite{zou2011very, cheng2019low, grm2018strengths,knoche2021image,lu2022novel} have demonstrated a significant performance deterioration of the most advanced deep face recognition systems in presence of resolution discrepancies. 
% in low-resolution scenarios.
In this work, we mainly focus on the problem of cross-resolution face recognition (CRFR), which intends to match low-resolution (LR) probe images with high-resolution (HR) gallery images in a database. 
% (``low-resolution'' in some work)

Most existing approaches to cope with CRFR can be divided into two categories. 
In the first category, HR images are reconstructed from LR images with face super-resolution (FSR) techniques \cite{zhang2018super, hsu2019sigan, kong2019cross, lai2019low, yin2020fan}, which are then recognized by a face recognition model. Although FSR methods can generate missing information, even facial details, they are mainly optimized for visual appearance and often ignore and even alter crucial identity information. This results in limited improvement of performance in LR domains. Furthermore, the high computational cost of the FSR module during both training and inference lays an additional burden on the entire face recognition system and heavily impairs its efficiency.

Different from FSR-based approaches, the second category converts LR and corresponding HR faces into a unified resolution-invariant feature space. These approaches rely fully on the identity information and learn a discriminative representation. Earlier work \cite{yang2017discriminative} leveraged a multidimensional scaling approach to learn a mapping matrix. Lu et al. \cite{lu2018deep} proposed a deep coupled ResNet model with two additional branch networks to map coupled HR and LR features to a common space.
Zangeneh et al. \cite{zangeneh2020low} directly employed a two-branch structure DCNNs to learn a non-linear feature transformation. 
\cite{kim2021quality} conceived an invertible decoder and learned a quality-agnostic model. 
\cite{zha2019tcn,lai2021deep,knoche2022octuplet} tackled the problem with a metric learning approach. They were all built on triplet loss and learned to reduce the resolution gap by pulling together positive HR-LR pairs and pushing away dissimilar ones. 

%%%%%%%%%%%%%%% KD Framework %%%%%%%%%%%%%%%%
\begin{figure*}[t]
	\centering
	\begin{adjustbox}{width=0.95\textwidth}
    \includegraphics[]{Framework.pdf}
	\end{adjustbox}
	\caption{The architecture of the proposed knowledge distillation framework and the identity-preserving student network.}
	\label{fig:kdframework}
\end{figure*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Knowledge distillation is a typical approach that builds resolution-invariant feature space by distilling HR domain knowledge to the LR domain. This idea was first proposed in \cite{44873} to transfer knowledge from a high-performing but computationally expensive teacher network into a simpler student network. 
Recent studies \cite{zhu2019low,ge2018low,ge2020look, massoli2020cross,feng2021resolution,shin2022teaching,huang2022feature} have shown the potential of this approach in solving recognition problems in low-resolution domains. For instance, Zhu et al. \cite{zhu2019low} and Huang et al. \cite{huang2022feature} addressed the low-resolution object recognition problem with the teacher-student learning paradigm. 
% Their proposed methods simultaneously optimize the recognition loss and distillation loss and manage to distill valuable information from a deep model pre-trained on high-resolution data. 
Authors in \cite{ge2018low, wang2019improved,feng2021resolution} developed efficient low-resolution face recognition models at low computational cost by distilling informative facial features from teacher to a lightweight student network. 
% \cite{massoli2020cross} simultaneously optimized the face recognition loss and distillation loss to distill information from a deep model pre-trained on HR data. 
Ge et al. \cite{ge2020look} obtained better performance in CRFR by distilling structural relationships across teacher and student networks. 
More recently, \cite{shin2022teaching} performed an attention similarity knowledge distillation. Instead of the feature map, they transferred attention maps obtained from the teacher network into a student network to boost performance in the LR domain. 
In this paper, a cross-resolution knowledge distillation framework is first employed, where the targeted student network is trained with multi-scale LR data and optimized with both face recognition and distillation losses.

Despite the guidance of the prior knowledge extracted from HR face images, the large resolution disparity between HR and LR images makes it difficult for the student network to capture informative features. Frequency analysis in \cite{knoche2021image, li2020wavelet} has shown that the high-frequency information in a facial image, such as edge and noise, is eliminated during the resolution reduction, while the low-frequency subbands still preserve the most discriminative features. 
Thus, this work proposes an identity-preserving network, WaveResNet, to capture the discriminative information stored in low-frequency components of the LR images. It is adapted from ResNet \cite{he2016deep} by replacing the pooling and stride convolution layers with a low-pass filter based on Discrete Wavelet Transform (DWT). 
The high-frequency subbands of the intermediate feature maps are filtered out to remove ambiguous and noisy information and enforce the network to focus on the more discriminative low-frequency information. 
In addition, a wavelet similarity loss is designed as an auxiliary distillation loss in order to further enhance attention in low-frequency subbands. 
Moreover, a degradation model is designed to simulate real-world LR training data and develop a more robust recognition system. The proposed method has been evaluated on four datasets in a variety of resolutions and it outperforms the baseline model and some other related solutions.
% Compared to the baseline and some other related work, it outperforms in most resolution settings. 

% In summary, the following contribution has been made in this paper.
% % \setlist{nolistsep}
% \begin{itemize}%[noitemsep]
%     \item An identity-preserving network (WaveResNet) is proposed to preserve the identity information stored in low-frequency subbands of LR face images, leading to performance improvement
%     \item This work employs a cross-resolution knowledge distillation framework to better fit the LRFR task and additionally designs a wavelet similarity loss to enhance attention on low-frequency components
%     \item A more practical degradation model is conceived for more realistic low-resolution image synthesis
% \end{itemize}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Method}
\label{sec:method}

\subsection{Problem Definition}
This paper mainly describes and resolves the cross-resolution face recognition (CRFR) problem, where the probe images are LR due to the limited definition of the camera or the large distance between the camera and the subject, while the gallery images registered in the database are all of higher quality and resolution. 
% The LR images are synthesized in a way that HR images are first downsampled to the target scale and then upsampled back to the original size. 
In the testing phase, one focuses on the face verification task and examines the matching between an LR probe image and an HR gallery image. 

\subsection{Identity Preserving Network}
% The previous FSR-based methods for cross-resolution face recognition aims at recovering high-frequency details in low-resolution images for identity matching. On the contrary, we focus on the information stored in the low-frequency domain. The main insight is that 

Different from FSR-based methods which aim at recovering high-frequency details for identity matching, this paper proposes to focus on the information stored in the low-frequency domain and directly mines deep identity features from LR training data. The main insight is that the high-frequency details are eliminated after the resolution reduction while the low-frequency components in LR images contain more discriminative information. 
In this subsection, an identity-preserving network, WaveResNet, is introduced for this purpose. The idea is to remove the ambiguous and noisy high-frequency information and enhance the discriminative features in LR images during the training process. Ideally, it performs more accurate recognition across various image resolutions. 

% the proposed method mainly focuses on the information stored in the low-frequency domain. 
% In this subsection, an identity-preserving network, WaveResNet, is introduced to mine the deep identity features directly from LR training data. 
% Different from FSR-based methods which recover high-frequency details for identity matching, the proposed method mainly focuses on the information stored in the low-frequency domain and directly mines deep identity features from LR training data. 

% The main insight is that the low-frequency components preseve 


In detail, as shown in Fig.~\ref{fig:kdframework}, a low-pass convolutional filter based on Discrete Wavelet Transformation (DWT) is embedded into ResNet, denoted as WaveConv. It replaces the Maxpooling and stride convolution operations. Given an input image $\boldsymbol{x}$, a low-pass filter $\boldsymbol{f_{LL}}$ based on 2D DWT converts $\boldsymbol{x}$ into its low-frequency subband image $\boldsymbol{x_{LL}}$. The filter itself is a stride 2 convolutional operator during the transformation and automatically downsamples the image by a factor 2. The embedded operation in the WaveConv layer is defined as $\boldsymbol{x_{LL}}=(\boldsymbol{f_{LL}} \circledast \boldsymbol{x})\downarrow_{2}$, where $\circledast$ refers to convolution operator and $\downarrow_{2}$ means downsampling by 2. 

\subsection{Knowledge Distillation Framework for CRFR}

\subsubsection{Face Recognition Framework}
Fig. \ref{fig:kdframework} illustrates the knowledge distillation framework for the CRFR task. The teacher model is built on the ResNet network. Different from many teacher-student frameworks where the student model is a much simpler network for the sake of efficiency, our student network utilizes the proposed WaveResNet with the same amount of parameters to pursue high representation capability in both HR and LR data. Under this framework, the teacher network is first trained on HR images and learns to extract rich and informative facial details from high-quality training data. Then, cross-resolution distillation adapts the knowledge of discriminative features to the student network, which is trained on multi-scale LR data. 
% \textbf{Consequently, the resulting student model is deployed to recognize both high-resolution and low-resolution faces. }

\subsubsection{Losses} 
Under the framework of knowledge distillation, the following loss functions have been conceived and employed.

\noindent\textbf{Recognition Loss:}
The popular ArcFace \cite{deng2019arcface} loss is employed by both teacher and student networks as a recognition loss to learn the discriminative power.
% The identity loss [] is formulated as follows.
\begin{align}
    \mathcal{L}_{arcface}=-\frac{1}{N} \sum_{i=1}^N \log \frac{e^{s\left(\cos \left(\theta_{y_i}+m\right)\right)}}{e^{s\left(\cos \left(\theta_{y_i}+m\right)\right)+\sum_{j=1, j \neq y_i}^n e^{s\left(\cos \left(\theta_j\right)\right)}}}.
\end{align}

\noindent\textbf{Cross-resolution Distillation Loss:}
In the training stage, the knowledge from the teacher network is transferred to the student model with a distillation loss. 
In order to improve the performance of the student network on different sizes of LR data, the distillation process is designed in a way to enforce a constraint over features across variant resolutions in one unified feature space. During the training, multi-scale versions of LR training data is collected and a cross-resolution distillation loss is applied to minimize the discrepancy between HR and LR features.
Specifically, given a pair of training samples, HR image $\boldsymbol{x_H}$ and LR image $\boldsymbol{x_L}$ of random size $s$, they are respectively passed into the teacher and student networks including classification layers to obtain the logits $\boldsymbol{z_H}$ and $\boldsymbol{z^s_L}$ and to calculate the loss. 
The distillation loss is expressed as:
\begin{align}
    \mathcal{L}_{distill} = \frac{1}{N} \sum_{i=1}^N  T^2 \mathcal{L}_{K L}\left(\sigma\left(\frac{z_H}{T}\right), \sigma\left(\frac{z^s_L}{T}\right)\right),
\end{align}
where $\mathcal{L}_{K L}$ refers to the KL Divergence, T is the temperature parameter to smooth the distillation loss, and $\sigma(\cdot)$ refers to the softmax function.

\noindent\textbf{Wavelet Similarity Loss:}
An additional auxiliary loss on the intermediate feature maps is introduced to further enhance the attention on low-frequency components, namely wavelet similarity loss. It enforces the student network to learn more discriminative knowledge stored in low-frequency features from the teacher network.
% distill more knowledge about low-frequency features to the student network, which learns to pay more attention to identity information stored in low-frequency subbands. 
First, the feature maps from both teacher and student streams are spotted and then decomposed into multiple frequency bands by DWT. Afterward, MSELoss is applied to the low-frequency components only. The formula of the proposed wavelet similarity loss is as follows.
\begin{align}
    \mathcal{L}_{wavesim} = \sum_{k=1}^2 \lVert f_{LL}(z_H^k) - f_{LL}(z_L^k)  \rVert_2^2,
\end{align}
where $z^k$ means the intermediate feature in the $k^{th}$ stage of ResNet and $f_{LL}$ refers to the DWT-based low-pass filter.

The total loss is a weighted combination of recognition loss, distillation loss, and wavelet similarity loss.
\begin{align}
\mathcal{L}_{total} = \mathcal{L}_{arcface}+\lambda_{1} \mathcal{L}_{distill} + \lambda_{2} \mathcal{L}_{wavesim}.
\end{align}

%%%%%%%%%%%%%%% Degradation Model %%%%%%%%%%%%%%%%
\begin{figure}[t]
	\centering
    \includegraphics[width=\linewidth]{DegradationModel.pdf}
	\caption{Data degradation model to produce realistic LR data.}
	\label{fig:degrade}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Degradation Model for LR Data Synthesis}
In the proposed learning framework, the student network is trained on synthesized low-resolution data. In order to develop a robust recognition system, the synthesized LR training data should not deviate much from those captured in the real world. 
% Similar ideas were explored in other applications \cite{pei2019effects} to improve the robustness of an image classifier. 
% Similar ideas were adopted in other applications \cite{lu2022novel} to improve the robustness of a detector.
% those in real-world scenarios. 
Previous studies in the CRFR task tend to add Gaussian blur before downsampling to better simulate the low-resolution effect on images. In more realistic scenarios, LR images captured by surveillance cameras are often accompanied by random motion blur, noise, and compression artifacts. 
This paper hand-designs a degradation model to produce LR face images that are closer to real-world data. As depicted in Fig. \ref{fig:degrade}, the HR image is first randomly corrupted by blur operation, synthetic noise, and JPEG compression artifacts. During experiments, the probability of applying each corruption in the degradation model is set to 0.5. Afterward, the data is downsampled into selected sizes by bicubic operation. Some examples are visualized in Fig. \ref{fig:kdframework}.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Performance report %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Table generated by Excel2LaTeX from sheet 'Sheet2'

\begin{table*}[t]
  \centering
  \caption{Verification accuracy (\%) of the proposed method on multiple datasets of different resolutions. Degradation means the degradation model. KD refers to the proposed knowledge distillation framework. WaveSim refers to the auxiliary wavelet similarity loss. 
  \textcolor[rgb]{ 1,  0,  0}{Red color} denotes the highest score and \textcolor[rgb]{ .267,  .447,  .769}{Blue color} denotes the second highest score. 
  }
    \begin{adjustbox}{width=0.85\textwidth}
    \begin{tabular}{cccc|cccc|c}
    \toprule
    \multicolumn{4}{c|}{Methods}          & \multicolumn{4}{c|}{Avg on $\cup$\{LFW,AgeDB,CPLFW,CALFW\}} & \multirow{2}[2]{*}{\shortstack{Overall\\ Average}} \\
    \cmidrule{1-8}
    Backbone  &  Degradation    & KD & WaveSim & \; 14x14 & 28x28 & 56x56 & 112x112\scriptsize(HR) &  \\
    \midrule
    % ResNet &       &       &       &       & 57.04 & 73.96 & 92.42 & \textcolor[rgb]{1,0,0}{95.38} & 79.70 \\
    ResNet       &       &       &       & 84.92 & \textcolor[rgb]{.267,.447,.769}{93.23} & 94.14 & 94.13 & 91.61 \\
    % ResNet & \checkmark      &  \checkmark     &       & \checkmark      & 75.76 & 87.85 & \textbf{93.28} & 94.24 & 87.78 \\
    WaveResNet       &       &       &       & 86.73 & 92.86 & 94.17 & 94.17 & 91.98 \\
    WaveResNet       & \checkmark      &       &       & 87.77 & 92.55 & 93.31 & 93.30 & 91.73 \\
    WaveResNet       & \checkmark      & \checkmark      &       & \textcolor[rgb]{1,0,0}{88.31}  & 93.18  & \textcolor[rgb]{.267,.447,.769}{94.21} & \textcolor[rgb]{.267,.447,.769}{94.31} & \textcolor[rgb]{.267,.447,.769}{92.50} \\
    WaveResNet       & \checkmark      & \checkmark      & \checkmark      & \textcolor[rgb]{.267,.447,.769}{88.30} & \textcolor[rgb]{1,0,0}{93.25} & \textcolor[rgb]{1,0,0}{94.33} & \textcolor[rgb]{1,0,0}{94.47} & \textcolor[rgb]{1,0,0}{92.59} \\
    \bottomrule
    \end{tabular}%
    \end{adjustbox}
  \label{tab:mixdataset}%
\end{table*}%


% \begin{table*}[t]
%   \centering
%   \caption{Verification accuracy of the proposed method on multiple datasets of different resolutions. Degradation means the degradation model. KD refers to the proposed knowledge distillation framework. WaveSim refers to the auxiliary wavelet similarity loss. 
%   \textcolor[rgb]{ 1,  0,  0}{Red color} denotes the highest score and \textcolor[rgb]{ .267,  .447,  .769}{Blue color} denotes the second highest score. 
%   }
%     \begin{adjustbox}{width=0.9\textwidth}
%     \begin{tabular}{cccc|cccc|c}
%     \toprule
%     \multicolumn{4}{c|}{Methods}          & \multicolumn{4}{c|}{Avg on $\cup$\{LFW,AgeDB,CPLFW,CALFW\}} & \multirow{2}[2]{*}{\shortstack{Overall\\ Average}} \\
%     \cmidrule{1-8}
%     Backbone  &  Degradation    & KD & WaveSim & \;7x7   & 14x14 & 28x28 & 112x112\scriptsize(HR) &  \\
%     \midrule
%     % ResNet &       &       &       &       & 57.04 & 73.96 & 92.42 & \textcolor[rgb]{1,0,0}{95.38} & 79.70 \\
%     ResNet       &       &       &       & 76.37 & 84.92 & \textcolor[rgb]{.267,.447,.769}{93.23} & 94.13 & 87.16 \\
%     % ResNet & \checkmark      &  \checkmark     &       & \checkmark      & 75.76 & 87.85 & \textbf{93.28} & 94.24 & 87.78 \\
%     WaveResNet       &       &       &       & \textcolor[rgb]{.267,.447,.769}{76.49} & 86.73 & 92.86 & 94.17 & 87.56 \\
%     WaveResNet       & \checkmark      &       &       & \textcolor[rgb]{1,0,0}{76.95} & 87.77 & 92.55 & 93.30 & 87.64 \\
%     WaveResNet       & \checkmark      & \checkmark      &       & 75.41 & \textcolor[rgb]{1,0,0}{88.31} & 93.18 & 94.31 & \textcolor[rgb]{.267,.447,.769}{87.80} \\
%     WaveResNet       & \checkmark      & \checkmark      & \checkmark      & 76.41 & \textcolor[rgb]{.267,.447,.769}{88.30} & \textcolor[rgb]{1,0,0}{93.25} & \textcolor[rgb]{.267,.447,.769}{94.47} & \textcolor[rgb]{1,0,0}{88.11} \\
%     \bottomrule
%     \end{tabular}%
%     \end{adjustbox}
%   \label{tab:mixdataset}%
% \end{table*}%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{table*}[t]
%   \centering
%   \caption{Verification accuracy of the proposed method on multiple datasets of different resolutions. Multi-scale refers to training with data in different resolutions. Degradation means the degradation model. KD refers to the proposed knowledge distillation framework. WaveSim refers to the auxiliary wavelet similarity loss. 
%   \textcolor[rgb]{ 1,  0,  0}{Red color} denotes the highest score and \textcolor[rgb]{ .267,  .447,  .769}{Blue color} denotes the second highest score. 
%   }
%     \begin{adjustbox}{width=0.9\textwidth}
%     \begin{tabular}{ccccc|p{30pt}p{30pt}p{30pt}p{30pt}|c}
%     \toprule
%     \multicolumn{5}{c|}{Methods}          & \multicolumn{4}{c|}{Avg on $\cup$\{LFW,AgeDB,CPLFW,CALFW\}} & \multirow{2}[2]{*}{\shortstack{Overall\\ Average}} \\
%     \cmidrule{1-9}
%     Backbone  & Multi-scale & Degradation    & KD & WaveSim & \;7x7   & 14x14 & 28x28 & 112x112\scriptsize(HR) &  \\
%     \midrule
%     ResNet &       &       &       &       & 57.04 & 73.96 & 92.42 & \textcolor[rgb]{1,0,0}{95.38} & 79.70 \\
%     ResNet & \checkmark      &       &       &       & 76.37 & 84.92 & \textcolor[rgb]{.267,.447,.769}{93.23} & 94.13 & 87.16 \\
%     % ResNet & \checkmark      &  \checkmark     &       & \checkmark      & 75.76 & 87.85 & \textbf{93.28} & 94.24 & 87.78 \\
%     WaveResNet & \checkmark      &       &       &       & \textcolor[rgb]{.267,.447,.769}{76.49} & 86.73 & 92.86 & 94.17 & 87.56 \\
%     WaveResNet & \checkmark      & \checkmark      &       &       & \textcolor[rgb]{1,0,0}{76.95} & 87.77 & 92.55 & 93.30 & 87.64 \\
%     WaveResNet & \checkmark      & \checkmark      & \checkmark      &       & 75.41 & \textcolor[rgb]{1,0,0}{88.31} & 93.18 & 94.31 & \textcolor[rgb]{.267,.447,.769}{87.80} \\
%     WaveResNet & \checkmark      & \checkmark      & \checkmark      & \checkmark      & 76.41 & \textcolor[rgb]{.267,.447,.769}{88.30} & \textcolor[rgb]{1,0,0}{93.25} & \textcolor[rgb]{.267,.447,.769}{94.47} & \textcolor[rgb]{1,0,0}{88.11} \\
%     \bottomrule
%     \end{tabular}%
%     \end{adjustbox}
%   \label{tab:mixdataset}%
% \end{table*}%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Experimental Results}
\label{sec:experiment}
\subsection{Experimental Settings}
\subsubsection{Datasets}
The cleaned MS1M dataset \cite{deng2019arcface} is used as the training set, which is composed of approximately 3.28M face images belonging to 72,778 identities. All the images in the training set are cropped to the size of 112x112 and aligned with five facial landmarks. Under the teacher-student training framework, every sample is randomly downsized in order to construct HR-LR training pairs. As for evaluation, four popular datasets are employed, i.e. LFW \cite{huang2008labeled}, AgeDB-30 \cite{moschoglou2017agedb}, CPLFW \cite{zheng2018cross}, and CALFW \cite{zheng2017cross}.
For a fair comparison with previous related work, all the testing samples are downsampled using linear interpolation instead of the degradation model. 

\subsubsection{Implementation Details}
In the proposed knowledge distillation framework, the teacher network leverages ResNet as a backbone and the student network employs the proposed WaveResNet. The teacher network is trained on HR images only, while the student network is trained on multi-scale LR images. The LR images are obtained through the proposed degradation model in random scales and then upsampled to the same size as HR images for training.
% the downsampling scales are randomly selected
Both teacher and student networks are trained for 18 epochs using the SGD optimizer with a batch size of 128. The learning rate is initially set to be 0.1 and divided by 10 at 10, 13, and 16 epochs. The weights in the loss function are set to be $\lambda_{1}=1$ and $\lambda_{2}=0.05$. 
% built on a ResNet50 backbone and ArcFace loss function and is 
% employs the proposed WaveResNet as the backbone and the same ArcFace loss and
% for distillation loss and wavelet similarity loss 
% It takes approximately 3 days for training on an NVIDIA-RTX-3090 GPU. 
% The baseline model consists of ResNet50 backbone and the same ArcFace loss function and is trained under the same setting of hyper-parameters. 

% \subsection{Results}

\subsection{Performance on Multiple Datasets}
Table. \ref{tab:mixdataset} shows the verification accuracy of the proposed method on multiple datasets of different resolutions. The results demonstrate the effectiveness of each proposed module. Compared to the baseline model in the first row, the proposed identity-preserving WaveResNet significantly improves the performance in very low-resolution testing images. Training with realistic synthetic data further improves the performance in LR test data but it impairs recognition accuracy on HR images. The cross-resolution distillation framework not only remedies the performance sacrifice in HR images but also enhances the accuracy in LR conditions, thereby improving the overall scores. 
% Finally, after employing all proposed techniques, 
Finally, after additionally employing the auxiliary wavelet similarity loss,
the model demonstrates promising results and significantly outperforms the baseline model on both low and high-resolution images.


% The results of two baseline models, in the first two columns, show that training with multi-scale data is necessary to get reasonable results on LR test images but will lead to performance deterioration on HR images. 
% The rest of the results demonstrate the effectiveness of each proposed module. The proposed identity-preserving WaveResNet 
% significantly improves the performance in LR testing data.
% % manages to improve the overall score. 
% Training with realistic synthetic data further improves the performance in LR scenarios but impairs recognition accuracy on HR images. The cross-resolution distillation framework remedies the performance sacrifice in HR images and improves the overall scores.
% Finally, after employing all proposed techniques, the model demonstrates promising results on both low and high-resolution images and outperforms the baseline models.
% The first two columns are regarded as baseline models. The results show that
% significantly improves the performance on average. 

% demonstrate the effectiveness of each proposed module

\begin{figure}[t]
	\centering
    \includegraphics[width=\linewidth]{SOTACompare3.pdf}
	\caption{Verification accuracy (\%) on the LFW dataset.}
	\label{fig:lfw}
\end{figure}

\subsection{Comparison with the State-of-the-Art Methods}
The performance of the proposed method is also compared with other state-of-the-art approaches. Due to a lack of open-source codes, it is not possible to re-train the SOTA methods under exactly the same configurations. Thus, we directly took the highest-performing scores in their original publications for comparison. Fig. \ref{fig:lfw} presents the results of SFace\cite{lai2019low}, DCR \cite{lu2018deep}, TCN \cite{zha2019tcn}, Lai and Lam \cite{lai2021deep}, Knoche et al. \cite{knoche2022octuplet}, and our proposed method on low-resolution LFW dataset. The results show that our method consistently outperforms other approaches in both low and high-resolution settings. 
An additional comparisons with Kim et al. \cite{kim2021quality} and Shin et al. \cite{shin2022teaching} have been made on the AgeDB-30 dataset, see Table. \ref{tab:agedb}. As a result, the proposed method achieves the best performance across all resolutions of images. 
Besides, it is also observable that the FSR-based method \cite{lai2019low} performs better on higher-resolution data than many other approaches based on resolution-invariant feature spaces \cite{lu2018deep, zha2019tcn, lai2021deep}, but it is less powerful in very low-resolution scenarios. 

% knowledge distillation-based methods generally show relatively higher scores on very low-resolution images when compared to other methods.
% It is also notable that knowledge distillation-based methods show relatively higher scores on very low-resolution images when compared to methods based on the triplet training scheme.

% methods based on the triplet training scheme show relatively weak results on very low-resolution images when compared to . 

%%%%%%%%%%%%% Compare with SOTA %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% \begin{table}[htbp]
%   \centering
%   \caption{Verification accuracy (\%) on LFW dataset.}
%     \begin{tabular}{cccccc}
%     \toprule
%     Methods & 8x8   & 12x12 & 16x16 & 20x20 & HR \\
%     \midrule
%     DCR   & 93.60  & 95.30  & 96.60  & 97.30  & 98.70 \\
%     TCN   & 90.50  & 94.70  & 97.20  & 97.80  & 98.80 \\
%     FAN   & 95.20  & -     & -     & -     & 99.50 \\
%     MK-MMD & 94.05 & 95.20  & 96.74 & 97.13 & 99.03 \\
%     Lai and Lam & 94.80  & 97.60  & 98.20  & 98.10  & 99.10 \\
%     Octuplet & 90.38 & 96.88 & 98.28 & -     & 99.55 \\
%     \bottomrule
%     \end{tabular}%
%   \label{tab:addlabel}%
% \end{table}%




\begin{table}[t]
  \centering
  \caption{Verification accuracy (\%) on the AgeDB-30 dataset.}
    \begin{tabular}{ccccc}
    \toprule
    Methods & 14x14 & 28x28 & 56x56 & 112x112 \\
    \midrule
    Kim et al. \cite{kim2021quality} & 73.20 & 87.05 & 91.27 & 92.22 \\
    Shin et al.  \cite{shin2022teaching} & 79.45 & 89.15 & 93.58 & 93.78 \\
    Proposed Method  & 81.87 & 93.95 & 96.05 & 96.50 \\
    \bottomrule
    \end{tabular}%
  \label{tab:agedb}%
\end{table}%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\subsection{Discussion}
The experimental results demonstrate the effectiveness of the proposed method in the CRFR task. In fact, each module of the method plays a different role. 
For example, the WaveResNet and synthetic LR training data mainly contribute to LR face recognition, and the cross-resolution knowledge distillation paradigm further elevates the performance in HR images. The wavelet similarity loss additionally improves the performance on all resolutions. It is notable that most of the previous work presents a relatively poor result either in high or very low-resolution data. On the contrary, the proposed method offers high performance across a variety of resolution scenarios after combination of all proposed modules.


% The modules in the proposed method play different roles in the LRFR task. For example, the WaveResNet and synthetic LR training data contribute a lot to the LR recognition scenario, while the cross-resolution knowledge distillation paradigm elevates the performance on HR images. The wavelet similarity loss further improves the performance on all scales. It is also noticeable that most of the other related work presents a relatively poor performance either in HR or LR data. On the contrary, the proposed method remains in high performance across variant resolutions after a proper combination of different modules.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Conclusion}
\label{sec:conclusion}
% This paper proposes a novel identity-preserving approach built on a knowledge distillation framework to address the low-resolution face recognition problem. A realistic data degradation model is also contributed to further improve the performance.
% Extensive experiments have shown that the proposed method outperforms the baseline model and other state-of-the-art approaches on multiple datasets under different resolution scenarios. 

% In this paper, a novel approach is proposed for the low-resolution face recognition problem. 
% This work resolves the problem by contributing an identity-preserving neural network, a cross-resolution knowledge distillation framework, and a

% This paper focuses on cross-resolution face recognition which is crucial problem in realistic recognition scenarios
A new approach to address the challenge of cross-resolution face recognition was proposed based on identity-preserving network built upon a knowledge distillation framework. 
% when facing resolution discrepancies. 
A realistic data degradation model is also contributed to further improve the performance in LR scenarios and demonstrating the discriminative power contained in the low-frequency domain of LR data. 
% developing more general quality-agnostic face recognition methods. 

% The proposed identity-preserving WaveResNet in the cross-resolution knowledge distillation framework 


% capturing the discriminative feature in the low-frequency component of low-resolution images



% not only low-resolution but also
% develop general quality-agnostic face recognition methods. 

% This paper aims at resolving the cross-resolution face recognition problem. 
% A new identity-preserving approach built on a knowledge distillation framework is proposed to address this issue. 
% performance deterioration problem 
% % resolution discrepancy

% This paper proposes a new 
% by concentrating on 

% under the knowledge distillation framework



% \newpage
\bibliographystyle{IEEEtran}
\bibliography{refs}



% \begin{thebibliography}{00}
% \bibitem{b1} G. Eason, B. Noble, and I. N. Sneddon, ``On certain integrals of Lipschitz-Hankel type involving products of Bessel functions,'' Phil. Trans. Roy. Soc. London, vol. A247, pp. 529--551, April 1955.
% \bibitem{b2} J. Clerk Maxwell, A Treatise on Electricity and Magnetism, 3rd ed., vol. 2. Oxford: Clarendon, 1892, pp.68--73.
% \bibitem{b3} I. S. Jacobs and C. P. Bean, ``Fine particles, thin films and exchange anisotropy,'' in Magnetism, vol. III, G. T. Rado and H. Suhl, Eds. New York: Academic, 1963, pp. 271--350.
% \bibitem{b4} K. Elissa, ``Title of paper if known,'' unpublished.
% \bibitem{b5} R. Nicole, ``Title of paper with only first word capitalized,'' J. Name Stand. Abbrev., in press.
% \bibitem{b6} Y. Yorozu, M. Hirano, K. Oka, and Y. Tagawa, ``Electron spectroscopy studies on magneto-optical media and plastic substrate interface,'' IEEE Transl. J. Magn. Japan, vol. 2, pp. 740--741, August 1987 [Digests 9th Annual Conf. Magnetics Japan, p. 301, 1982].
% \bibitem{b7} M. Young, The Technical Writer's Handbook. Mill Valley, CA: University Science, 1989.
% \end{thebibliography}
% \vspace{12pt}
% \color{red}
% IEEE conference templates contain guidance text for composing and formatting conference papers. Please ensure that all template text is removed from your conference paper prior to submission to the conference. Failure to remove the template text from your paper may result in your paper not being published.

\end{document}



\section{Introduction}
This document is a model and instructions for \LaTeX.
Please observe the conference page limits. 

\section{Ease of Use}

\subsection{Maintaining the Integrity of the Specifications}

The IEEEtran class file is used to format your paper and style the text. All margins, 
column widths, line spaces, and text fonts are prescribed; please do not 
alter them. You may note peculiarities. For example, the head margin
measures proportionately more than is customary. This measurement 
and others are deliberate, using specifications that anticipate your paper 
as one part of the entire proceedings, and not as an independent document. 
Please do not revise any of the current designations.

\section{Prepare Your Paper Before Styling}
Before you begin to format your paper, first write and save the content as a 
separate text file. Complete all content and organizational editing before 
formatting. Please note sections \ref{AA}--\ref{SCM} below for more information on 
proofreading, spelling and grammar.

Keep your text and graphic files separate until after the text has been 
formatted and styled. Do not number text heads---{\LaTeX} will do that 
for you.

\subsection{Abbreviations and Acronyms}\label{AA}
Define abbreviations and acronyms the first time they are used in the text, 
even after they have been defined in the abstract. Abbreviations such as 
IEEE, SI, MKS, CGS, ac, dc, and rms do not have to be defined. Do not use 
abbreviations in the title or heads unless they are unavoidable.

\subsection{Units}
\begin{itemize}
\item Use either SI (MKS) or CGS as primary units. (SI units are encouraged.) English units may be used as secondary units (in parentheses). An exception would be the use of English units as identifiers in trade, such as ``3.5-inch disk drive''.
\item Avoid combining SI and CGS units, such as current in amperes and magnetic field in oersteds. This often leads to confusion because equations do not balance dimensionally. If you must use mixed units, clearly state the units for each quantity that you use in an equation.
\item Do not mix complete spellings and abbreviations of units: ``Wb/m\textsuperscript{2}'' or ``webers per square meter'', not ``webers/m\textsuperscript{2}''. Spell out units when they appear in text: ``. . . a few henries'', not ``. . . a few H''.
\item Use a zero before decimal points: ``0.25'', not ``.25''. Use ``cm\textsuperscript{3}'', not ``cc''.)
\end{itemize}

\subsection{Equations}
Number equations consecutively. To make your 
equations more compact, you may use the solidus (~/~), the exp function, or 
appropriate exponents. Italicize Roman symbols for quantities and variables, 
but not Greek symbols. Use a long dash rather than a hyphen for a minus 
sign. Punctuate equations with commas or periods when they are part of a 
sentence, as in:
\begin{equation}
a+b=\gamma\label{eq}
\end{equation}

Be sure that the 
symbols in your equation have been defined before or immediately following 
the equation. Use ``\eqref{eq}'', not ``Eq.~\eqref{eq}'' or ``equation \eqref{eq}'', except at 
the beginning of a sentence: ``Equation \eqref{eq} is . . .''

\subsection{\LaTeX-Specific Advice}

Please use ``soft'' (e.g., \verb|\eqref{Eq}|) cross references instead
of ``hard'' references (e.g., \verb|(1)|). That will make it possible
to combine sections, add equations, or change the order of figures or
citations without having to go through the file line by line.

Please don't use the \verb|{eqnarray}| equation environment. Use
\verb|{align}| or \verb|{IEEEeqnarray}| instead. The \verb|{eqnarray}|
environment leaves unsightly spaces around relation symbols.

Please note that the \verb|{subequations}| environment in {\LaTeX}
will increment the main equation counter even when there are no
equation numbers displayed. If you forget that, you might write an
article in which the equation numbers skip from (17) to (20), causing
the copy editors to wonder if you've discovered a new method of
counting.

{\BibTeX} does not work by magic. It doesn't get the bibliographic
data from thin air but from .bib files. If you use {\BibTeX} to produce a
bibliography you must send the .bib files. 

{\LaTeX} can't read your mind. If you assign the same label to a
subsubsection and a table, you might find that Table I has been cross
referenced as Table IV-B3. 

{\LaTeX} does not have precognitive abilities. If you put a
\verb|\label| command before the command that updates the counter it's
supposed to be using, the label will pick up the last counter to be
cross referenced instead. In particular, a \verb|\label| command
should not go before the caption of a figure or a table.

Do not use \verb|\nonumber| inside the \verb|{array}| environment. It
will not stop equation numbers inside \verb|{array}| (there won't be
any anyway) and it might stop a wanted equation number in the
surrounding equation.

\subsection{Some Common Mistakes}\label{SCM}
\begin{itemize}
\item The word ``data'' is plural, not singular.
\item The subscript for the permeability of vacuum $\mu_{0}$, and other common scientific constants, is zero with subscript formatting, not a lowercase letter ``o''.
\item In American English, commas, semicolons, periods, question and exclamation marks are located within quotation marks only when a complete thought or name is cited, such as a title or full quotation. When quotation marks are used, instead of a bold or italic typeface, to highlight a word or phrase, punctuation should appear outside of the quotation marks. A parenthetical phrase or statement at the end of a sentence is punctuated outside of the closing parenthesis (like this). (A parenthetical sentence is punctuated within the parentheses.)
\item A graph within a graph is an ``inset'', not an ``insert''. The word alternatively is preferred to the word ``alternately'' (unless you really mean something that alternates).
\item Do not use the word ``essentially'' to mean ``approximately'' or ``effectively''.
\item In your paper title, if the words ``that uses'' can accurately replace the word ``using'', capitalize the ``u''; if not, keep using lower-cased.
\item Be aware of the different meanings of the homophones ``affect'' and ``effect'', ``complement'' and ``compliment'', ``discreet'' and ``discrete'', ``principal'' and ``principle''.
\item Do not confuse ``imply'' and ``infer''.
\item The prefix ``non'' is not a word; it should be joined to the word it modifies, usually without a hyphen.
\item There is no period after the ``et'' in the Latin abbreviation ``et al.''.
\item The abbreviation ``i.e.'' means ``that is'', and the abbreviation ``e.g.'' means ``for example''.
\end{itemize}
An excellent style manual for science writers is \cite{b7}.

\subsection{Authors and Affiliations}
\textbf{The class file is designed for, but not limited to, six authors.} A 
minimum of one author is required for all conference articles. Author names 
should be listed starting from left to right and then moving down to the 
next line. This is the author sequence that will be used in future citations 
and by indexing services. Names should not be listed in columns nor group by 
affiliation. Please keep your affiliations as succinct as possible (for 
example, do not differentiate among departments of the same organization).

\subsection{Identify the Headings}
Headings, or heads, are organizational devices that guide the reader through 
your paper. There are two types: component heads and text heads.

Component heads identify the different components of your paper and are not 
topically subordinate to each other. Examples include Acknowledgments and 
References and, for these, the correct style to use is ``Heading 5''. Use 
``figure caption'' for your Figure captions, and ``table head'' for your 
table title. Run-in heads, such as ``Abstract'', will require you to apply a 
style (in this case, italic) in addition to the style provided by the drop 
down menu to differentiate the head from the text.

Text heads organize the topics on a relational, hierarchical basis. For 
example, the paper title is the primary text head because all subsequent 
material relates and elaborates on this one topic. If there are two or more 
sub-topics, the next level head (uppercase Roman numerals) should be used 
and, conversely, if there are not at least two sub-topics, then no subheads 
should be introduced.

\subsection{Figures and Tables}
\paragraph{Positioning Figures and Tables} Place figures and tables at the top and 
bottom of columns. Avoid placing them in the middle of columns. Large 
figures and tables may span across both columns. Figure captions should be 
below the figures; table heads should appear above the tables. Insert 
figures and tables after they are cited in the text. Use the abbreviation 
``Fig.~\ref{fig}'', even at the beginning of a sentence.

\begin{table}[htbp]
\caption{Table Type Styles}
\begin{center}
\begin{tabular}{|c|c|c|c|}
\hline
\textbf{Table}&\multicolumn{3}{|c|}{\textbf{Table Column Head}} \\
\cline{2-4} 
\textbf{Head} & \textbf{\textit{Table column subhead}}& \textbf{\textit{Subhead}}& \textbf{\textit{Subhead}} \\
\hline
copy& More table copy$^{\mathrm{a}}$& &  \\
\hline
\multicolumn{4}{l}{$^{\mathrm{a}}$Sample of a Table footnote.}
\end{tabular}
\label{tab1}
\end{center}
\end{table}

\begin{figure}[htbp]
\centerline{\includegraphics{fig1.png}}
\caption{Example of a figure caption.}
\label{fig}
\end{figure}

Figure Labels: Use 8 point Times New Roman for Figure labels. Use words 
rather than symbols or abbreviations when writing Figure axis labels to 
avoid confusing the reader. As an example, write the quantity 
``Magnetization'', or ``Magnetization, M'', not just ``M''. If including 
units in the label, present them within parentheses. Do not label axes only 
with units. In the example, write ``Magnetization (A/m)'' or ``Magnetization 
\{A[m(1)]\}'', not just ``A/m''. Do not label axes with a ratio of 
quantities and units. For example, write ``Temperature (K)'', not 
``Temperature/K''.

\section*{Acknowledgment}

The preferred spelling of the word ``acknowledgment'' in America is without 
an ``e'' after the ``g''. Avoid the stilted expression ``one of us (R. B. 
G.) thanks $\ldots$''. Instead, try ``R. B. G. thanks$\ldots$''. Put sponsor 
acknowledgments in the unnumbered footnote on the first page.

\section*{References}

Please number citations consecutively within brackets \cite{b1}. The 
sentence punctuation follows the bracket \cite{b2}. Refer simply to the reference 
number, as in \cite{b3}---do not use ``Ref. \cite{b3}'' or ``reference \cite{b3}'' except at 
the beginning of a sentence: ``Reference \cite{b3} was the first $\ldots$''

Number footnotes separately in superscripts. Place the actual footnote at 
the bottom of the column in which it was cited. Do not put footnotes in the 
abstract or reference list. Use letters for table footnotes.

Unless there are six authors or more give all authors' names; do not use 
``et al.''. Papers that have not been published, even if they have been 
submitted for publication, should be cited as ``unpublished'' \cite{b4}. Papers 
that have been accepted for publication should be cited as ``in press'' \cite{b5}. 
Capitalize only the first word in a paper title, except for proper nouns and 
element symbols.

For papers published in translation journals, please give the English 
citation first, followed by the original foreign-language citation \cite{b6}.