%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                                                 %%
%% Please do not use \input{...} to include other tex files.       %%
%% Submit your LaTeX manuscript as one .tex document.              %%
%%                                                                 %%
%% All additional figures and files should be attached             %%
%% separately and not embedded in the \TeX\ document itself.       %%
%%                                                                 %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%\documentclass[referee,sn-basic]{sn-jnl}% referee option is meant for double line spacing

%%=======================================================%%
%% to print line numbers in the margin use lineno option %%
%%=======================================================%%

%%\documentclass[lineno,sn-basic]{sn-jnl}% Basic Springer Nature Reference Style/Chemistry Reference Style

%%======================================================%%
%% to compile with pdflatex/xelatex use pdflatex option %%
%%======================================================%%

%%\documentclass[pdflatex,sn-basic]{sn-jnl}% Basic Springer Nature Reference Style/Chemistry Reference Style

%%\documentclass[sn-basic]{sn-jnl}% Basic Springer Nature Reference Style/Chemistry Reference Style
% \documentclass[sn-mathphys]{sn-jnl}% Math and Physical Sciences Reference Style
%%\documentclass[sn-aps]{sn-jnl}% American Physical Society (APS) Reference Style
%%\documentclass[sn-vancouver]{sn-jnl}% Vancouver Reference Style
%%\documentclass[sn-apa]{sn-jnl}% APA Reference Style
%%\documentclass[sn-chicago]{sn-jnl}% Chicago-based Humanities Reference Style
%%\documentclass[sn-standardnature]{sn-jnl}% Standard Nature Portfolio Reference Style
% \documentclass[default]{sn-jnl}% Default
% \documentclass[lineno,sn-apa,iicol]{sn-jnl}% Default with double column layout
\documentclass[sn-apa,iicol]{sn-jnl}% Default with double column layout
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}

\usepackage{booktabs}
\usepackage{multirow}
\usepackage{bbm}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}
\usepackage{bm}
\usepackage{multirow}
% \usepackage{cite}
\usepackage{rotating}

% \usepackage{array}
% \usepackage{booktabs}
% \usepackage{multirow}
% \usepackage[flushleft]{threeparttable}


% % \usepackage[table]{xcolor}
% \usepackage{xcolor,colortbl}
% \usepackage{tabularx}
% \definecolor{LightCyan}{rgb}{0.88,1,1}
% \definecolor{Gray}{gray}{0.8999}
% \definecolor{LightGray}{gray}{0.9486}
% \usepackage{tabularray}
% \UseTblrLibrary{siunitx}
% \usepackage{ragged2e}

%%%% Standard Packages
%%<additional latex packages if required can be included here>
%%%%

%%%%%=============================================================================%%%%
%%%%  Remarks: This template is provided to aid authors with the preparation
%%%%  of original research articles intended for submission to journals published 
%%%%  by Springer Nature. The guidance has been prepared in partnership with 
%%%%  production teams to conform to Springer Nature technical requirements. 
%%%%  Editorial and presentation requirements differ among journal portfolios and 
%%%%  research disciplines. You may find sections in this template are irrelevant 
%%%%  to your work and are empowered to omit any such section if allowed by the 
%%%%  journal you intend to submit to. The submission guidelines and policies 
%%%%  of the journal take precedence. A detailed User Manual is available in the 
%%%%  template package for technical guidance.
%%%%%=============================================================================%%%%

\jyear{2023}%

%% as per the requirement new theorem styles can be included as shown below
\theoremstyle{thmstyleone}%
\newtheorem{theorem}{Theorem}%  meant for continuous numbers
%%\newtheorem{theorem}{Theorem}[section]% meant for sectionwise numbers
%% optional argument [theorem] produces theorem numbering sequence instead of independent numbers for Proposition
\newtheorem{proposition}[theorem]{Proposition}% 
%%\newtheorem{proposition}{Proposition}% to get separate numbers for theorem and proposition etc.

\theoremstyle{thmstyletwo}%
\newtheorem{example}{Example}%
\newtheorem{remark}{Remark}%

\theoremstyle{thmstylethree}%
\newtheorem{definition}{Definition}%

\raggedbottom
%%\unnumbered% uncomment this for unnumbered level heads

\begin{document}

\title[Net]{Harmonizing Base and Novel Classes: A Class-Contrastive Approach for Generalized Few-Shot Segmentation}

%%=============================================================%%
%% Prefix	-> \pfx{Dr}
%% GivenName	-> \fnm{Joergen W.}
%% Particle	-> \spfx{van der} -> surname prefix
%% FamilyName	-> \sur{Ploeg}
%% Suffix	-> \sfx{IV}
%% NatureName	-> \tanm{Poet Laureate} -> Title after name
%% Degrees	-> \dgr{MSc, PhD}
%% \author*[1,2]{\pfx{Dr} \fnm{Joergen W.} \spfx{van der} \sur{Ploeg} \sfx{IV} \tanm{Poet Laureate} 
%%                 \dgr{MSc, PhD}}\email{iauthor@gmail.com}
%%=============================================================%%


\author[1]{\fnm{Weide} \sur{Liu}}\email{weide001@e.ntu.edu.sg}
\author[2]{\fnm{Zhonghua} \sur{Wu}}\email{zhonghua001@e.ntu.edu.sg}
\author[1]{\fnm{Yang} \sur{Zhao}}\email{zhao\_yang@simtech.a-star.edu.sg}
\author[3]{\fnm{Yuming} \sur{Fang}}\email{fa0001ng@e.ntu.edu.sg}
\author[1]{\fnm{Chuan-Sheng} \sur{Foo}}\email{foo\_chuan\_sheng@i2r.a-star.edu.sg}
\author[1]{\fnm{Jun} \sur{Cheng}}\email{cheng\_jun@i2r.a-star.edu.sg}
\author[2]{\fnm{Guosheng} \sur{Lin}}\email{gslin@ntu.edu.sg}

\affil[1]{\orgname{A*STAR}, \orgaddress{\city{Singapore}, \postcode{138632}, \country{Singapore}}}

\affil[2]{\orgname{Nanyang Technological University}, \orgaddress{\city{Singapore}, \postcode{639798}, \country{Singapore}}}

\affil[3]{\orgname{Jiangxi Finance and Economics University}, \orgaddress{\city{Jiangxi}, \postcode{330000}, \country{China}}}

%%==================================%%
%% sample for unstructured abstract %%
%%==================================%%

\abstract{Current methods for few-shot segmentation (FSSeg) have mainly focused on improving the performance of novel classes while neglecting the performance of base classes. To overcome this limitation, the task of generalized few-shot semantic segmentation (GFSSeg) has been introduced, aiming to predict segmentation masks for both base and novel classes. However, the current prototype-based methods do not explicitly consider the relationship between base and novel classes when updating prototypes, leading to a limited performance in identifying true categories. To address this challenge, we propose a class contrastive loss and a class relationship loss to regulate prototype updates and encourage a large distance between prototypes from different classes, thus distinguishing the classes from each other while maintaining the performance of the base classes. Our proposed approach achieves new state-of-the-art performance for the generalized few-shot segmentation task on PASCAL VOC and MS COCO datasets.}

%%================================%%
%% Sample for structured abstract %%
%%================================%%


\keywords{}

%%\pacs[JEL Classification]{D8, H51}

%%\pacs[MSC Classification]{35A01, 65L10, 65L12, 65L20, 65L70}

\maketitle

\section{Introduction}
Semantic segmentation is a fundamental task that assigns a label to each pixel in an image. With the advent of deep neural networks, the performance of semantic segmentation has been significantly improved. However, training a segmentation model typically requires a substantial amount of labeled data for each class. Consequently, extending such models to new classes often demands a similar amount of labeled data, which can be time-consuming and costly.

Few-shot segmentation (FSSeg) has been introduced as a solution to alleviate the need for extensive labeling for novel classes. FSSeg enables the segmentation of categories with only a few labeled samples available for model training. Typically, FSSeg methods first train models with ample training samples from base categories and subsequently generalize them to recognize new categories with only a few annotated samples. Previous FSSeg methods often adopt a two-branch structure consisting of support and query branches. The support branch extracts and transfers information from support images for segmentation in the query branch, while the query branch takes the output from the support branch and query images as input to produce the segmentation mask for new classes.
\input{Figures/Motivation}


However, many FSSeg methods have primarily focused on improving the performance of new classes at the expense of the base classes. Recent studies have highlighted that emphasizing new classes from the support set can lead to a drop in performance for the segmentation of base classes, particularly when the new classes have similar appearances to the base classes. For instance, updating the models to segment a new class such as a dog may result in the misclassification of base classes such as a cat.

Experiments have demonstrated that FSSeg methods struggle to differentiate between certain base and novel categories. For example, previous results have shown only 1.89\% mean Intersection over Union (IoU) on novel classes and 8.83\% mean IoU on base classes with a 5-shot setting on the MS COCO dataset for the state-of-the-art FSSeg method~\cite{pfenet} (refer to Table~\ref{sota_voc_5shot} for more details). This performance is dissimilar to human recognition, as humans can recognize or segment previously known classes while learning to recognize new classes.

To overcome this limitation, Tian et al.\cite{capl} proposed the task of generalized few-shot semantic segmentation (GFSSeg), which aims to predict segmentation masks for both base and novel classes, making it more representative of real-world scenarios. Generalizing a model to new classes while preserving its performance on base classes is significantly more challenging than the goal of previous FSSeg methods. The context-aware prototype learning (CAPL)\cite{capl} method was proposed to exploit co-occurrence information from support samples and adapt the model to query samples, which is currently the state-of-the-art (SOTA). However, CAPL does not consider the relationship between novel and base classes, which compromises its performance. Figure~\ref{Figure: Motivation} illustrates examples of base class (car) and novel classes (giraffe, chair, and stop sign). As shown, the CAPL model accurately locates the boundaries of the base class car but misclassifies them as trucks, resulting in a drop in performance for base classes and negatively impacting the accuracy of some novel classes. Incorrect object prediction rather than boundary prediction leads to the primary performance drop. This motivates a focus on the labels of the masks for generalizing models for novel classes. CAPL~\cite{capl} updates prototypes using support and query images without considering their relationship, which may harm prototypes for some base classes as the learned prototypes lack sufficient between-class distances. As base classes may have been well-trained in previous fully supervised segmentation models, we aim to discourage large modifications of these prototypes when adapting models for new classes to maintain the segmentation performance of base classes. Simultaneously, we aim to encourage a large distance between prototypes from different classes. To achieve this, we propose a class contrastive loss and a class relationship loss to regularize the updating of prototypes.


During the training process, we model class features into a set of prototypes and update them using training samples. To prevent significant changes from previously obtained prototypes, we measure the distance between the current prototype and its previous prototype and strive to keep this distance small. On the other hand, to improve label accuracy, we aim for a larger distance between prototypes from different classes. To achieve this, we propose a novel class contrastive loss that minimizes the distance between prototypes belonging to the same classes and maximizes the distance between different classes.

Moreover, since the distance between a dog and a cat should typically be smaller than that between a dog and a table, simply computing the Euclidean distance is inadequate. Inspired by such intuition, we propose to model the relationships among different classes using a similarity-weighted heterogeneous graph. As shown in Figure~\ref{Figure: pipeline}, we also propose to compute a class relationship loss among different classes based on similarities.

The main contributions of this work are summarized as follows:
\begin{itemize}
    \item We propose a novel class contrastive loss that discourages large modifications when updating prototypes using data from the same classes, while encouraging prototypes from different categories to be farther apart.
    \item We employ a graph network and propose a new class relationship loss to effectively manage the within-class and between-class relationships for the novel and base classes.
    \item Our proposed method achieved new state-of-the-art on the PASCAL VOC and MS COCO dataset for generalized the few shot segmentation tasks. 
\end{itemize}

\section{Related Work}
\subsection{Semantic segmentation}
Fully supervised image semantic segmentation tasks have been proposed to generate dense predictions for each pixel. FCN~\cite{Long2015FCN} is the first fully convolutional network for semantic segmentation.
Following this FCN paradigm, many existing fully supervised segmentation methods~\citep{kirillov2019panoptic,zhao2017pyramid,chen2016attention,zhao2021contrastive,zhong2021pixel,wang2021exploring} have been proposed to improve performance.
However, the extension of these fully supervised methods for novel classes requires a decent amount of labelled data for the new classes, which is often costly. To reduce human labeling effort, this paper focuses on the generalized few-shot segmentation tasks, where only a few labelled images are required to extend the model for novel classes.

\input{Figures/Pipeline}

\subsection{Few-shot learning}

In order to extend fully supervised image classification to novel classes without enormous labelled training data for the new classes, the few-shot classification tasks have been proposed. During the model training,  only a few labelled images with novel classes are used together with the abundant labelled images with base classes.   To train the network, most of the existing methods~\cite{fan2021few,fan2022self,lang2022learning,kang2022integrative,chen2021apanet,iqbal2022msanet} follow the meta-learning framework to mimic the few-shot learning scenarios. Typically, there are three different types of few-shot learning methods: 1) metric-based  methods~\cite{li2019finding,hou2019cross,koch2015siamese,li2019revisiting}, which compare the feature similarity between the support and query samples; 2) optimization-based methods~\cite{finn2017model, jamal2019task, ravi2016optimization}, which design a meta-learner optimizer to let models to be optimized quickly with a few labelled samples; and 3) augmentation-based methods~\cite{chen2019image, chen2019image2}, which generate a large number of augmented samples for the model training. Our work is closely related to the metric-based approach, where we propose a contrastive class loss and a graph network to facilitate metric learning.

\subsection{Few-shot segmentation}
Few-shot segmentation~\cite{liu2021learning,dong2018few,crnet,xie2021few,wang2020few,zhang2020splitting,hou2022interaction,hou2022distilling} is a task that aims to extend few-shot classification to its segmentation counterpart. Previous works such as SiamFC~\cite{siam2020weakly}, Dynamic Few-Shot Visual Learning~\cite{liu2020dynamic}, BRIEF~\cite{yang2020brinet}, and Self-Supervised Few-Shot Learning with Meta Heuristic~\cite{zhu2020self} have treated few-shot segmentation as a foreground-background segmentation task. These methods have utilized various techniques such as dense comparison modules and iterative optimization modules to refine segmentation masks. Some other methods such as MM-Net~\cite{mmnet} and PFENet~\cite{pfenet} have introduced meta-class memory and prior-guided feature enrichment network to learn meta-class information and capture similarity between query and support features, respectively.

Recently, several methods such as ASGNet~\cite{asgnet}, RePRI~\cite{repri}, CWT~\cite{cwt}, and ABPNet~\cite{dong2021abpnet} have attempted to reduce model bias by adapting to the novel classes. ASGNet~\cite{asgnet} adaptively determines the number of prototypes and their spatial extents, whereas RePRI~\cite{repri} fine-tunes the model over support images to adapt to novel classes. CWT~\cite{cwt} updates the classifier weights of a self-attention block during both training and test phases using episodic training, whereas ABPNet~\cite{dong2021abpnet} formulates the background as the complement of the set of interested classes and uses the attention mechanism and a meta-training strategy to predict task-specific background adaptively. However, these methods do not consider how to harmonize the distance between novel and base classes to reduce the model bias.

In contrast to previous works, which focus on improving the performance of novel classes, GFSSeg~\cite{capl} proposes a new task that simultaneously validates both base and novel classes. In this paper, we propose a class contrastive loss and a relationship loss to handle the generalized few-shot segmentation task. Our method aims to reduce the model bias by harmonizing the distance between novel and base classes.

\section{Method}

\subsection{Motivation}

Contextual information is crucial for prototype learning-based segmentation tasks, yet existing approaches have not fully utilized the available contextual clues from support and query samples, leaving room for improvement. In CAPL~\cite{capl}, the model updates prototypes using support and query images without considering the relationship between them. Figure~\ref{Figure: Motivation} illustrates a situation where the CAPL model can locate the novel class (cars) accurately but fails to correctly identify its category. This highlights that the current limitation of current methods lies in the prototype updating process.

To improve the segmentation performance for both novel and base classes, we argue that the distance between prototypes from different classes should be large, while updates to prototypes from samples of the same class should be small. To achieve this, we propose a class contrastive loss and a class relationship loss. The class contrastive loss minimizes the distance between prototypes belonging to the same class and maximizes the distance between prototypes from different classes. Additionally, we construct a heterogeneous graph to illustrate the relationships among classes and define a class relationship loss. By minimizing this loss, we aim to have a high within-class similarity and a low between-class similarity.


\input{Figures/graph}


\subsection{Prototype learning} 
Assuming a generalized few-shot segmentation task with $b$ base classes $\mathbb{B}={c_1, c_2, \cdots, c_b}$ and $n$ novel classes $\mathbb{N}={c_{b+1},c_{b+2},\cdots, c_{b+n}}$, we follow the prototype learning approach of CAPL~\cite{capl}. For any class $c$ in the support image set $S$, the updating prototype $\Delta\bm{p}c$ is computed via masked average pooling:
\begin{equation}
\Delta \bm{p}c = \frac{\sum{s\in S}\sum{H,W} [\mathcal F(s) \odot M(s)]}{\sum_{s\in S}\sum_{H,W} M(s)}
\end{equation}
Here, $\mathcal F(s) \in \mathbb{R}^{H\times W\times d}$ denotes the features of the support image $s$, $M(s)$ denotes the binary mask of class $c$ for $s$, and $\odot$ represents the Hadamard product. The prototype for class $c$ is then updated at the $t^{th}$ iteration as follows:
\begin{equation}
\bm{p}_c^t = \gamma \cdot \bm{p}_c^{t-1}+(1-\gamma)\cdot \Delta \bm{p}_c,
\label{eq2}
\end{equation}
where $\bm p_x^y$ denotes the prototype of class $x$ at the $y^{th}$ iteration and $\gamma$ is a data-dependent parameter that controls the balance of the old and updated prototypes, following the same setting as CAPL~\cite{capl}.

\input{Tables/compare_other_1_shot_pascal}
\input{Tables/compare_other_5_shot_voc}
\input{Tables/compare_other_1_shot_coco}
\input{Tables/compare_other_5_shot_coco}



In the evaluation phase, 
a dynamic query contextual enrichment module is used to mine each query sample to obtain the essential semantic information and dynamically incorporate the information to adapt the prototypes to the different contexts. Similarly, for any base class $e \in \mathbb{B}$ that is contained in the query image set $Q$,  an updating prototype $\Delta \bm p_e$ is computed.
\begin{equation}
      \Delta \bm{p}_e =  \frac{\sum_{q\in Q}\sum_{H,W} [F(q) \odot M(q)]}{\sum_{q\in Q}\sum_{H,W} M(q)}
\end{equation}
The dynamically enriched prototype ${\bm p}_e$ is also updated by a weighted combination of the original classifier ${\bm p}_e^{t-1}$ and the query samples fed prototype. 
% ${\bm p}_{q}$.
\begin{equation}
    \bm{p}_e^{t} = \gamma \cdot \bm{p}_e^{t-1}+(1-\gamma)\cdot \Delta \bm{p}_e. \label{eq3}
\end{equation}



The main difference between Eq.(\ref{eq2}) and Eq.(\ref{eq3}) is that only classes contained in the support set $S$ are updated in Eq.(\ref{eq2})  while all base classes are updated in Eq.(\ref{eq3}).

All in all, CAPL~\cite{capl} takes advantage of contextual information of samples from both base and novel classes to significantly improve segmentation performance. However,  it does not consider how the update affects the prototypes and the relationship among various classes. To improve the performance, we propose a class contrastive loss to control how the support images and query images update the prototypes in the iteration. Moreover, we also use a graph convolutional network with a novel class relationship loss to consider the relationship among various classes for more accurate generalized few-shot segmentation. Our proposed approach values the contextual information comprehensively, including both the prototype updating and the relationships among classes. 

\subsection{Class contrastive loss}
As the prototypes are updated when extending the models for novel classes, it is important to control how these prototypes are updated. In this paper, we follow the framework of CAPL to update the prototypes, but we propose a novel loss function to regularize the updating. 

Inspired by the work in ~\cite{li2021beyond} by Li~\emph{et al.}, we propose a new class contrastive loss $\mathcal{L}_{c}$  to consider the distances among different classes during the iterative updating of the prototypes. Intuitively, we want to discourage a significant change in the prototypes when updating them with samples from the same classes as the original prototypes for the base classes that have been computed in a fully supervised setting. In this paper, we propose to minimize a within-class updating distance $d^W$ computed from the current iteration and previous iteration:
\begin{equation}
     d^{W} = \sum_{i=1}^{b}\|\bm p_i^{t} - \bm p_i^{t-1} \|_2^2,
\end{equation}
where $b$ represents the number of base classes.



Meanwhile, we also want to encourage the model to update the prototypes such that the between-class distance $d^B$  for prototypes from different classes is maximized. This helps to make the classification more accurate.
\begin{equation}
    d^B=\sum_{i=1}^{N} \sum_{j=1, j\neq i}^N \|\bm p_i^t - \bm p_j^{t} \|_2^2,
\end{equation}
where $N=b+n$ represents the number of all classes.

The proposed class contrastive loss $\mathcal{L}_{C}$ is   computed as
\begin{align}
    \mathcal{L}_{C} = \frac{d^W}   {d^B}.
\end{align}


\subsection{Graph convolutional network and class relationship loss} \label{sec:crl}
Besides updating the prototypes using the support  and query images, we also propose to use a graph convolutional network (GCN) to update the prototypes by considering the relationship among various prototypes. For this purpose, a graph is built, including a between-class sub-graph and a within-class sub-graph. Each prototype of a class serves as a node in the graph. The edge between two nodes/prototypes measures the cosine similarity of the two prototypes. For the between-class sub-graph: the edge is computed with the cosine similarity of the two classes' prototypes in the current iteration. For the within-class sub-graph,  the edge is computed as the cosine similarity of the prototypes before and after the GCN update.
We propose to compute class relationship loss from the prototypes to boost the segmentation accuracy, which includes a cross-class similarity loss and a self-similarity loss.  
\subsubsection{Cross-class similarity loss}
  Given $N$ classes, we construct a between-class subgraph   $G_{B} = \left( V, E_{B} \right)$, where $V$ denotes prototypes computed from the different classes, $E_{B}(\bm p_i, \bm p_j)$ denotes the edge between two prototypes $\bm p_i$ and $\bm p_j$, which is 
  computed as the cosine similarity between $\bm p_i$ and $\bm p_j$, $i\neq j$:
\begin{align}
    E_{B}(\bm p_i, \bm p_j) = \frac{\bm p_i^T \bm p_j}{\|\bm p_i\|_2 \cdot \|\bm p_j\|_2}~\label{eq-soft},
\end{align}
where $T$ denotes the transpose. 

Then, we then apply the softmax for each $\bm p_i$ to normalize its pairwise correlation with other prototypes by
\begin{align}
    S_{i}^{B} = \frac{\exp(E_{B}(\bm p_i,\bm p_j))}{\sum_{j \in N, j\neq i} \exp(E_{B}(\bm p_i, \bm p_j))}.
\end{align}
After building the graph with the pairwise correlation, we will perform the message passing among the graph to enhance the prototypes before obtaining the final segmentation prediction. 
\begin{equation} \label{Inter-Class GCN}
\bm p_i' = S_i^{B} \sum_{j=1, j\neq i}^N   {w}_{i,j}\cdot\bm p_i,
\end{equation}
where $w_{i,j}$ denotes the learnable weight for the edge between $\bm p_i$ and $\bm p_j$ for the between-class sub-graph.

With the updated prototypes $\bm p_i'$, following \cite{panet,capl}, we further adopt cosine similarity  as the distance metric $\phi$ to yield output $O$ for pixels in query sample $q$ as:
% \begin{small}
\begin{equation}
	\footnotesize
	O= \mathop{\arg\max}_{i}
	\frac
	{\exp(\alpha \cdot \phi(\mathcal F(q), \bm p_{i}') )}
	{\sum_{j} \exp(\alpha \cdot \phi (\mathcal F(q), \bm p_j')},
	\label{eqn:cosine_output}
\end{equation}
% \end{small}
where  $\mathcal F(q)$ denotes the feature maps of query image $q$, $i$ denotes the class number and $\alpha$ is empirically set to 10 in all experiments similar to that in ~\cite{capl, panet}.
%% how is F here.

Hence, we define a cross-class similarity loss for between-class proximity as:
\begin{align}
   \mathcal{L}_{B} = CE(O, y), \label{lb}
\end{align}
where the $CE(\cdot)$ denotes the cross-entropy loss and the $y$ denotes the ground truth of the query image.

\input{Tables/ablation_1_shot}

\input{Tables/ablation_hard}

\subsubsection{Self-similarity loss}

Within each class, the update of the prototype shall be small to make the model stable. Similarly to that in Eq.~(\ref{eq-soft}), we compute the cosine similarity between the current prototype $\bm p_i^t$ and previous prototype $\bm p_i^{t-1}$ as:
\begin{align}
    E_{W}(\bm p_i^{t-1}, \bm p_i^t) = \frac{ (\bm p_i^{t-1})^T \bm p_i^t}{\|\bm p_i^{t-1}\|_2 \cdot \| \bm p_i^t\|_2},
\end{align}
where $T$ denotes the transpose. 

Then, we apply the softmax for each class $\bm p_i$ to normalize its pairwise correlation by
\begin{align}
    S^{W} = \frac{\exp(E_{W}(\bm p_i^t,\bm p_i^{t-1}))}{\sum_{j=1}^N \exp(E_{W}(\bm p_j^t, \bm p_j^{t-1}))}.
\end{align}

Similar to that in Eq. (\ref{Inter-Class GCN}) and  Eq. (\ref{eqn:cosine_output}), we also compute $O'$. We define a self-similarity loss for the within-class proximity as:

\begin{align}
   \mathcal{L}_{W} = CE(O', y). \label{lw}
\end{align}
Combining Eq. (\ref{lw}) with Eq. (\ref{lb}), we obtain the class relationship loss as
\begin{align}
    \mathcal{L}_{R} = \mathcal{L}_{B} +\mathcal{L}_{W}. 
\end{align}

\subsection{Overall loss}
The overall loss function is computed as 
\begin{equation}
 \mathcal{L} = \mathcal{L}_{s}+\lambda_1\cdot  \mathcal{L}_{C}+\lambda_2 \cdot \mathcal{L}_{R}, 
\end{equation}
where $\mathcal{L}_{s}$ denotes the loss of segmentation task, same as that in ~\cite{capl}. $\lambda_1$ and $\lambda_2$  control the balances of the new proposed losses. In this paper, we empirically set $\lambda_1$=$\lambda_2$=1. 

\section{Experiments}

\subsection{Dataset and evaluation metric}
We conducted extensive validation experiments on the PASCAL VOC 2012 dataset~\cite{pascal} and MS COCO dataset~\cite{coco}. To validate our method on GFSSeg, following CAPL~\cite{capl}, we split the object categories into 4 folds for cross-validation, with three for training and one for testing. We use the standard mean intersection-over-union (mIoU) as our evaluation metric to validate our methods. More details about the dataset information and the evaluation metric can be found in~\cite{capl}. To further evaluate our method, following previous methods~\cite{zhuge2021deep,zhang2021self,liu2021anti,min2021hypercorrelation,li2020fss,yang2021mining,liu2020ppnet,gairola2020simpropnet,ouyang2020self,li2021adaptive,liu2020crnet,liu2020weakly,liu2020guided,liu2021cross,liu2021few,liu2021fewtmm,liu2022crcnet}, we also evaluate our method on FSSeg task with standard mean intersection-over-union (mIoU) as our evaluation metric.
 
\subsection{Comparison with generalized few-shot segmentation methods}
To evaluate our method against other GFSSeg methods, we first compare it with the current state-of-the-art method CAPL~\cite{capl}. We apply four-fold cross-validation and compute the accuracy for both novel and base classes for each fold. Additionally, we adapt the FSSeg methods PFENet~\cite{pfenet} and PANet~\cite{panet} to the GFSSeg task and validate them on the PASCAL VOC dataset for a better understanding of the performance of previous FSSeg methods in this new task (GFSSeg). If not explicitly stated otherwise, we utilize the ResNet50 as the backbone for all experiments.

\input{Tables/FSS}
\input{Figures/Qualitifed_Images}

\noindent\textbf{PASCAL VOC.}
Table~\ref{sota_voc_1shot} and Table~\ref{sota_voc_5shot} show the comparison results between our method and other methods under the 1-shot and 5-shot settings on the PASCAL VOC dataset, respectively. We can see that our method outperforms the state-of-the-art CAPL by an average of 4.25\% and 4.71\% in the 1-shot and 5-shot settings, respectively. 
Additionally, it is also observed that previous methods, such as PFENet\cite{pfenet} and PANet\cite{panet} work poorly when extended for GFSSeg tasks.

\noindent\textbf{MS COCO.}
Table~\ref{sota_coco_1shot} and Table~\ref{sota_coco_5shot}  show the comparison between the proposed method and CAPL in the 1-shot and 5-shot settings on the MS COCO dataset. Our method outperforms CAPL by 2.15\% and 3.90\% in the two different settings. It is worth noting that all the results are obtained using the original publicly available codes from the authors with default training configurations.

\noindent\textbf{The qualitative comparison examples.}
Figure~\ref{Figure: Qualitified_image} provides a qualitative comparison between our method and the state-of-the-art CAPL~\cite{capl}. We can see that, in most cases, both the CAPL and our proposed method are able to locate the object well. However, the CAPL often predicts a  wrong class label of the objects. With our proposed method using both class contrastive loss and class relationship loss, we are able to obtain more accurate results. 


\subsection{Ablation study}
\noindent\textbf{Effectiveness of the class contrastive loss.}
We conduct ablation experiments to show the effectiveness of our proposed class contrastive loss. Table~\ref{ablation_1_shot_voc} shows the mIoU results in the 1-shot and 5-shot settings. With class contrastive loss (denoted as ``+$L_C$''), we can see that ours can outperform the baseline by 2.80\% under the 1-shot setting and 2.61\% under the 5-shot setting, which suggests that class contrastive loss is able to help the model to obtain better feature representations and facilitate the model training. We use the prototype learning from CAPL~\cite{capl} as our baseline. 


\noindent\textbf{Effectiveness of the class relationship loss.}
We also conducted experiments to demonstrate the effectiveness of our proposed class relationship loss. The results are summarized in Table~\ref{ablation_1_shot_voc}. We can see that the self-similarity loss $L_W$ improves performance by 3.90\% and 3.76\% compared to the baseline in the 1-shot and 5-shot settings, respectively. Notably, for the novel class of fold 0, our self-similarity loss brings a \textbf{10.27\%} mIoU improvement over the baseline in the 5-shot setting.
The cross-class similarity loss $L_B$ improves performance by 4.03\% and 3.99\% in the two different settings. Notably, for the novel class of fold 0, our cross-similarity loss brings a \textbf{10.21\%} mIoU improvement over the baseline in the 5-shot setting.

By combining these three losses ($L_C$, $L_W$, and $L_B$), our proposed method improves performance by 4.25\% and 4.71\% compared to the baseline in the two settings.


\noindent\textbf{Learnable-edge vs. Fixed-edge}
In addition, we also evaluated a variant of the class relationship loss. In Section~\ref{sec:crl}, the edges are obtained in a learnable way, but the edges can also be pre-defined with fixed weights. We refer to this type of class relationship loss as ``$L_h$". In this implementation, we set all weights to 1. As shown in Table~\ref{table: ablation_hard}, we can see that the class relationship loss with fixed edges also improves performance over the baseline. Furthermore, our method with learnable edges is able to outperform the fixed-edge style, further validating the effectiveness of the proposed losses.

\subsection{Comparison with few-shot segmentation}
In addition to GFSSeg, we also evaluated our method on the FSSeg task, which is a specific case of the GFSSeg task. Specifically, only one class is designated as the target class for model prediction. As shown in Table~\ref{tab: fsseg}, our method outperforms all previous few-shot segmentation methods on the MS COCO dataset in both 1-shot and 5-shot settings.

\section{Conclusion}

In conclusion, we propose a novel approach for generalized few-shot semantic segmentation tasks by introducing a class contrastive loss and a class relationship loss. The class relationship loss is designed to handle the within-class and between-class relationships among base and novel classes using a graph network. Our proposed class contrastive loss enables the network to push away the prototypes of different categories and maintain the stability of prototypes. Our proposed method achieves new state-of-the-art results in generalized few-shot segmentation on the PASCAL VOC and MS COCO datasets.

\bibliography{sn-bibliography}

\end{document}
