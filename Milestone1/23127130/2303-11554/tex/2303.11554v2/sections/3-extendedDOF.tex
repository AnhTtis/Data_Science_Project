\section{Simulations}
\label{sec:simulation}
% Present reconstruction results for dual-depth object reconstruction showcasing the extended-depth-of-field capabilities of the radial coded mask when comparing to other coded aperture types
The primary goal of the radial coded mask is to extend the DOF of a lensless imaging system. So far, we have only searched for the best parameters for such a coded mask, without investigating its extended DOF properties. In this section, we determine the extended DOF of a radial mask through simulations. 

\begin{figure}[!t]
    \centering
    \includegraphics[width=\linewidth]{figs/DOF_simulation/SankenOU_diagram.pdf}
    \caption{Geometrical setup of simulations.
    The OU pattern object was positioned at a distance of $5.0$~cm away from the coded mask, while the farther object was at $30.0$~cm away.
    The distance between the image sensor and the coded mask was set to $4.0$ mm.}
    \label{fig:SimSetup}
\end{figure}

\begin{figure*}[!t]
    \centering
    \includegraphics[width=\textwidth, trim = 0cm 0cm 0cm 0cm]{figs/DOF_simulation/SimulationDualDepth_analysis.pdf}
    \caption{\red{Results of the imaging simulation.
    The top two rows present, respectively, the PSFs and sensor measurements.
    The next two rows are the reconstructed images using the ADMM algorithm and their close-ups of the OU pattern object, which cannot be correctly reconstructed by conventional methods.
    The next two rows are the reconstruction by the UDN algorithm and close-ups.
    From the left column, the results correspond to the ground truth, the radial mask, the FZA, and the random mask, respectively.}
    }
    \label{fig:SimDOF_recon}
\end{figure*}

\subsection{Conditions}
\label{subsec:DOF_sim}
The simulation was performed as a dual-depth object reconstruction, where we had two objects at two different distances from the lensless camera. We geometrically model the PSFs projected from both distances, and use them to generate the simulated sensor measurements for the full scene. The simulation of the sensing process was performed by calculating Eq.~\eqref{eq:3Dforward}. We then reconstructed an image from the full sensor measurements by using a single PSF, that could be from either of the distances. Generally, for any type of coded mask, it is expected that the object placed at the same distance at which the PSF was calibrated should be reconstructed with a higher quality than objects at different distances from the lensless camera.

We used two coded masks as non-radial baselines: a Fresnel zone aperture~(FZA)~\cite{Shimano2018, Wu2020, Nakamura2020OE} and a random mask~\cite{Nakamura2019,Zheng2021,Boominathan2020}. The random mask is a naive design for 2D lensless imaging that has good MTF up to cutoff frequency. The FZA is a coded mask with a structure only in the radial direction, opposite to the radial mask, and is suitable for digital refocusing applications. Additionally, we added two hand-crafted periodic radial masks with 20 and 60 radial sections, \red{as well as a randomly generated radial mask.} 


For the lensless image reconstruction, we used two algorithms, namely the alternating direction method of multipliers (ADMM) method~\cite{boyd_etal_ADMM_NOW_2011}, and the untrained deep network (UDN) method~\cite{monakhova_etal_UDN_OptExp_2021}.
Both methods are based on the iterative error-minimization algorithm involving regularization.
For the regularization, the ADMM uses the minimization of 2D total variation~(TV)~\cite{Rubin1992} of a reconstructed image, while the UDN implements it by an untrained generative deep neural network, i.e., employment of deep image prior~\cite{DIP}.
Compared to learning-based methods~\cite{Sinha2017, Monakhova2019OE, Barba2019, Rego2022}, the results can be explainable and their precision is not restricted to a domain of learning.

Figure~\ref{fig:SimSetup} shows the simulated experimental setup.
It involves a planar plush toy and a planar OU pattern positioned $30.0~\mathrm{cm}$ and $5.0~\mathrm{cm}$ away from the coded mask, respectively.
The axial interval between the mask and an image sensor was set to $4.0~\mathrm{mm}$.
Figure~\ref{fig:SimDOF_recon} presents the mask patterns used for simulations, corresponding captured images, and reconstruction results with the ADMM and UDN algorithms using the PSF calibrated for a distance of $30.0~\mathrm{cm}$.
We set the size of the RGB captured measurements, simulated PSFs, and reconstructed images to $512 \times 612$ pixels.
In simulations, the noise was ignored to analyze the upper limit of the effect of the proposed methodology; however, a noise analysis can be drawn from the prototype camera experiments.
The coded masks used in the simulations are the same ones that were used for the prototype camera experiment.
In the reconstruction process, we used 150,000 iterations for the UDN algorithm and 100 iterations per channel for the ADMM algorithm.
The optimization code was implemented in Pytorch with a computational environment including a GPU (GeForce 3090 by NVIDIA), 32 GiB RAM, and a 10-core CPU (i9-10900K by Intel).


\subsection{Reconstruction Results}
\label{sec:Sim_results}
We limited the effective area of the mask to approximately $50~\%$ in the central region for increasing the stability of reconstruction~\cite{Antipa2018}, and the remaining perimeter of the mask was light-shielded.
From the reconstruction results of Fig.~\ref{fig:SimDOF_recon}, we observed that the plush toy was correctly reconstructed by all types of coded masks used. This was expected, as the PSF used for reconstruction was the one calibrated for the same distance as the plush-toy distance. \red{Due to higher freedom of design, we expected the random coded mask to achieve the highest PSNR for the plush toy reconstruction, which was not the case. We argue this fact is due to the presence of the OU pattern which is outside the effective DOF of the calibrated camera. In section A of the suplementary material we show that the random coded mask indeed achieves highest PSNR for the simulation when the whole scene is contained in the depth of field region for the calibrated lensless camera. More details are provided in the supplementary material. We also performed a search for the best random coded mask parameters, in order to ensure the fairness of the simulations that were performed, and the results are shown in section B of the supplementary material.}

The OU pattern, however, was placed closer to the lensless system and because of that it was better reconstructed by the radial masks. We note that the peak signal-to-noise ratio (PSNR) of the radial-mask reconstruction was significantly higher than that of the FZA and random coded masks for this object. 

\red{When comparing to the hand-crafted radial masks, we observe that the periodic masks define a tradeoff on low- and high-frequency reconstructions. More specifically, the periodic radial mask with 20 sections has a better frequency response at lower frequencies and worse response at higher frequencies, as shown in Figure~\ref{fig:MtfComparison}, and because of that it achieved a higher PSNR for the reconstruction of the sparse OU pattern object but a lower PSNR for the more detailed plush toy object. The radial periodic 60 mask achieved the opposite result, due to it having overall better MTF at higher frequencies but worse response at lower frequencies. Our optimized mask achieved a balance between these two, where the OU pattern was reconstructed with better PSNR when compared to the periodic 20 mask, while still being competitive (i.e., $\leq 0.25dB$ difference) in the plush toy reconstruction when compared to the radial periodic 60 mask. The randomly generated radial mask also achieves a tradeoff between low- and high-frequency response, however its reconstruction achieves lower PSNR for all experiments when compared to other radial masks.}

\red{In section C of the supplementary material, we show the refocusing capabilities of a lensless imaging system and also how a radial mask is capable of extending the effective depth of field of the camera independently of the distance where the PSF was calibrated from.}

% \begin{figure}[!t]
%     \centering
%     \includegraphics[scale=0.8]{figs/DOF_simulation/electronicRefocus.pdf}
%     \caption{Demonstration of the refocusing ability of a lensless camera.
%     The lensless sensor measurements were the same ones shown in Fig.~\ref{fig:SimDOF_recon}.
%     The far-focus and close-focus PSFs indicate simulated PSFs created by a light source placed $30.0$~cm and $5.0$~cm away from the coded mask, respectively.
%     The images were reconstructed by the ADMM algorithm.}
%     \label{fig:ElectronicRefocus}
% \end{figure}

% \begin{table}[]
% \centering
% \caption{Quantitative analysis of the simulations.}
% \label{tab:SimAnalysis}
% \subcaption*{(a) Comparison of the quality of the reconstructed OU patterns for different types of coded masks, as presented in Figure~\ref{fig:SimDOF_recon}.}
% \label{tab:ReconCompare}
% \resizebox{\columnwidth}{!}{%

% \begin{tabular}{lcccccc}
% \multicolumn{1}{c}{}        &          & ADMM     &                                &          & UDN      &           \\
% \multicolumn{1}{c|}{}       & PSNR (↑) & SSIM (↑) & \multicolumn{1}{c|}{LPIPS (↓)} & PSNR (↑) & SSIM (↑) & LPIPS (↓) \\ \hline
% \multicolumn{1}{l|}{Radial} & \textbf{20.64} & \textbf{0.6151} & \multicolumn{1}{c|}{\textbf{0.2290}} & \textbf{19.71} & \textbf{0.6700} & \textbf{0.1451} \\
% \multicolumn{1}{l|}{FZA}    & 15.38    & 0.4536   & \multicolumn{1}{c|}{0.4285}    & 14.99    & 0.5300   & 0.3876    \\
% \multicolumn{1}{l|}{Random} & 14.97    & 0.4862   & \multicolumn{1}{c|}{0.6189}    & 15.04    & 0.5111   & 0.5493   
% \end{tabular}%
% }
% % \bigskip
% % \subcaption*{(b) Correlation between close focus and far focus PSFs presented in Fig.~\ref{fig:ElectronicRefocus}.}
% % \begin{tabular}{c|ccc}
% %     & Radial                                          & FZA                                             & Random                                           \\\hline
% % MAE & $1.37\times10^{-6}$ & $3.44\times10^{-6}$ & $3.54\times10^{-6}$
% % \end{tabular}%
% \end{table}

% \begin{figure*}[!t]
%     \centering
%     \includegraphics[width=\textwidth]{figs/DOF_exp/FullDofExpV2.pdf}
%     \caption{(a)~A prototype of a lensless camera, with an SLM creating the radial mask, and an image sensor behind it.
%     (b)~Setup for the experiment.
%     (c)~Calibrated PSFs for the radial, FZA, and random mask.
%     (d)~Experimentally obtained sensor measurements.
%     Reconstruction results using the (e)~ADMM and (f)~UDN algorithm.
%     (g) Close-ups of the OU pattern reconstructed by the UDN algorithm.}
%     \label{fig:RealExp}
% \end{figure*}

\begin{figure*}[!t]
    \centering
    \includegraphics[width=\textwidth]{figs/DOF_exp/LenslessExpSetupV2.jpg}
    \caption{\red{(a)~A prototype of a lensless camera, with a spatial light modulator (SLM) creating the radial mask, and an image sensor placed behind it.
    (b)~Setup for the experiment showing the distances between coded mask and the 4 pawns. Note that the colors of the pawns in order from closer to farther from the lensless camera is red, green, yellow and white. We also present a side view of the setup for better visualization of the slanted plane containing the chessboard pattern.}}
    \label{fig:RealExpSetup}
\end{figure*}

\begin{figure*}[!t]
    \centering
    \includegraphics[width=\textwidth]{figs/DOF_exp/MultiPawnV2_analysis.pdf}
    \caption{\red{Experimental results for a continuous depth reconstruction experiment. First row shows the calibrated PSFs. The second shows the captures from the image sensor. The third and fourth present the reconstruction results from the ADMM and UDN algorithms, respectively.}}
    \label{fig:RealExpResults}
\end{figure*}

% \subsection{Refocusing Ability}
% Additionally, we present in Fig.~\ref{fig:ElectronicRefocus} the refocus of the reconstructed image after sensor measurements capture. We show that by changing the scaling of the PSF, it is possible to focus on either the closer object, or the farther object in the simulated scene. We note, however, that our optimized radial mask is capable of reconstructing both objects independently of the scaling used for the PSF. Table~\ref{tab:SimAnalysis}(b) presents a quantitative comparison between far-focus and close-focus PSFs for the three types of coded masks presented in Fig.~\ref{fig:ElectronicRefocus}. We note that the radial mask presents a lower mean absolute error~(MAE) between far and close-focus PSFs, when compared to FZA and random masks. This justifies why the reconstruction algorithm is capable of reconstructing both objects successfully using either of the calibrated radial PSFs, which is not the case for the other types of coded masks. 


\section{\red{Continuous Depth Optical Experiment with a Prototype Camera}}

\subsection{Setup}
\label{sec:ContinuousExpSetup}
Finally, we create a prototype lensless camera, to validate the extended DOF of the radial mask in the real world. Figure~\ref{fig:RealExpSetup}(a) shows a frontal view of the prototype lensless camera, which consists of an axial stack of a coded mask and an image sensor.
The coded mask was implemented by a transmissive liquid-crystal SLM~(LC2012 by HOLOEYE Photonics) and two polarization plates in the crossed Nicols configuration.
All coded masks to be tested are originally binary, therefore, the light transmittance of the SLM was designed as binary, and central $188 \times 228$ pixels with $36 \times 36\ \mathrm{\mu m}$ pitches were used for implementing the coded masks.
The fill factor of the SLM was $58~\%$.
Approximately $4.0~\mathrm{mm}$ behind the modulation plane of the SLM, we placed a color CMOS image sensor~(BFS-U3-28S5C-BD by Teledyne FLIR) whose pixel count was $1464 \times 1936$ with $4.5~\mathrm{\mu m} \times 4.5~\mathrm{\mu m}$ pitches.
In the experiment, $8$-bit RGB captured images were readout and they were downsampled to $732 \times 968$ pixels for reconstruction.
As well as simulations, the periphery of the mask was shielded for increasing reconstruction stability where the effective area of the mask was approximately $50~\%$.
The center of the effective area of the SLM and the image sensor were aligned by translation stages, and the planes of the two elements were adjusted to be parallel.
% Image sensor information: http://softwareservices.flir.com/BFS-U3-28S5-BD2/latest/EMVA/EMVA.html


\red{Figure~\ref{fig:RealExpSetup}(b) shows the experimental setup including the camera and targets to be imaged.
In front of the prototype lensless camera, we placed a chessboard pattern in an inclined plane on top of which we positioned 4 pawns of different colors at different depths from the coded mask. The distances are described in Figure~\ref{fig:RealExpSetup}(b). These objects were illuminated by a white LED light installed above the lensless camera and turned towards the scene.}

\subsection{PSF Calibration}
\label{sec:PsfCalibration}
The top row in Fig.~\ref{fig:RealExpResults} shows the calibrated PSFs. The PSFs were calibrated by experimental capture of a spherical wave emitted from a light point source placed approximately $60~\mathrm{cm}$ away from the coded mask. The light-point source we used was composed of a semiconductor laser whose central wavelength was 532~nm~(Stradus 532 by Vortran Laser Technology), followed by a spatial filter~(SFB-16DMRO-OBL40-25 by SIGMA KOKI) which contained a pinhole whose diameter was $25~\mathrm{\mu m}$. The combination of the laser with the spatial filter generated a spherical wave.

\subsection{Experiments}
%Capture
The second row of Fig.~\ref{fig:RealExpResults} shows the captured lensless measurements.
Although the captured image cannot be recognized by human vision, the encoded images of objects at multiple distances were multiply recorded based on the physical model in Eq.~\eqref{eq:3Dforward}. The captured image also contains color information. Note that each coded image of Fig.~\ref{fig:RealExpResults} was normalized for visualization.

%Reconstruction
\red{The PSF was calibrated by the point light source positioned $60~\mathrm{cm}$ away from the coded mask. Therefore, it is expected for the objects farther from the coded mask to be more easily reconstructed, while the closer ones should be more challenging. We observe that in both the ADMM and UDN reconstructions in Figure~\ref{fig:RealExpResults}, as the square patterns of the chessboard closer to the camera have blurry edges for the non-radial masks. Similarly, the red pawn, which is closest to the camera, is poorly reconstructed for the non-radial coded masks as well. In contrast to this, all radial masks produce an extended depth of field, being capable of reconstructing sharper square patterns. }

\red{Similarly to the simulation results presented in section~\ref{sec:Sim_results}, we can observe the tradeoff between high- and low-frequency of the periodic radial masks. The Radial periodic mask with 20 sections produces less sharp edges for the square patterns and pawns. On the other hand, the periodic mask with 60 sections produces sharper edges but noisier reconstructions. Our mask achieves a balance between these two, being capable of reconstructing sharp edges for the squares, and smooth sparse areas for the pawns.}

\section{Dual Depth Optical Experiment with a Prototype Camera}

\begin{figure*}
    \centering
    \includegraphics[width=\textwidth]{figs/DOF_exp/FullDofExpV2.jpg}
    \caption{Dual depth object reconstruction experiment. a) Shows the experimental setup, where we placed a plush toy $30~\mathrm{cm}$ away from the coded mask, and a printed OU pattern at $9~\mathrm{cm}$ away. b) Presents the point spread functions calibrated from a distance of $30~\mathrm{cm}$ away from the camera. c) Presents the captured sensor measurements for each coded mask. d) and e) are the reconstructions by the ADMM and UDN techniques, respectively. f) Shows a zoomed in image of the printed OU pattern reconstructed by the UDN algorithm.}
    \label{fig:DualDepthExp}
\end{figure*}

We present another optical experiment, leveraging the same prototype camera presented in Section~\ref{sec:ContinuousExpSetup}, but with a simplified experimental setup to provide additional validation towards the extended depth-of-field capabilities of our proposed system.

\subsection{Setup}
The experimental setup is similar to the simulations performed in Section~\ref{sec:simulation}, in which we place two diffuse objects in front of a lensless camera. The first object is a stuffed toy known as SANKEN, which is one of the symbols of Osaka University (SANKEN plush toy). The second object is a black tape with the letters 'OU' printed on it (OU letters). The SANKEN toy and the OU letters were placed at approximately 30 and 9 centimeters (cm) away from the coded mask, respectively. These objects were illuminated by a white LED light Installed around 12 cm above the camera. 

For the calibration of the PSF to be used in the reconstruction process, we followed the setup presented in Section~\ref{sec:PsfCalibration}, but placing the light source 30 centimeters away from the coded mask.

\subsection{experiments}
Rows (b) and (c) in Fig.~\ref{fig:DualDepthExp} show the calibrated PSFs and captured measurements, respectively. Note that the captured images were normalized for visualization.

The PSF was calibrated by the point light source positioned 30 cm away from the coded mask. Therefore, it is expected that the object at a 30 cm distance to be correctly reconstructed for all three types of coded masks that were used. The OU pattern, however, was placed 9 cm away from the coded mask and was expected to be more challenging to be reconstructed properly. The coded masks and reconstruction algorithms used here were the same as those used in the simulations. The reconstructions using the ADMM and UDN algorithms are presented in rows (d) and (e) of Fig.~\ref{fig:DualDepthExp}, respectively. The bottom row (f) shows a close-up view of the OU letters reconstructed by the UDN algorithm. As expected, the plush toy was correctly reconstructed in all experiments, independently of the type of coded mask or the reconstruction algorithm employed. We note, however, that reconstructing with resolving the two lettern on the OU pattern was only successful by the radial coded mask, due to its robustness against scaling of its PSF, i.e., extended DOF characteristics. The reconstructions using the FZA and Random coded masks, on the other hand, were blurred and the two letters seemed to mix together.

% In Section D of the supplementary materials we provide additional experiments using our prototype camera. More specifically we demonstrate the all-in-focus potential of our technique for imaging semi-transparent objects and objects behind transparent planes. These are specific scenarios in which the use of 2 dimensional depth maps estimated by monocular depth-estimation techniques\cite{Ming2021} are expected to fail. This is caused by the fact that information from objects at multiple distances are encoded in the same sensor pixels. Therefore, it remains unclear which of the multiple objects' depth is estimated. In such cases, using the generated 2 dimensional depth-map from such techniques does not allows us to effectively refocus the calibrated PSF for multi-depth reconstruction. 

\red{One alternative potential solution to the extended DOF imaging for lensless cameras is to perform monocular depth estimation\cite{Ming2021}, and subsequent use of the estimated 2 dimensional depth map to refocus the calibrated PSF so that different objects in the scene are reconstructed with PSFs calibrated for their depth. Depth estimation methods, however, are limited when facing semi-transparent and transparent planes in the ambient scene. This is caused by the fact that in such scenarios, the information from objects at multiple depths are mapped to the same pixels in the sensor and it is not obvious which depth gets estimated. We provide an additional experiment in Section D of the supplementary material to showcase the all-in-focus imaging potential of our proposed lensless camera in scenarios with semi-transparent objects where depth-based estimation techniques are expected to fail.}

