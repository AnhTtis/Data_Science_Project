%===============================================================================
% $Id: ifacconf.tex 19 2011-10-27 09:32:13Z jpuente $  
% Template for IFAC meeting papers
% Copyright (c) 2007-2008 International Federation of Automatic Control
%===============================================================================
\documentclass{ifacconf}

\usepackage{natbib}        % required for bibliography
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
%\usepackage{amsthm}
\usepackage{tikz,pgfplots}
\pgfplotsset{compat=newest}
\usetikzlibrary{patterns}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{subcaption}
\usepackage{caption}
\usetikzlibrary{patterns}
\usetikzlibrary{graphs}
\usetikzlibrary{graphs.standard}
\usetikzlibrary{matrix,arrows,fit,backgrounds,mindmap,plotmarks,decorations.pathreplacing}

\newtheorem{proposition}{Proposition}
\newtheorem{theorem}{Theorem}
\newtheorem{definition}{Definition}
\newtheorem{lemma}{Lemma}
\newtheorem{corollary}{Corollary}
\newtheorem{remark}{Remark}
\newtheorem{assumption}{Assumption}

\DeclareMathOperator{\1}{\textbf{1}} 
\DeclareMathOperator*{\argmin}{argmin}
\DeclareMathOperator*{\cov}{Cov}
\DeclareMathOperator*{\col}{col}
\DeclareMathOperator*{\var}{Var}
\DeclareMathOperator*{\adj}{adj}
\DeclareMathOperator*{\dist}{dist}
\DeclareMathOperator*{\R}{\mathcal{R}}


\newcommand{\todo}[1]{\textcolor{red}{#1}}

\tikzstyle{sensor} = [draw, fill=blue!20, rectangle, rounded corners,
minimum height=2em, minimum width=7em]
\tikzstyle{est} = [draw, fill=orange!20, rectangle, rounded corners,
minimum height=2em, minimum width=7em]
\tikzstyle{pinstyle} = [pin edge={to-,thin,black}]
%===============================================================================
\begin{document}
\begin{frontmatter}

\title{Distributed Parameter Estimation under Gaussian Observation Noises\thanksref{footnoteinfo}} 
% Title, preferably not more than 10 words.

\thanks[footnoteinfo]{This work was supported in the part by JSPS under Grants-in-Aid for Scientific Research Grant No. 22H01508 and 21F40376.	}

\author[First]{Jiaqi Yan} 
%\author[Second]{Kuo Li} 
\author[First]{Hideaki Ishii}

\address[First]{Department of Computer Science, Tokyo Institute of Technology, Japan. Emails: {jyan@sc.dis.titech.ac.jp, ishii@c.titech.ac.jp}.}
%\address[Second]{Center for Intelligent and Networked System, Department of Automation and BNRist, Tsinghua University, Beijing, China. E-mail: li-k19@mails.tsinghua.edu.cn.}


\begin{abstract}                % Abstract of not more than 250 words.
In this paper, we consider the problem of distributed parameter estimation in sensor networks. Each sensor makes successive observations of an unknown $d$-dimensional parameter, which might be subject to Gaussian random noises. They aim to infer true value of the unknown parameter by cooperating with each other. 
To this end, we first generalize the so-called dynamic regressor extension and mixing (DREM) algorithm to stochastic systems, with which the problem of estimating a $d$-dimensional vector parameter is transformed to that of $d$ scalar ones: one for each of the unknown parameters. 
For each of the scalar problem, an estimation scheme is given, where each sensor fuses the regressors and measurements in its in-neighborhood and updates its local estimate by using least-mean squares. Particularly, a counter is also introduced for each sensor, which prevents any (noisy) measurement from being repeatedly used such that the estimation performance will not be greatly affected by certain extreme values.
A novel excitation condition termed as \textit{local persistent excitation} (Local-PE) condition is also proposed, which relaxes the traditional persistent excitation (PE) condition and only requires that the collective signals in each sensor's in-neighborhood are sufficiently excited. With the Local-PE condition and proper step sizes, we show that the proposed estimator guarantee that each sensor infers the true parameter in mean square, even if any individual of them cannot. Numerical examples are finally provided to illustrate the established results.

\end{abstract}

\end{frontmatter}
%===============================================================================

\section{Introduction}
As a fundamental problem appearing in various applications such as signal processing, system identification and adaptive control, parameter estimation has been extensively studied in the literature  (see, e.g., \cite{goodwin2014adaptive,xie2020convergence,schizas2009distributed,chen2013distributed,cattivelli2008diffusion,yan2022distributed,yan2021resilient}). In this problem, sensors observe (partial) information of a system with an unknown (vector) parameter, and attempt to infer the true parameter through a stream of observations.

As for a single sensor, it is well known that the consistent estimation is possible only if its regressor meets certain excitation conditions \cite{goodwin2014adaptive}. Moreover, a \textit{persistent excitation} (PE) condition, which requires that the input signals are sufficiently rich such that all modes of the plant can be excited, is particularly needed to achieve an exponential convergence. 
However, in a network of multiple sensors, the PE condition may not necessarily hold at each individual sensor's side. To solve this problem, researchers leverage communication among sensors and introduce consensus algorithms into the estimators design. By doing so, weaker excitation conditions have been proposed under which the estimation task can be cooperatively fulfilled by the entire sensor network. For example, by using least-mean squares estimators, \cite{chen2013distributed} have developed a cooperative PE condition with which sensors collectively satisfy the PE condition and obtain the parameter even if no individual sensor can do so. In a recent work \cite{matveev2021diffusion}, it has also been shown that a single sensor satisfying the PE condition can lead all the others to converge to the true parameter provided that it is sufficiently connected. 

Different from the aforementioned works which focus on deterministic systems, another group of literature further considers the case where the sensors' measurements might be subject to stochastic noises (\hspace{1pt} \cite{abdolee2014diffusion,piggott2015stability,gharehshiran2013distributed,xie2018analysis,nosrati2015adaptive}). Most of them require that the regressors are generated to satisfy certain statistical independence or stationarity conditions. For example, in \cite{abdolee2014diffusion} and \cite{takahashi2010diffusion}, performance of the proposed estimators has been studied in the sensor networks where the regessors and measurement noises are independently
and identically distributed (i.i.d.) in both time and space. Despite the elegant results therein, the independence and stationarity assumptions are easily violated when the regressors are generated
from feedback systems (\cite{xie2018analysis}). In this regard, \cite{xie2018analysis} and \cite{xie2020convergence} have proposed a cooperative stochastic information condition, with which the independence
and stationarity conditions in the previous works are relaxed. 

Inspired by all these issues, in this paper, we also consider the problem of distributed parameter estimation under Gaussian observation noises. Different from the existing works, a novel excitation condition termed as \textit{local persistent excitation} (Local-PE) condition is going to be proposed, which requires the collective signals in each sensor's in-neighborhood are sufficiently excited. Note that, the Local-PE condition naturally generalizes the PE condition from a single sensor to the sensor network. However, it is strictly
weaker than the traditional PE condition, since it is possible that the regressor of each individual sensor
does not verify the PE condition, while the regressors of all sensors cooperatively
satisfy the Local-PE condition. Moreover, since no stringent conditions such as statistical independence
and stationarity are required on the regressors, the Local-PE condition is easily to meet even in the stochastic systems with feedback.

Our estimator is developed inspired by the so-called dynamic regressor extension and mixing (DREM) algorithm,
which is a new procedure to design parameter identification schemes. The DREM algorithm was first introduced in \cite{aranovskiy2017performance} and recently reviewed in \cite{ortega2020new}. However, although it reveals decent performance in simplifying the implementation, relaxing the excitation condition, and guaranteeing the asymptotic convergence, the study of this procedure only involves deterministic systems and there is no relevant work in stochastic systems. Therefore, in this paper, in order to accommodate the Gaussian noises, we first generalize the traditional DREM algorithm to stochastic settings. Then, we propose a distributed estimation scheme based on the stochastic DREM algorithm. To be specific, by leveraging DREM, we transform the problem of estimating a $d$-dimensional vector parameter to that of $d$ scalar ones: one for each of the unknown parameters. For each of the scalar problem, an estimation strategy is given, where each sensor fuses the regressors and measurements in its in-neighborhood and updates its own estimate by using least-mean squares. Moreover, notice that under the Gaussian noises, some sensors' measurements might take rather extreme values such that the estimation performance would be greatly affected. To solve this problem, in the proposed algorithm, we also introduce a counter for each sensor, which prevents any (noisy) measurement from being repeatedly used.

With the Local-PE condition and proper step sizes in making updates, the proposed estimator is proved to be efficient, in the sense that it guarantee that all sensors cooperatively fulfill the estimation task. Specifically, each of them infers the true parameter in mean square, even if any individual of them cannot. 


%The remainder of this paper is organized as follows. In Section~\ref{sec:form}, we formulate the problem of parameter estimation, which is subject to the stochastic observation noises. The distributed estimator is proposed in Section~\ref{sec:algo}, with the convergence analysis carried out in Section~\ref{sec:analysis}.  Finally, we verify the established results through numerical examples in Section~\ref{sec:simulation} and conclude the paper in Section~\ref{sec:conclude}.

\textit{Notations}: For a vector $v,$ we denote by $v^\prime$ its transpose. Moreover, $\mathbb{R}$ and $\mathbb{N}$ represent the sets of real numbers and natural numbers, respectively.



\section{Problem Formulation}\label{sec:form}

Let us consider a sensor network consisting of $n$ sensors. At each time instant $k$, any sensor $i\in \{1, \cdots, n\}$ receives a noisy measurement $y_{i}(k)\in\mathbb{R}$ and a $d$-dimensional regressor $\phi_i(k) \in \mathbb{R}^{d}$, which are related via the following stochastic linear regression model:
\begin{equation}\label{eqn:LRE}
y_{i}(k)=\theta^\prime \phi_i(k)+v_{i}(k), \quad k \geq 0,
\end{equation}
where $\theta\in \mathbb{R}^{m}$ is the parameter to be estimated, and $v_{i}(k)\in\mathbb{R}$ is independent and identically distributed (i.i.d.) Gaussian noise with zero mean and covariance $R_i\geq0$. Notice that $R_i$ need not to be known by any sensor.

%\begin{remark}
%	As reported in \cite{goodwin2014adaptive}, the input-output relationship of a large class of stochastic
%	linear and nonlinear dynamical systems can be described by \eqref{eqn:LRE}. Specifically, $\phi_i(k)$ denotes a vector that is either a linear or nonlinear function of 
%	$\mathcal{U}_i(k)\triangleq\{u_i(0),\cdots,u_i(k-1)\}$ and $\mathcal{Y}_i(k)\triangleq\{y_i(0),\cdots,y_i(k-1)\}$, where $u_i(t)$ and $y_i(t)$ are respectively the input and output signals of the $i$-th subsystem at time $t$.
%	Notice that the problem of estimating $\theta$ through \eqref{eqn:LRE} is fundamental in many applications such as system identification and adaptive control (\cite{goodwin2014adaptive,aastrom2013adaptive}).
%\end{remark} 

The sensors aim to estimate $\theta$ from a stream of (noisy) measurable signals. However, in a practical network, a single sensor may not be sufficiently excited. That means the signals available at its local side are not enough to consistently estimate the parameter $\theta$. In this respect, each sensor aims to obtain an accurate estimate on $\theta$ by communicating over
a time-varying directed communication graph, which is modeled by $\mathcal{G}=(\mathcal{V},\mathcal{E}(k))$. Here, $\mathcal{V}$ is the set of sensors, and $\mathcal{E}(k)\subseteq \mathcal{V}\times\mathcal{V}$ is the set of edges. An edge from sensor $j$ to sensor $i$ is denoted by $e_{ij}(k)\in \mathcal{E}(k)$, indicating sensor $i$ can receive the information directly from sensor $j$ at time $k$. Accordingly, the sets of in-neighbors and out-neighbors of sensor $i$ are defined, respectively, as 
\begin{equation}
\begin{split}
\mathcal{N}_i^+(k)&\triangleq\{j\in \mathcal{V}|e_{ij}(k)\in \mathcal{E}(k)\},\\\mathcal{N}_i^-(k)&\triangleq\{j\in \mathcal{V}|e_{ji}(k)\in \mathcal{E}(k)\}.
\end{split}
\end{equation}
Moreover, let us denote
\begin{equation}
\mathcal{J}_i^+(k) \triangleq \mathcal{N}_i^+(k) \cup \{i\}.
\end{equation}

%In the following sections, we shall present a distributed algorithm for doing parameter estimation,  the performance of which would also be analyzed.

\section{Estimation Algorithm Design}\label{sec:algo}
This section will present the distributed estimation algorithm. Specifically, by extending the so-called \textit{dynamic regressor extension and mixing} (DREM) algorithm to stochastic settings, we decouple the $d$-dimensional estimation problem into $d$ scalar ones, each of which corresponds to an entry of the unknown vector parameter.

\subsection{Stochastic DREM}
To begin with, we shall introduce the DREM algorithm, which was first proposed in \cite{aranovskiy2017performance} and recently reviewed in \cite{ortega2020new}. Specifically, the DREM algorithm is expressed by the following variables for each sensor $i\in\mathcal{V}$:
\begin{equation}\label{eqn:definition}  
\begin{split}
\Phi_{i}(k)&\triangleq\begin{bmatrix}
\left(\phi_{i}(k)\right)^\prime \\
\left(\phi_{i}(k-1)\right)^\prime \\
\vdots \\
\left(\phi_{i}(k-d+1)\right)^\prime
\end{bmatrix}\in\mathbb{R}^{d\times d}, \\\overline{y}_{i}(k) &\triangleq\adj(\Phi_{i}(t))\left[\begin{array}{c}
y_{i}(k) \\
y_{i}(k-1) \\
\vdots \\
y_{i}(k-d+1)
\end{array}\right]\in\mathbb{R}^{d},\\
\overline\delta_{i}(k)&\triangleq \det(\Phi_{i}(k)),
\end{split}                      
\end{equation}
where we respectively denote by $\adj(\Phi_{i}(k))$ and $\det(\Phi_{i}(k))$ the adjugate matrix and determinant of matrix $\Phi_{i}(k)$.

However, note that the traditional DREM algorithm is only developed in deterministic systems (see, for example, \cite{aranovskiy2017performance,ortega2020new,pyrkin2019adaptive,yi2022conditions,matveev2021diffusion,bobtsov2022generation}). Therefore, in order to accommodate the Gaussian noises here, we should make subtle modifications on the DREM algorithm. To this end, let us define
\begin{equation}\label{eqn:noise}
\overline{v}_i(k)\triangleq \adj(\Phi_{i}(t)) \begin{bmatrix}
v_{i}(k) \\
v_{i}(k-1)\\
\vdots \\
v_{i}(k-d+1)
\end{bmatrix}.
\end{equation}
Notice that both $\overline{y}_{i}(k)$ and $\overline{v}_{i}(k)$ are vectors in the $d$-dimensional space. For simplicity, we denote by $\overline{y}^\ell_{i}(k)$ and $\overline{v}^\ell_{i}(k)$ the $\ell$-th entry of $\overline{y}_{i}(k)$ and $\overline{v}_{i}(k)$, respectively. 

We first introduce the following lemma, which extends a result in \cite{aranovskiy2017performance} to stochastic systems:

\begin{lemma}\label{lmm:Y}
	Consider the network of sensors satisfying the stochastic linear regression model \eqref{eqn:LRE}. For each $\ell\in\{1,\cdots,d\},$ it holds for any $i\in\mathcal{V}$ that
	\begin{equation}\label{eqn:scalarLRE}
	\overline{y}_i^\ell(k) = \overline\delta_{i}(k)\theta^\ell+\overline{v}_i^\ell(k),
	\end{equation}
	where $\theta^\ell$ is the $\ell$-th entry of the true parameter $\theta$.
\end{lemma}
%\begin{pf}
%	The proof follows immediately by noting that
%	\begin{equation}\label{eqn:Y}
%	\begin{split}
%	\begin{bmatrix}
%	y_{i}(k) \\
%	y_{i}(k-1) \\
%	\vdots \\
%	y_{i}(k-d+1)
%	\end{bmatrix}
%	&=\begin{bmatrix}
%	\left(\phi_{i}(k)\right)^\prime \\
%	\left(\phi_{i}(k-1)\right)^\prime \\
%	\vdots \\
%	\left(\phi_{i}(k-d+1)\right)^\prime
%	\end{bmatrix}\theta +\begin{bmatrix}
%	v_{i}(k) \\
%	v_{i}(k-1)\\
%	\vdots \\
%	v_{i}(k-d+1)
%	\end{bmatrix}.
%	\end{split}
%	\end{equation}
%	Then multiplying \eqref{eqn:Y} from left by $\adj(\Phi^{i}(t))$, we have
%	\begin{equation}
%	\overline{y}_i(k) = \overline\delta_{i}(k)\theta+\overline{v}_i(k),
%	\end{equation}
%	where the equality holds because for any matrix $M\in\mathbb{R}^{d\times d}$, it follows that
%	$
%	\adj(M) M=\det(M)  I_{d}.
%	$
%	We thus compete the proof.\hfill$\square$
%\end{pf}

%Notice that by \eqref{eqn:noise}, it is easy to see that the noises $\overline{v}_i(k+1)$ and $\overline{v}_i(k)$ are not independent anymore. The reason is that the sequence of measurements $\{y_i(t)\}_{t\in[k-d,k]}$ is used in calculating both $\overline{y}_{i}(k+1)$ and $\overline{y}_{i}(k)$. However, in this stochastic setting, some measurements might take rather extreme values such that the estimation performance would be greatly affected. To solve this problem, in the next section, we would propose a distributed estimation algorithm, where each sensor is assigned a counter. By doing so, it can be guaranteed that each (noisy) measurement is used at most once.


\subsection{The proposed algorithm}
Notice that, by leveraging the stochastic DREM, we generate $d$ scalar ones as presented in \eqref{eqn:scalarLRE}: one for each of the unknown parameters. Based on it, we are ready to propose our distributed estimation algorithm.
Specifically, each sensor $i\in \mathcal{V}$ starts with the initial counter $c_i(0)=0$ and any initial estimate $\hat\theta_i(0)\in \mathbb{R}^d$. At any time $k\geq 0$, it makes an estimation as outlined in Algorithm~\ref{alg:CTA}. 

\begin{algorithm}[h!] 
	1:\: Receive $(\overline{y}_j(k), \overline\delta_j(k))$ from all in-neighboring sensors.
	
	2:\: \textbf{for} $\ell\in\{1,2,...,d\}$ \textbf{do}
	\qquad \begin{itemize}
		\item[\;] By fusing the neighboring messages, sensor $i$ updates the $\ell$-th entry of its local estimate as
		\begin{equation}\label{eqn:update}
		\begin{split}
		&\hat{\theta}_i ^\ell(k+1)=\hat{\theta}^{\ell}_{i}(k)\\&+\!\frac{\alpha(k)\Big[\sum_{j\in\mathcal{J}_i^+(k)}\delta_{j}(k)\left(\overline{y}_j^\ell(k)-\delta_{j}(k)\hat{\theta}^\ell_i(k)\right)\!\!\Big]}{\mu_i+\sum_{j\in\mathcal{J}_i^+(k)}(\delta_j(k))^2},
		\end{split}
		\end{equation}
		where $\mu_i>0$, $\alpha(k)$ is the step size to be designed later, and
		\begin{equation}\label{eqn:delta}
			\delta_{j}(k) \triangleq \begin{cases}
				\overline\delta_{j}(k), \text{ if } c_i(k) \geq d,\\
				0, \text{ otherwise,}
			\end{cases} \forall j\in\mathcal{J}_i^+(k).
		\end{equation}
	\end{itemize}
	\quad \textbf{end for}
	
	3:\: Reset the counter as
	\begin{equation}\label{eqn:counter}
		c_i(k+1) =\begin{cases}
			0, \text{ if }	\sum_{j\in\mathcal{J}_i^+(k)}(\overline\delta_{i}(k))^2 \neq 0 \text{ and } c_i(k)\geq d,\\
			c_i(k)+1, \text{ otherwise. } 
		\end{cases}
	\end{equation} 

	4:\: Transmit $(\overline{y}_i(k+1), \overline\delta_i(k+1))$ to out-neighbors. \\
	\caption{A distributed estimation algorithm under Gaussian observation noises}
	\label{alg:CTA}
\end{algorithm}

In the proposed algorithm, the sensors communicate with each other their local information on $\overline{y}_i(k)\in\mathbb{R}^d$ and $\overline\delta_i(k)\in\mathbb{R}$. Therefore, at each instant, any sensor transmits the message of size $d+1$. After that, by fusing the information within the in-neighborhood, each sensor updates its local estimate by performing \eqref{eqn:update}, which is developed based on the well-known least-mean square (LMS) algorithm (\hspace{1pt}\cite{alexander2012adaptive,lopes2008diffusion,xie2018analysis,cattivelli2008diffusion}).  
%As reported in \cite{alexander2012adaptive}, the LMS algorithm achieves decent performance in many aspects: simplicity, robustness, and numerical stability, and is thus widely applied in many research areas, such as system identification, adaptive control, adaptive signal processing, and so on. 
Notice that, the counter $c_i(k)$ is particularly introduced to prevent any measurement from being repeatedly used. To see this, notice that sensor $i$ updates its local estimate only when $\sum_{j\in\mathcal{J}_i^+(k)}(\delta_{j}(k))^2 \neq 0$, where measurements $\{y_j(t)\}_{j\in \mathcal{J}_i^+(k), t\in[k-d+1,k]}$ will be incorporated. After that, it resets the counter $c_i(k)$ to $0$. From \eqref{eqn:update} and \eqref{eqn:counter}, we understand that sensor $i$ can only make a new update after time $k+d$, where certain measurements after $k+1$ will be needed. Therefore, each measurement can be used at most once, which guarantees that any extreme measurement will not greatly affect the estimation performance.


%\begin{remark}
%As compared to the estimation algorithms as proposed in \cite{matveev2021diffusion,aranovskiy2017performance,chen2013distributed}, which focus on deterministic systems, Algorithm~\ref{alg:CTA} is developed under Gaussian random noises. Particularly, in order to accommodate DREM to the stochastic settings, we introduce the counter $c_i(k)$ for each sensor, which protects the estimation algorithm from being significantly affected by extreme noisy measurements. Moreover, instead of directly communicating with their local estimates as in these existing works, Algorithm~\ref{alg:CTA} requires that sensors send out the ``coded measurements" $\overline{y}_i(k)$ and the determinant of regressors $\overline\delta_i(k)$ at each time. This helps to protect the privacy of individual sensors as well. We further notice that in \eqref{eqn:update}, a sequence of time-varying step sizes $\{\alpha(k)\}$ is adopted. This is different from the works \cite{xie2018analysis} and \cite{xie2020convergence}, in which the step sizes are constant. We will soon show in Section~\ref{sec:analysis} that, by properly designing $\alpha(k)$, our algorithm can guarantee the mean-square convergence to the true parameter at each sensor side.
%\end{remark}

\section{Performance Analysis}\label{sec:analysis}
This section is devoted to the performance analysis of Algorithm~\ref{alg:CTA}. Specifically, we would show that each sensor can infer the true parameter in mean square.

\subsection{Local persistent excitation (Local-PE) condition}
As observed from \eqref{eqn:update}, one factor that affects
convergence properties of the proposed estimator is the
determinant of the extended regressor. That is, the scalar regressor $\overline\delta_{i}(k),\;\forall i\in\mathcal{V}$. Therefore, in this subsection, we would first discuss the excitation conditions on $\overline\delta_{i}(k)$. 

In the literature, it is well known that a 
\textit{persistent excitation} (PE) condition, which guarantees the input signals to be sufficiently rich such that all modes of the plant can be
excited, is usually required to achieve an exponential convergence (\hspace{1pt}\cite{goodwin2014adaptive,aastrom2013adaptive,anderson1982exponential}). However, in the sensor network, the
PE condition may not necessarily hold at each sensor's side. To address this issue, in this paper, we will relax the PE condition
imposed on every local sensor. Instead, a new excitation condition will be proposed, which only requires
that persistently exciting signals are cooperatively generated within the neighborhood of each sensor:

\begin{definition}[Local persistent excitation (Local-PE)]\label{def:localPE}
	For any sensor $i\in\mathcal{V}$, the regressors within its in-neighborhood is said to satisfy a Local-PE condition, if there exist $\omega>0$ and a finite time $H\in\mathbb{N}_{+}$ such that 
	\begin{equation}\label{eqn:PE}
	\sum_{t=k}^{k+H-1}\Big[\sum_{j\in\mathcal{J}_i^+(k)}(\overline\delta_j(k))^2\Big] \geq \omega, \;\forall k.
	\end{equation} 
\end{definition}

%Notice that, in most of the existing solutions on distributed
%adaptive filters, such as \cite{abdolee2014diffusion,piggott2015stability,gharehshiran2013distributed}, it is required that the regressors are generated independently and statistically stationary, which is hard to be guaranteed if the regressors are generated
%from feedback systems (\cite{xie2018analysis}). In contrast, the proposed Local-PE condition \eqref{eqn:PE} naturally generalizes the PE condition from a single sensor to the sensor network. Yet, it is much
%weaker than the traditional PE condition, since it is possible that the regressor of each individual sensor
%does not verify the PE condition, while the regressors of all sensors cooperatively
%satisfy the Local-PE condition. We also highlight that the Local-PE condition can be applicable in time-varying communication graphs.

%\begin{remark}
%As proved by \cite{yi2022conditions} and \cite{aranovskiy2022preserving}, the scalar regressors $\{\delta_{i}(k)\}$ satisfy the persistent excitation condition if the original regressors $\{\phi_i(t)\}$ also satisfy the persistent excitation condition. Therefore, one could replace the Local-PE condition on scalar regressors, i.e., \eqref{eqn:PE}, with the following one:
%	\begin{equation}
%	\sum_{t=k}^{k+H-1}\Big[\sum_{j\in\mathcal{J}_i^+(k)}\phi_{j}(k)\phi_{j}^\prime (k)\Big] \geq \rho I, \;\forall k,
%	\end{equation} 
%	where $\rho>0$.
%\end{remark}

\subsection{Performance analysis}
In this subsection, we shall prove that, under the Local-PE condition \eqref{eqn:PE}, the network of sensors can cooperatively fulfill
the estimation task, even if none of the individual sensors can. To see this, we first introduce assumptions that would be adopted in this paper:

\begin{assumption}\label{assup:assumptions}
	\begin{enumerate}
		\item For each sensor $i$, the regressor $\phi_i(k)$ is bounded at any time $k$. 
		\item The Local-PE condition \eqref{eqn:PE} is verified by each sensor. 
		\item The stepsize $\{\alpha(k)\}$ is monotonically non-increasing. Moreover, it satisfies that 
		\begin{equation}\label{eqn:alpha}
		0<\alpha(k)\leq 1, \;\sum_{k=0}^\infty \alpha(k) = \infty, \;\lim_{k \to \infty} \alpha(k)=0.
		\end{equation}
	\end{enumerate}
\end{assumption}



%The following lemma is also presented, which is a key technique of our analysis:
%
%\begin{lemma}\hspace{1pt}{\cite[Lemma~A.1]{li2010consensus}}\label{lmm:converge}
%	\quad\newline
%	Let $\{u(k)\},\{p(k)\}$ and $\{q(k)\}$ be real sequences such that 
%	$$
%	u(k+1) \leq(1-q(k)) u(k)+p(k) .
%	$$
%	Suppose that the following conditions hold:
%	\begin{enumerate}
%		\item For any $k\in\mathbb{N}$, it holds that $p(k) \geq 0$.
%		\item The step size satisfies that $0< q(k) \leq 1$ and $\sum_{k=0}^{\infty} q(k)=\infty$.
%		\item $\lim\limits_{k \to \infty}\frac{p(k)}{q(k)} = 0$.
%	\end{enumerate}
%	If $u(k)\geq 0$, then $u(k)$ asymptotically converges to $0$, namely, $$\lim _{k \rightarrow \infty} u(k) =0.$$
%\end{lemma}
To prove the convergence of estimation error, let us consider any $\ell \in \{1,\cdots,d\}$. For each sensor $i\in\mathcal{V}$, we define the estimation error of it at $\ell$-th entry as
\begin{equation}
\widetilde{\theta}_i ^\ell(k) \triangleq \hat{\theta}_i ^\ell(k)-\theta^\ell.
\end{equation}
We are now ready to provide the main theorem of this paper:
\begin{theorem}\label{thm:converge}
	Consider the network of sensors satisfying the stochastic linear regression model \eqref{eqn:LRE}.
	Suppose that Assumption~\ref{assup:assumptions} holds. Then by performing Algorithm~\ref{alg:CTA}, the local estimate of each sensor converges to the true parameter in mean square. That is, for any $i\in\mathcal{V}$, it follows that
	\begin{equation}
	\lim\limits_{k\to \infty} \mathbb{E}[(\widetilde{\theta}_i^\ell(k))^2]=0.
	\end{equation}
\end{theorem}

\begin{pf}
First, notice that by \eqref{eqn:update}, for each sensor $i$, it updates the local estimate only when $c_i(k)\geq d$. Recall the definition of $\delta_{j}(k)$ in \eqref{eqn:delta}. From the Local-PE condition \eqref{eqn:PE}, there must exist $T\triangleq H+d$ such that
\begin{equation}\label{eqn:PE2}
	\sum_{t=k}^{k+T-1}\Big[\sum_{j\in\mathcal{J}_i^+(k)}(\delta_j(k))^2\Big] \geq \omega, \;\forall k.
\end{equation} 
Under this condition, it can be verified that, over any period of length $T$, sensor $i$ has at least one in-neighbor that is sufficiently excited.  That is, for any $m\in\mathbb{N}$, there exists $k\in [mT, (m+1)T-1]$ such that 
\begin{equation}
	\sum_{j\in\mathcal{J}_i^+(k)}(\delta_j(k))^2 \geq \Delta, 
\end{equation} 
where
$
\Delta \triangleq \omega/T.
$
	
Combining \eqref{eqn:scalarLRE} and \eqref{eqn:update}, the dynamics of $\widetilde{\theta}_i^\ell(k)$ is obtained as 
\begin{equation}
\begin{split}
&\widetilde{\theta}_i^\ell(k+1)=\hat{\theta}^{\ell}_{i}(k+1)-\theta^\ell\\&=\hat{\theta}^{\ell}_{i}(k)-\theta^\ell+\frac{\alpha(k)}{\mu_i+\sum_{j\in\mathcal{J}_i^+(k)}(\delta_j(k))^2}\\&\;\times\sum_{j\in\mathcal{J}_i^+(k)}\delta_{j}(k)\left((\overline{y}_j^\ell(k)-\overline\delta_{j}(k)\theta^\ell)-\delta_{j}(k)\hat{\theta}_i(k)+\overline\delta_{j}(k)\theta^\ell\right)\\&=\hat{\theta}^{\ell}_{i}(k)-\theta^\ell+\frac{\alpha(k)}{\mu_i+\sum_{j\in\mathcal{J}_i^+(k)}(\delta_j(k))^2}\\&\;\times\sum_{j\in\mathcal{J}_i^+(k)}\delta_{j}(k)\left(\overline{v}_j^\ell(k)-\delta_{j}(k)\hat{\theta}_i(k)+\overline\delta_{j}(k)\theta^\ell\right).
\end{split}
\end{equation}
Then, by \eqref{eqn:delta}, we 
can conclude
\begin{equation}\label{eqn:error}
\begin{split}
&\widetilde{\theta}_i^\ell(k+1)=\left(1-\frac{\alpha(k)\sum_{j\in\mathcal{J}_i^+(k)}(\delta_{j}(k))^2}{\mu_i+\sum_{j\in\mathcal{J}_i^+(k)}(\delta_j(k))^2}\right)\widetilde{\theta}_i^\ell(k)\\&\quad+\frac{\alpha(k)}{\mu_i+\sum_{j\in\mathcal{J}_i^+(k)}(\delta_j(k))^2}\sum_{j\in\mathcal{J}_i^+(k)}\delta_{j}(k)\overline{v}_j^\ell(k).
\end{split}
\end{equation}

We shall study both the mean and covariance of $\widetilde{\theta}_i^\ell(k)$. From \eqref{eqn:error}, it is not difficult to see that
\begin{equation}\label{eqn:mean_dyn}
\begin{split}
\mathbb{E}[\widetilde{\theta}_i^\ell(k+1)]&=\left(1-\frac{\alpha(k)\sum_{j\in\mathcal{J}_i^+(k)}(\delta_{j}(k))^2}{\mu_i+\sum_{j\in\mathcal{J}_i^+(k)}(\delta_j(k))^2}\right)\mathbb{E}[\widetilde{\theta}_i^\ell(k)] \\&=(1-\beta_i(k))\mathbb{E}[\widetilde{\theta}_i^\ell(k)],
\end{split}
\end{equation}
where
\begin{equation}\label{eqn:beta_def}
\begin{split}
\beta_i(k)\triangleq\frac{\alpha(k)\sum_{j\in\mathcal{J}_i^+(k)}(\delta_{j}(k))^2}{\mu_i+\sum_{j\in\mathcal{J}_i^+(k)}(\delta_j(k))^2}.
\end{split}
\end{equation}
It thus follows that
$
0\leq \beta_i(k)\leq \alpha(k).
$
Therefore, one knows that
$
\beta_i(k)\in[0,1].
$
Notice that when $\beta_i(k)=0$, namely, $\sum_{j\in\mathcal{J}_i^+(k)}(\delta_{j}(k))^2=0$, then $\widetilde{\theta}_i^\ell(k)$ remains unchanged. Therefore, we only focus on the case where  $\beta_i(k)\in(0,1]$.
To see this, notice that       
\begin{equation}
\begin{split}
\sum_{k=0}^{\infty} \beta_i(k) &= \sum_{m=0}^{\infty} \sum_{k=nT}^{(n+1)T-1}\beta_i(k)\\&= \sum_{m=0}^{\infty} \sum_{k=mT}^{(m+1)T-1}\frac{\alpha(k)\sum_{j\in\mathcal{J}_i^+(k)}(\delta_{j}(k))^2}{\mu_i+\sum_{j\in\mathcal{J}_i^+(k)}(\delta_j(k))^2}\\& \geq \sum_{m=0}^{\infty}\frac{\alpha((m+1)T-1)\Delta}{\mu_i+\Delta}\\&\geq \frac{\Delta}{\mu_i+\Delta}\frac{\sum_{k=T}^\infty \alpha(k)}{T}=\infty,
\end{split}
\end{equation}
where the inequalities hold due to Assumption~\ref{assup:assumptions}(3) on $\alpha(k)$. One thus concludes from \eqref{eqn:mean_dyn} that
\begin{equation}\label{eqn:mean}
\begin{split}
\lim_{k \to \infty} \mathbb{E}[\widetilde{\theta}_i^\ell(k)] &=\lim_{k \to \infty}\left(\prod_{t=0}^{k-1}(1-\beta_i(t))\right)\mathbb{E}[\widetilde{\theta}_i^\ell(0)]\\&\leq\lim_{k \to \infty} \left(\prod_{t=0}^{k-1}e^{-\beta_i(t)}\right)\mathbb{E}[\widetilde{\theta}_i^\ell(0)]\\&=e^{-\lim_{k \to \infty}\sum_{t=0}^{k-1}\beta_i(t)}\mathbb{E}[\widetilde{\theta}_i^\ell(0)]=0.
\end{split}
\end{equation}

On the other hand, we shall investigate the covariance of $\widetilde{\theta}_i^\ell(k)$. Again, we will only focus on the time instant $k$ when sensor $i$'s local estimate will be updated. As discussed previously, by introducing the counter $c_i(k)$, it follows that
$
\widetilde{\theta}_i^\ell(k) = \widetilde{\theta}_i^\ell(k-d).
$
Therefore, $\widetilde{\theta}_i^\ell(k-d)$ and $\overline{v}_j^\ell(k)$ are independent. We thus conclude from \eqref{eqn:error} that
\begin{equation}\label{eqn:cov}
\begin{split}
&\cov[\widetilde{\theta}_i^\ell(k+1)]\\=&\left(1-\frac{\alpha(k)\sum_{j\in\mathcal{J}_i^+(k)}(\delta_{j}(k))^2}{\mu_i+\sum_{j\in\mathcal{J}_i^+(k)}(\delta_j(k))^2}\right)^2\cov[\widetilde{\theta}_i^\ell(k)]\\+&\left(\frac{\alpha(k)}{\mu_i+\sum_{j\in\mathcal{J}_i^+(k)}(\delta_j(k))^2}\right)^2\sum_{j\in\mathcal{J}_i^+(k)}(\delta_{j}(k))^2\cov[\overline{v}_j^\ell(k)]\\=&\left(1-2\frac{\alpha(k)\sum_{j\in\mathcal{J}_i^+(k)}(\delta_{j}(k))^2}{\mu_i+\sum_{j\in\mathcal{J}_i^+(k)}(\delta_j(k))^2}\right. \\+&\left. \left(\frac{\alpha(k)\sum_{j\in\mathcal{J}_i^+(k)}(\delta_{j}(k))^2}{\mu_i+\sum_{j\in\mathcal{J}_i^+(k)}(\delta_j(k))^2}\right)^2\right)\cov[\widetilde{\theta}_i^\ell(k)]\\+&\left(\frac{\alpha(k)}{\mu_i+\sum_{j\in\mathcal{J}_i^+(k)}(\delta_j(k))^2}\right)^2 \sum_{j\in\mathcal{J}_i^+(k)}(\delta_j(k))^2\cov[\overline{v}_j^\ell(k)].
\end{split}
\end{equation}
Notice that
\begin{equation}
\begin{aligned}
&-\frac{\alpha(k)\sum_{j\in\mathcal{J}_i^+(k)}(\delta_{j}(k))^2}{\mu_i+\sum_{j\in\mathcal{J}_i^+(k)}(\delta_j(k))^2} + \left(\frac{\alpha(k)\sum_{j\in\mathcal{J}_i^+(k)}(\delta_{j}(k))^2}{\mu_i+\sum_{j\in\mathcal{J}_i^+(k)}(\delta_j(k))^2}\right)^2\\
&=\frac{\alpha(k)\sum_{j\in\mathcal{J}_i^+(k)}(\delta_{j}(k))^2}{\mu_i+\sum_{j\in\mathcal{J}_i^+(k)}(\delta_j(k))^2}\left( \frac{\alpha(k)\sum_{j\in\mathcal{J}_i^+(k)}(\delta_{j}(k))^2}{\mu_i+\sum_{j\in\mathcal{J}_i^+(k)}(\delta_j(k))^2}-1\right) \\&< 0,
\end{aligned}
\end{equation}
where the inequality holds as $\alpha(k)\in(0,1]$. We conclude 

\begin{equation}\label{eqn:cov2}
\begin{split}
&\cov[\widetilde{\theta}_i^\ell(k+1)]\\&<
\left(1-\frac{\alpha(k)\sum_{j\in\mathcal{J}_i^+(k)}(\delta_{j}(k))^2}{\mu_i+\sum_{j\in\mathcal{J}_i^+(k)}(\delta_j(k))^2} \right)\cov[\widetilde{\theta}_i^\ell(k)]\\&\;+\left(\frac{\alpha(k)}{\mu_i+\sum_{j\in\mathcal{J}_i^+(k)}(\delta_j(k))^2}\right)^2\sum_{j\in\mathcal{J}_i^+(k)}(\delta_j(k))^2\cov[\overline{v}_j^\ell(k)].
\end{split}
\end{equation}
For simplicity, let us define
\begin{equation}
\begin{split}
&\varepsilon_i(k) \\&\triangleq \Bigg(\frac{\alpha(k)}{\mu_i+\sum_{j\in\mathcal{J}_i^+(k)}(\delta_j(k))^2}\Bigg)^2\sum_{j\in\mathcal{J}_i^+(k)}(\delta_j(k))^2\cov[\overline{v}_j^\ell(k)]\\&\geq 0.
\end{split}
\end{equation}
We thus rewrite \eqref{eqn:cov2} as
\begin{equation}
\cov[\widetilde{\theta}_i^\ell(k+1)]< (1-\beta_i(k))\cov[\widetilde{\theta}_i^\ell(k)]+ \varepsilon_i(k).
\end{equation}
As assumed, $\phi_i(k)$ is bounded. Therefore, $\adj(\Phi_{i}(k))$ is also bounded by some constant $B<\infty$. From \eqref{eqn:noise}, for each sensor $j$, it follows that
$
\cov[\overline{v}_j^\ell(k)] \leq B^2 \max_{l\in\mathcal{V}} R_l.
$
Let $C\triangleq B^2 \max_{l\in\mathcal{V}} R_l.$ It thus holds that
\begin{equation}
\begin{split}
& \frac{\varepsilon_i(k)}{\beta_i(k)}=\frac{\alpha(k)}{\Big(\mu_i+\sum_{j\in\mathcal{J}_i^+(k)}(\delta_j(k))^2)\Big)\sum_{j\in\mathcal{J}_i^+(k)}(\delta_j(k))^2}\\&\qquad \times \sum_{j\in\mathcal{J}_i^+(k)}(\delta_j(k))^2\cov[\overline{v}_j^\ell(k)]\\
\leq & \frac{C\alpha(k)}{\Big(\mu_i+\sum_{j\in\mathcal{J}_i^+(k)}(\delta_j(k))^2)\Big)\sum_{j\in\mathcal{J}_i^+(k)}(\delta_j(k))^2}\\&\qquad \times\sum_{j\in\mathcal{J}_i^+(k)}(\delta_j(k))^2\\
=&\frac{C\alpha(k)}{\mu_i+\sum_{j\in\mathcal{J}_i^+(k)}(\delta_j(k))^2}\leq \frac{C\alpha(k)}{\mu_i}.
\end{split}
\end{equation}
As $\lim_{k \to \infty} \alpha(k)=0$, we know that
\begin{equation}
\lim_{k \to \infty} \frac{\varepsilon_i(k)}{\beta_i(k)}=0.
\end{equation}
By \cite[Lemma~A.1]{li2010consensus}, it is not difficult to conclude that
\begin{equation}
\lim_{k \to \infty} \cov[\widetilde{\theta}_i^\ell(k)] =0.
\end{equation}
Combining it with \eqref{eqn:mean}, one obtains that
\begin{equation}
\lim_{k\to \infty} \mathbb{E}[(\widetilde{\theta}_i^\ell(k))^2] = 0.
\end{equation}
Since this relationship holds for any $\ell \in\{1,\cdots,d\}$, we
finally complete the proof.\hfill$\square$
\end{pf}

In view of Theorem~\ref{thm:converge}, the convergence at each sensor side is guaranteed under the Local-PE condition. That means, through communicating with only immediate neighbors, the sensors can cooperatively fulfill the estimation task, even if any individual sensor cannot. 

%Clearly, Theorem~\ref{thm:converge} is a generalization of Corollary~\ref{col:converge} from a single sensor to all sensors in the network. Another implication of Corollary~\ref{col:converge} is that, even for a sensor network that is not fully connected, if any individual sensor or certain connected component meets the Local-PE condition \eqref{eqn:PE}, it is still possible for it to estimate the true parameter.


\section{Simulation}\label{sec:simulation}
In this section, we will present a numerical example to demonstrate the theoretical results established in the previous sections.

\begin{figure}[!htbp]
	\centering
	\includegraphics[width=0.1\textwidth]{tikz/topo.pdf}
	\caption{The communication network.}
	\label{fig:network}
\end{figure}

Let us consider the network of $4$ sensors shown in Fig~\ref{fig:network}. They aim to cooperatively estimate a $2$-dimensional parameter $\theta$ over the network. The regressor $\phi_i(k)$ for each sensor is given by
\begin{equation}
\begin{split}
\phi_1(k)&=
\begin{cases}
[1 \quad 2 ]^\prime, \text{ if } t \text{ is odd},\\
[2 \quad 3 ]^\prime, \text{ if } t \text{ is even},
\end{cases}
\\\phi_2(k)&=[
a(k) \quad 1
]^\prime,\;\phi_3(k)=[
1\quad b(k)
]^\prime,\;\phi_4(k)=[1 \quad 1 ]^\prime,
\end{split}
\end{equation}
where 
\begin{equation}
a(k)=a(k-1)+\cos\left(\frac{k\pi}{4}\right), \;b(k)=b(k-1)+\cos\left(\frac{k\pi}{2}\right),
\end{equation}
with $a(0)=1$ and $b(0)=2$. It can be checked that each sensor in the network verifies the Local-PE condition. Moreover, let the stepsize be $\alpha(k) = 0.7/k$, which meets the condition \eqref{eqn:alpha}.
We set the initial estimates of each sensors as $[0\quad 0]^\prime$ and other parameters as
$
\theta = [2.5, -1]^\prime, \;\mu_i = 0.1i,\; R_i = I, \;\forall i.
$



Here,
we repeat the simulation for $1000$ times with the same initial
states and parameters. The performance of Algorithm~\ref{alg:CTA} is demonstrated in Fig.~\ref{fig:err}. We can see that each sensor consistently infers the true parameter, as expected from Theorem~\ref{thm:converge}.
%\begin{figure}
%	%\centering
%	\begin{subfigure}[b]{0.4\textwidth}
%		\input{tikz/est.tikz}
%	\end{subfigure}
%	\begin{subfigure}[b]{0.4\textwidth}
%		\input{tikz/est2.tikz}
%	\end{subfigure}
%	\caption{Average of the local estimate $\hat{\theta}_i(k)$ of each sensor in $1000$-run Monte Carlo trials.}
%	\label{fig:est}
%\end{figure}

\begin{figure}[!htbp]
	\centering
	\input{tikz/err.tikz}
	\caption{Average of the Euclidean norm of estimation error of each sensor in $1000$-run Monte Carlo trials.}
	\label{fig:err}
\end{figure}



\section{Conclusion}\label{sec:conclude}
This paper has studied the problem of distributed parameter estimation in sensor networks, where measurements of sensors might be subject to Gaussian random noises. By generalizing the DREM algorithm to stochastic systems, a distributed estimator has been proposed, which guarantees that each sensor estimates the true parameter in mean square when the Local-PE condition and certain requirements on step sizes are met. This implies that, by working cooperatively with each other, the sensors can track a dynamic process from noisy measurements, even when none of them can fulfill the estimation task individually. Notice that in the proposed algorithm, the sensors transmit both their local (scalar) regressors and measurements.


\bibliography{reference}   

                                     
\end{document}
