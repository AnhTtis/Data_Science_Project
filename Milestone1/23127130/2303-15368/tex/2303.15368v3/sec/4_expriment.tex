\section{Experiments}

\emph{Datasets.} To evaluate our method, we use three datasets: DeepFashion3D~\cite{Zhu2020}, DTU~\cite{Jensen2014} and BlendedMVS~\cite{Yao_2020_CVPR}.
The DeepFashion3D dataset consists of clothing models, which are open models with boundaries. As only 3D points are available, we render 72 images of resolution $1024\times 1024$ with a white background from different viewpoints for each model. In addition to DeepFashion3D images rendered by us most of which are texture-less, we also take the image data from NeuralUDF~\cite{Long2023} most of which are texture-rich into our experiments. We call them DF3D\#Ours and DF3D\#NeuralUDF, respectively. The DTU dataset consists of models captured in a studio, all of which are watertight. We use this dataset to validate that our method also works well for watertight models. These datasets have been widely used in previous works such as~\cite{Yariv2021,Wang2021,Wang2022HFNeuSIS}. In our experiments, open models such as in DeepFashion3D are trained without mask supervision; DTU is trained with mask supervision.


\emph{Baselines.} To validate the effectiveness of our method, we compare it with state-of-the-art UDF learning methods: 
NeuralUDF~\cite{Long2023}, NeUDF~\cite{Liu2023NeUDF} and NeAT~\cite{Meng2023}; and SDF learning methods: VolSDF~\cite{Yariv2021} and NeuS~\cite{Wang2021}. 

\subsection{Comparisons on Open Models}
\begin{figure*}[!t]
\centering
\setlength\tabcolsep{0pt}
\begin{tabular}{ccccccccc}
    & Ref. Img. & GT & Ours & VolSDF & NeuS & NeAT & NeuralUDF & NeUDF\\
    \raisebox{.22in}{\#2} & \includegraphics[width=.386in]{gt/125-1}&
    \includegraphics[width=.386in]{gt/125-1-pc}\includegraphics[width=.3in]{gt/125-1-pc-topdown}&
    \includegraphics[width=.386in]{ours/125-1}\includegraphics[width=.3in]{ours/125-1-topdown}&
    \includegraphics[width=.386in]{volsdf/125-1}\includegraphics[width=.3in]{volsdf/125-1-topdown}&
    \includegraphics[width=.386in]{neus/125-1}\includegraphics[width=.3in]{neus/125-1-topdown}&
    \includegraphics[width=.386in]{neat/125-1}\includegraphics[width=.3in]{neat/125-1-topdown}&
    \includegraphics[width=.386in]{neuraludf/125-1}\includegraphics[width=.3in]{neuraludf/125-1-topdown}&
    \includegraphics[width=.386in]{neudf/125-1}\includegraphics[width=.3in]{neudf/125-1-topdown}\\
    \raisebox{.22in}{\#3} & \includegraphics[width=.386in]{gt/276-1}&
    \includegraphics[width=.386in]{gt/276-1-pc}\includegraphics[width=.386in]{gt/276-1-pc-topdown}&
    \includegraphics[width=.386in]{ours/276-1}\includegraphics[width=.386in]{ours/276-1-topdown}&
    \includegraphics[width=.386in]{volsdf/276-1}\includegraphics[width=.386in]{volsdf/276-1-topdown}&
    \includegraphics[width=.386in]{neus/276-1}\includegraphics[width=.386in]{neus/276-1-topdown}&
    \includegraphics[width=.386in]{neat/276-1}\includegraphics[width=.386in]{neat/276-1-topdown}&
    \includegraphics[width=.386in]{neuraludf/276-1}\includegraphics[width=.386in]{neuraludf/276-1-topdown}&
    \includegraphics[width=.386in]{neudf/276-1}\includegraphics[width=.386in]{neudf/276-1-topdown}\\
    \raisebox{.22in}{\#4} & \includegraphics[width=.429in]{gt/356-3}&
    \includegraphics[width=.429in]{gt/356-3-pc}\includegraphics[width=.386in]{gt/356-3-pc-topdown}&
    \includegraphics[width=.429in]{ours/356-3}\includegraphics[width=.386in]{ours/356-3-topdown}&
    \includegraphics[width=.429in]{volsdf/356-3}\includegraphics[width=.386in]{volsdf/356-3-topdown}&
    \includegraphics[width=.429in]{neus/356-3}\includegraphics[width=.386in]{neus/356-3-topdown}&
    \includegraphics[width=.429in]{neat/356-3}\includegraphics[width=.386in]{neat/356-3-topdown}&
    \includegraphics[width=.429in]{neuraludf/356-3}\includegraphics[width=.386in]{neuraludf/356-3-topdown}&
    \includegraphics[width=.429in]{neudf/356-3}\includegraphics[width=.386in]{neudf/356-3-topdown-roi}\\
    \raisebox{.22in}{LS-D0} & \includegraphics[width=.386in]{gt/204}&
    \includegraphics[width=.386in]{gt/204-pc}\includegraphics[width=.343in]{gt/204-pc-topdown}&
    \includegraphics[width=.386in]{ours/204}\includegraphics[width=.343in]{ours/204-topdown}&
    \includegraphics[width=.386in]{volsdf/204}\includegraphics[width=.343in]{volsdf/204-topdown}&
    \includegraphics[width=.386in]{neus/204}\includegraphics[width=.343in]{neus/204-topdown}&
    \includegraphics[width=.386in]{neat/204}\includegraphics[width=.343in]{neat/204-topdown}&
    \includegraphics[width=.386in]{neuraludf/204}\includegraphics[width=.343in]{neuraludf/204-topdown}&
    \raisebox{.22in}{N.A.} \\
\end{tabular}
\caption{Visual comparisons on selected models of the DeepFashion3D~\cite{Zhu2020} dataset. The surfaces produced by NeuS and VolSDF are closed watertight models, thereby post-processing is required to remove the unnecessary parts. NeAT can produce open models by learning an SDF and predicting which surfaces in the extracted meshes should be removed, but it needs mask for supervision. NeuralUDF can generate open surfaces, but struggles with textureless inputs, leading to double-layered regions and large reconstruction errors. NeUDF generally performs well, but its training is unstable and may stumble on less distinguished, darker models like LS-D0. In contrast, our \methodname{} consistently delivers effective reconstructions of non-watertight models. See the supplementary material for additional results.}

\label{fig:df3d}
\end{figure*}


\begin{table}[!htbp]
\centering
\setlength\tabcolsep{1.25pt}
\begin{footnotesize}
\begin{tabular}{c|ccccccccc|c}
    \hline
    Method & \#1 & \#2 & \#3 & \#4 & \#5 & \#6 & \#7 & \#8 & \#9 & Mean \\
    \hline
    NeuS & 6.69 & 13.50 & 10.32 & 15.01 & 8.99 & 12.92 & 12.94 & 9.93 & 9.49 & 11.09 \\
    VolSDF & 6.36 & 9.44 & 11.87 & 16.03 & 10.78 & 14.91 & 15.06 & 11.34 & 8.96 & 11.64 \\
    NeAT & 10.54 & 13.89 & 7.30 & 13.12 & 13.18 & 12.44 & 8.22 & 10.30 & 11.33 & 11.15 \\
    NeuralUDF & 6.07 & 11.58 & 7.68 & \underline{10.96} & 11.16 & 9.76 & 6.98 & 6.13 & 6.41 & 8.53 \\
    NeUDF & \textbf{4.39} & \underline{8.29} & \underline{4.94} & 19.56 & \underline{7.52} & \underline{8.18} & \underline{3.81} & \underline{3.81} & \underline{5.76} & \underline{7.36} \\
    Ours & \underline{4.55} & \textbf{5.77} & \textbf{4.27} & \textbf{7.43} & \textbf{6.59} & \textbf{4.77} & \textbf{2.88} & \textbf{3.21} & \textbf{5.73} & \textbf{5.02} \\
    \hline
\end{tabular}
\end{footnotesize}

\begin{footnotesize}
\begin{tabular}{c|ccccccc|c}
\hline
    Method & LS-C0 & SS-D0 & LS-D0 & NS-D1 & LS-C1 & Skirt1 & SS-C0 & Mean \\
    \hline
    NeuS & 3.18 & 4.82 & 5.71 & 2.21 & 3.60 & 2.44 & 5.13 & 3.87 \\
    VolSDF & 5.92 & 4.79 & 5.96 & 4.36 & 8.73 & 7.74 & 8.84 & 6.62 \\
    NeAT & 3.06 & 4.33 & 5.92 & 3.52 & 8.84 & 3.91 & 4.30 & 4.84 \\
    NeuralUDF & \textbf{1.92} & \underline{2.05} & \underline{4.11} & 1.50 & \underline{2.47} & \underline{2.16} & 2.15 & 2.34 \\
    NeUDF & \underline{1.95} & 2.93 & N.A. & \underline{1.48} & 2.66 & 2.74 & \textbf{1.77} & \underline{2.26} \\
    Ours & \textbf{1.92} & \textbf{1.97} & \textbf{2.46} & \textbf{1.47} & \textbf{2.14} & \textbf{1.84} & \underline{1.91} & \textbf{1.96} \\
    \hline
\end{tabular}
\end{footnotesize}

\caption{Chamfer distances ($\times 10^{-3}$) on DF3D\#Ours (top) and DF3D\#NeuralUDF (bottom). NeAT requires mask supervision and others do not need.}
\label{tab:df3d}
\end{table}

We evaluate our method and compare it with baselines using the garments from DeepFashion3D~\cite{Zhu2020}, where the models have multiple open boundaries.  VolSDF and NeuS always close the boundaries since they learn SDFs. 

NeuralUDF, NeUDF and NeAT are designed to learn non-watertight models. NeAT learns SDFs for open models, and requires mask supervision to produce reasonable results, but other methods do not require mask supervision for DeepFashion3D. The released codebase of NeuralUDF indicates that it also has a two-stage training process. We evaluate the results of NeuralUDF at the end of both stages, and present whichever is better.

In contrast, NeuralUDF, NeUDF and our method learn UDFs, which can generate open models. Table~\ref{tab:df3d} shows the Chamfer distances of the results on DeepFashion3D. Some of the Chamfer distances of the compared methods are large because the open holes are closed or the model is over-smoothed, resulting in significant errors.

\begin{figure}[ht]
\centering
\setlength\tabcolsep{1pt}
\begin{tabular}{ccccc}
    & GT & Ours & NeuralUDF & NeUDF \\
    \raisebox{.22in}{\#2} & \includegraphics[width=.7in]{gt/125-1-cut} &
    \includegraphics[width=.7in]{ours/125-1-cut} &
    \includegraphics[width=.7in]{neuraludf/125-1-cut} &
    \includegraphics[width=.7in]{neudf/125-1-cut} \\
    \raisebox{.22in}{\#3} & \includegraphics[width=.7in]{gt/276-1-cut} &
    \includegraphics[width=.7in]{ours/276-1-cut} &
    \includegraphics[width=.7in]{neuraludf/276-1-cut} &
    \includegraphics[width=.7in]{neudf/276-1-cut} \\
    \raisebox{.22in}{\#4} & \includegraphics[width=.7in]{gt/356-3-cut} &
    \includegraphics[width=.7in]{ours/356-3-cut} &
    \includegraphics[width=.7in]{neuraludf/356-3-cut} &
    \includegraphics[width=.7in]{neudf/356-3-cut} \\
    \raisebox{.22in}{LS-D0} & \includegraphics[width=.7in]{gt/204-cut} &
    \includegraphics[width=.7in]{ours/204-cut} &
    \includegraphics[width=.7in]{neuraludf/204-cut} &
    \raisebox{.22in}{N.A.} \\
\end{tabular}
\caption{Visualization of the learned UDFs on cross sections. Compared with the ground truth, our method can learn a UDFs that most closely resemble the ground truth, among our method, NeuralUDF, and NeUDF. NeAT is omitted in this visualization, because it learns SDFs in lieu of UDFs. Note that for LS-D0, NeUDF completely collapses without a reasonable UDF learned.}
\label{fig:vs_udf}
\end{figure}

As demonstrated in Figure~\ref{fig:df3d}, we test various types of garments, some of which have rich textures, while others are nearly a single color. Learning UDFs for textureless models is more challenging since various regions of a model are ambiguous without clear color differences. However, our \methodname{} generates satisfactory results even without masks. Though with mask supervision, the results of NeAT~\cite{Meng2023} are over-smoothed, missing details, resulting in large Chamfer distance errors. NeuralUDF~\cite{Long2023} is unable to properly reconstruct textureless models on most models, possibly due to their complex density function which is difficult to converge. Some of the NeUDF~\cite{Liu2023NeUDF} models become watertight.
To analyze the reasons, we illustrate these UDFs cross sections in Figure~\ref{fig:vs_udf}. To compute the ground truth UDFs, we sample 30,000 points from every input point model and compute the distances to the nearest sample point for every point in a 3D grid of resolution $512\times 512\times 512$. All other UDFs are extracted by querying the distance neural network in a 3D grid of the same resolution. Our learned UDFs resemble the ground truth with little difference. While, the UDFs of NeuralUDF deviate from the ground truth significantly explaining its difficulty to converge. The UDFs of NeUDF are better, but the distances approach to zero around open holes. As a result, it is challenging and tricky to generate non-watertight models and some of them are even closed. NeAT learns SDF, so we do not show their distance fields.

As illustrated in Figure~\ref{fig:vs_cd}, perhaps due to the absolute of an MLP for UDF representation, NeuralUDF possibly generates two layers of zero level-sets on both sides of the surface resulting in double-layered regions after Stage 1 learning. However, in its Stage 2 refinement, the surface is crushed into pieces and the Chamfer distance errors surge suddenly.

\begin{figure}[!ht]
    \centering
    \begin{scriptsize}
    \setlength\tabcolsep{2pt}
    \begin{tabular}{cccc}
        \multirow{6}{*}{\includegraphics[width=1.35in]{compare/CD}} &
        \includegraphics[width=.6in]{compare/8-7_neuraludf_p1} &
        \includegraphics[width=.6in]{compare/8-7_neuraludf_p1_closeup_edit} &
        \includegraphics[width=.6in]{compare/8-7_neuraludf_p2} \\
        & \multicolumn{2}{c}{Stage 1} & Stage 2\\
        & \multicolumn{3}{c}{NeuralUDF}\\
        & 
        \includegraphics[width=.6in]{compare/8-7_2sudf_p1} &
        \includegraphics[width=.6in]{compare/8-7_2sudf_p1_closeup_edit} &
        \includegraphics[width=.6in]{compare/8-7_2sudf_p2} \\
        & \multicolumn{2}{c}{Stage 1} & Stage 2\\
        & \multicolumn{3}{c}{\methodname{}} \\
    \end{tabular}
    \end{scriptsize}
    \caption{Plots of the Chamfer distance throughout the training process. Our method consistently reduces CD across both stages. In contrast, NeuralUDF, which also adopts a two-stage learning strategy, exhibits instability and yields a fragmented output following the second stage. The first-stage output of NeuralUDF, however, contains double-layered regions as marked above. In this figure, both methods start their stage 2 training at 250k iterations.}
    \label{fig:vs_cd}
\end{figure}

In Figure~\ref{fig:neudf-data}, we conduct additional experiments on some open model dataset provided by NeUDF~\cite{Liu2023NeUDF}. For the rack model, the thin structures reconstructed by NeuralUDF~\cite{Long2023} and NeUDF~\cite{Liu2023NeUDF} seem eroded, but ours don't. The thin structures reconstructed by NeAT~\cite{Meng2023} is the closest to the reference image, but the surface is dented inward with visible artifacts due to imperfect SDF validity learning. The plant model does not have an object mask, making NeAT~\cite{Meng2023} impractical for training. NeuralUDF~\cite{Long2023} completely fails to reconstruct a reasonable surface. Between our method and NeUDF~\cite{Liu2023NeUDF} which can reconstruct a sensible model, the flower pot region marked in red is missing in NeUDF but not in ours. These show our method's ability to reconstruct non-watertight models more robustly compared to other methods.

\begin{figure}[htbp]
\centering
\setlength\tabcolsep{1pt}
\begin{tabular}{cccccc}
    & Ref. & Ours & NeUDF & NeuralUDF & NeAT \\
    \raisebox{.12in}{\rotatebox{90}{rack}} & \includegraphics[width=.58in]{gt/rack_roi} &
    \includegraphics[width=.58in]{ours/rack_roi} &
    \includegraphics[width=.58in]{neudf/rack_roi} &
    \includegraphics[width=.58in]{neuraludf/rack_roi} &
    \includegraphics[width=.58in]{neat/rack_roi} \\
    \raisebox{.14in}{\rotatebox{90}{plant}} & \includegraphics[width=.58in]{gt/plant_roi} &
    \includegraphics[width=.58in]{ours/plant_roi} &
    \includegraphics[width=.58in]{neudf/plant_roi} &
    \includegraphics[width=.58in]{neuraludf/plant} &
    \raisebox{.22in}{N.A.} \\
\end{tabular}
\caption{Qualitative comparisons with NeAT~\cite{Meng2023}, NeuralUDF~\cite{Long2023} and NeUDF~\cite{Liu2023NeUDF} on some example data released by NeUDF~\cite{Liu2023NeUDF}. Note that NeAT cannot reconstruct ``plant'' dataset because the ground truth mask for ``plant'' is unavailable.}
\label{fig:neudf-data}
\end{figure}


\subsection{Comparisons on Watertight Models}
\label{sec:watertight}

\begin{table}
\centering
\setlength\tabcolsep{1.25pt}
\begin{footnotesize}
\begin{tabular}{c|cccccccccc|c}
    \hline
    Method & 37 & 55 & 65 & 69 & 97 & 105 & 106 & 114 & 118 & 122 & Mean \\
    \hline
    NeuralUDF & 1.18 & \textbf{0.44} & \textbf{0.66} & \textbf{0.67} & \textbf{0.94} & 0.95 & \textbf{0.57} & \textbf{0.37} & \textbf{0.56} & \underline{0.55} & \textbf{0.69} \\
    NeAT & 1.18 & \underline{0.47} & 0.82 & \underline{0.84} & 1.09 & 0.75 & 0.76 & \underline{0.38} & \textbf{0.56} & \underline{0.55} & 0.74 \\
    \hline
    NeUDF & \underline{0.90} & 0.65 & 0.73 & 0.97 & \underline{1.07} & \textbf{0.63} & 0.94 & 0.59 & 0.72 & 0.62 & 0.78 \\
    Ours & \textbf{0.89} & 0.55 & \underline{0.68} & 0.88 & 1.15 & \underline{0.70} & \underline{0.74} & 0.41 & \underline{0.61} & \textbf{0.51} & \underline{0.71} \\
    \hline
\end{tabular}
\end{footnotesize}
\caption{Chamfer distances on DTU dataset.}
\label{tab:dtu}
\end{table}

\begin{figure}
\centering
\setlength\tabcolsep{1pt}
\begin{tabular}{ccccc}
    & 55 & 65 & 118 & 122 \\
    \raisebox{.18in}{\rotatebox{90}{Ref.}} & \includegraphics[width=.75in]{gt/dtu_scan55_roi} & \includegraphics[width=.75in]{gt/dtu_scan65_roi} & \includegraphics[width=.75in]{gt/dtu_scan118_roi} & \includegraphics[width=.75in]{gt/dtu_scan122_roi} \\
    \raisebox{.18in}{\rotatebox{90}{Ours}} & \includegraphics[width=.375in]{ours/dtu_scan55_roi}\includegraphics[width=.375in]{ours/dtu_scan55_closeup} & \includegraphics[width=.375in]{ours/dtu_scan65_roi}\includegraphics[width=.375in]{ours/dtu_scan65_closeup} & \includegraphics[width=.342in]{ours/dtu_scan118_roi}\includegraphics[width=.342in]{ours/dtu_scan118_closeup} & \includegraphics[width=.292in]{ours/dtu_scan122_roi}\includegraphics[width=.292in]{ours/dtu_scan122_closeup} \\
    \raisebox{.1in}{\rotatebox{90}{NeUDF}} & \includegraphics[width=.375in]{neudf/dtu_scan55_roi}\includegraphics[width=.375in]{neudf/dtu_scan55_closeup} & \includegraphics[width=.375in]{neudf/dtu_scan65_roi}\includegraphics[width=.375in]{neudf/dtu_scan65_closeup} & \includegraphics[width=.342in]{neudf/dtu_scan118_roi}\includegraphics[width=.342in]{neudf/dtu_scan118_closeup} & \includegraphics[width=.292in]{neudf/dtu_scan122_roi}\includegraphics[width=.292in]{neudf/dtu_scan122_closeup} \\
    \scalebox{0.85}{\rotatebox{90}{NeuralUDF}} & \includegraphics[width=.375in]{neuraludf/dtu_scan55_roi}\includegraphics[width=.375in]{neuraludf/dtu_scan55_closeup} & \includegraphics[width=.375in]{neuraludf/dtu_scan65_roi}\includegraphics[width=.375in]{neuraludf/dtu_scan65_closeup} & \includegraphics[width=.342in]{neuraludf/dtu_scan118_roi}\includegraphics[width=.342in]{neuraludf/dtu_scan118_closeup} & \includegraphics[width=.292in]{neuraludf/dtu_scan122_roi}\includegraphics[width=.292in]{neuraludf/dtu_scan122_closeup} \\
    \raisebox{.18in}{\rotatebox{90}{NeAT}} & \includegraphics[width=.375in]{neat/dtu_scan55_roi}\includegraphics[width=.375in]{neat/dtu_scan55_closeup} & \includegraphics[width=.375in]{neat/dtu_scan65_roi}\includegraphics[width=.375in]{neat/dtu_scan65_closeup} & \includegraphics[width=.342in]{neat/dtu_scan118_roi}\includegraphics[width=.342in]{neat/dtu_scan118_closeup} & \includegraphics[width=.292in]{neat/dtu_scan122_roi}\includegraphics[width=.292in]{neat/dtu_scan122_closeup} \\
\end{tabular}
\caption{\label{fig:dtu}Qualitative comparisons with NeAT, NeuralUDF and NeUDF on the DTU~\cite{Jensen2014} dataset and close-up comparisons against NeUDF. Our method can reconstruct surfaces closer to the ground truth point clouds in various places such as the marked region, generally improving the reconstruction accuracy of NeUDF by around 10\%, on a par with NeuralUDF and NeAT at the bottom two rows.}
\end{figure}

Other methods can also be used as the first stage of our \methodname{}. We use NeUDF for the first stage training on the DTU dataset~\cite{Jensen2014}. As detailed in Table~\ref{tab:dtu}, we compare the Chamfer distances of the reconstruction results with NeuralUDF, NeAT and NeUDF without our second-stage training. SDFs generally excel at learning watertight models, and it is worth pointing out that NeuralUDF takes the absolute value of the output of MLP as the UDF value of a given point. Therefore for closed models, they can easily learn an SDF and take its absolute value to produce a UDF. NeAT, on the other hand, explicitly learns an SDF. NeUDF and our method truly learn UDFs. While UDF learning is much more complicated than SDF learning because the UDF gradient nearby 0 is blurry and the gradient is not available at 0, our method still improves the reconstruction quality of NeUDF by around 10\% as shown in Figure~\ref{fig:dtu}. We further provide a close-up view of specific parts of the models for detailed comparisons in Figure~\ref{fig:dtu}. These local callouts exhibit the ground truth points located on both sides of our surfaces, whereas most of the points are only on one side of the surfaces of NeUDF. These illustrate our reconstructed surfaces are closer to the ground truth points and thus improving the resulting quality over NeUDF, on a par with NeuralUDF and NeAT. 


\subsection{Ablation Studies}
\label{sec:ablation}

In this section, we present main ablation studies. We refer interested readers to the supplementary material for additional ablation studies.

\begin{table}[ht]
    \centering
    \begin{tabular}{c|cccccc}
    \hline
    Method & \#1 & \#7 & \#8 & LS-D0 \\
    \hline
    S1 \& S2 & \textbf{4.55} & \underline{2.88} & \textbf{3.21} & \textbf{2.46} \\
    S1 & 7.22 & \textbf{2.46} & \underline{3.38} & 6.04 &  \\
    S2 & \underline{5.75} & 4.00 & 5.96 & \underline{3.65}  \\
    \hline
    Method & NS-D1 & LS-C1 & DTU 114 & DTU 122 \\
    \hline
    S1 \& S2 & \underline{1.47} & \textbf{2.14} & \textbf{0.41} & \textbf{0.51} \\
    S1 & \textbf{1.46} & 6.23 & \underline{0.59} & 0.62 \\
    S2 & 1.64 & \underline{2.98} & 0.63 & \underline{0.60} \\
    \hline
    \end{tabular}
    \caption{Chamfer distances of models learned by both Stage 1 and 2 (S1 \& S2), only Stage 1 (S1) and only Stage 2 (S2) on selected datasets. Models learned by two stages yield similar Chamfer distances, but when trained with only Stage 1 or Stage 2, the Chamfer distances generally become significantly higher.}
    \label{tab:ablation}
\end{table}

\emph{Effect of the two-stage training.} We conduct an ablation study on the effect of the two-stage learning. We compare the Chamfer distances among both two stages, only Stage 1 and only Stage 2 training, shown in Table~\ref{tab:ablation}. Our results show that two-stage training improves the Chamfer distance (lower is better) compared to training with only Stage 1 or 2, under most circumstances.

It should be noted that training by the second stage from scratch is also capable of generating a generally reasonable result. However, the Chamfer distances, as shown in Table~\ref{tab:ablation}, indicate that its learning ability is limited. Therefore, the second refinement learning stage should cooperate with the first coarse learning stage to generate the best results.

\emph{Choice of accumulated weight threshold $\delta_{thres}$.} In Stage 2, being a ray truncate point requires the accumulated weight up until that point to be greater than $\delta_{thres}$, where we intuitively select $\delta_{thres}=0.5$. Figure~\ref{fig:vs_delta_thres} shows the reconstruction results for other choices of $\delta_{thres}$, namely 0.3 and 0.7, respectively. We observe that all threshold choices successfully reconstruct the model. Setting the threshold $\delta_{thres}$ up to 0.7 produces visually similar results. Setting the threshold $\delta_{thres}$ down to 0.3 also works fine generally despite that it may introduce more holes to the reconstructed meshes. We deduce that setting a lower threshold increases the possibility that a ray may be truncated prematurely, leading to less desirable results. Nevertheless, we still have a considerable range of $\delta_{thres}$ from 0.3 to 0.7 without major result regression, indicating that our Stage 2 training exhibits robustness against $\delta_{thres}$.

\begin{figure}
    \centering
    \begin{tabular}{ccc}
        \includegraphics[width=.6in]{compare/432-1_ablation_thres_0.3_roi} & \includegraphics[width=.6in]{ours/432-1} & \includegraphics[width=.6in]{compare/432-1_ablation_thres_0.7} \\
        \includegraphics[width=.6in]{compare/448-2_ablation_thres_0.3_roi} & \includegraphics[width=.6in]{ours/448-2} & \includegraphics[width=.6in]{compare/448-2_ablation_thres_0.7} \\
        $\delta_{thres}=0.3$ & $\delta_{thres}=0.5$ & $\delta_{thres}=0.7$ \\
    \end{tabular}
    \caption{Qualitative comparisons on different choices of accumulated weight $\delta_{thres}$. Setting a higher threshold works well few little visual differences; Setting a lower threshold generally works fine, but may introduce more holes in reconstructed meshes.}
    \label{fig:vs_delta_thres}
\end{figure}

\subsection{Limitations}
Since the light is cut off after going through a layer of surface, our method relinquishes the ability to model planes with transparency.
Occasionally, due to learning uncertainty, the Chamfer distance may increase slightly in the second stage, but the difference is quite small without visual impact. Overall, the two-stage learning improves the quality significantly.
For watertight models, SDF learning is more suitable than UDF learning, since UDF learning is more complicated than SDF learning.
We still advise using SDF learning, e.g., NeuS~\cite{Wang2021}, HF-NeuS~\cite{Wang2022HFNeuSIS} or PET-NeuS~\cite{Wang_2023_CVPR_PETNeuS}, for watertight model reconstruction.
Also, the mesh extraction of MeshUDF~\cite{Guillard2022} tends to generate holes and ``staircase'' artifacts affecting the mesh reconstruction quality. Adopting a more robust extraction method, e.g., DoubleCoverUDF~\cite{Hou2023}, could alleviate the problem, but we use MeshUDF here for all methods for a fair comparison.
