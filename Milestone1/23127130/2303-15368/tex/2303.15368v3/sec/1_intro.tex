\section{Introduction}
\begin{figure}[t]
    \centering
    \begin{tabular}{cccc}
        GT & Ours & NeuralUDF & NeUDF \\
        \includegraphics[width=.6in]{gt/432-1-teaser} & \includegraphics[width=.6in]{ours/432-1-teaser} & \includegraphics[width=.6in]{neuraludf/432-1-teaser} & \includegraphics[width=.6in]{neudf/432-1-teaser} \\
        \includegraphics[width=.6in]{gt/432-1-cut} & \includegraphics[width=.6in]{ours/432-1-cut} & \includegraphics[width=.6in]{neuraludf/432-1-cut} & \includegraphics[width=.6in]{neudf/432-1-cut} \\
    \end{tabular}
    \caption{We learn a UDF from multiview images for non-watertight model reconstruction. As illustrated in the cross sections of learned UDFs, our learned UDF approximates to the ground truth. In contrast, the learned UDF of NeuralUDF~\cite{Long2023} is choppy leading to significant artifacts, e.g., unexpected pit. The learned UDF of NeUDF~\cite{Liu2023NeUDF} is almost closed struggling to generate open surface.}
    \label{fig:teaser}
\end{figure}


As the success of neural radiance field (NeRF)~\cite{Mildenhall2020}, numerous volume rendering based 3D modeling methods are proposed to learn signed distance fields (SDF) for 3D model reconstruction from multi-view images~\cite{Yariv2021,Wang2021,Wang2022HFNeuSIS,Darmon2022}. 
These approaches map signed distance value to a density function, thereby enabling the use of volume rendering to learn an implicit SDF representation. To calculate pixel colors, they compute the weighted sum of radiances along each light ray. 
Achieving an accurate surface depiction requires the density function to meet three essential criteria. Firstly, the weights, which are derived from the density function, must reach their maximum value when the distance is zero, ensuring unbiasedness. Secondly, as a ray traverses through the surface, the accumulated density should tend towards infinity, rendering the surface opaque â€” a property referred to as occlusion-awareness. Finally, the density function should be bounded to prevent numerical issues.
The popular SDF approaches, such as NeuS~\cite{Wang2021} and VolSDF~\cite{Yariv2021}, adopt an S-shaped density function that meets all these requirements.
 

While SDF-based methods excel at reconstructing watertight models, they have limitations in representing open models. This is due to the intrinsic nature of SDF, which differentiates between the interior and exterior of a model, thus failing to accommodate open boundaries. Recent advances have attempted to mitigate this constraint by employing unsigned distance fields (UDF)~\cite{Long2023,Liu2023NeUDF,Meng2023}. 
Unlike signed distance fields, UDFs have non-negative distance values, making them suitable for representing non-watertight models. 
However, learning a UDF from multi-view images is a challenging task since the gradients of the UDF are unstable due to directional changes near the zero level-set, making it difficult to train the neural network. Another major challenge lies in formulating a UDF-induced density function that can simultaneously meet the above-mentioned three requirements. Unlike SDFs, UDFs cannot distinguish between the front and back of a surface based on distance values, thus, directly using an S-shaped density function is off the table. Opting for a bell-shaped density function brings its own issues. It is impossible for these integrations to approach infinity, so as to be occlusion-aware, unless the density becomes boundless at zero distance values. These conflicting requirements make UDF learning a non-trivial task, forcing existing methods to sacrifice at least one of these conditions. As shown in Figure~\ref{fig:teaser}, the existing methods NeuralUDF~\cite{Long2023} and NeUDF~\cite{Liu2023NeUDF} result in either choppy or nearly closed UDFs.

As designing a UDF-induced density function that simultaneously fulfills the three aforementioned conditions remains an unresolved challenge, we propose a novel approach that learns a UDF from multi-view images in two separate stages. In the first stage, we apply an easily trainable but slightly biased and transparent density function for coarse reconstruction.
Such a UDF, although being approximate, provides an important clue so that we can determine where to truncate the light rays. 
This accounts for the occlusion effect, where points behind the surface are not visible and should not contribute to the output color. With \textit{truncated} light rays, we are able to derive the weights from UDF directly bypassing the density function, to further refine the geometry and appearance in the second stage.
Our two-stage learning method, called \methodname{}, leads to an unbiased and occlusion-aware weight function. Furthermore, by sidestepping density function learning in Stage 2, we effectively bypass the challenges associated with ensuring its boundedness. This strategy enhances the numerical stability of our method.
Evaluations on benchmark datasets DeepFashion3D~\cite{Zhu2020} and DTU~\cite{Jensen2014} show that \methodname{} outperforms existing UDF learning methods in terms of both reconstruction accuracy and visual quality. Additionally, we observe that the training stability of \methodname{} is notably superior compared to other UDF learning neural networks.
