\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{iccv}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{arydshln}
\usepackage{multirow}

\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}

\iccvfinalcopy

\def\iccvPaperID{10499}
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

\ificcvfinal\pagestyle{empty}\fi

\graphicspath{{media/}}
\newtheorem{theorem}{Theorem}

% Define the name of our method here.
\newcommand{\methodname}{NeUDF}

\begin{document}

%%%%%%%%% TITLE
\title{\methodname: Learning Unsigned Distance Fields from Multi-view Images for Reconstructing Non-watertight Models}

\author{Fei Hou\thanks{Equal contributions.}\\
Institute of Software, CAS\\
{\tt\small houfei@ios.ac.cn}
\and
Junkai Deng\footnotemark[1]\\
Institute of Software, CAS \\
{\tt\small dengjk@ios.ac.cn}
\and
Xuhui Chen \\
Institute of Software, CAS \\
{\tt\small chenxh@ios.ac.cn}
\and Wencheng Wang\\
Institute of Software, CAS\\
{\tt\small whn@ios.ac.cn}
\and
Ying He\\
Nanyang Technological University\\
{\tt\small yhe@ntu.edu.sg}
}

\maketitle

\ificcvfinal\thispagestyle{empty}\fi

%%%%%%%%% ABSTRACT
\begin{abstract}
Volume rendering-based 3D reconstruction from multi-view images has gained popularity in recent years, largely due to the success of neural radiance fields (NeRF). A number of methods have been developed that build upon NeRF and use neural volume rendering to learn signed distance fields (SDFs) for reconstructing 3D models. However, SDF-based methods cannot represent non-watertight models and, therefore, cannot capture open boundaries.  This paper proposes a new algorithm for learning an accurate unsigned distance field (UDF) from multi-view images, which is specifically designed for reconstructing non-watertight, textureless models. The proposed method, called \methodname, addresses the limitations of existing UDF-based methods by introducing a simple and approximately unbiased and occlusion-aware density function. In addition, a smooth and differentiable UDF representation is presented to make the learning process easier and more efficient. Experiments on both texture-rich and textureless models demonstrate the robustness and effectiveness of the proposed approach, making it a promising solution for reconstructing challenging 3D models from multi-view images.
\end{abstract}

%%%%%%%%% BODY TEXT
\section{Introduction}
\begin{figure}
    \centering
    \begin{tabular}{ccc}
        \includegraphics[width=.8in]{media/gt_color/432-1} & \includegraphics[width=.8in]{media/ours/432-1} & \includegraphics[width=.8in]{media/neus/432-1} \\
        Reference Image & Ours & NeuS~\cite{Wang2021} \\
    \end{tabular}
    \caption{We show a reconstruction result using our method and NeuS~\cite{Wang2021} on a mostly blue short trouser with little texture. Our method successfully reconstructs the structure of the trouser, while NeuS fails to learn the correct structure of the object, and instead represents it as a closed model.}
    \label{fig:teaser}
\end{figure}

In recent years, volume rendering-based 3D model and scene reconstruction from multi-view images has gained popularity, largely due to the success of neural radiance fields (NeRF)~\cite{Mildenhall2020}.
A number of methods have been developed that build upon NeRF and use neural volume rendering to learn signed distance fields (SDFs) for reconstructing 3D models~\cite{Yariv2021,Wang2021,Wang2022HFNeuSIS,Darmon2022}.
While these SDF-based methods are effective in reconstructing watertight models, they cannot represent non-watertight models, as SDFs distinguish between the interior and exterior of a model, and therefore cannot capture open boundaries.

Recent research has attempted to address the limitation of signed distance fields in reconstructing non-watertight models by using unsigned distance fields (UDFs). For instance, NeuralUDF~\cite{Long2022} extends NeuS~\cite{Wang2021} to learn UDFs for reconstructing open models.
However, UDF-based methods have their own limitations. For example, as shown in Figure~\ref{fig:teaser}, NeuralUDF struggles to reconstruct \textit{textureless} models or models with few distinguishable features, as it relies on texture information to improve the accuracy of reconstruction. 
Also, UDF is not differentiable at the zero level-set, making it difficult to learn.
Reconstructing open, textureless models from multi-view images using volume rendering remains a challenging problem. 

We propose a new algorithm for learning an accurate UDF from multi-view images, which we call \methodname. Our method is specifically designed for reconstructing non-watertight, textureless models, which are particularly challenging to reconstruct using existing UDF-based methods. The key to learning an accurate UDF is to design an unbiased and occlusion-aware density function of the UDF.
Since the inside and outside of the model are not distinguished, the S-shape density function used in SDF-based methods~\cite{Wang2021} is not appropriate for UDFs.
Long et al.~\cite{Long2022} proposed a piecewise density function adapted from NeuS~\cite{Wang2021}, but this function is rather complicated and difficult to learn, limiting its applicability to texture-rich models.

To overcome these limitations, we propose a new UDF density function for learning UDFs, which can tolerate a small amount of bias to ease learning. Our density function is dense enough to be almost opaque, making it occlusion-aware and effective for reconstructing non-watertight models. 
Additionally, we present a smoothed UDF representation that makes the UDF differentiable at the zero level set, which is crucial for the learning process. We evaluate our proposed approach on both texture-rich and textureless models from the DeepFashion3D~\cite{Zhu2020} and DTU~\cite{Jensen2014} datasets. Our experiments demonstrate that our method is robust and can effectively learn UDFs for both types of inputs.

Our work makes the following contributions:
\begin{enumerate}
\item We propose a method for reconstructing non-watertight models based on NeRF that is capable of reconstructing both texture-rich and challenging textureless models.

\item We introduce a theoretically sound density function for UDFs and in practice adopt a variant that allows a small bias and is approximately occlusion-aware. 

\item We present a smooth and differentiable UDF representation that makes the learning process easier and more efficient.

\item We present a simple yet effective method for extracting the target surface from the learned UDFs, which can reduce the bias.
\end{enumerate}

\section{Related Work}
\subsection{3D Reconstruction from Multi-View Images}
Surface reconstruction from multi-view images has been a subject of study for several decades, and can generally be classified into two categories: voxel-based and point-based methods. Voxel-based methods~\cite{Bonet1999,Broadhurst2001,Ji2021,Kar2021,Sun2021} divide the 3D space into voxels and determine which ones belong to the object. These methods can be computationally expensive and may not be suitable for reconstructing complex surfaces.
Point-based methods~\cite{Galliani2015,Schonberger2016,Yao2019} use structure-from-motion~\cite{Hartley2004} to calibrate the images and generate a dense point cloud using multi-view stereo~\cite{Furukawa2015}. Finally, surface reconstruction methods (e.g.,  ~\cite{Bernardini1999,Kazhdan2013,Hou2022}) are used to generate a mesh. Since multi-view stereo requires dense correspondences to generate a dense point cloud, which are often difficult to compute, its results often contain various types of artifacts, such as noise, holes and incomplete structures.

Neural network-based 3D surface reconstruction has received attention in recent years with the emergence of neural rendering~\cite{Mildenhall2020}. Several methods have been proposed for volume rendering and surface reconstruction using neural networks. VolSDF~\cite{Yariv2021} uses the cumulative distribution function of Laplacian distribution to evaluate the density function from SDF for volume rendering and surface reconstruction.
NeuS~\cite{Wang2021} adopts an unbiased density function for SDFs for more accurate reconstruction.
SparseNeuS~\cite{Long2022SparseNeuS} extends NeuS to use fewer images for reconstruction.
HF-NeuS~\cite{Wang2022HFNeuSIS} improves NeuS by proposing a simplified and unbiased density function and using hierarchical MLPs for detail reconstruction.
Geo-NeuS~\cite{Fu2022} incorporates structure-from-motion to add more constraints.
NeuralWarp~\cite{Darmon2022} improves the accuracy by optimizing consistency between warped views of different images.
All of these methods learn SDFs, which can only reconstruct watertighjt models.

More recently, Long \etal proposed NeuralUDF~\cite{Long2022} for learning UDF for reconstructing open models.
It adapts the density function of NeuS to UDFs by introducing an indicator function. However, this method can only learn texture-rich models due to the complex density function used in training. In contrast, our proposed UDF learning method is capable of reconstructing both texture-rich and textureless models without the need for masks.


\subsection{3D Reconstruction from Point Clouds}
There has been recent interest in surface representation using signed distance fields (SDFs) and occupation fields. Several methods have been proposed for learning SDFs~\cite{Park2019,Chabra2020,Sitzmann2020,Ma2021,Wang2022}, while occupation fields have been used in methods such as ~\cite{Mescheder2019,Chibane2020a}.
However, both SDFs and occupation fields can only represent watertight models.

To represent non-watertight models, some methods are proposed to learn UDF from 3D point clouds~\cite{Chibane2020b,Zhao2021,Zhou2022}. Our proposed method also uses UDF for non-watertight models representation, but we learn it directly from multi-view images, which is a challenging problem.


\section{Method}
Our goal is to extract a set of surface points from a set of input images along with their corresponding known camera poses and intrinsic parameters. To achieve this, we first learn an implicit unsigned distance field, whose zero level set represents the 3D surface. Unlike signed distance fields, UDFs have positive distance values, making them suitable for representing non-watertight models. However, learning a UDF from multi-view images is a challenging task that requires a proper density function balancing occlusion-awareness, ease of learning, and unbiasedness.
Another technical issue is that UDFs are not differentiable at the zero level-set, making it difficult to train.  Our proposed method is designed to address the above challenges of learning UDFs from multi-view images.

In this section, we first review the key concepts of volume rendering and neural radiance fields. Next, we propose an unbiased density function and use its variant to balance the density properties for UDF learning.
We then introduce a differentiable form of UDF that can be used for stable learning. Finally, we detail our loss function design.

\subsection{Review of Volume Rendering}

Volume rendering is a crucial component of  neural radiance fields~\cite{Mildenhall2020}. During volume rendering, a ray $\mathbf{r}$ is cast from the camera position $\mathbf{o}$ to each pixel of the virtual canvas. The direction of the ray is denoted by $\mathbf{d}$, and any point $\mathbf{r}(t)$ along the ray can be expressed as $\mathbf{o} + t\mathbf{d}$, where $t$ is the arc-length parameter. In the following, we refer to the point $\mathbf{r}(t)$ using the parameter $t$ if there is no confusion.

The color of the corresponding pixel is determined by the color of the ray, which is the weighted integral of all colors $c(t)$ along the ray $\bf r$,
$
     c(\mathbf{r}) = \int_0^\infty w(t)c(t)\,\mathrm{d}t.
$

The integral is often computed numerically using quadrature, which is a sum of color values at a set of sampled points $t_1, t_2, \cdots, t_n$ on the ray, i.e.,
 $
     c(\mathbf{r}) = \sum_{i=1}^n w(t_i)c(t_i).
$

The weight $w(t)$ of each color value is the product between the volume density $\sigma$ and transparency $T$~\cite{Max1995}
$w(t) = T(t)\sigma(t)$,
 which measures the accumulated transmittance and occlusion of the ray up to that point.
 The transparency $T$, which is defined as   
 $
 T(t)=\exp\left(-\int_0^t\sigma(u)\,\mathrm{d}u\right),
 $
is the probability that the ray travels from $0$ to $t$ without hitting any other particle.
$T(t)$ is a monotonic decreasing function with a starting value of $T(0)=1$. 

In NeRF~\cite{Mildenhall2020}, the volume density $\sigma$ is directly computed by a multi-layer perceptron (MLP), which takes the 3D coordinates of a point as input and outputs the corresponding density value. This allows for a flexible representation of the volume density field and enables the synthesis of novel views from arbitrary viewpoints. However, since the volume density field does not provide explicit information about the surface geometry, extracting a high-quality surface from it is difficult. Moreover, the density field may not correspond to a valid surface at all due to the ill-posed nature of the reconstruction problem~\cite{Zhang2020}.

To overcome this limitation, NeuS~\cite{Wang2021} utilizes signed distance functions to represent 3D geometry. Specifically, it uses a scalar function $f$ that maps each point in 3D space to the signed distance from that point to the surface, where the zero level set of the function represents the target surface. It then maps the signed distance function $f$ to an S-density function $\phi_s(f)$ using a logistic density distribution $\phi_s(x)=se^{-sx}/(1+e^{-sx})^2$, which assigns a normalized density value to each point in space based on its signed distance from the surface. The normalized S-density function is then used to define the weight function $w(t)$ for volume rendering. 

Wang et al.~\cite{Wang2021}  proved that the weight function $w(t)$ in NeuS is both unbiased and occlusion-aware. Being unbiased means that the weight function attains a locally maximal value at a surface intersection point, while an occlusion-aware weight function implies that when two points have the same SDF value, the point closer to the viewpoint has a larger contribution to the output color than the other point. This accounts for the occlusion effect, where points behind the surface are not visible and should not contribute to the output color.

\subsection{Density Function}

A core part of \methodname\ is an unbiased and occlusion-aware weight function. 
Inspired by HF-NeuS~\cite{Wang2022HFNeuSIS}, we propose a bell-shaped weight function that maps unsigned distance to density. Our weight function is \emph{unbiased} in that the weights are maximum \textit{on} the surface.

\begin{figure}[!htbp]
    \centering
    \includegraphics[width=1.6in]{media/biased.png}
    \includegraphics[width=1.6in]{media/unbiased.png}   \\
    \makebox[1.6in]{(a)}
    \makebox[1.6in]{(b)}
    \caption{Weight function. (a) A biased weight function can have its maximum at a point away from the surface, reducing the accuracy of the reconstructed surfaces. (b) In contrast, an unbiased weight function attains its local maximum at the point where the ray intersects the surface. The use of an unbiased weight function is essential for accurate surface reconstruction in volume rendering.}
    \label{fig:density_function}
\end{figure}

\begin{theorem}
A ray $\mathbf{r}=\mathbf{o}+t\mathbf{d}$ hits a planar object $M$. Let $\theta$ be the angle between the UDF gradient and the ray direction $\mathbf{d}$. 
Define the bell-shaped density $\sigma$ as 
\begin{equation}
\label{eqn:theoreticaldensity}
    \sigma(t) = \frac{se^{-sf(t)}}{1+e^{-sf(t)}}|\cos(\theta)|,
\end{equation}
where $s>0$ is a learnable parameter.
Then the weight function $w(t)=T(t)\sigma(t)$ is unbiased.
\end{theorem}
\begin{proof}

Let $\mathbf{r}(t_0)$ be the intersection point. 

We first consider the transparency $T(t)$ with $t<t_0$, i.e., for any point along the ray up to $\mathbf{r}(t_0)$. 
Since the point $t$ is in front of $M$, we have $90^\circ<\theta\leq 180^\circ$ and $\cos(\theta)<0$. The transparency $T(t)$, $t<t_0$, is given by
\begin{displaymath}
    \begin{aligned}
    T(t)&=\exp\left(-\int^t_0\frac{se^{-sf(u)}}{1+e^{-sf(u)}}(-\cos(\theta))\mathrm{d} u\right)\\
    &=\exp\left(\int^t_0\frac{se^{-sf(u)}}{1+e^{-sf(u)}}\mathrm{d}f(u)\right)\\
    &=\exp\left(\ln(1+e^{-sf(0)})-\ln(1+e^{-sf(t)})\right)\\
    &=\frac{1+e^{-sf(0)}}{1+e^{-sf(t)}}.
\end{aligned}
\end{displaymath}

Next we compute the weight function $w(t)$, which is proportional to 
\begin{displaymath}
    \begin{aligned}
        w(t)=T(t)\sigma(t)&\propto \frac{e^{-sf(t)}}{(1+e^{-sf(t)})^2}.
    \end{aligned}
\end{displaymath}
It is straightforward to verify that the sign of $\frac{\mathrm{d}w}{\mathrm{d}f}$ is the same as $e^{-sf(t)}-1$. Using the chain rule, we can show that the derivative $\mathrm{d}w/\mathrm{d}t>0$ is positive for $t<t_0$, implying that the weight $w$ increases as the ray approaches $M$ from the front.

After the ray passes through $M$, $\cos\theta$ becomes positive and both $T(t)$ and $\sigma(t)$ are monotonically decreasing for $t>t_0$.
Therefore, the product $T(t)\sigma(t)$ decreases as the ray leaves $M$.

Since the weight is continuous, $w$ attains its maximum on the plane $M$, implying that it is unbiased.
\end{proof}

The density function $\sigma$ in Equation (\ref{eqn:theoreticaldensity}) is theoretically sound, satisfying the requirements of unbiased sampling. However, in practice, it may be too ``transparent'' to use (due to its bell-shaped geometry), leading to poor sampling efficiency. Moreover, the dependence of the bell-shaped function on the UDF gradients, as indicated by the $\cos(\theta)$ term, can introduce instability and sensitivity to noise and oscillations, thereby posing challenges in learning the function.

To address these issues, we replace the $\cos(\theta)$ term with a constant $c>1$, resulting in a modified density function 
\begin{displaymath}
    \hat{\sigma}(t) = \frac{cse^{-sf(t)}}{1+e^{-sf(t)}},\;s>0,\;c>1.
\end{displaymath}
This modification increases both numerical stability and opacity of the original density function $\sigma$.

However, unlike the original density $\sigma$, the modified density $\hat{\sigma}$ introduces bias in the weight function, since 
the maximum value of weight occurs at a point $t^{*}$ in front of $M$, which has a distance value 
\begin{equation}    
\label{eqn:maxweight}
f(t^*)=\frac{1}{s}\ln\frac{-c}{\cos (\theta)}.
\end{equation}

While the modified density function $\hat{\sigma}$ is not theoretically unbiased, setting $c$ as a small constant can greatly reduce the bias in practice. Additionally, we expect $\hat{\sigma}$ to be approximately occlusion-aware. To further understand the numerical properties of $\hat{\sigma}$, we consider the extreme case where the incident light ray is perpendicular to the planar surface $M$. In this case, the unsigned distance function is $f(t)=1-t$ for points in front of $M$. As $\hat{\sigma}$ is symmetric for the two sides of $M$, the surface transparency is the square of the transparency of the front side.
Our computation shows
\begin{displaymath}
\begin{aligned}
     &\left(e^{-\int_0^1\hat{\sigma}(u)\mathrm{d}u}\right)^2\\
    &=
    \left[\exp\left(-\int^1_0\frac{cse^{-s(1-u)}}{1+e^{-s(1-u)}}\mathrm{d} u\right)\right]^2
    =\left(\frac{1+e^{-s}}{2}\right)^{2c},
\end{aligned}
\end{displaymath}
and to reduce transparency, we should choose a relatively large $c$.

In our implementation, we set the constant $c=5$ based on the typical value of the learned parameter $s$, which ranges between $1000$ and $2000$ given that the models are learned in a unit sphere to become unitless. This enables us to estimate the upper bound of the bias. For points in front of the surface, the incident angle $\theta$ between the ray and the surface normal is obtuse, so we restrict $\theta$ to the range of $[91^\circ, 180^\circ]$. By setting $c=5$, the offset width between 0.00161 and 0.00566 is obtained relative to the true zero level set, indicating that the maximum relative bias is below 0.5\%. This error level is acceptable for most application scenarios. Moreover, the surface transparency in the extreme case mentioned above is less than $0.001$. When a ray has a larger incident angle, its transparency becomes even smaller, resulting in an almost opaque density $\hat{\sigma}$. As a result, the weight function $w$ is approximately occlusion-aware. Thus, setting the constant $c=5$ offers a good balance between occlusion-awareness and unbiasedness.


It is worth mentioning that our modified density function $\hat{\sigma}$ is much simpler than the density function used in NeuralUDF~\cite{Long2022}. Although both densities introduce biases, we can control the bias of our density function within a very small range. In contrast, it is unclear what the range of bias is for the density function used in NeuralUDF. 
Thanks to the simplicity, our density function is easier to learn, thereby our method can work for a wider range of models. This includes textureless models, which pose challenges for NeuralUDF.


\subsection{Training}
\textbf{Differentiable UDFs.} NeuS uses an MLP network to learn the signed distance function $f$, which is a differentiable function. In contrast, UDF is not differentiable at the zero level set, 
making the network difficult to learn the values and gradients of the UDF close to the zero level set. 

Another crucial requirement is to ensure non-negative values for the computed distances, which seems a trivial task as one may simply apply absolute value or normalization such as ReLU to the MLP output. However, applying the absolute value to the distance is not viable due to its non-differentiability at zero. Similarly, normalizing the output value using ReLU is not feasible as it is also non-differentiable at zero, and its gradient vanishes for negative inputs. This can be particularly problematic for learning UDFs, since when the MLP returns a negative distance value, the ReLU gradient vanishes, hindering the update of the distance to a positive value in the subsequent iterations.

We add a softplus function after the output layer of the MLP.
The softplus function~\cite{Dugas2000} is a smooth and differentiable approximation of the ReLU function that is defined as
$$\text{softplus}(x) = \frac{1}{\beta} \ln(1 + e^{\beta x}).$$
Softplus has the same shape as ReLU, but it is continuous and differentiable at every point, and its gradients do not vanish anywhere. Using the softplus function allows us to ensure that the output of the MLP is non-negative and differentiable, making it suitable for learning the UDF. We set $\beta=100$ in our experiments.

\textbf{Loss functions.} 
Following NeuralUDF~\cite{Long2022}, we adopt an iso-surface regularizer to penalize the UDF values of the non-surface points from being zero, therefore encouraging smooth and clean UDFs. The regularization loss is defined as~\cite{Long2022}
\begin{displaymath}    \mathcal{L}_{reg}=\frac{1}{MN} \sum_{i, k}\exp{\left(-\tau \cdot f(t_{i,k})\right)},
\end{displaymath}
where $\tau=5.0$ is a constant scalar that scales the learned UDF values, $M$ is the total number of sampled rays per training iteration, and $N$ is the number of sampled points on a single ray.

The value of $s$, which is learnable in our method, significantly affects the quality of the reconstruction. When $s$ is small, it introduces a larger bias and leads to a more blurred output. We observe that $s$ typically converges to a relatively large value between 1000 and 2000, leading to visually pleasing results. However, in rare cases when $s$ stops increasing during training, we apply a penalty to force it to increase. The penalty is defined as follows
\begin{displaymath}
    \mathcal{L}_{s} = \frac{1}{M} \sum_{i, k} \frac{1}{s_{i, k}},
\end{displaymath}
where $M$ is the number of rays during a training epoch. 
This term $\mathcal{L}_{s}$ aggregates the reciprocals of all $s$ values used for the point $t_{i,k}$ on ray $r_i$. 
Intuitively speaking, it encourages a larger $s$ during the early stage of training. In our implementation, we make this term optional since $s$ generally increases with a decreasing rate during training, and the penalty term is only necessary in rare cases when $s$ stops at a relatively low value.

As in other SDF- and UDF-based methods~\cite{Wang2021,Wang2022HFNeuSIS,Long2022}, we adopt color loss and Eikonal loss in our approach. Specifically, the color loss $\mathcal{L}_{color}$ is the $L_1$ loss between the predicted color and the ground truth color of a single pixel as used in~\cite{Wang2021}. The Eikonal loss $\mathcal{L}_{eik}$ is used to regularize the learned distance field to have a unit gradient~\cite{Gropp2020}. 
Putting it all together, we define the combined loss function as a weighted sum
\begin{displaymath}
    \mathcal{L} = \mathcal{L}_{color} + \lambda_1 \mathcal{L}_{eik} + \lambda_2 \mathcal{L}_{reg} + \lambda_3 \mathcal{L}_{s},
\end{displaymath}
where $\lambda_1$, $\lambda_2$, and $\lambda_3$ are hyperparameters that control the weight of each loss term. In our experiments, we empirically set $\lambda_1=0.1$, $\lambda_2=0.01$, and $\lambda_3=0.001$, although $\lambda_2$ is occasionally set to $0.02$, and $\lambda_3$ is optional.


\textbf{Adaptive hierarchical sampling.}
We propose an adaptive hierarchical sampling (AHS) strategy for efficiently finding sample points along each ray for color computation.
Existing approaches, such as NeuS~\cite{Wang2021} and NeuralUDF~\cite{Long2022}, adopt a fixed set of pre-defined sampling rates for all models, regardless of their geometry differences. They first uniformly sample 64 points on the ray and then iteratively conduct importance sampling on top of the coarse probability estimation. The sampling rate is doubled after each iteration, resulting in a maximum 512 samples on each ray. 

We argue that an effective sampling strategy should be self-adaptive to geometry, which is connected to the learnable parameter $s$. Essentially, a larger value of $s$ leads to a narrower bell-shaped density function, resulting in higher weights given to points closer to the surface and more concentrated samples on the surface. Our density function depends on the learnable parameter $s$, which usually ranges between 1000 and 2000. We use the learned $s$ to guide more precise sample selection. Specifically, in the $i$-th iteration of sampling, we set $s$ to
$\max\left(32\times 2^i, \frac{s}{2^{k-i}}\right)$,
where $k$ is the maximum number of sampling iterations, which we set as $k=4$ in our implementation. 
During the early stage of training, when $s$ is still relatively small, we use the sampling rates as in NeuS and NeuralUDF. As the training progresses and the value of $s$ consistently increases, our adaptive sampling strategy is activated. Our ablation study confirms that adaptive hierarchical sampling increases the accuracy of our approach by up to 25\%. See Section~\ref{sec:ablation}.

\subsection{Surface Extraction}

After learning the UDF $f$, extracting the object surface is essential. Instead of extracting the zero level set of the function $f$, we locate the sample points with the maximum weights to construct the surface. The reason for doing so is that extracting the zero level set from UDFs is notoriously unstable~\cite{Guillard2022}, often leading to artifacts such as holes, flipped triangles, and self-intersections in the output meshes.

The main technical challenge in locating the samples with maximum weights is that they depend on the view direction, due to the $\cos\theta$ term in Equation (\ref{eqn:maxweight}). However, since our method penalizes small values of $s$ and yields a relatively large $s \in [1000,2000]$, the $\ln\frac{-c}{\cos\theta}$ term has little effect to the distance value $f(t^*)$.

In our implementation, we obtain a subset of rays for each input image by uniformly sampling rays every $k=5$ pixels in both horizontal and vertical directions. For each ray $\bf r$, we sample points along it to find the location $t^*$ with the maximum weight. We classify the ray $\bf r$ as a foreground ray if the sum of weights for the samples along $\bf r$ inside the region of interest (a unit sphere centered at origin) is greater than 0.5. Otherwise, we consider $\bf r$ as a background ray that misses the target object.

After collecting all foreground rays from all input images, we use the sample with the maximum weight for each ray as a surface point. Computational results show that our strategy is effective in producing satisfactory results (see Section~\ref{sec:ablation}).

\section{Experiments}
\emph{Implementation Details.}
The MLP for the UDF network consists of 8 hidden layers, each with 256 elements. We also use skip connections after every 4 hidden layers. The output of the UDF network is a single value representing the predicted UDF and a 256-dimensional feature vector used in the color network.

For the color network, we use another MLP with 4 hidden layers, each having 256 elements.
We use the coarse-to-fine strategy proposed by Park \etal~\cite{Park2021} for position encoding, setting the maximum number of frequency bands to 16 for the UDF network and 6 for the color network. For background rendering, we use NeRF++~\cite{Zhang2020} for background prediction.
During training, we use the Adam optimizer~\cite{Kingma2015} with a global learning rate of 5e-4.
We sample 512 rays per batch and train our model for 300,000 iterations.


\emph{Data Sets.} To evaluate our method, we use two datasets: DeepFashion3D~\cite{Zhu2020} and DTU~\cite{Jensen2014}.
The DeepFashion3D dataset consists of clothing models, which are open models with boundaries. As only 3D points are available, we render 72 images of resolution $1024\times 1024$ with a white background from different viewpoints for each model. The DTU dataset consists of models captured in a studio, and all the models are watertight. We use this dataset to validate that our method also works well for watertight models. These datasets have been widely used in previous works such as~\cite{Yariv2021,Wang2021,Wang2022HFNeuSIS}.

\emph{Baselines.} Several methods have been proposed for learning signed distance functions (SDFs) from multi-view images, which generate watertight models. To validate the effectiveness of our proposed method, we compare with state-of-the-art methods, namely VolSDF~\cite{Yariv2021}, NeuS~\cite{Wang2021}, and HF-NeuS\cite{Wang2022HFNeuSIS}. 

To our knowledge, NeuralUDF~\cite{Long2022} \footnote{Since the source code of NeuralUDF is unavailable at the moment of submission, we cannot make quantitative comparison with it in the paper.} is the only method designed for open model reconstruction, but it is limited to texture-rich models. Our method, on the other hand, can reconstruct both textureless and texture-rich models thanks to the new and simpler density function we propose in this paper.

\begin{figure}[!htbp]
\centering
% \setlength\tabcolsep{2pt}
\begin{tabular}{ccc}
    \includegraphics[width=.8in]{./media/compare/22-1-gt} & \includegraphics[width=.8in]{./media/compare/22-1-ours} & \includegraphics[width=.8in]{./media/compare/22-1-hfs} \\
    Reference Image & Ours & HF-NeuS~\cite{Wang2022HFNeuSIS} \\
\end{tabular}
\caption{Qualitative comparison between our method and mask-supervised HF-NeuS~\cite{Wang2022HFNeuSIS}. Since the mask does not show the hole of the sleeve, HF-NeuS generates a watertight sleeve for the model. Our method, which is mask-free, correctly learns the structure of the sleeve.}
\label{fig:vs_hfneus}
\end{figure}

\begin{figure*}[!htbp]
\centering
\setlength\tabcolsep{0pt}
\begin{tabular}{cccccc:c}
    & Reference Image & GT & Ours & VolSDF~\cite{Yariv2021} & NeuS~\cite{Wang2021} & HF-NeuS~\cite{Wang2022HFNeuSIS}\\
    \#1 & \includegraphics[width=.6in]{./media/gt_color/5-13}&
    \includegraphics[width=.6in]{./media/gt_cloud/5-13}\includegraphics[width=.4in]{./media/gt_cloud/5-13-topdown}&
    \includegraphics[width=.6in]{./media/ours/5-13}\includegraphics[width=.4in]{./media/ours/5-13-topdown}&
    \includegraphics[width=.6in]{./media/volsdf/5-13}\includegraphics[width=.4in]{./media/volsdf/5-13-topdown}&
    \includegraphics[width=.6in]{./media/neus/5-13}\includegraphics[width=.4in]{./media/neus/5-13-topdown}&
    \includegraphics[width=.6in]{./media/hfs/5-13}\includegraphics[width=.4in]{./media/hfs/5-13-topdown}\\
    \#5 & \includegraphics[width=.6in]{./media/gt_color/125-1}&
    \includegraphics[width=.6in]{./media/gt_cloud/125-1}\includegraphics[width=.5in]{./media/gt_cloud/125-1-topdown}&
    \includegraphics[width=.6in]{./media/ours/125-1}\includegraphics[width=.5in]{./media/ours/125-1-topdown}&
    \includegraphics[width=.6in]{./media/volsdf/125-1}\includegraphics[width=.5in]{./media/volsdf/125-1-topdown}&
    \includegraphics[width=.6in]{./media/neus/125-1}\includegraphics[width=.5in]{./media/neus/125-1-topdown}&
    \includegraphics[width=.6in]{./media/hfs/125-1}\includegraphics[width=.5in]{./media/hfs/125-1-topdown}\\
    \#6 & \includegraphics[width=.6in]{./media/gt_color/276-1}&
    \includegraphics[width=.6in]{./media/gt_cloud/276-1}\includegraphics[width=.6in]{./media/gt_cloud/276-1-topdown}&
    \includegraphics[width=.6in]{./media/ours/276-1}\includegraphics[width=.6in]{./media/ours/276-1-topdown}&
    \includegraphics[width=.6in]{./media/volsdf/276-1}\includegraphics[width=.6in]{./media/volsdf/276-1-topdown}&
    \includegraphics[width=.6in]{./media/neus/276-1}\includegraphics[width=.6in]{./media/neus/276-1-topdown}&
    \includegraphics[width=.6in]{./media/hfs/276-1}\includegraphics[width=.6in]{./media/hfs/276-1-topdown}\\
\end{tabular}
\caption{Qualitative comparisons with VolSDF~\cite{Yariv2021}, NeuS~\cite{Wang2021}, and HF-NeuS~\cite{Wang2022} on the DeepFashion3D~\cite{Zhu2020} dataset. Row 1 is texture-rich, while rows 2 and 3 don't contain highly contrasting colors, and thus they are textureless. The surfaces produced by NeuS and VolSDF are closed watertight models, resulting in large reconstruction errors near the boundaries. HF-NeuS with the aid of masks can produce reasonable results. However, the results are still double-covered surfaces, which are closed from a topology perspective, and post-processing is required to extract the single-layered surface. In contrast, our \methodname\ can effectively reconstruct non-watertight models, leading to more faithful reconstruction results without relying on masks. See the supplementary material for additional results.}

\label{fig:df3d}
\end{figure*}

\subsection{Comparisons on Open Models}
We evaluate our method and compare it with baselines using the clothes from DeepFashion3D, where the models have multiple open boundaries. VolSDF, NeuS, and HF-NeuS always close the boundaries since they learn SDFs. In contrast, our method learns UDFs, which can generate open models. Table~\ref{tab:df3d} shows the point-to-point Chamfer distances of the results. Some of the Chamfer distances of the compared methods are large because the open holes are closed, resulting in significant errors.

It is worth noting that HF-NeuS requires mask supervision to produce reasonable results on the DeepFashion3D dataset. With mask supervision, it is capable of producing double-covered surfaces that capture the geometric features well. However, these surfaces are still closed from a topological perspective, and their Chamfer distances are higher than those produced by our method. Additionally, the ability of HF-NeuS to generate valid results relies heavily on the mask; if the mask lacks boundary information, HF-NeuS may close the boundaries. For example, as shown in Figure~\ref{fig:vs_hfneus}, the sleeve structure is not labeled in the mask, resulting in a closed sleeve. In contrast, our method is able to properly reconstruct the open sleeve structure without relying on a mask.


As demonstrated in Figure~\ref{fig:df3d}, we test various types of garments, some of which have rich textures, while others are nearly a single color. Learning UDFs for textureless models is more challenging since various regions of a model are ambiguous without clear color differences. 
However, our \methodname\ generates satisfactory results even without masks. On the other hand, NeuralUDF~\cite{Long2022} is unable to properly reconstruct textureless models, possibly due to their complex density function which is difficult to converge.

\begin{table*}[!htbp]
\centering
\setlength\tabcolsep{2pt}
\begin{tabular}{c|cccccccccccc|c}
    \hline
    Method & \#1 & \#2 & \#3 & \#4 & \#5 & \#6 & \#7 & \#8 & \#9 & \#10 & \#11 & \#12 & Mean \\
    \hline
    HF-NeuS~\cite{Wang2022HFNeuSIS} & 3.28 & \underline{2.93} & 4.11 & \underline{3.22} & 5.41 & 4.18 & 5.51 & 5.51 & 4.61 & 5.46 & 4.30 & \underline{3.37} & 4.32 \\
    \hline
    VolSDF~\cite{Yariv2021} & 6.22 & 5.75 & 9.45 & 8.33 & 8.99 & 16.37 & 17.01 & 5.95 & 12.88 & 9.08 & 17.39 & 11.54 & 10.75 \\
    NeuS~\cite{Wang2021} & 6.75 & 4.60 & 4.35 & 7.95 & 13.52 & 10.74 & 14.54 & 6.23 & 16.69 & 17.07 & 13.21 & 5.13 & 10.07 \\
    NeUDF w/ MeshUDF~\cite{Guillard2022}) & \underline{2.68} & 4.33 & \underline{2.90} & 3.81 & \underline{4.34} & \underline{3.32} & \underline{4.53} & \underline{3.57} & \underline{3.65} & \underline{3.78} & \underline{3.26} & 4.73 & \underline{3.74} \\
    NeUDF w/ max-weight & \textbf{2.09} & \textbf{1.77} & \textbf{1.75} & \textbf{2.00} & \textbf{2.34} & \textbf{2.16} & \textbf{3.54} & \textbf{2.60} & \textbf{2.81} & \textbf{3.63} & \textbf{2.64} & \textbf{2.61} & \textbf{2.50} \\
    \hline
\end{tabular}
\vspace{0.05in}
\caption{Quantitative evaluation of Chamfer distance (lower is better) on the DeepFashion3D~\cite{Zhu2020} dataset. HF-NeuS is trained with additional object mask supervision, while the other methods are not. Our \methodname\ results extracts by maximum weights achieve the lowest Chamfer distance for all cases, demonstrating its superior reconstruction capability for open models even without mask supervision. For comparison, our results extracted by MeshUDF~\cite{Guillard2022} are also quantified, which achieve the second best for most cases.}
\label{tab:df3d}
\end{table*}

\subsection{Comparisons on Watertight Models}
We compare the performance of \methodname\ and HF-NeuS in reconstructing watertight models using the DTU dataset, which is known for its rich geometric details. The comparison focuses on the visual quality of the models' output, specifically surface smoothness and the presence of artifacts. The DTU dataset poses challenges in 3D reconstruction due to the fact that many of the images only show a part of the object-of-interest. Despite this challenge, \methodname\ is able to reconstruct watertight models with acceptable visual quality, especially in regions that are only visible in a few images. In comparison, HF-NeuS produces smoother surfaces than ours but introduces noticeable artifacts in regions with limited visibility.
To illustrate the comparison, Figure~\ref{fig:dtu} shows two examples of the output from both \methodname\ and HF-NeuS on the DTU dataset.

\begin{figure}[h]
    \centering
    \setlength\tabcolsep{0pt}
    \begin{tabular}{ccc}
        Reference Image & Ours & HF-NeuS \\
        \includegraphics[width=1.25in]{media/compare/dtu_scan55-gt.png} &
        \includegraphics[width=.7in]{media/compare/dtu_scan55-ours.png} &
        \includegraphics[width=.7in]{media/compare/dtu_scan55-hfs.png} \\
        \includegraphics[width=1.25in]{media/compare/dtu_scan69-gt.png} &
        \includegraphics[width=0.85in]{media/compare/dtu_scan69-ours.png} &
        \includegraphics[width=0.85in]{media/compare/dtu_scan69-hfs.png} \\
    \end{tabular}
    \caption{Qualitative comparison on the DTU dataset. While HF-NeuS is able to produce smooth surfaces, it also introduces noticeable artifacts in regions with limited visibility. Our method uses UDFs which are generally more difficult to learn than SDFs, thereby our resulting surfaces are not as smooth as theirs. However, our results contain very small artifacts.}
    \label{fig:dtu}
\end{figure}



\subsection{Ablation Studies}
\label{sec:ablation}
\emph{Regularization loss.} The effectiveness of the iso-surface regularizer $\mathcal{L}_{reg}$ is verified through an ablation study. Figure~\ref{fig:vs_reg_loss} demonstrates that when trained with the regularization loss, the network successfully removes small and unwanted pieces, such as the parts covering the cuff of the sleeve in the reconstructed model. The iso-surface regularizer encourages clean and smooth UDFs, which reduces artifacts and produces a high-quality reconstructed model.

\begin{figure}[!htbp]
\centering
 \setlength\tabcolsep{2pt}
\begin{tabular}{cc}
    \includegraphics[width=1.5in]{./media/ablation/8-7-w-reg-loss} & \includegraphics[width=1.5in]{./media/ablation/8-7-wo-reg-loss} \\
    w/ reg. & w/o reg.\\
    \includegraphics[width=1.5in]{./media/ablation/22-1-w-ada-s} &
    \includegraphics[width=1.5in]{./media/ablation/22-1-wo-ada-s}\\
    w/ AHS &  w/o AHS \\
\end{tabular}
\caption{Ablation studies. Left two: The regularization loss $\mathcal{L}_{reg}$ helps to remove unwanted surfaces on the cuff of the sleeve. The Chamfer distance drops by 5.85\%. Right two: Adaptive hierarchical sampling leads to more accurate point sampling and helps to learn more accurate models. The Chamber distance also drops by 25\%. }
\label{fig:vs_reg_loss}
\end{figure}

\emph{Adaptive hierarchical sampling.} We train the model with and without AHS and compare their performance in terms of accuracy and visual quality. We found that using adaptive hierarchical upsampling helps to sample points more closely to the surface, especially for the parts close to boundaries, resulting in more accurate color computation in volume rendering. As shown in Figure~\ref{fig:vs_reg_loss}, training with AHS produces better reconstruction on the sleeves. 

\begin{figure}[!htbp]
    \centering
    \includegraphics[width=2.75in]{media/compare/vs_relu}
    \caption{Ablation study on the usage of ReLU (orange)~\cite{Fukushima1975} vs softplus (blue)~\cite{Dugas2000} in training. The former is non-differentiable at $0$ and its gradient vanishes for negative input, whereas the latter is differentiable everywhere. 
    Using ReLU after the output layer of the MLP, the network makes progress at the early stage of training, but collapses after 40K iterations, leading to a training loss reduction through the rendering of only backgrounds. In contrast, softplus leads to correct learning of both geometry and color, and consistently decreases the training loss over iterations.}
    \label{fig:vs_relu}
\end{figure}

\emph{Non-negativity.} Ensuring that the computed distances in the proposed method are non-negative is important, and can be achieved by applying either ReLU or softplus to the MLP output. However, ReLU is not differentiable at 0 and has vanishing gradients for negative inputs, which can make the network difficult to train. An ablation study confirms that training with ReLU only results in early progress, but fails to learn a valid UDF later on. See Figure~\ref{fig:vs_relu} for details.


\emph{Extracting surface points.} Instead of extracting the zero level-set from the learned UDFs, we construct the object surface by finding the points with the maximum weights.
We compare our surface extraction method with MeshUDF~\cite{Guillard2022}, the state-of-the-art method for extracting the zero level-set from UDFs. Since our method is tailored to the proposed density function $\hat{\sigma}$, it produces more accurate results than MeshUDF, which achieves the second best for most cases. See Table~\ref{tab:df3d}.


\emph{Limitations.}
Our method introduces a small bias during training, we can reduce the bias, but cannot eliminate it. Also, if the object has a very similar color to the background, our method has difficulty differentiating between the foreground object and the background, leading to incorrect result.

\section{Conclusions}

Overall, \methodname\ offers a promising approach to the problem of reconstructing both open and watertight models from multi-view images. Its advantages over existing methods lie in the use of  a simpler and more accurate density function, a smooth and differentiable UDF representation, and a simple yet effective surface reconstruction strategy tailored to the density function, which greatly improves the learning process.
Results from our experiments on the DeepFashion3D and DTU datasets demonstrate the effectiveness of our method, particularly in reconstructing textureless models and regions with limited visibility. Moreover, our method does not rely on object masks, making it more practical in real-world applications.

In the future, we plan to investigate strictly unbiased and occlusion-aware density functions to fill the theoretical gap and further improve the accuracy of our method. We also aim to explore the use of sparse views for UDF learning.


{\small
\bibliographystyle{ieee_fullname}
\bibliography{egpaper}
}

\newpage
\input{egpaper_suppl}

\end{document}