

\section{Experiments}




\begin{table}[t]
  \caption{\textbf{Comparisons via Objective Metrics.} We compare our performance against existing methods and ablation variants using objective evaluation metrics.}
  \vspace{-5pt}
  \begin{minipage}{0.50\textwidth} 
      \centering
      \small 
      \subcaption{\textbf{Baseline Comparison.} Performance against several existing methods.}
      \resizebox{0.99\linewidth}{!}{
          \begin{tabular}{>{\hspace{3pt}}l<{\hspace{3pt}}|>{\hspace{3pt}}c<{\hspace{3pt}}|>{\hspace{3pt}}c<{\hspace{3pt}}}
          \Xhline{1pt}
          \textbf{Method} & \textbf{\makecell{Relation Score $\uparrow$}} & \textbf{\makecell{Entity Score $\uparrow$}} \\ \Xhline{1pt}
          Text-to-Image~\cite{rombach2022ldm} &  0.3516 &  0.2896  \\ 
          Textual Inversion~\cite{gal2022textualinversion} &  0.3785 &   0.2679 \\ 
          DreamBooth~\cite{ruiz2022dreambooth} & 0.3576 & \textbf{0.2902} \\ 
          \textbf{Ours} & \textbf{0.3817} & \underline{0.2820} \\
          \Xhline{1pt}
          \end{tabular}
      }
      \label{tab:baseline_quantitative}
  \end{minipage}
  \hspace{0.01\textwidth}
  \begin{minipage}{0.50\textwidth} 
      \centering
      \small
      \vspace{8pt}
      \subcaption{\textbf{Ablation Study}. Steering or importance sampling is removed.}
      \resizebox{0.97\linewidth}{!}{
          \begin{tabular}{>{\hspace{3pt}}l<{\hspace{3pt}}|>{\hspace{3pt}}c<{\hspace{3pt}}|>{\hspace{3pt}}c<{\hspace{3pt}}}
          \Xhline{1pt}
          \textbf{Method} & \textbf{\makecell{Relation Score $\uparrow$}} & \textbf{\makecell{Entity Score $\uparrow$}} \\ \Xhline{1pt}
          Ours w/o Relation-Steering &  0.3748 &  0.2766  \\ 
          Ours w/o Importance Sampling &  0.3464 &   0.2790 \\ 
          \textbf{Ours} & \textbf{0.3817} & \textbf{0.2820} \\
          \Xhline{1pt}
          \end{tabular}
      }
      \label{tab:ablation_quantitative}    
  \end{minipage}
  \hspace{-2pt}
\end{table}



We present qualitative and quantitative results in this section, and more experiments and analysis are in the Supplementary File.
We adopt Stable Diffusion~\cite{rombach2022ldm} for all experiments since it achieves a good balance between quality and speed. We generate images at $512\times512$ resolution.



\subsection{Comparison Methods}



\noindent \textbf{Text-to-Image Generation using Stable Diffusion~\cite{rombach2022ldm}}. We use the original Stable Diffusion 1.5 as the text-to-image generation baseline. Since there is no ground-truth textual description for the relation in each set of exemplar images, we use natural language that can best describe the relation to replace the \R{} token.
%
For example, in Figure~\ref{fig:baseline_comparison} (a), the co-existing relation in the reference images can be roughly described as \textit{``is painted on"}. Thus, we use it to replace the \R{} token in the inference template \textit{``Spiderman \R{} building''}, resulting in a sentence \textit{``Spiderman is painted on building''}, which is then used as the text prompt for generation.

\noindent \textbf{Textual Inversion~\cite{gal2022textualinversion}}.
For fair comparison with our method developed on Stable Diffusion 1.5, we use the \textit{diffusers}~\cite{diffusers} implementation of Textual Inversion~\cite{gal2022textualinversion} on Stable Diffusion 1.5. Based on the default hyper-parameter settings, we tuned the learning rate and batch size for its optimal performance on our \RI{} task. We use Textual Inversion's LDM objective to optimize \R{} for 3000 iterations, and generate images using the obtained \R{}.

\noindent \textbf{DreamBooth~\cite{ruiz2022dreambooth}}.
We use \textit{diffusers} implementation of DreamBooth on Stable Diffusion 1.5.
To adapt DreamBooth to our \RI{} task for fair comparison, we made three modifications to the original implementation. First, instead of using the original training template like ``A photo of V* dog'', we explicitly inject the word ``relation'' into the text template to help DreamBooth focus on relation instead of entity, thereby using ``A photo of \R{} \textit{relation}'' to fine-tune the model. Second, the class-specific prior preservation loss is implemented with a text prompt ``A photo of \textit{relation}'' to avoid overfitting or language drift. Third, to align with fine-tuning stage's template, the template ``Entity A is \textit{in \R{} relation} with Entity B'' is used during inference. 

\subsection{Qualitative Comparisons}




%
\noindent\textbf{Our Results.}
%
%
In Figure~\ref{fig:qualitative_results}, we provide the generation results using \R{} inverted by ReVersion. We observe that our framework is capable of 1) synthesizing the entities in the inference template and 2) ensuring that entities follow the relation co-existing in the exemplar images. We provide additional qualitative results in the Supplementary File due to space constraint.

\noindent\textbf{Comparison of Relation Accuracy.}
Figure~\ref{fig:baseline_comparison} shows qualitative comparisons with existing methods. We compare our method with 1) Text-to-Image Generation via Stable Diffusion~\cite{rombach2022ldm}, 2) Textual Inversion~\cite{gal2022textualinversion}, and 3) DreamBooth~\cite{ruiz2022dreambooth}.
%
In Figure~\ref{fig:baseline_comparison} (a), although ``Text-to-Image Generation'' and ``DreamBooth'' successfully generate both entities (Spiderman and building), they fail to \textit{paint} Spiderman on the building as the exemplar images do. They severely rely on the bias between two entities: Spiderman usually \textit{climbs}/\textit{jumps} on the buildings, instead of being \textit{painted} onto the buildings. Similarly, in Figure~\ref{fig:baseline_comparison} (b), although all methods in comparison can generate at least one monkey, the relation between generated monkeys does not follow the \textit{``back to back''} relation in the exemplar images. In contrast,
Our ReVersion Framework does not have this problem. 

\noindent\textbf{Entity Leakage in Existing Methods.}
In Textual Inversion, entities in the exemplar images like canvas are leaked to \R{}, such that the generated image shows a Spiderman on the canvas even when the word \textit{``canvas''} is not in the inference prompt (see Figure~\ref{fig:baseline_comparison} (a)). In DreamBooth, the ``basket'' in exemplar images sometimes leak to the generated images (see Figure~\ref{fig:fig_dreambooth_leak}).
\extension{
  In Figure~\ref{fig:fig_neti}, we include comparisons with NeTI~\cite{alaluf2023neural} and also discuss its entity leakage problem.
} 




\subsection{Quantitative Comparisons via Human Evaluation}




\begin{table}[t]
  \centering
  \caption{\textbf{Comparison with Existing Methods (Human Preference).} Percentage of votes where users favor our results vs. comparison methods. Our method outperforms the baselines under all metrics.}
    \small 
    \resizebox{0.99\linewidth}{!}{
        \begin{tabular}{>{\hspace{3pt}}l<{\hspace{3pt}}|>{\hspace{3pt}}c<{\hspace{3pt}}|>{\hspace{3pt}}c<{\hspace{3pt}}|>{\hspace{3pt}}c<{\hspace{3pt}}}
        \Xhline{1pt}
        \textbf{Method} & \textbf{Relation Accuracy} & \textbf{Entity Accuracy} & \textbf{Overall Quality} \\ \Xhline{1pt}
        % \midrule
        Text-to-Image Generation~\cite{rombach2022ldm} & 6.45\% & 10.32\% & 9.68\%  \\ 
        Textual Inversion~\cite{gal2022textualinversion} & 6.13\% & 5.81\% & 5.16\% \\ 
        DreamBooth~\cite{ruiz2022dreambooth} & 18.39\% & 18.39\% & 19.03\% \\ 
        \textbf{Ours} & \textbf{69.03\%} & \textbf{65.48\%} & \textbf{66.13\%}  \\
        \Xhline{1pt}
      \end{tabular}
    }
  \label{tab:quantitative_human}
\end{table}
%
\begin{table}[t]
  \centering
  \vspace{5pt}
  \caption{\textbf{Ablation Study (Human Preference)}. Suppressing relation-steering or importance sampling introduces performance drops, which shows the necessity of both relation-steering and importance sampling.
  }
    \small % 
    \resizebox{0.99\linewidth}{!}{    
        \begin{tabular}{>{\hspace{3pt}}l<{\hspace{20pt}}|>{\hspace{3pt}}c<{\hspace{3pt}}|>{\hspace{3pt}}c<{\hspace{3pt}}|>{\hspace{3pt}}c<{\hspace{3pt}}}
        \Xhline{1pt}
        \textbf{Method} & \textbf{Relation Accuracy} & \textbf{Entity Accuracy} & \textbf{Overall Quality} \\ \Xhline{1pt}
        % \midrule
        w/o Relation-Steering & 11.20\% & 10.90\% & 13.31\%  \\ 
        w/o Importance Sampling & 11.20\% & 13.62\% & 7.14\% \\ 
        \textbf{Ours} & \textbf{77.60\%} & \textbf{75.48\%} & \textbf{79.55\%}  \\
        \Xhline{1pt}
        \end{tabular}
    }
  \label{tab:ablation_human}
\end{table}


We conduct user studies with 68 human evaluators to assess the performance of our ReVersion Framework on the \RI{} task. We sampled 20 groups of images. Each group has images generated by different methods or ablation variants. For each group, apart from the generated images, the following information is presented: 1) exemplar images of a particular relation, 2) text description of the exemplar images. We then ask the evaluators to vote for the best generated image with respect to the following metrics.

% Baseline Comparison Figure
\begin{figure*}[t]
  \centering
  \includegraphics[width=0.93\linewidth]{figures/fig_baseline_comparison.pdf}
  \vspace{-5pt}
  \caption{
        \textbf{Qualitative Comparisons with Existing Methods}. 
        Our method can generate entity and relation accurately. ``Text-to-Image Generation'' and ``DreamBooth'' can correctly generate entities described in text prompt, but fail to compose them following the desired relation. ``Textual Inversion'' suffers from appearance leakage (\eg, \R{} unexpectedly capturing the canvas in exemplar images), thus resulting in low entity accuracy (\eg, cannot generate spiderman and building simultaneously).
   }
   \label{fig:baseline_comparison}
  \end{figure*}

% Newer Method Comparison Figure
\begin{figure*}[t]
  \centering
  \includegraphics[width=0.90\linewidth]{figures/fig_neti.pdf}
  \vspace{-10pt}
   \caption{
      \extension{
        \textbf{Comparisons with Newer Method}. NeTI~\cite{alaluf2023neural} demonstrates \textit{some degree of effectiveness} for relation inversion, attributed to its adaptive adjustment at different network layers and denoising timesteps. For example, in (a) where \R{} denotes ``shaking hands'', NeTI successfully rendered rabbits extending their hands, trying to engage in the ``shake hands'' behaviour. However, NeTI is still prone to \textit{texture leakage}. For instance: (a) The striped patterns of cat fur from the exemplar images are unintentionally transferred to the rabbit fur in NeTI's outputs. (b) With the ``carved by'' relation, the metal dog appearance in the exemplar images is unintentionally captured by NeTI, resulting in images resembling a metal animal even when the text prompt is ``bodhisattva \R{} carrot''. Our relation steering is essential to help \R{} focus on the relation rather than the appearance, thereby producing results without texture leakage.
      }
   }
   \label{fig:fig_neti}
   \vspace{-35pt}
\end{figure*}



% Ablation Comparison Figure
\begin{figure*}[t]
  \centering
  \vspace{10pt}
   \includegraphics[width=1.00\linewidth]{figures/fig_ablation_comparison.pdf}
   \caption{
        \textbf{Ablation Study (Qualitative)}. Without relation-steering, \R{} suffers from appearance leak (\eg, white puppy in (a), gray background in (b)) and inaccurate relation capture (\eg, dog not being on top of plate in (b)). Without importance sampling, \R{} focuses on lower-level visual details (\eg, rattan around puppy in (a)) and misses high-level relations.
   }
   \label{fig:ablation_comparison}
   \vspace{5pt}
\end{figure*}

\noindent \textbf{Relation Accuracy}.
Human evaluators are asked to evaluate whether the relations of the two entities in the generated image are consistent with the relation co-existing in the exemplar images.
%

\noindent \textbf{Entity Accuracy}.
Given an inference template in the form of \textit{``$\langle$Entity A$\rangle$ \R{} $\langle$Entity B$\rangle$''}, we ask evaluators to determine whether \textit{$\langle$Entity A$\rangle$} and \textit{$\langle$Entity B$\rangle$} are both authentically generated in each image.

\noindent \textbf{Overall Quality}.
Human evaluators are asked to assess the overall performance on the ReVersion task, considering both the alignment of relation and entity, and the image quality.

\noindent
Table \ref{tab:quantitative_human} shows our method clearly obtains better results under all three metrics.






\subsection{Quantitative Comparisons via Objective Metrics}


\begin{figure}[t]
  \centering
  \vspace{10pt}
   \includegraphics[width=1.00\linewidth]{figures/fig_paper_complicated_relation.pdf}
   \caption{
    \textbf{ReVersion for Complicated Relation.}
     \textbf{\textit{(a) Exemplar images.}} In each exemplar image, people exhibit the similar relation of ``holding hands, leaning backwards''.
     \textbf{\textit{(b) Ours.}} ReVersion effectively captures this relation by \R{} and successfully applies it to new entities.
     \textbf{\textit{(c) Describe and T2I.}} The ``first describe the relation, then use text-to-image'' approach struggles to accurately represent such complex relation in newly synthesized images.
 }
   \label{fig:fig_paper_complicated_relation}
\end{figure}
% DreamBooth Leak Figure
\begin{figure}[t]
  \centering
  \vspace{10pt}
   \includegraphics[width=1.00\linewidth]{figures/fig_dreambooth_leak.pdf}
   \caption{
    \textbf{Appearance Leakage of DreamBooth}. 
    \textbf{\textit{(a) Entity Leakage (Red Boxes):}} 
    The basket from the exemplar images significantly leaks into images generated by DreamBooth. In contrast, our approach avoids this issue of entity leakage.
    \textbf{\textit{(b) Texture Leakage (Green Boxes):}} 
    While DreamBooth accurately generates the entity ``rabbit'', it encounters texture leakage from the exemplar images. That is, stripe patterns of cat fur texture (marked with green boxes) unintentionally transfer to the rabbit's fur in DreamBooth's outputs. Our method, in contrast, is free from such texture leakage.
}
   \label{fig:fig_dreambooth_leak}
\end{figure}




We devise automatic metrics to objectively evaluate ``relation accuracy'' and ``entity accuracy'', which are briefly introduced below. More implementation details of the objective metrics will be detailed in the Supplementary File.
For comparison experiments, we use the 1,000 inference templates in the ReVersion Benchmark for all relations, and generate 10 images using each template. 

\noindent\textbf{Relation Score.} 
We use PSGFormer~\cite{yang2022psg}, a pre-trained scene-graph generation network, to extract the relation features for relation accuracy evaluation.
Table~\ref{tab:baseline_quantitative} shows that our method outperforms all existing methods in comparison. 



\noindent\textbf{Entity Score.} 
%
We use CLIP~\cite{radford2021clip} score to calculate the alignment between the entity types in the text prompt versus the generated entities.
%
Table~\ref{tab:baseline_quantitative} shows that our method  outperforms Textual Inversion in terms of entity accuracy. This is because the \R{} learned by Textual Inversion contains leaked entity information, which distracts the model from generating the desired ``$E_{A}$'' and ``$E_{B}$''. Our steering loss effectively prevents entity information from leaking into \R{}, allowing for accurate entity synthesis. Furthermore, our approach achieves comparable entity score with ``Text-to-Image Generation'' and ``DreamBooth'', and significantly surpasses them in terms of relation score. 
It is worth mentioning that the CLIP-based metrics mainly focus on whether the correct class of object is generated, and does not fully take the pixel-level object quality into account. For example, as shown in Figure~\ref{fig:fig_dreambooth_leak}, the stripe textures of cat fur in exemplar images often leak to \R{}, resulting in unrealistic textures in generated rabbits. 





\subsection{Ablation Study}
\label{subsec:ablation}


From both Table~\ref{tab:ablation_human} (human evaluation) and Table~\ref{tab:ablation_quantitative} (objective metrics), we observe that removing steering or importance sampling results in deterioration in both relation accuracy and entity accuracy. This corroborates our observations that 1) relation-steering effectively guides \R{} towards the relation-dense regions and disentangles \R{} away from exemplar entities, and 2) importance sampling emphasizes high-level relations over low-level details, aiding \R{} to be relation-focal. We further show qualitatively the necessity of both modules in Figure~\ref{fig:ablation_comparison}.



\noindent \textbf{Effectiveness of Relation-Steering.}
%
In ``w/o Relation-Steering'', we remove the Steering Loss $L_\mathrm{steer}$ in the optimization process. As shown in Figure~\ref{fig:ablation_comparison}~(a), the appearance of the white puppy in the lower-left exemplar image is leaked into \R{}, resulting in similar puppies in the generated images. In Figure~\ref{fig:ablation_comparison}~(b), many appearance elements are leaked into \R{}, such as the gray background, the black cube, and the husky dog. The dog and the plate also do not follow the relation of \textit{``being on top of''} as shown in exemplar images. Consequently, the images generated via \R{} do not present the correct relation and introduced unwanted leaked imageries.


\noindent \textbf{Effectiveness of Importance Sampling.}
We replace our relation-focal importance sampling with uniform sampling, and observe that \R{} pays too much attention to low-level details rather than high-level relations. For instance, in Figure~\ref{fig:ablation_comparison}~(a) ``w/o Importance Sampling'', the basket rattan wraps around puppy's head in the same way as the exemplar image, instead of containing the puppy inside.




\subsection{Further Analysis}


\begin{figure*}[t]
  \centering
      \includegraphics[width=0.99\textwidth]{figures/fig_paper_styles.pdf}
      \caption{
          \textbf{ReVersion for Diverse Styles and Backgrounds}. The \R{} inverted by ReVersion can be applied robustly to relate entities under diverse backgrounds or styles.
      }
      \label{fig:fig_paper_styles}
  \end{figure*}
%

\noindent \textbf{Diverse Styles and Backgrounds}. As shown in Figure~\ref{fig:fig_paper_styles}, the \R{} inverted by ReVersion can be applied robustly to relate entities in scenes with diverse backgrounds or styles.



\noindent \textbf{More Comparisons on Complicated Relation}.
Some relations are hard to accurately express by text, 
or the description of such relation may be complex and difficult for the text-to-image generation model to effectively comprehend. For the relation shown in Figure~\ref{fig:fig_paper_complicated_relation} (a), our method (Figure~\ref{fig:fig_paper_complicated_relation} (b)) effectively captures these relations using \R{} and applies them to new entities. In Figure~\ref{fig:fig_paper_complicated_relation} (c), we engage four human subjects to observe the exemplar images in (a) and describe scenes where these relations are applied to new entities (detailed process in Supplementary File). Subsequently, we utilize text-to-image (T2I) to synthesize images based on these human descriptions. The results demonstrate that this ``describe and T2I'' approach struggles to accurately represent such complex relations in the newly synthesized images.



