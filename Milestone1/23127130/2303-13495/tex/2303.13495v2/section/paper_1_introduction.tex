\vspace{15pt}
\section{Introduction}
\label{sec:intro}

Recently, text-to-image (T2I) diffusion models~\cite{rombach2022ldm, ramesh2022dalle2, saharia2022imagen} have shown promising results and enabled subsequent explorations of various generative tasks. 
%
There have been several explorations~\cite{gal2022textualinversion, ruiz2022dreambooth, kumari2022customdiffusion, chen2023suti, wei2023elite, li2023blipdiffusion, jia2023tamingencoder} on the \textit{appearance inversion} task. Specifically, given a few images of a specific object (\eg, a cat statue), \textit{appearance inversion} learns to map a ``new word'' to this concept via the text-to-image model. 
The ``new word'' can then be used in prompts to generate new images that contain this concept.
%
While existing methods have made substantial progress in capturing object \textit{appearances}, such exploration for \textit{relations} between objects is rare. 

In this paper, we study the \textbf{\textit{Relation Inversion}} task, whose objective is to learn a \textit{relation} that co-exists in the given exemplar images. Specifically, with objects in each exemplar image following a specific relation, we aim to obtain a relation prompt in the text embedding space of the pre-trained text-to-image diffusion model. By composing the relation prompt with user-devised text prompts, users are able to synthesize images using the corresponding relation, with new objects, styles, and backgrounds, \etc. 
%
Studying \RI{} not only addresses a critical gap in text-to-image model inversion tasks but also paves the way for deeper understanding and generation of relation-rich visual content.

The \textit{Relation Inversion} task is intrinsically different from existing appearance inversion tasks, and thus poses unique challenges.
Appearance inversion~\cite{gal2022textualinversion, ruiz2022dreambooth, kumari2022customdiffusion, chen2023suti, wei2023elite, li2023blipdiffusion, jia2023tamingencoder} focuses on capturing the look of a specific entity, thus the commonly used pixel-level reconstruction loss is typically adequate to learn a prompt that encapsulates the shared information among exemplar images. In contrast, \textit{relation} is a more abstract visual concept, and a pixel-wise loss alone is insufficient for precise extraction of the intended relation. 
Consequently, linguistic and visual priors are needed to accurately represent these high-level relation concepts.


To this end, we propose the \textit{\textbf{ReVersion Framework}} to tackle the \RI{} problem. 
%
First, we exploit linguistic priors to steer the relation prompt in the text embedding space. Specifically, we found that in the text embedding space of Stable Diffusion, embeddings are generally clustered according to their Part-of-Speech (POS), as shown in Figure~\ref{fig:observation_a}. Also, the concept of ``relation'' is related to prepositional words. For example, the relation ``rides on'' is semantically related to the prepositions ``atop'', ``above'', and ``below''; the relation ``being contained within'' is semantically related to ``inside'', ``around'', ``in'', and ``including''.
This together with the POS clustering observation motivate us to steer the relation prompts towards the prepositional word cluster. Notably, we design a novel \textit{relation-steering contrastive learning scheme} to steer the relation prompt towards a relatively relation-dense region in the text embedding space. A set of basis prepositions are used as positive samples to pull the relation prompt, while words of other POS (\eg, nouns, adjectives) in text descriptions are regarded as negatives so that the semantics related to object appearances are disentangled away.
%

\begin{figure}[t]
    \vspace{6pt}
    \centering
     \includegraphics[width=0.7\linewidth]{figures/fig_tsne.pdf}
     \caption{
        \textbf{Part-of-Speech (POS) Clustering}. We use t-SNE~\cite{van2008visualizing} to visualize word distribution in CLIP's input text embedding space, where \R{} is optimized in our ReVersion framework. We observe that words of the same Part-of-Speech (POS) are closely clustered together, while words of different POS are generally at a distance from each other.
    }
     \label{fig:observation_a}
  \end{figure}
%
Second, to encourage attention on object interactions, we devise a \textit{relation-focal importance sampling} strategy. During the optimization process, we emphasize high-level interactions over relatively lower-level details (\eg, color and texture of objects), effectively leading to better \RI{} results. 


As the first attempt in this direction, we further contribute the \textit{\textbf{ReVersion Benchmark}}, which provides various exemplar images with diverse relations, from simple spatial arrangements to complex interactive behaviours.
%
The benchmark serves as an evaluation tool for future research in \textit{Relation Inversion}. Results on a variety of relations demonstrate the effectiveness of our ReVersion Framework.

Our contributions are summarized as follows:
%
\begin{itemize}
    \setlength\itemsep{0em}
    \item We study a new problem, \textbf{\textit{Relation Inversion}}, which requires learning a relation prompt for a relation that co-exists in several exemplar images. While existing T2I inversion works mainly focus on capturing appearances, we take the initiative to explore relation, an under-explored yet important pillar in the visual world.
    %
    \item We propose the \textbf{\textit{ReVersion Framework}}, where the \textit{relation-steering contrastive learning scheme} steers relation prompt using linguistic priors, and effectively disentangles the learned relation away from object appearances. The \textit{relation-focal importance sampling} further emphasizes high-level relations over low-level details.
    %
    \item We contribute the \textit{\textbf{ReVersion Benchmark}}, which serves as a diagnostic and benchmarking tool for the new task of \RI{}. 
\end{itemize}


