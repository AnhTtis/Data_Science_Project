
\section{More Experimental Details}
\label{sec:experiment}

\begin{figure*}[h!t]
    \centering
    \vspace{10pt}
    \includegraphics[width=0.60\linewidth]{figures_supp/fig_supp_user_study.pdf}
     \caption{
          \textbf{Example of Human Evaluation}. This is a screenshot of a user study question distributed to human evaluators. The order of different methods (\ie, $A$, $B$, $C$, and $D$) is randomized. Human evaluators are provided with the exemplar images, text prompt, and generated images. They are asked to vote for the best generated image among $A$, $B$, $C$, and $D$, for the three metrics (\ie, \textit{Relation Accuracy / Entity Accuracy / Overall Quality}) respectively.
     }
     \vspace{30pt}
     \label{fig_supp_user_study}
  \end{figure*}
  
  
  
  \begin{figure*}[h!t]
    \centering
     \includegraphics[width=0.60\linewidth]{figures_supp/fig_supp_difficult_relation_user_study.pdf}
     \caption{
          \textbf{Human Description of Relation}. This is a screenshot of a user study question distributed to human subjects. The human subjects are asked to observe the exemplar images and identify the co-existing relation in the exemplar images. They are then asked to use natural language to describe the relation. The description will then be used for the ``Describe and T2I'' baseline.
     }
     \vspace{10pt}
     \label{fig_supp_difficult_relation_user_study}
     \vspace{10pt}
  \end{figure*}


In this section, we provide more experimental details.


\subsection{Implementation Details of ReVersion}
\label{sec:implementation_details}

We introduce the implementation details of the ReVersion Framework.
Our framework is built on top of the \textit{diffusers}~\cite{diffusers} implementation of Stable Diffusion~\cite{rombach2022ldm} 1.5.
All experiments are conducted on $512{\times}512$ image resolution.
%
In Equation 4, the temperature parameter $\gamma$ in the steering loss $L_\mathrm{steer}$ is set as 0.07, following~\cite{he2020momentum}. In each iteration, 8 positive samples are randomly selected from the basis preposition set (see Table~\ref{tab:preposition_words}). 
%
In Equation 6, to ensure that the numerical values $\lambda_\mathrm{denoise}L_\mathrm{denoise}$ and $\lambda_\mathrm{steer}L_\mathrm{steer}$ are in comparable order of magnitude, we set $\lambda_\mathrm{denoise}=1.0$ and  $\lambda_\mathrm{steer}=0.01$. 
%
During the optimization process, we first initialize our relation prompt \R{} using the word \textit{``and''}, then optimize the prompt using the AdamW~\cite{loshchilov2017adamw} optimizer for 3,000 steps, with learning rate $2.5{\times}10^{-4}$ and batch size 2. 
%
During the inference process, we use classifier-free guidance for all experiments including the baselines and ablation variants, with a constant guidance weight 7.5.

\subsection{Human Evaluation}

We introduce the implementation details of the user studies in the main paper's Section 6.3.

Figure~\ref{fig_supp_user_study} is a screenshot of the user study form we distributed for main paper's Table 1, namely ``Comparing with Existing Methods''. We employ preference 
voting to differentiate the performance of different methods.
To ensure unbiased responses, the order of different methods' results is randomized. That is, the orders of generated images $A$, $B$, $C$, and $D$ are random and different in each question. 
%
For main paper's Table 1, ``Comparison with Existing Methods'', four methods are in comparison, so there are four choices: $A$, $B$, $C$, and $D$.
%
For main paper's Table 2, ``Ablation Study'', three methods are in comparison, so there are three choices: $A$, $B$, and $C$.


\subsection{Objective Evaluation Metrics}

We introduce the implementation details of the objective metrics used in the main paper's Section 6.4.

\noindent\textbf{Relation Score.}
%
We devise an objective evaluation metric to measure the quality and accuracy of the inverted relation. To do this, we train relation classifiers that categorize the ten relations in our ReVersion benchmark. We then use these classifiers to determine whether the entities in the generated images follow the specified relation. We employ PSGFormer~\cite{yang2022psg}, a pre-trained scene-graph generation network, to extract the relation feature vectors from a given image. The feature vectors are averaged-pooled and fed into linear SVMs for classification.
%
We calculate the \textit{Relation Score} as the percentage of generated images that follow the relation class in the exemplar images. 

\noindent\textbf{Entity Score.}
%
To evaluate whether the generated image contains the entities specified by the text prompt, we compute the CLIP score~\cite{radford2021clip} between a revised text prompt and the generated image, which we refer to as the \textit{Entity Score}.
%
CLIP~\cite{radford2021clip} is a vision-language model that has been trained on large-scale datasets. It uses an image encoder and a text encoder to project images and text into a common feature space. The CLIP score is calculated as the cosine similarity between the normalized image and text embeddings. A higher score usually indicates greater consistency between the output image and the text prompt.
%
In our approach, we calculate the CLIP score between the generated image and the revised text prompt ``$E_{A}$\textit{,} $E_{B}$'', which only includes the entity information. 

\subsection{Implementation of ``Describe and Text-to-Image''}
In main paper's Section 6.6 and Figure 6, we compared our method against the ``Describe and Text-to-Image (T2I)'' approach. We provide detailed process in Figure~\ref{fig_supp_difficult_relation_user_study}.

  