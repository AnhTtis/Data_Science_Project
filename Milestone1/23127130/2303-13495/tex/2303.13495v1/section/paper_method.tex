
\section{The Relation Inversion Task}
%
Relation Inversion aims to extract the common relation \R from several exemplar images.
%
Let $\mathcal{I}\,{=}\, \{I_{1}, I_{2}, ... I_{n}\}$ be a set of exemplar images, and $E_{i,A}$ and $E_{i,B}$ be two dominant entities in image $I_i$. In Relation Inversion, we assume that the entities in each exemplar image interacts with each other through a common relation $R$. A set of coarse descriptions ${C}~{=}\, \{c_{1}, c_{2}, ... c_{n}\}$ is associated to the exemplar images, where \mbox{``${c}_i\,{=}\,E_{i, A}$ \R 
$E_{i, B}$''} denotes the caption corresponding to image $I_i$. Our objective is to optimize the relation prompt \R such that the co-existing relation can be accurately represented by the optimized prompt. 


An immediate application of Relation Inversion is relation-specific text-to-image synthesis. Once the prompt is acquired, one can generate images with novel objects interacting with each other following the specified relation. More generally, this task reveals a new direction of inferring relations from a set of exemplar images. This could potentially inspire future research in representation learning, few-shot learning, visual relation detection, scene graph generation, and many more.



\section{The ReVersion Framework}

\subsection{Preliminaries}

%-----------------------------------------------
\noindent \textbf{Stable Diffusion.}
%
Diffusion models are a class of generative models that gradually denoise the Gaussian prior $\bx_T$ to the data $\bx_0$ (\eg, a natural image). 
%
The commonly used training objective $L_\mathrm{DM}$~\cite{ho2020ddpm} is:
%
\begin{gather}
    L_\mathrm{DM}(\theta) \defeq \Eb{t, \bx_0, \bepsilon}{ \left\| \bepsilon - \bepsilon_\theta(\bx_t, t) \right\|^2}, \label{eq:training_objective_simple}
\end{gather}
%
where $\bx_t$ is an noisy image constructed by adding noise $\bepsilon \sim \mathcal{N}(\bzero, \bI)$ to the natural image $\bx_0$, and the network $\bepsilon_\theta(\cdot)$ is trained to predict the added noise.
%
To sample data $\bx_0$ from a trained diffusion model $\bepsilon_\theta(\cdot)$, we iteratively denoise $\bx_t$ from $t = T$ to $t = 0$ using the predicted noise $\bepsilon_\theta(\bx_t, t)$ at each timestep $t$.



Latent Diffusion Model (LDM)~\cite{rombach2022ldm}, the predecessor of Stable Diffusion, mainly introduced two changes to the vanilla diffusion model~\cite{ho2020ddpm}. First, instead of directly modeling the natural image distribution, LDM models images' projections in autoencoder's compressed latent space. Second, LDM enables text-to-image generation by feeding encoded text input to the UNet~\cite{ronneberger2015unet} $\bepsilon_\theta(\cdot)$. The LDM loss is:
%
\begin{gather}
    L_\mathrm{LDM}(\theta) \defeq \Eb{t, \bx_0, \bepsilon}{ { \left\| \bepsilon - \bepsilon_\theta(\bx_t, t, \tau_\theta(c)) \right\|^2}}, \label{eq:ldm_loss}
\end{gather}
% 
where $\bx$ is the autoencoder latents for images, and $\tau_\theta(\cdot)$ is a BERT text encoder~\cite{devlin2018bert} that encodes the text descriptions $c$.

Stable Diffusion extends LDM by training on the larger LAION dataset~\cite{schuhmann2022laion}, and changing the trainable BERT text encoder to the pre-trained CLIP~\cite{radford2021clip} text encoder.

%-----------------------------------------------
\noindent \textbf{Inversion on Text-to-Image Diffusion Models.} 
%
Existing inversion methods focus on appearance inversion. Given several images that all contain a specific entity, they~\cite{gal2022textualinversion, ruiz2022dreambooth, kumari2022customdiffusion} find a text embedding V* for the pre-trained T2I model. The obtained V* can then be used to generate this entity in different scenarios.

In this work, we aim to capture object relations instead. Given several exemplar images which share a common relation $R$, we aim to find a relation prompt \R to capture this relation, such that \mbox{``$E_{A}$ \R 
$E_{B}$''} can be used to generate an image where \textit{$E_{A}$} and \textit{$E_{B}$} interact via relation \R.



\subsection{Preposition Prior}
\label{sec: prior}


Appearance inversion focuses on inverting low-level features of a specific entity, thus the commonly used pixel-level reconstruction loss is sufficient to learn a prompt that captures the shared information in exemplar images. In contrast, \textit{relation} is a high-level visual concept. A pixel-wise loss alone cannot accurately extract the target relation. Some linguistic priors need to be introduced to represent relations.


In this section, we present the \textbf{\textit{``preposition prior''}}, a language-based prior that steers the relation prompt towards a relation-dense region in the text embedding space. This prior is motivated by a well-acknowledged premise and two interesting observations on natural language.



% insight Single Column
\begin{figure}[t]
  \centering
   \includegraphics[width=0.80\linewidth]{figures/fig_tsne.pdf}
   \vspace{-5pt}
   \caption{
   \textbf{Part-of-Speech (POS) Clustering}. We use t-SNE~\cite{van2008visualizing} to visualize word distribution in CLIP's input embedding space, where \R is optimized in our ReVersion framework. We observe that words of the same Part-of-Speech (POS) are closely clustered together, while words of different POS are generally at a distance from each other.
   }
   \label{fig:observation_a}
\end{figure}
%

\noindent\textit{\textbf{Premise: Prepositions describe relations.}} In natural language, prepositions are words that express the relation between elements in a sentence~\cite{huddleston_pullum_2002}. This language prior naturally leads us to use prepositional words to regularize our relation prompt.


\noindent\textit{\textbf{Observation \uppercase\expandafter{\romannumeral1}: POS clustering.}} As shown in Figure~\ref{fig:observation_a}, in the text embedding space of language models, embeddings are generally clustered according to their Part-of-Speech (POS) labels. This observation together with the \textit{Premise} inspire us to steer our relation prompt \R towards the preposition subspace (\ie,~the red region in Figure~\ref{fig:observation_a}).


\noindent\textit{\textbf{Observation \uppercase\expandafter{\romannumeral2}: Sparse activation.}} As shown in Figure~\ref{fig:observation_b}, feature similarity between the a real-world relation and the prepositional words are sparsely distributed, and the activated prepositions are usually related to this relation's semantic meaning. For example, for the relation ``swinging'', the sparsely activated prepositions are ``underneath'', ``down'', ``beneath'', ``aboard', \etc., which together collaboratively describe the ``swinging'' interaction. This pattern suggests that only a subset of prepositions should be activated during optimization, leading to our noise-robust design in Section~\ref{sec: contrastive}.



% insight Single Column
\begin{figure}[t]
  \centering
   \includegraphics[width=0.98\linewidth]{figures/fig_heatmap.pdf}
   \vspace{-6pt}
   \caption{\textbf{Sparse Activation}. We visualize the cosine similarities between real-world relations and basis prepositional words, and observe that relation is generally sparsely activated \textit{w.r.t.} the basis prepositions. Note that each row of similarity scores are sparsely distributed, with few peak values in red.}
   \label{fig:observation_b}
   \vspace{-4pt}
\end{figure}



Based on the aforementioned analysis, we hypothesize that a common \textit{visual relation} can be generally expressed as a set of basis prepositions, with only a small subset of highly semantically-related prepositions activated. Motivated by this, we design a \textit{relation-steering contrastive learning scheme} to steer the relation prompt \R into a relation-dense region in the text embedding space.



\subsection{Relation-Steering Contrastive Learning} 
\label{sec: contrastive}

Recall that our goal is to acquire a relation prompt \R that accurately captures the co-existing relation in the exemplar images. 
A basic objective is to reconstruct the exemplar images using \R:
%
\begin{gather}
    \langle{R}\rangle = \argmin_{\langle{r}\rangle} \Eb{t, \bx_0, \bepsilon}{ { \left\| \bepsilon - \bepsilon_\theta(\bx_t, t, \tau_\theta(c)) \right\|^2}}, \label{eq:ti_loss}
\end{gather}
where $\bepsilon \sim \mathcal{N}(\bzero, \bI)$, \R is the optimized text embedding, and $\bepsilon_\theta(\cdot)$ is a pre-trained text-to-image diffusion model whose weights are frozen throughout optimization. $\langle{r}\rangle$ is the relation prompt being optimized, and is fed into the pre-trained T2I model as part of the text description $c$. 

However, as discussed in Section \ref{sec: prior} , this pixel-level reconstruction loss mainly focus on low-level reconstruction rather than visual relations. Consequently, directly applying this loss could result in appearance leakage and hence unsatisfactory relation inversion.


Motivated by our \textit{Premise} and \textit{Observation \uppercase\expandafter{\romannumeral1}}, we adopt the preposition prior as an important guidance to steer the relation prompt towards the relation-dense text embedding subspace. Specifically, we can use the prepositions as positive samples and other POS' words (\ie, nouns, adjectives) as negative samples to construct a contrastive loss. Following InfoNCE~\cite{oord2018representation}, this preliminary contrastive loss is derived as:
%
\begin{gather}
    L_\mathrm{pre} = -log\frac{e^{R^{\top}\cdot P_{i} / \gamma}}{e^{R^{\top}\cdot P_{i} / \gamma}  + \sum_{k=1}^{K}e^{R^{\top}\cdot N_i^k / \gamma}},
\label{eq:pre-loss}
\end{gather}
%
where $R$ is the relation embedding, and $\gamma$ is the temperature parameter. $P_i$ (\ie,~positive sample) is a randomly sampled preposition embedding at the $i$-th optimization iteration, and $N_i=\{N_i^1, ..., N_i^K\}$ (\ie,~negative samples) is a set of randomly sampled embeddings from other POS. All embeddings are normalized to unit length.


Since the relation prompt should also be disentangled away from object appearance, we further propose to select the object descriptions of exemplar images as the improved negative set. In this way, our choice of negatives serves two purposes: 1) provides POS guidance away from non-prepositional clusters, and 2) prevents appearance leakage by including exemplar object descriptions in the negative set.


In addition, \textit{Observation \uppercase\expandafter{\romannumeral2}} (sparse activation) implies that only a small set of prepositions should be considered as true positives.
%
Therefore, we need a contrastive loss that is tolerant about noises in the positive set (\ie, not all prepositions should be activated). Inspired by \cite{miech2020end}, we revise Equation~\ref{eq:pre-loss} to a noise-robust contrastive loss as our final Steering Loss:
%
\begin{gather}
    L_\mathrm{steer} = -log\frac{\sum_{l=1}^{L}{e^{R^{\top}\cdot P_{i}^l / \gamma}}}{\sum_{l=1}^{L}{e^{R^{\top}\cdot P_{i}^l / \gamma}  + \sum_{m=1}^{M}e^{R^{\top}\cdot N_i^m / \gamma}}},
    \label{eq:l_steer}
\end{gather}
%
where $P_i=\{P_i^1, ..., P_i^L\}$ refers to positive samples randomly drawn from a set of basis prepositions (more details provided in Supplementary File), and $N_i=\{N_i^1, ..., N_i^M\}$ refers to the improved negative samples.



\subsection{Relation-Focal Importance Sampling}
%
In the sampling process of diffusion models, high-level semantics usually appear first, and fine details emerge at later stages~\cite{wang2023diffusion, huang2023collaborative}. As our objective is to capture the relation (a high-level concept) in exemplar images, it is undesirable to emphasize on low-level details during optimization. Therefore, we conduct an importance sampling strategy to encourage the learning of high-level relations. Specifically, unlike previous reconstruction objectives, which samples the timestep $t$ from a uniform distribution, we skew the sampling distribution so that a higher probability is assigned to a larger $t$. The Denoising Loss for relation-focal importance sampling becomes:
%
\begin{gather}
\begin{split}
    L_\mathrm{denoise} &= \Eb{t\sim f, \bx_0, \bepsilon}{ { \left\| \bepsilon - \bepsilon_\theta(\bx_t, t, \tau_\theta(c)) \right\|^2}}, \qquad \\
    f(t)        &= \frac{1}{T}(1 - \alpha \cos{\frac{\pi t}{T}}),
    \label{eq:importance_sampling}
\end{split}
\end{gather}
%
where $f(t)$ is the importance sampling function, which characterizes the probability density function to sample $t$ from. The skewness of $f(t)$ increases with $\alpha\,{\in}\,(0, 1]$. We set $\alpha=0.5$ throughout our experiments.
%
The overall optimization objective of the ReVersion framework is written as:
\begin{gather}
    \langle{R}\rangle = \argmin_{\langle{r}\rangle} (\lambda_\mathrm{steer}L_\mathrm{steer} + \lambda_\mathrm{denoise}L_\mathrm{denoise}), \label{eq:overall_loss}
\end{gather}
%
where $\lambda_\mathrm{steer}$ and $\lambda_\mathrm{denoise}$ are the weighting factors. 
