
\section{More Experimental Comparisons}
\label{sec:experiment}


In this section, we provide more experimental comparison results and analysis. For each method in comparison, we use the 1,000 inference templates in the ReVersion benchmark, and generate 10 images using each template.


\begin{table}[b]
  \centering
  \caption{\textbf{Additional Quantitative Results (Baselines).}}
  \vspace{-0.5em}
    \small 
    \begin{tabular}{l|c|c}
    \Xhline{1pt}
    \textbf{Method} & \textbf{Relation Accuracy Score (\%) $\uparrow$} & \textbf{Entity Accuracy Score (\%) $\uparrow$} \\ \Xhline{1pt}
    Text-to-Image Generation~\cite{rombach2022ldm} &  35.16\% &  \textbf{28.96\%}  \\ 
    Textual Inversion~\cite{gal2022textualinversion} &  37.85\% &   26.79\% \\ 
    \textbf{Ours} & \textbf{38.17\%} & \underline{28.20}\% \\
    \Xhline{1pt}
  \end{tabular}
  \label{tab:baseline}
\end{table}


\begin{table}[b]
  \centering
  \caption{\textbf{Additional Quantitative Results (Ablation Study).}}
  \vspace{-0.5em}
    \small
    \begin{tabular}{l|c|c}
    \Xhline{1pt}
    \textbf{Method} & \textbf{Relation Accuracy Score (\%) $\uparrow$} & \textbf{Entity Accuracy Score (\%) $\uparrow$} \\ \Xhline{1pt}
    Ours w/o Steering &  37.48\% &  27.66\%  \\ 
    Ours w/o Importance Sampling &  34.64\% &   27.90\% \\ 
    \textbf{Ours} & \textbf{38.17\%} & \textbf{28.20\%} \\
    \Xhline{1pt}
  \end{tabular}
  \label{tab:ablation}
\end{table}


\subsection{Relation Accuracy Score}


We devise objective evaluation metric to measure the quality and accuracy of the inverted relation. To do this, we train relation classifiers that categorize the ten relations in our ReVersion benchmark. We then use these classifiers to determine whether the entities in the generated images follow the specified relation. We employ PSGFormer~\cite{yang2022psg}, a pre-trained scene-graph generation network, to extract the relation feature vectors from a given image. The feature vectors are averaged-pooled and fed into linear SVMs for classification.

We calculate the \textit{Relation Accuracy Score} as the percentage of generated images that follow the relation class in the exemplar images. Table~\ref{tab:baseline} shows that our method outperforms text-to-image generation~\cite{rombach2022ldm} and Textual Inversion~\cite{gal2022textualinversion}. Additionally, Table~\ref{tab:ablation} reveals that removing the steering scheme or importance sampling scheme results in a performance drop in relation accuracy.

\subsection{Entity Accuracy Score}

To evaluate whether the generated image contains the entities specified by the text prompt, we compute the CLIP score~\cite{radford2021clip} between a revised text prompt and the generated image, which we refer to as the \textit{Entity Accuracy Score}.

CLIP~\cite{radford2021clip} is a vision-language model that has been trained on large-scale datasets. It uses an image encoder and a text encoder to project images and text into a common feature space. The CLIP score is calculated as the cosine similarity between the normalized image and text embeddings. A higher score usually indicates greater consistency between the output image and the text prompt.
%
In our approach, we calculate the CLIP score between the generated image and the revised text prompt ``$E_{A}$\textit{,} $E_{B}$'', which only includes the entity information. 

In Table~\ref{tab:baseline}, we observe that our method  outperforms Textual Inversion in terms of entity accuracy. This is because the \R learned by Textual Inversion might contain leaked entity information, which might distract the model from generating the desired ``$E_{A}$'' and ``$E_{B}$''. Our steering loss effectively prevents entity information from leaking into \R, allowing for accurate entity synthesis. Furthermore, our approach achieves comparable entity accuracy score with text-to-image generation using Stable Diffusion~\cite{rombach2022ldm}, and significantly surpasses it in terms of relation accuracy. Table~\ref{tab:ablation} shows that removing the steering or importance sampling scheme results in a drop in entity accuracy.


% Figure: DBooth
\begin{figure*}[h]
  \centering
   \includegraphics[width=0.99\linewidth]{figures/fig_supp_dbooth.pdf}
   \caption{\textbf{Qualitative Comparison with DreamBooth~\cite{ruiz2022dreambooth}.} In (a), the Spiderman generated by DreamBooth is mostly climbing on the building rather than being a painting. In (b), DreamBooth fails to capture the \textit{``sits back to back with''} relation. In (c), while DreamBooth successfully captures the relation, the appearance of the basket from exemplar images are severely leaked into the generated images via \R.
   }
   \label{fig_supp_dbooth}
\end{figure*}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Comparison with Fine-Tuning Based Method}
%
We further compare our method with a fine-tuning based method, DreamBooth~\cite{ruiz2022dreambooth}. In order to adapt DreamBooth to our relation inversion task, we follow the original implementation to design a text prompt ``A photo of \R relation" containing the unique identifier ``\R'' to fine-tune the model. The class-specific prior preservation loss is also added with a text prompt ``A photo of relation" to avoid overfitting and language drift. As shown in Figure~\ref{fig_supp_dbooth}, directly using DreamBooth on our task could result in poor object relation and appearance leakage. For example, in Figure~\ref{fig_supp_dbooth}~(a) the Spiderman is mostly climbing on the building rather than being a painting, while in Figure~\ref{fig_supp_dbooth}~(c) the appearance of the basket in exemplar images is severely leaked into the DreamBooth generated images.
