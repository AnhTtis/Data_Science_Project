\section{Experiments}



% Baseline Comparison Figure
\begin{figure*}[t]
  \centering
   \includegraphics[width=0.99\linewidth]{figures/fig_baseline_comparison.pdf}
   \caption{\textbf{Qualitative Comparisons with Existing Methods}. Our method significantly surpasses both baselines in terms of relation accuracy and entity accuracy. 
   }
   \label{fig:baseline_comparison}
\end{figure*}


We present qualitative and quantitative results in this section, and more experiments and analysis are in the Supplementary File.
We adopt Stable Diffusion~\cite{rombach2022ldm} for all experiments since it achieves a good balance between quality and speed. We generate images at $512\times512$ resolution.


\subsection{Comparison Methods}

\noindent \textbf{Text-to-Image Generation using Stable Diffusion~\cite{rombach2022ldm}}. We use the original Stable Diffusion 1.5 as the text-to-image generation baseline. Since there is no ground-truth textual description for the relation in each set of exemplar images, we use natural language that can best describe the relation to replace the \R token.
%
For example, in Figure~\ref{fig:baseline_comparison} (a), the co-existing relation in the reference images can be roughly described as \textit{``is painted on"}. Thus we use it to replace the \R token in the inference template \textit{``Spiderman \R building''}, resulting in a sentence \textit{``Spiderman is painted on building''}, which is then used as the text prompt for generation.

\noindent \textbf{Textual Inversion~\cite{gal2022textualinversion}}.
For fair comparison with our method developed on Stable Diffusion 1.5, we use the \textit{diffuser}~\cite{diffusers} implementation of Textual Inversion~\cite{gal2022textualinversion} on Stable Diffusion 1.5. Based on the default hyper-parameter settings, we tuned the learning rate and batch size for its optimal performance on our Relation Inversion task. We use Textual Inversion's LDM objective to optimize \R for 3000 iterations, and generate images using the obtained \R.

\subsection{Qualitative Comparisons}
%
In Figure~\ref{fig:qualitative_results}, we provide the generation results using \R inverted by ReVersion. We observe that our framework is capable of 1) synthesizing the entities in the inference template and 2) ensuring that entities follow the relation co-existing in the exemplar images.
%
We then compare our method with 1) text-to-image generation via Stable Diffusion~\cite{rombach2022ldm} and 2) Textual Inversion~\cite{gal2022textualinversion} in Figure~\ref{fig:baseline_comparison}.
%
For example, in the first row, although the text-to-image baseline successfully generates both entities (Spiderman and building), it fails to \textit{paint} Spiderman on the building as the exemplar images do. Text-to-image generation severely relies on the bias between two entities: Spiderman usually \textit{climbs}/\textit{jumps} on the buildings, instead of being \textit{painted} onto the buildings. Using exemplar images and our ReVersion framework alleviates this problem. In Textual Inversion, entities in the exemplar images like canvas are leaked to \R, such that the generated image shows a Spiderman on the canvas even when the word  \textit{``canvas"} is not in the inference prompt.

\subsection{Quantitative Comparisons}

We conduct a user study with 37 human evaluators to assess the performance of our ReVersion framework on the Relation Inversion task. We sampled 20 groups of images. Each group has three images generated by different methods. For each group, apart from generated images, the following information is presented: 1) exemplar images of a particular relation 2) text description of the exemplar images. We then ask the evaluators to vote for the best generated image respect to the following metrics.



\begin{table}[t]
  \centering
  \caption{\textbf{Quantitative Results.} Percentage of votes where users favor our results vs. comparison methods. Our method outperforms the baselines under both metrics.}
  \vspace{-0.5em}
    \small 
    \begin{tabular}{l|c|c}
    \Xhline{1pt}
    \textbf{Method} & \textbf{Relation} & \textbf{Entity} \\ \Xhline{1pt}
    % \midrule
    Text-to-Image Generation & 7.86\% & 15.49\%  \\ 
    Textual Inversion & 8.94\% &  10.05\% \\ 
    \textbf{Ours} & \textbf{83.20\%} & \textbf{74.46\%} \\
    \Xhline{1pt}
  \end{tabular}
  \label{tab:quantitative}
\end{table}



\noindent \textbf{Entity Accuracy}.
Given an inference template in the form of \textit{``Entity A \R Entity B''}, we ask evaluators to determine whether \textit{Entity A} and \textit{Entity B} are both authentically generated in each image.


\noindent \textbf{Relation Accuracy}.
Human evaluators are asked to evaluate whether the relations of the two entities in the generated image are consistent with the relation co-existing in the exemplar images.
%
As shown in Table \ref{tab:quantitative}, our method clearly obtains better results under the two quality metrics.


\begin{table}[t]
  \centering
  \caption{\textbf{Ablation Study}. Suppressing steering or importance sampling introduces performance drops, which shows the necessity of both relation-steering and importance sampling.
  }
  \vspace{-0.5em}
    \small % 
    \begin{tabular}{l|c|c}
    \Xhline{1pt}
    \textbf{Method} & \textbf{Relation} & \textbf{Entity} \\ \Xhline{1pt}
    % \midrule
    w/o Steering & 11.20\% & 10.90\% \\ 
    w/o Importance Sampling & 11.20\% & 13.62\% \\ 
    \textbf{Ours} & \textbf{77.60\%} & \textbf{75.48\%} \\
    \Xhline{1pt}
  \end{tabular}
  \label{tab:ablation}
\end{table}



% Ablation Comparison Figure
\begin{figure*}[t]
  \centering
  \vspace{-10pt}
   \includegraphics[width=0.99\linewidth]{figures/fig_ablation_comparison.pdf}
   \vspace{-5pt}
   \caption{\textbf{Qualitative Comparisons with Ablation Variants}. Without relation-steering, \R suffers from appearance leak (\eg, white puppy in (a), gray background in (b)) and inaccurate relation capture (\eg, dog not being on top of plate in (b)). Without importance sampling, \R focuses on lower-level visual details (\eg, rattan around puppy in (a)) and misses high-level relations.
   }
   \vspace{-5pt}
   \label{fig:ablation_comparison}
\end{figure*}

\subsection{Ablation Study}

From Table~\ref{tab:ablation}, we observe that removing steering or importance sampling results in deterioration in both relation accuracy and entity accuracy. This corroborates our observations that 1) relation-steering effectively guides \R towards the relation-dense ``preposition prior'' and disentangles \R away from exemplar entities, and 2) importance sampling emphasizes high-level relations over low-level details, aiding \R to be relation-focal. We further show qualitatively the necessity of both modules in Figure~\ref{fig:ablation_comparison}.


\noindent \textbf{Effectiveness of Relation-Steering.}
%
In ``w/o Steering'', we remove the Steering Loss $L_\mathrm{steer}$ in the optimization process. As shown in Figure~\ref{fig:ablation_comparison}~(a), the appearance of the white puppy in the lower-left exemplar image is leaked into \R, resulting in similar puppies in the generated images. In Figure~\ref{fig:ablation_comparison}~(b), many appearance elements are leaked into \R, such as the gray background, the black cube, and the husky dog. The dog and the plate also do not follow the relation of \textit{``being on top of''} as shown in exemplar images. Consequently, the images generated via \R do not present the correct relation and introduced unwanted leaked imageries.


\noindent \textbf{Effectiveness of Importance Sampling.}
%
We replace our relation-focal importance sampling with uniform sampling, and observe that \R pays too much attention to low-level details rather than high-level relations. For instance, in Figure~\ref{fig:ablation_comparison}~(a) ``w/o Importance Sampling'', the basket rattan wraps around the puppy's head in the same way as the exemplar image, instead of containing the puppy inside.

