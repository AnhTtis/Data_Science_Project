\section{Introduction}


Recently, Text-to-image (T2I) diffusion models~\cite{rombach2022ldm, ramesh2022dalle2, saharia2022imagen} have shown promising results and enabled subsequent explorations of various generative tasks. 
%
There have been several attempts~\cite{gal2022textualinversion, ruiz2022dreambooth, kumari2022customdiffusion} to \textbf{\textit{invert}} a pre-trained text-to-image model, obtaining a text embedding representation to capture the object in the reference images. While existing methods have made substantial progress in capturing object appearances, such exploration for relations is rare. 
%
Capturing object relation is intrinsically a harder task as it requires the understanding of interactions between objects as well as the composition of an image, and existing inversion methods are unable to handle the task due to entity leakage from the reference images. Yet, this is an important direction that worths our attention.


In this paper, we study the \textbf{\textit{Relation Inversion}} task, whose objective is to learn a relation that co-exists in the given exemplar images. Specifically, with objects in each exemplar image following a specific relation, we aim to obtain a relation prompt in the text embedding space of the pre-trained text-to-image diffusion model. By composing the relation prompt with user-devised text prompts, users are able to synthesize images using the corresponding relation, with customized objects, styles, backgounds, \etc.


To better represent high-level relation concepts with the learnable prompt, we introduce a simple yet effective \textbf{\textit{preposition prior}}. The preposition prior is based on a premise and two observations in the text embedding space. Specifically, we find that 1) prepositions are highly related to relations, 2) prepositions and words of other Parts-of-Speech are individually clustered in the text embedding space, and 3) complex real-world relations can be expressed with a basic set of prepositions. Our experiments show that this language-based prior can be effectively used as a high-level guidance for the relation prompt optimization.


Based on our preposition prior, we propose the \textbf{ReVersion} framework to tackle the Relation Inversion problem. Notably, we design a novel \textit{relation-steering contrastive learning scheme} to steer the relation prompt towards a relation-dense region in the text embedding space. A set of basis prepositions are used as positive samples to pull the embedding into the sparsely activated region, while words of other Parts-of-Speech (\eg, nouns, adjectives) in text descriptions are regarded as negatives so that the semantics related to object appearances are disentangled away.
%
To encourage attention on object interactions, we devise a \textit{relation-focal importance sampling} strategy. It constrains the optimization process so that high-level interactions rather than low-level details are emphasized, effectively leading to better relation inversion results. 


As the first attempt in this direction, we further contribute the \textbf{ReVersion Benchmark}, which provides various exemplar images with diverse relations.
%
The benchmark serves as an evaluation tool for future research in the Relation Inversion task. Results on a variety of relations demonstrate the power of preposition prior and our ReVersion framework.

Our contributions are summarized as follows:
%
\vspace{-0.4em}
\begin{itemize}
    \setlength\itemsep{0em}
    \item We study a new problem, \textbf{\textit{Relation Inversion}}, which requires learning a relation prompt for a relation that co-exists in several exemplar images. While existing T2I inversion methods mainly focus on capturing appearances, we take the initiative to explore relation, an under-explored yet important pillar in the visual world.
    %
    \item We propose the \textbf{\textit{ReVersion framework}}, where the \textit{relation-steering contrastive learning scheme} steers the relation prompt using our \textbf{\textit{``preposition prior''}}, and effectively disentangles the learned relation away from object appearances. \textit{Relation-focal importance sampling} further emphasizes high-level relations over low-level details.
    %
    \item We contribute the \textit{\textbf{ReVersion Benchmark}}, which serves as a diagnostic and benchmarking tool for the new task of \RI{}. 
\end{itemize}
