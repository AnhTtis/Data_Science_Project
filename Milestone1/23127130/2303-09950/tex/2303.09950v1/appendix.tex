%!TEX root = graphsc.tex

\appendix

This supplementary material provides the implementation details of \ours{} and the baselines~(\cref{appx:implementation-details}), the details of the metrics~(\cref{appx:metrics}), more experiments and analysis~(\cref{appx:additional-experiments}), the details of N-ICP to estimate the warping function~(\cref{appx:deformation-estimation}), and discusses the limitations of our method~(\cref{appx:limitations}).


\section{Implementation Details}
\label{appx:implementation-details}

\ptitle{Network architecture.}
%
In the initial feature embedding, we use a three-layer MLP with ($256$, $256$, $256$) channels to project the correspondence embedding to a high-dimension representation. Group normalization~\cite{wu2018group} and LeakyReLU are used after each layer in the MLP.

Unless otherwise noted, we use $3$ correspondence embedding modules to generate the spatial-consistency-aware features, while each contains $2$ SCA-SA modules. All layers in the models have $d=256$ feature channels.
The node coverage is $\sigma_n = 0.08\text{m}$. For each correspondence, we use $k=6$ neighboring nodes to construct the graph. And the distance tolerance when computing spatial consistency is $\sigma_{d} = 0.08\text{m}$. 

At last, we adopt another three-layer MLP with ($128$, $64$, $1$) channels to classify each correspondences. Group normalization~\cite{wu2018group} and LeakyReLU are used after the first two layers in the MLP, and sigmoid activation is applied after the last layer. We select the correspondences whose confidence scores are above $\tau_{s} = 0.4$ as inliers and the others are removed as outliers.

\ptitle{Baselines.}
%
For the baseline models PointCN~\cite{pais20203dregnet} and PointDSC~\cite{bai2021pointdsc}, the initial feature embedding and the classification head are the same as aforementioned. In PointCN, we replace the correspondence embedding modules with $6$ MLP blocks, each of which consists of two linear layers with residual connection. In PointDSC, we use $6$ SCNonLocal~\cite{bai2021pointdsc} modules to learn the correspondence features. Due to memory limit, we randomly sample $2048$ input correspondences in PointDSC. The architectures of different models are illustrated in \cref{fig:architecture}. All the layers in the baseline models have $256$ feature channels as in \ours{}.

\ptitle{Training and testing.}
%
We implement and evaluate our method with PyTorch~\cite{paszke2019pytorch} on an NVIDIA $2080$Ti GPU.
The models are trained with Adam optimizer~\cite{kingma2014adam} for $40$ epochs. The batch size is $1$ and the weight decay is $10^{-6}$. The learning rate starts from $10^{-4}$ and decays exponentially by $0.05$ after each epoch. During training, we regard the correspondences as inliers if their residuals under the ground-truth deformation are below $\tau = 0.04\text{m}$, and outliers otherwise.
For data augmentation, we adopt a relatively weak data augmentation as in~\cite{yew2022regtr} with a random rotation sampled from $[0, 10^{\circ}]$ and a random translation sampled from $\mathcal{N}(0, 0.05)$.

In the experiments on 4DMatch, as the training data has been used to train the correspondence extractor, the putative correspondences on the training set are almost all inliers. In this case, the training data cannot provide effective supervision to train an outlier rejection network. To solve this problem, we split the official validation sequences by $90\%$/$10\%$ for training/validation, respectively, and evaluate the models on the official testing squences.


\input{figures/architecture}


\section{Metrics}
\label{appx:metrics}

Following~\cite{li2022non}, we mainly evaluate our method using $4$ metrics: 3D End Point Error, 3D Accuracy Strict, 3D Accuracy Relaxed and Outlier Ratio.

\emph{3D End Point Error} (EPE) measures the average error over all warped points under the estimated and the ground-truth warping functions $\mathcal{W}(\cdot)$ and $\mathcal{W}^{*}(\cdot)$:
\begin{equation}
\mathtt{EPE} = \frac{1}{\lvert \mathcal{P} \rvert} \sum_{\mathbf{p}_i \in \mathcal{P}} \lVert \mathcal{W}(\mathbf{p}_i) - \mathcal{W}^{*}(\mathbf{p}_i) \rVert_2.
\end{equation}

\emph{3D Accuracy Strict} (AccS) and \emph{3D Accuracy Relaxed} (AccR) measure the fractions of points whose EPEs are below a EPE threshold or relative errors are below a relative error threshold. For AccS, the EPE threshold is $2.5$cm and the relative error threshold is $2.5\%$. For AccR, the EPE threshold is $5$cm and the relative error threshold is $5\%$. The relative error is computed as:
\begin{equation}
\mathtt{RE}(\mathbf{p}_i) = \frac{\lVert \mathcal{W}(\mathbf{p}_i) - \mathcal{W}^{*}(\mathbf{p}_i) \rVert_2}{\lVert \mathcal{W}^{*}(\mathbf{p}_i) - \mathbf{p}_i \rVert_2}.
\end{equation}
And the 3D accuracy is defined as:
\begin{align}
\mathtt{AccS} & \tight{=} \frac{1}{\lvert \mathcal{P} \rvert} \sum_{\mathbf{p}_i \in \mathcal{P}} \llbracket \mathtt{EPE}(\mathbf{p}_i) \tight{<} 2.5\text{cm} \tight{\lor} \mathtt{RE}(\mathbf{p}_i) \tight{<} 2.5\% \rrbracket,\\
\mathtt{AccR} & \tight{=} \frac{1}{\lvert \mathcal{P} \rvert} \sum_{\mathbf{p}_i \in \mathcal{P}} \llbracket \mathtt{EPE}(\mathbf{p}_i) \tight{<} 5\text{cm} \lor \mathtt{RE}(\mathbf{p}_i) \tight{<} 5\% \rrbracket,
\end{align}
where $\llbracket \cdot \rrbracket$ is the Inversion bracket.

\emph{Outlier Ratio} (OR) measures the fraction of points which are not successfully registered. Following~\cite{li2022non}, a point is regarded as a failure if its relative error is above $30\%$:
\begin{equation}
\mathtt{OR} = \frac{1}{\lvert \mathcal{P} \rvert} \sum_{\mathbf{p}_i \in \mathcal{P}} \llbracket \mathtt{RE}(\mathbf{p}_i) > 30\% \rrbracket
\end{equation}



\section{Additional Experiments}
\label{appx:additional-experiments}


\subsection{Evaluations on Low-Inlier-Ratio Cases}

To evaluate the performance in low-inlier-ratio scenarios, we add random outliers into the correspondences from GeoTransformer, making the final inlier ratio less than $30\%$. In \cref{table:results-low-ir}, PointDSC and PointCN fail to achieve reasonable registration results due to enormous outliers. In contrast, our method still achieves promising results, showing strong generality to low-inlier-ratio cases.

\subsection{Evaluations on Large-Deformation Cases}

Next, we investigate the performance when the deformations are large. As there is no off-the-shelf benchmarks with large deformations, we evaluate our method on the testing pairs whose mean residuals are above $15$cm on 4DMatch. In \cref{table:results-large-deformation}, our method significantly outperforms the baselines, demonstrating its efficacy under large deformations.


\subsection{Additional Ablation Studies}

\ptitle{Euclidean distance \vs geodesic distance.}
%
We first replace the distance metric in building deformation graph from Euclidean distance to geodesic distance. Each correspondence is assigned to its $k=6$ nearest neighbors in the geodesic space. Note that we still use Euclidean distance during N-ICP for fair comparison. As shown in \cref{table:results-ablation-supp}~(a), geodesic distance consistently degrades the performance. Compared to the Euclidean distance, the geodesic distance is less robust to occlusion as the points on the geodesic shortest path between two points can be missing. On the contrary, according to local rigidity, Euclidean distance is approximatedly preserved near each graph node, but is more robust and efficient.


\ptitle{Positional embedding.}
%
Next, we study the impact of the positional embedding used in the initial feature embedding in \cref{table:results-ablation-supp}~(b). We first ablate the the fourier positional encoding and use only the point coordinates. This model achieves similar results on 4DMatch and slightly worse results on 4DLoMatch. We then ablate the point coordinates and use only the fourier positional encoding. This model achieves better recall but worse precision, especially in low-overlap scenarios. And the model with the both terms achieve the best results.

\ptitle{Loss functions.}
%
We further study the efficacy of the loss functions in \cref{table:results-ablation-supp}~(c). We first ablate the feature consistency loss, which degrades the classification performance especially in low-overlap scenarios. Explicitly supervising the feature consistency between correspondences helps learn more discriminative features between inliers and outliers and thus contributes to better performance. Next we replace the binary focal loss with a binary cross-entropy loss, which significantly decreases the performance. As the putative correspondences are commonly extremely unbalanced, either predominated by inliers or outliers, cross-entropy loss hampers the convergency of the model.

\ptitle{Local spatial consistency.}
%
At last, we ablate the local spatial consistency in the self-attention. In \cref{table:results-ablation-supp}~(d), removing the local spatial consistency considerably decreases the performance, especially in low-overlap scenarios. We also note that this model surpasses PointCN and PointDSC, indicating the efficacy of our deformation graph-based design.


\input{tables/results-low-ir}

\input{tables/results-large-deformation}

\input{tables/results-ablation-supp}


\subsection{Qualitative Results}


\input{figures/distribution}

We visualize the features of the detected true inliers by t-SNE. In \cref{fig:distribution}, the inliers in different parts have different features, while the spatially-near ones also lie closely in the feature space. These results indicate that our method effectively learns the local motions in different parts.

We then provide more qualitative comparisons of the filtered correspondences on 4DMatch (\cref{fig:gallery-4dmatch-supp}), CAPE (\cref{fig:gallery-cape-supp}) and DeepDeform (\cref{fig:gallery-deepdeform}).
Benefitting to the powerful local spatial consistency, \ours{} removes more outlier correspondences and achieves better inlier ratio (precision) than the baseline methods, especially under large deformations. Moreover, albeit achieving promising precision, PointDSC fails to preserve sufficient inliers. On the contrary, our method achieves both high precision and high recall, indicating it can effectively reject most outliers while better preserving inliers.


\section{Deformation Estimation}
\label{appx:deformation-estimation}

Given the source point cloud $\mathcal{P}$, the target point cloud $\mathcal{Q}$, and the correspondences $\mathcal{C} = \{(\mathbf{x}_i, \mathbf{y}_i) \mid \mathbf{x}_i \in \mathcal{P}, \mathbf{y}_i \in \mathcal{Q} \}$ between them, we adopt embedded deformation~\cite{sumner2007embedded} to formulate the warping function. It parameterizes the deformation on the deformation graph $\mathcal{G}=\{\mathcal{V}, \mathcal{E}\}$.
The graph nodes $\mathcal{V}$ are sampled from the \emph{source} point cloud with uniform furthest point sampling and the node coverage is $\sigma_{g} = 0.08\text{m}$. Each point $\mathbf{p}_i$ in the source point cloud is assigned to its $k_g = 6$ nearest nodes $\mathcal{K}_i$. Two nodes are connected by an undirected edge if they share a common point. Each node $\mathbf{v}_j$ is associated with a local rigid transformation $\{ \mathbf{R}_j, \mathbf{t}_j \}$. And the warping function $\mathcal{W}$ is then approximated as:
\begin{equation}
\mathcal{W}(\mathbf{p}_i) = \sum_{\mathbf{v}_j \in \mathcal{V}} \alpha_{i,j} \big(\mathbf{R}_j (\mathbf{p}_i - \mathbf{v}_j) + \mathbf{t}_j + \mathbf{v}_j\big),\notag
\end{equation}
where $\alpha_{i, j}$ is the skinning factor as defined in~\cite{newcombe2015dynamicfusion}:
\begin{equation}
\alpha_{i, j} = \llbracket \mathbf{v}_j \in \mathcal{K}_i \rrbracket \cdot \frac{\exp(-\lVert \mathbf{p}_i - \mathbf{v}_j \rVert^2 / (2 \sigma_n^2))}{\sum_{\mathbf{v}_k \in \mathcal{K}_i} \exp(-\lVert \mathbf{p}_i - \mathbf{v}_k \rVert^2 / (2 \sigma_n^2))},\notag
\end{equation}
where $\llbracket \cdot \rrbracket$ is the Iverson bracket.
We then solve for $\mathcal{W}$ by minimizing the following objective function:
\begin{equation}
E = \lambda_c E_{\text{corr}} + \lambda_r E_{\text{reg}},\notag
\end{equation}
where $E_{\text{corr}}$ is the mean squared residual between the correspondences and $E_{\text{reg}}$ is an as-rigid-as-possible~\cite{igarashi2005rigid} regularization term to constrain the smoothness of deformations:
\begin{equation}
\begin{aligned}
E_{\text{corr}} & = \sum_{(\mathbf{x}_i, \mathbf{y}_i) \in \mathcal{C}} \lVert \mathcal{W}(\mathbf{x}_i) - \mathbf{y}_i \rVert_{2}^{2},\\
E_{\text{reg}} & = \sum_{(\mathbf{v}_u, \mathbf{v}_v) \in \mathcal{E}} \lVert \mathbf{R}_{u}(\mathbf{v}_{v} - \mathbf{v}_{u}) + \mathbf{v}_{u} + \mathbf{t}_{u} - (\mathbf{v}_{v} + \mathbf{t}_{v}) \rVert_{2}^{2}.
\end{aligned}
\notag
\end{equation}
The weights to balance the two terms are set to $\lambda_c = 25$ and $\lambda_r = 1$, respectively.

This problem can be efficiently solved by Non-rigid ICP (N-ICP) algorithm~\cite{li2008global,sumner2007embedded}.
Following~\cite{bozic2020neural,li2022lepard}, we update the associated rigid transformations incrementally:
\begin{equation}
\begin{aligned}
\mathbf{R}^{(t)}_j & = \Delta\mathbf{R}^{(t)}_j \cdot \mathbf{R}^{(t-1)}_j,\\
\mathbf{t}^{(t)}_j & = \mathbf{t}^{(t-1)}_j + \Delta\mathbf{t}^{(t)}_j,
\end{aligned}
\notag
\end{equation}
where $\mathbf{R}^{(0)}_j = \mathbf{I}$ and $\mathbf{t}^{(0)}_j = \mathbf{0}$. For simplicity, we omit the superscript $(t)$ in the following text.
The residual rotations are formulated in the axis-angle representation $\Delta\mathbf{R}_j \tight{=} \exp(\boldsymbol{\omega}^{\land}_j)$, where $\exp(\cdot)$ is the exponential map function and $(\cdot)^{\land}$ computes the skew-symmetric matrix of a $3$-d vector.
We then solve for $\{ \boldsymbol{\omega}_j, \Delta\mathbf{t}_j \}$ with Gauss-Newton algorithm. The residual terms are computed as:
\begin{equation}
\begin{aligned}
\mathbf{r}^{i}_{\text{corr}} & \tight{=} \sqrt{\lambda_{c}} \bigl( \mathcal{W}(\mathbf{x}_i) - \mathbf{y}_i \bigr),\\
\mathbf{r}^{i}_{\text{reg}} & \tight{=} \sqrt{\lambda_{r}} \bigl( \mathbf{R}_{u}(\mathbf{v}_{v} - \mathbf{v}_{u}) + \mathbf{v}_{u} + \mathbf{t}_{u} - (\mathbf{v}_{v} + \mathbf{t}_{v}) \bigr).
\end{aligned}
\notag
\end{equation}
where $c_i = (\mathbf{x}_i, \mathbf{y}_i) \in \mathcal{C}$ and $e_i = (\mathbf{v}_u, \mathbf{v}_v) \in \mathcal{E}$.

Next, we compute the partial derivatives of $\{ \boldsymbol{\omega}_j, \Delta\mathbf{t}_j \}$.
As $\boldsymbol{\omega}_j$ is a residual rotation, it is expected to near $\mathbf{0}$ and thus we approximate its partial derivatives with those at $\mathbf{0}$:
\begin{equation}
\begin{aligned}
\frac{\partial \mathcal{W}(\mathbf{p}_i)}{\partial \boldsymbol{\omega}_j} & \approx \frac{\partial \mathcal{W}(\mathbf{p}_i)}{\partial \boldsymbol{\omega}_j}\bigg\arrowvert_{\mathbf{0}} = -\alpha_{i, j} \bigl(\mathbf{R}^{(t-1)}_j(\mathbf{x}_i - \mathbf{v}_j)\bigr)^{\land},\\
\frac{\partial \mathcal{W}(\mathbf{p}_i)}{\partial \Delta\mathbf{t}_j} & = \alpha_{i, j} \mathbf{I}.
\end{aligned}
\notag
\end{equation}
To this end, the partial derivatives are computed as:
\begin{equation}
\begin{aligned}
\frac{\partial \mathbf{r}^{i}_{\text{corr}}}{\partial \boldsymbol{\omega}_j} & = -\sqrt{\lambda_c} \alpha_{i, j} \bigl(\mathbf{R}^{(t-1)}_j(\mathbf{x}_i - \mathbf{v}_j)\bigr)^{\land},\\
\frac{\partial \mathbf{r}^{i}_{\text{corr}}}{\partial \Delta\mathbf{t}_j} & = \sqrt{\lambda_c} \alpha_{i, j} \mathbf{I},\\
\frac{\partial \mathbf{r}^{i}_{\text{reg}}}{\partial \boldsymbol{\omega}_u} & = -\sqrt{\lambda_r} \bigl(\mathbf{R}^{(t-1)}_u(\mathbf{v}_v - \mathbf{v}_u)\bigr)^{\land},\\
\frac{\partial \mathbf{r}^{i}_{\text{reg}}}{\partial \Delta\mathbf{t}_u} & = \sqrt{\lambda_r} \mathbf{I},\\
\frac{\partial \mathbf{r}^{i}_{\text{reg}}}{\partial \Delta\mathbf{t}_v} & = -\sqrt{\lambda_r} \mathbf{I}.
\end{aligned}
\notag
\end{equation}
We denote the collection of the residual terms as:
\begin{equation}
\mathbf{r} = [(\mathbf{r}^{1}_{\text{corr}})^T, ..., (\mathbf{r}^{\lvert \mathcal{C} \rvert}_{\text{corr}})^T, (\mathbf{r}^{1}_{\text{reg}})^T, ..., (\mathbf{r}^{\lvert \mathcal{E} \rvert}_{\text{reg}})^T]^T \in \mathbb{R}^{3 \lvert \mathcal{C} \rvert + 3 \lvert \mathcal{E} \rvert},
\notag
\end{equation}
the collections of variables $\{\boldsymbol{\omega}_j, \Delta\mathbf{t}_j \}$ as:
\begin{equation}
\Delta\mathbf{T} = [\boldsymbol{\omega}^T_1, ..., \boldsymbol{\omega}^T_{\lvert \mathcal{V} \rvert}, \Delta\mathbf{t}^T_1, ..., \Delta\mathbf{t}^T_{\lvert \mathcal{V} \rvert}]^T \in \mathbb{R}^{6 \lvert \mathcal{V} \rvert},
\notag
\end{equation}
and the Jaccobian matrix between $\mathbf{r}$ and $\Delta\mathbf{T}$ is denoted as $\mathbf{J} \tight{\in} \mathbb{R}^{(3 \lvert \mathcal{C} \rvert + 3 \lvert \mathcal{E} \rvert) \times (6 \lvert \mathcal{V} \rvert)}$ following the computation of derivatives above. $\Delta\mathbf{T}$ can then be computed by solving the linear system:
\begin{equation}
(\mathbf{J}^T\mathbf{J} + \lambda_m \mathbf{I})\Delta\mathbf{T} = \mathbf{J}^T \mathbf{r}.
\notag
\end{equation}
where $\lambda_m=0.01$ is the Marquardt factor.

\section{Limitations}
\label{appx:limitations}

Our method could have the following two potential limitations. First, our method serves as a post outlier rejection step after the correspondence extractor. To this end, our method is able to make given correspondences as clean as possible, but cannot infer new correspondences and improve the coverage of the correspondences on point clouds. Second, our method is based on deformation graph and local rigidity of deformations, so it could have difficulty in modeling sudden changes of geometric structures. We would leave these for future work.

\input{figures/gallery-4dmatch-supp}

\input{figures/gallery-deepdeform}

\input{figures/gallery-cape-supp}