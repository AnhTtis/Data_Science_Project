\newcommand{\stepZeroFROMs}{10,960,563\xspace}
\newcommand{\stepZeroUniqueWords}{46,070\xspace}
\newcommand{\stepZeroSelectedWords}{2,120\xspace}
\newcommand{\stepZeroOS}{74\xspace}
\newcommand{\stepZeroStopwords}{920\xspace}

\newcommand{\wordlistStop}{WL\textsubscript{stop}\xspace}
\newcommand{\wordlistOs}{WL\textsubscript{OS}\xspace}
\newcommand{\wordlistExclude}{WL\textsubscript{to-exclude}\xspace}

\newcommand{\dockerfileParsed}{769,385\xspace}

\newcommand{\datasetPretraining}{D\textsubscript{PT}\xspace}
\newcommand{\datasetFinetuning}{D\textsubscript{FT}\xspace}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Deep Learning for Generating Dockerfiles}
\label{sec:approach}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

We define a procedure to train a DL model for the generation of Dockerfiles from \nlRecipeNames. 
% \eject

In \figref{fig:workflow} we report the workflow we used to train a DL-based model (\approach) for the generation of Dockerfiles.
We first define a structured \nlRecipeName (\nlRecipe) for Dockerfiles. Then, we extract \nlRecipes from existing Dockerfiles through an automated approach.
Finally, we use such a dataset to train the \approach model for the generation of Dockerfiles (\nlRecipe $\rightarrow$ Dockerfile).
In the following, we describe in detail the steps to construct our model.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Dockerfile High-Level Specification}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Natural language can be used as an effective tool for reporting the requirements of the source code. When it comes to Dockerfiles, however, the high-level requirements that can be expressed are much more limited. For the source code, a developer might want to specify, for example, constraints on the input parameters and conditions that lead to errors. On the other hand, for Dockerfiles, it boils down to a matter of what the developer wants installed in the container, plus a few more characteristics.

Thus, to standardize the format of a Dockerfile requirements specification, written in natural language, the idea is to define a set of key-value requirements.
In a real-world application, it could be seen as a structured form where, for each field, the developer specifies the values to meet the requirements.
Based on the commands available in Dockerfiles and on how Dockerfiles are generally structured (based on our experience), we distilled a structured format for \nlRecipeNames (\nlRecipes), which we report in \tabref{tab:nlrecipes}.

We did not include in the specification requirements related to particular Dockerfile commands (\ie \texttt{ADD}, \texttt{COPY}, \texttt{HEALTHCHECK}, \texttt{MAINTAINER}, \texttt{ONBUILD}, \texttt{SHELL}, \texttt{STOPSIGNAL}, \texttt{USER}, \texttt{VOLUME}, \texttt{WORKDIR}) for three reasons: First, they are related more to low-level details (\eg how the user of the Dockerfile should be set up); Second, in general, developers do not frequently use all of them \cite{9cito2017empirical,eng2021revisiting}; Third, some of them are deprecated (\ie \texttt{MAINTAINER}). Ideally, our approach automatically generates such low-level instructions when needed, \ie based on the instruction needed by the application that must be containerized.
Also, developers might still easily tune them up on the generated Dockerfile, if they want to.

To validate the specification structure, we ran a survey in which we asked 12 professional software developers with at least 2 years of experience with Docker whether they would want to specify each of the fields we hypothesized to be relevant in a \nlRecipe (binary yes/no question). The respondents come from personal invitation and social media, with a $\sim$40\% response rate. We report the results in \tabref{tab:nlrecipes}. All the fields are important to at least 50\% of the participants. The least relevant field, according to the participants, is \fieldUsesLabel (50\% positive answers), while the most important ones are \fieldReq and \fieldUsesEnv (83\% positive answers).

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Inferring Requirements from Existing Dockerfiles}
\label{sec:nlparser}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
We need a large amount of associations \nlRecipe $\rightarrow$ Dockerfile, to train a DL-based Dockerfile generation model. While Dockerfiles are largely available, this does not hold for the requirements behind them. The latter could be manually inferred, but such a process would be infeasible for a large-scale dataset. Thus, we defined an automated procedure for inferring the \nlRecipe behind an existing Dockerfile.
% Indeed, as shown in previous studies \cite{azuma2022empirical}, developers tend to write an explanation comment for the instructions that they write in Dockerfiles. 
In summary, the process works as follows. Given a list of operating systems (\wordlistOs) and stop words (\wordlistStop), we extract the operating system (step 1), the software dependencies (step 2), and, finally, the other fields required for \nlRecipe (step 3).
In the following, we describe in detail each step of our methodology, and the procedure we used to define the two previously-mentioned lists of words.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Step 1: Inferring the OS} 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
To infer the OS required by the developer, we only focus on the \texttt{FROM} instruction, which comes in the format \texttt{FROM <name>[:<tag>]}.
% We extract from it the information about field \fieldOs and, in some cases, the field \fieldReq using the wordlists described in the preliminary step.
Since \texttt{<name>} and \texttt{<tag>} are usually composed by one or more words, we extract them by splitting their content of image name and tag by the typically employed as separators, \ie \texttt{-} and \texttt{\_}. 
We first check if any word in the \wordlistOs is present in the words extracted from the tag, and then in the name. If a OS-related keyword is present, we set \fieldOs with such a keyword, while we use the special keyword ``any'' otherwise, indicating that the developer did not have any requirement in terms of operating system. If more than a OS-related keyword is present, we consider the first match found. For example, for the instruction \texttt{FROM tomcat:9.0.20-jre8-alpine}, we extract the keywords (in the order): ``9.0.20'', ``jre8'', ``alpine'' (from the tag), and ``tomcat'' (from the image name). ``alpine'' is the first (and only, in this example) OS-related word. Therefore, we set \fieldOs to ``alpine''.
If the first OS-related keyword is found in the image name, we also add to it all the keywords containing only numbers from the tag name, which most likely refer to the version number. For example, for the instruction \texttt{FROM debian:10-slim}, we set \fieldOs to ``debian10''.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Step 2: Inferring Software Dependencies}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Software dependencies might be present in several Dockerfile instructions. The first instruction which might contain dependencies is the \texttt{FROM} instruction. To detect the (possible) dependencies explicitly expressed in such a field, we extract all the words from the image name (with the same procedure described in step 1), we remove the words in \wordlistOs (OSes) and \wordlistStop (stop words), and we exclude the non-alphabetic words. 
% \eject

All the remaining words are added to the list of software dependencies (\fieldReq). In the previous example, \texttt{FROM tomcat:9.0.20-jre8-alpine}), the dependency extracted (and only word used as image name) is ``tomcat''.
At this point, we need to extract all the other dependencies installed with package managers or other procedures (\eg downloaded and installed). This task is far from trivial. Simply considering the packages installed through package managers (\eg \texttt{apt install}) is not an option: Most of the packages installed do not correspond to high-level requirements, but rather to low-level details about support libraries. 

For example, in \figref{fig:intro:example}, the package \texttt{build-essential} is not a dependency of the software system, but rather a package incidentally needed (in this case, for building two actual dependencies, \ie \texttt{x265} and \texttt{ffmpeg}). We want our DL model to automatically infer the need of such packages.

However, it is known that developers tend to give an explanation comment of what each Dockerfile instruction does. Thus, we can use those comments to extract only high-level requirements, which are reported by the developers themselves.
To achieve this, we use the following heuristic. We first select all the comment lines that contain the word ``install''. We tokenize each comment with the \textit{spacy} Python library \cite{web:spacy}, and we detect the words that depend on such a keyword. We discard from the obtained list all the words in \wordlistStop, and we select the remaining ones as candidate dependencies for the specific comment line.
For each comment line $c_i$ with its candidate list of dependencies $d_{c_i}$, we process each \texttt{RUN} instruction between $c$ and the next comment line ($c_{i+1}$) or blank line. 
We use \textit{bashlex} \cite{web:bashlex} for parsing the bash script in each RUN instruction, and we split it in statements. Finally, we consider only the statements in which the most common package managers (\ie \texttt{apt}, \texttt{yum}, \texttt{apk}, \texttt{pip}, and \texttt{npm}) and commands for downloading files (\ie \texttt{curl} and \texttt{wget}) are present, and we extract their arguments. If there any argument (packages) matches a candidate requirement (from the comment), we add the candidate requirement in \fieldReq. Note that we do not simply consider the words depending on the ``install'' word from the comments because some words might not be software dependencies (\eg in ``install only ruby,'' the word ``only'' should be ignored).
Finally, we exclude duplicates, and we set the \fieldReq with the dependencies extracted both from the \texttt{FROM} instruction and from the \texttt{RUN} instructions.
% \eject

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Step 3: Parsing Additional Fields}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
All the remaining fields are straightforward to be set. For the fields \fieldUsesArg, \fieldUsesCmd, \fieldUsesEntrypoint, \fieldUsesEnv, \fieldUsesExpose, and \fieldUsesLabel, we simply check if at least one of the respective instructions is present in the Dockerfile.
The field \fieldPkgMan is defined by checking if any of the most commonly used package managers for Linux distributions (\ie \texttt{apt}, \texttt{apk}, and \texttt{yum}) are used in the \texttt{RUN} instructions. If this is the case, we set \fieldPkgMan to such a package manager (\eg \fieldPkgMan = \texttt{yum}). We also check for the coherence between the package manager and the OS (Linux distribution) detected in step 1: For example, if the OS is \texttt{ubuntu}, the package manager can not be \texttt{yum}. If this happens, or if no specific package manager is detected (\eg no package is installed), we set \fieldPkgMan to \texttt{any}.
Finally, for the field \fieldDownExtPkgs, we check if any \texttt{RUN} instruction contains one of the following: (i) a link in the context of a download-related instruction (\eg \texttt{wget} or \texttt{git clone}); (ii) the installation of a Python or JavaScript external library; (iii) the installation of an external package through the package managers (\eg \texttt{dpkg} for Debian/Ubuntu). If this happens, we set \fieldDownExtPkgs to \texttt{true} (\texttt{false} otherwise).

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Defining OSes and Stop Words}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
We use a systematic procedure for defining the two lists required by the \nlRecipe parser, \ie \wordlistOs and \wordlistStop.
We extracted all the \texttt{FROM} instructions contained in the Dockerfiles from the collection by Eng \etal \cite{eng2021revisiting}, that we later use to build our dataset. In total, we obtained \stepZeroFROMs instructions.
Then, we extract keywords from the image name and tag, like we do in step 1.
Next, we discard the words that (i) contain only non-alphabetic characters (\eg version numbers), or (ii) have less than 3 characters (most likely stop words).
We obtain a list of \stepZeroUniqueWords unique words, along with the respective count of occurrences. We filter out all the words with less than 150 occurrences, thus obtaining \stepZeroSelectedWords words, covering 97\% of the total occurrences of all the words extracted.
We manually analyzed and labeled each of them as ``OS'', ``stop word'', or ``dependency''. For example, ``centos'' is marked as ``OS'' keyword, ``baseimage'' as ``stop word'', while ``python'' as ``dependency''. 
In the end, we obtained \stepZeroStopwords stop words for \wordlistStop and \stepZeroOS words referring to OSs for \wordlistOs.
% \eject

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Defining a Dataset of \nlRecipes and Dockerfiles}
\label{sec:nldataset}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
We proceed with using our parser to build the dataset of \nlRecipes and target Dockerfiles.
We use the dataset built by Eng \etal \cite{eng2021revisiting}, which is the largest (9.4M) and the latest dataset of Dockerfiles currently available in the literature. That dataset comes from the S version of World of Code (WoC) \cite{ma2019world}, covering a period of time ranging between 2013 and 2020. To the best of our knowledge, this is the most recent and large collection of Dockerfiles available in the literature. The dataset provides all the versions (snapshots) of all the Dockerfiles extracted from GitHub projects, along with the related commit ID. For building our dataset, we only select the latest commit for each repository.
As a result, we obtain 3,010,141 Dockerfiles.

We filter out all the Dockerfiles that have no comments (required in step 2 of the parser), and also those that have more than one stage, as they are currently not supported by our parser.
Next, we drop all the duplicated Dockerfiles (according to their \textit{sha1} hash) and all the Dockerfiles that contain in the \texttt{FROM} instruction at least a keyword that we did not manually evaluate when defining the two lists \wordlistOs and \wordlistStop, to ensure that we do not unintentionally include stop words as dependencies.
After applying these filters, we obtain a total of \dockerfileParsed Dockerfiles, on which we run our approach to extract the \nlRecipes.

At this stage, we further exclude all the Dockerfiles for which our parser detected syntax errors in bash instructions and empty Dockerfile instructions (\eg \texttt{COPY} without arguments).
As a result, we obtain a total of \datasetInstancesAll pairs of Dockerfile, associated with the respective \nlRecipes (in total, we found 121,030 unique \nlRecipes). At this stage, each \nlRecipe is associated with one or more Dockerfiles since different Dockerfiles might result from the same requirements. We need, however, to select a single representative Dockerfile for each \nlRecipe that we can use for the training and the test of \approach. To do this, we first select all \nlRecipe associated with two or more Dockerfiles. We found 41,820 of them. 

Given a set of Dockerfiles associated with the same \nlRecipe, we select the one containing the highest number of ``typical'' instructions for that cluster, to obtain the most typical Dockerfile for the given \nlRecipe. We tokenize each instruction of all the Dockerfiles. Then, given two Dockerfiles $A$ and $B$, we compute the Jaccard similarity between each pair of instructions of the same kind (\eg \texttt{COPY} instructions are compared only with \texttt{COPY} instructions), with the formula $J(W_{A_i}, W_{B_j}) = \frac{|W_{A_i} \cap W_{B_j}|}{|W_{A_i} \cup W_{B_j}|}$, where $W_{A_i}$ and $W_{B_j}$ are the words from instructions $i$ and $j$ of $A$ and $B$, respectively. We choose as representative the Dockerfile with the highest mean similarity over all the instructions.


We further process the Dockerfiles to prepare them for training: First, given all the package installation instructions (\eg \texttt{apt install}), we sort the packages in lexicographic order, to avoid that different package orders confuse the DL approach. Second, we remove all the lines that contain only comments, to avoid that the model makes extra efforts in solving a sub-problem which is not in the scope of this paper.

From this, we extract two sub-datasets: \datasetPretraining for the pre-training, and \datasetFinetuning for the fine-tuning of the T5 model.
As for the former, we use all the Dockerfiles discarded while choosing the representative Dockerfiles for each \nlRecipe and all the Dockerfiles that are associated with a \nlRecipe for which the field \fieldReq is empty (no software dependency needs to be installed). It is most likely that such Dockerfiles do have dependencies, but we were not able to find them because of the lack of comments in the format we expected (\eg the word ``install'' has not been used in comments).
All the remaining pairs $\langle$\nlRecipe, Dockerfile$\rangle$ are placed in \datasetFinetuning.
In the end, we obtain 557,540 instances for \datasetPretraining and 113,442 instances for \datasetFinetuning. As a requirement for the training of T5, we can only use Dockerfiles having no more than 1024 token, obtaining a total of 113,131 instances.
Then, we divide the \datasetFinetuning in training- (\datasetFinetuningTrain), evaluation- (\datasetFinetuningEval), and test- (\datasetFinetuningTest) sets, by performing a typical 80\%-10\%-10\% spliting \cite{mastropaolo2021studying, mastropaolo2022tse}, obtaining 90,504, 11,313, and 11,314 instances, respectively. We use \datasetFinetuningTrain for fine-tuning \approach, \datasetFinetuningEval for the hyper-parameter tuning, and \datasetFinetuningTest for our experiment (see \secref{sec:design}).
%\SIMONE{There is something wrong here: I count 113,131 instances used for train-, eval-, and test- sets. Why is the total of \datasetPretraining 113,442?}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Training T5 for Generating Dockerfiles}
\label{sec:model}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Raffel \etal \cite{raffel2020exploring} introduced \approach to support multitask learning in Natural Language Processing. Such a model re-frames NLP tasks in a unified text-to-text format in which the input and output of all tasks are always text strings. A T5 model is trained in two phases: (i) \textit{pre-training}, in which the model is trained with a self-supervised objective that allows defining a shared knowledge-base useful for a large class of tasks, and \textit{fine-tuning}, which specializes the model on a downstream task (\eg language translation).
T5 already showed its effectiveness in code-related tasks \cite{mastropaolo2021studying, mastropaolo2021empirical, mastropaolo2022lance, tufano2022using, li2022auger, zhang2022coditt5, ciniselli2021empirical}. However, its application to the generation of Dockerfiles is novel and still unexplored.
As done in previous work \cite{mastropaolo2021studying, mastropaolo2021empirical}, we use the smallest T5 version available (T5 small), which is composed of 60M parameters.
\eject

Given a prediction provided by the model, the output token streams can be generated using several decoding strategies. We use \textit{greedy decoding} when generating an output sequence. In detail, such a strategy selects, at each time step $t$, the symbol having the highest probability of appearing in a specific position. 
We describe below both the pre-training and the fine-tuning procedure we applied for this task.

\subsubsection{Pre-Training Procedure}
\label{sub:pretraining}
The ``general knowledge'' \cite{raffel2020exploring} that we want to provide our model with is, in our case, a mixture of technical natural language (English) and technical language (Dockerfiles). We experiment with three pre-training variations: \tfivemodel{NL}, which only relies on natural language, \tfivemodel{DF}, which only relies on Dockerfiles, and \tfivemodel{NL+DL}, which relies on both. We test all such three variants and we pick the best after performing hyper-parameter tuning.

As for the first variant, \tfivemodel{NL}, we use the pre-trained checkpoint \cite{t5-checkpoint} released by Raffel \etal \cite{raffel2020exploring}. We do not perform any further pre-training for such a model. 

Instead, we leverage the knowledge that has been already gained when pre-training the T5 model on the English text (C4 corpus \cite{raffel2020exploring}) for 1M steps.
% \eject

As for the second variant, \tfivemodel{DL}, we adopt a classic \emph{masked language model} task, \ie we randomly mask 15\% of the tokens in a training instance, asking the model to predict them. We pre-train such a model on \datasetPretraining. 
Finally, as for the third variant, \tfivemodel{NL+DF}, we start from the \tfivemodel{NL} model and we further pre-train it for 500k steps on \datasetPretraining, using the same procedure used for pre-training \tfivemodel{DF}.
Finally, we created a new \emph{SentencePiece} model \cite{kudo2018sentencepiece} for tokenizing natural language text. We trained it on \datasetPretraining.
For both the models for which we performed additional pre-training steps (\tfivemodel{DF} and \tfivemodel{NL+DF}), we used a 2x2 TPU topology (8 cores) from Google Colab with a batch size of 16 and a sequence length of 512 tokens for the input and 1,250 for the output. As a learning rate, we use the Inverse Square Root with the default configuration \cite{raffel2020exploring}. For the pre-training phase, we use the default parameters defined for the T5 model \cite{raffel2020exploring}.  

\subsubsection{Hyper-parameter Tuning} 
We test four learning rate strategies, \ie constant learning rate (C-LR), slanted triangular learning rate (ST-LR), inverse square learning rate (ISQ-LR), and polynomial learning rate (PD-LR). We report in \tabref{tab:learning-rates} the parameters we use for each of them. 
% The only difference with respect to Mastropaolo \etal \cite{mastropaolo2021studying} is in the PD-LR strategy, for which we found a better stability of the network when changing the upper bound from $\mathit{LR}_{\mathit{end}} = 1\mathrm{e}{-06}$  to $\mathit{LR}_{\mathit{end}} = 0.001$

Given the three pre-trained models, \tfivemodel{NL}, \tfivemodel{DF}, and \tfivemodel{NL+DF}, we fine-tune them on \datasetFinetuningEval (100k steps, batch size of 32, input sequence length of 512 tokens, output sequence length of 1024 tokens), leading to 12 different models (3 models $\times$ 4 strategies). Then, to assess their performance, we compute the BLEU-4 \cite{papineni2002bleu} metric between the generated Dockerfiles and the target ones. Such a metric has been used in previous work for other coding tasks \cite{liu2018neural,leclair2019neural,mastropaolo2021studying, mastropaolo2022tse} and it ranges between 0 (completely different) and 1 (identical).
We report in \tabref{tab:hp-results} the results achieved by the 12 models in terms of BLEU-4.
% For instance, \tfivemodel{NL} performs at its best using a PD-LR scheduler. In contrast, \tfivemodel{DF} and \tfivemodel{NL+DF} achieve the best results with the ISQ-LR and the ST-LR, respectively. 
The best results are achieved with \tfivemodel{DF} with the ISQ-LR strategy and \tfivemodel{NL+DF} with the ST-RL strategy (17.20\% BLEU-4 for both). 

In the end, we select the latter since \tfivemodel{NL+DF} achieves better results also for the other strategies. 
% Such results confirm the findings by Tufano \etal \cite{tufano2022generating}\SIMONE{Which results?}.
\begin{table}
	\centering
	\caption{Configurations for the experimented learning rates}
	\label{tab:learning-rates}
% 	\resizebox{\linewidth}{!}{
	\begin{tabular}{ll|ll}   
	  \toprule
	  \textbf{Strategy}    & \textbf{Parameters}                   & \textbf{Strategy}    & \textbf{Parameters}     \\
	  \midrule                                                     
	  C-LR            & \textit{LR = 0.001}                        & ISQ-LR  & \textit{LR$_s$ = 0.01}   \\
	  ST-LR            & \textit{LR$_s$ = 0.001}                   &                      & \textit{W = 10,000}      \\
						  & \textit{LR$_{max}$ = 0.01}             & PD-LR     & \textit{LR$_s$ = 0.01}   \\
						  & \textit{Ratio = 32}                    &                      & \textit{LR$_e$ = 0.001}  \\
						  & \textit{Cut = 0.1}                     &                      & \textit{Pow = 0.5}     \\  
	  \bottomrule
	\end{tabular}
% 	}
\end{table}



\subsubsection{Fine-tuning}
We fine-tune the best pre-trained model (\tfivemodel{NL+DF}) with the best learning rate strategy (ST-LR) on \datasetFinetuningTrain. We use early stopping to avoid overfitting \cite{yao2007early, tufano2022using}: We save a checkpoint every 10k steps and compute the BLEU-4 score on the evaluation set every 100k steps. When the 100k steps do not lead to an improvement, we stop the training procedure, and we keep the last model. 
% \cite{replication}.

% \eject
\begin{table}
	\centering
	\caption{T5 hyper-parameter tuning results (BLEU-4).}
	\begin{tabular}{lrrrr}
		\toprule
		\textbf{Experiment}                  																		& \textbf{C-LR}              & \textbf{ST-LR}      & \textbf{ISQ-LR}        & \textbf{PD-LR} \\
		\midrule
		\tfivemodel{NL}                          &   13.80\%                & 13.70\%    		           & 5.50\%           &  \bf 14.50\%         \\
		\tfivemodel{DF}                        &   16.90\%                & 5.50\%    		           & \bf 17.20\%          &  15.90\%         \\
		\tfivemodel{NL+DF}                         &  16.60\%                & \bf 17.20\%    	&	           16.50\%           &  17.10\%         \\
		\midrule
	\end{tabular}
	\label{tab:hp-results}
	\vspace{-0.3cm}
\end{table}
