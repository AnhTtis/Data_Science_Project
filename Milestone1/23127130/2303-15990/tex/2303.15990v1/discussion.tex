\begin{figure}[t]
  \centering

  \begin{subfigure}{0.9\linewidth}
    \centering
\begin{lstlisting}
FROM golang:1.9.4-stretch

RUN apt-get update -y && apt-get upgrade -y 

RUN git clone https://github.com/edenhill/librdkafka.git && \
  cd librdkafka && \
  ./configure --prefix /usr && \
  make && \
  make install && \
  cd ..

WORKDIR /go/src/consumerpg
COPY . .

RUN go-wrapper download
RUN go-wrapper install

CMD ["go-wrapper", "run"]
\end{lstlisting}
    \vspace{-0.3cm}
    \caption{Target Dockerfile}
    \medskip
    \label{fig:target}
    \end{subfigure}
  
    \begin{subfigure}{0.9\linewidth}
        \centering
\begin{lstlisting}
FROM golang:1.9.4-stretch 

RUN apt-get update && apt-get install -y git 

RUN git clone https://github.com/edenhill/librdkafka.git && cd librdkafka && ./configure && make && make install 

RUN go get github.com/confluentinc/confluent-kafka-go 

WORKDIR /go/src/github.com/confluentinc/confluent-kafka-go 

CMD ["go", "run", "main.go
\end{lstlisting}
        \vspace{-0.3cm}
        \caption{Dockerfile generated by \approach}
        \label{fig:incompleteGen}
    \end{subfigure}
    \medskip
    \caption{Example of a generated incomplete Dockerfile}
  \vspace{-0.5cm}
\end{figure}

% \eject

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Discussion}
\label{sec:discussion}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% \begin{figure}[t]
% 	\centering
% 	\caption{Normalized AST edit distance for the different values of the \textit{temperature} parameter for the T5 model. We With \approach we indicate the same model used in \RQ{2}.}
% 	\bigskip
% 	\includegraphics[width=\linewidth]{figs/ast_discussion}
% 	\label{fig:boxplotDiscussion}
% \end{figure}

% \begin{figure}[t]
% 	\centering
% 	\caption{Layers count resulting from the images built in the context of \RQ{3}.}
% 	\bigskip
% 	\includegraphics[width=\linewidth]{figs/layers_count_boxplot}
% 	\label{fig:boxplotLayers}
% \end{figure}

% \begin{figure}[t]
% 	\centering
% 	\caption{Tokens count for the resulting Dockerfiles.}
% 	\bigskip
% 	\includegraphics[width=\linewidth]{figs/tokens_count_boxplot}
% 	\label{fig:boxplotTokens}
% \end{figure}

Generating Dockerfiles from high-level specifications is a challenging task.
In this study, we perform a first attempt to solve this problem using Deep Learning (\approach, specifically).
Considering the overall results, there is no clear evidence that \approach is better than using IR-based techniques, in practice. Given the lower effort in setting-up a IR-based technique (which is trivial in the case of \baseElastic, for example), at a superficial level, we can conclude that, at the moment, this would be the best option for practitioners.

However, we analyzed the results more in-depth to try to understand what went wrong, and why \approach does not work well for this task, despite it is works very well for other coding tasks \cite{mastropaolo2022tse}.
% A first observation we can make is that the Dockerfiles generated by \approach lead to smaller images in terms of layers (\figref{fig:boxplotLayers}) and size (679.96 MB vs. 881.24 for \baseElastic and 729.70 for \baseSentT).
First, we observed that \approach generates Dockerfiles with a much lower number of tokens compared to the two baseline approaches (36.80 vs. 135.70 for \baseElastic and 107.10 for \baseSentT). This explains why \approach works well for smaller Dockerfiles and gradually less well for bigger ones (\RQ{2}), and also why \approach achieves a good percentage of matching layers even if the Dockerfile similarity is low (\RQ{3}).


% For example, in \figref{fig:pPredAst} we report the largest perfect predictions in AST, where we have a size of 58 nodes for \approach, and 152 for \baseElastic.
% This could be due to the fact that the generated Dockerfiles are strictly related to what is defined in the input recipe, while the IR techniques just retrieve existing Dockerfiles.

We manually analyzed some Dockerfiles generated with \approach. We found that, in some instances, the Dockerfiles abruptly interrupt in the middle of the last instruction. While the remainder of the generated Dockerfiles is correct, the last instruction often contains issues. An example is provided in \figref{fig:incompleteGen}, with the target Dockerfile (a) and the one generated by \approach (b). The prediction is remarkably good, until, in the last line, \approach stops the generation at a certain point.
A pattern we observed is that interrupted Dockerfiles do not end with the token we used for indicating the new line (\texttt{<nl>}), while the ones in the training always end with such a token by design. We counted a total of 6,786 instances of such a kind ($\sim$60\% of the cases). In a real-world usage scenario, the developer would need to manually complete such Dockerfiles to make them work.

If we consider only the instances that terminate with the newline token, the results of \approach become better than the two IR baselines in all the aspects we considered: For \RQ{1}, \approach achieves better results than both the baselines for all the fields, except for \fieldPkgMan; for \RQ{2}, the edit distance becomes significantly lower, \ie 0.46, compared to 0.51 of the best baseline for such a sub-sample; For \RQ{3}, we obtain results in line with the previously presented ones.
We tried to address this issue by replacing the greedy decoding strategy with a sampling strategy, with different values for the \textit{temperature} hyper-parameter of the \textit{softmax} function. A high \textit{temperature} allows to increase the chances of picking tokens with lower likelihood, while a lower \textit{temperature} does the exact opposite, so that, when the temperature is close to 0, such a decoding strategy behaves like a greedy decoding strategy. 

% \eject
We tested temperature values between 0.7 and 1.0, with a step of 0.1.
% We report the results of the AST edit distance in \figref{fig:boxplotDiscussion}, where we evaluate different values of \textit{temperature}.
We observe that increasing the \textit{temperature} allows to reduce the number of incomplete Dockerfiles: With a \textit{temperature} of 0.7, we obtain a total of 1,648 incomplete generations (14.6\%) which decrease to 1,283 (11.3\%) with a \textit{temperature} of 1.
% The best results in terms of normalized edit distance (\RQ{2}) are achieved with a \textit{temperature} of 0.7. 
The average number of tokens contained in the Dockerfiles varies between 82 (\textit{temperature} = 1.0) and 84 (\textit{temperature} = 0.7), while with greedy decoding we have 37, on average.
In the end, however, the Dockerfiles generated with this strategy achieve generally worse results in terms of (i) number perfect predictions ($\sim$-28\%, with the best \textit{temperature}, \ie 0.8), (ii) number of perfectly matching images (-$\sim$80\%, best \textit{temperature} = 1.0) and layers (-$\sim$20\%, best \textit{temperature} = 0.9). 

This analysis shows that, while \approach learned how to generate Dockerfiles, to some extent, it does not have enough knowledge to  generate complete Dockerfiles well: It either partially generates good parts of Dockerfiles or generates complete less-good Dockerfiles.
There are two possible explanations for this phenomenon, which also represent open problems for this specific task:
\begin{itemize}
  \item \textbf{A larger dataset needs to be built.} Addressing this problem is only apparently easy. We considered in our study the largest collection of Dockerfiles available in the literature \cite{eng2021revisiting}, which includes \textit{all} the Dockerfiles from open-source projects produced up to 2020. While such a dataset can be updated with the last two years of activities in GitHub, it is  unlikely that the size of our dataset would drastically increase as a result. Indeed, we consider a single Dockerfile for each unique \nlRecipe, \ie we would not have new instances for the \nlRecipes already covered. We rely on comments by the authors to extract some requirements (specifically, the \fieldReq field). Some Dockerfiles, however, do not have explicit indications of such requirements and, therefore, we miss them in the inferred specifications. New and more precise ways of extracting high-level requirements from Dockerfiles are needed. Also, a promising direction would consist in the definition of techniques for data augmentation in this context, \eg by blending existing Dockerfiles to provide uncovered combinations of \nlRecipes.
   
  \item \textbf{A different training stop criterion needs to be defined.} In this study, we used the same procedure previously used for coding tasks \cite{mastropaolo2022tse}, for which the problem of abrupt interruption in the inference has not been observed. It is possible that the stop criterion and the metric (\textit{BLEU}) used are not the right ones in this context. As for the latter, it might be worth exploring different distance measures. While the AST distance is not a viable option for performance reasons, other metrics that do not consider the order in the instructions might be more useful.
\end{itemize}

As a takeaway for practitioners, it may be too early to reliably use these approaches to generate entire Dockerfiles in one shot, without the need to apply minor adjustments. %Thus, even if the results are promising, some steps are still to be taken in that direction.

% \eject
