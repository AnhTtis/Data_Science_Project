\newcommand{\trainingSetInstances}{90,504\xspace}
\newcommand{\testSetInstances}{11,314\xspace}

\newcommand{\instancesRqOneDL}{9,977\xspace}
\newcommand{\instancesRqOneElasticSentT}{11,314\xspace}

\newcommand{\instancesRqTwoDL}{9,963\xspace}
\newcommand{\instancesRqTwoElastic}{11,311\xspace}
\newcommand{\instancesRqTwoSentT}{11,129\xspace}

\newcommand{\testSetInstancesRqThree}{4,059\xspace}
\newcommand{\testSetReposRqThree}{3,909\xspace}
\newcommand{\testSetSampleRqThree}{500\xspace}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Empirical Study Design}
\label{sec:design}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The \textit{goal} of our study is to understand to what extent \approach is effective in generating Dockerfiles.
Our study is steered from the following research questions:
\begin{itemize}[itemindent=0.2cm]
  \item[\RQ{1}:] \textit{To what extent is \approach able to generate Dockerfiles meeting the input natural language specification?}    We evaluate the effectiveness of \approach in generating Dockerfiles that meet the requirements.

  \item[\RQ{2}:] \textit{To what extent are the Dockerfiles generated by \approach similar to the original ones written by developers?}    With this second RQ, we aim at understanding if \approach generates Dockerfiles similar to the targets ones.

  \item[\RQ{3}:] \textit{To what extent are the Docker Images built from the Dockerfiles generated by \approach similar to the original ones built form the Dockerfiles written by developers?} 
  Two Docker images can be equal and come from completely different Dockerfiles. Therefore, with this last RQ, we try to understand whether the images built from the generated Dockerfiles are similar to the original ones.
\end{itemize}

\subsection{Baseline Techniques}
We use as baseline techniques two Information Retrieval (IR)-based approaches.
The first one is \baseElastic, and it is based on the state-of-the-practice for implementing IR approaches and search engines, \ie Elasticsearch \cite{web:elastic}. Given a collection of documents ($D$) and a query ($q$), Elasticsearch first assigns a score to each document in $D$ according to $q$ and then it sorts them. The score is computed with Okapi BM25 \cite{web:bm25}. We add into an Elasticsearch instance all the instances in \datasetFinetuningTrain. Specifically, for each instance, we define a document that contains both the \nlRecipe and the associated Dockerfile. Given a new \nlRecipe for which we want to get a candidate Dockerfile, we perform a \textit{boolean query} composed of all the fields of the \nlRecipe in OR clause (\ie \texttt{should}, in Elasticsearch). 
%An example of the input query is reported in \figref{fig:elasticQuery}.
We report an example of the query in our replication package \cite{replication}.

The second baseline we consider is \baseSentT, and it is based on the \textit{SentenceTransformers} framework \cite{reimers2019sentencebert}. Such a framework allows to train embeddings for several data types (including text) so that they can be represented as numeric vectors. First, we use \datasetFinetuningTrain to train the model for computing the embeddings (\textit{bert-base-uncased}). Then, we compute the embeddings for each \nlRecipe as $e(d_{\mathit{Spec}})$ for each \nlRecipe in \datasetFinetuningTrain, and we store their associations with the respective Dockerfiles ($e(d_{\mathit{Spec}}) \rightarrow d_{\mathit{Dockerfile}}$). Given a new \nlRecipe, $t_{\mathit{Spec}}$, we compute its embeddings ($e(t_{\mathit{Spec}})$) and, then, the cosine similarity between $e(t_{\mathit{Spec}})$ and each $e(d_{\mathit{Spec}})$. Finally, we return the $d_{Dockerfile}$ for which the aforementioned similarity is maximum.
% \eject

\subsection{Context Selection}
The \textit{context} of our study is composed of (i) a set of associations \nlRecipe $\rightarrow$ Dockerfile, (ii) the Dockerfiles generated/retrieved by \approach and the two baseline techniques, and (iii) the source code of the software projects for which the Dockerfiles need to be built, for building the images and thus answering \RQ{3}.

As for the first object, we used the \datasetFinetuningTest dataset.
To obtain the second object, we ran \approach, \baseElastic, and \baseSentT on the \nlRecipes from \datasetFinetuningTest. As a result, we obtained three sets of generated/retrieved Dockerfiles, \ie DF$_{T5}$, DF$_{ES}$, DF$_{ST}$.
Finally, to obtain the third object, for each Dockerfile in \datasetFinetuningTest, we consider the original entry in the dataset by Eng \etal \cite{eng2021revisiting} and we recover the project from which it was extracted and the commit for that specific snapshot.
We cloned all the repositories corresponding to the test instances, discarding the ones for which the source commit or repository was no longer available.
As a result, we obtained a total of \testSetReposRqThree repositories, corresponding to \testSetInstancesRqThree instances of \datasetFinetuningTest. Since building Dockerfiles requires a large amount of time, we did this for a representative sample of \testSetSampleRqThree Dockerfiles from \datasetFinetuningTest (4.28\% margin of error, 95\% confidence level). For each instance in \datasetFinetuningTest, we tried to build the original Dockerfile: If the build failed, we discarded the instance, while, if it succeeded, we kept it, until we collected \testSetSampleRqThree instances. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Experimental Procedure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
To answer \textbf{\RQ{1}}, we compare the \nlRecipe related to the Dockerfiles returned by the approaches we consider with the one given as input.
As for the two baselines, we already have an associated \nlRecipe for each returned Dockerfile (\ie the one from \datasetFinetuningTrain). This, however, is not true for \approach since it generates Dockerfiles from scratch. In this case, we use the same process described in \secref{sec:nlparser} on the generated Dockerfiles for all the fields, except for \fieldReq. We check if requirements in the input \nlRecipe are met in the generated Dockerfile. The reason is that we trained \approach not to generate comments: Since our procedure for extracting \fieldReq strongly relies on comments, we can not directly use it on the generated Dockerfiles. 
We ignore the Dockerfiles generated by \approach for which the parser we defined is not able to infer all the fields in the \nlRecipe (1,337 instances, \ie $\sim$12\% of the total). We consider, instead, all of them for the two baselines, for which this cannot happen since, as mentioned, we do not infer the \nlRecipe.
Finally, we measure the similarity between the target \nlRecipes and the obtained ones.
To achieve this, we assign a \textit{score} for each field of the \nlRecipes, compared to the respective instance in the target \nlRecipe, which is computed by assigning 1 point if the field is equal, and 0 otherwise.
The only exception is the \fieldReq field: Since this is a collection of elements, in this case we compute the score by computing the percentage of elements in the target \nlRecipe that are present also in the obtained one (\ie the recall).
We compute and report the mean score obtained for each field of the \nlRecipes. 

% \eject

To answer \textbf{\RQ{2}}, we compare the sets DF$_{T5}$, DF$_{ES}$, DF$_{ST}$ with the respective target Dockerfiles. Simply computing textual similarity for Dockerfiles is not sufficient since, in many cases, instructions can be swapped without affecting the final result. Therefore, we rely on the AST representation of the Dockerfiles. To do this, we use the \textit{binnacle} tool by Henkel \etal \cite{19henkel2020learning}. Binnacle extracts ASTs at three different abstraction levels. We use the \textit{phase-2} representation. We do not use the \textit{phase-3} abstraction level since it abstracts the bash commands. For example, both the \texttt{apt-get install} and \texttt{pacman -S} instructions get replaced with a generic \textit{package install}. While such an abstraction is useful, the tool does not support all the possible bash commands, thus causing loss of information, in some cases. 
%In \figref{fig:ast} there is an example of a parsed AST.
We report an example of a parsed AST in our replication package \cite{replication}.
Given the ASTs of two Dockerfiles, we compute the \textit{edit distance} between them, \ie the number of modifications needed to transform one into the other, using the Zhang-Shasha algorithm \cite{zhang1989simple}. We normalize the obtained edit distance by dividing it by the sum of the sizes of the two trees we are comparing.
We discard all the instances for which the \textit{binnacle} tool is not able to extract the AST. We obtain \instancesRqTwoDL for \approach, \instancesRqTwoElastic for \baseElastic, and \instancesRqTwoSentT for \baseSentT.
We run the Mannâ€“Whitney U test \cite{wilcoxon1945individual} to compare \approach with the baselines in terms of normalized edit distance. The null hypothesis is that there is no difference between the edit distance obtained using \approach and the one obtained using an IR-based approach. We correct for multiple comparisons using the Benjamini-Hochberg procedure \cite{benjamini1995controlling}.
Finally, we compute the effect size using Cliff's Delta \cite{cliff1993dominance}, which is negligible for all of the performed comparison (\ie \approach compared with the two baselines).
This means that their difference is negligible, even if it is statistically significant.

To answer \textbf{\RQ{3}}, we compare the Docker images built from the resulting Dockerfiles provided by the three techniques and the one built from the target Dockerfile. We consider the GitHub projects we cloned for the sample of \testSetReposRqThree instances and, for each of them, we replace the original Dockerfile with the one generated/retrieved by the three approaches, one at a time, and we try to build it. For each instance, we memorized (i) if the build succeeded, (ii) if the the original and the obtained images are equal, and, if not, (iii) to what extent the latter provides what is also present in the former.
As for the second measurement, we rely on the image digest: We say that two images are equal if their digest are equal. While this is not true by design, we can safely assume that, in our context, the risk of obtaining different images with the same digest is negligible.
As for the third measurement, we compute the digest of each build layer (\ie one for each Dockerfile instruction), and we compute the percentage of layer digests of the image resulting from the original Dockerfile that also appear in the image built from the generated/retrieved Dockerfile. Note that if this measure is 100\%, in this context, it means that the generated/retrieved Dockerfile is able to provide everything that the original image already provided. Still, it is possible that it contains additional layers not present in the original image. 
% \eject

\input{rq1_table}  % force table position

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Data Availability}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
We provide a replication package \cite{replication} containing the tool to extract high-level requirements from Dockerfiles, the trained DL models, along with the datasets and the code for replicating the baseline techniques and the conducted experiment.

