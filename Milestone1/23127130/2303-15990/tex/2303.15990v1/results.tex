%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Empirical Study Results}
\label{sec:results}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{figure}[t]
	\centering
	\includegraphics[width=\linewidth]{figs/rq2_boxplot}
	\caption{Boxplots of the normalized AST edit distance (\RQ{2}).}
	\label{fig:rqTwoBoxplot}
	\vspace{-0.3cm}
\end{figure}

In this section, we report the results of our study and, thus, the answers to our research questions.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{\RQ{1}: Adherence to the High-Level Specification}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

In \tabref{tab:rqOneScore}, we report the results of the comparison with input \nlRecipe fields. 
First, it can be noticed that \baseElastic is the best-performing baseline, since it always achieves better or equal results than \baseSentT. Therefore, from now on, we only discuss the comparison between \approach and such a baseline in this RQ.
\approach is generally able to better meet the requirement in terms of \fieldOs (+7.6pp), while it achieves slightly worse results in terms of \fieldPkgMan (-1.9pp), \fieldReq (-1.2pp), and \fieldDownExtPkgs (-2.6pp). The last two are probably the most critical and hard-to-meet requirements since they also interact with each other, and we can observe that both the approaches generally achieve good results. As for the other requirements, we observe that \approach performs better on \fieldUsesEnv, \fieldUsesArg, and \fieldUsesLabel, while \baseElastic achieves better results in terms of \fieldUsesExpose, \fieldUsesCmd, and \fieldUsesEntrypoint. In summary, we can conclude that (i) there is no clear winner between the two approaches, and (ii) both the approaches generally return Dockerfiles that meet most of the requirements.

In summary, \approach and \baseElastic perform very similarly in terms of adherence to the input requirements: There is no clear winner as for this aspect.

% \eject

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{\RQ{2}: Dockerfile Similarity}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[t]
	\centering
	\includegraphics[width=\linewidth]{figs/rq3_boxplot}
	\caption{Boxplots of the percentage of matching layers (\RQ{3}).}
	\label{fig:rqThreeMatchingLayers}
	\vspace{-0.3cm}
\end{figure}

We report the adjusted boxplots \cite{hubert2008adjusted} for the normalized AST \textit{edit distance} in \figref{fig:rqTwoBoxplot}.
The boxplot shows the distribution of the edit distance between the generated Dockerfile and the original Dockerfiles for each instance of the test set. The higher the distance, the lower the similarity.
Also in this case, it seems that there is no clear winner: \approach has higher variance, thus being able to generate both  better and  worse Dockerfile compared to the two baselines. The mean edit distance is 0.55 ($\sigma$ = 0.19) for \approach, 0.53 ($\sigma$ = 0.18) for \baseElastic, and 0.51 ($\sigma$ = 0.19) for \baseSentT. This means that the two IR-based baselines perform slightly better than \approach. 
The difference is significant according to the Mannâ€“Whitney U tests we performed for comparing \approach with the \baseElastic and \baseSentT (adjusted $p$-value lower than 0.001 for both).
The Cliff's Delta between \approach and \baseElastic is 0.06, and 0.11 between \approach and \baseSentT. Thus, the difference is \textit{negligible} in both cases.
We took a closer look at the cases in which the generated/retrieved Dockerfile was perfectly equal to the original one (\ie edit distance 0). We have 93 of such cases for \approach, while only 18 and 11 for \baseElastic and \baseSentT, respectively. In terms of AST size, the perfect matches are rather small (10.1, $\sigma$ = 9.4, with a maximum of 58) compared to the average size (133.7, $\sigma$ = 132.5) for \approach, while it is remarkably higher for the two baselines (39.5, $\sigma$ = 38.2 for \baseElastic, and 41.8, $\sigma$ = 44.8 for \baseSentT). Such a result suggests that \approach works well when small Dockerfile need to be generated, while it struggles with bigger ones. To confirm such a conjecture, we computed the correlation (Spearman $\rho$) between the AST size of the target Dockerfile for \approach, and we found that it is significant and high ($\rho$ = 0.67), much more than the two baselines ($\rho$ = -0.12 for \baseElastic, and not significantly different from 0 for \baseSentT).

In summary, all the approaches returns Dockerfiles quite different from the target ones. \approach works well with small Dockerfiles, but not with bigger ones.

% \eject

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{\RQ{3}: Docker Images Similarity}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


% In \tabref{tab:rqThreeMetrics}, we report the results of the image build performed for \approach and the baseline approaches. 
\approach achieves a build success rate of 34\% (170/500 correctly built images), outperforming the \baseElastic (23\%, 166/500 images) and \baseSentT (32\%, 156/500 images).
Comparing the \textit{digest} of the built images (\ie hash value) with the source image (\ie the one built from the test instance), we obtain remarkably better results for \approach: We have 11.7\% of matches (20/170 instances), while the two baselines have no matching digest for their images. This result is confirmed also in \figref{fig:rqThreeMatchingLayers}, which depicts the distribution of the percentage of matching layers (adjusted boxplots \cite{hubert2008adjusted}). The mean percentage of matching layers is 32.4\% for \approach ($\sigma$ = 0.32), 17.5\% for \baseElastic ($\sigma$ = 0.26), and 10.9\% for \baseSentT ($\sigma$ = 0.19).
The obtained results complement the ones presented in \RQ{2}. Not only \approach works better when it needs to generate small Dockerfiles, but it also works better than the two baselines on bigger ones, up to a certain point; after that, it is not able to generate good instructions, thus the limited layer match, in absolute terms.

In summary, \approach achieves the best results compared to the two baselines in terms of build success, percentage of perfectly matching images, and percentage of matching layers.