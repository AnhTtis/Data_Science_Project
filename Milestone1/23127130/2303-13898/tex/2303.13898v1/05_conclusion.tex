\section{Conclusion and Limitation}
%\noindent \textbf{Conclusion}. 
This paper proposes a novel data-free incremental learning method inspired by human analogy capabilities. Instead of saving the original old data, we adopt analogical prompts to reminisce about the previously seen classes from the new-task data. On this basis, we can estimate and counteract the representation shift for the old prototypes, which is critical for unbiased classification. Our method achieves state-of-the-art performance on four incremental learning benchmarks, while having a much lower additional storage requirement than data-replay methods.

\noindent \textbf{Limitation}. The performance of our method is weakened when the semantic gap between tasks is too large. \XP{More efforts shall be made before our method can be } applied to scenarios where there are no task boundaries in the training stage, such as task-free continual learning~\cite{aljundi2019task,wang2022improving}, blurred  boundary  continual  learning~\cite{bang2021rainbow,PietroBuzzega2020der+}, and online learning~\cite{aljundi2019gradient}. 

%Our method cannot be directly
%PietroBuzzega2020der