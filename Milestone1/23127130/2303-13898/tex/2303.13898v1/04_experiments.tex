\section{Experiments}
\label{sec:experiments}
As our method is built upon the pre-trained ViT  model, we closely follow the setting proposed by previous state-of-the-art ViT-based methods such as L2P~\cite{wang2022L2P} and DualP~\cite{wang2022DualP}. We mainly focus on the challenging class incremental learning and domain incremental learning settings, where the task ID is not available during inference. 

\subsection{Benchmark and Implementation}
\noindent \textbf{Benchmark Datasets.} 
We conduct experiments on four challenging benchmark datasets: Split CIFAR-100~\cite{AlexKrizhevsky2009cifar}, Split ImageNet-R~\cite{wang2022DualP}, Sequential 5-Datasets~\cite{SaynaEbrahimi20205datasets}, and Core50~\cite{VincenzoLomonaco2017core50}. \textit{Split CIFAR-100} splits the original CIFAR-100~\cite{AlexKrizhevsky2009cifar} into 10 sessions, each containing 10 disjoint classes. \textit{Split ImageNet-R} is a challenging class incremental benchmark. It randomly divides the original ImageNet-R's~\cite{DanHendrycks2020imagenetr} 200 classes into 10 sessions, each with 20 classes. It is usually used for incremental learning with pre-trained models. \textit{Sequential 5-Datasets} incorporates five popular datasets, which are CIFAR-10, MNIST~\cite{lecun1998mnist}, Fashion-MNIST~\cite{xiao2017fashionmnist}, SVHN~\cite{netzer2011svhn}, and notMNIST~\cite{bulatov2011notmnist}. The combined datasets provide greater diversity for the evaluation of incremental learning approaches. \textit{Core50} is a trendy domain incremental dataset that contains 11 distinct domains with 50 classes each. Among them, 8 domains are used in the training phase, and 3 for the testing phase. Following the setup of L2P~\cite{wang2022L2P} and DualP~\cite{wang2022DualP}, we use Split CIFAR-100, Split ImageNet-R, and Sequential 5-Datasets for class incremental learning experiments, and use Core50 for domain incremental learning experiments.


\noindent \textbf{Implementation Details.} 
For a fair comparison, \textit{all methods} start with the same ImageNet pre-trained ViT-B/16~\cite{AlexeyDosovitskiy2020ViT}, with the hyper-parameters from the original setting. 

For the \emph{new task finetuning} stage, we use SGD optimizer with the learning rate = 0.001, the momentum = 0.9, and the batch size = 128. The training epoch is set to 5 for Split CIFAR-100 and Sequential 5-Datasets, 10 for Core50, and 50 for Split ImageNet-R, which strictly follows previous works~\cite{wang2022L2P,wang2022DualP} for a fair comparison. The \emph{trainable} parameters are the MLP in each transformer encoder and the classification header.

For the \emph{analogy making} stage, we use Adam optimizer with the learning rate = 0.001. The training epoch is set to 5 for Split CIFAR-100, ImageNet-R, and Core50, and 10 for Sequential 5-Datasets, according to the level of semantic gap between different tasks. 5-Datasets has the largest semantic gap between tasks among these four benchmarks. The \emph{trainable} parameters are A-prompts. 

For the \emph{hyper-parameters setting}, except for the hyper-parameters study experiments, we set the total prototypes per class $M$ to $6$, with comparable additional parameters to the DualP; We take scaling normalized euclidean distance as the distance function $d(\f_i,\f_j)=20 \cdot \ell_2\left(\frac{ \f_i}{\Vert\f_i\Vert_{2}}, \frac{ \f_j}{\Vert\f_j\Vert_{2}}\right)$, where the scaling factor is set to 20; we set $K$ to 50 in Eq.~\eqref{eq:sample}, and the token length $J$ of each prompt to 5.  

We evaluate our method under both the class incremental learning (CIL) and domain incremental learning (DIL) settings.
In DIL where different domains share the same categories, we directly use A-prompts to convert samples to the identity category in different domains, without the need to select samples by Eq.~\eqref{eq:sample}. The other processes are the same as in CIL. 

\noindent \textbf{Evaluation Metrics.} 
We use the Final Average Accuracy (FAA) and Final Forgetting (FF) as evaluation metrics~\cite{wang2022DualP,wang2022L2P,wang2022SPrompts}, both of which evaluate the model after the last task. FAA is more important than FF, since FAA is related to both the plasticity and stability of the model, while FF is only related to the stability of the model. 


\subsection{Comparison and Analysis}\label{sec:compare}

We compare our method with eleven modern CIL and DIL methods, including: 
1) Prompt-based methods, which achieve the top accuracy in the ViT-based incremental learning, including L2P~\cite{wang2022L2P}, DualP~\cite{wang2022DualP}, and S-Prompts~\cite{wang2022SPrompts}.
2) Data-replay methods, which use a buffer to save representative historical data, including ER~\cite{rolnick2019ER}, BiC~\cite{wu2019bic}, GDumb~\cite{prabhu2020gdumb}, DER++~\cite{PietroBuzzega2020der+}, and Co$^2$L~\cite{HyuntakCha2021co2l}. Since they are sensitive to the buffer size, a medium and a large buffer size are reported as ~\cite{wang2022L2P,wang2022DualP}. 3) Weight-regularization methods including LwF~\cite{li2017LWF} and EWC~\cite{kirkpatrick2017EWC}, which are widely used baseline methods. All these methods are re-implement on ViT for a fair comparison and the results are taken from~\cite{wang2022DualP,wang2022SPrompts,wang2022L2P}
%These latest state-of-the-art methods are re-implement on ViT for fair comparison.

%All results are taken from~\cite{wang2022DualP,wang2022SPrompts} except for our method, our baseline methods, and the upper-bound result. 
We also compare with SDC-SNMP~\cite{LuYu2020SDC}  to illustrate the effectiveness of our main contribution, \emph{i.e.}, the analogical shift counteraction. It uses the same classifier (Eq.~\eqref{eq:class}) and the same finetuning objective function (Eq.~\eqref{eq:ft}) as ours. Moreover, we train a model on the entire training set in a batch manner with the same trainable parameters as ours, which has the highest performance on the same architecture. It is considered as the \emph{upper bound} in the experiment.

The experimental results are shown in Tabs.~\ref{table:cifar} to~\ref{table:core50}. 
%The highlights and the conclusions are listed as follows: 
%improves the best performance
1) Our method \XP{performs best} on Split CIFAR-100, Split Image-R, and Core50, and achieves a performance with a marginal difference to the best one on 5-Datasets. 2) For DIL on Core50, our method achieves a performance very close to the \emph{upper bound}. 3) Our method outperforms all the data-replay methods with much lower memory costs.

\noindent \textbf{Comparison with SDC-SNMP}: our method consistently outperforms SDC-SNMP on all four datasets. It strongly supports the effectiveness and necessity of analogical shift counteraction. It is worth noting that the performance margin between SDC and our method is significant when the semantic gap between different tasks is large and vice versa. For instance, on the most diverse benchmark Sequential 5-Datasets, our method outperforms SDC-SNMP by $7.12\%$. 


% ################################################
% 【split cifar-100】
% ################################################
\begin{table}[] \small
\setlength{\tabcolsep}{5pt}
\begin{center}
\begin{tabular}{lccc}
\hline
Method & Buffer size & FAA ($\uparrow$) & FF ($\downarrow$) \\ 
\hline
ER~\cite{rolnick2019ER} & \multirow{6}{*}{10/class} &
    67.87\scriptsize{$\pm$0.57} & 
    33.33\scriptsize{$\pm$1.28}  
    \\
BiC~\cite{wu2019bic} && 
    66.11\scriptsize{$\pm$1.76} & 
    35.24\scriptsize{$\pm$1.64}  
    \\
GDumb~\cite{prabhu2020gdumb} && 
    67.14\scriptsize{$\pm$0.37} & 
    - \\
DER++~\cite{PietroBuzzega2020der+} && 
    61.06\scriptsize{$\pm$0.87} & 
    39.87\scriptsize{$\pm$0.99}  
    \\
Co$^2$L~\cite{HyuntakCha2021co2l} && 
    72.15\scriptsize{$\pm$1.32} & 
    28.55\scriptsize{$\pm$1.56}  
    \\
L2P~\cite{wang2022L2P} &&
    84.21\scriptsize{$\pm$0.53} &
    7.72\scriptsize{$\pm$0.77}
    \\
\hline
ER~\cite{rolnick2019ER} & \multirow{6}{*}{50/class} &
    82.53\scriptsize{$\pm$0.17} & 
    16.46\scriptsize{$\pm$0.25}  
    \\
BiC~\cite{wu2019bic} && 
    81.42\scriptsize{$\pm$0.85} & 
    17.31\scriptsize{$\pm$1.02}  
    \\
GDumb~\cite{prabhu2020gdumb} && 
    81.67\scriptsize{$\pm$0.02} & 
    - \\
DER++~\cite{PietroBuzzega2020der+} && 
    83.94\scriptsize{$\pm$0.34} & 
    14.55\scriptsize{$\pm$0.73}  
    \\
Co$^2$L~\cite{HyuntakCha2021co2l} && 
    82.49\scriptsize{$\pm$0.89} & 
    17.48\scriptsize{$\pm$1.80}  
    \\ 
L2P~\cite{wang2022L2P} &&
    86.31\scriptsize{$\pm$0.59} &
    5.83\scriptsize{$\pm$0.61}
    \\
\hline
FT-seq & \multirow{6}{*}{0} & 
    33.61\scriptsize{$\pm$0.85} & 
    86.87\scriptsize{$\pm$0.20}  
    \\
EWC~\cite{kirkpatrick2017EWC} && 
    47.01\scriptsize{$\pm$0.29} & 
    33.27\scriptsize{$\pm$1.17}  
    \\
LwF~\cite{li2017LWF} && 
    60.69\scriptsize{$\pm$0.63} &
    27.77\scriptsize{$\pm$2.17}
    \\
L2P~\cite{wang2022L2P} && 
    83.86\scriptsize{$\pm$0.28} & 
    7.35\scriptsize{$\pm$0.38}  
    \\
DualP~\cite{wang2022DualP} && 
    86.51\scriptsize{$\pm$0.33} & 
    5.16\scriptsize{$\pm$0.09} 
    \\ 
\cdashline{1-4}[0.8pt/2pt]
% PT-SNMP & \multirow{4}{*}{0} & 
%     75.37\scriptsize{$\pm$0.00} & 
%     7.43\scriptsize{$\pm$0.00} 
%     \\ 
% FTF-SNMP && 
%     84.78\scriptsize{$\pm$0.00} & 
%     4.64\scriptsize{$\pm$0.00} 
%     \\ 
SDC-SNMP &\multirow{2}{*}{0}& 
    82.31\scriptsize{$\pm$0.57} & 
    14.44\scriptsize{$\pm$0.68} 
    \\
Ours && 
    \bf{87.87\scriptsize{$\pm$0.24}} & 
    \bf{2.78\scriptsize{$\pm$0.07}}  
    \\
% Baseline4 && 
%     73.13\scriptsize{$\pm$0.00} & 
%     10.59\scriptsize{$\pm$0.00} 
%     \\ 
\hline
\hline
Upper bound & - & 
    91.98\scriptsize{$\pm$0.07} &
    - \\ 
    \hline
\end{tabular}
\end{center}
\caption{Result on Split CIFAR-100 for CIL. %Memory-based methods get results at buffer sizes of 50/class and 10/class. 
Buffer size 0 means no buffer required.
%\textbf{Bold}: the best data-free result.
}
\label{table:cifar}
\end{table}

% ################################################
% 【split ImageNet-R】
% ################################################

\begin{table}[] \small
\setlength{\tabcolsep}{5pt}
\begin{center}
\begin{tabular}{lccc}
\hline
Method & Buffer size & FAA ($\uparrow$) & FF ($\downarrow$) \\ 
\hline
ER~\cite{rolnick2019ER} & \multirow{6}{*}{10/class} &
    55.13\scriptsize{$\pm$1.29} & 
    35.38\scriptsize{$\pm$0.52}  
    \\
BiC~\cite{wu2019bic} && 
    52.14\scriptsize{$\pm$1.08} & 
    36.70\scriptsize{$\pm$1.05}  
    \\
GDumb~\cite{prabhu2020gdumb} && 
    38.32\scriptsize{$\pm$0.55} & 
    - \\
DER++~\cite{PietroBuzzega2020der+} && 
    55.47\scriptsize{$\pm$1.31} & 
    34.64\scriptsize{$\pm$1.50}  
    \\
Co$^2$L~\cite{HyuntakCha2021co2l} && 
    53.45\scriptsize{$\pm$1.55} & 
    37.30\scriptsize{$\pm$1.81}  
    \\ 
\hline
ER~\cite{rolnick2019ER} & \multirow{6}{*}{50/class} &
    65.18\scriptsize{$\pm$0.40} & 
    23.31\scriptsize{$\pm$0.89}  
    \\
BiC~\cite{wu2019bic} && 
    64.63\scriptsize{$\pm$1.27} & 
    22.25\scriptsize{$\pm$1.73}  
    \\
GDumb~\cite{prabhu2020gdumb} && 
    65.90\scriptsize{$\pm$0.28} & 
    - \\
DER++~\cite{PietroBuzzega2020der+} && 
    66.73\scriptsize{$\pm$0.87} & 
    20.67\scriptsize{$\pm$1.24}  
    \\
Co$^2$L~\cite{HyuntakCha2021co2l} && 
    65.90\scriptsize{$\pm$0.14} & 
    23.36\scriptsize{$\pm$0.71}  
    \\ 
\hline
FT-seq & \multirow{5}{*}{0} & 
    28.87\scriptsize{$\pm$1.36} & 
    63.80\scriptsize{$\pm$1.50}  
    \\
EWC~\cite{kirkpatrick2017EWC} && 
    35.00\scriptsize{$\pm$0.43} & 
    56.16\scriptsize{$\pm$0.88}  
    \\
LwF~\cite{li2017LWF} && 
    38.54\scriptsize{$\pm$1.23} &
    52.37\scriptsize{$\pm$0.64}
    \\
L2P~\cite{wang2022L2P} && 
    61.57\scriptsize{$\pm$0.66} & 
    9.73\scriptsize{$\pm$0.47}  
    \\
DualP~\cite{wang2022DualP} && 
    68.13\scriptsize{$\pm$0.49} & 
    4.68\scriptsize{$\pm$0.20} 
    \\ 
\cdashline{1-4}[0.8pt/2pt]
% PT-SNMP &\multirow{4}{*}{0}&
%     49.73\scriptsize{$\pm$0.00} &
%     6.51\scriptsize{$\pm$0.00}
%     \\
% FTF-SNMP &&
%     66.82\scriptsize{$\pm$0.00} &
%     5.46\scriptsize{$\pm$0.00}
%     \\
SDC-SNMP &\multirow{2}{*}{0}&
    67.90\scriptsize{$\pm$0.49} &
    15.44\scriptsize{$\pm$0.84}
    \\
% Baseline4 &&
%     67.30\scriptsize{$\pm$0.00} &
%     7.35\scriptsize{$\pm$0.00}
%     \\
Ours && 
    \bf{72.82\scriptsize{$\pm$0.30}} & 
    \bf{3.90\scriptsize{$\pm$0.21}}  
    \\
\hline
\hline
Upper bound & - & 
    81.95\scriptsize{$\pm$0.11}  & - 
    \\ 
\hline
\end{tabular}
\end{center}
\caption{Result on Split ImageNet-R for CIL. %Memory-based methods get results at buffer sizes of 50/class and 10/class. 0 represents no buffer required.
%\textbf{Bold}: the best data-free result.
}
\label{table:image}
\end{table}

% ################################################
% 【Sequential 5-Datasets】
% ################################################
\begin{table}[]\small
\setlength{\tabcolsep}{5pt}
\begin{center}
\begin{tabular}{lccc}
\hline
Method & Buffer size & FAA ($\uparrow$) & FF ($\downarrow$) \\ 
\hline
ER~\cite{rolnick2019ER} & \multirow{5}{*}{5/class} & 
    80.32\scriptsize{$\pm$0.55} & 
    15.69\scriptsize{$\pm$0.89}  
    \\
BiC~\cite{wu2019bic} && 
    78.74\scriptsize{$\pm$1.41} & 
    21.15\scriptsize{$\pm$1.00}  
    \\ 
DER++~\cite{PietroBuzzega2020der+} && 
    80.81\scriptsize{$\pm$0.07} & 
    14.38\scriptsize{$\pm$0.35}  
    \\ 
Co$^2$L~\cite{HyuntakCha2021co2l} && 
    82.25\scriptsize{$\pm$1.17} & 
    17.52\scriptsize{$\pm$1.35}  
    \\
\hline
ER~\cite{rolnick2019ER} & \multirow{5}{*}{10/class} &
    84.26\scriptsize{$\pm$0.84} & 
    12.85\scriptsize{$\pm$0.62}  
    \\
BiC~\cite{wu2019bic} && 
    85.53\scriptsize{$\pm$2.06} & 
    10.27\scriptsize{$\pm$1.32}  
    \\
DER++~\cite{PietroBuzzega2020der+} && 
    84.88\scriptsize{$\pm$0.57} & 
    10.46\scriptsize{$\pm$1.02}  
    \\
Co$^2$L~\cite{HyuntakCha2021co2l} && 
    86.05\scriptsize{$\pm$1.03} & 
    12.28\scriptsize{$\pm$1.44}  
    \\ 
\hline
FT-seq & \multirow{5}{*}{0} & 
    20.12\scriptsize{$\pm$0.42} & 
    94.63\scriptsize{$\pm$0.68}  
    \\
EWC~\cite{kirkpatrick2017EWC} && 
    50.93\scriptsize{$\pm$0.09} & 
    34.94\scriptsize{$\pm$0.07}  
    \\
LwF~\cite{li2017LWF} && 
    47.91\scriptsize{$\pm$0.33} & 
    38.01\scriptsize{$\pm$0.28}  
    \\
L2P~\cite{wang2022L2P} && 
    81.14\scriptsize{$\pm$0.93} & 
    4.64\scriptsize{$\pm$0.52} 
    \\
DualP~\cite{wang2022DualP} && 
    \bf{88.08\scriptsize{$\pm$0.36}} & 
    \bf{2.21\scriptsize{$\pm$0.69}} 
    \\
\cdashline{1-4}[0.8pt/2pt]
% PT-SNMP &\multirow{4}{*}{0}& 
%     62.24\scriptsize{$\pm$0.00} & 
%     0.18\scriptsize{$\pm$0.00}   
%     \\ 
% FTF-SNMP && 
%     66.63\scriptsize{$\pm$0.00} & 
%     \bf{0.068\scriptsize{$\pm$0.00}}   
%     \\ 
SDC-SNMP &\multirow{2}{*}{0}& 
    80.90\scriptsize{$\pm$0.87} & 
    22.15\scriptsize{$\pm$0.44}   
    \\ 
% Baseline4 && 
%     71.20\scriptsize{$\pm$0.00} & 
%     35.83\scriptsize{$\pm$0.00}   
    % \\ 
Ours && 
    \bf{88.02\scriptsize{$\pm$0.25}} & 
        5.27\scriptsize{$\pm$0.20}   
    \\ 
\hline
\hline
Upper bound & - & 
    95.87\scriptsize{$\pm$0.00} & 
    - \\ 
\hline
\end{tabular}
\end{center}
\caption{Result on Sequential 5-Datasets for CIL. 
%Memory-based methods get results at buffer sizes of 5/class and 10/class, which is enough for this dataset. 0 represents no buffer required.
%\textbf{Bold}: the best data-free result.
}
\label{table:5dataset}
\end{table}

% ################################################
% 【Core50】
% ################################################
\begin{table}[]\small
\setlength{\tabcolsep}{12pt}
\begin{center}
\begin{tabular}{lccc}
\hline
Method & Buffer size & FAA ($\uparrow$) \\ 
\hline
ER~\cite{rolnick2019ER} & \multirow{6}{*}{50/class} & 
    80.10\scriptsize{$\pm$0.56} 
    \\
GDumb~\cite{prabhu2020gdumb} && 
    74.92\scriptsize{$\pm$0.25} 
    \\
BiC~\cite{wu2019bic} && 
    79.28\scriptsize{$\pm$0.30} 
    \\
DER++~\cite{PietroBuzzega2020der+} && 
    79.70\scriptsize{$\pm$0.44} 
    \\
Co$^2$L~\cite{HyuntakCha2021co2l} && 
    79.75\scriptsize{$\pm$0.84}  
    \\
L2P~\cite{wang2022L2P} && 
    81.07\scriptsize{$\pm$0.13} 
    \\ 
\hline
EWC~\cite{kirkpatrick2017EWC} & \multirow{4}{*}{0} & 
    74.82\scriptsize{$\pm$0.60}  
    \\
LwF~\cite{li2017LWF} && 
    75.45\scriptsize{$\pm$0.40}  
    \\
L2P~\cite{wang2022L2P} && 
    78.33\scriptsize{$\pm$0.06}  
    \\
S-Prompts~\cite{wang2022SPrompts} &&
    83.13\scriptsize{$\pm$0.51}  
    \\
\cdashline{1-3}[0.8pt/2pt]
% PT-SNMP & \multirow{4}{*}{0} & 
%     78.61\scriptsize{$\pm$0.00}  
%     \\
% FTF-SNMP && 
%     87.95\scriptsize{$\pm$0.00}  
%     \\
SDC-SNMP && 
    89.98\scriptsize{$\pm$0.74}  
    \\
% Baseline4 && 
%     87.68\scriptsize{$\pm$0.00}  
%     \\
Ours &&
    \bf{92.18\scriptsize{$\pm$0.19}}
    \\
\hline
\hline
Upper bound & - & 
    92.20\scriptsize{$\pm$0.27}
    \\ 
\hline
\end{tabular}
\end{center}
\caption{Result on Sequential Core50 for DIL. %Memory-based methods get results at buffer sizes of 50/class. 0 represents no buffer required.
%\textbf{Bold}: the best data-free result.
}
\label{table:core50}
\end{table}


\begin{table}[] \small
\setlength{\tabcolsep}{2pt}
\begin{center}
\begin{tabular}{ccccc}
\hline
Method  & FAA ($\uparrow$)  & $M$ & Memory(MB) & Inference(ms)\\ 
\hline
L2P (10/class)  & 
    84.21\scriptsize{$\pm$0.53}  & - & 5.01 & 699.5\\
L2P (50/class)  &
    86.31\scriptsize{$\pm$0.59}  & - & 17.3 & 699.5\\
L2P~\cite{wang2022L2P} & 
    83.86\scriptsize{$\pm$0.28}  & - & 1.94 & 699.5\\
DualP~\cite{wang2022DualP} & 
    86.51\scriptsize{$\pm$0.33}  & - & 1.90 & 656.5\\
\hline
\multirow{6}{*}{Ours} & 86.33\scriptsize{$\pm$0.31}  & 1 & 0.31 & 315.9\\
 & 86.36\scriptsize{$\pm$0.24} & 2 & 0.61 & 315.9\\
 & 86.52\scriptsize{$\pm$0.21} & 3 & 0.92 & 316.0\\
 & 87.57\scriptsize{$\pm$0.27} & 4 & 1.23 & 316.1\\
 & 87.52\scriptsize{$\pm$0.30} & 5 & 1.54 & 316.3\\
 & 87.87\scriptsize{$\pm$0.24} & 6 & 1.84 & 316.4\\
\hline
\hline
Upper bound & 91.98\scriptsize{$\pm$0.07} & - & 0 & 315.9\\
\hline
\end{tabular}
\end{center}
\caption{Additional Memory Consumption and Inference Cost. L2P (10/class) and L2P-50 (50/class) represent the results of L2P with buffers of 10/class and 50/class. $M$ is the total number of prototypes per class used in our method. Memory represents the additional memory cost after finishing the last task, compared with the upper-bound model. Inference represents the inference time of different methods (batch size:128, GPU: One Nvidia 3090). 
}
\label{table:memory}
\end{table}

\noindent\textbf{Ablation Study and Hyper-parameter Analysis}. The most important hyper-parameter of our method is the number of saved prototypes for each class. Generally, more prototypes can better preserve the feature distribution of the old classes, but also increases the running memory consumption. Noting that we discard the A-prompts after the prototype update. It thus does not increase the storage memory of the saved model. Tab.~\ref{table:memory} analyzes the additional memory cost of prompt-based methods compared to the non-incremental \emph{upper bound}. We can easily observe that: 1)  Saving prototypes in the feature space is much more memory-efficient than saving the original images; with only 1.8\% (0.31M v.s. 17.3M) memory consumption, we can achieve the same performance as L2P (50/class). 2) Our method is also more memory-efficient than the best data-free method, DualP,  with only a 48.4\% memory needed to achieve the same performance (M=3). 3) Our inference cost is less than half that of DualP and L2P, and almost equal to the joint training upper bound. However, our training time is longer than DualP and L2P, because of the additional analogical making stage. According to our experiments, training the A-prompt for each class takes about 30 seconds on CIFAR-100 (one Nvidia 3090). The training overhead can be further reduced with multiple
GPUs, as the A-prompts are independent of each other. The reduction in inference costs is more economical than the acceptable increase in training costs to implement our method in real applications.

\begin{table}[] \small
\setlength{\tabcolsep}{9pt}
\begin{center}
\begin{tabular}{cccc}
\hline
hyperparameter & value & FAA ($\uparrow$) & FF ($\downarrow$)\\ 
\hline
\multirow{4}{*}{$K$} 
& 10      & 87.69\scriptsize{$\pm$0.17} & 3.08\scriptsize{$\pm$0.24} \\
& 20      & 87.70\scriptsize{$\pm$0.15} & 3.02\scriptsize{$\pm$0.25} \\
& \bf{50}      & 87.87\scriptsize{$\pm$0.24} & 2.78\scriptsize{$\pm$0.07}  \\
& 100     & 87.84\scriptsize{$\pm$0.23} & 2.32\scriptsize{$\pm$0.12} \\ 
\hline
\multirow{4}{*}{$J$} 
& 1  & 87.06\scriptsize{$\pm$0.29} & 3.42\scriptsize{$\pm$0.30} \\
& \bf{5}  & 87.87\scriptsize{$\pm$0.24} & 2.78\scriptsize{$\pm$0.07}  \\
& 10  & 87.88\scriptsize{$\pm$0.29} & 2.69\scriptsize{$\pm$0.23} \\
& 15  & 87.79\scriptsize{$\pm$0.20} & 2.21\scriptsize{$\pm$0.09} \\ 
\hline
\end{tabular}
\end{center}
\caption{Hyperparameter study on Split CIFAR-100. \textbf{Bold}: the selected value in other experiments. }
\label{table:hyper}
\end{table}

\begin{table}[] \small
\setlength{\tabcolsep}{6pt}
\begin{center}
\begin{tabular}{cccc}
\hline
Distance & Scaling Factor & FAA ($\uparrow$) & FF ($\downarrow$)\\ 
\hline
Euclidean 
& -  & 86.89\scriptsize{$\pm$0.29} & 4.13\scriptsize{$\pm$0.46} \\
\cdashline{1-4}[0.8pt/2pt]
\multirow{4}{*}{SN Euclidean} 
& 10  & 87.03\scriptsize{$\pm$0.37} & 3.63\scriptsize{$\pm$0.25} \\
& \bf{20}  & 87.87\scriptsize{$\pm$0.24} & 2.78\scriptsize{$\pm$0.07}  \\
& 30  & 87.46\scriptsize{$\pm$0.38} & 3.23\scriptsize{$\pm$0.19} \\
& 40  & 87.12\scriptsize{$\pm$0.27} & 3.57\scriptsize{$\pm$0.17} \\ 
\hline
\end{tabular}
\end{center}
\caption{Distance metric study on Split CIFAR-100. \textbf{Bold}: the selected value in other experiments, SN Euclidean represents the scaling normalized euclidean distance.}
\label{table:dis}
\end{table}

We also study other hyperparameters used in our method. As shown in Tab.~\ref{table:hyper}, Our method performs consistently over a wide range of values. Therefore, we just select the medium values that perform relatively well in other experiments. Tab.~\ref{table:dis} shows that the scaling normalized euclidean distance is better than the original euclidean distance. We list the ablation study results on the prompt training stage and the model finetuning stage in Tab.~\ref{table:ablation}. As can be seen, $\mathcal{L}_{PP}$ contributes more to the final performance than $\mathcal{L}_{DE}$; $\mathcal{L}_{SC}$ improves FAA by $4.08\%$.

\begin{table}[!t] \small
\setlength{\tabcolsep}{8pt}
\begin{center}
\begin{tabular}{ccccc}
\hline
\multicolumn{5}{c}{The Prompt Training Stage ($\mathcal{L}_{PT}$)} \\
$\mathcal{L_{CC}}$ & $\mathcal{L_{PP}}$ & $\mathcal{L}_{DE}$ & FAA ($\uparrow$) & FF ($\downarrow$)\\ 
\checkmark &&& 82.16\scriptsize{$\pm$0.77} & 12.74\scriptsize{$\pm$0.53} \\
\checkmark & \checkmark && 87.00\scriptsize{$\pm$0.36} &  3.05\scriptsize{$\pm$0.21}\\
\checkmark & \checkmark & \checkmark &  \bf{87.87\scriptsize{$\pm$0.24}} & 
    \bf{2.78\scriptsize{$\pm$0.07}}  \\
\hline
\hline
\multicolumn{5}{c}{The Model Finetuning Stage ($\mathcal{L}_{FT})$} \\
$\mathcal{L}_{LS}$ & $\mathcal{L}_{SC}$ && FAA ($\uparrow$) & FF ($\downarrow$)\\ 
\checkmark &&& 83.79\scriptsize{$\pm$0.35} &  8.74\scriptsize{$\pm$0.22} \\
\checkmark & \checkmark && \bf{87.87\scriptsize{$\pm$0.24}} & 
    \bf{2.78\scriptsize{$\pm$0.07}}   \\
\hline
\end{tabular}
\end{center}
\caption{Ablation study on the loss function.}
\label{table:ablation}
% \vspace{-0.5cm}
\end{table}

