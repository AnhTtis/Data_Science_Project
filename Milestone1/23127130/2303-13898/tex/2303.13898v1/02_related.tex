\section{Related Work}
\label{sec:related}
% Overcoming catastrophic forgetting~\cite{mccloskey1989catastrophic,french1999catastrophic} has been a long-standing hotspot in incremental learning research. Existing methods can be roughly divided as follows.
%: 1) regularization-base, 2) memory-based, and 3) architecture-based methods. In addition, we present separately the methods that are most relevant to us, i.e., 4) data-free and 5) prompt-based methods.


\noindent\textbf{Continual Learning}. We only discuss the most related continual learning methods in this section, please refer to the latest surveys~\cite{masana2020class,wang2023comprehensive,de2021continual} for comprehensive introduction. \textit{Regularization-based methods} impose restrictions on the model's outputs or parameters. Output regularization methods~\cite{li2017LWF,2017icarl,Hou_2019lucir,wu2019bic,rannen2017encoder,castro2018EEIL,PrithvirajDhar2018LWM, ahn2021ss, kang2022class,douillard2020podnet,zhao2020maintaining} use knowledge distillation (KD)~\cite{hinton2015KD} to alleviate forgetting with the old model as the teacher.
Parameter regularization methods~\cite{kirkpatrick2017EWC,schwarz2018progress,aljundi2018MAS,ritter2018online,zenke2017SI,ahn2019uncertainty,chaudhry2018riemannian,liu2018rotate,lee2020continual} selectively constrain parameters according to their ``importance" on previous tasks. The regularization-based methods can further combine with other methods to further improve performance. \textit{Architecture-based methods}~\cite{pham2021dualnet,mallya2018PackNet,mallya2018Piggyback,liu2021adaptive,serra2018overcoming,shi2021continual} construct task-specific parameters to reduce interference between tasks, which includes neuron expansion~\cite{yoon2017DEN,xu2018reinforced,li2019learn}, network expansion~\cite{yan2021dynamically, wang2022foster, aljundi2017expert,schwarz2018progress,hung2019compacting}, and prompt expansion methods~\cite{douillard2022dytox,wang2022L2P,wang2022DualP,wang2022SPrompts}. Unlike prompt expansion methods, which use prompts to isolate different tasks, we use A-prompts to establish correspondence between different tasks. These prompts can be discarded after model updating, allowing our method to reason from original images without retrieving prompts from an expanding prompt pool. \textit{Data-replay methods}~\cite{2017icarl,rolnick2019ER,Hou_2019lucir,tao2020bocl,SonglinDong2021RKD,aljundi2019gradient,liu2020mnemonics} mitigate forgetting by keeping a buffer for the old data, which introduce the negative side effect of data imbalance~\cite{castro2018EEIL,wu2019bic,Hou_2019lucir,kim2020imbalanced,bang2021rainbow}. 

Our method can be categorized into the \textit{Data-free-replay methods}. Instead of generating pseudo samples of the previous task from scratch~\cite{shin2017DGR,chenshen2018MemoryReplayGANs,kamra2017DGDM,JamesSmith2021ABD,liu2022ERDR}, our method reprograms samples of the current task into the previous task, conditioned on the learnable prompts. Compared to previous feature-replay methods, our method has better plasticity because our feature representation is tunable rather than fixed~\cite{hayes2020remind,wang2021acae,Petit_2023_WACV}, and it better handles the representation shift problem~\cite{LuYu2020SDC,iscen2020memory} by addressing semantic gaps between different tasks. The continual learning ability of pretrained foundation models is also studied by~\cite{mehta2021empirical,wu2022class,ramasesh2022effect} with different setup.

\noindent\textbf{Prompt tuning}. Prompt tuning is first applied to natural language processing for efficient knowledge transfer of large foundation models~\cite{BrianLester2021PEPT,XiangLisaLi2021PrefixTuningOC,ZexuanZhong2021FactualPI,liu2023pre,houlsby2019parameter,raffel2020exploring,brown2020language,chenadaptformer}, which reprograms the input rather than finetunes the network. It is further applied to computer vision tasks, such as transfer learning~\cite{jia2022visual,bahng2022exploring,tsai2020transfer,zhou2022cocoop,zhou2022coop}, domain generalization~\cite{zheng2022prompt,gao2022visual}, and incremental learning~\cite{wang2022L2P,wang2022DualP,wang2022SPrompts}.