\section{Method}
\label{sec:method}
\newcommand{\argmax}{\mathop{\mathrm{argmax}}}
\newcommand{\x}{\boldsymbol{x}}
\newcommand{\f}{\boldsymbol{f}}
\newcommand{\p}{\boldsymbol{p}}
\newcommand{\A}{\boldsymbol{a}}
\newcommand{\B}{\boldsymbol{\varphi}}
\newcommand{\D}{\boldsymbol{\gamma}}
\newcommand{\DD}{\boldsymbol{\Gamma}}

\newcommand{\J}{\left[j\right]}
\newcommand{\lgt}{\boldsymbol{l}}
\newcommand{\old}{\boldsymbol{o}}
\newcommand{\new}{\boldsymbol{n}}
\newcommand{\para}{\boldsymbol{\theta}}
\newcommand{\pt}{t\!-\!1}
\newcommand{\pc}{\p_{\left\{\B^{\pt}_{y}\right\}}}
\newcommand{\xc}{\mathcal{X}^{t}_{\left\{\B^{\pt}_{y}\right\}}}


Incremental learning (IL) is to train a single model on a non-stationary data stream without catastrophic forgetting. In this paper, we focus on the senior where a sequence of task is well defined, \emph{i.e.}, $t \in  \mathcal{T}=\left\{1,2,\cdots, T\right\}$, $T$ is the total task number, and we can only access the current task data. Let $\mathcal{Y}^{t}$ denote the label set of the task-$t$, and $\mathcal{S}^{t}$ denote all seen label until the task-$t$,\emph{i.e.}, $\mathcal{S}^{t}$ = $\cup \left\{\mathcal{Y}^{1}, \mathcal{Y}^{2},...,\mathcal{Y}^{t} \right\}$. Let $\x \in \mathcal{X}^{t}$ represent the accessible samples, and $(\x, y) \in \mathcal{D}^{t}$ represent the sample-label pairs of the task-$t$. $\mathcal{X}_y$ represents samples in class $y$. 

\subsection{Vision Transformer and Prompt Tuning}

The vision transformer (ViT) backbone consists of two components, \emph{i.e.}, $f = f_a \circ f_e$, where $f_e$ is the embedding block which transforms the input image $\x \in \mathbb{R}^{H \times W \times C}$ into patch tokens $f_e(\x) \in \mathbb{R}^{L \times D}$, whose first token is the $[class]$ token inherited from the pretrained model; $f_a$ consists of a sequence of transformer encoders, each of which includes a multi-head attention (MHA) and a multi-layer perception (MLP). We take the first token $\f \in \mathbb{R}^{D}$ of the last layer, the corresponding output of the $[class]$ token, as the feature vector for classification tasks, \emph{i.e.}, $\f = f_a(f_e(\x))\left[1\right]$.

As $f_a$ allows arbitrary length of tokens as input, we can attach additional learnable tokens to the original image tokens, \emph{i.e.}, $f_e(\x) \oplus \p$. $\oplus$ denotes the concatenate operator along the token sequence; $\p \in \mathbb{R}^{J \times D}$ is the learnable prompt tokens, which has the same feature dimension as the image tokens. Finally, the prompt-conditioned feature vector can be calculated as $\tilde{\f}= f_a(f_e(\x) \oplus \p)\left[1\right]$, $\tilde{\f} \in \mathbb{R}^{D}$. 

In the following section, we abbreviate the above two equations as:
\begin{equation}
  \f = f_{\para}(\x), \tilde{\f} = f_{\para}(\x|\p),  
\end{equation}
where $\f$ represents the original feature vector, $\hat{\f}$ represents the prompt-conditioned feature vector, and $\para$ is the model parameters. More details about ViT and prompt tuning can be found in~\cite{}, and our network structure is visualized in Fig.~\ref{}. 

\subsection{Overall Framework}

We adopt a Soft-Nearest-Multi-Prototype (SNMP) classifier in our method, which is an extension of  Nearest-Mean-of-Exemplars (NME) Classifier~\cite{2013distance}. NME is widely used in incremental learning~\cite{2013distance,2017icarl,LuYu2020SDC} since it can easily add novel classes by calculating their mean centers in feature space. SNMP maintains multiple prototypes for each class instead of a single mean center, better characterizing the feature distribution and the boundary between different classes, which is formulated as follows: 
\begin{equation}\label{eq:class}
    y^* = \argmax_{y \in \mathcal{S}^{t}} \frac{\sum_{m=1}^{M} \exp(\beta \cos(f_{\para}(\x), \B_{ym}))}{\sum_{y \in \mathcal{S}^{t}}\sum_{m=1}^{M} \exp(\beta \cos(f_{\para}(\x), \B_{ym}))},
\end{equation}
where $\B_{ym}$ represents the $m$-th prototype of the class $y$, and $M$ is the total prototype number per class. $\cos(\cdot,\cdot)$ is the cosine similarity between two vectors, \emph{i.e.}, $\cos(\A_i, \A_j) = \frac{\A_i\cdot \A_j}{\lVert \A_i\rVert \lVert \A_j \rVert}$. $\beta$ is the scaling factor controls the softness of the classification boundary. 

After finishing the model finetuning, we can easily initialize prototypes of the current task by taking the K-Means clustering centers for each new class. The main difficulty is how to keep track of historical prototypes affected by feature drift. Previous methods~\cite{2017icarl,} solve this problem by saving a representative subsets of historical data (memory), which are used to recalculate the mean centers after finetuning. However, memory-based methods have a privacy breach risk and are memory-inefficient for long-sequence incremental learning. SDC~\cite{LuYu2020SDC} is the first data-free feature compensation method, which estimates the drift of historical prototypes with the current task data. We elaborate on its advantages and disadvantages in the following section.

\subsection{Feature Drift Compensation}

For simplicity, we abbreviate $\B_{ym}$ to $\B_{y}$ in the following sections because tracking multiple prototypes for each class is just a simple repetition of tracking one prototype.

The general feature compensation method proposed by SDC~\cite{LuYu2020SDC} assumes that data points close together in the previous feature space should have similar drift after finetuning. $f^{\pt}(\cdot)$ is the abbreviation for $f_{\para^{\pt}}(\cdot)$, which represents the model before the task-$t$ finetuning; $f^{t}(\cdot)$ is the abbreviation for $f_{\para^{t}}(\cdot)$, which represents the model after the task-$t$ finetuning. $\B^{\pt}_{y}$ is the prototype of the class-$y$ in the feature space before finetuning. SDC directly uses the current task's data $\x \in \mathcal{X}^{t}$ as the reference to estimate the compensated the historical prototype $\left\{\hat{\B}_{y}^{t}| y \in \mathcal{S}^{\pt}\right\}$:

\begin{equation}\label{eq:cps}
\begin{aligned}
  \D_i &= f^{t}(\x_i) - f^{\pt}(\x_i), \x_i \in \mathcal{X}^{t}, \\
  \DD_{y} &= \frac{\sum_{i} \exp( \beta \cos(f^{\pt}(\x_i), \B^{\pt}_{y})) \D_i}{\sum_{i} \exp( \beta \cos(f^{\pt}(\x_i), \B^{\pt}_{y}))}, \\
  \hat{\B}^{t}_{y} &= \B^{\pt}_{y} + \DD_{y}, 
\end{aligned} 
\end{equation}
where $\D_i$ is the feature drift of the $i$-th sample, and $\DD_{y}$ is the estimated feature drift of $\B^{\pt}_{y}$ after the model finetuning on the task-$t$. 

As shown in the above equation, the quality of feature drift compensation is highly dependent on whether there exists data samples near the target feature in the previous feature space. SDC assumes that there exists semantic overlaps between different tasks. However, this assumption usually cannot be met in challenging scenarios when semantic gap between different tasks is enormous, which results in bias and unreliable drift estimations. As visualized in Fig.~\ref{}, ... 

Obviously, $\mathcal{X}_y$ is the most reliable reference samples to estimate the semantic drift of $\B_y$. However, samples in previous classes are not accessible in the data-free setting. To solve this problem, we ``convert" samples from $\mathcal{X}^{t}$ to the historical classes $y \in \mathcal{S}^{\pt}$ by attaching a prototype-specific analogical prompt $\pc$.

This prompt has two effects: 1) prompt-conditioned samples will be classified into the target class corresponding to the prompt by the model before task-$t$ finetuning; 2) prompt-conditioned samples will be pulled close to the historical prototype corresponding to the prompt. Thus, the analogical prompt successfully bridge the semantic gap between the current task data with previous classes. Then, we can use these prompt-conditioned samples to estimate the semantic drift of previous prototypes more reliably. As visualized in Fig.~\ref{},...

Our method includes three stages. Firstly, we sample sample subsets from $\mathcal{X}^{t}$ according their distance to the previous prototype $\B^{\pt}_y$:
\begin{equation}\label{eq:sample}
    \xc = \left\{ \x | \x \in \mathcal{X}^{t}, f^{\pt} (\x) \in \text{K-NN}(\B^{\pt}_y) \right\},
\end{equation}
where $K$ is the total sample number for each historical prototype, and $\text{K-NN}(\cdot)$ is the K-Nearest-Neighbor of a given feature vector. We use these selected samples to train the prototype-specific prompt $\left\{\pc | y \in \mathcal{S}^{\pt}\right\}$ (details in Sec.~\ref{sec:ptrain}).

Secondly, we finetune the model with the original dataset $\mathcal{X}^{t}$ (details in Sec.~\ref{sec:finetune}), and initialize prototypes with the K-Means cluster centers for each new class.

Thirdly, We compensate previous prototypes $\hat{\B}^{t}_y=\B^{\pt}_y+\DD_y$ as the same as Eq.~\ref{eq:cps}, but with prompt-conditioned data as the reference. In particular, we use $\x_i \in \xc$ as the reference dataset, and use the prompt-conditioned transformer to replace the origin model: 
\begin{equation}\label{eq:cps_p}
\begin{aligned}
  \D_i &= f^{t}(\x_i| \pc)- f^{\pt}(\x_i| \pc), \x \in \xc \\
  \DD_y &= \frac{\sum_{i} \exp( \beta \cos(f^{\pt}(\x_i| \pc), \B^{\pt}_{y})) \D_i}{\sum_{i} \exp( \beta \cos(f^{\pt}(\x_i| \pc), \B^{\pt}_{y}))}.
\end{aligned}
\end{equation}

Finally, we use Eq.~\ref{eq:class} to classify samples in the updated feature space.  More details about our method can be in Alg.~\ref{alg:algo_train}.  

\begin{algorithm}
\caption{Model Training}\label{alg:algo_train}
\begin{footnotesize}
\KwIn{the total task number $T$; the pretrained ViT model $f$; the training sequence $\left\{\mathcal{D}^{t}|t \in \mathcal{T}\right\}$}
\KwOut{$f^{T}$; $\left\{\B^{T}_{y}|y \in \mathcal{S}^{T}\right\}$}
Initialize $f^{0}=f$;\\
\For{$t=1,\ldots,T$}{
\eIf{$t=1$}{ Train $f^{\pt}$ by $\mathcal{L}_{c}$ on $\mathcal{D}^{t}$ to get $f^{t}$; \\
             Get $\left\{\B^{t}_{y}|y \in \mathcal{Y}^{t}\right\}$ by class-wise K-Means; 
}{
    \For{$y=1,\ldots,|\mathcal{S}^{\pt}|$}{
    Sample $\xc$ by Eq.~\ref{eq:sample}; \\
    Train $\pc$ by Eq.~\ref{eq:pt} on $\xc$;
    }
    Training $f^{\pt}$ by Eq.~\ref{eq:ft}  on $\mathcal{D}^{t}$ to get $f^{t}$; \\
    Get $\left\{\B^{t}_{y}|y \in \mathcal{Y}^{t}\right\}$ by class-wise K-Means; \\
    Update $\left\{\B^{\pt}_{y}|y \in \mathcal{S}^{\pt}\right\}$ to $\left\{\B^{t}_{y}|y \in \mathcal{S}^{\pt}\right\}$ in-place by Eq.~\ref{eq:cps_p} with $\left\{(\pc, \xc)|y \in \mathcal{S}^{\pt}\right\}$ ;
}
abandon $f^{\pt}$, $\mathcal{D}^{t}$, $\left\{(\pc, \xc)|y \in \mathcal{S}^{\pt}\right\}$;
}
\Return $f^{T}$, $\left\{\B^{T}_{y}|y \in \mathcal{S}^{T}\right\}$
\end{footnotesize}
\end{algorithm}


% \begin{algorithm}
% \caption{Model Training}\label{alg:algo_train}
% \KwIn{the current task $t$; the total task number $T$; the pretrained ViT model $f$; the training data sequence $\{\mathcal{D}^{t}\}_{t=1}^{T}$}
% \KwOut{the final model $f^{T}$; the final prototypes}
% Initialize $f^{0}=f$;\\
% \For{$t=1,\ldots,T$}{
% $u^{(t)} = U_{\theta^{(t)}}(I^{(t)})$; \\
% Initialize $k=1, \beta^{(t,k)} = \mathbb{O}_{M^{t}}$; \\ 
% \tcp{$\mathbb{O}_{M^{t}} \in \mathbb{R}^{M^{t}}$ is a zero vector}
% \Repeat{$\mathcal{G}^{(t,k)}$ has converged}{
% $\mathcal{G}^{(t,k)} = \mathcal{G}(\beta^{(t,k)}, u^{(t)}, v^{(t)})$; \\ 
% \tcp{\textbf{Maximize} $\mathcal{G}$ by optimizing $\beta$}
% Update $\beta^{(t,k+1)}$ using L-BFGS;\\
% Update $k:= k+1$;
% }
% $\hat{\beta}^{(t)}=\beta^{(t,k)}$, $W_{\varepsilon}^{ub}=\mathcal{G}(\hat{\beta}^{(t)}, u^{(t)}, v^{(t)})$;\\
% \tcp{\textbf{Minimize} $W_{\varepsilon}^{ub}$ by optimizing $\theta$}
% Update $\theta^{(t+1)}$ using Adam;
% }
% \Return $\para^{T}$, 
% \end{algorithm}


\subsection{Analogical Prompt Training}\label{sec:ptrain}
We describe the prompt training stage in his section. In this stage, the only trainable parameters are analogical prompts, while other parameters are frozen. The targeted-classification loss to train  $\p_{\left\{\B_{y}^{\pt}\right\}}$ is formulated as follows:

\begin{equation}\label{eq:adv}
 \begin{aligned}   
      \tilde{\lgt}_{i}^{\pt} &= h^{\pt}(f^{\pt}(\x_i| \p_{\left\{\B_{y}^{\pt}\right\}})), \x \in \xc \\
      \mathcal{L}_{pc} &= -\frac{1}{N}\sum_{i=1}^{N} \sum_{j=1}^{|\mathcal{S}^{\pt}|} \mathbbm{1}_{\left\{j=y \right\}}\log(A(\tilde{\lgt}^{\pt}_i)\left[j\right]),
\end{aligned} 
\end{equation}
where $N$ is the training batch size, $\tilde{\lgt}^{\pt}$ is the prompt-conditioned logits output of the model before task-$t$ finetuning, $h(\cdot)$ is the fully-connected classification layer, $\mathbbm{1}_{\left\{\cdot\right\}}$ is the indicator function that equals 1 when the condition is met and 0 otherwise. $A(\cdot)$ is the softmax normalize function $A(\lgt)\left[j\right] = \frac{\exp(\lgt_{j})}{\sum_{i}\exp(\lgt_{i})}$.


The loss function defined in Eq.~\ref{eq:adv} is to distill class-specific knowledge into the analogical prompt $\p_{\left\{\B_{y}^{\pt}\right\}}$, which is further used to bridge the semantic gap between different tasks.

We also define a loss function to pull the prompt-conditioned feature vector close to the target prototype, because it's more reliable to estimate the prototype drift with nearby samples. What's more, we add a term to avoid all feature vectors collapsing into a single point: 
\begin{equation}
\begin{aligned}
    \tilde{\f}_{i}^{\pt} &= f^{\pt}(\x_i| \p_{\left\{\B_{y}^{\pt}\right\}}), \x \in \xc \\
    \mathcal{L}_{pf} &= \frac{1}{N}\sum_{i=1}^{N}(1 - \cos(\B_{y}^{\pt}, \tilde{\f}_{i}^{\pt})), \\
    \mathcal{L}_{div} &= \frac{1}{N(N-1)}\sum_{i=1}^{N}\sum_{j=i+1}^{N}\lfloor \cos(\tilde{\f}_{i}^{\pt}, \tilde{\f}_{j}^{\pt}) - \Omega \rfloor_{+}, 
\end{aligned} 
\end{equation}
where $\tilde{f}^{\pt}$ is the prompt-conditioned feature of the model before task-$t$ finetuning, $\lfloor \cdot \rfloor_{+}$ truncate negative value to zero, $\Omega$ is a constant threshold, we set it to $0.95$. 

The final objective function for training analogical prompt is defined as follows:
\begin{equation}\label{eq:pt}
 \mathcal{L}_{pt} = \mathcal{L}_{pc} + \mathcal{L}_{pf} + \mathcal{L}_{div}
\end{equation}

\subsection{Model Fintuning}\label{sec:finetune}

In this section, we describe how to finetune the model on the task-$t$. For training efficiency and alleviating catastrophic forgetting , we only update MLP of each transformer encoder and the final classification header. We adopt the local softmax cross-entropy~\cite{masana2020class} instead of the original softmax cross-entropy to avoid over-penalizing historical classes, and knowledge distilling loss to maintain previous knowledge. 

Model is training with the current task data, $x \in \mathcal{X}^{t}$, $\lgt_{i}^{t} = h^{t}(f^{t}(\x_i)), \lgt_{i}^{t} \in \mathbb{R}^{|\mathcal{S}^{t}|} $. We can separate $\lgt^{t}$ into $\old^{t} \in \mathbb{R}^{|\mathcal{S}^{\pt}|}$, $\new^{t} \in \mathbb{R}^{|\mathcal{Y}^{t}|}$, where $\lgt^{t} = \old^{t} \oplus \new^{t}$. The training objective is defined as follows: 
\begin{equation}
\begin{aligned}
  \mathcal{L}_{c} &= -\frac{1}{N}\sum_{i=1}^{N} \sum_{j=1}^{|\mathcal{Y}^{t}|}\mathbbm{1}_{\left\{j+|\mathcal{S}^{\pt}|=y_i \right\}}\log(A(\new_{i}^{t})\left[j\right]), \\
  \mathcal{L}_{d} &=  -\frac{1}{N}\sum_{i=1}^{N}\sum_{j=1}^{|\mathcal{S}^{\pt}|} A\left(\lgt_{i}^{\pt}/ \zeta \right)\left[j\right]\log\left(A\left(\old_{i}^{t}/\zeta \right)\left[j\right]\right), 
\end{aligned}
\end{equation}
where $\zeta$ is the smoothing temperature, which is set to $2$ as previous methods~\cite{}. It is worth noting that the local softmax cross-entropy is only applied to the current task's classes, which avoids over-penalizing historical classes when the historical data are unavailable. 

Moreover, we propose a drifting consistency loss, which maximum the cosine similarity between and the true feature vector and the compensated feature vector: 
\begin{equation}
 \mathcal{L}_{f} = \frac{1}{N}\sum_{i=1}^{N} (1 - \cos(\f_{i}^{t} - \hat{\f}_{i}^{t})),  
\end{equation}
where $\f_i^{t}=f^{t}(\x_i)$ is the true feature vector of the current model, and $\hat{\f}_{i}^{t}= f^{\pt}(\x_i) + \DD_{i}$ is the compensated feature vector calculated by Eq.~\ref{eq:cps} but using $\left\{\x_j | j \neq i; y_{j}=y_{i}; j \in \left\{ 1,2,\cdots, N \right\} \right\}$ as the reference data to estimate $\DD_{i}$.

The final objective function to finetune the model on the task-$t$ is defined as follows:
\begin{equation}\label{eq:ft}
 \mathcal{L}_{ft} = \mathcal{L}_{c} +  \mathcal{L}_{d} + \mathcal{L}_{f} 
\end{equation}



















