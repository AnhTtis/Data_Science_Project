\input{_constants}
\rebuttal

\documentclass[10pt,twocolumn,letterpaper]{article}
\input{cvpr_header}
\myexternaldocument{_main}
\begin{document}
%% TITLE
% \title{\paperTitle}
% \maketitle
\thispagestyle{empty}
\appendix
%%
We thank the reviewers for their comments.  It is
encouraging that the reviewers (\textbf{R2}, \textbf{R3}) found our idea interesting and the consistent performance improvement (\textbf{R1}, \textbf{R3}).

\textbf{Response to Reviewer QsqS (\textbf{R1})}

\textbf{Q1:} The proposed framework is quite convolved ...  whether these 6 losses really help the network generalize ... or just further overfitting ... 

\textbf{A1:} Thanks for your advice. We focus on a challenge but a practical setting where we can not save any old data. If we can keep a rehearsal memory of the old data, the analogy-making stage can be discarded. 

Using a unified objective function for the different stage is difficult since the analogy-making stage aims to mimic the distribution of old classes while the finetuning stage aims to distinguish new classes. Combining these two stages into one is a promising future study direction. 

Although each stage combines three different terms, we do not introduce additional hyper-parameters to adjust relative strength of each loss term, which reduces the risk of over-fitting. 

\textbf{Q2:} How sensitive is the proposed method to the training order of different sample/class prototypes?

\textbf{A2:} The deviation (calculated by three runs) in all tables reflects the influence of the training order of different samples. As can be seen, the influence is slight. 

The training order of class prototypes within the same task does not influence the performance. The A-prompts for different classes can be trained in parallel, and so is the generation of class prototypes. However, the class order between different tasks do influence the performance. To make a fair comparison, we use the same class order as L2P and DualP. \textcolor{red}{We conduct an additional experiment on the influence of the class order on Cifar-100}.

\textbf{Q3:} the new naming “analogical features” is not necessary. Most of the prompt-based approaches essentially alter the input representation conditioned on the learnable/fixed prompts.

\textbf{A3:} Thanks for your advice. Our A-prompts explicitly build correspondence between different classes, while other prompts do not. The introduction of the human analogy mechanism can help readers understand our central idea and contribution intuitively. If you insist on your opinion, we can change "analogical features"  into "prompt-conditioned features."

\textbf{Response to Reviewer t2u4 (\textbf{R2})}

\textbf{Q1:}  1) using fully connected layers (SNCN) on MNIST/FMNIST and other related benchmarks [3]; 2) report backward and forward transfer as GEM [4] and it's variant by SNCN [3]; 3) the performance of the model with only 1 epoch, 5 epoch, and 10 epoch; 4) show performance on cifar-25 and 50; 5) track prior task performance per epoch? How does that vary? 6) report task matrix for any benchmark.

\textbf{A1:} Our experimental setup follows the previous works (L2P and DualP) that study incremental learning on the pre-trained ViT model. The experiments mentioned in the question have not been reported in previous papers, which means that the experimental results cannot be compared. \textcolor{red}{We provide some experimental results as follows:}

The experimental setup of SNCN is quite different from ours. It uses an MLP with three hidden layers as the base model and conducts experiments on easier datasets (MNIST/FMNIST). Implementing SNCN on ViT is not trivial since SNCN is highly dependent on the network structure. Moreover, SNCN is our concurrent work without code release, and our work has nothing similar to SNCN.


\textbf{Q2:} Other replay-based alternatives such as Hard attention to task [6], mnemonics [4, 10], GEM [5], A-GEM should be mentioned with some comparison?

\textbf{A2:} Thanks for your advice. We will cite and discuss these papers in our related work. HAT[6] is a task-incremental learning method that can not directly compare with our method. The class-incremental learning performance of GEM[5] and A-GEM is far behind the methods we compared (such as DER++ and Co$^2$L). We implement mnemonics [4,10] on pre-trained ViT, whose final average accuracy is 75.12\%  (10/class; Cifar-100); our performance is 87.87\% (0/class Cifar-100).

\textbf{Q3:} 1) Ablation study is missing for various hyper-parameters such as K, J, M, and Beta;
2)  why local cross entropy + KD + shift detection loss is important. Simple ablation study would be beneficial; 3)  report memory usage, training time, total FLOPS, and parameter.

\textbf{A3:} Please refer to Table 6 for K, J, and Beta; refer to Table 5 for M and memory usage; refer to Table 7 for ablation study of loss term; refer to Supplementary Material Table 1 for total FLOPS; refer to Implementation Details (Line 603) for parameters; refer to \textbf{R3A1} for training time.


\textbf{Q4:} The local cross-entropy loss is rather a limitation ...  use separate classification heads ... Other variants detect task boundaries and generate different classification heads ...

\textbf{A4:} Instead of separate classification heads, we use a unified NME-based classifier (line 322) in the inference stage, which does not need to detect task boundaries; the local cross-entropy loss and its corresponding head are only used in the training stage.

\textbf{Q5:} The citation 32 in the main paper is for local cross-entropy rather this concept was proposed by [7,8] ...

\textbf{A5:} Thanks for advice. We will cite these two papers.


\textbf{Q6:} Detecting feature shift is not novel; several prior works [2,3] ... no comparison  ...

\textbf{A6:} First, We do not claim shift correction is our contribution; our shift correction is developed from SDC. Second,  [2] is an axiv pre-print without any experiments, SNCN [3] can hardly compare with our method as mentioned in \textbf{R2A1}. Third, you may confuse shift detection with shift correction. [3] proposes a shift detection method since task boundaries are not available in training. In contrast, we have task boundaries in training and we do not need to detect shifts but only need to correct them. 


% We will add these two citations in our related works. However,  and [3] is a contemporary work in NeuIPS 2022, and none of them releases its code, which makes it difficult to re-implement them on pretrained ViT for fair comparison.  



\textbf{Q7:} Freezing weights of pre-trained network ... How do you ensure current approach efficiently handles stability-plasticity dilemma ...

\textbf{A7:} Our method \textbf{does not freeze weight} during fin-tuning, which ensures plasticity, and the shift correction ensures stability. 

\textbf{Q8:} Limitations of current work are not explained ...

\textbf{A8:} The performance of our methods is weakened when the semantic gap between tasks is too large.


\textbf{Q9:} Why only 5 epochs for few datasets and 10/50 for others ... criteria for convergence ...

\textbf{A9:} We follow the same setting as L2P and DualP. Criteria: training loss converges.



\textbf{Response to Reviewer jjdx (\textbf{R3})}

\textbf{Q1:} ... a computational cost comparison ...

\textbf{A1:} The comparison of inference costs is shown in Supplementary Material Table 1. As can be seen, our method has a much lower additional inference cost compared to other prompt-based methods (L2P and DuaP) because we do not use prompts during the inference stage. Our method requires more training time because of the A-prompts training. However, the training can be parallelized as the A-prompts are independent of each other. \textcolor{red}{Using Cifar-100 as an example, training A-prompts causes about 30\% overhead. (on 8 GTX3090).}

\textbf{Q2:} The proposed data-free methods are even better than many methods that explicitly store historical samples ... explanations on why this is possible ... One ablation study is to use the actual old data for correcting prototypes ...

\textbf{A2:} Saving the original historical samples is memory inefficient. Limited memory makes rehearsal-based methods prone to overfitting and biased towards a small number of preserved samples. In contrast, our method condenses class-specific information into A-prompts and builds correspondence between the old and new samples, which is a much more efficient way to keep knowledge. Intuitively, one actual old sample has higher quality than one synthetic sample, however, we can generate 1000 synthetic samples with our method. \textcolor{red}{As per your suggestion, we add an ablation study to use 1/5/10 actual old data per class to correcting prototypes. It's worth noting that, even saving 1 sample per class has higher memory consumption than our method.}




\textbf{Response to AC} 

Thanks for the great effort in reviewing our paper. We are reporting the diversity in the rating we received and the serious issue raised by comments from Reviewer t2u4 (R2). 

1) R2 requests significant additional experiments (R2Q1), which is unnecessary and can not compare previous methods with the same experimental setting. We think these comments violate CVPR's rebuttal policies.

2) R2 skipped a lot of our major experimental results and made hasty comments. (R2Q3).

3) The accusation of our novelty is untenable (R2Q6), our method does not need to detect feature shift, and we did not claim that feature shift detection is our contribution.

4) R2's understanding of our approach contains many factual errors (R2Q4, R2Q6, R2Q7). 

 \newpage

We thank the reviewers for their comments.  It is
encouraging that the reviewers (\textbf{R2}, \textbf{R3}) found our idea interesting and the consistent performance improvement (\textbf{R1}, \textbf{R3}).

\textbf{Response to Reviewer QsqS (\textbf{R1})}

\textbf{Q1:} The proposed framework is quite convolved ...  whether these 6 losses really help the network generalize ... or just further overfitting ... 

\textbf{A1:} Our framework is a widely used NME-based incremental learning framework. The only additional stage is analogy-making, which is introduced to replace the storage of historical data. Combining analogy-making with fine-tuning is a challenging but promising direction for future research. Although each stage combines three different terms, we do not introduce additional hyperparameters to adjust each loss term's relative strength, and we use the same hyperparameters in the loss to reduce the risk of overfitting. 

\textbf{Q2:} How sensitive is the proposed method to the training order of different sample/class prototypes?

\textbf{A2:} The influence of the training order of the different samples is small, which is reflected in the deviations of the experimental results. The training order of class prototypes within the same task does not influence the performance. However, the class order between different tasks does influence affect performance. To make a fair comparison, we use the same class order as L2P and DualP. We experiment on the influence of the class order on Cifar-100: 87.31{\scriptsize{$\pm$0.55}}. The deviation slightly increases but can not affect our conclusions.

\textbf{Q3:} the new naming “analogical features” is not necessary. Most of the prompt-based ... on conditioned on the learnable/fixed prompts.

\textbf{A3:} Thanks for your advice. Our A-prompts explicitly build correspondence between different classes, while other prompts do not. The introduction of the human analogy mechanism aims to help readers understand our central idea intuitively. We can change "analogical features"  into "prompt-conditioned features."

\textbf{Response to Reviewer jjdx (\textbf{R3})}

\textbf{Q1:} ... a computational cost comparison ...

\textbf{A1:} Please refer to Supp Table 1 for the inference cost comparison. Our method has advantages over other prompt-based methods. The major training overhead of our method is prompt training. Training the A-prompt for each class takes approximately 30 seconds (Cifar-100, GTX3090). Moreover, The A-prompts are independent of each other so that the training overhead can be further reduced with multiple GPUs.

\textbf{Q2:} The proposed data-free methods are even better than ... explicitly store historical samples ... explanations on why  ... One ablation study is to use the actual old data for correcting prototypes ...

\textbf{A2:} Saving the original historical samples is memory inefficient. Limited memory makes rehearsal-based methods prone to overfitting and biased towards a small number of preserved samples. In contrast, our method condenses class-specific information into A-prompts and builds correspondence between the old and new samples, which is a much more efficient way to keep knowledge. Intuitively, one actual old sample has higher quality than one synthetic sample, however, we can generate 1000 synthetic samples with our method. \textcolor{red}{As per your suggestion, we add an ablation study to use 1/5/10 actual old data per class to correcting prototypes. It's worth noting that, even saving 1 sample per class has higher memory consumption than our method.}

\textbf{Q3:}  ... visualize the evolution of the prototypes ...

\textbf{A3:} Thanks for your advice. The evolution of analogical features and prototypes is visualized in Supplementary Material Figure 1. As shown, our analogical features successfully track the shift of old prototypes.



% {\small
% \bibliographystyle{ieee_fullname}
% \bibliography{11_references}
% }

\end{document}
