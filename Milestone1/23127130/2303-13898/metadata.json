{
    "arxiv_id": "2303.13898",
    "paper_title": "Remind of the Past: Incremental Learning with Analogical Prompts",
    "authors": [
        "Zhiheng Ma",
        "Xiaopeng Hong",
        "Beinan Liu",
        "Yabin Wang",
        "Pinyue Guo",
        "Huiyun Li"
    ],
    "submission_date": "2023-03-24",
    "revised_dates": [
        "2023-03-27"
    ],
    "latest_version": 1,
    "categories": [
        "cs.CV",
        "cs.LG"
    ],
    "abstract": "Although data-free incremental learning methods are memory-friendly, accurately estimating and counteracting representation shifts is challenging in the absence of historical data. This paper addresses this thorny problem by proposing a novel incremental learning method inspired by human analogy capabilities. Specifically, we design an analogy-making mechanism to remap the new data into the old class by prompt tuning. It mimics the feature distribution of the target old class on the old model using only samples of new classes. The learnt prompts are further used to estimate and counteract the representation shift caused by fine-tuning for the historical prototypes. The proposed method sets up new state-of-the-art performance on four incremental learning benchmarks under both the class and domain incremental learning settings. It consistently outperforms data-replay methods by only saving feature prototypes for each class. It has almost hit the empirical upper bound by joint training on the Core50 benchmark. The code will be released at \\url{https://github.com/ZhihengCV/A-Prompts}.",
    "pdf_urls": [
        "http://arxiv.org/pdf/2303.13898v1"
    ],
    "publication_venue": null
}