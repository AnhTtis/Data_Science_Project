\section{Characteristic Study via Microbenchmark}
\label{sec:character}
In this section, we present our findings in evaluating CXL memory using our microbenchmark. We believe that this analysis provides insights into how CXL memory users can leverage CXL memory more efficiently according to their use cases. We will also compare these results with the assumptions and emulations from some recent work on CXL memory, where CXL memory is emulated using cross-NUMA data accesses with some additional latency. 

\subsection{Microbenchmark Description}
\label{sec:microbenchmark}
To thoroughly examine the properties of CXL memory, we have developed an extensive microbenchmark called \textbf{\arch}. This benchmark is designed to target various use cases of CXL memory and runs on the Linux user space. Users can provide command-line arguments to specify the workloads to be executed by \arch. We plan to open source \arch.

In particular, \arch provides the capability to (1) allocate memory from different sources, including the local DDR5 memory, the CXL memory CPU-less NUMA node, or the remote-socket DDR5, by using the \texttt{numa\_alloc\_onnode} function, (2) launch a specified number of testing threads, pin each thread to a core, and optionally enable or disable prefetching within the cores, and (3) perform memory accesses using inline assembly language. The benchmark reports the memory access latency or aggregated bandwidth from different instructions, such as \texttt{Load}, \texttt{Store}, and \texttt{Non-temporal Store}, with all memory accesses done using AVX-512 instructions. Additionally, \arch can perform pointer chasing on a region of memory, and by varying the working set size (WSS), the benchmark can show how the average access latency changes as WSS crosses the different sizes of the cache hierarchy.

\subsection{Latency Analysis}
\label{sec:latency}
\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figs/micro_lats_ratio.pdf}
    \vspace{-20pt}
    \caption{\textbf{Access latency.} Average latency for single AVX512 load (ld), store and write back (st+wb), non-temporal store (nt-st), and sequential pointer chasing in 1GB space(ptr-chase). Prefetching at all levels are disabled in all cases.}
    \label{fig:seq_latency}
    \vspace{-10pt}
\end{figure}
In a latency test, \arch starts by flushing the cacheline at the tested address and immediately issues a \texttt{mfence}. 
%
Then, \arch issues a set of \texttt{nop} instructions to flush the CPU pipeline. 
%
When testing with load instructions, we record the time it takes to access the flushed-out cacheline;
%
when testing with store instructions, we record the time it takes to do temporal store then a cacheline write back (\texttt{clwb}), or the execution time of non-temporal store, followed by a \texttt{sfence}. 
%
Additionally, we tested the average access latency through pointer chasing in a large memory space while all prefetching is disabled. Figure \figref{fig:seq_latency} shows the latency across the four tested instructions, average pointer chasing latency with 1GB of memory space, and pointer chasing latency within different working set sizes. 
%

Our results (\figref{fig:seq_latency}) show CXL memory access latency is about 2.2$\times$ higher than the 8-channel local-socket-DDR5 (DDR5-L8) accesses, while the single channel remote-socket DDR5 (DDR5-R1) is 1$\times$ $\sim$ 2.5$\times$ higher than that of DDR5-L8.
%
According to the prior study on persistent memory~\cite{eurosys22xiang}, accessing a recently flushed cacheline could incur a higher latency than normal cache-miss accesses due to extra cache coherence handshake for such flushed cachelines. 
%
Pointer chasing reflects a more realistic access latency experienced by applications. 
%
In \arch, the working set is first brought into the cache hierarchy in a warm-up run.
%
The result in \figref{fig:seq_latency} shows that pointer chasing in CXL memory has 3.7$\times$ higher latency than that of DDR5-L8 access. 
%
The pointer chasing latency on CXL memory is 2.2$\times$ higher than that of DDR5-R1 accesses. 
%
%
%Furthermore, given that pointer chasing involves single-threaded and dependent loads, the number of memory channels has little to no impact on its performance. 
%


It is worth noting that CXL memory's longer access latency can be partly attributed to its FPGA implementation, despite the CXL controller and DDR4 memory controller being hardened on the FPGA chip.
%
Although we anticipate that an ASIC implementation of the CXL memory device will result in improved latency, we maintain that it will still be higher than that of regular cross-NUMA access, primarily due to the overhead associated with the CXL protocol.
%We believe most of the CXL memory latency is attributed to 400MHz clock that drives the CXL IP on the FPGA. 
%
Additionally, our real-application profiling in \secref{sec:app} revealed a decrease in latency penalty depending on the specific characteristics of the application. 
%
It should also be noted that the benefits of FPGA-based CXL devices lie in their ability to add (inline) accelerating logic on the CXL memory data path, as well as to offload memory-intensive tasks in a near-memory fashion.
%
%We expect an ASIC implementation of the CXL memory controller will deliver a much better latency and will be widely deployed in the near future. 
%


On the other hand, non-temporal store instructions with \texttt{sfence} on CXL memory have notably lower latency than cacheline writeback after temporal store. 
%
Such latency difference is due to the read-for-ownership (RFO) behavior in CXL's MESI cache coherence protocol, where cachelines are loaded into the cache for each store miss. 
%
This difference in access latency is later translated into bandwidth difference, which will be discussed in \secref{sec:bandwidth}.
%
%We expect reduction in the temporal store latency when we test an ASIC CXL memory device in the standard type3 flow.


\subsection{Bandwidth Analysis}
\label{sec:bandwidth}
In our bandwidth test, \arch performs blocks of sequential or random access within each testing thread. The main program calculates the average bandwidth for a fixed interval by summing the number of bytes accessed. To facilitate a fair comparison of memory channel count, we tested remote-socket DDR5 with only 1 memory channel (DDR5-R1) alongside CXL memory.

\subsubsection{Sequential Access Pattern}
\label{sec:sequential_bandwidth}
\begin{figure*}[!t]
    \centering
    \begin{subfigure}[b]{0.34\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figs/micro_dram_seq.pdf}
        \caption{DDR5-L8}
        \label{fig:seq_dram}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.32\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figs/micro_cxl_seq.pdf}
        \caption{CXL memory}
        \label{fig:seq_cxl}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.32\textwidth}
        \centering
        %\includegraphics[width=\linewidth]{figs/micro_movdir64B_seq.pdf}
        \includegraphics[width=\linewidth]{figs/micro_remote_seq.pdf}
        \caption{DDR5-R1}
        \label{fig:seq_ddr5-r1}
    \end{subfigure}
    \vspace{-5pt}
    \caption{\textbf{Sequential access bandwidth.} An experiment showing the maximum possible bandwidth on local-socket DDR5 with 8-channels (a), and CXL memory (b), and remote-socket DDR5 with 1 channel (c). The grey dash line in (b) shows the theoretical max speed of DDR4-2666MT/s} 
    \label{fig:seq_bandwidth}
    \vspace{-10pt}
\end{figure*}

% \begin{figure}
%     \centering
%     \includegraphics{figs/micro_data_move.pdf}
%     \caption{}
%     \label{fig:my_label}
% \end{figure}

\begin{figure*}
    \centering
    \begin{center}
        \begin{subfigure}[b]{0.48\textwidth}
            \centering
            \includegraphics[width=\linewidth]{figs/micro_movdir64B_seq.pdf}
            \caption{\texttt{movdir64B}}
            \label{fig:movdir64B}
        \end{subfigure}
        \hfill
        \begin{subfigure}[b]{0.49\textwidth}
            \centering
            \includegraphics[width=\linewidth]{figs/micro_data_move.pdf}
            \caption{data copy}
            \label{fig:dsa}
        \end{subfigure}
    \end{center}
    \vspace{-10pt}
    \caption{\textbf{Data movement bandwidth.} An experiment showing data movement efficiency under different workloads. D2C is short for local-DDR5 to CXL memory. All experiments in (b) is done with a single thread.}
    \vspace{-10pt}
\end{figure*}
Sequential access reflects the maximum possible throughput of the memory scheme under certain operation, and the result is shown in \figref{fig:seq_bandwidth}. %, and it is mostly aligned with tests done on other studies~\cite{fast20yang}.
%
During testing with the DDR5-L8, we observed that the load bandwidth scaled linearly until it peaked at the maximum bandwidth of 221 GB/s with approximately 26 threads. In comparison, non-temporal store instructions reached their maximum bandwidth at 170 GB/s, which is lower than that of the load instructions but with a lower thread count of approximately 16.
%

In contrast, CXL memory exhibits a distinct bandwidth trend when compared to DDR5-L8. 
%
Specifically, with load instructions, CXL memory attains its maximum bandwidth with approximately 8 threads, but this value drops to 16.8 GB/s when we increase the thread count beyond 12 threads. 
%
Non-temporal stores, on the other hand, demonstrate an impressive maximum bandwidth of 22 GB/s with only 2 threads, which is close to the maximum theoretical bandwidth of the tested DRAM. 
%
However, this bandwidth drops immediately as we increase the thread count, indicating some interference on the FPGA memory controller.

The temporal-store bandwidth of CXL memory is significantly lower than that of non-temporal stores, which aligns with the high latency of temporal stores reported in \figref{sec:latency}. 
%
This discrepancy is due to the RFO behavior in temporal stores described in \secref{sec:latency}, which significantly reduces the transfer efficiency of CXL memory. 
%
This reduction is because RFO requires extra core resources and additional flit round trips for both loading and evicting a cache line compared to non-temporal stores.
%

Additionally, \figref{fig:seq_ddr5-r1} show that DDR5-R1's sequential-access performance is similar to CXL memory. 
%
With the benefit of higher transfer rate and lower latency in both DDR5 and the UPI interconnect, DDR5-R1 shows a higher throughput in loads and non-temporal stores, but similar throughput in temporal stores.
%

In addition to the aforementioned instructions, we also tested a new x86 instruction, \texttt{movdir64B}~\cite{intelmanual}, which is newly available on SPR. This instruction moves a 64B data from the source memory address to a destination memory address and explicitly bypasses the cache for both loading the source and storing it to the destination. As shown in \figref{fig:movdir64B}, our results indicate that D2* operations exhibit similar behavior, while C2* operations show lower throughput in general.
%
From these results, we can conclude that the slower load from CXL memory leads to the lower throughput in \texttt{movdir64B}, and this is even more true in the case of C2C.


% \yan{mention DSA here}
New to SPR, the Intel Data Streaming Accelerator (Intel DSA) enables memory moving operations to be offloaded from the host processor. 
%
Intel DSA is comprised of work queues (WQs) to hold offloaded work descriptors, and processing engines (PEs) to pull descriptors from the WQs to operate on. 
%
Descriptors can be sent synchronously by waiting for each offloaded descriptor to complete before offloading another, or asynchronously by continuously sending descriptors resulting in WQs having many in-flight. 
%
With programs designed to best use Intel DSA asynchronously, higher throughput can be achieved. To further improve throughput, operations can be batched to amortize the offload latency. 
%
\figref{fig:dsa} shows maximum throughput observed performing memory copy operations on the host processor via \texttt{memcpy()} or \texttt{movdir64B}, and synchronously/asynchronously using Intel DSA with varying batch sizes (\eg 1, 16, and 128). 
%
While a non-batched synchronous offload to Intel DSA matches the throughput of non-offloaded memory copying, any level of asynchronicity or batching brings improvements. 
%
Additionally, splitting the source and destination data locations yields higher throughput than exclusively using CXL attached memory, with the C2D case reporting higher throughput due to lower write latency on DRAM.


During our bandwidth analysis, we observed instances where increasing the thread count led to a decrease in bandwidth. While data accesses were sequential within each working thread, the memory controller between the CXL controller and the extended DRAM received requests with fewer patterns as the thread count increased. As a result, the performance of CXL memory was hindered.

\subsubsection{Random Access Pattern}
\label{sec:random_bandwidth}
\begin{figure*}
    \centering
    \includegraphics[width=\linewidth]{figs/micro_block_rand_all.pdf}
    \vspace{-15pt}
    \caption{\textbf{Random block access bandwidth.} Row order (top to bottom): local-socket DDR5, CXL memory, remote-socket DDR5. Column order (left to right): load, store, nt-store. Thread count is denoted in the top legend}
    \label{fig:rand_bandwidth}
    \vspace{-10pt}
\end{figure*}
To evaluate the performance of \arch for random block access, we issue a block of AVX-512 access sequentially, but each time with a random offset. 
%
This approach enables us to measure the system's performance under realistic working conditions where data access patterns are unpredictable. 
%
As we increase the block size across the tested thread counts, the memory access pattern will converge to sequential access where both the CPU cache and the memory controller can enhance the overall bandwidth. 
%
To ensure write order in block level, we issue a \texttt{sfence} after each block of \texttt{nt-store}. 
%
The result for random block access is shown in \figref{fig:rand_bandwidth}.
%

We observe similar patterns across the random block loads when the block size is small (1KB), with all three memory schemes suffering equally from random accesses. 
%
However, as we increase the block size to 16KB, a major difference emerges between DDR5-L8 and DDR5-R1/CXL memory. 
%
DDR5-L8's bandwidth scales sub-linearly with thread count, while DDR5-R1 and CXL memory benefit less from higher thread count (after 4 threads), and this is even more apparent in CXL memory. 
%
The memory channel count plays a crucial role, with DDR5-R1 and our CXL memory device having only one memory channel, while DDR5-L8 has eight channels in total. 
%
Random block stores exhibit a similar pattern to loads in terms of thread count, but with the added trend that bandwidth stops scaling with block size.

The behavior of random block non-temporal stores in CXL memory shows an interesting trend compared to all other tested workloads. Single-threaded \texttt{nt-store} scales nicely with block size, while higher thread counts experience a drop in throughput after reaching some sweet spots of block size and thread count. For instance, the 2-thread bandwidth reaches its peak when the block size is 32KB, and the 4-thread bandwidth peaks at a block size of 16KB.

We believe that this sweet spot is determined by the memory buffer inside the CXL memory device. Unlike regular \texttt{Store} instructions, \texttt{nt-store} does not occupy tracking resources in the CPU core. Therefore, it is easier to have more on-the-fly \texttt{nt-store} instructions at the same time, which may lead to buffer overflow in the CXL memory device.

Despite this, non-temporal instructions have the advantage of avoiding RFO (\secref{sec:latency}) and cache pollution, making them more attractive in the CXL memory setting. Programmers who wish to use \texttt{nt-store} should keep this behavior in mind to make the most out of \texttt{nt-stores} in CXL memory.

\subsection{Comparison with Emulation Using NUMA Systems}

Recent work on CXL memory has usually been conducted by emulating the access latency of CXL memory through cross-NUMA memory access, i.e., by imposing additional latency to main memory accesses~\cite{pond, tpp}. However, according to our observations, cross-NUMA emulation model cannot precisely mimic the following characteristics of CXL memory: 
(1) the impact of limited bandwidth in current CXL devices (unless remote-socket DIMMs are populated with the same number of channels as the CXL memory counterpart),
(2) a variety of CXL memory implementations that have higher latency than cross-NUMA accesses (the impact of higher latency becomes more severe in the latency-bounded applications, \S~\ref{sec:ycsb}), and
(3) data transfer efficiency under different workloads (i.e., load and non-temporal store bandwidth, \S~\ref{sec:sequential_bandwidth}).
%\ipoom{what is the intention of the third statement? (compared to the first one)}\yan{I changed it to seperate latency / bandwidth}

% In conclusion, random access bandwidth only scales with higher thread count when there are enough memory channels. In the current setup, single-threaded CXL memory performance is inevitably worse than both SNC and DRAM due to higher access latency and , while its multi-threaded performance has a similar trend


%\yan{Interference removed for now}
% \subsection{Interference}
% A quick glance at the bandwidth and the latency statistic might lead to a conclusion that applications should use non-temporal stores over temporal stores in CXL memory. However, we did an experiment on whether CXL memory accesses would affect DRAM accesses and found that interference indeed exists between the two. To see the effects of such interference, we launch two instances of \arch and pin each threads within each instances to isolated cores. The first instance will run bandwidth test on DRAM, and the second instance runs bandwidth test on CXL memory. The second instance is delayed for some fixed interval, and we will get (1) DRAM bandwidth, (2) overlapped bandwidth, (3) CXL memory bandwidth from this experiment. \tabref{table:interference} shows the bandwidth of these three steps.

% \yan{probably include D-* + C-ld interference, after talking to Intel CPU architects}


% \begin{table}[!tb]
% \centering
%     \caption{Bandwidth (GB/s) when co-running CXL memory and DRAM test with \arch. BW1 / BW2 column shows the bandwidth when the two instance of \arch overlaps. D-ld + C-ntst refers to running \arch with load on DRAM, then starts non-temporal store on CXL}
%     \begin{tabular}{cccc}
%         \toprule
%         Workload & BW1  & BW1 / BW2 & BW2 \\ 
%         \midrule
%         D-ld + C-ntst   & 198 & 7.60 / 13.6 & 13.5  \\
%         D-st + C-ntst   & 80.2 & 7.07 / 13.6 & 13.5  \\
%         D-ntst + C-ntst & 164 & 13.8 / 13.6 & 13.5  \\
%         D-ld + D-ntst   & 198 & 44.0 / 132 & 164  \\
%         \bottomrule
%     \end{tabular}
%     \label{table:interference}
%     \end{table}
% %\vspace{-10pt}

% Our result in \tabref{table:interference} shows that: (1) Running batches of non-temporal store instruction will have high interference with temporal instructions, regardless of the memory source or destination. (2) The interference is much more prevalent in the case of running non-temporal stores with CXL, where the DRAM bandwidth drops to staggering 7-13 GB/s. (3) When both instance of \arch runs non-temporal stores, DRAM bandwidth seems to reduce to the same level as the CXL bandwidth (13 GB/s). These observations matches the description of non-temporal store instructions in Intel's document~\cite[section 2.4.2]{intelmanual}, which states that issuing too many non-temporal instruction will make the processor run out of resource, causing stalls, and limit write bandwidth. Despite the benefit of by passing the cache, our result shows that non-temporal instructions, especially used with CXL memory, must be issued properly such that DRAM accesses are not affected. \yan{TODO, find sweet spot}

