\section{Real-World Applications}
\label{sec:app}
To study the performance impact of CXL memory, we explore the CXL memory by binding the application's memory fully or partially to the CXL memory. 
%
Linux offers the \texttt{numactl} program which allows user to (1) bind a program to a specific memory node (\texttt{membind} mode), or (2) prioritize allocation to a memory node and only allocate memory to other nodes when the specified node runs out of memory (\texttt{preferred} mode), or (3) have the allocation spread evenly across a set of nodes (\texttt{interleaved} mode).

One of the recent patches in the Linux Kernel now allows for fine-grained control over the page interleaving ratio between memory nodes~\cite{mn_il_patch}. 
%
This means that, for example, we can allocate 20\% of memory to CXL memory if we set the DRAM:CXL ratio to 4:1. 
%
To investigate the impact of CXL memory on application performance, we tuned this interleaving ratio for some applications. 
%
In addition, we disabled NUMA balancing to prevent page migration to DRAM.

The performance of these applications using this heterogeneous memory scheme should serve as a baseline for most memory tiering policies. This is because the proposed optimization should, at the very least, perform equally well when compared against a weighted round-robin allocation strategy.

%\yan{Skip mentioning LLC for now}
% LLC miss rate is an important metric for characterizing CXL memory's performance, as they attribute most to the difference in application's performance. 
% %
% However, when running the same application and settings across CXL and DRAM, we found that the LLC load and store counts are similar, while CXL has significantly lower LLC miss, in both loads and stores.
% %
% For the rest of this section, we assume that the LLC miss rate is the same when comparing applications that run on CXL and DRAM, and the LLC miss rate is extracted from running application on DRAM. 

\subsection{Redis-YCSB}
\label{sec:ycsb}
\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figs/ycsb_tail_only.pdf}
    \vspace{-20pt}
    \caption{\textbf{Redis p99 latency.}  Testing with YCSB workload A (50\% Read, 50\% Update). 50/100\%-R/U in the legend denotes the \textbf{read/update} latency when \textbf{50/100\%} of the Redis memory runs on CXL memory} 
    \label{fig:ycsb_tail}
    \vspace{-10pt}
\end{figure}
\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figs/ycsb_max_qps_only.pdf}
    \vspace{-20pt}
    \caption{\textbf{Redis max QPS.} Testing max sustainable Redis QPS with various CXL memory configuration. The legend denotes percent of Redis memory allocated to CXL memory. YCSB workload D defaults to read the most recently inserted elements \textbf{(lat)}, but we also tested this workload with read request in Zipfian \textbf{(zipf)} or uniform \textbf{(uni)} distribution to see the effect on access locality. Workload E is omitted here as it is range query.}
    \label{fig:ycsb_max_qps}
    \vspace{-15pt}
\end{figure}
Redis is a high-performance in-memory key-value store popular and widely used in the industry.
%
We use YCSB~\cite{socc10:ycsb} to test Redis' performance with different memory allocation schemes by pinning its memory to CXL memory, DRAM or spread across the two. 
%
To evaluate system performance, we conducted multiple workloads while throttling query per second (QPS) in the YCSB clients. 
%
Specifically, we measured two metrics: (1) the 99\textsuperscript{th} percentile tail latency among queries, and (2) the maximum sustainable QPS. 
%
With the exception of workload D, all workloads used a uniform distribution for requests, ensuring maximal stress on the memory. 
%
We also fine-tuned the interleave ratio (DRAM:CXL) to offload a certain amount of memory to CXL, with ratios such as 30:1 (3.23\%) and 9:1 (10\%) utilized in different experiments.

Our result in \figref{fig:ycsb_tail} shows that there exists a significant gap in p99 tail latency at low QPS (20k) when Redis runs purely on CXL memory. 
%
This gap remained relatively constant until 55k QPS, at which point the YCSB clients were not able to reach the targeted QPS, resulting in a sudden increase in tail latency.
%
When 50\% of Redis's memory was allocated to CXL memory, the p99 tail latency was between that of purely DRAM and purely CXL memory. Although the 50\%-CXL memory Redis did not saturate its QPS until 65k, the tail latency spiked around 55k.
%
Finally, the DRAM Redis exhibited a stable tail latency, and its QPS saturated at around 80k.


We believe the tail latency gap is attributed to the ultra-low response latency of Redis queries, making these $\mu$s-level responses' latency highly sensitive to memory access latency. 
%
This correlates well with our latency measurements presented in \secref{sec:latency}, where CXL memory access latency ranges from hundreds to one thousand nano-second, and it is 2-4x higher than that of DRAM. 
%
However, intermediate computations and cache hits reduce the latency difference, in terms of application tail latency, to about 2x before the QPS saturation point.


%
%
On the other hand, the maximum sustainable QPS that CXL memory Redis can deliver (\figref{fig:ycsb_max_qps}) is correlated to the random block access bandwidth observed in \secref{sec:random_bandwidth}, where CXL memory has a much lower single thread load/store bandwidth when compared to local-DDR5 or remote-DDR5. 
%

Single-thread random access bandwidth is bounded by the memory access latency, where data dependency within a single thread makes the load-store queues in the CPU hard to saturate.
%
Moreover, \figref{fig:ycsb_max_qps} shows the trend that having less memory allocated to CXL memory (lowered CXL memory percentage) delivers a higher max QPS across tested all workloads, but none of which can surpass the performance of running Redis purely on DRAM. 
%
In this case, memory interleaving is not able to improve a single application's performance as interleaving with CXL memory will always introduce higher access latency.
%
Note that the current CXL memory setup is FPGA-based, where its true merit lies in its flexibility. We expect an ASIC-based CXL memory will provide a relatively lower access latency, improving the performance of latency bounded applications.

%Keep in mind that the current CXL memory setup only has one memory channel, and the random access bandwidth is far from the max capability of DDR4; we expect the maximum deliverable QPS to scale linearly as we increase the number of the memory channels, and with an ASIC implementation of the CXL memory.

%\yan{SNC is actually a good argument for latency bound}
%\yan{SNC result omitted for now}
% SNC, albeit having two memory channels, outperforms the 8-channel DRAM and CXL memory in all tested workloads, which indicates that (1) a single instance of Redis is not bounded by the number of memory channels, (2) throughput drop when Redis runs on CXL memory is mainly attributed to the single-thread throughput difference in small random block access shown in \secref{sec:random_bandwidth}.
%
%


%\yan{LLC analysis removed for now (workload D analysis for different request distribution)}
% Additionally, \figref{fig:ycsb_max_qps} shows that CXL memory Redis is more sensitive to LLC load misses. 
% %
% When comparing the relative throughput drop within the three request distribution of workload d, Redis on 100\% CXL memory perceives the highest throughput penalty as the LLC load miss rate increases with the change in request distribution.
%
% Comparing the throughput penalty across all workloads with uniformly distributed request space, the throughput drop is inversely proportional to the LLC miss rate Redis on CXL memory. \yan{Need better description of this behavior}

% ============================================= DLRM 
\subsection{Embedding Reduction in DLRM}
\label{sec:dlrm}

\begin{figure}
    \includegraphics[width=\linewidth]{figs/merci_group0.pdf}
    \vspace{-10pt}
    \caption{\textbf{DLRM embedding-reduction throughput.} Testing with 8-channel DRAM and CXL memory; throughput vs. thread count (left); throughput of different memory schemes normalized to DRAM at 32 threads (right)} 
    \label{fig:embedding_reduction_spr}
    \vspace{-15pt}
\end{figure}

\begin{figure*}[!t]
    \includegraphics[width=\linewidth]{figs/merci_mn_snc.pdf}
    \vspace{-15pt}
    \caption{\textbf{DLRM embedding-reduction throughput} } 
    \label{fig:embedding_reduction_snc}
    \vspace{-10pt}
\end{figure*}

Deep learning recommendation models (DLRM) have been widely deployed in the industry. Embedding reduction, a step within the DLRM inference, is known to have a high memory footprint and occupies 50\% to 70\% of the inference latency~\cite{asplos21lee}. We tested embedding reduction on DRAM, CXL memory, and interleaved-memory with the same setup as MERCI~\cite{asplos21lee}. 

Result in \figref{fig:embedding_reduction_spr} shows that running DLRM inference on each scheme scales linearly and differ by the slope, as thread count increases.
%
% Meanwhile, when having embedding reduction run purely on CXL memory or DDR5-R1, the inference throughput saturates at around 20 threads, due to the limited random access bandwidth provided by two memory channels. 
%
The overall trend of DDR5-R1 and CXL memory is similar, which aligns with the observations in \secref{sec:random_bandwidth}, where DDR-R1 and CXL memory has a similar random load/store bandwidth when the access granularity is small.
%
Two points of memory interleaving (3.23\% and 50\% on CXL memory) are shown in \figref{fig:embedding_reduction_spr}. As we reduce the amount of memory interleaved to CXL, inference throughput increases. However, we again observed that even when having 3.23\% of memory allocated to CXL, the inference throughput can't match that of purely running on DRAM. 
%
Also note that the pure-DRAM inference throughput scales linearly, and its linear trend seems to extend beyond 32 threads. 
%
Combining these two observations, we can conclude that the 8-channel DDR5 memory can sustain DLRM inference beyond 32 threads. 

To demonstrate a scenario where an application is bounded by the memory bandwidth, we tested the inference throughput in the SNC mode. 
%
Recall that Intel introduced the sub-NUMA clustering feature in SPR, where the chiplets are split into four individual NUMA nodes, and the memory controllers on each NUMA node work independently of the other nodes. 
%
By running inference on one SNC node, we are effective limiting the inference to run on two DDR5 channels, making it memory bounded.

%
Figure \ref{fig:embedding_reduction_snc} illustrates the results of running inference on the SNC mode, with CXL memory interleaved in the same way as in all previous experiments. 
%
The green bar in the figure shows the inference throughput on SNC, which stops scaling linearly after 24 threads. 
%
At 28 threads, the inference is limited by the two memory channels, and interleaving memory to CXL yields a slightly higher inference throughput.
%
This trend persists, and at 32 threads, putting 20\% of memory on CXL increases the inference throughput by 11\% compared to the SNC case. In the future, we anticipate that CXL devices will have a bandwidth that is comparable to native DRAM, which will further enhance the throughput of memory bandwidth-bound applications.


% ============================================= DSB
\subsection{DeathStarBench}
\label{sec:dsb}

\begin{figure*}[!t]
    \centering
    \includegraphics[width=\linewidth]{figs/dsb_group0.pdf}
    \vspace{-15pt}
    \caption{\textbf{DeathStarBench p99 latency and memory breakdown.} Workloads: compose post (left), read user timeline (mid-left), mixed workload (mid-right), memory breakdown by functionality (right). The mixed workload has 60\% read-home-timeline, 30\% read-user-timeline, and 10\% composing-post.}
    \label{fig:dsb}
    \vspace{-15pt}
\end{figure*}
DeathStarBench (DSB)~\cite{deathstartbench} is an open-source benchmark suite designed to evaluate the performance of microservices on a system. It uses Docker to launch components of a microservice, including machine learning inference logic, web backend, load balancer, caching, and storage.
%
DSB provides three individual workloads and a mixed workload for a social network framework. 
%
Figure \ref{fig:dsb} shows the 99\textsuperscript{th} percentile tail latency for composing a post, reading user timeline, and the mixed workload. 
%
We omitted the results for reading home timeline as it does not operate on the databases, and hence, is indifferent to the memory type used for databases.
%
In our experiment, we pinned the components with high working set size (i.e., the storage and caching applications) to either DDR5-L8 or CXL memory. 
%
We left the computation-intensive parts to run purely on DDR5-L8. The memory breakdown of these components is shown in Figure \ref{fig:dsb}.
%

The results in Figure \ref{fig:dsb} show that there is a tail latency difference in the case of composing posts, while there is little to no difference in the case of reading user timeline and the mixed workload. Note that the tail latency in DSB is at the millisecond level, which is much higher than that of YCSB-Redis.

As we analyzed the trace of composing posts and reading user timeline, we found that composing posts involve more database operations, which puts a heavier load on the CXL memory. Meanwhile, most of the response time in reading user timeline is spent on the \texttt{nginx} front end. This allows the longer CXL memory access latency to be amortized among the computation-intensive components, making tail latency much less dependent on the databases' access latency.

Finally, the mixed workload shows a simulated use case of a real social network, where overall, most users read the posts composed by some users. Although in the mixed workload, having the database pinned to CXL memory shows a slightly higher latency as the QPS increases, the overall saturation point is similar to running the database on DDR5-L8.


The result from DSB offers an interesting use case of CXL memory, that as long as computation-intensive components are kept in DRAM, caching and storage component that operates on a low demand rate may be assigned to the slower CXL memory, and the application's performance remains mostly the same.

% \subsection{Fleetbench}
% \begin{figure}
%     \includegraphics[width=\linewidth]{figs/hot_swissmap_ddr_vs_cxl.pdf}
%     \vspace{-10pt}
%     \caption{dummy figure for fleetbench}
%     \label{fig:hot_swissmap}
%     \vspace{-15pt}
% \end{figure}
% Fleetbench~\cite{fleetbench} is a open-sourced benchmark release by Google. The benchmark tests hot functions and commonly accessed data structures across Google's fleet. Swiss table~\cite{swiss-table} is an optimized hash table developed by Google. \figref{fig:hot_swissmap} shows the latency of finding an void key in a swiss table when running on CXL memory or DRAM.

%\yan{Too much uncertainty, skip fleetbench for now}


%\subsection{Django-workload}
% https://github.com/facebookarchive/django-workload
% suggested by Meta, no data yet

%\subsection{HPCCG}
% 
% suggested by Meta, no data yet

%\subsection{MiniFE}
% suggested by Meta, no data yet


%\yan{Remove memory performance modeling for now (LLC miss rate related)}
% \subsection{Memory Performance Modeling}
% Prior works~\cite{sac18oh,iiswc15clapp} have extensively studied the relationship between the workload characteristics and their sensitivity to memory bandwidth and latency. These works propose mathematical models that divide the effective performance into on-chip performance and off-chip memory accesses~\cite{sac18oh,iiswc15clapp}. According to these models, ... 
