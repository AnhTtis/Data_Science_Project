\begin{figure*}[t]
    \includegraphics[width=\textwidth]{figs/latency_app_legend.pdf}
\end{figure*}
\begin{figure*}[t]
    \vspace{-46pt}
     \begin{subfigure}[b]{0.24\textwidth}
         \centering
         \includegraphics[width=\linewidth]{figs/ycsb_tail_only.pdf}
         \caption{[Redis] YCSB-A (RD:UPD = 5:5)}
         \label{fig:ycsb_redis}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.24\textwidth}
         \centering
         \includegraphics[width=\linewidth]{figs/dsb_compose.pdf}
         \caption{[DSB] compose posts}
         \label{fig:dsb_compose}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.24\textwidth}
         \centering
         \includegraphics[width=\linewidth]{figs/dsb_read_user.pdf}
         \caption{[DSB] read user timelines}
         \label{fig:dsb_read_user}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.24\textwidth}
         \centering
         \includegraphics[width=\linewidth]{figs/dsb_mixed.pdf}
         \caption{[DSB] mixed workloads}
         \label{fig:dsb_mixed}
     \end{subfigure}
    \vspace{-6pt}
    \caption{99\textsuperscript{th}-percentile (p99) latency values of \texttt{Redis} with various percentages of pages allocated to \cxlmem and \texttt{DSB} with 100\% of only `caching and storage' pages allocated to either \cxlmem or DDR memory.  
    %varying page interleaving ratios between DDR and \cxlmem devices.%\yan{All figures are up-to-date.} \yan{p99 gap is gone for 6b, TODO, need to re-explain} \yan{6D seems to show CXL offers some benefit (offloading DRAM pressure) -- re-tested, seems to show the same pattern} %; each bar in \figref{fig:ycsb_redis} plots the difference between targeted and delivered QPS values.
    %For this experiment, we enable the SNC mode to use two local DDR5 channels and one CXL channel to balance the bandwidth ratio between DDR5 and CXL.
    }
    %, following a guideline from Intel 
     
    %The lines in \figref{fig:ycsb_redis} are p99 latency values, and each bar is the difference between targeted and delivered requests.}
    %} 
    \label{fig:app:latency}
    %\vspace{-6pt}
\end{figure*}


\section{Impact of Using CXL Memory on Application Performance}
\label{sec:app}

To study the impact of using \cxlmem on the performance of applications (\S\ref{sec:setup:benchmark}), we take CXL-A, which provides the most balanced latency and bandwidth characteristics among the three \cxlmem devices.
% check
We use \texttt{numactl} in Linux to allocate memory pages of a given program, either fully or partially, to \cxlmem, 
%
%\cxlmem by binding the memory space demanded by a given application fully or partially with the \cxlmem space. 
% check
%For such allocation of pages to \cxlmem, 
%we use \texttt{numactl} in Linux, 
exploiting the fact that the OS recognizes \cxlmem as memory in a remote NUMA node by the OS.
% check
Specifically, \texttt{numactl} allows users to: (1) bind a program to a specific memory node (\texttt{membind} mode); (2) allocate memory pages to the local NUMA node first, and then other remote NUMA nodes only when the local NUMA node runs out of memory space (\texttt{preferred} mode); or (3) allocate memory pages evenly across a set of nodes (\texttt{interleaved} mode). %, \ie, allocating 50\% of memory pages to CXL and DDR5-L, respectively.
% check
A recent Linux kernel patch~\cite{mn_il_patch} enhances the \texttt{interleaved} mode to facilitate fine-grained control over the allocation of a specific percentage of pages to a chosen NUMA node. % (\eg, local DDR memory and \cxlmem). 
% check
For example, we can change the percentage of pages allocated to \cxlmem from the default 50\% to 25\%.
%of pages to \cxlmem, 
That is, 75\% of pages are allocated to local DDR5 memory. %adjusting the page allocation ratio between DDR and CXL from 1:1 to 3:1. 
% check

In this section, we will vary 
%the page allocation ratios between DDR memory and 
the percentage of pages allocated to
\cxlmem and analyze 
%the impact of these ratios 
its impact on performance using application-specific performance
%metrics. 
metrics,
% check
%This will set 
setting
the stage for our \cxlmem-aware dynamic page allocation policy (\S\ref{sec:policy}).
% check
Note that we enable the SNC mode to use only two local DDR5 memory channels along with one \cxlmem channel. 
% check
This is because our system can accommodate only one \cxlmem device, and it needs to make a meaningful contribution to the total bandwidth of the system. 
% check
In such a setup, the local DDR5 memory with two channels 
provides $\sim$2$\times$ higher bandwidth for \texttt{st} and $\sim$3.4$\times$ higher bandwidth for \texttt{ld} than \cxlmem. 
% check
%Still, our setup is aligned with the optimal DDR:CXL ratio suggested by multiple hardware manufacturers. %~\cite{intel-hetero-cxl, 10.1145/3533737.3535090}.
% check
As future platforms accommodate more \cxlmem devices, we may connect up to four \cxlmem devices to a single CPU socket with eight DDR5 memory channels, providing the same DDR5 to CXL channel ratio as our setup.
% check


\subsection{Latency}
\label{sec:app:latency}

\niparagraph{Redis.}
%
\figref{fig:ycsb_redis} shows the 99$^{th}$-percentile (p99) latency  values of \texttt{Redis} with \texttt{YCSB} workload \texttt{A} (50\% read and 50\% update) while varying the percentage of pages allocated to 
%page allocation ratio between 
\cxlmem or local DDR5 memory (referred to as DDR memory hereafter).
% check
First, allocating 100\% of pages to \cxlmem (CXL 100\%) significantly increases the p99 latency compared to allocating 100\% of pages to DDR memory (DDR 100\%), especially at high target QPS values.
% check
For example, CXL 100\% results in 10\%, 73\%, and 105\% higher p99 latency than DDR 100\% at 25~K, 45~K, and 85~K target QPS, respectively.
% check
Second, as more pages are allocated to \cxlmem, the p99 latency increases proportionally.
% 
For instance, at 85~K target QPS, allocating 25\%, 50\%, and 75\% of pages to \cxlmem results in p99 latency increases of 9\%, 23\%, and 45\%, respectively, compared to DDR 100\%.
% check
Finally, as expected, allocating 100\% of pages to DDR memory results in the lowest and most stable p99 latency. % until the targeted QPS reaches 100~K. 
% check
Explaining the substantial difference in the p99 latency values for various percentages of pages allocated to \cxlmem and different target QPS values, we note that \texttt{Redis} typically operates with response time at a $\mu$s scale, making it highly sensitive to memory access latency (\S\ref{sec:setup:benchmark}).
% check
Therefore, allocating more pages to \cxlmem and/or increasing the target QPS makes \texttt{Redis} more frequently access \cxlmem with almost 2$\times$ longer latency than DDR memory, resulting in higher p99 latency.
% check
\begin{comment}
Note that \texttt{Redis} begins to drop requests to limit the increase in p99 latency  at high QPS.
% check
To capture the impact of allocating more pages to \cxlmem on both p99 latency and throughput, we also measure the difference between various targeted QPS and delivered QPS. 
% check
This shows that the throughput of \texttt{Redis} saturates at lower QPS as more pages allocated to \cxlmem. 
% check
For example, when allocating 25\%, 50\%, and 75\% of pages to \cxlmem, we observe that the throughput saturates approximately at 104~K, 96~K, and 88~K, respectively.
% check
\end{comment}

\niparagraph{Redis+TPP.} We conduct an experiment to assess whether the most recent transparent page placement (TPP)~\cite{tpp} can minimize the impact of using \cxlmem on the p99 latency.
% check
The most recent publicly available release~\cite{tpp-patch} only offers an enhanced page migration policy, and it does not automatically place pages in \cxlmem.
% check
Thus, we begin by allocating 100\% of pages requested by \texttt{Redis} to \cxlmem and let TPP automatically migrate pages to DDR memory until the percentage of the pages allocated to \cxlmem becomes 25\%, based on the DDR to CXL bandwidth ratio in our setup. 
% check
Then we measure the latency values of \texttt{Redis}. 
% check
\figref{fig:app:latency:tpp} compares two distributions of the measured latency values. 
% check
The first one is from using TPP, while the second one is from statically allocating 25\% of pages to \cxlmem.
%and \cxlmem with the 75:25 ratio.
% check
%latency values over time when we use TPP along with p99 latency values from 100\% and 75\% of pages statically allocated to DDR5, respectively.  
\begin{figure}[!t]
    \centering
    %\vspace{-12pt}
    \includegraphics[width=0.7\linewidth]{figs/tpp_balance_1_5.19_il_snc__distro_stable200.pdf}
    \vspace{-12pt}
    \caption{Impact of TPP on latency of  \texttt{Redis} compared with statically allocating 
    %a random static allocation 
    25\% of (random) pages to \cxlmem. We show the distributions up to the p99 latency.}
    \label{fig:app:latency:tpp}
    \vspace{-6pt}
\end{figure}

%
TPP migrates a large number of pages to DDR memory in the beginning phase, requiring the CPU to (1) copy pages from one memory device to another and (2) update the OS page table entries associated with the migrated pages~\cite{10.1145/3297858.3304024}. 
%
Since (1) and (2) incur high overheads, we measure the p99 latency only after 75\% of pages are migrated to DDR memory.
% check
As shown in \figref{fig:app:latency:tpp}, TPP generally gives higher latency, resulting in 174\% higher p99 latency than statically allocating 25\% of pages to \cxlmem. 
% check
This is because TPP constantly migrates a small percentage of pages between DDR memory and \cxlmem over time, based on its metric assessing hotness/coldness of the pages.
% check
Although TPP has a feature that reduces ping-pong behavior (\ie, pages are constantly being promoted and demoted between DDR memory and \cxlmem), migrating pages incurs the overheads from (1) and (2) above. 
% check
(1) blocks the memory controllers from serving urgent memory read requests from latency-sensitive applications~\cite{10.1145/3243176.3243191}, and (2) also requires a considerable number of CPU cycles and memory accesses.
% check


\niparagraph{DSB.} \figref{fig:dsb_compose}--\ref{fig:dsb_mixed} present the p99 latency values of (b) \texttt{compose posts}, (c) \texttt{read user timelines}, and (d) \texttt{mixed workloads}. 
%
Table~\ref{table:dsb-components} summarizes the components of the benchmarks, their working set sizes and characteristics, and allocated memory devices.
%
In our experiment, we allocate 100\% of the pages pertinent to the caching and storage components with large working sets 
to either DDR memory (DDR 100\%) or \cxlmem (CXL 100\%). %, because these components can benefit from the capacity expanded by \cxlmem. 
%
Meanwhile, we always allocate 100\% of the pages associated with the remaining components, such as \texttt{nginx} front-end and analytic docker images (\eg, logic in Table~\ref{table:dsb-components}), to DDR memory, since these components are more sensitive to memory access latency than the caching and storage components. 
%
For example, \texttt{nginx} spends 60\% of CPU cycles on the CPU front-end, which is dominated by fetching instructions from memory~\cite{deathstartbench}. 
%
Therefore, %\texttt{nginx} is highly sensitive to memory access latency and 
pages of such components should be allocated to DDR memory. % along with the other memory-latency bounded components. 
%


\begin{table}[t]
\centering
    %\vspace{-6pt}
    \caption{Components of \texttt{DSB} social network benchmark.}
    \vspace{-8pt}
    \resizebox{\columnwidth}{!}{%
    \begin{tabular}{cccc}
        \toprule
        \textbf{Name}    & \textbf{Working set}  & \textbf{Intensiveness} & \textbf{Allocated mem. type} \\ 
        \midrule
        Frontend & 83~MB & Compute & DDR memory \\
        Logic & 208~MB & Compute & DDR memory \\
        Caching \& Storage & 628~MB & Memory & CXL memory \\
        \bottomrule
    \end{tabular}
    }
    \label{table:dsb-components}
    \vspace{-12pt}
\end{table}


This experiment shows that all three benchmarks, \texttt{compose posts}, \texttt{read user timelines}, and \texttt{mixed workloads} are not sensitive to long latency of accessing \cxlmem as they exhibit little difference in p99 latency values between CXL 100\% and DDR 100\%.
%
This is because of two reasons.
% check
First, most of the p99 latency in these benchmarks is contributed by the front-end and/or logic components (\ie, DDR 100\%). 
%
This makes the latency of accessing \cxlmem amortized by the these components,  and thus the p99 latency is much less dependent on the latency of accessing databases (\ie, CXL 100\%).
%such as \texttt{nginx},
%(\ie, \texttt{MongoDB}, \texttt{Redis}, and \texttt{memcached})
% check
Second, the p99 latency of \texttt{DSB} is at a $m$s scale and two orders of magnitude longer than that of \texttt{Redis}. 
% check
Therefore, it is not as sensitive to memory access latency as that of \texttt{Redis}.
% check
%This makes the latency of accessing \cxlmem amortized by the latency-dominant components, such as \texttt{nginx}, and thus the p99 latency is much less dependent on the latency of accessing databases (\ie, \texttt{MongoDB}, \texttt{Redis}, and \texttt{memcached}).
%
%Meanwhile, the p99 latency of \texttt{compos posts} is dominated by accessing databases, making it more sensitive to long latency of accessing \cxlmem. 
%

Note that CXL 100\% provides lower p99 latency values than DDR 100\% for \texttt{mixed workloads} when the QPS range is between 5~K and 11~K.
% check
This is because \texttt{mixed workloads} is far more memory-bandwidth-intensive than \texttt{compose posts} and \texttt{read user timelines}.
% check
Specifically, when we measure the average bandwidth consumption by these three benchmarks in the QPS range that saturates the throughput of the benchmarks, we observe that \texttt{mixed workloads} consumes 32~GB/s while \texttt{compose posts} and \texttt{read user timelines} consume only 7~GB/s and 10~GB/s, respectively.
% check
When a given application consumes such high bandwidth in our setup, we observe that the application's throughput, which is inversely proportional to its latency, becomes sensitive to the bandwidth available for the application (\S\ref{sec:app:throughput}).
% check
Lastly, as the QPS approaches to 11~K, the compute capability of the CPU cores becomes the dominant bottleneck, leading to a decrease in the p99 latency gap between DDR 100\% and CXL 100\%. 
% check
%This analysis on \texttt{DSB} offers a compelling use case for \cxlmem. 
%
%That is, we allocate pages pertinent to latency-sensitive components of a given application to fast DDR memory, 
%while allocating pages associated with latency-insensitive components to slow \cxlmem. 
%


\begin{figure}[t]
    \centering
    %\vspace{-6pt}
    \includegraphics[width=\linewidth]{figs/fio_p99_4GB_log_2.pdf}
    \vspace{-15pt}
    \caption{p99 latency values of \texttt{FIO} for various block sizes, and percentage values of increase in p99 latency by allocating page cache to CXL.  
    %We use a \texttt{Zipfian} distribution to evaluate the impact of using page cache on file-system performance.
    } 
    \label{fig:app:latency:fio:log-scale} %\yan{TODO, DDR5-L in the fig}
    \vspace{-9pt}
\end{figure}


\niparagraph{FIO.} 
\figref{fig:app:latency:fio:log-scale} presents the p99 latency values of \texttt{FIO} with 4~GB page cache allocated to either DDR memory or \cxlmem for various I/O block sizes. We use a \texttt{Zipfian} distribution for \texttt{FIO} to evaluate the impact of using page cache on file-system performance.
% check
It shows that allocating the page cache to \cxlmem gives only $\sim$3\% longer p99 latency than DDR5 memory for 4~KB block size. 
% check
This is because the p99 latency for a 4~KB block size is primarily  dominated by the Linux kernel operations related to page cache management, such as context switching, page cache lookup, sending an I/O request through the file system, block layer, and device driver.
% check
However, with a 8~KB block size, the cost of Linux kernel operations is amortized, as multiple 4~KB pages are brought from a storage device by a single system call.
% check
Consequently, longer access latency of \cxlmem affects the p99 latency of \texttt{FIO} more notably, resulting in a $\sim$4.5\% increase in the p99 latency. 
% check
Meanwhile, as the block size increases beyond 8~KB, the page cache hit rate decreases from 76\% for 8~KB to 65\% for 128~KB. 
% check
As lower page cache hit rates necessitate  more page transfers from the storage device, the storage access latency begins to dominate the p99 latency. 
% check
In such a case, the difference in memory access latency between DDR memory and \cxlmem exhibits a lower impact on p99 latency, since Data Direct I/O (DDIO)~\cite{ddio} directly injects pages read from the storage device into the LLC~\cite{alian-ispass,yuan-iat,254372,farshin2019make}. 
% check
Lastly, we observe another trend shift beyond 128~KB block size, which is mainly due to the limited read and write bandwidth of \cxlmem.
% check
As more page cache entries are evicted from the LLC to memory as well as from memory to the storage device, the limited bandwidth of \cxlmem increases the effective latency of I/O requests.
% check

%Overall, this shows that \cxlmem, as a memory capacity expander, can serve as large page cache with little impact on application performance, as prior work~\cite{tpp} suggests.
% check
%Yet, we need to carefully consider the impact of \cxlmem's limited bandwidth on application performance.
% check


\begin{figure*}[!b]
    \centering
    \begin{subfigure}[b]{0.572\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figs/merci_mn_1_way.pdf}
        \vspace{-15pt}
        \caption{[DLRM] embedding reduction}
        \label{fig:ycsb_max_qps:dlrm}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.407\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figs/ycsb_max_qps_only.pdf}
        \vspace{-15pt}
        \caption{[Redis] YCSB-A}
        \label{fig:ycsb_max_qps:redis}
    \end{subfigure}
    \hfill
    \vspace{-6pt}
    \caption{Impact of using \cxlmem on throughput of \texttt{Redis} and \texttt{DLRM} for various ratios of page allocation to \cxlmem.
    %for various CXL memory configurations. The legend denotes percent of Redis memory allocated to CXL memory. \texttt{YCSB} workload \texttt{D} defaults to read the most recently inserted elements \texttt{D-lat}, but we also evaluate this workload with read request in Zipfian \texttt{D-zipf} or uniform \texttt{D-uni} distribution to see the effect on access locality.
    }
    \label{fig:ycsb_max_qps}
    \vspace{-6pt}
\end{figure*}

\niparagraph{Key findings.} 
%
Based on our analyses above, we present the following three key \textbf{\underline{F}}indings. 
% check
\textbf{(F1)} Allocating any percentage of pages to \cxlmem proportionally increases the p99 latency of simple memory-intensive applications demanding $\mu$s-scale latency, since such applications are highly sensitive to memory access latency. %, especially when considering the p99 latency.
%
\textbf{(F2)} Even an intelligent page migration policy may further increase the p99 latency of such latency-sensitive applications because of the overheads of migrating pages. 
% check
\textbf{(F3)} Judiciously allocating certain pages to \cxlmem does not increase the p99 latency of complex applications exhibiting $m$s-scale latency. 
%
This is because the long latency of accessing \cxlmem marginally contributes to the end-to-end latency of such applications and it is amortized by
intermediate operations between accesses to \cxlmem.
%\textbf{(F4)} For memory-bandwidth-intensive complex applications, allocating n pages
%can be hidden (or amortized) by 
% experience practically no increase in p99 latency even when allocating 100\% of pages of the storage and cache components to \cxlmem. 
% check
%This is 




\subsection{Throughput}
\label{sec:app:throughput}
%
\niparagraph{DLRM.} \figref{fig:ycsb_max_qps:dlrm} shows the throughput of \texttt{DLRM} embedding reduction for various percentages of pages allocated to \cxlmem.
% check
As the throughput of \texttt{DLRM} embedding reduction is bounded by memory bandwidth~\cite{10.1109/ISCA45697.2020.00070,asplos21lee,280856}, it begins to saturate over 20 threads when 100\% of pages are allocated to DDR memory. 
% check
In such a case, we observe that allocating a certain percentage of pages to \cxlmem can improve the throughput further, as it supplements to the bandwidth of DDR memory, increasing the total bandwidth available for \texttt{DLRM}.
% check
For instance, when running 32 threads, we observe that allocating 63\% of pages to \cxlmem can maximize the throughput of \texttt{DLRM} embedding reduction, providing 88\% higher throughput than DDR 100\%.
% check
Note that a lower percentage of pages will be allocated to \cxlmem for achieving the maximum throughput if the maximum bandwidth capability of a given \cxlmem device is lower (\eg, CXL-C). 
% check
This clearly demonstrates the benefit of \cxlmem as a memory bandwidth expander. 
% check
%We have also observed that allocating certain pages of \texttt{DSB} \texttt{mixed workload}  to \cxlmem reduces the p99 latency (\S\ref{sec:app:latency})

\niparagraph{Redis.} Although \texttt{Redis} is a latency-sensitive application, its throughput is also an important performance metric.
% check
\figref{fig:ycsb_max_qps:redis} shows the maximum sustainable QPS for various percentages of pages allocated to \cxlmem.
% check
For example, for \texttt{YCSB-A}, allocating 25\%, 50\%, 75\%, and 100\% of pages to \cxlmem provides 8\%, 15\%, 22\%, and 30\% lower throughput than allocating 100\% of pages to DDR memory.  
%
As \texttt{Redis} does not fully utilize the memory bandwidth, its throughput is bounded by memory access latency.
% check
Thus, similar to its p99 latency trends (\figref{fig:ycsb_redis}), allocating more pages to \cxlmem reduces the throughput of \texttt{Redis}.
%as interleaving with \cxlmem will always introduce higher memory access latency.
% check
%
%
%
\begin{comment}
Note that \texttt{Redis} begins to drop requests to limit the increase in p99 latency  at high QPS.
% check
To capture the impact of allocating more pages to \cxlmem on both p99 latency and throughput, we also measure the difference between various targeted QPS and delivered QPS. 
% check
This shows that the throughput of \texttt{Redis} saturates at lower QPS as more pages allocated to \cxlmem. 
% check
For example, when allocating 25\%, 50\%, and 75\% of pages to \cxlmem, we observe that the throughput saturates approximately at 104~K, 96~K, and 88~K, respectively.
% check
\end{comment}


\niparagraph{Key findings.} 
%
Based on our analyses above, we present the following %two 
key \textbf{\underline{F}}inding. 
% check
\textbf{(F4)} For memory-bandwidth-intensive applications, na\"ively allocating 50\% of pages to \cxlmem based on the OS default policy may result in lower throughput than allocating 100\% of pages to DDR memory, even with higher total bandwidth from using both DDR memory and \cxlmem together.
% check
This motivates us to develop a dynamic page allocation policy that can automatically configure the percentage of pages allocated to \cxlmem at runtime based on the bandwidth capability of a given CXL memory device and bandwidth consumed by co-running applications (\S\ref{sec:policy}).
% check


\begin{table}[t!]
\centering
%\vspace{-6pt}
\caption{Throughput of \texttt{DLRM} using only 1 SNC node versus all 4 SNC nodes, normalized to the throughput of \texttt{DLRM} running on 1 SNC node allocating all the pages to local DDR memory.}
\vspace{-6pt}
\label{tab:throughput_snc}
\begin{center}
\resizebox{0.7\columnwidth}{!}{%
\begin{tabular}{cccc}
\hline
\multicolumn{2}{c}{\textbf{1 SNC node}}  & \multicolumn{2}{c}{\textbf{4 SNC nodes}}\TBstrut \\ 
\cmidrule(lr){1-2}\cmidrule(lr){3-4}
\textbf{DDR 100\%} & \textbf{CXL 100\%} & \textbf{DDR 100\%} & \textbf{CXL 100\%}\Bstrut \\
\hline
1 & 0.947 & 1 & 0.504 \TBstrut \\
\hline
\end{tabular}
}
\label{tab2}
\end{center}
\vspace{-6pt}
\end{table}


\subsection{Interaction with Cache Hierarchy}
\label{sec:app:cache}
Previously, we discussed that accessing \cxlmem breaks the LLC isolation among SNC nodes (\S\ref{sec:mem-charac:cache}). 
% check
To analyze the impact of such an attribute of accessing \cxlmem on application performance, we evaluate two cases. 
%
In the first case (`1 SNC node' in Table~\ref{tab:throughput_snc}), only one SNC node (\ie, SNC-0 in \figref{fig:spr_interation_cache_hierarchy}) runs 8 \texttt{DLRM} threads while the other three SNC nodes idle. 
% check
In the second case (`4 SNC nodes' in Table~\ref{tab:throughput_snc}),
%\figref{fig:app:latency:dlrm:interference}), 
%the four SNC nodes run 32 \texttt{DLRM} threads, with 
each SNC node runs 8 \texttt{DLRM} threads. 
% check
Only SNC-0 allocates 100\% of its pages to either its DDR memory or \cxlmem, while the other three SNC nodes (\ie, SNC-1, SNC-2, and SNC-3) allocate 100\% of their pages only to their respective local DDR memory. 
% check
The second case is introduced to induce interference at the LLC among all the SNC nodes when SNC-0 with CXL 100\%.
%
%Specifically, 
%cache lines in the L2 caches of SNC-1, SNC-2, and SNC-3 are always evicted only to their respective LLC slices. 
%
%Since cache lines in the L2 caches of SNC-0 with CXL 100\% are also evicted to LLC slices coupled with SNC-1, SNC-2, and SNC-3, the effective LLC capacity of SNC-1, SNC-2, and SNC-3 is reduced. 
%
%On the other hand, cache lines in the L2 caches of SNC-0 with CXL 100\% can be evicted to LLC slices in any SNC nodes. 
%
%As a result, the effective LLC capacity of SNC-0 would be reduced compared to the first case, because the LLC slices of SNC-1, SNC-2, and SNC-3 are now shared between L2 cache lines evicted from SNC-0 and those from SNC-1, SNC-2, and SNC-3, respectively.
%
%At the same time, SNC-0 perceives that SNC-1, SNC-2, and SNC-3 reduce its effective LLC capacity (\ie, LLC slices from all the SNC nodes) since they also evict LLC lines within their respective LLC slices and some of the evicted LLC lines are from the L2 caches of SNC-0.
% chck

Table~\ref{tab:throughput_snc} shows that SNC-0 with CXL 100\% in `1 SNC node' offers 88\% higher throughput than SNC-0 with CXL 100\% in `4 SNC nodes.'
%
This is because of the other three SNC nodes in `4 SNC nodes' reduces the effective LLC capacity of SNC-0 with CXL 100\% (\ie, LLC slices from all the SNC nodes).
%
Specifically, while the other three SNC nodes evict LLC lines within their respective LLC slices, they also inevitably evict many LLC lines from SNC-0 with CXL 100\%.
% check
Although not shown in Table~\ref{tab:throughput_snc}, SNC-0 with DDR 100\% in `1 SNC node'  provides 2\% higher throughput than each of the other three SNC nodes in `4 SNC nodes' when SNC-0 in `4 SNC nodes' is with CXL 100\%.
% check
This is because SNC-0 with CXL 100\% in `4 SNC nodes' pollutes the LLC slices of the other three SNC nodes with cache lines evicted from the L2 caches of SNC-0, breaking the LLC isolation among the SNC nodes. 
% check
Lastly, in our previous evaluation of \texttt{DLRM} throughput (\S\ref{sec:app:throughput}), when SNC-0 needs to run more than 8 threads of \texttt{DLRM} in the SNC mode, it makes the remaining threads run on the CPU cores in the other three SNC nodes.
% check
Nonetheless, the CPU cores in the other three SNC nodes continue to access the DDR memory of SNC-0, and cache lines in the L2 caches of these CPU cores are still evicted to the LLC slices of SNC-0 since the cache lines were from the DDR memory of SNC-0.
% check
