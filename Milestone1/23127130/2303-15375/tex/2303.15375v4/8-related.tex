% !TEX root = paper.tex
\section{Related Work}
\label{sec:related}
With the rapid development of memory technologies, diverse heterogeneous memory devices have been introduced.
%
These memory devices are often different from the standard DDR-based DRAM devices, and each memory device offers unique characteristics and trade-offs. 
%
These include but are not limited to persistent memory, such as Intel Optane DIMM~\cite{9251957,fast20yang,eurosys22xiang}, remote/disaggregated memory~\cite{farm, infiniswap, socc20kalia,10.1145/1555754.1555789,10.1145/3520263.3534650,10.1145/3387514.3405897,10.1145/3357526.3357543}, and even byte-addressable SSD~\cite{10.1145/3297858.3304061,8416845}. 
%
These heterogeneous memory devices in the memory hierarchy of datacenters have been applied to diverse domains of applications. 
%
For example, in a tiered memory/file system, pages can be dynamically placed, cached, and migrated across different memory devices, based on their hotness and persistency requirements~\cite{tpp,hemem,227786,285754,10.1145/3582016.3582031,258860,10.1145/3445814.3446745,8988604,10.1145/3297858.3304024,273808,10.1145/3079856.3080245,10.1145/3037697.3037706,8676386,6212453,10.1145/3445814.3446713,9065506,10.1145/3410463.3414672,8327039}. 
%
Besides, database or key-value store can leverage these memory devices for faster and more scalable data organization and retrieval~\cite{slmdb,novelsm,listdb,matrixkv,ChameleonDB,viper,flatstore,utree,dptree}.
%
Solutions similar to \policy were proposed in the context of HBM~\cite{memsys17-batman} and storage system ~\cite{fast21-wu}.
%
While they have been extensively profiled and studied, \cxlmem, as a new member in the memory tier, still has unclear performance characteristics and indications, especially its interaction with CPUs. 
%
This leads to new challenges and opportunities for applying \cxlmem to the aforementioned domains. This paper aims to bridge the gap of \cxlmem understanding, and thus enable the wide adoption of \cxlmem in the community. Lastly, our \policy is specifically optimized for \cxlmem, making the most out of the memory bandwidth available for a given system.



Since the inception of the concept in 2019, 
CXL has been heavily discussed and invested by researchers and practitioners. For instance, Meta envisioned using \cxlmem for memory tiering and swapping~\cite{tpp,tmo}; Microsoft built a \cxlmem prototype system for memory disaggregation exploration~\cite{pond, 10034802}. Most of them used NUMA servers to emulate the behavior of \cxlmem.
There are also efforts in building software-based CXL simulators~\cite{yang2023cxlmemsim,wang2022asynchronous}.
Gouk \etal built a CXL memory prototype on FPGA-based RISC-V CPU~\cite{directcxl}. 
There are also a body of work focusing on certain particular applications~\cite{9969883,10032695,10066614}.
Different from the prior studies, this paper presents the first comprehensive study on true \cxlmem and compares it with emulated \cxlmem using the commercial high-performance CPU and CXL devices with both microbenchmarks and widely-used applications, which can better help the design space exploration of both CXL-memory based software systems and simulators. %This makes our research more realistic and comprehensive. 

\begin{comment}
\begin{figure}[]
    \centering
    \begin{subfigure}{0.8\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figs/spec_tune_ipc_core_16.pdf}
        \caption{SPEC 507.cactuBSSN\_r}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.8\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figs/spec_tune_ipc_core_17.pdf}
        \caption{SPEC 554.roms\_r}
    \end{subfigure}
    \vspace{-5pt}
    \caption{IPC changes as \policy tune}
    \label{fig:tune-improvement}
    \vspace{-12pt}
\end{figure}
\end{comment}
