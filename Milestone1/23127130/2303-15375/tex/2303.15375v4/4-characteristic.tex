
\section{Memory Latency and Bandwidth Characteristics}
\label{sec:character}
In this section, we first evaluate the latency and bandwidth of accessing different memory devices: an emulated \cxlmem device based on DDR5 memory in a remote NUMA node (DDR5-R), and three true \cxlmem devices (CXL-A, CXL-B, and CXL-C). 
%
We conduct this evaluation to understand the performance characteristics of various \cxlmem devices for different memory access instruction types. %, and then to understand key differences among them. 
%
Next, we investigate interactions between the Intel SPR CPU's cache hierarchy and the \cxlmem devices.
%
%We conduct these evaluations and explorations to understand the performance characteristics of various \cxlmem devices for different memory access instruction types. %, and then to understand key differences among them. 
%

\subsection{Latency}
\label{sec:mem-charac:latency}
\begin{figure}[b]
    \centering
        \vspace{-6pt}

\includegraphics[width=\linewidth]{figs/mlc_lats0.pdf}
    \vspace{-18pt}
    \caption{Random memory access latency of various memory devices (DDR5-R, CXL-A, CXL-B, and CXL-C), measured by Intel MLC and \arch. 
    %All the latency values are normalized to those of local DDR5 memory (DDR5-L). 
    %Both DDR5-L and DDR5-R use only a single DRAM channel to emulate the bandwidth of a \cxlmem device. 
    %\texttt{st} instructions are followed by a cache line write-back instruction (\texttt{clwb}) to ensure that the stored value is written all the way back to memory. \texttt{nt-st} instructions are followed by \texttt{mfence} to confirm the completion of \texttt{nt-st}. The presented store latency is measured and perceived by software, and the true store latency  can be longer.
    } 
    \label{fig:seq_latency}
\end{figure}


\figref{fig:seq_latency} presents the measured latency values of accessing both emulated and true \cxlmem devices. % across four memory access instruction types.
%
The first group of bars shows average (unloaded idle) memory access latency values measured by Intel MLC that performs pointer-chasing (\ie, getting the memory address of a load from the value of the preceding load) in a memory region larger than the total LLC capacity of the CPU. 
%
This effectively measures the latency of serialized memory accesses.
%
The remaining four groups of bars show the average memory access latency values measured by \arch for four memory access instruction types: (1) temporal load (\texttt{ld}), (2) non-temporal load (\texttt{nt-ld}), (3) temporal store (\texttt{st}), and (4) non-temporal store (\texttt{nt-st}). 
%
For these groups, we first execute \texttt{clflush}  to flush all cache lines from the cache hierarchy and then \texttt{mfence} to ensure the completion of flushing the cache lines.
%
Then, we execute 16 memory access instructions back to back to 16 random memory addresses in a cacheable memory region.
%
To measure the execution time of these 16 memory access instructions, we execute \texttt{rdtsc}, which reads the current value of the CPUâ€™s 64-bit time-stamp counter into a register immediately before and after executing the 16 memory access instructions, followed by an appropriate fence instruction.
%
This effectively measures the latency of random parallel memory accesses for each memory access instruction type for a given memory device.
%
To obtain a representative latency value, we repeat the measurement 10,000 times and choose the median value to exclude outliers caused by TLB misses and OS activities.  
%
 
Analyzing the latency values shown in \figref{fig:seq_latency}, we make the following \textbf{\underline{O}}bservations. 
%

\niparagraph{(O1) The full-duplex CXL and UPI interfaces reduce memory access latency.} 
%
\arch gives emulated \cxlmem 76\% lower \texttt{ld} latency than Intel MLC. 
%
This difference arises because serialized memory accesses by Intel MLC cannot exploit the full-duplex capability of the UPI interface that connects NUMA nodes.
%
In contrast, random parallel memory accesses by \arch can send memory commands/addresses and receive data in parallel through the full-duplex UPI interface, effectively halving the average latency cost of going through the UPI interface.
%
Since true \cxlmem is also based on the full-duplex interface (\ie, PCIe), it enjoys the same benefit as emulated \cxlmem. 
%
%Nonetheless, CXL-A gives 3 percentage points lower \texttt{ld} latency than DDR5-R with \arch. 
Nonetheless, with \arch, CXL-A gets a 3 percentage points more \texttt{ld} latency reduction than for DDR5-R. 
%
(O3) explains the reason for this additional latency reduction.  
%

\niparagraph{(O2) The latency of accessing true \cxlmem devices is highly dependent on a given CXL controller design.} 
%
\figref{fig:seq_latency} shows that CXL-A exhibits only 35\% longer \texttt{ld} latency than DDR5-R, while CXL-B and CXL-C present almost 2$\times$ and 3$\times$ longer \texttt{ld} latency, respectively.
%
Even with the same DDR4 DRAM technology, CXL-C based on DDR4-3200 gives 67\% longer \texttt{ld} latency than CXL-B based on DDR4-2400.
%

\niparagraph{(O3) Emulated \cxlmem can give longer memory access latency than true \cxlmem.} 
%
When issuing memory requests to emulated \cxlmem, the local CPU  must first check with the remote CPU, which is connected through the inter-chip UPI interface, for cache coherence~\cite{loughlin2022moesi,molka2015cache}. 
% 
Moreover, the memory requests must travel through a long intra-chip interconnect within the remote CPU to reach its memory controllers~\cite{icpe22-velten}. 
% check
%Moreover, the memory requests must travel through a long interconnect path between the UPI interface and memory controllers in the remote CPU~\cite{icpe22-velten}. 
% check
These overheads increase with more CPU cores, \ie, more caches and a longer interconnect path. % check 
%
For example, %when measuring 
the \texttt{ld} latency values of DDR5-R with 26- and 40-core Intel SPR CPUs are 29\% lower and 19\%  higher, respectively, than those of DDR5-L with the 32-core Intel SPR CPU used for our primary evaluations. %(\cf Table~\ref{table:config-table}).  
% check
In contrast, true \cxlmem has neither caches nor CPU cores that modify caches, although it is exposed as a remote NUMA node.
% check
%That is, the local CPU has no remote CPU to check with for cache coherence for the memory address space provided by true \cxlmem.
%
As such, the CPU %supporting true \cxlmem 
implements an on-chip hardware structure to facilitate fast cache coherence checks for memory accesses to the true \cxlmem.
% check
Moreover, true \cxlmem features a short intra-chip interconnect within the CXL controller to reach its memory controllers.
%path between the CXL IP and the memory controller in the CXL controller. 
% check

Specifically for DDR5-R and CXL-A, \arch provides 76\% and 79\% lower \texttt{ld} latency values, respectively, than Intel MLC. 
% check
Although both DDR5-R and CXL-A benefit from (O1), CXL-A gives a more \texttt{ld} latency reduction than DDR5-R. 
% check
This arises from the following differences between \arch and Intel MLC. 
% check
As Intel MLC accesses memory serially, the local CPU performs the aforementioned cache coherence checks one by one. 
% check
By contrast, \arch accesses memory in parallel. 
% check
In such a case, memory accesses to emulated \cxlmem incur a burst of cache coherence checks that need to go through the inter-chip UPI interface, %and increase the pressure on the snoop filter in the remote socket. %, 
leading to cache coherence traffic congestion. 
% check
This, in turn, increases the time required for cache coherence checks. 
% check
%a burst of cache coherence checks of which the response time is usually larger than the latency of accessing DRAM~\cite{mccalpin2018topology}. 
%
However, memory accesses to true \cxlmem suffer notably less from this overhead because the  CPU checks cache coherence through its local on-chip structure described earlier. 
% check
This contributes to a further reduction in the \texttt{ld} latency for true \cxlmem. %between emulated \cxlmem and true \cxlmem. % check
%
%Since memory accesses to true \cxlmem do not suffer from such overhead, the \texttt{ld} latency gap between emulated \cxlmem and true \cxlmem is further reduced. 
%
%These explanations are supported by two other experiments. % check
%
%First, when the number of parallel memory accesses is doubled, % (\ie, 16 to 32), 
%the \texttt{ld} latency gap between \arch and Intel MLC  increases from 3 percentage points to 2 percentage points. %check 
%
%Second, DDR5-R, based on a 40-core Intel SPR CPU, presents 4.2\% longer \texttt{ld} latency than CXL-A due to a higher overhead of cache coherence checks. % check
%
%These explanations are supported by another experiment.
%
Also note that DDR5-R, based on a 40-core Intel SPR CPU, presents 4\% longer \texttt{ld} latency than CXL-A due to a higher overhead of cache coherence checks. % check
%

The overhead of cache coherence checks becomes even more prominent for \texttt{st} because of two reasons.
% check
First, the \texttt{st} latency is much higher than the \texttt{ld} latency in general.
% check
For example, the latency of \texttt{st} to DDR5-R is 2.3$\times$ longer than that of \texttt{ld} from DDR5-L.
% check
This is because of the cache write-allocate policy in Intel CPUs. 
% check
When \texttt{st} experiences an LLC miss, the CPU first reads 64-byte data from memory to a cache line (\ie, implicitly executing \texttt{ld}), and then writes modified data to the cache line~\cite{perf-analysis-guide-i7}.
% check
This overhead is increased for both emulated \cxlmem and true \cxlmem, as the overhead of traversing the UPI and CXL interfaces is doubled.
% check
Yet, the latency of \texttt{st} to emulated \cxlmem increases more than that of \texttt{st} to emulated \cxlmem, when compared to \texttt{ld} or \texttt{nt-ld}. 
% check
This is because the emulated \cxlmem incurs a higher overhead for cache coherence checks than the true \cxlmem, as discussed earlier.
% check.

Lastly, although both \texttt{nt-ld} and \texttt{nt-st} bypass caches and directly access memory, the local CPU accessing emulated \cxlmem still needs to check with the remote CPU for cache coherence~\cite{Inter64}. 
% check
This explains why the \texttt{nt-ld} latency values of all the memory devices are similar to those of \texttt{ld} that needs to be served by memory in \figref{fig:seq_latency}.
% check
Unlike \texttt{st}, however, \texttt{nt-st} does not read 64-byte data from the memory since it does not allocate a cache line by its semantics, which eliminates the memory access and cache coherence overheads associated with implicit \texttt{ld}. 
% check
Therefore, the absolute values of \texttt{nt-st} latency across all the memory devices are smaller than those of \texttt{st} latency. 
% check
Furthermore, \texttt{nt-st} can also offer shorter latency than \texttt{ld} and \texttt{nt-ld} because the CPU issuing \texttt{nt-st} sends the address and data simultaneously.
%
In contrast, the CPU issuing \texttt{ld} or \texttt{nt-ld} sends the address first and then receive the data later, which makes signals go through the UPI and CXL interfaces twice. 
%
With shorter latency for a cache coherency check, \texttt{nt-st} to true \cxlmem can be shorter than \texttt{nt-st} to emulated \cxlmem.
%
For instance, CXL-A exhibits a 25\% lower latency than DDR5-R for \texttt{nt-st}.
%
Note that \texttt{nt-st} behaves differently depending on whether the allocated memory region is cacheable or not~\cite{Inter64}, and we conduct our experiment with a cacheable memory region.
% check
%This explains why the \texttt{nt-st} latency relative to DDR5-L is lower than the \texttt{st} latency.
% check


\subsection{Bandwidth}
\label{sec:mem-charac:bandwidth}
The sequential memory access bandwidth represents the maximum throughput of the memory subsystem when all the CPU cores sends memory requests in this paper.
% check
Nonetheless, it notably varies across (1) CXL controller designs, (2) memory access instruction types, and (3) DRAM technologies (\ie, DDR5-4800, DDR4-3200, and DDR4-2400 in this paper).
% check
As such, for fair and insightful comparison, we use bandwidth efficiency as a metric, normalizing the measured bandwidth to the theoretical maximum bandwidth. 
%of a given memory device for each memory access instruction type, 
%(\eg, `All Read' in \figref{subfig:bw_eff_mlc} and \texttt{ld} in \figref{subfig:bw_eff_memo}), 
% using the following equation:
% check
%\begin{align}
%\frac{Max\_Bandwidth_{Measured}(Memory\_Type,\;Case)}{Max\_Bandwidth_{Theoretical}(Memory\_Type)}
%\label{eq:1}
%\end{align}
%
%\figref{subfig:bw_eff_mlc} and \figref{subfig:bw_eff_memo} present 
\figref{fig:seq_bandwidth} presents the bandwidth efficiency values of DDR5-R, CXL-A, CXL-B, and CXL-C for various read/write ratios and different memory instruction types, respectively.
% check
Analyzing these values, %the bandwidth efficiency values from \figref{fig:seq_bandwidth},
we make the following \textbf{\underline{O}}bservations. 
% check

\begin{figure}[t]
    %\vspace{-6pt}
    \centering
    \begin{subfigure}[b]{\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figs/mlc_bw0.pdf}
        \vspace{-12pt}
        \caption{MLC with various read and write ratios
        %\yan{All read, 3:1, 2:1, 1:1, stream-traid. Bars, group by devices, 1st norm to max mem b/w, 2nd norm to DDR-L1}
        }
        \label{subfig:bw_eff_mlc}
    \end{subfigure}
    \vfill
    \begin{subfigure}[b]{\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figs/micro_bw_norm0.pdf}
        \caption{\arch with different memory instruction types}
        \label{subfig:bw_eff_memo}
    \end{subfigure}
    \vspace{-15pt}
    \caption{Efficiency of maximum sequential memory access bandwidth across different memory types.}
    \label{fig:seq_bandwidth}
    \vspace{-6pt}
\end{figure}


\niparagraph{(O4) The bandwidth is strongly dependent on the efficiency of CXL controllers.}
%
The maximum sequential bandwidth values that can be provided by the DDR5-4800, DDR4-3200, and DDR4-2400 DRAM technologies are 38.4~GB/s, 25.6~GB/s, and 19.2~GB/s per channel, respectively.
% check
%Nonetheless, both Intel MLC `All Read' and \arch \texttt{ld} show that 
Nonetheless, \figref{subfig:bw_eff_mlc} shows that DDR5-R, CXL-A, CXL-B, and CXL-C provides only 70\%, 46\%, 47\%, and 20\% of the theoretical maximum bandwidth, respectively, for `All Read'.
% check
%The bandwidth efficiency gap between DDR5-L and DDR5-R based on the same memory controller is \textcolor{red}{X} percentage points, representing the overhead of accessing remote memory.
%
DDR5-R and CXL-A are based on the same DDR5-4800 DRAM technology, yet the bandwidth efficiency of DDR5-R is 23 percentage points higher than that of CXL-A.
% check
We speculate that the lower efficiency of the CXL-A's memory controller for memory read accesses contributes to this bandwidth efficiency gap, as both DDR5-R and CXL-A exhibit similar \texttt{ld} latency values.
% check

As the write ratio increases, however, CXL-A starts to provide higher bandwidth efficiencies.
% check
For example, \figref{subfig:bw_eff_mlc} shows that the bandwidth efficiency of CXL-A for `2:1-RW' is 23 percentage points higher than that of DDR5-R.
% check
We speculate that the CXL-A's memory controller is designed to handle interleaved memory read and write accesses more efficiently than the DDR5-R's and CXL-B's memory controllers.  
% check
This is supported by (1) the fact that \texttt{st} involves both memory read and write accesses due to implicit \texttt{ld} when it incurs a cache miss (\cf (O3)), and (2) 
%the bandwidth efficiency of CXL-A for \texttt{st} is 12.2 and \texttt{red}{X} percentage points 
%higher than that of DDR5-R and CXL-B, whereas 
the bandwidth efficiency of CXL-A for all the other memory access instruction types is lower than that of DDR5-R and CXL-B.
% check
This also implies that the higher bandwidth efficiency of CXL-A for \texttt{st} is not solely attributed to a unique property of true \cxlmem. %, as it is 11.6 percentage points higher than that of CXL-B that . 
% check

\figref{subfig:bw_eff_memo} shows that the bandwidth efficiency of CXL-B is higher than that of CXL-A, except for \texttt{st}, although the latency values of CXL-B is higher than those of CXL-A.
% check
Specifically, the bandwidth efficiency values of CXL-B for \texttt{ld}, \texttt{nt-ld}, and \texttt{nt-st} are 1, 1, and 6 percentage points higher than that of CXL-A, respectively.
% check
We speculate that the recently developed third-party DDR5 memory controllers may not be as efficient as the mature and highly optimized DDR4 memory controller used in CXL-B for read- or write-only memory accesses.
% check
Note that CXL-C is based on DDR4-3200 DRAM technology, but it generally exhibits poor bandwidth efficiency due to the FPGA-based implementation of the CXL controller.
% check
The bandwidth efficiency values of CXL-C for \texttt{ld}, \texttt{nt-ld}, \texttt{st}, and \texttt{nt-st} are 26, 26, 3, and 20 percentage points lower, respectively, than those of CXL-B, which is based on the same DDR4 DRAM technology but provides 25\% lower theoretical maximum bandwidth per channel.
% check

\niparagraph{(O5) True \cxlmem can offer competitive bandwidth efficiency for the store compared to emulated \cxlmem.}
% check
\figref{subfig:bw_eff_memo} shows that 
\texttt{st} yields %up to 74.3\% 
lower bandwidth efficiency values than \texttt{ld} across all the memory devices due to the overheads of  implicit \texttt{ld} and cache coherence checks (\cf (O3)).
% check
Specifically, \texttt{st} to DDR5-R, CXL-A, CXL-B, and CXL-C offers 74\%, 31\%, 59\%, and 15\% lower bandwidth efficiency values, respectively, than  \texttt{ld} from DDR5-R, CXL-A, CXL-B, and CXL-C.
% check
This suggests that emulated \cxlmem experiences a notably more bandwidth efficiency degradation than true \cxlmem partly because it suffers more from the overhead of cache coherence checks.
% check
As a result, the bandwidth efficiency values of CXL-A and CXL-B for \texttt{st} are 12 and 1 percentage points higher, respectively, than DDR5-R.
% check
For \texttt{nt-st}, the bandwidth efficiency gap between emulated \cxlmem and true \cxlmem is noticeably reduced compared to \texttt{nt-ld}.
% check
Specifically, the bandwidth efficiency gap between DDR5-R and CXL-A for \texttt{nt-ld} is 26 percentage points, whereas it is reduced to 6 percentage points for \texttt{nt-st}.
% check
CXL-B provides almost the same bandwidth efficiency as DDR5-R for \texttt{nt-st}.

% check
%Although CXL-A presents an exceptionally higher bandwidth efficiency than all other memory devices, Combined with the reasons discussed in \textbf{(O4)}, CXL-B gives 22\% and 17\% higher bandwidth efficiency for \texttt{ld} and \texttt{st}, respectively, than DDR5-R, although DDR5-R still provides higher absolute bandwidth than CXL-B. 

%Note that, we also measure the random memory access bandwidth of these memory types and observe a similar trend. The only difference is \texttt{nt-st}, where for all \cxlmem devices, the bandwidth drops after using more threads to conduct the test. This is because of the limited capacity of the CPU's HA, where coherence information is monitored and tracked~\cite{loughlin2022moesi}. As \texttt{nt-st} does not occupy the core resources, more \texttt{nt-st} instructions can be issued concurrently than the regular \texttt{st}. When the number of on-the-fly \texttt{nt-st} instructions, containing random address information, exceeds the HA capacity, they will be serialized to make sure they can be correctly parsed and routed. This degrades aggregate bandwidth consumption.


\subsection{Interaction with Cache Hierarchy}
\label{sec:mem-charac:cache}


\begin{figure}[b!]
    \centering
    %\vspace{-3pt}
    \includegraphics[width=0.88\linewidth]{figs/spr_interaction_cache_hierarchy.pdf}
    \caption{Difference in L2 cache line eviction paths between local DDR memory and \cxlmem in SNC mode. 
    %L2 cache lines from DDR memory are evicted only to LLC slices (light-green arrows) in the same SNC node while ones from \cxlmem are evicted to the LLC slices on one of the SNC nodes (red-dashed arrows).
    } 
    %\vspace{-12pt}
    \label{fig:spr_interation_cache_hierarchy}
\end{figure}

Starting with the Intel Skylake CPU, Intel has adopted non-inclusive cache architecture~\cite{non-inclusive-llc}.
% check
Suppose that a CPU core with non-inclusive cache architecture incurs an LLC miss that needs to be served by memory.
% check
Then it loads data from the memory into a cache line in the CPU core's (private) L2 cache rather than the (shared) LLC, in contrast to a CPU core with inclusive cache architecture.
% check
When evicting the cache line in the L2 cache, the CPU core will place it into the LLC. That is, the LLC serves as a victim cache.
%
The SNC mode (\S\ref{sec:setup:system}), however, restricts where the CPU core places evicted L2 cache lines within the LLC to provide LLC isolation among SNC nodes. 
% check
The LLC comprises as many slices as the number of CPU cores, %each coupled with an L2 cache, 
and
% check
%That is, 
L2 cache lines in an SNC node are evicted only to LLC slices within the same SNC node when data in the cache lines are from the local DDR memory of that SNC node (light-green lines in \figref{fig:spr_interation_cache_hierarchy}).
% check
%This provides LLC isolation among SNC nodes.
% check
In contrast, we notice that L2 cache lines can be evicted to any LLC slices within any SNC nodes when the data are from remote memory, including both emulated \cxlmem and true \cxlmem (red-dashed lines in \figref{fig:spr_interation_cache_hierarchy}). 
% check
As such, CPU cores accessing \cxlmem break LLC isolation among SNC nodes in SNC mode. 
%
%Nonetheless, they 
This makes such CPU cores benefit from 2--4$\times$ larger LLC capacity than the ones accessing local  DDR memory, notably compensating for the slower access latency of \cxlmem.
% check

To verify this, we run a single instance of Intel MLC on an idle CPU and measure the average latency of randomly accessing  32~MB buffers allocated to DDR5-L and CXL-A, respectively, in the SNC mode. 
% check
The total LLC capacity of the CPU with four SNC nodes in our system is $\sim$60~MB.
% check
A 32~MB buffer is larger than the total LLC capacity of a single SNC node but smaller than that of all four SNC nodes.
% check
This shows that accessing the buffer allocated to CXL-A gives an average memory access latency of 41~ns, whereas accessing the buffer allocated to DDR5-L offers an average memory access latency of 76.8~ns. 
% check
The shorter latency of accessing the buffer allocated to CXL-A  evidently shows that the CPU cores accessing \cxlmem can benefit from larger effective LLC capacity than the CPU cores accessing local DDR memory in the SNC mode.
%

\niparagraph{(O6) \cxlmem interacts with the CPU's cache hierarchy differently compared to local DDR memory.} 
%
As discussed above, the CPU cores accessing \cxlmem are exposed to a larger effective LLC capacity in the SNC mode.
%
This often significantly impacts LLC hit/miss and interference characteristics that the CPU cores experience, and thus affecting the performance of applications (\S\ref{sec:app:cache}).
%
Therefore, we must consider this attribute of \cxlmem when analyzing the performance of applications using \cxlmem, especially in the SNC mode.
%