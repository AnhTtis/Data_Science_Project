\section{Introduction}
\label{sec:intro}
Emerging applications have demanded memory with even larger capacity and higher bandwidth at lower power consumption. 
%
However, as the current memory technologies have almost reached their scaling limits, it has become more challenging to meet these demands cost-efficiently. 
%
Especially, when focusing on the memory interface technology, we observe that DDR5 requires 288 pins per channel~\cite{jedec}, making it more expensive to increase the number of channels for higher bandwidth under the CPU's package pin constraint. 
%
Besides, various signaling challenges in high-speed parallel interfaces, such as DDR, make it harder to further increase the rate of data transfers.
%
This results in super-linearly increasing energy consumption per bit transfer~\cite{8326997} and reducing the number of memory modules (DIMMs) per channel to one for the maximum bandwidth~\cite{4thGenIn27:online}. 
%
As the capacity and bandwidth of memory are functions of the number of channels per CPU package, the number of DIMMs per channel, and the rate of bit transfers per channel, DDR has already shown its limited bandwidth and capacity scalability. 
%
This calls for alternative memory interface technologies and memory subsystem architectures. 
%

\begin{figure}[t]
    \centering
    %\vspace{-15pt}
    \includegraphics[width=1\linewidth]{figs/cxl_memory_module_architecture.pdf}
    \vspace{-12pt}
    \caption{CXL memory module architecture.} 
    \vspace{-6pt}
    \label{fig:cxl-mem}
\end{figure}

Among them, Compute eXpress Link (CXL)~\cite{cxl2} has emerged as one of the most promising memory interface technologies. 
%
CXL is an open standard developed through a joint effort by major hardware manufacturers and hyperscalers. %cloud service providers. 
%
As CXL is built on the standard PCIe, which is a serial interface technology, it can offer much higher bit-transfer rates per pin (\eg, PCIe 4.0: 16~Gbps/lane vs. DDR4-3200: 3.2~Gbps/pin) and consumes lower energy per bit transfer (\eg, PCIe 4.0: 6~pJ/bit~\cite{LowPower73:online} vs. DDR4: 22~pJ/bit~\cite{8377983}), but at the cost of much longer link latency (\eg, PCIe 4.0: $\sim$40~ns~\cite{cxl2} vs. DDR4: $<$1~ns~\cite{6509675}).
%
Compared to PCIe, CXL implements additional features that enable the CPU to communicate with devices and their attached memory in a cache-coherent fashion using load and store instructions. 
%
\figref{fig:cxl-mem} illustrates a CXL memory device consisting of a CXL controller and memory devices.
%
Consuming $\sim$3$\times$ fewer pins than DDR5, a CXL memory device based on PCIe 5.0 $\times$8 may expand memory capacity and bandwidth of systems cost-efficiently.
%
Furthermore, with the CXL controller between the CPU and memory devices, CXL decouples memory technologies from a specific memory interface technology supported by the CPU.
%
This grants memory manufacturers unprecedented flexibility in designing and optimizing their memory devices.
%
Besides, by employing retimers and switches, a CPU with CXL support can easily access memory in remote nodes with lower latency than traditional network interface technologies like RDMA, efficiently facilitating memory disaggregation.
%not only for memory capacity/bandwidth expansion but also for memory disaggregation. % in both the industry and academia. 
%
These advantages position memory-related extension as one of the primary target use cases for CXL~\cite{micron-cxl, montage-cxl, rambus-cxl, smdk_github}, and major hardware manufacturers have announced CXL support in their product roadmaps~\cite{spr, amd_4th_gen_epic, micron-cxl, montage-cxl, smdk_github}.
%

Given its promising vision, \cxlmem has recently attracted significant attention with active investigation for datacenter-scale deployment~\cite{tpp,pond}. 
%
Unfortunately, due to the lack of commercially available hardware with CXL support, most of the recent research on \cxlmem has been based on emulation using memory in a remote NUMA node in a multi-socket system, since \cxlmem is exposed as such~\cite{pond, tpp,10.1145/3545008.3545054}. 
%
However, as we will reveal in this paper, there are fundamental differences between emulated \cxlmem and true \cxlmem. 
%
That is, the common emulation-based practice of using a remote NUMA node to explore \cxlmem may give us misleading performance characterization results and/or lead to suboptimal design decisions.
%

This paper addresses a pressing need to understand the capabilities and performance characteristics of true \cxlmem, as well as their impact on the performance of (co-running) applications and the design of OS policies to best use \cxlmem.
%
To this end, we take a system based on the CXL-ready 4\textsuperscript{th}-generation Intel Xeon  CPU~\cite{spr} and three \cxlmem devices from different manufacturers (\S\ref{sec:setup}).
%
Then, for the first time, we not only compare the performance of true \cxlmem %based on three different devices 
with that of emulated \cxlmem, but also conduct an in-depth analysis of the complex interplay between the CPU and \cxlmem.
%
Based on these comprehensive analyses, we make the following contributions. 
%

\niparagraph{\cxlmem $\neq$ remote NUMA memory (\S\ref{sec:character}).}  
%
We reveal that true \cxlmem exhibits notably different performance \textbf{\underline{C}}haracteristics  
from emulated \cxlmem.
%
\textbf{(C1)} Depending on CXL controller designs and/or memory technologies, true \cxlmem devices give a wide range of memory access latency and bandwidth values.
%
\textbf{(C2)} True \cxlmem  can give up to 26\% lower latency and 3--66\% higher bandwidth efficiency than emulated \cxlmem, depending on memory access instruction types and \cxlmem devices. 
%
%This is because true \cxlmem, without caches, does not incur the overhead of cache coherence checks in contrast to emulated \cxlmem.
This is because true \cxlmem has neither caches nor CPU cores that modify caches, although it is exposed as a NUMA node. 
%
As such, the CPU implements an on-chip hardware structure to facilitate fast cache coherence checks for memory accesses to the true \cxlmem.
%
These are important differences that may change conclusions made by prior work on the performance characteristics of \cxlmem 
and consequently the effectiveness of the proposals at the system level.
%
\textbf{(C3)}
The sub-NUMA clustering (SNC) mode provides LLC isolation among SNC nodes (\S\ref{sec:setup}) by directing the CPU cores within an SNC node to evict their L2 cache lines from its local memory exclusively to LLC slices within the same SNC node.  
%
However, when CPU cores access \cxlmem, they end up breaking the LLC isolation, as L2 cache lines from \cxlmem can be evicted to LLC slices in any SNC nodes.
%
%When CPU cores access \cxlmem, they end up breaking LLC isolation provided by the sub-NUMA clustering (SNC) mode (\S\ref{sec:setup}), in which the CPU cores in an SNC node are supposed to evict their L2 cache lines only to LLC slices within the same SNC node.  
%
%However, L2 cache lines from \cxlmem can be evicted to LLC slices in any SNC nodes.
%
Consequently, accessing \cxlmem can benefit from  effectively 2--4$\times$ larger LLC capacity than accessing local DDR memory, notably compensating for the longer latency of accessing \cxlmem for cache-friendly applications. 
%This effectively offers 2--4$\times$ larger LLC capacity for \cxlmem than local DDR memory, notably compensating for the longer latency of accessing \cxlmem for cache-friendly applications. 
%


\niparagraph{Na\"ively used \cxlmem considered harmful (\S\ref{sec:app}).}
%
Using a system with a \cxlmem device, we evaluate a set of applications with diverse memory access characteristics and different performance metrics (\eg, response time and throughput). 
%
Subsequently, we present the following \textbf{\underline{F}}indings.
%
\textbf{(F1)} Simple applications (\eg, key-value-store) demanding $\mu$s-scale latency are highly sensitive to memory access latency. 
%
Consequently, allocating pages to \cxlmem increases the tail latency of these applications by 10--82\%  compared to local DDR memory.
%
Besides, the state-of-the-art CXL-memory-aware page placement policy for a tiered memory system~\cite{tpp} actually increases tail latency even further when compared to statically partitioning pages between DDR memory and \cxlmem. 
%
This is due to the overhead of page migration.
%
\textbf{(F2)} Complex applications (\eg, social network microservices) exhibiting $m$s-scale latency experience a marginal increase in tail latency even when most of pages are allocated to \cxlmem. 
%
This is because the longer latency of accessing \cxlmem contributes marginally to the end-to-end latency of such applications.
% check
\textbf{(F3)} Even for memory-bandwidth-intensive applications, na\"ively allocating 50\% of pages to \cxlmem based on the default OS policy may result in lower throughput, despite higher aggregate bandwidth delivered by using both DDR memory and \cxlmem.
%

 

\niparagraph{CXL-memory-aware dynamic page allocation policy (\S\ref{sec:policy}).} 
%
To showcase the usefulness of our characterizations and findings described above, we propose \policy, a \textbf{\underline{C}}XL-memory-\textbf{\underline{a}}ware dynamic \textbf{\underline{p}}age alloca\textbf{\underline{tion}} policy for the OS to more efficiently use the bandwidth expansion capability of \cxlmem.
%
Specifically, \policy begins by determining the bandwidth of manufacturer-specific \cxlmem devices. % used by systems. 
%
Subsequently, \policy periodically monitors various CPU counters, such as memory access latency experienced by (co-running) applications and assesses the bandwidth consumed by them at runtime.
%
Lastly, based on the monitored CPU counter values, \policy estimates memory-subsystem performance over periods. 
%
When a given application demands an allocation of new pages, \policy considers the history of memory subsystem performance and the percentage of pages allocated to \cxlmem in the past. 
%
Then, it adjusts the percentage of the pages allocated to \cxlmem to improve the overall system throughput using a simple greedy algorithm. 
%
Our evaluation shows that \policy improves the throughput of a system co-running a set of memory-bandwidth-intensive SPEC CPU2017 benchmarks by 24\%, compared with the default static page allocation policy set by the OS.

