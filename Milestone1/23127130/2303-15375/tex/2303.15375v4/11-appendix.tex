%\clearpage
\appendix
\section{Artifact Appendix}

\subsection{Abstract}
This appendix explains how to reproduce the results presented in this paper, which is based on an evaluation platform described in Table~\ref{table:config-table}.
%
In the following sections, we provide guidelines to access, build, and setup the environments for individual (micro)benchmarks evaluated in this work.
%
We also provide a set of scripts that automatically collect the results after running experiments.
%
The artifact is available publicly through an archived repository.


\subsection{Artifact check-list}
{\small
\begin{itemize}
  \item {\bf Program: } 
  \begin{itemize}
  \item Intel MLC (Memory Latency Checker) v3.10~\cite{mlc}: \figref{fig:seq_latency}, \ref{fig:seq_bandwidth}.
  \item \arch: \figref{fig:seq_latency}, \ref{fig:seq_bandwidth}.
  \item \texttt{YCSB} (Yahoo! Cloud Serving Benchmark)~\cite{socc10:ycsb}: \figref{fig:app:latency}, \ref{fig:ycsb_max_qps}, \ref{fig:tune-improvement}. 
  \item \texttt{Redis}~\cite{redis}: \figref{fig:app:latency}, \ref{fig:ycsb_max_qps}, \ref{fig:tune-improvement}.
  \item \texttt{DSB} (DeathStarBench)~\cite{deathstartbench}: \figref{fig:app:latency}.
  \item \texttt{FIO} (Flexible I/O Tester)~\cite{GitHubax76:online}: \figref{fig:app:latency:fio:log-scale}.
  \item \texttt{MERCI}~\cite{asplos21lee}: \figref{fig:ycsb_max_qps}, \ref{fig:correlation}, \ref{fig:linear-model-throughput}.
  \item \texttt{SPEC CPU2017}~\cite{SPECCPU70:online}: \figref{fig:linear-model-throughput}, \ref{fig:tune-improvement}.
  \item \policy: \figref{fig:linear-model-throughput}, \ref{fig:tune-improvement}.
  \end{itemize}
  \item {\bf Compilation: } GCC-11.4.0 (\arch, FIO, SPEC2017, MERCI), Maven 3 (YCSB). For the other benchmarks, please refer to their repository for proper environemnet setup.
  \item {\bf Binary: } Intel MLC v3.10, Redis, DeathStarBench.
  \item {\bf OS environment: } Ubuntu 22.04.2 LTS with different kernel versions and patches.
  \begin{itemize}
      \item \textbf{TPP-related tests:} Linux kernel v5.13 with TPP patch ~\cite{tpp-patch}.
      \item \textbf{\policy, memory interleaving tests:} Linux kernel v5.19 with m-n memory interleave patch ~\cite{mn_il_patch}.  
      \item \textbf{Else:} Linux kernel v6.2.
  \end{itemize}
  \item {\bf Hardware: } An evaluation platform with dual-socket Intel Xeon 6430 CPUs, 8-channel DDR5-4800 modules for socket 0, 1-channel DDR5-4800 module for socket 0, and three CXL devices (Table~\ref{table:config-table}).
  \item {\bf Run-time state: } Fixed CPU frequency (2.1~GHz), Turbo Boost/Hyper-Threading off.
  \item {\bf Metrics: } The artifact reports memory access latency (ns), memory bandwidth (GB/s), or application throughput (query per second for Redis, inference per second for DLRM). 
  \item {\bf Output: }\texttt{.txt} files including collected metrics corresponding to individual experiments.
  \item {\bf Experiments reproduced: } \figref{fig:seq_latency}, \ref{fig:seq_bandwidth}, \ref{fig:app:latency}, \ref{fig:app:latency:fio:log-scale}, \ref{fig:ycsb_max_qps}, \ref{fig:tune-improvement}.
  \item {\bf How much disk space required (approximately)?: } 20~GB storage for DeathStarBench docker images, 50~GB storage for FIO, 5~GB for MERCI dataset.
  \item {\bf How much time is needed to prepare workflow (approximately)?: } More than 3 hours.
  \item {\bf How much time is needed to complete experiments (approximately)?: } More than 48 hours.
  \item {\bf Publicly available?: } Yes. 
  \item {\bf Code licenses (if publicly available)?: } GPL v2.0.
  \item {\bf Archived: } \url{https://zenodo.org/record/8332543}
\end{itemize}
}


\subsection{Description}

\subsubsection{How to access}
Our own microbenchmark (\arch) and proposed page allocation software (\policy) are archived at Github and Zenodo. The other (micro)beanchmarks (except for SPEC CPU2017) are open-sourced that are publicly available at:
\begin{itemize}
    \item Intel MLC v3.10: \url{https://www.intel.com/content/www/us/en/download/736633/763324/intel-memory-latency-checker-intel-mlc.html}
    \item \texttt{YCSB}: \url{https://github.com/brianfrankcooper/YCSB}
    \item \texttt{Redis}: \url{https://github.com/redis/redis}
    \item \texttt{DSB}: \url{https://github.com/delimitrou/DeathStarBench}
    \item \texttt{FIO}: \url{https://github.com/axboe/fio}
    \item \texttt{MERCI}: \url{https://github.com/SNU-ARC/MERCI}
    %\item mn interleave kernel patch: \url{https://lore.kernel.org/linux-mm/20220607171949.85796-1-hannes@cmpxchg.org/} 
\end{itemize}

%Since CXL devices and supported CPUs are hard to order, we can provide access to these hardware systems for AE purposes, as they are currently setup in the FAST Lab of UIUC-ECE. These systems can be accessed via UIUC VPN (software environment/libraries/benchmarks setup already). Please contact us for setting up connection to our servers. 

\subsubsection{Hardware dependencies}
We conduct evaluations on an Intel SPR platform equipped with three types of CXL memory devices, as described in Table~\ref{table:config-table}. Sub-NUMA clustering mode (SNC) is disabled for Intel MLC and \texttt{memo} and enabled (\ie, SNC-4) for the later experiments. For a fair comparison with the local CXL memory devices each of which uses a single PCIe link, we limit the remote DDR5 memory to use only a single DRAM channel, as follows:

\begin{lstlisting}
- Socket 0 (Local)
    - DIMM 0 <Inserted>
    - DIMM 1 <Inserted>
    ...
    - DIMM 7 <Inserted>
- Socket 1 (Remote)
    - DIMM 0 <Inserted>
    - DIMM 1 <Not used>
    ...
    - DIMM 7 <Not used>
\end{lstlisting}

\subsubsection{Software dependencies}
Our testbed runs Ubuntu 22.04.3 LTS with kernel version 6.2. Most native installation of Ubuntu does not come with \texttt{libnuma}, which is used in \arch for allocating memory on a specific NUMA node. The following command will install \texttt{libnuma} on Ubuntu.

\begin{lstlisting}
$ sudo apt install libnuma-dev
\end{lstlisting}

\noindent Additionally, we use \texttt{cpupower} to fix the CPU frequency to 2.1~GHz in all experiments. The following command will install Linux tools including \texttt{cpupower} on Ubuntu.
\begin{lstlisting}
$ sudo apt install linux-tools-$(uname -r)
\end{lstlisting}

%For the SPEC benchmarks, UIUC has SPEC benchmarks licensed under Prof. Saugata Ghose. Our machine has SPEC benchmark installed. 

\noindent For the rest of the benchmarks, please follow the guidelines in GitHub pages to setup the corresponding environments. %These environment, including the one for \policy, has been setup on our machine.


\subsection{Installation}
To install \arch or \policy,  we first need to clone the source code:
\begin{lstlisting}
$ git clone https://github.com/ece-fast-lab/cxl_type3_tests
\end{lstlisting}
% \begin{lstlisting}[escapeinside={(*@}{@*)}]
% $ git clone (*@\href{https://github.com/ece-fast-lab/cxl_type3_tests}{https://github.com/ece-fast-lab/cxl\_type3\_tests}@*)
% \end{lstlisting}

\noindent Then, go to the source code directory:
\begin{lstlisting}
$ cd memo_ae/src
\end{lstlisting}

\noindent Finally, build the executable binary from the source code:
\begin{lstlisting}
$ make
\end{lstlisting}

\noindent To test \policy, please go to the source code directory for \policy:

\begin{lstlisting}
$ cd caption_ae/src
\end{lstlisting}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Experiment workflow}
\label{sec:workflow}

We provide guidelines for BIOS configuration as well as automated scripts for all evaluated workflows. Within these scripts, the common operation goes as follows:
\begin{lstlisting}
$ # set software / hardware environment
$ # run application in loops
$ # unset the environment
\end{lstlisting}

\noindent In most cases, BIOS configuration involves enabling and disabling Sub-NUMA clustering (SNC) and hyperthreading. In the software level, Setting and un-setting the environment involves locking/unlocking the CPU frequency, and creating cgroup directories for fine-grain control over cpu and memory resources. Detailed experiment steps for each experiment (regarding Figure~\texttt{X}) is available at the \texttt{config\_figure\_X.md} file in the folders of the repository.


\begin{comment}
\input{ae-appendix/05-1-eval_memo}
\input{ae-appendix/05-2-eval_ycsb_redis}
\input{ae-appendix/05-3-eval_dsb}
\input{ae-appendix/05-4-eval_fio}
\input{ae-appendix/05-5-eval_dlrm}
\input{ae-appendix/05-6-eval_caption}
\end{comment}

\subsection{Evaluation and expected results}

After running each experiment (\S\ref{sec:workflow}), the result data (memory access latency and bandwidth, application throughput, \textit{etc.}) can be automatically collected using a provided script corresponding to each figure. For more detailed instructions, please refer to each \texttt{run\_figure\_X.md} file in the repository.

\begin{comment}
\subsubsection{Figure 3}
For the result produced by Intel MLC idle latency, we expect that CXL device has a higher latency than that of local DDR5 and NUMA-DDR5. For the result produced by \arch, we expect for load and non-temporal loads, the best CXL device will match, or even be lower than that of NUMA-DDR5. The store and nt-store latency further shows that NUMA-DDR5 has higher latency than some better-implemented CXL devices. These observations helps to justify O(1-3) in Section 4.1. The steps to produce this figure are as follows:
\begin{lstlisting}
$ cd ./AE_root/memo/evaluation
$ bash figure_3.sh
\end{lstlisting}
Results are placed under:
\begin{lstlisting}
cd ./AE_root/memo/result
\end{lstlisting}
This test takes less than 15 minutes to finish.

\subsubsection{Figure 4-a/b}
The result from 4-a is produced purely from Intel MLC. Under various read write ratio, CXL devices shows different access efficiencies. 
The steps to produce these figures are as follows:
\begin{lstlisting}
$ cd ./AE_root/memo/evaluation
$ bash figure_4a.sh
$ bash figure_4b.sh
\end{lstlisting}
Results are placed under:
\begin{lstlisting}
cd ./AE_root/memo/result
\end{lstlisting}
4a takes less than 15 minutes to finish. 4b takes about 30 minutes to finish. 

\subsubsection{Figure 6-a}
Under the same workload, with more memory allocate to CXL, Redis shows a higher p99 latency and saturate earlier. The steps to produce this figure are as follows:
\begin{lstlisting}
$ cd ./AE_root/ycsb_redis/
$ # Test tail latency @ QPS
$ bash fig_6a.sh
\end{lstlisting}
Results are placed under:
\begin{lstlisting}
$ ./AE_root/ycsb_redis/fig_6a
\end{lstlisting}
This test takes about 2 hours to finish.

\subsubsection{Figure 6-b/c/d}
There are three workload in the social network of the DeathStarBench, two of which invovles accessing the backend storage and caching layer. We see that there's a significant difference in p99 latency for the composing post operation, while little to no difference in the read user timeline operations. This is because the databases are accessed more intensively in the case of composing posts. A mix of these workloads represents a realistic environment, where most people read posts that were posted by a few. The steps to produce these figures are as follows:
\begin{lstlisting}
$ cd ./AE_root/dsb/socialNetwork/wrk2
$ fig_6b.sh
$ fig_6c.sh
$ fig_6d.sh
\end{lstlisting}
Results are placed under:
\begin{lstlisting}
$ ./AE_root/dsb/socialNetwork/wrk2/fig_6b
$ ./AE_root/dsb/socialNetwork/wrk2/fig_6c
$ ./AE_root/dsb/socialNetwork/wrk2/fig_6d
\end{lstlisting}
Each test takes about 1 hour to finish.

\subsubsection{Figure 7}
We compare the p99 latency of Redis for approximately the same memory ratio on CXL and DDR, where in one instance, the memory is statically allocated to a specific ratio, whereas memory is constantly promoted and demoted in the other case. We observed that p99 latency is much higher in the case of TPP, as page promotion and demotion between DDR and CXL memory inevitably incurs overhead that degrades Redis' performance.

\subsubsection{Figure 8}
By changing the block size of the storage access (from 4KB to 512KB), we observe that 1) the percent of p99 latency increase, from using DDR to using CXL, is relatively small when the block size is small. As the block size becomes larger, serving page cache from CXL is limited by the CXL bandwidth, making its p99 latency higher than that of DDR memory. The steps to produce these figures are as follows:
\begin{lstlisting}
$ cd /home/yans3/AE_root/fio
$ bash figure_8.sh
\end{lstlisting}
Results are placed under:
\begin{lstlisting}
$ cd ./AE_root/fio/result/fig_8
\end{lstlisting}
The test is lengthy and might take the entire night to finish (5 iteration * 7 config * 5 min/test).

\subsubsection{Figure 9-a}
By testing the DLRM throughput at various thread count and DDR:CXL ratio, we are able to make the following conclusion: 1) before the DDR bandwidth is saturated, placing all memory in DDR yields the optimal performance, as the application is still memory-latency sensitive. As the thread-count surpass 20, placing some memory on CXL will bring performance benefit, as the application is memory-bandwidth bounded. The steps to produce this figure are as follows:
\begin{lstlisting}
$ cd ./AE_root/asplos21-merci/4_performance_evaluation/
$ # test Redis max QPS
$ bash fig_9a.sh
\end{lstlisting}
Results are placed under:
\begin{lstlisting}
$ ./AE_root/asplos21-merci/4_performance_evaluation/\
    results/fig_9a
\end{lstlisting}
This test takes about 1 hour to finish.

\subsubsection{Figure 9-b}
Unlike DLRM, Redis is single threaded and highly memory-latency sensitive. Across all tested YCSB workloads, having all memory placed on DDR always yield a higher maximum throughput than place any amount of memory on CXL. The steps to produce this figure are as follows:
\begin{lstlisting}
$ cd ./AE_root/ycsb_redis/
$ # test Redis max QPS
$ bash fig_9b.sh
\end{lstlisting}
Results are placed under:
\begin{lstlisting}
$ ./AE_root/ycsb_redis/fig_9b
\end{lstlisting}
This test takes about 1 hour to finish.

\subsubsection{Figure 13}
We test \policy with various application, and we are able to observe that 1) for memory-bandwidth-bounded applications, \policy is capable of adjusting the interleaving knob to make use of the extra memory bandwidth provided by the CXL memory, 2) for memory-latency-sensitive applications, \policy will not tune toward using more CXL memory as the linear model is guarded by fundamental performance metrics like IPC and L1 miss latency. The steps to produce this figure are as follows:\\
Under this directory
\begin{lstlisting}
cd ./AE_root/caption/
\end{lstlisting}
For \textbf{SPEC (group 1-6)}, execute:
\begin{lstlisting}
$ # local node
$ bash experiment/spec/fotonik_3d/fotonik_3d_local.sh
$ # 50:50
$ bash experiment/spec/fotonik_3d/fotonik_3d_50_50.sh
$ # Caption tune
$ python3 caption.py -s experiment/spec/fotonik_3d/fotonik_3d_itlv.sh
\end{lstlisting}
This is an example for \texttt{fotonik\_3d}. Three other SPEC benchmark: \texttt{mcf}, \texttt{cactuBSSN}, \texttt{roms} will follow the same scheme of testing. Two other SPEC-mixs are available: \texttt{roms\_cactu}, \texttt{roms\_mcf} will also follow the same procedure. The result will be in:
\begin{lstlisting}
$ ./results/spec/<benchmark name>/<config>
\end{lstlisting}

For \textbf{Redis-QPS (group 7)}, execute:
\begin{lstlisting}
$ # local node
$ bash experiment/redis_test/redis_local.sh
$ # 50:50
$ bash experiment/redis_test/redis_50_50.sh
$ # Caption tune
$ python3 caption.py -s experiment/redis_test/redis_itlv.sh
\end{lstlisting}
The result will be in:
\begin{lstlisting}
$ ./results/spec/redis_tune/
\end{lstlisting}

For \textbf{DLRM (group 8)}, execute:
\begin{lstlisting}
$ # local node
$ bash experiment/dlrm/dlrm_local.sh
$ # 50:50
$ bash experiment/dlrm/dlrm_50_50.sh
$ # Caption tune
$ python3 caption.py -s experiment/dlrm/dlrm_itlv.sh
\end{lstlisting}
The result will be in:
\begin{lstlisting}
$ ./results/spec/dlrm_tune/
\end{lstlisting}

For \textbf{R+D (group 9)}, execute:
\begin{lstlisting}
$ # local node
$ bash experiment/redis_dlrm_corun/redis_dlrm_local.sh
$ # 50:50
$ bash experiment/redis_dlrm_corun/redis_dlrm_50_50.sh
$ # Caption tune
$ python3 caption.py -s experiment/redis_dlrm_corun/redis_dlrm_itlv.sh
\end{lstlisting}
The result will be in:
\begin{lstlisting}
$ ./results/spec/redis_dlrm_corun/
\end{lstlisting}
\end{comment}

\subsection{Experiment customization}
To run DeathStarBench with the proper NUMA node in \figref{fig:dsb_compose}, \ref{fig:dsb_read_user}, and \ref{fig:dsb_mixed}, we modify the following docker images to pin their CPU and memory to the desired NUMA node.
\begin{itemize}
    \item MongoDB: Change entrypoint.sh
    \item Redis: Install numactl
    \item Memcached: Install numactl
\end{itemize}

\noindent For Redis and Memcached, the \texttt{numactl} command is embedded in the docker-compose file, whereas MongoDB requires manual changes to the \texttt{entrypoint.sh} within the docker image. Please refer to the \texttt{setup\_docker.md} in the repository for the detailed steps in modifying the docker images.