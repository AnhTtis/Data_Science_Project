% !TEX root = paper.tex
\section{Background}
\label{sec:back}
\begin{figure*}
    \centering
    \includegraphics[width=\linewidth]{figs/arch.pdf}
    \vspace{-10pt}
    \caption{A typical system architecture with CXL-based memory devices (left) and the memory transaction flow in CXL protocol (right).}
    \label{fig:arch}
    \vspace{-15pt}
\end{figure*}

\subsection{PCIe}
PCI Express (PCIe) is a high-speed computer extension bus that replaces the older PCI buses. Since 2003, the bandwidth doubles in each generation, and by PCIe generation 5.0, the speed has reached 64 GB/s in a x16 setting. The growth of the PCIe bandwidth benefits devices like smart network interface card (SNIC), GPU, accelerators, and future SSDs, enabling higher throughput and lower latency among these computational units.

\subsection{CXL}
Compute Express Link (CXL)~\cite{cxl1,cxl2} builds a cache-coherent system over the PCIe physical layer. 
% PCIe vs. CXL?
Similar to the standard PCIe communication, where data transfers with Transaction-Layer Packets (TLPs) headers and Data-Link-Layer Packets (DLLPs), subset of the CXL protocol uses predefined headers and 16B blocks to transfer data. In CXL 1.1, depending on the protocol and the data transferred, the CXL hardware will pack the header and data into a 68B flit (64B CXL data + 2B CRC) based on a set of rules described in the CXL specification~\cite{cxlspec}. Unless otherwise stated, CXL refers to CXL 1.1 for the remainder of this paper. 

% brief intro of three modes
At the time of writing this paper, CXL supports three protocols: \cxlio, \cxlcache, and \cxlmem. \cxlio uses features like TLP and DLLP from standard PCIe transactions~\cite{cxl1}, and it is mainly used for protocol negotiation and host-device initialization. \cxlcache and \cxlmem use the aforementioned protocol headers for the device to access the host's memory and for the host to access the device's memory, respectively.

By combining these three protocols, CXL identifies three types of devices for different use cases. Type-1 devices use \cxlio and \cxlcache, which usually refer to SmartNICs and accelerators where host-managed memory does not apply. Type-2 devices support all three protocols. These devices, like GP-GPUs and FPGAs, have attached memory (DRAM, HBM) that the host CPU can access and cache, and they also use \cxlcache for device-to-host memory accesses. Type-3 devices support \cxlio and \cxlmem, and such devices are usually treated as memory extensions to existing systems. In this paper, we will focus on Type-3 devices and present the lower-level details of \cxlmem.

% CXL.mem communication
Since the \cxlmem protocol only accounts for host-to-device memory accesses, the protocol consists of two simple memory accesses: read and write from the host to the device memory. Each access is accompanied by a completion reply from the device. The reply contains data when reading from the device memory and only contains the completion header in the case of write. \figref{fig:arch} illustrates the round trips of these accesses. 

The \cxlmem protocol is communicated between the CPU home agent and the CXL controller on the device. While the home agent handles the protocol, the CPU issues load and store instructions to access memory in the same way as it accesses DRAM. This has an advantage over other memory expansion solutions, such as Remote Direct Memory Access (RDMA), which involves DMA engine on the device and thus has different semantics. Integrating \texttt{load/store} instructions with \cxlmem also means the CPU will cache the PCIe attached memory in all level of its caches, which is impossible for any other memory extension solution, beside persistent memory.

% some prelim stat?

% CXL.mem fig

%\yan{Maybe mention it's type3 in type2 flow}

\subsection{CXL-enabled Commodity Hardware}
\label{sec:cxl_enabled_hw}
CXL requires support from both the device side and the host (CPU) side. As of today, in addition to some research prototypes, multiple CXL-enabled memory devices have been designed by major hardware vendors such as Samsung~\cite{smdk_github}, SK Hynix~\cite{skhynix-cxl}, Micron~\cite{micron-cxl}, and Montage~\cite{montage-cxl}. To facilitate more flexible memory functionalities and near-memory computing research, Intel also enables \cxlmem on its latest Agilex-I series FPGA~\cite{intel-agi}, where the CXL- and memory-related IP cores are hard coded on the chiplet~\cite{rtile_cxl_ip} to achieve high performance. 
On the CPU side, Intel's latest 4\textsuperscript{th} Gen Xeon Scalable Processor (code name Sapphire Rapids or SPR) is among the first high-performance commodity CPUs to support CXL 1.1 standard~\cite{spr, amd_4th_gen_epic}. 
We anticipate that more and more hardware vendors will have richer CXL supports in their products in the near future.



%\yan{Emphasize Intel contribution}
%Previous studies on \cxlmem are usually done via emulation~\cite{pond, tpp}. A common starting point is to emulate \cxlmem's latency by adding additional latency to cross-NUMA memory accesses. Not until recently, CXL capable devices have become available on the market. Major CPU vendors have released the next generation CPU that supports CXL 1.1 ~\cite{spr, amd_4th_gen_epic}, while other vendors have release their \cxlmem solutions in hardware~\cite{rtile_cxl_ip, montage-cxl} and/or software~\cite{smdk_github}.
% Previously, emulation
% CPU, devices, samsung/agliex/micro/synopsys

\begin{table}[!tb]
    \centering
    \caption{\arch Testbed Configurations \nskim{add OS information}}
  \footnotesize
    \label{tab:res}
    \begin{tabular}{lr}
      \toprule
     \bf Single socket -- Intel\textsuperscript{\textregistered} Xeon 6414U CPUs@2.0GHz~\cite{6414u} &   \bf  \\
      \midrule
      \multicolumn{2}{l}{32 cores, hyperthreading enabled, running Ubuntu 22.04} \\ 
      \multicolumn{2}{l}{60MB shared LLC}\\
      \multicolumn{2}{l}{Eight DDR5-4800 channels, 128GB DRAM in total}\\
      \multicolumn{2}{l}{CXL 1.1 Type3 with PCIe 5.0 x16, 16GB DRAM in total}\\
      \midrule
      
     \bf Dual socket -- $2\times$ Intel\textsuperscript{\textregistered} Xeon 8460H CPUs@2.0GHz~\cite{8460h} &   \bf  \\
      \midrule
      \multicolumn{2}{l}{40 cores per socket, hyperthreading enabled, running Ubuntu 22.04} \\ 
      \multicolumn{2}{l}{105MB LLC per socket, 210MB in total}\\
      \multicolumn{2}{l}{Eight DDR5-4800 channels per socket, 256GB DRAM in total}\\
      \midrule
      
     \bf Intel\textsuperscript{\textregistered} Agliex I-series FPGA@400MHz~\cite{intel-agi} &   \bf  \\
      \midrule
     \multicolumn{2}{l}{Harden CXL 1.1 IP, runs on PCIe 5.0 x16}\\
     \multicolumn{2}{l}{Single DIMM with 16GB DDR4-2666}\\
    Logic utilization& 177K (19\%) \\ 
    Registers usage& 316K (\%)\\
    BRAM blocks usage& 1.5K (12\%) \\
    \bottomrule
    \label{table:config-table}
    \end{tabular}
\end{table}


\section{Experimental Setup}
\label{sec:setup}
In this work, we use two testbeds to evaluate the latest commodity CXL hardware, as summarized in Table~\ref{table:config-table}. 
The server is equipped with Intel Gold 6414U CPU and 128GB 4800MT/s DDR5 DRAM (spread across 8 memory channels). In the 4\textsuperscript{th} gen Xeon, CPU is implemented in four individual chiplets. The user may decide to use the 4 chiplets as a unified processor (shared LLC, iMCs, root complexes), or in the sub-NUMA clustering (SNC) mode, where each chiplet is a small NUMA node. Such flexibility allows users to fine-tune their system to fit their workload characteristics and apply fine-grain control over resource sharing and isolation. In our experiments, we will explore how memory interleaving across SNC and \cxlmem will affect the applications' performance. We also conducted some experiments on a dual-socket system, with two Intel Platinum 8460H and the same DDR5 DRAM, to draw some comparison between the NUMA-based memory and \cxlmem.

For \cxlmem device, the system has one Intel Agilex-I Development Kit~\cite{intel-agi}. It has 16GB 2666MT/s DDR4 DRAM as the CXL memory, and it is connected to the CPU via an x16 PCIe Gen5 (CXL-enabled) interface. The kit is transparently exposed to the CPU and OS as a NUMA node with no CPU core and 16GB memory, and the usage of the CXL memory is the same as regular NUMA-based memory management. 

Note that, the CXL protocol itself does not define the configurations of CXL memory. Such configurations include but are not limited to capacity, medium (DRAM, persistent memory, flash chips, \etc), and the number of memory channels. Hence, different devices may exhibit diverse performance characteristics.
 

