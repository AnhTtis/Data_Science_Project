\section{Background}
\label{sec:back}

\subsection{Compute eXpress Link (CXL)}
\label{sec:back:cxl}
PCIe is the industry standard for a high-speed serial interface between a CPU and I/O devices.
%
Each lane of the current PCIe 5.0 can deliver 32 GT/s (\eg, $\sim$64~GB/s with 16 lanes).
%
Built on the physical layer of PCIe, the CXL standard defines three separate protocols: \cxlio, \cxlcache, and \texttt{CXL.mem}. 
%
\cxlio uses protocol features of the standard PCIe, such as transaction-layer packet (TLP) and data-link-layer packet (DLLP), to initialize the interface between a CPU and a device~\cite{cxl1}. 
%
\cxlcache and \texttt{CXL.mem} use the aforementioned protocol features for the device to access the CPU's memory and for the CPU to access the device's memory, respectively.
%


The \texttt{CXL.mem} protocol accounts only for memory accesses from the CPU to the device facilitated by the Home Agent (HA) and the CXL controller on the CPU and the device, respectively~\cite{snia-cxl}. 
%
The HA handles the \texttt{CXL.mem} protocol and transparently exposes \cxlmem to the CPU as memory in a remote NUMA node. 
%
That is, the CPU can access \cxlmem with load and store instructions in the same way as it accesses memory in a remote NUMA node. 
%
This has an advantage over other memory expansion technologies, such as RDMA, which involves the device's DMA engine and thus has different memory access semantics. 
%
Lastly, %by being able to 
when the CPU accesses \cxlmem, 
%with load and store instructions, %the CPU caches 
it caches data from/to the \cxlmem in every level of its cache hierarchy. 
%
This has been impossible with any other memory extension technologies except for persistent memory.
%


\begin{figure}[!t]
    \centering
    %\vspace{-3pt}
    \includegraphics[width=\linewidth]{figs/cxl_controller_architecture.pdf}
    \vspace{-12pt}
    \caption{CXL.mem controller architecture.} 
    \label{fig:cxl-mem-ctrl}
    \vspace{-6pt}

\end{figure}


\subsection{CXL-ready Systems and Memory Devices}
\label{sec:cxl_enabled_hw}
CXL requires hardware support from both the CPU and  devices. 
%
Both the latest 4\textsuperscript{th}-generation Intel Xeon Scalable Processor (Sapphire Rapids) 
and the latest 4\textsuperscript{th}-generation AMD EPYC Processor (Genoa) are among the first server-class commodity CPUs to support the CXL 1.1 standard~\cite{spr, amd_4th_gen_epic}. 
%
\figref{fig:cxl-mem-ctrl} depicts a typical architecture of \texttt{CXL.mem} controllers.
%
It primarily consists of (1) PCIe physical layer, (2) CXL link layer, (3) CXL transaction layer, and (4) memory controller blocks.
%
(2), (3), and other CXL-related components are collectively referred to as CXL IP in this paper.
%
As of today, in addition to some research prototypes, multiple \cxlmem devices have been designed by major hardware manufacturers, such as Samsung~\cite{smdk_github}, SK Hynix~\cite{skhynix-cxl}, Micron~\cite{micron-cxl}, and Montage~\cite{montage-cxl}. 
%
To facilitate more flexible memory functionality and near-memory computing capability, Intel also enables the CXL protocol in its latest Agilex-I series FPGA~\cite{intel-agi}, integrated with hard CXL IPs to support the \cxlio, \cxlcache, and \texttt{CXL.mem}~\cite{rtile_cxl_ip}. 
%
Lastly, unlike a true NUMA node typically based on a large server-class CPU, a \cxlmem device does not have any CPU cores, caches, or long interconnects between the CXL IP and the memory controller in the device. 
%


\section{Evaluation Setup}
\label{sec:setup}
\subsection{System and Device}
\label{sec:setup:system}
\niparagraph{Systems.} We use a server to evaluate the latest commercial hardware supporting \cxlmem (Table~\ref{table:config-table}). 
%
The server consists of two Intel Sapphire Rapids (SPR) CPU sockets. 
%
One socket is populated with eight 4800~MT/s DDR5 DRAM DIMMs (128~GB) across eight memory channels. 
%
The other socket is populated with only one 4800~MT/s DDR5 DRAM DIMM to emulate the bandwidth and capacity of \cxlmem. %, without using the CPU cores in the socket. 
%
The Intel SPR CPU integrates four CPU chiplets, each with up to 15 cores and two DDR5 DRAM channels. 
%
A user can choose to use the 4 chiplets as a unified CPU, or each chiplet (or two chiplets) as a NUMA node in the SNC mode.
%
Such flexibility is to give users strong isolation of shared resources, such as LLC, among applications. 
%
Lastly, we turn off the hyper-threading feature and set the CPU core clock frequency to 2.1~GHz for more predictable performance.
%

\niparagraph{CXL memory devices.} We take three \cxlmem devices (`\cxlmem devices' in Table~\ref{table:config-table}), each featuring different CXL IPs (ASIC-based hard IP and FPGA-based soft IP) and DRAM technologies (DDR5-4800, DDR4-2400, and DDR4-3200). 
%
Since the CXL protocol itself does not prescribe the underlying memory technology, it can seamlessly and transparently accommodate not only DRAM but also persistent memory, flash~\cite{ms-ssd}, and other emerging memory technologies.
%
Consequently, various \cxlmem devices may exhibit different latency and bandwidth characteristics.
%


\begin{table}[t!]
\caption{System configurations.}
\label{table:config-table}
\vspace{-10pt}
\begin{center}
\resizebox{\columnwidth}{!}{%
\begin{tabular}{c||ccc}
\hline
\multicolumn{4}{c}{\textbf{Dual-socket server system}}\TBstrut\\
\hline
\textbf{Component} & \multicolumn{3}{c}{\textbf{Desription}}\TBstrut\\
\hline
OS (kernel) & \multicolumn{3}{c}{Ubuntu 22.04.2 LTS (Linux kernel v6.2)}\Tstrut\\
\multirow{2}{*}{CPU} & \multicolumn{3}{c}{2$\times$ Intel\textsuperscript{\textregistered} Xeon 6430 CPUs @2.1~GHz~\cite{6430}, 32 cores }\Tstrut\\
& \multicolumn{3}{c}{and 60~MB LLC per CPU, Hyper-Threading disabled}\\
\multirow{2}{*}{Memory} & \multicolumn{3}{l}{Socket 0: 8$\times$ DDR5-4800 channels}\Tstrut\\
& \multicolumn{3}{l}{Socket 1: 1$\times$ DDR5-4800 channel (emulated CXL memory)}\Bstrut\\
\hline
\hline
\multicolumn{4}{c}{\textbf{CXL memory devices}}\TBstrut\\
\hline
\textbf{Device} & \textbf{CXL IP} & \textbf{Memory technology} & \textbf{Max. bandwidth}\TBstrut\\
\hline
CXL-A & Hard IP & DDR5-4800 & 38.4~GB/s per channel\Tstrut\\
CXL-B & Hard IP & 2$\times$ DDR4-2400 & 19.2 GB/s per channel\\
CXL-C & Soft IP & DDR4-3200 & 25.6~GB/s per channel\Bstrut\\
\hline
\end{tabular}
}
\label{tab:smartssd}
\end{center}
\vspace{-6pt}
\end{table}


\subsection{Microbenchmark}
\label{sec:setup:microbench}
To characterize the performance of \cxlmem, we use two microbenchmarks.
%
First, we use Intel Memory Latency Checker (MLC)~\cite{mlc},  a tool used to measure memory latency and bandwidth for various usage scenarios.
%It also provides several options for more fine-grained investigation where b/w and latencies from a specific set of cores to caches or memory can be measured as well
%
Second, we use a microbenchmark dubbed \arch (\textbf{\underline{m}}easuring \textbf{\underline{e}}fficiency of \textbf{\underline{m}}em\textbf{\underline{o}}ry subsystems).
%
It shares some features with Intel MLC, but we develop it to give more control over characterizing memory subsystem performance in diverse ways.
%
For instance, it can measure the latency and bandwidth of a specific memory access instruction (\eg, AVX-512 non-temporal load and store instructions). 
%

\subsection{Benchmark}
\label{sec:setup:benchmark}
\niparagraph{Latency-sensitive applications.}
We run \texttt{Redis}~\cite{redis}, a popular high-performance in-memory key-value store, with \texttt{YCSB}~\cite{socc10:ycsb}. % as a latency-sensitive application. 
%
We use a uniform distribution of keys, ensuring maximum stress on the memory subsystem, unless we explicitly specify the use of other distributions. 
%
We also run DeathStarBench (\texttt{DSB})~\cite{deathstartbench}, an open-source benchmark suite designed to evaluate the performance of microservices. %, as another latency-sensitive application. 
%
It uses \texttt{Docker} to launch components of a microservice, including machine learning (ML) inference logic, web backend, load balancer, caching, and storage.
%
Specifically, we evaluate three \texttt{DSB} workloads: (1) \texttt{compose posts}, (2) \texttt{read user timelines}, and (3) \texttt{mixed workloads} (10\% of \texttt{compose posts}, 30\% of  \texttt{read user timelines}, and 60\% of \texttt{read home timelines}) as a social network framework. 
%
Lastly, we run \texttt{FIO}~\cite{GitHubax76:online}, an open-source  tool used for benchmarking storage devices and file systems, to evaluate the latency impact of using \cxlmem for OS page cache.
%
The page cache is supported by the standard Linux storage subsystem, which holds recently accessed storage data (\eg, files) in unused main memory space to reduce the number of accesses to slow storage devices. 
%


\niparagraph{Throughput applications.}
%
First, we run an inference application based on a deep learning recommendation model (\texttt{DLRM})  with the same setup as \texttt{MERCI}~\cite{asplos21lee}. 
%
The embedding reduction step in DLRM inference is known to have a large memory footprint and is responsible for 50--70\% of the inference latency~\cite{asplos21lee}. 
%
Second, we take the SPECrate CPU2017 benchmark suite~\cite{SPECCPU70:online}, which is commonly used to evaluate the throughput of systems in datacenters. Then we assess misses per kilo instructions (MPKI) of every benchmark and run the four benchmarks with the highest MPKI: (1) \texttt{fotonik3d}, (2) \texttt{mcf}, (3) \texttt{roms}, and (4) \texttt{cactuBSSN}.
%
We run multiple instances of a single benchmark or two different benchmarks.
%