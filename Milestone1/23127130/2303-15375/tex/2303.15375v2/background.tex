% !TEX root = paper.tex
\section{Background}
\label{sec:back}
\begin{figure*}
    \centering
    \includegraphics[width=\linewidth]{figs/V4.pdf}
    \vspace{-10pt}
    \caption{Typical CXL-enabled system architecture (left) and memory transaction flow of \cxlmem protocol (right)}
    \label{fig:arch}
    \vspace{-15pt}
\end{figure*}

\subsection{Compute eXpress Link (CXL)}
PCI Express (PCIe) is a standard for high-speed serial computer expansion bus that replaces the older PCI buses. Since 2003, the bandwidth has doubled in each generation, and as of PCIe Gen 5, the bandwidth has reached 32 GT/s (i.e., 64 GB/s with 16 lanes). Its point-to-point topology aid by the growth of bandwidth enables low-latency, high-bandwidth communication with PCIe-attached devices such as Graphics Processing Units (GPUs), Network Interface Cards (NICs), and NVMe Solid-State Drives (SSDs).

CXL~\cite{cxl1,cxl2} builds a cache-coherent system over the PCIe physical layer. 
% PCIe vs. CXL?
Similar to the standard PCIe communication, where data transfers with Transaction-Layer Packets (TLPs) headers and Data-Link-Layer Packets (DLLPs), subset of the CXL protocol uses predefined headers and 16~B blocks to transfer data. In CXL~1.1, depending on the protocol and the data transferred, the CXL hardware will pack the header and data into a 68~B flit (64~B CXL data + 2~B CRC + 2~B Protocol ID) based on a set of rules described in the CXL specification~\cite{cxlspec}. Unless otherwise stated, CXL refers to CXL 1.1 for the remainder of this paper. 

% brief intro of three modes
The CXL standard defines three separate protocols: \cxlio, \cxlcache, and \cxlmem. \cxlio uses features like TLP and DLLP from standard PCIe transactions~\cite{cxl1}, and it is mainly used for protocol negotiation and host-device initialization. \cxlcache and \cxlmem use the aforementioned protocol headers for the device to access the host's memory and for the host to access the device's memory, respectively.

By combining these three protocols, CXL identifies three types of devices for different use cases. Type-1 devices use \cxlio and \cxlcache, which usually refer to SmartNICs and accelerators where host-managed memory does not apply. Type-2 devices support all three protocols. These devices, like GP-GPUs and FPGAs, have attached memory (DRAM, HBM) that the host CPU can access and cache, and they also use \cxlcache for device-to-host memory accesses. Type-3 devices support \cxlio and \cxlmem, and such devices are usually treated as memory extensions to existing systems. In this paper, we will focus on Type-3 devices and present the lower-level details of \cxlmem.

% CXL.mem communication
Since the \cxlmem protocol only accounts for host-to-device memory accesses, the protocol consists of two simple memory accesses: read and write from the host to the device memory. Each access is accompanied by a completion reply from the device. The reply contains data when reading from the device memory and only contains the completion header in the case of write. \figref{fig:arch} illustrates the round trips of these accesses. 

The \cxlmem protocol is communicated between the CPU home agent and the CXL controller on the device. While the home agent handles the protocol, the CPU issues load and store instructions to access memory in the same way as it accesses DRAM. This has an advantage over other memory expansion solutions, such as Remote Direct Memory Access (RDMA), which involves DMA engine on the device and thus has different semantics. Integrating \texttt{load/store} instructions with \cxlmem also means the CPU will cache the PCIe attached memory in all level of its caches, which is impossible for any other memory extension solution, beside persistent memory.

% some prelim stat?

% CXL.mem fig

%\yan{Maybe mention it's type3 in type2 flow}

\subsection{CXL-enabled Commodity Hardware}
\label{sec:cxl_enabled_hw}
CXL requires supports from both the host CPU side and the device side. As of today, in addition to some research prototypes, multiple CXL-enabled memory devices have been designed by major hardware vendors such as Samsung~\cite{smdk_github}, SK Hynix~\cite{skhynix-cxl}, Micron~\cite{micron-cxl}, and Montage~\cite{montage-cxl}. To facilitate more flexible memory functionalities and near-memory computing research, Intel also enables \cxlmem on its latest Agilex-I series FPGA~\cite{intel-agi}, where the CXL- and memory-related IP cores are hard coded on the chiplet~\cite{rtile_cxl_ip} to achieve high performance. 
On the host CPU side, Intel's latest 4\textsuperscript{th} Gen Xeon Scalable Processor (codename Sapphire Rapids, SPR) is the first high-performance commodity CPUs to support CXL 1.1 standard~\cite{spr, amd_4th_gen_epic}. 
We anticipate that more and more hardware vendors will have richer CXL supports in their products in the near future.



%\yan{Emphasize Intel contribution}
%Previous studies on \cxlmem are usually done via emulation~\cite{pond, tpp}. A common starting point is to emulate \cxlmem's latency by adding additional latency to cross-NUMA memory accesses. Not until recently, CXL capable devices have become available on the market. Major CPU vendors have released the next generation CPU that supports CXL 1.1 ~\cite{spr, amd_4th_gen_epic}, while other vendors have release their \cxlmem solutions in hardware~\cite{rtile_cxl_ip, montage-cxl} and/or software~\cite{smdk_github}.
% Previously, emulation
% CPU, devices, samsung/agliex/micro/synopsys

\begin{table}[!tb]
    \centering
    \caption{Testbed configurations}
  \footnotesize
    \label{tab:res}
    \begin{tabular}{lr}
      \toprule
     \bf Single socket -- Intel\textsuperscript{\textregistered} Xeon 6414U CPUs @2.0~GHz~\cite{6414u} &   \bf  \\
      \midrule
      \multicolumn{2}{l}{32 cores, hyperthreading enabled, running Ubuntu 22.04} \\ 
      \multicolumn{2}{l}{60~MB shared LLC}\\
      \multicolumn{2}{l}{Eight DDR5-4800 channels, 128~GB DRAM in total}\\
      \multicolumn{2}{l}{CXL 1.1 with PCIe Gen 5 x16, 16~GB DRAM in total}\\
      \midrule
      
     \bf Dual socket -- $2\times$ Intel\textsuperscript{\textregistered} Xeon 8460H CPUs @2.0~GHz~\cite{8460h} &   \bf  \\
      \midrule
      \multicolumn{2}{l}{40 cores per socket, hyperthreading enabled, running Ubuntu 22.04} \\ 
      \multicolumn{2}{l}{105~MB LLC per socket, 210~MB in total}\\
      \multicolumn{2}{l}{Eight DDR5-4800 channels per socket, 256~GB DRAM in total}\\
      \midrule
      
     \bf Intel\textsuperscript{\textregistered} Agliex I-series FPGA Dev Kit @400~MHz~\cite{intel-agi} &   \bf  \\
      \midrule
     \multicolumn{2}{l}{Hard CXL 1.1 IP, runs on PCIe Gen 5 x16}\\
     \multicolumn{2}{l}{Single DIMM with 16~GB DDR4-2666}\\
    %Logic utilization& 177K (19\%) \\ 
    %Registers usage& 316K (\%)\\
    %BRAM blocks usage& 1.5K (12\%) \\
    \bottomrule
    \label{table:config-table}
    \end{tabular}
\end{table}

% \begin{table}[t!]
% \caption{Configurations of Testbeds}
% \vspace{-10pt}
% \label{tab:dsa-operation}
% \begin{center}
% \resizebox{\columnwidth}{!}{%
% \begin{tabular}{|c|c|c|}
% \hline 
% \multirow{5}{*}{\texttt{SPR1}}      & OS (kernel)   &   Ubuntu 22.04 (5.15)\\
%                                     & CPU           & Intel Gold 6414U (32 cores, 60 MB LLC)\\
%                                     & Memory (DRAM) &\\
%                                     & \multirow{2}{*}{Memory (CXL)} & 16 GB 2666 MT/s DDR4 DRAM \Bstrut\\
%                                     &                               & x16 PCIe Gen5 \Bstrut\\
% \hline
% \multirow{2}{*}{\texttt{SPR2}}      & OS (kernel)   &   \Tstrut\\
%                                     & CPU           & Intel Gold 6414U\\
% \hline
% \end{tabular}
% }
% \label{tab1}
% \end{center}
% \vspace{-14pt}
% \end{table}


\section{Experimental Setup}
\label{sec:setup}
In this work, we use two testbeds to evaluate the latest commodity CXL hardware, as summarized in Table~\ref{table:config-table}. 
The server is equipped with Intel Gold 6414U CPU and 128~GB 4800MT/s DDR5 DRAM (spread across 8 memory channels). In the 4\textsuperscript{th} generation, an Intel Xeon CPU is implemented as four individual chiplets. Users can decide to use the 4 chiplets as a unified processor (i.e., shared Last-Level Cache (LLC), Integrated Memory Controllers (iMCs), and root complexes), or in the Sub-NUMA Clustering (SNC) mode, where each chiplet operates as a small NUMA node. Such flexibility allows users to fine-tune their system to fit their workload characteristics and apply fine-grain control over resource sharing and isolation. In our experiments, we will explore how memory interleaving across SNC and \cxlmem will affect the performance of applications. We also conduct some experiments on a dual-socket system, with two Intel Platinum 8460H and the same DDR5 DRAM, to draw some comparison between the regular NUMA-based memory and \cxlmem.

For a CXL memory device, the system has one Intel Agilex-I Development Kit~\cite{intel-agi}. It has 16~GB 2666MT/s DDR4 DRAM as CXL memory, and it is connected to the CPU via an x16 PCIe Gen 5 interface. It is transparently exposed to the CPU and OS as a NUMA node having 16~GB memory without CPU cores, and the usage of the CXL memory is the same as regular NUMA-based memory management. 

Note that, the CXL protocol itself does not define the configurations of the underlying memory. Such configurations include but are not limited to capacity, medium (DRAM, persistent memory, flash chips, \etc), and the number of memory channels. Hence, different devices may exhibit diverse performance characteristics.
 

