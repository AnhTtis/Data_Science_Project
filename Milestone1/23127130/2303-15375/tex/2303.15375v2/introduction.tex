% !TEX root = paper.tex
\section{Introduction}
\label{sec:intro}

The explosive need of storing and processing data in the datacenters and the limited bandwidth and capacity scalability of traditional memory interfaces such as DDR have demanded new memory interface technologies and system architectures. Among them, Compute eXpress Link (CXL)~\cite{cxl2} has emerged as one of the most promising technology not only for memory capacity/bandwidth expansion, but also for memory disaggregation in both the industry and academia. 

CXL is an open standard made by a joint effort of major hardware vendors and cloud providers in 2019 and is still evolving rapidly. Specifically, compared to the conventional PCIe interconnect, it provides a set of new features that enable the CPU to communicate with peripheral devices (and their attached memory) in a cache-coherent way with \texttt{Load/Store} semantics. As such, memory-related device extension is one of CXL's major targeting scenarios~\cite{micron-cxl, montage-cxl, rambus-cxl, smdk_github}. As the de-facto standard for future datacenter, major hardware vendors have announced CXL support in their product roadmaps~\cite{spr, amd_4th_gen_epic, micron-cxl, montage-cxl, smdk_github}.

Given its popularity and promising visions, CXL memory has attracted much attention in the community. However, due to the lack of commercially-available hardware (especially CPU) with CXL support, most recent studies on CXL memory have been based on emulation using multi-socket NUMA systems, as CXL memory is exposed as a NUMA node~\cite{pond, tpp}. As such, these studies may not be able to accurately model and characterize the CXL memory in the real world.

With the arrival of Intel 4\textsuperscript{th}-generation Xeon scalable CPU (Sapphire Rapids or SPR)~\cite{spr} and commodity CXL devices~\cite{montage-cxl, intel-agi}, we are able to start to understand the practical characteristics of CXL memory, as well as customizing software systems that make the most out of such characteristics.  
In this work, we conduct a comprehensive analysis on CXL memory with multiple microbenchmarks and end-to-end applications on our testbeds consisting of Intel SPR CPUs and Intel Agilex-I FPGA based CXL memory (CXL controller hardened in R-Tile)~\cite{cxl-rtile-ip}. From our microbenchmarks, we find that CXL memory behaves differently from memory in a remote NUMA node, which is often used for emulation. Compared to NUMA-based memory, real CXL memory has: 
(1) higher latency, 
(2) fewer memory channels (leads to lower throughput), 
and (3) different transfer efficiency under various operations.

Based on the aforementioned observations, we also apply CXL memory to three real applications exhibiting different memory access behaviors. We find that they have diverse sensitivities to the CXL memory offloading. Specifically, we found that 
(1) $\mu$s-latency database is highly sensitive to the increase in memory latency, 
(2) ms-latency microservices, having layers of intermediate computations, are less affected when the databases run on CXL memory, 
(3) memory-intensive ML inference is sensitive to the random access throughput offered by CXL memory. 
In all cases, interleaving memory across the CPU-attached DRAMs and CXL memory reduces the performance penalty introduced by CXL memory.

Subsequently, we provide some practical guidelines for users to optimize their software stacks/library for the highest performance after analyzing the performance characteristics of the microbenchmarks and applications running on a system using CXL memory. For example, one should use evenly distribute the bandwidth across CXL memory and DRAM to maximize the performance; one should use cache-bypassing instructions for data movement from/to CXL memory; for a single CXL memory channel, since a few threads can easily saturate the load or store bandwidth, one should limit the number of write threads to CXL memory to reduce write interference; and one should target read-heavy application that run in ms-level latency, where the higher CXL memory latency can be amortized by intermediate computations.

The remainder of this paper is organized as follows. We give a brief introduction to CXL in \S~\ref{sec:back} and describe our experimental setup in \S~\ref{sec:setup}. We then present our findings by profiling CXL memory with our microbenchmark in \S~\ref{sec:character}, and with three representative applications in \S~\ref{sec:app}. Finally, we provide some guidelines for making efficient use of CXL memory in \S~\ref{sec:best-practice}.



