\section{Best Practices for CXL memory}
\label{sec:best-practice}
Given the unique hardware characteristic of CXL memory, we offer the following insights in making the best use of CXL memory. 

\niparagraph{Use non-temporal store or \texttt{movdir64B} when moving data from/to CXL memory.} 
As demonstrated in \secref{sec:character}, different x86 instructions exhibit significantly diverse performance in accessing CXL memory, which is due to both the CPU core's microarchitectural design and CXL memory's inherent behavior. Considering the use cases of CXL memory (\eg, memory tiering), where short-term data reuse is not highly likely, to achieve higher data movement throughput and avoid polluting the precious cache resources, we recommend that \texttt{nt-store} or \texttt{movdir64B} instructions should be prioritized in the corresponding software stacks. Note that, since both \texttt{nt-store} and \texttt{movdir64B} are weakly-ordered, a memory fence is necessary to make sure the data has been written.


\niparagraph{Limit the number of threads writing to CXL memory concurrently.} As analyzed before, CXL memory's performance depends on both CPU and the device controller. This is especially true for concurrent CXL memory accesses, as contention can happen in multiple places. Even though the current FPGA-based implementation of the CXL memory controller may limit the internal buffer size and thus the number of on-the-fly store instructions, we anticipate that the problem still exists on the pure ASIC-based CXL memory device. It is desirable to have a centralized communication stub on the CPU software side to conduct the data movement. We recommend that CXL memory should be managed by OS or a dedicated software daemon, instead of everything application. 

\niparagraph{Use Intel DSA for bulk memory movement from/to CXL memory.} The first two insights may still fall short when transferring a large amount of data across regular DRAM and CXL memory as they consume significant CPU cycles and still have limited instruction/memory-level parallelism. We found that Intel DSA --- with its high throughput, flexibility, and fewer restrictions compared to its predecessors --- can be a good candidate to further improve the performance and efficiency of such data movement. This is especially useful in a tiered memory system, where data movement often happens in page granularity (\ie, 4KB or 2MB).

\niparagraph{Interleave memory using NUMA polices and other tiering memory methods to evenly distribute the memory load across all DRAM and CXL channels.}
In addition to using CXL memory as a slower DRAM or faster SSD (\eg, memory tiering), CXL memory can also be interleaved with the regular memory channels to add to the total memory bandwidth, especially when the CXL memory device as more memory channels (and thus more comparable memory bandwidth). Carefully choosing the interleaving percentage/policies can largely mitigate the expected performance degradation.


\niparagraph{Avoid running application with $\mu$s-level latency entirely on the CXL memory.}
The relatively long access latency of CXL memory can become a major bottleneck for applications that require immediate data accesses in a fine time granularity ($\mu$s-level). Redis is such an example -- the delayed data accesses due to CXL memory will accumulate to a significant value for end-to-end query processing. This type of application should still consider pinning the data on a faster medium.


\niparagraph{Microservice can be a good candidate for CXL memory memory offloading.}
Microservice architecture has become a prevailing development methodology for today's internet and cloud services in datacenters because it is flexible, developer-friendly, scalable, and agile. However, its layered and modularized design does impose a higher runtime overhead compared to conventional monolithic applications. Such characterization makes it less sensitive to the underlying cache/memory configurations and parameters. Our study on DSB (see \secref{sec:dsb}) also proves this point. We envision that a significant portion of the microservice data can be offloaded to CXL memory without affecting its latency or throughput performance. 

\niparagraph{Explore the potential of inline acceleration with programmable CXL memory devices.} 
Given the insights above, those applications which are suitable for CXL memory offloading may not be highly sensitive to data access latency. This provides more design room for inline acceleration logic inside the CXL memory device -- even though such acceleration may add extra latency to data access, such overhead will not be visible from an end-to-end point of view of the target applications. Hence, we still advocate for FPGA-based CXL memory devices for their flexibility and programmability.  







\subsection{Application Categorization}
From the profiled applications, we identify two types of applications based on their performance when running on CXL memory: bandwidth-bounded and latency-bounded. 

\textbf{Memory-bandwidth-bounded} applications typically experience a sublinear increase in throughput beyond a certain thread count. Although running Redis and DLRM inference on CXL memory both yield lower saturation points, one should make a clear distinction between the two, that only DLRM inference is the bandwidth-bounded application. The single-threaded Redis is bounded by the higher latency of CXL memory, which reduces the processing speed of Redis.

\textbf{Memory-latency-bounded} applications will perceive throughput degrade even when a small amount of their working set is allocated to a higher-latency memory. In the case of databases, they are also likely to show a tail latency gap when running on CXL memory, even when the QPS is far from the point the saturation. Databases such as Redis and memcached that operate on $\mu$s-level latency have the highest penalty when they run purely on CXL memory. In contrast, microservices that operates on ms-level with layers of computation show a promising use case of offloading memory to CXL memory. 

In both cases, however, having interleaved memory between conventional CPU attached DRAM and CXL memory reduces the penalty of the slower CXL memory memory, in both throughput (\secref{sec:dlrm}), and tail latency (\secref{sec:ycsb}). Such a round-robin strategy should serve as a baseline for tier memory policies.   
