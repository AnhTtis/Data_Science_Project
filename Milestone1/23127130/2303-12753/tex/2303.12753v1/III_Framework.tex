
\section{SegHDC Framework}\label{sec:method}

\begin{figure}[t]
%\vskip 0.2in
\begin{center}
% \centerline{\includegraphics[width=\columnwidth]{figure/framework_overview.eps}}
\includegraphics[width=\columnwidth]{figures/framework_new.pdf}
\vskip -0.1in
% \vspace{-15pt}
\caption{Overview of the SegHDC framework.}
\label{fig:frame}
\end{center}
\vskip -0.1in
\end{figure}


% \subsection{Overviewof the framework}
In this section, we will formally introduce the SegHDC framework. As shown in Figure \ref{fig:frame}, SegHDC is mainly composed of 4 components: \textcircled{\raisebox{-1pt}{1}} 
position encoder module,
% to encode the position information to position HVs, 
\textcircled{\raisebox{-1pt}{2}} color encoder module,
% to encode the color information to color HVs, 
\textcircled{\raisebox{-1pt}{3}} pixel HV producer, 
% to get pixel HVs, 
and \textcircled{\raisebox{-1pt}{4}} clusterer. 
% to cluster the pixel HVs. 
In the rest of this section, each component will be introduced in detail. 
% including \textcircled{\raisebox{-1pt}{1}} Position encoding module, \textcircled{\raisebox{-1pt}{2}} Color encoding module, \textcircled{\raisebox{-1pt}{3}} way to get final HVs, and \textcircled{\raisebox{-1pt}{4}} clustering, as shown in Figure \todo{\ref{}}.


% \subsection{Position Encoder}
\noindent\textbf{\textcircled{\raisebox{-1pt}{1}} Position Encoder}

% \textbf{1. The limitation of classical HDC encoding method for image segmentation}

% Classical HDC uses all random HVs to encode each image. Given an $i$ rows and $j$ column images, classical HDC will randomly generate $i$ $\times$ $j$ HVs to represent the position HVs, in which the first HV to represent the position ($0$, $0$) and the last one to describe the position ($i$, $j$). Similarly, it will produce 256 HVs to represent the grayscale value from 0 to 255.
% It is useful just because there will be a lot of images in supervised learning and the position HVs and color HVs are consistent in each image. Thus, the features will be extracted when the HDC model is trained with plenty of training images. To improve performance, the second traditional way increases the relationship between the pixels. One HV is randomly generated, the second one  randomly flips several elements from the first HV, and the third one will repeat from the second HV, until the last HV.
% However, it will not work in unsupervised learning, since there will be no labeled data, specifically, annotated pixels for image segmentation. 
% % In unsupervised segmentation, each image 
% So, we need to design a new encoding method for unsupervised image segmentation.
% to increase the relationship between the pixels.

% \textbf{2. Encoding under the guidance of Manhattan distance}


Spatial information is one of the key pieces of information in one image, which is particularly important for segmentation. We need to clearly represent and measure both the \textit{relationship} and the \textit{difference} of pixels in different positions.
% Euclidean distance and 
Manhattan distance is a typical distance metric used to measure different positions in the 2-dimensional space (row and column), which will also be used in this paper to guide the position encoding.
% It is straightforward to choose the Euclidean distance or the Manhattan distance to describe the 
% In this paper, we select Manhattan distance because the datatype of the randomly generated HV we used is binary and we can represent the Manhattan distance in integer. 
% That's also one of the reasons why the classical HDC does not work in this case, the position HVs can not be measured with the Manhattan distance. 
The main concept of Manhattan distance is that the distance between two points is the sum of the absolute differences of their Cartesian coordinates \cite{ManhattanDist}, i.e., $L1$ norm. In a plane with point $p_1$ at $(x1, y1)$ and point $p_2$ at $(x2, y2)$, their Manhattan distance is calculated as $d_1(p_1, p_2) = \left | x_1 - x_2 \right | + \left | y_1 - y_2 \right |$.
% in Equ \ref{equ:Man1}. 
When we extend these two points $p$ and $q$ in a $n$ dimension space, the Manhattan distance can be described as Equ \ref{equ:Man2}, where, $p = (p_1, p_2, ..., p_n), q = (q_1, q_2, ..., q_n)$.
%\vskip -0.1in
% \begin{equation}
%     d_1(p_1, p_2) = \left | x_1 - x_2 \right | + \left | y_1 - y_2 \right |
%     \label{equ:Man1}
% \end{equation}
\vskip -0.1in
\begin{equation}
\small
\begin{aligned}
    d_1(p, q) = \sum_{i = 1}^{n}\left | p_i - q_i \right | 
    % \\
    % where, p = (p_1, p_2, ..., p_n), q = (q_1, q_2, ..., q_n)
    \label{equ:Man2}
\end{aligned}
\end{equation}
\vskip -0.1in
% The distance between two points measured along axes at right angles. In a plane with p1 at (x1, y1) and p2 at (x2, y2), it is |x1 - x2| + |y1 - y2|.
%\vskip -0.1in
%So, 
% given two pixels $p_1$ at $0-th$ row and  $0-th$ column and $p_2$ at  $i-th$ row and  $j-th$ column, their  Manhattan distance should meet Equ \ref{equ:Man3}. More generally, 
Given any three pixels in a single channel image, $p_1$ at $i-th$ row and $j-th$ column, $p_2$ at  $m_0-th$ row and  $n_0-th$ column and $p_3$ at  $m_1-th$ row and  $n_1-th$ column, we let the points meet Equ \ref{equ:Man4}.
% \begin{equation}
%     d_1(p_{(0,0)}, p_{(i,j)}) = d_1(p_{(0,0)}, p_{(0,j)}) + d_1(p_{(0,0)},p_{(i,0)})
%     \label{equ:Man3}
% \end{equation}
% \vskip -0.1in
\begin{equation}
\small
\begin{aligned}
d_1(p_1, p_2) =  d_1(p_1, p_3), \ iff \  m_0 + n_0 = m_1 + n_1\\
for \ \forall p_1 = (i,j), \forall p2 = (m_0, n_0), \forall p3 = (m_1, n_1)
    \label{equ:Man4}
\end{aligned}
\end{equation}
% \vskip -0.1in
Inspired by randomly flipped elements in HVs in the traditional encoding method, we are going to use the element flip to represent the distance. To this end, we need to control the row and column at the same time and treat the image matrix (single channel) as in two dimension space instead of one dimension in the traditional encoding method. 
Take the rows HVs as the example, we first randomly generate a binary HV (all elements are ``0'' and ``1'' ) with a high dimensionality $d$, e.g., 10,000. 
The second step is to flip the next $x_{row}$ (shown in Equ \ref{equ:uRow}, where $N_{row}$ and $N_{column}$ means the number of rows/columns) elements. The process will be continued until the last row.
A similar process will be conducted to columns, while the number of elements that need to be flipped is defined as $x_{col}$, shown in Equ \ref{equ:uRow}.
% \vskip -0.1in
\begin{equation}
\small
    x_{row} = \left \lfloor \frac{d}{N_{row}} \right \rfloor, \ x_{col} = \left \lfloor \frac{d}{N_{column}} \right \rfloor 
    \label{equ:uRow}
\end{equation}
% \vskip -0.1in
% \begin{equation}
% \small
%     x_{row} = \left \lfloor \frac{dimensionality}{number\ of\ rows} \right \rfloor, \ x_{col} = \left \lfloor \frac{dimensionality}{number\ of\ columns} \right \rfloor 
%     \label{equ:uRow}
% \end{equation}
% \begin{equation}
% x_{col} = \left \lfloor \frac{dimensionality}{number\ of\ columns} \right \rfloor 
% \label{equ:uCol}
% \end{equation}
% % unit\_row = \frac{dimensionality}{number\ of\ rows} 
The most important step is to use an HV to represent the pixel at $i-th$ row and $j-th$ column. Classical HDC uses element-wise XOR or 
% element-wise  
multiplication to associate two HVs. Element-wise multiplication can not keep the distance, since any 0 can diminish the distance, and we can not control where the 0 should be. Element-wise XOR is naturally suitable for this operation. Thus we get the position HV to represent the pixel at $i-th$ row and $j-th$ column as $y_{(i,j)} = r_i \oplus c_j$.
% as Equ \ref{equ:pHV} shows, 
% where $r_i$ is the HV of $i-th$ row and $c_j$ means the HV of $j-th$ column.
% \begin{equation}
%     y_{(i,j)} = r_i \oplus c_j
%     \label{equ:pHV}
% \end{equation}
Thus, we can use the position HVs to describe the Manhattan distance, and we can have Equ \ref{equ:Man5}.
% \vskip -0.1in
\begin{equation}
\small
\begin{aligned}
d_1(r_i \oplus c_j, r_{i+m_0} \oplus c_{j+n_0}) =  d_1(r_i \oplus c_j, r_{i+m_1} \oplus c_{j+n_1}),\\
for \ \ \forall i,j, \forall m_0, n_0, \forall m_1, n_1,\  iff \ \  m_0 + n_0 = m_1 + n_1
    \label{equ:Man5}
\end{aligned}
\end{equation}
% \vskip -0.1in

\begin{figure}[t]
%\vskip 0.2in
\begin{center}
% \centerline{\includegraphics[width=\columnwidth]{figure/framework_overview.eps}}
\includegraphics[width=0.8\columnwidth]{figures/distance4.pdf}
\vskip -0.1in
% \vspace{-15pt}
\caption{Distance between HV at 0-th row, 0-th column and others, when $\alpha$ and $\beta$ set as 0.5 and 2}
\label{fig:distance}
\end{center}
\vskip -0.1in
\end{figure}


% We use an image with 4 rows and 5 columns, and vectors with 20 dimensions to verify our encoding method.

The distance of the above encoding is shown in Figure \ref{fig:distance} (a). All the distances shown in the position are the distance between the position HV at $p_{(0,0)}$ ($0-th$ row and $0-th$ column) and the position HV at $p_{(i,i)}$ ($i-th$ row and $i-th$ column). 
For example, the distance between $p_{(0,0)}$ and $p_{(1,1)}$ is 0, shown in the 
position at the intersection of the second row and second column(in the red circle). The $x$ means the $x_{row}$ and $x_{column}$ defined in Euq \ref{equ:uRow}. 
% and \ref{equ:uCol}.
To simplify the question, we suppose $x_{row}$ = $x_{col}$ = $x$ here.
The zeroth row and zeroth column seem to meet the Manhattan distance, while other parts do not. For example, the distance between $p_{(0,0)}$ and $p_{(1,1)}$ (in the red circle) should be $2x$ instead of $0$.
% It can not meet the Manhattan distance, 
% Because,
This is because, for the pixel at $i-th$ row and $i-th$ column, the $r_i$ and $c_i$ flip the element on the same sites (shown in the top part of Figure \ref{fig:distance} (a)). Thus $r_1$ $\oplus$ $c_1$ = $r_i$ $\oplus$ $c_i$, and all the distances between $p_{(1,1)}$ and $p_{(i,i)}$ is 0. Similar distance diminishing occurs in other parts.
% Thus all the position HV at $i-th$ row and $i-th$ column would be the same, and the distance would be 0. Similar in 
So, we need to let the HVs of rows and columns flip different sites. Specifically, row HV can change the first half of row HV while column HV can change the second half of column HV. Thus the changes in row HV  will not affect the changes in column HV, and vice versa. The distance of this encoding is shown in Figure \ref{fig:distance}  (b), and it meets Equ \ref{equ:Man5} now. 
% Again take the $p_{(1,1)}$ as the example, the distance between the $p_{(2,2)}$
% and $p_{(1,1)}$ is shown in Equ \ref{equ:exampel1}. 
We call this encoding method Manhattan distance encoding. 
% \begin{equation}
%     d_1(p_{(2,2)}, p_{(1,1)}) = d_1(p_{(2,1)}, p_{(1,1)}) + d_1(p_{(1,2)}, p_{(1,1)})
%     \label{equ:exampel1}
% \end{equation}
%, equals to $p_{(2,2)}$
However, all the distances add $x$ from the closer one and it can not describe a smaller distance. Thus we bring a new hyperparameter $\alpha$ to 
describe the ratio of the half dimension which needs to be changed and thus control the flip unit. This encoding is called decay Manhattan distance encoding, and the distances when $\alpha$ set as 0.5 is shown in Figure \ref{fig:distance} (c). 
% When we set the $\alpha$ as $0.5$ in the example, we can have the distances shown in Figure \ref{fig:distance} (c). 
And thus Equ \ref{equ:uRow} 
% and Equ \ref{equ:uCol} 
changes to Equ \ref{equ:uRow2}.
% \vskip -0.1in
\begin{equation}
    x_{row} = \left \lfloor  \frac{\alpha \times d}{2 \times N_{rows}}\right \rfloor,\ x_{col} = \left \lfloor \frac{\alpha \times d}{2 \times N_{col}} \right \rfloor
    \label{equ:uRow2}
\end{equation}
% \vskip -0.05in
% and Equ \ref{equ:uCol2}. 
% \begin{equation}
%     x_{row} = \left \lfloor  \frac{\alpha \times dimensionality}{2 \times number\ of\ rows}\right \rfloor 
%     \label{equ:uRow2}
% \end{equation}
% \begin{equation}
% x_{col} = \left \lfloor \frac{\alpha \times dimensionality}{2 \times number\ of\ columns} \right \rfloor
% \label{equ:uCol2}
% \end{equation}
It can also meet Equ \ref{equ:Man5} and the smaller distance can be described. 
Another issue is that continuous near pixels are much more likely to be annotated as the same label. So it seems that the position HVs in a small area should not change.
% The distance between continuous pixels seems should not be as large as the decay Manhattan distance encoding shown in \ref{fig:distance} (c). 
Thus, we bring the second hyperparameter $\beta$ to let $\beta$ rows and $\beta$ columns as a block, and Manhattan distance is computed based on these blocks. 
% The block size is defined as $size_b = \beta ^{2}$.
% defined as Equ \ref{equ:sizeb}. 
We call this encoding method block decay Manhattan distance encoding.
% \begin{equation}
% \begin{aligned}
% size_b = \beta ^{2} 
%     \label{equ:sizeb}
% \end{aligned}
% \end{equation}
Figure \ref{fig:distance} (d) shows the distance when $\beta$ is set as 2. 
And all the distance between HVs of two positions should meet the new block Manhattan distance, as shown in Equ \ref{equ:Man6}. 
% Kindly note that $x$ can be smaller than the one shown in Figure \ref{fig:distance} (b) due to the variation of hyperparameter $\alpha$.
% different values 
% \vskip -0.2in
\begin{equation}
\small
\begin{aligned}
d_1(b_{r_i} \oplus b_{c_j}, b_{r_{i+m_0}} \oplus b_{c_{j+n_0}}) =  d_1(b_{r_i} \oplus b_{c_j}, b_{r_{i+m_1}} \oplus b_{c_{j+n_1}}),\\
iff \ \  m_0 + n_0 = m_1 + n_1, \ for \ \ \forall i,j, \forall m_0, n_0, \forall m_1, n_1
    \label{equ:Man6}
\end{aligned}
\end{equation}
% \vskip -0.1in
% \begin{equation}
% \begin{aligned}
% d_1(d_1(r_i \oplus c_j, r_{i+m_0} \oplus c_{j+n_0}), d_1(r_i \oplus c_j, r_{i+m_1} \oplus c_{j+n_1}))\\
% = d_1(d_1(r_i \oplus c_j, r_{i+m_0+m_1} \oplus c_{j+n_0+n1}),\\
% for \ \ \forall i,j, \forall m_0, n_0, \forall m_1, n_1 \\
%     \label{equ:Man5}
% \end{aligned}
% \end{equation}



% We now have the $i-th$ row HV and $j-th$ column, 



%Color and spatial information.


%1.传统的编码方式怎么做，为什么不行。
%2.因为我们要保证position之间有关系，但是不同，存在距离。自然想到用满是距离。传统的没办法keep距离。
%直观的做法是来管理变化的位置，那么我们想到用row和col的来变化。第二次的想法最直观，是不行的，因为同样不可以做到，距离很奇怪，画图（a）。那么我们控制不同位置来变化，前变后变。这样的话对于那个pixel来说距离太大。因为我们用

% \subsection{Color Encoder}
\noindent\textbf{\textcircled{\raisebox{-1pt}{2}} Color Encoder}
% \textbf{1. Single channel color encoding}

% Things may become easier in color encoding. Because the position is in a two-dimension space, row and column, while color is in a one-dimension space, the value varies from 0 to 255.
The color value is in a one-dimension space, and it varies from 0 to 255. However, we have three channels in most images. These three channels have the same position but possibly different color values. 
To simplify the question, we consider a single channel at first. 
Classical HDC randomly generates 256 HVs to represent the 256 values of color, or randomly flips several elements based on the previous HV.
While random means 
color values with a greater difference
% more different color values 
can be assigned to more similar HVs, which does not make sense. 
% Or it randomly flips several elements, which still can not be controlled by the consistent similarity of the HVs and the color values.
Similar to position encoding, we need to encode the color value according to the Manhattan  distance defined in Equ \ref{equ:Man2}. 
% Consider the 1-dimension value and the characteristics of the datatype of our generated HV, binary, Hamming distance is a desirable candidate.
% Hamming distance is used to describe the number of positions at which the corresponding symbols are different in two equal-length strings \cite{waggener1995pulse}. 
% We define the Hamming distance as Equ \ref{equ:hammingD}, where $v_i$ and $v_j$ represent the HV corresponding to the color value, and $d$ represents the dimension of HVs. 
% \begin{equation}
% \begin{aligned}
% d_h(v_1, v_2)= diff(v_1, v_2)
% & = \sum_{k=0}^{d-1} (v_1 \oplus v_2), \\
% i, j \in [0,255]
%     \label{equ:hammingD}
% \end{aligned}
% \end{equation}
% Hamming 
The Manhattan distance of two HVs is the number of different elements in the HVs, thus we can also use the summation of the element-wise XOR to obtain it. Similar to the position HVs, we can flip the number of continuous sites of HVs to add the Manhattan
% hamming 
distance to the HVs. Since the value variance is fixed from 0 to 255, we can define the unit length per flip as $u_c$ as $u_c = \left \lfloor \frac{d}{256} \right \rfloor $.
% in Equ \ref{equ:uColor}. 

% \begin{equation}
% u_c = \left \lfloor \frac{dimensionality}{256} \right \rfloor 
% \label{equ:uColor}
% \end{equation}
% \begin{figure}[t]
% %\vskip 0.2in
% \begin{center}
% % \centerline{\includegraphics[width=\columnwidth]{figure/framework_overview.eps}}
% \includegraphics[width=\columnwidth]{figures/colorGray.pdf}
% \vskip -0.1in
% % \vspace{-15pt}
% \caption{Hamming distance encoding for the color value of the single channel. The block with diagonal stripes means the elements in this block are flipped.}
% \label{fig:colorGray}
% \end{center}
% \vskip -0.1in
% \end{figure}
% Figure \ref{fig:colorGray} shows the design of variable color HVs. The Hamming distance is increased with the color value increased.
The Manhattan
% hamming 
distance of two color values $a$ and $b$ can be described as $ d_1(a,b) = \left \lfloor \frac{ \left | a - b \right | }{256} \right \rfloor $
% the Euq \ref{equ:hammingD2}, 
while the largest Manhattan distance, $\frac{255}{256} \times u_c $, is between the HVs corresponding to $0$ and $255$. Thus, we obtain the 256 HVs of 256 single-channel color values. 

% \begin{equation}
% d_h(a,b) = \frac{\left | a - b \right | }{256} 
% \times u_c 
% \label{equ:hammingD2}
% \end{equation}

% \textbf{2. 3-channels color encoding}

\begin{figure}[t]
%\vskip 0.2in
\begin{center}
% \centerline{\includegraphics[width=\columnwidth]{figure/framework_overview.eps}}
\includegraphics[width=\columnwidth]{figures/color_3channel.pdf}
\vskip -0.1in
% \vspace{-15pt}
\caption{3-channel color encoding under the guidance of Manhattan distance.
% distance encoding for the color value of the single channel.
The block with diagonal stripes means the elements in this block are flipped.}
\label{fig:3channel}
\end{center}
\vskip -0.1in
\end{figure}

To encode the 3-channels color value at one position actually encodes three different values into one HV with dimension $d$. From classical HDC, we may use element-wise $XOR$ or $multiplication$ to associate two HVs and then three. In fact, the difficulty is how to keep the 
% Hamming
Manhattan distance. However, neither element-wise $XOR$ nor element-wise $multiplication$ can keep the Hamming distance. Element-wise $multiplication$ can diminish the distance when there is a $0$ at the same site of any one of three HVs, while element-wise $XOR$ only counts number of 1s.
% Another solution is to do the element-wise summation. It seems good since the image actually adds the 3 channels. But the Hamming distance can not be kept, as well, it can affect the weight of position HVs when we obtain the pixel HVs. The threshold may help to solve the second issue. However, the accuracy may lose during the process.  
So we need a new way to encode the 3-channels color values.
To contain all the information of the three channels and keep the
% Hamming
Manhattan distance, we consider reducing the HV dimension of each channel from $d$ to $\frac{d}{3}$. Thus we can get a $3 \times 256 \times \frac{d}{3}$ HVs to represent the color values in 3 channels, and each channel obtain $256 \times \frac{d}{3}$ HVs. At last, when the three values in the three-channel are obtained, the three corresponding HVs will be concatenated together to produce a new HV to represent the color value of this pixel. As shown in Figure \ref{fig:3channel}, if the color values of a pixel are [255, i, 0], the HV to represent its color should consist of three parts:
(1) the first $\frac{d}{3}$ elements come from the $256th$ HV (value 255) of the first channel, (2) the second $\frac{d}{3}$ elements come from the $(i+1)-th$ HV (value $i$) of the second channel, and (3) the rest elements come from the $1st$ HV (value $0$) of the third channel.

%1.汉明距离定义，为什么适合HV。
%2.怎么去控制两个之间的汉明距离。
%3.怎么去keep 三通道之间的距离。用×或者XOR好像不能keep住距离，或者叠加。 叠加看起来make sense，因为图片就是叠加起来的， 但是仍然不可以，特别是在最后会被映射到高维。考虑三色各1/3，这样大家都有。


% We still need to use a 

% \subsection{Pixel HV Producer}
\noindent\textbf{\textcircled{\raisebox{-1pt}{3}} Pixel HV Producer}
\begin{figure}[t]
%\vskip 0.2in
\begin{center}
\includegraphics[width=\columnwidth]{figures/pixelHV_new2.pdf}
\vskip -0.1in
% \vspace{-15pt}
\caption{Different situations when producing pixel HV, and relative Manhattan distance changes when $\gamma = 2$ is applied to color HV. 
% Situations (a) (b) (c) keep the distance and are likely to happen, while (d) does not but (d) will occur much less than (a), (b), (c)
}
\label{fig:pixelHV}
\end{center}
\vskip -0.1in
\end{figure}

After the position and color values of the pixel are encoded to HVs, we will produce the pixel HVs based on these two types of HVs. The only issue that needs to be concerned about is how can we 
% try to
keep the Manhattan distance in position HVs and
% and Hamming distance in the 
color HVs. 
% We still use the
We use Hamming distance $d_h$ as guidance here. It describes the number of positions with the different symbols in two equal-length strings \cite{waggener1995pulse}.
% The element-wise $XOR$ works in most cases, except for the case of position HV and color HV flipping the same sites. 
To help adjust the composition ratio of position HV and color HV, we also bring a new hyperparameter $\gamma$. 
 % to address this problem. 
$\gamma$ makes the flipped element longer to the $\gamma$ bit filling in the flipped value, which can impact on either position HV or color HV. For example, when $\gamma = 2$, and $0$ can flip to $1$ and then change as long as to be $11$.
% Similarly, element-wise $multiplication$ will diminish the distance when there is a $0$ on the same sites in any HVs. 
% Thus, element-wise $XOR$ would be a better choice.
Figure \ref{fig:pixelHV} shows the different situations when producing pixel HV, and the relative Hamming distance changes when $\gamma = 2$ is applied to color HV $v_i$. 
In Figure \ref{fig:pixelHV} (b), color HV $v_2$ flips one element, thus the resulting pixel HV $y_2$ flips the same site. 
The Hamming distance is 1 between the new pixel HV $y_2$ and the original pixel HV $y_1$. 
% The same distance change will happen to only position HV flips. 
In Figure \ref{fig:pixelHV} (c), position HV $p_3$ and color HV $v_3$ flip at the same time but on different sites, thus the resulting pixel HV $y_3$ flips the two sites. 
Similarly, the Hamming distance is 2 between the new pixel HV $y_3$ and the original pixel HV $y_1$. For the case shown in Figure \ref{fig:pixelHV} (d), position HV $p_4$ and color HV $v_4$ flip at the same site, and the relative Hamming distance is 1.
% Case (d) can not keep the Hamming distance, but it will occur much less than cases(a), (b), and (c). Because the real situation is more complex, and the flipping unit $x$
% in position HVs and $u_c$ are not likely to be the same, so they are not likely to flip all the same sites. Flipping sites in intersections are possible, but other sites out of the intersections will affect.
So far, we have encoded an image with $(r, c)$ size to $r \times c$ HVs.
HDC requires the HVs are pseudo-orthogonal. 
We need to note that the pixel HVs come from position HVs and color HVs, which are pseudo-orthogonal to each other,
% encoded by are our method are also pseudo-orthogonal,
and the proof is shown in Lemma \ref{lemma1}.
\newtheorem{lemma}{Lemma}
\begin{lemma} 
\label{lemma1}
% Given any 1 pixel, the position HV $p_{(i,j)}$ and color HV $v_k$ of the pixel are pseudo-orthogonal. 
% Given
Any HVs which will do the element-wise operation in the encoding process, are pseudo-orthogonal.

Proof: 
The basic idea is that although we specify the flip segment, it seems they have a correlation, however, all the correlated HVs will never do an element-wise operation with each other.
From \cite{ge2020classification}, the 2 HVs are orthogonal when their normalized Hamming distance $N(d_h)$ = 0.5. 
% ($\frac{1}{d} \times$ Hamming distance, where $d$ is the dimension of HV) . 
Due to the high dimension and random generation, the first row HV $r_0$ and first column HV $c_0$ are pseudo-orthogonal \cite{ge2020classification}. 
This is because 50\% ``1'' and 50\% ``0'' are randomly assigned to the different sites of 2 HVs. 
% The 
% row HV
$r_i$ flips the $x_{row}$ elements based on $r_{i-1}$. 
% and thus $r_i$ and $r_{i-1}$ are not pseudo-orthogonal.
% This is the same as $c_0$ and $c_1$. 
The flipped $x_{row}$ elements have the same  
probability of distribution of ``1'' and ``0'' with the whole HV.
% having 50\% ``1'' and 50\% ``0'', 
% which is the same with the whole HV. 
Thus the number of ``1'' in $r_{i}$ will be very similar to $r_{i-1}$, and thus to $r_0$. This is the same for column HV $c_j$. So, $r_{i}$ and $c_j$ are also pseudo-orthogonal like the $r_{0}$ and $c_0$. Therefore, position HV $p_{(i,j)} = r_i \oplus c_j$ has nearly 50\% ``1'' and 50\% ``0''. 
Color HV $v_k$ is similar to row HV,
% any one of row HV or column HV,
with nearly 50\% ``1'' and 50\% ``0''.
Thus, the position HV $p_{(i,j)}$ is also pseudo-orthogonal to the color HV $v_k$, because $N(d_h(p_{(i,j)}, v_k) \approx 0.5)$. 
Note that, there is no
% we do not need 
any HV operations between any 2 position HVs, and any 2 color HVs. 
\end{lemma}








\noindent\textbf{\textcircled{\raisebox{-1pt}{4}} Clusterer}
% \subsection{Clusterer}

In this work, we use the K-Means algorithm to 
% help us to 
cluster the pixel HVs. 
In the K-means algorithm, the distance function 
% is crucial 
is a key part to measure the distances of points to centroids \cite{wu2012cluster}. 
For HDC, Hamming distance and cosine distance are widely used. 
In this work, we use cosine distance to be accommodated with HDC, and the reason will be mentioned later.
The cosine distance of two HVs is defined in Equ \ref{eq:sim}, where $y$ represents the pixel HV, $z$ represents the centroid HV and $d$ is the dimension of HVs.
In classical K-Means, the centroids are chosen randomly. To boost the performance, we choose the pixels with the largest color difference in this work. 
% This is because we can not know if pixels at different positions should be annotated as the same label, but pixels with different colors are likely to be classified into different labels. 
After the first  batch of 
centroids are chosen, the HV distances between all pixels and centroids will be calculated, and each pixel will be clustered to the class of its nearest centroid. After that, all HVs in the same class will be summed to produce the new centroid HV. 
This is the reason we choose the cosine distance. The length of a vector in space will not affect the angle, and the distance will only check the angle.
The process will be conducted iteratively until the preset iteration is achieved, and the pixels have been clustered into different classes.
% \vskip 0.1in
\begin{equation}
\small
     d_c({\overrightarrow{y}},{\overrightarrow{z}})= 1 - {{\overrightarrow{y}} {\overrightarrow{z}} \over \|{\overrightarrow{y}}\| \|{\overrightarrow{z}}\|} = 1 -\frac{ \sum_{i=1}^{d}{{\overrightarrow{y}}_i{\overrightarrow{z}}_i} }{ \sqrt{\sum_{i=1}^{d}{{\overrightarrow{y}}_i^2}} \sqrt{\sum_{i=1}^{d}{{\overrightarrow{z}}_i^2}} }
    \label{eq:sim}
\end{equation}
% \vskip 0.1in
% \clearpage


