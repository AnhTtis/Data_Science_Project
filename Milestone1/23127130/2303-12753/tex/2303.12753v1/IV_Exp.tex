
\section{Experiments}
\label{sec:exp}
This section reports the evaluation results of SegHDC on three nuclei segmentation datasets.
Results show our method outperforms the CNN-based unsupervised segmentation on both segmentation performance and latency.

\noindent\textbf{A. Experimental Setup}

\noindent\textbf{Dataset:}
We employ three segmentation datasets, including BBBC005\cite{ljosa2012annotated} (first 200 images are used), DSB2018\cite{DSB2018} (``stage1\_train'' set is used, cause DSB2018 does not provide the ground truth for the test set), and MoNuSeg\cite{kumar2019multi} (test set is used), to evaluate our method. 
% The detail of the datasets is shown below.

% \begin{itemize}
%     \item BBBC005\cite{ljosa2012annotated}: The dataset contains 19200 images. 
%     % These images were simulated for a given cell count with a clustering probability of 25\% and a CCD noise variance of 0.0001. 
%     % Focus blur was simulated by applying Gaussian filters to the images. 
%     Each image is 696 x 520 pixels in 8-bit TIF format, with the nuclei and cell areas matched to the average nuclei and cell areas. 
%     We use the first 200 images and
%     % in the dataset. 
%     the color values of the 3 channels are the same in all of the images.

%     \item DSB2018\cite{DSB2018}: The dataset is provided by the 2018 Data Science Bowl. 
%     % It contains a large number of segmented nuclei images. 
%     The images were acquired under a variety of conditions and varied in cell type.
%     % magnification, and imaging modality (brightfield vs. fluorescence). 
%     It is designed to challenge an algorithm's ability to generalize across these variations. The ``stage1\_train'' set is used, cause DSB2018 does not provide the ground truth for the test set.
%     % and thus we can not evaluate our method.

%     \item MoNuSeg\cite{kumar2019multi,kumar2017dataset}: The dataset 
%     % for this challenge 
%     is obtained by carefully annotating tissue images of several patients with tumors of different organs and who were diagnosed at multiple hospitals. 
%     % This dataset was created by downloading H\&E stained tissue images captured at 40x magnification from the TCGA archive. 
%     We use the test set, which contains 14 images with 7000 nuclear boundary annotations.

% \end{itemize}

\noindent\textbf{Training setting and baseline:}
The performance of SegHDC is evaluated by comparing it with the 
% state-of-the-art 
unsupervised image segmentation method \cite{kim2020unsupervised}. 
The default setting of clustering iteration is 10. The hyperparameters $\alpha$, and $\gamma$ are 
% set as
0.2 and 1, while $\beta$ is 
% set as
21 on the BBBC005 dataset and 26 on the DSB2018 and MoNuSeg datasets.
% $\beta$,  (described in section \ref{sec:method}) are set as 0.2, 21, and 1 on the BBBC005 dataset, 0.2, 26 and 1 on the DSB2018 dataset, and the MoNuSeg dataset. 
The number of clusters 
% (``K'' in K-Means) 
is set as 2 for BBBC005 and DSB2018 datasets, while 3 is set for the MoNuSeg dataset.
The baseline method runs on the default setting provided by \cite{kim2020unsupervised}.
The metrics of Intersection over Union (IoU) is applied, which is defined as the area of intersection between the predicted segmentation map and the ground truth, divided by the area of union between those two \cite{minaeeimage}. 

\noindent\textbf{Edge device:} To compare the latency of SegHDC and baseline, we employ an edge device: Raspberry PI 4 Model B \cite{RaspberryPI} with 
% roadcom BCM2711 equipping a 1.5 GHz quad-core ARM Cortex-A72 processor and 
4 GB memory. The latency is obtained by deploying SegHDC and baseline on the device and testing the processing time for one image.


\noindent\textbf{B . Experimental Results}
% Table generated by Excel2LaTeX from sheet 'TableInPaper'
\begin{table}[t]
% \small
\tabcolsep 4pt
\renewcommand\arraystretch{1.5}
\vspace{-6pt}
  \centering
  \caption{IoU score on 3 datasets}
    \begin{tabular}{|c|c|c|c|c|c|}
    \hline
    \textbf{Dataset} & BL \cite{kim2020unsupervised} & RPos \cite{ge2020classification} & RColor \cite{ge2020classification} & SegHDC & Improvement \\
    \hline
    \textbf{BBBC005} & 0.7490 & 0.0361 & 0.1016 & \textbf{0.9414} & 25.7\%$\uparrow $ \\
    \hline
    \textbf{DSB2018} & 0.6281 & 0.1172 & 0.2352 & \textbf{0.8038} & 28.0\%$\uparrow $ \\
    \hline
    \textbf{MoNuSeg} & 0.5088 & 0.1959 & 0.3832 & \textbf{0.5509} & 8.27\%$\uparrow $ \\
    \hline
    \end{tabular}%
  \label{tab:result}%
\end{table}%


% Table generated by Excel2LaTeX from sheet 'TableInPaper'
\begin{table}[t]
\vspace{-6pt}
  \centering
  \tabcolsep 4.5pt
\renewcommand\arraystretch{1.5}
  \caption{Result of latency on Raspberry Pi for processing an image in DSB2018 dataset and BBBC dataset}
  \begin{threeparttable}
    \begin{tabular}{|c|c|c|c|c|}
    \hline
          & \textbf{Image Size} & \textbf{IoU Score} & \textbf{Latency on PI} & \textbf{SpeedUp} \\
    \hline
    \textbf{Baseline} & 
    % \multirow{2}[1]{*}
    {$256 \times 320 \times 3$ } & 0.7612 & 11453.0s & baseline \\
    \cline{1-1}\cline{3-5}    
    \textbf{SegHDC} &  (DSB2018)     & 0.8275 & 35.8s  & 319.9$\times$ \\
    \hline
    \textbf{Baseline} & 
    % \multirow{2}[1]{*}
    {$520 \times 696 \times 1$} & $\times$ \tnote{*} & $\times$ \tnote{*} & baseline  \\
    \cline{1-1}\cline{3-5}    
    \textbf{SegHDC} &  (BBBC005)     & 0.9587 & 178.31s & - \\
    \hline
    \end{tabular}%
    \begin{tablenotes}
        \footnotesize
        \item[$\times ^*$] Out of memory.
      \end{tablenotes}
    \end{threeparttable}
    
  \label{tab:latency}%
\end{table}%



\begin{figure}[t]
%\vskip 0.2in
\begin{center}
% \centerline{\includegraphics[width=\columnwidth]{figure/framework_overview.eps}}
\includegraphics[width=\columnwidth]{figures/visualization2dataset.pdf}
\vskip -0.1in
% \vspace{-15pt}
\caption{Visualization of prediction masks of an image in BBBC005 dataset, DSB2018 dataset, and MoNuSeg dataset.}
\label{fig:visial2data}
\end{center}
\vskip -0.1in
\end{figure}



\textbf{(1) Our method beats CNN-based baseline}

Table \ref{tab:result} reports the average IoU score on three datasets. Besides the baseline, we also evaluate the encoding method using randomly generated HVs for the position part (denoted as RPos) and for the color part (denoted as RColor) to verify the effectiveness of our encoding method. 

We can have some observations from the results in Table \ref{tab:result}. Overall, SegHDC achieves better results over the baseline on all three datasets. 
Specifically, on the BBBC005 dataset, SegHDC achieves a 0.9414 average IoU score, which is 25.7\% higher than that obtained by the baseline (where the score is 0.749). When it comes to the DSB2018 dataset, our method gets a 0.8038 average IoU score and gains an improvement of 28.0\% compared with 
% which is 28.0\% higher than 
the score of 0.6281 obtained by the baseline method. On the MoNuSeg dataset, the score of SegHDC surpasses 0.042 over that of the baseline. 
Besides, there is another observation that the results of RPos and RColor are comparable to worse with extremely low IoU scores on these three datasets.
% And the results of RandomPos and RandomColor show the effectiveness of our encoding method. 
% No matter whether randomly generating HV is applied for position encoding or color encoding does not work. 
These results have demonstrated the effectiveness of our encoding method.
% for position encoding and color encoding.

Table \ref{tab:latency} reported the results of the latency obtained by deploying on the Raspberry Pi for processing an image. An image with size $256 \times 320 \times 3$ from the DSB2018 dataset and an image with size ($520 \times 696 \times 1$) from the BBBC005 dataset are used to test the latency on Raspberry Pi. Specifically,  we use the HV with 800 dimensions, three iterations, and 
$\alpha$ is set as 1 for the image from DSB2018. The baseline method obtains a 0.7612 score, but the latency is over 3 hours as shown in the table. In contrast, SegHDC can obtain a higher IoU score (0.8275) score, meanwhile with a processing time of only 35.8s on the Raspberry Pi, which has achieved $319.9 \times$ speedups than that of the baseline. For the image from BBBC005, the HV is with 2000 dimensions, three iterations are applied, and
% used, and the  iteration is set as 3,
$\alpha$ is set as 0.8.
Results show that SegHDC can obtain a 0.9587 IoU score with a latency of 178.31s. However, the baseline approach can not run on the Raspberry Pi (as $\times ^*$ indicated) due to memory limitation.
This set of experiments proves that unsupervised image segmentation is hard to perform on an edge device, where the resources are limited.


Figure \ref{fig:visial2data} demonstrates the sample image, ground truth, prediction mask of baseline and SegHDC, and the IoU score obtained by both approaches. For example, for the sample of BBBC005 in the first line, 
% both baseline and SegHDC can predict a good mask for the 5 nuclei. Specifically, 
the baseline method can only obtain a 0.6995 IoU score while SegHDC gets a 0.9559 IoU score, where it is obvious to see the difference between the two images in regard to the size and outline of the nuclei. 
When it comes to another sample image DSB2018, the baseline and SegHDC can predict two similar masks (IoU score 0.7612 v.s. 0.8259). However, there is more noise in nuclei obtained by the baseline compared with that obtained by our SegHDC (e.g., some inner noise in nuclei and a small part at the right-up corner in these two images). 
In order to further verify the performance of SegHDC, we have conducted an additional set of experiments on the image in MoNuSeg, where there are much more complex details in the image.
Similar results can be achieved and our proposed SegHDC can predict the mask of most separate nuclei and obtains a higher score of 0.5299. compared with that of the baseline method, where only a rough outline can be predicted with the IoU score of 0.3496. 




\textbf{(2) Exploration of SegHDC}

\begin{figure}[t]
%\vskip 0.2in
\begin{center}
% \centerline{\includegraphics[width=\columnwidth]{figure/framework_overview.eps}}
\includegraphics[width=\columnwidth]{figures/DimAndIter.pdf}
\vskip -0.1in
% \vspace{-15pt}
\caption{IoU score and latency on Raspberry Pi with different iterations.}
\label{fig:dimAndIter}
\end{center}
\vskip -0.1in
\end{figure}


\begin{figure}[t]
%\vskip 0.2in
\begin{center}
% \centerline{\includegraphics[width=\columnwidth]{figure/framework_overview.eps}}
\includegraphics[width=\columnwidth]{figures/visualization1-4.pdf}
\vskip -0.1in
% \vspace{-15pt}
\caption{Visualization of prediction masks of an image in DSB2018 dataset during different iterations.}
\label{fig:visial}
\end{center}
\vskip -0.1in
\end{figure}



Except for comparison with the baseline, we also explore how the SegHDC performs with different clustering iterations and dimensions.

By using the sample image in the DSB2018 dataset, 
% We still use the image in the DSB2018 dataset which is also reported in Table \ref{tab:latency}.
Figure \ref{fig:dimAndIter} (a) shows the result of the IoU score and the latency on Raspberry Pi with the different number of iterations, specifically when 1 iteration to 10 iterations clustering is applied. Figure \ref{fig:dimAndIter} (b) demonstrates the effect due to the different dimensions applied to this image, where the number of dimensions is set from 200 to 1000. To unify the variables for a fair comparison, the HV dimension is set as 10000 in the experiment shown in Figure \ref{fig:dimAndIter} (a), while the results shown in Figure \ref{fig:dimAndIter} (b) are obtained by processing in 10 iterations.
% test set we use in Figure \ref{fig:dimAndIter} (a) is 10000 dimension, 
In both sub-figures, %the horizontal axis is the number of iterations, and 
the left vertical axis is the IoU score, and the right vertical axis means the latency (s). 


There are some observations in this set of experimental results.
As shown in Figure \ref{fig:dimAndIter} (a), the latency time increases from around 20 seconds for 1 iteration to over 300 seconds for 10 iterations. Notated that, after the $4-th$ iteration, SegHDC can obtain a good prediction mask for this image. 
% Thus it is a good trade-off for IoU score and latency when $4-th$ iteration is applied.
As Figure \ref{fig:dimAndIter} (b) reports, the latency time raises from around 90 seconds for 200-dimension HV to about 110 seconds for 1000-dimension HV. It seems 800 dimensions is a good choice for dealing with this image.
Figure \ref{fig:visial} shows the 
% visualization results of
prediction masks of the experiment shown in Figure \ref{fig:dimAndIter} (a). We show the 3-channel test image, ground truth as well as the prediction masks in the first 4 iterations because the later iterations give similar results. 
As shown in the figure, only 1 iteration can not work well,
% the prediction mask of 1st iteration
% seems to be wrongly labeled, 
with more than 2 iterations, it gives a much better result which is close to ground truth.
% where we can see the shape of the nuclei. 
% SegHDC better clusters the pixels around the outline of the cell nuclei in the rest 2 iterations, because these pixels have similar position HVs. 
% And this also proves the importance and effectiveness of our proposed encoding method.




