\pdfoutput = 1
% Credit: https://arxiv.org/abs/1708.07459
% https://arxiv.org/abs/1611.01838
% -----------------------------------------------------------------------
% --------------------------------- PREAMBLE ----------------------------
% -----------------------------------------------------------------------
% scrartcl
\documentclass[twocolumn]{article}
\input{macros}
\usepackage{style}

\newcommand\cl{\texttt{CL}\xspace}
\newcommand\corloc{\texttt{CorLoc}\xspace}
\newcommand\resnetfifty{\texttt{ResNet50}\xspace}
\newcommand\youtubeobjects{\texttt{YouTube-Objects}\xspace}
\newcommand\ytovone{\texttt{YTOv1}\xspace}
\newcommand\ytovtwodtwo{\texttt{YTOv2.2}\xspace}
\newcommand\davis{\texttt{DAVIS}\xspace}


\newcommand\labelgt{\texttt{GT}\xspace}  % ground truth label
\newcommand\labelpr{\texttt{PR}\xspace}  % predicted label
\newcommand\labelavg{\texttt{AVG}\xspace}  % average performance over all labels

\definecolor{Gray}{gray}{0.85}
\definecolor{darkergreen}{RGB}{21, 152, 56}

\newcolumntype{g}{>{\columncolor{Gray}}c}

% \newcommand{\cmark}{\ding{51}}%
% \newcommand{\xmark}{\ding{55}}%



% \pagestyle{empty}


% save space
% \setlength{\floatsep}{0.05in}
% \setlength{\textfloatsep}{0.05in}
% \setlength{\intextsep}{0.05in}
% \setlength{\belowcaptionskip}{0.05in}
% \setlength{\abovecaptionskip}{0.05in}
% \setlength{\abovedisplayskip}{0.05in}
% \setlength{\belowdisplayskip}{0.05in}


\makeatletter
\@namedef{ver@everyshi.sty}{}
\newcommand{\removelatexerror}{\let\@latex@error\@gobble}

% notation goes here
\newcommand\red[1]{{\color{red}#1}}
\newcommand{\entropysgd}{\mathrm{Entropy}\textrm{-}\mathrm{SGD}}
\newcommand{\entropyadam}{\mathrm{Entropy}\textrm{-}\mathrm{Adam}}
\newcommand{\minibatch}[1]{\Xi^{#1}}
\newcommand{\mnistfc}{\textrm{mnistfc}}
\newcommand{\smallmnistfc}{\textrm{small-mnistfc}}
\newcommand{\charlstm}{\textrm{char-LSTM}}
\newcommand{\ptblstm}{\textrm{PTB-LSTM}}
\newcommand{\lenet}{\textrm{LeNet}}
\newcommand{\allcnn}{\textrm{All-CNN-BN}}




\title{CoLo-CAM: Class Activation Mapping for Object Co-Localization in \\ Weakly-Labeled Unconstrained Videos}



\renewcommand\footnotemark{}
% \renewcommand\footnotemark{}

\author{Soufiane~Belharbi$^{1}$,
  ~Shakeeb Murtaza$^{1}$,
  ~Marco Pedersoli$^{1}$,
  ~Ismail~Ben~Ayed$^{1}$,
  ~Luke~McCaffrey$^{2}$, and
  ~Eric~Granger$^{1}$\\
 %	$^1$\'ETS Montreal\\
 	$^1$ LIVIA, Dept. of Systems Engineering, Ã‰TS, Montreal, Canada \\
	$^2$ Goodman Cancer Research Centre, Dept. of Oncology, McGill University, Montreal, Canada\\
{\tt\footnotesize \textcolor{black}{soufiane.belharbi@etsmtl.ca} }
}

% \setlength{\marginparwidth}{1in}



\newcommand{\nt}[2]{
{\color{ForestGreen}#1}\marginpar{\tiny\noindent{\raggedright{\color{Sienna}[NOTE]} \color{Sienna}{#2} \par}}
}
\renewcommand{\ss}[2]{{\color{cyan}#1}\marginpar{\tiny\noindent{\raggedright{\color{BurntOrange}[SS]}\color{BurntOrange}{#2} \par}}}
\newcommand{\ac}[2]{{\color{magenta}#1}\marginpar{\tiny\noindent{\raggedright{\color{magenta}[AC]}\color{magenta}{#2} \par}}}

\newcommand{\status}[2]{{\color{red}#1}\marginpar{\tiny\noindent{\raggedright{\color{red}#2}}}}

\newcommand{\todoit}[1]{{\color{gray}#1}\marginpar{\tiny\noindent{\raggedright{\color{blue}[TODO]}}}}
\newcommand{\fix}[2]{{\color{blue}#1}\marginpar{\tiny\noindent{\raggedright{\color{blue}[FIX]}\color{blue}{ #2} \par}}}
\newcommand{\ignore}[1]{}
\newcommand{\mref}{{\color{BurntOrange}\&ref\& \xspace}}
\newcommand{\sota}{state-of-the-art\xspace}


% uncomment these for submission!
% \renewcommand{\pc}[2]{#1}
% \renewcommand{\ss}[2]{#1}
% \renewcommand{\ac}[2]{#1}
% \renewcommand{\todoit}[1]{#1}
% \renewcommand{\fix}[1]{#1}


% -----------------------------------------------------------------------
% --------------------------------- BODY --------------------------------
% -----------------------------------------------------------------------


\begin{document}
\maketitle\thispagestyle{fancy}


\maketitle
% \lhead{\color{gray} \small \today}
\rhead{\color{gray} \small Belharbi et al. \;  [Under review 2023]}


\begin{abstract}
Weakly supervised video object localization (WSVOL) methods often rely on visual and motion cues only, making them susceptible to inaccurate localization. Recently, discriminative models have been explored using a temporal class activation mapping (CAM) method.  Although their results are promising, objects are assumed to have limited movement from frame to frame, leading to degradation in performance for relatively long-term dependencies. 
%
In this paper, a novel CoLo-CAM method for WSVOL is proposed that leverages spatiotemporal information in activation maps during training without making assumptions about object position. Given a sequence of frames, explicit joint learning of localization is produced based on color cues across these maps, by assuming that an object has similar color across adjacent frames. CAM activations are constrained to respond similarly over pixels with similar colors, achieving co-localization. This joint learning creates direct communication among pixels across all image locations and over all frames, allowing for transfer, aggregation, and correction of learned localization, leading to better localization performance. This is achieved by minimizing the color term of a conditional random field (CRF) loss over a sequence of frames/CAMs. 
%
Empirical experiments\footnote{Code: \href{https://github.com/sbelharbi/colo-cam}{https://github.com/sbelharbi/colo-cam}.} on two challenging datasets with unconstrained videos, YouTube-Objects, show the merits of our method, and its robustness to long-term dependencies, leading to new state-of-the-art performance for WSVOL.
\end{abstract}

\textbf{Keywords:} Convolutional Neural Networks, Weakly-Supervised Video Object Localization, Unconstrained Videos, Class Activation Maps (CAMs).
% ===================================================================================================
%
%                                      INTRODUCTION
%
% ===================================================================================================


\begin{figure}[ht!]
\centering
  \centering
  \includegraphics[width=.8\linewidth]{proposal}
  \caption[Caption]{Illustration of the multi-frame training using our CoLo-CAM method with $n=3$ frames and F-CAM architecture~\citep{belharbi2022fcam}. Each pixel (dot), is connected (orange line) to every other pixel across the 3 frames to measure color similarity. The connection thickness indicates similarity strength. CAM locations at pixels with similar colors are constrained to have similar activations (green lines are for frame/CAM alignment). For simplicity, per-frame terms are not visualized, and only a few pixel interconnections are shown. Notation and per-frame and multi-frame loss terms are described in Section \ref{sec:method}.}
  \label{fig:proposal}
\end{figure}


\section{Introduction} 
\label{sec:intro}

Recent progress of online multimedia platforms such as YouTube has provided easy access to a large number of videos~\citep{ShaoCSJXYZX22,tang2013discriminative}. Consequently, designing techniques for automated video analysis becomes important. In this work, we focus on the task of localizing objects in videos, which plays a critical role in understanding video content and improving downstream tasks such as video-based summarization~\citep{ZhangHJYC17}, event detection~\citep{ChangYLZH16}, object detection~\citep{Chen0HW20,HanWCQ20,ShaoCSJXYZX22}, facial emotion recognition~\citep{xue22,Zhang22}, and visual object tracking~\citep{BergmannML19,LuoXMZLK21}. Unfortunately, the annotations required for fully supervised localization, \ie, bounding boxes for each frame, are costly for videos due to the large number of frames. As an alternative, weak supervision using a video tag or class label associated with the entire video is currently the most common annotation for weakly supervised video object localization (WSVOL)~\citep{JerripothulaCY16,tsai2016semantic}. %, similar to still images~\citep{choe2020evaluating,rony2023deep}. 
A video class label describes the main object appearing in the video without spatio-temporal information, \ie, spatial location, and start/end time of the object's appearance, which adds more uncertainty to the label.  %This makes the weak label uncertain.  
Additionally, unconstrained videos are often captured in the wild, with varying quality, moving objects and cameras, changing viewpoints and illumination, and decoding artifacts. Given these factors, localization from weakly-labeled unconstrained videos is a challenging task. 


% Challenges of WSVOL
State-of-the-art WSVOL methods often follow a similar strategy. Initial segments/proposals are generated based on visual or motion cues, which are then used to identify and refine object locations through post-processing~\citep{HartmannGHTKMVERS12,Kwak2015,prest2012learning,tang2013discriminative,Tokmakov2016,XuXC12,YanXCC17,zhang2020spftn}. This process is optimized under visual appearance and motion constraints to ensure consistency. Other methods rely on co-localization/co-segmentation across videos/images via graphs that allow for a larger view and interactions between segments~\citep{ChenCC12,FuXZL14,JerripothulaCY16,joulin2014,tsai2016semantic,ZhangJS14}. Despite their success, these methods have several limitations. For example, initial proposals are not necessarily discriminative \ie, aligned with the video class, due to unsupervised extraction using visual and motion cues (e.g., optical flow)~\citep{LeeKG11,SundaramBK10}. Moreover, video class labels are typically used to cluster videos only, which further reduces the discriminative information. Additionally, a per-video model is often optimized leading to several issues, including scalability, inference time, and deployment. Moreover, multi-stage training combines the local best solutions, which does not necessarily lead to the best global solution.


% cam-based and drawbacks.
Recently, the temporal class activation mapping (TCAM) method has been proposed to train discriminative multi-class deep learning (DL) model for WSVOL~\citep{tcamsbelharbi2023}. The authors consider CAM methods, a successful method for weakly supervised object localization approach (WSOL) on still images~\citep{choe2020evaluating,rony2023deep}. Using only global image class labels, CAM-based methods allow training a discriminative multi-class DL model (e.g., a CNN or vision transformer) to classify an image and localize the corresponding object of interest. Strong CAM activations indicate the potential presence of an object, making them suitable for object localization with low-cost weak annotations~\citep{oquab2015object}. The TCAM method~\citep{tcamsbelharbi2023} relies on pixel-wise pseudo-labels collected from a pre-trained classifier. It leverages temporal information to improve these pseudo-labels via a new spatio-temporal max-pooling over successive frames/CAMs. Despite its success and fast inference, TCAM assumes that objects have limited displacement over frames~\citep{tcamsbelharbi2023}, and has been shown to be constrained to very short time-dependencies.

 
% our method.
To alleviate this issue, while still benefiting from CAM methods, we introduce the CoLo-CAM method for WSVOL (shown in Fig.\ref{fig:proposal}), a new multi-class approach that leverages spatio-temporal information without an assumption about object movement. In particular, this method is considered for explicit joint learning of CAMs across multiple frames. Using a \emph{color} cue, CoLo-CAM constrains the set of CAMs extracted from a sequence of frames to be consistent by pushing them to activate similarly over pixels with a similar color. We rely on the assumption that objects in nearby frames have similar appearances, which is a fair assumption in videos. Unlike TCAM~\citep{tcamsbelharbi2023}, CoLo-CAM does not assume a limited displacement of objects. Rather, objects are allowed to appear anywhere in the image, making for a more flexible method. Our multi-frame optimization method is achieved via the \emph{color} term of the CRF loss~\citep{tang2018regularized} applied simultaneously over all images/CAMs, thereby allowing for interconnection among all their pixels, and performing explicit co-localization. This opens an explicit communication channel between CAMs, allowing for the transfer, aggregation, and correction of learned knowledge. CoLo-CAM relies on the F-CAM~\citep{belharbi2022fcam} architecture which was adapted from the U-Net~\citep{Ronneberger-unet-2015} model. It simultaneously classifies an image and locates the corresponding object using a full-resolution CAM. In addition to our new multi-frame constraint, we use per-frame local constraints to stimulate localization, which includes pseudo-labels and CRF loss~\citep{tcamsbelharbi2023,tang2018regularized}, in combination with a global size constraint. Our total training loss is composed of per-frame and multi-frame terms that are optimized simultaneously via standard Stochastic Gradient Descent (SGD). 


% contributions.
\noindent \textbf{Our main contributions are summarized as follows:}

\noindent \textbf{(1)} A novel CoLo-CAM method is proposed for WSVOL, requiring only video class labels for supervision. It builds a single deep WSVOL model that can simultaneously handle multiple classes. Using a color cue, CoLo-CAM explicitly constrains CAMs produced for a sequence of video frames to be consistent by activating similarly over pixels with similar colors, achieving co-localization. This joint multi-frame learning creates a direct communication between pixels across all image locations over all frames, allowing for the transfer, aggregation, and correction of learned localizations. This is achieved by minimizing the color term of a CRF loss~\citep{tang2018regularized} over a sequence of frames / CAMs. Unlike the recent CAM-based method~\citep{tcamsbelharbi2023}, our temporal term does not assume the same object location in different frames but instead allows the object to be located anywhere, making it more flexible and robust to movements. Our multi-frame loss term is optimized jointly with other per-frame loss terms to improve localization performance.

\noindent \textbf{(2)}  Our extensive experiments and ablation studies on two challenging public datasets of unconstrained videos, YouTube-Objects~\citep{prest2012learning,KalogeitonFS16}, show the merits of our proposed method and its robustness to long dependency. CoLo-CAM achieves a new state-of-the-art localization performance. Our results also confirm the potential benefit of discriminative learning in WSVOL tasks. Demonstrative videos and code are provided in the supplementary material.



 



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Related Work}
\label{sec:related-w}
 
This section reviews weakly-supervised methods for localization/segmentation and co-localization/co-segmentation in videos trained using only video class labels. Additionally, CAM-based methods for WSOL on still images are discussed, as they relate to WSVOL. 

\noindent \textbf{(a) Localization.}
%
Different methods exploit potential proposals as pseudo-supervision for localization~\citep{prest2012learning,zhang2020spftn}. In~\citep{prest2012learning}, class-specific detectors are proposed. First, segments of coherent motion~\citep{BroxM10} are extracted, and then a spatiotemporal bounding box is fitted to each segment, forming tubes. A single tube is jointly selected by minimizing energy based on the similarity between tubes, visual homogeneity over time, and the likelihood of containing an object. The selected tubes are used as box supervision to train a per-class detector~\citep{FelzenszwalbGMR10}. 
Other methods seek to discover a prominent object (foreground) in (a) video(s)~\citep{jun2016pod,RochanRBW16,Kwak2015}. Then, similar regions are localized and refined using visual appearance and motion consistency. 


\noindent \textbf{(b) Segmentation.} 
%
Independent spatiotemporal segments are first extracted~\citep{HartmannGHTKMVERS12,tang2013discriminative,XuXC12,YanXCC17} via unsupervised methods~\citep{BanicaAIS13,XuXC12} or with a proposals generator~\citep{zhang2015semantic} using pretrained detectors~\citep{zhang2015semantic}. Then, using multiple properties such as visual appearance and motion cues, these segments are labeled, while preserving temporal consistency. Graphs, such as the conditional random field (CRF) and GrabCut approaches~\citep{RotherKB04}, are often used to ensure consistency. 
%
In M-CNN~\citep{Tokmakov2016}, a fully convolutional network (FCN) is combined with motion cues to estimate pseudo-labels. Using motion segmentation, a Gaussian mixture model is exploited to estimate foreground appearance potentials~\citep{papazoglou2013fast}. Combined with the prediction of the FCN, these potentials are used to produce better segmentation pseudo-labels via a GrabCut-like method~\citep{RotherKB04}, and these are then used to fit the FCN. 
The previously mentioned segmentation methods use video class labels to cluster videos with the same class. While other methods do not rely on such supervision, the process is generally similar. Foreground regions are first estimated~\citep{Croitoru2019,Halle2017,papazoglou2013fast,umer2021efficient} using motion cues~\citep{SundaramBK10}, or PCA, or VideoPCA~\citep{StretcuL15}. This initial segmentation is later refined via graph methods~\citep{RotherKB04}.



\noindent \textbf{(c) Co-segmentation/Co-localization.}
%
The co-segmentation approach has been leveraged to segment similar objects across a set of videos. Typical methods rely on discerning common objects via the similarity of different features including visual appearance, motion, and shape. Using initial guessing of objects/segments, graphs, such as CRF and graph cuts, are considered to model relationships between them~\citep{ChenCC12,FuXZL14,tsai2016semantic,ZhangJS14}.  In~\citep{ChenCC12}, the authors use relative intra-video motion extracted from dense optical flow and inter-video co-features based on a GMM. The common object, \ie, the foreground, in two videos is isolated from the background using a Markov random field (MRF). It is solved iteratively via graph cuts while accounting for a unary term based on the distance to GMMs, and a pairwise energy based on feature distance weighted by the relative motion distance between super-voxels. 

Other methods aim to co-localize objects in images or videos using bounding boxes instead of segments~\citep{JerripothulaCY16,joulin2014}. In~\citep{joulin2014}, objectness~\citep{AlexeDF12} is used to generate proposals to search for the best candidate. Box similarity and discriminability~\citep{TangJLF14} in addition to multiple object properties including visual appearance, position, and size as used as constraints to ensure consistency. \citep{JerripothulaCY16} leverage co-saliency as a prior to filtering out noisy bounding box proposals.


%%%%%%%%%%%
\noindent \textbf{(d) WSOL in still images.}
CAM-based methods have emerged as a dominant approach for the WSOL~\citep{choe2020evaluating,rony2023deep} on still images. Using only global image-class labels, a DL model can be trained to classify an image and yield a CAM, that localizes objects. 
%
Early work focused on designing variant spatial pooling layers~\citep{durand2017wildcat,durand2016weldon, oquab2015object,zhou2016learning}. 
However, CAMs tend to under-activate by highlighting only small and the most discriminative parts of an object~\citep{choe2020evaluating,rony2023deep}, diminishing its localization performance. To overcome this, different strategies have been considered, including data augmentation on input image or deep features~\citep{belharbi2020minmaxuncer,ChoeS19,MaiYL20eil}, as well as architectural changes~\citep{gao2021tscam,LeeKLLY19,ZhangW020i2c}. Recently, learning via pseudo-labels has shown some potential, despite its reliance on noisy labels~\citep{belharbi2022fcam,negevsbelharbi2022,MeethalPBG20icprcstn,murtaza2022dipssypo,murtaza2022dips,wei2021shallowspol}. Most previous methods used only forward information in a CNN, with some models designed to leverage backward information as well. These include biologically inspired methods~\citep{cao2015look,zhang2018top}, and gradient~\citep{ChattopadhyaySH18wacvgradcampp,fu2020axiom,JiangZHCW21layercam,SelvarajuCDVPB17iccvgradcam} or confidence score aggregation methods~\citep{desai2020ablation,naidu2020iscam}. 
%
Recently, prompt-based methods such as CLIP-ES~\citep{lin2022} trained using text and images allow the prediction of ROIs related to a specific input class. 
While CAM-based methods are successful on still images, they still require adaptation to leverage the temporal information in videos as in~\citep{tcamsbelharbi2023}. Authors in~\citep{tcamsbelharbi2023} considered improving the quality of pseudo-labels to train an F-CAM architecture~\citep{belharbi2022fcam} by aggregating CAMs extracted from a sequence of frames into a single CAM that covers more ROIs. 



Despite the success, state-of-the-art WSVOL methods still suffer from different limitations. Proposal generation is a critical step often achieved through unsupervised learning using color cues and motion~\citep{HartmannGHTKMVERS12,Tokmakov2016,XuXC12,YanXCC17}. Additionally, the discriminative information, \ie, video class label, is not used directly, but rather to cluster videos~\citep{JerripothulaCY16,joulin2014,Kwak2015,LiuTSRCB14,zhang2020spftn}. As such, the initial localization is vulnerable to errors, which can hinder localization performance in subsequent steps. Moreover, localization is often formulated as the solution to an optimization problem over a single/cluster of videos~\citep{JerripothulaCY16,joulin2014,tsai2016semantic,ZhangJS14}. This requires a model per video/cluster of videos. Therefore, the operational deployment of WSVOL methods at inference time is expensive and impractical for real-world applications and does not scale well to the number of classes.

The recent TCAM method~\citep{tcamsbelharbi2023} exploits the F-CAM architecture to predict and combine full-resolution CAMs, and explicitly benefits from video class labels. However, its performance declines when increasing time dependency, a consequence of the TCAM assumption about the limited movement of objects. This leads to noisy pseudo-labels and poor localizations. In contrast, our CoLo-CAM method leverages spatio-temporal information via co-localization. We explicitly constrain the CAMs to respond similarly over objects with similar colors in a sequence of frames. This assumes objects have similar colors in adjacent frames. Meanwhile, no position assumption is made allowing objects to move freely which adds more flexibility. The interconnection between frames/CAMs allows knowledge transfer and correction of localization, and provides discriminative CAMs that can provide more precise object coverage with less noisy activations.  
 






%%%%%%%%%%%%%%%% Proposal
\section{Proposed Approach}
\label{sec:method}

\noindent \textbf{Notation.} We denote by ${\mathbb{D} = \{(\bm{V}, y)_i\}_{i=1}^N}$ a training set of videos, where ${\bm{V} = \{\bm{X}_t\}_{t=1}^{T}}$ is a video with $T$ frames, ${\bm{X}_t: \Omega \subset \reals^2}$ is the $t$-th frame, and ${\Omega}$ is a discrete image domain. The global video class is denoted ${y \in \{1, \cdots, K\}}$, with $K$ being the total number of classes. We assume that the video class label $y$ is assigned to all frames within a video.  Our model follows an F-CAM model~\citep{belharbi2022fcam} which has a U-Net style architecture~\citep{Ronneberger-unet-2015} with skip connections (see Fig.\ref{fig:proposal}). A detailed figure with full model is presented in the supplementary material (Section 6). It is composed of two parts: a standard encoder ${g}$ with parameters ${\bm{\theta^{\prime}}}$ for image classification and a standard decoder ${f}$ with parameters ${\bm{\theta}}$ for localization.  

Our classifier ${g}$ is composed of a backbone encoder for extracting features, as well as a classification scoring head. Classification probabilities per class are denoted ${g(\bm{X}) \in [0, 1]^K}$ where ${g(\bm{X})_k = \mbox{Pr}(k | \bm{X})}$. This module is trained to classify independent frames via standard cross-entropy: ${\min_{\bm{\theta^{\prime}}} \;  - \log(\mbox{Pr}(y | \bm{X})).}$ Its weights, ${\bm{\theta^{\prime}}}$, are then frozen and used to produce a CAM ${\bm{C}_t}$ for the frame ${\bm{X}_t}$ using the true label $y$~\citep{choe2020evaluating}. This makes ${\bm{C}_t}$ semantically consistent with the video class. Later, it is used to generate pseudo-labels to train the decoder ${f}$.

The localizer ${f}$ is a decoder that produces two  full-resolution CAMs normalized with softmax, and denoted ${\bm{S}_t = f(\bm{X}_t) \in [0, 1]^{\abs{\Omega} \times 2}}$. ${\bm{S}^0_t, \bm{S}^1_t}$ refer to the background and foreground maps, respectively. Let ${\bm{S}_t(p) \in [0, 1]^2}$ denotes a row of matrix ${\bm{S}_t}$, with index ${p \in \Omega}$ indicating a point within ${\Omega}$. We denote by ${\{\bm{X}\}^{n}_t = \{\bm{X}_{t-n-1}, \cdots, \bm{X}_{t-1}, \bm{X}_t\}}$ a set of $n$ consecutive frames going back from time $t$, and ${\{\bm{S}\}^n_t}$ is its corresponding set of CAMs at the decoder output, in the same order. We consider terms at two levels to train our decoder: per-frame and across-frames. The per-frame terms stimulate localization at the frame level, and ensure its local consistency. However, the multi-frame term coordinates localization across frames and ensures its consistency across frames.

\noindent \textbf{Multi-frame loss based on color cues.} Given a sequence of frames ${\{\bm{X}\}^{n}_t}$, we aim to consistently and simultaneously align their corresponding CAMs ${\{\bm{S}\}^{n}_t}$ with respect to \emph{color}. Thus, co-localization of objects is performed with similar colors across frames. This is translated by \emph{explicitly} constraining the CAMs ${\{\bm{S}\}^{n}_t}$ to activate similarly on similar color pixels across all frames. This assumes that an object appearing in adjacent frames maintains a similar color. However, unlike~\citep{tcamsbelharbi2023}, we do not assume limited displacement of objects. This gives our method more flexibility to localize an object independently of its location. Additionally, this joint learning opens an explicit communication tunnel between CAMs, allowing for the transfer and aggregation of knowledge. This can help attenuate localization errors caused by noisy pseudo-labels at the frame level. 
%
We perform this constraint by connecting each pixel at each frame to all pixels in every frame via the \emph{color term} of the CRF loss~\citep{tang2018regularized}. This creates a fully connected graph between all pixels, allowing for explicit communication. In practice, this is achieved by \emph{stitching} all frames (horizontally or vertically) ${\{\bm{X}\}^{n}_t}$ to build a single large image. We refer to the composite image by ${Cat(\{\bm{X}\}^{n}_t)=\bm{\bar{X}}}$, where ${Cat(\cdot)}$ is a stitching function. Similarly, we denote ${Cat(\{\bm{S}\}^{n}_t)=\bm{\bar{S}}}$ as the corresponding stitched CAMs done in the same order as ${Cat(\{\bm{X}\}^{n}_t)}$. The loss is formulated as follows:
\begin{equation}
    \label{eq:crf_rgb}
    \mathcal{R}_c(\{\bm{S}\}^{n}_t, \{\bm{X}\}^{n}_t) = \sum_{r \in \{0, 1\}} \bm{\bar{S}}^{r\top} \; \bm{W} \; (\bm{1} - \bm{\bar{S}}^r) \;,
\end{equation}
where ${\bm{W}}$ is the \emph{color} similarity matrix between pixels of the stitched image ${\bm{\bar{X}}}$.  We use a Gaussian kernel to capture color similarities~\citep{KrahenbuhlK11crf}. The kernel is implemented via the permutohedral lattice~\citep{AdamsBD10lattice} for fast computation.
We refer to this term as CoLoc. Minimizing Eq.\ref{eq:crf_rgb} pushes the activations of the sequence of CAMs ${\{\bm{S}\}^{n}_t}$ to be consistent, with respect to color, across the frames sequence ${\{\bm{X}\}^{n}_t}$. This process simulates tracking the same object over a few frames. To ignite this process, we need to stimulate the per-frame localization of objects. This is achieved through per-frame losses discussed in the next section.

\noindent \textbf{Per-frame loss.} At the frame level, we leverage three terms to stimulate CAM localization via pseudo-labels and ensure its consistency via full CRF loss~\citep{tang2018regularized}. In addition, we aim to alleviate a common CAM issue in still images which is unbalanced activations~\citep{choe2020evaluating,rony2023deep} using generic size prior.

\noindent \emph{Learning via pseudo-labels (PL).} It is commonly known that strong activations in a CAM are more likely to be a foreground, while low activations are assumed to be a background~\citep{zhou2016learning}. We leverage this information to generate pixel-wise pseudo-labels for foreground and background regions using the CAM ${\bm{C}_t}$ of the classifier ${g}$. Different from common practices where ROIs are generated once and kept fixed~\citep{KolesnikovL16}, we stochastically sample our ROIs. This has been shown to be more effective and avoids overfitting~\citep{negevsbelharbi2022}. Therefore, at each SGD step, we sample a foreground pixel using a multinomial distribution over strong activations assuming that an object is local. We use a uniform distribution over low activations to sample background pixels assuming that background regions are evenly distributed in an image. The location of these two random pixels is encoded in ${\Omega^{\prime}_t }$. The partially pseudo-labeled mask for the sample ${\bm{X}_t}$ is denoted ${\bm{Y}_t}$, where ${\bm{Y}_t(p) \in \{0, 1\}^2}$ with labels ${0}$ for background, ${1}$ for foreground,  and locations with unknown labels are encoded as unknown. At this stage, since the pre-trained classifier $g$ is frozen, its CAM ${\bm{C}_t}$ is fixed and does not change during the training of $f$, allowing more stable sampling. Leveraging ${\bm{Y}_t}$ is achieved using partial cross-entropy, expressed as:
\begin{equation}
    \label{eq:pl}
    \begin{aligned}
    &\bm{H}_p(\bm{Y}_t, \bm{S}_t) =\\ 
    &- (1 - \bm{Y}_t(p))\; \log(1 - \bm{S}_t^0(p))- \bm{Y}_t(p) \; \log(\bm{S}_t^1(p)) \;.
    \end{aligned}
\end{equation}

\noindent \emph{Local consistency (CRF).} Standard CAMs are low resolution leading to a bloby effect where activations do not align with the object's boundaries, thus contributing to poor localization~\citep{belharbi2022fcam,choe2020evaluating}. To avoid this, we use a CRF loss~\citep{tang2018regularized} to push the activations of ${\bm{S}_t}$ to be locally consistent in terms of color and proximity. For an image frame $\bm{X}_t$ and its maps ${\bm{S}_t}$, the CRF loss is formulated as,
\begin{equation}
    \label{eq:crf}
    \mathcal{R}(\bm{S}_t, \bm{X}_t) = \sum_{r \in \{0, 1\}} {\bm{S}^r_t}^{\top} \; \bm{W}_t \; (\bm{1} - \bm{S}^r_t) \;,
\end{equation}
where ${\bm{W}_t}$ is an affinity matrix  in which ${\bm{W}[i, j]}$ captures the color similarity and proximity between pixels ${i, j}$ in the image ${\bm{X}_t}$. 


\noindent \emph{Absolute size constraint (ASC).} Unbalanced activations is a common issue in CAMs~\citep{belharbi2022fcam,choe2020evaluating,rony2023deep}. Often, strong activations cover only small and most discriminative parts of an object, allowing the background to dominate. Alternatively, large parts of an image are activated as foreground~\citep{rony2023deep}. To avoid both these scenarios, we employ a global constraint over the CAM, in which we push the size of both regions, \ie, the foreground and background, to be as large as possible in a competitive way. To this end, we use an Absolute Size Constraint (ASC)~\citep{belharbi2020minmaxuncer} on the maps ${\bm{S}_t}$. This is achieved without requiring any knowledge of the true object size or prior knowledge of which region is larger~\citep{pathak2015constrained}. This prior is formulated as inequality constraints which are then solved via a standard log-barrier method~\citep{boyd2004convex},
\begin{equation}
\label{eq:size_loss}
 \sum \bm{S}^r_t \geq 0 \;, \quad r \in \{0, 1\} \;,
\end{equation}
where ${\psi(\bm{S}^0_t) = \sum \bm{S}^0_t, \psi(\bm{S}^1_t)}$ represent the area, \ie, size, of the background and foreground regions, respectively. Using log-barrier function, we set,
\begin{equation}
\label{eq:log-bar-sz}
 \mathcal{R}_s(\bm{S}_t) = \sum_{r \in \{0, 1\}}-\frac{1}{z} \log(\psi(\bm{S}^r_t)) \;,
\end{equation}
where ${z>0}$ is a weight that is increased periodically.


\noindent \textbf{Total training loss.} Our final loss combines per-frame and multi-frame losses to be optimized simultaneously. It is formulated as,
\begin{equation}
\label{eq:totalloss}
\begin{aligned}
\min_{\bm{\theta}} \quad & \sum_{p \in \Omega^{\prime}_t} \bm{H}_p(\bm{Y}_t, \bm{S}_t) + \lambda\; \mathcal{R}(\bm{S}_t, \bm{X}_t) + \mathcal{R}_s(\bm{S}_t) \\
& +  \frac{\lambda_c}{\abs{[\mathcal{R}_c]}}\; \mathcal{R}_c(\{\bm{S}\}^{n}_t, \{\bm{X}\}^{n}_t) \;,\\
\end{aligned}
\end{equation}
where $\lambda$ and $\lambda_c$ are positive weighting coefficients. All terms are optimized simultaneously via SGD.

We note that the magnitude\footnote{Typical magnitude of ${\mathcal{R}_c}$ is ${2\cdot10^9}$ for ${n=2}$, and can grow to ${2\cdot10^{11}}$ for ${n=18}$. As a result, adequate values of the non-adapative ${\lambda_c}$ should be below ${2\cdot10^{-9}}$.} of the term ${\mathcal{R}_c}$ increases with the number of frames $n$. Large magnitudes can easily overpower other terms in Eq.\ref{eq:totalloss}, hindering learning. In practice, this makes tuning the hyper-parameter ${\lambda_c}$ critical and challenging. To reduce this strong dependency on $n$ and stabilize learning, we propose an \emph{adaptive} weight ${\lambda_c}$ that automatically \emph{scales down} this term by its magnitude ${\abs{[\mathcal{R}_c]}}$. This is inspired by the recently proposed adaptive weight decay~\citep{Ghiasi2022} which uses the weight norm and its gradient for adaptation.
%
In this context, the operation ${[\cdot]}$ indicates that this value is now a constant and does not depend on any optimization parameters, \ie, ${\bm{\theta}}$. As a result, this term is always constant ${\lambda_c \; \frac{\mathcal{R}_c}{\abs{[\mathcal{R}_c]}} = \pm \lambda_c}$. However, its derivative is non-zero ${\partial \left( \lambda_c \; \frac{\mathcal{R}_c}{\abs{[\mathcal{R}_c]}} \right) / \partial \bm{\theta} = \frac{\lambda_c}{\abs{[\mathcal{R}_c]}} \frac{\partial \mathcal{R}_c}{\partial \bm{\theta}}}$. In this adaptive setup, the value ${\lambda_c}$ simply amplifies the scaled gradient. Its practical values are ${\lambda_c > 1}$, which makes it easier to tune compared to its non-adaptive version, which will be demonstrated empirically later. Most importantly, the adaptive version creates less dependency on $n$.

After being trained using Eq. \ref{eq:totalloss}, our model is applied on single frames using a forward pass for inference. This allows for fast inference and parallel computation over a video stream. Using standard procedures~\citep{tcamsbelharbi2023,choe2020evaluating}, a localization bounding box is extracted from the foreground CAM of the decoder ${\bm{S}^1_t}$. Our method yields a single model with parameters ${\{\bm{\theta^{\prime}}, \bm{\theta}\}}$ performing both tasks, classification and localization, while handling multiple classes at once.


\begin{table*}
\begin{center}
\resizebox{1.\linewidth}{!}{
\begin{tabular}{|c|l||*{10}{c|}|g|c|}
\hline
\textbf{Dataset} & \textbf{Method \emph{(venue)}} & \textbf{Aero} & \textbf{Bird} & \textbf{Boat} & \textbf{Car} & \textbf{Cat} & \textbf{Cow} & \textbf{Dog} & \textbf{Horse} & \textbf{Mbike} & \textbf{Train} & \textbf{Avg} & \textbf{Time/Frame} \\
\hhline{-----------||---}
\noalign{\vspace{\doublerulesep}}
\hhline{-----------||---}
 &\citep{prest2012learning} {\small \emph{(cvpr)}} & 51.7 & 17.5 & 34.4 & 34.7 & 22.3 & 17.9 & 13.5 & 26.7 & 41.2 & 25.0 & 28.5 & N/A   \\
 &\citep{papazoglou2013fast} {\small \emph{(iccv)}} & 65.4 & 67.3 & 38.9 & 65.2 & 46.3 & 40.2 & 65.3 & 48.4 & 39.0 & 25.0 & 50.1 & 4s  \\
 &\citep{joulin2014} {\small \emph{(eccv)}} & 25.1 & 31.2 & 27.8 & 38.5 & 41.2 & 28.4 & 33.9 & 35.6 & 23.1 & 25.0 & 31.0 & N/A \\
 &\citep{Kwak2015} {\small \emph{(iccv)}}  & 56.5 & 66.4 & 58.0 & 76.8 & 39.9 & 69.3 & 50.4 & 56.3 & 53.0 & 31.0 & 55.7 & N/A  \\
 &\citep{RochanRBW16} {\small \emph{(ivc)}} & 60.8 & 54.6 & 34.7 & 57.4 & 19.2 & 42.1 & 35.8 & 30.4 & 11.7 & 11.4 & 35.8 & N/A \\
 &\citep{Tokmakov2016} {\small \emph{(eccv)}} & 71.5 & 74.0 & 44.8 & 72.3 & 52.0 & 46.4 & 71.9 & 54.6 & 45.9 & 32.1 & 56.6 & N/A  \\
 &POD~\citep{jun2016pod} {\small \emph{(cvpr)}} & 64.3 & 63.2 & 73.3 & 68.9 & 44.4 & 62.5 & 71.4 & 52.3 & 78.6 & 23.1 & 60.2 & N/A  \\
 &\citep{tsai2016semantic} {\small \emph{(eccv)}} & 66.1 & 59.8 & 63.1 & 72.5 & 54.0 & 64.9 & 66.2 & 50.6 & 39.3 & 42.5 & 57.9 & N/A  \\
 &\citep{Halle2017} {\small \emph{(iccv)}} & 76.3 & 71.4 & 65.0 & 58.9 & 68.0 & 55.9 & 70.6 & 33.3 & 69.7 & 42.4 & 61.1 & 0.35s  \\
&\citep{Croitoru2019} (LowRes-Net\textsubscript{iter1}) {\small \emph{(ijcv)}} & 77.0 & 67.5 & 77.2 & 68.4 & 54.5 & 68.3 & 72.0 & 56.7 & 44.1 & 34.9 & 62.1 & 0.02s  \\
\multirow{6}{*}{\ytovone } & \citep{Croitoru2019} (LowRes-Net\textsubscript{iter2}) {\small \emph{(ijcv)}} & 79.7 & 67.5 & 68.3 & 69.6 & 59.4 & 75.0 & 78.7 & 48.3 & 48.5 & 39.5 & 63.5 & 0.02s\\
&\citep{Croitoru2019} (DilateU-Net\textsubscript{iter2}) {\small \emph{(ijcv)}} & 85.1 & 72.7 & 76.2 & 68.4 & 59.4 & 76.7 & 77.3 & 46.7 & 48.5 & 46.5 & 65.8 & 0.02s \\
&\citep{Croitoru2019} (MultiSelect-Net\textsubscript{iter2}) {\small \emph{(ijcv)}} & 84.7 & 72.7 & 78.2 & 69.6 & 60.4 & 80.0 & 78.7 & 51.7 & 50.0 & 46.5 & 67.3 & 0.15s \\
&SPFTN (M)~\citep{zhang2020spftn} {\small \emph{(tpami)}} & 66.4 & 73.8 & 63.3 & 83.4 & 54.5 & 58.9 & 61.3 & 45.4 & 55.5 & 30.1 & 59.3 & N/A  \\
&SPFTN (P)~\citep{zhang2020spftn} {\small \emph{(tpami)}}& \textbf{97.3} & 27.8 & 81.1 & 65.1 & 56.6 & 72.5 & 59.5 & \textbf{81.8} & 79.4 & 22.1 & 64.3 & N/A  \\
&FPPVOS~\citep{umer2021efficient} {\small \emph{(optik)}} & 77.0 & 72.3 & 64.7 & 67.4 & 79.2 & 58.3 & 74.7 & 45.2 & 80.4 & 42.6 & 65.8 & 0.29s  \\
% \hhline{--||---------||---}
\cline{2-14}
&CAM~\citep{zhou2016learning} {\small \emph{(cvpr)}} & 75.0 & 55.5 & 43.2 & 69.7 & 33.3 & 52.4 & 32.4 & 74.2 & 14.8 & 50.0 & 50.1 & 0.2ms  \\
&GradCAM~\citep{SelvarajuCDVPB17iccvgradcam} {\small \emph{(iccv)}} & 86.9 & 63.0 & 51.3 & 81.8 & 45.4 & 62.0 & 37.8 & 67.7 & 18.5 & 50.0 & 56.4 & 27.8ms  \\
&GradCAM++~\citep{ChattopadhyaySH18wacvgradcampp} {\small \emph{(wacv)}} & 79.8 & 85.1 & 37.8 & 81.8 & 75.7 & 52.4 & 64.9 & 64.5 & 33.3 & 56.2 & 63.2 & 28.0ms  \\
&Smooth-GradCAM++~\citep{omeiza2019corr} {\small \emph{(corr)}} & 78.6 & 59.2 & 56.7 & 60.6 & 42.4 & 61.9 & 56.7 & 64.5 & 40.7 & 50.0 & 57.1 & 136.2ms  \\
&XGradCAM~\citep{fu2020axiom} {\small \emph{(bmvc)}} & 79.8 & 70.4 & 54.0 & 87.8 & 33.3 & 52.4 & 37.8 & 64.5 & 37.0 & 50.0 & 56.7 & 14.2ms  \\
&LayerCAM~\citep{JiangZHCW21layercam} {\small \emph{(ieee)}} & 85.7 & \textbf{88.9} & 45.9 & 78.8 & 75.5 & 61.9 & 64.9 & 64.5 & 33.3 & 56.2 & 65.6 & 17.9ms  \\
&CLIP-ES~\citep{lin2022} {\small \emph{(corr)}}  & 46.4 & 74.0 & 5.4 & \textbf{90.9} & \textbf{90.9} & 66.6 & 75.6 & 83.8 & 74.0 & 43.7 & 65.1 & 49.9ms  \\
&TCAM~\citep{tcamsbelharbi2023} {\small \emph{(wacv)}} & 90.5 & 70.4 & 62.2 & 75.7 & 84.8 & \textbf{81.0} & 81.0 & 64.5 & 70.4 & 50.0 & 73.0 & 18.5ms  \\
&CoLo-CAM (ours) & 90.4 & 74.0 & \textbf{91.8} & 87.8 & 78.7 & 80.9 & 89.1 & 74.1 & \textbf{85.1} & \textbf{68.7} & \textbf{82.1} & 18.5ms  \\
\hhline{-----------||---}
\noalign{\vspace{1.5mm}}
\hhline{-----------||---}
&\citep{Halle2017}  {\small \emph{(iccv)}}& 76.3 & 68.5 & 54.5 & 50.4 & 59.8 & 42.4 & 53.5 & 30.0 & 53.5 & \textbf{60.7} & 54.9 & 0.35s   \\
&\citep{Croitoru2019} (LowRes-Net\textsubscript{iter1}) {\small \emph{(ijcv)}} & 75.7 & 56.0 & 52.7 & 57.3 & 46.9 & 57.0 & 48.9 & 44.0 & 27.2 & 56.2 & 52.2 & 0.02s    \\
&\citep{Croitoru2019} (LowRes-Net\textsubscript{iter2}) {\small \emph{(ijcv)}} & 78.1 & 51.8 & 49.0 & 60.5 & 44.8 & 62.3 & 52.9 & 48.9 & 30.6 & 54.6 & 53.4 & 0.02s    \\
&\citep{Croitoru2019} (DilateU-Net\textsubscript{iter2}){\small \emph{(ijcv)}} & 74.9 & 50.7 & 50.7 & 60.9 & 45.7 & 60.1 & 54.4 & 42.9 & 30.6 & 57.8 & 52.9 & 0.02s   \\
\multirow{6}{*}{\ytovtwodtwo } & \citep{Croitoru2019} (BasicU-Net\textsubscript{iter2}){\small \emph{(ijcv)}} & 82.2 & 51.8 & 51.5 & 62.0 & 50.9 & 64.8 & 55.5 & 45.7 & 35.3 & 55.9 & 55.6 & 0.02s  \\
&\citep{Croitoru2019} (MultiSelect-Net\textsubscript{iter2}){\small \emph{(ijcv)}} & 81.7 & 51.5 & 54.1 & 62.5 & 49.7 & 68.8 & 55.9 & 50.4 & 33.3 & 57.0 & 56.5 & 0.15s  \\
% \hhline{-----------||--}
\cline{2-14}
&CAM~\citep{zhou2016learning} {\small \emph{(cvpr)}} & 52.3 & 66.4 & 25.0 & 66.4 & 39.7 & \textbf{87.8} & 34.7 & 53.6 & 45.4 & 43.7 & 51.5 & 0.2ms  \\
&GradCAM~\citep{SelvarajuCDVPB17iccvgradcam} {\small \emph{(iccv)}} & 44.1 & 68.4 & 50.0 & 61.1 & 51.8 & 79.3 & 56.0 & 47.0 & 44.8 & 42.4 & 54.5 & 27.8ms  \\
&GradCAM++~\citep{ChattopadhyaySH18wacvgradcampp} {\small \emph{(wacv)}} & 74.7 & 78.1 & 38.2 & 69.7 & 56.7 & 84.3 & 61.6 & 61.9 & 43.0 & 44.3 & 61.2 & 28.0ms  \\
&Smooth-GradCAM++~\citep{omeiza2019corr} {\small \emph{(corr)}} & 74.1 & 83.2 & 38.2 & 64.2 & 49.6 & 82.1 & 57.3 & 52.0 & 51.1 & 42.4 & 59.5 & 136.2ms  \\
&XGradCAM~\citep{fu2020axiom} {\small \emph{(bmvc)}} & 68.2 & 44.5 & 45.8 & 64.0 & 46.8 & 86.4 & 44.0 & 57.0 & 44.9 & 45.0 & 54.6 & 14.2ms  \\
&LayerCAM~\citep{JiangZHCW21layercam} {\small \emph{(ieee)}} & 80.0 & 84.5 & 47.2 & \textbf{73.5} & 55.3 & 83.6 & 71.3 & 60.8 & 55.7 & 48.1 & 66.0 & 17.9ms  \\
&CLIP-ES~\citep{lin2022} {\small \emph{(corr)}}  & 51.7 & 78.7 & 11.8 & 63.9 & 75.8 & 78.5 & 77.4 & 64.6 & 54.5 & 45.5 & 60.2 & 49.9ms  \\
&TCAM~\citep{tcamsbelharbi2023} {\small \emph{(wacv)}}  & 79.4 & \textbf{94.9} & 75.7 & 61.7 & 68.8 & 87.1 & 75.0 & 62.4 & 72.1 & 45.0 & 72.2 & 18.5ms  \\
&CoLo-CAM (ours) & \textbf{82.9} & 92.2 & \textbf{85.4} & 67.7 & \textbf{80.1} & 85.7 & \textbf{79.2} & \textbf{67.4} & \textbf{72.7} & 58.2 & \textbf{77.1} & 18.5ms  \\
\hline
\end{tabular}}
\end{center}
\caption{\corloc localization accuracy on the \ytovone~\citep{prest2012learning} and \ytovtwodtwo~\citep{KalogeitonFS16} test sets when using the ResNet50 backbone.}
\label{tab:yto}
\end{table*}

\begin{figure*}
     \centering
     \begin{subfigure}[b]{0.32\textwidth}
         \centering
         \includegraphics[width=\textwidth]{ablation-n-YouTube-Objects-v1.0}
         \caption{}
         \label{fig:ablation-range-time}
     \end{subfigure}
     %\\
     \begin{subfigure}[b]{0.32\textwidth}
         \centering
         \includegraphics[width=\textwidth]{ablation-lambda-c-YouTube-Objects-v1.0}
         \caption{}
         \label{fig:ablation-lambda-c}
     \end{subfigure}
     \begin{subfigure}[b]{0.34\textwidth}
         \centering
         \includegraphics[width=\textwidth]{ablation-lambda-c-con-vs-ada-YouTube-Objects-v1.0}
         \caption{}
         \label{fig:ablation-lambda-c-const-vs-adap}
     \end{subfigure}
     %\\
        \caption{Ablations on the \ytovone test set. 
        %
        \textbf{(a)}: Impact on \corloc accuracy of the number of frames ${n}$. 
        %
        \textbf{(b)}: Impact on \corloc accuracy of the \emph{adaptive} ${\lambda_c}$.
        %
        \textbf{(c)}: \corloc accuracy on the \ytovone test set using constant and adaptive ${\lambda_c}$ weight (\emph{left y-axis}), and  ${\log{(\abs{\mathcal{R}_c})}}$ (\emph{right y-axis}). The \emph{x-axis} is the number of frames $n$. 
        }
        \label{fig:ablations-lambda-c-n}
\end{figure*}

{
%\setlength{\tabcolsep}{3pt}
%\renewcommand{\arraystretch}{1.1}
\begin{table}[ht!]
\centering
\resizebox{1.\linewidth}{!}{%
\centering
%\small
\begin{tabular}{|lllc|g|}
\hline
\multicolumn{3}{|l}{\textbf{Methods}}& &  \corloc  \\
\hline \hline %\cline{1-3}\cline{5-5} \\
\multicolumn{3}{|l}{Layer-CAM~\citep{JiangZHCW21layercam} {\small \emph{(ieee)}}} &  &  65.6  \\
\hline %\cline{1-3}\cline{5-5} \\
% \cline{1-4} \\
\multirow{3}{*}{Single-frame} & &       
PL &  &   $68.5$       \\
&& PL + CRF &  &   $69.6$       \\
&& PL + ASC &  &   $66.2$      \\
&& PL + ASC + CRF &  &   $70.5$\\ 
\hline 
%\cline{1-1}\cline{3-3} \cline{5-5} \\
Multi-frame & & PL + ASC + CRF + CoLoc (Ours) &  &   \textbf{82.1} \\
\hline
%\cline{1-1}\cline{3-3} \cline{5-5} \\
\multicolumn{3}{|l}{Improvement} &  &  +16.5\\ \hline 
%\cline{1-3}\cline{5-5} \\
\end{tabular}
}
\caption{Impact on \corloc localization accuracy of different CoLo-CAM loss terms on the \ytovone test set.}
\label{tab:ablation-parts}
\vspace{-1em}
\end{table}
}

\begin{figure}
     \centering
     \begin{subfigure}[b]{0.24\textwidth}
         \centering
         \includegraphics[width=1\textwidth]{1424_car_video-0002_shot-000018_00003429.jpg}
     \end{subfigure}
     \begin{subfigure}[b]{0.24\textwidth}
         \centering
         \includegraphics[width=1\textwidth]{1579_horse_video-0001_shot-000003_00000445.jpg}
     \end{subfigure}
     \\
     \begin{subfigure}[b]{0.24\textwidth}
         \centering
         \includegraphics[width=1\textwidth]{1770_train_video-0008_shot-000022_00046634.jpg}
     \end{subfigure}
     \begin{subfigure}[b]{0.24\textwidth}
         \centering
         \includegraphics[width=1\textwidth]{1776_train_video-0008_shot-000035_00051421.jpg}
     \end{subfigure}
        \caption{Typical challenges of our method. Bounding boxes: ground truth (green), prediction (red).}
        \label{fig:failure}
\end{figure}



\begin{figure}
     \centering
     \begin{subfigure}[b]{0.49\textwidth}
         \centering
         \includegraphics[width=\textwidth]{tag}
     \end{subfigure}
     \\
     \begin{subfigure}[b]{0.49\textwidth}
         \centering
         \includegraphics[width=\textwidth]{1-dog_data_0001_shots_029_frame0001.jpg}
     \end{subfigure}
     \\
     \begin{subfigure}[b]{0.49\textwidth}
         \centering
         \includegraphics[width=\textwidth]{2-train_video-0008_shot-000015_00045305.jpg}
     \end{subfigure}
     \\
     \begin{subfigure}[b]{0.49\textwidth}
         \centering
         \includegraphics[width=\textwidth]{75-train_video-0008_shot-000013_00045196.jpg}
     \end{subfigure}
     \\
     \begin{subfigure}[b]{0.49\textwidth}
         \centering
         \includegraphics[width=\textwidth]{1317-train_video-0008_shot-000029_00048230.jpg}
     \end{subfigure}
      \\
     \begin{subfigure}[b]{0.49\textwidth}
         \centering
         \includegraphics[width=\textwidth]{120-boat_data_0005_shots_010_frame0001.jpg}
     \end{subfigure}
     \\
     \begin{subfigure}[b]{0.49\textwidth}
         \centering
         \includegraphics[width=\textwidth]{147-car_data_0002_shots_012_frame0001.jpg}
     \end{subfigure}
     \\
     \begin{subfigure}[b]{0.49\textwidth}
         \centering
         \includegraphics[width=\textwidth]{1049-motorbike_video-0011_shot-000398_00051051.jpg}
     \end{subfigure}
     \\
     \begin{subfigure}[b]{0.49\textwidth}
         \centering
         \includegraphics[width=\textwidth]{961-aeroplane_video-0002_shot-000012_00004864.jpg}
     \end{subfigure}
     \\
     \begin{subfigure}[b]{0.49\textwidth}
         \centering
         \includegraphics[width=\textwidth]{970-aeroplane_video-0002_shot-000030_00007901.jpg}
     \end{subfigure}
        \caption{Localization examples of test sets frames. 
        %
        %
        \emph{Bounding boxes}: ground truth (green), prediction (red). The second column of each method is the predicted CAM over image.}
        \label{fig:visu-pred}
\end{figure}



%%%%%%%%%%%%%%%% Exps
\section{Results and Discussion}
\label{sec:results}

\subsection{Experimental Methodology}
\label{subsec:exp-method}

\noindent \textbf{Datasets.} To evaluate our method, experiments were conducted on standard public unconstrained video datasets for WSVOL. For training, each video is labeled globally via a class. Frame bounding boxes are provided to evaluate localization. We used two challenging public datasets from YouTube\footnote{\url{https://www.youtube.com}}:  YouTube-Object v1.0 (\ytovone~\citep{prest2012learning}) and YouTube-Object v2.2 (\ytovtwodtwo~\citep{KalogeitonFS16} ). In all our experiments, we followed the same protocol as described in~\citep{tcamsbelharbi2023,KalogeitonFS16,prest2012learning}.

\noindent\textit{YouTube-Object v1.0 (\ytovone) \citep{prest2012learning}:} A dataset composed of videos collected from YouTube by querying the names of 10 classes. A class has between 9-24 videos with a duration ranging from 30 seconds to 3 minutes. It has 155 videos, each split into short-duration clips, \ie, shots. There are 5507 shots with 571 089 frames in all. The dataset has been divided by the authors into 27 test videos for a total of 396 bounding boxes, and 128 videos for training. Some videos from the trainset are considered for validation to perform early-stopping~\citep{tcamsbelharbi2023,KalogeitonFS16,prest2012learning}.  We sampled 5 random videos per class to build a validation set with a total of 50 videos. Frame bounding box annotations are provided for test set.


\noindent\textit{YouTube-Object v2.2 (\ytovtwodtwo)~\citep{KalogeitonFS16}:} This is a large dataset built on \ytovone dataset with large and challenging test set. It is composed of more frames, 722 040 in total. The dataset was divided by the authors into 106 videos for training, and 49 videos for test. Following~\citep{tcamsbelharbi2023}, we sampled 3 random videos per class from the train to build a validation. The test set has more samples compared to \ytovone. It is composed of a total of 2 667 bounding box making it a much more challenging dataset.



\noindent \textbf{Implementation Details.} 
For all our experiments, we trained for 10 epochs with a mini-batch size of 32. ResNet50~\citep{heZRS16} was used as a backbone. Further ablations are done with VGG16~\citep{SimonyanZ14a}, InceptionV3~\citep{SzegedyVISW16}. Unless mentioned differently, all results are obtained using ResNet50 backbone. Images are resized to ${256\times256}$, and randomly cropped patches of size ${224\times224}$ are used for training. The temporal dependency ${n}$ in Eq.\ref{eq:crf_rgb} is set through the validation set from ${n \in \{2, \cdots, 18\}}$. The hyper-parameter ${\lambda_c}$ in Eq.\ref{eq:totalloss} is set from ${\{1, \cdots, 16\}}$ with the adaptive setup. In Eq.\ref{eq:crf_rgb}, frames are stitched horizontally to build a large image.
%
We set the CRF hyper-parameter $\lambda$ in Eq.\ref{eq:totalloss} to ${2e^{-9}}$ -- the same value as in~\citep{tang2018regularized}.  Its color and spatial kernel bandwidth are set to 15 and 100, respectively~\citep{tang2018regularized}. The same color bandwidth is used for Eq.\ref{eq:crf_rgb}. The initial value of the log-barrier coefficient, $z$ in Eq.\ref{eq:log-bar-sz}, is set to $1$ and then increases by a factor of ${1.01}$ in each epoch with a maximum value of $10$ as in~\citep{belharbi2019unimoconstraints,kervadec2019log}. The best learning rate was selected within ${\{0.1, 0.01, 0.001\}}$. The classifier $g$ was pre-trained on single frames. 


\noindent \textbf{Baseline Methods.} For comparison, publicly available results were considered. We compared our proposed approach with different WSVOL methods~\citep{Croitoru2019, Halle2017, joulin2014, Kwak2015, papazoglou2013fast, prest2012learning, RochanRBW16, Tokmakov2016, tsai2016semantic}, POD~\citep{jun2016pod}, SPFTN~\citep{zhang2020spftn}, and FPPVOS~\citep{umer2021efficient}. We also considered the performance of several CAM-based methods reported in~\citep{tcamsbelharbi2023}: CAM~\citep{zhou2016learning}, GradCAM~\citep{SelvarajuCDVPB17iccvgradcam}, GradCam++~\citep{ChattopadhyaySH18wacvgradcampp}, Smooth-GradCAM++~\citep{omeiza2019corr}, XGradCAM~\citep{fu2020axiom}, and LayerCAM~\citep{JiangZHCW21layercam}, which were all trained on single frames, \ie, no temporal dependency was considered. We compare as well to CLIP-ES~\citep{lin2022} using true class as input, and without background list or CAM-refinement for a fair comparison. The LayerCAM method~\citep{JiangZHCW21layercam} was employed to generate CAMs ${\bm{C}_t}$ used to create pseudo-labels for our method, which were then used to build pseudo-labels ${\bm{Y}_t}$ (Eq.\ref{eq:pl}). However, different CAM-based methods can be integrated with our approach.

\noindent \textbf{Evaluation Metric.} Localization accuracy was evaluated using the \corloc metric~\citep{tcamsbelharbi2023,deselaers2012weakly}. It describes the percentage of predicted bounding boxes that have an Intersection Over Union (IoU) between prediction and ground truth greater than half (IoU ${>50\%}$). 





%%%%%%%%%%%
\subsection{Comparison with the State-of-Art}
\label{subsec:compare}

The results in Tab.\ref{tab:yto} indicate that our CoLo-CAM method outperforms other methods by a large margin, more so on \ytovone compared to \ytovtwodtwo. This highlights the difficulty of \ytovtwodtwo. In terms of per-class performance, our method is competitive in most classes. In particular, we observe that our method achieved a large improvement over the challenging class 'Train'. In general, our method and CAM methods are still behind on the 'Horse' and 'Aero' classes compared to the SPFTN method~\citep{zhang2020spftn} which relies on optical flow to generate proposals. Both classes present different challenges. The former class shows with dense multi-instances, while the latter, \ie, 'Aero', appears in complex scenes (airport), often with very large size, and occlusion. Our method has the same inference time as TCAM~\citep{tcamsbelharbi2023}, and our new term in Eq.\ref{eq:crf_rgb} adds a small training time of ${\sim 81}$ ms per 64 frames (see supplementary material for more ablations: backbones, CLIP-ES, frames sampling).



%%%%%%%%%%%%%%%
\subsection{Ablation Studies}


\noindent\emph{Impact of different loss terms (Tab.\ref{tab:ablation-parts}).} Without any spatiotemporal dependency, the use of pseudo-labels (PL), CRF, and the absolute size constraint (ASC) helped to improve the localization performance. This brought about the localization performance from ${65.6\%}$ to ${70.5\%}$. However, adding our multi-frame term, \ie, CoLoc increased the performance up to ${82.1\%}$ demonstrating its benefits.

\noindent\emph{Impact of ${n}$ on localization performance (Fig.\ref{fig:ablation-range-time}).} We observe that both methods, TCAM~\citep{tcamsbelharbi2023} and ours, get better results when considering spatiotemporal information. However, TCAM reached its peak performance when using only ${n=2}$ frames. Large performance degradation is observed when increasing $n$. This comes as a result of assuming that objects have limited movement. On the opposite, our method improves when increasing $n$ until it reaches its peak at ${n=4}$. Different from TCAM, our method showed more robustness and stability with the increase of $n$ since we only assume the object's visual similarity.


\noindent\emph{Constant vs adaptive ${\lambda_c}$ (Fig.\ref{fig:ablation-lambda-c-const-vs-adap}).}  
Note that in the constant setup, adequate ${\lambda_c}$ can be determined using a grid search or given a prior on the magnitude loss. Both approaches are tedious, especially when typical values of ${\lambda_c}$ are below ${2*10^{-9}}$ creating a large search space. Exploring such space is computationally expensive. Using constant value provides good results up to 6 frames, while larger $n$ values lead to performance degradation, as adequate values are required. Our adaptive scheme is more intuitive and does not require prior knowledge. It achieved constantly better and more stable performance with less effort. Better results are obtained with ${\lambda_c}$ values between 3 and 7 (Fig.\ref{fig:ablation-lambda-c}). Performance degradation is observed when using large amplification, giving more importance to the ${\mathcal{R}_c}$ term, allowing it to outweigh other terms which hindered training. In contrast, using small amplification gave a small push to this term, hence, less contribution.







%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Visual Results}

Compared to TCAM~\citep{tcamsbelharbi2023} and LayerCAM~\citep{JiangZHCW21layercam}, we observe that our CAMs became more discriminative (Fig.\ref{fig:visu-pred}). This translates into several advantageous properties. Our CAMs showed sharp, more complete, and less noisy activations localized more precisely around objects. In addition, localizing small objects such as 'Bike', and large objects such as 'Train' becomes more accurate. CAM activations often suffer from co-occurrence issues~\cite {tcamsbelharbi2023,rony2023deep} where consistently appearing contextual objects (background) are confused with main objects. Our CAMs showed more robustness to this issue. This can be seen in the case of road vs. 'Car'/'Bike', or water vs. 'Boat'. 
%
Despite this improvement, our method still faces two main challenges (Fig.\ref{fig:failure}). The first is the case of samples with dense and overlapped instances (top Fig.\ref{fig:failure}). This often causes activations to spill across instances to form a large object hindering localization performance. The second issue concerns large localization errors (bottom Fig.\ref{fig:failure}). Since our localization is mainly stimulated by discriminative cues from CAMs, it is still challenging to detect when a classifier points to the wrong object. This often leads to large errors that are difficult to correct in downstream use. Future work may consider leveraging the classifier response over ROIs to ensure their quality. Additionally, object movement information can be combined with CAM cues. 









\section{Conclusion}
\label{sec:conclusion}

This paper introduces a new CAM-based approach for the WSVOL task. Using a color cue, we constrain the CAM response over a sequence of frames to the same over similar pixels, assuming that an object maintains a similar color. This achieves explicit co-localization across frames. 
%
Empirical experiments showed that our produced CAMs become more discriminative. This translates into several advantageous properties: sharp, more complete, and less noisy activations localized more precisely around objects. Moreover, localizing small/large objects becomes more accurate. Additionally, our methods is more robust to training with long time-dependency. This lead new state-of-the-art localization performance.



\section*{Acknowledgment}
This research was supported in part by the Canadian Institutes of Health Research, the Natural Sciences and Engineering Research Council of Canada, and the Digital Research Alliance of Canada (alliancecan.ca).

% \clearpage
% \newpage

\appendices


% ===================================================================================================
% 
%
%                                          SUPP-MATERIAL
% 
% 
% ===================================================================================================

The next supplementary materials are comprised of:
\begin{itemize}
    \item detailed illustrations of our approach for training and inference (Sec.\ref{sec:method-supp-mat});
    \item a discussion on pseudo-label sampling (Sec.\ref{sec:method-supp-mat});
    \item additional ablation studies (Sec.\ref{subsec:more-ablation}); and 
    \item additional visual results (Sec.\ref{subsec:more-visuals}). 
\end{itemize}

\begin{figure*}[ht!]
\centering
  \centering
  \includegraphics[width=\linewidth]{details}
  \caption[Caption]{Training and inference with our proposed CoLo-CAM method. The single-frame and multi-frame training loss terms are illustrated with $n=3$ frames. \emph{Left: training phase}: It combines the per-frame terms composed of pseudo-labels loss (PL), absolute size constraint (ASC), and CRF loss, as well as the multi-frame term.
  %
  \emph{Right: inference phase}: It operates at a single frame with no temporal dependency and predicts bounding box localization and classification.
  %
  The notation and loss terms are described in Section \ref{sec:method}.}
  \label{fig:proposal-details}
\end{figure*}


%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%
\section{Proposed Approach}
\label{sec:method-supp-mat}

\noindent \textbf{Detailed illustrations for training and inference.} Fig.\ref{fig:proposal-details} presents our detailed model for training and inference. Our model is based on F-CAM architecture~\citep{belharbi2022fcam} which has a U-Net style~\citep{Ronneberger-unet-2015}. During the training, we consider ${n=3}$ input frames for illustration. We use three per-frame terms that aim at stimulating localization at each frame without considering any spatio-temporal information. Using the classifier CAM ${\bm{C}_t}$, we sample pseudo-labels that are used at the pseudo-label term (PL). Additionally, we apply a CRF loss, and a size constraint (ASC). In parallel, we apply a new multi-frame term over all the frames allowing explicit communication between them. This is achieved by constraining the output CAMs of the frames to be activated similarly on pixels with similar colors. Note that the classifier is frozen. It has already been trained.

At inference time, we simply consider single frames, \ie, without spatiotemporal information, allowing for a fast inference time. A bounding box is estimated from the foreground CAM, at the decoder level. In addition, frame classification scores are obtained from the encoder.



\noindent \textbf{Learning with pseudo-labels.}
%
In this section, we provide more details on sampling pseudo-labels (PLs) through the classifier's CAM ${\bm{C}_t}$. These sampled pseudo-labels are used to train the decoder $f$, in combination with other terms. Through the discriminative property of the pseudo-labels, they initiate and stimulate object localization. In CAM-based methods for WSOL, it is common to assume that CAMs' strong activations are likely to point to the foreground, whereas low activations hint at background regions~\citep{durand2017wildcat,zhou2016learning}.

In the following, we denote the foreground region as ${\mathbb{C}^+_t}$ which is computed by the operation ${\mathcal{O}^+}$. For simplicity and without adding new hyper-parameters, ${\mathbb{C}^+_t}$ is defined as the set of pixels with CAM activation ${\bm{C}_t}$ greater than the Otsu threshold~\citep{otsuthresh} estimated from ${\bm{C}_t}$. The background region, \ie, ${\mathbb{C}^-_t}$, is the remaining set of pixels. Both regions are defined as follows:
\begin{equation}
    \label{eq:sets}
    \mathbb{C}^+_t = \mathcal{O}^+(\bm{C}_t), \quad \mathbb{C}^-_t = \Omega - \mathbb{C}^+_t \;, % \mathcal{O}^-(\bm{C}_t) \;.
\end{equation}
where ${\Omega}$ is a discrete image domain. 
Given the weak supervision, these regions are uncertain. For instance, the ${\mathbb{C}^+_t}$ region may hold only a small part of the foreground, along with the background parts. Consequently, the background region ${\mathbb{C}^-_t}$ may still have foreground parts. This uncertainty in partitioning the image into foreground and background makes it unreliable for localization to directly fit~\citep{KolesnikovL16} the model to entire regions. Instead, we pursue a stochastic sampling strategy in both regions to avoid overfitting and to provide the model enough time for the emergence of consistent regions~\citep{belharbi2022fcam,negevsbelharbi2022}. For each frame, and an SGD step, we sample a pixel from ${\mathbb{C}^+_t}$ as the foreground, and a pixel from ${\mathbb{C}^-_t}$ as background to be pseudo-labels. We encode their location according to:
\begin{equation}
    \label{eq:sset}
    \Omega^{\prime}_t = \mathcal{M}(\mathbb{C}^+_t)\; \cup \; \mathcal{U}(\mathbb{C}^-_t) \;, 
\end{equation}
where ${\mathcal{M}(\mathbb{C}^+_t)}$ is a multinomial sampling distribution over the foreground, and ${\mathcal{U}(\mathbb{C}^-_t)}$ is a uniform distribution over the background. This choice of distributions is based on the assumption that the foreground object is often concentrated in one place, whereas the background region is spread across the image. The partially pseudo-labeled mask for the sample ${\bm{X}_t}$ is referred to as ${Y_t}$, where ${Y_t(p) \in \{0, 1\}^2}$ with label ${0}$ for the background and ${1}$ for the foreground. Undefined regions are labeled as unknown. The pseudo-annotation mask ${Y_t}$ is then leveraged to train the decoder using partial cross-entropy. Such stochastic pseudo-labels are expected to stimulate the learning of the filters and guide them to generalize and respond in the same way over regions with similar color/texture.



%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Results and Discussion}
\label{sec:results-supp-mat}


\subsection{Additional Ablation Studies}
\label{subsec:more-ablation}

\noindent\emph{\textbf{(a) Computation time of our multi-frame loss (Eq. \ref{eq:crf_rgb}) with respect to ${n}$ (Fig.\ref{fig:ablation-time-joint-crf}).}} The computation of this loss term is divided into two types of devices (hybrid fashion): CPU and GPU. The left-side matrix product is performed on the GPU to accelerate the computation time, while the rest of the term is first computed on the CPU. Note that the transfer time from CPU to GPU is included in our time analysis. Overhead in processing time grows linearly with respect to the number of frames. However, this computational cost remains reasonable for training in a realistic time, even with a large $n$. For example, $n = 64$ frames can be processed in ${\sim 81}$ ms.

\begin{figure}[ht!]
\centering
  \centering
  \includegraphics[width=\linewidth]{ablation-time-n-joint-crf-YouTube-Objects-v1.0}
  \caption{Computation time of our multi-frame loss term (Eq.\ref{eq:crf_rgb}) in function of number of frames $n$. Computation devices: CPU (Intel(R) Xeon(R), 48 cores), and GPU (NVIDIA Tesla P100). Single image frame size: ${224\times224}$.}
  \label{fig:ablation-time-joint-crf}
\end{figure}

\noindent\emph{\textbf{(b) Effect of frame sampling strategy (Tab. \ref{tab:ablation-frame-sampling}).}} We explored different strategies to randomly sample $n$ training frames from a video, with an emphasis on \emph{frame diversity}:
\begin{itemize}
    \item \textbf{Consecutive scheme}: A first frame is uniformly sampled from the video. Then, its previous ${n-1}$ frames are also considered. This favors similar (less diverse) frames.
    \item \textbf{Interval scheme}: This is the opposite case of the consecutive scheme where the aim is to sample the most diverse frames. The video is divided into equal and disjoint intervals. Then, a single frame is uniformly sampled from each interval.
    \item \textbf{Gaussian scheme}: This middle-ground scheme lies between consecutive and interval strategies. We sample $n$ random frames via a Gaussian distribution centered in the middle of a video.
\end{itemize}
All the sampling is done without repetition. In Tab.\ref{tab:ablation-frame-sampling}, consecutive sampling performs the best, suggesting that our co-localization term is more beneficial when frames are similar. Note that using diverse frames via the Gaussian or interval scheme still yielded good performance.


{
%\setlength{\tabcolsep}{3pt}
%\renewcommand{\arraystretch}{1.1}
\begin{table}[ht!]
\centering
\resizebox{.5\linewidth}{!}{%
\centering
%\small
\begin{tabular}{|lc|g|}
\hline
\multicolumn{1}{|l}{\textbf{Methods}}& &  \corloc  \\
\hline \hline %\cline{1-3}\cline{5-5} \\     
Consecutive     &  &   $82.1$       \\
Gaussian        &  &   $80.2$       \\
Interval        &  &   $79.8$      \\
\hline 
\end{tabular}
}
\caption{Impact on localization accuracy (\corloc) of different frame sampling strategies on the  \ytovone test set.}
\label{tab:ablation-frame-sampling}
\vspace{-1em}
\end{table}
}

\noindent\emph{\textbf{(c) Ablation over backbone architecture (Tab. \ref{tab:ablation-arch}).}} An ablation study was conducted using different backbone classifiers for our method using three common models for WSOL task~\citep{choe2020evaluating}: ResNet50~\citep{heZRS16}, VGG16~\citep{SimonyanZ14a}, and InceptionV3~\citep{SzegedyVISW16}. We observe that in all cases, our method improves the baseline and still outperforms state-of-the-art of ${73.0\%}$. However, there is a discrepancy in performance between the three backbones. This difference is common in WSOL tasks in natural scene images~\citep{choe2020evaluating,belharbi2022fcam} and medical images~\citep{rony2023deep} where ResNet50 performs better. Furthermore, even though these backbones have similar performance on the test set using the baseline method (Layer-CAM~\citep{JiangZHCW21layercam}), they do not necessarily have the same performance on the train set used to train our method. Such a suspected difference could also explain the discrepancy in our results. Note that train sets do not have bounding box annotations, which prevents measuring each backbone localization performance.

{
%\setlength{\tabcolsep}{3pt}
%\renewcommand{\arraystretch}{1.1}
\begin{table}[ht!]
\centering
\resizebox{1.\linewidth}{!}{%
\centering
%\small
\begin{tabular}{|lc|ccc|}
\hline
\multirow{2}{*}{Cases / architectures} & &  \multicolumn{3}{c|}{\corloc (\ytovone)}  \\
\cline{3-5}
& &  ResNet50 & VGG16 & Inception  \\
\hline
Layer-CAM~\citep{JiangZHCW21layercam}         &  &  65.6    & 65.2 & 65.0 \\
\hline 
Layer-CAM~\citep{JiangZHCW21layercam} +  Ours &  &  82.1    & 73.9 & 76.9 \\
\hline
\end{tabular}
}
\caption{Ablation study about the backbone of our method. \corloc localization accuracy on the \ytovone test set.}
\label{tab:ablation-arch}
\vspace{-1em}
\end{table}
}

\noindent\emph{\textbf{(d) Analysis of CLIP-ES-~\citep{lin2022} localization performance (Tab. \ref{tab:clip-es}).}} In this section, the performance of CLIP-ES~\citep{lin2022} is analyzed under different settings for the WSVOL task. CLIP-ES is a prompt-based method trained on a large corpus of text and image datasets. This model takes the image as input, in addition to the class label (text) to localize the ROI of the concerned class. A major difference between this model and standard WSVOL/WSOL models is the requirement of the class label as \emph{input}. This means that all the computations performed by CLIP-ES are in function of the pair (image, class), while other competing methods use only the image as input. This gives CLIP-ES a huge advantage since it is fully aware of which class it looking for. While other methods only require the class at the end of the computations to simply select the concerned CAM. Moreover, CLIP-ES leverages two major supporting elements giving it more advantage compared to other methods:
%
1) Background prior list: It contains a list of objects that are potential backgrounds that are used to suppress and clean the initial CAM. Example of background classes\footnote{We have used the default background list provided in CLIP-ES code: $\{'ground','land','grass','tree','building','wall','sky','lake',\\
'water','river','sea','railway','railroad','keyboard','helmet',\\
'cloud','house','mountain','ocean','road','rock','street',\\
'valley','bridge','sign'\}.$}: 'ground', 'grass'. 
%
2) Refine CAM: After suppressing the background, CAM is then post-processed to improve its localization performance.

Note that all other methods do not have access to both options, making them in a disadvantage. In the main paper, and for a fair comparison, we report the CLIP-ES performance using only the initial CAM. From Tab.\ref{tab:clip-es}, it is clear that a major improvement in localization comes from the background list and CAM refinement. Initial CAM has a performance of ${65.1\%, 60.2\%}$ over \ytovone, and \ytovtwodtwo, respectively. Using these two post-processing techniques allows for a jump in localization performance to ${88.2\%, 84.8\%}$ on both datasets. 

Applying CLIP-ES in WSVOL/WSOL tasks has a major issue in real-world applications with relation to the class since it is required as input. Up to now, we used the true class provided as annotation (\labelgt). Since this model does not perform a classification task, we experiment with a scenario where a pre-trained classifier is used to predict the class in the image. Then, the predicted class is used as input to CLIP-ES. We report the performance in Tab.\ref{tab:clip-es} with the image label named \labelpr. The results show a clear drop in localization performance which is explained by the errors in the predicted classes. Note that we trained a classifier over each dataset using the ResNet50 backbone. We obtained ${72.5\%}$ of classification accuracy over \ytovone test set, and ${68.5\%}$ over \ytovtwodtwo test set. We extend this scenario to a poor classifier (uniform) by marginalizing localization performance over all possible classes (\labelavg). This is done by setting the input class as constant and averaging the localization performance over all classes.


%\labelavg

{
%\setlength{\tabcolsep}{3pt}
%\renewcommand{\arraystretch}{1.1}
\begin{table}[ht!]
\centering
\resizebox{1.\linewidth}{!}{%
\centering
%\small
\begin{tabular}{|ccccc|gg|}
\hline
\multicolumn{4}{|c}{Configurations} && \multicolumn{2}{c|}{Datasets} \\
\hline
Initial  & List prior   & Image & Refined  & &  &  \\
CAM  & background &  label  & CAM & &  \ytovone & \ytovtwodtwo  \\
\hline 
\cmark & \xmark                & \labelgt & \xmark           & &  65.1     & 60.2  \\
\cmark & \cmark                & \labelgt & \xmark           & &  76.6     & 72.6  \\
\cmark & \cmark                & \labelgt & \cmark           & &  88.2     & 84.8  \\
\hline
\cmark & \xmark                & \labelpr & \xmark           & &  56.7     & 50.7  \\
\cmark & \cmark                & \labelpr & \xmark           & &  70.9     & 64.9  \\
\cmark & \cmark                & \labelpr & \cmark           & &  83.9     & 78.9  \\
\hline
\cmark & \xmark                & \labelavg & \xmark           & &  22.0     & 21.7  \\
\cmark & \cmark                & \labelavg & \xmark           & &  51.9     & 44.6  \\
\cmark & \cmark                & \labelavg & \cmark           & &  68.7     & 63.7  \\
\hline
%\cline{1-3}\cline{5-5} \\
% \cline{1-4} \\
\end{tabular}
}
\caption{Ablation on CLIP-ES~\citep{lin2022} method with different configurations. \corloc localization accuracy over \ytovone and \ytovtwodtwo test sets. Labels: \labelgt: ground truth class label. \labelpr: class label predicted by a trained classifier over the corresponding dataset. \labelavg: average \corloc localization over all labels over a single sample.}
\label{tab:clip-es}
\vspace{-1em}
\end{table}
}



%%%
\subsection{Visual Results.} 
\label{subsec:more-visuals}

Fig.\ref{fig:visu-pred-supp-mat-1}, \ref{fig:visu-pred-supp-mat-2} display more visual results. Images show that our method yielded sharp and less noisy CAMs. However, multi-instances and complex scenes remain a challenge. 

In a separate file, we provide demonstrative videos which show the localization of objects. They highlight additional challenges that are not visible in still images, in particular:  
\begin{itemize}
    \item \textbf{Empty frames}. Some frames are without objects, yet still show class activations. This issue is the result of the weak global annotation of videos, and the assumption that all frames inherit that same label. It highlights the importance of detecting such frames and treating them accordingly, starting with classifier training. Discarding these frames will help the classifier better discern objects and reduce noisy CAMs. Without any additional supervision, WSVOL is still a difficult task. One possible solution is to leverage the per-class probabilities of the classifier to assess whether an object is present or not in a frame. Such likelihood can be exploited to discard frames or weight loss terms over them.
    \item \textbf{Temporal consistency}. Although our method brought a quantitative and qualitative improvement, there are some failure cases. In some instances, we observed inconsistencies between consecutive frames. This error is characterized by a large shift in localization where the bounding box (and the CAM activation), moves drastically. This often happens when objects become partially or fully occluded, the appearance of other instances, the scene becomes complex, or zooming in the image makes the object too large. Such setups lead to a \emph{sudden shift} of the CAM's focus, leading to an abrupt change of localization which is undesirable. This drawback is mainly inherited from the \emph{inference} procedure that is achieved at a single frame and without accounting for spatiotemporal information allowing such errors. Future works may consider improving the inference procedure by leveraging spatiotemporal information at the expense of inference time.
\end{itemize}


\newpage
\clearpage

\begin{figure}
     \centering
     \begin{subfigure}[b]{0.35\textwidth}
         \centering
         \includegraphics[width=\textwidth]{tag}
     \end{subfigure}
     \\
     \begin{subfigure}[b]{0.35\textwidth} 
 \centering 
 \includegraphics[width=\textwidth]{102-train_video-0003_shot-000006_00022239.jpg.png} 
  \end{subfigure} 
 \\
\begin{subfigure}[b]{0.35\textwidth} 
 \centering 
 \includegraphics[width=\textwidth]{79-aeroplane_video-0009_shot-000160_00046106.jpg.png} 
  \end{subfigure} 
 \\
\begin{subfigure}[b]{0.35\textwidth} 
 \centering 
 \includegraphics[width=\textwidth]{1105-motorbike_video-0003_shot-000115_00010559.jpg.png} 
  \end{subfigure} 
 \\
\begin{subfigure}[b]{0.35\textwidth} 
 \centering 
 \includegraphics[width=\textwidth]{38-car_video-0005_shot-000037_00007617.jpg.png} 
  \end{subfigure} 
 \\
\begin{subfigure}[b]{0.35\textwidth} 
 \centering 
 \includegraphics[width=\textwidth]{19-aeroplane_video-0002_shot-000005_00003421.jpg.png} 
  \end{subfigure} 
 \\
\begin{subfigure}[b]{0.35\textwidth} 
 \centering 
 \includegraphics[width=\textwidth]{127-dog_data_0001_shots_020_frame0181.jpg.png} 
  \end{subfigure} 
 \\
\begin{subfigure}[b]{0.35\textwidth} 
 \centering 
 \includegraphics[width=\textwidth]{45-boat_data_0005_shots_075_frame0001.jpg.png} 
  \end{subfigure} 
 \\
\begin{subfigure}[b]{0.35\textwidth} 
 \centering 
 \includegraphics[width=\textwidth]{1243-train_video-0014_shot-000271_00116963.jpg.png} 
  \end{subfigure} 
 \\
\begin{subfigure}[b]{0.35\textwidth} 
 \centering 
 \includegraphics[width=\textwidth]{268-train_video-0008_shot-000033_00050464.jpg.png} 
  \end{subfigure} 
 \\
\begin{subfigure}[b]{0.35\textwidth} 
 \centering 
 \includegraphics[width=\textwidth]{93-boat_video-0005_shot-000020_00036648.jpg.png} 
  \end{subfigure} 
 \\
\begin{subfigure}[b]{0.35\textwidth} 
 \centering 
 \includegraphics[width=\textwidth]{30-horse_data_0004_shots_031_frame0012.jpg.png} 
  \end{subfigure} 
 \\
\begin{subfigure}[b]{0.35\textwidth} 
 \centering 
 \includegraphics[width=\textwidth]{232-horse_video-0001_shot-000012_00001811.jpg.png} 
  \end{subfigure} 
 \\
\begin{subfigure}[b]{0.35\textwidth} 
 \centering 
 \includegraphics[width=\textwidth]{641-boat_video-0011_shot-000125_00074943.jpg.png} 
  \end{subfigure} 
 \\
\begin{subfigure}[b]{0.35\textwidth} 
 \centering 
 \includegraphics[width=\textwidth]{240-motorbike_data_0002_shots_020_frame0041.jpg.png} 
  \end{subfigure} 
 \\
\begin{subfigure}[b]{0.35\textwidth} 
 \centering 
 \includegraphics[width=\textwidth]{1307-motorbike_video-0010_shot-000373_00045134.jpg.png} 
  \end{subfigure} 
 \\
\begin{subfigure}[b]{0.35\textwidth} 
 \centering 
 \includegraphics[width=\textwidth]{134-boat_data_0005_shots_006_frame0001.jpg.png} 
  \end{subfigure} 
 \\
\begin{subfigure}[b]{0.35\textwidth} 
 \centering 
 \includegraphics[width=\textwidth]{14-car_video-0005_shot-000037_00007250.jpg.png} 
  \end{subfigure} 
 \\
\begin{subfigure}[b]{0.35\textwidth} 
 \centering 
 \includegraphics[width=\textwidth]{1313-motorbike_video-0011_shot-000409_00052054.jpg.png} 
  \end{subfigure} 
 \\
\begin{subfigure}[b]{0.35\textwidth} 
 \centering 
 \includegraphics[width=\textwidth]{95-motorbike_data_0007_shots_011_frame0034.jpg.png} 
  \end{subfigure} 
 \\
\begin{subfigure}[b]{0.35\textwidth} 
 \centering 
 \includegraphics[width=\textwidth]{142-motorbike_video-0003_shot-000105_00009865.jpg.png} 
  \end{subfigure}
        \caption{Additional localization examples of test sets frames (\ytovone, \ytovtwodtwo). 
        %
        %
        \emph{Bounding boxes}: ground truth (green), prediction (red). The second column of each method is the predicted CAM on the corresponding image.}
        \label{fig:visu-pred-supp-mat-1}
\end{figure}


\begin{figure}
     \centering
     \begin{subfigure}[b]{0.35\textwidth}
         \centering
         \includegraphics[width=\textwidth]{tag}
     \end{subfigure}
     \\
     \begin{subfigure}[b]{0.35\textwidth} 
 \centering 
 \includegraphics[width=\textwidth]{128-car_data_0008_shots_007_frame0001.jpg.png} 
  \end{subfigure} 
 \\
\begin{subfigure}[b]{0.35\textwidth} 
 \centering 
 \includegraphics[width=\textwidth]{884-motorbike_video-0003_shot-000056_00006519.jpg.png} 
  \end{subfigure} 
 \\
\begin{subfigure}[b]{0.35\textwidth} 
 \centering 
 \includegraphics[width=\textwidth]{767-train_video-0008_shot-000047_00054208.jpg.png} 
  \end{subfigure} 
 \\
\begin{subfigure}[b]{0.35\textwidth} 
 \centering 
 \includegraphics[width=\textwidth]{239-boat_data_0005_shots_005_frame0001.jpg.png} 
  \end{subfigure} 
 \\
\begin{subfigure}[b]{0.35\textwidth} 
 \centering 
 \includegraphics[width=\textwidth]{673-train_video-0008_shot-000053_00055950.jpg.png} 
  \end{subfigure} 
 \\
\begin{subfigure}[b]{0.35\textwidth} 
 \centering 
 \includegraphics[width=\textwidth]{191-boat_data_0005_shots_017_frame0001.jpg.png} 
  \end{subfigure} 
 \\
\begin{subfigure}[b]{0.35\textwidth} 
 \centering 
 \includegraphics[width=\textwidth]{1306-train_video-0008_shot-000038_00052298.jpg.png} 
  \end{subfigure} 
 \\
\begin{subfigure}[b]{0.35\textwidth} 
 \centering 
 \includegraphics[width=\textwidth]{896-train_video-0008_shot-000052_00055675.jpg.png} 
  \end{subfigure} 
 \\
\begin{subfigure}[b]{0.35\textwidth} 
 \centering 
 \includegraphics[width=\textwidth]{142-boat_data_0005_shots_078_frame0001.jpg.png} 
  \end{subfigure} 
 \\
\begin{subfigure}[b]{0.35\textwidth} 
 \centering 
 \includegraphics[width=\textwidth]{230-car_video-0002_shot-000018_00003388.jpg.png} 
  \end{subfigure} 
 \\
\begin{subfigure}[b]{0.35\textwidth} 
 \centering 
 \includegraphics[width=\textwidth]{954-cat_video-0007_shot-000045_00018934.jpg.png} 
  \end{subfigure} 
 \\
\begin{subfigure}[b]{0.35\textwidth} 
 \centering 
 \includegraphics[width=\textwidth]{1299-cat_video-0004_shot-000025_00011652.jpg.png} 
  \end{subfigure} 
 \\
\begin{subfigure}[b]{0.35\textwidth} 
 \centering 
 \includegraphics[width=\textwidth]{318-train_data_0012_shots_001_frame0200.jpg.png} 
  \end{subfigure} 
 \\
\begin{subfigure}[b]{0.35\textwidth} 
 \centering 
 \includegraphics[width=\textwidth]{1166-motorbike_video-0011_shot-000394_00050313.jpg.png} 
  \end{subfigure} 
 \\
\begin{subfigure}[b]{0.35\textwidth} 
 \centering 
 \includegraphics[width=\textwidth]{750-train_video-0003_shot-000006_00021988.jpg.png} 
  \end{subfigure} 
 \\
\begin{subfigure}[b]{0.35\textwidth} 
 \centering 
 \includegraphics[width=\textwidth]{989-car_video-0005_shot-000037_00007494.jpg.png} 
  \end{subfigure} 
 \\
\begin{subfigure}[b]{0.35\textwidth} 
 \centering 
 \includegraphics[width=\textwidth]{45-motorbike_video-0011_shot-000408_00051850.jpg.png} 
  \end{subfigure} 
 \\
\begin{subfigure}[b]{0.35\textwidth} 
 \centering 
 \includegraphics[width=\textwidth]{133-cow_data_0004_shots_006_frame0001.jpg.png} 
  \end{subfigure} 
 \\
\begin{subfigure}[b]{0.35\textwidth} 
 \centering 
 \includegraphics[width=\textwidth]{624-dog_video-0013_shot-000129_00056944.jpg.png} 
  \end{subfigure} 
 \\
\begin{subfigure}[b]{0.35\textwidth} 
 \centering 
 \includegraphics[width=\textwidth]{1190-motorbike_video-0003_shot-000056_00006492.jpg.png} 
  \end{subfigure} 
        \caption{Additional localization examples of test sets frames (\ytovone, \ytovtwodtwo). 
        %
        %
        \emph{Bounding boxes}: ground truth (green), prediction (red). The second column of each method is the predicted CAM on the corresponding image.}
        \label{fig:visu-pred-supp-mat-2}
\end{figure}


% \clearpage
% \newpage

\FloatBarrier

\bibliographystyle{apalike}
\bibliography{main}

\end{document}
