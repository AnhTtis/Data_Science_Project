\pdfoutput = 1
% Credit: https://arxiv.org/abs/1708.07459
% https://arxiv.org/abs/1611.01838
% -----------------------------------------------------------------------
% --------------------------------- PREAMBLE ----------------------------
% -----------------------------------------------------------------------
% scrartcl
\documentclass[twocolumn]{article}
\input{macros}
\usepackage{style}

\newcommand\cl{\texttt{CL}\xspace}
\newcommand\corloc{\texttt{CorLoc}\xspace}
\newcommand\resnetfifty{\texttt{ResNet50}\xspace}
\newcommand\youtubeobjects{\texttt{YouTube-Objects}\xspace}
\newcommand\ytovone{\texttt{YTOv1}\xspace}
\newcommand\ytovtwodtwo{\texttt{YTOv2.2}\xspace}

\newcommand\labelgt{\texttt{GT}\xspace}  % ground truth label
\newcommand\labelpr{\texttt{PR}\xspace}  % predicted label
\newcommand\labelavg{\texttt{AVG}\xspace}  % average performance over all labels



% \pagestyle{empty}


% save space
% \setlength{\floatsep}{0.05in}
% \setlength{\textfloatsep}{0.05in}
% \setlength{\intextsep}{0.05in}
% \setlength{\belowcaptionskip}{0.05in}
% \setlength{\abovecaptionskip}{0.05in}
% \setlength{\abovedisplayskip}{0.05in}
% \setlength{\belowdisplayskip}{0.05in}


\makeatletter
\@namedef{ver@everyshi.sty}{}
\newcommand{\removelatexerror}{\let\@latex@error\@gobble}

% notation goes here
\newcommand\red[1]{{\color{red}#1}}
\newcommand{\entropysgd}{\mathrm{Entropy}\textrm{-}\mathrm{SGD}}
\newcommand{\entropyadam}{\mathrm{Entropy}\textrm{-}\mathrm{Adam}}
\newcommand{\minibatch}[1]{\Xi^{#1}}
\newcommand{\mnistfc}{\textrm{mnistfc}}
\newcommand{\smallmnistfc}{\textrm{small-mnistfc}}
\newcommand{\charlstm}{\textrm{char-LSTM}}
\newcommand{\ptblstm}{\textrm{PTB-LSTM}}
\newcommand{\lenet}{\textrm{LeNet}}
\newcommand{\allcnn}{\textrm{All-CNN-BN}}




\title{CoLo-CAM: Class Activation Mapping for Object Co-Localization in \\ Weakly-Labeled Unconstrained Videos}



\renewcommand\footnotemark{}
% \renewcommand\footnotemark{}

\author{Soufiane~Belharbi$^{1}$,
  ~Shakeeb~Murtaza$^{1}$,
  ~Marco~Pedersoli$^{1}$,
  ~Ismail~Ben~Ayed$^{1}$,
  ~Luke~McCaffrey$^{2}$, and
  ~Eric~Granger$^{1}$\\
 %	$^1$\'ETS Montreal\\
 	$^1$ LIVIA, Dept. of Systems Engineering, Ã‰TS, Montreal, Canada \\
	$^2$ Goodman Cancer Research Centre, Dept. of Oncology, McGill University, Montreal, Canada\\
{\tt\footnotesize \textcolor{black}{soufiane.belharbi@etsmtl.ca} }
}

% \setlength{\marginparwidth}{1in}



\newcommand{\nt}[2]{
{\color{ForestGreen}#1}\marginpar{\tiny\noindent{\raggedright{\color{Sienna}[NOTE]} \color{Sienna}{#2} \par}}
}
\renewcommand{\ss}[2]{{\color{cyan}#1}\marginpar{\tiny\noindent{\raggedright{\color{BurntOrange}[SS]}\color{BurntOrange}{#2} \par}}}
\newcommand{\ac}[2]{{\color{magenta}#1}\marginpar{\tiny\noindent{\raggedright{\color{magenta}[AC]}\color{magenta}{#2} \par}}}

\newcommand{\status}[2]{{\color{red}#1}\marginpar{\tiny\noindent{\raggedright{\color{red}#2}}}}

\newcommand{\todoit}[1]{{\color{gray}#1}\marginpar{\tiny\noindent{\raggedright{\color{blue}[TODO]}}}}
\newcommand{\fix}[2]{{\color{blue}#1}\marginpar{\tiny\noindent{\raggedright{\color{blue}[FIX]}\color{blue}{ #2} \par}}}
\newcommand{\ignore}[1]{}
\newcommand{\mref}{{\color{BurntOrange}\&ref\& \xspace}}
\newcommand{\sota}{state-of-the-art\xspace}


% uncomment these for submission!
% \renewcommand{\pc}[2]{#1}
% \renewcommand{\ss}[2]{#1}
% \renewcommand{\ac}[2]{#1}
% \renewcommand{\todoit}[1]{#1}
% \renewcommand{\fix}[1]{#1}


% -----------------------------------------------------------------------
% --------------------------------- BODY --------------------------------
% -----------------------------------------------------------------------


\begin{document}
\maketitle\thispagestyle{fancy}


\maketitle
% \lhead{\color{gray} \small \today}
\rhead{\color{gray} \small Belharbi et al. \;  [Under review 2024]}


\begin{abstract}
Leveraging spatiotemporal information in videos is critical for weakly supervised video object localization (WSVOL) tasks. However, state-of-the-art methods only rely on visual and motion cues, while discarding discriminative information, making them susceptible to inaccurate localizations. Recently, discriminative models have been explored for WSVOL tasks using a temporal class activation mapping (CAM) method. Although their results are promising, objects are assumed to have limited movement from frame to frame, leading to degradation in performance for relatively long-term dependencies. 
%
This paper proposes a novel CAM method for WSVOL that exploits spatiotemporal information in activation maps during training without constraining an object's position. Its training relies on \emph{Co}-\emph{Lo}calization, hence, the name CoLo-CAM.
Given a sequence of frames, localization is jointly learned based on color cues extracted across the corresponding maps, by assuming that an object has similar color in consecutive frames. CAM activations are constrained to respond similarly over pixels with similar colors, achieving co-localization. This improves localization performance because the joint learning creates direct communication among pixels across all image locations and over all frames, allowing for transfer, aggregation, and correction of localizations. Co-localization is integrated into training by minimizing the color term of a conditional random field (CRF) loss over a sequence of frames/CAMs. 
%
Extensive experiments\footnote{Code: \url{https://github.com/sbelharbi/colo-cam}.} on two challenging YouTube-Objects datasets of unconstrained videos show the merits of our CoLo-CAM method, and its robustness to long-term dependencies, leading to new state-of-the-art performance for WSVOL task.
\end{abstract}

\textbf{Keywords:} Weakly-supervised Object Localization, Weakly-Labeled Unconstrained Videos, Co-localization, Class-activation Mapping (CAMs), Deep Learning, Convolutional Neural Networks 
% ===================================================================================================
%
%                                      INTRODUCTION
%
% ===================================================================================================


\begin{figure*}[ht!]
\centering
  \centering
  \includegraphics[width=0.7\linewidth]{full}
  \caption[Caption]{Illustration of training and inference procedures with our proposed CoLo-CAM method. The multi-frame training loss term is illustrated in the case where $n=3$ frames. Pixels are fully connected between three consecutive frames. Pixels with color \textcolor{green}{green} represent the foreground, while \textcolor{blue}{blue} indicates the background. For clarity, only a few pixels are connected over a simplified white grid of pixels. Solid connections illustrate strong visual similarity, while dotted connections indicate weak similarity.
  %
  \\
  \textbf{a) Training phase}: Only the decoder $f$ is trained. The classifier (encoder $g$ + classification head) is pre-trained, and frozen. Our training loss combines per-frame and our multi-frame terms. Our new multi-frame loss jointly learns to localize an object across frames, assuming an object maintains similar color.
  %
  The training detail is illustrated in Fig.\ref{fig:proposal}.
  %
  \\
  \textbf{b) Inference phase}: CoLo-CAM operates on a single frame and predicts a class label and bounding box localization. The notation and loss terms are described in Section \ref{sec:method}.}
  \label{fig:proposal-details}
\end{figure*}


%%%%%%%%% Intro
\section{Introduction}
\label{sec:intro}

Techniques for automated video analysis have become important due to the recent progress of online multimedia platforms such as YouTube that provide easy access to a large number of videos~\cite{ShaoCSJXYZX22,tang2013discriminative}.  
In this work, we focus on the task of localizing objects in videos, which plays a critical role in understanding video content and improving downstream tasks such as action recognition~\cite{kong2022}, video-based summarization~\cite{ZhangHJYC17}, event detection~\cite{ChangYLZH16}, object detection~\cite{Chen0HW20,HanWCQ20,ShaoCSJXYZX22}, facial emotion recognition~\cite{xue22,Zhang22}, and visual object tracking~\cite{BergmannML19,LuoXMZLK21}. Unfortunately, the video annotations required for fully supervised localization, \ie, bounding boxes for each frame, are costly due to the large number of frames. As an alternative, weak supervision using a video class label or tag associated with the entire video is currently the most common annotation for weakly supervised video object localization (WSVOL)~\cite{JerripothulaCY16,tsai2016semantic}.  
A video class label describes the main object appearing in the video without spatiotemporal information, \ie, spatial location, and start/end time of the object's appearance, which adds more uncertainty to the label.  
Additionally, unconstrained videos are often captured in the wild, with varying quality, moving objects and cameras, changing viewpoints and illumination, and decoding artifacts. Given these factors, localization from weakly-labeled unconstrained videos is a challenging task.


State-of-the-art WSVOL methods often follow a similar strategy. Localization is often formulated as an optimization problem solved over the entire video. Initial segments/proposals are generated based on visual or motion cues, which are then used to identify and refine object locations through post-processing~\cite{HartmannGHTKMVERS12,Kwak2015,prest2012learning,tang2013discriminative,Tokmakov2016,XuXC12,YanXCC17,zhang2020spftn}. This process is optimized under visual appearance and motion constraints to ensure consistency. Other methods rely on co-localization/co-segmentation across videos/images via graphs that allow for a larger view and interactions between segments~\cite{ChenCC12,FuXZL14,JerripothulaCY16,joulin2014,tsai2016semantic,ZhangJS14}. Despite their success, these methods have several limitations. For instance, initial proposals are not necessarily discriminative, \ie, aligned with the video class, due to unsupervised region extraction using visual and motion cues (e.g., optical flow)~\cite{LeeKG11,SundaramBK10}. Moreover, video class labels are typically only used to cluster videos, which further limits the beneficial impact of discriminative information. Additionally, a per-video model is often optimized, leading to several issues, including scalability, inference time, and deployment. Finally, multi-stage training combines the best local solutions, which does not necessarily lead to the best global solution.

Recently, the temporal class activation mapping (TCAM) method has been proposed to train discriminative multi-class deep learning (DL) model for WSVOL~\cite{tcamsbelharbi2023}. This paper focuses on CAM methods for weakly supervised object localization approach (WSOL) that have been successful on still images~\cite{choe2020evaluating,rony2023deep}. Using only global image class labels, CAM-based methods allow training a discriminative multi-class DL model (e.g., a CNN or vision transformer) to classify an image and localize the corresponding object of interest. Strong CAM activations indicate the potential presence of an object, making them particularly suitable for object localization with low-cost weak annotations~\cite{oquab2015object}. The TCAM method~\cite{tcamsbelharbi2023} relies on pixel-wise pseudo-labels collected from a pre-trained classifier. Built on the F-CAM architecture~\cite{belharbi2022fcam}, TCAM relied on a temporal max-pooling module over consecutive CAMs to improve pixel-wise pseudo-labels.
Despite its success and fast inference, TCAM assumes that objects have limited displacement over frames~\cite{tcamsbelharbi2023}. This can only be beneficial for short time dependencies over a few frames.


To alleviate this issue while benefiting from CAM methods, the CoLo-CAM method is proposed for WSVOL (Figs. \ref{fig:proposal-details} and \ref{fig:proposal}). This new multi-class discriminant approach leverages spatiotemporal information without constraining object movement. In particular, CoLo-CAM performs \emph{co-localization} via explicit joint learning of CAMs across multiple frames in a video. Under the assumption that an object maintains a similar color \emph{locally}, CoLo-CAM achieves co-localization by constraining a sequence of CAMs to be \emph{consistent} by pushing them to activate similarly over pixels with a similar color. To avoid major changes in color/contrast of object appearance, and therefore maintain the color assumption, our joint learning is applied locally over consecutive frames.
%
Unlike TCAM~\cite{tcamsbelharbi2023}, CoLo-CAM does not assume a limited displacement of objects. Rather, objects can appear anywhere in the image, making for a more flexible method. This builds a consistent localization. 
%

Our total training loss is composed of per-frame and multi-frame terms, as illustrated in Fig.\ref{fig:proposal-details}, which are optimized simultaneously via standard Stochastic Gradient Descent (SGD). Our multi-frame optimization method is achieved by constructing a fully connected graph between all pairs of pixels across a set of frames. This graph accounts for pixel-wise information in terms of image color and discriminative CAM response. In particular, pixel probability of CAMs is used as unary potentials, while the \emph{color} difference between every two pixels is considered as pairwise potentials. We then minimize a conditional random field (CRF) loss~\cite{tang2018regularized} using only color similarity,  and without spatial information. This allows for co-localization of an object of interest across multiple frames and independently of its location. 
%
Following TCAM~\cite{tcamsbelharbi2023}, CoLo-CAM also relies on standard per-frame local constraints aiming to stimulate and initiate localization. This includes pixel-wise pseudo-labels, and global size constraints, in addition to standard CRF loss based on color and location~\cite{tang2018regularized} in a single image. Since  pixel-wise pseudo labels are not reliably extracted, our co-localization training allows for the transfer, aggregation, and correction of localizations across frames. 

\noindent \textbf{Our main contributions are summarized as follows.}

\noindent \textbf{1)} A CoLo-CAM method is introduced for WSVOL in unconstrained
videos. 
Unlike the state-of-the-art TCAM method~\cite{tcamsbelharbi2023}, our novel temporal training term does not assume that an object maintains a similar location in different frames. Instead, it allows the object to be located anywhere, making it more robust to motion. This is achieved by performing co-localization over a sequence of CAMs constrained to yield similar activation over pixels with a similar color. To that end, CoLo-CAM builds a fully connected graph between all pair of pixels to optimize a CRF loss using color.

\noindent \textbf{2)}  Our extensive experiments and ablations on two challenging public datasets of unconstrained videos, YouTube-Objects~\cite{KalogeitonFS16,prest2012learning}, show the merits of our proposed method and its robustness to long dependency. Most importantly, our produced CAMs become more discriminative. This translates into several advantageous properties including sharp, more complete, and less noisy activations localized more precisely around objects. Moreover, small and large objects are more accurately localized. CoLo-CAM provides a new state-of-the-art localization performance on YouTube-Objects datasets. Additionally, our results confirm the potential of discriminative learning for WSVOL tasks, in contrast to relying only on motion and visual cues.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Related Work}
\label{sec:related-w}
 
This section reviews weakly supervised methods for localization, segmentation, and co-localization/co-segmentation in videos trained using only video class as supervision. Additionally, CAM methods for WSOL on images are discussed, as they relate to WSVOL. 

\noindent \textbf{(a) Localization.}
%
Different methods exploit potential proposals as pseudo-supervision for localization~\cite{prest2012learning,zhang2020spftn}. In~\cite{prest2012learning}, class-specific detectors are proposed. First, segments of coherent motion~\cite{BroxM10} are extracted, and then a spatiotemporal bounding box is fitted to each segment, forming tubes. A single tube is jointly selected by minimizing energy based on the similarity between tubes, visual homogeneity over time, and the likelihood of containing an object. The selected tubes are used as box supervision to train a per-class detector~\cite{FelzenszwalbGMR10}. Other methods seek to discover a prominent object (foreground) in (a) video(s)~\cite{Kwak2015,jun2016pod,RochanRBW16}. Then, similar regions are localized and refined using visual appearance and motion consistency.

\noindent \textbf{(b) Segmentation.} 
%
Independent spatiotemporal segments are first extracted~\cite{HartmannGHTKMVERS12,tang2013discriminative,XuXC12,YanXCC17} via unsupervised methods~\cite{BanicaAIS13,XuXC12} or with a proposals generator~\cite{zhang2015semantic} using pretrained detectors~\cite{zhang2015semantic}. Then, using multiple properties such as visual appearance and motion cues, these segments are pseudo-labeled, while preserving temporal consistency. Graphs, such as the conditional random field (CRF) and GrabCut approaches~\cite{RotherKB04}, are often used to ensure consistency. 
%
In M-CNN~\cite{Tokmakov2016}, a fully convolutional network (FCN) is combined with motion cues to estimate pseudo-labels. Using motion segmentation, a Gaussian mixture model is exploited to estimate foreground appearance potentials~\cite{papazoglou2013fast}. Combined with the prediction of the FCN, these potentials are used to produce better segmentation pseudo-labels via a GrabCut-like method~\cite{RotherKB04}, and these are then used to fit the FCN.
%
The previously mentioned segmentation methods use video class labels to cluster videos with the same class. While other methods do not rely on such supervision, the process is generally similar. Foreground regions are first estimated~\cite{Croitoru2019,Halle2017,papazoglou2013fast,umer2021efficient} using motion cues~\cite{SundaramBK10}, PCA, or VideoPCA~\cite{StretcuL15}. This initial segmentation is later refined via graph methods~\cite{RotherKB04}.

\noindent \textbf{(c) Co-segmentation/Co-localization.}
%
The co-segmentation approach has been leveraged to segment similar objects across a set of videos. Typical methods rely on discerning common objects via the similarity of different features including visual appearance, motion, and shape. Using initial guessing of objects/segments, graphs, such as CRF and graph cuts, are considered to model relationships between them~\cite{ChenCC12,FuXZL14,tsai2016semantic,ZhangJS14}.  In~\cite{ChenCC12}, the authors use relative intra-video motion extracted from dense optical flow and inter-video co-features based on a Gaussian Mixture Model (GMM). The common object, \ie, the foreground, in two videos is isolated from the background using a Markov random field (MRF). It is solved iteratively via graph cuts while accounting for a unary term based on the distance to GMMs, and a pairwise energy based on feature distance weighted by the relative motion distance between super-voxels. 

% co-localization: short
Other methods aim to co-localize objects in images or videos using bounding boxes instead of segments~\cite{JerripothulaCY16,joulin2014}. In~\cite{joulin2014}, objectness~\cite{AlexeDF12} is used to generate proposals to search for the best candidate. Box similarity and discriminability~\cite{TangJLF14} in addition to multiple object properties including visual appearance, position, and size are used as constraints to ensure consistency. Authors in~\cite{JerripothulaCY16} leverage co-saliency as before filtering out noisy bounding box proposals.



%%%%%%%%%%%
\noindent\textbf{(d) WSOL in still images.}
CAM-based methods have emerged as a dominant approach for the WSOL~\cite{choe2020evaluating,rony2023deep} on still images. Using only global image-class labels, a DL model can be trained to classify an image and yield a discriminative heatmap per class, \ie CAM, to localize the object in the image. 
%
Early work of these methods focused on designing variant spatial class-scores pooling layers~\cite{durand2017wildcat,durand2016weldon,oquab2015object,zhou2016learning}.  
However, CAMs tend to under-activate by highlighting only small and the most discriminative parts of an object~\cite{choe2020evaluating,rony2023deep}, diminishing its localization performance. To overcome this, different strategies have been considered, including data augmentation on input image or deep features~\cite{belharbi2020minmaxuncer,ChoeS19,MaiYL20eil}, as well as architectural changes~\cite{gao2021tscam,LeeKLLY19,ZhangW020i2c}. Recently, learning via pseudo-labels has shown good results, despite its reliance on noisy labels~\cite{negevsbelharbi2022,belharbi2022fcam,MeethalPBG20icprcstn,murtaza2022dipssypo,murtaza2022dips,Murtaza2023dips,wei2021shallowspol}. Most previous methods use only forward information in a model. Other  methods leverage backward information as well. These include biologically inspired methods~\cite{cao2015look,zhang2018top}, and gradient~\cite{ChattopadhyaySH18wacvgradcampp,fu2020axiom,JiangZHCW21layercam,SelvarajuCDVPB17iccvgradcam} or confidence/score aggregation methods~\cite{desai2020ablation,naidu2020iscam}. 
%
Recently, prompt-based methods such as CLIP-ES~\cite{lin2023} trained using text and images allow the prediction of ROIs related to a specific input class. However, such models lack classification capability. 
% 
%
While CAM-based methods are successful on still images, they still require adaptation to leverage the temporal information in videos. Authors in~\cite{tcamsbelharbi2023} considered improving the quality of pseudo-labels to train an F-CAM architecture~\cite{belharbi2022fcam}. To this end, they aggregate CAMs extracted from a sequence of frames into a single CAM that covers more ROIs. However, this led to limited success since degradation in localization performance has been reported after a sequence of 2 frames.


Despite the reported success, state-of-the-art WSVOL methods still suffer different limitations. Proposal generation is a critical step often achieved through unsupervised learning using color cues and motion~\cite{HartmannGHTKMVERS12,Tokmakov2016,XuXC12,YanXCC17}. Additionally, the discriminative information, \ie, video class label, is not used directly, but rather to cluster videos~\cite{JerripothulaCY16,joulin2014,Kwak2015,LiuTSRCB14,zhang2020spftn}. As such, the initial localization is vulnerable to errors, which can hinder localization performance in subsequent steps. Moreover, localization is often formulated as the solution to an optimization problem over a single/cluster of videos~\cite{JerripothulaCY16,joulin2014,tsai2016semantic,ZhangJS14}. This requires a model per video/cluster of videos. Therefore, the operational deployment of WSVOL methods at inference time is expensive and impractical for real-world applications, and does not scale well to large number of classes.

\noindent\textbf{(e) Relation to F-CAM~\cite{belharbi2022fcam} and TCAM~\cite{tcamsbelharbi2023}}: 
%
F-CAM~\cite{belharbi2022fcam} is a WSOL method for still images. While it does not leverage temporal information, its training relies on key terms to alleviate standard CAM issues such as CAM under-activation and bloby effect~\cite{rony2023deep}. It performs classification and it produces full size CAM, thanks to its U-Net like architecture~\cite{Ronneberger-unet-2015}, allowing the application of different pixel-wise losses, such as pixel pseudo-labels and CRF~\cite{tang2018regularized}.
The recent TCAM method~\cite{tcamsbelharbi2023} builds on top of F-CAM. It uses the same F-CAM architecture, and its same per-frame losses. Its aim is to leverage temporal information to improve localization. This is done by improving pixel pseudo-labels via a new temporal max-pooling over low resolution CAMs from a pretrained classifier. It allows to gather relevant spatial ROIs through time. However, its localization performance declines when increasing time dependency, a consequence of the TCAM assumption about the limited movement of objects. This leads to noisy pseudo-labels and, therefore, poor localization. 
%
Our CoLo-CAM method also builds on top of F-CAM architecture, and its per-frame losses. However, the major difference is how to leverage temporal information to improve localization in videos. In this paper, a new constraint loss is proposed for joint training over a sequence of frames to perform co-localization. We explicitly constrain the output CAMs to respond similarly over objects with similar colors in a sequence of frames. This assumes objects have similar colors in adjacent frames. Meanwhile, no position assumption is made allowing objects to move freely which adds more flexibility. The interconnection created between frames/CAMs allows for knowledge transfer and correction of localization. The resulting discriminative CAMs provide a more precise object coverage and less noisy activations. 



%%%%%%%%%%%%%%%% Proposal
\section{Proposed Approach}
\label{sec:method}

Let us denote by ${\mathbb{D} = \{(\bm{V}, y)_i\}_{i=1}^N}$ a training set of videos, where ${\bm{V} = \{\bm{X}_t\}_{t=1}^{T}}$ is a video with $T$ frames, ${\bm{X}_t: \Omega \subset \reals^2}$ is the $t$-th frame, and ${\Omega}$ is a discrete image domain. The global video class label is denoted ${y \in \{1, \cdots, K\}}$, with $K$ being the total number of classes. We assume that the label $y$ is assigned to all frames within a video.  Our model follows an F-CAM model~\cite{belharbi2022fcam} which has a U-Net style architecture~\cite{Ronneberger-unet-2015} with skip connections (Fig.\ref{fig:proposal-details}). It is composed of two parts: a standard encoder ${g}$ with parameters ${\bm{\theta^{\prime}}}$ for image classification and a standard decoder ${f}$ with parameters ${\bm{\theta}}$ for localization. 

Our classifier ${g}$ is composed of a backbone encoder for extracting features, as well as a classification scoring head. Classification probabilities per class are denoted ${g(\bm{X}) \in [0, 1]^K}$ where ${g(\bm{X})_k = \mbox{Pr}(k | \bm{X})}$. This module is trained to classify independent frames via standard cross-entropy: ${\min_{\bm{\theta^{\prime}}} \;  - \log(\mbox{Pr}(y | \bm{X}))}$. Once trained, its weights, ${\bm{\theta^{\prime}}}$, are then frozen and used to produce \emph{classifier low resolution CAM} which is then interpolated to image full size. This CAMs is denoted ${\bm{C}_t}$ for the frame ${\bm{X}_t}$. It is extracted using the video true label $y$~\cite{choe2020evaluating}. Therefore,  ${\bm{C}_t}$ is made semantically consistent with the video class. Later, it is used to generate pseudo-labels to train the decoder ${f}$.

The localizer ${f}$ is a decoder that produces two full-resolution CAMs normalized with softmax, and denoted ${\bm{S}_t = f(\bm{X}_t) \in [0, 1]^{\abs{\Omega} \times 2}}$. ${\bm{S}^0_t, \bm{S}^1_t}$ refer to the background and foreground maps, respectively. Pixel-wise training losses will be applied over these two maps ${\bm{S}_t}$, and they will be used for localization at inference time. Note that ${\bm{S}_t}$ CAMs is produced by the decoder, while ${\bm{C}_t}$ is produced by the classifier.
%
Let ${\bm{S}_t(p) \in [0, 1]^2}$ denotes a row of matrix ${\bm{S}_t}$, with index ${p \in \Omega}$ indicating a point within ${\Omega}$. We denote by ${\bm{F}^{t, n} = \{\bm{X}_{t-n-1}, \cdots, \bm{X}_{t-1}, \bm{X}_t\}}$ a set of $n$ consecutive frames going back from time $t$, and ${\bm{M}^{t, n} = \{\bm{S}_{t-n-1}, \cdots, \bm{S}_{t-1}, \bm{S}_t\}}$ is its corresponding set of CAMs at the decoder output, in the same order. We consider terms at two levels to train our decoder: per-frame and across-frames. The per-frame terms stimulate localization at the frame level, and ensure its local consistency. However, the multi-frame term coordinates localization and ensures its consistency across multiple frames.

\subsection{Per-Frame Losses} 
\label{subsec:perframe-losses}
At frame level, we adopt three common terms from F-CAM~\cite{belharbi2022fcam} work to ensure local localization consistency. The same terms have been used in T-CAM~\cite{tcamsbelharbi2023} work as well. This includes: pixel pseudo-labels, generic size prior, and standard CRF loss~\cite{tang2018regularized}. This has shown to be effective to reduce several limitations of standard CAMs such as bloby effect, unbalanced activations, and misalignment with object boundaries~\cite{tcamsbelharbi2023,belharbi2022fcam,choe2020evaluating,rony2023deep}. Using pixel pseudo-labels allows to stimulate localization at frame level. We review these three terms briefly in the following paragraph.

\noindent \textbf{a) Learning via pseudo-labels (PLs).} It is commonly known that strong activations in a CAM are more likely to be a foreground, while low activations are assumed to be a background~\cite{zhou2016learning}. Following this, pixel-wise PLs for foreground and background regions are generated using the CAM ${\bm{C}_t}$ generated by the classifier ${g}$. ROIs are stochastically sampled instead of fixed ROIs~\cite{KolesnikovL16}. This has been shown to be more effective and avoids overfitting~\cite{negevsbelharbi2022}. Therefore, at each SGD step, a foreground pixel is sampled using a multinomial distribution over strong activations assuming that an object is local. Uniform distribution over low activations is used to sample background pixels assuming that background regions are evenly distributed in an image. The location of these two random pixels is encoded in ${\Omega^{\prime}_t }$. The partially pseudo-labeled mask for the sample ${\bm{X}_t}$ is denoted ${\bm{Y}_t}$, where ${\bm{Y}_t(p) \in \{0, 1\}^2}$ at location $p$ with value ${0}$ for background, ${1}$ for foreground,  and locations with unknown labels are encoded as unknown. At this stage, since the pre-trained classifier $f$ is frozen, its CAM ${\bm{C}_t}$ is fixed and does not change during the training of the decoder $g$, allowing for more stable sampling. The decoder output ${\bm{S}_t}$ is then aligned with ${\bm{Y}_t}$ using partial cross-entropy,
\begin{equation}
    \label{eq:pl}
    \begin{aligned}
    &\bm{H}_p(\bm{Y}_t, \bm{S}_t) =\\
    & - (1 - \bm{Y}_t(p))\; \log(\bm{S}_t^0(p))- \bm{Y}_t(p) \; \log(\bm{S}_t^1(p)) ,\; p \in \Omega\;.
    \end{aligned}
\end{equation}
More details on pixel-wise learning is provided in the appendix.

\noindent \textbf{b) Local consistency (CRF).} Standard CAMs have low resolution leading to a bloby effect where activations do not align with the object's boundaries, thus contributing to poor localization~\cite{belharbi2022fcam,choe2020evaluating}. To avoid this, a CRF loss~\cite{tang2018regularized} is employed to push the activations of ${\bm{S}_t}$ to be locally consistent in terms of pixels proximity and color. For a \emph{single image frame} $\bm{X}_t$ and its map ${\bm{S}_t}$, the CRF loss is formulated as,
\begin{equation}
    \label{eq:crf}
    \mathcal{R}(\bm{S}_t, \bm{X}_t) = \sum_{r \in \{0, 1\}} {\bm{S}^r_t}^{\top} \; \bm{W}_t \; (\bm{1} - \bm{S}^r_t) \;,
\end{equation}
where ${\bm{W}_t}$ is an affinity matrix  in which ${\bm{W}[i, j]}$ captures the \emph{proximity} between pair of pixels ${i, j}$ in the \emph{single frame} ${\bm{X}_t}$, and their \emph{color similarity}; ${r=0}$ is the index of the background, while ${r=1}$ is for the foreground. Since ${\bm{S}_t}$ has the same size as the input image, it is beneficial to minimize Eq.\ref{eq:crf}, since pixels' details is available. This allows to more accurately compare their color.

\noindent \textbf{c) Absolute size constraint (ASC).} Unbalanced activations is a common issue in CAMs~\cite{choe2020evaluating,belharbi2022fcam,rony2023deep}. Often, strong activations cover only small and most discriminative parts of an object, allowing the background to dominate. Alternatively, large parts of an image are activated as foreground~\cite{rony2023deep}. To avoid such unbalance, global size prior over the CAM is employed. It consists of constraining the size of foreground and background regions to be as large as possible in a competitive way. To this end, Absolute Size Constraint (ASC)~\cite{belharbi2020minmaxuncer} on the decoder maps ${\bm{S}_t}$ is used. This is achieved without requiring any knowledge of the true object size or prior knowledge of which region is larger~\cite{pathak2015constrained}. This prior is formulated as inequality constraints which are then solved via a standard log-barrier method~\cite{boyd2004convex},
\begin{equation}
\label{eq:size_loss}
 \sum \bm{S}^r_t \geq 0 \;, \quad r \in \{0, 1\} \;,
\end{equation}
where ${\psi(\bm{S}^0_t) = \sum \bm{S}^0_t, \psi(\bm{S}^1_t)}$ represent the area, \ie, size, of the background (${r=0}$) and foreground (${r=1}$) regions, respectively. Using log-barrier function, we set,
\begin{equation}
\label{eq:log-bar-sz}
 \mathcal{R}_s(\bm{S}_t) = \sum_{r \in \{0, 1\}}-\frac{1}{z} \log(\psi(\bm{S}^r_t)) \;,
\end{equation}
where ${z>0}$ is a weight that is increased periodically; ${r=0}$ is for the background, while ${r=1}$ is for the foreground.



\begin{figure}[h!]
\centering
  \centering
  \includegraphics[width=\linewidth]{proposal}
  \caption[Caption]{Illustration of the combination of the single and multi-frame training process using our CoLo-CAM method in the case where $n=3$ frames. For the multi-frame loss, each pair of pixels across all three images are interconnected. For clarity, only few pixels and connections are shown. \textcolor{green}{Green dots} are foreground pixels. \textcolor{blue}{Blue dots} are background pixels. 
  %
  The classifier (encoder $g$ + classification head) is pre-trained, and frozen, and only the decoder $f$ is trained. We employ per-frame and multi-framer losses. Our multi-frame loss ${\mathcal{R}_c}$ assumes that an object of interest has similar colour over multiple adjacent frames, i.e. the object 'horse' in the frames. This assumption also applies to background objects, i.e., the 'sky', 'grass', and 'trees'. CAMs are constrained to have similar responses over pixels with similar color across different locations over all frames. Each pair of pixels across the three frames are interconnected. Solid connections illustrate strong visual similarity, while dotted connections indicate weak similarity. Additionally, the per-frame terms consist of the pseudo-label loss, absolute size constraint (${\mathcal{R}_s}$), and CRF loss (${\mathcal{R}}$).
  }
  \label{fig:proposal}
\end{figure}
  

\subsection{Multi-Frame Loss Based on Color Cues}
\label{subsec:multi-frameloss}
Our new temporal loss extends over a sequence of frames. It builds a fully connected graph over all pixel locations across all frames. Using discriminative response over the decoder CAMs, and only the pairwise \emph{color similarity}, a CRF energy loss~\cite{tang2018regularized} is set to be minimized. This amounts to achieve co-localization across multiple frames using color appearance. The emerged object localization at frame level via per-frame terms are propagated through all the frames, and ensured to be consistent. Regions that have been deemed to be foreground are maintained as foreground across all frames, and the same way for background. Such knowledge transfer allows reducing errors caused by noisy pseudo-labels at frame level leading to better localization. Our approach is depicted in Fig.\ref{fig:proposal}. Our loss is based on the assumption that \emph{locally}, \ie over the extent of few frames, objects and overall frames maintain the same color, which is a fair assumption in video processing. Unlike TCAM~\cite{tcamsbelharbi2023}, we do not assume limited displacement of objects. This gives our method more flexibility to localize an object independently of its location.

Given a sequence of ${n}$ frames ${\bm{F}^{t, n}}$, we aim to consistently and simultaneously align their corresponding CAMs ${\bm{S}^{t, n}}$ with respect to \emph{color}. This is translated by \emph{explicitly} constraining the CAMs ${\bm{S}^{t, n}}$ to activate similarly on similar color pixels across all frames.
%
This is achieved by minimizing a color-only CRF loss~\cite{tang2018regularized} across all considered frames. We refer to the concatenation of frames in ${\bm{F}^{t, n}}$ as ${\bm{\bar{X}}}$. Similarly, we denote ${\bm{\bar{S}}}$ as the composite CAMs resulting from the concatenation of the corresponding CAMs ${\bm{M}^{t, n}}$ of the same frames ${\bm{X}^{t, n}}$. Concatenation can be done horizontally or vertically. Our temporal term loss is formulated as follows:
\begin{equation}
    \label{eq:crf_rgb}
    \mathcal{R}_c(\bm{F}^{t, n}, \bm{M}^{t, n}) = \sum_{r \in \{0, 1\}} \bm{\bar{S}}^{r\top} \; \bm{W} \; (\bm{1} - \bm{\bar{S}}^r) \;,
\end{equation}
where ${\bm{W}}$ is the \emph{color}-only similarity matrix between pixels of the concatenated image ${\bm{\bar{X}}}$.  
%
We note the main difference between CRF loss in Eq.\ref{eq:crf} which operates only on a single frame level, and accounts for both pairwise proximity and color similarity between pixels. On the other hand, our temporal CRF loss in Eq.\ref{eq:crf_rgb} operates on multiple frames, and acts only on pairwise color similarity across all frames without accounting for proximity between pixels or frames. This allows the object to be anywhere in the sequence or within a frame yielding more localization flexibility compared to TCAM method~\cite{tcamsbelharbi2023}.
%
In our implementation, we use a Gaussian kernel to capture color similarities~\cite{KrahenbuhlK11crf}. The kernel is implemented via the permutohedral lattice~\cite{AdamsBD10lattice} for fast computation. 
We refer to the Eq.\ref{eq:crf_rgb} as \emph{CoLoc} term. 

The loss in Eq.\ref{eq:crf_rgb} takes roughly ${\sim80}$ ms to process ${n=64}$ frames of size ${224\times224}$.  It is achieved via a hybrid implementation to speedup computation. The affinity matrix ${\bm{W}}$ is computed in parallel on CPU, while the matrix product is conducted over GPU. This yield a less computation overhead, and allows training in a practical time.

\subsection{Total Training Loss}
\label{subsec:total-loss}
Our final loss combines per-frame and multi-frame losses to be optimized. It is formulated as,
\begin{equation}
\label{eq:totalloss}
\begin{aligned}
\min_{\bm{\theta}} \quad & \sum_{p \in \Omega^{\prime}_t} \bm{H}_p(\bm{Y}_t, \bm{S}_t) + \lambda\; \mathcal{R}(\bm{S}_t, \bm{X}_t) + \mathcal{R}_s(\bm{S}_t)  \\
& +  \frac{\lambda_c}{\abs{[\mathcal{R}_c]}}\; \mathcal{R}_c(\bm{F}^{t, n}, \bm{M}^{t, n}) \;,\\
\end{aligned}
\end{equation}
where $\lambda$ and $\lambda_c$ are positive weighting coefficients. All terms are optimized simultaneously through SGD method.

We note that the magnitude\footnote{Typical magnitude of ${\mathcal{R}_c}$ is ${2*10^9}$ for ${n=2}$, and can grow to ${2*10^{11}}$ for ${n=18}$. As a result, adequate values of the non-adapative ${\lambda_c}$ should be below ${2*10^{-9}}$.} of the term ${\mathcal{R}_c}$ increases with the number of frames $n$. Large magnitudes can easily overpower other terms in Eq.\ref{eq:totalloss}, hindering learning. In practice, this makes tuning the hyper-parameter ${\lambda_c}$ critical and challenging. To reduce this strong dependency on $n$ and stabilize learning, we propose an \emph{adaptive} weight ${\lambda_c}$ that automatically \emph{scales down} this term by its magnitude ${\abs{[\mathcal{R}_c]}}$. This is inspired by the recently proposed adaptive weight decay~\cite{Ghiasi2022} which uses the weight norm and its gradient for adaptation.
%
In this context, the operation ${[\cdot]}$ indicates that this value is now a constant and does not depend on any optimization parameters, \ie, ${\bm{\theta}}$. As a result, this term is always constant ${\lambda_c \; \frac{\mathcal{R}_c}{\abs{[\mathcal{R}_c]}} = \pm \lambda_c}$. However, its derivative is non-zero ${\partial \left( \lambda_c \; \frac{\mathcal{R}_c}{\abs{[\mathcal{R}_c]}} \right) / \partial \bm{\theta} = \frac{\lambda_c}{\abs{[\mathcal{R}_c]}} \frac{\partial \mathcal{R}_c}{\partial \bm{\theta}}}$. In this adaptive setup, the value ${\lambda_c}$ simply amplifies the scaled gradient. Its practical values are ${\lambda_c > 1}$, which makes it easier to tune compared to its non-adaptive version, which will be demonstrated empirically later. Most importantly, the adaptive version creates less dependency on $n$.

After being trained using Eq. \ref{eq:totalloss}, our model is applied on single frames using a forward pass for inference (Fig.\ref{fig:proposal-details}). This allows for fast inference and parallel computation over a video stream. Using standard procedures~\cite{tcamsbelharbi2023,choe2020evaluating}, a localization bounding box is extracted from the foreground CAM of the decoder ${\bm{S}^1_t}$. Our method yields a single model with parameters ${\{\bm{\theta^{\prime}}, \bm{\theta}\}}$ performing both tasks, classification and localization, while handling multiple classes at once.


\begin{table*}
\begin{center}
\resizebox{\linewidth}{!}{
\begin{tabular}{|l||*{10}{c|}|g|c|}
\hline
\multicolumn{13}{|c|}{\textbf{\ytovone dataset}} \\
\hline
\textbf{Method {\small \emph{(venue, publication year)}}} & \textbf{Aero} & \textbf{Bird} & \textbf{Boat} & \textbf{Car} & \textbf{Cat} & \textbf{Cow} & \textbf{Dog} & \textbf{Horse} & \textbf{Mbike} & \textbf{Train} & \textbf{Avg} & \textbf{Time/Frame} \\
\hhline{----------||---}
% \noalign{\vspace{\doublerulesep}}
\hhline{----------||---}
Prest et al.~\cite{prest2012learning} {\small \emph{(cvpr, 2012)}} & 51.7 & 17.5 & 34.4 & 34.7 & 22.3 & 17.9 & 13.5 & 26.7 & 41.2 & 25.0 & 28.5 & N/A   \\
Papazoglou et al.~\cite{papazoglou2013fast} {\small \emph{(iccv, 2013)}} & 65.4 & 67.3 & 38.9 & 65.2 & 46.3 & 40.2 & 65.3 & 48.4 & 39.0 & 25.0 & 50.1 & 4s  \\
Joulin et al.~\cite{joulin2014} {\small \emph{(eccv, 2014)}} & 25.1 & 31.2 & 27.8 & 38.5 & 41.2 & 28.4 & 33.9 & 35.6 & 23.1 & 25.0 & 31.0 & N/A \\
Kwak et al.~\cite{Kwak2015} {\small \emph{(iccv, 2015)}}  & 56.5 & 66.4 & 58.0 & 76.8 & 39.9 & 69.3 & 50.4 & 56.3 & 53.0 & 31.0 & 55.7 & N/A  \\
Rochan et al.~\cite{RochanRBW16} {\small \emph{(ivc, 2016)}} & 60.8 & 54.6 & 34.7 & 57.4 & 19.2 & 42.1 & 35.8 & 30.4 & 11.7 & 11.4 & 35.8 & N/A \\
Tokmakov et al.~\cite{Tokmakov2016} {\small \emph{(eccv, 2016)}} & 71.5 & 74.0 & 44.8 & 72.3 & 52.0 & 46.4 & 71.9 & 54.6 & 45.9 & 32.1 & 56.6 & N/A  \\
POD~\cite{jun2016pod} {\small \emph{(cvpr, 2016)}} & 64.3 & 63.2 & 73.3 & 68.9 & 44.4 & 62.5 & 71.4 & 52.3 & 78.6 & 23.1 & 60.2 & N/A  \\
Tsai et al.~\cite{tsai2016semantic} {\small \emph{(eccv, 2016)}} & 66.1 & 59.8 & 63.1 & 72.5 & 54.0 & 64.9 & 66.2 & 50.6 & 39.3 & 42.5 & 57.9 & N/A  \\
Haller et al.~\cite{Halle2017} {\small \emph{(iccv, 2017)}} & 76.3 & 71.4 & 65.0 & 58.9 & 68.0 & 55.9 & 70.6 & 33.3 & 69.7 & 42.4 & 61.1 & 0.35s  \\
Croitoru et al.~\cite{Croitoru2019} (LR-N\textsubscript{iter1}) {\small \emph{(ijcv, 2019)}} & 77.0 & 67.5 & 77.2 & 68.4 & 54.5 & 68.3 & 72.0 & 56.7 & 44.1 & 34.9 & 62.1 & 0.02s  \\
Croitoru et al.~\cite{Croitoru2019} (LR-N\textsubscript{iter2}) {\small \emph{(ijcv, 2019)}} & 79.7 & 67.5 & 68.3 & 69.6 & 59.4 & 75.0 & 78.7 & 48.3 & 48.5 & 39.5 & 63.5 & 0.02s\\
Croitoru et al.~\cite{Croitoru2019} (DU-N\textsubscript{iter2}) {\small \emph{(ijcv, 2019)}} & 85.1 & 72.7 & 76.2 & 68.4 & 59.4 & 76.7 & 77.3 & 46.7 & 48.5 & 46.5 & 65.8 & 0.02s \\
Croitoru et al.~\cite{Croitoru2019} (MS-N\textsubscript{iter2}) {\small \emph{(ijcv, 2019)}} & 84.7 & 72.7 & 78.2 & 69.6 & 60.4 & 80.0 & 78.7 & 51.7 & 50.0 & 46.5 & 67.3 & 0.15s \\
SPFTN (M)~\cite{zhang2020spftn} {\small \emph{(tpami, 2020)}} & 66.4 & 73.8 & 63.3 & 83.4 & 54.5 & 58.9 & 61.3 & 45.4 & 55.5 & 30.1 & 59.3 & N/A  \\
SPFTN (P)~\cite{zhang2020spftn} {\small \emph{(tpami, 2020)}}& \textbf{97.3} & 27.8 & 81.1 & 65.1 & 56.6 & 72.5 & 59.5 & \textbf{81.8} & 79.4 & 22.1 & 64.3 & N/A  \\
FPPVOS~\cite{umer2021efficient} {\small \emph{(optik, 2021)}} & 77.0 & 72.3 & 64.7 & 67.4 & 79.2 & 58.3 & 74.7 & 45.2 & 80.4 & 42.6 & 65.8 & 0.29s  \\
% \hhline{--||---------||---}
\cline{1-13}
CAM~\cite{zhou2016learning} {\small \emph{(cvpr, 2016)}} & 75.0 & 55.5 & 43.2 & 69.7 & 33.3 & 52.4 & 32.4 & 74.2 & 14.8 & 50.0 & 50.1 & 0.2ms  \\
GradCAM~\cite{SelvarajuCDVPB17iccvgradcam} {\small \emph{(iccv, 2017)}} & 86.9 & 63.0 & 51.3 & 81.8 & 45.4 & 62.0 & 37.8 & 67.7 & 18.5 & 50.0 & 56.4 & 27.8ms  \\
GradCAM++~\cite{ChattopadhyaySH18wacvgradcampp} {\small \emph{(wacv,2018)}} & 79.8 & 85.1 & 37.8 & 81.8 & 75.7 & 52.4 & 64.9 & 64.5 & 33.3 & 56.2 & 63.2 & 28.0ms  \\
Smooth-GradCAM++~\cite{omeiza2019corr} {\small \emph{(corr, 2019)}} & 78.6 & 59.2 & 56.7 & 60.6 & 42.4 & 61.9 & 56.7 & 64.5 & 40.7 & 50.0 & 57.1 & 136.2ms  \\
XGradCAM~\cite{fu2020axiom} {\small \emph{(bmvc, 2020)}} & 79.8 & 70.4 & 54.0 & 87.8 & 33.3 & 52.4 & 37.8 & 64.5 & 37.0 & 50.0 & 56.7 & 14.2ms  \\
LayerCAM~\cite{JiangZHCW21layercam} {\small \emph{(ieee, 2021)}} & 85.7 & \textbf{88.9} & 45.9 & 78.8 & 75.5 & 61.9 & 64.9 & 64.5 & 33.3 & 56.2 & 65.6 & 17.9ms  \\
F-CAM~\cite{belharbi2022fcam} {\small \emph{(wacv, 2022)}} & 75.4 & 85.7 & 75.6 & 85.4 & 59.2 & 87.0 & 52.9 & 66.6 & 62.5 & 50.0 & 70.0 & 18.5ms  \\
CLIP-ES~\cite{lin2023} {\small \emph{(cvpr, 2023)}}  & 46.4 & 74.0 & 5.4 & \textbf{90.9} & \textbf{90.9} & 66.6 & 75.6 & 83.8 & 74.0 & 43.7 & 65.1 & 49.9ms  \\
TCAM~\cite{tcamsbelharbi2023} with Layer-CAM {\small \emph{(wacv, 2023)}} & 90.5 & 70.4 & 62.2 & 75.7 & 84.8 & \textbf{81.0} & 81.0 & 64.5 & 70.4 & 50.0 & 73.0 & 18.5ms  \\
CoLo-CAM with Layer-CAM (ours) & 90.4 & 74.0 & \textbf{91.8} & 87.8 & 78.7 & 80.9 & \textbf{89.1} & 74.1 & \textbf{85.1} & \textbf{68.7} & \textbf{82.1} & 18.5ms  \\
\hline
\hline
\multicolumn{13}{|c|}{\textbf{\ytovtwodtwo dataset}} \\
\hline
% \hline
\textbf{Method {\small \emph{(venue, publication year)}}} & \textbf{Aero} & \textbf{Bird} & \textbf{Boat} & \textbf{Car} & \textbf{Cat} & \textbf{Cow} & \textbf{Dog} & \textbf{Horse} & \textbf{Mbike} & \textbf{Train} & \textbf{Avg} & \textbf{Time/Frame} \\
\hhline{----------||---}
% \noalign{\vspace{1.5mm}}
\hhline{----------||---}
Haller et al.~\cite{Halle2017}  {\small \emph{(iccv, 2017)}}& 76.3 & 68.5 & 54.5 & 50.4 & 59.8 & 42.4 & 53.5 & 30.0 & 53.5 & \textbf{60.7} & 54.9 & 0.35s   \\
Croitoru et al.~\cite{Croitoru2019} (LR-N\textsubscript{iter1}) {\small \emph{(ijcv, 2019)}} & 75.7 & 56.0 & 52.7 & 57.3 & 46.9 & 57.0 & 48.9 & 44.0 & 27.2 & 56.2 & 52.2 & 0.02s    \\
Croitoru et al.~\cite{Croitoru2019} (LR-N\textsubscript{iter2}) {\small \emph{(ijcv, 2019)}} & 78.1 & 51.8 & 49.0 & 60.5 & 44.8 & 62.3 & 52.9 & 48.9 & 30.6 & 54.6 &  53.4 & 0.02s    \\
Croitoru et al.~\cite{Croitoru2019} (DU-N\textsubscript{iter2}){\small \emph{(ijcv, 2019)}} & 74.9 & 50.7 & 50.7 & 60.9 & 45.7 & 60.1 & 54.4 & 42.9 & 30.6 & 57.8 & 52.9 & 0.02s   \\
Croitoru et al.~\cite{Croitoru2019} (BU-N\textsubscript{iter2}){\small \emph{(ijcv, 2019)}} & 82.2 & 51.8 & 51.5 & 62.0 & 50.9 & 64.8 & 55.5 & 45.7 & 35.3 & 55.9 & 55.6 & 0.02s  \\
Croitoru et al.~\cite{Croitoru2019} (MS-N\textsubscript{iter2}){\small \emph{(ijcv, 2019)}} & 81.7 & 51.5 & 54.1 & 62.5 & 49.7 & 68.8 & 55.9 & 50.4 & 33.3 & 57.0 & 56.5 & 0.15s  \\
% \hhline{-----------||--}
\cline{1-13}
CAM~\cite{zhou2016learning} {\small \emph{(cvpr, 2016)}} & 52.3 & 66.4 & 25.0 & 66.4 & 39.7 & \textbf{87.8} & 34.7 & 53.6 & 45.4 & 43.7 & 51.5 & 0.2ms  \\
GradCAM~\cite{SelvarajuCDVPB17iccvgradcam} {\small \emph{(iccv, 2017)}} & 44.1 & 68.4 & 50.0 & 61.1 & 51.8 & 79.3 & 56.0 & 47.0 & 44.8 & 42.4 & 54.5 & 27.8ms  \\
GradCAM++~\cite{ChattopadhyaySH18wacvgradcampp} {\small \emph{(wacv, 2018)}} & 74.7 & 78.1 & 38.2 & 69.7 & 56.7 & 84.3 & 61.6 & 61.9 & 43.0 & 44.3 & 61.2 & 28.0ms  \\
Smooth-GradCAM++~\cite{omeiza2019corr} {\small \emph{(corr, 2019)}} & 74.1 & 83.2 & 38.2 & 64.2 & 49.6 & 82.1 & 57.3 & 52.0 & 51.1 & 42.4 & 59.5 & 136.2ms  \\
XGradCAM~\cite{fu2020axiom} {\small \emph{(bmvc, 2020)}} & 68.2 & 44.5 & 45.8 & 64.0 & 46.8 & 86.4 & 44.0 & 57.0 & 44.9 & 45.0 & 54.6 & 14.2ms  \\
LayerCAM~\cite{JiangZHCW21layercam} {\small \emph{(ieee, 2021)}} & 80.0 & 84.5 & 47.2 & \textbf{73.5} & 55.3 & 83.6 & 71.3 & 60.8 & 55.7 & 48.1 & 66.0 & 17.9ms  \\
F-CAM~\cite{belharbi2022fcam} {\small \emph{(wacv, 2022)}} & 77.1 & 89.2 & 73.1 & 81.8 & 51.8 & 80.6 & 47.0 & 62.9 & 62.5 & 50.0 & 67.6 & 18.5ms  \\
CLIP-ES~\cite{lin2023} {\small \emph{(cvpr, 2023)}}  & 51.7 & 78.7 & 11.8 & 63.9 & 75.8 & 78.5 & 77.4 & 64.6 & 54.5 & 45.5 & 60.2 & 49.9ms  \\
TCAM~\cite{tcamsbelharbi2023} with Layer-CAM {\small \emph{(wacv, 2023)}}  & 79.4 & \textbf{94.9} & 75.7 & 61.7 & 68.8 & 87.1 & 75.0 & 62.4 & 72.1 & 45.0 & 72.2 & 18.5ms  \\
CoLo-CAM with Layer-CAM (ours) & \textbf{82.9} & 92.2 & \textbf{85.4} & 67.7 & \textbf{80.1} & 85.7 & \textbf{79.2} & \textbf{67.4} & \textbf{72.7} & 58.2 & \textbf{77.1} & 18.5ms  \\
\hline
\end{tabular}}
\end{center}
\caption{\corloc localization accuracy (\%) per-class and average over classes (Avg) of CoLo-CAM and state-of-the-art methods on the \ytovone~\cite{prest2012learning} and \ytovtwodtwo~\cite{KalogeitonFS16} test sets when using the ResNet50 CNN backbone. The computation inference time is over a single image with a size of ${224\times224}$.}
\label{tab:yto}
\end{table*}
%%%%%%%%%%%
\subsection{Comparison with the State-of-the-Art}
\label{subsec:compare}





%%%%%%%%%%%%%%%% Exps
\section{Results and Discussion}
\label{sec:results}

\subsection{Experimental Methodology}
\label{subsec:exp-method}

\noindent \textbf{Datasets.} To evaluate our method, we conducted experiments on two standard public unconstrained video datasets for WSVOL. For training, each video is labeled globally via a class. Frame bounding boxes are provided to evaluate localization. Following the literature, we used two challenging public datasets from YouTube\footnote{\url{https://www.youtube.com}}:  YouTube-Object v1.0 (\ytovone~\cite{prest2012learning}) and YouTube-Object v2.2 (\ytovtwodtwo~\cite{KalogeitonFS16} ). In all our experiments, we followed the same protocol as described in~\cite{tcamsbelharbi2023,KalogeitonFS16,prest2012learning}.

\noindent\textit{YouTube-Object v1.0 (\ytovone)~\cite{prest2012learning}:} A dataset composed of videos collected from YouTube by querying the names of 10 classes. A class has between 9-24 videos with a duration ranging from 30 seconds to 3 minutes. It has 155 videos, each split into short-duration clips, \ie, shots. There are 5507 shots with 571,089 frames in all. Images resolution vary from a video to another. It can go from ${400\times 300}$ up to ${1280\times 720}$.
The dataset has been divided by the authors~\cite{prest2012learning} into 27 test videos for a total of 396 bounding boxes, and 128 videos for training. Some videos from the train set are considered for validation to perform early-stopping~\cite{tcamsbelharbi2023,KalogeitonFS16,prest2012learning}.  We sampled 5 random videos per class to build a validation set with a total of 50 videos. Frame bounding box annotations are provided for test set.

\noindent\textit{YouTube-Object v2.2 (\ytovtwodtwo)~\cite{KalogeitonFS16}:} This is a large dataset built on \ytovone dataset with large and challenging test set. It is composed of more frames, 722,040 in total. The dataset was divided by the authors into 106 videos for training, and 49 videos for test. Following~\cite{tcamsbelharbi2023}, we sampled 3 random videos per class from the train set to build a validation set. The test set has more samples compared to \ytovone. It is composed of a total of 2,667 bounding box making it a much more challenging dataset.

\noindent \textbf{Implementation Details.} 
For all our experiments, we trained for 10 epochs with a mini-batch size of 32. ResNet50~\cite{heZRS16} was used as a backbone. Further ablations are done with VGG16~\cite{SimonyanZ14a}, InceptionV3~\cite{SzegedyVISW16}. Unless mentioned differently, all results are obtained using ResNet50 backbone. Images are resized to ${256\times256}$, and randomly cropped patches of size ${224\times224}$ are used for training. The temporal dependency ${n}$ in Eq.\ref{eq:crf_rgb} is set through the validation set from ${n \in \{2, \cdots, 18\}}$. The hyper-parameter ${\lambda_c}$ in Eq.\ref{eq:totalloss} is set from ${\{1, \cdots, 16\}}$ with the adaptive setup. In Eq.\ref{eq:crf_rgb}, frames are concatenated horizontally to build ${\bm{\bar{X}}}$.
%
We set the CRF hyper-parameter $\lambda$ in Eq.\ref{eq:totalloss} to ${2e^{-9}}$ -- the same value as in~\cite{tang2018regularized}.  Its color and spatial kernel bandwidth are set to 15 and 100, respectively, following~\cite{tang2018regularized}. The same color bandwidth is used for Eq.\ref{eq:crf_rgb}. The initial value of the log-barrier coefficient, $z$ in Eq.\ref{eq:log-bar-sz}, is set to $1$ and then increases by a factor of ${1.01}$ in each epoch with a maximum value of $10$ as in~\cite{belharbi2019unimoconstraints,kervadec2019log}. The best learning rate was selected within ${\{0.1, 0.01, 0.001\}}$. The classifier $g$ was pre-trained on single frames on the train set. 

\noindent \textbf{Baseline Methods.} For comparison, publicly available results were considered. We compared our proposed approach with different WSVOL methods~\cite{Croitoru2019, Halle2017,joulin2014,Kwak2015,papazoglou2013fast,prest2012learning,RochanRBW16,Tokmakov2016,tsai2016semantic}, POD~\cite{jun2016pod}, SPFTN~\cite{zhang2020spftn}, and FPPVOS~\cite{umer2021efficient}. We also considered the performance of several CAM-based methods reported in~\cite{tcamsbelharbi2023}: CAM~\cite{zhou2016learning}, GradCAM~\cite{SelvarajuCDVPB17iccvgradcam}, GradCam++~\cite{ChattopadhyaySH18wacvgradcampp}, Smooth-GradCAM++~\cite{omeiza2019corr}, XGradCAM~\cite{fu2020axiom}, and LayerCAM~\cite{JiangZHCW21layercam}, which were all trained on single frames, \ie, no temporal dependency was considered. We compare our results as well to CLIP-ES~\cite{lin2023} using true class as input, and without background list or CAM-refinement for a fair comparison. The LayerCAM method~\cite{JiangZHCW21layercam} was employed to generate classifier CAMs ${\bm{C}_t}$ used to create pseudo-labels (${\bm{Y}_t}$) for our method (Eq.\ref{eq:pl}). Note that different CAM-based methods can be integrated with our approach.

\noindent \textbf{Evaluation Metric.} Localization accuracy was evaluated using the \corloc metric~\cite{tcamsbelharbi2023,deselaers2012weakly}. It describes the percentage of predicted bounding boxes that have an Intersection Over Union (IoU) between prediction and ground truth greater than half (IoU ${>50\%}$). In addition, the  inference time per frame of size ${224\times224}$ is reported over GPU (NVIDIA Tesla P100) and CPU (Intel(R) Xeon(R), 48 cores). The same hardware is used to report the computation time of our multi-frame loss (Eq.\ref{eq:crf_rgb}).



The results in Tab.\ref{tab:yto} show that our CoLo-CAM method outperforms other methods by a large margin, more so on \ytovone compared to \ytovtwodtwo. This highlights the difficulty of \ytovtwodtwo. In terms of per-class performance, our method is competitive in most classes. In particular, we observe that our method achieved a large improvement over the challenging class 'Train'. In general, our method and CAM methods are still behind on the class 'Horse' and 'Aero' compared to the SPFTN method~\cite{zhang2020spftn} which relies on optical flow to generate proposals. Both classes present different challenges. The former class shows with dense multi-instances, while the latter, \ie, 'Aero', appears in complex scenes (airport), often with very large size, and occlusion. In term of inference time, our method is relatively fast which is similar to TCAM~\cite{tcamsbelharbi2023} and F-CAM~\cite{belharbi2022fcam}.


{
%\setlength{\tabcolsep}{3pt}
%\renewcommand{\arraystretch}{1.1}
\begin{table}[ht!]
\centering
\resizebox{\linewidth}{!}{%
\centering
%\small
\begin{tabular}{|lllc|c|}
\hline
\multicolumn{3}{|l}{\textbf{Methods}}& &  \corloc (\%) \\
\hline \hline %\cline{1-3}\cline{5-5} \\
\multicolumn{3}{|l}{Layer-CAM~\cite{JiangZHCW21layercam} {\small \emph{(ieee)}}} &  &  65.6  \\
\hline %\cline{1-3}\cline{5-5} \\
% \cline{1-4} \\
\multirow{3}{*}{Single-frame} & &       
PLs          &  &   $68.5$       \\
&& PLs + CRF &  &   $69.6$       \\
&& PLs + ASC &  &   $66.2$      \\
&& PLs + ASC + CRF &  &   $70.0$\\ 
\hline 
%\cline{1-1}\cline{3-3} \cline{5-5} \\
Multi-frame & & PLs + ASC + CRF + CoLoc (Ours) &  &   \textbf{82.1} \\
\hline
%\cline{1-1}\cline{3-3} \cline{5-5} \\
\multicolumn{3}{|l}{Improvement} &  &  +$16.5$ \%\\ \hline 
%\cline{1-3}\cline{5-5} \\
\end{tabular}
}
\caption{Impact of different CoLo-CAM loss terms on \corloc localization accuracy. Results are reported on the \ytovone test set. All the cases use the same low resolution CAMs built from a pretrained Layer-CAM~\cite{JiangZHCW21layercam} model.}
\label{tab:ablation-parts}
\vspace{-1em}
\end{table}
}


{
%\setlength{\tabcolsep}{3pt}
%\renewcommand{\arraystretch}{1.1}
\begin{table}[ht!]
\centering
\resizebox{\linewidth}{!}{%
\centering
%\small
\begin{tabular}{|lllc|c|c|}
\hline
\multicolumn{3}{|l}{\textbf{Methods}}& &  \ytovone & \ytovtwodtwo  \\
\hline \hline %\cline{1-3}\cline{5-5} \\
% \cline{1-4} \\
\multirow{1}{*}{Single-frame} & &       
Layer-CAM~\cite{JiangZHCW21layercam}          &  &   $65.6$      & $66.0$ \\
&& F-CAM~\cite{belharbi2022fcam}              &  &   $70.0$      & $67.6$ \\
\hline 
\multirow{1}{*}{Multi-frame} & &       
TCAM~\cite{tcamsbelharbi2023} (F-CAM + temporal max-pooling)          &  &   $73.0$    & $72.2$   \\
&& CoLo-CAM (F-CAM + our temporal module)                             &  &   \bm{$82.1$}    & \bm{$77.1$}   \\
\hline
%\cline{1-3}\cline{5-5} \\
\end{tabular}
}
\caption{Comparison of F-CAM~\cite{belharbi2022fcam}, TCAM~\cite{tcamsbelharbi2023}, and our proposed Colo-CAM in term of \corloc localization accuracy on the \ytovone and \ytovtwodtwo test sets. All the methods use the same low resolution CAMs built from a pretrained Layer-CAM~\cite{JiangZHCW21layercam} model.}
\label{tab:tcam-fcam-vs-ours}
\vspace{-1em}
\end{table}
}




\begin{figure*}[ht!]
     \centering
     \begin{subfigure}[b]{0.4\textwidth}
         \centering
         \includegraphics[width=\textwidth]{ablation-n-YouTube-Objects-v1.0}
         \caption{}
         \label{fig:ablation-range-time}
     \end{subfigure}
    %\\
     \begin{subfigure}[b]{0.39\textwidth}
         \centering
         \includegraphics[width=\textwidth]{ablation-lambda-c-YouTube-Objects-v1.0}
         \caption{}
         \label{fig:ablation-lambda-c}
     \end{subfigure}
     \\
     \begin{subfigure}[b]{0.4\textwidth}
         \centering
         \includegraphics[width=\textwidth]{ablation-lambda-c-con-vs-ada-YouTube-Objects-v1.0}
         \caption{}
         \label{fig:ablation-lambda-c-const-vs-adap}
     \end{subfigure}
     \begin{subfigure}[b]{0.35\textwidth}
         \centering
         \includegraphics[width=\linewidth]{ablation-time-n-joint-crf-YouTube-Objects-v1.0}
         \caption{}
     \label{fig:ablation-time-joint-crf}
     \end{subfigure}
        \caption{Ablations on the \ytovone test set. 
        %
        \textbf{(a)} Impact of the number of frames ${n}$ over \corloc accuracy. 
        %
        \textbf{(b)} Impact of the \emph{adaptive} ${\lambda_c}$ over \corloc accuracy.
        %
        \textbf{(c)} \emph{Left y-axis, Orange}: \corloc accuracy on the \ytovone test set using constant and adaptive ${\lambda_c}$ weight. \emph{Right y-axis, Green}: ${\log{(\abs{\mathcal{R}_c})}}$ . The \emph{x-axis} is the number of frames $n$. (Better visualized in color)
        %
        \textbf{(d)} Computation time of our multi-frame loss term (Eq.\ref{eq:crf_rgb}) in function of number of frames $n$. 
        }
        \label{fig:ablations-lambda-c-n}
\end{figure*}


%%%%%%%%%%%%%%%
\subsection{Ablation Studies}

The following experiments provide insight on how localization performance is affected by loss terms, backbone, sampling strategy. Computation time of our multi-frame loss is reported as well.

\noindent\textbf{a) Impact of different loss terms (Tab.\ref{tab:ablation-parts}).} 
%
Without any spatiotemporal dependency, the use of pseudo-labels, CRF, and the absolute size constraint (ASC) helped to improve the localization performance (Tab.\ref{tab:ablation-parts}). This brought about the localization performance from ${65.6\%}$ to ${70.0\%}$. However, adding our multi-frame term, \ie CoLoc, increased the performance up to ${82.1\%}$ demonstrating its benefit. Furthermore, Tab.\ref{tab:tcam-fcam-vs-ours} provides a comparison between F-CAM~\cite{belharbi2022fcam}, TCAM~\cite{tcamsbelharbi2023}, and our method. Note that F-CAM, TCAM and our method use the same per-frame losses, same architecture, and same low resolution CAMs, estimated from Layer-CAM~\cite{JiangZHCW21layercam} pretrained model, for a fair comparison. These results confirm the benefit of our new temporal loss.

\noindent\textbf{b) Impact of ${n}$ on localization performance (Fig.\ref{fig:ablation-range-time}).} 
%
We observe that both methods, TCAM~\cite{tcamsbelharbi2023} and ours, provide better results when considering spatiotemporal information. However, TCAM reached its peak performance when using only ${n=2}$ frames. Large performance degradation is observed when increasing $n$. This comes as a result of assuming that objects have limited movement. Their proposed ROIs collector can mistakenly gather a true ROI in a distant frame, but, spatially, it is no longer an ROI but background in the current frame because the object has moved.
On the opposite, our method improves when increasing $n$ until it reaches its peak at ${n=4}$. Different from TCAM, our method showed more robustness and stability with the increase of $n$ since we only assume the object's visual similarity. When using large time dependency (${n=18}$), our method still maintains a high localization performance (${\sim80\%}$) demonstrating its robustness against long dependency.

\noindent\textbf{c) Constant vs adaptive ${\lambda_c}$ (Fig.\ref{fig:ablation-lambda-c-const-vs-adap}).}  
%
Note that in the constant setup, adequate ${\lambda_c}$ can be determined using a grid search or given a prior on the magnitude loss. Both approaches are tedious, especially when ideal values of ${\lambda_c}$ are below ${2*10^{-9}}$ creating a large search space. Exploring such space is computationally expensive. Using constant value provides good results up to 6 frames, while larger $n$ values lead to performance degradation, as adequate values are required (Fig.\ref{fig:ablation-lambda-c-const-vs-adap}). Our adaptive scheme is more intuitive and does not require prior knowledge. It achieved constantly better and more stable performance with less effort. Better results are obtained with ${\lambda_c}$ values between 3 and 7 (Fig.\ref{fig:ablation-lambda-c}). Performance degradation is observed when using large amplification, giving more importance to the ${\mathcal{R}_c}$ term, allowing it to outweigh other terms which hindered training. In contrast, using small amplification, \eg ${\mathcal{R}_c \in \{1, 2}\}$, gave a small push to this term, hence, less contribution.



\noindent\textbf{d) Computation time of our multi-frame loss (Eq. \ref{eq:crf_rgb}) with respect to ${n}$ (Fig.\ref{fig:ablation-time-joint-crf}).}
%
The computation of our multi-frame loss term in Eq. \ref{eq:crf_rgb} is split over CPU and GPU devices to perform hybrid computation. The left-side matrix product is performed on GPU, while the rest of the term is computed on CPU. Note that the transfer time from CPU to GPU is included in our time analysis. As shown in Fig.\ref{fig:ablation-time-joint-crf}, computation overhead in processing time grows linearly with respect to the number of frames ${n}$. However, this computational cost remains reasonable for training in a realistic time, even with a large $n$. For example, $n = 64$ frames can be processed in ${\sim 81}$ ms. Furthermore, full training of our method over \ytovone for 10 epochs with ${n=4}$ takes less than an hour (${~\sim45}$ mins).


\noindent\textbf{e) Effect of frame sampling strategy (Tab. \ref{tab:ablation-frame-sampling}).}
%
We have explored different strategies to randomly sample $n$ training frames from a video. We focus on \emph{frame diversity} to asses its impact on co-localization versus when frames are adjacent. We present three sampling techniques:

\noindent\emph{1) Adjacent scheme}: A first frame is uniformly sampled from the video. In addition, its previous ${n-1}$ frames are considered. This favors similar (less diverse) frames.

\noindent\emph{2) Interval scheme}: This is the opposite case of the adjacent scheme where the aim is to sample the most diverse frames. The video is divided into ${n}$ equal and disjoint intervals. Then, a single frame is uniformly sampled from each interval.

\noindent\emph{3) Gaussian scheme}: This is a middle-ground scheme which lies between adjacent and interval strategies. We sample $n$ random frames via a Gaussian distribution centered in the middle of a video.

All the sampling is done without repetition. Results in Tab.\ref{tab:ablation-frame-sampling} show that adjacent sampling performs the best, suggesting that our co-localization term is more beneficial when frames are relatively similar. This is consistent with the color assumption. Sampling diverse frames can lead to contrast change and/or major change in context.  Note that using diverse frames via the Gaussian or interval scheme still yields good performance. Unless mentioned otherwise, all our results are obtained via a adjacent scheme sampling.


{
%\setlength{\tabcolsep}{3pt}
%\renewcommand{\arraystretch}{1.1}
\begin{table}[ht!]
\centering
\resizebox{0.6\linewidth}{!}{%
\centering
%\small
\begin{tabular}{|lc|c|}
\hline
\multicolumn{1}{|l}{\textbf{Methods}}& &  \corloc accuracy (\%)  \\
\hline \hline %\cline{1-3}\cline{5-5} \\     
Adjacent     &  &   $\bm{82.1}$       \\
Gaussian        &  &   $80.2$       \\
Interval        &  &   $79.8$      \\
\hline 
\end{tabular}
}
\caption{Impact of frame sampling strategies on localization performance (\corloc) on \ytovone test set.}
\label{tab:ablation-frame-sampling}
\vspace{-1em}
\end{table}
}

{
%\setlength{\tabcolsep}{3pt}
%\renewcommand{\arraystretch}{1.1}
\begin{table}[ht!]
\centering
\resizebox{\linewidth}{!}{%
\centering
%\small
\begin{tabular}{|lc|ccc|}
\hline
\multirow{2}{*}{Cases / architectures} & &  \multicolumn{3}{c|}{\corloc accuracy (\%)}  \\
\cline{3-5}
& &  ResNet50 & VGG16 & Inception  \\
\hline
Layer-CAM~\cite{JiangZHCW21layercam}         &  &  65.6    & 65.2 & 65.0 \\
\hline 
Colo-CAM with Layer-CAM~\cite{JiangZHCW21layercam} (Ours) &  &  \textbf{82.1}    & \textbf{73.9} & \textbf{76.9} \\
\hline
\end{tabular}
}
\caption{Impact of the classifier backbone over localization performance (\corloc) on \ytovone test set.}
\label{tab:ablation-arch}
\vspace{-1em}
\end{table}
}

\noindent\textbf{f) Effect of the classifier backbone architecture (Tab. \ref{tab:ablation-arch}).}
%
We studied the effect of the backbone classifier for our method over localization performance. To this end, we used three different common backbones for WSOL task~\cite{choe2020evaluating}: ResNet50~\cite{heZRS16}, VGG16~\cite{SimonyanZ14a}, and InceptionV3~\cite{SzegedyVISW16}. From Tab.\ref{tab:ablation-arch}, we observe that in all cases, our method improves the baseline and still outperforms state-of-the-art of ${73.0\%}$. However, there is a discrepancy in performance between the three backbones. Such difference is common in WSOL tasks in natural scene images~\cite{choe2020evaluating,belharbi2022fcam} and medical images~\cite{rony2023deep} where ResNet50 performs better. Furthermore, even though these backbones have similar performance on the test set using the baseline method (Layer-CAM~\cite{JiangZHCW21layercam}), they do not necessarily have the same performance on the train set used to train our method. Such a suspected difference could also explain the discrepancy in test results. Note that the train sets do not have bounding box annotations, which prevents measuring the localization performance of each backbone.

{
%\setlength{\tabcolsep}{3pt}
%\renewcommand{\arraystretch}{1.1}
\begin{table}[ht!]
\centering
\resizebox{\linewidth}{!}{%
\centering
%\small
\begin{tabular}{|ccccc|cc|}
\hline
\multicolumn{4}{|c}{Configurations} && \multicolumn{2}{c|}{Datasets} \\
\hline
Initial  & List prior   & Image & Refined  & &  &  \\
CAM  & background &  label  & CAM & &  \ytovone & \ytovtwodtwo  \\
\hline 
\cmark & \xmark                & \labelgt & \xmark           & &  65.1     & 60.2  \\
\cmark & \cmark                & \labelgt & \xmark           & &  76.6     & 72.6  \\
\cmark & \cmark                & \labelgt & \cmark           & &  88.2     & 84.8  \\
\hline
\cmark & \xmark                & \labelpr & \xmark           & &  56.7     & 50.7  \\
\cmark & \cmark                & \labelpr & \xmark           & &  70.9     & 64.9  \\
\cmark & \cmark                & \labelpr & \cmark           & &  83.9     & 78.9  \\
\hline
\cmark & \xmark                & \labelavg & \xmark           & &  22.0     & 21.7  \\
\cmark & \cmark                & \labelavg & \xmark           & &  51.9     & 44.6  \\
\cmark & \cmark                & \labelavg & \cmark           & &  68.7     & 63.7  \\
\hline
%\cline{1-3}\cline{5-5} \\
% \cline{1-4} \\
\end{tabular}
}
\caption{\corloc localization accuracy of CLIP-ES~\cite{lin2023} method with different configurations on \ytovone and \ytovtwodtwo test sets. Labels: \labelgt: ground truth class label. \labelpr: class label predicted by a trained classifier over the corresponding dataset. \labelavg: average \corloc localization over all labels over a single sample.}
\label{tab:clip-es}
\vspace{-1em}
\end{table}
}



\subsection{Analysis of CLIP-ES~\cite{lin2023} Localization Performance}

In this section, the performance of CLIP-ES~\cite{lin2023} is analyzed under different settings for the WSVOL task. CLIP-ES is a prompt-based method trained on a large corpus of text and image datasets. This model takes the image as input, in addition to the class label (text) to localize the ROI of the concerned class. A major difference between this model and standard WSVOL/WSOL models is the requirement of the class label as \emph{input}. This means that all the computations performed by CLIP-ES are in function of the pair (image, class), while other competing methods use only the image as input. This gives CLIP-ES an advantage since it is aware of which class it looking for. While other methods only require the class at the end of the computations to simply select the concerned CAM. This particular aspect makes CLIP-ES impractical for relatively large number of classes. Each class requires a new forward in the model. Standard CAM-based methods perform a single\footnote{Top-down WSOL methods~\cite{rony2023deep} such as grad-based methods (\eg, Layer-CAM~\cite{JiangZHCW21layercam}) require a forward. In addition, a backward pass per class is performed. However, the backward is often computed only over the top layer.} forward yielding the CAMs of all classes at once. In addition, CLIP-ES leverages two major supporting elements giving it more advantage compared to other methods:
%
1) Background prior list: It contains a list of objects that are potential background that are used to suppress and clean the initial CAM. Example of background classes\footnote{We have used the default background list provided in CLIP-ES code: $\{$'ground', 'land', 'grass', 'tree', 'building', 'wall', 'sky', 'lake', 'water', 'river', 'sea', 'railway', 
'railroad', 'keyboard', 'helmet', 'cloud', 'house', 'mountain', 'ocean', 'road', 'rock', 'street', 'valley', 'bridge', 'sign'$\}.$}: 'ground', 'grass'. 
%
2) Refine CAM: After suppressing the background, CAM is then post-processed to improve its localization performance.

Note that all other methods do not have access to both options, making them in a disadvantage. For a fair comparison, we report the CLIP-ES performance using only the initial CAM. From Tab.\ref{tab:clip-es}, it is clear that a major improvement in localization comes from the background list and CAM refinement. Initial CAM has a performance of ${65.1\%, 60.2\%}$ over \ytovone, and \ytovtwodtwo, respectively. Using these two post-processing techniques allows for a jump in localization performance to ${88.2\%, 84.8\%}$ on both datasets. 

Applying CLIP-ES in WSVOL/WSOL tasks has a major issue in real-world applications regarding the class label since it is required as \emph{input}. Up to now, we used the true class provided as annotation (\labelgt). Since this model does not perform a classification task, we experiment with a scenario where a pre-trained classifier is used to predict the class in the image. Then, the predicted class is used as input to CLIP-ES. We report the performance in Tab.\ref{tab:clip-es} with the image label named \labelpr. The results show a clear drop in localization performance which is explained by the errors in the predicted classes. Note that we trained a classifier over each dataset using the ResNet50 backbone. We obtained ${72.5\%}$ of classification accuracy over \ytovone test set, and ${68.5\%}$ over \ytovtwodtwo test set. We extend this scenario to a poor classifier (uniform) by marginalizing localization performance over all possible classes (\labelavg). This is done by setting the input class as constant and averaging the localization performance over all the classes.



\begin{figure}[hpt!]
     \centering
     \begin{subfigure}[b]{0.5\textwidth}
         \centering
         \includegraphics[width=\textwidth]{tag}
     \end{subfigure}
     \\
     \begin{subfigure}[b]{0.5\textwidth}
         \centering
         \includegraphics[width=\textwidth]{1-dog_data_0001_shots_029_frame0001.jpg}
     \end{subfigure}
     \\
     \begin{subfigure}[b]{0.5\textwidth}
         \centering
         \includegraphics[width=\textwidth]{2-train_video-0008_shot-000015_00045305.jpg}
     \end{subfigure}
     \\
     \begin{subfigure}[b]{0.5\textwidth}
         \centering
         \includegraphics[width=\textwidth]{75-train_video-0008_shot-000013_00045196.jpg}
     \end{subfigure}
     \\
     \begin{subfigure}[b]{0.5\textwidth}
         \centering
         \includegraphics[width=\textwidth]{1317-train_video-0008_shot-000029_00048230.jpg}
     \end{subfigure}
      \\
     \begin{subfigure}[b]{0.5\textwidth}
         \centering
         \includegraphics[width=\textwidth]{120-boat_data_0005_shots_010_frame0001.jpg}
     \end{subfigure}
     \\
     \begin{subfigure}[b]{0.5\textwidth}
         \centering
         \includegraphics[width=\textwidth]{147-car_data_0002_shots_012_frame0001.jpg}
     \end{subfigure}
     \\
     \begin{subfigure}[b]{0.5\textwidth}
         \centering
         \includegraphics[width=\textwidth]{1049-motorbike_video-0011_shot-000398_00051051.jpg}
     \end{subfigure}
     \\
     \begin{subfigure}[b]{0.5\textwidth}
         \centering
         \includegraphics[width=\textwidth]{961-aeroplane_video-0002_shot-000012_00004864.jpg}
     \end{subfigure}
     \\
     \begin{subfigure}[b]{0.5\textwidth}
         \centering
         \includegraphics[width=\textwidth]{970-aeroplane_video-0002_shot-000030_00007901.jpg}
     \end{subfigure}
        \caption{Localization examples of test sets frames. 
        %
        % \emph{Left}: CoLo-CAM (ours). 
        % %
        % \emph{Middle}: TCAM~\cite{tcamsbelharbi2023}. 
        % %
        % \emph{Right}: baseline CAM method, LayerCAM~\cite{JiangZHCW21layercam}. 
        %
        \emph{Bounding boxes}: ground truth (green), prediction (red). The second column of each method is the predicted CAM over image.}
        \label{fig:visu-pred}
\end{figure}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Visual Results}
%\noindent \textbf{Visual Results (Fig.\ref{fig:visu-pred}).} 

Compared to TCAM~\cite{tcamsbelharbi2023} and LayerCAM~\cite{JiangZHCW21layercam}, we observe that our CAMs became more discriminative (Fig.\ref{fig:visu-pred}). This translates into several advantageous properties. Our CAMs showed sharp, more complete, and less noisy activations localized more precisely around objects. In addition, localizing small objects such as 'Bike', and large objects such as 'Train' becomes more accurate. CAM activations often suffer from co-occurrence issues~\cite {tcamsbelharbi2023,rony2023deep} where consistently appearing contextual objects (background) are confused with main foreground objects. Our CAMs showed more robustness to this issue. This can be seen in the case of road vs. 'Car'/'Bike', or water vs. 'Boat'.


\begin{figure}[]
     \centering
     \begin{subfigure}[b]{0.49\linewidth}
         \centering
         \includegraphics[width=\linewidth]{1424_car_video-0002_shot-000018_00003429.jpg}
     \end{subfigure}
     \begin{subfigure}[b]{0.49\linewidth}
         \centering
         \includegraphics[width=\linewidth]{1579_horse_video-0001_shot-000003_00000445.jpg}
     \end{subfigure}
     \\
     \begin{subfigure}[b]{0.49\linewidth}
         \centering
         \includegraphics[width=\linewidth]{1770_train_video-0008_shot-000022_00046634.jpg}
     \end{subfigure}
     \begin{subfigure}[b]{0.49\linewidth}
         \centering
         \includegraphics[width=\linewidth]{1776_train_video-0008_shot-000035_00051421.jpg}
     \end{subfigure}
        \caption{Typical challenges of our method. \emph{Top row}: Dense and overlapped instances lead to fused localization of instances. \emph{Bottom row}: Object mis-localization. Classifier CAM does not activate over the right object. Our trained decoder using these CAMs is unable to correct this large error. Bounding boxes: ground truth (green), prediction (red).}
        \label{fig:failure}
\end{figure}

Despite this improvement, our method still faces two main challenges (Fig.\ref{fig:failure}). The first is the case of samples with dense and overlapped instances (top Fig.\ref{fig:failure}). This often causes activations to spill across instances to form a large object hindering localization performance. The second issue concerns object mis-localization (bottom Fig.\ref{fig:failure}). Since our localization is mainly stimulated by discriminative cues from CAMs, it is still challenging to detect when a classifier points to the wrong object. This often leads to large errors that are difficult to correct in downstream use. Future work may consider leveraging the classifier response over ROIs to ensure their quality. Additionally, object movement information can be combined with CAM cues. 


Demonstrative videos provided with the code highlight additional challenges that are not visible in still images, in particular:  

\noindent\emph{1) Empty frames}. In unconstrained videos, the labeled object in the video is not necessarily present in the entire video. Therefore, some frames may not contain the object. Despite this, classifier CAMs still show class activations on these frames. This issue is the result of the weak global annotation of videos, and the assumption that all frames inherit that same label. It highlights the importance of detecting such frames and treating them accordingly, starting with classifier training. Discarding these frames will help the classifier better discern objects and reduce noisy CAMs. Without any additional supervision, this adds more difficulty to WSVOL task. One possible solution to this issue is to leverage the per-class probabilities of the classifier to assess whether an object is present or not in a frame. Such likelihood can be exploited to discard frames or weight loss terms over them.

\noindent\emph{2) Temporal consistency}. Although our method brought a quantitative and qualitative improvement, there are some failure cases. In some instances, we observed inconsistencies between consecutive frames. This error is characterized by a large shift in localization where the bounding box (and the CAM activation), moves abruptly from one location at a frame to completely different location at the next frame. This often happens when objects become partially or fully occluded, the appearance of other instances, the scene becomes complex, or zooming in the image makes the object too large. Such setups lead to a \emph{sudden shift} of the CAM's focus, leading to an abrupt change of localization which is undesirable. This drawback is mainly inherited from the \emph{inference} procedure performed at a single frame which does not account for spatiotemporal information allowing such errors. Future works may consider improving the inference procedure by leveraging spatiotemporal information at the expense of inference time.


\section{Conclusion}
\label{sec:conclusion}

This paper introduces a new CAM-based approach for WSVOL. Using pixel-color cue, our proposed CoLo-CAM method constrains CAM responses to be similar over visually similar pixels across all frames, assuming that an object maintains a similar color over adjacent frames. Multi-frame optimization is achieved by constructing a fully connected graph between all pairs of pixels across all the considered frames. CoLo-CAM relies on the pixel probability of CAMs as unary potentials, and the \emph{color} difference between every two pixels as the pairwise potentials. 
This allows for interconnection among all the pixels and performing explicit co-localization among frames.
Our total training loss comprises per-frame and multi-frame terms optimized simultaneously via standard Stochastic Gradient Descent (SGD).
%
Empirical experiments showed that our produced CAMs become more discriminative. This translates into several advantageous properties: sharp, more complete, and less noisy activations localized more precisely around objects. Moreover, localizing small/large objects becomes more accurate. Additionally, our method is more robust to training with long time-dependency and leads to new state-of-the-art localization performance.



\section*{Acknowledgment}
This research was supported in part by the Canadian Institutes of Health Research, the Natural Sciences and Engineering Research Council of Canada, and the Digital Research Alliance of Canada (alliancecan.ca).

\clearpage
\newpage

\appendices


% ===================================================================================================
% 
%
%                                          SUPP-MATERIAL
% 
% 
% ===================================================================================================

\section{Proposed Approach: Learning with Pixel Pseudo-Labels}
\label{sec:method-supp-mat}

In this section, we provide additional details on sampling PLs through the classifier's CAM ${\bm{C}_t}$. These sampled pseudo-labels are used to train the decoder $f$, in combination with other terms. Through the discriminative property of the pseudo-labels, they initiate and stimulate object localization. In CAM-based methods for WSOL, it is common to assume that CAMs' strong activations are likely to point to the foreground, whereas low activations hint at background regions~\cite{durand2017wildcat,zhou2016learning}.

In the following, we denote the foreground region as ${\mathbb{C}^+_t}$ which is computed by the operation ${\mathcal{O}^+}$. For simplicity and without adding new hyper-parameters, ${\mathbb{C}^+_t}$ is defined as the set of pixels with CAM activation ${\bm{C}_t}$ greater than a threshold estimated automatically by Otsu method~\cite{otsuthresh} over ${\bm{C}_t}$. The background region, \ie, ${\mathbb{C}^-_t}$, is the remaining set of pixels. Both regions are defined as follows,
\begin{equation}
    \label{eq:sets}
    \mathbb{C}^+_t = \mathcal{O}^+(\bm{C}_t), \quad \mathbb{C}^-_t = \Omega - \mathbb{C}^+_t \;, % \mathcal{O}^-(\bm{C}_t) \;.
\end{equation}
where ${\Omega}$ is a discrete image domain. 
Given the weak supervision, these regions are uncertain. For instance, the ${\mathbb{C}^+_t}$ region may hold only a small part of the foreground, along with the background parts. Consequently, the background region ${\mathbb{C}^-_t}$ may still have foreground parts. This uncertainty in partitioning the image into foreground and background makes it unreliable for localization to directly fit the model to entire regions, as it is commonly done~\cite{KolesnikovL16}. Instead, we pursue a stochastic sampling strategy in both regions to avoid overfitting and to provide the model enough time for the emergence of consistent regions. This has shown to be a better strategy~\cite{negevsbelharbi2022,belharbi2022fcam}. For each frame, and an SGD step, we sample a pixel from ${\mathbb{C}^+_t}$ as the foreground, and a pixel from ${\mathbb{C}^-_t}$ as background to be pseudo-labels. We encode their location according to:
\begin{equation}
    \label{eq:sset}
    \Omega^{\prime}_t = \mathcal{M}(\mathbb{C}^+_t)\; \cup \; \mathcal{U}(\mathbb{C}^-_t) \;, 
\end{equation}
where ${\mathcal{M}(\mathbb{C}^+_t)}$ is a multinomial sampling distribution over the foreground, and ${\mathcal{U}(\mathbb{C}^-_t)}$ is a uniform distribution over the background. This choice of distributions is based on the assumption that the foreground object is often concentrated in one place, whereas the background region is spread across the image. The partially pseudo-labeled mask for the sample ${\bm{X}_t}$ is referred to as ${Y_t}$, where ${Y_t(p) \in \{0, 1\}^2}$ is the binary label of the location ${p}$ with value ${0}$ for the background and ${1}$ for the foreground. Undefined regions are labeled as unknown. The pseudo-annotation mask ${Y_t}$ is then leveraged to train the decoder using partial cross-entropy. Such stochastic pseudo-labels are expected to stimulate the learning of the model and guide it to generalize and respond in the same way over regions with similar color/texture.

%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Additional Visual Results}
\label{sec:results-supp-mat}

Fig.\ref{fig:visu-pred-supp-mat} displays more visual results. Images show that our method yields sharp and less noisy CAMs. However, multi-instances and complex scenes remain a challenge.




\begin{figure*}
     \centering
     \begin{subfigure}[b]{0.35\textwidth}
         \centering
         \includegraphics[width=\textwidth]{tag}
     \end{subfigure}
     \hspace{6mm}
     \begin{subfigure}[b]{0.35\textwidth}
         \centering
         \includegraphics[width=\textwidth]{tag}
     \end{subfigure}
     \\
     \begin{subfigure}[b]{0.35\textwidth} 
 \centering 
 \includegraphics[width=\textwidth]{102-train_video-0003_shot-000006_00022239.jpg.png} 
  \end{subfigure} 
 \hspace{6mm}
\begin{subfigure}[b]{0.35\textwidth} 
 \centering 
 \includegraphics[width=\textwidth]{79-aeroplane_video-0009_shot-000160_00046106.jpg.png} 
  \end{subfigure} 
 \\
\begin{subfigure}[b]{0.35\textwidth} 
 \centering 
 \includegraphics[width=\textwidth]{1105-motorbike_video-0003_shot-000115_00010559.jpg.png} 
  \end{subfigure} 
 \hspace{6mm}
\begin{subfigure}[b]{0.35\textwidth} 
 \centering 
 \includegraphics[width=\textwidth]{38-car_video-0005_shot-000037_00007617.jpg.png} 
  \end{subfigure} 
 \\
\begin{subfigure}[b]{0.35\textwidth} 
 \centering 
 \includegraphics[width=\textwidth]{19-aeroplane_video-0002_shot-000005_00003421.jpg.png} 
  \end{subfigure} 
 \hspace{6mm}
\begin{subfigure}[b]{0.35\textwidth} 
 \centering 
 \includegraphics[width=\textwidth]{127-dog_data_0001_shots_020_frame0181.jpg.png} 
  \end{subfigure} 
 \\
\begin{subfigure}[b]{0.35\textwidth} 
 \centering 
 \includegraphics[width=\textwidth]{45-boat_data_0005_shots_075_frame0001.jpg.png} 
  \end{subfigure} 
\hspace{6mm}
\begin{subfigure}[b]{0.35\textwidth} 
 \centering 
 \includegraphics[width=\textwidth]{1243-train_video-0014_shot-000271_00116963.jpg.png} 
  \end{subfigure} 
 \\
\begin{subfigure}[b]{0.35\textwidth} 
 \centering 
 \includegraphics[width=\textwidth]{268-train_video-0008_shot-000033_00050464.jpg.png} 
  \end{subfigure} 
 \hspace{6mm}
\begin{subfigure}[b]{0.35\textwidth} 
 \centering 
 \includegraphics[width=\textwidth]{93-boat_video-0005_shot-000020_00036648.jpg.png} 
  \end{subfigure} 
 \\
\begin{subfigure}[b]{0.35\textwidth} 
 \centering 
 \includegraphics[width=\textwidth]{30-horse_data_0004_shots_031_frame0012.jpg.png} 
  \end{subfigure} 
 \hspace{6mm}
\begin{subfigure}[b]{0.35\textwidth} 
 \centering 
 \includegraphics[width=\textwidth]{232-horse_video-0001_shot-000012_00001811.jpg.png} 
  \end{subfigure} 
 \\
\begin{subfigure}[b]{0.35\textwidth} 
 \centering 
 \includegraphics[width=\textwidth]{641-boat_video-0011_shot-000125_00074943.jpg.png} 
  \end{subfigure} 
 \hspace{6mm}
\begin{subfigure}[b]{0.35\textwidth} 
 \centering 
 \includegraphics[width=\textwidth]{240-motorbike_data_0002_shots_020_frame0041.jpg.png} 
  \end{subfigure} 
 \\
\begin{subfigure}[b]{0.35\textwidth} 
 \centering 
 \includegraphics[width=\textwidth]{1307-motorbike_video-0010_shot-000373_00045134.jpg.png} 
  \end{subfigure} 
 \hspace{6mm}
\begin{subfigure}[b]{0.35\textwidth} 
 \centering 
 \includegraphics[width=\textwidth]{134-boat_data_0005_shots_006_frame0001.jpg.png} 
  \end{subfigure} 
 \\
\begin{subfigure}[b]{0.35\textwidth} 
 \centering 
 \includegraphics[width=\textwidth]{14-car_video-0005_shot-000037_00007250.jpg.png} 
  \end{subfigure} 
 \hspace{6mm}
\begin{subfigure}[b]{0.35\textwidth} 
 \centering 
 \includegraphics[width=\textwidth]{1313-motorbike_video-0011_shot-000409_00052054.jpg.png} 
  \end{subfigure} 
 \\
\begin{subfigure}[b]{0.35\textwidth} 
 \centering 
 \includegraphics[width=\textwidth]{95-motorbike_data_0007_shots_011_frame0034.jpg.png} 
  \end{subfigure} 
 \hspace{6mm}
\begin{subfigure}[b]{0.35\textwidth} 
 \centering 
 \includegraphics[width=\textwidth]{142-motorbike_video-0003_shot-000105_00009865.jpg.png} 
  \end{subfigure}
  \\
  \begin{subfigure}[b]{0.35\textwidth} 
 \centering 
 \includegraphics[width=\textwidth]{128-car_data_0008_shots_007_frame0001.jpg.png} 
  \end{subfigure} 
 \hspace{6mm}
\begin{subfigure}[b]{0.35\textwidth} 
 \centering 
 \includegraphics[width=\textwidth]{884-motorbike_video-0003_shot-000056_00006519.jpg.png} 
  \end{subfigure} 
 \\
\begin{subfigure}[b]{0.35\textwidth} 
 \centering 
 \includegraphics[width=\textwidth]{767-train_video-0008_shot-000047_00054208.jpg.png} 
  \end{subfigure} 
 \hspace{6mm}
\begin{subfigure}[b]{0.35\textwidth} 
 \centering 
 \includegraphics[width=\textwidth]{239-boat_data_0005_shots_005_frame0001.jpg.png} 
  \end{subfigure} 
 \\
\begin{subfigure}[b]{0.35\textwidth} 
 \centering 
 \includegraphics[width=\textwidth]{673-train_video-0008_shot-000053_00055950.jpg.png} 
  \end{subfigure} 
 \hspace{6mm}
\begin{subfigure}[b]{0.35\textwidth} 
 \centering 
 \includegraphics[width=\textwidth]{191-boat_data_0005_shots_017_frame0001.jpg.png} 
  \end{subfigure} 
 \\
\begin{subfigure}[b]{0.35\textwidth} 
 \centering 
 \includegraphics[width=\textwidth]{1306-train_video-0008_shot-000038_00052298.jpg.png} 
  \end{subfigure} 
\hspace{6mm}
\begin{subfigure}[b]{0.35\textwidth} 
 \centering 
 \includegraphics[width=\textwidth]{896-train_video-0008_shot-000052_00055675.jpg.png} 
  \end{subfigure} 
 \\
\begin{subfigure}[b]{0.35\textwidth} 
 \centering 
 \includegraphics[width=\textwidth]{142-boat_data_0005_shots_078_frame0001.jpg.png} 
  \end{subfigure} 
 \hspace{6mm}
\begin{subfigure}[b]{0.35\textwidth} 
 \centering 
 \includegraphics[width=\textwidth]{230-car_video-0002_shot-000018_00003388.jpg.png} 
  \end{subfigure} 
 \\
\begin{subfigure}[b]{0.35\textwidth} 
 \centering 
 \includegraphics[width=\textwidth]{954-cat_video-0007_shot-000045_00018934.jpg.png} 
  \end{subfigure} 
 \hspace{6mm}
\begin{subfigure}[b]{0.35\textwidth} 
 \centering 
 \includegraphics[width=\textwidth]{1299-cat_video-0004_shot-000025_00011652.jpg.png} 
  \end{subfigure} 
 \\
\begin{subfigure}[b]{0.35\textwidth} 
 \centering 
 \includegraphics[width=\textwidth]{318-train_data_0012_shots_001_frame0200.jpg.png} 
  \end{subfigure} 
 \hspace{6mm}
\begin{subfigure}[b]{0.35\textwidth} 
 \centering 
 \includegraphics[width=\textwidth]{1166-motorbike_video-0011_shot-000394_00050313.jpg.png} 
  \end{subfigure} 
 \\
\begin{subfigure}[b]{0.35\textwidth} 
 \centering 
 \includegraphics[width=\textwidth]{750-train_video-0003_shot-000006_00021988.jpg.png} 
  \end{subfigure} 
\hspace{6mm}
\begin{subfigure}[b]{0.35\textwidth} 
 \centering 
 \includegraphics[width=\textwidth]{989-car_video-0005_shot-000037_00007494.jpg.png} 
  \end{subfigure} 
 \\
\begin{subfigure}[b]{0.35\textwidth} 
 \centering 
 \includegraphics[width=\textwidth]{45-motorbike_video-0011_shot-000408_00051850.jpg.png} 
  \end{subfigure} 
 \hspace{6mm}
\begin{subfigure}[b]{0.35\textwidth} 
 \centering 
 \includegraphics[width=\textwidth]{133-cow_data_0004_shots_006_frame0001.jpg.png} 
  \end{subfigure} 
 \\
\begin{subfigure}[b]{0.35\textwidth} 
 \centering 
 \includegraphics[width=\textwidth]{624-dog_video-0013_shot-000129_00056944.jpg.png} 
  \end{subfigure} 
 \hspace{6mm}
\begin{subfigure}[b]{0.35\textwidth} 
 \centering 
 \includegraphics[width=\textwidth]{1190-motorbike_video-0003_shot-000056_00006492.jpg.png} 
  \end{subfigure} 
        \caption{Additional localization examples of test sets frames (\ytovone, \ytovtwodtwo). 
        %
        % \emph{Left}: CoLo-CAM (ours). 
        % %
        % \emph{Middle}: TCAM~\cite{tcamsbelharbi2023}. 
        % %
        % \emph{Right}: baseline CAM method, LayerCAM~\cite{JiangZHCW21layercam}. 
        %
        \emph{Bounding boxes}: ground truth (green), prediction (red). The second column of each method is the predicted CAM on the corresponding image.}
        \label{fig:visu-pred-supp-mat}
\end{figure*}

% \clearpage
% \newpage

\FloatBarrier

\bibliographystyle{abbrv}
\bibliography{main}

\end{document}
