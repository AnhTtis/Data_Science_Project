\pdfoutput = 1
% Credit: https://arxiv.org/abs/1708.07459
% https://arxiv.org/abs/1611.01838
% -----------------------------------------------------------------------
% --------------------------------- PREAMBLE ----------------------------
% -----------------------------------------------------------------------
% scrartcl
\documentclass[twocolumn]{article}
\input{macros}
\usepackage{style}

\newcommand\cl{\texttt{CL}\xspace}
\newcommand\corloc{\texttt{CorLoc}\xspace}
\newcommand\resnetfifty{\texttt{ResNet50}\xspace}
\newcommand\youtubeobjects{\texttt{YouTube-Objects}\xspace}
\newcommand\ytovone{\texttt{YTOv1}\xspace}
\newcommand\ytovtwodtwo{\texttt{YTOv2.2}\xspace}



% \pagestyle{empty}


% save space
% \setlength{\floatsep}{0.05in}
% \setlength{\textfloatsep}{0.05in}
% \setlength{\intextsep}{0.05in}
% \setlength{\belowcaptionskip}{0.05in}
% \setlength{\abovecaptionskip}{0.05in}
% \setlength{\abovedisplayskip}{0.05in}
% \setlength{\belowdisplayskip}{0.05in}


\makeatletter
\@namedef{ver@everyshi.sty}{}
\newcommand{\removelatexerror}{\let\@latex@error\@gobble}

% notation goes here
\newcommand\red[1]{{\color{red}#1}}
\newcommand{\entropysgd}{\mathrm{Entropy}\textrm{-}\mathrm{SGD}}
\newcommand{\entropyadam}{\mathrm{Entropy}\textrm{-}\mathrm{Adam}}
\newcommand{\minibatch}[1]{\Xi^{#1}}
\newcommand{\mnistfc}{\textrm{mnistfc}}
\newcommand{\smallmnistfc}{\textrm{small-mnistfc}}
\newcommand{\charlstm}{\textrm{char-LSTM}}
\newcommand{\ptblstm}{\textrm{PTB-LSTM}}
\newcommand{\lenet}{\textrm{LeNet}}
\newcommand{\allcnn}{\textrm{All-CNN-BN}}




\title{CoLo-CAM: Class Activation Mapping for Object Co-Localization in \\ Weakly-Labeled Unconstrained Videos}



\renewcommand\footnotemark{}
% \renewcommand\footnotemark{}

\author{Soufiane~Belharbi$^{1}$,
  ~Shakeeb Murtaza$^{1}$,
  ~Marco Pedersoli$^{1}$,
  ~Ismail~Ben~Ayed$^{1}$,
  ~Luke~McCaffrey$^{2}$, and
  ~Eric~Granger$^{1}$\\
 %	$^1$\'ETS Montreal\\
 	$^1$ LIVIA, Dept. of Systems Engineering, Ã‰TS, Montreal, Canada \\
	$^2$ Goodman Cancer Research Centre, Dept. of Oncology, McGill University, Montreal, Canada\\
{\tt\footnotesize \textcolor{black}{soufiane.belharbi.1@ens.etsmtl.ca} }
}

% \setlength{\marginparwidth}{1in}



\newcommand{\nt}[2]{
{\color{ForestGreen}#1}\marginpar{\tiny\noindent{\raggedright{\color{Sienna}[NOTE]} \color{Sienna}{#2} \par}}
}
\renewcommand{\ss}[2]{{\color{cyan}#1}\marginpar{\tiny\noindent{\raggedright{\color{BurntOrange}[SS]}\color{BurntOrange}{#2} \par}}}
\newcommand{\ac}[2]{{\color{magenta}#1}\marginpar{\tiny\noindent{\raggedright{\color{magenta}[AC]}\color{magenta}{#2} \par}}}

\newcommand{\status}[2]{{\color{red}#1}\marginpar{\tiny\noindent{\raggedright{\color{red}#2}}}}

\newcommand{\todoit}[1]{{\color{gray}#1}\marginpar{\tiny\noindent{\raggedright{\color{blue}[TODO]}}}}
\newcommand{\fix}[2]{{\color{blue}#1}\marginpar{\tiny\noindent{\raggedright{\color{blue}[FIX]}\color{blue}{ #2} \par}}}
\newcommand{\ignore}[1]{}
\newcommand{\mref}{{\color{BurntOrange}\&ref\& \xspace}}
\newcommand{\sota}{state-of-the-art\xspace}


% uncomment these for submission!
% \renewcommand{\pc}[2]{#1}
% \renewcommand{\ss}[2]{#1}
% \renewcommand{\ac}[2]{#1}
% \renewcommand{\todoit}[1]{#1}
% \renewcommand{\fix}[1]{#1}


% -----------------------------------------------------------------------
% --------------------------------- BODY --------------------------------
% -----------------------------------------------------------------------


\begin{document}
\maketitle\thispagestyle{fancy}


\maketitle
% \lhead{\color{gray} \small \today}
\rhead{\color{gray} \small Belharbi et al. \;  [Under review 2023]}


\begin{abstract}
Weakly-supervised video object localization (WSVOL) methods often rely on visual and motion cues only, making them susceptible to inaccurate localization.
Recently, discriminative models via a temporal class activation mapping (CAM) method have been explored.  Although results are promising, objects are assumed to have minimal movement leading to degradation in performance for relatively long-term dependencies. 
%
In this paper, a novel CoLo-CAM method for object localization is proposed to leverage spatiotemporal information in activation maps without any assumptions about object movement. Over a given sequence of frames, explicit joint learning of localization is produced across these maps based on color cues, by assuming an object has similar color across frames. The CAMs' activations are constrained to activate similarly over pixels with similar colors, achieving co-localization. This joint learning creates direct communication among pixels across all image locations, and over all frames, allowing for transfer, aggregation, and correction of learned localization. This is achieved by minimizing a color term of a CRF loss over joint images/maps. In addition to our multi-frame constraint, we impose per-frame local constraints including pseudo-labels, and CRF loss in combination with a global size constraint to improve per-frame localization.
%
Empirical experiments\footnote{Code: \href{https://github.com/sbelharbi/colo-cam}{https://github.com/sbelharbi/colo-cam}.} on two challenging datasets for unconstrained videos, YouTube-Objects, show the merits of our method, and its robustness to long-term dependencies, leading to new state-of-the-art localization performance.
\end{abstract}

\textbf{Keywords:} Convolutional Neural Networks, Weakly-Supervised Video Object Localization, Unconstrained Videos, Class Activation Maps (CAMs).
% ===================================================================================================
%
%                                      INTRODUCTION
%
% ===================================================================================================


\begin{figure}[ht!]
\centering
  \centering
  \includegraphics[width=.8\linewidth]{proposal}
  \caption[Caption]{Illustration of the multi-frame training using our CoLo-CAM method with $n=3$ frames. Each pixel (dot), is connected (orange line) to every pixel across the 3 frames to measure color similarity (connection thickness indicates similarity strength). CAM locations at pixels with similar colors are constrained to have similar activations (green lines are  for alignment). 
  For simplicity, per-frame terms are not visualized, and only a few pixel interconnections are shown. Notation and per-frame and multi-frame terms are described in Section \ref{sec:method}.}
  \label{fig:proposal}
\end{figure}


%%%%%%%%% Intro
\section{Introduction} \label{sec:intro}

The recent progress of online multi-media platforms such as YouTube has provided easy access to large amounts of videos~\citep{ShaoCSJXYZX22,tang2013discriminative}. Consequently, it is important to design techniques for automated video analysis. In this work, we focus on the task of object localization in videos, which plays a critical role in the understanding of video content, and the improvement of downstream tasks like video summarization~\citep{ZhangHJYC17}, event detection~\citep{ChangYLZH16}, video object detection~\citep{Chen0HW20,HanWCQ20,ShaoCSJXYZX22}, facial expression recognition~\citep{xue22,Zhang22}, and visual object tracking~\citep{BergmannML19,LuoXMZLK21}. Unfortunately, the annotations required for fully supervised localization, \ie, bounding boxes for each frame, come at a high cost for videos because of the large number of frames.  As an alternative, weak supervision using a video tag\footnote{An object class label, \ie, video tag, is defined as weak annotation associated with the entire video.} is currently the most common annotation for weakly-supervised object localization in videos~\citep{JerripothulaCY16,tsai2016semantic} and still images~\citep{choe2020evaluating,rony2023deep}.  Video tags often describe the main object appearing in the video without spatiotemporal information. However, individual frames do not necessarily contain the labeled class object, leading to uncertain annotations at the frame level. Unconstrained videos are often captured in the wild, with varying quality, moving objects and cameras, changing viewpoints and illumination, and decoding artifacts, and localizing objects in such videos is a challenging task. 
%
The weakly-supervised video object localization task (WSVOL)~\citep{joulin2014,jun2016pod,Kwak2015,prest2012learning,RochanRBW16,zhang2020spftn} aims to localize objects spatially and temporally by predicting a bounding box for each frame, using weak supervision such as video tags. Other techniques perform weakly supervised object segmentation, then produce a bounding box via post-processing~\citep{tcamsbelharbi2023,Croitoru2019,FuXZL14,Halle2017,LiuTSRCB14,tsai2016semantic,Tokmakov2016,umer2021efficient,YanXCC17,ZhangJS14}.



State-of-art WSVOL methods rely often all follow the same strategy. Initial segments/proposals are generated based on visual or motion cues, which are then used to identify and refine object locations through post-processing~\citep{HartmannGHTKMVERS12,Kwak2015,prest2012learning,tang2013discriminative,Tokmakov2016,XuXC12,YanXCC17,zhang2020spftn}. This process is optimized under visual appearance and motion constraints to ensure consistency. Other methods rely on co-localization/co-segmentation across videos/images via graphs that allow for a larger view and interactions between segments~\citep{ChenCC12,FuXZL14,JerripothulaCY16,joulin2014,tsai2016semantic,ZhangJS14}.
%
Despite their success, these methods have several limitations. Relying on multi-sequential training stages that are not end-to-end, makes them vulnerable to sub-optimal solutions. Often, localization is formulated as a solution to an optimization problem over one or more videos of the same class, and therefore requires a per-class/per-video model. This makes the inference time, and deployment expensive and impractical for real-world applications, in addition to a challenging scaling to large numbers of classes. Moreover, these methods start with initial proposals estimated in an unsupervised fashion (without the explicit use of video tags), and by relying on visual and motion cues such as optical flow~\citep{LeeKG11,SundaramBK10}. WSVOL methods are therefore vulnerable to inaccurate localization, especially with unconstrained videos, when motion cues are less reliable due to the movement of objects and cameras. 


Recently, a discriminative multi-class DL model for WSVOL has been proposed~\citep{tcamsbelharbi2023}. The authors consider using Class Activation Map (CAM)-based methods, which have been successful for weakly-supervised object localization (WSOL) on still images~\citep{choe2020evaluating,rony2023deep}. Using only global image class labels, a CAM-based method allows training a DL model to classify an image, and localize the corresponding object via a CAM. Image regions corresponding to strong CAM activations indicate the potential presence of an object making it suitable for object localization with low-cost annotations~\citep{oquab2015object}. However, these methods are not equipped to explicitly leverage temporal information in videos to improve localization. Therefore, the authors in~\citep{tcamsbelharbi2023} propose a temporal CAM-based method that performs spatiotemporal max-pooling to aggregate regions of interest (ROIs) across multiple CAMs from consecutive frames. Although this method can effectively exploit the  short-term spatiotemporal dependencies in videos, it is vulnerable to the aggregation of inaccurate activations due to object movement. This issue is illustrated in their results with relatively long-term temporal dependencies (greater than frames), where the localization accuracy decreases rapidly. 

 

To alleviate this issue while still benefiting from CAM methods, we introduce the CoLo-CAM method to leverage spatiotemporal information. In particular, a training framework with explicit joint learning of the CAM across multiple frames is considered. Using a \emph{color} cue, we constrain CAMs extracted from a sequence of frames to be consistent by pushing them to activate similarly over pixels with a similar color. We rely on the assumption that objects in nearby frames appear similar, which is a fair assumption in videos. Unlike~\citep{tcamsbelharbi2023}, we do not assume a minimal displacement of objects. Rather, they are allowed to be anywhere in the image, making our method more flexible. Our multi-frame optimization method is achieved via a \emph{color} term of a CRF loss~\citep{tang2018regularized} applied simultaneously over all images/CAMs, thereby allowing for interconnection among all their pixels, and performing explicit co-localization. This opens an explicit communication channel between CAMs, allowing for the transfer, aggregation, and correction of learned knowledge. Similarly to~\citep{tcamsbelharbi2023}, we use a U-Net style architecture~\citep{Ronneberger-unet-2015} that simultaneously classifies an image and locates the corresponding object. In addition to our new multi-frame constraint, we use per-frame local constraints including pseudo-labels and CRF loss~\citep{tcamsbelharbi2023,tang2018regularized}, in combination with a global size constraint. This has been shown to improve per-frame localization~\citep{tcamsbelharbi2023}. Our total training loss is composed of per-frame and multi-frame terms which are optimized simultaneously via standard Stochastic Gradient Descent (SGD). At inference, our method operates at a single frame level by simply performing a forward pass in the model, allowing a fast inference time. 


\textbf{Our main contributions are summarized as follows:}

\noindent \textbf{(1)} We propose a novel CAM-based method called CoLo-CAM for the WSVOL task. Using a color cue, we explicitly constrain CAMs from a sequence of image frames to be consistent by activating similarly over pixels with similar color, achieving co-localization. This joint learning creates a direct communication between pixels across all image locations over all frames, allowing the transfer, aggregation, and correction of learned localization. This is achieved by minimizing a color term of a CRF loss~\citep{tang2018regularized} over joint images/CAMs. Unlike the recent CAM-based method~\citep{tcamsbelharbi2023}, our temporal term does not assume the same object location in different frames but rather, allows the object to be situated anywhere, making it more flexible and robust to movements. Our multi-frame term is optimized jointly with other per-frame terms to boost localization performance.


\noindent \textbf{(2)}  Our empirical experiments on two challenging public datasets of unconstrained videos, YouTube-Objects v1.0~\citep{prest2012learning} and v2.2~\citep{KalogeitonFS16}, show the merits of our method and its robustness to long dependency. This allows for achieving new state-of-the-art localization performance. Several ablations are provided. Our results also confirm the potential benefit of discriminative learning in WSVOL task.



 



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Related Work}
\label{sec:related-w}
 
This section reviews weakly-supervised methods for localization/segmentation and co-localization/co-segmentation in videos. Additionally, CAM-based methods for WSOL on still images are discussed, as they relate to WSVOL. 


\noindent \textbf{Localization.}
%
Different methods exploit potential proposals as pseudo-supervision for localization~\citep{prest2012learning,zhang2020spftn}. In~\citep{prest2012learning}, class-specific detectors are proposed. First, segments of coherent motion~\citep{BroxM10} are extracted, then a spatiotemporal bounding box is fitted to each segment forming tubes. A single tube is jointly selected by minimizing energy based on the similarity between tubes, visual homogeneity over time, and the likelihood to contain an object. The selected tubes are used as box supervision to train a per-class detector~\citep{FelzenszwalbGMR10}. In~\citep{zhang2020spftn}, a DL model is trained using noisy supervision to perform segmentation and localization. Given a set of videos from the same class, bounding boxes and segmentation proposals are estimated using advanced optical flow~\citep{LeeKG11}. 
%%
Other methods seek to discover a prominent object (foreground) in (a) video(s)~\citep{jun2016pod,RochanRBW16,Kwak2015}. Then, similar regions are localized and refined using visual appearance and motion consistency. In~\citep{RochanRBW16}, box proposals are generated in a video, and only the relevant ones are retained for building an object appearance model. Maximum a posteriori inference over an undirected chain graphical model is employed to enforce the temporal appearance consistency between adjacent frames in the same video. 
%
The method in~\citep{Kwak2015} leverages two simultaneous and complementary processes for object localization: the discovery of similar objects in different videos, and the tracking of prominent regions in individual videos. Region proposals~\citep{ManenGG13}, in addition to appearance and motion confidence, are used to determine foreground objects~\citep{BroxM10}, and to maintain temporal consistency between consecutive frames. 


\noindent \textbf{Segmentation.} 
%
%\textbf{a)} 
Independent spatiotemporal segments are first extracted~\citep{HartmannGHTKMVERS12,tang2013discriminative,XuXC12,YanXCC17} via unsupervised methods~\citep{BanicaAIS13,XuXC12} or with a proposals generator~\citep{zhang2015semantic} using pretrained detectors~\citep{zhang2015semantic}. Then, using multiple properties such as visual appearance, and motion cues, these segments are labeled, while preserving temporal consistency. Graphs, such as Conditional Random Field (CRF) and GrabCut approaches~\citep{RotherKB04}, are often employed to ensure consistency. 
%
In~\citep{LiuTSRCB14} deal with multi-class video segmentation is proposed, where a nearest neighbor-based label transfer between videos is exploited. First, videos are first segmented into spatiotemporal supervoxels~\citep{XuXC12}, and represented by high-dimensional feature vectors using color, texture, patterns, and motion. These features are compressed using binary hashing for semantic similarity measures. A label transfer algorithm is designed using a graph that fosters label smoothness between spatiotemporal adjacent supervoxels in the same video, and between visually similar supervoxels across videos.
%
In M-CNN~\citep{Tokmakov2016}, a fully convolutional network (FCN) is combined with motion cues to estimate pseudo-labels. Using motion segmentation, a Gaussian mixture model is exploited to estimate foreground appearance potentials~\citep{papazoglou2013fast}. Combined with the FCN prediction, these potentials are used to yield better segmentation pseudo-labels via a GrabCut-like method~\citep{RotherKB04}, and these are then used to fit the FCN.
%
The previously mentioned segmentation methods use video tags to cluster videos with the same class. While other methods do not rely on such supervision, the process is generally similar. Foreground regions are first estimated~\citep{Croitoru2019,Halle2017,papazoglou2013fast,umer2021efficient} using motion cues~\citep{SundaramBK10}, or PCA, or VideoPCA~\citep{StretcuL15}. This initial segmentation is later refined via graph-methods~\citep{RotherKB04}.




\noindent \textbf{Co-segmentation/Co-localization.}
%
The co-segmentation methods have been leveraged to segment similar objects across a set of videos. Typical methods rely on discerning common objects via the similarity of different features including visual appearance, motion, and shape. Using initial guessing of objects/segments, graphs, such as CRF and graph cuts, are considered to model relationships between them~\citep{ChenCC12,FuXZL14,tsai2016semantic,ZhangJS14}.  In~\citep{ChenCC12}, the authors use relative intra-video motion extracted from dense optical flow, and inter-video co-features based on Gaussian Mixture Models (GMM). The common object, \ie, the foreground, in two videos is isolated from the background using a Markov Random Field (MRF). It is solved iteratively via graph cuts while accounting for a unary term based on the distance to GMMs, and a pairwise energy based on feature distance weighted by the relative motion distance between super-voxels. The authors in~\citep{FuXZL14} pursue a similar path. The method starts by generating class-independent proposals~\citep{EndresH10}. To identify the foreground object in each frame, various object characteristics are used while accounting for inter- and intra- video coherence of the foreground. A co-selection graph is formulated as a CRF exploiting unary energy from motion (optical flow), and pairwise energy over the foreground that combines visual (histogram) and shape similarities. In~\citep{ZhangJS14}, co-segmentation in videos is performed by sampling, tracking, and matching object proposals using graphs. It prunes away noisy segments in each video by selecting proposal tracklets that are spatially salient and temporally consistent. An iterative grouping process is used to gather objects with similar shapes and appearances (histogram) within and across videos. The final object regions obtained are used to initialize a segmentation process~\citep{FulkersonVS09}. The authors in~\citep{tsai2016semantic} use a pre-trained FCN to generate object-like tracklets which are linked for each object category via a graph. To define the corresponding relation between tracklets, a sub-modular optimization is formulated  based on the similarities of the tracklets via object appearance, shape, and motion. To discover prominent objects in each video, tracklets are ranked based on their mutual similarities.


Other methods aim to co-localize objects in images or videos using bounding boxes instead of segments. In~\citep{joulin2014}, a large number of proposed bounding boxes is generated by relying on objectness~\citep{AlexeDF12} with the goal being to select a single box that is more likely to contain the common object across images/videos. This is achieved by solving a quadratic problem using the Frank-Wolfe algorithm~\citep{Frank1956AnAF}. Following~\citep{TangJLF14}, box similarity and discriminability are used as constraints. Additionally, to ensure consistency between consecutive frames, multiple object properties including visual appearance, position, and size are used. \citep{JerripothulaCY16} leverage co-saliency as a prior to filter out noisy bounding box proposals. The co-saliency map is derived from inter- and intra-video commonness, and total motion saliency maps. Potential proposals are used for tracklet generation, and tracklets with a high confidence score and exhibiting good spatiotemporal consistency yield the final localization.



\noindent \textbf{WSOL in still images.}
CAM-based methods have emerged as a dominant approach for the WSOL~\citep{choe2020evaluating,rony2023deep} on still images. 
Early works focused on designing variant spatial pooling layers~\citep{durand2017wildcat,durand2016weldon, lin2013network, oquab2015object, pinheiro2015image,sun2016pronet,zhou2016learning, ZhouZYQJ18PRM}. An extension to a multi-instance learning framework (MIL) has been considered~\citep{ilse2018attention}. However, CAMs tend to under-activate by highlighting only small and the most discriminative parts of an object~\citep{choe2020evaluating,rony2023deep}, diminishing its localization performance. To overcome this, different strategies have been considered, including data augmentation over input image or deep features~\citep{belharbi2020minmaxuncer,ChoeS19,LiWPE018CVPR,MaiYL20eil,SinghL17,wei2017object,YunHCOYC19,ZhangWF0H18,zhu2017soft}, as well as architectural changes~\citep{gao2021tscam,KiU0B20icl,LeeKLLY19,XueLWJJY19iccvdanet,YangKKK20,ZhangW020i2c}. Recently, learning via pseudo-labels shown some potential, despite its reliance on noisy labels~\citep{belharbi2022fcam,negevsbelharbi2022,MeethalPBG20icprcstn,murtaza2022dipssypo,murtaza2022dips,wei2021shallowspol,ZhangCW20rethink,ZhangWKYH18}. Most previous methods used only forward information in a CNN, with some models designed to leverage backward information as well. These include biologically inspired methods~\citep{cao2015look,zhang2018top}, and gradient~\citep{ChattopadhyaySH18wacvgradcampp,fu2020axiom,JiangZHCW21layercam,SelvarajuCDVPB17iccvgradcam} or confidence score aggregation methods~\citep{desai2020ablation,naidu2020iscam,naidu2020sscam,WangWDYZDMH20scorecam}. While CAM-based methods are successful on still images, they still require  adaptation to leverage the temporal information in videos as in~\citep{tcamsbelharbi2023}. In~\citep{tcamsbelharbi2023}, the authors considered  improving the quality of pseudo-labels by aggregating CAMs extracted from a sequence of frames into a single CAM that covers more ROIs. However, because of object motion, the benefit of their method is quickly diminished after two frames.






%%%%%%%%%%%%%%%% Proposal
\section{Proposed Approach}
\label{sec:method}

\noindent \textbf{Notation.} We denote by ${\mathbb{D} = \{(\bm{V}, y)_i\}_{i=1}^N}$ a training set of videos, where ${\bm{V} = \{\bm{X}_t\}_{t=1}^{T}}$ is a video with $T$ frames, ${\bm{X}_t: \Omega \subset \reals^2}$ is the $t$-th frame, ${\Omega}$ is a discrete image domain. The global video tag, \ie, class label, is denoted  ${y \in \{1, \cdots, K\}}$, with $K$ being the number of classes. It is assumed that the video label $y$ is transferred to all frames within the video. Our model follows a U-Net style architecture~\citep{Ronneberger-unet-2015} with skip connections (Fig.\ref{fig:proposal}). It is composed of two parts: an encoder ${g}$ with parameters ${\bm{\theta^{\prime}}}$ image classification and a decoder ${f}$ with parameters ${\bm{\theta}}$ for localization.  

Our classifier ${g}$ is composed of a backbone encoder for extracting features, as well as a classification scoring head. The per-class classification probabilities are denoted ${g(\bm{X}) \in [0, 1]^K}$ where ${g(\bm{X})_k = \mbox{Pr}(k | \bm{X})}$. This module is trained to classify independent frames via standard cross-entropy, ${\min_{\bm{\theta^{\prime}}} \;  - \log(\mbox{Pr}(y | \bm{X})).}$ Its weights, ${\bm{\theta^{\prime}}}$, are then frozen and used to produce a CAM ${\bm{C}_t}$ for the frame ${\bm{X}_t}$ using the true label $y$~\citep{choe2020evaluating}. This makes ${\bm{C}_t}$ semantically consistent with the video tag. It is used later to generate pseudo-labels to train the decoder ${f}$.

The localizer ${f}$ is a decoder that yields two full-resolution CAMs that are softmaxed, and denoted ${\bm{S}_t = f(\bm{X}_t) \in [0, 1]^{\abs{\Omega} \times 2}}$. ${\bm{S}^0_t, \bm{S}^1_t}$ refer to the background and foreground maps, respectively. Let ${\bm{S}_t(p) \in [0, 1]^2}$ denotes a row of matrix ${\bm{S}_t}$, with index ${p \in \Omega}$ indicating a point within ${\Omega}$. We denote by ${\{\bm{X}\}^{n}_t = \{\bm{X}_{t-n-1}, \cdots, \bm{X}_{t-1}, \bm{X}_t\}}$ a set of $n$ consecutive frames starting at time $t$, and ${\{\bm{S}\}^n_t}$ it's corresponding CAMs at the decoder output, in the same order. As described below, we consider terms at two levels to train our decoder: per-frame and across-frames.


\noindent \textbf{Per-frame priors.} At the frame level, we leverage three terms to improve the CAM localization performance and alleviate their common issues in still images including bloby and unbalanced activations~\citep{choe2020evaluating,rony2023deep}.

\noindent \emph{Learning via pseudo-labels (PL).} It is commonly known that strong activations in a CAM are more likely to be a foreground, while low activations are assumed to be a background~\citep{zhou2016learning}. We leverage this information to generate pixel-wise pseudo-labels for foreground and background regions using the CAM ${\bm{C}_t}$ of the classifier ${g}$. Different from common practices where ROIs are generated once and kept fixed~\citep{KolesnikovL16}, we stochastically sample our ROIs. This has been shown to be more effective and avoids overfitting~\citep{negevsbelharbi2022}. Therefore, at each SGD step, we randomly sample a foreground pixel using a multinomial distribution over strong activations assuming that an object is local. We use a uniform distribution over low activations to sample background pixels assuming that background regions are evenly distributed in an image. The location of these two random pixels is encoded in ${\Omega^{\prime}_t }$. The partially pseudo-labeled mask for the sample ${\bm{X}_t}$ is denoted ${\bm{Y}_t}$, where ${\bm{Y}_t(p) \in \{0, 1\}^2}$ with labels ${0}$ for background, ${1}$ for foreground,  and locations with unknown labels are encoded as unknown. At this stage, since the pre-trained classifier $g$ is frozen, its CAM ${\bm{C}_t}$ is fixed and does not change during the training of $f$, allowing more stable sampling. Leveraging ${\bm{Y}_t}$ is achieved using partial cross-entropy,
\begin{equation}
    \label{eq:pl}
    \begin{aligned}
    &\bm{H}_p(\bm{Y}_t, \bm{S}_t) =\\ 
    &- (1 - \bm{Y}_t(p))\; \log(1 - \bm{S}_t^0(p))- \bm{Y}_t(p) \; \log(\bm{S}_t^1(p)) \;.
    \end{aligned}
\end{equation}

\noindent \emph{Local consistency (CRF).} Standard CAMs are low resolution leading to a bloby effect where activations do not align with the object's boundaries, thus contributing to poor localization~\citep{belharbi2022fcam,choe2020evaluating}. To avoid this, we use a CRF loss~\citep{tang2018regularized} to push the activations of ${\bm{S}_t}$ to be locally consistent in terms of color and proximity. For an image frame $\bm{X}_t$ and its maps ${\bm{S}_t}$, the CRF loss is formulated as,
\begin{equation}
    \label{eq:crf}
    \mathcal{R}(\bm{S}_t, \bm{X}_t) = \sum_{r \in \{0, 1\}} {\bm{S}^r_t}^{\top} \; \bm{W}_t \; (\bm{1} - \bm{S}^r_t) \;,
\end{equation}
where ${\bm{W}_t}$ is an affinity matrix  in which ${\bm{W}[i, j]}$ captures the color similarity and proximity between pixels ${i, j}$ in the image ${\bm{X}_t}$. We use a Gaussian kernel to capture the color and spatial similarities~\citep{KrahenbuhlK11crf}. The kernel is implemented via the permutohedral lattice~\citep{AdamsBD10lattice} for fast computation.


\noindent \emph{Absolute size constraint (ASC).} Unbalanced activations are common in CAMs~\citep{belharbi2022fcam,choe2020evaluating,rony2023deep}. Often, strong activations cover only small and the most discriminative parts of an object allowing the background to dominate. Alternatively, large parts of an image are activated as foreground~\citep{rony2023deep}. To avoid both these scenarios, we employ a global constraint over the CAM, in which we push the size of both regions, \ie, the foreground and background, to be as large as possible in a competitive way. To this end, we use an Absolute Size Constraint (ASC)~\citep{belharbi2020minmaxuncer} over the maps ${\bm{S}_t}$. This is achieved without requiring any knowledge about the  object size or a prior on which region is larger~\citep{pathak2015constrained}. This generic prior is formulated as inequality constraints which are then solved via a standard log-barrier method~\citep{boyd2004convex},
\begin{equation}
\label{eq:size_loss}
 \sum \bm{S}^r_t \geq 0 \;, \quad r \in \{0, 1\} \;,
\end{equation}
where ${\psi(\bm{S}^0_t) = \sum \bm{S}^0_t, \psi(\bm{S}^1_t)}$ represent the area, \ie, size, of the background and foreground regions, respectively. Using log-barrier function, we set,
\begin{equation}
\label{eq:log-bar-sz}
 \mathcal{R}_s(\bm{S}_t) = \sum_{r \in \{0, 1\}}-\frac{1}{z} \log(\psi(\bm{S}^r_t)) \;,
\end{equation}
where ${z>0}$ is a weight that is increased periodically.


\noindent \textbf{Multi-frame prior: color cue.} Given a sequence of frames ${\{\bm{X}\}^{n}_t}$, we aim to consistently and simultaneously align their corresponding CAMs ${\{\bm{S}\}^{n}_t}$ in term of \emph{color}. Thus, we perform a co-localization of objects with similar colors across frames.
This is translated by \emph{explicitly} constraining the CAMs ${\{\bm{S}\}^{n}_t}$ to activate similarly over similar color pixels across all the frames. This assumes that an object appearing in a video sequence maintains similar color. However, unlike~\citep{tcamsbelharbi2023}, we do not assume minimal displacement of objects. This gives our method more flexibility to localize an object independently from its location. Additionally, this joint learning opens an explicit communication tunnel between CAMs, allowing a transfer and aggregation of knowledge. This can help attenuate localization errors caused by noisy pseudo-labels. 
%
We perform this constraint by connecting each pixel at each frame to all pixels in every frame via the \emph{color term} of the CRF loss~\citep{tang2018regularized}. This creates a fully connected graph between all pixels, allowing explicit communication.
In practice, this is achieved by \emph{stitching} all frames\footnote{Frames are stitched horizontally or vertically.} ${\{\bm{X}\}^{n}_t}$ to build a single large image. We refer to the composite image by ${Cat(\{\bm{X}\}^{n}_t)=\bm{\bar{X}}}$, where ${Cat(\cdot)}$ is a stitching function. Similarly, we denote ${Cat(\{\bm{S}\}^{n}_t)=\bm{\bar{S}}}$ as the corresponding stitched CAMs done in the same order as ${Cat(\{\bm{X}\}^{n}_t)}$. The loss is formulated as,
\begin{equation}
    \label{eq:crf_rgb}
    \mathcal{R}_c(\{\bm{S}\}^{n}_t, \{\bm{X}\}^{n}_t) = \sum_{r \in \{0, 1\}} \bm{\bar{S}}^{r\top} \; \bm{W} \; (\bm{1} - \bm{\bar{S}}^r) \;,
\end{equation}
where ${\bm{W}}$ is the \emph{color} similarity matrix between pixels of the stitched image ${\bm{\bar{X}}}$. We refer to this term as CoLoc. Minimizing Eq.\ref{eq:crf_rgb} pushes the sequence of CAMs ${\{\bm{S}\}^{n}_t}$ to be consistent, with respect to color, across the frames sequence ${\{\bm{X}\}^{n}_t}$.

\noindent \textbf{Total training loss.} Our final loss combines per-frame and multi-frame losses to be optimized simultaneously. It is formulated as,
\begin{equation}
\label{eq:totalloss}
\begin{aligned}
\min_{\bm{\theta}} \quad & \sum_{p \in \Omega^{\prime}_t} \bm{H}_p(\bm{Y}_t, \bm{S}_t) + \lambda\; \mathcal{R}(\bm{S}_t, \bm{X}_t) + \mathcal{R}_s(\bm{S}_t) \\
& +  \frac{\lambda_c}{\abs{[\mathcal{R}_c]}}\; \mathcal{R}_c(\{\bm{S}\}^{n}_t, \{\bm{X}\}^{n}_t) \;,\\
\end{aligned}
\end{equation}
where $\lambda$ and $\lambda_c$ are positive weighting coefficients. All the terms are optimized simultaneously via SGD.

We note that the magnitude\footnote{Typical magnitude of ${\mathcal{R}_c}$ is ${2\cdot10^9}$ for ${n=2}$, and can grow to ${2\cdot10^{11}}$ for ${n=18}$. As a result, adequate values of the non-adapative ${\lambda_c}$ should be below ${2\cdot10^{-9}}$.} of the term ${\mathcal{R}_c}$ increases with the number of frames $n$. Large magnitudes can easily overpower other terms in Eq.\ref{eq:totalloss}, hindering learning. In practice, this makes tuning the hyper-parameter ${\lambda_c}$ critical and challenging. To reduce this strong dependency on $n$ and stabilize learning, we propose an \emph{adaptive} weight ${\lambda_c}$ that automatically \emph{scales down} this term via its magnitude ${\abs{[\mathcal{R}_c]}}$. This is inspired by the recently proposed adaptive weight decay~\citep{Ghiasi2022} which uses the weights' norm and their gradient for adaptation.
%
In this context, the operation ${[\cdot]}$ indicates that this value is now a constant and does not depend on any optimization parameters, \ie, ${\bm{\theta}}$. As a result, this term is always constant ${\lambda_c \; \frac{\mathcal{R}_c}{\abs{[\mathcal{R}_c]}} = \pm \lambda_c}$. However, its derivative is non-zero ${\partial \left( \lambda_c \; \frac{\mathcal{R}_c}{\abs{[\mathcal{R}_c]}} \right) / \partial \bm{\theta} = \frac{\lambda_c}{\abs{[\mathcal{R}_c]}} \frac{\partial \mathcal{R}_c}{\partial \bm{\theta}}}$. In this adaptive setup, the value ${\lambda_c}$ simply amplifies the scaled gradient. Its practical values are ${\lambda_c > 1}$, making it easier to tune compared to its non-adaptive version, which will be demonstrated empirically later. Most importantly, the adaptive version creates less dependency on $n$.

After being trained using Eq.\ref{eq:totalloss}, our model is applied on single frames using a simple forward pass for inference. This allows for fast inference and parallel computation over a video. Using standard procedures~\citep{tcamsbelharbi2023,choe2020evaluating}, a localization bounding box is extracted from the foreground CAM  of the decoder ${\bm{S}^1_t}$.


\begin{table*}
\begin{center}
\resizebox{.8\linewidth}{!}{
\begin{tabular}{|c|l||*{10}{c|}|g|c|}
\hline
\textbf{Dataset} & \textbf{Method \emph{(venue)}} & \textbf{Aero} & \textbf{Bird} & \textbf{Boat} & \textbf{Car} & \textbf{Cat} & \textbf{Cow} & \textbf{Dog} & \textbf{Horse} & \textbf{Mbike} & \textbf{Train} & \textbf{Avg} & \textbf{Time/Frame} \\
\hhline{-----------||---}
\noalign{\vspace{\doublerulesep}}
\hhline{-----------||---}
 &\citep{prest2012learning} {\small \emph{(cvpr)}} & 51.7 & 17.5 & 34.4 & 34.7 & 22.3 & 17.9 & 13.5 & 26.7 & 41.2 & 25.0 & 28.5 & N/A   \\
 &\citep{papazoglou2013fast} {\small \emph{(iccv)}} & 65.4 & 67.3 & 38.9 & 65.2 & 46.3 & 40.2 & 65.3 & 48.4 & 39.0 & 25.0 & 50.1 & 4s  \\
 &\citep{joulin2014} {\small \emph{(eccv)}} & 25.1 & 31.2 & 27.8 & 38.5 & 41.2 & 28.4 & 33.9 & 35.6 & 23.1 & 25.0 & 31.0 & N/A \\
 &\citep{Kwak2015} {\small \emph{(iccv)}}  & 56.5 & 66.4 & 58.0 & 76.8 & 39.9 & 69.3 & 50.4 & 56.3 & 53.0 & 31.0 & 55.7 & N/A  \\
 &\citep{RochanRBW16} {\small \emph{(ivc)}} & 60.8 & 54.6 & 34.7 & 57.4 & 19.2 & 42.1 & 35.8 & 30.4 & 11.7 & 11.4 & 35.8 & N/A \\
 &\citep{Tokmakov2016} {\small \emph{(eccv)}} & 71.5 & 74.0 & 44.8 & 72.3 & 52.0 & 46.4 & 71.9 & 54.6 & 45.9 & 32.1 & 56.6 & N/A  \\
 &POD~\citep{jun2016pod} {\small \emph{(cvpr)}} & 64.3 & 63.2 & 73.3 & 68.9 & 44.4 & 62.5 & 71.4 & 52.3 & 78.6 & 23.1 & 60.2 & N/A  \\
 &\citep{tsai2016semantic} {\small \emph{(eccv)}} & 66.1 & 59.8 & 63.1 & 72.5 & 54.0 & 64.9 & 66.2 & 50.6 & 39.3 & 42.5 & 57.9 & N/A  \\
 &\citep{Halle2017} {\small \emph{(iccv)}} & 76.3 & 71.4 & 65.0 & 58.9 & 68.0 & 55.9 & 70.6 & 33.3 & 69.7 & 42.4 & 61.1 & 0.35s  \\
&\citep{Croitoru2019} (LowRes-Net\textsubscript{iter1}) {\small \emph{(ijcv)}} & 77.0 & 67.5 & 77.2 & 68.4 & 54.5 & 68.3 & 72.0 & 56.7 & 44.1 & 34.9 & 62.1 & 0.02s  \\
\multirow{6}{*}{\ytovone } & \citep{Croitoru2019} (LowRes-Net\textsubscript{iter2}) {\small \emph{(ijcv)}} & 79.7 & 67.5 & 68.3 & 69.6 & 59.4 & 75.0 & 78.7 & 48.3 & 48.5 & 39.5 & 63.5 & 0.02s\\
&\citep{Croitoru2019} (DilateU-Net\textsubscript{iter2}) {\small \emph{(ijcv)}} & 85.1 & 72.7 & 76.2 & 68.4 & 59.4 & 76.7 & 77.3 & 46.7 & 48.5 & 46.5 & 65.8 & 0.02s \\
&\citep{Croitoru2019} (MultiSelect-Net\textsubscript{iter2}) {\small \emph{(ijcv)}} & 84.7 & 72.7 & 78.2 & 69.6 & 60.4 & 80.0 & 78.7 & 51.7 & 50.0 & 46.5 & 67.3 & 0.15s \\
&SPFTN (M)~\citep{zhang2020spftn} {\small \emph{(tpami)}} & 66.4 & 73.8 & 63.3 & 83.4 & 54.5 & 58.9 & 61.3 & 45.4 & 55.5 & 30.1 & 59.3 & N/A  \\
&SPFTN (P)~\citep{zhang2020spftn} {\small \emph{(tpami)}}& \textbf{97.3} & 27.8 & 81.1 & 65.1 & 56.6 & 72.5 & 59.5 & \textbf{81.8} & 79.4 & 22.1 & 64.3 & N/A  \\
&FPPVOS~\citep{umer2021efficient} {\small \emph{(optik)}} & 77.0 & 72.3 & 64.7 & 67.4 & 79.2 & 58.3 & 74.7 & 45.2 & 80.4 & 42.6 & 65.8 & 0.29s  \\
% \hhline{--||---------||---}
\cline{2-14}
&CAM~\citep{zhou2016learning} {\small \emph{(cvpr)}} & 75.0 & 55.5 & 43.2 & 69.7 & 33.3 & 52.4 & 32.4 & 74.2 & 14.8 & 50.0 & 50.1 & 0.2ms  \\
&GradCAM~\citep{SelvarajuCDVPB17iccvgradcam} {\small \emph{(iccv)}} & 86.9 & 63.0 & 51.3 & 81.8 & 45.4 & 62.0 & 37.8 & 67.7 & 18.5 & 50.0 & 56.4 & 27.8ms  \\
&GradCAM++~\citep{ChattopadhyaySH18wacvgradcampp} {\small \emph{(wacv)}} & 79.8 & 85.1 & 37.8 & 81.8 & 75.7 & 52.4 & 64.9 & 64.5 & 33.3 & 56.2 & 63.2 & 28.0ms  \\
&Smooth-GradCAM++~\citep{omeiza2019corr} {\small \emph{(corr)}} & 78.6 & 59.2 & 56.7 & 60.6 & 42.4 & 61.9 & 56.7 & 64.5 & 40.7 & 50.0 & 57.1 & 136.2ms  \\
&XGradCAM~\citep{fu2020axiom} {\small \emph{(bmvc)}} & 79.8 & 70.4 & 54.0 & \textbf{87.8} & 33.3 & 52.4 & 37.8 & 64.5 & 37.0 & 50.0 & 56.7 & 14.2ms  \\
&LayerCAM~\citep{JiangZHCW21layercam} {\small \emph{(ieee)}} & 85.7 & \textbf{88.9} & 45.9 & 78.8 & 75.5 & 61.9 & 64.9 & 64.5 & 33.3 & 56.2 & 65.6 & 17.9ms  \\
&TCAM~\citep{tcamsbelharbi2023} {\small \emph{(wacv)}} & 90.5 & 70.4 & 62.2 & 75.7 & \textbf{84.8} & \textbf{81.0} & 81.0 & 64.5 & 70.4 & 50.0 & 73.0 & 18.5ms  \\
&CoLo-CAM (ours) & 90.4 & 74.0 & \textbf{91.8} & \textbf{87.8} & 78.7 & 80.9 & \textbf{89.1} & 74.1 & \textbf{85.1} & \textbf{68.7} & \textbf{82.1} & 18.5ms  \\
\hhline{-----------||---}
\noalign{\vspace{1.5mm}}
\hhline{-----------||---}
&\citep{Halle2017}  {\small \emph{(iccv)}}& 76.3 & 68.5 & 54.5 & 50.4 & 59.8 & 42.4 & 53.5 & 30.0 & 53.5 & \textbf{60.7} & 54.9 & 0.35s   \\
&\citep{Croitoru2019} (LowRes-Net\textsubscript{iter1}) {\small \emph{(ijcv)}} & 75.7 & 56.0 & 52.7 & 57.3 & 46.9 & 57.0 & 48.9 & 44.0 & 27.2 & 56.2 & 52.2 & 0.02s    \\
&\citep{Croitoru2019} (LowRes-Net\textsubscript{iter2}) {\small \emph{(ijcv)}} & 78.1 & 51.8 & 49.0 & 60.5 & 44.8 & 62.3 & 52.9 & 48.9 & 30.6 & 54.6 & 53.4 & 0.02s    \\
&\citep{Croitoru2019} (DilateU-Net\textsubscript{iter2}){\small \emph{(ijcv)}} & 74.9 & 50.7 & 50.7 & 60.9 & 45.7 & 60.1 & 54.4 & 42.9 & 30.6 & 57.8 & 52.9 & 0.02s   \\
\multirow{6}{*}{\ytovtwodtwo } & \citep{Croitoru2019} (BasicU-Net\textsubscript{iter2}){\small \emph{(ijcv)}} & 82.2 & 51.8 & 51.5 & 62.0 & 50.9 & 64.8 & 55.5 & 45.7 & 35.3 & 55.9 & 55.6 & 0.02s  \\
&\citep{Croitoru2019} (MultiSelect-Net\textsubscript{iter2}){\small \emph{(ijcv)}} & 81.7 & 51.5 & 54.1 & 62.5 & 49.7 & 68.8 & 55.9 & 50.4 & 33.3 & 57.0 & 56.5 & 0.15s  \\
% \hhline{-----------||--}
\cline{2-14}
&CAM~\citep{zhou2016learning} {\small \emph{(cvpr)}} & 52.3 & 66.4 & 25.0 & 66.4 & 39.7 & \textbf{87.8} & 34.7 & 53.6 & 45.4 & 43.7 & 51.5 & 0.2ms  \\
&GradCAM~\citep{SelvarajuCDVPB17iccvgradcam} {\small \emph{(iccv)}} & 44.1 & 68.4 & 50.0 & 61.1 & 51.8 & 79.3 & 56.0 & 47.0 & 44.8 & 42.4 & 54.5 & 27.8ms  \\
&GradCAM++~\citep{ChattopadhyaySH18wacvgradcampp} {\small \emph{(wacv)}} & 74.7 & 78.1 & 38.2 & 69.7 & 56.7 & 84.3 & 61.6 & 61.9 & 43.0 & 44.3 & 61.2 & 28.0ms  \\
&Smooth-GradCAM++~\citep{omeiza2019corr} {\small \emph{(corr)}} & 74.1 & 83.2 & 38.2 & 64.2 & 49.6 & 82.1 & 57.3 & 52.0 & 51.1 & 42.4 & 59.5 & 136.2ms  \\
&XGradCAM~\citep{fu2020axiom} {\small \emph{(bmvc)}} & 68.2 & 44.5 & 45.8 & 64.0 & 46.8 & 86.4 & 44.0 & 57.0 & 44.9 & 45.0 & 54.6 & 14.2ms  \\
&LayerCAM~\citep{JiangZHCW21layercam} {\small \emph{(ieee)}} & 80.0 & 84.5 & 47.2 & \textbf{73.5} & 55.3 & 83.6 & 71.3 & 60.8 & 55.7 & 48.1 & 66.0 & 17.9ms  \\
&TCAM~\citep{tcamsbelharbi2023} {\small \emph{(wacv)}}  & 79.4 & \textbf{94.9} & 75.7 & 61.7 & 68.8 & 87.1 & 75.0 & 62.4 & 72.1 & 45.0 & 72.2 & 18.5ms  \\
&CoLo-CAM (ours) & \textbf{82.9} & 92.2 & \textbf{85.4} & 67.7 & \textbf{80.1} & 85.7 & \textbf{79.2} & \textbf{67.4} & \textbf{72.7} & 58.2 & \textbf{77.1} & 18.5ms  \\
\hline
\end{tabular}}
\end{center}
\caption{\corloc localization accuracy on the \ytovone~\citep{prest2012learning} and \ytovtwodtwo~\citep{KalogeitonFS16} test sets.}
\label{tab:yto}
\end{table*}

\begin{figure*}
     \centering
     \begin{subfigure}[b]{0.29\textwidth}
         \centering
         \includegraphics[width=\textwidth]{ablation-n-YouTube-Objects-v1.0}
         \caption{}
         \label{fig:ablation-range-time}
     \end{subfigure}
     %\\
     \begin{subfigure}[b]{0.29\textwidth}
         \centering
         \includegraphics[width=\textwidth]{ablation-lambda-c-YouTube-Objects-v1.0}
         \caption{}
         \label{fig:ablation-lambda-c}
     \end{subfigure}
     \begin{subfigure}[b]{0.34\textwidth}
         \centering
         \includegraphics[width=\textwidth]{ablation-lambda-c-con-vs-ada-YouTube-Objects-v1.0}
         \caption{}
         \label{fig:ablation-lambda-c-const-vs-adap}
     \end{subfigure}
     %\\
        \caption{Ablations on the \ytovone test set. 
        %
        \textbf{(a)}: Impact on \corloc accuracy of the number of frames ${n}$. 
        %
        \textbf{(b)}: Impact on \corloc accuracy of the \emph{adaptive} ${\lambda_c}$.
        %
        \textbf{(c)}: \corloc accuracy on the \ytovone test set using constant and adaptive ${\lambda_c}$ weight (\emph{left y-axis}), and  ${\log{(\abs{\mathcal{R}_c})}}$ (\emph{right y-axis}). The \emph{x-axis} is the number of frames $n$. 
        }
        \label{fig:ablations-lambda-c-n}
\end{figure*}


%%%%%%%%%%%%%%%% Exps
\section{Results and Discussion}
\label{sec:results}

\subsection{Experimental Methodology}
\label{subsec:exp-method}

\noindent \textbf{Datasets.} To evaluate our method, experiments were conducted on standard unconstrained video datasets for WSVOL. For training, videos are labeled globally via a class-tag. Frame bounding boxes are provided to evaluate localization. We used two challenging public datasets from YouTube\footnote{\url{https://www.youtube.com}}:  YouTube-Object v1.0 (\ytovone~\citep{prest2012learning}) and YouTube-Object v2.2 (\ytovtwodtwo~\citep{KalogeitonFS16} ). In all our experiments, we followed the same protocol as described in~\citep{tcamsbelharbi2023,KalogeitonFS16,prest2012learning}.

\noindent\textit{YouTube-Object v1.0 (\ytovone) \citep{prest2012learning}:} A dataset composed of videos collected from YouTube by querying the names of 10 classes. A class has between 9-24 videos with a duration ranging from 30 seconds to 3 minutes. It has 155 videos, each split into short-duration clips, \ie, shots. There are 5507 shots with 571 089 frames in all. Each shot has only a few selected frames with bounding box annotations. The dataset has been divided by the authors into 27 test videos for a total of 396 bounding boxes, and 128 videos for training. Some videos from the trainset are considered for validation to perform early-stopping~\citep{tcamsbelharbi2023,KalogeitonFS16,prest2012learning}.  We sampled 5 random videos per class to build a validation set with a total of 50 videos.


\noindent\textit{YouTube-Object v2.2 (\ytovtwodtwo)~\citep{KalogeitonFS16}:} This is an extension of the \ytovone dataset with large and challenging test set. It is composed of more frames, 722 040 in total. The authors provided more bounding boxes annotation in this case. The dataset was divided by the authors into 106 videos for training, and 49 videos for test. Following~\citep{tcamsbelharbi2023}, we sampled 3 random videos per class from the train to build a validation. The test set has more samples compared to \ytovone. It is composed of a total of 2 667 bounding box making it a much more challenging dataset.


\noindent \textbf{Evaluation Metric.} Localization accuracy was evaluated using the \corloc metric~\citep{tcamsbelharbi2023,deselaers2012weakly}. It describes the percentage of predicted bounding boxes that have an Intersection Over Union (IoU) between prediction and ground truth greater than half (IoU > 50\%). 


\noindent \textbf{Implementation Details.} 
%
We trained for 10 epochs with 32 mini-batch for all our experiments. We used ResNet50~\citep{heZRS16} as a backbone. Images are resized to ${256\times256}$, and randomly cropped patches of size ${224\times224}$ for training. The temporal dependency ${n}$ in Eq.\ref{eq:crf_rgb} is set via the validation set from  ${n \in \{2, \cdots, 18\}}$. The hyper-parameter ${\lambda_c}$ in Eq.\ref{eq:totalloss} is set from ${\{1, \cdots, 16\}}$ with the adaptive setup. In Eq.\ref{eq:crf_rgb}, frames are stitched horizontally to build a large image.
%
We set the CRF's hyper-parameter $\lambda$ in Eq.\ref{eq:totalloss} to the same value as in~\citep{tang2018regularized} that is ${2e^{-9}}$.  Its color and spatial kernel bandwidth are set to 15 and 100, respectively~\citep{tang2018regularized}. We use the same color bandwidth for Eq.\ref{eq:crf_rgb}. The initial value of the log-barrier coefficient, $z$ in Eq.\ref{eq:log-bar-sz}, is set to $1$, and then increased by a factor of ${1.01}$ in each epoch with a maximum value of $10$ as in~\citep{belharbi2019unimoconstraints,kervadec2019log}. We selected the best learning rate from ${\{0.1, 0.01, 0.001\}}$. The classifier $g$ was pre-trained on single frames. 


\noindent \textbf{Baseline Methods.} For comparison, publicly available results were considered. We  compared our proposed approach with different WSVOL methods~\citep{
Croitoru2019, Halle2017, joulin2014, Kwak2015, papazoglou2013fast, prest2012learning, RochanRBW16, Tokmakov2016, tsai2016semantic}, POD~\citep{jun2016pod}, SPFTN~\citep{zhang2020spftn}, and FPPVOS~\citep{umer2021efficient}. We also considered the performance of several CAM-based methods reported in~\citep{tcamsbelharbi2023}: CAM~\citep{zhou2016learning}, GradCAM~\citep{SelvarajuCDVPB17iccvgradcam}, GradCam++~\citep{ChattopadhyaySH18wacvgradcampp}, Smooth-GradCAM++~\citep{omeiza2019corr}, XGradCAM~\citep{fu2020axiom}, and LayerCAM~\citep{JiangZHCW21layercam}, which were all trained on single frames, \ie, no temporal dependency was considered. LayerCAM method~\citep{JiangZHCW21layercam} was employed to generate CAMs ${\bm{C}_t}$ used to create pseudo-labels, which were then used to build pseudo-labels ${\bm{Y}_t}$ (Eq.\ref{eq:pl}). However, different CAM-based methods can be integrated with our approach.
    



%%%%%%%%%%%
\subsection{Results}
\label{subsec:compare}

\noindent \textbf{Comparison with the State-of-Art (Tab.\ref{tab:yto}).} Overall, our CoLo-CAM method outperformed other methods by a large margin, more so on \ytovone compared to \ytovtwodtwo. This highlights the difficulty of \ytovtwodtwo. In terms of per-class performance, our method is competitive over most classes. In particular, we observe that our method achieved large improvement over the challenging class 'Train'. In general, our method and CAM methods are still behind on the 'Horse' and 'Aero' classes compared to the SPFTN method~\citep{zhang2020spftn} which relies on optical flow to generate proposals. Both classes present different challenges. The former class shows with dense multi-instances, while the latter, \ie, 'Aero', appears in complex scenes (airport), often with very large size, and occlusion. Our method has the same inference time as TCAM~\citep{tcamsbelharbi2023}, and our new term in Eq.\ref{eq:crf_rgb} adds a small training time of ${\sim 81}$ ms per 64 frames (see supplementary material).
%


\noindent \textbf{Ablation Studies.} 
%

{
%\setlength{\tabcolsep}{3pt}
%\renewcommand{\arraystretch}{1.1}
\begin{table}[ht!]
\centering
\resizebox{0.9\linewidth}{!}{%
\centering
%\small
\begin{tabular}{|lllc|g|}
\hline
\multicolumn{3}{|l}{\textbf{Methods}}& &  \corloc  \\
\hline \hline %\cline{1-3}\cline{5-5} \\
\multicolumn{3}{|l}{Layer-CAM~\citep{JiangZHCW21layercam} {\small \emph{(ieee)}}} &  &  \tableplus{65.6}  \\
\hline %\cline{1-3}\cline{5-5} \\
% \cline{1-4} \\
\multirow{3}{*}{Single-frame} & &       
PL &  &   $68.5$       \\
&& PL + CRF &  &   $69.6$       \\
&& PL + ASC &  &   $66.2$      \\
&& PL + ASC + CRF &  &   $70.5$\\ 
\hline 
%\cline{1-1}\cline{3-3} \cline{5-5} \\
Multi-frame & & PL + ASC + CRF + CoLoc (Ours) &  &   $82.1$ \\
\hline
%\cline{1-1}\cline{3-3} \cline{5-5} \\
\multicolumn{3}{|l}{Improvement} &  &  \tableplus{+16.5}\\ \hline 
%\cline{1-3}\cline{5-5} \\
\end{tabular}
}
\caption{Impact on \corloc localization accuracy of different CoLo-CAM loss terms on the \ytovone test set.}
\label{tab:ablation-parts}
\vspace{-1em}
\end{table}
}



\noindent\emph{Impact of different loss terms (Tab.\ref{tab:ablation-parts}).} Without any spatiotemporal dependency, using pseud-labels (PL), CRF, and absolute size constraint (ASC) helped to improve localization performance. This brought up localization performance from ${65.6\%}$ to ${70.5\%}$. However, adding our multi-frame term, \ie, CoLoc, increased the performance up to ${82.1\%}$ demonstrating its benefits.

\noindent\emph{Impact of ${n}$ on localization performance (Fig.\ref{fig:ablation-range-time}).} We observe that both methods, TCAM~\citep{tcamsbelharbi2023} and ours, get better results when considering spatiotemporal information. However, TCAM reached its peak performance when using only ${n=2}$ frames. Large performance degradation is observed when increasing $n$. This comes as a result of assuming that objects have minimal movement. On the opposite, our method improves when increasing $n$ until it reaches its peak at ${n=4}$. Different from TCAM, our method showed more robustness and stability with the increase of $n$ since we only assume objects visual similarity.


\noindent\emph{Constant vs adaptive ${\lambda_c}$ (Fig.\ref{fig:ablation-lambda-c-const-vs-adap}).}  
Note that in the constant setup, adequate ${\lambda_c}$ can be determined using a blind search or given a prior on the magnitude loss. Both approaches are tedious especially when typical values of ${\lambda_c}$ are below ${2*10^{-9}}$ creating a large search space. Exploring such space is computationally expensive. Using constant value held good results up to 6 frames, while, large $n$ led to performance degradation since adequate values are required. Our adaptive scheme is more intuitive, and does not require a prior knowledge. It achieved constantly better and stable performance with less effort. Better results are obtained with ${\lambda_c}$ values between 3, and 7 (Fig.\ref{fig:ablation-lambda-c}). Performance degradation is observed when using large amplification giving more importance to ${\mathcal{R}_c}$ term, allowing it to outweigh other terms which hindered training. In the opposite, using small amplification gave small push to this term, hence, less contribution.


\begin{figure}
     \centering
     \begin{subfigure}[b]{0.24\textwidth}
         \centering
         \includegraphics[width=\textwidth]{1424_car_video-0002_shot-000018_00003429.jpg}
     \end{subfigure}
     \begin{subfigure}[b]{0.24\textwidth}
         \centering
         \includegraphics[width=\textwidth]{1579_horse_video-0001_shot-000003_00000445.jpg}
     \end{subfigure}
     \\
     \begin{subfigure}[b]{0.24\textwidth}
         \centering
         \includegraphics[width=\textwidth]{1770_train_video-0008_shot-000022_00046634.jpg}
     \end{subfigure}
     \begin{subfigure}[b]{0.24\textwidth}
         \centering
         \includegraphics[width=\textwidth]{1776_train_video-0008_shot-000035_00051421.jpg}
     \end{subfigure}
        \caption{Typical challenges of our method. (Colors: Fig.\ref{fig:visu-pred})}
        \label{fig:failure}
\end{figure}


\begin{figure}
     \centering
     \begin{subfigure}[b]{0.44\textwidth}
         \centering
         \includegraphics[width=\textwidth]{tag}
     \end{subfigure}
     \\
     \begin{subfigure}[b]{0.44\textwidth}
         \centering
         \includegraphics[width=\textwidth]{1-dog_data_0001_shots_029_frame0001.jpg}
     \end{subfigure}
     \\
     \begin{subfigure}[b]{0.44\textwidth}
         \centering
         \includegraphics[width=\textwidth]{2-train_video-0008_shot-000015_00045305.jpg}
     \end{subfigure}
     \\
     \begin{subfigure}[b]{0.44\textwidth}
         \centering
         \includegraphics[width=\textwidth]{75-train_video-0008_shot-000013_00045196.jpg}
     \end{subfigure}
     \\
     \begin{subfigure}[b]{0.44\textwidth}
         \centering
         \includegraphics[width=\textwidth]{1317-train_video-0008_shot-000029_00048230.jpg}
     \end{subfigure}
      \\
     \begin{subfigure}[b]{0.44\textwidth}
         \centering
         \includegraphics[width=\textwidth]{120-boat_data_0005_shots_010_frame0001.jpg}
     \end{subfigure}
     \\
     \begin{subfigure}[b]{0.44\textwidth}
         \centering
         \includegraphics[width=\textwidth]{147-car_data_0002_shots_012_frame0001.jpg}
     \end{subfigure}
     \\
     \begin{subfigure}[b]{0.44\textwidth}
         \centering
         \includegraphics[width=\textwidth]{1049-motorbike_video-0011_shot-000398_00051051.jpg}
     \end{subfigure}
     \\
     \begin{subfigure}[b]{0.44\textwidth}
         \centering
         \includegraphics[width=\textwidth]{961-aeroplane_video-0002_shot-000012_00004864.jpg}
     \end{subfigure}
     \\
     \begin{subfigure}[b]{0.44\textwidth}
         \centering
         \includegraphics[width=\textwidth]{970-aeroplane_video-0002_shot-000030_00007901.jpg}
     \end{subfigure}
        \caption{Localization examples of test sets frames. 
        \emph{Bounding boxes}: ground truth (green), prediction (red). The second column of each method is the predicted CAM over image.}
        \label{fig:visu-pred}
\end{figure}






\noindent \textbf{Visual Results (Fig.\ref{fig:visu-pred}).} Compared to TCAM~\citep{tcamsbelharbi2023} and LayerCAM~\citep{JiangZHCW21layercam}, we observe that our CAMs became more discriminative. This translates in several advantageous properties. Our CAMs showed sharp, more complete, and less noisy activations localized more precisely around objects. In addition, localizing small objects such as 'Bike', and large objects such as 'Train' becomes more accurate. CAM activations often suffer from co-occurrence issue~\citep{tcamsbelharbi2023,rony2023deep} where consistently appearing contextual objects are confused with main objects. Our CAMs showed more robustness to this issue. This can be seen in the case of road vs 'Car'/'Bike', or water vs 'Boat'. 
%
Despite this improvement, our method still faces two main challenges (Fig.\ref{fig:failure}). The first is the case of samples with dense and overlapped instances (top Fig.\ref{fig:failure}). This often causes activations to spill across instances to form a large object hindering localization performance. The second issue concerns large localization errors (bottom Fig.\ref{fig:failure}). Since our localization is mainly stimulated by discriminative cues from CAMs, it is still challenging to detect when a classifier points to the wrong object. This often leads to large errors that are difficult to correct in downstream use. Future works may consider leveraging classifier response over ROIs to ensure their quality. Additionally, object movement information can be combined with CAMs' cues. 






\section{Conclusion}
\label{sec:conclusion}

We have proposed a new CAM-based approach for the WSVOL task. Using a color cue, we constrain the CAM response over a sequence of frames to be similar over similar pixels, assuming that an object maintains a similar color. This achieves explicit co-localization across frames. It is performed by minimizing a color-term of a CRF loss over a sequence of images/CAMs. In addition to our multi-frame constraint, we imposed per-frame local constraints to be then all optimized simultaneously.
Empirical experiments showed the merits of our method and its robustness to long-term dependencies, leading to new state-of-the-art localization performance.



\section*{Acknowledgment}
This research was supported in part by the Canadian Institutes of Health Research, the Natural Sciences and Engineering Research Council of Canada, and the Digital Research Alliance of Canada (alliancecan.ca).

% \clearpage
% \newpage

\appendices


% ===================================================================================================
% 
%
%                                          SUPP-MATERIAL
% 
% 
% ===================================================================================================

The next supplementary materials are comprised of:
\begin{itemize}
    \item detailed illustrations of our approach for training and inference (Sec.\ref{sec:method-supp-mat});
    \item a discussion on pseudo-labels sampling (Sec.\ref{sec:method-supp-mat});
    \item additional ablation studies (Sec.\ref{subsec:more-ablation}); and 
    \item additional visual results (Sec.\ref{subsec:more-visuals}). 
\end{itemize}


%%%%%%%%%%%%%%%
\section{Proposed Approach}
\label{sec:method-supp-mat}

\noindent \textbf{Detailed illustrations for training and inference.} Fig.\ref{fig:proposal-details} presents our detailed model for training and inference. During the training, we consider ${n=3}$ input frames. We use three per-frame terms that aim at improving localization at each frame, and without considering any spatiotemporal information. Using the classifier CAM ${\bm{C}_t}$, we sample pseudo-labels which are used at the pseudo-label term (PL). Additionally, we apply a CRF, and a size constraint (ASC). In parallel, we apply a multi-frame term ver all the frames allowing explicit communication between them. This is achieved by constraining the output CAMs of the frames to activate similarly over pixels with similar color.

At inference time, we simply consider single frames, \ie, no spatiotemporal information. A bounding box is estimated from the foreground CAM, at the decoder level. In addition, frame classification scores are obtained from the encoder.

\begin{figure*}[ht!]
\centering
  \centering
  \includegraphics[width=\linewidth]{details}
  \caption[Caption]{Training and inference with our proposed CoLo-CAM method. The single-frame and multi-frame training loss terms are illustrated with $n=3$ frames. \emph{Left: training phase}: It combines the per-frame terms composed of pseudo-labels loss (PL), absolute size constraint (ASC), and CRF loss, as well as the multi-frame term.
  %
  \emph{Right: inference phase}: It operates at a single frame with no temporal dependency, and predicts bounding box localization and classification.
  %
  Notation and per-frame and multi-frame terms are described in Section \ref{sec:method}.}
  \label{fig:proposal-details}
\end{figure*}

\noindent \textbf{Learning via pseudo-labels (PLs).}
%
In this section, we provide more details on sampling pseudo-labels through the classifier's CAM ${\bm{C}_t}$. These labels are then used to train the decoder $f$, in combination with other terms. Through the discriminative property of the pseudo-labels, they initiate and stimulate object localization. In CAM-based methods for WSOL, it is common to assume that CAMs' strong activations are likely to point to the foreground, whereas low activations hint at background regions~\citep{durand2017wildcat,zhou2016learning}.

In the following, we denote the foreground region as ${\mathbb{C}^+_t}$ which is computed by the operation ${\mathcal{O}^+}$. For simplicity and without adding new hyper-parameters, ${\mathbb{C}^+_t}$ is defined as the set of pixels with CAM activation ${\bm{C}_t}$ greater than the Otsu threshold~\citep{otsuthresh} estimated from ${\bm{C}_t}$. The background region, \ie, ${\mathbb{C}^-_t}$, is the remaining set of pixels. Both regions are defined as follows,
\begin{equation}
    \label{eq:sets}
    \mathbb{C}^+_t = \mathcal{O}^+(\bm{C}_t), \quad \mathbb{C}^-_t = \mathcal{O}^-(\bm{C}_t) \;.
\end{equation}
Given the weak supervision, these regions are uncertain. For instance, the ${\mathbb{C}^+_t}$ region may hold only a small part of the foreground, along with the background parts. Consequently, the background region ${\mathbb{C}^-_t}$ may still have foreground parts. This uncertainty in partitioning the image foreground and background makes it unreliable for localization to directly fit~\citep{KolesnikovL16} the model to full regions. Instead, we pursue a stochastic sampling strategy over both regions to avoid overfitting and provide the model enough time for the emergence of consistent regions~\citep{belharbi2022fcam,negevsbelharbi2022}. For each frame, and for an SGD step, we randomly sample a pixel from ${\mathbb{C}^+_t}$ as foreground, and a pixel from ${\mathbb{C}^-_t}$ as background to be pseudo-labels. We encode their location according to:
\begin{equation}
    \label{eq:sset}
    \Omega^{\prime}_t = \mathcal{M}(\mathbb{C}^+_t)\; \cup \; \mathcal{U}(\mathbb{C}^-_t) \;, 
\end{equation}
where ${\mathcal{M}(\mathbb{C}^+_t)}$ is a multinomial sampling distribution over the foreground, and ${\mathcal{U}(\mathbb{C}^-_t)}$ is a uniform distribution over the background. This choice of distributions is based on the assumption that the foreground object is often concentrated in one place, while the background region is spread across the image. The partially pseudo-labeled mask for the sample ${\bm{X}_t}$ is referred to as ${Y_t}$, where ${Y_t(p) \in \{0, 1\}^2}$ with labels ${0}$ for the background, and ${1}$ for the foreground. Undefined regions are labeled as unknown. The pseudo-annotation mask ${Y_t}$ is then leveraged to train the decoder using partial cross-entropy. Such stochastic pseudo-labels are expected to stimulate the learning of the filters and guide them to generalize and respond in the same way over regions with similar color/texture.



%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Results and Discussion}
\label{sec:results-supp-mat}


\subsection{Additional Ablation Studies.}
\label{subsec:more-ablation}

\noindent\emph{Computation time of our multi-frame term (Eq.\ref{eq:crf_rgb}) with respect to ${n}$ (Fig.\ref{fig:ablation-time-joint-crf}).} The computation of this loss term is divided on two types of devices (hybrid fashion): CPU and GPU. The left-side matrix product is performed on the GPU to accelerate the computation time, while the rest of the term is first computed on the CPU. Note that the transfer time from CPU to GPU is included in our complexity analysis. The overhead in processing time grows linearly with respect to the number of frames. However, this computational cost remains resealable for training in a realistic time, even with a large $n$. For example, $n = 64$ frames can be processed in ${\sim 81}$ ms.

\begin{figure}[ht!]
\centering
  \centering
  \includegraphics[width=\linewidth]{ablation-time-n-joint-crf-YouTube-Objects-v1.0}
  \caption{Computation time of our multi-frame loss term (Eq.\ref{eq:crf_rgb}) in function of number of frames $n$. Computation devices: CPU (Intel(R) Xeon(R), 48 cores), and GPU (NVIDIA Tesla P100). Single image frame size: ${224\times224}$.}
  \label{fig:ablation-time-joint-crf}
\end{figure}

\noindent\emph{Effect of frame sampling strategy (Tab.\ref{tab:ablation-frame-sampling}).} We explored different strategies to randomly sample $n$ training frames from a video, with an emphasis on \emph{frame diversity}:
\begin{itemize}
    \item \textbf{Consecutive scheme}: A first frame is uniformly sampled from the video. Then, its previous ${n-1}$ frames are also considered. This favors similar (less diverse) frames.
    \item \textbf{Interval scheme}: This is the opposite case of the consecutive scheme where the aim is to sample the most diverse frames. The video is split into $n$ equal and disjoint intervals. Then, a single frame is uniformly sampled from each interval.
    \item \textbf{Gaussian scheme}: This middle-ground scheme lies between consecutive and interval strategies. We sample $n$ random frames via a Gaussian distribution centered in the middle of a video.
\end{itemize}
All the sampling is done without repetition. In Tab.\ref{tab:ablation-frame-sampling}, consecutive sampling performs the best, suggesting that our co-localization term is more beneficial when frames are similar. Note that using diverse frames via Gaussian or interval scheme still yielded good performance.

{
%\setlength{\tabcolsep}{3pt}
%\renewcommand{\arraystretch}{1.1}
\begin{table}[ht!]
\centering
\resizebox{.5\linewidth}{!}{%
\centering
%\small
\begin{tabular}{|lc|g|}
\hline
\multicolumn{1}{|l}{\textbf{Methods}}& &  \corloc  \\
\hline \hline %\cline{1-3}\cline{5-5} \\     
Consecutive     &  &   $82.1$       \\
Gaussian        &  &   $80.2$       \\
Interval        &  &   $79.8$      \\
\hline 
\end{tabular}
}
\caption{Impact on localization accuracy (\corloc) of different frame sampling strategies on the  \ytovone test set.}
\label{tab:ablation-frame-sampling}
\vspace{-1em}
\end{table}
}



%%%
\subsection{Visual Results.} 
\label{subsec:more-visuals}

Fig.\ref{fig:visu-pred-supp-mat-1}, \ref{fig:visu-pred-supp-mat-2} displays more visual results. Images show that our method yielded sharp and less noisy CAMs. However, multi-instances and complex scenes remain a challenge. 

In a separate file, we provide demonstrative videos which show the localization of objects. They highlight additional challenges not visible in still images, in particular:  
\begin{itemize}
    \item \textbf{Empty frames}. Some frames are without objects, yet still show class activations. This issue is the result of the weak global annotation of videos, and the assumption that all frames inherit that same label. It highlights the importance of detecting such frames and treating them accordingly, starting with classifier training. Discarding these frames will help the classifier better discern objects and reduce noisy CAMs. Without any additional supervision, WSVOL is still a difficult task. One possible solution is to leverage the per-class probabilities of the classifier to assess whether an object is present or not in a frame. Such likelihood can be exploited to discard frames or weight loss terms over them.
    \item \textbf{Temporal consistency}. Although our method brought a quantitative and qualitative improvement, there are some failure cases. In some instances, we observed inconsistencies between consecutive frames. This error is characterized by a large shift in localization where the bounding box (and the CAM activation), moves drastically. This often happens when objects become partially or fully occluded, the appearance of other instances, the scene becomes complex, or zooming in the image makes the object too large. Such setups lead to a \emph{sudden shift} of the CAM's focus, leading to an abrupt change of localization which is undesirable. This drawback is mainly inherited from the inference procedure that is achieved at a single frame and without accounting for spatiotemporal information allowing such errors. Future works may consider improving the inference procedure by leveraging spatiotemporal information at the expense of inference time.
\end{itemize}


\newpage
\clearpage

\begin{figure}
     \centering
     \begin{subfigure}[b]{0.35\textwidth}
         \centering
         \includegraphics[width=\textwidth]{tag}
     \end{subfigure}
     \\
     \begin{subfigure}[b]{0.35\textwidth} 
 \centering 
 \includegraphics[width=\textwidth]{102-train_video-0003_shot-000006_00022239.jpg.png} 
  \end{subfigure} 
 \\
\begin{subfigure}[b]{0.35\textwidth} 
 \centering 
 \includegraphics[width=\textwidth]{79-aeroplane_video-0009_shot-000160_00046106.jpg.png} 
  \end{subfigure} 
 \\
\begin{subfigure}[b]{0.35\textwidth} 
 \centering 
 \includegraphics[width=\textwidth]{1105-motorbike_video-0003_shot-000115_00010559.jpg.png} 
  \end{subfigure} 
 \\
\begin{subfigure}[b]{0.35\textwidth} 
 \centering 
 \includegraphics[width=\textwidth]{38-car_video-0005_shot-000037_00007617.jpg.png} 
  \end{subfigure} 
 \\
\begin{subfigure}[b]{0.35\textwidth} 
 \centering 
 \includegraphics[width=\textwidth]{19-aeroplane_video-0002_shot-000005_00003421.jpg.png} 
  \end{subfigure} 
 \\
\begin{subfigure}[b]{0.35\textwidth} 
 \centering 
 \includegraphics[width=\textwidth]{127-dog_data_0001_shots_020_frame0181.jpg.png} 
  \end{subfigure} 
 \\
\begin{subfigure}[b]{0.35\textwidth} 
 \centering 
 \includegraphics[width=\textwidth]{45-boat_data_0005_shots_075_frame0001.jpg.png} 
  \end{subfigure} 
 \\
\begin{subfigure}[b]{0.35\textwidth} 
 \centering 
 \includegraphics[width=\textwidth]{1243-train_video-0014_shot-000271_00116963.jpg.png} 
  \end{subfigure} 
 \\
\begin{subfigure}[b]{0.35\textwidth} 
 \centering 
 \includegraphics[width=\textwidth]{268-train_video-0008_shot-000033_00050464.jpg.png} 
  \end{subfigure} 
 \\
\begin{subfigure}[b]{0.35\textwidth} 
 \centering 
 \includegraphics[width=\textwidth]{93-boat_video-0005_shot-000020_00036648.jpg.png} 
  \end{subfigure} 
 \\
\begin{subfigure}[b]{0.35\textwidth} 
 \centering 
 \includegraphics[width=\textwidth]{30-horse_data_0004_shots_031_frame0012.jpg.png} 
  \end{subfigure} 
 \\
\begin{subfigure}[b]{0.35\textwidth} 
 \centering 
 \includegraphics[width=\textwidth]{232-horse_video-0001_shot-000012_00001811.jpg.png} 
  \end{subfigure} 
 \\
\begin{subfigure}[b]{0.35\textwidth} 
 \centering 
 \includegraphics[width=\textwidth]{641-boat_video-0011_shot-000125_00074943.jpg.png} 
  \end{subfigure} 
 \\
\begin{subfigure}[b]{0.35\textwidth} 
 \centering 
 \includegraphics[width=\textwidth]{240-motorbike_data_0002_shots_020_frame0041.jpg.png} 
  \end{subfigure} 
 \\
\begin{subfigure}[b]{0.35\textwidth} 
 \centering 
 \includegraphics[width=\textwidth]{1307-motorbike_video-0010_shot-000373_00045134.jpg.png} 
  \end{subfigure} 
 \\
\begin{subfigure}[b]{0.35\textwidth} 
 \centering 
 \includegraphics[width=\textwidth]{134-boat_data_0005_shots_006_frame0001.jpg.png} 
  \end{subfigure} 
 \\
\begin{subfigure}[b]{0.35\textwidth} 
 \centering 
 \includegraphics[width=\textwidth]{14-car_video-0005_shot-000037_00007250.jpg.png} 
  \end{subfigure} 
 \\
\begin{subfigure}[b]{0.35\textwidth} 
 \centering 
 \includegraphics[width=\textwidth]{1313-motorbike_video-0011_shot-000409_00052054.jpg.png} 
  \end{subfigure} 
 \\
\begin{subfigure}[b]{0.35\textwidth} 
 \centering 
 \includegraphics[width=\textwidth]{95-motorbike_data_0007_shots_011_frame0034.jpg.png} 
  \end{subfigure} 
 \\
\begin{subfigure}[b]{0.35\textwidth} 
 \centering 
 \includegraphics[width=\textwidth]{142-motorbike_video-0003_shot-000105_00009865.jpg.png} 
  \end{subfigure}
        \caption{Additional localization examples of test sets frames (\ytovone, \ytovtwodtwo). 
        \emph{Bounding boxes}: ground truth (green), prediction (red). The second column of each method is the predicted CAM over image.}
        \label{fig:visu-pred-supp-mat-1}
\end{figure}


\begin{figure}
     \centering
     \begin{subfigure}[b]{0.35\textwidth}
         \centering
         \includegraphics[width=\textwidth]{tag}
     \end{subfigure}
     \\
     \begin{subfigure}[b]{0.35\textwidth} 
 \centering 
 \includegraphics[width=\textwidth]{128-car_data_0008_shots_007_frame0001.jpg.png} 
  \end{subfigure} 
 \\
\begin{subfigure}[b]{0.35\textwidth} 
 \centering 
 \includegraphics[width=\textwidth]{884-motorbike_video-0003_shot-000056_00006519.jpg.png} 
  \end{subfigure} 
 \\
\begin{subfigure}[b]{0.35\textwidth} 
 \centering 
 \includegraphics[width=\textwidth]{767-train_video-0008_shot-000047_00054208.jpg.png} 
  \end{subfigure} 
 \\
\begin{subfigure}[b]{0.35\textwidth} 
 \centering 
 \includegraphics[width=\textwidth]{239-boat_data_0005_shots_005_frame0001.jpg.png} 
  \end{subfigure} 
 \\
\begin{subfigure}[b]{0.35\textwidth} 
 \centering 
 \includegraphics[width=\textwidth]{673-train_video-0008_shot-000053_00055950.jpg.png} 
  \end{subfigure} 
 \\
\begin{subfigure}[b]{0.35\textwidth} 
 \centering 
 \includegraphics[width=\textwidth]{191-boat_data_0005_shots_017_frame0001.jpg.png} 
  \end{subfigure} 
 \\
\begin{subfigure}[b]{0.35\textwidth} 
 \centering 
 \includegraphics[width=\textwidth]{1306-train_video-0008_shot-000038_00052298.jpg.png} 
  \end{subfigure} 
 \\
\begin{subfigure}[b]{0.35\textwidth} 
 \centering 
 \includegraphics[width=\textwidth]{896-train_video-0008_shot-000052_00055675.jpg.png} 
  \end{subfigure} 
 \\
\begin{subfigure}[b]{0.35\textwidth} 
 \centering 
 \includegraphics[width=\textwidth]{142-boat_data_0005_shots_078_frame0001.jpg.png} 
  \end{subfigure} 
 \\
\begin{subfigure}[b]{0.35\textwidth} 
 \centering 
 \includegraphics[width=\textwidth]{230-car_video-0002_shot-000018_00003388.jpg.png} 
  \end{subfigure} 
 \\
\begin{subfigure}[b]{0.35\textwidth} 
 \centering 
 \includegraphics[width=\textwidth]{954-cat_video-0007_shot-000045_00018934.jpg.png} 
  \end{subfigure} 
 \\
\begin{subfigure}[b]{0.35\textwidth} 
 \centering 
 \includegraphics[width=\textwidth]{1299-cat_video-0004_shot-000025_00011652.jpg.png} 
  \end{subfigure} 
 \\
\begin{subfigure}[b]{0.35\textwidth} 
 \centering 
 \includegraphics[width=\textwidth]{318-train_data_0012_shots_001_frame0200.jpg.png} 
  \end{subfigure} 
 \\
\begin{subfigure}[b]{0.35\textwidth} 
 \centering 
 \includegraphics[width=\textwidth]{1166-motorbike_video-0011_shot-000394_00050313.jpg.png} 
  \end{subfigure} 
 \\
\begin{subfigure}[b]{0.35\textwidth} 
 \centering 
 \includegraphics[width=\textwidth]{750-train_video-0003_shot-000006_00021988.jpg.png} 
  \end{subfigure} 
 \\
\begin{subfigure}[b]{0.35\textwidth} 
 \centering 
 \includegraphics[width=\textwidth]{989-car_video-0005_shot-000037_00007494.jpg.png} 
  \end{subfigure} 
 \\
\begin{subfigure}[b]{0.35\textwidth} 
 \centering 
 \includegraphics[width=\textwidth]{45-motorbike_video-0011_shot-000408_00051850.jpg.png} 
  \end{subfigure} 
 \\
\begin{subfigure}[b]{0.35\textwidth} 
 \centering 
 \includegraphics[width=\textwidth]{133-cow_data_0004_shots_006_frame0001.jpg.png} 
  \end{subfigure} 
 \\
\begin{subfigure}[b]{0.35\textwidth} 
 \centering 
 \includegraphics[width=\textwidth]{624-dog_video-0013_shot-000129_00056944.jpg.png} 
  \end{subfigure} 
 \\
\begin{subfigure}[b]{0.35\textwidth} 
 \centering 
 \includegraphics[width=\textwidth]{1190-motorbike_video-0003_shot-000056_00006492.jpg.png} 
  \end{subfigure} 
        \caption{Additional localization examples of test sets frames (\ytovone, \ytovtwodtwo). 
        \emph{Bounding boxes}: ground truth (green), prediction (red). The second column of each method is the predicted CAM over image.}
        \label{fig:visu-pred-supp-mat-2}
\end{figure}

% \clearpage
% \newpage

\FloatBarrier

\bibliographystyle{apalike}
\bibliography{main}

\end{document}
