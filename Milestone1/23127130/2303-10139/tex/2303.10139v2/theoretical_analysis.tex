% \newpage
\section{Analysis}
    In this section, we discuss the theoretical and computational aspects of our method.
    We first provide theoretical results supporting the hypothesis that good explanations of a global surrogate $\Psi$ also characterize good explanations of $\Phi$ --- in terms of faithfulness.
    Then, we discuss the convexity of the optimization problem DnX solves to extract explanations.
    We delegate proofs to the \autoref{append:proofs}.


        \label{sec:faith}

        Let $\mathcal{G}_u$ denote the subgraph of $\mathcal{G}$ induced by the $L$-hop neighborhood around node $u$.
        %
        We say an explanation $\mathcal{E}_u$ for a node $u$ is faithful with respect to $\Phi$ if: i) $\Phi$ outputs approximately the same predictions for $u$ regardless of using $\mathcal{E}_u$ to weigh the nodes of $\mathcal{G}_u$ or not; and ii) the same holds under small perturbations of $\mathcal{G}_u$. 
        %
        We can define a perturbation $\mathcal{G}_{u}^\prime$ of $\mathcal{G}_u$  by adding noise to $u$'s features or by randomly rewiring node $u$'s incident edges \citep{agarwal_probing_2022}.
        %
        In this work, we consider perturbations over node features. More precisely, this entails that $V(\mathcal{G}_{u}^\prime) = V(\mathcal{G}_u)$, $E(\mathcal{G}^\prime_u) = E(\mathcal{G}_u)$, and that features are corrupted by noise, i.e., $X^\prime_i = X_i + \epsilon_i$ for $i \in V(\mathcal{G}_u)$ and $\epsilon_i \in \mathbb{R}^{d}$. 
        

        \begin{definition}[Faithfulness]
            \label{def:faithfulness}
            Given a set $\mathcal{K}$ of perturbations of $\mathcal{G}_u$, an explanation $\mathcal{E}_u$ is \textit{faithful} to a model $f$ if
            \[\frac{1}{|\mathcal{K}| + 1} \sum_{ \mathcal{G}_{u}^\prime \in \mathcal{K} \cup \{\mathcal{G}_u\}} \left\lVert f(\mathcal{G}_{u}^\prime)- f(t(\mathcal{G}_{u}^\prime, \mathcal{E}_u))\right\rVert_2 \le \delta,\]
            where $\mathcal{G}_{u}^\prime$ is a possibly perturbed version of $\mathcal{G}_u$, $t$ is a function that applies the explanation $\mathcal{E}_u$ to the graph $\mathcal{G}_{u}^\prime$, and $\delta$ is a small constant \citep{agarwal_probing_2022}.
        \end{definition}



        \autoref{theo:bound_unfaithfulness} provides an upper bound on the  unfaithfulness of $\mathcal{E}_u$ with respect to the surrogate model $\Psi$. \autoref{theo:bound_unfaithfulness_2} extends this result to obtain a bound for $\mathcal{E}_u$ with respect to the model we originally want to explain, i.e., $\Phi$.
    
        \begin{lemma}[Unfaithfulness with respect to $\Psi$]
            \label{theo:bound_unfaithfulness}
            Given a node $u$ and a set $\mathcal{K}$ of perturbations, the unfaithfulness of the explanation $\mathcal{E}_u$ with respect to the prediction $Y_u^{(\Psi_\Theta)}$ of node $u$ is bounded as follows:
            \[\frac{1}{|\mathcal{K}| + 1} \sum_{\substack{ \mathcal{G}^\prime_{u} \in \\ \mathcal{K} \cup \{\mathcal{G}_u\}}} \left\lVert \Psi(\mathcal{G}_{u}^\prime)- \Psi(t(\mathcal{G}_{u}^\prime, \mathcal{E}_u))\right\rVert_2 \le \gamma \left\lVert \underset{\mathcal{E}_u}{\Delta} \widetilde A_u^L \right\rVert_2,\]
            where $\mathcal{G}_{u}^\prime$ is a possibly perturbed version of $\mathcal{G}_u$, $t$ is a function that applies the explanation $\mathcal{E}_u$ to the graph $\mathcal{G}_{u}^\prime$, $\gamma$ is a constant that depends on the model weights $\Theta$, node features $X$, and perturbation $\epsilon$. Furthermore, $\underset{\mathcal{E}_u}{\Delta} \widetilde A_u^L$ is the $u$-th row of the difference of the powered, normalized adjacency matrix $\widetilde A^L$ before and after applying the explanation $\mathcal{E}_u$.
        \end{lemma}
    
        \begin{proof}[Sketch of the proof.]
            We first show that
            \[\left\lVert \Psi(\mathcal{G}_{u})- \Psi(t(\mathcal{G}_{u}, \mathcal{E}_u))\right\rVert_2 \le \lVert (X \Theta)^\intercal \rVert_2 \left\lVert \widetilde A_u^L - \widetilde E_u^L \right\rVert_2\]
            by using Lipschitz continuity of the $\text{softmax}$ function and the compatibility property of the $L_2$ matrix norm.
            We repeat for $\mathcal{G}_u^\prime \in \mathcal{K}$, take the mean in $\mathcal{K} \cup \{\mathcal{G}_u\}$ and isolate $\left\lVert \underset{\mathcal{E}_u}{\Delta} \widetilde A_u^L \right\rVert_2 = \left\lVert \widetilde A_u^L - \widetilde E_u^L \right\rVert_2$.
            The complete proof is available in \autoref{append:proofs}.
        \end{proof}

        \begin{theorem}[Unfaithfulness with respect to $\Phi$]
            \label{theo:bound_unfaithfulness_2}
            Under the same assumptions of \autoref{theo:bound_unfaithfulness} and assuming the $L_2$ distillation error is bounded by $\alpha$, the unfaithfulness of the explanation $\mathcal{E}_u$ for the original model $\Phi$'s node $u$ prediction is bounded as follows:
            \begin{equation*}
                \begin{split}
                    \frac{1}{|\mathcal{K}| + 1} \sum_{\substack{\mathcal{G}_u^\prime \in \\ \mathcal{K} \cup \{\mathcal{G}_u\}}} \left\lVert \Phi(\mathcal{G}_{u}^\prime)- \Phi(t(\mathcal{G}_{u}^\prime, \mathcal{E}_u))\right\rVert_2 \le\ &\gamma \left\lVert \underset{\mathcal{E}_u}{\Delta} \widetilde A_u^L \right\rVert_2 \\
                    &+ 2\alpha.
                \end{split}
            \end{equation*}
        \end{theorem}

        Note that \autoref{theo:bound_unfaithfulness_2} establishes a bound on faithfulness that depends directly on the distillation error $\alpha$. Importantly, when $\Psi$ is a perfect approximation of $\Phi$, we retrieve upper-bound on the RHS of \autoref{theo:bound_unfaithfulness}.

        We note that Theorem 1 by \citet{agarwal_probing_2022} covers an upper bound for the unfaithfulness of GNN explanation methods. However, they do not cover the case in which the explanation is a (weighted) subset of nodes in the $L$-hop neighborhood of $u$, as in our method.

        
        For completeness, we also extend \autoref{theo:bound_unfaithfulness} and \autoref{theo:bound_unfaithfulness_2} to account for the (very often) probabilistic nature of the noise, i.e., for the case in which $\epsilon_i$ are random variables.
    
        \begin{lemma}[Probability bound on unfaithfulness \emph{w.r.t.} $\Psi$]
            \label{theo:prob_bound_unfaithfulness}
            Given a node $u$ and a set $\mathcal{K}$ of perturbations and assuming the perturbations are i.i.d. with distribution $\epsilon_i~\sim~\mathcal{N}(0, \sigma^2)$, the unfaithfulness of the explanation $\mathcal{E}_u$ with respect to the prediction $Y_u^{(\Psi_\Theta)}$ of node $u$ is bounded in probability as follows:
            \begin{equation*}
                \begin{split}
                    \mathbb{P}\left(
                    \frac{1}{|\mathcal{K}| + 1} \sum_{\substack{ \mathcal{G}^\prime_{u} \in \\ \mathcal{K} \cup \{\mathcal{G}_u\}}} \left\lVert \Psi(\mathcal{G}_{u}^\prime)- \Psi(t(\mathcal{G}_{u}^\prime, \mathcal{E}_u))\right\rVert_2 \le \xi \right) \ge \\
                    \ge F_{\chi_{|\mathcal{K}|nd}^2}\left(\frac{\xi - \gamma_1 \left\lVert \underset{\mathcal{E}_u}{\Delta} \widetilde A_u^L \right\rVert_2}{\gamma_2 \left\lVert \underset{\mathcal{E}_u}{\Delta} \widetilde A_u^L \right\rVert_2 \sigma} - |\mathcal{K}|\right)
                \end{split}
            \end{equation*}
            where $\gamma_1$ is a constant that depends on the model weights $\Theta$ and node features $X$, $\gamma_2$ is a constant that depends on the model weights $\Theta$, and $F_{\chi_{|\mathcal{K}|nd}^2}$ is the c.d.f. of a chi-square r.v. with $|\mathcal{K}| \times n \times d$ degrees of freedom where $(n, d)$ are the row- and column-wise dimensions of $X$.  
        \end{lemma}
        
        
        \begin{theorem}[Probability bound on unfaithfulness \emph{w.r.t.} $\Phi$]
            Under the same assumptions of \autoref{theo:prob_bound_unfaithfulness} and assuming the $L_2$ distillation error is bounded by $\alpha$, the unfaithfulness of the explanation $\mathcal{E}_u$ for the original model $\Phi$'s node $u$ prediction is bounded in probability as follows:
            \begin{equation*}
                \begin{split}
                    \mathbb{P}\left(
                    \frac{1}{|\mathcal{K}| + 1} \sum_{\substack{ \mathcal{G}^\prime_{u} \in \\ \mathcal{K} \cup \{\mathcal{G}_u\}}} \left\lVert \Phi(\mathcal{G}_{u}^\prime)- \Phi(t(\mathcal{G}_{u}^\prime, \mathcal{E}_u))\right\rVert_2 \le \xi \right) \ge \\
                    \ge F_{\chi_{|\mathcal{K}|nd}^2}\left(\frac{\xi - \gamma_1 \left\lVert \underset{\mathcal{E}_u}{\Delta} \widetilde A_u^L \right\rVert_2 - 2\alpha}{\gamma_2 \left\lVert \underset{\mathcal{E}_u}{\Delta} \widetilde A_u^L \right\rVert_2 \sigma} - |\mathcal{K}|\right)
                \end{split}
            \end{equation*}
            \label{theo:prob_bound}
        \end{theorem}

In \autoref{theo:prob_bound_unfaithfulness} and \autoref{theo:prob_bound}, when the variance $\sigma^2$ approaches zero, $\xi$ relinquishes its random nature and the probability in the RHS converges to one.
 We note that numerators in the RHS must be non-negative.

Recall DnX/FastDnX's pipeline  involves two steps: model distillation (\autoref{eq:destilador}) and explanation extraction (\autoref{eq:e2}). The former is done only once to learn the surrogate $\Psi$. The latter, however, must be executed for each node whose prediction we want to explain.  Then, gauging the cost of the extraction step may become a genuine concern from a practical point of view, especially for DnX, which implies solving an optimization problem repeatedly. Fortunately, the loss landscape of our extraction problem depends only on the shape of $\Psi$, and not on the original GNN $\Phi$ as in GNNExplainer. Since $\Psi$ is  an SGC, \autoref{eq:e2} is a convex program (\autoref{theo:convexity}) and we reach global optima using, e.g., gradient-based algorithms.
        
        \begin{theorem}[Convexity of DnX]
            \label{theo:convexity}
            The optimization problem of \autoref{eq:e2} is convex.
        \end{theorem}