\documentclass[twoside]{article}

\usepackage[accepted]{aistats2023}
\special{papersize = 8.5in, 11in}
\setlength{\pdfpageheight}{11in}
\setlength{\pdfpagewidth}{8.5in}
% \usepackage{aistats2023}
\usepackage[round]{natbib}
\renewcommand{\bibname}{References}
\renewcommand{\bibsection}{\subsubsection*{\bibname}}

\usepackage{algpseudocode}
\usepackage{algorithm}
\usepackage[usenames,dvipsnames]{xcolor} 
\usepackage{float}
\usepackage{listings}
\usepackage{enumitem}

\input{math.tex}
\usepackage{mathtools}
\usepackage{amsthm, amssymb}
\newcommand{\propositionautorefname}{Proposition}
% \renewcommand{\sectionautorefname}{Section}
\newcommand{\first}[1]{\mathbf{#1}}
\newcommand{\second}[1]{\underline{#1}}
\newcommand{\RomanNumeralCaps}[1]{\MakeUppercase{\romannumeral #1}}

% \newtheorem*{remark}{Remark}
\newtheorem{definition}{Definition}
\newtheorem{lemma}{Lemma}
\newtheorem{proposition}{Proposition}
\newtheorem{theorem}{Theorem}
\newtheorem{corollary}{Corollary}
\newtheorem{remark}{Remark}
\providecommand*\lemmaautorefname{Lemma}
\usepackage{multirow}

\definecolor{royal}{HTML}{1560BD}
\definecolor{light}{HTML}{1ea2a1}

\usepackage{hyperref}
\hypersetup{
    linkbordercolor=Melon,
    urlbordercolor=magenta,
    citebordercolor=Dandelion,
}

\begin{document}

\twocolumn[

\aistatstitle{Distill n' Explain: explaining graph neural networks using simple surrogates}

\aistatsauthor{ Tamara Pereira$^1$, Erik Nascimento$^1$, Lucas E. Resck$^2$, Diego Mesquita$^2$, Amauri Souza$^{1,3}$}


\aistatsaddress{ $^1$Federal Institute of Cear\'a $^2$Getulio Vargas Foundation $^3$Aalto university } ]

% \aistatsauthor{  Tamara Pereira \And Erik Nascimento \And  Lucas E. Resck \And Diego Mesquita \And Amauri Souza}

% \aistatsaddress{ Federal Institute of Cear\'a \And Federal Institute of Cear\'a  \And  Getulio Vargas Foundation \And Aalto university \And Federal Institute of Cear\'a } ]



\begin{abstract}
Explaining node predictions in graph neural networks (GNNs) often boils down to finding graph substructures that preserve predictions. Finding these structures usually implies back-propagating through the GNN, bonding the complexity (e.g., number of layers) of the GNN to the cost of explaining it. This naturally begs the question: \emph{Can we break this bond by explaining a simpler surrogate GNN?} To answer the question, we propose \emph{Distill n' Explain} (DnX). First, DnX learns a surrogate GNN via \emph{knowledge distillation}. Then, DnX extracts node or edge-level explanations by solving a simple convex program. We also propose FastDnX, a faster version of DnX that leverages the linear decomposition of our surrogate model. Experiments show that DnX and FastDnX often outperform state-of-the-art GNN explainers while being orders of magnitude faster. Additionally, we support our empirical findings with theoretical results linking the quality of the surrogate model (i.e., distillation error) to the faithfulness of explanations. 
\end{abstract}

\section{Introduction}

Graph neural networks (GNNs) \citep{Gori2005,scarselli2009} have become the pillars of representation learning on graphs. Typical GNNs resort to message passing on input graphs to extract meaningful node/graph representations for the task at hand. Despite the success of GNNs in many domains \citep{antibiotic_design,Gilmer2017,recommendersystems,ComplexPhysics}, their architectural design often results in models with limited interpretability. This naturally makes it hard to diagnose scenarios in which GNNs are fooled by confounding effects or align poorly with expert knowledge.

To mitigate this lack of interpretability, a popular strategy is to use post-hoc explanation methods \citep{Ribeiro2016,shap2017,Hima2022, Hima2022b, GraphLIME}. The idea is to increase model transparency by highlighting input/model elements that are particularly important for predictions, helping users to understand what is happening under the hood. 


There has been a recent outbreak of methods for explaining GNNs \citep{Yuan2022}. 
%
Although GNN explanations can come in different flavors \citep{GNNexplainer,subgraphx_icml21,Wang2021,Lucic2022,xgnn_kdd20}, they usually take the form of (minimal) substructures of input graphs that are highly influential to the prediction we want to explain. The seminal work of \citet[GNNExplainer]{GNNexplainer} proposes learning a \emph{soft} mask to weigh graph edges. To find meaningful masks, GNNExplainer maximizes the mutual information between the GNN predictions given the original graph and the masked one. To alleviate the burden of optimizing again whenever we want to explain a different node, \citet[PGExplainer]{PGExplainer} propose using node embeddings to parameterize the masks, i.e., amortizing the inference. 
Nonetheless, GNNExplainer and PGExplainer impose strong assumptions on our access to the GNN we are trying to explain. The former assumes we are able to back-propagate through the GNN. The latter further assumes that we can access hidden activations of the GNN. \citet[PGMExplainer]{PGMExplainer} relieve these assumptions by approximating the local behavior of the GNN with a probabilistic graphical model (PGM) over components, which can be used to rank the relevance of nodes and edges. On the other hand, getting explanations from PGMExplainer involves learning the structure of a PGM, and may not scale well.

In this work, we adopt the same black-box setting of \citet{PGMExplainer} but severely cut down on computational cost by extracting explanations from a \emph{global} surrogate model. In particular, we propose \emph{Distill n' Explain} (DnX). DnX uses knowledge distillation to learn a simple GNN $\Psi$, e.g. simple graph convolution~\citep[SGC]{SGC}, that mimics the behavior of the GNN $\Phi$ we want to explain. 
Then, it solves a simple convex program to find a mask that weighs the influence of each node in the output of $\Psi$.
%
We also propose FastDnX, a variant of DnX that leverages the linear nature of our surrogate to speed up the explanation procedure.
%
Notably, we only require evaluations of $\Phi$ to learn the surrogate $\Psi$ and, after $\Psi$ is fixed, we can use it to explain any node-level prediction. To back up the intuition that explaining a surrogate instead of the original GNN is a sensible idea, we provide a theoretical result linking the distillation quality to the faithfulness of our explanations.

Experiments on eight popular node classification benchmarks show that DnX and FastDnX often outperform GNN-, PG-, and PGM-Explainers. We also demonstrate that both DnX and FastDnX are much faster than the competitors. Remarkably, FastDnX presents a speedup of up to $65K\times$ over GNNExplainer. 
%
Finally, we discuss the limitations of current benchmarks and show that explainers capable of leveraging simple inductive biases can ace them.

\noindent\textbf{Our contributions} are three-fold:
\begin{enumerate}[itemsep=0pt]
    \item we propose a new framework for GNN explanations that treats  GNNs as black-box functions and hinges on explaining a simple surrogate model obtained through knowledge distillation;   
    \item we provide theoretical bounds on the quality of explanations based on these surrogates, linking the error in the distillation procedure to the faithfulness of the explanation;
    \item we carry out extensive experiments, showing that our methods outperform the prior art while running orders of magnitude faster.
\end{enumerate}


\input{method.tex}

\input{theoretical_analysis.tex}

\section{Additional related works}

\paragraph{Explanations for GNNs.}

The ever-increasing application of GNNs to support high-stake decisions on critical domains \citep{antibiotic_design,Luna2020,Pinion2021} has recently boosted interest in explainability methods for graph models.
%
\citet{Pope2019} first extended classical gradient-based explanation methods for GNNs. 
%
Importantly, \citet{GNNexplainer} introduced GNNExplainer and synthetic benchmarks that have been widely adopted to assess GNN explainers. 
%
Building on parameterized explainers by \citet{PGExplainer}, \citet{Wang2021} proposed ReFine to leverage both global information (e.g., class-wise knowledge) via pre-training and local one (i.e., instance specific patterns) using a fine-tuning process.  
%
\citet{Lucic2022,Bajaj2021} investigated counterfactual explanations for GNNs, aiming to find minimal perturbations to the input graph such that the prediction changes, e.g., using edge deletions.
%
\citet{DEGREE} proposed measuring the contribution of different components of the input graph to the GNN prediction by decomposing the information generation and aggregation mechanism of GNNs.
%
Recently, \citet{Games2022} introduced a structure-aware scoring function derived from cooperative game theory to determine node importance.
%
Explainability methods for GNNs have also been approached through the lens of causal inference \citep{Lin2021,Lin2022}.
%
For a more comprehensive coverage of the literature, we refer the reader to \citet{Yuan2022}.


\paragraph{Knowledge distillation.} Since the pivotal work of \citet{Hinton2015}, condensing the knowledge from  a possibly complex \emph{teacher} model into a simpler \emph{student} surrogate has been an active research topic~\citep[e.g.][]{Vadera, Malinin2020Ensemble, ryabinin2021scaling,Ba2022, Clayer}. Nonetheless, despite numerous works using  distillation in image domains~\citep[e.g.][]{Lamp2017, Arthur, Object}, the distillation of GNNs is still a blooming direction. \citet{destillGCN1} proposed the first method for GNN distillation, using a structure-preserving module to explicitly factor in the topological structure embedded by the teacher.
\citep{GCRD} proposed using contrastive learning to implicitly align the node embeddings of the student and the teacher in a common representation space.
%
\citet{Jing} combined the knowledge of complementary teacher networks into a single student using a dedicated convolutional operator and topological attribution maps.
%
\citet{Mscale} used an attention mechanism to weigh different teachers depending on the local topology of each node.


\input{experiments.tex}

\input{discussion.tex}

\section{Conclusion}
This work proposes \emph{DnX} as a simple and intuitive two-step framework for post-hoc explanation of GNNs. First, we distill the GNN into a simpler and more interpretable one, that serves as a global surrogate. Then, we leverage the simple structure of the surrogate to extract explanations. Experiments show that (Fast)DnX outperforms the prior art on a variety of benchmarks. Remarkably, our simple design allows FastDnX to run at least $200\times$ faster than relevant baselines on real-world tasks. 
%
Additionally, we provide theoretical results that justify our framework and support our empirical findings.
%
Besides advancing the current art, we hope this work will motivate other researchers to focus on developing compute-efficient explainability methods.

\section*{Acknowledgments}

This work was supported by the Silicon Valley Community Foundation (SVCF) through the Ripple impact fund, the Funda\c{c}\~ao de Amparo \`a Pesquisa do Estado do Rio de Janeiro (FAPERJ), the Funda\c{c}\~ao Cearense de Apoio ao Desenvolvimento Científico e Tecnológico (FUNCAP), the Coordena\c{c}\~ao de Aperfei\c{c}oamento de Pessoal de N\'ivel Superior (CAPES), and the Getulio Vargas Foundation's school of applied mathematics (FGV EMAp).


 \newpage

\bibliographystyle{plainnat}
\bibliography{references}

\appendix
\input{appendix.tex}

\end{document}
x`