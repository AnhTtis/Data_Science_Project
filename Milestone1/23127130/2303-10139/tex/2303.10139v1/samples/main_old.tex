%%
%% This is file `sample-sigconf.tex',
%% generated with the docstrip utility.
%%
%% The original source files were:
%%
%% samples.dtx  (with options: `sigconf')
%% 
%% IMPORTANT NOTICE:
%% 
%% For the copyright see the source file.
%% 
%% Any modified versions of this file must be renamed
%% with new filenames distinct from sample-sigconf.tex.
%% 
%% For distribution of the original source see the terms
%% for copying and modification in the file samples.dtx.
%% 
%% This generated file may be distributed as long as the
%% original source files, as listed above, are part of the
%% same distribution. (The sources need not necessarily be
%% in the same archive or directory.)
%%
%%
%% Commands for TeXCount
%TC:macro \cite [option:text,text]
%TC:macro \citep [option:text,text]
%TC:macro \citet [option:text,text]
%TC:envir table 0 1
%TC:envir table* 0 1
%TC:envir tabular [ignore] word
%TC:envir displaymath 0 word
%TC:envir math 0 word
%TC:envir comment 0 0
%%
%%
%% The first command in your LaTeX source must be the \documentclass command.
\documentclass[sigconf,anonymous,review]{acmart}

%%
%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    Bib\TeX}}}

%% Rights management information.  This information is sent to you
%% when you complete the rights form.  These commands have SAMPLE
%% values in them; it is your responsibility as an author to replace
%% the commands and values with those provided to you when you
%% complete the rights form.
\setcopyright{acmcopyright}
\copyrightyear{2018}
\acmYear{2018}
\acmDOI{XXXXXXX.XXXXXXX}

%% These commands are for a PROCEEDINGS abstract or paper.
\acmConference[Conference acronym 'XX]{Make sure to enter the correct
  conference title from your rights confirmation emai}{June 03--05,
  2018}{Woodstock, NY}
\acmPrice{15.00}
\acmISBN{978-1-4503-XXXX-X/18/06}

% \usepackage{lmodern}
% Lucas: This package was originally uncommented, however the plus sign would not appear in math mode.
\usepackage{babel}
\usepackage{adjustbox}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{float}
\usepackage{emoji}
\usepackage[caption = false]{subfig}

% \usepackage{amssymb}
\usepackage{amsmath,bm,amsfonts}
\usepackage{mathtools}
\usepackage{graphicx,wrapfig}
\DeclareMathOperator*{\argmin}{arg\,min}
\newcommand{\softmax}{\mathrm{softmax}}

%%%% Submission ID.
%% Use this when submitting an article to a sponsored event. You'll
%% receive a unique submission ID from the organizers
%% of the event, and this ID should be used as the parameter to this command.
%%\acmSubmissionID{123-A56-BU3}

%%
%% For managing citations, it is recommended to use bibliography
%% files in BibTeX format.
%%
%% You can then either use BibTeX with the ACM-Reference-Format style,
%% or BibLaTeX with the acmnumeric or acmauthoryear sytles, that include
%% support for advanced citation of software artefact from the
%% biblatex-software package, also separately available on CTAN.
%%
%% Look at the sample-*-biblatex.tex files for templates showcasing
%% the biblatex styles.
%%

%%
%% The majority of ACM publications use numbered citations and
%% references.  The command \citestyle{authoryear} switches to the
%% "author year" style.
%%
%% If you are preparing content for an event
%% sponsored by ACM SIGGRAPH, you must use the "author year" style of
%% citations and references.
%% Uncommenting
%% the next command will enable that style.
%%\citestyle{acmauthoryear}

%%
%% end of the preamble, start of the body of the document source.
\begin{document}

%%
%% The "title" command has an optional parameter,
%% allowing the author to define a "short title" to be used in page headers.
\title{Distill n' Explain: explaining graph neural nets with a still \emoji{alembic}}

%%
%% The "author" command and its associated commands are used to define
%% the authors and their affiliations.
%% Of note is the shared affiliation of the first two authors, and the
%% "authornote" and "authornotemark" commands
%% used to denote shared contribution to the research.
\author{Ben Trovato}
\authornote{Both authors contributed equally to this research.}
\email{trovato@corporation.com}
\orcid{1234-5678-9012}
\author{G.K.M. Tobin}
\authornotemark[1]
\email{webmaster@marysville-ohio.com}
\affiliation{%
  \institution{Institute for Clarity in Documentation}
  \streetaddress{P.O. Box 1212}
  \city{Dublin}
  \state{Ohio}
  \country{USA}
  \postcode{43017-6221}
}

\author{Lars Th{\o}rv{\"a}ld}
\affiliation{%
  \institution{The Th{\o}rv{\"a}ld Group}
  \streetaddress{1 Th{\o}rv{\"a}ld Circle}
  \city{Hekla}
  \country{Iceland}}
\email{larst@affiliation.org}

\author{Valerie B\'eranger}
\affiliation{%
  \institution{Inria Paris-Rocquencourt}
  \city{Rocquencourt}
  \country{France}
}

\author{Aparna Patel}
\affiliation{%
 \institution{Rajiv Gandhi University}
 \streetaddress{Rono-Hills}
 \city{Doimukh}
 \state{Arunachal Pradesh}
 \country{India}}

\author{Huifen Chan}
\affiliation{%
  \institution{Tsinghua University}
  \streetaddress{30 Shuangqing Rd}
  \city{Haidian Qu}
  \state{Beijing Shi}
  \country{China}}

\author{Charles Palmer}
\affiliation{%
  \institution{Palmer Research Laboratories}
  \streetaddress{8600 Datapoint Drive}
  \city{San Antonio}
  \state{Texas}
  \country{USA}
  \postcode{78229}}
\email{cpalmer@prl.com}

\author{John Smith}
\affiliation{%
  \institution{The Th{\o}rv{\"a}ld Group}
  \streetaddress{1 Th{\o}rv{\"a}ld Circle}
  \city{Hekla}
  \country{Iceland}}
\email{jsmith@affiliation.org}

\author{Julius P. Kumquat}
\affiliation{%
  \institution{The Kumquat Consortium}
  \city{New York}
  \country{USA}}
\email{jpkumquat@consortium.net}

%%
%% By default, the full list of authors will be used in the page
%% headers. Often, this list is too long, and will overlap
%% other information printed in the page headers. This command allows
%% the author to define a more concise list
%% of authors' names for this purpose.
\renewcommand{\shortauthors}{Trovato et al.}

%%
%% The abstract is a short summary of the work to be presented in the
%% article.
\begin{abstract}
Explaining node predictions in graph neural networks (GNNs) often boils down to finding graph substructures that preserve predictions.
%
Finding these structures usually implies back-propagating through the GNN, bonding the complexity (e.g., number of layers) of the GNN to the cost of explaining it.
%
This naturally begs the question: \emph{Can we break this bond by explaining a simpler surrogate GNN? }
%
To answer the question, we propose \emph{Distill n' Explain} (DnX). 
%
First, DnX learns a surrogate GNN via \emph{knowledge distillation}. Then, DnX extracts node or edge-level explanations by solving a simple convex program.
%
Experiments show that, using linear graph surrogates, DnX is competitive with, if not better than, state-of-the-art GNN explainers but is up to $\approx20$ times faster.
%
These results also suggest that the current benchmarking datasets are too simple, and we lack complex tasks in which more expressive GNNs may be an asset. 
%
In addition, we support our empirical findings with theoretical results linking the quality of the surrogate model (i.e., distillation error) to the faithfulness of DnX's explanations.  
%

\end{abstract}

%%
%% The code below is generated by the tool at http://dl.acm.org/ccs.cfm.
%% Please copy and paste the code instead of the example below.
%%
\begin{CCSXML}
<ccs2012>
 <concept>
  <concept_id>10010520.10010553.10010562</concept_id>
  <concept_desc>Computer systems organization~Embedded systems</concept_desc>
  <concept_significance>500</concept_significance>
 </concept>
 <concept>
  <concept_id>10010520.10010575.10010755</concept_id>
  <concept_desc>Computer systems organization~Redundancy</concept_desc>
  <concept_significance>300</concept_significance>
 </concept>
 <concept>
  <concept_id>10010520.10010553.10010554</concept_id>
  <concept_desc>Computer systems organization~Robotics</concept_desc>
  <concept_significance>100</concept_significance>
 </concept>
 <concept>
  <concept_id>10003033.10003083.10003095</concept_id>
  <concept_desc>Networks~Network reliability</concept_desc>
  <concept_significance>100</concept_significance>
 </concept>
</ccs2012>
\end{CCSXML}

\ccsdesc[500]{Computer systems organization~Embedded systems}
\ccsdesc[300]{Computer systems organization~Redundancy}
\ccsdesc{Computer systems organization~Robotics}
\ccsdesc[100]{Networks~Network reliability}

%%
%% Keywords. The author(s) should pick words that accurately describe
%% the work being presented. Separate the keywords with commas.
\keywords{datasets, neural networks, gaze detection, text tagging}
%% A "teaser" image appears between the author and affiliation
%% information and the body of the document, and typically spans the
%% page.
%\begin{teaserfigure}
%  \includegraphics[width=\textwidth]{sampleteaser}
%  \caption{Seattle Mariners at Spring Training, 2010.}
%  \Description{Enjoying the baseball game from the third-base
%  seats. Ichiro Suzuki preparing to bat.}
%  \label{fig:teaser}
%\end{teaserfigure}

%%
%% This command processes the author and affiliation and title
%% information and builds the first part of the formatted document.
\maketitle

\section{Introduction}
%GNN é foda...mas sofre de falta de interpretabilidade...entao é  importante explicabilidade.

Graph neural networks (GNNs) \citep{} are the pillars of representation learning on graphs. Typical GNNs resort to message passing on input graphs to encode them into node/graph embeddings. Despite the success of GNNs in many domains \citep{}, their architectural design often results in models with limited interpretability. Naturally, this makes it hard to diagnose scenarios in which GNNs are fooled by confounding effects or poorly align with expert knowledge.

To alleviate these issues, there has been a recent outbreak of methods for explaining GNN predictions. Although explanations can come at different flavors, they are usually given in the form of (minimal) components of input graphs that preserve/affect predictions. FALAR DE GLOBAL/LOCAL EXPLANATION. 

The most popular type of explanation for GNNs consists of finding small substructures that are highly influential to the node-level prediction we want to explain. The seminal work of \citet[GNNexplainer]{GNNexplainer} proposes learning a \emph{soft} mask to weigh graph edges. To find meaningful masks, GNNexplainer maximizes the mutual information between the GNN predictions given the original graph and the masked one. To alleviate the burden of optimizing again whenever we want to explain a different node, \citet[PGexplainer]{PGExplainer} propose using node embeddings to parameterize the masks, i.e., amortizing the inference. 
Nonetheless, GNNexplainer and PGexplainer impose strong assumptions on our access to the GNN we are trying to explain. The former assumes we are able to back-propagate through the GNN. The latter further assumes that we can draft node embeddings from the entrails of that GNN. \citet[PGMExplainer]{PGMExplainer} relieve these assumptions by approximating the local behavior of the GNN with a probabilistic graphical model (PGM) over components, which can be used to rank the relevance of nodes and edges. On the other hand, getting explanations from PGMExplainer involves learning the structure of a PGM, and may not scale well.

In this work, we adopt the same black-box setting of \citet{PGMExplainer} but severely cut down on computational cost by extracting explanations from a \emph{global} surrogate model. In particular, we use knowledge distillation to learn a simple GNN $\psi$ (e.g., simple graph convolution\citep{SGC}) that mimics the behavior of the GNN $\phi$ we want to explain. 
Then, we solve a simple convex program to find a mask that weighs the influence of each node in the output of $\psi$.
Notably, we only require evaluations of $\phi$ to learn the surrogate $\psi$ and, after $\psi$ is fixed, we can use it to explain any node-level prediction. 

Our experiments ...

\paragraph{Contributions.}


\newpage
\section{Background}

\paragraph{Notation. } We define a graph $G = (\mathcal{V}, \mathcal{E})$, with a set of nodes  $\mathcal{V} = \{1, \ldots, n\}$ and a set of edges $\mathcal{E} \subseteq \mathcal{V}\times \mathcal{V}$. We denote the adjacency matrix of $G$ by $\bm{A} \in \mathbb{R}^{n\times n}$, i.e., $A_{ij}$ is one if $(i,j) \in \mathcal{E}$ and zero otherwise. Let $\bm{D}$ be the diagonal degree matrix of $G$, i.e., $D_{i i} \coloneqq \sum_{j} A_{i j}$. We also define the \emph{normalized} adjacency matrix as $\widetilde{\bm{A}} = \bm{D}^{-1/2} (\bm{A} + \bm{I}) \bm{D}^{-1/2}$, where $\bm{I}$ is the $n$-dimensional identity matrix. Furthermore, let  $\bm{X} \in \mathbb{R}^{n \times d}$ be a matrix of $d$-dimensional node features. Throughout this work, we also often denote a graph $G$ using the pair $(\bm{A}, \bm{X})$.


\subsection{Graph neural networks}
% Graphs have the ability to describe complex systems, for example, they can represent social networks, proteins and their biological interactions or iterations between atoms and molecules.


% Due to the structure of graphs, traditional neural network models such as convolutional neural networks (CNNs) are not able to learn meaningful graph representations, since applying operations such as convolution in the domain of graphs is not a trivial task.

GNNs have gained traction over the last years due to their potential to extract meaningful graph representations while still preserving invariances (e.g, to node permutations). In general, modern GNNs apply a series of convolutions over the node states (initially equal to node features), after which we have refined representations for each node. Then, we can use these representations in downstream tasks.


The output of a (fully-convolutional) GNN after $\ell$ layers can be written as a function $\bm{H}^{(\ell)} = f(\bm{H}^{(\ell-1)},\bm{A})$, where $\bm{H}^{(\ell)}$ is matrix with $n$ rows, one for each node in $\mathcal{V}$.  Specific GNNs differ essentially in how they define $f(\cdot,\cdot)$. After a suitable number of layers, say $L$, we can apply a multi-layer perceptron (MLP) on top of node embeddings to get predictions. If we consider node classification, the logits for all nodes are given by $\bm{Y} = \text{MLP}(\bm{H}^{(L)})$, where $ \bm{Y} \in \mathbb{R}^{n \times C} $ and $C$ is the number of classes in our task.


The remaining of this subsection covers two GNN models: graph convolutional networks (GCNs)~\cite{GCN} and simplified graph convolutions (SGCs)~\cite{SGC}. The former is arguably the most popular GNN in the literature and is used profusely throughout our experiments. The latter is a linear graph model, which will be an asset for our DnX. 

\paragraph{Graph convolutional networks (GCNs)~\cite{GCN}} are multi-layer architectures in which nodes repeatedly gather their neighbors' states, subsequently combining them using a symmetric weighting scheme. Then, each node updates its state in a recurrent fashion using the result from neighborhood aggregation. Similarly to conventional feedforward networks, GCNs can be seen as a sequence of linear transformations followed by non-linear activation functions (e.g. ReLU). Denoting the weights of the $\ell$-th GCN layer by $\bm{\Theta}^{(\ell)}$ and the activation function as $\sigma$, we can write compactly the output of that layer as:


%Os autores desenvolveram um modelo que é a  aproximação polinomial de primeira ordem  da convolução espectral em grafo  na forma de uma rede de passagem de mensagens, onde a informação é propagada ao longo dos nós vizinhos. 

% GCN adopts a single  parameter matrix per layer, in addition,  uses the normalized adjacency matrix  of the graph $\widetilde{\bm{A}}$, so $\bm{H}^{(\ell)}$ computes recursively as follows

% A GCN usa apenas uma única matriz de parâmetros por camada, além disso, utiliza a matriz normalizada do grafo $\widetilde{\bm{A}}$, assim $\bm{H}^{(\ell)}$ é calculado recursivamente da seguinte forma 

\begin{equation}
    \bm{H}^{(\ell)} = \sigma\left(\widetilde{\bm{A}} \bm{H}^{(\ell-1)} {\bm{\Theta}^{(\ell)}}\right),\label{eq:GCN}
\end{equation}
% where $\bm{H}^{(0)}=\bm{X}$ (i.e , original input features) and $\sigma(\cdot)$ is a nonlinear activation function, for example, a Rectified Linear Unit (ReLU).
where $\bm{H}^{(\ell-1)}$ is the output of the previous layer and $\bm{H}^{(0)}$ equals the original features, i.e., $\bm{H}^{(0)} =\bm{X}$.



\paragraph{Simple graph convolution (SGC)}~\cite{SGC} is a simplification of GCN. We can derive SGC by removing the activation functions from intermediate layers and subsequently collapsing all weight matrices into one. To make it more concrete, recall the we can use the recursion in Equation (\ref{eq:GCN}) to write the  output of an $\ell$-layer  GCN  as:
\begin{align}
    \bm{H}^{(\ell)}   & =  \sigma\left(\widetilde{\bm{A}} \bm{H}^{(\ell-1)} \Theta^{(\ell)}\right), \\ 
                  & =  \sigma\left(\widetilde{\bm{A}} \,\sigma\left(\cdots \,\sigma\left(\widetilde{\bm{A}}\,\sigma\left(\widetilde{\bm{A}} \bm{X} \bm{\Theta}^{(1)}\right)\bm{\Theta}^{(2)}\right) \dots\right) \bm{\Theta}^{(\ell)}\right)
\end{align}
% where the  matrix $\bm{H}^{(\ell)} \in \mathbb{R}^{n \times d^{(\ell)}}$ comprises the representations calculated in the $\ell$-th layer.  
% $\bm{\Theta}^{(\ell)} \in \mathbb{R}^{d^{(\ell-1)} \times d^{(\ell)}}$ denotes the model parameters, $d^{(\ell)}$ denotes the dimensionality of the features in the $\ell$-th layer,  $\sigma(\cdot)$ is a nonlinear activation function, for example, a Rectified Linear Unit (ReLU) and finally $\widetilde{\bm{A}}$ is normalized adjacency matrix previously defined.
%
Removing the intermediate non-linear activations from the equation above leaves us with:
\begin{align}
\bm{H}^{(\ell)} &= \sigma\left(\widetilde{\bm{A}} \cdots \widetilde{\bm{A}}\widetilde{\bm{A}} \bm{X} \bm{\Theta}^{(1)} \bm{\Theta}^{(2)}\cdots  \bm{\Theta}^{(\ell)}\right)\\
&= \sigma\left(\widetilde{\bm{A}}^{\ell} \bm{X} \bm{\Theta}^{(1)} \bm{\Theta}^{(2)}\cdots  \bm{\Theta}^{(\ell)}\right).
\end{align}

To finish the derivation, we substitute the product $\bm{\Theta}^{(1)}\bm{\Theta}^{(2)} \cdots \bm{\Theta}^{(\ell)}$ by a single weight matrix $\bm{\Theta}$. Therefore, we wind up with the node embeddings for an $L$-layer SGC:
\begin{equation}
\bm{H} = \sigma\left(\widetilde{\bm{A}}^{L} \bm{X} \bm{\Theta}\right).     
\end{equation}

Notably, Wu \emph{et al.}~\cite{SGC} showed that  SGC often performs similarly to or better than GCN in a variety of node classification tasks. On top of that, training SGCs is computationally more than GCNs andd SGC models comprise significantly fewer parameters.

\section{Distill n' explain}
We now introduce ConveX --- a new method for node-level explanation of graph neural networks. ConveX follows a two-step procedure: i) we first fit a simple GNN to approximate the predictions of the model to be explained (distillation phase); ii) then, we explain the simplified GNN by solving an optimization problem that has a simple loss landscape (i.e., it is convex if the GNN is linear). 

Hereafter, we denote the GNN to be explained by $\Phi$. The matrix $\bm{\hat{Y}} = \Phi(G)$ comprises the predictions for all nodes in $G$ --- the $i$-th row of $\bm{\hat{Y}}$ contains the class predictions of node $i$.
%
Given the GNN $\Phi$ and its predictions $\bm{\hat{Y}}$, the interest is in finding an explainer model capable of identifying the most influential information for the prediction $\bm{y}_i$ from $\Phi$ to node $i$.
In this work, the GNN $\Phi$ is considered a black-box model, that is, the explainer model does not have access to any internal information, such as parameters or hidden representations of the model, but only to the input and output of the GNN.

\subsection{Knowledge distillation}
Despite the success of deep learning models, most of these models have high complexity due to the need to adjust a large number of parameters. With that in mind, the approach called knowledge distillation~\cite{Hinton2015DistillingTK} arose, in which the objective is to distill the knowledge of a complex network through a simple and interpretable model. The use of knowledge distillation for models of neural networks in graphs has been addressed in recent works~\cite{yang2020distilling,Zhou_2021_ICCV} that validate the efficiency of these strategies.

In this work, we use SGC~\cite{SGC}  to approximate the predictions obtained with the GNN $\Phi$.
Formally, the distilled model (SGC),  here denoted by $\Psi$, receives the input graph $G=(\bm{A}, \bm{X})$ and provides class predictions
$\hat{\bm{Y}}^{(\Psi)} = \Psi(G) = \softmax(\widetilde{\bm{A}}^L \bm{X} \bm{\Theta})$, where $\bm{\Theta}$ is the matrix of model parameters, $L$ is the number of layers of the GNN $\Phi$, and the softmax function acts row-wise. 

The distillation process consists of adjusting the parameters $\bm{\Theta}$ of the SGC model $\Psi$ so that it approximates the predictions of the network to be explained $\Phi$. This can be achieved by minimizing the Kullback-Leibler divergence $\mathrm{KL}$ between the predictions of $\Phi$ and $\Psi_{\bm{\Theta}}$. 
%
Consider that $\hat{\bm{y}}_i^{(\Psi_{\bm{\Theta}})}$ and $\hat{\bm{y}}_i^{(\Phi)}$ denote the class predictions for node $ i$ from the $\Psi_{\bm{\Theta}}$ and $\Phi$ models, respectively. More concisely, our distillation process consists in solving: 
\begin{equation}
\min_{\bm{\Theta}}~ \left\{ \sum_i \mathrm{KL} \left( \hat{\bm{y}}_i^{(\Phi)}, \hat{\bm{y}}_i^{(\Psi_\Theta)} \right) = 
\sum_{i\in \mathcal{V}} \sum_{c=1}^{C} \hat{y}_{ic}^{(\Phi)} \log \frac{\hat{y}_{ic}^{(\Phi)}}{\hat{y}_{ic}^{(\Psi_\Theta)} } \right\}.
\label{eq:destilador}
\end{equation}

%solucionar o seguinte problema de otimização

% \begin{equation}
% \argmin_{\bm{\Theta}}\mathcal{L} \left(\softmax(\bm{\zeta}(\bm{X}, \bm{A})), \bm{\Phi}(\bm{X}, \bm{A}) \right)
% \label{eq:destilador}
% \end{equation}

\subsection{Obtaining node-level explanations}

Intuitively, finding a good explanation $\bm{\mathcal{E}}$ for a prediction $\hat{\bm{y}}_i$ --- $\Phi$'s prediction for node $i \in \mathcal{V}$ given a graph $G$  --- can be seen as finding the smallest subgraph $G_\mathcal{E}$ of $G$ containing the nodes that influence the most that prediction. That being said, we define $\bm{\mathcal{E}}$ as an $n$-dimensional vector of indicator variables, one for each node in $G$'s vertex set $\mathcal{V}$. 

Since we focus on explaining $\Psi$ as a surrogate of $\Phi$, we start off by formulating our problem as finding the explanation $\bm{\mathcal{E}}$ the minimizes a discrepancy between output $\hat{\bm{y}}_i^{(\Psi)}$ of $\Psi$ given the original graph and the prediction using only the graph induced by $\bm{\mathcal{E}}$:
\begin{equation}
\min_{\bm{\mathcal{E}}\in\{0,1\}^n}\parallel \widetilde{\bm{A}}^{L}_{i}\mathrm{diag}(\bm{\mathcal{E}})\bm{X}\bm{\Theta} - \widetilde{\bm{A}}^{L}_{i}\bm{X}\bm{\Theta} \parallel_{2}^{2},
%- \bm{\zeta}(\bm{X},\bm{A}) \parallel_{2}
\label{eq:e1}
\end{equation}
where $\widetilde{\bm{A}}^{L}_{i}$ denotes the $i$-th row of the matrix $\widetilde{\bm{A}}^{L}$. Nonetheless, the formulation in Equation (\ref{eq:e1}) has a major issue: it does not impose any budget on our node selection, admitting trivial solutions like $\bm{\mathcal{E}} = \{1\}^n$. To solve this and simultaneously avoid binary optimization, we replace the search space by the simplex $\Delta = \{\bm{r} \in \mathbb{R}^n : \sum_i r_i = 1, \forall_i r_i \geq 0 \}$. Implementing this change and re-arranging computations, we wind up with:
\begin{equation}
\min_{\bm{\mathcal{E}} \in \Delta}\Big\| \widetilde{\bm{A}}^{L}_{i}\left(\mathrm{diag}(\bm{\mathcal{E}}) - \bm{I}_n\right)\bm{X}\bm{\Theta} \Big\|_{2}^{2},
%- \bm{\zeta}(\bm{X},\bm{A}) \parallel_{2}
\label{eq:e2}
\end{equation}
where $\bm{I}_n$ is the $n$-dimensional identity matrix. Notably, $\Delta$ is a convex set. It is also easy to prove the objective function in Equation (\ref{eq:e2}) is a quadratic program. 
To this end let us denote the objective function above by $f$. Then, it follows that:
\begin{align*}
   f(\bm{\mathcal{E}}) &= \Big\| \widetilde{\bm{A}}^{L}_{i}\left(\mathrm{diag}(\bm{\mathcal{E}}) - \bm{I}_n\right)\bm{X}\bm{\Theta} \Big\|_{2}^{2}\\
   &= \Big\| \widetilde{\bm{A}}^{L}_{i}\mathrm{diag}(\bm{\mathcal{E}}) \bm{X}\bm{\Theta} \Big\|_{2}^{2} - 2 \left( \bm{\mathcal{E}}^\intercal  \mathrm{diag}\left(\left(\widetilde{\bm{A}}^{L}_{i}\right)^\intercal\right) \bm{X}\bm{\Theta}^\intercal  \widetilde{\bm{A}}^{L}_{i} \bm{X}\bm{\Theta}\right)+ \delta\\
   &=  \Big\| \widetilde{\bm{A}}^{L}_{i}\mathrm{diag}(\bm{\mathcal{E}}) \bm{X}\bm{\Theta} \Big\|_{2}^{2} + \bm{\mathcal{E}}^\intercal \bm{c} + \delta\\
   &= \bm{\mathcal{E}}^\intercal  \mathrm{diag}\left(\left(\widetilde{\bm{A}}^{L}_{i}\right)^\intercal\right) \bm{X} \bm{\Theta} \bm{\Theta}^\intercal \bm{X}^\intercal \mathrm{diag}\left(\left(\widetilde{\bm{A}}^{L}_{i}\right)^\intercal\right) \bm{\mathcal{E}} + \bm{\mathcal{E}}^\intercal \bm{c} + \delta\\
   &= \bm{\mathcal{E}}^\intercal  \bm{Q} \bm{\mathcal{E}} + \bm{\mathcal{E}}^\intercal \bm{c} + \delta
\end{align*}
which is the cannonic quadratic form with $\delta$ being a constant and
\begin{align*}
    &\bm{c} = - 2    \mathrm{diag}\left(\left(\widetilde{\bm{A}}^{L}_{i}\right)^\intercal\right) \bm{X}\bm{\Theta}^\intercal  \widetilde{\bm{A}}^{L}_{i} \bm{X}\bm{\Theta},\\
    &\bm{Q} = \mathrm{diag}\left(\left(\widetilde{\bm{A}}^{L}_{i}\right)^\intercal\right) \bm{X} \bm{\Theta} \bm{\Theta}^\intercal \bm{X}^\intercal \mathrm{diag}\left(\left(\widetilde{\bm{A}}^{L}_{i}\right)^\intercal\right),
\end{align*}
and since $\bm{Q}$ has the form $\bm{P}^\intercal \bm{P}$, it is positive semidefinite, and our optimization problem in Equation (\ref{eq:e2}) is convex. 
\textcolor{red}{}

Since a conventional GNN layer uses first order information (i.e., the immediate neighborhood) to update node representations, only nodes at distance $\leq L$ influence each other's prediction in a GNN with depth $L$. Therefore, $G_\mathcal{E}$ must be a subgraph of the graph induced by $i$ and its $L$-neighborhood in $G$. For this reason, we mask out nodes outside this neighborhood, holding their importance at zero. For ease of implementation, we solve Equation (\ref{eq:e2}) in the unconstrained $\mathbb{R}^n$, mapping vectors from  $\mathbb{R}^n$ to $\Delta$ using the softmax function.
%
After optimizing for $\bm{\mathcal{E}}$, the values in $\bm{\mathcal{E}}$ serve as a ranking for the importance of each node.

\input{theoretical_analysis.tex}

\newpage
\section{Experiments}

\subsection{Datasets}

\paragraph{Synthetic datasets.} To evaluate the efficiency of the method we consider six synthetic datasets for node-level explanation of GNNs: BA-House-Shapes, BA-Community, BA-Grids, Tree-Cycles,  Tree-Grids, and BA-Bottle-Shaped. We note that these datasets are available in \cite{GNNexplainer} and \cite{PGMExplainer}. 



Datasets have a single graph consisting of a combination of multiple copies of the same motif (i.e. subgraph with a specific structural pattern) connected to base subgraphs. For the datasets, BA-House-Shapes, BA-Community, BA-Grids, and BA-Bottle-Shaped the base subgraphs are randomly generated using the Barabási– Albert (BA)\cite{Barabasi1999} model. For datasets Tree-Cycles and Tree-Grids, the base subgraphs are an 8-level balanced binary tree.

%
The class of each motif-node depends only on elements belonging to the same motif, and consequently, the explanation associated with any node must include only motif elements, i.e., it cannot include base nodes.
%
Therefore, the base subgraphs denote information irrelevant to the prediction of any node.


\paragraph{Real-world datasets. }
We also use two real-world datasets: Bitcoin-Alpha and Bitcoin-OTC~\cite{bitcoin-otc-alpha2016,bitcoin-otc-alpha2018}. These datasets denote networks in which nodes correspond to user accounts that trade Bitcoin. A directed edge $(u,v)$ (between users $u$ and $v$) denotes the degree of reliability assigned by $u$ to $v$, i.e., each edge has an associated score denoting the degree of trust.
%
The Bitcoin-Alpha and Bitcoin-OTC networks have 3783 and 5881 accounts, respectively. Platform members rate other members on a scale from $-10$ (total distrust) to $+10$ (total trust). Each account is labeled as trusted or untrusted based on the ratings of other members. In addition, accounts have features that capture account output information, such as the average rate or normalized number of votes the account has taken. Target explanations for each node are provided by experts.

\subsection{Experimental setup}

We compare the results against three explainers: GNNExplainer \cite{GNNexplainer}, and PGMExplainer \cite{PGMExplainer} and an extension of Shapley Additive explanations (SHAP) \cite{shap2017} to GNNs --- SHAP is an additive feature attribution method.
%
To ensure a valid comparison, we closely follow guidelines and the evaluation setup from the original works.
%
Following the setup in \cite{PGMExplainer}, the GNN to be explained consists of a 3-layer GCN~\cite{GCN} with ReLU activation. We use an 80\%/10\%/10\% (train/val/test) split for all datasets. 
%
Moreover, the GCN model is trained for 10000 epochs using early stopping with patience of 100 epochs.

For the distillation phase, we use a SGC~\cite{SGC} model with 3 layers. During model distillation, predictions from all nodes are used such that the distiller model better fits the model to be explained. We use learning rate of 0.1 with a weight decay of $5.0 \times 10^{-6}$ for 10000 epochs.

All implementations were developed using the PyTorch~\cite{pytorch} and Torch Geometric~\cite{torch_geometric} libraries. In addition, in all experiments, we use Adam~\cite{adam} optimizer. 

\subsection{Distillation results}

In Table ~\ref{tab:distillation}, we introduce results achieved with the $\Psi$ distiller for both synthetic and real datasets.
The evaluation metric used was the accuracy between the model to be explained and the distiller model. Table~\ref{tab:distillation} reports the mean and standard deviation calculated over 10 independent runs. In addition, the time cost for the distillation process for each data set is reported.


 
  \begin{table}[!htb]
\centering
\caption{Distillation Accuracy.}

\adjustbox{width=0.5\textwidth}{
\begin{tabular}{ccccccccccc}
\toprule
  %& syn1  &  syn2 & syn3 & syn4 & syn5 & syn6 & bitcoin-alpha & bitcoin-otc \\ \toprule
  BA-House & BA-Community  & BA-Grids & Tree-Cycles & Tree-Grids &  BA-Bottle & Bitcoin-Alpha & Bitcoin-OTC \\ \toprule
 $94.2 \pm{1.2}$& $86.6\pm{0.1}  $& $99.9\pm{0.1}$ & $97.7\pm{0.2} $& $98.0\pm{0.2}$ & $98.5\pm{0.2}$ & $90.4\pm{0.1}$ & $89.1\pm{0.2}$  \\  \hline
 &\multicolumn{7}{c}{Average Time (s)} \\ \hline
 13.996 & 16.347 & 2.721 & 3.820 & 3.803 & 3.181 & 28.317 & 32.414 \\

\bottomrule
\end{tabular}}
\label{tab:distillation}
\end{table}

the results reached an accuracy above 90\% for most of the datasets, showing that the  $\Psi$ distiller can get very close to the model to be explained $\Phi$.


It is interesting to observe that even when poor distillation results occur, this does not have an impact on the generation of explanation, as is the case of the BA-community dataset that had a result below the desired in the distillation (86.6\%), but the generation of explanation achieved significantly higher results than other state-of-the-art explainers (as we will see in Table \ref{}).

A possible explanation for this counter-intuitive result is that the distiller can differentiate between motive nodes and base nodes, and this is enough to get good explanations as we are only interested in explaining motive nodes. Figure \ref{fig:conf_mat} reports the confusion matrix for the BA community dataset. Despite the low distillation accuracy, the model correctly predicts the base nodes (classes 1 and 5). Therefore, the model achieves high precision for the binary classification problem of distinguishing motive and base nodes, supporting our hypothesis.


\begin{figure}[H]
\centering
\adjustbox{width=0.25\textwidth}{
    %\subfloat{\includegraphics[width=0.3\textwidth]{samples/images/conf_mat/BA-House-conf_matrix.png}}
    \subfloat{\includegraphics[width=0.3\textwidth]{samples/images/conf_mat/BA-Community-conf_matrix.png}}
    %\subfloat{\includegraphics[width=0.3\textwidth]{samples/images/conf_mat/BA-Grids-conf_matrix.png}}
    }\\
% \adjustbox{width=0.5\textwidth}{                
%     \subfloat{\includegraphics[width=0.3\textwidth]{samples/images/conf_mat/Tree-Cycles-conf_matrix.png}}
%     \subfloat{\includegraphics[width=0.3\textwidth]{samples/images/conf_mat/Tree-Grids-conf_matrix.png}}
%     \subfloat{\includegraphics[width=0.3\textwidth]{samples/images/conf_mat/BA-Bottle-conf_matrix.png}}
%     }

\caption{Confusion matrix for the BA-Community dataset. The classes 1 and 5 correspond to base nodes. Note that the distiller is able to differentiate well between motif- and base-nodes.}
\label{fig:conf_mat}

\end{figure}


\subsection{Results for node-level explanation}


In this section, we evaluate the generation of node-level explanations for the node classification task. Eight datasets were used to evaluate the efficiency of our explanatory methods.

The results achieved in the synthetic datasets are presented in the table~\ref{tab:data_syn}. ConveX and METHOD generated better explanations than all state-of-the-art methods in four datasets: BA-House-Shapes, BA-Community, BA-Grids, and BA-Bottle-Shaped.
PGMExplainer achieved better results for the other datasets: Tree-Cycles and Tree-Grids.

\begin{table}[!htb]
\centering

\caption{Performance (accuracy) of explanatory models for synthetic datasets.}
\adjustbox{width=0.5\textwidth}{\begin{tabular}{lccccccc}

\toprule

 % &  syn1  &  syn2 & syn3 & syn4 & syn5 & syn6 \\ \toprule
&BA-House & BA-Community  & BA-Grids & Tree-Cycles & Tree-Grids &  BA-Bottle \\ \toprule

SHAP & 0.947 & 0.741 &0.872 & 0.884 & 0.641 &0.741\\  
GNNEx & 0.925  & 0.836  & 0.741 & 0.948  & 0.875  & 0.612 \\  
PGM-Ex & 0.965 &0.926 & 0.885  & \bf{0.954} & \bf{0.878}  & 0.953  \\
\hline

ConveX &  0.979 & 0.951 & 0.896   & 0.836 & 0.805 & 0.994\\
our & \bf{0.996} & \bf{0.954}  & \bf{0.939} & 0.873 &  0.85 & \bf{0.998} \\
%$\widetilde{{A}}^{K}$ & 0.996 & 0.961 & 0.952 & 0.904 & 0.878 & 0.998\\

\bottomrule
\end{tabular}}
\label{tab:data_syn}
\end{table}


\begin{table}[]
\centering

\caption{Performance (accuracy) of explanatory models for synthetic datasets.}
\adjustbox{width=0.5\textwidth}{
\begin{tabular}{lccccccc}

\toprule

 % &  syn1  &  syn2 & syn3 & syn4 & syn5 & syn6 \\ \toprule
&BA-House & BA-Community  & BA-Grids & Tree-Cycles & Tree-Grids &  BA-Bottle \\ \toprule

SHAP & 0.947 & 0.741 &0.872 & 0.884 & 0.641 &0.741\\  
GNNEx & 0.850  & 0.918  & 0.875 & 0.799  & 0.770  & 0.842 \\  
PGEx & 0.909  & 0.803  &  0.889 & \bf{0.955}  & \bf{0.931}  & 0.912 \\  
PGM-Ex & 0.965 &0.926 & 0.885  & {0.954} & 0.878  & 0.953  \\
\hline

ConveX &  0.979 & 0.951 & 0.896   & 0.836 & 0.805 & 0.994\\
our & \bf{0.996} & \bf{0.954}  & \bf{0.939} & 0.873 &  0.85 & \bf{0.998} 

\bottomrule
\end{tabular}}
\label{tab:data_syn}
\end{table}

The accuracy for explanation generation for each ConveX explainer and METHOD for the real Bitcoin-Alpha and Bitcoin-OTC datasets is reported in Table 1. Here we test the varying number of beads (nodes) for explanation in 3, 4, and 5.

ConveX outperforms GNNExplainer and SHAP by a large margin. Notably, ConveX is the best performing model for top-3 and top-4 predictions, while PGM-Explainer achieves the highest accuracy for top-5. The METHOD generated an explanation...

% \begin{table}[!htb]
% \centering
% \caption{Performance (precision) of explanatory models for real-world datasets.}
% \begin{tabular}{cccc|ccc}
% \toprule
% &\multicolumn{3}{c}{Bitcoin-Alpha} & \multicolumn{3}{c}{Bitcoin-OTC}\\ \hline
%  &    top 3 & top 4 & top 5  &  top 3 & top 4 & top 5\\ \hline
% SHAP  &  0.537 & 0.498 & 0.465 & 0.607 & 0.587  & 0.566 \\
% GNNEx   & 0.375 & 0.332 & 0.307 & 0.385 & 0.338 & 0.312\\
% PGM-Ex  & 0.873 & 0.857 & \textbf{0.848} & 0.833 & 0.817 &  \textbf{0.808}\\ \hline
% ConveX  & \textbf{0.912} &\textbf{ 0.879} & 0.823  &\textbf{0.885} & \textbf{0.835} & 0.775 \\
% our  & \textbf{0.878} & 0.845 & 0.793  &\textbf{0.849} & 0.798 & 0.746 \\
%$\widetilde{{A}}^{K}$ &  0.807 & 0.774 & 0.730 & 0.766 & 0.725 & 0.681\\

% \bottomrule
% \end{tabular}

% \label{tab:result_exp_bitcoin}
% \end{table}



\begin{table}[!htb]
\centering
\caption{Performance (precision) of explanatory models for real-world datasets.}
\begin{tabular}{cccc|ccc}
\toprule
&\multicolumn{3}{c}{Bitcoin-Alpha} & \multicolumn{3}{c}{Bitcoin-OTC}\\ \hline
 &    top 3 & top 4 & top 5  &  top 3 & top 4 & top 5\\ \hline
SHAP  &  0.537 & 0.498 & 0.465 & 0.607 & 0.587  & 0.566 \\
GNNEx   & 0.831 & 0.778 & 0.729 & 0.732 & 0.658 & 0.591\\
PGEx   & 0.906 & 0.867 & 0.821 & 0.875 & 0.824 & 0.793\\
PGM-Ex  & 0.873 & 0.857 & \textbf{0.848} & 0.833 & 0.817 &  \textbf{0.808}\\ \hline
ConveX  & \textbf{0.912} &\textbf{ 0.879} & 0.823  &\textbf{0.885} & \textbf{0.835} & 0.775 \\
our  & {0.878} & 0.845 & 0.793  &{0.849} & 0.798 & 0.746 \\
%$\widetilde{{A}}^{K}$ &  0.807 & 0.774 & 0.730 & 0.766 & 0.725 & 0.681\\

\bottomrule
\end{tabular}

\label{tab:result_exp_bitcoin}
\end{table}


Figure \ref{img:results_syn} shows motifs, base subgraphs and explanations provided from different explainers for the synthetic datasets. 
%
Overall, Metod and METHOD is the model that better captures the motif structure on BA-House, BA-Community, BA-Grids and BA-Bottle, validating its higher accuracy on these benchmarks.....


We report in the Table~\ref{tab:time} the time consumption statistics of the ConveX and METHOD models to generate the explanation of a node for each dataset.
The cost of generating an explanation is composed of a portion of the distillation step time plus the time of the step generating the explanation.
The ConveX approach can be up to 10 times faster than the baseline methods, while the METHOD because it has no training step, can be even more efficient.

\begin{table*}[!htb]
\centering
\caption{\textbf{Explanation time.}}

\adjustbox{width=\textwidth}{
\begin{tabular}{lcccccccc}
\toprule
 & BA-House & BA-Community  & BA-Grids & Tree-Cycles & Tree-Grids &  BA-Bottle  & bitcoin-alpha & bitcoin-otc   \\  \toprule
PGM-Explainer  & 1.808  & 6.965  & 7.086 & 1.608  & 7.157  & 1.736 & 6.452 & 8.823    \\
%PGExplainer &  10.92 &  24.07 &  6.89 & 6.36 & 6.72 &  10.74 &  521.0 & 587.8 &  305.87        & 460.740      & 592.86      \\
GNNExplainer  & 1.626   & 0.870 & 0.979 & 1.981 & 0.991 & 1.605 &  &         \\\hline
%Zorro   & 24387.7  & 188210 & 3518.496  & 4955.472  & 21115.29  & 40410.24   & —    & —  & 19500 & 54050 & 9953.96              \\  
% ConveX & 0.169 & 0.290 & 0.394 & 0.413 & 0.456 & 0.394 & 3.464 & 4.598\\
% our  & 0.0008    & 0.0007  & 0.0005 & 0.0003    & 0.0004 & 0.0005   & 0.0069  & 0.0175 \\


% 13.996 & 16.347 & 2.721 & 3.820 & 3.803 & 3.181 & 28.317 & 32.414 \\
%13.996/700 & 16.347/ 1400 & 2.721/1020 & 3.820/871  & 3.803/1231 & 3.181/700 & 28.317/3783 & 32.414/5881

ConveX & 0.189 & 0.302 & 0.397 & 0.417 & 0.461 & 0.401 & 3.480 & 4.603\\
our  & 0.021    & 0.012  & 0.003 & 0.004    & 0.003 & 0.010   & 0.014  & 0.023 \\



\bottomrule
\end{tabular}}
\label{tab:time}
\end{table*}


% \begin{figure}[htb]%
% \centering
% \adjustbox{width=0.5\textwidth}{
% \includegraphics[width=\textwidth]{samples/images/result_expl.pdf}
% }
% \caption{Illustration of how synthetic datasets are assembled with their motifs and base nodes. The node labels are represented by colors. Furthermore, to evaluate how each explainer behaves, examples of explanations extracted from GNNExplainer, PGM-Explainer, and ConveX for the same prediction in each dataset are shown.}\label{img:results_syn}
% \end{figure}

\subsection{Results for edge-level explanation}

Our model can also be easily modified to provide edge-level explanations. This modification allows us to compare our model with some edge-level explainers such as PGexplainer, GNNexplainer, and other unofficial baselines (but which are used in the literature for comparative purposes).

As can be seen in table 3, our method does not demonstrate such a significant performance compared to PGexplainer. This is mainly due to the edge embedding construction step (Our method proved to be very sensitive to the way in which the edge embeddings are generated). However, a point to consider is that our method outperforms PGexplainer and GNNexplainer in all scenarios when they are used to explain the distilled models (PGex-Dest and GNNex-Dest show the results of PGexplainer and GNNexplainer for the task of explain the distilled models).


\begin{table}[!htb]
\centering
\caption{Accuracy performance of GNN models used on Edge explanations}
\adjustbox{width=0.5\textwidth}{\begin{tabular}{lccccccc}

\toprule
  &BA-House & BA-Community  & BA-Grids & Tree-Cycles & Tree-Grids &  BA-Bottle \\ \toprule

Training & 0.98 & 0.99 & 0.98 & 0.99 & 0.92 & 0.97 \\
Validation & 1.0 & 0.88 & 0.95 & 1.0  & 0.94 & 0.99\\ 
Testing  & 0.97 & 0.93 & 0.96 & 0.99 & 0.94 & 0.97\\

\bottomrule
\end{tabular}}
\label{tab:datasets}
\end{table}


% \begin{table}[!htb]
% \centering
% \caption{Performance (accuracy) of explanatory models for synthetic datasets for edge level.}
% \adjustbox{max width=\textwidth}{\begin{tabular}{lccccccc}

% \toprule

%   &  syn1  &  syn2 & syn3 & syn4 & syn5 & syn6 \\ \toprule
% NearestNeighbor & 0.932 & 0.501 & 0.843 & 0.673 & 0.632 & 0.952 \\
% PageRank & 0.984 & 0.558 & \textbf{0.994} & 0.816  & 0.767 & \textbf{0.991}\\ 
% Edge Gradients  & 0.441 & 0.502 & 0.438 & 0.419 & 0.456 & 0.52\\
% %PGex-Dest & 0.999 & 0.446 & 0.816 & 0.163 \\  
% GNNExplainer & 0.925 & 0.836 & 0.761 & 0.948 & 0.875 & 0.902\\  
% PGexplainer & 0.963 & 0.945 & 0.959 &\textbf{0.987} & \textbf{0.907} & 0.964\\  
% %GNNex-Dest  & 0.730 & 0.522 & 0.586 & 0.539\\  
% \hline
% Our  & \textbf{0.997} & \textbf{0.972} & 0.565 & 0.927 & 0.801 & 0.910\\
% %$\widetilde{{A}}^{K}$ & 0.995 & 0.931 & 0.869 & 0.996\\
% %ConvEx & & & & \\

% \bottomrule
% \end{tabular}}
% \label{tab:datasets}
% \end{table}




\begin{table}[!htb]
\centering
\caption{Performance (accuracy) of explanatory models for synthetic datasets for edge level.}
\adjustbox{width=0.5\textwidth}{\begin{tabular}{lccccccc}

\toprule

  &BA-House & BA-Community  & BA-Grids & Tree-Cycles & Tree-Grids &  BA-Bottle  \\ \toprule
NearestNeighbor & 0.932 & 0.501 & 0.843 & 0.673 & 0.632 & 0.952 \\
PageRank & 0.984 & 0.558 & 0.994 & 0.816  & 0.767 & 0.991\\ 
Edge Gradients  & 0.441 & 0.502 & 0.438 & 0.419 & 0.456 & 0.52\\
%PGex-Dest & 0.999 & 0.446 & 0.816 & 0.163 \\  
GNNExplainer & 0.925 & 0.836 & 0.761 & 0.948 & 0.875 & 0.902\\  
PGexplainer & 0.963 & 0.945 & 0.959 &\textbf{0.987} & \textbf{0.907} & 0.964\\  
%GNNex-Dest  & 0.730 & 0.522 & 0.586 & 0.539\\  
\hline
Our  & 0.997 & 0.972 & 0.565 & 0.927 & 0.801 & 0.910\\
$\widetilde{{A}}^{K}$ & \textbf{0.999} & \textbf{0.981} & \textbf{0.999} & 0.959 & 0.904 & \textbf{0.999}\\
%ConvEx & & & & \\

\bottomrule
\end{tabular}}
\label{tab:datasets}
\end{table}

For $\widetilde{{A}}^{K}$, a value of K=3 was used. The weight for each edge was calculated by multiplying the weights of the edge's node: eij = vi * vj.


\begin{table}[!htb]
\centering

\caption{Performance (accuracy) of explanatory models for synthetic datasets.}
\adjustbox{width=0.5\textwidth}{\begin{tabular}{lccccccc}

\toprule

% &  syn1  &  syn2 & syn3 & syn4 & syn5 & syn6 \\ \toprule
&BA-House & BA-Community  & BA-Grids & Tree-Cycles & Tree-Grids &  BA-Bottle \\ \toprule

SHAP & 0.947 & 0.741 &0.872 & 0.884 & 0.641 &0.741\\  
GNNEx & 0.925  & 0.836  & 0.741 & 0.948  & 0.875  & 0.612 \\  
PGM-Ex & 0.965 &0.926 & 0.885  & \bf{0.954} & \bf{0.878}  & 0.953  \\
\hline

ConveX &  0.979 & 0.951 & 0.896   & 0.836 & 0.805 & 0.994\\
our & \bf{0.996} & 0.954  & 0.939 & 0.873 &  0.85 & \bf{0.998} \\
$\widetilde{{A}}^{K}$ & \textbf{0.996} & \textbf{0.961} & \textbf{0.952} & 0.904 & \textbf{0.878} & \textbf{0.998}\\

\bottomrule
\end{tabular}}
\label{tab:datasets}
\end{table}



\begin{figure}[H]
\centering
\adjustbox{width=0.5\textwidth}{
    \subfloat{\includegraphics[width=\textwidth]{samples/images/degree/BA-House-degree.png}}
    \subfloat{\includegraphics[width=\textwidth]{samples/images/degree/BA-Community-degree.png}}}\\
\adjustbox{width=0.5\textwidth}{
    \subfloat{\includegraphics[width=\textwidth]{samples/images/degree/BA-Grids-degree.png}}
    \subfloat{\includegraphics[width=\textwidth]{samples/images/degree/BA-Bottle-degree.png}}}\\
\adjustbox{width=0.5\textwidth}{    \subfloat{\includegraphics[width=\textwidth]{samples/images/degree/Tree-Cycles-degree.png}}
    \subfloat{\includegraphics[width=\textwidth]{samples/images/degree/Tree-Grids-degree.png}}}
    

\caption{}
\label{fig:degree}

\end{figure}



\section{Conclusion}






\newpage

\bibliographystyle{ACM-Reference-Format}
\bibliography{references}


%%
%% If your work has an appendix, this is the place to put it.



\newpage

\appendix

\input{appendix.tex}

\end{document}
\endinput
%%
%% End of file `sample-sigconf.tex'.
