\section{Background}
\paragraph{Notation. } We define a graph $\mathcal{G} = (V, E)$, with a set of nodes  $V = \{1, \ldots, n\}$ and a set of edges $E \subseteq V \times V$. We denote the adjacency matrix of $\mathcal{G}$ by $A \in \mathbb{R}^{n\times n}$, i.e., $A_{ij}$ is one if $(i,j) \in E$ and zero otherwise. Let $D$ be the diagonal degree matrix of $\mathcal{G}$, i.e., $D_{i i} = \sum_{j} A_{i j}$. We also define the \emph{normalized} adjacency matrix with added self-loops as $\widetilde{A} = (D + I_n)^{-1/2} (A + I_n) (D + I_n)^{-1/2}$, where $I_n$ is the $n$-dimensional identity matrix. Furthermore, let  $X \in \mathbb{R}^{n \times d}$ be a matrix of $d$-dimensional node features. Throughout this work, we often represent a graph $\mathcal{G}$ using the pair $(A, X)$.

\paragraph{Graph neural networks (GNNs).} We consider the general framework of message-passing GNNs \citep{Gilmer2017}. Typical GNNs interleave aggregation and update steps at each layer. Specifically, for each node $v$ at layer $\ell$, the aggregation is a nonlinear function of the ($\ell-1$)-layer representations of $v$'s neighbors. The update step computes a new representation for $v$ based on its representation at layer $\ell-1$ and the aggregated messages (output of the aggregation step). 
%
Here we cover two specific GNN architectures: graph convolutional networks \citep[GCNs]{GCN} and simplified graph convolutions \citep[SGC]{SGC}. The former is arguably the most popular GNN in the literature and is used profusely throughout our experiments. The latter is a linear graph model, which will be an asset to our explanation method. 
%
For a more thorough overview of GNNs, we refer the reader to \citet{book-graph-learning}.



Graph convolutional networks combine local filtering operations (i.e., graph convolutions) and non-linear activation functions (most commonly ReLU) at each layer.
Denoting the weights of the $\ell$-th GCN layer by $W^{(\ell)}$ and the element-wise activation function by $\sigma$, we can recursively write the output of the $\ell$-th layer $H^{(\ell)}$ as:
\begin{equation}
    H^{(\ell)} = \sigma\left(\widetilde{A} H^{(\ell-1)} {W^{(\ell)}}\right),\label{eq:GCN}
\end{equation}
where $H^{(0)} =X$. To obtain node-level predictions, we propagate the final embeddings --- after an arbitrary number of layers --- through a modified convolution with a row-wise softmax instead of $\sigma$, i.e., $\hat{Y} = \softmax(\widetilde{A} H^{(\ell)} W^{(\ell+1)})$. In practice, it is also common to apply multilayer perceptron on top of the final embeddings.


SGC can be viewed as a simplification of the GCN model. \citet{SGC} derive SGC by removing the nonlinear activation functions in GCNs. Consequently, the chained linear transformations become redundant and we can use a single parameter matrix $\Theta$. 
%
Thus, node predictions from an $L$-layer SGC are:
\begin{align}
\hat{Y} &= \softmax(\widetilde{A}^{L} X \Theta).
\end{align}

Interestingly, \citet{SGC} showed that  SGC often performs similarly to or better than GCN in a variety of node classification tasks. On top of that, training SGCs is computationally more efficient than training GCNs, and SGC has significantly fewer parameters.

\section{DnX: Distill n' Explain}

We now introduce DnX --- a new post-hoc explanation method for GNNs. DnX comprises two steps: knowledge distillation and explanation extraction. During the former, we use a linear GNN $\Psi$ to approximate the predictions from the GNN $\Phi$ we want to explain. 
In the second step, we extract explanations directly from $\Psi$ (instead of $\Phi$). We hypothesize that, as long as $\Psi$ is a good approximation of $\Phi$, substructures highly influential to the output of $\Phi$ should also be relevant to $\Psi$. Therefore, explanations of our surrogate should also explain well the original GNN. To obtain explanations, we exploit the linear nature of $\Psi$ and propose two simple procedures. The first consists of solving a convex program. The second ranks nodes based on a simple decomposition of predictions into additive terms.  

Following \citet{PGMExplainer}, we assume $\Phi$ is a black-box model that we can only probe to get outputs. More specifically, we cannot access gradients of $\Phi$, nor can we access inner layers to extract node embeddings.

\subsection{Knowledge distillation}
\label{sec:kd}

We use SGC~\citep{SGC} to approximate the predictions obtained with the GNN $\Phi$.
Formally, the surrogate model (SGC) $\Psi$ receives the input graph $\mathcal{G}=(A, X)$ and provides class predictions
$\hat{Y}^{(\Psi_\Theta)} = \softmax(\widetilde{A}^L X \Theta)$, where $\Theta$ is the matrix of model parameters, and $L$ is a hyper-parameter. 

The distillation process consists of adjusting the parameters of $\Psi_\Theta$ to match its predictions to those of the network $\Phi$. We do so by minimizing the Kullback-Leibler divergence $\mathrm{KL}$ between the predictions of $\Phi$ and $\Psi_{\Theta}$. 
%
Let $\hat{Y}_i^{(\Psi_{\Theta})}$ and $\hat{Y}_i^{(\Phi)}$ denote the class predictions for node $i$ from the $\Psi_{\Theta}$ and $\Phi$ models, respectively. We distill $\Phi$ into $\Psi$  by solving: 
\begin{equation}
\min_{\Theta} \left\{  \mathrm{KL} \left( \hat{Y}^{(\Phi)}, \hat{Y}^{(\Psi_\Theta)} \right) \coloneqq
\sum_{i\in V} \sum_{c} \hat{Y}_{i c}^{(\Phi)}  \log \frac{\hat{Y}_{i c}^{(\Phi)}}{\hat{Y}_{i c}^{(\Psi_\Theta)}} \right\},
\label{eq:destilador}
\end{equation}
which is equivalent to  the categorical cross-entropy between $\hat{Y}^{(\Phi)}$ and $\hat{Y}^{(\Psi_\Theta)}$. Note that minimizing this loss does not require back-propagating through the original GNN $\Phi$, only through the surrogate $\Psi$. We also do not require any knowledge about $\Phi$'s architecture.


\subsection{Explanation extraction}
\label{sec:obtaining}

To obtain an explanation to a given prediction $\hat{Y}^{(\Psi_\theta)}_i$, we want to identify a subgraph of $\mathcal{G}$ containing the nodes that influence the most that prediction.
%
We denote an explanation $\mathcal{E}$ as an $n$-dimensional vector of importance scores (higher equals more relevant), one for each node in the vertex set $V$. 
%
We introduce two strategies to compute $\mathcal{E}$.

\paragraph{Optimizing for $\mathcal{E}$.} We can formulate the problem of finding the explanation $\mathcal{E}$ by treating it as a vector of 0-1 weights, and minimizing the squared $L_2$ norm between the logits associated with $\hat{Y}_i^{(\Psi_\Theta)}$ and those from the graph with node features masked by $\mathcal{E}$:
\begin{equation}
\min_{\mathcal{E}\in\{0,1\}^n}\parallel \widetilde{A}^{L}_{i}\mathrm{diag}(\mathcal{E})X\Theta - \widetilde{A}^{L}_{i}X\Theta \parallel_{2}^{2},
%- \bm{\zeta}(\bm{X},\bm{A}) \parallel_{2}
\label{eq:e1}
\end{equation}
where $\widetilde{A}^{L}_{i}$ denotes the $i$-th row of the matrix $\widetilde{A}^{L}$. Note that the formulation in \autoref{eq:e1} has a major issue: it admits the trivial solution $\mathcal{E} = [1, 1, \dots, 1]$. To circumvent the issue and simultaneously avoid binary optimization, we replace the search space $\{0, 1\}^n$ by the $(n-1)$-simplex $\Delta = \{r \in \mathbb{R}^n : \sum_i r_i = 1, \forall_i r_i \geq 0 \}$. Implementing this change and re-arranging computations, we wind up with:
\begin{equation}
\min_{\mathcal{E} \in \Delta}\Big\| \widetilde{A}^{L}_{i}\left(\mathrm{diag}(\mathcal{E}) - I_n\right)X\Theta \Big\|_{2}^{2}.
%- \bm{\zeta}(\bm{X},\bm{A}) \parallel_{2}
\label{eq:e2}
\end{equation}


Note that nodes outside the $L$-hop neighborhood of node $i$ do not affect how $\Psi$ classifies it. Thus, we can mask all nodes at distance $\geq L+1$ without altering the solution of \autoref{eq:e2}. For ease of implementation, we solve \autoref{eq:e2} reparameterizing $\mathcal{E}$ as a softmax-transformed vector.
%
% After optimizing for $\bm{\mathcal{E}}$, the values in $\bm{\mathcal{E}}$ serve as a ranking for the importance of each node.

\paragraph{Finding $\mathcal{E}$ via linear decomposition.} Let $Z_i$ denote the logit vector associated with the prediction $\hat{Y}_i^{(\Psi_\Theta)}$. Due to the linear nature of $\Psi$, we can decompose $Z_i$ into a sum of $n$ terms, one for each node in $V$ (plus the bias): 
\begin{equation}
    \widetilde{{A}}^{L}_{i 1} X_1 \Theta+     \widetilde{{A}}^{L}_{i 2} X_2 \Theta + \ldots +     \widetilde{A}^{L}_{i n} X_n\Theta + b=  Z_i. 
\end{equation}
 
Therefore, we can measure the contribution of each node to the prediction as  its scalar projection onto $Z_i - b$:
\begin{equation}
    \mathcal{E}_j \coloneqq \widetilde{{A}}^{L}_{i j} X_j \Theta (Z_i - b)^\intercal
\end{equation}
%
When we use this strategy instead of solving \autoref{eq:e2}, we refer to our method as FastDnX.