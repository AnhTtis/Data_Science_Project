\clearpage
\newpage

\section{Proofs}
    \label{append:proofs}
    
    \begin{proof}[Proof of Lemma~\ref{theo:bound_unfaithfulness}]
        For node $u$, it is known, by definition, that
        \begin{equation*}
            \begin{split}
                &\left\lVert \Psi(\mathcal{G}_{u})- \Psi(t(\mathcal{G}_{u}, \mathcal{E}_u))\right\rVert_2 \\
                =\ & \left\lVert \sigma\left(\widetilde A^L X \Theta\right)_u - \sigma\left(\widetilde E^L X \Theta\right)_u \right\rVert_2 \\
                =\ &\left\lVert \sigma\left[\left(\widetilde A^L X \Theta\right)_u\right] - \sigma\left[\left(\widetilde E^L X \Theta\right)_u\right] \right\rVert_2,
            \end{split}
        \end{equation*}
        if we call $\sigma$ the $\text{softmax}$ function and $\widetilde E^L$ the powered, normalized adjancency matrix $\widetilde A^L$ after applying the explanation $\mathcal{E}_u$.
        Because $\text{softmax}$ is a Lipschitz continuous function with Lipschitz constant $1$ with respect to norm $\lVert \cdot \rVert_2$ \citep{gao_properties_2018} and considering induced matrix norm compatibility property \citep[Lemma 1.3.8.7]{van_de_geijn_advanced_2022},
        \begin{equation*}
            \begin{split}
                &\left\lVert \Psi(\mathcal{G}_{u})- \Psi(t(\mathcal{G}_{u}, \mathcal{E}_u))\right\rVert_2 \\
                =\ &\left\lVert \sigma\left[\left(\widetilde A^L X \Theta\right)_u\right] - \sigma\left[\left(\widetilde E^L X \Theta\right)_u\right] \right\rVert_2 \\
                \le\ &\left\lVert (X \Theta)^\intercal \right\rVert_2 \left\lVert \widetilde A_u^L - \widetilde E_u^L \right\rVert_2.
            \end{split}
        \end{equation*}
        Similarly, for a perturbation $\mathcal{G}_u'$ of $\mathcal{G}_u$ given by $\Sigma_{u}'$ being added to $X$,
        \begin{equation*}
            \begin{split}
                &\left\lVert \Psi(\mathcal{G}_{u}')- \Psi(t(\mathcal{G}_{u}', \mathcal{E}_u))\right\rVert_2 \\
                \le\ &\lVert [\left(X + \Sigma_{u}'\right) \Theta]^\intercal \rVert_2 \left\lVert \widetilde A_u^L - \widetilde E_u^L \right\rVert_2.
            \end{split}
        \end{equation*}
        Computing the mean over $\mathcal{K} \cup \{\mathcal{G}_u\}$:
        \begin{equation*}
            \begin{split}
                &\frac{1}{|\mathcal{K}| + 1} \sum_{\mathcal{G}_u' \in \mathcal{K} \cup \{\mathcal{G}_u\}} \left\lVert \Psi(\mathcal{G}_{u}')- \Psi(t(\mathcal{G}_{u}', \mathcal{E}_u))\right\rVert_2 \\
                \le\ &  \frac{1}{|\mathcal{K}| + 1} \Biggl[ \lVert (X \Theta)^\intercal \rVert_2 \left\lVert \widetilde A_u^L - \widetilde E_u^L \right\rVert_2 \\
                & \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \left. + \sum_{\mathcal{G}_u' \in \mathcal{K}}  \lVert [\left(X + \Sigma_{u}'\right) \Theta]^\intercal \rVert_2 \left\lVert \widetilde A_u^L - \widetilde E_u^L \right\rVert_2 \right] \\
                \le\ &  \lVert\Theta^\intercal\rVert_2 \left\lVert \widetilde A_u^L - \widetilde E_u^L \right\rVert_2  \frac{1}{|\mathcal{K}| + 1} \Biggl(\lVert X^\intercal \rVert_2 \\
                & \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ + \left.\sum_{\mathcal{G}_u' \in \mathcal{K}} \lVert (X + \Sigma_{u}')^\intercal \rVert_2 \right) \\
                \le\ &  \gamma_\Theta \left\lVert \underset{\mathcal{E}_u}{\Delta} \widetilde A_u^L \right\rVert_2 \max \left(\{\lVert X^\intercal \rVert_2\} \cup \{\lVert (X + \Sigma_{u}')^\intercal \rVert_2\}_{\mathcal{G}_u' \in \mathcal{K}}\right) \\
                =\ &  \gamma_{\Theta, X, \Sigma} \left\lVert \underset{\mathcal{E}_u}{\Delta} \widetilde A_u^L \right\rVert_2.
            \end{split}
        \end{equation*}
        
        When $\Sigma$ is limited, the constant $\gamma$ may not depend on $\Sigma$.
    \end{proof}
    
    \begin{proof}[Proof of Theorem~\ref{theo:bound_unfaithfulness_2}]
        We know that
        \begin{equation*}
            \begin{split}
                &\left\lVert \Phi(\mathcal{G}_{u}')- \Phi(t(\mathcal{G}_{u}', \mathcal{E}_u))\right\rVert_2 \\
                =\ &\lVert \Phi(\mathcal{G}_{u}') - \Psi(\mathcal{G}_{u}') + \Psi(\mathcal{G}_{u}') - \Psi(t(\mathcal{G}_{u}', \mathcal{E}_u)) \\
                &+ \Psi(t(\mathcal{G}_{u}', \mathcal{E}_u)) - \Phi(t(\mathcal{G}_{u}', \mathcal{E}_u))\rVert_2.
            \end{split}
        \end{equation*}
        So, by using triangle inequality and \autoref{theo:bound_unfaithfulness},
        \begin{equation*}
            \begin{split}
                &\frac{1}{|\mathcal{K}| + 1} \sum_{\mathcal{G}_u' \in \mathcal{K} \cup \{\mathcal{G}_u\}} \left\lVert \Phi(\mathcal{G}_{u}')- \Phi(t(\mathcal{G}_{u}', \mathcal{E}_u))\right\rVert_2 \\
                \le\ & \frac{1}{|\mathcal{K}| + 1} \sum_{\mathcal{G}_u' \in \mathcal{K} \cup \{\mathcal{G}_u\}} \left\lVert \Phi(\mathcal{G}_{u}') - \Psi(\mathcal{G}_{u}')\right\rVert_2 \\
                &+ \frac{1}{|\mathcal{K}| + 1} \sum_{\mathcal{G}_u' \in \mathcal{K} \cup \{\mathcal{G}_u\}} \left\lVert \Psi(\mathcal{G}_{u}') - \Psi(t(\mathcal{G}_{u}', \mathcal{E}_u))\right\rVert_2 \\
                &+ \frac{1}{|\mathcal{K}| + 1} \sum_{\mathcal{G}_u' \in \mathcal{K} \cup \{\mathcal{G}_u\}} \left\lVert \Psi(t(\mathcal{G}_{u}', \mathcal{E}_u)) - \Phi(t(\mathcal{G}_{u}', \mathcal{E}_u))\right\rVert_2 \\
                \le\ & \gamma \left\lVert \underset{\mathcal{E}_u}{\Delta} \widetilde A_u^L \right\rVert_2 + 2\alpha.
            \end{split}
        \end{equation*}
    \end{proof}
    
    \begin{proof}[Proof of Lemma~\ref{theo:prob_bound_unfaithfulness}]
        From the proof of Lemma~\ref{theo:bound_unfaithfulness}, we know that
        \begin{equation*}
            \begin{split}
                &\frac{1}{|\mathcal{K}| + 1} \sum_{\mathcal{G}_u' \in \mathcal{K} \cup \{\mathcal{G}_u\}} \left\lVert \Psi(\mathcal{G}_{u}')- \Psi(t(\mathcal{G}_{u}', \mathcal{E}_u))\right\rVert_2 \le\ \\
                 &  \lVert\Theta^\intercal\rVert_2 \left\lVert \underset{\mathcal{E}_u}{\Delta} \widetilde A_u^L \right\rVert_2  \frac{1}{|\mathcal{K}| + 1} (\lVert X^\intercal \rVert_2 + \sum_{\mathcal{G}_u' \in \mathcal{K}} \lVert (X + \Sigma_{u}')^\intercal \rVert_2 )
            \end{split}
        \end{equation*}
        By using triangle inequality, we can write
        \begin{equation*}
            \begin{split}
                &\frac{1}{|\mathcal{K}| + 1} \sum_{\mathcal{G}_u' \in \mathcal{K} \cup \{\mathcal{G}_u\}} \left\lVert \Psi(\mathcal{G}_{u}')- \Psi(t(\mathcal{G}_{u}', \mathcal{E}_u))\right\rVert_2 \\
                \le\ &  \lVert\Theta^\intercal\rVert_2 \left\lVert \underset{\mathcal{E}_u}{\Delta} \widetilde A_u^L \right\rVert_2 \lVert X^\intercal \rVert_2 \\
                &+ \lVert\Theta^\intercal\rVert_2 \left\lVert \underset{\mathcal{E}_u}{\Delta} \widetilde A_u^L \right\rVert_2 \frac{1}{|\mathcal{K}| + 1} \sum_{\mathcal{G}_u' \in \mathcal{K}} \lVert (\Sigma_{u}')^\intercal \rVert_2.
            \end{split}
        \end{equation*}
        We can work in the perturbation summand:
        \begin{equation*}
            \begin{split}
                \sum_{\mathcal{G}_u' \in \mathcal{K}} \lVert (\Sigma_{u}')^\intercal \rVert_2 \le\ & \sum_{\mathcal{G}_u' \in \mathcal{K}} \lVert \Sigma_{u}' \rVert_\text{F} \\
                =\ & \sum_{\mathcal{G}_u' \in \mathcal{K}} \sqrt{\sum_{i=1}^{nd} \epsilon_{u',i}^2} \\
                =\ & \sigma \sum_{\mathcal{G}_u' \in \mathcal{K}} \sqrt{\sum_{i=1}^{nd} Z_{u',i}^2} \\
                \le\ & \sigma \sum_{\mathcal{G}_u' \in \mathcal{K}} \max\left(1, \ {\sum_{i=1}^{nd} Z_{u',i}^2}\right) \\
                \le\ & \sigma \sum_{\mathcal{G}_u' \in \mathcal{K}} \left(1 + {\sum_{i=1}^{nd} Z_{u',i}^2}\right) \\
                =\ & \sigma |\mathcal{K}| + \sigma Q,
            \end{split}
        \end{equation*}
        with
        \begin{itemize}
            \item $\epsilon_{j, i}~\sim~\mathcal{N}(0, \sigma^2)$;
            \item $Z_{j, i}~\sim~\mathcal{N}(0, 1)$;
            \item $Q~\sim~\chi_{|\mathcal{K}|nd}^2$;
            \item $(n, d)~=~\text{dim}(X)$.
        \end{itemize}
        
        If $\alpha_p$ is the $p$-percentile of $Q$, then the probability of the last inequality is $p$:
        \begin{equation*}
            \begin{split}
                &\frac{1}{|\mathcal{K}| + 1} \sum_{\mathcal{G}_u' \in \mathcal{K} \cup \{\mathcal{G}_u\}} \left\lVert \Psi(\mathcal{G}_{u}')- \Psi(t(\mathcal{G}_{u}', \mathcal{E}_u))\right\rVert_2 \\
                \le\ &  \lVert\Theta^\intercal\rVert_2 \left\lVert \underset{\mathcal{E}_u}{\Delta} \widetilde A_u^L \right\rVert_2 \lVert X^\intercal \rVert_2 \\
                &+ \lVert\Theta^\intercal\rVert_2 \left\lVert \underset{\mathcal{E}_u}{\Delta} \widetilde A_u^L \right\rVert_2 \frac{1}{|\mathcal{K}| + 1} \sum_{\mathcal{G}_u' \in \mathcal{K}} \lVert (\Sigma_{u}')^\intercal \rVert_2 \\
                \le\ & \gamma_1 \left\lVert \underset{\mathcal{E}_u}{\Delta} \widetilde A_u^L \right\rVert_2 + \lVert\Theta^\intercal\rVert_2 \left\lVert \underset{\mathcal{E}_u}{\Delta} \widetilde A_u^L \right\rVert_2 \frac{1}{|\mathcal{K}| + 1} \sigma (Q + |\mathcal{K}|) \\
                \le\ & \gamma_1 \left\lVert \underset{\mathcal{E}_u}{\Delta} \widetilde A_u^L \right\rVert_2 + \lVert\Theta^\intercal\rVert_2 \left\lVert \underset{\mathcal{E}_u}{\Delta} \widetilde A_u^L \right\rVert_2 \frac{1}{|\mathcal{K}| + 1} \sigma (\alpha_p + |\mathcal{K}|) \\
                =\ &   \gamma_1 \left\lVert \underset{\mathcal{E}_u}{\Delta} \widetilde A_u^L \right\rVert_2 + \gamma_2 \left\lVert \underset{\mathcal{E}_u}{\Delta} \widetilde A_u^L \right\rVert_2 \sigma (\alpha_p + |\mathcal{K}|)
            \end{split}
        \end{equation*}
        Notice that, when $\sigma$ approaches zero, the bound's probabilistic characteristic  becomes negligible.
        
        Finally, if we ask the bound to be $\xi$, then the probability is
        \[p = F_{\chi_{|\mathcal{K}|nd}^2}\left(\frac{\xi - \gamma_1 \left\lVert \underset{\mathcal{E}_u}{\Delta} \widetilde A_u^L \right\rVert_2}{\gamma_2 \left\lVert \underset{\mathcal{E}_u}{\Delta} \widetilde A_u^L \right\rVert_2 \sigma} - |\mathcal{K}| \right),\]
        $F_{\chi_{|\mathcal{K}|nd}^2}$ being the c.d.f. of $Q$.
    \end{proof}
    
    \begin{proof}[Proof of \autoref{theo:convexity}]
        The objective function of the problem in \autoref{eq:e2} can be written as
        \begin{equation*}
            \begin{split}
                f(\mathcal{E}) =\ & \left\lVert \widetilde A_i^L \text{diag}(\mathcal{E}) X \Theta - \widetilde A_i^L X \Theta\right\rVert_2^2 \\
                =\ &\left( \widetilde A_i^L \text{diag}(\mathcal{E}) X \Theta - \widetilde A_i^L X \Theta\right) \cdot \\
                &\ \cdot \left( \widetilde A_i^L \text{diag}(\mathcal{E}) X \Theta - \widetilde A_i^L X \Theta\right)^\intercal \\
                =\ &\mathcal{E}^\intercal \text{diag}\left[\left(\widetilde A_i^L\right)^\intercal\right] X \Theta \Theta^\intercal X^\intercal \text{diag}\left[\left(\widetilde A_i^L\right)^\intercal\right] \mathcal{E} \\
                &- 2 \mathcal{E}^\intercal \text{diag}\left[\left(\widetilde A_i^L\right)^\intercal\right] X \Theta \Theta^\intercal X^\intercal \left(\widetilde A_i^L\right)^\intercal \\
                &\ + \left\lVert\widetilde A_i^L X \Theta\right\rVert_2^2 \\
                =\ &\frac{1}{2}\mathcal{E}^\intercal Q \mathcal{E} + \mathcal{E}^\intercal c + \delta.
            \end{split}
        \end{equation*}
        Note also that \[Q = P^\intercal P \text{ with } \ P = \sqrt{2} \Theta^\intercal X^\intercal \text{diag}\left[\left(\widetilde A_i^L\right)^\intercal\right],\]
        thus $Q$ is symmetric and positive semidefinite. Since both the objective function and feasible set are convex, the optimization problem is also convex. 
    \end{proof}
    
    % \begin{theorem}[Relaxed optimization problem solution]
    %     \label{theo:solution}
    %     The optimization problem of Equation~\ref{eq:e2}, with the relaxation of $\mathcal{E}$ belonging to the hyperplane generated by $\Delta$ (i.e., $\mathcal{E}$ sums up to one but can have negative values) has solutions of the form $\mathcal{E} = Zy + d$, where $Z$ and $d$ are constant matrix and vector (with respect to $X, \Theta, \widetilde A^L$) and $y$ is a solution for a problem of the form $\widetilde Q y = \widetilde c$,
    %     where $\widetilde Q$ and $\widetilde c$ depend on $X, \Theta, \widetilde A^L$.
    %     $\widetilde Q$ is positive semidefinite.
    %     Assuming $\Theta^\intercal X^\intercal \text{diag}\left[\left(\widetilde A_i^L\right)^\intercal\right]$ has rank equal to the number of nodes in the explanation $\mathcal{E}$, $\widetilde Q$ is invertible and $\mathcal{E}$ is unique.
    % \end{theorem}
        
    % \begin{proof}[Sketch proof]
    %     We first rewrite the objective function of the relaxation of Equation~\ref{eq:e2} as a canonical quadratic problem with equality constraints.
    %     After that, we remove the equality constraints by a variable transformation.
    %     We solve this simpler quadratic problem and state the result.
    %     When the matrices of the original problem are such that $\Theta^\intercal X^\intercal \text{diag}\left[\left(\widetilde A_i^L\right)^\intercal\right]$ has nullity zero, we can conclude $\widetilde Q$ is invertible and the solution $\mathcal{E}$ is unique.
    %     For the details, refer to the complete proof.
    % \end{proof}
    
    % \begin{proof}
        % The objective function of the problem of Equation~\ref{eq:e2} can be restated as
        % \begin{equation*}
        %     \begin{split}
        %         f(\mathcal{E}) =\ & \left\lVert \widetilde A_i^L \text{diag}(\mathcal{E}) X \Theta - \widetilde A_i^L X \Theta\right\rVert_2^2 \\
        %         =\ &\left( \widetilde A_i^L \text{diag}(\mathcal{E}) X \Theta - \widetilde A_i^L X \Theta\right) \cdot \\
        %         &\ \cdot \left( \widetilde A_i^L \text{diag}(\mathcal{E}) X \Theta - \widetilde A_i^L X \Theta\right)^\intercal \\
        %         =\ &\mathcal{E}^\intercal \text{diag}\left[\left(\widetilde A_i^L\right)^\intercal\right] X \Theta \Theta^\intercal X^\intercal \text{diag}\left[\left(\widetilde A_i^L\right)^\intercal\right] \mathcal{E} \\
        %         &- 2 \mathcal{E}^\intercal \text{diag}\left[\left(\widetilde A_i^L\right)^\intercal\right] X \Theta \Theta^\intercal X^\intercal \left(\widetilde A_i^L\right)^\intercal \\
        %         &\ + \left\lVert\widetilde A_i^L X \Theta\right\rVert_2^2 \\
        %         =\ &\frac{1}{2}\mathcal{E}^\intercal Q \mathcal{E} + \mathcal{E}^\intercal c + \delta.
        %     \end{split}
        % \end{equation*}
        % Because \[Q = P^\intercal P,\ P = \sqrt{2} \Theta^\intercal X^\intercal \text{diag}\left[\left(\widetilde A_i^L\right)^\intercal\right],\]
        % $Q$ is symmetric and positive semidefinite.
        % Therefore, the optimization problem can be rewritten as
        % \begin{equation*}
        %     \begin{split}
        %         \min\ \ &\frac{1}{2}x^\intercal Q x + x^\intercal c, \\
        %         \text{subject to}\ \ &\sum_{i = 1}^n x_i = 1, x \in \mathbb{R}^n.
        %     \end{split}
        % \end{equation*}
        % Note that this is equivalent to
        % \begin{equation*}
        %     \begin{split}
        %         \min\ \ &\frac{1}{2}x^\intercal Q x + x^\intercal c, \\
        %         \text{subject to}\ \ &
        %         \begin{bmatrix}
        %                 x_1 \\
        %                 \vdots \\
        %                 x_{n-1} \\
        %                 x_n
        %         \end{bmatrix} = 
        %         \begin{bmatrix}
        %                 y_1 \\
        %                 \vdots \\
        %                 y_{n-1} \\
        %                 1 - \sum_{i = 1}^{n-1} y_i
        %         \end{bmatrix}, \\
        %             &\ \ x \in \mathbb{R}^n, y \in \mathbb{R}^{n-1},
        %     \end{split}
        % \end{equation*}
        % because $\sum x_i = 1$.
        % We can write
        % \[x = Zy + d \coloneqq \begin{bmatrix}
        %     1 & \cdots & 0 \\
        %     \vdots & \ddots & \vdots \\
        %     0 & \cdots & 1 \\
        %     -1 & \cdots & -1
        % \end{bmatrix} y + \begin{bmatrix}
        %     0 \\
        %     \vdots \\
        %     0 \\
        %     1
        % \end{bmatrix},\]
        % so that the problem becomes
        % \begin{equation*}
        %     \begin{split}
        %         \min\ \ &\frac{1}{2}(Zy + d)^\intercal Q (Zy + d) + (Zy + d)^\intercal c, \\
        %         \text{subject to}\ \ & y \in \mathbb{R}^{n-1}.
        %     \end{split}
        % \end{equation*}
        % This new objective function can be written
        % \begin{equation*}
        %     \begin{split}
        %         &\frac{1}{2}(Zy + d)^\intercal Q (Zy + d) + (Zy + d)^\intercal c \\
        %         =\ &\frac{1}{2}y^\intercal(Z^\intercal QZ)y + \left[Z^\intercal\left(Qd + c\right)\right]^\intercal y + \frac{1}{2}d^\intercal Qd + c^\intercal d \\
        %         =\ &\frac{1}{2}y^\intercal \widetilde Q y + \tilde c^\intercal y + \gamma.
        %     \end{split}
        % \end{equation*}
        % Finally, by setting the gradient to zero, the solution to this new unconstrained quadratic programming problem is a solution to
        % \[\widetilde Q y + \tilde c = 0 \Rightarrow \widetilde Q y = - \tilde c.\]
        
        % If we assume $P = \Theta^\intercal X^\intercal \text{diag}\left[\left(\widetilde A_i^L\right)^\intercal\right]$ has rank equal to the number of nodes in the explanation $\mathcal{E}$, then $P$ has nullity zero.
        % Therefore,
        % \[y \ne 0 \Rightarrow Zy \ne 0 \Rightarrow PZy \ne 0\]
        % and, if $y \ne 0$,
        % \[y^\intercal \widetilde Q y = y^\intercal (Z^\intercal P^\intercal P Z) y = \lVert PZy \rVert_2^2 > 0.\]
        % So $\widetilde Q$, which is symmetric, is also positive definite, therefore invertible, which leads to $y = -\widetilde Q^{-1} \tilde c$.
        % In this case, the unique solution is
        % \[\mathcal{E} = x = Zy + d = -Z\widetilde Q^{-1} \tilde c + d.\]
    % \end{proof}


 

\section{Datasets and implementation details}\label{append:implementation}
\label{sec:details_datasets}

\subsection{Datasets}

Bitcoin-Alpha and Bitcoin-OTC are real-world networks comprising 3783 and 5881 nodes (user accounts), respectively. Users rate their trust in each other using a score between $-10$ (total distrust) and $10$ (total trust). Then, user accounts are labeled as trusted or not based on how other users rate them. Accounts (nodes) have features such as average ratings. Ground-truth explanations for each node are provided by experts.
%
The synthetic datasets are available in \citep{GNNexplainer} and \citep{PGMExplainer}.
%
\autoref{tab:details_datasets} shows summary statistics for all datasets.


\begin{table}[!htb]
\centering
\caption{Statistics of the datasets used in our experiments.}
\adjustbox{width=0.35\textwidth}{
\begin{tabular}{lccc}
\toprule
\textbf{Dataset} & \textbf{nodes} & \textbf{edges} & \textbf{labels}\\
\midrule
BA-House & 700 & 4110 & 4\\
BA-Community & 1400 & 8920 & 8\\
BA-Grids & 1020 & 5080 & 2\\
Tree-Cycles & 871 & 1950 & 2\\
Tree-Grids & 1231 & 3410 & 2 \\
BA-Bottle & 700 & 3948 & 4\\
Bitcoin-Alpha & 3783 & 28248 & 2\\
Bitcoin-OTC & 5881 & 42984 & 2\\
\bottomrule
\end{tabular}}
\label{tab:details_datasets}
\end{table}


\subsection{Implementation details}
\input{tables/result_class_gnns}
We ran all experiments using a laptop with an Intel Xeon 2.20 GHz CPU,  13 GB RAM DDR5, and RTX 3060ti 16GB GPU.

The architecture of the GNNs to be explained in this work are: 
$i$) a GCN model with 3 layers (20 hidden units) followed by a two-layer MLP with 60 hidden units; $ii$) an ARMA model with 3 layers (20 hidden units each) followed by a two-layer MLP with 60 hidden units; $iii$) a GIN model with 3 layers (20 hidden units each); and $iv$) a GATED model with 3 layers (100 hidden units) followed by a two-layer MLP with 300 hidden units. We train all these GNNs using with a learning rate of 0.001. All models use ReLU activations in their hidden units. We also use the one-hot vector of the degree as node features.

For computational reasons, on Bitcoin datasets, we provide results for 500 test nodes whenever the candidate explanation set contemplates 3-hop neighborhood (matching the GNNs we want to explain). For 1-hop cases, we use the full set of 2000 nodes as in the original setup.



\paragraph{Node and edge-level explanations.} DnX and FastDnX were originally designed to generate node-level explanations like PGM-Explainer but some baselines such as PGexplainer and GNNexplainer provide edge-level explanations. For a fair comparison with these baselines, we use a procedure to convert edge-level explanations to node-level ones (and vice-versa). To convert node scores to edge scores, we sum the scores of endpoints of each edge, i.e., an edge $(u,v)$ gets score $s_{u,v} = s_u + s_v$ where $s_u$ and $s_v$ are node scores. For the reverse, we say $s_u = {|\mathcal{N}_u|}^{-1}\sum_{j \in \mathcal{N}_u} s_{u, j}$ where $\mathcal{N}_u$ is the neighborhood of $u$.


 \section{Additional experiments}
\label{append:sup_experiments} 

\vspace{-1pt}
\paragraph{GNN performance.} \autoref{tab:classification} shows the classification accuracy of each model we explain in our experimental campaign. Means and standard deviations reflect the outcome of 10 repetitions. All classifiers achieve accuracy $>85\%$.
\input{tables/results_distillers}

\vspace{-1pt}
\paragraph{Distillation.} \autoref{tab:distillation_models} shows the extent to which the distiller network $\Psi$ agrees with the GNN $\Phi$. We measure agreement as accuracy, using the predictions of $\Phi$ as ground truth. The means and standard deviations reflect the outcome of 10 repetitions. Additionally, \autoref{tab:distillation_models} shows the  time elapsed during the distillation step. For all cases, we observe accuracy values over $88\%$.



\vspace{-1pt}
\paragraph{Results for edge-level explanations.} \autoref{tab:result_exp_edges} and \autoref{tab:result_exp_bitcoin_edges} complement Tables 1 and 2 in the main paper, showing results for edge-level explanations. All experiments were repeated ten times. For all datasets, we measure performance in terms of AUC, following \citet{PGExplainer}. Both tables corroborate our findings from the main paper. In most cases, FastDnX is the best or second-best model for all models and datasets. For GCN and GATED, PGExplainer yields the best results. Overall, both Dnx and FastDnX outperform GNNExplainer and PGM-Explainer. Remarkably FastDnX and DnX's performance is steady, with small fluctuations depending on the model we are explaining. The same is not true for the competing methods, e.g., PGExplainer loses over $15\%$ AUC for the BA-Community (cf., GCN and ARMA). Note also that FastDnX and DnX are the best models on the real-world datasets.

\input{tables/result_syn_edge}

\input{tables/result_bitcoin_edge}

\vspace{-1pt}
\paragraph{More results for node-level explanations.}
\autoref{tab:result_exp_bitcoin_node_others} shows results for ARMA, GATED, and GIN. As observed in \autoref{tab:result_exp_bitcoin_node} for GCNs, DnX FastDnX are the best methods.


\input{tables/result_bitcoin_node}


