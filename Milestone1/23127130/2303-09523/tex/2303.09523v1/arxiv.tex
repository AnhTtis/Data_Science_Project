%% bare_jrnl.tex
%% V1.4b
%% 2015/08/26
%% by Michael Shell
%% see http://www.michaelshell.org/
%% for current contact information.
%%
%% This is a skeleton file demonstrating the use of IEEEtran.cls
%% (requires IEEEtran.cls version 1.8b or later) with an IEEE
%% journal paper.
%%
%% Support sites:
%% http://www.michaelshell.org/tex/ieeetran/
%% http://www.ctan.org/pkg/ieeetran
%% and
%% http://www.ieee.org/

%%*************************************************************************
%% Legal Notice:
%% This code is offered as-is without any warranty either expressed or
%% implied; without even the implied warranty of MERCHANTABILITY or
%% FITNESS FOR A PARTICULAR PURPOSE! 
%% User assumes all risk.
%% In no event shall the IEEE or any contributor to this code be liable for
%% any damages or losses, including, but not limited to, incidental,
%% consequential, or any other damages, resulting from the use or misuse
%% of any information contained here.
%%
%% All comments are the opinions of their respective authors and are not
%% necessarily endorsed by the IEEE.
%%
%% This work is distributed under the LaTeX Project Public License (LPPL)
%% ( http://www.latex-project.org/ ) version 1.3, and may be freely used,
%% distributed and modified. A copy of the LPPL, version 1.3, is included
%% in the base LaTeX documentation of all distributions of LaTeX released
%% 2003/12/01 or later.
%% Retain all contribution notices and credits.
%% ** Modified files should be clearly indicated as such, including  **
%% ** renaming them and changing author support contact information. **
%%*************************************************************************


% *** Authors should verify (and, if needed, correct) their LaTeX system  ***
% *** with the testflow diagnostic prior to trusting their LaTeX platform ***
% *** with production work. The IEEE's font choices and paper sizes can   ***
% *** trigger bugs that do not appear when using other class files.       ***                          ***
% The testflow support page is at:
% http://www.michaelshell.org/tex/testflow/


\documentclass{article}


\usepackage{PRIMEarxiv}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc} 
%\documentclass[journal]{IEEEtran}
%
% If IEEEtran.cls has not been installed into the LaTeX system files,
% manually specify the path to it like:
% \documentclass[journal]{../sty/IEEEtran}



\usepackage{comment}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{amsmath}
\usepackage{adjustbox}
\usepackage{subfig}
\usepackage{amsfonts} 
\usepackage{lipsum}
\usepackage[square,numbers]{natbib}
\usepackage{hyperref}
\usepackage{url}


% Some very useful LaTeX packages include:
% (uncomment the ones you want to load)


% *** MISC UTILITY PACKAGES ***
%
%\usepackage{ifpdf}
% Heiko Oberdiek's ifpdf.sty is very useful if you need conditional
% compilation based on whether the output is pdf or dvi.
% usage:
% \ifpdf
%   % pdf code
% \else
%   % dvi code
% \fi
% The latest version of ifpdf.sty can be obtained from:
% http://www.ctan.org/pkg/ifpdf
% Also, note that IEEEtran.cls V1.7 and later provides a builtin
% \ifCLASSINFOpdf conditional that works the same way.
% When switching from latex to pdflatex and vice-versa, the compiler may
% have to be run twice to clear warning/error messages.

% *** CITATION PACKAGES ***
%
%\usepackage{cite}
% cite.sty was written by Donald Arseneau
% V1.6 and later of IEEEtran pre-defines the format of the cite.sty package
% \cite{} output to follow that of the IEEE. Loading the cite package will
% result in citation numbers being automatically sorted and properly
% "compressed/ranged". e.g., [1], [9], [2], [7], [5], [6] without using
% cite.sty will become [1], [2], [5]--[7], [9] using cite.sty. cite.sty's
% \cite will automatically add leading space, if needed. Use cite.sty's
% noadjust option (cite.sty V3.8 and later) if you want to turn this off
% such as if a citation ever needs to be enclosed in parenthesis.
% cite.sty is already installed on most LaTeX systems. Be sure and use
% version 5.0 (2009-03-20) and later if using hyperref.sty.
% The latest version can be obtained at:
% http://www.ctan.org/pkg/cite
% The documentation is contained in the cite.sty file itself.






% *** GRAPHICS RELATED PACKAGES ***
%
%\ifCLASSINFOpdf
  % \usepackage[pdftex]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../pdf/}{../jpeg/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.pdf,.jpeg,.png}
%\else
  % or other class option (dvipsone, dvipdf, if not using dvips). graphicx
  % will default to the driver specified in the system graphics.cfg if no
  % driver is specified.
  % \usepackage[dvips]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../eps/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.eps}
%\fi
% graphicx was written by David Carlisle and Sebastian Rahtz. It is
% required if you want graphics, photos, etc. graphicx.sty is already
% installed on most LaTeX systems. The latest version and documentation
% can be obtained at: 
% http://www.ctan.org/pkg/graphicx
% Another good source of documentation is "Using Imported Graphics in
% LaTeX2e" by Keith Reckdahl which can be found at:
% http://www.ctan.org/pkg/epslatex
%
% latex, and pdflatex in dvi mode, support graphics in encapsulated
% postscript (.eps) format. pdflatex in pdf mode supports graphics
% in .pdf, .jpeg, .png and .mps (metapost) formats. Users should ensure
% that all non-photo figures use a vector format (.eps, .pdf, .mps) and
% not a bitmapped formats (.jpeg, .png). The IEEE frowns on bitmapped formats
% which can result in "jaggedy"/blurry rendering of lines and letters as
% well as large increases in file sizes.
%
% You can find documentation about the pdfTeX application at:
% http://www.tug.org/applications/pdftex


% *** MATH PACKAGES ***
%
%\usepackage{amsmath}
% A popular package from the American Mathematical Society that provides
% many useful and powerful commands for dealing with mathematics.
%
% Note that the amsmath package sets \interdisplaylinepenalty to 10000
% thus preventing page breaks from occurring within multiline equations. Use:
%\interdisplaylinepenalty=2500
% after loading amsmath to restore such page breaks as IEEEtran.cls normally
% does. amsmath.sty is already installed on most LaTeX systems. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/pkg/amsmath


% *** SPECIALIZED LIST PACKAGES ***
%
%\usepackage{algorithmic}
% algorithmic.sty was written by Peter Williams and Rogerio Brito.
% This package provides an algorithmic environment fo describing algorithms.
% You can use the algorithmic environment in-text or within a figure
% environment to provide for a floating algorithm. Do NOT use the algorithm
% floating environment provided by algorithm.sty (by the same authors) or
% algorithm2e.sty (by Christophe Fiorio) as the IEEE does not use dedicated
% algorithm float types and packages that provide these will not provide
% correct IEEE style captions. The latest version and documentation of
% algorithmic.sty can be obtained at:
% http://www.ctan.org/pkg/algorithms
% Also of interest may be the (relatively newer and more customizable)
% algorithmicx.sty package by Szasz Janos:
% http://www.ctan.org/pkg/algorithmicx


% *** ALIGNMENT PACKAGES ***
%
%\usepackage{array}
% Frank Mittelbach's and David Carlisle's array.sty patches and improves
% the standard LaTeX2e array and tabular environments to provide better
% appearance and additional user controls. As the default LaTeX2e table
% generation code is lacking to the point of almost being broken with
% respect to the quality of the end results, all users are strongly
% advised to use an enhanced (at the very least that provided by array.sty)
% set of table tools. array.sty is already installed on most systems. The
% latest version and documentation can be obtained at:
% http://www.ctan.org/pkg/array


% IEEEtran contains the IEEEeqnarray family of commands that can be used to
% generate multiline equations as well as matrices, tables, etc., of high
% quality.




% *** SUBFIGURE PACKAGES ***
%\ifCLASSOPTIONcompsoc
%  \usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
%\else
%  \usepackage[caption=false,font=footnotesize]{subfig}
%\fi
% subfig.sty, written by Steven Douglas Cochran, is the modern replacement
% for subfigure.sty, the latter of which is no longer maintained and is
% incompatible with some LaTeX packages including fixltx2e. However,
% subfig.sty requires and automatically loads Axel Sommerfeldt's caption.sty
% which will override IEEEtran.cls' handling of captions and this will result
% in non-IEEE style figure/table captions. To prevent this problem, be sure
% and invoke subfig.sty's "caption=false" package option (available since
% subfig.sty version 1.3, 2005/06/28) as this is will preserve IEEEtran.cls
% handling of captions.
% Note that the Computer Society format requires a larger sans serif font
% than the serif footnote size font used in traditional IEEE formatting
% and thus the need to invoke different subfig.sty package options depending
% on whether compsoc mode has been enabled.
%
% The latest version and documentation of subfig.sty can be obtained at:
% http://www.ctan.org/pkg/subfig




% *** FLOAT PACKAGES ***
%
%\usepackage{fixltx2e}
% fixltx2e, the successor to the earlier fix2col.sty, was written by
% Frank Mittelbach and David Carlisle. This package corrects a few problems
% in the LaTeX2e kernel, the most notable of which is that in current
% LaTeX2e releases, the ordering of single and double column floats is not
% guaranteed to be preserved. Thus, an unpatched LaTeX2e can allow a
% single column figure to be placed prior to an earlier double column
% figure.
% Be aware that LaTeX2e kernels dated 2015 and later have fixltx2e.sty's
% corrections already built into the system in which case a warning will
% be issued if an attempt is made to load fixltx2e.sty as it is no longer
% needed.
% The latest version and documentation can be found at:
% http://www.ctan.org/pkg/fixltx2e


%\usepackage{stfloats}
% stfloats.sty was written by Sigitas Tolusis. This package gives LaTeX2e
% the ability to do double column floats at the bottom of the page as well
% as the top. (e.g., "\begin{figure*}[!b]" is not normally possible in
% LaTeX2e). It also provides a command:
%\fnbelowfloat
% to enable the placement of footnotes below bottom floats (the standard
% LaTeX2e kernel puts them above bottom floats). This is an invasive package
% which rewrites many portions of the LaTeX2e float routines. It may not work
% with other packages that modify the LaTeX2e float routines. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/pkg/stfloats
% Do not use the stfloats baselinefloat ability as the IEEE does not allow
% \baselineskip to stretch. Authors submitting work to the IEEE should note
% that the IEEE rarely uses double column equations and that authors should try
% to avoid such use. Do not be tempted to use the cuted.sty or midfloat.sty
% packages (also by Sigitas Tolusis) as the IEEE does not format its papers in
% such ways.
% Do not attempt to use stfloats with fixltx2e as they are incompatible.
% Instead, use Morten Hogholm'a dblfloatfix which combines the features
% of both fixltx2e and stfloats:
%
% \usepackage{dblfloatfix}
% The latest version can be found at:
% http://www.ctan.org/pkg/dblfloatfix




%\ifCLASSOPTIONcaptionsoff
%  \usepackage[nomarkers]{endfloat}
% \let\MYoriglatexcaption\caption
% \renewcommand{\caption}[2][\relax]{\MYoriglatexcaption[#2]{#2}}
%\fi
% endfloat.sty was written by James Darrell McCauley, Jeff Goldberg and 
% Axel Sommerfeldt. This package may be useful when used in conjunction with 
% IEEEtran.cls'  captionsoff option. Some IEEE journals/societies require that
% submissions have lists of figures/tables at the end of the paper and that
% figures/tables without any captions are placed on a page by themselves at
% the end of the document. If needed, the draftcls IEEEtran class option or
% \CLASSINPUTbaselinestretch interface can be used to increase the line
% spacing as well. Be sure and use the nomarkers option of endfloat to
% prevent endfloat from "marking" where the figures would have been placed
% in the text. The two hack lines of code above are a slight modification of
% that suggested by in the endfloat docs (section 8.4.1) to ensure that
% the full captions always appear in the list of figures/tables - even if
% the user used the short optional argument of \caption[]{}.
% IEEE papers do not typically make use of \caption[]'s optional argument,
% so this should not be an issue. A similar trick can be used to disable
% captions of packages such as subfig.sty that lack options to turn off
% the subcaptions:
% For subfig.sty:
% \let\MYorigsubfloat\subfloat
% \renewcommand{\subfloat}[2][\relax]{\MYorigsubfloat[]{#2}}
% However, the above trick will not work if both optional arguments of
% the \subfloat command are used. Furthermore, there needs to be a
% description of each subfigure *somewhere* and endfloat does not add
% subfigure captions to its list of figures. Thus, the best approach is to
% avoid the use of subfigure captions (many IEEE journals avoid them anyway)
% and instead reference/explain all the subfigures within the main caption.
% The latest version of endfloat.sty and its documentation can obtained at:
% http://www.ctan.org/pkg/endfloat
%
% The IEEEtran \ifCLASSOPTIONcaptionsoff conditional can also be used
% later in the document, say, to conditionally put the References on a 
% page by themselves.




% *** PDF, URL AND HYPERLINK PACKAGES ***
%
%\usepackage{url}
% url.sty was written by Donald Arseneau. It provides better support for
% handling and breaking URLs. url.sty is already installed on most LaTeX
% systems. The latest version and documentation can be obtained at:
% http://www.ctan.org/pkg/url
% Basically, \url{my_url_here}.




% *** Do not adjust lengths that control margins, column widths, etc. ***
% *** Do not use packages that alter fonts (such as pslatex).         ***
% There should be no need to do such things with IEEEtran.cls V1.6 and later.
% (Unless specifically asked to do so by the journal or conference you plan
% to submit to, of course. )


% correct bad hyphenation here
%\hyphenation{op-tical net-works semi-conduc-tor}



\title{Fast 3D Volumetric Image Reconstruction from 2D MRI Slices by Parallel Processing}
% \fancyhead[RE]{Firstauthor and Secondauthor} % Firstauthor et al. if more than 2 - must use \documentclass[twoside]{article}


  
\author{Somoballi Ghoshal$^{1*}$, Shremoyee Goswami$^{1}$, Amlan Chakrabarti$^{1}$, Susmita Sur-Kolay$^{2}$\\~\\
$^1$A. K. Choudhury School of Information Technology, University of Calcutta\\
$^2$Advanced Computing and Microelectronics Unit, Indian Statistical Institute\\~\\
$^*$somoballi@gmail.com}

\begin{document}
\maketitle

\begin{abstract}
Magnetic Resonance Imaging (MRI) is a technology for non-invasive imaging of anatomical features in detail. It can help in functional analysis of organs of a specimen but it is very costly.  In this work, methods for (i) virtual three-dimensional (3D) reconstruction from a single sequence of two-dimensional (2D) slices of MR images of a human spine  and brain along a single axis, and (ii) generation of missing inter-slice data are proposed. Our approach helps in preserving the edges, shape, size, as well as the internal tissue structures of the object being captured.  The sequence of original 2D slices along a single axis is divided into smaller equal sub-parts which are then reconstructed using edge preserved kriging interpolation to predict the missing slice information. In order to speed up the process of interpolation, we have used multiprocessing
by carrying out the initial interpolation  on  parallel cores.  From the 3D matrix thus formed, shearlet transform is applied to estimate the edges considering the 2D blocks along the $Z$ axis,  and to minimize the blurring effect using a proposed mean-median logic.  Finally, for visualization, the sub-matrices are merged into a final 3D matrix. Next, the newly formed 3D matrix is split up into voxels and marching cubes method is applied to get the approximate 3D image for viewing.  To the best of our knowledge it is a first of its kind approach based on kriging interpolation and multiprocessing for 3D reconstruction from 2D slices, and approximately 98.89\% accuracy  is achieved with respect to similarity metrics for image comparison. The time required for reconstruction has also been reduced by approximately 70\% with multiprocessing even for a large input data set compared to that with single core processing.
\end{abstract}

% Note that keywords are not normally used for peerreview papers.
\keywords{3D volumetric reconstruction \and kriging interpolation \and shearlet transform \and MRI.}






% For peer review papers, you can put extra information on the cover
% page as needed:
% \ifCLASSOPTIONpeerreview
% \begin{center} \bfseries EDICS Category: 3-BBND \end{center}
% \fi
%
% For peerreview papers, this IEEEtran command inserts a page break and
% creates the second title. It will be ignored for other modes.
%\IEEEpeerreviewmaketitle
\maketitle



\section{Introduction}
% The very first letter is a 2 line initial drop letter followed
% by the rest of the first word in caps.
% 
% form to use if the first word consists of a single letter:
% \IEEEPARstart{A}{demo} file is ....
% 
% form to use if you need the single drop letter followed by
% normal text (unknown if ever used by the IEEE):
% \IEEEPARstart{A}{}demo file is ....
% 
% Some journals put the first two words in caps:
% \IEEEPARstart{T}{his demo} file is ....
% 
% Here we have the typical use of a "T" for an initial drop letter
% and "HIS" in caps to complete the first word.
Medical imaging captures the internal structure of a specimen. It can also help in the functional analysis of some of the organs or tissues  of the specimen \cite{1}.  There are several prevalent imaging technologies such as  medical ultrasonography or ultrasound, endoscopy, medical photography, elastography, tactile imaging, thermography, and then  radiological ones, namely X-ray radiography, computer aided tomography (CT), micro-computer tomography (micro-CT), magnetic resonance imaging (MRI), optical coherence tomography (OCT) \cite{1} . In this work, the focus is only on the most intensive one, i.e., MRI images. 


\par
Magnetic resonance imaging (MRI) is  primarily used to form pictures of the anatomy and the physiological processes of the body in both health and disease \cite{3,10,2}. It is widely used for diagnosis of any brain and spine abnormality/disease  because it captures the tissue structure of the body most effectively. The diversity and complexity of lesion cells, particularly in functionally critical organs, make it very challenging to visualize a lesion in magnetic resonance imaging. This has led to 3D MRI for better visualization.

\par 3D MRI can be captured directly, but it comes at a colossal cost. The alternate most prominent approach for 3D MRI visualization is 3D reconstruction from 2D MRI slices. There are two types of 3D reconstruction techniques \cite{37}, namely, surface rendering and volume rendering. While surface rendering \cite{37} reconstructs to preserve the surface of the 3D object, volume rendering \cite{8} enables a volumetric visualization with the internal structure of the 3D object also reconstructed. Thus, when this 3D reconstructed data is sliced into 2D along a specified plane, we can also view the internal structures of the 3D object. 3D visualization has become salient for medical diagnosis \cite{31}, as it offers abundant and accurate information for medical experts. However, there are several challenges which need to be addressed. All the existing techniques mostly focus on preserving the shape of the object but not the internal structures. These also consider the information of all the three orthogonal planes $XY$, $YZ$, $ZX$. 

These techniques for 3D reconstruction of MRI of brain images have been exhibited in the literature \cite{5,6,9} but these fail for spine reconstruction. The reason is that in MRI of spine, the information of both hard and soft tissues are of equal importance, since the vertebrae as well as the bony spinal cord and inter vertebral discs are all present together. Thus, in the case of the spine, edge preservation while reconstruction is of utmost importance. This leads us to the goal of designing efficient 3D reconstruction for MRI of spine. In this work, the focus is on preserving the information of the internal tissues as well as the shape and size of the spine and on accelerating the speed for reconstruction. Moreover, all the existing interpolation techniques are deterministic and does not have any scope for error estimation while reconstruction, which can be overcome by the use of kriging interpolation. We have carried out 3D reconstruction using edge preservation by kriging interpolation, shearlet transform  along with multiprocessing. 
\par The elaborate explanation of our work  starts with motivation and related works in Sections II and III respectively followed by preliminary knowledge of the techniques used for interpolation, registration and quality assessment in Section IV. Our proposed methodology is described in Section V and our experimental results appear in Section VI. Section VII has a brief discussion along with conclusion.


\section{Motivation}
 Various existing techniques for 3D MRI scan capture a sequence of 2D image slices at equal intervals along  each of the three orthogonal axes and then the 3D image is generated \cite{1}. In case of CT scans, the slices along a single axis are captured and stacked to get the 3D. But this stacking only method results in loss of information in between the slices. In all types of medical imaging techniques, there is always a uniform slice gap ranging from 1 mm to 5 mm between two consecutive slices \cite{3}, hence a minimum of approximately 4-pixel information is missing between two slices which may result in erroneous internal structure reconstruction. The time taken for capturing  medical data for 3D reconstruction is approximately 45 mins \cite{9}, which means more energy is consumed and the computational complexity is also high. 

Our goal is to create efficient accurate 3D reconstruction and visualization of MRI from a sequence of 2D slices along a single axis, and also providing an user interface for the professionals to cut the reconstructed 3D image as needed with virtual scissors and to view any slice along any  plane. Thus, the scanning time can be reduced from 45 mins \cite{9} to close to the time taken to capture a single set of slices along a specified axis. The power consumption of a MRI machine \cite{38} to capture a patient is approximately 15 kW, which can be reduced  considerably with the reduction of time to capture the image. Since the time is proportional to cost, the huge cost of MRI especially in the developing countries,  can also be reduced and better health-care can be attained with cost, power consumption and exposure to radiation being minimized. 
 

In \cite{12,39}, there are methods for 3D reconstruction and slicing of brain and spine respectively, but blurring along the edges do include 4\% error and these also fail for a large sized dataset. These works also deals with deterministic interpolation and spacial co-relation and error estimation is not incorporated. They also carry out 2D interpolation along an axis, thus all the voxels related to a voxel are never considered. In this work, we speed up the interpolation with error estimation while reconstruction based on spatial co-relation. The internal structures are preserved more accurately with the use of 3D interpolation for inter slice missing data reconstruction considering the information of all possible 64 neighbouring voxels. The error for reconstruction is approximately reduced to 1\% for real MR images. Our main contributions are proposing fast methods for:

\begin{itemize}

\item reconstructing a 3D volumetric image of a human spine or a brain from a sequence of 2D slices along a single axis, with preservation of internal structure by using edge preserving kriging interpolation on the voxels which in turn helps in shape and size restoration; 
\item dividing the image sequence into sub-sequences (8 here) and implementing the interpolation in parallel to attain a speed up of approximately 70\% ; 

\end{itemize}

To the best of our knowledge, this is the first of its kind method for 3D reconstruction  from a sequence of slices in one plane using parallel processing and for slicing this regenerated 3D as per user instructions to visualise the internal structure with a feature of 3D crop and zooming in.



 
\section{Related Works}

Several works for 3D reconstruction from 2D images in medical imaging domain \cite{4,31,14,28,33,35,26,44,45,46}  exist, but they fail to give good accurate
results with internal structure preservation very fast. While most methods suffice for CT images as the information is mostly of
hard tissue structure and outlines, but for MRI have lacuna with preservation of the internal tissue structure. 
The Marching cubes \cite{15} algorithm is widely accepted for
reconstructing a 3D surface from a given 3D image. For
approximating contours, it uses patterned cubes or iso-surfaces.
However, it requires certain techniques to reduce memory and time for reconstructing a surface from large volumetric data. The usual way to solve this problem \cite{4} is by diminishing the size of a volumetric image, but the quality of the surface of 3D reconstructed image becomes substandard if only sub-sampling is applied. Due to poor reconstruction by only volumetric sub-sampling, another method is proposed, which improves the quality of a surface reconstructed from the sampled volumetric data. It is based on a pipeline of Visualization Toolkit (VTK) \cite{22,21}. Their approach  consists of three major steps: preprocessing,
reconstructing and displaying. In \cite{25}, the authors have used kriging interpolation for reconstruction of inter-slice data but it gives a blurring effect not only along the boundary but along all the edges, thus making the reconstructed images noisy.  In \cite{23}, the authors have used tri-linear interpolation for reconstruction of MRI of brain and it gave better results than marching cube, but again fails in preserving the minute internal details.  In \cite{34}, the authors have used edge based interpolation for
correction of blurred and noisy edges in a 2D plane, but this work is inadequate for generating a large number of missing points in a 3D image. For 3D reconstruction by this technique applied on our real data, the accuracy after slicing is obtained to be 75.47\%. 

Several works for 3D reconstruction specifically for MRI \cite{5,6} also exist but they consider all three planes for reconstruction. In \cite{12}, the authors have used bilinear interpolation for reconstruction of 3D brain from 2D slices, along a single plane but in this case the edges are not properly preserved. Further, in \cite{39}, the authors have  reconstructed 3D image of human spine from a sequence of slice images along a single axis with 96\% accuracy by employing a combination of bilinear and bicubic algorithm, but the time complexity is very high even with a small number of inputs and a blurring effect along the boundary remains. It is unable to process a large sized dataset. 

In \cite{43}, the authors use deep learning for interpolating the missing data in between slices which gives good results for the trained set of data with no anomaly, but fails if there exist any deformity. Also, as every individual is unique,  generating the missing slices based on the information of the slices of some other individuals does not always give accurate results. Moreover, the structure of spine is always different in men, women and child and it also varies a lot  with height. None of the existing deep learning techniques have taken onto account all of these and hence fails in most cases and are not used or accepted by medical practitioners.


\par 
There are a few techniques for MRI reconstruction using multiple cores \cite{40,42} but these take into consideration the information for all three planes either while capturing the data (real time) or after capturing the full information along all three planes. Thus, we propose an advanced technique of 3D reconstruction from a single sequence of slices which takes information only from its preceding and succeeding slices to generate the missing information with edge preservation and also multiprocessing to speed up the process of 3D reconstruction.


\section{Methodology}

Figure \ref{blockdia} shows the step-wise overview of our proposed approach. At first, the 2D slices along a single axis are denoised using shearlet transform \cite{24} (see Appendix~\ref{shear}). Next, we divide the data set into $N$ (a power of 2) equal parts. These $N$ sub sequences of 2D images are interpolated in parallel, using kriging interpolation \cite{25} (see Appendix~\ref{kriging}) to generate the missing slices in between two consecutive slices. Single instruction multiple data architecture (SIMD) is used to execute these $N$ blocks in parallel by multiprocessing on $N$ cores
%of a 32 core CPU with 64 GB memory 
to speed up the process of reconstruction.  From these 3D sub matrices, we then consider the 2D images along the $YZ$ plane and apply Shearlet transform. Edge detection is carried out as in \cite{29} and  the blur effect along the edges is minimized by a proposed mean-median logic.  
\begin{figure}[h!]
	\centering
	\includegraphics[width=8.2cm, height=12cm]{krigflow.jpg}
	\hspace{.01 mm}
     \caption{Block diagram for 3D reconstruction from a single sequence of slices along one axis}	
     \label{blockdia}
\vspace{0.01 mm}
\end{figure}
In order to get the final 3D matrix of the entire object we need to merge the 3D sub matrices that are obtained thus far. After joining the sub matrices, we break the 3D matrix into voxels and further apply the Marching cubes algorithm followed by smoothing \cite{32} for 3D reconstruction. We apply color map and activate rotation operation and then display the image on screen so that
the user can rotate the 3D image and view all sides of it as needed.  

Since we have generated the complete 3D volume data, we can easily cut and view  any specified required part of the object as required.

\subsection{3D reconstruction from 2D slices}
The proposed method for 3D reconstruction has three major components: parallel processing of the 3D reconstruction to gain speed,  edge preserved Kriging interpolation to predict the missing data in between slices, followed by merging the 3D reconstructed parts in parallel for visualization of the complete object.

\subsubsection{3D reconstruction with multiprocessing}
We assume that the slices registered with each other and are numbered in sequence.  The final 3D image to be reconstructed is represented
by a 3D matrix $M(i, j, k)$, which has a typical size of $m$ $\times$ $m$ $\times$ $(gn)$ as  the 2D slices are of dimension $m$ $\times$ $m$ and there are $n$ slices and $g$ is the number of pixels missing between slices. Initially, we split each image slice in $4$ equal parts, bisecting the image horizontally as well as vertically, thus each sub-image is of size $m/4$ $\times$ $m/4$ . We consider the leftmost corner of the original slice to be $(0, 0)$ and based on this assumption, the corresponding coordinate values of the split image with respect to the original slice are stored in an array $A$.  Now, for 3D reconstruction, we again divide the data set in $k$ equal parts, i.e., each sub set of images has $n/k$ slices in sequence.

For example, if we have a data set in which there are $50$ slices of size $1024 \times 1024$ and $k$ is 2, then we divide this input sequence into $8$ blocks with each block having $25$ slices of size  $512 \times 512$. Single instruction multiple data architecture (SIMD) is used to execute these $8$ blocks in parallel on 8 cores. The main part of the reconstruction procedure is the interpolation, hence by dividing the input set and applying multiprocessing on this yields an effective speedup. The input data is divided in such a way that they are mutually exclusive.

\begin{algorithm}
\begin{algorithmic}
\STATE \textbf{Input:} A sequence of $n$ 2D  slices along a plane $M$, $s_{gap}$ the inter-slice gap, integer $k$

\STATE\textbf{Output:} Set of 3D reconstructed sub images
\STATE \textbf{begin}	

  \STATE \hspace{0.1cm}\textit{Step 1:} Divide each image $M(i)$ into $4$ equal quadrants
\STATE \hspace{1.2cm} to get $4$ subsets of slices $M_1$ $\ldots$ $M_{4}$; 

\STATE\hspace{0.1cm}\textit{Step 2:} Divide each of the $4$ subsets into $4k$ subsets by  
\STATE \hspace{1.2cm} dividing the $n$ slices to get $k$ subsets  $M_1$ $\ldots$ $M_{4k}$ 
\STATE \hspace{1cm} of $n/k$ slices each;

\hspace{0.1cm} \textit{Step 3:} 
Execute  $R_i$ $=$ $Conc(M_i)$ on $4k$ parallel cores for  
\STATE \hspace{1.2cm} all the subsets $i=$ $1$ to $4k$; 
\STATE\hspace{1.2cm}/*partially fill up 3D matrices $R_i$, where $ i=$ $1$ to 
\STATE \hspace{1.2cm} $k$ with the available data $M_i$ along
\STATE \hspace{1.2cm}  the missing  axis for slice gap $s_{gap}$ */

\STATE \hspace{0.1cm}\textit{Step 4}
        Apply $F(i)[x,y,z]=$Edg\_prsvd\_Kriging$($R(i)$)$;
        
\STATE \hspace{0.1cm}\textit{Step 5} Store a set of $4k$ 3D sub images.
\STATE\textbf{end}
\end{algorithmic}
\caption{ Proposed Algorithm : 3D reconstruction from 2D MR image using multiprocessing}
\label{algo1}
\end{algorithm}

At first, for each block, a 3D matrix is created with the given data and each inter slice gap is padded with $NaN$. For each block, we calculate the maximum and minimum possible value, assuming that the measurement precision is correct upto 6 decimal points. Edge preserved Kriging interpolation is applied to calculate the value of the missing pixels. After this, we get 3D sub-images in each core which are to be joined together to get the final 3D volumetric image. Algorithm \ref{algo1} shows a step wise implementation of this approach.

\subsubsection{Edge preserved Kriging interpolation}

Initially, the 2D sub sequences of images are interpolated using kriging interpolation, in parallel, to generate the missing slices in between two consecutive slices.  Universal kriging \cite{25}, the widely used technique in spatial analysis and computer experiments, has been chosen for interpolation. It is applied in 3D domain considering all possible 64 neighbours  of a voxel. It gives better result than other state of the art interpolation technique as it computes the value for the unknown data point using a weighted linear sum of
known data values. The weights are chosen to minimize
the estimation error variance and to maintain unbiasedness. We get a 3D sub matrix with the interpolated slices that fill in the missing data. From this 3D matrix, we now consider the 2D images along the $YZ$ plane. Shearlet transform is again applied on these 2D images. We carry out multiprocessing and $8$ images are evaluated simultaneously to speed up the process. After applying shearlet transform, edge detection is carried out as in \cite{29}. We can easily get the orientation of each edge point $e$ from the shear coefficients \cite{29} as:

\begin{equation}
\theta_j(e) = \arg \max\limits_k|{SH(I)(j, k, e)}|.
\end{equation}

The blur effect incurred due to interpolation, is minimised by moving in clockwise direction and checking for $\theta_j$ of every edge point.  We consider a window of 16 edge pixels every time we traverse. If the direction of the edge points change frequently back and forth then we assume that there is noise. Then, we calculate $mean (\theta_j)$ and choose the points as edge with the mean value of $\theta_j$ corresponding to the earlier detected edge points. 

Suppose there is an edge orientation sequence  $\theta_1\theta_2\theta_1\theta_2\theta_3\theta_1\theta_2\theta_1\theta_1\theta_2\theta_1\theta_1\theta_2\theta_1\theta_2\theta_1$, then only the edge orientation for the initial and final pixel remain same and that for all the others changes to $mean(\theta_j)$, i.e., $\theta_1$ in this case,  and the edge points are changed accordingly. The pixel values adjacent to the newly selected edge pixel $e(x, y)$ are to be adjusted as follows: if the next selected pixel in clockwise direction is $e(x, y+1)$ then all the other pixels adjacent to $e(x, y)$ towards the left of it are assigned the median value of the pixels $e(x-1, y)$, $e(x-1, y+1)$, and $e(x-1, y-1)$, and the pixels adjacent to $e(x, y)$ towards the right of it are assigned the median value of the pixels $e(x+1, y)$, $e(x+1, y+1)$, and $e(x+1, y-1)$. Algorithm \ref{edgK} presents the steps of this approach.

\begin{algorithm}
\begin{algorithmic}
\STATE \textbf{Input:} Reconstructed 2D sub-images $R(i,j,1)$ extracted along $z$ axis
\STATE\textbf{Output:}  Edge preserved 2D sub-images $R(i,j,1)$
\STATE \textbf{begin}	
             \STATE \hspace{0.1cm}\textit{Step 1:} Apply shearlet transform;
             
            \STATE \hspace{0.1cm}\textit{Step 2:} Find the edges and the orientation $\theta$ of each edge \STATE \hspace{1.2cm} pixel;
            
             \STATE \hspace{0.1cm}\textit{Step 3:} Select an arbitrary edge pixel and move in 
             \STATE \hspace{1.2cm} clockwise direction for a window of 16 pixels;
             \STATE \hspace{1.2cm} (size of window may be chosen as per the dataset) 
              
              \STATE \hspace{0.1cm}\textit{Step 4:} Find the mean $\theta$ of these 16 pixels and select the \STATE \hspace{1.2cm} new edge pixel with $\theta$ orientation from the 8
              \STATE \hspace{1.2cm}  neighbours of the previous edge pixel if 
              \STATE \hspace{1.2cm} orientation of edge pixel is not $\theta$;
             
            \STATE \hspace{0.1cm}\textit{Step 5:} Adjust the 8 neighbour pixels of the newly found 
            \STATE \hspace{1.2cm} edge pixel $(x,y)$ by replacing 3 adjacent pixels to 
            \STATE \hspace{1.2cm} the left of it with their median, and the 3 adjacent 
            \STATE \hspace{1.2cm} pixels to the right of it with their median, 
            i.e,
              \STATE
           \STATE \hspace{1.2cm}$e(x$-$1, y)$ = $e(x$-$1, y$+$1)$ = $e(x$-$1, y$-$1)$ = \\  
           \STATE \hspace{1.2cm}$ median(e(x$-$1, y), e(x$-$1, y$+$1), e(x$-$1, y$-$1))$;
          \STATE
           \STATE \hspace{1.2cm} $e(x$+$1, y)$ = $e(x$+$1, y$+$1)$ = $e(x$+$1, y$-$1)$ = \\  
           \STATE \hspace{1.2cm}$ median(e(x$+$1, y), e(x$+$1, y$+$1), e(x$+$1, y$-$1))$.
          
\STATE\textbf{end}
					
\end{algorithmic}
\caption{ Edge preserved Kriging interpolation()}
\label{edgK}
\end{algorithm}


 
\subsection{3D visualization of the complete object}

For 3D visualization, we need to merge the 3D sub matrices by incorporating all the 3D sub matrices into a single 3D matrix based on their reference position. While dividing the data in eight equal parts we have stored the matrix indices to which they will correspond in the final image. Thus, by simply filling in the data form the sub matrices based on the matrix index we get the final 3D image.
Then,  for better viewing, we break the 3D matrix in voxels and further apply the Marching cubes algorithm followed by smoothing \cite{32} for 3D
reconstruction. We apply color map and activate rotation operation and display the image on screen so that
the user can rotate the 3D image and view all sides of it as needed. 


\subsection{3D visualization of a particular section of the object}

The reconstructed volume data, $\mathcal V$ = $F$, is simply a set of voxels (i.e., 3-cells \cite{79}), and it can easily be cut and viewed using digital planes. 
The user needs to specify as input the view-direction and view-depth. 
Accordingly, the real plane $P$ with the corresponding orientation and depth in the local coordinate system is considered. 
Then, its corresponding thinnest digital model 
${\mathbb D}(P)$\footnote{referred to as {\em $2$-minimal digital plane} 
in the literature of digital geometry \cite{79}} is taken up, 
and its intersection with $\mathcal V$ is computed using elementary set-theoretic operations. 
As ${\mathbb D}(P)$ is 2-minimal, it partitions $\mathcal V$ into two well-defined components that are guaranteed to be 2-separable from each other; 
that is, there does not exist any 2-connected path of voxels from one component to the other. 
If the user  wants to isolate, cut, or process a block out of the 3D volume, then he/she needs to specify the cut-planes accordingly, as given in Algorithm~\ref{algo2}. 
As it involves thinnest digital planes, the operations with intersecting, cutting, joining, and related procedures including viewing are fast and efficient.  


\begin{algorithm}
\begin{algorithmic}
\STATE \textbf{Input:} Reconstructed 3D sub-images, $R_i$, array of boundary coordinated, $C$, user specified coordinates of the bounded area to be extracted and displayed, $UC$
\STATE\textbf{Output:}  3D image for visualization, $PR$, of the required area
\STATE \textbf{begin}	
            
 \STATE \hspace{0.1cm}\textit{Step 1:} Search for the specified coordinates,$UC$ in $C$, 
   \STATE \hspace{1.2cm} Choose all $R_i$ that contains $UC$
     \STATE \hspace{1.2cm} Merge the thus found $R_i$ to get $PR$;
  
 \STATE \hspace{0.1cm}\textit{Step 2:} 
   if size($PR$) $>$  ($512 \times 512 \times 40$) then
 \STATE \hspace{1.2cm} Resize the  $PR$ to ($512 \times 512 \times 40$) using seam 
  \STATE \hspace{1.2cm} carving technique;
             
 \STATE \hspace{0.1cm}\textit{Step 3:} Fill $PR$ with $0$ for the area that lies outside the   
 \STATE \hspace{1.2cm} specified bounded region;
             
 \STATE \hspace{0.1cm} \textit{Step 4:}  Apply marching cube on $PR$; 

\STATE \hspace{0.1cm} \textit{Step 5:} R = $Colormap$(PR);	/*MATLAB function*/		

\STATE \hspace{0.1cm} \textit{Step 6:} $Rotate\_para$(R); /*MATLAB function*/

\STATE \hspace{0.1cm} \textit{Step 7:} $Display$(R). /*MATLAB function*/

\STATE\textbf{end}
	
					
\end{algorithmic}
\caption{ Proposed Algorithm : 3D visualization of required specified area}
\label{algo2}
\end{algorithm}


\section{Experimental Analysis}

We validated our fast 3D volumetric image reconstruction method on 24 real life T2-MRI data ($512\times512$ pixels) of human spine  with an inter slice gap of $3$ to $5 mm$, and 30 real life T2-MRI data ($512\times512$ pixels) of human brain  collected from Bangur Institute of Neurosciences, S.S.K.M, Kolkata and brain MRI data set of python.
 
\subsection{Multiprocessing}
Multiprocessing was carried out on a 32 core CPU with 64 GB memory. The edge preserved kriging interpolation for 3D reconstruction form 2d slices was executed in parallel for all submatrices. In addition to employing multiprocessing with 8 cores, we have also experimented  with 16 cores by breaking the data set into further smaller parts. But in this case the overhead time of joining the 3D submatrices is high, thus no evident gain in time is obtained compared to that with 8 cores. We also performed the experiment with 4 cores by  dividing each slice only into 4 parts but not halving the sequence of 2D slices. In this case, the time required to execute the interpolation for large sized data was approximately 2 mins 23 seconds which is higher than that for using 8 cores. Thus, we decided to continue with 8 cores to execute and speed up the reconstruction for our entire dataset. Table \ref{tab_time} in  shows the comparative time and overhead required with respect to the number of cores. The times required are an approximate value with an average image size $1024\times 1024$, and input set of 44 slices. The overhead time in this case is considered to be the time required to split the input dataset, the transfer time of each input set from CPU to each core, transfer time of each result from each core to CPU, time required to join all the sub matrices. We have tested our approach by decreasing or increasing the number of cores by a factor of 2 for the ease of splitting the data.

\begin{table}
\centering
\caption{Comparative study of number of cores with respect to execution time and overhead-time}
\label{tab_time}
 \begin{tabular}{|c|c|c|}
\hline
&&\\
No. of Cores  &Reconstruction time   &Overhead time\\
&&\\
\hline
&&\\
1&3 mins&5seconds\\
&&\\
\hline
&&\\
4&1 mins  8 seconds& 15 seconds\\
&&\\
\hline
&&\\
8&34 seconds&20 seconds\\
&&\\
\hline
&&\\
16&20 seconds&34 seconds\\
&&\\
\hline
 \end{tabular} 
 \end{table}
 




\begin{figure}[]
\centering
\subfloat[Original image slice]{\label{fig:ls}\includegraphics[width=2.5cm,height=4cm]{lumbarspineslice90.jpg}}
         \hspace{1 cm}
         \vspace{0.2mm}
	\subfloat[Image slice divided into 4 equal parts ]{\label{fig:split}\includegraphics[width=2.5cm,height=4cm]{split.JPG}}
         \hspace{2 cm}
         \vspace{0.2mm}
	\subfloat[Sub-images of the slice]{\label{fig:splitted4}\includegraphics[width=7cm,height=2.5cm]{splitset.JPG}}
	\caption{Splitting a 2D image slice for a human spine}	
	\label{MRspinesplit}
\end{figure}

\begin{figure}
	\centering
	\includegraphics[width=.48\textwidth]{stack.JPG}
	\hspace{.01 mm}
     \caption{Sequence of original 3D sub-images with missing data for a human spine}
     \label{stack}
\vspace{0.01 mm}
\end{figure}

\begin{figure}[]
\centering
	\subfloat[]{\label{fig:ri1}\includegraphics[width=4cm, height=4cm]{3.JPG}}
         \hspace{.01 mm}
	\subfloat[]{\label{fig:ri2}\includegraphics[width=4cm,height=4cm]{1.JPG}}
\hspace{.01 mm}
	\subfloat[]{\label{fig:ri3}\includegraphics[width=4cm, height=4cm]{7.JPG}}
	\hspace{.01 mm}
		\subfloat[]{\label{fig:ri4}\includegraphics[width=4cm, height=4cm]{5.JPG}}
		\hspace{.01 mm}
			\subfloat[]{\label{fig:ri5}\includegraphics[width=4cm, height=4cm]{6.JPG}}
			\hspace{.01 mm}
			\subfloat[]{\label{fig:ri6}\includegraphics[width=4cm, height=4cm]{8.JPG}}
			\hspace{.01 mm}
			\subfloat[]{\label{fig:ri7}\includegraphics[width=4cm, height=4cm]{4.JPG}}
			\hspace{.01 mm}
			\subfloat[]{\label{fig:ri8}\includegraphics[width=4cm, height=4cm]{2.JPG}}
			\hspace{.01 mm}
	\caption{3D reconstructed sub-images for a human spine}	
	\label{3Dsub}
\vspace{0.2mm}
\end{figure}

\begin{figure}
	\centering
\subfloat[]{\includegraphics[width=3cm, height=5cm]{3dan.jpg}}
	\hspace{.02 mm}
	\subfloat[]	{\includegraphics[width=3cm, height=5cm]{3dpjpg.jpg}}
	
     \caption{Complete 3D visualization for a human spine after joining the eight 3D sub-images: (a) Anterior view, (b) Posterior view  [a,b,c,d,e,f,g,h marked on this figure corresponds to the sub-images in Figure \ref{3Dsub}] }	
     \label{3Dspine}
\vspace{0.01 mm}
\end{figure}


\begin{figure}[]
\centering
	\subfloat[Captured slice \#90 along the coronal plane for the slice sequence of Figure 1]{\includegraphics[width=3.5cm,height=4.6cm]{lumbarspineslice90.jpg}}
         \hspace{.4 mm}
         \vspace{0.2mm}
	\subfloat[Reconstructed slice 90 along the coronal plane for slice sequence of Figure \ref{3Dspine} ]{\label{fig:ri}\includegraphics[width=3.5cm,height=4.6cm]{lumbarspineslice90rc.jpg}}
\hspace{.01 mm}
	\caption{Slice \# 90 for a human spine along the coronal plane as captured and after reconstruction}
	\label{brlumbar}
\vspace{0.2mm}
\end{figure}
\subsection{Our results for human spine and brain}
Initially,  the  $2D$ slices are split in 4 sub parts as shown in Figure \ref{MRspinesplit} for a human spine, and the data set in divided in two parts. We use single instruction multiple data architecture using 8 logical cores. In parallel, for each block of sub-image, a $3D$ matrix is created which is filled with the corresponding data leaving the specified slice gap in between slices as specified for each set as shown in Figure \ref{stack}. Then edge preserved  kriging interpolation is used to generate the 3d subimages. If we want to visualize these sub-images, then we can apply marching cube with color map and visualize the images as shown in Figure \ref{3Dsub} and the complete 3D image for visualisation of full spine is as in Figure \ref{3Dspine}. 

\begin{figure}[]
\centering
\subfloat[Input Sequence]{\label{fig:ri1}\includegraphics[width=7cm,height=7cm]{cervicalinput.JPG}}
         \hspace{2 cm}
         \vspace{0.2mm}
	\subfloat[3D Reconstructed Image]{\label{fig:ri3d}\includegraphics[width=5cm,height=5cm]{3d3hc.jpg}}
         \hspace{2 cm}
         \vspace{0.2mm}
	\subfloat[Axial Slice]{\label{fig:ri23}\includegraphics[width=2.5cm,height=3.5cm]{23.jpg}}
\hspace{.01 mm}
	\subfloat[Sagittal Slice]{\label{fig:ri7hcs}\includegraphics[width=2.5cm, height=3.5cm]{7hcs.jpg}}
	\hspace{.01 mm}
	\subfloat[Coronal Slice]{\label{fig:ri8hcc}\includegraphics[width=2.5cm, height=3.5cm]{18hcc.jpg}}
	\caption{3D Reconstruction of MRI for a human spine and slicing it along the three orthogonal planes as specified}	
	\label{MRspine}
\end{figure}

\begin{figure}[]
\centering
	\subfloat[Captured slice \#63 along the saggital plane for Slice sequence of Figure \ref{fig:ri1} ]{\includegraphics[width=3.5cm,height=4.6cm]{7hcs.jpg}}
         \hspace{.4 mm}
         \vspace{0.2mm}
	\subfloat[3D Reconstructed slice \#63 along the saggital plane  for slice sequence of Figure \ref{fig:ri3d} ]{\label{fig:ri}\includegraphics[width=3.5cm,height=4.6cm]{7hcsrc.jpg}}
\hspace{.01 mm}
	\caption{Slice \#63 for a human spine along the saggital plane  for slice sequence of Figure \ref{fig:ri3d} while capture vs after reconstruction}
	\label{brsag}
\vspace{0.2mm}
\end{figure}

\begin{figure}[]
\centering
	\subfloat[Input Sequence of slices with slice gap of 3mm ]{\label{fig:rifull}\includegraphics[width=7cm, height=6cm]{fullspineinput.JPG}}
         \hspace{.01 mm}
         \vspace{0.2mm}
	\subfloat[Reconstructed 3D image]{\label{fig:ri3dfull}\includegraphics[width=5cm,height=6cm]{fullspine3d.JPG}}
\hspace{.01 mm}
	\caption{3D reconstruction from MRI image sequence of a human full spine}	
	\label{brfull}
\vspace{0.2mm}
\end{figure}

\begin{figure}[]
\centering
	\subfloat[Captured slice \#148 along the axial plane for slice sequence of Figure \ref{fig:rifull}]{\includegraphics[width=3cm,height=2cm]{slice48.jpg}}
         \hspace{4 mm}
         \vspace{0.2mm}
	\subfloat[Reconstructed slice \#148 along the axial plane for slice sequence of Figure \ref{fig:ri3dfull} ]{\label{fig:ri48}\includegraphics[width=3cm,height=2cm]{slice48rc.jpg}}
\hspace{.01 mm}
	\caption{Slice \#148 for a human spine along the axial plane for slice sequence of Figure \ref{fig:ri3dfull} while capture vs after reconstruction }
	\label{brax}
\vspace{0.2mm}
\end{figure}


\begin{figure}[]
\centering
	\subfloat[Input Sequence of slices with slice gap of 5mm  ]{\label{fig:ri}\includegraphics[width=7cm, height=6cm]{spinelumbarinput.JPG}}
         \hspace{.01 mm}
         \vspace{0.2mm}
	\subfloat[Reconstructed 3D]{\label{fig:ri}\includegraphics[width=5cm,height=4cm]{3d2.jpg}}
\hspace{.01 mm}
	\caption{3D reconstruction from MRI image sequence of human lumbar spine}	
	\label{brl}
\vspace{0.2mm}
\end{figure}
 In Figure \ref{MRspine} an example of 3D volumetric reconstruction, visualization as well as slicing are depicted. Figures \ref{brfull}, \ref{brl}, \ref{mribra} provide the results of our 3D reconstruction  from a sequence of 2D slices for a full spine, a lumbar spine and a brain respectively. We compared the results after slicing with the available ground truth data (Figures \ref{brlumbar}, \ref{brsag}, \ref{brax}, \ref{brcompare} for human lumbar along coronal, saggital, axial planes and human brain along sagittal plane respectively) based on mutual information, entropy, root mean square error and  structural similarity index (see Appendix~\ref{sm}). The average time taken by our $3D$ reconstruction fo human spine is 54 seconds, depending on the size of the input data set. The time taken for slicing is a fraction of a second. The average accuracy percentage of our method, as shown in Table \ref{tab:1},  is calculated as :

\begin{equation}
%\resizebox{1.0\hsize}{!}{$
A_{imagetype}{\%}=Avg(A_{imagetype}^{ED}{\%},A_{imagetype}^{{MI}}{\%},A_{imagetype}^{{RMSE}}{\%},A_{imagetype}^{{SSIM}}{\%})%$}
\end{equation}
where,
\begin{equation*}
%\resizebox{1.0\hsize}{!}{$
A_{imagetype}^{ED}{\%}=\dfrac{1}{n}[\sum_{i=1}^{n}[1-[\dfrac{1}{m}\sum_{gap=1}^{m}\dfrac{\sum_{all\_slices,all\_gaps}ED(slice)}{total\_{slices}}]]]*100%$}
\end{equation*}


\begin{equation*}
%\resizebox{1.0\hsize}{!}{$
A_{imagetype}^{{MI}}{\%}=\dfrac{1}{n}[\sum_{i=1}^{n}[1-[\dfrac{1}{m}\sum_{gap=1}^{m}\dfrac{\sum_{all\_slices,all\_gaps}(MI(G,G)-MI(G,slice))}{total\_{slices}}]]]*100%$}
\end{equation*}

\begin{equation*}
%\resizebox{1.0\hsize}{!}{$
A_{imagetype}^{{RMSE}}{\%}=\dfrac{1}{n}[\sum_{i=1}^{n}[1-[\dfrac{1}{m}\sum_{gap=1}^{m}\dfrac{\sum_{all\_slices,all\_gaps}(RMSE(G,slice))}{total\_{slices}}]]]*100%$}
\end{equation*}

\begin{equation*}
%\resizebox{1.0\hsize}{!}{$
A_{imagetype}^{{SSIM}}{\%}=\dfrac{1}{n}[\sum_{i=1}^{n}[1-[\dfrac{1}{m}\sum_{gap=1}^{m}\dfrac{\sum_{all\_slices,all\_gaps}(SSIM(G,slice))}{total\_{slices}}]]]*100%$}
\end{equation*}
and the average time is calculated as :
\begin{equation*}
\centering
%\resizebox{0.7\hsize}{!}{$
T_{imagetype}=\dfrac{1}{s}[\sum_{i=1}^{s} time(t)]]%$}
\end{equation*}
where $s$ is the number of  set of images of $imagetype$,
$n$ the number of samples of each type. Entropy difference ($ED$) is the percentage of difference in entropy between the sliced image and the original captured slice. Mutual information $MI$($G$, slice) is the  mutual information between the original ground truth image and the sliced image and $MI(G,G)$ is the mutual information if the original image is the sliced image. Root mean square error (RMSE) is computed between the ground truth image and the reconstructed image. Structural similarity index measure (SSIM) is the measure of structural similarity between the reconstructed image and the ground truth image. Table \ref{tab:1} shows the average mutual information,entropy difference,root mean square error and structural similarity index measure along each plane sliced after reconstruction, using our proposed method in comparison to the original data as shown in Figures \ref{brlumbar}, \ref{brsag}, \ref{brax} \ref{brcompare}. The average accuracy of the slices after applying our method along all 3 sequence of slices is $98.86\%$. Since the original data had a slice gap of either 5mm, 3mm or 1mm, these slices could be exactly matched with the original data set. The average accuracy of the slices generated after reconstruction compared to the original slices is 98.86\%. 

Table \ref{tab:3} gives a comparison  of the existing works for slice interpolation  with our proposed technique based on average accuracy, and  our algorithm outperforms the other methods.

\begin{table}
\centering
\caption{Average Mutual Information, Entropy Difference, Root Mean Square and Structural Similarity Index of slices along all three axes for cervical, lumbar and full spine dataset}
\label{tab:1}
 \begin{tabular}{|c|c|c|c|c|c|c|}
\hline
Sample  &Inter-slice  &Avg&Avg  &Avg  &Avg \\
&gap& MI&$EN\_D$&RMSE&SSIM\\
\hline
 &1mm &5.768 &0.001 &0.0001 &0.992\\
Brain &2mm&5.77 &0.001&0.0001&0.992\\
MRI&3mm&5.766&0.001&0.0001&0.99\\
&4mm &5.768&0.001&0.0001&0.99\\
&5mm &5.769 &0.001&0.0001&0.99\\
\hline
 &1mm &6.411 &0.001 &0.0001 &0.989\\
Spine &2mm&6.410 &0.001&0.0001&0.989\\
MRI&3mm&6.411&0.001&0.0001&0.989\\
&4mm &6.410&0.001&0.0001&0.988\\
&5mm &6.411 &0.001&0.0001&0.988\\
\hline
 \end{tabular} 
 \end{table}


\begin{table}
\centering
\caption{Comparison of accuracy of interslice reconstruction on the real data sets for human brain and spine that were available}
\label{tab:3}
 \begin{tabular}{|c|c|c|}
\hline
&&\\
Type of image &Algorithm  &Accuracy \%\\
&&\\
\hline
&&\\
&3plane method \cite{22}&98.8\% \\
&&\\

&&\\
&Kriging interpolation \cite{25}&85\%\\
&&\\

&&\\
Brain & Deep learning \cite{43}&94\% \\
&&\\

&&\\
&Bilinear \cite{12} &95\%\\
&&\\

&&\\
&Bilinear Bicubic Combined \cite{39}\ &97.86\%\\
&&\\
&&\\
&Proposed method&$\bold{99\%}$\\
&&\\
\hline
&&\\
&3plane method \cite{22}&98.8\%\\
&&\\

&&\\
&Kriging interpolation \cite{25}&80\%\\
&&\\

&&\\
Spine &Deep learning \cite{43}&90\%\\
&&\\

&&\\
&Bilinear \cite{12} &90\%\\
&&\\

&&\\
&Bilinear Bicubic Combined \cite{39}\ &96\%\\
&&\\

&&\\
&Proposed method&$\bold{98.9\%}$\\
&&\\
\hline
 \end{tabular} 
 \end{table}


%\begin{table*}
%\centering
%\caption{Comparitive Analysis}
%\label{tab:1}
% \begin{tabular}{|c|c|c|c|}
%\hline
%Region  &Method used &Avg Accuracy\\
%\hline
%Cervical Spine &kriging&85\%\\
%&Bicubic&89\%\\
%&Proposed Method&95\%\\
%\hline
%lumbar Spine   &kriging  &87\%\\
%&Bicubic&90\%\\
%&Proposed Method&96\%\\
%\hline
%Full Spine &kriging &85\%\\
%&Bicubic&88\%\\
%&Proposed Method&95\%\\
%\hline
%
% \end{tabular} 
% \end{table*}

 
 \subsection{User Interface}
We have designed an user interface as shown in Figure \ref{usr}, with which an user can generate a 3D view  from a set of 2D MRI slices along a single plane and slice out this 3D as per specifications along any plane or the user can cut out a portion of the thus formed 3D using virtual scissors.

\begin{figure}[]
\centering
	\subfloat[Input Sequence of slices ]{\label{fig:ribraininput}\includegraphics[width=7cm, height=5cm]{brainnewinput.JPG}}
         \hspace{.01 mm}
         \vspace{0.2mm}
	\subfloat[Reconstructed 3D]{\label{fig:ribrain3d}\includegraphics[width=4cm,height=5.7cm]{3dbrain.jpg}}
\hspace{.01 mm}
	\caption{3D reconstruction from MRI image sequence of human brain}	
	\label{mribra}
\vspace{0.2mm}
\end{figure}
\begin{figure}[]
\centering
	\subfloat[Captured slice \#40 along the saggital plane for Slice sequence of Figure \ref{fig:ribraininput}]{\includegraphics[width=3cm,height=2cm]{brainsag26.JPG}}
         \hspace{4 mm}
         \vspace{0.2mm}
	\subfloat[Reconstructed slice \#40  along the saggital plane, given the Slice sequence of Figure \ref{fig:ribrain3d} ]{\label{fig:ri}\includegraphics[width=3cm,height=2cm]{brainsagrec26.JPG}}
\hspace{.01 mm}
	\caption{Slice \#40 for a human brain along the saggital plane given the Slice sequence of Figure \ref{fig:ribrain3d} while capture vs after reconstruction}
	\label{brcompare}
\vspace{0.2mm}
\end{figure}

\begin{figure*}
	\centering
\subfloat[Initial User Interface]{\label{fig:usr1}\includegraphics[width=7cm,height=5cm]{usr1.JPG}}
         \hspace{1 cm}
         \vspace{0.2mm}
	\subfloat[3D output with interface to view slices/a portion of the image ]{\label{fig:usr3}\includegraphics[width=7cm,height=5cm]{usr3.JPG}}
         \hspace{2 cm}
         \vspace{0.2mm}
	\subfloat[Interface after selecting a portion by user]{\label{fig:splitted4}\includegraphics[width=7cm,height=5cm]{usr4.JPG}}
	 \hspace{2 cm}
         \vspace{0.2mm}
	\subfloat[Output after selecting a portion]{\label{fig:splitted4}\includegraphics[width=7cm,height=5cm]{usr5.JPG}}	
	 \caption{User Interface}
	\label{usr}
\end{figure*}


 
\section{Concluding Remarks}

We have proposed an efficient multiprocessing based method for 3D volumetric reconstruction. We have carried out our experiments on  real life human  brain and spine  MRI collected from Bangur Institute of Neurosciences, Kolkata. For the purpose of reconstruction from large volume of data, we  divided  the data from its original size into $4$ equal parts and further divided the data set in two parts, thus ensuring minimal information loss for internal data. Edge preserved kriging interpolation has been applied to obtain accurate shape and size preservation. The reconstruction of the sub parts were carried out in parallel in two phases. The time taken for reconstruction is approximately 56 seconds and that for virtual slicing in 2D is a fraction of a second. Viewing a cross sectional 3D internal tissue structure  takes approximately 25 seconds, depending on the number of slices required to get the desired cross section.  The proposed approach has been shown experimentally to work on MR images of human spine, brain and can be extended for any form of medical image with minor modification based on the technique of capture. In neuroscience, brain and spine are both of equal importance, thus we chose both these organs to validate our results.   Validation of this method on medical images of other types of organs, as a sequence of 2D slices can be taken up in future. This  method will help medical practitioners for pre-operative research as well as in virtual 3D volumetric modelling.


\section*{Acknowledgment}


The authors would like to thank Dr. Alok Pandit, Bangur Institute of Neurosciences, Kolkata, India for helping us with understanding of the real MR images and also validating our results.

\begin{comment}


\begin{thebibliography}{li}

\bibitem{1}
R. Smith-Bindman, D. L. Miglioretti, E. Johnson, ``Use of diagnostic imaging studies and associated radiation exposure for patients enrolled in large integrated health care systems," 1996-2010. \textit{JAMA.} 2012;307(22): 2400-2409. doi:10.1001/jama.2012.5960



\bibitem{3}

M. Hammer, ``MRI Physics: Diffusion-Weighted Imaging" 2013-2014. \textit{XRayPhysics.} Retrieved 2017-10-15.

\bibitem{10}
X. Hu , K. K. Tan, D. N. Levin, S. Galhatra,  J. F. Mullan, J. Hekmatpanah, J. P. Spire, ``Three-dimensional magnetic resonance images of the brain: application to neurosurgical planning", \textit{Journal of Neurosurgery} 72:433-440, 1990.

\bibitem{37}
J. Udupa, H.  Hng, K. S. Chuang,   ``Surface and volume rendering in three-dimensional imaging: A comparison," \textit{Journal of Digital Imaging, The official journal of the Society for Computer Applications in Radiology}. 4. 159-68. 10.1007/BF03168161, (1991).
%


\bibitem{8}
C. S. Hung, C. F. Huang, M. Ouhyoung, ``Fast Volume Rendering for Medical Image Data", \textit{Communication and Multimedia Laboratory Department of Computer Science and Information Engineering National Taiwan University, Taiwan.}

\bibitem{31}
P. Herghelegiu, M. Gavrilescu, V. Manta, ``Visualization of Segmented Structures in 3D Multimodal Medical Data Sets",\textit{ Advances in Electrical and Computer Engineering}, Volume 11, No. 3, 2011.

\bibitem{5}
D. Kleut,  M. Jovanovic, B. R. Reljin, ``3D  Visualisation  of  MRI  images  using  MATLAB", \textit{Journal  Of  Automatic  Control,  University Of Belgrade}, VOL. 16:1-3, 2006

\bibitem{6}
M. Borse , S. B. Patil, B. S. Patil, ``Literature survey for 3d reconstruction of brain MRI images",\textit{ IJRET }Volume: 02 Issue: 11,Nov-2013. Available @ http://www.ijret.org

\bibitem{9}
P. Rinck, ``Magnetic Resonance in Medicine." \textit{The Basic Textbook of the European Magnetic Resonance Forum. 11th edition; 2017.} Electronic version 11, published 1 June 2017

\bibitem{2}
R. W. Brown, Y.N. Cheng, E. M. Haacke, M. R. Thompson, R. Venkatesan, ``Magnetic Resonance Imaging: Physical Principles and Sequence Design", Wiley, ISBN 978-1-118-63397-7, (2 May 2014).

\bibitem{38}
http://large.stanford.edu/courses/2011/ph240/tilghman1/


\bibitem{12}
S. Ghoshal, P. Chatterjee, S. Banu, A. Chakrabarti,  E. Mangina, ``A Software tool for 3D visualization and slicing of MR images. In Proceedings of the 10th EAI International Conference on Simulation Tools and Techniques (SIMUTOOLS)". ACM, New York, NY, USA, 103-107,2017.
 

\bibitem{39}
S. Ghoshal, S Banu, A Chakrabarti, S. Sur-Kolay, A. Pandit: ``3D reconstruction of spine image from 2D MRI slices along one axis", IET Image Processing, 2020, 14, (12), p. 2746-2755, DOI: 10.1049/iet-ipr.2019.0800IET Digital Library, https://digital-library.theiet.org/content/journals/10.1049/iet-ipr.2019.0800

\bibitem{4}
G. T. Herman, H. K. Liu, ``Three-dimensional display of human organs from computed tomograms",\textit{ Computer Graphic Imaging Proceedings} 9: 1$-$29

\bibitem{28}
R. Haq, R. Aras, D.A. Besachio,  R.C. Borgie, M.A. Audette, ``3D lumbar spine intervertebral disc segmentation and compression simulation from MRI using shape-aware models.", \textit{International Journal of Computer Assisted Radiology and Surgery},10:45-54, 2015.

\bibitem{33}

A. Caliskan and U. Cevik, ``Three-Dimensional Modeling in Medical Image Processing by Using Fractal Geometry", \textit{Journal of Computers}, 12. 479-485. 10.17706/jcp.12.5.479-485, 2017. 
%

\bibitem{35}
Y. Huang, Z. Qiu and Z. Song, ``3D reconstruction and visualization from 2D CT images," 2011 IEEE International Symposium on IT in Medicine and Education, Cuangzhou, 2011, pp. 153-157.
doi: 10.1109/ITiME.2011.6132078

\bibitem{26}
G. D. Rubin, C. F.Beaulieu, V. Argiro, H. Ringl , A. M. Norbash, J. F. Feller,  M. D. Dake, R. B. Jeffrey , S. Napel,`` Perceptive volume rendering of CT and MR images: applications for endoscopic imaging.", \textit{Radiology} 199:321-330, 1996.

\bibitem{15}
W. E. Lorensen and H. E. Cline, ``Marching cubes: A high resolution 3d surface construction algorithm". SIGGRAPH Computational Graphics 21 (4): 163$$169, 1987.


\bibitem{22}
W. Narkbuakaew, S. Sotthivirat, D. Gansawat, P. Yampri, K. Koonsanit, W. Areeprayolkij, W. Sinthupinyo, and S. Watcharabutsarakham, ``3d Surface Reconstruction Of Large Medical Data Using Marching Cubes In Vtk" \textit{National Electronics and Computer Technology Center}, Phahon Yothin Rd, Klong Luang, Pathumthani, Thailand


\bibitem{21}
H. C. Purchase, N. Andrienko, T. J. Jankun-Kelly, M. Ward, ``Theoretical Foundations of Information Visualization",Information Visualization, LNCS 4950, pp. 46-64, 2008. 

\bibitem{25}
M. R. Stytz, R. W. Parrott, ``Using KRIGING for 3D Medical Imaging", Computerized Medical Imaging and Graphics, Vol. 17. No. 6, pp. 421-442, 1993

\bibitem{29}
M. A. Duval-Poo, F. Odone and E. De Vito, ``Edges and Corners With Shearlets," in IEEE Transactions on Image Processing, vol. 24, no. 11, pp. 3768-3780, Nov. 2015, doi: 10.1109/TIP.2015.2451175

\bibitem{24}
H. R. Shahdoosti and O. Khayat, ``Image denoising using sparse representation classification and non-subsampled shearlet transform", \textit{Signal, Image and Video Processing} 10: 1081.(2016) \textit{https://doi.org/10.1007/s11760-016-0862-0}

\bibitem{18}
 R. C. Gonzalez, R. E. Woods, ``Digital Image Processing", \textit{Pearson Education,} Third Edition,2009

\bibitem{16}
K. Gu, G. Zhai, X. Yang, W. Zhang and M. Liu, ``Subjective and objective quality assessment for images with contrast change," \textit{ IEEE International Conference on Image Processing, Melbourne,} VIC, pp. 383-387, 2013.
\textit{doi: 10.1109/ICIP.2013.6738079}

\bibitem{19}
Z. Wang, A. C. Bovik, H. R. Sheikh and E. P. Simoncelli, ``Image quality assessment: from error visibility to structural similarity," \textit{IEEE Transactions on Image Processing,} vol. 13, no. 4, pp. 600-612, April 2004.
\textit{doi: 10.1109/TIP.2003.819861}


\bibitem{23}
C. Q. T. Thanh, N. T. Hai, ``Trilinear Interpolation Algorithm for Reconstruction of 3D MRI Brain Image",\textit{American Journal of Signal Processing},p-ISSN: 2165-9354    e-ISSN: 2165-9362;  7(1): 1-11,2017.


\bibitem{34}
Z. Mai, J. Rajan, M. Verhoye and J. Sijbers, ``Robust edge-directed interpolation of magnetic resonance images," 2011 4th International Conference on Biomedical Engineering and Informatics (BMEI), Shanghai, 2011, pp. 472-476. doi: 10.1109/BMEI.2011.6098244

\bibitem{20}
G. M. Nielson, ``On marching cubes," \textit{IEEE Transactions on Visualization and Computer Graphics}, vol. 9, no. 3, pp. 283-297, July-Sept. 2003.
\textit{doi: 10.1109/TVCG.2003.1207437}

\bibitem{41}
H. J. Wang, C. Y Lee, J. H. Lai, Y. C. Chang, C. M. Chen,  ``Image registration method using representative feature detection and iterative coherent spatial mapping for infrared medical images with flat regions". Scientific Reports. 12. 10.1038/s41598-022-11379-2, (2022).

\bibitem{44}
M. Xia, H. Yang, Y. Huang, Y Qu, Y. Guo, G. Zhou, F. Zhang, Y. Wang,  "AwCPM-Net: A Collaborative Constraint GAN for 3D Coronary Artery Reconstruction in Intravascular Ultrasound Sequences," in IEEE Journal of Biomedical and Health Informatics, vol. 26, no. 7, pp. 3047-3058, July 2022, doi: 10.1109/JBHI.2022.3147888.

\bibitem{45}
J. Fu, D. Xiao, D. Li, H. R. Thomas, C. Li,
``Stochastic reconstruction of 3D microstructures from 2D cross-sectional images using machine learning-based characterization,
Computer Methods in Applied Mechanics and Engineering'',
Volume 390,
2022,
114532,
ISSN 0045-7825,
https://doi.org/10.1016/j.cma.2021.114532.

\bibitem{46}
D. Srinikhil, A. Darsani, V. Sai, K. Keerthana, C. G,  G. Sathya, (2022). Evaluation of Tools Used for 3D Reconstruction of 2D Medical Images, Proceedings of Second International Conference on Advances in Computer Engineering and Communication Systems (pp.287-298) 

\bibitem{14} 
R. Keys, ``Cubic convolution interpolation for digital image processing''. \textit{IEEE Transactions on Acoustics, Speech, and Signal Processing.} 29 (6): 11531160, 1981.

\bibitem{43}
J. J. Wang, S. W. Chen, J. Q. Shao, X. W. Gu, and H. S. Zhu. 2021. ``Medical 3D reconstruction based on deep learning for healthcare''. In Proceedings of the 14th IEEE/ACM International Conference on Utility and Cloud Computing Companion (UCC '21). Association for Computing Machinery, New York, NY, USA, Article 6, 15. https://doi.org/10.1145/3492323.3495618




\bibitem{40}
B. Deka, S. Datta, ``Calibrationless joint compressed sensing reconstruction for rapid parallel MRI ``,\textit{Biomedical Signal Processing and Control},Volume 58,101871,ISSN 1746-8094, (2020)
https://doi.org/10.1016/j.bspc.2020.101871.

\bibitem{42}
D. Kim, J. Trzasko, M. Smelyanskiy, C. Haider, P. Dubey, and A. Manduca, ``High-Performance 3D Compressive Sensing MRI Reconstruction Using Many-Core Architectures``, \textit{International Journal of Biomedical Imaging}, vol. 2011, Article ID 473128, 11 pages, (2011) https://doi.org/10.1155/2011/473128

 \bibitem{27}
 G. Shi, ``Data Mining and Knowledge Discovery for Geoscientists", Elsevier, 2014, Pages 238-274, ISBN 9780124104372,
https://doi.org/10.1016/B978-0-12-410437-2.00008-4.


\bibitem{32}
C. Shekhar, ``On Simplified Application of Multidimensional Savitzky-Golay Filters and Differentiators." \textit{Progress in Applied Mathematics in Science and Engineering.} 1705 (1): 020014.(2015)

\bibitem{79}
R. Klette, A. Rosenfeld,  ``Digital Geometry: {Geometric} Methods for Digital Picture Analysis'', published by Morgan Kaufmann, San Francisco, 2004
\end{thebibliography}
\end{comment}


%\bibliographystyle{abbrvnat}
%\bibliographystyle{unsrtnat}
\bibliographystyle{elsarticle-num}
\bibliography{bibliography}

% An example of a floating figure using the graphicx package.
% Note that \label must occur AFTER (or within) \caption.
% For figures, \caption should occur after the \includegraphics.
% Note that IEEEtran v1.7 and later has special internal code that
% is designed to preserve the operation of \label within \caption
% even when the captionsoff option is in effect. However, because
% of issues like this, it may be the safest practice to put all your
% \label just after \caption rather than within \caption{}.
%
% Reminder: the "draftcls" or "draftclsnofoot", not "draft", class
% option should be used if it is desired that the figures are to be
% displayed while in draft mode.
%
%\begin{figure}[!t]
%\centering
%\includegraphics[width=2.5in]{myfigure}
% where an .eps filename suffix will be assumed under latex, 
% and a .pdf suffix will be assumed for pdflatex; or what has been declared
% via \DeclareGraphicsExtensions.
%\caption{Simulation results for the network.}
%\label{fig_sim}
%\end{figure}

% Note that the IEEE typically puts floats only at the top, even when this
% results in a large percentage of a column being occupied by floats.


% An example of a double column floating figure using two subfigures.
% (The subfig.sty package must be loaded for this to work.)
% The subfigure \label commands are set within each subfloat command,
% and the \label for the overall figure must come after \caption.
% \hfil is used as a separator to get equal spacing.
% Watch out that the combined width of all the subfigures on a 
% line do not exceed the text width or a line break will occur.
%
%\begin{figure*}[!t]
%\centering
%\subfloat[Case I]{\includegraphics[width=2.5in]{box}%
%\label{fig_first_case}}
%\hfil
%\subfloat[Case II]{\includegraphics[width=2.5in]{box}%
%\label{fig_second_case}}
%\caption{Simulation results for the network.}
%\label{fig_sim}
%\end{figure*}
%
% Note that often IEEE papers with subfigures do not employ subfigure
% captions (using the optional argument to \subfloat[]), but instead will
% reference/describe all of them (a), (b), etc., within the main caption.
% Be aware that for subfig.sty to generate the (a), (b), etc., subfigure
% labels, the optional argument to \subfloat must be present. If a
% subcaption is not desired, just leave its contents blank,
% e.g., \subfloat[].


% An example of a floating table. Note that, for IEEE style tables, the
% \caption command should come BEFORE the table and, given that table
% captions serve much like titles, are usually capitalized except for words
% such as a, an, and, as, at, but, by, for, in, nor, of, on, or, the, to
% and up, which are usually not capitalized unless they are the first or
% last word of the caption. Table text will default to \footnotesize as
% the IEEE normally uses this smaller font for tables.
% The \label must come after \caption as always.
%
%\begin{table}[!t]
%% increase table row spacing, adjust to taste
%\renewcommand{\arraystretch}{1.3}
% if using array.sty, it might be a good idea to tweak the value of
% \extrarowheight as needed to properly center the text within the cells
%\caption{An Example of a Table}
%\label{table_example}
%\centering
%% Some packages, such as MDW tools, offer better commands for making tables
%% than the plain LaTeX2e tabular which is used here.
%\begin{tabular}{|c||c|}
%\hline
%One & Two\\
%\hline
%Three & Four\\
%\hline
%\end{tabular}
%\end{table}


% Note that the IEEE does not put floats in the very first column
% - or typically anywhere on the first page for that matter. Also,
% in-text middle ("here") positioning is typically not used, but it
% is allowed and encouraged for Computer Society conferences (but
% not Computer Society journals). Most IEEE journals/conferences use
% top floats exclusively. 
% Note that, LaTeX2e, unlike IEEE journals/conferences, places
% footnotes above bottom floats. This can be corrected via the
% \fnbelowfloat command of the stfloats package.











% if have a single appendix:
%\appendix[Proof of the Zonklar Equations]
% or
%\appendix  % for no appendix heading
% do not use \section anymore after \appendix, only \section*
% is possibly needed

% use appendices with more than one appendix
% then use \section to start each appendix
% you must declare a \section before using any
% \subsection or using \label (\appendices by itself
% starts a section numbered zero.)


%
\begin{comment}


\appendices
\section{Proof of the First Zonklar Equation}
Appendix one text goes here.

% you can choose not to have a title for an appendix
% if you want by leaving the argument blank
\section{}
Appendix two text goes here.


% use section* for acknowledgment



% Can use something like this to put references on a page
% by themselves when using endfloat and the captionsoff option.
\ifCLASSOPTIONcaptionsoff
  \newpage
\fi



% trigger a \newpage just before the given reference
% number - used to balance the columns on the last page
% adjust value as needed - may need to be readjusted if
% the document is modified later
%\IEEEtriggeratref{8}
% The "triggered" command can be changed if desired:
%\IEEEtriggercmd{\enlargethispage{-5in}}

% references section

% can use a bibliography generated by BibTeX as a .bbl file
% BibTeX documentation can be easily obtained at:
% http://mirror.ctan.org/biblio/bibtex/contrib/doc/
% The IEEEtran BibTeX style support page is at:
% http://www.michaelshell.org/tex/ieeetran/bibtex/
%\bibliographystyle{IEEEtran}
% argument is your BibTeX string definitions and bibliography database(s)
%\bibliography{IEEEabrv,../bib/paper}
%
% <OR> manually copy in the resultant .bbl file
% set second argument of \begin to the number of references
% (used to reserve space for the reference number labels box)
\begin{thebibliography}{1}

\bibitem{IEEEhowto:kopka}
H.~Kopka and P.~W. Daly, \emph{A Guide to \LaTeX}, 3rd~ed.\hskip 1em plus
  0.5em minus 0.4em\relax Harlow, England: Addison-Wesley, 1999.
  
  

\end{thebibliography}

% biography section
% 
% If you have an EPS/PDF photo (graphicx package needed) extra braces are
% needed around the contents of the optional argument to biography to prevent
% the LaTeX parser from getting confused when it sees the complicated
% \includegraphics command within an optional argument. (You could create
% your own custom macro containing the \includegraphics command to make things
% simpler here.)
%\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{mshell}}]{Michael Shell}
% or if you just want to reserve a space for a photo:


% if you will not have a photo at all:


% insert where needed to balance the two columns on the last page with
% biographies
%\newpage


% You can push biographies down or up by placing
% a \vfill before or after them. The appropriate
% use of \vfill depends on what kind of text is
% on the last page and whether or not the columns
% are being equalized.

%\vfill

% Can be used to pull up biographies so that the bottom of the last one
% is flush with the other column.
%\enlargethispage{-5in}

\end{comment}
%\begin{comment}
\appendix

\section{ Kriging interpolation}\label{kriging}
Kriging \cite{25} is a spatial prediction technique that combines a regression of the dependent variable on auxiliary variables  with interpolation of the regression residuals. It is mathematically equivalent to the interpolation method variously called universal kriging and kriging with external drift, where auxiliary predictors are used directly to solve the kriging weights. 
In the case of kriging with external drift (KED), predictions at new locations are made by \cite{25}:




\begin{equation}
    \hat{z}_{KED}(s_0)=\sum_{i=1}^n w_i^{KED}(s_0) \cdot z(s_i)
\end{equation}

for

\begin{equation}
    \sum_{i=1}^n w_i^{KED}(s_0) \cdot q_k(s_i)= q_k(s_0)
\end{equation}

for $k=1, \dots, p$ or in matrix notation:
\begin{equation}
    \hat{z}_{KED}(s_0)=\delta_0^T \cdot z
\end{equation}

where $z$ is the target, $q_k$'s are the predictor variables $i.e.,$ values at a new location ($s_0$), $\delta_0$ is the vector of $KED$ weights ($w_i^{KED}$), $p$ is the number of predictors and $z$ is the vector of $n$ observations at primary locations. The $KED$ weights are solved using the extended matrices:

\begin{equation}
\resizebox{0.91\hsize}{!}{$
    \lambda_0^{KED}=\{w_1^{KED}(s_0),\dots, w_n^{KED}(s_0), \varphi_0(s_0),\dots, \varphi_p(s_0)\}^T= C^{KED-1} \cdot C_0^{KED}$}
\end{equation}

where $\lambda_0^{KED}$ is the vector of solved weights, $\varphi_p$ are the Lagrange multipliers, $C^{KED}$ is the extended covariance matrix of residuals and $c_0^{KED}$ is the extended vctor covariance at new location.


\section{Discrete Shearlet Transform}\label{shear}


A shearlet \cite{29} is generated by the dilation, shearing and translation of a function $\psi$ $\in$ $L^2 (\mathbb{R}^2)$, called the mother shearlet, in the following way \cite{24}

\begin{equation}
    \psi_{a,s,t}(x) = a^{-3/4}\psi(A^{-1}_a S^{-1}_s(x-t))
\end{equation}
where $t$ $\in$ $(\mathbb{R}^2)$
is a translation, $A_a$ is a scaling (or dilation)
matrix and $S_s$ a shearing matrix defined respectively by

\[
A_a = \begin{bmatrix} 
    a & 0 \\
    
    0 & \sqrt{a} 
    \end{bmatrix}
\qquad
S_s = \begin{bmatrix} 
    1 & -s \\
    
    0 & 1 
    \end{bmatrix}
\]

with $a \in (\mathbb{R}^+)$
 and  $s \in (\mathbb{R})$. The anisotropic dilation $A_a$ controls the scale of the shearlets, by applying a different dilation factor along the two axes. The shearing matrix $S_s$, not expansive, determines the orientation of the shearlets. The normalization factor $a^{-3/4}$ ensures that $\vert\psi_{a,s,t}\vert = \vert\psi\vert$, where $\vert\psi\vert$ is the norm in $L^2 (\mathbb{R}^2)$. In the classical setting the mother shearlet $\psi$ is assumed to
factorize in the Fourier domain as
\begin{equation}
\hat{\psi}(\omega_1, \omega_2) = \hat{\psi_1}(\omega_1)\hat{\psi_2}(\frac{
\omega_2}{\omega_1})
\end{equation}

where $\hat{\psi}$ is the Fourier transform of $\psi$, $\psi_1$ is a one dimensional wavelet and $\hat{\psi_2}$ is any non-zero square-integrable function. There are several examples of functions $\psi_1$, $\psi_2$ satisfying these
properties. The shearlet definition in the frequency domain:
\begin{equation}
\hat{\psi}_{a,s,t}(\omega_1, \omega_2) = a^{3/4}\hat{\psi_1}(a\omega_1)\hat{\psi}_2
(\frac{\omega_2 - s \omega_1}{\sqrt{a}\omega_1})
e^{-2\pi i(\omega_1,\omega_2)t}.
\end{equation}
The shearlet transform $SH(f)$ of a signal  $f$ $\in$ $L^2 (\mathbb{R}^2)$  is defined by

\begin{equation}
    SH(f)(a, s, t) = <f, \psi_{a,s,t}>
\end{equation}

where $<f, \psi_{a,s,t}>$ is the scalar product in $L^2 (\mathbb{R}^2)$. As a consequence of the Plancherel formula : 

\begin{equation}
\begin{split}
SH(f)(a, s, t) = a^{3/4}\int_{\hat{R}^2}
\hat{f}(\omega_1, \omega_2)\hat{\psi}_1(a\omega_1)\\\times \hat{\psi}_2
(\frac{\omega_2 - s \omega_1}{\sqrt{a}\omega_1}) \times e^{-2\pi i(\omega_1,\omega_2)}
d\omega_1 d\omega_2.
\end{split}
\end{equation}




\section{Marching Cube}\label{mc}
The Marching Cubes method \cite{15} is a simple iterative algorithm for creating a mesh of triangles to represent the surfaces of a given 3D object specified as a 3D array of pixels. The algorithm works by \emph{marching} over the entire image of the 3D object, which has been equally sub-divided into cubes. Each cube is called a voxel. The algorithm then determines whether the 3D image intersects a cube, and assigns boolean values to the corners of the cube accordingly. Intuitively, suppose the values at all the corners of the cube (i.e., the voxel) are 1. Then the cube is said to lie entirely inside the surface. Similarly, if all the corners of the cube have a value 0, then the cube is said to lie entirely outside the surface. In both cases, there would be no triangular surface passing through the cube. The main aim of the algorithm is to determine triangles (its intersection points, normals) in the cases where some of the corners of a cube are 1 and the others are 0. As there are 8 corners in a cube (voxel), there  are 256 cube configurations, which are stored in a look-up table. The final mesh is obtained through iterative linear interpolation. We have used the Marching cube algorithm \cite{20} for surface rendering part of the 3D reconstruction from the 2D slices of MRI.


\section{Similarity Metrics for 2D Images}\label{sm}
The definitions of three most popular similarity metrics for images that we have used for validating our results, are presented next. We have sliced out 2D images from the virtually reconstructed 3D image and have matched these with the ground-truth 2D images that we started with.

\subsection{ Root Mean Square Error}
The Root Mean Square Error ($RMSE$) \cite{16} is a frequently used measure of the differences between values predicted by an estimator and the values observed. It is the square root of the average of the square of the errors. $RMSE$ of an image $f_{1}(m,n)$ with respect to an image $f_{2}(m,n)$ is defined as the square root of the mean square error ($MSE$)\cite{19} :

\begin{equation*}
MSE=\dfrac{1}{MN}\sum_{n=1}^{N}\sum_{m=1}^{M}{[f_{2}(n,m)-f_{1}(n,m)]}^2
\end{equation*}

where $M\times N$ is the size of the image matrix. Thus, the 
$RMSE=\sqrt{MSE}$.
A value of $RMSE$ close to 0 implies that the probability of the two images being identical is higher. 


\subsection{Mutual Information}
 Mutual information ($MI$) \cite{16} is a quantitative measure of information about one random variable $(Y)$ with respect to another random variable $(X)$. However, information is a reduction in the uncertainty of a variable. Hence, the higher is the mutual information between $X$ and $Y$, the lower is the uncertainty of $X$ given $Y$, or vice versa. 
 Let $G$ and $R$ be the ground-truth and the reconstructed images respectively. The mutual information $MI_{GR}$ between them is defined as:
\begin{equation}
MI_{GR}=\sum_{g,r}p_{G,R}(g,r)\log \frac{p_{GR}(g,r)}{p_G(g)p_R(r)}
\end{equation} 
where $p_{G,R}$ is the joint probability mass function of $G$ and $R$, $p_G$ and $p_R$ are the marginal probability mass function of $G$ and $R$, and $g$, $r$ represent the pixel value of image $G$ and image $ R$ respectively. 

$MI$ has been used for checking the accuracy of the generated missing data, compared to the available ground truth data. The greater is the $MI$ between a generated slice and the original  slice, the better is the accuracy of our reconstruction algorithm.

\subsection{Structural Similarity Index Method}
Structural Similarity Index Method ($SSIM$) \cite{19} is quantifies the similarity between two images. The measurement or prediction of image quality is based on an initial uncompressed or distortion-free image as reference. The $SSIM$ index between two images $X$ and $Y$  is obtained as:

\begin{equation*}
SSIM(X,Y)=\dfrac{(2\mu_X\mu_Y+c_1)(2\sigma_{XY}+c_2)}{(\mu_X^2+\mu_Y^2+c_1)(\sigma_X^2+\sigma_Y^2+c_2)}
\end{equation*}

where $\mu_X$ and $\mu_Y$ are the mean, $\sigma_X^2$ and $\sigma_Y^2$ are the standard deviation  of $X$ and $Y$ respectively. $\sigma_{XY}$ is the covariance of $X$ and $Y$, and the constants $c_1$ and $c_2$  stabilize the ratio with a weak denominator. $SSIM = 1$ implies that the two images can be considered to be identical.
%\end{comment}
% that's all folks

\end{document}


