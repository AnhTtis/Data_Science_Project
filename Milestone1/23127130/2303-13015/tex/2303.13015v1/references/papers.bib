@article{lecun-mnisthandwrittendigit-2010,
  added-at = {2010-06-28T21:16:30.000+0200},
  author = {LeCun, Yann and Cortes, Corinna},
  groups = {public},
  howpublished = {http://yann.lecun.com/exdb/mnist/},
  interhash = {21b9d0558bd66279df9452562df6e6f3},
  intrahash = {935bad99fa1f65e03c25b315aa3c1032},
  keywords = {MSc _checked character_recognition mnist network neural},
  lastchecked = {2016-01-14 14:24:11},
  timestamp = {2016-07-12T19:25:30.000+0200},
  title = {{MNIST} handwritten digit database},
  username = {mhwombat},
  year = 2010
}

@inproceedings{gupta_protonn_2017,
	title = {{ProtoNN}: {Compressed} and {Accurate} {kNN} for {Resource}-scarce {Devices}},
	shorttitle = {{ProtoNN}},
	abstract = {Several real-world applications require real-time prediction on resource-scarce devices such as an Internet of Things (IoT) sensor. Such applications demand prediction models with small storage and...},
	language = {en},
	urldate = {2020-10-04},
	booktitle = {International {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Gupta, Chirag and Suggala, Arun Sai and Goyal, Ankit and Simhadri, Harsha Vardhan and Paranjape, Bhargavi and Kumar, Ashish and Goyal, Saurabh and Udupa, Raghavendra and Varma, Manik and Jain, Prateek},
	month = jul,
	year = {2017},
	note = {ISSN: 2640-3498},
	pages = {1331--1340},
	file = {Full Text PDF:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/QJNIMNZM/Gupta et al. - 2017 - ProtoNN Compressed and Accurate kNN for Resource-.pdf:application/pdf;Snapshot:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/XADJJCUE/gupta17a.html:text/html}
}

@misc{xiao2017fashionmnist,
      title={Fashion-MNIST: a Novel Image Dataset for Benchmarking Machine Learning Algorithms},
      author={Han Xiao and Kashif Rasul and Roland Vollgraf},
      year={2017},
      eprint={1708.07747},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}


@article{zhang_deep_2018,
author={Zhang, Chaoyun and Patras, Paul and Haddadi, Hamed},
  journal={IEEE Communications Surveys   Tutorials},
  title={Deep Learning in Mobile and Wireless Networking: A Survey},
  year={2019},
  volume={21},
  number={3},
  pages={2224-2287}
}

@book{goodfellow_deep_2016,
	title = {Deep {Learning}},
	publisher = {MIT Press},
	author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
	year = {2016},
	file = {Deep Learning:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/5442W4I5/www.deeplearningbook.org.html:text/html}
}

@article{shi_spectrum_2019,
	title = {Spectrum {Data} {Poisoning} with {Adversarial} {Deep} {Learning}},
	abstract = {Machine learning has been widely applied in wireless communications. However, the security aspects of machine learning in wireless applications have not been well understood yet. We consider the case that a cognitive transmitter senses the spectrum and transmits on idle channels determined by a machine learning algorithm. We present an adversarial machine learning approach to launch a spectrum data poisoning attack by inferring the transmitter's behavior and attempting to falsify the spectrum sensing data over the air. For that purpose, the adversary transmits for a short period of time when the channel is idle to manipulate the input for the decision mechanism of the transmitter. The cognitive engine at the transmitter is a deep neural network model that predicts idle channels with minimum sensing error for data transmissions. The transmitter collects spectrum sensing data and uses it as the input to its machine learning algorithm. In the meantime, the adversary builds a cognitive engine using another deep neural network model to predict when the transmitter will have a successful transmission based on its spectrum sensing data. The adversary then performs the over-the-air spectrum data poisoning attack, which aims to change the channel occupancy status from idle to busy when the transmitter is sensing, so that the transmitter is fooled into making incorrect transmit decisions. This attack is more energy efficient and harder to detect compared to jamming of data transmissions. We show that this attack is very effective and reduces the throughput of the transmitter substantially.},
	urldate = {2019-08-02},
	journal = {arXiv:1901.09247 [cs]},
	author = {Shi, Yi and Erpek, Tugba and Sagduyu, Yalin E. and Li, Jason H.},
	month = jan,
	year = {2019},
	note = {arXiv: 1901.09247},
	keywords = {Computer Science - Machine Learning, Computer Science - Networking and Internet Architecture},
	file = {arXiv\:1901.09247 PDF:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/DQZLIMYA/Shi et al. - 2019 - Spectrum Data Poisoning with Adversarial Deep Lear.pdf:application/pdf;arXiv.org Snapshot:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/UIXCCTAD/1901.html:text/html}
}

@inproceedings{shi_adversarial_2018,
	title = {Adversarial {Deep} {Learning} for {Cognitive} {Radio} {Security}: {Jamming} {Attack} and {Defense} {Strategies}},
	shorttitle = {Adversarial {Deep} {Learning} for {Cognitive} {Radio} {Security}},
	abstract = {This paper presents an adversarial machine learning approach to launch jamming attacks on wireless communications and introduces a defense strategy. In a cognitive radio network, a transmitter senses channels, identifies spectrum opportunities, and transmits data to its receiver in idle channels. On the other hand, an attacker may also sense channels, identify busy channels and aim to jam transmissions of legitimate users. In a dynamic system with complex channel, traffic and interference characteristics, the transmitter applies some pre-trained machine learning algorithm to classify a channel as idle or busy. This classifier is unknown to the attacker that senses a channel, captures the transmitter's decisions by tracking the acknowledgments and applies deep learning (in form of an exploratory attack, i.e., inference attack) to build a classifier that is functionally equivalent to the one at the transmitter. This approach is shown to support the attacker to reliably predict successful transmissions based on the sensing results and effectively jam these transmissions. Then, a defense scheme is developed against adversarial deep learning by exploiting the sensitivity of deep learning to training errors. The transmitter deliberately takes a small number of wrong actions (in form of a causative attack, i.e., poisoning attack, launched against the attacker) when it accesses the spectrum. The objective is to prevent the attacker from building a reliable classifier. For that purpose, the attacker systematically selects when to take wrong actions to balance the conflicting effects of deceiving the attacker and making correct transmission decisions. This defense scheme successfully fools the attacker into making prediction errors and allows the transmitter to sustain its performance against intelligent jamming attacks.},
	booktitle = {2018 {IEEE} {International} {Conference} on {Communications} {Workshops} ({ICC} {Workshops})},
	author = {Shi, Y. and Sagduyu, Y. E. and Erpek, T. and Davaslioglu, K. and Lu, Z. and Li, J. H.},
	month = may,
	year = {2018},
	keywords = {adversarial deep learning, adversarial machine learning approach, causative attack, cognitive radio, cognitive radio security, intelligent jamming attacks, jamming, Jamming, learning (artificial intelligence), Machine learning, Machine learning algorithms, radio spectrum management, Radio transmitters, Receivers, Sensors, telecommunication computing, telecommunication network reliability, telecommunication security, wireless channels, wireless communications},
	pages = {1--6},
	file = {IEEE Xplore Abstract Record:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/CXSGMQ6Z/8403655.html:text/html;IEEE Xplore Full Text PDF:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/TIERSHWM/Shi et al. - 2018 - Adversarial Deep Learning for Cognitive Radio Secu.pdf:application/pdf}
}

@article{goodfellow_nips_2016,
	title = {{NIPS} 2016 {Tutorial}: {Generative} {Adversarial} {Networks}},
	shorttitle = {{NIPS} 2016 {Tutorial}},
	abstract = {This report summarizes the tutorial presented by the author at NIPS 2016 on generative adversarial networks (GANs). The tutorial describes: (1) Why generative modeling is a topic worth studying, (2) how generative models work, and how GANs compare to other generative models, (3) the details of how GANs work, (4) research frontiers in GANs, and (5) state-of-the-art image models that combine GANs with other methods. Finally, the tutorial contains three exercises for readers to complete, and the solutions to these exercises.},
	urldate = {2019-08-02},
	journal = {arXiv:1701.00160 [cs]},
	author = {Goodfellow, Ian},
	month = dec,
	year = {2016},
	note = {arXiv: 1701.00160},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv\:1701.00160 PDF:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/8R9HAKM3/Goodfellow - 2016 - NIPS 2016 Tutorial Generative Adversarial Network.pdf:application/pdf;arXiv.org Snapshot:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/LJ5QIGS8/1701.html:text/html}
}

@article{esteban_real-valued_2017,
	title = {Real-valued ({Medical}) {Time} {Series} {Generation} with {Recurrent} {Conditional} {GANs}},
	abstract = {Generative Adversarial Networks (GANs) have shown remarkable success as a framework for training models to produce realistic-looking data. In this work, we propose a Recurrent GAN (RGAN) and Recurrent Conditional GAN (RCGAN) to produce realistic real-valued multi-dimensional time series, with an emphasis on their application to medical data. RGANs make use of recurrent neural networks (RNNs) in the generator and the discriminator. In the case of RCGANs, both of these RNNs are conditioned on auxiliary information. We demonstrate our models in a set of toy datasets, where we show visually and quantitatively (using sample likelihood and maximum mean discrepancy) that they can successfully generate realistic time-series. We also describe novel evaluation methods for GANs, where we generate a synthetic labelled training dataset, and evaluate on a real test set the performance of a model trained on the synthetic data, and vice-versa. We illustrate with these metrics that RCGANs can generate time-series data useful for supervised training, with only minor degradation in performance on real test data. This is demonstrated on digit classiﬁcation from ‘serialised’ MNIST and by training an early warning system on a medical dataset of 17,000 patients from an intensive care unit. We further discuss and analyse the privacy concerns that may arise when using RCGANs to generate realistic synthetic medical time series data, and demonstrate results from differentially private training of the RCGAN.},
	language = {en},
	urldate = {2019-08-05},
	journal = {arXiv:1706.02633 [cs, stat]},
	author = {Esteban, Cristóbal and Hyland, Stephanie L. and Rätsch, Gunnar},
	month = jun,
	year = {2017},
	note = {arXiv: 1706.02633},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Esteban et al. - 2017 - Real-valued (Medical) Time Series Generation with .pdf:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/SZ7M8RDA/Esteban et al. - 2017 - Real-valued (Medical) Time Series Generation with .pdf:application/pdf}
}

@article{glorot_understanding_nodate,
	title = {Understanding the difﬁculty of training deep feedforward neural networks},
	abstract = {Whereas before 2006 it appears that deep multilayer neural networks were not successfully trained, since then several algorithms have been shown to successfully train them, with experimental results showing the superiority of deeper vs less deep architectures. All these experimental results were obtained with new initialization or training mechanisms. Our objective here is to understand better why standard gradient descent from random initialization is doing so poorly with deep neural networks, to better understand these recent relative successes and help design better algorithms in the future. We ﬁrst observe the inﬂuence of the non-linear activations functions. We ﬁnd that the logistic sigmoid activation is unsuited for deep networks with random initialization because of its mean value, which can drive especially the top hidden layer into saturation. Surprisingly, we ﬁnd that saturated units can move out of saturation by themselves, albeit slowly, and explaining the plateaus sometimes seen when training neural networks. We ﬁnd that a new non-linearity that saturates less can often be beneﬁcial. Finally, we study how activations and gradients vary across layers and during training, with the idea that training may be more difﬁcult when the singular values of the Jacobian associated with each layer are far from 1. Based on these considerations, we propose a new initialization scheme that brings substantially faster convergence.},
	language = {en},
	author = {Glorot, Xavier and Bengio, Yoshua},
	pages = {8},
	file = {Glorot and Bengio - Understanding the difﬁculty of training deep feedf.pdf:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/K76FK2JX/Glorot and Bengio - Understanding the difﬁculty of training deep feedf.pdf:application/pdf}
}

@article{heusel_gans_2017,
	title = {{GANs} {Trained} by a {Two} {Time}-{Scale} {Update} {Rule} {Converge} to a {Local} {Nash} {Equilibrium}},
	abstract = {Generative Adversarial Networks (GANs) excel at creating realistic images with complex models for which maximum likelihood is infeasible. However, the convergence of GAN training has still not been proved. We propose a two time-scale update rule (TTUR) for training GANs with stochastic gradient descent on arbitrary GAN loss functions. TTUR has an individual learning rate for both the discriminator and the generator. Using the theory of stochastic approximation, we prove that the TTUR converges under mild assumptions to a stationary local Nash equilibrium. The convergence carries over to the popular Adam optimization, for which we prove that it follows the dynamics of a heavy ball with friction and thus prefers ﬂat minima in the objective landscape. For the evaluation of the performance of GANs at image generation, we introduce the ‘Fréchet Inception Distance” (FID) which captures the similarity of generated images to real ones better than the Inception Score. In experiments, TTUR improves learning for DCGANs and Improved Wasserstein GANs (WGAN-GP) outperforming conventional GAN training on CelebA, CIFAR-10, SVHN, LSUN Bedrooms, and the One Billion Word Benchmark.},
	language = {en},
	urldate = {2019-09-15},
	journal = {arXiv:1706.08500 [cs, stat]},
	author = {Heusel, Martin and Ramsauer, Hubert and Unterthiner, Thomas and Nessler, Bernhard and Hochreiter, Sepp},
	month = jun,
	year = {2017},
	note = {arXiv: 1706.08500},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Heusel et al. - 2017 - GANs Trained by a Two Time-Scale Update Rule Conve.pdf:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/AH549E2R/Heusel et al. - 2017 - GANs Trained by a Two Time-Scale Update Rule Conve.pdf:application/pdf}
}

@article{goodfellow_generative_2014,
	title = {Generative {Adversarial} {Networks}},
	abstract = {We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to 1/2 everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples.},
	language = {en},
	urldate = {2019-09-23},
	journal = {arXiv:1406.2661 [cs, stat]},
	author = {Goodfellow, Ian J. and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
	month = jun,
	year = {2014},
	note = {arXiv: 1406.2661},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Goodfellow et al. - 2014 - Generative Adversarial Networks.pdf:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/6HIMGIJX/Goodfellow et al. - 2014 - Generative Adversarial Networks.pdf:application/pdf}
}

@article{borji_pros_2018,
	title = {Pros and {Cons} of {GAN} {Evaluation} {Measures}},
	abstract = {Generative models, in particular generative adversarial networks (GANs), have gained signiﬁcant attention in recent years. A number of GAN variants have been proposed and have been utilized in many applications. Despite large strides in terms of theoretical progress, evaluating and comparing GANs remains a daunting task. While several measures have been introduced, as of yet, there is no consensus as to which measure best captures strengths and limitations of models and should be used for fair model comparison. As in other areas of computer vision and machine learning, it is critical to settle on one or few good measures to steer the progress in this ﬁeld. In this paper, I review and critically discuss more than 24 quantitative and 5 qualitative measures for evaluating generative models with a particular emphasis on GAN-derived models. I also provide a set of 7 desiderata followed by an evaluation of whether a given measure or a family of measures is compatible with them.},
	language = {en},
	urldate = {2019-09-23},
	journal = {arXiv:1802.03446 [cs]},
	author = {Borji, Ali},
	month = feb,
	year = {2018},
	note = {arXiv: 1802.03446},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Borji - 2018 - Pros and Cons of GAN Evaluation Measures.pdf:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/BAIASIGB/Borji - 2018 - Pros and Cons of GAN Evaluation Measures.pdf:application/pdf}
}

@article{salimans_improved_2016,
	title = {Improved {Techniques} for {Training} {GANs}},
	abstract = {We present a variety of new architectural features and training procedures that we apply to the generative adversarial networks (GANs) framework. We focus on two applications of GANs: semi-supervised learning, and the generation of images that humans find visually realistic. Unlike most work on generative models, our primary goal is not to train a model that assigns high likelihood to test data, nor do we require the model to be able to learn well without using any labels. Using our new techniques, we achieve state-of-the-art results in semi-supervised classification on MNIST, CIFAR-10 and SVHN. The generated images are of high quality as confirmed by a visual Turing test: our model generates MNIST samples that humans cannot distinguish from real data, and CIFAR-10 samples that yield a human error rate of 21.3\%. We also present ImageNet samples with unprecedented resolution and show that our methods enable the model to learn recognizable features of ImageNet classes.},
	urldate = {2019-10-01},
	journal = {arXiv:1606.03498 [cs]},
	author = {Salimans, Tim and Goodfellow, Ian and Zaremba, Wojciech and Cheung, Vicki and Radford, Alec and Chen, Xi},
	month = jun,
	year = {2016},
	note = {arXiv: 1606.03498},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Neural and Evolutionary Computing},
	file = {arXiv\:1606.03498 PDF:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/5MYWZUJ2/Salimans et al. - 2016 - Improved Techniques for Training GANs.pdf:application/pdf;arXiv.org Snapshot:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/CAEIIJ2Q/1606.html:text/html}
}

@article{park_sinusoidal_2019,
	title = {Sinusoidal wave generating network based on adversarial learning and its application: synthesizing frog sounds for data augmentation},
	shorttitle = {Sinusoidal wave generating network based on adversarial learning and its application},
	abstract = {Simulators that generate observations based on theoretical models can be important tools for development, prediction, and assessment of signal processing algorithms. In order to design these simulators, painstaking effort is required to construct mathematical models according to their application. Complex models are sometimes necessary to represent a variety of real phenomena. In contrast, obtaining synthetic observations from generative models developed from real observations often require much less effort. This paper proposes a generative model based on adversarial learning. Given that observations are typically signals composed of a linear combination of sinusoidal waves and random noises, sinusoidal wave generating networks are first designed based on an adversarial network. Audio waveform generation can then be performed using the proposed network. Several approaches to designing the objective function of the proposed network using adversarial learning are investigated experimentally. In addition, amphibian sound classification is performed using a convolutional neural network trained with real and synthetic sounds. Both qualitative and quantitative results show that the proposed generative model makes realistic signals and is very helpful for data augmentation and data analysis.},
	urldate = {2019-10-16},
	journal = {arXiv:1901.02050 [cs, eess]},
	author = {Park, Sangwook and Han, David K. and Ko, Hanseok},
	month = jan,
	year = {2019},
	note = {arXiv: 1901.02050},
	keywords = {Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
	file = {arXiv\:1901.02050 PDF:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/TF2G994C/Park et al. - 2019 - Sinusoidal wave generating network based on advers.pdf:application/pdf;arXiv.org Snapshot:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/BWEF5D7A/1901.html:text/html}
}

@article{donahue_adversarial_2018,
	title = {Adversarial {Audio} {Synthesis}},
	abstract = {Audio signals are sampled at high temporal resolutions, and learning to synthesize audio requires capturing structure across a range of timescales. Generative adversarial networks (GANs) have seen wide success at generating images that are both locally and globally coherent, but they have seen little application to audio generation. In this paper we introduce WaveGAN, a first attempt at applying GANs to unsupervised synthesis of raw-waveform audio. WaveGAN is capable of synthesizing one second slices of audio waveforms with global coherence, suitable for sound effect generation. Our experiments demonstrate that, without labels, WaveGAN learns to produce intelligible words when trained on a small-vocabulary speech dataset, and can also synthesize audio from other domains such as drums, bird vocalizations, and piano. We compare WaveGAN to a method which applies GANs designed for image generation on image-like audio feature representations, finding both approaches to be promising.},
	urldate = {2019-10-16},
	journal = {arXiv:1802.04208 [cs]},
	author = {Donahue, Chris and McAuley, Julian and Puckette, Miller},
	month = feb,
	year = {2018},
	note = {arXiv: 1802.04208},
	keywords = {Computer Science - Machine Learning, Computer Science - Sound},
	file = {arXiv\:1802.04208 PDF:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/P6WB7T7F/Donahue et al. - 2018 - Adversarial Audio Synthesis.pdf:application/pdf;arXiv.org Snapshot:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/MXDADWB6/1802.html:text/html}
}

@article{springenberg_unsupervised_2016,
	title = {Unsupervised and semi-supervised learning with categorical generative adversarial networks},
	abstract = {In this paper we present a method for learning a discriminative classiﬁer from unlabeled or partially labeled data. Our approach is based on an objective function that trades-off mutual information between observed examples and their predicted categorical class distribution, against robustness of the classiﬁer to an adversarial generative model. The resulting algorithm can either be interpreted as a natural generalization of the generative adversarial networks (GAN) framework or as an extension of the regularized information maximization (RIM) framework to robust classiﬁcation against an optimal adversary. We empirically evaluate our method – which we dub categorical generative adversarial networks (or CatGAN) – on synthetic data as well as on challenging image classiﬁcation tasks, demonstrating the robustness of the learned classiﬁers. We further qualitatively assess the ﬁdelity of samples generated by the adversarial generator that is learned alongside the discriminative classiﬁer, and identify links between the CatGAN objective and discriminative clustering algorithms (such as RIM).},
	language = {en},
	author = {Springenberg, Jost Tobias},
	year = {2016},
	pages = {20},
	file = {Springenberg - 2016 - UNSUPERVISED AND SEMI-SUPERVISED LEARNING WITH CAT.pdf:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/KBTD2F2D/Springenberg - 2016 - UNSUPERVISED AND SEMI-SUPERVISED LEARNING WITH CAT.pdf:application/pdf}
}

@article{nielsen_gan_nodate,
	title = {{GAN} {Data} {Augmentation} {Through} {Active} {Learning} {Inspired} {Sample} {Acquisition}},
	abstract = {Data augmentation is frequently used to increase the effective training set size when training deep neural networks for supervised learning tasks. This technique is particularly beneficial when the size of the training set is small. Recently, data augmentation using GAN generated samples has been shown to provide performance improvement for supervised learning tasks. In this paper we propose a method of GAN data augmentation for image classification that uses the prediction uncertainty of the classifier network to determine the optimal GAN samples to augment the training set. We apply the acquisition function framework originally developed for active learning to evaluate the sample uncertainty. Preliminary experimental results are provided to demonstrate the benefit of this technique.},
	language = {en},
	author = {Nielsen, Christopher and Okoniewski, Michal},
	file = {Nielsen and Okoniewski - GAN Data Augmentation Through Active Learning Insp.pdf:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/DLR4DESE/Nielsen and Okoniewski - GAN Data Augmentation Through Active Learning Insp.pdf:application/pdf}
}

@article{antoniou_data_2018,
	title = {Data {Augmentation} {Generative} {Adversarial} {Networks}},
	abstract = {Effective training of neural networks requires much data. In the low-data regime, parameters are underdetermined, and learnt networks generalise poorly. Data Augmentation alleviates this by using existing data more effectively. However standard data augmentation produces only limited plausible alternative data. Given there is potential to generate a much broader set of augmentations, we design and train a generative model to do data augmentation. The model, based on image conditional Generative Adversarial Networks, takes data from a source domain and learns to take any data item and generalise it to generate other within-class data items. As this generative process does not depend on the classes themselves, it can be applied to novel unseen classes of data. We show that a Data Augmentation Generative Adversarial Network (DAGAN) augments standard vanilla classifiers well. We also show a DAGAN can enhance few-shot learning systems such as Matching Networks. We demonstrate these approaches on Omniglot, on EMNIST having learnt the DAGAN on Omniglot, and VGG-Face data. In our experiments we can see over 13\% increase in accuracy in the low-data regime experiments in Omniglot (from 69\% to 82\%), EMNIST (73.9\% to 76\%) and VGG-Face (4.5\% to 12\%); in Matching Networks for Omniglot we observe an increase of 0.5\% (from 96.9\% to 97.4\%) and an increase of 1.8\% in EMNIST (from 59.5\% to 61.3\%).},
	urldate = {2019-11-20},
	journal = {arXiv:1711.04340 [cs, stat]},
	author = {Antoniou, Antreas and Storkey, Amos and Edwards, Harrison},
	month = mar,
	year = {2018},
	note = {arXiv: 1711.04340},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Neural and Evolutionary Computing},
	file = {arXiv Fulltext PDF:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/YJDMND3G/Antoniou et al. - 2018 - Data Augmentation Generative Adversarial Networks.pdf:application/pdf;arXiv.org Snapshot:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/QJ75NZYD/1711.html:text/html}
}

@article{li_mad-gan:_2019,
	title = {{MAD}-{GAN}: {Multivariate} {Anomaly} {Detection} for {Time} {Series} {Data} with {Generative} {Adversarial} {Networks}},
	shorttitle = {{MAD}-{GAN}},
	abstract = {The prevalence of networked sensors and actuators in many real-world systems such as smart buildings, factories, power plants, and data centres generate substantial amounts of multivariate time series data for these systems. Many of these cyber-physical systems (CPSs) are engineered for mission-critical tasks and are thus targets for cyber-attacks. The rich sensor data can be continuously monitored for intrusion events through anomaly detection. However, conventional threshold-based anomaly detection methods are inadequate due to the dynamic complexities of these systems, while supervised machine learning methods are unable to exploit the large amounts of data due to the lack of labeled data. On the other hand, current unsupervised machine learning approaches have not fully exploited the spatial-temporal correlation and other dependencies amongst the multiple variables (sensors/actuators) in the system for detecting anomalies. Most of the current techniques also employed simple comparison between the present states and predicted normal ranges for anomaly detection, which can be inadequate given the highly dynamic behaviors of the systems. In this work, we propose an unsupervised multivariate anomaly detection method based on Generative Adversarial Networks (GANs), using the Long-Short-Term-Memory Recurrent Neural Networks (LSTM-RNN) as the base models (namely, the generator and discriminator) in the GAN framework to capture the temporal correlation of time series distributions. Instead of treating each data stream independently, our proposed Multivariate Anomaly Detection with GAN (MAD-GAN) framework considers the entire variable set concurrently to capture the latent interactions amongst the variables. We also fully exploit both the generator and discriminator produced by the GAN, using a novel anomaly score called DR-score to detect anomalies by discrimination and reconstruction. We have tested our proposed MAD-GAN using two recent datasets collected from real world CPS: the Secure Water Treatment (SWaT) and the Water Distribution (WADI) datasets. Our experimental results showed that the proposed MAD-GAN is eﬀective in reporting anomalies caused by various cyberintrusions compared in these complex real-world systems.},
	language = {en},
	urldate = {2019-11-24},
	journal = {arXiv:1901.04997 [cs, stat]},
	author = {Li, Dan and Chen, Dacheng and Shi, Lei and Jin, Baihong and Goh, Jonathan and Ng, See-Kiong},
	month = jan,
	year = {2019},
	note = {arXiv: 1901.04997},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Li et al. - 2019 - MAD-GAN Multivariate Anomaly Detection for Time S.pdf:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/TUZVEV3Y/Li et al. - 2019 - MAD-GAN Multivariate Anomaly Detection for Time S.pdf:application/pdf}
}

@article{schlegl_unsupervised_2017,
	title = {Unsupervised {Anomaly} {Detection} with {Generative} {Adversarial} {Networks} to {Guide} {Marker} {Discovery}},
	abstract = {Obtaining models that capture imaging markers relevant for disease progression and treatment monitoring is challenging. Models are typically based on large amounts of data with annotated examples of known markers aiming at automating detection. High annotation effort and the limitation to a vocabulary of known markers limit the power of such approaches. Here, we perform unsupervised learning to identify anomalies in imaging data as candidates for markers. We propose AnoGAN, a deep convolutional generative adversarial network to learn a manifold of normal anatomical variability, accompanying a novel anomaly scoring scheme based on the mapping from image space to a latent space. Applied to new data, the model labels anomalies, and scores image patches indicating their fit into the learned distribution. Results on optical coherence tomography images of the retina demonstrate that the approach correctly identifies anomalous images, such as images containing retinal fluid or hyperreflective foci.},
	urldate = {2019-11-25},
	journal = {arXiv:1703.05921 [cs]},
	author = {Schlegl, Thomas and Seeböck, Philipp and Waldstein, Sebastian M. and Schmidt-Erfurth, Ursula and Langs, Georg},
	month = mar,
	year = {2017},
	note = {arXiv: 1703.05921},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/8NEURRRL/Schlegl et al. - 2017 - Unsupervised Anomaly Detection with Generative Adv.pdf:application/pdf;arXiv.org Snapshot:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/ZTFMQP4I/1703.html:text/html}
}

@inproceedings{arjovsky_wasserstein_2017,
	title = {Wasserstein {Generative} {Adversarial} {Networks}},
	abstract = {We introduce a new algorithm named WGAN, an alternative to traditional GAN training. In this new model, we show that we can improve the stability of learning, get rid of problems like mode collapse...},
	language = {en},
	urldate = {2019-11-25},
	booktitle = {International {Conference} on {Machine} {Learning}},
	author = {Arjovsky, Martin and Chintala, Soumith and Bottou, Léon},
	month = jul,
	year = {2017},
	pages = {214--223},
	file = {Full Text PDF:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/EGG95GQ2/Arjovsky et al. - 2017 - Wasserstein Generative Adversarial Networks.pdf:application/pdf;Snapshot:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/33P4HWII/arjovsky17a.html:text/html}
}

@article{hitawala_comparative_2018,
	title = {Comparative {Study} on {Generative} {Adversarial} {Networks}},
	abstract = {In recent years, there have been tremendous advancements in the field of machine learning. These advancements have been made through both academic as well as industrial research. Lately, a fair amount of research has been dedicated to the usage of generative models in the field of computer vision and image classification. These generative models have been popularized through a new framework called Generative Adversarial Networks. Moreover, many modified versions of this framework have been proposed in the last two years. We study the original model proposed by Goodfellow et al. as well as modifications over the original model and provide a comparative analysis of these models.},
	urldate = {2019-11-25},
	journal = {arXiv:1801.04271 [cs]},
	author = {Hitawala, Saifuddin},
	month = jan,
	year = {2018},
	note = {arXiv: 1801.04271},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/595KNG7T/Hitawala - 2018 - Comparative Study on Generative Adversarial Networ.pdf:application/pdf;arXiv.org Snapshot:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/PXZRKF87/1801.html:text/html}
}

@article{zenati_efficient_2019,
	title = {Efficient {GAN}-{Based} {Anomaly} {Detection}},
	abstract = {Generative adversarial networks (GANs) are able to model the complex highdimensional distributions of real-world data, which suggests they could be effective for anomaly detection. However, few works have explored the use of GANs for the anomaly detection task. We leverage recently developed GAN models for anomaly detection, and achieve state-of-the-art performance on image and network intrusion datasets, while being several hundred-fold faster at test time than the only published GAN-based method.},
	urldate = {2019-11-25},
	journal = {arXiv:1802.06222 [cs, stat]},
	author = {Zenati, Houssam and Foo, Chuan Sheng and Lecouat, Bruno and Manek, Gaurav and Chandrasekhar, Vijay Ramaseshan},
	month = may,
	year = {2019},
	note = {arXiv: 1802.06222},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/43EZ7ATZ/Zenati et al. - 2019 - Efficient GAN-Based Anomaly Detection.pdf:application/pdf;arXiv.org Snapshot:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/JUVMPKDI/1802.html:text/html}
}

@article{ho_population_2019,
	title = {Population {Based} {Augmentation}: {Efficient} {Learning} of {Augmentation} {Policy} {Schedules}},
	shorttitle = {Population {Based} {Augmentation}},
	abstract = {A key challenge in leveraging data augmentation for neural network training is choosing an effective augmentation policy from a large search space of candidate operations. Properly chosen augmentation policies can lead to significant generalization improvements; however, state-of-the-art approaches such as AutoAugment are computationally infeasible to run for the ordinary user. In this paper, we introduce a new data augmentation algorithm, Population Based Augmentation (PBA), which generates nonstationary augmentation policy schedules instead of a fixed augmentation policy. We show that PBA can match the performance of AutoAugment on CIFAR-10, CIFAR-100, and SVHN, with three orders of magnitude less overall compute. On CIFAR-10 we achieve a mean test error of 1.46\%, which is a slight improvement upon the current state-of-the-art. The code for PBA is open source and is available at https://github.com/arcelien/pba.},
	urldate = {2019-11-27},
	journal = {arXiv:1905.05393 [cs, stat]},
	author = {Ho, Daniel and Liang, Eric and Stoica, Ion and Abbeel, Pieter and Chen, Xi},
	month = may,
	year = {2019},
	note = {arXiv: 1905.05393},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/ZIPAICZV/Ho et al. - 2019 - Population Based Augmentation Efficient Learning .pdf:application/pdf;arXiv.org Snapshot:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/U8I54CYM/1905.html:text/html}
}

@article{perez_effectiveness_2017,
	title = {The {Effectiveness} of {Data} {Augmentation} in {Image} {Classification} using {Deep} {Learning}},
	abstract = {In this paper, we explore and compare multiple solutions to the problem of data augmentation in image classification. Previous work has demonstrated the effectiveness of data augmentation through simple techniques, such as cropping, rotating, and flipping input images. We artificially constrain our access to data to a small subset of the ImageNet dataset, and compare each data augmentation technique in turn. One of the more successful data augmentations strategies is the traditional transformations mentioned above. We also experiment with GANs to generate images of different styles. Finally, we propose a method to allow a neural net to learn augmentations that best improve the classifier, which we call neural augmentation. We discuss the successes and shortcomings of this method on various datasets.},
	urldate = {2019-11-27},
	journal = {arXiv:1712.04621 [cs]},
	author = {Perez, Luis and Wang, Jason},
	month = dec,
	year = {2017},
	note = {arXiv: 1712.04621},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/VPUEBEUT/Perez and Wang - 2017 - The Effectiveness of Data Augmentation in Image Cl.pdf:application/pdf;arXiv.org Snapshot:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/MGFAJ578/1712.html:text/html}
}

@article{choromanska_loss_2015,
	title = {The {Loss} {Surfaces} of {Multilayer} {Networks}},
	abstract = {We study the connection between the highly non-convex loss function of a simple model of the fully-connected feed-forward neural network and the Hamiltonian of the spherical spin-glass model under the assumptions of: i) variable independence, ii) redundancy in network parametrization, and iii) uniformity. These assumptions enable us to explain the complexity of the fully decoupled neural network through the prism of the results from random matrix theory. We show that for large-size decoupled networks the lowest critical values of the random loss function form a layered structure and they are located in a well-defined band lower-bounded by the global minimum. The number of local minima outside that band diminishes exponentially with the size of the network. We empirically verify that the mathematical model exhibits similar behavior as the computer simulations, despite the presence of high dependencies in real networks. We conjecture that both simulated annealing and SGD converge to the band of low critical points, and that all critical points found there are local minima of high quality measured by the test error. This emphasizes a major difference between large- and small-size networks where for the latter poor quality local minima have non-zero probability of being recovered. Finally, we prove that recovering the global minimum becomes harder as the network size increases and that it is in practice irrelevant as global minimum often leads to overfitting.},
	urldate = {2019-12-03},
	journal = {arXiv:1412.0233 [cs]},
	author = {Choromanska, Anna and Henaff, Mikael and Mathieu, Michael and Arous, Gérard Ben and LeCun, Yann},
	month = jan,
	year = {2015},
	note = {arXiv: 1412.0233
version: 3},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/UV85E47F/Choromanska et al. - 2015 - The Loss Surfaces of Multilayer Networks.pdf:application/pdf;arXiv.org Snapshot:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/Q7GBURCM/1412.html:text/html}
}

@incollection{neyshabur_path-sgd:_2015,
	title = {Path-{SGD}: {Path}-{Normalized} {Optimization} in {Deep} {Neural} {Networks}},
	shorttitle = {Path-{SGD}},
	urldate = {2019-12-03},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 28},
	publisher = {Curran Associates, Inc.},
	author = {Neyshabur, Behnam and Salakhutdinov, Russ R and Srebro, Nati},
	editor = {Cortes, C. and Lawrence, N. D. and Lee, D. D. and Sugiyama, M. and Garnett, R.},
	year = {2015},
	pages = {2422--2430},
	file = {NIPS Full Text PDF:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/KVYK5RFP/Neyshabur et al. - 2015 - Path-SGD Path-Normalized Optimization in Deep Neu.pdf:application/pdf;NIPS Snapshot:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/7EUAMT2S/5797-path-sgd-path-normalized-optimization-in-deep-neural-networks.html:text/html}
}

@article{oliehoek_gangs:_2017,
	title = {{GANGs}: {Generative} {Adversarial} {Network} {Games}},
	shorttitle = {{GANGs}},
	abstract = {Generative Adversarial Networks (GAN) have become one of the most successful frameworks for unsupervised generative modeling. As GANs are difficult to train much research has focused on this. However, very little of this research has directly exploited game-theoretic techniques. We introduce Generative Adversarial Network Games (GANGs), which explicitly model a finite zero-sum game between a generator (\$G\$) and classifier (\$C\$) that use mixed strategies. The size of these games precludes exact solution methods, therefore we define resource-bounded best responses (RBBRs), and a resource-bounded Nash Equilibrium (RB-NE) as a pair of mixed strategies such that neither \$G\$ or \$C\$ can find a better RBBR. The RB-NE solution concept is richer than the notion of `local Nash equilibria' in that it captures not only failures of escaping local optima of gradient descent, but applies to any approximate best response computations, including methods with random restarts. To validate our approach, we solve GANGs with the Parallel Nash Memory algorithm, which provably monotonically converges to an RB-NE. We compare our results to standard GAN setups, and demonstrate that our method deals well with typical GAN problems such as mode collapse, partial mode coverage and forgetting.},
	urldate = {2019-12-05},
	journal = {arXiv:1712.00679 [cs, stat]},
	author = {Oliehoek, Frans A. and Savani, Rahul and Gallego-Posada, Jose and van der Pol, Elise and de Jong, Edwin D. and Gross, Roderich},
	month = dec,
	year = {2017},
	note = {arXiv: 1712.00679},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computer Science and Game Theory},
	file = {arXiv Fulltext PDF:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/VTM56259/Oliehoek et al. - 2017 - GANGs Generative Adversarial Network Games.pdf:application/pdf;arXiv.org Snapshot:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/YSYQ9GIB/1712.html:text/html}
}

@article{thirey_distribution_2015,
	title = {Distribution of {Euclidean} {Distances} {Between} {Randomly} {Distributed} {Gaussian} {Points} in n-{Space}},
	abstract = {The curse of dimensionality is a common phenomenon which affects analysis of datasets characterized by large numbers of variables associated with each point. Problematic scenarios of this type frequently arise in classification algorithms which are heavily dependent upon distances between points, such as nearest-neighbor and \$k\$-means clustering. Given that contributing variables follow Gaussian distributions, this research derives the probability distribution that describes the distances between randomly generated points in n-space. The theoretical results are extended to examine additional properties of the distribution as the dimension becomes arbitrarily large. With this distribution of distances between randomly generated points in arbitrarily large dimensions, one can then determine the significance of distance measurements between any collection of individual points.},
	urldate = {2019-12-09},
	journal = {arXiv:1508.02238 [math]},
	author = {Thirey, Benjamin and Hickman, Randal},
	month = aug,
	year = {2015},
	note = {arXiv: 1508.02238},
	keywords = {60D05 (Primary) 52A38, 53C65 (Secondary), Mathematics - Probability},
	file = {arXiv Fulltext PDF:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/BYQDPIH5/Thirey and Hickman - 2015 - Distribution of Euclidean Distances Between Random.pdf:application/pdf;arXiv.org Snapshot:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/SP4YZERQ/1508.html:text/html}
}

@article{weerasinghe_support_2019,
	title = {Support vector machines resilient against training data integrity attacks},
	volume = {96},
	issn = {00313203},
	abstract = {Support Vector Machines (SVMs) are vulnerable to integrity attacks, where malicious attackers distort the training data in order to compromise the decision boundary of the learned model. With increasing real-world applications of SVMs, malicious data that is classiﬁed as innocuous may have harmful consequences. This paper presents a novel framework that utilizes adversarial learning, nonlinear data projections, and game theory to improve the resilience of SVMs against such training-data-integrity attacks. The proposed approach introduces a layer of uncertainty through the use of random projections on top of the learners, making it challenging for the adversary to guess the speciﬁc conﬁgurations of the learners. To ﬁnd appropriate projection directions, we introduce novel indices that ensure the contraction of the data and maximize the detection accuracy. Experiments with benchmark data sets show increases in detection rates up to 13.5\% for OCSVMs and up to 14.1\% for binary SVMs under diﬀerent attack algorithms when compared with the respective base algorithms.},
	language = {en},
	urldate = {2019-12-11},
	journal = {Pattern Recognition},
	author = {Weerasinghe, Sandamal and Erfani, Sarah M. and Alpcan, Tansu and Leckie, Christopher},
	month = dec,
	year = {2019},
	pages = {106985},
	file = {Weerasinghe et al. - 2019 - Support vector machines resilient against training.pdf:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/7X69WJI2/Weerasinghe et al. - 2019 - Support vector machines resilient against training.pdf:application/pdf}
}

@article{kobyzev_normalizing_2019,
	title = {Normalizing {Flows}: {An} {Introduction} and {Review} of {Current} {Methods}},
	shorttitle = {Normalizing {Flows}},
	abstract = {Normalizing Flows are generative models which produce tractable distributions where both sampling and density evaluation can be efficient and exact. The goal of this survey article is to give a coherent and comprehensive review of the literature around the construction and use of Normalizing Flows for distribution learning. We aim to provide context and explanation of the models, review current state-of-the-art literature, and identify open questions and promising future directions.},
	language = {en},
	urldate = {2019-12-13},
	journal = {arXiv:1908.09257 [cs, stat]},
	author = {Kobyzev, Ivan and Prince, Simon and Brubaker, Marcus A.},
	month = dec,
	year = {2019},
	note = {arXiv: 1908.09257},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Kobyzev et al. - 2019 - Normalizing Flows An Introduction and Review of C.pdf:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/TTE2M4V4/Kobyzev et al. - 2019 - Normalizing Flows An Introduction and Review of C.pdf:application/pdf}
}

@article{mitola_integrated_2000,
	title = {An {Integrated} {Agent} {Architecture} for {Software} {Defined} {Radio}},
	abstract = {Software radio has emerged as a focus of both academic research and commercial development for future wireless systems. This paper briefly reviews the foundation concepts of the software radio. It then characterizes the tradeoffs among core software-radio technologies. Objectoriented analysis leads to the definition of the radio reference platform and the related layered object-oriented architecture supporting simultaneous hardware and software evolution. Research issues include layering, tunneling, virtual machines and intelligent agents.},
	journal = {Ph.D. Dissertation, KTH},
	author = {Mitola, Joseph},
	month = jul,
	year = {2000},
	file = {Mitola - 2000 - An Integrated Agent Architecture for Software Defi.pdf:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/RIX9ZI64/Mitola - 2000 - An Integrated Agent Architecture for Software Defi.pdf:application/pdf}
}


@article{mitola_software_1999,
	title = {Software radio architecture: a mathematical perspective},
	volume = {17},
	issn = {1558-0008},
	shorttitle = {Software radio architecture},
	abstract = {As the software radio makes its transition from research to practice, it becomes increasingly important to establish provable properties of the software radio architecture on which product developers and service providers can base technology insertion decisions. Establishing provable properties requires a mathematical perspective on the software radio architecture. This paper contributes to that perspective by critically reviewing the fundamental concept of the software radio, using mathematical models to characterize this rapidly emerging technology in the context of similar technologies like programmable digital radios. The software radio delivers dynamically defined services through programmable processing capacity that has the mathematical structure of the Turing machine. The bounded recursive functions, a subset of the total recursive functions, are shown to be the largest class of Turing-computable functions for which software radios exhibit provable stability in plug-and-play scenarios. Understanding the topological properties of the software radio architecture promotes plug-and-play applications and cost-effective reuse. Analysis of these topological properties yields a layered distributed virtual machine reference model and a set of architecture design principles for the software radio. These criteria may be useful in defining interfaces among hardware, middleware, and higher level software components that are needed for cost-effective software reuse.},
	number = {4},
	journal = {IEEE Journal on Selected Areas in Communications},
	author = {Mitola, J.},
	month = apr,
	year = {1999},
	keywords = {telecommunication computing, Application software, bounded recursive functions, Computer architecture, cost-effective software reuse, Digital communication, digital radio, distributed processing, hardware, Hardware, interfaces, layered distributed virtual machine reference model, Mathematical model, mathematical models, middleware, plug-and-play applications, product developers, programmable digital radios, programmable processing, provable properties, radio equipment, recursive functions, Service oriented architecture, service providers, software architecture, software components, Software radio, software radio architecture, software reusability, stability, Stability, topological properties, total recursive functions, Turing machine, Turing machines, Turing-computable functions, Virtual machining},
	pages = {514--538},
	file = {IEEE Xplore Abstract Record:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/NEA3I6DN/761033.html:text/html;IEEE Xplore Full Text PDF:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/UEWLWRTG/Mitola - 1999 - Software radio architecture a mathematical perspe.pdf:application/pdf}
}

@article{mitola_cognitive_1999,
	title = {Cognitive radio: making software radios more personal},
	volume = {6},
	issn = {1558-0652},
	shorttitle = {Cognitive radio},
	abstract = {Software radios are emerging as platforms for multiband multimode personal communications systems. Radio etiquette is the set of RF bands, air interfaces, protocols, and spatial and temporal patterns that moderate the use of the radio spectrum. Cognitive radio extends the software radio with radio-domain model-based reasoning about such etiquettes. Cognitive radio enhances the flexibility of personal services through a radio knowledge representation language. This language represents knowledge of radio etiquette, devices, software modules, propagation, networks, user needs, and application scenarios in a way that supports automated reasoning about the needs of the user. This empowers software radios to conduct expressive negotiations among peers about the use of radio spectrum across fluents of space, time, and user context. With RKRL, cognitive radio agents may actively manipulate the protocol stack to adapt known etiquettes to better satisfy the user's needs. This transforms radio nodes from blind executors of predefined protocols to radio-domain-aware intelligent agents that search out ways to deliver the services the user wants even if that user does not know how to obtain them. Software radio provides an ideal platform for the realization of cognitive radio.},
	number = {4},
	journal = {IEEE Personal Communications},
	author = {Mitola, J. and Maguire, G.Q.},
	month = aug,
	year = {1999},
	keywords = {telecommunication computing, Hardware, software architecture, Software radio, air interfaces, Application specific integrated circuits, automated reasoning, Baseband, cellular radio, Cognitive radio, cognitive radio agents, cognitive systems, equalisers, Equalizers, GSM, knowledge representation languages, model-based reasoning, Modems, multiband multimode personal communications systems, personal communication networks, personal services, protocol stack, protocols, radio equalizer, radio etiquette, Radio frequency, radio knowledge representation language, radio networks, radio nodes, radio propagation, radio spectrum, radio-domain model-based reasoning, radio-domain-aware intelligent agents, RF bands, software agents, Software algorithms, software modules, software radios, spatial patterns, temporal patterns},
	pages = {13--18},
	file = {IEEE Xplore Abstract Record:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/YJZIQN8G/788210.html:text/html;IEEE Xplore Full Text PDF:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/56NCT25B/Mitola and Maguire - 1999 - Cognitive radio making software radios more perso.pdf:application/pdf}
}

@article{clancy_applications_2007,
	title = {Applications of {Machine} {Learning} to {Cognitive} {Radio} {Networks}},
	volume = {14},
	issn = {1536-1284},
	abstract = {Cognitive radio offers the promise of intelligent radios that can learn from and adapt to their environment. To date, most cognitive radio research has focused on policybased radios that are hard-coded with a list rules of how the radio should behave in certain scenarios. Some work has been done on radios with learning engines tailored for very speciﬁc applications.},
	language = {en},
	number = {4},
	urldate = {2020-01-14},
	journal = {IEEE Wireless Communications},
	author = {Clancy, Charles and Hecker, Joe and Stuntebeck, Erich and O'Shea, Tim},
	month = aug,
	year = {2007},
	pages = {47--52},
	file = {Clancy et al. - 2007 - Applications of Machine Learning to Cognitive Radi.pdf:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/WRRQ3CY9/Clancy et al. - 2007 - Applications of Machine Learning to Cognitive Radi.pdf:application/pdf}
}

@article{bkassiny_survey_2013,
	title = {A {Survey} on {Machine}-{Learning} {Techniques} in {Cognitive} {Radios}},
	volume = {15},
	issn = {1553-877X},
	abstract = {In this survey paper, we characterize the learning problem in cognitive radios and state the importance of artiﬁcial intelligence in achieving real cognitive systems. We review various learning approaches that have been proposed for cognitive radios classifying them under supervised and unsupervised learning paradigms. Unsupervised learning is presented as an autonomous learning procedure that is suitable for unknown RF environments, whereas supervised learning methods can be used to exploit prior information available to cognitive radios during the learning process. We describe some challenging learning problems that arise in cognitive radio networks, in particular in non-Markovian environments, and present their possible solution methods. Finally, we present some generic cognitive radio problems and show suitable machine learning approaches for learning in these contexts.},
	language = {en},
	number = {3},
	urldate = {2020-01-14},
	journal = {IEEE Communications Surveys \& Tutorials},
	author = {Bkassiny, Mario and Li, Yang and Jayaweera, Sudharman K.},
	year = {2013},
	pages = {1136--1159},
	file = {Bkassiny et al. - 2013 - A Survey on Machine-Learning Techniques in Cogniti.pdf:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/I7NG7SWT/Bkassiny et al. - 2013 - A Survey on Machine-Learning Techniques in Cogniti.pdf:application/pdf}
}

@inproceedings{shah_cognitive_2013,
	title = {Cognitive radio networks for {Internet} of {Things}: {Applications}, challenges and future},
	shorttitle = {Cognitive radio networks for {Internet} of {Things}},
	abstract = {Cognitive Radio (CR) has emerged as an intelligent technology to address the spectrum scarcity issues. CR aims to use the unoccupied spectrum band when it is not used by the licensed user. An extensive research has been carried out since the inception of this technology in 1999 where different challenges like spectrum sensing, cooperation amongst CR users and applicability of CR networks have been widely explored. In this paper, we provide new applications of CR technology for Internet of Things (IoT) and propose appropriate solutions to the real challenges in CR technology that will make IoT more affordable and applicable.},
	booktitle = {2013 19th {International} {Conference} on {Automation} and {Computing}},
	author = {Shah, Munam Ali and Zhang, Sijing and Maple, Carsten},
	month = sep,
	year = {2013},
	keywords = {cognitive radio, radio spectrum management, Sensors, Wireless sensor networks, Cognitive radio, cognitive radio networks, cooperative communication, CR networks, CR technology, CR users, intelligent technology, Internet, Internet of Things, IoT, licensed user, pervasive computing, Safety, Security, signal detection, spectrum scarcity issue, spectrum sensing, unoccupied spectrum band},
	pages = {1--6},
	file = {IEEE Xplore Abstract Record:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/GAR6KUDY/6662016.html:text/html;IEEE Xplore Full Text PDF:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/I4IWG2FB/Shah et al. - 2013 - Cognitive radio networks for Internet of Things A.pdf:application/pdf}
}

@article{tandiya_deep_2018,
	title = {Deep {Predictive} {Coding} {Neural} {Network} for {RF} {Anomaly} {Detection} in {Wireless} {Networks}},
	abstract = {Intrusion detection has become one of the most critical tasks in a wireless network to prevent service outages that can take long to fix. The sheer variety of anomalous events necessitates adopting cognitive anomaly detection methods instead of the traditional signature-based detection techniques. This paper proposes an anomaly detection methodology for wireless systems that is based on monitoring and analyzing radio frequency (RF) spectrum activities. Our detection technique leverages an existing solution for the video prediction problem, and uses it on image sequences generated from monitoring the wireless spectrum. The deep predictive coding network is trained with images corresponding to the normal behavior of the system, and whenever there is an anomaly, its detection is triggered by the deviation between the actual and predicted behavior. For our analysis, we use the images generated from the time-frequency spectrograms and spectral correlation functions of the received RF signal. We test our technique on a dataset which contains anomalies such as jamming, chirping of transmitters, spectrum hijacking, and node failure, and evaluate its performance using standard classifier metrics: detection ratio, and false alarm rate. Simulation results demonstrate that the proposed methodology effectively detects many unforeseen anomalous events in real time. We discuss the applications, which encompass industrial IoT, autonomous vehicle control and mission-critical communications services.},
	urldate = {2020-01-14},
	journal = {arXiv:1803.06054 [eess]},
	author = {Tandiya, Nistha and Jauhar, Ahmad and Marojevic, Vuk and Reed, Jeffrey H.},
	month = mar,
	year = {2018},
	note = {arXiv: 1803.06054},
	keywords = {Electrical Engineering and Systems Science - Signal Processing},
	file = {arXiv Fulltext PDF:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/MA8NFB3Z/Tandiya et al. - 2018 - Deep Predictive Coding Neural Network for RF Anoma.pdf:application/pdf;arXiv.org Snapshot:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/8S5C5H43/1803.html:text/html}
}

@misc{afgani_information_2010,
	type = {Research article},
	title = {The {Information} {Theoretic} {Approach} to {Signal} {Anomaly} {Detection} for {Cognitive} {Radio}},
	abstract = {Efficient utilisation and sharing of limited spectrum resources in an autonomous fashion is one of the primary goals of cognitive radio. However, decentralised spectrum sharing can lead to interference scenarios that must be detected and characterised to help achieve the other goal of cognitive radio—reliable service for the end user. Interference events can be treated as unusual and therefore anomaly detection algorithms can be applied for their detection. Two complementary algorithms based on information theoretic measures of statistical distribution divergence and information content are proposed. The first method is applicable to signals with periodic structures and is based on the analysis of Kullback-Leibler divergence. The second utilises information content analysis to detect unusual events. Results from software and hardware implementations show that the proposed algorithms are effective, simple, and capable of processing high-speed signals in real time. Additionally, neither of the algorithms require demodulation of the signal.},
	language = {en},
	urldate = {2020-01-14},
	journal = {International Journal of Digital Multimedia Broadcasting},
	author = {Afgani, Mostafa and Sinanović, Sinan and Haas, Harald},
	year = {2010},
	file = {Full Text PDF:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/9I65JXVW/Afgani et al. - 2010 - The Information Theoretic Approach to Signal Anoma.pdf:application/pdf;Snapshot:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/WWSNRUUK/740594.html:application/xhtml+xml}
}

@article{jondral_software-defined_2005,
	title = {Software-{Defined} {Radio}—{Basics} and {Evolution} to {Cognitive} {Radio}},
	volume = {2005},
	issn = {1687-1499},
	language = {en},
	number = {3},
	urldate = {2020-01-19},
	journal = {EURASIP Journal on Wireless Communications and Networking},
	author = {Jondral, Friedrich K},
	month = dec,
	year = {2005},
	pages = {652784},
	file = {Jondral - 2005 - Software-Defined Radio—Basics and Evolution to Cog.pdf:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/AMWPFVEL/Jondral - 2005 - Software-Defined Radio—Basics and Evolution to Cog.pdf:application/pdf}
}

@article{chen_defense_2008,
	title = {Defense against {Primary} {User} {Emulation} {Attacks} in {Cognitive} {Radio} {Networks}},
	volume = {26},
	issn = {1558-0008},
	abstract = {Cognitive Radio (CR) is a promising technology that can alleviate the spectrum shortage problem by enabling unlicensed users equipped with CRs to coexist with incumbent users in licensed spectrum bands while causing no interference to incumbent communications. Spectrum sensing is one of the essential mechanisms of CRs and its operational aspects are being investigated actively. However, the security aspects of spectrum sensing have garnered little attention. In this paper, we identify a threat to spectrum sensing, which we call the primary user emulation (PUE) attack. In this attack, an adversary's CR transmits signals whose characteristics emulate those of incumbent signals. The highly flexible, software-based air interface of CRs makes such an attack possible. Our investigation shows that a PUE attack can severely interfere with the spectrum sensing process and significantly reduce the channel resources available to legitimate unlicensed users. To counter this threat, we propose a transmitter verification scheme, called LocDef (localization-based defense), which verifies whether a given signal is that of an incumbent transmitter by estimating its location and observing its signal characteristics. To estimate the location of the signal transmitter, LocDef employs a non-interactive localization scheme. Our security analysis and simulation results suggest that LocDef is effective in identifying PUE attacks under certain conditions.},
	number = {1},
	journal = {IEEE Journal on Selected Areas in Communications},
	author = {Chen, Ruiliang and Park, Jung-Min and H. Reed, Jeffrey},
	month = jan,
	year = {2008},
	keywords = {cognitive radio, Radio transmitters, telecommunication security, Wireless sensor networks, Cognitive radio, spectrum sensing, channel resource, Chromium, cognitive radio network, communication system security, Communication system security, Emulation, FCC, incumbent signal, incumbent transmitter, Interference, localization-based defense, location estimation, primary user emulation attack, radio receivers, Radio spectrum management, radio transmitters, security analysis, signal transmitter, software-based air interface, spectral analysis, spectrum shortage, telecommunication channels, transmitter verification, White spaces, wireless sensor network, wireless sensor networks},
	pages = {25--37},
	file = {IEEE Xplore Abstract Record:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/B9MG7XL9/4413138.html:text/html;IEEE Xplore Full Text PDF:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/UYEEZKX3/Chen et al. - 2008 - Defense against Primary User Emulation Attacks in .pdf:application/pdf}
}

@article{dong_explore_2018,
	title = {Explore {Recurrent} {Neural} {Network} for {PUE} {Attack} {Detection} in {Practical} {CRN} {Models}},
	abstract = {The proliferation of the Internet of Things (IoTs) and pervasive use of many different types of mobile computing devices make wireless communication spectrum a precious resource. In order to accommodate the still fast increasing number of devices requesting wireless connection, more efficient, fine-grained spectrum allocation and sharing schemes are badly in need. Cognitive radio networks (CRNs) have been widely recognized as one promising solution, in which the secondary users (SUs) are allowed to share channels with licensed primary users (PUs) as long as bringing no interference to the normal operations of the PUs. However, malicious attackers or selfish SUs may mimic the behavior of PUs to occupy the channels illegally. It is nontrivial to accurately, timely detect such kind of primary user emulation (PUE) attacks. In this paper, an efficient PUE attacked detection method is introduced leveraging the recurrent neural network (RNN). After a fundamental algorithm using basic RNN, an advanced version taking advantage of long-short-term-memory (LSTM) is proposed, which is more efficient on processing time series with long term memory. The experimental study has provided deeper insights about the different performances the RNNs achieved and validated the effectiveness of the proposed detectors.},
	urldate = {2020-01-19},
	journal = {arXiv:1805.00428 [cs]},
	author = {Dong, Qi and Chen, Yu and Li, Xiaohua and Zeng, Kai},
	month = may,
	year = {2018},
	note = {arXiv: 1805.00428},
	keywords = {Computer Science - Networking and Internet Architecture},
	file = {arXiv Fulltext PDF:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/KW4Z75ZK/Dong et al. - 2018 - Explore Recurrent Neural Network for PUE Attack De.pdf:application/pdf;arXiv.org Snapshot:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/8284V9YZ/1805.html:text/html}
}

@article{srinivasan_semi-supervised_2019,
	title = {Semi-supervised machine learning for primary user emulation attack detection and prevention through core-based analytics for cognitive radio networks},
	volume = {15},
	issn = {1550-1477},
	abstract = {Cognitive radio networks are software controlled radios with the ability to allocate and reallocate spectrum depending upon the demand. Although they promise an extremely optimal use of the spectrum, they also bring in the challenges of misuse and attacks. Selfish attacks among other attacks are the most challenging, in which a secondary user or an unauthorized user with unlicensed spectrum pretends to be a primary user by altering the signal characteristics. Proposed methods leverage advancement to efficiently detect and prevent primary user emulation future attack in cognitive radio using machine language techniques. In this paper novel method is proposed to leverage unique methodology which can efficiently handle during various dynamic changes includes varying bandwidth, signature changes etc… performing learning and classification at edge nodes followed by core nodes using deep learning convolution network. The proposed method is compared with that of two other state-of-art machine learning-based attack detection protocols and has found to significantly reduce the false alarm to secondary network, at the same time improve the overall detection accuracy at the primary network.},
	language = {en},
	number = {9},
	urldate = {2020-01-19},
	journal = {International Journal of Distributed Sensor Networks},
	author = {Srinivasan, Sundar and Shivakumar, KB and Mohammad, Muazzam},
	month = sep,
	year = {2019},
	keywords = {Cognitive radio network, deep learning convolution network, dynamic spectrum sensing, feed forward neural network, primary user emulation, reinforced machine learning, semi-supervised machine learning, supervised machine learning, unsupervised machine learning},
	pages = {1550147719860365},
	file = {SAGE PDF Full Text:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/8RSIZH8P/Srinivasan et al. - 2019 - Semi-supervised machine learning for primary user .pdf:application/pdf}
}

@inproceedings{pu_detecting_2011,
	title = {Detecting {Primary} {User} {Emulation} {Attack} in {Cognitive} {Radio} {Networks}},
	abstract = {In this paper, we propose an approach for detecting primary user emulation attacks in cognitive radio networks. Cognitive radios (CRs) have been proposed as a promising solution for improving spectrum utilization via opportunistic spectrum sharing. In a CR network environment, primary (licensed) users have priority over secondary (unlicensed) users when accessing the wireless channel. Thus, if a malicious secondary user exploits this spectrum access etiquette by mimicking the spectral characteristics of a primary user, it can gain priority access to a wireless channel over other secondary users. Our proposed approach is initiated by energy detection to locate the existing users on the frequency band. The approach employs a cyclostationary calculation to represent the features of the user signals, which are then fed into an artificial neural network for classification. As opposed to current techniques for detecting primary user emulation attacks in CR networks, our proposed approach does not require any special hardware or time synchronization algorithms in the wireless network. Consequently, existing systems can readily employ the proposed approach without significant structural and functional modifications. The proposed approach is validated via computer simulations as well as by experimental hardware implementations using USRP2 platform. The hardware experiment shows that our approach can achieve a percentage of correct detection around 98\% in actual wireless environments.},
	booktitle = {2011 {IEEE} {Global} {Telecommunications} {Conference} - {GLOBECOM} 2011},
	author = {Pu, Di and Shi, Yuan and Ilyashenko, Andrei V. and Wyglinski, Alexander M.},
	month = dec,
	year = {2011},
	note = {ISSN: 1930-529X},
	keywords = {cognitive radio, Radio transmitters, Receivers, telecommunication computing, telecommunication security, wireless channels, Hardware, Cognitive radio, radio networks, cognitive radio networks, signal detection, artificial intelligence, artificial neural network, computer simulations, CR network environment, energy detection, Frequency modulation, neural nets, opportunistic spectrum sharing, primary user emulation attack detection, Reliability, signal classification, spectrum utilization, time synchronization algorithms, user signal detection, USRP2 platform, wireless channel, wireless network},
	pages = {1--5},
	file = {IEEE Xplore Abstract Record:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/WK6AXKZD/6134419.html:text/html;IEEE Xplore Full Text PDF:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/QEP8MLK7/Pu et al. - 2011 - Detecting Primary User Emulation Attack in Cogniti.pdf:application/pdf}
}

@article{rehman_detection_2017,
	title = {Detection of {PUE} {Attack} in {CRN} with {Reduced} {Error} in {Location} {Estimation} {Using} {Novel} {Bat} {Algorithm}},
	volume = {6},
	abstract = {Cognitive Radio Network Technology makes the efficient utilization of scarce spectrum resources by allowing the unlicensed users to opportunistically use the licensed spectrum. Cognitive Radio Network due to its flexible and open nature is vulnerable to a number of security attacks. This paper is mainly concerned with one of the physical layer attack called Primary User Emulation Attack and its detection. This paper solves the problem of PUE attack by localization technique based on TDOA measurements with reduced error in location estimation using a Novel Bat Algorithm (NBA). A number of cooperative secondary users are used for detecting the PUEA by comparing its estimated position with the known position of incumbent. The main goal of NBA is to minimize two fitness functions namely non-linear least square and the maximum likelihood in order to optimize the estimation error. After evaluation, simulation results clearly demonstrates that NBA results in reduced estimation error as compared to Taylor Series Estimation and Particle Swarm Optimization.},
	journal = {International Journal of Wireless Networks and Broadband Technologies},
	author = {Rehman, Aasia and Prakash, Deo},
	month = jul,
	year = {2017},
	pages = {1--25}
}

@article{fassi_fihri_decision-making_2019,
	title = {A decision-making approach for detecting the primary user emulation attack in cognitive radio networks},
	abstract = {Cognitive radio network (CRN) is a promising technology, which enables secondary users to use the free spectrum channels without causing detrimental interference with the primary user (PU). Nevertheless, CRN is subject to numerous cyber attacks that have a negative impact on its performance. Among the CRN attacks, the primary user emulation (PUE) attack is known to be one of the malicious attacks threatening CRN security. Several attacks detection techniques, based on attacker localization, have been investigated in the literature. These techniques include the trilateration, received signal strength indication (RSSI), and network coding approach as well. However, most of these techniques do not consider the uncertainty related to CRN, which can be modeled by a cost function defined as a weighted sum of conditional probabilities. In this paper, a localization technique, relied on a trilateration computation and a Bayesian model, is proposed for PUE position detection purpose under uncertainty conditions assumption. Particularly, the estimation of PUE position is performed through trilateration method based on RSSI at the anchor nodes for the signal coming from either PU or PUE, whereas, the Bayesian decision model, based on a cost function, is involved to check the PU legitimacy. The simulation results show that the decision‐making approach "Security, productivity, Balancing" influences directly the zone of the PUE attack detection. Bayesian model for PU/PUE location estimation based on RSSI/trilateration for distinguishing the attacker from the legitimate PU in uncertain area The proposed uncertainty method relies on both the decision maker cost matrix parameters (Security, Productivity and balanced) and the Bayesian model for stopping any eventual PUE attack lead the decision maker to the adequate decision, deny any potential malicious activities, and avoid any dysfunction of the CRN.},
	journal = {International Journal of Communication Systems},
	author = {Fassi Fihri, Wassim and el ghazi, Hassan and Abou El Majd, Badr and El Bouanani, Faissal},
	month = jul,
	year = {2019},
	pages = {e4026}
}

@inproceedings{dong_anomaly_2017,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Anomaly {Detection} in {Cognitive} {Radio} {Networks} {Exploiting} {Singular} {Spectrum} {Analysis}},
	isbn = {978-3-319-65127-9},
	abstract = {Cognitive radio networks (CRNs) is a promising technology that allows secondary users (SUs) extensively explore spectrum resource usage efficiency, while not introducing interference to licensed users. Due to the unregulated wireless network environment, CRNs are susceptible to various malicious entities. Thus, it is critical to detect anomalies in the first place. However, from the perspective of intrinsic features of CRNs, there is hardly in existence of an universal applicable anomaly detection scheme. Singular Spectrum Analysis (SSA) has been theoretically proven an optimal approach for accurate and quick detection of changes in the characteristics of a running (random) process. In addition, SSA is a model-free method and no parametric models have to be assumed for different types of anomalies, which makes it a universal anomaly detection scheme. In this paper, we introduce an adaptive parameter and component selection mechanism based on coherence for basic SSA method, upon which we built up a sliding window based anomaly detector in CRNs. Our experimental results indicate great accuracy of the SSA-based anomaly detector for multiple anomalies.},
	language = {en},
	booktitle = {Computer {Network} {Security}},
	publisher = {Springer International Publishing},
	author = {Dong, Qi and Yang, Zekun and Chen, Yu and Li, Xiaohua and Zeng, Kai},
	editor = {Rak, Jacek and Bay, John and Kotenko, Igor and Popyack, Leonard and Skormin, Victor and Szczypiorski, Krzysztof},
	year = {2017},
	keywords = {Anomaly detection, Cognitive radio networks, Singular spectrum analysis},
	pages = {247--259},
	file = {Springer Full Text PDF:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/KNARDZ8R/Dong et al. - 2017 - Anomaly Detection in Cognitive Radio Networks Expl.pdf:application/pdf}
}

@article{chunsheng_xin_detection_2014,
	title = {Detection of {PUE} {Attacks} in {Cognitive} {Radio} {Networks} {Based} on {Signal} {Activity} {Pattern}},
	volume = {13},
	issn = {1536-1233},
	abstract = {Promising to signiﬁcantly improve spectrum utilization, cognitive radio networks (CRNs) have attracted a great attention in the literature. Nevertheless, a new security threat known as the primary user emulation (PUE) attack raises a great challenge to CRNs. The PUE attack is unique to CRNs and can cause severe denial of service (DoS) to CRNs. In this paper, we propose a novel PUE detection system, termed Signal activity Pattern Acquisition and Reconstruction System. Different from current solutions of PUE detection, the proposed system does not need any a priori knowledge of primary users (PUs), and has no limitation on the type of PUs that are applicable. It acquires the activity pattern of a signal through spectrum sensing, such as the ON and OFF periods of the signal. Then it reconstructs the observed signal activity pattern through a reconstruction model. By examining the reconstruction error, the proposed system can smartly distinguish a signal activity pattern of a PU from a signal activity pattern of an attacker. Numerical results show that the proposed system has excellent performance in detecting PUE attacks.},
	language = {en},
	number = {5},
	urldate = {2020-01-19},
	journal = {IEEE Transactions on Mobile Computing},
	author = {{ChunSheng Xin} and Song, M.},
	month = may,
	year = {2014},
	pages = {1022--1034},
	file = {ChunSheng Xin and Song - 2014 - Detection of PUE Attacks in Cognitive Radio Networ.pdf:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/7NUZRSP3/ChunSheng Xin and Song - 2014 - Detection of PUE Attacks in Cognitive Radio Networ.pdf:application/pdf}
}

@article{khaliq_defence_2019,
	title = {Defence against {PUE} attacks in ad hoc cognitive radio networks: a mean field game approach},
	volume = {70},
	shorttitle = {Defence against {PUE} attacks in ad hoc cognitive radio networks},
	abstract = {Abstract Cognitive Radio (CR) is an emerging and promising communication technology geared towards improving vacant licensed band utilization, intended for unlicensed users. Security of Cognitive Radio Networks (CRN) is a highly challenging domain. At present, plenty of efforts are in place for defining new paradigms, techniques and technologies to secure radio spectrum. In a distributed cognitive radio ad-hoc network, despite dynamically changing topologies, lack of central administration, bandwidth-constraints and shared wireless connections, the nodes are capable of sensing the spectrum and selecting the appropriate channels for communication. These unique characteristics unlock new paths for attackers. Standard security techniques are not an effective shield against attacks on these networks e.g. Primary User Emulation (PUE) attacks. The paper presents a novel PUE attack detection technique based on energy detection and location verification. Next, a game model and a mean field game approach are introduced for the legitimate nodes of CRN to reach strategic defence decisions in the presence of multiple attackers. Simulation of the proposed technique shows a detection accuracy of \$\$\{89{\textbackslash}\%\}\$\$ 89 \% when the probability of false alarm is 0.09. This makes it 1.32 times more accurate than compared work. Furthermore, the proposed framework for defence is state considerate in making decisions.},
	language = {en},
	number = {1},
	urldate = {2020-01-19},
	journal = {Telecommunication Systems: Modelling, Analysis, Design and Management},
	author = {Khaliq, Saim Bin Abdul and Amjad, Muhammad Faisal and Abbas, Haider and Shafqat, Narmeen and Afzal, Hammad},
	year = {2019},
	keywords = {Cognitive Radio Network (CRN), Game theory, Primary user emulation (PUE) attack},
	pages = {123--140},
	file = {Snapshot:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/X5CDL5ZI/v70y2019i1d10.1007_s11235-018-0472-y.html:text/html}
}

@inproceedings{luo_specific_2011,
	title = {Specific primary user sensing for wireless security in {IEEE} 802.22 network},
	abstract = {Cognitive Radio (CR) is considered as an effective technology for alleviating the spectrum shortage problem by enabling secondary users to utilize vacant spectrum allocated to a primary user (PU). IEEE 802.22 which is the first standard based on CR is developed to increase the efficiency of TV bands. However the physical layer security of IEEE 802.22 has been studied rarely. In this paper, we propose a scheme to sense the specific primary user for dealing with Primary User Emulation (PUE) Attack which is a possible threat to wireless security. According to ATSC digital TV which is one of the important PUs, we estimate the accuracies of pilot and symbol rate as RF fingerprint (RFF), and Support Vector Data Description (SVDD) is introduced to sense the fine differences of RFF to distinguish PUE attacker from PU. The simulation results show that the proposed scheme is feasible to verify the PUE attacker at a relative high SNR level.},
	booktitle = {2011 11th {International} {Symposium} on {Communications} {Information} {Technologies} ({ISCIT})},
	author = {Luo, Zhenxing and Lou, Caiyi and Chen, Shichuan and Zheng, Shilian and Li, Shaowei},
	month = oct,
	year = {2011},
	keywords = {cognitive radio, Sensors, telecommunication security, Radio frequency, primary user emulation, Accuracy, ATSC digital TV, Cognitive Radio (CR), digital television, Fingerprint recognition, IEEE 802.22 network, Primary User Emulation (PUE), primary user sensing, RF fingerprint, RF fingerprinting (RFF), Signal to noise ratio, Specific Primary User Sensing (SPUS), spectrum shortage problem, support vector data description, SVDD, Training, TV bands, wireless security},
	pages = {18--22},
	file = {IEEE Xplore Abstract Record:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/2H5NVKJN/6089728.html:text/html;IEEE Xplore Full Text PDF:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/M8H222E2/Luo et al. - 2011 - Specific primary user sensing for wireless securit.pdf:application/pdf}
}

@book{leyton-brown_essentials_2008,
	address = {San Rafael, California},
	series = {Synthesis lectures on artificial intelligence and machine learning},
	title = {Essentials of game theory: a concise, multidisciplinary introduction},
	isbn = {978-1-59829-593-1 978-1-59829-594-8},
	shorttitle = {Essentials of game theory},
	language = {en},
	number = {Lecture 3},
	publisher = {Morgan \& Claypool Publishers},
	author = {Leyton-Brown, Kevin and Shoham, Yoav},
	year = {2008},
	note = {OCLC: 236164649},
	file = {Leyton-Brown and Shoham - 2008 - Essentials of game theory a concise, multidiscipl.pdf:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/VZL4HE94/Leyton-Brown and Shoham - 2008 - Essentials of game theory a concise, multidiscipl.pdf:application/pdf}
}

@article{atzori_internet_2010,
	title = {The {Internet} of {Things}: {A} survey},
	volume = {54},
	issn = {13891286},
	shorttitle = {The {Internet} of {Things}},
	abstract = {This paper addresses the Internet of Things. Main enabling factor of this promising paradigm is the integration of several technologies and communications solutions. Identiﬁcation and tracking technologies, wired and wireless sensor and actuator networks, enhanced communication protocols (shared with the Next Generation Internet), and distributed intelligence for smart objects are just the most relevant. As one can easily imagine, any serious contribution to the advance of the Internet of Things must necessarily be the result of synergetic activities conducted in different ﬁelds of knowledge, such as telecommunications, informatics, electronics and social science. In such a complex scenario, this survey is directed to those who want to approach this complex discipline and contribute to its development. Different visions of this Internet of Things paradigm are reported and enabling technologies reviewed. What emerges is that still major issues shall be faced by the research community. The most relevant among them are addressed in details.},
	language = {en},
	number = {15},
	urldate = {2020-01-28},
	journal = {Computer Networks},
	author = {Atzori, Luigi and Iera, Antonio and Morabito, Giacomo},
	month = oct,
	year = {2010},
	pages = {2787--2805},
	file = {Atzori et al. - 2010 - The Internet of Things A survey.pdf:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/8LMILZGH/Atzori et al. - 2010 - The Internet of Things A survey.pdf:application/pdf}
}

@book{giusto_internet_2010,
	address = {New York},
	title = {The internet of things: 20th {Tyrrhenian} workshop on digital communications},
	isbn = {978-1-4419-1673-0},
	shorttitle = {The internet of things},
	language = {en},
	publisher = {Springer},
	editor = {Giusto, Daniel},
	year = {2010},
	keywords = {Ambient intelligence, Congresses, Digital communications, Ubiquitous computing, Wireless communication systems},
	file = {Giusto - 2010 - The internet of things 20th Tyrrhenian workshop o.pdf:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/AYP8EI88/Giusto - 2010 - The internet of things 20th Tyrrhenian workshop o.pdf:application/pdf}
}

@article{cormio_survey_2017,
	title = {A survey on {MAC} protocols for cognitive radio networks},
	volume = {7},
	issn = {15708705},
	abstract = {In cognitive radio (CR) networks, identifying the available spectrum resource through spectrum sensing, deciding on the optimal sensing and transmission times, and coordinating with the other users for spectrum access are the important functions of the medium access control (MAC) protocols. In this survey, the characteristic features, advantages, and the limiting factors of the existing CR MAC protocols are thoroughly investigated for both infrastructure-based and ad hoc networks. First, an overview of the spectrum sensing is given, as it ensures that the channel access does not result in interference to the licensed users of the spectrum. Next, a detailed classiﬁcation of the MAC protocols is presented while considering the infrastructure support, integration of spectrum sensing functionalities, the need for time synchronization, and the number of radio transceivers. The main challenges and future research directions are presented, while highlighting the close coupling of the MAC protocol design with the other layers of the protocol stack.},
	language = {en},
	number = {7},
	urldate = {2020-01-28},
	journal = {Ad Hoc Networks},
	author = {Cormio, Claudia and Chowdhury, Kaushik R.},
	year = {2017},
	pages = {1315--1329},
	file = {Cormio and Chowdhury - 2009 - A survey on MAC protocols for cognitive radio netw.pdf:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/BNFKDZQY/Cormio and Chowdhury - 2009 - A survey on MAC protocols for cognitive radio netw.pdf:application/pdf}
}

@inproceedings{bian_security_2008,
	address = {Maui, HI, USA},
	title = {Security vulnerabilities in {IEEE} 802.22},
	isbn = {978-963-9799-36-3},
	abstract = {Cognitive Radio (CR) is seen as one of the enabling technologies for realizing a new spectrum access paradigm, viz. Opportunistic Spectrum Sharing (OSS). IEEE 802.22 is the world’s ﬁrst wireless standard based on CR technology. It deﬁnes the air interface for a wireless regional area network (WRAN) that uses fallow segments of the licensed (incumbent) TV broadcast bands. CR technology enables unlicensed (secondary) users in WRANs to utilize licensed spectrum bands on a non-interference basis to incumbent users. The coexistence between incumbent users and secondary users is referred to as incumbent coexistence. On the other hand, the coexistence between secondary users in diﬀerent WRAN cells is referred to as self-coexistence. The 802.22 draft standard prescribes several mechanisms for addressing incumbent- and self-coexistence issues. In this paper, we describe how adversaries can exploit or undermine such mechanisms to degrade the performance of 802.22 WRANs and increase the likelihood of those networks interfering with incumbent networks. The standard includes a security sublayer to provide subscribers with privacy, authentication, and conﬁdentiality. Our investigation, however, revealed that the security sublayer falls short of addressing all of the key security threats. We also discuss countermeasures that may be able to address those threats.},
	language = {en},
	urldate = {2020-01-28},
	booktitle = {4th {International} {ICST} {Conference} on {Wireless} {Internet}},
	publisher = {ICST},
	author = {Bian, Kaigui and Park, Jung-Min “Jerry”},
	year = {2008},
	file = {Bian and Park - 2008 - Security vulnerabilities in IEEE 802.22.pdf:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/SE63FCPM/Bian and Park - 2008 - Security vulnerabilities in IEEE 802.22.pdf:application/pdf}
}

@article{qian_anomaly_2013,
	title = {Anomaly {Spectrum} {Usage} {Detection} in {Multihop} {Cognitive} {Radio} {Networks}: {A} {Cross}-{Layer} {Approach}},
	volume = {8},
	shorttitle = {Anomaly {Spectrum} {Usage} {Detection} in {Multihop} {Cognitive} {Radio} {Networks}},
	abstract = {awareness of their environment and internal state, fulfills the need of dynamic spectrum access for higher spectrum utilization. However, at the same time, the use of cognitive radios further complicates the security problems in wireless networks and introduces additional challenges for a counter measure. Due to the intelligence of the attackers, many of the attacks may be stealthy by nature, such as the anomalous spectrum usage attacks, which usually cannot be detected only using information from one layer of the protocol stack. In this paper, we propose a crosslayer model for anomalous spectrum usage attacks detection with stealthy jammer as a prime example. Quickest detection technique is adopted and embedded in the proposed framework since the attacks usually happen at unknown time and are unpredictable due to the lack of prior knowledge of the attackers. A case study is performed by combining physical layer spectrum sensing with multipath routing at the network layer. The results demonstrate the effectiveness of the proposed approach.},
	journal = {JCM},
	author = {Qian, Lijun and Li, Xiangfang and Wei, Shuangqing},
	year = {2013},
	keywords = {Anomaly detection, Cognitive radio, Embedded system, Multipath routing, Protocol stack},
	pages = {259--266},
	file = {Submitted Version:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/NT35K8ZU/Qian et al. - 2013 - Anomaly Spectrum Usage Detection in Multihop Cogni.pdf:application/pdf}
}

@article{ashton_that_2009,
	title = {That '{Internet} of {Things}' {Thing}},
	language = {en},
	author = {Ashton, Kevin},
	month = jun,
	year = {2009},
	pages = {1},
	file = {Ashton - That 'Internet of Things' Thing.pdf:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/7X9XYFVF/Ashton - That 'Internet of Things' Thing.pdf:application/pdf}
}

@inproceedings{al-sarawi_internet_2017,
	address = {Amman, Jordan},
	title = {Internet of {Things} ({IoT}) communication protocols: {Review}},
	isbn = {978-1-5090-6332-1},
	shorttitle = {Internet of {Things} ({IoT}) communication protocols},
	abstract = {Internet of Things (IoT) consists of smart devices that communicate with each other. It enables these devices to collect and exchange data. Besides, IoT has now a wide range of life applications such as industry, transportation, logistics, healthcare, smart environment, as well as personal, social gaming robot, and city information. Smart devices can have wired or wireless connection. As far as the wireless IoT is the main concern, many different wireless communication technologies and protocols can be used to connect the smart device such as Internet Protocol Version 6 (IPv6), over Low power Wireless Personal Area Networks (6LoWPAN), ZigBee, Bluetooth Low Energy (BLE), Z-Wave and Near Field Communication (NFC). They are short range standard network protocols, while SigFox and Cellular are Low Power Wide Area Network (LPWAN).standard protocols.},
	language = {en},
	urldate = {2020-01-29},
	booktitle = {2017 8th {International} {Conference} on {Information} {Technology} ({ICIT})},
	publisher = {IEEE},
	author = {Al-Sarawi, Shadi and Anbar, Mohammed and Alieyan, Kamal and Alzubaidi, Mahmood},
	month = may,
	year = {2017},
	pages = {685--690},
	file = {Al-Sarawi et al. - 2017 - Internet of Things (IoT) communication protocols .pdf:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/6NDFL5SV/Al-Sarawi et al. - 2017 - Internet of Things (IoT) communication protocols .pdf:application/pdf}
}

@article{khan_cognitive-radio-based_2017,
	title = {Cognitive-{Radio}-{Based} {Internet} of {Things}: {Applications}, {Architectures}, {Spectrum} {Related} {Functionalities}, and {Future} {Research} {Directions}},
	volume = {24},
	issn = {1558-0687},
	shorttitle = {Cognitive-{Radio}-{Based} {Internet} of {Things}},
	abstract = {Recent research and technology trends are shifting toward IoT and CRNs. However, we think that the things-oriented, Internet-oriented, and semantic-oriented versions of IoT are meaningless if IoT objects are not equipped with cognitive radio capability. Equipping IoT objects with CR capability has lead to a new research dimension of CR-based IoT. In this article, we present an overview of CR-based IoT systems. We highlight potential applications of CR-based IoT systems. We survey architectures and frameworks of CR-based IoT systems. We furthermore discuss spectrum-related functionalities for CR-based IoT systems. Finally, we present open issues, research challenges, and future direction for these CR-based IoT networks.},
	number = {3},
	journal = {IEEE Wireless Communications},
	author = {Khan, Athar Ali and Rehmani, Mubashir Husain and Rachedi, Abderrezak},
	month = jun,
	year = {2017},
	keywords = {cognitive radio, radio spectrum management, Sensors, Wireless sensor networks, Cognitive radio, Internet of Things, cognitive radio capability, CR-based IoT networks, CR-based IoT systems, CRN, Energy consumption, IoT objects, Machine-to-machine communications, Network architecture, spectrum-related functionalities},
	pages = {17--25},
	file = {IEEE Xplore Abstract Record:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/LZZ7W4FY/references.html:text/html;IEEE Xplore Full Text PDF:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/VQKIT5CG/Khan et al. - 2017 - Cognitive-Radio-Based Internet of Things Applicat.pdf:application/pdf}
}

@article{aijaz_cognitive_2015,
	title = {Cognitive {Machine}-to-{Machine} {Communications} for {Internet}-of-{Things}: {A} {Protocol} {Stack} {Perspective}},
	volume = {2},
	issn = {2372-2541},
	shorttitle = {Cognitive {Machine}-to-{Machine} {Communications} for {Internet}-of-{Things}},
	abstract = {Machine-to-machine (M2M) communications enables networked devices to exchange information among each other as well as with business application servers and therefore creates what is known as the Internet-of-Things (IoT). The research community has a consensus for the need of a standardized protocol stack for M2M communications. On the other hand, cognitive radio technology is very promising for M2M communications due to a number of factors. It is expected that cognitive M2M communications will be indispensable in order to realize the vision of IoT. However cognitive M2M communications requires a cognitive radio-enabled protocol stack in addition to the fundamental requirements of energy efficiency, reliability, and Internet connectivity. The main objective of this paper is to provide the state of the art in cognitive M2M communications from a protocol stack perspective. This paper covers the emerging standardization efforts and the latest developments on protocols for cognitive M2M networks. In addition, this paper also presents the authors' recent work in this area, which includes a centralized cognitive medium access control (MAC) protocol, a distributed cognitive MAC protocol, and a specially designed routing protocol for cognitive M2M networks. These protocols explicitly account for the peculiarities of cognitive radio environments. Performance evaluation demonstrates that the proposed protocols not only ensure protection to the primary users (PUs) but also fulfil the utility requirements of the secondary M2M networks.},
	number = {2},
	journal = {IEEE Internet of Things Journal},
	author = {Aijaz, Adnan and Aghvami, A. Hamid},
	month = apr,
	year = {2015},
	keywords = {cognitive radio, Cognitive radio, protocol stack, Internet of Things, IoT, Machine-to-machine communications, access protocols, business application servers, centralized cognitive medium access control protocol, cognitive M2M networks, cognitive machine-to-machine communications, cognitive radio-enabled protocol stack, distributed cognitive MAC protocol, IEEE 802.15 Standards, Internet-of-Things, Internet-of-Things (IoT), LLN, low power and lossy network (LLN), M2M communications, MAC routing, Machine-to-Machine, machine-to-machine (M2M), Media Access Protocol, medium access control (MAC), Modulation, protocol stack perspective, routing, routing for low power and lossy networks (RPL), RPL},
	pages = {103--112},
	file = {IEEE Xplore Abstract Record:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/3ZBHPIQ2/7006643.html:text/html;IEEE Xplore Full Text PDF:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/JEP94GQ2/Aijaz and Aghvami - 2015 - Cognitive Machine-to-Machine Communications for In.pdf:application/pdf}
}

@article{devries_dataset_2017,
	title = {Dataset {Augmentation} in {Feature} {Space}},
	abstract = {Dataset augmentation, the practice of applying a wide array of domain-specific transformations to synthetically expand a training set, is a standard tool in supervised learning. While effective in tasks such as visual recognition, the set of transformations must be carefully designed, implemented, and tested for every new domain, limiting its re-use and generality. In this paper, we adopt a simpler, domain-agnostic approach to dataset augmentation. We start with existing data points and apply simple transformations such as adding noise, interpolating, or extrapolating between them. Our main insight is to perform the transformation not in input space, but in a learned feature space. A re-kindling of interest in unsupervised representation learning makes this technique timely and more effective. It is a simple proposal, but to-date one that has not been tested empirically. Working in the space of context vectors generated by sequence-to-sequence models, we demonstrate a technique that is effective for both static and sequential data.},
	urldate = {2020-01-29},
	journal = {arXiv:1702.05538 [cs, stat]},
	author = {DeVries, Terrance and Taylor, Graham W.},
	month = feb,
	year = {2017},
	note = {arXiv: 1702.05538},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/5EXDHXGN/DeVries and Taylor - 2017 - Dataset Augmentation in Feature Space.pdf:application/pdf;arXiv.org Snapshot:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/XXT9Z7Y2/1702.html:text/html}
}

@inproceedings{cubuk_autoaugment_2019,
	title = {{AutoAugment}: {Learning} {Augmentation} {Strategies} {From} {Data}},
	shorttitle = {{AutoAugment}},
	urldate = {2020-01-29},
	author = {Cubuk, Ekin D. and Zoph, Barret and Mane, Dandelion and Vasudevan, Vijay and Le, Quoc V.},
	year = {2019},
	pages = {113--123},
	file = {Full Text PDF:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/3RMTDAVA/Cubuk et al. - 2019 - AutoAugment Learning Augmentation Strategies From.pdf:application/pdf;Snapshot:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/JLH97ZAD/Cubuk_AutoAugment_Learning_Augmentation_Strategies_From_Data_CVPR_2019_paper.html:text/html}
}

@article{meamari_game-theory-based_2016,
	title = {Game-theory-based analysis on interactions among secondary and malicious users in coordinated jamming attack in cognitive radio systems},
	volume = {25},
	issn = {0218-1266, 1793-6454},
	abstract = {IEEE 802.22 standard utilizes cognitive radio (CR) techniques to allow sharing unused spectrum band. The cognitive radio is vulnerable to various attacks such as jamming attacks. This paper has focused on coordinated jamming attacks. A simple strategy for secondary users is to change their bands and switch to other appropriate bands when the jamming attack is occurred. Also, the malicious users should switch to other bands in order to jam the secondary users. To address this problem, a game theoretical method is proposed to analyze coordinated jamming attacks in CR. Then, using Nash equilibrium on the proposed game, the most appropriate bands have been found to switch as well as the optimal switching probabilities for both secondary and malicious users. Meanwhile, effects of different parameters like the number of malicious users are investigated in changing the optimal switching probabilities by analysis on the model.},
	number = {08},
	urldate = {2020-01-31},
	journal = {Journal of Circuits, Systems and Computers},
	author = {Meamari, Ehsan and Afhamisisi, Khadijeh and Shahhoseini, Hadi Shahriar},
	month = aug,
	year = {2016},
	note = {arXiv: 1912.12173},
	keywords = {Computer Science - Computer Science and Game Theory, Computer Science - Cryptography and Security},
	pages = {1650097},
	file = {arXiv Fulltext PDF:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/UCAVM5CG/Meamari et al. - 2016 - Game-theory-based analysis on interactions among s.pdf:application/pdf;arXiv.org Snapshot:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/93S5LQRK/1912.html:text/html}
}

@article{zheng_deep_2019,
	title = {Deep {Learning} for {Cooperative} {Radio} {Signal} {Classification}},
	abstract = {Radio signal classification has a very wide range of applications in cognitive radio networks and electromagnetic spectrum monitoring. In this article, we consider scenarios where multiple nodes in the network participate in cooperative classification. We propose cooperative radio signal classification methods based on deep learning for decision fusion, signal fusion and feature fusion, respectively. We analyze the performance of these methods through simulation experiments. We conclude the article with a discussion of research challenges and open problems.},
	urldate = {2020-01-31},
	journal = {arXiv:1909.06031 [eess]},
	author = {Zheng, Shilian and Chen, Shichuan and Yang, Xiaoniu},
	month = sep,
	year = {2019},
	note = {arXiv: 1909.06031},
	keywords = {Electrical Engineering and Systems Science - Signal Processing},
	file = {arXiv Fulltext PDF:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/6RLPXIW8/Zheng et al. - 2019 - Deep Learning for Cooperative Radio Signal Classif.pdf:application/pdf;arXiv.org Snapshot:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/L8SXV2UI/1909.html:text/html}
}

@article{zheng_spectrum_2019,
	title = {Spectrum {Sensing} {Based} on {Deep} {Learning} {Classification} for {Cognitive} {Radios}},
	abstract = {Spectrum sensing is a key technology for cognitive radios. We present spectrum sensing as a classification problem and propose a sensing method based on deep learning classification. We normalize the received signal power to overcome the effects of noise power uncertainty. We train the model with as many types of signals as possible as well as noise data to enable the trained network model to adapt to untrained new signals. We also use transfer learning strategies to improve the performance for real-world signals. Extensive experiments are conducted to evaluate the performance of this method. The simulation results show that the proposed method performs better than two traditional spectrum sensing methods, i.e., maximum-minimum eigenvalue ratio-based method and frequency domain entropy-based method. In addition, the experimental results of the new untrained signal types show that our method can adapt to the detection of these new signals. Furthermore, the real-world signal detection experiment results show that the detection performance can be further improved by transfer learning. Finally, experiments under colored noise show that our proposed method has superior detection performance under colored noise, while the traditional methods have a significant performance degradation, which further validate the superiority of our method.},
	urldate = {2020-01-31},
	journal = {arXiv:1909.06020 [cs, eess]},
	author = {Zheng, Shilian and Chen, Shichuan and Qi, Peihan and Zhou, Huaji and Yang, Xiaoniu},
	month = sep,
	year = {2019},
	note = {arXiv: 1909.06020},
	keywords = {Computer Science - Machine Learning, Electrical Engineering and Systems Science - Signal Processing},
	file = {arXiv Fulltext PDF:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/8SS7SP8Z/Zheng et al. - 2019 - Spectrum Sensing Based on Deep Learning Classifica.pdf:application/pdf;arXiv.org Snapshot:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/MBHP72E3/1909.html:text/html}
}

@article{gao_deep_2019,
	title = {Deep {Learning} for {Spectrum} {Sensing}},

	abstract = {In cognitive radio systems, the ability to accurately detect primary user's signal is essential to secondary user in order to utilize idle licensed spectrum. Conventional energy detector is a good choice for blind signal detection, while it suffers from the well-known SNR-wall due to noise uncertainty. In this letter, we firstly propose a deep learning based signal detector which exploits the underlying structural information of the modulated signals, and is shown to achieve the state of the art detection performance, requiring no prior knowledge about channel state information or background noise. In addition, the impacts of modulation scheme and sample length on performance are investigated. Finally, a deep learning based cooperative detection system is proposed, which is shown to provide substantial performance gain over conventional cooperative sensing methods.},
	urldate = {2020-01-31},
	journal = {arXiv:1909.02730 [cs, eess, math]},
	author = {Gao, Jiabao and Yi, Xuemei and Zhong, Caijun and Chen, Xiaoming and Zhang, Zhaoyang},
	month = sep,
	year = {2019},
	note = {arXiv: 1909.02730},
	keywords = {Electrical Engineering and Systems Science - Signal Processing, Computer Science - Information Theory},
	file = {arXiv Fulltext PDF:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/RCS2UXKR/Gao et al. - 2019 - Deep Learning for Spectrum Sensing.pdf:application/pdf;arXiv.org Snapshot:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/Y79PVRCL/1909.html:text/html}
}

@article{shi_generative_2019,
	title = {Generative {Adversarial} {Network} for {Wireless} {Signal} {Spoofing}},

	abstract = {The paper presents a novel approach of spoofing wireless signals by using a general adversarial network (GAN) to generate and transmit synthetic signals that cannot be reliably distinguished from intended signals. It is of paramount importance to authenticate wireless signals at the PHY layer before they proceed through the receiver chain. For that purpose, various waveform, channel, and radio hardware features that are inherent to original wireless signals need to be captured. In the meantime, adversaries become sophisticated with the cognitive radio capability to record, analyze, and manipulate signals before spoofing. Building upon deep learning techniques, this paper introduces a spoofing attack by an adversary pair of a transmitter and a receiver that assume the generator and discriminator roles in the GAN and play a minimax game to generate the best spoofing signals that aim to fool the best trained defense mechanism. The output of this approach is two-fold. From the attacker point of view, a deep learning-based spoofing mechanism is trained to potentially fool a defense mechanism such as RF fingerprinting. From the defender point of view, a deep learning-based defense mechanism is trained against potential spoofing attacks when an adversary pair of a transmitter and a receiver cooperates. The probability that the spoofing signal is misclassified as the intended signal is measured for random signal, replay, and GAN-based spoofing attacks. Results show that the GAN-based spoofing attack provides a major increase in the success probability of wireless signal spoofing even when a deep learning classifier is used as the defense.},
	urldate = {2020-01-31},
	journal = {arXiv:1905.01008 [cs, eess, stat]},
	author = {Shi, Yi and Davaslioglu, Kemal and Sagduyu, Yalin E.},
	month = may,
	year = {2019},
	note = {arXiv: 1905.01008},
	keywords = {Computer Science - Machine Learning, Computer Science - Networking and Internet Architecture, Statistics - Machine Learning, Electrical Engineering and Systems Science - Signal Processing},
	file = {arXiv Fulltext PDF:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/L6MYS6YE/Shi et al. - 2019 - Generative Adversarial Network for Wireless Signal.pdf:application/pdf;arXiv.org Snapshot:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/IPBHE3WM/1905.html:text/html}
}

@article{ye_neural_2019,
	title = {A {Neural} {Network} {Detector} for {Spectrum} {Sensing} under {Uncertainties}},

	abstract = {Spectrum sensing is of critical importance in any cognitive radio system. When the primary user's signal has uncertain parameters, the likelihood ratio test, which is the theoretically optimal detector, generally has no closed-form expression. As a result, spectrum sensing under parameter uncertainty remains an open question, though many detectors exploiting specific features of a primary signal have been proposed and have achieved reasonably good performance. In this paper, a neural network is trained as a detector for modulated signals. The result shows by training on an appropriate dataset, the neural network gains robustness under uncertainties in system parameters including the carrier frequency offset, carrier phase offset, and symbol time offset. The result displays the neural network's potential in exploiting implicit and incomplete knowledge about the signal's structure.},
	urldate = {2020-01-31},
	journal = {arXiv:1907.07326 [cs, eess, math, stat]},
	author = {Ye, Ziyu and Peng, Qihang and Levick, Kelly and Rong, Hui and Gilman, Andrew and Cosman, Pamela and Milstein, Larry},
	month = aug,
	year = {2019},
	note = {arXiv: 1907.07326},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Electrical Engineering and Systems Science - Signal Processing, Computer Science - Information Theory},
	file = {arXiv Fulltext PDF:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/479GNUKP/Ye et al. - 2019 - A Neural Network Detector for Spectrum Sensing und.pdf:application/pdf;arXiv.org Snapshot:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/MWQNQJA7/1907.html:text/html}
}

@article{mrabet_primary_2018,
	title = {Primary {User} {Emulation} {Attacks}: {A} {Detection} {Technique} {Based} on {Kalman} {Filter}},
	volume = {7},
	issn = {2224-2708},
	shorttitle = {Primary {User} {Emulation} {Attacks}},
	abstract = {Cognitive radio technology addresses the problem of spectrum scarcity by allowing secondary users to use the vacant spectrum bands without causing interference to the primary users. However, several attacks could disturb the normal functioning of the cognitive radio network. Primary user emulation attacks are one of the most severe attacks in which a malicious user emulates the primary user signal characteristics to either prevent other legitimate secondary users from accessing the idle channels or causing harmful interference to the primary users. There are several proposed approaches to detect the primary user emulation attackers. However, most of these techniques assume that the primary user location is fixed, which does not make them valid when the primary user is mobile. In this paper, we propose a new approach based on the Kalman filter framework for detecting the primary user emulation attacks with a non-stationary primary user. Several experiments have been conducted and the advantages of the proposed approach are demonstrated through the simulation results.},
	number = {3},
	urldate = {2020-01-31},
	journal = {Journal of Sensor and Actuator Networks},
	author = {Mrabet, Zakaria El and Arjoune, Youness and Ghazi, Hassan El and Majd, Badr Abou Al and Kaabouch, Naima},
	month = jul,
	year = {2018},
	note = {arXiv: 1903.03684},
	keywords = {Computer Science - Machine Learning, Computer Science - Networking and Internet Architecture, Computer Science - Cryptography and Security, Computer Science - Performance},
	pages = {26},
	file = {arXiv Fulltext PDF:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/8S68JJGM/Mrabet et al. - 2018 - Primary User Emulation Attacks A Detection Techni.pdf:application/pdf;arXiv.org Snapshot:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/J9E82XPR/1903.html:text/html}
}

@article{gregoire_visualisation_2005,
	title = {Visualisation for {Network} {Situational} {Awareness} in {Computer} {Network} {Defence}},
	language = {en},
	author = {Gregoire, Marc and Beaudoin, Luc},
	year = {2005},
	pages = {6},
	file = {Gregoire and Beaudoin - Visualisation for Network Situational Awareness in.pdf:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/NBHYXBFG/Gregoire and Beaudoin - Visualisation for Network Situational Awareness in.pdf:application/pdf}
}

@article{akyildiz_next_2006,
	title = {{NeXt} generation/dynamic spectrum access/cognitive radio wireless networks: {A} survey},
	volume = {50},
	issn = {1389-1286},
	shorttitle = {{NeXt} generation/dynamic spectrum access/cognitive radio wireless networks},
	abstract = {Today’s wireless networks are characterized by a fixed spectrum assignment policy. However, a large portion of the assigned spectrum is used sporadically and geographical variations in the utilization of assigned spectrum ranges from 15\% to 85\% with a high variance in time. The limited available spectrum and the inefficiency in the spectrum usage necessitate a new communication paradigm to exploit the existing wireless spectrum opportunistically. This new networking paradigm is referred to as NeXt Generation (xG) Networks as well as Dynamic Spectrum Access (DSA) and cognitive radio networks. The term xG networks is used throughout the paper. The novel functionalities and current research challenges of the xG networks are explained in detail. More specifically, a brief overview of the cognitive radio technology is provided and the xG network architecture is introduced. Moreover, the xG network functions such as spectrum management, spectrum mobility and spectrum sharing are explained in detail. The influence of these functions on the performance of the upper layer protocols such as routing and transport are investigated and open research issues in these areas are also outlined. Finally, the cross-layer design challenges in xG networks are discussed.},
	language = {en},
	number = {13},
	urldate = {2020-02-03},
	journal = {Computer Networks},
	author = {Akyildiz, Ian F. and Lee, Won-Yeol and Vuran, Mehmet C. and Mohanty, Shantidev},
	month = sep,
	year = {2006},
	keywords = {Cognitive radio networks, Dynamic spectrum access networks, Next generation networks, Spectrum management, Spectrum mobility, Spectrum sensing, Spectrum sharing},
	pages = {2127--2159},
	file = {ScienceDirect Snapshot:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/FC2FXHVJ/S1389128606001009.html:text/html}
}

@misc{kolodzy_spectrum_2002,
	title = {Spectrum {Policy} {Task} {Force}},

	abstract = {Spectrum Policy Task Force report setting forth recommendations for spectrum policy reform},
	language = {en},
	urldate = {2020-02-03},
	journal = {Federal Communications Commission},
	author = {Kolodzy, Paul},
	year = {2002},
	file = {DOC-228542A1.pdf:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/FKVANAQ8/DOC-228542A1.pdf:application/pdf;Snapshot:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/G8DK2ZK3/spectrum-policy-task-force.html:text/html}
}

@article{donahue_adversarial_2017,
	title = {Adversarial {Feature} {Learning}},

	abstract = {The ability of the Generative Adversarial Networks (GANs) framework to learn generative models mapping from simple latent distributions to arbitrarily complex data distributions has been demonstrated empirically, with compelling results showing that the latent space of such generators captures semantic variation in the data distribution. Intuitively, models trained to predict these semantic latent representations given data may serve as useful feature representations for auxiliary problems where semantics are relevant. However, in their existing form, GANs have no means of learning the inverse mapping -- projecting data back into the latent space. We propose Bidirectional Generative Adversarial Networks (BiGANs) as a means of learning this inverse mapping, and demonstrate that the resulting learned feature representation is useful for auxiliary supervised discrimination tasks, competitive with contemporary approaches to unsupervised and self-supervised feature learning.},
	urldate = {2020-02-04},
	journal = {arXiv:1605.09782 [cs, stat]},
	author = {Donahue, Jeff and Krähenbühl, Philipp and Darrell, Trevor},
	month = apr,
	year = {2017},
	note = {arXiv: 1605.09782},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Neural and Evolutionary Computing, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/UEPMVY6G/Donahue et al. - 2017 - Adversarial Feature Learning.pdf:application/pdf;arXiv.org Snapshot:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/553GIQW6/1605.html:text/html}
}

@misc{noauthor_structural_nodate,
	title = {Structural {Bioinformatics} {Library}: {Earth}\_mover\_distance},

	urldate = {2020-02-09}
}

@article{kockesen_introduction_2007,
	title = {An {Introduction} to {Game} {Theory}},
	language = {en},
	author = {Kockesen, Levent and Ok, Efe A},
	year = {2007},
	pages = {141},
	file = {Kockesen and Ok - An Introduction to Game Theory.pdf:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/2EB2Q9XD/Kockesen and Ok - An Introduction to Game Theory.pdf:application/pdf}
}

@inproceedings{sinha_stackelberg_2018,
	address = {Stockholm, Sweden},
	title = {Stackelberg {Security} {Games}: {Looking} {Beyond} a {Decade} of {Success}},
	isbn = {978-0-9992411-2-7},
	shorttitle = {Stackelberg {Security} {Games}},
	abstract = {The Stackelberg Security Game (SSG) model has been immensely inﬂuential in security research since it was introduced roughly a decade ago. Furthermore, deployed SSG-based applications are one of most successful examples of game theory applications in the real world. We present a broad survey of recent technical advances in SSG and related literature, and then look to the future by highlighting the new potential applications and open research problems in SSG.},
	language = {en},
	urldate = {2020-02-11},
	booktitle = {Twenty-{Seventh} {International} {Joint} {Conference} on {Artificial} {Intelligence}},
	publisher = {International Joint Conferences on Artificial Intelligence Organization},
	author = {Sinha, Arunesh and Fang, Fei and An, Bo and Kiekintveld, Christopher and Tambe, Milind},
	month = jul,
	year = {2018},
	pages = {5494--5501},
	file = {Sinha et al. - 2018 - Stackelberg Security Games Looking Beyond a Decad.pdf:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/QM2RNIAX/Sinha et al. - 2018 - Stackelberg Security Games Looking Beyond a Decad.pdf:application/pdf}
}

@incollection{von_neumann_zero-sum_1944,
	title = {Zero-sum {Two}-person {Games}: {Theory}},
	isbn = {978-0-691-13061-3},
	shorttitle = {{ZERO}-{SUM} {TWO}-{PERSON} {GAMES}},

	abstract = {12.1.1. In the preceding chapter we obtained an all-inclusive formal characterization of the general game of \textit{n} persons (cf. 10.1.). We followed up by developing an exact concept of strategy which permitted us to replace the rather complicated general scheme of a game by a much more simple special one, which was nevertheless shown to be fully equivalent to the former (cf. 11.2.). In the discussion which follows it will sometimes be more convenient to use one form, sometimes the other. It is therefore desirable to give them specific technical names. We will accordingly call them the \textit{extensive} and the},
	urldate = {2020-02-11},
	booktitle = {Theory of {Games} and {Economic} {Behavior} (60th {Anniversary} {Commemorative} {Edition})},
	publisher = {Princeton University Press},
	author = {von Neumann, John and Morgenstern, Oskar and Kuhn, Harold W. and Rubinstein, Ariel},
	year = {1944},
	pages = {85--168},
	file = {von Neumann et al. - 1944 - Zero-sum Two-person Games Theory.pdf:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/SLRHFHTC/von Neumann et al. - 1944 - Zero-sum Two-person Games Theory.pdf:application/pdf}
}

@incollection{tellambura_overview_2017,
	title = {{AN} {OVERVIEW} {OF} {COGNITIVE} {RADIO} {NETWORKS}},
	author = {Tellambura, Chintha and Kusaladharma, Sachitha},
	month = mar,
	year = {2017},
	file = {Full Text PDF:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/BRR7NLC3/Tellambura and Kusaladharma - 2017 - AN OVERVIEW OF COGNITIVE RADIO NETWORKS.pdf:application/pdf}
}

@article{zhao_radio_2006,
	title = {Radio {Environment} {Map} {Enabled} {Situation}-aware {Cognitive} {Radio} {Learning} {Algorithms}},
	abstract = {This paper presents an innovative and generic approach to developing cognitive radios (CR) based on the radio environment map (REM). REM is envisioned as an integrated database consisting of multi-domain information, which supports global cross-layer optimization by enabling CR to “look” through various layers. The REM, as a vehicle of network support to CR, can be exploited by the cognitive radio engine (CE) for various cognitive functionalities such as situation awareness, reasoning, learning, planning and decision support. This paper presents the system flow and framework of REM-enabled situation-aware learning algorithms. Simulations demonstrate the effectiveness and efficiency of REM-enabled CR learning algorithms. Furthermore, by sharing information about the radio environment through REM dissemination, the hidden node problem can be mitigated and the secondary users can coexist with primary users (PUs) with minimal harmful interference. Link level and network level simulations are conducted with MATLAB and NS-2, respectively.},
	language = {en},
	author = {Zhao, Youping and Gaeddert, Joseph and Bae, Kyung K and Reed, Jeffery H},
	year = {2006},
	pages = {6},
	file = {Zhao et al. - 2006 - RADIO ENVIRONMENT MAP ENABLED SITUATION-AWARE COGN.pdf:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/S4MEH2HU/Zhao et al. - 2006 - RADIO ENVIRONMENT MAP ENABLED SITUATION-AWARE COGN.pdf:application/pdf}
}

@inproceedings{jain_security_2010,
	title = {Security {Games} with {Arbitrary} {Schedules}: {A} {Branch} and {Price} {Approach}},
	copyright = {Authors who publish a paper in this conference agree to the following terms:   Author(s) agree to transfer their copyrights in their article/paper to the Association for the Advancement of Artificial Intelligence (AAAI), in order to deal with future requests for reprints, translations, anthologies, reproductions, excerpts, and other publications. This grant will include, without limitation, the entire copyright in the article/paper in all countries of the world, including all renewals, extensions, and reversions thereof, whether such rights current exist or hereafter come into effect, and also the exclusive right to create electronic versions of the article/paper, to the extent that such right is not subsumed under copyright.  The author(s) warrants that they are the sole author and owner of the copyright in the above article/paper, except for those portions shown to be in quotations; that the article/paper is original throughout; and that the undersigned right to make the grants set forth above is complete and unencumbered.  The author(s) agree that if anyone brings any claim or action alleging facts that, if true, constitute a breach of any of the foregoing warranties, the author(s) will hold harmless and indemnify AAAI, their grantees, their licensees, and their distributors against any liability, whether under judgment, decree, or compromise, and any legal fees and expenses arising out of that claim or actions, and the undersigned will cooperate fully in any defense AAAI may make to such claim or action. Moreover, the undersigned agrees to cooperate in any claim or other action seeking to protect or enforce any right the undersigned has granted to AAAI in the article/paper. If any such claim or action fails because of facts that constitute a breach of any of the foregoing warranties, the undersigned agrees to reimburse whomever brings such claim or action for expenses and attorneys’ fees incurred therein.  Author(s) retain all proprietary rights other than copyright (such as patent rights).  Author(s) may make personal reuse of all or portions of the above article/paper in other works of their own authorship.  Author(s) may reproduce, or have reproduced, their article/paper for the author’s personal use, or for company use provided that AAAI copyright and the source are indicated, and that the copies are not used in a way that implies AAAI endorsement of a product or service of an employer, and that the copies per se are not offered for sale. The foregoing right shall not permit the posting of the article/paper in electronic or digital form on any computer network, except by the author or the author’s employer, and then only on the author’s or the employer’s own web page or ftp site. Such web page or ftp site, in addition to the aforementioned requirements of this Paragraph, must provide an electronic reference or link back to the AAAI electronic server, and shall not post other AAAI copyrighted materials not of the author’s or the employer’s creation (including tables of contents with links to other papers) without AAAI’s written permission.  Author(s) may make limited distribution of all or portions of their article/paper prior to publication.  In the case of work performed under U.S. Government contract, AAAI grants the U.S. Government royalty-free permission to reproduce all or portions of the above article/paper, and to authorize others to do so, for U.S. Government purposes.  In the event the above article/paper is not accepted and published by AAAI, or is withdrawn by the author(s) before acceptance by AAAI, this agreement becomes null and void.},
	shorttitle = {Security {Games} with {Arbitrary} {Schedules}},

	abstract = {Security games, and important class of Stackelberg games, are used in deployed decision-support tools in use by LAX police and the Federal Air Marshals Service. The algorithms used to solve these games find optimal randomized schedules to allocate security resources for infrastructure protection. Unfortunately, the state of the art algorithms either fail to scale or to provide a correct solution for large problems with arbitrary scheduling constraints. We introduce ASPEN, a branch-and-price approach that overcomes these limitations based on two key contributions: (i) A column-generation approach that exploits a novel network flow representation, avoiding a combinatorial explosion of schedule allocations; (ii) A branch-and-bound algorithm that generates bounds via a fast algorithm for solving security games with relaxed scheduling constraints. ASPEN is the first known method for efficiently solving massive security games with arbitrary schedules.},
	language = {en},
	urldate = {2020-02-12},
	booktitle = {Twenty-{Fourth} {AAAI} {Conference} on {Artificial} {Intelligence}},
	author = {Jain, Manish and Kardes, Erim and Kiekintveld, Christopher and Ordonez, Fernando and Tambe, Milind},
	month = jul,
	year = {2010},
	file = {Full Text PDF:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/RHK3QX75/Jain et al. - 2010 - Security Games with Arbitrary Schedules A Branch .pdf:application/pdf;Snapshot:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/D2CCQYMN/1698.html:text/html}
}


@article{shorten_survey_2019,
	title = {A survey on {Image} {Data} {Augmentation} for {Deep} {Learning}},
	volume = {6},
	issn = {2196-1115},
	abstract = {Deep convolutional neural networks have performed remarkably well on many Computer Vision tasks. However, these networks are heavily reliant on big data to avoid overfitting. Overfitting refers to the phenomenon when a network learns a function with very high variance such as to perfectly model the training data. Unfortunately, many application domains do not have access to big data, such as medical image analysis. This survey focuses on Data Augmentation, a data-space solution to the problem of limited data. Data Augmentation encompasses a suite of techniques that enhance the size and quality of training datasets such that better Deep Learning models can be built using them. The image augmentation algorithms discussed in this survey include geometric transformations, color space augmentations, kernel filters, mixing images, random erasing, feature space augmentation, adversarial training, generative adversarial networks, neural style transfer, and meta-learning. The application of augmentation methods based on GANs are heavily covered in this survey. In addition to augmentation techniques, this paper will briefly discuss other characteristics of Data Augmentation such as test-time augmentation, resolution impact, final dataset size, and curriculum learning. This survey will present existing methods for Data Augmentation, promising developments, and meta-level decisions for implementing Data Augmentation. Readers will understand how Data Augmentation can improve the performance of their models and expand limited datasets to take advantage of the capabilities of big data.},
	language = {en},
	number = {1},
	urldate = {2020-02-12},
	journal = {Journal of Big Data},
	author = {Shorten, Connor and Khoshgoftaar, Taghi M.},
	month = jul,
	year = {2019},
	keywords = {Big data, Data Augmentation, Deep Learning, GANs, Image data},
	pages = {60},
	file = {Springer Full Text PDF:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/KXBKKVDQ/Shorten and Khoshgoftaar - 2019 - A survey on Image Data Augmentation for Deep Learn.pdf:application/pdf}
}

@article{liu_survey_2017,
	title = {A survey of deep neural network architectures and their applications},
	volume = {234},
	issn = {0925-2312},
	abstract = {Since the proposal of a fast learning algorithm for deep belief networks in 2006, the deep learning techniques have drawn ever-increasing research interests because of their inherent capability of overcoming the drawback of traditional algorithms dependent on hand-designed features. Deep learning approaches have also been found to be suitable for big data analysis with successful applications to computer vision, pattern recognition, speech recognition, natural language processing, and recommendation systems. In this paper, we discuss some widely-used deep learning architectures and their practical applications. An up-to-date overview is provided on four deep learning architectures, namely, autoencoder, convolutional neural network, deep belief network, and restricted Boltzmann machine. Different types of deep neural networks are surveyed and recent progresses are summarized. Applications of deep learning techniques on some selected areas (speech recognition, pattern recognition and computer vision) are highlighted. A list of future research topics are finally given with clear justifications.},
	language = {en},
	urldate = {2020-02-18},
	journal = {Neurocomputing},
	author = {Liu, Weibo and Wang, Zidong and Liu, Xiaohui and Zeng, Nianyin and Liu, Yurong and Alsaadi, Fuad E.},
	month = apr,
	year = {2017},
	keywords = {Autoencoder, Convolutional neural network, Deep belief network, Deep learning, Restricted Boltzmann machine},
	pages = {11--26},
	file = {ScienceDirect Full Text PDF:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/48UJQLM5/Liu et al. - 2017 - A survey of deep neural network architectures and .pdf:application/pdf;ScienceDirect Snapshot:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/PAFCGRM3/S0925231216315533.html:text/html}
}

@article{lelidis_fftgan_nodate,
	title = {{FFTGAN}: {Generative} {Adversarial} {Network} system for generating cosmological images in the frequency space},
	abstract = {In this paper, we discuss techniques to quantify quality of cosmological images in the frequency domain. After selecting the neural network topology and input data for the discriminator that give us the best results, we discuss why the same data doesn’t work for the generator. We also explain why the same approach fails for the generator and present our solution for that case. Finally, we discuss the optimizations used to generate large images.},
	language = {en},
	author = {Lelidis, Alexander and Aldughayem, Khalid and Tesic, Uros},
	pages = {5},
	file = {Lelidis et al. - FFTGAN Generative Adversarial Network system for .pdf:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/9CD7PXJ4/Lelidis et al. - FFTGAN Generative Adversarial Network system for .pdf:application/pdf}
}

@article{youm_overview_2017,
	title = {An {Overview} of {Security} and {Privacy} {Issues} for {Internet} of {Things}},
	volume = {E100.D},
	issn = {0916-8532, 1745-1361},
	abstract = {The Internet of Things (IoT) is deﬁned as a global infrastructure for the Information Society, enabling advanced services by interconnecting (physical and virtual) things based on, existing and evolving, interoperable information and communication technologies by ITU-T. Data may be communicated in low-power and lossy environments, which causes complicated security issues. Furthermore, concerns are raised over access of personally identiﬁable information pertaining to IoT devices, network and platforms. Security and privacy concerns have been main barriers to implement IoT, which needs to be resolved appropriate security and privacy measures. This paper describes security threats and privacy concerns of IoT, surveys current studies related to IoT and identiﬁes the various requirements and solutions to address these security threats and privacy concerns. In addition, this paper also focuses on major global standardization activities for security and privacy of Internet of Things. Furthermore, future directions and strategies of international standardization for the Internet of Thing’s security and privacy issues will be given. This paper provides guidelines to assist in suggesting the development and standardization strategies forward to allow a massive deployment of IoT systems in real world.},
	language = {en},
	number = {8},
	urldate = {2020-02-19},
	journal = {IEICE Transactions on Information and Systems},
	author = {Youm, Heung Youl},
	year = {2017},
	pages = {1649--1662},
	file = {Youm - 2017 - An Overview of Security and Privacy Issues for Int.pdf:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/I7HU437X/Youm - 2017 - An Overview of Security and Privacy Issues for Int.pdf:application/pdf}
}

@article{molina-tenorio_machine_2019,
	title = {Machine {Learning} {Techniques} {Applied} to {Multiband} {Spectrum} {Sensing} in {Cognitive} {Radios}},
	volume = {19},
	issn = {1424-8220},
	abstract = {In this work, three speciﬁc machine learning techniques (neural networks, expectation maximization and k-means) are applied to a multiband spectrum sensing technique for cognitive radios. All of them have been used as a classiﬁer using the approximation coeﬃcients from a Multiresolution Analysis in order to detect presence of one or multiple primary users in a wideband spectrum. Methods were tested on simulated and real signals showing a good performance. The results presented of these three methods are eﬀective options for detecting primary user transmission on the multiband spectrum. These methodologies work for 99\% of cases under simulated signals of SNR higher than 0 dB and are feasible in the case of real signals.},
	language = {en},
	number = {21},
	urldate = {2020-02-19},
	journal = {Sensors},
	author = {Molina-Tenorio, Yanqueleth and Prieto-Guerrero, Alfonso and Aguilar-Gonzalez, Rafael and Ruiz-Boqué, Silvia},
	month = oct,
	year = {2019},
	pages = {4715},
	file = {Molina-Tenorio et al. - 2019 - Machine Learning Techniques Applied to Multiband S.pdf:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/4RI2IY6M/Molina-Tenorio et al. - 2019 - Machine Learning Techniques Applied to Multiband S.pdf:application/pdf}
}

@article{tembine_deep_nodate,
	title = {Deep {Learning} {Meets} {Game} {Theory}},
	abstract = {This paper presents an interplay between deep learning and game theory. It models basic deep learning tasks as strategic games. Then, distributionally robust games and their relationship with deep generative adversarial networks are presented. To achieve higher order convergence rate without using second derivative of the objective function, a Bregman discrepancy is used to construct a risk-aware speedup deep learning. Each player has a continuous action space which corresponds into weight space and aims to learn her optimal strategy. The convergence rate of the proposed risk-aware deep learning algorithm is derived using a mean estimate.},
	language = {en},
	author = {Tembine, Hamidou},
	pages = {46},
	file = {Tembine - Deep Learning Meets Game Theory.pdf:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/8897RNEU/Tembine - Deep Learning Meets Game Theory.pdf:application/pdf}
}

@article{tran_improving_2019,
	title = {Improving {GAN} with {Neighbors} {Embedding} and {Gradient} {Matching}},
	volume = {33},
	issn = {2374-3468, 2159-5399},
	abstract = {We propose two new techniques for training Generative Adversarial Networks (GANs) in the unsupervised setting. Our objectives are to alleviate mode collapse in GAN and improve the quality of the generated samples. First, we propose neighbor embedding, a manifold learning-based regularization to explicitly retain local structures of latent samples in the generated samples. This prevents generator from producing nearly identical data samples from different latent samples, and reduces mode collapse. We propose an inverse t-SNE regularizer to achieve this. Second, we propose a new technique, gradient matching, to align the distributions of the generated samples and the real samples. As it is challenging to work with high-dimensional sample distributions, we propose to align these distributions through the scalar discriminator scores. We constrain the difference between the discriminator scores of the real samples and generated ones. We further constrain the difference between the gradients of these discriminator scores. We derive these constraints from Taylor approximations of the discriminator function. We perform experiments to demonstrate that our proposed techniques are computationally simple and easy to be incorporated in existing systems. When Gradient matching and Neighbour embedding are applied together, our GN-GAN achieves outstanding results on 1D/2D synthetic, CIFAR-10 and STL-10 datasets, e.g. FID score of 30.80 for the STL-10 dataset. Our code is available at: https://github.com/tntrung/gan},
	language = {en},
	urldate = {2020-02-19},
	journal = {AAAI Conference on Artificial Intelligence},
	author = {Tran, Ngoc-Trung and Bui, Tuan-Anh and Cheung, Ngai-Man},
	month = jul,
	year = {2019},
	pages = {5191--5198},
	file = {Tran et al. - 2019 - Improving GAN with Neighbors Embedding and Gradien.pdf:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/DSRKR7LW/Tran et al. - 2019 - Improving GAN with Neighbors Embedding and Gradien.pdf:application/pdf}
}

@article{chen_training_2018,
	title = {Training {Generative} {Adversarial} {Networks} via {Primal}-{Dual} {Subgradient} {Methods}: {A} {Lagrangian} {Perspective} on {GAN}},
	shorttitle = {Training {Generative} {Adversarial} {Networks} via {Primal}-{Dual} {Subgradient} {Methods}},

	abstract = {We relate the minimax game of generative adversarial networks (GANs) to ﬁnding the saddle points of the Lagrangian function for a convex optimization problem, where the discriminator outputs and the distribution of generator outputs play the roles of primal variables and dual variables, respectively. This formulation shows the connection between the standard GAN training process and the primal-dual subgradient methods for convex optimization. The inherent connection does not only provide a theoretical convergence proof for training GANs in the function space, but also inspires a novel objective function for training. The modiﬁed objective function forces the distribution of generator outputs to be updated along the direction according to the primal-dual subgradient methods. A toy example shows that the proposed method is able to resolve mode collapse, which in this case cannot be avoided by the standard GAN or Wasserstein GAN. Experiments on both Gaussian mixture synthetic data and real-world image datasets demonstrate the performance of the proposed method on generating diverse samples.},
	language = {en},
	urldate = {2020-02-19},
	journal = {arXiv:1802.01765 [cs, stat]},
	author = {Chen, Xu and Wang, Jiang and Ge, Hao},
	month = feb,
	year = {2018},
	note = {arXiv: 1802.01765},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Chen et al. - 2018 - Training Generative Adversarial Networks via Prima.pdf:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/QI8NQ2H7/Chen et al. - 2018 - Training Generative Adversarial Networks via Prima.pdf:application/pdf}
}

@article{mohammadi_deep_2018,
	title = {Deep {Learning} for {IoT} {Big} {Data} and {Streaming} {Analytics}: {A} {Survey}},
	shorttitle = {Deep {Learning} for {IoT} {Big} {Data} and {Streaming} {Analytics}},

	abstract = {In the era of the Internet of Things (IoT), an enormous amount of sensing devices collect and/or generate various sensory data over time for a wide range of ﬁelds and applications. Based on the nature of the application, these devices will result in big or fast/real-time data streams. Applying analytics over such data streams to discover new information, predict future insights, and make control decisions is a crucial process that makes IoT a worthy paradigm for businesses and a quality-of-life improving technology. In this paper, we provide a thorough overview on using a class of advanced machine learning techniques, namely Deep Learning (DL), to facilitate the analytics and learning in the IoT domain. We start by articulating IoT data characteristics and identifying two major treatments for IoT data from a machine learning perspective, namely IoT big data analytics and IoT streaming data analytics. We also discuss why DL is a promising approach to achieve the desired analytics in these types of data and applications. The potential of using emerging DL techniques for IoT data analytics are then discussed, and its promises and challenges are introduced. We present a comprehensive background on different DL architectures and algorithms. We also analyze and summarize major reported research attempts that leveraged DL in the IoT domain. The smart IoT devices that have incorporated DL in their intelligence background are also discussed. DL implementation approaches on the fog and cloud centers in support of IoT applications are also surveyed. Finally, we shed light on some challenges and potential directions for future research. At the end of each section, we highlight the lessons learned based on our experiments and review of the recent literature.},
	language = {en},
	urldate = {2020-02-19},
	journal = {arXiv:1712.04301 [cs]},
	author = {Mohammadi, Mehdi and Al-Fuqaha, Ala and Sorour, Sameh and Guizani, Mohsen},
	month = jun,
	year = {2018},
	note = {arXiv: 1712.04301},
	keywords = {Computer Science - Machine Learning, Computer Science - Networking and Internet Architecture, Computer Science - Databases},
	file = {Mohammadi et al. - 2018 - Deep Learning for IoT Big Data and Streaming Analy.04301:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/7B7RRXKB/Mohammadi et al. - 2018 - Deep Learning for IoT Big Data and Streaming Analy.04301:application/pdf}
}

@incollection{kondratenko_toward_2019,
	address = {Cham},
	title = {Toward a {Secure} {IoT} {Architecture}},
	volume = {203},
	isbn = {978-3-030-21926-0 978-3-030-21927-7},

	abstract = {The design of a cyber-secure, Internet of Things (IoT), supply chain risk management architecture is proposed. The purpose of the architecture is to reduce vulnerabilities of malicious supply chain risks by applying machine learning (ML), cryptographic hardware monitoring (CHM), and distributed network coordination (DNC) techniques to guard against unforeseen hardware component failures and malicious attacks. These crosscutting technologies are combined into an Instrumentation-and-Control/Operator-in-the-Loop (ICOL) architecture that learns normal and abnormal system behaviors. In the event that the ICOL detects possible abnormal system-component behaviors, an ICOL network alert is triggered that requires an operator veriﬁcation-response action before any control parameters can affect the operation of the system. The operator veriﬁcation-response is fed back into the ML systems to recalibrate the classiﬁcation of normal and abnormal states of the system. As a consequence, this proposal adheres to the notion of an Operator-in-theLoop strategy that combines DNC, ML, and CHM with the creative problem-solving capabilities of human intelligence.},
	language = {en},
	urldate = {2020-02-19},
	booktitle = {Advanced {Control} {Techniques} in {Complex} {Engineering} {Systems}: {Theory} and {Applications}},
	publisher = {Springer International Publishing},
	author = {Hiromoto, Robert E. and Haney, Michael and Vakanski, Aleksandar and Shareef, Bryar},
	editor = {Kondratenko, Yuriy P. and Chikrii, Arkadii A. and Gubarev, Vyacheslav F. and Kacprzyk, Janusz},
	year = {2019},
	pages = {297--323},
	file = {Hiromoto et al. - 2019 - Toward a Secure IoT Architecture.pdf:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/FWDEDEZE/Hiromoto et al. - 2019 - Toward a Secure IoT Architecture.pdf:application/pdf}
}

@article{luo_multivariate_nodate,
	title = {Multivariate {Time} {Series} {Imputation} with {Generative} {Adversarial} {Networks}},
	abstract = {Multivariate time series usually contain a large number of missing values, which hinders the application of advanced analysis methods on multivariate time series data. Conventional approaches to addressing the challenge of missing values, including mean/zero imputation, case deletion, and matrix factorization-based imputation, are all incapable of modeling the temporal dependencies and the nature of complex distribution in multivariate time series. In this paper, we treat the problem of missing value imputation as data generation. Inspired by the success of Generative Adversarial Networks (GAN) in image generation, we propose to learn the overall distribution of a multivariate time series dataset with GAN, which is further used to generate the missing values for each sample. Different from the image data, the time series data are usually incomplete due to the nature of data recording process. A modiﬁed Gate Recurrent Unit is employed in GAN to model the temporal irregularity of the incomplete time series. Experiments on two multivariate time series datasets show that the proposed model outperformed the baselines in terms of accuracy of imputation. Experimental results also showed that a simple model on the imputed data can achieve state-of-the-art results on the prediction tasks, demonstrating the beneﬁts of our model in downstream applications.},
	language = {en},
	author = {Luo, Yonghong and Cai, Xiangrui and Zhang, Ying and Xu, Jun},
	pages = {12},
	file = {Luo et al. - Multivariate Time Series Imputation with Generativ.pdf:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/X335RQTZ/Luo et al. - Multivariate Time Series Imputation with Generativ.pdf:application/pdf}
}

@article{hardy_md-gan_2019,
	title = {{MD}-{GAN}: {Multi}-{Discriminator} {Generative} {Adversarial} {Networks} for {Distributed} {Datasets}},
	shorttitle = {{MD}-{GAN}},

	abstract = {A recent technical breakthrough in the domain of machine learning is the discovery and the multiple applications of Generative Adversarial Networks (GANs). Those generative models are computationally demanding, as a GAN is composed of two deep neural networks, and because it trains on large datasets. A GAN is generally trained on a single server.},
	language = {en},
	urldate = {2020-02-21},
	journal = {arXiv:1811.03850 [cs, stat]},
	author = {Hardy, Corentin and Merrer, Erwan Le and Sericola, Bruno},
	month = feb,
	year = {2019},
	note = {arXiv: 1811.03850},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Hardy et al. - 2019 - MD-GAN Multi-Discriminator Generative Adversarial.pdf:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/4EVHHGZ5/Hardy et al. - 2019 - MD-GAN Multi-Discriminator Generative Adversarial.pdf:application/pdf}
}

@article{jiang_novel_2019,
	title = {A {Novel} {GAN}-based {Fault} {Diagnosis} {Approach} for {Imbalanced} {Industrial} {Time} {Series}},

	abstract = {This paper proposes a novel fault diagnosis approach based on generative adversarial networks (GAN) for imbalanced industrial time series where normal samples are much larger than failure cases. We combine a well-designed feature extractor with GAN to help train the whole network. Aimed at obtaining data distribution and hidden pattern in both original distinguishing features and latent space, the encoder-decoder-encoder three-sub-network is employed in GAN, based on Deep Convolution Generative Adversarial Networks (DCGAN) but without Tanh activation layer and only trained on normal samples. In order to verify the validity and feasibility of our approach, we test it on rolling bearing data from Case Western Reserve University and further verify it on data collected from our laboratory. The results show that our proposed approach can achieve excellent performance in detecting faulty by outputting much larger evaluation scores.},
	urldate = {2020-03-11},
	journal = {arXiv:1904.00575 [cs, stat]},
	author = {Jiang, Wenqian and Cheng, Cheng and Zhou, Beitong and Ma, Guijun and Yuan, Ye},
	month = apr,
	year = {2019},
	note = {arXiv: 1904.00575},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/WCZIEHNM/Jiang et al. - 2019 - A Novel GAN-based Fault Diagnosis Approach for Imb.pdf:application/pdf;arXiv.org Snapshot:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/6PF635UH/1904.html:text/html}
}

@article{marafioti_adversarial_nodate,
	title = {Adversarial {Generation} of {Time}-{Frequency} {Features} with application in audio synthesis},
	abstract = {Time-frequency (TF) representations provide powerful and intuitive features for the analysis of time series such as audio. But still, generative modeling of audio in the TF domain is a subtle matter. Consequently, neural audio synthesis widely relies on directly modeling the waveform and previous attempts at unconditionally synthesizing audio from neurally generated invertible TF features still struggle to produce audio at satisfying quality. In this article, focusing on the short-time Fourier transform, we discuss the challenges that arise in audio synthesis based on generated invertible TF features and how to overcome them. We demonstrate the potential of deliberate generative TF modeling by training a generative adversarial network (GAN) on short-time Fourier features. We show that by applying our guidelines, our TF-based network was able to outperform a state-of-the-art GAN generating waveforms directly, despite the similar architecture in the two networks.},
	language = {en},
	author = {Marafioti, Andrés and Holighaus, Nicki and Perraudin, Nathanaël and Majdak, Piotr},
	pages = {11},
	file = {Marafioti et al. - Adversarial Generation of Time-Frequency Features .pdf:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/PKQDE98I/Marafioti et al. - Adversarial Generation of Time-Frequency Features .pdf:application/pdf}
}

@incollection{vinayakumar_scalable_2018,
	address = {Singapore},
	series = {Studies in {Big} {Data}},
	title = {Scalable {Framework} for {Cyber} {Threat} {Situational} {Awareness} {Based} on {Domain} {Name} {Systems} {Data} {Analysis}},
	isbn = {978-981-10-8476-8},

	abstract = {There are myriad of security solutions that have been developed to tackle the Cyber Security attacks and malicious activities in digital world. They are firewalls, intrusion detection and prevention systems, anti-virus systems, honeypots etc. Despite employing these detection measures and protection mechanisms, the number of successful attacks and the level of sophistication of these attacks keep increasing day-by-day. Also, with the advent of Internet-of-Things, the number of devices connected to Internet has risen dramatically. The inability to detect attacks on these devices are due to (1) the lack of computational power for detecting attacks, (2) the lack of interfaces that could potentially indicate a compromise on this devices and (3) the lack of the ability to interact with the system to execute diagnostic tools. This warrants newer approaches such as Tier-1 Internet Service Provider level view of attack patterns to provide situational awareness of Cyber Security threats. We investigate and explore the event data generated by the Internet protocol Domain Name Systems (DNS) for the purpose of Cyber threat situational awareness. Traditional methods such as Static and Binary analysis of Malware are sometimes inadequate to address the proliferation of Malware due to the time taken to obtain and process the individual binaries in order to generate signatures. By the time the Anti-Malware signature is available, there is a chance that a significant amount of damage might have happened. The traditional Anti-Malware systems may not identify malicious activities. However, it may be detected faster through DNS protocol by analyzing the generated event data in a timely manner. As DNS was not designed with security in mind (or suffers from vulnerabilities), we explore how the vast amount of event data generated by these systems can be leveraged to create Cyber threat situational awareness. The main contributions of the book chapter are two-fold: (1). A scalable framework that can perform web scale analysis in near real-time that provide situational awareness. (2). Detect early warning signals before large scale attacks or malware propagation occurs. We employ deep learning approach to classify and correlate malicious events that are perceived from the protocol usage. To our knowledge this is the first time, a framework that can analyze and correlate the DNS usage information at continent scale or multiple Tier-1 Internet Service Provider scale has been studied and analyzed in real-time to provide situational awareness. Merely using a commodity hardware server, the developed framework is capable of analyzing more than 2 Million events per second and it could detect the malicious activities within them in near real-time. The developed framework can be scaled out to analyze even larger volumes of network event data by adding additional computing resources. The scalability and real-time detection of malicious activities from early warning signals makes the developed framework stand out from any system of similar kind.},
	language = {en},
	urldate = {2020-03-13},
	booktitle = {Big {Data} in {Engineering} {Applications}},
	publisher = {Springer},
	author = {Vinayakumar, R. and Poornachandran, Prabaharan and Soman, K. P.},
	editor = {Roy, Sanjiban Sekhar and Samui, Pijush and Deo, Ravinesh and Ntalampiras, Stavros},
	year = {2018},
	keywords = {Machine learning, Deep learning, Big data analytics, DNS log analysis},
	pages = {113--142},
	file = {Springer Full Text PDF:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/Q7Z3B88X/Vinayakumar et al. - 2018 - Scalable Framework for Cyber Threat Situational Aw.pdf:application/pdf}
}

@article{jolicoeur-martineau_gans_2018,
	title = {{GANs} beyond divergence minimization},

	abstract = {Generative adversarial networks (GANs) can be interpreted as an adversarial game between two players, a discriminator D and a generator G, in which D learns to classify real from fake data and G learns to generate realistic data by "fooling" D into thinking that fake data is actually real data. Currently, a dominating view is that G actually learns by minimizing a divergence given that the general objective function is a divergence when D is optimal. However, this view has been challenged due to inconsistencies between theory and practice. In this paper, we discuss of the properties associated with most loss functions for G (e.g., saturating/non-saturating f-GAN, LSGAN, WGAN, etc.). We show that these loss functions are not divergences and do not have the same equilibrium as expected of divergences. This suggests that G does not need to minimize the same objective function as D maximize, nor maximize the objective of D after swapping real data with fake data (non-saturating GAN) but can instead use a wide range of possible loss functions to learn to generate realistic data. We define GANs through two separate and independent D maximization and G minimization steps. We generalize the generator step to four new classes of loss functions, most of which are actual divergences (while traditional G loss functions are not). We test a wide variety of loss functions from these four classes on a synthetic dataset and on CIFAR-10. We observe that most loss functions converge well and provide comparable data generation quality to non-saturating GAN, LSGAN, and WGAN-GP generator loss functions, whether we use divergences or non-divergences. These results suggest that GANs do not conform well to the divergence minimization theory and form a much broader range of models than previously assumed.},
	urldate = {2020-03-16},
	journal = {arXiv:1809.02145 [cs, stat]},
	author = {Jolicoeur-Martineau, Alexia},
	month = sep,
	year = {2018},
	note = {arXiv: 1809.02145},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/VQ4QJQLR/Jolicoeur-Martineau - 2018 - GANs beyond divergence minimization.pdf:application/pdf;arXiv.org Snapshot:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/LKDRHHVS/1809.html:text/html}
}

@article{dong_towards_2019,
	title = {Towards a {Deeper} {Understanding} of {Adversarial} {Losses}},

	abstract = {Recent work has proposed various adversarial losses for training generative adversarial networks. Yet, it remains unclear what certain types of functions are valid adversarial loss functions, and how these loss functions perform against one another. In this paper, we aim to gain a deeper understanding of adversarial losses by decoupling the effects of their component functions and regularization terms. We ﬁrst derive some necessary and sufﬁcient conditions of the component functions such that the adversarial loss is a divergence-like measure between the data and the model distributions. In order to systematically compare different adversarial losses, we then propose DANTest—a new, simple framework based on discriminative adversarial networks. With this framework, we evaluate an extensive set of adversarial losses by combining different component functions and regularization approaches. This study leads to some new insights into the adversarial losses. For reproducibility, all source code is available at https: //github.com/salu133445/dan.},
	language = {en},
	urldate = {2020-03-16},
	journal = {arXiv:1901.08753 [cs, stat]},
	author = {Dong, Hao-Wen and Yang, Yi-Hsuan},
	month = jan,
	year = {2019},
	note = {arXiv: 1901.08753},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Dong and Yang - 2019 - Towards a Deeper Understanding of Adversarial Loss.pdf:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/3S2TWS6B/Dong and Yang - 2019 - Towards a Deeper Understanding of Adversarial Loss.pdf:application/pdf}
}

@article{arora_gans_2018,
	title = {{DO} {GANS} {LEARN} {THE} {DISTRIBUTION}? {SOME} {THEORY} {AND} {EMPIRICS}},
	abstract = {Do GANS (Generative Adversarial Nets) actually learn the target distribution? The foundational paper of Goodfellow et al. (2014) suggested they do, if they were given “sufﬁciently large” deep nets, sample size, and computation time. A recent theoretical analysis in Arora et al. (2017) raised doubts whether the same holds when discriminator has bounded size. It showed that the training objective can approach its optimum value even if the generated distribution has very low support —in other words, the training objective is unable to prevent mode collapse. The current paper makes two contributions. (1) It proposes a novel test for estimating support size using the birthday paradox of discrete probability. Using this evidence is presented that well-known GANs approaches do learn distributions of fairly low support. (2) It theoretically studies encoder-decoder GANs architectures (e.g., BiGAN/ALI), which were proposed to learn more meaningful features via GANs and (consequently) to also solve the mode-collapse issue. Our result shows that such encoder-decoder training objectives also cannot guarantee learning of the full distribution because they cannot prevent serious mode collapse. More seriously, they cannot prevent learning meaningless codes for data, contrary to usual intuition.},
	language = {en},
	author = {Arora, Sanjeev and Zhang, Yi and Risteski, Andrej},
	year = {2018},
	pages = {16},
	file = {Arora et al. - 2018 - DO GANS LEARN THE DISTRIBUTION SOME THEORY AND EM.pdf:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/UH8UUNTN/Arora et al. - 2018 - DO GANS LEARN THE DISTRIBUTION SOME THEORY AND EM.pdf:application/pdf}
}

@inproceedings{luo_distributed_2018,
	address = {Kansas City, MO},
	title = {Distributed {Anomaly} {Detection} {Using} {Autoencoder} {Neural} {Networks} in {WSN} for {IoT}},
	isbn = {978-1-5386-3180-5},
	abstract = {Wireless sensor networks (WSN) are fundamental to the Internet of Things (IoT) by bridging the gap between the physical and the cyber worlds. Anomaly detection is a critical task in this context as it is responsible for identifying various events of interests such as equipment faults and undiscovered phenomena. However, this task is challenging because of the elusive nature of anomalies and the volatility of the ambient environments. In a resource-scarce setting like WSN, this challenge is further elevated and weakens the suitability of many existing solutions. In this paper, for the ﬁrst time, we introduce autoencoder neural networks into WSN to solve the anomaly detection problem. We design a two-part algorithm that resides on sensors and the IoT cloud respectively, such that (i) anomalies can be detected at sensors in a fully distributed manner without the need for communicating with any other sensors or the cloud, and (ii) the relatively more computationintensive learning task can be handled by the cloud with a much lower (and conﬁgurable) frequency. In addition to the minimal communication overhead, the computational load on sensors is also very low (of polynomial complexity) and readily affordable by most COTS sensors. Using a real WSN indoor testbed and sensor data collected over 4 consecutive months, we demonstrate via experiments that our proposed autoencoderbased anomaly detection mechanism achieves high detection accuracy and low false alarm rate. It is also able to adapt to unforeseeable and new changes in a non-stationary environment, thanks to the unsupervised learning feature of our chosen autoencoder neural networks.},
	language = {en},
	urldate = {2020-04-16},
	booktitle = {2018 {IEEE} {International} {Conference} on {Communications} ({ICC})},
	publisher = {IEEE},
	author = {Luo, Tie and Nagarajan, Sai G.},
	month = may,
	year = {2018},
	pages = {1--6},
	file = {Luo and Nagarajan - 2018 - Distributed Anomaly Detection Using Autoencoder Ne.pdf:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/9XE5QQWB/Luo and Nagarajan - 2018 - Distributed Anomaly Detection Using Autoencoder Ne.pdf:application/pdf}
}

@article{di_mattia_survey_2019,
	title = {A {Survey} on {GANs} for {Anomaly} {Detection}},

	abstract = {Anomaly detection is a significant problem faced in several research areas. Detecting and correctly classifying something unseen as anomalous is a challenging problem that has been tackled in many different manners over the years. Generative Adversarial Networks (GANs) and the adversarial training process have been recently employed to face this task yielding remarkable results. In this paper we survey the principal GAN-based anomaly detection methods, highlighting their pros and cons. Our contributions are the empirical validation of the main GAN models for anomaly detection, the increase of the experimental results on different datasets and the public release of a complete Open Source toolbox for Anomaly Detection using GANs.},
	urldate = {2020-04-22},
	journal = {arXiv:1906.11632 [cs, stat]},
	author = {Di Mattia, Federico and Galeone, Paolo and De Simoni, Michele and Ghelfi, Emanuele},
	month = jun,
	year = {2019},
	note = {arXiv: 1906.11632},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/4MX4889C/Di Mattia et al. - 2019 - A Survey on GANs for Anomaly Detection.pdf:application/pdf;arXiv.org Snapshot:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/7H2CEAWK/1906.html:text/html}
}

@article{ferdowsi_generative_2019,
	title = {Generative {Adversarial} {Networks} for {Distributed} {Intrusion} {Detection} in the {Internet} of {Things}},

	abstract = {To reap the beneﬁts of the Internet of Things (IoT), it is imperative to secure the system against cyber attacks in order to enable mission critical and real-time applications. To this end, intrusion detection systems (IDSs) have been widely used to detect anomalies caused by a cyber attacker in IoT systems. However, due to the large-scale nature of the IoT, an IDS must operate in a distributed manner with minimum dependence on a central controller. Moreover, in many scenarios such as health and ﬁnancial applications, the datasets are private and IoTDs may not intend to share such data. To this end, in this paper, a distributed generative adversarial network (GAN) is proposed to provide a fully distributed IDS for the IoT so as to detect anomalous behavior without reliance on any centralized controller. In this architecture, every IoTD can monitor its own data as well as neighbor IoTDs to detect internal and external attacks. In addition, the proposed distributed IDS does not require sharing the datasets between the IoTDs, thus, it can be implemented in IoTs that preserve the privacy of user data such as health monitoring systems or ﬁnancial applications. It is shown analytically that the proposed distributed GAN has higher accuracy of detecting intrusion compared to a standalone IDS that has access to only a single IoTD dataset. Simulation results show that, the proposed distributed GAN-based IDS has up to 20\% higher accuracy, 25\% higher precision, and 60\% lower false positive rate compared to a standalone GAN-based IDS.},
	language = {en},
	urldate = {2020-04-27},
	journal = {arXiv:1906.00567 [cs, stat]},
	author = {Ferdowsi, Aidin and Saad, Walid},
	month = jun,
	year = {2019},
	note = {arXiv: 1906.00567},
	keywords = {Computer Science - Machine Learning, Computer Science - Networking and Internet Architecture, Statistics - Machine Learning, Computer Science - Cryptography and Security},
	file = {Ferdowsi and Saad - 2019 - Generative Adversarial Networks for Distributed In.pdf:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/WWD3L9PB/Ferdowsi and Saad - 2019 - Generative Adversarial Networks for Distributed In.pdf:application/pdf}
}

@inproceedings{hoang_mgan_2018,
title = "MGAN: training generative adversarial nets with multiple generators",
abstract = "We propose in this paper a new approach to train the Generative Adversarial Nets (GANs) with a mixture of generators to overcome the mode collapsing problem. The main intuition is to employ multiple generators, instead of using a single one as in the original GAN. The idea is simple, yet proven to be extremely effective at covering diverse data modes, easily overcoming the mode collapsing problem and delivering state-of-the-art results. A minimax formulation was able to establish among a classifier, a discriminator, and a set of generators in a similar spirit with GAN. Generators create samples that are intended to come from the same distribution as the training data, whilst the discriminator determines whether samples are true data or generated by generators, and the classifier specifies which generator a sample comes from. The distinguishing feature is that internal samples are created from multiple generators, and then one of them will be randomly selected as final output similar to the mechanism of a probabilistic mixture model. We term our method Mixture Generative Adversarial Nets (MGAN). We develop theoretical analysis to prove that, at the equilibrium, the Jensen-Shannon divergence (JSD) between the mixture of generators{\textquoteright} distributions and the empirical data distribution is minimal, whilst the JSD among generators{\textquoteright} distributions is maximal, hence effectively avoiding the mode collapsing problem. By utilizing parameter sharing, our proposed model adds minimal computational cost to the standard GAN, and thus can also efficiently scale to large-scale datasets. We conduct extensive experiments on synthetic 2D data and natural image databases (CIFAR-10, STL-10 and ImageNet) to demonstrate the superior performance of our MGAN in achieving state-of-the-art Inception scores over latest baselines, generating diverse and appealing recognizable objects at different resolutions, and specializing in capturing different types of objects by the generators.",
keywords = "GANs, Mode Collapse, Mixture, Jensen-Shannon Divergence, Inception Score, Generator, Discriminator, CIFAR-10, STL-10, ImageNet",
author = "Quan Hoang and Nguyen, {Tu Dinh} and Trung Le and Dinh Phung",
year = "2018",
language = "English",
editor = "Murray, {Iain } and Ranzato, {Marc{\textquoteright}Aurelio } and Vinyals, {Oriol }",
booktitle = "6th International Conference on Learning Representations, ICLR 2018",
publisher = "OpenReview"
}


@article{yonetani_decentralized_2019,
	title = {Decentralized {Learning} of {Generative} {Adversarial} {Networks} from {Non}-iid {Data}},

	abstract = {This work addresses a new problem that learns generative adversarial networks (GANs) from multiple data collections that are each i) owned separately by different clients and ii) drawn from a non-identical distribution that comprises different classes. Given such non-iid data as input, we aim to learn a distribution involving all the classes input data can belong to, while keeping the data decentralized in each client storage. Our key contribution to this end is a new decentralized approach for learning GANs from non-iid data called Forgiver-First Update (F2U), which a) asks clients to train an individual discriminator with their own data and b) updates a generator to fool the most ‘forgiving’ discriminators who deem generated samples as the most real. Our theoretical analysis proves that this updating strategy allows the decentralized GAN to achieve a generator’s distribution with all the input classes as its global optimum based on f-divergence minimization. Moreover, we propose a relaxed version of F2U called Forgiver-First Aggregation (F2A) that performs well in practice, which adaptively aggregates the discriminators while emphasizing forgiving ones. Our empirical evaluations with image generation tasks demonstrated the effectiveness of our approach over state-of-the-art decentralized learning methods.},
	language = {en},
	urldate = {2020-04-28},
	journal = {arXiv:1905.09684 [cs, stat]},
	author = {Yonetani, Ryo and Takahashi, Tomohiro and Hashimoto, Atsushi and Ushiku, Yoshitaka},
	month = nov,
	year = {2019},
	note = {arXiv: 1905.09684},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Yonetani et al. - 2019 - Decentralized Learning of Generative Adversarial N.pdf:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/GPVB5VJW/Yonetani et al. - 2019 - Decentralized Learning of Generative Adversarial N.pdf:application/pdf}
}

@article{im_generative_2016,
	title = {Generative {Adversarial} {Parallelization}},

	abstract = {Generative Adversarial Networks have become one of the most studied frameworks for unsupervised learning due to their intuitive formulation. They have also been shown to be capable of generating convincing examples in limited domains, such as low-resolution images. However, they still prove difficult to train in practice and tend to ignore modes of the data generating distribution. Quantitatively capturing effects such as mode coverage and more generally the quality of the generative model still remain elusive. We propose Generative Adversarial Parallelization, a framework in which many GANs or their variants are trained simultaneously, exchanging their discriminators. This eliminates the tight coupling between a generator and discriminator, leading to improved convergence and improved coverage of modes. We also propose an improved variant of the recently proposed Generative Adversarial Metric and show how it can score individual GANs or their collections under the GAP model.},
	urldate = {2020-04-28},
	journal = {arXiv:1612.04021 [cs, stat]},
	author = {Im, Daniel Jiwoong and Ma, He and Kim, Chris Dongjoo and Taylor, Graham},
	month = dec,
	year = {2016},
	note = {arXiv: 1612.04021},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/C84NQ846/Im et al. - 2016 - Generative Adversarial Parallelization.pdf:application/pdf;arXiv.org Snapshot:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/SFIJBRFG/1612.html:text/html}
}

@article{hoang_multi-generator_2017,
	title = {Multi-{Generator} {Generative} {Adversarial} {Nets}},

	abstract = {We propose in this paper a new approach to train the Generative Adversarial Nets (GANs) with a mixture of generators to overcome the mode collapsing problem. The main intuition is to employ multiple generators, instead of using a single one as in the original GAN. The idea is simple, yet proven to be extremely effective at covering diverse data modes, easily overcoming the mode collapsing problem and delivering state-of-the-art results. A minimax formulation was able to establish among a classiﬁer, a discriminator, and a set of generators in a similar spirit with GAN. Generators create samples that are intended to come from the same distribution as the training data, whilst the discriminator determines whether samples are true data or generated by generators, and the classiﬁer speciﬁes which generator a sample comes from. The distinguishing feature is that internal samples are created from multiple generators, and then one of them will be randomly selected as ﬁnal output similar to the mechanism of a probabilistic mixture model. We term our method Mixture Generative Adversarial Nets (MGAN). We develop theoretical analysis to prove that, at the equilibrium, the Jensen-Shannon divergence (JSD) between the mixture of generators’ distributions and the empirical data distribution is minimal, whilst the JSD among generators’ distributions is maximal, hence effectively avoiding the mode collapsing problem. By utilizing parameter sharing, our proposed model adds minimal computational cost to the standard GAN, and thus can also efﬁciently scale to large-scale datasets. We conduct extensive experiments on synthetic 2D data and natural image databases (CIFAR-10, STL-10 and ImageNet) to demonstrate the superior performance of our MGAN in achieving state-of-the-art Inception scores over latest baselines, generating diverse and appealing recognizable objects at different resolutions, and specializing in capturing different types of objects by the generators.},
	language = {en},
	urldate = {2020-04-28},
	journal = {arXiv:1708.02556 [cs, stat]},
	author = {Hoang, Quan and Nguyen, Tu Dinh and Le, Trung and Phung, Dinh},
	month = oct,
	year = {2017},
	note = {arXiv: 1708.02556},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence},
	file = {Hoang et al. - 2017 - Multi-Generator Generative Adversarial Nets.pdf:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/RHTSHL9C/Hoang et al. - 2017 - Multi-Generator Generative Adversarial Nets.pdf:application/pdf}
}

@article{durugkar_generative_2017,
	title = {Generative {Multi}-{Adversarial} {Networks}},

	abstract = {Generative adversarial networks (GANs) are a framework for producing a generative model by way of a two-player minimax game. In this paper, we propose the {\textbackslash}emph\{Generative Multi-Adversarial Network\} (GMAN), a framework that extends GANs to multiple discriminators. In previous work, the successful training of GANs requires modifying the minimax objective to accelerate training early on. In contrast, GMAN can be reliably trained with the original, untampered objective. We explore a number of design perspectives with the discriminator role ranging from formidable adversary to forgiving teacher. Image generation tasks comparing the proposed framework to standard GANs demonstrate GMAN produces higher quality samples in a fraction of the iterations when measured by a pairwise GAM-type metric.},
	urldate = {2020-04-28},
	journal = {arXiv:1611.01673 [cs]},
	author = {Durugkar, Ishan and Gemp, Ian and Mahadevan, Sridhar},
	month = mar,
	year = {2017},
	note = {arXiv: 1611.01673},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Computer Science - Multiagent Systems},
	file = {arXiv Fulltext PDF:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/XYI925ZE/Durugkar et al. - 2017 - Generative Multi-Adversarial Networks.pdf:application/pdf;arXiv.org Snapshot:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/4FBIJHS6/1611.html:text/html}
}

@article{liu_generative_2019,
	title = {Generative {Adversarial} {Active} {Learning} for {Unsupervised} {Outlier} {Detection}},
	issn = {1558-2191},
	abstract = {Outlier detection is an important topic in machine learning and has been used in a wide range of applications. In this paper, we approach outlier detection as a binary-classification issue by sampling potential outliers from a uniform reference distribution. However, due to the sparsity of data in high-dimensional space, a limited number of potential outliers may fail to provide sufficient information to assist the classifier in describing a boundary that can separate outliers from normal data effectively. To address this, we propose a novel Single-Objective Generative Adversarial Active Learning (SO-GAAL) method for outlier detection, which can directly generate informative potential outliers based on the mini-max game between a generator and a discriminator. Moreover, to prevent the generator from falling into the mode collapsing problem, the stop node of training should be determined when SO-GAAL is able to provide sufficient information. But without any prior information, it is extremely difficult for SO-GAAL. Therefore, we expand the network structure of SO-GAAL from a single generator to multiple generators with different objectives (MO-GAAL), which can generate a reasonable reference distribution for the whole dataset. We empirically compare the proposed approach with several state-of-the-art outlier detection methods on both synthetic and real-world datasets. The results show that MO-GAAL outperforms its competitors in the majority of cases, especially for datasets with various cluster types or high irrelevant variable ratio. The experiment codes are available at: https://github.com/leibinghe/GAAL-based-outlier-detection},
	journal = {IEEE Transactions on Knowledge and Data Engineering},
	author = {Liu, Yezheng and Li, Zhe and Zhou, Chong and Jiang, Yuanchun and Sun, Jianshan and Wang, Meng and He, Xiangnan},
	year = {2019},
	note = {Conference Name: IEEE Transactions on Knowledge and Data Engineering},
	keywords = {Anomaly detection, Training, Computational modeling, Curse of Dimensionality, Data models, Gallium nitride, Generate Potential Outliers, Generative Adversarial Active Learning, Generative adversarial networks, Generators, Mode Collapsing Problem, Multiple-Objective Generative Adversarial Active Learning, Outlier Detection},
	pages = {1--1},
	file = {IEEE Xplore Abstract Record:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/YFRM2XFD/8668550.html:text/html;IEEE Xplore Full Text PDF:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/KTFL3ABS/Liu et al. - 2019 - Generative Adversarial Active Learning for Unsuper.pdf:application/pdf}
}



@inproceedings{xiang_incremental_2019,
	address = {Seoul, Korea (South)},
	title = {Incremental {Learning} {Using} {Conditional} {Adversarial} {Networks}},
	isbn = {978-1-72814-803-8},
	abstract = {Incremental learning using Deep Neural Networks (DNNs) suffers from catastrophic forgetting. Existing methods mitigate it by either storing old image examples or only updating a few fully connected layers of DNNs, which, however, requires large memory footprints or hurts the plasticity of models. In this paper, we propose a new incremental learning strategy based on conditional adversarial networks. Our new strategy allows us to use memory-efﬁcient statistical information to store old knowledge, and ﬁne-tune both convolutional layers and fully connected layers to consolidate new knowledge. Speciﬁcally, we propose a model consisting of three parts, i.e., a base sub-net, a generator, and a discriminator. The base sub-net works as a feature extractor which can be pre-trained on large scale datasets and shared across multiple image recognition tasks. The generator conditioned on labeled embeddings aims to construct pseudo-examples with the same distribution as the old data. The discriminator combines real-examples from new data and pseudo-examples generated from the old data distribution to learn representation for both old and new classes. Through adversarial training of the discriminator and generator, we accomplish the multiple continuous incremental learning. Comparison with the state-of-the-arts on public CIFAR-100 and CUB-200 datasets shows that our method achieves the best accuracies on both old and new classes while requiring relatively less memory storage.},
	language = {en},
	urldate = {2021-01-30},
	booktitle = {2019 {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	publisher = {IEEE},
	author = {Xiang, Ye and Fu, Ying and Ji, Pan and Huang, Hua},
	month = oct,
	year = {2019},
	pages = {6618--6627},
	file = {Xiang et al. - 2019 - Incremental Learning Using Conditional Adversarial.pdf:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/8QRKFXJJ/Xiang et al. - 2019 - Incremental Learning Using Conditional Adversarial.pdf:application/pdf}
}


@article{li_anomaly_2019,
	title = {Anomaly {Detection} with {Generative} {Adversarial} {Networks} for {Multivariate} {Time} {Series}},

	abstract = {Today's Cyber-Physical Systems (CPSs) are large, complex, and affixed with networked sensors and actuators that are targets for cyber-attacks. Conventional detection techniques are unable to deal with the increasingly dynamic and complex nature of the CPSs. On the other hand, the networked sensors and actuators generate large amounts of data streams that can be continuously monitored for intrusion events. Unsupervised machine learning techniques can be used to model the system behaviour and classify deviant behaviours as possible attacks. In this work, we proposed a novel Generative Adversarial Networks-based Anomaly Detection (GAN-AD) method for such complex networked CPSs. We used LSTM-RNN in our GAN to capture the distribution of the multivariate time series of the sensors and actuators under normal working conditions of a CPS. Instead of treating each sensor's and actuator's time series independently, we model the time series of multiple sensors and actuators in the CPS concurrently to take into account of potential latent interactions between them. To exploit both the generator and the discriminator of our GAN, we deployed the GAN-trained discriminator together with the residuals between generator-reconstructed data and the actual samples to detect possible anomalies in the complex CPS. We used our GAN-AD to distinguish abnormal attacked situations from normal working conditions for a complex six-stage Secure Water Treatment (SWaT) system. Experimental results showed that the proposed strategy is effective in identifying anomalies caused by various attacks with high detection rate and low false positive rate as compared to existing methods.},
	urldate = {2020-05-01},
	journal = {arXiv:1809.04758 [cs, stat]},
	author = {Li, Dan and Chen, Dacheng and Goh, Jonathan and Ng, See-kiong},
	month = jan,
	year = {2019},
	note = {arXiv: 1809.04758},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/4XQAZR86/Li et al. - 2019 - Anomaly Detection with Generative Adversarial Netw.pdf:application/pdf;arXiv.org Snapshot:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/WC7FW2AH/1809.html:text/html}
}

@article{moustafa_evaluation_2016,
	title = {The evaluation of {Network} {Anomaly} {Detection} {Systems}: {Statistical} analysis of the {UNSW}-{NB15} data set and the comparison with the {KDD99} data set},
	volume = {25},
	issn = {1939-3555, 1939-3547},
	shorttitle = {The evaluation of {Network} {Anomaly} {Detection} {Systems}},
	abstract = {Over the last three decades, Network Intrusion Detection Systems (NIDSs), particularly, Anomaly Detection Systems (ADSs), have become more significant in detecting novel attacks than Signature Detection Systems (SDSs). Evaluating NIDSs using the existing benchmark data sets of KDD99 and NSLKDD does not reflect satisfactory results, due to three major issues: (1) their lack of modern low footprint attack styles, (2) their lack of modern normal traffic scenarios, and (3) a different distribution of training and testing sets. To address these issues, the UNSW-NB15 data set has recently been generated. This data set has nine types of the modern attacks fashions and new patterns of normal traffic, and it contains 49 attributes that comprise the flow based between hosts and the network packets inspection to discriminate between the observations, either normal or abnormal. In this paper, we demonstrate the complexity of the UNSW-NB15 data set in three aspects. First, the statistical analysis of the observations and the attributes are explained. Second, the examination of feature correlations is provided. Third, five existing classifiers are used to evaluate the complexity in terms of accuracy and false alarm rates (FARs) and then, the results are compared with the KDD99 data set. The experimental results show that UNSW-NB15 is more complex than KDD99 and is considered as a new benchmark data set for evaluating NIDSs.},
	language = {en},
	number = {1-3},
	urldate = {2020-05-04},
	journal = {Information Security Journal: A Global Perspective},
	author = {Moustafa, Nour and Slay, Jill},
	month = apr,
	year = {2016},
	pages = {18--31},
	file = {Moustafa and Slay - 2016 - The evaluation of Network Anomaly Detection System.pdf:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/EA3E6MUV/Moustafa and Slay - 2016 - The evaluation of Network Anomaly Detection System.pdf:application/pdf}
}

@article{li_alice_2017,
	title = {{ALICE}: {Towards} {Understanding} {Adversarial} {Learning} for {Joint} {Distribution} {Matching}},
	shorttitle = {{ALICE}},

	abstract = {We investigate the non-identifiability issues associated with bidirectional adversarial training for joint distribution matching. Within a framework of conditional entropy, we propose both adversarial and non-adversarial approaches to learn desirable matched joint distributions for unsupervised and supervised tasks. We unify a broad family of adversarial models as joint distribution matching problems. Our approach stabilizes learning of unsupervised bidirectional adversarial learning methods. Further, we introduce an extension for semi-supervised learning tasks. Theoretical results are validated in synthetic data and real-world applications.},
	urldate = {2020-05-12},
	journal = {arXiv:1709.01215 [cs, stat]},
	author = {Li, Chunyuan and Liu, Hao and Chen, Changyou and Pu, Yunchen and Chen, Liqun and Henao, Ricardo and Carin, Lawrence},
	month = nov,
	year = {2017},
	note = {arXiv: 1709.01215},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Neural and Evolutionary Computing, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/DGB7QMW5/Li et al. - 2017 - ALICE Towards Understanding Adversarial Learning .pdf:application/pdf;arXiv.org Snapshot:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/IR22CURL/1709.html:text/html}
}

@incollection{yoon_time-series_2019,
	title = {Time-series {Generative} {Adversarial} {Networks}},

	urldate = {2020-06-05},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 32},
	publisher = {Curran Associates, Inc.},
	author = {Yoon, Jinsung and Jarrett, Daniel and van der Schaar, Mihaela},
	editor = {Wallach, H. and Larochelle, H. and Beygelzimer, A. and Alché-Buc, F. d{\textbackslash}textquotesingle and Fox, E. and Garnett, R.},
	year = {2019},
	pages = {5508--5518},
	file = {NIPS Full Text PDF:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/VCA2XBSW/Yoon et al. - 2019 - Time-series Generative Adversarial Networks.pdf:application/pdf;NIPS Snapshot:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/I3BBC3IA/8789-time-series-generative-adversarial-networks.html:text/html}
}

@article{li_adversarial_nodate,
	title = {Adversarial {Discrete} {Sequence} {Generation} without {Explicit} {Neural} {Networks} as {Discriminators}},
	abstract = {This paper presents a novel approach to train GANs for discrete sequence generation without resorting to an explicit neural network as the discriminator. We show that when an alternative mini-max optimization procedure is performed for the value function where a closed form solution for the discriminator exists in the maximization step, it is equivalent to directly optimizing the Jensen-Shannon divergence (JSD) between the generator’s distribution and the empirical distribution over the training data without sampling from the generator, thus optimizing the JSD becomes computationally tractable to train the generator that generates sequences of discrete data. Extensive experiments on synthetic data and real-world tasks demonstrate signiﬁcant improvements over existing methods to train GANs that generate discrete sequences.},
	language = {en},
	author = {Li, Zhongliang and Xia, Tian and Lou, Xingyu and Xu, Kaihe and Wang, Shaojun and Xiao, Jing},
	pages = {10},
	file = {Li et al. - Adversarial Discrete Sequence Generation without E.pdf:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/XPAFHCPU/Li et al. - Adversarial Discrete Sequence Generation without E.pdf:application/pdf}
}

@article{creswell_inverting_2018,
	title = {Inverting {The} {Generator} {Of} {A} {Generative} {Adversarial} {Network} ({II})},

	abstract = {Generative adversarial networks (GANs) learn a deep generative model that is able to synthesise novel, high-dimensional data samples. New data samples are synthesised by passing latent samples, drawn from a chosen prior distribution, through the generative model. Once trained, the latent space exhibits interesting properties, that may be useful for down stream tasks such as classification or retrieval. Unfortunately, GANs do not offer an "inverse model", a mapping from data space back to latent space, making it difficult to infer a latent representation for a given data sample. In this paper, we introduce a technique, inversion, to project data samples, specifically images, to the latent space using a pre-trained GAN. Using our proposed inversion technique, we are able to identify which attributes of a dataset a trained GAN is able to model and quantify GAN performance, based on a reconstruction loss. We demonstrate how our proposed inversion technique may be used to quantitatively compare performance of various GAN models trained on three image datasets. We provide code for all of our experiments, https://github.com/ToniCreswell/InvertingGAN.},
	urldate = {2020-06-15},
	journal = {arXiv:1802.05701 [cs]},
	author = {Creswell, Antonia and Bharath, Anil A.},
	month = feb,
	year = {2018},
	note = {arXiv: 1802.05701},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/NFHHSFPT/Creswell and Bharath - 2018 - Inverting The Generator Of A Generative Adversaria.pdf:application/pdf;arXiv.org Snapshot:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/WB96AVZ5/1802.html:text/html}
}

@article{zhao_pyod_2019,
	title = {{PyOD}: {A} {Python} {Toolbox} for {Scalable} {Outlier} {Detection}},
	volume = {20},

	abstract = {PyOD is an open-source Python toolbox for performing scalable outlier detection on multivariate data. Uniquely, it provides access to a wide range of outlier detection algorithms, including established outlier ensembles and more recent neural network-based approaches, under a single, well-documented API designed for use by both practitioners and researchers. With robustness and scalability in mind, best practices such as unit testing, continuous integration, code coverage, maintainability checks, interactive examples and parallelization are emphasized as core components in the toolbox’s development. PyOD is compatible with both Python 2 and 3 and can be installed through Python Package Index (PyPI) or https://github.com/yzhao062/pyod.},
	language = {en},
	author = {Zhao, Yue and Nasrullah, Zain and Li, Zheng},
	year = {2019},
	pages = {7},
	file = {Zhao et al. - PyOD A Python Toolbox for Scalable Outlier Detect.pdf:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/GPKN34MI/Zhao et al. - PyOD A Python Toolbox for Scalable Outlier Detect.pdf:application/pdf}
}

@article{franci_game-theoretic_2020,
	title = {A game-theoretic approach for {Generative} {Adversarial} {Networks}},

	abstract = {Generative adversarial networks (GANs) are a class of generative models, known for producing accurate samples. The key feature of GANs is that there are two antagonistic neural networks: the generator and the discriminator. The main bottleneck for their implementation is that the neural networks are very hard to train. One way to improve their performance is to design reliable algorithms for the adversarial process. Since the training can be cast as a stochastic Nash equilibrium problem, we rewrite it as a variational inequality and introduce an algorithm to compute an approximate solution. Specifically, we propose a stochastic relaxed forward-backward algorithm for GANs. We prove that when the pseudogradient mapping of the game is monotone, we have convergence to an exact solution or in a neighbourhood of it.},
	urldate = {2020-06-18},
	journal = {arXiv:2003.13637 [cs, math, stat]},
	author = {Franci, Barbara and Grammatico, Sergio},
	month = mar,
	year = {2020},
	note = {arXiv: 2003.13637},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computer Science and Game Theory, Mathematics - Optimization and Control},
	file = {arXiv Fulltext PDF:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/MVG5PN6Z/Franci and Grammatico - 2020 - A game-theoretic approach for Generative Adversari.pdf:application/pdf;arXiv.org Snapshot:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/6NTUAHZU/2003.html:text/html}
}

@article{wang_ensembles_2016,
	title = {Ensembles of {Generative} {Adversarial} {Networks}},

	abstract = {Ensembles are a popular way to improve results of discriminative CNNs. The combination of several networks trained starting from different initializations improves results significantly. In this paper we investigate the usage of ensembles of GANs. The specific nature of GANs opens up several new ways to construct ensembles. The first one is based on the fact that in the minimax game which is played to optimize the GAN objective the generator network keeps on changing even after the network can be considered optimal. As such ensembles of GANs can be constructed based on the same network initialization but just taking models which have different amount of iterations. These so-called self ensembles are much faster to train than traditional ensembles. The second method, called cascade GANs, redirects part of the training data which is badly modeled by the first GAN to another GAN. In experiments on the CIFAR10 dataset we show that ensembles of GANs obtain model probability distributions which better model the data distribution. In addition, we show that these improved results can be obtained at little additional computational cost.},
	urldate = {2020-06-20},
	journal = {arXiv:1612.00991 [cs]},
	author = {Wang, Yaxing and Zhang, Lichao and van de Weijer, Joost},
	month = dec,
	year = {2016},
	note = {arXiv: 1612.00991},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/TZZF5NMS/Wang et al. - 2016 - Ensembles of Generative Adversarial Networks.pdf:application/pdf;arXiv.org Snapshot:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/73EVTCS5/1612.html:text/html}
}

@article{zhu_unpaired_2018,
	title = {Unpaired {Image}-to-{Image} {Translation} using {Cycle}-{Consistent} {Adversarial} {Networks}},

	abstract = {Image-to-image translation is a class of vision and graphics problems where the goal is to learn the mapping between an input image and an output image using a training set of aligned image pairs. However, for many tasks, paired training data will not be available. We present an approach for learning to translate an image from a source domain \$X\$ to a target domain \$Y\$ in the absence of paired examples. Our goal is to learn a mapping \$G: X {\textbackslash}rightarrow Y\$ such that the distribution of images from \$G(X)\$ is indistinguishable from the distribution \$Y\$ using an adversarial loss. Because this mapping is highly under-constrained, we couple it with an inverse mapping \$F: Y {\textbackslash}rightarrow X\$ and introduce a cycle consistency loss to push \$F(G(X)) {\textbackslash}approx X\$ (and vice versa). Qualitative results are presented on several tasks where paired training data does not exist, including collection style transfer, object transfiguration, season transfer, photo enhancement, etc. Quantitative comparisons against several prior methods demonstrate the superiority of our approach.},
	urldate = {2020-06-24},
	journal = {arXiv:1703.10593 [cs]},
	author = {Zhu, Jun-Yan and Park, Taesung and Isola, Phillip and Efros, Alexei A.},
	month = nov,
	year = {2018},
	note = {arXiv: 1703.10593},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/PAIT34E8/Zhu et al. - 2018 - Unpaired Image-to-Image Translation using Cycle-Co.pdf:application/pdf;arXiv.org Snapshot:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/L73F88SJ/1703.html:text/html}
}

@article{lucic_are_2018,
	title = {Are {GANs} {Created} {Equal}? {A} {Large}-{Scale} {Study}},
	shorttitle = {Are {GANs} {Created} {Equal}?},

	abstract = {Generative adversarial networks (GAN) are a powerful subclass of generative models. Despite a very rich research activity leading to numerous interesting GAN algorithms, it is still very hard to assess which algorithm(s) perform better than others. We conduct a neutral, multi-faceted large-scale empirical study on state-of-the art models and evaluation measures. We find that most models can reach similar scores with enough hyperparameter optimization and random restarts. This suggests that improvements can arise from a higher computational budget and tuning more than fundamental algorithmic changes. To overcome some limitations of the current metrics, we also propose several data sets on which precision and recall can be computed. Our experimental results suggest that future GAN research should be based on more systematic and objective evaluation procedures. Finally, we did not find evidence that any of the tested algorithms consistently outperforms the non-saturating GAN introduced in {\textbackslash}cite\{goodfellow2014generative\}.},
	urldate = {2020-06-24},
	journal = {arXiv:1711.10337 [cs, stat]},
	author = {Lucic, Mario and Kurach, Karol and Michalski, Marcin and Gelly, Sylvain and Bousquet, Olivier},
	month = oct,
	year = {2018},
	note = {arXiv: 1711.10337},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/PPW2DS6G/Lucic et al. - 2018 - Are GANs Created Equal A Large-Scale Study.pdf:application/pdf;arXiv.org Snapshot:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/DG7CGKQH/1711.html:text/html}
}

@inproceedings{lazarevic_theoretically_2009,
	address = {Miami, FL, USA},
	title = {Theoretically {Optimal} {Distributed} {Anomaly} {Detection}},
	isbn = {978-1-4244-5384-9},
	abstract = {A novel general framework for distributed anomaly detection with theoretical performance guarantees is proposed. Our algorithmic approach combines existing anomaly detection procedures with a novel method for computing global statistics using local sufﬁcient statistics. Under a Gaussian assumption, our distributed algorithm is guaranteed to perform as well as its centralized counterpart, a condition we call ‘zero information loss’. We further report experimental results on synthetic as well as real-world data to demonstrate the viability of our approach.},
	language = {en},
	urldate = {2020-06-26},
	booktitle = {2009 {IEEE} {International} {Conference} on {Data} {Mining} {Workshops}},
	publisher = {IEEE},
	author = {Lazarevic, Aleksandar and Srivastava, Nisheeth and Tiwari, Ashutosh and Isom, Josh and Oza, Nikunj and Srivastava, Jaideep},
	month = dec,
	year = {2009},
	pages = {515--520},
	file = {Lazarevic et al. - 2009 - Theoretically Optimal Distributed Anomaly Detectio.pdf:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/TKX8Y8CG/Lazarevic et al. - 2009 - Theoretically Optimal Distributed Anomaly Detectio.pdf:application/pdf}
}

@article{mescheder_which_2018,
	title = {Which {Training} {Methods} for {GANs} do actually {Converge}?},

	abstract = {Recent work has shown local convergence of GAN training for absolutely continuous data and generator distributions. In this paper, we show that the requirement of absolute continuity is necessary: we describe a simple yet prototypical counterexample showing that in the more realistic case of distributions that are not absolutely continuous, unregularized GAN training is not always convergent. Furthermore, we discuss regularization strategies that were recently proposed to stabilize GAN training. Our analysis shows that GAN training with instance noise or zero-centered gradient penalties converges. On the other hand, we show that Wasserstein-GANs and WGAN-GP with a finite number of discriminator updates per generator update do not always converge to the equilibrium point. We discuss these results, leading us to a new explanation for the stability problems of GAN training. Based on our analysis, we extend our convergence results to more general GANs and prove local convergence for simplified gradient penalties even if the generator and data distribution lie on lower dimensional manifolds. We find these penalties to work well in practice and use them to learn high-resolution generative image models for a variety of datasets with little hyperparameter tuning.},
	urldate = {2020-06-26},
	journal = {arXiv:1801.04406 [cs]},
	author = {Mescheder, Lars and Geiger, Andreas and Nowozin, Sebastian},
	month = jul,
	year = {2018},
	note = {arXiv: 1801.04406},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computer Science and Game Theory},
	file = {arXiv Fulltext PDF:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/GUQEM4YV/Mescheder et al. - 2018 - Which Training Methods for GANs do actually Conver.pdf:application/pdf;arXiv.org Snapshot:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/Z5P2T68R/1801.html:text/html}
}

@article{lee_anomaly_2013,
	title = {Anomaly {Detection} via {Online} {Oversampling} {Principal} {Component} {Analysis}},
	volume = {25},
	issn = {1041-4347},
	abstract = {Anomaly detection has been an important research topic in data mining and machine learning. Many real-world applications such as intrusion or credit card fraud detection require an effective and efﬁcient framework to identify deviated data instances. However, most anomaly detection methods are typically implemented in batch mode, and thus cannot be easily extended to large-scale problems without sacriﬁcing computation and memory requirements. In this paper, we propose an online over-sampling principal component analysis (osPCA) algorithm to address this problem, and we aim at detecting the presence of outliers from a large amount of data via an online updating technique. Unlike prior PCA based approaches, we do not store the entire data matrix or covariance matrix, and thus our approach is especially of interest in online or large-scale problems. By over-sampling the target instance and extracting the principal direction of the data, the proposed osPCA allows us to determine the anomaly of the target instance according to the variation of the resulting dominant eigenvector. Since our osPCA need not perform eigen analysis explicitly, the proposed framework is favored for online applications which have computation or memory limitations. Compared with the well-known power method for PCA and other popular anomaly detection algorithms, our experimental results verify the feasibility of our proposed method in terms of both accuracy and efﬁciency.},
	language = {en},
	number = {7},
	urldate = {2020-07-03},
	journal = {IEEE Transactions on Knowledge and Data Engineering},
	author = {Lee, Yuh-Jye and Yeh, Yi-Ren and Wang, Yu-Chiang Frank},
	month = jul,
	year = {2013},
	pages = {1460--1470},
	file = {Lee et al. - 2013 - Anomaly Detection via Online Oversampling Principa.pdf:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/H7TQRMW6/Lee et al. - 2013 - Anomaly Detection via Online Oversampling Principa.pdf:application/pdf}
}

@article{alpcan_template_2019,
	title = {A {Template} and {Suggestions} for {Writing} {Easy}-to-{Read} {Research} {Articles}},

	abstract = {The number of research papers written has been growing at least linearly -- if not exponentially -- in recent years. In proportion, the amount of time a reader allocates per paper has been decreasing. While an accessible paper will be appreciated by a large audience, hard-to-read papers may remain obscure for a long time regardless of scientific merit. Unfortunately, there is still insufficient emphasis on good written and oral communication skills in technical disciplines, especially in engineering. As an academic, I have realised over the years that I keep telling my students the same things over and over again when they write papers, reports, presentations, and theses. This article contains some of those suggestions and serves as a limited template for organising research articles. I have adopted a very practical and personal approach and don't claim that this is a formal contribution to the scientific communication literature. However, I hope that this article will not only make my life a bit easier but also help other graduate students and academic supervisors.},
	urldate = {2020-07-05},
	journal = {arXiv:1907.12204 [cs]},
	author = {Alpcan, Tansu},
	month = jul,
	year = {2019},
	note = {arXiv: 1907.12204},
	keywords = {Computer Science - General Literature},
	file = {arXiv Fulltext PDF:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/UJZSZKFT/Alpcan - 2019 - A Template and Suggestions for Writing Easy-to-Rea.pdf:application/pdf;arXiv.org Snapshot:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/YG99EIZG/1907.html:text/html}
}

@article{bhuyan_network_2014,
	title = {Network {Anomaly} {Detection}: {Methods}, {Systems} and {Tools}},
	volume = {16},
	issn = {1553-877X},
	shorttitle = {Network {Anomaly} {Detection}},
	abstract = {Network anomaly detection is an important and dynamic research area. Many network intrusion detection methods and systems (NIDS) have been proposed in the literature. In this paper, we provide a structured and comprehensive overview of various facets of network anomaly detection so that a researcher can become quickly familiar with every aspect of network anomaly detection. We present attacks normally encountered by network intrusion detection systems. We categorize existing network anomaly detection methods and systems based on the underlying computational techniques used. Within this framework, we brieﬂy describe and compare a large number of network anomaly detection methods and systems. In addition, we also discuss tools that can be used by network defenders and datasets that researchers in network anomaly detection can use. We also highlight research directions in network anomaly detection.},
	language = {en},
	number = {1},
	urldate = {2020-07-06},
	journal = {IEEE Communications Surveys \& Tutorials},
	author = {Bhuyan, Monowar H. and Bhattacharyya, D. K. and Kalita, J. K.},
	year = {2014},
	pages = {303--336},
	file = {Bhuyan et al. - 2014 - Network Anomaly Detection Methods, Systems and To.pdf:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/7CP7QWXC/Bhuyan et al. - 2014 - Network Anomaly Detection Methods, Systems and To.pdf:application/pdf}
}

@article{rajasegarar_ellipsoidal_2014,
	title = {Ellipsoidal neighbourhood outlier factor for distributed anomaly detection in resource constrained networks},
	volume = {47},
	issn = {00313203},
	abstract = {Anomaly detection in resource constrained wireless networks is an important challenge for tasks such as intrusion detection, quality assurance and event monitoring applications. The challenge is to detect these interesting events or anomalies in a timely manner, while minimising energy consumption in the network. We propose a distributed anomaly detection architecture, which uses multiple hyperellipsoidal clusters to model the data at each sensor node, and identify global and local anomalies in the network. In particular, a novel anomaly scoring method is proposed to provide a score for each hyperellipsoidal model, based on how remote the ellipsoid is relative to their neighbours. We demonstrate using several synthetic and real datasets that our proposed scheme achieves a higher detection performance with a signiﬁcant reduction in communication overhead in the network compared to centralised and existing schemes.},
	language = {en},
	number = {9},
	urldate = {2020-07-06},
	journal = {Pattern Recognition},
	author = {Rajasegarar, Sutharshan and Gluhak, Alexander and Ali Imran, Muhammad and Nati, Michele and Moshtaghi, Masud and Leckie, Christopher and Palaniswami, Marimuthu},
	month = sep,
	year = {2014},
	pages = {2867--2879},
	file = {Rajasegarar et al. - 2014 - Ellipsoidal neighbourhood outlier factor for distr.pdf:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/T36H2TYQ/Rajasegarar et al. - 2014 - Ellipsoidal neighbourhood outlier factor for distr.pdf:application/pdf}
}

@article{kwon_survey_2019,
author={Kwon, Donghwoon
and Kim, Hyunjoo
and Kim, Jinoh
and Suh, Sang C.
and Kim, Ikkyun
and Kim, Kuinam J.},
title={A survey of deep learning-based network anomaly detection},
journal={Cluster Computing},
year={2019},
month={Jan},
day={01},
volume={22},
number={1},
pages={949-961},
abstract={A great deal of attention has been given to deep learning over the past several years, and new deep learning techniques are emerging with improved functionality. Many computer and network applications actively utilize such deep learning algorithms and report enhanced performance through them. In this study, we present an overview of deep learning methodologies, including restricted Bolzmann machine-based deep belief network, deep neural network, and recurrent neural network, as well as the machine learning techniques relevant to network anomaly detection. In addition, this article introduces the latest work that employed deep learning techniques with the focus on network anomaly detection through the extensive literature survey. We also discuss our local experiments showing the feasibility of the deep learning approach to network traffic analysis.},
issn={1573-7543},
doi={10.1007/s10586-017-1117-8},

}

@misc{weerasinghe_sandamalomnet_simulation_2020,
	title = {sandamal/omnet\_simulation},

	abstract = {Contribute to sandamal/omnet\_simulation development by creating an account on GitHub.},
	urldate = {2020-07-10},
	journal = {https://github.com/sandamal/omnet\_simulation},
	author = {Weerasinghe, Sandamal},
	year = {2020},
	note = {Available: https://github.com/sandamal/omnet\_simulation. [Accessed: 10-June-2020]},
	keywords = {dataset, omnet}
}

@misc{the_uci_kdd_archive_kdd_1999,
	title = {{KDD} {Cup} 1999 {Data}},

	urldate = {2020-07-10},
	author = {The UCI KDD Archive},
	month = oct,
	year = {1999},
	file = {KDD Cup 1999 Data:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/MN2NXIZ2/kddcup99.html:text/html}
}

@article{pelechrinis_cognitive_2012,
	title = {Cognitive {Radio} {Networks}: {Realistic} or {Not}?},
	shorttitle = {Cognitive {Radio} {Networks}},

	abstract = {A large volume of research has been conducted in the cognitive radio (CR) area the last decade. However, the deployment of a commercial CR network is yet to emerge. A large portion of the existing literature does not build on real world scenarios, hence, neglecting various important interactions of the research with commercial telecommunication networks. For instance, a lot of attention has been paid to spectrum sensing as the front line functionality that needs to be completed in an efficient and accurate manner to enable an opportunistic CR network architecture. This is necessary to detect the existence of spectrum holes without which no other procedure can be fulfilled. However, simply sensing (cooperatively or not) the energy received from a primary transmitter cannot enable correct dynamic spectrum access. For example, the low strength of a primary transmitter's signal does not assure that there will be no interference to a nearby primary receiver. In addition, the presence of a primary transmitter's signal does not mean that CR network users cannot access the spectrum since there might not be any primary receiver in the vicinity. Despite the existing elegant and clever solutions to the DSA problem no robust, implementable scheme has emerged. In this paper, we challenge the basic premises of the proposed schemes. We further argue that addressing the technical challenges we face in deploying robust CR networks can only be achieved if we radically change the way we design their basic functionalities. In support of our argument, we present a set of real-world scenarios, inspired by realistic settings in commercial telecommunications networks, focusing on spectrum sensing as a basic and critical functionality in the deployment of CRs. We use these scenarios to show why existing DSA paradigms are not amenable to realistic deployment in complex wireless environments.},
	urldate = {2020-07-11},
	journal = {arXiv:1209.2154 [cs]},
	author = {Pelechrinis, Konstantinos and Krishnamurthy, Prashant and Weiss, Martin and Znati, Taied},
	month = sep,
	year = {2012},
	note = {arXiv: 1209.2154},
	keywords = {Computer Science - Networking and Internet Architecture},
	file = {arXiv Fulltext PDF:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/4HW9DPNG/Pelechrinis et al. - 2012 - Cognitive Radio Networks Realistic or Not.pdf:application/pdf;arXiv.org Snapshot:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/RYYRLKU5/1209.html:text/html}
}

@misc{hindupur_hindupuravinash-gan-zoo_2020,
	title = {hindupuravinash/the-gan-zoo},
	copyright = {MIT License         ,                 MIT License},

	abstract = {A list of all named GANs! Contribute to hindupuravinash/the-gan-zoo development by creating an account on GitHub.},
	urldate = {2020-07-13},
	author = {Hindupur, Avinash},
	month = jul,
	year = {2020},
	note = {original-date: 2017-04-14T16:45:24Z},
	keywords = {gan, generative-adversarial-network, machine-learning}
}

@misc{draelos_measuring_2019,
	title = {Measuring {Performance}: {AUC} ({AUROC})},
	shorttitle = {Measuring {Performance}},

	abstract = {The area under the receiver operating characteristic (AUROC) is a performance metric that you can use to evaluate classification models. AUROC tells you whether your model is able to correctly rank…},
	language = {en},
	urldate = {2020-07-13},
	journal = {Glass Box},
	author = {Draelos, Rachel},
	month = feb,
	year = {2019},
	note = {Library Catalog: glassboxmedicine.com},
	file = {Snapshot:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/SDLCXEC2/measuring-performance-auc-auroc.html:text/html}
}

@incollection{hutchison_towards_2011,
	address = {Berlin, Heidelberg},
	title = {Towards a {Cooperative} {Intrusion} {Detection} {System} for {Cognitive} {Radio} {Networks}},
	volume = {6827},
	isbn = {978-3-642-23040-0 978-3-642-23041-7},

	abstract = {Cognitive Radio Networks (CRNs) arise as a promising solution to the scarcity of spectrum. By means of cooperation and smart decisions inﬂuenced by previous knowledge, CRNs are able to detect and proﬁt from the best spectrum opportunities without interfering primary licensed users. However, besides the well-known attacks to wireless networks, new attacks threat this type of networks. In this paper we analyze these threats and propose a set of intrusion detection modules targeted to detect them. Provided method will allow a CRN to identify attack sources and types of attacks, and to properly react against them.},
	language = {en},
	urldate = {2020-07-15},
	booktitle = {{NETWORKING} 2011 {Workshops}},
	publisher = {Springer Berlin Heidelberg},
	author = {León, Olga and Román, Rodrigo and Hernández-Serrano, Juan},
	editor = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Doug and Vardi, Moshe Y. and Weikum, Gerhard and Casares-Giner, Vicente and Manzoni, Pietro and Pont, Ana},
	year = {2011},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {231--242},
	file = {León et al. - 2011 - Towards a Cooperative Intrusion Detection System f.pdf:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/93A5RBNG/León et al. - 2011 - Towards a Cooperative Intrusion Detection System f.pdf:application/pdf}
}

@article{abarca_contributions_2012,
	title = {Contributions to the {Security} of {Cognitive} {Radio} {Networks}},

	journal = {UPC, Departament d'Enginyeria Telemàtica},
	author = {Abarca, Olga León},
	month = jan,
	year = {2012},
	pages = {191},
	file = {Abarca - Contributions to the Security of Cognitive Radio N.pdf:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/KT8AYH7T/Abarca - Contributions to the Security of Cognitive Radio N.pdf:application/pdf}
}

@inproceedings{lichuan_dynamic_2014,
	address = {Cham},
	series = {Advances in {Intelligent} {Systems} and {Computing}},
	title = {A {Dynamic} {Intrusion} {Detection} {Mechanism} {Based} on {Smart} {Agents} in {Distributed} {Cognitive} {Radio} {Networks}},
	isbn = {978-3-319-01796-9},
	abstract = {To overcome the lack of available spectrum in wireless communications, the Cognitive Radio Networks arise. But this new technology also brings new threats to the whole network, especially the lion attack. Concerned with the weakness of the existing intrusion detection systems in Cognitive Radio Networks, we propose a dynamic intrusion detection mechanism on basis of smart agents with the utility of Markov chain model. And simulation results verify the efficiency of our mechanism.},
	language = {en},
	booktitle = {Genetic and {Evolutionary} {Computing}},
	publisher = {Springer International Publishing},
	author = {Lichuan, Ma and Ying, Min and Qingqi, Pei},
	editor = {Pan, Jeng-Shyang and Krömer, Pavel and Snášel, Václav},
	year = {2014},
	keywords = {Cognitive Radio Networks, Intrusion Detection Mechanism, Markov Chain Model},
	pages = {283--290},
	file = {Springer Full Text PDF:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/MRAEAPGH/Lichuan et al. - 2014 - A Dynamic Intrusion Detection Mechanism Based on S.pdf:application/pdf}
}

@inproceedings{srinu_physical_2019,
	title = {Physical layer security against cooperative anomaly attack using bivariate data in distributed {CRNs}},
	abstract = {Wireless communication network (WCN) performance is primarily depends on physical layer security which is critical among all other layers of OSI network model. It is typically prone to anomaly/malicious user's attacks owing to openness of wireless channels. Cognitive radio networking (CRN) is a recently emerged wireless technology that is having numerous security challenges because of its unlicensed access of wireless channels. In CRNs, the security issues occur mainly during spectrum sensing and is more pronounced during distributed spectrum sensing. In recent past, various anomaly effects are modelled and developed detectors by applying advanced statistical techniques. Nevertheless, many of these detectors have been developed based on sensing data of one variable (energy measurement) and degrades their performance drastically when the data is contaminated with multiple anomaly nodes, that attack the network cooperatively. Hence, one has to develop an efficient multiple anomaly detection algorithm to eliminate all possible cooperative attacks. To achieve this, in this work, the impact of anomaly on detection probability is verified beforehand in developing an efficient algorithm using bivariate data to detect possible attacks with mahalanobis distance measure. Result discloses that detection error of cooperative attacks by anomaly has significant impact on eigenvalue-based sensing.},
	booktitle = {2019 11th {International} {Conference} on {Communication} {Systems} {Networks} ({COMSNETS})},
	author = {Srinu, Sesham and Reddy, M. Kranthi Kumar and Temaneh-Nyah, Clement},
	month = jan,
	year = {2019},
	note = {ISSN: 2155-2509},
	keywords = {cognitive radio, radio spectrum management, Sensors, telecommunication security, wireless channels, Wireless sensor networks, cooperative communication, signal detection, Communication system security, Cognitive radio networks, Computational modeling, Anomaly, bivariate data, Bivariate data, cognitive radio networking, cooperative anomaly attack, Cooperative SSDF attacks, distributed spectrum sensing, eigenvalue-based sensing, eigenvalues and eigenfunctions, Eigenvalues and eigenfunctions, mahalanobis distance measure, multiple anomaly detection algorithm, OSI network model, physical layer security, Physical layer security, Robust distance measure, Wireless communication, wireless communication network, wireless technology},
	pages = {410--413},
	file = {IEEE Xplore Abstract Record:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/LEQQ4RHM/8711071.html:text/html}
}

@article{meng_collaborative_2011,
	title = {Collaborative {Spectrum} {Sensing} from {Sparse} {Observations} in {Cognitive} {Radio} {Networks}},
	volume = {29},
	issn = {1558-0008},
	abstract = {Spectrum sensing, which aims at detecting spectrum holes, is the precondition for the implementation of cognitive radio (CR). Collaborative spectrum sensing among the cognitive radio nodes is expected to improve the ability of checking complete spectrum usage. Due to hardware limitations, each cognitive radio node can only sense a relatively narrow band of radio spectrum. Consequently, the available channel sensing information is far from being sufficient for precisely recognizing the wide range of unoccupied channels. Aiming at breaking this bottleneck, we propose to apply matrix completion and joint sparsity recovery to reduce sensing and transmission requirements and improve sensing results. Specifically, equipped with a frequency selective filter, each cognitive radio node senses linear combinations of multiple channel information and reports them to the fusion center, where occupied channels are then decoded from the reports by using novel matrix completion and joint sparsity recovery algorithms. As a result, the number of reports sent from the CRs to the fusion center is significantly reduced. We propose two decoding approaches, one based on matrix completion and the other based on joint sparsity recovery, both of which allow exact recovery from incomplete reports. The numerical results validate the effectiveness and robustness of our approaches. In particular, in small-scale networks, the matrix completion approach achieves exact channel detection with a number of samples no more than 50\% of the number of channels in the network, while joint sparsity recovery achieves similar performance in large-scale networks.},
	number = {2},
	journal = {IEEE Journal on Selected Areas in Communications},
	author = {Meng, Jia Jasmine and Yin, Wotao and Li, Husheng and Hossain, Ekram and Han, Zhu},
	month = feb,
	year = {2011},
	note = {Conference Name: IEEE Journal on Selected Areas in Communications},
	keywords = {cognitive radio, Sensors, Cognitive radio, cognitive radio networks, telecommunication channels, Approximation algorithms, channel detection, channel sensing information, Collaboration, collaborative spectrum sensing, Collaborative spectrum sensing, compressive sensing, decoding, decoding approach, Fading, frequency selective filter, Heuristic algorithms, joint sparsity recovery, joint sparsity recovery algorithm, Joints, matrix completion, matrix completion algorithm, sparse matrices, spectrum hole detection},
	pages = {327--337},
	file = {IEEE Xplore Abstract Record:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/IHTQRVYY/5701687.html:text/html;IEEE Xplore Full Text PDF:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/TAAIPMSQ/Meng et al. - 2011 - Collaborative Spectrum Sensing from Sparse Observa.pdf:application/pdf}
}

@inproceedings{zeng_framework_2019,
	address = {Los Cabos Mexico},
	title = {A {Framework} for {Analyzing} {Spectrum} {Characteristics} in {Large} {Spatio}-temporal {Scales}},
	isbn = {978-1-4503-6169-9},
	abstract = {Understanding spectrum characteristics with little prior knowledge requires fine-grained spectrum data in the frequency, spatial, and temporal domains; gathering such a diverse set of measurements results in a large data volume. Analysis of the resulting dataset poses unique challenges; methods in the status quo are tailored for specific spectrum-related applications (apps), and are ill equipped to process data of this magnitude. In this paper, we design BigSpec, a generalpurpose framework that allows for fast processing of apps. The key idea is to reduce computation costs by performing computation extensively on compressed data that preserves signal features. Adhering to this guideline, we build solutions for three apps, i.e., energy detection, spatio-temporal spectrum estimation, and anomaly detection. These apps were chosen to highlight BigSpec’s efficiency, scalability, and extensibility. To evaluate BigSpec’s performance, we collect more than 1 terabyte of spectrum data spanning a year, across 300MHz-4GHz, covering 400 km2. Compared with baselines and prior works, we achieve 17× run time efficiency, sublinear rather than linear run time scalability, and extend the definition of anomaly to different domains (frequency \& spatio-temporal). We also obtain high-level insights from the data to provide valuable advice on future spectrum measurement and data analysis.},
	language = {en},
	urldate = {2020-07-15},
	booktitle = {The 25th {Annual} {International} {Conference} on {Mobile} {Computing} and {Networking}},
	publisher = {ACM},
	author = {Zeng, Yijing and Chandrasekaran, Varun and Banerjee, Suman and Giustiniano, Domenico},
	month = oct,
	year = {2019},
	pages = {1--16},
	file = {Zeng et al. - 2019 - A Framework for Analyzing Spectrum Characteristics.pdf:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/9HDRXYVW/Zeng et al. - 2019 - A Framework for Analyzing Spectrum Characteristics.pdf:application/pdf}
}

@misc{noauthor_80222-2019_nodate,
	title = {802.22-2019 - {IEEE} {Standard} for {Information} {Technology}--{Telecommunications} and information exchange between systems--{Wireless} {Regional} {Area} {Networks} ({WRAN})--{Specific} requirements--{Part} 22: {Cognitive} {Wireless} {RAN} {Medium} {Access} {Control} ({MAC}) and {Physical} {Layer} ({PHY}) {Specifications}: {Policies} and {Procedures} for {Operation} in the {Bands} that {Allow} {Spectrum} {Sharing} where the {Communications} {Devices} {May} {Opportunistically} {Operate} in the {Spectrum} of {Primary} {Service}},

	urldate = {2020-07-15},
	file = {802.22-2019 - IEEE Standard for Information Technology--Telecommunications and information exchange between systems--Wireless Regional Area Networks (WRAN)--Specific requirements--Part 22\: Cognitive Wireless RAN Medium Access Control (MAC) and Physical Layer (PHY) Specifications\: Policies and Procedures for Operation in the Bands that Allow Spectrum Sharing where the Communications Devices May Opportunistically Operate in the Spectrum of Primary Service:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/WNWLEWDA/802_22-2019.html:text/html}
}

@inproceedings{rajendran_saife_2018,
	title = {{SAIFE}: {Unsupervised} {Wireless} {Spectrum} {Anomaly} {Detection} with {Interpretable} {Features}},
	shorttitle = {{SAIFE}},
	abstract = {Detecting anomalous behavior in wireless spectrum is a demanding task due to the sheer complexity of the electromagnetic spectrum use. Wireless spectrum anomalies can take a wide range of forms from the presence of an unwanted signal in a licensed band to the absence of an expected signal, which makes manual labeling of anomalies difficult and suboptimal. We present, Spectrum Anomaly Detector with Interpretable FEatures (SAIFE), an Adversarial Autoencoder (AAE) based anomaly detector for wireless spectrum anomaly detection using Power Spectral Density (PSD) data which achieves good anomaly detection and localization in an unsupervised setting. In addition, we investigate the model's capabilities to learn interpretable features such as signal bandwidth, class and center frequency in a semi-supervised fashion. Along with anomaly detection the model exhibits promising results for lossy PSD data compression up to 120X and semi-supervised signal classification accuracy close to 100\% on three datasets just using 20\% labeled samples. Finally the model is tested on data from one of the distributed Electrosense sensors over a long term of 500 hours showing its anomaly detection capabilities.},
	booktitle = {2018 {IEEE} {International} {Symposium} on {Dynamic} {Spectrum} {Access} {Networks} ({DySPAN})},
	author = {Rajendran, Sreeraj and Meert, Wannes and Lenders, Vincent and Pollin, Sofie},
	month = oct,
	year = {2018},
	note = {ISSN: 2334-3125},
	keywords = {telecommunication computing, Anomaly detection, Wireless sensor networks, spectral analysis, wireless sensor networks, neural nets, signal classification, Deep learning, Data models, Wireless communication, adversarial autoencoder, anomalous behavior detection, data compression, deep learning, Detectors, electromagnetic spectrum, Feature extraction, Hidden Markov models, lossy PSD data compression, power spectral density, PSD, SAIFE, semisupervised signal classification accuracy, signal bandwidth, spectrum anomaly detector with interpretable features, Spectrum monitoring, unsupervised learning, unsupervised setting, wireless spectrum anomalies},
	pages = {1--9},
	file = {IEEE Xplore Abstract Record:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/Q9YG5NI2/8610471.html:text/html;IEEE Xplore Full Text PDF:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/R2K7B6Y5/Rajendran et al. - 2018 - SAIFE Unsupervised Wireless Spectrum Anomaly Dete.pdf:application/pdf}
}

@inproceedings{alpcan_distributed_2009,
	title = {A distributed machine learning framework},
	abstract = {A distributed online learning framework for support vector machines (SVMs) is presented and analyzed. First, the generic binary classification problem is decomposed into multiple relaxed subproblems. Then, each of them is solved iteratively through parallel update algorithms with minimal communication overhead. This computation can be performed by individual processing units, such as separate computers or processor cores, in parallel and possibly having access to only a subset of the data. Convergence properties of continuous- and discrete-time variants of the proposed parallel update schemes are studied. A sufficient condition is derived under which synchronous and asynchronous gradient algorithms converge to the approximate solution. Subsequently, a class of stochastic update algorithms, which may arise due to distortions in the information flow between units, is shown to be globally stable under similar sufficient conditions. Active set methods are utilized to decrease communication and computational overhead. A numerical example comparing centralized and distributed learning schemes indicates favorable properties of the proposed framework such as configurability and fast convergence.},
	booktitle = {48h {IEEE} {Conference} on {Decision} and {Control} ({CDC}) held jointly with 2009 28th {Chinese} {Control} {Conference}},
	author = {Alpcan, Tansu and Bauckhage, Christian},
	month = dec,
	year = {2009},
	note = {ISSN: 0191-2216},
	keywords = {learning (artificial intelligence), Machine learning, Wireless sensor networks, minimal communication overhead, active set method, asynchronous gradient algorithm, centralized learning, computational overhead, Concurrent computing, continuous-time variants, convergence property, discrete-time variants, distributed machine learning, distributed online learning, generic binary classification, gradient methods, information flow, Iterative algorithms, Multicore processing, multiple relaxed subproblems, Multiprocessing systems, parallel algorithms, parallel update algorithm, pattern classification, Stochastic processes, stochastic update, Sufficient conditions, support vector machine, Support vector machine classification, support vector machines, Support vector machines},
	pages = {2546--2551},
	file = {IEEE Xplore Abstract Record:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/SWULWDCX/5399634.html:text/html;IEEE Xplore Full Text PDF:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/R27882HE/Alpcan and Bauckhage - 2009 - A distributed machine learning framework.pdf:application/pdf}
}

@article{assran_stochastic_2018,
	title = {Stochastic {Gradient} {Push} for {Distributed} {Deep} {Learning}},
	abstract = {Distributed data-parallel algorithms aim to accelerate the training of deep neural networks by parallelizing the computation of large mini-batch gradient updates across multiple nodes. Approaches that synchronize nodes using exact distributed averaging (e.g., via ALLREDUCE) are sensitive to stragglers and communication delays. The PUSHSUM gossip algorithm is robust to these issues, but only performs approximate distributed averaging. This paper studies Stochastic Gradient Push (SGP), which combines PUSHSUM with stochastic gradient updates. We prove that SGP converges to a stationary point of smooth, nonconvex objectives at the same sub-linear rate as SGD, and that all nodes achieve consensus. We empirically validate the performance of SGP on image classiﬁcation (ResNet-50, ImageNet) and machine translation (Transformer, WMT’16 EnDe) workloads.},
	language = {en},
	author = {Assran, Mahmoud and Loizou, Nicolas and Ballas, Nicolas and Rabbat, Mike},
	year = {2018},
	pages = {10},
	file = {Assran et al. - Stochastic Gradient Push for Distributed Deep Lear.pdf:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/3CQ94JGU/Assran et al. - Stochastic Gradient Push for Distributed Deep Lear.pdf:application/pdf}
}

@article{ben-nun_demystifying_2019,
	title = {Demystifying {Parallel} and {Distributed} {Deep} {Learning}: {An} {In}-depth {Concurrency} {Analysis}},
	volume = {52},
	issn = {0360-0300, 1557-7341},
	shorttitle = {Demystifying {Parallel} and {Distributed} {Deep} {Learning}},
	language = {en},
	number = {4},
	urldate = {2020-07-15},
	journal = {ACM Computing Surveys},
	author = {Ben-Nun, Tal and Hoefler, Torsten},
	month = sep,
	year = {2019},
	pages = {1--43},
	file = {Ben-Nun and Hoefler - 2019 - Demystifying Parallel and Distributed Deep Learnin.pdf:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/URFXG2SQ/Ben-Nun and Hoefler - 2019 - Demystifying Parallel and Distributed Deep Learnin.pdf:application/pdf}
}

@article{shaat_computationally_2010,
	title = {Computationally {Efficient} {Power} {Allocation} {Algorithm} in {Multicarrier}-{Based} {Cognitive} {Radio} {Networks}: {OFDM} and {FBMC} {Systems}},
	volume = {2010},
	issn = {1687-6180},
	shorttitle = {Computationally {Efficient} {Power} {Allocation} {Algorithm} in {Multicarrier}-{Based} {Cognitive} {Radio} {Networks}},
	language = {en},
	number = {1},
	urldate = {2020-07-15},
	journal = {EURASIP Journal on Advances in Signal Processing},
	author = {Shaat, Musbah and Bader, Faouzi},
	month = dec,
	year = {2010},
	pages = {528378},
	file = {Shaat and Bader - 2010 - Computationally Efficient Power Allocation Algorit.pdf:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/2IQQ4Q8L/Shaat and Bader - 2010 - Computationally Efficient Power Allocation Algorit.pdf:application/pdf}
}

@article{makhzani_adversarial_2016,
	title = {Adversarial {Autoencoders}},

	abstract = {In this paper, we propose the "adversarial autoencoder" (AAE), which is a probabilistic autoencoder that uses the recently proposed generative adversarial networks (GAN) to perform variational inference by matching the aggregated posterior of the hidden code vector of the autoencoder with an arbitrary prior distribution. Matching the aggregated posterior to the prior ensures that generating from any part of prior space results in meaningful samples. As a result, the decoder of the adversarial autoencoder learns a deep generative model that maps the imposed prior to the data distribution. We show how the adversarial autoencoder can be used in applications such as semi-supervised classification, disentangling style and content of images, unsupervised clustering, dimensionality reduction and data visualization. We performed experiments on MNIST, Street View House Numbers and Toronto Face datasets and show that adversarial autoencoders achieve competitive results in generative modeling and semi-supervised classification tasks.},
	urldate = {2020-07-16},
	journal = {arXiv:1511.05644 [cs]},
	author = {Makhzani, Alireza and Shlens, Jonathon and Jaitly, Navdeep and Goodfellow, Ian and Frey, Brendan},
	month = may,
	year = {2016},
	note = {arXiv: 1511.05644},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/GH6VPHDE/Makhzani et al. - 2016 - Adversarial Autoencoders.pdf:application/pdf;arXiv.org Snapshot:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/4NQHJCPR/1511.html:text/html}
}

@incollection{steinhardt_certified_2017,
	title = {Certified {Defenses} for {Data} {Poisoning} {Attacks}},

	urldate = {2020-07-16},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 30},
	publisher = {Curran Associates, Inc.},
	author = {Steinhardt, Jacob and Koh, Pang Wei W and Liang, Percy S},
	editor = {Guyon, I. and Luxburg, U. V. and Bengio, S. and Wallach, H. and Fergus, R. and Vishwanathan, S. and Garnett, R.},
	year = {2017},
	pages = {3517--3529},
	file = {NIPS Full Text PDF:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/2QGBAF6W/Steinhardt et al. - 2017 - Certified Defenses for Data Poisoning Attacks.pdf:application/pdf;NIPS Snapshot:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/TV3BGYZ8/6943-certified-defenses-for-data-poisoning-attacks.html:text/html}
}

@article{zhao_shielding_2020,
author={Zhao, Lingchen and Hu, Shengshan and Wang, Qian and Jiang, Jianlin and Shen, Chao and Luo, Xiangyang and Hu, Pengfei},
  journal={IEEE Transactions on Dependable and Secure Computing},
  title={Shielding Collaborative Learning: Mitigating Poisoning Attacks Through Client-Side Detection},
  year={2021},
  volume={18},
  number={5},
  pages={2029-2041}}

@inproceedings{pfammatter_software-defined_2015,
	address = {Seattle, Washington},
	title = {A software-defined sensor architecture for large-scale wideband spectrum monitoring},
	isbn = {978-1-4503-3475-4},
	abstract = {Today’s spectrum measurements are mainly performed by governmental agencies which drive around using expensive specialized hardware. The idea of crowdsourcing spectrum monitoring has recently gained attention as an alternative way to capture the usage of wide portions of the wireless spectrum at larger geographical and time scales. To support this vision, we develop a ﬂexible software-deﬁned sensor architecture that enables distributed data collection in real-time over the Internet. Our sensor design builds upon low-cost commercial off-the-shelf (COTS) hardware components with a total cost per sensor device below \$100. The low-cost nature of our sensor platform makes the sensing approach particularly suitable for large-scale deployments but imposes technical challenges regarding performance and quality. To circumvent the limits of our solution, we have implemented and evaluated different sensing strategies and noise reduction techniques. Our results suggest that our sensor architecture may be useful in application areas such as dynamic spectrum access in cognitive radios, detecting regions with elevated electro-smog, or simply to gain an understanding of the spectrum usage for advanced signal intelligence such as anomaly detection or policy enforcement.},
	language = {en},
	urldate = {2020-07-16},
	booktitle = {The 14th {International} {Conference} on {Information} {Processing} in {Sensor} {Networks} - {IPSN} '15},
	publisher = {ACM Press},
	author = {Pfammatter, Damian and Giustiniano, Domenico and Lenders, Vincent},
	year = {2015},
	pages = {71--82},
	file = {Pfammatter et al. - 2015 - A software-defined sensor architecture for large-s.pdf:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/2FAI8NIN/Pfammatter et al. - 2015 - A software-defined sensor architecture for large-s.pdf:application/pdf}
}

@article{rajendran_deep_2018,
	title = {Deep {Learning} {Models} for {Wireless} {Signal} {Classification} {With} {Distributed} {Low}-{Cost} {Spectrum} {Sensors}},
	volume = {4},
	issn = {2332-7731},
	abstract = {This paper looks into the modulation classification problem for a distributed wireless spectrum sensing network. First, a new data-driven model for automatic modulation classification based on long short term memory (LSTM) is proposed. The model learns from the time domain amplitude and phase information of the modulation schemes present in the training data without requiring expert features like higher order cyclic moments. Analyses show that the proposed model yields an average classification accuracy of close to 90\% at varying signal-to-noise ratio conditions ranging from 0 dB to 20 dB. Further, we explore the utility of this LSTM model for a variable symbol rate scenario. We show that a LSTM based model can learn good representations of variable length time domain sequences, which is useful in classifying modulation signals with different symbol rates. The achieved accuracy of 75\% on an input sample length of 64 for which it was not trained, substantiates the representation power of the model. To reduce the data communication overhead from distributed sensors, the feasibility of classification using averaged magnitude spectrum data and on-line classification on the low-cost spectrum sensors are studied. Furthermore, quantized realizations of the proposed models are analyzed for deployment on sensors with low processing power.},
	number = {3},
	journal = {IEEE Transactions on Cognitive Communications and Networking},
	author = {Rajendran, Sreeraj and Meert, Wannes and Giustiniano, Domenico and Lenders, Vincent and Pollin, Sofie},
	month = sep,
	year = {2018},
	note = {Conference Name: IEEE Transactions on Cognitive Communications and Networking},
	keywords = {cognitive radio, Machine learning, Sensors, Wireless sensor networks, spectrum sensing, Communication system security, signal classification, Modulation, Deep learning, Data models, Wireless communication, automatic modulation classification, average classification accuracy, CNN, data communication overhead, data-driven model, deep learning models, distributed low-cost spectrum sensors, distributed sensors, distributed wireless spectrum sensing network, expert features, higher order cyclic moments, higher order statistics, input sample length, long short term memory, low processing power, LSTM, LSTM model, magnitude spectrum data, modulation, modulation classification, modulation classification problem, modulation signals, on-line classification, phase information, representation power, symbol rates, time domain amplitude, training data, variable length time domain sequences, variable symbol rate scenario, varying signal-to-noise ratio conditions, wireless signal classification},
	pages = {433--445},
	file = {IEEE Xplore Abstract Record:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/WHMJIMNY/8357902.html:text/html;Submitted Version:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/X9BHLSW4/Rajendran et al. - 2018 - Deep Learning Models for Wireless Signal Classific.pdf:application/pdf}
}

@article{rajendran_electrosense_2018,
	title = {Electrosense: {Open} and {Big} {Spectrum} {Data}},
	volume = {56},
	issn = {0163-6804},
	shorttitle = {Electrosense},
	abstract = {While the radio spectrum allocation is well regulated, there is little knowledge about its actual utilization over time and space. This limitation hinders taking effective actions in various applications including cognitive radios, electrosmog monitoring, and law enforcement. We introduce Electrosense, an initiative that seeks a more efficient, safe and reliable monitoring of the electromagnetic space by improving the accessibility of spectrum data for the general public. A collaborative spectrum monitoring network is designed that monitors the spectrum at large scale with low-cost spectrum sensing nodes. The large set of data is stored and processed in a big data architecture and provided back to the community with an open spectrum data as a service model, that allows users to build diverse and novel applications with different requirements. We illustrate useful usage scenarios of the Electrosense data.},
	number = {1},
	urldate = {2020-07-17},
	journal = {IEEE Communications Magazine},
	author = {Rajendran, Sreeraj and Calvo-Palomino, Roberto and Fuchs, Markus and Bergh, Bertold Van den and Cordobés, Héctor and Giustiniano, Domenico and Pollin, Sofie and Lenders, Vincent},
	month = jan,
	year = {2018},
	note = {arXiv: 1703.09989},
	keywords = {Computer Science - Networking and Internet Architecture},
	pages = {210--217},
	file = {arXiv Fulltext PDF:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/5SLGS8W8/Rajendran et al. - 2018 - Electrosense Open and Big Spectrum Data.pdf:application/pdf;arXiv.org Snapshot:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/AJTGMGZR/1703.html:text/html}
}

@article{rajendran_deep_2018-1,
	title = {Deep {Learning} {Models} for {Wireless} {Signal} {Classification} {With} {Distributed} {Low}-{Cost} {Spectrum} {Sensors}},
	volume = {4},
	issn = {2332-7731},
	abstract = {This paper looks into the modulation classification problem for a distributed wireless spectrum sensing network. First, a new data-driven model for automatic modulation classification based on long short term memory (LSTM) is proposed. The model learns from the time domain amplitude and phase information of the modulation schemes present in the training data without requiring expert features like higher order cyclic moments. Analyses show that the proposed model yields an average classification accuracy of close to 90\% at varying signal-to-noise ratio conditions ranging from 0 dB to 20 dB. Further, we explore the utility of this LSTM model for a variable symbol rate scenario. We show that a LSTM based model can learn good representations of variable length time domain sequences, which is useful in classifying modulation signals with different symbol rates. The achieved accuracy of 75\% on an input sample length of 64 for which it was not trained, substantiates the representation power of the model. To reduce the data communication overhead from distributed sensors, the feasibility of classification using averaged magnitude spectrum data and on-line classification on the low-cost spectrum sensors are studied. Furthermore, quantized realizations of the proposed models are analyzed for deployment on sensors with low processing power.},
	number = {3},
	journal = {IEEE Transactions on Cognitive Communications and Networking},
	author = {Rajendran, Sreeraj and Meert, Wannes and Giustiniano, Domenico and Lenders, Vincent and Pollin, Sofie},
	month = sep,
	year = {2018},
	note = {Conference Name: IEEE Transactions on Cognitive Communications and Networking},
	keywords = {cognitive radio, Machine learning, Sensors, Wireless sensor networks, spectrum sensing, Communication system security, signal classification, Modulation, Deep learning, Data models, Wireless communication, automatic modulation classification, average classification accuracy, CNN, data communication overhead, data-driven model, deep learning models, distributed low-cost spectrum sensors, distributed sensors, distributed wireless spectrum sensing network, expert features, higher order cyclic moments, higher order statistics, input sample length, long short term memory, low processing power, LSTM, LSTM model, magnitude spectrum data, modulation, modulation classification, modulation classification problem, modulation signals, on-line classification, phase information, representation power, symbol rates, time domain amplitude, training data, variable length time domain sequences, variable symbol rate scenario, varying signal-to-noise ratio conditions, wireless signal classification},
	pages = {433--445},
	file = {IEEE Xplore Abstract Record:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/FMSLU75Q/8357902.html:text/html;Submitted Version:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/M5UKSTSF/Rajendran et al. - 2018 - Deep Learning Models for Wireless Signal Classific.pdf:application/pdf}
}

@article{roy_machine_2019,
	title = {Machine {Learning} in {Adversarial} {RF} {Environments}},
	volume = {57},
	issn = {1558-1896},
	abstract = {With more and more autonomous deployments of wireless networks, accurate knowledge of the RF environment is becoming indispensable. Various techniques have been developed over the years that can not only assess the RF environment but can also characterize the various radio transmitters (sources) that define the ambient RF environment. Machine learning techniques have shown promise for such characterizations through the development of RF machine learning (RFML) systems delivering autonomous control. Although classical machine learning techniques work well for a large variety of tasks, they have not done as well for RFML. For RFML tasks, deep feature learners with an inherent recurrent structure have been shown to perform well. Even so, the field of RFML is still very young, and a lot needs to be done to bridge the gap between the ML community and the wireless community for RFML to be successfully applied for solving large-scale real-life problems. This article is a step in that direction.},
	number = {5},
	journal = {IEEE Communications Magazine},
	author = {Roy, Debashri and Mukherjee, Tathagata and Chatterjee, Mainak},
	month = may,
	year = {2019},
	note = {Conference Name: IEEE Communications Magazine},
	keywords = {learning (artificial intelligence), Machine learning, Radio transmitters, telecommunication computing, Radio frequency, radio transmitters, Wireless communication, Support vector machines, adversarial RF environments, Artificial intelligence, autonomous control, autonomous deployments, Bayes methods, Data science, Inference algorithms, RF machine learning systems, RFML tasks, wireless networks},
	pages = {82--87},
	file = {IEEE Xplore Abstract Record:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/JCJU65WN/8713804.html:text/html;IEEE Xplore Full Text PDF:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/79M3LD98/Roy et al. - 2019 - Machine Learning in Adversarial RF Environments.pdf:application/pdf}
}

@article{zheng_big_2018,
	title = {Big {Data} {Processing} {Architecture} for {Radio} {Signals} {Empowered} by {Deep} {Learning}: {Concept}, {Experiment}, {Applications} and {Challenges}},
	volume = {6},
	issn = {2169-3536},
	shorttitle = {Big {Data} {Processing} {Architecture} for {Radio} {Signals} {Empowered} by {Deep} {Learning}},
	abstract = {In modern society, the demand for radio spectrum resources is increasing. As the information carriers of wireless transmission data, radio signals exhibit the characteristics of big data in terms of volume, variety, value, and velocity. How to uniformly handle these radio signals and obtain value from them is a problem that needs to be studied. In this paper, a big data processing architecture for radio signals is presented and a new approach of end-to-end signal processing based on deep learning is discussed in detail. The radio signal intelligent search engine is used as an example to verify the architecture, and the system components and experimental results are introduced. In addition, the applications of the architecture in cognitive radio, spectrum monitoring, and cyberspace security are introduced. Finally, challenges are discussed, such as unified representation of radio signal features, distortionless compression of wideband sampled data, and deep neural networks for radio signals.},
	journal = {IEEE Access},
	author = {Zheng, Shilian and Chen, Shichuan and Yang, Lifeng and Zhu, Jiawei and Luo, Zhenxing and Hu, Junjie and Yang, Xiaoniu},
	year = {2018},
	note = {Conference Name: IEEE Access},
	keywords = {cognitive radio, learning (artificial intelligence), Machine learning, radio spectrum management, telecommunication computing, telecommunication security, Wireless communication, deep learning, big data, Big Data, big data processing architecture, cyberspace, cyberspace security, Data mining, deep neural networks, end-to-end signal, Frequency shift keying, neural networks, radio signal feature representation, radio signal intelligent search engine, radio signal processing, Radio signals, radio spectrum resources, search engine, search engines, signal representation, signal sampling, spectrum monitoring, Time-frequency analysis, wideband sampled data distortionless compression, wireless transmission data},
	pages = {55907--55922},
	file = {IEEE Xplore Abstract Record:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/IJ5HJCAP/8476607.html:text/html;IEEE Xplore Full Text PDF:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/J9RPIKDA/Zheng et al. - 2018 - Big Data Processing Architecture for Radio Signals.pdf:application/pdf}
}

@inproceedings{lee_integrating_2016,
	title = {Integrating machine learning in embedded sensor systems for {Internet}-of-{Things} applications},
	abstract = {Interpreting sensor data in Internet-of-Things applications is a challenging problem particularly in embedded systems. We consider sensor data analytics where machine learning algorithms can be fully implemented on an embedded processor/sensor board. We develop an efficient real-time realization of a Gaussian mixture model (GMM) for execution on the NXP FRDM-K64F embedded sensor board. We demonstrate the design of a customized program and data structure that generates real-time sensor features, and we show details and training/classification results for select IoT applications. The integrated hardware/software system enables real-time data analytics and continuous training and re-training of the machine learning (ML) algorithm. The real-time ML platform can accommodate several applications with lower sensor data traffic.},
	booktitle = {2016 {IEEE} {International} {Symposium} on {Signal} {Processing} and {Information} {Technology} ({ISSPIT})},
	author = {Lee, Jongmin and Stanley, Michael and Spanias, Andreas and Tepedelenlioglu, Cihan},
	month = dec,
	year = {2016},
	keywords = {learning (artificial intelligence), Internet of Things, Internet-of-Things, condition monitoring, customized program design, data analysis, data structure, data structures, embedded machine learning, embedded processor-sensor board, embedded sensor systems, embedded systems, Gaussian mixture model, Gaussian processes, GMM, integrated hardware-software system, intelligent sensors, machine learning algorithms, mixture models, NXP FRDM-K64F embedded sensor board, real-time data analytics, real-time ML platform, real-time sensor feature generation, sensor data analytics, sensor data traffic},
	pages = {290--294},
	file = {IEEE Xplore Abstract Record:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/PP34K6YT/7886051.html:text/html;IEEE Xplore Full Text PDF:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/V8WFPJ7H/Lee et al. - 2016 - Integrating machine learning in embedded sensor sy.pdf:application/pdf}
}

@article{haigh_machine_2015,
	title = {Machine {Learning} for {Embedded} {Systems}: {A} {Case} {Study}},
	abstract = {We describe our application’s need for Machine Learning on a General Purpose Processor of an embedded device. Existing ML toolkits tend to be slow and consume memory, making them incompatible with real-time systems, limited hardware resources, or the rapid timing requirements of most embedded systems. We present our ML application, and the suite of optimizations we performed to create a system that can operate effectively on an embeddded platform. We perform an ablation study to analyze the impact of each optimization, and demonstrate over 20x improvement in runtimes over the original implementation, over a suite of 19 benchmark datasets. We present our results on two embedded systems.},
	language = {en},
	author = {Haigh, Karen Zita and Mackay, Allan M and Cook, Michael R and Lin, Li G},
	year = {2015},
	pages = {12},
	file = {Haigh et al. - 2015 - Machine Learning for Embedded Systems A Case Stud.pdf:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/V59CXXZ6/Haigh et al. - 2015 - Machine Learning for Embedded Systems A Case Stud.pdf:application/pdf}
}

@article{jiang_collaborative_2017,
	title = {Collaborative {Deep} {Learning} in {Fixed} {Topology} {Networks}},

	abstract = {There is significant recent interest to parallelize deep learning algorithms in order to handle the enormous growth in data and model sizes. While most advances focus on model parallelization and engaging multiple computing agents via using a central parameter server, aspect of data parallelization along with decentralized computation has not been explored sufficiently. In this context, this paper presents a new consensus-based distributed SGD (CDSGD) (and its momentum variant, CDMSGD) algorithm for collaborative deep learning over fixed topology networks that enables data parallelization as well as decentralized computation. Such a framework can be extremely useful for learning agents with access to only local/private data in a communication constrained environment. We analyze the convergence properties of the proposed algorithm with strongly convex and nonconvex objective functions with fixed and diminishing step sizes using concepts of Lyapunov function construction. We demonstrate the efficacy of our algorithms in comparison with the baseline centralized SGD and the recently proposed federated averaging algorithm (that also enables data parallelism) based on benchmark datasets such as MNIST, CIFAR-10 and CIFAR-100.},
	urldate = {2020-07-17},
	journal = {arXiv:1706.07880 [cs, stat]},
	author = {Jiang, Zhanhong and Balu, Aditya and Hegde, Chinmay and Sarkar, Soumik},
	month = jun,
	year = {2017},
	note = {arXiv: 1706.07880},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/QUGU6R54/Jiang et al. - 2017 - Collaborative Deep Learning in Fixed Topology Netw.pdf:application/pdf;arXiv.org Snapshot:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/KCQHJBDV/1706.html:text/html}
}

@article{kumar_equivalent_2019,
	title = {Equivalent and {Approximate} {Transformations} of {Deep} {Neural} {Networks}},

	abstract = {Two networks are equivalent if they produce the same output for any given input. In this paper, we study the possibility of transforming a deep neural network to another network with a different number of units or layers, which can be either equivalent, a local exact approximation, or a global linear approximation of the original network. On the practical side, we show that certain rectiﬁed linear units (ReLUs) can be safely removed from a network if they are always active or inactive for any valid input. If we only need an equivalent network for a smaller domain, then more units can be removed and some layers collapsed. On the theoretical side, we constructively show that for any feed-forward ReLU network, there exists a global linear approximation to a 2-hidden-layer shallow network with a ﬁxed number of units. This result is a balance between the increasing number of units for arbitrary approximation with a single layer and the known upper bound of log(n0 + 1) + 1 layers for exact representation, where n0 is the input dimension. While the transformed network may require an exponential number of units to capture the activation patterns of the original network, we show that it can be made substantially smaller by only accounting for the patterns that deﬁne linear regions. Based on experiments with ReLU networks on the MNIST dataset, we found that l1-regularization and adversarial training reduces the number of linear regions signiﬁcantly as the number of stable units increases due to weight sparsity. Therefore, we can also intentionally train ReLU networks to allow for effective loss-less compression and approximation.},
	language = {en},
	urldate = {2020-07-17},
	journal = {arXiv:1905.11428 [cs, stat]},
	author = {Kumar, Abhinav and Serra, Thiago and Ramalingam, Srikumar},
	month = may,
	year = {2019},
	note = {arXiv: 1905.11428},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	file = {Kumar et al. - 2019 - Equivalent and Approximate Transformations of Deep.pdf:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/KPQUJ8Q9/Kumar et al. - 2019 - Equivalent and Approximate Transformations of Deep.pdf:application/pdf}
}

@inproceedings{thierens_non-redundant_1996,
	title = {Non-redundant genetic coding of neural networks},
	abstract = {Feedforward neural networks have a number of functionally equivalent symmetries that make them difficult to optimise with genetic recombination operators. Although this problem has received considerable attention in the past, the proposed solutions all have a heuristic nature. We discuss a neural network genotype representation that completely eliminates the functional redundancies by transforming each neural network into its canonical form. This transformation is computationally extremely simple, since it only requires flipping the sign of some of the weights, followed by sorting the hidden neurons according to their bias. We have compared the redundant and non-redundant representations on the basis of their crossover correlation coefficient. As expected, the redundancy elimination results in a much higher crossover correlation coefficient, which shows that more information is now transmitted from the parents to the children. Finally, experimental results are given for the two-spirals classification problem.},
	booktitle = {{IEEE} {International} {Conference} on {Evolutionary} {Computation}},
	author = {Thierens, D.},
	month = may,
	year = {1996},
	keywords = {pattern classification, Algorithm design and analysis, bias, canonical form, Computer networks, correlation theory, crossover correlation coefficient, Design optimization, feedforward neural nets, feedforward neural network optimization, Feedforward neural networks, functional redundancy elimination, functionally equivalent symmetries, genetic algorithms, Genetic algorithms, genetic recombination operators, hidden neuron sorting, Network topology, neural network genotype representation, neural network transformation, Neural networks, Neurons, nonredundant genetic coding, parent-child information transmission, redundancy, sorting, Sorting, Spirals, symmetry, two-spirals classification problem, weight sign flipping},
	pages = {571--575},
	file = {IEEE Xplore Abstract Record:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/L88HTY4N/542662.html:text/html;IEEE Xplore Full Text PDF:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/YQ2GHWH4/Thierens - 1996 - Non-redundant genetic coding of neural networks.pdf:application/pdf}
}

@article{mirza_conditional_2014,
	title = {Conditional {Generative} {Adversarial} {Nets}},

	abstract = {Generative Adversarial Nets [8] were recently introduced as a novel way to train generative models. In this work we introduce the conditional version of generative adversarial nets, which can be constructed by simply feeding the data, y, we wish to condition on to both the generator and discriminator. We show that this model can generate MNIST digits conditioned on class labels. We also illustrate how this model could be used to learn a multi-modal model, and provide preliminary examples of an application to image tagging in which we demonstrate how this approach can generate descriptive tags which are not part of training labels.},
	urldate = {2020-07-19},
	journal = {arXiv:1411.1784 [cs, stat]},
	author = {Mirza, Mehdi and Osindero, Simon},
	month = nov,
	year = {2014},
	note = {arXiv: 1411.1784},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/RNEHAK97/Mirza and Osindero - 2014 - Conditional Generative Adversarial Nets.pdf:application/pdf;arXiv.org Snapshot:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/7YTDNEAQ/1411.html:text/html}
}

@article{behrmann_invertible_2019,
	title = {Invertible {Residual} {Networks}},
	language = {en},
	author = {Behrmann, Jens},
	year = {2019},
	pages = {30},
	file = {Behrmann - Invertible Residual Networks.pdf:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/BQTPHTRZ/Behrmann - Invertible Residual Networks.pdf:application/pdf}
}

@misc{amiclaus_adalm-pluto_2018,
	title = {{ADALM}-{PLUTO} {Detailed} {Specifications} [{Analog} {Devices} {Wiki}]},

	urldate = {2020-07-19},
	author = {Amiclaus},
	year = {2018},
	file = {ADALM-PLUTO Detailed Specifications [Analog Devices Wiki]:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/4XQN376W/specs.html:text/html}
}

@article{goodfellow_explaining_2015,
	title = {Explaining and {Harnessing} {Adversarial} {Examples}},

	abstract = {Several machine learning models, including neural networks, consistently misclassify adversarial examples---inputs formed by applying small but intentionally worst-case perturbations to examples from the dataset, such that the perturbed input results in the model outputting an incorrect answer with high confidence. Early attempts at explaining this phenomenon focused on nonlinearity and overfitting. We argue instead that the primary cause of neural networks' vulnerability to adversarial perturbation is their linear nature. This explanation is supported by new quantitative results while giving the first explanation of the most intriguing fact about them: their generalization across architectures and training sets. Moreover, this view yields a simple and fast method of generating adversarial examples. Using this approach to provide examples for adversarial training, we reduce the test set error of a maxout network on the MNIST dataset.},
	urldate = {2020-07-20},
	journal = {arXiv:1412.6572 [cs, stat]},
	author = {Goodfellow, Ian J. and Shlens, Jonathon and Szegedy, Christian},
	month = mar,
	year = {2015},
	note = {arXiv: 1412.6572},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/GLFAGZ84/Goodfellow et al. - 2015 - Explaining and Harnessing Adversarial Examples.pdf:application/pdf;arXiv.org Snapshot:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/8IJ2HB3C/1412.html:text/html}
}

@inproceedings{lu_inverting_1999,
	title = {Inverting feedforward neural networks using linear and nonlinear programming”, {Neural} {Networks}},
	abstract = {Abstract—The problem of inverting trained feedforward neu-ral networks is to find the inputs which yield a given output. In general, this problem is an ill-posed problem because the mapping from the output space to the input space is a one-to-many mapping. In this paper, we present a method for dealing with the inverse problem by using mathematical programming techniques. The principal idea behind the method is to formulate the inverse problem as a nonlinear programming (NLP) problem, a separable programming (SP) problem, or a linear programming (LP) problem according to the architectures of networks to be inverted or the types of network inversions to be computed. An important advantage of the method over the existing iterative in-version algorithm is that various designated network inversions of multilayer perceptrons (MLP’s) and radial basis function (RBF) neural networks can be obtained by solving the corresponding SP problems, which can be solved by a modified simplex method, a well-developed and efficient method for solving LP problems. We present several examples to demonstrate the proposed method and the applications of network inversions to examining and improving the generalization performance of trained networks. The results show the effectiveness of the proposed method. Index Terms — Boundary training data, feedforward neural networks, generalization, inverse problem, iterative inversion algorithm, linear programming, neural-network inversions, non-linear programming, separable programming. I.},
	booktitle = {{IEEE} {Transactions} on , {Volume}: 10 {Issue}: 6},
	author = {Lu, Bao-liang and Kita, Hajime and Nishikawa, Yoshikazu},
	year = {1999},
	pages = {1271--1290},
	file = {Citeseer - Full Text PDF:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/84FJ6IMI/Lu et al. - 1999 - Inverting feedforward neural networks using linear.pdf:application/pdf;Citeseer - Snapshot:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/TNV42YGK/summary.html:text/html}
}

@article{jensen_inversion_1999,
	title = {Inversion of feedforward neural networks: algorithms and applications},
	volume = {87},
	issn = {00189219},
	shorttitle = {Inversion of feedforward neural networks},
	language = {en},
	number = {9},
	urldate = {2020-07-23},
	journal = {IEEE},
	author = {Jensen, C.A. and Reed, R.D. and Marks, R.J. and El-Sharkawi, M.A. and {Jae-Byung Jung} and Miyamoto, R.T. and Anderson, G.M. and Eggen, C.J.},
	month = sep,
	year = {1999},
	pages = {1536--1549},
	file = {Jensen et al. - 1999 - Inversion of feedforward neural networks algorith.pdf:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/VM4TI66I/Jensen et al. - 1999 - Inversion of feedforward neural networks algorith.pdf:application/pdf}
}

@article{dinh_nice_2015,
	title = {{NICE}: {Non}-linear {Independent} {Components} {Estimation}},
	shorttitle = {{NICE}},

	abstract = {We propose a deep learning framework for modeling complex high-dimensional densities called Non-linear Independent Component Estimation (NICE). It is based on the idea that a good representation is one in which the data has a distribution that is easy to model. For this purpose, a non-linear deterministic transformation of the data is learned that maps it to a latent space so as to make the transformed data conform to a factorized distribution, i.e., resulting in independent latent variables. We parametrize this transformation so that computing the Jacobian determinant and inverse transform is trivial, yet we maintain the ability to learn complex non-linear transformations, via a composition of simple building blocks, each based on a deep neural network. The training criterion is simply the exact log-likelihood, which is tractable. Unbiased ancestral sampling is also easy. We show that this approach yields good generative models on four image datasets and can be used for inpainting.},
	urldate = {2020-08-03},
	journal = {arXiv:1410.8516 [cs]},
	author = {Dinh, Laurent and Krueger, David and Bengio, Yoshua},
	month = apr,
	year = {2015},
	note = {arXiv: 1410.8516},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/3CXK4556/Dinh et al. - 2015 - NICE Non-linear Independent Components Estimation.pdf:application/pdf;arXiv.org Snapshot:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/T8TCP3A3/1410.html:text/html}
}

@inproceedings{baird_one-step_2005,
	title = {One-step neural network inversion with {PDF} learning and emulation},
	volume = {2},
	abstract = {We present two new types of neural networks (both of which can be trained with ordinary error backpropagation) and we present a new algorithm for learning a probability density function (pdf) from example vectors. It is normally difficult to invert a neural network, but for the new bijective neural network, it is efficient to find an input producing any desired output, and such an input is guaranteed to exist and to be unique. Furthermore, it can be used as one component in building a pdf neural network, which is a neural network with a nonnegative output, and for which it is guaranteed that the integral of the output is exactly 1.0 (as in a pdf function). Both of these can be used for supervised learning using standard error backpropagation. Finally, the new pdf learning algorithm is capable of using those networks to learn a pdf given i.i.d. samples drawn from that pdf, and to then generate new vectors from the learned pdf. This, in turn, allows inversion of a function with non-unique inverses, where each inverse is generated with just a single evaluation of the network.},
	booktitle = {Proceedings. 2005 {IEEE} {International} {Joint} {Conference} on {Neural} {Networks}, 2005.},
	author = {Baird, L. and Smalenberger, D. and Ingkiriwang, S.},
	month = jul,
	year = {2005},
	note = {ISSN: 2161-4407},
	keywords = {learning (artificial intelligence), Emulation, neural nets, Neural networks, Backpropagation algorithms, bijective neural network, Computer errors, Computer science, Intelligent networks, Multi-layer neural network, one-step neural network inversion, probability density function, Probability density function, Springs, standard error backpropagation, supervised learning, World Wide Web},
	pages = {966--971 vol. 2},
	file = {IEEE Xplore Abstract Record:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/A6JDARVM/1555983.html:text/html;IEEE Xplore Full Text PDF:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/BR8TXRB8/Baird et al. - 2005 - One-step neural network inversion with PDF learnin.pdf:application/pdf}
}

@article{jacobsen_i-revnet_2018,
	title = {i-{RevNet}: {Deep} {Invertible} {Networks}},
	shorttitle = {i-{RevNet}},

	abstract = {It is widely believed that the success of deep convolutional networks is based on progressively discarding uninformative variability about the input with respect to the problem at hand. This is supported empirically by the difficulty of recovering images from their hidden representations, in most commonly used network architectures. In this paper we show via a one-to-one mapping that this loss of information is not a necessary condition to learn representations that generalize well on complicated problems, such as ImageNet. Via a cascade of homeomorphic layers, we build the i-RevNet, a network that can be fully inverted up to the final projection onto the classes, i.e. no information is discarded. Building an invertible architecture is difficult, for one, because the local inversion is ill-conditioned, we overcome this by providing an explicit inverse. An analysis of i-RevNets learned representations suggests an alternative explanation for the success of deep networks by a progressive contraction and linear separation with depth. To shed light on the nature of the model learned by the i-RevNet we reconstruct linear interpolations between natural image representations.},
	urldate = {2020-08-03},
	journal = {arXiv:1802.07088 [cs, stat]},
	author = {Jacobsen, Jörn-Henrik and Smeulders, Arnold and Oyallon, Edouard},
	month = feb,
	year = {2018},
	note = {arXiv: 1802.07088},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/Z3MIU2M6/Jacobsen et al. - 2018 - i-RevNet Deep Invertible Networks.pdf:application/pdf;arXiv.org Snapshot:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/WS43D4CI/1802.html:text/html}
}

@article{kirichenko_why_2020,
	title = {Why {Normalizing} {Flows} {Fail} to {Detect} {Out}-of-{Distribution} {Data}},

	abstract = {Detecting out-of-distribution (OOD) data is crucial for robust machine learning systems. Normalizing flows are flexible deep generative models that often surprisingly fail to distinguish between in- and out-of-distribution data: a flow trained on pictures of clothing assigns higher likelihood to handwritten digits. We investigate why normalizing flows perform poorly for OOD detection. We demonstrate that flows learn local pixel correlations and generic image-to-latent-space transformations which are not specific to the target image dataset. We show that by modifying the architecture of flow coupling layers we can bias the flow towards learning the semantic structure of the target data, improving OOD detection. Our investigation reveals that properties that enable flows to generate high-fidelity images can have a detrimental effect on OOD detection.},
	urldate = {2020-08-08},
	journal = {arXiv:2006.08545 [cs, stat]},
	author = {Kirichenko, Polina and Izmailov, Pavel and Wilson, Andrew Gordon},
	month = jun,
	year = {2020},
	note = {arXiv: 2006.08545},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/T7A4IMTF/Kirichenko et al. - 2020 - Why Normalizing Flows Fail to Detect Out-of-Distri.pdf:application/pdf;arXiv.org Snapshot:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/HSDFSSZ6/2006.html:text/html}
}

@article{jacobsen_i-revnet_2018-1,
	title = {i-{RevNet}: {Deep} {Invertible} {Networks}},
	shorttitle = {i-{RevNet}},

	abstract = {It is widely believed that the success of deep convolutional networks is based on progressively discarding uninformative variability about the input with respect to the problem at hand. This is supported empirically by the difficulty of recovering images from their hidden representations, in most commonly used network architectures. In this paper we show via a one-to-one mapping that this loss of information is not a necessary condition to learn representations that generalize well on complicated problems, such as ImageNet. Via a cascade of homeomorphic layers, we build the i-RevNet, a network that can be fully inverted up to the final projection onto the classes, i.e. no information is discarded. Building an invertible architecture is difficult, for one, because the local inversion is ill-conditioned, we overcome this by providing an explicit inverse. An analysis of i-RevNets learned representations suggests an alternative explanation for the success of deep networks by a progressive contraction and linear separation with depth. To shed light on the nature of the model learned by the i-RevNet we reconstruct linear interpolations between natural image representations.},
	urldate = {2020-08-08},
	journal = {arXiv:1802.07088 [cs, stat]},
	author = {Jacobsen, Jörn-Henrik and Smeulders, Arnold and Oyallon, Edouard},
	month = feb,
	year = {2018},
	note = {arXiv: 1802.07088},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/SP7N89BA/Jacobsen et al. - 2018 - i-RevNet Deep Invertible Networks.pdf:application/pdf;arXiv.org Snapshot:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/5V3LNJ58/1802.html:text/html}
}

@article{perarnau_invertible_2016,
	title = {Invertible {Conditional} {GANs} for image editing},

	abstract = {Generative Adversarial Networks (GANs) have recently demonstrated to successfully approximate complex data distributions. A relevant extension of this model is conditional GANs (cGANs), where the introduction of external information allows to determine specific representations of the generated images. In this work, we evaluate encoders to inverse the mapping of a cGAN, i.e., mapping a real image into a latent space and a conditional representation. This allows, for example, to reconstruct and modify real images of faces conditioning on arbitrary attributes. Additionally, we evaluate the design of cGANs. The combination of an encoder with a cGAN, which we call Invertible cGAN (IcGAN), enables to re-generate real images with deterministic complex modifications.},
	urldate = {2020-08-08},
	journal = {arXiv:1611.06355 [cs]},
	author = {Perarnau, Guim and van de Weijer, Joost and Raducanu, Bogdan and Álvarez, Jose M.},
	month = nov,
	year = {2016},
	note = {arXiv: 1611.06355},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/QA3LMDZ6/Perarnau et al. - 2016 - Invertible Conditional GANs for image editing.pdf:application/pdf;arXiv.org Snapshot:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/DCJN44TK/1611.html:text/html}
}

@article{grover_flow-gan_2018,
	title = {Flow-{GAN}: {Combining} {Maximum} {Likelihood} and {Adversarial} {Learning} in {Generative} {Models}},
	shorttitle = {Flow-{GAN}},

	abstract = {Adversarial learning of probabilistic models has recently emerged as a promising alternative to maximum likelihood. Implicit models such as generative adversarial networks (GAN) often generate better samples compared to explicit models trained by maximum likelihood. Yet, GANs sidestep the characterization of an explicit density which makes quantitative evaluations challenging. To bridge this gap, we propose Flow-GANs, a generative adversarial network for which we can perform exact likelihood evaluation, thus supporting both adversarial and maximum likelihood training. When trained adversarially, Flow-GANs generate high-quality samples but attain extremely poor log-likelihood scores, inferior even to a mixture model memorizing the training data; the opposite is true when trained by maximum likelihood. Results on MNIST and CIFAR-10 demonstrate that hybrid training can attain high held-out likelihoods while retaining visual fidelity in the generated samples.},
	urldate = {2020-08-08},
	journal = {arXiv:1705.08868 [cs, stat]},
	author = {Grover, Aditya and Dhar, Manik and Ermon, Stefano},
	month = jan,
	year = {2018},
	note = {arXiv: 1705.08868},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Neural and Evolutionary Computing, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/ZWKQYXNH/Grover et al. - 2018 - Flow-GAN Combining Maximum Likelihood and Adversa.pdf:application/pdf;arXiv.org Snapshot:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/5YQIRFVZ/1705.html:text/html}
}

@article{dinh_density_2017,
	title = {Density estimation using {Real} {NVP}},

	abstract = {Unsupervised learning of probabilistic models is a central yet challenging problem in machine learning. Specifically, designing models with tractable learning, sampling, inference and evaluation is crucial in solving this task. We extend the space of such models using real-valued non-volume preserving (real NVP) transformations, a set of powerful invertible and learnable transformations, resulting in an unsupervised learning algorithm with exact log-likelihood computation, exact sampling, exact inference of latent variables, and an interpretable latent space. We demonstrate its ability to model natural images on four datasets through sampling, log-likelihood evaluation and latent variable manipulations.},
	urldate = {2020-08-09},
	journal = {arXiv:1605.08803 [cs, stat]},
	author = {Dinh, Laurent and Sohl-Dickstein, Jascha and Bengio, Samy},
	month = feb,
	year = {2017},
	note = {arXiv: 1605.08803},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Neural and Evolutionary Computing, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/IXBL8Q7S/Dinh et al. - 2017 - Density estimation using Real NVP.pdf:application/pdf;arXiv.org Snapshot:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/CDVV8L3T/1605.html:text/html}
}

@article{tanielian_learning_2020,
	title = {Learning disconnected manifolds: a no {GANs} land},
	shorttitle = {Learning disconnected manifolds},

	abstract = {Typical architectures of Generative AdversarialNetworks make use of a unimodal latent distribution transformed by a continuous generator. Consequently, the modeled distribution always has connected support which is cumbersome when learning a disconnected set of manifolds. We formalize this problem by establishing a no free lunch theorem for the disconnected manifold learning stating an upper bound on the precision of the targeted distribution. This is done by building on the necessary existence of a low-quality region where the generator continuously samples data between two disconnected modes. Finally, we derive a rejection sampling method based on the norm of generators Jacobian and show its efficiency on several generators including BigGAN.},
	urldate = {2020-08-14},
	journal = {arXiv:2006.04596 [cs, stat]},
	author = {Tanielian, Ugo and Issenhuth, Thibaut and Dohmatob, Elvis and Mary, Jeremie},
	month = jun,
	year = {2020},
	note = {arXiv: 2006.04596},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/GPIX6R2N/Tanielian et al. - 2020 - Learning disconnected manifolds a no GANs land.pdf:application/pdf;arXiv.org Snapshot:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/4BFDA4T6/2006.html:text/html}
}

@book{demeniconi_proceedings_2020,
	address = {Philadelphia, PA},
	title = {Proceedings of the 2020 {SIAM} {International} {Conference} on {Data} {Mining}},
	isbn = {978-1-61197-623-6},

	abstract = {Deep learning is increasingly used for unsupervised feature extraction and anomaly detection in big datasets. Most deep learning based anomaly detection techniques separately train a neural network for feature extraction, then apply a traditional anomaly detection method on the extracted features. These hybrid techniques have achieved higher accuracy than traditional anomaly detection methods and reconstruction-error-based deep autoencoders. However, recent research demonstrates that jointly optimising the objectives of the deep network and the anomaly detection technique in a hybrid architecture substantially improves detection performance. Existing methods that use this objective assume that the normal (i.e., non-anomalous) data comes from a single distribution. In this paper, we show that violation of this assumption negatively aﬀects performance of these methods and creates model bias in the favour of anomalies. We propose Deep Multi-sphere Support Vector Data Description, which jointly optimises the objectives of the deep network and anomaly detection. It generates useful and discriminative features by embeding normal data with a multi-modal distribution into multiple data-enclosing hyperspheres with minimum volume. We empirically show that our proposed method outperforms state-of-the-art shallow and deep anomaly detection methods.},
	language = {en},
	urldate = {2020-08-14},
	publisher = {Society for Industrial and Applied Mathematics},
	editor = {Demeniconi, Carlotta and Chawla, Nitesh},
	month = jan,
	year = {2020},
	file = {Demeniconi and Chawla - 2020 - Proceedings of the 2020 SIAM International Confere.pdf:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/8RSRVQHN/Demeniconi and Chawla - 2020 - Proceedings of the 2020 SIAM International Confere.pdf:application/pdf}
}

@inproceedings{hara_intrusion_2020,
	title = {Intrusion {Detection} {System} using {Semi}-{Supervised} {Learning} with {Adversarial} {Auto}-encoder},
	abstract = {As computer networks become vulnerable to attacks from intruders, Intrusion Detection Systems (IDS) is a critical component that monitors activities of computer networks and classifies them as either normal or anomalous. Remarkable advancement of machine learning makes us consider to use supervised machine learning to build IDS. Supervised machine learning requires a large amount of training data, leading to costly human-labor operation; it requires the human operator to examine data, classify them, and annotate them with a label. To address this issue, we propose an IDS that employs semi-supervised learning. Semi-supervised learning uses a small number of labeled data in training dataset to reduce costly human-labor tasks and improves the performance with support of unlabeled data in training dataset. The proposed method employs Adversarial Auto-encoder (AAE), a semi-supervised learning algorithm that incorporates the Generative Adversarial Nets (GAN) into the Auto-encoder (AE). We evaluate the effectiveness of the proposed method using NSL-KDD dataset. We confirm that the proposed method that uses only 0.1 percent of labeled data achieves comparable performance with existing IDSs that use machine learning methods.},
	booktitle = {{NOMS} 2020 - 2020 {IEEE}/{IFIP} {Network} {Operations} and {Management} {Symposium}},
	author = {Hara, Kazuki and Shiomoto, Kohei},
	month = apr,
	year = {2020},
	note = {ISSN: 2374-9709},
	keywords = {Machine learning, supervised machine learning, neural nets, pattern classification, training data, supervised learning, adversarial auto-encoder, Adversarial Auto-encoder, computer network security, computer networks, costly human-labor operation, generative adversarial nets, human operator, human-labor tasks, IDS, Intrusion Detection System, intrusion detection systems, machine learning, Semi-supervised learning, semisupervised learning, training dataset},
	pages = {1--8},
	file = {IEEE Xplore Abstract Record:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/MCC4L3DE/9110343.html:text/html;IEEE Xplore Full Text PDF:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/EN629TVQ/Hara and Shiomoto - 2020 - Intrusion Detection System using Semi-Supervised L.pdf:application/pdf}
}

@article{leveau_adversarial_2017,
	title = {{ADVERSARIAL} {AUTOENCODERS} {FOR} {NOVELTY} {DE}- {TECTION}},
	abstract = {In this paper, we address the problem of novelty detection, i.e recognizing at test time if a data item comes from the training data distribution or not. We focus on Adversarial autoencoders (AAE) that have the advantage to explicitly control the distribution of the known data in the feature space. We show that when they are trained in a (semi-)supervised way, they provide consistent novelty detection improvements compared to a classical autoencoder. We further improve their performance by introducing an explicit rejection class in the prior distribution coupled with random input images to the autoencoder.},
	language = {en},
	author = {Leveau, Valentin and Joly, Alexis},
	year = {2017},
	pages = {6},
	file = {Leveau and Joly - 2017 - ADVERSARIAL AUTOENCODERS FOR NOVELTY DE- TECTION.pdf:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/IVULI3K4/Leveau and Joly - 2017 - ADVERSARIAL AUTOENCODERS FOR NOVELTY DE- TECTION.pdf:application/pdf}
}

@incollection{pidhorskyi_generative_2018,
	title = {Generative {Probabilistic} {Novelty} {Detection} with {Adversarial} {Autoencoders}},

	urldate = {2020-08-23},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 31},
	publisher = {Curran Associates, Inc.},
	author = {Pidhorskyi, Stanislav and Almohsen, Ranya and Doretto, Gianfranco},
	editor = {Bengio, S. and Wallach, H. and Larochelle, H. and Grauman, K. and Cesa-Bianchi, N. and Garnett, R.},
	year = {2018},
	pages = {6822--6833},
	file = {NIPS Full Text PDF:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/IJ8MFHHS/Pidhorskyi et al. - 2018 - Generative Probabilistic Novelty Detection with Ad.pdf:application/pdf;NIPS Snapshot:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/G3E7DCM9/7915-generative-probabilistic-novelty-detection-with-adversarial-autoencoders.html:text/html}
}

@article{peng_federated_2019,
	title = {Federated {Adversarial} {Domain} {Adaptation}},

	abstract = {Federated learning improves data privacy and efficiency in machine learning performed over networks of distributed devices, such as mobile phones, IoT and wearable devices, etc. Yet models trained with federated learning can still fail to generalize to new devices due to the problem of domain shift. Domain shift occurs when the labeled data collected by source nodes statistically differs from the target node's unlabeled data. In this work, we present a principled approach to the problem of federated domain adaptation, which aims to align the representations learned among the different nodes with the data distribution of the target node. Our approach extends adversarial adaptation techniques to the constraints of the federated setting. In addition, we devise a dynamic attention mechanism and leverage feature disentanglement to enhance knowledge transfer. Empirically, we perform extensive experiments on several image and text classification tasks and show promising results under unsupervised federated domain adaptation setting.},
	urldate = {2020-08-24},
	journal = {arXiv:1911.02054 [cs]},
	author = {Peng, Xingchao and Huang, Zijun and Zhu, Yizhe and Saenko, Kate},
	month = dec,
	year = {2019},
	note = {arXiv: 1911.02054},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/SQSSZJU8/Peng et al. - 2019 - Federated Adversarial Domain Adaptation.pdf:application/pdf;arXiv.org Snapshot:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/8KRVQUVX/1911.html:text/html}
}

@inproceedings{oshea_learning_2017,
	title = {Learning robust general radio signal detection using computer vision methods},
	abstract = {We introduce a new method for radio signal detection and localization within the time-frequency spectrum based on the use of convolutional neural networks for bounding box regression. Recently, this class of approach has surpassed human-level performance on computer vision benchmarks for object detection, but similar techniques have not yet been adopted for radio applications. We introduce the basic approach explain how labeled training data containing wideband spectrum annotated with masks and bounding boxes can be used to train a highly effective radio signal detector which achieves higher levels of contextual understanding and improved sensitivity performance when compared with more traditional nave energy thresholding based signal detection schemes. We extend prior work from the computer vision field, employing a variation of the You Only Look Once (YOLO) architecture which is a fast and accurate visual object detector. Results are shown from illustrating the effectiveness from our entry into the DARPA Battle-of-the-ModRecs competition and over the air datasets.},
	booktitle = {2017 51st {Asilomar} {Conference} on {Signals}, {Systems}, and {Computers}},
	author = {O'Shea, Tim and Roy, Tamohgna and Clancy, T. Charles},
	month = oct,
	year = {2017},
	note = {ISSN: 2576-2303},
	keywords = {learning (artificial intelligence), neural nets, Training, Task analysis, Detectors, bounding boxes, box regression, computer vision, Computer vision, computer vision benchmarks, computer vision field, computer vision methods, convolutional neural networks, fast object detector, feature extraction, human-level performance, image classification, labeled training data, object detection, Object detection, radio applications, regression analysis, robust general radio signal detection, Sensitivity, Spectrogram, time-frequency spectrum, traditional nave energy thresholding based signal detection schemes, visual object detector, YOLO architecture, you only look once architecture},
	pages = {829--832},
	file = {IEEE Xplore Abstract Record:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/5KYBF4TP/8335463.html:text/html;IEEE Xplore Full Text PDF:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/TFGD8HGX/O'Shea et al. - 2017 - Learning robust general radio signal detection usi.pdf:application/pdf}
}

@article{zhang_age_2017,
	title = {Age {Progression}/{Regression} by {Conditional} {Adversarial} {Autoencoder}},

	abstract = {"If I provide you a face image of mine (without telling you the actual age when I took the picture) and a large amount of face images that I crawled (containing labeled faces of different ages but not necessarily paired), can you show me what I would look like when I am 80 or what I was like when I was 5?" The answer is probably a "No." Most existing face aging works attempt to learn the transformation between age groups and thus would require the paired samples as well as the labeled query image. In this paper, we look at the problem from a generative modeling perspective such that no paired samples is required. In addition, given an unlabeled image, the generative model can directly produce the image with desired age attribute. We propose a conditional adversarial autoencoder (CAAE) that learns a face manifold, traversing on which smooth age progression and regression can be realized simultaneously. In CAAE, the face is first mapped to a latent vector through a convolutional encoder, and then the vector is projected to the face manifold conditional on age through a deconvolutional generator. The latent vector preserves personalized face features (i.e., personality) and the age condition controls progression vs. regression. Two adversarial networks are imposed on the encoder and generator, respectively, forcing to generate more photo-realistic faces. Experimental results demonstrate the appealing performance and flexibility of the proposed framework by comparing with the state-of-the-art and ground truth.},
	urldate = {2020-08-25},
	journal = {arXiv:1702.08423 [cs]},
	author = {Zhang, Zhifei and Song, Yang and Qi, Hairong},
	month = mar,
	year = {2017},
	note = {arXiv: 1702.08423},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/D8UN7MVB/Zhang et al. - 2017 - Age ProgressionRegression by Conditional Adversar.pdf:application/pdf;arXiv.org Snapshot:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/IPILJHNP/1702.html:text/html}
}

@article{oshea_radio_nodate,
	title = {Radio {Machine} {Learning} {Dataset} {Generation} with {GNU} {Radio}},
	abstract = {This paper surveys emerging applications of Machine Learning (ML) to the Radio Signal Processing domain. Provides some brief background on enabling methods and discusses some of the potential advancements for the ﬁeld. It discusses the critical importance of good datasets for model learning, testing, and evaluation and introduces several public open source synthetic datasets for various radio machine learning tasks. These are intended to provide a robust common baselines for those working in the ﬁeld and to provide a benchmark measure against which many techniques can be rapidly evaluated and compared.},
	language = {en},
	author = {O'Shea, Tim and West, Nathan},
	pages = {6},
	file = {O'Shea and West - Radio Machine Learning Dataset Generation with GNU.pdf:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/NFPNY7YC/O'Shea and West - Radio Machine Learning Dataset Generation with GNU.pdf:application/pdf}
}

@article{kauffmann_clever_2020,
	title = {The {Clever} {Hans} {Effect} in {Anomaly} {Detection}},

	abstract = {The 'Clever Hans' effect occurs when the learned model produces correct predictions based on the 'wrong' features. This effect which undermines the generalization capability of an ML model and goes undetected by standard validation techniques has been frequently observed for supervised learning where the training algorithm leverages spurious correlations in the data. The question whether Clever Hans also occurs in unsupervised learning, and in which form, has received so far almost no attention. Therefore, this paper will contribute an explainable AI (XAI) procedure that can highlight the relevant features used by popular anomaly detection models of different type. Our analysis reveals that the Clever Hans effect is widespread in anomaly detection and occurs in many (unexpected) forms. Interestingly, the observed Clever Hans effects are in this case not so much due to the data, but due to the anomaly detection models themselves whose structure makes them unable to detect the truly relevant features, even though vast amounts of data points are available. Overall, our work contributes a warning against an unrestrained use of existing anomaly detection models in practical applications, but it also points at a possible way out of the Clever Hans dilemma, specifically, by allowing multiple anomaly models to mutually cancel their individual structural weaknesses to jointly produce a better and more trustworthy anomaly detector.},
	urldate = {2020-09-11},
	journal = {arXiv:2006.10609 [cs, stat]},
	author = {Kauffmann, Jacob and Ruff, Lukas and Montavon, Grégoire and Müller, Klaus-Robert},
	month = jun,
	year = {2020},
	note = {arXiv: 2006.10609},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/TSHFI9XC/Kauffmann et al. - 2020 - The Clever Hans Effect in Anomaly Detection.pdf:application/pdf;arXiv.org Snapshot:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/S4WFH249/2006.html:text/html}
}

@article{kirkpatrick_overcoming_2017,
	title = {Overcoming catastrophic forgetting in neural networks},
	volume = {114},
	issn = {0027-8424, 1091-6490},
	abstract = {The ability to learn tasks in a sequential fashion is crucial to the development of artificial intelligence. Until now neural networks have not been capable of this and it has been widely thought that catastrophic forgetting is an inevitable feature of connectionist models. We show that it is possible to overcome this limitation and train networks that can maintain expertise on tasks that they have not experienced for a long time. Our approach remembers old tasks by selectively slowing down learning on the weights important for those tasks. We demonstrate our approach is scalable and effective by solving a set of classification tasks based on a hand-written digit dataset and by learning several Atari 2600 games sequentially.},
	language = {en},
	number = {13},
	urldate = {2020-09-14},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Kirkpatrick, James and Pascanu, Razvan and Rabinowitz, Neil and Veness, Joel and Desjardins, Guillaume and Rusu, Andrei A. and Milan, Kieran and Quan, John and Ramalho, Tiago and Grabska-Barwinska, Agnieszka and Hassabis, Demis and Clopath, Claudia and Kumaran, Dharshan and Hadsell, Raia},
	month = mar,
	year = {2017},
	pages = {3521--3526},
	file = {Kirkpatrick et al. - 2017 - Overcoming catastrophic forgetting in neural netwo.pdf:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/YBF3S76B/Kirkpatrick et al. - 2017 - Overcoming catastrophic forgetting in neural netwo.pdf:application/pdf}
}

@article{kemker_measuring_2017,
	title = {Measuring {Catastrophic} {Forgetting} in {Neural} {Networks}},

	abstract = {Deep neural networks are used in many state-of-the-art systems for machine perception. Once a network is trained to do a specific task, e.g., bird classification, it cannot easily be trained to do new tasks, e.g., incrementally learning to recognize additional bird species or learning an entirely different task such as flower recognition. When new tasks are added, typical deep neural networks are prone to catastrophically forgetting previous tasks. Networks that are capable of assimilating new information incrementally, much like how humans form new memories over time, will be more efficient than re-training the model from scratch each time a new task needs to be learned. There have been multiple attempts to develop schemes that mitigate catastrophic forgetting, but these methods have not been directly compared, the tests used to evaluate them vary considerably, and these methods have only been evaluated on small-scale problems (e.g., MNIST). In this paper, we introduce new metrics and benchmarks for directly comparing five different mechanisms designed to mitigate catastrophic forgetting in neural networks: regularization, ensembling, rehearsal, dual-memory, and sparse-coding. Our experiments on real-world images and sounds show that the mechanism(s) that are critical for optimal performance vary based on the incremental training paradigm and type of data being used, but they all demonstrate that the catastrophic forgetting problem has yet to be solved.},
	urldate = {2020-09-14},
	journal = {arXiv:1708.02072 [cs]},
	author = {Kemker, Ronald and McClure, Marc and Abitino, Angelina and Hayes, Tyler and Kanan, Christopher},
	month = nov,
	year = {2017},
	note = {arXiv: 1708.02072},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/TWKWG9CP/Kemker et al. - 2017 - Measuring Catastrophic Forgetting in Neural Networ.pdf:application/pdf;arXiv.org Snapshot:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/TH8LQQUR/1708.html:text/html}
}

@inproceedings{robins_catastrophic_1993,
	title = {Catastrophic forgetting in neural networks: the role of rehearsal mechanisms},
	shorttitle = {Catastrophic forgetting in neural networks},
	abstract = {The author examines the problem of catastrophic forgetting-the overwriting of old information-in neural networks. He notes that R. Ratcliff's (1990) experiments with rehearsal regimes are a possible solution to catastrophic forgetting and describes sweep rehearsal-a much more effective regime. The use of sweep rehearsal, however, eventually encounters practical limits as the ability to recognize learned items begins to diminish. The author suggests that sweep rehearsal extends the approach of rehearsal mechanisms as far as is practicable, and exposes their eventual limitations.{\textless}{\textgreater}},
	booktitle = {Proceedings 1993 {The} {First} {New} {Zealand} {International} {Two}-{Stream} {Conference} on {Artificial} {Neural} {Networks} and {Expert} {Systems}},
	author = {Robins, A.},
	month = nov,
	year = {1993},
	keywords = {learning (artificial intelligence), Stability, neural nets, neural networks, Neural networks, Computer science, Intelligent networks, catastrophic forgetting, Formal specifications, Information processing, Learning systems, rehearsal mechanisms, Robustness, Supervised learning, sweep rehearsal, Unsupervised learning},
	pages = {65--68},
	file = {IEEE Xplore Abstract Record:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/H6XBK2NJ/323080.html:text/html;IEEE Xplore Full Text PDF:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/IHMK63N9/Robins - 1993 - Catastrophic forgetting in neural networks the ro.pdf:application/pdf}
}

@article{dong_survey_2018,
	title = {A {Survey} on {Simulation} {Tools} and {Testbeds} for {Cognitive} {Radio} {Networks} {Study}},

	abstract = {Efficient utility of radio spectrum has been a hot topic as the wireless communication spectrum is a precious resource. The past decade has witnessed intensive research in spectrum sharing techniques. Most of the techniques are based on cognitive radio networks (CRNs) because cognitive capabilities are essential for optimizing spectrum efficiency and guaranteeing safe coexistence in the presence of the spectrum uncertainty. However, due to the high complexity of the problem, most research has been limited to theoretical analysis. It is non-trivial to build a simulator that is capable of carrying out a comprehensive experimental study. In this paper, a survey is conducted to provide a big picture of the available simulators in CRNs research. By illustrating their major functionalities, the insight enables researchers to select tools that match their needs. In addition, with a better understanding of the advantages and constraints, this survey aims at providing a guideline for simulator designers who have been trying to meet the requirements from the CRN research community.},
	urldate = {2020-09-19},
	journal = {arXiv:1808.09858 [cs]},
	author = {Dong, Qi and Chen, Yu and Li, Xiaohua and Zeng, Kai},
	month = aug,
	year = {2018},
	note = {arXiv: 1808.09858},
	keywords = {Computer Science - Networking and Internet Architecture},
	file = {arXiv Fulltext PDF:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/XIZD555L/Dong et al. - 2018 - A Survey on Simulation Tools and Testbeds for Cogn.pdf:application/pdf;arXiv.org Snapshot:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/S2ZAUJ2P/1808.html:text/html}
}

@inproceedings{sakurada_anomaly_2014,
	address = {New York, NY, USA},
	series = {{MLSDA}'14},
	title = {Anomaly {Detection} {Using} {Autoencoders} with {Nonlinear} {Dimensionality} {Reduction}},
	isbn = {978-1-4503-3159-3},
	abstract = {This paper proposes to use autoencoders with nonlinear dimensionality reduction in the anomaly detection task. The authors apply dimensionality reduction by using an autoencoder onto both artificial data and real data, and compare it with linear PCA and kernel PCA to clarify its property. The artificial data is generated from Lorenz system, and the real data is the spacecrafts' telemetry data. This paper demonstrates that autoencoders are able to detect subtle anomalies which linear PCA fails. Also, autoencoders can increase their accuracy by extending them to denoising autoenconders. Moreover, autoencoders can be useful as nonlinear techniques without complex computation as kernel PCA requires. Finaly, the authors examine the learned features in the hidden layer of autoencoders, and present that autoencoders learn the normal state properly and activate differently with anomalous input.},
	urldate = {2020-09-21},
	booktitle = {Proceedings of the {MLSDA} 2014 2nd {Workshop} on {Machine} {Learning} for {Sensory} {Data} {Analysis}},
	publisher = {Association for Computing Machinery},
	author = {Sakurada, Mayu and Yairi, Takehisa},
	month = dec,
	year = {2014},
	keywords = {anomaly detection, auto-assosiative neural network, autoencoder, denoising autoencoder, dimensionality reduction, fault detection, nonlinear, novelty detection, spacecrafts},
	pages = {4--11},
	file = {Full Text PDF:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/T4645GYH/Sakurada and Yairi - 2014 - Anomaly Detection Using Autoencoders with Nonlinea.pdf:application/pdf}
}

@article{schreyer_detection_2019,
	title = {Detection of {Accounting} {Anomalies} in the {Latent} {Space} using {Adversarial} {Autoencoder} {Neural} {Networks}},

	abstract = {The detection of fraud in accounting data is a long-standing challenge in financial statement audits. Nowadays, the majority of applied techniques refer to handcrafted rules derived from known fraud scenarios. While fairly successful, these rules exhibit the drawback that they often fail to generalize beyond known fraud scenarios and fraudsters gradually find ways to circumvent them. In contrast, more advanced approaches inspired by the recent success of deep learning often lack seamless interpretability of the detected results. To overcome this challenge, we propose the application of adversarial autoencoder networks. We demonstrate that such artificial neural networks are capable of learning a semantic meaningful representation of real-world journal entries. The learned representation provides a holistic view on a given set of journal entries and significantly improves the interpretability of detected accounting anomalies. We show that such a representation combined with the networks reconstruction error can be utilized as an unsupervised and highly adaptive anomaly assessment. Experiments on two datasets and initial feedback received by forensic accountants underpinned the effectiveness of the approach.},
	urldate = {2020-09-21},
	journal = {arXiv:1908.00734 [cs, q-fin, stat]},
	author = {Schreyer, Marco and Sattarov, Timur and Schulze, Christian and Reimer, Bernd and Borth, Damian},
	month = aug,
	year = {2019},
	note = {arXiv: 1908.00734},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Quantitative Finance - Statistical Finance},
	file = {arXiv Fulltext PDF:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/5LWWBH35/Schreyer et al. - 2019 - Detection of Accounting Anomalies in the Latent Sp.pdf:application/pdf;arXiv.org Snapshot:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/DXX9AVX6/1908.html:text/html}
}

@article{cullen_spectral_nodate,
	title = {Spectral {Occupancy} within {Cognitive} {Radio} {Networks}},
	abstract = {The rise of the Internet of Things, and the corresponding demand for wireless spectrum has lead to signiﬁcant issues relating to spectrum conﬂict and congestion. Accounting for this demand requires the development of intelligent Cognitive Radio systems and policies that can adapt to changes in demand, while balancing public and private needs.},
	language = {en},
	author = {Cullen, Andrew C and Alpcan, Tansu and Rubinstein, Benjamin and Sithamparanathan, Kandeepan and Flower, Barry and Leong, H W},
	pages = {17},
	file = {Cullen et al. - Spectral Occupancy within Cognitive Radio Networks.pdf:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/8PKLFHYX/Cullen et al. - Spectral Occupancy within Cognitive Radio Networks.pdf:application/pdf}
}


@inproceedings{silver_generating_2020,
	title = {Generating {Accurate} {Pseudo} {Examples} for {Continual} {Learning}},
	abstract = {Continual learning (CL) is concerned with the persistent and cumulative nature of learning. This requires a method of successfully consolidating new knowledge into long-term memory without the loss of prior knowledge. Prior research has addressed this CL retention problem through the efficient rehearsal of prior examples while learning the examples of a new task within a long-term Multiple Task Learning (MTL) network. The approach maintains or improves prior knowledge while allowing its representation to remain plastic for the integration of new task examples. Preferably, rehearsal is done using pseudo examples synthesized by the MTL network; eliminating the need to retain prior task training examples or a generate them with an additional model. Previous work has shown that to properly retain knowledge the pseudo examples must adhere to the input probability distribution of those original examples. Two approaches are investigated for creating appropriate pseudo examples from a Restricted Boltzmann Machine (RBM) autoencoder, which can reside in the lowest layers of the long-term MTL Deep Belief network. We show that appropriate pseudo examples can be reconstructed by passing uniform random examples to a generative RBM model and selecting only those with reconstruction error less than the mean training error. These pseudo examples are shown to adhere to the probability distribution of the input variables of the original training examples and retain prior task knowledge during rehearsal as well as those examples. As part of the research, we develop and test a new metric called the Autoencoder Divergence Measure for comparing the probability distributions of two datasets given to a generative RBM network based on their reconstruction mean squared error.},
	booktitle = {2020 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} {Workshops} ({CVPRW})},
	author = {Silver, D. L. and Mahfuz, S.},
	month = jun,
	year = {2020},
	note = {ISSN: 2160-7516},
	keywords = {accurate pseudoexamples generation, appropriate pseudoexamples, belief networks, Boltzmann machines, CL retention problem, continual learning, Generative adversarial networks, generative RBM model, generative RBM network, Input variables, Knowledge engineering, learning (artificial intelligence), long-term memory, long-term MTL deep belief network, long-term multiple task learning network, MTL network, Neural networks, original examples, original training examples, prior task knowledge, prior task training examples, probability, probability distribution, Probability distribution, rehearsal, Task analysis, task examples, Training, uniform random examples},
	pages = {1035--1042},
	file = {IEEE Xplore Abstract Record:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/3FYEG5QS/9150934.html:text/html;IEEE Xplore Full Text PDF:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/N4FNEXVH/Silver and Mahfuz - 2020 - Generating Accurate Pseudo Examples for Continual .pdf:application/pdf}
}

@article{choi_autoencoder-based_2019,
	title = {Autoencoder-{Based} {Incremental} {Class} {Learning} without {Retraining} on {Old} {Data}},

	abstract = {Incremental class learning, a scenario in continual learning context where classes and their training data are sequentially and disjointedly observed, challenges a problem widely known as catastrophic forgetting. In this work, we propose a novel incremental class learning method that can significantly reduce memory overhead compared to previous approaches. Apart from conventional classification scheme using softmax, our model bases on an autoencoder to extract prototypes for given inputs so that no change in its output unit is required. It stores only the mean of prototypes per class to perform metric-based classification, unlike rehearsal approaches which rely on large memory or generative model. To mitigate catastrophic forgetting, regularization methods are applied on our model when a new task is encountered. We evaluate our method by experimenting on CIFAR-100 and CUB-200-2011 and show that its performance is comparable to the state-of-the-art method with much lower additional memory cost.},
	urldate = {2020-09-27},
	journal = {arXiv:1907.07872 [cs, stat]},
	author = {Choi, Euntae and Lee, Kyungmi and Choi, Kiyoung},
	month = jul,
	year = {2019},
	note = {arXiv: 1907.07872},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/WVCTDDS2/Choi et al. - 2019 - Autoencoder-Based Incremental Class Learning witho.pdf:application/pdf;arXiv.org Snapshot:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/F4A2V667/1907.html:text/html}
}

@article{robins_catastrophic_1995,
	title = {Catastrophic {Forgetting}, {Rehearsal} and {Pseudorehearsal}},
	volume = {7},
	issn = {0954-0091, 1360-0494},
	abstract = {This paper reviews the problem of catastrophic forgetting (the loss or disruption of previously learned information when new information is learned) in neural networks, and explores rehearsal mechanisms (the retraining of some of the previously learned information as the new information is added) as a potential solution. W e replicate some of the experiments described by Ratcliff (1990), including those relating to a simple `recency’ based rehearsal regime. W e then develop further rehearsal regimes which are more effective than recency rehearsal. In particular, `sweep rehearsal’ is very successful at minimizing catastrophic forgetting. One possible limitation of rehearsal in general, however, is that previously learned information may not be available for retraining. W e describe a solution to this problem, `pseudorehearsal’ , a method which provides the advantages of rehearsal without actually requiring any access to the previously learned information (the original training population) itself. We then suggest an interpretation of these rehearsal mechanisms in the context of a function approximation based account of neural network learning. Both rehearsal and pseudorehearsal may have practical applications, allowing new information to be integrated into an existing network with minimum disruption of old inform a tion.},
	language = {en},
	number = {2},
	urldate = {2020-09-27},
	journal = {Connection Science},
	author = {Robins, Anthony},
	month = jun,
	year = {1995},
	pages = {123--146},
	file = {Robins - 1995 - Catastrophic Forgetting, Rehearsal and Pseudorehea.pdf:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/GQMDATJN/Robins - 1995 - Catastrophic Forgetting, Rehearsal and Pseudorehea.pdf:application/pdf}
}


@article{goodfellow_empirical_2015,
	title = {An {Empirical} {Investigation} of {Catastrophic} {Forgetting} in {Gradient}-{Based} {Neural} {Networks}},

	abstract = {Catastrophic forgetting is a problem faced by many machine learning models and algorithms. When trained on one task, then trained on a second task, many machine learning models "forget" how to perform the first task. This is widely believed to be a serious problem for neural networks. Here, we investigate the extent to which the catastrophic forgetting problem occurs for modern neural networks, comparing both established and recent gradient-based training algorithms and activation functions. We also examine the effect of the relationship between the first task and the second task on catastrophic forgetting. We find that it is always best to train using the dropout algorithm--the dropout algorithm is consistently best at adapting to the new task, remembering the old task, and has the best tradeoff curve between these two extremes. We find that different tasks and relationships between tasks result in very different rankings of activation function performance. This suggests the choice of activation function should always be cross-validated.},
	urldate = {2021-01-31},
	journal = {arXiv:1312.6211 [cs, stat]},
	author = {Goodfellow, Ian J. and Mirza, Mehdi and Xiao, Da and Courville, Aaron and Bengio, Yoshua},
	month = mar,
	year = {2015},
	note = {arXiv: 1312.6211},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/N43MGBF3/Goodfellow et al. - 2015 - An Empirical Investigation of Catastrophic Forgett.pdf:application/pdf;arXiv.org Snapshot:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/UWX4XWEC/1312.html:text/html}
}


@article{lecun_deep_2015,
	title = {Deep learning},
	volume = {521},
	copyright = {2015 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
	issn = {1476-4687},
	abstract = {Deep learning allows computational models that are composed of multiple processing layers to learn representations of data with multiple levels of abstraction. These methods have dramatically improved the state-of-the-art in speech recognition, visual object recognition, object detection and many other domains such as drug discovery and genomics. Deep learning discovers intricate structure in large data sets by using the backpropagation algorithm to indicate how a machine should change its internal parameters that are used to compute the representation in each layer from the representation in the previous layer. Deep convolutional nets have brought about breakthroughs in processing images, video, speech and audio, whereas recurrent nets have shone light on sequential data such as text and speech.},
	language = {en},
	number = {7553},
	urldate = {2021-01-31},
	journal = {Nature},
	author = {LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
	month = may,
	year = {2015},
	note = {Number: 7553
Publisher: Nature Publishing Group},
	pages = {436--444},
	file = {Full Text PDF:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/KXDE8WK6/LeCun et al. - 2015 - Deep learning.pdf:application/pdf;Snapshot:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/BJ6ZT2E9/nature14539.html:text/html}
}


@article{mellado_self-improving_2019,
	title = {Self-{Improving} {Generative} {Artificial} {Neural} {Network} for {Pseudo}-{Rehearsal} {Incremental} {Class} {Learning}},
	abstract = {Deep learning models are part of the family of artificial neural networks and, as such, it suffers of catastrophic interference when they learn sequentially. In addition, most of these models have a rigid architecture which prevents the incremental learning of new classes. To overcome these drawbacks, in this article we propose the Self-Improving Generative Artificial Neural Network (SIGANN), a type of end-to-end Deep Neural Network system which is able to ease the catastrophic forgetting problem when leaning new classes. In this method, we introduce a novelty detection model to automatically detect samples of new classes, moreover an adversarial auto-encoder is used to produce samples of previous classes. This system consists of three main modules: a classifier module implemented using a Deep Convolutional Neural Network, a generator module based on an adversarial autoencoder; and a novelty detection module, implemented using an OpenMax activation function. Using the EMNIST data set, the model was trained incrementally, starting with a small set of classes. The results of the simulation show that SIGANN is able to retain previous knowledge with a gradual forgetfulness for each learning sequence. Moreover, SIGANN can detect new classes that are hidden in the data and, therefore, proceed with incremental class learning.},
	language = {en},
	urldate = {2020-09-28},
	author = {Mellado, Diego and Saavedra, Carolina and Chabert, Steren and Torres, Romina and Salas, Rodrigo},
	month = jul,
	year = {2019},
	note = {Publisher: Preprints},
	file = {Full Text PDF:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/IWENVHRP/Mellado et al. - 2019 - Self-Improving Generative Artificial Neural Networ.pdf:application/pdf;Snapshot:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/XQYT8N7G/v1.html:text/html}
}

@inproceedings{wiewel_continual_2019,
	address = {Brighton, United Kingdom},
	title = {Continual {Learning} for {Anomaly} {Detection} with {Variational} {Autoencoder}},
	isbn = {978-1-4799-8131-1},
	abstract = {Detecting anomalies using a variational autoencoder (VAE) suffers from catastrophic forgetting when trained on a continually growing set of normal data where only the most recently added data is available. Solving this problem would allow the use of the VAE for anomaly detection in settings where it is difﬁcult or even impossible to retain all normal data at the same time. We propose an efﬁcient extension of a method for continual learning which alleviates catastrophic forgetting for anomaly detection using a VAE. We show on some anomaly detection problems that the deﬁnition of normal data can be continually expanded without requiring all previously seen data.},
	language = {en},
	urldate = {2020-09-28},
	booktitle = {{ICASSP} 2019 - 2019 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	publisher = {IEEE},
	author = {Wiewel, Felix and Yang, Bin},
	month = may,
	year = {2019},
	pages = {3837--3841},
	file = {Wiewel and Yang - 2019 - Continual Learning for Anomaly Detection with Vari.pdf:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/JK94YCQK/Wiewel and Yang - 2019 - Continual Learning for Anomaly Detection with Vari.pdf:application/pdf}
}


@article{atkinson_pseudo-rehearsal_2018,
	title = {Pseudo-{Rehearsal}: {Achieving} {Deep} {Reinforcement} {Learning} without {Catastrophic} {Forgetting}},
	volume = {428},
	issn = {09252312},
	shorttitle = {Pseudo-{Rehearsal}},
	abstract = {Neural networks can achieve excellent results in a wide variety of applications. However, when they attempt to sequentially learn, they tend to learn the new task while catastrophically forgetting previous ones. We propose a model that overcomes catastrophic forgetting in sequential reinforcement learning by combining ideas from continual learning in both the image classification domain and the reinforcement learning domain. This model features a dual memory system which separates continual learning from reinforcement learning and a pseudo-rehearsal system that "recalls" items representative of previous tasks via a deep generative network. Our model sequentially learns Atari 2600 games without demonstrating catastrophic forgetting and continues to perform above human level on all three games. This result is achieved without: demanding additional storage requirements as the number of tasks increases, storing raw data or revisiting past tasks. In comparison, previous state-of-the-art solutions are substantially more vulnerable to forgetting on these complex deep reinforcement learning tasks.},
	urldate = {2021-01-17},
	journal = {Neurocomputing},
	author = {Atkinson, Craig and McCane, Brendan and Szymanski, Lech and Robins, Anthony},
	month = mar,
	year = {2018},
	note = {arXiv: 1812.02464},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	pages = {291--307},
	file = {arXiv Fulltext PDF:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/5S5VXBTQ/Atkinson et al. - 2021 - Pseudo-Rehearsal Achieving Deep Reinforcement Lea.pdf:application/pdf;arXiv.org Snapshot:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/77RUH2KT/1812.html:text/html}
}


@article{shin_continual_2017,
	title = {Continual {Learning} with {Deep} {Generative} {Replay}},

	abstract = {Attempts to train a comprehensive artificial intelligence capable of solving multiple tasks have been impeded by a chronic problem called catastrophic forgetting. Although simply replaying all previous data alleviates the problem, it requires large memory and even worse, often infeasible in real world applications where the access to past data is limited. Inspired by the generative nature of hippocampus as a short-term memory system in primate brain, we propose the Deep Generative Replay, a novel framework with a cooperative dual model architecture consisting of a deep generative model ("generator") and a task solving model ("solver"). With only these two models, training data for previous tasks can easily be sampled and interleaved with those for a new task. We test our methods in several sequential learning settings involving image classification tasks.},
	urldate = {2021-02-01},
	journal = {arXiv:1705.08690 [cs]},
	author = {Shin, Hanul and Lee, Jung Kwon and Kim, Jaehong and Kim, Jiwon},
	month = dec,
	year = {2017},
	note = {arXiv: 1705.08690},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/ZDXFDDN7/Shin et al. - 2017 - Continual Learning with Deep Generative Replay.pdf:application/pdf;arXiv.org Snapshot:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/9WQW68XQ/1705.html:text/html}
}

@article{ebrahimi_adversarial_2020,
	title = {Adversarial {Continual} {Learning}},

	abstract = {Continual learning aims to learn new tasks without forgetting previously learned ones. We hypothesize that representations learned to solve each task in a sequence have a shared structure while containing some task-specific properties. We show that shared features are significantly less prone to forgetting and propose a novel hybrid continual learning framework that learns a disjoint representation for task-invariant and task-specific features required to solve a sequence of tasks. Our model combines architecture growth to prevent forgetting of task-specific skills and an experience replay approach to preserve shared skills. We demonstrate our hybrid approach is effective in avoiding forgetting and show it is superior to both architecture-based and memory-based approaches on class incrementally learning of a single dataset as well as a sequence of multiple datasets in image classification. Our code is available at {\textbackslash}url\{https://github.com/facebookresearch/Adversarial-Continual-Learning\}.},
	urldate = {2020-09-28},
	journal = {arXiv:2003.09553 [cs, stat]},
	author = {Ebrahimi, Sayna and Meier, Franziska and Calandra, Roberto and Darrell, Trevor and Rohrbach, Marcus},
	month = jul,
	year = {2020},
	note = {arXiv: 2003.09553},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/AK3WEMF4/Ebrahimi et al. - 2020 - Adversarial Continual Learning.pdf:application/pdf;arXiv.org Snapshot:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/WBYQIMF2/2003.html:text/html}
}


@article{maltoni_continuous_2019,
	title = {Continuous learning in single-incremental-task scenarios},
	volume = {116},
	issn = {0893-6080},
	abstract = {It was recently shown that architectural, regularization and rehearsal strategies can be used to train deep models sequentially on a number of disjoint tasks without forgetting previously acquired knowledge. However, these strategies are still unsatisfactory if the tasks are not disjoint but constitute a single incremental task (e.g., class-incremental learning). In this paper we point out the differences between multi-task and single-incremental-task scenarios and show that well-known approaches such as LWF, EWC and SI are not ideal for incremental task scenarios. A new approach, denoted as AR1, combining architectural and regularization strategies is then specifically proposed. AR1 overhead (in terms of memory and computation) is very small thus making it suitable for online learning. When tested on CORe50 and iCIFAR-100, AR1 outperformed existing regularization strategies by a good margin.},
	language = {en},
	urldate = {2021-01-30},
	journal = {Neural Networks},
	author = {Maltoni, Davide and Lomonaco, Vincenzo},
	month = aug,
	year = {2019},
	keywords = {Continuous learning, Deep learning, Incremental class learning, Lifelong learning, Object recognition, Single-incremental-task},
	pages = {56--73},
	file = {ScienceDirect Full Text PDF:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/27HRRSF4/Maltoni and Lomonaco - 2019 - Continuous learning in single-incremental-task sce.pdf:application/pdf;ScienceDirect Snapshot:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/YAQVXEWU/S0893608019300838.html:text/html}
}


@inproceedings{erfani_privacy-preserving_2014,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Privacy-{Preserving} {Collaborative} {Anomaly} {Detection} for {Participatory} {Sensing}},
	isbn = {978-3-319-06608-0},
	abstract = {In collaborative anomaly detection, multiple data sources submit their data to an on-line service, in order to detect anomalies with respect to the wider population. A major challenge is how to achieve reasonable detection accuracy without disclosing the actual values of the participants’ data. We propose a lightweight and scalable privacy-preserving collaborative anomaly detection scheme called Random Multiparty Perturbation (RMP), which uses a combination of nonlinear and participant-specific linear perturbation. Each participant uses an individually perturbed uniformly distributed random matrix, in contrast to existing approaches that use a common random matrix. A privacy analysis is given for Bayesian Estimation and Independent Component Analysis attacks. Experimental results on real and synthetic datasets using an auto-encoder show that RMP yields comparable results to non-privacy preserving anomaly detection.},
	language = {en},
	booktitle = {Advances in {Knowledge} {Discovery} and {Data} {Mining}},
	publisher = {Springer International Publishing},
	author = {Erfani, Sarah M. and Law, Yee Wei and Karunasekera, Shanika and Leckie, Christopher A. and Palaniswami, Marimuthu},
	editor = {Tseng, Vincent S. and Ho, Tu Bao and Zhou, Zhi-Hua and Chen, Arbee L. P. and Kao, Hung-Yu},
	year = {2014},
	keywords = {Anomaly detection, Collaborative learning, Horizontally partitioned data, Participatory sensing, Privacy-preserving data mining},
	pages = {581--593},
	file = {Springer Full Text PDF:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/IN66PB2V/Erfani et al. - 2014 - Privacy-Preserving Collaborative Anomaly Detection.pdf:application/pdf}
}

@inproceedings{lyu_improved_2016,
	address = {Sydney, Australia},
	title = {An improved scheme for privacy-preserving collaborative anomaly detection},
	isbn = {978-1-5090-1941-0},
	abstract = {The ubiquity of mobile sensing devices in the Internet of Things (IoT) enables an emerging data crowdsourcing paradigm called participatory sensing, where multiple individuals collect data and use a cloud service to analyse the union of the collected data. An example of such collaborative analysis is collaborative anomaly detection. Given the possibility that the cloud service is honest but curious, a major challenge is how to protect the participants’ privacy. The scheme called Random Multiparty Perturbation (RMP) addresses this challenge by allowing each participant to perturb his/her tabular data by passing the data through a nonlinear function, and projecting the data to a lower dimension using a participant-speciﬁc random matrix. Here, we propose an improvement to RMP by introducing a new nonlinear function. The improved scheme is assessed in terms of its recovery resistance to the maximum a priori (MAP) estimation attack. Experimental results and preliminary theoretical analysis indicate that RMP is resistant to collusion attacks and has better recovery resistance to MAP estimation attacks compared to the original scheme. It also achieves a good trade-off between accuracy and privacy.},
	language = {en},
	urldate = {2020-10-29},
	booktitle = {2016 {IEEE} {International} {Conference} on {Pervasive} {Computing} and {Communication} {Workshops} ({PerCom} {Workshops})},
	publisher = {IEEE},
	author = {Lyu, Lingjuan and Law, Yee Wei and Erfani, Sarah M. and Leckie, Christopher and Palaniswami, Marimuthu},
	month = mar,
	year = {2016},
	pages = {1--6},
	file = {Lyu et al. - 2016 - An improved scheme for privacy-preserving collabor.pdf:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/JDM4UMKQ/Lyu et al. - 2016 - An improved scheme for privacy-preserving collabor.pdf:application/pdf}
}

@inproceedings{alpcan_lightweight_2008,
	address = {Istanbul, Turkey},
	title = {A lightweight biometric signature scheme for user authentication over networks},
	isbn = {978-1-60558-241-2},
	abstract = {We introduce a lightweight biometric solution for user authentication over networks using online handwritten signatures. The algorithm proposed is based on a modiﬁed Hausdorff distance and has favorable characteristics such as low computational cost and minimal training requirements. Furthermore, we investigate an information theoretic model for capacity and performance analysis for biometric authentication which brings additional theoretical insights to the problem. A fully functional proof-of-concept prototype that relies on commonly available off-the-shelf hardware is developed as a client-server system that supports Web services. Initial experimental results show that the algorithm performs well despite its low computational requirements and is resilient against over-theshoulder attacks.},
	language = {en},
	urldate = {2020-11-17},
	booktitle = {Proceedings of the 4th international conference on {Security} and privacy in communication netowrks - {SecureComm} '08},
	publisher = {ACM Press},
	author = {Alpcan, Tansu and Kesici, Sinan and Bicher, Daniel and Mihçak, M. Kivanç and Bauckhage, Christian and Çamtepe, S. Ahmet},
	year = {2008},
	pages = {1},
	file = {Alpcan et al. - 2008 - A lightweight biometric signature scheme for user .pdf:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/Z766BZIF/Alpcan et al. - 2008 - A lightweight biometric signature scheme for user .pdf:application/pdf}
}

@article{quian_quiroga_no_2020,
	title = {No {Pattern} {Separation} in the {Human} {Hippocampus}},
	volume = {24},
	issn = {13646613},
	language = {en},
	number = {12},
	urldate = {2020-11-17},
	journal = {Trends in Cognitive Sciences},
	author = {Quian Quiroga, Rodrigo},
	month = dec,
	year = {2020},
	pages = {994--1007},
	file = {Quian Quiroga - 2020 - No Pattern Separation in the Human Hippocampus.pdf:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/LSCDCPCA/Quian Quiroga - 2020 - No Pattern Separation in the Human Hippocampus.pdf:application/pdf}
}

@article{socolofsky_how_nodate,
	title = {How to write a research journal article in engineering and science},
	abstract = {Writing a research article can be a daunting task, and often, writers are not certain what should be included and how the information should be conveyed. Fortunately, scientiﬁc and engineering journal articles follow an accepted format. They contain an introduction which includes a statement of the problem, a literature review, and a general outline of the paper, a methods section detailing the methods used, separate or combined results, discussion and application sections, and a ﬁnal summary and conclusions section. Here, each of these elements is described in detail using examples from the published literature as illustration. Guidance is also provided with respect to style, getting started, and the revision/review process.},
	language = {en},
	author = {Socolofsky, Scott A},
	pages = {17},
	file = {Socolofsky - How to write a research journal article in enginee.pdf:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/U6FRWBWN/Socolofsky - How to write a research journal article in enginee.pdf:application/pdf}
}

@article{ding_fast_2020,
	title = {Fast {Multi}-{Agent} {Temporal}-{Difference} {Learning} via {Homotopy} {Stochastic} {Primal}-{Dual} {Optimization}},

	abstract = {We consider a distributed multi-agent policy evaluation problem in reinforcement learning. In our setup, a group of agents with jointly observed states and private local actions and rewards collaborates to learn the value function of a given policy. When the dimension of state-action space is large, the temporal-difference learning with linear function approximation is widely used. Under the assumption that the samples are i.i.d., the best-known convergence rate for multi-agent temporal-difference learning is \$O(1/{\textbackslash}sqrt\{T\})\$ minimizing the mean square projected Bellman error. In this paper, we formulate the temporal-difference learning as a distributed stochastic saddle point problem, and propose a new homotopy primal-dual algorithm by adaptively restarting the gradient update from the average of previous iterations. We prove that our algorithm enjoys an \$O(1/T)\$ convergence rate up to logarithmic factors of \$T\$, thereby significantly improving the previously-known convergence results on multi-agent temporal-difference learning. Furthermore, since our result explicitly takes into account the Markovian nature of the sampling in policy evaluation, it addresses a broader class of problems than the commonly used i.i.d. sampling scenario. From a stochastic optimization perspective, to the best of our knowledge, the proposed homotopy primal-dual algorithm is the first to achieve \$O(1/T)\$ convergence rate for distributed stochastic saddle point problem.},
	urldate = {2020-11-20},
	journal = {arXiv:1908.02805 [cs, math]},
	author = {Ding, Dongsheng and Wei, Xiaohan and Yang, Zhuoran and Wang, Zhaoran and Jovanović, Mihailo R.},
	month = sep,
	year = {2020},
	note = {arXiv: 1908.02805},
	keywords = {Computer Science - Machine Learning, Computer Science - Multiagent Systems, Mathematics - Optimization and Control},
	file = {arXiv Fulltext PDF:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/KIA27JTF/Ding et al. - 2020 - Fast Multi-Agent Temporal-Difference Learning via .pdf:application/pdf;arXiv.org Snapshot:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/87K6XMNB/1908.html:text/html}
}

@article{zhou_mirror_2017,
	title = {Mirror descent in non-convex stochastic programming},
	abstract = {In this paper, we examine a class of nonconvex stochastic optimization problems which we call variationally coherent, and which properly includes all quasi-convex programs. In view of solving such problems, we focus on the widely used stochastic mirror descent (SMD) family of algorithms, and we establish that the method's last iterate converges with probability 1. We further introduce a localized version of variational coherence which ensures local convergence of SMD with high probability. These results contribute to the landscape of nonconvex stochastic optimization by showing that quasiconvexity is not essential for convergence: rather, variational coherence, a much weaker requirement, suffices. Finally, building on the above, we reveal an interesting insight regarding the convergence speed of SMD: in variationally coherent problems with sharp minima (e.g. generic linear programs), the last iterate of SMD reaches an exact global optimum in a finite number of steps (a.s.), even in the presence of persistent noise. This result is to be contrasted with existing work on black-box stochastic linear programming which only exhibits asymptotic convergence rates.},
	author = {Zhou, Zhengyuan and Mertikopoulos, Panayotis and Bambos, Nicholas and Boyd, Stephen and Glynn, Peter},
	month = jun,
	year = {2017}
}

@article{tack_csi_2020,
	title = {{CSI}: {Novelty} {Detection} via {Contrastive} {Learning} on {Distributionally} {Shifted} {Instances}},
	shorttitle = {{CSI}},

	abstract = {Novelty detection, i.e., identifying whether a given sample is drawn from outside the training distribution, is essential for reliable machine learning. To this end, there have been many attempts at learning a representation well-suited for novelty detection and designing a score based on such representation. In this paper, we propose a simple, yet effective method named contrasting shifted instances (CSI), inspired by the recent success on contrastive learning of visual representations. Specifically, in addition to contrasting a given sample with other instances as in conventional contrastive learning methods, our training scheme contrasts the sample with distributionally-shifted augmentations of itself. Based on this, we propose a new detection score that is specific to the proposed training scheme. Our experiments demonstrate the superiority of our method under various novelty detection scenarios, including unlabeled one-class, unlabeled multi-class and labeled multi-class settings, with various image benchmark datasets. Code and pre-trained models are available at https://github.com/alinlab/CSI.},
	urldate = {2020-11-30},
	journal = {arXiv:2007.08176 [cs, stat]},
	author = {Tack, Jihoon and Mo, Sangwoo and Jeong, Jongheon and Shin, Jinwoo},
	month = oct,
	year = {2020},
	note = {arXiv: 2007.08176
version: 2},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/YLUTIIR3/Tack et al. - 2020 - CSI Novelty Detection via Contrastive Learning on.pdf:application/pdf;arXiv.org Snapshot:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/YAIIBCBT/2007.html:text/html}
}

@article{ruff_deep_nodate,
	title = {Deep {One}-{Class} {Classification}},
	abstract = {Despite the great advances made by deep learning in many machine learning problems, there is a relative dearth of deep learning approaches for anomaly detection. Those approaches which do exist involve networks trained to perform a task other than anomaly detection, namely generative models or compression, which are in turn adapted for use in anomaly detection; they are not trained on an anomaly detection based objective. In this paper we introduce a new anomaly detection method—Deep Support Vector Data Description—, which is trained on an anomaly detection based objective. The adaptation to the deep regime necessitates that our neural network and training procedure satisfy certain properties, which we demonstrate theoretically. We show the effectiveness of our method on MNIST and CIFAR-10 image benchmark datasets as well as on the detection of adversarial examples of GTSRB stop signs.},
	language = {en},
	author = {Ruff, Lukas and Vandermeulen, Robert A and Görnitz, Nico and Deecke, Lucas and Siddiqui, Shoaib A and Binder, Alexander and Müller, Emmanuel and Kloft, Marius},
	pages = {10},
	file = {Ruff et al. - Deep One-Class Classification.pdf:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/VZI2ZFDH/Ruff et al. - Deep One-Class Classification.pdf:application/pdf}
}

@article{parisi_continual_2018,
	title = {Continual {Lifelong} {Learning} with {Neural} {Networks}: {A} {Review}},
	shorttitle = {Continual {Lifelong} {Learning} with {Neural} {Networks}},
	abstract = {Humans and animals have the ability to continually acquire and fine-tune knowledge throughout their lifespan. This ability, referred to as lifelong learning, is mediated by a rich set of neurocognitive mechanisms that together contribute to the development and specialization of our sensorimotor skills as well as to long-term memory consolidation and retrieval. Consequently, lifelong learning capabilities are crucial for computational learning systems and autonomous agents interacting in the real world and processing continuous streams of information. However, lifelong learning remains a long-standing challenge for machine learning and neural network models since the continual acquisition of incrementally available information from non-stationary data distributions generally leads to catastrophic forgetting or interference. This limitation represents a major drawback also for state-of-the-art deep and shallow neural network models that typically learn representations from stationary batches of training data, thus without accounting for situations in which the number of tasks is not known a priori and the information becomes incrementally available over time. In this review, we critically summarize the main challenges linked to lifelong learning for artificial learning systems and compare existing neural network approaches that alleviate, to different extents, catastrophic forgetting. Although significant advances have been made in domain-specific learning with neural networks, extensive research efforts are required for the development of robust lifelong learning on autonomous agents and robots. We discuss well-established and emerging research motivated by lifelong learning factors in biological systems such as neurosynaptic plasticity, critical developmental stages, multi-task transfer learning, intrinsically motivated exploration, and crossmodal learning.},
	journal = {Neural Networks},
	author = {Parisi, German and Kemker, Ronald and Part, Jose and Kanan, Christopher and Wermter, Stefan},
	month = feb,
	year = {2018},
	file = {Full Text PDF:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/6DTGL7DT/Parisi et al. - 2018 - Continual Lifelong Learning with Neural Networks .pdf:application/pdf}
}

@article{li_learning_2017,
	title = {Learning without {Forgetting}},

	abstract = {When building a unified vision system or gradually adding new capabilities to a system, the usual assumption is that training data for all tasks is always available. However, as the number of tasks grows, storing and retraining on such data becomes infeasible. A new problem arises where we add new capabilities to a Convolutional Neural Network (CNN), but the training data for its existing capabilities are unavailable. We propose our Learning without Forgetting method, which uses only new task data to train the network while preserving the original capabilities. Our method performs favorably compared to commonly used feature extraction and fine-tuning adaption techniques and performs similarly to multitask learning that uses original task data we assume unavailable. A more surprising observation is that Learning without Forgetting may be able to replace fine-tuning with similar old and new task datasets for improved new task performance.},
	urldate = {2020-12-07},
	journal = {arXiv:1606.09282 [cs, stat]},
	author = {Li, Zhizhong and Hoiem, Derek},
	month = feb,
	year = {2017},
	note = {arXiv: 1606.09282},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/FNJ2MI6T/Li and Hoiem - 2017 - Learning without Forgetting.pdf:application/pdf;arXiv.org Snapshot:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/TIZ3YAEI/1606.html:text/html}
}

@article{goebel_lecture_nodate,
	title = {Lecture {Notes} in {Artiﬁcial} {Intelligence}},
	language = {en},
	author = {Goebel, Edited R and Siekmann, J and Wahlster, W},
	pages = {521},
	file = {Goebel et al. - Lecture Notes in Artiﬁcial Intelligence.pdf:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/LYELUN26/Goebel et al. - Lecture Notes in Artiﬁcial Intelligence.pdf:application/pdf}
}

@article{parisi_continual_2019,
	title = {Continual lifelong learning with neural networks: {A} review},
	volume = {113},
	issn = {08936080},
	shorttitle = {Continual lifelong learning with neural networks},
	abstract = {Humans and animals have the ability to continually acquire, ﬁne-tune, and transfer knowledge and skills throughout their lifespan. This ability, referred to as lifelong learning, is mediated by a rich set of neurocognitive mechanisms that together contribute to the development and specialization of our sensorimotor skills as well as to long-term memory consolidation and retrieval. Consequently, lifelong learning capabilities are crucial for computational systems and autonomous agents interacting in the real world and processing continuous streams of information. However, lifelong learning remains a long-standing challenge for machine learning and neural network models since the continual acquisition of incrementally available information from non-stationary data distributions generally leads to catastrophic forgetting or interference. This limitation represents a major drawback for state-of-the-art deep neural network models that typically learn representations from stationary batches of training data, thus without accounting for situations in which information becomes incrementally available over time. In this review, we critically summarize the main challenges linked to lifelong learning for artiﬁcial learning systems and compare existing neural network approaches that alleviate, to different extents, catastrophic forgetting. Although signiﬁcant advances have been made in domain-speciﬁc learning with neural networks, extensive research efforts are required for the development of robust lifelong learning on autonomous agents and robots. We discuss well-established and emerging research motivated by lifelong learning factors in biological systems such as structural plasticity, memory replay, curriculum and transfer learning, intrinsic motivation, and multisensory integration.},
	language = {en},
	urldate = {2020-12-10},
	journal = {Neural Networks},
	author = {Parisi, German I. and Kemker, Ronald and Part, Jose L. and Kanan, Christopher and Wermter, Stefan},
	month = may,
	year = {2019},
	pages = {54--71},
	file = {Parisi et al. - 2019 - Continual lifelong learning with neural networks .pdf:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/PP7I8N6P/Parisi et al. - 2019 - Continual lifelong learning with neural networks .pdf:application/pdf}
}

@article{ans_self-refreshing_2004,
	title = {Self-refreshing memory in artificial neural networks: learning temporal sequences without catastrophic forgetting},
	volume = {16},
	issn = {0954-0091, 1360-0494},
	shorttitle = {Self-refreshing memory in artificial neural networks},
	abstract = {While humans forget gradually, highly distributed connectionist networks forget catastrophically: newly learned information often completely erases previously learned information. This is not just implausible cognitively, but disastrous practically. However, it is not easy in connectionist cognitive modelling to keep away from highly distributed neural networks, if only because of their ability to generalize. A realistic and effective system that solves the problem of catastrophic interference in sequential learning of ‘static’ (i.e. non-temporally ordered) patterns has been proposed recently (Robins 1995, Connection Science, 7: 123– 146, 1996, Connection Science, 8: 259– 275, Ans and Rousset 1997, CR Acade´mie des Sciences Paris, Life Sciences, 320: 989– 997, French 1997, Connection Science, 9: 353– 379, 1999, Trends in Cognitive Sciences, 3: 128– 135, Ans and Rousset 2000, Connection Science, 12: 1 – 19). The basic principle is to learn new external patterns interleaved with internally generated ‘pseudopatterns’ (generated from random activation) that reﬂect the previously learned information. However, to be credible, this self-refreshing mechanism for static learning has to encompass our human ability to learn serially many temporal sequences of patterns without catastrophic forgetting. Temporal sequence learning is arguably more important than static pattern learning in the real world. In this paper, we develop a dual-network architecture in which self-generated pseudopatterns reﬂect (non-temporally) all the sequences of temporally ordered items previously learned. Using these pseudopatterns, several self-refreshing mechanisms that eliminate catastrophic forgetting in sequence learning are described and their efﬁciency is demonstrated through simulations. Finally, an experiment is presented that evidences a close similarity between human and simulated behaviour.},
	language = {en},
	number = {2},
	urldate = {2020-12-10},
	journal = {Connection Science},
	author = {Ans, Bernard and Rousset, Stéphane and French, Robert M. and Musca, Serban},
	month = jun,
	year = {2004},
	pages = {71--99},
	file = {Ans et al. - 2004 - Self-refreshing memory in artificial neural networ.pdf:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/BPETHV8X/Ans et al. - 2004 - Self-refreshing memory in artificial neural networ.pdf:application/pdf}
}

@article{parisi_continual_2019-1,
	title = {Continual {Lifelong} {Learning} with {Neural} {Networks}: {A} {Review}},
	volume = {113},
	issn = {08936080},
	shorttitle = {Continual {Lifelong} {Learning} with {Neural} {Networks}},
	abstract = {Humans and animals have the ability to continually acquire, fine-tune, and transfer knowledge and skills throughout their lifespan. This ability, referred to as lifelong learning, is mediated by a rich set of neurocognitive mechanisms that together contribute to the development and specialization of our sensorimotor skills as well as to long-term memory consolidation and retrieval. Consequently, lifelong learning capabilities are crucial for autonomous agents interacting in the real world and processing continuous streams of information. However, lifelong learning remains a long-standing challenge for machine learning and neural network models since the continual acquisition of incrementally available information from non-stationary data distributions generally leads to catastrophic forgetting or interference. This limitation represents a major drawback for state-of-the-art deep neural network models that typically learn representations from stationary batches of training data, thus without accounting for situations in which information becomes incrementally available over time. In this review, we critically summarize the main challenges linked to lifelong learning for artificial learning systems and compare existing neural network approaches that alleviate, to different extents, catastrophic forgetting. We discuss well-established and emerging research motivated by lifelong learning factors in biological systems such as structural plasticity, memory replay, curriculum and transfer learning, intrinsic motivation, and multisensory integration.},
	urldate = {2020-12-10},
	journal = {Neural Networks},
	author = {Parisi, German I. and Kemker, Ronald and Part, Jose L. and Kanan, Christopher and Wermter, Stefan},
	month = may,
	year = {2019},
	note = {arXiv: 1802.07569},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Quantitative Biology - Neurons and Cognition},
	pages = {54--71},
	file = {arXiv Fulltext PDF:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/YSW5XFGF/Parisi et al. - 2019 - Continual Lifelong Learning with Neural Networks .pdf:application/pdf;arXiv.org Snapshot:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/ZIBUC93D/1802.html:text/html}
}

@article{atkinson_pseudo-recursal_2018,
	title = {Pseudo-{Recursal}: {Solving} the {Catastrophic} {Forgetting} {Problem} in {Deep} {Neural} {Networks}},
	abstract = {In general, neural networks are not currently capable of learning tasks in a sequential fashion. When a novel, unrelated task is learnt by a neural network, it substantially forgets how to solve previously learnt tasks. One of the original solutions to this problem is pseudo-rehearsal, which involves learning the new task while rehearsing generated items representative of the previous task/s. This is very effective for simple tasks. However, pseudo-rehearsal has not yet been successfully applied to very complex tasks because in these tasks it is difﬁcult to generate representative items. We accomplish pseudo-rehearsal by using a Generative Adversarial Network to generate items so that our deep network can learn to sequentially classify the CIFAR-10, SVHN and MNIST datasets. After training on all tasks, our network loses only 1.67\% absolute accuracy on CIFAR-10 and gains 0.24\% absolute accuracy on SVHN. Our model’s performance is a substantial improvement compared to the current state of the art solution.},
	language = {en},
	author = {Atkinson, Craig and Robins, Anthony and McCane, Brendan and Szymanski, Lech},
	year = {2018},
	pages = {8},
	file = {Atkinson et al. - Pseudo-Recursal Solving the Catastrophic Forgetti.pdf:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/ZIEXIQNA/Atkinson et al. - Pseudo-Recursal Solving the Catastrophic Forgetti.pdf:application/pdf}
}

@article{rebuffi_icarl_2017,
	title = {{iCaRL}: {Incremental} {Classifier} and {Representation} {Learning}},
	shorttitle = {{iCaRL}},

	abstract = {A major open problem on the road to artificial intelligence is the development of incrementally learning systems that learn about more and more concepts over time from a stream of data. In this work, we introduce a new training strategy, iCaRL, that allows learning in such a class-incremental way: only the training data for a small number of classes has to be present at the same time and new classes can be added progressively. iCaRL learns strong classifiers and a data representation simultaneously. This distinguishes it from earlier works that were fundamentally limited to fixed data representations and therefore incompatible with deep learning architectures. We show by experiments on CIFAR-100 and ImageNet ILSVRC 2012 data that iCaRL can learn many classes incrementally over a long period of time where other strategies quickly fail.},
	urldate = {2021-01-04},
	journal = {arXiv:1611.07725 [cs, stat]},
	author = {Rebuffi, Sylvestre-Alvise and Kolesnikov, Alexander and Sperl, Georg and Lampert, Christoph H.},
	month = apr,
	year = {2017},
	note = {arXiv: 1611.07725},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/SM4PHC35/Rebuffi et al. - 2017 - iCaRL Incremental Classifier and Representation L.pdf:application/pdf;arXiv.org Snapshot:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/M75SYE33/1611.html:text/html}
}

@article{ayub_storing_2020,
	title = {Storing {Encoded} {Episodes} as {Concepts} for {Continual} {Learning}},
	abstract = {The two main challenges faced by continual learning approaches are catastrophic forgetting and memory limitations on the storage of data. To cope with these challenges, we propose a novel, cognitively-inspired approach which trains autoencoders with Neural Style Transfer to encode and store images. Reconstructed images from encoded episodes are replayed when training the classiﬁer model on a new task to avoid catastrophic forgetting. The loss function for the reconstructed images is weighted to reduce its effect during classiﬁer training to cope with image degradation. When the system runs out of memory the encoded episodes are converted into centroids and covariance matrices, which are used to generate pseudo-images during classiﬁer training, keeping classiﬁer performance stable with less memory. Our approach increases classiﬁcation accuracy by 13-17\% over state-of-the-art methods on benchmark datasets, while requiring 78\% less storage space.},
	language = {en},
	author = {Ayub, Ali and Wagner, Alan R},
	year = {2020},
	pages = {7},
	file = {Ayub and Wagner - Storing Encoded Episodes as Concepts for Continual.pdf:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/MLWYBFLY/Ayub and Wagner - Storing Encoded Episodes as Concepts for Continual.pdf:application/pdf}
}


@article{yang_scheduling_2019,
	title = {Scheduling {Policies} for {Federated} {Learning} in {Wireless} {Networks}},
	urldate = {2021-05-02},
	journal = {arXiv:1908.06287 [cs, eess, math]},
	author = {Yang, Howard H. and Liu, Zuozhu and Quek, Tony Q. S. and Poor, H. Vincent},
	month = oct,
	year = {2019}
}


@inproceedings{hegedus_gossip_2019,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Gossip {Learning} as a {Decentralized} {Alternative} to {Federated} {Learning}},
	isbn = {978-3-030-22496-7},
	abstract = {Federated learning is a distributed machine learning approach for computing models over data collected by edge devices. Most importantly, the data itself is not collected centrally, but a master-worker architecture is applied where a master node performs aggregation and the edge devices are the workers, not unlike the parameter server approach. Gossip learning also assumes that the data remains at the edge devices, but it requires no aggregation server or any central component. In this empirical study, we present a thorough comparison of the two approaches. We examine the aggregated cost of machine learning in both cases, considering also a compression technique applicable in both approaches. We apply a real churn trace as well collected over mobile phones, and we also experiment with different distributions of the training data over the devices. Surprisingly, gossip learning actually outperforms federated learning in all the scenarios where the training data are distributed uniformly over the nodes, and it performs comparably to federated learning overall.},
	language = {en},
	booktitle = {Distributed {Applications} and {Interoperable} {Systems}},
	publisher = {Springer International Publishing},
	author = {Hegedűs, István and Danner, Gábor and Jelasity, Márk},
	editor = {Pereira, José and Ricci, Laura},
	year = {2019},
	pages = {74--90},
	file = {Springer Full Text PDF:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/3FJQ9D48/Hegedűs et al. - 2019 - Gossip Learning as a Decentralized Alternative to .pdf:application/pdf}
}



@inproceedings{ickin_privacy_2019,
	author = {Ickin, Selim and Vandikas, Konstantinos and Fiedler, Markus},
	title = {Privacy Preserving QoE Modeling Using Collaborative Learning},
	year = {2019},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	booktitle = {Proceedings of the 4th Internet-QoE Workshop on QoE-Based Analysis and Management of Data Communication Networks},
	pages = "13-18",
}


@inproceedings{katzef_distributed_2020,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Distributed {Generative} {Adversarial} {Networks} for {Anomaly} {Detection}},
	isbn = {978-3-030-64793-3},
	language = {en},
	booktitle = {Decision and {Game} {Theory} for {Security}},
	publisher = {Springer International Publishing},
	author = {Katzef, Marc and Cullen, Andrew C. and Alpcan, Tansu and Leckie, Christopher and Kopacz, Justin},
	editor = {Zhu, Quanyan and Baras, John S. and Poovendran, Radha and Chen, Juntao},
	year = {2020},
	keywords = {Anomaly detection, Cognitive radio networks, Distributed, Generative adversarial networks},
	pages = {3--22},
	file = {Springer Full Text PDF:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/KEFU3LZZ/Katzef et al. - 2020 - Distributed Generative Adversarial Networks for An.pdf:application/pdf}
}

@article{schlegl_f-anogan_2019,
	title = {f-{AnoGAN}: {Fast} unsupervised anomaly detection with generative adversarial networks},
	volume = {54},
	issn = {13618415},
	shorttitle = {f-{AnoGAN}},
	abstract = {Obtaining expert labels in clinical imaging is diﬃcult since exhaustive annotation is time-consuming. Furthermore, not all possibly relevant markers may be known and suﬃciently well described a priori to even guide annotation. While supervised learning yields good results if expert labeled training data is available, the visual variability, and thus the vocabulary of ﬁndings, we can detect and exploit, is limited to the annotated lesions. Here, we present fast AnoGAN (f-AnoGAN), a generative adversarial network (GAN) based unsupervised learning approach capable of identifying anomalous images and image segments, that can serve as imaging biomarker candidates. We build a generative model of healthy training data, and propose and evaluate a fast mapping technique of new data to the GAN’s latent space. The mapping is based on a trained encoder, and anomalies are detected via a combined anomaly score based on the building blocks of the trained model – comprising a discriminator feature residual error and an image reconstruction error. In the experiments on optical coherence tomography data, we compare the proposed method with alternative approaches, and provide comprehensive empirical evidence that f-AnoGAN outperforms alternative approaches and yields high anomaly detection accuracy. In addition, a visual Turing test with two retina experts showed that the generated images are indistinguishable from real normal retinal OCT images. The f-AnoGAN code is available at https://github.com/tSchlegl/f-AnoGAN.},
	language = {en},
	urldate = {2020-04-27},
	journal = {Medical Image Analysis},
	author = {Schlegl, Thomas and Seeböck, Philipp and Waldstein, Sebastian M. and Langs, Georg and Schmidt-Erfurth, Ursula},
	month = may,
	year = {2019},
	pages = {30--44},
}


@inproceedings{mcmahan2017communication,
  title={Communication-{Efficient} {Learning} of {Deep} {Networks} from {Decentralized} {Data}},
  author={McMahan, Brendan and Moore, Eider and Ramage, Daniel and Hampson, Seth and y Arcas, Blaise Aguera},
  booktitle={Artificial {I}ntelligence and {S}tatistics},
  pages={1273--1282},
  year={2017},
  organization={PMLR}
}


@INPROCEEDINGS{8862686,
  author={Vani, S. and Rao, T. V. Madhusudhana},
  booktitle={2019 3rd International Conference on Trends in Electronics and Informatics (ICOEI)},
  title={An Experimental Approach towards the Performance Assessment of Various Optimizers on Convolutional Neural Network},
  year={2019},
  volume={},
  number={},
  pages={331-336}
}


@article{xie_anomaly_2011,
	series = {Advanced {Topics} in {Cloud} {Computing}},
	title = {Anomaly detection in wireless sensor networks: {A} survey},
	volume = {34},
	issn = {1084-8045},
	shorttitle = {Anomaly detection in wireless sensor networks},
	language = {en},
	number = {4},
	journal = {Journal of Network and Computer Applications},
	author = {Xie, Miao and Han, Song and Tian, Biming and Parvin, Sazia},
	month = jul,
	year = {2011},
	keywords = {Anomaly detection, Information security, Wireless sensor networks},
	pages = {1302--1325}
}


@article{chalapathy_deep_2019,
	title = {Deep {Learning} for {Anomaly} {Detection}: {A} {Survey}},
	shorttitle = {Deep {Learning} for {Anomaly} {Detection}},
	journal = {arXiv:1901.03407 [cs, stat]},
	author = {Chalapathy, Raghavendra and Chawla, Sanjay},
	month = jan,
	year = {2019},
	note = {arXiv: 1901.03407},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning}
}



@article{imteaj_survey_2021,
	title = {A {Survey} on {Federated} {Learning} for {Resource}-{Constrained} {IoT} {Devices}},
	issn = {2327-4662},
	doi = {10.1109/JIOT.2021.3095077},
	abstract = {Federated learning (FL) is a distributed machine learning strategy that generates a global model by learning from multiple decentralized edge clients. FL enables on-device training, keeping the client’s local data private, and further, updating the global model based on the local model updates. While FL methods offer several advantages, including scalability and data privacy, they assume there are available computational resources at each edge-device/client. However, the Internet-of-Things (IoTs) enabled devices, e.g., robots, drone swarms, and low-cost computing devices (e.g., Raspberry Pi), may have limited processing ability, low bandwidth and power, or limited storage capacity. In this survey paper, we propose to answer this question: how to train distributed machine learning models for resource-constrained IoT devices? To this end, we first explore the existing studies on FL, relative assumptions for distributed implementation using IoT devices, and explore their drawbacks. We then discuss the implementation challenges and issues when applying FL to an IoT environment. We highlight an overview of FL and provide a comprehensive survey of the problem statements and emerging challenges, particularly during applying FL within heterogeneous IoT environments. Finally, we point out the future research directions for scientists and researchers who are interested in working at the intersection of FL and resource-constrained IoT environments.},
	journal = {IEEE Internet of Things Journal},
	author = {Imteaj, Ahmed and Thakker, Urmish and Wang, Shiqiang and Li, Jian and Amini, M. Hadi},
	year = {2021},
	note = {Conference Name: IEEE Internet of Things Journal},
	keywords = {Internet of Things, Training, Computational modeling, Data models, Edge computing, Federated Learning, Servers, Collaborative work, convergence., global model, local model, on-device training, resource-constrained IoT devices},
	pages = {1--1},
	file = {IEEE Xplore Abstract Record:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/E9J78JPT/9475501.html:text/html;IEEE Xplore Full Text PDF:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/PG7QJYD2/Imteaj et al. - 2021 - A Survey on Federated Learning for Resource-Constr.pdf:application/pdf},
}

@article{wu_safa_2021,
	title = {{SAFA}: {A} {Semi}-{Asynchronous} {Protocol} for {Fast} {Federated} {Learning} {With} {Low} {Overhead}},
	volume = {70},
	issn = {1557-9956},
	shorttitle = {{SAFA}},
	doi = {10.1109/TC.2020.2994391},
	abstract = {Federated learning (FL) has attracted increasing attention as a promising approach to driving a vast number of end devices with artificial intelligence. However, it is very challenging to guarantee the efficiency of FL considering the unreliable nature of end devices while the cost of device-server communication cannot be neglected. In this article, we propose SAFA, a semi-asynchronous FL protocol, to address the problems in federated learning such as low round efficiency and poor convergence rate in extreme conditions (e.g., clients dropping offline frequently). We introduce novel designs in the steps of model distribution, client selection and global aggregation to mitigate the impacts of stragglers, crashes and model staleness in order to boost efficiency and improve the quality of the global model. We have conducted extensive experiments with typical machine learning tasks. The results demonstrate that the proposed protocol is effective in terms of shortening federated round duration, reducing local resource wastage, and improving the accuracy of the global model at an acceptable communication cost.},
	number = {5},
	journal = {IEEE Transactions on Computers},
	author = {Wu, Wentai and He, Ligang and Lin, Weiwei and Mao, Rui and Maple, Carsten and Jarvis, Stephen},
	month = may,
	year = {2021},
	note = {Conference Name: IEEE Transactions on Computers},
	keywords = {Machine learning, Training, Data models, machine learning, edge intelligence, Convergence, Distributed computing, Distributed databases, federated learning, Optimization, Protocols},
	pages = {655--668},
	file = {IEEE Xplore Abstract Record:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/7NUK4HB8/9093123.html:text/html;IEEE Xplore Full Text PDF:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/U483H5A7/Wu et al. - 2021 - SAFA A Semi-Asynchronous Protocol for Fast Federa.pdf:application/pdf},
}

@article{stripelis_semi-synchronous_2021,
	title = {Semi-{Synchronous} {Federated} {Learning}},
	url = {http://arxiv.org/abs/2102.02849},
	abstract = {There are situations where data relevant to a machine learning problem are distributed among multiple locations that cannot share the data due to regulatory, competitiveness, or privacy reasons. For example, data present in users' cellphones, manufacturing data of companies in a given industrial sector, or medical records located at different hospitals. Federated Learning (FL) provides an approach to learn a joint model over all the available data across silos. In many cases, participating sites have different data distributions and computational capabilities. In these heterogeneous environments previous approaches exhibit poor performance: synchronous FL protocols are communication efficient, but have slow learning convergence; conversely, asynchronous FL protocols have faster convergence, but at a higher communication cost. Here we introduce a novel Semi-Synchronous Federated Learning protocol that mixes local models periodically with minimal idle time and fast convergence. We show through extensive experiments that our approach significantly outperforms previous work in data and computationally heterogeneous environments.},
	urldate = {2021-12-14},
	journal = {arXiv:2102.02849 [cs]},
	author = {Stripelis, Dimitris and Ambite, Jose Luis},
	month = feb,
	year = {2021},
	note = {arXiv: 2102.02849},
	keywords = {68T07, 68T09, 68M14, 68W15, Computer Science - Machine Learning, I.2.6, I.5.1, K.6.4},
	file = {arXiv Fulltext PDF:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/2XLU7ES2/Stripelis and Ambite - 2021 - Semi-Synchronous Federated Learning.pdf:application/pdf;arXiv.org Snapshot:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/U47L7E2D/2102.html:text/html},
}

@article{zhang_csafl_2021,
	title = {{CSAFL}: {A} {Clustered} {Semi}-{Asynchronous} {Federated} {Learning} {Framework}},
	shorttitle = {{CSAFL}},
	url = {http://arxiv.org/abs/2104.08184},
	abstract = {Federated learning (FL) is an emerging distributed machine learning paradigm that protects privacy and tackles the problem of isolated data islands. At present, there are two main communication strategies of FL: synchronous FL and asynchronous FL. The advantages of synchronous FL are that the model has high precision and fast convergence speed. However, this synchronous communication strategy has the risk that the central server waits too long for the devices, namely, the straggler effect which has a negative impact on some time-critical applications. Asynchronous FL has a natural advantage in mitigating the straggler effect, but there are threats of model quality degradation and server crash. Therefore, we combine the advantages of these two strategies to propose a clustered semi-asynchronous federated learning (CSAFL) framework. We evaluate CSAFL based on four imbalanced federated datasets in a non-IID setting and compare CSAFL to the baseline methods. The experimental results show that CSAFL significantly improves test accuracy by more than +5\% on the four datasets compared to TA-FedAvg. In particular, CSAFL improves absolute test accuracy by +34.4\% on non-IID FEMNIST compared to TA-FedAvg.},
	urldate = {2021-12-14},
	journal = {arXiv:2104.08184 [cs]},
	author = {Zhang, Yu and Duan, Moming and Liu, Duo and Li, Li and Ren, Ao and Chen, Xianzhang and Tan, Yujuan and Wang, Chengliang},
	month = apr,
	year = {2021},
	note = {arXiv: 2104.08184},
	keywords = {Computer Science - Distributed, Parallel, and Cluster Computing, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/2W756FMD/Zhang et al. - 2021 - CSAFL A Clustered Semi-Asynchronous Federated Lea.pdf:application/pdf;arXiv.org Snapshot:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/55TSV38F/2104.html:text/html},
}

@article{duan_flexible_2021,
	title = {Flexible {Clustered} {Federated} {Learning} for {Client}-{Level} {Data} {Distribution} {Shift}},
	url = {http://arxiv.org/abs/2108.09749},
	abstract = {Federated Learning (FL) enables the multiple participating devices to collaboratively contribute to a global neural network model while keeping the training data locally. Unlike the centralized training setting, the non-IID, imbalanced (statistical heterogeneity) and distribution shifted training data of FL is distributed in the federated network, which will increase the divergences between the local models and the global model, further degrading performance. In this paper, we propose a flexible clustered federated learning (CFL) framework named FlexCFL, in which we 1) group the training of clients based on the similarities between the clients' optimization directions for lower training divergence; 2) implement an efficient newcomer device cold start mechanism for framework scalability and practicality; 3) flexibly migrate clients to meet the challenge of client-level data distribution shift. FlexCFL can achieve improvements by dividing joint optimization into groups of sub-optimization and can strike a balance between accuracy and communication efficiency in the distribution shift environment. The convergence and complexity are analyzed to demonstrate the efficiency of FlexCFL. We also evaluate FlexCFL on several open datasets and made comparisons with related CFL frameworks. The results show that FlexCFL can significantly improve absolute test accuracy by +10.6\% on FEMNIST compared to FedAvg, +3.5\% on FashionMNIST compared to FedProx, +8.4\% on MNIST compared to FeSEM. The experiment results show that FlexCFL is also communication efficient in the distribution shift environment.},
	urldate = {2021-12-14},
	journal = {arXiv:2108.09749 [cs]},
	author = {Duan, Moming and Liu, Duo and Ji, Xinyuan and Wu, Yu and Liang, Liang and Chen, Xianzhang and Tan, Yujuan},
	month = aug,
	year = {2021},
	note = {arXiv: 2108.09749},
	keywords = {Computer Science - Distributed, Parallel, and Cluster Computing, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/NCBLDWVD/Duan et al. - 2021 - Flexible Clustered Federated Learning for Client-L.pdf:application/pdf;arXiv.org Snapshot:/home/student.unimelb.edu.au/mkatzef/Zotero/storage/W2I6ZZWU/2108.html:text/html},
}




@inproceedings{bonawitz_practical_2017,
	address = {Dallas Texas USA},
	title = {Practical {Secure} {Aggregation} for {Privacy}-{Preserving} {Machine} {Learning}},
	isbn = {978-1-4503-4946-8},
	url = {https://dl.acm.org/doi/10.1145/3133956.3133982},
	doi = {10.1145/3133956.3133982},
	abstract = {We design a novel, communication-efficient, failure-robust protocol for secure aggregation of high-dimensional data. Our protocol allows a server to compute the sum of large, user-held data vectors from mobile devices in a secure manner (i.e. without learning each user’s individual contribution), and can be used, for example, in a federated learning setting, to aggregate user-provided model updates for a deep neural network. We prove the security of our protocol in the honest-but-curious and active adversary settings, and show that security is maintained even if an arbitrarily chosen subset of users drop out at any time. We evaluate the efficiency of our protocol and show, by complexity analysis and a concrete implementation, that its runtime and communication overhead remain low even on large data sets and client pools. For 16-bit input values, our protocol offers 1.73× communication expansion for 210 users and 220-dimensional vectors, and 1.98× expansion for 214 users and 224-dimensional vectors over sending data in the clear.},
	language = {en},
	urldate = {2021-04-02},
	booktitle = {Proceedings of the 2017 {ACM} {SIGSAC} {Conference} on {Computer} and {Communications} {Security}},
	publisher = {ACM},
	author = {Bonawitz, Keith and Ivanov, Vladimir and Kreuter, Ben and Marcedone, Antonio and McMahan, H. Brendan and Patel, Sarvar and Ramage, Daniel and Segal, Aaron and Seth, Karn},
	month = oct,
	year = {2017},
	pages = {1175--1191},
	file = {Bonawitz et al. - 2017 - Practical Secure Aggregation for Privacy-Preservin.pdf:/Users/mac/Zotero/storage/PC4BDFLD/Bonawitz et al. - 2017 - Practical Secure Aggregation for Privacy-Preservin.pdf:application/pdf},
}


@article{li_convergence_2020,
	title = {On the {Convergence} of {FedAvg} on {Non}-{IID} {Data}},
	url = {http://arxiv.org/abs/1907.02189},
	abstract = {Federated learning enables a large amount of edge computing devices to jointly learn a model without data sharing. As a leading algorithm in this setting, Federated Averaging ({\textbackslash}texttt\{FedAvg\}) runs Stochastic Gradient Descent (SGD) in parallel on a small subset of the total devices and averages the sequences only once in a while. Despite its simplicity, it lacks theoretical guarantees under realistic settings. In this paper, we analyze the convergence of {\textbackslash}texttt\{FedAvg\} on non-iid data and establish a convergence rate of \${\textbackslash}mathcal\{O\}({\textbackslash}frac\{1\}\{T\})\$ for strongly convex and smooth problems, where \$T\$ is the number of SGDs. Importantly, our bound demonstrates a trade-off between communication-efficiency and convergence rate. As user devices may be disconnected from the server, we relax the assumption of full device participation to partial device participation and study different averaging schemes; low device participation rate can be achieved without severely slowing down the learning. Our results indicate that heterogeneity of data slows down the convergence, which matches empirical observations. Furthermore, we provide a necessary condition for {\textbackslash}texttt\{FedAvg\} on non-iid data: the learning rate \${\textbackslash}eta\$ must decay, even if full-gradient is used; otherwise, the solution will be \${\textbackslash}Omega ({\textbackslash}eta)\$ away from the optimal.},
	urldate = {2021-07-12},
	journal = {arXiv:1907.02189 [cs, math, stat]},
	author = {Li, Xiang and Huang, Kaixuan and Yang, Wenhao and Wang, Shusen and Zhang, Zhihua},
	month = jun,
	year = {2020},
	note = {arXiv: 1907.02189},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Mathematics - Optimization and Control},
	file = {arXiv Fulltext PDF:/Users/mac/Zotero/storage/B2M873TK/Li et al. - 2020 - On the Convergence of FedAvg on Non-IID Data.pdf:application/pdf;arXiv.org Snapshot:/Users/mac/Zotero/storage/NCKR4YTC/1907.html:text/html},
}

@article{dinh_federated_2021,
	title = {Federated {Learning} over {Wireless} {Networks}: {Convergence} {Analysis} and {Resource} {Allocation}},
	volume = {29},
	issn = {1063-6692, 1558-2566},
	shorttitle = {Federated {Learning} over {Wireless} {Networks}},
	url = {http://arxiv.org/abs/1910.13067},
	doi = {10.1109/TNET.2020.3035770},
	abstract = {There is an increasing interest in a fast-growing machine learning technique called Federated Learning, in which the model training is distributed over mobile user equipments (UEs), exploiting UEs' local computation and training data. Despite its advantages in data privacy-preserving, Federated Learning (FL) still has challenges in heterogeneity across UEs' data and physical resources. We first propose a FL algorithm which can handle the heterogeneous UEs' data challenge without further assumptions except strongly convex and smooth loss functions. We provide the convergence rate characterizing the trade-off between local computation rounds of UE to update its local model and global communication rounds to update the FL global model. We then employ the proposed FL algorithm in wireless networks as a resource allocation optimization problem that captures the trade-off between the FL convergence wall clock time and energy consumption of UEs with heterogeneous computing and power resources. Even though the wireless resource allocation problem of FL is non-convex, we exploit this problem's structure to decompose it into three sub-problems and analyze their closed-form solutions as well as insights to problem design. Finally, we illustrate the theoretical analysis for the new algorithm with Tensorflow experiments and extensive numerical results for the wireless resource allocation sub-problems. The experiment results not only verify the theoretical convergence but also show that our proposed algorithm outperforms the vanilla FedAvg algorithm in terms of convergence rate and testing accuracy.},
	number = {1},
	urldate = {2021-07-12},
	journal = {IEEE/ACM Transactions on Networking},
	author = {Dinh, Canh T. and Tran, Nguyen H. and Nguyen, Minh N. H. and Hong, Choong Seon and Bao, Wei and Zomaya, Albert Y. and Gramoli, Vincent},
	month = feb,
	year = {2021},
	note = {Number: 1
arXiv: 1910.13067},
	keywords = {Computer Science - Machine Learning, Computer Science - Networking and Internet Architecture, Statistics - Machine Learning, Computer Science - Distributed, Parallel, and Cluster Computing},
	pages = {398--409},
	file = {arXiv Fulltext PDF:/Users/mac/Zotero/storage/PMFGDAUH/Dinh et al. - 2021 - Federated Learning over Wireless Networks Converg.pdf:application/pdf;arXiv.org Snapshot:/Users/mac/Zotero/storage/66VHJI8V/1910.html:text/html},
}

@inproceedings{li_federated_nodate,
	title = {Federated {Learning} for {Time} {Series} {Forecasting} {Using} {Hybrid} {Model}},
	booktitle= {Degree Project Thesis, KTH Royal Institute of Technology, Sweden, 2019},
	language = {en},
	author = {Li, Yuntao},
	pages = {76},
	file = {Li - Federated Learning for Time Series Forecasting Usi.pdf:/Users/mac/Zotero/storage/STQYBY9N/Li - Federated Learning for Time Series Forecasting Usi.pdf:application/pdf},
}

@inproceedings{liu_client-edge-cloud_2020,
	title = {Client-{Edge}-{Cloud} {Hierarchical} {Federated} {Learning}},
	doi = {10.1109/ICC40277.2020.9148862},
	abstract = {Federated Learning is a collaborative machine learning framework to train a deep learning model without accessing clients' private data. Previous works assume one central parameter server either at the cloud or at the edge. The cloud server can access more data but with excessive communication overhead and long latency, while the edge server enjoys more efficient communications with the clients. To combine their advantages, we propose a client-edge-cloud hierarchical Federated Learning system, supported with a HierFAVG algorithm that allows multiple edge servers to perform partial model aggregation. In this way, the model can be trained faster and better communication-computation trade-offs can be achieved. Convergence analysis is provided for HierFAVG and the effects of key parameters are also investigated, which lead to qualitative design guidelines. Empirical experiments verify the analysis and demonstrate the benefits of this hierarchical architecture in different data distribution scenarios. Particularly, it is shown that by introducing the intermediate edge servers, the model training time and the energy consumption of the end devices can be simultaneously reduced compared to cloud-based Federated Learning.},
	booktitle = {{ICC} 2020 - 2020 {IEEE} {International} {Conference} on {Communications} ({ICC})},
	author = {Liu, Lumin and Zhang, Jun and Song, S.H. and Letaief, Khaled B.},
	month = jun,
	year = {2020},
	note = {ISSN: 1938-1883},
	keywords = {Machine learning, Training, Cloud computing, Computational modeling, Data models, Convergence, Edge Learning, Federated Learning, Mobile Edge Computing, Servers},
	pages = {1--6},
	file = {IEEE Xplore Abstract Record:/Users/mac/Zotero/storage/KA9H2P2B/9148862.html:text/html;IEEE Xplore Full Text PDF:/Users/mac/Zotero/storage/Y5SJIJNN/Liu et al. - 2020 - Client-Edge-Cloud Hierarchical Federated Learning.pdf:application/pdf},
}

@inproceedings{chou_efficient_2021,
	title = {Efficient and {Less} {Centralized} {Federated} {Learning}},
  author={Chou, Li and Liu, Zichang and Wang, Zhuang and Shrivastava, Anshumali},
  booktitle={{Joint} {European} {Conference} on {Machine} {Learning} and {Knowledge} {Discovery} in {Databases}},
  pages={772--787},
  year={2021},
  organization={Springer}
}

@book{miao_zander_sung_ben_slimane_2016,
place={Cambridge}, title={Fundamentals of Mobile Data Networks}, DOI={10.1017/CBO9781316534298}, publisher={Cambridge University Press}, author={Miao, Guowang and Zander, Jens and Sung, Ki Won and Ben Slimane, Slimane}, year={2016}}


@article{xie_multi-center_2021,
	title = {Multi-{Center} {Federated} {Learning}},
	url = {http://arxiv.org/abs/2005.01026},
	abstract = {Federated learning (FL) can protect data privacy in distributed learning since it merely collects local gradients from users without access to their data. However, FL is fragile in the presence of heterogeneity that is commonly encountered in practical settings, e.g., non-IID data over different users. Existing FL approaches usually update a single global model to capture the shared knowledge of all users by aggregating their gradients, regardless of the discrepancy between their data distributions. By comparison, a mixture of multiple global models could capture the heterogeneity across various users if assigning the users to different global models (i.e., centers) in FL. To this end, we propose a novel multi-center aggregation mechanism . It learns multiple global models from data, and simultaneously derives the optimal matching between users and centers. We then formulate it as a bilevel optimization problem that can be efﬁciently solved by a stochastic expectation maximization (EM) algorithm. Experiments on multiple benchmark datasets of FL show that our method outperforms several popular FL competitors.},
	language = {en},
	urldate = {2022-06-19},
	publisher = {arXiv},
	author = {Xie, Ming and Long, Guodong and Shen, Tao and Zhou, Tianyi and Wang, Xianzhi and Jiang, Jing and Zhang, Chengqi},
	month = aug,
	year = {2021},
	note = {Number: arXiv:2005.01026
arXiv:2005.01026 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Distributed, Parallel, and Cluster Computing}
}

@article{duan_fedgroup_2021,
	title = {{FedGroup}: {Efficient} {Clustered} {Federated} {Learning} via {Decomposed} {Data}-{Driven} {Measure}},
	shorttitle = {{FedGroup}},
	url = {http://arxiv.org/abs/2010.06870},
	abstract = {Federated Learning (FL) enables the multiple participating devices to collaboratively contribute to a global neural network model while keeping the training data locally. Unlike the centralized training setting, the non-IID and imbalanced (statistical heterogeneity) training data of FL is distributed in the federated network, which will increase the divergences between the local models and the global model, further degrading performance. In this paper, we propose a novel clustered federated learning (CFL) framework FedGroup, in which we 1) group the training of clients based on the similarities between the clients’ optimization directions for high training performance; 2) construct a new data-driven distance measure to improve the efﬁciency of the client clustering procedure. 3) implement a newcomer device cold start mechanism based on the auxiliary global model for framework scalability and practicality.},
	language = {en},
	urldate = {2022-06-19},
	publisher = {arXiv},
	author = {Duan, Moming and Liu, Duo and Ji, Xinyuan and Liu, Renping and Liang, Liang and Chen, Xianzhang and Tan, Yujuan},
	month = jul,
	year = {2021},
	note = {Number: arXiv:2010.06870
arXiv:2010.06870 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Distributed, Parallel, and Cluster Computing}
}

@inproceedings{NEURIPS2020_e32cc80b,
 author = {Ghosh, Avishek and Chung, Jichan and Yin, Dong and Ramchandran, Kannan},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {19586--19597},
 publisher = {Curran Associates, Inc.},
 title = {An Efficient Framework for Clustered Federated Learning},
 volume = {33},
 year = {2020}
}


@misc{katzef_commsml,
	title = {Comms-ML: Communications ML Dataset Generator},
	url = {https://github.com/mkatzef/comms-ml},
    urldate = {2022-06-10},
	journal = {https://github.com/sandamal/omnet\_simulation},
	author = {Katzef, Marc},
	year = {2022}
}

@article{chandola_ad_survey_2009,
author = {Chandola, Varun and Banerjee, Arindam and Kumar, Vipin},
title = {Anomaly Detection: A Survey},
year = {2009},
issue_date = {July 2009},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {41},
number = {3},
issn = {0360-0300},
url = {https://doi.org/10.1145/1541880.1541882},
doi = {10.1145/1541880.1541882},
journal = {ACM Comput. Surv.},
month = {jul},
articleno = {15},
numpages = {58},
keywords = {outlier detection, Anomaly detection}
}