%\documentclass{article}


%\begin{document}
\section{Overview of compared image-to-image translation approaches}
We evaluate three stain transfer methods that are based on traditional approaches.
~\citet{ReinhardAGS01} presented one of the first approaches for transfering color statistics (\textbf{ColorStat}) that was later adopted for stain conversion in histopathology. The authors suggest to transfer the mean and standard deviation of images from the target domain to images from the source domain for each of the three components of LAB color space.    
   \citet{ref_macenko} proposed to handle variability in staining by introducing a color normalization method. It assumes the linear combination of two stains in the optical density (OD) space and uses singular value decomposition to represent OD by the product of the stain and the concentration matrices. The former characterizes the properties of the used stains, while the latter characterizes the strength of the performed staining.
Using the stain matrix, estimated from the reference image from the target domain, and the concentration matrix of a source image, the image is normalized to have the appearance of the target domain.    
\citet{ref_vahadane} build on \cite{ref_macenko} work, but forces the values of the concentration matrix to be non-negative and sparse, which makes the OD decomposition biologically more plausible. A reference image is usually used to learn the unknown parameters of the traditional methods. Instead of relying on a single reference image, we use all the images in the training dataset from the target domain. For Macenko and Vahadane we averaged the stain matrix and pseudo-maxima of stain concentrations over the training dataset, while for ColorStat we averaged mean and standard deviation.

We also evaluate nine stain transfer methods that are based on Generative Adversarial Networks (GAN)~\cite{goodfellow2014generative}. 
We denote GAN's generator as $G$ and discriminator as $D$. $x$ and $y$ are the images from $X$ and $Y$ domains, respectively, between which we want to find a mapping. %Generative Adversarial Networks (GANs)~\cite{goodfellow2014generative} are suitable methods for this task. 
%GANs are built by establishing two major components implemented as neural networks: generator ($G$) and discriminator ($D$).  
%The task of generator is to learn how to generate realistic enough images to fool the discriminator. 
%The task of discriminator is to learn how to distinguish between artificially generated and real images. 
\textbf{Pix2Pix}~\cite{ref_pix2pix} uses a conditional GAN (cGAN) to translate $X$ → $Y$, conditioning the discriminator on source images $x \in X$. The discriminator is fed with pairs, either of source and generated $\{x, G(x)\}$ images or of source and target $\{x, y\}$ aligned images. An $L_1$ term is added to the adversarial loss to improve the cGAN performance. One disadvantage of this method is that it requires aligned images $\{x, y\}$ for training. To avoid this limitation, 
we employ the following trick. We train cGAN to colorize images according to $Y$ using pairs of $y$ and corresponding gray-scale images. To transform $x$ to $y$ we first convert $x$ to gray-scale and then apply the trained cGAN. More specifically, we train cGAN on true pairs $\{\mbox{AB}(y), \mbox{L}(y)\}$, and fake pairs $\{G(\mbox{L}(y)), \mbox{L}(y)\}$, where $\mbox{L}()$ and $\mbox{AB}()$ denote the mapping of an image to the lightness and the two color channels of the LAB color space, and G generates $\mbox{A, B}$ color channels. During inference the trained $G$ generates $G(\mbox{L}(x))$ color channels that are merged with lightness $\mbox{L}(x)$.
%In Sec.~\ref{Sec:training_details} we show a trick that allowed us in our experiments to rely on unpaired images from two domains. 

The following GAN-based methods, by design, do not require paired images for training.
%In contrast to traditional GANs the proposed conditional GANs learns mapping from an input image $x$ and random noise vector $z$, to $y$, $G:{x,z}$ → $y$. 
%These networks both learn the mapping from one domain to another and the loss function to train it.
%\textbf{Pix2Pix}~\cite{ref_pix2pix} proposes to use conditional adversarial networks to solve the I2I translation problem. 
%In contrast to traditional GANs the proposed conditional GANs learns mapping from an input image $x$ and random noise vector $z$, to $y$, $G:{x,z}$ → $y$. 
%These networks both learn the mapping from one domain to another and the loss function to train it.
%%    One epoch takes 11 minutes and 22 seconds approximately.
%%    Trained for 30 epochs, which took 5 hours and 41 minutes (estimated time)
%%    Needs to be trained in both directions, so two different experiments run here.
\textbf{CycleGAN}~\cite{zhu2017unpaired} proposed training two generators $G$ : $X$ → $Y$ and $F$ : $Y$ → $X$, enforcing, in addition to two adversarial loss terms, a cycle consistency loss that enforces $ F(G(X)) \approx X$ and $G(F(Y)) \approx Y $.  
%The idea of the method is to take two image collections from $X$ and $Y$ domains that do not contain paired images and learns two mapping functions $G$ : $X$ → $Y$  and $F$ : $Y$ → $X$.  
%To enforce the intuition  behind bijectional mapping, the authors introduce a cycle consistency loss so that $ F(G(X)) \approx X$ and $G(F(Y)) \approx Y $.
%CycleGAN has been applied as a stain normalization technique in paper . The authors of the paper focused on the task of translation of images taken using different %scanners to a reference style.
%We have employed the reproduced the strain normalization procedure by training on a data set provided by BI. See the example of conversion of H&E to M&E domain using %CycleGAN.
%    One epoch takes 3.34 hours approximately.
%    Trained for 30 epochs, which took 4 days, 4 hour and 12 minutes (estimated time).
%    Model was further trained with a lower learning rate for 10 epochs, 1 day, 9 hours and 24 minutes (estimated time).
\textbf{UTOM}~\cite{ref_utom} adopts the CycleGAN architecture adding a saliency constraint to the loss function. This constraint forces the retention of the content mask for source and translated images, which should reduce content distortion during translation.
%Unsupervised content-preserving Transformation for Optical Microscopy (\textbf{UTOM})~\cite{ref_utom} focuses on content preserving way of the mapping between two image domains. 
%The authors of UTOM introduce salience constraint that allows to avoid distortion of the source image content. 
%The generator and discriminator architecture is adopted from CycleGAN. 
%However, the loss function was redesigned to maintain the consistency of the source image content mask.
%    One epoch takes 34 minutes approximately.
%    Trained for 200 epochs, which took 4 days, 17 hours and 23 minutes (estimated time).
\textbf{StainGAN}~\cite{ref_staingan} employed the CycleGAN approach for stain normalization in histological images. 
In contrast to CycleGAN, a standard ResNet~\citep{he2016deep} model was used in generator networks.
%    One epoch takes 4 hours approximately.
%    Trained for 40 epochs, which took 6 days and 16 hours (estimated time).
\textbf{StainNet}~\cite{ref_stainnet} leverages StainGAN as a teacher to learn a color pixel-to-pixel mapping with a small convolutional network (using 1x1 convolutions). 
The $L_1$ loss is used to match the output of StainGAN. \textbf{CUT}~\cite{park2020contrastive} proposed patchwise contrastive (PatchNCE) loss as an alternative to the cycle loss. PatchNCE is used to minimize the distance between the feature representations of patches from a source image $x$ and corresponding patches from $G(x)$ relatively to the distance
to randomly sampled patches from $x$ at other locations.
An additional loss (PatchNCE), applied to images from the target domain, functions as identity loss that prevents the generator from making unnecessary changes.   
%\textbf{CUT} is a contrastive learning framework for unpaired I2I translation presented in~\cite{park2020contrastive}. 
%The authors proposed an approach that encourages mapping of two corresponding input image patches to a similar point within the shared embedded space learned by a multi-layer perceptron network. 
%The other patches from the data set defined as negatives. 
%Patchwise contrastive loss is employed to ensure that negatives are drawn from within the same input image. 


%\textbf{StainNet}~\cite{ref_stainnet} leverages StainGAN to learn the color mapping by distillation learning. 
%To this end, L1 loss is used by StainNen to learn the output of StainGAN. 
%The authors indicate that StainNet is able to reach similar to StainGAN performance while increasing the speed up to 40 times. 
%%    One epoch takes 1 minute and 45 seconds approximately.
%%    Trained for 300 epochs, which took 8 hours and 45 minutes (estimated time).

\textbf{UNIT}~\cite{ref_unit} 
introduces a shared latent space forcing corresponding images from two domains to map to the same latent representation. The architecture consists of two GANs and two Variational Autoencoders~\cite{KingmaW13} (VAE) with encoders that generate latent codes and with decoders that are also generators of GANs. 
The loss function consists of two adversarial losses, two VAE  losses, and two VAE-like losses, which
implicitly model cycle consistency forcing the distribution of the latent codes of translated and original images to coincide.
%is a predecessor of MUNIT method described above. 
%Unlike MUNIT and DRIT it employs a fully shared latent space. 
%In order to achieve it, the authors employ architecture that consist of variational autoencoders (VAE) and GANs. 
%To establish the shared latent space the authors tie last few layers of the encoders trained for each domain as well as the connection weights of the first few layers of generators. 
%The objective of generators and encoders is to minimize cycle-consistency loss and VAE loss.
%    One epoch takes 3.12 hours approximately.
%    Trained for 115 epochs, which took 14 days,22 hours and 50 minutes (exact time).
While UNIT assumes a shared latent space, \textbf{MUNIT}~\cite{ref_munit} postulates that only part of the latent space (the content) can be shared across domains whereas the other part (the style) is domain specific. To translate an image to the target domain, its content code is recombined with a random style code in the target style space. The objective comprises an adversarial loss, an $L_1$ reconstruction error in the image space, as well as  content and style reconstruction errors in the latent space, all in both directions.
%is another method that proposes to decompose domain-specific (style code) and domain-agnostic (content code) latent spaces. 
%Doing so each domain gets an autoencoder that is composed of style and content code. 
%The model is trained with adversarial objectives to ensure that the generated image corresponds to a real image from the target domain and biderectional reconstruction objective that is used to reconstruct both images and embedding.
%    One epoch takes 2.66 hours approximately.
%    Trained for 46 epochs which took 5 days, 5 hours and 45 minutes (exact time).
%    Spectral weight normalization in the generator mandatory to be removed to avoid the inversion problem.
%    Loss weights need to be modified to preserve structure:
%    gan: 1\\
%    image recon: 10\\
%    content recon: 1\\
%    style recon: 1\\
%    cycle recon: 10\\
Like MUNIT, \textbf{DRIT}~\cite{ref_drit} factorizes feature representation space to domain-invariant content and domain-specific attribute (style) spaces. Similarly to MUNIT, the loss function includes an adversarial loss, $L_1$ self-reconstruction (image reconstruction) and $L_1$ latent regression (attribute reconstruction) losses. Cross-cycle consistency forces the twice translated image with swapped attribute features to be close to the source image. An additional content adversarial loss aims at distinguishing the domain membership of content codes. Finally, the KL loss forces attribute codes to obey a normal distribution.

%\textbf{DRIT} is a method proposed in~\cite{ref_drit} suggests to use two feature representation spaces: domain specific attribute space and domain-invariant content space that shares information from both domains. 
%This disentangled representation is achieved by the cross-cycle consistency loss function. 
%It employs adversarial loss to encourage the prevention of sharing the domain-specific attributes by content features and latent regression loss that aims to provide the invertible mapping between attribute embedding space and the corresponding output.
    
%    One epoch takes 16 minutes and 48 seconds approximately.
%    Trained for 300 epochs, which took 3 days and 12 hours (estimated time).


 
%    One epoch takes 1.63 hours approximately.
%    Trained for 30 epochs, which took 2 days, 1 hour and 2 minutes (estimated time).
%    Needs to be trained in both directions, so two different experiments run here.


%\end{document}
