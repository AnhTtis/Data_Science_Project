\section{Training details}
\label{appendix:b}

%In this paper, we evaluate and compare the performance of classical as well as GAN-based approaches described in Sections 2 and 3.
%We evaluate all the GAN-based methods according to their original implementation.

For all methods, we used the original code available from corresponding online repositories.
The training guidelines provided by the authors were followed, except for the methods mentioned below.
For MUNIT we removed the spectral weight normalization in the generator to avoid the erroneous inversion of the bright and dark parts of the tissue.
For UTOM we adapted the saliency thresholds to background and foreground intensities. 
%Instead of training on a reference image, for Macenko and Vahadane we averaged the stain matrix, and pseudo-maxima of stain concentrations, for ColorStat we averaged mean and standard deviation, over all the images in the training dataset from the target domain.
%Each model was initially trained with recommended number of epochs, saving intermediate checkpoints.
%Fake images were generated with each of the checkpoints, and visual quality and FID score as well as other metrics described in \ref{eval_metrics} were inspected to determine a reasonable number of epochs needed for each model given our dataset size.


%In order to apply Pix2Pix method we used intermidiate step of conversion of source image to greyscale image that served as a paired data. By its nature Pix2Pix is a paired method. Due to the lack of a paired data, we employed it as a colorization network. So we create 2 images from one image (in Lab color space). The input image is the L channel, and the ground truth is the AB channel, this allow us to generate paired data. At inference time, the L channel of the image is fed to the network, and it predicts the AB channel. We then combine these predicted channels with the input L channel to create an RGB image.

A few training hyperparameters (including the number of training epochs) for some I2I methods deviate from the recommended ones. They were adjusted based on the validation set, which we used to ensure acceptable quality of generated histological images relying on visual inspection and the FID score. The resulting number of epochs, as well as training and inference times are outlined in \tableref{Tab:table_model_details}. Other parameters will be outlined in the code repository accompanying this paper. For our experiments we used a machine with NVIDIA T4 GPU, 16 GB RAM (AWS 4DN Extra Large instance).
%Each model was initially trained with recommended number of epochs, saving intermediate checkpoints.
%Fake images were generated with each of the checkpoints, and visual quality and FID score as well as other metrics described in \ref{eval_metrics} were inspected to determine a reasonable number of epochs needed for each model given our dataset size.
%Multiple experiments with different configurations were conducted, each with a total of \textpm{20} epochs around the number of epochs considered necessary. Basic hyperparameters like initial learning rates, learning rate policies, weights assigned to different parts of the loss function, normalization types and others were tuned within this range of epochs. All final training hyperparameters will be made available as well the trained models.
%The batch size was set to use most of the available GPU RAM to speed up training.
%Training and inference times for each of the models are detailed in  \tableref{table_model_details} in Appendix \ref{appendix:a}.

\begin{table}[htbp]
    \centering
    \floatconts
    {Tab:table_model_details}%
    {\caption{The number of training epochs, time per epoch, total training time, the number of network parameters and the inference times.}}
    \bgroup
    \def\arraystretch{1}
    \def\aboverulesep{0.ex}
    \def\belowrulesep{0.ex}
    {
        \begin{tabular}{l|rrr|rrr}
            \toprule
            \multicolumn{1}{l}{\multirow{2}{*}{Model}} & \multicolumn{3}{|c|}{Training} & \multicolumn{3}{c}{Inference} \\
            & Epochs & Epoch (hours) & Time (days) & Params  & GPU (s) & CPU (s) \\
            \midrule
            CUT       & 30     & 1.63          & 2.04        & 11.38 M & 0.034   & 0.876   \\
            ColorStat & 1      & 0.059         & 0.0024      & 6       &         & 0.009   \\
            CycleGAN  & 40     & 3.34          & 5.56        & 11.38 M & 0.027   & 0.571   \\
            DRIT      & 300    & 0.27          & 3.5         & 21.27 M & 0.035   & 0.597   \\
            Macenko   & 1      & 1.27          & 0.0528      & 8       &         & 0.317   \\
            MUNIT     & 46     & 2.66          & 5.24        & 30.26 M & 0.045   & 0.833   \\
            Pix2Pix   & 30     & 0.18          & 0.23        & 54.41 M & 0.010   & 0.115   \\
            StainGAN  & 40     & 4             & 6.67        & 11.38 M & 0.028   & 0.560   \\
            StainNet  & 300    & 0.029         & 0.36        & 1.28 K  & 0.002   & 0.009   \\
            UNIT      & 115    & 3.12          & 14.95       & 12.56 M & 0.030   & 0.474   \\
            UTOM      & 200    & 0.57          & 4.71        & 54.41 M & 0.007   & 0.113   \\
            Vahadane  & 1      & 7.95          & 0.3313      & 8       &         & 2.276   \\
            \bottomrule
        \end{tabular}
    }
    \egroup
\end{table}
