\documentclass{article}
\include{definitionchen}
\graphicspath{{figs/}}

% Language setting
% Replace `English' with, e.g., `Spanish' to change the document language
\usepackage[english]{babel}

% Set page size and margins
% Replace `letter paper' with `a4paper' for UK/EU standard size

\title{RKHS-based Latent Position Random Graph Correlation}
\author{Xiaoyi Wen, Liping Zhu}

\linenumbers
\renewcommand\makeLineNumber{}

\begin{document}


\begin{center}
	{\bf\Large RKHS-BASED LATENT POSITION RANDOM GRAPH CORRELATION}\\
\renewcommand{\thefootnote}{\fnsymbol{footnote}}%\renewcommand{\thefootnote}{\arabic{footnote}}
\vskip0.3cm
XIAOYI WEN$^{1}$, JUNHUI WANG$^{2}$, LIPING ZHU$^{1}$
\\\vskip0.3cm
{\it
Renmin University of China$^{1}$, Chinese University of Hong Kong$^{2}$ 
}\\

\today

\begin{singlespace}
	\footnotetext[1]{Xiaoyi Wen is a Ph.D. student, and  Liping Zhu (Email: zhu.liping@ruc.edu.cn) is a Professor and corresponding author, Center for Applied Statistics and Institute of Statistics and Big Data,    	Renmin University of China, % 59 Zhongguancun Avenue, Haidian District,
		Beijing 100872, China. Junhui Wang is a Professor, Department of Statistics, Chinese University of Hong Kong, Hong Kong, China.}
	% \footnotetext[4]{\scriptsize The authors thank the editor, the AE, and the reviewers for their constructive comments, which have dramatically improved the earlier version of this article. }
\end{singlespace}
\end{center}
\vskip12pt
\linenumbers


\begin{abstract}
In this article, we consider the problem of testing whether two latent position random graphs are correlated. We propose a test statistic based on the kernel method and introduce the estimation procedure based on the spectral decomposition of adjacency matrices. Even if no kernel function is specified, the sample graph covariance based on our proposed estimation method will converge to the population version. The asymptotic distribution of the sample covariance can also be obtained. We design a procedure for testing independence under permutation tests and demonstrate that our proposed test statistic is consistent and valid. Our estimation method can be extended to the spectral decomposition of normalized Laplacian matrices and inhomogeneous random graphs. Our method achieves promising results on both simulated and real data.
\vskip2mm
\noindent{\bf KEY WORDS:} \ independence test; latent position random graphs; adjacency spectral embedding; kernel methods.
\end{abstract}

\section{Introduction}

Identifying statistically significant dependency between data attribute values is often the key to further analysis. There are many methods to test structured data's linear and nonlinear dependence, and they have achieved satisfactory results. However, the technique for the dependency between high-dimensional or complex unstructured data still needs to be further developed. 

The development of social science, biology and economics poses challenges to the research of network data. Identifying the correlation between two sets of graphs and testing for independence is a crucial step in many random graph analyses. Calculating the correlation coefficient between two random graphs can provide more information and help us make statistical inferences. Correlations between protein networks may imply that proteins sharing different functions are involved in the same biological process \citep{ito2001comprehensive}. Homologous relationships between protein networks can be used to predict the binding of drugs to proteins associated with known drug targets while helping to predict indirect consequences of drug treatment, such as side effects and adverse drug interactions \citep{chu2008construction, kuhn2008large, klipp2010biochemical}. Habitual abuse is caused by dysfunction in specific regions of the brain that are thought to be part of the brain network associated with drug addiction. Comparing the correlation differences between functional networks of different brain regions in patients and healthy controls can identify which areas play a role in drug addiction behavior \citep{janes2012prefrontal, sjoerds2017loss}. Besides, the correlations between the brain networks’ activations can reflect the unique synergistic relationship between neurons \citep{dorum2016age}. In social science and business analysis, the correlation analysis of international trade and financial networks can provide new explanations for the spread of financial crises \citep{schiavo2010international}. In addition, Correlations between stock trading networks on different dates can also help reveal stock market interdependencies \citep{liu2012dynamics}.

Despite the widespread application of network correlation analysis, statistical methods for correlation analysis of network data are still limited. Traditional correlation coefficients such as Pearson's only detect the linear correlation between random variables, which is unsuitable for network data with topology and nonlinear relationships. Since the edges of the adjacency matrix are not independent, the correlation coefficient of the two adjacency matrices cannot be calculated directly. One approach is to pack the attributes of each node into a vector of smaller dimensions through graph embedding and then apply existing methods directly \citep{shen2019distance, lee2019network, xiong2019graph}. Another approach is to assume that the random graph was generated by a specific model and then propose a test method based on the model assumptions. For example, \citep{fosdick2015testing} assumes that the latent positions in a random graph follow a multivariate normal distribution, then applies a likelihood ratio test to test the dependencies between network and node attributes. In addition, some papers construct test statistics based on the graph's topology attributes. To test the independence of the two groups of random graphs, \citep{fujita2017correlation} proposes Spearman's rank correlation coefficient calculated from the largest eigenvalues of the two groups of random graphs. Using the low-rank factorization, \citep{durante2018bayesian} presents a Bayesian procedure for inference and testing group differences in network structure.

The methods mentioned above for testing the independence of random graphs are almost all based on Pearson's correlation or distance correlation. Another independence testing method is based on the kernel method \citep{gretton2005measuring}. The data is sampled by a kernel matrix and is generally consistent when using any feature kernel. The kernel method has better finite sample testing power in some nonlinear dependencies than the distance correlation. The kernel method is mainly divided into two parts in studying random graphs. One is to assume that the kernel function generates the link matrix $\bP$ of the random graph, which is also a general form of the latent position random graph \citep{hoff2002latent}. The second is to solve problems in random graphs via kernel-based methods, such as two-sample testing \citep{tang2017nonparametric} or community detection \citep{kloster2014heat}. 

Although kernel methods have vital applications in the statistical inference of random graphs, the literature on kernel methods in the independence test of random graphs is still relatively lacking. Independence tests based on kernel methods are more flexible and perform well in various nonlinear dependencies, and kernel methods are closely related to the generation mechanism of random graphs. Therefore, the main contributions of our paper are listed below:
\begin{itemize}
    \item A random graph independence test method based on the kernel method is proposed, and an estimation of the test statistic is given without explicitly specifying the kernel function.
    \item Proved that the sample covariance estimated by the adjacency matrix spectral decomposition under the latent position graph model converges to the covariance of the population version.
    \item Proved the consistency of the estimated covariances using the normalized Laplacian's spectral decomposition and the consistency in the inhomogeneous random graphs.
\end{itemize}

The paper is organized as follows: In Section \ref{sec2}, according to the way that the latent position random graph is generated and the relationship between distance correlation and Hilbert-Schmidt correlation, we define the population graph correlation ($\mathrm{Gcor}$) and the unbiased estimates of sample $\mathrm{Gcor}$. In Section \ref{sec3}, we estimate the sample $\mathrm{Gcor}$ by spectral decomposition of the adjacency matrix and prove that it converges to the population version. We also give the asymptotic distribution of the sample covariance under the null and alternative hypotheses and prove that $\mathrm{Gcor}$ is valid and consistent under permutation tests. In Section \ref{sec4}, we discuss the relationship between the independence test of random graphs and traditional two-sample tests, demonstrate the consistency of independence tests using normalized Laplacian spectral decomposition, and extend our method to inhomogeneous random graphs. Section \ref{sec5} presents some numerical simulation results and the actual data application of the proposed method, and the article ends in Section \ref{sec6}. All the proofs and the setup of the simulation part are in the supplementary material.


\section{Independence Test in Random Graph}\label{sec2}

\subsection{Distance Covariance and HSIC}
First, we briefly review distance correlation \citep{szekely2007measuring} and Hilbert–Schmidt independence criterion \citep{gretton2007kernel}. The setting of the hypothesis testing of independence is as follows: given an i.i.d sample $Z=\left \{ (X_{i}, Y_{i})_{i=1}^{n} \right \} \sim F_{XY}$, we want to test
\begin{equation*}
    \begin{aligned}
    & H_{0}:F_{XY}=F_{X}F_{Y}, 
    & H_{1}:F_{XY}\ne F_{X}F_{Y}.
    \end{aligned}
\end{equation*}
For two random vectors $X \in \mathbb{R}^{p}$ and $Y \in \mathbb{R}^{q}$, the population distance covariance is defined as:
\begin{equation*}
    \mathrm{dCov}(X,Y)=\int_{\mathbb{R}^{p+q}}\frac{\left | \phi_{X,Y}(t,s)-\phi_{X}(t)\phi_{Y}(s) \right |^2}{c_{p}c_{q}\left | t \right |^{1+p}\left | s \right |^{1+q}}dtds, 
\end{equation*}
where $c_{p}=\frac{\pi^{(1+p)/2}}{\Gamma ((1+p)/2)}$. Theorem 7 in \citep{szekely2009brownian} shows another form of expression of the squared distance covariance:
\begin{equation*}
    \mathrm{dCov}(X,Y) =\mathrm{E}\left | X-{X}' \right |\left | Y-{Y}' \right |+\mathrm{E}\left | X-{X}' \right |\mathrm{E}\left | Y-{Y}' \right |-2\mathrm{E}\left | X-{X}' \right |\left | Y-{Y}'' \right |,
\end{equation*}
where $({X}',{Y}')$ and $({X}'',{Y}'')$ are independent copies of $(X,Y)$. As a generalization of distance covariance, the Hilbert-Schmidt covariance (hCov), also known as HSIC, was obtained by kernelizing the Euclidean distance, that is
\begin{equation*}
    \mathrm{hCov}(X,Y)=\mathrm{E}\left [ K(X,{X}')L(Y,{Y}') \right ]+\mathrm{E}\left [ K(X,{X}') \right ]\mathrm{E}\left [ L(Y,{Y}') \right ]-2\mathrm{E}\left [ K(X,{X}')L(Y,{Y}'') \right ],
\end{equation*}
where $K$, $L$ are user specified kernels. Like distance correlation, the Hilbert-Schmidt independence criterion equals 0 if and only if $X$ and $Y$ are independent \citep{gretton2005measuring}. The relationship between the semi-metric and kernels connects distance-based and RKHS-based statistics in hypothesis testing \citep{sejdinovic2013equivalence}. Theorem 4 in \citep{shen2021exact} states that when the metric and kernel are bijective to each other, the population distance covariance and HSIC in hypothesis testing have the following relationship: 
\begin{equation*}\label{eq_hd}
     \begin{aligned}
     \mathrm{dCov}(X,Y)&=\mathrm{E}\left [ d(X,{X}')d(Y,{Y}') \right ]+\mathrm{E}\left [ d(X,{X}') \right ]\mathrm{E}\left [ d(Y,{Y}') \right ]-2\mathrm{E}\left [ d(X,{X}')d(Y,{Y}'') \right ] \\
     &= \mathrm{hCov}(X,Y).
     \end{aligned}
\end{equation*}

\citep{szekely2014partial} proposed the $\mathcal{U}$-centering based unbiased sample distance covariance can be written as:
\begin{equation*}
    \mathrm{dCov}_{n}(X,Y) = (\widetilde{\mathbf{B}} \cdot \widetilde{\mathbf{C}}):= \frac{1}{n(n-3)}\sum_{i\neq j}\widetilde{b}_{ij}\widetilde{c}_{ij},
\end{equation*}
where $\widetilde{\mathbf{B}}$ and $\widetilde{\mathbf{C}}$ are $\mathcal{U}$-centered matrices of the distance matrices $\mathbf{B}$ and $\mathbf{C}$. The $\mathcal{U}$-centered matrix $\widetilde{\mathbf{B}}=(\widetilde{b}_{ij}) \in \mathbb{R}^{n \times n}$ is defined as follows:

\begin{equation*}
  \widetilde{b}_{ij}=\left\{\begin{gathered}b_{ij}-\frac{1}{n-2}\sum_{v=1}^{n}b_{iv}-\frac{1}{n-2}\sum_{u=1}^{n}b_{uj}+\frac{1}{(n-1)(n-2)}\sum_{u,v=1}^{n}b_{uv}, \quad  i\neq j,\\ 
0, \quad \hfill \ i= j.
\end{gathered}\right.
\end{equation*}

Combined with the relationship between distance covariance and Hilbert-Schmidt covariance, it is natural to propose the same estimator for $\mathrm{hCov}^2(X, Y)$ which is defined as\citep{zhu2020distance}: 
\begin{equation*}
    \mathrm{hCov}_{n}(X,Y) = (\widetilde{\mathbf{R}} \cdot \widetilde{\mathbf{H}}),
\end{equation*}
where $\widetilde{\mathbf{R}}$ and $\widetilde{\mathbf{H}}$ are $\mathcal{U}$-centered version of $\mathbf{R}=(r_{ij})_{i,j=1}^n$ and $\mathbf{H}=(h_{ij})_{i,j=1}^n$. For $i=j$, $r_{ij}=h_{ij}=0$, otherwise $r_{ij}=K(X_{i}, X_{j})$ and $h_{ij}=L(Y_{i}, Y_{j})$, where $K$ and $L$ are user-specified kernel function such as Gaussian kernel or Laplacian kernel. 

\subsection{Population Graph Correlation}

Network data is high-dimensional and unstructured data. Therefore the definition of correlation between random graphs is the key to constructing the random graph independence test. \citep{lyzinski2014seeded} defines the correlated Erd{\"o}s-R{\H e}nyi random graphs, that is, a pair of random graphs $G_{1}$ and $G_{2}$ are correlated if $A^{G_{1}}_{ij}$ and $A^{G_{2}}_{ij} $ have Pearson correlation $\rho$. Since in the Erd{\"o}s-R{\H e}nyi random graphs, the entries $A_{ij}$ in the adjacency matrix follows a Bernoulli distribution with probability $p$. Then we extend the definition of correlation networks to latent position random graphs. Suppose we have two latent positions $\left \{ X_{i} \right \}_{i=1}^n \overset{\mathrm{i.i.d} }{\sim} F_{X}$ and $\left \{ Y_{i} \right \}_{i=1}^n \overset{\mathrm{i.i.d} }{\sim} F_{Y}$, where $F_{X}$ and $F_{Y}$ are two distributions taking values in $\mathbb{R}^d$. Then we have two latent position matrices $\X=\left [ X_{1},X_{2},\cdots, X_{n}\right ]^{\mathrm{T} } \in \mathbb{R}^{n\times d} $ and $\Y=\left [ Y_{1},Y_{2},\cdots, Y_{n}\right ]^{\mathrm{T} } \in \mathbb{R}^{n\times d} $. The random graph $G_{1}$ was generated by kernel function $K(\cdot,\cdot)$, where kernel matrix $\mathbf{K}=(k_{ij})_{i,j=1}^n$ and $k_{ij}=\rho_{n} K(X_{i},X_{j})$ for $i \neq j$. The latent position random graph generated by the kernel function has a symmetric adjacency matrix whose entries $\left \{ A_{1,ij} \right \}_{i<j}$ follows independent Bernoulli distribution with probability $\left \{ k_{ij} \right \}_{i<j}$. Similarly, the random graph $G_{2}$ was generated by kernel function $L$, that is the elements in kernel matrix $\mathbf{L}=(l_{ij})_{i,j=1}^n$ are $l_{ij}=\rho_{n} L(Y_{i},Y_{j})$, where $\left \{ A_{2,ij} \right \}_{i<j}$ follows independent Bernoulli distribution with probability $\left \{ l_{ij} \right \}_{i<j}$. In this paper, we focus on the case with $\rho_{n}=1$ where the concentration property of the random graph assures that the connection probability can be consistently estimated by the graph embedding. We further extend the developed test to the semi-sparse random graph in Section \ref{sec4_3}.

When the kernel function $K$ and $L$ encode all the distribution information, the kernel is characteristic \citep{fukumizu2007kernel}. Then the $\left \{ A_{1,ij} \right \}_{i<j}$ and $\left \{ A_{2,ij} \right \}_{i<j}$ are correlated if and only if the latent positions are correlated. We say that two random graphs are independent if the latent positions $\X$ and $\Y$ are independent, and $G_{1}$ and $G_{2}$ are correlated if two latent positions $\X$ and $\Y$ are not independent. Therefore, the independence test of the latent position random graph is equivalent to the test of the independence of the latent positions that generate the random graph and can be described as follows:
\begin{equation*}
    \begin{aligned}
    & H_{0}:F_{XY}=F_{X}F_{Y}, 
    & H_{1}:F_{XY}\ne F_{X}F_{Y},
    \end{aligned}
\end{equation*}
The population graph correlation can be defined as follows for the undirected and no self-loops latent position random graphs.

\begin{definition}[Population Graph Correlation]

Suppose the latent position random graph $G_{1}$ was generated by the kernel function $K$, and the distribution of latent position for $G_{1}$ is $F_{X}$. Similarly, the latent position random graph $G_{2}$ was generated by the kernel function $L$, and the distribution of the latent position for $G_{2}$ is $F_{Y}$. Suppose kernel $K$ and $L$ are characteristic and $(X,Y)$, $({X}',{Y}')$, $({X}'',{Y}'')$ are iid as $F_{XY}$. We define the graph covariance of the population version as follows:
\begin{equation}\label{pop_gcov}
    \mathrm{gCov}(G_{1},G_{2})=\mathrm{E}\left [ K(X,{X}')L(Y,{Y}') \right ]+\mathrm{E}\left [ K(X,{X}') \right ]\mathrm{E}\left [ L(Y,{Y}') \right ]-2\mathrm{E}\left [ K(X,{X}')L(Y,{Y}'') \right ].
\end{equation}
The population variance and correlation are defined as
\begin{gather}\label{pop_gcor}
    \mathrm{gVar}(G_{1})=\mathrm{gCov}(G_{1},G_{1}),\nonumber \\
    \mathrm{gVar}(G_{2})=\mathrm{gCov}(G_{2},G_{2}),\nonumber \\
    \mathrm{gCor}(G_{1},G_{2}) = \mathrm{gCov}(G_{1},G_{2})/\sqrt{\mathrm{gVar}(G_{1})\cdot \mathrm{gVar}(G_{2})} .
\end{gather}

\end{definition}


Unlike other graph independence test methods using the distances in Euclidean space, we organize the correlation coefficients between random graphs with kernel distances in this paper, connecting with the generation mechanism of the latent position random graph. The latent position random graph model is a more general form of random dot product graph model \citep{young2007random}, and it is also closely related to inhomogeneous random graph model \citep{bollobas2007phase} and exchangeable random graph model \citep{diaconis2007graph}. The proposed graph covariance appears related to HSIC, but it makes natural use of the graph generating kernel functions and thus circumvents the long-standing difficulty of kernel selection in HSIC. We can also extend our dependence test method to the inhomogeneous random graph. Since latent position variables are unobservable, when calculating the correlation coefficient of the random graphs based on kernel function, it is necessary to estimate the latent position variable by graph embedding. 


\section{Sample Graph Correlations}\label{sec3}

In this section, we first show how to use the eigendecomposition of the adjacency matrix to approximate the feature maps of latent position random graphs generated by kernel functions. Then we propose a one-step estimation method based on the eigendecomposition of the adjacency matrix to estimate the population graph correlation and give some main conclusions.

\subsection{Estimation of Feature Maps}

Assume we have a compact metric space $(\mathcal{X},d)$ and $\nu$ denotes the strictly positive and finite Borel $\sigma$-field of $\mathcal{X}$. The $K:\mathcal{X} \times \mathcal{X}\rightarrow [0,1]$ is a continuous and positive definite kernel function. Then define the space of square-integrable functions on $\nu$ as $L^2(\mathcal{X},\nu)$, the integral operator $T_{K}:L^2(\mathcal{X},\nu) \rightarrow L^2(\mathcal{X},\nu)$ defined by:
\begin{equation*}
    T_{K}f := \int _{\mathcal{X}}K(x,{x}')f({x}')d\nu ({x}'), \quad f\in L^2(\mathcal{X},\nu).
\end{equation*}
$T_{K}$ is a continuous and compact operator. Let $\left \{ \lambda _{i} \right \}$ be the set of eigenvalues of $T_{K}$ and $\lambda _{1} \geq \lambda _{2} \geq \cdots \geq 0$. $\left \{ \psi _{i} \right \}$ be the set of corresponding eigenvectors.

Considering the feature mapping of a kernel which takes the form 
\begin{equation*}
    \left \langle \phi(x),\phi({x}')  \right \rangle_{\mathcal{H}}=K(x,{x}'),
\end{equation*}
the Mercer's representation theorem \citep{cucker2002mathematical} shows that the feature map $\phi(x)$ can be denoted as
\begin{equation*}
    \phi(x) = (\sqrt{\lambda_{j}}\psi _{j}(x):j=1,2,\cdots).
\end{equation*}
Let $\phi_{d}(x)$ as the truncation of $\phi(x)$ to $\mathbb{R}^d$, where
\begin{equation*}
    \phi_{d}(x) = (\sqrt{\lambda_{j}}\psi _{j}(x):j=1,2,\cdots,d).
\end{equation*}
The feature map can be approximated by the following decomposition method of the adjacency matrix.
\begin{definition}[Adjacency spectral embedding (ASE)]
Suppose a random graph generated by latent positions $\X \in \mathbb{R}^{n \times p}$ has an adjacency matrix $\A \in \mathbb{R}^{n \times n}$. The eigendecomposition of $\left | \A \right |= (\A^{\mathrm{T}}\A)^{1/2}$ can be written as
\begin{equation}
\left | \A \right |=\sum_{i=1}^{n}\lambda_{i}\mathbf{u}_{i}\mathbf{u}_{i}^{\mathrm{T}}
\end{equation}
where $\lambda _{1}\geq \lambda _{2}\geq \cdots \lambda _{n}$ are the eigenvalues of $\left | \A \right |$ and $\mathbf{u}_{1},\cdots ,\mathbf{u}_{n}$ are the corresponding eigenvectors. Let $\bS_{\mathrm{A}}=\mathrm{diag}\left ( \lambda _{1},\lambda _{2},\cdots,\lambda _{d} \right )$, which composed of the first $d$ eigenvalues. $\U_{\mathrm{A}}$ an $n \times d$ matrix composed of the corresponding eigenvectors $\mathbf{u}_{1},\cdots ,\mathbf{u}_{d}$.
\end{definition}
The following Lemma shows that the eigendecomposition of the adjacency matrix can be used to estimate the truncated feature map, and the deviation error vanishes at the rate of $\mathcal{O}(\sqrt{\log n/n} )$. 
\begin{lemm}\label{lemma1}{\citep{tang2013universally}}
In the latent position graph with kernel function, let $\U_{\mathrm{A}}\bS_{\mathrm{A}}\U_{\mathrm{A}}^{\mathrm{T}}$ be the eigendecomposition of adjacency matrix $\A$, then for some unitary matrix $\W \in \mathbb{R}^{d \times d}$ we have
\begin{equation}
\left \|\U_{\mathrm{A}}\bS_{\mathrm{A}}^{1/2}\W- \bm{\phi}_{d}  \right \|_{F}\leq 27\delta _{d}^{-2}\sqrt{d\ \mathrm{log}(n/\eta )},
\end{equation}
where $\bm{\phi}_{d}$ denotes the matrix on $\mathbb{R}^{n \times d}$ and its i-th row is $\phi _{d}(\x_{i})$, $\delta _{d}$ denotes the quantity $\lambda _{d}-\lambda _{d+1}$ and $\eta$ is a constant in $(0,1/2)$. Let $\hat{\phi }_{d}(\x_{i})$ the i-th row of $\U_{\mathrm{A}}\bS_{\mathrm{A}}^{1/2}\W$, for $i\in \left [ n \right ]$ and any $\varepsilon >0$:
\begin{equation}
\mathbb{P}\left ( \left \| \hat{\phi }_{d}(\x_{i}) -\phi _{d}(\x_{i})  \right \|> \varepsilon  \right )\leq 27\delta _{d}^{-2}\varepsilon ^{-1}\sqrt{\frac{6d\ \mathrm{log}n}{n}}.
\end{equation}
\end{lemm}

The above lemma was established for the dense random graph, according to the concentration inequality proposed by \cite{oliveira2009concentration}:
\begin{equation*}
\left \| \A -E \A \right \| \le 2\sqrt{n \log (n/ \eta)},
\end{equation*}
with a probability $1-\eta$. We have $E \A = \mathbf{K}$ for the latent position random graph generated by the kernel function.

\subsection{Definition}

Considering two latent position random graphs $G_{1}$ and $G_{2}$. $G_{1}$ are generated by the latent position matrix $\X \in \mathbb{R}^{n \times p}$ with kernel function $K$ and kernel matrix $\mathbf{K}$. $G_{2}$ are generated by the latent position matrix $\Y \in \mathbb{R}^{n \times q}$ with kernel function $L$ and kernel matrix $\mathbf{L}$. According to Mercer's representation theorem, the $d$-th truncated feature map for $\mathbf{K}$ is $\bm{\phi}_{1,d}$ and $\bm{\phi}_{2,d}$ for $\mathbf{L}$. $\A_{1}$ and $\A_{2}$ are their adjacency matrices, $\U_{1}\bS_{1}\U_{1}^{\mathrm{T}}$ and $\U_{2}\bS_{2}\U_{2}^{\mathrm{T}}$ are their eigendecomposition, respectively. The $\bS_{l}$ is a diagonal matrix composed of the top $d$ eigenvalues, and $\U_{l}$ is the corresponding eigenvectors for $l=1,2$. Denote $\U_{1}\bS_{1}^{1/2} \in \mathbb{R}^{n \times d}$ by $\hat{\bm{\phi}}_{1,d}$ and $\U_{2}\bS_{2}^{1/2} \in \mathbb{R}^{n \times d}$ by $\hat{\bm{\phi}}_{2,d}$. Then define the estimated kernel function $\hat{\mathbf{K}}=\hat{\bm{\phi}}_{1,d}\hat{\bm{\phi}}_{1,d}^{\mathrm{T}}$ and $\hat{\mathbf{L}}=\hat{\bm{\phi}}_{2,d}\hat{\bm{\phi}}_{2,d}^{\mathrm{T}}$. The $\mathcal{U}$-centered matrices are defined as $\widetilde{\mathbf{K}}$ and $\widetilde{\mathbf{L}}$ and the $\widetilde{\mathbf{K}}$ can be computed by the following formula:
\begin{equation*}
  \widetilde{k}_{ij}=\left\{\begin{gathered}\hat{k}_{ij}-\frac{1}{n-2}\sum_{v=1}^{n}\hat{k}_{iv}-\frac{1}{n-2}\sum_{u=1}^{n}\hat{k}_{uj}+\frac{1}{(n-1)(n-2)}\sum_{u,v=1}^{n}\hat{k}_{uv}, \quad  i\neq j,\\ 
0, \quad \hfill \ i= j.
\end{gathered}\right.
\end{equation*}
Similarly, we can obtain $\widetilde{\mathbf{L}}$. The unbiased estimator of population graph covariance can be represented as
\begin{equation}\label{sample_gcov}
    \mathrm{gCov}_{n}(G_{1},G_{2})=(\widetilde{\mathbf{K}} \cdot \widetilde{\mathbf{L}}):= \frac{1}{n(n-3)}\sum_{i\neq j}\widetilde{k}_{ij}\widetilde{l}_{ij}.
\end{equation}
The sample graph variance and correlations also can be defined as follows:
\begin{gather}\label{sample_gcor}
    \mathrm{gVar}_{n}(G_{1})=(\widetilde{\mathbf{K}} \cdot \widetilde{\mathbf{K}}),\nonumber \\
    \mathrm{gVar}_{n}(G_{2})=(\widetilde{\mathbf{L}} \cdot \widetilde{\mathbf{L}}),\nonumber \\
    \mathrm{gCor}_{n}(G_{1},G_{2})=\mathrm{gCov}_{n}(G_{1},G_{2})/\sqrt{\mathrm{gVar}_{n}(G_{1})\mathrm{gVar}_{n}(G_{2})}.
\end{gather}

There have been some methods for the choice of embedding dimension $d$. \citep{gu2021principled} proposes a principled technique to choose the dimension of the network embedding and compares the impact of the embedding dimension on the model in node2vec and LINE algorithms. \citep{fishkind2013consistent} estimates the random block model's embedding dimension by the consistent adjacency spectral partition method. The conclusion in \citep{oliveira2009concentration} shows that the embedding dimension can be selected by setting a threshold on the eigenvalues. Besides the selection methods in the random graph, the profile likelihood methods can also be used to determine the embedding dimension \citep{zhu2006automatic}. Lemma \ref{lemma1} shows that $d$ needs to satisfy $d\ll n/\log{n} $ to assure consistent estimation of the latent variables, so it requires extra attention when selecting embedding dimension in the random graph of small nodes.

\subsection{Main Results}

Lemma \ref{lemma1} shows that by eigendecomposition of the adjacency matrix $\A$, we can consistently estimate the truncated feature maps of the kernel matrix. Therefore, the kernel distance matrix estimated by the spectral decomposition of $\A$ can be regarded as an approximation of $\mathbf{K}$. Our proposed sample covariances, variances and correlations converge to their respective population versions in probability as the number of nodes increases. The expectation of sample correlations is equal to population correlations with a corresponding difference of $\mathcal{O}(\log{}n/n)$.

%The sampling process can be explained by the empirical kernel approximation in \citep{rosasco2010learning}.

{\theo{\label{theorem1}} 
Suppose two latent position graphs $G_{1}$ and $G_{2}$ are generated by latent positions $\X \in \mathbb{R}^{n \times p}$ and $\Y \in \mathbb{R}^{n \times q}$, respectively. Each row of $\X$ and $\Y$ are generated i.i.d from $(X,Y) \sim F_{XY}$. The population Hilbert-Schmidt covariance is defined as \ref{pop_gcov}. The sample graph covariance is calculated by \ref{sample_gcov}. Then we have
\begin{gather*}
    E(\mathrm{gCov}_{n}(G_{1},G_{2}))=\mathrm{gCov}(G_{1},G_{2})+\mathcal{O}(\log{}n/n),\\
   \mathrm{var} (\mathrm{gCov}_{n}(G_{1},G_{2}))=\mathcal{O}(\log{}n/n) ,\\
   \mathrm{gCov}_{n}(G_{1},G_{2}) \overset{n\rightarrow \infty }{\longrightarrow}  \mathrm{gCov}(G_{1},G_{2}).
\end{gather*}
}
This is one of the most important theorems in our paper. The complete proof is in the supplementary material. 

{\theo{\label{theorem2}} 
For two latent position random graphs $G_{1}=(V_{1},E_{1})$ and $G_{2}=(V_{2},E_{2})$ generated by kernel functions $K$ and $L$, respectively. Suppose the kernel functions are characteristic, then we have $\mathrm{gCor}_{n}(G_{1},G_{2}) \rightarrow 0$ when $G_{1}$ and $G_{2}$ are independent, $\mathrm{gCor}_{n}(G_{1},G_{2}) \rightarrow$ a positive constant when $G_{1}$ and $G_{2}$ are not independent.
}

Similar to the population version of $\mathrm{gCov}$, we can derive an asymptotic distribution of the sample covariance $\mathrm{gCov}_{n}(G_{1},G_{2})$. The sample covariance $\mathrm{gCov}_{n}(G_{1},G_{2})$ can alternatively represented using U-statistics \citep{song2007supervised}:
\begin{equation}\label{UhCov}
    \mathrm{gCov}_{n}(G_{1},G_{2})=(\textrm{C}_{n}^{4})^{-1}\sum_{i<j<q<r}^{n}h(U_{i},U_{j},U_{q},U_{r}),
\end{equation}
where the kernel $h$ of the U-statistics is defined by
\begin{equation*}
    \frac{1}{4!}\sum_{(s,t,u,v)}^{(i,j,q,r)}(K_{st}L_{st}+K_{st}L_{uv}-2K_{st}L_{su}),
\end{equation*}
and $K_{st}=K(X_{s}, X_{t})$ for $1\leq s,t\leq n$ and $U_{i}=(X_{i}, Y_{i})$. The sum of the above equation denotes all 4-tuples drawn without replacement from $\left \{ 1,\cdots,n \right \}$. The Theorem B in Chap.6 of \citep{serfling2009approximation} shows the asymptotic normality of U-statistics. Many papers give the asymptotic properties of HSIC based on this theorem \citep{gretton2007kernel, zhang2018large}, so we have the following theorem under $H_{1}$.
{\theo{\label{theorem3}} 
      Under $H_{1}$, the $\mathrm{gCov}_{n}(G_{1},G_{2})$ converges in distribution to a Gaussian according to
      \begin{equation*}
          n^{\frac{1}{2}}\left ( \mathrm{gCov}_{n}(G_{1},G_{2})- \mathrm{gCov}(G_{1},G_{2}) \right )\overset{D}{\rightarrow}\mathcal{N}(0,\sigma_{u}^2),
      \end{equation*}
      where $\sigma_{u}^2=16(R-\mathrm{gCov}^2)$ and 
      \begin{equation*}
          R = \frac{1}{n}\sum_{i=1}^{n}\left \{ (n-1)_{3}^{-1}\sum_{(j,q,r)\setminus  \left \{ i \right \}}h(u_{i},u_{j},u_{q},u_{r}) \right \}^2.
      \end{equation*}
where $(n)_{m}=\frac{n!}{(n-m)!}$, $(j,q,r)\setminus  \left \{ i \right \}$ denotes all 3-tuples drawn without replacement from $\left \{ 1,\cdots,n \right \}\setminus  \left \{ i \right \}$. 
}

The next theorem applies under $H_{0}$.
%definition of h,g not clear
{\theo{\label{theorem4}} 
      Under $H_{0}$, the $\mathrm{gCov}_{n}(G_{1},G_{2})$ converges in distribution according to
      \begin{equation*}
          n\cdot \mathrm{gCov}_{n}(G_{1},G_{2}) \overset{D}{\rightarrow} \sum_{l=1}^{\infty }\lambda_{l}(z_{l}^2-1),
      \end{equation*}
      where $z_{l} \overset{\mathrm{i.i.d.}}{\sim} \mathcal{N}(0,1)$, and $\lambda_{l}$ are the solutions of the eigenvalue problem:
      \begin{equation*}
          \int h(u_{i},u_{j},u_{q},u_{r})g_{l}(u_{j})dP_{u_{i}}dP_{u_{q}}dP_{u_{r}}=\lambda_{l}g_{l}(u_{i}).
      \end{equation*}
}

We are dealing with $\mathrm{gCov}_{n}(G_{1},G_{2})$ is an unbiased estimator of the population covariance $\mathrm{gCov}(G_{1},G_{2})$. For the biased estimator, the U-statistics will be replaced by the V-statistics, and the sum will be $\lambda_{l}z_{l}^2$ under $H_{0}$ \citep{gretton2007kernel}. We display the density of $\mathrm{gCov}_{n}(G_{1}, G_{2})$ under the null and alternative hypotheses, respectively, and the results are shown in figure \ref{density_gcov}.
%%% want to claim our estimated power is larger than the distance correlation between two graph embedding vectors.

\graphicspath{{figs/}}
	\begin{figure}[htbp!]
		\scriptsize
		\begin{center}
			\begin{tabular}{cc}
				\includegraphics[width=0.45\textwidth]{densityH0.pdf} & \includegraphics[width=0.45\textwidth]{densityH1.pdf} 
				\\
				(A): Empirical distribution of $\mathrm{gCov}_{n}$ under $H_{0}$ & (B): Empirical distribution of $\mathrm{gCov}_{n}$ under $H_{1}$ \\
			\end{tabular}
		\end{center}
		\caption{Empirical distribution of $\mathrm{gCov}_{n}$ under $H_{0}$ and $H_{1}$. In the univariate setting $p=q=1$, the number of nodes $n$ is 500, the kernel function of $G_{1}$ is a Gaussian kernel, and the kernel function of $G_{2}$ is a Laplace kernel. Under the null hypothesis, the latent variable $X$ follows a beta distribution with parameters $\alpha=1$ and $\beta=2$, and $Y$ follows a standard normal distribution. Under the alternative hypothesis, the latent variable $X$ follows a beta distribution with $\alpha=1$ and $\beta=2$, and $Y$ has a linear relationship with $X$. The histogram of empirical $\mathrm{gCov}_{n}$ is obtained by computing 500 independent cases.}
		\label{density_gcov}
\end{figure}


\subsection{Permutation Test}

We use the permutation test to test the independence of a pair of random graphs \citep{nichols2002nonparametric}. The permutation test procedure can approximate the distribution of a test statistic under the null hypothesis by randomly permuting the dataset, and this method is used in many studies \citep{shen2019distance, lee2019network, xiong2019graph}. Next, we discuss the calculation steps and prove the testing consistency of $\mathrm{gCov}$.

The latent position random graphs $G_{1}$ and $G_{2}$ are generated by latent variables $\X$ and $\Y$, respectively. We used the eigendecomposition of the adjacency matrix to estimate the latent position and denoted as $\hat{\X}$ and $\hat{\Y}$. The initial sample $\mathrm{gCor}$ statistic is calculated by $\hat{\X}$ and $\hat{\X}$ and denoted by $\mathrm{gCor}_{n}^{0}(G_{1},G_{2})$. Then shuffle the order of $\hat{\Y}$, record it as $\hat{\Y}^{*}$, calculate the sample $\mathrm{gCor}$ and record it as $\mathrm{gCor}_{n}^{*}(G_{1},G_{2})$. The permutation step is repeated $r$ times, and the following probability obtains the p-value estimate
\begin{equation*}
    Pr\left \{ \mathrm{gCor}_{n}^{*}(G_{1},G_{2})>\mathrm{gCor}_{n}^{0}(G_{1},G_{2}) \right \}.
\end{equation*}
The null hypothesis is rejected if the calculated p-value is less than a pre-specified critical level, usually set to 0.05 or 0.01. The following theorem states that the p-value obtained by the permutation test method is valid for testing the independence of random graphs using $\mathrm{gCor}$.

{\theo{\label{theorem5}} 
      Suppose two latent position random graphs $G_{1}$ and $G_{2}$ are generated by latent variables $\X$ and $\Y$, respectively. Given the Type \uppercase\expandafter{\romannumeral1} error level $\alpha > 0$, the sample $\mathrm{gCor}$ is a valid test statistic under the permutation test.
}

\section{Some Extentions}\label{sec4}

\subsection{Maximum Mean Discrepancy for Independence Test}\label{sec4_1}

In this section, we will introduce the maximum mean discrepancy and use it to illustrate the difference and connection between the independence test of the network and the traditional two-sample test problem of the random graph.

Assume $X \sim F_{X}$ and $Y \sim F_{Y}$, where $F_{X}$, $F_{Y}$ are two Borel probability measures defined on a domain $\mathcal{X}$. The definition of maximum mean discrepancy is as follows.
\begin{definition}[Maximum Mean Discrepancy (MMD)]
Let $\mathcal{F}$ denotes the set of measurable functions $f$, then the MMD is defined as
\begin{equation*}
\begin{aligned}
    \mathrm{MMD}(\mathcal{F},F_{X},F_{Y})& := \underset{f \in \mathcal{F}}{\mathrm{sup}}\left | \int fdF_{X}- \int fdF_{Y} \right |\\
    &= \underset{f \in \mathcal{F}}{\mathrm{sup}}\left | \mathrm{E}_{x}\left [ f(x) \right ] -\mathrm{E}_{y}\left [ f(y) \right ] \right |.
\end{aligned}
\end{equation*}
In \citep{muller1997integral}, this metric can also be called the integral probability metric.
\end{definition}

According to the properties in the RKHS, \citep{gretton2006kernel} proposed the maximum mean discrepancy for the two-sample test problem in kernel space. The kernel mean of $X$ is defined as follows:
\begin{equation*}
    \mu_{X}=\int K(x,\cdot)d F_{X}(x)=E_{X}K(X,\cdot),
\end{equation*}
The kernel mean of $Y$ is also similar. Then the MMD can be represented by the norm of kernel mean, as the Lemma 4 in \citep{gretton2006kernel}.
\begin{equation*}
   \mathrm{MMD}(\mathcal{F},F_{X},F_{Y})=\underset{\left \| f \right \|_{\mathcal{H}}\leq 1}{\mathrm{sup}}\left \langle \mu_{X}-\mu_{Y},f \right \rangle_{\mathcal{H}}=\left \| \mu_{X}-\mu_{Y} \right \|_{\mathcal{H}}.
\end{equation*}
The traditional two-sample hypothesis testing problem of random graphs aims to see whether two random graphs have similar structures. That is, whether the latent positions that generate the probability matrix are samples from the same distribution, then the null hypothesis is $H_{0}: F_{X}=F_{Y}$. For the MMD we have 
\begin{equation*}
    \mathrm{MMD}(\mathcal{F},F_{X},F_{Y})=0 \quad \mathrm{if \ and \ only \ if} \quad F_{X}=F_{Y}.
\end{equation*}
Based on the above conclusions, \citep{tang2017nonparametric} constructed a nonparametric hypothesis testing method to test whether the structures of two random graphs are similar. In the independence test problem for random graphs, the null hypothesis is $H_{0}: F_{XY}=F_{X}F_{Y}$, then we have the following theorem:
{\theo{\label{theorem6}} 
      Suppose two latent position random graphs $G_{1}$ and $G_{2}$ are generated by latent matrices $\X$ and $\Y$, respectively. Then we have
      \begin{equation*}
   \mathrm{MMD}(\mathcal{F},F_{XY},F_{X}F_{Y})=0 \quad \mathrm{if \ and \ only \ if} \quad G_{1}\perp G_{2} .
\end{equation*}
}

Although the traditional two-sample hypothesis testing and independent test in random graphs are two different problems, they can be related by MMD. The difference is the distribution functions compared to MMD.


\subsection{Extension to the normalized Laplacian}\label{sec4_2}

The eigendecomposition of Laplacian or the normalized Laplacian matrices can also be used to find feature representations for each node in a random graph. The Laplacian matrix is the discrete form of the Laplace-Beltrami operator, widely used in manifold learning or nonlinear dimension reduction such as Laplacian eigenmaps \citep{belkin2003laplacian} and diffusion maps \citep{coifman2006diffusion}. 

For the matrix $\M$ with non-negative elements, the normalized Laplacian of matrix $\M$ is defined as follows:
\begin{equation}
    \mathbf{\cL}(\M) : =  (\mathrm{diag}(\M \mathbf{1}))^{-1/2}\M (\mathrm{diag}(\M \mathbf{1}))^{-1/2}.
\end{equation}
We use the definition of the normalized Laplacian matrix in \citep{tang2018limit}, which may be slightly different from the normalized Laplacian in some papers, such as denoting it as $\mathbf{I}-\mathbf{\cL}(\M) $ \citep{belkin2003laplacian}. For the graph embedding, which uses the eigenvalues and eigenvectors of the normalized Laplacian, the definitions of these two normalized Laplacians are equivalent. Next, we introduce the definition of Laplacian spectral embedding in random graphs. 

\begin{definition}[Laplacian spectral embedding (LSE)]
Suppose a random graph generated by latent positions $\X \in \mathbb{R}^{n \times p}$ has an adjacency matrix $\A \in \mathbb{R}^{n \times n}$. The normalized Laplacian matrix is $\mathbf{\cL}(\A)$. Similar to the eigendecomposition of the adjacency matrix, the eigendecomposition of the adjacency matrix's normalized Laplacian matrix is
\begin{equation*}
    \mathbf{\cL}(\A)=\sum_{i=1}^{n}\tilde{\lambda }_{i}\tilde{\mathbf{u}}_{i}\tilde{\mathbf{u}}_{i}^{\mathrm{T}},
\end{equation*}
where $\tilde{\lambda} _{1}\geq \tilde{\lambda} _{2}\geq \cdots \tilde{\lambda} _{n}$ are the eigenvalues and $\tilde{\mathbf{u}}_{1},\cdots ,\tilde{\mathbf{u}}_{n}$ are the corresponding eigenvectors. Let $\tilde{\bS}_{\mathrm{A}}=\mathrm{diag}\left ( \tilde{\lambda} _{1},\tilde{\lambda} _{2},\cdots,\tilde{\lambda} _{d} \right )$, which composed of the first $d$ eigenvalues. $\tilde{\U}_{\mathrm{A}}$ an $n \times d$ matrix composed of the corresponding eigenvectors $\tilde{\mathbf{u}}_{1},\cdots ,\tilde{\mathbf{u}}_{d}$. The Lapalcian spectral embedding of $\A$ is $\breve{\X}=\tilde{\U}_{\mathrm{A}}\tilde{\bS}_{\mathrm{A}}^{1/2}$.
\end{definition}

For the latent position random graph $G=(V, E)$, suppose the kernel matrix $\K$ is generated by the latent positions $\X \in \mathbb{R}^{n \times p}$. Based on the concentration inequality in \citep{oliveira2009concentration}, we extend the Lemma \ref{lemma1} into the form of the normalized Laplacian matrix, shown as follows:

\begin{lemm}\label{lemma2}
In the latent position graph with kernel function, let $\tilde{\U}_{\mathrm{A}}\tilde{\bS}_{\mathrm{A}}\tilde{\U}_{\mathrm{A}}^{\mathrm{T}}$ be the eigendecomposition of normalized Laplacian matrix $\mathbf{\cL}(\A)$. The eigendecomposition of $\mathbf{\cL}(\K)$ is $\tilde{\U}_{\mathrm{K}}\tilde{\bS}_{\mathrm{K}}\tilde{\U}_{\mathrm{K}}^{\mathrm{T}}$, let $\tilde{\X}_{n}=\tilde{\U}_{\mathrm{K}}\tilde{\bS}_{\mathrm{K}}^{1/2}$. Then for some orthogonal matrix $\W \in \mathbb{R}^{d \times d}$ we have
\begin{equation}
\left \| \breve{\X}_{n}\W- \tilde{\X}_{n}  \right \|_{F}\leq 168\delta_{d}^{-2}\sqrt{\frac{d\log{(4n/\eta)}}{\log{n}}},
\end{equation}
$\delta _{d}$ denotes the quantity $\lambda _{d}-\lambda _{d+1}$ and $\eta$ is a constant in $(0,1/2)$. Let $\breve{\phi }_{d}(\x_{i})$ the i-th row of $\tilde{\U}_{\mathrm{A}}\tilde{\bS}_{\mathrm{A}}^{1/2}\W$ and $\tilde{\phi }_{d}(\x_{i})$ the i-th row of $\tilde{\U}_{\mathrm{K}}\tilde{\bS}_{\mathrm{K}}^{1/2}$, for $i\in \left [ n \right ]$ and any $\varepsilon >0$:
\begin{equation}
\mathbb{P}\left ( \left \| \breve{\phi}_{d}(X_{i}) -\tilde{\phi}_{d}(X_{i})  \right \|> \varepsilon  \right )\leq 168\delta_{d}^{-2}\varepsilon ^{-1}\sqrt{\frac{2d\log{(4n^3)}}{n\log{n}}}.
\end{equation}
\end{lemm}

Lemma \ref{lemma2} indicates that the eigendecomposition of $\mathbf{\cL}(\A)$ can be used to approximate the eigendecomposition of $\mathbf{\cL}(\K)$, so we have the following theorem.

{\theo{\label{theorem7}} 
      Suppose two latent position random graphs $G_{1}$ and $G_{2}$ are generated by latent matrices $\X$ and $\Y$, respectively, and the kernel matrices are $\K_{1}$ and $\K_{2}$. Let $\A_{1}$ be the adjacency matrix of $G_{1}$ and $\A_{2}$ be the adjacency matrix of $G_{2}$. The spectral decomposition of $\mathbf{\cL}(\A_{1})$ is $\breve{\X}$ and $\mathbf{\cL}(\A_{2})$ is $\breve{\Y}$. The spectral decomposition of $\mathbf{\cL}(\K_{1})$ is $\tilde{\X}$ and $\mathbf{\cL}(\K_{2})$ is $\tilde{\Y}$. Then we have 
      \begin{gather*}
    E(\mathrm{dCov}_{n}(\breve{\X},\breve{\Y}))=\mathrm{dCov}_{n}(\tilde{\X},\tilde{\Y})+\mathcal{O}(1/n),\\
   \mathrm{dCov}_{n}(\breve{\X},\breve{\Y}) \overset{n\rightarrow \infty }{\longrightarrow}  \mathrm{dCov}_{n}(\tilde{\X},\tilde{\Y}),
     \end{gather*}
     where $\mathrm{dCov}_{n}$ denotes the sample version of distance covariance.
}

The above theorem still holds when the Hilbert-Schmidt covariance replaces the distance covariance. Considering the calculation of Hilbert-Schmidt covariance needs to select the kernel function, we choose distance covariance to avoid confusion with the kernel function that generates random graphs. This theorem is still established under the Hilbert-Schmidt covariance. 

The spectral decomposition of $\mathbf{\cL}(\A)$ can only approximate the spectral decomposition of $\mathbf{\cL}(\K)$. This step can be regarded as the graph embedding of the random graph. Then we can calculate the distance correlation or Hilbert-Schmidt correlation after obtaining lower-dimensional data. Using normalized Laplacian differs from our previous one-step estimation method, a two-step estimation procedure. However,  graph embedding is necessary when dealing with random graphs because the direct computation of Euclidean distances in high dimensions ignores the manifold's topology. This two-step estimation is also widely adopted \citep{lee2019network}, and our theorem above is theoretically proven valid.

\subsection{Extension to Inhomogeneous Random Graphs}\label{sec4_3}

We can extend these conclusions to inhomogeneous random graphs \citep{bollobas2007phase}. Given the i.i.d latent variable $\left \{ X_{i} \right \}_{i=1}^n \sim F$, the kernel matrix $\K_{n}=\left \{ \rho_{n} K(X_{i},X_{j}) \right \} _{i,j=1}^n$. The $\rho_{n} \in (0,1)$ is a scaling parameter that makes the random graph sparse, which is more reliable with some practical application scenarios. A common choice for $\rho_{n}$ is $\rho_{n}=(\log{n})/n $ \citep{bollobas2007phase, oliveira2009concentration}. We assume the scaling parameter $\rho_{n}$ and the concentration of the random graph satisfies the following two conditions.

\begin{itemize}
  \item[] \textbf{Condition 1.} The scaling parameter $\rho_{n}$ satisfies $n \rho_{n}=\omega \left ( (\log n)^{\frac{1}{2}+\epsilon} \right ) $ for $\epsilon > 0$. Here the sequence $a_{n}=\omega (b_{n})$ means for any positive constant $C$, there exists $n_{0}$ such that $a_{n}>C b_{n}$ for all $n \le n_{0}$.
  \item[] \textbf{Condition 2.} Let $\A$ be the adjacency matrix of the inhomogeneous random graph and $E \A$ denotes its expectation. Then we have
  \begin{equation*}
     \left \|  \A - E\A \right \| \le \sqrt{\log n}
  \end{equation*}
  with high probability.
\end{itemize}
Condition 1 makes the inhomogeneous random graph become a semi-sparse random graph. The semi-sparse random graph also concentrates however, the deviation bound may change. So condition 2 is not very strict. Satisfying the above two conditions, our proposed method for calculating the sample $\mathrm{gCov}_{n}$ can still converge to the population version of $\mathrm{gCov}$, as shown in the following theorem.

{\theo{\label{theorem8}} 
Suppose two random graphs $G_{1}$ and $G_{2}$ are generated by latent positions $\X \in \mathbb{R}^{n \times p}$ and $\Y \in \mathbb{R}^{n \times q}$, respectively. The scaling parameter is $\rho_{n}$. Each row of $\X$ and $\Y$ are generated i.i.d from $(X,Y) \sim F_{XY}$. The population Hilbert-Schmidt covariance is defined as \ref{pop_gcov}. The sample graph covariance is calculated by \ref{sample_gcov}. Then we have
\begin{gather*}
   \mathrm{gCov}_{n}(G_{1},G_{2}) \overset{n\rightarrow \infty }{\longrightarrow}  \mathrm{gCov}(G_{1},G_{2}).
\end{gather*}
}

We can see that the main difference between the sparse and dense settings is that the convergence of the sample $\mathrm{gCov}_{n}$ will be affected by the scaling parameter $\rho_{n}$ under the sparse setting. We can also recover concentration after regularization for sparse random graphs, and the effect of regularization remains to explore.


\section{Numerical Studies}\label{sec5}

\subsection{Simulations}

In the numerical simulation section, we verify the convergence of Gcorr and compare the simulation results of Gcorr, MGC, and Dcorr under nine different simulation settings \citep{szekely2007measuring, shen2019distance, lee2019network}. The first four can be considered linear or approximate linear relationships, and the last five are nonlinear. The specific distributions are shown in the appendix. 

\textbf{Study 1.} First, we use two linear relationships to verify that the sample Gcorr will converge to the HSIC directly calculated from the latent position when the number of network nodes increases. The graph's number of nodes $n$ is from 100 to 5000. Suppose the latent positions $X_{1}, \cdots,X_{n}$ are generated from a beta distribution with $\alpha=1$ and $\beta=2$, we consider two linear relationships of $X$ and $Y$: (i) $Y=X$; (ii) $Y=\mathrm{exp}(X)$. Two noise levels are also set for each relationship separately. The kernel functions for generating random graphs are Gaussian kernel $K(x,x^{'})=\mathrm{exp}(-\frac{(x-x^{'})^2}{\sigma ^2} ) $ with $\sigma=1$ and Laplace Kernel $K(y,y^{'})=\mathrm{exp}(-c\left | y-y^{'} \right | )  $ with $c=1$. For every choice of nodes, we calculate the Gcorr from the embedding dimension $d$ 1 to 5 and get the mean Gcorr. Figure \ref{fig:asym_gcorr} shows that Gcorr converges to the HISC computed directly from the latent positions as the number of nodes increases. Convergence also holds under white noise with variances of 0.05 and 0.1 but may require more nodes. It is worth noting that in practical applications, the latent position cannot be observed and can only be estimated by graph embedding, which also shows the rationality of our estimation method. 

\graphicspath{{figs/}}
	\begin{figure}[h!]
		\scriptsize
		\captionsetup{}
		\begin{center}
			\begin{tabular}{cc}
				\psfig{figure=linear_asym,width=2.3in,angle=0} & \psfig{figure=exp_asym,width=2.3in,angle=0} \\
				(A): Linear with noise level 0 & (B): Exponential with noise level 0 \\
				\psfig{figure=linear_asym1,width=2.3in,angle=0} & \psfig{figure=exp_asym1,width=2.3in,angle=0} \\
				(C): Linear with noise level 0.05 & (D): Exponential with noise level 0.05 \\
				\psfig{figure=linear_asym2,width=2.3in,angle=0} & \psfig{figure=exp_asym2,width=2.3in,angle=0} \\
				(E): Linear with noise level 0.1 & (F): Exponential with noise level 0.1 \\
			\end{tabular}
		\end{center}
		\caption{Simulation results of the sample Gcorr ($\bullet $) and HSIC ($\times$). The left column denotes latent positions $X$, and $Y$ have a linear relationship, and the right column denotes the exponential relationship. The first row results from noise level 0, the second is noise level 0.05, and the third is 0.1. The vertical axis represents the calculated correlation, and the horizontal axis represents the number of graph nodes.} 
		\label{fig:asym_gcorr}
	\end{figure}

\textbf{Study 2.} Figure \ref{fig:compare_gcorr_mgc} shows the sample statistics of Gcorr and MGC for nine correlation settings. For each simulation, we generate a latent position sample $(X, Y)$ at $p=q=1$ and nodes $n=100,500,1500,2000$ with white noise and compare their sample statistics. For the MGC of the random graph,  we use the method in \citep{lee2019network}, which uses the eigendecomposition of normalized Laplacian matrix as the sample data. For the dependence relationships (types 1-8), Gcorr and MGC are almost all significantly greater than 0. When the number of nodes in the network is small, Gcorr is considerably higher than MGC, especially in the four nonlinear relationships (types 5-8). As a one-step estimation method, the calculation of Gcorr directly uses the adjacency matrix spectral decomposition to estimate the kernel distance of the latent position. In study 1, we can also see that Gcorr will converge to HSIC of latent position vectors as the number of nodes increases. Although HSIC may not perform as well as MGC in characterizing local correlations, this one-step approach makes estimation more straightforward and interpretable.

\textbf{Study 3.} Figure \ref{fig:compare_pow} shows the testing power of Gcorr, MGC and Dcorr under finite samples. Let $p=q=1$ and the number of nodes from 20 to 100 with white noise. We use the following method to estimate the testing power. First, we generate the dependent sample data and calculate the sample statistics, then shuffle the order of $Y$ to generate independent $(X, Y)$ and calculate the new sample statistic. The above process is repeated 1000 times to obtain the p-value, and the testing power is estimated at the Type \uppercase\expandafter{\romannumeral1} error level $\alpha=0.05$ with 100 repetitions. It can be seen that Dcorr does not perform as well as Gcorr and MGC, while the performance of Gcorr and MGC are almost identical in different simulation settings. The testing power reaches 1 when the number of nodes increases to 100, except for the relationship of the W-shape. Therefore, there should be no significant difference between Gcorr and MGC in testing power because the number of nodes is relatively large in the actual data. Since MGC considers local correlations, it is reasonable that the testing power of MGC is better than Dcorr under nonlinear relations. Our method directly estimated the kernel space's distance, which is also a nonlinear relationship.

\graphicspath{{figs/}}
	\begin{figure}[htbp!]
		\scriptsize
		\begin{center}
			\begin{tabular}{c}
				\includegraphics[width=0.9\textwidth]{sim_diff_dis.pdf}
				\\
			\end{tabular}
		\end{center}
		\caption{The top right corner is the visualization $(X, Y)$ generated with $n=200$ and $p=q=1$. The bar plot compares sample Gcorr (//) and MGC ($ \backslash\backslash$) when the number of nodes is $n=100, 500, 1500, 2000$. The values of the statistics are between 0 and 1, where 0 indicates no relationship. The horizontal axis of the bar plot represents the number of nodes, and the vertical axis represents the sample statistics.}
		\label{fig:compare_gcorr_mgc}
\end{figure}

\graphicspath{{figs/}}
	\begin{figure}[htbp!]
		\scriptsize
		\begin{center}
			\begin{tabular}{c}
				\includegraphics[width=0.9\textwidth]{compare_pow.pdf}
				\\
			\end{tabular}
		\end{center}
		\caption{Comparing the testing power of Gcorr, MGC, and Dcorr for nine univariate simulations. The dependent sample data was generated 100 times, and independent data was generated 1000 times. The number of nodes $n$ from 20 to 100 and the Type \uppercase\expandafter{\romannumeral1} error level $\alpha=0.05$. The solid line represents Gcorr, the long dashed line represents Dorr, and the short dashed line represents MGC. The horizontal axis represents the number of nodes, and the vertical axis represents the empirical power.}
		\label{fig:compare_pow}
\end{figure}

\textbf{Study 4.}

Figure \ref{fig:compare_time} shows the running time of Gcorr, MGC and Dcorr under finite samples. The p-value of MGC is calculated using the \textit{multiscale\_graphcorr} function of the \textit{scipy.stats} package in python, the p-value of Dcorr is calculated using the \textit{dcor} package in python, and Gcorr uses the function written by ourselves, and the timing uses the \textit{time} package. MGC and Dcorr use a two-step estimation method, first performing eigendecomposition of the normalized Laplacian of the random graph to obtain the low-dimensional vector representation of the random graph and then implementing an independence test between the low-dimensional vectors. Among the three methods, Dcorr is the fastest, MGC is the slowest, and Gcorr runs faster than MGC.

\graphicspath{{figs/}}
	\begin{figure}[htbp!]
		\scriptsize
		\begin{center}
			\begin{tabular}{c}
				\includegraphics[width=0.6\textwidth]{compare_time.pdf}
				\\
			\end{tabular}
		\end{center}
		\caption{Comparing the running time of Gcorr, MGC, and Dcorr for different nodes. The number of nodes $n$ is from 100 to 2000. The $\blacklozenge$ represents Gcorr, the $\times$ represents Dorr, and the $\bullet$ represents MGC. The horizontal axis represents the number of nodes, and the vertical axis represents the logscale running time.}
		\label{fig:compare_time}
\end{figure}

\subsection{Real Data Analysis}

Synaptic connections between neurons play an essential role in understanding the nervous system. The collaboration between neurons can be seen as a complex network through the connection of synapses. While this problem is widely addressed in theory, studying neuronal connections in the existing nervous systems requires electron microscopy, so this method is unsuitable for large structures. The anatomical wiring diagram of the nervous system that has now been fully observed is that of neuronal connections in Caenorhabditis elegans \citep{jarrell2012connectome}. 

The dataset we used is the neuronal connectivity network of the hermaphrodite C. elegans. This data was collected and organized in \citep{chen2006wiring,varshney2011structural}, and now it can be directly downloaded from \url{https://www.wormatlas.org/neuronalwiring.html#NeuronalconnectivityII}. The dataset contains 6417 connections of 280 nonpharyngeal neurons, covering six synaptic connection types: Send or output, Send-poly, Receive or input, Receive-poly, Electric junction, on and Neuromuscular junction. A complete network of neuronal connections in Caenorhabditis elegans can be established by this dataset. In addition, we divide the neurons into two parts that connect the sensory and the muscles of the body. The data used in this step also came from previous studies \citep{white1986structure, dixon2005muscle}. Two hundred sensory and motor neurons connect 20 sensory features and 95 body wall muscles. To enrich our study, we subdivided the connections between neurons and muscles into dorsal and ventral body wall muscles. The previously complete network of neuronal connections can be divided into three sub-networks responsible for sensory control, the dorsal body wall muscles and the ventral body wall muscles. Our study aims to explore whether there is a correlation between these three sub-networks. Figure \ref{fig:category_adj} shows the adjacency matrices for these sub-networks.

\graphicspath{{figs/}}
	\begin{figure}[h!]
		\scriptsize
		\captionsetup{}
		\begin{center}
			\begin{tabular}{ccc}
				\psfig{figure=adj_sensory,width=2in,angle=0} & \psfig{figure=adj_muscle1,width=2in,angle=0} &
				\psfig{figure=adj_muscle2,width=2in,angle=0} \\
				(A): Sensory & (B): Dorsal muscle & (C): Ventral muscle \\
			\end{tabular}
		\end{center}
		\caption{The adjacency matrices of three sub-networks.} 
		\label{fig:category_adj}
	\end{figure}
	
Next, we use the method proposed in this paper to test the network independence between three sub-networks. For the graph correlation,  we calculate the three sub-networks pairs: sensory versus the dorsal body wall muscle, sensory versus the ventral body wall muscle, and dorsal body wall muscle versus ventral body wall muscle, respectively. The embedding dimension d is set from 1 to 10. The P value obtained from the permutation test and corresponding graph correlation coefficient are shown in Figure \ref{fig:real_data_pval}. In addition, we set up a control group, that is, the sensory sub-network and a randomly generated Erd{\"o}s-R{\' e}nyi model with 80 nodes to test the correlation. Our results show a strong correlation between these three sub-networks; the P values are almost all less than 0.05 under different embedding dimensions.
	
\graphicspath{{figs/}}
	\begin{figure}[h!]
		\scriptsize
		\captionsetup{}
		\begin{center}
			\begin{tabular}{cc}
				\psfig{figure=sensory_muscle1,width=2.5in,angle=0} &
				\psfig{figure=sensory_muscle2,width=2.5in,angle=0} \\
				(A): Sensory and Dorsal muscle & (B): Sensory and Ventral muscle \\
				\psfig{figure=muscle1_muscle2,width=2.5in,angle=0} &
				\psfig{figure=control_group,width=2.5in,angle=0} \\
				(C): Dorsal muscle and Ventral muscle & (D): Control Group \\
			\end{tabular}
		\end{center}
		\caption{Comparison of graph correlation coefficients and p-values. The permutation test was repeated $m=1000$. The Bar plot shows the graph correlation coefficient under different embedding dimensions, the solid blue line shows the test p-value, and the gray dotted line denotes the p-value of 0.05.} 
		\label{fig:real_data_pval}
\end{figure}
	
\section{Discussion}\label{sec6}

In this paper, we formalized the population version correlation of the latent position random graph, estimated the unbiased sample version by spectral decomposition of the adjacency matrix, and demonstrated convergence from the sample version to the population version. We proved the asymptotic distribution of the sample graph covariance. Numerical simulation confirms that $\mathrm{Gcor}$ performs well in linear and nonlinear dependencies and reduces computational complexity.

There are many potential aspects worthy of further pursuit. For example, our proposed method is based on latent space random graphs. Although we demonstrate that this method can be generalized to inhomogeneous random graphs, the estimation of scaling parameters for inhomogeneous random graphs needs further exploration. In addition, testing random graphs with small nodes displays relatively small testing power, and the choice of embedding dimension may affect the estimation of test statistics. The solution to these problems will further improve the dependence test of the random graph.




\begin{center}
	\bibliographystyle{cheng}
	\bibliography{Gcorr_ref}
\end{center}
%\begin{center}
	%\bibliographystyle{cheng}
	%\bibliography{sample}
%\end{center}

\clearpage

\begin{center}
	{\bf\Large SUPPLEMENT TO ``RKHS-BASED LATENT POSITION RANDOM GRAPH CORRELATION"}\\
%\renewcommand{\thefootnote}{\fnsymbol{footnote}}%\renewcommand{\thefootnote}{\arabic{footnote}}
%\vskip0.3cm
%CANYI CHEN$^{1}$, BINGZHEN CHEN$^{1,2}$, LINGCHEN KONG$^{2}$ and LIPING ZHU$^{1}$
%\\\vskip0.3cm
%{\it
%Renmin University of China$^{1}$ and  Beijing Jiaotong University$^{2}$
%}\\
%This version: \today

%\begin{singlespace}
%	\footnotetext[1]{Canyi Chen is Ph.D student, Bingzhen Chen is Assistant Professor, and  Liping Zhu (Email: zhu.li\\ping@ruc.edu.cn) is Professor and corresponding author, Center for Applied Statistics and Institute of Statistics and Big Data,    	Renmin University of China, % 59 Zhongguancun Avenue, Haidian District,
%		Beijing 100872, China.  Lingchen Kong is Professor, School of Science, Beijing Jiaotong University, Beijing 100044, China. The first two authors contribute equally to this paper. This research  is supported by National  Natural Science Foundation of China (12171477) and Beijing Natural Science Foundation  (Z190002).  }
%	% \footnotetext[4]{\scriptsize The authors thank the editor, the AE, and the reviewers for their constructive comments, which have led to a dramatic improvement of the earlier version of this article. }
%\end{singlespace}
\end{center}
\vskip12pt
\linenumbers



%\setcounter{section}{0} % reset section counter
% \renewcommand{\theequation}{S.\arabic{equation}} % prefix equation counter with S
\renewcommand{\theequation}{\Alph{section}.\arabic{equation}} % prefix equation counter with S
\renewcommand{\thesection}{\Alph{section}}% Adjust section printing (from here onward)
\renewcommand{\thetheo}{\Alph{section}.\arabic{theo}}
\renewcommand{\theprop}{\Alph{section}.\arabic{prop}}
\renewcommand{\thelemm}{\Alph{section}.\arabic{lemm}}
% \section{Technical Conditions and Proofs of Theorems}\label{appendix:conditions_and_proofs}
\begin{appendices}

All the proofs for theorems and technical lemmas are involved in \ref{proof}.
The simulation dependence functions and subnetwork visualizationin of real data application are in Section \ref{simu_and_describ}.

\section{Proof}\label{proof}

\begin{proof}[Proof of Theorem 1]

\textbf{Step 1.} First, introduce the Mercer's representation theorem in RKHS, and define the truncated population graph covariance.

From our previous definition, the population graph covariance $\mathrm{hCov}(G_{1},G_{2})$ of two random graphs can be written as
\begin{equation*}
    \begin{aligned}
     \mathrm{hCov}(G_{1},G_{2}) = \mathrm{E}\left [ K(X,{X}')L(Y,{Y}') \right ]+\mathrm{E}\left [ K(X,{X}') \right ]\mathrm{E}\left [ L(Y,{Y}') \right ]-2\mathrm{E}\left [ K(X,{X}')L(Y,{Y}'') \right ].
    \end{aligned}
\end{equation*}
Assume we have a compact metric space $(\mathcal{X},d)$ and $\nu$ denotes the strictly positive and finite Borel $\sigma$-field of $\mathcal{X}$. In the calculation equation of $\mathrm{hCov}(G_{1},G_{2})$, the $K:\mathcal{X} \times \mathcal{X}\rightarrow [0,1]$ is continuous and positive definite kernel function. For the dot product $\left \langle \cdot,\cdot \right \rangle$, the RKHS has the following reproducing property:
\begin{equation*}
    \left \langle f(\cdot),K(x,\cdot) \right \rangle=f(x).
\end{equation*}
Then define the space of square-integrable functions on $\nu$ as $L^2(\mathcal{X},\nu)$, the integral operator $T_{K}:L^2(\mathcal{X},\nu) \rightarrow L^2(\mathcal{X},\nu)$ defined by:
\begin{equation*}
    T_{K}f := \int _{\mathcal{X}}K(x,{x}')f({x}')d\nu ({x}'), \quad f\in L^2(\mathcal{X},\nu).
\end{equation*}
$T_{K}$ is a continuous and compact operator. Let $\left \{ \lambda _{i} \right \}$ be the set of eigenvalues of $\mathscr{K}$ and $\lambda _{1} \geq \lambda _{2} \geq \cdots \geq 0$. $\left \{ \psi _{i} \right \}$ be the set of corresponding eigenfunctions of $T_{K}$.

Considering the feature mapping of a kernel which takes the form 
\begin{equation*}
    \left \langle \phi(x),\phi({x}')  \right \rangle_{\mathcal{H}}=K(x,{x}'),
\end{equation*}
the Mercer's representation theorem shows that the feature map $\phi(x)$ can be denoted as
\begin{equation*}
    \phi(x) = (\sqrt{\lambda_{j}}\psi _{j}(x):j=1,2,\cdots).
\end{equation*}
Let $\phi_{d}(x)$ as the truncation of $\phi(x)$ to $\mathbb{R}^d$, where
\begin{equation*}
    \phi_{d}(x) = (\sqrt{\lambda_{j}}\psi _{j}(x):j=1,2,\cdots,d).
\end{equation*}
Let $K_{d}(x,{x}')=\left \langle \phi_{d}(x),\phi_{d}({x}')  \right \rangle_{\mathcal{H}}$ represents the d-th truncation of the kernel space, and the d-th truncation of $\mathrm{hCov}(G_{1},G_{2})$ is $\mathrm{hCov}^{(d)}(G_{1},G_{2})$, then the truncated population graph covariance can be defined as follows:
\begin{equation*}
\begin{aligned}
     \mathrm{hCov}^{(d)}(G_{1},G_{2}) &= \mathrm{E}\left [ K_{d}(X,{X}')L_{d}(Y,{Y}') \right ]+\mathrm{E}\left [ K_{d}(X,{X}') \right ]\mathrm{E}\left [ L_{d}(Y,{Y}') \right ] \\
& -2\mathrm{E}\left [ K_{d}(X,{X}')L_{d}(Y,{Y}'') \right ].
\end{aligned}
\end{equation*}

\textbf{Step 2. } Then we need to show the $\mathrm{hCov}^{(d)}(G_{1},G_{2})$ is equvalent to $\mathrm{hCov}(G_{1},G_{2})$, the proof process refers to \citep{tang2013universally}'s Lemma 3.4. We will use the projection to shows that the subspace spanned by $\phi_{d}$ is equivalent to the subspace spanned by $\phi$. 

%need add some kernel PCA

Let $\mathcal{H}$ be the RKHS induced by kernel function $K$ and define the operators $T_{\mathcal{H}}:\mathcal{H} \rightarrow \mathcal{H}$ and $T_{\mathcal{H},n}:\mathcal{H} \rightarrow \mathcal{H}$ given by:
\begin{equation*}
\begin{aligned}
    T_{\mathcal{H}}f := \int _{\mathcal{X}}\left \langle f, K(\cdot,x) \right \rangle_{\mathcal{H}} K(\cdot,x)d\nu(x), \\
    T_{\mathcal{H},n}f := \frac{1}{n}\sum_{i=1}^{n} \left \langle f, K(\cdot,x_{i}) \right \rangle_{\mathcal{H}} K(\cdot,x_{i}).
\end{aligned}
\end{equation*}
Assume $\lambda$ is a nonzero eigenvalue of $T_{K}$ and $u$, $v$ are associated eigenfunctions of $T_{K}$ and $T_{\mathcal{H}}$, normalized to norm 1 in $L^2(\mathcal{X},\nu)$ and $\mathcal{H}$ respectively, then for $x \in \mathrm{supp}(\nu)$ we have:
\begin{equation}\label{uv}
    \begin{aligned}
    u(x) = \frac{1}{\sqrt{\lambda}}v(x), \quad v(x) = \frac{1}{\sqrt{\lambda}}\int_{\mathcal{X}}K(x,s)u(s)d\nu(s).
    \end{aligned}
\end{equation}
The spectra decomposition of kernel matrix $\R$ can be used to estimate the eigenfunction of $T_{K}$. Suppose $\hat{\lambda}$ is a nonzero eigenvalue and $\hat{u}$, $\hat{v}$ are the associated eigenvector and eigenfunction of $\R$ and $T_{\mathcal{H},n}$, normalized to norm 1 in $\mathbb{R}^n$ and $\mathcal{H}$ respectively, then:
\begin{equation}\label{hat_uv}
    \hat{u}= \frac{1}{\sqrt{\hat{\lambda}}}(\hat{v}(x_{1},\cdots,\hat{v}(x_{n}), \quad \hat{v}(\cdot)= \frac{1}{\sqrt{\hat{\lambda}}}\left ( \frac{1}{n}\sum_{i=1}^{n}K(\cdot,x_{i}) \hat{u}_{i}\right ).
\end{equation}
Let $\mathcal{P}_{\R}=\U_{\R}\U_{\R}^{\mathrm{T}}$, where $\U_{\R}$ denotes the eigenvectors of kernel matrix $\R=\left \{ K(x_{i},x_{j}) \right \}_{i,j=1}^n$ in graph $G_{1}$. For the samples $X_{1},X_{2},\cdots,X_{n}$, let $\phi_{r,n} \in \mathbb{R}^n$ be the vector with elements $\sqrt{\lambda_{r}}\psi_{r}(X_{i})$ for $i=1,2,\cdots,n$, where $\lambda_{r}$ are eigenvalues of $T_{K}$. Then $\R = \sum_{r=1}^{\infty}\phi_{r,n} \phi_{r,n}^{\mathrm{T}}$. Selecting the $d$ largest eigenvalues of $\R$ and denotes the associated eigenvectors as $\hat{u}^{(1)},\cdots,\hat{u}^{(d)}$. We have
\begin{equation*}
    \left \{ \mathcal{P}_{\R}(\R) \right \}_{i,j=1}^n = \sum_{s=1}^{d}\sum_{r=1}^{\infty }\hat{u}_{i}^{(s)}(\hat{u}^{(s)})^{\mathrm{T}}\phi_{r,n}\phi_{r,n}^{\mathrm{T}}\hat{u}^{(s)}\hat{u}_{j}^{(s)}.
\end{equation*}
For the $\hat{v}^{(1)},\cdots,\hat{v}^{(d)}$ defined in \ref{hat_uv}, the following formula is established:
\begin{equation*}
    \begin{aligned}
    \left \langle \hat{v}^{(s)} ,\sqrt{\lambda_{r}}\psi _{r}\right \rangle_{\mathcal{H}} &= \left \langle \frac{1}{\sqrt{\hat{\lambda_{s}}}}\left ( \frac{1}{n}\sum_{i=1}^{n}K(\cdot,X_{i}) \hat{u}_{i}^{(s)}\right ),\sqrt{\lambda_{r}}\psi _{r}\right \rangle_{\mathcal{H}} \\
    &= \frac{1}{\sqrt{\hat{\lambda_{s}}}n}\sum_{i=1}^{n}\psi_{r}(X_{i})\sqrt{\lambda_{r}}\hat{u}_{i}^{(s)} \\
    &= \frac{1}{\sqrt{\hat{\lambda_{s}}}n} \left \langle \hat{u}^{(s)},\phi_{r,n} \right \rangle_{\mathbb{R}^n} .
    \end{aligned}
\end{equation*}
Thus we have the following equivalent expression
\begin{equation*}
    \hat{u}_{i}^{(s)}(\hat{u}^{(s)})^{\mathrm{T}}\phi_{r,n} = \hat{u}_{i}^{(s)} \left \langle \hat{u}^{(s)},\phi_{r,n} \right \rangle_{\mathbb{R}^n} = \hat{v}^{(s)}(X_{i})\left \langle \hat{v}^{(s)} ,\sqrt{\lambda_{r}}\psi _{r}\right \rangle_{\mathcal{H}}.
\end{equation*}
Next, define the $\hat{\mathcal{P}}_{d}$ as the d-dimensional subspace of $T_{K}$, and we have
\begin{equation*}
    \begin{aligned}
    & \quad \left \langle \hat{\mathcal{P}}_{d}K(\cdot,X_{i}),\hat{\mathcal{P}}_{d}K(\cdot,X_{j}) \right \rangle_{\mathcal{H}} \\
&= \left \langle \sum_{s=1}^{d}\left \langle \hat{v}^{(s)},K(\cdot,X_{i}) \right \rangle _{\mathcal{H}}\hat{v}^{(s)}, \sum_{s=1}^{d}\left \langle \hat{v}^{(s)},K(\cdot,X_{i}) \right \rangle _{\mathcal{H}}\hat{v}^{(s)}\right \rangle_{\mathcal{H}} \\
    &=\sum_{s=1}^{d}\left \langle \hat{v}^{(s)}(X_{i})\sum_{r=1}^{\infty } \left \langle  \hat{v}^{(s)},\sqrt{\lambda_{r}}\psi_{r}\right \rangle_{\mathcal{H}}\sqrt{\lambda_{r}}\psi_{r}, \hat{v}^{(s)}(X_{j})\sum_{r=1}^{\infty } \left \langle  \hat{v}^{(s)},\sqrt{\lambda_{r}}\psi_{r}\right \rangle_{\mathcal{H}}\sqrt{\lambda_{r}}\psi_{r}\right \rangle_{\mathcal{H}}\\
    &= \sum_{s=1}^{d}\sum_{r=1}^{\infty }\hat{u}_{i}^{(s)}(\hat{u}^{(s)})^{\mathrm{T}}\phi_{r,n}\phi_{r,n}^{\mathrm{T}}\hat{u}^{(s)}\hat{u}_{j}^{(s)} = \left \{ \mathcal{P}_{\R}(\R) \right \}_{i,j=1}^n.
    \end{aligned}
\end{equation*}
The above equations means there exists a matrix $\X \in \mathbb{R}^{n \times d}$ such that $\X \X^{\mathrm{T}}=\R$ and the rows of $\X$ correspond to the projections $\hat{\mathcal{P}}_{d}K(\cdot,X_{i})$. This means the d-dimensional embedding Hilbert space is isometric with $T_{K}$, thus it has distance-preserving property, in that way we can have $\mathrm{hCov}^{(d)}(G_{1},G_{2})=\mathrm{hCov}(G_{1},G_{2})$.

\textbf{Step 3. } Finally, we want to show the sample graph covariance $\mathrm{gCov}_{n}(G_{1},G_{2})$ converges to the truncated population graph covariance. 

Let $\hat{K}_{d}(x,{x}')=\left \langle \hat{\phi}_{d}(x),\hat{\phi}_{d}({x}')  \right \rangle_{\mathcal{H}}$, then
\begin{equation*}
    \begin{aligned}
     \left | E\left [ K_d(x,{x}') \right ]- E\left [ \hat{K}_d(x,{x}') \right ] \right |  &=
     \left | \frac{1}{n(n-1)}\sum_{i=1}^{n}\sum_{j\neq i}^{n}K_{d}(x_{i},x_{j})-\frac{1}{n(n-1)}\sum_{i=1}^{n}\sum_{j\neq i}^{n}\hat{K}_{d}(x_{i},x_{j}) \right |  \\
     &= \left | \frac{1}{n(n-1)}\sum_{i=1}^{n}\sum_{j\neq i}^{n}\left [ K_{d}(x_{i},x_{j})- \hat{K}_{d}(x_{i},x_{j}) \right ]  \right | 
    \end{aligned}
\end{equation*}
For $K_{d}(x_{i},x_{j})- \hat{K}_{d}(x_{i},x_{j})$:
\begin{equation*}
    \begin{aligned}
     \left | K_{d}(x_{i},x_{j})- \hat{K}_{d}(x_{i},x_{j}) \right | &= \left | \left \langle \phi_{d}(x_{i}),\phi_{d}(x_{j}) \right \rangle_{\mathcal{H}} - \left \langle \hat{\phi}_{d}(x_{i}),\hat{\phi}_{d}(x_{j}) \right \rangle_{\mathcal{H}} \right | \\
     &= \left | \left \langle \phi_{d}(x_{i})-\hat{\phi}_{d}(x_{i}),\phi_{d}(x_{j}) \right \rangle_{\mathcal{H}} - \left \langle \hat{\phi}_{d}(x_{i}),\phi_{d}(x_{j})-\hat{\phi}_{d}(x_{j}) \right \rangle_{\mathcal{H}} \right | \\
     &\leq \left \| \phi_{d}(x_{i})-\hat{\phi}_{d}(x_{i}) \right \| \left \| \phi_{d}(x_{j}) \right \| + \left \| \hat{\phi}_{d}(x_{i}) \right \|\left \|  \phi_{d}(x_{j})-\hat{\phi}_{d}(x_{j}) \right \| \\
     & \leq C\delta _{d}^{-2}\sqrt{\frac{d \mathrm{log}n}{n}}.
    \end{aligned}
\end{equation*}
Where $C$ is a constant which can bound $\phi_{d}(x_{i})$. The second inequality is obtained by the proof process of Theorem 3.1 in \citep{tang2013universally}. Since $K_{d}(x_{i},x_{j}) \in \left [ 0,1 \right ] $ for $\forall i,j $, then $K_{d}(x_{i},x_{j})$ is uniformly integrable. Then we have:

%%%% need add more descripition, e.g. using the SLLN to show converge almost surely. 
\begin{equation*}
    \mathbb{P}(\left | E\left [ K_d(x,{x}') \right ]- E\left [ \hat{K}_d(x,{x}') \right ] \right | \geq \varepsilon )\rightarrow 0  \quad n \rightarrow \infty.
\end{equation*}
Then the proposed $\mathrm{gCov}_{n}(G_{1},G_{2})$ can approximate the d-th truncation of $\mathrm{hCov}(G_{1},G_{2})$, where random graphs are generated by $\X$ and $\Y$.

\end{proof}

\begin{proof}[Proof of Theorem 2]

When $G_{1}$ and $G_{2}$ are independent, the random variables $\left \{ X_{i} \right \}_{i=1}^n$ and $\left \{ Y_{i} \right \}_{i=1}^n$ that generate $G_{1}$ and $G_{2}$ are independent. 

Considering the Hilbert-Schmidt Independent Criterion (HISC) of random variables X and Y. The Hilbert-Schmidt Independence Criterion is given by 
\begin{equation*}
    \mathrm{HSIC}(p_{xy},\mathcal{F},\mathcal{G}):= \left \| C_{xy} \right \|_{\mathcal{H}}^2,
\end{equation*}
where $p_{xy}$ denotes the joint measure, random variable $X$ mapped to a reproducing kernel Hilbert space $\mathcal{F}$ and $Y$ mapped to $\mathcal{G}$. $C_{xy}$ denotes the cross-covariance operator. The cross-covariance operator can be calculated by:
\begin{equation*}
\begin{aligned}
    C_{xy}&=\mathrm{E}_{x,y}\left [ (f(x)-\mu_{x})\otimes (g(y)-\mu_{y}) \right ]\\
    &= \mathrm{E}_{x,y}\left [ f(x)\otimes g(y) \right ]-\mu_{x}\otimes \mu_{y} = \tilde{C}_{xy}-M_{xy}.
\end{aligned}
\end{equation*}
In the above equations, $\otimes$ denotes the tensor product in Hilbert space. $f(x)$ and $g(y)$ are feature map functions, then $\left \langle f(x) , f({x}')\right \rangle_{\mathcal{F}}=K(x,{x}')$ and $\left \langle g(y) , g({y}')\right \rangle_{\mathcal{G}}=L(y,{y}')$, where $K$ and $L$ are associated kernel functions. Theorem 4 in \citep{gretton2005measuring} shows $\left \| C_{xy} \right \|_{\mathcal{H}}=0$ if and only if $x$ and $y$ are independent. Therefore, combined with the definition of $C_{xy}$, the $\mathrm{HSIC}(p_{xy},\mathcal{F},\mathcal{G})$ can be represented in terms of kernel by the definition of the cross-covariance and the relationship between kernel function and feature map:
\begin{equation*}
    \begin{aligned}
    \mathrm{HSIC}(p_{xy},\mathcal{F},\mathcal{G})
    &= \left \langle \tilde{C}_{xy}-M_{xy} , \tilde{C}_{xy}-M_{xy}\right \rangle_{\mathcal{H}}\\
    &= \mathrm{E}\left [ \left \langle f(x)\otimes g(y), f(x)\otimes g(y)\right \rangle _{\mathcal{H}}\right ] - 2\mathrm{E}\left [ \left \langle f(x)\otimes g(y), \mu_{x}\otimes \mu_{y}\right \rangle _{\mathcal{H}}\right ] \\
   &+ \left \langle \mu_{x}\otimes \mu_{y}, \mu_{x}\otimes \mu_{y}\right \rangle _{\mathcal{H}}\\
    &=  \mathrm{E}\left [ K(x,{x}')L(y,{y}') \right ]-2\mathrm{E}\left [ K(x,{x}')L(y,{y}'') \right ]+\mathrm{E}\left [ K(x,{x}') \right ]\mathrm{E}\left [ L(y,{y}') \right ]
    \end{aligned}
\end{equation*}
Then the independent criterion is equivalent to $\mathrm{hCov}(G_{1},G_{2})=0$ if and only if $G_{1}$ and $G_{2}$ are independent. In the Theorem 1 we proved that $\mathrm{gCov}_{n}(G_{1},G_{2})$ converge to $\mathrm{gCov}(G_{1},G_{2})$ almost surely. 

Besides, when $G_{1}$ and $G_{2}$ are independent, $\mathrm{hCov}(G_{1},G_{1})=0$, so $\mathrm{hCor}(G_{1},G_{1})=0$ as well. On the other hand, $\mathrm{hCor}(G_{1},G_{1})>0$ if dependent. Then follows the convergence property of $\mathrm{gCor}_{n}(G_{1},G_{1})$, the proof was completed.

\end{proof}

\begin{proof}[Proof of Theorem 3]
First, we want to obtain the asymptotic distribution of $\mathrm{hCov}_{n}(G_{1},G_{2})$. 

Since the U-statistics can be used to estimate $\mathrm{hCov}_{n}(G_{1},G_{2})$. According to Theorem A in Chap 5.5 of \citep{serfling2009approximation}, the $\mathrm{hCov}_{n}(G_{1},G_{2})$ is asymptotic normality with mean $\mathrm{hCov}(G_{1},G_{2})$ and variance $\frac{m^2}{n}\zeta_{1}$ when $\zeta_{1} \neq 0$. The $\zeta_{c}$ wa defined as follows:
\begin{equation}\label{zeta_def}
    \zeta_{c}=E_{F}\left \{ \tilde{h}(X_{a_{1}},\cdots,X_{a_{m}}) \tilde{h}(X_{b_{1}},\cdots,X_{b_{m}}) \right \},
\end{equation}
where $\tilde{h}(X_{a_{1}},\cdots,X_{a_{m}})= h(X_{a_{1}},\cdots,X_{a_{m}}) - \mu$. $c$ represents the number of common elements in sets $\left \{ a_{1},\cdots,a_{m} \right \}$ and $\left \{ b_{1},\cdots,b_{m} \right \}$. In our estimation procedure, we have $m=4$ and $c=1$. Thus the variance $\sigma_{u}^2$ can be written as
\begin{equation*}
    \sigma_{u}^2=16E_{F}\left \{ h(X_{a_{1}},\cdots,X_{a_{4}}) h(X_{b_{1}},\cdots,X_{b_{4}})-\mathrm{hCov}^2 \right \}.
\end{equation*}
Known that there is one common element in the two sets $\left \{ a_{1},\cdots,a_{4} \right \}$ and $\left \{ b_{1},\cdots,b_{4} \right \}$, then we have
\begin{equation*}
    \begin{aligned}
    E_{F}\left \{ h(X_{a_{1}},\cdots,X_{a_{4}}) h(X_{b_{1}},\cdots,X_{b_{4}}) \right \}&=(\textrm{C}_{n}^{1})^{-1}\sum_{i=1}^{n}\left \{(\textrm{C}_{n-1}^{4-1})^{-1}\sum_{(j,q,r)\setminus  \left \{ i \right \}}\frac{1}{3!}h(i,j,q,r)  \right \}^2\\
    &=\frac{1}{n}\sum_{i=1}^{n}\left \{ (n-1)_{3}^{-1}\sum_{(j,q,r)\setminus  \left \{ i \right \}}h(i,j,q,r) \right \}^2 = R.
    \end{aligned}
\end{equation*}
where $(n)_{m}=\frac{n!}{(n-m)!}$. 

Since $\mathrm{gCov}_{n}(G_{1},G_{2})$ converges to $\mathrm{hCov}_{n}(G_{1},G_{2})$ almost surely, then $\mathrm{gCov}_{n}(G_{1},G_{2})$ also has the same asymptotic normal distribution, which complete the proof.

\end{proof}

\begin{proof}[Proof of Theorem 4]

By the property of the reproducing kernel of the Hilbert space, we can define a centered kernel at probability measure $\nu$ as follows:
\begin{equation*}
    \begin{aligned}
    \bar{K}(x,{x}')& := \left \langle K(x,\cdot)-E_{X}K(X,\cdot) ,K({x}',\cdot)-E_{X}K(X,\cdot)\right \rangle_{\mathcal{H}} \\
    &=K(x,{x}')+E_{X{X}'}K(X,{X}')-E_{X}K(x,X)-E_{X}K({x}',X).
    \end{aligned}
\end{equation*}
Since the $\bar{K}(x,{x}')$ can also be written as $\bar{K}(x,{x}')= \left \langle K(x,\cdot)-\mu_{x} ,K({x}',\cdot)-\mu_{{x}'}\right \rangle_{\mathcal{H}}$ by the Bochner integral $\mu_{x}=\int K(x,\cdot)d \nu(x)=E_{X}K(X,\cdot)$. Then we have
\begin{equation*}
    \begin{aligned}
    \mathrm{E}(\bar{K}(x,{x}')\bar{L}(y,{y}')) &=  \mathrm{E}\left ( \left \langle K(x,\cdot)-\mu_{x} ,K({x}',\cdot)-\mu_{{x}'}\right \rangle_{\mathcal{H}}\cdot \left \langle L(y,\cdot)-\mu_{y} ,L({y}',\cdot)-\mu_{{y}'}\right \rangle_{\mathcal{H}} \right ) \\
    &= \mathrm{E}\left ( \left \langle (K(x,\cdot)-\mu_{x})\otimes (L(y,\cdot)-\mu_{y}),(K({x}',\cdot)-\mu_{{x}'})\otimes (L({y}',\cdot)-\mu_{{y}'})\right \rangle _{\mathcal{H}}\right ) \\
    &=  \mathrm{E}\left [ \left \langle K(x,\cdot)\otimes L(y,\cdot), K({x}',\cdot)\otimes L({y}',\cdot)\right \rangle _{\mathcal{H}}\right ] - 2\mathrm{E}\left [ \left \langle K(x,\cdot)\otimes L(y,\cdot), \mu_{x}\otimes \mu_{y}\right \rangle _{\mathcal{H}}\right ] \\
    & + \left \langle \mu_{x}\otimes \mu_{y}, \mu_{x}\otimes \mu_{y}\right \rangle _{\mathcal{H}}\\
    &= \mathrm{E}\left [ K(x,{x}')L(y,{y}') \right ]-2\mathrm{E}\left [ K(x,{x}')L(y,{y}'') \right ]+\mathrm{E}\left [ K(x,{x}') \right ]\mathrm{E}\left [ L(y,{y}') \right ] \\
    & = \mathrm{hCov}(G_{1},G_{2}).
    \end{aligned}
\end{equation*}
Let $(X^{i}, Y^{i})$ be independent for $i \in \left \{ 1,2,\cdots,6 \right \}$. In the \citep{lyons2013distance} introduce the "core" defined as follows:
\begin{equation*}
    h((X^{1},Y^{1}),(X^{2},Y^{2}),\cdots,(X^{6},Y^{6})):= f(X^{1},X^{2},X^{3},X^{4})f(Y^{1},Y^{2},Y^{5},Y^{6}),
\end{equation*}
where for $x_{i} \in \mathcal{X}$:
\begin{equation*}
\begin{aligned}
  f(x_{1},x_{2},x_{3},x_{4})& := d(x_{1},x_{2})-d(x_{1},x_{3})-d(x_{2},x_{4})+d(x_{3},x_{4})\\
  &= -2\left \{ K(x_{1},x_{2})-K(x_{1},x_{3})-K(x_{2},x_{4})+K(x_{3},x_{4}) \right \}.
\end{aligned}
\end{equation*}
The second equation is obtained from the relation in \citep{sejdinovic2013equivalence} that
\begin{equation*}
    d(x,{x}')=K(x,x)+K({x}',{x}')-2K(x,{x}').
\end{equation*}
Then, using Fubini's theorem, we can obtain that:
\begin{equation*}
    \begin{aligned}
    \mathrm{E} \left \{ h((X^{1},Y^{1}),\cdots,(X^{6},Y^{6})) \right \} &=E\left \{ f(X^{1},X^{2},X^{3},X^{4})f(Y^{1},Y^{2},Y^{5},Y^{6}) \right \}\\
    &= 4\mathrm{E}(\bar{K}(X,{X}')\bar{L}(Y,{Y}')) = 4\mathrm{hCov}(G_{1},G_{2}).
    \end{aligned}
\end{equation*}
Under the null hypothesis, $\mathrm{hCov}(G_{1},G_{2})=0$ since $G_{1}$ and $G_{2}$ are independent, then the kernel $h$ is degenerate of order 1, that is, $\zeta_{1}=0$, where $\zeta_{c}$ is defined in \ref{zeta_def}. Then we need to show that $\zeta_{2} \neq 0$. Define the average of kernel $h$ as follows:
\begin{equation*}
    \bar{h}((X^{1},Y^{1}),\cdots,(X^{6},Y^{6})):= \frac{1}{6!}\sum_{\sigma \in A}h((X^{\sigma(1)},Y^{\sigma(1)}),\cdots,(X^{\sigma(6)},Y^{\sigma(6)})),
\end{equation*}
where $A$ denotes the set of all permutations of $\left \{ 1,2,\cdots,6 \right \}$. Then we have 
\begin{equation*}
    \begin{aligned}
    \bar{h}_{2}((x,y),({x}',{y}'))&=\frac{1}{6!}\cdot 2\cdot 4!\cdot h((X^{1},Y^{1}),\cdots,(X^{6},Y^{6}))\\
    &= \frac{4}{15}\mathrm{E}(\bar{K}(X,{X}')\bar{L}(Y,{Y}')).
    \end{aligned}
\end{equation*}
Therefore the $h((X^{1}, Y^{1}),\cdots,(X^{6}, Y^{6}))$ has finite second moments. We can use the theory of degenerate U-statistic in Theorem 5.5.2 in \citep{serfling2009approximation} that the $\mathrm{gCov}_{n}(G_{1},G_{2})$ is approximate a chi-square distribution under $H_{0}$.

\end{proof}

\begin{proof}[Proof of Theorem 5]

Define the maps $\tau:\left \{ 1,2,\cdots,n \right \} \to \left \{ 1,2,\cdots,n \right \}$ and $\tau_{i}$ will be a permutations of $\left \{ 1,2,\cdots,n \right \}$ for $i=1,2,\cdots,B$, where $B$ denotes the permutation time. To simplify notation, we denote $\mathrm{gCor}^{*}_{n}(G_{1},G_{2})$ as $\hat{g}(\tau_{i}G)$ and $\mathrm{gCor}^{0}_{n}(G_{1},G_{2})$ as $\hat{g}(G)$. Then we define $R$ as:
\begin{equation*}
R = \sum_{i=1}^{B}\mathbbm{1}\left \{ \hat{g}(\tau_{i}G) \ge  \hat{g}(G) \right \} ,
\end{equation*}
and reject if $\hat{p}:=\frac{1+R}{1+B}\le \alpha$, where $\alpha >0$ denotes the Type \uppercase\expandafter{\romannumeral1} erroe level. The finite-sample permutation test has p-value
\begin{equation*}
\hat{p} = \frac{1}{B+1} + \frac{1}{B+1}\sum_{i=1}^{B}\mathbbm{1}\left \{ \hat{g}(\tau_{i}G) \ge  \hat{g}(G) \right \}.
\end{equation*}
When $B$ is large enough, $\hat{p} \le \alpha$ can be approximated as $\mathbb{P}\left (\hat{g}(\tau_{i}G) \ge  \hat{g}(G) \right ) \le \alpha$. Let $d_{n}$ be the number of permutations of $\left \{ 1,2,\cdots,n \right \} $ that are derangements. Then we have
\begin{equation*}
\mathbb{P}\left (\hat{g}(\tau_{i}G) \ge  \hat{g}(G) \right ) = \mathbb{P}\left (\hat{g}(\tau_{i}G) \ge  \hat{g}(G) \mid \tau_{i}=d_{n} \right ) \mathbb{P}(\tau_{i}=d_{n}) 
\end{equation*}
Known that as $n\to \infty $, we have $\mathbb{P}(\tau_{i}=d_{n}) \to e^{-1}/d_{n}!$ and $\hat{g}(G) \to \epsilon >0$ under dependence. Thus it suffices to show that for any $\epsilon >0$,
\begin{equation*}
\lim_{n \to \infty} e^{-1}\sum_{d_{n}=0}^{n}\mathbb{P}\left (\hat{g}(\tau_{i}G) \ge  \epsilon  \mid \tau_{i}=d_{n} \right )/d_{n}!\to 0 .
\end{equation*}
Given the permutation size $d_{n}$, $G_{1}$ and $G_{2}$ are asymptotically independent, implies $\mathbb{P}\left (\hat{g}(\tau_{i}G) \ge  \epsilon  \mid \tau_{i}=d_{n} \right ) \to 0$ as $n \to \infty$. Then for any $\alpha >0$, there exists $N_{2} > N_{1}$ that for any $d_{n} \le N_{1}$ and $n>N_{2}$, $\mathbb{P}\left (\hat{g}(\tau_{i}G) \ge  \epsilon  \mid \tau_{i}=d_{n} \right ) < \alpha/2$. Besides, since $\lim_{n \to \infty } \sum_{d_{n}=0}^{n}\frac{1}{d_{n}!}\to e $, we can deduce that $\lim_{n \to \infty } e^{-1}\sum_{d_{n}=N_{1}+1}^{n}\frac{1}{d_{n}!} \le \alpha/2$. Then for all $n>N_{2}$, we have
\begin{equation*}
\begin{aligned}
& \quad e^{-1}\sum_{d_{n}=0}^{n}\mathbb{P}\left (\hat{g}(\tau_{i}G) \ge  \epsilon  \mid \tau_{i}=d_{n} \right )/d_{n}! \\
&= e^{-1}\sum_{d_{n}=0}^{N_{1}}\mathbb{P}\left (\hat{g}(\tau_{i}G) \ge  \epsilon  \mid \tau_{i}=d_{n} \right )/d_{n}! + e^{-1}\sum_{d_{n}=N_{1}+1}^{n}\mathbb{P}\left (\hat{g}(\tau_{i}G) \ge  \epsilon  \mid \tau_{i}=d_{n} \right )/d_{n}! \\
& \le e^{-1}\sum_{d_{n}=0}^{N_{1}}\alpha/2d_{n}! + e^{-1}\sum_{d_{n}=N_{1}+1}^{n}1/d_{n}! \\
& \le \alpha.
\end{aligned}
\end{equation*}
Therefore, in the case of $G_{1}$ and $G_{2}$ dependence, for any Type \uppercase\expandafter{\romannumeral1} error level $\alpha >0$, the p-value obtained by the permutation test will be less than $\alpha$ as the sample size $n$ increases. Therefore, the permutation test is consistent for the $\mathrm{gCor}_{n}$ of finite samples. When $G_{1}$ and $G_{2}$ are independent, $\mathrm{gCor}_{n} \to 0$, so $\mathbb{P}\left (\hat{g}(\tau_{i}G) \ge  \hat{g}(G) \right )$ is uniformly distributed between 0 and 1, and the permutation test is also valid.
\end{proof}

\begin{proof}[Proof of Theorem 6]
Using the definition of $\mathrm{MMD}(\mathcal{F},F_{XY},F_{X}F_{Y})$, we can have the following equations:
\begin{equation*}
    \begin{aligned}
    \mathrm{MMD}^2(\mathcal{F},F_{XY},F_{X}F_{Y}) &= \left \| \mu_{XY}-\mu_{X}\otimes \mu_{Y} \right \|_{\mathcal{H}}^2 \\
    &= \left \langle \mu_{XY}-\mu_{X}\otimes \mu_{Y},\mu_{XY}-\mu_{X}\otimes \mu_{Y}  \right \rangle_{\mathcal{H}} \\
    &= \left \langle \mu_{XY},\mu_{XY} \right \rangle_{\mathcal{H}}+\left \langle \mu_{X}\otimes \mu_{Y},\mu_{X}\otimes \mu_{Y} \right \rangle_{\mathcal{H}}-2 \left \langle \mu_{XY},\mu_{X}\otimes \mu_{Y} \right \rangle_{\mathcal{H}} \\
    &= \mathrm{E}\left [ K(X,{X}')L(Y,{Y}') \right ]+\mathrm{E}\left [ K(X,{X}') \right ]\mathrm{E}\left [ L(Y,{Y}') \right ] \\
    &-2\mathrm{E}\left [ K(X,{X}')L(Y,{Y}'') \right ] \\
    &= \mathrm{HSIC}(X,Y) = \mathrm{hCov}(G_{1},G_{2}).
    \end{aligned}
\end{equation*}
Thus $\mathrm{MMD}(\mathcal{F},F_{XY},F_{X}F_{Y})=0$ if and only if $G_{1}$ and $G_{2}$ are independent.
\end{proof}

\begin{proof}[Proof of Theorem 7]

The unbiased estimator of sample distance covariance can be calculated by the following formula:
\begin{equation*}
\mathrm{dCov}_{n}(X,Y)=\left ( \tilde{\mathbf{B}}\cdot \tilde{\mathbf{C}}  \right ) =\frac{1}{n(n-3)} \sum_{i\ne j}\tilde{b}_{ij} \tilde{c}_{ij},
\end{equation*}
where $\tilde{\mathbf{B}}$ and $\tilde{\mathbf{C}}$ are $\mathcal{U}$-centered matrices of the distance matrices $\mathbf{B}$ and $\mathbf{C}$. Considering the notation $b_{k\cdot}=\sum_{l=1}^n b_{kl}$, $b_{\cdot l}=\sum_{k=1}^n b_{kl}$ and $b_{\cdot \cdot}=\sum_{k,l=1}^n b_{kl}$. The Proposition 1 in \citep{szekely2014partial} shows that the $\left ( \tilde{\mathbf{B}}\cdot \tilde{\mathbf{C}}  \right )$ can be rewritten as:
\begin{equation*}
n(n-3)\left ( \tilde{\mathbf{B}}\cdot \tilde{\mathbf{C}}  \right ) = T_{1}-\frac{T_{2}}{(n-1)(n-2)^2}-\frac{2T_{3}}{n-2},
\end{equation*} 
where 
\begin{equation*}
T_{1}=\sum_{k \ne l}b_{kl}c_{kl}, \quad T_{2}=b_{\cdot \cdot}c_{\cdot \cdot}, \quad T_{3}=\sum_{k}b_{k \cdot}c_{k \cdot}.
\end{equation*}
Let 
\begin{equation*}
\begin{aligned}
n(n-3)\mathrm{dCov}_{n}(\tilde{\X},\tilde{\Y})=\tilde{T}_{1}-\frac{\tilde{T}_{2}}{(n-1)(n-2)^2}-\frac{2\tilde{T}_{3}}{n-2},  \\
n(n-3)\mathrm{dCov}_{n}(\breve{\X},\breve{\Y})=\breve{T}_{1}-\frac{\breve{T}_{2}}{(n-1)(n-2)^2}-\frac{2\breve{T}_{3}}{n-2}.
\end{aligned}
\end{equation*}
The sepctral decomposition of $\mathbf{\cL}(\A_{1})$ is $\breve{\X}$ and $\mathbf{\cL}(\A_{2})$ is $\breve{\Y}$, and the spectral decomposition of $\mathbf{\cL}(\K_{1})$ is $\tilde{\X}$ and $\mathbf{\cL}(\K_{2})$ is $\tilde{\Y}$. let $\breve{X}_{i}$ the i-th row of $\breve{\X}$, $\breve{Y}_{i}$ the i-th row of $\breve{\Y}$, $\tilde{X}_{i}$ the i-th row of $\tilde{\X}$ and $\tilde{Y}_{i}$ the i-th row of $\tilde{\Y}$. Then we have
\begin{equation*}
\begin{aligned}
\tilde{T}_{1} - \breve{T}_{1} & = \sum_{k \ne l}\left ( \left \| \tilde{X}_{k}-\tilde{X}_{l} \right \| \left \| \tilde{Y}_{k}-\tilde{Y}_{l} \right \| - \left \| \breve{X}_{k}-\breve{X}_{l} \right \| \left \| \breve{Y}_{k}-\breve{Y}_{l} \right \|  \right )  \\
& = \sum_{k \ne l}( \left \| \tilde{X}_{k}-\tilde{X}_{l} \right \| \left \| \tilde{Y}_{k}-\tilde{Y}_{l} \right \| -\left \| \tilde{X}_{k}-\tilde{X}_{l} \right \| \left \| \breve{Y}_{k}-\breve{Y}_{l} \right \| \\
& + \left \| \tilde{X}_{k}-\tilde{X}_{l} \right \| \left \| \breve{Y}_{k}-\breve{Y}_{l} \right \|- \left \| \breve{X}_{k}-\breve{X}_{l} \right \| \left \| \breve{Y}_{k}-\breve{Y}_{l} \right \| ) \\ 
& = l_{1}+l_{2}
\end{aligned}
\end{equation*}
where 
\begin{equation*}
\begin{aligned}
l_{1} &= \sum_{k \ne l}\left ( \left \| \tilde{X}_{k}-\tilde{X}_{l} \right \| \left \| \tilde{Y}_{k}-\tilde{Y}_{l} \right \| -\left \| \tilde{X}_{k}-\tilde{X}_{l} \right \| \left \| \breve{Y}_{k}-\breve{Y}_{l} \right \|  \right )  \\
l_{2} &=\sum_{k \ne l}\left ( \left \| \tilde{X}_{k}-\tilde{X}_{l} \right \| \left \| \breve{Y}_{k}-\breve{Y}_{l} \right \|- \left \| \breve{X}_{k}-\breve{X}_{l} \right \| \left \| \breve{Y}_{k}-\breve{Y}_{l} \right \|  \right ). 
\end{aligned}
\end{equation*}
Then we have
\begin{equation*}
\begin{aligned}
l_{1}&= \sum_{k \ne l}\left ( \left \| \tilde{X}_{k}-\tilde{X}_{l} \right \| \left \| \tilde{Y}_{k}-\tilde{Y}_{l} \right \| -\left \| \tilde{X}_{k}-\tilde{X}_{l} \right \| \left \| \breve{Y}_{k}-\breve{Y}_{l} \right \|  \right ) \\
&= \sum_{k \ne l} \left \| \tilde{X}_{k}-\tilde{X}_{l}  \right \|  \left (  \left \| \tilde{Y}_{k}-\tilde{Y}_{l} \right \| - \left \| \breve{Y}_{k}-\breve{Y}_{l} \right \|  \right ) \\
& = \sum_{k \ne l} \left \| \tilde{X}_{k}-\tilde{X}_{l}  \right \|  \left (  \left \| \tilde{Y}_{k}-\breve{Y}_{k} + \breve{Y}_{k}-\tilde{Y}_{l} \right \| - \left \| \breve{Y}_{k}-\breve{Y}_{l} \right \|  \right ) \\
& \le \sum_{k \ne l} \left \| \tilde{X}_{k}-\tilde{X}_{l}  \right \|  \left ( \left \| \tilde{Y}_{k}-\breve{Y}_{k} \right \| + \left \|  \breve{Y}_{k} - \breve{Y}_{l} + \breve{Y}_{l}-\tilde{Y}_{l} \right \| - \left \| \breve{Y}_{k}-\breve{Y}_{l} \right \|  \right ) \\
& \le \sum_{k \ne l} \left \| \tilde{X}_{k}-\tilde{X}_{l}  \right \|  \left ( \left \| \tilde{Y}_{k}-\breve{Y}_{k} \right \| + \left \|  \breve{Y}_{l}-\tilde{Y}_{l} \right \|   \right ).
\end{aligned}
\end{equation*}
Similarly, we have
\begin{equation*}
l_{2} \le \sum_{k \ne l} \left \| \breve{Y}_{k}-\breve{Y}_{l}  \right \|  \left ( \left \| \tilde{X}_{k}-\breve{X}_{k} \right \| + \left \|  \breve{X}_{l}-\tilde{X}_{l} \right \|   \right ).
\end{equation*}
Then using the results in Lemma 2 we can have the following inequality:
\begin{equation*}
\frac{1}{n(n-3)}(\tilde{T}_{1} - \breve{T}_{1} ) \le \frac{2}{n(n-3)}2d(n-1)\cdot168\delta_{d}^{-2}\sqrt{\frac{d\log{(4n/\eta)}}{\log{n}}} \longrightarrow 0
\end{equation*}
as $n \rightarrow \infty$. The rest of $T_{2}$ and $T_{3}$ can also be proved similarly.

\end{proof}

\begin{proof}[Proof of Theorem 8]

For the semi-sparse random graph, we denote the concentration property as 
\begin{equation*}
  \left \|  \A - E\A \right \| \le C
\end{equation*}
Since $E \A = \rho_{n} \bP$, then we have
\begin{equation*}
  \left \|  \tilde{\A} - \bP \right \| \le \rho_{n}^{-1} C
\end{equation*}
where $\tilde{\A} = \rho_{n}^{-1} \A$. The eigendecomposition of $\bP$ is $\U_{\mathrm{P}}\bS_{\mathrm{P}}\U_{\mathrm{P}}^{\mathrm{T}}$, where $\U_{\mathrm{P}}$ composed by the eigenvectors corresponding to the $d$ largest eigenvalues. Let $\mathcal{P}_{\tilde{\A}}=\U_{\mathrm{\tilde{A}}}\U_{\mathrm{\tilde{A}}}^{\mathrm{T}}$ and $\mathcal{P}_{\bP}=\U_{\mathrm{P}}\U_{\mathrm{P}}^{\mathrm{T}}$ and $\delta_{d}=\lambda_{d}(\mathcal{K})-\lambda_{d+1}(\mathcal{K})$. The Theorem B.2 in \citep{tang2013universally} shows that with probability at least $1-\eta$, we have
\begin{equation*}
    \frac{\lambda_{d}(\bP)}{n} - \frac{\lambda_{d+1}(\bP)}{n} \ge \delta_{d}-4\sqrt{2}\sqrt{\frac{\log{(2/\eta)} }{n} }.
\end{equation*}
Meanwhile, define the $S_{1}$ and $S_{2}$ as follows:
\begin{equation*}
    \begin{aligned}
    S_{1}&=\left \{ \lambda:\lambda \ge n \lambda_{d}(\bP)-\rho_{n}^{-1}C\right \}, \\
    S_{2}&=\left \{ \lambda:\lambda <  n \lambda_{d+1}(\bP)+\rho_{n}^{-1}C\right \}.
    \end{aligned}
\end{equation*}
Then
\begin{equation*}
    \mathrm{dist}(S_{1},S_{2}) \ge n\delta_{d}-4\sqrt{2}\sqrt{n\log{(2/\eta)} } -2\rho_{n}^{-1}C.
\end{equation*}
Assume $S_{1}$ and $S_{2}$ are disjoint, then we have $\mathrm{dist}(S_{1},S_{2})>0$. According to the $\sin \Theta $ theorem in \citep{davis1970rotation} we have:
\begin{equation*}
   \begin{aligned}
   \left \| \mathcal{P}_{\tilde{\A}}(S_{1})-\mathcal{P}_{\bP}(S_{1}) \right \| & \le \frac{\left \| \tilde{\A}-\bP \right \|}{\mathrm{dist}(S_{1},S_{2})}\\
   & \le \frac{\rho_{n}^{-1}C}{n\delta_{d}-4\sqrt{2}\sqrt{n\log{(2/\eta)} } -2\rho_{n}^{-1}C} \\
   & \le \frac{2\rho_{n}^{-1}C}{n\delta_{d}}
   \end{aligned}
\end{equation*}
Since $4\sqrt{2} \sqrt{n\log{(2/\eta)}} + 2\rho_{n}^{-1}C \le n \delta_{d}/2$. Then we have
\begin{equation*}
    \begin{aligned}
    \left \| \mathcal{P}_{\tilde{\A}}(\tilde{\A})-\mathcal{P}_{\bP}(\bP) \right \| & \le \left \| \mathcal{P}_{\tilde{\A}}(\tilde{\A}-\bP)  \right \| +\left \| (\mathcal{P}_{\tilde{\A}}-\mathcal{P}_{\bP})\bP \right \|  \\
    & \le \rho_{n}^{-1} C + \frac{2\rho_{n}^{-1}C}{\delta_{d}} \\
    & \le \delta_{d}^{-1} \rho_{n}^{-1} C + 2\delta_{d}^{-1} \rho_{n}^{-1}C \\
    & = 3 \delta_{d}^{-1}\rho_{n}^{-1} C.
    \end{aligned}
\end{equation*}
Having $\mathcal{P}_{\tilde{\A}}=\U_{\mathrm{\tilde{A}}}\U_{\mathrm{\tilde{A}}}^{\mathrm{T}}=\rho_{n}^{-1}\U_{\mathrm{A}}\U_{\mathrm{A}}^{\mathrm{T}}=\rho_{n}^{-1}\mathcal{P}_{\A}$, for some orthogonal matrix $\mathbf{W} \in \mathbb{R}^{d \times d}  $, we can obtain the following inequalities:
\begin{equation*}
\begin{aligned}
    \left \| \rho_{n}^{-1/2}\U_{\mathrm{A}}\bS_{\mathrm{A}}^{1/2} \W -\U_{\mathrm{P}}\bS_{\mathrm{P}}^{1/2} \right \| & \le \left \| \mathcal{P}_{\tilde{\A}}(\tilde{\A})-\mathcal{P}_{\bP}(\bP) \right \|\frac{\sqrt{d\mathcal{P}_{\tilde{\A}}(\tilde{\A})}+\sqrt{d \mathcal{P}_{\bP}(\bP)}}{\lambda_{d}(\bP)} \\
    & \le 3 \delta_{d}^{-1}\rho_{n}^{-1} C \frac{2\sqrt{dn} }{\lambda_{d}(\bP)} \le 3 \delta_{d}^{-1}\rho_{n}^{-1} C \frac{4\sqrt{dn} }{\lambda_{d}(\mathcal{K})} \le 12 \delta_{d}^{-2}\rho_{n}^{-1} C \sqrt{\frac{d}{n}}.
\end{aligned}
\end{equation*}
Let $\hat{\phi}_{d}(X_{i})$ denotes the i-th column of $\rho_{n}^{-1/2}\U_{\mathrm{A}}\bS_{\mathrm{A}}^{1/2}\W$ and $\phi_{d}(X_{i})$ denotes the i-th column of $\U_{\mathrm{P}}\bS_{\mathrm{P}}^{1/2}$. Then we have
\begin{equation*}
    \begin{aligned}
    E\left [ \left \| \hat{\phi}_{d}(X_{i}) -\phi_{d}(X_{i}) \right \|  \right ] & \le \sqrt{E\left [ \left \| \hat{\phi}_{d}(X_{i}) -\phi_{d}(X_{i}) \right \| ^2 \right ] }  \\
    & \le \sqrt{\frac{1}{n} E\left [ \left \| \rho_{n}^{-1/2}\U_{\mathrm{A}}\bS_{\mathrm{A}}^{1/2}\W -\U_{\mathrm{P}}\bS_{\mathrm{P}}^{1/2} \right \|_{F} ^2 \right ] } \\
    & \le \frac{1}{\sqrt{n} }\sqrt{(1-\frac{2}{n^2} )\left ( 12 \delta_{d}^{-2}\rho_{n}^{-1} C \sqrt{\frac{d}{n}} \right )^2 +\frac{2}{n^2}2n }  \\ 
    & \le 12 \delta_{d}^{-2}\rho_{n}^{-1} C\frac{\sqrt{2d}}{n} .
    \end{aligned}
\end{equation*}
Then, by combining the proof process of Theorem 1 and the two conditions, we can derive the following:
\begin{gather*}
   \mathrm{gCov}_{n}(G_{1},G_{2}) \overset{n\rightarrow \infty }{\longrightarrow}  \mathrm{gCov}(G_{1},G_{2}).
\end{gather*}

\end{proof}


\begin{proof}[Proof of Lemma 2]

We can rewrite the probability link matrix in the following form for the inhomogeneous random graph:
\begin{equation*}
    \bP=\U_{\mathrm{P}}\bS_{\mathrm{P}}\U_{\mathrm{P}}^{\mathrm{T}}=\U_{\mathrm{P}}\bS_{\mathrm{P}}^{1/2}(\U_{\mathrm{P}}\bS_{\mathrm{P}}^{1/2})^{\mathrm{T}}=\X \X^{\mathrm{T}}.
\end{equation*}
We can rewrite the probability link for the inhomogeneous random graph in the following form. Then the probability link matrix can be expressed as a random dot product graph model under the kernel assumption. The central limit theorem of the Laplacian transform of the random dot product graph model in \citep{tang2018limit} can be directly generalized to a more general model, and we can have $E(\mathbf{\cL}(\A))=\mathbf{\cL}(\bP)$. 

Then, using the concentration inequality in \citep{oliveira2009concentration}, for the normalized Laplacian matrices $\mathbf{\cL}(\A)$ and $\mathbf{\cL}(\bP)$, with probability at least $1-\eta$, we have
\begin{equation*}
    \left \| \mathbf{\cL}(\A)-\mathbf{\cL}(\bP) \right \| \le 14\sqrt{\frac{\log{(4n/\eta)}}{d_{0}} } \le 14\sqrt{\frac{\log{(4n/\eta)}}{\log{n}}},
\end{equation*}
where $d_{0}$ is the minimum vertex degree and we suppose $d_{0} \geq C \log{n}$ for some constant $C$.

Similar to $\mathbf{\cL}(\A)$, the eigendecomposition of $\mathbf{\cL}(\bP)$ is $\tilde{\U}_{\mathrm{P}}\tilde{\bS}_{\mathrm{P}}\tilde{\U}_{\mathrm{P}}^{\mathrm{T}}$, where $\tilde{\U}_{\mathrm{P}}$ composed by the eigenvectors corresponding to the $d$ largest eigenvalues. Let $\tilde{\mathcal{P}}_{\A}=\tilde{\U}_{\mathrm{A}}\tilde{\U}_{\mathrm{A}}^{\mathrm{T}}$ and $\tilde{\mathcal{P}}_{\bP}=\tilde{\U}_{\mathrm{P}}\tilde{\U}_{\mathrm{P}}^{\mathrm{T}}$ and $\delta_{d}=\lambda_{d}(\mathcal{K})-\lambda_{d+1}(\mathcal{K})$. Like the proof of theorem 8 we have, with probability at least $1-\eta$, 
\begin{equation*}
    \frac{\lambda_{d}(\bP)}{n} - \frac{\lambda_{d+1}(\bP)}{n} \ge \delta_{d}-4\sqrt{2}\sqrt{\frac{\log{(2/\eta)} }{n} }.
\end{equation*}
Meanwhile, define the $S_{1}$ and $S_{2}$ as follows:
\begin{equation*}
    \begin{aligned}
    S_{1}&=\left \{ \lambda:\lambda \ge n \lambda_{d}(\bP)-14\sqrt{\frac{\log{(4n/\eta)}}{\log{n}}}\right \}, \\
    S_{2}&=\left \{ \lambda:\lambda <  n \lambda_{d+1}(\bP)+14\sqrt{\frac{\log{(4n/\eta)}}{\log{n}}}\right \}.
    \end{aligned}
\end{equation*}
Then
\begin{equation*}
    \begin{aligned}
    \mathrm{dist}(S_{1},S_{2}) & \ge n\delta_{d}-4\sqrt{2}\sqrt{n\log{(2/\eta)} } -28\sqrt{\frac{\log{(4n/\eta)}}{\log{n}}} \\
    & \ge n\delta_{d}-4(\sqrt{2}+7)\sqrt{\frac{n\log{(n/\eta)}}{\log{n}}}.
    \end{aligned}
\end{equation*}
Then we have:
\begin{equation*}
   \begin{aligned}
   \left \| \tilde{\mathcal{P}}_{\A}(S_{1})-\tilde{\mathcal{P}}_{\bP}(S_{1}) \right \| & \le \frac{\left \| \mathbf{\cL}(\A)-\mathbf{\cL}(\bP) \right \|}{\mathrm{dist}(S_{1},S_{2})}\\
   & \le \frac{14\sqrt{\frac{\log{(4n/\eta)}}{\log{n}}}}{n\delta_{d}-4(\sqrt{2}+7)\sqrt{\frac{n\log{(n/\eta)}}{\log{n}}}} \le 28\sqrt{\frac{\log{(4n/\eta)}}{n \delta_{d}^2\log{n}}},
   \end{aligned}
\end{equation*}
provided that $4(\sqrt{2}+7)\sqrt{\frac{n\log{(n/\eta)}}{\log{n}}}\le n\delta_{d}/2$. Then with probability at least $1-2\eta$,
\begin{equation*}
    \left \| \tilde{\mathcal{P}}_{\A}-\tilde{\mathcal{P}}_{\bP} \right \|\le 28\sqrt{\frac{\log{(4n/\eta)}}{n \delta_{d}^2\log{n}}}.
\end{equation*}

Since $\left \| \mathbf{\cL}(\A) \right \| \le n$ and $\left \| \mathbf{\cL}(\bP) \right \| \le n$ for the normalized Laplacian matrix, we can obtain the following inequalities:
\begin{equation*}
    \begin{aligned}
    \left \| \tilde{\mathcal{P}}_{\A}\mathbf{\cL}(\A)-\tilde{\mathcal{P}}_{\bP}\mathbf{\cL}(\bP) \right \| & \le \left \| \tilde{\mathcal{P}}_{\A}(\mathbf{\cL}(\A)-\mathbf{\cL}(\bP))  \right \| +\left \| (\tilde{\mathcal{P}}_{\A}-\tilde{\mathcal{P}}_{\bP}) \mathbf{\cL}(\bP) \right \|  \\
    & \le 14\sqrt{\frac{\log{(4n/\eta)}}{\log{n}}} + 28\delta_{d}^{-1}\sqrt{\frac{n\log{(4n/\eta)}}{\log{n}}} \le 42\delta_{d}^{-1}\sqrt{\frac{n\log{(4n/\eta)}}{\log{n}}}.
    \end{aligned}
\end{equation*}
Then using the Lemma A.1 in \citep{tang2013universally}, there exists an orthogonal matrix $\W \in \mathbb{R}^{d \times d}$ such that
\begin{equation*}
\begin{aligned}
    \left \| \tilde{\U}_{\mathrm{A}}\tilde{\bS}_{\mathrm{A}}^{1/2} \W -\tilde{\U}_{\mathrm{P}}\tilde{\bS}_{\mathrm{P}}^{1/2} \right \| & \le \left \| \tilde{\mathcal{P}}_{\A}\mathbf{\cL}(\A)-\tilde{\mathcal{P}}_{\bP}\mathbf{\cL}(\bP) \right \|\frac{\sqrt{d\tilde{\mathcal{P}}_{\A}\mathbf{\cL}(\A)}+\sqrt{d\tilde{\mathcal{P}}_{\A}\mathbf{\cL}(\bP)}}{\lambda_{d}(\bP)} \\
    & \le 42\delta_{d}^{-1}\sqrt{\frac{n\log{(4n/\eta)}}{\log{n}}}\frac{2\sqrt{nd}}{\lambda_{d}(\bP)} = 84\delta_{d}^{-1}n\lambda_{d}(\bP)^{-1}\sqrt{\frac{d\log{(4n/\eta)}}{\log{n}}}.
\end{aligned}
\end{equation*}
Noted that $\lambda_{d}(\bP) \ge n\lambda_{d}(\mathcal{K})/2$, then we have
\begin{equation*}
    \left \| \tilde{\U}_{\mathrm{A}}\tilde{\bS}_{\mathrm{A}}^{1/2} \W -\tilde{\U}_{\mathrm{P}}\tilde{\bS}_{\mathrm{P}}^{1/2} \right \|_{F} \le 168\delta_{d}^{-1}\lambda_{d}(\mathcal{K})^{-1}\sqrt{\frac{d\log{(4n/\eta)}}{\log{n}}} \le 168\delta_{d}^{-2}\sqrt{\frac{d\log{(4n/\eta)}}{\log{n}}}
\end{equation*}
with probability at least $1-2\eta$. Let $\tilde{\phi}_{d}(X_{i})$ denotes the i-th column of $\tilde{\U}_{\mathrm{A}}\tilde{\bS}_{\mathrm{A}}^{1/2}\W$ and $\breve{\phi}_{d}(X_{i})$ denotes the i-th column of $\tilde{\U}_{\mathrm{P}}\tilde{\bS}_{\mathrm{P}}^{1/2}$. Besides, let $\eta=n^{-2}$ we have
\begin{equation*}
    \begin{aligned}
    E\left [ \left \| \tilde{\phi}_{d}(X_{i}) -\breve{\phi}_{d}(X_{i}) \right \|  \right ] & \le \sqrt{E\left [ \left \| \tilde{\phi}_{d}(X_{i}) -\breve{\phi}_{d}(X_{i}) \right \| ^2 \right ] }  \\
    & \le \sqrt{\frac{1}{n} E\left [ \left \| \tilde{\U}_{\mathrm{A}}\tilde{\bS}_{\mathrm{A}}^{1/2} \W -\tilde{\U}_{\mathrm{P}}\tilde{\bS}_{\mathrm{P}}^{1/2} \right \|_{F} ^2 \right ] } \\
    & \le \frac{1}{\sqrt{n} }\sqrt{(1-\frac{2}{n^2} )\left ( 168\delta_{d}^{-2}\sqrt{\frac{d\log{(4n^3)}}{\log{n}}} \right )^2 +\frac{2}{n^2}2n }  \\ 
    & \le 168\delta_{d}^{-2}\sqrt{\frac{2d\log{(4n^3)}}{n\log{n}}} .
    \end{aligned}
\end{equation*}
Then, by Markov's inequality, for any $\varepsilon >0$ we have
\begin{equation*}
    \mathbb{P}\left ( \left \| \tilde{\phi}_{d}(X_{i}) -\breve{\phi}_{d}(X_{i})  \right \|> \varepsilon  \right )\leq 168\delta_{d}^{-2}\varepsilon ^{-1}\sqrt{\frac{2d\log{(4n^3)}}{n\log{n}}}.
\end{equation*}

\end{proof}

\section{Simulation functions and Sub-networks}\label{simu_and_describ}

\subsection{Simulation dependence functions}

This section presents our nine simulation settings, including four linear relationships, four nonlinear relationships and one independent relationship. Our settings are mainly from the previous work of \citep{shen2019distance}, and we have modified some initial distributions and noise values. In this paper, we only consider the simulation results of univariate, that is, $X, Y \in \mathbb{R}$. Besides, $\mathrm{Beta}(a,b)$ denotes the beta distribution with parameters $a$ and $b$,  $U(a,b)$ denotes the uniform distribution on the interval $(a,b)$, $N(a,b)$ denotes the normal distribution with mean $a$ and variance $b$ and $B(p)$ denotes the Bernoulli distribution with probability $p$. $\kappa$ is used to control the existence of noise. When there is noise, $\kappa$ equals 1, and 0 otherwise. $c$ represents the variance of the normal distribution, which is used to control the noise level. For example, if the noise level is 0.1, $c$ equals 0.1.
\begin{enumerate}

\item Linear $(X, Y) \in \mathbb{R} \times \mathbb{R}$:
\begin{equation*}
\begin{aligned}
X & \sim \mathrm{Beta}(1,2),\\
Y & = X + \kappa N(0,c).
\end{aligned}
\end{equation*}
\item Exponential $(X, Y) \in \mathbb{R} \times \mathbb{R}$:
\begin{equation*}
\begin{aligned}
X & \sim \mathrm{Beta}(1,2),\\
Y & = exp(X) + \kappa N(0,c).
\end{aligned}
\end{equation*}
\item Cubic $(X, Y) \in \mathbb{R} \times \mathbb{R}$:
\begin{equation*}
\begin{aligned}
X & \sim \mathrm{Beta}(1,2),\\
Y & = 128(X-\frac{1}{3})^{3}+48(X-\frac{1}{3})^2-12(X-\frac{1}{3}) + \kappa N(0,c).
\end{aligned}
\end{equation*}
\item Joint normal $(X, Y) \in \mathbb{R} \times \mathbb{R}$:
\begin{equation*}
(X,Y) \sim N(0,\Sigma).
\end{equation*}
where $\Sigma = \begin{bmatrix}
 1 & 0.5\\
 0.5 & 2
\end{bmatrix}$.
\item W shape $(X, Y) \in \mathbb{R} \times \mathbb{R}$:
\begin{equation*}
\begin{aligned}
X & \sim \mathrm{Beta}(1,2),\\
Y & = 4 \left \{ (X^2-\frac{1}{2})^2-\frac{1}{500}U(-1,1) \right \} + 0.5 \kappa N(0,c).
\end{aligned}
\end{equation*}
\item Circle $(X, Y) \in \mathbb{R} \times \mathbb{R}$, for radius $r=1$ and $\theta = U(0,2\pi)$:
\begin{equation*}
\begin{aligned}
X & = r \cos \theta,\\
Y & = r \sin \theta + 0.5 \kappa N(0,c).
\end{aligned}
\end{equation*}
\item Diamond $(X, Y) \in \mathbb{R} \times \mathbb{R}$, for $u \sim U(-1,1)$, $v \sim U(-1,1)$ and $\theta = -\pi/4$:
\begin{equation*}
\begin{aligned}
X & = u \cos \theta + v \sin \theta,\\
Y & = -u \sin \theta + v\cos \theta + 0.5 \kappa N(0,c).
\end{aligned}
\end{equation*}
\item Multiplicative Noise $(X, Y) \in \mathbb{R} \times \mathbb{R}$, for $u \sim N(0,1)$,
\begin{equation*}
\begin{aligned}
X & N(0,1),\\
Y & = X u + 0.5 \kappa N(0,c).
\end{aligned}
\end{equation*}
\item Multimodal Independence $(X, Y) \in \mathbb{R} \times \mathbb{R}$, for $u \sim N(0,1)$, $v \sim N(0,1)$, $u_{1} \sim B(0.5)$ and $v_{1} \sim B(0.5)$,
\begin{equation*}
\begin{aligned}
X & = u/3 + 2 u_{1} -1,\\
Y & = v/3 + 2 v_{1} -1.
\end{aligned}
\end{equation*}

\end{enumerate}


\subsection{Sub-networks visualization of real data application}

In our actual data study, the 280 neurons of Caenorhabditis elegans can be divided into three parts: the neurons controlling the senses, the neurons controlling the dorsal muscles and the neurons controlling the ventral muscles. The interaction of neurons in each area is shown in the figure below.

\graphicspath{{figs/}}
	\begin{figure}[htbp!]
		\scriptsize
		\begin{center}
			\begin{tabular}{c}
				\includegraphics[width=0.9\textwidth]{sensory_subnet.pdf}
				\\
			\end{tabular}
		\end{center}
		\caption{Sub-network for sensory.}
\end{figure}

\graphicspath{{figs/}}
	\begin{figure}[htbp!]
		\scriptsize
		\begin{center}
			\begin{tabular}{c}
				\includegraphics[width=0.9\textwidth]{muscle1_subnet.pdf}
				\\
			\end{tabular}
		\end{center}
		\caption{Sub-network for dorsal body wall muscle.}
\end{figure}

\graphicspath{{figs/}}
	\begin{figure}[htbp!]
		\scriptsize
		\begin{center}
			\begin{tabular}{c}
				\includegraphics[width=0.9\textwidth]{muscle2_subnet.pdf}
				\\
			\end{tabular}
		\end{center}
		\caption{Sub-network for ventral body wall muscle.}
\end{figure}

\end{appendices}

\end{document}