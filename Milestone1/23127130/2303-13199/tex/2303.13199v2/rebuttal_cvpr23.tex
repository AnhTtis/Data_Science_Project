\documentclass[10pt,twocolumn,letterpaper]{article}
\usepackage[rebuttal]{cvpr}

% Include other packages here, before hyperref.
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{arydshln}

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[pagebackref,breaklinks,colorlinks,bookmarks=false]{hyperref}

% Support for easy cross-referencing
\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}
\usepackage{caption}
\captionsetup[table]{name=R-Tbl}

% If you wish to avoid re-using figure, table, and equation numbers from
% the main paper, please uncomment the following and change the numbers
% appropriately.
%\setcounter{figure}{2}
%\setcounter{table}{1}
%\setcounter{equation}{2}

% If you wish to avoid re-using reference numbers from the main paper,
% please uncomment the following and change the counter for `enumiv' to
% the number of references you have in the main paper (here, 6).
%\let\oldthebibliography=\thebibliography
%\let\oldendthebibliography=\endthebibliography
%\renewenvironment{thebibliography}[1]{%
%     \oldthebibliography{#1}%
%     \setcounter{enumiv}{6}%
%}{\oldendthebibliography}


%%%%%%%%% PAPER ID  - PLEASE UPDATE
\def\cvprPaperID{9865} % *** Enter the CVPR Paper ID here
\def\confName{CVPR}
\def\confYear{2023}

\begin{document}

%%%%%%%%% TITLE - PLEASE UPDATE
\title{Questioning the Efficacy of Continuous Feature Adaptation in Class-Incremental Learning}  % **** Enter the paper title here

\maketitle
\thispagestyle{empty}
\appendix

We thank all reviewers for their insightful comments.
We want to clarify a main point, which reviewers URBi and Ajsq seem to miss: we are not advocating the use of pre-trained backbones, but rather that they should be \textit{adapted}, using an adapter like FiLM. Leveraging only the data available in the first session of continual learning improves substantially over no-adaptation in many cases and solves a nonnegligible  part of the incremental learning challenge.
\vspace{2mm}\\
{\color{blue}\textbf{R. URBi:}}
 \textbf{Majority of the data cannot be obtained in the first session.}
We do not assume that a majority of the data arrives in the first session. As Tbl.~3 shows, FSA significantly improves over NA  even in the few-shot setting (where the first session necessarily contains only a small number of examples). In this setting, the first session contains as few data points as preceding sessions, see lines 528 \& 624 main paper, which is different from previous works. 
\\
 \textbf{Results show that frozen backbone is better in some cases. Criteria for when to finetune?}
Usually adapting to the first session improves performance significantly (Tbl.~1,~2,~3). In cases when the number of data in the first session is very small, it may be best not to adapt the body e.g.~in Fig.~3 first session has only 5 classes. The CH-index is predictive of the benefits of adaptation (sec 4.5 ln 751).
\\
 \textbf{Comparison to [1] with a pre-trained backbone.}
We want to emphasise the substantial differences between our approach and [1]: 1) For us, the pretraining is not part of the continual learning evaluation, but in~[1] it is the first continual learning session. 
%
We therefore do not care about preserving the performance on the pretraining distribution, while in [1] that is one of the main challenges.
%
2) We adapt FILM parameters in the first session using the few available data, whilst [1] relies on a big initial session of (800 classes) to train the model and then adapts the model by using additional convolutional blocks. 3) [1] does not consider the few-shot scenario and relies on storing samples. This is different from our replay-free work  that is tailored for few-shot incremental learning. 
In addition, we have provided an experimental comparison of our FiLM adapters to the adaptation strategy of [1],  namely FSA-LL, in R-Tbl~1,2; The FSA-FiLM method performs substantially better.
\vspace{2mm}\\
{\color{blue}\textbf{R. yNGG:}}
\textbf{Cosine distance-based classifier, evaluation under LDA head?}
We initially investigated cosine heads, but found that they did not improve over the LDA head. Note that the LDA head is a distance based classifier and it improves over the nearest class mean approach (Fig.~1). \vspace{2mm}\\
{\color{blue}\textbf{R. Ajsq:}}
\textbf{Using pre-trained feature extractor should be checked carefully. ImageNet dataset has overlapped classes  with those in CL datasets e.g. CIFAR-10. }
In real-world situations, pre-trained backbones need to be leveraged to obtain strong performance. It is also the case that the application target domain may well be close to, or overlapping with, the pre-training dataset. However, adaptation typically helps e.g.~we see large improvements for CIFAR100 ($12.5\%$ over NA in few-shot) even though one would think that this dataset  overlaps substantially with ImageNet. Note that we experimented with various distributions with a wide range of distances from  ImageNet (Tbl.~3).
\\
\textbf{In Fig.1, LDA  is not better than linear and NCM classifiers.}
Fig.~1 does not show continual learning, but rather the offline setting to motivate the use of the LDA head as it is on par with the linear head and outperforms the NCM head. 
\\
\textbf{Comparison to SOTA methods (why GDumb and FACT).}
Our method is replay-free, so fair comparison to replay-based methods is difficult. We use GDumb as a reference to show FSA is competitive and can improve over replay methods, see R-Tbl.~1. We have now also compared to E-EWC+SDC\footnote{\url{https://rb.gy/uyczdq}} a recent extension of EWC that overcomes the complications of the incremental head with no replay, and outperform it substantially, see R-Tbl.~1. In the main paper, we compared to FACT a recent work with competitive performance. In [G] improvements over FACT with no auto-augmentation were only around 3\% in average (we do not use any auto-augmentation).  
We thank the reviewer for the suggestions, we will extend our related work section.  We have now compared to ALICE[F] (thanks for the pointer -- we will add this to the paper) in both high- and few-shot settings. FSA performs far better; see R-Tbl.~1 and~2.
% DER [D] (and PODNET [B]) is a memory based method, and so is not directly comparable as we do not use replay, and does not improve much over GDumb for large buffer sizes. We compare with GDumb as a reference to replay based methods performance. Might be able to claim that our results are in the regime where GDumb $\approx$ DER performance.
% ALICE [F] uses a large number of base classes in the first session
% %E-EWC+SDC [\url{https://rb.gy/uyczdq}]
% Ares please add new results.
\begin{table}[t]
%\vspace*{1.5 cm}
%\begingroup
\setlength{\tabcolsep}{3.7pt} % Default value: 6pt
%\renewcommand{\arraystretch}{.9}
\begin{footnotesize}
%\begin{sc}
\begin{tabular}{l c c c c c }
\toprule
%& \multicolumn{5}{c}{\textbf{Datasets}} \\
%\cmidrule(lr){2-6}
\textbf{Method} & \textbf{CIFAR100} & \textbf{CORE50} &  \textbf{FGVC} & {\textbf{Cars}}  & {\textbf{Letters}} \\
 \midrule
GDumb-1k & 54.5 & 82.4 & 25.3 & 14.2 & 70.1 \\
\hdashline
E-EWC+SDC & 32.4 &  21.7 & 25.6 & 30.0 & 33.6 \\
ALICE & 52.4 & 72.8 & 39.8 & 36.4 & 75.7 \\
FSA-LL & 60.5 & 79.0 & 45.4  & 45.7 & 77.2 \\
FSA-FiLM &\textbf{73.8} & \textbf{85.4} & \textbf{55.9} & \textbf{55.9}  & \textbf{79.7} \\
\bottomrule
\end{tabular}
%\end{sc}
\end{footnotesize}
%\endgroup
\vspace*{-0.3cm}
\caption{Last session's test accuracy for the  \textbf{high-shot} CIL.}
\end{table}
\begin{table}[t]
\vspace*{-0.3cm}
%\begingroup
\setlength{\tabcolsep}{3.7pt} % Default value: 6pt
%\renewcommand{\arraystretch}{.9}
\begin{footnotesize}
%\begin{sc}
\begin{tabular}{l c c c c c c c}
\toprule
 \textbf{Method} & \textbf{CIFAR100} & \textbf{SVHN} &  \textbf{dSprites} &  \textbf{FGVC} & \textbf{Letters} \\
 \midrule
ALICE & 58.0 & 23.0  & 23.0 & 42.0 & 66.5 \\
FSA-LL & 62.0 & 43.5 & 18.8 & 45.8 & 69.4 \\
FSA-FiLM & \textbf{70.9} & \textbf{51.3} & \textbf{35.7} & \textbf{55.8} & \textbf{73.4} \\
\bottomrule
\end{tabular}
%\end{sc}
\end{footnotesize}
\vspace*{-0.3cm}
\caption{Last session's test accuracy for the \textbf{few-shot} CIL.}
\vspace*{-0.6cm}
%\endgroup
\end{table}
% \begin{table}[htb]
% \begingroup
% \setlength{\tabcolsep}{2.4pt} % Default value: 6pt
% \begin{footnotesize}
% \begin{tabular}{l c c}
% \toprule
% \textbf{Method} & \textbf{CIFAR100} & \textbf{CUB200} \\
% \midrule
% ALICE &  62.7 & \textbf{63.5} \\ 
% FSA-LL  & 61.4 & 55.9 \\ 
% FSA & \textbf{66.1} & 63.4 \\ 
% \bottomrule
% \end{tabular}
% %\end{sc}
% \end{footnotesize}
% \endgroup
% \end{table}

\noindent
\textbf{Novelty over CEC[39] based on fixed feature extractor.}
We do not use a fixed feature extractor, rather we adapt the feature extractor in the first session using e.g.~FiLM layers. We now compare to ALICE[F] that improves over CEC[39].
\\
\textbf{Evaluation on ``standard" CL datesets, ImageNet  and few-shot  miniimagenet?}
In general, these datasets are not appropriate for evaluating the performance of real CL systems as they result in a set of very in-domain sessions and do not test transfer from a source domain to a target domain. Instead, we use ImageNet as a source domain and then look at a range of realistic target datasets in the CL setting.
% \textbf{FACT as a baseline (its performance is only good with autoaugmentation[G]).}


% Comment to the area chair
% We would like to bring to your attention that the two reviewers URBi and Ajsq have considerably missed some of the main points of our work. Specifically 

%%%%%%%%% REFERENCES
%{\small
%\bibliographystyle{ieee_fullname}
%\bibliography{refs}
%}

\end{document}
