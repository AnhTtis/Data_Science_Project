\section{Proposed Algorithm}\label{sec:methodology}
In this section, first we discuss the main components of the proposed methods FSA/FSA-FiLM, namely body adaptation techniques and classifier heads. Then we formally introduce the two methods for tackling CIL.
\subsection{Problem Formulation}
In CIL, we are given a dataset $\cD_s = \{ \bx_{i,s}, y_{i,s} \}_{i=1}^{N_s}$ for each session $s \in \{1, \ldots, S\}$, where $X_s = \{ \bx_{i,s} \}_{i=1}^{N_s}$ is a set of images and $Y_s = \{ y_{i,s} \}_{i=1}^{N_s}$ is the set of the corresponding labels with $y_{i,s} \in \cY_s$. Here $\cY_s$ is the label space of session $s$. 
%A key characteristic of 
It is common in CIL that label spaces are mutually exclusive across sessions, i.e.~$\cY_s \cap \cY_{s^{\prime}} = \varnothing,~\forall s \neq s^{\prime}$ and we only have access to $\cD_s$ in the current session $s$ to train our model. The proposed FSA method can naturally handle sessions with overlapping label spaces too, however, we follow the former paradigm in our experiments. Another typical assumption in CIL is that the data in all sessions come from the same dataset. We also adopt this assumption in this work, although our experiments on DomainNet are a step toward considering different datasets in each session. 

In session $s$, we will use models defined as 
\begin{equation}
f_s (\bx) = W_s^{\top} g_{\btheta_s}(\bx)  + \bb_s, \label{eq:model}
\end{equation}
where $g_{\btheta_s}(\bx) \in \bbR^d$ is a feature extractor backbone\footnote{In this paper, we interchangeably use the terms feature extractor, backbone, and body.} with session-dependent parameters $\btheta_s$. The linear classifier head comprises the class weights $W_s \in \bbR^{d \times |\cY_{1:s}|}$ and biases $\bb_s \in \bbR^{|\cY_{1:s}|}$, and $\cY_{1:s} = \cup_{j=1}^s \cY_j$ is the label space of all distinct classes we have seen up to session $s$. Ideally, when a new dataset $\cD_s$ is available at the $s$-th session, the model should be able to update the backbone parameters $\btheta$ and the linear classifier head $W_s$ with the minimum computational overhead and without compromising performance over the previously seen classes $\cY_{1:s}$.

%\vspace*{-0.3 cm}
\paragraph{Backbone adaptation.} Traditionally, the adaptation of the body at the current session $s$ requires updating the full set of  the network parameters $\btheta_s$ (full-body adaptation). Options include using specifically designed loss functions that mitigate catastrophic forgetting or using a memory buffer that stores a subset of the previously encountered data. See \cite{mai2022online} for a review of continual learning techniques. Nevertheless, when training data availability is scarce relative to the size of the model then full-body adaptation ceases to be suitable, and few-shot learning techniques \cite{wang2020generalizing}, such as meta-learning \cite{hospedales2021meta} and transfer learning \cite{yosinski2014transferable} become favorable. 


%Briefly, a meta-learner is trained on a large number of tasks in order to learn-how-to-learn in order to generalize well.  when new tasks arrive while in transfer learning.
%
%we start with a pre-trained backbone on a large upstream dataset \cite{kolesnikov2020big} and then we update the full set or a subset of the model parameters on a downstream task. 

Recent works in few-shot learning \cite{requeima2019fast} showed that a pre-trained backbone can be efficiently adapted by keeping its parameters $\btheta_s$ frozen and introducing Feature-wise Linear Modulation (FiLM) \cite{perez2018film} layers with additional parameters $\bxi_s$ which scale and shift the activations produced by a convolutional layer. 
%
In \cite{requeima2019fast}, these parameters are generated by a meta-trained network when a downstream task is given. Alternatively, $\bxi_s$ can be learned (fine-tuned) by the downstream data as in \cite{shysheya2022fit}. In this work, we consider both meta-learned (Supplement) and fine-tuned (\cref{sec:experiments}) FiLM parameters.

%\vspace*{-0.2 cm}
\paragraph{Classifier heads.}
In both offline and CIL settings, (linear) classifier heads can be divided into two groups; parametrized and parameter-free. Parametrized heads require the weights/biases in \cref{eq:model} to be updated by an iterative gradient-based procedure whilst for parameter-free heads, a closed-form formula is available that directly computes the weights/biases after the update of the backbone. We consider three different heads (for notational simplicity we consider the offline setting, and thus suppress session $s$ dependence), one parametrized, and two parameter-free.
\begin{itemize}
    \item The linear (learnable) head where $W, \bb$  are learned.
    
    \item The Nearest Class Mean (NCM) classifier where the weight and bias of the $k$-th class is given by
    \begin{equation}\label{eq:ncm}
        \bw_k = \hat{\bmu}_k~\text{and}~b_k = \ln \frac{|X^{(k)}|}{N}-\frac{1}{2} \hat{\bmu}_k^{\top} \hat{\bmu}_k  , 
    \end{equation}
    where $X^{(k)} = \{\bx_i : y_i = k \}$ is the set of images belonging in class $k$ and $\hat{\bmu}_k =  \frac{1}{|X^{(k)}|} \sum_{\bx \in X^{(k)}} g_{\btheta}(\bx)$ is the mean vector of the embedded images of class $k$.

    \item The Linear Discriminant Analysis (LDA) classifier with weights/biases defined as
    \begin{equation}\label{eq:lda}
        \bw_k = \Tilde{S}^{-1}\hat{\bmu}_k~\text{and}~b_k = \ln \frac{|X^{(k)}|}{N}-\frac{1}{2} \hat{\bmu}_k^{\top} \Tilde{S}^{-1} \hat{\bmu}_k ,
    \end{equation}
    where $\Tilde{S} = S + I_d$ is the regularized sample covariance matrix with sample covariance matrix $S = \frac{1}{|X|-1} \sum_{\bx \in X} (g_{\btheta}(\bx) - \hat{\bmu}) (g_{\btheta}(\bx) - \hat{\bmu})^{\top} $, $\hat{\bmu} =  \frac{1}{|X|} \sum_{\bx \in X} g_{\btheta}(\bx)$, and identity matrix $I_d \in \bbR^{d \times d}$.
\end{itemize}

Both NCM and LDA classifiers are suitable for CIL since their closed-form updates in \cref{eq:ncm} and \cref{eq:lda} support exact, computationally inexpensive, continual updates (via running averages) that incur no forgetting. Updating the linear head, in contrast, requires more computational effort and machinery when novel classes appear in a new session. Notice that setting $\Tilde{S} = I_d$ in LDA recovers NCM. 

%Notice that NCM is a special case of LDA when you set $\Tilde{S} = I_d$ for LDA. 

The continuous update of the LDA head is not as straightforward as it is for NCM, since LDA  also requires updating the sample covariance $S$. We discuss how this can be attained in the next section where we introduce our proposed method FSA/FSA-FiLM.


%Without loss of generality, we can assume that we have two sessions in total, i.e. $S=2$, and after creating the LDA head in the first session using \cref{eq:lda} for dataset $X_1$, we want to update the head using the data of the second session $X_2$. This can be done by computing and storing the following quantities:


%define CIL mathematically
%- define sessions

%backbones
%- general discussion 
%- FiLM

%heads
%- linear head
%- LDA head
%- explain how the head can updated incrementally

\subsection{First Session Adaptation}
Motivated by the efficient adaptation techniques and CIL-friendly classifiers discussed in the previous section, we propose two simple and computationally efficient CIL methods called First Session Adaptation (FSA) and FSA-FiLM. FSA is based on a full-body adaptation while FSA-FiLM uses FiLM layers to adapt the body. Both methods utilize pre-trained backbones. FSA (-FiLM)  (i) adapts the backbone only at the first session and then the backbone remains frozen for the rest of the sessions, and (ii) makes use of an LDA classifier which is continuously updated as new data become available. %Despite the computational convenience of LDA, we found empirically that adapting the body using an LDA head makes the optimization process  unstable and slow, leading to sub-optimal solutions. 
For adapting the body (either full or FiLM-based adaptation) at the first session,  we use a linear head and a cross-entropy loss first, and then after the optimization is over, the linear head is removed  and an LDA head is deployed instead, based on the optimized parameters $\btheta^*$ and \cref{eq:lda}. Updating the LDA head when the data of the next session becomes available, can be done by using  \cref{algo:lda_update} recursively until the last session $S$. Specifically, by having access to the running terms $\{A, \bb, \textrm{count}\}$, the sample covariance matrix is given by
\begin{equation}
S = \frac{1}{\textrm{count}-1} \left(A - \frac{1}{\textrm{count}} \bb \bb^{\top} \right). \label{eq:updated_sample_cov}
\end{equation}
%
\begin{algorithm}
  \caption{Update of sample covariance running terms}\label{algo:lda_update}
  \begin{algorithmic}[1]
    \Require $g(\cdot) \equiv$ a feature extractor backbone
    \Require $X_s = \{ \bx_{i,s} \}_{i=1}^{N_s}$: Images of session $s$
    \Require $A \in \bbR^ {d \times d}, \bb \in \bbR^d, \textrm{count} \in \bbN^*$ if $s > 1$
    \Function{IncUpdate}{$g(\cdot), X_s, A, \bb, \textrm{count}$}
       \If{$s = 1$} \Comment{Initialize $A, \bb, \textrm{count}$}
          \State $A\gets \mathbf{0}_{d \times d}, \bb \gets \mathbf{0}_d, \textrm{count}\gets 0$
          %\State $\bb \gets \mathbf{0}_d$
          %\State $\textrm{count}\gets 0$
        \EndIf
      \State $A\gets A + \sum_{\bx \in X_s} g(\bx) g(\bx)^{\top}$
      \State $\bb \gets \bb + \sum_{\bx \in X_s} g(\bx)$
      \State $\textrm{count}\gets \textrm{count} + N_s$
      \State \textbf{return} $A, \bb, \textrm{count}$ 
    \EndFunction
  \end{algorithmic}
\end{algorithm}
%
The time complexity of LDA scales as $\cO (d^3 + |\cY_s| d^2)$ and its space complexity is $\cO (d^2 +  |\cY_{1:s}| d)$ at the $s$-th session.
This computational burden is negligible compared to taking gradient steps for learning a linear head and as we will see in \cref{sec:head_comparisons} using covariance information boosts performance significantly against the vanilla NCM head.
%\vspace*{-0.3cm}

% Add differences Beyond Simple Meta-Learning:
% Multi-Purpose Models for Multi-Domain, Active
% and Continual Few-Shot Learning

%pre-trained backbone
%adapt on first session of continual learning using a linear head, fix the backbone thereafter
%replace linear head with LDA head and update head in an incremental way thereafter
