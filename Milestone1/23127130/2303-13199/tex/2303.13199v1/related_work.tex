\section{Related Work}
%In this work, we focus on continual learning for classification problems. The goal is to learn a representation that is beneficial for all classes being learnt and to incrementally construct a classifier with limited or restricted access to data of classes learnt in the past. This setting is referred to as Class Incremental Learning. This setting differs from Task Incremental learning scenario that focuses on learning a shared representation while deploying a separate classifier for each task. We refer to \cite{de2021continual} for a survey.
% \textbf{Task Incremental Learning}
% One form of continual learning is to assume that each training session belongs to a separate task that is learned using a separate classification head. Methods usually rely on regularization to prevent forgetting of previous sessions. However, it is assumed that the task ID is known at test time. 
%\aresnote{Cite suggested papers by CVPR's reviewers.}

Class Incremental Learning is more challenging than task-incremental continual learning as a shared output layer is used for  the classes of different learning sessions. We refer to \cite{de2021continual} for a survey on both class and task incremental learning. While softmax cross entropy loss is considered a standard in classification models, it is shown to be a source of class interference in continual learning~\cite{wu2019large,caccia2022new}. Thus, recent works either focus on fixing the softmax classifier~\cite{caccia2022new,Ahn_2021_ICCV,zhao2020maintaining} or deploy  nearest class mean classifiers (NCM) as an alternative~\cite{rebuffi2017icarl,davari2022probing,kang2022class}. In this work, we deploy an LDA classifier that uses the mean embedding of each class and an incrementally updated covariance  matrix shared across all classes. We show that this approach is comparable to a softmax classifier in the offline setting and that it outperforms NCM.

Due to the challenging nature of class incremental learning, some methods  employ a buffer of stored samples from previous sessions~\cite{prabhu2020gdumb,yan2021dynamically}. \cite{prabhu2020gdumb} suggests a simple baseline, GDumb, that performs offline training on both buffer data and new session data at each incremental step. GDumb shows strong performance compared to sophisticated continual learning solutions. Here we show that our simple yet effective solution often outperforms variants of GDumb that have a large memory without leveraging any buffer of stored samples.

In addition to the issues arising from the shared classification layer, learning a feature extractor that can provide representative features of all classes is a key element in continual learning. Some class incremental learning methods leverage large initial training session or start from a pretrained network, e.g.,~\cite{wang2022learning,hersche2022constrained,wu2021striking} and show improved overall continual learning performance without questioning the role of such pre-training steps and whether the learned representations have in fact improved. Recently, \cite{ostapenko2022continual} studies continual learning with strong pre-trained models and proposes deploying exemplar-based replay in the latent space using a small multilayer network on top of the fixed pre-trained model. The authors show that such latent replay improves performance, especially for tasks that are different from  the distribution used to train the initial model. In this work, we show that adaptation of pre-trained models is essential for strong continual learning performance. However, different from all existing continual learning works, we show that adapting the representation only in the first session is sufficient to obtain representative features for the classes being learned and that this forms a strong baseline for future continual learning methods. 

In addition to the incremental classification in the full data regime, we focus on the few-shot scenario where only a few labeled examples per class are provided at each training session.
% Recently \cite{wu2022class} examines training with a model pre-trained on a large number of base classes with different settings of CL.  They branch the network after some layer and then after learning new classes, they fuse the branches.
\cite{hersche2022constrained} utilize a meta-trained network for few-shot classification and explore multiple design choices   including a no-adaptation baseline and a method to optimize orthogonal prototypes with the last classification layer only. The method heavily relies on a pre-training and meta-training step on a large subset of classes from the original dataset. In this work, we show that the meta-training step is not essential and that a simple LDA head is sufficient  without the reliance on a big initial training stage. 
% Mode 1: no network retrained with prototypes as the average of class samples (features).
% Mode 2: optimize bipolarized prototypes by adding noise to the average prototypes. The FC layer is trained using stored activations to maximize the similarities between the class activations and the prototypes. 
% Mode 3: Instead of bipolarizing the prototypes they optimize nudged prototypes: prototypes that minimize inter class similarities and remain close to the initial avenged prototypes. Then they optimize the FC layer to align the features of the stored activations with the nudged prototypes. 
%
In  \cite{kang2022class} 
the authors propose a new distillation loss functioning at the feature map level where importance values are estimated per feature map along with replay and consider an NCM classifier at test time.
\cite{zhou2022forward}
heavily relies on the training of an initial session using a specific loss function that takes into account classes that will be added in future. It further requires the total number of classes to be known \emph{a priori} which is unrealistic for real applications.
Our solution adapts FILM parameters to the first session data and fixes the representation for the remaining training sessions. We show that surprisingly few samples in the first session are sufficient for  adaptation  and that our solution is more powerful than current few-shot continual learning solutions. 
