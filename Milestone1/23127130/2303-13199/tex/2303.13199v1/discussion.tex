\section{Discussion}\label{sec:discussion}
We have presented FSA (-FiLM), a simple yet effective replay-free baseline for CIL which adapts a pre-trained backbone via full body adaptation or FiLM layers only at the first session of continual learning and then utilizes a flexible incrementally updated LDA classifier on top of the body. Extensive experiments in several CIL scenarios have shown
that FSA outperforms previous SOTA baselines in most cases, thus, questioning the efficacy of current approaches to continuous body adaption. Furthermore, the experiments have revealed that FiLM layers are helpful when the amount of data available in the target domain is relatively low and the number of parameters in the neural network and the size of the source domain data set is large. For very large foundation models trained on very large source domain data sets, it is likely we are often in this situation and FiLM-like layers will be more effective than full-body adaptation, even when the target domain has a fairly large amount of data.

%\vspace*{-.05cm}
\noindent
\textbf{Limitations and future work.} The main limitation of this work is inextricably linked with the distributional assumptions of CIL in general. If the data distribution at each session shifts considerably (e.g.~the first session includes natural images while the next session includes images of numbers) then first session body adaptation is not suitable. In future work, we plan to combine FSA (-FiLM) with a memory method like GDumb to get the best of both worlds and deal with more challenging CIL scenarios.

%For very large foundation models trained on very large source domain data sets, it is likely we are often in this situation and adapters will be more effective than full-body adaptation, even when the target domain has a fairly large amount of data.  

%Extensions: combine first session adaptation with a memory method like GDumb to get the best of both worlds