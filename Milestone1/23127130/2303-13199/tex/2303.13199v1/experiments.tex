\begin{figure*}[htb]
\centering
\includegraphics[width = \textwidth]{plots/barplot_compare_all_heads_shots} 
\caption{Average accuracy across all VTAB+ datasets using no-adaptation (NA), FiLM adaptation (A-FiLM), and full body adaptation (A-FB) for different classifier heads (NCM, LDA, Linear) and number of shots (5, 10, 50, All Data). Note, this is the offline setting we consider here where all classes are available without any incremental learning.}
\label{fig:head_comparisons}
\end{figure*}

\section{Experiments}\label{sec:experiments}
In this section, we present a series of experiments to investigate the performance of FSA (-FiLM) under different CIL settings. First, we detail the datasets used for the experiments and discuss our implementation. Second, we perform a large empirical comparison between LDA, linear, and NCM classifiers, in combination with different body adaptation techniques, in the offline setting. We also consider how these findings are affected by the number of shots available. Third, we compare FSA (-FiLM) with state-of-the-art continual learning methods and conduct ablation studies. Finally, we investigate the effect of adapting the backbone when the CIL dataset is ``similar'' to ImageNet-1k and discuss a similarity metric that indicates whether body adaptation is required. 


\subsection{Datasets and Implementation details}\label{sec:datasets}
\paragraph{Datasets.} We employ a diverse collection of 26 image-classification datasets across the offline and CIL experiments. For the offline experiments of \cref{sec:head_comparisons}, we use the 19 datasets of VTAB \cite{zhai2019large}, a low-shot transfer learning benchmark, plus three additional datasets, FGVC-Aircraft \cite{maji2013fine}, Stanford Cars \cite{krause20133d}, and Letters \cite{deCampos2009character}. We refer to this group of datasets as VTAB+. For the CIL-based experiments, we choose 5 VTAB+ datasets from different domains, having an adequate number of classes to create realistic CIL scenarios. The datasets are CIFAR100, SVHN, dSprites-location, FGVC-Aircraft, Cars, and Letters while also including 4 extra datasets: CUB200 \cite{WahCUB_200_2011}, CORE50 \cite{lomonaco2017core50}, iNaturalist \cite{van2018inaturalist}, and DomainNet \cite{peng2019moment}. Exact details for each dataset are provided in the Supplement.

\paragraph{Training details.} All models are implemented with PyTorch \cite{paszke2019pytorch}. We use a pre-trained EfficientNet-B0 \cite{tan2019efficientnet} on Imagenet-1k as the main backbone for all methods. For the few-shot+ CIL experiment in \cref{sec:cil_comparisons}, we also consider two ResNet architectures, ResNet-18 and ResNet-20 \cite{he2016deep} to enable direct comparison to the original settings used in \cite{zhou2022forward}. All the deployed backbones (except ResNet-20, due to the unavailability of pre-trained weights on ImageNet-1k), are pre-trained on ImageNet-1k for all methods. We keep the optimization settings the same across all baselines for fairness. Optimization details are given in the Supplement.

%\vspace*{-0.3 cm}
\paragraph{Evaluation protocol.} We report the Top-1 accuracy after the last continual learning session evaluated on a test set including instances from all the previously seen classes. In the Supplement, we provide the accuracy after each session. To quantify the forgetting behavior, we use a scaled modification of the performance dropping rate \cite{tao2020few}, namely percent performance dropping rate (PPDR), defined as $ \text{PPDR} = 100 \times \frac{\mathcal{A}_1 - \mathcal{A}_S}{\mathcal{A}_1} \%$, where $\mathcal{A}_1$ denotes  test accuracy after the first session, and $\mathcal{A}_S$  the test accuracy after the last session.


\subsection{Head Comparisons}\label{sec:head_comparisons}
To motivate the choice of the LDA head for FSA (-FiLM), we compare its average accuracy across all VTAB+ datasets with that of the linear and NCM classifiers under the offline setting where all the classes are available and no incremental learning is required. For the body, we use no adaptation as a baseline (i.e.~using the unadapted original backbone), FiLM-based adaptation, and full-body adaptation while ranging the number of shots from 5 to 10, then 50, and finally using all available training data. As Fig.~\ref{fig:head_comparisons} illustrates, LDA consistently outperforms both NCM and linear heads when the number of shots is equal to 5 and 10, regardless of adaptation technique. Notice also that as the number of learnable parameters increases, the choice of the head plays a less significant role in the predictive power of the method. Overall, LDA performs  similarly to the linear head in the high-shot settings and  performs the best of all heads in the low-shot settings. This is crucial for our proposed FSA (-FiLM) method as the computational burden associated with updating the LDA head is negligible and incremental updates are straightforward as we discuss in~\cref{sec:methodology}. On the other hand, incremental updates for a linear head are problematic and generally result in a significant loss in performance %over the offline setting 
~\cite{wu2019large,caccia2022new}. Finally, the full covariance information of LDA provides a consistent advantage over its isotropic covariance counterpart, i.e.~the NCM classifier. This is in agreement with the results presented in~\cite{shysheya2022fit} for the LDA and NCM classifier. In addition to the results presented here, we have also tested meta-learning based body adaptation methods~\cite{bronskill2021memory,requeima2019fast,bateni2020improved} which support continual learning out of the box. We find these   perform poorly compared to the fine-tuning based methods. See Supplement for details.

\begin{table*}
%\vspace*{1.5 cm}
%\begingroup
\setlength{\tabcolsep}{4.9pt} % Default value: 6pt
%\renewcommand{\arraystretch}{.9}
\begin{center}
\begin{small}
%\begin{sc}
\begin{tabular}{l c c c c c c c | c}
\toprule
%& \multicolumn{5}{c}{\textbf{Datasets}} \\
%\cmidrule(lr){2-6}
\textbf{Method} & \textbf{CIFAR100} & \textbf{CORE50} & \textbf{SVHN} &  \textbf{dSprites-loc} &  \textbf{FGVC-Aircraft} & \textbf{Cars}  & \textbf{Letters}  & \textbf{Avg Diff} \\
 \midrule
NA & 68.2 (26.8) & 82.6 (14.2) & 39.9 (51.1) & 20.6 (54.0) & 41.3 (1.7) & 43.3 (40.3) &  68.4 (24.1) & 0.0\\
\hdashline
E-EWC+SDC~\cite{yu2020semantic} & 32.4 (66.7) &  21.7 (78.1) & 39.5 (60.1) & 18.6 (81.4) & 25.6 (55.8) & 30.0 (62.6) & 33.6 (66.3) & -23.3\\
FACT~\cite{zhou2022forward} & 10.2 (89.4) & 22.0 (77.7) & 33.8 (65.9) & 6.4 (93.6) & 4.7 (90.3) & 0.6 (99.3) &  20.9 (79.1) & -38.0 \\
ALICE~\cite{peng2022few} & 52.4 (45.8) & 72.8 (25.8) & 46.1 (53.6) & 68.3 (31.7) & 39.8 (35.0)& 36.4 (56.0) & 75.7 (24.2) & +3.9\\
FSA & 62.8 (34.5) & 82.8 (15.5)  & 71.3 (26.6) & \textbf{91.5} (\textbf{8.5}) & 50.8 (6.3)  & 50.3 (36.6)  & 78.4 (21.4) & +17.7 \\
FSA-LL & 60.5 (37.2) & 79.0 (19.2) & 64.6 (33.1) & 91.3 (8.5) & 45.4 (21.7) & 45.7 (43.8) & 77.2 (22.7) & +14.2 \\
FSA-FiLM & \textbf{73.8} (\textbf{23.4}) & \textbf{85.4} (\textbf{13.3})  & \textbf{75.9} (\textbf{23.4}) &  76.9 (22.8)  &  \textbf{55.9} (\textbf{-5.7}) & \textbf{55.9} (\textbf{30.2})  & \textbf{79.7} (\textbf{20.0}) & \textbf{+19.9} \\
\midrule
GDumb~\cite{prabhu2020gdumb} & 54.5 (42.2) & 82.4 (15.0) & 78.2 (19.6) & 79.5 (12.9) & 25.3 (56.9)  & 14.2 (82.6) & 70.1 (27.0) & +5.7 \\
\midrule
Offline-FiLM & 78.2 & 88.4 & 93.1  & 98.5 & 67.5 & 67.3 & 85.2 & +30.6\\
\bottomrule
\end{tabular}
%\end{sc}
\end{small}
\end{center}
\caption{Last session's test accuracy (\%) ($\uparrow$) and the PPDR (\%) ($\downarrow$) in parentheses, for the high-shot CIL setting (\cref{sec:cil_comparisons}). The last column reports the average accuracy difference ($\uparrow$) across all datasets between  a baseline and NA. A pre-trained EfficientNet-B0 on Imagenet-1k is used as a backbone for all methods. For reference, we include the replay-based baseline GDumb where 1k images are used for the memory buffer.}
\label{table:cifar100_core50_fullshots}
%\endgroup
%\vspace*{-0.2cm}
\end{table*}

\subsection{Class-Incremental Learning Comparisons}\label{sec:cil_comparisons}
We consider three different CIL scenarios: (i) high-shot CIL, (ii) few-shot+ CIL, and (iii) few-shot CIL. We compare our FSA/FSA-FiLM methods with recent state-of-the-art FSCIL methods, including  Decoupled-Cosine~\cite{vinyals2016matching}, CEC~\cite{zhang2021few}, FACT~\cite{zhou2022forward}, and ALICE. Additionally, for the high-shot CIL setting, we consider a strong replay-based baseline for CIL,  GDumb~\cite{prabhu2020gdumb} and a competitive replay-free method, E-EWC+SDC~\cite{yu2020semantic}. Finally, we introduce an additional baseline adapter inspired by \cite{wu2022class}, called FSA-LL (Last Layer). In FSA-LL only the parameters of the backbone's last block are fine-tuned which can be compared to the FSA-FiLM adaption method.


%\vspace*{-0.2cm}
\paragraph{High-shot CIL.} In this setting, we consider all the available training data for each class while keeping the number of novel classes in each session low. We use CIFAR100, CORE50, SVHN, dSprites-loc, FGVC-Aircraft, Cars, and Letters for our experiments. For CIFAR100, CORE50, and FGVC-Aircraft, the first session consists of 10 classes, 4 for dSprites-loc, 16 for Cars, and 12 for letters. The rest of the sessions include 10 classes for CIFAR100, FGVC-Aircraft, 20 for Cars, 5 for CORE50 and Letters, 2 for SVHN and dSprites-loc. Therefore, the total number of incremental sessions for CIFAR100, FGVC-Aircraft, and Cars is 10 while for CORE50 we have 9 sessions. A pre-trained on Imagenet-1k EfficientNet-B0 is deployed as a backbone for all methods. FSA-FiLM outperforms all the competitors by a significant margin on all datasets except dSprites-loc. We attribute the performance gap on dSprites-loc due to the large number of data points (25k) and  the portion of the classes ($25\%$) in the first session. It is also apparent the efficiency of fine-tuning FiLM parameters over fine-tuning the parameters of the model's last layer; note the number of FiLM parameters is only $20.5$k while the last layer of an EfficientNet-B0 comprises $2.9$M parameters. Furthermore, fine-tuning the FiLM parameters offer almost $20\%$ accuracy increase on average compared to using a pretrained model without doing any further adaptation. Interestingly, the replay-free FSA-FILM is able to outperform significantly the replay-based method GDumb with a 1k memory buffer on most of the datasets; accuracy and time comparisons between FSA-FiLM and GDumb  with varying buffer sizes can be found in Figure 1 of the \suppl. 

%We see from \Cref{table:cifar100_core50_fullshots} that FSA-FiLM, overall, achieves performance not far from the offline setting and it outperforms all baselines in 3 out of the 4 datasets. For CORE50, FSA-FiLM is outperformed by GDumb which uses a large memory buffer of 5k images. The trade-off between accuracy and training time for different continual learning methods on CIFAR100 and CORE50 is illustrated in \cref{fig:gdumb_cifar100_core50_time_acc}. Several different memory sizes are used for GDumb. FSA-FiLM attains the highest accuracy (and the lowest PPDR) $\sim$13.5x faster than GDumb with a 5k memory buffer on CIFAR100 while on CORE50, GDumb requires at least a 5k memory buffer to outperform FSA-FiLM and $\sim$3x more training time than FSA-FiLM. Notice that FACT is unable to perform well under this setting due to the small number of available classes in the first session. 

\iffalse
\begin{table}
%\vspace*{1.5 cm}
%\begingroup
\setlength{\tabcolsep}{1.9pt} % Default value: 6pt
%\renewcommand{\arraystretch}{.9}
\begin{footnotesize}
%\begin{sc}
\caption{Last session's test accuracy (\%) ($\uparrow$) and the PPDR (\%) ($\downarrow$) in parentheses, for the high-shot CIL setting (\cref{sec:cil_comparisons}). The last column reports the average accuracy difference across all datasets between  a baseline and NA. GDumb  is the only memory-based method used for comparisons; we use a buffer size equal to $\min (5\text{k}, N_1)$ where $N_1$ is the first session's number of images. The best results across all methods are in bold while the best results across the no-memory methods are underlined. An EfficientNet-B0 pre-trained on Imagenet-1k is used as a backbone for all methods.}
\label{table:cifar100_core50_fullshots}
\begin{tabular}{l c c c c | c |}
\toprule
%& \multicolumn{5}{c}{\textbf{Datasets}} \\
%\cmidrule(lr){2-6}
\textbf{Method} & \textbf{CIFAR} & \textbf{CORE} &  \textbf{FGVC} & {\textbf{Cars}} & {\textbf{Avg Diff}}  \\
 \midrule
NA & 68.2 (26.8) & 82.6 (14.2)  & 41.3 (1.7) & 43.3 (40.3) & 0.0\\
\hdashline
%GDumb-1k & 48.8 (48.6) & 81.2 (15.9) \\
GDumb & 69.3 (28.6) & \textbf{88.1} (\textbf{7.9}) & 25.3 (56.9)  &  14.2 (82.6) & -9.6 \\
%GDumb-10k & - & \textbf{90.0}(\textbf{7.6}) \\
\hdashline
FACT\cite{zhou2022forward} & 10.2 (89.4) & 22.0 (77.7) & 4.7 (90.3) & 0.6 (99.3) & -49.5 \\
FSA & 62.8 (34.5) & 82.8 (15.5) & 50.8 (6.3)  & 50.3 (36.6) & +2.82 \\
FSA-FiLM & \underline{\textbf{73.8}} (\underline{\textbf{23.4}}) & \underline{85.4} (\underline{13.3}) &  \underline{\textbf{55.9}} (\underline{\textbf{-5.7}}) & \underline{\textbf{55.9}} (\underline{\textbf{30.2}}) & \underline{\textbf{+8.9}} \\
\midrule
Offline-FiLM & 76.5 & 88.4 & 66.8 & 67.7 & +16.0 \\
\bottomrule
\end{tabular}
%\end{sc}
\end{footnotesize}
%\endgroup
%\vspace*{-0.2cm}
\end{table}
\fi



%----------------------------------------------------------------------------------------------------
%\vspace*{-0.25cm}
\paragraph{Few-shot+ CIL.} This setting is the one that has most commonly been used for few-shot CIL (FSCIL) and it involves an initial session that contains a large number of classes (around 50-60\% of the total number of classes in the dataset) and all the available training images of these classes. The remaining sessions comprise a small number of classes and shots (typically 5/10-way 5-shot). Here we follow the exact FSCIL settings as described in \cite{zhou2022forward,peng2022few} for CIFAR100 and CUB200. We use ResNet-18/20 and EfficientNet-B0 as backbones. \Cref{table:fact_vs_our} summarizes the performance comparison between baselines. FSA performs on par with FACT on CIFAR100 when we use the original backbone used in \cite{zhou2022forward}, and it outperforms FACT by almost 10\%  and ALICE by 3.4\% when EfficientNet-B0 is utilized while FSA-FiLM exhibits the lowest PPDR score. Notice also that FSA is only marginally worse than its offline counterpart, meaning there is little room for continuous body adaptation to improve things further. For CUB200, FSA with an EfficientNet-B0 performs on par with ALICE. Interestingly, we observe that NA performs well on this dataset. This indicates that CUB200 is not far from ImageNet-1k. The current results of FSA set new SOTA performance on CIFAR100 for the FSCIL setting. %For CUB200, FSA with a ResNet-18 already outperforms all SOTA baselines while similar patterns are present when  EfficientNet-B0 is used. Interestingly, we observe that NA performs well on this dataset. This indicates that CUB200 is not far from ImageNet-1k. The current results of FSA/FSA-FiLM set new SOTA performance on CIFAR100 and CORE50 for the FSCIL setting.
\begin{table}[htb]

\setlength{\tabcolsep}{7.4pt} % Default value: 6pt
%\renewcommand{\arraystretch}{.9}
\begin{center}
    
\begin{small}
%\begin{sc}

\begin{tabular}{l c c c}
\toprule
& & \multicolumn{2}{c}{ \textbf{Datasets}} \\
\cmidrule(lr){3-4}  \textbf{Method} & \textbf{Backbone} &  \textbf{CIFAR100} &  \textbf{CUB200} \\
\midrule
CEC~\cite{zhang2021few} & \multirow{3}{*}{RN-20} & 49.1 (32.7)* & - \\
FACT~\cite{zhou2022forward} & & 52.1 (30.2)* & - \\ 
FSA & & 52.0 (30.8) & - \\ 
\hdashline
NA & \multirow{6}{*}{RN-18} & 50.4 (26.8) & 50.0 (29.3) \\ 
CEC~\cite{zhang2021few} & & - & 52.3 (31.1)* \\
FACT~\cite{zhou2022forward} & & 49.5 (34.8)* & 56.9 (25.0)* \\
ALICE~\cite{peng2022few} &  & 54.1 (31.5$)^{\dagger}$ & 60.1 (22.4$)^{\dagger}$ \\
FSA-FiLM &  & 55.2 (24.4) & 52.7 (27.6) \\ 
FSA & & 61.4 (25.1) & 57.6 (24.3) \\ 
\hdashline
NA & \multirow{6}{*}{EN-B0} & 55.2 (25.8) &  63.2 (\textbf{19.6}) \\ 
FACT~\cite{zhou2022forward} & & 56.5 (34.6) & 62.9 (23.3) \\ 
ALICE~\cite{peng2022few} & & 62.7 (28.4) &  \textbf{63.5} (22.2) \\ 
FSA-LL &  & 61.4 (25.7) & 55.9 (22.3) \\ 
FSA-FiLM &  & 61.8 (\textbf{22.4}) & 62.9 (20.4) \\ 
FSA &  &  \textbf{66.1} (24.6) & 63.4 (20.9) \\ 
\midrule
Offline & EN-B0 & 67.0 & 65.1 \\
\bottomrule
\end{tabular}
%\end{sc}
\end{small}
\end{center}
\caption{Baseline comparison under the few-shot+ CIL setting (\cref{sec:cil_comparisons}). We report the accuracy (\%) ($\uparrow$) of the last session and the PPDR (\%) ($\downarrow$) in parentheses. Asterisk (*) indicates that the reported results are from \cite{zhou2022forward} and $\dagger$ indicates results reported in \cite{peng2022few}. We use three different backbones, EfficientNet-B0 (EN-B0) and ResNet-18/20 (RN-18/20); EN-B0 and RN-18 are pre-trained on Imagenet-1k.}
\label{table:fact_vs_our}
\end{table}

%\vspace*{-.9cm}
\paragraph{Few-shot CIL.} The final setting, which is first introduced in this work, considers an alternative  FSCIL setting in which only a small number of data points are available in all sessions, including the first. We use 50 shots per session while the first session includes 20\% of the total number of classes of the dataset. Each of the remaining sessions includes around 10\% of the total number of classes; more details are available in the Supplement. We repeat experiments 5 times and we report the mean accuracy and PPDR in \Cref{table:50shot_CL_acc_ppd}. FSA-FiLM outperforms all the other baselines by a large margin in terms of both accuracy and PPDR, indicating that transfer learning is considerably advantageous for CIL when the data availability is low. Notice that both ALICE and FACT struggle to achieve good performance under this setting due to the limited amount of data in the first session. Finally, we find 
that FGVC-Aircraft exhibits a positive backward transfer behavior
that we attribute to a  difficult initial session followed by sessions that contain classes that are comparatively easier to distinguish.

\iffalse
\begin{table*}[htb]
%\vspace*{1.5 cm}
%\begingroup
\setlength{\tabcolsep}{10.0pt} % Default value: 6pt
%\renewcommand{\arraystretch}{.9}
\begin{footnotesize}
%\begin{sc}
\caption{Accuracy (\%) ($\uparrow$)  of the last session and PPDR (\%) ($\downarrow$) in parentheses for the few-shot CIL setting of \cref{sec:cil_comparisons}.  GDumb  is the only memory-based method used for comparisons; we use a buffer size equal to the first session's number of images $N_1$. The best results across all methods are in bold while the best results across the no-memory methods are underlined. A pre-trained EfficientNet-B0 on Imagenet-1k is used as a backbone for all methods.}
\label{table:50shot_CL_acc_ppd}
\begin{tabular}{l c c c c c c c}
\toprule
 \textbf{Method/Dataset} & \textbf{CIFAR100} & \textbf{SVHN} &  \textbf{dSprites-loc} &  \textbf{FGVC-Aircraft} & \textbf{Letters}  & \textbf{DomainNet} & \textbf{iNaturalist} \\
 \midrule
NA & 57.4 (28.4) & 28.3 (61.5) & 11.9 (66.6) & 41.0 (-15.5) & 57.6 (29.9) & 69.0 (17.2) & 49.7 (4.3)\\
\hdashline
GDumb & 55.7 (33.7) & 21.0 (73.1) & 16.4 (55.0) & 38.6 (24.5) & 41.2 (54.8) & 63.2 (28.4) & 40.4 (28.4) \\
\hdashline
FACT~\cite{zhou2022forward} & 16.8 (79.7) & 24.1 (66.2) & 11.7 (64.1) & 8.3 (79.9) & 49.8 (40.9) & 20.6 (75.6) & 14.3 (74.0) \\
FSA & 60.3 (27.0) & 32.9 (53.5) & 33.7 (\underline{\textbf{41.3}}) & 50.1 (-16.8) & 62.2 (28.5) & 70.3 (17.5) & 51.5 (\underline{\textbf{1.1}}) \\
FSA-FiLM & \underline{\textbf{70.9}} (\underline{\textbf{20.5}}) & \underline{\textbf{51.3}} (\underline{\textbf{43.5}}) & \underline{\textbf{35.7}} (43.1) & \underline{\textbf{55.8}} (\underline{\textbf{-19.8}}) & \underline{\textbf{73.4}} (\underline{\textbf{22.1}}) & \underline{\textbf{74.0}} (\underline{\textbf{15.6}}) & \underline{\textbf{58.8}} (4.8)\\
\midrule
Offline-FiLM & 73.8 & 77.2 & 83.7 & 65.1 & 79.7 & 75.1 & 62.1 \\
\bottomrule
\end{tabular}
%\end{sc}
\end{footnotesize}
%\endgroup
\end{table*}
\fi
\begin{table*}[htb]
%\vspace*{1.5 cm}
%\begingroup
\setlength{\tabcolsep}{4.8pt} % Default value: 6pt
%\renewcommand{\arraystretch}{.9}
\begin{center}
\begin{small}
%\begin{sc}
\begin{tabular}{l c c c c c c c | c}
\toprule
 \textbf{Method/Dataset} & \textbf{CIFAR100} & \textbf{SVHN} &  \textbf{dSprites-loc} &  \textbf{FGVC-Aircraft} & \textbf{Letters}  & \textbf{DomainNet} & \textbf{iNaturalist} & \textbf{Avg Diff} \\
 \midrule
NA & 57.4 (28.4) & 28.3 (61.5) & 11.9 (66.6) & 41.0 (-15.5) & 57.6 (29.9) & 69.0 (17.2) & 49.7 (4.3) & 0.0\\
\hdashline
FACT~\cite{zhou2022forward} & 16.8 (79.7) & 24.1 (66.2) & 11.7 (64.1) & 8.3 (79.9) & 49.8 (40.9) & 20.6 (75.6) & 14.3 (74.0) & -24.2\\
ALICE~\cite{peng2022few} & 58.0 (33.8) & 23.0 (65.9) & 23.0 (46.0) & 42.0 (27.5) & 66.5 (31.4) & 66.5 (25.1) & 47.9 (13.7) & +1.7\\
FSA & 60.3 (27.0) & 32.9 (53.5) & 33.7 (\textbf{41.3}) & 50.1 (-16.8) & 62.2 (28.5) & 70.3 (17.5) & 51.5 (\textbf{1.1}) & +6.6\\
FSA-LL & 62.0 (25.7) & 43.5 (47.0) & 18.8 (61.7) & 45.8 (-2.1) & 69.4 (26.1) & 67.6 (16.9) & 49.2 (14.1) & +5.9 \\
FSA-FiLM & \textbf{70.9} (\textbf{20.5}) & \textbf{51.3} (\textbf{43.5}) & \textbf{35.7} (43.1) & \textbf{55.8} (\textbf{-19.8}) & \textbf{73.4} (\textbf{22.1}) & \textbf{74.0} (\textbf{15.6}) & \textbf{58.8} (4.8) & +15.0 \\
\midrule
Offline-FiLM & 73.8 & 77.2 & 83.7 & 65.1 & 79.7 & 75.1 & 62.1 & +28.0 \\
\bottomrule
\end{tabular}
%\end{sc}
\end{small}
\end{center}
\caption{Accuracy (\%) ($\uparrow$)  of the last session and PPDR (\%) ($\downarrow$) in parentheses for the few-shot CIL setting of \cref{sec:cil_comparisons}. The last column reports the average accuracy difference ($\uparrow$) across all datasets between  a baseline and NA. A pre-trained EfficientNet-B0 on Imagenet-1k is used as a backbone for all methods. A pre-trained EfficientNet-B0 on Imagenet-1k is used as a backbone for all methods.}
\label{table:50shot_CL_acc_ppd}
%\endgroup
\end{table*}

%-----------------------------------------------------------------------------
\begin{table}[ht]
%\begingroup
\setlength{\tabcolsep}{7.5pt} % Default value: 6pt
%\renewcommand{\arraystretch}{1.1}
\begin{center}
\begin{footnotesize}
%\begin{sc}
\begin{tabular}{l c c c c c}
\toprule
\textbf{Dataset} & \textbf{NA} & \textbf{Aq} & \textbf{DevF} &  \textbf{Veh} \\
 \midrule
CIFAR100 & 57.4{\scriptsize $\pm 1.0$} & 66.1{\scriptsize $\pm 1.6$} & 66.2{\scriptsize $\pm 1.9$} & 67.9{\scriptsize $\pm 1.2$} \\
\midrule
\midrule
& \textbf{NA} & \textbf{El-R} & \textbf{F-Clp} &  \textbf{Tr-Sk} \\
\midrule
DomainNet & 69.0{\scriptsize $\pm 0.4$} & 70.6{\scriptsize $\pm 0.6$} & 71.7{\scriptsize $\pm 0.6$} & 72.8{\scriptsize $\pm 0.5$} \\
\bottomrule
\end{tabular}
%\end{sc}
\end{footnotesize}
\end{center}
\caption{Last session accuracy (\%) ($\uparrow$) of FSA-FiLM for three different session splits on CIFAR100 and DomainNet as described in \cref{sec:cil_nonhomogeneous}. For reference, we also include the accuracy of NA. Results are averaged over 5 runs (mean$\pm$std).}
\label{table:cifar100_domainnet_superclass}
%\endgroup
\end{table}
\subsection{Inhomogeneous Class-Incremental Learning}\label{sec:cil_nonhomogeneous}

FSA adapts the body only on the first session of continual learning and it is therefore likely to be sensitive to the particular classes which are present in this session. To investigate the degree of performance sensitivity for FSA, we devise a setting similar to the few-shot CIL setting of \cref{sec:cil_comparisons}, where each session includes classes that share some common attribute. We select CIFAR100 which provides superclass information, and DomainNet which consists of different domains and also has superclass information available.
%
We create three distinct CIL configurations for each dataset, each of which has different types of data in the first session. For CIFAR100, we split the data into 19 sessions. 
%
The first session includes 10 classes from super-classes (i) aquatic mammals and fish (\textit{Aq}), (ii) electric devices and furniture (\textit{DevF}), or (iii) Vehicles 1 and Vehicles 2 (\textit{Veh}). Each of the other 18 sessions include images from the remaining 18 super-classes.
%
Similarly, for DomainNet, we split the data into 6 sessions using 50 shots with 10 classes in each session. The first session includes 10 classes of (i) the ``electricity'' superclass of the real domain (\textit{El-R}), (ii) the ``furniture'' superclass of the clipart domain (\textit{F-Clp}), or (iii) the ``transportation'' superclass of the sketch domain (\textit{Tr-Sk}). In this way, we can vary the content of the first session and analyze the effect this has on performance.
%
\Cref{table:cifar100_domainnet_superclass} reveals that FSA-FiLM's performance is similar even if the data in the first session used for the body adaptation appears disparate from that contained in the remaining sessions. Even for DomainNet where the distribution shift of the data across sessions is considerable, performance is only marginally affected in each of the three settings. This provides evidence that adapting the body only on the first session achieves competitive performance regardless of the class order, given the assumption that the data come from a single dataset (albeit a varied one in the case of DomainNet). 


\begin{figure}[ht]
\centering
\includegraphics[width =\linewidth]{plots/cos_dist_scores_mini} 
\caption{Scatter plot of the accuracy differences between FSA-FiLM and NA against the minimum cosine distance between a dataset and \emph{mini}Imagenet dataset evaluated using the NA method. We consider the offline setting with 50 shots.}
\label{fig:ch_index}
\end{figure}
\vspace*{-.18cm}
\subsection{When to adapt the body?}\label{sec:body_adaptation}
% old name: The Effect of Body Adaptation in Low-Data Availability
Despite the strong performance of FSA (-FiLM) in the few-shot settings of \cref{sec:cil_comparisons}, there are cases where the NA method achieves very close accuracy to FSA (e.g.~see CUB200 results in \Cref{table:fact_vs_our}). This implies that there may be datasets where adaptation is not required and all we need is the pre-trained backbone. In order to decide whether we require body adaptation or not, 
%given the data of the first session, we utilize the Calinski-Harabasz (CH) index \cite{calinski1974dendrite} which quantifies how dense and well-separated are the classes. The score is evaluated on the training data of the first session through the embedded representations of the pre-trained backbone. Larger scores indicate that the pre-trained backbone is sufficient and no adaptation is required. 
we compute the minimum cosine distance in the embedding space of the pre-trained backbone between the downstream dataset and the \emph{mini}Imagenet~\cite{vinyals2016matching} dataset. We use \emph{mini}Imagenet ($60$k images) as a proxy of Imagenet-1k ($1.3$M images) to reduce the computational overhead of evaluating pairwise distances. This allows us to approximately measure the dissimilarity between the downstream dataset and Imagenet-1k.

%
%\vspace*{-.51cm}
%\vspace*{-.44cm}
%
%We first consider a challenging setting for FSA with only 5 classes and 50 shots per session. This results in sessions that are far smaller, on average than those considered in the few-shot CIL setting of \cref{sec:cil_comparisons}, making body adaptation non-trivial. We choose 12 datasets from VTAB+ and we run 12 few-shot CIL experiments. \cref{fig:ch_index} shows  the accuracy difference between FSA-FiLM and NA at the end of the last session as a function of the CH index. Only SVHN, FGVC-Aircraft, and Letters benefit significantly from body adaptation in this setting. If we use a threshold of 10 for the CH-index and use body adaptation below this number, but not above, we improve over adapting everywhere by  $2.4\%$ and improve over no-adaptation everywhere by $+7.3\%$.
%\vspace*{-.18cm}
\cref{fig:ch_index} shows  the accuracy difference between FSA-FiLM and NA as a function of the cosine distance for the offline setting with 50 shots whilst \cref{fig:na_film_diff_50_shots_offline} illustrates the same accuracy difference where the datasets are grouped based on the VTAB+ categorization. For this experiment, we use 24 datasets (VTAB+, DomainNet, and iNaturalist). We observe that adaptation is more beneficial for datasets in the structured domain, which contain images that are dissimilar to those of ImageNet-1k.
%SVHN, dSprites-loc, and dSprites-ori benefit significantly from FiLM adaptation. %\aresnote{Change this} If we use a threshold of 0.5 for the cosine distance and use FiLM adaptation above this number, but not below, we improve  over no-adaptation by $+12.6\%$.
%In particular, the average accuracy difference for datasets with CH index larger than 10 is $-2.4\%$, meaning that we lose $2.4\%$ accuracy on average when we decide to adapt the body instead of keeping it fixed. On the other hand, for datasets with CH index lower than 10, the average accuracy difference is $+7.3\%$, showing that body-adaptation is beneficial. Therefore, we suggest using 10 as a cut-off CH value for deciding whether to adapt or not the body.

%We first consider the few-shot CIL setting of \cref{sec:cil_comparisons}. \cref{fig:ch_index} shows  the accuracy difference between FSA-FiLM and NA at the end of the last session as a function of the CH index. SVHN and dSprites-loc benefit significantly from FiLM adaptation. If we use a threshold of 10 for the CH-index and use FiLM adaptation below this number, but not above, we improve  over no-adaptation by $+23.4\%$.
%In particular, the average accuracy difference for datasets with CH index larger than 10 is $-2.4\%$, meaning that we lose $2.4\%$ accuracy on average when we decide to adapt the body instead of keeping it fixed. On the other hand, for datasets with CH index lower than 10, the average accuracy difference is $+7.3\%$, showing that body-adaptation is beneficial. Therefore, we suggest using 10 as a cut-off CH value for deciding whether to adapt or not the body.
%A similar plot for the accuracy differences is given in \cref{fig:na_film_diff_50_shots_offline} where the datasets are grouped based on the VTAB+ categorization. We observe that adaptation is more beneficial for datasets in the structured domain, which contain images that are dissimilar to those of ImageNet-1k.

\begin{figure}[ht]
\centering
\includegraphics[width = \linewidth]{plots/barplot_na_film_diff_shots=50} 
\caption{Bar plot of the accuracy differences between FiLM and NA for the offline case with 50 shots. %Notice that the benefits of body adaptation are more tangible compared to those in \cref{fig:ch_index} due to the larger number of data in this setting.
}
\label{fig:na_film_diff_50_shots_offline}
\end{figure}

\begin{table}[ht]
\vspace*{-.3 cm}
%\begingroup
\setlength{\tabcolsep}{0.8pt} % Default value: 6pt
%\renewcommand{\arraystretch}{.9}
\begin{center}

\begin{footnotesize}
%\begin{sc}
\begin{tabular}{l c c c c c c c}
\toprule
  & \textbf{CFR100} & \textbf{SVHN} &  \textbf{dSp-loc} &  \textbf{FGVC} & \textbf{Letters}  & \textbf{DNet} & \textbf{iNat} \\
 \midrule
ConvNext & \textbf{87.1{\tiny $\pm 0.6$}} & 43.6{\tiny $\pm 1.6$} & 13.6{\tiny $\pm 0.5$} & 50.2{\tiny $\pm 0.5$} & 63.1{\tiny $\pm 1.1$} & \textbf{82.9 {\tiny $\pm 0.7$}} & \textbf{72.6} \\
FSA-FiLM & 70.9{\tiny $\pm 1.0$} & \textbf{51.3{\tiny $\pm 2.1$}} & \textbf{35.7{\tiny $\pm 2.1$}} &\textbf{55.8{\tiny $\pm 0.6$}} & \textbf{73.4{\tiny $\pm 0.4$}} & 74.0{\tiny $\pm 0.2$} & 58.8 \\
\bottomrule
\end{tabular}
%\end{sc}
\end{footnotesize}
\end{center}
\caption{Accuracy comparison between a pre-trained ConvNext on ImageNet-21k and FSA-FiLM. We use the few-shot CIL setting  (\cref{sec:cil_comparisons}) and we report the accuracy (\%) ($\uparrow$) after the last session. Results are averaged over 5 runs (meanÂ±std).}
\label{table:convnext_vs_film}
%\endgroup
\end{table}

%\vspace*{-.14cm}
To stress the importance of adaptation on datasets far from ImageNet, we compare FSA-FiLM with an EfficientNet-B0 backbone to the no-adaptation method with a ConvNext \cite{liu2022convnet} pre-trained on Imagenet-21k. The total number of parameters for FSA-FiLM (backbone and FiLM parameters) is $\sim$4M while for ConvNext is 348M.  \Cref{table:convnext_vs_film} shows that a small adapted backbone can significantly surpass the accuracy of a much larger pre-trained backbone for datasets far from ImageNet. %The conclusion from these results also coincides with that of \cref{fig:ch_index}. 
As the community employs ever-larger models and datasets, these results indicate that adapters are likely to continue to bring improvements over simply learning a classifier head (the NA baseline). 




