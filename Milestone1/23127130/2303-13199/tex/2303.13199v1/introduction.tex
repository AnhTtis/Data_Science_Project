\section{Introduction}
% general intro to continual learning
Continual learning (CL) is needed to bring machine learning models to many real-life applications. After a model is trained on a given set of data, and once deployed in  its test environment, it is likely that new classes will naturally emerge. For example, in an autonomous driving scenario, new types of road transportation and traffic signage can be encountered.  The deployed model needs to efficiently acquire this new knowledge with minimal cost (e.g.~annotation requirements) without deteriorating the performance on existing classes of objects. The process of efficiently acquiring new knowledge while preserving what is already captured by the model is what continual learning methods target.

% types of continual learning / why it arises
Continual learning can be mainly divided into task incremental learning and class incremental learning. Task incremental learning (TIL) sequentially learns independent sets of heterogeneous tasks and is out of this paper's scope. Class incremental learning (CIL), in contrast, assumes that all classes (already learnt and future ones) form part of a single classification task. 
% perhaps add one or two more sentences about general context?
In class-incremental learning, data arrive in a sequence of sessions with new classes appearing as the sessions progress. Critically, the data in each session cannot be stored in its entirety and cannot be revisited. The goal is to train a single classification model under these constraints. 

In both task incremental and class incremental learning, the data distribution will change as the sessions progress. However, these changes tend to be smaller in real-world class incremental learning settings than in the task incremental setting. For, example consider a human support robot learning about new objects in a home in an incremental manner (CIL) vs.~the same robot learning to adapt to different homes (TIL).

Practically, CIL has two major uses. First, in situations where large amounts of data arrive in each session and so retraining the model is computationally prohibitive.  Second, in situations where data are not allowed to be revisited due to privacy reasons such as under General Data Protection Regulation (GDPR). The latter situation is relevant for applications such as personalization where small numbers of data points are available i.e.~few-shot continual learning.
%
So-called replay-free methods are necessary for such settings, as samples of previous sessions are not allowed to be memorized, but CIL is known to be challenging in such settings.

%In CIL forgetting can be severe when samples of previous sessions are not memorized. Replay-free methods thus become of high importance in such scenario. 

Current SOTA methods for class incremental learning start from a pre-trained backbone~\cite{lesort2022scaling,ostapenko2022continual,wang2022learning,wang2022dualprompt} and then adapt the features at each session of continual learning. The use of a pre-trained backbone has been shown to lead to strong performance especially in the few-shot CIL setting due to the lack of data~\cite{peng2022few}. However, it is unclear to what extent continuous adaption of the features in each session is helpful. In general, in CIL there is a trade-off between adaptation (which helps adapt to the new statistics of the target domain) and catastrophic forgetting (whereby earlier classes are forgotten as the representation changes).  When memorization of previous samples is restricted, the benefits from adapting the feature extractor body to learn better features might well be out-weighted by the increase in forgetting of old classes. Moreover, body adaptation in earlier sessions is arguably more critical (e.g.~adapting the backbone to the new domain in the first session), whilst it is less essential in later sessions where the changes in the ideal representation between sessions are smaller. 


% focus on practical continual learning
%This paper is primarily interested in developing techniques that will be useful in the practical deployment of continual learning, rather than building general continual learning systems in order to emulate human intelligence in the abstract or to understand neural processing in the brain \cite{kirkpatrick2017overcoming,zenke2017continual}. This ethos will be reflected in the experimental setup where we will focus on class incremental learning within a single domain rather than across multiple domains. For example, here we are not interested in designing systems that can handle one session containing handwritten digits and another naturalistic images as this arguably has limited practical value at this time. The ethos will also be reflected in the approaches to learning that we will use. Specifically, rather than learning classifiers from scratch, like much of the continual learning literature, we will instead make extensive use of neural network backbones pre-trained on large image data sets and adapt them from the source domain in a continuous way on the target domain of interest, as this transfer learning approach is key to achieving state-of-the-art (SOTA) performance.  
% It is worth noting that for certain benchmarks, class incremental methods are already based on a pretrained backbone, however, the margin of improvement brought by the continual adaptation of the pretrained backbone has been overlooked.

% existing work and 
%The area of CIL has garnered significant interest recently and it is widely agreed that good performance will generally require the features extracted by a neural network body to be adapted in each session of continual learning (continuous adaptation of the body) so that the features become refined to the task at hand. However, it is well known that this can lead to the algorithm forgetting classes that are seen in the earlier stages of continual learning. This is known as the catastrophic forgetting/intransigence trade-off.  

% outline of the paper [work in section references here] 
This work explores under what conditions continuous adaptation of the body is beneficial, both in principle and in current practice. In order to do this, we develop a new replay-free method, inspired by the first encoding method of \cite{bateni2020improved}, called First Session Adaptation (FSA). FSA adapts the body of a pre-trained neural network on only the first session of continual learning. We investigate adapting all body parameters and the use of Feature-wise Layer Modulation (FiLM) adapters \cite{perez2018film} which learn only a small number of parameters. The head, in contrast, is adapted at each session using an approach that is similar to Linear Discriminate Analysis (LDA), which suffers from no forgetting, and improves over the Nearest Class Mean classifier (NCM) \cite{mensink2013distance} approach. The efficacy of this general approach, including comparisons to a standard linear head, is first motivated through experiments in the offline setting (\cref{sec:head_comparisons}).    
%
%We then consider continual learning, first considering the high-shot setting (high-shot CIL). Next, we consider few-shot continual learning following previous approaches that employ an initial session with a large number of data points and few-shot sessions thereafter (few-shot+ CIL). Finally, we consider few-shot continual learning in which each task contains only a small amount of data (few-shot CIL).  
We then carry out experiments under three CIL settings. First, we consider the high-shot setting (high-shot CIL). The other two settings consider few-shot continual learning. Specifically, one setting follows previous approaches that employ an initial session with a large number of data points and few-shot sessions thereafter (few-shot+ CIL) while the other exclusively includes sessions with only a small amount of data (few-shot CIL).  

The contributions of the paper are as follows: (1) We develop a replay-free CIL baseline, namely FSA, that is extremely simple to implement and performs well in many different scenarios. (2) We empirically motivate FSA through a set of offline experiments that evaluate different forms of neural network head and body adaptation;  (3) We then compare FSA to a set of strong continual learning baseline methods in a fair way using the same pre-trained backbone for all methods. (4) We show that the FSA-FiLM baseline performs well in the high-shot CIL setting, outperforming the SOTA whilst avoiding data  memorization. (5) In the few-shot learning settings, FSA outperforms existing continual learning methods on eight benchmarks, often by a substantial margin, and is statistically tied with the best performing method on the one remaining dataset. (6) Finally, we propose a measure that can be applied to a set of unlabelled inputs which is predictive of the benefits of body adaptation.
 %   \item We find the FiLM approach to be especially useful when there is relatively little data in the target domain and it is relatively far from the source domain used for pre-training. 
%\end{itemize}

%[?] three important facets of algorithms: regularization (keeping new parameters close to old ones), memory-based approaches, architecture - position work in this context: alternative to regularization, could be combined with a memory (but don't look at it here), explore the use of different architectures
