\section{Experiment}



\subsection{Datasets}
We evaluate our methods on two opinion summarization datasets: \textsc{Space} \cite{angelidis-etal-2021-extractive}, containing reviews from \textit{hotel} domain, and \textsc{Oposum+} \cite{amplayo-etal-2021-aspect}, containing Amazon product reviews from six different domains. Both datasets are comprised of a large corpus of raw reviews and a small development and test set with human-annotated aspects and general opinion summaries for evaluation. Aspect seed words are usually obtained with a small amount of human effort. For \textsc{SW-LOO}, we use the same seed words as in \citet{amplayo-etal-2021-aspect} (Appendix \ref{sec:seed_words_lists}). Refer to Appendix \ref{sec:dataset_details} for detailed descriptions and statistics of the two datasets.


\begin{table}[t]
\hspace*{-0.3cm} 
\centering
\scalebox{0.75}{
    \begin{tabular}{@{}clcccccc@{}}
    \toprule
    \multicolumn{2}{c}{} & \multicolumn{3}{c}{\textsc{Space}} & \multicolumn{3}{c}{\textsc{Oposum+}} \\
    \multicolumn{2}{c}{Model} & R1 & R2 & RL & R1 & R2 & RL \\ 
     \midrule

     \parbox[t]{0.05em}{\multirow{6}{*}{\rotatebox[origin=c]{90}{\small Extractive}}} & 
    \textsc{LexRank} & 24.61 & 3.41 & 18.03 & 22.51 & 3.35 & 17.27 \\
     & \textsc{QT} & 28.95 & 8.34 & 21.77 & 23.99 & 4.36 & 16.61 \\ 
     & \textsc{AceSum\textsubscript{Ext}} & 30.91 & 8.77 & 23.61 & 26.16 & 5.75 & 18.55 \\
     & \textsc{SemAE} & 31.24 & 10.43 & 24.14 & - & - & - \\
     & \textbf{\textsc{SW-LOO\textsubscript{Ext}}} & \underline{33.14} & 10.32 & 25.81 & 28.14 & 6.10 & 19.51 \\
     & \textbf{\textsc{NLI-LOO\textsubscript{Ext}}} & 27.18 & 6.63 & 20.60 & 26.78 & 6.48 & 18.07 \\
    
    \midrule

     \parbox[t]{0.05em}{\multirow{5}{*}{\rotatebox[origin=c]{90}{\small Abstractive}}} & 
     \textsc{MeanSum} & 25.68 & 4.61 & 18.44 & 24.63 & 3.47 & 17.53 \\
     & \textsc{CopyCat} & 27.19 & 5.63 & 19.18 & 26.17 & 4.30 & 18.20 \\
     & \textsc{AceSum} & 32.41 & 9.47 & 25.46 & \underline{29.53} & \underline{6.79} & \textbf{21.06} \\
     & \textbf{\textsc{SW-LOO}} & \textbf{34.68} & \textbf{11.50} & \textbf{28.83} & \textbf{30.00} & \textbf{6.92} & \underline{20.76} \\
     & \textbf{\textsc{NLI-LOO}} & 31.57 & \underline{10.44} & \underline{26.66} & 28.90 & 6.60 & 20.11 \\

    \midrule
     & \textsc{Human} & 44.86 & 18.45 & 34.58 & 43.03 & 16.16 & 31.53 \\
    \bottomrule
    \end{tabular}
}
\caption{\small Evaluation for \textit{aspect summaries} on \textsc{Space} and \textsc{Oposum+} test sets. Best performances are  in \textbf{bold} and second best performances are \underline{underlined}.}
\label{tab:aspect_summaries}
\end{table}


\subsection{Baselines}
We compare our methods with several unsupervised extractive and abstractive approaches. Extractive approaches include \textsc{Centroid} \cite{Radev2004CentroidbasedSO}, \textsc{LexRank} \cite{Erkan2004LexRankGL}, \textsc{QT} \cite{angelidis-etal-2021-extractive}, \textsc{SemAE} \cite{basu-roy-chowdhury-etal-2022-unsupervised}, and two extractive variants of our methods, \textsc{SW-LOO\textsubscript{Ext}} and \textsc{NLI-LOO\textsubscript{Ext}}, by feeding identified aspect-related sentences to \textsc{LexRank} instead of \textsc{T5}, similar to the idea in \citet{amplayo-etal-2021-aspect}. Abstractive approaches include \textsc{MeanSum} \cite{chu2019meansum}, \textsc{CopyCat} \cite{brazinskas-etal-2020-unsupervised}, and \textsc{AceSum} \cite{amplayo-etal-2021-aspect}. 
% We choose the above approaches as baselines since they are the most recent approaches and also widely compared with in existing literature. 
Appendix \ref{sec:baseline_details} contains more details on baselines.



We also compare with two upper bounds reported in \citet{amplayo-etal-2021-aspect}: an \textsc{Oracle} that selects the review with the highest ROUGE score to the gold summary as the summary and a \textsc{Human} upper bound that is calculated as the inter-annotator ROUGE scores. 



\subsection{Implementation} 
\label{sec:implementation}
We first pre-process the raw corpus such as removing products with very few reviews and too long or short reviews as in Appendix \ref{sec:datasets_preprocess}. We use \textsc{T5-Small} as our summarization models and larger \textsc{T5} size does not show improvements as shown in Appendix \ref{sec:t5_size}. We use a \textsc{MNLI} \cite{williams-etal-2018-broad} finetuned \textsc{BART-Large} \cite{lewis-etal-2020-bart} model in \textsc{NLI-LOO}. We choose this model given its better performance\footnote{\url{https://joeddav.github.io/blog/2020/05/29/ZSL.html}} in zero-shot topic classification. We perform simple hyper-parameter tuning on dev sets and select checkpoints with the best ROUGE-L scores to report performances on test sets. Please refer to Appendix \ref{sec:implementation_detials} for more details such as training configurations and other analyses.
% on hyper-parameter tuning and implementation such as linear learning rate warm-up, token budget, synthetic datasets sizes, etc.



\subsection{Results}
We evaluate the quality of generated opinion summaries using ROUGE1/2/L F1 scores \cite{lin-2004-rouge}. Example summaries generated by our methods are shown in Table \ref{tab:space_summaries} and Table \ref{tab:oposum_summaries} in Appendix.



\begin{table}[t]
\hspace*{-0.3cm}
\centering
\scalebox{0.75}{
    \begin{tabular}{@{}clcccccc@{}}
    \toprule
    \multicolumn{2}{c}{} & \multicolumn{3}{c}{\textsc{Space}} & \multicolumn{3}{c}{\textsc{Oposum+}} \\
    \multicolumn{2}{c}{Model} & R1 & R2 & RL & R1 & R2 & RL \\ 
    \midrule
    \parbox[t]{0.05em}{\multirow{7}{*}{\rotatebox[origin=c]{90}{\small Extractive}}} & 
     \textsc{Centroid} & 31.29 & 4.91 & 16.43 & 33.44 & 11.00 & 20.54 \\
     & \textsc{LexRank} & 31.41 & 5.05 & 18.12 & 35.42 & 10.22 & 20.92 \\
     & \textsc{QT} & 38.66 & 10.22 & 21.90 & 37.72 & 14.65 & 21.69 \\ 
     & \textsc{AceSum\textsubscript{Ext}} & 35.50 & 7.82 & 20.09 & 38.48 & 15.17 & 22.82 \\
     & \textsc{SemAE} & \textbf{43.46} & \textbf{13.48} & \textbf{26.40} & - & - & - \\
     & \textbf{\textsc{SW-LOO\textsubscript{Ext}}} & 38.44 & 11.01 & \underline{25.62} & \textbf{40.45} & \textbf{19.13} & \underline{23.20} \\
     & \textbf{\textsc{NLI-LOO\textsubscript{Ext}}} & 25.07 & 4.52 & 16.16 & \underline{39.79} & \underline{18.33} & \textbf{23.49}  \\
    \midrule
    \parbox[t]{0.05em}{\multirow{5}{*}{\rotatebox[origin=c]{90}{\small Abstractive}}} &
     \textsc{MeanSum} & 34.95 & 7.49 & 19.92 & 26.25 & 4.62 & 16.49 \\
     & \textsc{CopyCat} & 36.66 & 8.87 & 20.90 & 27.98 & 5.79 & 17.07 \\
     & \textsc{AceSum} & 40.37 & 11.51 & 23.23 & 32.98 & 10.72 & 20.27 \\
     & \textbf{\textsc{SW-LOO}} & \underline{42.27} & \underline{12.99} & 23.47 & 36.19 & 12.17 & 21.11 \\
     & \textbf{\textsc{NLI-LOO}} & 41.25 & 12.79 & 24.31 & 31.22 & 9.93 & 19.08 \\
    \midrule
     & \textsc{Oracle} & 40.23 & 13.96 & 23.46 & 41.88 & 21.52 & 29.30 \\
     & \textsc{Human} & 49.80 & 18.80 & 29.19 & 55.42 & 37.26 & 44.85 \\
    \bottomrule
    \end{tabular}
}
\caption{\small Evaluation for \textit{general summaries} on \textsc{Space} and \textsc{Oposum+} test sets. Best performances are highlighted in bold and second-best performances are underlined.}
\label{tab:general_summaries}
\end{table}


\smallskip
\noindent
\paragraph{Aspect Opinion Summarization} Table \ref{tab:aspect_summaries} contains the results of all baselines and our methods on the two benchmark datasets. Despite its simplicity, \textsc{SW-LOO} achieves the highest scores on both datasets across all metrics except RL for \textsc{Oposum+} with only $0.3$ points behind the best-performing baseline. On the other hand, \textsc{NLI-LOO} achieves higher R2 and RL scores on \textsc{Space} than existing methods despite using no seed words. While it falls behind other methods on \textsc{Oposum+}, it is at most $1$ point behind across all metrics. This highlights that even without aspect seed words, \textsc{NLI-LOO} is possible to compete with SOTA aspect-based opinion summarization methods.

Next, we turn to the evaluation of extractive versions of our methods. We observe \textsc{SW-LOO\textsubscript{Ext}} achieves higher R1 and RL scores on \textsc{Space} but falls behind on \textsc{Oposum+} by at most $1.5$ point compared with all baselines. This is consistent with the finding in \citet{amplayo-etal-2021-aspect} that a simple centrality-based extractive approach such as \textsc{LexRank} are strong baselines as long as input sentences are already aspect-related. And \textsc{SW-LOO\textsubscript{Ext}} outperforming \textsc{AceSum\textsubscript{Ext}} further shows that our simple filtering method using exact-matching seed words already produces good enough aspect-related sentences compared with the extra learning module used in \citet{amplayo-etal-2021-aspect}. However, \textsc{NLI-LOO\textsubscript{Ext}}, is not able to outperform the best baseline, and we hypothesize the reason is that NLI model filtered aspect-related sentences are still noisy so that a summarization model is required to serve as regularization.

Finally, comparing our four methods, \textsc{SW-LOO} achieves the best performances with the supervision of seed words, \textsc{NLI-LOO} comes second despite the lack of seed words supervision, and our two extractive versions come last since the ground truth summaries are in nature abstractive.



\smallskip
\noindent
\textbf{General Opinion Summarization} \hspace{4pt} 
As shown in Table \ref{tab:general_summaries}, on \textsc{Space}, \textsc{SW-LOO} and \textsc{NLI-LOO} outperform the SOTA abstractive system, \textsc{AceSum}, but under-perform SOTA extractive system, \textsc{SemAE}. We observe the same trend between \textsc{SW-LOO\textsubscript{Ext}} and \textsc{AceSum\textsubscript{Ext}} as in aspect opinion summarization and this again shows the simple yet effective nature of our filtering method. For \textsc{Oposum+}, \textsc{SW-LOO\textsubscript{Ext}} and \textsc{NLI-LOO\textsubscript{Ext}} outperform existing methods given that the annotated general summaries for \textsc{Oposum+} are extractive, \textsc{SW-LOO} outperforms existing abstractive approaches, and \textsc{NLI-LOO} falls behind with only $1$ point.


\begin{table}[t]
\hspace*{-0.2cm}
\centering
\scalebox{0.75}{
    \begin{tabular}{@{}lcccc@{}}
    \toprule
     & \multicolumn{2}{c}{\textsc{Space}} & \multicolumn{2}{c}{\textsc{Oposum+}} \\
    \multicolumn{1}{c}{Model} & Aspect & General & Aspect & General \\ 
    \midrule
    \textsc{SW-LOO} & 27.59 & 23.42 & 20.41 & 20.58 \\
    \multicolumn{1}{l}{\hspace{2pt} \textit{w/ Training Random}} & 24.24 & 24.70 & 19.75 & 18.71 \\
    \multicolumn{1}{l}{\hspace{2pt} \textit{w/ Inference Random}} & 23.46 & 22.04 & 18.76 & 19.41 \\
    \multicolumn{1}{l}{\hspace{2pt} \textit{w/ Both Random}} & 14.71 & 21.82 & 18.06 & 18.15 \\
    \midrule
    \textsc{NLI-LOO} & 25.92 & 25.13 & 19.21 & 19.32 \\
    \multicolumn{1}{l}{\hspace{2pt} \textit{w/ Training Random}} & 22.05 & 22.06 & 18.42 & 19.37 \\
    \multicolumn{1}{l}{\hspace{2pt} \textit{w/ Inference Random}} & 24.33 & 24.56 & 18.10 & 16.97 \\
    \multicolumn{1}{l}{\hspace{2pt} \textit{w/ Both Random}} & 16.14 & 22.50 & 17.83 & 19.69 \\
    \bottomrule
    \end{tabular}
}
\caption{\small \textit{Training Random} means randomly selecting sentences as pseudo summary and input during synthetic dataset construction. \textit{Inference Random} means randomly selecting sentences as input during inference. We report RL scores of our approaches on dev sets.}
\label{tab:ablation}
\end{table}


\subsection{Ablation Study}
We conduct ablation experiments with random filtering to study the importance of the filtering strategies in our two methods. We introduce randomness in two different phases. First, when constructing synthetic pairs, instead of using our filtering strategies before applying \textsc{LOO} construction, we randomly select sentences as pseudo-summary and input. This is essentially a random \textsc{LOO} baseline. Second, during inference, we sample random sentences to feed into \textsc{T5} encoder instead of using our filtering strategies to select aspect-related elements. Finally, we combine these two random strategies. Results in Table \ref{tab:ablation} show that our sentence filtering strategies are crucial since ROUGE scores drop drastically as more randomness is introduced. This is more severe for aspect summarization since aspect-specific synthetic dataset construction needs to focus on particular aspects. However, randomly selecting sentences is possible to cover most aspects by chance for general summarization.


