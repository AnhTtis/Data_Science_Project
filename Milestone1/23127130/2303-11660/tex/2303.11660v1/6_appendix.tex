\appendix

\section{Related Works}
Unsupervised opinion summarization is the task of summarizing opinionated text such as customer reviews without training on gold reviews-summary pairs. Recent works have been using autoencoders \cite{Kingma2014AutoEncodingVB} and synthetic datasets construction, or a mix of both, to tackle the zero-shot setting. 

An autoencoder model consists of an encoder that maps the input into latent embedding space and a decoder that reconstructs the original input from the latent space. The latent representation learned can be later aggregated or can be used to cluster and select text to perform both extractive and abstractive summarization. \citet{chu2019meansum, brazinskas-etal-2020-unsupervised} aggregate the input reviews latent representations by averaging then generate the summaries conditioned on it. \citet{angelidis-lapata-2018-summarizing} utilizes the latent representation with aspect specificity and sentiment polarity to guide the selection of review texts as extractive summaries. Recently, \citet{angelidis-etal-2021-extractive} proposes the first approach that generates both general and \textit{aspect-specific} opinion summaries in an extractive manner. They first leverage Vector-Quantized Variational Autoencoder \cite{van2017neural} to cluster review sentences and then use a popularity-driven extraction algorithm to summarize. Similar to \citet{angelidis-etal-2021-extractive}, \citet{basu-roy-chowdhury-etal-2022-unsupervised} learns representations of texts over latent semantic units using dictionary learning \cite{dumitrescu2018dictionary}. Other autoencoder-related methods include denoising autoencoder \cite{amplayo-lapata-2020-unsupervised} and \citet{coavoux-etal-2019-unsupervised}, an encoder-decoder architecture that utilizes clustering of encoding space to extract summaries.

Another direction of work creates synthetic datasets utilizing the largely available amount of online customer reviews. Synthetic datasets are usually constructed in a \textit{leave-one-out} (\textsc{LOO}) style that one review is first randomly sampled as a pseudo-summary, and then a subset of reviews are selected or generated as input reviews to be paired with the pseudo-summary to enable supervised training. Methods of selecting and generating input reviews include random sampling \cite{brazinskas-etal-2020-unsupervised}, generating noisy versions of the pseudo-summary \cite{amplayo-lapata-2020-unsupervised}, selecting reviews that have closer distribution with the pseudo-summary in the embedding space \cite{amplayo2021unsupervised, ke2022consistsum}, and selecting more textual similar reviews \cite{elsahar-etal-2021-self, brazinskas-etal-2022-efficient}. Recently, \citet{amplayo-etal-2021-aspect} proposes the first abstractive approach that can generate both general and aspect summaries. Their method build synthetic datasets by identifying aspect-specific elements with a multiple instance learning (MIL) model \cite{keeler1991self} using aspect seed words. Our work is closest to \citet{amplayo-etal-2021-aspect} in that we also build synthetic datasets by identifying aspect-specific elements, however, our methods do not require extra learning components such as MIL but achieve better performances.

Besides unsupervised opinion summarization, our second method, \textsc{NLI-LOO} is related to the recent approach \cite{yin-etal-2019-benchmarking} that utilizes NLI \cite{bowman-etal-2015-large, williams-etal-2018-broad} models to tackle zero-shot text classification \cite{Chang2008ImportanceOS} (multi-class and multi-label) problem such as topic detection \cite{NIPS2015_250cf8b5} and emotion detection \cite{bostan-klinger-2018-analysis}. The main idea is to solve the classification problem by casting the problem into NLI format. Specifically, the text to be classified becomes the premise, and class labels are converted into natural language format (verbalization) to be used as the hypothesis. If the text entails the verbalized class label, then the text belongs to this class. In our work, we identify the relatedness of a review sentence to an aspect in such a way to construct synthetic datasets.



\section{\textsc{SW-LOO} Details}
\label{sec:sw_loo_details}
For general synthetic pairs construction, after filtering each review with seed words for each aspect, we make sure to sample one review such that its aspect-related portions for all aspects are non-empty and concatenate them as pseudo-summary. We retrieve top similar filtered reviews to each aspect-related portion in the pseudo-summary and concatenate them as general synthetic input, and the retrieval process is the same as in aspect synthetic pairs construction. General synthetic input and output are both approximately $M$ times the length of those in aspect synthetic pairs where $M$ is the number of aspects. For the summarization model, \textit{\{aspect\}} and \textit{\{seed words\}} are the concatenation of all aspects and all seed words for general synthetic pairs. Finally, we train all synthetic pairs together.

At inference time, we also first filter each review into aspect-related portions. However, since there is no reference pseudo summary, we cannot truncate based on similarities to fit into \textsc{T5} encoder. We adopt the \textit{principle strategy} used in PEGASUS \cite{zhang2020pegasus} Gap Sentences Generation pretraining objective to select important reviews as input for inference. We show the effectiveness of adopting the principle strategy in Appendix \ref{sec:principle_strategy}.


\section{\textsc{NLI-LOO} Details}
\label{sec:nli_loo_details}
Different from \textsc{SW-LOO} where we use ROUGE-1 scores, we calculate similarities based on aspect entailment probabilities to rank and truncate aspect-related sentences as synthetic input. For aspect synthetic pairs, we simply calculate the absolute probability difference between pseudo summary and aspect-related sentences. For general synthetic pairs, each review sentence (no matter whether aspect-related) corresponds to a probability vector of dimension $M$ where $M$ is the number of aspects and each element is the probability of the sentence being related to each aspect, and we calculate cosine similarities between the probability vectors of pseudo summary and review sentences that are related to at least one aspect (sum of the probability vector is non-zero). We use the same token budget to truncate review sentences to fit into \textsc{T5} encoder for both aspect and general synthetic pairs. We also train all synthetic pairs together. Another way to calculate similarities is directly using cosine similarity between sentence embeddings, however, results reported in Appendix \ref{sec:nli_similarity} do not show better performance. 

During inference, we use $1$ and all-one vectors with dimension $M$ as reference vectors to rank and truncate review sentences for aspect and general input construction.


\section{Datasets Details}
\label{sec:dataset_details}
\textit{Hotel} reviews in \textsc{Space} are collected from TripAdvisor and each hotel in the evaluation sets is annotated with seven types of summaries: six aspect-specific and one general, with three gold summaries for each type. The number of reviews for a hotel in the raw corpus varies but each hotel in the evaluation sets comes with $100$ reviews. Product reviews from six domains: \textit{laptop bag}, \textit{Bluetooth headset}, \textit{boots}, \textit{keyboard}, \textit{television}, and \textit{vacuum} in \textsc{Oposum+} are initially down-sampled from \textit{Amazon Product Dataset}\footnote{\url{http://jmcauley.ucsd.edu/data/amazon/}} \cite{10.1145/2766462.2767755} by \citet{angelidis-lapata-2018-summarizing} and then further expanded by \citet{amplayo-etal-2021-aspect}. Each product in the evaluation sets is annotated with four types of summaries: three aspect-specific and one general, with also three gold summaries for each type. The number of reviews for a product in the raw corpus also varies but each product in the evaluation sets comes with $10$ reviews. All human-annotated summaries are abstractive except that general summaries in \textsc{Oposum+} are extractive. Detailed statistics of the datasets are shown in Table \ref{tab:datsets_stats}.


\begin{table}[t]
\centering
\hspace*{-0.5cm} 
\scalebox{0.85}{
    \begin{tabular}{@{}lcc@{}}
    \toprule
    Statistics & \textsc{Space} & \textsc{Oposum+} \\
    \midrule
    domain & 1 & 6 \\
    aspects per entity & 6 & 3 \\ 
    \midrule
    \textit{raw review corpus} & \\
    \hspace{8pt} entities & 11.40K & 95.55K \\
    \hspace{8pt} total reviews & 1.14M & 4.13M \\
    \midrule
    \textit{dev / test set} & \\
    \hspace{8pt} entities & 25 & 30 \\ 
    \hspace{8pt} reviews per entity & 100 & 10 \\
    \hspace{8pt} summaries per entity & 3 & 3 \\
    \hspace{8pt} total aspect summaries & 450 & 270 \\
    \hspace{8pt} total general summaries & 75 & \underline{90} \\
    \bottomrule
    \end{tabular}
}
\caption{\small Detailed statistic for \textsc{Space} and \textsc{Oposum+} datasets. Note that only gold general summaries for \textsc{Oposum+}, which is underlined in the table, are extractive.}
\label{tab:datsets_stats}
\end{table}



\begin{table}[t!]
\centering
\hspace*{-0.3cm} 
\scalebox{0.9}{
    \begin{tabular}{@{}lc@{}}
        \toprule[1.25pt]
        Aspect & Hotel \\
        \midrule
        \textsf{building} & lobby pool decor gym area \\
        \textsf{cleanliness} & clean spotless garbage dirty stain \\
        \textsf{food} & breakfast food buffet restaurant meal \\
        \textsf{location} & location walk station distance bus \\
        \textsf{rooms} & room bed bathroom shower spacious \\
        \textsf{service} & staff service friendly helpful desk \\
        \bottomrule[1.25pt]
    \end{tabular}
}
\caption{\small Seed words for \textit{hotel} domain in \textsc{Space} dataset.}
\label{tab:space_seed_words}
\vspace{1em}
\hspace*{-0.3cm} 
\scalebox{0.9}{
    \begin{tabular}{@{}lc@{}}
        \toprule[1.25pt]
        Aspect & Laptop Bag \\
        \midrule
        \textsf{looks} & looks color stylish looked pretty \\
        \textsf{quality} & quality material poor broke durable \\
        \textsf{size} & fit fits size big space \\
        \bottomrule[1.25pt]
        \\
        \toprule[1.25pt]
        Aspect & Boots \\
        \midrule
        \textsf{comfort} & comfortable foot hurt ankle comfy \\
        \textsf{looks} & cute look looked fringe style \\
        \textsf{size} & size half big little bigger \\
        \bottomrule[1.25pt]
        \\
        \toprule[1.25pt]
        Aspect & Bluetooth Headset \\
        \midrule
        \textsf{comfort} & ear fit comfortable fits buds \\
        \textsf{ease of use} & easy button simple setup control \\
        \textsf{sound quality} & sound quality hear noise volume \\
        \bottomrule[1.25pt]
        \\
        \toprule[1.25pt]
        Aspect & Keyboard \\
        \midrule
        \textsf{quality} & working months build stopped quality \\
        \textsf{comfort} & feel comfortable feels mushy shallow \\
        \textsf{layout} & key keys delete backspace size \\
        \bottomrule[1.25pt]
        \\
        \toprule[1.25pt]
        Aspect & TV \\
        \midrule
        \textsf{connectivity} & hdmi computer port usb internet \\
        \textsf{image quality} & picture color colors bright clear \\
        \textsf{sound quality} & sound speakers loud tinny bass \\
        \bottomrule[1.25pt]
        \\
        \toprule[1.25pt]
        Aspect & Vacuum \\
        \midrule
        \textsf{accessory} & filter brush attachments attachment turbo \\
        \textsf{ease of use} & easy push concerns awkward impossible \\
        \textsf{suction} & suction powerful power hair quiet \\
        \bottomrule[1.25pt]
    \end{tabular}
}
\caption{\small Seed words for various domains in \textsc{Oposum+} dataset.}
\label{tab:oposum_seed_words}
\end{table}



\section{List of Seed Words}
\label{sec:seed_words_lists}
Aspect seed words (listed in Table \ref{tab:space_seed_words} and \ref{tab:oposum_seed_words}) are usually automatically extracted using a variant of clarity scoring function \cite{CronenTownsend2002PredictingQP} applied on a small amount of aspect annotation as described in \citet{angelidis-lapata-2018-summarizing}, and they can be further manually improved by domain experts as in \citet{amplayo-etal-2021-aspect}.



\section{Baselines Details}
\label{sec:baseline_details}

\textbf{\textit{Extractive Approaches}} \hspace{4pt} We first compare against two traditional approaches: \textsc{Centroid} selects the review closest to the centroid of all reviews as the summary; \textsc{LexRank} selects the most salient review sentences as summary similar to \textsc{PageRank} \cite{Page1999ThePC}. \textsc{BERT} \cite{devlin-etal-2019-bert} embedding is used to represent sentences in both traditional methods. More recent systems include \textsc{QT} (described in Section \ref{sec:intro}) and \textsc{SemAE}. Inspired by \textsc{QT}, \textsc{SemAE} represents text over latent semantic units using dictionary learning. 

\noindent
\textbf{\textit{Abstractive Approaches}} \hspace{4pt} \textsc{MeanSum} generates summaries by reconstructing the mean of reviews' representations using autoencoder. \textsc{CopyCat} uses a hierarchical variational autoencoder to learn latent codes for the summaries. The most recent approach is \textsc{AceSum}. (described in Section \ref{sec:intro}).

Note that \textsc{LexRank}, \textsc{MeanSum}, and \textsc{CopyCat} do not support aspect-specific summary generation, \citet{amplayo-etal-2021-aspect} adopt a simple sentence-filtering strategy to enable it. Specifically, after training a general opinion summarization model, during inference for aspect summaries, they filter out input review sentences that are not aspect-related using cosine similarities scores between \textsc{BERT} embeddings of review sentences and aspect seed words before feeding into general summarization model.


\section{Datasets Pre-Processing}
\label{sec:datasets_preprocess}
We pre-process differently for our two methods on the same dataset since we want to control the constructed synthetic datasets to have reasonable sizes and resemble properties of test time data such as the number of reviews per product and average review length. We use dev sets to observe such properties. In \textsc{SW-LOO}, we first remove reviews with less than $20$ words and then remove hotels with less than $10$ reviews for \textsc{Space}; we first remove reviews less than $20$ or more than $100$ words then remove products with less than $12$ reviews for \textsc{Oposum+}. In \textsc{NLI-LOO}, we remove reviews with less than $10$ or more than $120$ words for \textsc{Space} and remove reviews with less than $20$ or more than $100$ words for \textsc{Oposum+}.



\begin{table}[t]
\centering
\hspace*{-0.5cm} 
\scalebox{0.85}{
    \begin{tabular}{@{}lrl@{}}
    \toprule[1.25pt]
    \multicolumn{3}{c}{\textsc{SW-LOO}} \\
    \midrule 
    \multirow{2}{*}{\textsc{Space}} & asp. & \texttt{lr=$3\mathrm{e}{-4}$, bch=$16$, bm=$2$} \\
    & gen. & \texttt{lr=$3\mathrm{e}{-4}$, bch=$16$, bm=$2$} \\
    \midrule
    \multirow{2}{*}{\textsc{Oposum+}} & asp. & \texttt{lr=$3\mathrm{e}{-4}$, bch=$16$, bm=$2$} \\
    & gen. & \texttt{lr=$1\mathrm{e}{-6}$, bch=$16$, bm=$2$} \\
    \midrule[1.25pt]
    \multicolumn{3}{c}{\textsc{NLI-LOO}} \\
    \midrule 
    \multirow{2}{*}{\textsc{Space}} & asp. & \texttt{lr=$4\mathrm{e}{-5}$, bch=$16$, bm=$2$} \\
    & gen. & \texttt{lr=$4\mathrm{e}{-5}$, bch=$16$, bm=$2$} \\
    \midrule
    \multirow{2}{*}{\textsc{Oposum+}} & asp. & \texttt{lr=$3\mathrm{e}{-4}$, bch=$8$, bm=$4$} \\
    & gen. & \texttt{lr=$1\mathrm{e}{-6}$, bch=$16$, bm=$2$} \\
    \midrule[1.25pt]
    \multicolumn{3}{c}{\textsc{SW-LOO\textsubscript{Ext}}} \\
    \midrule
    \multirow{2}{*}{\textsc{Space}} & asp. & \texttt{\textsc{BERT-Base}, n=$2$} \\
    & gen. & \texttt{\textsc{BERT-Base}, n=$2$} \\
    \midrule
    \multirow{2}{*}{\textsc{Oposum+}} & asp. & \texttt{\textsc{BERT-Base}, n=$2$} \\
    & gen. & \texttt{\textsc{BERT-Large}, n=$2$} \\
    \midrule[1.25pt]
    \multicolumn{3}{c}{\textsc{NLI-LOO\textsubscript{Ext}}} \\
    \midrule
    \multirow{2}{*}{\textsc{Space}} & asp. & \texttt{\textsc{BERT-Large}, n=$2$} \\
    & gen. & \texttt{\textsc{BERT-Base}, n=$4$} \\
    \midrule
    \multirow{2}{*}{\textsc{Oposum+}} & asp. & \texttt{\textsc{BERT-Large}, n=$4$} \\
    & gen. & \texttt{\textsc{BERT-Large}, n=$4$} \\
    \bottomrule[1.25pt]
    \end{tabular}
}
\caption{\small Best hyper-parameter settings on \textsc{Space} and \textsc{Oposum+} dev sets: \texttt{lr} stands for \texttt{AdamW} initial learning rate, \texttt{bch} stands for training batch size, and \texttt{bm} stands for beam search size at inference time.}
\label{tab:best_hyper_parameter}
\end{table}



\section{Implementation Details}
\label{sec:implementation_detials}
We use \textsc{T5} implementation from HuggingFace\footnote{\url{https://huggingface.co/docs/transformers/model_doc/t5}} \cite{wolf-etal-2020-transformers}. We use \texttt{AdamW} \cite{Loshchilov2019DecoupledWD} optimizer without weight decay and set $0.9$, $0.999$, $1 \times 10\textsuperscript{-8}$ for $\beta\textsubscript{1}$, $\beta\textsubscript{2}$, $\epsilon$. We train all summarization models for a total of $25$K steps on the combination of aspect and general synthetic pairs. We set ngram refraining size \cite{paulus2018a} to $3$ during inference. We tune initial learning rate in [$1\mathrm{e}{-6}$, $4\mathrm{e}{-5}$, $3\mathrm{e}{-4}$] and batch size in [$8$, $16$]. We tune beam search size during inference in [$2$, $4$]. For \textsc{SW-LOO\textsubscript{Ext}} and \textsc{NLI-LOO\textsubscript{Ext}}, we use \texttt{[CLS]} token embedding in the last layer of \textsc{BERT} as the sentence representation. We concatenate top $6$ sentences returned by \textsc{LexRank} as general summary, and tune in [$2$, $4$] for aspect summary. We also use two sizes of \textsc{BERT}: \textsc{BERT-Base} and \textsc{BERT-Large}. All computations are performed on 8-GPU p3.16xlarge Amazon instance. The best hyper-parameter settings for all experiments can be found in Table \ref{tab:best_hyper_parameter}.
% 8-GPU p3.16xlarge Amazon instance
% $8$ Tesla V100 GPU

During preliminary studies for aspect synthetic pairs construction, we find that for \textsc{Space}, using sampled filtered aspect-related review portion as pseudo-summary rather than the original review that contains the pseudo-summary gives better downstream ROUGE scores, but it is the opposite way with \textsc{Oposum+}. Please refer to Appendix \ref{sec:granularity} for analyses on pseudo-summary granularity. 

\smallskip
\noindent
\textbf{\textsc{SW-LOO}} \hspace{4pt} For \textsc{Space}, we add a linear learning rate warm-up in the first $500$ steps and save checkpoints every $500$ steps. Since there are totally $6$ aspects for \textsc{Space} and very few reviews containing seed words for all $6$ aspects can be sampled as pseudo summaries for general synthetic pairs construction, we relax the constraint of pseudo summaries containing seed words for all $6$ aspects to $4$ aspects. We set $200$ as the token budget to truncate ranked aspect-related review portions for aspect synthetic pairs construction, and $150$ as the token budget in principle strategy when selecting important sentences as input during inference for \textsc{Space}. We set $1536$ and $200$ as the maximum input and output token length of \textsc{T5} for all \textsc{SW-LOO} experiments. Notice that this exceeds $512$, which is the maximum token length that \textsc{T5} is pretrained on, but recent works \cite{zhang2020pegasus, rothe-etal-2021-thorough} have shown that seq2seq PLMs generalize well even when finetuned on longer sequences not observed at pretraining phase. For \textsc{Oposum+}, we add linear learning rate warm-up in the first $250$ steps. We set $300$ as the token budget to truncate for aspect synthetic pairs construction. There are $\sim50$K aspect and $\sim5$K general synthetic pairs for \textsc{Space}, $\sim70$K aspect and $\sim6$K general synthetic pairs for \textsc{Oposum+}.

\smallskip
\noindent
\textbf{\textsc{NLI-LOO}} \hspace{4pt} 
For \textsc{Space}, we add linear learning rate warm-up to first $1000$ steps, and $500$ steps for \textsc{Oposum+}. We set $0.9$ and $0.8$ as the entailment probability threshold for \textsc{Space} and \textsc{Oposum+} based on our preliminary experiments (lower thresholds make identified aspect-related sentences too noisy and further hurt downstream ROUGE scores). For summarization models, we set $500$ as the token budget for both aspect and general synthetic pairs construction for both datasets and set $512$ and $150$ for maximum input and output token length of \textsc{T5}. There are $\sim36$K aspect and $\sim6$K general synthetic pairs for \textsc{Space}, $\sim70$K aspect-specific and $\sim28$K general synthetic pairs for \textsc{Oposum+}. 



\begin{table}[t]
\centering
\scalebox{0.75}{
    \begin{tabular}{@{}lcccccc@{}}
    \toprule
     & \multicolumn{3}{c}{Aspect} & \multicolumn{3}{c}{General} \\
    \multicolumn{1}{c}{Model} & R1 & R2 & RL & R1 & R2 & RL \\ 
    \midrule
    \textsc{SW-LOO} & 33.11 & 10.98 & 27.59 & 40.26 & 12.04 & 23.42 \\
    \multicolumn{1}{l}{\hspace{2pt} \textit{w/o Prin. Sel.}} & 30.88 & 9.74 & 25.78 & 35.84 & 10.28 & 21.80 \\
    \bottomrule
    \end{tabular}
}
\caption{\small Randomly or using principle strategy to select aspect-related review portions in order to fit into the encoder of \textsc{T5}. Performances are reported on \textsc{Space} dev set.}
\label{tab:principle_selection}
\end{table}


\section{Principle Strategy Effectiveness}
\label{sec:principle_strategy}
Unlike \textsc{Oposum+}, there are $100$ reviews for each hotel in \textsc{Space} evaluation sets. During inference, we cannot simply concatenate all filtered reviews as input since they cannot fit into \textsc{T5} encoder. We adopt the principle strategy introduced in \textsc{Pegasus} to select the most important filtered reviews and concatenate them as input for inference. In Table \ref{tab:principle_selection}, we show the effectiveness of the principle strategy by comparing it with randomly selecting filtered reviews as input for inference. 


\begin{table}[t]
\hspace*{-0.3cm}
\centering
\scalebox{0.75}{
    \begin{tabular}{@{}c|lcccccc@{}}
    \toprule
    \multicolumn{2}{c}{} & \multicolumn{3}{c}{\textsc{Space}} & \multicolumn{3}{c}{\textsc{Oposum+}} \\
    \multicolumn{2}{c}{Model} & R1 & R2 & RL & R1 & R2 & RL \\ 
    \midrule
    \parbox[t]{0.8em}{\multirow{2}{*}{\rotatebox[origin=c]{90}{Asp.}}} & 
    \textsc{NLI-LOO} & 30.20 & 9.84 & 25.92 & 27.48 & 5.64 & 19.21  \\
     & \multicolumn{1}{l}{\hspace{2pt} \textit{w/ Sent. Sim.}} & 29.87 & 9.30 & 25.26 & 27.00 & 6.20 & 19.06 \\
    \midrule
    \parbox[t]{0.8em}{\multirow{2}{*}{\rotatebox[origin=c]{90}{Gen.}}} & 
    \textsc{NLI-LOO} & 41.17 & 12.34 & 25.13 & 31.10 & 10.09 & 19.32 \\
     & \multicolumn{1}{l}{\hspace{2pt} \textit{w/ Sent. Sim.}} & 25.01 & 9.68 & 17.34 & 31.11 & 10.43 & 19.86 \\
    \bottomrule
    \end{tabular}
}
\caption{\small Calculate cosine similarity using aspect entailment probability or sentence embeddings when constructing synthetic datasets in \textsc{NLI-LOO}. Performances are reported on dev sets for both datasets.}
\label{tab:nli_sent_emb}
\end{table}


\section{Similarity Metric}
\label{sec:nli_similarity}
Different from using aspect entailment probability, we can also use sentence embeddings \cite{reimers-gurevych-2019-sentence} to calculate the cosine similarity between pseudo summary and aspect-related review sentences to construct synthetic input. Specifically, we use \texttt{all-mpnet-base-v2}\footnote{\url{https://www.sbert.net/docs/pretrained_models.html}}, which is a sentence embedding model finetuned on a $1$B sentence pairs dataset with a self-supervised contrastive learning objective. Results in Table \ref{tab:nli_sent_emb} show that there is no significant difference except general summarization for \textsc{Space} where using sentence embeddings is much worse than using aspect entailment probability.


\begin{table}[t]
\hspace*{-0.5cm}
\centering
\scalebox{0.75}{
    \begin{tabular}{@{}lcccccc@{}}
    \toprule
     & \multicolumn{3}{c}{Aspect} & \multicolumn{3}{c}{General} \\
    \multicolumn{1}{c}{Granularity} & R1 & R2 & RL & R1 & R2 & RL \\ 
    \midrule
    \textsc{Space} & \\
    \multicolumn{1}{l}{\hspace{2pt} Sentence} & 33.11 & 10.98 & 27.59 & 40.26 & 12.04 & 23.42 \\
    \multicolumn{1}{l}{\hspace{2pt} \textit{Review}} & 25.01 & 6.42 & 18.04 & 39.86 & 11.21 & 23.07 \\
    \midrule
    \textsc{Oposum+} & \\
    \multicolumn{1}{l}{\hspace{2pt} Review} & 29.18 & 6.38 & 20.41 & 36.16 & 11.89 & 20.58 \\
    \multicolumn{1}{l}{\hspace{2pt} \textit{Sentence}} & 22.34 & 5.06 & 17.33 & 20.06 & 6.76 & 13.86 \\
    \bottomrule
    \end{tabular}
}
\caption{\small Pseudo summary granularity study for \textsc{SW-LOO} and \textsc{NLI-LOO}. Performances are reported on dev sets. Note that in our main experiments, we use sentence level pseudo summary for \textsc{Space} and review level for \textsc{Oposum+}}
\label{tab:granularity_study}
\end{table}



\section{Pseudo Summary Granularity}
\label{sec:granularity}
We use different pseudo-summary granularity for two datasets: sentence level for \textsc{Space} and review level for \textsc{Oposum+}. Sentence level directly uses sampled filtered aspect-related review portion (\textsc{SW-LOO}) or sampled aspect-related review sentence (\textsc{NLI-LOO}) as pseudo-summary, and review level uses the original review that contains the sampled pseudo-summary as pseudo-summary. Results in Table \ref{tab:granularity_study} show the importance of design choices for synthetic datasets construction. 


\begin{table}[t]
\hspace*{-0.4cm}
\centering
\scalebox{0.75}{
    \begin{tabular}{@{}c|lcccccc@{}}
    \toprule
    \multicolumn{2}{c}{} & \multicolumn{3}{c}{\textsc{Space}} & \multicolumn{3}{c}{\textsc{Oposum+}} \\
    \multicolumn{2}{c}{Model} & R1 & R2 & RL & R1 & R2 & RL \\ 
    \midrule
    \parbox[t]{0.8em}{\multirow{8}{*}{\rotatebox[origin=c]{90}{Aspect Summary}}} & 
    \textsc{\small SW-LOO} &  \\
     & \multicolumn{1}{l}{\hspace{2pt} \textsc{T5-small}} & 33.11 & 10.98 & 27.59 & 29.18 & 6.38 & 20.41 \\
     & \multicolumn{1}{l}{\hspace{2pt} \textsc{T5-base}} & 33.43 & 11.08 & 27.73 & 30.03 & 6.60 & 20.53 \\
     & \multicolumn{1}{l}{\hspace{2pt} \textsc{T5-large}} & 33.70 & 10.77 & 27.60 & 28.98 & 6.15 & 20.32 \\
    \cline{2-8}
     & \textsc{\small NLI-LOO} &  \\
     & \multicolumn{1}{l}{\hspace{2pt} \textsc{T5-small}} & 30.20 & 9.84 & 25.92 & 27.48 & 5.64 & 19.21 \\
     & \multicolumn{1}{l}{\hspace{2pt} \textsc{T5-base}} & 30.24 & 10.04 & 25.95 & 27.58 & 5.28	& 19.29 \\
     & \multicolumn{1}{l}{\hspace{2pt} \textsc{T5-large}} & 30.61 & 9.50 & 25.68 & 27.14 & 5.47 & 19.55 \\
    \midrule
    \parbox[t]{0.8em}{\multirow{8}{*}{\rotatebox[origin=c]{90}{General Summary}}} & 
    \textsc{\small SW-LOO} &  \\
     & \multicolumn{1}{l}{\hspace{2pt} \textsc{T5-small}} & 40.26 & 12.04 & 23.42 & 36.16 & 11.89 & 20.58 \\
     & \multicolumn{1}{l}{\hspace{2pt} \textsc{T5-base}} & 41.31 & 12.47 & 23.12 & 35.53 & 11.65 & 20.33 \\
     & \multicolumn{1}{l}{\hspace{2pt} \textsc{T5-large}} & 39.90 & 10.94 & 22.64 & 32.96 & 10.24 & 19.41 \\
    \cline{2-8}
     & \textsc{\small NLI-LOO} &  \\
     & \multicolumn{1}{l}{\hspace{2pt} \textsc{T5-small}} & 41.17 & 12.34 & 25.13 & 31.10 & 10.09 & 19.32 \\
     & \multicolumn{1}{l}{\hspace{2pt} \textsc{T5-base}} & 37.49 & 11.44 & 22.91 & 26.51 & 6.74	& 17.08 \\
     & \multicolumn{1}{l}{\hspace{2pt} \textsc{T5-large}} & 37.57 & 10.14 & 21.77 & 30.41 & 6.77 & 17.37 \\
    \bottomrule
    \end{tabular}
}
\caption{\small Using different \textsc{T5} sizes as summarization model. Performances are reported on dev sets for both datasets.}
\label{tab:t5_sizes}
\end{table}


\section{\textsc{T5} Model Sizes}
\label{sec:t5_size}
We use different \textsc{T5} sizes including \textsc{T5-Small}, \textsc{T5-Base}, and \textsc{T5-Large} as summarization models. Results in Table \ref{tab:t5_sizes} show that larger summarization models do not necessarily guarantee better downstream ROUGE scores and sometimes even hurt downstream performances. Our hypothesis is that larger models overfit synthetic datasets and thus perform slightly worse on downstream evaluation sets.


 

% \section{Example Summaries}
% \label{sec:example_summaries}
% We show example aspect-specific and general summaries generated by \textsc{SW-LOO} and \textsc{NLI-LOO} on \textsc{Space} and \textsc{Oposum+} in Table \ref{tab:space_summaries} and Table \ref{tab:oposum_summaries}.



\begin{table*}[th!]
\small
\centering
\scalebox{1}{
    \begin{tabularx}{\textwidth}{X}
    \toprule[1.5pt]
    \multicolumn{1}{c}{\textsc{SW-LOO} Summaries} \\
    \midrule
    \textbf{\normalsize Building} \hspace{6pt} The pool area was very nice and the room was clean and comfortable.\\
    \midrule
    \textbf{\normalsize Cleanliness} \hspace{6pt} Our room was very clean and comfortable. \\
    \midrule
    \textbf{\normalsize Food} \hspace{6pt} The breakfast was great and the staff was very helpful and helpful. \\
    \midrule
    \textbf{\normalsize Location} \hspace{6pt} The hotel is located right next to the main road and is a short walk from the beach. \\
    \midrule
    \textbf{\normalsize Rooms} \hspace{6pt} The room was very clean and comfortable. \\
    \midrule
    \textbf{\normalsize Service} \hspace{6pt} The staff was very friendly and helpful. \\
    \midrule
    \textbf{\normalsize General} \hspace{6pt} The pool was very nice and clean. We were able to walk to the beach and Duval st. from the hotel, so we had a nice view of the harbor and the sea! The breakfast was great and we stayed in October and were very pleased with the location - right next to all the restaurants ... The room was small but very small and very comfortable with clean and comfortable beds. \\
    \bottomrule[1.5pt]
    \\
    \toprule[1.5pt]
    \multicolumn{1}{c}{\textsc{NLI-LOO} Summaries} \\
    \midrule
    \textbf{\normalsize Building} \hspace{6pt} The hotel is a beautiful old hotel. \\
    \midrule
    \textbf{\normalsize Cleanliness} \hspace{6pt} The room was clean and the staff was very helpful. \\
    \midrule
    \textbf{\normalsize Food} \hspace{6pt} The breakfast was great and the view from the rooftop was amazing. \\
    \midrule
    \textbf{\normalsize Location} \hspace{6pt} The location is great - just a short walk to the Spanish Steps and the metro station. \\
    \midrule
    \textbf{\normalsize Rooms} \hspace{6pt} The rooms are small by European standards, but very clean and comfortable. \\
    \midrule
    \textbf{\normalsize Service} \hspace{6pt} the service was excellent and the staff was very friendly and helpful. \\
    \midrule
    \textbf{\normalsize General} \hspace{6pt} The hotel is very clean and the staff is friendly and helpful. The room was very small and clean, but the bathroom was a bit small compared to the other rooms in the UK. It is OK to stay here again. I would stay there again if you want to go back to Europe! The location is great - the city is just ten minutes walk from the metro station andn't be disappointed with the price of the rooms. \\
    \bottomrule[1.5pt]
    \end{tabularx}
}
\caption{\small \textit{General} and \textit{aspect-level} summaries for a hotel in \textsc{Space} dataset generated by \textsc{SW-LOO} and \textsc{NLI-LOO}}
\label{tab:space_summaries}
\end{table*}


\begin{table*}[th!]
\small
\centering
\scalebox{1}{
    \begin{tabularx}{\textwidth}{X}
    \toprule[1.5pt]
    \multicolumn{1}{c}{\textsc{SW-LOO} Summaries} \\
    \midrule
    \textbf{\normalsize Sound Quality} \hspace{6pt} I love this headset. It's a great product, but it doesn't have any issues with the sound! It is OK if you are looking for something that can be used for your Samsung TV? \\
    \midrule
    \textbf{\normalsize Comfort} \hspace{6pt} I love this headset. It's a great headset for the price, but it doesn't fit my ear perfectly! \\
    \midrule
    \textbf{\normalsize Ease of Use} \hspace{6pt} I bought this for my Motorola. It is very easy to set up, and the buttons are very comfortable! \\
    \midrule
    \textbf{\normalsize General} \hspace{6pt} I haven't found any way of getting that to be consistently good. The earpieces are not as sturdy or high quality in material as a Motorola, but the buttons are quite accessible and the sound varies based on how it's fitting into my ears! The set is very comfortable and has great range ( roughly 100 feet ) and connects easily to my iPhone with me - but it is not too big for me to wear if it doesnâ€™t fit my TV. \\
    \bottomrule[1.5pt]
    \\
    \toprule[1.5pt]
    \multicolumn{1}{c}{\textsc{NLI-LOO} Summaries} \\
    \midrule
    \textbf{\normalsize Sound Quality} \hspace{6pt} I love these headphones. They are very comfortable, sound quality is good and they're very good quality for the price! \\
    \midrule
    \textbf{\normalsize Comfort} \hspace{6pt} I love these headphones. They are very comfortable, and the sound quality is great! They're a little tight on my ears but if you aren't sure how long they will last you... \\
    \midrule
    \textbf{\normalsize Ease of Use} \hspace{6pt} I bought these for my Motoactv. They are very comfortable to wear, and they don't touch my neck at all! \\
    \midrule
    \textbf{\normalsize General} \hspace{6pt} I bought these headphones in a package for the Motoactv. They are very comfortable, the neck band doesn't touch my neck at all allowing for free movement! The sound is very good and fits comfortably in my ears... but it takes some time to find the right angel and fit it right in. \\
    \bottomrule[1.5pt]
    \end{tabularx}
}
\caption{\small \textit{General} and \textit{aspect-level} summaries for a product in "Bluetooth Headset" domain of \textsc{Oposum+} dataset generated by \textsc{SW-LOO} and \textsc{NLI-LOO}.}
\label{tab:oposum_summaries}
\end{table*}



