\section{Synthetic Dataset Construction}

\smallskip
\noindent
\textbf{Leave-One-Out (\textsc{LOO})} \hspace{4pt} 
We construct synthetic datasets in a \textsc{LOO} style: from a pool of review elements (reviews or review sentences), an element is randomly sampled as a \textit{pseudo-summary}, then we select input reviews from the remaining review elements.

\subsection{Seed Words Based \textsc{LOO}}
\label{sec:sw_loo}
% To build a synthetic reviews-summary pair for aspect $a$, we first filter each review $r$ into its aspect-related portion $r^{\prime}$ where $r^{\prime} \subseteq \{x_1, x_2, \dots\}$ with each sentence in $r^{\prime}$ containing at least one seed word in $A_e$. Then we apply \textsc{LOO} construction on the filtered aspect-related review portions $\{ r_1^{\prime}, r_2^{\prime}, \dots \}$. We rank filtered reviews based on ROUGE-1 score \cite{lin-2004-rouge} with pseudo-summary and then truncate with a token budget $j$ (truncate up to $j$ tokens) since a concatenation of all filtered reviews cannot fit into the encoder of a PLM. 
To build a synthetic reviews-summary pair for aspect $a$, as shown in the upper diagram of Figure \ref{fig:methods_figure}, we first filter each review $r$ into its aspect-related portion $r^{\prime}$ where $r^{\prime} \subseteq \{x_1, x_2, \dots\}$ with each sentence in $r^{\prime}$ containing at least one seed word in $A_e$. For example, for \textit{food} aspect with its seed words \{\textit{breakfast}, \textit{buffet}, ...\}, a hotel review $r_i^{\prime}$: \textit{"They have the most wonderful buffet in Bay Area. And the hotel is close to the airport. Forgot to mention, especially the breakfast is terrific."} will be filtered into its aspect-related review portion $r_i^{\prime}$: \textit{"They have the most wonderful buffet in Bay Area. Forgot to mention, especially the breakfast is terrific."}. Noticed that $r_2^{\prime}$ is empty suppose there is no sentence in $r_2$ containing any seed word. 
% Then we apply \textsc{LOO} construction on the filtered aspect-related review portions $\{ r_1^{\prime}, r_2^{\prime}, \dots \}$. 
Then we apply \textsc{LOO} construction on the filtered aspect-related review portions $\{ r_1^{\prime}, r_3^{\prime}, \dots, r_P^{\prime}\}$ as shown in the diagram: $r_i^{\prime}$ is randomly sampled as the pseudo summary and inputs are chosen from $\{ r_1^{\prime}, r_3^{\prime}, \dots, r_P^{\prime}\} \setminus \{r_i^{\prime}\}$ by first ranking them with the pseudo-summary $r_i^{\prime}$ based on ROUGE-1 score \cite{lin-2004-rouge} and then truncating with a token budget $j$ (truncate up to $j$ tokens) since a concatenation of all filtered reviews cannot fit into the encoder of a PLM. 
% We rank filtered reviews based on ROUGE-1 score \cite{lin-2004-rouge} with pseudo-summary and then truncate with a token budget $j$ (truncate up to $j$ tokens) since a concatenation of all filtered reviews cannot fit into the encoder of a PLM. 
Please refer to Appendix \ref{sec:sw_loo_details} for more details and analysis on \textsc{SW-LOO}.


\begin{figure}[t!]
\centering
\hspace*{-0.23cm} 
\includegraphics[width=1.025\linewidth]{methods.pdf}
\caption{\small One synthetic data pair construction for aspect $a$ in \textsc{SW-LOO} and \textsc{NLI-LOO}.} 
\label{fig:methods_figure}
\end{figure}


\subsection{NLI Based \textsc{LOO}}
% \smallskip
% \noindent
% \paragraph{NLI Component} 
%Inspired by recent success in zero-shot text classification problem \cite{yin-etal-2019-benchmarking}\shuai{let's motivate it from the task itself - this sounds like this approach is with weak motivation and lack of novelty}, 
\textbf{NLI Component} \hspace{4pt} In order to relax the requirement of aspect seeds (provided by humans) and to make a more scalable and general solution, we propose to use an NLI model to infer whether a review sentence is related to an aspect. Specifically, we set a review sentence as the premise and verbalize an aspect with the template: \textit{the text is about \{aspect\}}, which we use as the hypothesis. If the entailment probability is higher than a threshold ($0.9$ for \textsc{Space} and $0.8$ for \textsc{Oposum+}), we identify the sentence as related to the aspect with this entailment probability, else we set the aspect-related probability to $0$.

To build a synthetic pair for aspect $a$, we first break all reviews into review sentences and filter out those that are not related to aspect $a$ with the NLI model. As shown in the lower diagram of Figure \ref{fig:methods_figure}, each sentence is first passed through the NLI model to infer its probability of relatedness to aspect $a$, so $s_2$ with entailment probability of $0.45$ will be filtered out if the threshold is set to $0.5$. 
Then we apply \textsc{LOO} construction on all aspect-related sentences $\{ s_1, s_3, \dots, s_Q\}$ and we also use a token budget to truncate ranked synthetic input similar to \textsc{SW-LOO}, however, different from \textsc{SW-LOO} where we use ROUGE-1 scores to rank, we calculate similarities based on entailment probabilities. Please refer to Appendix \ref{sec:nli_loo_details} for more details and analysis on \textsc{NLI-LOO}. Note that we filter the input reviews at sentence level for \textsc{NLI-LOO} and at review level in \textsc{SW-LOO}.





\section{Summarization Model} 
We use \textsc{T5} \cite{raffel2020exploring}, a sequence-to-sequence Transformer-based \cite{NIPS2017_3f5ee243} PLM, to finetune our synthetic datasets similar to previous works \cite{ke2022consistsum, amplayo-etal-2021-aspect}. For \textsc{SW-LOO}, we use the following template: \texttt{``summarize based on aspect: [ASPECT]} \textit{\{aspect\}} \texttt{[ASPECT] with seed words: [SEED]} \textit{\{seed words\}} \texttt{[SEED]:} \textit{\{filtered review\}} \texttt{[SEP]} \textit{\{filtered review\}} $\dots$ \texttt{''} to convert synthetic input and for \textsc{NLI-LOO}, we use: \texttt{``[ASPECT]} \textit{\{aspect\}} \texttt{[SEP]} \textit{\{aspect-related sent\}} \texttt{[SEP]} \textit{\{aspect-related sent\}} $\dots$ \texttt{''}. \texttt{[ASPECT]}, \texttt{[SEED]}, and \texttt{[SEP]} are special tokens, \textit{\{aspect\}} is an aspect name, \textit{\{seed words\}} are concatenation of seed words for an aspect, each \textit{\{filtered review\}} is a $r_i^{\prime}$ in \textsc{SW-LOO} synthetic input, and each \textit{\{aspect-related sent\}} is a $s_k$ in \textsc{NLI-LOO} synthetic input. For both methods, outputs are pseudo summaries. 










