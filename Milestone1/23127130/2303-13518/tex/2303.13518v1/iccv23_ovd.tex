\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{iccv}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}

%
\input{includes.tex}

%
%
%
\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}

\isArXiv{
\iccvfinalcopy
}{}

\def\iccvPaperID{1802}  %
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

%
\isArXiv{
}{
\ificcvfinal\pagestyle{empty}\fi
}

\begin{document}

%
\def\fulltitle{Three ways to improve feature alignment for open vocabulary detection}
\title{\fulltitle}
\isArXiv{
\hypersetup{pdfauthor={Relja ArandjeloviÄ‡},pdftitle={\fulltitle}}
}
{
\hypersetup{pdfauthor={\iccvPaperID},pdftitle={\iccvPaperID: \fulltitle}}
}

\author{%
Relja Arandjelovi\'c\textsuperscript{1,*}
\quad
Alex Andonian\textsuperscript{2,$^*$,$^\circ$}
\\
Arthur Mensch\textsuperscript{1}
\quad
Olivier J.\ H\'enaff\textsuperscript{1}
\quad
Jean-Baptiste Alayrac\textsuperscript{1}
\quad
Andrew Zisserman\textsuperscript{1,3}
\\[0.5em]
$^1$DeepMind \quad {$^2$MIT} \quad {$^3$VGG, Dept.\  of Engineering Science, University of Oxford}
}
%

\maketitle
\isArXiv{
}{
%
\ificcvfinal\thispagestyle{empty}\fi
}

\input{floats.tex}


%
\begin{abstract}
The core problem in zero-shot open vocabulary detection is how to align
visual and text features, so that the detector performs well on unseen
classes. Previous approaches train the feature pyramid and detection head
from scratch, which breaks the vision-text feature alignment established
during pretraining, and struggles to prevent the language model from
forgetting unseen classes.

We propose three methods to alleviate these issues. Firstly, a simple
scheme is used to augment the text embeddings which prevents overfitting
to a small number of classes seen during training, while simultaneously
saving memory and computation. Secondly, the feature pyramid network and
the detection head are modified to include trainable gated shortcuts,
which encourages vision-text feature alignment and guarantees it at
the start of detection
training. Finally, a self-training approach is used to leverage a larger
corpus of image-text pairs thus improving detection performance on classes
with no human annotated bounding boxes.

Our three methods are evaluated on the zero-shot version of the LVIS benchmark,
each of them showing clear and significant benefits. Our final network
achieves the new state-of-the-art on the mAP-all metric and demonstrates
competitive performance for mAP-rare,
as well as superior transfer to COCO and Objects365.
\vspace{-0.4cm}
\end{abstract}

\isArXiv{
{\let\thefootnote\relax\footnotetext{$^*$Equal contribution. $^\circ$Work done during internship at DeepMind.}}
}{}


%
\section{Introduction}
\label{sec:intro}

Traditional closed vocabulary detection is limited to a fixed set of
predetermined classes, and does not satisfactorily address user needs
-- imagine Google where you are only able to search for
a predefined list of terms.
Adding support for more terms requires large and costly
annotation efforts, which is simply not scalable.
Our objective in this paper is zero-shot open vocabulary detection, where the task is to
detect any object the user queries for, in a form of a textual query
(\eg ``Gargoyle''; Figure~\ref{fig:teaser}),
even if it has not been seen during training.

The common approach to building an open vocabulary detector is to
borrow heavily from the design of standard closed vocabulary detectors
(\ie detectors capable of detecting only a fixed set of predetermined classes),
and simply modify the bounding box classification procedure.
Instead of producing the logits for the fixed set of classes
via a fully connected layer, the score for the textual query is
obtained via a scalar product between its embedding,
produced by a language model,
and the image region embedding, produced by the detector head.
The zero-shot capability strongly relies on a good alignment between
visual and textual representations --
the only way queries not seen during training can be detected successfully
is if the vision-text alignment holds even beyond the seen classes.
%
In this work, we explicitly consider feature alignment
and devise three ways of improving it:

(i) Many works~\cite{zareian2020openvocabulary,zhou2022detecting,kuo2023fvlm}
choose to freeze the pretrained language model, while others,
observe this yielding bad performance~\cite{minderer2022simple,li2022glip,zhang2022glipv2}
and choose to train it, but with a small learning rate
to prevent ``catastrophic forgetting''. We also find that a frozen language model alone yields poor performance (Section~\ref{sec:aug:res}), but propose instead
to use the frozen language model together with a simple and
efficient data augmentation approach, which provides superior results to
both alternatives while speeding up training and decreasing accelerator
memory consumption.

\figTeaser
\figSetup

(ii) A typical detector pretrains the vision backbone
and language model on image-text datasets to obtain aligned image and text embeddings~\cite{zhou2022detecting,feng2022promptdet,gu2022openvocabulary,minderer2022simple},
but also inserts many modules
(feature pyramid network~\cite{lin2017feature},
detection heads~\cite{lin2017feature,tian2019fcos,feng2021tood})
that are trained from scratch.
The added modules break the vision-text alignment established during pretraining,
and we propose to side-step this issue by modifying their architecture.
Explicitly, we add shortcuts and trainable gating layers which ensure the features
are aligned at the start of detector training, and promote alignment
throughout the training.

(iii) Feature alignment that can be achieved from relatively scarce
detection training data is sparse and limited. The alignment can be improved by
making use of readily available large-scale image-text data
through a self-training approach~\cite{xie2020selftraining,lee2013pseudolabel,sohn2020simple,zoph2020rethinking}.
We examine self-training
via pseudo-labelling in detail and observe it is crucial
to use batch-negatives.

Our final approach based on all three improvements achieves the best
\APall on the challenging \LVISminusrare benchmark, beating the next method
by more than 9\% points, while achieving very competitive zero-shot
results and superior transfer to COCO and Objects365.

%
%


%
\subsection{Related work}

\paragraph{Zero-shot open vocabulary detection.}
Zero-shot (ZS) in the context of object detection
refers to never seeing even a single
annotated bounding box of the class of interest during training~\cite{minderer2022simple};
note that this definition allows for the existence of the object in the
training set images as long as no annotations are associated with it,
and it permits weak supervision, \eg an image-text dataset
where the object is mentioned in the text can be used as long as no bounding boxes are provided.
There is a large overlap in the ZS and open vocabulary (OV) approaches,
so, confusingly, the terms are often used interchangeably, which we avoid here.

Bansal \etal~\cite{bansal2018zeroshot} introduce ZS+OV detection
where the classification layer of a closed vocabulary detector
is replaced with the text embeddings of the class names,
an approach taken by many subsequent works~\cite{zhou2022detecting,du2022learning,minderer2022simple,zareian2020openvocabulary,gu2022openvocabulary,zhou2022detecting,kuo2023fvlm,feng2022promptdet},
including this one.
Some works~\cite{zareian2020openvocabulary,gu2022openvocabulary,kuo2023fvlm}
take the OV classification closer to the
backbone features by directly extracting them from object proposals
with ROI-Align~\cite{he2017mask}, and optionally distill a strong
OV classifier into the detector~\cite{gu2022openvocabulary}.
To improve ZS performance, Detic~\cite{zhou2022detecting} and PromptDet~\cite{feng2022promptdet} forego the OV aspect --
knowing the names of the classes of interest
(\ie in evaluation: test classes)
already during training
enables them to obtain high-quality weak labels,
and thus improve the detection performance for those classes.

\paragraph{Self-training} is often used in the weakly- and semi-supervised
settings to improve the low-shot performance of a detector~\cite{redmon2017yolo9000,sohn2020simple,zoph2020rethinking,ramanathan2020dlwl},
by first training a detector, followed by using it to pseudo-label additional
images, which are in turn used to train a better detector.
This has been adapted by Detic~\cite{zhou2022detecting}
to the ZS scenario, who argue for using region proposals
rather than the detector outputs to perform the pseudo-labelling.
Motivated by self-supervision and contrastive learning~\cite{oord2018representation,chen2020simple,arandjelovic2017look,he2020momentum,radford2021learning,alayrac2022flamingo},
we show that using batch-negatives is crucial for obtaining good performance
in self-training as well.

\paragraph{Pretraining-preserving init.}
The seminal ResNet paper~\cite{He16} showed the importance of shortcuts
for signal propagation during training, while
SkipInit~\cite{de2020batch} introduced a learnt gating that further encourages
identity functions. We take most inspiration from
the trainable gating of Flamingo~\cite{alayrac2022flamingo}
where the vision-language model is initialized such that the visual branch
is ignored, thus preserving the language model pretraining.
FIBER~\cite{dou2022coarsetofine} uses the Flamingo-style gating
to initialize a joint vision-text encoder with pretrained dual encoders.
We instead aim to preserve alignment between visual and language
features obtained during pretraining but broken due to the injected
detection-specific modules.

%


%
\section{Baseline detector and experimental setup}
\label{sec:setup}

In this section, we describe the baseline
open vocabulary detector (Figure~\ref{fig:setup}),
that we build and improve upon in Section~\ref{sec:rec}.
We also specify the main benchmark with some implementation details,
while the full details are available in \isArXiv{Appendix~\ref{sec:app:imp}.}{the supplementary material.}


%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%


\paragraph{Open vocabulary detector.}
We follow the design of the single-stage FCOS detector~\cite{tian2019fcos},
illustrated in Figure~\ref{fig:setup}.
%
It starts by processing the image with a vision backbone,
features from different blocks are then passed to the feature pyramid network
(FPN~\cite{lin2017feature}),
followed by the application of detection heads (parameters shared across levels);
we use the T-Head~\cite{feng2021tood} but also experiment with the classic
FCOS head~\cite{tian2019fcos}.
Each head produces dense detections associated with three quantities:
bounding box coordinates, quality, and classification features.
In line with other ZS/OV approaches~\cite{bansal2018zeroshot,zhou2022detecting,zareian2020openvocabulary,gu2022openvocabulary},
the classification features are dotted with the embedding of the query text,
obtained via a language model (LM),
producing the classification logits for the given query.
The final scores for all dense detections are computed by multiplying the classification probabilities with the quality scores.
Non-maximum suppression~\cite{felzenszwalb10object} is then applied to produce the final detections.
Training follows the standard FCOS method and its improvements,
\ie
the dense predictions are assigned to a ground truth box or deemed
as a negative through ATSS~\cite{zhang2020bridging},
and the classification, bounding box prediction, and
quality prediction branches
use the focal~\cite{lin2017focal}, gIoU~\cite{rezatofighi2019generalized}
and IoU prediction~\cite{wu2020iou} losses, respectively.

Free form textual queries are naturally supported,
while it is still possible
to detect a desired object class as
the query text for that class (hereafter also referred to as the ``class embedding'')
can be produced by populating
the default template (``A photo of a \emph{\{object\}}'')
with the class name.


%
%
%
%


\paragraph{Zero-shot benchmark.}
We use the LVIS v1.0~\cite{gupta2019lvis} object detection benchmark
adapted for zero-shot evaluation; we call this setup \LVISminusrare.
Following standard practice~\cite{gu2022openvocabulary,zhou2022detecting,kuo2023fvlm},
the \emph{rare} class annotations are removed from the training set, keeping
only the \emph{frequent} and \emph{common} annotations (often called LVIS-base).
Evaluation is then performed on all classes, reporting the
box mAP for all classes (\APall) and the mAP on rare classes (\APrare),
with the emphasis on \APrare as this measures the zero-shot performance,
\emph{rare} classes playing the role of the \emph{unseen} classes.
%
%
%
%
As is best practice~\cite{lvisbestpractices}, we run all experiments with three
different random seeds and report the mean and standard deviation.


\paragraph{Implementation details.}
The vision backbone and the language model are pretrained contrastively
on the ALIGN~\cite{jia2021scaling} and LTIP datasets~\cite{alayrac2022flamingo}
as in~\cite{alayrac2022flamingo},
while the FPN and the detector head are initialized
from scratch.
We follow a standard training procedure for \LVISminusrare
and tune the hyper-parameters to maximize the baseline performance;
full details are listed in \isArXiv{Appendix~\ref{sec:app:imp}.}{the supplementary material.}
With the NFNet-F0~\cite{brock2021highperformance} backbone
we achieve an \APall of \meanstd{32.1}{0.31}, and
\APrare of \meanstd{18.9}{1.13}.
This is a strong baseline,
as for example the baseline used in a recent work~\cite{zhou2022detecting}
achieves \meanstd{30.0}{0.4} and \meanstd{16.3}{0.7}, respectively.


%
\section{Three paths to alignment}
\label{sec:rec}

In this section we describe three complementary methods for improving
vision-text alignment,
starting from efficient text augmentation which alleviates overfitting
and facilitates large scale training (Section~\ref{sec:aug}),
followed by an architectural modification that preserves and promotes the alignment
(Section~\ref{sec:init}),
and ending with an approach for self-training which further improves
the detection performance on unseen classes (Section~\ref{sec:selft}).


%
\subsection{Efficient text augmentation}
\label{sec:aug}

When training a zero-shot detector, a difficult choice has to be made
whether to train or to freeze the language model (LM).
Many works, such as OVD~\cite{zareian2020openvocabulary}, Detic~\cite{zhou2022detecting}
and F-VLM~\cite{kuo2023fvlm},
follow the natural intuition to freeze it --
the language model learnt a comprehensive textual representations during
pretraining, and fine-tuning for detection on a small number of classes
could make it forget about the unseen classes~\cite{minderer2022simple}.
However, freezing it also comes with downsides --
the vision model is ``forced'' into the language-model ``mould''
making it less able to adapt to the task change from
pretraining which only involved global image understanding.
Multiple works, such as OWL~\cite{minderer2022simple}, GLIP~\cite{li2022glip}, GLIPv2~\cite{zhang2022glipv2},
do train the language model as well, but typically use a smaller learning
rate in order to prevent ``catastrophic forgetting'', \eg OWL~\cite{minderer2022simple}
sets it to $1/100$ of the vision model learning rate
and notes poor performance when the language model is frozen.

In fact, experimentally we find that the main issue behind the poor
detection performance of a system with a frozen language model is
overfitting of the visual representations to the small fixed set of
textual embeddings corresponding to the training classes.
Augmenting of the class embeddings during training can be used as an
effective way of alleviating these issues, and we consider two alternatives:

\noindent (i) \emph{Freeze + Dropout}: despite freezing the LM, enable the
dropout (commonly present in Transformer-based LMs during training).

\noindent (ii) \emph{Variants}: precompute 64 variants of the class embeddings
by using (i) and randomly sample a variant for each training sample.

Freezing the LM makes the training faster and simultaneously saves memory
due to not having to perform backpropagation or keep the optimizer state
(\eg for popular stateful optimizers such as SGD with momentum or Adam~\cite{Kingma15}).
The \emph{Variants} approach makes it possible to completely remove the LM
during training as precomputed embeddings can be used,
thus making training even faster and providing further memory savings.
This can be essential as detection training requires high-resolution images
which for some large vision models makes it hard to fit even
a batch size of 1 into the accelerator memory --
it is exactly the case for our self-training NFNet-F6 experiments
(Section~\ref{sec:selft})
which are not possible without the \emph{Variants} method.


%
\vspace{-0.15cm}
\subsubsection{Results}
\vspace{-0.1cm}
\label{sec:aug:res}

\tabFreeze

All experiments are performed on the \LVISminusrare benchmark (\cf Section~\ref{sec:setup}).
The effect of different approaches to training or freezing the language model
are shown in Table~\ref{tab:freeze}.

\paragraph{Training}
the LM with the same learning rate as the vision model is unstable
and results in poor performance.
Using the OWL~\cite{minderer2022simple} strategy of training the LM with a very small
learning rate yields good performance on all classes.
However,
when compared with our augmentation approaches,
it becomes clear that it is underperforming on unseen classes,
as even this small learning rate
causes some forgetting, albeit arguably not ``catastrophic''.

\paragraph{Freezing}
the LM underperforms and is unstable, testifying to overfitting;
this is equivalent to the \emph{1 Variant} scenario which performs
equally badly.
However, simply using dropout while keeping the network frozen performs
very well -- achieving the best mAP on the unseen classes.
Furthermore, the \emph{64 Variants} approach, where the variants of
the class embeddings are precomputed and sampled during training, performs
equally well while
enabling us to remove the LM inference from training.
This in turn achieves
a 9\% reduction in memory use and a speedup of 53\% vs \emph{Freeze + Dropout},
and 33\% memory savings and a 2$\times$ speedup vs the LM-training approaches.

\figGating

\paragraph{Templates.}
An alternative to the \emph{Variants} approach is to use
many different text templates
(\eg ``A close-up photo of the \emph{\{object\}}'' or ``A low resolution photo of the \emph{\{object\}}'')
to compute the
class embeddings and randomly sample them for each training sample~\cite{minderer2022simple}.
The 8 templates are formed by combining the ``7 best'' CLIP templates~\cite{radford2021learning}
and the default one (``A photo of a \emph{\{object\}}''), while the 80 templates are
the CLIP 80 templates (the default is already included).
The use of multiple templates during training has a similar effect to
\emph{64 Variants} in that it trains stably and outperforms
the LM-training approaches.
However, for good performance it requires inference with multiple
templates~\cite{minderer2022simple}
(\ie class probabilities are averaged across the different templates)
%
%
%
%
which increases complexity and
inference memory requirements, while still being beaten by our simple
\emph{Variants} approach.
We hypothesise this is because the templates have been designed for
ImageNet classification and
contain obscure concepts such as ``An origami \emph{\{object\}}''.
It is not easy to design many good templates, so our approach to
simply compute \emph{64 Variants} of the class embedding by using the
natural default template and enabling dropout is more effective.


%
\subsection{Alignment preserving architecture}
\label{sec:init}

As outlined in Section~\ref{sec:intro} and shown in Figure~\ref{fig:setup},
a typical setup that we also follow is to:
(1) pretrain a vision-language model,
(2) construct the open-vocabulary detector by re-using the vision and
text backbones and adding detection-specific layers
(feature pyramid network (FPN) and detector heads),
(3) initialize the backbones from (1) while initializing the rest
(FPN, heads) from scratch, and
(4) train all or subsets of parameters \eg freezing the LM backbone.

The disconnect between steps (1) and (3) stands out --
the vision and language backbones were trained together to produce aligned
representations of their respective modalities,
and we sever that alignment by introducing many layers in-between that are
trained from scratch.
The detector training then spends a long time seeking to realign
the features, and it is very likely that during this initial chaos some
of the pretrained alignment is forever lost.
Here we introduce small architectural changes
to the detector-specific layers
which serve to maintain the alignment of vision-text features at the start,
and promote it throughout the detector training.

The architectural modifications are shown in Figure~\ref{fig:gating} and consist
of strategically adding shortcut connections and
trainable gating layers~\cite{alayrac2022flamingo}.
A trainable gating layer,
with inputs $x$ and $y$ and a trainable scalar parameter $\alpha$,
produces the output $o = x (1-\tan\alpha) + y \tan{\alpha}$,
where $\alpha$ is initialized to 0 meaning $o = x$ at the start of training.
The shortcuts and gates are added such that at
the start of training, the features from the end of the vision backbone
are ``forwarded'' through the FPN and detector heads
all the way to the final classification features.
In other words, at the start of training, the detector head classification
features at all levels of the pyramid are equal to the backbone features.
Recall that the vision and text backbones have been pretrained for alignment.
This means that due to the specific gated-shortcut architecture and
initialization, the detection head classification features
(now equal to the backbone features)
are already aligned with the text embeddings at the beginning of
detector training.
Thus, the training is improved as it starts from a good initial guess
for the object classification and only needs to learn to improve
the classification and bounding box prediction,
rather than spend effort in rediscovering the vision-text alignment.


\paragraph{Aligned architecture design.}
Here we explain in more detail the recipe for converting an architecture
to its gated-shortcut version.
As explained above, the overall aim is to forward the final backbone features
(as they are pretrained to be aligned with the text embeddings)
to the end of the detection head.
So one only needs to follow the ``flow'' of the final backbone features
and apply the following operations:
(i) if they are mixed with another signal, add a gate that zeroes-out
the second signal at the start of training,
(ii) if an alignment preserving operation is performed (\eg upsampling)
do nothing,
(ii) if an alignment damaging transformation is performed (\eg a convolution),
make a shortcut connection and
add a gate such that the output equals the shortcut at the start of training.

These principles and the resulting architecture are illustrated in
Figure~\ref{fig:gating},
where the FPN is augmented with the shortcuts and gates,
while a single shortcut+gate combination is used around the entire detector head.
This makes it easy to apply the design to different detector heads
(\eg FCOS~\cite{tian2019fcos} vs T-Head~\cite{feng2021tood}) which contain
potentially more complex operations.


\vspace{-0.2cm}
%
\subsubsection{Results}
\label{sec:init:res}

\tabInit

Table~\ref{tab:gating} shows the results of our the alignment preserving
architectures (APA).

\paragraph{Alignment preserving vs vanilla architecture.}
Coupled with the NFNet-F0 vision backbone and the FCOS~\cite{tian2019fcos}
detector head, our design improves
\APall and \APrare by +2.3\% and +3.7\%, respectively.
Similarly, for the better performing T-Head~\cite{feng2021tood}
APA achieves +1.7\% and +2\%, respectively.
It is impressive that the improvements transfer to the larger
NFNet-F6 network which already exhibits an excellent \APall of 41.6\%,
which is further boosted by APA by +1.9\% to reach 43.5\%.
The largest improvement can be observed for \APrare where
the alignment preserving architecture tops the strong baseline by +6.5\%
and yields 27.6\%.


\paragraph{In the FPN, Head or everywhere?}
Table~\ref{tab:gating} shows it is more important to apply APA onto
the detection head than the FPN -- we speculate that this is because
the detection head is much deeper and therefore without APA
it takes longer to learn to re-learn the feature alignment
in the detector head than in the FPN.
However, applying APA onto both simultaneously clearly dominates,
confirming our intuition that maintaining alignment from the very
start of training is important.


%
\subsection{Self-training}
\label{sec:selft}

Text augmentation (Section~\ref{sec:aug}) and the
alignment preserving architecture (Section~\ref{sec:init})
bring significant gains in zero-shot performance due to the improved
feature alignment. However, it is still ambitious to ask for the detector
to extrapolate to completely unseen classes.
In this section, we investigate how to use self-training via pseudo-labelling
to further improve the feature alignment beyond the seen classes.

We propose a simple three-stage approach.
First, a good open vocabulary detector is trained using the previous
two improvements (Sections~\ref{sec:aug} and~\ref{sec:init}), called \emph{\tworec}.
The detector is then used to pseudo-label an additional dataset that
contains only images-text pairs scraped from the internet,
\ie it contains weak image-level information (the text), without any
human supervision nor finer-grained annotations such as classes,
bounding boxes or segmentations.
The detector uses the text embedding of the entire caption as the object query,
and we simply use the single highest scoring box per image if it passes a
confidence threshold of 0.25.
Finally, a new, stronger, open vocabulary detector (\emph{\threerec}) is trained
by combining the strongly supervised data (\LVISminusrare) and the
pseudo-labelled dataset, and treating the pseudo-labels as ground truth.

It is worth elaborating on the exact details of the final training stage.
Recall from Section~\ref{sec:setup} that for training with the true
ground truth annotations, we follow
the standard training procedure; \ie, certain detector head classification
features are assigned to be positives for particular classes
(in the open vocabulary case, its text embedding)
based on their pyramid level and location in the feature map~\cite{zhang2020bridging}.
The same features are negatives for other classes, and all remaining features
are negatives for all classes.
For example, if an image has a \emph{dog} in it and no \emph{cat}, we have:
(i) some features depending on scale and location are positives for \emph{dog},
(ii) features that are not positives for \emph{dog} are negatives for \emph{dog},
(iii) all features are negatives for \emph{cat}.
Training then proceeds with the standard per-class binary focal loss~\cite{lin2017focal}.

We propose an analogous mechanism when training with the pseudo-labels.
The single pseudo-bounding box per image is deemed to correspond to the entire caption,
and other captions in the batch are used as negatives.
Therefore, for the $i$-th image in the batch, we have:
(i) some features are deemed positive for the $i$-th caption
again following~\cite{zhang2020bridging},
(ii) features that are not positive are negatives for the $i$-th caption,
(iii) all features are negatives for the $j$-th caption where $i \neq j$;
we call this the use of ``batch-negatives''.
The same binary focal loss is used for training.
As will be shown in Section~\ref{sec:selft:res} and is commonly observed
in the self-supervised literature~\cite{oord2018representation,chen2020simple,arandjelovic2017look,he2020momentum},
batch-negatives are crucial to obtain good performance.

\paragraph{Relation to other methods.}
While multiple works have used self-training with pseudo-labelling to boost
the detector performance, none
%
follow the above approach.
Pseudo-labelling is popular in the weakly-supervised
(classes in the image are specified but not their location)
or semi-supervised
low-shot works~\cite{redmon2017yolo9000,sohn2020simple,zoph2020rethinking,ramanathan2020dlwl}
with closed vocabulary detectors.
This means that pseudo-labelling is easier as all classes are seen during training,
but also that the self-training follows exactly the same setup as the initial training,
where the negatives are other classes and there is no need or way to use
batch-negatives.

Our \emph{\threerec} uses the pseudo-detections from the \emph{\tworec} detector
conditioned on the image caption, while
Detic~\cite{zhou2022detecting} computes the pseudo-detection independently
of the caption by taking the largest bounding box proposal.
Detic adopts batch-negatives but does so on the image-level rather than the bounding box-level;
a more detailed discussion is available in \isArXiv{Appendix~\ref{sec:app:selft}.}{the supplementary material.}
GLIPv2~\cite{zhang2022glipv2} also uses batch-negatives,
but does not consider the zero-shot scenario.
While we use the entire caption at once to produce pseudo-detections,
GLIPv2 extracts noun phrases and pseudo-labels them individually.
This could provide better quality pseudo-labels, but comes with its downsides as well,
in that it depends on the quality of the text parser and requires additional bookkeeping
and special handling of repeated noun phrases in the batch.


\vspace*{-0.15cm}%
%
\subsubsection{Results}
\label{sec:selft:res}
\vspace{-0.2cm}

\tabSelfTrain
\tabSota
%

\paragraph{Implementation details.}
We start from the strong \emph{\tworec} detector from Section~\ref{sec:init}
and verify that longer training on \LVISminusrare does not improve
results further.
Conceptual Captions 12M~\cite{changpinyo2021conceptual} (CC12M), an image-caption dataset gathered
automatically from the internet containing 12M images,
is used for all self-training experiments.
The self-training starts from the \emph{\tworec} detector checkpoint
and continues training, where each training step simultaneously optimizes
the losses on a batch of \LVISminusrare images with true ground-truth
and a batch of pseudo-labelled images from CC12M.
In line with~\cite{zhou2022detecting}, we find we can reduce the resolution
of the CC12M images (for \LVISminusrare we use $800 \times 1024$,
while for CC12M $400 \times 512$ is sufficient)
thus fitting a larger number of images in the batch and allowing for
more batch-negatives.

\paragraph{Performance.}
Table~\ref{tab:selftrain} shows the self-training results --
it is clear that our self-training, \emph{\threerec}, significantly improves
both metrics and on both backbones, providing an especially large boost
for the unseen classes.

\paragraph{Comparison to Detic~\cite{zhou2022detecting}.}
We do not simply copy the numbers from~\cite{zhou2022detecting}
as this wouldn't be a fair comparison -- we use different visual backbones,
detector type, the self-training dataset, \etc.
Furthermore, Detic~\cite{zhou2022detecting} focuses on the zero-shot and not
open-vocabulary aspect (\eg the full approach specifically searches for
the LVIS-rare classes in the captions and uses this as the pseudo-label).
Therefore, we reimplement the open-vocabulary version of Detic$^\dagger$,
using our \emph{\tworec} detector (see \isArXiv{Appendix~\ref{sec:app:selft}}{the supplementary material for details}).
Detic$^\dagger$ performs well, giving improvements over \emph{\tworec}.
However, our self-training approach also significantly
beats Detic$^\dagger$.
We also compare to another approach proposed by~\cite{zhou2022detecting}
where the pseudo-detection is simply taken to be the image bounding box.
In fact, \emph{Image bbox} beats Detic$^\dagger$ slightly,
but \emph{\threerec} is still superior.
%
%
%

\paragraph{Importance of batch-negatives.}
As an ablation, we also train versions of
the \emph{Image bbox} and our \emph{\threerec} approaches where batch-negatives
are not used. The results show a clear large benefit of using
batch-negatives -- without them there is barely any gain from self-training
as the task is too easy.


%
\section{Results and discussion}
\label{sec:res}

Comparison on an equal footing with the state-of-the-art is hard
because most works use different visual backbones,
pretraining, detector architecture, training procedure,
augmentations, \etc.
Sections~\ref{sec:aug:res},~\ref{sec:init:res} and~\ref{sec:selft:res}
demonstrate the performance of each of our methods individually
through fair comparisons where all these aspects are identical,
while here we resort to the standard practice of reporting system-level
performance (Table~\ref{tab:sota}).
We list best performing methods that are truly zero-shot and open vocabulary,
and are trained following the
\LVISminusrare benchmark rules (\ie the only detection annotations used are
the LVIS training set with the rare classes removed).
For example, this criterion disqualifies
PromptDet~\cite{feng2022promptdet} and
the best performing versions of Detic~\cite{zhou2022detecting}
(they actively use the list of LVIS classes to pseudo-label additional images,
\ie not open vocabulary),
FIBER~\cite{dou2022coarsetofine} and some OWL~\cite{minderer2022simple} experiments
(train on many more detection annotations),
GLIP~\cite{li2022glip} and GLIPv2~\cite{zhang2022glipv2}
(rare classes are not removed during training so not zero-shot, and
more training data is used),
\etc.

Our final open vocabulary detector, \emph{\threerec}, achieves the highest
\APall and competitive \APrare.
On \APall it sets the state-of-the-art by a large margin
-- the largest network (NFNet-F6 with 440M parameters)
achieves 44.6\% (and 43.5\% without self-training) while the best second
is at 35.3\% (OWL~\cite{minderer2022simple}'s VIT-H/14 with 630M parameters),
making for an impressive improvement of 9.3\% points
(8.2\% without self-training).
Even our smaller network (NFNet-F0 with 71M parameters)
with self-training beats the previously best reported performance.

On the unseen classes, \APrare, we compare favourably to the latest
approaches. Only the concurrent \isArXiv{}{unpublished} F-VLM~\cite{kuo2023fvlm} method
performs better,
achieving 32.8\% for the largest R50x64 model,
while our equally large NFNet-F6 is a close second at 30.1\%.
It should be noted that, when trained on full LVIS, it has been observed
that \APrare inherently has high variance as these are the long tail categories,
and an absolute difference of 1\% might not be
significant~\cite{lvisbestpractices};
for example, our best performing run achieves 31.7\%.
The shared third place with a significantly lower \APrare value of 25.6\%
is achieved by OWL with VIT-L/14 (310M parameters)
and our much smaller NFNet-F0 model (71M parameters).

The good performance is partially due to the use of the strong vision backbone.
However, simply using it out of the box (\emph{\zerorec}) fails
(Table~\ref{tab:sota}, \cf Section~\ref{sec:aug:res})
and our methods are required to unlock its power.

\tabTransfer

\paragraph{Transfer}
capabilities are tested by evaluating the
\LVISminusrare trained networks on COCO~\cite{lin2014coco} and Objects365-v1~\cite{shao2019objects365}.
Table~\ref{tab:transfer}
shows the networks achieve impressive performance:
on COCO
even our smallest model without self-training beats all previous approaches,
while on Objects365 (estimated by~\cite{kuo2023fvlm} to have only 63\% overlap  %
with \LVISminusrare training classes)
\emph{\threerec} improves upon the previous best mAP by 5.1\% points.

Qualitative results are provided in \isArXiv{Appendix~\ref{sec:app:qual}.}{the supplementary.}


%
\section{Conclusions}

We introduced three methods for improving alignment between
visual and text features, which in turn boosts zero-shot
detection performance.
They reduce overfitting and forgetting of concepts learnt during
pretraining, improve training speed while decreasing accelerator memory
requirements, and make use of large image-text datasets without
costly detection annotations.
We achieve superior \APall on the challenging \LVISminusrare benchmark,
and transfer to COCO and Objects365, while obtaining \APrare competitive
with concurrent \isArXiv{}{unpublished} work.
Further research directions include investigating how to even more
efficiently make use of plentiful image-text data with
improved pseudo-labelling, losses, or combinations with
self-supervised learning.


%

\isArXiv{
\paragraph{Acknowledgments.}
We thank Evan Shelhamer for fruitful discussions
and Iain Barr for help with the codebase.
}{}

{\small
\bibliographystyle{ieee_fullname}
\bibliography{bib/shortstrings,bib/more,bib/vgg_other,bib/vgg_local}
}

\isArXiv{
\appendix
\input{iccv23_ovd_suppmat.tex}
}{}

\end{document}
