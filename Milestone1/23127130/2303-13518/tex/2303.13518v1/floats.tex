\newcommand{\meanstd}[2]{#1 {\tiny $\pm$ #2}}
%
\newcommand{\fail}[1]{\textcolor{BrickRed}{#1}}
\newcommand{\best}[1]{\textcolor{NavyBlue}{\bf{#1}}}
\newcommand{\second}[1]{\textcolor{RoyalPurple}{\underline{#1}}}
%
%


\newcommand{\figTeaser}{
\begin{figure}[t]
\vspace{-0.2cm}
\centering
\includegraphics[width=\linewidth]{figures/teaser.jpg}
%
%
\caption{{\bf Zero-shot open vocabulary detection.}
The detector is able to answer the queries ``Gargoyle'' and ``Eiffel tower''
despite never seeing human-annotated bounding boxes for them.
\vspace{-0.4cm}
}
\label{fig:teaser}
\end{figure}
}


\newcommand{\tabFreeze}{
\begin{table}[t]
\hspace*{-0.25cm}%
\centering
\begin{tabular}{lr@{~~}rcr} \toprule
LM train or freeze & \multicolumn{1}{c}{\small \APall} & \multicolumn{1}{c}{\small \APrare} & \multicolumn{1}{c}{\small speed} & \multicolumn{1}{c}{\small mem.} \\
\midrule
Train w/ lr-ratio 1 & \fail{\meanstd{12.0}{15.9}} & \fail{\meanstd{5.2}{7.30}} & 1.3 & 14.1G \\  %
Train w/ lr-ratio 0.01 & \best{\meanstd{33.2}{0.30}} & \meanstd{16.5}{0.95} & 1.3 & 14.1G \\  %
Freeze		& \fail{\meanstd{24.4}{13.2}} & \fail{\meanstd{13.0}{8.51}} & 1.9 & 10.4G \\  %
Freeze + Dropout & \meanstd{31.8}{0.20} & \best{\meanstd{18.7}{1.39}} & 1.7 & 10.4G \\  %
8 Templates, infer.\ 1 & \meanstd{31.3}{0.31} & \meanstd{16.4}{1.86} & \best{2.6} & \best{9.4G} \\  %
8 Templates, infer.\ 8 & \meanstd{31.5}{0.10} & \meanstd{17.1}{1.28} & \best{2.6} & \best{9.4G} \\  %
80 Templates, infer.\ 1 & \meanstd{31.6}{0.25} & \meanstd{17.4}{0.38} & \best{2.6} & \best{9.6G} \\  %
80 Templates, infer.\ 8 & \meanstd{31.9}{0.06}	& \meanstd{18.1}{0.66} & \best{2.6} & \best{9.6G} \\  %
1 Variant & \fail{\meanstd{16.3}{13.0}}	& \fail{\meanstd{6.9}{7.89}} & \best{2.6} & \best{9.4G}	\\  %
64 Variants & \meanstd{32.1}{0.31} & \best{\meanstd{18.9}{1.13}} & \best{2.6} & \best{9.5G} \\  %
\bottomrule
\end{tabular}
\caption{{\bf To train or to freeze the language model (\LVISminusrare benchmark).}
Speed is measured as the number of gradient steps per second,
while `mem.' denotes the peak accelerator memory usage.
Methods where at least 1 out of the 3 training runs has failed are
in \fail{red}.
Our \emph{Freeze + Dropout} and \emph{64 Variants} approaches
perform best on the unseen classes, while speeding up training
and requiring less memory.  %
}
\label{tab:freeze}
\vspace{-0.2cm}
\end{table}
}


\newcommand{\tabInit}{
\def\y{\checkmark}
\def\n{~}
\begin{table}[t]
\centering
\begin{tabular}{cc@{~~}c@{~~}cc@{~~}c@{~~}cc} \toprule
\multicolumn{2}{c}{Architecture} & ~ & \multicolumn{2}{c}{APA} & ~ & \multicolumn{2}{c}{\LVISminusrare performance} \\
\cmidrule{1-2} \cmidrule{4-5} \cmidrule{7-8}
Backb. & Head && FPN & Head & & \APall & \APrare \\
\midrule
NF-F0 & FCOS && \n & \n && \meanstd{30.4}{0.21} & \meanstd{16.1}{1.46} \\  %
NF-F0 & FCOS && \y & \y && \best{\meanstd{32.7}{0.15}} & \best{\meanstd{19.8}{0.34}} \\  %
\midrule
NF-F0 & T-Head && \n & \n && \meanstd{32.1}{0.31} & \meanstd{18.9}{1.13} \\  %
NF-F0 & T-Head && \y & \n && \meanstd{32.4}{0.44} & \meanstd{18.3}{1.58} \\  %
NF-F0 & T-Head && \n & \y && \meanstd{33.3}{0.15} & \meanstd{19.6}{0.49} \\  %
NF-F0 & T-Head && \y & \y && \best{\meanstd{33.8}{0.15}} & \best{\meanstd{20.9}{0.34}} \\  %
\midrule
NF-F6 & T-Head && \n & \n && \meanstd{41.6}{0.17} & \meanstd{21.1}{0.40} \\  %
NF-F6 & T-Head && \y & \y && \best{\meanstd{43.5}{0.12}} & \best{\meanstd{27.6}{0.80}} \\  %
\bottomrule
\end{tabular}
\caption{{\bf Alignment preserving architecture (APA).}
All networks were trained with the \emph{64 Variants} approach (Section~\ref{sec:aug}).
The added shortcuts and trainable gating layers consistently improve
the detection performance for both the backbone and detection head architectures.
}
\label{tab:gating}
\vspace{-0.2cm}
\end{table}
}


\newcommand{\tabSelfTrain}{
\begin{table}[t]
\hspace*{-0.25cm}%
\centering
\begin{tabular}{llrr} \toprule
Self-training method & Backb. & \multicolumn{1}{c}{\APall} & \multicolumn{1}{c}{\APrare} \\
\midrule
\tworec (no self-training) & NF-F0 & \meanstd{33.8}{0.15} & \meanstd{20.9}{0.34} \\  %
Image bbox w/o batch-negs & NF-F0 & \meanstd{33.9}{0.30} & \meanstd{20.9}{1.06}  \\ %
Image bbox & NF-F0 & \meanstd{35.1}{0.26} & \meanstd{24.2}{1.37} \\  %
Detic~\cite{zhou2022detecting} \scriptsize{open-voc.} $^\dagger$ & NF-F0 & \meanstd{34.8}{0.32} & \meanstd{23.4}{1.49} \\  %
\threerec w/o batch-negs & NF-F0 & \meanstd{34.2}{0.15} & \meanstd{20.7}{0.35}  \\ %
\threerec & NF-F0 & \best{\meanstd{35.7}{0.20}} & \best{\meanstd{25.6}{1.12}} \\  %
\midrule
\tworec (no self-training) & NF-F6 & \meanstd{43.5}{0.12} & \meanstd{27.6}{0.80} \\  %
\threerec & NF-F6 & \best{\meanstd{44.6}{0.31}} & \best{\meanstd{30.1}{1.83}} \\  %
\bottomrule
\end{tabular}
\caption{{\bf Self-training (\LVISminusrare benchmark).}
%
%
CC12M is pseudo-labelled with the \emph{\tworec} detector (Sections~\ref{sec:aug} and~\ref{sec:init}).
Detic$^\dagger$ is our reimplementation of Detic~\cite{zhou2022detecting},
Image bbox uses the entire image as the pseudo-detection while
\threerec uses the \tworec's best prediction;
Section~\ref{sec:selft} explains all methods.
Self-training helps, and batch-negatives are important.
}
\label{tab:selftrain}
\vspace{-0.2cm}
\end{table}
}


\newcommand{\tabSota}{
\def\y{\checkmark}
\def\n{~}
\begin{table*}[t]
\vspace{-0.4cm}
\centering
\begin{tabularx}{\linewidth}{>{\hsize=1.2\hsize\linewidth=\hsize}X >{\hsize=0.8\hsize\linewidth=\hsize}X rcllll}
\toprule
%
Method & Backbone & \multicolumn{1}{c}{\#Params} & Self-training & \multicolumn{1}{c}{\APall} & \multicolumn{1}{c}{\APrare} & \multicolumn{1}{c}{\APcomm} & \multicolumn{1}{c}{\APfreq} \\
\midrule
Detic~\cite{zhou2022detecting} \scriptsize{open-voc.}$^{(m)}$ & R50 & 26M & \y & 30.4 & 17.4 \\
DetPro~\cite{du2022learning} & R50 & 26M & \n & 28.4 & 20.8 & 27.8 & 32.4 \\
%
RegionCLIP~\cite{zhong2022regionclip} & R50x4 & 87M & \y & 32.1 & 22.0 & 32.1 & 36.9 \\
%
OWL~\cite{minderer2022simple} & VIT-L/14 & 303M & \n & 34.7 & 25.6 \\
OWL~\cite{minderer2022simple} & VIT-H/14 & 627M & \n & 35.3 & 23.3 \\
%
F-VLM~\cite{kuo2023fvlm} & R50x4 & 87M & \n & 28.5 & 26.3 \\
%
F-VLM~\cite{kuo2023fvlm} & R50x64 & 420M & \n & 34.9 & \best{32.8} \\
\zerorec \scriptsize{[this work]} & NFNet-F0 &  71M & \n & \meanstd{16.3}{13.0}	& \multicolumn{1}{r}{\meanstd{6.9}{7.89}} & \meanstd{13.2}{11.3} & \meanstd{23.7}{11.7} \\  %
\onerec \scriptsize{[this work]} & NFNet-F0 &  71M & \n & \meanstd{32.1}{0.31} & \meanstd{18.9}{1.13} & \meanstd{29.5}{0.15} & \meanstd{40.9}{0.08} \\  %
\tworec \scriptsize{[this work]} & NFNet-F0 &  71M & \n & \meanstd{33.8}{0.15} & \meanstd{20.9}{0.34} & \meanstd{32.4}{0.20} & \meanstd{41.0}{0.05} \\  %
\threerec \scriptsize{[this work]} & NFNet-F0 &  71M & \y & \meanstd{35.7}{0.20} & \meanstd{25.6}{1.12} & \meanstd{34.2}{0.05} & \meanstd{41.8}{0.02} \\  %
\zerorec \scriptsize{[this work]} & NFNet-F6 & 440M & \n & \multicolumn{1}{r}{\meanstd{0.8}{0.19}} & \multicolumn{1}{r}{\meanstd{0.4}{0.12}} & \multicolumn{1}{r}{\meanstd{0.7}{0.16}} & \multicolumn{1}{r}{\meanstd{1.0}{0.24}} \\  %
\onerec \scriptsize{[this work]} & NFNet-F6 & 440M & \n & \meanstd{41.6}{0.17} & \meanstd{21.1}{0.40} & \meanstd{42.9}{0.19} & \meanstd{49.2}{0.09} \\  %
\tworec \scriptsize{[this work]} & NFNet-F6 & 440M & \n & \meanstd{43.5}{0.12} & \meanstd{27.6}{0.80} & \meanstd{44.9}{0.10} & \meanstd{48.8}{0.01} \\  %
\threerec \scriptsize{[this work]} & NFNet-F6 & 440M & \y & \best{\meanstd{44.6}{0.31}} & \meanstd{30.1}{1.83} & \best{\meanstd{46.0}{0.17}} & \best{\meanstd{49.3}{0.08}} \\  %
\bottomrule
\end{tabularx}
%
\caption{{\bf State-of-the-art for zero-shot open vocabulary detection on \LVISminusrare.}
$^{(m)}$ denotes that Detic~\cite{zhou2022detecting} only reports the mask mAPs,
box mAP should not be much higher.
\emph{\zerorec}, \emph{\onerec}, \emph{\tworec} and \emph{\threerec}
refer to
the baseline detector with the frozen LM and no text augmentation,
and cumulative application of our three methods
from Sections~\ref{sec:aug},~\ref{sec:init} and~\ref{sec:selft}, respectively.
\emph{\threerec} performs well, yielding the best \APall by a large margin
and achieving a favourable \APrare.
}
\vspace{-0.2cm}
\label{tab:sota}
\end{table*}
}


\newcommand{\figSetup}{
\begin{figure*}[t]
\definecolor{lang}{HTML}{134f5c}
\definecolor{vis}{HTML}{1155cc}
\definecolor{det}{HTML}{b45f06}
\vspace{-0.6cm}
\includegraphics[width=\linewidth]{figures/ovd_setup.pdf}
\caption{{\bf A standard approach to open vocabulary detection and pretraining.}
A standard single-stage detector adapted to open vocabulary detection,
as explained in Section~\ref{sec:setup},
makes use of a \textcolor{lang}{language model}, \textcolor{vis}{vision backbone}, \textcolor{det}{feature pyramid network (FPN)},
and \textcolor{det}{detector heads}.
The \textcolor{vis}{vision backbone} and \textcolor{lang}{language model} are typically pretrained in
a contrastive manner, while the \textcolor{det}{FPN} and the \textcolor{det}{detector heads} are initialized
from scratch.
}
\label{fig:setup}
\vspace{-0.2cm}
\end{figure*}
}


\newcommand{\figGating}{
\begin{figure*}[t]
\definecolor{arrow}{HTML}{6aa84f}
\definecolor{gate}{HTML}{ff9900}
%
\definecolor{back}{HTML}{cfe2f3}
\DeclareRobustCommand{\orangehexagon}{$\mathord{\raisebox{0.6pt}{\tikz{\node[draw,scale=.65,regular polygon, regular polygon sides=6,fill=gate](){};}}}$\@\xspace}
\includegraphics[width=\linewidth]{figures/arch_apa.pdf}
\caption{{\bf Alignment preserving architecture (APA).}
The standard single-stage object detector architecture
[Backbone → Feature pyramid network (FPN) → Detector heads]
is augmented with shortcuts and trainable gating layers
(\orangehexagon),
which at init propagate
the \textcolor{arrow}{green} input and block the \textcolor{red}{red} input.
The output is computed as
$\textcolor{arrow}{x} (1-\tan\alpha) + \textcolor{red}{y} \tan{\alpha}$,
and $\alpha=0$ at init.
The \textcolor{arrow}{green arrows} show the propagation of the last
backbone features at init all the way to the final
detector head classification features.
\textcolor{CornflowerBlue}{Light blue} and \textcolor{Apricot}{light yellow} parallelograms
represent the backbone and FPN feature maps, respectively;
circles with ↑2 and ↓2 are non-trainable up- and down-sampling,
squares are trainable modules (\eg convolutions),
and \orangehexagon are the trainable gates;
convolution blocks show the kernel size
and potential striding (`s2': stride 2).
The standard architecture (\ie without the shortcuts and gates) is shown
in the supplementary material.
}
\label{fig:gating}
\vspace{-0.2cm}
\end{figure*}
}


\newcommand{\tabTransfer}{
\begin{table}[t]
%
\centering
\begin{tabular}{llrcc} \toprule
Method & Backbone & \#Params & COCO & O365 \\
\midrule
ViLD~\cite{gu2022openvocabulary} & R50 & 26M & 36.6 & 11.8 \\
DetPro~\cite{du2022learning} & R50 & 26M & 34.9 & 12.1 \\
%
F-VLM~\cite{kuo2023fvlm} & R50x4 & 87M & 36.0 & 14.2 \\
%
F-VLM~\cite{kuo2023fvlm} & R50x64 & 420M & 39.8 & 17.7 \\
\tworec \scriptsize{[this work]} & NF-F0 & 71M & 40.6 & 14.6 \\
\threerec \scriptsize{[this work]} & NF-F0 & 71M & 41.5 & 16.4 \\
\tworec \scriptsize{[this work]} & NF-F6 & 440M & 46.5 & 20.3 \\
\threerec \scriptsize{[this work]} & NF-F6 & 440M & \best{46.9} & \best{22.8} \\
\bottomrule
\end{tabular}
\caption{{\bf Transfer.}
The \LVISminusrare trained networks are evaluated
on COCO~\cite{lin2014coco} and Objects365-v1~\cite{shao2019objects365} without any additional training.
%
%
}
\label{tab:transfer}
\vspace{-0.3cm}
\end{table}
}
