\section{Related Work}\label{sec:rw}
We relate our work to image-based reconstruction, transformers, and image captioning.

\noindent\textbf{Image-based reconstruction:}
Reconstructing 3D objects using one or multiple image views to a 3D model has been an open research problem for decades. Some approaches use Long Short Term Memory~\cite{lstm} to generate 3D occupancy grids~\cite{3dr2n2}, and Octree~\cite{octree} based deep learning model~\cite{OGN}.
Shen et al.~\cite{frequencyDomain} used images directly by exploiting the Fourier slice theorem~\cite{fourierSlice} as a feature to reconstruct a 3D model. 
Wang et al.~\cite{pix2mesh} used a single image to reconstruct a triangular mesh.

A boundary of a smooth 3D model can be reconstructed by the approach~\cite{occupancy}. The main advantage of this method is a lower memory requirement compared to meshes and voxels. Moreover, Chen et al.~\cite{Chen_2019_CVPR} trained a Generative Adversarial Network (GAN)~\cite{wgan} to learn implicit fields to reconstruct 3D models using images.
Yang et al.~\cite{yang2020robust} used the attention mechanism~\cite{attention} in a feed-forward neural network to reconstruct 3D in voxel from multiple images. Instead of using a Recurrent Neural network (RNN) that suffers from long-term memory loss, Xie et al.~\cite{pix2voxpp} introduced a model with the encoder and decoder in multi- and single-view image reconstruction. Image features are extracted using the encoder and then the decoder then generates coarse 3D volumes.

Yang et al.~\cite{Yang_2021_CVPR} introduced a model that is robust against background noise by using a shape prior. Instead of directly generating 3D volumes, Park et al.~\cite{deepsdf} used a signed distance field (SDF) to generate 3D shapes. Using camera positions and viewing directions, Mildenhall et al.~\cite{nerf} introduced NeRFs that generate a volume density and their spatial radiance. Yang et al.~\cite{Yang_2022_CVPR} also introduced 3D reconstruction from images using estimated camera poses.

Shen et al.~\cite{shen20193d} used a frequency-based lossy compression that can reconstruct about 93\% of the input data. Contrary to this related work, our method uses lossless compression and also encodes it into a 1D structure. SnakeVoxFormer reconstructs 3D voxel models in a space efficient way, by using RLE and the codebook.

\noindent\textbf{Transformer} introduced in NLP~\cite{attention} has been applied in computer vision for many tasks. The state-of-the-art performance in image classification~\cite{vit,swin} is achieved by treating images as tokens that are then fed into various transformer models and object detection in images~\cite{DETR}. Zheng et al.~\cite{SETR} used transformers for image segmentation and it has been shown to classify videos effectively~\cite{arnab2021vivit}.
Liu et al.~\cite{Liu_2022_CVPR} introduced a transformer-based model to classify videos using an inductive bias of locality. 
With notable achievements using transformer network, Wang et al.~\cite{wang2021multi} approached image-to-3D model reconstruction as a sequence-to-sequence problem that transformer scores the state of the art performance. We use the transformer to reconstruct 3D models by mapping the 3D voxel space into a compressed 1D domain.

\noindent\textbf{Image Captioning} adds textual information about the context of an image. Vinyals et al.~\cite{showAndTell} showed how a combination of Convolutional Neural Networks (CNN) and Recurrent Neural Networks (RNNs) is capable of generating captions for a given image. In addition to the CNN and RNN, the attention mechanism has been shown to focus important visual features~\cite{showAndAttend}. 
Li et al.~\cite{imageTransformer} used the transformer-based model for image captioning, and Cornia et al.~\cite{Cornia_2020_CVPR} showed how their meshed attention mechanism can apply a transformer-based model for  image translations. Instead of learning from one feature, Luo et al.~\cite{luo2021dual} use grid and region features to achieve a new state of the art. Better contextual information can be extracted by using region and grid features as opposed to using region features. Our model is different in that it encodes the 3D structure into a 1D lossless compressed data that is highly suitable for deep learning by using the transformer.