\section{SnakeVoxFormer Architecture and Training}\label{section:architecture}
\subsection{Architecture}
SnakeVoxFormer consists of the encoder  and decoder. The encoder is based on the Vision Transformer~\cite{vit}, which extracts image features inspired by the image captioning style that are then used as input to the decoder to generate its corresponding tokens (Sec.~\ref{section:token}). The tokens are then used to reconstruct the 3D voxels objects using RLE (Sec.~\ref{section:RLE}).

\noindent\textbf{Encoder}\label{section:encoder}
We extract image features using Vision Transformer~\cite{vit} (ViT B/32) pre-trained on ImageNet~\cite{imagenet}. We freeze all the trainable parameters from the ViT to take image features as the encoder of SnakeVoxFormer then we resize images (Fig.~\ref{fig:inputImage}) to $224^2$ in RGB as the input of the encoder.

\noindent\textbf{Decoder}\label{section:decoder}
The decoder takes image features from the encoder and generates tokens $T_i$ (Sec.~\ref{section:token}). We use the original architecture of the transformer~\cite{attention}, but we modified its hyper-parameters, such as the number of heads and the number of hidden units in a feed-forward layer. 


\subsection{Training}
We use ShapeNet~\cite{shapenet} dataset and objects categories airplane, bench, cabinet, car, cellphone, chair, couch, firearm, lamp, monitor, table, and speaker which are widely used in a reconstruction problem. The dataset contains 24 different views of each object  and 3D objects are in $32^3$ voxels.

We trained SnakeVoxFormer using CrossEntropy~\cite{crossentropy} as a loss function using Adam~\cite{adam} with default parameters as an optimizer with setting 1e-4 as a learning rate. We set 32, 24, and 96 as the batch size of Snake, Raster Scanning, and Spiral traversal model training.
We validate the model by training four separate models that used 4, 8, 16, and 20 different image views from different angles per object. 

We also trained the three models on different traversing strategies using snake, spiral, and raster scanning.

The model was implemented in Tensorflow and it ran on an Intel i9-12900K clocked at 4.8GHz with Nvidia GeForce RTX 3090. The models were trained for 17 hours (snake traversal), 24 hours (spiral), and 143.5 hours (raster scanning). The different training time comes from different batch size and the size of the used dataset. We did not observe a notable performance difference after 75 hours of training on the raster scanning model.

We will provide the source code of SnakeVoxFormer.
