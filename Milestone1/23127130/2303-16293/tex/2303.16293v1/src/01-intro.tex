\section{Introduction}
\begin{figure*}[hbt]
\centering
\includegraphics[width=0.95\linewidth]{images/overview.pdf}
  \caption{Overview of SnakeVoxFormer:
  The input is an RGB image that is encoded by using a ViT encoder to extract its features. The features are input to the transformer-based decoder that  produces tokens corresponding to image features. The tokens are stored in a codebook that allows the decompression of the Run Length Encoded data back to its original form in voxels.
  }\vspace{-3mm}
  \label{fig:overview}
\end{figure*}

3D object reconstruction from images is an important problem that has been addressed by many methods. In particular, captioning helps to explain images~\cite{Guo_2019_CVPR,vinyals2015show} or even videos~\cite{Hou_2019_ICCV,wu2017deep} in human language. One important area is voxel object reconstruction from single images that provides immediate space occupancy and data structure that is readily available for tasks that require collision detection. Transformer~\cite{attention} was introduced for Natural Language Process (NLP), and it has achieved supported an outstanding performance in many areas, such as image classification~\cite{vit, swin, Wang_2021_ICCV} and image captioning~\cite{ji2021improving,li2019entangled}. However, acquiring a 3D voxel model from a single image is an open problem that has been solved only partially.

We introduce SnakeVoxFormer, a novel algorithm for 3D voxel model reconstruction from a single image. The key idea of our approach is to encode the voxel space by traversing through it in a snake-like way. We encode all voxels by Run Length Encoding (RLE) compression~\cite{RLE} that stores the data as a sequence of couples of numbers~$[rep,val]$, where $rep$ indicates the number of repetitions and $val$ the voxel value, so e.g., the couple $[7,0]$ indicates seven consecutive empty voxels. We effectively convert the 3D voxel space into a 1D structure. In the next step, we create a dictionary of the detected structures (the codebase) that is then encoded as a set of tokens and used for learning with a transformer. For each input image, SnakeVoxFormer extracts image features by using Vision Transformer~\cite{vit} pre-trained on ImageNet~\cite{imagenet} and maps them to the corresponding tokens. 

SnakeVoxFormer outperforms the state-of-the-art methods for 3D voxel reconstruction
3D-R2N2~\cite{3dr2n2}, AttSets~\cite{yang2020robust}, Pix2Vox-A~\cite{pix2vox}, Pix2Vox++/A~\cite{pix2voxpp}, VoIT~\cite{wang2021multi}, VoIT+~\cite{wang2021multi}, EVoIT~\cite{wang2021multi}, and TMVNet~\cite{TMVNet} by at least 2.8\% and up to 19.8\%.

We claim the following contributions: 1)~Compressing voxels by Run Length Encoding and dictionary-based encoding into 1D data representation suitable for deep learning, 2)~transformer-based 3D object reconstruction network with multiple views using RLE. 

We will provide the source code of SnakeVoxFormer.
