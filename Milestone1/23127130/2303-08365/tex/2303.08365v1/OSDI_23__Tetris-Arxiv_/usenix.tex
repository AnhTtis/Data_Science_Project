% TEMPLATE for Usenix papers, specifically to meet requirements of
%  USENIX '05
% originally a template for producing IEEE-format articles using LaTeX.
%   written by Matthew Ward, CS Department, Worcester Polytechnic Institute.
% adapted by David Beazley for his excellent SWIG paper in Proceedings,
%   Tcl 96
% turned into a smartass generic template by De Clarke, with thanks to
%   both the above pioneers
% use at your own risk.  Complaints to /dev/null.
% make it two column with no page numbering, default is 10 point

% Munged by Fred Douglis <douglis@research.att.com> 10/97 to separate
% the .sty file from the LaTeX source template, so that people can
% more easily include the .sty file into an existing document.  Also
% changed to more closely follow the style guidelines as represented
% by the Word sample file. 

% Note that since 2010, USENIX does not require endnotes. If you want
% foot of page notes, don't include the endnotes package in the 
% usepackage command, below.

\documentclass[letterpaper,twocolumn,10pt]{article}
\usepackage{usenix,epsfig,endnotes}

\usepackage[misc]{ifsym} 
\usepackage{lipsum}
\usepackage{verbatim}  
\usepackage{comment}
\usepackage{algorithm}
\usepackage{algpseudocode}  
\usepackage{subfigure}
\usepackage{multirow}
\usepackage{booktabs}
\usepackage{graphicx}
\usepackage{threeparttable}
\usepackage{listings}
% \usepackage{framed}
%\usepackage{minted}

\begin{document}
 

\title{\Large \bf {Gamify Stencil Dwarf on Cloud for Democratizing Scientific Computing}}
%make title bold and 14 pt font (Latex default is non-bold, 16 pt)
%\title{\Large \bf Tetris: Towards Stencil-driven Scientific Computing with { Tiling Tetrominoes} on Heterogeneous Processors of Cloud}%
%Tetris: HPC Dwarf on Cloud by {\Large \bf Tetris: Towards High Performance Computing on CPU and GPU Cloud Infrastructure}}
%Gamify HPC Dwarf on Cloud for Democratizing Scientific Computing}
\author{
{\rm Kun Li}\\
Microsoft Research
\and
{\rm Zhichun Li}\\
HPC Research Center, CAS
\and
{\rm Yuetao Chen}\\
Microsoft Research
\and
{\rm Zixuan Wang}\\
HPC Research Center, CAS
\and
{\rm Yiwei Zhang}\\
HPC Research Center, CAS
\and
{\rm Liang Yuan}\\
HPC Research Center, CAS
\and
{\rm Haipeng Jia}\\
HPC Research Center, CAS
\and
{\rm Yunquan Zhang}\\
HPC Research Center, CAS
\and
{\rm Ting Cao $^{\textrm{\Letter}}$ }\\
Microsoft Research
\and
{\rm Mao Yang}\\
Microsoft Research
}
 
\maketitle
\newcommand\blfootnote[1]{%
\begingroup
\renewcommand\thefootnote{}\footnote{#1}%
\addtocounter{footnote}{-1}%
\endgroup
}
% Use the following at camera-ready time to suppress page numbers.
% Comment it out when you first submit the paper for review.
\thispagestyle{empty}
\blfootnote{\hspace{-1em}$^{\textrm{\Letter}}$ Corresponding Author.} 
\blfootnote{\hspace{-1em} This work was completed in 2022.} 

\subsection*{Abstract}
%As one of the seven dwarfs in HPC, Stencil computation is widely used in various scientific and engineering applications.
%Over the past three decades, high-performance computing (HPC) has made rapid progress and plays an increasingly important role in scientific computing and other fields. 
%As Turing Award winner Jack Dongarra pointed out, the fusion of HPC and AI is the future trend. However, there still remains a significant gap between HPC and AI today both on algorithm and architecture level, making a scalable heterogeneous mix intractable and will even lead to performance degradation. 
%The convergence of AI and HPC promises to transform the scientific computing landscape, 
%The booming HPC techniques promise to transform the scientific and engineering landscape, 
%with significant potentials to enable scientists to tackle challenges that would otherwise have been beyond their capabilities. 
%{High Performance Computing (HPC) techniques are rapidly developing to solve challenging problems in science and engineering. }
%However, %TANSTAAFL, 
%most HPC applications still rely heavily on supercomputers, suffering from expensive access, poor scalability, and duplicated optimizations. {By comparison, Cloud infrastructure is unified, high-performance, low-cost, and elastic.}
%High Performance Computing (HPC) techniques are rapidly developed to solve challenging problems in science and engineering. 
{Stencil computation is one of the most important kernels in various scientific computing.}
%{Scientific computing is prosperously developed to solve Grand Challenge problems in science and engineering, where Stencil computation is one of the most important kernels in them}
{Nowadays, most Stencil-driven scientific computing} still relies heavily on supercomputers, suffering from expensive access, poor scalability, and duplicated optimizations.% By comparison, Cloud infrastructure is unified, high-performance, low-cost, and elastic. 
%{However, with complicated spatial and temporal data dependencies, it is highly challenging to democratize scientific computing on Cloud for heterogeneous processors due to significant architecture differences, breaking existing tiling algorithms and causing severe performance degradation.}  
%strong thirsts for high accuracy, under-utilization on Cloud, and high bar of HPC accessibility. %most of the existing AI-driven HPC applications obtain better efficiency at the cost of accuracy by alternating numerical   calculation with approximate calculation by AI models, where the AI infrastructure like GPU is typically treated an autism only constrained to AI tasks and provides no acceleration for HPC simulations. 
%in a cohesive fashion
%Based on this observation, unprecedented

%Aiming a new paradigm for scientific computing on the Cloud, we propose Tetris, the first  {high-performance Stencil system to democratize scientific computing on the CPU+GPU heterogeneous architecture of the Cloud.} 
%{This paper proposes Tetris, the first system for high-performance Stencil on heterogeneous CPU+GPU, towards democratizing Stencil-driven scientific computing on Cloud. }
%Aiming a new paradigm for scientific computing on Cloud, we propose {a fully-tessellated tiling technique, Tetris, to tessellate the spatial and temporal dimensions perfectly for democratizing Stencil-driven scientific computing on heterogeneous processors.}
%This paper proposes Tetris, the first disruptive HPC Paradigm to democratize HPC on CPU and GPU Cloud infrastructure with cheap access, elastic scalability, and unified architecture.
% by orchestrating the Cloud resources, which contributes to cheap access, elastic scalability, and unified architecture.
%scales elastically,
%unprecedentedly, 
%allowing scientists to easily perform an accuracy-satisfied simulation and engineers to achieve performance-boosting optimization simultaneously. 
%Taking the data transfers through the whole memory hierarchy into account,
%Characterized by hierarchical design, 
{This paper proposes Tetris, the first system for high-performance Stencil on heterogeneous CPU+GPU, towards democratizing Stencil-driven scientific computing on Cloud. In Tetris, polymorphic tiling tetrominoes are first proposed to bridge different hardware architectures and various application contexts with a perfect spatial and temporal tessellation automatically. Tetris is contributed by three main components: (1) Underlying hardware characteristics are first captured to achieve a sophisticated Pattern Mapping by register-level tetrominoes; (2) An efficient Locality Enhancer is first presented for data reuse on spatial and temporal dimensions simultaneously by cache/SMEM-level tetrominoes; (3) A novel Concurrent Scheduler is first designed to exploit the full potential of on-cloud memory and computing power by memory-level tetrominoes.} Tetris is orthogonal to (and complements) the optimizations or deployments for a wide variety of emerging and legacy scientific computing applications. Results of thermal diffusion simulation demonstrate that the performance is improved by 29.6×, reducing time cost from day to hour, while preserving the original accuracy. 
%polymorphic \textit{Tiling Tetrominoes} are first proposed to tessellate the spatial and temporal dimensions perfectly on different architectures and application contexts automatically; 
%for multi-layered data reuse on different architectures and application contexts automatically;%, making flexible mappings on parallel compute units and exploiting more efficient data reuse to boost higher performance. 
%presents an architecture-aware heterogeneous scheduler in a coarse-grained style acting on memory first, and it %enables large-scale scientific computing by  offloads data and compute to GPU while balancing workloads and minimizing data movements. 
%Targeted on cache or shared memory level, medium-grained tiled blocks are produced by a tiling generator to further preserve the data locality. 

%\textit{Sophisticated Dwarf Adaptor}, 
%Then Tetris captures the underlying hardware characteristics and redesigns HPC calculation procedures to exploit the specialized hardware units such as Vector Registers and Tensor Cores on Cloud for the first time.
%and (3) a novel \textit{Concurrent Scheduler} is first designed to exploit the full potential of on-cloud memory and computing power. 
%Tetris is the first disruptive on-Cloud work that bridges the accuracy gap in scientific computing, breaks the performance wall between CPU and GPU, and lowers the democratization bar for users. 

%without performance degradation and Tensor Cores without accuracy discrimination, Tetris %draws an analogy with Convolution in AI field to
%While Cloud equipped with high-performance CPU+GPU heterogeneous architectures are pervasive, interests are increasingly derived in exploring scientific computing with HPC Dwarfs on Cloud. However, as stated above, almost all the existing work only concentrate on a specific technique on a single architecture, such as vectorization on CPU, spatial tiling on GPU, etc. This makes it hard for these methods to scale to Cloud directly. Our proposal is a major shift from these methods in that we leverage Cloud with abundant HPC techniques to democratize large-scale scientific computing for the first time, and it is orthogonal to (and complements) the optimizations or deployments for a wide variety of emerging and legacy scientific computing applications. 

%a series of fine-grained template tetrominoes are proposed to support an efficient and flexible data manipulation with the collaboration of Vector Registers on CPU and Tensor Cores on GPU for the first time. 
 %, which paves the way for a new HPC+AI for science paradigm.
%The design of Tetris is overviewed with 3 layers that acts on different memory hierarchy. 

%Based on polymorphic data tetrominoes, it relieve data alignment conflicts in scientific computing effectively with vector registers on CPU, provides high-precision computing acceleration innovatively with tensor cores on GPU, and eliminates heavy communication and offloading overheads significantly with architecture-aware scheduler on heterogeneous system.
%Taking widely-used HPC Stencil computation into account, We observe that scalability challenges on CPU are fundamentally different from that on GPU, and that commonly used techniques are targeted separately on heterogeneous architectures, which yields underutilized computing potentials. 
%Based on this observation, we present Tetris, an HPC+AI mix paradigm designed to achieve scalable data-level parallelism for Stencil by orchestrating data blocks on heterogeneous architecture, which enables large-scale scientific computing by offloading data and compute to GPU without requiring any theoretical change from scientists or sacrificing computational accuracy from GPU.
%Our approach efficiently relieves data alignment conflicts in SIMD manner on CPU, precisely design Convolution-analogous Tensor core acceleration in SIMT manner on GPU, and effectively eliminates high communication and offloading overheads on heterogeneous architectures.
%We compare Tetris against two state-of-the-art industrial systems on a diverse array of benchmarks. We
%demonstrate that Tetris achieves the same level of accuracy but is up to one order of magnitude faster. We
%also show that Tetris can scale training to datasets an order of magnitude beyond a single machine’s GPU and
%CPU memory capacity, enabling training of configurations with more than a billion edges and 550 GB of total parameters on a single machine with 16 GB of GPU memory and 64 GB of CPU memory.


%Stencil in HPC and Convolution in AI for scientific computing.
%exploiting  on heterogeneous architecture for 
%a fusion further improve the Stencil performance in HPC fusion of Stencil in HPC and Convolution in AI
%further improve tiled stencil performance by exploiting  on heterogeneous architecture. 

\section{Introduction}
%Over the past three decades, high-performance computing (HPC) has made rapid progress and plays an increasingly important role in scientific computing and other fields. 
%As Turing Award winner Jack Dongarra pointed out, the fusion of HPC and AI is the future trend. However, there still remains a significant gap between HPC and AI today both on algorithm and architecture level, making a scalable heterogeneous mix intractable and will even lead to performance degradation. 

%High performance computing is an important branch of computing science. Over the past three decades, HPC has been rapidly developed to solve Grand Challenge problems in science and engineering~\cite{GrandChallenges}, such as solving genetic mysteries on Summit~\cite{grandchallenge}, exploring seismic simulation on Sunway Taihulight~\cite{fu20179,fu2017redesigning}, and creating COVID-19 epidemic models on Fugaku~\cite{ando2021digital}, which utilize supercomputers to make significant progress. The global HPC market size is forecasted to reach USD 50.3 billion by the year 2028~\cite{hpcmarket}, and it plays an increasingly important role in more and more newly emerging areas. 
Over the past decades, scientific computing is prosperously developed to solve Grand Challenges~\cite{GrandChallenges}, such as solving genetic mysteries on Summit~\cite{grandchallenge}, exploring seismic simulation on Sunway Taihulight~\cite{fu20179}, and creating COVID-19 epidemic models on Fugaku~\cite{ando2021digital}, which utilize supercomputers to make significant progress.
%such as solving genetic mysteries on Summit~\cite{grandchallenge}, exploring seismic simulation on Sunway Taihulight~\cite{fu20179,fu2017redesigning}, and creating COVID-19 epidemic models on Fugaku~\cite{ando2021digital},} 
{%Stencil is one of the most important kernels widely used across a set of scientific and engineering applications. 
%As one of the seven computational dwarfs presented in the Berkeley View~\cite{asanovic2006landscape},  Stencil is ubiquitously involved in various large-scale scientific computing simulations~\cite{datta2008stencil}, which lies at the heart of thermal diffusion ($\sim$100\%), earth system model ($>$90\%), and earthquake prediction model ($>$90\%), etc~\cite{li2021reducing,xiao2018communication,bluman1969general,heatformula,xushun2021,denzler2021casper,christen2012patus}. 
}
%Stencil is also included as one of the seven computational dwarfs presented in the BerkeleyView~\cite{asanovic2006landscape} and arises as a principal class of floating-point kernels in high-performance computing.}

%{Generally, Stencil contains a pre-defined pattern that updates each point in $d-$dimensional spatial grid iteratively along the time dimension. The value of one point at time $t$ is a weighted sum of itself and neighboring points at the previous time~\cite{10.1145/1989493.1989508}. Characterized by this regular computational structure, Stencil is inherently a bandwidth-bound kernel with a low arithmetic intensity and poor data reuse, which makes it notorious for performance optimization in HPC community~\cite{frigo2005cache,10.1145/3126908.3126920,10.1145/1273442.1250761}. }


\begin{figure*}
  \begin{center}
  \centering
  \includegraphics[width=0.97\textwidth]{tetris_design_new.pdf}
  \caption{\label{fusion_overview}Tetris overview.}
  \end{center} 
\end{figure*} 

{Unfortunately, most scientific computing still relies heavily on traditional supercomputers~\cite{li2019openkmc,fu2017redesigning,fu20179}, and it hinders the democratization of science by three main obstacles. }
%Unfortunately, now most of the HPC systems still rely heavily on traditional supercomputers~\cite{li2019openkmc,fu2017redesigning,fu20179}, and it hinders the democratization of HPC by three main obstacles.
First, the access to a supercomputer is luxurious for users, and it is even prohibitively expensive to employ a full machine for {large-scale scientific computing}. 
Second, duplication of efforts is made on them by {engineers} case by case due to diverse supercomputer architectures.
Moreover, the scalability is poor as the assigned quota on a supercomputer is hard to adjust elastically.

%As Turing Award winner Jack Dongarra pointed out, the fusion of HPC and AI is the future trend. 
%Recently, AI-driven methods facilitate the use of accelerators like GPU to accomplish scientific research in the form of trained neural networks as surrogates to some expensive calculations like first-principles functions~\cite{jia2020pushing}.
Backed by advances in hardware and networking techniques, Cloud provides much higher performance scalability and cheaper access than traditional supercomputers. What is more, Cloud infrastructures, such as the ones from Microsoft Azure, Google Cloud, and Amazon Web Services~\cite{ogbole2021cloud,soh2020microsoft}, widely adopt unified CPU and GPU heterogeneous architecture, driving the prosperity of AI. Therefore, Cloud contributes a promising unified, high-performance, low-cost, and elastic infrastructure for {scientific computing}.

%Backed by advances in hardware and networking techniques that allow Cloud to provide much higher performance scalability and cheaper access than traditional supercomputers, Cloud computing that enables various applications to use remote resources are increasingly ubiquitous and appealing~\cite{alam2023cloud,gupta2021review}. More and more companies are turning to adopt CPU and GPU heterogeneous architecture on Cloud, such as Microsoft Azure, Google Cloud, and Amazon Web Services~\cite{ogbole2021cloud,soh2020microsoft}, which contributes a promising infrastructure to HPC for a unified design.  %as the solution for HPC-based productivity and innovation, such as Microsoft Azure, Google Cloud, and Amazon Web Services~\cite{ogbole2021cloud,soh2020microsoft}. 
 


 


%As for scientists, undoubtedly, it is not an easy job to penetrate into gigantic legacy codes for further change or optimization, and both HPC experts or expertise and supercomputers are barely handy resources for users~\cite{jia2020pushing}. 
%Some outdated work explored scientific computing on cloud with only CPU or GPU architecture, or a combination of them when their performance  
%However, none of the work now supports {scientific computing on heterogeneous Cloud}. The traditional techniques on supercomputers cannot be easily applied to the Cloud either, because of the significant differences in architecture. Though the AI surrogate models for {scientific computing}~\cite{jia2020pushing,cai2021physics,raissi2019physics} could potentially leverage the cloud resources, most applications cannot use AI models due to the precision loss~\cite{bailey2005high,bailey2012high,li2019openkmc}. 
However, none of the work supports {scientific computing on Cloud for heterogeneous processors}. The traditional techniques on supercomputers cannot be easily applied to the Cloud either, because of the significant differences in architecture. {Although the AI surrogate models for scientific computing~\cite{jia2020pushing,cai2021physics,raissi2019physics} could potentially leverage the cloud resources, the introduced precision loss is not afforded by diverse applications with the demanding taste of accuracy} ~\cite{bailey2005high,bailey2012high,li2019openkmc}. 


{Though the applications are diverse in scientific computing, analogy to AI, there are several common and performance-critical operations in scientific computing, named \textit{Dwarf}, defined by the Berkeley View~\cite{yuan2019tessellating}.} {As one of the seven computational Dwarfs, Stencil is ubiquitously involved in various scientific computing~\cite{datta2008stencil}, which lies at the heart of thermal diffusion ($\sim$100\%), earth system model ($>$90\%), and earthquake prediction model ($>$90\%), etc~\cite{li2021reducing,xiao2018communication,bluman1969general,heatformula,xushun2021,denzler2021casper,christen2012patus}.} 

%^{Therefore, taking Stencil Dwarf as a study case is particularly significant to pave the way for a new paradigm on Cloud.} 
%illustrate how Tetris works for scientific computing, and paves the way for a new  Paradigm on Cloud.}
To empower {scientific computing on Cloud, we take Stencil-driven scientific computing as a case study and} identify the following challenges.

%Since HPC techniques on supercomputers are architecture-specific, few pioneer works accelerate HPC research in the new form of trained neural networks as surrogates to some expensive calculations in an attempt to extend the generality of GPU~\cite{jia2020pushing,cai2021physics,raissi2019physics}. Although AI surrogate models provide possible candidates on Cloud, they only cover a narrow range of HPC applications and far from substitutes. To exploit the potential of leveraging Cloud for HPC, several key challenges are required to be well addressed.



%\noindent
\paragraph{Challenge \#1: Complicated Spatial and Temporal Data Dependencies.} 
 %Sophisticated Tiling.} 
%Different from regular AI computation, HPC is usually %numerically intensive 
%characterized with limited data input while immense data output after a great deal of FP64 calculations, where computational pattern of data is inherently 
{Scientific computing} is characterized by strong data dependencies in spatial and temporal dimensions since neighboring data are required to be collected to describe a phenomenon in a particular location and period of time, such as the computations of molecular motion, grid update, and interaction force among a variety of applications~\cite{li2019openkmc,yuan2019tessellating,fu20179,fu2017redesigning}. %in HPC applications like scientific computing is inherently dependent along spatial and temporal dimensions, place tremendous pressure on tiling by moving memory pressure from external memory to on-chip memory. 
%From an architecture perspective, 
% In HPC, the exhaustively studied tiling is one of the most powerful transformation techniques, and it explores the data locality and parallelism on CPU cache level with a well-designed shape~\cite{li2022efficient,yuan2019tessellating}.
% %high performance of HPC applications can only be achieved if data tiling could make optimal use of the multi-layered compute and memory hierarchies efficiently.
% However, correctly proposing a versatile tiling shape while considering the complexity of the architecture hierarchy and compute characteristics is an incredibly difficult task. Furthermore, the heterogeneous nature of Cloud containing multiple CPUs as well as accelerators (usually GPUs) makes it impossible to take full advantage of the available  resources afforded by Cloud with only a single tiling shape, which will severely degrade the performance of HPC applications and thus hinders unlocking the further potential on Cloud.
In {Stencil-driven scientific computing}, the exhaustively studied tiling is one of the most powerful techniques to improve data reuse and performance ~\cite{li2022efficient,yuan2019tessellating}. In general, they are designed for one type of memory on a specific hardware. However, there are different {on-Cloud memory types for CPU and GPU}, such as the registers and caches on the CPU and GPU, and the shared memory (SMEM) on GPU. One tiling shape cannot take full use of these resources, leading to severe performance degradation.
%There is no one versatile tiling shape that can take full use of these resources, which causes performance degradation.

\paragraph{Challenge \#2: Unfriendly On-Cloud Hardware Architecture.}%Inefficient Pipelining.} 
%\paragraph{Challenge \#2: Unfriendly On-Cloud Hardware Architecture for HPC. }%Inefficient Pipelining.} 
%To explore a higher performance on Cloud, it demands orchestrating the full use of specialized hardware units such as Vector Registers on CPUs and Tensor Cores on GPU for further acceleration. However, 
%these 
High-performance facilities such as Vector Registers on CPUs and Tensor Cores on GPU suffer from over specialization, as only specific operations are supported on them. However, the operators in {scientific computing} are highly diverse, and they cannot reap the benefits of these facilities trivially.
On the one hand, though vectorization serves as an effective means of utilizing SIMD facilities on CPU, the data alignment conflicts are the main performance-limiting factors introduced by vectorization for {scientific computing}~\cite{henretty2011data,henretty2013compiler}, where the neighbors for a grid point appear in the same vector register but at different positions~\cite{li2022efficient}. On the other hand, Tensor Cores are specially designed for deep learning, and only simple matrix multiplication (MM) of specific size is supported~\cite{pisha2021accelerating,dakkak2019accelerating}. To the best of our knowledge, no previous work adopts Tensor Cores for {scientific computing} as it is prominently challenging to adapt abundant non-MM operations to the only one setting of FP64 MM operations on Tensor Cores~\cite{dakkak2019accelerating}.
%Different from AI operators,  the computational patterns in HPC field are quite diverse except for GEMM~\cite{asanovic2006landscape}. Thus, HPC cannot reap the benefits of them for a pipelined computation due to abundant non-MM operations, and the only one setting for FP64 operations on Tensor Cores makes the challenges even harder.

\paragraph{Challenge \#3: Essential CPU and GPU Collaboration.}%Intractable Scheduling.} 
Most applications on the Cloud use CPU or GPU only due to the large performance difference. However, {scientific computing} calls for the collaborated CPU and GPU execution. 
%The collaboration between CPU and GPU on Cloud
%Nearly all of existing efforts on heterogeneous system enables a fast training by offloading data and compute to GPU entirely~\cite{ren2021zero}, and it is not designed to work in tandem with heterogeneous architectures on Cloud for HPC and presents scalability challenges. {the sentence above feels a bit strong. There are a bunch of reseach works to use CPU memory for model training too. what is the scalability challenge is not clear to me. Can we say like "Most applications on the Cloud use CPU or GPU only due to the large performance difference. However, HPC calls for the collaborated CPU and GPU execution." "}
%First, the memory requirements involved in HPC increase dramatically as tremendous data is produced in HPC applications. For example, the memory needed per simulated atom is approximately 0.72KB and a large-scale kinetic Monte Carlo(KMC) simulation system is contributed by nearly hundred-billion atoms~\cite{li2019openkmc}! Apparently, only employing aggregated GPU memory to hold all required data is a prohibitively expensive solution beyond the reach of many scientists~\cite{ren2021zero}.%, and offloading data and compute to CPU will result in significant replication of communication and CPU compute.
%As increasing the size of a neural network typically increases the memory and compute requirements, 
%Second, like with any distributed system, the steady state throughput of the whole heterogeneous architectures is crucially determined by the slowest worker. Popular stereotype is that the GPU computation throughput is several orders of magnitude faster than that of CPU computation throughput, and thus having CPU and GPU worked without an effective scheduling prevents full utilization of their computational performance. 
%Third, increased communication volume between CPU and GPU will complicate the compute process and lead to vast bubbles in compute pipeline, which aggravates a poor scalability.
%Therefore, the interests in more cost-effective and strong-scaling solutions within reach of scientists are highly increasing. 
Firstly, {scientific computing} requires huge memory costs. For example, the memory needed per simulated atom is approximately 0.72KB and a large-scale kinetic Monte Carlo (KMC) simulation system is contributed by nearly hundred-billion atoms~\cite{li2019openkmc}! It is very costly to use GPU memory only, which calls for the use of CPU memory. Secondly, different from AI computations, {scientific computing} requires vast {FP64 operations}. GPU has a similar performance as the CPU for this kind of computation, which calls for the collaborated execution of CPU and GPU. Third, the collaboration of CPU and GPU introduces increased communication which complicates the computing process, leading to bubbles in the pipeline.    

%Therefore, the interest in more cost-effective and scalable solutions on the same amount of processing power is highly increasing. 


\begin{comment}

%In Tetris, data movements are treated as first-class citizens with polymorphic data tetrominoes, which enables more flexible structures to boost higher performance in view of hardware characteristics. 
As a middle layer in memory hierarchy, an efficient tiling on cache in CPU (or shared memory in GPU) is challenging to meet the requirements of enhancing data reuse meanwhile matching the data tetrominoes efficiently in top and bottom memory hierarchies.
 
Since modern high-performance CPU is always equipped with multicore or manycore configuration, the data space is required to be tiled spatially to enable fully parallelism such that all data blocks between synchronizations can execute concurrently without redundant computation. Moreover, temporal tiling is also essential to increase data reuse before sweeping the data from cache, and appropriate tetrominoes are constructed accordingly to tessellate the time dimension for synchronization. 

 
Efficiently using the GPU’s compute and memory hierarchy requires the coordinated data movements. Typically, the results are generated and distributed across the registers of its threads. In order to achieve fully parallelism on available warps and coalesced writes to global memory, it is crucial to adopt blocking in shared memory for data coordination. Furthermore, it is required to address two major different conflicts on shared memory. One is the bank conflict triggered by hardware mechanism, and the other is block conflict introduced by interblock compute dependencies, which complicates the blocking strategy and hinders unlocking higher performance. 

%More and more companies are also turning to heterogeneous architectures as the solution for HPC-based productivity and AI-enabled innovation, such as Microsoft Azure, Google Cloud TPU, and Amazon AWS AIHPC.%, which augments the high growth of the HPC Market in recent years.
%HPC Dwarfs are defined as seven classical algorithmic methods in HPC field for scientific computing by the Berkeley View.
%At present, it is customary that more scientific computing scenarios in HPC are fused with AI models and methods on heterogeneous architectures, 
%e.g., Physics-informed Neural Networks (PINNs) that are trained to solve supervised learning tasks on GPUs while respecting specific laws of physics described by partial differential equations on CPU. 
%Typically, this representative AI-driven scientific computing like PINNs is achieved in a loosely coupled manner, where the HPC and AI infrastructures~\footnote{We use CPU and GPU as the concrete instances of HPC and AI infrastructures respectively.} do each duty separately. Although the existing fusion paradigm has offered an appreciable acceleration to classical scientific computing, it still leaves various unaddressed challenges to tackle.% due to the existing simplistic fusion on algorithm level. 

%Scalability issues are at the heart of high performance computing, and it is also an important challenge that ranks first among the twelve research goals in the future information technology field.
 %The computing streams are not shared between HPC and AI infrastructures and they only do each duty separately and exclusively. shared, which is unfriendly to a good scaling .


%The convergence of AI and HPC promises to transform the scientific computing landscape, with its potential to enable research groups to tackle challenges that would otherwise have been beyond their capabilities.
%HPC and AI computing have both similar and different characteristics. HPC is usually numerically intensive, with limited data input while immense data output after a great deal of double-precision numerical calculations in scientific simulation scenarios. In the field of AI, immerse data input is typically required, but relatively small output data is obtained by calculations of single or mixed precision in various scenarios. 

%However, the existing fusion paradigm of HPC+AI is combined straightforward in algorithm-level parallelism. For example,  and still leaves various unaddressed challenges. 
\paragraph{Bridge the Accuracy Gap.}
%First, the current fusion prevents engineers from closing the remaining performance gap between CPU and GPU because the scientific data are treated as second-class citizens: they are coordinated for training AI models or algorithms instead of an adapted structure for boosting performance. 
%Second, the scientists are generally dissatisfied with the result accuracy reported from introduced AI-driven methods. Most of the existing AI-driven HPC applications obtain better efficiency at the cost of accuracy by alternating traditional theoretical calculation with deep learning models. 
Scientific computing itself is fundamentally based on the pillars of accuracy~\cite{10.1145/3126908.3126920,li2021reducing}. 
However, most of the existing AI-driven methods obtain better efficiency at the cost of accuracy by alternating numerical calculation with approximate calculation by AI models. 
Even though acceptable accuracy is hyped in some work, the convergence tolerance is actually slacked and meets the strict taste of scientists hardly from physical observable, especially in the domains of weather forecast, galaxies collision, nuclear reactors, etc~\cite{li2019openkmc,xiao2018communication}. 
For example, small chaos, described as the “butterfly effect” in weather forecast, make weather less predictable in complex processes, thus unrealistic results are easily obtained by AI prediction like forecasting temperature extremes beyond the bounds of nature. % even in numerical simulation.
%Unlike numerical models, AI-driven models are rarely constrained by all physical laws that govern the atmosphere, thus they produce unrealistic results easily like forecasting temperature extremes beyond the bounds of nature. 
Besides, most scientific applications involved in vast high-precision modules such as weather forecast, materials design and drug discovery can't be accomplished by AI-driven methods alone, where AI models work as surrogates partially not substitutes entirely.
% which require AI to be combined with high-precision scientific computing.
Furthermore, though the AI-driven methods have their own place, it is also not a panacea for all scientific computing. %especially the simulation comprised of massive computation of HPC Dwarfs. HPC Dwarfs are defined as seven classical algorithmic methods in HPC field by the Berkeley View, which earn the name by acting as performance footdraggers in serving theoretical calculations.
Exemplifying weather forecast, locations of relatively few weather observations or poor data diversity with which to train an AI model forcibly may not benefit from it, which will distort the quality of simulation severely.


\paragraph{Break the Performance Wall.}
%Essentially, 
First, it is a popular stereotype that the GPU computation throughput is several orders of magnitude faster than that of CPU computation throughput, and it is still hardly well-addressed yet~\cite{ren2021zero}. Thus, narrowing performance gaps between HPC and AI infrastructures is a key issue for a good scalability under scientific computing circumstances, especially for high-precision parts.
Second, from an overall perspective, though the use of AI surrogates within HPC simulations improves time-to-solution by replacing accurate first-principles functions with approximations, the whole computing stream is still a serial process switching across CPU and GPU to and fro, which leads to a poor scalability across heterogeneous architectures~\cite{jia2020pushing}.  Furthermore, AI-free scenarios make GPU an awkward situation in numerical calculation, where GPU is a hero without battlefield for its weak capacity in high-precision computing and thus typically wasted in-idle or employed under-utilized.
%It is a popular stereotype that the GPU computation throughput is several orders of magnitude faster than that of CPU computation throughput. In practice, the capacity of performing double precision (FP64) operations on GPU is not good as half or single precision (FP16, FP32)


%Furthermore, since the GPU is naturally designed for AI tasks, it fails to unlock higher performance to share the computing burden of HPC simulation, and thus leads to a poor scalability across heterogeneous architectures.

\paragraph{Lower the Democratization Bar.}
Most science applications are necessarily powered by HPC techniques. Actually, a great deal of HPC patterns for science could be captured and abstracted as a Dwarf~\cite{asanovic2006landscape}, which is ignored by most of the existing work and duplication of efforts are still made on supercomputers by HPC experts case by case now.
As for scientists, undoubtedly, it is not an easy job to penetrate into gigantic legacy codes for further change or optimization by them, and both HPC experts or expertise and supercomputers are barely handy resources for users~\cite{jia2020pushing}.
Recently, AI-driven methods facilitate the use of GPU to accomplish scientific research in the form of trained neural networks as surrogates. 
However, With the orders of magnitude growth in simulation size, the memory requirements involved in scientific computing scenarios also increase dramatically. For example,  the memory needed per simulated atom is approximately 0.72KB and a large-scale kinetic Monte Carlo(KMC) simulation system is contributed by nearly hundred-billion atoms~\cite{li2019openkmc}!
Apparently, employing aggregated GPU memory to hold all required data is a prohibitively expensive solution beyond the reach of many scientists~\cite{ren2021zero}.
Therefore, the interests in more general-to-domains, easy-to-use and cost-effective solutions within reach of public are highly increasing. 

\end{comment}
%For the most part, HPC occurs on traditional supercomputers with massive CPUs. 
 %A series of challenges on computation and communication are also derived when employing GPUs for HPC Dwarfs acceleration. 
%Existing efforts on heterogeneous architectures are further limited in two major ways: utilizing the batched algorithm to transfer data between CPUs and GPUs, or employing aggregated GPU memory to hold all required data. The first one could exploit CPU memory, but not CPU compute, to relieve the requirements for the number of GPUs while suffering heavy communication traffics. The second one is easy to implement while the GPU dependence is prohibitively expensive beyond the reach of many scientists.

%Besides, AI-driven methods, such as deep learning, are often expensive to train and require large amounts of data. It is therefore hardly applied to the areas where AI-driven methods show little performance advantage to classical methods while introducing additional training errors.

%Moreover, there is also a tricky measurement in a considerable amount of work that how the training data is generated, and whether the related cost is taken into account when benchmarking.
%As there's no shortcut to rigorous theoretical calculation under these circumstances, the high demand for accuracy of scientists makes GPU an awkward situation in AI-free scenarios, where GPU accelerator is a hero without battlefield for its weak capacity in high-precision computing and thus wasted in idle. 
%Therefore, the interest in more cost-effective and scalable solutions on the same amount of processing power is highly increasing. 


%the representative AI-driven scientific computing is typically achieved in a loosely coupled manner, where the AI models are trained on GPU while HPC simulations are restricted to CPU separately. The computing power of AI infrastructures provides no contribution to HPC simulation, and the scientists are even dissatisfied with the precision of introduced AI-driven results at times. On the other hand, although the AI-driven methods have their own place, it is not a panacea for all algorithms especially classical HPC Dwarfs, and there's no substitute for traditional theoretical calculation in most scientific computing scenarios. The high demand for accuracy of scientists makes heterogeneous architecture an awkward situation in AI-free scenarios, where GPU accelerator is a hero without battlefield for its weak capacity in high-precision computing and thus wasted in idle. Therefore, the interest in more cost-effective solutions on the same amount of processing power is highly increasing. 


%Generally, the AI-driven scientific computing applications could obtain a better efficiency at the cost of accuracy by alternating traditional theoretical calculation with deep learning models. Most research achieves the fusion of HPC and AI in a loosely-coupled manner, where the AI models are trained on GPU while HPC simulation are computed on CPU separately. Although the AI-driven methods have their own place, there's no substitute for traditional theoretical calculation in most scientific computing scenarios. 
%The high demand for accuracy of scientists makes heterogeneous architecture in an awkward situation, where GPU accelerator is a hero without battlefield for its weak high-precision computing power. %  leading a poor scalability across architectures and limited generality to applications. 
%Moreover, . In addition, data centers are now a mix of
%HPC-systems that often include accelerators such as GPUs.
%Therefore, the demand and interest for more cost-effective solutions for the same amount of processing power is highly increasing, while there still remains a significant gap between HPC and AI today both on algorithm and architecture level, making a scalable heterogeneous mix intractable and will even lead to performance degradation or precision loss. 

%Stencil computation is one of the most important kernels in various scientific and engineering applications, which is also listed as one of the seven HPC dwarfs presented in the Berkeley View.
%A stencil contains a pre-defined pattern that updates each point in $d-$dimensional spatial grid iteratively along the time dimension. The value of one point at time $t$is a weighted sum of itself and neighboring points at the previous time. 
%Characterized by this regular computational structure, Stencil is inherently a bandwidth-bound kernel with a low arithmetic intensity and poor data reuse, which makes it notorious for performance optimization.
%Originated from the finite difference method (FDM) based on this regular grid, Stencil is extensively employed to solve partial differential equation (PDE) involved in various domains from HPC physical simulations to AI image processing, posing a number of challenges on modern heterogeneous architectures to achieve a high performance. 
%The optimization strategy also varies significantly across algorithms, architectures, and types of applications.  
%To explore a scalable heterogeneous mix paradigm, we focus on digging into data-level parallelism for Stencil in scientific computing scenarios on heterogeneous architecture, which demands orchestrating efficient use of wide SIMD units in CPUs and SIMT threads in GPU.


%Vectorization serves as an effective means of utilizing the SIMD facilities in CPU to perform multiple data processing in parallel, and it has been exhaustively investigated in the literature for Stencil computation.
%Data alignment conflicts are the main performance-limiting factors caused by vectorization, where the neighbors for a grid point appear in the same vector register but at different positions.
%To address the problem of spatial conflicts, two common strategies are often adopted. The first one loads all the needed elements from memory in a vector form straightforward. Compared with the scalar code, this multiple load method further increases the data transfer volume. Moreover, in each iteration of this code, it has at least two unaligned memory references where the first data address is not at a 32-byte boundary. Since CPU implementations favor aligned data loads and stores, these unaligned memory references will degrade the performance considerably.
%The second solution is similar to the scalar code in terms of the CPU-memory data transfer. It loads each input element to vector register only once and assembles the required vectors via data permutation instructions provided by SIMD facilities. 
%Compared with the multiple load method, this data permutation method reduces the memory bandwidth usage and takes advantage of the rich set of data-reordering instructions supported by most SIMD architectures. However, the execution unit for data permutations inside the CPU may become the bottleneck.

%As mentioned above, developing high-performance kernels for GPUs is challenging because of their unskilled double-precision computing capabilities. Significant efforts have been devoted to optimize Stencil computation on GPU, including both manual optimization and code generation framework, to enhance the data-level parallelism by utilizing SIMT facilities on GPU. Nonetheless, the current approaches prevent experts from closing the remaining performance gap because they treat GPU as second-class citizens: Only AI-driven methods are trained on them while the time-consuming theoretical calculations are still reserved on CPUs.
%From NVIDIA Ampere GPU architecture, the newly-introduced specialized hardware units known as NVIDIA’s Tensor Cores could offer a full range of precisions to boost the performance of general matrix multiplication (GEMM) even with the highest accuracy needed (double precision). 
%However, since the Tensor Core Units (TCUs) are originally designed for accelerating GEMM, the representative computing patterns in HPC field like Stencil are different from AI methods and not supported by TCUs straightforward.  To unlock the potential of TCUs without precision discrimination, we draw an analogy to Convolution in AI field because of their similarity in computing pattern, and adapt calculations dynamically to accelerate throughput while preserving accuracy. To the best of our knowledge, there is no previous work that adapts Stencil computation to TCUs efficiently by considering the analogy to Convolution and unique characteristics of hardware.

% The optimization strategy varies significantly across architectures, types of stencils, and types of applications. 

%In addition to the accuracy challenge, the limited GPU memory is also a bottleneck to achieve scalable heterogeneous mix paradigm. With the orders of magnitude growth in simulation size, the memory requirements involved in scientific computing scenarios increase dramatically. %For example,  the memory needed per simulated atom is approximately 0.72KB i 
%Existing efforts on heterogeneous architectures are further limited in two major ways: utilizing the batched algorithm to transfer data between CPUs and GPUs, or employing aggregated GPU memory to hold all required data. The first one could exploit CPU memory, but not CPU compute, to relieve the requirements for the number of GPUs while suffering heavy communication traffics. The second one is easy to implement while the GPU dependence is prohibitively expensive beyond the reach of many scientists.


%To address these challenges, we present Tetris, an HPC+AI mix paradigm designed to achieve scalable data-level parallelism for Stencil by orchestrating data blocks on heterogeneous architecture, which enables large-scale scientific computing by offloading data and compute to GPU without requiring any theoretical change from scientists or sacrificing computational accuracy from GPU. Tetris is inspired by a classic game of sliding blocks (also called tetrominoes), where shaped tetrominoes are just kept piling onto old ones to fill the vacated spaces, and motivated by three key observations illustrated previously: SIMD on CPU, sIMT on GPU, and their collaborations on heterogeneous architectures.
%{\paragraph{State of the Art.} In the past, supporting scientific computing that contains complicated spatial and temporal data dependencies is not the priority of Cloud. Although there exists few work years ago to explore scientific computing on Cloud, such work is obsolete today with a growing architecture gap between CPU and GPU on heterogeneous Cloud. Techniques on supercomputers focuses primarily on improving tiling for one type of memory on a specific hardware with a tiling shape. However, there are different memory types and distinct hardware units for CPU and GPU on Cloud; as a result, developing new tiling techniques to support scientific computing becomes a crucial task.}


{\paragraph{\textit{Tetris.}} This paper proposes Tetris, the first system for high-performance Stencil on heterogeneous CPU+GPU, towards democratizing Stencil-driven scientific computing on Cloud.}%tessellate the spatial and temporal dimensions perfectly for democratizing Stencil-driven scientific computing on Cloud.}

%This paper attempts to democratize {scientific computing} in a new paradigm on the Cloud. Taking the widely-used Stencil Dwarf into account, we propose Tetris, the first high-performance Stencil system in  for scientific computing on the Cloud.

{The design of Tetris is based on two key observations.}

{First, changing data-update order would not break the application semantics. Specifically, Stencil aims to update data with restricted neighbors along the time dimension, while the updating order of data does not matter.}

{Second, creating a breakpoint would not destroy the computational logic. The dependencies of a point can be decomposed and computed partially first, and the remaining dependencies are collected at a postponed time.}

%the data-access order in updating a point, is still not necessarily aligned spatially or temporally at once. Similarly, the dependencies could be decomposed carefully and computed partially 

%Second, the updating of the current data in a time step, is still not necessarily aligned spatially or temporally at once. Similarly, the dependencies could be decomposed carefully and computed partially 
%the data in different location, or different time step, are not necessarily aligned spatially or temporally at once. An appropriate postponed alignment on data with different updating states is more elastic and makes no difference to the results.%A maximal updating is performed on the current memory hierarchy, and the  postponed at a scheduled point during the execution like synchronization.

{Guided by these observations, the principal insight is: the data are not necessarily aligned spatially or temporally in a regular order; similarly, the calculation of the current point in a time step is also allowed with a brief pause for tessellating other calculations.}
%alternating other scheduled operations.}
%allowed to update in different location and different time step
%, are allowed updated in  not necessarily aligned spatially or temporally at once. Similarly, the updating of the point in a time step is also allowed with a brief pause for other scheduled calculations.}

%Though there are diverse applications, analogy to AI, there are several common and performance-critical operations in {scientific computing}, named \textit{Dwarf}, defined by the Berkeley View~\cite{asanovic2006landscape}. {As one of the seven computational Dwarfs, Stencil is ubiquitously involved in various scientific computing~\cite{datta2008stencil}, which lies at the heart of thermal diffusion ($\sim$100\%), earth system model ($>$90\%), and earthquake prediction model ($>$90\%), etc~\cite{li2021reducing,xiao2018communication,bluman1969general,heatformula,xushun2021,denzler2021casper,christen2012patus}.} 
%{Tackling the aforementioned challenges and taking the widely-used Stencil Dwarf into account, we propose Tetris, the first high-performance Stencil system for scientific computing on the Cloud.} 
% the first HPC Paradigm on Cloud.% that bridges the accuracy gap in scientific computing, breaks the performance wall between CPU and GPU, and lowers the democratization bar for users once and for all. 
  
{Based on this insight, the key design of Tetris is \textbf{\textit{tiling tetrominoes tessellation}}}. The high-level idea is inspired by a classic Tetris game of sliding blocks (named tetrominoes). In a Tetris game, different tetrominoes are just kept piling onto old ones first to fill the vacated spaces, {until they are aligned in a line. Analogously, with polymorphic tiling tetrominoes on different hardware hierarchies, Tetris carefully reorders the updating objects or marks a breakpoint on the partially-computed points, allowing a maximal updating is performed locally first while aligned spatially and temporally at a scheduled time point.}
%On different hardware hierarchies, Tetris carefully reorders the updating objects, allowing a maximal updating is performed locally while aligned spatially and temporally with data from others. In Stencil computation of scientific computing, Tetris marks a breakpoint on the points only partially computed, enabling a maximal dependency is collected locally while updated completely with computation from others. All of these are achieved in a way that polymorphic tiling tetrominoes are used to tessellate the spatial and temporal dimensions perfectly, which bridges different hardware architectures and various application contexts.
%on different architectures and application contexts automatically.
%Consequently, polymorphic tiling tetrominoes to explore a fully-tessellated tiling technique to bridge different hardware architectures and various application contexts automatically. 



 

%Tackling the aforementioned challenges, we attempt to democratize  HPC applications in a new paradigm on the Cloud. %by developing a new HPC Paradigm.%, allowing scientists to easily perform an accuracy-satisfied simulation and engineers to achieve performance-boosting optimization simultaneously. %The system design is based on three key observations.
%Specifically, In HPC field, Dwarfs are defined as a series of classical algorithmic methods by the Berkeley View~\cite{asanovic2006landscape}. 
%As one of the seven Dwarfs, Stencil contains a pre-defined pattern that updates each point with neighbors in $d-$dimensional spatial grid iteratively along the time dimension.% which
%Originated from the finite difference method (FDM) based on regular grids, Stencil 
%It is ubiquitously inlvolved %extensively %employed% to solve partial differential equation (PDE) 
%involved in various HPC applications and ubiquitous in scientific computing in various HPC applications~\cite{datta2008stencil}, which lies at the heart of thermal diffusion ($\sim$100\%), Earth System Model (ESM) ($>$90\%), and earthquake system model ($>$90\%), etc~\cite{xiao2018communication,bluman1969general,heatformula,xushun2021,denzler2021casper,christen2012patus}. 
%Guided by these observations and 
%Taking widely-used Stencil Dwarf into account, we propose Tetris, the first HPC Paradigm on Cloud.% that bridges the accuracy gap in scientific computing, breaks the performance wall between CPU and GPU, and lowers the democratization bar for users once and for all. 
%It is inspired by a classic game of sliding blocks (also called tetrominoes), where shaped tetrominoes are just kept piling onto old ones to fill the vacated spaces, and motivated by three key challenges stated above on Cloud. As shown in Figure~\ref{fusion_overview}, Tetris consists of three main components:%: Sophisticated Tiling, Scalable Scheduling, and Algorithmic Adaptation. %To the best of our knowledge, this data-level fusion of HPC+AI is first explored for accelerating scientific computing without any accuracy loss.


%To tackle these challenges, the design of Tetris is based on three key observations.







%Taking widely-used Stencil Dwarf into account, we propose polymorphic data tetrominoes to boost the tiling performance through the whole memory hierarchy on Cloud, design an efficient architecture-aware scheduler to break the performance wall between CPU and GPU, and make full use of specialized hardware units on heterogeneous architecture for accelerating a broader range of scientific computing applications. %To the best of our knowledge, this data-level fusion of HPC+AI is first explored for scientific computing. 

{As a crucial thread in Tetris, tiling tetrominoes run through the whole memory hierarchy on heterogeneous processors of Cloud. Figure~\ref{fusion_overview} illustrates that Tetris consists of three main components: }

\paragraph{\textit{{Pattern Mapping by Register-level Tetrominoes}} }
%\paragraph{\textit{Sophisticated Dwarf Adaptor} }
%{High-performance unit adaptor}
%Developing HPC Dwarfs in scientific computing on GPUs is challenging because of their unskilled double-precision computing capabilities. From NVIDIA Ampere GPU architecture, the newly-introduced specialized hardware units known as Tensor Core Unit (TCU) could offer a full range of precision to boost the performance of general matrix multiplication (GEMM) even with the highest FP64 accuracy needed, promising a potential to leverage GPUs for accelerating HPC Dwarfs in scientific computing.
%Significant efforts have been devoted to optimize HPC Dwarfs on GPU, including both manual optimization and code generation framework. Nonetheless, the current approaches prevent experts from closing the remaining performance gap because they treat GPU only as an AI agent: Only those AI-driven methods stand a chance to utilize the GPU potential while the demanding theoretical calculations are still reserved on CPUs.
%However, since the Tensor Core Units are originally designed for accelerating GEMM, the representative computing patterns in HPC Dwarfs like Stencil are radically different from AI methods and not supported by TCUs straightforward. 
Conventionally, only the workloads suitable for vectorization or MM operations are taken into account for offloading to specialized hardware units in existing work. %Our idea is rooted in the deep observation that a 256-bit YMM register is actually promoted by two separate data lanes aliased to two 128-bit XMM registers, and thus compute pipeline could be vectorized efficiently in a new design free of expensive cross-lane SIMD instructions for the first time (typically the CPU latency decreases from 3 to 1 per instruction~\cite{guide21url}). 

%To efficiently exploit the specialized units such as Vector Registers and Tensor Cores on Cloud, 
%without performance degradation and Tensor Cores without accuracy discrimination, Tetris %draws an analogy with Convolution in AI field to
%Guided by these observations, 
Tetris captures the underlying hardware characteristics and {designs distinctive tetrominoes to perform a sophisticated pattern mapping from Stencil computation to vectorization and MM operations}.
Briefly, (1) Vector Skewed Swizzling is designed on Vector Registers, which %throws away the routine thoughts, breaks new ground from the observable of register architecture, and 
utilizes skewed tetrominoes for building a conflict-free vectorized pipeline. It is the first design free of expensive cross-lane SIMD instructions (the CPU latency decreases from 3 to 1 per instruction~\cite{guide21url}). %Compared with existing methods, the Vector Skewed Swizzling achieves the first one to abandon expensive cross-lane operations, and the whole vectorized process is pipelined efficiently meanwhile with more registers reused and saved.
(2) Tensor Trapezoid Folding is proposed on Tensor Cores, and it employs stair tetrominoes to adapt non-MM operations in {Stencil} as a series of reduction and summation operations. It can leverage the computing power of Tensor Cores for MM meanwhile preserving high accuracy (FP64).
%(2) Tensor Trapezoid Folding is proposed on GPU, and it employs stair tetrominoes to leverage the computing power of TCU meanwhile preserving high accuracy (FP64). Similarly, non-MM operations in HPC could also be adapted carefully as a series of reduction and summation operations to enable accelerating a broader range of HPC workloads on Tensor Cores.%to leverage the computing power of TCU by adapting HPC calculations with a series of matrix multiplication (MM) operations for an efficient compute pipeline meanwhile preserving high accuracy (FP64). %To the best of our knowledge, Tensor Trapezoid Folding is also the first disruptive work that adapts high-precision HPC calculations to TCU efficiently by considering its unique characteristics.   %there is no previous work that finds out the root cause of on CPU and adapts HPC computation in scientific computing to TCUs efficiently %by drawing an analogy to AI models and adopting unique characteristics of hardware with high accuracy.
%{(2) Tensor Trapezoid Folding is proposed on GPU, and it employs stair tetrominoes to adapt non-MM operations in HPC as a series of reduction and summation operations. It can   leverage the computing power of TCU for MM meanwhile preserving high accuracy (FP64).} %Similarly, non-MM operations in HPC could also be adapted carefully as a series of reduction and summation operations to enable accelerating a broader range of HPC workloads on Tensor Cores.}

\paragraph{\textit{ {Locality Enhancer by Cache/SMEM-level Tetrominoes} } }%Polymorphic
%As a middle layer in memory hierarchy, an efficient tiling on cache in CPU (or shared memory in GPU) is challenging to meet the requirements of enhancing data reuse meanwhile matching the data tetrominoes efficiently in top and bottom memory hierarchies.
 
%Since modern high-performance CPU is always equipped with multicore or manycore configuration, the data space is required to be tiled spatially to enable fully parallelism such that all data blocks between synchronizations can execute concurrently without redundant computation. Moreover, temporal tiling is also essential to increase data reuse before sweeping the data from cache, and appropriate tetrominoes are constructed accordingly to tessellate the time dimension for synchronization. 

%As a middle layer in memory hierarchy, cache in CPU (or shared memory in GPU) offers a small but fast data buffer for a potential performance improvement to avoid abundant memory accesses~\cite{li2022efficient}. 
%Based on this characteristic, Tetris achieves an efficient tiling adaptor for data reuse, which receives the coarse-grained scientific data from memory and adapts it to medium-grained tiled data blocks on cache (or shared memory) with shaped tetrominoes, and then provides fine-grained data tetrominoes to registers.

Instead of the conventional perspective that employs a single tiling shape to {improve data locality, Tetris achieves an efficient locality enhancer with polymorphic tiling tetrominoes for data reuse on spatial and temporal dimensions simultaneously.} %, we propose polymorphic tiling tetrominoes to shuttle through memory hierarchies on the CPU and GPU. 
%It exploits the characteristics of memory hierarchies, computational patterns, and hardware architectures.  
{In the design of locality enhancer, the coarse-grained tetrominoes from memory could be tessellated as an aggregate unit of  tetrominoes on cache (or SMEM). Similarly, cache/SMEM-level tetrominoes can be decomposed as fine-grained tetrominoes to registers.
This flexible layout makes it possible for {scientific computing} adapted automatically to different architectures and application contexts simply by adjusting the number of data tetrominoes on cache (or SMEM).} 

%{Specifically, Locality Enhancer enables the spatial tiling with fully parallelism such that all tetrominoes between synchronizations can execute concurrently without redundant computation. Temporal tiling is then also achieved by multidimensional tetrominoes for tessellating the time dimension additionally. }
%The spatial and temporal dimensions are then tessellated as an aggregate unit of data tetrominoes.


%For example, skewed tetrominoes are designed in registers, and aggregates of them are tessellated as triangle tetrominoes for 2D spatial tiling. Then they are stacked into 3D tetrahedron tetrominoes for temporal tessellation in memory. At last, the two parts sliced on CPU and GPU are tessellated together for a complete result.
% whose size can be tailored to different memory hierarchies, allowing flexible mappings to parallel compute units and exploiting more efficient data reuse to boost higher performance.  }
%First, %unlike previous work that mostly blocks the spatial space directly, 


%We change the conventional perspective that employing a single tiling shape to meet all requirements raised from hardware or application. Conversely, the characteristics of memory hierarchies, computational patterns, and hardware architectures are deeply exploited for maximum use of memory and computing power on Cloud,
%guide the way for   to the fullest extent. %proposing
%the potential could be further exploited by combining the hardware architecture of multi-layered compute and memory hierarchies in tandem with computation pattern characteristics in HPC field. %Instead of employing a single tiling shape, 
%a set of flexible blocks with different shapes.  aimming to tessellate the spatial and temporal space perfectly in the whole iteration space.% and extend along the time dimension to form a tessellation of the iteration space at different memory hierarchies.
% of spatial and temporal dimensions %in HPC applications like scientific computing is inherently dependent along spatial and temporal dimensions, place tremendous pressure on tiling by moving memory pressure from external memory to on-chip memory.  
%high performance of HPC applications can only be achieved if data tiling could make optimal use of the multi-layered compute and memory hierarchies efficiently.
%As a middle layer in memory hierarchy, cache in CPU (or shared memory in GPU) offers a small but fast data buffer for a potential performance improvement to avoid abundant memory accesses~\cite{li2022efficient}. 
%Based on this characteristic, Tetris achieves an efficient tiling adaptor for data reuse, which receives the coarse-grained scientific data from memory and adapts it to medium-grained tiled data blocks on cache (or shared memory) with shaped tetrominoes, and then provides fine-grained data tetrominoes to registers.

%On modern heterogeneous architectures, achieving optimal performance essentially boils down to efficient data tiling through memory hierarchy. 
%In Tetris, polymorphic tiling tetrominoes are proposed to shuttle through memory hierarchies on Cloud, and this flexible layout makes it possible for HPC adapted automatically to different architectures and application contexts simply by adjusting the data tetrominoes. The spatial and temporal dimension is then tessellated as an aggregate unit of data tetrominoes whose size can be tailored to different memory hierarchies, allowing flexible mappings to parallel compute units and exploiting more efficient data reuse to boost higher performance. 

%In order to unlock the highest performance, it is crucial to define precise mappings of both, computations to parallel compute units but also how data movements are coordinated through the memory hierarchy


\paragraph{\textit{ {Concurrent Scheduler by Memory-level Tetrominoes} }}
%\paragraph{\textit{Concurrent Heterogeneous Scheduler}}
%Typically, GPU computation throughput is multiple orders of magnitude faster than CPU in deep learning, while that is not the case for HPC workloads.% due to  with a deep exploitation on system. 
%The performance of GPU degrades seriously for double-precision operations that are extensively involved in HPC, and parallel computing techniques on multiple CPU cores boosting with high-performance SIMD facilities could also close the remaining performance gap.
%In addition to the accuracy challenge, the limited GPU memory is also a bottleneck to achieve a scalable HPC+AI mix paradigm. With the orders of magnitude growth in simulation size, the memory requirements involved in scientific computing increase dramatically. For example, the memory needed per simulated atom is approximately 0.72KB and a large-scale kinetic Monte Carlo(KMC) simulation system is contributed by nearly hundred-billion atoms! %A series of challenges on computation and communication are also derived when employing GPUs for HPC Dwarfs acceleration. 
%Existing efforts on narrowing memory gaps between HPC and AI infrastructures are categorized as two major ways: utilizing the batched algorithms to transfer data between CPUs and GPUs, or employing aggregated memory of more GPUs to hold all required data. The first one could leverage CPU memory for more data space, but not CPU compute, to relieve the requirements for the number of GPUs while incurring heavy communication traffics. The second one is easy to implement, while the GPU requirements are prohibitively expensive beyond the reach of many scientists.


Tetris proposes a novel concurrent scheduler designed specifically for {scientific computing} on the CPU and GPU. It identifies the implicit and explicit dependencies for {Stencil} workloads between CPU and GPU, {and a two-way partitioning is performed on input to obtain memory-level tetrominoes. } 
Concretely, (1) it explores a bidirectional design for squeezing memory consumption by maximizing memory savings on GPU while exploiting CPU memory for simulation.
(2) To preserve compute efficiency, a balanced {tetrominoes} partitioning strategy is designed to achieve orders-of-magnitude equal computation on CPU compared to GPU, preventing the CPU compute from becoming a performance bottleneck. (3) Moreover, the communication of {boundary tetrominoes} between CPU and GPU is significantly reduced to enable better scalability.

%To enable scalable collaborations on heterogeneous architectures, we propose Zero Heterogeneous Offloading in Tetris. 
%to better overlap data copy and computation

%Kernels, and the implicit and explicit dependencies between them, can be abstracted at a high level as a Directed Acyclic Graph (DAG). The memory and computational dependencies can then be determined by the library allowing optimisation of kernel-scheduling to minimise the overall latency of the computation.
%It contains 3 apparent benefits: (1) Bridging the performance and precision gaps between HPC and AI by utilizing the high-precision computing parts on AI infrastructures for HPC workloads; (2) Building a general abstraction of data-level fusion of HPC+AI on seven classical HPC Dwarfs for accelerating a broader range of scientific applications; (3) More performance-portable on heterogeneous architectures, especially on HPC Cloud with AI infrastructures like Microsoft Azure.


%by exploiting partially equivalent transformations. 
%allowing us to easily 



%Tetris is not the first to address data alignment conflicts on CPU for SIMD implementation. While there are many available vectorized methods, a majority of them have focused on reducing memory access, decreasing register permutation, or seeking a balance between them, which fails to dig out the performance-limiting factor fundamentally. In Tetris, a pioneering Vector Skewed Swizzling strategy is proposed to alleviate data alignment conflicts effectively based on the characteristics of register lanes. 
%Specifically, Tetris performs mappings of vector registers to tetrominoes that can be manipulated in data movements to and from registers at the cost of low-latency in-lane operations. Compared with existing methods, the Vector Skewed Swizzling requires no expensive cross-lane operations in registers, and the whole vectorized process is pipelined efficiently with more registers saved.


%Efficiently using the GPU’s compute and memory hierarchy requires the coordinated application of multiple optimizations. With the introduction of Tensor Cores, Tensor Trapezoid Folding is proposed in Tetris to accelerate non-GEMM operations for high-precision HPC support, where we draw an analogy between the Stencil and Convolution to leverage the computing power of TCUs. In addition, we also present corresponding optimizations for data movements coordinated through the memory hierarchy, and it achieves more efficient reads and writes.


%To enable scalable collaborations on heterogeneous architectures, we propose Zero Heterogeneous Offloading in Tetris. To preserve compute efficiency, it is designed to minimize the data movement to/from GPU by inbuilt Strip area and reduce CPU compute time  while maximizing memory savings on GPU. Moreover, the communication between CPU and GPU is further stripped from computation and overlapped with fine-grained stream control.


%The Tetris is evaluated with AVX2 and WMMA instructions on CPU and GPU respectively for 1D, 2D, and 3D stencils. The results show that Tetris is obviously competitive with the classic vectorization methods (Auto Vectorization [38] and Data Reorganization [48]), state-of-the-art compilers (Pluto [5, 7] and SDSL [15, 16, 48]) and existing highly-optimized work (YASK[46] and Tessellation [47]).




%\paragraph{Data-reused Tiling Adaptor} 

%Specifically, Tessellate Tiling with tetrahedron tetrominoes on cache is presented to improve spatial and temporal data locality, and Checkerboard Blocking with square tetrominoes on shared memory is designed to support a conflict-free coalesced writes to global memory.
 
%Vectorized SIMD extensions are contained in nearly all high-performance CPUs. For a mainstream 256-bit vector register, these extensions could provide 4-way adds and multiplies operations for double-precision elements, promising significant speed-up to narrow the performance gap with GPUs. 
%While ubiquitous, the hybrid of vectorization with HPC Dwarfs in scientific computing limps along and suffers from the severe performance-limiting factor of data alignment conflicts. While many vectorized methods are presented, a majority of them have focused on reducing memory access, decreasing register permutation, or seeking a balance between them, which fails to dig out the performance-limiting factor fundamentally. 

%To achieve a harmonious vectorization on HPC Dwarfs in scientific computing, Tetris throws away the routine thoughts, break new ground from the observable of register architecture, and construct skew tetrominoes for a conflict-free vectorized pipelining. Briefly, Tetris performs mappings of vector registers to tetrominoes data structure that can be abstractly manipulated by data movements to and from registers at the cost of low-latency in-lane operations. Compared with existing methods, the Vector Skewed Swizzling requires no expensive cross-lane operations in registers, and the whole vectorized process is pipelined efficiently meanwhile with more registers reused and saved.

%In this paper, we focus on the Stencil Dwarf as the optimization target to demonstrate our design. Stencil contains a pre-defined pattern that updates each point with neighbors in $d-$dimensional spatial grid iteratively along the time dimension.
% which
%Originated from the finite difference method (FDM) based on regular grids, Stencil 
%It is ubiquitously involved 
%extensively %employed% to solve partial differential equation (PDE) 
%involved in various HPC applications and ubiquitous in scientific computing
%in various HPC applications~\cite{datta2008stencil}, which lies at the heart of thermal diffusion ($\sim$100\%), Earth System Model (ESM) ($>$90\%), and earthquake system model ($>$90\%), etc~\cite{li2021reducing,xiao2018communication,bluman1969general,heatformula,xushun2021,denzler2021casper,christen2012patus}. 


\paragraph{Results}
We have evaluated Tetris on varied classic benchmarks of Stencil Dwarf used for real-world applications. A thorough evaluation with Data Reorganization~\cite{10.1145/3126908.3126920}), Auto Vectorization~\cite{li2022efficient}, Pluto~\cite{bandishti2012tiling}, Folding~\cite{li2021reducing}, Brick~\cite{zhao2019exploiting} and AN5D~\cite{matsumura2020an5d} demonstrates that Tetris improves the performance by an overall of 4.4× and 2.8× on average compared to the state-of-the-arts (Data Reorganization and AN5D) and achieve a nearly linear scaling on Cloud. Furthermore, we employ Tetris for a simulation of thermal diffusion on a square copper plate where Stencil computations dominates overwhelmingly the whole simulation ($\sim$100\%) and it improves the performance of simulation by 29.6× \textbf{from day to  hour} while preserving the \textbf{original accuracy}.

% \paragraph{Key Takeway}

% %At present, most of the HPC systems still rely heavily on traditional supercomputers, suffering from expensive access, poor scalability, and duplicated optimizations. 
% To leverages Cloud infrastructure with cheap access, elastic scalability, and unified architecture, this paper presents Tetris, a disruptive HPC paradigm that democratize HPC on CPU and GPU Cloud infrastructure with polymorphic tiling tetrominoes to tessellate the spatial and temporal dimensions perfectly.
% With polymorphic tetrominoes, Tetris achieves a maximum use of memory and computing power on Cloud, %to boost the tiling performance through memory hierarchies, 
% designs an efficient architecture-aware scheduler to break the performance wall between CPU and GPU, and make full use of specialized hardware units on heterogeneous architecture for further acceleration. %Tetris leverages Cloud with abundant HPC techniques to present a disruptive on-cloud HPC Dwarf Paradigm %democratize large-scale HPC applications 
% %for the first time, 
% %Tetris is the first disruptive on-cloud HPC Paradigm, and 
% Tetris is also orthogonal to (and complements) the optimizations or deployments for a wide variety of emerging and legacy HPC applications. 

%Taking widely-used Stencil Dwarf into account, we propose polymorphic data tetrominoes to boost the tiling performance through the whole memory hierarchy on Cloud, design an efficient architecture-aware scheduler to break the performance wall between CPU and GPU, and make full use of specialized hardware units on heterogeneous architecture for accelerating a broader range of scientific computing applications. %To the best of our knowledge, this data-level fusion of HPC+AI is first explored for scientific computing.
%This paper makes the following contributions:


%\begin{itemize}
%\item We propose a new HPC Dwarf Paradigm, Tetris, which enables a broader range of Stencil-related scientific computing more efficiently by orchestrating HPC and AI infrastructures without requiring HPC expertise from scientists or sacrificing any computational accuracy from AI infrastructures.

%\item Tetris present flexible Polymorphic Data Tetrominoes, Vector Skewed Swizzling
%\item We abstract Polymorphic Data Tetrominoes to boost higher performance across heterogeneous architecture flexibly, propose an Architecture-aware Heterogeneous Scheduler to offload data and compute to both CPU and GPU collaboratively, present a Data-reused Tiling Adaptor to exploit the data reuse through the whole memory hierarchy efficiently, and design High-precision Pipelining Templates to unlock the potential of specialized hardware accelerators exhaustively.
%by an efficient architecture-aware scheduler Vector Skewed Swizzling to address alignment conflicts of scientific data with vector registers on CPU, present Tensor Trapezoid Folding to first adapt HPC computation with tensor cores on GPU, and design an efficient architecture-aware Scheduler to offload data and compute between CPU and GPU automatically.

%\item only with 1 GPU achieve ...  We employ the most widely-used HPC Dwarfs in scientific computing, Stencil, to illustrate the benefits of Tetris, and it achieves up to 2.5× speedup compared to state-of-the-art frameworks.
%\item We propose Tetris, an HPC+AI mix paradigm designed to achieve scalable data-level
%parallelism for Stencil by orchestrating new CPU and GPU features for HPC workloads on heterogeneous architecture.

%\item Vector Skewed Swizzling is presented in Tetris  to address data alignment conflicts for SIMD implementation on CPU, which is disruptive by exploiting the characteristics of register lanes and enhanced with shifts reusing, cache blocking and semi-automatic code generation for an efficient vectorization.

%\item Tensor Trapezoid Folding is proposed in Tetris for first adapting non-GEMM high-precision Stencil computation on GPU to leverage the computing power of TCU. Data movements are further coordinated through the memory hierarchy for significantly boosting the performance of GPU.

%\item Zero Heterogeneous Offloading is designed in Tetris to achieve a scalable heterogeneous mix paradigm, which enables large-scale scientific computing by offloading data and compute to both CPU and GPU collaboratively without requiring any theoretical change from scientists or sacrificing computational accuracy from GPU. 

%\end{itemize}






%The demand and interest for more cost-effective solutions for the same amount of processing power is increasing. In addition, data centers are now a mix of HPC-systems that often include accelerators such as GPUs.

%We are particularly interested in exploring new GPU features for HPC workloads.

%How to take advantage of these tensor cores for non-ML work loads is still a formidable challenge, and one that will be explored in this thesis work

%As mentioned in the introduction, recent GPUs have Tensor Cores that can perform fast matrix tailored for Machine Learning (ML), but that also may provide useful for general HPC loads that can leverage matrix operations. Unlike regular CUDA cores that may execute one operation per cycle, the tensor core are executed at multiple operations per cycle, a bit like CPU vector processors.

 


\section{Background}
%We begin with a brief background on the current style of HPC+AI paradigm for science, and then motivate the key challenges on accelerating high-precision scientific computing with specialized hardware units on HPC and AI infrastructures.



%\subsection{Existing HPC+AI Paradigm}



\subsection{{Stencil Dwarf}} 
\label{pde_derivation} 
{Dwarfs are defined as a series of classic algorithmic methods by the Berkeley View~\cite{asanovic2006landscape}, }
%A Dwarf is an algorithmic method that captures a pattern of computation and communication, covering a broad range of HPC applications~\cite{asanovic2006landscape}.  
which earn the name by dragging down the performance in serving "Snow White" ({scientific computing}) due to {their performance-critical operations}. %their high demands for FP64 computations. %and AI methods are generally incapable of lending a hand to it due to their high demands for accuracy. %They capture the characteristics of computation and communication pattern at a high level and cover a broad range of HPC applications for scientific computing. 
%Typically, the AI methods are incapable of lending a hand to the theoretical calculations involved in traditional HPC Dwarfs because of their high demand for accuracy.
%As one of the seven classic Dwarfs, Stencil is ubiquitously involved in various scientific computing simulations~\cite{datta2008stencil}.


Listed as one of the most important Dwarfs, Stencil is extensively employed in various scientific and engineering applications, and arises as a principal class of floating-point kernels in science~\cite{asanovic2006landscape,asanovic2008parallel,10.1145/3126908.3126920}. 
Generally, Stencil contains a pre-defined pattern that updates each point in $d-$dimensional spatial grid iteratively along the time dimension. The value of one point at time $t$ is a weighted sum of itself and neighboring points at the previous time~\cite{10.1145/1989493.1989508}. 


Listing~\ref{algrls} shows the Stencil of a regular Heat-2D (5 points) kernel, where c1 $\sim$ c5 are the accumulation coefficients. 
Here we use one of the heat equations as a case study to explore Heat-2D Stencil introduced in Listing~\ref{algrls}, and it models the physical transfer of heat in a region over time~\cite{heatformula}. Equation \ref{eq:heat1} is the standard form in three dimensions, where $U(t, x, y)$ represent the heat at a point $(x,y)$ at time $t$, and $\alpha$ is the thermal diffusivity.
%Listed as one of the most important HPC Dwarfs, Stencil is extensively employed in various scientific and engineering applications, and arises as a principal class of floating-point kernels in HPC field~\cite{asanovic2006landscape,asanovic2008parallel,10.1145/3126908.3126920}. 
%{Therefore, taking Stencil Dwarf as a study case is particularly significant to illustrate how Tetris works for scientific computing, and paves the way for a new  Paradigm on Cloud.}%Since it is originated from the finite difference method (FDM) based on the regular grids, Stencil is also widely utilized to solve partial differential equation (PDE) involved in physical simulations.
%For example, Weather Research and Forecasting (WRF), a well-known and widely-used Numerical Weather Prediction scientific application, is composed of a dynamic core with vast and complex Stencil computations to provide timely and accurate weather forecasts.
%and physical parameterizations with multiple physical modules. The simulation of WRF model requires
%huge computing power 

%Generally, Stencil contains a pre-defined pattern that updates each point in $d-$dimensional spatial grid iteratively along the time dimension. The value of one point at time $t$ is a weighted sum of itself and neighboring points at the previous time~\cite{10.1145/1989493.1989508}. 
%The right part of Figure~\ref{old_fusion} shows a brief sketch of atmospheric simulation  based on HPC system.
%Characterized by this regular computational structure, Stencil Dwarf is inherently a bandwidth-bound kernel with a low arithmetic intensity and poor data reuse, which makes it notorious for performance optimization in HPC community~\cite{frigo2005cache,10.1145/3126908.3126920,10.1145/1273442.1250761}.
%Therefore, taking Stencil Dwarf as a study case is particularly significant to illustrate how Tetris works for HPC, and paves the way for a new HPC Paradigm on Cloud. %we focus on digging into data-level parallelism for Stencil in scientific computing scenarios on heterogeneous architecture, which demands orchestrating efficient use of wide SIMD units in CPUs and SIMT threads in GPU.
%Originated from the finite difference method (FDM) based on the regular grids, Stencil is widely employed to solve partial differential equation (PDE) involved in various domains from HPC physical simulations to AI image processing, posing a number of challenges on modern heterogeneous architectures to achieve a high performance. 
%The optimization strategy also varies significantly across algorithms, architectures, and types of applications.  
%To explore a scalable heterogeneous mix paradigm, we focus on digging into data-level parallelism for Stencil in scientific computing scenarios on heterogeneous architecture, which demands orchestrating efficient use of wide SIMD units in CPUs and SIMT threads in GPU.

\begin{algorithm}[b]   
    \caption{\label{algrls}Heat-2D Stencil.}  
    \begin{algorithmic}[1] %每行显示行号  
    \small
    \Require mesh $A$, coefficient $c1 \sim c5$.
        \For{$t = 0 \to T$}
            \For{$i = 0 \to N_x$, $j = 0 \to N_y$} 
                \State  $A[(t + 1) \% 2][i][j]  = c1 \times A[t \% 2][i ][j] +$
                \Statex \qquad \qquad \quad  $ c2 \times A[t \% 2][i - 1][j] +  c3 \times A[t \% 2][i][j + 1] +$ 
                \Statex \qquad \qquad \quad   $   c4 \times A[t \% 2][i][j - 1] +  c5 \times A[t \% 2][i + 1][j]$
            \EndFor 
        \EndFor 
  \end{algorithmic}  
\end{algorithm}
%\end{listing} 


\begin{equation}
\small
\label{eq:heat1}
\frac{\partial U(t, x, y)}{\partial t}=\alpha\left(\frac{\partial^2 U(t, x, y)}{\partial x^2}+\frac{\partial^2 U(t, x, y)}{\partial y^2}\right)
\end{equation}


To obtain a numerical algorithm performed on computers, we discretize the dimensions $x$, $y$, and $t$ into points spaced $\Delta x$, $\Delta y$, and $\Delta t$ apart, which transforms a point $(t, x, y)$ in continuous space as $(n \Delta t, i \Delta x, j \Delta y)$ in discretized space-time in Equation \ref{eq:heat2}:
\begin{equation}
\small
\label{eq:heat2}
    u^{n}_{i,j} \approx U(n \Delta t, i \Delta x, j \Delta y).
\end{equation}

\begin{comment}
Then we can use the finite difference method to obtain an approximation of the first derivative with respect to time as
\begin{equation}
\small
\label{eq:heat3}
\frac{\partial U(t, x, y)}{\partial t} \approx \frac{u_{i,j}^{n+1}-u_{i,j}^n}{\Delta t}.
\end{equation}
Assuming $\Delta x = \Delta y$, a common approximation for the spatial second derivatives are
\begin{equation}
\small
\label{eq:heat4}
\frac{\partial^2 U(t, x, y)}{\partial x^2} \approx \frac{u_{i,j}^n-2 u_{i,j}^n+u_{i,j}^n}{\Delta x^2}.
\end{equation}    
\end{comment}

With the discretization and simplification of Equation \ref{eq:heat1} by using CFL number $\mu$~\cite{heatformula}, % = {\alpha \Delta t}/{\Delta x^2}$, 
%we can substitute the Equations \ref{eq:heat3} and \ref{eq:heat4} into it and yields the 
the following update is yielded as:
\begin{equation}
\small
u_{i,j}^{n+1}=\left(1-4\mu\right)u_{i,j}^{n}+\mu\left(u_{i-1, j}^n+u_{i+1, j}^n+u_{i, j-1}^n+u_{i, j+1}^n\right),
\end{equation}
where Stencil is derived from heat equations in {scientific computing}.


\subsection{{Surrogate Models }}

\begin{figure}
  \begin{center}
  \centering
  \includegraphics[width=0.47\textwidth]{old_fusion.pdf}
  \caption{\label{old_fusion}AI surrogate models serving for {scientific computing}.}
  \end{center} 
\end{figure} 


More recently, there has been considerable progress and promise in the use of AI surrogate models to complex calculations like first-principles functions~\cite{jia2020pushing}, which could utilize GPU to accelerate {scientific computing} and create potential possibilities for adapting to Cloud infrastructure. Albeit the use of AI surrogate models within {scientific computing} improves time-to-solution, most applications cannot utilize them due to the strict taste of accuracy and convergency, especially in the domains of weather forecast, galaxies collision, nuclear reactors, etc~\cite{bailey2005high,bailey2012high,li2021reducing}.  % it will not magically complete the calculations without any cost and HPC Dwarfs are still highly required for accurate and  convergent results. 


Figure~\ref{old_fusion} provides a schematic representation of utilizing AI surrogate models for {scientific computing}~\cite{jia2020pushing}, which includes the three critical phases. 
First, AI surrogate models are designed and trained with simulation data. Once an appropriate trained model is built, it will replace partial calculations of physical properties. 
Second, the prepared physical properties are applied with Dwarfs for high-precision numerical simulation. 
At last, the numerical results produced by Dwarfs serve for "Snow White": the upper-level {scientific computing}, and a correction is promoted for potential biases or inaccuracies in trained models.
%The core of any HPC+AI fusion system in scientific computing is the creation and training of models based on scientific data, which are also referred to as AI-driven scientific models. Briefly, they are taken in the form of trained neural networks as surrogates to the complex calculations of physical properties like first-principles functions~\cite{jia2020pushing}.%, demonstrating the potential for providing improvements even of multiple orders of magnitude in time-to-solution for HPC simulations.

%Most of the existing work obtain better efficiency at the cost of accuracy by alternating numerical algorithm with approximate calculation by above AI surrogate models. 
%Generally, AI surrogate models produce better efficiency at the cost of accuracy by alternating numerical algorithm with approximate calculation. Since HPC itself is fundamentally based on the pillars of accuracy~\cite{bailey2005high,bailey2012high,10.1145/3126908.3126920}, it meets the strict taste of scientists hardly from physical observable, especially in the domains of weather forecast, galaxies collision, nuclear reactors, etc~\cite{li2019openkmc,xiao2018communication}. 
%Even though acceptable accuracy is hyped in some work, the convergence tolerance is actually slacked and meets the strict taste of scientists hardly from physical observable, especially in the domains of weather forecast, galaxies collision, nuclear reactors, etc~\cite{li2019openkmc,xiao2018communication}. 
%Most of them are generally trained in a domain-agnostic style, ignoring domain knowledge that extends far beyond the raw data itself and introducing unexplainable errors that deviate from actual results. %Thus, the obtained accuracy is hardly guaranteed  
%To shorten the convergence time with more accurate results, foundational knowledge within science disciplines are devoted to promoting a more rigorous model with a monitor for correcting potential biases or inaccuracies in collected data~\cite{pandey2022transformational}.%significant efforts are devoted to incorporating domain knowledge into AI systems, including physical laws, mathematical principles, and established invariances that underpin AI-driven scientific models.
%Albeit replacing first-principles functions with approximations, the use of AI surrogates within scientific computing improves time-to-solution effectively, and plays an essential role in the existing HPC+AI fusion paradigm for science~\cite{jia2020pushing}.


%\subsection{AI Paradigm for Science}
%The present HPC+AI paradigm for science is enabling vast advances across a wide range of scientific and engineering disciplines, for instance employing broadly-adopted PINNs to accelerate computational physics or computational chemistry.





%\subsection{Stencil-driven HPC applications}

%Originated from the finite difference method (FDM) based on regular grids, Stencil is extensively employed to solve partial differential equation (PDE) involved in various domains and ubiquitous in scientific computing~\cite{datta2008stencil}, which lies at the heart of thermal diffusion ($\sim$100\%), Earth System Model (ESM) ($>$90\%), earthquake system model ($>$90\%), etc~\cite{xiao2018communication,bluman1969general,heatformula,xushun2021,denzler2021casper,christen2012patus}.
%from scientific computing in HPC to image processing in AI.
%As stated previously, Stencil occurs in many areas and ubiquitous in scientific computing. They range from the simple Jacobi iterations to the extremely complex ones used in the solution of highly nonlinear partial differential equations (PDE).


\begin{comment}


\subsection{Challenges on Specialized Accelerators}

\paragraph{Data Alignment Conflicts on Vector Registers.}
Vectorization serves as an effective means of utilizing the high-performance SIMD facilities in CPU. However, the intractable data alignment conflicts are the main performance-limiting factors caused by vectorization in dealing with scientific data, where the neighbors for a grid point appear in the same vector register but at different positions. Second, the introduced data-reordering instructions inevitably breaks the regular execution of a compute pipelining, which prevents the performance improvements. Moreover, as the number of processor registers on a processor is considered constraint, we must design the compute pipelining carefully to avoid performance degradation caused by register spilling.

\paragraph{HPC Dwarfs Adaptation on Tensor Cores.}

From NVIDIA Ampere GPU architecture like A100, the newly-introduced specialized hardware units known as NVIDIA’s Tensor Cores could offer a full range of precision to boost the performance of MM even with the highest accuracy needed (FP64).
Although TCUs are prevalent and promising to provide an increase in performance, they suffer from over specialization as only MM on specific size of small matrices is supported. In particular, the size of supported matrix for FP64 precision is only constrained to one setting shown in Equation~\ref{eq:mma}:
\begin{equation}
\label{eq:mma}
   D_{m\times k} = A_{m\times n} \times B_{n\times k} + C_{m\times k},
\end{equation}
where m, n, k is 8, 4, 8 respectively.
Thus, the Stencil Dwarf cannot reap the benefits of them due to abundant non-MM operations, and the only available setting for FP64 operations makes the challenges even harder. This results in low chip utilization and narrow application domains for GPU, which are still paid little contribution by existing work. 





\subsection{Challenges in Tetris}

There are many open challenges in implementing and translating Tetris to practice poses for Stencil-based scientific computing. 
%We first discuss necessary background on data alignment conflicts in scientific data to employ vector registers on CPU. Then, we review challenges related to adapting scientific computing for GEMM to utilize Tensor Cores on GPU. Furthermore, the problems in scheduling are investigated on heterogeneous architectures. These are the major challenges that this work addresses.
%We first discuss the scalable scheduler design across heterogeneous architecture on memory level. 
%First, the scheduling problems of coarse-grained scientific data in memory are investigated across heterogeneous architectures. 
%Then, we review challenges related to tiling on medium-grained data blocks to enhance data reuse. Furthermore, the Stencil Dwarf adaptation are discussed with specialized hardware units. These are the major challenges that this work addresses.

\subsubsection{Challenge \#1: Scalable Scheduler Design across Heterogeneous Architecture}

In the HPC community, data parallelism is generally used as the de facto standard to scale  HPC workloads to the same compute units. However, it is barely explored before to work with AI infrastructures for sharing high-precision HPC workloads, where good scalability is crucial and challenging to take full advantage of available  computing power on heterogeneous architecture. 
\paragraph{Memory Consumption.} %First, with larger simulation size and more accurate results are required, the memory requirements involved in scientific computing scenarios increase dramatically. 
Although aggregated GPU memory can hold more data, the excessive usage of GPU is typically beyond the reach of most scientists, which demands a bidirectional design for squeezing memory consumption by maximizing memory savings on GPU while exploiting CPU memory for  simulation. % in scientific computing, physical properties are the primary source of memory bottleneck which are clearly beyond the memory capacity of even the current flagship NVIDIA A100 GPU with 80 GB of memory.
\paragraph{Computation Partition.} %Second, it is a popular stereotype that the GPU computation throughput is several orders of magnitude faster than that of CPU computation throughput. 
Though GPU is weak in high-precision computation and high-performance SIMD facilities are employed on CPU with an attempt to narrow their performance gap in Tetris, it still requires a balanced computation partition to prevent the CPU compute from becoming a performance bottleneck.
\paragraph{Communication Cost.}
Furthermore, the communication volume between CPU and GPU memory is also required to be minimized or overlapped for avoiding the bandwidth limits.

\subsubsection{Challenge \#2: Efficient Tiling Generator through Memory Hierarchies} 

%In Tetris, data movements are treated as first-class citizens with polymorphic data tetrominoes, which enables more flexible structures to boost higher performance in view of hardware characteristics. 
As a middle layer in memory hierarchy, an efficient tiling on cache in CPU (or shared memory in GPU) is challenging to meet the requirements of enhancing data reuse meanwhile matching the data tetrominoes efficiently in top and bottom memory hierarchies.
\paragraph{Tiling on Cache.}
Since modern high-performance CPU is always equipped with multicore or manycore configuration, the data space is required to be tiled spatially to enable fully parallelism such that all data blocks between synchronizations can execute concurrently without redundant computation. Moreover, temporal tiling is also essential to increase data reuse before sweeping the data from cache, and appropriate tetrominoes are constructed accordingly to tessellate the time dimension for synchronization. 


\paragraph{Blocking on Shared Memory.}
Efficiently using the GPU’s compute and memory hierarchy requires the coordinated data movements. Typically, the results are generated and distributed across the registers of its threads. In order to achieve fully parallelism on available warps and coalesced writes to global memory, it is crucial to adopt blocking in shared memory for data coordination. Furthermore, it is required to address two major different conflicts on shared memory. One is the bank conflict triggered by hardware mechanism, and the other is block conflict introduced by interblock compute dependencies, which complicates the blocking strategy and hinders unlocking higher performance. 


\subsubsection{Challenge \#3: Accurate Pipelining Template on Specialized Accelerators}

\paragraph{Data Alignment Conflicts on Vector Registers.}
Vectorization serves as an effective means of utilizing the high-performance SIMD facilities in CPU. However, the intractable data alignment conflicts are the main performance-limiting factors caused by vectorization in dealing with scientific data, where the neighbors for a grid point appear in the same vector register but at different positions. Second, the introduced data-reordering instructions inevitably breaks the regular execution of a compute pipelining, which prevents the performance improvements. Moreover, as the number of processor registers on a processor is considered constraint, we must design the compute pipelining carefully to avoid performance degradation caused by register spilling.

\paragraph{HPC Dwarfs Adaptation on Tensor Cores.}

%\subsubsection{Challenge \#1: Data Alignment Conflicts in Vector Registers} 

%Vectorization serves as an effective means of utilizing the high-performance SIMD facilities in CPU, and it has been exhaustively investigated in the literature for Stencil dwarf in scientific computing. However, the intractable data alignment conflicts are the main performance-limiting factors caused by vectorization in dealing with scientific data, where the neighbors for a grid point appear in the same vector register but at different positions.

%To address the problem of spatial conflicts, two common strategies are often adopted in Figure~\ref{cpu_alignconflicts}. The first one loads all the needed elements from memory in a vector form straightforward. Compared with the scalar code, this multiple load method further increases the data transfer volume. Moreover, in each iteration of this code, it has at least two unaligned memory references where the first data address is not at a 32-byte boundary. Since CPU implementations favor aligned data loads and stores, these unaligned memory references will degrade the performance considerably.
%The second solution is similar to the scalar code in terms of the CPU-memory data transfer. It loads each input element to vector register only once and assembles the required vectors via data permutation instructions provided by SIMD facilities. 
%Compared with the multiple load method, this data permutation method reduces the memory bandwidth usage and takes advantage of the rich set of data-reordering instructions supported by most SIMD architectures. However, the execution unit for data permutations inside the CPU may become the bottleneck.
%Basically, most existing work are all in an attempt to seek a balance between the number of data reorganization in CPU and the volume of data reuse in cache, which fails to explore the intrinsic cause from underlying architecture of vector registers.

%\subsubsection{Challenge \#3: HPC Dwarfs Adaptation on Tensor Cores}

%Developing HPC Dwarfs on AI infrastructures is highly challenging because of their unskilled general double-precision computing capabilities. %Significant efforts have been devoted to optimizing Stencil computation on GPU, including both manual optimization and code generation framework, to enhance the data-level parallelism by utilizing SIMT facilities on GPU. Nonetheless, the current approaches prevent experts from closing the remaining performance gap because they treat GPU as second-class citizens: Only AI-driven methods are trained on them while the time-consuming theoretical calculations are still reserved on CPUs.

From NVIDIA Ampere GPU architecture like A100, the newly-introduced specialized hardware units known as NVIDIA’s Tensor Cores could offer a full range of precision to boost the performance of MM even with the highest accuracy needed (FP64).%, which is surprisingly 2.5x faster than F64 CUDA Cores on NVIDIA V100. 
Although TCUs are prevalent and promising to provide an increase in performance, they suffer from over specialization as only MM on specific size of small matrices is supported. In particular, the size of supported matrix for FP64 precision is only constrained to one setting shown in Equation~\ref{eq:mma}:
\begin{equation}
\label{eq:mma}
   D_{m\times k} = A_{m\times n} \times B_{n\times k} + C_{m\times k},
\end{equation}
where m, n, k is 8, 4, 8 respectively. %Currently, no algorithm other than MM utilizes the NVIDIA TCUs, which results in low chip utilization and narrow application domains.
%In particular, the size of the matrix is constrained to specific settings, for example,  
%However, 
%Since the TCUs are originally designed for accelerating MM in AI methods, the representative HPC Dwarfs in scientific computing like 
Thus, the Stencil Dwarf cannot reap the benefits of them due to abundant non-MM operations, and the only available setting for FP64 operations makes the challenges even harder. This results in low chip utilization and narrow application domains for GPU, which are still paid little contribution by existing work. 
%To unlock the potential of TCUs without precision discrimination, we draw an analogy to Convolution in AI field because of their similarity in computing pattern, and adapt calculations dynamically to accelerate throughput while preserving accuracy. 
%To the best of our knowledge, there is no previous work that adapts Stencil computation to TCUs efficiently by considering the analogy to Convolution and unique characteristics of hardware.
\end{comment}

%\section{Tetris: Democratize Stencil-driven Scientific Computing on Cloud}

 
%In this section, we introduce Tetris’s core design, which promotes it as a scalable Stencil Dwarf Paradigm for science. %First, we present Vector Skewed Swizzling to relieve data alignment conflicts for high-performance SIMD vectorization on CPU. Then a Tensor Trapezoid Folding strategy is proposed to leverage the computing power of TCU for high-precision SIMT acceleration on GPU. Next, we present Zero Heterogeneous Offloading to provide an optimal solution in maximizing memory saving while minimizing communication overhead on heterogeneous architecture for an HPC+AI mix paradigm.



\section{{Pattern Mapping by Register-level Tetrominoes}}
With polymorphic register-level tetrominoes, Tetris designs a sophisticated {\textit{Pattern Mapping}} to make full use of specialized hardware units on heterogeneous architecture. 

\label{Sec:Template_Tetrominoes}

\subsection{Vector Skewed Swizzling} 
 
%Intra-register manipulation of SIMD data elements. AVX provides several flexible SIMD floating-point data manipulation primitives: — insert/extract multiple SIMD floating-point data elements to/from 256-bit SIMD registers
%— permute primitives to facilitate efficient manipulation of floating-point data elements in 256-bit SIMD registers
Existing vectorized optimizations for Stencil have mainly focused on either memory access or register permutation, aiming at decreasing transferred data volume and employing rich SIMD instructions respectively~\cite{li2021reducing}. To address the data alignment conflicts~\cite{li2022efficient} essentially, we perform a deep analysis of register architecture to find out the root cause of this challenge.

\paragraph{Vector Tetrominoes. } 


\begin{figure}
  \begin{center}
  \centering
  \includegraphics[width=0.44\textwidth]{cpu_tetromino.pdf}
  \caption{\label{cpu_layout}Skewed tetrominoes on CPU.}
  \end{center} 
\end{figure} 

Given a comprehensive view of registers in CPU, a 256-bit SIMD YMM register is actually promoted by two separate data lanes, where the lower 128-bits are aliased to a 128-bit XMM register~\cite{guide21url}. 
Taking FP64 computations into consideration, we fill up this 256-bit SIMD register with 4 elements initially as a straight tetromino.  
%As shown in Figure~\ref{cpu_layout}, a straight tetromino constituted by 4 squares are by 2 bars, which correspond to the 2 register lanes exactly. 
Since a 256-bit SIMD register is composed of 2 128-bit register lanes, a straight tetromino is also viewed as an assembly by 2 bars accordingly in Figure~\ref{cpu_layout}(a). 
To adapt this lane-based characteristic of registers, variants are derived from straight tetrominoes to skewed tetrominoes by employing the 128-bit lanes as basic processing units.


As shown in Figure~\ref{cpu_layout}(b), a skewed tetromino constituted by 4 squares could hold a complete vector, which also indicates a 256-bit SIMD register is configured with 4 $double$ type slots. Then we achieve \textit{forward stacking} in Figure~\ref{cpu_layout}(d) by manipulating a crowd of skewed tetrominoes to assemble pipeline forward, and the whole data space for a time step could be tessellated flawlessly in this way. 
%In view of the lane-based characteristic of registers, an equivalent conversion principle from forward stacking to backward stacking is designed subtly with reflected skewed tetrominoes, and they can be converted swimmingly to each other by a cheap lane-based \verb=permute128f= instruction every two vectors. 
In view of the lane-based characteristic of registers, an equivalent conversion principle from forward stacking to  \textit{backward stacking} in Figure~\ref{cpu_layout}(e) is also designed swimmingly by a cheap lane-based \verb=permute128f= instruction every two vectors illustrated in Figure~\ref{cpu_layout}(c). 
%For example in Figure~\ref{cpu_layout}, the 128-bit data lane loaded with C and D assembles squares A and B forward to build a blue skewed tetromino A, B, C and D. Provided with lane-based SIMD instruction, we can also perform a backward assembly flexibly with squares E and F to construct a new skewed tetromino C, D, E and F, which is adopted as a building block for backward stacking. Therefore, two ways for constructing a tiled data space are paved by stacking skewed tetrominoes.
%Both tetrominoes could be employed for 

\paragraph{Skewed Swizzling.} 

\begin{figure}
  \begin{center}
  \centering
  \includegraphics[width=0.4\textwidth]{cpu_swizzling.pdf}
  \caption{\label{cpu_swizzling}Skewed Swizzling in Vector Registers.}
  \end{center} 
\end{figure} 

The vectorization mechanism on CPU SIMD facilities is achieved by performing calculations on the slots of different registers vertically in one go, and each calculation flow on aligned slots is independent to each other. Thus, the key challenge to address the data alignment conflicts lies in an efficient adaption that remapping the adjacent elements to the same slot positions in different registers with minimal costs.

As illustrated preciously, lane-based operations are recommended for low-latency implementation when utilizing SIMD facilities on CPU. %To build an efficient quadruple pipelining in Figure~\ref{cpu_pipeline} physically, we employ \verb=shuffle= instructions to swizzle the double-precision floating-point elements within 128-bit lanes between two registers. 
Figure~\ref{cpu_swizzling} shows the process of skewed swizzling for two tetromino registers, where elements at the same slot position of each 128-bit lane are swizzled with elements in the other register correspondingly. Specifically, the two skewed tetrominoes A, B, C, D and E, F, G, H are swizzled for A, E, C, G and B, F, D, H by unpacking and interleaving elements from the low half and high half of each 128-bit lane respectively, which only consumes 1 latency time by lane-based \verb=shuffle= instructions~\cite{granlund2012instruction}. Based on the efficient skewed swizzling, %we achieve quadruple pipelining physically, where 
two skewed tetrominoes are swizzled as diagonal tetrominoes, where the data alignment conflicts are all well addressed with minimal cost.
%, and 4 threads of calculation flows are unfolded horizontally by moving a vector sliding window to the right. 


\paragraph{Quadruple Pipelining.}

\begin{figure}
  \begin{center}
  \centering
  \includegraphics[width=0.49\textwidth]{cpu_pipeline.pdf}
  \caption{\label{cpu_pipeline}Quadruple Pipelining on Vector Registers.}
  \end{center} 
\end{figure} 


%As described above, a 256-bit SIMD register is typically configured with 4 $double$ type slots, and each slot is involved in an independent calculation flow with the same positions in other registers. Thus, 
Here we stack the forward and backward stacking in a well-designed principle to build a quadruple pipelining for a high-performance vectorized adaptation of Stencil Dwarf by Tetris.

Figure~\ref{cpu_pipeline} (a) exhibits the principle of quadruple pipelining, where the forward stacking is tessellated with backward stacking tightly. 
More specifically, each skewed tetromino in forward stacking is inserted at intervals in backward stacking orderly to build a dual pipelining first. Then the head tetromino in each stacking is removed, and similar operations are performed again to obtain a new staggered dual pipelining. At last, we stack two dual pipelinings vertically and still keep a skewed style, and the two skewed tetrominoes are further converted to diagonal tetrominoes with a simple skewed swizzling in Figure~\ref{cpu_pipeline} (b). With polymorphic tetrominoes, Tetris adapts Stencil Dwarf efficiently to CPU SIMD facilities by relieving data alignment conflicts in vectorization.  
%Following this principle, a quadruple pipelining is built in Figure~\ref{cpu_pipeline}. %However, the quadruple pipelining still requires a further skewed swizzling, as each skewed straight tetromino is not a real register physically.




\subsection{Tensor Trapezoid Folding}

\paragraph{Tensor Tetromino.} 

%\subparagraph{Warp Matrix Multiply and Accumulate} 
\begin{figure}
  \begin{center}
  \centering
  \includegraphics[width=0.48\textwidth]{gpu_tetromino.pdf}
  \caption{\label{gpu_tetromino}Plane tetrominoes on GPU.}
  \end{center} 
\end{figure} 

%As shown in Figure~\ref{gpu_tetromino}, since TCUs can be programmed in a manner of MM at warp level with CUDA C++ Warp Matrix Multiply and Accumulate (WMMA) API, the design objective of tensor tetrominoes is to expand the class of algorithms performed on TCUs for a wider range of non-MM operations in scientific computing. %For simplicity, this paper will employ a 2D9P Star Stencil as a study case to illustrate the design of tensor tetrominoes.
%As shown in Figure~\ref{gpu_tetromino}, TCUs can be programmed in a manner of MM at warp level with CUDA C++ Warp Matrix Multiply and Accumulate (WMMA) API directly. These APIs operate on a special data type, fragment, which holds a matrix tile in thread-local registers. %Then data are processed by provided specific functions to load input matrices (load\_matrix\_sync), perform MMA operations (mma\_sync) and store the result matrix (store\_matrix\_sync). 
%Listing 1 shows a TCU kernel that computes a ⟨16,16,16⟩ matrix multiplication within a warp using the WMMA API. Lines 4– 6 declare the matrix fragments. For both the A and B kinds, users specify whether the matrix is in column- or row-major order. Users also specify the stride between rows and load the data from either shared or global memory (Lines 7–8). Line 9 initializes the matrix\_c elements to zero by broadcasting the scalar value 0 into the fragment. Once the data is loaded, users perform the matrix multiplication operation (Line 10) and store the results (Line 11)
%\subparagraph{Formulation on TCUs} 

% Figure 2 (a) illustrates the computing scheme for the 2D9P Star Stencil. To update O, it first gathers the neighbors along the unit stride dimension (the row) with weights multiplied, and then the neighbors along the non-unit stride dimension (the column) are collected similarly for an update in this step. 
%Figure 3 shows a straightforward approach to adapting Stencil computation on TCUs by MM operations. 
%The idea is based on the observation that the HPC Dwarfs like Stencil could be split into a combination of intermediate results from multiple directions, and the result of each direction could be redesigned as a series of reduction and summation operations completed by MM. 
%Thus, the computation of traditional HPC Dwarfs are adapted effectively on TCUs, which could be generalized concretely for any Stencil Dwarf in scientific computing. 
%\subparagraph{Fine-grained Adaption}  
From NVIDIA Ampere GPU architecture like A100, the newly-introduced specialized hardware units known as NVIDIA’s Tensor Cores could offer a full range of precision to boost the performance of MM even with the highest accuracy needed (FP64).
Although Tensor Cores are prevalent and promising to provide an increase in performance, they suffer from over specialization as only MM on specific size of small matrices is supported. In particular, the size of supported matrix for FP64 precision is only constrained to one setting shown in Equation~\ref{eq:mma}:
\begin{equation}
\label{eq:mma}
   D_{m\times k} = A_{m\times n} \times B_{n\times k} + C_{m\times k},
\end{equation}
where m, n, k is 8, 4, 8 respectively.
%As introduced in Equation~\ref{eq:mma} preciously, the TCUs suffer from over specialization as only one setting of matrix multiplication is supported with FP64 precision.%, where all input matrix shapes are widely different.

Here a series of tensor tetrominoes are designed to achieve an efficient Stencil Dwarf adaptation. Concretely, at each step of Stencil, it applies the parameter kernels onto a portion of the elements in the specific shape, compute their element-wise products, and further sum the products together to update the value at this position. With this in mind, we could actually formulate the Stencil computation by using MM operations equivalently. 
%To address this harsh restriction, we orchestrate the $8\times4$ data matrix $A$ and $4\times8$ parameter matrix $B$ to produce an 8$\times$8 output matrix, which is abstracted as a plane tetromino in tensors extended from straight tetromino in vectors. 
%an $8\times4$ input matrix $A$ and construct the parameter matrix $B$ that contains the coefficients of weighted reduction. 
Then Stencil is adapted as weighted reductions for a $8\times8$ plane tetromino.%out matrix $C$, which is abstracted as a plane tetromino. 
At last, the adjacent plane tetrominoes are accumulated with overlaps for the final update at this position. 

%We take the example of horizontal slices to illustrate how to complete the Stencil computation by using MM operations.
%Intuitively, the matrix $C$ (8$\times$8) is twice as wide as matrix $A$ (8$\times$4) after an MM operation with matrix $B$ (4$\times$8), and this irregular manner makes aligned computation intractable.
%We redesigned the layout of parameter matrix $B$ to fill it as much as possible, aiming to employ extra space budget of $C$ for caching computing dependencies. Specifically, the left and right two columns of matrix $C$ are utilized for caching the forward and backward dependencies that the data  matrix $A$ provides respectively; the sandwiched four columns are then employed for the calculation contributed by matrix $A$ itself. For example in Figure~\ref{gpu_tetromino}, the $i$th column of matrix $C$ is calculated by $C[i][1]=A[i][1]\times e$, which indicates the elements in the first column of matrix $A$ contributes to the second forward column with weight $e$.
 

\paragraph{Trapezoid Folding.} 

\begin{figure}
  \begin{center}
  \centering
  \includegraphics[width=0.48\textwidth]{gpu_folding.pdf}
  \caption{\label{gpu_folding}Tensor Trapezoid Folding on Tensor Cores.}
  \end{center} 
  \vspace{-0.4in}
\end{figure} 

A plane tetromino is an abstraction of the matrix $C$ described from a two-dimensional perspective. To make different weights counts, we upgrade the plane tetrominoes as stair tetrominoes from a three-dimensional perspective. Then a Trapezoid Folding strategy is presented to build a tessellation of data space by utilizing Stair tetrominoes as building blocks.

Figure~\ref{gpu_folding}(a) exhibits a sketch of Trapezoid Folding. 
Each position in a $xy-$plane tetromino rises prominently towards the $z$ dimension, building a symmetrical stair tetromino. 
The height is determined by the weights contributions from the parameter matrix $B$, and the positions in the same column are also of equal height due to the identical $B$ used in MM operations. 
To distinguish it more intuitively, the stair tetrominoes are inverted and colored alternatively, and then they are stacked to tessellate the whole data space. We call the
calculation along the third dimension with two stair tetrominoes a \textit{folding}. For example, a folding is performed first on the right part of stair tetrominoes A (blue) with the left part of adjacent stair tetrominoes B (yellow). Then the overlapped positions are all updated horizontally, where the heights are accumulated with 5 weights from all dependent neighbors. Similar operations are performed for obtaining a vertical folding in Figure~\ref{gpu_folding}(b).

\paragraph{Octuple Pipelining.} 

\begin{figure}
  \begin{center}
  \centering
  \includegraphics[width=0.48\textwidth]{gpu_pipeline.pdf}
  \caption{\label{gpu_pipeline}Octuple Pipelining on Tensor Cores (FP64).}
  \end{center} 
  \vspace{-0.2in}
\end{figure} 

Extended from vector to tensor, the tetrominoes defined on GPU are manipulated with a $8\times 8$ matrix in Tensor Core fragments. 
Since the elements of the same column in matrix $C$ are obtained by identical parameters in matrix $B$, the computations are performed along the $x$-direction horizontally, and each row represents a thread of calculation flow. 

Based on the efficient Trapezoid Folding and Checkerboard Blocking, we achieve Octuple Pipelining for Stencil in Figure~\ref{gpu_pipeline}.
First, the scientific data inputs on Checkerboard in shared memory are loaded into  fragments as a $8\times 4$ matrix  with different colors in Figure~\ref{gpu_pipeline} (a).
Then stair tetrominoes collecting dependencies of the adjacents are built and stacked by Trapezoid Folding to construct Octuple Pipelining. 
As shown in Figure~\ref{gpu_pipeline} (c), an efficient Octuple Pipelining is built to tessellate the whole data space, which achieves a high-precision adaptation from Stencil Dwarf to MM operations on Tensor Cores.


 
 
 
\section{{Locality Enhancer by Cache/SMEM-level Tetrominoes}}
\label{sec:Tiling_Generator}
{%As introduced previously, Tetris is characterized by polymorphic tiling tetrominoes, which are employed to tessellate the spatial and temporal dimensions perfectly on different architectures and application contexts automatically.  
In this subsection, we will introduce the \textit{Locality Enhancer} by tetrominoes of the middle layer (cache on CPU and shared memory on GPU) in memory hierarchy, which enhances the data reuse spatially and temporally meanwhile matching other tetrominoes in top and bottom memory hierarchies. }

\subsection{Tessellate Tiling} 
\begin{figure}
  \begin{center}
  \centering
  \includegraphics[width=0.48\textwidth]{cpu_tiling.pdf}
  \caption{\label{cpu_tiling} {Tessellate Tiling ($T_b=3$). Triangle tetrominoes are tessellated in 2D and upgrade as tetrahedron tetrominoes in 3D.}}
  \end{center}  
\end{figure} 
{%Stencil Dwarf in scientific computing is highly bandwidth-intensive.%, where the required data transfers through the memory hierarchy to and fro. 
%Stencil exhibits poor data locality and is a typical bandwidth-bound kernel.
Since modern high-performance CPU is generally configured with multiple cores, the data space can be tessellated spatially to enable fully parallelism such that all tetrominoes between synchronizations can execute concurrently without redundant computation. To develop more data reuse with fine optimizations, temporal tiling is then also presented by stretching the tetrominoes along the time dimension additionally.}

Typically, the working sets are many orders of magnitude larger than a processor’s cache memory. 
Let $T_d\{m,n\}$ be the data transfer time through the memory hierarchy $m$ to $n$, and we use this shorthand notation for expressing data transfer time in each iteration (e.g., $T_d\{L2,Reg\}=T_{L2L1}+T_{L1Reg}$ when the data is in the L2 cache). 
Since all grid cells are updated once before the next update sweep starts, a bad data locality is common to Stencil Dwarf with $T_d\{Mem,Reg\}=T_{MemL3}+T_{L3L2}+T_{L2L1}+T_{L1Reg}$, where $T_{MemL3}\gg T_{L3L2}>T_{L2L1}>T_{L1Reg}$.
%Tessellate Tiling is presented in Tetris on CPU. It reduces the data transfer between cache and main memory spatially and enhance in-cache data reuse between successive updates temporally.

\paragraph{Spatial Tiling.} 
Tessellate Tiling is used first by employing shaped tetrominoes to reduce $T_d\{Mem,Cache\}$, which dominates a major cost of data transfer time~\cite{yuan2019tessellating,li2021reducing}.
It can be viewed as a tessellation in iteration space by utilizing shaped tetrominoes. Figure~\ref{cpu_tiling} illustrates the tiling strategy for a two-dimensional Stencil computation with $T_b=3$ (temporal tiling size), where the timestamp of each element along the time dimension is annotated on it. 
%The iteration space is tessellated by triangle and inverted triangle tetrominoes in alternative stages.
Triangle tetrominoes are updated first, where the center element is updated three steps and its neighbors are updated fewer steps proportional to the distance with the center element.
Then two half parts from adjacent triangle tetrominoes constitute new inverted triangle tetrominoes, and identical operations are performed to make all elements updated with the same steps.
From the perspective of front view, The iteration space is tessellated by triangle tetrominoes and inverted triangle tetrominoes in alternative stages. 

\paragraph{Temporal Tiling.} 
As shown in Figure~\ref{cpu_tiling}, concurrent execution is processed by two stages over the same steps temporally with spatial tessellation completed, and the updated step of a specific element in a tetrominoes is proportional to its distance with the center element. Taking updated steps into account, the 2D iteration space in Figure~\ref{cpu_tiling} rises straight from the ground. At this point, 
the 2D triangle tetrominoes are also expanded as 3D tetrahedron tetrominoes, and two types of them tessellate the whole data space along the spatial and temporal dimensions.

With the tessellate tiling strategy, concurrent execution for different tiles is enabled over a given time range without redundant computation~\cite{yuan2019tessellating}, and a high in-memory flops/byte ratio is achieved with more data reused. 
 

\subsection{Checkerboard Blocking}  
{An efficiently optimized GPU code must allocate appropriate GPU memory resources, especially on shared memory, to improve data locality. Most code generators automatically determine this resource assignment (or mapping), but without considering the balance on resource limits of the underlying GPU device and the requirements of applications. Taking the warp size (32 threads) and SMEM capacity into account, Checkerboard Blocking is carefully designed to provide an enhanced locality manually.  }
\begin{figure}
  \begin{center}
  \centering
  \includegraphics[width=0.45\textwidth]{gpu_blocking.pdf}
  \caption{\label{gpu_blocking}Checkerboard Blocking on shared memory. }
  \end{center} 
  \vspace{-0.2in}
\end{figure} 
%To further improve the performance of HPC Dwarfs on GPU, it is necessary to design the mappings of how data movements are coordinated through the memory hierarchy carefully. 
%Parallel execution is achieved by executing, simultaneously, several groups of 32 threads. These groups are called warps and play an important role in tensor core programming
 
\paragraph{Spatial Blocking.} 
As a bridge for connecting global memory and registers, the shared memory is utilized as the stored buffer to guarantee the coalesced global memory access, where the computed results from registers will also be stored back to it first.
As shown in Figure~\ref{gpu_blocking}, Checkerboard Blocking strategy is proposed for utilizing the shared memory efficiently. To achieve a high parallelism, the shared memory in a GPU block is abstracted as an $8\times 8$ checkerboard that is criss-crossed by two types of square tetrominoes. Each row on the checkerboard is computed by a warp, and the threads in a warp are responsible for each concrete square tetromino. With Checkerboard Blocking, the data space on shared memory is occupied spatially without leaving any memory space unexploited or computing threads idle, and the coordinated
data exchange could be performed with coalesced writes to global memory.

\paragraph{Conflict-free Blocking.}
It is required to address two major different conflicts on shared memory for unlocking a higher performance. One is the bank conflict triggered by hardware mechanism, and the other is block conflict introduced by inter-block compute dependencies. 
Instead of padding the shared memory buffer with additional columns, Checkerboard Blocking reduces the read and write bank conflicts entirely by a precise control of thread access. Specifically, each square tetromino on checkerboard is further decomposed into an $8\times 8$ grid, and a grid point represents a real data. Then the half part of tetromino with $8\times 4$ grid points is loaded into a fragment on Tensor Cores, where the data and threads are mapped with a biunique correspondence without any conflict. As for block conflicts, Checkerboard Blocking stagger the square tetrominoes with two colors in Figure~\ref{gpu_blocking}, and the updates are performed alternatively by warp scheduling and thread controlling precisely  to avoid inter-block compute dependencies.



\section{{Concurrent Scheduler by Memory-level Tetrominoes}}

\begin{figure}
  \begin{center}
  \centering
  \includegraphics[width=0.48\textwidth]{tetris_scheduler.pdf}
  \caption{\label{tetris_scheduler}Concurrent heterogeneous scheduler.}
  \end{center} 
  \vspace{-0.2in}
\end{figure} 

{Tetris identifies the implicit and explicit dependencies between CPU and GPU, and a two-way partitioning is then performed on input as memory-level tetrominoes. Towards an efficient \textit{Concurrent Scheduler} on heterogeneous processors, the following techniques are proposed on memory-level tetrominoes.}


\subsection{Bidirectional Memory Squeezing} 

Memory consumption is still a tough challenge for {scientific computing}, especially in large-scale high-precision simulation~\cite{ren2021zero,li2019openkmc}.
Unlike previous work only utilizing CPU memory for high-precision simulation, Bidirectional Memory Squeezing in Tetris is designed to enable more efficient simulation by sharing memory resources on CPU and GPU. %offloading some workloads from CPU to GPU during computation. 

The {scientific computing} workloads can be represented as a directed graph of data and computation. Figure~\ref{tetris_scheduler} shows a Stencil Dwarf graph, where the output in the current step will be used as input for the next step. Since the spatial dependencies are much weaker than temporal dependencies, the offload strategy between GPU and CPU can then be abstracted by employing a two-way partitioning of input, such that
tetrominoes in a partition would be executed and stored on that compute worker to explore more memory quota. It is also worth noting that the strategy is achieved with little offload cost for more computing and storage benefits.

Bidirectional Memory Squeezing is also a bidirectional design for squeezing memory consumption. Once the GPU memory is fully occupied, the remaining part left on CPU is still well-addressed since both CPU and GPU provide memory and compute for better efficiency. When multiple GPUs are available, Bidirectional Memory Squeezing could be extended similarly to offer excellent scalability in even larger {scientific computing} applications.


  
\subsection{Auto-tuning Computation Scheduling} 
%It is a popular stereotype that the GPU computation throughput is several orders of magnitude faster than that of CPU computation throughput. In practice, the capacity of performing double precision (FP64) operations on GPU is not good as half or single precision (FP16, FP32), and the over specialization of TCUs makes it harder to achieve the peak performance due to the necessary adaptation. 
Tetris treats {scientific computing} applications as decomposable workloads since tetrominoes are coordinated explicitly by it, with each type of worker (CPU or GPU) executing a subset of the tetrominoes. Like with any distributed system, the steady state throughput of the whole heterogeneous architectures is determined crucially by the slowest worker. Having CPU and GPU process subtask at vastly different throughput can lead to bubbles in
the computing pipeline, starving the faster worker of subtasks to work on and
resulting in resource under-utilization.

Tetris sticks to the fact that Stencil Dwarf exhibits little variance in computation time across the same size of inputs. As shown in Figure~\ref{tetris_scheduler}, in the startup phase, the input stage admits enough tetrominoes to keep each worker full in steady state.
Then the computation time taken by the first iteration, the input size
of scientific data, the size of Dwarf parameters and the number of iterations are recorded as part of a profile initialization. This profile is employed as the input to the  scheduler for performing a balanced partition on a heterogeneous platform.
Apart from the constraints of software, the scheduler is also architecture-aware in a design of taking into account other hardware characteristics, such as host/device memory capacity, host/device compute power, and the interconnect bandwidths, and it computes: (1) a partitioning on the input into workers, (2) the estimated communication or replication volume between workers, and (3) optimal number of in-flight tetrominoes to keep the computing pipeline busy.

Intuitively, this process finds the optimal partitioning on CPU and GPU and can also be extended easily across clusters. Based on the partitioning strategy, Tetris’s scheduler dispatches a balanced workload to each worker on the main memory/global memory level. 


\subsection{Minimized Communication Cost} 

\paragraph{Less Communication Volume.} Most of the existing work employing GPU accelerators requires a busy data transfers between CPU and GPU, as the CPU only plays a role of stream controlling and data is updated between CPU and GPU round by round. 
Tetris can communicate far less than this. Instead of having to perform a memory turnover periodically, each worker first deals with the halo region introduced by balanced computation scheduling, and has to communicate only this small subset of the working set. 
\paragraph{More Communication Overlap.} Despite using a highly reduced halo exchange, the communication interlude can still become a bottleneck that disturbs the regular computing pipeline. For such limited cases, we separate the streams of computation and communication carefully for a parallel execution, and address halo updates first in each working set. As shown in Figure~\ref{tetris_scheduler}, with the halo region updated, the exchange is performed immediately, which overlaps computation and communication to hide the halo exchange overhead. 

\paragraph{Centralized Communication Launch.} For the communication part, we collect the halo exchanges for $T_b$ steps and then only conduct one centralized communication instead of conducting $T_b$ small communications.% in Figure~\ref{tetris_scheduler}. 
The reason is analyzed below. The time for communication could be formulated by $k\times (\alpha + n_b \times \beta)$, where $k$ is the number of messages; $\alpha$ is launch latency; $1/\beta$ is the bandwidth; $n_b$ is the number of bytes per message. In practice, since we have $\alpha >> \beta$, an intact big message is much cheaper than $k$ split small messages due to $k\times (\alpha+n_b\beta)>>(\alpha+k\times n_b\times \beta)$. Thus, centralized communication launch are proposed to further decrease communication cost for $T_b$ steps. %, where $T_b$ is a temporal tiling size. % introduced in Section~\ref{sec:Tiling_Generator}.


\section{Evaluation}

\subsection{Experimental Setup}

\begin{table}[b] \small 
%\setlength{\abovecaptionskip}{0.2cm}
  \caption{Configuration for Stencil benchmarks}\label{parameters}
  \renewcommand\tabcolsep{1.5pt} 
  \centering\begin{tabular}{lccr}
  \toprule
  Type & Pts & Problem Size & Blocking Size\\ \hline
  Heat-1D&3    & 10,000,000$\times$100,000 & 2,000$\times$1,000  \\
  Star-1D5P&5    & 10,000,000$\times$100,000 & 2,000$\times$500   \\
  Heat-2D&5    & 10,000$\times$10,000$\times$10,000 &  200$\times$200$\times$50                \\
  Star-2D9P&9    & 10,000$\times$10,000$\times$10,000 &  200$\times$200$\times$50                \\
  Box-2D9P&9    & 10,000$\times$10,000$\times$10,000 & 2,000$\times$500   \\
  Box-2D25P&25    & 10,000 $\times$10,000$\times$10,000 & 120$\times$128$\times$60                \\
  Heat-3D&7    &  1,024$\times$1,024$\times$1,024$\times$1,000   &20$\times$20$\times$10                     \\
  Box-3D27P&27   &   1,024$\times$1,024$\times$1,024$\times$1,000           &  20$\times$20$\times$10                    \\ \bottomrule
  \end{tabular}
\end{table}  
\paragraph{Machine} Experimental results presented in this paper are obtained by using a high-performance node on Microsoft Azure, which is composed of an AMD EPYC 7V13 processor with 24 physical cores of 2.45 GHz clock speed. It features the AVX2 SIMD instruction set, and each core contains a 768 KB private L1 cache, a 12 MB private L2 cache, and a unified 96 MB L3 cache. DDR4 DRAM and 8 memory channels are supported for a total 216 GB memory, and it yields a peak memory bandwidth of 409.6 GBps. An Nvidia A100 GPU is also configured with 80 GB memory, and the memory bandwidth could reach 1,935 GB/s. The most characteristic Tensor Cores with 108 SMs deliver a peak FP64 throughput of 19.5 TFLOPS, which is 2.5x that of Nvidia V100. 

\paragraph{State-of-the-arts} %We first performed the performance breakdown experiments with
Although recent studies on Stencil Dwarf only exhibit their own absolute performance without comparison in experiments~\cite{matsumura2020an5d,zhao2019exploiting,ahmad2021fast}, we reproduce artifacts exhaustively and compare the performance of different state-of-the-arts for a comprehensive analysis.
Two classic vectorization methods (Auto Vectorization~\cite{li2022efficient} and Data Reorganization~\cite{10.1145/3126908.3126920}) are employed first as a standard baseline on CPUs. Then the newly-related state-of-the-arts (Tessellation~\cite{yuan2019tessellating} and Folding~\cite{li2021reducing}) are adopted for further comparison. At last, the highly-optimized work with GPU support, Brick~\cite{zhao2019exploiting} and AN5D~\cite{matsumura2020an5d}, are also evaluated thoroughly.
%Since the recent tiling technique (denoted as tessellation) proposed by Yuan \cite{10.1145/3126908.3126920} and the nested/hybrid tiling technique (denoted as SDSL, which is the name of the software package.) presented by Henretty \cite{10.1145/2464996.2467268} outperform the other stencil research like Pluto \cite{bondhugula2008practical,bandishti2012tiling} and Pochoir \cite{10.1145/1989493.1989508}, we take them as two bases of our work, which are vectorized by the compiler and DLT methods, respectively. 
%The parallelization is inherently supported by OpenMP scheme, and W
We utilize the OpenMP pragma \textit{parallel for} on all methods for scalability experiments.% All programs were compiled using the ICC compiler version 19.0.3, with the '-O3 -xHost -qopenmp -ipo' optimization flags.


\paragraph{Benchmarks} We use a variety of Stencil kernels with different shapes as benchmarks. Details are listed in Table \ref{parameters}, which consists of five star kernels (Heat-1D, 1D5P, Heat-2D, Star-2D9P, and Heat-3D) and three box kernels (Box-2D9P, Box-2D25P, and Box-3D27P) drawn from~\cite{10.1145/3126908.3126920,li2021reducing,bandishti2012tiling}.  %Star and box equations are symmetric examples that can represent a wide variety of stencil kernels.
%Other parameters of each stencil are also fine-tuned based on reference work to guarantee that the peak performance for all methods could be reached exactly.  
\paragraph{Programming}
Provided with a mathematical look into various benchmarks, we abstract the application skeletons as a semi-automatic code generator with Python wrapper. For programming on CPU, we use C/C++ for Tessellate Tiling and AVX2 SIMD instruction set for Vector Skewed Swizzling~\cite{guide21url}. Then Checkerboard Blocking is programmed with CUDA C/C++ and Tensor Trapezoid Folding is coded in a manner of MM by Warp Matrix Multiply and Accumulate (WMMA) API~\cite{CUDAToolkit}. 



\paragraph{Metrics}
Most work on Stencil Dwarf (e.g., Tessellation~\cite{yuan2019tessellating,10.1145/3126908.3126920}, Pluto~\cite{bandishti2012tiling}, Folding~\cite{li2022efficient,li2021reducing}, etc.) exhibit results in terms of arithmetic performance (Stencils/s). In this work, we also adopt the metric of stencils per second (Stencils/s) defined in Equation~\ref{eq:stencilss} for measuring the performance. Here, $N_x$, $N_y$, $N_z$ are the problem size for each spatial dimension; $T$ is the iteration item in temporal dimension; $time$ is the total execution time. %Since stencil computation is memory-intensive, memory throughput is also a metric to measure the performance for stencil computation. The related discussion is expanded in Section~\ref{sca} on memory bandwidth for an additional analysis.
\begin{equation}
\label{eq:stencilss}
   \textbf{ stencils per second}=\frac{N_x\cdot N_y\cdot N_z}{time} \times T
\end{equation} 




\subsection{Performance Breakdown}
\begin{figure}
  \begin{center}
  \centering
  \includegraphics[width=0.48\textwidth]{breakdown.pdf}
  \caption{\label{breakdown}{Performance breakdown of Tetris.}}
  \end{center} 
  \vspace{-0.2in}
  %\vspace{-0.5cm}
\end{figure}  
In this subsection, we investigate how the performance is improved progressively with different optimizations by Tetris. 


Figure~\ref{breakdown} shows the performance breakdown of Tetris on three representative benchmarks (Star-1D5P, Box-2D25P, and Box-3D27P) as they contain more complex data access patterns and pose greater performance demands of Tetris. %higher optimization pressure on the performance improvements of Tetris.
%The first bar in each subfigure represents a naive version on multicore CPU. Then optimizations of Tessellate Tiling, Vector Skewed Swizzling, Tensor Trapezoid Folding, and Checkerboard Blocking are accumulated in turn.
%To exhibit the brought benefits more intuitively, the speedup value compared to previous version is markedly annotated on the top of each bar.

%Tetris integrates different optimization methods for better performance.  In figure~\ref{breakdown}, \textcircled{1} in the x-axis represents parallel CPU computation. Specifically, it opens all the -O3 compiling options and openmp except vectorization. \textcircled{2} adds tessellate tiling based on \textcircled{1} to reuse data in the CPU cache. \textcircled{3} attaches Vector Skewed Swizzling to \textcircled{2}, which constitutes the CPU part of Tetris. \textcircled4 additionally leverages the Tensor Core Units computation throughput. In particular, it directly loads data from global memory to local memory. \textcircled{5} additionally uses shared memory for best performance, which finally constitutes the complete Tetris.

As can be seen from Figure~\ref{breakdown}, optimizations on CPU in Tetris are first investigated on three benchmarks. Tessellate Tiling enhances the data reuse in CPU cache effectively and even provide a 45x speedup considerably in Star-1D5P benchmark. Next, Vector Skewed Swizzling utilizing the high-performance SIMD facilities in CPU is performed and apparently outperforms the previous version in all experiments. Since Vector Skewed Swizzling reduces redundant loading costs and minimizes the intra- \& inter- register operations, it could relieve data alignment conflicts in vectorization efficiently and provide significant speedups from 1.6x to 5.7x. %It is worth noting that the Skewed Swizzling speedup is not as much as others in figure~\ref{breakdown}(c) due to the overstretched register allocation caused by excessively dependent data access during the 3D Stencil computation. 
Here we have shown that a nerfed Tetris with multicore CPU could reach 112.5x, 12.0x, 3.1x speedups respectively on these representative benchmarks.

Then GPU is introduced in Tetris for further performance improvements. With the deep exploitation of Tensor Cores, all benchmarks achieve a minimum of doubled performance compared to the nerfed Tetris on CPU. % which demonstrates the effectiveness of GPU compute pipeline optimized by Tensor Trapezoid Folding. 
Like CPU cache, shared memory provides more efficient data retrieves but limited capacity between local register and global memory. After utilizing shared memory by Checkerboard Blocking, 1.3x-1.7x speedups are obtained compared to the previous version. Up to this point, Tetris has accumulatively achieved speedups of 307.1x, 50.9x, 9.0x respectively, showing significant performance benefits brought by Tetris on multidimensional benchmarks.


% This can be attributed to the cheaper dimension-lifting transpose operation in small size for DLT. The multiple loads method exhibits the worst performance among them due to the overhead caused by redundant loads.

%First, tiling always multiplies the speed due to leveraging data reuse in CPU cache. Next, Skewed Swizzling utilizes the high-performance SIMD facilities in CPU. It does not bring more loading overhead than scalar computation and minimizes the intra- \& inter- register operation overhead compared to other vectorization methods. In our experiments, Skewed Swizzling provides 1.6x-5.7x speedups. In figure~\ref{breakdown}(c), the Skewed Swizzling speedup is not as much as others, because a part of required data is evicted from cache during the 3D stencil computing pipeline. Up to this point, we show what the traditional HPC infrastructure can do at best, then we introduce AI infrastructure. With the support of Tensor Core Unit, the speeds of stencil computation at least doubles. Moreover, data reuse in shared memory further improve the stencil computation speeds on GPU. Just like CPU cache, shared memory, as the buffer between local memory and global memory, provides more efficient data retrieves but limited accessibility. After efficiently utilizing shared memory, our Tetris once again gains 1.3x-1.7x speedups. The performance breakdown of Tetris demonstrates both the HPC and AI infrastructures are helpful to HPC problems, which shows efficiency of our new HPC+AI paradigm.
 


\subsection{State-of-the-art Comparison \label{eva:comp}}

\begin{figure*}
%\setlength {\belowcaptionskip} {-2cm}
  \begin{center}
  \centering
  \includegraphics[width=0.98\textwidth]{state-of-the-art.pdf}
  \caption{\label{state-of-the-art}{Performance and speedup comparison of state-of-the-arts and Tetris. The speedups of each group are compared to the lowest base which is annotated with the triangles by default value of 1.0.} }
  \end{center}
  \vspace{-0.2in}
  %\vspace{-0.7cm}
\end{figure*} 


\begin{table}[hbtp]
  \centering \small  
  \begin{threeparttable}
  \renewcommand\tabcolsep{0.1pt} 
  %\setlength{\abovecaptionskip}{0.2cm}
  \caption{\label{tab:benchcomp}{Technical overview of state-of-the-arts and Tetris}}
  \begin{tabular}{lccr}
  \toprule {Methods}&Tiling&  Pipeling  &{Architectures} \\  \midrule 
    Data Reorg.~\cite{10.1145/3126908.3126920} & Split  & Data Reorg. &  C(VR)$^1$ \\
    Auto Vec.~\cite{li2022efficient}  & Split  &Auto Vec.&  C(VR) \\
  %&{POT1\%)}& - & {9.9} & {1.2}   & {6.4}   \\ 
   Pluto~\cite{bandishti2012tiling}& Diamond~\cite{bondhugula2008practical}&AutoVec.   &  C(VR) \\ 
   Folding~\cite{li2021reducing}&Polyhedral&  Folding  & C(VR)  \\
   Tetris (CPU) &Tessellate &  Skewed$^2$ &C(VR)  \\
   Brick~\cite{zhao2019exploiting}&  Brick & Scatter  & G(CCU)  \\
   AN5D~\cite{matsumura2020an5d} & Temporal & - &G(CCU) \\
   Tetris (GPU)& Checkerboard & Trapezoid &G(TCU)  \\
   Tetris &Polymorphic & Template &C(VR) + G(TCU)  \\ 
  %&{POT(\%)}&-&{11.7}   &{3.6}  &{6.2}\\
  \bottomrule \end{tabular} \footnotesize
  %Note: Performance improvements are calculated on the basis of multiple loads. POT index (\%) is calculated based on the performance of the method itself. 
  \begin{tablenotes}
  \item[1] For better clarity, CPU, GPU, Vector Register, CUDA Core Units and Tensor Core Units are abbreviated with C, G, VR, CCU and TCU respectively. 
  \item[2] Notice we use a keyword as an abbreviation for some algorithms. 
  \end{tablenotes}
  \end{threeparttable}
  \vspace{-0.2in}
\end{table} 

In this subsection, we present the experiments that exhibit the benefits of Tetris with various state-of-the-arts on different benchmarks, and the technical overview of them are listed in Table~\ref{tab:benchcomp}.
%Figure~\ref{state-of-the-art} shows the comparison of absolute performance and speedups for different methods under various benchmarks, where speedups are based on multiples of the first bar annotated with a red triangle valued of 1.0x. We also leave out the results in some specific benchmarks that are not supported by a certain method. 
%A tiny minority of  results are incomplete in specific benchmarks that are not supported by all state-of-the-arts. 


As shown in Figure~\ref{state-of-the-art}, taking all benchmarks with AVX2 instructions on CPU into account, remarkable performance improvements are observed from Tetris (CPU). Compared with the newly-released state-of-the-art Folding, Tetris (CPU) improves the overall performance sustainably by an average of 21\% and a maximum of 25\% in Box-2D9P and Box-3D27P benchmarks, which demonstrates that our optimization on CPU architecture could provide a significant benefit especially in complex pattern compared to the referenced work.


Here, to evaluate whether a nerfed Tetris only run on GPU can achieve a competitive performance, we additionally employ another two state-of-the-arts for a further comparison, where AN5D is a highly-optimized work using CUDA Cores. Apparently, Figure~\ref{state-of-the-art} shows Tetris (GPU) achieves the best for all benchmarks  even compared to the previous CPU baselines. Compared to AN5D, Tetris (GPU) achieves an overall 1.9x speedup on average and even reach a maximum 2.2x speedup in Figure~\ref{state-of-the-art}(f)! The reason is that, the Dwarf computation on Tensor Core is adapted efficiently by Tensor Trapezoid Folding, and Checkerboard Blocking make full use of shared memory for data reuse especially more friendly to complex benchmarks.
%We compared the performance of Tetris with various previous State-of-the-arts in different shapes. The results are shown in figure~\ref{state-of-the-art}. Each sub figure shows the performance of different stencil kernels in a certain shape. The left y-axis represents GStencils per second, while the right one represents the speedup compared to the lowest base.
%It is shown that our method achieves the state-of-the-art performance in all dimensions. In most shapes, our Tetris GPU kernels have already shown the best performance, not to speak of the full-Tetris. AN5D is a highly-optimized work using CUDA core. By comparing Tetris GPU with AN5D, the AI infrastructure Tensor Cores' computing power can be thoroughly demonstrated. Tetris GPU is faster than AN5D in all dimensions as shown in the figure. Our method performs best in Box-2D25P, where it gets more than 7.9x speedup, because GPU kernel utilizes shared memory better in this shape.

When Tetris is performed on CPU and GPU, the performance potential is fully expressed and it runs rings around all state-of-the-arts. We still observe that the Tetris performance is arithmetically close to the sum of two nerfed versions (Tetris (CPU) and Tetris (GPU)) for all benchmarks. This is because the cost of communication between CPU and GPU is greatly reduced and overlapped with computation, and the auto-tuning scheduling mechanism provides a balanced task partition, which make the utmost of computing power on Cloud. Compared to Data Reorganization, Tetris improves the overall performance by an average of 4.4x and a maximum of 8.1x in Box-2D25P.

%Our full-Tetris method combines Tetris CPU and GPU kernels to cooperate. The input data will be split by a constant ratio and be assigned to CPU and GPU respectively. The split ratio is a super parameter which is confirmed by experiment. After both work is accomplished, a data transfer will be conducted to correct the answer around the split border. Since Tetris combines CPU and GPU's computing ability, the data split ratio is crucial to the performance. Tetris with a good split ratio will get a performance close to the summary of Tetris CPU's and GPU's performance. We search the best ratio under different shapes by experiment, starting from a low CPU work ratio to a higher one to balance the CPU and GPU's computing power. As the result shows, a ratio can be found to makes the Tetris's GStencils close to Tetris CPU's GStencils plus GPU's.

\subsection{Scalability Evaluation}
\begin{figure}
  \begin{center}
  \centering
  \includegraphics[width=0.48\textwidth]{scaling.pdf}
  \caption{\label{scaling}{Scalability for Tetris with various benchmarks. Solid lines with markers represent the performance, and scheduling ratios (GPU to CPU) of task partition are illustrated with dotted lines.}}
  \end{center} 
  \vspace{-0.5cm}
\end{figure}   

We also evaluate the scalability of Tetris on Cloud for a comprehensive analysis. The detailed parameters are given in Table~\ref{parameters}, where all problem sizes are large enough to approach the scale in a real simulation.  

It can be observed from Figure~\ref{scaling} that Tetris could achieve a good scaling with increased CPU cores. In 1D and 2D benchmarks, nearly linear scalings on Cloud are obtained by Tetris and the CPU with 24 cores provides a competitive performance to GPU in the same order of magnitude.
Surprisingly, a 49.9\% scheduling ration is determined by Tetris, which demonstrates a scalable scheduler design and breaks the stereotype of CPU inferior to GPU.
With the increase of the benchmark dimension, the scalability drops slightly when more CPU cores are employed due to the inherent complexity for multidimensional computations. Moreover, it has to perform a great deal of coordination among CPU cores, incurring communication overheads inevitably.  
 


\subsection{Case Study: Thermal Diffusion  }
\begin{figure}[hbt]
  \begin{center}
  \centering
  \includegraphics[width=0.4\textwidth]{interface.pdf}
  \caption{\label{interface}{Code snippets for thermal diffusion simulation with Tetris.}}
  \end{center} 
  \vspace{-0.2in}
  %\vspace{-0.5cm}
\end{figure}  


\begin{figure}[hbt]
  \begin{center}
  \centering
  \includegraphics[width=0.45\textwidth]{heat_diff.pdf}
  \caption{\label{heat_diff}{Visualization of temperature distribution before and
after the thermal diffusion on the square Cu plate. The difference is measured by  absolute errors (the reds: positive errors (hotter), the greens: zero errors, the blues: negative errors (colder)).}}
  \end{center} 
  \vspace{-0.5cm}
\end{figure}   


\begin{table}[hbt] \small  
  \caption{Performance improvements by Tetris}\label{label:heat}
  \renewcommand\tabcolsep{2pt} 
  \centering\begin{tabular}{lccr}
  \toprule
  Methods &     Time(s) & Performance (GStencil/s) & Speedup\\ \hline
  Naive &  124,448.5 & 2.8 & -\\
  Tetris (CPU) &   27,709.1 & 14.8 &4.5x\\
  Tetris (GPU) &    5,597.4 & 63.3 &22.6x\\
  Tetris &    4,270.9 & 82.9 &29.6x\\
  \bottomrule
  \end{tabular}
\end{table}  

 

\begin{table}[hbt]\small 
\begin{threeparttable}
\caption{Analytical accuracy comparison$^1$}\label{label:error}
  \renewcommand\tabcolsep{1.5pt} 
\begin{tabular}{lcccccr}
\toprule
 \multirow{2}{*}{Deviation}& \multicolumn{3}{l}{ Absolute Error ($^{\circ}$C)} & \multicolumn{3}{l}{Relative Error (\%)} \\% \cmidrule(r){0-0} \cmidrule(r){2-4}  \cmidrule(r){5-7}
 &  $>$0.1 & $>$0.5 & $>$1.0 &      $>$1.0 & $>$3.0 & $>$5.0  \\\cmidrule(r){0-0} \cmidrule(r){2-4}  \cmidrule(r){5-7}
Tetris by FP64 (\%)  &    0.0 & 0.0 & 0.0    &     0.0 & 0.0 & 0.0   \\
Naive by FP32 (\%)&   73.1 & 17.2 & 11.3  &     62.1 & 14.3 & 0.2\\
 \bottomrule  
\end{tabular}
\begin{tablenotes}\footnotesize
  \item[1] The accuracy errors are all compared with the Naive method(FP64).  
  \end{tablenotes}
\end{threeparttable}
\vspace{-0.1in}
\end{table}

To further prove the validity of Tetris in real application, simulations of thermal diffusion on a square copper plate (CFL number: $\mu = 0.23$) are performed at 100$^{\circ}$C with dimensions of 15 mm on a side (Grid size: $9,600\times 9600$). The simulation is to calculate the temperature distribution $D$  depending on time for a long-time evolution ($3.8\times 10^6$ time steps), i.e. $D = U(t, x, y)$ simulated by Laplacian numerical calculation using 5-point Stencil finite difference methods in Section~\ref{pde_derivation}. 
%In order to evaluate the performance improvement of the Tetris for practical problems, we carried out a classic problem for testing, the thermal diffusion.

%providing an interface for specifying both coarse- and fine-grained parallelism, as well as providing the user with a simple means for configuring all aspects of parallelism to allow good performance to be achieved for their application with minimal effort, and with limited knowledge of GPUs or parallel and distributed systems.

Provided with a simple interface illustrated in Figure~\ref{interface}, scientists could democratize large-scale thermal simulation on Cloud with minimal efforts by Tetris. Table~\ref{label:heat} shows the detailed performance improvements on
thermal diffusion by Tetris. The speedup ranges from 4.5x to 29.6x with different versions of Tetris, showing that it could provide a significant benefit over real {scientific computing} applications.

As shown in Figure~\ref{heat_diff}(a), at the beginning of the simulation, it is clear that the initial temperature is a Gaussian, and it will be cooler at the edges with the hottest temperature at the plate center. After a long-time thermal evolution by Tetris, the temperature distribution in Figure~\ref{heat_diff}(b) is radically different from the one obtained at the beginning, and the temperature of plate center decreases to 52.0$^{\circ}$C, which is coherent with the thermodynamic theory~\cite{bluman1969general}. 
%Thus, the result obtained with long-time evolution is another powerful evidence for the validity of OpenKMC.

It is worth noting that we also perform a comparison experiment with FP32 on the same initial state of the system. The results by FP32 and the differences with Tetris (FP64) are visualized in Figure~\ref{heat_diff}(c) and Figure~\ref{heat_diff}(d) respectively.
Table~\ref{label:error} also lists an analytical accuracy comparison of FP64 and FP32 intuitively. Significantly, an overall 73.1\% deviation is measured for temperatures fluctuated by 0.1$^{\circ}$C, which demonstrates an unignorable variation caused by insufficient accuracies (FP32). % high-precision arithmetic facilities are now an indispensable component of a modern large-scale scientific computing environment

\begin{comment}
\begin{listing}[!ht]
\begin{minted}[frame=lines]
{python}
def heat_transfer(size, times, params):
    # Init matrices and parameters
    def init(size): return matrix_in
    # Calc heat propagation via GPU and CPU
    def calc_mix(m_in):
        # Calc heat propagation via GPU
        def calc_gpu(m_in): return m_out
        # Calc heat propagation via CPU
        def calc_cpu(m_in): return m_out
    # Draw the heat distribution map
    def draw(m_in,m_out): return f_in,f_out
\end{minted}
\caption{Heat Propagation Computational Simulation}
\label{listing:heat}
\end{listing}
\end{comment}
%Listing 1 outlines the process of simulating heat calculations in Tetris.Using our API, developers can customize the size of the matrix and initial data, iteration time and calculation parameters (CFL number $\mu$) to simulate heat propagation and obtain visualization results. We use both CPU and GPU simultaneously to efficiently calculate the heat matrix, and its performance improvement can be seen in Table \ref{label:heat}.

 



%Since CFL number $\mu$ needs to be less than $1/2$(otherwise there will be decaying spurious oscillations, $2/9$ here), it will take numerous iteration steps to obtain an obvious propagation effect when the size is large, a moderate size($9600\times9600$) was chosen for simulation. Although limited by size, Tetris cannot maximize its performance, but it still has a huge improvement in heat propagation simulation, whether relative to the optimized CPU or GPU algorithm, it is 1.3 and 4.2 factors, respectively. And since the naive stencil calculation method takes too long at the size, what we provide is only a estimated time in Table \ref{label:heat}.

%\subsubsection{Result graph}

%Figure 2 shows a visualization of the simulation after 3840000 steps using our methods for the size $(9600\times 9600)$, the obvious difference between single and double precision indicating that high precision is essential for stencil calculations.


\section{Related Work}

%\paragraph{HPC+AI for science} While high-performance heterogeneous architectures equipped with GPUs are pervasive, there are various couplings of integrating traditional HPC applications with AI methodologies for science. 
%AI4HPC innovations are enthusiastically driven by the need for accuracy approaching that of theoretical methods, while retaining the low computational cost and linear strong scaling. 
%In the area of High Energy physics, AtlFast3 is presented for the simulation needs of the ATLAS experiments, and it is deployed with machine learning techniques in place of intensive Geant4 simulations. 
%For structural biology tasks, DeepMind’s AlphaFold 2 achieves atomic accuracy for protein structure prediction by leveraging a transformer-based model architecture.  Another exemplar applications of AI4HPC can be found in ab initio molecular dynamics (AIMD) methods for modeling atomistic phenomena.
%Jia employs an AI-based method by utilizing surrogates (Deep Potential MD) in tandem with a highly-optimized code to provide a powerful example of AI4HPC application, and it underpins the Gordon Bell Prize that year.  However, in spite of even orders of magnitude improvement in computing efficiency from these ensemble algorithms, achieving the necessary accuracy in scientific computing with AI-driven methods introduced is still an intractable challenge, where an artful scale-accuracy trade-off is not applicable for all scenarios.
%More scientists stick to employing numerical calculation exclusively in practice to avoid the accuracy influence caused by AI methodologies. For example, climate scientists are more conservative in the scenarios of atmosphere circulation, ocean modelling, and weather prediction. The lack of understanding on the coupling of AI models with the physical mechanism makes it difficult for scientists to trust AI-driven black-box solutions, where the minor errors are magnified iteratively to produce false predictions.
%Recently, some emerging work for scientific computing makes an attempt to run HPC workloads concurrently across diverse and heterogeneous platforms configured with AI infrastructures. Ben accelerates large-scale excited-state GW calculations on full leadership HPC systems, Summit, with double precision implemented by OpenACC on GPUs. K. Nguyen-Cong also implements SNAP force kernel in LAMMPS using the Kokkos CUDA backend on NVIDIA GPUs combined with good strong scaling for a 20 billion atom MD simulation on the full Summit HPC systems. These work leverages the computing power of GPUs with automated tools such as OpenACC, OpenMP and Kokkos or adapting kernels on CUDA cores in a straightforward way, which fails to boost the performance of GPU effectively. Up to now, most of existing work still adopts optimization case by case, and Tetris is the first work exploring to accelerate Stencil-related scientific computing by a general one-stop solution on HPC+AI heterogeneous infrastructures without any accuracy loss. 

%\paragraph{HPC Stencil Dwarf for Science} 

%{ In the past, supporting scientific computing is not the priority of Cloud because of the complicated spatial and temporal data dependencies~\cite{alam2023cloud,ogbole2021cloud,soh2020microsoft}. Although there exists few work years ago to explore scientific computing on Cloud, such work is constrained to a single architecture and obsolete today with a growing architecture gap between CPU and GPU~\cite{rehr2010scientific,srirama2010scicloud,srirama2012adapting,ostermann2009extending}.} %Techniques on supercomputers focuses primarily on improving tiling for one type of memory on a specific hardware with a tiling shape. However, there are different memory types and distinct hardware units for CPU and GPU on Cloud; as a result, developing new tiling techniques to support scientific computing becomes a crucial task.}
Known as one of the seven Dwarfs, Stencil is extensively involved in scientific computing~\cite{li2021reducing,asanovic2006landscape}. The representative work can largely be classified in two directions in terms of architectures.

It is acknowledged that {scientific computing} is sensitive to the data precision, and thus a large quantity of work addresses this issue by utilizing FP64 operations on CPU architectures. Traditional approaches have mainly focused on either vectorization or tiling schemes, aiming at improving the in-core data parallelism and the data locality in cache respectively~\cite{yuan2019tessellating}. %Although they are often regarded as orthogonal methods working at different levels, most work still concentrates on a single technique for optimization due to complex and intractable challenges involved in them.
%Exploring data-level parallelism is an effective way to improve computation performance for stencils~\cite{li2022efficient}. 
For SIMD implementation on CPU, data alignment conflicts incurred by vectorization is a crucial performance-limiting factor~\cite{li2022efficient,henretty2011data,zhao2019exploiting}. %Data reorganization on optimizing the order of execution instructions could decrease loads/stores operations to relieve the register pressure, while the execution unit for data permutations inside the CPU may
%become the bottleneck~\cite{li2022efficient}. Henretty proposes a new method DLT to overcome input spatial data conflicts at the expense of a dimension-lifting transpose, which makes it infeasible to perfectly utilize the tiling technique as a result of its spatially separated data elements~\cite{henretty2011data}.  
%Basu designs a vector code generation scheme to reuse several vectors in the computation process, and it is constrained to constant-coefficient and isotropic stencils~\cite{basu2015compiler}. 
%YASK could improve data reuse by using common expression elimination and unrolling based on their vector-folding methods with fine-grained blocks, which is optimized only for high-order 3-dimensional stencils~\cite{yount2015vector,yount2016yask}. 
Bandishti et al. enhance the Pluto compiler to incorporate a strategy for diamond tiling, and is particularly effective in parallelizing stencil computations~\cite{bandishti2012tiling}.
Folding reduces the data reorganization overhead for vectorization and shows a better performance than Pluto~\cite{bandishti2012tiling}, Tessellation~\cite{yuan2019tessellating}, YASK~\cite{yount2015vector,yount2016yask} and DLT~\cite{henretty2013compiler}, which
is considered as one of state-of-the-arts approach for vectorization. However, the frequent in-CPU transpose easily incurs register spilling~\cite{li2021reducing}.  
Tiling~\cite{Irigoin.Triolet:popl88,McKeller.Coffman:cacm69,Lam+:asplos91,Wolf.Lam:pldi91,Wolfe:sc89} is one of the most powerful transformation techniques to explore the data locality of multiple loop nests. Notable work for Stencil includes hyper-rectangle tiling~\cite{Ding.He:sc01,Rastello.Dauxois:ipdps02,Rivera.Tseng:sc00,Nguyen+:sc10},
time skewed tiling~\cite{Song.Li:pldi99,Wonnacott:ijpp02,Jin+:sc01},
diamond tiling~\cite{bondhugula2008practical,bandishti2012tiling},
cache oblivious Tiling~\cite{10.1145/1989493.1989508,strzodka2010cache,Frigo.Strumpen:ics05}, split-tiling~\cite{henretty2013compiler} and Tessellation~\cite{yuan2019tessellating}, which are mostly compiler transformation techniques. A variety of auto-tuning frameworks \cite{christen2011patus,gysi2015modesto,kamil2010auto,zhang2012auto} have also been presented by using varied hyper-rectangular tiles to exploit data reuse alone. However, redundant computations are involved in these work to resolve the introduced inter-tile dependencies especially when combined with vectorization, which hinder the concurrent execution on different cores.  

With the interests in more efficient solutions, Stencil on GPU is also being exploited in the community~\cite{holewinski2012high,zhang2012auto,grosser2013promises}. Common stencil optimizations on GPU include tiling and loop unrolling~\cite{holewinski2012high,krishnamoorthy2007effective,meng2009performance}, however, correct and efficient implementation of these techniques are challenging because careful utilization of limited registers and shared memory is required for complex data dependencies~\cite{matsumura2020an5d}. Since GPU runs low-precision operations (FP16/FP32) highly faster than high precision (FP64), most existing work employ low-precision designs while they are only applied to a limited set of benchmarks and hardly meet the strict tastes of {scientific computing}~\cite{maruyama2014optimizing,nguyen20103}.    %Rawat presents another DSL-based stencil framework called ARTEMIS, and it supports flexible resource allocation from different memory hierarchies on GPUs. Compared to the aforementioned work through the optimization on memory hierarchy, SIMT techniques are less explored and mostly implemented by automated tools. 
%Claimed by YASK~\cite{yount2015vector,yount2016yask}, the GPU support is coming to fruition via OpenMP device offload. 
Zhao presents a general stencil framework by exploiting data reuse within a block~\cite{zhao2019exploiting}, however, the algorithms are implemented separately on CPUs and GPUs in practice. AN5D achieves high-degree temporal blocking on GPUs, while it only conducts the parameter search with single-precision, and the method is limited to CUDA cores~\cite{matsumura2020an5d}. %Liu adapts stencil computation to TCU by considering the unique characteristics on GPU, while it achieves all kernels with single precision. 
%To the best of our knowledge, Tetris is also the first disruptive work that adapts high-precision HPC calculations to GPU efficiently by considering its unique hardware characteristics.  

%While Cloud equipped with high-performance CPU+GPU heterogeneous architectures is pervasive, interests are increasingly derived in exploring {scientific computing} Dwarfs on Cloud. However, as stated above, almost all the existing work only concentrate on a specific technique on a single architecture, such as vectorization on CPU, spatial tiling on GPU, etc. This makes it hard for these methods to scale to Cloud directly. Our proposal is a major shift from these methods in that we leverage Cloud to democratize {scientific computing} for the first time, and it is orthogonal to (and complements) the optimizations or deployments for a wide variety of emerging and legacy {scientific computing} applications. 


\section{Conclusion}
This paper presents Tetris, a disruptive system that democratizes {Stencil-driven scientific computing} on CPU and GPU of Cloud with polymorphic tiling tetrominoes to tessellate the spatial and temporal dimensions perfectly. Sophisticated Pattern Mapping by register-level tetrominoes, efficient Locality Enhancer by cache/SMEM-level tetrominoes, and novel concurrent scheduler by memory-level tetrominoes are also first proposed to improve the performance on heterogeneous architecture. Our promising results demonstrate that Tetris can achieve an unprecedented performance improvement compared to the existing state-of-the-arts.

\bibliographystyle{acm}
\bibliography{ref.bib}


%\theendnotes

\end{document}







