\documentclass[letterpaper]{article} % DO NOT CHANGE THIS

\usepackage{aaai22}  % DO NOT CHANGE THIS
\usepackage{times}  % DO NOT CHANGE THIS
\usepackage{helvet}  % DO NOT CHANGE THIS
\usepackage{courier}  % DO NOT CHANGE THIS
\usepackage[hyphens]{url}  % DO NOT CHANGE THIS
\usepackage{graphicx} % DO NOT CHANGE THIS
\urlstyle{rm} % DO NOT CHANGE THIS
\def\UrlFont{\rm}  % DO NOT CHANGE THIS
\usepackage{natbib}  % DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT
\usepackage{caption} % DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT
\DeclareCaptionStyle{ruled}{labelfont=normalfont,labelsep=colon,strut=off} % DO NOT CHANGE THIS
\frenchspacing  % DO NOT CHANGE THIS
\setlength{\pdfpagewidth}{8.5in}  % DO NOT CHANGE THIS
\setlength{\pdfpageheight}{11in}  % DO NOT CHANGE THIS
\usepackage{bibentry}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{multirow}
\usepackage{booktabs}

\usepackage{amsmath}
\usepackage{amssymb}

\usepackage{newfloat}
\usepackage{listings}
\lstset{%
	basicstyle={\footnotesize\ttfamily},% footnotesize acceptable for monospace
	numbers=left,numberstyle=\footnotesize,xleftmargin=2em,% show line numbers, remove this entire line if you don't want the numbers.
	aboveskip=0pt,belowskip=0pt,%
	showstringspaces=false,tabsize=2,breaklines=true}
\floatstyle{ruled}
\newfloat{listing}{tb}{lst}{}
\floatname{listing}{Listing}

\nocopyright

\setcounter{secnumdepth}{2} %May be changed to 1 or 2 if section numbers are desired.


\title{Data Might be Enough: Bridge Real-World Traffic Signal Control Using  Offline Reinforcement Learning}
% \title{Optimzing Traffic Signal Control with Offline Reinforcement Learning}
\author{
    Liang Zhang\textsuperscript{\rm 1},
    Jianming Deng\textsuperscript{\rm 1}\thanks{Jianming Deng is the corresponding author. Email: dengjm@lzu.edu.cn}
}
\affiliations{
\textsuperscript{\rm 1}  College of Ecology, Lanzhou University, Lanzhou 730000, China\\
}



\begin{document}

\maketitle

\begin{abstract}

Applying reinforcement learning (RL) to traffic signal control (TSC) has become a promising solution.
%
However, most RL-based methods focus solely on optimization within simulators and give little thought to deployment issues in the real world.
%
Online RL-based methods, which require interaction with the environment, are limited in their interactions with the real-world environment.
%
Additionally, acquiring an offline dataset for offline RL is challenging in the real world.
%
Moreover, most real-world intersections prefer a cyclical phase structure.
%
% However, most RL-based methods require interaction with the environment, limiting their deployment in real-world scenarios. 
% Moreover, most real-world intersections prefer a cyclical phase structure. 
%
To address these challenges, we propose: 
(1) a cyclical offline dataset (COD), designed based on common real-world scenarios  to facilitate easy collection; 
(2) an offline RL model called DataLight, capable of learning satisfactory control strategies from the COD; 
and (3) a method called Arbitrary To Cyclical (ATC), which can transform most RL-based methods into cyclical signal control.
%
Extensive experiments using real-world datasets on simulators demonstrate that: 
(1) DataLight outperforms most existing methods and achieves comparable results with the best-performing method; 
(2) introducing ATC into some recent RL-based methods achieves satisfactory performance; 
and (3) COD is reliable, with DataLight remaining robust even with a small amount of data.
% 
These results suggest that the cyclical offline dataset might be enough for offline RL for TSC.
Our proposed methods make significant contributions to the TSC field and successfully bridge the gap between simulation experiments and real-world applications.
%
Our code is released on Github~\footnote{https:github.com/LiangZhang1996/DataLight}.

\end{abstract}


\section{Introduction}

% introduce to TSC & RL for TSC
% TSC
Managing and alleviating traffic congestion is a significant challenge, with traffic signal control (TSC) playing a crucial role.
% traditional methods
Although traditional methods, such as FixedTime~\cite{fixedtime}, GreenWave~\cite{greenwave}, SCATS~\cite{scats2}, and SOTL~\cite{sotl2013} are widely used, they heavily rely on hand-crafted traffic signal plans or pre-defined rules and are not  adaptive and flexible enough to handle varying traffic conditions.
%
Surprisingly, some deep reinforcement learning (RL) methods have not yet surpassed the performance of traditional TSC methods~\cite{efficient,ql,advanced}, indicating that traditional TSC methods remain highly competitive.

Recently, RL approaches~\cite{rl} have gained popularity in TSC due to their ability to learn and adapt to different traffic conditions by interacting with the environments through trial and error. 
%
% RL-based methods such as CoLight~\cite{colight}, AttendLight~\cite{attend}, Advanced-XLight~\cite{advanced}, and DynamicLight~\cite{dynamiclight} have shown remarkable results in TSC, such as superior performance for large-scale TSC, handling different intersection topologies, and achieving state-of-the-art performance for TSC. 
Some RL-based methods have yielded remarkable results. 
%
For instance, CoLight~\cite{colight} has shown superior performance and capacity for large-scale TSC;
%
AttendLight~\cite{attend} developed a universal model capable of handling different intersection topologies;
%
Advanced-XLight~\cite{advanced} adopts more effective state representations than CoLight and MPLight~\cite{mplight}, resulting in further improvements in control performance;
%
DynamicLight~\cite{dynamiclight} developed a two-stage framework that achieves state-of-the-art performance for TSC by realizing dynamic phase duration.
Therefore, RL-based methods remain a promising solution for adaptive TSC.


Despite the satisfactory performance of RL-based methods in numerical experiments, most of them cannot be directly applied in the real world 
%
Online interaction, where RL agents learn from direct interaction with the environment, is often impractical due to the high cost of data collection. 
%
Furthermore, RL agents trained on simulations may not be suitable for real-world scenarios due to the limitation of the numerical simulators, such as SUMO~\cite{sumo} and CityFlow~\cite{cityflow}.
%Moreover, although RL agents trained on simulations show promising results, their applicability in real-world scenarios can be limited by the accuracy and fidelity of the numerical simulators used for training, such as SUMO~\cite{sumo} and CityFlow~\cite{cityflow}.
%
To overcome these limitations, an alternate approach is to utilize offline RL algorithms that effectively leverage previously collected experience without requiring online interaction.
%
Recent developments, such as conservative Q-learning (CQL)~\cite{CQL} and decision transformer~\cite{DT}, hold tremendous promise for turning datasets into powerful decision-making engines.%  similar to how datasets have proven key to the success of supervised learning in vision and NLP. 
%
Therefore, offline RL is considered a promising solution for building efficient transportation systems that can be deployed in reality.


Collecting an appropriate offline dataset poses a significant challenge since acquiring experience from an RL agent interacting with the real world is not feasible, and random data from the real world is not suitable. 
%
To bridge the gap between the numerical simulator and the real world, we need to gather appropriate offline datasets from intersections that follow a cyclical phase structure,  which is the most commonly used method for TSC in modern cities. 
% In modern cities, FiexdTime~\cite{fixedtime} is the most commonly used method for TSC, and most intersections follow a cyclical phase structure.
% %
% To bridge the gap between the numerical simulator and the real world, we need to gather appropriate offline datasets from these intersections and train a reliable RL agent.
In the development of real-world applications, offline datasets are more critical than offline RL methods, since some offline RL methods have already achieved satisfactory results.
Nonetheless, offline datasets are the key to bridging the gap between simulation and reality.


Most RL-based methods allow for any phase to be actuated in any sequence, resulting in irregular and unpredictable signal patterns that can lead to confusion and safety hazards for drivers in real-world applications.
%
Although this non-cyclical phase selection may increase traffic throughput, it can also cause  unbounded waiting times and the appearance of some waiting drivers being skipped. 
% 
Therefore, a cyclical phase structure for traffic signals is typically preferred in real-world applications as it enables drivers to anticipate signal patterns based on their driving habits, thereby reducing confusion and improving safety.
%
Additionally, some intersections require conditional phase orders that can further complicate the signal control process. 
%
For instance, at intersections with waiting areas~\cite{leftturn2}, the 'go straight' phase must follow the 'turn left' phase. 
%
Thus, developing approaches that maintain a cyclical phase structure while leveraging RL to enhance efficacy is crucial for practical deployments.
 

%
This paper aims to address the challenges mentioned above by:
\begin{itemize}
    \item Collecting Cyclical Offline Datasets (COD) from intersections with a cyclical phase structure;
    \item Developing DataLight, which can learn satisfactory performance from the COD;
    \item Proposing Arbitrary To Cyclical (ATC), a method can convert most RL-based methods into cyclical signal control.
\end{itemize}

Extensive experiments on real-world datasets demonstrate that: the COD is reliable for offline RL; DataLigt outperforms most existing methods and achieves comparable results with the best-performing method; introducing ATC into some RL-based methods achieves satisfactory performance; and  DataLight remains robust even with low data regimes. 
These proposed methods provide significant contributions to the TSC field and successfully bridge the gap between experiments and real-world applications.


\section{Related Work}
This section introduces RL-based TSC methods, offline RL, and cyclical signal control.

\subsection{RL-based TSC Methods}

To enhance the performance of RL-based TSC methods, various approaches have been proposed.
% There are various approaches for enhancing the performance of RL-based TSC methods. 
One such approach is to design an effective state or reward function, which has been employed by different RL-based methods to improve TSC performance. 
For instance, PressLight~\cite{presslight} has been improved in LIT~\cite{LIT} and IntelliLight~\cite{intellilight} by introducing 'pressure' into the state and reward function design. 
Similarly, Efficient-XLight~\cite{efficient} has proposed a pressure calculation schema and introduced pressure as a state representation to enhance the control performance of MPLight~\cite{mplight} and CoLight~\cite{colight}. 
AttentionLight~\cite{ql} has used the queue length as both state representation and reward function, leading to improvement over FRAP~\cite{frap}. 
Advanced-XLight~\cite{advanced} has introduced the number of effective running vehicles and traffic movement pressure as state representations. 


% Various approaches have been employed to explore how to improve the performance of RL-based TSC methods.
% %
% RL-based methods can improve the TSC performance by designing an effective state or reward function.
% %
% PressLight~\cite{presslight} has been improved in LIT~\cite{LIT} and IntelliLight~\cite{intellilight}  by introducing 'pressure' into state and reward design.
% % 
% Efficient-XLight~\cite{efficient} proposed a pressure calculation schema and introduced pressure as a state representation, improving the control performance of MPLight~\cite{mplight} and CoLight~\cite{colight}.
% %
% %
% AttentionLight~\cite{ql} applied queue length and used it both as state representation and reward function, getting improvement from FRAP~\cite{frap}.
% %
% Advanced-XLight~\cite{advanced} introduced the number of effective running vehicles and traffic movement pressure as state representations.

Another approach for improving TSC performance is by developing an effective network structure. 
For example, \citeauthor{frap} have developed a particular network structure to construct phase features and capture phase competition relations. 
CoLight~\cite{colight} has adopted a graph attention network~\cite{gats} for intersection cooperation. 
AttendLight~\cite{attend} has used an attention network to handle different topologies of intersections.
% 


% Some methods enhance TSC performance by developing an effective network structure.
% %FRAP~
% \citeauthor{frap} developed a particular network structure to construct phase features and capture phase competition relations.
% %
% CoLight~\cite{colight} adopted graph attention network~\cite{gats} to realize intersection cooperation.
% %
% AttendLight~\cite{attend} adopted the attention network to handle the different topologies of intersections.


Advanced RL techniques can also help improve TSC performance.
%
HiLight~\cite{hilight} has enabled each agent to learn a high-level policy that optimized the objective locally with hierarchical RL~\cite{hierarchical}. 
%
Meta-learning~\cite{meta} has enabled MetaLight~\cite{metalight} to adapt more quickly and stably in new traffic scenarios.
%
Moreover, DynamicLight~\cite{dynamiclight} novelty proposes a two-stage framework that uses Max Queue-Length~\cite{ql} to select the proper phase and employs a deep Q-learning network to determine the duration of the corresponding phase, achieving state-of-the-art performance for TSC.

However, it should be noted that all the aforementioned methods use an online approach to learn RL models, which may not be suitable for real-world applications. 

\subsection{Offline RL}
Offline RL, also known as batch RL~\cite{batchRL}, addresses the problem of learning a policy from a fixed dataset consisting of single-step transitions, allowing the learning of powerful decision-making engines from previously collected experiences without requiring online interactions.
% This approach enables the learning of powerful decision-making engines from previously collected experiences without requiring online interactions.
%
However, a fundamental problem with off-policy RL is the extrapolation error~\cite{BCQ}, in which unseen state-action pairs are erroneously estimated to have unrealistic values. 
 %
Recent works such as batch-constrained deep Q-learning (BCQ)~\cite{BCQ} and bootstrapping error accumulation reduction (BEAR)~\cite{BEAR} address this issue by using state-conditioned generative models to encourage on-policy behavior and penalize divergence from the behavior policy support.

Moreover, standard off-policy RL methods can also fail due to the overestimation of values induced by the distributional shift between the dataset and the learned policy, especially when training on complex and multi-modal data distributions. 
%
To overcome this challenge, conservative Q-learning (CQL)~\cite{CQL} learns a conservative Q-function that lower bounds the expected value of a policy under this Q-function to its true value. 

Encouraging the learned policy to be close to the behavior policy is a common theme in previous approaches to offline RL. 
%To evaluate the effect of different behavior policy regularizers, 
Behavior regularized actor-critic (BRAC)~\cite{BRAC} generalizes existing approaches while providing more implementation options.
% Unlike prior approaches that fit value functions or compute policy gradients, 
Decision Transformer~\cite{DT} outputs the optimal actions by leveraging a causally masked Transformer. 

CQL and Decision Transformer have achieved state-of-the-art performance in offline RL tasks such as robot control and Atari games.
%
However, these offline RL methods have not yet been applied to the TSC field.

 It should be noted that the offline RL approach has some limitations when it comes to real-world applications. 
 For instance, it assumes that the dataset is fully representative of the real-world environment, which may not always be the case. 
 Additionally, offline RL methods require extensive data collection and processing, which can be time-consuming and expensive. 
 Therefore, it is important to carefully consider the trade-offs between offline and online RL methods when applying RL to real-world problems.


\subsection{Cyclical Signal Control}

Recent RL-based TSC methods have not been designed to handle cyclical signal control. 
However, a few studies have focused on this area within the traditional TSC field using max-pressure, including the work by \citeauthor{mp2015} and \citeauthor{mp2020}.
%
DynamicLight~\cite{dynamiclight} is a two-stage TSC framework that can achieve cyclical signal control by replacing the Max Queue-Length with a cyclical signal control policy.
% DynamicLight~\cite{dynamiclight} can realize cyclical signal control with a two-stage TSC framework that the Max Queue-Length can be replaced by a cyclical signal control policy.
%
The unique structure of DynamicLight gives it certain advantages that cannot be easily replicated by other RL-based methods, such as CoLight~\cite{colight} and FRAP~\cite{frap}.
% The advantages of DynamicLight stem from its unique structure  and cannot be easily replicated by other RL-based methods such as CoLight~\cite{colight} and FRAP~\cite{frap}.
% Few studies have investigated cyclical signal control in RL-based TSC due to the performance limitations and algorithmic complexity, although some research has been conducted in this area, such as the work by \citeauthor{liang} and \citeauthor{dynamiclight}.
Although some research has been conducted in this area, such as the work by \citeauthor{liang} and \citeauthor{dynamiclight}., few studies have investigated cyclical signal control in RL-based TSC due to performance limitations and algorithmic complexity.
 %
 
 It is worth noting that while DynamicLight has shown promising results in handling cyclical signal control, recent RL-based TSC methods have yet to be applied to this area. Further research is needed to explore the potential of RL-based TSC methods for cyclical signal control.

\section{Preliminary}

In this section, we provide the necessary definitions of the traffic environment and offline RL, as well as the problem statement for this article.

\subsection{Traffic Environment}
\paragraph{Traffic network} 
A traffic network (Figure~\ref{fig:inter} (a)) is usually described as a directed graph in which intersections and roads are represented by nodes and edges, respectively. 
Each road consists of several lanes, which are the basic units to support vehicle movement.
%
% Each road consists of several lanes, which are the basic units to support vehicle movement and determine the way each vehicle passes through an intersection, such as turning left, going straight, and turning right.
% An incoming lane is where vehicles enter the intersection, and an outgoing lane is where vehicles leave the intersection.
The set of incoming lanes and outgoing lanes of intersection $i$ is denoted by $\mathcal{L}_i^{in}$ and $\mathcal{L}_i^{out}$, respectively.

\paragraph{Traffic movement} Each traffic movement is defined as vehicles traveling across an intersection towards a certain direction, i.e., left, straight, and right.

\paragraph{Signal phase} 
Each signal phase, denoted by $p$, is a set of permitted traffic movements, and $\mathcal{P}$ denotes the set of all the phases at each intersection.
%
We use $\mathcal{L}_{p}^{in}$ to denote the participating incoming lanes of phase $p$.
As shown in Figure~\ref{fig:inter} (a), there are four signal phases and each phase has different $\mathcal{L}_{p}^{in}$.

\paragraph{State representation} 
The state representation of each intersection is lane-based, such as the number of vehicles and queue length on each incoming lane.
We use $x_i(l), l\in\mathcal{L}^{in}, i\in\mathbb{N}^{+}$ to represent the $i$-th characters on lane $l$.

\begin{figure}[htb]
    \centering
    \includegraphics[width=1\linewidth]{fig/inter.png}
    \caption{Illustration of a traffic network and an intersection. In the intersection, arrows represent traffic movements, and they can be organized into phases. Phase \#D is actuated for this intersection.}
    \label{fig:inter}
\end{figure}

\subsection{Offline RL}


We consider learning in a Markov decision process (MDP) described by the tuple $(S, A, P, R, \gamma)$. The MDP tuple consists of states $s \in S$, actions $a \in A$, transition dynamics $P(s|s, a)$, a reward function $r = R(s, a)$, and a discount factor $\gamma$. 
%
We use $s_t$, $a_t$, and $r_t= R(s_t, a_t)$ to denote the state, action, and reward at timestep $t$, respectively.
%
The goal of reinforcement learning is to learn a policy that maximizes the expected return $\mathbb{E}\left[\sum_{t=1}^T r_t\right]$ in an MDP.

%
Offline RL involes learning a policy $\pi$ from a fixed dataset consisting of single-step transitions $\mathcal{D}={(s_i^t, a_i^t, s_i^{t+1}, r_i^t)}$ without interacting with the environment.
% In offline reinforcement learning, instead of obtaining data via environment interactions, we only have access to some fixed limited dataset consisting of MDP tuples from the environment. 
%
This setting removes the ability for agents to explore the environment and collect additional feedback, making it more challenging.
% This setting is harder as it removes the ability for agents to explore the environment and collect additional feedback.


\subsection{Problem Definition} 

We consider multi-intersection TSC, where each intersection is controlled by an RL agent.
%
At timestep $t$, agent $i$ views the environment as its observation $o_i^t$, takes action $a_i^t$ to control the signal phase of intersection $i$, and then obtains reward $r_i^t$.

Each agent can learn a control policy from previously collected MDP tuple $\mathcal{D}$.
%
The goal of all the agents is to learn an optimal policy to maximize their expected reward over the historical trajectories, denoted as:
\begin{equation}
\max _{\pi^*} \mathbb{E}_{(s \sim \mathcal{D},a=\pi^*(s))}[R(s, a)]
\end{equation}
where $R$ is the reward function.

This optimal policy obtained through offline RL will be implemented in the multi-intersection scenarios in a decentralized manner, where a single agent is responsible for controlling all the intersections.
%
To evaluate the effectiveness of the proposed approach, will compare the average travel time in each scenario against the baseline methods.
% We will obtain the average travel time for each scenario, which will be compared to the baseline approach to evaluate the performance of the model.




\section{Method}
We present this section from three aspects. 
First, we introduced the collection and organization of offline datasets, which are closely related to real-world scenarios.
%
Second, we introduced our proposed offline RL model called DataLight, which suggests that 'data might be enough for the traffic light'. 
%
Finally, we present a method called Arbitrary To Cyclical (ATC), which can transform most RL-based methods from arbitrary phase sequences to cyclical phase structures.
% Finally, we discussed how to transform most RL-based methods into cyclical signal control methods. 
These proposals aim to tackle the urgent challenges that arise in real-world traffic applications.

\subsection{Cyclical Offline Datasets}

Intersections with cyclical phase structures, such as Fixed-Time~\cite{fixedtime}, are widespread in the real world, making data collection from such intersections both convenient and efficient.
%
To capture actual traffic conditions, we have collected the memory of FixedTime from CityFlow~\cite{cityflow} and used this to generate  Cyclical Offline Datasets (COD). 
%
We used two widely used real-world traffic datasets, JiNan and HangZhou, to increase the diversity of intersection data.
% The COD is generated using FixedTime and two widely used real-world traffic datasets, JiNan and HangZhou, which are described in more detail in the next section. 
%
To increase the diversity of action selection, we adopted a random action selection every 20 times of action. 
%
Each episode lasted for 3600 seconds, and every intersection produced 240 tuples of $(s_t, a_t,s_{t+1},r_t)$ with the action duration set to 15 seconds. 
%
We conducted 10 episodes for each dataset, yielding a total of 144000 tuples of $(s_t, a_t,s_{t+1},r_t)$, with 28800 from each dataset(any surplus data was discarded). 
%
We will employ offline RL to learn from this static dataset.




\subsection{DataLight}
In this section, we introduce an offline RL model, DataLight, which is learned from COD. 
The name suggests that offline data might be enough for traffic lights. 
We discuss the RL agent, network design, offline learning, and decentralized RL paradigm used in DataLight.

\subsubsection{RL Agent}

Firstly, we describe the RL agent in detail:
\begin{itemize}
	\item \textbf{State.} Current phase, number of vehicles, effective running vehicle number, and number of vehicles under segmented road are used as the state representations.
 The effective running vehicle number~\cite{advanced} is the number of vehicles within the effective range, which is the maximum distance to the intersection that a vehicle can pass through within the action duration. 
 For the number of vehicles under segmented road, we consider a segment of 400 meters from the intersection and split it into four chunks of 100 meters, and zero padding is used if the road is shorter than 400 meters.
	\item \textbf{Action.} Each agent chooses a phase $\hat{p}$ as its action $a_t$ according to the predefined action space $\{A, B, C, D\}$ (Figure~\ref{fig:inter} (b)), indicating the traffic signal should be set to phase $\hat{p}$.
	\item \textbf{Reward.} Negative intersection queue length is used as the reward function, denoted as:
			\begin{equation}
				r = -\sum_{l\in \mathcal{L}_{in}} q(l)
			\end{equation}	
	where $\mathcal{L}_{in}$ is the set of incoming lanes, and $q(l)$ is the queue length of lane $l$.
\end{itemize}



\subsubsection{Nueral Network}

The network of DataLight is inspired by DynamicLight~\cite{dynamiclight} and AttentionLight~\cite{ql}, consisting of four modules: lane feature embedding, phase feature construction, phase correlation, and phase score prediction.
%

\begin{figure}[htb]
    \centering
    \includegraphics[width=1\linewidth]{fig/network_2.png}
    \caption{Network design of DataLight.}
    \label{fig:network}
\end{figure}

The network design of DataLight is illustrated in Figure~\ref{fig:network} and described as follows:
\begin{itemize}
\item \textbf{Lane feature embedding.}
The characteristics belonging to one lane are first concatenated as $m$-dimensional and then embedded into a $d_1$-dimensional latent space via a layer of multi-layer perceptron (MLP):
\begin{equation}
\begin{split}
    x(l) &= \operatorname{Concatenate}(x_1(l), x_2(l),..., x_m(l)) \\
    h_1(l) &= \operatorname{Embed}(x(l)) = \sigma(x(l)W_e+b_e)
\end{split}
\end{equation}
where $x_i(l), i=1,2,...,m, l\in\mathcal{L}^{in}$ is one of the characteristics of lane $l$, $m$ is the total number of characteristics, $W_e\in\mathbb{R}^{m\times d_1}$ and $b_e\in\mathbb{R}^{d_1}$ are weight matrix and bias vector to learn, $\sigma$ is the sigmoid function. In this paper, $d_1=32$.

\item \textbf{Phase feature construction.}
The feature of each phase is constructed through feature fusion of the participating incoming lanes:
\begin{equation}
    h_2(p) = \operatorname{Mean}(\operatorname{Fusion}(h_1(l), ..., h_1(k))
\end{equation}
where $p$ is one phase, $l,...,k\in \mathcal{L}_p$, and $\mathcal{L}_p$ is the set of participating entering lanes of phase $p$.
%
We adopt multi-head self-attention as the feature fusion approach (inspired by DynamicLight~\cite{dynamiclight}) and the head number selects four in our implementation.


\item \textbf{Phase correlation.}
The phase correlation is learned using multi-head self-attention (MHA) with the phase features as the input:
%Our model takes the phase feature as input and uses multi-head self-attention (MHA) to learn the phase correlation:
\begin{equation}
    h_3 = \operatorname{MHA}(h_2(p_1),...,h_2(p_4))
\end{equation}
where $p_i\in\mathcal{P}, i=1,2,3,4$, and $\mathcal{P}$ is the set of phases.

\item \textbf{Phase score prediction.}
The correlated phase features are embedded into $1$-dimensional latent space via an MLP to get the phase scores:
\begin{equation}
    h_4 = \operatorname{Embed}(h_3) = f_3W_c+b_c
\end{equation}
where $W_c\in\mathbb{R}^{d_1\times 1}$ and $b_c\in\mathbb{R}^{1}$ are weight matrix and bias vector to learn. 
%
Finally, we determine the phase that has the maximum score, and the phase will be set to that phase.
%
For example, if the phase is $1$ with the action space as $\{A, B, C, D\}$, then the phase will be set to $B$.
\end{itemize}

\subsubsection{Offline Learning}

CQL~\cite{CQL} is used to update DataLight:
\begin{equation}
\begin{split}
\min _Q \alpha \mathbb{E}_{\mathbf{s} \sim \mathcal{D}}&\left[\log \sum_{\mathbf{a}} \exp (Q(\mathbf{s}, \mathbf{a}))  -\mathbb{E}_{\mathbf{a} \sim \hat{\pi}_\beta(\mathbf{a} \mid \mathbf{s})}[Q(\mathbf{s}, \mathbf{a})]\right] \\
&+\frac{1}{2} \mathbb{E}_{\mathbf{s}, \mathbf{a}, \mathbf{s}^{\prime} \sim \mathcal{D}}\left[\left(Q-\hat{\mathcal{B}}^{\pi_k} \hat{Q}^k\right)^2\right]
\end{split}
\end{equation}
where $\alpha$ is a weight and is adopted as 0.0005, $\mathcal{D}$ is the COD, $\hat{\mathcal{B}}^{\pi_k} $ is the bellman operator, $\hat{Q}^k$ is the Q value obtained by iteration, $\pi_k$ is the learned policy, and $\hat{\pi}_\beta(\mathbf{a} \mid \mathbf{s})$ is the behavior policy.

\subsubsection{Decentralized RL Paradigm}
We employ a decentralized RL paradigm to ensure scalability, which has been proven to significantly improve model performance and is used by many methods such as MPLight~\cite{mplight} and Advanced-XLight~\cite{advanced}.
We combine data collected from different intersections and scenarios to train one model. 
This means that we will use a single agent to control all intersections, and the offline experiences are shared. 
This model, trained offline, will be deployed in various intersections and scenarios, and we will use simulation results to evaluate its performance.


\subsection{Cyclical Signal Control}
We propose a novel approach for implementing cyclical signal control based on pre-trained RL models, which can learn whether to maintain or change the current traffic light phase.
%
This pathway enables us to determine whether to maintain the current phase or switch to the next phase with a cyclical phase structure. 
Our method, named Arbitrary To Cyclical (ATC), can change most RL-based methods from arbitrary phase sequences to cyclical phase structures, as summarized in Algorithm 1. 
%
However, it is important to note that the RL model still needs to be trained properly to determine when to switch to the next phase. 
Different RL models will yield vastly different results when applied to cyclical signal control.

% We propose a pathway that enables cyclical signal control based on pre-trained RL models.
% %
% The pre-trained RL models can learn whether to maintain or change the current phase, and we can use it to determine whether to maintain the current phase or change to the next phase with a cyclical phase structure.
% %
% Based on this property, these models can also work properly if the phase changes in fixed cyclical order.
% %
% Our method is named Arbitrary To Cyclical (ATC), indicating can change most RL-based methods from arbitrary phase sequences to cyclical phase structures.
% This method is summarized in Algorithm~\ref{alg:cycle}.

\begin{algorithm}[htb]
    \caption{Arbitarty To Cyclical}
   \label{alg:cycle}
	\textbf{Parameters}: Intersection number $n$; current phase at intersection $i$: $p_i^{pre}$; determined next phase at intersection $i$: $\hat{p}_i$; fixed phase cycle order: \{A, B, C, D, A, B,...\}; action duration: $t_{action}$ % duration of phase $\hat{p}_i$: $\hat{t}^i_{phase}$; current phase time at intersection $i$: $t^i$.
	\begin{algorithmic}[1]
	\STATE Pre-train the ordinary DRL method (such as CoLight);
	\STATE Initialize $p_i^{pre}$, and $t^i$;
	\FOR{(time step)}	
		\FOR{i=1:n}
			\STATE $t^i=t^i+1$;
			\IF{$t^i=\hat{t}^i_{phase}$}
			\STATE Determine the  phase $\hat{p}_i$ for intersection $i$ with the DRL method;
			\IF{$\hat{p}_i$ == $p_i^{pre}$}	
				\STATE Continue;
			\ELSE	
				\STATE Select the phase from the sequence for $p_i^{pre}$; 
			\ENDIF
			\STATE $t^i=0$;
			\ENDIF
		\ENDFOR
	\ENDFOR
\end{algorithmic}
\end{algorithm}


% The RL model still needs to be trained properly to better determine when to switch to the next phase. 
% Different RL models will yield vastly different results when changed to cyclical signal control. 
% In the next section, we will demonstrate the feasibility of our proposed method through experiments. 
Previous research has mainly focused on the control performance of RL models without considering the phase sequence requirements. 
However, the arbitrary order of phases can have an impact on its real-world application. 
%
Our proposed method addresses this issue by converting any order of phases into a cyclical order of phases, thus solving a major challenge faced by RL-based TSC methods in practical use. 
In the next section, we will demonstrate the feasibility of our proposed method through experiments.

%
% Many recent methods have focused on the control effectiveness of the model without considering the phase sequence requirements.
% However, the arbitrary order of phases can have an impact on its real-world application. 
% Our proposed method can convert any order of phases into a cyclical order of phases, solving a major challenge faced by RL-based TSC methods in practical use.

It should be emphasized that the proposed method can effectively bridge the gap between simulation experiments and real-world applications. 
By pre-training the RL model and converting the order of phases into a cyclical structure, the model can better reflect the actual traffic conditions and work properly in the real world. 
Our approach is a significant contribution to the field of TSC as it enables RL-based methods to be practically applied in the real world with greater effectiveness and efficiency.



\subsection{Discussion}
\subsubsection{Why collect data exclusively from intersections with cyclical phase structure?}
Our objective is to establish a bridge between experiments and real-world applications.
%
Therefore, when collecting offline data, it is crucial to consider the feasibility of collecting data in the real world, despite the abundance of data obtained through simulation experiments.
%
Obtaining data through RL interaction with the real world is challenging, while many real-world intersections possess cyclical phase structures, and collecting data from such intersections is safe and convenient. 
% On the one hand, collecting data through interaction with the real world using reinforcement learning (RL) in the real world is difficult. 
%
% On the other hand, many real-world intersections possess cyclical phase structures, and collecting data from such intersections is safe and convenient. 
%
Consequently, we opt to collect data only from intersections with a cyclical phase structure, 
facilitating a stronger link between simulation and reality.

% Obtaining data through interaction with the real world using reinforcement learning (RL) is challenging due to the difficulty of acquiring suitable datasets, while realistic datasets for offline analysis are relatively easier to obtain.
% %
% While offline data obtained through interaction with simulators can be useful, models learned on such data are not necessarily meaningful when considering real-world issues. Our aim is to identify a reliable model that can be trained on easily obtained real-world data.

\subsubsection{Can experiments on a simulator validate feasibility in reality?}
Our aim is to establish a framework that effectively connects simulated experiments and real-world applications, rather than directly applying the model obtained from simulated experiments to the real world. 
%
While simulators cannot completely replicate the constantly changing conditions in the real world, the patterns learned from simulated experiments can still be applied to the real world. 
%
Through simulated experiments, we can verify the reliability of these learned patterns. 
%
Subsequently, we can collect real-world data to train a model that can be effectively implemented in reality.
% Once the learned patterns are validated as reliable through simulated experiments, we can collect data from the real world to train a model that can be applied in reality.

% Our aim is to identify a pattern that can bridge the gap between simulated experiments and real-world applications, rather than directly applying models from simulated experiments to the real world, due to the unreliability of simulators. While simulators cannot accurately capture the dynamics of the real world, the learned patterns from simulators are transferable to the real world with some reliability.

% \paragraph{How to connect to the real world?}
% If we can demonstrate that, in simulated experiments, models learned from cyclical-offline-datasets (COD) can approach the optimal performance, then we can also achieve a highly effective model on COD data collected from the real world, which can be applied to real-world problems.
% %
% Meanwhile, the incorporation of cyclical signal control gives our model a significant advantage when used in real-world applications. Although collecting COD data from the real world is relatively easy, there are numerous details and challenges that require careful consideration and attention to overcome. We will focus on addressing these issues in our future work.

\section{Experiments}



\subsection{Settings}


\begin{table*}[htb]
    \centering
    \caption{Overall performance. For average travel time, the smaller the better.}
    \label{tab:overall}
    \begin{tabular}{lccccccc}
    \toprule
    \multirow{2}{*}{ Method } & \multicolumn{3}{c}{ JiNan } & \multicolumn{2}{c}{ HangZhou } & \multicolumn{2}{c}{New York} \\
    \cmidrule { 2 - 8 } & 1 & 2 & 3 & 1 & 2 & 1 & 2\\
    \midrule
    FixedTime & $429.27$ & $370.34$ & $384.89$ & $497.87$ & $408.31$ & $1507.12$ & $1733.30$ \\
    Efficient-MP & $269.87$ & $239.75$ & $240.03$ & $284.44$ & $327.62$ & $1122.00$ & $1462.96$ \\
    M-QL & $268.87$ & $240.02$ & $238.51$ & $284.32$ & $325.44$ & $1197.59$ & $1551.46$ \\
    \midrule
    FRAP & $299.06$ & $266.19$ & $273.58$ & $321.88$ & $353.89$ & $1192.23$ & $1470.51$ \\
    PRGLight & $ 291.27$& $ 257.52$& $ 261.74$& $ 301.06$& $ 369.98$& $ 1283.37$& $ 1472.73$\\
    CoLight & $269.47$ & $252.35$ & $249.05$ & $297.64$ & $337.25$ & $1065.64$ & $1367.54$ \\
    AttentionLight & $256.36$ & $240.10$ & $237.00$ & $284.85$ & $313.89$ & $1013.78$ & $1401.32$ \\
    Efficient-MPLight & $266.55$& $241.64$ &$241.78$& $285.67$& $328.59$& $1301.83$& $1485.50$ \\
    Advanced-CoLight & $247.00$& $233.68$& $229.61$& $271.62$& $311.07$& $970.05$& $1300.62$ \\
    DynamicLight& $242.97$& $229.01$& $225.51$& $272.14$& $310.46$& $1121.47$& $1484.64$ \\
    \midrule 
    DataLight & $248.47$& $230.96$& $227.67$& $270.19$& $294.29$& $1006.47$& $1333.17$\\
%    DataLight-ATC & $310.69$& $279.87$& $285.73$& $304.74$& $331.65$& $1056.55$& $1405.99$\\
    \bottomrule
    \end{tabular}
\end{table*}


% simulator
We conduct numerical experiments using CityFlow~\cite{cityflow}, a large-scale TSC simulator with faster simulation speeds than SUMO.
% Numerical experiments are conducted under CityFlow~\cite{cityflow}, which supports large-scale  TSC and has a faster simulation speed than SUMO~\cite{sumo}.
The action duration is 15 seconds, and a five-second red time is followed to prepare for the phase transition.

% datasets
To evaluate the performance of our model, we utilize seven real-world traffic datasets~\footnote{https://traffic-signal-control.github.io} obtained from JiNan, HangZhou, and New York, which have been widely used by various methods such as CoLight~\cite{colight}, HiLight~\cite{hilight}, and Advanced-XLight~\cite{advanced}.
% These datasets have been wildly used by various methods such as CoLight~\cite{colight}, HiLight~\cite{hilight}, and Advanced-XLight~\cite{advanced}.
% The topologies of the traffic networks under JiNan, HangZhou, and New York are different from each other. 
%
These datasets have varying topologies, 
with 12 intersections in JiNan (3 × 4) and each intersection connecting two 400-meter road segments (East-West) and two 800-meter road segments (South-North); 
16 intersections in HangZhou (3 × 4) and each intersection connecting two 800-meter road segments (East-West) and two 600-meter road segments (South-North); 
and 192 intersections in New York (28 × 7) and each intersection connecting four 300-meter road segments (two East-West and two South-North). 
% In JiNan, the road network has 12 intersections ($3\times4$) with each intersection connecting two 400-meter road segments (East-West) and two 800-meter road segments (South-North); in HangZhou, the road network has 16 intersections ($3\times4$) with each intersection connects two 800-meter road segments (East-West) and two 600-meter road segments (South-North); in New York, the road network has 192 intersections ($28\times 7$) with each intersection connects four 300-meter (two East-West and two South-North) road segments.
%
Additionally, the seven traffic flow datasets are not only different in terms of the arrival rate but also vary in travel patterns,  providing diversity and validity to our experiment.
% These datasets support the diversity and validity of our experiments.

\subsection{Compared Methods}
% metric
Average travel time is used to evaluate the model performance of different models for traffic signal control, which is the most frequently used metric~\cite{survey}.
%
Each testing episode is a 60-minute simulation, and we adopt one result as the average of the last five testing episodes.
%
Each reported result is the average of three independent experiments.
% baselines
The following baseline methods are used for comparison.

\subsubsection{Traditional Methods}
\begin{itemize}
	\item \textbf{FixedTime}~\cite{fixedtime}: a policy uses fixed cycle length with a predefined phase split among all the phases.
	\item \textbf{Max-QueueLength}~\cite{ql}: a policy selects the phase that has the maximum queue length.
	\item \textbf{Efficient Max-Pressure}~\cite{efficient}: a policy selects the phase that has the maximum efficient pressure.
\end{itemize}

\subsubsection{RL-based Methods}
\begin{itemize}
	\item \textbf{FRAP}~\cite{frap}: using a specially designed network to model phase features and capture phase competition relation.
	\item \textbf{CoLight}~\cite{colight}: using graph attention network~\cite{gats} to model intersection cooperation.
	\item \textbf{AttentionLight}~\cite{ql}: using self-attention~\cite{attention} to construct phase feature and introduce queue length as an effective state representation.
	\item \textbf{PRGLight}~\cite{prglight}: using graph neural network to predict traffic state and determines phase and phase duration according to the currently observed state and predicted state.
	\item \textbf{Efficient-MPLight}~\cite{efficient}: using FRAP as the base model and introducing efficient pressure as an efficient state representation.
	\item \textbf{Advanced-CoLight}~\cite{advanced}: using CoLight as the base model, introduce efficient pressure and effective running vehicle number as the state representations.
	\item \textbf{DynamicLight}~\cite{dynamiclight}: using a two-stage framework to realize dynamic phase duration. It is a state-of-the-art model. 
\end{itemize}


\subsection{Overall Performance}


Table~\ref{tab:overall} summarizes the overall performance across multiple real-world datasets in terms of average travel time.
%
DataLight outperforms most existing methods and achieves comparable results with the best-performing methods, demonstrating the effectiveness of our model in learning effective control strategies from COD.
% The fact that DataLight can learn such effective control strategies from COD data demonstrates the effectiveness of our framework. 
%
Furthermore, the performance of DataLight highlights the unique characteristics of the TSC environment, where satisfactory results can be learned from suboptimal policy experiences, unlike many robots and Atari environments~\cite{CQL}. 
%
The performance of DataLight suggests that it is feasible to train reliable models from cyclical data collected in the real world, thus bridging the gap between simulation experiments and real-world applications.


\subsection{ATC Validation}

\begin{figure}[htb]
    \centering
    \includegraphics[width=1\linewidth]{fig/cycle_3.png}
    \caption{Cyclical signal control.}
    \label{fig:cycle}
\end{figure}

Our proposed method, Arbitrary To Cyclical (ATC), can enable most RL-based methods to realize cyclical signal control.
% Most RL-based methods can realize cyclical signal control with our proposed methods.
%
We test this approach on several RL-based methods, including FRAP~\cite{frap}, CoLight~\cite{colight}, AttentionLight~\cite{ql}, Advanced-CoLight~\cite{advanced}, and DataLight, by converting them into a cyclical phase structure and comparing them with FixedTime~\cite{fixedtime}.
% We convert them into a cyclical phase structure and compare them with FixedTime.
% 
Figure~\ref{fig:cycle} illustrates the performance comparison in terms of average travel time.
%
We observed that different RL methods can lead to different performance outcomes. 
%
Overall, these methods show significant improvement over FT, highlighting the benefits of RL.
%
Moreover, DataLight with ATC outperforms most  methods and achieves results comparable to the best-performing methods, thus demonstrating its reliability in real-world scenarios.


\subsection{COD Validation}


The performance of offline RL agents can be greatly affected by the quality of the offline datasets.
% Offline datasets can significantly influence the performance of offline RL agents.
%
Typically, agents trained on expert datasets tend to outperform those trained on random datasets.
% The performance of agents is usually significantly better under expert datasets than under random datasets.
%
To evaluate the impact of offline datasets on the performance of DataLight, we conducted experiments using \textbf{Expert} and \textbf{Random} offline datasets. 
These datasets are described as follows:
% Agents usually can get significantly better performance under Expert datasets than Random datasets.
% %
% We add Expert and Random offline datasets into experiments. 
% %
% These Datasets are described as follows:
\begin{itemize}
    \item \textbf{Expert}. Collected from the replay memory of Advanced-CoLight~\cite{advanced}.
    \item \textbf{Random}. Collected from the memory of a random agent that chooses an action randomly every time.
    \item \textbf{Cycle}. Collected from the memory of FixedTime. It is essentially the same as COD.
\end{itemize}
%

\begin{figure}[htb]
    \centering
    \includegraphics[width=1\linewidth]{fig/differdata_1.png}
    \caption{The performance of DataLight under different offline datasets.}
    \label{fig:differdata}
\end{figure}

Figure~\ref{fig:differdata} demonstrates the performance of DataLight under three different offline datasets.
% 
DataLight shows similar performances under \textbf{Expert}, \textbf{Random}, and \textbf{Cycle}, which is distinctive from other RL scenarios~\cite{CQL}.
%  Figure~\ref{fig:differdata} not only reveals the distinctiveness of TSC but also indicates the reliability of our offline dataset.
%
This result not only highlights the uniqueness of TSC but also indicates the reliability of our offline dataset.

%


\subsection{Low Data Scenarios}

The performance of offline RL agents is heavily dependent on the amount of training data used. 
%
To evaluate the impact of low data on our framework, we train DataLight with different amounts of COD and analyze its performance under multiple real-world datasets, as shown in Figure 5.

% With the different amounts of samples, offline RL usually has different learning effects.
% We train DataLight with different amounts of data to evaluate the influence of low data.
% Figure~\ref{fig:percent} demonstrates the performance under multiple real-world datasets.

\begin{figure}[htb]
    \centering
    \includegraphics[width=1\linewidth]{fig/percent_2.png}
    \caption{The performance of DataLight under different amount of COD.}
    \label{fig:percent}
\end{figure}

As expected, the performance of the learned model decreases as the amount of data used for training decreases.
Notably, a significant drop in performance is observed only when the data is reduced to 1\%. However, when the data is reduced to 50\% and 10\%, the decrease in model performance is not substantial. 
It is worth noting that our framework achieves reliable results with only 144000 tuples, which is relatively small compared to other offline RL studies. 
These findings demonstrate the effectiveness and reliability of our learned model, which can generalize well with a limited amount of data.


% As the amount of data used for training decreases, the performance of the learned model also decreases, with a significant drop in performance observed only when the data is reduced to 1\%. 
% %
% However, overall, the decrease in model performance is not substantial when the data is reduced to 50\% and 10\%. 
% %
% Moreover, our framework only utilizes 144,000 data points, which is relatively small compared to the amount of data used in other offline RL studies. 
% %
% These results not only demonstrate the reliability of our framework but also highlight the effectiveness of our learned model.



\section{Conclusion}


In this article, we address two main challenges of RL-based TSC in practical applications: (1) online RL training is not feasible for real-world usage, and (2) most intersections require a cyclical phase structure. 
%
To tackle these challenges, we propose (1) COD, a dataset that can be easily collected from the real world, and DataLight,  an  offline RL model capable of learning traffic light control policies from COD, and (2) Arbitrary To Cyclical (ATC), a method that can convert most RL-based methods into cyclical signal control. 
%
Extensive experiments are conducted to verify our proposed methods. 
The results demonstrate that DataLight can learn control policies close to optimal from COD data, and some RL-based methods can achieve significantly better performance than FixedTime when converted into a cyclical signal control approach with satisfactory control performance. 
Our proposed methods effectively address the two challenges and reduce the gap between experimental results and practical applications.

In future research, we aim to collect real-world traffic data, develop robust RL models, and deploy them in practical scenarios to establish a strong connection between academic research and real-world applications.



\section{Acknowledgments}
% This work is supported by Lanzhou University.

\bibliography{references}



\end{document}
