% \documentclass[lettersize,journal]{IEEEtran}
\documentclass[10pt,journal,compsoc]{IEEEtran}


\usepackage{amsmath,amsfonts}
\usepackage{array}
\usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
\usepackage{textcomp}
\usepackage{stfloats}
\usepackage{url}
\usepackage{verbatim}
\usepackage{graphicx}
\usepackage{cite}
\hyphenation{op-tical net-works semi-conduc-tor IEEE-Xplore}
% updated with editorial comments 8/9/2021

\usepackage{times}
\usepackage{epsfig}
\usepackage{xcolor}
\usepackage{amssymb}
\usepackage{multirow}
\usepackage{makecell}
\usepackage{enumitem}
\usepackage{pifont}
\usepackage{booktabs}
\usepackage{hyperref}

\newcommand\best[1]{\textbf{\textcolor{red}{#1}}}
\newcommand\subbest[1]{\textcolor{blue}{#1}}


\begin{document}

\title{MoWE: Mixture of Weather Experts for Multiple Adverse Weather Removal}

\author{\IEEEauthorblockN{Yulin Luo, Rui Zhao, Xiaobao Wei, Jinwei Chen, Yijie Lu, Shenghao Xie, Tianyu Wang,\\Ruiqin Xiong, Ming Lu, and Shanghang Zhang}
\thanks{Y. Luo is with Shanghai Jiaotong University. Shanghai, China. E-mail: lyl010221@sjtu.edu.cn}
\thanks{R. Zhao and J. Chen are with Peking University, Beijing, China. E-mail: ruizhao@stu.pku.edu.cn, cjw@pku.edu.cn.
}
\thanks{X. Wei is with Beihang University. E-mail: weixiaobao0210@gmail.com}
\thanks{Y. Lu and T. Wang are with Peking University, Beijing, China. E-mail: 2000012967@stu.pku.edu.cn, tianyuw2001@gmail.com}
\thanks{S. Xie is with Wuhan University. Wuhan, China. E-mail: xieshenghao@whu.edu.cn.}
\thanks{R. Xiong and S. Zhang are with Peking University. Beijing, China. E-mail: \{rqxiong, shanghang\}@pku.edu.cn}
\thanks{M. Lu is with Intel Labs China. Beijing, China. E-mail: lu199192@gmail.com}
}

\IEEEtitleabstractindextext{
\begin{abstract}
    Currently, most adverse weather removal tasks are handled independently, such as deraining, desnowing, and dehazing. However, in autonomous driving scenarios, the type, intensity, and mixing degree of the weather are unknown, so the separated task setting cannot deal with these complex conditions well. Besides, the vision applications in autonomous driving often aim at high-level tasks, but existing weather removal methods neglect the connection between performance on perceptual tasks and signal fidelity. To this end, in upstream task, we propose a novel \textbf{Mixture of Weather Experts(MoWE)} Transformer framework to handle complex weather removal in a perception-aware fashion. We design a \textbf{Weather-aware Router} to make the experts targeted more relevant to weather types while without the need for weather type labels during inference. To handle diverse weather conditions, we propose \textbf{Multi-scale Experts} to fuse information among neighbor tokens. In downstream task, we propose a \textbf{Label-free Perception-aware Metric} to measure whether the outputs of image processing models are suitable for high level perception tasks without the demand for semantic labels. We collect a syntactic dataset \textbf{MAW-Sim} towards autonomous driving scenarios to benchmark the multiple weather removal performance of existing methods. Our MoWE achieves SOTA performance in upstream task on the proposed dataset and two public datasets, i.e. All-Weather and Rain/Fog-Cityscapes, and also have better perceptual results in downstream segmentation task compared to other methods. Our codes and datasets will be released after acceptance.
   % 补充文章中展示的例子和突出的优势
\end{abstract}}

\maketitle

\IEEEdisplaynontitleabstractindextext

\IEEEpeerreviewmaketitle


%%%%%%%%% BODY TEXT
\section{Introduction}
In autonomous driving visual perception tasks, adverse weather could have a severe impact on the imaging quality of traditional cameras, leading to the degradation of algorithm performance. Thus, the adverse weather removal task is significant.
%%% Modified by Rzhao %%%

In autonomous driving scenarios, existing methods have limitations in two aspects. One is that the modeling of weather conditions is relatively simple. In the real world, the categories of weather are unpredictable, and different types of adverse weather often co-occur.

Currently, weather removal methods can be roughly divided into three types: task-specific methods, task-agnostic methods, and multi-task-in-one methods. The \textbf{task-specific} methods are designed for a specific kind of weather, such as rain~\cite{jiang2020multi, li2018recurrent, ren2019progressive}, haze~\cite{qu2019enhanced, liu2019griddehazenet, dong2020multi, qin2020ffa, wu2021contrastive}, and snow~\cite{liu2018desnownet, chen2020jstasr, chen2021all}. These methods are difficult to perform well on other tasks due to their weather-related inductive bias. \textbf{Task-agnostic} methods have a unified scheme for different tasks but need to be trained separately~\cite{zamir2021multi,chen2021hinet, liang2021swinir, zamir2022restormer}. We need to select specific parameters according to the weather type. Thus, the application of task-agnostic methods is also limited. \textbf{Multi-task-in-one} method can remove different types of weather using a single set of parameters~\cite{chen2021ipt, valanarasu2022transweather, chen2022unified}, but there are still some aspects that can be improved. The schemes of perceiving weather types in existing methods are complex and inflexible, and hybrid weather conditions are not considered. 

\begin{figure}[!th]
    \centering
\includegraphics[width=\linewidth]{./Figure/Method/Our_Methods_intro.pdf}
    \caption{Introduction to our research topic of Mixture of
    Weather Experts(MoWE).}
    \label{fig:method_intro}
\end{figure}

The other limitation is lacking consideration of downstream tasks. The performance of autonomous driving vision tasks is often embodied by downstream tasks, such as segmentation and detection. However, existing weather removal methods mainly focus on metrics for the signal fidelity of the restored images, and images' perceptual quality is not completely consistent with signal fidelity~\cite{zhang2021just}. Overall, the research topics can be summarized in fig.\ref{fig:method_intro}.

Based on the thinking of the above-mentioned limitations, we propose a novel \textbf{Mixture-of-Weather-Experts(MoWE)} Transformer framework to realize flexible and perception-aware weather removal in autonomous driving scenarios. In MoWE, a vision Transformer\cite{dosovitskiy2020vit, chen2021ipt} with task-shared convolution head and tail is adopted as a baseline. The mixture of experts (MoE)\cite{ma2018mmoe} is designed to assign different tasks to different experts, making the computation of neural networks more efficient and targeted. 

%%% 不是很确定 %%%
For modeling complex weather conditions, we propose a Weather-aware Router based on additional token-level weather information from a parallel light-weight weather classifier, and the network does not need weather-type labels. Based on this, the taxonomy standard of the router is relevant to the weather types rather than other classification standards, such as texture richness and brightness. To improve the performance of the model on different types and intensities of adverse weather, we propose Multi-scale Experts to enhance the spatial modeling capability. The existing feed-forward network (FFN) in MoE is mainly pixel-wise. We propose to fuse information in different tokens for spatial modeling in larger scopes.

For perception-aware weather removal task, we propose a Label-free Percepual Metric to predict the performance on downstream tasks without ground truth of the downstream task, which is more practical to be applied in real world.
%%% Modified by Rzhao %%%

The primary contributions are summarized as follows:

\textbf{(1)} We propose Mixture of Weather Experts (MoWE) to remove hybrid and perception-aware adverse weather removal in autonomous driving scenarios, requiring no prior information on weather type in inference stage.

\textbf{(2)} In MoWE, We propose a weather-aware router to ensure the tokens' assigning is more relevant to weather types, which enhances the model's capability to handle complex and diverse weather conditions. And we also propose multi-scale experts to make better use of locality information to removal weather with different intensity.

\textbf{(3)} We propose a novel label-free perception-aware metric to measure whether the outputs of image processing models are suitable for high level perception tasks without the demand for semantic labels.

\textbf{(4)} We propose a new syntactic dataset towards autonomous driving scenarios to benchmark the capabilities to remove various kinds of weather condition, including rain, haze, snow and mixture weather type.

\textbf{(5)} Experimental results show that MoWE can achieve state-of-the-art performance in multiple weather removal task on the proposed dataset MAWSim by PSNR +1.08  and SSIM +0.0142 , the public dataset Allweather by PSNR +0.8 and SSIM +0.0255, RainCityscapes by PSNR +0.31 and FogCityscapes by PSNR +1.58. MoWE can also consistently improve the downstream task performance compared to other methods.


%------------------------------------------------------------------------
\section{Related Work}

\textbf{Adverse Weather Removal.}
Adverse weather removal has been investigated in many aspects. From the perspective of universality, the related works can be divided into task-specific and task-agnostic methods. The adverse weather mainly includes rain, snow, and haze. For task-specific methods, one network aims to deal with certain weather. Li et al.~\cite{li2018recurrent}, Ren et al.~\cite{ren2019progressive}, and Jiang et al.~\cite{jiang2020multi} remove the rain based on progressively refining the image. Qu et al.~\cite{qu2019enhanced} predict images without haze based on a generative network. Dong et al.~\cite{dong2020multi} remove the haze based on boosting and error feedback. GridDehazeNet~\cite{liu2019griddehazenet} and FFA-Net~\cite{qin2020ffa} use the attention operations for dehazing. Wu et al.~\cite{wu2021contrastive} introduce contrastive learning to dehaze. DeSnowNet~\cite{liu2018desnownet}, JSTASR~\cite{chen2020jstasr}, and Chen et al.~\cite{chen2021all} aim to remove snow with different status adaptively.

In contrast to task-specific methods, some methods can be adopted for different weathers, i.e., the task-agnostic methods. Some of these methods can deal with only one type of degradation. These methods need to be trained for each task separately. MPRNet~\cite{zamir2021multi}, HINet~\cite{chen2021hinet}, SwinIR~\cite{liang2021swinir}, and Restormer~\cite{zamir2022restormer} are architectures for general image restoration. Most of these architectures implement deraining as one of the tasks in the experiments. Some methods can remove multiple adverse weathers at once. All-in-One~\cite{li2020all} uses neural architecture search (NAS) to discriminate between different tasks.
Several strategies are proposed to handle multiple adverse weathers simultaneously. TransWeather~\cite{valanarasu2022transweather} uses learnable weather-type embeddings in the decoder. Chen et al.~\cite{chen2022learning} use a two-stage knowledge learning mechanism for comprehensive bad weather. BID~\cite{han2022blind} aims to decompose the degraded images into constituent underlying images and other components.
%-------------------------------------------------------------------------------------------------
\begin{figure*}[!ht]
    \centering
\includegraphics[width=0.85\linewidth]{./Figure/Method/Our_Methods.pdf}
    \caption{The overall framework of the proposed Mixture of
    Weather Experts(MoWE).}
    \label{fig:method}
\end{figure*}

\textbf{Transformer in Image Restoration Task.}
Transformers~\cite{vaswani2017transformer} have been increasingly applied in the vision area since ViT~\cite{dosovitskiy2020vit} employ Transformers to visual recognition task~\cite{han2022survey}. IPT~\cite{chen2021ipt} introduces Transformers pretrained on a large dataset for image restoration tasks.
SwinIR~\cite{liang2021swinir} introduces the Transformers with shifted windows~\cite{liu2021swin} for image restoration. UFormer~\cite{wang2022uformer} and Restormer~\cite{zamir2022restormer} use Transformers to construct pyramidal network structures for image restoration based on locally-enhanced windows and channel-wise self-attention, respectively. ELAN~\cite{zhang2022efficient} and DATSR~\cite{cao2022reference} consider long-range attention based on Transformers for super-resolution. CAT~\cite{chen2022cross} and Xiao et al.~\cite{xiao2022stochastic} propose Transformers with adaptive windows to perform more flexible image restoration.

\textbf{Mixture-of-Experts(MoE).}
MoE is a type of neural network, whose parameters are partitioned into different subsets called "experts"\cite{fedus2022review}. Different parts of inputs will be routed to specific expert by some router mechanisms in training and inference time\cite{fedus2022review}. MoE is applied and popularized first in Natural Language Processing(NLP) in deep learning era. 

In computer vision, MoE has been employed in some high-level task such as image classification\cite{riquelme2021cv-moe-vit, lou2021cv-moe-mlp, liu2022swinT-v2}, object detection\cite{liu2022swinT-v2, wu2022residual-moe} and segmentation\cite{wu2022residual-moe}. However, few works adopt MoE in image processing, although low-level tasks are often joint learning for better generalization\cite{chen2021ipt, valanarasu2022transweather, chen2022unified} and the MoE philosophy is very suitable for multi-task learning due to its branch design\cite{ma2018mmoe, kudugunta2021beyond, gururangan2021demix}. So in this work, to make up the blank, we make the first attempt to apply MoE in multiple adverse weather removal tasks.
%------------------------------------------------------------------------
\section{Proposed Approach -- MoWE}

To solve the proposed upstream task, multiple weather removal without explicit weather labels in test time, the key is to process inputs with underlying different weather conditions dynamically according to the captured weather representation. To achieve this goal, we propose a novel weather-aware multi-scale Mixture-of-Experts module, which includes a light-weight weather feature extraction module, a token-level router based on weather and content embedding, and multiple expert groups specializing in processing information at different scales. To realize perception-aware weather removal results for downstream task, we extend the recognition loss in \cite{liu2022exploring} to semantic perception metric after. We first present the overall pipeline of our MoWE framework(see fig \ref{fig:method}). Then we describe the two key components mentioned above designed for upstream and downstream task respectfully. Finally, we provide the details of a novel metric, which is proposed for downstream task performance estimation of upstream task models(as a preprocessing model) when labels are not accessible.


\subsection{Overall Framework}
Our MoWE has two parallel branches, one for obtaining weather representation efficiently, and the other for image restoration utilizing the task-related features. 

Vision Transformer has demonstrated its capacity in low-level visual tasks recently \cite{chen2021ipt, liang2021swinir, zamir2022restormer, valanarasu2022transweather}. The long-range dependence modeling and adaptive spatial aggregation \cite{liang2021swinir} are primary advantages of Transformer, making it suitable for weather removal tasks because they require much global information according to weather physical model\cite{li2019heavy, li2020all, valanarasu2022transweather}. To this end, We employ vision Transformer \cite{dosovitskiy2020vit, chen2021ipt} as our baseline for both branches.

Given an image $\textbf{I}_{adverse} \in \mathbb{R}^{3\times H \times W}$ with adverse weather, for the weather representation captured branch, we use ViT\cite{dosovitskiy2020vit} to finish supervised classification for weather. In the end of Transformer blocks, Global Average Pooling(GAP) is adopted to form the final image representation, and MLP is employed to predict the weather type accordingly. In this way, token-level weather information is obtained, which is beneficial for the restoration branch in fine-grained handling tokens with different weather.

For the weather removal branch, we first obtain the shallow image embedding $\textbf{x} \in \mathbb{R}^{C \times H \times W}$ by a task-shared convolution head to capture general low level features, where $C$ and $H \times W$ denote the channel and spatial dimension respectfully. After applying patch embedding $\textbf{E}$ to image features $\textbf{x}$, the token sequence $\textbf{z}$ could be formulated as $ \textbf{z} = [\textbf{E}x_1, \textbf{E}x_2, \dots, \textbf{E}x_N] \in \mathbb{R}^{N\times D}$, where $N$ is the number of tokens and $D$ denotes embedding dimension. Then, the tokens $\textbf{z}$ are passed through the Transformer encoder\cite{vaswani2017transformer, dosovitskiy2020vit} with $L$ blocks to model the global relationship by standard self-attention module and branch processing of different weather features by the proposed weather-aware multi-scale MoE module. After that, we employ a linear layer to expand the token dimension of Transformer output $\textbf{Z}^{L}$ and reshape them to the origin resolution $\textbf{x}^{L} \in \mathbb{R}^{C \times H \times W}$, just the same as the input image feature $\textbf{x}$. Finally, a task-shared convolution tail $T$ is utilized to refine the features $\textbf{x}^{L}$ and adjust the channels to obtain the restoration clean image $\textbf{I}_{clean} \in \mathbb{R}^{3\times H \times W}$ by $\textbf{I}_{clean} = T(\textbf{x}^{L})$.


\subsection{Mixture-of-Experts Module in ViT}

Due to the complex and diverse situation of weather type, intensity, and mixing degree in autonomous driving scenario, branch cooperative processing for different weather condition is necessary. 

Mixture-of-Experts(MoE)\cite{fedus2022review, shazeer2017lstm-moe, lepikhin2020gshard, ma2018mmoe} is a type of model that has a learnable gate to route the input tokens to different experts sub-network. The philosophy of MoE is about dynamic and condition inputs, specialization experts, and collaborative processing, which is very suitable for multiple weather removal setting. Motivated by MoE, we introduce a novel \textbf{Weather-aware Multi-scale Mixture-of-Experts Module} to address the above issue. 

MoE baseline\cite{lepikhin2020gshard, fedus2021switch} includes two parts. The experts consist of multiple parallel FFNs. The input tokens are passed through each expert and then fusion by weighted summation as the final output. Weights are generated by the router. It takes each token as input and outputs the probability of each token belong to specific expert. We choose Top-K experts for every token. Overall, the formulation of the naive MoE module can be summarized as follows:
\begin{align}
    g(\textbf{y}^{\ell}) & = \text{Softmax}(\text{Top-K}(\textbf{W} \textbf{y}^{\ell})) \\
    \textbf{z}^{\ell +1} & = [\sum_{i=1}^{M}g_i(\textbf{y}^{\ell})\times \text{FFN}_{i}(\text{LN}(\textbf{y}^{\ell})) ] + \textbf{y}^{\ell}    
\end{align}
where $\textbf{y}^{\ell} \in \mathbb{R}^{N\times D}$ is the output of self-attention module, $\textbf{z}^{\ell} \in \mathbb{R}^{N\times D}$ is the input tokens of $\ell$-th Transformer block, $g(\textbf{z}^{\ell}) \in \mathbb{R}^{M}$ is output weights of the router, $g_i(\textbf{z}^{\ell})$ and $\text{FFN}_{i}$ are the fusion weights and the $i$-th expert respectively, and $M$ is the number of the task experts. The experiments in Section 5. verify the effective of the standard MoE module.

\subsection{Weather-aware Router}

Further, we want to explore the learned routing rules for new insight. We think in ideal conditions, experts will automatically split into different groups with each one proficient in dealing with specific weather condition with the help of router for its selective and dynamic token routing mechanism.

To verify the hypothesis, we add all weights of tokens from images with different weather type as routing scores. If the hypothesis is right, specific expert groups will have higher scores in certain weather type.

However, we don't observe this phenomenon. The routing scores have no obvious difference for diverse weather type. So we rethink the router designed in naive MoE. We find in fact it's hard for the original router to select the tokens according to the weather condition. The essence of the routing mechanism is to complete the pattern matching and clustering of tokens for each expert. In high-level tasks, different tokens contain abstract semantic/category information with high similarity\cite{riquelme2021cv-moe-vit}, resulting in easier clustering. But in the multiple weather removal task, the content information of patch embedding is coupled with the weather information, making router difficult to select specific experts according to the content or weather information. 


In view of the limitation of current router, we propose a novel Weather-aware Router to explicitly make use of the weather information as an auxiliary basis. We realize this by introducing an additional classification branch based on ViT\cite{dosovitskiy2020vit}. In the training stage, we conduct weather classification supervised learning to learn weather representation. The output tokens of ViT are aggregated by Global Averaging Pooling(GAP) and then pass through MLP to get the prediction. In this way, each token embedding contains the weather information. 

To simulate the uncertainty of mixture weather, we propose the Random Label strategy to learn more robust weather feature. We assign a certain id for images with single weather, and choose random id for each image with mixuture weather in each training epoch. The Random Label strategy reduces the confidence of the classifier when dealing with mixture weather.

These token-level weather embedding could be used for the router as a supplement information in the restoration branch. We concatenate the weather and the content tokens alone the channel dimension and then utilize a non linear adaptor to aggregate the feature. The adaptor's outputs are as the final inputs to the router. In this way, the Weather-aware Router can select experts based on both the content and weather information, leading to better weather removal performance.

\begin{figure}[ht]
  \centering
  \includegraphics[width=1\linewidth]{Figure/Method/Multi-scale_expert.pdf}
  \caption{Illutration of Multi-scale Expert designed in our method.}
  \label{fig:Multi-scale_expert}
\end{figure}

\subsection{Multi-scale Experts}

Different types of weather require different receptive field information, for example, the occlusion is more serious in heavy rain, so the receptive field must be larger than that of light rain for removal. However, standard MoE can only process token level information without token interaction due to point-wise FFN. To this end, we design multi-scale experts to process information at different scales. Motivated by shunted Transformer and Inception, we propose grouping experts at different scales. Each expert has a parallel Depth-Wise ConVolution(DWConv) with nxn kernel size, and different groups have different n. Combined with Weather-aware router, the model can adaptively select experts with corresponding receptive fields to process different weather condition. The overall architecture is shown in fig \ref{fig:Multi-scale_expert}.



\subsection{Label-free Perception-aware Metric}

Previous works\cite{liu2022exploring} measure downstream tasks performance based on reconstructed images. However, there is a problem in this evaluation protocol that it can't be used when labels of test data for downstream tasks are inaccessible, such as segmentation mask and 2D/3D bounding box, which limits the application for many practical scenarios.

To solve this problem, we propose a novel evaluation method to estimate the downstream performance of restored images, which only requires the corresponding weather-free images instead of semantic labels. \cite{liu2022exploring} points out optimizing the recognition loss leading to consistent improvement in downstream performance, i.e. the smaller the loss is, the higher the performance is, no matter what recognition architectures, tasks and datasets. So, we can get an intuitive deduction that the loss value is highly related to downstream performance in a variety of situations, which can be considered as a general metric.  

Motivated by this, we define a Perception-aware Metric $M_{pa}$ by measuring the discrepancy between the restored image and clean image in both semantic output and intermediate feature of a general segmentation model such as Unet or MaskRCNN pretrained on large scale dense prediction datasets like Coco. $M_{pa}$ can be formulated as follow:

\begin{equation}
    M_{pa} = l_{dis}(R(P(I_{in})), R(I_{target}))
\end{equation}

$M_{pa}$ indicates the semantic similarity between the restored images and ground truth from the model's point of view. The experiments in section 5 show there are high correlations between our Perception-aware Metric $M_{pa}$ and downstream segmentation metrics mIoU and mAcc, while the traditional image quality metrics PSNR and SSIM do not, which demonstrates the effectiveness of $M_{pa}$. 

The proposed metric greatly alleviates the label dependency when judging whether image processing model's outputs are suitable for downstream task, which has broader prospects in the perception-aware adverse weather removal task in autonomous driving.

\section{New benchmarks for multiple adverse weather removal} \label{MAW-Sim_intro}
In order to train our MoWE and implement synthetic datasets for multiple adverse weather removal, we managed to collect and annotate Multiple Adverse Weather (MAW) datasets named MAW-Sim. The new benchmark contains 5 types of weather including rainy, foggy, snowy, mix of them and clear day. Table \ref{tab:Summary_datasets} is a summary of MAW-Sim.
\newcommand\MyXhlineA[0]{\Xhline{2.5\arrayrulewidth}}
\renewcommand\arraystretch{1.0}%调行高
\setlength\tabcolsep{6pt}%调列距
\begin{table}[ht]
	\centering
        \small
        % \footnotesize
        
	% \resizebox{0.65\columnwidth}{!}{%
 \caption{Summary of the proposed MAW-Sim datasets. We count the number of images in the train, val, and test sets of different weather types in the datasets. Mix represents the mix of the rainy, snowy, foggy weather types. }
		\begin{tabular}{|c|c|c|c|}
            \MyXhlineA
			{MAW-Sim}            & Train  & Validation & Test \\ 
            % \cline{2-7}
            \MyXhlineA
			Rainy      & 0.84K  & 0.12K  & 0.24K\\
			Snowy      & 0.84K  & 0.12K  & 0.24K\\
		  Foggy      & 0.84K  & 0.12K  & 0.24K\\
		  Mix      & 0.84K  & 0.12K  & 0.24K\\
		  Clear    & 0.84K  & 0.12K  & 0.24K\\
            \MyXhlineA
			Total    & 4.2K  & 0.12K  & 0.24K\\
            \MyXhlineA
		\end{tabular}
	% }
	
	\label{tab:Summary_datasets}
	% \vspace{-1em}
\end{table}


We propose MAW-Sim as a large synthetic deweather benchmark for spike data with 50 scenes. The dataset is generated in Unity3D engine, in which we build a large city traffic system and a weather generation system. To make the simulation environment more realistic, we set the path of the vehicle movement and the time to change the traffic signals. For each scene, we equip a car with cameras to record automatically. We provide the images rendered at 30fps with a bit of natural motion blur. Each scene contains 5 different kinds of weather, including rain, snow, fog, and a mix of them as model input and clear day as ground truth. Each scene has 30 frames of RGB images with a resolution of 1024×768. For training, evaluation, and testing, we further divide the dataset according to the ratio of 7:1:2. 
% In supplementary material, we show several example pairs of MAW-Sim in a scene, including the ground truth, RGB images under rainy, snowy, and foggy sky.
% As shown in Figure \ref{fig:vis_CitySpike100K}, we show an example pair of MAW-Sim in a scene, including the ground truth, RGB images under rainy, snowy, and foggy sky.


\newcommand\MyXhlineB[0]{\Xhline{2.5\arrayrulewidth}}
\renewcommand\arraystretch{1.0}%调行高
\setlength\tabcolsep{3pt}%调列距
\begin{table*}[!ht]
	\centering
        \small
        % \footnotesize
        
	\caption{Quantitative Comparison on MAW-Sim based on PSNR and SSIM.}
	% \resizebox{2.0\columnwidth}{!}{%
		\begin{tabular}{c|c|c|c|c|c|c|c|c}
			\MyXhlineB
% 			\multicolumn{2}{c|}{Type}  & Method  & Venue   & Derain     & Dehaze   & Desnow & Dehaze+Derain+Desnow & Average\\
            \multicolumn{2}{c|}{\multirow{2}*{Type}} & \multirow{2}*{Method}  & \multirow{2}*{Venue}   & Derain     & Dehaze  & Desnow & Mixed Weather & Average\\
            \cline{5-9}
            \multicolumn{2}{c|}{\multirow{2}{*}{}} & \multirow{2}{*}{} & \multirow{2}{*}{} & 
            PSNR / SSIM  & PSNR / SSIM & PSNR / SSIM & PSNR / SSIM  & PSNR / SSIM      \\
            \MyXhlineB
            
			\multirow{9}{*}{\begin{tabular}[c]{@{}c@{}}Task\\ Specific\end{tabular}} &\multirow{2}{*}{Derain} 
                                                   & RESCAN~\cite{li2018recurrent} & \textit{ECCV 2018} & 21.42 / 0.7467 & 22.30 / 0.8554 & 21.58 / 0.7177 & 20.77 / 0.6539 & 21.52 / 0.7434  \\
			\multirow{11}{*}{} & \multirow{2}{*}{} & PReNet~\cite{ren2019progressive} & \textit{CVPR 2019} & 28.89 / 0.9170 & 29.47 / \subbest{0.9467} & \subbest{29.24} / \subbest{0.9330} & 28.09 / \subbest{0.9081} & 28.17 / \subbest{0.9262}  \\
            \cline{2-9}
            
            \multirow{11}{*}{} & \multirow{5}{*}{Dehaze}  
                                                   & EPDN~\cite{qu2019enhanced}                  & \textit{CVPR 2019} & 14.19 / 0.5330 & 15.75 / 0.6892 & 13.41 / 0.4922 & 11.87 / 0.3858 & 13.81 / 0.5251  \\
			\multirow{11}{*}{} & \multirow{5}{*}{} & GridDehazeNet~\cite{liu2019griddehazenet}   & \textit{ICCV 2019} & 28.96 / 0.8901 & 28.94 / 0.9216 & 29.22 / 0.9045 & \subbest{28.21} / 0.8759 & 28.83 / 0.8980  \\
			\multirow{11}{*}{} & \multirow{5}{*}{} & MSBDN-DFF~\cite{dong2020multi}              & \textit{CVPR 2020} & 28.09 / 0.8985 & 28.55 / 0.9321 & 28.30 / 0.9118 & 27.48 / 0.8854 & 28.11 / 0.9070  \\
			\multirow{11}{*}{} & \multirow{5}{*}{} & FFA-Net~\cite{qin2020ffa}                   & \textit{AAAI 2020} & 27.44 / 0.8776 & 28.73 / 0.9330 & 27.54 / 0.8810 & 26.30 / 0.8454 & 27.50 / 0.8843  \\ 
            \multirow{11}{*}{} & \multirow{5}{*}{} & AECR-Net~\cite{wu2021contrastive}           & \textit{CVPR 2021} & 26.89 / 0.8584 & 27.76 / 0.9062 & 26.89 / 0.8658 & 26.29 / 0.8365 & 26.96 / 0.8667  \\ 
            \cline{2-9}

            \multirow{11}{*}{} & \multirow{2}{*}{Desnow}  
                                                   & DesnowNet~\cite{liu2018desnownet}   & \textit{TIP 2018}  & 20.51 / 0.6663 & 25.01 / 0.8352 & 20.16 / 0.6198 & 18.25 / 0.5446 & 20.98 / 0.6665  \\
			\multirow{11}{*}{} & \multirow{2}{*}{} & HDCWNet~\cite{chen2021all}          & \textit{ICCV 2021} & 23.98 / 0.7236 & 25.42 / 0.8392 & 24.36 / 0.8254 & 21.36 / 0.6724 & 23.78 / 0.7652  \\
            
            \MyXhlineB
			\multicolumn{2}{c|}{\multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Task\\ Agnostic\end{tabular}}}     
                                                     & MPR~\cite{zamir2021multi}            & \textit{CVPR 2021}  & 27.02 / 0.8881 & 28.14 / 0.9297 & 27.81 / 0.9170 & 26.39 / 0.8753 & 27.34 / 0.9025  \\
            % \multicolumn{2}{c|}{\multirow{4}{*}{}}   & HINet     & \textit{CVPR 2021}  & 32.25 / 0.9473 & 32.51 / 0.9679 & 33.12 / 0.9598 & 30.91 / 0.9409 & 32.20 / 0.9539  \\
            % \multicolumn{2}{c|}{\multirow{4}{*}{}}   & SwinIR    & \textit{CVPR 2021}  & 31.80 / 0.9275 & 32.15 / 0.9560 & 32.35 / 0.9431 & 30.52 / 0.9183 & 31.71 / 0.9437  \\
            \multicolumn{2}{c|}{\multirow{4}{*}{}}   & Restormer~\cite{zamir2022restormer} & \textit{CVPR 2022}  & 28.23 / 0.9016 & 28.21 / 0.9350 & 28.30 / 0.9137 & 27.11 / 0.8859 & 27.96 / 0.9091  \\

            \MyXhlineB
			\multicolumn{2}{c|}{\multirow{3}{*}{\begin{tabular}[c]{@{}c@{}}Multi\\ Task in One\end{tabular}}}     
                                                     & Transweather~\cite{valanarasu2022transweather}   & \textit{CVPR 2022}  & 22.55 / 0.6608 & 22.97 / 0.7163 & 21.90 / 0.5857 & 21.38 / 0.5507 & 22.20 / 0.6284  \\
            \multicolumn{2}{c|}{\multirow{3}{*}{}}   & Unified Model~\cite{chen2022learning}            & \textit{CVPR 2022}  & \subbest{29.04} / \subbest{0.8913} & \subbest{29.63} / 0.9312 & 29.11 / 0.9002 & 28.00 / 0.8701 & \subbest{28.95} / 0.8982  \\
            \multicolumn{2}{c|}{\multirow{3}{*}{}}   & \textbf{MoWE}           & \textbf{Ours}                            & \best{30.11} / \best{0.9348} & \best{30.85} / \best{0.9622} & \best{30.22} / \best{0.9429} & \best{28.94} / \best{0.9217} & \best{30.03} / \best{0.9404}  \\

            % \MyXhlineB
            % \multicolumn{4}{c|}{Our advantages over the best other method} &  \ 1.07 / 0.0435 & \ 1.22 / 0.0155 & \ 0.98 / 0.0099 & \  0.73 / 0.0136 & \ 1.08 / 0.0142 \\
            
            \MyXhlineB
            
		\end{tabular}
	% }
	\label{synthetic_results}
\end{table*} 

\begin{figure*}[ht]
  \centering
  \includegraphics[width=\linewidth]{Figure/Experiment/vis_allweather.pdf}
  \caption{Qualitative results to visualize the deweather performance on All-Weather.}
  \label{fig:vis_allweather}
\end{figure*}

\section{Experiments}
In this section, we evaluate our work on MAW-Sim, All-Weather and Cityscapes datasets and compare it with several state-of-the-art methods. To further illustrate the effectiveness of MoWE, the performance of downstream tasks for semantic segmentation in Cityscapes is also provided. Moreover, we also implement an ablation study to shed light on why our method performs better on deweather tasks. 
% \subsection{Datasets}

\subsection{Experiment Setting}
\textbf{Dataset.} We train and evaluate MoWE and state-of-the-art methods on MAW-Sim, All-weather, and Cityscapes datasets.
% with 4, 3, and 2 different weather types including rain, haze, snow, and a combination of them. 
MAW-Sim has been described in Section \ref{MAW-Sim_intro}. 
All-weather\cite{valanarasu2022transweather} contains data from different public datasets. The training set consists of Outdoor-Rain\cite{li2019outdoor-rain}, Raindrop\cite{qian2018raindrop} and Snow100K\cite{liu2018snow100k}. The test set is sampled from Outdoor-Rain\cite{li2019outdoor-rain}, the Raindrop test set\cite{qian2018raindrop}, and the Snow 100k-L test set\cite{liu2018snow100k}. Cityscapes\cite{cordts2016cityscapes} is collected from various scenarios of outdoor street scenes in different cities. These images are recorded in normal weather conditions by video cameras mounted on cars. Foggy Cityscapes and Rainy Cityscapes are synthesized from the images in the Cityscapes dataset. We further combine the train sets of them for training. We also adopt the fine semantic segmentation label of Cityscapes for downstream tasks.


% \textbf{Training}

% \textbf{Finetuning}
\newcommand\MyXhlineC[0]{\Xhline{2.5\arrayrulewidth}}
\renewcommand\arraystretch{1.0}%调行高
\setlength\tabcolsep{6pt}%调列距
\begin{table*}[ht]
	\centering
        \small
        % \footnotesize
        
	% \resizebox{2.0\columnwidth}{!}{%
	\caption{Quantitative Comparison on All-Weather based on PSNR and SSIM. }
		\begin{tabular}{c|c|c|c|c|c|c|c}
            \MyXhlineC
            \multicolumn{2}{c|}{\multirow{2}*{Type}} & \multirow{2}*{Method}  & \multirow{2}*{Venue}   & Derain & Deraindrop  & Desnow & Average\\
            \cline{5-8}
            \multicolumn{2}{c|}{\multirow{2}{*}{}} & \multirow{2}{*}{} & \multirow{2}{*}{} & 
            PSNR / SSIM  & PSNR / SSIM & PSNR / SSIM & PSNR / SSIM      \\
            \MyXhlineC
            
			\multirow{9}{*}{\begin{tabular}[c]{@{}c@{}}Task\\ Specific\end{tabular}} &\multirow{2}{*}{Derain} 
                                                   & RESCAN~\cite{li2018recurrent}    & \textit{ECCV 2018} & 21.57 / 0.7255 & 24.26 / 0.8367 & 24.30 / 0.7586 & 23.38 / 0.7736  \\
			\multirow{11}{*}{} & \multirow{2}{*}{} & PReNet~\cite{ren2019progressive} & \textit{CVPR 2019} & 23.16 / 0.8624 & 24.96 / 0.8629 & 25.19 / 0.8483 & 24.44 / 0.8579  \\
            \cline{2-8}
            
            \multirow{11}{*}{} & \multirow{5}{*}{Dehaze}  
                                                   & EPDN~\cite{qu2019enhanced}                & \textit{CVPR 2019} & 12.15  / 0.2116  & 20.26  / 0.7266  & 13.89  / 0.5235  & 15.43  / 0.4872  \\
			\multirow{11}{*}{} & \multirow{5}{*}{} & GridDehazeNet~\cite{liu2019griddehazenet} & \textit{ICCV 2019} & 25.31  / 0.8657  & 27.32  / 0.8723  & 27.19  / 0.8457  & 26.61  / 0.8612  \\
			\multirow{11}{*}{} & \multirow{5}{*}{} & MSBDN-DFF~\cite{dong2020multi}            & \textit{CVPR 2020} & 22.62  / 0.8217  & 21.22  / 0.8189  & 25.46  / 0.8130  & 23.10  / 0.8179  \\
			\multirow{11}{*}{} & \multirow{5}{*}{} & FFA-Net~\cite{qin2020ffa}                 & \textit{AAAI 2020} & 27.96  / 0.8857  & 27.73  / 0.8894  & 27.21  / 0.8578  & 27.63  / 0.8776  \\
            \multirow{11}{*}{} & \multirow{5}{*}{} & AECR-Net~\cite{wu2021contrastive}         & \textit{CVPR 2021} & 26.77  / 0.8493  & 26.54  / 0.8846  & 26.77  / 0.8509  & 26.70  / 0.8616  \\
            \cline{2-8}

            \multirow{11}{*}{} & \multirow{2}{*}{Desnow}  
                                                   & DesnowNet~\cite{liu2018desnownet}  & \textit{TIP 2018}  & 12.73 / 0.5327 & 23.80 / 0.8440 & 21.89 / 0.7682 & 19.47 / 0.7150 \\
			\multirow{11}{*}{} & \multirow{2}{*}{} & HDCWNet~\cite{chen2021all}         & \textit{ICCV 2021} & 14.59 / 0.6314 & 24.51 / 0.8514 & 20.21 / 0.7447 & 19.77 / 0.7425 \\
            
            \MyXhlineC
			\multicolumn{2}{c|}{\multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Task\\ Agnostic\end{tabular}}}     
                                                     & MPR~\cite{zamir2021multi}          & \textit{CVPR 2021}  & \subbest{28.35} / \subbest{0.9100} & \subbest{28.33} / \subbest{0.9063} & 27.77 / 0.8772 & \subbest{28.15} / \subbest{0.8978} \\
            % \multicolumn{2}{c|}{\multirow{4}{*}{}}   & HINet     & \textit{CVPR 2021}  & 26.09 / 0.8136 & 25.81 / 0.8570 & 24.60 / 0.7119 & 25.50 / 0.7942 \\
            % \multicolumn{2}{c|}{\multirow{4}{*}{}}   & SwinIR    & \textit{CVPR 2021}  & 24.61 / 0.7901 & 22.48 / 0.6853 & 24.30 / 0.8010 & 23.80 / 0.7588 \\
            \multicolumn{2}{c|}{\multirow{4}{*}{}}   & Restormer~\cite{zamir2022restormer} & \textit{CVPR 2022}  & 27.85 / 0.8802 & 28.32 / 0.8881 & \subbest{28.18} / \subbest{0.8684} & 28.12 / 0.8789 \\

            \MyXhlineC
			\multicolumn{2}{c|}{\multirow{3}{*}{\begin{tabular}[c]{@{}c@{}}Multi\\ Task in One\end{tabular}}}     
                                                     & Transweather~\cite{valanarasu2022transweather}   & \textit{CVPR 2022}  & 25.64 / 0.8103 & 27.37 / 0.8570 & 26.98 / 0.8305 & 26.66 / 0.8326 \\
            \multicolumn{2}{c|}{\multirow{3}{*}{}}   & Unified Model~\cite{chen2022learning}            & \textit{CVPR 2022}  & 25.81 / 0.8544 & \subbest{28.33} / 0.8832 & 27.94 / 0.8679 & 27.36 / 0.8685 \\
            \multicolumn{2}{c|}{\multirow{3}{*}{}}   & \textbf{MoWE}           & \textbf{Ours}                   & \best{28.47} / \best{0.9376} & \best{29.10} / \best{0.9333} & \best{29.28} / \best{0.8989} & \best{28.95} / \best{0.9233} \\

            % \MyXhlineC
            % \multicolumn{4}{c|}{Our advantages over the best other method} & \ 0.12 / 0.0276 & \ 0.77 / 0.027 & \ 1.1 / 0.0305 & \ 0.8 / 0.0255 \\
            
            \MyXhlineC
            
		\end{tabular}
	% }
    % \vskip -9 pt
	\label{real_results}
\end{table*} 

% \textbf{Finetuning}
\newcommand\MyXhlineE[0]{\Xhline{2.5\arrayrulewidth}}
\renewcommand\arraystretch{1.0}%调行高
\setlength\tabcolsep{1pt}%调列距
\begin{table*}[ht]
	\centering
        \small
        % \footnotesize
	\caption{Quantitative Comparison on Cityscapes. }
	% \resizebox{2.0\columnwidth}{!}{%
		\begin{tabular}{c|c|c|c|c|c|c|c|c|c|c|c|c|c}
            \MyXhlineE
            \multicolumn{2}{c|}{\multirow{2}*{Type}} & \multirow{3}*{Method}  & \multirow{3}*{Venue}   & \multicolumn{5}{c|}{Derain}  & \multicolumn{5}{c}{Dehaze} \\
            \cline{5-14}
            \multicolumn{2}{c|}{\multirow{2}{*}{}} & \multirow{2}{*}{} & \multirow{2}{*}{} & 
            \multicolumn{2}{c|}{Upstream} & \multicolumn{3}{c|}{Downstream} & \multicolumn{2}{c|}{Upstream} & \multicolumn{3}{c}{Downstream}      \\
            \cline{5-14}
            \multicolumn{2}{c|}{\multirow{2}{*}{}} & \multirow{2}{*}{} & \multirow{2}{*}{} & 
            PSNR $\uparrow$ & SSIM $\uparrow$   & mIoU $\uparrow$ & mAcc $\uparrow$ & Metric $\downarrow$ & PSNR $\uparrow$ & SSIM $\uparrow$  & mIoU $\uparrow$ & mAcc $\uparrow$ & Metric $\downarrow$      \\
            \MyXhlineE
            
			\multirow{7}{*}{\begin{tabular}[c]{@{}c@{}}Task\\ Specific\end{tabular}} &\multirow{2}{*}{Derain} 
                                                   & RESCAN~\cite{li2018recurrent}    & \textit{ECCV 2018} & 19.11 & 0.9118 & 0.1007 & 0.2064 & 0.1588       & 16.96 & 0.9033 & 0.1262 & 0.2116 & 0.1216 \\
			\multirow{11}{*}{} & \multirow{2}{*}{} & PReNet~\cite{ren2019progressive} & \textit{CVPR 2019} & 19.95 & 0.8822 & 0.1321 & 0.2943 & 0.1107      & 18.22 & 0.8729 & 0.3305 & 0.2508 & 0.0727 \\
            \cline{2-14}
            
            \multirow{11}{*}{} & \multirow{5}{*}{Dehaze}  
                                                   & EPDN~\cite{qu2019enhanced}                & \textit{CVPR 2019} & 11.67 & 0.5013 & 0.3603 & 0.5632 & 0.0684      & 12.15 & 0.5112 & 0.4052 & 0.5716 & 0.0781 \\
			\multirow{11}{*}{} & \multirow{5}{*}{} & GridDehazeNet~\cite{liu2019griddehazenet} & \textit{ICCV 2019} & 22.08 & 0.9171 & 0.4259 & 0.6945 & 0.0453      & 23.18 & 0.9183 & 0.4453 & 0.7108 & 0.0533 \\
			\multirow{11}{*}{} & \multirow{5}{*}{} & MSBDN-DFF~\cite{dong2020multi}            & \textit{CVPR 2020} & 26.26 & 0.8853 & 0.1744 & 0.2733 & 0.1083      & 26.79 & 0.8903 & 0.3426 & 0.4019 & 0.0673 \\
			\multirow{11}{*}{} & \multirow{5}{*}{} & FFA-Net~\cite{qin2020ffa}                 & \textit{AAAI 2020} & 28.29 & 0.9411 & 0.3458 & 0.5799 & 0.0789      & 28.96 & 0.9432 & 0.4257 & 0.6005 & 0.0614 \\
            \multirow{11}{*}{} & \multirow{5}{*}{} & AECR-Net~\cite{wu2021contrastive}         & \textit{CVPR 2021} & 26.27 & 0.9075 & 0.2230 & 0.3516 & 0.0869      & 27.75 & 0.9062 & 0.3714 & 0.4652 & 0.0842 \\
            \cline{2-8}

            \MyXhlineE
			\multicolumn{2}{c|}{\multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Task\\ Agnostic\end{tabular}}}     
                                                     & MPR~\cite{zamir2021multi}          & \textit{CVPR 2021}  & \subbest{32.68} & \best{0.9810} & \subbest{0.4657} & \subbest{0.7580} & \best{0.0287}      & \subbest{29.73} & \best{0.9752} & \subbest{0.4537} & \subbest{0.7301} & \subbest{0.0292} \\
            \multicolumn{2}{c|}{\multirow{4}{*}{}}   & Restormer~\cite{zamir2022restormer} & \textit{CVPR 2022} & 28.06 & 0.9630 & 0.4383 & 0.6833 & 0.0386      & 22.72 & 0.9411 & 0.4085 & 0.6920 & 0.0542 \\

            \MyXhlineE
			\multicolumn{2}{c|}{\multirow{4}{*}{\begin{tabular}[c]{@{}c@{}}Multi\\ Task in One\end{tabular}}}     
                                                     & Transweather~\cite{valanarasu2022transweather}   & \textit{CVPR 2022}  & 24.08 & 0.8481 & 0.4425 & 0.671 & 0.0622      & 22.56 & 0.8736 & 0.3643 & 0.6105 & 0.0693 \\
            \multicolumn{2}{c|}{\multirow{3}{*}{}}   & Unified Model~\cite{chen2022learning}            & \textit{CVPR 2022}  & 28.25 & 0.9504 & 0.4190 & 0.7246 & 0.0469      & 27.96 & 0.9167 & 0.4336 & 0.7231 & 0.0523 \\
            \multicolumn{2}{c|}{\multirow{3}{*}{}}   & \textbf{MoWE}           & \textbf{Ours}                                & \best{32.99} & \subbest{0.9755} & \best{0.4686} & \best{0.7701} & \subbest{0.0300} & \best{31.31} & \subbest{0.9647} & \best{0.4545} & \best{0.7473} & \best{0.0270} \\
            % \multicolumn{2}{c|}{\multirow{3}{*}{}}   & \textbf{MoWE + sem loss} & \textbf{Ours}                               & 30.74 & 0.9697 & 0.4573 & 0.7058 & 0.0256 & 30.46 & 0.9484 & 0.4479 & 0.7282 & 0.0241 \\

            % \MyXhlineE
            % \multicolumn{4}{c|}{Our advantages over the best other method} & \ 0.12 / 0.0276 & \ 0.77 / 0.027 & \ 1.1 / 0.0305 & \ 0.8 / 0.0255 \\
            
            \MyXhlineE
            \multicolumn{4}{c|}{Lower Bound}  & \textbackslash{} & \textbackslash{} & 0.3416 & 0.6020 & \textbackslash{} & \textbackslash{} & \textbackslash{}      & 0.4250 & 0.6391 & \textbackslash{} \\
            \MyXhlineE
            \multicolumn{4}{c|}{Upper Bound}             & \textbackslash{} & \textbackslash{} & 0.4594 & 0.7685 & \textbackslash{} & \textbackslash{} & \textbackslash{}      & 0.4594 & 0.7685 & \textbackslash{} \\
            \MyXhlineE
            
		\end{tabular}
	% }
    % \vskip -9 pt
	\label{cityscapes_results}
\end{table*} 


\begin{figure*}[ht]
  \centering
  \includegraphics[width=\linewidth]{Figure/Experiment/vis_cityscapes.pdf}
  \caption{Qualitative results to visualize the deweather performance and facilitation for downstream semantic segmentation on Cityscapes.}
  \label{fig:vis_cityscapes}
\end{figure*}

\subsection{Comparison with State-of-the-art Methods} \label{formal version}

\textbf{Baselines.} To verify the effectiveness of our proposed method, we compare it with several representative and state-of-the-art baseline methods. We divide all the baselines into 3 groups consisting of task-specific, task-agnostic, and multi-task-in-one. Task-specific group represents the previous networks that aim to deal with certain weather, including derain, dehaze, and desnow. For deraining, we compare our method with RESCAN\cite{li2018recurrent}, PReNet\cite{ren2019progressive}. For dehazing, we compare our method with EPDN\cite{qu2019enhanced}, GridDehazeNet\cite{liu2019griddehazenet}, MSBDN-DFF\cite{dong2020multi}, FFA-Net\cite{qin2020ffa}, and AECR-Net\cite{wu2021contrastive}. For desnowing, we compare our method with DesnowNet\cite{liu2018desnownet}, and HDCWNet\cite{chen2021all}. Task-agnostic group indicates that a network can deal with different weathers but trains and works on the same weather. In other words, these methods need to be trained for each task separately. We make a comparison with MPR\cite{zamir2021multi}, HINet\cite{chen2021hinet}, SwinIR\cite{liang2021swinir}, and Restormer\cite{zamir2022restormer}. The last group multi-task-in-one represents the network that can train and work on different types of weather. Our method also belongs to this group. We compare with Transweather\cite{valanarasu2022transweather} and Unified Model\cite{chen2022learning}. To better compare the performance of downstream tasks, we also implement experiments of lower bound and upper bound, which takes weather input of upstreams and deweather ground truth as downstream input respectively.

\textbf{Metrics.} For the quantitative evaluation, we adopt metrics of the peak signal to noise ratio (PSNR) and the structural similarity (SSIM). We also take mean intersection over union(mIoU), mean accuracy(mAcc) and semantic perception(Metric) as our metrics on semantic segmentation for downstream tasks. To obtain the semantic perception metric, we take the ground truth and predictions of deweather tasks as the input of a Fully Convolutional Network(FCN), which has been pretrained on semantic segmentation. Then we apply the cross-entropy loss to the output of FCN and obtain the metric, which indicates the difference between deweather prediction and ground truth for semantic segmentation. To achieve a fair comparison, all experiments are trained on the mix of all types of weather and tested on specific types of weather respectively. 

\newcommand\MyXhlineD[0]{\Xhline{2.5\arrayrulewidth}}
\renewcommand\arraystretch{1}
\setlength\tabcolsep{3pt}%调列距
\begin{table*}[!ht]
    \small
    % \footnotesize
    \centering
    \caption{Ablation study on MoWE.}
    \begin{tabular}{ccccc|c|c|c|c|c|c|c|c}
    \MyXhlineD
    \multirow{2}{*}{Baseline} & 
    Moe-Token & 
    \multirow{2}{*}{\makecell[c]{Weather-aware \\ router}} & 
    \multirow{2}{*}{\makecell[c]{Multi-scale \\ experts}} & 
    \multirow{2}{*}{\makecell[c]{Random \\ label}} & 
    \multicolumn{2}{c|}{Parameters (M)} &
    \multicolumn{2}{c|}{Flops(G)} &
    \multicolumn{2}{c|}{PSNR} & 
    \multicolumn{2}{c}{SSIM} \\
    \cline{6-13}
    & n4-k0 $\vert$ n16-k4 & & & & n4-k0 & n16-k4 & n4-k0 & n16-k4& n4-k0 & n16-k4 & n4-k0 & n16-k4
    \\
    \MyXhlineD
    \ding{51} &           &           &           &           & \multicolumn{2}{c|}{4.95} & \multicolumn{2}{c|}{71.96} & \multicolumn{2}{c|}{29.08} & \multicolumn{2}{c}{0.9263} \\ \cline{6-13}
    \ding{51} & \ding{51} &           &           &           & 6.53 & 12.86 & 75.20 & 88.12 & 29.21 & 29.46 & 0.9297 & 0.9337 \\ \cline{6-13}
    \ding{51} & \ding{51} & \ding{51} &           &           & 10.78 & 17.11 & 88.88 & 101.82 & 29.41 & 29.58 & 0.9309 & 0.9340 \\ \cline{6-13}
    \ding{51} & \ding{51} & \ding{51} & \ding{51} &           & 10.87 & 17.47 & 89.06 & 102.54 & 29.62 & 29.71 & 0.9354 & 0.9365 \\ \cline{6-13}
    \ding{51} & \ding{51} & \ding{51} & \ding{51} & \ding{51} & 10.87 & 17.47 & 89.06 & 102.54 & 29.73 & 29.87 & 0.9365 & 0.9379 \\ 
    
    \MyXhlineD
    \end{tabular}
    \label{tab:ablation}
\end{table*}

\subsubsection{Quantitative Comparison and Analysis}

\textbf{Analysis on Main results.} We report several types of weather and the average of the results in Tables \ref{synthetic_results}, \ref{real_results} and \ref{cityscapes_results}. It can be noted that our method achieves state-of-the-art performances on both MAW-Sim and All-weather datasets. MoWe also obtains competitive results on Cityscapes. For the task specific methods designed for specific deweather tasks like deraining, dehazing and desnowing, the proposed method outperforms all of them by a significant margin. Compared with the task agnostic methods, our method also achieves more promising results. In addition, it can be seen that our method still performs better than multi task in one works on the combination of 3 different weather types. Thanks to the Mixture-of-Experts FFN Module, the model enhances its capacity to exploit the correlation of different tasks and can handle the Multi-Task-in-One problem better. 


\textbf{Analysis on Downstream results.} As for the downstream results illustrated in Tables \ref{cityscapes_results}, it can be seen that weather removal benefits semantic segmentation tasks. MoWE achieves competitive results on deweather tasks while helping the downstream network obtain promising results, which is close to the upper bound performance. It is also noteworthy that the proposed semantic perception metric is negatively correlated with the performance of downstream tasks. A lower semantic perception metric implies higher downstream task performance. The metric allows us to evaluate the impact of the upstream neural network on the specific downstream task without downstream task labels.



\subsubsection{Visual Qualitative Comparison and Analysis}
We illustrate the predictions from All-Weather and Cityscapes in Figure \ref{fig:vis_allweather} and \ref{fig:vis_cityscapes} respectively. Due to the space limitation, visualization on MAW-Sim can be seen in the supplementary material. Compared to the other baselines, it can be seen that our method achieves visually pleasing results on different deweather tasks. Take visualization in All-Weather as an example, our method works better in removing mix of raindrops, snowflakes, and fog while other methods fail to deal with the rain and snow removal and results in artifacts, which is shown in Figure \ref{fig:vis_allweather}. As for the semantic segmentation tasks on Cityscapes, it can also be seen that weather removal benefits the results of downstream tasks in \ref{fig:vis_cityscapes}. After deweather processing by MoWE, the downstream network can achieve semantic segmentation masks more accurately, which demonstrates MoWE is helpful for downstream tasks.



\subsection{Ablation Study}

In this part, we evaluate the contribution of each component in our methods. Our baseline is the vision Transformer with a task-shared convolution head and tail. First, we replace the original FFN in Transformer block with the proposed Task-aware Mixture-of-experts FFN module(TMoE) to verify the effectiveness of the multi-branch structure with gate mechanism in hybrid- All the ablation studies are conducted in the MAW-Sim dataset. The training setting is the same as in \ref{formal version}. The evaluation metrics are the average PSNR and SSIM of the four-weather type testset. The results of the ablation study are presented in Table \ref{tab:ablation}, from which we demonstrate that every design in MoWE could lead to a performance gain.

\section{Conclusion}

In this work, we propose a novel adverse weather removal task setting, training a single model to deal with various adverse weathers without human guidance, which is challenging but also more practical in the autonomous driving scenario. We collect a synthetic dataset MAW-Sim including various weather conditions and choose wildly used All-Weather and Cityscapes datasets as the benchmarks for evaluation. We propose a novel Mixture of Weather Experts(MoWE) framework, which consists of vision Transformer backbone with task-shared head and tail and other two spotlight designs. The Weather-aware Router is proposed to exploit the task correlation while the Multi-scale Experts are utilized to deal with diverse weather condition. Our method achieves SOTA performance in both MAW-Sim and All-Weather and also has competitive results in Cityscapes. Experiments also demonstrate that our proposed MoWE is beneficial to downstream tasks such as semantic segmentation.


{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\end{document}