\documentclass[lettersize,journal]{IEEEtran}
\usepackage{amsmath,amsfonts}
\usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{array}
\usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
\usepackage{textcomp}
\usepackage{stfloats}
\usepackage{url}
\usepackage{verbatim}
\usepackage{graphicx}
\usepackage{cite}
\usepackage{multirow}
\usepackage{xcolor}
\usepackage{makecell}
\usepackage{enumitem}
\usepackage{pifont}
\usepackage{bm}

\newcommand\best[1]{\textbf{\textcolor{red}{#1}}}
\newcommand\subbest[1]{\textcolor{blue}{#1}}
\hyphenation{op-tical net-works semi-conduc-tor IEEE-Xplore}
% updated with editorial comments 8/9/2021

\begin{document}

\title{WM-MoE: Weather-aware Multi-scale Mixture-of-Experts for Blind Adverse\\Weather Removal}

\author{Yulin Luo, Rui Zhao, Xiaobao Wei, Jinwei Chen, Yujie Lu, Shenghao Xie, Tianyu Wang, \\ Ruiqin Xiong, Ming Lu, Shanghang Zhang
    \thanks{Y. Luo, R. Zhao, J. Chen, Y. Lu, T. Wang, R. Xiong, and S. Zhang are with Peking University, Beijing, China. 
    (e-mail: \{yulin, ruizhao, 2000012967\}@stu.pku.edu.cn, tianyuw2001@gmail.com, \{cjw, rqxiong, shanghang\}@pku.edu.cn) (Corresponding author: Shanghang Zhang)}
    \thanks{X. Wei is with University of Chinese Academy of Sciences, Beijing, China. (e-mail: weixiaobao0210@gmail.com)}
    \thanks{S. Xie is with Wuhan University, Wuhan, China. (e-mail: xieshenghao@whu.edu.cn)}
    \thanks{M. Lu is with Intel Labs China. Beijing, China. (e-mail: lu199192@gmail.com)}
}

% The paper headers
\markboth{Journal of \LaTeX\ Class Files,~Vol.~14, No.~8, August~2021}%
{Shell \MakeLowercase{\textit{et al.}}: A Sample Article Using IEEEtran.cls for IEEE Journals}

% \IEEEpubid{0000--0000/00\$00.00~\copyright~2021 IEEE}
% Remember, if you use this you must call \IEEEpubidadjcol in the second
% column for its text to clear the IEEEpubid mark.

%%%
% Intro里可以提到这些现有方法的对比
% Related Work MoE在恶劣天气中的应用
%%%

\maketitle
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{abstract}
Adverse weather removal tasks like deraining, desnowing, and dehazing are usually treated as separate tasks. However, in practical autonomous driving scenarios, the  type, intensity, and mixing degree of weather are unknown, so handling each task separately cannot deal with the complex practical scenarios. In this paper, we study the blind adverse weather removal problem. Mixture-of-Experts (MoE) is a popular model that adopts a learnable gate to route the input to different expert networks. The principle of MoE involves using adaptive networks to process different types of unknown inputs. Therefore, MoE has great potential for blind adverse weather removal. However, the original MoE module is inadequate for coupled multiple weather types and fails to utilize multi-scale features for better performance. To this end, we propose a method called Weather-aware Multi-scale MoE (WM-MoE) based on Transformer for blind weather removal. WM-MoE includes two key designs: WEather-Aware Router (WEAR) and Multi-Scale Experts (MSE). WEAR assigns experts for each image token based on decoupled content and weather features, which enhances the model’s capability to process multiple adverse weathers. To obtain discriminative weather features from images, we propose Weather Guidance Fine-grained Contrastive Learning (WGF-CL), which utilizes weather cluster information to guide the assignment of positive and negative samples for each image token. Since processing different weather types requires different receptive fields, MSE leverages multi-scale features to enhance the spatial relationship modeling capability, facilitating the high-quality restoration of diverse weather types and intensities. Our method achieves state-of-the-art performance in blind adverse weather removal on two public datasets and our dataset. We also demonstrate the advantage of our method on downstream segmentation tasks.
\end{abstract}

\begin{IEEEkeywords}
Blind Weather Removal, Vision Transformer, Mixture-of-Experts (MoE), Contrastive Learning, Deep Learning
\end{IEEEkeywords}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}
In autonomous driving scenarios, adverse weather could severely impact the imaging quality of cameras, leading to the degradation of AI performance~\cite{liu2023deep}. Thus, adverse weather removal is a crucial technique for autonomous driving. Most existing methods handle different weather types separately, such as deraining, desnowing, and dehazing. They can be divided into three categories: task-specific, task-agnostic, and multi-task-in-one methods. The task-specific methods are designed for a specific type of weather, such as deraining~\cite{jiang2020multi, li2018recurrent, ren2019progressive, yu2021single, yang2021end, wang2020deep}, dehazing~\cite{liu2019griddehazenet, dong2020multi, qin2020ffa, wu2021contrastive, li2022usid, lin2022msaff, shin2021region, hu2019adaptive, song2017single}, and desnowing~\cite{liu2018desnownet, chen2020jstasr, chen2021all}. These methods have a weather-related inductive bias, making them difficult to perform well on other tasks. Task-agnostic methods have a unified solution to different tasks but need to be trained separately~\cite{zamir2021multi, liang2021swinir, zamir2022restormer}, and need users to select specific parameters according to the weather type. The applications of task-agnostic methods are quite limited. Hence, their usefulness is limited as well. Multi-task-in-one methods can handle different types of weather using a single set of parameters~\cite{chen2021ipt, valanarasu2022transweather, chen2022unified, kulkarni2022unified}, but they still have several limitations. For example, the schemes of defining weather types in existing methods are complex, and mixed weather types are not considered. BIDeN~\cite{han2022blind} formulates the real-world mixture weather removal as a blind image decomposition task. During training, the multi-domain GAN-based model requires each weather component, which is not available in real scenarios, thus limiting its applicability.

Since the type, intensity, and mixing degree of the weather are unknown in the real world, recent blind weather removal aims to restore corrupted images with unknown weather types. It has gained increasing attention from the community~\cite{li2020all, han2022blind, valanarasu2022transweather, chen2022unified}. The key to blind weather removal is dynamically processing the input based on the weather type. Mixture-of-Experts (MoE)~\cite{han2022survey} is a model that adopts adaptive expert networks to process different inputs with the help of a router. Therefore, MoE has great potential for blind weather removal. Some methods have tried to apply MoE to the weather removal task. HCT-FFN~\cite {chen2023hybrid} utilizes degradation-aware MoE (DaMoE) to extract local features for restoring spatially-varying rain degradation, DRSformer~\cite{chen2023learning} learns enriched sparse content features for deraining via a mixture of experts feature compensator. DAN-Net~\cite{ye2022towards} employs adaptive gated neural to modulate the outputs of task-specific experts, which can be utilized to deal with the mix of snow and haze. However, they neglect using weather features to guide expert selection for the blind weather removal task.

Our work also explores the application of MoE in blind weather removal. We first introduce the MoE module into the ViT-based image restoration network and take it as our baseline. Although performance can be improved by introducing MoE directly, we still find two limitations of this baseline. Firstly, the basic router of MoE cannot well assign the correct weather type to the input due to the coupled weather and content embedding. Secondly, since processing different weather types requires different receptive fields, naive experts fail to exploit the multi-scale features for better performance.

To this end, we propose a method called Weather-aware Multi-scale Mixture-of-Experts (WM-MoE) based on the Transformer for blind adverse weather removal. We use the vision transformer as the baseline, with a task-shared convolution head and tail, and a naive MoE module. To handle diverse adverse weather conditions with multiple weather experts, we design a WEather-Aware Router (WEAR) that can route each image token to specific experts based on decoupled content and weather features. Therefore, WEAR can focus more on weather information to select experts than image content such as texture richness and brightness. To obtain distinctive weather features, we propose Weather Guidance Fine-grained Contrastive Learning (WGF-CL). After obtaining weather tokens by a light-weight ViT encoder, WGF-CL optimizes the mutual distance of these embeddings in a token-level supervised contrastive learning manner, which utilizes the intra-weather similarity and inter-weather difference to guide the selection of positive and negative samples for each token. We further introduce Multi-Scale Experts (MSE) to fuse multi-scale features and enhance the spatial modeling capability, leading to better performance than the original token-wise FFN experts. We demonstrate the effectiveness of the proposed WM-MoE on multiple benchmarks. In the upstream blind weather removal task, WM-MoE surpasses current state-of-the-art (SOTA) methods on the proposed dataset MAWSim by PSNR +1.51 and
SSIM +0.022, the public dataset Allweather~\cite{valanarasu2022transweather} by PSNR +0.96 and SSIM +0.0304, RainCityscapes~\cite{hu2019rain_cityscapes} by PSNR +0.31 and HazeCityscapes~\cite{sakaridis2018haze_cityscapes} by PSNR +1.58. In the downstream task, the image recovered by our method can also improve the performance of the semantic segmentation model, which is more stable than other methods. The contributions can be concluded as follows:
\begin{itemize}
\item We propose a novel framework named Weather-aware Multi-scale Mixture-of-Experts (WM-MoE) for blind adverse weather removal. We design a WEather-Aware Router (WEAR) to assign specific experts for each image token more effectively based on decoupled content and weather features. We also develop Multi-Scale Experts (MSE) to aggregate local and multi-scale features, improving spatial modeling capability.

\item We propose Weather Guidance Fine-grained Contrastive Learning (WGF-CL) to capture discriminative and detailed weather representation. The learned weather features serve as supplementary input for WEAR to select experts, effectively handling complex weather conditions.

\item We conduct comprehensive experiments on two public datasets and our dataset and achieve SOTA blind weather removal performance. We also demonstrate the advantage of our method on downstream segmentation tasks.
\end{itemize}

% \begin{itemize}
% \item We propose a novel method named Weather-aware Multi-scale Mixture-of-Experts (WM-MoE) for blind adverse weather removal. Our method uses Weather-aware Router to assign the correct weather type and Multi-scale Experts to improve the spatial modeling capability.

% \item We propose Weather Guidance Fine-grained Contrastive Learning strategy to decouple the weather and content information from the input image. 

% \item We conduct comprehensive experiments on two public datasets and our own dataset, and achieve SOTA blind weather removal performance. We also demonstrate our method's advantage on downstream segmentation tasks.

% % \item We propose a novel label-free perception-aware metric to measure whether the outputs of image processing models are suitable for high level perception tasks without the demand for semantic labels.

% % \item We propose a new syntactic dataset towards autonomous driving scenarios to benchmark the capabilities to remove various kinds of weather condition, including rain, haze, snow and mixture weather type.

% %\item Experimental results show that MoWE can achieve SOTA performance in blind weather removal task on our synthetic dataset MAWSim, public datasets Allweather, RainCityscapes, and FogCityscapes. MoWE can also consistently improve the downstream task performance compared to other methods.
% % by PSNR +1.08 and SSIM +0.0142, the public dataset Allweather by PSNR +0.8 and SSIM +0.0255, RainCityscapes by PSNR +0.31 and FogCityscapes by PSNR +1.58. 
% \end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Related Work}
\subsection{Adverse Weather Removal}
Adverse weather removal has been explored over the past years. Related works can be divided into task-specific and task-agnostic. The adverse weather mainly includes rain, snow, and haze. For task-specific methods, one network aims to deal with certain weather. Li et al.~\cite{li2018recurrent}, Ren et al.~\cite{ren2019progressive}, and Jiang et al.~\cite{jiang2020multi} remove rain based on progressively refining the image. Dong et al.~\cite{dong2020multi} remove haze based on boosting and error feedback. GridDehazeNet~\cite{liu2019griddehazenet} and FFA-Net~\cite{qin2020ffa} use attention operations for dehazing. Wu et al.~\cite{wu2021contrastive} introduce contrastive learning to dehaze. DeSnowNet~\cite{liu2018desnownet}, JSTASR~\cite{chen2020jstasr}, and Chen et al.~\cite{chen2021all} aims to remove snow with different status adaptively.

In contrast to task-specific methods, some methods can be adopted for different weathers, i.e., the task-agnostic methods. Some of these methods can deal with only one type of degradation. These methods need to be trained for each task separately. MPRNet~\cite{zamir2021multi}, SwinIR~\cite{liang2021swinir}, and Restormer~\cite{zamir2022restormer} are architectures for general image restoration. Most of these architectures implement deraining as one of the tasks in the experiments. Some methods can remove multiple adverse weathers at once. All-in-One~\cite{li2020all} uses neural architecture search (NAS) to discriminate between different tasks.
Several strategies are proposed to handle multiple adverse weathers simultaneously. TransWeather~\cite{valanarasu2022transweather} uses learnable weather-type embeddings in the decoder. Chen et al.~\cite{chen2022learning} use a two-stage knowledge-learning mechanism for comprehensive bad weather. BID~\cite{han2022blind} aims to decompose degraded images into constituent underlying images and other components.

\subsection{Transformer in Image Restoration Task}
Transformers~\cite{vaswani2017transformer} have been increasingly applied in the vision area since ViT~\cite{dosovitskiy2020vit} employ Transformers to visual recognition task~\cite{han2022survey}. IPT~\cite{chen2021ipt} introduces Transformers pre-trained on a large dataset for image restoration tasks.
SwinIR~\cite{liang2021swinir} introduces the Transformers with shifted windows~\cite{liu2021swin} for image restoration. UFormer~\cite{wang2022uformer} and Restormer~\cite{zamir2022restormer} use Transformers to construct pyramidal network structures for image restoration based on locally-enhanced windows and channel-wise self-attention, respectively. ELAN~\cite{zhang2022efficient} and DATSR~\cite{cao2022reference} consider long-range attention based on Transformers for super-resolution. CAT~\cite{chen2022cross} and Xiao et al.~\cite{xiao2022stochastic} propose Transformers with adaptive windows to perform more flexible image restoration. Image De-raining Transformer (IDT)~\cite{xiao2022idt} develops the complementary window-based and spatial-based transformer to capture local and non-local features. Dehazeformer~\cite{song2023dehazeformer} explores the limitation of Swin Transformer~\cite{liu2021swin} when applied to haze removal and proposes several improvements including modified normalization layer, activation function, and spatial information aggregation scheme. SnowFormer~\cite{chen2022snowformer} employs cross-attention to model the local-global context interaction across patches for better desnowing. 

\begin{figure*}[!ht]
    \centering
\includegraphics[width=0.85\linewidth]{./Figure/Method/AAAI_Methods_V2.pdf}
    \caption{Comparison between (a) naive Mixture-of-Experts (MoE) module~\cite{lepikhin2020moetransformer} and (b) our proposed Weather-aware Multi-scale Mixture-of-Experts (WM-MoE) module. The linear layer router and point-wise FFN experts in the original MoE fail to deal with coupled content and weather features, and local information. Respectfully, we propose a Weather-aware Router to select experts dynamically based on decoupled content and weather embedding and Multi-scale Experts to make full use of local and multi-scale features.}
    \label{fig:method}
\end{figure*}

\subsection{Mixture-of-Experts (MoE)}
MoE is a type of neural network, whose parameters are partitioned into different subsets called "experts"~\cite{fedus2022review}. Different parts of inputs will be routed to specific experts by some router mechanisms in training and inference time~\cite{fedus2022review}. MoE was applied and popularized first in Natural Language Processing (NLP) in the deep learning era. It usually appears as a basic model component% , such as the MoE layers in LSTM~\cite{shazeer2017lstm-moe} and 
, such as the expert FFN layers in Transformer~\cite{lepikhin2020gshard, lepikhin2020moetransformer}. 

The primary purpose of MoE is to scale up model parameters to achieve better performance while maintaining high computation efficiency~\cite{fedus2022review}. At the same time, the applications of MoE in multi-task learning and cross-domain learning are also explored. MMoE~\cite{ma2018mmoe} utilizes multi-gate routing mechanism to model the relationship of different tasks.~\cite{kudugunta2021beyond}
introduces task-level routing instead of common-used token-level routing to realize more efficient inference. DEMix~\cite{gururangan2021demix} introduces domain experts, each of them specialized in different language domains (e.g. medical, News, etc.) and could be ensembled to better generalize to unseen domains. All these works indicate the potential of MoE beyond scaling up.

In computer vision, MoE has been employed in some high-level tasks such as image classification~\cite{riquelme2021cv-moe-vit, liu2022swinT-v2}, object detection~\cite{liu2022swinT-v2, wu2022residual-moe} and segmentation~\cite{wu2022residual-moe}. MoE has also been used in low-level vision. Literature~\cite{emad2022moesr} and~\cite{liang2022efficient} extract underlying degradation features to construct MoE adaptive network to handle different degradation in blind super-resolution. HCT-FFN~\cite{chen2023hybrid} uses MoE to learn spatially-varying rain distribution features in the deraining task. DRSformer~\cite{chen2023learning} uses MoE to extract sparse content features from rainy images. DAN-Net~\cite{ye2022towards} utilizes adaptive attention gate to modulate the outputs of task-specific experts to handle adverse winter weather conditions. Compared with works mentioned above, we focus on the blind adverse weather removal task. We first explore the limitation of the naive MoE structure when applied to this scenario from token assignment and multi-scale features extraction, and then propose specific MoE design to improve performance.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Proposed Approach -- WM-MoE}
In this section, we first present the overall pipeline of our WM-MoE framework. Then we introduce the baseline, naive MoE module in ViT, and analyze its limitation when applied to the task setting. Afterward, we provide the details of the proposed Weather-aware Multi-scale Mixture-of-Experts (WM-MoE), which includes several key components specially designed for blind weather removal tasks: Weather-aware Router, Weather Guidance Fine-grained Contrastive Learning and Multi-scale Experts.

\subsection{Overall Framework}
The overall pipeline of WM-MoE is shown in Fig.\ref{fig:method} (b). WM-MoE has two parallel branches, one for obtaining weather representation efficiently, and the other for image restoration using task-related features. Both branches are based on Vision Transformer due to their ability in low-level visual tasks~\cite{zamir2022restormer}. 
% The long-range dependence modeling and adaptive spatial aggregation~\cite{liang2021swinir} of Transformer make it suitable for weather removal tasks because they require much global information according to weather physical model~\cite{li2019heavy, li2020all, valanarasu2022transweather}.

Given an image $\textbf{I}_{\rm adverse}\in\mathbb{R}^{3\times H \times W}$ with adverse weather, for the weather representation captured branch, we utilize ViT~\cite{dosovitskiy2020vit} to obtain patch-level weather embedding. Then we optimize Weather Guidance Fine-grained Contrastive Learning loss to achieve discriminative features, which is an additional input to Weather-aware Router in the restoration branch.

For the weather removal branch, we first obtain shallow image embeddings by a task-shared convolution head to capture low-level features. After applying patch embedding to image features, tokens are passed through Transformer encoder with $L$ blocks. In each block, self-attention module models global relationships, followed by the proposed Weather-aware Multi-scale MoE module, which includes a Weather-aware Router to assign tokens reasonably and flexibly based on decoupled content and weather embeddings, and Multi-scale Experts to make full use of local and multi-scale information to handle weather with various conditions. After that, we employ a linear layer to expand token dimensions and reshape them to the origin resolution. Finally, a task-shared convolution tail is utilized to refine features and adjust channels to obtain the restoration clean image.

\begin{figure}[!t]
  \centering
  \includegraphics[width=1\linewidth]{Figure/Method/hist.pdf}
  \caption{Comparison of normalized routing scores histogram between MoE and Weather-aware Router (ours). }
  \label{fig:hist}
\end{figure}

\begin{figure}[!t]
  \centering
  \includegraphics[width=1\linewidth]{Figure/Method/tsne.png}
  \caption{Comparison of t-SNE of routing weights between MoE and Weather-aware Router (ours). }
  \label{fig:tsne}
\end{figure}

\subsection{Baseline: Mixture-of-Experts Module in ViT}

We first introduce our baseline, MoE in ViT, and then analyze its problems for blind weather removal tasks.

MoE baseline~\cite{lepikhin2020gshard, fedus2021switch} includes two parts. The experts consist of multiple parallel FFNs. The input tokens are passed through each expert and then fusion by weighted summation. Weights are generated by the router. It takes each token as input and outputs the probability of each token belonging to a specific expert. We choose Top-K experts for every token. Overall, the formulation of the naive MoE module can be summarized as follows:
\begin{align}
    g (\textbf{y}^{\ell}) & = \text{Softmax} (\text{Top-K} (\textbf{W} \textbf{y}^{\ell})) \\
    \textbf{z}^{\ell +1} & = [\sum_{i=1}^{M}g_i (\textbf{y}^{\ell})\times \text{FFN}_{i} (\text{LN} (\textbf{y}^{\ell})) ] + \textbf{y}^{\ell}    
\end{align}
where $\textbf{y}^{\ell} \in \mathbb{R}^{N\times D}$ is the output of self-attention module, $\textbf{z}^{\ell} \in \mathbb{R}^{N\times D}$ is the input tokens of $\ell$-th Transformer block, $g (\textbf{z}^{\ell}) \in \mathbb{R}^{M}$ is output weights of the router, $g_i (\textbf{z}^{\ell})$ and $\text{FFN}_{i}$ are the fusion weights and the $i$-th expert respectively, and $M$ is the number of the task experts.

% \begin{figure}[!t]
%     \centering
%     \subfigure{
%         \includegraphics[width=0.2\textwidth]{Figure/Method/tsne_moe_baseline.pdf}
%         \label{fig:tsne_moe_baseline}
%     }
%     \subfigure{
%         \includegraphics[width=0.2\textwidth]{Figure/Method/tsne_moe_cls.pdf}
%         \label{fig:tsne_moe_cls}
%     }
%     \caption{Comparison of t-SNE of routing weights between MoE and Weather-aware Router (ours).}
%     \label{fig:tsne}
% \end{figure}

\subsection{Weather-aware Router}

In ideal conditions, experts will form different groups automatically with each one proficient in dealing with specific weather conditions with the help of a router with dynamic tokens routing mechanism.

To verify the hypothesis, we calculate the routing scores $s_{i, w}$ by averaging the output weights of the router for each token from images with different weather types, where $i$ is the ID of experts, $w$ is the weather type including rain, haze, and snow. To compare $s_{i, w}$ of different weather more clearly, we present the histogram of normalized routing scores $s^{norm}_{i, w} = s_{i, w} - \Sigma_{w} s_{i, w}$ in Fig.\ref{fig:hist} (a). If the hypothesis is right, the variance of $s^{norm}_{i, w}$ will be large.
However, we observe normalized routing score almost tends to zero for all weather, which means there is no preference for the router to deal with different weather. We also visualize the t-SNE of the router's weights for different weather in Fig.\ref{fig:tsne} (a), which also matches the above conclusion because the t-SNE features are mixed together.

So we rethink the router designed in naive MoE. We find in fact it's hard for the original router to select tokens according to weather conditions. The essence of the routing mechanism is to complete the pattern matching and clustering of tokens for each expert. In high-level tasks, different tokens contain abstract semantic features with high similarity~\cite{riquelme2021cv-moe-vit}, resulting in easier clustering. But in the blind weather removal task, the content and weather feature of patch embedding are coupled, making router difficult to select specific experts according to the content or weather information, which limits the assignment flexibility.

We analyze this is because the original router is hard to select tokens according to weather conditions due to the coupled content and weather features (details in supplementary material). In view of the current router's drawback, we propose a Weather-aware Router to explicitly make use of the weather feature. 
% We introduce the weather representation learning branch. 
In the training stage, we employ Weather Guidance Fine-grained Contrastive Learning to learn token-level weather features as the weather representation learning branch, which will be introduced in the next section. 

These token-level weather embeddings could be used as a supplementary assignment basis for the router in the restoration branch. We concatenate weather tokens $\textbf{z}^{l}$ and content tokens $\textbf{y}^{\ell}$ alone the channel dimension to obtain $[\textbf{y}^{\ell}, \textbf{z}^{\ell}] \in \mathbb{R}^{N \times 2D}$, and then utilize a nonlinear adaptor to aggregate the feature. The adaptor's outputs are the final inputs of the router. The formulation can be summarized as follows:

\begin{equation}
    \label{weather_aware_router}
    g (\textbf{y}^{\ell}, \textbf{z}^L) = \text{Softmax} (\text{Top-K} ( \text{Adaptor} ([\textbf{y}^{\ell}, \textbf{z}^{\ell}]) \textbf{W}))
\end{equation}

In this way, WEAR can select experts based on decoupled content and weather features, which is more flexible and leads to better performance. For WEAR, the normalized routing score has a larger variance and t-SNE features are more separated, shown in Fig. \ref{fig:hist} and \ref{fig:tsne} (b) respectfully, demonstrating WEAR's effectiveness. 


\subsection{Weather Guidance Fine-grained Contrastive Learning}


Contrastive Learning (CL) has been explored for learning representation to modulate networks in low-level vision tasks. DASR~\cite{wang2021dasr} proposes unsupervised degradation contrastive learning (UDCL) for blind super-resolution (blind SR). UDCL follows a prior assumption, the low-resolution degradation is the same for the same image, and different for different images, resulting in corresponding instance-level positive and negative samples. UDCL first randomly selects B low-resolution images and randomly crops two patches from each image, with patch embeddings $p_i^1, p_i^2, i=1,\dots B$. UDCL builds a online queue~\cite{he2020moco} to store negative samples $p_{queue}^j, j=1,\dots,N_{queue}$. The formulation is as follows:
\begin{equation}
    \label{udcl}
    L_{udcl} = \sum_{i=1}^{B}-\text{log}\frac{\text{exp} (p^1_{i}\cdot p^2_{i} /\tau)}{\sum_{j=1}^{N_{queue}} \text{exp} (p^1_{i}\cdot p^j_{queue}/\tau)}
\end{equation}
where $\tau \in \mathbb{R}^{+}$ is a scalar temperature parameter.

Different from blind SR, the degradation in blind weather removal shows obvious group characteristics. A real weather image consists of some basic weather elements like rain streaks, raindrops, haze, and snowflows. If we apply UDCL in blind weather, it's easy to get false negative examples. For example, patch embeddings from a rain image will be pushed to those from another rain image. 

To better enable CL to learn blind weather features, motivated by Supervised Contrastive Learning (SCL)~\cite{khosla2020scl}, which uses classification labels to avoid false negative samples, we propose Weather Guidance Fine-grained Contrastive Learning (WGF-CL) (Fig. \ref{fig:WGF-CL}).
Though we can't obtain the weather type in test time, we can still utilize weather labels to help model training. SCL regards different augmented views (global features) of images with the same semantic label as positive samples, while WGF-CL regards all patch embeddings (local features) of images with the same weather as positive samples. The formulation is summarized as follows.

\begin{figure}[!t]
  \centering
  \includegraphics[width=1\linewidth]{Figure/Method/Multi-scale_expert_8.14.pdf}
  \caption{Comparison of ViT~\cite{dosovitskiy2020vit}, PVT~\cite{wang2021pvt}, Shunted Transformer~\cite{ren2022shunted}, MoE~\cite{lepikhin2020gshard} and proposed Weather-aware Multi-scale MoE in terms of FFN module.}
  \label{fig:Weather-aware Multi-scale MoE}
\end{figure}

\begin{figure}[!t]
  \centering
  \includegraphics[width=1\linewidth]{Figure/Method/contrastive_learning.pdf}
  \caption{The illustration of proposed Weather Guidance Fine-grained Contrastive Learning. Patch embeddings from images with the same weather are regarded as positive samples and from different weather are negative samples.}
  \label{fig:WGF-CL}
\end{figure}

\begin{align}
    \label{wgf-cl}
    \textbf{z}^{L}_{i} & = \text{ViT} (\textbf{I}_{\rm adverse}) = [z^L_{i, 1}, \dots, z^L_{i, j}, \dots, z^L_{i, N}] \\
    L_{wgf-cl} & = \sum_{i\in I} \frac{-1}{|P (i)|}\sum_{p\in P (i)}\text{log}\frac{\text{exp} (\textbf{z}^{L}_{i}\cdot \textbf{z}^{L}_{p} /\tau)}{\sum_{a\in A (i)}^{ } \text{exp} (\textbf{z}^{L}_{i}\cdot \textbf{z}^{L}_{a}/\tau)}
\end{align}
where $\textbf{z}_{i}^{L} \in \mathbb{R}^{N\times D}$ is the output of the weather feature encoder, $i\in I\equiv\{1, \dots, B\}$ is the index of minibatch, $A (i)\equiv I \backslash \{i\}$, $P (i)\equiv \{p\in A (i): y_p=y_i\}$ is the positive samples group of the $i$-th anchor sample. 

Compared to UDCL, WGF-CL utilizes weather prior information to guide contrastive learning to select appropriate positive samples and help to capture group-level weather representation.


\begin{figure*}[!t]
  \centering
  \includegraphics[width=0.9\linewidth]{Figure/Experiment/dataset_demo.pdf}
  \caption{Examples of MAW-Sim visualizations. We provide pair of RGB images in 4 weather types. (Best view on screen)}
	\label{fig:vis_CitySpike100K}
\end{figure*}

\input{Table/comparison_all_weather}

\subsection{Multi-scale Experts}

Different weather requires information on different receptive fields to deal with. For example, the occlusion is more serious in heavy rain, so the receptive field should be larger than in light rain for removal. However, the original MoE can only process token-level information due to point-wise FFN. 

To this end, we design Multi-scale Experts (Fig. \ref{fig:Weather-aware Multi-scale MoE}) to process information at different scales. Motivated by Inception~\cite{szegedy2015inception}, we propose grouping experts at different scales. Each expert has a parallel depth-wise convolution with $n\times n$ kernel size, denoted as $\text{DWConv}_{n}$, and different groups have different $n$, where $n \in \{1,3,5,7\}$. The formulations are as follows:

\begin{align}
    \label{multi-scale moe1}
    \textbf{z}^{\ell +1} & = [\sum_{i=1}^{M}g_i (\textbf{y}^{\ell})\times \text{FFN}_{MS, i} (\text{LN} (\textbf{y}^{\ell})) ] + \textbf{y}^{\ell}
\end{align}
\begin{align}
    \label{multi-scale moe2}
    \text{FFN}_{MS, i} (\textbf{x}) & = \text{FC} (\sigma (\text{FC} (\textbf{x}) + \text{DWConv}_{n} (\text{FC} (\textbf{x}))))
\end{align}
Where $\text{FFN}_{MS, i}$ is the $i$-th multi-scale expert, FC is the fully connected layer, and $\sigma$ is GELU. Combined with WEAR, the model can adaptively select experts with appropriate receptive fields to process different weather conditions.

\subsection{Loss Funciton}
We use the smooth-L1 loss and perceptual loss~\cite{johnson2016perceptual} for the restoration branch and weather guidance fine-grained contrastive learning (WGF-CL) loss for the weather representation learning branch. For perceptual loss, we extract features from the $3^{rd}$, $8^{th}$ and $15^{th}$ layers of VGG16~\cite{simonyan2014vgg} pretrained on ImageNet and calculate the MSE of features from the restored image and GT. For WGF-CL loss, we set $\tau=2$. The overall loss can be summarized as follows:
\begin{equation}
    L_{total} = L_{smooth-L1} + \lambda_{1} L_{perceptual} + \lambda_{2} L_{WGF-CL}
\end{equation}
where $\lambda_{1} = 0.04$ and $\lambda_{2} = 0.01$.


% \subsection{Label-free Perception-aware Metric}

% Previous works~\cite{liu2022exploring} measure downstream tasks performance based on reconstructed images. However, there is a problem in this evaluation protocol that it can't be used when labels of test data for downstream tasks are inaccessible, such as segmentation mask and 2D/3D bounding box, which limits the application for many practical scenarios.

% To solve this problem, we propose a novel evaluation method to estimate the downstream performance of restored images, which only requires the corresponding weather-free images instead of semantic labels.~\cite{liu2022exploring} points out optimizing the recognition loss leading to consistent improvement in downstream performance, i.e. the smaller the loss is, the higher the performance is, no matter what recognition architectures, tasks and datasets. So, we can get an intuitive deduction that the loss value is highly related to downstream performance in a variety of situations, which can be considered as a general metric.  

% Motivated by this, we define a Perception-aware Metric $M_{pa}$ by measuring the discrepancy between the restored image and clean image in both semantic output and intermediate feature of a general segmentation model such as Unet or MaskRCNN pretrained on large scale dense prediction datasets like Coco. $M_{pa}$ can be formulated as follow:

% \begin{equation}
%     M_{pa} = l_{dis} (R (P (I_{in})), R (I_{target}))
% \end{equation}

% $M_{pa}$ indicates the semantic similarity between the restored images and ground truth from the model's point of view. The experiments in section 5 show there are high correlations between our Perception-aware Metric $M_{pa}$ and downstream segmentation metrics mIoU and mAcc, while the traditional image quality metrics PSNR and SSIM do not, which demonstrates the effectiveness of $M_{pa}$. 

% The proposed metric greatly alleviates the label dependency when judging whether image processing model's outputs are suitable for downstream task, which has broader prospects in the perception-aware adverse weather removal task in autonomous driving.

% \section{New benchmarks for multiple adverse weather removal} \label{MAW-Sim_intro}
% In order to train our MoWE and implement synthetic datasets for multiple adverse weather removal, we managed to collect and annotate Multiple Adverse Weather (MAW) datasets named MAW-Sim. The new benchmark contains 5 types of weather including rainy, foggy, snowy, mix of them and clear day. Table \ref{tab:Summary_datasets} is a summary of MAW-Sim.
% \newcommand\MyXhlineA[0]{\Xhline{2.5\arrayrulewidth}}
% \renewcommand\arraystretch{1.0}%调行高
% \setlength\tabcolsep{6pt}%调列距
% \begin{table}[ht]
% 	\centering
%         % \small
%         \footnotesize
        
% 	% \resizebox{0.65\columnwidth}{!}{%
% 		\begin{tabular}{|c|c|c|c|}
%             \MyXhlineA
% 			{MAW-Sim}            & Train  & Validation & Test \\ 
%             % \cline{2-7}
%             \MyXhlineA
% 			Rainy      & 0.84K  & 0.12K  & 0.24K\\
% 			Snowy      & 0.84K  & 0.12K  & 0.24K\\
% 		  Foggy      & 0.84K  & 0.12K  & 0.24K\\
% 		  Mix      & 0.84K  & 0.12K  & 0.24K\\
% 		  Clear    & 0.84K  & 0.12K  & 0.24K\\
%             \MyXhlineA
% 			Total    & 4.2K  & 0.12K  & 0.24K\\
%             \MyXhlineA
% 		\end{tabular}
% 	% }
% 	\caption{Summary of the proposed MAW-Sim datasets. We count the number of images in the train, val, and test sets of different weather types in the datasets. Mix represents the mix of the rainy, snowy, foggy weather types. }
% 	\label{tab:Summary_datasets}
% 	% \vspace{-1em}
% \end{table}


% % \subsection{MAW-Sim}
% % \begin{figure}[h]
% % 	\centering
% %     \includegraphics[width=1\linewidth]{Figure/Method/vis_CitySpike100K.png}
% % 	\caption{Examples of MAW-Sim visualizations. We provide pair RGB images in 4 weather types. (Best view on screen)}
% % 	\label{fig:vis_CitySpike100K}
% % \end{figure}
% We propose MAW-Sim as a large synthetic deweather benchmark for spike data with 50 scenes. The dataset is generated in Unity3D engine, in which we build a large city traffic system and a weather generation system. To make the simulation environment more realistic, we set the path of the vehicle movement and the time to change the traffic signals. For each scene, we equip a car with cameras to record automatically. We provide the images rendered at 30fps with a bit of natural motion blur. Each scene contains 5 different kinds of weather, including rain, snow, fog, and a mix of them as model input and clear day as ground truth. Each scene has 30 frames of RGB images with a resolution of 1024×768. For training, evaluation, and testing, we further divide the dataset according to the ratio of 7:1:2. 
% % In supplementary material, we show several example pairs of MAW-Sim in a scene, including the ground truth, RGB images under rainy, snowy, and foggy sky.
% % As shown in Figure \ref{fig:vis_CitySpike100K}, we show an example pair of MAW-Sim in a scene, including the ground truth, RGB images under rainy, snowy, and foggy sky.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Experiments}
In this section, we evaluate our work on MAW-Sim, All-Weather, and Cityscapes datasets and compare it with several SOTA methods. We also provide the performance comparison of downstream segmentation tasks in Cityscapes. Moreover, we implement an ablation study to demonstrate the effectiveness of each component of WM-MoE.

\subsection{Experiment Setting}
\textbf{MAW-Sim.} In order to train models for blind adverse weather removal, we collect and annotate a simulated dataset with multiple adverse weathers named \textit{MAW-Sim}. It's generated based on the Unity3D engine with a large city traffic system and a weather generation system. The images are recorded with a virtual camera on cars at 30fps. The dataset has 30 scenes. Each scene has 5 different cases, including clear day as ground truth, rain, snow, fog, and a random mix of them. Each scene has 30 frames with 1024$\times$768 resolution. We further divide the dataset for training, validation, and testing according to the ratio of 7:1:2.

\textbf{Dataset.} We evaluate WM-MoE and state-of-the-art methods on MAW-Sim, All-weather, and Cityscapes datasets.
% with 4, 3, and 2 different weather types including rain, haze, snow, and a combination of them. 
% MAW-Sim has been described in Section \ref{MAW-Sim_intro}. 
All-weather~\cite{valanarasu2022transweather} contains data from different public datasets. The training set consists of Outdoor-Rain~\cite{li2019outdoor-rain}, Raindrop~\cite{qian2018raindrop} and Snow100K~\cite{liu2018snow100k}. The test set is sampled from Outdoor-Rain~\cite{li2019outdoor-rain}, the Raindrop test set~\cite{qian2018raindrop}, and the Snow 100k-L test set~\cite{liu2018snow100k}. Cityscapes~\cite{cordts2016cityscapes} is collected from various scenarios of outdoor street scenes in different cities. These images are recorded in normal weather conditions by video cameras mounted on cars. Foggy Cityscapes~\cite{sakaridis2018haze_cityscapes} and Rain Cityscapes~\cite{hu2019rain_cityscapes} are synthesized from the images in the Cityscapes dataset. We further combine the train sets of them for training. We also adopt the fine segmentation label of Cityscapes for downstream tasks evaluation.


\newcommand\MyXhlineG[0]{\Xhline{2.5\arrayrulewidth}}
\renewcommand\arraystretch{1.0}%调行高
\setlength\tabcolsep{6pt}%调列距
\begin{table}[!ht]
	\centering
        % \small
        % \footnotesize
        
	% \resizebox{0.65\columnwidth}{!}{%
	\caption{Summary of the proposed MAW-Sim datasets. We count the number of images in the train, val, and test sets of different weather types in the datasets. Mix represents the mix of the rainy, snowy, and foggy weather types. }
		\begin{tabular}{|c|c|c|c|}
            \MyXhlineG
			{MAW-Sim}            & Train  & Validation & Test \\ 
            % \cline{2-7}
            \MyXhlineG
			Rainy      & 0.84K  & 0.12K  & 0.24K\\
			Snowy      & 0.84K  & 0.12K  & 0.24K\\
		  Foggy      & 0.84K  & 0.12K  & 0.24K\\
		  Mix      & 0.84K  & 0.12K  & 0.24K\\
		  Clear    & 0.84K  & 0.12K  & 0.24K\\
            \MyXhlineG
			Total    & 4.2K  & 0.12K  & 0.24K\\
            \MyXhlineG
		\end{tabular}
	% }
	\label{tab:Summary_datasets}
	% \vspace{-1em}
\end{table}

\input{Table/comparison_maw_sim}

\input{Table/comparison_cityscapes}

\textbf{Implementation Details.}
WM-MoE is trained for 200 epochs with 32 batch sizes. We adopt the AdamW optimizer and Cosine scheduler with initial learning rates $2e-4$ gradually reduced to $2e-6$ and the warm-up strategy for 3 epochs. We randomly crop images to 256 × 256 for training and apply a non-overlap crop for the same patch size in the test. For data augmentation, we use random flips and rotations. 

\begin{figure*}[ht]
  \centering
  \includegraphics[width=0.92\linewidth]{Figure/Experiment/vis_allweather_V2.pdf}
  \caption{Qualitative results to visualize the deweather performance on All-Weather.}
  \label{fig:vis_allweather}
\end{figure*}

\begin{figure*}[!t]
	\centering
	\includegraphics[width=0.95\linewidth]{Figure/Experiment/vis_cityscapes_V3.pdf}
	\caption{Qualitative results of the deweather performance and downstream semantic segmentation on Cityscapes.}
	\label{fig:vis_cityscapes}
\end{figure*}

\begin{figure*}[ht]
  \centering
  \includegraphics[width=0.8\linewidth]{Figure/Experiment/mawsim_upstream_V2.pdf}
  \caption{Qualitative results to visualize the deweather performance on MAW-Sim.}
  \label{fig:vis_mawsim_upstream}
\end{figure*}

\subsection{Network Details} \label{network details}
\textbf{Task-shared Head.}
We adopt a similar head setting as IPT~\cite{chen2021ipt}, which consists of one convolution layer and two residual blocks. The former parameters are $3 \times 3$ kernel size, $3$ input channels, and $32$ output channels. The latter block consists of a shortcut and parallel two convolution layers with $3 \times 3$ kernel size, $32$ input channels, and $32$ output channels. The spatial resolution of feature maps remains unchanged in the process.

\textbf{Transformer Encoder.}
We first obtain the patch embedding from feature maps by a non-overlap convolution projection and flatten operation. We set the dimension $D$ to $256$, $384$ and $256$ for MAW-Sim, Allweather~\cite{valanarasu2022transweather} and Cityscapes~\cite{hu2019rain_cityscapes, sakaridis2018haze_cityscapes} respectfully. The resultant sequence $[ B, N, D]$ is the input of Transformer, where $B$ is the batch size, $N$ is the number of patches and $D$ is the embedding dimension.

For the restoration branch, the number of Transformer blocks is $2$. In each block, standard multi-head self-attention module with $8$ heads is adopted. The weather-aware token-level router consists of one linear layer and softmax function with $2 \times D$ and $E$ as input and output channels, where $E$ denotes the number of experts. Before getting into the router, we adopt an adaptor with a two-layer MLP to process the concatenation of the content and weather token embeddings. $E$ multi-scale experts are employed and divided into 4 groups, with $1\times 1$, $3\times 3$, $5\times5 $ and $7\times 7$ for their bypass depth-wise convolution layer respectfully. To reduce the parameters and flops, the hidden layer dimension of each expert is set to $2D$ instead of commonly used $4D$~\cite{dosovitskiy2020vit, fedus2021switch}.

For the weather representation captured branch, we use ViT~\cite{dosovitskiy2020vit} with $2$ blocks, including multi-head self-attention module, and feed-forward network. All settings are the same as the restoration branch. 

\input{Table/ablation}

\input{Table/ablation3}

\input{Table/ablation2}


\textbf{Task-shared Tail.}
The task-shared tail consists of $4$ sequential blocks to adjust the channels of feature maps reconstructed from the Transformer output. The residual block and convolution layer are used twice alternately. The input channels are $128$ and the output channels of each layer are $128, 64, 64, 3$ respectively. $3 \times 3$ kernel size is adopted for all of them. In the end, We utilize the Sigmoid function to normalize the final output in the range of 0 to 1.

\subsection{Comparison with State-of-the-art Methods} \label{formal version}

\textbf{Baselines.} We compare our WM-MoE with task-specific, task-agnostic, and multi-task-in-one methods. Task-specific group includes derain (RESCAN~\cite{li2018recurrent} and PReNet~\cite{ren2019progressive}), dehaze (GridDehazeNet~\cite{liu2019griddehazenet}, MSBDN-DFF~\cite{dong2020multi}, FFA-Net~\cite{qin2020ffa}, and AECR-Net~\cite{wu2021contrastive}), and desnow (DesnowNet~\cite{liu2018desnownet} and HDCWNet~\cite{chen2021all}). Task-agnostic group includes MPR~\cite{zamir2021multi}, and Restormer~\cite{zamir2022restormer}. The multi-task-in-one group includes Transweather~\cite{valanarasu2022transweather} and Unified Model~\cite{chen2022learning}. We retrain the models using open-source code. For downstream tasks, we implement experiments of the lower and upper bound, which take weather and clean images as downstream input respectively. 

\textbf{Metrics.} For the quantitative evaluation of blind weather removal, we adopt PSNR and SSIM as metrics. We calculate the metrics in RGB space. For downstream semantic segmentation tasks, we take mIoU, and mAcc as our metrics. 


\begin{figure*}[!t]
  \centering
  \includegraphics[width=0.65\linewidth]{Figure/Experiment/mawsim_downstream_V2.pdf}
  \caption{Qualitative results to visualize the downstream semantic segmentation performance on MAW-Sim.}
  \label{fig:vis_mawsim_downstream}
\end{figure*}

\subsubsection{Quantitative Comparison and Analysis} \ \ \ \ \

\textbf{Main results.} We report several weather and the average results in Tables \ref{real_results}, \ref{synthetic_results}, and \ref{cityscapes_results}. It can be noted that our method achieves SOTA performances on both MAW-Sim and All-weather. WM-MoE also obtains competitive results on Cityscapes. For task-specific methods designed for specific deweather tasks, WM-MoE outperforms all of them by a significant margin. Compared with task-agnostic methods, our method also achieves more promising results, especially in mixed weather. In addition, our method still performs better than another multi-task in one work, regardless of scenario and weather conditions. Thanks to the Weather-aware Router and Multi-scale Experts, the model enhances the capacity to process complex weather conditions and exploit multi-scale features, which handles blind weather better. 


\textbf{Downstream results.} As for downstream results in Tables \ref{cityscapes_results}, it can be seen that weather removal benefits segmentation tasks. WM-MoE achieves competitive results on deweather tasks while helping the downstream model obtain promising results, which is close to the upper bound performance. 

% \begin{figure*}[h]
% 	\centering
% 	\includegraphics[width=0.75\linewidth]{Figure/Experiment/vis_city.pdf} 
% 	\caption{Qualitative results to visualize the deweather performance on MAW-Sim. We provide the RGB images of model input and predictions. The deweather types from top to bottom are desnowing, draining, dehazing and combination of them.}
%     \label{fig:vis_city}
% \end{figure*}

% \begin{figure*}[h]
% 	\centering
% 	\includegraphics[width=0.75\linewidth]{Figure/Experiment/vis_real.pdf} 
% 	\caption{Qualitative results to visualize the deweather performance on MAW-Real. We provide the RGB images of model input and predictions. The deweather types from top to bottom are draining, dehazing and desnowing.}
%     \label{fig:vis_real}
% \end{figure*}

%\subsubsection{Visual Qualitative Comparison and Analysis}
%We present the predictions of All-Weather and Cityscapes in Figure \ref{fig:vis_allweather} and \ref{fig:vis_cityscapes} respectively. Due to the space limitation, visualization on MAW-Sim can be seen in the supplementary material. Compared to other baselines, our method achieves more visually pleasing results on different deweather tasks. The images restored by WM-MoE are close to ground truth, while other methods fail to deal with the multiple weather and thus result in obvious artifacts, which is shown in Figure \ref{fig:vis_allweather} and \ref{fig:vis_cityscapes}. As for the segmentation task on Cityscapes in Figure \ref{fig:vis_cityscapes}, it can be seen that the images restored by WM-MoE help the downstream model to obtain more accurate segmentation masks, improve the segmentation quality of distant and small objects, and reduce the missed and false detection caused by adverse weather. which demonstrates WM-MoE's benefit for downstream tasks.

\subsection{Ablation Study}

% \newcommand\MyXhlineD[0]{\Xhline{2.5\arrayrulewidth}}
% \renewcommand\arraystretch{1.0}%调行高
% \setlength\tabcolsep{8pt}%调列距
% \begin{table}[h]
%   \centering
%   % \small
%   \footnotesize
  
%   \resizebox{0.65\columnwidth}{!}{
%       \begin{tabular}{ccccc}
%         \toprule
%         % Vision Transformer +                            &\multirow{2}*{TMoE}   &Subpatch   &Subpatch     &Channel-     &\multirow{2}*{PSNR ($\uparrow$)}  &\multirow{2}*{SSIM ($\uparrow$)} \\
%         % Task-shared head \& tail      &~                    & (R=0.5)                         & (R=0.25)   & Attention    &~                                 &~                       \\
%         TMoE & SSM  & CA & PSNR  & SSIM \\
        
%         \midrule
%                     &             &          &27.01           &0.8493           \\
%         \checkmark  &             &             &27.38           &0.8608             \\
%         \checkmark  & \checkmark        &             &28.68           &0.8883  \\
%         \checkmark  & \checkmark       &\checkmark   &28.86           &0.8919           \\
        
%         \bottomrule
%       \end{tabular}
%   }
%   \caption{Ablation study on different components.}
%   \label{ablation}
% \end{table}

 \textbf{Ablations on proposed modules}. 
We evaluate the contribution of each component to our method (Table~\ref{tab:ablation}). The baseline is a vision Transformer with task-shared convolution head and tail. First, we replace the original FFN with naive MoE. Then we verify the WEAR to replace the linear layer router and the ME to replace point-wise FFN experts, respectively. Here we present two parameters of the number of experts $n$ and TopK gate $k$. More ablations about $n$ and $k$ are in the supplementary material. All the ablations are conducted in MAW-Sim. The training settings are the same as Table~\ref{synthetic_results}. 
% The metrics are average PSNR and SSIM in four weather types. 
It's been shown that every design in WM-MoE could lead to a performance gain.  

\textbf{Ablations on representation learning methods.}
We also compare different representation learning methods (Table~\ref{tab:ablation2}). WGF-CL achieves the best performance. It's worth noting that the result of UDCL drops obviously, proving the hypothesis that instance-level CL against group characteristics in blind weather results in false negative samples.

\begin{figure}[!ht]
  \centering
  \includegraphics[width=1.0\linewidth]{Figure/Experiment/MoE_Ablation_New.pdf}
  \caption{Ablation study on the proposed Weather-aware Multi-scale Mixture-of-Experts Module. The same results with Table \ref{tab:ablationKN}}
  \label{fig:moe_ablation}
\end{figure}


\textbf{Ablations on the number of experts and top-k gate.}
We present more ablation experiments on different N \& K in the MAW-Sim dataset with the same training setting as the main experiments. The concrete value of PSNR and SSIM is in Table \ref{tab:ablationKN} and the corresponding line chart is in Figure \ref{fig:moe_ablation}.

From the Table \ref{tab:ablationKN} and Figure \ref{fig:moe_ablation}, we can summarize the following conclusion:

 (1) The proposed MoE module can improve the baseline consistently, whatever N or K is.

 (2) When K is fixed, the performance is guaranteed to go up with N increasing from 4 to 32, and the PSNR-N growth curve is close to a logistics curve like the formulation of PSNR, which demonstrates the scaling ability of MoE. Due to limited computing resources, we don't explore more experts, but we will take it as the direction for future improvements.

 (3) For K=1, the performance increase unsteadily. We think this is because smaller K and larger N make each expert get fewer tokens to obtain strong capacity when the trainset and training time are limited.

 (3) When N is fixed, larger K brings about better performance while more computation at the same time because K controls the sparsity of MoE module~\cite{fedus2021switch, shazeer2017lstm-moe}.

After considering the overall performance and efficiency, we choose N = 16 \& K = 4 as the final version. 

\section{Conclusion}
In this work, we study the blind weather removal problem. To better release the potential of MoE in the setting, we propose a novel Weather-aware Multi-scale MoE (WM-MoE), which consists of the Weather-aware Router (WEAR) to assign the correct weather experts to tokens and Multi-scale Experts (ME) to improve the spatial modeling capability. We also propose Weather Guidance Fine-grained Contrastive Learning (WGF-CL) to decouple the weather and content information from the input image. Our method achieves SOTA performance in MAW-Sim, All-Weather, and competitive results in Cityscapes. Experiments also demonstrate that WM-MoE is beneficial to the downstream segmentation task.


\bibliographystyle{IEEEtran}
\bibliography{WM-MoE}

% \newpage

% \section{Biography Section}
% If you have an EPS/PDF photo (graphicx package needed), extra braces are
%  needed around the contents of the optional argument to biography to prevent
%  the LaTeX parser from getting confused when it sees the complicated
%  $\backslash${\tt{includegraphics}} command within an optional argument. (You can create
%  your own custom macro containing the $\backslash${\tt{includegraphics}} command to make things
%  simpler here.)
 
% \vspace{11pt}

% \bf{If you include a photo:}\vspace{-33pt}
% \begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{fig1}}]{Michael Shell}
% Use $\backslash${\tt{begin\{IEEEbiography\}}} and then for the 1st argument use $\backslash${\tt{includegraphics}} to declare and link the author photo.
% Use the author name as the 3rd argument followed by the biography text.
% \end{IEEEbiography}

% \vspace{11pt}

% \bf{If you will not include a photo:}\vspace{-33pt}
% \begin{IEEEbiographynophoto}{John Doe}
% Use $\backslash${\tt{begin\{IEEEbiographynophoto\}}} and the author name as the argument followed by the biography text.
% \end{IEEEbiographynophoto}




\vfill

\end{document}


