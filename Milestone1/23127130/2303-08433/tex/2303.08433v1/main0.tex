\documentclass[11pt]{article}
\pdfoutput=1

\usepackage{mathrsfs}
\usepackage{amsmath,amssymb}
\usepackage{bm}
\usepackage{natbib}
\usepackage[usenames]{color}
\usepackage{amsthm}

\usepackage{multirow} 
\usepackage{enumitem}

\newcommand\dd{\mathrm{d}}
\DeclareMathOperator{\sech}{sech}
\DeclareMathOperator{\csch}{csch}
\DeclareMathOperator{\arcsec}{arcsec}
\DeclareMathOperator{\arccot}{arcCot}
\DeclareMathOperator{\arccsc}{arcCsc}
\DeclareMathOperator{\arccosh}{arcCosh}
\DeclareMathOperator{\arcsinh}{arcsinh}
\DeclareMathOperator{\arctanh}{arctanh}
\DeclareMathOperator{\arcsech}{arcsech}
\DeclareMathOperator{\arccsch}{arcCsch}
\DeclareMathOperator{\arccoth}{arcCoth} 

\allowdisplaybreaks

\usepackage[colorlinks,
linkcolor=red,
anchorcolor=blue,
citecolor=blue
]{hyperref}

\renewcommand{\baselinestretch}{1.05}

\def \dd {\rm{d}}



\usepackage{mylatexstyle}


\usepackage{setspace}
%\setstretch{1.5}
\usepackage[left=1in, right=1in, top=1in, bottom=1in]{geometry}

%\usepackage{xcolor}
%\newcommand{\bin}[1]{\textcolor{blue}{[Bin: #1]}}

\usepackage[textsize=tiny]{todonotes}
\setlength{\marginparwidth}{0.8in}
\newcommand{\todoy}[2][]{\todo[size=\scriptsize,color=blue!20!white,#1]{Yuan: #2}}
\newcommand{\todoq}[2][]{\todo[size=\scriptsize,color=orange!20!white,#1]{Quanquan: #2}}





\title{\huge The Benefits of Mixup for Feature Learning}
% \title{\huge Over-parameterized Logistic Regression for Sub-Gaussian Mixtures: Risk Bounds and Benign Overfitting}

% \title{\huge Over-parameterized Logistic Regression for Sub-Gaussian Mixtures: Risk Bounds and Benign Overfitting}

%\title{\huge Is Polylogarithmic Width Over-parameterization Sufficient to Learn Deep ReLU Networks?}
\author
{
    Difan Zou\thanks{Department of Computer Science and Institute of Data Science, The University of Hong Kong, Hong Kong; e-mail:  {\tt dzou@cs.hku.hk}}
    ~~~and~~~
	Yuan Cao\thanks{Department of Statistics and Actuarial Science and Department of Mathematics, The University of Hong Kong, Hong Kong; e-mail:  {\tt yuancao@hku.hk}} 
	 ~~~and~~~
	Yuanzhi Li\thanks{Machine Learning Department, Carnegie Mellon University, Pittsburgh, PA, USA; e-mail: {\tt yuanzhil@andrew.cmu.edu}} 
	~~~and~~~
	Quanquan Gu\thanks{Department of Computer Science, University of California, Los Angeles, CA, USA; e-mail: {\tt qgu@cs.ucla.edu}}
}
\date{}


\date{}
%\date{Feb XXX, 2019\\ Version 3\footnote{V2 improves the sample complexity result so that it almost does not depend on the number of nodes per layer (only has a logarithmic dependence). This version relaxes the assumption on the data distribution, so the generalization theory of GD is now applicable to an even larger class of data distributions.}}
%\begin{titlepage}
%Version 2\thanks{XXX}
%\end{titlepage}
%\being{center}
%{Version 2}\thanks{XXX}
%\end{center}
%\date{February 1, 2019}
%\maketitle






\def\supp{\mathrm{supp}}
\def\rmI{\mathrm{I}}
\def\Tr{\mathrm{Tr}}
\def\poly{\mathrm{poly}}
\def\Diag{\mathrm{Diag}}
\def\bbvec{\mathrm{\mathbf{b}}}
\def\cb{\mathrm{\mathbf{c}}}
\def\polylog{\mathrm{polylog}}
\def\logit{\mathrm{Logit}}


\newtheorem{hypothesis}{Hypothesis}

\def\cN{\mathcal{N}}

\def\dd{\mathrm{d}}

\def\BN{\mathrm{BN}}

\newcommand{\la}{\langle}
\newcommand{\ra}{\rangle}
\def\CC{\textcolor{red}}

\def\CCC{\textcolor{red}}

% \def\CC{}





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\def\replacewithrealradius{\tau_{\mathrm{real}}}






%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%










\begin{document}

\maketitle




\begin{abstract}
Mixup, a simple data augmentation method that randomly mixes two data points via linear interpolation, has been extensively applied in various deep learning applications to gain better generalization. However, the theoretical underpinnings of its efficacy are not yet fully understood. In this paper, we aim to seek a fundamental understanding of the benefits of Mixup. We first show that Mixup using different linear interpolation parameters for features and labels can still achieve similar performance to the standard Mixup. This indicates that the intuitive linearity explanation in \citet{zhang2018mixup} may not fully explain the success of Mixup. Then we perform a theoretical study of Mixup from the feature learning perspective. We consider a feature-noise data model and show that Mixup training can effectively learn the rare features (appearing in a small fraction of data) from its mixture with the common features (appearing in a large fraction of data). In contrast, standard training can only learn the common features but fails to learn the rare features, thus suffering from bad generalization performance. Moreover, our theoretical analysis also shows that the benefits of Mixup for feature learning are mostly gained in the early training phase, based on which we propose to apply early stopping  in Mixup. Experimental results verify our theoretical findings and demonstrate the effectiveness of the early-stopped Mixup training.

% Mixup, a simple data augmentation method that randomly mixes two data points via linear interpolation, has been extensively applied in various deep learning applications to gain better generalization. However, its theoretical explanation remains largely unclear. In this work, we aim to seek a fundamental understanding towards the benefits of Mixup. We first show that applying \textit{the same} linear interpolation for features and labels is not necessary, arguing that the intuitive linearity explanation in \citet{zhang2018mixup} may not be appropriate. Then, we perform a theoretical study of Mixup from the feature learning perspective. We consider a feature-noise data model and show that Mixup training can effectively learn the rare features (appearing in a small fraction of data) from its mixture with the common features (appearing in a large fraction of data). In contrast, standard training can only learn the common features but memorizes the noise components of the rare feature data points. Moreover, our theoretical analysis also shows that the benefit of feature learning is mostly gained in the early training phase, based on which we propose to apply early stopping  in Mixup. Experimental results verify our theoretical findings and demonstrate the effectiveness of the early-stopped Mixup training.

% It actively  applies \textit{the same} linear interpolations between the features and labels of a random pair of training data points, 



% It is commonly conjectured that the benefit of Mixup stems from its 
% inductive bias to encourage linearity during model training, since it actively applies \textit{the same} linear interpolations between the features and labels of a pair of training data points.


% A common conjecture states that the benefit It is commonly conjectured that the benefit of Mixup stems from its inductive bias to encourage linearity during model training, since it actively applies \textit{the same} linear interpolations between the features and labels of a pair of training data points.


% In this work, we first show that a commom


% investigate the dynamics of Mixup from a feature learning perspective.

%  However, 



%  It is commonly conjectured that the benefit of Mixup is closely tied to its ability to e

 





% a multi-view  data distribution that consists of multiple features and noise. In particular, we formally characterize the difference between Mixup training and standard training in their learning dynamics and demonstrate the benefit of Mixup for learning the rare features. We show that at the core 



\end{abstract}


\section{Introduction}


The Mixup method \citep{zhang2018mixup} is a popular data augmentation technique in deep learning, known to yield notable improvements in generalization and robustness across multiple domains, such as image recognition \citep{berthelot2019mixmatch}, natural language processing \citep{guo2019augmenting,chen2020mixtext}, and graph learning \citep{Han2022mixupgraph}. Unlike traditional data augmentation approaches that require domain knowledge of the dataset (e.g., random rotation and cropping for image data, and randomly modifying edges for graph data), Mixup relies on convex combinations of both features and labels from a pair of randomly selected training data points. As a result, this technique does not require any specialized knowledge or expertise to be performed.

Despite the remarkable empirical success of Mixup, there is a considerable gap in the theoretical understanding of this technique.  In the original work of Mixup \citep{zhang2018mixup}, it has been argued that the efficacy of Mixup can be attributed to its inductive bias, which encourages the trained model to behave linearly, leading to (relatively) simple decision boundaries. This inductive bias has been further supported by a series of works \citep{guo2019Mixup,zhang2020does,zhang2022and,chidambaram2021towards}, which prove that the Mixup behaves similarly to standard training for linear models.
In particular, Mixup applies the same linear interpolation on the features and labels of a pair of training data points $(\xb_1,y_1)$ and $(\xb_2,y_2)$: denoted by $\lambda\xb_1+(1-\lambda)\xb_2$ and labels $\lambda y_1 + (1-\lambda)y_2$, where $\lambda\in[0.5, 1]$ is randomly chosen. Then, the trained neural network (NN) model $F$ is naturally encouraged to conduct the mapping $F(\lambda\xb_1+(1-\lambda)\xb_2) \rightarrow \lambda y_1 + (1-\lambda)y_2$ for all $\lambda\in[0.5 ,1]$, $(\xb_1,y_1)$ and $(\xb_2,y_2)$, implying that $F$ tends to behave linearly at least within the line segments between all training data pairs. 

Although linearity is a nice inductive bias that tends to learn the models with low complexities, we are not clear about whether such an intuition from the algorithm design (i.e., performing the same linear interpolation for features and labels) can indeed explain the improvement in generalization.
To examine this, we conduct a proof-of-concept experiment on CIFAR-10 dataset. Instead of using the same linear interpolation in the feature and label space, we implement the interpolations using different $\lambda$'s for features and labels, i.e., we implement the  Mixup data augmentation on the features and labels as: $\lambda\xb_1+(1-\lambda)\xb_2$ and $g(\lambda)y_1 + [1-g(\lambda)]y_2$ for some nonlinear or even random function $g(\cdot):\RR^{[0.5, 1]}\rightarrow \RR^{[0.5,1]}$. Our results, shown in Figure \ref{fig:linearity_negative}, demonstrate that the substantial performance gain of Mixup training over standard training does not require $g(\lambda)=\lambda$. Other choices, such as fixed or independently random $\lambda$ and $g(\lambda)$, can lead to comparable or even better performance.

\begin{figure}[!t]
\vskip -0.1in
     \centering
     \subfigure[ResNet18]{\includegraphics[width=0.49\columnwidth]{figure/different_lambda_resnet_noaug.pdf}}
      \subfigure[VGG16]{\includegraphics[width=0.49\columnwidth]{figure/different_lambda_vgg_noaug.pdf}}
      % \vskip -0.1in
    \caption{Test accuracy achieved by Mixup training with different configurations of $\lambda$ and $g(\lambda)$. The results are evaluated by training ResNet18 and VGG16 on CIFAR-10 dataset without random crop \& flip data augmentation and weight decay regularization. We consider $5$ different configurations: (1) $\lambda=g(\lambda)=1$, i.e., standard training; (2) $\lambda = g(\lambda)\sim U[0.5, 1]$, i.e., standard Mixup; (3) $\lambda\sim U[0.5,1]$ and $g(\lambda)=1.5-\lambda$; (4) $\lambda\sim U[0.5,1]$ and $g(\lambda)\sim U[0.5,1]$; (5) $\lambda=0.7$ and $g(\lambda)=0.8$. It is clear that the performance gain of Mixup does not require setting $g(\lambda)=\lambda$. }
    \vspace{-2mm}\label{fig:linearity_negative}
\end{figure}
Therefore, it demands seeking a more fundamental understanding of Mixup that is beyond the linearization illustration. To address this issue, we draw inspiration from a recent work \citep{shen2022data}, which regards standard image data augmentation as a form of feature manipulation. This perspective offers a general framework to investigate the behavior of various data augmentation techniques, including Mixup in deep learning.  In particular, they consider a multi-view data model that consists of multiple feature vectors and noise vectors with different strengths and frequencies. More specifically, the feature vectors are categorized as the common ones (i.e., ``easy to learn'' features) and the rare ones (i.e., ``hard to learn'' features): the former refers to the feature appearing in a large fraction of data (thus contribute a lot to the gradient updates), and the latter refers to the features occurring in a small fraction of data (thus have limited contribution to the gradient). They further assume that the common features are the ones with rare orientations compared to the rare features and they can be balanced by applying data augmentations. For example, the common feature of a cow could be the left-facing cow, while the rare feature could be the right-facing cow, which can be generated by applying a horizontal flip to the common feature.

However, in many cases, the common and rare features may not be easily balanced by standard data augmentations. Let's still take the cow image as an example, the common and rare features could be brown cows and black cows, or front-view cows and side-view cows. Then the standard rotation or flip operations clearly cannot convert the common features to rare ones. We conjecture that Mixup may exhibit certain benefits in tackling this type of feature, as it has been shown to improve test accuracy when combined with standard data augmentations \citep{zhang2018mixup}. This motivates the problem setup considered in this study.

Particularly, we perform the theoretical study of the learning dynamics of Mixup based on a similar multi-view data model (see Definition \ref{def:data_distribution_new} for more details): each data point will either contain a common feature vector with a relatively high probability $1-\rho$, or a rare feature vector  with a relatively low probability $\rho$. The remaining components will be filled with random noise or feature noise. We then consider a two-layer convolutional neural network (CNN) model and study the learning behaviors of both standard training and Mixup training using gradient descent. The main contributions of this paper are highlighted as follows:

\begin{itemize}[leftmargin=*]
\item We identify that the linearity illustration may not be able to fully elucidate the exceptional performance of Mixup. In particular, we  show that using the same linear interpolations for both features and labels is not necessary, while some other choices, e.g., independently random linear interpolations, can also lead to substantial performance gains compared to standard training.

\item We prove a negative result (Theorem \ref{thm:std_training}) for standard training, demonstrating its inability to learn the rare features of the multi-view distribution. This failure leads to the domination of the rare feature data by its noise components during the test period, resulting in a $\Theta(\rho)$ test error. The reason for this lies in the tendency of the standard training algorithm to memorize the noise component of rare feature data to attain zero training error, while the rare feature itself, which appears in only a small fraction of the data, is not prominent enough to be effectively discovered by the algorithm.

\item More importantly, we establish a positive result (Theorem \ref{thm:Mixup_training}) for Mixup training by showcasing its ability to attain near-zero test errors on the multi-view distribution. Specifically, we demonstrate that Mixup can successfully mix the common and rare features so that the gradients along these two
features are correlated. As a result, the rare feature learning can be boosted by the fast learning
of common features, and ultimately reaches a sufficiently high level to overshadow the effects of noise on test data.
% such that the overfitting of the random noise vectors is benign, i.e., has a negligible effect on the test data. 

\item Our theory also suggests that the feature learning (especially the rare feature) benefits of Mixup are mostly gained in the early training phase. Then we develop the early-stopped Mixup, i.e., turning off the Mixup data augmentation after a certain number of iterations. Experimental results show that the test error achieved by early-stopped Mixup is comparable to or even better than that achieved by standard Mixup (i.e., using Mixup throughout the entire training). This not only corroborates our theoretical findings but also justifies the necessity to study the entire feature learning dynamics of Mixup rather than only the solution to the (equivalent) empirical risk of Mixup.

\end{itemize}

\paragraph{Notations.} We use $\poly(n)$ and $\polylog(n)$ to denote a polynomial function, with a sufficiently large (constant) degree, of $n$ or $\log(n)$ respectively. We use $o(1/\polylog(n))$ (and $\omega(\polylog(n))$) to denote some quantities that decrease (or grow) faster than $1/\log^c(n)$ (or $\log^c(n)$) for any constant $c$. We use $\tilde O$, $\tilde \Omega$, and $\tilde\Theta$ to hide some log factors in the standard Big-O, Big-Omega, and Big-Theta notations.




% Following the similar high-level spirit, a very recent work \citep{chidambaram2022provably} has attempted to  study Mixup from a feature learning perspective, by showing whether Mixup has certain benefit in discovering features from noisy data. However, their results are only established on a special version of Mixup called midpoint Mixup (i.e., fixing $\lambda=0.5$) on a technically-designed data model, thus cannot fully explain the behavior of Mixup. 



% Data augmentation has been demonstrated to be a powerful method in training modern deep learning models.  It is performed by adding random perturbations on the input data, thus enlarges the size and diversity of the training dataset. more





\section{Related Work}
\paragraph{Theoretical Analysis of Mixup.}
We would like to comment on some recent works that attempt to explain the benefits of Mixup from different angles. To name a few, \citet{thulasidasan2019mixup} showed that the models trained by Mixup are substantially better calibrated, i.e., the softmax logits are closer to the actual likelihood than that obtained by standard training. \citet{carratino2020mixup} studied the regularization effect of Mixup training and connected it to multiple known data-dependent regularization
schemes such as label smoothing. Following 
the same direction, \citet{parkunified} further developed a unified analysis for a class of Mixup methods, including the original one and CutMix \citep{yun2019cutmix}, and proposed a hybrid version of Mixup that achieves better test performance. \citet{chidambaram2021towards} studied the Mixup-optimal classifier and characterized its performance on original training data points. However, these works mostly focus on the solution to certain Mixup-version regularized empirical risk, while our experiments on early-stopped Mixup suggest that the entire learning dynamics could be more important.

Very recently, \citet{chidambaram2022provably} conducted feature learning-based analyses for Mixup and demonstrated its benefits. However, we would like to clarify some differences in our theoretical analysis. Firstly, in terms of the Mixup method, they considered only the mid-point Mixup, where $\lambda=g(\lambda)=0.5$, while we allow a more general choice of $\lambda\in(0.5, 1)$. Secondly, for the data model, they considered two features generated from a symmetric distribution for each class, along with feature noise, whereas we followed \citet{shen2022data} by considering a data model with two features of different frequencies (common and rare), feature noise, and random noise. Notably, the random noise component, which plays an important role in memorizing all training data points \citep{allen2020towards,shen2022data}, was ignored in \citet{chidambaram2022provably}. Finally,  their focus was on the competence between learning two symmetric features, while our focus was on the competence between rare feature learning and noise memorization. In conclusion, while \citet{chidambaram2022provably} and our work share a similar high-level spirit for understanding the benefits of Mixup, we approach this problem from different angles. 


\paragraph{Data Augmentation.}
There are also many works studying the effect of standard data augmentation methods (i.e., performed within the data points) from different perspectives, such as regularization effect \citep{bishop1995training,dao2019kernel,pmlr-v119-wu20g}, algorithm bias \citep{hanin2021data},  margins \citep{rajput2019does}, model invariance \citep{chen2020group}, and feature learning \citep{shen2022data}. We view these works as orthogonal to our work as they mostly concern the data augmentation within the data points (e.g., random perturbation, random rotation, etc), which is different from the cross-data Mixup data augmentation.

\paragraph{Feature Learning in Deep Learning Theory.}
In the field of deep learning theory, there has emerged a series of works studying feature learning behavior during NN training. They focus on characterizing how different training approaches affect feature learning, such as ensembling \& knowledge distillation \citep{allen2020towards}, using adaptive gradients \citep{zou2021understanding}, mixture of expert \citep{chen2022towards}, and contrastive learning \citep{wen2021toward}. We point out that feature learning in Mixup is more complicated as the learning dynamics for different features are heavily coupled.




\section{Problem Setting.}\label{section:problemsetting}
As mentioned in the introduction section, we theoretically investigate the behaviors of standard training and Mixup training on a multi-view data model. In this section, we will first deliver a detailed set up of the multi-view data model and then introduce the two-layer CNN model as well as the gradient descent algorithms of standard training and Mixup training.


\subsection{Data Model}

In this work, we consider a binary classification problem on the data $(\xb,y)\in\RR^{dP}\times\{1,2\}$, where $\xb = (\xb^{(1)},\dots,\xb^{(P)})$ has $P$ patches and $y\in\{1,2\}$ denotes the data label. For ease of presentation, we define the data of label $y=1$ as the \textit{positive data} and the data of label $y=2$ as the \textit{negative data}. Moreover, the data will be randomly generated according to the following detailed process.


\begin{definition}\label{def:data_distribution_new}
Let $\cD$ denote the data distribution, from which a data point $(\xb,y)\in \RR^{d  P} \times \{1, 2\}$ is randomly generated as follows: 
\begin{enumerate}[leftmargin = *,nosep]
    \item Generate $y\in\{1, 2\}$ uniformly.
    \item Generate $\xb$ as a vector with $P$ patches $\xb = (\xb^{(1)},\ldots,\xb^{(2)})\in (\RR^d)^P$, where 
    \begin{itemize} [nosep, leftmargin=*]       
        \item \textbf{Feature Patch.}
        One patch, among all $P$ patches, will be randomly selected as the feature patch: with probability $1-\rho$ for some $\rho\in(0,1)$, this patch will contain a \textit{common feature}  ($\vb$ for positive data, $\ub$ for negative data); otherwise, this patch will contain a \textit{rare feature} ($\vb'$ for positive data, $\ub'$ for negative data). 
        % Moreover, we assume the number of the feature
    % patches follows from a uniform distribution in $[1, \Theta(1)]$
    
        \item \textbf{Feature Noise.} For all data, a feature vector from $\alpha\cdot\{\ub,\vb\}$ is randomly sampled and assigned to up to $b$ patches.
        \item \textbf{Noise patch.} The remaining patches (those haven't been assigned with a feature or feature noise) are random Gaussian noise  $\sim N(\boldsymbol{0}, \sigma_p^2\cdot \Hb)$, where $\Hb = \Ib - \frac{\ub\ub^\top}{\|\ub\|_2^2}-\frac{\vb\vb^\top}{\|\vb\|_2^2}-\frac{\vb'\vb'^\top}{\|\vb'\|_2^2}-\frac{\ub'\ub'^\top}{\|\ub'\|_2^2}$.
    \end{itemize}
\end{enumerate}
Without loss of generality, we assume all feature vectors are orthonormal, i.e., $\|\ab\|_2=1$ and $\la\ab,\bb\ra=0$ for all $\ab,\bb\in\{\vb,\ub,\vb',\ub'\}$ and $\ab\neq \bb$. 
Moreover, we set $d = \omega(n^6)$, $P,b = \polylog(n)$, $\rho = \Theta(n^{-3/4})$,  $\sigma_p = \Theta(d^{-1/2}n^{1/4})$, and $\alpha = \Theta(1/n)$\footnote{The choice of these parameters is not unique, here we only pick a feasible one for the ease of presentation.}.
\end{definition}

% \CC{add assumptions on the parameter values}
% $ d = \omega(n^3P^3/\rho^2)$.



The multi-view model includes three types of critical vectors: common features, rare features, and noise vectors (the feature noise vectors can be categorized into common features since they are only different in terms of strength). 
All of them can be leveraged to fit the training data points and thus achieve a small training accuracy/loss. However, in order to achieve a nearly perfect test accuracy, one has to learn both common features and rare features as overfitting the random noise vectors of training data points will make no contribution or even be detrimental to the test performance, then the prediction will be heavily affected by the feature noise. Given the data model in Definition \ref{def:data_distribution_new}, we aim to show that Mixup is able to learn all informative features while standard training may only learn a part of them.


The feature-noise data model has been widely adopted to study many algorithmic aspects of deep learning, including adversarial training \citep{allen2020feature}, momentum \citep{jelassi2022towards},  ensemble and knowledge distillation \citep{allen2020towards}, benign overfitting \citep{cao2022benign}, and data augmentation \citep{shen2022data}.  Our data model mostly follows from the one considered in \citet{shen2022data}, which also includes the design of common features and rare features for studying the learning behaviors of data augmentation (that is performed within one single data point, e.g., random flip/rotation). However, instead of assuming that the rare features ($\vb'$ and $\ub'$) can be re-generated by applying data augmentation  on the common features ($\vb$ and $\ub$), we make nearly no assumption on their relationships. Therefore, learning the rare features in our model can be regarded as a harder problem,  and our theoretical analyses for Mixup are orthogonal to those in \citet{shen2022data}.


% The reason behind this is that Mixup is a data augmentation technique that actively mixes two different data together, then the rare features can be potentially discovered by the learner from its mixture with common features.
% then the rare features will be naturally mixed with the strong features, thus can be more likely to be discovered and learned. 
% Therefore, learning the rare features in our model can be regarded as a harder problem compared to that in \citet{shen2022data}.







\subsection{Neural Network Function}

\paragraph{Two-layer CNN model.}
We consider a two-layer CNN model $F$ using quadratic activation function $\sigma(z) = z^2$. Note that we consider binary classification problem with $y\in\{1,2\}$, then given the input feature $\xb=(\xb^{(1)},\dots,\xb^{(p)})$, the $k$-th output of the network ($k\in\{1, 2\}$) is formulated as
\begin{align*}
F_k(\Wb;\xb)  =\sum_{p=1}^P\sum_{r=1}^m (\la\wb_{k,r},\xb^{(p)}\ra)^2.
\end{align*}
where  $\wb_{k,r}\in\RR^{d}$ denotes the neuron weight corresponding to the $k$-th output, $\Wb$ denotes the collection of all model weights, and $m$ denotes the NN width, which is set as $m=\polylog(n)$ throughout this paper\footnote{This choice of network width is to guarantee some nice properties hold with probability at least $1-1/\poly(n)$ at the initialization. We can also resort to setting $m$ as some large constant at the price of deriving a constant probability guarantee, e.g., $>0.9$.}.
Moreover, given the input $\xb$, we denote $\mathrm{Logit}_k(\Wb;\xb)$ by the logit of the $k$-th output of the NN model, which can be calculated via performing a softmax function on the NN outputs:
\begin{align*}
\textstyle{\mathrm{Logit}_k(\Wb;\xb) = e^{F_k(\Wb;\xb_i)}/\sum_{s\in\{1,2\}} e^{F_s(\Wb,\xb_i)}}.
\end{align*}
Using a polynomial activation function (or ReLU with polynomial smoothing) is not new in deep learning theory. The purpose is to better illustrate/distinguish the feature and noise learning dynamics during the neural network training  \citep{frei2022benign,cao2022benign,shen2022data,glasgow2022max}. Our analysis can also be extended to other polynomial functions $\sigma(x)=x^q$ for some $q> 1$. 

 



% Besides, we denote $h(\Wb,\xb)= \arg\max_k F_k(\Wb;\xb)$ as the predicted label of the neural network model given input $\xb$ and model weight $\Wb$.





\subsection{Training Algorithms}\label{sec:training_algorithms}
\paragraph{Initialization.} We assume that the initial weights of the neural network model are generated i.i.d. from the Gaussian initialization: $\wb_{k,r}^{(0)}\sim N(\boldsymbol{0}, \sigma_0^2\Ib)$, where $\sigma_0 = o(d^{-1/2})$.

\paragraph{Standard training.}
Given the training data points $\cS:=\{(\xb_i,y_i)\}_{i=1,\dots,n}$, we train the neural network model via applying standard full-batch gradient descent to optimize the following empirical risk function:
\begin{align*}
&L_\cS(\Wb) = \frac{1}{n}\sum_{i=1}^n \ell(\Wb;\xb_i,y_i),\quad\text{where}\quad 
\ell(\Wb;\xb_i,y_i) = -\log \frac{e^{F_{y_i}(\Wb,\xb_i)}}{\sum_{k\in\{1,2\}} e^{F_k(\Wb;\xb_i)}}.
\end{align*}
Starting from the initialization $\Wb^{(0)}$, the gradient descent of the standard training takes the following update step
\begin{align}\label{eq:update_std}
\Wb^{(t+1)}  = \Wb^{(t)} - \frac{\eta}{n}\sum_{i=1}^n \nabla_{\Wb} \ell(\Wb^{(t)};\xb_i,y_i),
\end{align}
where $\eta$ is the learning rate.
Then, the detailed calculation of the partial derivative $\nabla_{\wb_{k,r}} \ell(\Wb;\xb_i,y_i)$ is given by
\begin{align*}
\nabla_{\wb_{k,r}} \ell(\Wb;\xb_i,y_i) = -2\ell_{k,i}\cdot\sum_{p=1}^P\la\wb_{k,r},\xb_i^{(p)}\ra\cdot\xb_i^{(p)}.
\end{align*}
where $\ell_{k,i}=\ind_{k=y_i}-\mathrm{Logit}_k(\Wb^{(t)};\xb_i)$.



\textbf{Mixup Training.}
Given two training data points $(\xb_1,y_1)$ and $(\xb_2,y_2)$, Mixup trains a neural network based on 
the convex combinations of them: $(\lambda \xb_1 + (1-\lambda)\xb_2, \lambda y_1 + (1-\lambda) y_2)$ and $((1-\lambda) \xb_1 + \lambda\xb_2, (1-\lambda) y_1 + \lambda y_2)$, where we slightly abuse the notation by viewing the labels $y_1$ and $y_2$ as their one-hot encoding. Besides, Figure \ref{fig:linearity_negative} suggested that $\lambda$ does not need to be randomly sampled to achieve better performance than standard training, we will focus on a fixed constant $\lambda\in(0.5,1)$ in our theoretical analysis.
Finally, if considering all possible combinations of the training data pairs with a fixed $\lambda$, the (equivalent) training dataset of Mixup training is $
\cS_{\mathrm{Mixup}}: = \{\xb_{i,j}, y_{i,j}\}_{i, j\in[n]}$,
% \notag\\&= \{\lambda \xb_i + (1-\lambda)\xb_j, \lambda y_i + (1-\lambda) y_j:  i, j\in[n]\},
% \end{align*}
where we denote $\xb_{i,j}$ and $y_{i,j}$ by $\lambda\xb_i+(1-\lambda)\xb_j$ and $\lambda y_i+(1-\lambda)y_j$ respectively. Motivated by this, we can claim that the Mixup training actually aims to learn the model parameter by optimizing the following loss function:
% In particular, in comparison with standard training that takes gradient descent based on the original training dataset $\cS = \{(\xb_i, y_i)\}_{i=1,\dots,n}$, Mixup considers training a neural network on the augmented training dataset, which covers the convex combination of all pairs of training data points. In particular, the 
\begin{align}\label{eq:trainingloss_Mixup}
L_{\cS}^{\mathrm{Mixup}}(\Wb) = \frac{1}{n^2}\sum_{i,j\in[n]} \ell(\Wb; \xb_{i,j}, y_{i,j}),
\end{align}
where 
\begin{align*}
\ell(\Wb;\xb_{i,j},y_{i,j}) = \lambda \ell(\Wb;\xb_{i,j}, y_i) + (1-\lambda)\ell(\Wb;\xb_{i,j}, y_j).
\end{align*}
% \begin{align*}
% \ell(\Wb;\xb_{i,j}, y_{i,j}) = \left\{
% \begin{array}{ll}
% -\log \frac{e^{F_{y_i}(\Wb,\xb_{i,j})}}{\sum_{k\in\{1,2\}} e^{F_k(\Wb;\xb_{i,j})}}& y_{i,j} = [0, 1] \text{ or } [1,0]\\
% -\lambda\cdot\log \frac{e^{F_{1}(\Wb,\xb_i)}}{\sum_{k\in\{1,2\}} e^{F_k(\Wb;\xb_{i,j})}} - (1-\lambda )\cdot\log \frac{e^{F_{2}(\Wb,\xb_{i,j})}}{\sum_{k\in\{1,2\}} e^{F_k(\Wb;\xb_{i,j})}}& y_{i,j}=[\lambda, 1-\lambda]\\
% -\lambda\cdot\log \frac{e^{F_{2}(\Wb,\xb_{i,j})}}{\sum_{k\in\{1,2\}} e^{F_k(\Wb;\xb_{i,j})}} - (1-\lambda )\cdot\log \frac{e^{F_{1}(\Wb,\xb_{i,j})}}{\sum_{k\in\{1,2\}} e^{F_k(\Wb;\xb_{i,j})}}& y_{i,j}=[1-\lambda, \lambda]
% \end{array}
% \right.
% \end{align*}

In this paper, in order to better illustrate the key aspect of Mixup training as well as simplify the theoretical analysis, we resort to the gradient descent on the loss function \eqref{eq:trainingloss_Mixup}, which takes the following update step:
\begin{align*}
\Wb^{(t+1)} = \Wb^{(t)} - \frac{\eta}{n^2}\sum_{i=1}^n \sum_{j=1}^n\nabla_{\Wb} \ell(\Wb^{(t)};\xb_{i,j},y_{i,j}).
\end{align*}
Then, the detailed calculations of all partial derivatives are given as follows: for any Mixup data $(\xb_{i,j},y_{i,j})$, we have
\begin{align*}
\nabla_{\wb_{k,r}} \ell(\Wb;\xb_{i,j})
    &= 2\ell_{k,(i,j)}\cdot \sum_{p=1}^P\la\wb_{k,r}, \xb_{i,j}^{(p)}\ra\cdot\xb_{i,j}^{(p)},
\end{align*}
where $\ell_{k,i}$ is the loss derivative with respect to the network output $F_k(\Wb; \xb_{i,j}, y_{i,j})$:
\begin{align*}
\ell_{k,(i,j)}=\lambda\ind_{k=y_i}+(1-\lambda)\ind_{k=y_j}-\mathrm{Logit}_k(\Wb; \xb_{i,j}).
\end{align*}



% . More specifically, we have 
% \begin{itemize}
%     \item For clean Mixup data $(\xb_{i,j},y_{i,j} )$, we have
%     \begin{align*}
%     \ell_{k;(i,j)} &= \ind_{k=y_i}-\mathrm{Logit}_k(\Wb;  \xb_{i,j}).
%     \end{align*}
%     \item For positive Mixup data $(\xb_{i,j},y_{i,j} )$, i.e., $y_{i,j}=[\lambda, 1-\lambda]$, we have
%     \begin{align*}
%     \ell_{k;(i,j)} = \lambda\cdot\ind_{k=1}+(1-\lambda)\cdot\ind_{k=2}-\mathrm{Logit}_k(\Wb; \xb_{i,j}) .
%     \end{align*}
%     \item For negative Mixup data $(\xb_{i,j},y_{i,j} )$, i.e., $y_{i,j}=[1-\lambda, \lambda]$, we have
%     \begin{align*}
%     \ell_{k;(i,j)}= (1-\lambda)\cdot\ind_{k=1}+\lambda\cdot\ind_{k=2}-\mathrm{Logit}_k(\Wb; \xb_{i,j}).
%     \end{align*}
    
% \end{itemize}


\section{Main Theory}
In this section, we will theoretically characterize the generalization errors achieved by standard training and Mixup training on the multi-view model. In particular, the following Theorem states the negative result of standard training. 

\begin{theorem}\label{thm:std_training}
Suppose that the training data are generated according to Definition \ref{def:data_distribution_new}, let $\eta=1/\poly(n)$,  $T=\polylog(n)/\eta$, and $\{\Wb_{\mathrm{standard}}^{(t)}\}_{t=0,\dots,T}$ be the iterates of standard training, then with probability at least $1-1/\poly(n)$, it holds that for all $t\in[0,T]$, $\PP_{(\xb,y)\sim\cD}\big[\argmax_kF_k(\Wb_{\mathrm{standard}}^{(t)};\xb)\neq y\big]\ge \frac{\rho}{2.01}$.
\end{theorem}
Theorem \ref{thm:std_training} basically states that the two-layer CNN model obtained via standard training will lead to at least $\Theta(\rho)$ test error on the data model defined in Definition \ref{def:data_distribution_new}. In fact, as we will clarify in Section \ref{sec:proof_sketch_std_train}, this is due to the fact that the rare feature data will be fitted via their random noise components, while the rare features $\vb'$ and $\ub'$ will not be learned. Consequently, nearly a half of test rare feature data will be misled by the feature noise components, resulting in a $\Theta(\rho)$ test error.  

In comparison, Mixup training can help learn the rare features and thus achieve a smaller generalization error. We formally state this result in the following theorem.
\begin{theorem}\label{thm:Mixup_training}
Suppose the training data are generated according to Definition \ref{def:data_distribution_new}, let $\eta=\frac{1}{\poly(n)}$,  $T=\frac{\polylog(n)}{\eta}$, and $\{\Wb_{\mathrm{Mixup}}^{(t)}\}_{t=0,\dots,T}$ be the iterates of Mixup training, then with probability at least $1-\frac{1}{\poly(n)}$, it holds that for some $t\in[0,T]$, $\PP_{(\xb,y)\sim\cD}\big[\argmax_kF_k(\Wb_{\mathrm{Mixup}}^{(t)};\xb)\neq y\big]= o\big(\frac{1}{\poly(n)}\big)$.
\end{theorem}


Theorem \ref{thm:Mixup_training} shows that the two-layer CNN model obtained via Mixup training can achieve nearly zero test error, which is much better than that of standard training as $\rho=\Theta(n^{-3/4})\gg o(1/\poly(n))$ (see Definition \ref{def:data_distribution_new}). In particular, as we will show in Section \ref{sec:proof_sketch_mixup_train}, at the core of Mixup training is that it mixes common features and rare features together, thus the learning of these two types of features will be coupled. Consequently, the learning of rare features will be ``boosted'' by the learning of common features, reaching a sufficiently large level that dominates the effect of feature noise.






\section{Overview of the Analysis}


According to the data model in Definition \ref{def:data_distribution_new}, the critical step of the generalization analysis for standard training and Mixup training is to sharply characterize the magnitude of the feature learning, including both common features ($\vb$ and $\ub$) and rare features ($\vb'$, $\ub'$), as well as the noise learning, including all noise vectors $\bxi_i^{(p)}$'s (denoted by $\{\bxi\}$). Then, the key step to show the generalization gap between standard training and Mixup training is to identify their difference in terms of feature and noise learning. 

\subsection{Feature and Noise Learning of Standard Training}\label{sec:proof_sketch_std_train}
% We first recall the update rule of the model weights in standard training: 
% \begin{align*}
%     \wb_{k,r}^{(t+1)} &= \wb_{k,r}^{(t)} + \frac{2\eta}{n}\cdot \sum_{i=1}^n \ell_{k,i}^{(t)} \sum_{p=1}^P  \la\wb_{k,r}^{(t)},\xb_i^{(p)}\ra\cdot \xb_i^{(p)}.
% \end{align*}
% During the entire training process, we seek  precise characterizations for the learning ability of common features $\ub$ and $\vb$, noise vectors $\bxi_i^{(p)}$'s , and rare features $\vb'$ and $\ub'$. 
According to Definition \ref{def:data_distribution_new}, we define $\cS_0^{+}$ and $\cS_0^{-}$ as the set of training data that have strong positive and negative features respectively and $\cS_1^{+}$ and $\cS_1^{-}$  as the set of data that have weak positive and negative features respectively. In the following, the learning patterns of these vectors will be characterized by studying the inner products $\la\wb_{k,r}^{(t)},\ab\ra$, where $\ab\in\{\vb,\ub,\vb',\ub'\}\cup\{\bxi\}$. Intuitively, a larger inner product implies that the neural network has a stronger learning ability of $\ab$. Given the multi-view data model in Definition \ref{def:data_distribution_new} and the update rule \eqref{eq:update_std}, we have for any $\ab\in\{\vb,\ub,\vb',\ub'\}\cup\{\bxi\}$, 
\begin{align}\label{eq:update_features_main}
\la\wb_{k,r}^{(t+1)},\ab\ra & =
\la\wb_{k,r}^{(t)},\ab\ra +\frac{2\eta}{n}\cdot\sum_{i\in[n]} \ell_{k,i}^{(t)} \sum_{p=1}^P \la\wb_{k,r}^{(t)},\xb_i^{(p)}\ra\cdot \la\xb_i^{(p)},\ab\ra.
% \la\wb_{k,r}^{(t+1)},\ub\ra & = \la\wb_{k,r}^{(t)},\ub\ra + \frac{2\eta}{n}\cdot\sum_{i\in[n]} \ell_{k,i}^{(t)} \sum_{p\in \cP_i(\ub)} \la\wb_{k,r}^{(t)},\ub\ra\cdot \alpha_{i,p}^2\|\ub\|_2^2,\notag\\
% \la\wb_{k,r}^{(t+1)},\vb'\ra & = \la\wb_{k,r}^{(t)},\vb\ra + \frac{2\eta}{n}\cdot\sum_{i\in\cS_1^{+}} \ell_{k,i}^{(t)} \sum_{p\in \cP_i(\vb')} \la\wb_{k,r}^{(t)},\vb'\ra\cdot \|\vb'\|_2^2,\notag\\
% \la\wb_{k,r}^{(t+1)},\ub'\ra & = \la\wb_{k,r}^{(t)},\ub'\ra + \frac{2\eta}{n}\cdot\sum_{i\in\cS_1^{-}} \ell_{k,i}^{(t)} \sum_{p\in \cP_i(\ub')} \la\wb_{k,r}^{(t)},\ub'\ra\cdot \|\ub'\|_2^2,
\end{align}
Then by the data model in Definition \ref{def:data_distribution_new}, we can see that for common feature vector $\ab\in\{\vb,\ub\}$, there will be $\Theta(n)$ training data points contributing  to the learning of $\ab$; while for rare feature vector $\ab\in\{\vb',\ub'\}$, only $\Theta(\rho n)$ data points contributing to the learning. Besides, since each noise vector $\ab\in\{\bxi\}$ in the training data point is randomly generated, its learning will largely rely on one single data, i.e., the data consisting of that noise vector. This difference clearly shows that the common features will be  preferably discovered and learned during the standard training. 
% 

% where $\cP_i(\ab)$ denotes the set of patches in $\xb_i$ containing the feature $\ab$ and $\alpha_{i,p}^2=1$ if $\xb_i^{(p)}$ is a feature patch and $\alpha_{i,p}^2=\alpha^2$ if $\xb_i^{(p)}$ is the feature noise. 

% Additionally, note that the update of rare features only depends on the data in $\cS_1^+$ and $\cS_1^-$ since the data $(\xb_i, y_i)$ in $\cS_0^+$ and $\cS_0^-$ satisfies $\cP_i(\vb')=\emptyset$ and $\cP_i(\ub')=\emptyset$. Similarly, we can also obtain the following result regarding noise learning
% \begin{align*}
% &\la\wb_{k,r}^{(t+1)},\bxi_s^{(q)}\ra \notag\\
% &= \la\wb_{k,r}^{(t)},\bxi_s^{(q)}\ra + \frac{2\eta}{n}\cdot\sum_{i=1}^n\ell_{k,i}^{(t)} \sum_{p=1}^P \la\wb_{k,r}^{(t)},\bxi_s^{(q)}\ra\cdot \la\xb_i^{(p)}, \bxi_s^{(q)}\ra.
% % & = \la\wb_{k,r}^{(t)},\bxi_s^{(q)}\ra + \frac{2\eta}{n}\cdot \ell_{k,s}^{(t)}\cdot\la\wb_{k,r}^{(t)},\bxi_s^{(q)}\ra\cdot\|\bxi_s^{(q)}\|_2^2 + \frac{2\eta}{n}\cdot\sum_{i\neq s || p\neq q} \ell_{k,i}^{(t)}\la\wb_{k,r}^{(t)},\bxi_s^{(q)}\ra\cdot\la\xb_i^{(p)},\bxi_s^{(q)}\ra.
% \end{align*}
% Moreover, note that if $\xb_i^{(p)}\neq \bxi_s^{(q)}$ (i.e., $i\neq s$ or $p\neq q$), then $|\la\xb_i^{(p)},\bxi_s^{(p)}\ra|$ is in the order of $\tilde O(d^{1/2}\sigma_p^2)$.
% % depending on whether $\bxi_s^{(p)}$ is the feature patch or noise patch. 
% Therefore, we further have
% \begin{align}\label{eq:update_noise_simplified}
% \la\wb_{k,r}^{(t+1)},\bxi_s^{(q)}\ra 
% & = \la\wb_{k,r}^{(t)},\bxi_s^{(q)}\ra \cdot\bigg[ 1 + \frac{2\eta}{n}\cdot \ell_{k,s}^{(t)}\cdot\|\bxi_s^{(q)}\|_2^2 \notag\\
% &\pm \frac{2\eta}{n}\cdot\sum_{i\neq s || p\neq q} |\ell_{k,i}^{(t)}|\cdot \tilde O\big(d^{1/2}\sigma_p^2\big)\bigg].
% \end{align}


In the following analysis, we will decompose the entire standard training process into three phases, according to the learning of common features and noises. In particular, the \textbf{Phase 1} referred to the initial training iterations such that the neural network output, with respect to all input training data, is in the order of $O(1)$. In this phase, the loss derivatives $\ell_i^{(t)}$ will remain in the constant order and all critical vectors will be learned at a fast rate. Then 
The \textbf{Phase 2} is defined as the training period starting from the end of \textbf{Phase 1} to the iteration that the neural network output has reached $\tilde\Theta(1)$ for all training inputs. Finally, we refer to \textbf{Phase 3} as the training period starting from the end of \textbf{Phase 2} to convergence, i.e., the gradient converges to zero. 


% As we will show later, the learning of common features dominates the learning of rare features and noises in this phase. As a consequence, the neural network output will finally reach $\tilde\Theta(1)$ for all common feature data points, and remain
% $o(1/\polylog(n))$ for the remaining data points (i.e., rare feature data). 
 % We will show that after this phase, the training data point will be well fitted via either common feature vectors (if the data consists of them) or the noise vector, while the learning of rare feature vectors still remains at the initialization level. 

\paragraph{Standard Training, Phase 1.}
The following lemma characterizes the learning of all features and noise  in Phase 1.
\begin{lemma}\label{lemma:standard_learning_phase1_main}
There exists a iteration number $T_0 = \tilde\Theta(1/\eta)$ such that for any $t\le T_0$, it holds that
\begin{align}\label{eq:good_innerproduct}
\la\wb_{1,r}^{(t+1)},\vb\ra  &= \la\wb_{1,r}^{(t)},\vb\ra\cdot \big(1 + \Theta(\eta)\big),\quad
\la\wb_{2,r}^{(t+1)},\ub\ra  = \la\wb_{2,r}^{(t)},\ub\ra\cdot \big(1 + \Theta(\eta)\big).
\end{align}
Besides, for all remaining inner products, it holds that
\begin{align*}
\max_{r}|\la\wb_{k,r}^{(t+1)},\ab\ra| \le \max_{r}|\la\wb_{k,r}^{(t)},\ab\ra|\cdot \big[1 + o(\eta/\polylog(n))\big]
\end{align*}
where $t\le T_0$, $r\in[m]$, $k\in[2]$, $q\in[P]$, $\ab\in\{\ub,\vb,\ub',\vb'\}\cup\{\bxi\}$ are arbitrarily chosen as long as the inner products are different from those in \eqref{eq:good_innerproduct}.
% &|\la\wb_{2,r}^{(t)},\vb\ra| = \tilde O(\sigma_0),\quad |\la\wb_{1,r}^{(t)},\ub\ra| = \tilde O(\sigma_0),\\
% &|\la\wb_{k,r}^{(t)},\vb'\ra| = \tilde O(\sigma_0),\quad |\la\wb_{k,r}^{(t)},\ub'\ra| = \tilde O(\sigma_0), \quad|\la\wb_{k,r}^{(t)}, \bxi_s^{(q)}\ra| = \tilde O\big(d^{1/2}\sigma_p\sigma_0\big).
% \end{align*}
% Consequently, at the end of Phase 1, it holds that
% \begin{align*}
% &\sum_{r=1}^m (\la\wb_{1,r}^{(T_0)},\vb\ra)^2 = \tilde\Theta(1), \ \sum_{r=1}^m (\la\wb_{2,r}^{(T_0)},\ub\ra)^2 = \tilde\Theta(1);\notag\\
% &|\la\wb_{2,r}^{(T_0)}, \vb\ra|, |\la\wb_{1,r}^{(T_0)}, \ub\ra|, |\la\wb_{k,r}^{(T_0)}, \ub'\ra|, |\la\wb_{k,r}^{(T_0)}, \vb'\ra|=\tilde O(\sigma_0); \quad|\la\wb_{k,r}^{(T_0)}, \bxi\ra|=\tilde O(d^{1/2}\sigma_p\sigma_0)
% \end{align*}
% for all $k\in[2]$, $r\in[m]$ and $\bxi\in\{\bxi\}$.
\end{lemma}
Lemma \ref{lemma:standard_learning_phase1_main} shows the competence results of learning common features, rare features, and noise vectors in Phase 1. In particular, it can be observed that the learning of common features ($\vb$, $\ub$) enjoys a much faster rate, while other critical vectors, including rare features and noise vectors, will be staying at their initialization levels. 
% As a consequence, the neural network weights will finally have a $\tilde \Theta(1)$ component along the directions of $\vb$ and $\ub$ and span nearly nothing along the directions of rare features and noise vectors (note that $\sigma_0, d^{1/2}\sigma_p\sigma_0 = o(1/\polylog(n))$).


\paragraph{Standard Training, Phase 2.}
% After \textbf{Phase 1}, the neural network output will become larger so that the loss derivatives (i.e, $\ell_{k,i}^{(t)}$) or the output logits may no longer be viewed as a quantity in the constant order.
% Particularly, as shown in Lemma \ref{lemma:results_phase1}, when $t>T_0$, the feature learning, i.e., $\la\wb_{1,r}^{(t)},\vb\ra$ and $\la\wb_{2,r}^{(t)},\ub\ra$ will reach the constant order, implying that $|\ell_{k,i}^{(t)}|$ will be closer to $1$ or $0$ for all common feature data. 
During this phase, the loss derivative will remain in the constant order for the rare feature data, since either the rare feature learning (e.g, $\la\wb_{1,r}^{(t)}, \vb'\ra$) or the noise learning (e.g., $\la\wb_{1,r}^{(t)},\bxi_i^{(p)}\ra$) are still quite small. Recall that the common features have already been fitted during Phase 1, we will then focus on the competence between learning rare features and learning noise vectors in Phase 2. The following lemma characterizes the dynamics of standard training in Phase 2.
% (See), so that the corresponding neural network outputs are also in the order of $o\big(1/\polylog(n)\big)$. Therefore, we define \textbf{Phase 2} by the period that (1) is after \textbf{Phase 1} and (2) the neural network outputs for the rare feature data are still in the order of $O\big(1/\polylog(n)\big)$ (or equivalently, the loss derivatives of rare feature data are in the constant order.)



\begin{lemma}\label{lemma:standard_learning_phase2_main}
There exists a iteration number $T_1=\tilde O\big(\frac{n}{d\sigma^2 \eta}\big)$ such that for any $t\in[T_0, T_1]$, it holds that
\begin{align*}
\la\wb_{1,r}^{(t+1)},\vb'\ra &= \la\wb_{1,r}^{(t)},\vb'\ra\cdot \big[1 + \Theta(\rho\eta)\big],\quad
\la\wb_{2,r}^{(t+1)},\ub'\ra = \la\wb_{2,r}^{(t)},\ub'\ra\cdot \big[1 + \Theta(\rho\eta)\big].
% \la\wb_{1,r}^{(t+1)},\ub'\ra &= \la\wb_{1,r}^{(t)},\ub'\ra\cdot \big[1 - \Theta(\rho\eta)\big],\  \la\wb_{2,r}^{(t+1)},\ub'\ra = \la\wb_{2,r}^{(t)},\ub'\ra\cdot \big[1 + \Theta(\rho\eta)\big]
\end{align*}
Besides, for any $i\in\cS_1^+\cup\cS_1^-$, any $q\in[P]$ and $k=y_s$,
\begin{align*}
\max_{r}|\la\wb_{k,r}^{(t+1)},\bxi_s^{(q)}\ra| =
\max_{r}|\la\wb_{k,r}^{(t)},\bxi_s^{(q)}\ra|\cdot \big[1 + \eta/n\cdot\tilde\Theta(d\sigma_p^2)\big]
\end{align*}
% Consequently, at the end of Phase 2, it holds that:
% for all $i\in\cS_0^+\cup\cS_0^-$,
% \begin{align*}
% |\la\wb_{1,r}^{(T_1)}, \vb\ra|, |\la\wb_{2,r}^{(T_1)}, \ub\ra| = \tilde\Theta(1),\  |\la\wb_{2,r}^{(T_1)}, \vb\ra|, |\la\wb_{1,r}^{(T_1)}, \ub\ra| = \tilde O(\sigma_0),\  |\la\wb_{k,r}^{(T_1)}, \bxi_i^{(p)}\ra|=\tilde O\big(d^{1/2}\sigma_p\sigma_0\big);
% \end{align*}
% for all $i\in\cS_1^+\cup\cS_1^-$,
% \begin{align*}
% |\la\wb_{1,r}^{(T_1)}, \vb'\ra|, |\la\wb_{2,r}^{(T_1)}, \ub'\ra| = \tilde O(\sigma_0),\ |\la\wb_{2,r}^{(T_1)}, \vb'\ra|, |\la\wb_{1,r}^{(T_1)}, \ub'\ra| = \tilde O(\sigma_0),\  |\la\wb_{y_i,r}^{(T_1)}, \bxi_i^{(p)}\ra|=\tilde\Theta(1).
% \end{align*}
\end{lemma}
Lemma \ref{lemma:standard_learning_phase2_main} shows that for rare feature data points, standard training admits a faster noise learning speed compared to rare feature learning (note that $d\sigma_p^2\gg \rho$, according to Definition \ref{def:data_distribution_new}). This consequently leads to adequate learning of noise ($|\la\wb_{y_i,r}^{(T_1)}, \bxi_i^{(p)}\ra|=\tilde\Theta(1)$ for some $p\in[P]$) and nearly no learning of rare features ($|\la\wb_{k,r}^{(T_1)},\vb'\ra|,|\la\wb_{k,r}^{(T_1)},\ub'\ra|=\tilde O\big(\sigma_0\big)$). 



\paragraph{Standard Training, Final Phase.}
The final phase is defined as the training period after the end of Phase 2 until convergence. In the following lemma, we will show that (1) the convergence can be guaranteed; and (2) the learning of features and noise vectors at  Phase 2 will be maintained.

\begin{lemma}\label{lemma:standard_learning_phase3_main}
Let $T_1$ be the iteration number defined in Lemma \ref{lemma:standard_learning_phase2_main}, then for any $t=\poly(n)>T_1$ and $k\in\{1,2\}$, 
\begin{align*}
\frac{1}{n}\sum_{\tau=T_1}^t\sum_{i=1}^n |\ell_{k,i}^{(\tau)}| = \tilde O(1/\eta).
\end{align*}
Moreover, we have $\sum_{r=1}^m(\la\wb_{1,r}^{(t)}, \vb\ra)^2, \sum_{r=1}^m(\la\wb_{2,r}^{(t)}, \ub\ra)^2 = \tilde\Theta(1)$ and $|\la\wb_{k,r}^{(t)},\vb'\ra|, |\la\wb_{k,r}^{(t)},\ub'\ra|=\tilde O(\sigma_0)$.
% for all $i\in\cS_0^+\cup\cS_0^-$, $k\in[2]$, $r\in[m]$, and $p\in[P]$,
% \begin{align*}
% &\sum_{r=1}^m(\la\wb_{1,r}^{(t)}, \vb\ra)^2 = \tilde\Theta(1),\ \sum_{r=1}^m\big(\la\wb_{2,r}^{(t)},\ub\ra\big)^2 = \tilde\Theta(1),\ |\la\wb_{2,r}^{(t)}, \vb\ra|, |\la\wb_{1,r}^{(t)}, \ub\ra| ,  |\la\wb_{k,r}^{(t)}, \bxi_i^{(p)}\ra|=o\bigg(\frac{1}{\polylog(n)}\bigg);
% \end{align*}
% for all $i\in\cS_1^+\cup\cS_1^-$, $k\in[2]$, $r\in[m]$, and $p\in[P]$
% \begin{align*}
% \sum_{r=1}^m\sum_{p\in\cP_i(\bxi)}(\la\wb_{y_i,r}^{(t)}, \bxi_i^{(p)}\ra)^2 = \tilde\Theta(1),\ |\la\wb_{k,r}^{(t)}, \vb'\ra|, |\la\wb_{k,r}^{(t)}, \ub'\ra|, |\la\wb_{y_i^c,r}^{(t)}, \bxi_i^{(p)}\ra|=  o\bigg(\frac{1}{\polylog(n)}\bigg),
% \end{align*}
% where we define $y_i^c = 2$ if $y_i=1$ and $y_i^c=1$ if $y_i=2$.
\end{lemma}

It can be clearly seen that the gradient descent can converge to the point with a small gradient (the averaged loss derivative will be roughly in the order of $\tilde O(1/(t\eta))$, which approaches zero when  $t$ is large). More importantly, the common feature data and rare feature data will be correctly classified by fitting different components: common feature data will be fitted by learning $\vb$ and $\ub$, while the rare feature data will be fitted by noise memorization (as standard training nearly makes no progress in learning. Consequently, when it comes to a fresh test rare feature data, the model prediction will be heavily affected by the feature noise component, thus leading to an incorrect prediction with a constant probability (the formal proof is deferred to Section \ref{sec:proof_main_std}).  







\subsection{Feature and Noise Learning of Mixup Training}\label{sec:proof_sketch_mixup_train}

As mentioned in Section \ref{sec:training_algorithms}, any data pair sampled  from training dataset will be considered, which gives in total $n^2$ Mixup data.  
% In particular, note that we have two types of data: common feature data and rare feature data, while each of them consists of two labels. Let $\cS_0^+$, $\cS_0^-$, $\cS_1^+$, and $\cS_1^-$ denote the classes of positive common feature data (the data consisting of common feature $\vb$), negative common feature data (the data consisting of common feature $\ub$),  positive rare feature data (the data consisting of common feature $\vb'$),  and negative rare feature data (the data consisting of common feature $\ub'$), respectively. 
Note that we have two types of data in the origin training dataset: common feature data and rare feature data with two labels, denoted by $\cS_0^+$, $\cS_0^-$, $\cS_1^+$, and $\cS_1^-$ (see  Section \ref{sec:proof_sketch_std_train}),  we can also categorize the Mixup data points into multiple sets accordingly. Particularly, let $\cS_{*,**}^{\dagger, \dagger\dagger}$ be the set of mixed data $\xb_{i,j}=\lambda\xb_i+(1-\lambda)\xb_j$ with $\xb_i\in\cS_*^\dagger$ and $\xb_j\in\cS_{**}^{\dagger\dagger}$, we can accordingly categorize all Mixup data with the following $4$ classes:
\begin{itemize}[leftmargin=*,nosep]
    \item Mix between two common feature data points, including $\cS_{0,0}^{+, +}$, $\cS_{0,0}^{-, -}$, $\cS_{0,0}^{+, -}$, $\cS_{0,0}^{-, +}$, each of them is of size $\Theta(n^2)$.
    \item Mix between  common feature and rare feature data points with the same label, including  $\cS_{0,1}^{+, +}$, $\cS_{0,1}^{-, -}$, $\cS_{1,0}^{+,+}$, and $\cS_{1,0}^{-,-}$, each of them is of size $\Theta(\rho n^2)$.
    \item Mix between common feature  and rare feature data points with different labels, including  $\cS_{0,1}^{+, -}$, $\cS_{0,1}^{-, +}$, $\cS_{1, 0}^{+, -}$, and $\cS_{1, 0}^{-, +}$, each of them is of size $\Theta(\rho n^2)$.
    \item Mix between two rare feature data points, including $\cS_{1,1}^{+, +}$,$\cS_{1,1}^{-, -}$, $\cS_{1,1}^{+, -}$ and $\cS_{1, 1}^{-, +}$, each of them is of size $\Theta(\rho^2 n^2)$.
    
\end{itemize}
In contrast to standard training that nearly admits  separate learning dynamics for common and rare features, the second and third classes of Mixup training data points, actively mix the common  and rare features together. For instance, some data points in $\cS_{0,1}^{+,+}$ will contain a data patch of form $\lambda\vb + (1-\lambda)\vb'$. Then the learning of $\vb$ will benefit the learning of $\vb'$, since their gradient updates are positively correlated. In the following, we will provide a precise characterization on the learning dynamics of feature and noise vectors. 



% Note that we have $\Theta(n)$ common feature data and $\Theta(\rho n)$ rare feature data in the original training dataset.
% The following lemma further characterizes the size of each class of Mixup data.
% \begin{proposition}
% Assuming the data is generated according to Definition \ref{def:data_distribution_new}, then it holds that
% \begin{align*}
% &|\cS_{0,0}^{+,+}|,   |\cS_{0,0}^{-,-}|, |\cS_{0,0}^{+,-}|= \Theta(n^2);\notag\\
% & |\cS_{0,1}^{+,+}|, |\cS_{0,1}^{-,-}|, |\cS_{0,1}^{+,-}|, |\cS_{0,1}^{-,+}| = \Theta(\rho n^2);\notag\\
% &|\cS_{1,1}^{+,+}|,   |\cS_{1,1}^{-,-}|,  |\cS_{0,0}^{+,-}|= \Theta(\rho^2n^2).
% \end{align*}
% \end{proposition}


In particular, noting that we consider the full-batch gradient descent on the entire Mixup training dataset (see Section \ref{sec:training_algorithms}), the update formula of all critical vectors are provided as follows: for any $\ab\in\{\ub,\vb,\ub',\vb'\}\cup\{\bxi\}$, we have
\begin{align}\label{eq:update_allfeatures_Mixup}
\la\wb_{k,r}^{(t+1)}, \ab\ra &= \la\wb_{k,r}^{(t)}, \ab\ra - \eta\cdot\la\nabla_{\wb_{k,r}} L(\Wb^{(t)}),\ab\ra.
\end{align}
where we denote $L(\Wb^{(t)})$ as the short-hand notation of $L_\cS^{\mathrm{Mixup}}$ (defined in \eqref{eq:trainingloss_Mixup}) for simplifying the notation.
% \la\wb_{k,r}^{(t+1)}, \ub\ra &= \la\wb_{k,r}^{(t)}, \ub\ra + \frac{\eta}{n^2}\cdot   \sum_{i,j\in[n]} \ell_{k,(i,j)}^{(t)} \sum_{p\in[P]}\la\wb_{k,r}^{(t)},\xb_{i,j}^{(p)}\ra\cdot\la \xb_{i,j}^{(p)},\ub\ra\notag\\
% \la\wb_{k,r}^{(t+1)}, \vb'\ra &= \la\wb_{k,r}^{(t)}, \vb'\ra + \frac{\eta}{n^2}\cdot  \sum_{i,j\in[n]} \ell_{k,(i,j)}^{(t)} \sum_{p\in[P]}\la\wb_{k,r}^{(t)},\xb_{i,j}^{(p)}\ra\cdot\la \xb_{i,j}^{(p)},\vb'\ra\notag\\
% \la\wb_{k,r}^{(t+1)}, \ub'\ra &= \la\wb_{k,r}^{(t)}, \ub'\ra + \frac{\eta}{n^2}\cdot  \sum_{i,j\in[n]} \ell_{k,(i,j)}^{(t)} \sum_{p\in[P]}\la\wb_{k,r}^{(t)},\xb_{i,j}^{(p)}\ra\cdot\la \xb_{i,j}^{(p)},\ub'\ra.
% \la\wb_{k,r}^{(t+1)}, \bxi_s^{(q)}\ra &= \la\wb_{k,r}^{(t)}, \ub'\ra + \frac{\eta}{n^2}\cdot  \sum_{i,j\in[n]} \ell_{k,(i,j)}^{(t)} \sum_{p\in[P]}\la\wb_{k,r}^{(t)},\xb_{i,j}^{(p)}\ra\cdot\la \xb_{i,j}^{(p)},\ub'\ra.
% \end{align}
More specifically, we summarize the update of all critical vectors (e.g., common features, rare features, and data noise vectors) in the following Proposition.
\begin{proposition}\label{prop:update_Mixup_main}
For any critical vector $\ab\in\{\vb, \ub, \vb', \ub'\}\cup\{\bxi\}$, we have
\begin{align*}
-\la\nabla_{\wb_{k,r}} L(\Wb^{(t)}),\ab\ra = \hspace{-4mm}\sum_{\bb\in\{\vb,\ub,\vb',\ub'\}\cup\{\bxi\}}\gamma_k^{(t)}(\bb,\ab)\la\wb_{k,r}^{(t)},\bb\ra
% \notag\\
% &=  \gamma_k^{(t)}(\vb,\ab)\cdot\la\wb_{k,r}^{(t)}, \vb\ra + \gamma_k^{(t)}(\ub,\ab)\cdot\la\wb_{k,r}^{(t)},\ub\ra +\gamma_k^{(t)}(\vb',\ab)\cdot\la\wb_{k,r}^{(t)},\vb'\ra \notag\\
% &\qquad + \gamma_k^{(t)}(\ub',\ab)\cdot\la\wb_{k,r}^{(t)},\ub'\ra +  \sum_{i=1}^n\sum_{p\in[P]}\gamma_k^{(t)}(\bxi_i^{(p)},\ab)\cdot\la\wb_{k,r}^{(t)},\bxi_i^{(p)}\ra,
\end{align*}
where $\gamma_k^{(t)}(\bb,\ab)$ is a scalar output function that depends on $\bb, \ab\in\{\vb, \ub, \vb', \ub'\}\cup\{\bxi\}$. More specifically, let 
\begin{align*}
\xb_{i,j}^{(p)} &= \theta_{i,j}^{(p)}(\vb) \cdot\vb + \theta_{i,j}^{(p)}(\ub)\cdot \ub +\theta_{i,j}^{(p)}(\vb') \cdot\vb'  + \theta_{i,j}^{(p)}(\ub')\cdot \ub'+ \sum_{s=1}^n\sum_{q\in[P]}\theta_{i,j}^{(p)}(\bxi_s^{(q)})\cdot \bxi_s^{(q)} 
\end{align*}
be a linear expansion of $\xb_{i,j}^{(p)}$ on the space spanned by $\{\vb, \ub, \vb', \ub'\}\cup\{\bxi\}$, we have
\begin{align*}
\gamma_k^{(t)}(\bb, \ab) = \frac{1}{n^2}\sum_{i,j\in[n]}\ell_{k,(i,j)}^{(t)} \sum_{p\in[P]} \theta_{i,j}^{(p)}(\bb)\cdot\la\xb_{i,j}^{(p)},\ab\ra.
\end{align*}

\end{proposition}
\vspace{-2mm}

From Proposition \ref{prop:update_Mixup_main}, it can be seen that the learning of common features, rare features,  and noise vectors are heavily coupled. 
% This is because the Mixup training dataset consists of the data patch with form $\lambda \ab + (1-\lambda)\bb$. Then a stronger learning of $\ab$ will lead to a larger output for this data patch, which in turn produces a large gradient along the direction of $\bb$. 
Mathematically, the  coefficient $\gamma_k^{(t)}(\ab,\bb)$ precisely describes how the learning of $\ab$ affects the learning of $\bb$, where $\ab,\bb\in\{\vb,\ub,\vb',\ub'\}\cup\{\bxi\}$.
This effect can be either positive or negative, depending on the sign of $\gamma_k^{(t)}(\ab,\bb)$. Then, the next step is to sharply characterize the coefficients $\gamma_k^{(t)}(\bb, \ab)$. We will focus on early phase of Mixup training, where the loss derivatives can be regarded as the constant (i.e., approximately $0.5$, $-0.5$, $\lambda-0.5$, or $0.5-\lambda$). Particularly, we will consider the training stage such that $\max_{k\in[2],i,j\in[n]} |F_k(\Wb^{(t)};\xb_{i,j})|\le \zeta$, where $\zeta=o\big(\frac{1}{\polylog(n)}\big)$ is a user-defined parameter. Then based on $\zeta$, we summarize the results of some critical coefficients in the following lemma, while the results for all coefficients are presented in Lemma \ref{lemma:feature_learning_coefficients_v_mixup}-\ref{lemma:noise_learning_coefficients_incorrect_mixup}.

\begin{lemma}\label{lemma:critical_coefficients}
Assume $\max_{k\in[2],i,j\in[n]} |F_k(\Wb^{(t)};\xb_{i,j})|\le \zeta$ for some $\zeta\in[\omega(d\sigma_p^2/(Pn)),o(d^{-1/2}\sigma_p^{-1})]$, then,
\begin{align*}
&\gamma_1^{(t)}(\vb,\vb), \gamma_2^{(t)}(\ub,\ub) = \Theta(1),\ \gamma_{y_i}^{(t)}(\bxi_i^{(p)}, \bxi_i^{(p)})=\Theta\big(d\sigma_p^2/n\big),\notag\\
&\gamma_1^{(t)}(\vb,\vb'), \gamma_2^{(t)}(\ub,\ub') = \Theta(\rho/P),\ |\gamma_2^{(t)}(\ub,\vb')|, |\gamma_1^{(t)}(\vb,\ub' )|=O(\zeta\rho/P).
\end{align*}
\end{lemma}
\vspace{-2mm}
The coefficients presented in Lemma \ref{lemma:critical_coefficients} reveal some key differences between learning common features, rare features, and noise. Let's consider $\vb$ without loss of generality. First, similar to the standard training, the learning of common features is much faster than the learning of noises, since the leading terms of common feature learning (i.e., $\gamma_1^{(t)}(\vb,\vb)$) and noise learning (i.e., $(\gamma_{y_i}^{(t)}(\bxi_i^{(p)}, \bxi_i^{(p)})$) satisfy:
$\gamma_1^{(t)}(\vb,\vb)\gg\gamma_{y_i}^{(t)}(\bxi_i^{(p)}, \bxi_i^{(p)})$. Second, different from standard training where the rare features are nearly unexplored, Mixup training has the ability to boost the learning of rare features via common feature learning, which is characterized by $\gamma_1^{(t)}(\vb, \vb')\cdot \la\wb_{1,r}^{(t)},\vb\ra$ or $\gamma_2^{(t)}(\ub, \vb')\cdot \la\wb_{2,r}^{(t)},\ub\ra$. Finally, we also show that such a boosting effect is positive: the  boosting of $\vb'$ to the correct neurons (i.e., $\{\wb_{1,r}^{(t)}\}_{r\in[m]}$) is stronger than that to the incorrect neurons (i.e., $\{\wb_{2,r}^{(t)}\}_{r\in[m]}$), since $\gamma_1^{(t)}(\vb,\vb')\gg|\gamma_2^{(t)}(\ub,\vb')|$ (recall we pick $\zeta = o\big(\frac{1}{\polylog(n)}\big)$). This implies that the rare features will be effectively discovered by  Mixup training, and finally, the neural network will have non-negligible components along the directions of $\vb'$ and $\ub'$. We formally stated this in the following lemma. 

\begin{lemma}\label{lemma:outcome_Mixup_main}
Let $\zeta$ be the same as that in Lemma \ref{lemma:critical_coefficients} and $T$ be the smallest iteration number such that $\max_{k\in[2], i,j\in[n]} |F_k(\Wb^{(T)}; \xb_{i,j})|\ge \zeta/2$, then $T=\tilde O(1/\eta)$ and  with probability at least $1-1/\poly(n)$, it holds that
\begin{align*}
&\max_{r}|\la\wb_{1,r}^{(T)},\vb\ra|, \max_{r}|\la\wb_{2,r}^{(T)},\ub\ra| = \tilde\Omega(\zeta^{1/2}), \ \max_{r}|\la\wb_{1,r}^{(T)},\vb'\ra|, \max_{r}|\la\wb_{2,r}^{(T)},\ub'\ra| =\Omega(\rho \zeta^{1/2})\notag\\
&\max_{r}|\la\wb_{2,r}^{(T)},\vb\ra|, \max_{r}|\la\wb_{1,r}^{(T)},\ub\ra| = \tilde O(\zeta^{3/2}),\ \max_{r}|\la\wb_{2,r}^{(T)},\vb'\ra|, \max_{r}|\la\wb_{1,r}^{(T)},\ub'\ra| =\tilde O(\zeta^{3/2}).
\end{align*}
\end{lemma}
\vspace{-2mm}
We can then make a comparison between Lemma \ref{lemma:standard_learning_phase3_main} and Lemma \ref{lemma:outcome_Mixup_main} to illustrate the similarities and differences between standard training and Mixup training in feature learning. In particular, it is clear that both standard and Mixup training can successfully learn the common features, i.e., the inner products $\la\wb_{1,r}^{(T)},\vb\ra$ and $\la\wb_{2,r}^{(T)},\ub\ra$ are the dominating ones among all critical inner products. While more importantly, the Mixup training can lead to much better rare feature learning compared to standard training: the standard training gives $|\la\wb_{1,r}^{(t)},\vb\ra|,|\la\wb_{2,r}^{(t)},\ub\ra| = \tilde O(\sigma_0)$ for all iterations; in contrast, the Mixup training gives $|\la\wb_{1,r}^{(T)},\vb'\ra|, |\la\wb_{2,r}^{(T)},\ub'\ra|=\Omega(\rho\zeta^{1/2})$, which are much larger. Consequently, the strength of rare feature learning in Mixup training will dominate the effect of feature noise, thus achieving a nearly zero test error (the formal proof is deferred to Section \ref{sec:proof_mixup}).

% This suggests that the Mixup training has a stronger ability to discover and learn the rare features from the training data, and consequently attains a smaller test error. 


\begin{figure}[!t]
\vskip -0.1in
     \centering
     \subfigure[Common Feature Learning]{\includegraphics[width=0.48\columnwidth]{figure/common_feat_learn.pdf}}
      \subfigure[Rare Feature Learning]{\includegraphics[width=0.505\columnwidth]{figure/rare_feat_learn.pdf}}
      \vskip -0.1in
    \caption{Common feature learning and rare feature learning on synthetic data, all experiments are conducted using full-batch gradient descent. Here we consider three training methods: standard training, Mixup training, and Mixup training with early stopping (at the $10000$-th iteration).}
    \label{fig:synthetic_feat_learn}
\end{figure}

\subsection{Implications to the Early Stopping of Mixup} 
In addition to demonstrating the ability of Mixup in learning rare features,
Lemma \ref{lemma:outcome_Mixup_main} also reveals that the benefits of Mixup training mostly come from its early training phase. Therefore, this motivates us to study the early-stopped Mixup training, i.e., the Mixup data augmentation will be turned off after a number of iterations. Then clearly, after turning off the Mixup data augmentation, the learned features will never be forgotten since the gradient update in this period will be always positively correlated (by \eqref{eq:update_features_main}). This immediately leads to the following fact.
\begin{fact}\label{fact:nodecrease}
Let $T$ be the same as that in Lemma \ref{lemma:outcome_Mixup_main}, then if early stopping Mixup training at the iteration $T$, we have for any $t>T$, it holds that $
\max_{r}|\la\wb_{1,r}^{(t)},\vb'\ra|, \max_{r}|\la\wb_{2,r}^{(t)},\ub'\ra| =\Omega(\rho\zeta^{1/2})$.
\end{fact}
\vspace{-2mm}

This further implies that applying proper early stopping in Mixup training will not affect the rare feature learning. Besides, turning off Mixup will  enhance the learning of common features (since its learning speed will no longer be affected by the mix with rare features and noises), which could potentially lead to even better generalization performance. In the next section, we will empirically justify the effectiveness of applying early stopping in Mixup training.

\section{Experiments}

\paragraph{Synthetic Data.} We first perform numerical experiments on synthetic data to verify our theoretical results. In particular, the synthetic data is generated according to Definition \ref{def:data_distribution_new}.  In particular, we set dimension $d=2000$, training sample size $n=300$, the ratio of rare feature data $\rho=0.1$, noise strength $\sigma_p=0.15$, feature noise strength $\alpha=0.05$, number of total patches $P=5$, and number feature noise patches $b=2$. For the two-layer CNN model and the training algorithm, we  set network width $m=10$, and conduct full-batch gradient descent with learning rate $\eta=0.05$ and total iteration number $T=20000$. We characterize the learning of common features and rare features via calculating $\sum_{r=1}^m(\la\wb_{1,r},\vb\ra)^2$ and $\sum_{r=1}^m(\la\wb_{1,r},\vb'\ra)^2$ (we only consider $\vb$ and $\vb'$ as the dynamics for $\ub$ and $\ub'$ are similar). The results are reported in Figure \ref{fig:synthetic_feat_learn}.
It is clear that both standard training, Mixup training, and Mixup with early stopping can exhibit sufficiently common feature learning, while the rare feature learning of standard training is much lower than those of Mixup and Mixup with early stopping. This verifies Lemmas \ref{lemma:standard_learning_phase3_main} and \ref{lemma:outcome_Mixup_main}. Besides, we can also see that turning off Mixup after a number of iterations will lead to no decrease in rare feature learning and an increase in common feature learning. This verifies Fact \ref{fact:nodecrease} and demonstrates the benefits of early stopping.\\
\begin{figure}[!t]
\vskip -0.1in
     \centering
     \subfigure[Training Loss]{\includegraphics[width=0.49\columnwidth]{figure/earlystop_resnet_noaug_trainloss.pdf}}
      \subfigure[Test Accuracy]{\includegraphics[width=0.49\columnwidth]{figure/earlystop_resnet_noaug.pdf}}
      \vskip -0.1in
    \caption{Training loss (the cross-entropy loss on the mixup data/clean data) and test accuracy achieved by Mixup with different early stopping iterations: 0 (standard), 50, 125, 150, 200 (Mixup), numbers in the legend denote the average accuracy of the last $10$ iterates. The results are evaluated by training ResNet18 on CIFAR-10 dataset without random crop \& flip data augmentation and weight decay regularization.}
\label{fig:early_stop_noaug}
\end{figure}
\paragraph{CIFAR-10 Data.} We further perform the Mixup training on CIFAR-10 dataset to evaluate the performance of early stopping, where we use SGD with momentum $0.9$ and learning rate $0.1$, followed by $\times0.1$ decaying at the $100$-th and $150$-th iterations. We first train the ResNet18 model \citep{he2015delving} via Mixup without other data augmentations and regularizations. We consider applying early stopping at the $0$-th (standard training), $50$-th, $125$-th, $150$-th, and $200$-th (Mixup training) iterations and report the training loss and test accuracy in Figure \ref{fig:early_stop_noaug}. First, it can be observed that the cross-entropy loss on the training data quickly drops to nearly zero after the stopping of Mixup, showing that the neural network has correctly predicted the labels of training data points with high confidence. Besides, the test accuracy results show that 
such a high-confidence fitting on training data will not affect the test performance, while proper early stopping can even gain further improvements, e.g., Mixup with early stopping at the $125$-th iteration achieves substantially higher test accuracy than that of Mixup training. This demonstrates the effectiveness of early-stopped Mixup and backs up our theoretical finding that the benefits of Mixup mainly stem from the early training phase. 

We further perform Mixup training for different neural network models and add the random crop/flip data augmentation and weight decay regularization (set as $10^{-4}$). In particular, we consider two (relatively) high-capacity models: ResNet18 and ResNet34; and two low-capacity models: LeNet and VGG16. For ResNet18 and ResNet34, we set the learning rate as $0.1$; for LeNet and VGG16, we set the learning rate
as $0.02$ and $0.1$ respectively. Then we can clearly see that applying proper early stopping in Mixup will not downgrade the test performance but can even lead to higher test accuracy. In particular, Mixup with early stopping at the $50$-th, $125$-th, and $150$-th iterations can still achieve a substantial performance improvement compared to standard training for LeNet, VGG16, and ResNet18. Moreover, 
we can also observe that Mixup with early stopping at the $150$-th iteration performs better than the standard Mixup for all $4$  models, especially for LeNet and VGG16, two relatively simpler models. This justifies our theoretical findings and demonstrates the benefit of early stopping in Mixup.



\begin{figure*}[!t]
\vskip -0.1in
     \centering
     \subfigure[LeNet]{\includegraphics[width=0.24\textwidth]{figure/earlystop_lenet_aug.pdf}}
      \subfigure[VGG16]{\includegraphics[width=0.24\textwidth]{figure/earlystop_vgg_aug.pdf}}
      \subfigure[ResNet18]{\includegraphics[width=0.24\textwidth]{figure/earlystop_resnet18_aug.pdf}}
      \subfigure[ResNet34]{\includegraphics[width=0.24\textwidth]{figure/earlystop_resnet34_aug.pdf}}
      \vskip -0.1in
    \caption{Test errors achieved by Mixup training with different early stopping iterations: 0 (standard), 50, 125, 150, 200 (Mixup), numbers in the legend denote the average accuracy of the last $10$ iterates. The results are evaluated by training LeNet, VGG16, ResNet18, and ResNet34 on CIFAR-10 dataset  with random crop \& flip data augmentation and weight decay regularization. Experimental results suggest that applying proper early stopping in Mixup will not downgrade the test performance but can even lead to higher test accuracy, especially for simpler models such as LeNet and VGG16.}
    \label{fig:early_stop_aug}
\end{figure*}





% More experiments on other DNN models are deferred to Appendix \ref{sec:add_exp}, where we further demonstrate the benefits of early-stopped Mixup, especially for some simpler models (e.g., LeNet and VGG16).

\section{Conclusion}
In this work, we attempted to develop a comprehensive understanding of the benefits of Mixup training.  We first identified that the benefits cannot be fully explained by the   linearity inductive bias of Mixup. Then we theoretically studied the dynamics of Mixup training from a feature learning. We showed that Mixup is more beneficial in learning rare features compared to standard training. Moreover, our analysis revealed that the benefits of Mixup in feature learning mostly stem from early training stages, based on which we developed the early-stopped Mixup. Our experimental results demonstrated that the early-stopped Mixup can achieve a comparable or even better performance than the standard one, which supports our theoretical findings.



% \newpage
\appendix

\input{proof_standard.tex}
\input{proof_Mixup.tex}


\bibliography{ref}
\bibliographystyle{ims}


















\end{document}