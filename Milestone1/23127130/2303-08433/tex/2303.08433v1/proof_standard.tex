



\section{Detailed Proof for Standard Training}

 


\subsection{Critical Quantities at the Initialization}
Before moving on to the detailed characterization of the dynamics of standard training and Mixup training, we first characterize a set of critical quantities at the initialization. Recall (1) the data model in Definition \ref{def:data_distribution_new} that the feature vectors have unit norm and the noise vectors are randomly generated  from $N(\boldsymbol{0},\sigma_p^2\Ib)$; and (2) the initial model parameter $\wb_{k,r}^{(0)}$ is randomly generated from $N(\boldsymbol{0},\sigma_0^2\Ib)$, we first give the following lemma that characterizes some critical quantities that will be repeatedly used in the later analysis.
\begin{lemma}\label{lemma:initialization}
With probability at least $1-1/\poly(n)$, it holds that for all $i\in[n]$, $k\in[2]$, $r\in[m]$, $\ab\in\{\vb,\ub,\vb',\ub'\}$,
\begin{align*}
&|\la\wb_{k,r}^{(0)},\ab\ra| = \tilde O(\sigma_0), \ \sum_{r\in[m]}\big(\la\wb_{k,r}^{(0)},\ab\ra\big)^2 = \tilde\Theta(\sigma_0^2).
\end{align*}
Additionally, for any noise patch $\bxi\in\{\bxi\}$,
\begin{align*}
|\la\wb_{k,r}^{(0)},\bxi\ra|=\tilde O(d^{1/2}\sigma_p\sigma_0),\ \sum_{r\in[m]}\big(\la\wb_{k,r}^{(0)},\bxi\ra\big)^2 = \tilde \Theta(d\sigma_p^2\sigma_0^2).
\end{align*}
\end{lemma}
\begin{proof}
Note that $\wb_{k,r}^{(0)}$ is randomly generated from $N(\boldsymbol{0},\sigma_0^2\Ib)$. Then using the fact that $m=\polylog(n)$, $\|\ab\|_2^2=1$, and $\|\bxi\|_2^2=\Theta(d\sigma_p^2)$ with probability at least $1-1/\poly(n)$, applying standard concentration arguments can lead to the desired results. 

\end{proof}




\subsection{Feature and Noise Learning of Standard Training}

We first restate the feature and noise learning of standard training as follows: for features, we have
\begin{align}\label{eq:update_features}
\la\wb_{k,r}^{(t+1)},\vb\ra & = \la\wb_{k,r}^{(t)},\ub\ra + \frac{2\eta}{n}\cdot\sum_{i\in[n]} \ell_{k,i}^{(t)} \sum_{p\in \cP_i(\vb)} \la\wb_{k,r}^{(t)},\vb\ra\cdot \alpha_{i,p}^2\|\vb\|_2^2\notag\\
\la\wb_{k,r}^{(t+1)},\ub\ra & = \la\wb_{k,r}^{(t)},\ub\ra + \frac{2\eta}{n}\cdot\sum_{i\in[n]} \ell_{k,i}^{(t)} \sum_{p\in \cP_i(\ub)} \la\wb_{k,r}^{(t)},\ub\ra\cdot \alpha_{i,p}^2\|\ub\|_2^2,\notag\\
\la\wb_{k,r}^{(t+1)},\vb'\ra & = \la\wb_{k,r}^{(t)},\vb\ra + \frac{2\eta}{n}\cdot\sum_{i\in\cS_1^{+}} \ell_{k,i}^{(t)} \sum_{p\in \cP_i(\vb')} \la\wb_{k,r}^{(t)},\vb'\ra\cdot \|\vb'\|_2^2,\notag\\
\la\wb_{k,r}^{(t+1)},\ub'\ra & = \la\wb_{k,r}^{(t)},\ub'\ra + \frac{2\eta}{n}\cdot\sum_{i\in\cS_1^{-}} \ell_{k,i}^{(t)} \sum_{p\in \cP_i(\ub')} \la\wb_{k,r}^{(t)},\ub'\ra\cdot \|\ub'\|_2^2,
\end{align}
where $\cP_i(\ab)$ denotes the set of patches in $\xb_i$ containing the feature $\ab$ and $\alpha_{i,p}^2=1$ if $\xb_i^{(p)}$ is a feature patch and $\alpha_{i,p}^2=\alpha^2$ if $\xb_i^{(p)}$ is the feature noise. 
Additionally, note that the update of rare features only depends on the data in $\cS_1^+$ and $\cS_1^-$ since the data $(\xb_i, y_i)$ in $\cS_0^+$ and $\cS_0^-$ satisfies $\cP_i(\vb')=\emptyset$ and $\cP_i(\ub')=\emptyset$. Similarly, we can also obtain the following result regarding noise learning
\begin{align*}
&\la\wb_{k,r}^{(t+1)},\bxi_s^{(q)}\ra = \la\wb_{k,r}^{(t)},\bxi_s^{(q)}\ra + \frac{2\eta}{n}\cdot\sum_{i=1}^n\ell_{k,i}^{(t)} \sum_{p=1}^P \la\wb_{k,r}^{(t)},\xb_i^{(p)}\ra\cdot \la\xb_i^{(p)}, \bxi_s^{(q)}\ra.
% & = \la\wb_{k,r}^{(t)},\bxi_s^{(q)}\ra + \frac{2\eta}{n}\cdot \ell_{k,s}^{(t)}\cdot\la\wb_{k,r}^{(t)},\bxi_s^{(q)}\ra\cdot\|\bxi_s^{(q)}\|_2^2 + \frac{2\eta}{n}\cdot\sum_{i\neq s || p\neq q} \ell_{k,i}^{(t)}\la\wb_{k,r}^{(t)},\bxi_s^{(q)}\ra\cdot\la\xb_i^{(p)},\bxi_s^{(q)}\ra.
\end{align*}
Moreover, note that if $\xb_i^{(p)}\neq \bxi_s^{(q)}$ (i.e., $i\neq s$ or $p\neq q$), then $|\la\xb_i^{(p)},\bxi_s^{(p)}\ra|$ is in the order of $\tilde O(d^{1/2}\sigma_p^2)$.
% depending on whether $\bxi_s^{(p)}$ is the feature patch or noise patch. 
Therefore, we further have
\begin{align}\label{eq:update_noise_simplified}
\la\wb_{k,r}^{(t+1)},\bxi_s^{(q)}\ra 
& = \la\wb_{k,r}^{(t)},\bxi_s^{(q)}\ra \cdot\bigg[ 1 + \frac{2\eta}{n}\cdot \ell_{k,s}^{(t)}\cdot\|\bxi_s^{(q)}\|_2^2\bigg]  \pm \frac{2\eta}{n}\cdot\sum_{i\neq s || p\neq q} |\ell_{k,i}^{(t)}|\cdot|\la\wb_{k,r}^{(t)},\bxi_i^{(q)}\ra|\cdot \tilde O\big(d^{1/2}\sigma_p^2\big).
\end{align}


\paragraph{Phase 1, Fitting Common Feature Data.}

The following lemma characterizes the learning of all feature and noise vectors in Phase 1.

\begin{lemma}[Phase 1, Standard Training]\label{lemma:learning_phase1}
Let $T_0$ be the iteration number such that the neural network output satisfies $|F_k(\Wb^{(t)};\xb_i)|\le O(1)$ for all $t\le T_0$ and $i\in[n]$, then for any $t\le T_0$, it holds that
\begin{align*}
\la\wb_{1,r}^{(t+1)},\vb\ra  = \la\wb_{1,r}^{(t)},\vb\ra\cdot \big(1 + \Theta(\eta)\big),\quad
\la\wb_{2,r}^{(t+1)},\ub\ra  = \la\wb_{2,r}^{(t)},\ub\ra\cdot \big(1 + \Theta(\eta)\big).
\end{align*}
Besides, we also have for any $t\le T_0$, $r\in[m]$, $k\in[2]$, $q\in[P]$, and $s\in[n]$,
\begin{align*}
&|\la\wb_{2,r}^{(t)},\vb\ra| = \tilde O(\sigma_0),\quad |\la\wb_{1,r}^{(t)},\ub\ra| = \tilde O(\sigma_0),\\
&|\la\wb_{k,r}^{(t)},\vb'\ra| = \tilde O(\sigma_0),\quad |\la\wb_{k,r}^{(t)},\ub'\ra| = \tilde O(\sigma_0), \quad|\la\wb_{k,r}^{(t)}, \bxi_s^{(q)}\ra| = \tilde O\big(d^{1/2}\sigma_p\sigma_0\big).
\end{align*}
\end{lemma}
\begin{proof}
First, note that in the first stage, the neural network outputs are in the order of $O(1)$, implying that the loss derivatives satisfy $|\ell_{k,i}^{(t)}| = \Theta(1)$. More specifically, we can get that $\ell_{k,i}^{(t)} = \Theta(1)$ if $k=y_i$ and $\ell_{k,i}^{(t)} = -\Theta(1)$ otherwise. Then by \eqref{eq:update_features}, we have
\begin{align*}
\la\wb_{1,r}^{(t+1)}, \vb\ra = \la\wb_{1,r}^{(t)}, \vb\ra\cdot\bigg[1 + \frac{2\eta}{n}\cdot\sum_{i\in\cS_0^+} \ell_{1,i}^{(t)} \sum_{p\in \cP_i(\vb)} \alpha_{i,p}^2\|\vb\|_2^2 + \frac{2\eta}{n}\cdot\sum_{i\in[n]\backslash\cS_0^+} \ell_{1,i}^{(t)} \sum_{p\in \cP_i(\vb)}  \alpha_{i,p}^2\|\vb\|_2^2\bigg].
\end{align*}
Note that by Definition \ref{def:data_distribution_new}, for any data $i\in[n]$ let $\cP_i'(\vb)$ and $\cP_i'(\ub)$ be the set of patches corresponding to the feature noise vectors $\vb$ and $\ub$ respectively, we have $|\cP_i'(\vb)|\le b$ and $\sum_{p\in\cP'_i(\vb)}\alpha_{i,p}^2\le b\alpha^2=o\big(1/\polylog(n)\big)$. Additionally, note that $\ell_{1,i}^{(t)}=\Theta(1)$ for $i\in\cS_0^+$ and $\cP_i(\vb) = \cP_i'(\vb)$ for all $i\in[n]\backslash \cS_0^+$, we have
\begin{align}\label{eq:update_v_phase1_std}
\la\wb_{1,r}^{(t+1)}, \vb\ra = \la\wb_{1,r}^{(t)}, \vb\ra\cdot\bigg[1 + \frac{2\eta}{n}\cdot |\cS_0^+|\cdot C_v^{(t)} \pm o\big(\eta/\polylog(n)\big)\bigg] = \la\wb_{1,r}^{(t)}, \vb\ra\cdot \big[1+\Theta(\eta)\big], 
\end{align}
where $C_v^{(t)}=|\cS_0^+|^{-1}\cdot\sum_{i\in\cS_0^+}\ell_{1,i}^{(t)}$ remains in the constant level for all $t\le T_0$.
Similarly, we can also get that 
\begin{align}\label{eq:update_u_phase1_std}
\la\wb_{2,r}^{(t+1)}, \ub\ra = \la\wb_{2,r}^{(t)}, \ub\ra\cdot\bigg[1 + \frac{2\eta}{n}\cdot |\cS_0^-|\cdot C_u^{(t)} \pm o\big(\eta/\polylog(n)\big)\bigg] =  \la\wb_{2,r}^{(t)}, \ub\ra\cdot \big[1+\Theta(\eta)\big],
\end{align}
where $C_u^{(t)}=|\cS_0^-|^{-1}\cdot\sum_{i\in\cS_0^-}\ell_{2,i}^{(t)}$ remains in the constant level for all $t\le T_0$.
Moreover, in terms of the learning of wrong features, we have
\begin{align}\label{eq:update_wrongstrongfeat}
\la\wb_{2,r}^{(t+1)},\vb\ra &= \la\wb_{2,r}^{(t)}, \vb\ra\cdot\bigg[1 + \frac{2\eta}{n}\cdot\sum_{i\in\cS_0^+} \ell_{2,i}^{(t)} \sum_{p\in \cP_i(\vb)} \alpha_{i,p}^2\|\vb\|_2^2 + \frac{2\eta}{n}\cdot\sum_{i\in[n]\backslash\cS_0^+} \ell_{2,i}^{(t)} \sum_{p\in \cP_i(\vb)}  \alpha_{i,p}^2\|\vb\|_2^2\bigg]\notag\\
& = \la\wb_{2,r}^{(t)}, \vb\ra\cdot\bigg[1 - \frac{2\eta}{n}\cdot |\cS_0^+|\cdot\Theta(1)\pm o(\eta)\bigg]\notag\\
& = \la\wb_{2,r}^{(t)}, \vb\ra\cdot\big[1 - \Theta(\eta)\big].
\end{align}
Then by Lemma \ref{lemma:initialization}, this further implies that for all $t$ in the first stage, we have
\begin{align}\label{eq:bound_wrongstrongfeat}
|\la\wb_{2,r}^{(t)},\vb\ra|\le |\la\wb_{2,r}^{(t-1)},\vb\ra|\le\dots\le |\la\wb_{2,r}^{(0)},\vb\ra|=\tilde O(\sigma_0).
\end{align}

Now we can move on to the learning of rare features and noise vectors. Particularly, for rare features, we have
\begin{align*}
\la\wb_{1,r}^{(t+1)}, \vb'\ra &= \la\wb_{1,r}^{(t)}, \vb'\ra\cdot \bigg[1 + \frac{2\eta}{n}\cdot \sum_{i\in\cS_1^+}\ell_{k,i}^{(t)}\cdot \sum_{p\in\cP_i(\vb')}\|\vb'\|_2^2\bigg] \notag\\
& = \la\wb_{1,r}^{(t)}, \vb'\ra\cdot \bigg[1 + \Theta\bigg(\frac{\eta|\cS_1^+|}{n}\bigg)\bigg]\notag\\
& = \la\wb_{1,r}^{(t)}, \vb'\ra\cdot \big[1 + \Theta(\rho \eta)\big],
\end{align*}
where the second equality is due to $|\cP_i(\vb')|=\Theta(1)$ and the last equality is due to $|\cS_1^+| = \Theta(\rho n)$ with probability at least $1-1/\poly(n)$. Therefore, by Lemma \ref{lemma:initialization}, we can then obtain
\begin{align*}
|\la\wb_{1,r}^{(t)}, \vb'\ra|\le \big[1+\Theta(\rho\eta)\big]^{t}\cdot |\la\wb_{1,r}^{(t)}, \vb'\ra|\le \tilde O(\sigma_0)\cdot e^{\Theta(T_0\eta)}=\tilde O(\sigma_0),
\end{align*}
where we use the fact that $T_0 = \tilde O(1/\eta)$.
Similarly, it also follows that
\begin{align*}
\la\wb_{1,r}^{(t+1)}, \ub'\ra = \la\wb_{1,r}^{(t)}, \ub'\ra\cdot [1 + \Theta(\rho \eta)] = \tilde O(\sigma_0).
\end{align*}
Moreover, using the fact that $\ell_{2,i}^{(t)}= -\Theta(1)$ for $i\in\cS_1^+$ and $\ell_{1,i}^{(t)}= -\Theta(1)$ for $i\in\cS_2^+$, we can follow the same proof in  \eqref{eq:update_wrongstrongfeat} and \eqref{eq:bound_wrongstrongfeat} and get
\begin{align*}
|\la\wb_{2,r}^{(t)}, \vb'\ra| \le |\la\wb_{2,r}^{(0)}, \vb'\ra|=\tilde O(\sigma_0), \ |\la\wb_{1,r}^{(t)}, \ub'\ra| \le |\la\wb_{1,r}^{(0)}, \ub'\ra| =\tilde O(\sigma_0).
\end{align*}
where the results for $|\la\wb_{1,r}^{(t)}, \vb'\ra|$ and $|\la\wb_{1,r}^{(t)}, \ub'\ra|$ are by Lemma \ref{lemma:initialization}.


Finally, regarding the learning of the noise vector $\bxi_s^{(q)}$, if $j = y_s$, we have the following by \eqref{eq:update_noise_simplified},
\begin{align*}
\max_{s,r}|\la\wb_{k,r}^{(t+1)},\bxi_s^{(q)}\ra| \le \max_{s,r}|\la\wb_{k,r}^{(t)},\bxi_s^{(q)}\ra |\cdot\bigg[1 + \frac{\eta}{n}\cdot \tilde \Theta(d\sigma_p^2) + \frac{\eta}{n}\cdot \tilde O\big(nP d^{1/2}\sigma_p^2\big)\bigg].
\end{align*}
Note that we have $nP=o(d^{1/2})$, then the above equation further leads to
\begin{align*}
\max_{s,r}|\la\wb_{k,r}^{(t+1)},\bxi_s^{(q)}\ra| =\max_{s,r}|\la\wb_{k,r}^{(t)},\bxi_s^{(q)}\ra| \cdot\bigg[1 + \frac{\eta}{n}\cdot \tilde \Theta(d\sigma_p^2) \bigg].
\end{align*}
Besides, we can also get if $k\neq y_s$,
\begin{align*}
\max_{s,r}|\la\wb_{k,r}^{(t+1)},\bxi_s^{(q)}\ra| \le \max_{s,r}|\la\wb_{k,r}^{(t)},\bxi_s^{(q)}\ra| \cdot\bigg[1 - \frac{\eta}{n}\cdot \tilde \Theta(d\sigma_p^2) \bigg].
\end{align*}
Then for any $t\le T_0 = \tilde O(1/\eta)$ and any $k$, we have
\begin{align*}
\max_{s,r}|\la\wb_{k,r}^{(t)},\bxi_s^{(q)}| &\le \max_{s,r}|\la\wb_{k,r}^{(0)},\bxi_s^{(q)}\ra| \cdot \bigg[1 + \frac{\eta}{n}\cdot \tilde \Theta(d\sigma_p^2) \bigg]^t\notag\\
&\le \max_{s,r}|\la\wb_{k,r}^{(0)},\bxi_s^{(q)}\ra|\cdot \bigg[1 + \frac{\eta}{n}\cdot \tilde \Theta(d\sigma_p^2) \bigg]^{T_0}\notag\\
&\le \max_{s,r}|\la\wb_{k,r}^{(0)},\bxi_s^{(q)}\ra|\cdot \exp\big\{\tilde\Theta(\eta T_0 d \sigma_p^2/n)\big\}\notag\\
& \le \max_{s,r}|\la\wb_{k,r}^{(0)},\bxi_s^{(q)}\ra|\cdot \Theta(1)\notag\\
& = \tilde \Theta(d^{1/2}\sigma_0\sigma_p).
\end{align*}
This completes the proof.
\end{proof}


\begin{lemma}\label{lemma:results_phase1}
At the end of Phase 1 with maximum iteration number $T_0 = \tilde O(1/\eta)$, we have
\begin{align*}
\sum_{r=1}^m (\la\wb_{1,r}^{(T_0)},\vb\ra)^2 = \tilde\Theta(1), \ \sum_{r=1}^m (\la\wb_{2,r}^{(T_0)},\ub\ra)^2 = \tilde\Theta(1);
\end{align*}
besides, it holds that
\begin{align*}
|\la\wb_{2,r}^{(T_0)}, \vb\ra|, |\la\wb_{1,r}^{(T_0)}, \ub\ra|, |\la\wb_{k,r}^{(T_0)}, \ub'\ra|, |\la\wb_{k,r}^{(T_0)}, \vb'\ra|=\tilde O(\sigma_0); \quad|\la\wb_{k,r}^{(T_0)}, \bxi\ra|=\tilde O(d^{1/2}\sigma_p\sigma_0)
\end{align*}
for all $k\in[2]$, $r\in[m]$ and $\bxi\in\{\bxi\}$.
\end{lemma}
\begin{proof}
We first characterize the difference between $C_v^{(t)}$ and $C_u^{(t)}$ in \eqref{eq:update_v_phase1_std} and \eqref{eq:update_u_phase1_std}. Particularly, we consider the iterations that $\max_{i\in[n], k\in[2]}|F_k(\Wb^{(t)};\xb_i)|\le \zeta$ for some $\zeta = \Theta\big(1/\log(1/\sigma_0)\big)=\Theta(1/\polylog(n))$, then we can immediately get that it holds that $|\ell_{1,i}^{(t)}-0.5|\le O(\zeta)$ for all $i\in\cS_0^+$ and $|\ell_{2,i}^{(t)}-0.5|\le O(\zeta)$ for all $i\in\cS_0^-$. Therefore, we can further get
\begin{align*}
 C_v^{(t)} = \frac{1}{|\cS_0^+|}\cdot \sum_{i\in\cS_0^+}\ell_{1,i}^{(t)} = 0.5 \pm O(\zeta),\quad C_u^{(t)} = \frac{1}{|\cS_0^-|}\cdot \sum_{i\in\cS_0^-}\ell_{2,i}^{(t)} = 0.5 \pm O(\zeta).
\end{align*}
Further note that the positive and negative data are independently generated from the data distribution, which implies that with probability at least $1-1/\poly(n)$, it holds that $||\cS_0^+| - (1-\rho)n/2|\le \tilde O(n^{1/2})$ and $||\cS_0^-| - (1-\rho)n/2|\le \tilde O(n^{1/2})$. Therefore, applying the fact that $\zeta = \Theta(1/\polylog(n))$,  we can obtain the following by \eqref{eq:update_v_phase1_std} and \eqref{eq:update_u_phase1_std} 
\begin{align}\label{eq:update_strong_feature_phase1_tmp1}
\sum_{r=1}^m(\la\wb_{1,r}^{(t+1)},\vb\ra)^2 &= \sum_{r=1}^m(\la\wb_{1,r}^{(t)},\vb\ra)^2\cdot \bigg[1+(1-\rho)\eta \pm O(\zeta\eta))\bigg]\notag\\
\sum_{r=1}^m(\la\wb_{2,r}^{(t+1)},\ub\ra)^2 &= \sum_{r=1}^m(\la\wb_{2,r}^{(t)},\ub\ra)^2\cdot \bigg[1+(1-\rho)\eta \pm O(\zeta\eta))\bigg].
\end{align}
Then let $T_0'$ be the largest iteration number such that $\max_{k,i}|F_k(\Wb^{(t)};\xb_i)|\le \zeta$, which clearly satisfies $T_0'<T_0$ ($T_0$ is defined in Lemma \ref{lemma:learning_phase1}), applying Lemma \ref{lemma:learning_phase1} and considering the data $i$ with largest neural network output (w.o.l.g assuming it's positive data),
\begin{align*}
\sum_{r=1}^m(\la\wb_{1,r}^{(T_0'+1)},\vb\ra)^2 \ge  c\cdot F_1(\Wb^{(T_0'+1)};\xb_i) \ge c\cdot\zeta
\end{align*}
for some absolute constant $c$. By \eqref{eq:update_strong_feature_phase1_tmp1}, we can immediately obtain that $T_0'=\Theta(\log(\zeta/(m\sigma_0^2))/\eta)$, where we apply the initialization results in Lemma \ref{lemma:initialization}. Besides, we can also obtain that
\begin{align*}
\frac{\sum_{r=1}^m(\la\wb_{2,r}^{(t+1)},\ub\ra)^2}{\sum_{r=1}^m(\la\wb_{1,r}^{(t+1)},\vb\ra)^2 } &\ge  \frac{\sum_{r=1}^m(\la\wb_{2,r}^{(0)},\ub\ra)^2}{\sum_{r=1}^m(\la\wb_{1,r}^{(0)},\vb\ra)^2 }\cdot \bigg(\frac{1+(1-\rho)\eta - O(\zeta \eta))}{1+(1-\rho)\eta + O(\zeta\eta)}\bigg)^{T_0'}\notag\\
& = \tilde\Theta(1)\cdot \big(1 - O(\eta\zeta T_0) \big).
\end{align*}
Then note that $\zeta = \Theta\big(1/\log(1/\sigma_0)\big)$, we can get $\zeta T_0\eta = \Theta\big(\zeta\log(\zeta) + \zeta \log(1/(m\sigma_0^2))\big)=o(1)$, which implies that $\sum_{r=1}^m(\la\wb_{2,r}^{(T_0'+1)},\ub\ra)^2\ge \Theta(\zeta)$.  Finally, by Lemma \ref{lemma:learning_phase1}, we know that $\sum_{r=1}^m(\la\wb_{1,r}^{(t)},\vb\ra)^2$
and $\sum_{r=1}^m(\la\wb_{2,r}^{(t+1)},\ub\ra)^2$ will keep increasing for all $t\le T_0$. Then based on the definition of $T_0$ and the fact that $\zeta = \tilde\Theta(1)$, we can conclude that 
\begin{align*}
\sum_{r=1}^m (\la\wb_{1,r}^{(T_0)},\vb\ra)^2 = \tilde\Theta(1), \ \sum_{r=1}^m (\la\wb_{2,r}^{(T_0)},\ub\ra)^2 = \tilde\Theta(1).
\end{align*}
The remaining arguments in this lemma directly follow from Lemma \ref{lemma:learning_phase1}, thus we omit their proof here.
\end{proof}






\paragraph{Phase 2. Fitting Rare Feature Data.} After \textbf{Phase 1}, the neural network output will become larger so that the loss derivatives (i.e, $\ell_{k,i}^{(t)}$) or the output logits may no longer be viewed as a quantity in the constant order.
Particularly, as shown in Lemma \ref{lemma:results_phase1}, when $t>T_0$, the feature learning, i.e., $\la\wb_{1,r}^{(t)},\vb\ra$ and $\la\wb_{2,r}^{(t)},\ub\ra$ will reach the constant order, implying that $|\ell_{k,i}^{(t)}|$ will be closer to $1$ or $0$ for all common feature data. Additionally, the loss derivative will remain in the constant order for the rare feature data, since either the rare feature learning (e.g, $\la\wb_{1,r}^{(t)}, \vb'\ra$) or the noise learning (e.g., $\la\wb_{1,r}^{(t)},\bxi_i^{(p)}\ra$) will be in the order of $o\big(1/\polylog(n)\big)$, so that the corresponding neural network outputs are also in the order of $o\big(1/\polylog(n)\big)$. Therefore, we define \textbf{Phase 2} by the period that (1) is after \textbf{Phase 1} and (2) the neural network outputs for the rare feature data are still in the order of $O\big(1/\polylog(n)\big)$ (or equivalently, the loss derivatives of rare feature data are in the constant order.)

Then, similar to the analysis in Phase 1, we will also characterize the learning of feature and noise separately. Regarding the learning of common feature, by \eqref{eq:update_features}, we have
\begin{align}\label{eq:update_feature_v_phase2}
\la\wb_{k,r}^{(t+1)},\vb\ra & = \la\wb_{k,r}^{(t)},\vb\ra + \frac{2\eta}{n}\cdot\sum_{i\in[n]} \ell_{k,i}^{(t)} \sum_{p\in \cP_i(\vb)} \la\wb_{k,r}^{(t)},\vb\ra\cdot \alpha_{i,p}^2\notag\\
& = \la\wb_{k,r}^{(t)},\vb\ra\cdot\bigg[1 + \frac{2\eta}{n}\cdot\bigg(\sum_{i\in\cS_0^+}\ell_{k,i}^{(t)}\sum_{p\in\cP_i(\vb)}\alpha_{i,p}^2 +\sum_{i\in\cS_0^-}\ell_{k,i}^{(t)}\sum_{p\in\cP_i(\vb)}\alpha_{i,p}^2+\sum_{\cS_1^+\cup\cS_1^-}\ell_{k,i}^{(t)}\sum_{p\in\cP_i(\vb)}\alpha_{i,p}^2 \bigg)\bigg].
\end{align}
Similarly, we can also get that
\begin{align}\label{eq:update_feature_u_phase2}
\la\wb_{k,r}^{(t+1)},\ub\ra 
& = \la\wb_{k,r}^{(t)},\ub\ra\cdot\bigg[1 + \frac{2\eta}{n}\cdot\bigg(\sum_{i\in\cS_0^+}\ell_{k,i}^{(t)}\sum_{p\in\cP_i(\ub)}\alpha_{i,p}^2 +\sum_{i\in\cS_0^-}\ell_{k,i}^{(t)}\sum_{p\in\cP_i(\ub)}\alpha_{i,p}^2+\sum_{\cS_1^+\cup\cS_1^-}\ell_{k,i}^{(t)}\sum_{p\in\cP_i(\ub)}\alpha_{i,p}^2 \bigg)\bigg].
\end{align}
Moreover, according to the data distribution in Definition \ref{def:data_distribution_new}, we have
\begin{itemize}
    \item For any $i\in\cS_0^+$, it holds that $\sum_{p\in\cP_i(\vb)}\alpha_{i,p}^2 = \Theta(1)$ and $\sum_{p\in\cP_i(\ub)}\alpha_{i,p}^2 = b\alpha^2 = o\big(1/\polylog(n)\big)$.
    \item For any $i\in\cS_0^-$, it holds that $\sum_{p\in\cP_i(\ub)}\alpha_{i,p}^2 = \Theta(1)$ and $\sum_{p\in\cP_i(\vb)}\alpha_{i,p}^2 = b\alpha^2 = o\big(1/\polylog(n)\big)$.
    \item For any $i\in\cS_1^+\cup\cS_1^-$, it holds that $\sum_{p\in\cP_i(\ub)}\alpha_{i,p}^2 = b\alpha^2 = o\big(1/\polylog(n)\big)$ and $\sum_{p\in\cP_i(\vb)}\alpha_{i,p}^2 = b\alpha^2 = o\big(1/\polylog(n)\big)$
\end{itemize}
Therefore, we have the following results regarding the relation between $\la\wb_{k,r}^{(t)},\vb\ra$ and $\la\wb_{k,r}^{(t)},\ub\ra$.

\begin{lemma}\label{lemma:update_feature_relation_phase2}
Let $T_1'=O\big(1/(\eta\rho b\alpha^2)\big)$ be a  quantity that is greater than $T_0$, then for any $t\in [T_0, T_1]$, there exists an absolute constant $C$ such that
\begin{align*}
\frac{|\la \wb_{1,r}^{(t)}, \vb \ra|}{|\la \wb_{1,r}^{(t)}, \ub\ra|}\ge C\cdot \frac{|\la \wb_{1,r}^{(T_0)}, \vb \ra|}{|\la \wb_{1,r}^{(T_0)}, \ub\ra|}=\tilde\Omega\bigg(\frac{1}{\sigma_0}\bigg),\quad\mbox{and}\quad
\frac{|\la \wb_{2,r}^{(t)}, \ub \ra|}{|\la \wb_{2,r}^{(t)}, \vb\ra|}\ge C\cdot \frac{|\la \wb_{2,r}^{(T_0)}, \ub \ra|}{|\la \wb_{2,r}^{(T_0)}, \vb\ra|}=\tilde\Omega\bigg(\frac{1}{\sigma_0}\bigg).
\end{align*}
\end{lemma}
\begin{proof}
Based on the update rules in \eqref{eq:update_feature_v_phase2} and \eqref{eq:update_feature_u_phase2}, we have
\begin{align*}
\la\wb_{1,r}^{(t+1)},\vb\ra  &= \la\wb_{1,r}^{(t)},\vb\ra\cdot\bigg[1 + \frac{\eta}{n}\cdot\bigg(\Theta(1)\cdot\sum_{i\in\cS_0^+}\ell_{1,i}^{(t)} +o\big(1/\polylog(n)\big)\cdot\sum_{i\in\cS_0^-}\ell_{1,i}^{(t)}\pm O(\rho n b\alpha^2) \bigg)\bigg];\notag\\
\la\wb_{1,r}^{(t+1)},\ub\ra  &= \la\wb_{1,r}^{(t)},\ub\ra\cdot\bigg[1 + \frac{\eta}{n}\cdot\bigg(\Theta(1)\cdot\sum_{i\in\cS_0^-}\ell_{1,i}^{(t)} +o\big(1/\polylog(n)\big)\cdot\sum_{i\in\cS_0^+}\ell_{1,i}^{(t)}\pm O(\rho n b\alpha^2) \bigg)\bigg].
\end{align*}
where we use the fact that $|\ell_{k,i}^{(t)}|\le 1$. 
This further implies that
\begin{align*}
\frac{|\la\wb_{1,r}^{(t+1)},\vb\ra|}{|\la\wb_{1,r}^{(t+1)},\ub\ra|} = \frac{|\la\wb_{1,r}^{(t)},\vb\ra|}{|\la\wb_{1,r}^{(t)},\ub\ra|}\cdot \underbrace{\frac{1 +\frac{\eta}{n}\cdot\bigg(\Theta(1)\cdot\sum_{i\in\cS_0^+}\ell_{1,i}^{(t)} +o\big(1/\polylog(n)\big)\cdot\sum_{i\in\cS_0^-}\ell_{1,i}^{(t)}\pm O(\rho n b\alpha^2) \bigg) }{1 + \frac{\eta}{n}\cdot\bigg(\Theta(1)\cdot\sum_{i\in\cS_0^-}\ell_{1,i}^{(t)} +o\big(1/\polylog(n)\big)\cdot\sum_{i\in\cS_0^+}\ell_{1,i}^{(t)}\pm O(\rho n b\alpha^2) \bigg)}}_{\star}.
\end{align*}
Note that we have $\ell_{1,i}^{(t)}>0$ for $i\in\cS_0^+$ and $\ell_{1,i}^{(t)}>0$ for $i\in\cS_0^-$. Then it can be readily verified that
\begin{align*}
\Theta(1)\cdot\sum_{i\in\cS_0^+}\ell_{1,i}^{(t)} +o\big(1/\polylog(n)\big)\cdot\sum_{i\in\cS_0^-}\ell_{1,i}^{(t)}\ge \Theta(1)\cdot\sum_{i\in\cS_0^-}\ell_{1,i}^{(t)} +o\big(1/\polylog(n)\big)\cdot\sum_{i\in\cS_0^+}\ell_{1,i}^{(t)}.
\end{align*}
Then we can get that
\begin{align*}
(\star)\ge 1 - \frac{O(\rho \eta b \alpha^2)}{1 + \frac{\eta}{n}\cdot\bigg(\Theta(1)\cdot\sum_{i\in\cS_0^-}\ell_{1,i}^{(t)} +o\big(1/\polylog(n)\big)\cdot\sum_{i\in\cS_0^+}\ell_{1,i}^{(t)}\pm O(\rho n b\alpha^2) \bigg)}\ge 1 - O(\rho \eta b \alpha^2). 
\end{align*}
Therefore we have for all $t\in[T_0, T_1']$,
\begin{align*}
\frac{|\la \wb_{1,r}^{(t)}, \vb \ra|}{|\la \wb_{1,r}^{(t)}, \ub\ra|}\ge \frac{|\la \wb_{1,r}^{(T_0)}, \vb \ra|}{|\la \wb_{1,r}^{(T_0)}, \ub\ra|}\cdot \big[1- O(\rho \eta b \alpha^2) \big]^{T_1' - T_0}\ge \frac{|\la \wb_{1,r}^{(T_0)}, \vb \ra|}{|\la \wb_{1,r}^{(T_0)}, \ub\ra|}\cdot \big[1- O(\rho \eta b \alpha^2) \big]^{O\big(\frac{1}{\rho \eta b \alpha^2}\big)}.
\end{align*}
Then applying the fact that $\big[1- O(\rho \eta b \alpha^2) \big]^{O\big(\frac{1}{\rho \eta b \alpha^2}\big)} \ge C$ holds
for some absolute constant $C$, we are able to complete the proof for bounding $\frac{|\la \wb_{2,r}^{(t)}, \ub \ra|}{|\la \wb_{2,r}^{(t)}, \vb\ra|}$. The results on $\frac{|\la \wb_{2,r}^{(t)}, \ub \ra|}{|\la \wb_{2,r}^{(t)}, \vb\ra|}$ can be obtained similarly. 
\end{proof}

In the next step, we will show that the learning of common features $\vb$ and $\ub$ will not be too large, i.e., exceeding the $\polylog(n)$ order. 
\begin{lemma}\label{lemma:upperbound_feature_phase2}
Let $T_1'$ be the same quantity defined in Lemma \ref{lemma:update_feature_relation_phase2}, we have for all $t\in[T_0, T_1']$, it holds that
\begin{align*}
|\la\wb_{1,r}^{(t+1)},\vb\ra|, |\la\wb_{2,r}^{(t+1)},\ub\ra| \le O\big(\polylog(n)\big).
\end{align*}
\end{lemma}
\begin{proof}[Proof of Lemma \ref{lemma:upperbound_feature_phase2}]
Based on the update rules in \eqref{eq:update_feature_v_phase2} and \eqref{eq:update_feature_u_phase2}, we have
\begin{align*}
\la\wb_{1,r}^{(t+1)},\vb\ra  &= \la\wb_{1,r}^{(t)},\vb\ra\cdot\bigg[1 + \frac{\eta}{n}\cdot\bigg(\Theta(1)\cdot\sum_{i\in\cS_0^+}\ell_{1,i}^{(t)} +o\big(1/\polylog(n)\big)\cdot\sum_{i\in\cS_0^-}\ell_{1,i}^{(t)}\pm O(\rho n b\alpha^2) \bigg)\bigg].
\end{align*}
Using the fact that $\ell_{1,i}^{(t)}<0$ for all $i\in\cS_0^{-}$, we further have
\begin{align*}
(\la\wb_{1,r}^{(t+1)},\vb\ra)^2  &\le (\la\wb_{1,r}^{(t)},\vb\ra)^2\cdot\bigg[1 + \frac{\eta}{n}\cdot\bigg(\Theta(1)\cdot\sum_{i\in\cS_0^+}\ell_{1,i}^{(t)} + O(\rho n b\alpha^2) \bigg)\bigg]^2\notag\\
& = (\la\wb_{1,r}^{(t)},\vb\ra)^2\cdot\bigg[1 + \frac{\eta}{n}\cdot\bigg(\Theta(1)\cdot\sum_{i\in\cS_0^+}\ell_{1,i}^{(t)} + O(\rho n b\alpha^2) \bigg)\bigg],
\end{align*}
where the second equality holds since $(1+o(1))^2= 1+o(1)$. Further take a summation over $r\in[m]$ leads to
\begin{align}\label{eq:feature_learning_v_allneurons_phase2}
\sum_{r=1}^m (\la\wb_{1,r}^{(t+1)},\vb\ra)^2 \le \bigg[\sum_{r=1}^m (\la\wb_{1,r}^{(t)},\vb\ra)^2\bigg]\cdot \bigg[1 + \frac{\eta}{n}\cdot\bigg(\Theta(1)\cdot\sum_{i\in\cS_0^+}\ell_{1,i}^{(t)} + O(\rho n b\alpha^2) \bigg)\bigg].
\end{align}
Similarly, we can also get that
\begin{align}\label{eq:feature_learning_u_allneurons_phase2}
\sum_{r=1}^m (\la\wb_{2,r}^{(t+1)},\ub\ra)^2 \le \bigg[\sum_{r=1}^m (\la\wb_{1,r}^{(t)},\ub\ra)^2\bigg]\cdot \bigg[1 + \frac{\eta}{n}\cdot\bigg(\Theta(1)\cdot\sum_{i\in\cS_0^-}\ell_{1,i}^{(t)} + O(\rho n b\alpha^2) \bigg)\bigg].
\end{align}
Regarding the loss derivative $\ell_{1,i}^{(t)}$, we can get that for any $i\in\cS_0^+$,
\begin{align}\label{eq:upperbound_lossderivate}
\ell_{1,i}^{(t)} &= 1 - \mathrm{Logit}_1(\Wb^{(t)}; \xb_i)=\frac{\exp\big[F_2(\Wb^{(t)};\xb_i)-F_1(\Wb^{(t)};\xb_i)\big]}{1 + \exp\big[F_2(\Wb^{(t)};\xb_i)-F_1(\Wb^{(t)};\xb_i)\big]}\le \exp\big[F_2(\Wb^{(t)};\xb_i)-F_1(\Wb^{(t)};\xb_i)\big]
\end{align}
Before moving to the analysis on the feature, we first show that the model weight corresponding to the wrong label will not learn the noise of the data, i.e., $|\la\wb_{2,r}^{(t)},\bxi_s^{(q)}\ra|$
will be very small for all $q\in[P]$ and $s\in\cS_0^+$. Particularly, we have the following by \eqref{eq:update_noise_simplified}
\begin{align*}
\max_{r,s}|\la\wb_{2,r}^{(t+1)},\bxi_s^{(q)}\ra |&\le \max_{r,s}|\la\wb_{2,r}^{(t)},\bxi_s^{(q)}\ra| \cdot\bigg[ 1 + \frac{\eta}{n}\cdot \ell_{2,s}^{(t)}\cdot\tilde\Theta(d\sigma_p^2)+ \frac{\eta}{n}\cdot\sum_{i\neq s || p\neq q} |\ell_{2,s}^{(t)}|\cdot \tilde O\big(d^{1/2}\sigma_p^2\big)\bigg]\notag\\
&\le \max_{r,s}|\la\wb_{2,r}^{(t)},\bxi_s^{(q)}\ra| \cdot\bigg[ 1 +  \frac{\eta}{n}\cdot\tilde O\big(nPd^{1/2}\sigma_p^2\big)\bigg],
\end{align*}
where the second inequality is due to $|\ell_{k,i}^{(t)}|\le 1$ and $\ell_{2,s}^{(t)}<0$ for $s\in\cS_0^+$. Therefore, we can get that for all $t\in\big[T_0, T_1'\big]$, where $T_1'\le \tilde O\big(1/(\eta Pd^{1/2}\sigma_p^2)\big)$, that
\begin{align}\label{eq:bound_noise_strong_phase2}
\max_{r,s}|\la\wb_{2,r}^{(t)},\bxi_s^{(q)}\ra|&\le \max_{r,s}|\la\wb_{2,r}^{(T_0)},\bxi_s^{(q)}\ra|\cdot \bigg[ 1 +  \tilde O\big(\eta Pd^{1/2}\sigma_p^2\big)\bigg]^{\tilde O\big(\frac{1}{\eta Pd^{1/2}\sigma_p^2}\big)}\notag\\
&\le C\cdot \max_{r,s}|\la\wb_{2,r}^{(T_0)},\bxi_s^{(q)}\ra| =\tilde O (d^{1/2}\sigma_p\sigma_0),
\end{align}
where the last equality is by Lemma \ref{lemma:learning_phase1}. Therefore, we can get the following bound on $F_2(\Wb^{(t)};\xb_i)-F_1(\Wb^{(t)};\xb_i)$ for any $i\in \cS_0^+$,
\begin{align*}
F_2(\Wb^{(t)};\xb_i)-F_1(\Wb^{(t)};\xb_i)& = \sum_{r=1}^m\sum_{p=1}^P(\la\wb_{2,r}^{(t)},\xb_i^{(p)}\ra)^2-\sum_{r=1}^m\sum_{p=1}^P(\la\wb_{1,r}^{(t)},\xb_i^{(p)}\ra)^2\notag\\
&\le \sum_{r=1}^m\sum_{p\in\cP_i(\vb)}(\la\wb_{2,r}^{(t)},\vb\ra)^2 + \alpha^2\sum_{r=1}^m\sum_{p\in\cP_i(\ub)}(\la\wb_{2,r}^{(t)},\ub\ra)^2 \notag\\
&\qquad+ \sum_{r=1}^m\sum_{p\in\cP_i(\bxi)}(\la\wb_{2,r}^{(t)},\bxi_i^{(p)}\ra)^2 - \sum_{r=1}^m(\la\wb_{1,r}^{(t)},\vb\ra)^2.
\end{align*}
Then by Lemma \ref{lemma:update_feature_relation_phase2} and \eqref{eq:bound_noise_strong_phase2}, we can further get that
\begin{align*}
F_2(\Wb^{(t)};\xb_i)-F_1(\Wb^{(t)};\xb_i)\le  O(b\alpha^2)\cdot \sum_{r=1}^m(\la\wb_{2,r}^{(t)},\ub\ra)^2-\sum_{r=1}^m(\la\wb_{1,r}^{(t)},\vb\ra) + \tilde O\big(mPd\sigma_p^2\sigma_0^2\big),
\end{align*}
where we use the fact that $|\cP_i(\ub)|\le b$ and $(\la\wb_{2,r}^{(t)},\vb\ra/\la\wb_{2,r}^{(t)},\ub\ra)^2=o(\alpha^2)$ by Lemma \ref{lemma:update_feature_relation_phase2}. 
This further implies the following according to \eqref{eq:upperbound_lossderivate}: for all $i\in\cS_0^+$,
\begin{align*}
\ell_{1,i}^{(t)}&\le \exp\big[F_2(\Wb^{(t)};\xb_i)-F_1(\Wb^{(t)};\xb_i)\big]\notag\\
&\le 2 \exp\bigg[O(b\alpha^2)\cdot \sum_{r=1}^m(\la\wb_{2,r}^{(t)},\ub\ra)^2-\sum_{r=1}^m(\la\wb_{1,r}^{(t)},\vb\ra)\bigg],
\end{align*}
where we use the fact that $mPd\sigma_p^2\sigma_0^2=o(1)$. Similarly, we can also get that for all $i\in\cS_0^-$,
\begin{align*}
\ell_{2,i}^{(t)}\le 2 \exp\bigg[O(b\alpha^2)\cdot \sum_{r=1}^m(\la\wb_{1,r}^{(t)},\vb\ra)^2-\sum_{r=1}^m(\la\wb_{2,r}^{(t)},\ub\ra)\bigg].
\end{align*}
Consequently, let $a_t:=\sum_{r=1}^m(\la\wb_{1,r}^{(t+1)},\vb\ra)^2$ and $b_t:=\sum_{r=1}^m(\la\wb_{2,r}^{(t+1)},\ub\ra)^2$, 
further applying \eqref{eq:feature_learning_v_allneurons_phase2} and \eqref{eq:feature_learning_u_allneurons_phase2} gives
\begin{align*}
a_{t+1} &\le a_t \cdot \Big[1 + \Theta(\eta)\cdot \exp\big[O(b\alpha^2)\cdot b_t -a_t\big] +O(\eta \rho  b \alpha^2) \Big]\notag\\
b_{t+1} &\le b_t \cdot \Big[1 + \Theta(\eta)\cdot \exp\big[O(b\alpha^2)\cdot a_t -b_t\big] +O(\eta \rho  b \alpha^2)\Big].
\end{align*}
Then we will first prove a weaker argument on $a_t$ and $b_t$: for all $t\le T_1'$ it holds that $a_t, b_t = o\big(1/(b\alpha^2)\big)$. In particular, we will apply standard induction techniques. First, it is easy to verify that this condition holds for $t=T_0$ according to Lemma \ref{lemma:results_phase1}. Then assuming this condition holds for all $\tau\le t$, we have $\exp\big[O(b\alpha^2)\cdot b_t\big], \exp\big[O(b\alpha^2)\cdot a_t\big]=\Theta(1)$ and thus
\begin{align}
a_{\tau+1} &\le a_\tau \cdot \Big[1 + \Theta(\eta)\cdot \exp(-a_\tau) +O(\eta \rho  b \alpha^2) \Big],\notag\\
b_{\tau+1} &\le b_\tau \cdot \Big[1 + \Theta(\eta)\cdot \exp( -b_\tau) +O(\eta \rho  b \alpha^2)\Big],
\end{align}
for all $\tau\in[T_0, t]$. 
Then by Lemma \ref{lemma:technical_lemma1}, we can immediately get that 
\begin{align*}
a_{t+1}\le O\bigg( \log\bigg(\frac{1}{\rho b \alpha^2}\cdot e^{t\eta \rho b \alpha^2}\bigg)\bigg).
\end{align*}
Then recall that $T_1' = O\big(1/(\eta\rho b \alpha^2)\big)$ and $t\le T_1'$, we can further get $a_{t+1}, b_{t+1} = O\big(\log(\frac{1}{\rho b \alpha^2})\big)$, which verify the hypothesis that $a_{t+1}, b_{t+1}\le o(1/(b\alpha^2))$. Moreover, recall the definitions of $a_t$ and $b_t$: $a_t:=\sum_{r=1}^m(\la\wb_{1,r}^{(t+1)},\vb\ra)^2$ and $b_t:=\sum_{r=1}^m(\la\wb_{2,r}^{(t+1)},\ub\ra)^2$, we can further get that for all $r\in[m]$,
\begin{align*}
|\la\wb_{1,r}^{(t+1)},\vb\ra| = \tilde O\bigg(\log^{1/2}\bigg(\frac{1}{\rho b \alpha^2}\bigg)\bigg)=O\big(\polylog(n)\big),
\end{align*}
and $|\la\wb_{2,r}^{(t+1)},\ub\ra| = O\big(\polylog(n)\big)$. This completes the proof.


\end{proof}

\begin{lemma}\label{lemma:technical_lemma1}
Let $\{a_t\}_{t=0,\dots,}$ be a sequence with $a_0\in[0, 1]$ that satisfies
\begin{align*}
a_{t+1}  = a_t \cdot [1 + c_1\cdot e^{-a_t} + c_2],
\end{align*}
where $c_1$ and $c_2$ are two constants satisfying $c_1, c_2\in[0, 1]$ and $c_2\le c_1$. Then it holds that
\begin{align*}
a_t \le O\Big(\log(c_1/c_2)\cdot e^{2c_2 t}\Big).
\end{align*}
\end{lemma}
\begin{proof}[Proof of Lemma \ref{lemma:technical_lemma1}]
Note that $c_2\le c_1$, we will then consider two cases: (1) $c_1 e^{-a_t}\ge c_2$ and (2) $c_1 e^{-a_t}< c_2$. Then case (2) will occur after case (1) since $a_t$ is strictly increasing. Regarding case (1), it is easy to see that $a_t \le \log(c_1/c_2)$ by the condition that $c_1 e^{-a_t}\ge c_2$. For case (2),
let $t_0$ be the first iteration $t$ that $c_1 e^{-a_t}<c_2$, we can get that $a_{t_0} = O\big(\log(c_1/c_2)\big)$ and then for all $t>t_0$
\begin{align*}
a_{t+1} \le a_t\cdot[1+2c_2], 
\end{align*}
which implies that
\begin{align*}
a_t \le a_{t_0}\cdot [1+2c_2]^{t-T_0}\le O\Big(\log(c_1/c_2)\cdot e^{2c_2 t}\Big).
\end{align*}
Combining the results for case (1) and case (2), we can complete the proof.


\end{proof}





Then we will focus on the rare feature data. Note that in the early stage of the second phase, their corresponding loss derivatives $\ell_{k,i}^{(t)}$'s are still in the constant order. The following Lemma summarizes the learning of rare features and noises for the rare feature data.
\begin{lemma}\label{lemma:learning_noise_weakdata_phase2}
Let $T_1= O\big(\frac{n\log(1/(\sigma_0d^{1/2}\sigma_p))}{d\sigma^2\eta}\big)$ be a quantity that satisfies $T_0<T_1<T_1'$. Then for any $t\in[T_0, T_1]$, it holds that
\begin{align*}
\la\wb_{1,r}^{(t+1)},\vb'\ra &= \la\wb_{1,r}^{(t)},\vb'\ra\cdot \big[1 + \Theta(\rho\eta)\big],\  \la\wb_{2,r}^{(t+1)},\vb'\ra = \la\wb_{2,r}^{(t)},\vb'\ra\cdot \big[1 - \Theta(\rho\eta)\big]\notag\\
\la\wb_{1,r}^{(t+1)},\ub'\ra &= \la\wb_{1,r}^{(t)},\ub'\ra\cdot \big[1 - \Theta(\rho\eta)\big],\  \la\wb_{2,r}^{(t+1)},\ub'\ra = \la\wb_{2,r}^{(t)},\ub'\ra\cdot \big[1 + \Theta(\rho\eta)\big]
\end{align*}
Besides, for any $i\in\cS_1^+\cup\cS_1^-$ and $k=y_s$, we have
\begin{align*}
\max_{r,p}|\la\wb_{k,r}^{(t+1)},\bxi_s^{(q)}\ra| = \max_{r,p}|\la\wb_{k,r}^{(t)},\bxi_s^{(q)}\ra|\cdot \bigg[1 + \frac{\eta}{n}\cdot\tilde\Theta(d\sigma_p^2)\bigg];
\end{align*}
for $k\neq y_s$,
\begin{align*}
\max_{r,p}|\la\wb_{k,r}^{(t+1)},\bxi_s^{(q)}\ra| = \tilde O(d^{-1/2}n).
\end{align*}
\end{lemma}
\begin{proof}
The proof is similar to that of Lemma \ref{lemma:learning_phase1}, except  the proof for the dynamics of $\la\wb_{k,r}^{(t+1)},\bxi_s^{(q)}\ra$.  
First, by standard concentration argument, we can get with probability $1-1/\poly(n)$, for all $\bxi_i^{(p)}\in\{\bxi\}$, it holds that
\begin{align*}
d\sigma_p^2 -\tilde O\big(d^{1/2}\sigma_p^2\big)\le \|\bxi_i^{(p)}\|_2^2 \le d\sigma_p^2 + \tilde O\big(d^{1/2}\sigma_p^2\big).
\end{align*}
Then by \eqref{eq:update_noise_simplified}, we can get
\begin{align*}
\la\wb_{k,r}^{(t+1)},\bxi_s^{(q)}\ra 
& = \la\wb_{k,r}^{(t)},\bxi_s^{(q)}\ra \cdot\bigg[ 1 + \frac{2\eta}{n}\cdot \ell_{k,s}^{(t)}\cdot\|\bxi_s^{(q)}\|_2^2\bigg]  \pm \frac{2\eta}{n}\cdot\sum_{i\neq s || p\neq q} |\ell_{k,i}^{(t)}|\cdot|\la\wb_{k,r}^{(t)},\bxi_i^{(q)}\ra|\cdot \tilde O\big(d^{1/2}\sigma_p^2\big)\notag\\
&=\la\wb_{k,r}^{(t)},\bxi_s^{(q)}\ra\cdot \bigg[ 1 + \frac{2\eta}{n}\cdot \ell_{k,s}^{(t)}\cdot d\sigma_p^2\bigg]\pm \frac{2\eta}{n}\cdot\sum_{\bxi_i^{(q)}\in\{\bxi\}} |\ell_{k,i}^{(t)}|\cdot|\la\wb_{k,r}^{(t)},\bxi_i^{(q)}\ra|\cdot \tilde O\big(d^{1/2}\sigma_p^2\big).
\end{align*}
Then let $\zeta=O\big(1/\polylog(n)\big)$ be some user-defined constant, then let $T'$ be the smallest iteration number such that $\max_{i}|\ell_{k,i}^{(t)}|\in[0.5-\zeta,0.5+\zeta]$. Then we can get for any $t\le T'$ and any $i,r$, 
\begin{align}\label{eq:update_noise_proof}
\la\wb_{k,r}^{(t+1)},\bxi_s^{(q)}\ra 
&=\la\wb_{k,r}^{(t)},\bxi_s^{(q)}\ra\cdot \bigg[ 1 + \frac{2\eta}{n}\cdot (0.5\pm \zeta)\cdot d\sigma_p^2\bigg]\pm \frac{2\eta}{n}\cdot\sum_{\bxi_i^{(q)}\in\{\bxi\}} |\ell_{k,i}^{(t)}|\cdot|\la\wb_{k,r}^{(t)},\bxi_i^{(q)}\ra|\cdot \tilde O\big(d^{1/2}\sigma_p^2\big).
\end{align}
Then we will prove the main arguments via mathematical induction, including the following hypothesis: 
\begin{itemize}
\item For all $i\in\cS_1^+\cup\cS_1^-$, it holds that $\max_{r,q}|\la\wb_{k,r}^{(t)},\bxi_i^{(p)}\ra|\ge \frac{1}{n^{0.1}}\cdot \max_{r,i',q}|\la\wb_{k,r}^{(t)},\bxi_{i'}^{(q)}\ra|$
\item $\max_{r,q}|\la\wb_{k,r}^{(t+1)},\bxi_s^{(q)}\ra| 
=\max_{r,q}|\la\wb_{k,r}^{(t)},\bxi_s^{(q)}\ra|\cdot \bigg[ 1 + \frac{2\eta}{n}\cdot (0.5\pm 2\zeta)\cdot d\sigma_p^2\bigg]$.
\end{itemize}
Then it is clear that the first argument holds for $t=T_0$ as with probability at least $1-1/\poly(n)$ we have $\max_{r,q}|\la\wb_{k,r}^{(T_0)},\bxi_i^{(p)}\ra| = \tilde\Theta(\sigma_0d^{1/2}\sigma_p)$ and $\max_{r,i',q}|\la\wb_{k,r}^{(t)},\bxi_{i'}^{(q)}\ra|=\tilde\Theta(\sigma_0d^{1/2}\sigma_p)$, which implies that $|\la\wb_{k,r}^{(t)},\bxi_i^{(p)}\ra|\ge \frac{1}{\polylog(n)}\cdot \max_{r,i',q}|\la\wb_{k,r}^{(t)},\bxi_{i'}^{(q)}\ra|$.


Besides, given the first argument, we have
\begin{align*}
\sum_{\bxi_i^{(q)}\in\{\bxi\}} |\ell_{k,i}^{(t)}|\cdot|\la\wb_{k,r}^{(t)},\bxi_i^{(q)}\ra|\cdot \tilde O\big(d^{1/2}\sigma_p^2\big)&\le \tilde O(nPd^{1/2}\sigma_p^2)\cdot \max_{r,i',q}|\la\wb_{k,r}^{(t)},\bxi_{i'}^{(q)}\ra|\notag\\
&\le O(n^{1.1}Pd^{1/2}\sigma_p^2)\cdot \max_{r,q}|\la\wb_{k,r}^{(t)},\bxi_i^{(p)}\ra|\notag\\
&\le \zeta \cdot d\sigma_p^2\cdot\max_{r,q}|\la\wb_{k,r}^{(t)},\bxi_i^{(p)}\ra|,
\end{align*}
where we use the fact that $d^{-1/2}n^{1.1}P = o\big(1/\polylog(n)\big)=o(\zeta)$. Then by \eqref{eq:update_noise_proof}, we can directly obtain the second argument.

Now we will verify the hypotheses by induction. First, similar to the previous derivation, the first argument at the $t$-th iteration can directly imply the second argument at the $t+1$-th iteration. Then it remains to verify the first argument. In fact, given the second argument, we have for any $i$ and $i'$ and $\tau\le t$,
\begin{align*}
\frac{\max_{r,q}|\la\wb_{k,r}^{(\tau+1)},\bxi_i^{(p)}\ra|}{\max_{r,q}|\la\wb_{k,r}^{(\tau+1)},\bxi_{i'}^{(p)}\ra|}&\le \frac{1+\frac{2\eta}{n}\cdot (0.5 + 2\zeta)\cdot d\sigma_p^2}{1+\frac{2\eta}{n}\cdot (0.5 - 2\zeta)\cdot d\sigma_p^2} \cdot \frac{\max_{r,q}|\la\wb_{k,r}^{(\tau)},\bxi_i^{(p)}\ra|}{\max_{r,q}|\la\wb_{k,r}^{(\tau)},\bxi_{i'}^{(p)}\ra|}\notag\\
&\le \bigg(1+\frac{\eta\zeta d\sigma_p^2}{n}\bigg)^{T_1}\cdot \frac{\max_{r,q}|\la\wb_{k,r}^{(T_1)},\bxi_i^{(p)}\ra|}{\max_{r,q}|\la\wb_{k,r}^{(T_1)},\bxi_{i'}^{(p)}\ra|}.
\end{align*}
Therefore, using the fact that $T_1 = O\big(\frac{n\log(1/(\sigma_0d^{1/2}\sigma_p))}{d\sigma^2\eta}\big)$, setting $\zeta = 1/\log^2(1/(\sigma_0d^{1/2}\sigma_p)$, we can directly get that
\begin{align*}
\frac{\max_{r,q}|\la\wb_{k,r}^{(t+1)},\bxi_i^{(p)}\ra|}{\max_{r,q}|\la\wb_{k,r}^{(t+1)},\bxi_{i'}^{(p)}\ra|}
\le \bigg(1+\frac{\eta\zeta d\sigma_p^2}{n}\bigg)^{T_1}\cdot O(\polylog(n))= o(n^{0.1}).
\end{align*}
Note that the above holds for all $i$ and $i'$, taking $i' = \arg\max_i|\la\wb_{k,r}^{(t+1)},\bxi_{i'}^{(p)}\ra|$ directly completes the verification of the first argument.

The proof for $\max_{r,q}|\la\wb_{k,r}^{(t)},\bxi_s^{(q)}\ra|$ with $k\neq y_s$, we have the following by \eqref{eq:update_noise_simplified},
\begin{align*}
\max_{r,q}|\la\wb_{k,r}^{(t+1)},\bxi_s^{(q)}\ra| 
& \le \max_{r,q}|\la\wb_{k,r}^{(t)},\bxi_s^{(q)}\ra |+ \frac{2\eta}{n}\cdot\sum_{i\neq s || p\neq q} |\ell_{k,i}^{(t)}|\cdot|\la\wb_{k,r}^{(t)},\bxi_i^{(q)}\ra|\cdot \tilde O\big(d^{1/2}\sigma_p^2\big)\\
&\le \max_{r,q}|\la\wb_{k,r}^{(T_0)},\bxi_s^{(q)}\ra | + T_1 P\eta \cdot \tilde O(d^{1/2}\sigma_p^2)\notag\\
& = \tilde O(d^{-1/2}n),
\end{align*}
where we use the fact that for all $t\le T_1$, it holds that $\max_{i,r,q}|\la\wb_{k,r}^{(t+1)},\bxi_s^{(q)}\ra|=\tilde O(1)$.







% The exception is that we need to ensure that this is true for all $t\le T_1$. In fact, let $\xb_i$ be the data with feature $\vb'$, it suffices to show that
% \begin{align*}
% F_k(\Wb^{(t)};\xb_i) &= \sum_{r=1}^m \sum_{p=1}^P(\la\wb_{k,r}^{(t)}, \xb_i^{(p)}\ra)^2\notag\\
% &=\sum_{r=1}^m\bigg(\alpha^2\cdot\sum_{p\in\cP_i(\vb)}(\la\wb_{k,r}^{(t)}, \vb\ra)^2+\alpha^2\cdot\sum_{p\in\cP_i(\ub)}(\la\wb_{k,r}^{(t)}, \ub\ra)^2   +(\la\wb_{k,r}^{(t)},\vb'\ra)^2+\sum_{p\in\cP_i(\bxi)}(\la\wb_{k,r}^{(t)},\bxi_i^{(p)}\ra)^2\bigg)\notag\\
% &=O(1).
% \end{align*}
% Moreover, by Definition \ref{def:data_distribution_new} and Lemma \ref{lemma:upperbound_feature_phase2}, we have $|\cP_i(\vb)|, |\cP_i(\ub)|\le b$ and $|\la\wb_{k,r}^{(t)},\vb\ra|, |\la\wb_{k,r}^{(t)},\ub\ra|=\tilde O(1)$, and $\alpha=o\big(1/\polylog(n)\big)$. Then it suffices to show that
% \begin{align*}
% (\la\wb_{k,r}^{(t)},\vb'\ra)^2, \ \sum_{p\in\cP_i(\bxi)}(\la\wb_{k,r}^{(t)},\bxi_i^{(p)}\ra)^2 = O(1/m) = O\big(1/\polylog(n)\big).
% \end{align*}
% By the update rules in the statement of the lemma, it suffices to
% show that the iteration number $T_1$ satisfies
% \begin{align*}
% T_1 = \tilde O\bigg(\frac{1}{\rho\eta}\wedge \frac{n}{d\sigma^2 \eta}\bigg).
% \end{align*}
% Then recall the data distribution in Definition \ref{def:data_distribution_new}, we have $d\sigma^2\in[\omega(\rho n),  o(n)]$, implying that $T_1 = \tilde O\big(\frac{n}{d\sigma^2 \eta}\big)\ge T_0$. This completes the proof.


\end{proof}


\begin{lemma}[End of Phase 2]\label{lemma:results_endofphase2}
Let $T_1$ be the same quantity defined in Lemma \ref{lemma:learning_noise_weakdata_phase2}, we have for all $i\in\cS_0^+\cup\cS_0^-$,
\begin{align*}
|\la\wb_{1,r}^{(T_1)}, \vb\ra|, |\la\wb_{2,r}^{(T_1)}, \ub\ra| = \tilde\Theta(1),\quad |\la\wb_{2,r}^{(T_1)}, \vb\ra|, |\la\wb_{1,r}^{(T_1)}, \ub\ra| = \tilde O(\sigma_0),\quad |\la\wb_{k,r}^{(T_1)}, \bxi_i^{(p)}\ra|=\tilde O\big(d^{-1/2}n\big);
\end{align*}
for all $i\in\cS_1^+\cup\cS_1^-$,
\begin{align*}
|\la\wb_{1,r}^{(T_1)}, \vb'\ra|, |\la\wb_{2,r}^{(T_1)}, \ub'\ra| = \tilde O(\sigma_0),\quad |\la\wb_{2,r}^{(T_1)}, \vb'\ra|, |\la\wb_{1,r}^{(T_1)}, \ub'\ra| = \tilde O(\sigma_0),\quad |\la\wb_{y_i,r}^{(T_1)}, \bxi_i^{(p)}\ra|=\tilde\Theta(1).
\end{align*}

\end{lemma}
\begin{proof}
The proof of this lemma is simply a combination of Lemmas \ref{lemma:learning_noise_weakdata_phase2} and \ref{lemma:learning_noise_weakdata_phase2}, where we only need to verify the bound for $|\la\wb_{k,r}^{(T_1)}, \vb'\ra|$ and $|\la\wb_{k,r}^{(T_1)}, \ub'\ra|$. This can be done as follows:
\begin{align*}
|\la\wb_{k,r}^{(T_1)}, \vb'\ra|\le \big[1+\Theta(\rho\eta)\big]^{T_1}\cdot |\la\wb_{k,r}^{(T_0)}, \vb'\ra| \le \exp\big[\tilde O(n\rho /(d\sigma^2))\big]\cdot|\la\wb_{k,r}^{(T_0)}, \vb'\ra| = \tilde O(\sigma_0), 
\end{align*}
where we use the fact that $\rho n = o(d\sigma_p^2)$. The proof for $|\la\wb_{k,r}^{(T_1)}, \vb'\ra|$ will be similar and thus is ommited here.




\end{proof}
\paragraph{Phase 3. Training until convergence.} 
In this phase, we will show that the feature learning and noise learning in \textbf{Phase 2} will be maintained. Particularly, we first make the following hypothesis and then verify them via mathematical induction. 
\begin{hypothesis}\label{hypothesis}
For all $t = \poly(n)$ that is greater than $T_1$, it holds that
\begin{enumerate}[label=(\alph*)]
    \item \label{item:strongfeat_correct} We have $\sum_{r=1}^m(\la\wb_{1,r}^{(t)}, \vb\ra)^2 = \tilde\Theta(1)$ and $\sum_{r=1}^m\big(\la\wb_{2,r}^{(t)},\ub\ra\big)^2 = \tilde\Theta(1)$.
    \item \label{item:strongfeat_incorrect} We have $|\la\wb_{1,r}^{(t)}, \ub\ra|=O\big(|\la\wb_{1,r}^{(T_1)}, \ub\ra|\big)=o\big(\frac{1}{\polylog(n)}\big)$ and $|\la\wb_{2,r}^{(t)}, \vb\ra|=O\big(|\la\wb_{2,r}^{(T_1)}, \vb\ra|\big)=o\big(\frac{1}{\polylog(n)}\big)$.
    \item \label{item:weakfeat} We have $|\la\wb_{k,r}^{(t)}, \vb'\ra|=O\big(|\la\wb_{k,r}^{(T_1)}, \vb'\ra|\big)=o\big(\frac{1}{\polylog(n)}\big)$ and $|\la\wb_{k,r}^{(t)}, \ub'\ra|=O\big(|\la\wb_{k,r}^{(T_1)}, \ub'\ra|\big)=o\big(\frac{1}{\polylog(n)}\big)$.
    \item \label{item:strongfeat_noise} For all $i\in\cS_0^+\cup\cS_0^-$, we have
    $|\la\wb_{k,r}^{(t)},\bxi_i^{(p)}\ra|=O\big(|\la\wb_{k,r}^{(T_1)},\bxi_i^{(p)}\ra|\big)=o\big(\frac{1}{\polylog(n)}\big)$.
    \item \label{item:weakfeat_noise_correct} For all $i\in\cS_1^+$, we have
    $\sum_{r=1}^m\sum_{p\in\cP_i(\bxi)}(\la\wb_{1,r}^{(t)}, \bxi_i^{(p)}\ra)^2 = \tilde\Theta(1)$; for all $i\in\cS_1^-$, we have $\sum_{r=1}^m\sum_{p\in\cP_i(\bxi)}(\la\wb_{2,r}^{(t)},\bxi_i^{(p)}\ra)^2 = \tilde\Theta(1)$.
    \item \label{item:weakfeat_noise_incorrect} For all $i\in\cS_1^+$, we have
    $|\la\wb_{2,r}^{(t)}, \bxi_i^{(p)}\ra| =o\big(\frac{1}{\polylog(n)}\big)$; for all $i\in\cS_1^-$, we have $|\la\wb_{1,r}^{(t)}, \bxi_i^{(p)}\ra| =o\big(\frac{1}{\polylog(n)}\big)$.
\end{enumerate}
\end{hypothesis}

The hypothesis will be verified via induction. First, it is clear that all hypothesis are satisfied at $t=T_1$ according to Lemma \ref{lemma:results_endofphase2}. Then, the following lemma is useful in the entire proof.
\begin{lemma}\label{lemma:bound_sum_lossderivative}
Assuming all hypothesis in Hypothesis \ref{hypothesis} hold for $\tau\in[0, t]$, then we have for all $k\in[2]$,
\begin{align*}
\sum_{\tau=T_1}^{t}\sum_{i\in\cS_0^+\cup\cS_0^-} |\ell_{k,i}^{(\tau)}| = \tilde O\bigg(\frac{n}{\eta}\bigg),\quad\mbox{and}\quad \sum_{\tau=T_1}^{t}\sum_{i\in\cS_1^+\cup\cS_1^-}|\ell_{k,i}^{(\tau)}|\le \tilde O\bigg(\frac{\rho n^2}{d\sigma_p^2 \eta}\bigg),
\end{align*}
moreover, for any $i\in\cS_0^+\cup\cS_0^-$, we have
\begin{align*}
\sum_{t=T_1}^t|\ell_{k,i}^{(\tau)}|=\tilde O(1/\eta).
\end{align*}

\end{lemma}
\begin{proof}[Proof of Lemma \ref{lemma:bound_sum_lossderivative}]
By \eqref{eq:update_features}, we have 
\begin{align}\label{eq:total_feature_learning_phase3}
\sum_{r=1}^m(\wb_{1,r}^{(\tau+1)},\vb)^2 &= \sum_{r=1}^m(\wb_{1,r}^{(\tau)},\vb)^2\cdot \bigg[1+\frac{2\eta}{n}\cdot \sum_{i\in[n]}\ell_{1,i}^{(\tau)}\sum_{p\in\cP_i(\vb)}\alpha_{i,p}^2\bigg]^2\notag\\
&\ge \sum_{r=1}^m(\wb_{1,r}^{(\tau)},\vb)^2\cdot \bigg[1+\Theta\bigg(\frac{\eta}{n}\bigg)\cdot \sum_{i\in\cS_0^+}|\ell_{1,i}^{(\tau)}| - \Theta\bigg(\frac{b\eta\alpha^2}{n}\bigg)\cdot \sum_{i\not\in\cS_0^+}|\ell_{1,i}^{(\tau)}|\bigg];\notag\\
\sum_{r=1}^m(\wb_{2,r}^{(\tau+1)},\ub)^2 &= \sum_{r=1}^m(\wb_{2,r}^{(\tau)},\vb)^2\cdot \bigg[1+\frac{2\eta}{n}\cdot \sum_{i\in[n]}\ell_{2,i}^{(\tau)}\sum_{p\in\cP_i(\vb)}\la\wb_{2,r}^{(\tau)},\vb\ra\cdot\alpha_{i,p}^2\bigg]^2\notag\\
&\ge \sum_{r=1}^m(\wb_{2,r}^{(\tau)},\vb)^2\cdot \bigg[1+\Theta\bigg(\frac{\eta}{n}\bigg)\cdot \sum_{i\in\cS_0^-}|\ell_{2,i}^{(\tau)}| - \Theta\bigg(\frac{b\eta\alpha^2}{n}\bigg)\cdot \sum_{i\not\in\cS_0^-}|\ell_{2,i}^{(\tau)}|\bigg].
\end{align}
where we use the fact that $|\ell_{1,i}|=|\ell_{2,i}|$
Summing them up and further taking a summation over $\tau\in[T_1, t-1]$, applying Hypothesis \ref{hypothesis}\ref{item:strongfeat_correct} gives
\begin{align}\label{eq:bound_sum_lossderivatives1}
\Theta\bigg(\frac{\eta}{n}\bigg)\cdot\sum_{\tau=T_1}^{t-1}\sum_{i\in\cS_0^+\cup\cS_0^-}|\ell_{1,i}^{(\tau)}| - \Theta\bigg(\frac{b\eta\alpha^2}{n}\bigg)\cdot \sum_{\tau=T_1}^{t-1}\sum_{i=1}^n|\ell_{1,i}^{(\tau)}|\le \tilde O(1).
\end{align}
where we use the fact that $|\ell_{1,i}^{(t)}|=|\ell_{2,i}^{(t)}|$ and $\sum_{r=1}^m(\la\wb_{1,r}^{(t)}, \vb\ra)^2, \sum_{r=1}^m\big(\la\wb_{2,r}^{(t)},\ub\ra\big)^2 = \tilde\Theta(1)$.
Besides, by \eqref{eq:update_noise_simplified} and Hypotheses \ref{item:weakfeat_noise_correct} and \ref{item:weakfeat_noise_incorrect}, we know that the correct noise learning for different weak feature data will be different by at most 
$O(\polylog(n))$ factors, therefore, we can  get that
\begin{align}\label{eq:total_noise_learning_phase3}
\sum_{i\in\cS_1^+}\sum_{p\in[P]}(\la\wb_{1,r}^{(\tau+1)},\bxi_i^{(p)}\ra)^2 
&\ge  \sum_{i\in\cS_1^+}\sum_{p\in[P]}(\la\wb_{k,r}^{(\tau)},\bxi_i^{(p)}\ra)^2 \cdot\bigg[ 1 + \frac{2\eta}{n}\cdot \ell_{1,i}^{(\tau)}\cdot\|\bxi_i^{(p)}\|_2^2 - \frac{2\eta}{n}\cdot\sum_{i=1}^n |\ell_{1,i}^{(\tau)}|\cdot \tilde O\big(Pd^{1/2}\sigma_p^2\big)\bigg]^2\notag\\
&\ge \sum_{i\in\cS_1^+}\sum_{p\in[P]}(\la\wb_{1,r}^{(\tau)},\bxi_i^{(p)}\ra)^2 \cdot\bigg[ 1 + \tilde\Theta\bigg(\frac{\eta d\sigma_p^2}{n}\bigg)\cdot |\ell_{1,i}^{(\tau)}| - \tilde\Theta\bigg(\frac{\eta d^{3/2}\sigma_p^4 P}{n}\bigg)\cdot\sum_{i=1}^n |\ell_{1,i}^{(\tau)}|\bigg],
\end{align}
and similarly,
\begin{align*}
\sum_{i\in\cS_1^-}\sum_{p\in[P]}(\la\wb_{2,r}^{(\tau+1)},\bxi_i^{(p)}\ra)^2 
&\ge \sum_{i\in\cS_1^-}\sum_{p\in[P]}(\la\wb_{2,r}^{(\tau)},\bxi_i^{(p)}\ra)^2 \cdot\bigg[ 1 + \tilde\Theta\bigg(\frac{\eta d\sigma_p^2}{n}\bigg)\cdot |\ell_{2,i}^{(\tau)}| - \tilde\Theta\bigg(\frac{\eta d^{3/2}\sigma_p^4 P}{n}\bigg)\cdot\sum_{i=1}^n |\ell_{2,i}^{(\tau)}|\bigg].
\end{align*}
Therefore, taking a summation over $r\in[m]$ and $\tau\in[T_1, t-1]$, and using the Hypothesis  \ref{hypothesis}\ref{item:weakfeat_noise_correct}, we have
\begin{align}\label{eq:bound_sum_lossderivatives2}
\Theta\bigg(\frac{\eta d\sigma_p^2}{n}\bigg)\cdot\sum_{\tau=T_1}^{t-1}\sum_{i\in\cS_1^+\cup\cS_1^-}|\ell_{1,i}^{(\tau)}| - \Theta\bigg(\frac{\eta d^{3/2}\sigma_p^4P}{n}\bigg)\cdot \sum_{\tau=T_1}^{t-1}\sum_{i=1}^n|\ell_{1,i}^{(\tau)}|\le \tilde O(|\cS_1^+\cup\cS_1^-|)=\tilde O(\rho n).
\end{align}
Combining \eqref{eq:bound_sum_lossderivatives1} and \eqref{eq:bound_sum_lossderivatives2} and using the fact that $d\sigma_p^2=\omega(1)$ and $b\alpha^2=\omega(d^{3/2}\sigma_p^4P)$, we can get that
\begin{align*}
\Theta\bigg(\frac{\eta}{n}\bigg)\cdot\sum_{\tau=T_1}^{t-1}\sum_{i=1}^n|\ell_{1,i}^{(\tau)}| -\Theta\bigg(\frac{b\eta\alpha^2}{n}\bigg)\cdot\sum_{\tau=T_1}^{t-1}\sum_{i=1}^n|\ell_{1,i}^{(\tau)}|\le \tilde O(\rho n).
\end{align*}
Note that $b\alpha^2=o(1)$, the above inequality immediately implies that
\begin{align*}
\sum_{\tau=T_1}^{t-1}\sum_{i=1}^n|\ell_{1,i}^{(\tau)}|\le \tilde O\bigg(\frac{\rho n^2}{\eta}\bigg).
\end{align*}
We will further use this argument to sharpen our result. First, \eqref{eq:bound_sum_lossderivatives2} directly leads to
\begin{align*}
\Theta\bigg(\frac{\eta d\sigma_p^2}{n}\bigg)\cdot\sum_{\tau=T_1}^{t-1}\sum_{i\in\cS_1^+\cup\cS_1^-}|\ell_{1,i}^{(\tau)}|\le \tilde O(\rho n) + \tilde O\big(d^{3/2}\sigma_p^4 P \rho n\big) = \tilde O(\rho n),
\end{align*}
which implies that 
\begin{align*}
\sum_{\tau=T_1}^{t-1}\sum_{i\in\cS_1^+\cup\cS_1^-}|\ell_{1,i}^{(\tau)}|\le \tilde O\bigg(\frac{\rho n^2}{d\sigma_p^2 \eta}\bigg).
\end{align*}
Plugging the above inequality into \eqref{eq:bound_sum_lossderivatives2} and using the fact that $b\alpha^2=o(1)$ gives
\begin{align*}
\Theta\bigg(\frac{\eta}{n}\bigg)\cdot\sum_{\tau=T_1}^{t-1}\sum_{i\in\cS_0^+\cup\cS_0^-}|\ell_{1,i}^{(\tau)}| \le \tilde O(1) + \Theta\bigg(\frac{b\eta\alpha^2}{n}\bigg)\cdot \sum_{\tau=T_1}^{t-1}\sum_{i\in\cS_1^+\cup\cS_1^-}|\ell_{1,i}^{(\tau)}|\le \tilde O(1) + \tilde O\bigg(\frac{\rho n b\alpha^2}{d\sigma_p^2}\bigg)=\tilde O(1).
\end{align*}
where the last inequality is due to $\rho n = o(d\sigma_p^2)$. 
Further note that $|\ell_{1,i}^{(t)}|\le 1$ and $\eta =o(1)$, we have
\begin{align*}
&\sum_{\tau=T_1}^{t}\sum_{i\in\cS_1^+\cup\cS_1^-}|\ell_{1,i}^{(\tau)}|\le \tilde O\bigg(\frac{\rho n^2}{d\sigma_p^2 \eta}\bigg) + O(\rho n) = \tilde O\bigg(\frac{\rho n^2}{d\sigma_p^2 \eta}\bigg);\notag\\
&\sum_{\tau=T_1}^{t}\sum_{i\in\cS_0^+\cup\cS_0^-}|\ell_{1,i}^{(\tau)}| \le\tilde O\bigg(\frac{n}{\eta}\bigg)+O(n) =  \tilde O\bigg(\frac{n}{\eta}\bigg).
\end{align*}
Moreover, by Hypothesis \ref{hypothesis} for all $\tau\in[T_1, t]$, we also have for all $i\in\cS_0^+$,
\begin{align*}
|\ell_{1,i}^{(\tau)}| = \frac{\exp\big[F_2(\Wb^{(\tau)};\xb_i)-F_1(\Wb^{(\tau)};\xb_i)\big]}{1 + \exp\big[F_2(\Wb^{(\tau)};\xb_i)-F_1(\Wb^{(\tau)};\xb_i)\big]}.
\end{align*}
Moreover, we have
\begin{align*}
F_2(\Wb^{(\tau)};\xb_i)-F_1(\Wb^{(\tau)};\xb_i) &= \sum_{r=1}^m\sum_{p\in[P]}\la\wb_{2,r}^{(\tau)},\xb_i^{(p)}\ra - \sum_{r=1}^m\sum_{p\in[P]}\la\wb_{1,r}^{(\tau)},\xb_i^{(p)}\ra\notag\\
& = -\sum_{r=1}^{m}\sum_{p\in\cP_i(\vb)}(\la\wb_{k,r}^{(\tau)},\vb\ra)^2 \pm o\bigg(\frac{1}{\polylog(n)}\bigg)\le 0.
\end{align*}
This implies that for any $i,j\in\cS_0^+$ with $|\cP_i(\vb)|=|\cP_j(\vb)|$, we have
\begin{align*}
\frac{|\ell_{1,i}^{(\tau)}|}{|\ell_{1,j}^{(\tau)}|} = \Theta\bigg(\frac{\exp\big[F_2(\Wb^{(\tau)};\xb_i)-F_1(\Wb^{(\tau)};\xb_i)\big]}{\exp\big[F_2(\Wb^{(\tau)};\xb_j)-F_1(\Wb^{(\tau)};\xb_j)\big]}\bigg)= \Theta\big(\exp[o\big(1/\polylog(n)\big)]\big)=\Theta(1).
\end{align*}
Further note that, by Definition \ref{def:data_distribution_new}, the number of feature patches are uniformly sampled from $[1, \Theta(1)]$, implying that with probability at least $1-1/\poly(n)$, for any $i\in\cS_0^+$,
\begin{align*}
\#\big\{j:j\in\cS_0^+, |\cP_j(\vb)|=|\cP_i(\vb)|\big\} = \Theta(n)
\end{align*}
Therefore, let $\cS'$  be the above set of data points, we have for any $s\in\cS_0^+$ or $s\in\cS_1^+$,
\begin{align*}
\sum_{\tau=T_1}^{t}|\ell_{1,s}^{(\tau)}|&= \Theta\big(|\cS'|^{-1}\big)\sum_{\tau=T_1}^{t}\sum_{i\in\cS'}|\ell_{1,i}^{(\tau)}|\le \Theta\big(|\cS'|^{-1}\big)\sum_{\tau=T_1}^{t}\sum_{i\in\cS_0^+\cup\cS_0^-}|\ell_{1,i}^{(\tau)}|\le \tilde O\bigg(\frac{1}{ \eta}\bigg).
\end{align*}
where the last inequality is due to $|\cS'|=\Theta(n)$. This completes the proof.
\end{proof}

We will then verify Hypothesis \ref{hypothesis}\ref{item:weakfeat}, which is summarized in the following lemma.
\begin{lemma}\label{lemma:weakfeat_phase3}
Let Hypothesis \ref{hypothesis} holds for all $\tau\le t$, then we have
$|\la\wb_{k,r}^{(t+1)}, \vb'\ra|=O\big(|\la\wb_{k,r}^{(T_1)},\vb'\ra|\big)$ and $|\la\wb_{k,r}^{(t+1)}, \ub'\ra|=O\big(|\la\wb_{k,r}^{(T_1)},\ub'\ra|\big)$.
\end{lemma}
\begin{proof}[Proof of Lemma \ref{lemma:weakfeat_phase3}]
Recall the update of rare features in \eqref{eq:update_features}, we have
\begin{align*}
\la\wb_{k,r}^{(\tau+1)},\vb'\ra & = \la\wb_{k,r}^{(\tau)},\vb'\ra + \frac{2\eta}{n}\cdot\sum_{i\in\cS_1^{+}} \ell_{k,i}^{(\tau)} \sum_{p\in \cP_i(\vb')} \la\wb_{k,r}^{(\tau)},\vb'\ra\notag\\
\la\wb_{k,r}^{(\tau+1)},\ub'\ra & = \la\wb_{k,r}^{(\tau)},\ub'\ra + \frac{2\eta}{n}\cdot\sum_{i\in\cS_1^{-}} \ell_{k,i}^{(\tau)} \sum_{p\in \cP_i(\ub')} \la\wb_{k,r}^{(\tau)},\ub'\ra.
\end{align*}
Then according to the Hypothesis \ref{hypothesis}\ref{item:weakfeat} for all $\tau\in[T_1, t]$, we have
\begin{align*}
|\la\wb_{k,r}^{(t+1)},\vb'\ra| &\le |\la\wb_{k,r}^{(T_1)},\vb'\ra| + \frac{2\eta}{n}\cdot\sum_{\tau=T_1}^{t}\sum_{i\in\cS_1^{+}} |\ell_{k,i}^{(\tau)}| \sum_{p\in \cP_i(\vb')} |\la\wb_{k,r}^{(\tau)},\vb'\ra|\notag\\
& \le O\big(|\la\wb_{k,r}^{(T_1)},\vb'\ra|\big) + \frac{\eta}{n}\cdot \sum_{\tau=T_1}^{t}\sum_{i\in\cS_1^{+}} |\ell_{k,i}^{(\tau)}|\cdot O\big(|\la\wb_{k,r}^{(T_1)},\vb'\ra|\big), 
\end{align*}
where the last inequality is due to the fact that $|\cP_i(\vb')|=\Theta(1)$. By Lemma \ref{lemma:bound_sum_lossderivative}, it is clear that $\sum_{\tau=T_1}^{t}\sum_{i\in\cS_1^{+}} |\ell_{k,i}^{(\tau)}| = \tilde O\big(\frac{\rho n^2}{d\sigma_p^2 \eta}\big)$. Therefore,
\begin{align*}
|\la\wb_{k,r}^{(t+1)},\vb'\ra| \le O\big(|\la\wb_{k,r}^{(T_1)},\vb'\ra|\big) + \tilde O\bigg(\frac{\rho n}{d\sigma^2}\bigg)\cdot O\big(|\la\wb_{k,r}^{(T_1)},\vb'\ra|\big) = O\big(|\la\wb_{k,r}^{(T_1)},\vb'\ra|\big).
\end{align*}
The proof for $|\la\wb_{k,r}^{(t+1)},\ub'\ra|$ is similar so we omit it here. 
\end{proof}

Using the similar proof technique, we are able to verify Hypothesis \ref{hypothesis}\ref{item:strongfeat_incorrect}, \ref{hypothesis}\ref{item:strongfeat_noise}, and \ref{hypothesis}\ref{item:weakfeat_noise_incorrect}, which are summarized in the following lemmas.

\begin{lemma}\label{lemma:strongfeat_incorrect_phase3}
Let Hypothesis \ref{hypothesis} holds for all $\tau\le t$, then we have
$|\la\wb_{2,r}^{(t+1)}, \vb\ra|=O\big(|\la\wb_{2,r}^{(T_1)}, \vb\ra|\big)$ and $|\la\wb_{1,r}^{(t+1)}, \ub\ra|=O\big(|\la\wb_{1,r}^{(T_1)}, \ub\ra|\big)$.
\end{lemma}
\begin{proof}[Proof of Lemma \ref{lemma:strongfeat_incorrect_phase3}]
Since the proofs for $|\la\wb_{2,r}^{(t+1)}, \vb\ra|$ and $|\la\wb_{1,r}^{(t+1)}, \ub\ra|$ are basically identical, we will only provide the proof regarding $|\la\wb_{2,r}^{(t+1)}, \vb\ra|$. By \eqref{eq:update_features} and data distribution in Definition \ref{def:data_distribution_new}, we have
\begin{align*}
\la\wb_{2,r}^{(\tau+1)},\vb\ra &= \la\wb_{2,r}^{(\tau)}, \vb\ra\cdot\bigg[1 + \frac{2\eta}{n}\cdot\sum_{i\in\cS_0^+} \ell_{2,i}^{(\tau)} \sum_{p\in \cP_i(\vb)} \alpha_{i,p}^2\|\vb\|_2^2 + \frac{2\eta}{n}\cdot\sum_{i\in[n]\backslash\cS_0^+} \ell_{2,i}^{(\tau)} \sum_{p\in \cP_i(\vb)}  \alpha_{i,p}^2\|\vb\|_2^2\bigg]\notag\\
&\le \la\wb_{2,r}^{(\tau)}, \vb\ra + \frac{2\eta b\alpha^2}{n}\cdot \la\wb_{2,r}^{(\tau)}, \vb\ra\cdot \sum_{i=1}^n|\ell_{2,i}^{(\tau)}|.
\end{align*}
Taking an absolute value on both sides and then applying Hypothesis \ref{hypothesis}\ref{item:strongfeat_incorrect}, we have
\begin{align*}
|\la\wb_{2,r}^{(t+1)},\vb\ra|& \le |\la\wb_{2,r}^{(T_1)},\vb\ra| + \frac{2\eta b\alpha^2}{n}\cdot\sum_{\tau=T_1}^{t}\sum_{i=1}^n|\ell_{2,i}^{(\tau)}|\la\wb_{2,r}^{(\tau)},\vb\ra|\notag\\
& \le |\la\wb_{2,r}^{(T_1)},\vb\ra| + O\big(|\la\wb_{2,r}^{(T_1)},\vb\ra|\big)\cdot O\bigg(\frac{\eta b\alpha^2}{n}\bigg)\cdot \tilde O\bigg(\frac{\rho n^2}{d\sigma_p^2\eta}+\frac{n}{\eta}\bigg)\notag\\
& =O\big(|\la\wb_{2,r}^{(T_1)},\vb\ra|\big),
\end{align*}
where the second inequality is by Lemma \ref{lemma:bound_sum_lossderivative} and the last inequality is due to the fact that $\rho n=o( d\sigma_p^2)$ and $b\alpha^2=o(1)$.
This completes the proof.

\end{proof}


\begin{lemma}\label{lemma:strongfeat_noise}
Let Hypothesis \ref{hypothesis} holds for all $\tau\le t$, then we have
$|\la\wb_{k,r}^{(t+1)}, \bxi_s^{(q)}\ra|=o\big(1/\polylog(n)\big)$ for all $s\in\cS_0^+\cup\cS_0^-$, $r\in[m]$, $k\in[2]$, and $q\in[P]$.
\end{lemma}
\begin{proof}[Proof of Lemma \ref{lemma:strongfeat_noise}]

By \eqref{eq:update_noise_simplified}, we have
\begin{align*}
|\la \wb_{k,r}^{(t+1)},\bxi_s^{(q)}\ra| &\le |\la\wb_{k,r}^{(t)}, \bxi_s^{(q)}\ra| + |\la\wb_{k,r}^{(t)}, \bxi_s^{(q)}\ra|\cdot \tilde O\bigg(\frac{\eta d\sigma_p^2}{n}\bigg)\cdot |\ell_{k,s}^{(t)}| + \tilde O\bigg(\frac{d^{1/2}\sigma_p^2\eta}{n}\bigg)\cdot\sum_{p\in[P]}\sum_{i=1}^n |\ell_{k,i}^{(t)}|\cdot |\la\wb_{k,r}^{(t)},\bxi_i^{(p)}\ra|\notag\\
&\le |\la\wb_{k,r}^{(T_1)}, \bxi_s^{(q)}\ra| + \sum_{\tau=T_1}^t|\la\wb_{k,r}^{(\tau)}, \bxi_s^{(q)}\ra|\cdot \tilde O\bigg(\frac{\eta d\sigma_p^2}{n}\bigg)\cdot |\ell_{k,s}^{(t)}| \notag\\
&\qquad+ \tilde O\bigg(\frac{d^{1/2}\sigma_p^2\eta}{n}\bigg)\cdot\sum_{\tau=T_1}^t\sum_{p\in[P]}\sum_{i=1}^n |\ell_{k,i}^{(\tau)}|\cdot |\la\wb_{k,r}^{(\tau)},\bxi_i^{(p)}\ra|.
% & = |\la\wb_{k,r}^{(T_1)}, \bxi_s^{(q)}\ra| + O\big(|\la\wb_{k,r}^{(T_1)}, \bxi_s^{(q)}\ra|\big)\cdot \bigg[\tilde O\bigg(\frac{\eta d\sigma_p^2}{n}\bigg)\cdot \sum_{\tau=T_1}^t|\ell_{k,s}^{(\tau)}| + \tilde O\bigg(\frac{Pd^{1/2}\sigma_p^2\eta}{n}\bigg)\cdot\sum_{\tau=T_1}^t\sum_{i=1}^n |\ell_{k,i}^{(\tau)}|\bigg].
\end{align*}
Then by Hypotheses \ref{hypothesis}, we can further get
\begin{align}\label{eq:update_strongfeat_noise_phase3}
|\la \wb_{k,r}^{(t+1)},\bxi_s^{(q)}\ra| \le |\la \wb_{k,r}^{(T_1)},\bxi_s^{(q)}\ra| + o\bigg(\frac{1}{\polylog(n)}\bigg)\cdot \tilde O\bigg(\frac{\eta d\sigma_p^2}{n}\bigg)\cdot \sum_{\tau=T_1}^t|\ell_{k,s}^{(t)}| + \tilde O\bigg(\frac{Pd^{1/2}\sigma_p^2\eta}{n}\bigg)\cdot\sum_{\tau=T_1}^t\sum_{i=1}^n|\ell_{k,i}^{(\tau)}|.
\end{align}
Note that $s\in\cS_0^+\cup\cS_0^-$, 
then by Lemma \ref{lemma:bound_sum_lossderivative}, we have
\begin{align*}
\sum_{t=T_1}^t|\ell_{k,s}^{(\tau)}| = \tilde O\bigg(\frac{1}{\eta}\bigg),\quad \sum_{\tau=T_1}^t\sum_{i=1}^n |\ell_{k,i}^{(\tau)}|\le \tilde O\bigg(\frac{n}{\eta}\bigg).
\end{align*}
Therefore, plugging the above inequalities into \eqref{eq:update_strongfeat_noise_phase3} gives
\begin{align*}
|\la \wb_{k,r}^{(t+1)},\bxi_s^{(q)}&\ra|\le |\la\wb_{k,r}^{(T_1)}, \bxi_s^{(q)}\ra| + o\bigg(\frac{1}{\polylog(n)}\bigg)\cdot \tilde O\bigg(\frac{\eta d\sigma_p^2}{n}\bigg)\cdot \tilde O\bigg(\frac{1}{\eta}\bigg) + \tilde O\bigg(\frac{Pd^{1/2}\sigma_p^2\eta}{n}\bigg)\cdot\tilde O\bigg(\frac{n}{\eta}\bigg)\notag\\
& =o\bigg(\frac{1}{\polylog(n)}\bigg) + o\bigg(\frac{d\sigma_p^2}{n}\bigg) + \tilde O\big(d^{1/2}\sigma_p^2\big)\notag\\
& = o\bigg(\frac{1}{\polylog(n)}\bigg),
\end{align*}
where we use the fact that $d\sigma_p^2=o(n)$ and $d^{1/2}\sigma_p^2=o(1/\polylog(n))$. This completes the proof.
\end{proof}
\begin{lemma}\label{lemma:weakfeat_noise_incorrect}
Let Hypothesis \ref{hypothesis} holds for all $\tau\le t$, then we have
$|\la\wb_{k,r}^{(t+1)}, \bxi_s^{(q)}\ra|=O\big(|\la \wb_{2,r}^{(T_1)},\bxi_s^{(q)}\ra|\big)$ for all $s\in\cS_1^+\cup\cS_1^-$, $r\in[m]$, $k\neq y_s$, and $q\in[P]$.
\end{lemma}
\begin{proof}[Proof of Lemma \ref{lemma:weakfeat_noise_incorrect}]
Similar to the previous proof, we will only prove the argument for $s\in\cS_1^+$, the proof for $s\in\cS_1^-$ can be performed using exactly the same analysis. By \eqref{eq:update_noise_simplified}, we have for $s\in\cS_1^+$
\begin{align*}
|\la \wb_{2,r}^{(t+1)},\bxi_s^{(q)}\ra| &\le |\la\wb_{2,r}^{(t)}, \bxi_s^{(q)}\ra| -|\la\wb_{2,r}^{(t)}, \bxi_s^{(q)}\ra|\cdot \tilde O\bigg(\frac{\eta d\sigma_p^2}{n}\bigg)\cdot |\ell_{2,s}^{(t)}| + \tilde O\bigg(\frac{d^{1/2}\sigma_p^2\eta}{n}\bigg)\cdot\sum_{p\in[P]}\sum_{i=1}^n |\ell_{2,i}^{(t)}|\cdot|\la\wb_{2,r}^{(t)},\bxi_i^{(P)}\ra|\notag\\
&\le |\la\wb_{2,r}^{(T_1)}, \bxi_s^{(q)}\ra| + \sum_{\tau=T_1}^t \tilde O\bigg(\frac{Pd^{1/2}\sigma_p^2\eta}{n}\bigg)\cdot\sum_{i=1}^n |\ell_{k,i}^{(\tau)}|\notag\\
& = |\la \wb_{2,r}^{(T_1)},\bxi_s^{(q)}\ra| +  \tilde O\bigg(\frac{Pd^{1/2}\sigma_p^2\eta}{n}\bigg)\cdot\sum_{\tau=T_1}^t\sum_{i=1}^n |\ell_{2,i}^{(\tau)}|\notag\\
& = o\bigg(\frac{1}{\polylog(n)}\bigg),
\end{align*}
where the last inequality is by Lemma \ref{lemma:bound_sum_lossderivative}.
This completes the proof.


\end{proof}


Finally, we will verify the common features learning (Hypothesis \ref{hypothesis}\ref{item:strongfeat_correct}) and noise learning for rare feature data (Hypothesis \ref{hypothesis}\ref{item:weakfeat_noise_correct}).

\begin{lemma}\label{lemma:strongfeat_correct_phase3}
Let Hypothesis \ref{hypothesis} holds for all $\tau \le t$, then we have $\sum_{r=1}^m(\la\wb_{1,r}^{(t+1)},\vb\ra)^2=\tilde\Theta(1)$ and $\sum_{r=1}^m(\la\wb_{2,r}^{(t+1)},\ub\ra)^2=\tilde\Theta(1)$. 
\end{lemma}
\begin{proof}[Proof of Lemma \ref{lemma:strongfeat_correct_phase3}]
We first prove the upper bound: $\sum_{r=1}^m(\la\wb_{1,r}^{(t+1)},\vb\ra)^2\le\tilde\Theta(1)$. Particularly, by \eqref{eq:update_feature_v_phase2}, \eqref{eq:update_feature_u_phase2} and Definition \ref{def:data_distribution_new}, we have
\begin{align*}
\la\wb_{1,r}^{(t+1)}, \vb\ra &\le \la\wb_{1,r}^{(t)},\vb\ra \cdot\bigg[1 + \Theta\bigg(\frac{\eta}{n}\bigg)\cdot\sum_{i\in\cS_0^+}|\ell_{1,i}^{(t)}| + \Theta\bigg(\frac{ b\alpha^2\eta}{n} \big)\cdot\sum_{i=1}^n |\ell_{1,i}^{(t)}|  \bigg].
% \la\wb_{2,r}^{(t+1)}, \ub\ra &\le \la\wb_{2,r}^{(t)},\ub\ra \cdot\bigg[1 + \Theta\bigg(\frac{ b\alpha^2\eta}{n} \big)\cdot\sum_{i\in\cS_0^-}|\ell_{2,i}^{(t)}| + \Theta\big(\rho b\alpha^2 \eta\big)\cdot\sum_{i=1}^n |\ell_{2,i}^{(t)}|  \bigg]
\end{align*}
Therefore, we can get that
\begin{align}\label{eq:update_feature_learning_phase3_proof}
\sum_{r=1}^m(\la\wb_{1,r}^{(t+1)},\vb\ra)^2 &\le \sum_{r=1}^m(\la\wb_{1,r}^{(t)},\vb\ra)^2  \cdot\bigg[1 + \Theta\bigg(\frac{\eta}{n}\bigg)\cdot\sum_{i\in\cS_0^+}|\ell_{1,i}^{(t)}| + \Theta\bigg(\frac{ b\alpha^2\eta}{n} \bigg)\cdot\sum_{i=1}^n |\ell_{1,i}^{(t)}|  \bigg],
\end{align}
where we use the fact that $|\ell_{1,i}^{(t)}|=|\ell_{2,i}^{(t)}|$. By Hypothesis \ref{hypothesis}, we have for all $\tau\le t$ and $i$,
\begin{align*}
\ell_{1,i}^{(\tau)} = \frac{\exp\big[F_2(\Wb^{(\tau)};\xb_i)-F_1(\Wb^{(\tau)};\xb_i)}{1+ \exp\big[F_2(\Wb^{(\tau)};\xb_i)-F_1(\Wb^{(\tau)};\xb_i)\big]} = \exp\bigg[-\Theta\bigg(\sum_{r=1}^m(\la\wb_{1,r}^{(\tau)},\vb\ra)^2\bigg)\bigg].
\end{align*}
Therefore, let $a_\tau: = \sum_{r=1}^m(\la\wb_{1,r}^{(\tau)},\vb\ra)^2$, we have the following according to \eqref{eq:update_feature_learning_phase3_proof}
\begin{align}\label{eq:update_at_correct_strongfeat}
a_{\tau+1}\le a_\tau \cdot\bigg[1 +  \Theta(\eta)\cdot e^{-c a_\tau}+\Theta\bigg(\frac{ b\alpha^2\eta}{n} \bigg)\cdot\sum_{i=1}^n |\ell_{1,i}^{(\tau)}|\bigg],
\end{align}
where $c$ is an absolute positive constant. Let $T=\polylog(n)$ be the total iteration number, then we will show that $a_t\le 3c^{-1}\log(T)$ for all $\tau\le t$. Particularly, we will prove that either (1) $a_\tau < 2c^{-1}\log(T)$ or (2) $a_{\tau}>2\log(T)>a_{\tau-1}$ but it will not reach $3\log(T)$ as $\tau$ increases before it becomes less than $2c^{-1}\log(T)$ again. The first case immediately implies that $a_\tau<3c^{-1}\log(T)$, so we will only need to focus on case (2). In this case, we have $a_\tau\le a_{\tau-1}+\Theta(\eta)\le 2.1c^{-1}\log(T)$. Then before $a_{\tau}$ becomes less than $2c^{-1}\log(T)$, we have for any $\tau'\in[\tau, t]$ that
\begin{align*}
a_{\tau'} \le a_\tau + \sum_{s=\tau}^{\tau'-1} a_s \cdot\bigg[ \Theta(\eta)\cdot e^{-c a_s}+\Theta\bigg(\frac{ b\alpha^2\eta}{n} \bigg)\cdot\sum_{i=1}^n |\ell_{1,i}^{(s)}|\bigg].
\end{align*}
Note that $a_s\cdot e^{-c a_s} \le 2c^{-1}\log(T)/T^2\le 0.1c^{-1}/T$ if $T=\omega(1)$, then using the fact that $\eta=o(1)$,
\begin{align*}
a_{\tau'} \le a_\tau + \sum_{s=\tau}^{\tau'-1} \bigg[\frac{\eta}{T}+\Theta\bigg(\frac{ b\alpha^2\eta}{n} \bigg)\cdot a_s\sum_{i=1}^n |\ell_{1,i}^{(s)}|\bigg]\le 2.2 c^{-1}\log(T) + \Theta\bigg(\frac{ b\alpha^2\eta}{n} \bigg)\cdot \sum_{s=\tau}^{\tau'-1}  a_s\sum_{i=1}^n |\ell_{1,i}^{(s)}|.
\end{align*}
Then as long as $a_s<10c^{-1}\log(T)$ for $s\in[\tau,\tau']$, we have the following according to Lemma \ref{lemma:bound_sum_lossderivative},
\begin{align*}
\Theta\bigg(\frac{ b\alpha^2\eta}{n} \bigg)\cdot \sum_{s=\tau}^{\tau'-1}  a_s\sum_{i=1}^n |\ell_{1,i}^{(s)}| = \tilde O\bigg(b\alpha^2+\frac{\rho n b\alpha^2}{d\sigma_p^2}\bigg) = o(1)\le 0.1c^{-1}\log(T),
\end{align*}
where we use the fact that $b\alpha^2=o(1)$ and $d\sigma_p^2=\omega(\rho n)$. Therefore, we can conclude that before $\alpha_\tau'$ reaches $10c^{-1}\log(T)$, it must satisfy
\begin{align*}
a_{\tau'}\le 2.3 c^{-1}\log(T),
\end{align*}
for any $\tau'\le t$. This further implies that
\begin{align*}
a_{t+1}\le a_t + \tilde O(\eta)\le 3c^{-1}\log(T) = O\big(\polylog(n)\big),
\end{align*}
which completes the proof of $\sum_{r=1}^m(\la\wb_{1,r}^{(t+1)},\vb\ra)^2=\tilde O(1)$. 

The next step is to show that $\sum_{r=1}^m(\la\wb_{1,r}^{(t+1)},\vb\ra)^2=\tilde \Omega(1)$. Similar to \eqref{eq:update_at_correct_strongfeat}, we can get that
\begin{align*}
a_{\tau+1}\ge a_\tau \cdot\bigg[ 1 + \Theta(\eta)\cdot e^{-C a_\tau}-\Theta\bigg(\frac{ b\alpha^2\eta}{n} \bigg)\cdot\sum_{i=1}^n |\ell_{1,i}^{(\tau)}|\bigg],
\end{align*}
where $C$ is an absolute positive constant. In fact, we must have $a_\tau\ge \frac{1}{\polylog(n)}$ since otherwise,
\begin{align*}
a_{\tau+1}\ge a_\tau \cdot\bigg[ 1 + \Theta(\eta)\cdot e^{-C a_\tau}-\Theta\bigg(\frac{ b\alpha^2\eta}{n} \bigg)\cdot\sum_{i=1}^n |\ell_{1,i}^{(\tau)}|\bigg]\ge a_\tau\cdot\big[ 1 +\Theta(\eta)\big],
\end{align*}
where the first inequality is due to $e^{-ca_\tau}=\Theta(1)$ if $a_\tau=O(1)$ and the second inequality is due to $|\ell_{1,i}^{(\tau)}|\le 1$ and $b\alpha^2=o(1)$. This implies that $a_{\tau+1}$ will keep increase, which will at least continue to the case that $a_\tau>1$. This completes the proof that $a_{t+1}=\tilde\Omega(1)$. 

The proof for $\sum_{r=1}^m(\la\wb_{2,r}^{(t+1)},\ub\ra)^2=\tilde \Theta(1)$ will be basically the same so we omit it here.




\end{proof}

\begin{lemma}\label{lemma:weakfeat_correctnoise_phase3}
Let Hypothesis \ref{hypothesis} holds for all $\tau \le t$, then we have $\sum_{r=1}^m\sum_{q\in\cP_s(\bxi)}(\la\wb_{1,r}^{(t+1)},\bxi_s^{(q)}\ra)^2=\tilde\Theta(1)$ for all $s\in\cS_1^+$, and $\sum_{r=1}^m\sum_{q\in\cP_s(\bxi)}(\la\wb_{2,r}^{(t+1)},\bxi_i^{(q)}\ra)^2=\tilde\Theta(1)$ for all $s\in\cS_1^-$. 
\end{lemma}
\begin{proof}[Proof of Lemma \ref{lemma:weakfeat_correctnoise_phase3}]
Note that $P,m=\Theta(\polylog(n))$, it suffices to prove that $\max_{q,r}(\la\wb_{1,r}^{(t+1)},\bxi_s^{(q)}\ra)^2=\tilde \Theta(1)$ for all $s\in\cS_1^+$
 and $\max_{q,r}(\la\wb_{2,r}^{(t+1)},\bxi_s^{(q)}\ra)^2=\tilde \Theta(1)$ all $s\in\cS_1^-$. In the following proof we will only consider $s\in\cS_1^+$ as the proof for $s\in\cS_1^-$ will exactly the same.


By \eqref{eq:update_noise_simplified}, we have for all $s\in\cS_1^+$,
\begin{align}\label{eq:noise_weakdata_phase3}
\la\wb_{1,r}^{(\tau+1)},\bxi_s^{(q)}\ra 
& = \la\wb_{1,r}^{(\tau)},\bxi_s^{(q)}\ra \cdot\bigg[ 1 + \frac{2\eta}{n}\cdot \ell_{1,s}^{(\tau)}\cdot\|\bxi_s^{(q)}\|_2^2\bigg] \pm \frac{2\eta}{n}\cdot\tilde O\big(d^{1/2}\sigma_p^2\big)\cdot\sum_{i\neq s || p\neq q} |\ell_{k,i}^{(\tau)}|\cdot |\la\wb_{1,r}^{(\tau)},\bxi_i^{(p)}\ra|.
\end{align}
We first prove the upper bound of $\sum_{r=1}^m\sum_{q\in\cP_s(\bxi)}(\la\wb_{2,r}^{(\tau)},\bxi_s^{(q)}\ra)^2$. Then, using the Hypothesis \ref{hypothesis} \ref{item:weakfeat_noise_correct}, we have for any $i\in[n]$,  $s\in\cS_1^+$, $r\in[m]$, and $p\in[P]$
\begin{align*}
(\la\wb_{1,r}^{(\tau)},\bxi_i^{(p)}\ra)^2\le O\bigg(\polylog(n)\bigg)\cdot O(mP)\cdot \max_{r,q}(\la\wb_{1,r}^{(\tau)},\bxi_s^{(q)}\ra)^2 =O\bigg(\polylog(n)\bigg)\cdot  \max_{r,q}(\la\wb_{1,r}^{(\tau)},\bxi_s^{(q)}\ra)^2.
\end{align*}
Then \eqref{eq:noise_weakdata_phase3} implies that
\begin{align*}
\max_{r,q}(\la\wb_{1,r}^{(\tau+1)},\bxi_s^{(q)}\ra)^2 
& \le  \max_{r,q}(\la\wb_{1,r}^{(\tau)},\bxi_s^{(q)}\ra)^2 \cdot\bigg[ 1 + \Theta\bigg(\frac{\eta d\sigma_p^2}{n}\bigg)\cdot \ell_{1,s}^{(\tau)} + \tilde O\bigg(\frac{\eta P d^{1/2}\sigma_p^2}{n}\bigg)\cdot\sum_{i=1}^n |\ell_{1,i}^{(\tau)}|\bigg]. 
\end{align*}

% , we can get
% \begin{align*}
%  (\la\wb_{1,r}^{(\tau+1)},\bxi_s^{(q)}\ra)^2 
% & \le  (\la\wb_{1,r}^{(\tau)},\bxi_s^{(q)}\ra)^2 \cdot\bigg[ 1 + \Theta\bigg(\frac{\eta d\sigma_p^2}{n}\bigg)\cdot \ell_{1,s}^{(\tau)}\big] + \tilde O\bigg(\frac{\eta P d^{1/2}\sigma_p^2}{n}\bigg)\cdot\sum_{i=1}^n |\ell_{1,i}^{(\tau)}|\bigg].   
% \end{align*}
% Taking the sum over $r\in[m]$ and $q\in\cP_s(\bxi)$, we have for all $\tau\in[T_1,t]$
% \begin{align*}
% \sum_{r=1}^m\sum_{q\in\cP_s(\bxi)}(\la\wb_{1,r}^{(\tau+1)},\bxi_s^{(q)}\ra)^2 
% & \le  \sum_{r=1}^m\sum_{q\in\cP_s(\bxi)}(\la\wb_{1,r}^{(\tau)},\bxi_s^{(q)}\ra)^2 \cdot\bigg[ 1 + \Theta\bigg(\frac{\eta d\sigma_p^2}{n}\bigg)\cdot \ell_{1,s}^{(\tau)} + \tilde O\bigg(\frac{\eta P d^{1/2}\sigma_p^2}{n}\bigg)\cdot\sum_{i=1}^n |\ell_{1,i}^{(\tau)}|\bigg],
% \end{align*}
% where $c$ is an absolute constant.
Then by Hypothesis \ref{hypothesis}, we can further get that the quantity $\sum_{r=1}^m\sum_{q\in\cP_s(\bxi)}(\la\wb_{1,r}^{(\tau+1)},\bxi_s^{(q)}\ra)^2$ will be the dominating term in the neural network output function, so that $\ell_{1,s}^{(\tau)}\ge e^{-c \max_{r,q}(\la\wb_{1,r}^{(\tau)},\bxi_s^{(q)}\ra)^2 }$ for some constant $c$. Therefore, let $a_\tau = \max_{r,q}(\la\wb_{1,r}^{(\tau)},\bxi_s^{(q)}\ra)^2$, we can follow the similar derivation of \eqref{eq:update_at_correct_strongfeat}. Thus, it follows that
\begin{align*}
a_{\tau+1}\le a_\tau\cdot\bigg[ 1 + \Theta\bigg(\frac{\eta d\sigma_p^2}{n}\bigg)\cdot e^{-c a_\tau} + \tilde O\bigg(\frac{\eta P d^{1/2}\sigma_p^2}{n}\bigg)\cdot\sum_{i=1}^n |\ell_{1,i}^{(\tau)}|\bigg]
\end{align*}
Then we can follow the exact proof technique in Lemma \ref{lemma:strongfeat_correct_phase3} to conclude that $a_{t+1}=\tilde O(1)$, while it only requires to verify that
\begin{align*}
\tilde O\bigg(\frac{\eta P d^{1/2}\sigma_p^2}{n}\bigg)\cdot\sum_{\tau=T_1}^t\sum_{i=1}^n |\ell_{1,i}^{(\tau)}|=o(1),
\end{align*}
which clearly holds by Lemma \ref{lemma:bound_sum_lossderivative} and the fact that $P d^{1/2}\sigma_p^2=o(1)$. 

The lower bound can be similarly obtained as the following can be deduced by \eqref{eq:noise_weakdata_phase3}:
\begin{align*}
\max_{r,q}(\la\wb_{1,r}^{(\tau+1)},\bxi_s^{(q)}\ra)^2 
& \ge  \max_{r,q}(\la\wb_{1,r}^{(\tau)},\bxi_s^{(q)}\ra)^2 \cdot\bigg[ 1 + \Theta\bigg(\frac{\eta d\sigma_p^2}{n}\bigg)\cdot \ell_{1,s}^{(\tau)} - \tilde O\bigg(\frac{\eta P d^{1/2}\sigma_p^2}{n}\bigg)\cdot\sum_{i=1}^n |\ell_{1,i}^{(\tau)}|\bigg],
\end{align*}
which leads to 
\begin{align*}
a_{\tau+1}\ge a_\tau\cdot\bigg[ 1 + \Theta\bigg(\frac{\eta d\sigma_p^2}{n}\bigg)\cdot e^{-C a_\tau} - \tilde O\bigg(\frac{\eta P d^{1/2}\sigma_p^2}{n}\bigg)\cdot\sum_{i=1}^n |\ell_{1,i}^{(\tau)}|\bigg]
\end{align*}
for some absolute constant $C$. Then following the same proof of Lemma \ref{lemma:strongfeat_correct_phase3}, we can get that $a_{t+1}=\tilde\Omega(1)$. This completes the proof.
\end{proof}



\subsection{Proof of Theorem \ref{thm:std_training}}\label{sec:proof_main_std}
\begin{proof}[Proof of Theorem \ref{thm:std_training}.]
We first show that $\|\wb_{k,r}^{(T)}\|_2 = \tilde O(n)$ for all $k\in[2]$ and $r\in[m]$. In particular, note that the update of standard training is always the linear combination of all critical vectors, i.e., $\vb$, $\ub$, $\vb'$, $\ub'$, and $\bxi_i^{(p)}$'s. Therefore, we have
\begin{align*}
\wb_{k,r}^{(t)} = \wb_{k,r}^{(0)} + \rho_{k,r}^{(t)}(\vb) \cdot \vb + \rho_{k,r}^{(t)}(\ub) \cdot \ub + \rho_{k,r}^{(t)}(\vb') \cdot \vb' + \rho_{k,r}^{(t)}(\ub') \cdot \ub' + \sum_{i=1}^n\sum_{p\in\cP_i(\bxi)}\rho_{k,r}^{(t)}(\bxi_i^{(p)}) \cdot \bxi_i^{(p)}.
\end{align*}
Here we use $\rho_{k,r}^{(t)}(\ab)$ to denote the coefficient of $\ab$ for all $\ab\in\{\vb,\ub,\vb',\ub'\}\cup\{\bxi\}$. Then by Lemma \ref{lemma:standard_learning_phase3_main} and using the fact that $\|\vb\|_2,\|\ub\|_2, \|\vb'\|_2, \|\ub'\|_2=1$, we have
\begin{align*}
|\rho_{k,r}^{(t)}(\vb)|, |\rho_{k,r}^{(t)}(\ub)| =\tilde O(1), \quad |\rho_{k,r}^{(t)}(\vb')|, |\rho_{k,r}^{(t)}(\ub')| =o\bigg(\frac{1}{\polylog(n)}\bigg).
\end{align*}
Moreover, using the fact that $|\la\bxi_i^{(p)},\bxi_j^{(q)}\ra|=o(1/\polylog(n))$ for any $i\neq j$ or $p\neq q$, applying Lemma \ref{lemma:standard_learning_phase3_main} and the fact that $\|\bxi_i^{(p)}\|_2^2=\Omega(1)$ for all $i\in[n]$ and $p\in[P]$, we have
\begin{align*}
\bigg\|\sum_{i=1}^n\sum_{p\in\cP_i(\bxi)}\rho_{k,r}^{(t)}(\bxi_i^{(p)}) \cdot \bxi_i^{(p)}\bigg\|_2^2\le \tilde O(n^2).
\end{align*}
Combining the above results, we can readily conclude that $\|\wb_{k,r}^{(t)}\|_2=\tilde O(n)$.


Then we will characterize the test errors for common
feature data and rare feature data separately. 
Regarding the common feature data, we can take a positive common feature data $(\xb, 1)$ as an example and obtain the following by Lemma \ref{lemma:standard_learning_phase3_main},
\begin{align}\label{eq:correct_output_std_training_strong}
F_1(\Wb^{(t)};\xb) = \sum_{r=1}^m \sum_{p=1}^P\big(\la\wb_{1,r}^{(t)},\xb^{(p)}\ra\big)^2 \ge \sum_{r=1}^m\sum_{p:\xb^{(p)}=\vb} \big(\la\wb_{1,r}^{(t)},\vb\ra\big)^2=\tilde\Theta(1).
\end{align}
Besides, we have the following regarding $F_2(\Wb^{(t)};\xb)$:
\begin{align}\label{eq:wrong_output_std_training_strong}
F_2(\Wb^{(t)};\xb) &= \sum_{r=1}^m\sum_{p:\xb^{(p)}=\vb} \big(\la\wb_{2,r}^{(t)},\vb\ra\big)^2 + \sum_{r=1}^m\sum_{p:\xb^{(p)}\neq \vb} \big(\la\wb_{2,r}^{(t)},\xb^{(p)}\ra\big)^2 \notag\\
&= \sum_{r=1}^m\sum_{p:\xb^{(p)}\neq \vb} \big(\la\wb_{2,r}^{(t)},\xb^{(p)}\ra\big)^2 + o\bigg(\frac{1}{\polylog(n)}\bigg).
\end{align}
where we use the result  $|\la\wb_{2,r}^{(t)},\vb\ra|=o\big(1/\polylog(n)\big)$. Then, note that if $\xb^{(p)}\neq \vb$, it can be either feature noise (i.e., $\alpha\ub$ or $\alpha\vb$) or random noise $\bzeta_i^{(p)}$, which is independent of the random noise vectors in the training data points (i.e., $\{\bxi\}$). Therefore, using the result that $\|\wb_{k,r}^{(t)}\|_2=\tilde O(n)$, we can obtain with probability at least $1- \exp(-\Omega(d^{1/2}))$, it holds that for all $r\in[m]$
\begin{align}\label{eq:noise_test_std_training}
(\la\wb_{2,r}^{(t)},\bzeta_i^{(p)}\ra)^2 = \tilde O(\sigma_p^2n^2).
\end{align}
Besides, note that there are at most $b$ patches within the total $P$ patches that are feature noise, we have
\begin{align*}
\sum_{r=1}^m\sum_{p:\xb^{(p)}\neq \vb} \big(\la\wb_{2,r}^{(t)},\xb^{(p)}\ra\big)^2\le O(mb\alpha^2) + \tilde O(mP\sigma_p^2 n^2) = o\bigg(\frac{1}{\polylog(n)}\bigg),
\end{align*}
where the last equality is by the data model in Definition \ref{def:data_distribution_new}: $b\alpha^2=o\big(1/\polylog(n)\big)$ and $\sigma_p = o(d^{-1/2}n^{1/2})$.
Therefore, comparing \eqref{eq:correct_output_std_training_strong} and \eqref{eq:wrong_output_std_training_strong}, we can get $F_1(\Wb^{(t)};\xb)>F_2(\Wb^{(t)};\xb)$ with probability at least $1-1/\poly(n)$. 
% Consequently, this implies that  
% \begin{align*}
% \PP_{(\xb, y)\sim \cD_{\mathrm{strong}}}[ \argmax_k F_k(\Wb^{(t)};\xb)\neq y] .
% \end{align*}




Then we will move on to study  the rare feature data. In particular, we consider the rare feature data with incorrect feature noise. Without loss of generality, we take a positive data $(\xb,1)$ as an example, which contains rare feature $\vb$ and incorrect feature noise $\alpha\ub$. Then we can get the following results for $F_k(\Wb^{(t)};\xb)$
\begin{align*}
F_k(\Wb^{(t)};\xb) &= \sum_{r=1}^m\sum_{p:\xb^{(p)}=\vb'} \big(\la\wb_{k,r}^{(t)},\vb'\ra\big)^2 + \sum_{r=1}^m\sum_{p:\xb^{(p)}=\alpha\ub} \big(\la\wb_{k,r}^{(t)},\alpha\ub\ra\big)^2 + \sum_{r=1}^m\sum_{p:\xb^{(p)}\not\in\{\vb',\alpha\ub\}} \big(\la\wb_{k,r}^{(t)},\xb^{(p)}\ra\big)^2.
\end{align*}
Note that if $\xb^{(p)}\not\in\{\vb',\alpha\ub\}$, then $\xb^{(p)}$ must be a random noise vector that is independent of $\wb_{k,r}^{(t)}$. To begin with, the first two terms of the above equation for different $k$'s can be bounded by applying Lemma \ref{lemma:standard_learning_phase3_main} (particularly $\sum_{r=1}^m(\la\wb_{2,r}^{(t)},\ub\ra)^2=\tilde\Omega(1)$), we have
\begin{align*}
&\sum_{r=1}^m\sum_{p:\xb^{(p)}=\vb'} \big(\la\wb_{1,r}^{(t)},\vb'\ra\big)^2 = \tilde O(\sigma_0^2), \quad \sum_{r=1}^m\sum_{p:\xb^{(p)}=\alpha\ub}\big(\la\wb_{1,r}^{(t)},\alpha\ub\ra\big)^2 = \tilde O(b\alpha^2\sigma_0^2),\notag\\
&\sum_{r=1}^m\sum_{p:\xb^{(p)}=\vb'} \big(\la\wb_{2,r}^{(t)},\vb'\ra\big)^2 = \tilde O(\sigma_0^2), \quad \sum_{r=1}^m\sum_{p:\xb^{(p)}=\alpha\ub}\big(\la\wb_{2,r}^{(t)},\alpha\ub\ra\big)^2 = \tilde\Omega(\alpha^2).
\end{align*}
Moreover, by \eqref{eq:noise_test_std_training}, we can further get that with probability at least $1-\exp(-\Omega(d^{1/2}))>1-1/\poly(n)$, we have
\begin{align*}
\sum_{r=1}^m\sum_{p:\xb^{(p)}\not\in\{\vb',\alpha\ub\}} \big(\la\wb_{k,r}^{(t)},\xb^{(p)}\ra\big)^2 = \tilde O(mP\sigma_p^2 n^2) = o(\alpha^2). 
\end{align*}
where the last equality is by our data model in Definition \ref{def:data_distribution_new}. This further implies that 
 conditioning on $\Wb^{(t)}$, with probability at least $1-1/\poly(n)$, we have
 \begin{align*}
 F_2(\Wb^{(t)};\xb)>  F_1(\Wb^{(t)};\xb)
 \end{align*}
on the positive rare feature data that has incorrect feature noise. 
\begin{align*}
\PP_{(\xb, y)\sim \cD_{\mathrm{rare}}}[ \argmax_k F_k(\Wb^{(t)};\xb)\neq y] \ge \frac{1}{2} - \frac{1}{\poly(n)}\ge \frac{1}{2.01}.
\end{align*}
Therefore, combining the test error analysis for common feature data and rare feature data and using the fact that the fraction of rare feature data is $\rho$, we can finally obtain:
\begin{align*}
\PP_{(\xb, y)\sim \cD}[ \argmax_k F_k(\Wb^{(t)};\xb)\neq y] \ge \rho \cdot \PP_{(\xb, y)\sim \cD_{\mathrm{rare}}}[ \argmax_k F_k(\Wb^{(t)};\xb)\neq y]  \ge \frac{\rho}{2.01}.
\end{align*}
This completes the proof.

\end{proof}


