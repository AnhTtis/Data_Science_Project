@inproceedings{zhang2018mixup,
  title={mixup: Beyond Empirical Risk Minimization},
  author={Zhang, Hongyi and Cisse, Moustapha and Dauphin, Yann N and Lopez-Paz, David},
  booktitle={International Conference on Learning Representations},
  year={2018}
}
@article{thulasidasan2019mixup,
  title={On mixup training: Improved calibration and predictive uncertainty for deep neural networks},
  author={Thulasidasan, Sunil and Chennupati, Gopinath and Bilmes, Jeff A and Bhattacharya, Tanmoy and Michalak, Sarah},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}
@article{carratino2020mixup,
  title={On mixup regularization},
  author={Carratino, Luigi and Ciss{\'e}, Moustapha and Jenatton, Rodolphe and Vert, Jean-Philippe},
  journal={arXiv preprint arXiv:2006.06049},
  year={2020}
}
@inproceedings{zhang2022and,
  title={When and how mixup improves calibration},
  author={Zhang, Linjun and Deng, Zhun and Kawaguchi, Kenji and Zou, James},
  booktitle={International Conference on Machine Learning},
  pages={26135--26160},
  year={2022},
  organization={PMLR}
}
@inproceedings{shen2022data,
  title={Data augmentation as feature manipulation},
  author={Shen, Ruoqi and Bubeck, Sebastien and Gunasekar, Suriya},
  booktitle={International Conference on Machine Learning},
  pages={19773--19808},
  year={2022},
  organization={PMLR}
}

@InProceedings{han2022mixupgraph,
  title = 	 {G-Mixup: Graph Data Augmentation for Graph Classification},
  author =       {Han, Xiaotian and Jiang, Zhimeng and Liu, Ninghao and Hu, Xia},
  booktitle = 	 {Proceedings of the 39th International Conference on Machine Learning},
  pages = 	 {8230--8248},
  year = 	 {2022},
  volume = 	 {162},
  month = 	 {17--23 Jul},
  publisher =    {PMLR},
}

@article{chidambaram2022provably,
  title={Provably Learning Diverse Features in Multi-View Data with Midpoint Mixup},
  author={Chidambaram, Muthu and Wang, Xiang and Wu, Chenwei and Ge, Rong},
  journal={arXiv preprint arXiv:2210.13512},
  year={2022}
}
@inproceedings{chidambaram2021towards,
  title={Towards Understanding the Data Dependency of Mixup-style Training},
  author={Chidambaram, Muthu and Wang, Xiang and Hu, Yuzheng and Wu, Chenwei and Ge, Rong},
  booktitle={International Conference on Learning Representations},
  year={2021}
}
@inproceedings{guo2019mixup,
  title={Mixup as locally linear out-of-manifold regularization},
  author={Guo, Hongyu and Mao, Yongyi and Zhang, Richong},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={33},
  number={01},
  pages={3714--3722},
  year={2019}
}
@inproceedings{zhang2020does,
  title={How Does Mixup Help With Robustness and Generalization?},
  author={Zhang, Linjun and Deng, Zhun and Kawaguchi, Kenji and Ghorbani, Amirata and Zou, James},
  booktitle={International Conference on Learning Representations},
  year={2020}
}
@article{bartlett2020benign,
  title={Benign overfitting in linear regression},
  author={Bartlett, Peter L and Long, Philip M and Lugosi, G{\'a}bor and Tsigler, Alexander},
  journal={Proceedings of the National Academy of Sciences},
  year={2020},
  publisher={National Acad Sciences}
}
@article{allen2020towards,
  title={Towards Understanding Ensemble, Knowledge Distillation and Self-Distillation in Deep Learning},
  author={Allen-Zhu, Zeyuan and Li, Yuanzhi},
  journal={arXiv preprint arXiv:2012.09816},
  year={2020}
}
@article{chen2020dimension,
  title={Dimension Independent Generalization Error with Regularized Online Optimization},
  author={Chen, Xi and Liu, Qiang and Tong, Xin T},
  journal={arXiv preprint arXiv:2003.11196},
  year={2020}
}
@inproceedings{balles2018dissecting,
  title={Dissecting adam: The sign, magnitude and variance of stochastic gradients},
  author={Balles, Lukas and Hennig, Philipp},
  booktitle={International Conference on Machine Learning},
  pages={404--413},
  year={2018},
  organization={PMLR}
}
@article{anandkumar2017analyzing,
  title={Analyzing tensor power method dynamics in overcomplete regime},
  author={Anandkumar, Animashree and Ge, Rong and Janzamin, Majid},
  journal={The Journal of Machine Learning Research},
  volume={18},
  number={1},
  pages={752--791},
  year={2017},
  publisher={JMLR. org}
}
@inproceedings{bernstein2018signsgd,
  title={signSGD: Compressed optimisation for non-convex problems},
  author={Bernstein, Jeremy and Wang, Yu-Xiang and Azizzadenesheli, Kamyar and Anandkumar, Animashree},
  booktitle={International Conference on Machine Learning},
  pages={560--569},
  year={2018},
  organization={PMLR}
}
@article{dieuleveut2017harder,
  title={Harder, better, faster, stronger convergence rates for least-squares regression},
  author={Dieuleveut, Aymeric and Flammarion, Nicolas and Bach, Francis},
  journal={The Journal of Machine Learning Research},
  volume={18},
  number={1},
  pages={3520--3570},
  year={2017},
  publisher={JMLR. org}
}
@article{jain2017parallelizing,
  title={Parallelizing stochastic gradient descent for least squares regression: mini-batching, averaging, and model misspecification},
  author={Jain, Prateek and Netrapalli, Praneeth and Kakade, Sham M and Kidambi, Rahul and Sidford, Aaron},
  journal={The Journal of Machine Learning Research},
  volume={18},
  number={1},
  pages={8258--8299},
  year={2017},
  publisher={JMLR. org}
}
@book{vershynin2018high,
  title={High-dimensional probability: An introduction with applications in data science},
  author={Vershynin, Roman},
  volume={47},
  year={2018},
  publisher={Cambridge university press}
}
@article{jain2017markov,
  title={A markov chain theory approach to characterizing the minimax optimality of stochastic gradient descent (for least squares)},
  author={Jain, Prateek and Kakade, Sham M and Kidambi, Rahul and Netrapalli, Praneeth and Pillutla, Venkata Krishna and Sidford, Aaron},
  journal={arXiv preprint arXiv:1710.09430},
  year={2017}
}
@article{bach2013non,
  title={Non-strongly-convex smooth stochastic approximation with convergence rate $o(1/n)$},
  author={Bach, Francis and Moulines, Eric},
  journal={Advances in neural information processing systems},
  volume={26},
  pages={773--781},
  year={2013}
}
@article{muthukumar2020classification,
  title={Classification vs regression in overparameterized regimes: Does the loss function matter?},
  author={Muthukumar, Vidya and Narang, Adhyyan and Subramanian, Vignesh and Belkin, Mikhail and Hsu, Daniel and Sahai, Anant},
  journal={arXiv preprint arXiv:2005.08054},
  year={2020}
}
@article{hastie2019surprises,
  title={Surprises in high-dimensional ridgeless least squares interpolation},
  author={Hastie, Trevor and Montanari, Andrea and Rosset, Saharon and Tibshirani, Ryan J},
  journal={arXiv preprint arXiv:1903.08560},
  year={2019}
}
@article{tsigler2020benign,
  title={Benign overfitting in ridge regression},
  author={Tsigler, Alexander and Bartlett, Peter L},
  journal={arXiv preprint arXiv:2009.14286},
  year={2020}
}
@article{mei2019generalization,
  title={The generalization error of random features regression: Precise asymptotics and double descent curve},
  author={Mei, Song and Montanari, Andrea},
  journal={arXiv preprint arXiv:1908.05355},
  year={2019}
}
@article{nakkiran2019deep,
  title={Deep double descent: Where bigger models and more data hurt},
  author={Nakkiran, Preetum and Kaplun, Gal and Bansal, Yamini and Yang, Tristan and Barak, Boaz and Sutskever, Ilya},
  journal={arXiv preprint arXiv:1912.02292},
  year={2019}
}
@inproceedings{thrampoulidis2015regularized,
  title={Regularized linear regression: A precise analysis of the estimation error},
  author={Thrampoulidis, Christos and Oymak, Samet and Hassibi, Babak},
  booktitle={Conference on Learning Theory},
  pages={1683--1709},
  year={2015},
  organization={PMLR}
}
@article{dobriban2018high,
  title={High-dimensional asymptotics of prediction: Ridge regression and classification},
  author={Dobriban, Edgar and Wager, Stefan and others},
  journal={The Annals of Statistics},
  volume={46},
  number={1},
  pages={247--279},
  year={2018},
  publisher={Institute of Mathematical Statistics}
}
@article{belkin2020two,
  title={Two models of double descent for weak features},
  author={Belkin, Mikhail and Hsu, Daniel and Xu, Ji},
  journal={SIAM Journal on Mathematics of Data Science},
  volume={2},
  number={4},
  pages={1167--1180},
  year={2020},
  publisher={SIAM}
}
@article{nakkiran2020optimal,
  title={Optimal regularization can mitigate double descent},
  author={Nakkiran, Preetum and Venkat, Prayaag and Kakade, Sham and Ma, Tengyu},
  journal={arXiv preprint arXiv:2003.01897},
  year={2020}
}
@book{shalev2014understanding,
  title={Understanding machine learning: From theory to algorithms},
  author={Shalev-Shwartz, Shai and Ben-David, Shai},
  year={2014},
  publisher={Cambridge university press}
}
@article{wang2020benign,
  title={Benign Overfitting in Binary Classification of Gaussian Mixtures},
  author={Wang, Ke and Thrampoulidis, Christos},
  journal={arXiv preprint arXiv:2011.09148},
  year={2020}
}
@article{chatterji2020finite,
  title={Finite-sample analysis of interpolating linear classifiers in the overparameterized regime},
  author={Chatterji, Niladri S and Long, Philip M},
  journal={arXiv preprint arXiv:2004.12019},
  year={2020}
}

@article{zhang2016understanding,
  title={Understanding deep learning requires rethinking generalization},
  author={Zhang, Chiyuan and Bengio, Samy and Hardt, Moritz and Recht, Benjamin and Vinyals, Oriol},
  journal={arXiv preprint arXiv:1611.03530},
  year={2016}
}

@inproceedings{neyshabur2017exploring,
  title={Exploring generalization in deep learning},
  author={Neyshabur, Behnam and Bhojanapalli, Srinadh and McAllester, David and Srebro, Nati},
  booktitle={Advances in neural information processing systems},
  pages={5947--5956},
  year={2017}
}


@inproceedings{lakshminarayanan2018linear,
  title={Linear stochastic approximation: How far does constant step-size and iterate averaging go?},
  author={Lakshminarayanan, Chandrashekar and Szepesvari, Csaba},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={1347--1355},
  year={2018}
}

@inproceedings{defossez2015averaged,
  title={Averaged least-mean-squares: Bias-variance trade-offs and optimal sampling distributions},
  author={D{\'e}fossez, Alexandre and Bach, Francis},
  booktitle={Artificial Intelligence and Statistics},
  pages={205--213},
  year={2015}
}

@Article{DieuleveutB15,
  title =	{Non-parametric Stochastic Approximation with Large Step sizes},
  author =	"Aymeric Dieuleveut and Francis R. Bach",
  journal = "The Annals of Statistics",
  year = 	"2015",
}
@article{guo2019augmenting,
  title={Augmenting data with mixup for sentence classification: An empirical study},
  author={Guo, Hongyu and Mao, Yongyi and Zhang, Richong},
  journal={arXiv preprint arXiv:1905.08941},
  year={2019}
}
@inproceedings{yun2019cutmix,
  title={Cutmix: Regularization strategy to train strong classifiers with localizable features},
  author={Yun, Sangdoo and Han, Dongyoon and Oh, Seong Joon and Chun, Sanghyuk and Choe, Junsuk and Yoo, Youngjoon},
  booktitle={Proceedings of the IEEE/CVF international conference on computer vision},
  pages={6023--6032},
  year={2019}
}
@article{chen2022towards,
  title={Towards understanding mixture of experts in deep learning},
  author={Chen, Zixiang and Deng, Yihe and Wu, Yue and Gu, Quanquan and Li, Yuanzhi},
  journal={arXiv preprint arXiv:2208.02813},
  year={2022}
}
@inproceedings{wen2021toward,
  title={Toward understanding the feature learning process of self-supervised contrastive learning},
  author={Wen, Zixin and Li, Yuanzhi},
  booktitle={International Conference on Machine Learning},
  pages={11112--11122},
  year={2021},
  organization={PMLR}
}
@article{zou2021understanding,
  title={Understanding the generalization of adam in learning neural networks with proper regularization},
  author={Zou, Difan and Cao, Yuan and Li, Yuanzhi and Gu, Quanquan},
  journal={arXiv preprint arXiv:2108.11371},
  year={2021}
}
@article{chen2020group,
  title={A group-theoretic framework for data augmentation},
  author={Chen, Shuxiao and Dobriban, Edgar and Lee, Jane H},
  journal={The Journal of Machine Learning Research},
  volume={21},
  number={1},
  pages={9885--9955},
  year={2020},
  publisher={JMLRORG}
}
@inproceedings{rajput2019does,
  title={Does data augmentation lead to positive margin?},
  author={Rajput, Shashank and Feng, Zhili and Charles, Zachary and Loh, Po-Ling and Papailiopoulos, Dimitris},
  booktitle={International Conference on Machine Learning},
  pages={5321--5330},
  year={2019},
  organization={PMLR}
}
@InProceedings{pmlr-v119-wu20g,
  title = 	 {On the Generalization Effects of Linear Transformations in Data Augmentation},
  author =       {Wu, Sen and Zhang, Hongyang and Valiant, Gregory and Re, Christopher},
  booktitle = 	 {Proceedings of the 37th International Conference on Machine Learning},
  pages = 	 {10410--10420},
  year = 	 {2020},
  editor = 	 {III, Hal Daumé and Singh, Aarti},
  volume = 	 {119},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {13--18 Jul},
  publisher =    {PMLR},
}

@article{hanin2021data,
  title={How data augmentation affects optimization for linear regression},
  author={Hanin, Boris and Sun, Yi},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={8095--8105},
  year={2021}
}
@inproceedings{dao2019kernel,
  title={A kernel theory of modern data augmentation},
  author={Dao, Tri and Gu, Albert and Ratner, Alexander and Smith, Virginia and De Sa, Chris and R{\'e}, Christopher},
  booktitle={International Conference on Machine Learning},
  pages={1528--1537},
  year={2019},
  organization={PMLR}
}
@article{bishop1995training,
  title={Training with noise is equivalent to Tikhonov regularization},
  author={Bishop, Chris M},
  journal={Neural computation},
  volume={7},
  number={1},
  pages={108--116},
  year={1995},
  publisher={MIT Press}
}
@inproceedings{parkunified,
  title={A Unified Analysis of Mixed Sample Data Augmentation: A Loss Function Perspective},
  author={Park, Chanwoo and Yun, Sangdoo and Chun, Sanghyuk},
  booktitle={Advances in Neural Information Processing Systems},
  year={2022}
}
@inproceedings{chen2020mixtext,
  title={MixText: Linguistically-Informed Interpolation of Hidden Space for Semi-Supervised Text Classification},
  author={Chen, Jiaao and Yang, Zichao and Yang, Diyi},
  booktitle={Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
  pages={2147--2157},
  year={2020}
}
@article{berthelot2019mixmatch,
  title={Mixmatch: A holistic approach to semi-supervised learning},
  author={Berthelot, David and Carlini, Nicholas and Goodfellow, Ian and Papernot, Nicolas and Oliver, Avital and Raffel, Colin A},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}
@article{berthier2020tight,
  title={Tight Nonparametric Convergence Rates for Stochastic Gradient Descent under the Noiseless Linear Model},
  author={Berthier, Rapha{\"e}l and Bach, Francis and Gaillard, Pierre},
  journal={arXiv preprint arXiv:2006.08212},
  year={2020}
}
@article{li2020benign,
  title={Benign Overfitting and Noisy Features},
  author={Li, Zhu and Su, Weijie and Sejdinovic, Dino},
  journal={arXiv preprint arXiv:2008.02901},
  year={2020}
}

@article{polyak1992acceleration,
  title={Acceleration of stochastic approximation by averaging},
  author={Polyak, Boris T and Juditsky, Anatoli B},
  journal={SIAM journal on control and optimization},
  volume={30},
  number={4},
  pages={838--855},
  year={1992},
  publisher={SIAM}
}

@Article{HsuKZ14,
  title =       "Random Design Analysis of Ridge Regression",
  author =      "Daniel J. Hsu and Sham M. Kakade and Tong Zhang",
  journal =     "Foundations of Computational Mathematics",
  year =        "2014",
  number =      "3",
  volume =      "14",
  pages =       "569--600",
}

@article{jain2018accelerating,
	title={Accelerating Stochastic Gradient Descent for Least Squares Regression},
	author={Jain, Prateek and Kakade, M. Sham and Kidambi, Rahul and Netrapalli, Praneeth and Sidford, Aaron},
	journal={COLT},
	pages={545--604},
	year={2018}
}


@ARTICLE{Bach14,
  title =	{Adaptivity of averaged stochastic gradient descent to local strong convexity for logistic regression},
  author =	"Francis R. Bach",
  journal =	"Journal of Machine Learning Research (JMLR)",
  year = 	"2014",
  volume =	{volume 15},
}

@InProceedings{FrostigGKS15b,
  title =	{Un-regularizing: approximate proximal point and faster stochastic algorithms for empirical risk minimization},
  author =	"Roy Frostig and Rong Ge and Sham Kakade and Aaron Sidford",
  booktitle =	"ICML",
  year = 	"2015",
}

@InProceedings{pmlr-v75-jain18a,
title = {Accelerating Stochastic Gradient Descent for Least Squares Regression},
author = {Jain, Prateek and Kakade, Sham M. and Kidambi, Rahul and Netrapalli, Praneeth and Sidford, Aaron},
booktitle = {Proceedings of the 31st Conference On Learning Theory},
year = {2018},
volume = {75},
series = {Proceedings of Machine Learning Research},
publisher = {PMLR},
}
@article{cao2022benign,
  title={Benign Overfitting in Two-layer Convolutional Neural Networks},
  author={Cao, Yuan and Chen, Zixiang and Belkin, Mikhail and Gu, Quanquan},
  journal={arXiv preprint arXiv:2202.06526},
  year={2022}
}
@inproceedings{jelassi2022towards,
  title={Towards understanding how momentum improves generalization in deep learning},
  author={Jelassi, Samy and Li, Yuanzhi},
  booktitle={International Conference on Machine Learning},
  pages={9965--10040},
  year={2022},
  organization={PMLR}
}
@article{glasgow2022max,
  title={Max-Margin Works while Large Margin Fails: Generalization without Uniform Convergence},
  author={Glasgow, Margalit and Wei, Colin and Wootters, Mary and Ma, Tengyu},
  journal={arXiv preprint arXiv:2206.07892},
  year={2022}
}
@inproceedings{frei2022benign,
  title={Benign overfitting without linearity: Neural network classifiers trained by gradient descent for noisy linear data},
  author={Frei, Spencer and Chatterji, Niladri S and Bartlett, Peter},
  booktitle={Conference on Learning Theory},
  pages={2668--2703},
  year={2022},
  organization={PMLR}
}
@inproceedings{wangdoes,
  title={Does Momentum Change the Implicit Regularization on Separable Data?},
  author={Wang, Bohan and Meng, Qi and Zhang, Huishuai and Sun, Ruoyu and Chen, Wei and Ma, Zhi-Ming and Liu, Tie-Yan},
  booktitle={Advances in Neural Information Processing Systems}
}
@book{scholkopf2002learning,
  title={Learning with kernels: support vector machines, regularization, optimization, and beyond},
  author={Sch{\"o}lkopf, Bernhard and Smola, Alexander J and Bach, Francis and others},
  year={2002},
  publisher={MIT press}
}




@article{duchi2011adaptive,
  title={Adaptive subgradient methods for online learning and stochastic optimization},
  author={Duchi, John and Hazan, Elad and Singer, Yoram},
  journal={Journal of Machine Learning Research},
  volume={12},
  number={Jul},
  pages={2121--2159},
  year={2011}
}




@misc{hinton2012neural,
  title={Neural networks for machine learning lecture 6a overview of mini-batch gradient descent},
  author={Hinton, Geoffrey and Srivastava, Nitish and Swersky, Kevin},
  year={2012},
  publisher={Coursera Lecture slides}
}




@article{kingma2014adam,
  title={Adam: A method for stochastic optimization},
  author={Kingma, Diederik P and Ba, Jimmy},
  journal={International Conference on Learning Representations},
  year={2015}
}



@inproceedings{reddi2018convergence,
  title={On the convergence of adam and beyond},
  author={Reddi, Sashank J and Kale, Satyen and Kumar, Sanjiv},
  booktitle={International Conference on Learning Representations},
  year={2018}
}




@inproceedings{wilson2017marginal,
  title={The marginal value of adaptive gradient methods in machine learning},
  author={Wilson, Ashia C and Roelofs, Rebecca and Stern, Mitchell and Srebro, Nati and Recht, Benjamin},
  booktitle={Advances in Neural Information Processing Systems},
  pages={4151--4161},
  year={2017}
}



@inproceedings{chen2020closing,
  title={Closing the Generalization Gap of Adaptive Gradient Methods in Training Deep Neural Networks},
  author={Chen, Jinghui and Zhou, Dongruo and Tang, Yiqi and Yang, Ziyan and Cao, Yuan and Gu, Quanquan},
  booktitle={International Joint Conferences on Artificial Intelligence},
  year={2020}
}




@article{keskar2017improving,
  title={Improving generalization performance by switching from adam to sgd},
  author={Keskar, Nitish Shirish and Socher, Richard},
  journal={arXiv preprint arXiv:1712.07628},
  year={2017}
}



@article{luo2019adaptive,
  title={Adaptive gradient methods with dynamic bound of learning rate},
  author={Luo, Liangchen and Xiong, Yuanhao and Liu, Yan and Sun, Xu},
  journal={arXiv preprint arXiv:1902.09843},
  year={2019}
}


@article{zhou2020towards,
  title={Towards Theoretically Understanding Why Sgd Generalizes Better Than Adam in Deep Learning},
  author={Zhou, Pan and Feng, Jiashi and Ma, Chao and Xiong, Caiming and Hoi, Steven Chu Hong and others},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  year={2020}
}



@article{agarwal2019revisiting,
  title={Revisiting the Generalization of Adaptive Gradient Methods},
  author={Agarwal, Naman and Anil, Rohan and Hazan, Elad and Koren, Tomer and Zhang, Cyril},
  year={2019}
}




@inproceedings{merity2018regularizing,
  title={Regularizing and Optimizing LSTM Language Models},
  author={Merity, Stephen and Keskar, Nitish Shirish and Socher, Richard},
  booktitle={International Conference on Learning Representations},
  year={2018}
}















@article{lyu2019gradient,
  title={Gradient Descent Maximizes the Margin of Homogeneous Neural Networks},
  author={Lyu, Kaifeng and Li, Jian},
  journal={arXiv preprint arXiv:1906.05890},
  year={2019}
}
inproceedings{lee2019wide,
  title={Wide neural networks of any depth evolve as linear models under gradient descent},
  author={Lee, Jaehoon and Xiao, Lechao and Schoenholz, Samuel and Bahri, Yasaman and Novak, Roman and Sohl-Dickstein, Jascha and Pennington, Jeffrey},
  booktitle={Advances in neural information processing systems},
  pages={8572--8583},
  year={2019}
}
@inproceedings{kawaguchi2019gradient,
  title={Gradient descent finds global minima for generalizable deep neural networks of practical sizes},
  author={Kawaguchi, Kenji and Huang, Jiaoyang},
  booktitle={2019 57th Annual Allerton Conference on Communication, Control, and Computing (Allerton)},
  pages={92--99},
  year={2019},
  organization={IEEE}
}
@article{shamir2020gradient,
  title={Gradient Methods Never Overfit On Separable Data},
  author={Shamir, Ohad},
  journal={arXiv preprint arXiv:2007.00028},
  year={2020}
}
@article{ji2018risk,
  title={Risk and parameter convergence of logistic regression},
  author={Ji, Ziwei and Telgarsky, Matus},
  journal={arXiv preprint arXiv:1803.07300},
  year={2018}
}
@article{krizhevsky2009learning,
  title={Learning multiple layers of features from tiny images},
  author={Krizhevsky, Alex and others},
  year={2009},
  publisher={Citeseer}
}
@article{nacson2019lexicographic,
  title={Lexicographic and Depth-Sensitive Margins in Homogeneous and Non-Homogeneous Deep Models},
  author={Nacson, Mor Shpigel and Gunasekar, Suriya and Lee, Jason D and Srebro, Nathan and Soudry, Daniel},
  journal={arXiv preprint arXiv:1905.07325},
  year={2019}
}
@Article{zou2019gradient,
author="Zou, Difan
and Cao, Yuan
and Zhou, Dongruo
and Gu, Quanquan",
title="Gradient descent optimizes over-parameterized deep {ReLU} networks",
journal="Machine Learning",
year="2019",
month="Oct",
day="23"
}


@inproceedings{gao2019learning,
  title={Learning One-hidden-layer Neural Networks under General Input Distributions},
  author={Gao, Weihao and Makkuva, Ashok and Oh, Sewoong and Viswanath, Pramod},
  booktitle={The 22nd International Conference on Artificial Intelligence and Statistics},
  pages={1950--1959},
  year={2019}
}
@article{e2019comparative,
  title={A Comparative Analysis of the Optimization and Generalization Property of Two-layer Neural Network and Random Feature Models Under Gradient Descent Dynamics},
  author={E, Weinan and Ma, Chao and Wu, Lei and others},
  journal={arXiv preprint arXiv:1904.04326},
  year={2019}
}



@article{neyshabur2018towards,
  title={Towards understanding the role of over-parametrization in generalization of neural networks},
  author={Neyshabur, Behnam and Li, Zhiyuan and Bhojanapalli, Srinadh and LeCun, Yann and Srebro, Nathan},
  journal={arXiv preprint arXiv:1805.12076},
  year={2018}
}
@inproceedings{brutzkus2017globally,
  title={Globally optimal gradient descent for a convnet with gaussian inputs},
  author={Brutzkus, Alon and Globerson, Amir},
  booktitle={Proceedings of the 34th International Conference on Machine Learning-Volume 70},
  pages={605--614},
  year={2017},
  organization={JMLR. org}
}
@inproceedings{oymak2018overparameterized,
  title={Overparameterized Nonlinear Learning: Gradient Descent Takes the Shortest Path?},
  author={Oymak, Samet and Soltanolkotabi, Mahdi},
  booktitle={International Conference on Machine Learning},
  pages={4951--4960},
  year={2019}
}
@article{zhang2019training,
  title={Training Over-parameterized Deep {ResNet} Is almost as Easy as Training a Two-layer Network},
  author={Zhang, Huishuai and Yu, Da and Chen, Wei and Liu, Tie-Yan},
  journal={arXiv preprint arXiv:1903.07120},
  year={2019}
}
@article{wu2019global,
  title={Global Convergence of Adaptive Gradient Methods for An Over-parameterized Neural Network},
  author={Wu, Xiaoxia and Du, Simon S and Ward, Rachel},
  journal={arXiv preprint arXiv:1902.07111},
  year={2019}
}
@inproceedings{he2015delving,
  title={Delving deep into rectifiers: Surpassing human-level performance on imagenet classification},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={1026--1034},
  year={2015}
}

@inproceedings{cotter2011better,
  title={Better mini-batch algorithms via accelerated gradient methods},
  author={Cotter, Andrew and Shamir, Ohad and Srebro, Nati and Sridharan, Karthik},
  booktitle={Advances in neural information processing systems},
  pages={1647--1655},
  year={2011}
}
@article{polyak1963gradient,
  title={Gradient methods for minimizing functionals},
  author={Polyak, Boris Teodorovich},
  journal={Zhurnal Vychislitel'noi Matematiki i Matematicheskoi Fiziki},
  volume={3},
  number={4},
  pages={643--653},
  year={1963},
  publisher={Russian Academy of Sciences, Branch of Mathematical Sciences}
}
@article{shalev2013stochastic,
  title={Stochastic dual coordinate ascent methods for regularized loss minimization},
  author={Shalev-Shwartz, Shai and Zhang, Tong},
  journal={Journal of Machine Learning Research},
  volume={14},
  number={Feb},
  pages={567--599},
  year={2013}
}
@article{brutzkus2017sgd,
  title={{SGD} learns over-parameterized networks that provably generalize on linearly separable data},
  author={Brutzkus, Alon and Globerson, Amir and Malach, Eran and Shalev-Shwartz, Shai},
  journal={arXiv preprint arXiv:1710.10174},
  year={2017}
}


@article{ji2018gradient,
  title={Gradient descent aligns the layers of deep linear networks},
  author={Ji, Ziwei and Telgarsky, Matus},
  journal={arXiv preprint arXiv:1810.02032},
  year={2018}
}




@article{slepian1962one,
  title={The one-sided barrier problem for Gaussian noise},
  author={Slepian, David},
  journal={Bell Labs Technical Journal},
  volume={41},
  number={2},
  pages={463--501},
  year={1962},
  publisher={Wiley Online Library}
}




######################## Teacher Network



######################### Deep Linear Network
@article{saxe2013exact,
  title={Exact solutions to the nonlinear dynamics of learning in deep linear neural networks},
  author={Saxe, Andrew M and McClelland, James L and Ganguli, Surya},
  journal={arXiv preprint arXiv:1312.6120},
  year={2013}
}


@inproceedings{bartlett2018gradient,
  title={Gradient descent with identity initialization efficiently learns positive definite linear transformations},
  author={Bartlett, Peter and Helmbold, Dave and Long, Phil},
  booktitle={International Conference on Machine Learning},
  pages={520--529},
  year={2018}
}

@article{arora2018convergence,
  title={A Convergence Analysis of Gradient Descent for Deep Linear Neural Networks},
  author={Arora, Sanjeev and Cohen, Nadav and Golowich, Noah and Hu, Wei},
  journal={arXiv preprint arXiv:1810.02281},
  year={2018}
}




#####################Optimization Landscape


######################Generalization



@article{du2017convolutional,
	title={When is a Convolutional Filter Easy To Learn?},
	author={Du, Simon S and Lee, Jason D and Tian, Yuandong},
	journal={arXiv preprint arXiv:1709.06129},
	year={2017}
}

@article{li2017convergence,
  title={Convergence Analysis of Two-layer Neural Networks with {ReLU} Activation},
  author={Li, Yuanzhi and Yuan, Yang},
  journal={arXiv preprint arXiv:1705.09886},
  year={2017}
}

@article{soltanolkotabi2017learning,
  title={Learning ReLUs via Gradient Descent},
  author={Soltanolkotabi, Mahdi},
  journal={arXiv preprint arXiv:1705.04591},
  year={2017}
}

@article{zhong2017recovery,
  title={Recovery Guarantees for One-hidden-layer Neural Networks},
  author={Zhong, Kai and Song, Zhao and Jain, Prateek and Bartlett, Peter L and Dhillon, Inderjit S},
  journal={arXiv preprint arXiv:1706.03175},
  year={2017}
}

@article{tian2017analytical,
  title={An Analytical Formula of Population Gradient for two-layered {ReLU} network and its Applications in Convergence and Critical Point Analysis},
  author={Tian, Yuandong},
  journal={arXiv preprint arXiv:1703.00560},
  year={2017}
}

@article{brutzkus2017globally,
  title={Globally optimal gradient descent for a ConvNet with Gaussian inputs},
  author={Brutzkus, Alon and Globerson, Amir},
  journal={arXiv preprint arXiv:1702.07966},
  year={2017}
}
@article{hsu2012tail,
  title={A tail inequality for quadratic forms of subgaussian random vectors},
  author={Hsu, Daniel and Kakade, Sham and Zhang, Tong and others},
  journal={Electronic Communications in Probability},
  volume={17},
  year={2012},
  publisher={The Institute of Mathematical Statistics and the Bernoulli Society}
}


@article{dally2015high,
  title={High-performance hardware for machine learning},
  author={Dally, William},
  journal={NIPS Tutorial},
  year={2015}
}
@article{hinton2012deep,
  title={Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups},
  author={Hinton, Geoffrey and Deng, Li and Yu, Dong and Dahl, George E and Mohamed, Abdel-rahman and Jaitly, Navdeep and Senior, Andrew and Vanhoucke, Vincent and Nguyen, Patrick and Sainath, Tara N and others},
  journal={IEEE Signal Processing Magazine},
  volume={29},
  number={6},
  pages={82--97},
  year={2012},
  publisher={IEEE}
}
@inproceedings{sutskever2014sequence,
  title={Sequence to sequence learning with neural networks},
  author={Sutskever, Ilya and Vinyals, Oriol and Le, Quoc V},
  booktitle={Advances in neural information processing systems},
  pages={3104--3112},
  year={2014}
}

@inproceedings{krizhevsky2012imagenet,
  title={Imagenet classification with deep convolutional neural networks},
  author={Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
  booktitle={Advances in neural information processing systems},
  pages={1097--1105},
  year={2012}
}
@inproceedings{szegedy2015going,
  title={Going deeper with convolutions},
  author={Szegedy, Christian and Liu, Wei and Jia, Yangqing and Sermanet, Pierre and Reed, Scott and Anguelov, Dragomir and Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich, Andrew},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={1--9},
  year={2015}
}

@inproceedings{devlin2014fast,
  title={Fast and Robust Neural Network Joint Models for Statistical Machine Translation.},
  author={Devlin, Jacob and Zbib, Rabih and Huang, Zhongqiang and Lamar, Thomas and Schwartz, Richard M and Makhoul, John},
  booktitle={ACL (1)},
  pages={1370--1380},
  year={2014}
}
@article{bahdanau2014neural,
  title={Neural machine translation by jointly learning to align and translate},
  author={Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1409.0473},
  year={2014}
}
@article{silver2016mastering,
  title={Mastering the game of Go with deep neural networks and tree search},
  author={Silver, David and Huang, Aja and Maddison, Chris J and Guez, Arthur and Sifre, Laurent and Van Den Driessche, George and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc and others},
  journal={Nature},
  volume={529},
  number={7587},
  pages={484--489},
  year={2016},
  publisher={Nature Research}
}

@article{hornik1991approximation,
  title={Approximation capabilities of multilayer feedforward networks},
  author={Hornik, Kurt},
  journal={Neural networks},
  volume={4},
  number={2},
  pages={251--257},
  year={1991},
  publisher={Elsevier}
}

@inproceedings{daniely2016toward,
  title={Toward deeper understanding of neural networks: The power of initialization and a dual view on expressivity},
  author={Daniely, Amit and Frostig, Roy and Singer, Yoram},
  booktitle={Advances In Neural Information Processing Systems},
  pages={2253--2261},
  year={2016}
}
@article{lu2017expressive,
  title={The Expressive Power of Neural Networks: A View from the Width},
  author={Lu, Zhou and Pu, Hongming and Wang, Feicheng and Hu, Zhiqiang and Wang, Liwei},
  journal={arXiv preprint arXiv:1709.02540},
  year={2017}
}
@inproceedings{cohen2016expressive,
  title={On the expressive power of deep learning: A tensor analysis},
  author={Cohen, Nadav and Sharir, Or and Shashua, Amnon},
  booktitle={Conference on Learning Theory},
  pages={698--728},
  year={2016}
}
@inproceedings{cohen2016convolutional,
  title={Convolutional rectifier networks as generalized tensor decompositions},
  author={Cohen, Nadav and Shashua, Amnon},
  booktitle={International Conference on Machine Learning},
  pages={955--963},
  year={2016}
}
@article{telgarsky2016benefits,
  title={Benefits of depth in neural networks},
  author={Telgarsky, Matus},
  journal={arXiv preprint arXiv:1602.04485},
  year={2016}
}
@article{raghu2016expressive,
  title={On the expressive power of deep neural networks},
  author={Raghu, Maithra and Poole, Ben and Kleinberg, Jon and Ganguli, Surya and Sohl-Dickstein, Jascha},
  journal={arXiv preprint arXiv:1606.05336},
  year={2016}
}
@inproceedings{poole2016exponential,
  title={Exponential expressivity in deep neural networks through transient chaos},
  author={Poole, Ben and Lahiri, Subhaneil and Raghu, Maithreyi and Sohl-Dickstein, Jascha and Ganguli, Surya},
  booktitle={Advances In Neural Information Processing Systems},
  pages={3360--3368},
  year={2016}
}
@inproceedings{montufar2014number,
  title={On the number of linear regions of deep neural networks},
  author={Montufar, Guido F and Pascanu, Razvan and Cho, Kyunghyun and Bengio, Yoshua},
  booktitle={Advances in neural information processing systems},
  pages={2924--2932},
  year={2014}
}

@article{vsima2006training,
  title={Training a single sigmoidal neuron is hard},
  author={{\v{S}}{\'\i}ma, Ji{\v{r}}{\'\i}},
  journal={Training},
  volume={14},
  number={11},
  year={2006},
  publisher={MIT Press}
}
@inproceedings{livni2014computational,
  title={On the computational efficiency of training neural networks},
  author={Livni, Roi and Shalev-Shwartz, Shai and Shamir, Ohad},
  booktitle={Advances in Neural Information Processing Systems},
  pages={855--863},
  year={2014}
}
@article{shamir2016distribution,
  title={Distribution-specific hardness of learning neural networks},
  author={Shamir, Ohad},
  journal={arXiv preprint arXiv:1609.01037},
  year={2016}
}
@inproceedings{shalev2017failures,
  title={Failures of gradient-based deep learning},
  author={Shalev-Shwartz, Shai and Shamir, Ohad and Shammah, Shaked},
  booktitle={International Conference on Machine Learning},
  pages={3067--3075},
  year={2017}
}
@article{shalev2017weight,
  title={Weight Sharing is Crucial to Succesful Optimization},
  author={Shalev-Shwartz, Shai and Shamir, Ohad and Shammah, Shaked},
  journal={arXiv preprint arXiv:1706.00687},
  year={2017}
}
@inproceedings{blum1989training,
  title={Training a 3-node neural network is NP-complete},
  author={Blum, Avrim and Rivest, Ronald L},
  booktitle={Advances in neural information processing systems},
  pages={494--501},
  year={1989}
}
@article{goel2016reliably,
  title={Reliably learning the {ReLU} in polynomial time},
  author={Goel, Surbhi and Kanade, Varun and Klivans, Adam and Thaler, Justin},
  journal={arXiv preprint arXiv:1611.10258},
  year={2016}
}
@article{zhang2015learning,
  title={Learning halfspaces and neural networks with random initialization},
  author={Zhang, Yuchen and Lee, Jason D and Wainwright, Martin J and Jordan, Michael I},
  journal={arXiv preprint arXiv:1511.07948},
  year={2015}
}
@article{sedghi2014provable,
  title={Provable methods for training neural networks with sparse connectivity},
  author={Sedghi, Hanie and Anandkumar, Anima},
  journal={arXiv preprint arXiv:1412.2693},
  year={2014}
}
@article{janzamin2015beating,
  title={Beating the perils of non-convexity: Guaranteed training of neural networks using tensor methods},
  author={Janzamin, Majid and Sedghi, Hanie and Anandkumar, Anima},
  journal={arXiv preprint arXiv:1506.08473},
  year={2015}
}
@inproceedings{ge2015escaping,
  title={Escaping from saddle points—online stochastic gradient for tensor decomposition},
  author={Ge, Rong and Huang, Furong and Jin, Chi and Yuan, Yang},
  booktitle={Conference on Learning Theory},
  pages={797--842},
  year={2015}
}
@inproceedings{jin2016provable,
  title={Provable efficient online matrix completion via non-convex stochastic gradient descent},
  author={Jin, Chi and Kakade, Sham M and Netrapalli, Praneeth},
  booktitle={Advances in Neural Information Processing Systems},
  pages={4520--4528},
  year={2016}
}
@article{jin2017escape,
  title={How to Escape Saddle Points Efficiently},
  author={Jin, Chi and Ge, Rong and Netrapalli, Praneeth and Kakade, Sham M and Jordan, Michael I},
  journal={arXiv preprint arXiv:1703.00887},
  year={2017}
}

@article{lu2017depth,
  title={Depth Creates No Bad Local Minima},
  author={Lu, Haihao and Kawaguchi, Kenji},
  journal={arXiv preprint arXiv:1702.08580},
  year={2017}
}

@inproceedings{freeman2016topology,
  title={Topology and Geometry of Half-Rectified Network Optimization},
  author={Freeman, C Daniel and Bruna, Joan},
  booktitle={International Conference on Learning Representations},
  year={2017}
}
@article{taghvaei2017regularization,
  title={How regularization affects the critical points in linear networks},
  author={Taghvaei, Amirhossein and Kim, Jin W and Mehta, Prashant G},
  journal={arXiv preprint arXiv:1709.09625},
  year={2017}
}

@article{feizi2017porcupine,
  title={Porcupine Neural Networks:(Almost) All Local Optima are Global},
  author={Feizi, Soheil and Javadi, Hamid and Zhang, Jesse and Tse, David},
  journal={arXiv preprint arXiv:1710.02196},
  year={2017}
}



@article{carmon2017convex,
  title={" Convex Until Proven Guilty": Dimension-Free Acceleration of Gradient Descent on Non-Convex Functions},
  author={Carmon, Yair and Hinder, Oliver and Duchi, John C and Sidford, Aaron},
  journal={arXiv preprint arXiv:1705.02766},
  year={2017}
}


@inproceedings{choromanska2015loss,
  title={The loss surfaces of multilayer networks},
  author={Choromanska, Anna and Henaff, Mikael and Mathieu, Michael and Arous, G{\'e}rard Ben and LeCun, Yann},
  booktitle={Artificial Intelligence and Statistics},
  pages={192--204},
  year={2015}
}
@inproceedings{safran2016quality,
  title={On the quality of the initial basin in overspecified neural networks},
  author={Safran, Itay and Shamir, Ohad},
  booktitle={International Conference on Machine Learning},
  pages={774--782},
  year={2016}
}

@article{soudry2016no,
  title={No bad local minima: Data independent training error guarantees for multilayer neural networks},
  author={Soudry, Daniel and Carmon, Yair},
  journal={arXiv preprint arXiv:1605.08361},
  year={2016}
}
@article{arora2016provable,
  title={Provable learning of Noisy-or Networks},
  author={Arora, Sanjeev and Ge, Rong and Ma, Tengyu and Risteski, Andrej},
  journal={arXiv preprint arXiv:1612.08795},
  year={2016}
}

@inproceedings{zhang2017learnability,
  title={On the Learnability of Fully-Connected Neural Networks},
  author={Zhang, Yuchen and Lee, Jason and Wainwright, Martin and Jordan, Michael},
  booktitle={Artificial Intelligence and Statistics},
  pages={83--91},
  year={2017}
}
@inproceedings{jain2015fast,
  title={Fast exact matrix completion with finite samples},
  author={Jain, Prateek and Netrapalli, Praneeth},
  booktitle={Conference on Learning Theory},
  pages={1007--1034},
  year={2015}
}
@inproceedings{pan2016expressiveness,
  title={Expressiveness of rectifier networks},
  author={Pan, Xingyuan and Srikumar, Vivek},
  booktitle={International Conference on Machine Learning},
  pages={2427--2435},
  year={2016}
}

@article{barron1993universal,
  title={Universal approximation bounds for superpositions of a sigmoidal function},
  author={Barron, Andrew R},
  journal={IEEE Transactions on Information theory},
  volume={39},
  number={3},
  pages={930--945},
  year={1993},
  publisher={IEEE}
}


@article{mei2016landscape,
  title={The landscape of empirical risk for non-convex losses},
  author={Mei, Song and Bai, Yu and Montanari, Andrea},
  journal={arXiv preprint arXiv:1607.06534},
  year={2016}
}

@inproceedings{auer1996exponentially,
  title={Exponentially many local minima for single neurons},
  author={Auer, Peter and Herbster, Mark and Warmuth, Manfred K},
  booktitle={Advances in neural information processing systems},
  pages={316--322},
  year={1996}
}
@article{ge2017learning,
  title={Learning One-hidden-layer Neural Networks with Landscape Design},
  author={Ge, Rong and Lee, Jason D and Ma, Tengyu},
  journal={arXiv preprint arXiv:1711.00501},
  year={2017}
}
@inproceedings{kalai2009isotron,
  title={The Isotron Algorithm: High-Dimensional Isotonic Regression.},
  author={Kalai, Adam Tauman and Sastry, Ravi},
  booktitle={COLT},
  year={2009},
  organization={Citeseer}
}

@article{fu2018local,
  title={Local Geometry of One-Hidden-Layer Neural Networks for Logistic Regression},
  author={Fu, Haoyu and Chi, Yuejie and Liang, Yingbin},
  journal={arXiv preprint arXiv:1802.06463},
  year={2018}
}

@inproceedings{kakade2011efficient,
  title={Efficient learning of generalized linear and single index models with isotonic regression},
  author={Kakade, Sham M and Kanade, Varun and Shamir, Ohad and Kalai, Adam},
  booktitle={Advances in Neural Information Processing Systems},
  pages={927--935},
  year={2011}
}

@inproceedings{jain2013low,
  title={Low-rank matrix completion using alternating minimization},
  author={Jain, Prateek and Netrapalli, Praneeth and Sanghavi, Sujay},
  booktitle={Proceedings of the forty-fifth annual ACM symposium on Theory of computing},
  pages={665--674},
  year={2013},
  organization={ACM}
}

@inproceedings{sutskever2013importance,
  title={On the importance of initialization and momentum in deep learning},
  author={Sutskever, Ilya and Martens, James and Dahl, George and Hinton, Geoffrey},
  booktitle={International conference on machine learning},
  pages={1139--1147},
  year={2013}
}


@inproceedings{livni2014computational,
  title={On the computational efficiency of training neural networks},
  author={Livni, Roi and Shalev-Shwartz, Shai and Shamir, Ohad},
  booktitle={Advances in Neural Information Processing Systems},
  pages={855--863},
  year={2014}
}


@article{hornik1991approximation,
  title={Approximation capabilities of multilayer feedforward networks},
  author={Hornik, Kurt},
  journal={Neural networks},
  volume={4},
  number={2},
  pages={251--257},
  year={1991},
  publisher={Elsevier}
}


@article{barron1993universal,
  title={Universal approximation bounds for superpositions of a sigmoidal function},
  author={Barron, Andrew R},
  journal={IEEE Transactions on Information theory},
  volume={39},
  number={3},
  pages={930--945},
  year={1993},
  publisher={IEEE}
}



@article{liang2016deep,
  title={Why deep neural networks for function approximation?},
  author={Liang, Shiyu and Srikant, R},
  journal={arXiv preprint arXiv:1610.04161},
  year={2016}
}

@article{yarotsky2017error,
  title={Error bounds for approximations with deep {ReLU} networks},
  author={Yarotsky, Dmitry},
  journal={Neural Networks},
  volume={94},
  pages={103--114},
  year={2017},
  publisher={Elsevier}
}

@article{neyshabur2014search,
  title={In search of the real inductive bias: On the role of implicit regularization in deep learning},
  author={Neyshabur, Behnam and Tomioka, Ryota and Srebro, Nathan},
  journal={arXiv preprint arXiv:1412.6614},
  year={2014}
}



@inproceedings{sutskever2013importance,
  title={On the importance of initialization and momentum in deep learning},
  author={Sutskever, Ilya and Martens, James and Dahl, George and Hinton, Geoffrey},
  booktitle={International conference on machine learning},
  pages={1139--1147},
  year={2013}
}

@inproceedings{blum1989training,
  title={Training a 3-node neural network is NP-complete},
  author={Blum, Avrim and Rivest, Ronald L},
  booktitle={Advances in neural information processing systems},
  pages={494--501},
  year={1989}
}

#########gaussian##############
@article{brutzkus2017globally,
  title={Globally optimal gradient descent for a convnet with {Gaussian} inputs},
  author={Brutzkus, Alon and Globerson, Amir},
  journal={arXiv preprint arXiv:1702.07966},
  year={2017}
}




@article{zhong2017learning,
  title={Learning Non-overlapping Convolutional Neural Networks with Multiple Kernels},
  author={Zhong, Kai and Song, Zhao and Dhillon, Inderjit S},
  journal={arXiv preprint arXiv:1711.03440},
  year={2017}
}


#############independent assumption 




@article{arora2018optimization,
  title={On the optimization of deep networks: Implicit acceleration by overparameterization},
  author={Arora, Sanjeev and Cohen, Nadav and Hazan, Elad},
  journal={arXiv preprint arXiv:1802.06509},
  year={2018}
}





@inproceedings{bartlett2018gradient,
  title={Gradient descent with identity initialization efficiently learns positive definite linear transformations},
  author={Bartlett, Peter and Helmbold, Dave and Long, Phil},
  booktitle={International Conference on Machine Learning},
  pages={520--529},
  year={2018}
}

@inproceedings{langford2002not,
  title={(Not) bounding the true error},
  author={Langford, John and Caruana, Rich},
  booktitle={Advances in Neural Information Processing Systems},
  pages={809--816},
  year={2002}
}


###############generalization#################


@inproceedings{neyshabur2017exploring,
  title={Exploring generalization in deep learning},
  author={Neyshabur, Behnam and Bhojanapalli, Srinadh and McAllester, David and Srebro, Nati},
  booktitle={Advances in Neural Information Processing Systems},
  pages={5947--5956},
  year={2017}
}

@inproceedings{bartlett2017spectrally,
  title={Spectrally-normalized margin bounds for neural networks},
  author={Bartlett, Peter L and Foster, Dylan J and Telgarsky, Matus J},
  booktitle={Advances in Neural Information Processing Systems},
  pages={6240--6249},
  year={2017}
}

@book{anthony2009neural,
  title={Neural network learning: Theoretical foundations},
  author={Anthony, Martin and Bartlett, Peter L},
  year={2009},
  publisher={cambridge university press}
}

@book{vapnik2013nature,
  title={The nature of statistical learning theory},
  author={Vapnik, Vladimir},
  year={2013},
  publisher={Springer science \& business media}
}


@article{arora2016understanding,
  title={Understanding deep neural networks with rectified linear units},
  author={Arora, Raman and Basu, Amitabh and Mianjy, Poorya and Mukherjee, Anirbit},
  journal={arXiv preprint arXiv:1611.01491},
  year={2016}
}


###########more citation#################
@article{yarotsky2018optimal,
  title={Optimal approximation of continuous functions by very deep {ReLU} networks},
  author={Yarotsky, Dmitry},
  journal={arXiv preprint arXiv:1802.03620},
  year={2018}
}

@article{hanin2017universal,
  title={Universal function approximation by deep neural nets with bounded width and {ReLU} activations},
  author={Hanin, Boris},
  journal={arXiv preprint arXiv:1708.02691},
  year={2017}
}

@article{hanin2017approximating,
  title={Approximating Continuous Functions by {ReLU} Nets of Minimal Width},
  author={Hanin, Boris and Sellke, Mark},
  journal={arXiv preprint arXiv:1710.11278},
  year={2017}
}


@inproceedings{safran2017spurious,
  title={Spurious Local Minima are Common in Two-Layer ReLU Neural Networks},
  author={Safran, Itay and Shamir, Ohad},
  booktitle={International Conference on Machine Learning},
  pages={4430--4438},
  year={2018}
}



@article{zhang2018learning,
  title={Learning One-hidden-layer {ReLU} Networks via Gradient Descent},
  author={Zhang, Xiao and Yu, Yaodong and Wang, Lingxiao and Gu, Quanquan},
  journal={arXiv preprint arXiv:1806.07808},
  year={2018}
}

@article{telgarsky2015representation,
  title={Representation benefits of deep feedforward networks},
  author={Telgarsky, Matus},
  journal={arXiv preprint arXiv:1509.08101},
  year={2015}
}

@article{telgarsky2016benefits,
  title={Benefits of depth in neural networks},
  author={Telgarsky, Matus},
  journal={arXiv preprint arXiv:1602.04485},
  year={2016}
}



@article{vaswani2018fast,
  title={Fast and faster convergence of sgd for over-parameterized models and an accelerated perceptron},
  author={Vaswani, Sharan and Bach, Francis and Schmidt, Mark},
  journal={arXiv preprint arXiv:1810.07288},
  year={2018}
}

@inproceedings{cotter2011better,
  title={Better mini-batch algorithms via accelerated gradient methods},
  author={Cotter, Andrew and Shamir, Ohad and Srebro, Nati and Sridharan, Karthik},
  booktitle={Advances in neural information processing systems},
  pages={1647--1655},
  year={2011}
}
@article{shalev2013stochastic,
  title={Stochastic dual coordinate ascent methods for regularized loss minimization},
  author={Shalev-Shwartz, Shai and Zhang, Tong},
  journal={Journal of Machine Learning Research},
  volume={14},
  number={Feb},
  pages={567--599},
  year={2013}
}
@article{brutzkus2017sgd,
  title={SGD learns over-parameterized networks that provably generalize on linearly separable data},
  author={Brutzkus, Alon and Globerson, Amir and Malach, Eran and Shalev-Shwartz, Shai},
  journal={arXiv preprint arXiv:1710.10174},
  year={2017}
}


@article{ji2018gradient,
  title={Gradient descent aligns the layers of deep linear networks},
  author={Ji, Ziwei and Telgarsky, Matus},
  journal={arXiv preprint arXiv:1810.02032},
  year={2018}
}
@article{tian2017analytical,
  title={An analytical formula of population gradient for two-layered relu network and its applications in convergence and critical point analysis},
  author={Tian, Yuandong},
  journal={arXiv preprint arXiv:1703.00560},
  year={2017}
}
@article{du2017convolutional,
  title={When is a Convolutional Filter Easy to Learn?},
  author={Du, Simon S and Lee, Jason D and Tian, Yuandong},
  journal={arXiv preprint arXiv:1709.06129},
  year={2017}
}
@article{zhong2017recovery,
  title={Recovery guarantees for one-hidden-layer neural networks},
  author={Zhong, Kai and Song, Zhao and Jain, Prateek and Bartlett, Peter L and Dhillon, Inderjit S},
  journal={arXiv preprint arXiv:1706.03175},
  year={2017}
}
@inproceedings{li2017convergence,
  title={Convergence analysis of two-layer neural networks with relu activation},
  author={Li, Yuanzhi and Yuan, Yang},
  booktitle={Advances in Neural Information Processing Systems},
  pages={597--607},
  year={2017}
}


@inproceedings{neyshabur2017exploring,
  title={Exploring generalization in deep learning},
  author={Neyshabur, Behnam and Bhojanapalli, Srinadh and McAllester, David and Srebro, Nati},
  booktitle={Advances in Neural Information Processing Systems},
  pages={5947--5956},
  year={2017}
}





@article{bartlett2002rademacher,
  title={Rademacher and {Gaussian} complexities: Risk bounds and structural results},
  author={Bartlett, Peter L and Mendelson, Shahar},
  journal={Journal of Machine Learning Research},
  volume={3},
  number={Nov},
  pages={463--482},
  year={2002}
}



@book{shalev2014understanding,
  title={Understanding machine learning: From theory to algorithms},
  author={Shalev-Shwartz, Shai and Ben-David, Shai},
  year={2014},
  publisher={Cambridge university press}
}




@inproceedings{ji2019polylogarithmic,
  title={Polylogarithmic width suffices for gradient descent to achieve arbitrarily small test error with shallow ReLU networks},
  author={Ji, Ziwei and Telgarsky, Matus},
  booktitle={International Conference on Learning Representations},
  year={2020}
}




@article{fang2019over,
  title={Over Parameterized Two-level Neural Networks Can Learn Near Optimal Feature Representations},
  author={Fang, Cong and Dong, Hanze and Zhang, Tong},
  journal={arXiv preprint arXiv:1910.11508},
  year={2019}
}



@inproceedings{mei2019mean,
  title={Mean-field theory of two-layers neural networks: dimension-free bounds and kernel limit},
  author={Mei, Song and Misiakiewicz, Theodor and Montanari, Andrea},
  booktitle={Conference on Learning Theory},
  year={2019}
}




@inproceedings{allen2019can,
  title={What Can {ResNet} Learn Efficiently, Going Beyond Kernels?},
  author={Allen-Zhu, Zeyuan and Li, Yuanzhi},
  booktitle={Advances in Neural Information Processing Systems},
  year={2019}
}



@article{nitanda2019refined,
  title={Refined Generalization Analysis of Gradient Descent for Over-parameterized Two-layer Neural Networks with Smooth Activations on Classification Problems},
  author={Nitanda, Atsushi and Suzuki, Taiji},
  journal={arXiv preprint arXiv:1905.09870},
  year={2019}
}


@article{woodworth2019kernel,
  title={Kernel and Deep Regimes in Overparametrized Models},
  author={Woodworth, Blake and Gunasekar, Suriya and Lee, Jason and Soudry, Daniel and Srebro, Nathan},
  journal={arXiv preprint arXiv:1906.05827},
  year={2019}
}






@article{venturi2018neural,
  title={Neural networks with finite intrinsic dimension have no spurious valleys},
  author={Venturi, Luca and Bandeira, Afonso and Bruna, Joan},
  journal={arXiv preprint arXiv:1802.06384},
  volume={15},
  year={2018}
}






@inproceedings{ji2019implicit,
  title={The implicit bias of gradient descent on nonseparable data},
  author={Ji, Ziwei and Telgarsky, Matus},
  booktitle={Conference on Learning Theory},
  pages={1772--1798},
  year={2019}
}







@inproceedings{bai2019beyond,
  title={Beyond Linearization: On Quadratic and Higher-Order Approximation of Wide Neural Networks},
  author={Bai, Yu and Lee, Jason D},
  booktitle={International Conference on Learning Representations},
  year={2019}
}


@article{belkin2019reconciling,
  title={Reconciling modern machine-learning practice and the classical bias--variance trade-off},
  author={Belkin, Mikhail and Hsu, Daniel and Ma, Siyuan and Mandal, Soumik},
  journal={Proceedings of the National Academy of Sciences},
  volume={116},
  number={32},
  pages={15849--15854},
  year={2019},
  publisher={National Acad Sciences}
}


@article{belkin2019two,
  title={Two models of double descent for weak features},
  author={Belkin, Mikhail and Hsu, Daniel and Xu, Ji},
  journal={arXiv preprint arXiv:1903.07571},
  year={2019}
}



@article{hastie2019surprises,
  title={Surprises in high-dimensional ridgeless least squares interpolation},
  author={Hastie, Trevor and Montanari, Andrea and Rosset, Saharon and Tibshirani, Ryan J},
  journal={arXiv preprint arXiv:1903.08560},
  year={2019}
}


@article{mei2019generalization,
  title={The generalization error of random features regression: Precise asymptotics and double descent curve},
  author={Mei, Song and Montanari, Andrea},
  journal={arXiv preprint arXiv:1908.05355},
  year={2019}
}


















@inproceedings{cotter2011better,
  title={Better mini-batch algorithms via accelerated gradient methods},
  author={Cotter, Andrew and Shamir, Ohad and Srebro, Nati and Sridharan, Karthik},
  booktitle={Advances in neural information processing systems},
  pages={1647--1655},
  year={2011}
}
@article{shalev2013stochastic,
  title={Stochastic dual coordinate ascent methods for regularized loss minimization},
  author={Shalev-Shwartz, Shai and Zhang, Tong},
  journal={Journal of Machine Learning Research},
  volume={14},
  number={Feb},
  pages={567--599},
  year={2013}
}


@article{soudry2017implicit,
  title={The implicit bias of gradient descent on separable data},
  author={Soudry, Daniel and Hoffer, Elad and Nacson, Mor Shpigel and Gunasekar, Suriya and Srebro, Nathan},
  journal={The Journal of Machine Learning Research},
  volume={19},
  number={1},
  pages={2822--2878},
  year={2018},
  publisher={JMLR. org}
}


@article{ji2018gradient,
  title={Gradient descent aligns the layers of deep linear networks},
  author={Ji, Ziwei and Telgarsky, Matus},
  journal={arXiv preprint arXiv:1810.02032},
  year={2018}
}



@article{slepian1962one,
  title={The one-sided barrier problem for {Gaussian} noise},
  author={Slepian, David},
  journal={Bell Labs Technical Journal},
  volume={41},
  number={2},
  pages={463--501},
  year={1962},
  publisher={Wiley Online Library}
}




######################## Teacher Network



######################### Deep Linear Network
@article{saxe2013exact,
  title={Exact solutions to the nonlinear dynamics of learning in deep linear neural networks},
  author={Saxe, Andrew M and McClelland, James L and Ganguli, Surya},
  journal={arXiv preprint arXiv:1312.6120},
  year={2013}
}


@inproceedings{bartlett2018gradient,
  title={Gradient descent with identity initialization efficiently learns positive definite linear transformations},
  author={Bartlett, Peter and Helmbold, Dave and Long, Phil},
  booktitle={International Conference on Machine Learning},
  pages={520--529},
  year={2018}
}




#####################Optimization Landscape


######################Generalization



@article{du2017convolutional,
	title={When is a Convolutional Filter Easy To Learn?},
	author={Du, Simon S and Lee, Jason D and Tian, Yuandong},
	journal={arXiv preprint arXiv:1709.06129},
	year={2017}
}

@article{li2017convergence,
  title={Convergence Analysis of Two-layer Neural Networks with ReLU Activation},
  author={Li, Yuanzhi and Yuan, Yang},
  journal={arXiv preprint arXiv:1705.09886},
  year={2017}
}

@article{soltanolkotabi2017learning,
  title={Learning ReLUs via Gradient Descent},
  author={Soltanolkotabi, Mahdi},
  journal={arXiv preprint arXiv:1705.04591},
  year={2017}
}

@article{zhong2017recovery,
  title={Recovery Guarantees for One-hidden-layer Neural Networks},
  author={Zhong, Kai and Song, Zhao and Jain, Prateek and Bartlett, Peter L and Dhillon, Inderjit S},
  journal={arXiv preprint arXiv:1706.03175},
  year={2017}
}

@article{tian2017analytical,
  title={An Analytical Formula of Population Gradient for two-layered ReLU network and its Applications in Convergence and Critical Point Analysis},
  author={Tian, Yuandong},
  journal={arXiv preprint arXiv:1703.00560},
  year={2017}
}

@article{brutzkus2017globally,
  title={Globally optimal gradient descent for a {ConvNet} with {Gaussian} inputs},
  author={Brutzkus, Alon and Globerson, Amir},
  journal={arXiv preprint arXiv:1702.07966},
  year={2017}
}
@article{hsu2012tail,
  title={A tail inequality for quadratic forms of subgaussian random vectors},
  author={Hsu, Daniel and Kakade, Sham and Zhang, Tong and others},
  journal={Electronic Communications in Probability},
  volume={17},
  year={2012},
  publisher={The Institute of Mathematical Statistics and the Bernoulli Society}
}


@article{dally2015high,
  title={High-performance hardware for machine learning},
  author={Dally, William},
  journal={NIPS Tutorial},
  year={2015}
}
@article{hinton2012deep,
  title={Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups},
  author={Hinton, Geoffrey and Deng, Li and Yu, Dong and Dahl, George E and Mohamed, Abdel-rahman and Jaitly, Navdeep and Senior, Andrew and Vanhoucke, Vincent and Nguyen, Patrick and Sainath, Tara N and others},
  journal={IEEE Signal Processing Magazine},
  volume={29},
  number={6},
  pages={82--97},
  year={2012},
  publisher={IEEE}
}
@inproceedings{sutskever2014sequence,
  title={Sequence to sequence learning with neural networks},
  author={Sutskever, Ilya and Vinyals, Oriol and Le, Quoc V},
  booktitle={Advances in neural information processing systems},
  pages={3104--3112},
  year={2014}
}

@inproceedings{szegedy2015going,
  title={Going deeper with convolutions},
  author={Szegedy, Christian and Liu, Wei and Jia, Yangqing and Sermanet, Pierre and Reed, Scott and Anguelov, Dragomir and Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich, Andrew},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={1--9},
  year={2015}
}

@inproceedings{devlin2014fast,
  title={Fast and Robust Neural Network Joint Models for Statistical Machine Translation.},
  author={Devlin, Jacob and Zbib, Rabih and Huang, Zhongqiang and Lamar, Thomas and Schwartz, Richard M and Makhoul, John},
  booktitle={ACL (1)},
  pages={1370--1380},
  year={2014}
}
@article{bahdanau2014neural,
  title={Neural machine translation by jointly learning to align and translate},
  author={Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1409.0473},
  year={2014}
}


@article{hornik1991approximation,
  title={Approximation capabilities of multilayer feedforward networks},
  author={Hornik, Kurt},
  journal={Neural networks},
  volume={4},
  number={2},
  pages={251--257},
  year={1991},
  publisher={Elsevier}
}

@inproceedings{daniely2016toward,
  title={Toward deeper understanding of neural networks: The power of initialization and a dual view on expressivity},
  author={Daniely, Amit and Frostig, Roy and Singer, Yoram},
  booktitle={Advances In Neural Information Processing Systems},
  pages={2253--2261},
  year={2016}
}
@article{lu2017expressive,
  title={The Expressive Power of Neural Networks: A View from the Width},
  author={Lu, Zhou and Pu, Hongming and Wang, Feicheng and Hu, Zhiqiang and Wang, Liwei},
  journal={arXiv preprint arXiv:1709.02540},
  year={2017}
}
@inproceedings{cohen2016expressive,
  title={On the expressive power of deep learning: A tensor analysis},
  author={Cohen, Nadav and Sharir, Or and Shashua, Amnon},
  booktitle={Conference on Learning Theory},
  pages={698--728},
  year={2016}
}
@inproceedings{cohen2016convolutional,
  title={Convolutional rectifier networks as generalized tensor decompositions},
  author={Cohen, Nadav and Shashua, Amnon},
  booktitle={International Conference on Machine Learning},
  pages={955--963},
  year={2016}
}
@article{telgarsky2016benefits,
  title={Benefits of depth in neural networks},
  author={Telgarsky, Matus},
  journal={arXiv preprint arXiv:1602.04485},
  year={2016}
}
@article{raghu2016expressive,
  title={On the expressive power of deep neural networks},
  author={Raghu, Maithra and Poole, Ben and Kleinberg, Jon and Ganguli, Surya and Sohl-Dickstein, Jascha},
  journal={arXiv preprint arXiv:1606.05336},
  year={2016}
}
@inproceedings{poole2016exponential,
  title={Exponential expressivity in deep neural networks through transient chaos},
  author={Poole, Ben and Lahiri, Subhaneil and Raghu, Maithreyi and Sohl-Dickstein, Jascha and Ganguli, Surya},
  booktitle={Advances In Neural Information Processing Systems},
  pages={3360--3368},
  year={2016}
}
@inproceedings{montufar2014number,
  title={On the number of linear regions of deep neural networks},
  author={Montufar, Guido F and Pascanu, Razvan and Cho, Kyunghyun and Bengio, Yoshua},
  booktitle={Advances in neural information processing systems},
  pages={2924--2932},
  year={2014}
}

@article{vsima2006training,
  title={Training a single sigmoidal neuron is hard},
  author={{\v{S}}{\'\i}ma, Ji{\v{r}}{\'\i}},
  journal={Training},
  volume={14},
  number={11},
  year={2006},
  publisher={MIT Press}
}
@inproceedings{livni2014computational,
  title={On the computational efficiency of training neural networks},
  author={Livni, Roi and Shalev-Shwartz, Shai and Shamir, Ohad},
  booktitle={Advances in Neural Information Processing Systems},
  pages={855--863},
  year={2014}
}
@article{shamir2016distribution,
  title={Distribution-specific hardness of learning neural networks},
  author={Shamir, Ohad},
  journal={arXiv preprint arXiv:1609.01037},
  year={2016}
}
@inproceedings{shalev2017failures,
  title={Failures of gradient-based deep learning},
  author={Shalev-Shwartz, Shai and Shamir, Ohad and Shammah, Shaked},
  booktitle={International Conference on Machine Learning},
  pages={3067--3075},
  year={2017}
}
@article{shalev2017weight,
  title={Weight Sharing is Crucial to Succesful Optimization},
  author={Shalev-Shwartz, Shai and Shamir, Ohad and Shammah, Shaked},
  journal={arXiv preprint arXiv:1706.00687},
  year={2017}
}
@inproceedings{blum1989training,
  title={Training a 3-node neural network is NP-complete},
  author={Blum, Avrim and Rivest, Ronald L},
  booktitle={Advances in neural information processing systems},
  pages={494--501},
  year={1989}
}
@article{goel2016reliably,
  title={Reliably learning the relu in polynomial time},
  author={Goel, Surbhi and Kanade, Varun and Klivans, Adam and Thaler, Justin},
  journal={arXiv preprint arXiv:1611.10258},
  year={2016}
}
@article{zhang2015learning,
  title={Learning halfspaces and neural networks with random initialization},
  author={Zhang, Yuchen and Lee, Jason D and Wainwright, Martin J and Jordan, Michael I},
  journal={arXiv preprint arXiv:1511.07948},
  year={2015}
}
@article{sedghi2014provable,
  title={Provable methods for training neural networks with sparse connectivity},
  author={Sedghi, Hanie and Anandkumar, Anima},
  journal={arXiv preprint arXiv:1412.2693},
  year={2014}
}
@article{janzamin2015beating,
  title={Beating the perils of non-convexity: Guaranteed training of neural networks using tensor methods},
  author={Janzamin, Majid and Sedghi, Hanie and Anandkumar, Anima},
  journal={arXiv preprint arXiv:1506.08473},
  year={2015}
}
@inproceedings{ge2015escaping,
  title={Escaping from saddle points—online stochastic gradient for tensor decomposition},
  author={Ge, Rong and Huang, Furong and Jin, Chi and Yuan, Yang},
  booktitle={Conference on Learning Theory},
  pages={797--842},
  year={2015}
}
@inproceedings{jin2016provable,
  title={Provable efficient online matrix completion via non-convex stochastic gradient descent},
  author={Jin, Chi and Kakade, Sham M and Netrapalli, Praneeth},
  booktitle={Advances in Neural Information Processing Systems},
  pages={4520--4528},
  year={2016}
}
@article{jin2017escape,
  title={How to Escape Saddle Points Efficiently},
  author={Jin, Chi and Ge, Rong and Netrapalli, Praneeth and Kakade, Sham M and Jordan, Michael I},
  journal={arXiv preprint arXiv:1703.00887},
  year={2017}
}
@inproceedings{kawaguchi2016deep,
  title={Deep learning without poor local minima},
  author={Kawaguchi, Kenji},
  booktitle={Advances in Neural Information Processing Systems},
  pages={586--594},
  year={2016}
}
@article{lu2017depth,
  title={Depth Creates No Bad Local Minima},
  author={Lu, Haihao and Kawaguchi, Kenji},
  journal={arXiv preprint arXiv:1702.08580},
  year={2017}
}


@article{taghvaei2017regularization,
  title={How regularization affects the critical points in linear networks},
  author={Taghvaei, Amirhossein and Kim, Jin W and Mehta, Prashant G},
  journal={arXiv preprint arXiv:1709.09625},
  year={2017}
}

@inproceedings{nguyen2017loss,
  title={The Loss Surface of Deep and Wide Neural Networks},
  author={Nguyen, Quynh and Hein, Matthias},
  booktitle={International Conference on Machine Learning},
  pages={2603--2612},
  year={2017}
}
@article{feizi2017porcupine,
  title={Porcupine Neural Networks:(Almost) All Local Optima are Global},
  author={Feizi, Soheil and Javadi, Hamid and Zhang, Jesse and Tse, David},
  journal={arXiv preprint arXiv:1710.02196},
  year={2017}
}



@article{carmon2017convex,
  title={" Convex Until Proven Guilty": Dimension-Free Acceleration of Gradient Descent on Non-Convex Functions},
  author={Carmon, Yair and Hinder, Oliver and Duchi, John C and Sidford, Aaron},
  journal={arXiv preprint arXiv:1705.02766},
  year={2017}
}




@article{soudry2016no,
  title={No bad local minima: Data independent training error guarantees for multilayer neural networks},
  author={Soudry, Daniel and Carmon, Yair},
  journal={arXiv preprint arXiv:1605.08361},
  year={2016}
}
@article{arora2016provable,
  title={Provable learning of Noisy-or Networks},
  author={Arora, Sanjeev and Ge, Rong and Ma, Tengyu and Risteski, Andrej},
  journal={arXiv preprint arXiv:1612.08795},
  year={2016}
}

@inproceedings{zhang2017learnability,
  title={On the Learnability of Fully-Connected Neural Networks},
  author={Zhang, Yuchen and Lee, Jason and Wainwright, Martin and Jordan, Michael},
  booktitle={Artificial Intelligence and Statistics},
  pages={83--91},
  year={2017}
}
@inproceedings{jain2015fast,
  title={Fast exact matrix completion with finite samples},
  author={Jain, Prateek and Netrapalli, Praneeth},
  booktitle={Conference on Learning Theory},
  pages={1007--1034},
  year={2015}
}
@inproceedings{pan2016expressiveness,
  title={Expressiveness of rectifier networks},
  author={Pan, Xingyuan and Srikumar, Vivek},
  booktitle={International Conference on Machine Learning},
  pages={2427--2435},
  year={2016}
}

@article{barron1993universal,
  title={Universal approximation bounds for superpositions of a sigmoidal function},
  author={Barron, Andrew R},
  journal={IEEE Transactions on Information theory},
  volume={39},
  number={3},
  pages={930--945},
  year={1993},
  publisher={IEEE}
}

@article{mei2016landscape,
  title={The landscape of empirical risk for non-convex losses},
  author={Mei, Song and Bai, Yu and Montanari, Andrea},
  journal={arXiv preprint arXiv:1607.06534},
  year={2016}
}

@inproceedings{auer1996exponentially,
  title={Exponentially many local minima for single neurons},
  author={Auer, Peter and Herbster, Mark and Warmuth, Manfred K},
  booktitle={Advances in neural information processing systems},
  pages={316--322},
  year={1996}
}
@article{ge2017learning,
  title={Learning One-hidden-layer Neural Networks with Landscape Design},
  author={Ge, Rong and Lee, Jason D and Ma, Tengyu},
  journal={arXiv preprint arXiv:1711.00501},
  year={2017}
}
@inproceedings{kalai2009isotron,
  title={The Isotron Algorithm: High-Dimensional Isotonic Regression.},
  author={Kalai, Adam Tauman and Sastry, Ravi},
  booktitle={COLT},
  year={2009},
  organization={Citeseer}
}

@article{fu2018local,
  title={Local Geometry of One-Hidden-Layer Neural Networks for Logistic Regression},
  author={Fu, Haoyu and Chi, Yuejie and Liang, Yingbin},
  journal={arXiv preprint arXiv:1802.06463},
  year={2018}
}

@inproceedings{kakade2011efficient,
  title={Efficient learning of generalized linear and single index models with isotonic regression},
  author={Kakade, Sham M and Kanade, Varun and Shamir, Ohad and Kalai, Adam},
  booktitle={Advances in Neural Information Processing Systems},
  pages={927--935},
  year={2011}
}

@inproceedings{jain2013low,
  title={Low-rank matrix completion using alternating minimization},
  author={Jain, Prateek and Netrapalli, Praneeth and Sanghavi, Sujay},
  booktitle={Proceedings of the forty-fifth annual ACM symposium on Theory of computing},
  pages={665--674},
  year={2013},
  organization={ACM}
}

@inproceedings{sutskever2013importance,
  title={On the importance of initialization and momentum in deep learning},
  author={Sutskever, Ilya and Martens, James and Dahl, George and Hinton, Geoffrey},
  booktitle={International conference on machine learning},
  pages={1139--1147},
  year={2013}
}



@inproceedings{livni2014computational,
  title={On the computational efficiency of training neural networks},
  author={Livni, Roi and Shalev-Shwartz, Shai and Shamir, Ohad},
  booktitle={Advances in Neural Information Processing Systems},
  pages={855--863},
  year={2014}
}


@article{hornik1991approximation,
  title={Approximation capabilities of multilayer feedforward networks},
  author={Hornik, Kurt},
  journal={Neural networks},
  volume={4},
  number={2},
  pages={251--257},
  year={1991},
  publisher={Elsevier}
}


@article{barron1993universal,
  title={Universal approximation bounds for superpositions of a sigmoidal function},
  author={Barron, Andrew R},
  journal={IEEE Transactions on Information theory},
  volume={39},
  number={3},
  pages={930--945},
  year={1993},
  publisher={IEEE}
}



@article{liang2016deep,
  title={Why deep neural networks for function approximation?},
  author={Liang, Shiyu and Srikant, R},
  journal={arXiv preprint arXiv:1610.04161},
  year={2016}
}

@article{yarotsky2017error,
  title={Error bounds for approximations with deep ReLU networks},
  author={Yarotsky, Dmitry},
  journal={Neural Networks},
  volume={94},
  pages={103--114},
  year={2017},
  publisher={Elsevier}
}

@article{neyshabur2014search,
  title={In search of the real inductive bias: On the role of implicit regularization in deep learning},
  author={Neyshabur, Behnam and Tomioka, Ryota and Srebro, Nathan},
  journal={arXiv preprint arXiv:1412.6614},
  year={2014}
}



@inproceedings{sutskever2013importance,
  title={On the importance of initialization and momentum in deep learning},
  author={Sutskever, Ilya and Martens, James and Dahl, George and Hinton, Geoffrey},
  booktitle={International conference on machine learning},
  pages={1139--1147},
  year={2013}
}

@inproceedings{blum1989training,
  title={Training a 3-node neural network is NP-complete},
  author={Blum, Avrim and Rivest, Ronald L},
  booktitle={Advances in neural information processing systems},
  pages={494--501},
  year={1989}
}

#########gaussian##############





@article{zhong2017learning,
  title={Learning Non-overlapping Convolutional Neural Networks with Multiple Kernels},
  author={Zhong, Kai and Song, Zhao and Dhillon, Inderjit S},
  journal={arXiv preprint arXiv:1711.03440},
  year={2017}
}


#############independent assumption 



@inproceedings{bartlett2018gradient,
  title={Gradient descent with identity initialization efficiently learns positive definite linear transformations},
  author={Bartlett, Peter and Helmbold, Dave and Long, Phil},
  booktitle={International Conference on Machine Learning},
  pages={520--529},
  year={2018}
}

@inproceedings{langford2002not,
  title={(Not) bounding the true error},
  author={Langford, John and Caruana, Rich},
  booktitle={Advances in Neural Information Processing Systems},
  pages={809--816},
  year={2002}
}


###############generalization#################


@inproceedings{neyshabur2017exploring,
  title={Exploring generalization in deep learning},
  author={Neyshabur, Behnam and Bhojanapalli, Srinadh and McAllester, David and Srebro, Nati},
  booktitle={Advances in Neural Information Processing Systems},
  pages={5947--5956},
  year={2017}
}


@book{anthony2009neural,
  title={Neural network learning: Theoretical foundations},
  author={Anthony, Martin and Bartlett, Peter L},
  year={2009},
  publisher={cambridge university press}
}

@book{vapnik2013nature,
  title={The nature of statistical learning theory},
  author={Vapnik, Vladimir},
  year={2013},
  publisher={Springer science \& business media}
}


@article{arora2016understanding,
  title={Understanding deep neural networks with rectified linear units},
  author={Arora, Raman and Basu, Amitabh and Mianjy, Poorya and Mukherjee, Anirbit},
  journal={arXiv preprint arXiv:1611.01491},
  year={2016}
}


###########more citation#################
@article{yarotsky2018optimal,
  title={Optimal approximation of continuous functions by very deep ReLU networks},
  author={Yarotsky, Dmitry},
  journal={arXiv preprint arXiv:1802.03620},
  year={2018}
}

@article{hanin2017universal,
  title={Universal function approximation by deep neural nets with bounded width and relu activations},
  author={Hanin, Boris},
  journal={arXiv preprint arXiv:1708.02691},
  year={2017}
}

@article{hanin2017approximating,
  title={Approximating Continuous Functions by ReLU Nets of Minimal Width},
  author={Hanin, Boris and Sellke, Mark},
  journal={arXiv preprint arXiv:1710.11278},
  year={2017}
}



@article{zhang2018learning,
  title={Learning One-hidden-layer ReLU Networks via Gradient Descent},
  author={Zhang, Xiao and Yu, Yaodong and Wang, Lingxiao and Gu, Quanquan},
  journal={arXiv preprint arXiv:1806.07808},
  year={2018}
}

@article{telgarsky2015representation,
  title={Representation benefits of deep feedforward networks},
  author={Telgarsky, Matus},
  journal={arXiv preprint arXiv:1509.08101},
  year={2015}
}


@article{nacson2018convergence,
  title={Convergence of Gradient Descent on Separable Data},
  author={Nacson, Mor Shpigel and Lee, Jason and Gunasekar, Suriya and Srebro, Nathan and Soudry, Daniel},
  journal={arXiv preprint arXiv:1803.01905},
  year={2018}
}

@article{nag2018,
  title={Deterministic PAC-Bayesian generalization bounds for deep networks via generalizing noise-resilience. },
  author={Vaishnavh Nagarajan and J. Zico Kolter},
  journal={Integration of Deep Learning Theories, NeurIPS 2018},
  year={2018}
}

@article{minshuo2018,
  title={On Generalization Bounds for a Family of Recurrent Neural Networks. },
  author={Chen, Minshuo and Li, Xingguo and Zhao, Tuo},
  journal={Integration of Deep Learning Theories, NeurIPS 2018},
  year={2018}
}

@article{neyshabur2018towards,
  title={Towards Understanding the Role of Over-Parametrization in Generalization of Neural Networks},
  author={Neyshabur, Behnam and Li, Zhiyuan and Bhojanapalli, Srinadh and LeCun, Yann and Srebro, Nathan},
  journal={arXiv preprint arXiv:1805.12076},
  year={2018}
}


@article{li2018tighter,
  title={On Tighter Generalization Bound for Deep Neural Networks: {CNNs}, {ResNets}, and Beyond},
  author={Li, Xingguo and Lu, Junwei and Wang, Zhaoran and Haupt, Jarvis and Zhao, Tuo},
  journal={arXiv preprint arXiv:1806.05159},
  year={2018}
}

@article{bousquet2002stability,
  title={Stability and generalization},
  author={Bousquet, Olivier and Elisseeff, Andr{\'e}},
  journal={Journal of machine learning research},
  volume={2},
  number={Mar},
  pages={499--526},
  year={2002}
}

@article{mou2017generalization,
  title={Generalization bounds of SGLD for non-convex learning: Two theoretical viewpoints},
  author={Mou, Wenlong and Wang, Liwei and Zhai, Xiyu and Zheng, Kai},
  journal={arXiv preprint arXiv:1707.05947},
  year={2017}
}



@article{chen2018stability,
  title={Stability and Convergence Trade-off of Iterative Optimization Algorithms},
  author={Chen, Yuansi and Jin, Chi and Yu, Bin},
  journal={arXiv preprint arXiv:1804.01619},
  year={2018}
}

@article{zhou2018generalization,
  title={Generalization Error Bounds with Probabilistic Guarantee for SGD in Nonconvex Optimization},
  author={Zhou, Yi and Liang, Yingbin and Zhang, Huishuai},
  journal={arXiv preprint arXiv:1802.06903},
  year={2018}
}



@article{zagoruyko2016wide,
  title={Wide residual networks},
  author={Zagoruyko, Sergey and Komodakis, Nikos},
  journal={arXiv preprint arXiv:1605.07146},
  year={2016}
}

@inproceedings{glorot2010understanding,
  title={Understanding the difficulty of training deep feedforward neural networks},
  author={Glorot, Xavier and Bengio, Yoshua},
  booktitle={Proceedings of the thirteenth international conference on artificial intelligence and statistics},
  pages={249--256},
  year={2010}
}





@article{liang2018just,
  title={Just Interpolate: Kernel" Ridgeless" Regression Can Generalize},
  author={Liang, Tengyuan and Rakhlin, Alexander},
  journal={arXiv preprint arXiv:1808.00387},
  year={2018}
}

@article{haeffele2015global,
  title={Global optimality in tensor factorization, deep learning, and beyond},
  author={Haeffele, Benjamin D and Vidal, Ren{\'e}},
  journal={arXiv preprint arXiv:1506.07540},
  year={2015}
}


@article{belkin2018understand,
  title={To understand deep learning we need to understand kernel learning},
  author={Belkin, Mikhail and Ma, Siyuan and Mandal, Soumik},
  journal={arXiv preprint arXiv:1802.01396},
  year={2018}
}




@article{soudry2017exponentially,
  title={Exponentially vanishing sub-optimal local minima in multilayer neural networks},
  author={Soudry, Daniel and Hoffer, Elad},
  journal={arXiv preprint arXiv:1702.05777},
  year={2017}
}


@article{yun2018critical,
  title={A Critical View of Global Optimality in Deep Learning},
  author={Yun, Chulhee and Sra, Suvrit and Jadbabaie, Ali},
  journal={arXiv preprint arXiv:1802.03487},
  year={2018}
}

@article{sirignano2018mean,
  title={Mean field analysis of neural networks: A central limit theorem},
  author={Sirignano, Justin and Spiliopoulos, Konstantinos},
  journal={arXiv preprint arXiv:1808.09372},
  year={2018}
}

@article{yun2018small,
  title={Small nonlinearities in activation functions create bad local minima in neural networks},
  author={Yun, Chulhee and Sra, Suvrit and Jadbabaie, Ali},
  year={2018}
}

@inproceedings{lin2018resnet,
  title={{ResNet} with one-neuron hidden layers is a Universal Approximator},
  author={Lin, Hongzhou and Jegelka, Stefanie},
  booktitle={Advances in Neural Information Processing Systems},
  pages={6172--6181},
  year={2018}
}

@inproceedings{he2016deep,
  title={Deep residual learning for image recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Proceedings of CVPR},
  pages={770--778},
  year={2016}
}

@article{rotskoff2018neural,
  title={Neural networks as Interacting Particle Systems: Asymptotic convexity of the Loss Landscape and Universal Scaling of the Approximation Error},
  author={Rotskoff, Grant M and Vanden-Eijnden, Eric},
  journal={arXiv preprint arXiv:1805.00915},
  year={2018}
}


@book{mohri2018foundations,
  title={Foundations of machine learning},
  author={Mohri, Mehryar and Rostamizadeh, Afshin and Talwalkar, Ameet},
  year={2018},
  publisher={MIT press}
}



@article{yang2019scaling,
  title={Scaling limits of wide neural networks with weight sharing: {Gaussian} process behavior, gradient independence, and neural tangent kernel derivation},
  author={Yang, Greg},
  journal={arXiv preprint arXiv:1902.04760},
  year={2019}
}



@inproceedings{rahimi2008random,
  title={Random features for large-scale kernel machines},
  author={Rahimi, Ali and Recht, Benjamin},
  booktitle={Advances in neural information processing systems},
  pages={1177--1184},
  year={2008}
}


@article{song2018mean,
  title={A mean field view of the landscape of two-layers neural networks},
  author={Song, Mei and Montanari, A and Nguyen, P},
  journal={PNAS},
  volume={115},
  pages={E7665--E7671},
  year={2018}
}





@inproceedings{hardt2015train,
  title={Train faster, generalize better: Stability of stochastic gradient descent},
  author={Hardt, Moritz and Recht, Ben and Singer, Yoram},
  booktitle={International Conference on Machine Learning},
  pages={1225--1234},
  year={2016}
}



@inproceedings{gao2019learning,
  title={Learning One-hidden-layer Neural Networks under General Input Distributions},
  author={Gao, Weihao and Makkuva, Ashok and Oh, Sewoong and Viswanath, Pramod},
  booktitle={The 22nd International Conference on Artificial Intelligence and Statistics},
  pages={1950--1959},
  year={2019}
}


















@string{JASA = {Journal of the American Statistical Association}}
@string{JC  = {Journal of Classification}}
@string{JSPI  = {Journal of Statistical Planning and Inference}}
@string{JRSSB  = {Journal of the Royal Statistical Society, Series B}}
@string{SAGMB  = {Statistical Applications in Genetics and Molecular Biology}}
@string{NIPS  = {Advances in Neural Information Processing Systems}}
@string{AOS  = {Annals of Statistics}}
@string{AOAS  = {Annals of Applied Statistics}}
@string{JMLR  = {Journal of Machine Learning Research}}
@string{EJS  = {Electronic Journal of Statistics}}
@string{AISTATS  = {International Conference on Artificial Intelligence and Statistics}}
@string{UAI  = {Conference on Uncertainty in Artificial Intelligence}}
@string{NIPS  = {Advances in Neural Information Processing Systems}}
@string{ICML  = {International Conference on Machine Learning}}
@string{TIT = {IEEE Transactions on Information Theory}}


@inproceedings{harvey2017nearly,
  title={Nearly-tight VC-dimension bounds for piecewise linear neural networks},
  author={Harvey, Nick and Liaw, Christopher and Mehrabian, Abbas},
  booktitle={Conference on Learning Theory},
  pages={1064--1068},
  year={2017}
}

@inproceedings{bartlett1999almost,
  title={Almost linear VC dimension bounds for piecewise polynomial networks},
  author={Bartlett, Peter L and Maiorov, Vitaly and Meir, Ron},
  booktitle={Advances in Neural Information Processing Systems},
  pages={190--196},
  year={1999}
}

@inproceedings{du2017convolutional,
	title={When is a Convolutional Filter Easy To Learn?},
	author={Du, Simon S and Lee, Jason D and Tian, Yuandong},
	booktitle={International Conference on Learning Representations},
	year={2018}
}


@article{soltanolkotabi2017learning,
  title={Learning ReLUs via Gradient Descent},
  author={Soltanolkotabi, Mahdi},
  journal={arXiv preprint arXiv:1705.04591},
  year={2017}
}


@article{tian2017analytical,
  title={An Analytical Formula of Population Gradient for two-layered ReLU network and its Applications in Convergence and Critical Point Analysis},
  author={Tian, Yuandong},
  journal={arXiv preprint arXiv:1703.00560},
  year={2017}
}

@article{hsu2012tail,
  title={A tail inequality for quadratic forms of subgaussian random vectors},
  author={Hsu, Daniel and Kakade, Sham and Zhang, Tong and others},
  journal={Electronic Communications in Probability},
  volume={17},
  year={2012},
  publisher={The Institute of Mathematical Statistics and the Bernoulli Society}
}

@article{vershynin2010introduction,
  title={Introduction to the non-asymptotic analysis of random matrices},
  author={Vershynin, Roman},
  journal={arXiv preprint arXiv:1011.3027},
  year={2010}
}

@article{dally2015high,
  title={High-performance hardware for machine learning},
  author={Dally, William},
  journal={NIPS Tutorial},
  year={2015}
}


@inproceedings{szegedy2015going,
  title={Going deeper with convolutions},
  author={Szegedy, Christian and Liu, Wei and Jia, Yangqing and Sermanet, Pierre and Reed, Scott and Anguelov, Dragomir and Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich, Andrew},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={1--9},
  year={2015}
}
@inproceedings{ren2015faster,
  title={Faster {R-CNN}: Towards real-time object detection with region proposal networks},
  author={Ren, Shaoqing and He, Kaiming and Girshick, Ross and Sun, Jian},
  booktitle={Advances in neural information processing systems},
  pages={91--99},
  year={2015}
}
@inproceedings{devlin2014fast,
  title={Fast and Robust Neural Network Joint Models for Statistical Machine Translation.},
  author={Devlin, Jacob and Zbib, Rabih and Huang, Zhongqiang and Lamar, Thomas and Schwartz, Richard M and Makhoul, John},
  booktitle={ACL (1)},
  pages={1370--1380},
  year={2014}
}
@article{bahdanau2014neural,
  title={Neural machine translation by jointly learning to align and translate},
  author={Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1409.0473},
  year={2014}
}

@article{baum1990polynomial,
  title={A polynomial time algorithm that learns two hidden unit nets},
  author={Baum, Eric B},
  journal={Neural Computation},
  volume={2},
  number={4},
  pages={510--522},
  year={1990},
  publisher={MIT Press}
}
@inproceedings{zhang2016convexified,
  title={Convexified Convolutional Neural Networks},
  author={Zhang, Yuchen and Liang, Percy and Wainwright, Martin J},
  booktitle={International Conference on Machine Learning},
  pages={4044--4053},
  year={2017}
}


@article{nguyen2017bloss,
  title={The loss surface and expressivity of deep convolutional neural networks},
  author={Nguyen, Quynh and Hein, Matthias},
  journal={arXiv preprint arXiv:1710.10928},
  year={2017}
}




@article{tian2016symmetry,
  title={Symmetry-breaking convergence analysis of certain two-layered neural networks with ReLU nonlinearity},
  author={Tian, Yuandong},
  year={2016}
}


@incollection{klivans2009baum,
  title={Baum’s algorithm learns intersections of halfspaces with respect to log-concave distributions},
  author={Klivans, Adam R and Long, Philip M and Tang, Alex K},
  booktitle={Approximation, Randomization, and Combinatorial Optimization. Algorithms and Techniques},
  pages={588--600},
  year={2009},
  publisher={Springer}
}


@inproceedings{daniely2016toward,
  title={Toward deeper understanding of neural networks: The power of initialization and a dual view on expressivity},
  author={Daniely, Amit and Frostig, Roy and Singer, Yoram},
  booktitle={Advances In Neural Information Processing Systems},
  pages={2253--2261},
  year={2016}
}
@article{lu2017expressive,
  title={The Expressive Power of Neural Networks: A View from the Width},
  author={Lu, Zhou and Pu, Hongming and Wang, Feicheng and Hu, Zhiqiang and Wang, Liwei},
  journal={arXiv preprint arXiv:1709.02540},
  year={2017}
}
@inproceedings{cohen2016expressive,
  title={On the expressive power of deep learning: A tensor analysis},
  author={Cohen, Nadav and Sharir, Or and Shashua, Amnon},
  booktitle={Conference on Learning Theory},
  pages={698--728},
  year={2016}
}

@article{telgarsky2016benefits,
  title={Benefits of depth in neural networks},
  author={Telgarsky, Matus},
  journal={arXiv preprint arXiv:1602.04485},
  year={2016}
}
@article{raghu2016expressive,
  title={On the expressive power of deep neural networks},
  author={Raghu, Maithra and Poole, Ben and Kleinberg, Jon and Ganguli, Surya and Sohl-Dickstein, Jascha},
  journal={arXiv preprint arXiv:1606.05336},
  year={2016}
}
@inproceedings{poole2016exponential,
  title={Exponential expressivity in deep neural networks through transient chaos},
  author={Poole, Ben and Lahiri, Subhaneil and Raghu, Maithreyi and Sohl-Dickstein, Jascha and Ganguli, Surya},
  booktitle={Advances In Neural Information Processing Systems},
  pages={3360--3368},
  year={2016}
}
@inproceedings{montufar2014number,
  title={On the number of linear regions of deep neural networks},
  author={Montufar, Guido F and Pascanu, Razvan and Cho, Kyunghyun and Bengio, Yoshua},
  booktitle={Advances in neural information processing systems},
  pages={2924--2932},
  year={2014}
}

@article{vsima2006training,
  title={Training a single sigmoidal neuron is hard},
  author={{\v{S}}{\'\i}ma, Ji{\v{r}}{\'\i}},
  journal={Training},
  volume={14},
  number={11},
  year={2006},
  publisher={MIT Press}
}
@inproceedings{livni2014computational,
  title={On the computational efficiency of training neural networks},
  author={Livni, Roi and Shalev-Shwartz, Shai and Shamir, Ohad},
  booktitle={Advances in Neural Information Processing Systems},
  pages={855--863},
  year={2014}
}

@inproceedings{shalev2017failures,
  title={Failures of gradient-based deep learning},
  author={Shalev-Shwartz, Shai and Shamir, Ohad and Shammah, Shaked},
  booktitle={International Conference on Machine Learning},
  pages={3067--3075},
  year={2017}
}
@article{shalev2017weight,
  title={Weight Sharing is Crucial to Succesful Optimization},
  author={Shalev-Shwartz, Shai and Shamir, Ohad and Shammah, Shaked},
  journal={arXiv preprint arXiv:1706.00687},
  year={2017}
}

@article{goel2016reliably,
  title={Reliably learning the relu in polynomial time},
  author={Goel, Surbhi and Kanade, Varun and Klivans, Adam and Thaler, Justin},
  journal={arXiv preprint arXiv:1611.10258},
  year={2016}
}
@article{zhang2015learning,
  title={Learning halfspaces and neural networks with random initialization},
  author={Zhang, Yuchen and Lee, Jason D and Wainwright, Martin J and Jordan, Michael I},
  journal={arXiv preprint arXiv:1511.07948},
  year={2015}
}
@article{sedghi2014provable,
  title={Provable methods for training neural networks with sparse connectivity},
  author={Sedghi, Hanie and Anandkumar, Anima},
  journal={arXiv preprint arXiv:1412.2693},
  year={2014}
}

@inproceedings{ge2015escaping,
  title={Escaping from saddle points—online stochastic gradient for tensor decomposition},
  author={Ge, Rong and Huang, Furong and Jin, Chi and Yuan, Yang},
  booktitle={Conference on Learning Theory},
  pages={797--842},
  year={2015}
}
@inproceedings{jin2016provable,
  title={Provable efficient online matrix completion via non-convex stochastic gradient descent},
  author={Jin, Chi and Kakade, Sham M and Netrapalli, Praneeth},
  booktitle={Advances in Neural Information Processing Systems},
  pages={4520--4528},
  year={2016}
}
@article{jin2017escape,
  title={How to Escape Saddle Points Efficiently},
  author={Jin, Chi and Ge, Rong and Netrapalli, Praneeth and Kakade, Sham M and Jordan, Michael I},
  journal={arXiv preprint arXiv:1703.00887},
  year={2017}
}

@article{lu2017depth,
  title={Depth Creates No Bad Local Minima},
  author={Lu, Haihao and Kawaguchi, Kenji},
  journal={arXiv preprint arXiv:1702.08580},
  year={2017}
}
@inproceedings{yun2017global,
  title={Global optimality conditions for deep neural networks},
  author={Yun, Chulhee and Sra, Suvrit and Jadbabaie, Ali},
  booktitle={International Conference on Learning Representations},
  year={2018}
}

@article{taghvaei2017regularization,
  title={How regularization affects the critical points in linear networks},
  author={Taghvaei, Amirhossein and Kim, Jin W and Mehta, Prashant G},
  journal={arXiv preprint arXiv:1709.09625},
  year={2017}
}

@article{feizi2017porcupine,
  title={Porcupine Neural Networks:(Almost) All Local Optima are Global},
  author={Feizi, Soheil and Javadi, Hamid and Zhang, Jesse and Tse, David},
  journal={arXiv preprint arXiv:1710.02196},
  year={2017}
}


@article{soltanolkotabi2017theoretical,
  title={Theoretical insights into the optimization landscape of over-parameterized shallow neural networks},
  author={Soltanolkotabi, Mahdi and Javanmard, Adel and Lee, Jason D},
  journal={IEEE Transactions on Information Theory},
  volume={65},
  number={2},
  pages={742--769},
  year={2018},
  publisher={IEEE}
}


@article{carmon2017convex,
  title={" Convex Until Proven Guilty": Dimension-Free Acceleration of Gradient Descent on Non-Convex Functions},
  author={Carmon, Yair and Hinder, Oliver and Duchi, John C and Sidford, Aaron},
  journal={arXiv preprint arXiv:1705.02766},
  year={2017}
}


@inproceedings{hardt2016identity,
  title={Identity matters in deep learning},
  author={Hardt, Moritz and Ma, Tengyu},
  booktitle={International Conference on Learning Representations},
  year={2017}
}

@article{arora2016provable,
  title={Provable learning of Noisy-or Networks},
  author={Arora, Sanjeev and Ge, Rong and Ma, Tengyu and Risteski, Andrej},
  journal={arXiv preprint arXiv:1612.08795},
  year={2016}
}

@inproceedings{zhang2017learnability,
  title={On the Learnability of Fully-Connected Neural Networks},
  author={Zhang, Yuchen and Lee, Jason and Wainwright, Martin and Jordan, Michael},
  booktitle={Artificial Intelligence and Statistics},
  pages={83--91},
  year={2017}
}
@inproceedings{jain2015fast,
  title={Fast exact matrix completion with finite samples},
  author={Jain, Prateek and Netrapalli, Praneeth},
  booktitle={Conference on Learning Theory},
  pages={1007--1034},
  year={2015}
}
@inproceedings{pan2016expressiveness,
  title={Expressiveness of rectifier networks},
  author={Pan, Xingyuan and Srikumar, Vivek},
  booktitle={International Conference on Machine Learning},
  pages={2427--2435},
  year={2016}
}

@article{barron1993universal,
  title={Universal approximation bounds for superpositions of a sigmoidal function},
  author={Barron, Andrew R},
  journal={IEEE Transactions on Information theory},
  volume={39},
  number={3},
  pages={930--945},
  year={1993},
  publisher={IEEE}
}


@inproceedings{auer1996exponentially,
  title={Exponentially many local minima for single neurons},
  author={Auer, Peter and Herbster, Mark and Warmuth, Manfred K},
  booktitle={Advances in neural information processing systems},
  pages={316--322},
  year={1996}
}
@inproceedings{ge2017learning,
  title={Learning One-hidden-layer Neural Networks with Landscape Design},
  author={Ge, Rong and Lee, Jason D and Ma, Tengyu},
  booktitle={International Conference on Learning Representations},
  year={2017}
}
@inproceedings{kalai2009isotron,
  title={The Isotron Algorithm: High-Dimensional Isotonic Regression.},
  author={Kalai, Adam Tauman and Sastry, Ravi},
  booktitle={COLT},
  year={2009},
  organization={Citeseer}
}
\nshortmid




@inproceedings{kakade2011efficient,
  title={Efficient learning of generalized linear and single index models with isotonic regression},
  author={Kakade, Sham M and Kanade, Varun and Shamir, Ohad and Kalai, Adam},
  booktitle={Advances in Neural Information Processing Systems},
  pages={927--935},
  year={2011}
}



@inproceedings{yi2015regularized,
  title={Regularized em algorithms: A unified framework and statistical guarantees},
  author={Yi, Xinyang and Caramanis, Constantine},
  booktitle={Advances in Neural Information Processing Systems},
  pages={1567--1575},
  year={2015}
}




@misc{hoeffding1940masstabinvariante,
  title={Masstabinvariante Korrelationtheorie, Schriften des Mathematis chen Instituts und des Instituts f{\"u}r Angewandte Mathematik der Universit{\"a}t Berlin 5, 181\# 233.(Translated in Fisher, NI and PK Sen (1994). The Collected Works of Wassily Hoeffding, New York},
  author={Hoeffding, W},
  year={1940},
  publisher={Springer-Verlag}
}


@article{cuadras2002covariance,
  title={On the covariance between functions},
  author={Cuadras, Carles M},
  journal={Journal of Multivariate Analysis},
  volume={81},
  number={1},
  pages={19--27},
  year={2002},
  publisher={Elsevier}
}


@incollection{sen1994impact,
  title={The impact of Wassily Hoeffding’s research on nonparametrics},
  author={Sen, Pranab K},
  booktitle={The Collected Works of Wassily Hoeffding},
  pages={29--55},
  year={1994},
  publisher={Springer}
}



@article{gordon1985some,
  title={Some inequalities for {Gaussian} processes and applications},
  author={Gordon, Yehoram},
  journal={Israel Journal of Mathematics},
  volume={50},
  number={4},
  pages={265--289},
  year={1985},
  publisher={Springer}
}


@inproceedings{goel2018learning,
  title={Learning One Convolutional Layer with Overlapping Patches},
  author={Goel, Surbhi and Klivans, Adam and Meka, Raghu},
  booktitle={International Conference on Machine Learning},
  pages={1778--1786},
  year={2018}
}


@article{du2018improved,
  title={Improved Learning of One-hidden-layer Convolutional Neural Networks with Overlaps},
  author={Du, Simon S and Goel, Surbhi},
  journal={arXiv preprint arXiv:1805.07798},
  year={2018}
}

@inproceedings{du2018many,
  title={How Many Samples are Needed to Learn a Convolutional Neural Network?},
  author={Du, Simon S and Wang, Yining and Zhai, Xiyu and Balakrishnan, Sivaraman and Salakhutdinov, Ruslan and Singh, Aarti},
  booktitle={Advances in Neural Information Processing Systems},
  year={2018}
}



@book{talagrand2014upper,
  title={Upper and lower bounds for stochastic processes: modern methods and classical problems},
  author={Talagrand, Michel},
  volume={60},
  year={2014},
  publisher={Springer Science \& Business Media}
}





@inproceedings{du2018gradient,
  title={Gradient Descent Provably Optimizes Over-parameterized Neural Networks},
  author={Du, Simon S and Zhai, Xiyu and Poczos, Barnabas and Singh, Aarti},
  booktitle={International Conference on Learning Representations},
  year={2019}
}
@inproceedings{li2018learning,
  title={Learning overparameterized neural networks via stochastic gradient descent on structured data},
  author={Li, Yuanzhi and Liang, Yingyu},
  booktitle={Advances in Neural Information Processing Systems},
  pages={8157--8166},
  year={2018}
}
@inproceedings{cotter2011better,
  title={Better mini-batch algorithms via accelerated gradient methods},
  author={Cotter, Andrew and Shamir, Ohad and Srebro, Nati and Sridharan, Karthik},
  booktitle={Advances in neural information processing systems},
  pages={1647--1655},
  year={2011}
}
@article{shalev2013stochastic,
  title={Stochastic dual coordinate ascent methods for regularized loss minimization},
  author={Shalev-Shwartz, Shai and Zhang, Tong},
  journal={Journal of Machine Learning Research},
  volume={14},
  number={Feb},
  pages={567--599},
  year={2013}
}
@inproceedings{brutzkus2017sgd,
  title={SGD learns over-parameterized networks that provably generalize on linearly separable data},
  author={Brutzkus, Alon and Globerson, Amir and Malach, Eran and Shalev-Shwartz, Shai},
  booktitle = {International Conference on Learning Representations},
  year={2018}
}

@inproceedings{nacson2018stochastic,
  title={Stochastic Gradient Descent on Separable Data: Exact Convergence with a Fixed Learning Rate},
  author={Nacson, Mor Shpigel and Srebro, Nathan and Soudry, Daniel},
  booktitle={The 22nd International Conference on Artificial Intelligence and Statistics},
  pages={3051--3059},
  year={2019}
}


@article{ji2018gradient,
  title={Gradient descent aligns the layers of deep linear networks},
  author={Ji, Ziwei and Telgarsky, Matus},
  journal={arXiv preprint arXiv:1810.02032},
  year={2018}
}



@article{slepian1962one,
  title={The one-sided barrier problem for {Gaussian} noise},
  author={Slepian, David},
  journal={Bell Labs Technical Journal},
  volume={41},
  number={2},
  pages={463--501},
  year={1962},
  publisher={Wiley Online Library}
}




######################## Teacher Network



######################### Deep Linear Network
@article{saxe2013exact,
  title={Exact solutions to the nonlinear dynamics of learning in deep linear neural networks},
  author={Saxe, Andrew M and McClelland, James L and Ganguli, Surya},
  journal={arXiv preprint arXiv:1312.6120},
  year={2013}
}


@inproceedings{bartlett2018gradient,
  title={Gradient descent with identity initialization efficiently learns positive definite linear transformations},
  author={Bartlett, Peter and Helmbold, Dave and Long, Phil},
  booktitle={International Conference on Machine Learning},
  pages={520--529},
  year={2018}
}

@article{arora2018convergence,
  title={A Convergence Analysis of Gradient Descent for Deep Linear Neural Networks},
  author={Arora, Sanjeev and Cohen, Nadav and Golowich, Noah and Hu, Wei},
  journal={arXiv preprint arXiv:1810.02281},
  year={2018}
}




#####################Optimization Landscape


######################Generalization



@inproceedings{li2017convergence,
  title={Convergence analysis of two-layer neural networks with relu activation},
  author={Li, Yuanzhi and Yuan, Yang},
  booktitle={Advances in Neural Information Processing Systems},
  pages={597--607},
  year={2017}
}

@article{soltanolkotabi2017learning,
  title={Learning ReLUs via Gradient Descent},
  author={Soltanolkotabi, Mahdi},
  journal={arXiv preprint arXiv:1705.04591},
  year={2017}
}

@inproceedings{zhong2017recovery,
  title={Recovery Guarantees for One-hidden-layer Neural Networks},
  author={Zhong, Kai and Song, Zhao and Jain, Prateek and Bartlett, Peter L and Dhillon, Inderjit S},
  booktitle={International Conference on Machine Learning},
  pages={4140--4149},
  year={2017}
}

@article{tian2017analytical,
  title={An Analytical Formula of Population Gradient for two-layered ReLU network and its Applications in Convergence and Critical Point Analysis},
  author={Tian, Yuandong},
  journal={arXiv preprint arXiv:1703.00560},
  year={2017}
}


@article{hsu2012tail,
  title={A tail inequality for quadratic forms of subgaussian random vectors},
  author={Hsu, Daniel and Kakade, Sham and Zhang, Tong and others},
  journal={Electronic Communications in Probability},
  volume={17},
  year={2012},
  publisher={The Institute of Mathematical Statistics and the Bernoulli Society}
}


@article{dally2015high,
  title={High-performance hardware for machine learning},
  author={Dally, William},
  journal={NIPS Tutorial},
  year={2015}
}

@inproceedings{sutskever2014sequence,
  title={Sequence to sequence learning with neural networks},
  author={Sutskever, Ilya and Vinyals, Oriol and Le, Quoc V},
  booktitle={Advances in neural information processing systems},
  pages={3104--3112},
  year={2014}
}

@inproceedings{krizhevsky2012imagenet,
  title={Imagenet classification with deep convolutional neural networks},
  author={Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
  booktitle={Advances in neural information processing systems},
  pages={1097--1105},
  year={2012}
}
@inproceedings{szegedy2015going,
  title={Going deeper with convolutions},
  author={Szegedy, Christian and Liu, Wei and Jia, Yangqing and Sermanet, Pierre and Reed, Scott and Anguelov, Dragomir and Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich, Andrew},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={1--9},
  year={2015}
}
@inproceedings{devlin2014fast,
  title={Fast and Robust Neural Network Joint Models for Statistical Machine Translation.},
  author={Devlin, Jacob and Zbib, Rabih and Huang, Zhongqiang and Lamar, Thomas and Schwartz, Richard M and Makhoul, John},
  booktitle={ACL (1)},
  pages={1370--1380},
  year={2014}
}
@article{bahdanau2014neural,
  title={Neural machine translation by jointly learning to align and translate},
  author={Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1409.0473},
  year={2014}
}
@article{silver2016mastering,
  title={Mastering the game of Go with deep neural networks and tree search},
  author={Silver, David and Huang, Aja and Maddison, Chris J and Guez, Arthur and Sifre, Laurent and Van Den Driessche, George and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc and others},
  journal={Nature},
  volume={529},
  number={7587},
  pages={484--489},
  year={2016},
  publisher={Nature Research}
}

@article{hornik1991approximation,
  title={Approximation capabilities of multilayer feedforward networks},
  author={Hornik, Kurt},
  journal={Neural networks},
  volume={4},
  number={2},
  pages={251--257},
  year={1991},
  publisher={Elsevier}
}

@inproceedings{daniely2016toward,
  title={Toward deeper understanding of neural networks: The power of initialization and a dual view on expressivity},
  author={Daniely, Amit and Frostig, Roy and Singer, Yoram},
  booktitle={Advances In Neural Information Processing Systems},
  pages={2253--2261},
  year={2016}
}
@article{lu2017expressive,
  title={The Expressive Power of Neural Networks: A View from the Width},
  author={Lu, Zhou and Pu, Hongming and Wang, Feicheng and Hu, Zhiqiang and Wang, Liwei},
  journal={arXiv preprint arXiv:1709.02540},
  year={2017}
}
@inproceedings{cohen2016expressive,
  title={On the expressive power of deep learning: A tensor analysis},
  author={Cohen, Nadav and Sharir, Or and Shashua, Amnon},
  booktitle={Conference on Learning Theory},
  pages={698--728},
  year={2016}
}
@inproceedings{cohen2016convolutional,
  title={Convolutional rectifier networks as generalized tensor decompositions},
  author={Cohen, Nadav and Shashua, Amnon},
  booktitle={International Conference on Machine Learning},
  pages={955--963},
  year={2016}
}
@article{telgarsky2016benefits,
  title={Benefits of depth in neural networks},
  author={Telgarsky, Matus},
  journal={arXiv preprint arXiv:1602.04485},
  year={2016}
}
@article{raghu2016expressive,
  title={On the expressive power of deep neural networks},
  author={Raghu, Maithra and Poole, Ben and Kleinberg, Jon and Ganguli, Surya and Sohl-Dickstein, Jascha},
  journal={arXiv preprint arXiv:1606.05336},
  year={2016}
}
@inproceedings{poole2016exponential,
  title={Exponential expressivity in deep neural networks through transient chaos},
  author={Poole, Ben and Lahiri, Subhaneil and Raghu, Maithreyi and Sohl-Dickstein, Jascha and Ganguli, Surya},
  booktitle={Advances In Neural Information Processing Systems},
  pages={3360--3368},
  year={2016}
}
@inproceedings{montufar2014number,
  title={On the number of linear regions of deep neural networks},
  author={Montufar, Guido F and Pascanu, Razvan and Cho, Kyunghyun and Bengio, Yoshua},
  booktitle={Advances in neural information processing systems},
  pages={2924--2932},
  year={2014}
}

@article{vsima2006training,
  title={Training a single sigmoidal neuron is hard},
  author={{\v{S}}{\'\i}ma, Ji{\v{r}}{\'\i}},
  journal={Training},
  volume={14},
  number={11},
  year={2006},
  publisher={MIT Press}
}
@inproceedings{livni2014computational,
  title={On the computational efficiency of training neural networks},
  author={Livni, Roi and Shalev-Shwartz, Shai and Shamir, Ohad},
  booktitle={Advances in Neural Information Processing Systems},
  pages={855--863},
  year={2014}
}
@article{shamir2016distribution,
  title={Distribution-specific hardness of learning neural networks},
  author={Shamir, Ohad},
  journal={The Journal of Machine Learning Research},
  volume={19},
  number={1},
  pages={1135--1163},
  year={2018},
  publisher={JMLR. org}
}
@inproceedings{shalev2017failures,
  title={Failures of gradient-based deep learning},
  author={Shalev-Shwartz, Shai and Shamir, Ohad and Shammah, Shaked},
  booktitle={International Conference on Machine Learning},
  pages={3067--3075},
  year={2017}
}
@article{shalev2017weight,
  title={Weight Sharing is Crucial to Succesful Optimization},
  author={Shalev-Shwartz, Shai and Shamir, Ohad and Shammah, Shaked},
  journal={arXiv preprint arXiv:1706.00687},
  year={2017}
}

@article{goel2016reliably,
  title={Reliably learning the relu in polynomial time},
  author={Goel, Surbhi and Kanade, Varun and Klivans, Adam and Thaler, Justin},
  journal={arXiv preprint arXiv:1611.10258},
  year={2016}
}
@article{zhang2015learning,
  title={Learning halfspaces and neural networks with random initialization},
  author={Zhang, Yuchen and Lee, Jason D and Wainwright, Martin J and Jordan, Michael I},
  journal={arXiv preprint arXiv:1511.07948},
  year={2015}
}
@article{sedghi2014provable,
  title={Provable methods for training neural networks with sparse connectivity},
  author={Sedghi, Hanie and Anandkumar, Anima},
  journal={arXiv preprint arXiv:1412.2693},
  year={2014}
}
@article{janzamin2015beating,
  title={Beating the perils of non-convexity: Guaranteed training of neural networks using tensor methods},
  author={Janzamin, Majid and Sedghi, Hanie and Anandkumar, Anima},
  journal={arXiv preprint arXiv:1506.08473},
  year={2015}
}
@inproceedings{ge2015escaping,
  title={Escaping from saddle points—online stochastic gradient for tensor decomposition},
  author={Ge, Rong and Huang, Furong and Jin, Chi and Yuan, Yang},
  booktitle={Conference on Learning Theory},
  pages={797--842},
  year={2015}
}
@inproceedings{jin2016provable,
  title={Provable efficient online matrix completion via non-convex stochastic gradient descent},
  author={Jin, Chi and Kakade, Sham M and Netrapalli, Praneeth},
  booktitle={Advances in Neural Information Processing Systems},
  pages={4520--4528},
  year={2016}
}
@article{jin2017escape,
  title={How to Escape Saddle Points Efficiently},
  author={Jin, Chi and Ge, Rong and Netrapalli, Praneeth and Kakade, Sham M and Jordan, Michael I},
  journal={arXiv preprint arXiv:1703.00887},
  year={2017}
}

@article{lu2017depth,
  title={Depth Creates No Bad Local Minima},
  author={Lu, Haihao and Kawaguchi, Kenji},
  journal={arXiv preprint arXiv:1702.08580},
  year={2017}
}

@article{taghvaei2017regularization,
  title={How regularization affects the critical points in linear networks},
  author={Taghvaei, Amirhossein and Kim, Jin W and Mehta, Prashant G},
  journal={arXiv preprint arXiv:1709.09625},
  year={2017}
}

@article{feizi2017porcupine,
  title={Porcupine Neural Networks:(Almost) All Local Optima are Global},
  author={Feizi, Soheil and Javadi, Hamid and Zhang, Jesse and Tse, David},
  journal={arXiv preprint arXiv:1710.02196},
  year={2017}
}



@article{carmon2017convex,
  title={" Convex Until Proven Guilty": Dimension-Free Acceleration of Gradient Descent on Non-Convex Functions},
  author={Carmon, Yair and Hinder, Oliver and Duchi, John C and Sidford, Aaron},
  journal={arXiv preprint arXiv:1705.02766},
  year={2017}
}


@inproceedings{safran2016quality,
  title={On the quality of the initial basin in overspecified neural networks},
  author={Safran, Itay and Shamir, Ohad},
  booktitle={International Conference on Machine Learning},
  pages={774--782},
  year={2016}
}


@article{arora2016provable,
  title={Provable learning of Noisy-or Networks},
  author={Arora, Sanjeev and Ge, Rong and Ma, Tengyu and Risteski, Andrej},
  journal={arXiv preprint arXiv:1612.08795},
  year={2016}
}
@inproceedings{xie2017diverse,
  title={Diverse Neural Network Learns True Target Functions},
  author={Xie, Bo and Liang, Yingyu and Song, Le},
  booktitle={Artificial Intelligence and Statistics},
  pages={1216--1224},
  year={2017}
}
@inproceedings{zhang2017learnability,
  title={On the Learnability of Fully-Connected Neural Networks},
  author={Zhang, Yuchen and Lee, Jason and Wainwright, Martin and Jordan, Michael},
  booktitle={Artificial Intelligence and Statistics},
  pages={83--91},
  year={2017}
}
@inproceedings{jain2015fast,
  title={Fast exact matrix completion with finite samples},
  author={Jain, Prateek and Netrapalli, Praneeth},
  booktitle={Conference on Learning Theory},
  pages={1007--1034},
  year={2015}
}
@inproceedings{pan2016expressiveness,
  title={Expressiveness of rectifier networks},
  author={Pan, Xingyuan and Srikumar, Vivek},
  booktitle={International Conference on Machine Learning},
  pages={2427--2435},
  year={2016}
}

@article{barron1993universal,
  title={Universal approximation bounds for superpositions of a sigmoidal function},
  author={Barron, Andrew R},
  journal={IEEE Transactions on Information theory},
  volume={39},
  number={3},
  pages={930--945},
  year={1993},
  publisher={IEEE}
}
@inproceedings{zhang2016understanding,
  title={Understanding deep learning requires rethinking generalization},
  author={Zhang, Chiyuan and Bengio, Samy and Hardt, Moritz and Recht, Benjamin and Vinyals, Oriol},
  booktitle={International Conference on Learning Representations},
  year={2017}
}
@inproceedings{mei2016landscape,
  title={The landscape of empirical risk for non-convex losses},
  author={Mei, Song and Bai, Yu and Montanari, Andrea},
  booktitle={The Annals of Statistics},
  year={2018}
}
@inproceedings{du2017gradient,
  title={Gradient Descent Learns One-hidden-layer {CNN}: Don’t be Afraid of Spurious Local Minima},
  author={Du, Simon S and Lee, Jason D and Tian, Yuandong and Singh, Aarti and Poczos, Barnabas},
  booktitle={International Conference on Machine Learning},
  pages={1338--1347},
  year={2018}
}
@inproceedings{auer1996exponentially,
  title={Exponentially many local minima for single neurons},
  author={Auer, Peter and Herbster, Mark and Warmuth, Manfred K},
  booktitle={Advances in neural information processing systems},
  pages={316--322},
  year={1996}
}

@inproceedings{kalai2009isotron,
  title={The Isotron Algorithm: High-Dimensional Isotonic Regression.},
  author={Kalai, Adam Tauman and Sastry, Ravi},
  booktitle={COLT},
  year={2009},
  organization={Citeseer}
}

@inproceedings{fu2018local,
  title={Local Geometry of Cross Entropy Loss in Learning One-Hidden-Layer Neural Networks},
  author={Fu, Haoyu and Chi, Yuejie and Liang, Yingbin},
  booktitle={2019 IEEE International Symposium on Information Theory (ISIT)},
  pages={1972--1976},
  year={2019},
  organization={IEEE}
}

@inproceedings{kakade2011efficient,
  title={Efficient learning of generalized linear and single index models with isotonic regression},
  author={Kakade, Sham M and Kanade, Varun and Shamir, Ohad and Kalai, Adam},
  booktitle={Advances in Neural Information Processing Systems},
  pages={927--935},
  year={2011}
}

@inproceedings{jain2013low,
  title={Low-rank matrix completion using alternating minimization},
  author={Jain, Prateek and Netrapalli, Praneeth and Sanghavi, Sujay},
  booktitle={Proceedings of the forty-fifth annual ACM symposium on Theory of computing},
  pages={665--674},
  year={2013},
  organization={ACM}
}

@inproceedings{sutskever2013importance,
  title={On the importance of initialization and momentum in deep learning},
  author={Sutskever, Ilya and Martens, James and Dahl, George and Hinton, Geoffrey},
  booktitle={International conference on machine learning},
  pages={1139--1147},
  year={2013}
}



@inproceedings{livni2014computational,
  title={On the computational efficiency of training neural networks},
  author={Livni, Roi and Shalev-Shwartz, Shai and Shamir, Ohad},
  booktitle={Advances in Neural Information Processing Systems},
  pages={855--863},
  year={2014}
}




@article{barron1993universal,
  title={Universal approximation bounds for superpositions of a sigmoidal function},
  author={Barron, Andrew R},
  journal={IEEE Transactions on Information theory},
  volume={39},
  number={3},
  pages={930--945},
  year={1993},
  publisher={IEEE}
}



@article{liang2016deep,
  title={Why deep neural networks for function approximation?},
  author={Liang, Shiyu and Srikant, R},
  journal={arXiv preprint arXiv:1610.04161},
  year={2016}
}

@article{yarotsky2017error,
  title={Error bounds for approximations with deep {ReLU} networks},
  author={Yarotsky, Dmitry},
  journal={Neural Networks},
  volume={94},
  pages={103--114},
  year={2017},
  publisher={Elsevier}
}

@article{neyshabur2014search,
  title={In search of the real inductive bias: On the role of implicit regularization in deep learning},
  author={Neyshabur, Behnam and Tomioka, Ryota and Srebro, Nathan},
  journal={arXiv preprint arXiv:1412.6614},
  year={2014}
}


@inproceedings{sutskever2013importance,
  title={On the importance of initialization and momentum in deep learning},
  author={Sutskever, Ilya and Martens, James and Dahl, George and Hinton, Geoffrey},
  booktitle={International conference on machine learning},
  pages={1139--1147},
  year={2013}
}

@inproceedings{blum1989training,
  title={Training a 3-node neural network is NP-complete},
  author={Blum, Avrim and Rivest, Ronald L},
  booktitle={Advances in neural information processing systems},
  pages={494--501},
  year={1989}
}

#########gaussian##############





@article{zhong2017learning,
  title={Learning Non-overlapping Convolutional Neural Networks with Multiple Kernels},
  author={Zhong, Kai and Song, Zhao and Dhillon, Inderjit S},
  journal={arXiv preprint arXiv:1711.03440},
  year={2017}
}


#############independent assumption 



@inproceedings{du2018power,
  title={On the Power of Over-parametrization in Neural Networks with Quadratic Activation},
  author={Du, Simon S and Lee, Jason D},
  booktitle={International Conference on Machine Learning},
  pages={1328--1337},
  year={2018}
}


@inproceedings{allen2018convergence,
  title={A Convergence Theory for Deep Learning via Over-Parameterization},
  author={Allen-Zhu, Zeyuan and Li, Yuanzhi and Song, Zhao},
  booktitle={International Conference on Machine Learning},
  pages={242--252},
  year={2019}
}


@inproceedings{du2018gradientdeep,
  title={Gradient Descent Finds Global Minima of Deep Neural Networks},
  author={Du, Simon and Lee, Jason and Li, Haochuan and Wang, Liwei and Zhai, Xiyu},
  booktitle={International Conference on Machine Learning},
  pages={1675--1685},
  year={2019}
}


@inproceedings{bartlett2018gradient,
  title={Gradient descent with identity initialization efficiently learns positive definite linear transformations},
  author={Bartlett, Peter and Helmbold, Dave and Long, Phil},
  booktitle={International Conference on Machine Learning},
  pages={520--529},
  year={2018}
}

@inproceedings{langford2002not,
  title={(Not) bounding the true error},
  author={Langford, John and Caruana, Rich},
  booktitle={Advances in Neural Information Processing Systems},
  pages={809--816},
  year={2002}
}


###############generalization#################
@inproceedings{neyshabur2017pac,
  title={A pac-bayesian approach to spectrally-normalized margin bounds for neural networks},
  author={Neyshabur, Behnam and Bhojanapalli, Srinadh and McAllester, David and Srebro, Nathan},
  booktitle={International Conference on Learning Representation},
  year={2018}
}

@inproceedings{neyshabur2017exploring,
  title={Exploring generalization in deep learning},
  author={Neyshabur, Behnam and Bhojanapalli, Srinadh and McAllester, David and Srebro, Nati},
  booktitle={Advances in Neural Information Processing Systems},
  pages={5947--5956},
  year={2017}
}

@inproceedings{neyshabur2015norm,
  title={Norm-based capacity control in neural networks},
  author={Neyshabur, Behnam and Tomioka, Ryota and Srebro, Nathan},
  booktitle={Conference on Learning Theory},
  pages={1376--1401},
  year={2015}
}


@book{anthony2009neural,
  title={Neural network learning: Theoretical foundations},
  author={Anthony, Martin and Bartlett, Peter L},
  year={2009},
  publisher={cambridge university press}
}

@book{vapnik2013nature,
  title={The nature of statistical learning theory},
  author={Vapnik, Vladimir},
  year={2013},
  publisher={Springer science \& business media}
}

@inproceedings{golowich2017size,
  title={Size-Independent Sample Complexity of Neural Networks},
  author={Golowich, Noah and Rakhlin, Alexander and Shamir, Ohad},
  booktitle={Conference On Learning Theory},
  pages={297--299},
  year={2018}
}

@inproceedings{arora2018stronger,
  title={Stronger Generalization Bounds for Deep Nets via a Compression Approach},
  author={Arora, Sanjeev and Ge, Rong and Neyshabur, Behnam and Zhang, Yi},
  booktitle={International Conference on Machine Learning},
  pages={254--263},
  year={2018}
}

@article{arora2016understanding,
  title={Understanding deep neural networks with rectified linear units},
  author={Arora, Raman and Basu, Amitabh and Mianjy, Poorya and Mukherjee, Anirbit},
  journal={arXiv preprint arXiv:1611.01491},
  year={2016}
}


###########more citation#################
@article{yarotsky2018optimal,
  title={Optimal approximation of continuous functions by very deep {ReLU} networks},
  author={Yarotsky, Dmitry},
  journal={arXiv preprint arXiv:1802.03620},
  year={2018}
}

@article{hanin2017universal,
  title={Universal function approximation by deep neural nets with bounded width and relu activations},
  author={Hanin, Boris},
  journal={arXiv preprint arXiv:1708.02691},
  year={2017}
}

@article{hanin2017approximating,
  title={Approximating Continuous Functions by {ReLU} Nets of Minimal Width},
  author={Hanin, Boris and Sellke, Mark},
  journal={arXiv preprint arXiv:1710.11278},
  year={2017}
}


@inproceedings{allen2018rnn,
  title={On the Convergence Rate of Training Recurrent Neural Networks},
  author={Allen-Zhu, Zeyuan and Li, Yuanzhi and Song, Zhao},
  booktitle={Advances in Neural Information Processing Systems},
  year={2019}
}

@inproceedings{zhang2018learning,
  title={Learning One-hidden-layer {ReLU} Networks via Gradient Descent},
  author={Zhang, Xiao and Yu, Yaodong and Wang, Lingxiao and Gu, Quanquan},
  booktitle={The 22nd International Conference on Artificial Intelligence and Statistics},
  pages={1524--1534},
  year={2019}
}

@article{telgarsky2015representation,
  title={Representation benefits of deep feedforward networks},
  author={Telgarsky, Matus},
  journal={arXiv preprint arXiv:1509.08101},
  year={2015}
}


@inproceedings{allen2018learning,
  title={Learning and Generalization in Overparameterized Neural Networks, Going Beyond Two Layers},
  author={Allen-Zhu, Zeyuan and Li, Yuanzhi and Liang, Yingyu},
  booktitle={Advances in Neural Information Processing Systems},
  year={2019}
}


@inproceedings{nacson2018convergence,
  title={Convergence of Gradient Descent on Separable Data},
  author={Nacson, Mor Shpigel and Lee, Jason and Gunasekar, Suriya and Savarese, Pedro Henrique Pamplona and Srebro, Nathan and Soudry, Daniel},
  booktitle={The 22nd International Conference on Artificial Intelligence and Statistics},
  pages={3420--3428},
  year={2019}
}



@article{nag2018,
  title={Deterministic PAC-Bayesian generalization bounds for deep networks via generalizing noise-resilience. },
  author={Vaishnavh Nagarajan and J. Zico Kolter},
  journal={Integration of Deep Learning Theories, NeurIPS 2018},
  year={2018}
}

@article{minshuo2018,
  title={On Generalization Bounds for a Family of Recurrent Neural Networks. },
  author={Chen, Minshuo and Li, Xingguo and Zhao, Tuo},
  journal={Integration of Deep Learning Theories, NeurIPS 2018},
  year={2018}
}



@article{bousquet2002stability,
  title={Stability and generalization},
  author={Bousquet, Olivier and Elisseeff, Andr{\'e}},
  journal={Journal of machine learning research},
  volume={2},
  number={Mar},
  pages={499--526},
  year={2002}
}

@article{mou2017generalization,
  title={Generalization bounds of SGLD for non-convex learning: Two theoretical viewpoints},
  author={Mou, Wenlong and Wang, Liwei and Zhai, Xiyu and Zheng, Kai},
  journal={arXiv preprint arXiv:1707.05947},
  year={2017}
}

@inproceedings{hardt2016train,
  title={Train faster, generalize better: stability of stochastic gradient descent},
  author={Hardt, Moritz and Recht, Benjamin and Singer, Yoram},
  booktitle={Proceedings of the 33rd International Conference on International Conference on Machine Learning-Volume 48},
  pages={1225--1234},
  year={2016},
  organization={JMLR. org}
}

@article{chen2018stability,
  title={Stability and Convergence Trade-off of Iterative Optimization Algorithms},
  author={Chen, Yuansi and Jin, Chi and Yu, Bin},
  journal={arXiv preprint arXiv:1804.01619},
  year={2018}
}

@article{zhou2018generalization,
  title={Generalization Error Bounds with Probabilistic Guarantee for SGD in Nonconvex Optimization},
  author={Zhou, Yi and Liang, Yingbin and Zhang, Huishuai},
  journal={arXiv preprint arXiv:1802.06903},
  year={2018}
}

@inproceedings{chizat2018global,
  title={On the global convergence of gradient descent for over-parameterized models using optimal transport},
  author={Chizat, Lenaic and Bach, Francis},
  booktitle={Advances in neural information processing systems},
  pages={3036--3046},
  year={2018}
}


@inproceedings{arora2018optimization,
  title={On the Optimization of Deep Networks: Implicit Acceleration by Overparameterization},
  author={Arora, Sanjeev and Cohen, Nadav and Hazan, Elad},
  booktitle={International Conference on Machine Learning},
  pages={244--253},
  year={2018}
}


@article{zagoruyko2016wide,
  title={Wide residual networks},
  author={Zagoruyko, Sergey and Komodakis, Nikos},
  journal={arXiv preprint arXiv:1605.07146},
  year={2016}
}

@inproceedings{glorot2010understanding,
  title={Understanding the difficulty of training deep feedforward neural networks},
  author={Glorot, Xavier and Bengio, Yoshua},
  booktitle={Proceedings of the thirteenth international conference on artificial intelligence and statistics},
  pages={249--256},
  year={2010}
}




@inproceedings{dziugaite2017computing,
  title={Computing nonvacuous generalization bounds for deep (stochastic) neural networks with many more parameters than training data},
  author={Dziugaite, Gintare Karolina and Roy, Daniel M},
  booktitle={Uncertainty in Artificial Intelligence},
  year={2017}
}

@inproceedings{gunasekar2018implicit,
  title={Implicit bias of gradient descent on linear convolutional networks},
  author={Gunasekar, Suriya and Lee, Jason D and Soudry, Daniel and Srebro, Nati},
  booktitle={Advances in Neural Information Processing Systems},
  pages={9461--9471},
  year={2018}
}


@inproceedings{gunasekar2018characterizing,
  title={Characterizing Implicit Bias in Terms of Optimization Geometry},
  author={Gunasekar, Suriya and Lee, Jason and Soudry, Daniel and Srebro, Nathan},
  booktitle={International Conference on Machine Learning},
  pages={1827--1836},
  year={2018}
}


@article{liang2018just,
  title={Just Interpolate: Kernel" Ridgeless" Regression Can Generalize},
  author={Liang, Tengyuan and Rakhlin, Alexander},
  journal={arXiv preprint arXiv:1808.00387},
  year={2018}
}



@article{mei2018mean,
  title={A mean field view of the landscape of two-layer neural networks},
  author={Mei, Song and Montanari, Andrea and Nguyen, Phan-Minh},
  journal={Proceedings of the National Academy of Sciences},
  volume={115},
  number={33},
  pages={E7665--E7671},
  year={2018},
  publisher={National Acad Sciences}
}

@inproceedings{li2018algorithmic,
  title={Algorithmic regularization in over-parameterized matrix sensing and neural networks with quadratic activations},
  author={Li, Yuanzhi and Ma, Tengyu and Zhang, Hongyang},
  booktitle={Conference On Learning Theory},
  pages={2--47},
  year={2018}
}

@article{belkin2018understand,
  title={To understand deep learning we need to understand kernel learning},
  author={Belkin, Mikhail and Ma, Siyuan and Mandal, Soumik},
  journal={arXiv preprint arXiv:1802.01396},
  year={2018}
}

@inproceedings{gunasekar2017implicit,
  title={Implicit regularization in matrix factorization},
  author={Gunasekar, Suriya and Woodworth, Blake E and Bhojanapalli, Srinadh and Neyshabur, Behnam and Srebro, Nati},
  booktitle={Advances in Neural Information Processing Systems},
  pages={6151--6159},
  year={2017}
}



@inproceedings{arora2019fine,
  title={Fine-Grained Analysis of Optimization and Generalization for Overparameterized Two-Layer Neural Networks},
  author={Arora, Sanjeev and Du, Simon and Hu, Wei and Li, Zhiyuan and Wang, Ruosong},
  booktitle={International Conference on Machine Learning},
  pages={322--332},
  year={2019}
}

@inproceedings{soltanolkotabi2017learning,
  title={Learning {ReLUs} via gradient descent},
  author={Soltanolkotabi, Mahdi},
  booktitle={Advances in Neural Information Processing Systems},
  pages={2007--2017},
  year={2017}
}


@inproceedings{daniely2017sgd,
  title={SGD learns the conjugate kernel class of the network},
  author={Daniely, Amit},
  booktitle={Advances in Neural Information Processing Systems},
  pages={2422--2430},
  year={2017}
}



@article{wei2018regularization,
  title={Regularization Matters: Generalization and Optimization of Neural Nets v.s. their Induced Kernel},
  author={Wei, Colin and Lee, Jason D and Liu, Qiang and Ma, Tengyu},
  journal={Advances in Neural Information Processing Systems},
  year={2019}
}

@article{soudry2017exponentially,
  title={Exponentially vanishing sub-optimal local minima in multilayer neural networks},
  author={Soudry, Daniel and Hoffer, Elad},
  journal={arXiv preprint arXiv:1702.05777},
  year={2017}
}

@article{zhou2017critical,
  title={Critical Points of Neural Networks: Analytical Forms and Landscape Properties},
  author={Zhou, Yi and Liang, Yingbin},
  journal={arXiv preprint arXiv:1710.11205},
  year={2017}
}

@article{yun2018critical,
  title={A Critical View of Global Optimality in Deep Learning},
  author={Yun, Chulhee and Sra, Suvrit and Jadbabaie, Ali},
  journal={arXiv preprint arXiv:1802.03487},
  year={2018}
}

@article{sirignano2018mean,
  title={Mean field analysis of neural networks: A central limit theorem},
  author={Sirignano, Justin and Spiliopoulos, Konstantinos},
  journal={arXiv preprint arXiv:1808.09372},
  year={2018}
}

@article{yun2018small,
  title={Small nonlinearities in activation functions create bad local minima in neural networks},
  author={Yun, Chulhee and Sra, Suvrit and Jadbabaie, Ali},
  year={2018}
}

@inproceedings{lin2018resnet,
  title={{ResNet} with one-neuron hidden layers is a Universal Approximator},
  author={Lin, Hongzhou and Jegelka, Stefanie},
  booktitle={Advances in Neural Information Processing Systems},
  pages={6172--6181},
  year={2018}
}

@inproceedings{he2016deep,
  title={Deep residual learning for image recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={770--778},
  year={2016}
}


@inproceedings{jacot2018neural,
  title={Neural tangent kernel: Convergence and generalization in neural networks},
  author={Jacot, Arthur and Gabriel, Franck and Hongler, Cl{\'e}ment},
  booktitle={Advances in neural information processing systems},
  pages={8571--8580},
  year={2018}
}

@inproceedings{chizat2018note,
  title={On Lazy Training in Differentiable Programming},
  author={Chizat, Lenaic and Oyallon, Edouard and Bach, Francis},
  booktitle={Advances in Neural Information Processing Systems},
  year={2019}
}








@inproceedings{rahimi2009weighted,
  title={Weighted sums of random kitchen sinks: Replacing minimization with randomization in learning},
  author={Rahimi, Ali and Recht, Benjamin},
  booktitle={Advances in neural information processing systems},
  pages={1313--1320},
  year={2009}
}

@inproceedings{mendelson2014learning,
  title={Learning without concentration},
  author={Mendelson, Shahar},
  booktitle={Conference on Learning Theory},
  pages={25--39},
  year={2014}
}





@inproceedings{maurer2016vector,
  title={A vector-contraction inequality for rademacher complexities},
  author={Maurer, Andreas},
  booktitle={International Conference on Algorithmic Learning Theory},
  pages={3--17},
  year={2016},
  organization={Springer}
}



@article{cesa2004generalization,
  title={On the generalization ability of on-line learning algorithms},
  author={Cesa-Bianchi, Nicolo and Conconi, Alex and Gentile, Claudio},
  journal={IEEE Transactions on Information Theory},
  volume={50},
  number={9},
  pages={2050--2057},
  year={2004},
  publisher={IEEE}
}




@inproceedings{yehudai2019power,
  title={On the Power and Limitations of Random Features for Understanding Neural Networks},
  author={Yehudai, Gilad and Shamir, Ohad},
  booktitle={Advances in Neural Information Processing Systems},
  year={2019}
}




@article{e2019comparative,
  title={A Comparative Analysis of the Optimization and Generalization Property of Two-layer Neural Network and Random Feature Models Under Gradient Descent Dynamics},
  author={E, Weinan and Ma, Chao and Wu, Lei and others},
  journal={arXiv preprint arXiv:1904.04326},
  year={2019}
}



@inproceedings{rahimi2008random,
  title={Random features for large-scale kernel machines},
  author={Rahimi, Ali and Recht, Benjamin},
  booktitle={Advances in neural information processing systems},
  pages={1177--1184},
  year={2008}
}


@article{oymak2019towards,
  title={Towards moderate overparameterization: global convergence guarantees for training shallow neural networks},
  author={Oymak, Samet and Soltanolkotabi, Mahdi},
  journal={arXiv preprint arXiv:1902.04674},
  year={2019}
}

@inproceedings{neyshabur2018role,
  title={Towards Understanding the Role of Over-Parametrization in Generalization of Neural Networks},
  author={Neyshabur, Behnam and Li, Zhiyuan and Bhojanapalli, Srinadh and LeCun, Yann and Srebro, Nathan},
  booktitle={International Conference on Learning Representations},
  year={2019}
}


@inproceedings{arora2019exact,
  title={On exact computation with an infinitely wide neural net},
  author={Arora, Sanjeev and Du, Simon S and Hu, Wei and Li, Zhiyuan and Salakhutdinov, Ruslan and Wang, Ruosong},
  booktitle={Advances in Neural Information Processing Systems},
  year={2019}
}

@inproceedings{lee2019wide,
  title={Wide neural networks of any depth evolve as linear models under gradient descent},
  author={Lee, Jaehoon and Xiao, Lechao and Schoenholz, Samuel S and Bahri, Yasaman and Sohl-Dickstein, Jascha and Pennington, Jeffrey},
  booktitle={Advances in Neural Information Processing Systems},
  year={2019}
}


@article{long2019size,
  title={Size-free generalization bounds for convolutional neural networks},
  author={Long, Philip M and Sedghi, Hanie},
  journal={arXiv preprint arXiv:1905.12600},
  year={2019}
}

@inproceedings{jain2019making,
  title={Making the Last Iterate of SGD Information Theoretically Optimal},
  author={Jain, Prateek and Nagaraj, Dheeraj and Netrapalli, Praneeth},
  booktitle={Conference on Learning Theory},
  year={2019}
}


@article{lecun1998gradient,
  title={Gradient-based learning applied to document recognition},
  author={LeCun, Yann and Bottou, L{\'e}on and Bengio, Yoshua and Haffner, Patrick and others},
  journal={Proceedings of the IEEE},
  volume={86},
  number={11},
  pages={2278--2324},
  year={1998},
  publisher={Taipei, Taiwan}
}

@inproceedings{zou2019improved,
  title={An Improved Analysis of Training Over-parameterized Deep Neural Networks},
  author={Zou, Difan and Gu, Quanquan},
  booktitle={Advances in Neural Information Processing Systems},
  year={2019}
}


@inproceedings{cao2019generalizationsgd,
  title={Generalization Bounds of Stochastic Gradient Descent for Wide and Deep Neural Networks},
  author={Cao, Yuan and Gu, Quanquan},
  booktitle={Advances in Neural Information Processing Systems},
  year={2019}
}




@article{fang2019convexformulation,
  title={Convex Formulation of Overparameterized Deep Neural Networks},
  author={Fang, Cong and Gu, Yihong and Zhang, Weizhong and Zhang, Tong},
  journal={arXiv preprint arXiv:1911.07626},
  year={2019}
}


@inproceedings{nguyen2019connected,
  title={On Connected Sublevel Sets in Deep Learning},
  author={Nguyen, Quynh},
  booktitle={International Conference on Machine Learning},
  pages={4790--4799},
  year={2019}
}


@article{zou2018stochastic,
  title={Stochastic gradient descent optimizes over-parameterized deep relu networks},
  author={Zou, Difan and Cao, Yuan and Zhou, Dongruo and Gu, Quanquan},
  journal={arXiv preprint arXiv:1811.08888},
  year={2018}
}



@inproceedings{frei2019algorithm,
  title={Algorithm-Dependent Generalization Bounds for Overparameterized Deep Residual Networks},
  author={Frei, Spencer and Cao, Yuan and Gu, Quanquan},
  booktitle={Advances in Neural Information Processing Systems},
  pages={14769--14779},
  year={2019}
}



@article{sirignano2019mean,
  title={Mean field analysis of neural networks: A central limit theorem},
  author={Sirignano, Justin and Spiliopoulos, Konstantinos},
  journal={Stochastic Processes and their Applications},
  year={2019},
  publisher={Elsevier}
}




@inproceedings{cao2019generalization,
  title={Generalization Error Bounds of Gradient Descent for Learning Over-parameterized Deep ReLU Networks},
  author={Cao, Yuan and Gu, Quanquan},
  booktitle={the Thirty-Fourth AAAI Conference on Artificial Intelligence},
  year={2020}
}




@inproceedings{bietti2019inductive,
  title={On the inductive bias of neural tangent kernels},
  author={Bietti, Alberto and Mairal, Julien},
  booktitle={Advances in Neural Information Processing Systems},
  year={2019}
}




@inproceedings{basri2019convergence,
  title={The Convergence Rate of Neural Networks for Learned Functions of Different Frequencies},
  author={Basri, Ronen and Jacobs, David and Kasten, Yoni and Kritchman, Shira},
  booktitle={Advances in Neural Information Processing Systems},
  year={2019}
}




@inproceedings{nakkiran2019sgd,
  title={SGD on Neural Networks Learns Functions of Increasing Complexity},
  author={Nakkiran, Preetum and Kaplun, Gal and Kalimeris, Dimitris and Yang, Tristan and Edelman, Benjamin L and Zhang, Fred and Barak, Boaz},
  booktitle={Advances in Neural Information Processing Systems},
  year={2019}
}


@inproceedings{rahaman2019spectral,
  title={On the Spectral Bias of Neural Networks},
  author={Rahaman, Nasim and Baratin, Aristide and Arpit, Devansh and Draxler, Felix and Lin, Min and Hamprecht, Fred and Bengio, Yoshua and Courville, Aaron},
  booktitle={International Conference on Machine Learning},
  pages={5301--5310},
  year={2019}
}



@inproceedings{vempala2018gradient,
  title={Gradient Descent for One-Hidden-Layer Neural Networks: Polynomial Convergence and SQ Lower Bounds},
  author={Vempala, Santosh and Wilmes, John},
  booktitle={Conference on Learning Theory},
  year={2019}
}



@inproceedings{su2019learning,
  title={On Learning Over-parameterized Neural Networks: A Functional Approximation Prospective},
  author={Su, Lili and Yang, Pengkun},
  booktitle={Advances in Neural Information Processing Systems},
  year={2019}
}


@inproceedings{nagarajan2019uniform,
  title={Uniform convergence may be unable to explain generalization in deep learning},
  author={Nagarajan, Vaishnavh and Kolter, J Zico},
  booktitle={Advances in Neural Information Processing Systems},
  pages={11611--11622},
  year={2019}
}




@inproceedings{chen2019much,
  title={How Much Over-parameterization Is Sufficient to Learn Deep ReLU Networks?},
  author={Chen, Zixiang and Cao, Yuan and Zou, Difan and Gu, Quanquan},
  booktitle={International Conference on Learning Representations},
  year={2021}
}



@article{allen2020backward,
  title={Backward feature correction: How deep learning performs deep learning},
  author={Allen-Zhu, Zeyuan and Li, Yuanzhi},
  journal={arXiv preprint arXiv:2001.04413},
  year={2020}
}


@inproceedings{li2020learning,
  title={Learning over-parametrized two-layer neural networks beyond ntk},
  author={Li, Yuanzhi and Ma, Tengyu and Zhang, Hongyang R},
  booktitle={Conference on Learning Theory},
  pages={2613--2682},
  year={2020},
  organization={PMLR}
}







@article{allen2020feature,
  title={Feature purification: How adversarial training performs robust deep learning},
  author={Allen-Zhu, Zeyuan and Li, Yuanzhi},
  journal={arXiv preprint arXiv:2005.10190},
  year={2020}
}








@article{papyan2017convolutional,
  title={Convolutional neural networks analyzed via convolutional sparse coding},
  author={Papyan, Vardan and Romano, Yaniv and Elad, Michael},
  journal={The Journal of Machine Learning Research},
  volume={18},
  number={1},
  pages={2887--2938},
  year={2017},
  publisher={JMLR. org}
}






@article{reddi2019convergence,
  title={On the convergence of adam and beyond},
  author={Reddi, Sashank J and Kale, Satyen and Kumar, Sanjiv},
  journal={arXiv preprint arXiv:1904.09237},
  year={2019}
}



