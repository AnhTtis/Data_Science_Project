\section{Mixup data}


\subsection{Characterization of the mixup dataset}


\paragraph{Category of different Mixup data patches.}
First recall the category of different Mixup training data points:
\begin{itemize}
    \item Mix between two common feature data points, including $\cS_{0,0}^{+, +}$, $\cS_{0,0}^{-, -}$, $\cS_{0,0}^{+, -}$, $\cS_{0,0}^{-, +}$, each of them is of size $\Theta(n^2)$.
    \item Mix between  common feature and rare feature data points with the same label, including  $\cS_{0,1}^{+, +}$, $\cS_{0,1}^{-, -}$, $\cS_{1,0}^{+,+}$, and $\cS_{1,0}^{-,-}$, each of them is of size $\Theta(\rho n^2)$.
    \item Mix between common feature  and rare feature data points with different labels, including  $\cS_{0,1}^{+, -}$, $\cS_{0,1}^{-, +}$, $\cS_{1, 0}^{+, -}$, and $\cS_{1, 0}^{-, +}$, each of them is of size $\Theta(\rho n^2)$.
    \item Mix between two rare feature data points, including $\cS_{1,1}^{+, +}$,$\cS_{1,1}^{-, -}$, $\cS_{1,1}^{+, -}$ and $\cS_{1, 1}^{-, +}$, each of them is of size $\Theta(\rho^2 n^2)$.
    
\end{itemize}


% In particular, note that we have two types of data: common feature data and rare feature data, while each of them consists of two labels. Let $\cS_0^+$, $\cS_0^-$, $\cS_1^+$, and $\cS_1^-$ denote the classes of positive common feature data (the data consisting of common feature $\vb$), negative common feature data (the data consisting of common feature $\ub$),  positive rare feature data (the data consisting of common feature $\vb'$),  and negative rare feature data (the data consisting of common feature $\ub'$), respectively. Then, let $\cS_{*,**}^{\dagger, \dagger\dagger}$ be the set of mixed data $\xb_{i,j}=\lambda\xb_i+(1-\lambda)\xb_j$ with $\xb_i\in\cS_*^\dagger$ and $\xb_j\in\cS_{**}^{\dagger\dagger}$, we can accordingly categorize all mixup data with the following six classes:
% \begin{itemize}
%     \item two strong data with the same label, including $\cS_{0,0}^{+, +}$ and $\cS_{0,0}^{-, -}$;
%     \item two strong data with different labels, denoted as  $\cS_{0,0}^{+, -}$ and $\cS_{0, 0}^{-, +}$;
%     \item one strong data and one weak data with the same label, including  $\cS_{0,1}^{+, +}$, $\cS_{0,1}^{-, -}$, $\cS_{1,0}^{+,+}$, and $\cS_{1,0}^{-,-}$;
%     \item one strong data and one weak data with different labels, including  $\cS_{0,1}^{+, -}$, $\cS_{0,1}^{-, +}$, $\cS_{1, 0}^{+, -}$, and $\cS_{1, 0}^{-, +}$;
%     \item two weak data with the same label, including $\cS_{1,1}^{+, +}$ and $\cS_{1,1}^{-, -}$;
%     \item two weak data with different labels, including  $\cS_{1,1}^{+, -}$ and $\cS_{1, 1}^{-, +}$.
% \end{itemize}
% Note that we have $\Theta(n)$ strong data and $\Theta(\rho n)$ weak data in the original training dataset. The following lemma characterizes the size of each class of mixup data.
% \begin{proposition}
% Assuming the data is generated according to Definition \ref{def:data_distribution_new}, then it holds that
% \begin{align*}
% &|\cS_{0,0}^{+,+}|,   |\cS_{0,0}^{-,-}|, |\cS_{0,0}^{+,-}|= \Theta(n^2);\notag\\
% & |\cS_{0,1}^{+,+}|, |\cS_{0,1}^{-,-}|, |\cS_{0,1}^{+,-}|, |\cS_{0,1}^{-,+}| = \Theta(\rho n^2);\notag\\
% &|\cS_{1,1}^{+,+}|,   |\cS_{1,1}^{-,-}|,  |\cS_{0,0}^{+,-}|= \Theta(\rho^2n^2).
% \end{align*}
% \end{proposition}


% \paragraph{Category of different mixup data patches}
Then, given $n^2$ mixed data points, we have in total $n^2P$ data patches. Besides, note that in the original dataset that consists of $n$ training data points, each data patch $\xb_i^{(p)}$ satisfies
\begin{align*}
\xb_i^{(p)}\in\big\{\vb,\ub, \alpha \ub, \alpha \vb, \vb', \ub', \bxi_i^{(p)}\big\}.
\end{align*}
Moreover, by the data distribution defined in Definition \ref{def:data_distribution_new}, we have
\begin{itemize}
\item $\vb$ and $\ub$ will appear in $\Theta(n)$ data and $\Theta(n)$ data patches.
\item $\alpha\vb$ and $\alpha\ub$ will appear in $n$ data and $\Theta(b n)$ data patches.
\item $\vb'$ and $\ub'$ will appear in $\Theta(\rho n)$ data and $\Theta(\rho n)$ data patches.
\item $\bxi_i^{(p)}$, if it is not zero, will appear in one data and one data patch.
\end{itemize}

Then based on the above facts, we provide the following lemma that characterizes the number of different types of data patches on the mixup dataset.

\begin{lemma}\label{lemma:occurance_mixed_patch}
Let $\cP:=\{\xb_{i,j}^{(p)}\}_{i,j\in[n], p\in[P]}$ be the collection of all data patches of the mixup dataset, then among these $n^2P$ data patches, with probability at least $1-1/\poly(n)$, let $\xb_{i,j}^{(p)}=\lambda\ab+(1-\lambda)\bb$, we have
\begin{itemize}
\item 
The vector with $\ab\in\{\vb,\ub\}$ and $\bb\in\{\vb,\ub\}$
will appear in $\Theta(n^2/P)$ data patches.
\item The vector with $\ab\in\{\vb,\ub\}$ and $\bb\in\{\vb',\ub'\}$   will appear in $\Theta(\rho n^2/P)$ patches.
\item The vector with $\ab\in\{\vb,\ub\}$ and $\bb\in\{\alpha\vb,\alpha\ub\}$ will appear in $O(bn^2/P)$ patches.
\item The vector with $\ab\in\{\vb,\ub\}$ and $\bb\in\{\bxi\}$ will appear in $\Theta(n^2)$ patches.
\item The vector with $\ab\in\{\vb',\alpha\ub'\}$ and $\bb\in\{\vb',\ub'\}$ will appear in $\Theta(\rho^2n^2/P)$ data patches.
\item The vector with $\ab\in\{\vb',\alpha\ub'\}$ and $\bb\in\{\alpha\vb,\alpha\ub\}$ will appear in $O(\rho bn^2/P)$ patches.
\item The vector with $\ab\in\{\vb',\alpha\ub'\}$ and $\bb\in\{\bxi\}$ will appear in $\Theta(\rho n^2)$ patches.
\item The vector with $\ab\in\{\alpha\vb,\alpha\ub\}$ and $\bb\in\{\alpha\vb,\alpha\ub\}$ will appear in $O(b^2n^2/P)$ patches.
\item The vector with $\ab\in\{\alpha\vb,\alpha\ub\}$ and $\bb\in\{\bxi\}$  will appear in $O(bn^2)$ patches.

\end{itemize}
Besides, regarding any non-zero noise vector $\bxi_i^{(p)}$, we have, among the collection of data patches $\{\xb_{i,j}^{(p)}\}_{j\in[n]}$, with probability at least $1-1/\poly(n)$, 
\begin{itemize}
\item $\xb_{i,j}^{(p)}=\lambda \bxi_{i,j}^{(p)}+(1-\lambda)\bb$ with $\bb\in\{\vb,\ub\}$ will appear in $\Theta(n/P)$ patches.
\item $\xb_{i,j}^{(p)}=\lambda \bxi_{i,j}^{(p)}+(1-\lambda)\bb$ with $\bb\in\{\alpha\vb,\alpha\ub\}$ will appear in $O(bn/P)$ patches.
\item $\xb_{i,j}^{(p)}=\lambda \bxi_{i,j}^{(p)}+(1-\lambda)\bb$ with $\bb\in\{\vb',\ub'\}$ will appear in $\Theta(\rho n/P)$ patches.
\item $\xb_{i,j}^{(p)}=\lambda \bxi_{i,j}^{(p)}+(1-\lambda)\bb$ with $\bb\in\{\bxi\}$ will appear in $\Theta(n)$ patches.
\end{itemize}

\end{lemma}
\begin{proof}[Proof of Lemma \ref{lemma:occurance_mixed_patch}]
We first consider a fixed $\xb_i$ and the corresponding collection of data patches $\{\xb_{i,j}^{(p)}\}_{j\in[n], p\in[P]}$.  Then by Definition \ref{def:data_distribution_new}, conditioning on $\xb_i^{(p)} = \vb$, we have for any $j\neq i$
\begin{align*}
\PP[\xb_j^{(p)}=\vb|\xb_i^{(p)} = \vb] =  \PP[\xb_j^{(p)}=\vb] = \Theta\bigg(\frac{1}{P}\bigg).
\end{align*}
Therefore, we can further get that conditioning on $\xb_{i}^{(p)}=\vb$, the summation $\sum_{j\neq i}\ind[\xb_{i,j}^{(p)}=\vb]$ follows Binomial distribution $\mathrm{Binom}(n-1, p)$ with probability parameter $p=\Theta(1/P)$. Then by Hoeffding's inequality, we can get that with probability at least $1-\exp(-n^2/P^2)$, it holds that 
\begin{align*}
\sum_{j\in[n]}\ind[\xb_j^{(p)}=\vb|\xb_i^{(p)}=\vb] = \Theta\bigg(\frac{n}{P}\bigg).
\end{align*}
Note that we have at least $\Theta(n)$ number of $\xb_i$'s that consist of the common feature vector $\vb$, then applying union bound over these $\xb_i$'s, we can further get with probability at least $1-1/\poly(n)$, it holds that
\begin{align*}
\sum_{i,j\in[n]}\sum_{p\in[P]}\ind[\xb_{i,j}^{(p)}=\vb]& \ge \sum_{i,j\in[n]}\ind[\xb_{i,j}^{(p_i)}=\vb|\xb_i^{(p_i)}=\vb]\cdot \ind[\xb_i^{(p_i)}=\vb]\notag\\
&\ge \Theta(n)\cdot\Theta\bigg(\frac{n}{P}\bigg)\notag\\
& = \Theta\bigg(\frac{n^2}{P}\bigg).
\end{align*}
Here we define $p_i$ as the index of the data patch that is $\vb$ if the data $\xb_i$ has such a common feature vector, otherwise, $p_i$ is arbitrarily chosen.
On the other hand, we can also get
\begin{align*}
\sum_{i,j\in[n]}\sum_{p\in[P]}\ind[\xb_{i,j}^{(p)}=\vb]\le \sum_{i,j\in[n]}\sum_{p\in[P]}\ind[\xb_{i,j}^{(p)}=\vb|\xb_i^{(p)}=\vb]\cdot \ind[\xb_i^{(p)}=\vb]\le n\cdot \Theta(1)\cdot\Theta\bigg(\frac{n}{P}\bigg)=\Theta\bigg(\frac{n^2}{P}\bigg),
\end{align*}
where the second inequality is due to that each data will have at most $\Theta(1)$ patches being $\vb$. Similarly, we can also prove the same results for the case of $\xb_{i,j}^{(p)} = \lambda\ab + (1-\lambda)\bb$ with $\ab,\bb\in\{\ub,\vb\}$.

The proof for the case of $\xb_{i,j}^{(p)} = \lambda\ab + (1-\lambda)\bb$ with $\ab\in\{\ub,\vb\}$ and $\bb\in\{\alpha\vb,\alpha\ub\}$ will be also similar, the only difference is that conditioning on $\xb_i^{(p)}=\vb$, the probability of $\xb_j^{(p)}=\alpha\vb$
or $\xb_j^{(p)}=\alpha\ub$ will be $O(b/P)$. Finally, we can get that (here we take $\ab=\vb$ and $\bb=\vb$ as an example)
\begin{align*}
\sum_{i,j\in[n]}\sum_{p\in[P]}\ind[\xb_{i,j}^{(p)}=\lambda\vb + \alpha(1-\lambda)\vb] =\Theta(n)\cdot\Theta\bigg(\frac{bn}{P}\bigg)=\Theta\bigg(\frac{bn^2}{P}\bigg).
\end{align*}

The proof for the case of $\xb_{i,j}^{(p)} = \lambda\ab + (1-\lambda)\bb$  with $\ab\in\{\ub,\vb\}$ and $\bb\in\{\ub',\vb'\}$ will also be similar, where we only need to use the fact that $\PP[\xb_j^{(p)}=\vb'|\xb_i^{(p)} = \vb]=\Theta(\rho/P)$. Here we take $\ab=\vb$ and $\bb=\vb'$ as an example.

Regarding the case of $\xb_{i,j}^{(p)} = \lambda\ab + (1-\lambda)\bb$ with $\ab\in\{\ub,\vb\}$ and $\bb\in\{\bxi\}$, we only need to use the fact that $\PP[\xb_{i,j}^{(p)}=\lambda\vb+(1-\lambda)\bxi_j^{(p)}|\xb_i^{(p)} = \vb]=\Theta(1)$, where we take $\ab=\vb$ as an example. Then the desired result can be proved in a similar way.


 When $\ab\in\{\vb', \ub'\}$ we will also need to use the fact that we have in total $\Theta(\rho n)$ number of $\xb_i$'s that consist of $\vb'$ or $\ub'$. Then take $\ab=\vb'$ and $\bb=\vb'$ as an example, conditioning on $\xb_i^{(p)}=\vb'$, we have for any $j\neq i$
\begin{align*}
\PP[\xb_j^{(p)}=\vb'|\xb_i^{(p)} = \vb'] =  \PP[\xb_j^{(p)}=\vb'] = \Theta\bigg(\frac{\rho}{P}\bigg).
\end{align*} 
Therefore, we can get that with probability at least $1-1/\poly(n)$,
\begin{align*}
\sum_{j\in[n]}\ind[\xb_j^{(p)}=\vb'|\xb_i^{(p)}=\vb'] = \Theta\bigg(\frac{\rho n}{P}\bigg).
\end{align*}
Accordingly, we can further obtain
\begin{align*}
\sum_{i,j\in[n]}\sum_{p\in[P]}\ind[\xb_{i,j}^{(p)}=\vb']= \sum_{i,j\in[n]}\sum_{p\in[P]}\ind[\xb_{i,j}^{(p)}=\vb'|\xb_i^{(p)}=\vb']\cdot \ind[\xb_i^{(p)}=\vb']=\Theta(\rho n)\cdot \Theta\bigg(\frac{\rho n}{P}\bigg) = \Theta\bigg(\frac{\rho^2 n^2}{P}\bigg).
\end{align*}

The proof for the case of $\xb_{i,j}^{(p)} = \lambda\ab + (1-\lambda)\bb$  with $\ab\in\{\ub',\vb'\}$ and $\bb\in\{\alpha\vb,\alpha\vb\}$ or  $\bb\in\{\bxi\}$ will also be similar, where we only need to use the fact that $\PP[\xb_j^{(p)}=\alpha\vb|\xb_i^{(p)} = \vb']=O(b/P)$ and $\PP[\xb_j^{(p)}=\alpha\bxi_j^{(p)}|\xb_i^{(p)} = \vb']=\Theta(1)$.

When $\ab\in\{\alpha\vb, \alpha\ub\}$ we only need to use the fact that we have in total $\Theta(n)$ number of $\xb_i$'s that consist of $\Theta(b)$ number of $\vb'$ or $\ub'$. The remaining proof will be similar to previous ones based on the fact that $\PP[\xb_j^{(p)}=\alpha\vb|\xb_i^{(p)} = \alpha\vb]=O(b/P)$ and $\PP[\xb_j^{(p)}=\alpha\bxi_j^{(p)}|\xb_i^{(p)} = \alpha\vb]=\Theta(1)$, where we take $\ab=\alpha\vb$ and $\bb=\alpha\vb$ as an example.


Lastly, we will move on to the case of $\ab = \bxi_i^{(p)}$. In this case, we only need to use the facts that for any $j\neq i$,
\begin{align*}
\PP[\xb_j^{(p)}=\vb|\xb_i^{(p)}= \bxi_i^{(p)}]&=\PP[\xb_j^{(p)}=\vb] = \Theta(1/P)\notag\\
\PP[\xb_j^{(p)}=\ub|\xb_i^{(p)} = \bxi_i^{(p)}]&=\PP[\xb_j^{(p)}=\ub] = \Theta(1/P)\notag\\
\PP[\xb_j^{(p)}=\vb'|\xb_i^{(p)} = \bxi_i^{(p)}]&=\PP[\xb_j^{(p)}=\vb'] = \Theta(\rho/P) \notag\\
\PP[\xb_j^{(p)}=\ub'|\xb_i^{(p)} = \bxi_i^{(p)}]&=\PP[\xb_j^{(p)}=\ub'] = \Theta(\rho/P)\notag\\
\PP[\xb_j^{(p)}=\alpha\vb|\xb_i^{(p)} = \bxi_i^{(p)}]&=\PP[\xb_j^{(p)}=\alpha\vb] = O(b/P)\notag\\
\PP[\xb_j^{(p)}=\alpha\ub|\xb_i^{(p)} = \bxi_i^{(p)}]&=\PP[\xb_j^{(p)}=\alpha\ub] = O(b/P)\notag\\
\PP[\xb_j^{(p)}\in\{\bxi\}|\xb_i^{(p)} = \bxi_i^{(p)}]&=\PP[\xb_j^{(p)}\in\{\bxi\}] = \Theta(1).
\end{align*}
Then applying the standard concentration argument for binomial distribution yields the desired results.


\end{proof}
















% Moreover, note that the feature patches are uniformly assigned to $\Theta(1)$ patches. Then the data patch of the mixup data can be: mixture of feature vectors, mixture of feature and noise vectors, or mixture of noise vectors. In particular, let $\tilde \xb_{i,j}$ be a mixup data, then 
% \begin{itemize}
%     \item For $(i,j)\in\cS_{0,0}^{+, +}$, we have the data patch $\xb_{i,j}^{(p)}$ satisfies 
%     \begin{align*}
%     \xb_{i,j}^{(p)}\in\Big\{ \lambda \ab + (1-\lambda) \bb: \ab\in\{\vb, \alpha\vb, \alpha\ub, \bxi_i^{(p)}\}, \bb\in\{\vb, \alpha\vb, \alpha\ub, \bxi_j^{(p)}\} \Big\}
%     \end{align*}
%     \item For $(i,j)\in\cS_{0,0}^{-, -}$, we have the data patch $\xb_{i,j}^{(p)}$ satisfies 
%     \begin{align*}
%     \xb_{i,j}^{(p)}\in\Big\{ \lambda \ab + (1-\lambda) \bb: \ab\in\{\ub, \alpha\vb, \alpha\ub, \bxi_i^{(p)}\}, \bb\in\{\ub, \alpha\vb, \alpha\ub, \bxi_j^{(p)}\} \Big\}
%     \end{align*}
%     \item For $(i,j)\in\cS_{0,0}^{+, -}$, we have the data patch $\xb_{i,j}^{(p)}$ satisfies 
%     \begin{align*}
%     \xb_{i,j}^{(p)}\in\Big\{ \lambda \ab + (1-\lambda) \bb: \ab\in\{\vb, \alpha\vb, \alpha\ub, \bxi_i^{(p)}\}, \bb\in\{\ub, \alpha\vb, \alpha\ub, \bxi_j^{(p)}\} \Big\}
%     \end{align*}
%     \item For $(i,j)\in\cS_{0,1}^{+, +}$, we have the data patch $\xb_{i,j}^{(p)}$ satisfies 
%     \begin{align*}
%     \xb_{i,j}^{(p)}\in\Big\{ \lambda \ab + (1-\lambda) \bb: \ab\in\{\vb, \alpha\vb, \alpha\ub, \bxi_i^{(p)}\}, \bb\in\{\vb', \alpha\vb, \alpha\ub, \bxi_j^{(p)}\} \Big\}
%     \end{align*}
%     \item For $(i,j)\in\cS_{0,1}^{-, -}$, we have the data patch $\xb_{i,j}^{(p)}$ satisfies 
%     \begin{align*}
%     \xb_{i,j}^{(p)}\in\Big\{ \lambda \ab + (1-\lambda) \bb: \ab\in\{\ub, \alpha\vb, \alpha\ub, \bxi_i^{(p)}\}, \bb\in\{\ub', \alpha\vb, \alpha\ub, \bxi_j^{(p)}\} \Big\}
%     \end{align*}
%     \item For $(i,j)\in\cS_{0,1}^{+, -}$, we have the data patch $\xb_{i,j}^{(p)}$ satisfies 
%     \begin{align*}
%     \xb_{i,j}^{(p)}\in\Big\{ \lambda \ab + (1-\lambda) \bb: \ab\in\{\vb, \alpha\vb, \alpha\ub, \bxi_i^{(p)}\}, \bb\in\{\ub', \alpha\vb, \alpha\ub, \bxi_j^{(p)}\} \Big\}
%     \end{align*}
%     \item For $(i,j)\in\cS_{0,1}^{-, +}$, we have the data patch $\xb_{i,j}^{(p)}$ satisfies 
%     \begin{align*}
%     \xb_{i,j}^{(p)}\in\Big\{ \lambda \ab + (1-\lambda) \bb: \ab\in\{\ub, \alpha\vb, \alpha\ub, \bxi_i^{(p)}\}, \bb\in\{\vb', \alpha\vb, \alpha\ub, \bxi_j^{(p)}\} \Big\}
%     \end{align*}
%     \item For $(i,j)\in\cS_{1,1}^{+, +}$, we have the data patch $\xb_{i,j}^{(p)}$ satisfies 
%     \begin{align*}
%     \xb_{i,j}^{(p)}\in\Big\{ \lambda \ab + (1-\lambda) \bb: \ab\in\{\vb', \alpha\vb, \alpha\ub, \bxi_i^{(p)}\}, \bb\in\{\vb', \alpha\vb, \alpha\ub, \bxi_j^{(p)}\} \Big\}
%     \end{align*}
%     \item For $(i,j)\in\cS_{1,1}^{-, -}$, we have the data patch $\xb_{i,j}^{(p)}$ satisfies 
%     \begin{align*}
%     \xb_{i,j}^{(p)}\in\Big\{ \lambda \ab + (1-\lambda) \bb: \ab\in\{\ub', \alpha\vb, \alpha\ub, \bxi_i^{(p)}\}, \bb\in\{\ub', \alpha\vb, \alpha\ub, \bxi_j^{(p)}\} \Big\}
%     \end{align*}
%     \item For $(i,j)\in\cS_{1,1}^{+, -}$, we have the data patch $\xb_{i,j}^{(p)}$ satisfies 
%     \begin{align*}
%     \xb_{i,j}^{(p)}\in\Big\{ \lambda \ab + (1-\lambda) \bb: \ab\in\{\vb', \alpha\vb, \alpha\ub, \bxi_i^{(p)}\}, \bb\in\{\ub', \alpha\vb, \alpha\ub, \bxi_j^{(p)}\} \Big\}
%     \end{align*}
% \end{itemize}

\subsection{Learning Dynamics of Feature and Noise vectors}


Now, we will seek to study the learning of feature and noise vectors. Particularly, the update formulas of all feature vectors are provided as follows: for any $\ab\in\{\ub,\vb,\ub',\vb'\}\cup\{\bxi\}$, we have
\begin{align}\label{eq:update_allfeatures_mixup}
\la\wb_{k,r}^{(t+1)}, \ab\ra &= \la\wb_{k,r}^{(t)}, \ab\ra - \eta\cdot\la\nabla_{\wb_{k,r}} L(\Wb^{(t)}),\ab\ra\notag\\
&=\la\wb_{k,r}^{(t)}, \ab\ra + \frac{\eta}{n^2}\cdot   \sum_{i,j\in[n]} \ell_{k,(i,j)}^{(t)} \sum_{p\in[P]}\la\wb_{k,r}^{(t)},\xb_{i,j}^{(p)}\ra\cdot\la \xb_{i,j}^{(p)},\ab\ra
\end{align}
% \la\wb_{k,r}^{(t+1)}, \ub\ra &= \la\wb_{k,r}^{(t)}, \ub\ra + \frac{\eta}{n^2}\cdot   \sum_{i,j\in[n]} \ell_{k,(i,j)}^{(t)} \sum_{p\in[P]}\la\wb_{k,r}^{(t)},\xb_{i,j}^{(p)}\ra\cdot\la \xb_{i,j}^{(p)},\ub\ra\notag\\
% \la\wb_{k,r}^{(t+1)}, \vb'\ra &= \la\wb_{k,r}^{(t)}, \vb'\ra + \frac{\eta}{n^2}\cdot  \sum_{i,j\in[n]} \ell_{k,(i,j)}^{(t)} \sum_{p\in[P]}\la\wb_{k,r}^{(t)},\xb_{i,j}^{(p)}\ra\cdot\la \xb_{i,j}^{(p)},\vb'\ra\notag\\
% \la\wb_{k,r}^{(t+1)}, \ub'\ra &= \la\wb_{k,r}^{(t)}, \ub'\ra + \frac{\eta}{n^2}\cdot  \sum_{i,j\in[n]} \ell_{k,(i,j)}^{(t)} \sum_{p\in[P]}\la\wb_{k,r}^{(t)},\xb_{i,j}^{(p)}\ra\cdot\la \xb_{i,j}^{(p)},\ub'\ra.
% \la\wb_{k,r}^{(t+1)}, \bxi_s^{(q)}\ra &= \la\wb_{k,r}^{(t)}, \ub'\ra + \frac{\eta}{n^2}\cdot  \sum_{i,j\in[n]} \ell_{k,(i,j)}^{(t)} \sum_{p\in[P]}\la\wb_{k,r}^{(t)},\xb_{i,j}^{(p)}\ra\cdot\la \xb_{i,j}^{(p)},\ub'\ra.
% \end{align}

More specifically, we summarize the update of all critical vectors (e.g., common features, rare features, and data noise vectors) in the following Proposition.
\begin{proposition}\label{prop:update_mixup}
For any critical vector $\ab\in\{\vb, \ub, \vb', \ub'\}\cup\{\bxi\}$, we have
\begin{align*}
-\la\nabla_{\wb_{k,r}} L_{\cS}(\Wb^{(t)}),\ab\ra &=  \gamma_k^{(t)}(\vb,\ab)\cdot\la\wb_{k,r}^{(t)}, \vb\ra + \gamma_k^{(t)}(\ub,\ab)\cdot\la\wb_{k,r}^{(t)},\ub\ra +\gamma_k^{(t)}(\vb',\ab)\cdot\la\wb_{k,r}^{(t)},\vb'\ra \notag\\
&\qquad + \gamma_k^{(t)}(\ub',\ab)\cdot\la\wb_{k,r}^{(t)},\ub'\ra +  \sum_{i=1}^n\sum_{p\in[P]}\gamma_k^{(t)}(\bxi_i^{(p)},\ab)\cdot\la\wb_{k,r}^{(t)},\bxi_i^{(p)}\ra,
\end{align*}
where $\gamma_k^{(t)}(\bb,\ab)$ is a scalar output function that depends on $\bb, \ab\in\{\vb, \ub, \vb', \ub'\}\cup\{\bxi\}$. More specifically, let 
\begin{align}\label{eq:linear_expansion_data}
\xb_{i,j}^{(p)} = \theta_{i,j}^{(p)}(\vb) \cdot\vb + \theta_{i,j}^{(p)}(\ub)\cdot \ub +\theta_{i,j}^{(p)}(\vb') \cdot\vb' + \theta_{i,j}^{(p)}(\ub')\cdot \ub' + \sum_{s=1}^n\sum_{q\in[P]}\theta_{i,j}^{(p)}(\bxi_s^{(q)})\cdot \bxi_s^{(q)} 
\end{align}
be a linear expansion of $\xb_{i,j}^{(p)}$ on the space spanned by $\{\vb, \ub, \vb', \ub'\}\cup\{\bxi\}$, we have
\begin{align}\label{eq:expansion_mixed_data}
\gamma_k^{(t)}(\bb, \ab) = \frac{1}{n^2}\sum_{i,j\in[n]}\ell_{k,(i,j)}^{(t)} \sum_{p\in[P]} \theta_{i,j}^{(p)}(\bb)\cdot\la\xb_{i,j}^{(p)},\ab\ra.
\end{align}



\end{proposition}
\begin{proof}[Proof of Proposition \ref{prop:update_mixup}]
Recall \eqref{eq:update_allfeatures_mixup} and the decomposition of $\xb_{i,j}^{(p)}$ in \eqref{eq:linear_expansion_data}, we have
\begin{align*}
-\la\nabla_{\wb_{k,r}}L_\cS(\Wb^{(t)}),\ab\ra &= \frac{1}{n^2}\sum_{i,j\in[n]} \ell_{k,(i,j)}^{(t)} \sum_{p\in[P]}\la\wb_{k,r}^{(t)},\xb_{i,j}^{(p)}\ra\cdot\la \xb_{i,j}^{(p)},\ab\ra\notag\\
& = \frac{1}{n^2}\sum_{i,j\in[n]}\ell_{k,(i,j)}^{(t)} \sum_{p\in[P]} \sum_{\bb\in\{\vb,\ub,\vb',\ub'\}\cup\{\bxi\}}\theta_{i,j}^{(p)}(\bb)\cdot \la\wb_{k,r}^{(t)},\bb\ra\cdot \la\xb_{i,j}^{(p)},\ab\ra\notag\\
& =  \sum_{\bb\in\{\vb,\ub,\vb',\ub'\}\cup\{\bxi\}}\bigg[\frac{1}{n^2}\sum_{i,j\in[n]}\ell_{k,(i,j)}^{(t)} \sum_{p\in[P]} \theta_{i,j}^{(p)}(\bb)\cdot \la\xb_{i,j}^{(p)},\ab\ra\bigg]\cdot \la\wb_{k,r}^{(t)},\bb\ra. 
\end{align*}
Therefore, it is easy to see that using the definition of $\gamma_k^{(t)}(\bb,\ab)$ in \eqref{eq:expansion_mixed_data}, we have
\begin{align*}
-\la\nabla_{\wb_{k,r}}L_\cS(\Wb^{(t)}),\ab\ra = \sum_{\bb\in\{\vb,\ub,\vb',\ub'\}\cup\{\bxi\}} \gamma_k^{(t)}(\bb,\ab)\cdot \la\wb_{k,r}^{(t)},\bb\ra,
\end{align*}
which completes the proof.

\end{proof}



Note that the neural network outputs are in the order of $o(1)$ in the first few iterations, which implies that the output logits are within the range $[0.5-o(1), 0.5+o(1)]$. Further note that the loss derivatives $\ell_{k;(i,j)}^{(t)}$ satisfies
\begin{align*}
|\ell_{k;(i,j)}^{(t)}|\in\big\{1 - \logit_k(\Wb^{(t)};\xb_{i,j}), \logit_k(\wb^{(t)};\xb_{i,j}), \lambda - \logit_k(\Wb^{(t)};\xb_{i,j}), \logit_k(\Wb^{(t)};\xb_{i,j})+\lambda - 1\big\},
\end{align*}
which will also be in the constant order. Then similar to the previous analysis on the standard training, we will directly take $|\ell_{k,(i,j)}^{(t)}|=\Theta(1)$ when characterizing the learning of feature and noise vectors in the initial phase.

Then, the challenging part in the analysis is the characterization of the mixed data patches $\{\xb_{i,j}^{(p)}\}_{p\in[P]}$, since it can be: mixture of common features, mixture of rare features, mixture of common and rare features, mixture of feature and noise, which will produce different gradients. For any mixed data $\xb_{i,j}=\lambda \xb_i+(1-\lambda)\xb_j$, we will denote it as the positive mixed data if $y_i=1$ and the negative mixed data if $y_i=-1$. The following lemma gives the characterization of the data patch of all mixed data.

% \begin{lemma}\label{lemma:data_patches_mixup}
% Assume the training data are generated according to Definition \ref{def:data_distribution_new}, then regarding the $n^2$ mixed data, we have
% \begin{itemize}
% \item The feature vector $\vb$ will appear in the following data patches:
% \begin{itemize}
%     \item The data in $\cS_{0,0}^{+, +}\cup\cS_{0, 0}^{+, -}\cup\cS_{0, 1}^{+, +}\cup\cS_{0, 1}^{+, -}$, as a leading feature, in the form of  $\lambda \vb + (1-\lambda)\bb$, where $\bb\in\{\vb, \ub, \alpha\vb, \alpha\ub, \vb', \ub'\}\cup\{\bxi\}$;
%     \item The data in $\cS_{0, 0}^{-, +}\cup\cS_{0, 1}^{-, +}\cup\cS_{0, 1}^{-, +}$, as a secondary feature, in the form of  $(1-\lambda) \vb + \lambda\bb$, where $\bb\in\{\ub, \alpha\vb, \alpha\ub, \vb',\ub'\}\cup\{\bxi\}$;
%     \item All mixed data, as a feature noise, in the form of  $(1-\lambda)\alpha \vb + \lambda\bb$ or $\lambda \alpha\vb + (1-\lambda)\bb$, where $\bb\in\{\ub, \alpha\vb, \alpha\ub, \vb',\ub'\}\cup\{\bxi\}$
    
    
% \end{itemize}
% \item The feature vector $\ub$ will appear in the following data patches:
% \begin{itemize}
%     \item The data in $\cS_{0,0}^{-, -}\cup\cS_{0, 0}^{-, +}\cup\cS_{0, 1}^{-, +}\cup\cS_{0, 1}^{-, +}$, as a leading feature, in the form of  $\lambda \ub + (1-\lambda)\bb$, where $\bb\in\{\vb,\ub, \alpha\vb, \alpha\ub, \vb',\ub'\}\cup\{\bxi\}$;
%     \item The data in $\cS_{0, 0}^{+, -}\cup\cS_{0, 1}^{+, +}\cup\cS_{0, 1}^{+, -}$, as a secondary feature, in the form of  $(1-\lambda) \ub + \lambda\bb$, where $\bb\in\{\vb, \alpha\vb, \alpha\ub, \vb', \ub'\}\cup\{\bxi\}$;
    
%     \item All mixed data, as a feature noise, in the form of  $(1-\lambda)\alpha \ub + \lambda\bb$ or $\lambda \alpha\ub + (1-\lambda)\bb$, where $\bb\in\{\ub, \alpha\vb, \alpha\ub, \vb',\ub'\}\cup\{\bxi\}$
%     \end{itemize}
    
%     \item The feature vector $\vb'$ will appear in the following data patches:
%  \begin{itemize}
%     \item The data in $\cup\cS_{1, 1}^{+, +}\cup\cS_{1,1}^{+,-}\cup\cS_{1, 0}^{+, +}\cup \cS_{1, 0}^{+, -}$, as a leading feature, in the form of  $\lambda \vb' + (1-\lambda)\bb$, where $\bb\in\{\vb, \ub, \alpha\vb, \alpha\ub, \vb', \ub'\}\cup\{\bxi\}$;
%     \item The data in $\cS_{1, 1}^{-, +}\cup\cS_{1, 0}^{-, +}\cup\cS_{1, 0}^{-, +}$, as a secondary feature, in the form of  $(1-\lambda) \vb' + \lambda\bb$, where $\bb\in\{\vb, \ub, \alpha\vb, \alpha\ub, \ub'\}\cup\{\bxi\}$;
% \end{itemize}   
%         \item The feature vector $\ub'$ will appear in the following data patches:
%  \begin{itemize}
    
%     \item The data in $\cS_{1,1}^{-,-}\cup\cS_{1, 1}^{-, +}\cup\cS_{1, 0}^{-, +}\cup\cS_{1, 0}^{-, +}$, as a secondary feature, in the form of  $(1-\lambda) \ub' + \lambda\bb$, where $\bb\in\{\vb, \ub, \alpha\vb, \alpha\ub, \vb', \ub'\}\cup\{\bxi\}$;
%     \item The data in $\cS_{1,1}^{+,-}\cup\cS_{1, 0}^{-, -}\cup \cS_{1, 0}^{+, -}$, as a leading feature, in the form of  $\lambda \ub' + (1-\lambda)\bb$, where $\bb\in\{\vb, \ub, \alpha\vb, \alpha\ub, \vb'\}\cup\{\bxi\}$;
% \end{itemize} 
% \end{itemize}


% \item  For any data index $(i,j)\in\cS_{0,0}^{+, +}\cup \cS_{0,0}^{+, -}$, there will be at least a constant number of data patches are in the form of $\lambda \vb + (1-\lambda)\bb$, where $\bb\in\{\ub, \alpha\vb, \alpha\ub\}\cup\{\bxi\}$.
% \item  For any data index $(i,j)\in\cS_{0,0}^{-, -}\cup \cS_{0,0}^{-, +}$, there will be at least a constant number of data patches are in the form of $\lambda \ub + (1-\lambda)\bb$, where $\bb\in\{\vb, \alpha\vb, \alpha\ub\}\cup\{\bxi\}$.
% \item For the data $\cS_{0, 1}^{+, +}$, there will be $\Theta(\rho n^2/P)$ among them have at least a constant number of data patches in the form of $\lambda \vb' + (1-\lambda)\vb$. The remaining data will have at least a constant number of data patches in the the form of $\lambda \vb + (1-\lambda)\bb$, where $\bb\in\{\alpha\vb, \alpha\ub\}\cup\{\bxi\}$
% \item For the data $\cS_{0, 1}^{-, -}$, there will be $\Theta(\rho n^2/P)$ among them have at least a constant number of data patches in the form of $\lambda \ub' + (1-\lambda)\ub$. The remaining data will have at least a constant number of data patches in the the form of $\lambda \vb + (1-\lambda)\bb$, where $\bb\in\{\alpha\vb, \alpha\ub\}\cup\{\bxi\}$.
% \begin{itemize}
% \item For any data $(i,j)$ in $\cS_{0, 0}^{+, +}$, it will have a constant number of data patches $\xb_{i,j}^{(p)}$ satisfying
% \begin{align*}
% \xb_{i,j}^{(p)} \in \big\{\vb, \lambda\vb+(1-\lambda)\bxi_j^{(p)}, \lambda\vb+(1-\lambda)\bxi_i^{(p)}, [(1-\alpha)\lambda + \alpha]\vb, \lambda\vb + \alpha(1-\lambda)\ub\big\}
% \end{align*}

% \item For any data $(i,j)$ in $\cS_{0, 0}^{-, -}$, it will have a constant number of data patches $\xb_{i,j}^{(p)}$ satisfying
% \begin{align*}
% \xb_{i,j}^{(p)} \in \big\{\vb, \lambda\ub+(1-\lambda)\bxi_j^{(p)}, \lambda\ub+(1-\lambda)\bxi_i^{(p)}, [(1-\alpha)\lambda + \alpha]\ub, \lambda\ub + \alpha(1-\lambda)\vb\big\}
% \end{align*}

% \item For any data $(i,j)$ in $\cS_{0, 1}^{-, +}$, it will have a constant number of data patches $\xb_{i,j}^{(p)}$ satisfying
% \begin{align*}
% \xb_{i,j}^{(p)} \in \big\{\vb, \lambda\ub+(1-\lambda)\bxi_j^{(p)}, \lambda\ub+(1-\lambda)\bxi_i^{(p)}, [(1-\alpha)\lambda + \alpha]\ub, \lambda\ub + \alpha(1-\lambda)\vb\big\}
% \end{align*}
% \end{itemize}




% \end{lemma}


% Now we are able to characterize the learning speed of the feature vectors. 
% In particular, denote 
% \begin{align*}
% \cS^+&:= \cS_{0, 0}^{+, +}\cup\cS_{0, 0}^{+, -}\cup\cS_{0, 1}^{+, +}\cup\cS_{0, 1}^{+, -}\cup\cS_{1,1}^{+,+}\cup\cS_{1,1}^{+, -}\cup\cS_{1, 0}^{+, +}\cup\cS_{1, 0}^{+, -}\notag\\
% \cS^-&:=\cS_{0, 0}^{-, -}\cup\cS_{0, 0}^{-, +}\cup\cS_{0, 1}^{-, -}\cup\cS_{0, 1}^{-, +}\cup\cS_{1,1}^{-,-}\cup\cS_{1,1}^{-, +}\cup\cS_{1, 0}^{-, -}\cup\cS_{1, 0}^{-, +},
% \end{align*}
% consider the case of $k=1$, we have
% \begin{align*}
% \sum_{i,j\in[n]} \ell_{1, (i,j)}^{(t)}\sum_{p\in[P]}\la\wb_{1,r}^{(t)},\xb_{i,j}^{(p)}\ra\cdot\la\wb_{i,j}^{(p)}, \vb\ra &= \sum_{(i,j)\in\cS^+} \ell_{1, (i,j)}^{(t)}\sum_{p\in[P]}\la\wb_{1,r}^{(t)},\xb_{i,j}^{(p)}\ra\cdot\la\xb_{i,j}^{(p)}, \vb\ra\notag\\
% &\qquad + \sum_{(i,j)\in\cS^-} \ell_{1, (i,j)}^{(t)}\sum_{p\in[P]}\la\wb_{1,r}^{(t)},\xb_{i,j}^{(p)}\ra\cdot\la\xb_{i,j}^{(p)}, \vb\ra.
% \end{align*}
% where we have $\ell_{1,(i,j)}^{(t)}>0$ in the first term on the R.H.S. of the above equation and $\ell_{1,(i,j)}^{(t)}
% <0$ in the second term.




















\subsection{Characterizing the Coefficient $\gamma_k^{(t)}(\cdot,\cdot)$}


\subsubsection{Correct Common Feature Learning}

\begin{lemma}\label{lemma:feature_learning_coefficients_v_mixup}
Assume $\max_{k\in[2], (i,j)\in\cS} |F_k(\Wb^{(t)}; \xb_{i,j})|\le \zeta\in\big[\omega(b\alpha),o\big(\frac{1}{\polylog(n)}\big)\big]$, then recalling the update form in Proposition \ref{prop:update_mixup}, we have
\begin{align*}
&\gamma_1^{(t)}(\vb, \vb) = \Theta(1), \quad |\gamma_1^{(t)}(\ub, \vb)| =  O(\zeta+\alpha), \quad |\gamma_1^{(t)}(\vb',\vb)| = O(\rho /P), \notag\\
&|\gamma_1(\ub',\vb)| = O(\zeta\rho/P), \quad |\gamma_1^{(t)}(\bxi_s^{(q)}, \vb)| = \tilde O\big(1/(Pn)\big).
\end{align*}




\end{lemma}
\begin{proof}[Proof of Lemma \ref{lemma:feature_learning_coefficients_v_mixup}]
We will prove all the arguments in order. 
\paragraph{Proof for $\gamma_1^{(t)}(\vb,\vb)$.}
We first prove the bound for $\gamma_1^{(t)}(\vb, \vb)$. By \eqref{eq:expansion_mixed_data}, we have
\begin{align}\label{eq:coefficient_update_v_v_mixup}
\gamma_k^{(t)}(\vb, \vb) = \frac{1}{n^2}\sum_{i,j\in[n]}\ell_{k,(i,j)}^{(t)} \sum_{p\in[P]} \theta_{i,j}^{(p)}(\vb)\cdot\la\xb_{i,j}^{(p)},\vb\ra,
\end{align}
where $\theta_{i,j}^{(p)}(\vb) = \la\xb_{i,j}^{(p)},\vb\ra$. Therefore, we only need to consider the data patches that contain $\vb$ (including common feature $\vb$ and feature noise $\alpha\vb$). The regarding the mixed data $\xb_{i,j}$, we consider the following cases
\begin{itemize}
    \item $i\in\cS_0^+$ and $j\in\cS_0^+$;
    \item $i\in\cS_0^+$ and $j\in\cS_1^+$, and $i\in\cS_1^+$ and $j\in\cS_0^+$;
    \item $i\in\cS_0^+$ and $j\in\cS_0^-\cup\cS_1^-$, and $i\in\cS_0^-\cup\cS_1^-$ and $j\in\cS_0^+$
    \item $i\in\cS_0^-\cup\cS_1^+\cup\cS_1^-$ and $j\in\cS_0^-\cup\cS_1^+\cup\cS_1^-$.
\end{itemize}

\textit{Analysis on the data  $i\in\cS_0^+$ and $j\in\cS_0^+$} In particular, note that before the mixup, both the data $\xb_i$ and $\xb_j$ have a constant number of common feature patches. Therefore, let $\cP_{i,j}^*(\vb)$ denote the set of patches with the common feature $\vb$ (which appears in either $\xb_i$ or $\xb_j$), we have
\begin{align}\label{eq:decomposition_case1_v_mixup}
\sum_{p\in[P]}\theta_{i,j}^{(p)}(\vb)\cdot\la\xb_{i,j}^{(p)},\vb\ra  = \sum_{p\in \cP_{i,j}^*(\vb)}\theta_{i,j}^{(p)}(\vb)\cdot\la\xb_{i,j}^{(p)},\vb\ra  + \sum_{p\in \cP_{i,j}(\vb)\backslash\cP_{i,j}^*(\vb)}\theta_{i,j}^{(p)}(\vb)\cdot\la\xb_{i,j}^{(p)},\vb\ra.
\end{align}
Regarding the first term on the R.H.S. of the above equation, by Definition \ref{def:data_distribution_new}, we know that there exists at least one common feature patch in both $\xb_i$ and $\xb_j$, which leads to $\theta_{i,j}^{(p)}\ge \lambda$ for at least one $p\in\cP_{i,j}^*(vb)$. This further gives 
\begin{align*}
 \sum_{p\in \cP_{i,j}^*(\vb)}\theta_{i,j}^{(p)}(\vb)\cdot\la\xb_{i,j}^{(p)},\vb\ra=   \sum_{p\in \cP_{i,j}^*(\vb)}[\theta_{i,j}^{(p)}(\vb)]^2\ge \lambda^2.
\end{align*}
Besides, we also have that the number of common feature patches are upper bounded by some constant (i.e., $|\cP_{i,j}^*(\vb)|=\Theta(1)$), this further leads to
\begin{align*}
\sum_{p\in \cP_{i,j}^*(\vb)}\theta_{i,j}^{(p)}(\vb)\cdot\la\xb_{i,j}^{(p)},\vb\ra \le \Theta(1).
\end{align*}

Regarding the second term on the R.H.S. of \eqref{eq:decomposition_case1_v_mixup}, we have $\theta_{i,j}^{(p)}\le \alpha$ since $\vb$ can only appear in the form of feature noise. Besides, by Definition \ref{def:data_distribution_new}, we know that the number of patches containing feature noise is at most $b$, then
\begin{align*}
\sum_{p\in \cP_{i,j}(\vb)\backslash\cP_{i,j}^*(\vb)}\theta_{i,j}^{(p)}(\vb)\cdot\la\xb_{i,j}^{(p)},\vb\ra =  \sum_{p\in \cP_{i,j}(\vb)\backslash\cP_{i,j}^*(\vb)}[\theta_{i,j}^{(p)}(\vb)]^2 \le b\alpha^2=o\bigg(\frac{1}{\polylog(n)}\bigg).
\end{align*}
Moreover, note that in the initial phase we have $\ell_{1,(i,j)}^{(t)}=\Theta(1)$ for $(i,j)\in \cS_{0,0}^{+, +}$, we can further get that
\begin{align*}
\ell_{1,(i,j)}^{(t)} \sum_{p\in[P]} \theta_{i,j}^{(p)}(\vb)\cdot\la\xb_{i,j}^{(p)},\vb\ra =\Theta(1).
\end{align*}


\textit{Analysis on  the data $i\in\cS_0^+$ and $j\in\cS_1^+$.}
The analysis for this type of data will be similar. In fact, we will  consider two types of data: $i\in\cS_0^+$ and $j\in\cS_1^+$, and $i\in\cS_1^+$ and $j\in\cS_0^+$ since two original training data will give two mixed data. 

In particular, note that $\ell_{i,j}^{(t)}=\Theta(1)$ for these two types of data, we can immediately get that there is a constant number of patches that satisfy $\theta_{i,j}^{(p)}\ge 1-\lambda$, while the remaining patches $p\in\cP_{i,j}(\vb)$ satisfy $\theta_{i,j}^{(p)}\le\alpha$. Therefore, we can follow the same proof technique as that for the data $(i,j)\in\cS_{0,0}^{+, +}$ and get that for all $(i,j)\in\cS_{0,1}^{+, +}\cup\cS_{1, 0}^{+, +}$,
\begin{align}\label{eq:}
\ell_{k,(i,j)}^{(t)} \sum_{p\in[P]} \theta_{i,j}^{(p)}(\vb)\cdot\la\xb_{i,j}^{(p)},\vb\ra =\ell_{k,(i,j)}^{(t)} \sum_{p\in\cP_{i,j}^*(\vb)} [\theta_{i,j}^{(p)}(\vb)]^2 + \sum_{p\in\cP_{i,j}(\vb)\backslash\cP_{i,j}^*(\vb)} [\theta_{i,j}^{(p)}(\vb)]^2 =\Theta(1).
\end{align}

\textit{ Analysis on  the data $i\in\cS_0^+$ and $j\in\cS_0^-\cup\cS_1^-$.}
In this part, we will handle data $\xb_{i,j}$ and $\xb_{j,i}$ together. Different from the previous cases where the loss derivatives $\ell_{1, (i,j)}^{(t)}$ are positive, here the loss derivative $\ell_{1, (i,j)}^{(t)}$ will become negative for $(i,j)\in\cS_{0, 0}^{-, +}\cup\cS_{0,1}^{-, +}$. Particularly, for any $(i,j)\in\cS_{0, 0}^{+, -}$, we have $(j,i)\in\cS_{0,0}^{-, +}$, then
\begin{align}\label{eq:coefficient_differentlabel_data_v_mixup}
&\ell_{1,(i,j)}^{(t)} \sum_{p\in[P]} \theta_{i,j}^{(p)}(\vb)\cdot\la\xb_{i,j}^{(p)},\vb\ra + \ell_{1,(j,i)}^{(t)} \sum_{p\in[P]} \theta_{j,i}^{(p)}(\vb)\cdot\la\xb_{j,i}^{(p)},\vb\ra\notag\\
&=\ell_{1,(i,j)}^{(t)}\sum_{p\in\cP_{i,j}(\vb)}\big[[\theta_{i,j}^{(p)}(\vb)]^2-[\theta_{j,i}^{(p)}(\vb)]^2\big] + \big[\ell_{1,(i,j)}^{(t)}+\ell_{1,(j,i)}^{(t)}\big]\cdot \sum_{p\in\cP_{i,j}(\vb)}\big[\theta_{j,i}^{(p)}(\vb)\big]^2,
\end{align}
where we use the fact that $\cP_{i,j}(\vb) = \cP_{j,i}(\vb)$ and $\la\xb_{i,j}^{(p)},\vb\ra = \theta_{i,j}^{(p)}(\vb)$.
Recall that the neural network output is upper bounded by $\zeta$, then it is easy to see
\begin{align*}
|\ell_{1,(i,j)}^{(t)} + \ell_{1,(j,i)}^{(t)}| = |\lambda - 0.5 \pm O(\zeta) + 0.5-\lambda \pm O(\zeta)| = O(\zeta). 
\end{align*}
Besides, note that
\begin{align*}
\xb_{i,j}^{(p)} = \lambda \xb_i^{(p)} + (1-\lambda)\xb_j^{(p)},\quad \xb_{j,i}^{(p)} + (1-\lambda)\xb_i^{(p)} + \lambda\xb_j^{(p)}.
\end{align*}
Then we will also define $\cP_{i,j}^*(\vb)$ as the set of patches with common feature. Note that $\xb_j^{(p)}$ does not have the common feature patch since $j\in\cS_0^-\cup\cS_1^-$, we can immediately get that $\cP_{i,j}^*(\vb) = \cP_i^*(\vb)$, where $\cP_i^*(\vb)$ denotes the set of common feature patches of $\xb_i$. Besides, it is also clear that all data patches in $\cP_{i,j}(\vb)$ only contain the feature noise $\alpha\vb$. Then it follows that
\begin{align*}
&\ell_{1,(i,j)}^{(t)}\sum_{p\in\cP_{i,j}(\vb)}\big[[\theta_{i,j}^{(p)}(\vb)]^2-[\theta_{j,i}^{(p)}(\vb)]^2\big] \notag\\
&= \Theta(1)\cdot\bigg[ \sum_{p\in\cP_{i,j}^*(\vb)}[\lambda^2-(1-\lambda)^2] +  \sum_{p\in\cP_{i,j}(\vb)\backslash\cP_{i,j}^*(\vb)}\big[[\theta_{i,j}^{(p)}(\vb)]^2-[\theta_{j,i}^{(p)}(\vb)]^2\big]\bigg]\notag\\
&=\Theta(1)  \pm O(b\alpha^2)\notag\\
&=\Theta(1).
\end{align*}
Similarly, we can also get $\sum_{p\in\cP_{i,j}(\vb)}[\theta_{j,i}^{(p)}]^2=\Theta(1)$. Therefore, putting everything to \eqref{eq:coefficient_differentlabel_data_v_mixup}, we can finally obtain the following 
\begin{align*}
\ell_{1,(i,j)}^{(t)} \sum_{p\in[P]} \theta_{i,j}^{(p)}(\vb)\cdot\la\xb_{i,j}^{(p)},\vb\ra + \ell_{1,(j,i)}^{(t)} \sum_{p\in[P]} \theta_{j,i}^{(p)}(\vb)\cdot\la\xb_{j,i}^{(p)},\vb\ra = \Theta(1) \pm \Theta(1)\cdot O(\zeta) = \Theta(1).
\end{align*}


\textit{Analysis on the data $i,j\in\cS_0^-\cup\cS_1^+\cup\cS_1^-$} In this case, we can observe that there is no common feature patches in $\xb_i$ and $\xb_j$, while the vector $\vb$ will only appear in at most $2b$ patches of $\xb_{i,j}$ in the form of feature noise. Therefore, we have $|\Theta_{i,j}^{(p)}|\in [(1-\lambda)\alpha, \alpha]$ for at most $2b$ patches and the remaining patches will give $|\Theta_{i,j}^{(p)}|=0$. Consequently, we have
\begin{align*}
\bigg|\ell_{k,(i,j)}^{(t)} \sum_{p\in[P]} \theta_{i,j}^{(p)}(\vb)\cdot\la\xb_{i,j}^{(p)},\vb\ra\bigg| =\bigg|\ell_{k,(i,j)}^{(t)} \sum_{p\in\cP_{i,j}(\vb)} [\theta_{i,j}^{(p)}(\vb)]^2\bigg|=O(b\alpha^2)=o\bigg(\frac{1}{\polylog(n)}\bigg).
\end{align*}

\textit{Completing the analysis for $\gamma_1^{(t)}(\vb, \vb)$.} Now we are able to complete the analysis on $\gamma_1^{(t)}(\vb,\vb)$ based on \eqref{eq:coefficient_update_v_v_mixup}:
\begin{align*}
\gamma_1^{(t)}(\vb, \vb) &= \frac{1}{n^2}\sum_{i,j\in[n]}\ell_{1,(i,j)}^{(t)} \sum_{p\in[P]} \theta_{i,j}^{(p)}(\vb)\cdot\la\xb_{i,j}^{(p)},\vb\ra\notag\\
&= \frac{1}{n^2}\bigg[\sum_{(i,j)\in\cS_{0,0}^{+,+}}\ell_{1,(i,j)}^{(t)} \sum_{p\in[P]} \theta_{i,j}^{(p)}(\vb)\cdot\la\xb_{i,j}^{(p)},\vb\ra \notag\\
&\qquad + \sum_{(i,j)\in\cS_{0,1}^{+,+}\cup\cS_{1,0}^{+,+}}\ell_{1,(i,j)}^{(t)} \sum_{p\in[P]} \theta_{i,j}^{(p)}(\vb)\cdot\la\xb_{i,j}^{(p)},\vb\ra\notag\\
&\qquad + \sum_{(i,j)\in\cS_{0,0}^{+,-}\cup\cS_{0, 1}^{+, -}\cup\cS_{0,0}^{-, +}\cup\cS_{0,1}^{-,+}}\ell_{1,(i,j)}^{(t)} \sum_{p\in[P]} \theta_{i,j}^{(p)}(\vb)\cdot\la\xb_{i,j}^{(p)},\vb\ra\notag\\
&\qquad + \sum_{i,j\in\cS_0^-\cup\cS_1^+\cup\cS_1^-}\ell_{1,(i,j)}^{(t)} \sum_{p\in[P]} \theta_{i,j}^{(p)}(\vb)\cdot\la\xb_{i,j}^{(p)},\vb\ra\bigg]\notag\\
& = \frac{1}{n^2}\bigg[\Theta(1)\cdot|\cS_{0,0}^{+,+}| + \Theta(1)\cdot |\cS_{0,1}^{+,+}\cup\cS_{1,0}^{+,+}| + \Theta(1)\cdot |\cS_{0,0}^{+,-}\cup\cS_{0,1}^{+,-}\cup\cS_{0,0}^{-,+}\cup\cS_{0,1}^{-,+}|\notag\\
&\qquad \pm o\bigg(\frac{1}{\polylog(n)}\bigg)\cdot|\cS_{0,0}^{-,-}\cup\cS_{0,1}^{-,+}\cup\cS_{0,1}^{-,-}\cup\cS_{1,0}^{+,-}\cup\cS_{1,1}^{+,+}\cup\cS_{1,1}^{+,-}\cup\cS_{1,0}^{-,-}\cup\cS_{1,1}^{-,+}\cup\cS_{1,1}^{-,-}|\bigg]\notag\\
& = \frac{1}{n^2}\bigg[\Theta(n^2) \pm o\bigg(\frac{n^2}{\polylog(n)}\bigg)\bigg]\notag\\
&=\Theta(1).
\end{align*}

\paragraph{Proof for $\gamma_k^{(t)}(\ub,\vb)$.}
The next step is to characterize $\gamma_k^{(t)}(\ub,\vb)$. We will split the entire mixed training dataset into the following classes:
\begin{itemize}
    \item $i\in\cS_0^+$ and $j\in\cS_0^-$, and $i\in\cS_0^-$ and $j\in\cS_0^+$, i.e., $\cS_{0,0}^{+,-}\cup\cS_{0,0}^{-,+}$.
    \item all $(i,j)\not\in\cS_{0,0}^{+,-}\cup\cS_{0,0}^{-,+}$.
\end{itemize}

We first recall the formula of $\gamma_1^{(t)}(\ub,\vb)$ (see Proposition \ref{prop:update_mixup}):
\begin{align}\label{eq:formula_correct_u_v_mixup}
\gamma_1^{(t)}(\ub,\vb) = \frac{1}{n^2}\sum_{i,j\in[n]}\ell_{1,(i,j)}^{(t)}\sum_{p\in[P]}\theta_{i,j}^{(p)}(\ub)\cdot\la\xb_{i,j}^{(p)},\vb\ra.
\end{align}

\textit{Analysis on the data $(i,j)\in\cS_{0,0}^{+,-}\cup\cS_{0,0}^{-,+}$.}
Since $\cS_{0,0}^{+,-}$ and $\cS_{0,0}^{-,+}$ are symmetric: i.e., for any $(i,j)\in\cS_{0,0}^{+,-}$, we have $(j,i)\in\cS_{0,0}^{-,+}$ and vise versa. Then we will handle data $\xb_{i,j}$ and  $\xb_{j,i}$ together by studying the following quantity:
\begin{align*}
*&:=\ell_{1,(i,j)}^{(t)} \sum_{p\in[P]}\theta_{i,j}^{(p)}(\ub)\cdot\la\xb_{i,j}^{(p)},\vb\ra + \ell_{1,(j,i)}^{(t)} \sum_{p\in[P]}\theta_{j,i}^{(p)}(\ub)\cdot\la\xb_{j,i}^{(p)},\vb\ra\notag\\
& = \ell_{1,(i,j)}^{(t)}\sum_{p\in[P]}\theta_{i,j}^{(p)}(\ub)\cdot \theta_{i,j}^{(p)}(\vb)+ \ell_{1,(j,i)}^{(t)} \sum_{p\in[P]}\theta_{j,i}^{(p)}(\ub)\cdot \theta_{j,i}^{(p)}(\vb).
\end{align*}
Note that we will only consider the patch that contains both $\ub$ and $\vb$. Then consider a data patch $\xb_{i,j}^{(p)}$ satisfy this condition: $\xb_i^{(p)}=\alpha_i\vb$ and $\xb_j^{(p)}=\alpha_j\ub$, where $\alpha_i,\alpha_j\in\{\alpha, 1\}$, which further leads to $\xb_{i,j}^{(p)} = \lambda\alpha_i\vb+(1-\lambda)\alpha_j\ub$ and $\xb_{j,i}^{(p)} = \lambda\alpha_j\ub+(1-\lambda)\alpha_i\vb$. Accordingly, it further gives
\begin{align*}
\theta_{i,j}^{(p)}(\ub)\cdot\theta_{i,j}^{(p)}(\vb) = (1-\lambda)\alpha_j\cdot \alpha_i\lambda_i = \lambda\alpha_j\cdot(1-\lambda)\alpha_i =  \theta_{j,i}^{(p)}(\ub)\cdot\theta_{j,i}^{(p)}(\vb).
\end{align*}
Additionally, for any $p\in\cP_{i,j}(\vb)$, we have at most $\Theta(1)$ among them satisfy $\theta_{j,i}^{(p)}(\ub)=\Theta(1)$ and at most $\Theta(1)$ among them satisfy $\theta_{j,i}^{(p)}(\vb)=\Theta(1)$, while the remaining, with size at most $2b$, can only give $\theta_{j,i}^{(p)}(\ub),\theta_{j,i}^{(p)}(\vb)=\Theta(\alpha)$. This implies that
\begin{align*}
\sum_{p\in[P]}\theta_{j,i}^{(p)}(\ub)\cdot\theta_{j,i}^{(p)}(\vb) = O(1) + O(\alpha) +O(b\alpha^2) = O(1).
\end{align*}
Therefore, applying the above equations, we can get that
\begin{align*}
* &= \ell_{1,(i,j)}^{(t)}\sum_{p\in\cP_{i,j}(\vb)}\big[\theta_{i,j}^{(p)}(\ub)\cdot\theta_{i,j}^{(p)}(\vb) - \theta_{j,i}^{(p)}(\ub)\cdot\theta_{j,i}^{(p)}(\vb)\big] + \big[\ell_{1,(i,j)}^{(t)}+\ell_{1,(j,i)}^{(t)}\big]\cdot \sum_{p\in[P]}\theta_{j,i}^{(p)}(\ub)\cdot\theta_{j,i}^{(p)}(\ub)\notag\\
& = \big[\ell_{1,(i,j)}^{(t)}+\ell_{1,(j,i)}^{(t)}\big]\cdot O(1).
\end{align*}
Further note that in the initial phase we have $\ell_{1,(i,j)}^{(t)}+\ell_{1,(j,i)}^{(t)} = O(\zeta)$, we consequently get 
\begin{align*}
|*|=\bigg|\ell_{1,(i,j)}^{(t)} \sum_{p\in[P]}\theta_{i,j}^{(p)}(\ub)\cdot\la\xb_{i,j}^{(p)},\vb\ra + \ell_{1,(j,i)}^{(t)} \sum_{p\in[P]}\theta_{j,i}^{(p)}(\ub)\cdot\la\xb_{j,i}^{(p)},\vb\ra\bigg| = O(\zeta).
\end{align*}

\textit{Analysis on the remaining data $(i,j)\not\in\cS_{0,0}^{+,-}\cup\cS_{0,0}^{-,+}$.} 
In this case, we note that there are no data patches that satisfy $\theta_{j,i}^{(p)}(\vb)=\Theta(1)$ and $\theta_{j,i}^{(p)}(\ub)=\Theta(1)$ simultaneously. Therefore, for any data $\xb_{i,j}$, there will exist at most $\Theta(1)$ patches that satisfy $\theta_{j,i}^{(p)}(\vb)\cdot\theta_{j,i}^{(p)}(\ub)=\alpha $ and at most $2b$ patches satisfying $\theta_{j,i}^{(p)}(\vb)\cdot\theta_{j,i}^{(p)}(\ub)=\alpha^2 $, while the remaining patches will give $\theta_{j,i}^{(p)}(\vb)\cdot\theta_{j,i}^{(p)}(\ub)=0 $. Therefore, we can get that
\begin{align*}
\sum_{p\in[P]}\theta_{i,j}^{(p)}(\ub)\cdot\la\xb_{i,j}^{(p)},\vb\ra & = \sum_{p\in[P]}\theta_{i,j}^{(p)}(\ub)\cdot \theta_{i,j}^{(p)}(\ub)=\Theta(1)\cdot \alpha + 2b\alpha^2 = O(\alpha),
\end{align*}
where the last equality follows from the setting of the data distribution that $b\alpha<1$. 

\textit{Completing the analysis for $\gamma_1^{(t)}(\ub,\vb)$.} By \eqref{eq:formula_correct_u_v_mixup} and using the fact that $|\ell_{1,(i,j)}^{(t)}|\le 1$, we have
\begin{align*}
|\gamma_1^{(t)}(\ub,\vb)| &= \frac{1}{n^2}\bigg|\sum_{i,j\in[n]}\ell_{1,(i,j)}^{(t)}\sum_{p\in[P]}\theta_{i,j}^{(p)}(\ub)\cdot\theta_{i,j}^{(p)}(\vb)\bigg| \notag\\
&= \frac{1}{n^2}\cdot \big[|\cS_{0,0}^{+,-}\cup\cS_{0,0}^{-,+}|\cdot O(\zeta) + \big(n^2-|\cS_{0,0}^{+,-}\cup\cS_{0,0}^{-,+}|\big)\cdot O(\alpha)\big]\notag\\
& = O(\zeta + \alpha).
\end{align*}

\paragraph{Proof for $\gamma_1^{(t)}(\vb', \vb)$.} We then tend to characterize $\gamma_1^{(t)}(\vb', \vb)$. We will consider the following two classes of data:
\begin{itemize}
    \item $(i,j)\in\cS_{0,1}^{+, +}\cup\cS_{1,0}^{+, +}$
    \item all $(i,j)\not\in \cS_{0,1}^{+, +}\cup\cS_{1,0}^{+, +}$.
\end{itemize}

\textit{Analysis on the data $(i,j)\in\cS_{0,1}^{+, +}\cup\cS_{1,0}^{+, +}$}
First, it is easy to see that with probability at least $1-1/\poly(n)$, we have $|\cS_{0,1}^{+, +}\cup\cS_{1,0}^{+, +}|=O(\rho n^2)$.
For this class of data, with probability $\Theta(1/P)$ we have the data $\xb_{i,j}^{(p)}$ has a constant number of patches that satisfy $\theta_{i,j}^{(p)}(\vb)\cdot \theta_{i,j}^{(p)}(\vb')=\Theta(1)$. Besides, by Lemma \ref{lemma:occurance_mixed_patch}, we have with probability at least $1-1/\poly(n)$, there are $\Theta(b\rho n^2/P)$ patches are the mixture of $\alpha\vb$ and $\vb'$, leading to $\theta_{i,j}^{(p)}(\vb)\cdot \theta_{i,j}^{(p)}(\vb')=\Theta(\alpha)$. The remaining patches will give $\theta_{i,j}^{(p)}(\vb)\cdot \theta_{i,j}^{(p)}(\vb')=0$. Combine the above results, we can get
\begin{align*}
\sum_{(i,j)\in\cS_{0,1}^{+, +}\cup\cS_{1,0}^{+, +}}\ell_{1,(i,j)}^{(t)}\sum_{p\in\cP} \theta_{i,j}^{(p)}(\vb)\cdot \theta_{i,j}^{(p)}(\vb') = \Theta\bigg(\frac{\rho n^2}{P}\bigg)  + \Theta\bigg(\frac{b\alpha\rho n^2}{P}\bigg) = \Theta\bigg(\frac{\rho n^2}{P}\bigg)
\end{align*}
where we use the fact that $b\alpha=o(1)$.  

\textit{Analysis on the remaining data} Particular, we will only consider the data $(i,j)\in\cS_{0,1}^{-,+}\cup\cS_{1,1}^{-,+}\cup\cS_{1, 0}^{+,-}$ since otherwise there is no data containing the rare feature vector $\vb'$. Moreover, note that for this class of data we only have $\theta_{i,j}^{(p)}(\vb)\cdot \theta_{i,j}^{(p)}(\vb')=O(\alpha)$ since there is no data consisting of common feature patch (but only contain feature noise $\alpha\vb$). Therefore, similar to the previous analysis, we can get that, by Lemma \ref{lemma:occurance_mixed_patch}, with probability at least $1-1/\poly(n)$, there are $\Theta(b\rho n^2/P)$  patches that give $\theta_{i,j}^{(p)}(\vb)\cdot \theta_{i,j}^{(p)}(\vb')=\Theta(\alpha)$, which consequently leads to
\begin{align*}
\sum_{(i,j)\in\cS_{0,1}^{-,+}\cup\cS_{1,1}^{-,+}\cup\cS_{1, 0}^{+,-}}\bigg|\ell_{1,(i,j)}^{(t)}\sum_{p\in\cP}\theta_{i,j}^{(p)}(\vb)\cdot \theta_{i,j}^{(p)}(\vb') \bigg|= O\bigg(\frac{b\alpha\rho n^2}{P}\bigg).
\end{align*}

\textit{Completing the analysis for $\gamma_1^{(t)}(\vb',\vb)$.} Completing the previous analysis, we have
\begin{align*}
\gamma_1^{(t)}(\vb',\vb) &= \frac{1}{n^2}\sum_{i,j\in[n]}\ell_{1,(i,j)}^{(t)}\sum_{p\in[P]}\theta_{i,j}^{(p)}(\vb')\cdot\theta_{i,j}^{(p)}(\vb) = \Theta\bigg(\frac{\rho }{P}\bigg)\pm O\bigg(\frac{b\alpha\rho }{P}\bigg)= \Theta\bigg(\frac{\rho }{P}\bigg).
\end{align*}

\paragraph{Proof for $\gamma_1^{(t)}(\ub',\vb)$.} Regarding the coefficient $\gamma_1^{(t)}(\ub',\vb)$, we consider two cases (1) mixup between $\ub'$ and $\vb$; (2) mixup between $\ub'$ and $\alpha\vb$. Then it can be seen that the first cases cover the data $(i,j)\in\cS_{1,0}^{-, +}$ and $(i,j)\in\cS_{0,1}^{+, -}$, which is equivalent to the dataset $\{(i,j), (j,i): (i,j)\in\cS_{1,0}^{+,-}\}$. Therefore, we will handle the data $(i,j)$ and $(j,i)$ together in this case. In particular, we have
\begin{align*}
&\ell_{1,(i,j)}^{(t)}\sum_{p\in[P]}\theta_{i,j}^{(p)}(\ub')\cdot \theta_{i,j}^{(p)}(\vb) + \ell_{1,(j,i)}^{(t)}\sum_{p\in[P]}\theta_{j,i}^{(p)}(\ub')\cdot \theta_{j,i}^{(p)}(\vb)\notag\\
& = \ell_{1,(i,j)}^{(t)}\sum_{p\in[P]}\big[\theta_{i,j}^{(p)}(\ub')\cdot \theta_{i,j}^{(p)}(\vb) - \theta_{j,i}^{(p)}(\ub')\cdot \theta_{j,i}^{(p)}(\vb)\big] + \big[\ell_{1,(i,j)}^{(t)}+\ell_{1,(j,i)}^{(t)}\big]\cdot\sum_{p\in[P]}\theta_{j,i}^{(p)}(\ub')\cdot \theta_{j,i}^{(p)}(\vb).
\end{align*}
It is clear that the first term on the R.H.S. of the above equation is zero since in case (1) 
\begin{align*}
\theta_{i,j}^{(p)}(\ub')\cdot \theta_{i,j}^{(p)}(\vb) = \theta_{j,i}^{(p)}(\ub')\cdot \theta_{j,i}^{(p)}(\vb) = \lambda(1-\lambda).
\end{align*}
Regarding the second term, we can use Lemma \ref{lemma:occurance_mixed_patch} and get that the number of patches falling in case (1) is $\Theta(\rho n^2/P)$. Then using the fact that $|\ell_{1,(i,j)}^{(t)} + \ell_{1,(i,j)}^{(t)}|=O(\zeta)$ can lead to the final bound for case (1). 

Regarding case (2), we can follow the analysis for $\gamma_1^{(t)}(\vb',\vb)$, which relies on the fact that $\theta_{i,j}^{(p)}(\ub)\cdot\theta_{i,j}^{(p)}(\vb')=\Theta(\alpha)$. Therefore, we can finally get
\begin{align*}
|\gamma_1^{(t)}(\ub,\vb')|&=\bigg|\frac{1}{n^2}\sum_{(i,j)\in\cS_{0,1}^{-,+}\cup\cS_{1,0}^{+,-}} \ell_{1,(i,j)}^{(t)}\sum_{p\in[P]}\theta_{i,j}^{(p)}(\ub)\cdot \theta_{i,j}^{(p)}(\vb')\bigg|\notag\\
&= O\bigg(\frac{\rho }{P}\bigg)\cdot\Theta(\zeta) + O\bigg(\frac{b\alpha\rho }{P}\bigg)\notag\\
&= \Theta\bigg(\frac{\zeta\rho  }{P}\bigg),
\end{align*}
where we use the fact that $\zeta = \omega(b\alpha)$. 

% e can follow the same proof and get a similar results:
% \begin{align*}
% |\gamma_{i,j}^{(t)}(\ub',\vb)| =\frac{1}{n^2}\bigg|\sum_{i,j\in[n]}\ell_{1,(i,j)}^{(t)}\sum_{p\in[P]}\theta_{i,j}^{(p)}(\ub')\cdot\theta_{i,j}^{(t)}(\vb)\bigg| =\Theta\bigg(\frac{\rho }{P}\bigg).
% \end{align*}
% % Then given the mixed data $\xb_{i,j}^{(p)}$, denote by $\cP_{i,j}(\vb)$ the set of patches that contain $\vb$, we only need to consider the data $(i,j)$ that satisfies $\cP_{i,j}(\vb)>0$. 

\paragraph{Proof for $\gamma_1^{(t)}(\bxi_s^{(q)},\vb)$.}
Finally, we will study $\gamma_1^{(t)}(\bxi_s^{(q)},\vb)$. Recall its formula in \eqref{eq:expansion_mixed_data} we can get 
\begin{align*}
\gamma_1^{(t)}(\bxi_s^{(q)},\vb) = \frac{1}{n^2}\sum_{i,j\in[n]}\ell_{1,(i,j)}^{(t)}\sum_{p\in[P]}\theta_{i,j}^{(p)}(\bxi_s^{(q)})\cdot\theta_{i,j}^{(p)}(\vb).
\end{align*}

Then it can be seen that the noise vector $\bxi_s^{(p)}$ will appear in $2n-1$ mixup data patches. By Lemma \ref{lemma:occurance_mixed_patch}, we have with probability at least $1-1/\poly(n)$,  $\Theta(1/P)$ fraction of them are mixed with $\vb$ and $O(b/P)$ fraction of them are mixed with $\alpha\vb$. Therefore, we can get that
\begin{align*}
\gamma_1^{(t)}(\bxi_s^{(q)},\vb) = \underbrace{\frac{1}{n^2}\sum_{p=q,i=s || p=q, j=s}\ell_{1,(i,j)}^{(t)}\theta_{i,j}^{(p)}(\bxi_s^{(q)})\cdot\theta_{i,j}^{(p)}(\vb)}_{I_1} + \underbrace{\frac{1}{n^2}\sum_{p\neq q|| i\neq s, j\neq s}\ell_{1,(i,j)}^{(t)}\sum_{p\in[P]}\theta_{i,j}^{(t)}(\bxi_s^{(q)})\cdot\theta_{i,j}^{(p)}(\vb)}_{I_2},
\end{align*}
where it holds that
\begin{align*}
|I_1| \le \frac{1}{n^2}\cdot \big[ \Theta(n/P) + \Theta(b\alpha/P)\big] = \Theta\bigg(\frac{1}{Pn}\bigg),
\end{align*}
and
\begin{align*}
|I_2| \le \tilde O\bigg(\frac{P}{d^{1/2}}\bigg),
\end{align*}
where we use the fact that $b\alpha=o(1)$ and $\theta_{i,j}^{(t)}(\bxi_s^{(q)})=\tilde O(d^{-1/2})$ for all $i\neq s $ and $j\neq s$. This further implies that 
\begin{align*}
|\gamma_1^{(t)}(\bxi_s^{(q)},\vb)| = O\bigg(\frac{1}{Pn}\bigg)
\end{align*}
since we have assumed that $d\ge P^4n^2$.
\end{proof}

We can also get a similar result for the learning of common feature $\ub$.
\begin{lemma}\label{lemma:feature_learning_coefficients_u_mixup}
Assume $\max_{k\in[2], (i,j)\in\cS} |F_k(\Wb^{(t)}; \xb_{i,j})|\le \zeta=o\big(\frac{1}{\polylog(n)}\big)$, then recalling the update form in Proposition \ref{prop:update_mixup}, we have for any $r\in[m]$, $q\in[P]$, and $s\in[n]$,
\begin{align*}
&\gamma_2^{(t)}(\ub, \ub) = \Theta(1), \quad |\gamma_2^{(t)}(\vb, \ub)| =  O(\zeta+\alpha), \quad |\gamma_2^{(t)}(\vb',\ub)| = O(\zeta\rho /P), \notag\\
&|\gamma_2(\ub',\ub)| = O(\rho/P), \quad |\gamma_2^{(t)}(\bxi_s^{(q)}, \ub)| = \tilde O\big(1/(Pn)\big).
\end{align*}



\end{lemma}



\subsubsection{Incorrect Common Feature Learning}
In this part, we will study the incorrect common feature learning, i.e., quantifying the inner products $\la\wb_{2,r}^{(t)},\vb\ra$ and $\la\wb_{1,r}^{(t)},\ub\ra$.


\begin{lemma}\label{lemma:feature_learning_coefficients_v_incorrect_mixup}
Assume $\max_{k\in[2], (i,j)\in\cS} |F_k(\Wb^{(t)}; \xb_{i,j})|\le \zeta=o\big(\frac{1}{\polylog(n)}\big)$, then recalling the update form in Proposition \ref{prop:update_mixup}, we have
\begin{align*}
&\gamma_2^{(t)}(\vb, \vb) = -\Theta(1), \quad |\gamma_2^{(t)}(\ub, \vb)| =  O(\zeta+\alpha), \quad |\gamma_2^{(t)}(\vb',\vb)| = O(\rho /P), \notag\\
&|\gamma_2^{(t)}(\ub',\vb)| = O(\zeta\rho/P), \quad |\gamma_2^{(t)}(\bxi_s^{(q)}, \vb)| = \tilde O\big(1/(Pn)\big).
\end{align*}

\end{lemma}
\begin{proof}[Proof of Lemma \ref{lemma:feature_learning_coefficients_v_incorrect_mixup}]
Recall the definition of $\gamma_2^{(t)}(\vb, \vb)$, we have
\begin{align*}
\gamma_2^{(t)}(\vb,\vb) = \frac{1}{n^2}\sum_{i,j\in[n]}\ell_{2,(i,j)}^{(t)}\sum_{p\in[P]}[\theta_{i,j}^{(p)}(\vb)]^2.
\end{align*}
Then comparing with the previous analysis on $\gamma_2^{(t)}(\vb, \vb)$, the only difference is to replace $\ell_{1,(i,j)}^{(t)}$ to $\ell_{2,(i,j)}^{(t)} = - \ell_{2,(i,j)}^{(t)}$. Therefore, we can immediately get that $\gamma_2^{(t)}(\vb,\vb)=-\gamma_1^{(t)}(\vb,\vb)=-\Theta(1)$.

Regarding other terms that are bounded in terms of their absolute values, we can get the same results as in Theorem \ref{lemma:feature_learning_coefficients_v_mixup}. This completes the proof.
\end{proof}
Similarly, we can get the following results for $\ub$.
\begin{lemma}\label{lemma:feature_learning_coefficients_u_incorrect_mixup}
Assume $\max_{k\in[2], (i,j)\in\cS} |F_k(\Wb^{(t)}; \xb_{i,j})|\le \zeta=o\big(\frac{1}{\polylog(n)}\big)$, then recalling the update form in Proposition \ref{prop:update_mixup}, we have
\begin{align*}
&\gamma_1^{(t)}(\ub, \ub) = -\Theta(1), \quad |\gamma_1^{(t)}(\vb, \ub)| =  O(\zeta+\alpha), \quad |\gamma_1^{(t)}(\vb',\ub)| = O(\rho /P), \notag\\
&|\gamma_1(\ub',\ub)| = O(\zeta\rho/P), \quad |\gamma_1^{(t)}(\bxi_s^{(q)}, \ub)| = \tilde O\big(1/(Pn)\big).
\end{align*}

\end{lemma}



\subsubsection{Rare Feature Learning}
In this part, we will study the rare feature learning, i.e., quantifying the inner products $\la\wb_{1,r}^{(t)},\vb'\ra$ and $\la\wb_{2,r}^{(t)},\ub'\ra$. 
\begin{lemma}\label{lemma:feature_learning_coefficients_v'_mixup}
Assume $\max_{k\in[2], (i,j)\in\cS} |F_k(\Wb^{(t)}; \xb_{i,j})|\le \zeta$ for some $\zeta =o\big(\frac{1}{\polylog(n)}\big)$ and $\zeta > b\alpha$, then recalling the update form in Proposition \ref{prop:update_mixup}, we have
\begin{align*}
&\gamma_1^{(t)}(\vb', \vb') = \Theta(\rho), \quad \gamma_1^{(t)}(\vb, \vb') = \Theta(\rho/P), \quad|\gamma_1^{(t)}(\ub,\vb')| = O(\zeta\rho /P), \notag\\
&|\gamma_1^{(t)}(\ub',\vb')| = O(\zeta\rho^2/P), \quad |\gamma_1^{(t)}(\bxi_s^{(q)}, \vb')| = \tilde O\big(\rho/(Pn)\big).
\end{align*}

\end{lemma}





\begin{proof}[Proof of Lemma \ref{lemma:feature_learning_coefficients_v'_mixup}]
Recalling the definition of $\gamma_1^{(t)}(\vb',\vb')$:
\begin{align*}
\gamma_1^{(t)}(\vb',\vb') = \frac{1}{n^2}\sum_{i,j\in[n]}\ell_{1,(i,j)}^{(t)}\sum_{p\in[P]}[\theta_{i,j}^{(p)}(\vb')]^2.
\end{align*}
Note that the rare feature $\vb'$ will not appear in the form of feature noise, then we will only need to focus on the mixed data $(i,j)$ with either $i\in\cS_1^+$ or $j\in\cS_1^+$, where the rare feature can only appear in the form of $\vb$, $\lambda\vb$, or $(1-\lambda)\vb$. Particularly, regarding the data $(i,j)\in\cS_{1,1}^{+,+}\cup\cS_{1,0}^{+, +}\cup_{0,1}^{+,+}$, let $\cP_{i,j}^*(\vb')$ be the set of patches that contain the feature $\vb'$, we have $|\cP_{i,j}^*(\vb')|=\Theta(1)$ and then
\begin{align*}
\ell_{1, (i,j)}^{(t)}\cdot\sum_{p\in[P]}[\theta_{i,j}^{(p)}(\vb')]^2 = \ell_{1, (i,j)}^{(t)}\cdot\sum_{p\in \cP_{i,j}^*(\vb)}[\theta_{i,j}^{(p)}(\vb')]^2 = \Theta(1),
\end{align*}
where we use the fact that $\ell_{1,(i,j)}^{(t)}=\Theta(1)$ for any $(i,j)\in\cS_{1,1}^{+, +}$.

Regarding the data $(i,j)\in\cS_{1,0}^{+, -}\cup\cS_{1,1}^{+,-}$, we will consider $(i,j)$ and $(j,i)$ together. Particularly, we have
\begin{align*}
\ell_{1, (i,j)}^{(t)}\cdot\sum_{p\in[P]}[\theta_{i,j}^{(p)}(\vb')]^2 + \ell_{1, (j,i)}^{(t)}\cdot\sum_{p\in[P]}[\theta_{j,i}^{(p)}(\vb')]^2 &= \underbrace{\ell_{1,(i,j)}^{(t)}\cdot\sum_{p\in[P]}\Big[[\theta_{i,j}^{(p)}(\vb')]^2 - [\theta_{j,i}^{(p)}(\vb')]^2\Big]}_{I_1}\notag\\
&\qquad + \underbrace{\big[\ell_{1,(i,j)}^{(t)}+\ell_{1,(j,i)}^{(t)}\big]\cdot\sum_{p\in[P]}[\theta_{j,i}^{(p)}(\vb')]^2}_{I_2}.
\end{align*}
Then using the same definition of $\cP_{i,j}^*(\vb)$, we have for any $p\in\cP_{i,j}^*(\vb)$, it holds that $\theta_{i,j}^{(p)}(\vb')=\lambda$ and $\theta_{j,i}^{(p)}(\vb')=1-\lambda$, then
\begin{align*}
I_1 = \Theta(1)\cdot |\cP_{i,j}^*(\vb')|\cdot [\lambda^2 - (1-\lambda)^2] = \Theta(1).
\end{align*}
Regarding $I_2$, we can use the condition that the neural network output is upper bounded by $\zeta$, then 
\begin{align*}
|I_2| = \bigg|\big[\lambda - 0.5 + 0.5 - \lambda \pm O(\zeta)\big]\cdot \sum_{p\in\cP_{i,j}^*(\vb')}[\theta_{j,i}^{(p)}(\vb')]^2\bigg| = O(\zeta).
\end{align*}
Therefore, combining these results for $I_1$ and $I_2$, we can get
\begin{align*}
\ell_{1, (i,j)}^{(t)}\cdot\sum_{p\in[P]}[\theta_{i,j}^{(p)}(\vb')]^2 + \ell_{1, (j,i)}^{(t)}\cdot\sum_{p\in[P]}[\theta_{j,i}^{(p)}(\vb')]^2 = I_1 + I_2 = \Theta(1).
\end{align*}
To complete the analysis, we have
\begin{align*}
\gamma_1^{(t)}(\vb',\vb') &= \frac{1}{n^2}\sum_{i,j\in[n]}\sum_{p\in[P]}[\theta_{i,j}^{(p)}(\vb')]^2\notag\\
& = \frac{1}{n^2}\sum_{i\in\cS_1^+,j\in[n] || i\in[n],j\in\cS_1^+}\sum_{p\in[P]}[\theta_{i,j}^{(p)}(\vb')]^2\notag\\
& = |\cS_{1,1}^{+,+}\cup\cS_{1,0}^{+, +}\cup_{0,1}^{+,+}| + |\cS_{1,0}^{+, -}\cup\cS_{1,1}^{+,-}|\notag\\
& = \Theta(\rho).
\end{align*}

The characterization of $\gamma_1(\vb,\vb')$ and $\gamma_1(\ub,\vb')$ will be exactly the same as $\gamma_1(\vb',\vb)$ and $\gamma_1(\vb', \ub)$ due to the  fact that $\gamma_1(\ab,\bb)=\gamma_1(\bb,\ab)$. Therefore, we can apply Lemmas \ref{lemma:feature_learning_coefficients_v_mixup} and \ref{lemma:feature_learning_coefficients_u_incorrect_mixup} to get the desired results.

% Next we will prove the bound for $\gamma_1(\vb,\vb')$. Particularly, we will focus on the data patches that contain both $\vb$ and $\vb'$, which can be categorized into two cases: (1) mixup between $\vb$ and $\vb'$; (2) mixup between $\alpha\vb$ and $\vb'$.  For the first case, it will only appear in the mixed data $(i,j)\in\cS_{0,1}^{+, +}$ and $\cS_{1, 0}^{+,+}$, which satisfy $\ell_{i,j}^{(t)}=\Theta(1)$. Moreover, it can be obtained that the number of patches that fall in case (1), among the total $n^2P$ patches, is $\Theta(\rho n^2/P)$, while the number of patches that fall in case (2) is $\Theta(\rho b n^2/P)$. Therefore, we can get that
% \begin{align*}
% \gamma_1^{(t)}(\vb,\vb') = \frac{1}{n^2}\sum_{i,j\in[n]}\ell_{1,(i,j)}^{(t)}\sum_{p\in[P]}\theta_{i,j}^{(p)}(\vb)\cdot \theta_{i,j}^{(p)}(\vb').
% \end{align*}
% Note that in case (1), we have $\theta_{i,j}^{(p)}(\vb)\cdot \theta_{i,j}^{(p)}(\vb')=\lambda(1-\lambda)=\Theta(1)$ while in case (2) we have $\theta_{i,j}^{(p)}(\vb)\cdot \theta_{i,j}^{(p)}(\vb')=\Theta(\alpha)$. Therefore, it follows that
% \begin{align*}
% \gamma_1(\vb,\vb') = \frac{1}{n^2}\cdot\bigg[\Theta\bigg(\frac{\rho n^2 }{P}\bigg)\cdot \Theta(1) \pm O\bigg(\frac{\rho b n^2}{P}\bigg)\cdot\Theta(\alpha)\bigg] = \Theta\bigg(\frac{\rho }{P}\bigg),
% \end{align*}
% where the last equality holds since $b\alpha=O(1)$. 

% Next we will study $\gamma_1^{(t)}(\ub,\vb')$. We will also consider two cases: (1) mixup between $\ub$ and $\vb'$; (2) mixup between $\alpha\ub$ and $\vb'$. Then it can be seen that the first cases cover the data $(i,j)\in\cS_{1,0}^{+,-}$ and $(i,j)\in\cS_{0,1}^{-, +}$, which is equivalent to the dataset $\{(i,j), (j,i): (i,j)\in\cS_{1,0}^{+,-}\}$. Therefore, we will handle the data $(i,j)$ and $(j,i)$ together in this case. In particular, we have
% \begin{align*}
% &\ell_{1,(i,j)}^{(t)}\sum_{p\in[P]}\theta_{i,j}^{(p)}(\ub)\cdot \theta_{i,j}^{(p)}(\vb') + \ell_{1,(j,i)}^{(t)}\sum_{p\in[P]}\theta_{j,i}^{(p)}(\ub)\cdot \theta_{j,i}^{(p)}(\vb')\notag\\
% & = \ell_{1,(i,j)}^{(t)}\sum_{p\in[P]}\big[\theta_{i,j}^{(p)}(\ub)\cdot \theta_{i,j}^{(p)}(\vb') - \theta_{j,i}^{(p)}(\ub)\cdot \theta_{j,i}^{(p)}(\vb')\big] + \big[\ell_{1,(i,j)}^{(t)}+\ell_{1,(j,i)}^{(t)}\big]\cdot\sum_{p\in[P]}\theta_{j,i}^{(p)}(\ub)\cdot \theta_{j,i}^{(p)}(\vb').
% \end{align*}
% It is clear that the first term on the R.H.S. of the above equation is zero since in case (1) 
% \begin{align*}
% \theta_{i,j}^{(p)}(\ub)\cdot \theta_{i,j}^{(p)}(\vb') = \theta_{j,i}^{(p)}(\ub)\cdot \theta_{j,i}^{(p)}(\vb') = \lambda(1-\lambda).
% \end{align*}
% Regarding the second term, we can use the fact that the number of patches falling in case (1) is $\Theta(\rho n^2/P)$ and $|\ell_{1,(i,j)}^{(t)} + \ell_{1,(i,j)}^{(t)}|=O(\zeta)$. 

% Regarding case (2), we can follow the exact analysis of the case (2) for $\gamma_1^{(t)}(\vb,\vb')$, which gives $\theta_{i,j}^{(p)}(\ub)\cdot\theta_{i,j}^{(p)}(\vb')=\Theta(\alpha)$. Therefore, we can finally get
% \begin{align*}
% |\gamma_1^{(t)}(\ub,\vb')|&=\bigg|\frac{1}{n^2}\sum_{(i,j)\in\cS_{0,1}^{-,+}\cup\cS_{1,0}^{+,-}} \ell_{1,(i,j)}^{(t)}\sum_{p\in[P]}\theta_{i,j}^{(p)}(\ub)\cdot \theta_{i,j}^{(p)}(\vb')\bigg|\notag\\
% &= O\bigg(\frac{\rho }{P}\bigg)\cdot\Theta(\zeta) + O\bigg(\frac{\rho b }{P}\bigg)\cdot\Theta(\alpha)\notag\\
% &= \Theta\bigg(\frac{\zeta\rho  }{P}\bigg),
% \end{align*}
% where we use the fact that $\zeta = \omega(b\alpha)$. 

Regarding the proof for $\gamma_1^{(t)}(\ub',\vb')$, we will follow a similar proof for $\gamma_1^{(t)}(\ub',\vb)$ in Lemma \ref{lemma:feature_learning_coefficients_v_mixup}, while two differences need to be considered: (1) the rare feature vectors $\ub'$ and $\vb'$ will not appear in the form of feature noise, thus we only need to consider the data $(i,j)\in\cS_{1,1}^{+,-}\cup\cS_{1,1}^{-,+}$; (2) the cardinality of the critical subset of data satisfies $|\cS_{1,1}^{+,-}\cup\cS_{1,1}^{-,+}|=\rho^2 n^2$. Therefore, for any $(i,j)\in\cS_{1,1}^{+,-}$, we have
\begin{align*}
&\ell_{1,(i,j)}^{(t)}\sum_{p\in[P]}\theta_{i,j}^{(p)}(\ub')\cdot \theta_{i,j}^{(p)}(\vb') + \ell_{1,(j,i)}^{(t)}\sum_{p\in[P]}\theta_{j,i}^{(p)}(\ub')\cdot \theta_{j,i}^{(p)}(\vb')\notag\\
& = \ell_{1,(i,j)}^{(t)}\sum_{p\in[P]}\big[\theta_{i,j}^{(p)}(\ub')\cdot \theta_{i,j}^{(p)}(\vb') - \theta_{j,i}^{(p)}(\ub')\cdot \theta_{j,i}^{(p)}(\vb')\big] + \big[\ell_{1,(i,j)}^{(t)}+\ell_{1,(j,i)}^{(t)}\big]\cdot\sum_{p\in[P]}\theta_{j,i}^{(p)}(\ub')\cdot \theta_{j,i}^{(p)}(\vb').
\end{align*}
It is easy to see that $\theta_{i,j}^{(p)}(\ub')\cdot \theta_{i,j}^{(p)}(\vb') = \theta_{j,i}^{(p)}(\ub')\cdot \theta_{j,i}^{(p)}(\vb')=\lambda(1-\lambda)$. Besides, we have in total $\rho^2n^2/P$ patches that consist of both $\ub'$ and $\vb'$. This further implies that
\begin{align*}
|\gamma_1^{(t)}(\ub',\vb')| &= \bigg|\frac{1}{n^2}\sum_{(i,j)\in\cS_{1,1}^{+,-}\cup\cS_{1,1}^{-,+}}\big[\ell_{1,(i,j)}^{(t)}+\ell_{1,(j,i)}^{(t)}\big]\cdot\sum_{p\in[P]}\theta_{j,i}^{(p)}(\ub')\cdot\theta_{j,i}^{(p)}(\vb')\bigg|\notag\\
& = O\bigg(\frac{\rho^2}{P}\bigg)\cdot O(\zeta) \notag\\
& = O\bigg(\frac{\zeta\rho^2}{P}\bigg),
\end{align*}
where we use the fact that $|\ell_{1,(i,j)}^{(t)}+\ell_{1,(j,i)}^{(t)}|=O(\zeta)$.

Lastly, we will characterize $\gamma_1^{(t)}(\bxi_s^{(q)},\vb')$. First recall its definition:
\begin{align*}
\gamma_1^{(t)}(\bxi_s^{(q)},\vb') &= \frac{1}{n^2}\sum_{i,j\in[n]}\ell_{1,(i,j)}^{(t)}\sum_{p\in[P]}\theta_{i,j}^{(p)}(\bxi_s^{(q)})\cdot\theta_{i,j}^{(p)}(\vb')\notag\\
& = \underbrace{\frac{1}{n^2}\sum_{p=q,i=s||p=q,j=s}\ell_{1,(i,j)}^{(t)}\theta_{i,j}^{(p)}(\bxi_s^{(q)})\cdot\theta_{i,j}^{(p)}(\vb')}_{I_1} +\underbrace{\frac{1}{n^2}\sum_{p\neq q||i\neq s, j\neq s}\ell_{1,(i,j)}^{(t)}\sum_{p\in[P]}\theta_{i,j}^{(p)}(\bxi_s^{(q)})\cdot\theta_{i,j}^{(p)}(\vb')}_{I_2} .
\end{align*}
Note that for any fixed $\bxi_s^{(p)}$, it will be mixed with $n$ data patches in total, while, by Lemma \ref{lemma:occurance_mixed_patch}, we know that there are only $\Theta(\rho/P)$ fraction among them are $\vb'$. Using the fact that $|\ell_{1,(i,j)}^{(t)}|\le 1$, we have
\begin{align*}
|I_1|\le \frac{1}{n} \cdot \Theta(\rho/P) = O\bigg(\frac{\rho}{Pn}\bigg).
\end{align*}
Besides, note that $|\theta_{i,j}^{(p)}(\bxi_s^{(q)})| = \tilde O(d^{-1/2})$ if $i,j\neq s$ or $p\neq q$, we have
\begin{align*}
|I_2|\le \tilde O\bigg(\frac{\rho P}{d^{1/2}}\bigg) = O\bigg(\frac{\rho}{Pn}\bigg) ,
\end{align*}
where the last equality is by the assumption that $d\ge P^4n^2$.
Combining the above results for $I_1$ and $I_2$, we can get
\begin{align*}
|\gamma_1^{(t)}(\bxi_s^{(q)},\vb')| = O\bigg(\frac{\rho}{Pn}\bigg).
\end{align*}
\end{proof}

Following the exactly same procedure, we can get the following results regarding the learning of $\ub'$.
\begin{lemma}\label{lemma:feature_learning_coefficients_u'_mixup}
Assume $\max_{k\in[2], (i,j)\in\cS} |F_k(\Wb^{(t)}; \xb_{i,j})|\le \zeta$ for some $\zeta =o\big(\frac{1}{\polylog(n)}\big)$ and $\zeta > b\alpha$, then recalling the update form in Proposition \ref{prop:update_mixup}, we have
\begin{align*}
&\gamma_2^{(t)}(\ub', \ub') = \Theta(\rho), \quad \gamma_2^{(t)}(\ub, \ub') = \Theta(\rho/P), \quad|\gamma_2^{(t)}(\vb,\ub')| = O(\zeta\rho /P), \notag\\
&|\gamma_2^{(t)}(\vb',\ub')| = O(\zeta\rho^2/P), \quad |\gamma_2^{(t)}(\bxi_s^{(q)}, \ub')| = \tilde O\big(\rho/(Pn)\big).
\end{align*}

\end{lemma}














\subsubsection{Incorrect Rare Feature Learning}
In contrast to the previous section that studies $\la\wb_{1,r}^{(t)},\vb'\ra$ and $\la\wb_{2,r}^{(t)},\ub'\ra$, the incorrect rare feature learning aims to characterize the quantities $\la\wb_{2,r}^{(t)},\vb'\ra$ and $\la\wb_{1,r}^{(t)},\ub'\ra$. Similar to the proof of Lemmas \ref{lemma:feature_learning_coefficients_v_incorrect_mixup} and  \ref{lemma:feature_learning_coefficients_u_incorrect_mixup}, we only need to replace $\ell_{1,(i,j)}^{(t)}$ with $\ell_{2,(i,j)}^{(t)}=-\ell_{1,(i,j)}^{(t)}$ or $\ell_{2,(i,j)}^{(t)}$ with $\ell_{1,(i,j)}^{(t)}=-\ell_{2,(i,j)}^{(t)}$. Based on this, the update of $\la\wb_{2,r}^{(t)},\vb'\ra$ and $\la\wb_{1,r}^{(t)},\ub'\ra$ in each iteration are characterized in the following lemmas.

\begin{lemma}\label{lemma:feature_learning_coefficients_v'_incorrect_mixup}
Assume $\max_{k\in[2], (i,j)\in\cS} |F_k(\Wb^{(t)}; \xb_{i,j})|\le \zeta$ for some $\zeta =o\big(\frac{1}{\polylog(n)}\big)$ and $\zeta > b\alpha$, then recalling the update form in Proposition \ref{prop:update_mixup}, we have
\begin{align*}
&\gamma_2^{(t)}(\vb', \vb') = -\Theta(\rho), \quad \gamma_2^{(t)}(\vb, \vb') = -\Theta(\rho/P), \quad|\gamma_2^{(t)}(\ub,\vb')| = O(\zeta\rho /P), \notag\\
&|\gamma_2^{(t)}(\ub',\vb')| = O(\zeta\rho^2/P), \quad |\gamma_2^{(t)}(\bxi_s^{(q)}, \vb')| = \tilde O\big(\rho/(Pn)\big).
\end{align*}

\end{lemma}
\begin{lemma}\label{lemma:feature_learning_coefficients_u'_incorrect_mixup}
Assume $\max_{k\in[2], (i,j)\in\cS} |F_k(\Wb^{(t)}; \xb_{i,j})|\le \zeta$ for some $\zeta =o\big(\frac{1}{\polylog(n)}\big)$ and $\zeta > b\alpha$, then recalling the update form in Proposition \ref{prop:update_mixup}, we have
\begin{align*}
&\gamma_1^{(t)}(\ub', \ub') = -\Theta(\rho), \quad \gamma_1^{(t)}(\ub, \ub') = -\Theta(\rho/P), \quad|\gamma_1^{(t)}(\vb,\ub')| = O(\zeta\rho /P), \notag\\
&|\gamma_1^{(t)}(\vb',\ub')| = O(\zeta\rho^2/P), \quad |\gamma_1^{(t)}(\bxi_s^{(q)},\ub'| = \tilde O\big(\rho/(Pn)\big).
\end{align*}

\end{lemma}


\subsubsection{Noise Learning}

\begin{lemma}\label{lemma:noise_learning_coefficients_incorrect_mixup}
Assume $\max_{k\in[2], (i,j)\in\cS} |F_k(\Wb^{(t)}; \xb_{i,j})|\le \zeta$ for some $\zeta =o\big(\frac{1}{\polylog(n)}\big)$ and $\zeta > b\alpha$, then recalling the update form in Proposition \ref{prop:update_mixup}, for any $\bxi_s^{(q)}$ with $y_s=1$, we have
\begin{align*}
&\gamma_1^{(t)}(\bxi_s^{(q)}, \bxi_s^{(q)}) = \frac{d\sigma_p^2\cdot [n\lambda^3 - (2\lambda-1)(1-\lambda)^2]}{2n^2} \pm \tilde O\bigg(\frac{\zeta d\sigma_p^2}{n}\bigg)\notag\\
&|\gamma_1^{(t)}(\vb, \bxi_s^{(q)})| = O\big(d\sigma_p^2/(Pn)\big), \quad|\gamma_1^{(t)}(\ub, \bxi_s^{(q)})| = O\big(d\sigma_p^2/(Pn)\big),\quad |\gamma_1^{(t)}(\vb', \bxi_s^{(q)})| = O\big(d\sigma_p^2\rho/(Pn)\big), \notag\\
&|\gamma_1^{(t)}(\ub', \bxi_s^{(q)})| = O\big(d\sigma_p^2\rho/(Pn)\big), \quad |\gamma_1^{(t)}(\bxi_i^{(q)}, \bxi_s^{(q)})| = \ind[y_i=y_s] \cdot \frac{\lambda(1-\lambda)d\sigma_p^2}{n^2} \pm O\bigg(\frac{\zeta d\sigma_p^2}{n^2}\bigg).
\end{align*}

\end{lemma}

\begin{proof}[Proof of Lemma \ref{lemma:noise_learning_coefficients_incorrect_mixup}]
Without loss of generality, we assume $y_s=1$. According to the definition of $\gamma_1^{(t)}\big(\bxi_s^{(q)},\bxi_s^{(q)}\big)$, we have
\begin{align*}
\gamma_1^{(t)}\big(\bxi_s^{(q)},\bxi_s^{(q)}\big) &= \frac{1}{n^2}\sum_{i,j\in[n]}\ell_{1,(i,j)}^{(t)}\sum_{p\in[P]}[\theta_{i,j}^{(p)}(\bxi_s^{(q)})]^2\cdot \|\bxi_s^{(q)}\|_2^2\notag\\
& = \frac{ \|\bxi_s^{(q)}\|_2^2}{n^2}\cdot \bigg(\sum_{i\in[n]}\ell_{1,(s, i)}^{(t)}[\theta_{s, i}^{(q)}(\bxi_s^{(q)})]^2+\sum_{i\neq s}\ell_{1,(i, s)}^{(t)}[\theta_{i, s}^{(q)}(\bxi_s^{(q)})]^2\bigg)\notag\\
& = \frac{ \|\bxi_s^{(q)}\|_2^2}{n^2}\cdot \bigg(\lambda^2\cdot\sum_{i\in[n]}\ell_{1,(s,i)}^{(t)}+(1-\lambda)^2\cdot\sum_{i\neq s}\ell_{1,(i,s)}^{(t)}\bigg)\notag\\
& = \frac{ \|\bxi_s^{(q)}\|_2^2}{n^2}\cdot\big[ 0.5n\lambda^3 - (\lambda-0.5)(1-\lambda)^2 \pm O(n\zeta)  \big]\notag\\
& = \frac{ \|\bxi_s^{(q)}\|_2^2\cdot[n\lambda^3 - (2\lambda-1)(1-\lambda)^2]}{2n^2} \pm O\bigg(\frac{\zeta \|\bxi_s^{(q)}\|_2^2}{n}\bigg),
 \end{align*}
 where the second equation is due to the fact that only $\xb_{i,s}$ or $\xb_{s,i}$ will contain the component of $\bxi_s^{(q)}$, the fourth inequality holds since we assume there have $n/2$ positive samples and $n/2$ negative samples in the training data.
Moreover, note that $\bxi_s^{(q)}\sim N(0,\sigma_p^2\Ib)$,  applying union bound over all $s\in[n]$ and $p\in[P]$, we can get that with probability at least $1-1/\poly(n)$, we have
\begin{align*}
\big|\|\bxi_s^{(q)}\|_2^2-d\sigma_p^2\big| \le \polylog(n)\cdot d^{1/2}\sigma_p^2. 
\end{align*}
Therefore, it follows that for all $s\in[n]$ and $p\in[P]$, with probability at least $1-1/\poly(n)$,
\begin{align*}
\gamma_1^{(t)}\big(\bxi_s^{(q)},\bxi_s^{(q)}\big)& = \frac{d\sigma_p^2\cdot [n\lambda^3 - (2\lambda-1)(1-\lambda)^2]}{2n^2} \pm \tilde O\bigg(\frac{d^{1/2}\sigma_p^2}{n}+\frac{\zeta d\sigma_p^2}{n}\bigg)\notag\\
&= \frac{d\sigma_p^2\cdot [n\lambda^2-(1-\lambda)^2]}{2n^2} \pm \tilde O\bigg(\frac{\zeta d\sigma_p^2}{n}\bigg),
\end{align*}
where we use the fact that $\zeta = \omega(d^{-1/2})$.

Regarding $\gamma_1^{(t)}(\bxi_i^{(q)}, \bxi_s^{(q)})$, we have
\begin{align*}
\gamma_1^{(t)}(\bxi_i^{(q)}, \bxi_s^{(q)}) &= \frac{\lambda(1-\lambda)}{n^2}\cdot[\ell_{1,(i,s)}^{(t)}+\ell_{1,(s,i)}^{(t)}]\cdot \|\bxi_s^{(q)}\|_2^2\notag\\
& =\ind[y_i=y_s] \cdot \frac{\lambda(1-\lambda)d\sigma_p^2}{n^2} \pm O\bigg(\frac{\zeta d\sigma_p^2}{n^2}\bigg).
\end{align*}

Regarding the remaining quantities, we can directly apply the aforementioned lemmas on the learning of common and rare features, since the following holds 
\begin{align*}
\gamma_1^{(t)}(\ab, \bxi_s^{(q)}) = \gamma_1^{(t)}(\bxi_s^{(q)},\ab)\cdot \|\bxi_s^{(q)}\|_2^2 = \gamma_1^{(t)}(\bxi_s^{(q)},\ab)\cdot\tilde O(d\sigma_p^2),
\end{align*}
where $\ab\in\{\vb,\ub,\vb',\ub'\}$.
This completes the proof.


\end{proof}







% \subsubsection{Incorrect Noise Learning}




\subsection{Outcome of Phase 1 Mixup Training.}

In this part, we will provide the outcome of Phase 1 mixup training. 

% In particular, we first state the arguments as the hypotheses and then verify them via mathematical induction.
% \begin{hypothesis}
% \begin{enumerate}
%     \item 
% \end{enumerate}
% \end{hypothesis}


We first recall Proposition \ref{prop:update_mixup} and Lemma \ref{lemma:feature_learning_coefficients_v_mixup} to obtain the learning dynamics of the common feature vector $\vb$.
\begin{align*}
\la\wb_{1,r}^{(t+1)},\vb\ra  &= \la\wb_{1,r}^{(t)},\vb\ra - \eta\cdot\la\nabla_{\wb_{1,r}}L_\cS(\Wb^{(t)}),\vb\ra\notag\\
& = \big[1 + \eta\gamma_1^{(t)}(\vb,\vb)\big]\cdot\la\wb_{1,r}^{(t)},\vb\ra + \eta\gamma_1^{(t)}(\ub,\vb)\cdot\la\wb_{1,r}^{(t)},\ub\ra + \eta\gamma_1^{(t)}(\vb',\vb)\cdot\la\wb_{1,r}^{(t)},\vb'\ra \notag\\
&\qquad + \eta\gamma_1^{(t)}(\ub',\vb)\cdot\la\wb_{1,r}^{(t)}, \ub'\ra + \sum_{i=1}^n\sum_{p\in[P]}\eta\gamma_1^{(t)}(\bxi_i^{(p)},\vb)\cdot\la\wb_{1,r}^{(t)},\bxi_i^{(p)}\ra.
\end{align*}

Then it can be seen that the most complicated part in the above update form is the composition of noise learning, i.e., $\la\wb_{1,r}^{(t)},\bxi_i^{(p)}\ra$.The following lemma provides an upper bound on the term $\sum_{i=1}^n\sum_{p\in[P]}\eta\gamma_1^{(t)}(\bxi_i^{(p)},\vb)\cdot\la\wb_{1,r}^{(t)},\bxi_i^{(p)}\ra$, which will leverage the randomness of $\bxi_i^{(p)}$ at the initialization.
\begin{lemma}\label{lemma:update_sum_noise}
Assume $\max_{k\in[2], (i,j)\in\cS} |F_k(\Wb^{(t)}; \xb_{i,j})|\le \zeta$ for some $\zeta\in\big[\omega\big((nP)^{-1/2}\big), o\big(\frac{1}{\polylog(n)}\big)\big]$. Let $z_t:=\sum_{i=1}^n\sum_{p\in[P]}\gamma_1^{(t)}(\bxi_i^{(p)},\vb)\cdot\la\wb_{1,r}^{(t)},\bxi_i^{(p)}\ra$, then we have with probability at least $1-1/\poly(n)$, for all $t = O\big(n\eta^{-1}/(d\sigma_p^2)\big)$, we have
\begin{align*}
|z_t|&\le O\bigg(\frac{d^{1/2}\sigma_0\sigma_p}{P^{1/2}n^{1/2}}\bigg) +O\bigg(\frac{\zeta}{Pn}\bigg)\cdot\sum_{s=1}^n\sum_{p\in[P]}|\la\wb_{1,r}^{(t)},\bxi_s^{(p)}\ra|+ O\bigg(\frac{\eta d\sigma_p^2}{Pn}\bigg)\cdot\sum_{\tau=0}^{t-1}\big[|\la\wb_{1,r}^{(\tau)},\vb\ra|+|\la\wb_{1,r}^{(\tau)},\ub\ra|\big] \notag\\
&\qquad+ O\bigg(\frac{\eta\rho d\sigma_p^2}{Pn}\bigg)\cdot\sum_{\tau=0}^{t-1}\big[|\la\wb_{1,r}^{(\tau)},\vb'\ra|+|\la\wb_{1,r}^{(\tau)},\ub'\ra|\big]+ O\bigg(\frac{\eta\zeta d\sigma_p^2}{n^2}\bigg)\cdot\sum_{\tau=0}^{t-1}\sum_{p\in[P]}\sum_{s=1}^n |\la\wb_{1,r}^{(\tau)},\bxi_s^{(p)}\ra|.
\end{align*}
\end{lemma}
\begin{proof}
Based on the definition of $z_t$, we can conduct the following decomposition:
\begin{align*}
z_t &= \sum_{i=1}^n\sum_{p\in[P]}\gamma_1^{(t)}(\bxi_i^{(p)},\vb)\cdot\la\wb_{1,r}^{(t)},\bxi_i^{(p)}\ra\notag\\
& = \frac{1}{n^2}\sum_{i'=1}^n\sum_{q\in[P]}\sum_{i,j\in[n]}\ell_{1,(i,j)}^{(t)}\sum_{p\in[P]}\theta_{i,j}^{(p)}(\bxi_{i'}^{(q)})\cdot\theta_{i,j}^{(p)}(\vb)\cdot\la\wb_{1,r}^{(t)},\bxi_{i'}^{(q)}\ra.
\end{align*}
Note that during the initial training phase $\ell_{1, (i,j)}^{(t)}$ is close to the constant $l_{1,(i,j)}\in\{0.5, -0.5, 0.5-\lambda, \lambda-0.5\}$, which is independent of the random noise vectors $\{\bxi\}$ and random initial weights $\{\wb_{1,r}^{(0)}\}_{r\in[m]}$. Then using the fact that $|\ell_{1, (i,j)}^{(t)}-l_{1,(i,j)}| = O(\zeta)$, we can get
\begin{align*}
|z_0| & \le  \bigg|\underbrace{\frac{1}{n^2}\sum_{i'=1}^n\sum_{q\in[P]}\sum_{i,j\in[n]}l_{1,(i,j)}\sum_{p\in[P]}\theta_{i,j}^{(p)}(\bxi_{i'}^{(q)})\cdot\theta_{i,j}^{(p)}(\vb)\cdot\la\wb_{1,r}^{(0)},\bxi_{i'}^{(q)}\ra}_{I_1}\bigg| \notag\\
&\qquad + \underbrace{\bigg|\frac{1}{n^2}\sum_{i'=1}^n\sum_{q\in[P]}\sum_{i,j\in[n]}[\ell_{1,(i,j)}^{(0)}-l_{1,(i,j)}]\sum_{p\in[P]}\theta_{i,j}^{(p)}(\bxi_{i'}^{(q)})\cdot\theta_{i,j}^{(p)}(\vb)\cdot\la\wb_{1,r}^{(0)},\bxi_{i'}^{(q)}\ra\bigg|}_{I_2}.
\end{align*}
Regarding $I_1$, note that $\ell_{1,(i,j)}$, $\theta_{i,j}^{(p)}(\bxi_{i'}^{(q)})$, and $\theta_{i,j}^{(p)}(\vb)$ are independent of the random noise vectors $\{\bxi\}$ and random initial weights $\{\wb_{1,r}^{(0)}\}_{r\in[m]}$. Besides, note that the inner products $\{\la\wb_{1,r}^{(0)},\bxi_{i'}^{(q)}\ra\}_{i'\in[n], q\in[P]}$ are independent conditioning on $\wb_{1,r}^{(0)}$ and for all $i'\in[n]$ and $q\in[P]$, . We can apply standard concentration arguments to get the upper bound of $I_1$. Before approaching this, we first apply Lemma \ref{lemma:occurance_mixed_patch} and follow the similar proof of Lemma \ref{lemma:feature_learning_coefficients_v_mixup}, and obtain that with probability at least $1-1/\poly(n)$
\begin{align}\label{eq:bound_sum_coefficients_noise}
\bigg|\frac{1}{n^2}\sum_{i,j\in[n]}l_{1,(i,j)}\sum_{p\in[P]}\theta_{i,j}^{(p)}(\bxi_{i'}^{(q)})\cdot\theta_{i,j}^{(p)}(\vb)\bigg| = O\bigg(\frac{1}{nP}\bigg).
\end{align}
Then performing the following decomposition on $I_1$ according to the value of $y_{i'}$:
\begin{align*}
I_1 &= \underbrace{\frac{1}{n^2}\sum_{i':y_{i'}=1}\sum_{q\in[P]}\sum_{i,j\in[n]}l_{1,(i,j)}\sum_{p\in[P]}\theta_{i,j}^{(p)}(\bxi_{i'}^{(q)})\cdot\theta_{i,j}^{(p)}(\vb)\cdot\la\wb_{1,r}^{(0)},\bxi_{i'}^{(q)}\ra}_{I_1^{(1)}}\notag\\
&\qquad + \underbrace{\frac{1}{n^2}\sum_{i':y_{i'}=2}\sum_{q\in[P]}\sum_{i,j\in[n]}l_{1,(i,j)}\sum_{p\in[P]}\theta_{i,j}^{(p)}(\bxi_{i'}^{(q)})\cdot\theta_{i,j}^{(p)}(\vb)\cdot\la\wb_{1,r}^{(0)},\bxi_{i'}^{(q)}\ra}_{I_1^{(2)}}.
\end{align*}
Therefore, note that conditioning on $\wb_{1,r}^{(0)}$, the quantity $\la\wb_{1,r}^{(0)},\bxi_{i'}^{(q)}\ra$ is $\|\wb_{1,r}^{(0)}\|_2\cdot \sigma_p$-subGaussian, by \eqref{eq:bound_sum_coefficients_noise}, we can immediately get that both $I_1^{(1)}$ and $I_1^{(2)}$ are $\|\wb_{1,r}^{(0)}\|_2\cdot \sigma_p\cdot (nP)^{-1/2}$-subGuassian. Then using the fact that $\wb_{1,r}^{(0)}\in N(0,\sigma_0^2\Ib)$, we can get that with probability at least $1-1/\poly(n)$, 
\begin{align}\label{eq:bound_I1_noise_sum}
|I_1^{(1)}|,|I_1^{(2)}|  \le \tilde O\bigg(\frac{d^{1/2}\sigma_0\sigma_p}{(nP)^{1/2}}\bigg).
\end{align}

Regarding $I_2$, we can also apply Lemma \ref{lemma:occurance_mixed_patch} and follow the similar proof of Lemma \ref{lemma:feature_learning_coefficients_v_mixup}, then with probability at least $1-1/\poly(n)$,
\begin{align*}
\bigg|\frac{1}{n^2}\sum_{i,j\in[n]}[\ell_{1,(i,j)}^{(0)}-l_{1,(i,j)}]\sum_{p\in[P]}\theta_{i,j}^{(p)}(\bxi_{i'}^{(q)})\cdot\theta_{i,j}^{(p)}(\vb)\bigg| = O\bigg(\frac{\max_{i,j}|\ell_{1,(i,j)}^{(0)}-l_{1,(i,j)}|}{nP}\bigg)=O\bigg(\frac{\zeta}{nP}\bigg).
\end{align*}
This further implies that
\begin{align}\label{eq:bound_I2_noise_sum}
I_2 \le O\bigg(\frac{\zeta}{nP}\bigg) \cdot \sum_{i'=1}^n\sum_{q\in[P]}|\la\wb_{1,r}^{(0)},\bxi_{i'}^{(q)}\ra|\le \tilde O\big(\zeta d^{1/2}\sigma_0\sigma_p\big).
\end{align}
where we use the fact that $\wb_{1,r}^{(0)}\sim N(0,\sigma_0^2\Ib)$ and $\bxi_{i'}^{(q)}\sim N(0,\sigma_p^2\Ib)$. Combining \eqref{eq:bound_I1_noise_sum} and \eqref{eq:bound_I2_noise_sum} leads to
\begin{align*}
|z_0|\le \tilde O\bigg(\frac{d^{1/2}\sigma_0\sigma_p}{(nP)^{1/2}} + \zeta d^{1/2}\sigma_0\sigma_p\bigg)=O\big(\zeta d^{1/2}\sigma_0\sigma_p\big).
\end{align*}
where we use the condition that $\zeta = \omega\big((nP)^{-1/2}\big)$. 

Next we will move on to study the update of $z_t$ using the update results of $\la\wb_{1,r}^{(t)},\bxi_i^{(p)}\ra$ in Lemma \ref{lemma:noise_learning_coefficients_incorrect_mixup}. Particularly, we can again use the quantities $l_{1,(i,j)}$'s and get the following decomposition
\begin{align*}
z_t &= \underbrace{\frac{1}{n^2}\sum_{i'=1}^n\sum_{q\in[P]}\sum_{i,j\in[n]}l_{1,(i,j)}\sum_{p\in[P]}\theta_{i,j}^{(p)}(\bxi_{i'}^{(q)})\cdot\theta_{i,j}^{(p)}(\vb)\cdot\la\wb_{1,r}^{(t)},\bxi_{i'}^{(q)}\ra}_{I_3} \notag\\
& \qquad + \underbrace{\frac{1}{n^2}\sum_{i'=1}^n\sum_{q\in[P]}\sum_{i,j\in[n]}\big[\ell_{1,(i,j)}^{(t)} - l_{1,(i,j)}\big]\sum_{p\in[P]}\theta_{i,j}^{(p)}(\bxi_{i'}^{(q)})\cdot\theta_{i,j}^{(p)}(\vb)\cdot\la\wb_{1,r}^{(t)},\bxi_{i'}^{(q)}\ra}_{I_4}.
\end{align*}
Recall the update results of $\la\wb_{1,r}^{(t)},\bxi_i^{(p)}\ra$ in Lemma \ref{lemma:noise_learning_coefficients_incorrect_mixup}: for any $y_i=1$,
\begin{align}\label{eq:update_positive_noise}
\la\wb_{1,r}^{(t+1)},\bxi_i^{(p)}\ra   &= \bigg[1 + \eta\cdot\bigg(\frac{d\sigma_p^2\cdot [n\lambda^3 - (2\lambda-1)(1-\lambda)^2]}{2n^2} \pm \tilde O\bigg(\frac{\zeta d\sigma_p^2}{n}\bigg)\bigg)\bigg]\cdot \la\wb_{1,r}^{(t)},\bxi_i^{(p)}\ra\pm O\bigg(\frac{\eta d\sigma_p^2}{Pn}\bigg)\cdot\la\wb_{1,r}^{(t)},\vb\ra  \notag\\
& \qquad\pm O\bigg(\frac{\eta d\sigma_p^2}{Pn}\bigg)\cdot\la\wb_{1,r}^{(t)},\ub\ra \pm O\bigg(\frac{\eta d\sigma_p^2\rho}{Pn}\bigg)\cdot\la\wb_{1,r}^{(t)},\vb'\ra \pm O\bigg(\frac{\eta d\sigma_p^2\rho}{Pn}\bigg)\cdot\la\wb_{1,r}^{(t)},\ub'\ra\notag\\
&\qquad +  O\bigg(\frac{\eta\lambda(1-\lambda)d\sigma_p^2}{n^2}\bigg)\sum_{s:y_s=1} \la\wb_{1,r}^{(t)},\bxi_s^{(p)}\ra \pm O\bigg(\frac{\eta\zeta d\sigma_p^2}{n^2}\bigg)\cdot\sum_{s=1}^n |\la\wb_{1,r}^{(t)},\bxi_s^{(p)}\ra| .
\end{align}
For any $y_i=2$, we have
\begin{align}\label{eq:update_negative_noise}
\la\wb_{1,r}^{(t+1)},\bxi_i^{(p)}\ra   &= \bigg[1 - \eta\cdot\bigg(\frac{d\sigma_p^2\cdot [n\lambda^3 - (2\lambda-1)(1-\lambda)^2]}{2n^2} \pm \tilde O\bigg(\frac{\zeta d\sigma_p^2}{n}\bigg)\bigg)\bigg]\cdot \la\wb_{1,r}^{(t)},\bxi_i^{(p)}\ra \pm O\bigg(\frac{\eta d\sigma_p^2}{Pn}\bigg)\cdot\la\wb_{1,r}^{(t)},\vb\ra\notag\\
& \qquad \pm O\bigg(\frac{\eta d\sigma_p^2}{Pn}\bigg)\cdot\la\wb_{1,r}^{(t)},\ub\ra \pm O\bigg(\frac{\eta d\sigma_p^2\rho}{Pn}\bigg)\cdot\la\wb_{1,r}^{(t)},\vb'\ra\pm O\bigg(\frac{\eta d\sigma_p^2\rho}{Pn}\bigg)\cdot\la\wb_{1,r}^{(t)},\ub'\ra\notag\\
&\qquad - O\bigg(\frac{\eta\lambda(1-\lambda)d\sigma_p^2}{n^2}\bigg)\cdot\sum_{s:y_s=1} \la\wb_{1,r}^{(t)},\bxi_s^{(p)}\ra \pm O\bigg(\frac{\eta\zeta d\sigma_p^2}{n^2}\bigg)\cdot\sum_{s=1}^n |\la\wb_{1,r}^{(t)},\bxi_s^{(p)}\ra| .
\end{align}

We first prove the bound of the quantity $ \sum_{p\in[P]}\sum_{s:y_s=1}\la\wb_{1,r}^{(t)},\bxi_s^{(p)}\ra$. First, using the standard concentration result gives $|\sum_{p\in[P]}\sum_{s:y_s=1}\la\wb_{1,r}^{(0)},\bxi_s^{(p)}\ra|=\tilde O\big(d^{1/2}\sigma_0\sigma_pP^{1/2}n^{1/2}\big)$. Then, by the above update rule, we can get
\begin{align*}
\sum_{p\in[P]}\sum_{s:y_s=1}\la\wb_{1,r}^{(t+1)},\bxi_s^{(p)}\ra &= \bigg[1 + \Theta\bigg(\frac{\eta d\sigma_p^2}{n}\bigg)\bigg]\cdot \sum_{p\in[P]}\sum_{s:y_s=1}\la\wb_{1,r}^{(t)},\bxi_s^{(p)}\ra\notag\\
& \qquad \pm O\big(\eta d\sigma_p^2\big)\cdot\la\wb_{1,r}^{(t)},\vb\ra\pm O\big(\eta d\sigma_p^2\big)\cdot\la\wb_{1,r}^{(t)},\ub\ra \pm O\big(\eta\rho d\sigma_p^2\big)\cdot\la\wb_{1,r}^{(t)},\vb'\ra\notag\\
&\qquad\pm O\big(\eta\rho d\sigma_p^2\big)\cdot\la\wb_{1,r}^{(t)},\ub'\ra  \pm O\bigg(\frac{\eta\zeta d\sigma_p^2}{n}\bigg)\cdot\sum_{p\in[P]}\sum_{s=1}^n |\la\wb_{1,r}^{(t)},\bxi_s^{(p)}\ra|. 
\end{align*}
Then we can get that for any $t = O\big(n\eta^{-1}/(d\sigma_p^2)\big)$, we have
\begin{align*}
\bigg|\sum_{p\in[P]}\sum_{s:y_s=1}\la\wb_{1,r}^{(t)},\bxi_s^{(p)}\ra\bigg|&\le O\big(n^{1/2}P^{1/2}d^{1/2}\sigma_0\sigma_p\big)+ O(\eta d\sigma_p^2)\cdot\sum_{\tau=0}^{t-1}\big[|\la\wb_{1,r}^{(\tau)},\vb\ra|+|\la\wb_{1,r}^{(\tau)},\ub\ra|\big] \notag\\
&\qquad+ O(\eta\rho d\sigma_p^2)\cdot\sum_{\tau=0}^{t-1}\big[|\la\wb_{1,r}^{(\tau)},\vb'\ra|+|\la\wb_{1,r}^{(\tau)},\ub'\ra|\big] \notag\\
&\qquad + O\bigg(\frac{\eta\zeta d\sigma_p^2}{n}\bigg)\cdot\sum_{\tau=0}^{t-1}\sum_{p\in[P]}\sum_{s=1}^n |\la\wb_{1,r}^{(\tau)},\bxi_s^{(p)}\ra|.
\end{align*}
Moreover, similar result can be obtained for $\sum_{s:y_s=2}\la\wb_{1,r}^{(t)},\bxi_s^{(p)}\ra$ and we omit the proof here. 

Now we are ready to upper bound $I_3$. Particularly, let $\alpha_1^{(t)}$ and $\alpha_2^{(t)}$ be denoted as follows:
\begin{align*}
\alpha_1^{(t)} &= \frac{1}{n^2}\sum_{i': y_{i'}=1}^n\sum_{q\in[P]}\sum_{i,j\in[n]}l_{1,(i,j)}\sum_{p\in[P]}\theta_{i,j}^{(p)}(\bxi_{i'}^{(q)})\cdot\theta_{i,j}^{(p)}(\vb)\cdot\la\wb_{1,r}^{(t)},\bxi_{i'}^{(q)}\ra\notag\\
\alpha_2^{(t)} &= \frac{1}{n^2}\sum_{i': y_{i'}=2}^n\sum_{q\in[P]}\sum_{i,j\in[n]}l_{1,(i,j)}\sum_{p\in[P]}\theta_{i,j}^{(p)}(\bxi_{i'}^{(q)})\cdot\theta_{i,j}^{(p)}(\vb)\cdot\la\wb_{1,r}^{(t)},\bxi_{i'}^{(q)}\ra.
\end{align*}
Then it is clear that $I_3 = \alpha_1^{(t)} + \alpha_2^{(t)}$. Then by \eqref{eq:bound_sum_coefficients_noise} and \eqref{eq:update_positive_noise}, 
% let
% \begin{align*}
% \bar\gamma_1^{(t)}(\bxi_{i'}^{(q)}, \vb) := \sum_{q\in[P]}\sum_{i,j\in[n]}l_{1,(i,j)}\sum_{p\in[P]}\theta_{i,j}^{(p)}(\bxi_{i'}^{(q)})\cdot\theta_{i,j}^{(p)}(\vb)
% \end{align*}
we can get
\begin{align*}
\alpha_1^{(t+1)} & = \bigg[1 + \Theta\bigg(\frac{\eta d\sigma_p^2}{n}\bigg)\bigg]\cdot \alpha_1^{(t)}\pm O\bigg(\frac{\eta d\sigma_p^2}{Pn}\bigg)\cdot\la\wb_{1,r}^{(t)},\vb\ra\pm O\bigg(\frac{\eta d\sigma_p^2}{Pn}\bigg)\cdot\la\wb_{1,r}^{(t)},\ub\ra \notag\\
& \qquad \pm O\bigg(\frac{\eta\rho d\sigma_p^2}{Pn}\bigg)\cdot\la\wb_{1,r}^{(t)},\vb'\ra\pm O\bigg(\frac{\eta\rho d\sigma_p^2}{Pn}\bigg)\cdot\la\wb_{1,r}^{(t)},\ub'\ra\notag\\
&\qquad  \pm O\bigg(\frac{\eta d\sigma_p^2}{n^2P}\bigg)\cdot \bigg|\sum_{p\in[P]}\sum_{s:y_s=1}\la\wb_{1,r}^{(t)},\bxi_s^{(p)}\ra\bigg|\pm O\bigg(\frac{\eta \zeta d\sigma_p^2}{n^2P}\bigg)\cdot \sum_{p\in[P]}\sum_{s=1}^n|\la\wb_{1,r}^{(t)},\bxi_s^{(p)}\ra|.
\end{align*}
Similarly, we can also obtain
\begin{align*}
\alpha_2^{(t+1)} & = \bigg[1 - \Theta\bigg(\frac{\eta d\sigma_p^2}{n}\bigg)\bigg]\cdot \alpha_2^{(t)}\pm O\bigg(\frac{\eta d\sigma_p^2}{Pn}\bigg)\cdot\la\wb_{1,r}^{(t)},\vb\ra\pm O\bigg(\frac{\eta d\sigma_p^2}{Pn}\bigg)\cdot\la\wb_{1,r}^{(t)},\ub\ra \notag\\
& \qquad \pm O\bigg(\frac{\eta\rho d\sigma_p^2}{Pn}\bigg)\cdot\la\wb_{1,r}^{(t)},\vb'\ra\pm O\bigg(\frac{\eta\rho d\sigma_p^2}{Pn}\bigg)\cdot\la\wb_{1,r}^{(t)},\ub'\ra\notag\\
&\qquad  \pm O\bigg(\frac{\eta d\sigma_p^2}{n^2P}\bigg)\cdot \bigg|\sum_{p\in[P]}\sum_{s:y_s=2}\la\wb_{1,r}^{(t)},\bxi_s^{(p)}\ra\bigg|\pm O\bigg(\frac{\eta \zeta d\sigma_p^2}{n^2P}\bigg)\cdot \sum_{p\in[P]}\sum_{s=1}^n|\la\wb_{1,r}^{(t)},\bxi_s^{(p)}\ra|.
\end{align*}
Then using the previous results on $\big|\sum_{p\in[P]}\sum_{s:y_s=1}\la\wb_{1,r}^{(t)},\bxi_s^{(p)}\ra\big|$ and $\big|\sum_{p\in[P]}\sum_{s:y_s=2}\la\wb_{1,r}^{(t)},\bxi_s^{(p)}\ra\big|$ and \eqref{eq:bound_I1_noise_sum}, we can get that for any $t=O\big(n\eta^{-1}/(d\sigma_p^2)\big)$,
\begin{align*}
|\alpha_1^{(t)}| &\le O\bigg(\frac{d^{1/2}\sigma_0\sigma_p}{P^{1/2}n^{1/2}}\bigg) + O\bigg(\frac{\eta d\sigma_p^2}{Pn}\bigg)\cdot\sum_{\tau=0}^{t-1}\big[|\la\wb_{1,r}^{(\tau)},\vb\ra|+|\la\wb_{1,r}^{(\tau)},\ub\ra|\big] \notag\\
&\qquad+ O\bigg(\frac{\eta\rho d\sigma_p^2}{Pn}\bigg)\cdot\sum_{\tau=0}^{t-1}\big[|\la\wb_{1,r}^{(\tau)},\vb'\ra|+|\la\wb_{1,r}^{(\tau)},\ub'\ra|\big]+ O\bigg(\frac{\eta\zeta d\sigma_p^2}{n^2P}\bigg)\cdot\sum_{\tau=0}^{t-1}\sum_{p\in[P]}\sum_{s=1}^n |\la\wb_{1,r}^{(\tau)},\bxi_s^{(p)}\ra|,
\end{align*}
where we use the upper bound of $|\alpha_1^{(0)}|$ provided in 
Similarly, we can obtain the same results for $\alpha_2^{(t)}$ as follows:
\begin{align*}
|\alpha_2^{(t)}| &\le O\bigg(\frac{d^{1/2}\sigma_0\sigma_p}{P^{1/2}n^{1/2}}\bigg) + O\bigg(\frac{\eta d\sigma_p^2}{Pn}\bigg)\cdot\sum_{\tau=0}^{t-1}\big[|\la\wb_{1,r}^{(\tau)},\vb\ra|+|\la\wb_{1,r}^{(\tau)},\ub\ra|\big] \notag\\
&\qquad+ O\bigg(\frac{\eta\rho d\sigma_p^2}{Pn}\bigg)\cdot\sum_{\tau=0}^{t-1}\big[|\la\wb_{1,r}^{(\tau)},\vb'\ra|+|\la\wb_{1,r}^{(\tau)},\ub'\ra|\big]+ O\bigg(\frac{\eta\zeta d\sigma_p^2}{n^2P}\bigg)\cdot\sum_{\tau=0}^{t-1}\sum_{p\in[P]}\sum_{s=1}^n |\la\wb_{1,r}^{(\tau)},\bxi_s^{(p)}\ra|.
\end{align*}
Combining the above results leads to the bound of $I_3$.

We will finally bound $I_4$ as follows: using the fact that $|\ell_{1,(i,j)}-l_{1,(i,j)}|=O(\zeta)$ and a similar characterization of \eqref{eq:bound_sum_coefficients_noise}, we can get
\begin{align*}
I_4 \le O\bigg(\frac{\zeta}{Pn}\bigg)\cdot\sum_{s=1}^n\sum_{p\in[P]}|\la\wb_{1,r}^{(t)},\bxi_s^{(p)}\ra|.
\end{align*}
Combining the above bounds on $I_3$ and $I_4$, we can finally get
\begin{align*}
|z_t| &\le |I_3| + |I_4|\notag\\
&\le O\bigg(\frac{d^{1/2}\sigma_0\sigma_p}{P^{1/2}n^{1/2}}\bigg) +O\bigg(\frac{\zeta}{Pn}\bigg)\cdot\sum_{s=1}^n\sum_{p\in[P]}|\la\wb_{1,r}^{(t)},\bxi_s^{(p)}\ra|+ O\bigg(\frac{\eta d\sigma_p^2}{Pn}\bigg)\cdot\sum_{\tau=0}^{t-1}\big[|\la\wb_{1,r}^{(\tau)},\vb\ra|+|\la\wb_{1,r}^{(\tau)},\ub\ra|\big] \notag\\
&\qquad+ O\bigg(\frac{\eta\rho d\sigma_p^2}{Pn}\bigg)\cdot\sum_{\tau=0}^{t-1}\big[|\la\wb_{1,r}^{(\tau)},\vb'\ra|+|\la\wb_{1,r}^{(\tau)},\ub'\ra|\big]+ O\bigg(\frac{\eta\zeta d\sigma_p^2}{n^2P}\bigg)\cdot\sum_{\tau=0}^{t-1}\sum_{p\in[P]}\sum_{s=1}^n |\la\wb_{1,r}^{(\tau)},\bxi_s^{(p)}\ra|.
\end{align*}



This completes the proof.


\end{proof}


Then the following lemma characterizes the growth of  common feature learning.
\begin{lemma}\label{lemma:dominance_v}
Assume $\max_{k\in[2], (i,j)\in\cS} |F_k(\Wb^{(t)}; \xb_{i,j})|\le \zeta$ for some $\zeta =o\big(d^{-1/2}\sigma_p^{-1}\big)$. Then for any $t=O\big(\polylog(n)/\eta\big)$ that satisfies this condition, we have with probability at least $1-1/\poly(n)$, there exists at least one $r\in[m]$ such that
\begin{align*}
\la\wb_{1,r}^{(t+1)},\vb\ra= \big[1 + \Theta(\eta)\big]\cdot \la\wb_{1,r}^{(t)},\vb\ra.
\end{align*}

\end{lemma}
\begin{proof} 
First, note that $\la\wb_{1,r}^{(0)},\vb\ra$ follows $N(0, \sigma_0^2)$, then it is easy to get that 
\begin{align}\label{eq:prob_largest_neuron}
\mathbb P\bigg[\max_{r\in[m]}|\la\wb_{1,r}^{(0)},\vb\ra|\ge \sigma_0\bigg] = 1 - \big(\mathbb P_{\xi\sim N(0,\sigma_0^2)}[|\xi|\le \sigma_0]\big)^m\ge 1-0.7^m \ge 1-1/\poly(n),
\end{align}
where the last inequality is by our assumption that $m=\polylog(n)>C\log(n)$ for some sufficiently large constant $C$.

Recall the update rule of $\la\wb_{1,r}^{(t)},\vb\ra$:
\begin{align*}
\la\wb_{1,r}^{(t+1)},\vb\ra  
& = \big[1 + \eta\gamma_1^{(t)}(\vb,\vb)\big]\cdot\la\wb_{1,r}^{(t)},\vb\ra + \eta\gamma_1^{(t)}(\ub,\vb)\cdot\la\wb_{1,r}^{(t)},\ub\ra + \eta\gamma_1^{(t)}(\vb',\vb)\cdot\la\wb_{1,r}^{(t)},\vb'\ra \notag\\
&\qquad + \eta\gamma_1^{(t)}(\ub',\vb)\cdot\la\wb_{1,r}^{(t)}, \ub'\ra + \sum_{i=1}^n\sum_{p\in[P]}\eta\gamma_1^{(t)}(\bxi_i^{(p)},\vb)\cdot\la\wb_{1,r}^{(t)},\bxi_i^{(p)}\ra.
\end{align*}
Taking absolute value on both sides leads to
\begin{align*}
|\la\wb_{1,r}^{(t+1)},\vb\ra|&\ge \big[1 + \eta\gamma_1^{(t)}(\vb,\vb)\big]\cdot|\la\wb_{1,r}^{(t)},\vb\ra| - \eta\big|\gamma_1^{(t)}(\ub,\vb)\cdot\la\wb_{1,r}^{(t)},\ub\ra\big| - \eta\big|\gamma_1^{(t)}(\vb',\vb)\cdot\la\wb_{1,r}^{(t)},\vb'\ra\big| \notag\\
&\qquad - \eta\big|\gamma_1^{(t)}(\ub',\vb)\cdot\la\wb_{1,r}^{(t)}, \ub'\ra\big| - \bigg|\sum_{i=1}^n\sum_{p\in[P]}\eta\gamma_1^{(t)}(\bxi_i^{(p)},\vb)\cdot\la\wb_{1,r}^{(t)},\bxi_i^{(p)}\ra\bigg|.
\end{align*}
Therefore, the next step is to show that these ``negative'' terms in the above inequality are dominated by $\eta\gamma_1^{(t)}(\vb,\vb)\cdot |\la\wb_{1,r}^{(t)},\vb\ra|$, i.e., showing that 
\begin{align*}
&\big|\gamma_1^{(t)}(\ub,\vb)\cdot\la\wb_{1,r}^{(t)},\ub\ra\big|, \big|\gamma_1^{(t)}(\vb',\vb)\cdot\la\wb_{1,r}^{(t)},\vb'\ra\big|, \big|\gamma_1^{(t)}(\ub',\vb)\cdot\la\wb_{1,r}^{(t)}, \ub'\ra\big| \ll |\la\wb_{1,r}^{(t)},\vb\ra|;\notag\\
&\bigg|\sum_{i=1}^n\sum_{p\in[P]}\eta\gamma_1^{(t)}(\bxi_i^{(p)},\vb)\cdot\la\wb_{1,r}^{(t)},\bxi_i^{(p)}\ra\bigg|\ll |\la\wb_{1,r}^{(t)},\vb\ra|,
\end{align*}
where we use our result in Lemma \ref{lemma:feature_learning_coefficients_v_mixup} that $\gamma_1^{(t)}(\vb,\vb)=\Theta(1)$. Then we are able to get that
\begin{align}\label{eq:updates_v_tmp}
|\la\wb_{1,r}^{(t+1)},\vb\ra|&=\big[1 + \eta\gamma_1^{(t)}(\vb,\vb) \pm o\big(1/\polylog(n)\big)\big]\cdot|\la\wb_{1,r}^{(t)},\vb\ra|\ge \big[1+\Theta(\eta)\big]\cdot|\la\wb_{1,r}^{(t)},\vb\ra|. 
\end{align}

Regarding the first three terms, we will prove them by mathematical induction on a stronger argument (recall that $\gamma_1^{(t)}(\vb,\vb)=\Theta(1)$, $|\gamma_1^{(t)}(\ub,\vb)|,|\gamma_1^{(t)}(\vb',\vb)|,|\gamma_1^{(t)}(\ub',\vb)|=o\big(1/\polylog(n)\big)$, according to Lemma \ref{lemma:feature_learning_coefficients_v_mixup}): we aim to verify the hypothesis
\begin{align}\label{eq:hypothesis_dominance_v}
 & |\la\wb_{1,r}^{(t)},\ub\ra\big|, \quad |\la\wb_{1,r}^{(t)},\vb'\ra\big| ,\quad |\la\wb_{1,r}^{(t)},\ub'\ra\big| \le  c\cdot\log^2(n)|\la\wb_{1,r}^{(t)},\vb\ra|,
%  \notag\\
% & \bigg|\sum_{i=1}^n\sum_{p\in[P]}\eta\gamma_1^{(t)}(\bxi_i^{(p)},\vb)\cdot\la\wb_{1,r}^{(t)},\bxi_i^{(p)}\ra\bigg|\bigg]\le  c\cdot|\la\wb_{1,r}^{(t)},\vb\ra|,
\end{align}
where $c$ is some sufficiently small constant.



In particular, we can first consider the initialization where $t=0$, then by \eqref{eq:prob_largest_neuron} and standard concentration bound of Gaussian random variable, we have with probability at least $1-1/\poly(n)$,
\begin{align*}
&|\la\wb_{1,r}^{(0)},\vb\ra| = \Omega(\sigma_0),\quad \big|\la\wb_{1,r}^{(0)},\ub\ra\big| = O\big(\log(n)\sigma_0\big), \notag\\
&\big|\la\wb_{1,r}^{(0)},\vb'\ra\big| = O\big(\log(n)\sigma_0\big),\quad\big|\la\wb_{1,r}^{(0)},\vb'\ra\big| = O\big(\log(n)\sigma_0\big).
% \notag\\
% &\bigg|\sum_{i=1}^n\sum_{p\in[P]}\gamma_1^{(0)}(\bxi_i^{(p)},\vb)\cdot\la\wb_{1,r}^{(0)},\bxi_i^{(p)}\ra\bigg| = \tilde O \bigg(\frac{d^{1/2}\sigma_0\sigma_p}{P^{1/2}n^{1/2}}+\zeta d^{1/2}\sigma_0\sigma_p\bigg).
\end{align*}
Therefore, using the fact that $\zeta = o\big(d^{-1/2}\sigma_p^{-1}\big)$, it is easy to verify the hypothesis. We will then assume the hypothesis holds for all $\tau\le t$ and aim to verify it for $t+1$. Particularly, recall the update rules of $\la\wb_{1,r}^{(t)},\ub\ra$, we have
\begin{align}\label{eq:upperbound_w_u}
|\la\wb_{1,r}^{(t+1)},\ub\ra| &\le \big[1 - \eta\gamma_1^{(t)}(\ub,\ub)\big]\cdot|\la\wb_{1,r}^{(t)},\ub\ra| + \eta\big|\gamma_1^{(t)}(\vb,\ub)\cdot\la\wb_{1,r}^{(t)},\vb\ra\big| + \eta\big|\gamma_1^{(t)}(\vb',\vb)\cdot\la\wb_{1,r}^{(t)},\vb'\ra\big| \notag\\
&\qquad + \eta\big|\gamma_1^{(t)}(\ub',\ub)\cdot\la\wb_{1,r}^{(t)}, \ub'\ra\big| + \bigg|\sum_{i=1}^n\sum_{p\in[P]}\eta\gamma_1^{(t)}(\bxi_i^{(p)},\ub)\cdot\la\wb_{1,r}^{(t)},\bxi_i^{(p)}\ra\bigg|\notag\\
&\le |\la\wb_{1,r}^{(0)},\ub\ra| + \eta\sum_{\tau=0}^{t}\big|\gamma_1^{(\tau)}(\vb,\ub)\cdot\la\wb_{1,r}^{(\tau)},\vb\ra\big| + \eta\sum_{\tau=0}^{t}\big|\gamma_1^{(\tau)}(\vb',\vb)\cdot\la\wb_{1,r}^{(\tau)},\vb'\ra\big| \notag\\
&\qquad + \eta\sum_{\tau=0}^{t}\big|\gamma_1^{(\tau)}(\ub',\ub)\cdot\la\wb_{1,r}^{(\tau)}, \ub'\ra\big| + \sum_{\tau=0}^{t}\bigg|\sum_{i=1}^n\sum_{p\in[P]}\eta\gamma_1^{(\tau)}(\bxi_i^{(p)},\ub)\cdot\la\wb_{1,r}^{(\tau)},\bxi_i^{(p)}\ra\bigg|\notag\\
&\le O\big(\log(n)\sigma_0\big) + \tilde O\big(\eta(\zeta+\alpha)\big)\cdot\sum_{\tau=0}^t|\la\wb_{1,r}^{(\tau)},\vb\ra| + O\bigg(\frac{\eta\rho}{P}+\frac{t\rho\eta^2 d\sigma_p^2}{Pn}\bigg)\sum_{\tau=0}^t\big[|\la\wb_{1,r}^{(\tau)},\vb'\ra| +|\la\wb_{1,r}^{(\tau)},\ub'\ra| \big]\notag\\
& \qquad+ O\bigg(\frac{t \eta d^{1/2}\sigma_0\sigma_p}{P^{1/2}n^{1/2}}\bigg)+ O\bigg(\frac{t\eta^2 d\sigma_p^2}{Pn}\bigg)\cdot\sum_{\tau=0}^t\big[|\la\wb_{1,r}^{(\tau)},\vb\ra| +|\la\wb_{1,r}^{(\tau)},\ub\ra| \big]\notag\\
&\qquad  + O\bigg(\frac{\eta\zeta}{Pn}+\frac{t\eta^2\zeta d\sigma_p^2}{n^2P}\bigg)\cdot\sum_{\tau=0}^{t}\sum_{s=1}^n\sum_{p\in[P]}|\la\wb_{1,r}^{(\tau)},\bxi_s^{(p)}\ra|.
\end{align}
where the last inequality is by Lemma \ref{lemma:update_sum_noise}.
Then by \eqref{eq:update_positive_noise}, we have the following results regarding $|\la\wb_{1,r}^{(\tau)},\bxi_s^{(p)}\ra|$
\begin{align*}
\max_{i\in[n],p\in[P]}|\la\wb_{1,r}^{(\tau+1)}, \bxi_i^{(p)}\ra| &\le \bigg[1 + O\bigg(\frac{d\sigma_p^2}{n}\bigg)\bigg]\cdot \max_{i\in[n],p\in[P]}|\la\wb_{1,r}^{(\tau)}, \bxi_i^{(p)}\ra| +O\bigg(\frac{\eta d\sigma_p^2}{Pn}\bigg)\cdot\big[|\la\wb_{1,r}^{(\tau)},\vb\ra| + |\la\wb_{1,r}^{(\tau)},\ub\ra|\big]\notag\\
& \qquad+ O\bigg(\frac{\eta\rho d\sigma_p^2}{Pn}\bigg)\cdot\big[|\la\wb_{1,r}^{(\tau)},\vb'\ra| + |\la\wb_{1,r}^{(\tau)},\ub'\ra|\big]\notag\\
& =  O\big(d^{1/2}\sigma_0\sigma_p\big) + O\bigg(\frac{\eta d\sigma_p^2}{Pn}\bigg)\cdot\sum_{s=0}^{\tau}\big[|\la\wb_{1,r}^{(s)},\vb\ra| + |\la\wb_{1,r}^{(s)},\ub\ra|\big] \notag\\
&\qquad + O\bigg(\frac{\eta\rho d\sigma_p^2}{Pn}\bigg)\cdot\sum_{s=0}^{\tau}\big[|\la\wb_{1,r}^{(s)},\vb'\ra| + |\la\wb_{1,r}^{(s)},\ub'\ra|\big].
\end{align*}
Therefore,  we can accordingly get the following upper bound regarding the last term in the RHS of \eqref{eq:upperbound_w_u},
\begin{align}\label{eq:upperbound_sum_aboslute_noise}
\frac{1}{nP}\sum_{\tau=0}^{t}\sum_{s=1}^n\sum_{p\in[P]}|\la\wb_{1,r}^{(\tau)},\bxi_s^{(p)}\ra|&\le  \sum_{\tau=0}^t\max_{i\in[n],p\in[P]}|\la\wb_{1,r}^{(\tau)}, \bxi_i^{(p)}\ra|\notag\\
& \le O\big(td^{1/2}\sigma_0\sigma_p\big) + O\bigg(\frac{t\eta d\sigma_p^2}{Pn}\bigg)\cdot\sum_{\tau=0}^{t-1}\big[|\la\wb_{1,r}^{(\tau)},\vb\ra| + |\la\wb_{1,r}^{(\tau)},\ub\ra|\big] \notag\\
&\qquad + O\bigg(\frac{t\eta\rho d\sigma_p^2}{Pn}\bigg)\cdot\sum_{\tau=0}^{t-1}\big[|\la\wb_{1,r}^{(\tau)},\vb'\ra| + |\la\wb_{1,r}^{(\tau)},\ub'\ra|\big].
\end{align}
Then using the fact that $t\eta =O\big(\polylog(n)\big)$ and $n=\omega(d\sigma_p^2)$, we can further get the following on \eqref{eq:upperbound_w_u}
\begin{align*}
|\la\wb_{1,r}^{(t+1)},\ub\ra|&\le  O\big(\log(n)\sigma_0\big) + \tilde O\big(\eta(\zeta+\alpha)\big)\cdot\sum_{\tau=0}^t|\la\wb_{1,r}^{(\tau)},\vb\ra| + O\bigg(\frac{\eta\rho }{P}\bigg)\sum_{\tau=0}^t\big[|\la\wb_{1,r}^{(\tau)},\vb'\ra| +|\la\wb_{1,r}^{(\tau)},\ub'\ra| \big]\notag\\
& \qquad+ O\bigg(\frac{t \eta d^{1/2}\sigma_0\sigma_p}{P^{1/2}n^{1/2}}+t\eta\zeta d^{1/2}\sigma_0\sigma_p\bigg)+ O\bigg(\frac{\eta }{P}\bigg)\cdot\sum_{\tau=0}^t\big[|\la\wb_{1,r}^{(\tau)},\vb\ra| +|\la\wb_{1,r}^{(\tau)},\ub\ra| \big].
\end{align*}
Then according to the Hypothesis \ref{eq:hypothesis_dominance_v} for any $\tau\le t$, it is easy to get that
\begin{align*}
|\la\wb_{1,r}^{(t+1)},\ub\ra|\ge |\la\wb_{1,r}^{(t)},\ub\ra|\ge\ldots\ge|\la\wb_{1,r}^{(0)},\ub\ra|.
\end{align*}
Then we can get $|\la\wb_{1,r}^{(t+1)},\ub\ra|=\Omega(\sigma_0)$, applying  the fact that $t\eta=O(\polylog(n))$ further gives
\begin{align}\label{eq:characterize1}
O\big(\log(n)\sigma_0\big) +O\bigg(\frac{t \eta d^{1/2}\sigma_0\sigma_p}{P^{1/2}n^{1/2}}+t\eta\zeta d^{1/2}\sigma_0\sigma_p\bigg)\bigg) \bigg]=o(\log^2(n)\sigma_0) = o\big(\log^2(n)|\la\wb_{1,r}^{(t+1)},\ub\ra|\big).
\end{align}
Besides, note that the Hypothesis \ref{eq:hypothesis_dominance_v} holds for all $\tau\le t$, we have
\begin{align}\label{eq:characterize2}
|\la\wb_{1,r}^{(\tau)},\ub\ra|,|\la\wb_{1,r}^{(\tau)},\vb'\ra|,|\la\wb_{1,r}^{(\tau)},\ub'\ra|\le c\cdot \log^2(n)\cdot |\la\wb_{1,r}^{(\tau)},\vb\ra|\le \log^2(n)\cdot |\la\wb_{1,r}^{(t+1)},\vb\ra|,
\end{align}
we can immediately get that
\begin{align}\label{eq:characterize3}
&\tilde O\big(\eta(\zeta+\alpha)\big)\cdot\sum_{\tau=0}^t|\la\wb_{1,r}^{(\tau)},\vb\ra|\le \tilde O\big(t\eta(\zeta+\alpha)\big)\cdot|\la\wb_{1,r}^{(t+1)},\vb\ra| =o\big( \log^2(n)\cdot |\la\wb_{1,r}^{(t+1)},\vb\ra|\big)\notag\\
 &O\bigg(\frac{\eta\rho}{P}\bigg)\cdot\sum_{\tau=0}^t\big[|\la\wb_{1,r}^{(\tau)},\vb'\ra| +|\la\wb_{1,r}^{(\tau)},\ub'\ra| \big]\le \tilde O\bigg(\frac{t\eta\rho}{P}\bigg)\cdot|\la\wb_{1,r}^{(t+1)},\vb\ra| =o\big( \log^2(n)\cdot |\la\wb_{1,r}^{(t+1)},\vb\ra|\big)\notag\\
&O\bigg(\frac{\eta}{P}\bigg)\cdot\sum_{\tau=0}^t\big[|\la\wb_{1,r}^{(\tau)},\vb\ra| +|\la\wb_{1,r}^{(\tau)},\ub\ra| \big] \le \tilde O\bigg(\frac{t\eta}{P}\bigg)\cdot|\la\wb_{1,r}^{(t+1)},\vb\ra| =o\big( \log^2(n)\cdot |\la\wb_{1,r}^{(t+1)},\vb\ra|\big).
\end{align}
Putting the above results together, we can verify that
\begin{align*}
|\la\wb_{1,r}^{(t+1)},\ub\ra| = o\big( \log^2(n)\cdot |\la\wb_{1,r}^{(t+1)},\vb\ra|\big).
\end{align*}

We will then verify the Hypothesis for $\la\wb_{1,r}^{(t+1)},\vb'\ra$. By its update rule, Lemma \ref{lemma:feature_learning_coefficients_v'_mixup}, and Lemma \ref{lemma:update_sum_noise}, we have
\begin{align*}
% \label{eq:upperbound_w_v'}
|\la\wb_{1,r}^{(t+1)},\vb'\ra| &\le \big[1 + \eta\gamma_1^{(t)}(\vb',\vb')\big]\cdot|\la\wb_{1,r}^{(t)},\vb'\ra| + \eta\big|\gamma_1^{(t)}(\vb,\vb')\cdot\la\wb_{1,r}^{(t)},\vb\ra\big| + \eta\big|\gamma_1^{(t)}(\ub,\vb')\cdot\la\wb_{1,r}^{(t)},\ub\ra\big| \notag\\
&\qquad + \eta\big|\gamma_1^{(t)}(\ub',\vb')\cdot\la\wb_{1,r}^{(t)}, \ub'\ra\big| + \bigg|\sum_{i=1}^n\sum_{p\in[P]}\eta\gamma_1^{(t)}(\bxi_i^{(p)},\vb')\cdot\la\wb_{1,r}^{(t)},\bxi_i^{(p)}\ra\bigg|\notag\\
&\le2|\la\wb_{1,r}^{(0)},\vb'\ra| + \eta\sum_{\tau=0}^{t}\big|\gamma_1^{(\tau)}(\vb,\vb')\cdot\la\wb_{1,r}^{(\tau)},\vb\ra\big| + \eta\sum_{\tau=0}^{t}\big|\gamma_1^{(\tau)}(\ub,\vb')\cdot\la\wb_{1,r}^{(\tau)},\ub\ra\big| \notag\\
&\qquad + \eta\sum_{\tau=0}^{t}\big|\gamma_1^{(\tau)}(\ub',\vb')\cdot\la\wb_{1,r}^{(\tau)}, \ub'\ra\big| + \sum_{\tau=0}^{t}\bigg|\sum_{i=1}^n\sum_{p\in[P]}\eta\gamma_1^{(\tau)}(\bxi_i^{(p)},\vb')\cdot\la\wb_{1,r}^{(\tau)},\bxi_i^{(p)}\ra\bigg|\notag\\
&\le O\big(\log(n)\sigma_0\big) + O\bigg(\frac{\eta\rho}{P}+\frac{t\rho\eta^2 d\sigma_p^2}{Pn}\bigg)\cdot\sum_{\tau=0}^t|\la\wb_{1,r}^{(\tau)},\vb\ra| + O\bigg(\frac{\eta\zeta\rho}{P}+\frac{t\eta^2\rho d\sigma_p^2}{Pn}\bigg)\cdot\sum_{\tau=0}^t|\la\wb_{1,r}^{(\tau)},\ub\ra|\notag\\
&\qquad + O\bigg(\frac{\eta\zeta\rho^2}{P}+\frac{t\rho^2\eta^2 d\sigma_p^2}{Pn}\bigg)\cdot\sum_{\tau=0}^t|\la\wb_{1,r}^{(\tau)},\ub'\ra| + O\bigg(\frac{t\eta\rho d^{1/2}\sigma_0\sigma_p}{P^{1/2}n^{1/2}}\bigg)\notag\\
&\qquad+ O\bigg(\frac{\eta\rho\zeta}{Pn}+\frac{t\eta^2\zeta\rho d\sigma_p^2}{n^2P}\bigg)\cdot\sum_{\tau=0}^t\sum_{s=1}^n\sum_{p\in[P]}|\la\wb_{1,r}^{(\tau)},\bxi_s^{(p)}\ra|.
\end{align*}
Then by \eqref{eq:upperbound_sum_aboslute_noise} and using the fact that $t\eta=O(\polylog(n))$ and $n=\omega(d\sigma_p^2)$, we can finally get
\begin{align*}
|\la\wb_{1,r}^{(t+1)},\vb'\ra| &\le O\big(\log(n)\sigma_0\big) + O\bigg(\frac{\eta\rho}{P}\bigg)\cdot\sum_{\tau=0}^t|\la\wb_{1,r}^{(\tau)},\vb\ra| + O\bigg(\frac{\eta\rho}{P}\bigg)\cdot\sum_{\tau=0}^t|\la\wb_{1,r}^{(\tau)},\ub\ra| \notag\\
&\qquad  + O\bigg(\frac{\eta \rho^2}{P}\bigg)\cdot\sum_{\tau=0}^t|\la\wb_{1,r}^{(\tau)},\ub'\ra|+ O\bigg(\frac{t\eta\rho d^{1/2}\sigma_0\sigma_p}{P^{1/2}n^{1/2}}+t\eta\rho\zeta d^{1/2}\sigma_0\sigma_p\bigg).
\end{align*}
Then applying \eqref{eq:characterize1}, \eqref{eq:characterize2}, and \eqref{eq:characterize3}, we can also verify that 
\begin{align*}
|\la\wb_{1,r}^{(t+1)},\vb'\ra| = o\big(\log^2(n)\cdot|\la\wb_{1,r}^{(t+1)},\vb\ra|\big).
\end{align*}
The using exactly the same proof, we are also able to verify that 
\begin{align*}
|\la\wb_{1,r}^{(t+1)},\ub'\ra| = o\big(\log^2(n)\cdot|\la\wb_{1,r}^{(t+1)},\vb\ra|\big).
\end{align*}

Lastly, we will prove that 
\begin{align}\label{eq:verification_upperbound_allnoise_v}
\bigg|\sum_{i=1}^n\sum_{p\in[P]}\gamma_1^{(t)}(\bxi_i^{(p)},\vb)\cdot\la\wb_{1,r}^{(t)},\bxi_i^{(p)}\ra\bigg| \le c\cdot |\la\wb_{1,r}^{(t)}, \vb\ra|
\end{align}
for some sufficiently small constant $c$ and all $t=O\big(\polylog(n)/\eta\big)$. This can be proved by the combination of Lemma \ref{lemma:update_sum_noise}, \eqref{eq:upperbound_sum_aboslute_noise}, and our previous characterizations  \eqref{eq:characterize1}, \eqref{eq:characterize2}, \eqref{eq:characterize3}. In particular, using the fact that $|\la\wb_{1,r}^{(t)}, \vb\ra|=\Omega(\sigma_0)$,
we have
\begin{align}\label{eq:bound_sum_noise_v}
&\bigg|\sum_{i=1}^n\sum_{p\in[P]}\gamma_1^{(t)}(\bxi_i^{(p)},\vb)\cdot\la\wb_{1,r}^{(t)},\bxi_i^{(p)}\ra\bigg|\notag\\
&\le O\bigg(\frac{d^{1/2}\sigma_0\sigma_p}{P^{1/2}n^{1/2}}\bigg) +O\bigg(\frac{\zeta}{Pn}\bigg)\cdot\sum_{s=1}^n\sum_{p\in[P]}|\la\wb_{1,r}^{(t)},\bxi_s^{(p)}\ra|+ O\bigg(\frac{\eta d\sigma_p^2}{Pn}\bigg)\cdot\sum_{\tau=0}^{t-1}\big[|\la\wb_{1,r}^{(\tau)},\vb\ra|+|\la\wb_{1,r}^{(\tau)},\ub\ra|\big] \notag\\
&\qquad+ O\bigg(\frac{\eta\rho d\sigma_p^2}{Pn}\bigg)\cdot\sum_{\tau=0}^{t-1}\big[|\la\wb_{1,r}^{(\tau)},\vb'\ra|+|\la\wb_{1,r}^{(\tau)},\ub'\ra|\big]+ O\bigg(\frac{\eta\zeta d\sigma_p^2}{n^2P}\bigg)\cdot\sum_{\tau=0}^{t-1}\sum_{p\in[P]}\sum_{s=1}^n |\la\wb_{1,r}^{(\tau)},\bxi_s^{(p)}\ra|\notag\\
& \le O\bigg(\frac{d^{1/2}\sigma_0\sigma_p}{P^{1/2}n^{1/2}}+\zeta d^{1/2}\sigma_0\sigma_p\bigg) + O\bigg(\frac{\eta d\sigma_p^2}{Pn}\bigg)\cdot\sum_{\tau=0}^{t-1}\big[|\la\wb_{1,r}^{(\tau)},\vb\ra|+|\la\wb_{1,r}^{(\tau)},\ub\ra|\big] \notag\\
&\qquad+ O\bigg(\frac{\eta\rho d\sigma_p^2}{Pn}\bigg)\cdot\sum_{\tau=0}^{t-1}\big[|\la\wb_{1,r}^{(\tau)},\vb'\ra|+|\la\wb_{1,r}^{(\tau)},\ub'\ra|\big]\notag\\
& \le O\bigg(\frac{d^{1/2}\sigma_0\sigma_p}{P^{1/2}n^{1/2}}+\zeta d^{1/2}\sigma_0\sigma_p\bigg) +  O\bigg(\frac{d\sigma_p^2}{Pn}\bigg)\cdot |\la\wb_{1,r}^{(t)},\vb\ra|\notag\\
&\le O\bigg(\bigg(\frac{d^{1/2}\sigma_p}{P^{1/2}n^{1/2}}+\zeta d^{1/2}\sigma_p+ \frac{d\sigma_p^2}{Pn}\bigg)\cdot |\la\wb_{1,r}^{(t)},\vb\ra|\bigg).
\end{align}
Then using the facts that $\zeta = o(d^{-1/2}\sigma_p^{-1})$ and $d\sigma_p^2 = o(n)$, we are able to complete the proof of \eqref{eq:verification_upperbound_allnoise_v}.



\end{proof}

\begin{lemma}\label{lemma:closeness_gamma_v_gamma_u}
Assume $\max_{k\in[2], (i,j)\in\cS} |F_k(\Wb^{(t)}; \xb_{i,j})|\le \zeta$ for some $\zeta =o\big(d^{-1/2}\sigma_p^{-1}\big)$. Then for any $t=O\big(\polylog(n)/\eta\big)$ that satisfies this condition, we have with probability at least $1-1/\poly(n)$, 
\begin{align*}
|\gamma_1^{(t)}(\vb,\vb) - \gamma_2^{(t)}(\ub,\ub)|\le o\bigg(\frac{1}{\polylog(n)}\bigg).
\end{align*}
\end{lemma}
\begin{proof}
Recall $\gamma_1^{(t)}(\vb,\vb)$, we have
\begin{align*}
\gamma_1^{(t)}(\vb, \vb) &=  \frac{1}{n^2}\sum_{i,j\in[n]}\ell_{1,(i,j)}^{(t)} \sum_{p\in[P]} [\theta_{i,j}^{(p)}(\vb)]^2\notag\\
& = \underbrace{\frac{1}{n^2}\sum_{i\in \cS_0^+\text{ or }j\in\cS_0^+}\ell_{1,(i,j)}^{(t)} \sum_{p\in[P]} [\theta_{i,j}^{(p)}(\vb)]^2}_{I_1} + \underbrace{\frac{1}{n^2}\sum_{i\not\in \cS_0^+\text{ and }j\not\in\cS_0^+}\ell_{1,(i,j)}^{(t)} \sum_{p\in[P]} [\theta_{i,j}^{(p)}(\vb)]^2}_{I_2}.  
% \gamma_2^{(t)}(\ub, \ub) &=  \frac{1}{n^2}\sum_{i,j\in[n]}\ell_{2,(i,j)}^{(t)} \sum_{p\in[P]} [\theta_{i,j}^{(p)}(\ub)]^2,
\end{align*}
Regarding $I_2$, using the similar proof in Lemma \ref{lemma:feature_learning_coefficients_v_mixup}, we can obtain that $I_2 = o\big(1/\polylog(n)\big)$. For $I_1$, using the condition that $\max_{k\in[2],(i,j)\in\cS}|F_k(\Wb^{(t)};\xb_{i,j})|\le \zeta$, we have
\begin{align*}
I_1 = \frac{1}{n^2}\sum_{i\in \cS_0^+\text{ or }j\in\cS_0^+}l_{1,(i,j)} \sum_{p\in[P]} [\theta_{i,j}^{(p)}(\vb)]^2 \pm O(\zeta),
\end{align*}
where $l_{1,(i,j)}\in\{0.5,-0.5,0.5-\lambda,\lambda-0.5\}$ denotes the loss derivative of data $(\xb_{i,j},y_{i,j})$ when its neural network output is forced to be zero. To this end, using the similar decomposition for $\gamma_2^{(t)}(\ub, \ub)$ and noting $\zeta = o(1/\polylog(n))$, we can obtain
\begin{align}\label{eq:difference_gamma_v_gamma_u}
|\gamma_1^{(t)}(\vb, \vb)-\gamma_2^{(t)}(\ub, \ub)|&\le \bigg|\frac{1}{n^2}\sum_{i\in \cS_0^+\text{ or }j\in\cS_0^+}l_{1,(i,j)} \sum_{p\in[P]} [\theta_{i,j}^{(p)}(\vb)]^2-\frac{1}{n^2}\sum_{i\in \cS_0^-\text{ or }j\in\cS_0^-}l_{2,(i,j)} \sum_{p\in[P]} [\theta_{i,j}^{(p)}(\ub)]^2\bigg|\notag\\
&\qquad+ o\big(1/\polylog(n)\big).
\end{align}
Moreover, for any $i\in\cS_0^+$, note that
\begin{align*}
\sum_{j\in[n]}l_{1,(i,j)}\sum_{p\in[P]}[\Theta_{i,j}^{(p)}(\vb)]^2 &= \sum_{j\in[n]}l_{1,(i,j)}\sum_{p\in\cP^*_{i,j}(\vb)}[\Theta_{i,j}^{(p)}(\vb)]^2 + \sum_{j\in[n]}l_{1,(i,j)}\sum_{p\not\in\cP^*_{i,j}(\vb)}[\Theta_{i,j}^{(p)}(\vb)]^2\notag\\
&=\ell_{1,(i,i)}+\sum_{j\neq i}l_{1,(i,j)}\cdot z_{i,j}^2\pm o\big(n/\polylog(n)\big),
\end{align*}
where $z_{i,j}=(1-\lambda)^2$ if $j\in\cS_1^+\cup\cS_0^-\cup\cS_1^-$ and
\begin{align*}
z_{i,j}=\begin{cases}
1& \text{with probability } 1/P;\\
(1-\lambda)^2+\lambda^2& \text{with probability } (P-1)/P,
\end{cases}
\end{align*}
if $j\in\cS_0^+$.
Consequently, applying Hoeffeding's inequality regarding the random variable $z_{i,j}$ (when $j\in\cS_0^+$), we have with probability at least $1-1/\poly(n)$,
\begin{align*}
&\frac{1}{n^2}\sum_{i\in\cS_0^+,j\not\in\cS_0^+}l_{1,(i,j)} \sum_{p\in[P]} [\theta_{i,j}^{(p)}(\vb)]^2 = \frac{(1-\lambda)^2}{n^2}\sum_{i\in\cS_0^+,j\in\cS_1^+\cup\cS_0^-\cup\cS_1^-}l_{1,(i,j)}\pm o\bigg(\frac{1}{\polylog(n)}\bigg)\notag\\
&\frac{1}{n^2}\sum_{i\in\cS_0^+,j\in\cS_0^+}l_{1,(i,j)} \sum_{p\in[P]} [\theta_{i,j}^{(p)}(\vb)]^2 = \frac{\ell_{1,(i,i)}}{n^2} + \frac{1+[(1-\lambda)^2+\lambda^2](P-1)}{Pn^2}\cdot\sum_{i\in\cS_0^+,j\in\cS_0^+, j\neq i}l_{1,(i,j)} \pm o\bigg(\frac{1}{\polylog(n)}\bigg).
\end{align*}
Similarly, we can also obtain
\begin{align*}
\frac{1}{n^2}\sum_{ i\not\in\cS_0^+,j\in\cS_0^+}l_{1,(i,j)} \sum_{p\in[P]} [\theta_{i,j}^{(p)} = \frac{(1-\lambda)^2}{n^2}\sum_{i\in\cS_1^+\cup\cS_0^-\cup\cS_1^-,j\in\cS_0^+}l_{1,(i,j)}\pm o\big(1/\polylog(n)\big)
\end{align*}
Therefore, combining the above results, we can get
\begin{align*}
&\frac{1}{n^2}\sum_{i\in \cS_0^+\text{ or }j\in\cS_0^+}l_{1,(i,j)} \sum_{p\in[P]} [\theta_{i,j}^{(p)}(\vb)]^2 \notag\\
&=  \frac{1}{n^2}\sum_{i\in\cS_0^+,j\not\in\cS_0^+}l_{1,(i,j)} \sum_{p\in[P]} [\theta_{i,j}^{(p)}(\vb)]^2 + \frac{1}{n^2}\sum_{i\in\cS_0^+,j\in\cS_0^+}l_{1,(i,j)} \sum_{p\in[P]} [\theta_{i,j}^{(p)}(\vb)]^2 + \frac{1}{n^2}\sum_{ i\not\in\cS_0^+,j\in\cS_0^+}l_{1,(i,j)} \sum_{p\in[P]} [\theta_{i,j}^{(p)}(\vb)]^2\notag\\
& = \frac{(1-\lambda)^2}{n^2}\sum_{i\in\cS_0^+,j\not\in\cS_0^+ \text{ or } i\not\in\cS_0^+,j\in\cS_0^+ }l_{1,(i,j)} + \frac{\ell_{1,(i,i)}}{n^2} + \frac{1+[(1-\lambda)^2+\lambda^2](P-1)}{Pn^2}\cdot\sum_{i\in\cS_0^+,j\in\cS_0^+, j\neq i}l_{1,(i,j)} \notag\\
&\qquad \pm o\bigg(\frac{1}{\polylog(n)}\bigg).
\end{align*}
Similarly, we can get
\begin{align*}
&\frac{1}{n^2}\sum_{i\in \cS_0^-\text{ or }j\in\cS_0^-}l_{1,(i,j)} \sum_{p\in[P]} [\theta_{i,j}^{(p)}(\ub)]^2 \notag\\
&=  \frac{1}{n^2}\sum_{i\in\cS_0^-,j\not\in\cS_0^-}l_{1,(i,j)} \sum_{p\in[P]} [\theta_{i,j}^{(p)}(\ub)]^2 + \frac{1}{n^2}\sum_{i\in\cS_0^-,j\in\cS_0^-}l_{1,(i,j)} \sum_{p\in[P]} [\theta_{i,j}^{(p)}(\ub)]^2 + \frac{1}{n^2}\sum_{ i\not\in\cS_0^-,j\in\cS_0^-}l_{1,(i,j)} \sum_{p\in[P]} [\theta_{i,j}^{(p)}(\ub)]^2\notag\\
& = \frac{(1-\lambda)^2}{n^2}\sum_{i\in\cS_0^-,j\not\in\cS_0- \text{ or } i\not\in\cS_0^-,j\in\cS_0^- }l_{1,(i,j)} + \frac{\ell_{1,(i,i)}}{n^2} + \frac{1+[(1-\lambda)^2+\lambda^2](P-1)}{Pn^2}\cdot\sum_{i\in\cS_0^-,j\in\cS_0^-, j\neq i}l_{1,(i,j)} \notag\\
&\qquad \pm o\bigg(\frac{1}{\polylog(n)}\bigg).
\end{align*}
Then note that the positive and negative data are generated with equal probability, we have $|\cS_0^-|$ and $|\cS_0^+|$ are different by at most $o\big(1/\polylog(n)\big)$, therefore, it is easy to get that
\begin{align*}
\bigg|\frac{1}{n^2}\sum_{i\in \cS_0^+\text{ or }j\in\cS_0^+}l_{1,(i,j)} \sum_{p\in[P]} [\theta_{i,j}^{(p)}(\vb)]^2-\frac{1}{n^2}\sum_{i\in \cS_0^-\text{ or }j\in\cS_0^-}l_{2,(i,j)} \sum_{p\in[P]} [\theta_{i,j}^{(p)}(\ub)]^2\bigg|\le o\big(1/\polylog(n)\big).
\end{align*}
Plugging the above inequality into \eqref{eq:difference_gamma_v_gamma_u} we can conclude that
\begin{align*}
|\gamma_1^{(t)}(\vb, \vb)-\gamma_2^{(t)}(\ub, \ub)|\le o\bigg(\frac{1}{\polylog(n)}\bigg).
\end{align*}
This completes the proof.
\end{proof}





Finally, we state the outcome of noise learning, common feature learning, and rare feature learning in the following Lemma.
\begin{lemma}
Let $\zeta$ be a preset quantity satisfying $\zeta = [\omega(d\sigma_p^2/(Pn)),o(d^{-1/2}\sigma_p^{-1})]$ and $T$ be the smallest iteration number such that $\max_{k\in[2], (i,j)\in\cS} |F_k(\Wb^{(T)}; \xb_{i,j})|\ge \zeta/2$, then with probability at least $1-1/\poly(n)$, it holds that
\begin{align*}
&\max_{r}|\la\wb_{1,r}^{(T)},\vb\ra|, \max_{r}|\la\wb_{2,r}^{(T)},\ub\ra| = \Omega\bigg(\frac{\zeta^{1/2}}{m^{1/2}}\bigg),\quad \max_{r}|\la\wb_{1,r}^{(T)},\vb'\ra|, \max_{r}|\la\wb_{2,r}^{(T)},\ub'\ra| =\Omega\bigg(\frac{\rho\zeta^{1/2}}{P m^{1/2}}\bigg)\notag\\
&\max_{r}|\la\wb_{2,r}^{(T)},\vb\ra|, \max_{r}|\la\wb_{1,r}^{(T)},\ub\ra| = \tilde O(\zeta^{3/2}),\quad \max_{r}|\la\wb_{2,r}^{(T)},\vb'\ra|, \max_{r}|\la\wb_{1,r}^{(T)},\ub'\ra| =\tilde O(\zeta^{3/2})\notag\\
\end{align*}
\end{lemma}
\begin{proof}
% First we show that $T$ exists since by Lemma \ref{lemma:dominance_v}, we have for some $r\in[m]$,
% $|\la\wb_{1,r}^{(t+1)},\vb\ra|=[1+\Theta(\eta)]\cdot |\la\wb_{1,r}^{(t+1)},\vb\ra|$. Then this quantity will continue increasing until 


We will only prove the results for the inner products $\la\wb_{1,r}^{(t)},\vb\ra$, 
$\la\wb_{2,r}^{(t)},\ub\ra$,
$\la\wb_{1,r}^{(t)},\vb'\ra$, $\la\wb_{1,r}^{(t)},\ub\ra$, and $\la\wb_{1,r}^{(t)},\ub'\ra$, as the proof for the remaining  inner products will be exactly the same.





We first recall the update of $\la\wb_{1,r},\vb'\ra$:
\begin{align*}
\la\wb_{1,r}^{(t+1)},\vb'\ra &= \big[1 + \eta\gamma_1^{(t)}(\vb',\vb')\big]\cdot\la\wb_{1,r}^{(t)},\vb'\ra + \eta\gamma_1^{(t)}(\vb,\vb')\cdot\la\wb_{1,r}^{(t)},\vb\ra + \eta\gamma_1^{(t)}(\ub,\vb')\cdot\la\wb_{1,r}^{(t)},\ub\ra \notag\\
&\qquad + \eta\gamma_1^{(t)}(\ub',\vb')\cdot\la\wb_{1,r}^{(t)}, \ub'\ra + \sum_{i=1}^n\sum_{p\in[P]}\eta\gamma_1^{(t)}(\bxi_i^{(p)},\vb')\cdot\la\wb_{1,r}^{(t)},\bxi_i^{(p)}\ra.
\end{align*}
The using Lemma \ref{lemma:feature_learning_coefficients_v'_mixup} and the similar proof of Lemma \ref{lemma:dominance_v}, we can get
\begin{align*}
&\big|\gamma_1^{(t)}(\ub,\vb')\cdot\la\wb_{1,r}^{(t)},\ub\ra\big| = O\bigg(\frac{\zeta\rho}{P}\bigg)\cdot |\la\wb_{1,r}^{(t)},\ub\ra| = O\bigg(\frac{\zeta\rho\log^2(n)}{P}\bigg)\cdot |\la\wb_{1,r}^{(t)},\vb\ra|\notag\\
&\big|\gamma_1^{(t)}(\ub',\vb')\cdot\la\wb_{1,r}^{(t)},\ub\ra\big| = O\bigg(\frac{\zeta\rho^2}{P}\bigg)\cdot |\la\wb_{1,r}^{(t)},\ub\ra| = O\bigg(\frac{\zeta\rho^2\log^2(n)}{P}\bigg)\cdot |\la\wb_{1,r}^{(t)},\vb\ra|\\
&\bigg|\sum_{i=1}^n\sum_{p\in[P]}\gamma_1^{(t)}(\bxi_i^{(p)},\vb')\cdot\la\wb_{1,r}^{(t)},\bxi_i^{(p)}\ra\bigg|\le O\bigg(\bigg(\frac{\rho d^{1/2}\sigma_p}{P^{1/2}n^{1/2}}+\rho\zeta d^{1/2}\sigma_p\bigg)\cdot |\la\wb_{1,r}^{(t)},\vb\ra|\bigg).
\end{align*}
Therefore, noting that we have assumed $d\sigma_p=o(n/P)$ and $\zeta = o\big(\frac{1}{Pd^{1/2}\sigma_p}\big)$, 
\begin{align*}
\big|\gamma_1^{(t)}(\ub,\vb')\cdot\la\wb_{1,r}^{(t)},\ub\ra\big|, \big|\gamma_1^{(t)}(\ub',\vb')\cdot\la\wb_{1,r}^{(t)}, \ub'\ra\big|,\bigg|\sum_{i=1}^n\sum_{p\in[P]}\gamma_1^{(t)}(\bxi_i^{(p)},\vb')\cdot\la\wb_{1,r}^{(t)},\bxi_i^{(p)}\ra\bigg|\le c\cdot |\gamma_1^{(t)}(\vb,\vb')\cdot\la\wb_{1,r}^{(t)},\vb\ra|
\end{align*}
for some sufficiently small constant $c<0.5$.
Therefore, further applying Lemma \ref{lemma:feature_learning_coefficients_v'_mixup}, we can get that
\begin{align}\label{eq:update_v'_simplified}
\la\wb_{1,r}^{(t+1)},\vb'\ra &= \big[1 + \Theta(\eta\rho)\big]\cdot\la\wb_{1,r}^{(t)},\vb'\ra + \Theta(\eta\rho/P)\cdot\la\wb_{1,r}^{(t)},\vb\ra
\end{align}
Given the above equation, we are able to complete the proof by combining it with Lemma \ref{lemma:dominance_v}:
\begin{align}\label{eq:update_v_proof_v'}
\la\wb_{1,r}^{(t+1)},\vb\ra = \big[1 + \Theta(\eta)\big]\cdot \la\wb_{1,r}^{(t)},\vb\ra.
\end{align}
In particular, given the fact that $|\la\wb_{1,r}^{(0)},\vb\ra|=\Omega(\sigma_0)$,  we can get the following 
\begin{align}\label{eq:bound_v_proof_v'}
|\la\wb_{1,r}^{(T)},\vb\ra| = \Omega\bigg(\frac{\zeta^{1/2}}{m^{1/2}}\bigg)
\end{align}
for some $T = O\big(\frac{\log(\zeta/(m\sigma_0))}{\eta}\big)$. Besides, by Lemma \ref{lemma:closeness_gamma_v_gamma_u} and \eqref{eq:updates_v_tmp}, we have for any $r'\in[m]$,
\begin{align*}
\frac{|\la\wb_{1,r}^{(t+1)},\ub\ra|}{|\la\wb_{2,r'}^{(t+1)},\ub\ra|} &= \frac{|\la\wb_{1,r}^{(t)},\vb\ra|}{|\la\wb_{2,r'}^{(t)},\ub\ra|}\cdot\bigg(\frac{1 + \eta \gamma_1^{(t)}(\vb,\vb)\pm o\big(\eta/\polylog(n)\big)}{1+\eta \gamma_2^{(t)}(\ub,\ub) \pm o\big(\eta/\polylog(n)\big)}\bigg)\notag\\
&= \frac{|\la\wb_{1,r}^{(t)},\vb\ra|}{|\la\wb_{2,r'}^{(t)},\ub\ra|}\cdot \big[1 + \eta \cdot\big(\gamma_1^{(t)}(\vb,\vb) - \gamma_2^{(t)}(\ub,\ub) \big)\pm o\big(\eta/\polylog(n)\big)\big]\notag\\
& = \frac{|\la\wb_{1,r}^{(0)},\vb\ra|}{|\la\wb_{2,r'}^{(0)},\ub\ra|}\cdot \big[1 \pm o\big(\eta/\polylog(n)\big)\big]^t.
\end{align*}
Note that $t \le T= \tilde O(1/\eta)$, we can further get $\frac{|\la\wb_{1,r}^{(t+1)},\ub\ra|}{|\la\wb_{2,r'}^{(t+1)},\ub\ra|}=\Theta(1)\cdot \frac{|\la\wb_{1,r}^{(0)},\vb\ra|}{|\la\wb_{2,r'}^{(0)},\ub\ra|}$. This immediately implies that $\max_r|\la\wb_{2,r}^{(T)},\ub\ra| = \Theta(\max_r|\la\wb_{1,r}^{(T)},\vb\ra|) = \Omega\big(\zeta^{1/2}/m^{1/2}\big)$.






Moreover, \eqref{eq:update_v'_simplified} implies that
\begin{align*}
\la\wb_{1,r}^{(T)},\vb'\ra & = \big[1 + \Theta(\eta\rho)\big]^T\cdot \la\wb_{1,r}^{(0)},\vb'\ra +  \Theta(\eta\rho/P)\cdot\sum_{t=0}^{T-1}\big[1 + \Theta(\eta\rho)\big]^t\cdot \la\wb_{1,r}^{(t)},\vb\ra.
\end{align*}
Further note that $\la\wb_{1,r}^{(t)},\vb\ra$ has the same sign for all $t\le T$ and $[1+\Theta(\eta\rho)]^t=\Theta(1)$ for all $t\le T$, then define $T' = T-\Theta(1/\eta)$, we have
\begin{align*}
|\la\wb_{1,r}^{(T)},\vb'\ra| &= \bigg|\Theta(1)\cdot \la\wb_{1,r}^{(0)},\vb'\ra + \Theta(\eta\rho/P)\cdot\sum_{t=0}^{T-1} \la\wb_{1,r}^{(t)},\vb\ra\bigg|\notag\\
& \ge \Theta(\eta\rho/P)\cdot\bigg|\sum_{t=0}^{T-1} \la\wb_{1,r}^{(t)},\vb\ra\bigg| - \Theta(1)\cdot \big|\la\wb_{1,r}^{(0)},\vb'\ra\big|\notag\\
& \ge \Theta(\eta\rho/P)\cdot\sum_{t=T'}^{T-1} \big|\la\wb_{1,r}^{(t)},\vb\ra\big| - \tilde O(\sigma_0).
\end{align*}
Then by \eqref{eq:update_v_proof_v'} and \eqref{eq:bound_v_proof_v'}, we have for all $t\in[T', T-1]$, it holds that
\begin{align*}
|\la\wb_{1,r}^{(t)},\vb\ra| = \Theta\big(|\la\wb_{1,r}^{(T)},\vb\ra|\big) = \Omega\bigg(\frac{\zeta^{1/2}}{m^{1/2}}\bigg).
\end{align*}
Therefore, we can finally get
\begin{align*}
|\la\wb_{1,r}^{(T)},\vb'\ra| &\ge \Theta\bigg(\frac{(T-T')\eta \rho}{P}\cdot \bigg)\cdot\Omega\bigg(\frac{\zeta^{1/2}}{m^{1/2}}\bigg) - \tilde O(\sigma_0)\notag\\
& = \Omega\bigg(\frac{\rho\zeta^{1/2}}{Pm^{1/2}}\bigg).
\end{align*}

% On the other hand, we can also get the upper bound of $|\la\wb_{1,r}^{(T)},\vb'\ra|$: using \eqref{eq:update_v'_simplified}, we can get 
% \begin{align*}

% \end{align*}



The remaining part is to establish the upper bounds in terms of incorrect feature learning, i.e., $\la\wb_{1,r}^{(T)},\ub\ra$ and $\la\wb_{1,r}^{(T)},\ub'\ra$. Particularly, recall their update forms as follows:
\begin{align*}
\la\wb_{1,r}^{(t+1)},\ub\ra &= \big[1 - \eta\gamma_1^{(t)}(\ub,\ub)\big]\cdot\la\wb_{1,r}^{(t)},\ub\ra + \eta\gamma_1^{(t)}(\vb,\ub)\cdot\la\wb_{1,r}^{(t)},\vb\ra + \eta\gamma_1^{(t)}(\vb',\ub)\cdot\la\wb_{1,r}^{(t)},\vb'\ra \notag\\
&\qquad + \eta\gamma_1^{(t)}(\ub',\ub')\cdot\la\wb_{1,r}^{(t)}, \ub'\ra + \sum_{i=1}^n\sum_{p\in[P]}\eta\gamma_1^{(t)}(\bxi_i^{(p)},\ub)\cdot\la\wb_{1,r}^{(t)},\bxi_i^{(p)}\ra,\notag\\
\la\wb_{1,r}^{(t+1)},\ub'\ra &= \big[1 - \eta\gamma_1^{(t)}(\ub',\ub')\big]\cdot\la\wb_{1,r}^{(t)},\ub'\ra + \eta\gamma_1^{(t)}(\vb,\ub')\cdot\la\wb_{1,r}^{(t)},\vb\ra + \eta\gamma_1^{(t)}(\ub,\ub')\cdot\la\wb_{1,r}^{(t)},\ub\ra \notag\\
&\qquad + \eta\gamma_1^{(t)}(\vb',\ub')\cdot\la\wb_{1,r}^{(t)}, \vb'\ra + \sum_{i=1}^n\sum_{p\in[P]}\eta\gamma_1^{(t)}(\bxi_i^{(p)},\ub')\cdot\la\wb_{1,r}^{(t)},\bxi_i^{(p)}\ra.
\end{align*}
Then by Lemmas \ref{lemma:feature_learning_coefficients_u_incorrect_mixup} and \ref{lemma:feature_learning_coefficients_u'_incorrect_mixup}, we have
\begin{align*}
\max\big\{|\gamma_1^{(t)}(\ub',\ub)|, \gamma_1^{(t)}(\ub,\ub')|\big\}\le\min\big\{\gamma_1^{(t)}(\ub,\ub), \gamma_1^{(t)}(\ub',\ub')\big\}, 
\end{align*}
the above equations further yield
\begin{align*}
&|\la\wb_{1,r}^{(t+1)},\ub\ra| + |\la\wb_{1,r}^{(t+1)},\ub'\ra| \\&\le |\la\wb_{1,r}^{(t)},\ub\ra| + |\la\wb_{1,r}^{(t)},\ub'\ra| + O(\eta\zeta)\cdot |\la\wb_{1,r}^{(t)},\vb\ra| + O(\eta\zeta\rho/P) \cdot |\la\wb_{1,r}^{(t)},\vb'\ra|\notag\\
& \qquad + \bigg|\sum_{i=1}^n\sum_{p\in[P]}\eta\gamma_1^{(t)}(\bxi_i^{(p)},\ub)\cdot\la\wb_{1,r}^{(t)},\bxi_i^{(p)}\ra\bigg| + \bigg|\sum_{i=1}^n\sum_{p\in[P]}\eta\gamma_1^{(t)}(\bxi_i^{(p)},\ub')\cdot\la\wb_{1,r}^{(t)},\bxi_i^{(p)}\ra\bigg|.
\end{align*}
Then using the fact that $T\eta = O(\polylog(n))$, we can further obtain
\begin{align*}
&|\la\wb_{1,r}^{(T)},\ub\ra| + |\la\wb_{1,r}^{(T)},\ub'\ra| \le \tilde O(\sigma_0) + \tilde O(\zeta)\cdot \max_{t\in[T]}|\la\wb_{1,r}^{(t)},\vb\ra| + \tilde O(\zeta \rho/P)\cdot \max_{t\in[T]}|\la\wb_{1,r}^{(t)},\vb'\ra| \notag\\
&\qquad + \sum_{t=0}^{T-1}\bigg[\bigg|\sum_{i=1}^n\sum_{p\in[P]}\eta\gamma_1^{(t)}(\bxi_i^{(p)},\ub)\cdot\la\wb_{1,r}^{(t)},\bxi_i^{(p)}\ra\bigg| + \bigg|\sum_{i=1}^n\sum_{p\in[P]}\eta\gamma_1^{(t)}(\bxi_i^{(p)},\ub')\cdot\la\wb_{1,r}^{(t)},\bxi_i^{(p)}\ra\bigg|\bigg].
\end{align*}
Moreover, following the same procedure of \eqref{eq:bound_sum_noise_v}, we can get
\begin{align*}
&\sum_{t=0}^{T-1}\bigg|\sum_{i=1}^n\sum_{p\in[P]}\eta\gamma_1^{(t)}(\bxi_i^{(p)},\ub)\cdot\la\wb_{1,r}^{(t)},\bxi_i^{(p)}\ra\bigg|, \sum_{t=0}^{T-1}\bigg|\sum_{i=1}^n\sum_{p\in[P]}\eta\gamma_1^{(t)}(\bxi_i^{(p)},\ub')\cdot\la\wb_{1,r}^{(t)},\bxi_i^{(p)}\ra\bigg|\\
&\le \tilde O(\sigma_0) + \tilde O\bigg(\frac{d\sigma_p^2}{Pn}\bigg)\cdot\max_{t\in[T]}\big[|\la\wb_{1,r}^{(t)},\vb\ra|+|\la\wb_{1,r}^{(t)},\ub\ra|+|\la\wb_{1,r}^{(t)},\vb'\ra|+|\la\wb_{1,r}^{(t)},\ub'\ra|\big]
\end{align*}
Finally, using the assumption that $\zeta = \omega(d\sigma_p^2/(Pn))$, we can get that
\begin{align*}
&|\la\wb_{1,r}^{(T)},\ub\ra| + |\la\wb_{1,r}^{(T)},\ub'\ra| \notag\\
&\le \tilde O(\sigma_0) + \tilde O(\zeta)\cdot \Big[\max_{t\in[T]}|\la\wb_{1,r}^{(t)},\vb\ra|+\max_{t\in[T]}|\la\wb_{1,r}^{(t)},\vb'\ra|+\max_{t\in[T]}\big[|\la\wb_{1,r}^{(t)},\ub\ra|+|\la\wb_{1,r}^{(t)},\ub'\ra|\big]\Big].
\end{align*}
Besides, note that the above inequality actually holds for any $T'\le T$, thus
\begin{align*}
&|\la\wb_{1,r}^{(T')},\ub\ra| + |\la\wb_{1,r}^{(T')},\ub'\ra| \notag\\
&\le \tilde O(\sigma_0) + \tilde O(\zeta)\cdot \Big[\max_{t\in[T']}|\la\wb_{1,r}^{(t)},\vb\ra|+\max_{t\in[T']}|\la\wb_{1,r}^{(t)},\vb'\ra|+\max_{t\in[T']}\big[|\la\wb_{1,r}^{(t)},\ub\ra|+|\la\wb_{1,r}^{(t)},\ub'\ra|\big]\Big]\notag\\
&\le\tilde O(\sigma_0) + \tilde O(\zeta)\cdot \Big[\max_{t\in[T]}|\la\wb_{1,r}^{(t)},\vb\ra|+\max_{t\in[T]}|\la\wb_{1,r}^{(t)},\vb'\ra|+\max_{t\in[T]}\big[|\la\wb_{1,r}^{(t)},\ub\ra|+|\la\wb_{1,r}^{(t)},\ub'\ra|\big]\Big].
\end{align*}
This further implies that
\begin{align*} &\max_{t\in[T]}\big[|\la\wb_{1,r}^{(t)},\ub\ra| + |\la\wb_{1,r}^{(t)},\ub'\ra|\big]\notag\\
&\le \tilde O(\sigma_0) + \tilde O(\zeta)\cdot \Big[\max_{t\in[T]}|\la\wb_{1,r}^{(t)},\vb\ra|+\max_{t\in[T]}|\la\wb_{1,r}^{(t)},\vb'\ra|+\max_{t\in[T]}\big[|\la\wb_{1,r}^{(t)},\ub\ra|+|\la\wb_{1,r}^{(t)},\ub'\ra|\big]\Big].
\end{align*}
Then, rearranging terms will readily give the following result:
\begin{align*}
|\la\wb_{1,r}^{(T)},\ub\ra| + |\la\wb_{1,r}^{(T)},\ub'\ra| &\le \max_{t\in[T]}\big[|\la\wb_{1,r}^{(t)},\ub\ra| + |\la\wb_{1,r}^{(t)},\ub'\ra|\big]\notag\\
&\le \tilde O(\sigma_0) + \tilde O(\zeta)\cdot \big[\max_{t\in[T]}\big[|\la\wb_{1,r}^{(t)},\vb\ra|+\max_{t\in[T]}\big[|\la\wb_{1,r}^{(t)},\vb'\ra|\big]\notag\\
&\le \tilde O(\zeta^{3/2}),
\end{align*}
where the last inequality holds since we must have
\begin{align*}
\max_{t\in[T]}|\la\wb_{1,r}^{(t)},\vb\ra|,\max_{t\in[T]}|\la\wb_{1,r}^{(t)},\vb'\ra| = O\big(\log(n)\cdot\zeta^{1/2}\big)
\end{align*}
as otherwise, we cannot have
$\max_{k\in[2], (i,j)\in\cS} |F_k(\Wb^{(T)}; \xb_{i,j})|\le \zeta/2$ for all $t\le T$, which contradicts the condition made in this lemma. This completes the upper bounds of $|\la\wb_{1,r}^{(T)},\ub\ra|$  and $|\la\wb_{1,r}^{(T)},\ub'\ra|$.
% Plugging the above inequality into \eqref{eq:upperbound_w_u}, we can get
% \begin{align}\label{eq:upperbound_w_u}
% |\la\wb_{1,r}^{(t+1)},\ub\ra| 
% &\le \tilde O(\sigma_0) + \tilde O\big(\eta(\zeta+\alpha)\big)\cdot\sum_{\tau=0}^t|\la\wb_{1,r}^{(\tau)},\ub\ra| + O\bigg(\frac{\eta\rho}{P}+\frac{t\rho\eta^2}{Pn}\bigg)\sum_{\tau=0}^t\big[|\la\wb_{1,r}^{(\tau)},\vb'\ra| +|\la\wb_{1,r}^{(\tau)},\ub'\ra| \big]\notag\\
% & \qquad+ O\bigg(\frac{t \eta d^{1/2}\sigma_0\sigma_p}{P^{1/2}n^{1/2}}\bigg)+ O\bigg(\frac{t\eta^2}{Pn}\bigg)\cdot\sum_{\tau=0}^t\big[|\la\wb_{1,r}^{(\tau)},\vb\ra| +|\la\wb_{1,r}^{(\tau)},\ub\ra|+|\la\wb_{1,r}^{(\tau)},\ub'\ra| \big]\notag\\
% &\qquad  + O\bigg(\frac{\eta\zeta}{Pn}+\frac{t\eta^2\zeta d\sigma_p^2}{n^2P}\bigg)\cdot\sum_{\tau=0}^{t}\sum_{s=1}^n\sum_{p\in[P]}|\la\wb_{1,r}^{(\tau)},\bxi_s^{(p)}\ra|.
% \end{align}



\end{proof}


\subsection{Proof of Theorem \ref{thm:Mixup_training}}\label{sec:proof_mixup}
\begin{proof}[Proof of Theorem \ref{thm:Mixup_training}] 

We will evaluate the test error for common feature data and rare feature data separately. In particular, take the positive data $(\xb,1)$ as an example. Then note that the data $\xb$ consists of the common feature $\vb$, we can obtain the following by Lemma \ref{lemma:outcome_Mixup_main}:
\begin{align*}
F_1(\Wb^{(t)};\xb) = \sum_{r=1}^m \sum_{p=1}^P\big(\la\wb_{1,r}^{(t)},\xb^{(p)}\ra\big)^2 \ge \sum_{r=1}^m\sum_{p:\xb^{(p)}=\vb} \big(\la\wb_{1,r}^{(t)},\vb\ra\big)^2=\tilde \Omega(\zeta).
\end{align*}
On the other hand, we can follow the similar proof of Theorem \ref{thm:std_training} to show that $|\la\wb_{k,r}^{(T)},\bzeta\ra|^2=\tilde O(\sigma_p^2n^2)$ with probability at least $1-1/\poly(n)$, then it follows that
\begin{align*}
F_2(\Wb^{(t)};\xb) = \sum_{r=1}^m \sum_{p=1}^P\big(\la\wb_{2,r}^{(t)},\xb^{(p)}\ra\big)^2  \le \tilde O(b\alpha^2\zeta^3) + \tilde O(\sigma_p^2n^2) < F_1(\Wb^{(t)};\xb).
\end{align*}
where we use the fact that $b\alpha^2 = o(1/\polylog(n))$ and $d = \omega(n^3P)$. 
This clearly suggests that
\begin{align*}
\PP_{(\xb, y)\sim \cD_{\mathrm{common}}}[ \argmax_k F_k(\Wb^{(t)},\xb)\neq y] \le \frac{1}{\poly(n)}.
\end{align*}

Then let's move on to the rare feature data. In particular, consider the positive rare feature data $(\xb, 1)$, which contains the rare feature $\vb'$, we have
\begin{align*}
F_1(\Wb^{(t)};\xb) = \sum_{r=1}^m \sum_{p=1}^P\big(\la\wb_{1,r}^{(t)},\xb^{(p)}\ra\big)^2\ge \ge \sum_{r=1}^m\sum_{p:\xb^{(p)}=\vb'} \big(\la\wb_{1,r}^{(t)},\vb\ra\big)^2=\tilde \Omega(\rho^2\zeta).  
\end{align*}
On the other hand, it holds that
\begin{align*}
F_2(\Wb^{(t)};\xb) = \sum_{r=1}^m \sum_{p=1}^P\big(\la\wb_{2,r}^{(t)},\xb^{(p)}\ra\big)^2  \le \tilde O(b\alpha^2\zeta^3) + \tilde O(\sigma_p^2n^2) =o(\rho^2\zeta)< F_1(\Wb^{(t)};\xb),
\end{align*}
where we use the fact that $b\alpha^2\zeta^2 = o(\rho)$ and $d = \omega(n^3P^3/\rho^2)$. Therefore, this implies that 
\begin{align*}
\PP_{(\xb, y)\sim \cD_{\mathrm{rare}}}[ \argmax_k F_k(\Wb^{(t)},\xb)\neq y] \le \frac{1}{\poly(n)}.
\end{align*}
Putting the results for common feature data and rare feature data together, we are able to complete the proof.

\end{proof}




% \paragraph{Analysis on the data $(i,j)\in\cS_{0, 0}^{+, +}\cup\cS_{0, 0}^{+, -}\cup\cS_{0, 1}^{+, +}\cup\cS_{0, 1}^{+, -}$.}

% Note that by Lemma \ref{lemma:data_patches_mixup}, we have for all data $(i,j)\in\cS_{0, 0}^{+, +}\cup\cS_{0, 0}^{+, -}\cup\cS_{0, 1}^{+, +}\cup\cS_{0, 1}^{+, -}$, there will be at least a constant patches of the data that can be formulated as $\lambda \vb +(1-\lambda)\bb$, where $\bb$ is either parallel to $\vb$ (e.g., $\vb$ or $\alpha\vb$) or orthogonal to $\vb$. Therefore, denote this set of data patches by $\cP_{i,j}^*(\vb)$, 
% % using the fact that in the initial phase, $\ell_{1,(i,j)}^{(t)} = \Theta(1)$ for all $(i,j)\in\cS_{*, *}^{+, *}$, where $*$ can be arbitrary choice, 
% then for any $(i,j)\in\cS_{0, 0}^{+, +}\cup\cS_{0, 0}^{+, -}\cup\cS_{0, 1}^{+, +}\cup\cS_{0, 1}^{+, -}$, we have
% \begin{align}\label{eq:gradient_positive_data_mixup}
% \sum_{p\in[P]}\la\wb_{1,r}^{(t)},\xb_{i,j}^{(p)}\ra\cdot\la\xb_{i,j}^{(p)}, \vb\ra &= \sum_{p\in\cP_{i,j}^*(\vb)}\la\wb_{1,r}^{(t)},\lambda\vb + (1-\lambda)\bb_{i,j}^{(p)}\ra\cdot\big(\lambda\|\vb\|_2^2 + (1-\lambda)\la\bb_{i,j}^{(p)},\vb\ra\big)\notag\\
% &\qquad + \sum_{p\not\in\cP_{i,j}^*(\vb)}\la\wb_{1,r}^{(t)}, \xb_{i,j}^{(p)}\ra\cdot\la \xb_{i,j}^{(p)},\vb\ra.
% \end{align}
% Moreover, regarding the patch $p\in\cP_{i,j}^*(\vb)$, we have either
% \begin{align*}
% \la\wb_{1,r}^{(t)},\lambda\vb + (1-\lambda)\bb_{i,j}^{(p)}\ra\cdot\big(\lambda\|\vb\|_2^2 + (1-\lambda)\la\bb_{i,j}^{(p)},\vb\ra\big) = a_{i,j}^{(p)} \la\wb_{1,r}^{(t)}, \vb\ra,
% \end{align*}
% where $a_{i,j}^{(p)}\in\{1, [\lambda + (1-\lambda)\alpha]^2\}$ if $\bb_{i,j}^{(p)}$ is parallel to $\vb$, or
% \begin{align*}
% \la\wb_{1,r}^{(t)},\lambda\vb + (1-\lambda)\bb_{i,j}^{(p)}\ra\cdot\big(\lambda\|\vb\|_2^2 + (1-\lambda)\la\bb_{i,j}^{(p)},\vb\ra\big) = \lambda^2\la\wb_{1,r}^{(t)},\vb\ra + \lambda(1-\lambda)\la\wb_{1,r}^{(t)},\bb_{i,j}^{(p)}\ra
% \end{align*}
% if $\bb_{i,j}^{(p)}$ is orthogonal to $\vb$, where we use the fact that $\|\vb\|_2=1$. Let $\cP_{i,j}^{*,1}(\vb)$ and $\cP_{i,j}^{*,2}(\vb)$ denote the set of patches that fall into the above two cases respectively, we can further extend the definition of $a_{i,j}^{(p)}$ to $p\not\in\cP_{i,j}^{*,1}(\vb)$ by setting $a_{i,j}^{(p)}=\lambda^2$ for $p\not\in\cP_{i,j}(\vb)$, we have $a_{i,j}^{(p)} \ge \lambda^2$ and 
% \begin{align}\label{eq:feature_learning_strongfeat_data_mixup}
% \sum_{p\in\cP_{i,j}^*(\vb)}\la\wb_{1,r}^{(t)},\xb_{i,j}^{(p)}\ra\cdot\la\xb_{i,j}^{(p)}, \vb\ra &= \sum_{p\in\cP_{i,j}^{*,1}(\vb)}a_{i,j}^{(p)}\la\wb_{1,r}^{(t)}, \vb\ra + \sum_{p\in\cP_{i,j}^{*,2}(\vb)}\Big[\lambda^2\la\wb_{1,r}^{(t)},\vb\ra+\lambda(1-\lambda)\la\wb_{1,r}^{(t)}, \bb_{i,j}^{(p)}\ra\Big]\notag\\
% & = \sum_{p\in\cP_{i,j}^*(\vb)}a_{i,j}^{(p)}\la\wb_{1,r}^{(t)},\vb\ra +\lambda(1-\lambda) \sum_{p\in\cP_{i,j}^{*,2}(\vb)}\la\wb_{1,r}^{(t)}, \bb_{i,j}^{(p)}\ra.
% \end{align}
% where it can be seen that $\bb_{i,j}^{(p)}\in\{\ub, \alpha\ub, \vb', \ub'\}\cup\{\bxi\}$.

% Besides, let $\cP_{i,j}'(\vb)$ be the set of $(i,j)$ that satisfies $(i,j)\not\in\cP_{i,j}^*(\vb)$ and $\la\xb_{i,j}^{(p)},\vb\ra>0$, which implies that $\cP_{i,j}'(\vb)\cup\cP_{i,j}^*(\vb) = \cP_{i,j}(\vb)$. Then we have
% \begin{align*}
% \sum_{p\in\cP_{i,j}'(\vb)}\la\wb_{1,r}^{(t)}, \xb_{i,j}^{(p)}\ra\cdot\la\xb_{i,j}^{(p)},\vb\ra = \sum_{p\in\cP_{i,j}'(\vb)}\Big[a_{i,j}^{(p)}\la\wb_{1,r}^{(t)}, \vb\ra + b_{i,j}^{(p)}\la\wb_{1,r},\bb_{i,j}^{(p)}\ra\Big],
% \end{align*}
% where $a_{i,j}^{(p)}\in(0, 1)$,  $b_{i,j}^{(p)}\in[0, 1]$, and $\bb_{i,j}^{(p)}\in\{\ub, \alpha\ub, \vb', \ub'\}\cup\{\bxi\}$ if $b_{i,j}^{(p)}\neq 0$.

% Lastly, for the data $(i, j)\not\in\cP_{i,j}(\vb)$, we have $\xb_{i,j}^{(p)} \perp \vb$ so that $\la\xb_{i,j}^{(p)},\vb\ra=0$.

% Combining the above results and plugging them into \eqref{eq:gradient_positive_data_mixup}, we can get
% \begin{align}\label{eq:update_decomposition_data1_strongfeat_mixup}
% \sum_{p\in[P]}\la\wb_{1,r}^{(t)},\xb_{i,j}^{(p)}\ra\cdot\la\xb_{i,j}^{(p)}, \vb\ra = \sum_{p\in\cP_{i,j}(\vb)}\Big[a_{i,j}^{(p)}\la\wb_{1,r}^{(t)},\vb\ra+b_{i,j}^{(p)}\la\wb_{1,r}^{(t)}, \bb_{i,j}^{(p)}\ra\Big],
% \end{align}
% where $\bb_{i,j}^{(p)}\in\{\ub, \alpha\ub, \vb', \ub'\}\cup\{\bxi\}$ if $b_{i,j}^{(p)}\neq 0$.
% Moreover, we can also have $b_{i,j}\in[0, 1]$ and 
% \begin{align*}
% \sum_{p\in\cP_{i,j}(\vb)}a_{i,j}^{(p)}\ge \lambda^2\cdot |\cP_{i,j}^*(\vb)|,
% \end{align*}
% according to \eqref{eq:feature_learning_strongfeat_data_mixup}.
























% \paragraph{Analysis on the data $(i,j)\in\cS_{1, 1}^{+, +}\cup\cS_{1, 1}^{+, -}\cup\cS_{1, 0}^{+, +}\cup\cS_{1,0}^{+, -}$.}
% Following the similar analysis on the data $(i,j)\in\cS_{0, 0}^{+, +}\cup\cS_{0, 0}^{+, -}\cup\cS_{0, 1}^{+, +}\cup\cS_{0, 1}^{+, -}$, we can also get that
% \begin{align}\label{eq:update_decomposition_data2_strongfeat_mixup}
% \sum_{p\in[P]}\la\wb_{1,r}^{(t)},\xb_{i,j}^{(p)}\ra\cdot\la\xb_{i,j}^{(p)}, \vb\ra = \sum_{p\in\cP_{i,j}(\vb)}\Big[a_{i,j}^{(p)}\la\wb_{1,r}^{(t)},\vb\ra+b_{i,j}^{(p)}\la\wb_{1,r}^{(t)}, \bb_{i,j}^{(p)}\ra\Big],
% \end{align}
% where $a_{i,j}^{(p)}\in[0, 1]$,  $b_{i,j}^{(p)}\in[0, 1]$, and $\bb_{i,j}^{(p)}\in\{\ub, \alpha\ub, \vb', \ub'\}\cup\{\bxi\}$ if $b_{i,j}^{(p)}\neq 0$.

% \paragraph{Analysis on the data $(i,j)\in\cS_{0, 0}^{-, -}\cup\cS_{0, 0}^{-, +}\cup\cS_{0, 1}^{-, -}\cup\cS_{0, 1}^{-, +}$.} 
% Similarly, we will focus on the data patches that contain the feature vector $\vb$. Particularly, we can also get the similar decomposition as in \eqref{eq:update_decomposition_data1_strongfeat_mixup} and \eqref{eq:update_decomposition_data2_strongfeat_mixup}:
% \begin{align}\label{eq:update_decomposition_data3_strongfeat_mixup}
% \sum_{p\in[P]}\la\wb_{1,r}^{(t)},\xb_{i,j}^{(p)}\ra\cdot\la\xb_{i,j}^{(p)}, \vb\ra = \sum_{p\in\cP_{i,j}(\vb)}\Big[a_{i,j}^{(p)}\la\wb_{1,r}^{(t)},\vb\ra+b_{i,j}^{(p)}\la\wb_{1,r}^{(t)}, \bb_{i,j}^{(p)}\ra\Big],
% \end{align}
% where $a_{i,j}^{(p)}\in[0, 1]$,  $b_{i,j}^{(p)}\in[0, 1]$, and $\bb_{i,j}^{(p)}\in\{\ub, \alpha\ub, \vb', \ub'\}\cup\{\bxi\}$ if $b_{i,j}^{(p)}\neq 0$. We will then seek for a sharp characterization on $a_{i,j}^{(p)}$, which basically relates to the magnitude of the projection of $\xb_{i,j}^{(p)}$ on $\vb$. 

% First, for some data such as $(i,j)\in\cS_{0,0}^{-,+}\cup\cS_{0, 1}^{-,+}$, we will have a constant number of patches that contain the feature vector $\vb$ with strength within the range $[1-\lambda, 1-\lambda + \alpha\lambda]$, because the original data $\xb_j$ has a constant number of patches that are $\vb$. Therefore, we define $\cP_{i,j}^*(\vb)$ as the set of patch indices that $|\la\xb_{i,j}^{(p)},\vb\ra|\ge \lambda$.
% Then it is clear that for any $p\in\cP_{i,j}(\vb)\backslash \cP_{i,j}^*(\vb)$, we have $|\la\xb_{i,j}^{(p)},\vb\ra|\le \alpha$ since the data patches only have feature noise vectors that contain $\vb$. Based on this, we can get that  
% \begin{align*}
% \sum_{p\in\cP_{i,j}(\vb)}a_{i,j}^{(p)} = \sum_{p\in\cP_{i,j}^*(\vb)}a_{i,j}^{(p)} + \sum_{p\in\cP_{i,j}\backslash\cP_{i,j}^*(\vb)}a_{i,j}^{(p)}\le [1-\lambda + \alpha\lambda]^2\cdot |\cP_{i,j}^*(\vb)| + O(b\alpha^2),
% \end{align*}
% where inequality is due to the data distribution that each data has at most $b$ feature noise patches. 

% \paragraph{Analysis on the data $(i,j)\in\cS_{1, 1}^{-, -}\cup\cS_{1, 1}^{-, +}\cup\cS_{1, 0}^{-, +}\cup\cS_{1,0}^{-, -}$.}

% Following the similar analysis on the data $(i,j)\in\cS_{1, 1}^{+, +}\cup\cS_{1, 1}^{+, -}\cup\cS_{1, 0}^{+, +}\cup\cS_{1,0}^{+, -}$, we can get that
% \begin{align}\label{eq:update_decomposition_data4_strongfeat_mixup}
% \sum_{p\in[P]}\la\wb_{1,r}^{(t)},\xb_{i,j}^{(p)}\ra\cdot\la\xb_{i,j}^{(p)}, \vb\ra = \sum_{p\in\cP_{i,j}(\vb)}\Big[a_{i,j}^{(p)}\la\wb_{1,r}^{(t)},\vb\ra+b_{i,j}^{(p)}\la\wb_{1,r}^{(t)}, \bb_{i,j}^{(p)}\ra\Big],
% \end{align}
% where $a_{i,j}^{(p)}\in[0, 1]$,  $b_{i,j}^{(p)}\in[0, 1]$, and $\bb_{i,j}^{(p)}\in\{\ub, \alpha\ub, \vb', \ub'\}\cup\{\bxi\}$ if $b_{i,j}^{(p)}\neq 0$.
% Moreover, following the similar analysis on the data $(i,j)\in\cS_{0, 0}^{-, -}\cup\cS_{0, 0}^{-, +}\cup\cS_{0, 1}^{-, -}\cup\cS_{0, 1}^{-, +}$, we can get that
% \begin{align*}
% \sum_{p\in\cP_{i,j}(\vb)}a_{i,j}^{(p)}\le [\lambda + \alpha(1-\lambda)]^2\cdot|\cP_{i,j}^*(\vb)| + O(b\alpha^2).
% \end{align*}


% \paragraph{Putting everything together.}
% We first recall the definition of $\ell_{1,(i,j)}^{(t)}$, we can get that in the initial phase it holds that:
% \begin{itemize}
%     \item For all mixed data $(i,j)$ with $y_i=1$ and $y_j=1$, we have $\ell_{1,(i,j)}^{(t)} = 0.5 \pm O\big(\frac{1}{\polylog(n)}\big)$;
%     \item For all mixed data $(i,j)$ with $y_i=1$ and $y_j=2$, we have $\ell_{1,(i,j)}^{(t)} = \lambda - 0.5 \pm O\big(\frac{1}{\polylog(n)}\big)$;
%     \item For all mixed data $(i,j)$ with $y_i=2$ and $y_j=1$, we have $\ell_{1,(i,j)}^{(t)} = 0.5 - \lambda \pm O\big(\frac{1}{\polylog(n)}\big)$;
%     \item For all mixed data $(i,j)$ with $y_i=2$ and $y_j=2$, we have $\ell_{1,(i,j)}^{(t)} = -0.5 \pm O\big(\frac{1}{\polylog(n)}\big)$.
% \end{itemize}

% Combining the previous analyses on all mixed data, we can get that
% \begin{align*}
% \sum_{(i,j)\in\cS^+} \ell_{1, (i,j)}^{(t)}\sum_{p\in[P]}\la\wb_{1,r}^{(t)},\xb_{i,j}^{(p)}\ra\cdot\la\xb_{i,j}^{(p)}, \vb\ra &= \sum_{(i,j)\in\cS^+} \ell_{1, (i,j)}^{(t)}\sum_{p\in\cP_{i,j}(\vb)}\Big[a_{i,j}^{(p)}\la\wb_{1,r}^{(t)},\vb\ra+b_{i,j}^{(p)}\la\wb_{1,r}^{(t)}, \bb_{i,j}^{(p)}\ra\Big]\notag\\
% & = a^+(\vb) \cdot\la\wb_{1,r}^{(t)}, \vb\ra + \sum_{(i,j)\in\cS^+}\sum_{p\in\cP_{i,j}(\vb)}|b_{i,j}^{(p)}||\la\wb_{1,r}^{(t)}, \bb_{i,j}^{(p)}\ra|,
% \end{align*}
% where we use the fact that $|b_{i,j}^{(p)}|\le 1$ and
% \begin{align*}
% a^+(\vb): = \sum_{(i,j)\in\cS^+} \ell_{1, (i,j)}^{(t)}\sum_{p\in\cP_{i,j}(\vb)}a_{i,j}^{(p)}.
% \end{align*}
% Then using the results for $\ell_{1,(i,j)}^{(t)}$, we can get
% \begin{align*}
% a^+(\vb)&\ge \sum_{(i,j)\in\cS_{0, 0}^{+, +}\cup\cS_{0, 0}^{+, -}\cup\cS_{0, 1}^{+, +}\cup\cS_{0, 1}^{+, -}}\ell_{1, (i,j)}^{(t)} \sum_{p\in\cP_{i,j}(\vb)}a_{i,j}^{(p)} \notag\\
% & \ge \lambda^2\cdot\bigg(0.5 - O\bigg(\frac{1}{\polylog(n)}\bigg)\bigg)\cdot\sum_{(i,j)\in\cS_{0,0}^{+, +}\cup\cS_{0, 1}^{+, +}}  |\cP_{i,j}^*(\vb)|\notag\\
% & \qquad + \lambda^2\cdot\bigg(\lambda - 0.5 - O\bigg(\frac{1}{\polylog(n)}\bigg)\bigg)\cdot\sum_{(i,j)\in\cS_{0,0}^{+, -}\cup\cS_{0, 1}^{+, -}} |\cP_{i,j}^*(\vb)|.
% \end{align*}
% Moreover, we can further get that 
% \begin{align*}
% \sum_{(i,j)\in\cS_{0,0}^{+, +}}|\cP_{i,j}^*(\vb)| &\ge \sum_{i\in\cS_0^+, j\in\cS_0^+}|\cP_{i,j}^*(\vb)| = |\cS_0^+|\cdot \sum_{i\in\cS_0^+}|\cP_{i,j}^*(\vb)| \notag\\
% \sum_{(i,j)\in\cS_{0,1}^{+, +}}|\cP_{i,j}^*(\vb)|& = \sum_{i\in\cS_0^+, j\in\cS_1^+}|\cP_{i,j}^*(\vb)| = |\cS_1^+|\cdot \sum_{i\in\cS_0^+}|\cP_{i,j}^*(\vb)|\notag\\
% \sum_{(i,j)\in\cS_{0,0}^{+, -}}|\cP_{i,j}^*(\vb)|&=\sum_{i\in\cS_0^+, j\in\cS_0^-}|\cP_{i,j}^*(\vb)| = |\cS_0^-|\cdot \sum_{i\in\cS_0^+}|\cP_{i,j}^*(\vb)|\notag\\
% \sum_{(i,j)\in\cS_{0,1}^{+, -}}|\cP_{i,j}^*(\vb)| &=\sum_{i\in\cS_0^+, j\in\cS_1^-}|\cP_{i,j}^*(\vb)| = |\cS_1^-|\cdot \sum_{i\in\cS_0^+}|\cP_{i,j}^*(\vb)|,
% \end{align*}
% where we recall the definition that $\cS_0^+$ denotes the set of common feature data with label $1$ and $\cP_i^*(\vb)$ denotes the set of patches satisfying $\xb_i^{(p)}=\vb$. This further implies that
% \begin{align}\label{eq:bound_a+}
% a^+(\vb)&\ge \lambda^2\cdot \sum_{i\in\cS_0^+}|\cP_{i,j}^*(\vb)|\cdot \bigg[\bigg(0.5 - O\bigg(\frac{1}{\polylog(n)}\bigg)\bigg)\cdot(|\cS_0^+| + |\cS_1^+|) \notag\\
% &\hspace{30mm}+ \bigg(\lambda - 0.5 - O\bigg(\frac{1}{\polylog(n)}\bigg)\bigg)\cdot(|\cS_0^-| + |\cS_1^-|)\bigg].
% \end{align}
% % Finally, using the fact that $|\cS_0^+|+|\cS_1^+|+|\cS_0^-|+|\cS_1^-|=n$, we can get if $\la\wb_{1,r},\vb\ra\ge 0$,
% % \begin{align*}
% % \sum_{(i,j)\in\cS^+} \ell_{1, (i,j)}^{(t)}\sum_{p\in[P]}\la\wb_{1,r}^{(t)},\xb_{i,j}^{(p)}\ra\cdot\la\xb_{i,j}^{(p)}, \vb\ra &\ge  n\sum_{i\in\cS_0^+}|\cP_i^*(\vb)|\cdot\la\wb_{1,r}^{(t)}, \vb\ra - \sum_{(i,j)\in\cS^+}\sum_{p\in\cP_{i,j}(\vb)}|\la\wb_{1,r}^{(t)}, \bb_{i,j}^{(p)}\ra|;
% % \end{align*}
% % and if $\la\wb_{1,r},\vb\ra<0$,
% % \begin{align*}
% % \sum_{(i,j)\in\cS^+} \ell_{1, (i,j)}^{(t)}\sum_{p\in[P]}\la\wb_{1,r}^{(t)},\xb_{i,j}^{(p)}\ra\cdot\la\xb_{i,j}^{(p)}, \vb\ra &\le n \sum_{i\in\cS_0^+}|\cP_i^*(\vb)|\cdot\la\wb_{1,r}^{(t)}, \vb\ra + \sum_{(i,j)\in\cS^+}\sum_{p\in\cP_{i,j}(\vb)}|\la\wb_{1,r}^{(t)}, \bb_{i,j}^{(p)}\ra|.
% % \end{align*}

% On the other hand, we can get the following results for $(i,j)\in\cS^-$:
% \begin{align*}
% \sum_{(i,j)\in\cS^-} \ell_{1, (i,j)}^{(t)}\sum_{p\in[P]}\la\wb_{1,r}^{(t)},\xb_{i,j}^{(p)}\ra\cdot\la\xb_{i,j}^{(p)}, \vb\ra &= \sum_{(i,j)\in\cS^-} \ell_{1, (i,j)}^{(t)}\sum_{p\in\cP_{i,j}(\vb)}\Big[a_{i,j}^{(p)}\la\wb_{1,r}^{(t)},\vb\ra+b_{i,j}^{(p)}\la\wb_{1,r}^{(t)}, \bb_{i,j}^{(p)}\ra\Big]\notag\\
% & = a^-(\vb) \cdot\la\wb_{1,r}^{(t)}, \vb\ra + \sum_{(i,j)\in\cS^-}\ell_{1, (i,j)}^{(t)}\sum_{p\in\cP_{i,j}(\vb)}b_{i,j}^{(p)}\la\wb_{1,r}^{(t)}, \bb_{i,j}^{(p)}\ra,
% \end{align*}
% where
% \begin{align*}
% a^-(\vb): = \sum_{(i,j)\in\cS^-} \ell_{1, (i,j)}^{(t)}\sum_{p\in\cP_{i,j}(\vb)}a_{i,j}^{(p)}.
% \end{align*}
% Therefore, by our previous analyses on the data $(i,j)\in\cS^-$ and use the fact that $\ell_{1,(i,j)}^{(t)}<0$ for all $(i,j)\in\cS^-$, we have
% \begin{align*}
% a^-(\vb) \ge \sum_{(i,j)\in\cS^-}\ell_{1,(i,j)}^{(t)}\cdot \big[[1-\lambda + \alpha\lambda]^2\cdot |\cP_{i,j}^*(\vb)| + O(b\alpha^2)\big].
% \end{align*}
% Note that for any mixed data $(i,j)\in\cS^-$, the data $\xb_i$ has no common feature data patch, implying that $\cP_{i,j}^*(\vb) = \cP_j^*(\vb)$. Therefore, we have 
% \begin{align*}
% \sum_{(i,j)\in\cS_{0,0}^{-,-}}|\cP_{i,j}^*(\vb)|, \sum_{(i,j)\in\cS_{0,1}^{-,-}}|\cP_{i,j}^*(\vb)|, \sum_{(i,j)\in\cS_{0,1}^{-,+}}|\cP_{i,j}^*(\vb)| &=0\notag\\
% \sum_{(i,j)\in\cS_{1,0}^{-,-}}|\cP_{i,j}^*(\vb)|, \sum_{(i,j)\in\cS_{1,1}^{-,-}}|\cP_{i,j}^*(\vb)|, \sum_{(i,j)\in\cS_{1,1}^{-,+}}|\cP_{i,j}^*(\vb)|& = 0,
% \end{align*}
% and 
% \begin{align*}
% \sum_{(i,j)\in\cS_{0,0}^{-, +}}|\cP_{i,j}^*(\vb)|& = \sum_{i\in\cS_0^-, j\in\cS_0^+}|\cP_{i,j}^*(\vb)| = |\cS_0^+|\cdot \sum_{i\in\cS_0^+}|\cP_i^*(\vb)|\notag\\
% \sum_{(i,j)\in\cS_{1,0}^{-, +}}|\cP_{i,j}^*(\vb)|&=\sum_{i\in\cS_1^-, j\in\cS_1^+}|\cP_{i,j}^*(\vb)| = |\cS_1^-|\cdot \sum_{i\in\cS_0^+}|\cP_i^*(\vb)|.
% \end{align*}
% Therefore, we can finally get the following lower bound on $a^-(\vb)$:
% \begin{align}\label{eq:bound_a-}
% a^{-}(\vb)\ge -\bigg(\lambda-0.5 + O\bigg(\frac{1}{\polylog(n)}\bigg)\bigg)\cdot\bigg[ [1-\lambda + \alpha\lambda]^2(|\cS_0^+|+|\cS_1^-|)\cdot\sum_{i\in\cS_0^+}|\cP_i^*(\vb) | + O\big(n^2b\alpha^2\big)\bigg],
% \end{align}
% where we use the fact that $|\cS^-| = n^2$ and $\ell_{1,(i,j)}^{(t)} = 0.5-\lambda \pm O\big(\frac{1}{\polylog(n)}\big)$ for all $(i,j)\in \cS^-$.

% Finally, combining the results for data in $\cS^+$ and $\cS^-$, we can get
% \begin{align*}
% \sum_{i,j\in[n]} \ell_{1, (i,j)}^{(t)}\sum_{p\in[P]}\la\wb_{1,r}^{(t)},\xb_{i,j}^{(p)}\ra\cdot\la\wb_{i,j}^{(p)}, \vb\ra &= \big[a^+(\vb) + a^-(\vb)\big]\cdot\la\wb_{1,r}^{(t)}, \vb\ra + \sum_{i,j\in[n]}\ell_{1,(i,j)}^{(t)}\sum_{p\in\cP_{i,j}(\vb)}b_{i,j}^{(p)}\la\wb_{1,r}^{(t)},\bb_{i,j}^{(p)}\ra.
% \end{align*}
% According to our bounds on $a^+(\vb)$ and $a^-(\vb)$ in \eqref{eq:bound_a+} and \eqref{eq:bound_a-}, we can get
% \begin{align*}
% a^+(\vb) + a^-(\vb) &\ge \bigg[-[1-\lambda + \alpha\lambda]^2\bigg(\lambda-0.5+O\bigg(\frac{1}{\polylog(n)}\bigg)\bigg)\cdot(|\cS_0^+|+\cS_1^-|)\notag\\
% &\qquad +\lambda^2\bigg(0.5-O\bigg(\frac{1}{\polylog(n)}\bigg)\bigg)\cdot (|\cS_0^+| + |\cS_1^+|)  \notag\\
% &\qquad+ \lambda^2 \bigg(\lambda -0.5-O\bigg(\frac{1}{\polylog(n)}\bigg)\bigg)\cdot (|\cS_0^-| + |\cS_1^-|)\bigg]\cdot\sum_{i\in\cS_0^+}|\cP_i^*(\vb)| -O(n^2b\alpha^2)\notag\\
% &\ge \bigg(\lambda - 0.5-O\bigg(\frac{1}{\polylog(n)}\bigg)\bigg)\cdot (|\cS_0^-| + |\cS_1^-|)\cdot\sum_{i\in\cS_0^+}|\cP_i^*(\vb)|- O(n^2b\alpha^2)\notag\\
% & = \Theta(n^2),
% \end{align*}
% where the second inequality is due to the fact that 
% \begin{align*}
% \lambda^2\bigg(0.5-O\bigg(\frac{1}{\polylog(n)}\bigg)\ge [1-\lambda + \alpha\lambda]^2\bigg(\lambda-0.5+O\bigg(\frac{1}{\polylog(n)}\bigg)\bigg)
% \end{align*}
% for all $\lambda>0.5$ and the last inequality is due to $\sum_{i\in\cS_0^+}|\cP_i^*(\vb)|\ge |\cS_0^+|=\Theta(n)$ and $b\alpha^2=o\big(1/\polylog(n)\big)$. Besides, it is also easy to get that $a^+(\vb)+a^-(\vb)\le \Theta(n^2)$ since $|\ell_{1,(i,j)}^{(t)}|\le 1$ and $\sum_{i\in\cS_0^+}|\cP_i^*(\vb)|=\Theta(1)$.

% Consequently, according to \eqref{eq:update_allfeatures_mixup} we can get the update of $\vb$ as follows: if $\la\wb_{1,r}^{(t)}, \vb\ra>0$, then
% \begin{align*}
% \la\wb_{1,r}^{(t+1)},\vb\ra &= \la\wb_{1,r}^{(t)},\vb\ra + \frac{\eta}{n^2}\cdot \sum_{i,j\in[n]} \ell_{1, (i,j)}^{(t)}\sum_{p\in[P]}\la\wb_{1,r}^{(t)},\xb_{i,j}^{(p)}\ra\cdot\la\wb_{i,j}^{(p)}, \vb\ra\notag\\
% &=\la\wb_{1,r}^{(t)},\vb\ra \cdot \bigg(1 + \frac{\eta [a^+(\vb) + a^-(\vb)]}{n^2}\bigg) \pm O\bigg(\frac{\eta}{n^2}\sum_{i,j\in[n]}\sum_{p\in\cP_{i,j}(\vb)}b_{i,j}^{(p)}|\la\wb_{1,r}^{(t)}, \bb_{i,j}^{(p)}\ra|\bigg)\notag\\
% & = \la\wb_{1,r}^{(t)},\vb\ra \cdot \big[1+\Theta(\eta)\big] \pm O\bigg(\frac{\eta}{n^2}\sum_{i,j\in[n]}\sum_{p\in\cP_{i,j}(\vb)}b_{i,j}^{(p)}|\la\wb_{1,r}^{(t)}, \bb_{i,j}^{(p)}\ra|\bigg),
% \end{align*}
% where $b_{i,j}^{(p)}\in[0, 1]$ and $b_{i,j}^{(p)}=0$ if $\bb_{i,j}^{(p)}\not\perp \vb$.
