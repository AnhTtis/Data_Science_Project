{
    "arxiv_id": "2303.17619",
    "paper_title": "Gaze-based Attention Recognition for Human-Robot Collaboration",
    "authors": [
        "Pooja Prajod",
        "Matteo Lavit Nicora",
        "Matteo Malosio",
        "Elisabeth Andr√©"
    ],
    "submission_date": "2023-03-30",
    "revised_dates": [
        "2023-04-03"
    ],
    "latest_version": 1,
    "categories": [
        "cs.HC",
        "cs.AI",
        "cs.CV",
        "cs.RO"
    ],
    "abstract": "Attention (and distraction) recognition is a key factor in improving human-robot collaboration. We present an assembly scenario where a human operator and a cobot collaborate equally to piece together a gearbox. The setup provides multiple opportunities for the cobot to adapt its behavior depending on the operator's attention, which can improve the collaboration experience and reduce psychological strain. As a first step, we recognize the areas in the workspace that the human operator is paying attention to, and consequently, detect when the operator is distracted. We propose a novel deep-learning approach to develop an attention recognition model. First, we train a convolutional neural network to estimate the gaze direction using a publicly available image dataset. Then, we use transfer learning with a small dataset to map the gaze direction onto pre-defined areas of interest. Models trained using this approach performed very well in leave-one-subject-out evaluation on the small dataset. We performed an additional validation of our models using the video snippets collected from participants working as an operator in the presented assembly scenario. Although the recall for the Distracted class was lower in this case, the models performed well in recognizing the areas the operator paid attention to. To the best of our knowledge, this is the first work that validated an attention recognition model using data from a setting that mimics industrial human-robot collaboration. Our findings highlight the need for validation of attention recognition solutions in such full-fledged, non-guided scenarios.",
    "pdf_urls": [
        "http://arxiv.org/pdf/2303.17619v1"
    ],
    "publication_venue": "Accepted to PETRA 2023"
}