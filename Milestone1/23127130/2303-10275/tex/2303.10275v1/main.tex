\documentclass[10pt,twocolumn,letterpaper,table]{article}

\usepackage{iccv}

\makeatletter
\@namedef{ver@everyshi.sty}{}
\makeatother

\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}

% Include other packages here, before hyperref.
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{mathtools}
\usepackage{booktabs}
\usepackage{placeins}
\usepackage{stmaryrd} % for arrow symbols
\usepackage[table]{xcolor} % for colored text
\usepackage{subfig}          % for (a) (b) clickable references on main scheme figure
\usepackage{verbatim}        % for monospace font
%\usepackage{cprotect}        % for monospace in captions
\usepackage{float}           % for controlling figures placement
\usepackage{amssymb}         % for more math characters
%\setlength{\abovedisplayskip}{0pt}
%\setlength{\abovedisplayshortskip}{0pt}
\usepackage{cellspace, makecell} % for dnr-vs-warp figure 
\usepackage{graphicx} % for teaser
\usepackage{tikz} % for teaser

\usetikzlibrary{positioning,shapes.misc,fit} % for teaser

%\usepackage[draft]{pdfpages} % for disabling images -> faster compilation
%\setkeys{Gin}{draft}         % comment out to enable figures

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}

\iccvfinalcopy % *** Uncomment this line for the final submission



\def\iccvPaperID{4354} % *** Enter the ICCV Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
\ificcvfinal\pagestyle{empty}\fi

\begin{document}

%%%%%%%%% TITLE
\newcommand{\titleabbreviation}{\mbox{MoRF}}
\title{\titleabbreviation{}: Mobile Realistic Fullbody Avatars from a Monocular Video}
\author{
{Alexey Larionov\textsuperscript{1}}
\and{Evgeniya Ustinova\textsuperscript{1}} 
\and{Mikhail Sidorenko\textsuperscript{1}}
\and{David Svitov\textsuperscript{1}} 
\and{Ilya Zakharkin\textsuperscript{3}}\thanks{Contributed during employment at Samsung AI Center, Moscow.}
\and{Victor Lempitsky\textsuperscript{2}}
\and{Renat Bashirov\textsuperscript{1}}
\and
\and\textsuperscript{1}Samsung AI Center, Moscow 
\hspace{0.3cm}\textsuperscript{2}Cinemersive Labs
\hspace{0.3cm}\textsuperscript{3}zero10.app
}

\maketitle
% Remove page # from the first page of camera-ready.
\ificcvfinal\thispagestyle{empty}\fi


%%%%%%%%% ABSTRACT
\begin{abstract}
   We present a new approach for learning Mobile Realistic Fullbody (\titleabbreviation{}) avatars. \titleabbreviation{} avatars can be rendered in real-time on mobile phones, have high realism, and can be learned from monocular videos. As in previous works, we use a combination of neural textures and the mesh-based body geometry modeling \mbox{SMPL-X}. We improve on prior work, by learning per-frame warping fields in the neural texture space, allowing to better align the training signal between different frames. We also apply existing \mbox{SMPL-X} fitting procedure refinements for videos to improve overall avatar quality. In the comparisons to other monocular video-based avatar systems, \titleabbreviation{} avatars achieve higher image sharpness and temporal consistency. Participants of our user study also preferred avatars generated by \titleabbreviation{}.
\end{abstract}

%%%%%%%%% BODY TEXT
\section{Introduction}
\label{sec:intro}

This work focuses on fullbody human avatars with fast rendering, realism and the ease of acquisition. More specifically, we aim to develop a system that can render avatars in real-time on mobile devices, as well as acquire personalized realistic avatars with low processing time, based on short (\ie minute-long) monocular videos. As far as we know, despite impressive progress, existing systems fall short of these criteria. Specifically, some rely on multi-view videos \cite{mixture-volum-primitives:2021, dracon:2022}, or a depth sensor \cite{robust-3d-portraits-in-seconds-rgbd:2020, metaavatar-rgbd:2021}, some leverage implicit representations whose rendering process is far from realtime especially on mobile devices \cite{phorhum,neural-body:2021, nerf:2020, animnerf:2021, neural-actor, eva3d:2022}, whilst others take days to converge on multiple GPUs \cite{dance-in-the-wild:2021,dynamic-humans:2022}.

{%\setlength{\abovecaptionskip}{0pt plus 2pt minus 2pt}
\setlength{\belowcaptionskip}{0pt plus 2pt minus 2pt}
\begin{figure}
\centering
    \setlength{\tabcolsep}{0pt}
    \setlength{\arrayrulewidth}{0pt}
    \newcommand{\mysmartphonefakeheight}{6cm}
    \begin{tikzpicture}[every node/.style={inner sep=0,outer sep=0}]
        \clip [rounded corners=0.3cm] (0,0) rectangle coordinate (centerpoint) ++(\columnwidth-0.15cm,\mysmartphonefakeheight);
        \node(current bounding box.south west) [anchor=south west] (bb) at (0,0) {\scalebox{-1}[1]{\includegraphics[height=\mysmartphonefakeheight,trim={110 40 100 200},clip]{img/smartphone/05_big-min-anon.jpg}}};
        \node [right=1pt of bb] (david) {\includegraphics[height=\mysmartphonefakeheight,trim={150 40 20 400},clip]{img/smartphone/11_david-min.jpg}};
        \node [right=1pt of david] (minsoo) {\includegraphics[height=\mysmartphonefakeheight,trim={20 00 0 220},clip]{img/smartphone/minsoo-min-anon.jpg}};
    \end{tikzpicture}
\caption{Three MoRF avatars (and a real person in gray) rendered on a mobile phone in realtime (at least 30 frames per second) in the augmented reality mode. Each of the avatars was created from a monocular video, while the poses and the camera positions of these examples differ from the training frames.} 
\label{fig:smartphone}
\end{figure}
}

We therefore present a system for \textbf{Mo}bile \textbf{R}ealistic \textbf{F}ullbody (\titleabbreviation{}) avatars. Our avatars are realistic and exhibit temporal consistency of generated images, and they can be rendered in realtime at 30 FPS on mobile devices (Fig.~\ref{fig:smartphone}) with Qualcomm Snapdragon 888 SoC (2020's flagship SoC for mobile phones). Furthermore, they can be acquired from monocular videos in a few hours on a single NVIDIA RTX 3090 GPU. We rely on the deferred neural rendering (DNR) idea \cite{dnr:2019}, like in numerous prior avatar systems~\cite{anr:2021,dynamic-garments:2021,dynamic-humans:2022,dressing-avatars:2022}, to achieve rendering speed and realism. DNR represents 3D objects using coarse 3D geometry and \textit{neural textures}, which are supplemented by a rendering image-to-image translation network to produce realistic images. For geometric modeling, we use the well-established SMPL-X body model~\cite{SMPL-X:2019}.

{\setlength{\abovecaptionskip}{0pt plus 2pt minus 2pt}
\setlength{\belowcaptionskip}{0pt plus 2pt minus 2pt}
\begin{figure*}[t]
    \centering
    \includegraphics[width=1.0\textwidth]{img/video_avatar_scheme_flat_v4_nofeatmatching.pdf}
    \subfloat{\label{fig:arch:a-dataset}}
    \subfloat{\label{fig:arch:b-warp}}
    \subfloat{\label{fig:arch:c-ntex}}
    \subfloat{\label{fig:arch:d-renderer}}
    \subfloat{\label{fig:arch:e-discriminator}}
    \caption{The proposed architecture. Except for gray blocks, the colored blocks highlight end-to-end optimized parts. \protect\subref{fig:arch:a-dataset}~The frames, pose vectors, and segmentation masks (not shown) are all inputs to the modeling process. In addition, for each frame, a latent texture warping vector is introduced. \protect\subref{fig:arch:b-warp}~A texture warping field is computed for each frame using a convolutional architecture and the respective warping vector.  \protect\subref{fig:arch:c-ntex}~A neural texture associated with the avatar is warped according to the predicted field and is superimposed over a posed SMPL-X mesh. \protect\subref{fig:arch:d-renderer}~The rendering network predicts an RGB image as well as a segmentation mask. This network inpaints out-of-mesh details, \eg garments and hair. \protect\subref{fig:arch:e-discriminator}~We supervise with pixelwise, perceptual, discrimiator losses on predicted images; Dice loss on predicted segmentations.}  
    \label{fig:arch}
\end{figure*}
}

The requirement for real-time performance constrains the size of the rendering network that we can apply. With the size we can afford, we discovered that naively implementing the DNR approach results in underfitting (as the training process fails to reconstruct a person realistically) and overfitting (manifesting itself as temporal flicker during rendering). To address both of these issues, we introduce a new component into the DNR learning process that we call the \textit{neural texture warping}. For each training image/frame, it fits a warping field for the neural texture, thus facilitating the fitting process and compensating for the imperfections and inconsistencies of the geometry and sliding clothing. 

The warping fields are discarded after the training process has converged, so that the new component has no effect on the DNR approach's fast rendering time. While the warping fields do not generalize to new poses during inference due to poor variety of body poses during training, we show that the addition of texture warping alleviates both underfitting and overfitting significantly, especially in the presence of sliding clothing. 

We evaluate our approach over a set of benchmarks through quantitative measurements and a user study. We gather a dataset of training and evaluation sequences with a controlled scenario of body movements to which we apply a refined mesh-fitting procedure~\cite{SMPLify:2016, SMPL-X:2019}. We report the contributions of individual components and compare to recent closely related avatar systems~\cite{anr:2021,stylepeople}, achieving superior results in terms of rendering quality.

\section{Related Work}\label{sec:related}

Modeling geometry of dynamic non-rigid scenes is considered in a number of recent approaches either for capturing human actors \cite{driving-signal-aware, neuralgif:2021, avatarcap:2022, tava:2022,neural-actor, power-of-points-humans-clothing:2021, habermann2021real, arch, arch++, humannerf:2022} or general scenes \cite{neural-volumes:2019, pumarola2020d, nerfies:2021}. Towards this end,  3D scans are required for supervision in \cite{arch, arch++, power-of-points-humans-clothing:2021, neuralgif:2021, avatarcap:2022}  to learn rigged human geometry. Likewise, \cite{neural-volumes:2019, neural-actor, habermann2021real, driving-signal-aware,  drivable-volumetric-avatars:2022, tava:2022} utilize multi-view data to capture appearance and produce photo-realistic avatars under arbitrary viewpoints and in arbitrary poses. Several methods \cite{humannerf:2022, pumarola2020d, nerfies:2021} use monocular videos but allow free-viewpoint rendering only. 

 Some of the monocular fullbody avatar methods model the human geometry implicitly \cite{neural-actor, animnerf:2021, humannerf:2022}, others  output the classical mesh+static texture format \cite{video-avatar, octopus, alldieck2018detailed, hf-avatar, stylepeople, anr:2021}. Similarly to StylePeople~\cite{stylepeople} and ANR~\cite{anr:2021}, our method is based on DNR~\cite{dnr:2019} allowing for efficient convolutional rendering on a mobile device. However, StylePeople leaves modeling of geometry imperfections and frame-to-frame misalignments to the neural renderer, while ANR handles these issues by limiting the number of frames used for the neural texture optimization. Another neural rendering work \cite{hf-avatar} uses the synthetic images as supervision for the neural texture, because of their consistency and good alignment with the geometry.
 %In addition to 3D vertex displacements for geometry refinement, 
 \cite{alldieck2018detailed} proposes a texture merging optimization procedure to prevent texture averaging among different views. 
 Our neural texture warping allows for consistent learning of the neural texture from different poses and views without training on synthetic data \cite{hf-avatar}, excluding most frames from texture learning~\cite{anr:2021}, or over-fitting the neural renderer to unmodeled variations~\cite{stylepeople}. % warpings space 
 
 Many of the mentioned works introduce learned sub-modules \cite{neural-volumes:2019, habermann2021real, neural-actor, neuralgif:2021, avatarcap:2022, tava:2022, hf-avatar} or pipeline stages \cite{alldieck2018detailed, video-avatar} to estimate additional geometry warping in 3D canonical (unposed or view-agnostic) space. Likewise, \cite{neuralgif:2021, avatarcap:2022, tava:2022, neural-actor} parametrize the 3D warping fields or displacements \cite{power-of-points-humans-clothing:2021, hf-avatar} as neural networks conditioned on human pose to facilitate learning of complex pose-dependent geometry. Methods for general scenes leverage other variants of conditioning. For example\ \cite{nerfies:2021, pumarola2020d}, that rely on monocular videos, condition 3D warping fields on the point in time and learnable latent of a training frame respectively, while  \cite{neural-volumes:2019} introduce view-conditional warpings as they use multi-view capture.  
 The way we introduce warping in this work is different to all above-referenced approaches, as our learnable warping operates in the canonical 2D texture space. Operating in the texture space naturally fits the DNR approach that we are based upon. 

\section{Method}\label{sec:method}

Our approach aims to learn a rigged full-body human avatar using a relatively short (e.g.~a minute-long) monocular video. We begin by briefly describing deffered neural rendering framework, as it is at the core of our system. We then introduce the neural texture warping that facilitates the training process. We also mention important architectural decisions that contribute to the avatars quality.

\subsection{Recap: Deferred Neural Rendering}

Deferred neural rendering (DNR) \cite{dnr:2019} models scenes  by combining a geometric proxy defined by a triangular mesh, a neural texture $\bf T$ (with $C$ channels), and an image-to-image rendering network (renderer) $\mathcal{R}$ with a convolutional architecture. Novel views for certain camera parameters are synthesized in two steps. The first is to project the mesh onto the camera view, while superimposing the texture on the mesh using texture mapping. The resulting $C$-channeled image can then be augmented with additional channels, and ``translated'' into an RGB image (plus optionally a mask) by the rendering network. During training/fitting to a given dataset of images with known corresponding camera parameters, the latent texture $\bf T$ and the parameters of the rendering network $\mathcal{R}$ are jointly optimized to minimize losses between training images and images predicted by $\mathcal{R}$.

Better results with less overfitting are obtained, as suggested in the original paper~\cite{dnr:2019}, if the neural texture is (over)-parameterized by the texture pyramid with additional regularization on the high-frequency pyramid levels.

\subsection{Neural Texture Warping}\label{sec:method:warping}

{%\setlength{\abovecaptionskip}{0pt plus 2pt minus 2pt}
\setlength{\belowcaptionskip}{0pt plus 2pt minus 2pt}
\begin{figure}[t]
\centering
    % CAN MAKE IMAGES BIGGER (HEIGHT=6.4cm)
    \subfloat[\centering Image space \label{fig:misalign:rgb}]
    {\includegraphics[height=6.0cm]{img/misalignment-rgb.jpg}}
    \hspace{1em}
    % CAN MAKE IMAGES BIGGER (HEIGHT=6.4cm)
    \subfloat[\centering Texture space \label{fig:misalign:uv}]
    {\includegraphics[height=6.0cm]{img/misalignment-diff-half.jpg}}%
\caption{\protect\subref{fig:misalign:rgb}~Mesh surfaces (represented by red lines) fitted to two different frames are located inconsistently.  Because of the rendering network's overfitting tendency, such misalignment can cause artifacts of the learned avatar. \protect\subref{fig:misalign:uv}~A partial plot of $\Pi_{\bf T}(\mathbf{I}_2) - \Pi_{\bf T}(\mathbf{I}_1)$, where $\Pi_{\bf T}$ is the texture space projection. The red and blue colors correspond to the two poses from \protect\subref{fig:misalign:rgb}, whilst the white color indicates that the colors match. The texture space is clearly misaligned.} 
\label{fig:misalign}
\end{figure}
}

The notable finding of \cite{dnr:2019} is that the quality of deferred neural rendering degrades very gracefully in the presence of coarseness or systematic errors in the geometric proxy. This has allowed several recent works~\cite{anr:2021,dynamic-garments:2021,dynamic-humans:2022,dressing-avatars:2022} to apply DNR to fullbody modeling using \mbox{SMPL-X} as an underlying geometry, despite the big gap between this geometric proxy and the actual avatar content that might include sliding or loose clothing. 

DNR based on \mbox{SMPL-X} geometry essentially ``off-loads'' clothing and hair modeling to the rendering network that operates in the 2D domain. While appealing for its simplicity,  we discovered that such an approach frequently fails to fit the training data, because frames with similar camera parameters and body pose can differ in terms of hair geometry or clothing draping (see Figure~\ref{fig:misalign}), which the DNR approach cannot model well. \mbox{SMPL-X} does not support these latent variations of underlying geometry. We propose modeling them in the texture space with 2D warping maps (see Figure~\ref{fig:arch}). Our cornerstone is that the texture space by construction is a canonical space for the human body -- it is invariant to body poses and camera views. At the same time it covers the whole 3D body surface. As a result, our warping approach is less ambiguous than commonly applied warping in the 3D spatial domain, and arguably is easier to learn from monocular videos. On the other hand, it has to be noticed that the texture space splits the 3D mesh into disconnected 2D parts (\eg head and torso in Figure \ref{fig:misalign:uv}), therefore the warping on borders of those parts may not be smooth enough. 

For each training frame, we learn a frame-specific warping field for the neural texture. The frame specific information is represented by a combination of the frame's \mbox{SMPL-X} pose vector~${\bf p}$ and a latent~$\bf z$. Those are passed through a multi-layered perceptron (MLP)~$\mathcal{W}$ to obtain a style vector~$\bf w$. We predict the warping fields with an encoder-decoder convolutional architecture~$\mathcal{E}\odot{}\mathcal{F}$, the input to which is a constant tensor of positional encodings, namely harmonic functions of the texture coordinates. The style vector~$\bf w$ conditions the encoder part via the Adaptive Instance Normalization blocks~\cite{adain:2017}. A predicted warping field is a $2$-channel tensor, that specifies per-texel offsets of texture coordinates, in a differentiable manner. See the supplementary materials for the exact details of the architectures $\mathcal{E}$, $\mathcal{F}$, $\mathcal{W}$. Figure~\ref{fig:warp-effect} shows the effect of the learned warping fields. 

During model fitting, we use gradient descent to fit per-frame latent vectors $\bf z$, and parameters of the warping networks $\mathcal{E}$, $\mathcal{F}$, $\mathcal{W}$, the neural texture $\bf T$ and the rendering network $\mathcal{R}$, which are shared for all training frames. The latent vectors encode the variations in avatar geometry that were not captured by the underlying \mbox{SMPL\nobreakdash-X} model, and the frame-specific imperfections of \mbox{SMPL\nobreakdash-X} fitting (Fig. \ref{fig:warp-effect}).

For rendering in novel poses, we compute the warping field corresponding to the neutral A-pose ${\bf p}$ and the average latent vector $\bf z$. Such warping field is applied to the neural texture once, eliminating the need for warping computations during inference, thus preserving high rendering speed. We experimented with the generalization ability of the learned warping on pose vectors ${\bf p}$. As we build avatars from short videos, we have found  that the generalization is poor, and conditioning on the pose during rendering results in tearing, that may not be plausible for the combinations of viewpoints and camera views not present in the input video. Overall, pose vectors ${\bf p}$ act as a weak prior on the body configuration, whilst the strongest warping conditioning comes from latents $\bf z$.  We speculate that much better results can be achieved by either learning avatars from much longer videos or by sharing some of the model parameters across multiple avatars. For the remainder of this work, we stick to fixing the warping fields once an avatar is learned.

{%\setlength{\abovecaptionskip}{2pt plus 2pt minus 2pt}
\setlength{\belowcaptionskip}{0pt plus 2pt minus 2pt}
\renewcommand*{\arraystretch}{4}
\begin{figure}[t]
    \centering
    \setlength{\abovecaptionskip}{2pt}
    \subfloat[\centering\label{fig:warp-effect:gt}]
    {\includegraphics[width=0.32\columnwidth]{img/warp-effect/mesh_effect_short_gt.jpg}}%
    \subfloat[\centering\label{fig:warp-effect:warp-on}]
    {\includegraphics[width=0.32\columnwidth]{img/warp-effect/mesh_effect_short_warp.jpg}}%
    \subfloat[\centering\label{fig:warp-effect:warp-off}]
    {\includegraphics[width=0.32\columnwidth]{img/warp-effect/mesh_effect_short_nowarp.jpg}}%
\caption{\protect\subref{fig:warp-effect:gt} The \mbox{SMPL-X} (shown by red lines) superimposed on a training frame. \protect\subref{fig:warp-effect:warp-on} The \textbf{avatar} image, that fits and aligns the training image very well. \protect\subref{fig:warp-effect:warp-off} The same as \protect\subref{fig:warp-effect:warp-on}, but the fitted texture warping field of this frame was replaced with a zero warping field. Note, that the clothes appear in a \textit{neutral} position: the centralized shirt, the undeformed hemline. As seen, the warping field successfully ``absorbs'' the unique cloth deformation on this training frame, allowing the neural texture and the rendering network to learn an easier task and to not overfit.} 
\label{fig:warp-effect}
\end{figure}
}

\paragraph{Texture initialization}\label{sec:method:texture-initialization}

 DNR-based methods usually initialize the neural textures with random noise. 
 In constrast, we found that better results can be achieved with the \textit{spectral initialization} of the neural texture as in \mbox{StylePeople}~\cite{stylepeople}, in which texel value is initialized to spectral coordinates of the corresponding point on the \mbox{SMPL-X} mesh graph (see examples in supplementary materials). We learn neural textures with small learning rates, to avoid vanishing of the spectral information. We found that the use of spectral initialization was more pronounced in our system compared to \cite{stylepeople}. In particular, this initialization has minor effect on image fidelity, but it significantly slows down overfitting of the rendering network, giving the warping network time to converge and facilitating temporal consistency. It also contributes in plausible inpainting of body parts not covered in the input video (see Figure~\ref{fig:spectral}). 

{%\setlength{\abovecaptionskip}{2pt plus 2pt minus 2pt}
\setlength{\belowcaptionskip}{0pt plus 2pt minus 2pt}
\begin{figure}
\centering
    \renewcommand*{\arraystretch}{4}
    \setlength{\abovecaptionskip}{2pt}
    \newcommand{\myfakeheight}{3.6cm}
    %\newcommand{\myfakeheightcrop}{2cm}
    %\newcommand{\myfakeraise}{2.1cm}
    %\newcommand{\myfakeheightzoom}{1.07cm}
    %\renewcommand\cellset{\renewcommand\arraystretch{0}}%
    \subfloat[\centering\label{fig:spectral:05-rand}]
    {\includegraphics[trim=275 225 350 100,clip,height=\myfakeheight]{img/spectral/05_rand_000449_arrow_gs.jpg}}
    \subfloat[\centering\label{fig:spectral:05-spectral}]
    {\includegraphics[trim=192 157 244 70,clip,height=\myfakeheight]{img/spectral/05_spectral_000449.jpg}}%
    %     \subfloat[\centering\label{fig:spectral:04-rand}]
    % {\makecell{\includegraphics[height=\myfakeheight,trim={75 40 140 90},clip]{img/spectral/04_rand_001227.jpg}\\}}%
    % \subfloat[\centering\label{fig:spectral:04-spectral}]
    % {\makecell{\includegraphics[height=\myfakeheight,trim={75 40 140 90},clip]{img/spectral/04_spectral_001227.jpg}\\}}%
    \hspace{1em}
    \subfloat[\centering\label{fig:spectral:08-rand}]
    {\includegraphics[height=\myfakeheight,trim={270 40 285 80},clip]{img/spectral/08_rand_000272_arrow_gs.jpg}}%
    \subfloat[\centering\label{fig:spectral:08-spectral}]
    {\includegraphics[height=\myfakeheight,trim={200 30 211 60},clip]{img/spectral/08_spectral_000272.jpg}}%
\caption{Results of \titleabbreviation{} on novel poses, for two neural texture initialization methods: \protect\subref{fig:spectral:05-rand}/\protect\subref{fig:spectral:08-rand} - random initialization,  \protect\subref{fig:spectral:05-spectral}/\protect\subref{fig:spectral:08-spectral} - spectral initialization. Some body parts may exhibit overfitting artifacts. Local smoothness of spectral initialization acts as a prior to solve that.}
\label{fig:spectral}
\end{figure}
}

\section{Implementation details}

\subsection{Mesh fitting}\label{sec:details:meshfitting}

For both our system and the standard DNR model to perform well, precise and time-consistent \mbox{SMPL-X} mesh fitting to the training video frames (and at test time) is important. As for RGB photos, the convolutional regression-based techniques~\cite{vibe,hmr,expose,pixie,romp,bev,spin} produce somewhat accurate but inconsistent results that frequently mismatch the silhouettes. On the contrary, results of iterative non-linear optimization ~\cite{SMPLify:2016} better correspond 2D RGB images but are less plausible or unstable in 3D, since the fitting process relies on 2D image keypoints, which are poorly constrained along the depth dimension. Regression-based and optimization-based methods are unable to handle complex cases such as poses with self contact. Below we describe existing enhancements to \mbox{SMPL-X} optimization-based fitting that increase the stability of DNR-based avatar systems such as ours. We later validate the individual impact of these adjustments through ablation study. 


\paragraph{Shared shape}
%The simplest way to improve temporal consistency and fitting quality is to notice 
A person's identity does not change between frames of a training video, thus the shape-specific parameters of \mbox{SMPL-X} model can be shared. Since those shape parameters are also suitable for inference, the overhead of \mbox{SMPL-X} inference on mobile can be reduced.
% To find more precise person's body shape we make it shared across all fitting frames, making the estimated shape to have more much data for supervision. To do this we add batch dimension when fitting. We use the single estimated shape during inference.

\paragraph{Silhouette loss}
We perform differentiable rendering of \mbox{SMPL-X} meshes via \mbox{\texttt{nvdiffrast}}~\cite{nvdiffrast}.
We then apply \mbox{Dice}~\cite{dice} loss to pull rendered \mbox{SMPL-X} silhouettes towards estimations of a semantic segmentation network. In the presence of loose clothing, the inclusion of this term tends to ``fatten'' the human shape. It can be detrimental, if the goal is to estimate shape parameters for a naked person. However, we argue that such fattening is advantageous for DNR, as the more the mesh surface covers the person's content, the easier the job for the neural rendering network.

\paragraph{Temporal loss}
The non-linear optimization-based fitting method tends to flicker on side views when the person's farther side is not visible. To compensate for this effect, we compute the $\text{L2}$ distance between adjacent body pose vectors (in axis-angle format). This term is efficient for our scenario of training videos with slow body movements. Applying it to ``in-the-wild'' videos with fast motions may be ineffective or even harmful. 

\paragraph{Automatic frame filtering}\label{sec:details:meshfitting:autofiltering}
The mesh fitting procedure is vulnerable to quick movements and depth ambiguity even with temporal guidance. Some training frames may be estimated with significantly incorrect poses. This causes a conflict between the 3D mesh surface and the neural texture, resulting in the appearance of ``ghost'' limbs and flickering. We use a simple heuristic that we describe in the supplementary materials to automatically filter out bad frames (typically, less than 5\% of frames are discarded). 

\subsection{Architectural details} 
\paragraph{Training} As a rendering network, we use U-Net~\cite{unet:2015} (a total of $\approx$14.4M parameters). Its decoder is based on bi-linear upsampling blocks. We provide the definitions of neural layers in supplementary materials. We use ADAM~\cite{adam:2014} to optimize the architecture from scratch for a target person. On a single RTX 3090 GPU, convergence to plateau takes 8 hours. 
Our preliminary results suggest that faster acquisition is possible, if the architecture is pretrained on a dataset of people, and then fine-tuned to a target person. Below, however, we report on experiments that fit the model to a target video from scratch. 

\paragraph{Ad-hoc inpainting of unseen body parts} Our minute-long training videos rarely include all body parts of the target person. Often top of the head, soles, and armpits have no ground truth. Those parts can be blurred on avatar renders, or worse -- excluded from the predicted segmentation. We overcome this by post-train finetuning for areas of the neural texture that correspond to unseen body parts. We first generate a pseudo ground-truth by projecting training frames into an RGB texture using \mbox{SMPL-X} fits, then we fill gaps on the RGB texture with nearby color, and render new ground-truth with extreme body and camera poses that show unseen body parts. We then fine-tune with the same learning objective as per Section \ref{sec:method:loss-functions}.   

\label{sec:details:priors}\paragraph{Inputs} We use \mbox{OpenPose}~\cite{OpenPose} to detect 2D keypoints, which is required for mesh fitting. We retrain \mbox{Graphonomy}~\cite{graphonomy:2019} segmentation network on \mbox{CCIHP} dataset~\cite{ccihp_dataset_2021}, which includes accessories like watches, belts, and glasses, etc. Meanwhile the publicly available \mbox{Graphonomy} trained on \mbox{CIHP} dataset predicts those as background.


\subsection{Loss functions}\label{sec:method:loss-functions}
For \titleabbreviation{} training, we use popular loss functions from previous works~\cite{dnr:2019,stylepeople,pix2pix} with modifications. We further elaborate on our learning objective:
{\setlength{\abovedisplayskip}{0.5em}
\setlength{\abovedisplayshortskip}{0.5em}
\setlength{\belowdisplayshortskip}{0.5em}
\setlength{\belowdisplayskip}{0.5em}
\label{eq:total_loss}
\begin{equation}
\begin{gathered}
%\mathcal{L}_{\text{total}} = \mathcal{L}_{\text{photo}} + \mathcal{L}_{\text{adv}} + \mathcal{L}_{\text{Dice}} + \mathcal{L}_{\text{prior}}
\mathcal{L}_{\text{total}} = \sum_{i \in \mathfrak{L}} \lambda_i \mathcal{L}_i \\
\mathfrak{L} = \{\text{L1}, \text{Percept}, \text{GAN}, \text{Dice},\text{TV}_\text{RGB}, \text{TV}_\Delta, \Delta\shortrightarrow0 \}
\end{gathered}
\end{equation}
}

\paragraph{Image losses} These include $\text{L1}$ per-pixel loss $\mathcal{L}_{\text{L1}}$ and the VGG \cite{vgg} based perceptual loss~\cite{percept} $\mathcal{L}_{\text{Percept}}$. To enforce the realism of the rendered images, we utilized the non-saturating adversarial loss~\cite{nsgan} $\mathcal{L}_{\text{GAN}}$ on predictions of multi-scale patch-based discriminators (\mbox{PatchGAN} from \mbox{Isola \etal}~\cite{patch-gan}). 

\paragraph{Segmentation loss} In our case, the estimated body mesh (\mbox{SMPL-X}) is coarse and frequently misaligned with the underlying human content. To assist the renderer in generating content outside of the rasterized mesh, the renderer predicts the human mask in addition to the RGB image. We supervise the masks on the pseudo ground truth masks (see paragraph \ref{sec:details:priors}), using Dice~\cite{dice} $\mathcal{L}_{\text{Dice}}$ segmentation loss.

\paragraph{Regularization} For the warping fields, we penalize large $\text{L2}$ norm ($\mathcal{L}_{\Delta\shortrightarrow0}$) to prevent large deviations from identity warping, and also apply total-variation (TV) loss~\cite{tv-functional:2004} $\mathcal{L}_{\text{TV}_\Delta}$ for local smoothness. We use a low weighted TV loss on generated images $\mathcal{L}_{\text{TV}_{\text{RGB}}}$ to slightly improve smoothness.


\subsection{Avatar rendering on a mobile device}
We developed a telepresence mobile app for Qualcomm Snapdragon SoC using \mbox{Android}, \mbox{ARCore}~\cite{arcore}, and \mbox{OpenGL}. Our method can run natively at resolution of $640\times640$ pixels and above 30 frames per second (measured on Qualcomm Snapdragon 888, see Fig.~\ref{fig:smartphone}). We use \mbox{OpenGL} shaders to generate input neural rasterizations and to infer posed \mbox{SMPL-X} meshes from body joint rotations. We stumbled upon \mbox{OpenGL} limit on the depth of the neural texture channels. We were able to pack at most 16 channels of neural rasterizations into \mbox{OpenGL} framebuffers, using quantization \cite{quantization-whitepaper:2021} into eight-bit integers. Similarly, we use post-training quantization on the weights of the rendering network. This enables the network to run at high speeds on the Qualcomm Digital Signal Processor. Quality degradation due to quantization is relatively low in our system. Because single-person avatars have low variance in appearance, the numerical range of neural network activations is small, which is advantageous for quantization.


{%\setlength{\abovecaptionskip}{0pt plus 2pt minus 2pt}
\setlength{\belowcaptionskip}{0pt plus 2pt minus 2pt}
\begin{figure*}[t]
\centering
    \newcommand{\myfakeheight}{4.8cm}
    \newcommand{\myfakeraise}{2.3cm}
    \newcommand{\myfakeheightzoom}{0.97cm}
    %\renewcommand\cellset{\renewcommand\arraystretch{0}%
    \newcolumntype{P}[1]{>{\centering\arraybackslash}p{#1}}
    \setlength\extrarowheight{0pt}
    \begin{tabular}{@{}P{1cm}P{1.0cm}P{1.6cm}P{1.2cm}P{1.2cm}P{0.1cm}P{0.1cm}l@{}} % 
    % DED
    %\hline
    \setlength{\tabcolsep}{0pt} % General space between cols (6pt standard)
    \setlength{\arrayrulewidth}{0pt}
    \includegraphics[height=\myfakeheight,trim={345 165 370 85},clip]{img/methods-compare/hfavatar-512/403_seq_02_hfavatar-512.jpg}
    & \includegraphics[height=\myfakeheight,trim={150 20 190 20},clip]{img/methods-compare/animnerf-512/403_seq_02_animnerf-512.jpg}
    %& \includegraphics[height=\myfakeheight,trim={175 20 190 20},clip]{img/methods-compare/warp-512/403_seq_02.jpg}
    % <GT>
    & \includegraphics[height=\myfakeheight,trim={215 20 200 20},clip]{img/methods-compare/gt/02.jpg}
    % </GT>
    & \includegraphics[height=\myfakeheight,trim={200 20 230 20},clip]{img/methods-compare/warp/403_seq_02_warp.jpg}
    & \includegraphics[height=\myfakeheight,trim={200 20 230 20},clip]{img/methods-compare/anr-r-anr/403_seq_02_anr-r-anr.jpg}
    % & \colorbox{red}{!}\includegraphics[height=\myfakeheight,trim={200 20 230 20},clip]{img/methods-compare/anr-r-our/403_seq_02.jpg} 
    &  \includegraphics[height=\myfakeheight,trim={210 20 250 20},clip]{img/methods-compare/dnr/403_seq_02_dnr.jpg}
    & \includegraphics[height=\myfakeheight,trim={165 25 160 25},clip]{img/methods-compare/human_nerf_640/403_seq_02.jpg}
    & \raisebox{\myfakeraise}{\makecell[l]{
    \raisebox{-.5\height}{\includegraphics[height=\myfakeheightzoom,trim={230 420 230 150},clip]{img/methods-compare/gt/02.jpg}}\ Target\\
    \raisebox{-.5\height}{\includegraphics[height=\myfakeheightzoom,trim={230 420 230 150},clip]{img/methods-compare/warp/403_seq_02_warp.jpg}}\ \underline{\textbf{\titleabbreviation{}}}\\
    \raisebox{-.5\height}{\includegraphics[height=\myfakeheightzoom,trim={230 420 230 150},clip]{img/methods-compare/anr-r-anr/403_seq_02_anr-r-anr.jpg}}\ ANR\\
    \raisebox{-.5\height}{\includegraphics[height=\myfakeheightzoom,trim={230 420 230 150},clip]{img/methods-compare/dnr/403_seq_02_dnr.jpg}}\ \makecell[l]{Style\\People}\\
    \raisebox{-.5\height}{\includegraphics[height=\myfakeheightzoom]{img/methods-compare/human_nerf_640/zoom_403_seq_02}}\ \makecell[l]{Human\\NeRF}}}
    \\ 
    % EVGENIYA
    %\hline
    \includegraphics[height=\myfakeheight,trim={345 115 370 85},clip]{img/methods-compare/hfavatar-512/403_seq_09_hfavatar-512.jpg}
    & \includegraphics[height=\myfakeheight,trim={150 0 190 10},clip]{img/methods-compare/animnerf-512/403_seq_09_animnerf-512.jpg}
    %& \includegraphics[height=\myfakeheight,trim={180 20 190 20},clip]{img/methods-compare/warp-512/403_seq_09.jpg}
    % <GT>
    & \includegraphics[height=\myfakeheight,trim={240 40 230 40},clip]{img/methods-compare/gt/09.jpg}
    % </GT>
    & \includegraphics[height=\myfakeheight,trim={200 20 230 20},clip]{img/methods-compare/warp/403_seq_09_warp.jpg}
    & \includegraphics[height=\myfakeheight,trim={200 20 230 20},clip]{img/methods-compare/anr-r-anr/403_seq_09_anr-r-anr.jpg}
    %& \colorbox{red}{!}\includegraphics[height=\myfakeheight,trim={200 20 230 20},clip]{img/methods-compare/anr-r-our/403_seq_09.jpg} 
    & \includegraphics[height=\myfakeheight,trim={210 20 250 20},clip]{img/methods-compare/dnr/403_seq_09_dnr.jpg}
    & \includegraphics[height=\myfakeheight,trim={165 25 170 25},clip]{img/methods-compare/human_nerf_640/403_seq_09.jpg}
    & \raisebox{\myfakeraise}{\makecell[l]{
    \raisebox{-.5\height}{\includegraphics[height=\myfakeheightzoom,trim={230 310 230 260},clip]{img/methods-compare/gt/09.jpg}}\ Target\\
    \raisebox{-.5\height}{\includegraphics[height=\myfakeheightzoom,trim={230 320 230 250},clip]{img/methods-compare/warp/403_seq_09_warp.jpg}}\ \underline{\textbf{\titleabbreviation{}}}\\
    \raisebox{-.5\height}{\includegraphics[height=\myfakeheightzoom,trim={230 320 230 250},clip]{img/methods-compare/anr-r-anr/403_seq_09_anr-r-anr.jpg}}\ ANR\\
    \raisebox{-.5\height}{\includegraphics[height=\myfakeheightzoom,trim={230 320 230 250},clip]{img/methods-compare/dnr/403_seq_09_dnr.jpg}}\ \makecell[l]{Style\\People}\\
    \raisebox{-.5\height}{\includegraphics[height=\myfakeheightzoom]{img/methods-compare/human_nerf_640/zoom_403_seq_09}}\ \makecell[l]{Human\\NeRF}}}
    \\ 
    % KACHOK
    %\hline
    \includegraphics[height=\myfakeheight,trim={350 140 380 80},clip]{img/methods-compare/hfavatar-512/403_seq_10_hfavatar-512.jpg}
    & \includegraphics[height=\myfakeheight,trim={150 0 150 10},clip]{img/methods-compare/animnerf-512/403_seq_10_animnerf-512.jpg}
    %& \includegraphics[height=\myfakeheight,trim={170 20 190 20},clip]{img/methods-compare/warp-512/403_seq_10.jpg}
    % <GT>
    & {\centering\includegraphics[height=\myfakeheight,trim={220 40 180 40},clip]{img/methods-compare/gt/10.jpg}}
    % </GT>
    & \includegraphics[height=\myfakeheight,trim={200 20 230 20},clip]{img/methods-compare/warp/403_seq_10_warp.jpg}
    & \includegraphics[height=\myfakeheight,trim={200 20 230 20},clip]{img/methods-compare/anr-r-anr/403_seq_10_anr-r-anr.jpg}
    %& \colorbox{red}{!}\includegraphics[height=\myfakeheight,trim={200 20 230 20},clip]{img/methods-compare/anr-r-our/403_seq_10.jpg}
    & \includegraphics[height=\myfakeheight,trim={210 20 230 20},clip]{img/methods-compare/dnr/403_seq_10_dnr.jpg}
    & \includegraphics[height=\myfakeheight,trim={170 25 165 25},clip]{img/methods-compare/human_nerf_640/403_seq_10.jpg}
    & \raisebox{\myfakeraise}{\makecell[l]{
    \raisebox{-.5\height}{\includegraphics[height=\myfakeheightzoom,trim={250 420 210 150},clip]{img/methods-compare/gt/10.jpg}}\ Target\\
    \raisebox{-.5\height}{\includegraphics[height=\myfakeheightzoom,trim={230 420 230 150},clip]{img/methods-compare/warp/403_seq_10_warp.jpg}}\ \underline{\textbf{\titleabbreviation{}}}\\
    \raisebox{-.5\height}{\includegraphics[height=\myfakeheightzoom,trim={230 420 230 150},clip]{img/methods-compare/anr-r-anr/403_seq_10_anr-r-anr.jpg}}\ ANR\\
    \raisebox{-.5\height}{\includegraphics[height=\myfakeheightzoom,trim={230 420 230 150},clip]{img/methods-compare/dnr/403_seq_10_dnr.jpg}}\ \makecell[l]{Style\\People}\\
    \raisebox{-.5\height}{\includegraphics[height=\myfakeheightzoom]{img/methods-compare/human_nerf_640/zoom_403_seq_10}}\ \makecell[l]{Human\\NeRF}}}
    \\ 
    \multicolumn{1}{c|}{\small HF-Avatar}
    & \multicolumn{1}{c}{\small AnimNeRF}
    % & \underline{\textbf{\titleabbreviation{}}}
    & \multicolumn{1}{|c|}{\small Target}
    & \multicolumn{1}{c|}{\small \underline{\textbf{\titleabbreviation{}}}}
    & \multicolumn{1}{c|}{\small ANR}
    & \multicolumn{1}{c|}{\small StylePeople}
    & \multicolumn{1}{c|}{\small HumanNeRF}
    & \multicolumn{1}{c}{\small (zoom in)}
    \\
    %\cline{1-3}\cline{5-7}
    \multicolumn{2}{c}{\small $512\times512$ px, on A-poses}
    & \multicolumn{1}{|c|}{\small identity}
    & \multicolumn{5}{c}{\small $640\times640$ px, on full training sequences}
    \\
    %\cline{1-3}\cline{5-7}
    \cline{1-2}\cline{4-8}
    \end{tabular}
\caption{Avatars created with different monocular video-based approaches. \titleabbreviation{} achieves the highest visual quality in this comparison. See text for more details.} 
\label{fig:methods-comparison}
\end{figure*}
}

\section{Experiments}\label{sec:experim}

We compare our method to similar recent approaches and provide an ablation study for our mesh fitting adjustments. We conduct quantitative comparisons as well as user study comparisons. We provide sample side-by-side comparisons of the methods in the supplementary material.

\paragraph{Datasets}We have gathered a dataset of stationary monocular videos of ten people with different demographics, body shapes, and cloth topology. We note that despite improvements in mesh fitting, it is still imperfect, therefore we captured the training videos with a specific scenario that avoids complex poses. In one-minute training videos people move slowly, turn $360^\circ$, and show their hands in front of their torso. The one-minute hold-out videos include mostly novel poses not present in training videos, namely various arm and hip movements. See samples of training and hold-out data in supplementary materials. 

\paragraph{Comparison with state-of-the-art}
{%
%\setlength{\abovecaptionskip}{0pt plus 2pt minus 2pt}
\setlength{\belowcaptionskip}{0pt plus 2pt minus 2pt}
\setlength{\tabcolsep}{4pt}

\newcommand{\myup}[1][black]{\textcolor{#1}{\ensuremath\uparrow}}
\definecolor{olivegreen}{HTML}{00A64F}
\begin{table}[!h]
\renewcommand*{\arraystretch}{0.9}
\caption{Quantitative metrics and the results of the user preference study, in which each method is compared against \titleabbreviation{}. See the text for the discussion of the results.
}
%\small
\label{tab:metrics}
%\rowcolors{4}{gray!10}{white}
\centering
\begin{tabular}{c|ccc||c} 
 \hline
 %{\footnotesize\textbf{\thead{LPIPS↓\\$\cdot10^2$}}}
 & SSIM   & LPIPS        & tLPIPS             & {\small \% of users} \\
 & $\cdot10^2$↑ & $\cdot10^2$↓ & $\cdot10^3$↓   & {\small preferred \textbf{\titleabbreviation{}}} \\
% &  &  &    & {\small preference} \\
 \hline
 \multicolumn{5}{c}{\textit{Scenario A: $512\times512$ px, training only on A-poses}} \\
 \hline
 HF-Avatar & 93.0 & 7.25 & 4.00 & 90.0\% {\myup[olivegreen]} \\ 
 AnimNeRF & 92.7 & 6.74 & 2.96{\iffalse CORRECT VALUE IS 29.85 \fi} & 98.0\% {\myup[olivegreen]}  \\ 
 \textbf{\titleabbreviation{}} & \cellcolor{green!25}{94.5} & \cellcolor{green!25}{3.95} & \cellcolor{green!25}{2.56} & N/A \\
 \hline
 \multicolumn{5}{c}{\textit{Scenario B: $640\times640$ px, full training sequences}} \\
 \hline
 HumanNeRF & 87.4 & 13.00 & 19.3 & 95.0\% {\myup[olivegreen]}\\
 StylePeople & \cellcolor{green!25}{90.0}  & \cellcolor{green!25}{6.13} & 4.77 & 60.5\% {\myup[olivegreen]}\\
 ANR & \cellcolor{green!25}{90.0}  & \cellcolor{yellow!25}{6.14} & \cellcolor{yellow!25}{4.76} & 69.2\% {\myup[olivegreen]} \\
 \textbf{\titleabbreviation{}} & \cellcolor{yellow!25}{89.9}  & 6.23{\iffalse CORRECT VALUE IS 6.293 \fi} & \cellcolor{green!25}{4.66} & N/A\\
% \textbf{\titleabbreviation{}} FT & 89.9  & 6.33 & \textbf{4.65} & 62.0\%\\
 \hline
\end{tabular}
\end{table}
}

\begin{table}[!h]
\renewcommand*{\arraystretch}{0.9}
\caption{Mesh fitting used in training and validation.}
\label{tab:mesh-fitting}
\centering
\rowcolors{1}{gray!10}{white}
\begin{tabular}{c|c|c} 
 \hline
 & Training   & Hold-out \\
\hline
 HF-Avatar & their SMPL & \makecell{our SMPL \& \\ their train body shape} \\
 AnimNeRF & their SMPL & their SMPL \\
 HumanNeRF & \makecell{our SMPL} & \makecell{our SMPL} \\
 StylePeople & our SMPL-X & our SMPL-X\\
 ANR & our SMPL-X &  our SMPL-X\\
 \textbf{\titleabbreviation{}} &  our SMPL-X &  our SMPL-X \\
 \hline
\end{tabular}
\end{table}

We compare our \titleabbreviation{} method against \mbox{Anim-NeRF}~\cite{animnerf:2021}, HF-Avatar~\cite{hf-avatar}, \mbox{HumanNeRF}~\cite{humannerf:2022}, \mbox{ANR}~\cite{anr:2021} and \mbox{StylePeople}~\cite{stylepeople} (their multi-shot variant, combining DNR rendering, \mbox{SMPL-X} mesh body modeling, and a discriminator network). 

We used publicly available implementations of \mbox{Anim-NeRF}, \mbox{HF-Avatar} and \mbox{HumanNeRF}, which are based on SMPL~\cite{SMPL:2015} mesh, rather than SMPL-X~\cite{SMPL-X:2019}. \mbox{Anim-NeRF} and \mbox{HF-Avatar} require videos of people turning $360^\circ$ in an A-pose. To compare with these methods, we exclude the sections of training videos where people show their hands in front of their torso (leaving about half of the frames), this forms \textit{Scenario~A} comparison. We train \mbox{Anim-NeRF} in its natural resolution ${512\times512}$ px. We resize the results for HF-Avatar from ${1024\times1024}$ to ${512\times512}$ px and train \titleabbreviation{} at ${512\times512}$ resolution to match the scenario setup. \mbox{HF-Avatar}'s mesh fitting adequately performed for training sequences, but struggled for hold-out sequences, therefore for them we used our modified mesh fitting. \mbox{HumanNeRF} implementation does not include its own mesh fitting method, thus we use ours instead. See Table~\ref{tab:mesh-fitting} for an overview of mesh fitting methods applied in our experiments.

We compare against \mbox{HumanNeRF}, \mbox{ANR} and \mbox{StylePeople} trained on full uncut videos at $640\times640$~px resolution -- the maximum resolution at which we can run with stable 30 FPS on a mobile device. This forms \textit{Scenario~B} comparison. For \mbox{StylePeople}, we use all the proposed components as per \cite{dnr:2019,stylepeople}, but take our segmentation, mesh fitting, losses, spectral texture initialization and architectures of the rendering and discriminator networks. In the comparison table, \mbox{StylePeople} baseline differs from \titleabbreviation{} only by the absence of the neural texture warping, in order to compare only novelty part of our work. To compare with ANR, we re-implement their split optimization scheme and add it to the \mbox{StylePeople} baseline (see supplementary materials for more details). 

To quantify visual quality, we use three metrics, computed between real and generated images. We employ two image-based metrics: structural similarity (SSIM)~\cite{ssim:2004} and learned perceptual similarity (LPIPS)~\cite{lpips:2018}. While these metrics focus on individual images, we also use the temporal learned perceptual similarity (\mbox{tLPIPS}) introduced in ~\cite{tlpips} to account for temporal stability.

\begingroup
\renewcommand*{\arraystretch}{4}

\begin{figure*}[t]
    \centering
    \setlength{\abovecaptionskip}{0pt plus 2pt minus 2pt}
    \setlength{\belowcaptionskip}{0pt plus 2pt minus 2pt}
    \newcommand\rowincludegraphics[2][]{\raisebox{-0.45\height}{\includegraphics[#1]{#2}}}
    \newcommand{\myfigheight}{1.42cm}
    \newcommand{\myparboxwidth}{0.5cm}
    \newcommand{\myparboxsep}{1.5em}
    \setlength{\tabcolsep}{0pt} % General space between cols (6pt standard)
    \setlength{\arrayrulewidth}{1pt}

    % SlSlSlm
    
    \begin{tabular}{m{1em}Slp{1em}SlSlm{1em}}
    \parbox[c]{\myparboxwidth}{\rotatebox[origin=c]{90}{\titleabbreviation{} \hspace{\myparboxsep} SP}}
    & \makecell{
    \rowincludegraphics[height=\myfigheight]{img/dnr-vs-warp/dnr_shirt.jpg} 
    \\ \rowincludegraphics[height=\myfigheight]{img/dnr-vs-warp/warp_shirt.jpg}} &
    & \makecell{
    \rowincludegraphics[height=\myfigheight]{img/dnr-vs-warp/dnr_shirt_sample62_vis.jpg} \\ \rowincludegraphics[height=\myfigheight]{img/dnr-vs-warp/warp_shirt_sample62_vis.jpg}} 
    & \makecell{
    \rowincludegraphics[height=\myfigheight]{img/dnr-vs-warp/dnr_shirt_sample403_vis.jpg} \\
    \rowincludegraphics[height=\myfigheight]{img/dnr-vs-warp/warp_shirt_sample403_vis.jpg}}
    & \parbox[c]{\myparboxwidth}{\rotatebox[origin=c]{90}{\titleabbreviation{} \hspace{\myparboxsep} SP}} \\\cline{1-4}
    \parbox[c]{\myparboxwidth}{\rotatebox[origin=c]{90}{\titleabbreviation{} \hspace{\myparboxsep} SP}}
    & \makecell{
    \rowincludegraphics[height=\myfigheight]{img/dnr-vs-warp/dnr_hem.jpg}\\
    \rowincludegraphics[height=\myfigheight]{img/dnr-vs-warp/warp_hem.jpg}} &
    & \makecell{
    \rowincludegraphics[height=\myfigheight]{img/dnr-vs-warp/dnr_hem_sample.jpg} \\ \rowincludegraphics[height=\myfigheight]{img/dnr-vs-warp/warp_hem_sample.jpg}}
    & \multicolumn{1}{|c}{\makecell{
    \rowincludegraphics[height=\myfigheight]{img/dnr-vs-warp/dnr_shirt_sample423_vis.jpg}\\
    \rowincludegraphics[height=\myfigheight]{img/dnr-vs-warp/warp_shirt_sample423_vis.jpg}}}
    & \parbox[c]{\myparboxwidth}{\rotatebox[origin=c]{90}{\titleabbreviation{} \hspace{\myparboxsep} SP}} \\ \cline{5-5}
    \end{tabular}
    
    
    \subfloat{\label{fig:dnr-vs-warp:a-tshirt}}
    \subfloat{\label{fig:dnr-vs-warp:b-tshirt}}
    \subfloat{\label{fig:dnr-vs-warp:c-tshirt}}
    \subfloat{\label{fig:dnr-vs-warp:d-hem}}
    \caption{\label{fig:dnr-vs-warp} To illustrate the difference in temporal consistency, we show the space-time $y$-$t$ slices (left). In the case of \mbox{StylePeople} (SP), the T-shirt pattern (top plots) abruptly tears vertically (accentuated with red arrows), resulting in \protect\subref{fig:dnr-vs-warp:a-tshirt} flickering, \protect\subref{fig:dnr-vs-warp:b-tshirt} ``ghost'' patterns, \protect\subref{fig:dnr-vs-warp:c-tshirt} poor clarity, and loss of pattern rendering for rotated human body (see the section labeled \textit{``side''}). These artifacts are absent in our approach's results, which also achieves good temporal stability on the hemline (bottom plots). \protect\subref{fig:dnr-vs-warp:d-hem} Shows an average image over a 20-frame interval (colored in light-blue).}  
    
\end{figure*}
\endgroup

We conduct a \textit{user study} (the right-most column of Table~\ref{tab:metrics}) in which we show pairs of sequences, one of which is the \titleabbreviation{} result (in randomized order). We took novel body motion sequences from \mbox{AMASS}~\cite{amass} dataset. We asked people to indicate their preference, and thus obtain the percentage of responses when \titleabbreviation{} was preferred over each of the other methods. 

The results are shown in Table~\ref{tab:metrics}. In our comparison, \mbox{AnimNeRF} produces severe NeRF artifacts, while \mbox{HF-Avatar} suffers from significant blurring. \mbox{HumanNeRF} similarly exhibits \mbox{NeRF} artifacts in motion, and its training diverged for some training videos, with the reasons unrelated to \titleabbreviation{}'s mesh fitting quality (see supplementary materials for evaluation scores and images). Due to such divergence, \mbox{HumanNeRF} has very high values for LPIPS and tLPIPS metrics. In comparison to \mbox{StylePeople} and ANR, our approach outperforms both of them in temporal stability while slightly underperforming in SSIM and \mbox{LPIPS} metrics. We explain it by noticing, that in the absence of the warping fields for hold-out sequences, the rendered avatar clothing appears in a neutral position (Fig. \ref{fig:warp-effect}), which is almost certainly misaligned with the real dynamic garments from the hold-out ground truth. Thus, the metrics capture mostly pixel-wise discrepancy between garments, which might not match the human perception of pleasant avatars. Qualitatively, we observe at least as good image quality for \titleabbreviation{} as that of \mbox{StylePeople} and ANR. The advantage of our method is validated by the user study on the videos, where \titleabbreviation{} is preferred over other methods. We also provide a qualitative comparison of \titleabbreviation{} with \mbox{StylePeople} in our mobile AR app in supplementary materials.

In Figure~\ref{fig:methods-comparison} we show the qualitative comparisons between the methods. In Figure~\ref{fig:dnr-vs-warp}, we further compare the temporal stability of \titleabbreviation{} avatars with \mbox{StylePeople} avatars using time-space slices. Here we created avatar images with a novel animation sequence and centered them on a single \mbox{SMPL-X} mesh vertex: on a T-shirt pattern, or the hemline. We demonstrate extreme failure cases for the competing method that do not occur with \titleabbreviation{}. As seen, our method provides good vertical stability of rendered parts, confirming that the tasks of mesh misalignment absorption and translation for latent neural renderings into color images are almost completely disentangled. The neural texture and rendering network's capacities are spent on more quality rendering, instead of remembering the training data.

%\colorbox{red}{TODO}.

\paragraph{Ablation study}\label{sec:experim:ablation}

In Table~\ref{tab:ablation}, we perform an ablation study on mesh fitting modifications described in Section \ref{sec:details:meshfitting}. We start with the \mbox{SMPLify-X} baseline~\cite{SMPLify:2016} and add the improvements one-by-one. We retrain the \titleabbreviation{} avatars for the new fits, and perform a user study on pairs of videos: one is the baseline, the other is a modification. Every pair is shown to 25 people; one person judges at most 6 pairs every 6 hours. Although on average any modification is favored by the user study over \mbox{SMPLify-X}, on particular avatar subjects the score can significantly drop, \eg as low as 12\% for the temporal loss. However, when all modifications are used, the lowest preference score among all subjects was 52\%, indicating the consistently positive impact of the proposed fitting improvements on the avatar quality. 

{\setlength{\abovecaptionskip}{3pt plus 2pt minus 2pt}
\setlength{\belowcaptionskip}{0pt plus 2pt minus 2pt}
\begin{table}[!h]
\centering
\small
\setlength{\tabcolsep}{1pt}
\caption{Ablation user study on the mesh fitting procedure. We report a percentage of users preferring avatars trained on modified mesh fits over baseline SMPLify-X~\cite{SMPLify:2016, SMPL-X:2019}.}
\rowcolors{1}{gray!10}{white}
\label{tab:ablation}
\begin{tabular}{c|c|c|c|c|c} 
\hline
 \textbf{{\small SMPLify-X}} &  \textbf{\makecell{\small+Shared\\shape}} & \textbf{\makecell{\small+Frames\\filter}} & \textbf{\makecell{\small+Temporal\\loss}} & \textbf{\makecell{\small+Silhouette\\loss}} & \textbf{\small+All}  \\
 \hline
 N/A & 57.6\% & 58.0\%  & 64.4\%  & \cellcolor{yellow!25}{84.4}\% & \cellcolor{green!25}{84.8\%}\\
 \hline
\end{tabular}
\end{table}
}




\section{Discussion}\label{sec:discussion}

We have presented a fullbody avatar system that has high realism, can be rendered in real-time on a mobile phone, and is learned from monocular videos. Our approach addresses the issue of poor stability and overfitting caused by imperfect human mesh fitting. We improve a recent \mbox{StylePeople} method via \textit{neural texture warping}. As a result, the neural texture and rendering network's fitting capacities are used to generate sharper images while avoiding overfitting to motions that are not modeled by the proxy geometry. 

The approach's main limitation is that we simply ``factor out'' the garment and hair motions by leveraging constant warping at test time. It would be preferable to have more intricate modeling of pose-dependent garment and hair motion. Because the current \mbox{SMPL-X} proxy geometry is ``naked'', it would be advantageous to off-load rendering capacity to explicit modelling of detailed geometry. Also artifacts can appear due to unmodelled environmental lighting of training videos. Another ground for future work is the joint modeling of multiple identities, which should allow to model new avatars from fewer images and with less compute time.

\FloatBarrier
%\ifnum\value{page}>8 \errmessage{Number of pages exceeded!!!!}\fi

%%%%%%%%% REFERENCES
{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}


\end{document}