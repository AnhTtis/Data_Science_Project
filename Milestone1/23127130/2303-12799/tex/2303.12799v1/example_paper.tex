%%%%%%%% ICML 2023 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}
% \input{math_commands.tex}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2023} with \usepackage[nohyperref]{icml2023} above.
\usepackage{hyperref}

% Attempt to make hyperref and algorithmic work together better:
% \newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2023}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2023}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

%%%%%%%%%%%%%%%%%%%%%%%
% Mark sections of captions for referring to divisions of figures
\newcommand{\figleft}{{\em (Left)}}
\newcommand{\figcenter}{{\em (Center)}}
\newcommand{\figright}{{\em (Right)}}
\newcommand{\figtop}{{\em (Top)}}
\newcommand{\figbottom}{{\em (Bottom)}}
\newcommand{\captiona}{{\em (a)}}
\newcommand{\captionb}{{\em (b)}}
\newcommand{\captionc}{{\em (c)}}
\newcommand{\captiond}{{\em (d)}}

% Highlight a newly defined term
\newcommand{\newterm}[1]{{\bf #1}}


% Figure reference, lower-case.
\def\figref#1{figure~\ref{#1}}
% Figure reference, capital. For start of sentence
\def\Figref#1{Figure~\ref{#1}}
\def\twofigref#1#2{figures \ref{#1} and \ref{#2}}
\def\quadfigref#1#2#3#4{figures \ref{#1}, \ref{#2}, \ref{#3} and \ref{#4}}
% Section reference, lower-case.
\def\secref#1{section~\ref{#1}}
% Section reference, capital.
\def\Secref#1{Section~\ref{#1}}
% Reference to two sections.
\def\twosecrefs#1#2{sections \ref{#1} and \ref{#2}}
% Reference to three sections.
\def\secrefs#1#2#3{sections \ref{#1}, \ref{#2} and \ref{#3}}
% Reference to an equation, lower-case.
\def\eqref#1{equation~\ref{#1}}
% Reference to an equation, upper case
\def\Eqref#1{Equation~\ref{#1}}
% A raw reference to an equation---avoid using if possible
\def\plaineqref#1{\ref{#1}}
% Reference to a chapter, lower-case.
\def\chapref#1{chapter~\ref{#1}}
% Reference to an equation, upper case.
\def\Chapref#1{Chapter~\ref{#1}}
% Reference to a range of chapters
\def\rangechapref#1#2{chapters\ref{#1}--\ref{#2}}
% Reference to an algorithm, lower-case.
\def\algref#1{algorithm~\ref{#1}}
% Reference to an algorithm, upper case.
\def\Algref#1{Algorithm~\ref{#1}}
\def\twoalgref#1#2{algorithms \ref{#1} and \ref{#2}}
\def\Twoalgref#1#2{Algorithms \ref{#1} and \ref{#2}}
% Reference to a part, lower case
\def\partref#1{part~\ref{#1}}
% Reference to a part, upper case
\def\Partref#1{Part~\ref{#1}}
\def\twopartref#1#2{parts \ref{#1} and \ref{#2}}

\def\ceil#1{\lceil #1 \rceil}
\def\floor#1{\lfloor #1 \rfloor}
\def\1{\bm{1}}
\newcommand{\train}{\mathcal{D}}
\newcommand{\valid}{\mathcal{D_{\mathrm{valid}}}}
\newcommand{\test}{\mathcal{D_{\mathrm{test}}}}

\def\eps{{\epsilon}}


% Random variables
\def\reta{{\textnormal{$\eta$}}}
\def\ra{{\textnormal{a}}}
\def\rb{{\textnormal{b}}}
\def\rc{{\textnormal{c}}}
\def\rd{{\textnormal{d}}}
\def\re{{\textnormal{e}}}
\def\rf{{\textnormal{f}}}
\def\rg{{\textnormal{g}}}
\def\rh{{\textnormal{h}}}
\def\ri{{\textnormal{i}}}
\def\rj{{\textnormal{j}}}
\def\rk{{\textnormal{k}}}
\def\rl{{\textnormal{l}}}
% rm is already a command, just don't name any random variables m
\def\rn{{\textnormal{n}}}
\def\ro{{\textnormal{o}}}
\def\rp{{\textnormal{p}}}
\def\rq{{\textnormal{q}}}
\def\rr{{\textnormal{r}}}
\def\rs{{\textnormal{s}}}
\def\rt{{\textnormal{t}}}
\def\ru{{\textnormal{u}}}
\def\rv{{\textnormal{v}}}
\def\rw{{\textnormal{w}}}
\def\rx{{\textnormal{x}}}
\def\ry{{\textnormal{y}}}
\def\rz{{\textnormal{z}}}

% Random vectors
\def\rvepsilon{{\mathbf{\epsilon}}}
\def\rvtheta{{\mathbf{\theta}}}
\def\rva{{\mathbf{a}}}
\def\rvb{{\mathbf{b}}}
\def\rvc{{\mathbf{c}}}
\def\rvd{{\mathbf{d}}}
\def\rve{{\mathbf{e}}}
\def\rvf{{\mathbf{f}}}
\def\rvg{{\mathbf{g}}}
\def\rvh{{\mathbf{h}}}
\def\rvu{{\mathbf{i}}}
\def\rvj{{\mathbf{j}}}
\def\rvk{{\mathbf{k}}}
\def\rvl{{\mathbf{l}}}
\def\rvm{{\mathbf{m}}}
\def\rvn{{\mathbf{n}}}
\def\rvo{{\mathbf{o}}}
\def\rvp{{\mathbf{p}}}
\def\rvq{{\mathbf{q}}}
\def\rvr{{\mathbf{r}}}
\def\rvs{{\mathbf{s}}}
\def\rvt{{\mathbf{t}}}
\def\rvu{{\mathbf{u}}}
\def\rvv{{\mathbf{v}}}
\def\rvw{{\mathbf{w}}}
\def\rvx{{\mathbf{x}}}
\def\rvy{{\mathbf{y}}}
\def\rvz{{\mathbf{z}}}

% Elements of random vectors
\def\erva{{\textnormal{a}}}
\def\ervb{{\textnormal{b}}}
\def\ervc{{\textnormal{c}}}
\def\ervd{{\textnormal{d}}}
\def\erve{{\textnormal{e}}}
\def\ervf{{\textnormal{f}}}
\def\ervg{{\textnormal{g}}}
\def\ervh{{\textnormal{h}}}
\def\ervi{{\textnormal{i}}}
\def\ervj{{\textnormal{j}}}
\def\ervk{{\textnormal{k}}}
\def\ervl{{\textnormal{l}}}
\def\ervm{{\textnormal{m}}}
\def\ervn{{\textnormal{n}}}
\def\ervo{{\textnormal{o}}}
\def\ervp{{\textnormal{p}}}
\def\ervq{{\textnormal{q}}}
\def\ervr{{\textnormal{r}}}
\def\ervs{{\textnormal{s}}}
\def\ervt{{\textnormal{t}}}
\def\ervu{{\textnormal{u}}}
\def\ervv{{\textnormal{v}}}
\def\ervw{{\textnormal{w}}}
\def\ervx{{\textnormal{x}}}
\def\ervy{{\textnormal{y}}}
\def\ervz{{\textnormal{z}}}

% Random matrices
\def\rmA{{\mathbf{A}}}
\def\rmB{{\mathbf{B}}}
\def\rmC{{\mathbf{C}}}
\def\rmD{{\mathbf{D}}}
\def\rmE{{\mathbf{E}}}
\def\rmF{{\mathbf{F}}}
\def\rmG{{\mathbf{G}}}
\def\rmH{{\mathbf{H}}}
\def\rmI{{\mathbf{I}}}
\def\rmJ{{\mathbf{J}}}
\def\rmK{{\mathbf{K}}}
\def\rmL{{\mathbf{L}}}
\def\rmM{{\mathbf{M}}}
\def\rmN{{\mathbf{N}}}
\def\rmO{{\mathbf{O}}}
\def\rmP{{\mathbf{P}}}
\def\rmQ{{\mathbf{Q}}}
\def\rmR{{\mathbf{R}}}
\def\rmS{{\mathbf{S}}}
\def\rmT{{\mathbf{T}}}
\def\rmU{{\mathbf{U}}}
\def\rmV{{\mathbf{V}}}
\def\rmW{{\mathbf{W}}}
\def\rmX{{\mathbf{X}}}
\def\rmY{{\mathbf{Y}}}
\def\rmZ{{\mathbf{Z}}}

% Elements of random matrices
\def\ermA{{\textnormal{A}}}
\def\ermB{{\textnormal{B}}}
\def\ermC{{\textnormal{C}}}
\def\ermD{{\textnormal{D}}}
\def\ermE{{\textnormal{E}}}
\def\ermF{{\textnormal{F}}}
\def\ermG{{\textnormal{G}}}
\def\ermH{{\textnormal{H}}}
\def\ermI{{\textnormal{I}}}
\def\ermJ{{\textnormal{J}}}
\def\ermK{{\textnormal{K}}}
\def\ermL{{\textnormal{L}}}
\def\ermM{{\textnormal{M}}}
\def\ermN{{\textnormal{N}}}
\def\ermO{{\textnormal{O}}}
\def\ermP{{\textnormal{P}}}
\def\ermQ{{\textnormal{Q}}}
\def\ermR{{\textnormal{R}}}
\def\ermS{{\textnormal{S}}}
\def\ermT{{\textnormal{T}}}
\def\ermU{{\textnormal{U}}}
\def\ermV{{\textnormal{V}}}
\def\ermW{{\textnormal{W}}}
\def\ermX{{\textnormal{X}}}
\def\ermY{{\textnormal{Y}}}
\def\ermZ{{\textnormal{Z}}}

% Vectors
\def\vzero{{\bm{0}}}
\def\vone{{\bm{1}}}
\def\vmu{{\bm{\mu}}}
\def\vtheta{{\bm{\theta}}}
\def\va{{\bm{a}}}
\def\vb{{\bm{b}}}
\def\vc{{\bm{c}}}
\def\vd{{\bm{d}}}
\def\ve{{\bm{e}}}
\def\vf{{\bm{f}}}
\def\vg{{\bm{g}}}
\def\vh{{\bm{h}}}
\def\vi{{\bm{i}}}
\def\vj{{\bm{j}}}
\def\vk{{\bm{k}}}
\def\vl{{\bm{l}}}
\def\vm{{\bm{m}}}
\def\vn{{\bm{n}}}
\def\vo{{\bm{o}}}
\def\vp{{\bm{p}}}
\def\vq{{\bm{q}}}
\def\vr{{\bm{r}}}
\def\vs{{\bm{s}}}
\def\vt{{\bm{t}}}
\def\vu{{\bm{u}}}
\def\vv{{\bm{v}}}
\def\vw{{\bm{w}}}
\def\vx{{\bm{x}}}
\def\vy{{\bm{y}}}
\def\vz{{\bm{z}}}

% Elements of vectors
\def\evalpha{{\alpha}}
\def\evbeta{{\beta}}
\def\evepsilon{{\epsilon}}
\def\evlambda{{\lambda}}
\def\evomega{{\omega}}
\def\evmu{{\mu}}
\def\evpsi{{\psi}}
\def\evsigma{{\sigma}}
\def\evtheta{{\theta}}
\def\eva{{a}}
\def\evb{{b}}
\def\evc{{c}}
\def\evd{{d}}
\def\eve{{e}}
\def\evf{{f}}
\def\evg{{g}}
\def\evh{{h}}
\def\evi{{i}}
\def\evj{{j}}
\def\evk{{k}}
\def\evl{{l}}
\def\evm{{m}}
\def\evn{{n}}
\def\evo{{o}}
\def\evp{{p}}
\def\evq{{q}}
\def\evr{{r}}
\def\evs{{s}}
\def\evt{{t}}
\def\evu{{u}}
\def\evv{{v}}
\def\evw{{w}}
\def\evx{{x}}
\def\evy{{y}}
\def\evz{{z}}

% Matrix
\def\mA{{\bm{A}}}
\def\mB{{\bm{B}}}
\def\mC{{\bm{C}}}
\def\mD{{\bm{D}}}
\def\mE{{\bm{E}}}
\def\mF{{\bm{F}}}
\def\mG{{\bm{G}}}
\def\mH{{\bm{H}}}
\def\mI{{\bm{I}}}
\def\mJ{{\bm{J}}}
\def\mK{{\bm{K}}}
\def\mL{{\bm{L}}}
\def\mM{{\bm{M}}}
\def\mN{{\bm{N}}}
\def\mO{{\bm{O}}}
\def\mP{{\bm{P}}}
\def\mQ{{\bm{Q}}}
\def\mR{{\bm{R}}}
\def\mS{{\bm{S}}}
\def\mT{{\bm{T}}}
\def\mU{{\bm{U}}}
\def\mV{{\bm{V}}}
\def\mW{{\bm{W}}}
\def\mX{{\bm{X}}}
\def\mY{{\bm{Y}}}
\def\mZ{{\bm{Z}}}
\def\mBeta{{\bm{\beta}}}
\def\mPhi{{\bm{\Phi}}}
\def\mLambda{{\bm{\Lambda}}}
\def\mSigma{{\bm{\Sigma}}}

% Tensor
\DeclareMathAlphabet{\mathsfit}{\encodingdefault}{\sfdefault}{m}{sl}
\SetMathAlphabet{\mathsfit}{bold}{\encodingdefault}{\sfdefault}{bx}{n}
\newcommand{\tens}[1]{\bm{\mathsfit{#1}}}
\def\tA{{\tens{A}}}
\def\tB{{\tens{B}}}
\def\tC{{\tens{C}}}
\def\tD{{\tens{D}}}
\def\tE{{\tens{E}}}
\def\tF{{\tens{F}}}
\def\tG{{\tens{G}}}
\def\tH{{\tens{H}}}
\def\tI{{\tens{I}}}
\def\tJ{{\tens{J}}}
\def\tK{{\tens{K}}}
\def\tL{{\tens{L}}}
\def\tM{{\tens{M}}}
\def\tN{{\tens{N}}}
\def\tO{{\tens{O}}}
\def\tP{{\tens{P}}}
\def\tQ{{\tens{Q}}}
\def\tR{{\tens{R}}}
\def\tS{{\tens{S}}}
\def\tT{{\tens{T}}}
\def\tU{{\tens{U}}}
\def\tV{{\tens{V}}}
\def\tW{{\tens{W}}}
\def\tX{{\tens{X}}}
\def\tY{{\tens{Y}}}
\def\tZ{{\tens{Z}}}


% Graph
\def\gA{{\mathcal{A}}}
\def\gB{{\mathcal{B}}}
\def\gC{{\mathcal{C}}}
\def\gD{{\mathcal{D}}}
\def\gE{{\mathcal{E}}}
\def\gF{{\mathcal{F}}}
\def\gG{{\mathcal{G}}}
\def\gH{{\mathcal{H}}}
\def\gI{{\mathcal{I}}}
\def\gJ{{\mathcal{J}}}
\def\gK{{\mathcal{K}}}
\def\gL{{\mathcal{L}}}
\def\gM{{\mathcal{M}}}
\def\gN{{\mathcal{N}}}
\def\gO{{\mathcal{O}}}
\def\gP{{\mathcal{P}}}
\def\gQ{{\mathcal{Q}}}
\def\gR{{\mathcal{R}}}
\def\gS{{\mathcal{S}}}
\def\gT{{\mathcal{T}}}
\def\gU{{\mathcal{U}}}
\def\gV{{\mathcal{V}}}
\def\gW{{\mathcal{W}}}
\def\gX{{\mathcal{X}}}
\def\gY{{\mathcal{Y}}}
\def\gZ{{\mathcal{Z}}}

% Sets
\def\sA{{\mathbb{A}}}
\def\sB{{\mathbb{B}}}
\def\sC{{\mathbb{C}}}
\def\sD{{\mathbb{D}}}
% Don't use a set called E, because this would be the same as our symbol
% for expectation.
\def\sF{{\mathbb{F}}}
\def\sG{{\mathbb{G}}}
\def\sH{{\mathbb{H}}}
\def\sI{{\mathbb{I}}}
\def\sJ{{\mathbb{J}}}
\def\sK{{\mathbb{K}}}
\def\sL{{\mathbb{L}}}
\def\sM{{\mathbb{M}}}
\def\sN{{\mathbb{N}}}
\def\sO{{\mathbb{O}}}
\def\sP{{\mathbb{P}}}
\def\sQ{{\mathbb{Q}}}
\def\sR{{\mathbb{R}}}
\def\sS{{\mathbb{S}}}
\def\sT{{\mathbb{T}}}
\def\sU{{\mathbb{U}}}
\def\sV{{\mathbb{V}}}
\def\sW{{\mathbb{W}}}
\def\sX{{\mathbb{X}}}
\def\sY{{\mathbb{Y}}}
\def\sZ{{\mathbb{Z}}}

% Entries of a matrix
\def\emLambda{{\Lambda}}
\def\emA{{A}}
\def\emB{{B}}
\def\emC{{C}}
\def\emD{{D}}
\def\emE{{E}}
\def\emF{{F}}
\def\emG{{G}}
\def\emH{{H}}
\def\emI{{I}}
\def\emJ{{J}}
\def\emK{{K}}
\def\emL{{L}}
\def\emM{{M}}
\def\emN{{N}}
\def\emO{{O}}
\def\emP{{P}}
\def\emQ{{Q}}
\def\emR{{R}}
\def\emS{{S}}
\def\emT{{T}}
\def\emU{{U}}
\def\emV{{V}}
\def\emW{{W}}
\def\emX{{X}}
\def\emY{{Y}}
\def\emZ{{Z}}
\def\emSigma{{\Sigma}}

% entries of a tensor
% Same font as tensor, without \bm wrapper
\newcommand{\etens}[1]{\mathsfit{#1}}
\def\etLambda{{\etens{\Lambda}}}
\def\etA{{\etens{A}}}
\def\etB{{\etens{B}}}
\def\etC{{\etens{C}}}
\def\etD{{\etens{D}}}
\def\etE{{\etens{E}}}
\def\etF{{\etens{F}}}
\def\etG{{\etens{G}}}
\def\etH{{\etens{H}}}
\def\etI{{\etens{I}}}
\def\etJ{{\etens{J}}}
\def\etK{{\etens{K}}}
\def\etL{{\etens{L}}}
\def\etM{{\etens{M}}}
\def\etN{{\etens{N}}}
\def\etO{{\etens{O}}}
\def\etP{{\etens{P}}}
\def\etQ{{\etens{Q}}}
\def\etR{{\etens{R}}}
\def\etS{{\etens{S}}}
\def\etT{{\etens{T}}}
\def\etU{{\etens{U}}}
\def\etV{{\etens{V}}}
\def\etW{{\etens{W}}}
\def\etX{{\etens{X}}}
\def\etY{{\etens{Y}}}
\def\etZ{{\etens{Z}}}

% The true underlying data generating distribution
\newcommand{\pdata}{p_{\rm{data}}}
% The empirical distribution defined by the training set
\newcommand{\ptrain}{\hat{p}_{\rm{data}}}
\newcommand{\Ptrain}{\hat{P}_{\rm{data}}}
% The model distribution
\newcommand{\pmodel}{p_{\rm{model}}}
\newcommand{\Pmodel}{P_{\rm{model}}}
\newcommand{\ptildemodel}{\tilde{p}_{\rm{model}}}
% Stochastic autoencoder distributions
\newcommand{\pencode}{p_{\rm{encoder}}}
\newcommand{\pdecode}{p_{\rm{decoder}}}
\newcommand{\precons}{p_{\rm{reconstruct}}}

\newcommand{\laplace}{\mathrm{Laplace}} % Laplace distribution

\newcommand{\E}{\mathbb{E}}
\newcommand{\Ls}{\mathcal{L}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\emp}{\tilde{p}}
\newcommand{\lr}{\alpha}
\newcommand{\reg}{\lambda}
\newcommand{\rect}{\mathrm{rectifier}}
\newcommand{\softmax}{\mathrm{softmax}}
\newcommand{\sigmoid}{\sigma}
\newcommand{\softplus}{\zeta}
\newcommand{\KL}{D_{\mathrm{KL}}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\standarderror}{\mathrm{SE}}
\newcommand{\Cov}{\mathrm{Cov}}
% Wolfram Mathworld says $L^2$ is for function spaces and $\ell^2$ is for vectors
% But then they seem to use $L^2$ for vectors throughout the site, and so does
% wikipedia.
\newcommand{\normlzero}{L^0}
\newcommand{\normlone}{L^1}
\newcommand{\normltwo}{L^2}
\newcommand{\normlp}{L^p}
\newcommand{\normmax}{L^\infty}

\newcommand{\parents}{Pa} % See usage in notation.tex. Chosen to match Daphne's book.

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\Tr}{Tr}
\let\ab\allowbreak
%%%%%%%%%%%%%%%%%%%%%%%
% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

% \newcommand{\valstd}[2]{$#1 {\scriptstyle \,\pm\, #2}$}
% \newcommand{\valstdb}[2]{$\mbf{#1} {\scriptstyle \,\pm\, #2}$}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}

% \DeclareTextFontCommand{\textbfit}{%
%   \fontseries\bfdefault % change series without selecting the font yet
%   \itshape
% }
% \newcommand{\fix}{\marginpar{FIX}}
% \newcommand{\new}{\marginpar{NEW}}

\usepackage{url}
\usepackage{xspace}
\usepackage{multirow}
\usepackage{bm}
\usepackage{times}
\usepackage{wrapfig}
\usepackage[frozencache,cachedir=.]{minted}

% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Time Series as Images: Vision Transformer for Irregularly Sampled Time Series}

\begin{document}

\twocolumn[
\icmltitle{Time Series as Images: Vision Transformer for Irregularly Sampled Time Series}
% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2023
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
% \icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Zekun Li}{ucsb}
\icmlauthor{Shiyang Li}{ucsb}
\icmlauthor{Xifeng Yan}{ucsb}
\end{icmlauthorlist}

\icmlaffiliation{ucsb}{Department of Computer Science, University of California, Santa Barbara, CA}

\icmlcorrespondingauthor{Zekun Li}{zekunli@cs.ucsb.edu}
\icmlcorrespondingauthor{Xifeng Yan}{xyan@cs.ucsb.edu}
% \icmlcorrespondingauthor{Firstname2 Lastname2}{first2.last2@www.uk}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
% \icmlkeywords{, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
% \printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}
Irregularly sampled time series are becoming increasingly prevalent in various domains, especially in medical applications. Although different highly-customized methods have been proposed to tackle irregularity, how to effectively model their complicated dynamics and high sparsity is still an open problem.
This paper studies the problem from a whole new perspective: transforming irregularly sampled time series into line graph images and adapting powerful vision transformers to perform time series classification in the same way as image classification.
Our approach largely simplifies algorithm designs without assuming prior knowledge and can be potentially extended as a general-purpose framework. Despite its simplicity, we show that it substantially outperforms state-of-the-art specialized algorithms on several popular healthcare and human activity datasets. Especially in the challenging leave-sensors-out setting where a subset of variables is masked during testing, the performance improvement is up to 54.0\% in absolute F1 score points. Our code and data are available at \url{https://github.com/Leezekun/ViTST}.
% 
\end{abstract}


\section{Introduction}\label{sect:intro}
Time series data are ubiquitous in a wide range of domains, including healthcare, finance, traffic, and climate science. With the advances in deep learning architectures such as LSTM~\cite{lstm}, Temporal Convolutional Network (TCN)~\cite{tcn}, and Transformer~\cite{transformer}, numerous algorithms have been developed for time series modeling. 
% With the recent advances in deep learning architectures, numerous algorithms have been developed for time series analysis. 
However, these methods typically assume fully observed data points at regular intervals and fixed-size numerical inputs. They cannot deal with irregularly sampled ones, a sequence of samples with irregular intervals between their observation times. 
To tackle this challenge, highly specialized models were developed, which require a considerable amount of prior knowledge in the model architecture choice and design~\cite{marlin2012unsupervised,lipton2016directly,gru-d,seft,raindrop,mtand,Zhang2022ImprovingMP}. 

The recently emerging transformer-based vision models, most notably Vision Transformers~\cite{vit}\footnote{In this paper, we refer to vision transformers as a type of vision models based on Transformer, including ViT~\cite{vit}, Swin Transformer~\cite{swin}, DeiT~\cite{deit}, to name a few.}, have demonstrated strong performance on various vision tasks such as image classification and object detection. In this paper, we raise a simple question: \textit{Since vision transformers have exceeded humans in various image recognition tasks, can they ``visually'' capture temporal patterns in the visualized time series data?} To answer this question, we explore the following minimalist approach: transform the irregularly sampled multivariate time series into line graphs (Fig.~\ref{fig:illustration}), arrange these line graphs into a standard RGB image, and train a vision transformer to perceive the image and perform the classification task. We dub this approach \textbf{ViTST}, short for \textbf{Vi}sion \textbf{T}ime \textbf{S}eries \textbf{T}ransformer. 

The line graph images could encode two kinds of informative patterns in multivariate time series: (1) the temporal dynamics of each variable in its corresponding line graph; and (2) the correlation of variables across different line graphs. We assume that vision transformers can capture pattern (1) via modeling local patch interactions within a single time series line graph images and pattern (2) from global patch interactions across different line graphs. Experimental results demonstrate that our approach ViTST outperforms previous state-of-the-art (SOTA) results by 2.4 and 1.2 AUROC points (\%) on two irregularly sampled healthcare datasets P19~\cite{p19} and P12~\cite{p12}, and 7.6 accuracy and 6.8 F1 score points (\%) on a human activity dataset PAM~\cite{pam}. Our approach also shows superior robustness to missing observations. It improves the prior work by up to 54.0\% in absolute F1-score points in the challenging leave-sensors-out setting where a part of the sensors (variables) in the test set are masked. We also evaluate our approach ViTST on regular time series data, where it still attains excellent results compared with SOTA algorithms designed for regular time series modeling, demonstrating its generality as regularly sampled time series algorithms that usually do not work well for irregularly sampled data and vice versa.

In summary, the contributions of this work are three-fold: (1) We propose a simple but effective approach for multivariate irregularly sampled time series classification. The simplicity contrasts with the state-of-the-art performance it has achieved. (2) The proposed approach attains excellent results on both irregular and regular time series data, and can be potentially extended as a general-purpose framework for time series modeling. 
% It adds to the collection of works exploring utilizing large-scale pre-trained foundation models to deal with tasks in various domains, and is an extension of prompt-based learning from NLP to CV field.
% It adds to the collection of works exploring artificial general intelligence (AGI) to deal with tasks in various domains instead of dedicated designs for each task.
(3) It opens up a new direction and might encourage the utilization of fast-evolving and well-studied computer vision techniques in the time series domain, such as better model architecture~\cite{liu2022swin}, data augmentation~\cite{shorten2019survey}, interpretability~\cite{chefer2021transformer}, self-supervised pre-training~\cite{mae}, to name a few.


\begin{figure*}[tbp]
    \centering
    \includegraphics[width = 0.95\linewidth]{./pic/illustration13.pdf}
    \caption{An illustration of our approach ViTST. The example is from a healthcare dataset P12~\cite{p12}, which provides the irregularly sampled observations of 36 variables for patients (we only show 4 variables here for simplicity). Each column in the table is an observation of a variable, with the observed time and value. We plot separate line graphs for each variable and arrange them into an image, which is then fed into the vision transformer to perform the classification task.}
    \vspace{-4mm}
    \label{fig:illustration}
\end{figure*}



\section{Related work}\label{sect:related_work}

\textbf{Irregularly Sampled Time Series.~}
An irregularly sampled time series is a sequence of observations with irregular intervals between observation times. In a multivariate setting, different variables within the same time series may not align. Such characteristics have posed a significant challenge to the standard time series modeling methods, which typically assume fully observed and regularly sampled data points.

A common approach to handle irregular sampling is to convert continuous-time observations into fixed time intervals~\cite{marlin2012unsupervised,lipton2016directly}. To incorporate the dynamics between observations, GRU-D~\cite{gru-d} decays the hidden states based on gated recurrent units (GRU)~\cite{chung2014empirical}, which takes as input the observations' values and also times. \cite{pham2017predicting} modified the forget gate of LSTM~\cite{lstm} to better account for irregularity.
Similarly, \cite{yoon2017multi} proposed an approach based on multi-directional RNN, which can capture the inter- and intra-steam patterns. Besides the recurrent and differential equation-based model architectures, recent work has explored attention-based models. Transformer~\cite{transformer} is naturally able to handle arbitrary sequences of observations. ATTAIN~\cite{attain} incorporates attention mechanism with LSTM to model the time irregularity between observations. SeFT~\cite{seft} maps the irregular time series into a set of observations based on differentiable set functions and utilizes an attention mechanism for classification. mTAND~\cite{mtand} presented a multi-time attention network, which learns continuous-time embeddings coupled with a multi-time attention mechanism to deal with the continuous-time inputs. UTDE~\cite{Zhang2022ImprovingMP} integrated embeddings from mTAND and classical imputed time series with learnable gates to take their advantages for tackling complex temporal patterns. Raindrop~\cite{raindrop} modeled the irregularly sampled time series as graphs and utilized graph neural networks~\citep{gcn,graphsage} to model the relationships between different variables. Overall, these methods are all highly specialized for irregular time series. In this work, we explore a simple and general vision transformer-based approach for irregularly sampled time series modeling without using dedicated model architecture modifications.

\noindent\textbf{Numerical Time Series Modeling Methods with Transformer.~}
Transformers possess superior abilities to capture long-range dependencies in sequential data, making them appealing to time series modeling~\cite{logtrans}. A surge of transformer-based methods have been proposed and successfully applied to various time series modeling tasks, such as forecasting~\cite{logtrans,informer,autoformer,fedformer}, classification~\cite{tst}, and anomaly detection~\cite{anomalytrans}. These methods are usually designed for regular time series settings, where they view multivariate numerical values at the same timestamp as a unit and model temporal interactions across different units. A recent work~\cite{patchtst}, on the other hand, proposes to segment each univariate time series into a sequence of sub-series and model their interactions independently. 


\noindent\textbf{Time Series as Other Modalities.~}
The recently emerging pre-trained transformer-based models, initially proposed in Natural Language Processing (NLP) field, have since come to monopolize the state-of-the-art performance across various downstream tasks in NLP and Computer Vision (CV) fields. For example, the pre-trained language model BERT~\cite{bert} and GPTs~\cite{gpt1,gpt2,gpt3} can be adapted to various NLP tasks. Some non-language tasks can also be solved by these pre-trained transformer-based language models by transforming them into language sentence prompts~\cite{lift}. A recent work~\cite{pisa} tried to represent the time series in natural language and utilize pre-trained language models to forecast. However, such a method has difficulties modeling long-range multivariate time series as it usually involves tens of thousands of numerical values, which cannot be fitted into the language models (512/1024 max tokens for most LMs). In addition, it is hard to express the informative irregularity of time series in natural language sentences. By contrast, we transform numerical time series data into images and utilize pre-trained transformer-based vision models to perform time series modeling, which doesn't have the issues.
% which is more efficient and effective in expressing and perceiving the patterns in irregularly sampled multivariate time series \shiyang{Do you have any evidence to say it's more efficient and effective?}.
Note that some prior studies tried to transform time series into Gramian fields~\cite{wang2015imaging}, recurring plots~\cite{hatami2018classification,tripathy2018use}, and Markov transition fields~\cite{wang2015spatially} images and utilize CNN to perform classifications. However, these methods are not domain-agnostic and require domain knowledge in designing specialized imaging methods. By contrast, we simply transform time series into line graph RGB images without assuming prior knowledge.


\section{Approach}\label{sect:approach}
% Let $\gD=\{(\rvs_{i},y_{i})|i=1,\cdots,N\}$ denotes a time series dataset containing $N$ samples. Each individual data sample is associated with a label $y_i \in \{1,\cdots, C\}$, where $C$ is the number of classes. 
As illustrated in Fig.~\ref{fig:illustration},  ViTST consists of two steps: (1) transform multivariate time series into a concatenated line graph image; (2) utilize the vision transformer as an image classifier for the classification task. To begin with, we present some basic notations and problem formulation.
 

\noindent\textbf{Notation.~} Let $\gD=\{(\gS_{i},y_{i})|i=1,\cdots,N\}$ denote a time series dataset containing $N$ samples. Every data sample is associated with a label $y_i \in \{1,\cdots, C\}$, where $C$ is the number of classes. Each multivariate time series $\gS_{i}$ consists of observations of $D$ variables at most (some variables might have no observations). The observations for each variable $d$ are given by a sequence of tuples with observed time and value $[(t_1^d,v_1^d), (t_2^d,v_2^d), \cdots,(t_{n_d}^d,v_{n_d}^d)]$, where $n_d$ is the number of observations for variable $d$. If the successive intervals among observation times $[t_1^d, t_2^d, \cdots, t_{n_d}^d]$ are different within the same variable or across variables/samples, $\gS_{i}$ is an irregularly sampled time series. Otherwise, it is regular time series. 

\textbf{Problem Formulation.~} Given the dataset $\gD=\{(\gS_{i},y_{i})|i=1,\cdots,N\}$ containing $N$ multivariate time series, we aim to predict the label $\hat{y}_i \in \{1,\cdots, C\}$ for each time series $\gS_{i}$. 
There are mainly two components in our framework: (1) a function that transforms the time series $\gS_i$ into an image $\rx_i$; (2) a vision transformer serves as an image classifier that takes the line graph image $\rx_i$ as input and predict the corresponding label $\hat{y}_i$. 

\subsection{Time Series to Image Transformation}\label{sect:line_graph}

\noindent\textbf{Time Series Line Graph.~}
Time series line graph is a widely-used data visualization method to illustrate temporal data points at successive intervals. Each point on the line graph corresponds to an observation with an observed time and value. The horizontal axis is used to plot timestamps, and the vertical axis is used to plot values. Straight lines connect the points on the graph in the order of time, where the missing value interpolation is done automatically. 
We use markers ``$\star$'' to indicate the data point in the line. As the scale of different variables varies greatly, we plot the observations of each variable in an individual line graph, as shown in Fig.~\ref{fig:illustration}. The scales of each line graph $\rg_{i,d}$ are kept the same across different time series $\gS_i$. Different colors are used for each line graph to distinguish them. We experimentally found that the tick labels and other components in the line graph figure are unnecessary, as the position of an observation in a line graph indicates the relative magnitude of observed time and value.

\noindent{\textbf{Image Creation.~}}
Given a set of time series line graphs $\gG_i=\{\rg_1, \rg_2, \cdots, \rg_D\}$ for time series $\gS_i$, we place them in a single image $\rx_i$ using a pre-defined grid layout, in which the line graph of each variable is in a grid cell. Similar to \cite{sifar}, we experimentally found that a compact layout (\textit{i.e.}, square grid) leads to consistently good performance. Specifically, given the $D$ time series line graphs for a time series $\gS_i$, we place them in a grid, whose size is $l \times l$ when $l \times (l-1) < D <= l \times l$, and $l \times (l+1) $ when $l \times l < D <= l \times (l+1)$. For example, there are 34, 36, and 17 variables in P19, P12, and PAM datasets, respectively. The default grid layouts are thus $6\times6$, $6\times6$, and $4\times5$. If the grid is not full, the cells at the end of the grid are left blank (see Fig.~\ref{fig:attention_map} for examples of created images from these three datasets). More details are provided in Appendix~\ref{sect:image_creation}.

\subsection{Vision Transformers for Time Series Modeling}\label{sect:vit}
Given the image $\rx_i$ transformed from time series $\gS_i$, we leverage an image classifier to perceive the image and perform the classification task. The time series patterns in a line graph image involve both local (\textit{i.e.}, the temporal dynamics of a single variable in a line graph) and global (the correlation among variables across different line graphs) contexts. To better capture these patterns, we choose the recently developed vision transformers. Unlike the predominant CNNs, vision transformers are proven to have much less image-specific inductive bias and stronger abilities to capture local and global dependencies~\cite{vit,swin}. 

\begin{figure}
    \centering
    \includegraphics[width=0.9\linewidth]{./pic/window.pdf}
    \caption{Illustration of the shifted window approach of Swin Transformer. The self-attention is calculated within each window (grey box). When the window is within a single line graph, the local interactions are captured. After shifting, the window contains patches from different line graphs, and thus the global cross-variable interactions are modeled.}
    \label{fig:window}
    \vspace{-4mm}
\end{figure}

\noindent\textbf{Preliminary.~} Vision Transformer (ViT)~\cite{vit} is originally adapted from NLP. An image is split into fix-sized patches, each linearly embedded and augmented with position embeddings. The resulting sequence of vectors is fed into a standard Transformer encoder consisting of a stack of multi-head attention modules (MSA) and MLP to obtain patch representations. An extra classification token is added to the sequence to perform classification or other tasks. ViT models \textit{global} inter-unit interactions between each pair of patches, which faces efficiency issues when dealing with high-resolution images. Swin Transformer~\citep{swin}, on the other hand, has a hierarchical architecture that contains multi-level feature maps and computes self-attention locally within non-overlapping windows, significantly reducing the computation complexity and improving the recognition performance. We thus use Swin Transformer as the default backbone vision model if not specified. Note that any other vision model can be applied under this framework.

Specifically, Swin Transformer constructs the hierarchical representation starting from small-sized patches in earlier layers to capture the fine-grained \textit{local} information and gradually merging neighboring patches in deeper layers to model \textit{global} coarse-grained information. As illustrated in Fig.~\ref{fig:window}, the self-attention is calculated within each non-overlapping window in the W-MSA block. When the sliding window is within a single line graph for variable $d$, the local intra-variable interactions and temporal dynamics of the variable $d$ are captured. The shifted window block SW-MSA enables the connection of different windows. After shifting, the window spans across different line graphs. 
Mathematically, the consecutive Swin Transformer blocks are calculated as:
\begin{align}
    &{{\hat{\bf{z}}}^{l}} = \text{W-MSA}\left( {\text{LN}\left( {{{\bf{z}}^{l - 1}}} \right)} \right) + {\bf{z}}^{l - 1},\nonumber\\
    &{{\bf{z}}^l} = \text{MLP}\left( {\text{LN}\left( {{{\hat{\bf{z}}}^{l}}} \right)} \right) + {{\hat{\bf{z}}}^{l}},\nonumber\\
    &{{\hat{\bf{z}}}^{l+1}} = \text{SW-MSA}\left( {\text{LN}\left( {{{\bf{z}}^{l}}} \right)} \right) + {\bf{z}}^{l}, \nonumber\\
    &{{\bf{z}}^{l+1}} = \text{MLP}\left( {\text{LN}\left( {{{\hat{\bf{z}}}^{l+1}}} \right)} \right) + {{\hat{\bf{z}}}^{l+1}}, \label{eq:swin}
\end{align}
where ${\hat{\bf{z}}}^l$ and ${\bf{z}}^l$ denote the output features of the (S)W-MSA module and the MLP module for block $l$, respectively; LN stands for the layer normalization~\cite{Ba2016LayerN}. After multiple stages of blocks, the global interactions among the patches from all the line graphs can be modeled, and thus the correlation between different variables is learned.

\textbf{Inference.~} 
We use the vision transformers to predict the labels of time series in the same way as image classification. The outputs of Swin Transformer blocks at the final stage are used as the patch representations, upon which a flattened layer with a linear head is applied to obtain the prediction $\hat{y}_i$. As for ViT, the representation of the additional classification token at the final layer is used for prediction. We use the cross-entropy loss when fine-tuning the model on the classification task.

\section{Experiments}\label{sect:experiments}

\subsection{Experimental Setup}

\begin{table*}[htbp]
\caption{Statistics of the irregularly sampled time series datasets~\cite{raindrop}. ``\#Avg. obs.'' denotes the average number of observations for each sample. ``Static info'' indicates if the time series sample is associated with static attributes (\textit{e.g.}, genders).} 
\centering
\small
\begin{tabular}{lcccrrrr}
\toprule
Datasets & \#Samples & \#Variables & \#Avg. obs.& \#Classes & Static info & Imbalanced & Missing ratio\\ \midrule
P19 & 38,803 & 34 & 401 & 2 & True & True &94.9\%\\
P12 & 11,988 & 36 & 233 & 2 & True & True &88.4\%\\
PAM & 5,333 & 17 & 4,048 & 8 & False & False &60.0\%\\ 
\bottomrule
\end{tabular}
\vspace{-2mm}
\label{tab:datasets}
\end{table*}

\begin{table*}[!h]
\scriptsize
\centering
\caption{Comparison with the baseline methods on irregularly sampled time series classification task. \textbf{Bold} indicates the best performer, while \underline{underline} represents the second best. Results are reported as \%.}
% \vspace{-2mm}
\label{tab:main_result}
\resizebox{\textwidth}{!}{ 
\begin{tabular}{l|ll|ll|llll}
\toprule
& \multicolumn{2}{c|}{P19} & \multicolumn{2}{c|}{P12} & \multicolumn{4}{c}{PAM} \\ \cmidrule{2-9}
\multirow{-2}{*}{Methods} & AUROC & AUPRC & AUROC & AUPRC & Accuracy & Precision & Recall & F1 score \\ \midrule
Transformer & 80.7 $\pm$ 3.8 & 42.7 $\pm$ 7.7 & 83.3 $\pm$ 0.7 &  47.9 $\pm$ 3.6 & 83.5 $\pm$ 1.5 & 84.8 $\pm$ 1.5 & 86.0 $\pm$ 1.2 & 85.0 $\pm$ 1.3 \\
Trans-mean & 83.7 $\pm$ 1.8 & 45.8 $\pm$ 3.2 & 82.6 $\pm$ 2.0 & 46.3 $\pm$ 4.0 &	83.7 $\pm$ 2.3&	84.9 $\pm$ 2.6&	86.4 $\pm$ 2.1&	85.1 $\pm$ 2.4\\
GRU-D & 83.9 $\pm$1.7 & 46.9 $\pm$ 2.1 & 81.9 $\pm$ 2.1 & 46.1 $\pm$ 4.7 & 83.3 $\pm$ 1.6 & 84.6 $\pm$ 1.2 & 85.2 $\pm$ 1.6 & 84.8 $\pm$ 1.2\\
SeFT & 81.2 $\pm$ 2.3 & 41.9 $\pm$ 3.1 & 73.9 $\pm$ 2.5 & 31.1 $\pm$ 4.1 & 67.1 $\pm$ 2.2 & 70.0 $\pm$ 2.4 & 68.2 $\pm$ 1.5 & 68.5 $\pm$ 1.8\\
mTAND & 84.4 $\pm$ 1.3 & 50.6 $\pm$ 2.0 & 84.2 $\pm$ 0.8 & \underline{48.2} $\pm$ 3.4  & 74.6 $\pm$ 4.3 & 74.3 $\pm$ 4.0 & 79.5 $\pm$ 2.8 & 76.8 $\pm$ 3.4\\ 
IP-Net & 84.6 $\pm$ 1.3 & 38.1 $\pm$ 3.7 & 82.6 $\pm$ 1.4 & 47.6 $\pm$ 3.1 & 74.3 $\pm$ 3.8 & 75.6 $\pm$ 2.1 & 77.9 $\pm$ 2.2 & 76.6 $\pm$ 2.8 \\
DGM$^2$-O & 86.7 $\pm$ 3.4 & 44.7 $\pm$ 11.7 & \underline{84.4} $\pm$ 1.6  & 47.3 $\pm$ 3.6 & 82.4 $\pm$ 2.3 &  85.2 $\pm$ 1.2 & 83.9 $\pm$ 2.3 & 84.3 $\pm$ 1.8 \\
MTGNN &81.9 $\pm$ 6.2  & 39.9 $\pm$ 8.9  & 74.4 $\pm$ 6.7 &  35.5 $\pm$ 6.0 & 83.4 $\pm$ 1.9 & 85.2 $\pm$ 1.7 & 86.1 $\pm$ 1.9 & 85.9 $\pm$ 2.4 \\
Raindrop & \underline{87.0} $\pm$ 2.3  & \underline{51.8} $\pm$ 5.5  & 82.8 $\pm$ 1.7 & 44.0 $\pm$ 3.0 & \underline{88.5} $\pm$ 1.5 & \underline{89.9} $\pm$ 1.5 & \underline{89.9} $\pm$ 1.5 & \underline{89.8} $\pm$ 1.0 \\
\midrule
% ResNet & 80.1 $\pm$ 2.1 & 38.3 $\pm$ 2.7 &0 &0 &0 &0 &0 &0 &0\\
% \textbf{ViTST-ViT}  &87.6 $\pm$ 2.2 & 51.4 $\pm$ 1.8 & 83.5 $\pm$ 1.8 & 47.1 $\pm$ 3.3 & 94.6 $\pm$ 0.9 & 95.6 $\pm$ 1.0 & 95.0 $\pm$ 0.8 & 95.3 $\pm$ 0.9\\
\textbf{ViTST} &\textbf{89.4} $\pm$ 1.9 &\textbf{52.8} $\pm$ 3.8 &\textbf{85.6} $\pm$ 1.1 & \textbf{49.8} $\pm$ 2.5 &\textbf{96.1} $\pm$ 0.7 &\textbf{96.8} $\pm$ 1.1 & \textbf{96.5} $\pm$ 0.7 &\textbf{96.6} $\pm$ 0.9\\
\bottomrule
\end{tabular}
}
\vspace{-2mm}
\end{table*}


\textbf{Datasets and Metrics.~} We conduct experiments using three popular datasets in healthcare and human activity, as shown in Table~\ref{tab:datasets}. The P19 dataset~\cite{p19} contains information from 38,803 patients, including 34 sensor variables and a binary label indicating sepsis. The P12 dataset~\cite{p12} consists of data from 11,988 patients, including 36 sensor variables and a binary label indicating survival during hospitalization. The PAM dataset~\cite{pam} includes 5,333 samples from 8 different human activities, with 17 sensor variables provided for each sample.
We used the processed data provided by Raindrop~\cite{raindrop}\footnote{https://github.com/mims-harvard/Raindrop}. More details are given in Appendix~\ref{sect:dataset}.
We employed the same data splits for all comparison baselines, as provided. The evaluation metrics were consistent across all experiments, including the Area Under a ROC Curve (AUROC) and Area Under Precision-Recall Curve (AUPRC) for the imbalanced datasets P12 and P19. For the balanced PAM dataset, we reported Accuracy, Precision, Recall, and F1 score. All the results are reported as \%.


\textbf{Implementation.~} 
% The time-series-to-image transformation can be implemented using the Matplotlib package within a few lines of Python code.
We use the Matplotlib package to draw the line graphs and save them as standard RGB images. The grid layouts of data in P19, P12, and PAM dataset are $6\times6$, $6\times6$, and $4\times5$. We set the size of each grid cell (line graph) as $64\times64$, and thus the image sizes are $384\times384$, $384\times384$, and $256\times320$, respectively. 
% The sizes of created images for each dataset are listed in Table~\ref{tab:size}. 
One can also directly set the image size to any size, regardless of the grid cell size.
We use the checkpoint of Swin Transformer pre-trained on the ImageNet-21K dataset\footnote{https://huggingface.co/microsoft/swin-base-patch4-window7-224-in22k}. 
The default patch size and window size are $4$ and $7$, respectively.


\textbf{Training.~}
We apply the cutout~\cite{cutout} augmentation method on the input images from P12 and P19 datasets during training to avoid over-fitting caused by upsampling. Specifically, 16 square regions with $16 \times 16$ size are randomly masked in each image. The models are trained using A6000 GPUs with 48G memory. As P12 and P19 datasets are highly imbalanced, we upsample the minority class to the same size as the majority class. We fine-tune Swin Transformer 2 and 4 epochs on upsampled P19 and P12 datasets and 20 epochs on the PAM dataset. The batch sizes are 48 for P19 and P12 and 72 for PAM. The learning rate is 2e-5.

\textbf{Incorporating static features.~}
The P12 and P19 datasets provide patients' demographics, such as weight, height, and ICU type. This static information will not change over time and can be well described by the natural language. To incorporate them into our framework, we transform them into natural language sentences via a template and utilize a text encoder RoBERTa-base~\cite{roberta} to encode them. The obtained text embedding is concatenated with the image embeddings obtained from the vision transformer to perform classification. Note that the static feature is also applied to all the baselines we compare.


\begin{figure*}
\centering
\subfigure[Leave-\textbf{fixed}-sensors-out]{
\includegraphics[width=0.98\linewidth]{pic/fixed.pdf}
}
\quad
\subfigure[Leave-\textbf{random}-sensors-out]{
\includegraphics[width=0.98\linewidth]{pic/random.pdf}
}
\caption{Performance in leave-\textbf{fixed}-sensors-out and leave-\textbf{random}-sensors-out settings on PAM dataset. The x-axis is the ``missing ratio'' which denotes the ratio of masked variables. Results are reported as \%. Detailed numbers are provided in Table~\ref{tab:leave-sensors-out} in Appendix~\ref{sect:full_results}.}
\vspace{-4mm}
\label{fig:sensors}
\end{figure*}

\subsection{Main Results}\label{sect:experiment}

\textbf{Comparison to state-of-the-art.~} We compare our approach with several state-of-the-art methods specialized for irregularly sampled time series: Transformer~\cite{transformer} which replaces the missing values with 0, Trans-mean which is Transformer with an imputation method that replaces the missing value with the average observed value of the variable), GRU-D~\cite{gru-d}, SeFT~\cite{seft}, mTAND~\cite{mtand}, IP-Net~\cite{ipnet}, and Raindrop~\cite{raindrop}. 
% Raindrop is the best-performing method before this work. 
Besides, two methods initially designed for forecasting tasks are also compared, including DGM$^2$-O~\cite{dgm} and MTGNN~\cite{mtgnn}. The implementations and hyperparameter settings of these baselines all follow Raindrop~\cite{raindrop}: The batch size is 128, and all the compared models are trained for 20 epochs. As the P12 and P19 datasets are highly imbalanced, it is ensured that each batch is balanced with half negative and half positive samples. 
The performances are averaged on 5 different data splits, which are kept the same across all the compared methods. 

As seen from Table~\ref{tab:main_result}, our approach substantially outperforms the specialized state-of-the-art algorithms on all these three datasets. On the P19 and P12 datasets, ViTST improves the state-of-the-art results by 2.4 and 1.2 AUROC points (\%), respectively. As for the PAM dataset, the improvement is even more significant: 7.6 points in Accuracy, 6.9 points in Precision, 6.6 points in Recall, and 6.8 points in F1 score (\%). 
% Our approach demonstrates substantially strong performance against the highly specialized state-of-the-art methods. 
The larger improvement on PAM dataset might be due to the lower missing ratio of PAM (60.0\%) than P19 (94.9\%) and P12 (88.4\%), meaning that there are more observed values to better recover the fully observed line graphs and reflect patterns (see Fig.~\ref{fig:attention_map} for created images from these three datasets).

\begin{figure*}[!t]
    \centering
    \includegraphics[width=1\linewidth]{./pic/ablation1.pdf}
    \caption{Performance of different backbone vision models and the state-of-the-art model Raindrop on P19, P12, and PAM datasets. Results are reported as \%. Detailed numbers are provided in Table~\ref{tab:vision_models} in Appendix~\ref{sect:full_results}. }
    \label{fig:ablation1}
\vspace{-2mm}
\end{figure*}

\begin{table*}[!h]
\scriptsize
\centering
\caption{Ablation studies on different designs when drawing the time series line graphs. Results are reported as \%.
}
\label{tab:ablation_1}
\resizebox{\textwidth}{!}{ 
\begin{tabular}{l|cc|cc|cccc}
\toprule
& \multicolumn{2}{c|}{P19} & \multicolumn{2}{c|}{P12} & \multicolumn{4}{c}{PAM} \\ \cmidrule{2-9}
\multirow{-2}{*}{Methods} & AUROC & AUPRC & AUROC & AUPRC & Accuracy & Precision & Recall & F1 score \\ \midrule
ViTST & 89.4 $\pm$ 1.9 & 52.8 $\pm$ 3.8 & 85.6 $\pm$ 1.1 & 49.8 $\pm$ 2.5 & 96.1 $\pm$ 0.7 &96.8 $\pm$ 1.1 & 96.5 $\pm$ 0.7 &96.6 $\pm$ 0.9 \\ \midrule
~w/o interpolation & 87.5 $\pm$ 1.5 & 51.2 $\pm$ 3.6 & 84.1 $\pm$ 1.4 & 48.3 $\pm$ 3.5 & 96.0 $\pm$ 1.1 & 96.8 $\pm$ 0.9 & 96.4 $\pm$ 0.9 & 96.6 $\pm$ 0.9 \\ 
~w/o markers & 88.3 $\pm$ 1.6 & 51.0 $\pm$ 2.4 & 84.8 $\pm$ 1.3 & 48.7 $\pm$ 3.8 & 94.1 $\pm$ 0.9 & 95.1 $\pm$ 0.7 & 94.8 $\pm$ 1.1 & 94.9 $\pm$ 0.8 \\ 
~w/o colors & 85.3 $\pm$ 0.8 & 48.5 $\pm$ 2.1 & 83.9 $\pm$ 1.1 & 46.5 $\pm$ 3.2 & 92.9 $\pm$ 1.9 & 94.9 $\pm$ 1.2 & 93.6 $\pm$ 1.5 & 94.1 $\pm$ 1.5 \\ 
\bottomrule
\end{tabular}
}
\vspace{-2mm}
\end{table*}

\textbf{Leaving-sensors-out Settings.~}
We further evaluate models' performance in more challenging leave-sensors-out settings, where the observations of a subset of sensors (variables) are masked during testing. This setting simulates real-world scenarios when some sensors fail or become unreachable. Following \cite{raindrop}, we experiment with two setups on PAM dataset: (1) \textit{leave-\textbf{fixed}-sensors-out}, which drops a fixed set of sensors across all the samples and compared methods; (2) \textit{leave-\textbf{random}-sensors-out} which drops the sensors randomly. Only the observations in the validation and test set are dropped. The training set is kept unchanged. For a fair comparison, we dropped the same set of sensors in the \textit{leave-\textbf{fixed}-sensors-out} setting as in \cite{raindrop}. 
% Note that in our case, if a sensor is masked, the corresponding line graph is left blank.

The results are presented in Fig.~\ref{fig:sensors}, from which we observe that our approach consistently achieves the best performance and outperforms the second-best by a large margin. With the missing ratio ranging from 10\% to 50\%, the performance improvement over the previous best model becomes increasingly significant. When half of the variables are dropped, our approach can still achieve acceptable performance, exceeding the best-performed baseline by up to 50.2\% in Accuracy, 40.7\% in Precision, 59.6\% in Recall, and 54.0\% in the F1 score, which suggests the robustness of our approach to missing observations in time series.


\subsection{Ablation Studies}\label{sect:ablation}
To evaluate the effectiveness of our proposed design, we conduct ablation studies to answer the following questions.

\textbf{How do backbone vision models affect the performance?~}
We first tested the performance of different backbone vision models under our framework. We tried another popular vision transformer ViT. For a fair comparison with Swin Transformer, we use the checkpoint pre-trained on ImageNet-21k dataset\footnote{https://huggingface.co/google/vit-base-patch16-224-21k}. We also tested a CNN-based model ResNet\footnote{https://huggingface.co/microsoft/resnet-50}.
In addition, we report the performance of Swin Transformer trained from scratch and Raindrop for comparison. The results are presented in Fig.~\ref{fig:ablation1}. 
The pre-trained ViT performs similarly to Swin Transformer. They both outperform the previous state-of-the-art method Raindrop, which suggests the effectiveness of our proposed framework that utilizes vision transformers for time series modeling. However, the CNN-based ResNet achieves much worse performance than the transformer-based models Swin Transformer and ViT, showing that our framework's superior performance derives not only from the idea of casting time series classification to image classification but also the strong image recognition ability of vision transformers. Swin Transformer trained from scratch underperforms its pre-trained counterpart by a large margin, which shows that knowledge obtained from pre-training on natural images could contribute to recognizing patterns in synthetic time series line graph images. It also reveals the advantages of our proposed framework: pre-trained vision models can be easily leveraged for time series modeling.



\textbf{How to create the time series line graph images?~}
As mentioned in Section~\ref{sect:line_graph}, there are several key designs in drawing the line graphs for irregularly sampled multivariate time series and creating the images: (1) the linear \textit{interpolation}, \textit{i.e.}, linking the consecutive observed data points on the line graphs; (2) \textit{markers} for observed data points to distinguish them from the ``interpolated'' ones on the line graph; (3) variable-specific line \textit{colors} to distinguish different line graphs.
% (4) the order of line graphs according to missing ratios. 
We conducted ablation studies to test their effectiveness. The results are presented in Table~\ref{tab:ablation_1}. 
% Note that the missing ratios of each variable in the PAM dataset are approximately balanced, so we didn't test the performance without order on it. 
We can see that the performance decreases without either of these designs. However, the performance drop of removing interpolation and markers are not as significant as removing variable-specific line colors, which is reasonable as it is most observable on the line graph images and distinguishes different line graphs. We defer more details on time series line graph image creation to Appendix~\ref{sect:image_creation}.


\begin{figure}[!t]
    \centering
    \includegraphics[width=1\linewidth]{./pic/visualization2.pdf}
    \caption{Illustration of the averaged attention map of ViTST on three images from P19, P12, and PAM datasets, respectively. Left: input images. Right: attention maps.}
    \label{fig:attention_map}
    \vspace{-4mm}
\end{figure}

\textbf{What patterns does ViTST capture?~}
To understand what patterns ViTST capture in the time series line graph images, we presented the averaged attention map of a ViTST with ViT as the backbone model in Fig.~\ref{fig:attention_map}. The model learns to attend to the lines instead of the whitespace. In addition, we observe that the model correctly focuses on observed data points (dots) and changing slopes on the lines, which indicates the observation and trend information. Some flat line graphs which don't reflect many dynamic patterns receive less attention.


\subsection{Regular Time Series Classification}\label{sect:regular}
The advantage of our approach is that it can be used to model any shape of time series, whether it is regular or not. We thus also tested the performance of our approach on regular time series data.  
We selected seven representative regular multivariate time series datasets from the UEA Time Series Classification Archive~\cite{uea}, which have diverse characteristics in terms of the number of classes, variables, and time series length.
We follow \cite{tst} to use these baselines for comparison: DTW$_D$ which stands for dimension-Dependent DTW combined with dilation-CNN~\cite{franceschi2019unsupervised}, LSTM~\cite{lstm}, XGBoost~\cite{xgboost}, Rocket~\cite{rocket}, and a transformer-based TST~\cite{tst} which operates on fully observed numerical time series data. 
% We thus use the methods designed for regular time series as baselines.


\begin{table}[!h]
    \footnotesize
    \centering
    \small
    \caption{Performance comparison on regular time series datasets. 
    % ``Avg. Acc.'' denotes the average accuracy on all datasets. 
    \textbf{Bold} indicates the best performer, while \underline{underline} represents the second best. }
    \scalebox{0.88}{
    \begin{tabular}{lccccc|c}
    \toprule
    Datasets &DTW$_D$ &LSTM &XGBoost &Rocket &TST & ViTST \\\midrule
    EC &0.323 &0.323 &0.437 &\underline{0.452} &0.326 &\textbf{0.456} \\
    UW &0.903 &0.412 &0.759 &\textbf{0.944} &\underline{0.913} &0.862 \\
    SCP1 &0.775 &0.689 &0.846 &\underline{0.908} &\textbf{0.922} &0.898 \\
    SCP2 &0.539 &0.466 &0.489 &0.533 &\textbf{0.604}&\underline{0.561} \\
    JV &0.949 &0.797 &0.865 &\underline{0.962} &\textbf{0.997} &0.946 \\
    SAD &0.963 &0.319 &0.696 &0.712 &\textbf{0.998}&\underline{0.985} \\
    HB &0.717 &0.722 &0.732 &0.756 &\textbf{0.776} &\underline{0.766} \\\midrule
    Avg. &0.738 &0.533 &0.689 &0.703 &\textbf{0.791} &\underline{0.782}\\
    \bottomrule
    \end{tabular}}
    \label{tab:regular_result}
    \vspace{-4mm}
\end{table}

The performance comparisons are shown in Table~\ref{tab:regular_result}. Our approach performs consistently well on these seven datasets with different characteristics. Its average accuracy is second-best and close to the best-performed baseline method TST. By contrast, on the irregularly sampled time series datasets, ViTST outperforms Transformer (Trans-mean) as discussed in Sect~\ref{sect:experiment}.
It should be noted that most of the algorithms on regular and irregular time series are studied separately and could not handle the other type of time series well. 
However, our approach achieves promising results on both regular and irregular time series, showing its superiority in generality and effectiveness.

\begin{figure}[!t]
    \centering
    \includegraphics[width=0.95\linewidth]{./pic/mim.pdf}
    \caption{Illustration of the masking area with a mask ratio of 0.5 in our mask image modeling exploration on time series line graph images. The model is trained to reconstruct the masking areas.}
    \label{fig:mim}
    \vspace{-4mm}
\end{figure}

\subsection{Self-supervised Learning}
Self-supervised learning has become a popular approach to learning representation from unlabelled data, which can benefit various downstream tasks. Masked language modeling (MLM)~\cite{bert} and masked image modeling (MIM)~\cite{mim,mae} have been dominant self-supervised approaches in NLP and CV domains. We also perform a preliminary exploration on the \textit{masked image modeling} on time series line graph images: we mask a portion of patches in the line graph images and train the model to recover them. 

As shown in Fig.~\ref{fig:mim}, we randomly mask several columns of the line graphs in the images. In this way, we can ensure that some regions containing line graphs are masked instead of all the empty places. A one-layer prediction head is applied on the vision transformer encoder to reconstruct the pixels of masked patches with $\ell_1$ loss, \textit{i.e.,} predicting the missing parts on the line graphs. With self-supervised pre-training on the largest P19 dataset with 38803 samples, Our approach further improves AUPRC from 52.8 ($\pm$ 3.8) to 53.8 ($\pm$ 3.2). However, AUROC points (\%) slightly dropped from 89.4 ($\pm$ 1.9) to 88.9 ($\pm$ 2.1), which is within a standard deviation. More details are given in Appendix~\ref{sect:mim}. 
% Note that we did not perform extensive hyperparameter search.  Further explorations remain as future work. 
Note that we did not perform an extensive hyperparameter search in the preliminary explorations. We believe this is worth further explorations, which we leave for future work.



\section{Conclusion}
In this paper, we introduced a new perspective for multivariate time series modeling by transforming them into images, which enables the use of powerful vision transformers.
This approach is simple and general since any type of time series can be transformed into line graph images and handled. Despite its simplicity, our approach demonstrates strong performance against highly specialized state-of-the-art methods on several popular datasets and shows robustness to missing observations. We also evaluate our approach on regular time series and witness promising results. 
We believe our approach can be potentially extended as a general-purpose framework for
various time series tasks and encourage the reuse of fast-evolving computer vision techniques in the time series modeling domain.

% Our approach can be further extended as a general-purpose framework for various time series tasks. In addition, our introduced new perspective makes it possible to consider various fast-evolving computer vision techniques in the time series modeling domain, which are also worth further exploration.

% We believe this paper could provide a new perspective for time series analysis and encourage the reuse of fast-evolving computer vision techniques in the time series domain. 
% For example, more powerful models~\cite{liu2022swin}, data augmentation~\cite{shorten2019survey}, interpretability~\cite{chefer2021transformer}, self-supervised learning~\cite{chen2021empirical}, and so on.
% not only work in time series classification tasks, but can also be applied to other tasks such as time series forecasting, where the problem is cast as image completion task, which has also been well studied in the computer vision community. In the future, we hope to extend our framework as a generic and unified interface for solving time series tasks using mature computer vision techniques.


% \subsubsection*{Acknowledgments}
% Use unnumbered third level headings for the acknowledgments. All
% acknowledgments, including those to funding agencies, go at the end of the paper.


% In the unusual situation where you want a paper to appear in the
% references without citing it in the main text, use \nocite
\nocite{langley00}

\bibliography{example_paper}
\bibliographystyle{icml2023}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn


\section{More Details on Time Series Line Graph Image Creation}\label{sect:image_creation}
% \subsection{Implementation}
\textbf{Implementation.~}
The time-series-to-image transformation can be implemented using the Matplotlib package\footnote{https://matplotlib.org/} with the following few lines of code.
% Some examples of created images with different strategies for the three datasets are shown in Fig.~\ref{fig:created_images}. 
\begin{figure}[!hb]
\vspace{-2mm}
\centering
{\footnotesize
\begin{minted}[fontsize=\small,linenos]{python}
def TS2Image(t, v, D, colors, image_height, image_width, grid_height, grid_width):
    import matplotlib.pyplot as plt
    plt.figure(figsize=(image_height/100, image_width/100), dpi=100)
    for d in range(D): # enumerate the multiple variables
        plt.subplot(grid_height, grid_width, d+1) # position in the grid
        # plot line graph of variable d
        plt.plot(t[d], v[d], color=colors[d], linestyle="-", marker="*") 
\end{minted}
}
\vspace{-2mm}
% \caption{Implementation of time-series-to-image transformation in Python using Matplotlib package.}
\label{fig:code}
\end{figure}

As mentioned in Section~\ref{sect:line_graph}, the observations of each variable are plotted in a separate line graph. We use straight lines ``--'' to connect consecutive points on the line graph. To distinguish the observed data points from the line, we use markers ``*''. To differentiate these line graphs, we use variable-specific colors for each line graph. Besides, we have tried different strategies from the following perspectives when creating the images.

% To better reflect the dynamic and missing patterns of each time series and distinguish different time series, we tried several ways to create time series line graph images in terms of the following aspects.

% \noindent\textbf{Missing values.~} There are missing values in the time series data. When plotting the data, we remove the points with no observations and use straight lines to connect the points, which is the same as linear interpolation. In this case, the plotted line graph is continuous. We use markers to distinguish the observed data points from the ``interpolated'' dots in the line.  We also tried to mask the points without observed values, in which case only the observed data points will be plotted, and thus, there will be gaps in the plotted line graph. We denote this strategy as ``GAP''.

\noindent\textbf{Axis Limits of Line Graphs.~} The axis limits determine the plot area of the line graphs and the range of displayed timestamps and values. The default strategy is to directly set the limits of the x-axis and y-axis as the ranges of all the observed timestamps and values across the dataset. However, we notice that there exist some extreme observed values for some variables, making the range of the y-axis very large. As a consequence, most plotted points of observations clusters in a small area, and the drawn line graphs are flat (see Fig.~\ref{fig:created_images}). 
Note that the widely used linear normalization and standardization methods will not make a difference on the created images, as the relative magnitudes keep unchanged.
We thus tried the following strategies to remove extreme values and narrow the range of the y-axis:
\begin{itemize}
    \item{Interquartile Range (IQR)}: IQR is one of the most extensively used methods for outlier detection and removal. The interquartile range is calculated based on the first and third quartiles of all the observed values of each variable in the dataset and then used to calculate the upper and lower limits.
    \item{Standard Deviation (SD)}: The upper and lower boundaries are calculated by taking 3 standard deviations from the mean of observed values for each variable across the dataset. This method usually assumes the data is normally distributed.
    \item{Modified Z-score (MZ)}: A z-score measures how many standard deviations away a value is from the mean and is similar to the standard deviation method to detect outliers. However, z-scores can be influenced by extreme values, which modified z-scores can better handle. We set the upper and lower limits as the values whose modified z-scores are 3.5 and -3.5.
\end{itemize}

\begin{table}[htbp]
\scriptsize
\centering
\caption{Ablation study on different strategies to decide the line graph limit. The default strategy is directly set the axis limit as the range of all observed values on the dataset. ``IQR'', ``SD'', and ``MZS' denote three strategies to remove extreme value, \textit{i.e.,} Interqurtile Range, Standard Deviation, and Modified Z-score. The numbers are reported as \% and averaged on 5 data splits.}
% \vspace{-2mm}
\label{tab:ablation_2}
\resizebox{\textwidth}{!}{ 
\begin{tabular}{l|cc|cc|cccc}
\toprule
& \multicolumn{2}{c|}{P19} & \multicolumn{2}{c|}{P12} & \multicolumn{4}{c}{PAM} \\ \cmidrule{2-9}
\multirow{-2}{*}{Strategies} & AUROC & AUPRC & AUROC & AUPRC & Accuracy & Precision & Recall & F1 score \\ \midrule
Default &\textbf{89.4} $\pm$ 1.9 &\textbf{52.8} $\pm$ 3.8 &\textbf{85.6} $\pm$ 1.1 & \textbf{49.8} $\pm$ 2.5 & 96.1 $\pm$ 0.7 & 96.8 $\pm$ 1.1 & 96.5 $\pm$ 0.7 & 96.6 $\pm$ 0.9\\ \midrule
IQR & 88.2 $\pm$ 0.8 & 49.6 $\pm$ 1.7 & 84.5 $\pm$ 1.1 & 48.9 $\pm$ 2.6 & 95.9 $\pm$ 0.7 & 96.8 $\pm$ 0.7 & 96.1 $\pm$ 0.7 & 96.4 $\pm$ 0.7\\
SD & 87.4 $\pm$ 1.6 & 51.2 $\pm$ 3.6 & 84.6 $\pm$ 1.7 & 47.1 $\pm$ 2.9 & \textbf{96.6} $\pm$ 0.9 & \textbf{97.1} $\pm$ 0.8 & \textbf{97.0} $\pm$ 0.6 & \textbf{97.0} $\pm$ 0.7  \\
MZS & 87.3 $\pm$ 1.0 & 50.8 $\pm$ 3.7 & 84.3 $\pm$ 1.4 & 47.1 $\pm$ 2.1 & 96.0 $\pm$ 1.1 & 96.8 $\pm$ 0.9 & 96.4 $\pm$ 0.9 & 96.6 $\pm$ 0.9\\
\bottomrule
\end{tabular}
}
% \vspace{-2mm}
\end{table}

Using these methods to narrow the range of y-axis means some extreme values will be removed and not displayed in the plotted line graph. The comparison of model performance trained on images created with different strategies is shown in Table~\ref{tab:ablation_2}. We observe that these three methods that remove extreme values hurt the performance, except SD on PAM dataset. Although they narrow the value range and highlight the dynamic patterns of line graphs, they discard the extreme values which might be informative themselves. This observation suggests that our approach might not need data preprocessing on the time series, which further proves its advantage in simplicity.


\begin{figure}[htbp]
	\centering
	
	\subfigure{
        \rotatebox{90}{\scriptsize{~~~~~~~~~~~~~~~~~~~~~~~~~Default}}
		\begin{minipage}[t]{0.3\linewidth}
			\centering
			\includegraphics[width=0.95\linewidth]{pic/p19/p000019.png}
		\end{minipage}
	}
	\subfigure{
		\begin{minipage}[t]{0.3\linewidth}
			\centering
			\includegraphics[width=0.95\linewidth]{pic/p12/132584.png}
		\end{minipage}
	}
	\subfigure{
		\begin{minipage}[t]{0.3\linewidth}
			\centering
			\includegraphics[width=0.95\linewidth]{pic/pam/0.png}
		\end{minipage}
	}
	
	\vspace{-2mm}
	\setcounter{subfigure}{0}

 %    	\subfigure{
 %        \rotatebox{90}{\scriptsize{~~~~~~~~~~~~~~~~~~~~~~~~~~~w/o interpolation}}
	% 	\begin{minipage}[t]{0.3\linewidth}
	% 		\centering
	% 		\includegraphics[width=0.95\linewidth]{pic/p19/p000019_removal.png}
	% 	\end{minipage}
	% }
	% \subfigure{
	% 	\begin{minipage}[t]{0.3\linewidth}
	% 		\centering
	% 		\includegraphics[width=0.95\linewidth]{pic/p12/132584_removal.png}
	% 	\end{minipage}
	% }
	% \subfigure{
	% 	\begin{minipage}[t]{0.3\linewidth}
	% 		\centering
	% 		\includegraphics[width=0.95\linewidth]{pic/pam/0_removal.png}
	% 	\end{minipage}
	% }
	
	% \vspace{-2mm}
	% \setcounter{subfigure}{0}

    	\subfigure{
        \rotatebox{90}{\scriptsize{~~~~~~~~~~~~~~~~~~~~~~~~~~~IQR}}
		\begin{minipage}[t]{0.3\linewidth}
			\centering
			\includegraphics[width=0.95\linewidth]{pic/p19/p000019_iqr.png}
		\end{minipage}
	}
	\subfigure{
		\begin{minipage}[t]{0.3\linewidth}
			\centering
			\includegraphics[width=0.95\linewidth]{pic/p12/132584_iqr.png}
		\end{minipage}
	}
	\subfigure{
		\begin{minipage}[t]{0.3\linewidth}
			\centering
			\includegraphics[width=0.95\linewidth]{pic/pam/0_iqr.png}
		\end{minipage}
	}
	
	\vspace{-2mm}
	\setcounter{subfigure}{0}

    	\subfigure{
        \rotatebox{90}{\scriptsize{~~~~~~~~~~~~~~Standard Deviation}}
		\begin{minipage}[t]{0.3\linewidth}
			\centering
			\includegraphics[width=0.95\linewidth]{pic/p19/p000019_sd.png}
		\end{minipage}
	}
	\subfigure{
		\begin{minipage}[t]{0.3\linewidth}
			\centering
			\includegraphics[width=0.95\linewidth]{pic/p12/132584_sd.png}
		\end{minipage}
	}
	\subfigure{
		\begin{minipage}[t]{0.3\linewidth}
			\centering
			\includegraphics[width=0.95\linewidth]{pic/pam/0_sd.png}
		\end{minipage}
	}
	
	\vspace{-2mm}
	\setcounter{subfigure}{0}
 
    \subfigure[P19]{
        \rotatebox{90}{\scriptsize{~~~~~~~~~~~~~~~~~Modified Z-score}}
		\begin{minipage}[t]{0.3\linewidth}
			\centering
			\includegraphics[width=0.95\linewidth]{pic/p19/p000019_mzs.png}
		\end{minipage}
	}
	\subfigure[P12]{
		\begin{minipage}[t]{0.3\linewidth}
			\centering
			\includegraphics[width=0.95\linewidth]{pic/p12/132584_mzs.png}
		\end{minipage}
	}
	\subfigure[PAM]{
		\begin{minipage}[t]{0.3\linewidth}
			\centering
			\includegraphics[width=0.95\linewidth]{pic/pam/0_mzs.png}
		\end{minipage}
	}
	\caption{The images created with different strategies for three samples from P19, P12, and PAM dataset, respectively (sample ``p000019'' for P19, ``132548'' for P12, and ``0'' for PAM).}
	\label{fig:created_images}
\end{figure}

\textbf{Grid Layout and Image Size.~}
We conducted experiments to investigate the influence of grid layouts and image sizes on performance. Specifically, for a fair comparison of different grid layouts, we fixed the size of each grid cell as $64\times64$ and alter the grid layouts. The results on P19, P12, and PAM datasets are listed in Table~\ref{tab:grid_p19}, Table~\ref{tab:grid_p12}, and Table~\ref{tab:grid_pam}, respectively. As can be seen, the square grid layouts achieve consistently good results on three datasets. We conjecture that this is because the square layout ensures that the distance between any two line graphs is shortest. We also tried directly setting the image size as standard $224\times224$, and found the performance differences are marginal, showing that our method is robust to various image sizes.

\begin{table*}[!t]
\begin{minipage}[t]{0.5\textwidth}
  \renewcommand{\arraystretch}{1.3}
  \caption{Ablation study on grid layouts and image sizes on P19.}
  \label{tab:grid_p19}
  \centering
  \scalebox{0.9}{
  \setlength{\tabcolsep}{1.2mm}{
  \begin{tabular}{cc|ll}
    \toprule
    Grid Layout & Image Size & AUROC & AUPRC \\ \midrule
    $4 \times 9$ & $256 \times 576$ & 87.4 $\pm$ 1.9 & 48.1 $\pm$ 4.5  \\
    $5 \times 7$ & $320 \times 448$ & 87.9 $\pm$ 1.9 & 49.6 $\pm$ 2.7 \\
    $6 \times 6$ & $384 \times 384$ & \textbf{89.4} $\pm$ 1.9 & \textbf{52.8} $\pm$ 3.8 \\\midrule
    $6 \times 6$ & $224 \times 224$ & 88.7 $\pm$ 1.4 & 52.3 $\pm$ 0.6 \\
    \bottomrule
    \end{tabular}}}
  \end{minipage}
\begin{minipage}[t]{0.5\textwidth}
  \renewcommand{\arraystretch}{1.3}
  \caption{Ablation study on grid layouts and image sizes on P12.}
  \label{tab:grid_p12}
  \centering
  \scalebox{0.9}{
  \setlength{\tabcolsep}{1.2mm}{
  \begin{tabular}{cc|ll}
    \toprule
    Grid Layout & Image Size & AUROC & AUPRC \\ \midrule
    $4 \times 9$ & $256 \times 576$ & 84.0 $\pm$ 1.4 & 47.9 $\pm$ 2.6  \\
    $5 \times 8$ & $320 \times 512$ & 84.1 $\pm$ 1.6 & 47.2 $\pm$ 2.3 \\
    $6 \times 6$ & $384 \times 384$ & \textbf{85.6} $\pm$ 1.1 & \textbf{49.8} $\pm$ 2.5 \\\midrule
    $6 \times 6$ & $224 \times 224$ & 85.2 $\pm$ 2.1 & 48.8 $\pm$ 3.7 \\
    \bottomrule
    \end{tabular}}}
  \end{minipage}
  
\begin{minipage}[t]{1.0\textwidth}
  \renewcommand{\arraystretch}{1.3}
  \caption{Ablation study on grid layouts and image sizes on PAM.}
  \label{tab:grid_pam}
  \centering
  \scalebox{0.9}{
  \setlength{\tabcolsep}{1.2mm}{
  \begin{tabular}{cc|llll}
    \toprule
    Grid Layout & Image Size & Accuracy & Precision & Recall & F1 score \\ \midrule
    $2 \times 9$ & $128 \times 576$ & 95.9 $\pm$ 1.4 & 96.5 $\pm$ 1.0 & \textbf{95.9} $\pm$ 1.2 & 96.0 $\pm$ 0.5  \\
    $3 \times 6$ & $192 \times 384$ & \textbf{96.1} $\pm$ 0.8 & 96.7 $\pm$ 0.5 & \textbf{95.9} $\pm$ 0.9 & 96.2 $\pm $0.7 \\
    $4 \times 5$ & $256 \times 320$ & \textbf{96.1} $\pm$ 0.7 & \textbf{96.8} $\pm$ 1.1 & 96.5 $\pm$ 0.7 & \textbf{96.6} $\pm$ 0.9 \\\midrule
    $4 \times 5$ & $224 \times 224$ & 95.9 $\pm$ 0.6 & 96.7 $\pm$ 0.8 & \textbf{95.9} $\pm$ 0.6 & 96.3 $\pm$ 0.7 \\
    \bottomrule
    \end{tabular}}}
  \end{minipage}
\vspace{-4mm}
\end{table*}


\begin{table}[!h]
\scriptsize
\centering
\caption{Ablation studies on variables orders on P19 and P12 datasets.
% The reported numbers are averaged on 5 data splits. 
% We omit variances for simplicity. The numbers in brackets denote the performance change \textit{w.r.t.} the full model.
}
\label{tab:order}
\resizebox{0.55\textwidth}{!}{ 
\begin{tabular}{l|cc|cc}
\toprule
& \multicolumn{2}{c|}{P19} & \multicolumn{2}{c}{P12} \\ \cmidrule{2-5}
\multirow{-2}{*}{Sorted} & AUROC & AUPRC & AUROC & AUPRC \\ \midrule
$\times$ & 88.3 $\pm$ 1.5 & 51.3 $\pm$ 3.0 & 85.6 $\pm$ 1.1 & 49.8 $\pm$ 2.5  \\
\checkmark & 89.4 $\pm$ 1.9 & 52.8 $\pm$ 3.8 & 84.9 $\pm$ 1.8 & 49.8 $\pm$ 4.2 \\
\bottomrule
\end{tabular}
}
\end{table}

\textbf{Order of Line Graphs.}
We place the line graphs of each variable in a pre-defined grid layout to form the image. If the grid is not full, the empty cells at the end will be left blank. We empirically found that ViTST-Swin will learn to pay less attention to this empty region. We thus sort variables according to their missing ratios on the whole training set and keep the order fixed across different samples, ensuring the sparse line graphs of variables with few observations are placed at the end, next to the padded empty cells. We hypothesize such an order may facilitate the model to ignore the line graphs with the least observations instead of the informative line graphs with more observations if there exists the ignorance bias caused by padded empty cells. 
% least informative ones with  the least stay close and the least informative 
To verify this, we trained a ViTST model on P19 and P12 dataset with random and sorted variable orders. The missing ratios of variables in PAM dataset are balanced, so we didn't test it.
As shown in Table~\ref{tab:order}, images from the P19 dataset have 2 empty cells at the end ($6\times6$ grid for 34 variables), and the sorted order produces better results than random order. By contrast, the grid is full in images from the P12 dataset ($6\times6$ grid for 36 variables), and there is no significant performance difference between these two orders, which confirms our hypothesis.


% We also sort variables according to their missing ratios on the training set and fix the order across different samples, placing the most sparse line graphs at the end, next to the padded empty cells.



\begin{table}[!h]
\centering
\caption{Templates for transforming static features to natural language sentences.}
\label{tab:dataset_template}
\footnotesize
% \addtolength{\tabcolsep}{-0.65ex}
\begin{tabular}{c|p{1.5in}|p{2.2in}|p{1.5in}} \toprule
Dataset & Static features & Template & Example \\ \hline
\multirow{4}{*}{P19} & \textit{Age}, \textit{Gender}, \textit{Unit1 (medical ICU)}, \textit{Unit2 (surgery ICU)}, \textit{HospAdmTime}; \textit{ICULOS (ICU length-of-stay)} & A patient is \{\textit{Age}\} years old, \{\textit{Gender}\}, went to \{\textit{Unit1\&Unit2}\} \{\textit{HospAdmTime}\} hours after hospital admit, had stayed there for \{\textit{ICULOS}\} hours. & A patient is 65 years old, female, went to the medical ICU 10 hours after hospital admit, had stayed there for 20 hours. \\ \cline{1-4}
\multirow{4}{*}{P12} & \textit{RecordID}, \textit{Age}, \textit{Gender}, \textit{Height} (cm), \textit{ICUType}, \textit{Weight} (kg) & A patient is \{\textit{Age}\} years old, \{\textit{Gender}\}, \{\textit{Height}\} cm, \{\textit{Weight}\} kg, stayed in \{\textit{ICUType}\}. & A patient is 48 years old, male, 171 cm, 78 kg, stayed in surgical ICU. \\ 
\bottomrule
\end{tabular}
\end{table}


\section{More Experimental Details}\label{sect:experiment_details}


\subsection{Datasets}\label{sect:dataset}
We used the datasets processed by \citep{raindrop}, whose details are given below.

\textbf{P19: PhysioNet Sepsis Early Prediction Challenge 2019.~\footnote{\url{https://physionet.org/content/challenge-2019/1.0.0/}}}
P19 dataset~\citep{p19} contains the clinical data of 38,803 patients, and the goal is to predict the occurrence of sepsis within the next 6 hours. Each patient is monitored by 34 irregularly sampled sensors with 8 vital signs and 26 laboratory values. 6 demographic information are also provided. We transform these static features using templates as shown in Table~\ref{tab:dataset_template}, and utilize the pre-trained Roberta-base to obtain textual features, which are concatenated with the visual feature from the vision transformer to perform classification. This is a binary classification task, and the dataset is highly imbalanced with around 4\% positive samples. The missing ratio is 94.9\%.

\textbf{P12: PhysioNet Mortality Prediction Challenge 2012.~\footnote{\url{https://physionet.org/content/challenge-2012/1.0.0/}}}
P12 dataset~\citep{p12} includes the clinical data of 11,988 ICU patients (12 inappropriate patient samples are removed). 36 irregularly sampled sensor observations and 6 static demographic features of each patient are provided. The goal is to predict the mortality of patients (binary classification). This dataset is also highly imbalanced with around 7\% negative samples. The missing ratio is 88.4\%.

\textbf{PAM: PAMAP2 Physical Activity Monitoring.~\footnote{\url{https://archive.ics.uci.edu/ml/datasets/pamap2+physical+activity+monitoring}}}
The original PAM dataset contains data of 18 physical activities with 9 subjects wearing 3 inertial measurement units. To make it suitable for irregular time series classification, \cite{raindrop} excluded the ninth subject due to its short length of sensor readouts. The continuous signals are segmented with a time window size of 600 and an overlapping rate of 50\%. 10 out of the 18 activities in the original dataset are excluded as they are associated with less than 500 samples, and 8 activities remain. Therefore, the task is an 8-way classification. Finally, there are 5,333 samples, each with 600 continuous observations. To simulate the irregular time series setting, 60\% of the observations are randomly removed. No static features are provided, and the 8 categories are approximately balanced. The missing ratio is 60.0\%.

\begin{table}[!hb]
    \caption{The statistics and hyperparameter settings of the evaluated regular multivariate time series datasets. }
    \centering
    \small
    \scalebox{1.0}{
    \begin{tabular}{lcccccccc}
    \toprule
    Datasets & EC & UW & SCP1 & SCP2 & JV & SAD & HB \\ \midrule
    \#Variables & 3 & 3 & 6 & 7 & 12 & 13 & 61 \\
    \#Classes & 4 & 8 & 2 & 2 &9 & 10 & 2 \\
    Length & 1,751 & 315 & 896 & 1,152 & 29 & 93 & 405 \\ 
    Train size &261 &120 &268 &200 &270 &6599 &204 \\ \midrule
    Grid layout & $2\times2$ & $2\times2$ & $2\times3$ & $3\times3$ & $4\times4$ & $4\times4$ &$4\times4$\\
    Image size & $256\times256$ & $256\times256$ & $256\times384$ & $384\times384$ & $384\times384$ & $384\times384$ & $384\times384$\\
    Learning rate &1e-4 &1e-4 &1e-4 &5e-5 &1e-4 &1e-5 &1e-4 \\
    Epochs &20 &100 &100 &100 &100 &20 &100 \\
    \bottomrule
    \end{tabular}}
    \label{tab:regular_dataset}
    \vspace{-2mm}
\end{table}


\subsection{Experiment on Regular Time Series}
We selected seven representative regular multivariate time series datasets from the UEA Time Series Classification Archive~\cite{uea}: EthanolConcentration (EC), Handwriting (HW), UWaveGestureLibrary (UW), SelfRegulationSCP1 (SCP1), SelfRegulationSCP2 (SCP2), JapaneseVowels (JV), SpokenArabicDigits (SAD), and Heartbeat (HB). As shown in Table~\ref{tab:regular_dataset}, these seven datasets have diverse characteristics in terms of numbers of classes, variables, and time series length. The hyperparameter settings are also provided in the table. We set different image sizes according to their grid layouts for these datasets. One can also uniformly set a size for all the datasets, such as $384\times384$. We applied the cutout data augmentation methods to SCP1, SCP2, and JV datasets because of the small size of their training set. 

\subsection{Self-supervised Learning}\label{sect:mim}
We preliminary explored masked image modeling self-supervised pre-training on the time series line graph images. We randomly mask columns of patches with a width of 32 on each line graph within a grid cell. The masking ratio is set as 50\%. Since the self-supervised pre-training usually requires a large amount of unlabelled data, we experimented on the largest dataset P19, which has 38803 samples. We trained the Swin Transformer model for 10 epochs with batch size 48. The learning rate is 2e-5. Following~\cite{mim}, we use a linear layer to reconstruct the pixel values and employ an $\ell_1$ loss on the masked pixels:
\begin{equation}
\mathcal{L}=\frac{1}{\Omega(\bf{p}_{\text{M}})}\left \| \hat{\bf{p}_{\text{M}}} - \bf{p}_{\text{M}} \right \|_1,
\end{equation}
where $\bf{p}_{\text{M}}$ and $\hat{\bf{p}_{\text{M}}}$ are the masked and reconstructed pixels, respectively; $\Omega(\cdot)$ denotes the number of elements.

After self-supervised pre-training, we further fine-tuned the model on the classification task with the same setting as directly fine-tuning: 2 epochs on the upsampled dataset with a learning rate of 2e-5. The performance improved by 1.0 AUPRC points (\%) from 52.8 ($\pm$ 3.8) to 53.8 ($\pm$ 3.2). The AUROC points (\%) slightly dropped 0.5 points: from 89.4 ($\pm$ 1.9) to 88.9 ($\pm$ 2.1). We also experimented with P12 and PAM datasets. However, there are no significant differences, which might be because of their relatively small dataset sizes (11988 for P12 and 5333 for the PAM dataset).
% Note that we did not perform extensive hyperparameter search in the preliminary explorations. We believe this is worth further explorations, which we leave for future work.

% With self-supervised masked image modeling, the performance improves 1.0 AUPRC points from 52.8 ($\pm$ 3.8) to 53.8 ($\pm$ 3.2). The AUROC points slightly dropped from 89.4 ($\pm$ 1.9) to 88.9 ($\pm$ 2.1).


\subsection{Full Experimental Results}\label{sect:full_results}
We presented the full experimental results in the leave-sensors-out settings in Table~\ref{tab:leave-sensors-out}, and the full results of ablation studies on backbone vision models as illustrated in Fig.~\ref{fig:ablation1} are presented in Table~\ref{tab:vision_models}.

\begin{table*}[!h]
\scriptsize
\centering
\caption{Full results (\%) in the leave-sensors-out settings on PAM dataset. The ``missing ratio'' denotes the ratio of masked variables.}
\resizebox{\textwidth}{!}{
\begin{tabular}{cl|llll|llll}
\toprule
\multicolumn{1}{c}{\multirow{2}{*}{\begin{tabular}[c]{@{}l@{}}Missing \\ ratio\end{tabular}}}  & \multirow{2}{*}{Methods} & \multicolumn{4}{c|}{PAM (Leave-\textbf{fixed}-sensors-out)} & \multicolumn{4}{c}{PAM (Leave-\textbf{random}-sensors-out)} \\ \cmidrule{3-10}
\multicolumn{1}{l}{} &  & Accuracy & Precision & Recall & F1 score & Accuracy & Precision & Recall & F1 score \\ \midrule
\multirow{7}{*}{10\%} & Transformer & 60.3 $\pm$ 2.4 & 57.8 $\pm$ 9.3 & 59.8 $\pm$ 5.4 & 57.2 $\pm$ 8.0 & 60.9 $\pm$ 12.8 & 58.4 $\pm$ 18.4 & 59.1 $\pm$ 16.2 & 56.9 $\pm$ 18.9 \\
 & Trans-mean & 60.4 $\pm$ 11.2&	61.8 $\pm$ 14.9&	60.2 $\pm$ 13.8&	58.0 $\pm$ 15.2&	62.4 $\pm$ 3.5&	59.6 $\pm$ 7.2&	63.7 $\pm$ 8.1&	62.7 $\pm$ 6.4 \\
 & GRU-D & 65.4 $\pm$ 1.7 & 72.6 $\pm$ 2.6 & 64.3 $\pm$ 5.3 & 63.6 $\pm$ 0.4 & 68.4 $\pm$ 3.7 & 74.2 $\pm$ 3.0 & 70.8 $\pm$ 4.2 & 72.0 $\pm$ 3.7 \\
 & SeFT & 58.9 $\pm$ 2.3 & 62.5 $\pm$ 1.8 & 59.6 $\pm$ 2.6 & 59.6 $\pm$ 2.6 & 40.0 $\pm$ 1.9 & 40.8 $\pm$ 3.2 & 41.0 $\pm$ 0.7 & 39.9 $\pm$ 1.5 \\
 & mTAND & 58.8 $\pm$ 2.7 & 59.5 $\pm$ 5.3 & 64.4 $\pm$ 2.9 & 61.8 $\pm$ 4.1 & 53.4 $\pm$ 2.0 & 54.8 $\pm$ 2.7 & 57.0 $\pm$ 1.9 & 55.9 $\pm$ 2.2 \\
 & Raindrop & 77.2 $\pm$ 2.1 & 82.3 $\pm$ 1.1 & 78.4 $\pm$ 1.9 & 75.2 $\pm$ 3.1 & 76.7 $\pm$ 1.8 & 79.9 $\pm$ 1.7 & 77.9 $\pm$ 2.3 & 78.6 $\pm$ 1.8 \\ \cmidrule{2-10}
 & \textbf{ViTST}  & \textbf{92.7} $\pm$ 0.9 & \textbf{94.2} $\pm$ 0.9 & \textbf{93.2} $\pm$ 0.4 & \textbf{93.6} $\pm$ 0.6 & \textbf{88.4} $\pm$ 1.4 & \textbf{92.3} $\pm$ 0.5 & \textbf{88.6} $\pm$ 1.9 & \textbf{89.8} $\pm$ 1.5 \\ \midrule
\multirow{7}{*}{20\%} & Transformer & 63.1 $\pm$ 7.6 & 71.1 $\pm$ 7.1 & 62.2 $\pm$ 8.2 & 63.2 $\pm$ 8.7 & 62.3 $\pm$ 11.5 & 65.9 $\pm$ 12.7 & 61.4 $\pm$ 13.9 & 61.8 $\pm$ 15.6 \\
 & Trans-mean & 61.2 $\pm$ 3.0&	74.2 $\pm$ 1.8&	63.5 $\pm$ 4.4&	64.1 $\pm$ 4.1&	56.8 $\pm$ 4.1&	59.4 $\pm$ 3.4&	53.2 $\pm$ 3.9&	55.3 $\pm$ 3.5 \\
 & GRU-D & 64.6 $\pm$ 1.8 & 73.3 $\pm$ 3.6 & 63.5 $\pm$ 4.6 & 64.8 $\pm$ 3.6 & 64.8 $\pm$ 0.4 & 69.8 $\pm$ 0.8 & 65.8 $\pm$ 0.5 & 67.2 $\pm$ 0.0 \\
 & SeFT & 35.7 $\pm$ 0.5 & 42.1 $\pm$ 4.8 & 38.1 $\pm$ 1.3 & 35.0 $\pm$ 2.2 & 34.2 $\pm$ 2.8 & 34.9 $\pm$ 5.2 & 34.6 $\pm$ 2.1 & 33.3 $\pm$ 2.7 \\
 & mTAND & 33.2 $\pm$ 5.0 & 36.9 $\pm$ 3.7 & 37.7 $\pm$ 3.7 & 37.3 $\pm$ 3.4 & 45.6 $\pm$ 1.6 & 49.2 $\pm$ 2.1 & 49.0 $\pm$ 1.6 & 49.0 $\pm$ 1.0 \\
 & Raindrop & 66.5 $\pm$ 4.0 & 72.0 $\pm$ 3.9 & 67.9 $\pm$ 5.8 & 65.1 $\pm$ 7.0 & 71.3 $\pm$ 2.5 & 75.8 $\pm$ 2.2 & 72.5 $\pm$ 2.0 & 73.4 $\pm$ 2.1 \\ \cmidrule{2-10}
 & \textbf{ViTST}  & \textbf{88.4} $\pm$ 1.0 & \textbf{90.4} $\pm$ 1.4 & \textbf{89.3} $\pm$ 0.8 & \textbf{89.7} $\pm$ 1.0 & \textbf{85.1} $\pm$ 1.2 & \textbf{91.1} $\pm$ 1.0 & \textbf{85.6} $\pm$ 1.0 & \textbf{87.0} $\pm$ 1.0 \\ \midrule
\multirow{5}{*}{30\%} & Transformer & 31.6 $\pm$ 10.0 & 26.4 $\pm$ 9.7 & 24.0 $\pm$ 10.0 & 19.0 $\pm$ 12.8 & 52.0 $\pm$ 11.9 & 55.2 $\pm$ 15.3 & 50.1 $\pm$ 13.3 & 48.4 $\pm$ 18.2 \\
 & Trans-mean &  42.5 $\pm$ 8.6&	45.3 $\pm$ 9.6&	37.0 $\pm$ 7.9&	33.9 $\pm$ 8.2&	65.1 $\pm$ 1.9&	63.8 $\pm$ 1.2&	67.9 $\pm$ 1.8&	64.9 $\pm$ 1.7\\
 & GRU-D & 45.1 $\pm$ 2.9 & 51.7 $\pm$ 6.2 & 42.1 $\pm$ 6.6 & 47.2 $\pm$ 3.9 & 58.0 $\pm$ 2.0 & 63.2 $\pm$ 1.7 & 58.2 $\pm$ 3.1 & 59.3 $\pm$ 3.5 \\
 & SeFT & 32.7 $\pm$ 2.3 & 27.9 $\pm$ 2.4 & 34.5 $\pm$ 3.0 & 28.0 $\pm$ 1.4 & 31.7 $\pm$ 1.5 & 31.0 $\pm$ 2.7 & 32.0 $\pm$ 1.2 & 28.0 $\pm$ 1.6 \\
 & mTAND & 27.5 $\pm$ 4.5 & 31.2 $\pm$ 7.3 & 30.6 $\pm$ 4.0 & 30.8 $\pm$ 5.6 & 34.7 $\pm$ 5.5 & 43.4 $\pm$ 4.0 & 36.3 $\pm$ 4.7 & 39.5 $\pm$ 4.4 \\
 & Raindrop & 52.4 $\pm$ 2.8 & 60.9 $\pm$ 3.8 & 51.3 $\pm$ 7.1 & 48.4 $\pm$ 1.8 & 60.3 $\pm$ 3.5 & 68.1 $\pm$ 3.1 & 60.3 $\pm$ 3.6 & 61.9 $\pm$ 3.9 \\ \cmidrule{2-10}
 & \textbf{ViTST}  & \textbf{84.1} $\pm$ 1.3 & \textbf{86.5} $\pm$ 0.4 & \textbf{83.1} $\pm$ 0.8 & \textbf{84.9} $\pm$ 1.0 & \textbf{80.6} $\pm$ 1.2 & \textbf{89.5} $\pm$ 1.3 & \textbf{80.9} $\pm$ 1.1 & \textbf{82.6} $\pm$ 1.1 \\ \midrule
\multirow{7}{*}{40\%} & Transformer & 23.0 $\pm$ 3.5 & 7.4 $\pm$ 6.0 & 14.5 $\pm$ 2.6 & 6.9 $\pm$ 2.6 & 43.8 $\pm$ 14.0 & 44.6 $\pm$ 23.0 & 40.5 $\pm$ 15.9 & 40.2 $\pm$ 20.1 \\
 & Trans-mean &  25.7 $\pm$ 2.5&	9.1 $\pm$ 2.3&	18.5 $\pm$ 1.4&	9.9 $\pm$ 1.1&	48.7 $\pm$ 2.7&	55.8 $\pm$ 2.6&	54.2 $\pm$ 3.0&	55.1 $\pm$ 2.9\\
 & GRU-D & 46.4 $\pm$ 2.5 & 64.5 $\pm$ 6.8 & 42.6 $\pm$ 7.4 & 44.3 $\pm$ 7.9 & 47.7 $\pm$ 1.4 & 63.4 $\pm$ 1.6 & 44.5 $\pm$ 0.5 & 47.5 $\pm$ 0.0 \\
 & SeFT & 26.3 $\pm$ 0.9 & 29.9 $\pm$ 4.5 & 27.3 $\pm$ 1.6 & 22.3 $\pm$ 1.9 & 26.8 $\pm$ 2.6 & 24.1 $\pm$ 3.4 & 28.0 $\pm$ 1.2 & 23.3 $\pm$ 3.0 \\
 & mTAND & 19.4 $\pm$ 4.5 & 15.1 $\pm$ 4.4 & 20.2 $\pm$ 3.8 & 17.0 $\pm$ 3.4 & 23.7 $\pm$ 1.0 & 33.9 $\pm$ 6.5 & 26.4 $\pm$ 1.6 & 29.3 $\pm$ 1.9 \\
 & Raindrop & 52.5 $\pm$ 3.7 & 53.4 $\pm$ 5.6 & 48.6 $\pm$ 1.9 & 44.7 $\pm$  3.4 & 57.0 $\pm$ 3.1 & 65.4 $\pm$ 2.7 & 56.7 $\pm$ 3.1 & 58.9 $\pm$ 2.5 \\ \cmidrule{2-10}
 & \textbf{ViTST}  & \textbf{76.5} $\pm$ 1.9 & \textbf{83.5} $\pm$ 0.9 & \textbf{76.7} $\pm$ 2.4 & \textbf{78.3} $\pm$ 2.1 & \textbf{73.7} $\pm$ 2.2 & \textbf{86.4} $\pm$ 1.1 & \textbf{74.0} $\pm$ 2.2 & \textbf{75.8} $\pm$ 1.8 \\ \midrule
\multirow{7}{*}{50\%} & Transformer & 21.4 $\pm$ 1.8 & 2.7 $\pm$ 0.2 & 12.5 $\pm$ 0.4 & 4.4 $\pm$ 0.3 & 43.2 $\pm$ 2.5 & 52.0 $\pm$ 2.5 & 36.9 $\pm$ 3.1 & 41.9 $\pm$ 3.2 \\
 & Trans-mean &21.3 $\pm$ 1.6&	2.8 $\pm$ 0.4	&12.5 $\pm$ 0.7&	4.6 $\pm$ 0.2&	46.4 $\pm$ 1.4&	59.1 $\pm$ 3.2&	43.1 $\pm$ 2.2&	46.5 $\pm$ 3.1  \\
 & GRU-D & 37.3 $\pm$ 2.7 & 29.6 $\pm$ 5.9 & 32.8 $\pm$ 4.6 & 26.6 $\pm$ 5.9 & 49.7 $\pm$ 1.2 & 52.4 $\pm$ 0.3 & 42.5 $\pm$ 1.7 & 47.5 $\pm$ 1.2 \\
 & SeFT & 24.7 $\pm$ 1.7 & 15.9 $\pm$ 2.7 & 25.3 $\pm$ 2.6 & 18.2 $\pm$ 2.4 & 26.4 $\pm$ 1.4 & 23.0 $\pm$ 2.9 & 27.5 $\pm$ 0.4 & 23.5 $\pm$ 1.8 \\
 & mTAND & 16.9 $\pm$ 3.1 & 12.6 $\pm$ 5.5 & 17.0 $\pm$ 1.6 & 13.9 $\pm$ 4.0 & 20.9 $\pm$ 3.1 & 35.1 $\pm$ 6.1 & 23.0 $\pm$ 3.2 & 27.7 $\pm$ 3.9 \\
 & Raindrop & 46.6 $\pm$ 2.6 & 44.5 $\pm$ 2.6 & 42.4 $\pm$ 3.9 & 38.0 $\pm$ 4.0 & 47.2 $\pm$ 4.4 & 59.4 $\pm$ 3.9 & 44.8 $\pm$ 5.3 & 47.6 $\pm$ 5.2 \\ \cmidrule{2-10}
 & \textbf{ViTST} & \textbf{70.0} $\pm$ 2.7 & \textbf{79.9} $\pm$ 2.2 & \textbf{70.5} $\pm$ 3.1 & \textbf{72.2} $\pm$ 3.0 & \textbf{70.9} $\pm$ 1.2 & \textbf{83.6} $\pm$ 2.4 & \textbf{71.5} $\pm$ 1.4 & \textbf{73.3} $\pm$ 2.1 \\ \bottomrule
\end{tabular}
}
\label{tab:leave-sensors-out}
\end{table*}

\begin{table*}[!h]
\scriptsize
\centering
\caption{Full results of our approach with different backbone vision models and the compared baselines. \textbf{Bold} indicates the best performer, while \underline{underline} represents the second best. Results are reported as \%.}
% \vspace{-2mm}
\label{tab:vision_models}
\resizebox{\textwidth}{!}{ 
\begin{tabular}{l|ll|ll|llll}
\toprule
& \multicolumn{2}{c|}{P19} & \multicolumn{2}{c|}{P12} & \multicolumn{4}{c}{PAM} \\ \cmidrule{2-9}
\multirow{-2}{*}{Methods} & AUROC & AUPRC & AUROC & AUPRC & Accuracy & Precision & Recall & F1 score \\ \midrule
Transformer & 80.7 $\pm$ 3.8 & 42.7 $\pm$ 7.7 & 83.3 $\pm$ 0.7 &  47.9 $\pm$ 3.6 & 83.5 $\pm$ 1.5 & 84.8 $\pm$ 1.5 & 86.0 $\pm$ 1.2 & 85.0 $\pm$ 1.3 \\
Trans-mean & 83.7 $\pm$ 1.8 & 45.8 $\pm$ 3.2 & 82.6 $\pm$ 2.0 & 46.3 $\pm$ 4.0 &	83.7 $\pm$ 2.3&	84.9 $\pm$ 2.6&	86.4 $\pm$ 2.1&	85.1 $\pm$ 2.4\\
GRU-D & 83.9 $\pm$1.7 & 46.9 $\pm$ 2.1 & 81.9 $\pm$ 2.1 & 46.1 $\pm$ 4.7 & 83.3 $\pm$ 1.6 & 84.6 $\pm$ 1.2 & 85.2 $\pm$ 1.6 & 84.8 $\pm$ 1.2\\
SeFT & 81.2 $\pm$ 2.3 & 41.9 $\pm$ 3.1 & 73.9 $\pm$ 2.5 & 31.1 $\pm$ 4.1 & 67.1 $\pm$ 2.2 & 70.0 $\pm$ 2.4 & 68.2 $\pm$ 1.5 & 68.5 $\pm$ 1.8\\
mTAND & 84.4 $\pm$ 1.3 & 50.6 $\pm$ 2.0 & 84.2 $\pm$ 0.8 & \underline{48.2} $\pm$ 3.4  & 74.6 $\pm$ 4.3 & 74.3 $\pm$ 4.0 & 79.5 $\pm$ 2.8 & 76.8 $\pm$ 3.4\\ 
IP-Net & 84.6 $\pm$ 1.3 & 38.1 $\pm$ 3.7 & 82.6 $\pm$ 1.4 & 47.6 $\pm$ 3.1 & 74.3 $\pm$ 3.8 & 75.6 $\pm$ 2.1 & 77.9 $\pm$ 2.2 & 76.6 $\pm$ 2.8 \\
DGM$^2$-O & 86.7 $\pm$ 3.4 & 44.7 $\pm$ 11.7 & 84.4 $\pm$ 1.6  & 47.3 $\pm$ 3.6 & 82.4 $\pm$ 2.3 &  85.2 $\pm$ 1.2 & 83.9 $\pm$ 2.3 & 84.3 $\pm$ 1.8 \\
MTGNN &81.9 $\pm$ 6.2  & 39.9 $\pm$ 8.9  & 74.4 $\pm$ 6.7 &  35.5 $\pm$ 6.0 & 83.4 $\pm$ 1.9 & 85.2 $\pm$ 1.7 & 86.1 $\pm$ 1.9 & 85.9 $\pm$ 2.4 \\
Raindrop & 87.0 $\pm$ 2.3  & \underline{51.8} $\pm$ 5.5  & 82.8 $\pm$ 1.7 & 44.0 $\pm$ 3.0 & 88.5 $\pm$ 1.5 & 89.9 $\pm$ 1.5 & 89.9 $\pm$ 1.5 & 89.8 $\pm$ 1.0 \\
\midrule
ResNet &76.3 $\pm$ 3.3 & 34.7 $\pm$ 4.1 & 71.9 $\pm$ 1.0 & 28.8 $\pm$ 2.4 & 73.1 $\pm$ 0.9 & 82.4 $\pm$ 5.6 & 69.7 $\pm$ 0.9 & 71.4 $\pm$ 1.8\\
ViT  &\underline{87.9} $\pm$ 2.5 & 51.6 $\pm$ 3.7 & \underline{84.8} $\pm$ 1.3 & 48.1 $\pm$ 3.8 & \underline{93.4} $\pm$ 0.7 & \underline{94.7} $\pm$ 0.9 & \underline{94.1} $\pm$ 0.7 & \underline{94.3} $\pm$ 0.7\\
\textbf{Swin} &\textbf{89.4} $\pm$ 1.9 &\textbf{52.8} $\pm$ 3.8 &\textbf{85.6} $\pm$ 1.1 & \textbf{49.8} $\pm$ 2.5 &\textbf{96.1} $\pm$ 0.7 &\textbf{96.8} $\pm$ 1.1 & \textbf{96.5} $\pm$ 0.7 &\textbf{96.6} $\pm$ 0.9\\\midrule
Swin-scratch & 74.6 $\pm$ 2.5 & 29.9 $\pm$ 4.6 & 66.9 $\pm$ 1.6 & 26.5 $\pm$ 2.6 & 84.5 $\pm$ 0.5 & 86.6 $\pm$ 0.6 & 87.1 $\pm$ 1.2 &86.6 $\pm$ 0.6\\
% \midrule
% % ResNet+  \\
% ViT+ &87.6$\pm$2.2 & 51.4$\pm$1.8 &83.5$\pm$1.8 &47.1$\pm$3.3 &- &- &- &-\\
% Swin+ &\textbf{88.6$\pm$1.9} &\textbf{52.0$\pm$2.1} &\textbf{84.9$\pm$1.9} &48.0$\pm$3.4 &- &- &- &-\\
\bottomrule
\end{tabular}
}
\vspace{-2mm}
\end{table*}


% \subsection{More Details on Training}\label{sect:training}








\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022. 
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
