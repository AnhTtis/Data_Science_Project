\documentclass{article}
% \input{math_commands.tex}


% if you need to pass options to natbib, use, e.g.:
    \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2023


% ready for submission
% \usepackage[final]{neurips_2023}

% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
%     \usepackage[preprint]{neurips_2023}


% to compile a camera-ready version, add the [final] option, e.g.:
\usepackage[final]{neurips_2023}


% to avoid loading the natbib package, add option nonatbib:
%    \usepackage[nonatbib]{neurips_2023}

\usepackage{amsmath,amsfonts,bm}

% Mark sections of captions for referring to divisions of figures
\newcommand{\figleft}{{\em (Left)}}
\newcommand{\figcenter}{{\em (Center)}}
\newcommand{\figright}{{\em (Right)}}
\newcommand{\figtop}{{\em (Top)}}
\newcommand{\figbottom}{{\em (Bottom)}}
\newcommand{\captiona}{{\em (a)}}
\newcommand{\captionb}{{\em (b)}}
\newcommand{\captionc}{{\em (c)}}
\newcommand{\captiond}{{\em (d)}}

% Highlight a newly defined term
\newcommand{\newterm}[1]{{\bf #1}}


% Figure reference, lower-case.
\def\figref#1{figure~\ref{#1}}
% Figure reference, capital. For start of sentence
\def\Figref#1{Figure~\ref{#1}}
\def\twofigref#1#2{figures \ref{#1} and \ref{#2}}
\def\quadfigref#1#2#3#4{figures \ref{#1}, \ref{#2}, \ref{#3} and \ref{#4}}
% Section reference, lower-case.
\def\secref#1{section~\ref{#1}}
% Section reference, capital.
\def\Secref#1{Section~\ref{#1}}
% Reference to two sections.
\def\twosecrefs#1#2{sections \ref{#1} and \ref{#2}}
% Reference to three sections.
\def\secrefs#1#2#3{sections \ref{#1}, \ref{#2} and \ref{#3}}
% Reference to an equation, lower-case.
\def\eqref#1{equation~\ref{#1}}
% Reference to an equation, upper case
\def\Eqref#1{Equation~\ref{#1}}
% A raw reference to an equation---avoid using if possible
\def\plaineqref#1{\ref{#1}}
% Reference to a chapter, lower-case.
\def\chapref#1{chapter~\ref{#1}}
% Reference to an equation, upper case.
\def\Chapref#1{Chapter~\ref{#1}}
% Reference to a range of chapters
\def\rangechapref#1#2{chapters\ref{#1}--\ref{#2}}
% Reference to an algorithm, lower-case.
\def\algref#1{algorithm~\ref{#1}}
% Reference to an algorithm, upper case.
\def\Algref#1{Algorithm~\ref{#1}}
\def\twoalgref#1#2{algorithms \ref{#1} and \ref{#2}}
\def\Twoalgref#1#2{Algorithms \ref{#1} and \ref{#2}}
% Reference to a part, lower case
\def\partref#1{part~\ref{#1}}
% Reference to a part, upper case
\def\Partref#1{Part~\ref{#1}}
\def\twopartref#1#2{parts \ref{#1} and \ref{#2}}

\def\ceil#1{\lceil #1 \rceil}
\def\floor#1{\lfloor #1 \rfloor}
\def\1{\bm{1}}
\newcommand{\train}{\mathcal{D}}
\newcommand{\valid}{\mathcal{D_{\mathrm{valid}}}}
\newcommand{\test}{\mathcal{D_{\mathrm{test}}}}

\def\eps{{\epsilon}}


% Random variables
\def\reta{{\textnormal{$\eta$}}}
\def\ra{{\textnormal{a}}}
\def\rb{{\textnormal{b}}}
\def\rc{{\textnormal{c}}}
\def\rd{{\textnormal{d}}}
\def\re{{\textnormal{e}}}
\def\rf{{\textnormal{f}}}
\def\rg{{\textnormal{g}}}
\def\rh{{\textnormal{h}}}
\def\ri{{\textnormal{i}}}
\def\rj{{\textnormal{j}}}
\def\rk{{\textnormal{k}}}
\def\rl{{\textnormal{l}}}
% rm is already a command, just don't name any random variables m
\def\rn{{\textnormal{n}}}
\def\ro{{\textnormal{o}}}
\def\rp{{\textnormal{p}}}
\def\rq{{\textnormal{q}}}
\def\rr{{\textnormal{r}}}
\def\rs{{\textnormal{s}}}
\def\rt{{\textnormal{t}}}
\def\ru{{\textnormal{u}}}
\def\rv{{\textnormal{v}}}
\def\rw{{\textnormal{w}}}
\def\rx{{\textnormal{x}}}
\def\ry{{\textnormal{y}}}
\def\rz{{\textnormal{z}}}

% Random vectors
\def\rvepsilon{{\mathbf{\epsilon}}}
\def\rvtheta{{\mathbf{\theta}}}
\def\rva{{\mathbf{a}}}
\def\rvb{{\mathbf{b}}}
\def\rvc{{\mathbf{c}}}
\def\rvd{{\mathbf{d}}}
\def\rve{{\mathbf{e}}}
\def\rvf{{\mathbf{f}}}
\def\rvg{{\mathbf{g}}}
\def\rvh{{\mathbf{h}}}
\def\rvu{{\mathbf{i}}}
\def\rvj{{\mathbf{j}}}
\def\rvk{{\mathbf{k}}}
\def\rvl{{\mathbf{l}}}
\def\rvm{{\mathbf{m}}}
\def\rvn{{\mathbf{n}}}
\def\rvo{{\mathbf{o}}}
\def\rvp{{\mathbf{p}}}
\def\rvq{{\mathbf{q}}}
\def\rvr{{\mathbf{r}}}
\def\rvs{{\mathbf{s}}}
\def\rvt{{\mathbf{t}}}
\def\rvu{{\mathbf{u}}}
\def\rvv{{\mathbf{v}}}
\def\rvw{{\mathbf{w}}}
\def\rvx{{\mathbf{x}}}
\def\rvy{{\mathbf{y}}}
\def\rvz{{\mathbf{z}}}

% Elements of random vectors
\def\erva{{\textnormal{a}}}
\def\ervb{{\textnormal{b}}}
\def\ervc{{\textnormal{c}}}
\def\ervd{{\textnormal{d}}}
\def\erve{{\textnormal{e}}}
\def\ervf{{\textnormal{f}}}
\def\ervg{{\textnormal{g}}}
\def\ervh{{\textnormal{h}}}
\def\ervi{{\textnormal{i}}}
\def\ervj{{\textnormal{j}}}
\def\ervk{{\textnormal{k}}}
\def\ervl{{\textnormal{l}}}
\def\ervm{{\textnormal{m}}}
\def\ervn{{\textnormal{n}}}
\def\ervo{{\textnormal{o}}}
\def\ervp{{\textnormal{p}}}
\def\ervq{{\textnormal{q}}}
\def\ervr{{\textnormal{r}}}
\def\ervs{{\textnormal{s}}}
\def\ervt{{\textnormal{t}}}
\def\ervu{{\textnormal{u}}}
\def\ervv{{\textnormal{v}}}
\def\ervw{{\textnormal{w}}}
\def\ervx{{\textnormal{x}}}
\def\ervy{{\textnormal{y}}}
\def\ervz{{\textnormal{z}}}

% Random matrices
\def\rmA{{\mathbf{A}}}
\def\rmB{{\mathbf{B}}}
\def\rmC{{\mathbf{C}}}
\def\rmD{{\mathbf{D}}}
\def\rmE{{\mathbf{E}}}
\def\rmF{{\mathbf{F}}}
\def\rmG{{\mathbf{G}}}
\def\rmH{{\mathbf{H}}}
\def\rmI{{\mathbf{I}}}
\def\rmJ{{\mathbf{J}}}
\def\rmK{{\mathbf{K}}}
\def\rmL{{\mathbf{L}}}
\def\rmM{{\mathbf{M}}}
\def\rmN{{\mathbf{N}}}
\def\rmO{{\mathbf{O}}}
\def\rmP{{\mathbf{P}}}
\def\rmQ{{\mathbf{Q}}}
\def\rmR{{\mathbf{R}}}
\def\rmS{{\mathbf{S}}}
\def\rmT{{\mathbf{T}}}
\def\rmU{{\mathbf{U}}}
\def\rmV{{\mathbf{V}}}
\def\rmW{{\mathbf{W}}}
\def\rmX{{\mathbf{X}}}
\def\rmY{{\mathbf{Y}}}
\def\rmZ{{\mathbf{Z}}}

% Elements of random matrices
\def\ermA{{\textnormal{A}}}
\def\ermB{{\textnormal{B}}}
\def\ermC{{\textnormal{C}}}
\def\ermD{{\textnormal{D}}}
\def\ermE{{\textnormal{E}}}
\def\ermF{{\textnormal{F}}}
\def\ermG{{\textnormal{G}}}
\def\ermH{{\textnormal{H}}}
\def\ermI{{\textnormal{I}}}
\def\ermJ{{\textnormal{J}}}
\def\ermK{{\textnormal{K}}}
\def\ermL{{\textnormal{L}}}
\def\ermM{{\textnormal{M}}}
\def\ermN{{\textnormal{N}}}
\def\ermO{{\textnormal{O}}}
\def\ermP{{\textnormal{P}}}
\def\ermQ{{\textnormal{Q}}}
\def\ermR{{\textnormal{R}}}
\def\ermS{{\textnormal{S}}}
\def\ermT{{\textnormal{T}}}
\def\ermU{{\textnormal{U}}}
\def\ermV{{\textnormal{V}}}
\def\ermW{{\textnormal{W}}}
\def\ermX{{\textnormal{X}}}
\def\ermY{{\textnormal{Y}}}
\def\ermZ{{\textnormal{Z}}}

% Vectors
\def\vzero{{\bm{0}}}
\def\vone{{\bm{1}}}
\def\vmu{{\bm{\mu}}}
\def\vtheta{{\bm{\theta}}}
\def\va{{\bm{a}}}
\def\vb{{\bm{b}}}
\def\vc{{\bm{c}}}
\def\vd{{\bm{d}}}
\def\ve{{\bm{e}}}
\def\vf{{\bm{f}}}
\def\vg{{\bm{g}}}
\def\vh{{\bm{h}}}
\def\vi{{\bm{i}}}
\def\vj{{\bm{j}}}
\def\vk{{\bm{k}}}
\def\vl{{\bm{l}}}
\def\vm{{\bm{m}}}
\def\vn{{\bm{n}}}
\def\vo{{\bm{o}}}
\def\vp{{\bm{p}}}
\def\vq{{\bm{q}}}
\def\vr{{\bm{r}}}
\def\vs{{\bm{s}}}
\def\vt{{\bm{t}}}
\def\vu{{\bm{u}}}
\def\vv{{\bm{v}}}
\def\vw{{\bm{w}}}
\def\vx{{\bm{x}}}
\def\vy{{\bm{y}}}
\def\vz{{\bm{z}}}

% Elements of vectors
\def\evalpha{{\alpha}}
\def\evbeta{{\beta}}
\def\evepsilon{{\epsilon}}
\def\evlambda{{\lambda}}
\def\evomega{{\omega}}
\def\evmu{{\mu}}
\def\evpsi{{\psi}}
\def\evsigma{{\sigma}}
\def\evtheta{{\theta}}
\def\eva{{a}}
\def\evb{{b}}
\def\evc{{c}}
\def\evd{{d}}
\def\eve{{e}}
\def\evf{{f}}
\def\evg{{g}}
\def\evh{{h}}
\def\evi{{i}}
\def\evj{{j}}
\def\evk{{k}}
\def\evl{{l}}
\def\evm{{m}}
\def\evn{{n}}
\def\evo{{o}}
\def\evp{{p}}
\def\evq{{q}}
\def\evr{{r}}
\def\evs{{s}}
\def\evt{{t}}
\def\evu{{u}}
\def\evv{{v}}
\def\evw{{w}}
\def\evx{{x}}
\def\evy{{y}}
\def\evz{{z}}

% Matrix
\def\mA{{\bm{A}}}
\def\mB{{\bm{B}}}
\def\mC{{\bm{C}}}
\def\mD{{\bm{D}}}
\def\mE{{\bm{E}}}
\def\mF{{\bm{F}}}
\def\mG{{\bm{G}}}
\def\mH{{\bm{H}}}
\def\mI{{\bm{I}}}
\def\mJ{{\bm{J}}}
\def\mK{{\bm{K}}}
\def\mL{{\bm{L}}}
\def\mM{{\bm{M}}}
\def\mN{{\bm{N}}}
\def\mO{{\bm{O}}}
\def\mP{{\bm{P}}}
\def\mQ{{\bm{Q}}}
\def\mR{{\bm{R}}}
\def\mS{{\bm{S}}}
\def\mT{{\bm{T}}}
\def\mU{{\bm{U}}}
\def\mV{{\bm{V}}}
\def\mW{{\bm{W}}}
\def\mX{{\bm{X}}}
\def\mY{{\bm{Y}}}
\def\mZ{{\bm{Z}}}
\def\mBeta{{\bm{\beta}}}
\def\mPhi{{\bm{\Phi}}}
\def\mLambda{{\bm{\Lambda}}}
\def\mSigma{{\bm{\Sigma}}}

% Tensor
\DeclareMathAlphabet{\mathsfit}{\encodingdefault}{\sfdefault}{m}{sl}
\SetMathAlphabet{\mathsfit}{bold}{\encodingdefault}{\sfdefault}{bx}{n}
\newcommand{\tens}[1]{\bm{\mathsfit{#1}}}
\def\tA{{\tens{A}}}
\def\tB{{\tens{B}}}
\def\tC{{\tens{C}}}
\def\tD{{\tens{D}}}
\def\tE{{\tens{E}}}
\def\tF{{\tens{F}}}
\def\tG{{\tens{G}}}
\def\tH{{\tens{H}}}
\def\tI{{\tens{I}}}
\def\tJ{{\tens{J}}}
\def\tK{{\tens{K}}}
\def\tL{{\tens{L}}}
\def\tM{{\tens{M}}}
\def\tN{{\tens{N}}}
\def\tO{{\tens{O}}}
\def\tP{{\tens{P}}}
\def\tQ{{\tens{Q}}}
\def\tR{{\tens{R}}}
\def\tS{{\tens{S}}}
\def\tT{{\tens{T}}}
\def\tU{{\tens{U}}}
\def\tV{{\tens{V}}}
\def\tW{{\tens{W}}}
\def\tX{{\tens{X}}}
\def\tY{{\tens{Y}}}
\def\tZ{{\tens{Z}}}


% Graph
\def\gA{{\mathcal{A}}}
\def\gB{{\mathcal{B}}}
\def\gC{{\mathcal{C}}}
\def\gD{{\mathcal{D}}}
\def\gE{{\mathcal{E}}}
\def\gF{{\mathcal{F}}}
\def\gG{{\mathcal{G}}}
\def\gH{{\mathcal{H}}}
\def\gI{{\mathcal{I}}}
\def\gJ{{\mathcal{J}}}
\def\gK{{\mathcal{K}}}
\def\gL{{\mathcal{L}}}
\def\gM{{\mathcal{M}}}
\def\gN{{\mathcal{N}}}
\def\gO{{\mathcal{O}}}
\def\gP{{\mathcal{P}}}
\def\gQ{{\mathcal{Q}}}
\def\gR{{\mathcal{R}}}
\def\gS{{\mathcal{S}}}
\def\gT{{\mathcal{T}}}
\def\gU{{\mathcal{U}}}
\def\gV{{\mathcal{V}}}
\def\gW{{\mathcal{W}}}
\def\gX{{\mathcal{X}}}
\def\gY{{\mathcal{Y}}}
\def\gZ{{\mathcal{Z}}}

% Sets
\def\sA{{\mathbb{A}}}
\def\sB{{\mathbb{B}}}
\def\sC{{\mathbb{C}}}
\def\sD{{\mathbb{D}}}
% Don't use a set called E, because this would be the same as our symbol
% for expectation.
\def\sF{{\mathbb{F}}}
\def\sG{{\mathbb{G}}}
\def\sH{{\mathbb{H}}}
\def\sI{{\mathbb{I}}}
\def\sJ{{\mathbb{J}}}
\def\sK{{\mathbb{K}}}
\def\sL{{\mathbb{L}}}
\def\sM{{\mathbb{M}}}
\def\sN{{\mathbb{N}}}
\def\sO{{\mathbb{O}}}
\def\sP{{\mathbb{P}}}
\def\sQ{{\mathbb{Q}}}
\def\sR{{\mathbb{R}}}
\def\sS{{\mathbb{S}}}
\def\sT{{\mathbb{T}}}
\def\sU{{\mathbb{U}}}
\def\sV{{\mathbb{V}}}
\def\sW{{\mathbb{W}}}
\def\sX{{\mathbb{X}}}
\def\sY{{\mathbb{Y}}}
\def\sZ{{\mathbb{Z}}}

% Entries of a matrix
\def\emLambda{{\Lambda}}
\def\emA{{A}}
\def\emB{{B}}
\def\emC{{C}}
\def\emD{{D}}
\def\emE{{E}}
\def\emF{{F}}
\def\emG{{G}}
\def\emH{{H}}
\def\emI{{I}}
\def\emJ{{J}}
\def\emK{{K}}
\def\emL{{L}}
\def\emM{{M}}
\def\emN{{N}}
\def\emO{{O}}
\def\emP{{P}}
\def\emQ{{Q}}
\def\emR{{R}}
\def\emS{{S}}
\def\emT{{T}}
\def\emU{{U}}
\def\emV{{V}}
\def\emW{{W}}
\def\emX{{X}}
\def\emY{{Y}}
\def\emZ{{Z}}
\def\emSigma{{\Sigma}}

% entries of a tensor
% Same font as tensor, without \bm wrapper
\newcommand{\etens}[1]{\mathsfit{#1}}
\def\etLambda{{\etens{\Lambda}}}
\def\etA{{\etens{A}}}
\def\etB{{\etens{B}}}
\def\etC{{\etens{C}}}
\def\etD{{\etens{D}}}
\def\etE{{\etens{E}}}
\def\etF{{\etens{F}}}
\def\etG{{\etens{G}}}
\def\etH{{\etens{H}}}
\def\etI{{\etens{I}}}
\def\etJ{{\etens{J}}}
\def\etK{{\etens{K}}}
\def\etL{{\etens{L}}}
\def\etM{{\etens{M}}}
\def\etN{{\etens{N}}}
\def\etO{{\etens{O}}}
\def\etP{{\etens{P}}}
\def\etQ{{\etens{Q}}}
\def\etR{{\etens{R}}}
\def\etS{{\etens{S}}}
\def\etT{{\etens{T}}}
\def\etU{{\etens{U}}}
\def\etV{{\etens{V}}}
\def\etW{{\etens{W}}}
\def\etX{{\etens{X}}}
\def\etY{{\etens{Y}}}
\def\etZ{{\etens{Z}}}

% The true underlying data generating distribution
\newcommand{\pdata}{p_{\rm{data}}}
% The empirical distribution defined by the training set
\newcommand{\ptrain}{\hat{p}_{\rm{data}}}
\newcommand{\Ptrain}{\hat{P}_{\rm{data}}}
% The model distribution
\newcommand{\pmodel}{p_{\rm{model}}}
\newcommand{\Pmodel}{P_{\rm{model}}}
\newcommand{\ptildemodel}{\tilde{p}_{\rm{model}}}
% Stochastic autoencoder distributions
\newcommand{\pencode}{p_{\rm{encoder}}}
\newcommand{\pdecode}{p_{\rm{decoder}}}
\newcommand{\precons}{p_{\rm{reconstruct}}}

\newcommand{\laplace}{\mathrm{Laplace}} % Laplace distribution

\newcommand{\E}{\mathbb{E}}
\newcommand{\Ls}{\mathcal{L}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\emp}{\tilde{p}}
\newcommand{\lr}{\alpha}
\newcommand{\reg}{\lambda}
\newcommand{\rect}{\mathrm{rectifier}}
\newcommand{\softmax}{\mathrm{softmax}}
\newcommand{\sigmoid}{\sigma}
\newcommand{\softplus}{\zeta}
\newcommand{\KL}{D_{\mathrm{KL}}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\standarderror}{\mathrm{SE}}
\newcommand{\Cov}{\mathrm{Cov}}
% Wolfram Mathworld says $L^2$ is for function spaces and $\ell^2$ is for vectors
% But then they seem to use $L^2$ for vectors throughout the site, and so does
% wikipedia.
\newcommand{\normlzero}{L^0}
\newcommand{\normlone}{L^1}
\newcommand{\normltwo}{L^2}
\newcommand{\normlp}{L^p}
\newcommand{\normmax}{L^\infty}

\newcommand{\parents}{Pa} % See usage in notation.tex. Chosen to match Daphne's book.

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\Tr}{Tr}
\let\ab\allowbreak

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography

\usepackage{graphicx}
\usepackage{subfigure}

\usepackage[table]{xcolor}
% \usepackage{colortbl}
\definecolor{Gray}{gray}{0.95}

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2023} with \usepackage[nohyperref]{icml2023} above.
\usepackage{xspace}
\usepackage{multirow}
\usepackage{bm}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{times}
\usepackage[ruled,vlined]{algorithm2e}
% \usepackage{subfig}
\usepackage{wrapfig}
\usepackage[frozencache,cachedir=.]{minted}



\newcommand{\valstd}[2]{$#1 {\scriptstyle \,\pm\, #2}$}
\newcommand{\valstdb}[2]{$\mathbf{#1} {\scriptstyle \,\pm\, #2}$}
\newcommand{\valstdu}[2]{$\underline{#1} {\scriptstyle \,\pm\, #2}$}

\title{Time Series as Images: Vision Transformer for Irregularly Sampled Time Series}

\author{%
  Zekun Li, Shiyang Li, Xifeng Yan\\
  University of California, Santa Barbara\\
  \texttt{\{zekunli, shiyangli, xyan\}@cs.ucsb.edu}
}


\begin{document}


\maketitle


\begin{abstract}
Irregularly sampled time series are increasingly prevalent, particularly in medical domains. While various specialized methods have been developed to handle these irregularities, effectively modeling their complex dynamics and pronounced sparsity remains a challenge. 
This paper introduces a novel perspective by converting irregularly sampled time series into line graph images, then utilizing powerful pre-trained vision transformers for time series classification in the same way as image classification. 
This method not only largely simplifies specialized algorithm designs but also presents the potential to serve as a universal framework for time series modeling. Remarkably, despite its simplicity, our approach outperforms state-of-the-art specialized algorithms on several popular healthcare and human activity datasets. Especially in the rigorous leave-sensors-out setting where a portion of variables is omitted during testing, our method exhibits strong robustness against varying degrees of missing observations, achieving an impressive improvement of 42.8\% in absolute F1 score points over leading specialized baselines even with half the variables masked. Code and data are available at \url{https://github.com/Leezekun/ViTST}.
\end{abstract}


\section{Introduction}\label{sect:intro}
Time series data are ubiquitous in a wide range of domains, including healthcare, finance, traffic, and climate science. With the advances in deep learning architectures such as LSTM~\citep{lstm}, Temporal Convolutional Network (TCN)~\citep{tcn}, and Transformer~\citep{transformer}, numerous algorithms have been developed for time series modeling. 
However, these methods typically assume fully observed data at regular intervals and fixed-size numerical inputs. Consequently, these methods encounter difficulties when faced with irregularly sampled time series, which consist of a sequence of samples with irregular intervals between their observation times.
To address this challenge, highly specialized models have been developed, which require substantial prior knowledge and efforts in model architecture selection and algorithm design~\citep{marlin2012unsupervised,lipton2016directly,gru-d,seft,raindrop,mtand,Zhang2022ImprovingMP}. 
% Although these models provide solutions, they come with limitations due to their high degree of customization and the need for expert knowledge.
% These models are tailored to handle irregularly sampled time series data and require careful consideration in terms of their architecture and algorithm design.

In parallel, pre-trained transformer-based vision models, most notably vision transformers,\footnote{In this paper, we use the term ``vision transformers'' to denote a category of pre-trained transformer-based vision models, such as ViT~\citep{vit}, Swin Transformer~\citep{swin}, DeiT~\citep{deit}, etc.} have emerged and demonstrated strong abilities in various vision tasks such as image classification and object detection, nearly approaching human-level performance.
Motivated by the flexible and effective manner in which humans analyze complex numerical time series data through visualization, we raise the question: \textit{Can these powerful pre-trained vision transformers capture temporal patterns in visualized time series data, similar to how humans do?} 

To investigate this question, we propose a minimalist approach called \textbf{ViTST} (\underline{Vi}sion \underline{T}ime \underline{S}eries \underline{T}ransformer), which involves transforming irregularly sampled multivariate time series into line graphs, organizing them into a standard RGB image format, and finetuning a pre-trained vision transformer for classification using the resulting image as input, as illustrated in Figure~\ref{fig:illustration}.

Line graphs serve as an effective and efficient visualization technique for time series data, regardless of irregularity, structures, and scales. They can capture crucial patterns, such as the temporal dynamics represented within individual line graphs and interrelations between variables throughout separate graphs. Such a visualization technique benefits our approach because it is both simple and intuitively comprehensible to humans, enabling straightforward decisions for time series-to-image transformations. Leveraging vision models for time series modeling in this manner mirrors the concept of \textbf{prompt engineering}, wherein individuals can intuitively comprehend and craft prompts to potentially enhance the processing efficiency of language models.

We conduct a comprehensive investigation and validation of the proposed approach, ViTST, which has demonstrated its superior performance over state-of-the-art (SoTA) methods specifically designed for irregularly sampled time series. Specifically, ViTST exceeded prior SoTA by 2.2\% and 0.7\% in absolute AUROC points, and 1.3\% and 2.9\% in absolute AUPRC points for healthcare datasets P19~\citep{p19} and P12~\citep{p12}, respectively. For the human activity dataset, PAM~\citep{pam}, we observed improvements of 7.3\% in accuracy, 6.3\% in precision, 6.2\% in recall, and 6.7\% in F1 score (absolute points) over existing SoTA methods.
Our approach also exhibits strong robustness to missing observations, surpassing previous leading solutions by a notable 42.8\% in absolute F1 score points under the leave-sensors-out scenario, where half of the variables in the test set are masked during testing. 
Furthermore, when tested on regular time series data including those with varying numbers of variables and extended sequence lengths, ViTST still achieves excellent results comparable to the specialized algorithms designed for regular time series modeling. This underscores the versatility of our approach, as traditional methods designed for regularly sampled time series often struggle with irregularly sampled data, and vice versa.

\begin{figure*}[tbp]
    \centering
    \includegraphics[width = 0.92\linewidth]{./pic/illustration13.pdf}
    \caption{An illustration of our approach ViTST. The example is from a healthcare dataset P12~\citep{p12}, which provides the irregularly sampled observations of 36 variables for patients (we only show 4 variables here for simplicity). Each column in the table is an observation of a variable, with the observed time and value. We plot separate line graphs for each variable and arrange them into a single image, which is then fed into the vision transformer for classification.}
    \vspace{-5mm}
    \label{fig:illustration}
\end{figure*}


%We compare the proposed approach with state-of-the-art methods highly customized for irregularly sampled time series.
In summary, the contributions of this work are three-fold: (1) We propose a simple yet highly effective approach for multivariate irregularly sampled time series classification. Despite its simplicity, our approach achieves strong results against the highly specialized SoTA methods.
(2) Our approach demonstrates excellent results on both irregular and regular time series data, showcasing its versatility and potential as a general-purpose framework for time series modeling. It offers a robust solution capable of handling diverse time series datasets with varying characteristics.
(3) Our work demonstrates the successful transfer of knowledge from vision transformers pre-trained on natural images to synthetic visualized time series line graph images. We anticipate that this will facilitate the utilization of fast-evolving and well-studied computer vision techniques in the time series domains, such as better model architecture~\citep{liu2022swin}, data augmentation~\citep{shorten2019survey}, interpretability~\citep{chefer2021transformer}, and self-supervised pre-training~\citep{mae}. 



\section{Related work}\label{sect:related_work}

\textbf{Irregularly sampled time series.~}
An irregularly sampled time series is a sequence of observations with varying time intervals between them. In a multivariate setting, different variables within the same time series may not align. These characteristics present a significant challenge to standard time series modeling methods, which usually assume fully observed and regularly sampled data points.
% Classification is the dominant downstream task for irregularly sampled time series modeling.
% A basic way to deal with irregular sampling is to impute the missing values and transform the irregular time series into regular ones for processing~\citep{li2020learning,shan2021nrtsi}. 
% Classification is the dominant downstream task for irregularly sampled time series modeling. A classic way to deal with irregular sampling is to discretize  continuous-time observations into fixed time intervals\citep{marlin2012unsupervised,lipton2016directly}. However, this kind of method needs to choose the discretization interval length carefully. Otherwise, there will be too many values in some intervals, and many other intervals will be empty, containing missing values. Some methods thus impute the missing values to change the irregular time series into regular ones for processing. However, the missing value itself is informative. Recent methods thus directly use the irregular time series as input. For example, \citep{gru-d} proposed several methods based on gated recurrent units (GRU)~\citep{chung2014empirical}, which take as input the observations' values and also times.
% On the other hand, \citep{pham2017predicting} modified the forget gate of LSTM~\citep{lstm} to capture the irregularity. Similarly, \citep{yoon2017multi} proposed an approach based on multi-directional RNN, which can capture the inter- and intra-steam patterns. \citep{ipnet} developed the interpolation-prediction network, which consists of an interpolation network with several semi-parametric RBF interpolation layers and a prediction layer based on GRU. \citep{seft} introduced an algorithm to map the irregular time series into a set of observations for classification based on differentiable set functions. \citep{mtand} presented a multi-time attention network, which learns continuous-time embeddings coupled with a multi-time attention mechanism to deal with the continuous-time inputs. \citep{raindrop} modeled the irregularly sampled time series as graphs and utilized graph neural networks to model the relationships between different variables. As can be seen, these methods are all highly specialized for irregular time series. In this work, we investigate whether general vision models, such as vision transformers can be adapted to model the irregularly sampled time series.
A common approach to handling irregular sampling is to convert continuous-time observations into fixed time intervals~\citep{marlin2012unsupervised,lipton2016directly}. To account for the dynamics between observations, several models have been proposed, such as GRU-D~\citep{gru-d}, which decays the hidden states based on gated recurrent units (GRU)~\citep{chung2014empirical}.
% which takes as input the observations' values and also times. \citep{pham2017predicting} modified the forget gate of LSTM~\citep{lstm} to better account for irregularity.
Similarly, \citep{yoon2017multi} proposed an approach based on multi-directional RNN, which can capture the inter- and intra-steam patterns. Besides the recurrent and differential equation-based model architectures, recent work has explored attention-based models. Transformer~\citep{transformer} is naturally able to handle arbitrary sequences of observations. ATTAIN~\citep{attain} incorporates attention mechanism with LSTM to model time irregularity. SeFT~\citep{seft} maps the irregular time series into a set of observations based on differentiable set functions and utilizes an attention mechanism for classification. mTAND~\citep{mtand} learns continuous-time embeddings coupled with a multi-time attention mechanism to deal with continuous-time inputs. UTDE~\citep{Zhang2022ImprovingMP} integrates embeddings from mTAND and classical imputed time series with learnable gates to tackle complex temporal patterns. Raindrop~\citep{raindrop} models irregularly sampled time series as graphs and utilizes graph neural networks to model relationships between variables. While these methods are specialized for irregular time series, our work explores a simple and general vision transformer-based approach for irregularly sampled time series modeling.

% In recent years, there has been a surge of work that develops highly specialized neural networks to deal with irregularly sampled time series. Some methods discretize continuous-time observations into fixed time intervals and utilize probabilistic clustering models~\citep{marlin2012unsupervised} or Recurrent Neural Networks (RNN)~\citep{lipton2016directly}. To incorporate the dynamics between observations, GRU-D~\citep{gru-d} decays the hidden states based on gated recurrent units (GRU)~\citep{chung2014empirical}, which takes as input the observations' values and also times. \citep{pham2017predicting} modified the forget gate of LSTM~\citep{lstm} to capture the irregularity. Similarly, \citep{yoon2017multi} proposed an approach based on multi-directional RNN, which can capture the inter- and intra-steam patterns. IP-Net~\citep{ipnet} utilizes semi-parametric interpolation layers for interpolation and GRU for prediction. Neural ordinary differential equations (neural ODEs)~\citep{chen2018neural} are used to model the continuous dynamics of a hidden state. For example, ODE-RNN~\citep{ode} uses RNN gates to update the hidden states when there is an observation.

% Besides the recurrent and differential equation-based architectures, recent work has explored attention mechanisms. Transformer~\citep{transformer} is able to handle arbitrary sequences of observations. ATTAIN~\citep{attain} incorporates an attention mechanism with LSTM to model the time irregularity between observations. SeFT~\citep{seft} maps the irregular time series into a set of observations based on differentiable set functions and thus applies an attention mechanism for classification. mTAND~\citep{mtand} presented a multi-time attention network, which learns continuous-time embeddings coupled with a multi-time attention mechanism to deal with the continuous-time inputs. Raindrop~\citep{raindrop} instead considers the irregularly sampled time series as separate sensor graphs and utilizes graph neural networks to learn the dependencies between different sensors (variables).
% Most of these existing methods are highly customized, requiring considerable prior knowledge and effort to design and modify model architectures. In this work, we propose a simple and general approach without assuming any prior knowledge or specific engineering.

\textbf{Numerical time series modeling with transformers.~}
Transformers have gained significant attention in time series modeling due to their exceptional ability to capture long-range dependencies in sequential data. A surge of transformer-based methods have been proposed and successfully applied to various time series modeling tasks, such as forecasting~\citep{logtrans,informer,autoformer,fedformer}, classification~\citep{tst}, and anomaly detection~\citep{anomalytrans}. However, these methods are usually designed for regular time series settings, where multivariate numerical values at the same timestamp are viewed as a unit, and temporal interactions across different units are explicitly modeled. A recent work~\citep{patchtst} suggests dividing each univariate time series into a sequence of sub-series and modeling their interactions independently. 
% These methods all operate on numerical values and assume fully-observed input, while our proposed method deals with time series data in the visual modality. 
% By transforming the time series into visualized line graphs, we can handle irregularly time series data and leverage the powerful vision transformers which have been pre-trained on large-scale of natural image data and possess strong general-purpose visual information processing abilities.
These methods all operate on numerical values and assume fully observed input, while our proposed method deals with time series data in the visual modality. By transforming the time series into visualized line graphs, we can effectively handle irregularly sampled time series and harness the strong visual representation learning abilities of pre-trained vision transformers.


% \cite{tst} introduced an unsupervised pre-training scheme on the numerical time series data for additional performance gain. 
% We demonstrate that vision transformers pre-trained on natural images can effectively handle visualized time series data. 

\textbf{Imaging time series.~}
Previous studies have explored transforming time series data into different types of images, such as Gramian fields~\citep{wang2015imaging}, recurring plots~\citep{hatami2018classification, tripathy2018use}, and Markov transition fields~\citep{wang2015spatially}. These approaches typically employ convolutional neural networks (CNNs) for classification tasks. However, they often require domain expertise to design specialized imaging techniques and are not universally applicable across domains. Another approach~\citep{sood2021visual} involves utilizing convolutional autoencoders to complete images transformed from time series, specifically for forecasting purposes.
Similarly, \citep{semenoglou2023image} utilized CNNs to encode images converted from time series and use a regressor for numerical forecasting. These approaches, however, still need plenty of specialized designs and modifications to adapt to time series modeling. Furthermore, they still lag behind the current leading numerical techniques. 
In contrast, our proposed method leverages the strong abilities of pre-trained vision transformer to achieve superior results by transforming the time series into line graph images, sidestepping the need of prior knowledge and specific modifications and designs.

% \noindent\textbf{Time Series as Other Modalities.~}
% The recently emerging pre-trained transformer-based models, initially proposed in Natural Language Processing (NLP) field, have since come to monopolize the state-of-the-art performance across various downstream tasks in NLP and Computer Vision (CV) fields. For example, the pre-trained language model BERT~\citep{bert} and GPTs~\citep{gpt1,gpt2,gpt3} can be adapted to various NLP tasks. Some non-language tasks can also be solved by these pre-trained transformer-based language models by transforming them into language sentence prompts~\citep{lift}. A recent work~\citep{pisa} tried to represent the time series in natural language and utilize pre-trained language models to forecast. However, such a method has difficulties modeling long-range multivariate time series as it usually involves tens of thousands of numerical values, which cannot be fitted into the language models (512/1024 max tokens for most LMs). In addition, it is hard to express the informative irregularity of time series in natural language sentences. By contrast, we transform numerical time series data into images and utilize pre-trained transformer-based vision models to perform time series modeling, which doesn't the issues.
% Note that some prior studies tried to transform time series into various forms of images such as Gramian fields~\citep{wang2015imaging}, recurring plots~\citep{hatami2018classification,tripathy2018use}, and Markov transition fields~\citep{wang2015spatially} and utilize CNN for classification purpose. However, they are not domain-agnostic and require domain expertise to design specialized imaging methods. ~\citep{sood2021visual} employed convolutional autoencoders to complete the image converted from time series for forecasting purposes. However, this method is less effective than numerical techniques and is not suitable for irregularly sampled time series data. By contrast, leveraging the powerful vision transformer, our method achieves better results than the highly specialized methods on line graph RGB images transformed from time series without assuming prior knowledge.



\section{Approach}\label{sect:approach}
% Let $\gD=\{(\rvs_{i},y_{i})|i=1,\cdots,N\}$ denotes a time series dataset containing $N$ samples. Each individual data sample is associated with a label $y_i \in \{1,\cdots, C\}$, where $C$ is the number of classes. 
As illustrated in Fig.~\ref{fig:illustration},  ViTST consists of two main steps: (1) transforming multivariate time series into a concatenated line graph image, and (2) utilizing a pre-trained vision transformer as an image classifier for the classification task. To begin with, we present some basic notations and problem formulation.
 
% Specifically, we draw each variable of the multivariate time series in an individual line graph and place them in a super image. We choose the recently prevalent Vision Transformers~\citep{vit} as the image encoder. To ensure the complex spatial-temporal patterns within the image can be captured, we introduce \textbfit{graph-aware position embedding} to the architecture of Vision Transformer, to make it aware of not only the patches' global positions in the whole super image but also their local positions in the corresponding graphs.

\textbf{Notation.~} Let $\gD=\{(\gS_{i},y_{i})|i=1,\cdots,N\}$ denote a time series dataset containing $N$ samples. Every data sample is associated with a label $y_i \in \{1,\cdots, C\}$, where $C$ is the number of classes. Each multivariate time series $\gS_{i}$ consists of observations of $D$ variables at most (some might have no observations). The observations for each variable $d$ are given by a sequence of tuples with observed time and value $[(t_1^d,v_1^d), (t_2^d,v_2^d), \cdots,(t_{n_d}^d,v_{n_d}^d)]$, where $n_d$ is the number of observations for variable $d$. If the intervals between observation times $[t_1^d, t_2^d, \cdots, t_{n_d}^d]$ are different across variables or samples, $\gS_{i}$ is an irregularly sampled time series. Otherwise, it is a regular time series. 
% \shiyang{Not quite sure if this definition is correct. my understanding is that if the interval between timestamps is not the same, they are irregular. Otherwise, they are regular.}.
% We will omit the data sample index $i\in\{1,2,\cdots,N\}$ for simplicity when necessary.

\textbf{Problem formulation.~} Given the dataset $\gD=\{(\gS_{i},y_{i})|i=1,\cdots,N\}$ containing $N$ multivariate time series, we aim to predict the label $\hat{y}_i \in \{1,\cdots, C\}$ for each time series $\gS_{i}$. 
% \textit{We aim to learn a function $f: \gS_i \rightarrow \vz_i $ to map the multivariate time series $\gS_i$ to its representation $\vz_i$, which can be used to predict the corresponding label $\hat{y}_i$. 
There are mainly two components in our framework: (1) a function that transforms the time series $\gS_i$ into an image $\rx_i$; (2) an image classifier that takes the line graph image $\rx_i$ as input and predicts the label $\hat{y}_i$. 
% \shiyang{Notation mismatch $\rvs_{i}$ vs $\gS_i$?}

\subsection{Time Series to Image Transformation}\label{sect:line_graph}
% Given a $D$-dimensional multivariate time series $\rvs_i$, we aim to transform it into an image $\rx_i \in \R^{H \times W \times C}$, where $(H,W)$ is the resolution of the image and $C$ is the number of channels. We choose to transform the time series into line graphs, which might be the most intuitive and straightforward way to represent time series data and doesn't require any domain knowledge. 

\textbf{Time series line graph.~}
The line graph is a prevalent method for visualizing temporal data points. In this representation, each point signifies an observation marked by its time and value: the horizontal axis captures timestamps, and the vertical axis denotes values. Observations are connected with straight lines in chronological order, with any missing values interpolated seamlessly. This graphing approach allows for flexibility for users in plotting time series as images, intuitively suited for the processing efficiency of vision transformers. The practice mirrors \textbf{prompt engineering} when using language models, where users can understand and adjust natural language prompts to enhance the model performance.

In our practice, we use marker symbols ``$\ast$'' to indicate the observed data points in the line graph. Since the scales of different variables may vary significantly, we plot the observations of each variable in an individual line graph, as shown in Fig.~\ref{fig:illustration}. The scales of each line graph $\rg_{i,d}$ are kept the same across different time series $\gS_i$. We employ distinct colors for each line graph for differentiation. Our initial experiments indicated that tick labels and other graphical components are superfluous, as an observation's position inherently signals its relative time and value magnitude. We investigated the influences of different choices of time series-to-image transformation in Section~\ref{sect:ablation}.

\textbf{Image Creation.~}
Given a set of time series line graphs $\gG_i={\rg_1, \rg_2, \cdots, \rg_D}$ for a time series $\gS_i$, we arrange them in a single image $\rx_i$ using a pre-defined grid layout. We adopt a square grid by default, following \citep{sifar}. Specifically, we arrange the $D$ time series line graphs in a grid of size $l \times l$ if $l \times (l-1) < D \leq l \times l$, and a grid of size $l \times (l+1)$ if $l \times l < D \leq l \times (l+1)$. 
For example, the P19, P12, and PAM datasets contain 34, 36, and 17 variables, respectively, and the corresponding default grid layouts are $6\times6$, $6\times6$, and $4\times5$. Any grid spaces not occupied by a line graph remain empty. Figure~\ref{fig:attention_map} showcases examples of the resulting images. As for the order of variables, we sort them according to the missing ratios for irregularly sampled time series. We explored the effects of different grid layouts and variable orders in Section~\ref{sect:ablation}.

% \vspace{-4mm}
\subsection{Vision Transformers for Time Series Modeling}\label{sect:vit}
Given the image $\rx_i$ transformed from time series $\gS_i$, we leverage an image classifier to perceive the image and perform the classification task. The time series patterns in a line graph image involve both local (\textit{i.e.}, the temporal dynamics of a single variable in a line graph) and global (the correlation among variables across different line graphs) contexts. To better capture these patterns, we choose the recently developed vision transformers. Unlike the predominant CNNs, vision transformers are proven to excel at maintaining spatial information and have stronger abilities to capture local and global dependencies~\citep{vit,swin}. 

\textbf{Preliminary.~}
Vision Transformer (ViT)~\citep{vit} is originally adapted from NLP.  The input image is split into fixed-sized patches, and each patch is linearly embedded and augmented with position embeddings. The resulting sequence of vectors is then fed into a standard Transformer encoder consisting of a stack of multi-head attention modules (MSA) and MLP to obtain patch representations. An extra classification token is added to the sequence to perform classification or other tasks. ViT models \textit{global} inter-unit interactions between all pairs of patches, which can be computationally expensive for high-resolution images. Swin Transformer, on the other hand, adopts a hierarchical architecture with multi-level feature maps and performs self-attention locally within non-overlapping windows, reducing computational complexity while improving performance. We use Swin Transformer as the default backbone vision model unless otherwise specified, but any other vision model can also be used within this framework.

Swin Transformer captures the \textit{local} and \textit{global} information by constructing a hierarchical representation starting from small-sized patches in earlier layers and gradually merging neighboring patches in deeper layers. Specifically, in the W-MSA block, self-attention is calculated within each non-overlapping window, allowing for the capture of local intra-variable interactions and temporal dynamics of a single line graph for a variable $d$. The shifted window block SW-MSA then enables connections between different windows, which span across different line graphs, to capture \textit{global} interactions. Fig.~\ref{fig:window} illustrates this process.
% The self-attention is calculated within each non-overlapping window in the W-MSA block, and the shifted window block SW-MSA enables the connection of different windows.
Mathematically, the consecutive Swin Transformer blocks are calculated as:
\begin{align}
    &{{\hat{\bf{z}}}^{l}} = \text{W-MSA}\left( {\text{LN}\left( {{{\bf{z}}^{l - 1}}} \right)} \right) + {\bf{z}}^{l - 1},\nonumber\\
    &{{\bf{z}}^l} = \text{MLP}\left( {\text{LN}\left( {{{\hat{\bf{z}}}^{l}}} \right)} \right) + {{\hat{\bf{z}}}^{l}},\nonumber\\
    &{{\hat{\bf{z}}}^{l+1}} = \text{SW-MSA}\left( {\text{LN}\left( {{{\bf{z}}^{l}}} \right)} \right) + {\bf{z}}^{l}, \nonumber\\
    &{{\bf{z}}^{l+1}} = \text{MLP}\left( {\text{LN}\left( {{{\hat{\bf{z}}}^{l+1}}} \right)} \right) + {{\hat{\bf{z}}}^{l+1}}, \label{eq:swin}
\end{align}

\begin{wrapfigure}{r}{0.45\textwidth}
\centering
    \includegraphics[width=1.0\linewidth]{./pic/window.pdf}
        \vspace{-5mm}
    \caption{
Illustration of the shifted window approach in Swin Transformer. Self-attention is calculated within each window (grey box). When the window is contained within a single line graph, it captures local interactions. After shifting, the window includes patches from different line graphs, allowing for the modeling of global cross-variable interactions.
    }
    \label{fig:window}
    \vspace{-5mm}
\end{wrapfigure}

where ${\hat{\bf{z}}}^l$ and ${\bf{z}}^l$ denote the output features of the (S)W-MSA module and the MLP module for block $l$, respectively; LN stands for the layer normalization~\citep{Ba2016LayerN}. After multiple stages of blocks, the global interactions among patches from all line graphs can be captured, enabling the modeling of correlations between different variables. 
We have also explored the use of additional positional embeddings, including local positional embeddings to indicate the position of each patch within its corresponding line graph, and global positional embeddings to represent the index of the associated line graph in the entire image.
However, we didn't observe consistent improvement over the already highly competitive performance, which might suggest that the original pre-trained positional embeddings have already been able to capture the information regarding local and global patch positions.



\textbf{Inference.~}
We use vision transformers to predict the labels of time series in the same way as image classification. The outputs of Swin Transformer blocks at the final stage are used as the patch representations, upon which a flattened layer with a linear head is applied to obtain the prediction $\hat{y}_i$. 


\section{Experiments}\label{sect:experiments}

\subsection{Experimental Setup}

\begin{table*}[htbp]
% \vspace{-2mm}
\caption{Statistics of the irregularly sampled time series datasets used in our experiments. 
% ``\#Avg. obs.'' denotes the average number of observations for each sample. ``Static info'' indicates if the time series sample is associated with static attributes (\textit{e.g.}, genders).
} 
% \vspace{-2mm}
\centering
\small
\scalebox{0.9}{
\begin{tabular}{lccccccc}
\toprule
Datasets & \#Samples & \#Variables & \#Avg. obs.& \#Classes & Demographic info & Imbalanced & Missing ratio\\ \midrule
P19 & 38,803 & 34 & 401 & 2 & True & True &94.9\%\\
P12 & 11,988 & 36 & 233 & 2 & True & True &88.4\%\\
PAM & 5,333 & 17 & 4,048 & 8 & False & False &60.0\%\\ 
\bottomrule
\end{tabular}
}
% \vspace{-2mm}
\label{tab:datasets}
\end{table*}


\textbf{Datasets and metrics.~}
We conducted experiments using three widely used healthcare and human activity datasets, as outlined in Table~\ref{tab:datasets}. The P19 dataset~\citep{p19} contains information from 38,803 patients, with 34 sensor variables and a binary label indicating sepsis. The P12 dataset~\citep{p12} comprises data from 11,988 patients, including 36 sensor variables and a binary label indicating survival during hospitalization. Lastly, the PAM dataset~\citep{pam} includes 5,333 samples from 8 distinct human activities, with 17 sensor variables provided for each sample.
We used the processed data provided by Raindrop~\citep{raindrop}. 
To ensure consistency, we employed identical data splits across all comparison baselines, and evaluated the performance using standard metrics such as Area Under a ROC Curve (AUROC) and Area Under Precision-Recall Curve (AUPRC) for the imbalanced P12 and P19 datasets. For the more balanced PAM dataset, we reported Accuracy, Precision, Recall, and F1 score. 
% More details on the datasets and our approach are provided in Appendix~\ref{sect:dataset}.


\textbf{Implementation.~}
We utilized the Matplotlib package to plot line graphs and save them as standard RGB images. For the P19, P12, and PAM datasets, we implemented grid layouts of $6\times6$, $6\times6$, and $4\times5$, respectively. For a fair comparison, we assigned a fixed size of $64\times64$ to each grid cell (line graph), resulting in image sizes of $384\times384$, $384\times384$, and $256\times320$, respectively. It is important to note that image sizes can also be directly set to any size, irrespective of grid cell dimensions. We plot the images according to the value scales on training sets.
% , as shown in Fig.~\ref{fig:code}.
% We evaluate our approach using two representative vision transformers: ViT~\citep{vit}, Swin Transformer~\citep{swin}, and also the predominant CNN-based model ResNet~\citep{resnet}. 
We employed the checkpoint of Swin Transformer pre-trained on the ImageNet-21K dataset\footnote{https://huggingface.co/microsoft/swin-base-patch4-window7-224-in22k}. The default patch size and window size are $4$ and $7$, respectively.
% In this case, the introduced temporal position embedding can be added to both ViT and Swin Transformer when applied to the created images. 
% We also investigate the influence of different grid layouts on the model performance, which will be discussed in Sec.~\ref{sect:ablation}. 

% The original ViT has a slightly worse performance in our setting. 

% We evaluate our approach using two representative vision transformers: ViT~\citep{vit} and Swin Transformer~\citep{swin}. We use Swin Transformer by default if not explicitly mentioned.
% We use the pre-trained weights provided in HuggingFace~\citep{huggingface} to initialize ViT\footnote{https://huggingface.co/google/vit-base-patch16-224} and Swin Transformer\footnote{https://huggingface.co/microsoft/swin-base-patch4-window7-224-in22k}. 

% The default patch size of ViT and Swin Transformer are $16\times16$ and $4 \times 4$, respectively. 


\textbf{Training.~}
Given the highly imbalanced nature of the P12 and P19 datasets, we employed upsampling of the minority class to match the size of the majority class. We finetuned the Swin Transformer for 2 and 4 epochs on the upsampled P19 and P12 datasets, respectively, and for 20 epochs on the PAM dataset. The batch sizes used for training were 48 for P19 and P12, and 72 for PAM, while the learning rate was set to 2e-5. The models were trained using A6000 GPUs with 48G memory. 
% To mitigate overfitting due to upsampling, we utilized the cutout~\citep{cutout} augmentation method on input images from the P12 and P19 datasets during training. Specifically, we randomly masked 16 square regions of size $16 \times 16$ in each image. 


\textbf{Incorporating static features.~}
In real-world applications, especially in the healthcare domain, irregular time series data often accompanies additional information such as categorical or textual features. In the P12 and P19 datasets, each patient's demographic information, including weight, height, and ICU type, is provided. This static information remains constant over time and can be expressed using natural language. To incorporate this information into our framework, we employed a template to convert it into natural language sentences and subsequently encoded the resulting text using a RoBERTa-base~\citep{roberta} text encoder. The resulting text embeddings were concatenated with the image embeddings obtained from the vision transformer to perform classification. The static features were also applied to all compared baselines.

\subsection{Main Results}\label{sect:experiment}

\begin{table*}[!t]
\scriptsize
\centering
\caption{Comparison with the baseline methods on irregularly sampled time series classification task. \textbf{Bold} indicates the best performer, while \underline{underline} represents the second best. }
% \vspace{-2mm}
\label{tab:main_result}
\resizebox{\textwidth}{!}{ 
\begin{tabular}{l|ll|ll|llll}
\toprule
& \multicolumn{2}{c|}{P19} & \multicolumn{2}{c|}{P12} & \multicolumn{4}{c}{PAM} \\ \cmidrule{2-9}
\multirow{-2}{*}{Methods} & AUROC & AUPRC & AUROC & AUPRC & Accuracy & Precision & Recall & F1 score \\ \midrule
Transformer & \valstd{80.7}{3.8} & \valstd{42.7}{7.7} & \valstd{83.3}{0.7} & \valstd{47.9}{3.6} & \valstd{83.5}{1.5} & \valstd{84.8}{1.5} & \valstd{86.0}{1.2} & \valstd{85.0}{1.3} \\
Trans-mean & \valstd{83.7}{1.8} & \valstd{45.8}{3.2} & \valstd{82.6}{2.0} & \valstd{46.3}{4.0} &	\valstd{83.7}{2.3} &\valstd{84.9}{2.6} & \valstd{86.4}{2.1}&\valstd{85.1}{2.4}\\
GRU-D & \valstd{83.9}{1.7} & \valstd{46.9}{2.1} & \valstd{81.9}{2.1} & \valstd{46.1}{4.7} & \valstd{83.3}{1.6} & \valstd{84.6}{1.2} & \valstd{85.2}{1.6} & \valstd{84.8}{1.2}\\
SeFT & \valstd{81.2}{2.3} & \valstd{41.9}{3.1} & \valstd{73.9}{2.5} & \valstd{31.1}{4.1} & \valstd{67.1}{2.2} & \valstd{70.0}{2.4} & \valstd{68.2}{1.5} & \valstd{68.5}{1.8}\\
mTAND & \valstd{84.4}{1.3} & \valstd{50.6}{2.0} & \valstd{84.2}{0.8} & \valstdu{48.2}{3.4} & \valstd{74.6}{4.3} & \valstd{74.3}{4.0} & \valstd{79.5}{2.8} & \valstd{76.8}{3.4}\\ 
IP-Net & \valstd{84.6}{1.3} & \valstd{38.1}{3.7} & \valstd{82.6}{1.4} & \valstd{47.6}{3.1} & \valstd{74.3}{3.8} & \valstd{75.6}{2.1} & \valstd{77.9}{2.2} & \valstd{76.6}{2.8}\\
DGM$^2$-O & \valstd{86.7}{3.4} & \valstd{44.7}{11.7} & \valstdu{84.4}{1.6} & \valstd{47.3}{3.6} & \valstd{82.4}{2.3} & \valstd{85.2}{1.2} & \valstd{83.9}{2.3} & \valstd{84.3}{1.8}\\
MTGNN & \valstd{81.9}{6.2}  & \valstd{39.9}{8.9} & \valstd{74.4}{6.7} & \valstd{35.5}{6.0} & \valstd{83.4}{1.9} & \valstd{85.2}{1.7} & \valstd{86.1}{1.9} & \valstd{85.9}{2.4} \\
Raindrop & \valstdu{87.0}{2.3} & \valstdu{51.8}{5.5} & \valstd{82.8}{1.7} & \valstd{44.0}{3.0} & \valstdu{88.5}{1.5} & \valstdu{89.9}{1.5} & \valstdu{89.9}{0.6} & \valstdu{89.8}{1.0}\\
\midrule
% ResNet & 80.1 $\pm$ 2.1 & 38.3 $\pm$ 2.7 &0 &0 &0 &0 &0 &0 &0\\
% \textbf{ViTST-ViT}  &87.6 $\pm$ 2.2 & 51.4 $\pm$ 1.8 & 83.5 $\pm$ 1.8 & 47.1 $\pm$ 3.3 & 94.6 $\pm$ 0.9 & 95.6 $\pm$ 1.0 & 95.0 $\pm$ 0.8 & 95.3 $\pm$ 0.9\\
\textbf{ViTST} &\valstdb{89.2}{2.0} &\valstdb{53.1}{3.4} &\valstdb{85.1}{0.8}  &\valstdb{51.1}{4.1} & \valstdb{95.8}{1.3} &\valstdb{96.2}{1.3} &\valstdb{96.1}{1.1} & \valstdb{96.5}{1.2}\\
% \midrule
% % ResNet+  \\
% ViT+ &87.6$\pm$2.2 & 51.4$\pm$1.8 &83.5$\pm$1.8 &47.1$\pm$3.3 &- &- &- &-\\
% Swin+ &\textbf{88.6$\pm$1.9} &\textbf{52.0$\pm$2.1} &\textbf{84.9$\pm$1.9} &48.0$\pm$3.4 &- &- &- &-\\
\bottomrule
\end{tabular}
}
% \vspace{-5mm}
\end{table*}

\textbf{Comparison to state-of-the-art.~}
We compare our approach with several state-of-the-art methods that are specifically designed for irregularly sampled time series, including Transformer~\citep{transformer}, Trans-mean (Transformer with an imputation method that replaces the missing value with the average observed value of the variable), GRU-D~\citep{gru-d}, SeFT~\citep{seft}, mTAND~\citep{mtand}, IP-Net~\citep{ipnet}, and Raindrop~\citep{raindrop}. In addition, we also compared our method with two approaches initially designed for forecasting tasks, namely DGM$^2$-O~\citep{dgm} and MTGNN~\citep{mtgnn}. The implementation and hyperparameter settings of these baselines were kept consistent with those used in Raindrop~\citep{raindrop}. Specifically, a batch size of 128 was employed, and all compared models were trained for 20 epochs. 
To ensure fairness in our evaluation, we averaged the performance of each method over 5 data splits that were kept consistent across all compared approaches.

As shown in Table~\ref{tab:main_result}, our proposed approach demonstrates strong performance against the specialized state-of-the-art algorithms on all three datasets. Specifically, on the P19 and P12 datasets, ViTST achieves improvements of 2.2\% and 0.7\% in absolute AUROC points, and 1.3\% and 2.9\% in absolute AUPRC points over the state-of-the-art results, respectively. For the PAM dataset, the improvement is even more significant, with an increase of 7.3\% in Accuracy, 6.3\% in Precision, 6.2\% in Recall, and 6.7\% in absolute F1 score points. 

\begin{figure}[!h]
\centering
\subfigure[Leave-\textbf{fixed}-sensors-out]{
\includegraphics[width=0.98\linewidth]{pic/fixed2.pdf}
}
\quad
\subfigure[Leave-\textbf{random}-sensors-out]{
\includegraphics[width=0.98\linewidth]{pic/random2.pdf}
}
\caption{Performance in leave-\textbf{fixed}-sensors-out and leave-\textbf{random}-sensors-out settings on PAM dataset. The x-axis is the ``missing ratio'' which denotes the ratio of masked variables. 
% Detailed numbers are provided in Table~\ref{tab:leave-sensors-out} in Appendix~\ref{sect:full_results}.
}
\label{fig:sensors}
\end{figure}

\textbf{Leaving-sensors-out settings.~}
We conducted additional evaluations to assess the performance of our model under more challenging scenarios, where the observations of a subset of sensors (variables) were masked during testing. This setting simulates real-world scenarios when some sensors fail or become unreachable. Following the approach adopted in \citep{raindrop}, we experimented with two setups using the PAM dataset: (1) \textit{leave-\textbf{fixed}-sensors-out}, which drops a fixed set of sensors across all samples and compared methods, and (2) \textit{leave-\textbf{random}-sensors-out}, which drops sensors randomly. It is important to note that only the observations in the validation and test sets were dropped, while the training set was kept unchanged. To ensure a fair comparison, we dropped the same set of sensors in the \textit{leave-\textbf{fixed}-sensors-out} setting as in \citep{raindrop}.
% Note that in our case, if a sensor is masked, the corresponding line graph is left blank.

The results are presented in Fig.~\ref{fig:sensors}, from which we observe that our approach consistently achieves the best performance and outperforms all the specialized baselines by a large margin. With the missing ratio increasing from 10\% to 50\%, our approach maintains strong performance, staying above 80\%. In contrast, the comparison baseline shows a marked drop. The advantage of ViTST over the comparison baselines becomes increasingly significant. Even when half of the variables were dropped, our approach was still able to achieve acceptable performance over 80, surpassing the best-performing baseline Raindrop by 33.1\% in Accuracy, 40.9\% in Precision, 39.4\% in Recall, and 42.8\% in F1 score in the \textit{leave-\textbf{fixed}-sensors-out} setting (all in absolute points). We also notice that the variances in our results are notably lower compared to the baselines. These results suggest that our approach is highly robust to varying degrees of missing observations in time series.
% which can be attributed to our image framework, enabling the use of image augmentation methods such as cutout~\citep{cutout}.

\subsection{Additional Analysis}\label{sect:ablation}

\begin{figure}[htbp]
\centering
\vspace{-3mm}
\subfigure[P19]{
\begin{minipage}[t]{0.295\linewidth}
\centering
\includegraphics[width=1.6in]{./pic/models-p19.png}
%\caption{fig1}
\end{minipage}%
}%
\subfigure[P12]{
\begin{minipage}[t]{0.295\linewidth}
\centering
\includegraphics[width=1.6in]{./pic/models-p12.png}
%\caption{fig2}
\end{minipage}%
}%
\subfigure[PAM]{
\begin{minipage}[t]{0.4\linewidth}
\centering
\includegraphics[width=1.86in]{./pic/models-pam.png}
%\caption{fig2}
\end{minipage}
}%
\centering
\caption{Performance of different backbone vision models on P19, P12, and PAM datasets. We do not use static features for our approach here to exclude their influence. }\label{fig:ablation1}
% \vspace{-2mm}
\end{figure}

\textbf{Where does the performance come from?~}
Our approach transforms time series into line graph images, allowing the use of vision transformers for time series modeling. We hypothesize that vision transformers can leverage their general-purpose image recognition ability acquired from large-scale pre-training on natural images (such as ImageNet~\citep{imagenet}) to capture informative patterns in the line graph images. To validate that, we compared the performance of a pre-trained Swin Transformer with a Swin Transformer trained from scratch, as shown in Fig.~\ref{fig:ablation1}. The significant drop in performance without pre-training provides evidence that Swin transformer could transfer the knowledge obtained from pre-training on natural images to our synthetic time series line graph images, achieving impressive performance. Nevertheless, the underlying mechanism needs further exploring and probing in future studies.
% , which is the primary source of performance improvement.

\textbf{How do different vision models perform?~}
We benchmarked several backbone vision models within our framework. Specifically, we tried another popular pre-trained vision transformer ViT\footnote{https://huggingface.co/google/vit-base-patch16-224-21k} and a pre-trained CNN-based model, ResNet\footnote{https://huggingface.co/microsoft/resnet-50}. 
The results are presented in Fig.~\ref{fig:ablation1}. The pre-trained transformer-based ViT and Swin Transformer demonstrate comparable performance, both outperforming the previous state-of-the-art method, Raindrop. In contrast, the pre-trained CNN-based ResNet lagged considerably behind the vision transformer models. This performance gap is consistent with observations in image classification tasks on datasets like ImageNet, where vision transformers have been shown to excel in preserving spatial information compared to conventional CNN models. This advantage enables vision transformers to effectively capture the positions of patches within each line graph sub-image and the entire image and facilitates the modeling of complex dynamics and relationships between variables.


\begin{table*}[!t]
\scriptsize
\centering
\caption{Ablation studies on different strategies of time series-to-image transformation.
% The reported numbers are averaged on 5 data splits. 
% We omit variances for simplicity. The numbers in brackets denote the performance change \textit{w.r.t.} the full model.
}
\label{tab:ablation_1}
\resizebox{\textwidth}{!}{ 
\begin{tabular}{l|cc|cc|cccc}
\toprule
& \multicolumn{2}{c|}{P19} & \multicolumn{2}{c|}{P12} & \multicolumn{4}{c}{PAM} \\ \cmidrule{2-9}
\multirow{-2}{*}{Methods} & AUROC & AUPRC & AUROC & AUPRC & Accuracy & Precision & Recall & F1 score \\ \midrule
Default & \valstd{89.2}{2.0} & \valstd{53.1}{3.4} & \valstd{85.1}{0.8} & \valstd{51.1}{4.1} & \valstd{95.8}{1.3} & \valstd{96.2}{1.1} & \valstd{96.2}{1.3} & \valstd{96.5}{1.2} \\ \midrule
~w/o interpolation & \valstd{89.6}{2.1} & \valstd{52.9}{3.4} & \valstd{85.7}{1.0} & \valstd{51.9}{3.4} & \valstd{95.6}{1.1} & \valstd{96.6}{0.9} & \valstd{95.9}{1.0} & \valstd{96.2}{1.0}\\
~w/o markers & \valstd{89.0}{2.1} & \valstd{51.7}{2.5} & \valstd{85.3}{0.9} & \valstd{50.3}{3.2} & \valstd{95.8}{1.1} & \valstd{96.9}{0.7} & \valstd{96.0}{1.0} & \valstd{96.4}{0.9} \\ 
~w/o colors & \valstd{88.8}{1.8} & \valstd{51.4}{4.1} & \valstd{84.4}{0.7} & \valstd{47.0}{2.9}& \valstd{95.0}{1.0} & \valstd{96.2}{0.7} & \valstd{95.3}{1.0} &  \valstd{95.7}{0.9}\\ 
~w/o order & \valstd{89.3}{2.3} & \valstd{52.7}{4.5} & \valstd{84.0}{1.8} & \valstd{47.8}{4.6} & - & - & - & - \\ 
\bottomrule
\end{tabular}
}
% \vspace{-3mm}
\end{table*}

\textbf{How to create time series line graph images?~}
Using line graphs to visualize time series offers us an intuitive way to interpret the data and adjust the visualization strategy for enhanced clarity and potentially augment the performance. To offer insights on effective time series-to-image transformation, we here analyze the effects of several key elements in practice: (1) the default linear \textit{interpolation} to connect partially observed data points on the line graphs; (2) the \textit{markers} to indicate observed data points; (3) the variable-specific \textit{colors} to differentiate between line graphs representing different variables; (4) the \textit{order} determined by missing ratios when organizing multiple line graphs in a single image.

The results are presented in Table~\ref{tab:ablation_1}. Given the balanced missing ratios in the PAM dataset, we excluded results without the sorting order. Interestingly, plotting only observed data points without linear interpolation led to better results on P19 and P12 datasets. This could be attributed to the potential inaccuracies introduced by interpolation, blurring distinctions between observed and interpolated points. Additionally, omitting markers complicates the model's task of discerning observed data points from interpolated ones, degrading its performance. The absence of distinctive colors led to the most significant performance drop, underlining the necessity of using varied hues for individual line graphs to help the model to distinguish them. While a specific sorting order may not ensure optimal outcomes across all datasets, it does provide relatively stable results over multiple datasets and evaluation metrics. For the PAM dataset, these nuances seem to have minimal impact, indicating the robustness of our approach against these variations in some scenarios.

\begin{figure}[htbp]
\centering
\subfigure[P19]{
\begin{minipage}[t]{0.295\linewidth}
\centering
\includegraphics[width=1.6in]{./pic/new-grid-p19.png}
%\caption{fig1}
\end{minipage}%
}%
\subfigure[P12]{
\begin{minipage}[t]{0.295\linewidth}
\centering
\includegraphics[width=1.6in]{./pic/new-grid-p12.png}
%\caption{fig2}
\end{minipage}%
}%
\subfigure[PAM]{
\begin{minipage}[t]{0.4\linewidth}
\centering
\includegraphics[width=1.86in]{./pic/new-grid-pam.png}
%\caption{fig2}
\end{minipage}
}%
\centering
\caption{Ablation study of the influence of grid layouts and image sizes. For instance, 4x9 (256x576) denotes a grid layout of 4$\times$9 with an image size of 256$\times$576 pixels.}\label{fig:grid}
\vspace{-5mm}
\end{figure}

\textbf{Effects of grid layouts and image sizes.~} We explored the influence of grid layouts and image dimensions on our approach's efficacy. For a fair comparison across grid layouts, we fixed the size of each grid cell as $64\times64$ and altered the grid layouts. As shown in Figure~\ref{fig:grid}. we observed our approach's robustness to variations in grid layouts, with square layout consistently yielding good results across different datasets and metrics, which was particularly evident for the P12 dataset. Regarding image size, when we maintained the grid layout but reduced the overall image dimensions, a noticeable performance decline was observed on the P12 and PAM datasets, which complies with our intuition.

\textbf{Robustness against varied plotting parameters.~}
To gauge the robustness of our approach against different plotting parameters, we assessed aspects including line style/width and marker style/size, primarily using the P19 dataset. As shown in Table~\ref{tab:plotting}, our approach demonstrates robustness against changes in these parameters, maintaining strong performance across different plotting configurations.

\begin{wraptable}{r}{0.5\linewidth}
\scriptsize
\centering
\vspace{-3mm}
\caption{Robustness regarding the style and size of lines and markers. In the brackets, the first element denotes style, and the second represents size.}
\label{tab:plotting}
\resizebox{0.48\textwidth}{!}{ 
\begin{tabular}{ll|cc}
    \toprule
    Line & Marker & AUROC & AUPRC \\ \midrule
    (solid,1) & ($\ast$,2) & \valstd{89.2}{2.0} & \valstd{53.1}{3.4} \\
    (dashed,1) & ($\ast$,2) & \valstd{89.2}{2.1} & \valstd{53.7}{4.1} \\
    (dotted,1) & ($\ast$,2) & \valstd{89.2}{2.1} & \valstd{52.8}{4.0} \\
    \rowcolor{Gray}
    (solid,0.5) & ($\ast$,2) & \valstd{88.6}{1.7} & \valstd{53.0}{3.6} \\
    \rowcolor{Gray}
    (solid,1) & ($\ast$,2) & \valstd{89.2}{2.0} & \valstd{53.1}{3.4} \\
    \rowcolor{Gray}
    (solid,2) & ($\ast$,2) & \valstd{88.5}{2.3} & \valstd{53.6}{3.1} \\
    (solid,1) & ($\ast$,2) & \valstd{89.2}{2.0} & \valstd{53.1}{3.4} \\
    (solid,1) & ($\wedge$,2) & \valstd{89.3}{1.9} & \valstd{52.6}{4.0} \\
    (solid,1) & ($\circ$,2) & \valstd{89.1}{1.9} & \valstd{51.3}{4.2} \\
    \rowcolor{Gray}
    (solid,1) & ($\ast$,1) & \valstd{88.2}{1.4} & \valstd{52.1}{4.5} \\
    \rowcolor{Gray}
    (solid,1) & ($\ast$,2) & \valstd{89.2}{2.0} & \valstd{53.1}{3.4} \\
    \rowcolor{Gray}
    (solid,1) & ($\ast$,3) & \valstd{88.9}{1.9} & \valstd{52.8}{3.2} \\
    \bottomrule
\end{tabular}
}
\end{wraptable}


\textbf{What does ViTST capture?~}
To gain insights into the patterns captured by ViTST in the time series line graph images, we analyzed the averaged attention map of a ViTST model with ViT as the backbone, as depicted in Fig.~\ref{fig:attention_map}. The attention map reveals that the model consistently attends to the informative part, i.e., line graph contours within the image.
Furthermore, we observed that the model appropriately focuses on observed data points and areas where the slopes of the lines change. Conversely, flat line graphs that lack dynamic patterns seem to receive less attention. This demonstrates that ViTST might be able to discern between informative and uninformative features in the line graph images, enabling it to extract meaningful patterns.

\begin{figure*}[!h]
\vspace{3mm}
    \centering
    \includegraphics[width=0.75\linewidth]{./pic/visualization4.pdf}
    \caption{Illustration of the averaged attention map of ViTST.
    }
    \label{fig:attention_map}
% \vspace{-3mm}
\end{figure*}

\subsection{Regular Time Series Classification}\label{sect:regular}
An advantage of our approach is its ability to model time series of diverse shapes and scales, whether they are regular or irregular. To evaluate the performance of our approach on regular time series data, we conducted experiments on ten representative multivariate time series datasets from the UEA Time Series Classification Archive~\citep{uea}. These datasets exhibit diverse characteristics, as summarized in Table~\ref{tab:regular_result}.
It is worth noting that the PS dataset in our evaluation contains an exceptionally high number of variables (963), while the EW dataset has extremely long time series (17984). We specifically selected these two datasets to assess the effectiveness of our approach in handling large numbers of variables and long time series.
We follow \citep{tst} to use these baselines for comparison: DTW$_D$ which stands for dimension-Dependent DTW combined with dilation-CNN~\citep{franceschi2019unsupervised}, LSTM~\citep{lstm}, XGBoost~\citep{xgboost}, Rocket~\citep{rocket}, and a transformer-based TST~\citep{tst} which operates on fully observed numerical time series. 
% We thus use the methods designed for regular time series as baselines.


\begin{table}[tbp]
    \footnotesize
    \centering
    \caption{Performance comparison on regular multivariate time series datasets. 
    % The EW dataset is not included in the average score for LSTM, XGBoost, and Rocket. 
    % ``Avg. Acc.'' denotes the average accuracy on all datasets. 
    \textbf{Bold} indicates the best performer, while \underline{underline} represents the second best. 
    }
    \scalebox{0.92}{
    \begin{tabular}{lcccccccccc|c}
    \toprule
    Datasets &EC &UW &SCP1 &SCP2 &JV &SAD &HB &FD &PS &EW & Average\\\midrule
    \rowcolor{Gray}
\multicolumn{12}{c}{\textit{Dataset statistics}} \\
\#Variables &3 &3 &6 &7 &12 &13 &61 &144 &\textbf{963} &6 & - \\
Length &1,751 &315 &896 &1,152 &29 &93 &405 &62 &144 &\textbf{17984} & -\\
    \rowcolor{Gray}
\multicolumn{12}{c}{\textit{Model performances}} \\
    DTW$_D$ &0.323 &0.903 &0.775 &0.539 &0.949 &0.963 &0.717 &0.529 &0.711 &0.618 & 0.717 \\
    LSTM & 0.323 &0.412 &0.689 &0.466 &0.797 &0.319 &0.722 &0.577 &0.399 &- &0.523 \\
    XGBoost &0.437 &0.759 &0.846 &0.489 &0.865 &0.696 &0.732 &\underline{0.633} &\textbf{0.983} &- &0.727 \\
    Rocket &\underline{0.452} &\textbf{0.944} &\underline{0.908} &0.533 &\underline{0.962} &0.712 &0.756 &0.647 &0.751 &- &0.741 \\
    TST &0.326 &\underline{0.913} &\textbf{0.922} &\textbf{0.604} &\textbf{0.997} &\textbf{0.998} &\textbf{0.776} &\textbf{0.689} &0.896 &\underline{0.748} &\textbf{0.791} \\
    ViTST &\textbf{0.456} &0.862 &0.898 &\underline{0.561} &0.946 &\underline{0.985} &\underline{0.766} &0.632 &\underline{0.913} &\textbf{0.878} &\underline{0.780} \\
    \bottomrule
    \end{tabular}
    }
    \label{tab:regular_result}
    % \vspace{-5mm}
\end{table}

The performance of our approach on regular time series datasets is consistently strong, as demonstrated in Table~\ref{tab:regular_result}. With an average accuracy that is second-best and closely aligned with the top-performing baseline method TST, our approach showcases its competitive capabilities. Notably, it excels on challenging datasets PS and EW with massive variables and observation length. These results were achieved using the same image resolution ($384\times384$) as the other datasets, indicating the effectiveness and efficiency of our approach. The ability of our approach to handle both irregular and regular time series data further emphasizes its versatility and broad applicability.


\section{Conclusion}
In this paper, we introduce a novel perspective on modeling irregularly sampled time series. By transforming time series data into line graph images, we could effectively leverage the strengths of pre-trained vision transformers. This approach is straightforward yet effective and versatile, enabling the modeling of time series of diverse characteristics, regardless of irregularity, different structures, and scales. Through extensive experiments, we demonstrate that our approach surpasses state-of-the-art methods designed for irregular time series and maintains strong robustness to varying degrees of missing observations. Additionally, our approach achieves promising results on regular time series data. We envision its potential as a general-purpose framework for various time series tasks. Our results underscore the potential of adapting rapidly advancing computer vision techniques to time series modeling. We anticipate that this will inspire further exploration, fostering deeper understanding and expansion in this interdisciplinary field.

\section{Limitations and Future Work}
In this work, we utilized a straightforward method to image multivariate time series by converting them into line graph images using matplotlib and then saving them as RGB images. While our results are promising and exhibit robustness against variations in the time series-to-image transformation process, there may be alternative ways to visualize the data. This includes potentially more controllable and accurate plotting methods or different image representations beyond line graphs. Our findings also highlight the efficacy of pre-trained vision transformers for time series classification, suggesting that these models might leverage knowledge acquired from pre-training on natural images. Yet, the underlying reasons for their remarkable success still need deeper exploration and investigation. This research serves as a promising starting point in this domain, suggesting various potential directions. We leave these further explorations and investigations for future work.

\bibliography{example_paper}
\bibliographystyle{neurips_2023}



% \subsubsection*{Acknowledgments}
% Use unnumbered third level headings for the acknowledgments. All
% acknowledgments, including those to funding agencies, go at the end of the paper.


% In the unusual situation where you want a paper to appear in the
% references without citing it in the main text, use \nocite
\nocite{langley00}

\newpage
\appendix

\section{More Details on Time Series Line Graph Image Creation}\label{sect:image_creation}
% \subsection{Implementation}
\paragraph{Implementation}
The time-series-to-image transformation can be implemented using the Matplotlib package\footnote{https://matplotlib.org/} with the following few lines of code.
% Some examples of created images with different strategies for the three datasets are shown in Fig.~\ref{fig:created_images}. 
\begin{figure}[!hb]
\vspace{-2mm}
\centering
{\footnotesize
\begin{minted}[fontsize=\small,linenos]{python}
def TS2Image(t, v, D, colors, image_height, image_width, grid_height, grid_width):
    import matplotlib.pyplot as plt
    plt.figure(figsize=(image_height/100, image_width/100), dpi=100)
    for d in range(D): # enumerate the multiple variables
        plt.subplot(grid_height, grid_width, d+1) # position in the grid
        # plot line graph of variable d
        plt.plot(t[d], v[d], color=colors[d], linestyle="-", marker="*") 
\end{minted}
}
\vspace{-2mm}
% \caption{Implementation of time-series-to-image transformation in Python using Matplotlib package.}
\label{fig:code}
\end{figure}

\vspace{-3mm}\paragraph{Axis Limits of Line Graphs} The axis limits determine the plot area of the line graphs and the range of displayed timestamps and values. By default, we set the limits of the x-axis and y-axis as the ranges of all the observed timestamps and values. However, we found that some extreme observed values for some variables can largely expand the range of the y-axis, causing most plotted points of observations to cluster in a small area and resulting in flat line graphs. Common normalization and standardization methods will not solve this issue, as the relative magnitudes remain unchanged in the created images. We thus tried the following strategies to remove extreme values and narrow the range of the y-axis in our preliminary experiments:
\begin{itemize}
    \item{Interquartile Range (IQR)}: IQR is one of the most extensively used methods for outlier detection and removal. The interquartile range is calculated based on the first and third quartiles of all the observed values of each variable in the dataset and then used to calculate the upper and lower limits.
    \item{Standard Deviation (SD)}: The upper and lower boundaries are calculated by taking 3 standard deviations from the mean of observed values for each variable across the dataset. This method usually assumes the data is normally distributed.
    \item{Modified Z-score (MZ)}: A z-score measures how many standard deviations away a value is from the mean and is similar to the standard deviation method to detect outliers. However, z-scores can be influenced by extreme values, which modified z-scores can better handle. We set the upper and lower limits as the values whose modified z-scores are 3.5 and -3.5.
\end{itemize}
We show examples of the created images with these strategies in Figure~\ref{fig:created_images}.


\begin{table}[htbp]
\scriptsize
\centering
\caption{Preliminary experiments on different strategies to decide the line graph limit. The default strategy is to directly set the axis limit as the range of all observed values on the dataset. ``IQR'', ``SD'', and ``MZS' denote three strategies to remove extreme value, \textit{i.e.,} Interqurtile Range, Standard Deviation, and Modified Z-score. The reported numbers are averaged on 5 data splits.}
% \vspace{-2mm}
\label{tab:ablation_2}
\resizebox{\textwidth}{!}{ 
\begin{tabular}{l|cc|cc|cccc}
\toprule
& \multicolumn{2}{c|}{P19} & \multicolumn{2}{c|}{P12} & \multicolumn{4}{c}{PAM} \\ \cmidrule{2-9}
\multirow{-2}{*}{Strategies} & AUROC & AUPRC & AUROC & AUPRC & Accuracy & Precision & Recall & F1 score \\ \midrule
Default &\valstdb{89.4}{1.9} &\valstdb{52.8}{3.8} &\valstdb{85.6}{1.1} & \valstdb{49.8}{2.5} & \valstd{96.1}{0.7} & \valstd{96.8}{1.1} & \valstd{96.5}{0.7} & \valstd{96.6}{0.9}\\ \midrule
IQR & \valstd{88.2}{0.8} & \valstd{49.6}{1.7} & \valstd{84.5}{1.1} & \valstd{48.9}{2.6} & \valstd{95.9}{0.7} & \valstd{96.8}{0.7} & \valstd{96.1}{0.7} & \valstd{96.4}{0.7}\\
SD & \valstd{87.4}{1.6} & \valstd{51.2}{3.6} & \valstd{84.6}{1.7} & \valstd{47.1}{2.9} & \valstdb{96.6}{0.9} & \valstdb{97.1}{0.8} & \valstdb{97.0}{0.6} & \valstdb{97.0}{0.7}  \\
MZS & \valstd{87.3}{1.0} & \valstd{50.8}{3.7}& \valstd{84.3}{1.4} & \valstd{47.1}{2.1} & \valstd{96.0}{1.1} & \valstd{96.8}{0.9}9 & \valstd{96.4}{0.9} & \valstd{96.6}{0.9}\\
\bottomrule
\end{tabular}
}
% \vspace{-2mm}
\end{table}

The performance comparison of models trained on images created with different strategies is shown in Table~\ref{tab:ablation_2}. We observe that the methods that remove extreme values hurt the performance, except for SD on the PAM dataset. Although these methods narrow the value range and highlight the dynamic patterns of line graphs, they discard the extreme values that might be informative themselves. Therefore, we stick with the default way that sets the axis limits as the range of all observed values.
% This observation suggests that our approach may not require additional data preprocessing on the time series, further demonstrating its advantage in simplicity.


\begin{figure}[htbp]
	\centering
	
	\subfigure{
        \rotatebox{90}{\scriptsize{~~~~~~~~~~~~~~~~~~~~~~~~~Default}}
		\begin{minipage}[t]{0.3\linewidth}
			\centering
			\includegraphics[width=0.95\linewidth]{pic/p19/p000019.png}
		\end{minipage}
	}
	\subfigure{
		\begin{minipage}[t]{0.3\linewidth}
			\centering
			\includegraphics[width=0.95\linewidth]{pic/p12/132584.png}
		\end{minipage}
	}
	\subfigure{
		\begin{minipage}[t]{0.3\linewidth}
			\centering
			\includegraphics[width=0.95\linewidth]{pic/pam/0.png}
		\end{minipage}
	}
	
	\vspace{-2mm}
	\setcounter{subfigure}{0}

 %    	\subfigure{
 %        \rotatebox{90}{\scriptsize{~~~~~~~~~~~~~~~~~~~~~~~~~~~w/o interpolation}}
	% 	\begin{minipage}[t]{0.3\linewidth}
	% 		\centering
	% 		\includegraphics[width=0.95\linewidth]{pic/p19/p000019_removal.png}
	% 	\end{minipage}
	% }
	% \subfigure{
	% 	\begin{minipage}[t]{0.3\linewidth}
	% 		\centering
	% 		\includegraphics[width=0.95\linewidth]{pic/p12/132584_removal.png}
	% 	\end{minipage}
	% }
	% \subfigure{
	% 	\begin{minipage}[t]{0.3\linewidth}
	% 		\centering
	% 		\includegraphics[width=0.95\linewidth]{pic/pam/0_removal.png}
	% 	\end{minipage}
	% }
	
	% \vspace{-2mm}
	% \setcounter{subfigure}{0}

    	\subfigure{
        \rotatebox{90}{\scriptsize{~~~~~~~~~~~~~~~~~~~~~~~~~~~IQR}}
		\begin{minipage}[t]{0.3\linewidth}
			\centering
			\includegraphics[width=0.95\linewidth]{pic/p19/p000019_iqr.png}
		\end{minipage}
	}
	\subfigure{
		\begin{minipage}[t]{0.3\linewidth}
			\centering
			\includegraphics[width=0.95\linewidth]{pic/p12/132584_iqr.png}
		\end{minipage}
	}
	\subfigure{
		\begin{minipage}[t]{0.3\linewidth}
			\centering
			\includegraphics[width=0.95\linewidth]{pic/pam/0_iqr.png}
		\end{minipage}
	}
	
	\vspace{-2mm}
	\setcounter{subfigure}{0}

    	\subfigure{
        \rotatebox{90}{\scriptsize{~~~~~~~~~~~~~~Standard Deviation}}
		\begin{minipage}[t]{0.3\linewidth}
			\centering
			\includegraphics[width=0.95\linewidth]{pic/p19/p000019_sd.png}
		\end{minipage}
	}
	\subfigure{
		\begin{minipage}[t]{0.3\linewidth}
			\centering
			\includegraphics[width=0.95\linewidth]{pic/p12/132584_sd.png}
		\end{minipage}
	}
	\subfigure{
		\begin{minipage}[t]{0.3\linewidth}
			\centering
			\includegraphics[width=0.95\linewidth]{pic/pam/0_sd.png}
		\end{minipage}
	}
	
	\vspace{-2mm}
	\setcounter{subfigure}{0}
 
    \subfigure[P19]{
        \rotatebox{90}{\scriptsize{~~~~~~~~~~~~~~~~~Modified Z-score}}
		\begin{minipage}[t]{0.3\linewidth}
			\centering
			\includegraphics[width=0.95\linewidth]{pic/p19/p000019_mzs.png}
		\end{minipage}
	}
	\subfigure[P12]{
		\begin{minipage}[t]{0.3\linewidth}
			\centering
			\includegraphics[width=0.95\linewidth]{pic/p12/132584_mzs.png}
		\end{minipage}
	}
	\subfigure[PAM]{
		\begin{minipage}[t]{0.3\linewidth}
			\centering
			\includegraphics[width=0.95\linewidth]{pic/pam/0_mzs.png}
		\end{minipage}
	}
	\caption{The images created with different strategies for three samples from P19, P12, and PAM dataset, respectively (sample ``p000019'' for P19, ``132548'' for P12, and ``0'' for PAM).}
	\label{fig:created_images}
 \vspace{-5mm}
\end{figure}


\begin{table}[htbp]
    \vspace{-3mm}
    \caption{The inference time (in seconds) of different methods on the test sets.}
    \centering
    \small
    % \scalebox{0.9}{
    \begin{tabular}{lcccccccc}
    \toprule
    Datasets & Transformers & mTAND & SeFT & Raindrop & MTGNN & DGM$^2$-O & GRU-D & ViTST \\\midrule
    P19 & 0.21 & 0.52 & 2.72 & 3.05 & 3.62 & 2.47 & 31.04 & 44.51 \\
    P12 & 0.12 & 0.44 & 0.97 & 1.27 & 1.46 & 2.80 &10.13 & 12.14 \\ 
    PAM & 0.06 & 0.23 & 0.89 & 0.67 & 1.16 & 2.98 & 4.55 & 5.30\\ 
    \bottomrule
    \end{tabular}
    % }
    \label{tab:time}
    \vspace{-3mm}
\end{table}

\paragraph{Computation cost}
We list the inference time (in seconds) of different methods on the test sets of three datasets in Table~\ref{tab:time}. All the inferences are made in a single Nvidia A6000 GPU. It is observed that our vision-based method consumes more inference time than the non-vision baselines. However, we believe this cost remains within an acceptable range in the context of today's ML practice and medical applications, considering the inference on each sample only costs around 0.01 seconds.


\section{More Experimental Details}\label{sect:experiment_details}


\subsection{Datasets}\label{sect:dataset}
We used the datasets processed by \citep{raindrop}, whose details are given below.

\textbf{P19: PhysioNet Sepsis Early Prediction Challenge 2019.~\footnote{\url{https://physionet.org/content/challenge-2019/1.0.0/}}}
The P19 dataset~\citep{p19} consists of clinical data for 38,803 patients, and aims to predict whether sepsis will occur within the next 6 hours. The dataset includes 34 irregularly sampled sensors with 8 vital signs and 26 laboratory values for each patient, as well as 6 demographic features. To process the static features, we use templates outlined in Table~\ref{tab:dataset_template}, and utilize a pre-trained Roberta-base model to extract textual features. These textual features are then combined with visual features obtained from the vision transformer to perform binary classification. The dataset is highly imbalanced with only 4\% of samples being positive, and has a missing ratio of 94.9\%.

\textbf{P12: PhysioNet Mortality Prediction Challenge 2012.~\footnote{\url{https://physionet.org/content/challenge-2012/1.0.0/}}}
P12 dataset~\citep{p12} includes clinical data from 11,988 ICU patients, with 36 irregularly sampled sensor observations and 6 static demographic features provided for each patient. The goal is to predict patient mortality, which is a binary classification task. The dataset is highly imbalanced, with around 86\% of samples being negative. The missing ratio of the dataset is 88.4\%.

\textbf{PAM: PAMAP2 Physical Activity Monitoring.~\footnote{\url{https://archive.ics.uci.edu/ml/datasets/pamap2+physical+activity+monitoring}}}
The PAM dataset originally contains data of 18 physical activities with 9 subjects wearing 3 inertial measurement units. However, to make it suitable for irregular time series classification, \cite{raindrop} excluded the ninth subject due to its short length of sensor readouts, and 10 out of the 18 activities that had less than 500 samples were also excluded. As a result, the task is an 8-way classification with 5,333 samples, each with 600 continuous observations. To simulate the irregular time series setting, 60\% of the observations are randomly removed. No static features are provided, and the 8 categories are approximately balanced. The missing ratio is 60.0\%.


\begin{table}[!ht]
\centering
\caption{Templates for transforming static features to natural language sentences.}
\label{tab:dataset_template}
\footnotesize
% \addtolength{\tabcolsep}{-0.65ex}
\resizebox{0.9\textwidth}{!}{ 
\begin{tabular}{c|p{1.3in}|p{1.8in}|p{1.3in}} \toprule
Dataset & Static features & Template & Example \\ \hline
\multirow{5}{*}{P19} & \textit{Age}, \textit{Gender}, \textit{Unit1 (medical ICU)}, \textit{Unit2 (surgery ICU)}, \textit{HospAdmTime}; \textit{ICULOS (ICU length-of-stay)} & A patient is \{\textit{Age}\} years old, \{\textit{Gender}\}, went to \{\textit{Unit1\&Unit2}\} \{\textit{HospAdmTime}\} hours after hospital admit, had stayed there for \{\textit{ICULOS}\} hours. & A patient is 65 years old, female, went to the medical ICU 10 hours after hospital admit, had stayed there for 20 hours. \\ \cline{1-4}
\multirow{3}{*}{P12} & \textit{RecordID}, \textit{Age}, \textit{Gender}, \textit{Height} (cm), \textit{ICUType}, \textit{Weight} (kg) & A patient is \{\textit{Age}\} years old, \{\textit{Gender}\}, \{\textit{Height}\} cm, \{\textit{Weight}\} kg, stayed in \{\textit{ICUType}\}. & A patient is 48 years old, male, 171 cm, 78 kg, stayed in surgical ICU. \\ 
\bottomrule
\end{tabular}
}
\end{table}

\begin{table*}[!h]
\scriptsize
\centering
\caption{Ablation studies on different methods to encode static features.
% The reported numbers are averaged on 5 data splits. 
% We omit variances for simplicity. The numbers in brackets denote the performance change \textit{w.r.t.} the full model.
}
\label{tab:static_feature}
\vspace{-2mm}
\resizebox{0.6\textwidth}{!}{ 
\begin{tabular}{l|cc|cc}
\toprule
& \multicolumn{2}{c|}{P19} & \multicolumn{2}{c}{P12} \\ \cmidrule{2-5}
\multirow{-2}{*}{Methods} & AUROC & AUPRC & AUROC & AUPRC \\ \midrule
Raindrop & \valstd{87.0}{2.3} & \valstd{51.8}{5.5} & \valstd{82.8}{1.7} & \valstd{44.0}{3.0} \\ \midrule
Swin & \valstd{89.4}{1.8} & \valstd{50.2}{3.0} & \valstd{84.3}{0.6} & \valstd{49.3}{3.7} \\
Swin-MLP & \valstd{88.6}{1.3} & \valstd{51.4}{3.7} & \valstd{84.6}{0.9} & \valstd{48.7}{3.2}  \\ 
Swin-Roberta & \valstd{89.4}{1.9} & \valstd{52.8}{3.8} & \valstd{85.6}{1.1} & \valstd{49.8}{2.5} \\ 
% ~w/o order & 85.3 $\pm$ 0.8 & 48.5 $\pm$ 2.1 & 83.9 $\pm$ 1.1 & 46.5 $\pm$ 3.2 & - & - & - & - \\ 
\bottomrule
\end{tabular}
}
\vspace{-4mm}
\end{table*}

\subsection{Experiments on Static Features}
Time series data is often associated with information from other modalities, such as the textual clinical notes in electronic health records (EHRs) in the healthcare domain. Our approach is naturally suitable for incorporating such information since we convert time series data to images, and thus various vision-language and multi-modal techniques can be utilized to incorporate the visual (time series) information and information from other modalities. 
For example, the CLIP~\citep{clip} learns a shared hidden feature space where the paired image and text stay close. Under our framework, such a shared space can also be learned for the paired visual time series images and textual clinical notes, which is our future direction. It also paves the way for the application of multi-modal models such as GPT-4~\citep{openai2023gpt4} to handle the visualized time series data and the clinical notes simultaneously. 
In our current experiments, we used a text encoder, Roberta-base, to encode textual demographic information in the P19 and P12 datasets. We also experimented with normalizing the original categorical features and encoding them using an MLP as in previous work, and compare with the strong baseline, Raindrop. The results are shown in Table~\ref{tab:static_feature}. We observe that even without using static features, our method has already outperformed Raindrop. In addition, utilizing Roberta to encode and incorporate the textual feature is more effective than applying MLP over categorical features. 

\begin{table}[htbp]
\scriptsize
\centering
\caption{Preliminary experiments on two settings to fuse the line graph images for different variables. The default setting is to first arrange all line graph images into a single image and then learn the representation for classification. \textbf{ViT-subimage} stands for that we first learn the representation for each line graph subimage separately and then concatenate their representation for classification.}
% \vspace{-2mm}
\label{tab:subimage}
\resizebox{\textwidth}{!}{ 
\begin{tabular}{l|cc|cc|cccc}
\toprule
& \multicolumn{2}{c|}{P19} & \multicolumn{2}{c|}{P12} & \multicolumn{4}{c}{PAM} \\ \cmidrule{2-9}
\multirow{-2}{*}{Strategies} & AUROC & AUPRC & AUROC & AUPRC & Accuracy & Precision & Recall & F1 score \\ \midrule
ViT &\valstd{87.9}{2.5} &\valstd{51.6}{3.7} &\valstd{84.8}{1.3} & \valstd{48.1}{3.8} & \valstd{93.4}{0.7} & \valstd{94.7}{0.9} & \valstd{94.1}{0.7} & \valstd{94.3}{0.7}\\ 
ViT-subimage &\valstd{85.1}{1.5} &\valstd{47.9}{3.4} &\valstd{77.6}{3.2} & \valstd{35.5}{6.2} & \valstd{90.4}{1.3} & \valstd{92.9}{1.0} & \valstd{91.1}{0.9} & \valstd{91.9}{1.0}\\
\bottomrule
\end{tabular}
}
% \vspace{-2mm}
\end{table}

 \subsection{Experiments on Image Fusion}
In our preliminary experiments, we examined two distinct approaches to fusing the line graph sub-images in each multivariate time series data. First, we processed each sub-image independently to learn the patch/image representations and then concatenated their respective patch representations to input into the final prediction layer. In contrast, our default method aggregates all sub-images into a single image to learn the patch embeddings. The key distinction between these strategies is whether a patch can attend to those from other sub-images and how position embedding factors in during the representation learning phase. Aside from this, other parameters were consistent across both methods. We tested on ViT and the performance comparisons with these two settings are shown in Table~\ref{tab:subimage}. It is evidenced that attending to patches from other sub-images offers advantages. This likely allows for capturing cross-variable correlations at a granular level within the self-attention layers, as opposed to only at the final linear prediction layer.



\begin{table}[htbp]
    \caption{Statistics and hyperparameter settings of evaluated regular multivariate time series datasets. }
    \centering
    \small
    \scalebox{0.9}{
    \begin{tabular}{lcccccccc}
    \toprule
    Datasets & \#Variables & \#Classes & Length & Train size & Grid layout & Image size & Learning rate & Epochs \\\midrule
    EC & 3 & 4 & 1,751 & 261 & $2\times2$ & $256\times256$ & 1e-4 & 20\\ 
    UW & 3 & 8 & 315 & 120 & $2\times2$ & $256\times256$ & 1e-4 & 100\\ 
    SCP1 & 6 & 2 & 896 & 268 & $2\times3$ & $256\times384$ & 1e-4 & 100\\ 
    SCP2 & 7 & 2 & 1,152 & 200 & $3\times3$ & $384\times384$ & 5e-5 & 100\\ 
    JV & 12 & 9 & 29 & 270 & $4\times4$ & $384\times384$ & 1e-4 & 100\\ 
    SAD & 13 & 10 & 93 & 6599 & $4\times4$ & $384\times384$ & 1e-5 & 20\\ 
    HB & 61 & 2 & 405 & 204 & $4\times4$ & $384\times384$ & 1e-4 & 100\\ 
    FD & 144 & 2 & 62 & 5890 & $12\times12$ & $384\times384$ & 5e-4 & 100\\ 
    PS & \textbf{963} & 7 & 144 & 267 & $32\times32$ & $384\times384$ & 5e-4 & 100\\ 
    EW & 6 & 5 & \textbf{17984} & 128 & $2\times3$ & $256\times384$ & 2e-5 & 100\\ 
    \bottomrule
    \end{tabular}
    }
    \label{tab:regular_dataset}
    \vspace{-2mm}
\end{table}

\subsection{Experiment on Regular Time Series}\label{sect:regular_ts}
We selected ten representative multivariate time series datasets from the UEA Time Series Classification Archive~\cite{uea} with diverse characteristics, including the number of classes, variables, and time series length. The datasets we chose are EthanolConcentration (EC), Handwriting (HW), UWaveGestureLibrary (UW), SelfRegulationSCP1 (SCP1), SelfRegulationSCP2 (SCP2), JapaneseVowels (JV), SpokenArabicDigits (SAD), Heartbeat (HB), FaceDetection (FD), PEMS-SF (PS), and EigenWorms (EW). Notably, the PS dataset has an exceptionally high number of variables (963), while the EW dataset has extremely long time series (17984). These two datasets allow us to assess the effectiveness of our approach when dealing with large numbers of variables and long time series. We applied different image sizes according to the grid layouts for these datasets. The hyperparameter settings are provided in Table~\ref{tab:regular_dataset}, and we applied cutout~\citep{cutout} data augmentation methods to SCP1, SCP2, and JV datasets due to the small size of their training sets.


\subsection{Self-supervised Learning}\label{sect:mim}
We preliminary explored masked image modeling self-supervised pre-training on the time series line graph images. We randomly mask columns of patches with a width of 32 on each line graph within a grid cell. The masking ratio is set as 50\%. We finetuned the Swin Transformer model for 10 epochs with batch size 48. The learning rate is 2e-5. Following~\cite{mim}, we use a linear layer to reconstruct the pixel values and employ an $\ell_1$ loss on the masked pixels:
\begin{equation}
\mathcal{L}=\frac{1}{\Omega(\bf{p}_{\text{M}})}\left \| \hat{\bf{p}_{\text{M}}} - \bf{p}_{\text{M}} \right \|_1,
\end{equation}
where $\bf{p}_{\text{M}}$ and $\hat{\bf{p}_{\text{M}}}$ are the masked and reconstructed pixels, respectively; $\Omega(\cdot)$ denotes the number of elements.
With self-supervised masked image modeling, the performance improves 1.0 AUPRC points from 52.8 ($\pm$ 3.8) to 53.8 ($\pm$ 3.2). The AUROC points slightly dropped from 89.4 ($\pm$ 1.9) to 88.9 ($\pm$ 2.1).

% After self-supervised pre-training, we further finetuned the model on the classification task with the same setting as directly finetuning: 2 epochs on the upsampled dataset with a learning rate of 2e-5. The performance improved by 1.0 AUPRC points from \valstd{52.8}{3.8} to \valstd{53.8}{3.2}. The AUROC points slightly dropped 0.5 points: from \valstd{89.4}{1.9} to \valstd{88.9}{2.1}. We also experimented with P12 and PAM datasets. However, there are no significant differences, which might be because of their relatively small dataset sizes (11988 for P12 and 5333 for the PAM dataset).
% Note that we did not perform extensive hyperparameter search in the preliminary explorations. We believe this is worth further explorations, which we leave for future work.




\subsection{Full Experimental Results}\label{sect:full_results}
% We presented the full experimental results in the leave-sensors-out settings in Table~\ref{tab:leave-sensors-out}, and the full results of ablation studies on backbone vision models are presented in Table~\ref{tab:vision_models}.
We presented the full experimental results in the leave-sensors-out settings in Table~\ref{tab:leave-sensors-out}.

\begin{table*}[htbp]
\scriptsize
\centering
\caption{Full results in the leave-sensors-out settings on PAM dataset. The ``missing ratio'' denotes the ratio of masked variables.}
\resizebox{\textwidth}{!}{
\begin{tabular}{cl|llll|llll}
\toprule
\multicolumn{1}{c}{\multirow{2}{*}{\begin{tabular}[c]{@{}l@{}}Missing \\ ratio\end{tabular}}}  & \multirow{2}{*}{Methods} & \multicolumn{4}{c|}{PAM (Leave-\textbf{fixed}-sensors-out)} & \multicolumn{4}{c}{PAM (Leave-\textbf{random}-sensors-out)} \\ \cmidrule{3-10}
\multicolumn{1}{l}{} &  & Accuracy & Precision & Recall & F1 score & Accuracy & Precision & Recall & F1 score \\ \midrule
\multirow{7}{*}{10\%} & Transformer & \valstd{60.3}{2.4} & \valstd{57.8}{9.3} & \valstd{59.8}{5.4} & \valstd{57.2}{8.0} & \valstd{60.9}{12.8} & \valstd{58.4}{18.4} & \valstd{59.1}{16.2} & \valstd{56.9}{18.9} \\
 & Trans-mean & \valstd{60.4}{11.2} &\valstd{61.8}{14.9} &\valstd{60.2}{13.8} &\valstd{58.0}{15.2}&\valstd{62.4}{3.5}&\valstd{59.6}{7.2}&\valstd{63.7}{8.1}&\valstd{62.7}{6.4}\\
 & GRU-D & \valstd{65.4}{1.7} & \valstd{72.6}{2.6}& \valstd{64.3}{5.3} & \valstd{63.6}{0.4} &\valstd{68.4}{3.7} & \valstd{74.2}{3.0} & \valstd{70.8}{4.2} & \valstd{72.0}{3.7} \\
 & SeFT & \valstd{58.9}{2.3} & \valstd{62.5}{1.8} & \valstd{59.6}{2.6} & \valstd{59.6}{2.6}& \valstd{40.0}{1.9}& \valstd{40.8}{3.2} & \valstd{41.0}{0.7} & \valstd{39.9}{1.5} \\
 & mTAND & \valstd{58.8}{2.7} & \valstd{59.5}{5.3} & \valstd{64.4}{2.9} & \valstd{61.8}{4.1} & \valstd{53.4}{2.0} & \valstd{54.8}{2.7} & \valstd{57.0}{1.9} & \valstd{55.9}{2.2} \\
 & Raindrop & \valstd{77.2}{2.1} & \valstd{82.3}{1.1} & \valstd{78.4}{1.9} & \valstd{75.2}{3.1} & \valstd{76.7}{1.8} & \valstd{79.9}{1.7} & \valstd{77.9}{2.3} & \valstd{78.6}{1.8}\\ 
 % \cmidrule{2-10}
 \rowcolor{Gray}
 & \textbf{ViTST}  & \valstdb{92.8}{1.6} & \valstdb{94.2}{1.3} & \valstdb{93.4}{1.8} & \valstdb{93.7}{1.6} & \valstdb{93.1}{0.9} & \valstdb{94.3}{0.9} & \valstdb{94.0}{1.2} & \valstdb{94.1}{1.1} \\ \midrule
\multirow{7}{*}{20\%} & Transformer & \valstd{63.1}{7.6} & \valstd{71.1}{7.1} & \valstd{62.2}{8.2} & \valstd{63.2}{8.7} & \valstd{62.3}{11.5} & \valstd{65.9}{12.7} & \valstd{61.4}{13.9} & \valstd{61.8}{15.6} \\
 & Trans-mean & \valstd{61.2}{3.0} &\valstd{74.2}{1.8} &\valstd{63.5}{4.4} &\valstd{64.1}{4.1}&	\valstd{56.8}{4.1} &\valstd{59.4}{3.4} &	\valstd{53.2}{3.9}&	\valstd{55.3}{3.5} \\
 & GRU-D & \valstd{64.6}{1.8} & \valstd{73.3}{3.6} & \valstd{63.5}{4.6} & \valstd{64.8}{3.6} & \valstd{64.8}{0.4} & \valstd{69.8}{0.8} & \valstd{65.8}{0.5} & \valstd{67.2}{0.0} \\
 & SeFT & \valstd{35.7}{0.5} & \valstd{42.1}{4.8} & \valstd{38.1}{1.3} & \valstd{35.0}{2.2} & \valstd{34.2}{2.8} & \valstd{34.9}{5.2} & \valstd{34.6}{2.1} & \valstd{33.3}{2.7}\\
 & mTAND & \valstd{33.2}{5.0} & \valstd{36.9}{3.7} & \valstd{37.7}{3.7} & \valstd{37.3}{3.4} & \valstd{45.6}{1.6} & \valstd{49.2}{2.1} & \valstd{49.0}{1.6} & \valstd{49.0}{1.0} \\
 & Raindrop & \valstd{66.5}{4.0} & \valstd{72.0}{3.9} & \valstd{67.9}{5.8} & \valstd{65.1}{7.0} & \valstd{71.3}{2.5} & \valstd{75.8}{2.2} & \valstd{72.5}{2.0} & \valstd{73.4}{2.1}\\ 
 % \cmidrule{2-10}
  \rowcolor{Gray}
 & \textbf{ViTST}  & \valstdb{89.7}{1.7} & \valstdb{91.0}{1.4} & \valstdb{90.9}{1.9} & \valstdb{90.8}{1.6} & \valstdb{92.0}{1.4} & \valstdb{93.4}{1.2} & \valstdb{92.8}{1.6} & \valstdb{93.0}{1.4} \\ \midrule
\multirow{5}{*}{30\%} & Transformer & \valstd{31.6}{10.0} & \valstd{26.4}{9.7} & \valstd{24.0}{10.0} & \valstd{19.0}{12.8} & \valstd{52.0}{11.9} & \valstd{55.2}{15.3} & \valstd{50.1}{13.3} & \valstd{48.4}{18.2} \\
 & Trans-mean & \valstd{42.5}{8.6} &	\valstd{45.3}{9.6} &\valstd{37.0}{7.9} &\valstd{33.9}{8.2}&	\valstd{65.1}{1.9} &	\valstd{63.8}{1.2} &\valstd{67.9}{1.8} &	\valstd{64.9}{1.7}\\
 & GRU-D & \valstd{45.1}{2.9} & \valstd{51.7}{6.2} & \valstd{42.1}{6.6} & \valstd{47.2}{3.9} & \valstd{58.0}{2.0} & \valstd{63.2}{1.7} & \valstd{58.2}{3.1} & \valstd{59.3}{3.5} \\
 & SeFT & \valstd{32.7}{2.3} & \valstd{27.9}{2.4} & \valstd{34.5}{3.0} & \valstd{28.0}{1.4} & \valstd{31.7}{1.5} & \valstd{31.0}{2.7} & \valstd{32.0}{1.2} & \valstd{28.0}{1.6} \\
 & mTAND & \valstd{27.5}{4.5} & \valstd{31.2}{7.3} & \valstd{30.6}{4.0} & \valstd{30.8}{5.6} & \valstd{34.7}{5.5} & \valstd{43.4}{4.0} & \valstd{36.3}{4.7} & \valstd{39.5}{4.4} \\
 & Raindrop & \valstd{52.4}{2.8} & \valstd{60.9}{3.8} & \valstd{51.3}{7.1} & \valstd{48.4}{1.8} & \valstd{60.3}{3.5} & \valstd{68.1}{3.1} & \valstd{60.3}{3.6} & \valstd{61.9}{3.9} \\ 
 % \cmidrule{2-10}
  \rowcolor{Gray}
 & \textbf{ViTST}  & \valstdb{86.4}{2.1} & \valstdb{88.3}{1.8} & \valstdb{88.0}{1.7} & \valstdb{87.6}{1.7} & \valstdb{88.5}{0.7} & \valstdb{89.8}{0.9} & \valstdb{90.1}{1.0}& \valstdb{89.8}{0.9} \\ \midrule
\multirow{7}{*}{40\%} & Transformer & \valstd{23.0}{3.5} & \valstd{7.4}{6.0} & \valstd{14.5}{2.6} & \valstd{6.9}{2.6} & \valstd{43.8}{14.0} & \valstd{44.6}{23.0} & \valstd{40.5}{15.9} & \valstd{40.2}{20.1}\\
 & Trans-mean &  \valstd{25.7}{2.5}&	\valstd{9.1}{2.3}&	\valstd{18.5}{1.4}&	\valstd{9.9}{1.1}&\valstd{48.7}{2.7}&\valstd{55.8}{2.6} &	\valstd{54.2}{3.0} &	\valstd{55.1}{2.9}\\
 & GRU-D & \valstd{46.4}{2.5} & \valstd{64.5}{6.8} & \valstd{42.6}{7.4} & \valstd{44.3}{7.9} & \valstd{47.7}{1.4} & \valstd{63.4}{1.6} & \valstd{44.5}{0.5} & \valstd{47.5}{0.0} \\
 & SeFT & \valstd{26.3}{0.9} & \valstd{29.9}{4.5} & \valstd{27.3}{1.6} & \valstd{22.3}{1.9} & \valstd{26.8}{2.6} & \valstd{24.1}{3.4} & \valstd{28.0}{1.2} & \valstd{23.3}{3.0}\\
 & mTAND & \valstd{19.4}{4.5} & \valstd{15.1}{4.4} & \valstd{20.2}{3.8} & \valstd{17.0}{3.4} & \valstd{23.7}{1.0} & \valstd{33.9}{6.5} & \valstd{26.4}{1.6} & \valstd{29.3}{1.9} \\
 & Raindrop & \valstd{52.5}{3.7} & \valstd{53.4}{5.6} & \valstd{48.6}{1.9} & \valstd{44.7}{3.4}& \valstd{57.0}{3.1} & \valstd{65.4}{2.7} & \valstd{56.7}{3.1} & \valstd{58.9}{2.5} \\ 
 % \cmidrule{2-10}
  \rowcolor{Gray}
 & \textbf{ViTST}  & \valstdb{80.0}{2.6} & \valstdb{83.7}{2.7} & \valstdb{82.3}{2.4} & \valstdb{81.2}{2.7} & \valstdb{83.7}{1.3} & \valstdb{85.5}{1.1} & \valstdb{85.6}{1.4} & \valstdb{85.1}{1.3} \\ \midrule
\multirow{7}{*}{50\%} & Transformer & \valstd{21.4}{1.8} & \valstd{2.7}{0.2} & \valstd{12.5}{0.4} & \valstd{4.4}{0.3} & \valstd{43.2}{2.5} & \valstd{52.0}{2.5} & \valstd{36.9}{3.1} & \valstd{41.9}{3.2}\\
 & Trans-mean & \valstd{21.3}{1.6} &	\valstd{2.8}{0.4}	&\valstd{12.5}{0.7} &	\valstd{4.6}{0.2}&	\valstd{46.4}{1.4} &\valstd{59.1}{3.2}&	\valstd{43.1}{2.2}&	\valstd{46.5}{3.1}  \\
 & GRU-D & \valstd{37.3}{2.7} & \valstd{29.6}{5.9} & \valstd{32.8}{4.6} & \valstd{26.6}{5.9} & \valstd{49.7}{1.2} & \valstd{52.4}{0.3} & \valstd{42.5}{1.7} & \valstd{47.5}{1.2} \\
 & SeFT & \valstd{24.7}{1.7} & \valstd{15.9}{2.7} & \valstd{25.3}{2.6} & \valstd{18.2}{2.4} & \valstd{26.4}{1.4} & \valstd{23.0}{2.9} & \valstd{27.5}{0.4} & \valstd{23.5}{1.8} \\
 & mTAND & \valstd{16.9}{3.1} & \valstd{12.6}{5.5} & \valstd{17.0}{1.6} & \valstd{13.9}{4.0} & \valstd{20.9}{3.1} & \valstd{35.1}{6.1} & \valstd{23.0}{3.2} & \valstd{27.7}{3.9} \\
 & Raindrop & \valstd{46.6}{2.6} & \valstd{44.5}{2.6} & \valstd{42.4}{3.9} & \valstd{38.0}{4.0}& \valstd{47.2}{4.4}& \valstd{59.4}{3.9} & \valstd{44.8}{5.3} & \valstd{47.6}{5.2} \\ 
 % \cmidrule{2-10}
  \rowcolor{Gray}
 & \textbf{ViTST} & \valstdb{79.7}{2.1} & \valstdb{83.4}{2.3} & \valstdb{81.8}{1.9} & \valstdb{80.8}{2.2}& \valstdb{82.8}{1.8} & \valstdb{84.9}{2.0} & \valstdb{84.9}{1.8} & \valstdb{84.4}{1.9} \\ \bottomrule
\end{tabular}
}
\label{tab:leave-sensors-out}
\end{table*}





% \section{Supplementary Material}

% Authors may wish to optionally include extra information (complete proofs, additional experiments and plots) in the appendix. All such materials should be part of the supplemental material (submitted separately) and should NOT be included in the main submission.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}