% Template for ICIP-2022 paper; to be used with:
%          spconf.sty  - ICASSP/ICIP LaTeX style file, and
%          IEEEbib.bst - IEEE bibliography style file.
% --------------------------------------------------------------------------
\documentclass{article}
\usepackage{spconf,amsmath,graphicx}

% Example definitions.
% --------------------
\def\x{{\mathbf x}}
\def\L{{\cal L}}

\usepackage{epsfig}
\usepackage{amsfonts}
%\usepackage{algorithm}   %%%%% 不注释会报错
%\usepackage{algorithmicx}
%\usepackage{algpseudocode}
\usepackage{multirow}
\usepackage{url}

% 章节符号
\usepackage[utf8]{inputenc}
\usepackage{cleveref}
\crefname{section}{§}{§§}
\Crefname{section}{§}{§§}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% AACL模板迁移包 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Standard package includes
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.



%\usepackage[margin=3cm]{geometry}
\usepackage{algorithm2e}
\RestyleAlgo{ruled}

%%%%%%%%%% algorithm packages %%%%%%%%%%
\usepackage{ragged2e}
\usepackage{amssymb}

\usepackage{graphicx}
\graphicspath{ {./images/} }

%\usepackage{subfigure}
\usepackage{subfig}

\usepackage{amsmath}
\usepackage{bm}

\usepackage{longtable}
%\usepackage[dvipsnames]{xcolor}
\usepackage{booktabs} 
\usepackage{multirow}



\newcommand{\docred}{DocRED}
\newcommand{\docredfe}{DocRED-FE}

\newcommand{\yifan}[1]{\textcolor{red}{\bf \small [ #1 --Yifan]}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



% Title.
% ------
\title{DocRED-FE: A Document-level Fine-grained Entity and Relation Extraction Dataset}
%
% Single address.
% ---------------

\name{Hongbo Wang$^{}\sthanks{\quad indicates equal contribution}$, Weimin Xiong$^{*}$, Yifan Song, Dawei Zhu, Yu Xia, Sujian Li$^{}$\sthanks{\quad Corresponding author}}

%\address{Key Laboratory of Computational Linguistics, MOE, Peking University \\
%whb@stu.pku.edu.cn, \{wmxiong, yfsong, dwzhu, yuxia, lisujian\}@pku.edu.cn}

\address{National Key Laboratory for Multimedia Information Processing, Peking University \\
whb@stu.pku.edu.cn, \{wmxiong, yfsong, dwzhu, yuxia, lisujian\}@pku.edu.cn}


%
% For example:
% ------------
%\address{School\\
%	Department\\
%	Address}
%
% Two addresses (uncomment and modify for two-address case).
% ----------------------------------------------------------
%\twoauthors
%  {A. Author-one, B. Author-two\sthanks{Thanks to XYZ agency for funding.}}
%	{School A-B\\
%	Department A-B\\
%	Address A-B}
%  {C. Author-three, D. Author-four\sthanks{The fourth author performed the work
%	while at ...}}
%	{School C-D\\
%	Department C-D\\
%	Address C-D}
%
\begin{document}
%\ninept
%
\maketitle
%
\begin{abstract}
Joint entity and relation extraction (JERE) is one of the most important tasks in information extraction.
However, most existing works focus on sentence-level coarse-grained JERE, which have limitations in real-world scenarios.
%Existing information extraction (IE) dataset usually only focuses on fine-grained named entity recognition task or fine-grained relation extraction task.
% Both fine-grained named entities and  fine-grained relations are much more helpful to natural language understanding, 
% compared to coarse-grained ones.
% However, existing information extraction dataset usually focuses on either of the two kinds of fine-grained types, since fine-grained types bring a high cost for the annotation process.
%Existing information extraction (IE) dataset usually focuses on either fine-grained named entity  or fine-grained relation.
% / Relation Classification (RC)
%However, both of them are much more helpful than coarse-grained ones for knowledge graph construction and natural language understanding, and more suitable for practical scenarios, while constructing a fine-grained dataset always have excessive cost.
%%%细粒度实体和细粒度关系识别更适合于实际场景，并且两个细粒度互相辅助。由于构建成本高，同时考虑细粒度实体和细粒度关系的工作和语料很少。
In this paper, we construct a large-scale document-level fine-grained JERE dataset \docredfe{}, which improves \docred{} with \textbf{F}ine-Grained \textbf{E}ntity Type.
Specifically, we redesign a hierarchical entity type schema including 11 coarse-grained types and 119 fine-grained types, and then re-annotate  \docred{} manually according to this schema. Through comprehensive experiments we find that: 
(1) DocRED-FE is challenging to existing JERE models; 
(2) Our fine-grained entity types promote relation classification.
%We adopt strong models as baselines, and comprehensive experiments show that \docredfe{} is challenging to existing joint entity and relation extraction models. We also demonstrate that our fine-grained entity types are helpful for relation classification. 
We make DocRED-FE with instruction and the code for our baselines publicly available at \url{https://github.com/PKU-TANGENT/DOCRED-FE}.
\end{abstract}
%

\begin{keywords}
 Joint Entity and Relation Extraction, Information Extracion, Fine-Grained Entity Types
\end{keywords}
%

%%%%%%%%%%%%%%%%%%%%%%%%%% 数据举例 + 新旧schema对比 %%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[t]
    \centering
    \subfloat{\includegraphics[width = 0.9\linewidth]{case.pdf}}
    \quad
    \setlength{\lineskip}{-0.5em}  % 表和图行间距
    \subfloat{\includegraphics[width = 0.5\linewidth]{schema_before.pdf}}
    \subfloat{\includegraphics[width = 0.5\linewidth]{schema_after.pdf}}
    \caption{A document case of \docredfe{}, with the corresponding origin (left) and new (right) schema. Only highlights the first mention of each entity.}% in Table~\ref{tab:document case}}
    \label{fig:schema}
    \vspace{-0.3cm}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Introduction}
\label{sec:intro}

%%% 原先的Related Work 和 Corpus Construction开头内容 移入Introduction %%%

% 开头引入，任务介绍
% \textbf{N}amed \textbf{E}ntity \textbf{R}ecognition (NER), \textbf{R}elation \textbf{C}lassification (RC) and \textbf{J}oint \textbf{E}ntity and \textbf{R}alation \textbf{E}xtraction (JERE) are mainstream \textbf{I}nformation \textbf{E}xtracion (IE) tasks, which attract many pioneers to endeavour to research. In this paper, for clarity, the NER aims to locate the named entity and classify them correctly, and the RC is a multi-classification task between the given entity pair, while the JERE task need to do both above from the plain text. Most research develop on published dataset, but the latter have different granularity due to the lack of universal standard of schema. 

The goal of joint entity and relation extraction (JERE) is to identify named entities and their relations from unstructured text. 
It is an essential problem in information extraction (IE) since it is critical to constructing knowledge graph. 
Recently, various works on JERE, including pipelined and end-to-end approaches, have achieved remarkable success \cite{bekoulis_2018_joint, wang_2020_pretraining, zhong_2021_frustratingly}.

Despite these successful efforts, most existing JERE works are conducted on sentence-level coarse-grained datasets, \textsl{e.g.}, ACE04/05~\cite{ace}, CoNLL04~\cite{conll}, SciERC~\cite{scierc}, TACRED~\cite{zhang2017position}, FewRel~\cite{han2018fewrel}.
% , and how to leverage cross-sentence information to conduct fine-grained extraction is still under-explored.
However, in real-world application scenarios, the model usually needs to understand the semantics of document-level texts and extract more precise information.


% Besides, published JERE datasets only focus on a coarse-grained entity or relation types, which will hinder the model ability to handle real applications with fine-gained types.


% 以往数据集简介

% ACE04, 05, SciERC, TACRED, %BC5CDR, %MUC-7, DocRED
% Published dataset used for JERE research are of great variety. ACE~\cite{doddington2004automatic, walker2006ace} is collected from different domains. SciERC~\cite{luan2018multi}  comes from scientific paper abstracts. %for scientific knowledge graph construction. 
% TACRED ~\cite{zhang2017position}, and its variant TACRED Revisited~\cite{alt2020tacred}, RE-TACRED ~\cite{stoica2021re} keep the high popularity.

% Published dataset used for JERE research are of great variety. ACE~\cite{doddington2004automatic, walker2006ace}, SciERC~\cite{luan2018multi}, TACRED ~\cite{zhang2017position}, and its variant TACRED Revisited~\cite{alt2020tacred}, RE-TACRED ~\cite{stoica2021re} keep their high popularity.
% However, most of them still stay at the sentence-level, hinder the model from leveraging cross-sentence information to make decision. 



% NER数据集介绍及缺点
% For the NER tasks, most classical datasets like ACE 2005~\cite{doddington2004automatic,walker2006ace}, CoNLL'03~\cite{sang2003introduction}, and OntoNotes 5.0~\cite{weischedel2013ontonotes}, concentrate on coarse-fined entity typing, and a lot of models with brilliant results have developed on them(***Ref***). However it seems that this line of research has reached a bottleneck. After that, fine-grained NER have received growing attention. The dataset FIGER~\cite{ling2012fine} uses Freebase types and aligns Wikipedia text to Freebase, constructing a first well-known fine-grained Entity Recognition (ER) dataset. Few-NERD~\cite{ding-etal-2021-nerd} created a large scale NER dataset for Few-shot learning. The dataset~\citet{choi-etal-2018-ultra} including 10,000+ ultra-fine types is also proposed before. However, almost all of them still stay at the sentence-level, hindering the model from leveraging cross-sentence information to make decision.

% RE数据集介绍 及缺点
% Research on RE task has a very long history, as well as the RE dataset. NYT~\cite{riedel2010modeling}, SemEval2010 Task 8~\cite{hendrickx2010semeval}, SciERC~\cite{luan2018multi}, TAC-KBPs~\cite{ji2010overview,ji2015overview,ji2017overview}, Rich ERE~\cite{song2015light}, TACRED ~\cite{zhang2017position} and the variants, TACRED Revisited~\cite{alt2020tacred} and RE-TACRED~\cite{stoica2021re} have proved their popularity. As for the fine-grained, FewRel~\cite{han2018fewrel}, proposes a Few-shot RE dataset. Doc-RED~\cite{yao-etal-2019-docred} creates a document-level dataset and Cod-RED~\cite{yao-etal-2021-codred} creates a cross-document level dataset. Most of them ignore the implicit connection between the NER task and the RE task, since they only have coarse-fined entity labels. DWIE~\cite{zaporojets2021dwie} constructs an entity-centric dataset for multi-tasks even including Entity Linking. 

The research on document-level fine-grained entity and relation extraction requires a large-scale annotated dataset for both training and evaluation.
It is usually expensive and time consuming to construct such a large-scale dataset from scratch.
Therefore, we choose to re-annotate a popular document-level relation extraction dataset, \docred{}~\cite{yao2019docred}.
There are 96 fine-grained relation types in \docred{}, while the entities are divided into only 6 types and are recognized by automatic tools.


% Doc-RED数据集介绍
% Emphatically, \docred{}~\cite{yao-etal-2019-docred} is a widely used document-level relation extraction dataset.
% %, which is constructed by aligning Wikipedia to Wikidata. 
% In terms of task setting, to be precise, it focuses on RC task while knowing entities and their types, and takes evidence prediction as an optional task. The \docred{} defines 6 entity types and 96 relation types, however, the lack of entity type information and complex relation type information brings great challenges to RC. %Therefore, it's a good starting point.

% 我们的数据集介绍
In this paper, we propose \docredfe{}, a large-scale human-annotated fined-grained entity and relation extraction dataset based on \docred{}. 
Specifically, we first re-design the entity type schema to a hierarchical structure including 11 coarse-grained types and 119 fine-grained types.
Combined with the 96 fine-grained relation types in original \docred{}, \docredfe{} contains richer contextual information with a finer granularity.
We illustrate an example document with its corresponding schema in Fig.~\ref{fig:schema}.
Compared to the coarse schema in \docred{}, our newly designed entity schema is more precise and expressive with most of the self-pointing relation edges dissolved, such as \textit{ORG-record\_label-ORG}.
% after that, dozens of NLP researchers are paid to re-annotate the entity types according to new schema. As for the text content, relation and evidence annotations, we keep their original forms. Relevant details are descrbied in Section~\ref{sec:corpus construction}.

% 所做实验
To assess the JERE benchmark on \docredfe{}, we adopt several representative document-level JERE baselines, including a pipeline model based on BERT~\cite{devlin2018bert} and an end-to-end model based on BART~\cite{lewis-etal-2020-bart}.
Experimental results show that document-level fine-grained JERE poses a great challenge to current models and remains an open problem. Furthermore, we also confirm that fine-grained entity information can improve the performance on relation classification (RC). We believe our analysis can help to design more powerful models.
% About JERE task, we try pipeline model based on BERT~\cite{devlin2018bert} and generation model based on BART~\cite{lewis-etal-2020-bart}, visible decline on RE score as well as NER score reveals that it brings a great challenge to current model. As for RC task, We get overall improvement on different models through integrating richer information in entity types, and further prove that there is still room for improvement. 

% In short, our contributions are summarized as follows:

% % Contribution列举
% \begin{itemize}
%     \setlength{\itemsep}{0pt}
%     \setlength{\parsep}{0pt}
%     \setlength{\parskip}{0pt}
%     \item We construct \docredfe{}, which is a large-scale human-annotated document-level fine-grained entity and relation extraction dataset.
%     \item Experimental results show that document-level fine-grained JERE is a challenging task and remains an open problem.
%     \item Our analysis provide some insight for future model design.
% \end{itemize}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 原本的Relatied Work 删去 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\iffalse  %%%%%%%%% 整体注释

\section{Related Work}
\subsection{NER Datasets}

\label{sec:bibtex}


However, almost all of them still stay at the sentence-level, hindering the model from leveraging cross-sentence information to make decision.


\subsection{RE Datasets}
Research on RE task has a very long history, as well as the RE dataset. NYT~\cite{riedel2010modeling}, SemEval2010 Task 8~\cite{hendrickx2010semeval}, SciERC~\cite{luan2018multi}, TAC-KBPs~\cite{ji2010overview,ji2015overview,ji2017overview}, Rich ERE~\cite{song2015light}, TACRED ~\cite{zhang2017position}, the variants, TACRED Revisited~\cite{alt2020tacred} and RE-TACRED ~\cite{stoica2021re} have proved their popularity. As for the fine-grained, FewRel~\cite{han2018fewrel}, propose a Few-shot RE dataset. Doc-RED~\cite{yao-etal-2019-docred} create a document-level dataset and Cod-RED~\cite{yao-etal-2021-codred} create a cross-document level dataset. Most of them ignore the implicit connection between the NER task and the RE task, since they only have coarse-fined entity labels. DWIE~\cite{zaporojets2021dwie} construct an entity-centric dataset for multi-tasks even including Entity Linking.


Overall, in comparison with existing datasets, our dataset have both fine-grained types on entity and relation, explores inner relationship of different IE tasks, and investigates the more open and challenging scenario of information acquisition.
\fi
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%













\section{Dataset Construction}
\label{sec:Dataset Construction}

%In this section, we describe the process of constructing our dataset in detail.

%\subsection{\docred{} Dataset}

% \docred{}~\cite{yao-etal-2019-docred} is a widely used document-level relation extraction dataset, which is constructed by aligning Wikipedia to Wikidata. In terms of task setting, to be precise, it focuses on RC task while knowing entities and their coarse-grained types, and takes evidence prediction as an optional task. There are different data version which consist of manually annotated portion and distantly supervision portion. These rich information provides a lot of convenience for information extraction research.

% %%%数据集的构建过程重点写一下，比如采用了什么标注系统，几个人标注？保证正确的一些手段等等。。。
% %%% 数据的一些统计信息要给出
% As a result, our dataset is totally based on the \docred{}. The Doc-RED defines 6 entity types and 96 relation types, however, the lack of entity type information and complex relation type information brings great challenges to RC. Accordingly, we redefine and expand the entity type schema to a hierarchy structures include 100+ fine-grained types. Moreover, the Test part of the Doc-RED dataset is hidden, so we only take the Train and Dev parts of the human annotated portion, and re-annotate them by crowd-sourcing according to our fine-grained entity type schema. Besides, some missing or wrong annotations on coreference resolution were discovered during our annotatation process. We revised and cleaned the data finally. As for the text content, relation and evidence annotations, we keep their original forms.

% As previously introduced, our dataset is built based on \docred{}, with strengthening the entity information. Doc-RED has different data versions which consist of manually annotated portion and distantly supervision portion, while the test part is hidden, so we only take the the \emph{Train} and \emph{Dev} parts of the former. Some missing or wrong annotations on coreference resolution were discovered during our annotatation process. which were revised and cleaned finally.

\docredfe{} is built based on \docred{}, a large-scale crowd-sourced dataset from Wikipedia and Wikidata.
% DocRED has 5053 documents, 132375 entities, 97 relation types (including \textit{no\_relation}), and 56354 relational facts in total.
\docred{} is mainly built for relation extraction and has only 6 coarse entity types. 
Thus, we develop \docredfe{} via refining the entity type schema and re-annotating the entities in \docred{}.
%We detailedly describe the construction process in following subsections.

% \subsection{Schema Design}
% %The Doc-RED perform NER use spaCy(https://spacy.io), and the 6 classical labels: PER, LOC, ORG, MISC, TIME, NUM.
% We now introduce our \emph{bottom-up} data-driven annotation approach.
% We aims to establish a schema which can reflect the characteristics of entities in texts properly, and make the frequency of each label in schema as balanced as possible. Therefore, controlling the granularity and dividing boundaries of the schema is particularly critical. 
% Formally, we design the entity schema in the following three steps:

% \noindent \textbf{Stage 1: Entity Linking.}
% For each entity in the dataset, we link it to Wikidata to get its top-4 candidate types. All of the candidate types constitute our initial fine-grained schema.
% For example, for the entity \emph{China}, we get its candidate types as \emph{ {Country, State, Sovereign State, Socialist State}} and add them into our initial fine-grained schema.
% %Specifically, for entities without hyperlinks to Wikidata, we fetch their candidate types by Wikidata hierarchy structure according to their names. 
% %Since the documents are from a Wikipedia 2018 dump, some entities have hyperlinks to Wikidata. For other entities, according to their names, we fetch the types defined by Wikidata hierarchy structure. 
% %Then, we collect the top-k of the results, which are most possibly corresponding ones. 
% %For example, for the entity \emph{China}, we get its candidate types \emph{ {Country, State, Sovereign State, Socialist State, etc}}. 
% %All of the them make up the whole candidate label set.

% \noindent \textbf{Stage 2: Combining with Existing Fine-Grained Schema.}
% In this step, we augment our initial fine-grained schema with existing fine-grained entity schemas \textsl{i.e.} FIGER~\cite{ling2012fine} and Few-NERD~\cite{ding-etal-2021-nerd}. 
% Types with high frequency are merged into our fine-grained schema.
% %The former comes from Freebase, while the latter is purely hand-made. 
% %Considering homology (general encyclopedia), some types with high frequency are used to expand our candidate label set.
% %Besides, the naming of relation types in \docred{} is also our guideline for naming the fine-grained entity type. For example, we prefer to define an entity type like \emph{Record Label} but not \emph{Record Company} or \emph{Musical Company}, to match the relation type \emph{Record Label}, which may lead to positive effect to the model.
% %All of above greatly help us to maintain the naming consistency between entity and relation types. 

% \noindent \textbf{Stage 3: Iterative Exploratory Annotation and Refinement.}
% After the prior two stages, a preliminary schema is built. To validate whether this schema is suitable, % for the dataset, 
% we carry out a few rounds of exploratory annotation, and refine the schema according to the feedback by the annotators after each round. 
% For example, we subdivide the fine-grained type \emph{GPE} into several subtypes such as \emph{Country, City} and so on, since entities with type GPE account for a large proportion. We also merge type \emph{Actor} and \emph{Director} into \emph{Actor\&Director} since they are always used to label the same person and hard to distinguish.

% % Consequently, the finalized schema includes 11 coarse-grained types and 119 fine-grained types, which is detailedly shown accompanied with selected examples in Appendix~\ref{sec:schema}.




\subsection{Schema Design}
%The Doc-RED perform NER use spaCy(https://spacy.io), and the 6 classical labels: PER, LOC, ORG, MISC, TIME, NUM.
In this section, we introduce our \emph{bottom-up} data-driven annotation approach.
We aims to establish a schema which can reflect the characteristics of entities in texts properly, and make the frequency of each label in schema as balanced as possible. Therefore, controlling the granularity and dividing boundaries of the schema is particularly critical. 
Formally, we design the entity schema in the following three steps:

\noindent \textbf{Stage 1: Entity Linking.}
For each entity in the dataset, we link it to Wikidata to get its types. For example, for the entity \emph{China}, we get its candidate types as \emph{ {Country, State, Socialist State}}. Then we statistically compute the frequency of each type and filter about top-100 of them to constitute our initial fine-grained candidate set. 
%Specifically, for entities without hyperlinks to Wikidata, we fetch their candidate types by Wikidata hierarchy structure according to their names. 
%Since the documents are from a Wikipedia 2018 dump, some entities have hyperlinks to Wikidata. For other entities, according to their names, we fetch the types defined by Wikidata hierarchy structure. 
%Then, we collect the top-k of the results, which are most possibly corresponding ones. 
%For example, for the entity \emph{China}, we get its candidate types \emph{ {Country, State, Sovereign State, Socialist State, etc}}. 
%All of the them make up the whole candidate label set.

\noindent \textbf{Stage 2: Combining with Existing Schema.}
As for the FIGER~\cite{ling2012fine} and Few-NERD~\cite{ding-etal-2021-nerd}, they are both general encyclopedia datasets. Considering their homology with Doc-RED, we combine their entity schema with our initial candidate set. In the process of combination, both coarse-grained and fine-garined types are adjusted. For example, we prefer to make the initial fine-grained type \emph{GPE} in Few-NERD to be coarse-grained one, while for the types \emph{Military\_operation}, \emph{War}, \emph{Battle} that have similar meaning, we only keep the first one remained. After the prior two stages, a preliminary hierarchy schema is built.

%In this step, we augment our initial fine-grained schema with existing fine-grained entity schemas \textsl{i.e.} FIGER~\cite{ling2012fine} and Few-NERD~\cite{ding-etal-2021-nerd}. Types with high frequency are merged into our fine-grained schema.
%The former comes from Freebase, while the latter is purely hand-made. 
%Considering homology (general encyclopedia), some types with high frequency are used to expand our candidate label set.
%Besides, the naming of relation types in \docred{} is also our guideline for naming the fine-grained entity type. For example, we prefer to define an entity type like \emph{Record Label} but not \emph{Record Company} or \emph{Musical Company}, to match the relation type \emph{Record Label}, which may lead to positive effect to the model.
%All of above greatly help us to maintain the naming consistency between entity and relation types. 

\noindent \textbf{Stage 3: Iteratively Exploratory Annotation and Refinement.}
To validate whether this schema is suitable, we carry out a few rounds of exploratory annotation, and refine the schema according to the feedback by the annotators after each round. 
For example, we subdivide the type \emph{GPE} into more subtypes not only \emph{Country, City}, but also \emph{Continent, State\&Province} since entities with type GPE account for a large proportion. We also merge type \emph{Actor} and \emph{Director} into \emph{Actor\&Director} since they are always used to label the same person and hard to distinguish. Consequently, the finalized schema includes 11 coarse-grained types and 119 fine-grained types is established.

% Consequently, the finalized schema includes 11 coarse-grained types and 119 fine-grained types, which is detailedly shown accompanied with selected examples in Appendix~\ref{sec:schema}.


\subsection{Human Annotation}

In this stage, we perform human re-annotation on the entities according to our new schema. To ease the annotation burden, we ask annotators to assign one single type that best suits the entity according to its contexts. For example, sometimes \emph{World War II} represents a book but not the war, so it should be tagged by \emph{Written\_work} rather than \emph{Military\_operation}. Sometimes a person is a \emph{Soldier}, \emph{Politician} and \emph{Artist}, we ask the annotators to select the most prominent type depending on the contexts.


Annotators consist of about 20 experts that work on NLP research. They have linguistic knowledge and are instructed with detailed and formal annotation rules. To ease the annotation, the description of each entity on Wikipedia and its structured information on Wikidata are provided to annotators as references. We ensure that all the annotators are fairly compensated by market price according to their workload. The dataset is randomly divided and delivered to different annotators. To ensure the quality of our dataset, we conduct consistency checking %algorithm in data verification stage to 
via calculating the Cohen’s Kappa Score~\cite{cohen1960coefficient}, and the result is 68.58\%, which indicates a relatively high degree of consistency. 
%The calculating algorithm is shown in Appendix~\ref{sec:kappa}.





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 统计表格 %%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{table}[t]
\caption{Comparison to well-known RE/JERE datasets (Doc.: document, Tok.: token, Ent.: entity, ET.: entity type, Rel.: relation, RT.: relation type).}
\tiny  %表格字体
\centering
\footnotesize
\setlength\tabcolsep{2pt}
\begin{tabular}{c c c c c c c}
\toprule
\textbf{Dataset} & \textbf{\# Doc.} & \textbf{\# Tok.} & \textbf{\# Ent.} & \textbf{\# ET.} & \textbf{\# Rel.} & \textbf{\# RT.}\\
\midrule

% {CoNLL03} & {-} & {301k} & {35089} & {4} & {-} & {-}\\
% {OntoNotes5.0} & {-} & {2088k} & {136037} & {-} & {-} & {-}\\
% {FIGER} & {-} & {+1397k} & {+11200} & {112} & {-} & {-}\\
% {Few-NERD} & {-} & {4601k} & {491700} & {66} & {-} & {-}\\
% \midrule
{ACE2005} & {-} & {259k} & {37,622} & {51} & {7,786} & {18}\\
{TACRED} & {-} & {3,866k} & {212,528} & {17} & {106,264} & {41}\\
{SciERC} & {-} & {65,334} & {1,015} & {6} & {2,687} & {7}\\
%{NYT} & {-} & {5765k} & {1,388,982} & {-} & {142823} & {52}\\
%{SemEval2010} & {-} & {207k} & {21434} & {-} & {6674} & {9}\\
{FewRel} & {-} & {1,397k} & {112,000} & {-} & {56,000} & {100}\\
{DWIE} & {802} & {501k} & {23,130} & {311} & {21,749} & {65}\\
{DocRED} & {5,053} & {1,002k} & {98,610} & {6} & {56,354} & {96}\\
\midrule
\textbf{\docredfe} & \textbf{2,596} & \textbf{516k} & \textbf{50,549} & \textbf{119} & \textbf{32,366} & \textbf{96}\\

\bottomrule
\end{tabular}
\label{tab:statistic}
%\vspace{-0.2cm}
\end{table}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%




\section{Data Analysis}
In this section, we analyze our dataset from various aspects to have a deeper understanding.

%1.compare with existing dataset
%\textbf{Compare with related dataset}

\subsection{Comparison with Related Datasets}
Without the test part, \docredfe{} consists of 65\% of the train part and whole dev part of the DocRED. Table ~\ref{tab:statistic} shows statistics of our dataset and some well-known RE/JERE datasets. Except DWIE~\cite{zaporojets2021dwie}, all of above are only fine-grained in either entity or relation. As for DWIE, we do not have enough statistics on entity information since they do not provide a complete entity schema. To the best of our knowledge, it has fewer documents and tokens but more complicated entity types, which make NER difficult due to insufficient training. 
%On the other hand, multi-relation in the same entity pair is more common in DWIE.
%while heavy reuse of relation type and sparse entity types has severe imbalance.




\subsection{Entity Distribution}

In Figure~\ref{fig:statisticChart}, the pie chart shows the entity distribution of first-level entity type while the bar chart shows that of the second-level. For the bar chart, the top-5 types are \emph{Time}, \emph{Country}, \emph{Number}, \emph{City} and \emph{Musician}, the last-5 are \emph{Chemical\_and\_biological}, \emph{Natural\_phenomenon}, \emph{Dam}, \emph{Disease} and \emph{Sports\_season}.
Long-tail phenomenon still exists, but the downtrend is relatively flat after the top-5.

%%%%%%%%%%%%%%%%%%%%%%%%%%%% 饼图 + 柱状图 复合图 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[tbp]
    \vspace{-0.3cm}
    \includegraphics[width = 1\linewidth]{combo_chart.pdf}
    \caption{Statistics on first-level (pie chart) and second-level (bar chart) entity type.}
    \label{fig:statisticChart}
    %\vspace{-0.5cm}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection{Schema for Each Document}
After our transformation from the original coarse schema to the finer one, the triplets in our schema also differ from  \docred{} due to the variety of the head and tail entity type as shown in Figure~\ref{fig:schema}. Compared with the latter, there are following optimizations:
(1) Diversity of the entity type makes the relation distribution more even, amount of self-pointing edge descends, and overall density tends to be more reasonable.
(2) Naming consistency of entity and relation makes the meaning clear and unambiguous, domain information like music becomes prominent.
(3) The rich entity information can provide more significant hints for RC tasks. To measure the gain quantitatively, we compute the information gain $\Delta H$ and the information gain ratio $\Delta H_{ratio}$ comparing to the original \docred{}. 
The information entropy means average weighted information entropy of the triplet set computed by each entity type pair~\cite{thomas2006elements}, the $\Delta H$ means the value of decline of the information entropy between \docred{} and \docredfe{}, the result is 1.09. The $\Delta H_{ratio}$ is 65.67\%, which means the $\Delta H$ relative to the information entropy of \docred{}. It theoretically indicating that fine-grained entity types can benefit RC.
%(see Appendix~\ref{sec:infogain} for more details).








\section{Experiments}
% In the experimental part, we conduct comprehensive experiments to evaluate state-of-the-art document level RE systems on the dataset. We mainly conduct two parts of experiments. In relation extraction part, we test our datasets on two relation extraction model, extracting relations in documents from scratch. Specifically, we conduct the evaluation part under two schemes, a strict mode and a relaxed mode, to analyze models' performance on our dataset and prove that our dataset provides a more difficult benchmark for later models. In relation classification part, we test our dataset on several relation classification models which consider entity type as an important factor to inference the relation between them.

%In the experiment part,
%We conduct comprehensive experiments to evaluate current document-level JERE models on \docredfe.
% There are mainly two questions that we are concerned about in our experiments. How do current document-level JERE models perform on DocRED-FE (\cref{sec:4.3})? 
% Do fine-grained entity type information promote the relation classification (\cref{sec:4.4})?

We conduct two experiments using the proposed \docredfe{} dataset. Firstly, in Section 4.1, we conduct a JERE experiment on the \docred{} and \docredfe{} to show that \docredfe{} provides a more difficult benchmark. Secondly, in Section 4.2, we show that the \docredfe{} can help the RC by providing double-level and fine-grained entity type information.

For the JERE task, we adopt two JERE models as baselines of our dataset, including a pipelined model JEREX~\cite{eberts-ulges-2021-end} and an end-to-end generation model REBEL~\cite{huguet-cabot-navigli-2021-rebel-relation}. JEREX links entity recognition and relation classification tasks by feeding the results of entity recognition to the subsequent relation classification. REBEL employs BART-large as the base model and translates a raw input dataset into a set of triplets that can be uniquely decoded into entity-relation pairs. 

For the RC task, we adopt several representative RC models, including CNN, LSTM, BiLSTM proposed from \docred{}, GAIN~\cite{zeng2020double}, JEREX and SSAN~\cite{xu-etal-2021}.

For both experiments, we train the model using the same part of \docred{} and \docredfe{}. As mentioned above, we use 21k entity-relation pair instances as the train part and 10k instances as the validation part. We use the default settings mentioned in the references and fine-tune the model until the loss function converges and report the results on the validation part.


\begin{table}[t]
\caption{Performance of different JERE models on \docred{} and \docredfe{} (relations).}
\footnotesize
\centering
% \setlength\tabcolsep{4pt}
\begin{tabular}{c c c c}
\toprule
\multirow{2.5}{*}{\textbf{Model}} & \multicolumn{3}{c}{\textbf{DocRED}} \\ \cmidrule(r){2-4}
\multirow{2}{*}{} & \textbf{RE F1 (strict)} & \textbf{RE F1 (relaxed)} & \textbf{NER F1}\\
\midrule
JEREX & {40.26} & {40.62} & {80.25} \\
REBEL & {45.29} & {46.45} & {-} \\ \midrule \midrule
\multirow{2.5}{*}{\textbf{Model}} & \multicolumn{3}{c}{\textbf{\docredfe}} \\ \cmidrule(r){2-4}
\multirow{2}{*}{} & \textbf{RE F1 (strict)} & \textbf{RE F1 (relaxed)} & \textbf{NER F1}\\
\midrule
JEREX & {31.52} & {39.02} & {67.74}\\
REBEL & {37.51} & {45.94} & {-}\\
% \verb|JEREX| & {40.26} & {40.62} & {31.52} & {39.02} \\
% \verb|REBEL| & {45.29} & {46.45} & {37.51} & {45.94} \\
\bottomrule
\end{tabular}
\label{tab:RE}
%\vspace{-0.2cm}
\end{table}



\subsection{Joint Entity and Relation Extraction}
\label{exp:jere}
In this experiment, we follow previous work~\cite{eberts-ulges-2021-end,taille_2020_let} and use two modes of F1 score. The strict mode counts a relation as correct if and only if its mention span, coreference resolution, entity type and relation type are correct. The relaxed mode focuses on the relation accuracy and ignores the correctness of the entity type. 
When acquiring the golden entity type during pipelined process, 6 entity types in \docred{} are replaced by 119 entity types when testing \docredfe{}.

As shown in Table~\ref{tab:RE}, both the strict and relaxed F1 of a model trained on \docredfe{} are lower than those trained on \docred{}. Strict F1 declines as expected due to the more complex entity types. The reason why relax F1 become slightly lower is that although fine-grained entities provide more information, incorrect entity predictions may have a negative impact on relation classification and degrade the whole JERE performance. The result suggests that \docredfe{} presents great challenges to current models.




% \subsection{Models}
% We adopt two representative JERE models as baselines of our dataset, including a pipelined model JEREX~\cite{eberts-ulges-2021-end} and a end-to-end generation model REBEL~\cite{huguet-cabot-navigli-2021-rebel-relation}.
% JEREX links  entity recognition and relation classification tasks by applying the results of  entity recognition to the subsequent relation classification.
% REBEL employs BART-large as the base model and translates a raw input document into a set of triplets that explicitly refer to those relations. 


% \subsection{Evaluation Metrics} 
% Following previous work~\cite{taille_2020_let,eberts-ulges-2021-end}, we adopt two modes of F1 score in our experiments. 
% The strict mode counts a relation as correct if and only if its mention span, coreference resolution, entity type and relation type are correct. 
% However, with much more entity types than DocRED, the model might wrongly predict the entity type and this error may propagate to the relation classification.
% Therefore, we also propose a relaxed mode that focuses on the relation accuracy and ignores the correctness of the entity type. 
% We also adopt NER F1 score to analyse the entity extraction performance. 
% As REBEL only extracts the first mention of each entity, we ignore its NER performance.

\subsection{Relation Classification}
While JERE task on \docredfe{} poses a challenge for existing models, we want to find how our double-level entity types help the RC part of JERE task and provide some reasonable methods. 
Therefore, we utilize different levels of entity information to build the following three methods:
% For subsequent baselines, they all integrate entity type information by utilizing the embedding of coarse-fined types id in \docred{}, as a result, we test three different methods using \docredfe{}:

1.Only use the fine-grained entity type id.

2.Use the fine-grained entity type id and its semantic representation.

3.Use the fine-grained entity type id and corresponding coarse-grained entity type id.

% 4.Use the entity-pair mask summarized from the train part.


\noindent \textbf{Only Fine-Grained Types}
Similar to Section \ref{exp:jere}, for the entity type embedding part in every models, we replace the original coarse-grained types id embedding by our fine-grained types id embedding. As shown in Table~\ref{tab:RC}, fine-grained entities add more information to relation classifier and constrain the appropriate relation types. This helps the model to classify relations better.

% \subsection{Model Performance}
% \label{sec:4.3}



\noindent \textbf{Fine-Grained Types with Its Semantic Representation}
While the entity type id provides necessary information for relation classification, the entity type semantic information could also be utilized. From this perspective, we use BERT to encode each entity type such as \emph{Musician}, \emph{Country}, \emph{Military\_operation} to get its 768-dim semantic embedding $e_s$.
In the way of aggregating, we use concatenation or directly adding depending on different models.

For JEREX, we use MLP to map the $e_s$ into target embedding shape, and concatenate the semantic embedding $e_s$, fine-grained type id embedding $e_f$ with the entity-pair embedding $e_{en}$ before feeding to the final relation classifier $RC$. This process can be expressed as the following formula.
\begin{equation}
    Score = \mathbb{RC}[concat(e_{en}, e_f, e_s)]
\end{equation}

For BiLSTM, GAIN, SSAN, we use MLP to map the $e_s$ into the same shape with word embedding $e_w$, so that we can add these embeddings directly in the embedding block which is in the front of the model. By the way, we add \emph{"Padding"} type for non-entity token intentionally. The token embedding $E_t$ of SSAN compute as
\begin{equation}
    E_t = e_w + e_f + e_s * \alpha
\end{equation}
As shown in Table~\ref{tab:DLESE}, model with added semantic information performs better than that without it. F1 of BiLSTM using semantic information improves from 49.28 to 50.26, indicating that adding semantic information in the \docredfe{} dataset is helpful to relation classification. 



\noindent \textbf{Both Fine-Grained and Coarse-Grained Types}
Considering information in the hierarchical structure of entity schema of \docredfe{}, we further integrate new coarse-grained type based on first method. For simplicity, we train a coarse-grained type id embedding layer, project the new coarse-grained type id to type embedding $e_c$, and make fusion like the second method.
For BiLSTM, GAIN and JEREX, concatenation operation as
\begin{equation}
    Score = \mathbb{RC}[concat(e_{en}, e_f, e_c)]
\end{equation}
For SSAN, the token embedding $E_t$ compute as
\begin{equation}
    E_t = e_w + (e_f + e_c) * \gamma
\end{equation}

The second and fourth columns of Table~\ref{tab:DLESE} show that by adding double-level entity type information, RC models could utilize coarse-grained and fine-grained entity type jointly, and perform better than only using fine-grained entity type. % in \docredfe{}.
That means, the hierarchical entity types could be leveraged in designing  RC models.
%We believe that the hierarchical entity type could be an available point to leverage when designing future RC models.

%About why does the double-level types embedding work, by logically analysis that, after adding $e_c$, entities with different fine-grained types will stay closer when sharing same coarse-grained type, and will be pulled further when having unique coarse-grained type, which can lead the model to learn finer.
%In this way, the RC model could consider coarse-grained and fine-grained entity type jointly, similar to combining the results of them.





\begin{table}[t]
\caption{Infusing different granularity of entity information into RC models. "Coarse-grained Type" represent the entity type in \docred{}. We report the F1 score on dev set.}
\footnotesize
% \setlength\tabcolsep{3pt}
\centering
\begin{tabular}{c c c}
\toprule
\textbf{Model} & \textbf{Coarse-grained Type} & \textbf{Fine-grained Type}\\ \midrule
CNN & {43.90} & {44.59} \\
LSTM & {47.72} & {49.34} \\
BiLSTM & {48.64} & {49.28}\\
GAIN & {58.09} & {58.35}\\
JEREX & {57.99} & {58.19}\\
SSAN & {58.10} & {58.24} \\
% MRN & {59.31} & {59.68} \\
\bottomrule
\end{tabular}
\label{tab:RC}
%\vspace{-0.2cm}
\end{table}


% \begin{table}[t]
% \footnotesize
% \setlength\tabcolsep{3pt}
% \centering
% \begin{tabular}{c c c c c c c}
% \toprule
% \multirow{2}{*}{\textbf{Setting}} & \multicolumn{3}{c}{\textbf{DocRED}} & \multicolumn{3}{c}{\textbf{\docredfe{}}} \\ \cmidrule(r){2-4} \cmidrule(r){5-7} 
% \multirow{2}{*}{} & {Recall} & {Precision} & {F1} & {Recall} & {Precision} & {F1} \\ \midrule
% CNN & {39.94} & {48.73} & {43.90} & {41.18} & {48.62} & {44.59} \\
% LSTM & {46.33} & {49.20} & {47.72} & {46.06} & {53.12} & {49.34} \\
% BiLSTM & {48.16} & {49.13} & {48.64} & {46.55} & {52.34} & {49.28}\\
% GAIN & {55.78} & {60.61} & {58.09} & {57.04} & {59.71} & {58.35}\\
% JEREX & {54.73} & {61.66} & {57.99} & {53.73} & {63.47} & {58.19}\\
% SSAN & {54.11} & {62.72} & {58.10} & {56.20} & {60.42} & {58.24} \\
% MRN & {61.20} & {57.52} & {59.31} & {62.40} & {57.18} & {59.68} \\
% \bottomrule
% \end{tabular}
% \caption{Comparison of RC model performance on \docred{} and \docredfe{}. We report the results on dev part of both datasets.}
% \label{tab:RC}
% \vspace{-0.2cm}
% \end{table}





% \subsection{Relation Classification with Masks}
% \label{sec:4.5}
% Previous models simply use a projection layer to get the entity type embedding and concatenate it with entity context-aware embedding or use entity type implicitly, which is not adequate for fine-grained entity and relation. To further explore the information gain of fine-grained entities, we add a mask on the predicted logits to filter entity-relation-entity triplet types which do not appear in the dataset.
% use knowledge learned from \docredfe~and a mask to filter entity-relation triplets which do not appear in the dataset. 
\begin{table}[t]
\caption{%Model Comparison in \docredfe{}. 
%Model Comparison on DocRED-FE.
F1 represents only using fine-grained type id. F1 with SE represents additionally aggregating semantic representations of fine-grained types. F1 with DLE represents aggregating double-level entity information.}
\footnotesize
\centering
% \setlength\tabcolsep{17pt}
\begin{tabular}{c c c c}
\toprule
{\textbf{Model}} & {\textbf{F1}} & {\textbf{F1 with SE}} & {\textbf{F1 with DLE}} \\ \midrule
BiLSTM & {49.28} & {50.26} & {49.81}\\
GAIN & {58.35} & {58.69} & {58.61}\\
JEREX & {58.19} &{58.26} & {58.34}\\
SSAN & {58.24} & {58.75} & {58.30}\\
\bottomrule
\end{tabular}
\label{tab:DLESE}
%\vspace{-0.5cm}
\end{table}

% Table~\ref{tab:mask} shows that adding mask to previous model could still improve RC results significantly. That proves previous models can not well use information in fine-grained entity. Thus there still exists space to mine  fine-grained entity information for improving RC performance.





\section{Conclusion}
\label{sec:bibtex}
% We propose a large-scale dataset named \docredfe{} to facilitate the research on document-level fine-grained JERE. Through abundant experiments we find that, document-level fine-grained JERE remains a challenging problem.
%Analysis also provide insight that entity information can further help relation extraction.
% Besides, the performance of relation classification improves with simple integration of fine-grained entity type information. In the future, we'd like to explore more expressive model architectures for mining fine-grained entity information. 
% In the future, we will 
% From the above experimental results and analysis we can conclude that fine-grained JERE is more challenging to former models and intensive efforts are needed to mine the information in fine-grained entity. Analysis on RC support that fine-grained type also has positive effect.
% Construction of \docredfe{} is continuing, the larger scale will provide sufficent trainning for the models. More possible availability of \docredfe{} like document-level NER task will be explore in the future.
% We believe the following research directions are worth following: (1) Exploring models explicitly considering entity type; (2) Designing more expressive model architectures for mining fine-grained entity information.

In this paper, we introduce \docredfe{}, a new dataset consisting of double-level entity types, including 11 coarse-grained types and 119 fine-grained types. Compared with existing JERE dataset, \docred{}, our proposed dataset provides a more difficult benchmark for current JERE models and richer entity-level information. To explore the advantages of our new entity schema, we propose and experiment some simple, general but effective ways. We hope \docredfe{} can contribute to future research on building accurate and robust JERE models. 



\section{Acknowledgement}

We thank the anonymous reviewers for their helpful comments on this paper. This work was partially supported by National Key Research and Development Project (2022YFC3600402) and National Natural Science Foundation of China (61876009).


%\newpage



% References should be produced using the bibtex program from suitable
% BiBTeX files (here: strings, refs, manuals). The IEEEbib.bst bibliography
% style file from IEEE produces unsorted bibliography list.
% -------------------------------------------------------------------------
\bibliographystyle{IEEEbib}
\bibliography{strings,refs}

\end{document}
