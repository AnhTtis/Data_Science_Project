%%
%% This is file `sample-sigconf.tex',
%% generated with the docstrip utility.
%%
%% The original source files were:
%%
%% samples.dtx  (with options: `sigconf')
%% 
%% IMPORTANT NOTICE:
%% 
%% For the copyright see the source file.
%% 
%% Any modified versions of this file must be renamed
%% with new filenames distinct from sample-sigconf.tex.
%% 
%% For distribution of the original source see the terms
%% for copying and modification in the file samples.dtx.
%% 
%% This generated file may be distributed as long as the
%% original source files, as listed above, are part of the
%% same distribution. (The sources need not necessarily be
%% in the same archive or directory.)
%%
%%
%% Commands for TeXCount
%TC:macro \cite [option:text,text]
%TC:macro \citep [option:text,text]
%TC:macro \citet [option:text,text]
%TC:envir table 0 1
%TC:envir table* 0 1
%TC:envir tabular [ignore] word
%TC:envir displaymath 0 word
%TC:envir math 0 word
%TC:envir comment 0 0
%%
%%
%% The first command in your LaTeX source must be the \documentclass
%% command.
%%
%% For submission and review of your manuscript please change the
%% command to \documentclass[manuscript, screen, review]{acmart}.
%%
%% When submitting camera ready or to TAPS, please change the command
%% to \documentclass[sigconf]{acmart} or whichever template is required
%% for your publication.
%%
%%
\documentclass[sigconf, nonacm]{acmart}

%%
%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    Bib\TeX}}}

%% Rights management information.  This information is sent to you
%% when you complete the rights form.  These commands have SAMPLE
%% values in them; it is your responsibility as an author to replace
%% the commands and values with those provided to you when you
%% complete the rights form.
\setcopyright{acmcopyright}
\copyrightyear{2023}
\acmYear{2023}
\acmDOI{XXXXXXX.XXXXXXX}

% %% These commands are for a PROCEEDINGS abstract or paper.
% \acmConference[Conference acronym 'XX]{Make sure to enter the correct
%   conference title from your rights confirmation emai}{June 03--05,
%   2018}{Woodstock, NY}
% %%
% %%  Uncomment \acmBooktitle if the title of the proceedings is different
% %%  from ``Proceedings of ...''!
% %%
% %%\acmBooktitle{Woodstock '18: ACM Symposium on Neural Gaze Detection,
% %%  June 03--05, 2018, Woodstock, NY}
% \acmPrice{15.00}
% \acmISBN{978-1-4503-XXXX-X/18/06}

%% NOTE that a single column version is required for 
%% submission and peer review. This can be done by changing
%% the \doucmentclass[...]{acmart} in this template to 
%% \documentclass[manuscript,screen]{acmart}
%% 
%% To ensure 100% compatibility, please check the white list of
%% approved LaTeX packages to be used with the Master Article Template at
%% https://www.acm.org/publications/taps/whitelist-of-latex-packages 
%% before creating your document. The white list page provides 
%% information on how to submit additional LaTeX packages for 
%% review and adoption.
%% Fonts used in the template cannot be substituted; margin 
%% adjustments are not allowed.

%%
%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    \normalfont B\kern-0.5em{\scshape i\kern-0.25em b}\kern-0.8em\TeX}}}

    %% Self-defined macros
\newcommand{\swap}[3][-]{#3#1#2} % just an example
% \newcommand{\texttt{CUB-S}}{\texttt{CUB-S}}
\newcommand{\Xcal}{\mathcal{X}}
\newcommand{\Ycal}{\mathcal{Y}}

\newcommand{\Ccal}{\mathcal{C}}
\newcommand{\br}[1]{\left({#1}\right)}

%% Some suggested packages, as needed:
\usepackage{natbib} % has a nice set of citation styles and commands
    \bibliographystyle{plainnat}
    \renewcommand{\bibsection}{\subsubsection*{References}}
\usepackage{mathtools} % amsmath with fixes and additions
% \usepackage{siunitx} % for proper typesetting of numbers and units
\usepackage{booktabs} % commands to create good-looking tables
\usepackage{tikz} % nice language for creating drawings and diagrams
\usepackage[mode=buildnew]{standalone}

\usepackage{caption}
\usepackage{subcaption}
% \usepackage{subfigure}


\usepackage{todonotes, xcolor}
\newcommand{\djcomment}[1]{{\color{red} Dj:{#1}}}
\newcommand{\kccomment}[1]{{\color{blue} KC:{#1}}}

%% Rights management information.  This information is sent to you
%% when you complete the rights form.  These commands have SAMPLE
%% values in them; it is your responsibility as an author to replace
%% the commands and values with those provided to you when you
%% complete the rights form.
% \setcopyright{acmcopyright}
% \copyrightyear{2018}
% \acmYear{2018}
% \acmDOI{XXXXXXX.XXXXXXX}


%%
%% These commands are for a JOURNAL article.
% \acmJournal{TOG}
% \acmVolume{37}
% \acmNumber{4}
% \acmArticle{111}
% \acmMonth{8}

%%
%% Submission ID.
%% Use this when submitting an article to a sponsored event. You'll
%% receive a unique submission ID from the organizers
%% of the event, and this ID should be used as the parameter to this command.
%%\acmSubmissionID{123-A56-BU3}

%%
%% For managing citations, it is recommended to use bibliography
%% files in BibTeX format.
%%
%% You can then either use BibTeX with the ACM-Reference-Format style,
%% or BibLaTeX with the acmnumeric or acmauthoryear sytles, that include
%% support for advanced citation of software artefact from the
%% biblatex-software package, also separately available on CTAN.
%%
%% Look at the sample-*-biblatex.tex files for templates showcasing
%% the biblatex styles.
%%

%%
%% The majority of ACM publications use numbered citations and
%% references.  The command \citestyle{authoryear} switches to the
%% "author year" style.
%%
%% If you are preparing content for an event
%% sponsored by ACM SIGGRAPH, you must use the "author year" style of
%% citations and references.
\citestyle{acmauthoryear}

%%
%% end of the preamble, start of the body of the document source.
\begin{document}

%%
%% The "title" command has an optional parameter,
%% allowing the author to define a "short title" to be used in page headers.


% \title{On the Importance of Human Uncertainty in Human-Machine Collaborations}
% \title{On the Importance of Human Uncertainty in Intervenable Concept-Based AI Systems}

\title{Human Uncertainty in Concept-Based AI Systems}
% \title{On the Importance of Human Uncertainty in Concept-Based AI Systems}

%%
%% The "author" command and its associated commands are used to define
%% the authors and their affiliations.
%% Of note is the shared affiliation of the first two authors, and the
%% "authornote" and "authornotemark" commands
%% used to denote shared contribution to the research.


\author{Katherine M. Collins}
% \authornotemark[1]
\affiliation{%
  \institution{University of Cambridge}
  \country{United Kingdom}
}
\email{kmc61@cam.ac.uk}


\author{Matthew Barker}
\authornote{Contributed equally (sorted alphabetically by last name).}
% \authornotemark[1]
% \email{}
\affiliation{%
  \institution{University of Cambridge}
  \country{United Kingdom}
}

\author{Mateo Espinosa Zarlenga}
\authornotemark[1]
% \email{kmc61@cam.ac.uk}
\affiliation{%
  \institution{University of Cambridge}
  \country{United Kingdom}
}

\author{Naveen Raman}
\authornotemark[1]
% \email{kmc61@cam.ac.uk}
\affiliation{%
  \institution{University of Cambridge}
  \country{United Kingdom}
}

\author{Umang Bhatt}
% \email{kmc61@cam.ac.uk}
\affiliation{%
  \institution{University of Cambridge \\ Alan Turing Institute}
  \country{United Kingdom}
}

\author{Mateja Jamnik}
% \email{kmc61@cam.ac.uk}
\affiliation{%
  \institution{University of Cambridge}
  \country{United Kingdom}
}

\author{Ilia Sucholutsky}
% \email{kmc61@cam.ac.uk}
\affiliation{%
  \institution{Princeton University}
  \country{United States}
}

\author{Adrian Weller}
% \email{kmc61@cam.ac.uk}
\affiliation{%
  \institution{University of Cambridge \\ Alan Turing Institute}
  \country{United Kingdom}
}

\author{Krishnamurthy (Dj) Dvijotham}
% \email{kmc61@cam.ac.uk}
\affiliation{%
  \institution{Google Research (Brain Team)}
  \country{United States}
}

% \author{Ben Trovato}
% \email{trovato@corporation.com}
% \orcid{1234-5678-9012}
% \author{G.K.M. Tobin}
% \authornotemark[1]
% \email{webmaster@marysville-ohio.com}
% \affiliation{%
%   \institution{Institute for Clarity in Documentation}
%   \streetaddress{P.O. Box 1212}
%   \city{Dublin}
%   \state{Ohio}
%   \country{USA}
%   \postcode{43017-6221}
% }

% \author{Lars Th{\o}rv{\"a}ld}
% \affiliation{%
%   \institution{The Th{\o}rv{\"a}ld Group}
%   \streetaddress{1 Th{\o}rv{\"a}ld Circle}
%   \city{Hekla}
%   \country{Iceland}}
% \email{larst@affiliation.org}

% \author{Valerie B\'eranger}
% \affiliation{%
%   \institution{Inria Paris-Rocquencourt}
%   \city{Rocquencourt}
%   \country{France}
% }

% \author{Aparna Patel}
% \affiliation{%
%  \institution{Rajiv Gandhi University}
%  \streetaddress{Rono-Hills}
%  \city{Doimukh}
%  \state{Arunachal Pradesh}
%  \country{India}}

% \author{Huifen Chan}
% \affiliation{%
%   \institution{Tsinghua University}
%   \streetaddress{30 Shuangqing Rd}
%   \city{Haidian Qu}
%   \state{Beijing Shi}
%   \country{China}}

% \author{Charles Palmer}
% \affiliation{%
%   \institution{Palmer Research Laboratories}
%   \streetaddress{8600 Datapoint Drive}
%   \city{San Antonio}
%   \state{Texas}
%   \country{USA}
%   \postcode{78229}}
% \email{cpalmer@prl.com}

% \author{John Smith}
% \affiliation{%
%   \institution{The Th{\o}rv{\"a}ld Group}
%   \streetaddress{1 Th{\o}rv{\"a}ld Circle}
%   \city{Hekla}
%   \country{Iceland}}
% \email{jsmith@affiliation.org}

% \author{Julius P. Kumquat}
% \affiliation{%
%   \institution{The Kumquat Consortium}
%   \city{New York}
%   \country{USA}}
% \email{jpkumquat@consortium.net}

%%
%% By default, the full list of authors will be used in the page
%% headers. Often, this list is too long, and will overlap
%% other information printed in the page headers. This command allows
%% the author to define a more concise list
%% of authors' names for this purpose.


\renewcommand{\shortauthors}{Collins, et al.}

%%
%% The abstract is a short summary of the work to be presented in the
%% article.


\begin{abstract}
Placing a human in the loop may abate the risks of deploying AI systems in safety-critical settings ( e.g., a clinician working with a medical AI system). However, mitigating risks arising from human error and uncertainty within such human-AI interactions is an important and understudied issue. 
In this work, we study human uncertainty in the context of concept-based models, a family of AI systems that enable human feedback via concept interventions where an expert intervenes on human-interpretable concepts relevant to the task. 
Prior work in this space often assumes that humans are oracles who are always certain and correct. Yet, real-world decision-making by humans is prone to occasional mistakes and uncertainty. We study how existing concept-based models deal with uncertain interventions from humans using two novel datasets: \texttt{UMNIST}, a visual dataset with controlled simulated uncertainty based on the MNIST dataset, and \texttt{CUB-S}, a relabeling of the popular \texttt{CUB} concept dataset with rich, densely-annotated soft labels from humans. We show that training with uncertain concept labels may help mitigate weaknesses of concept-based systems when handling uncertain interventions. These results allow us to identify several open challenges, which we argue can be tackled through future multidisciplinary research on building interactive uncertainty-aware systems. To facilitate further research, we release a new elicitation platform, \texttt{UElic}, to collect uncertain feedback from humans in collaborative prediction tasks. 
\end{abstract}

%%
%% The code below is generated by the tool at http://dl.acm.org/ccs.cfm.
%% Please copy and paste the code instead of the example below.
%%
% \begin{CCSXML}
% <ccs2012>
%  <concept>
%   <concept_id>10010520.10010553.10010562</concept_id>
%   <concept_desc>Computer systems organization~Embedded systems</concept_desc>
%   <concept_significance>500</concept_significance>
%  </concept>
%  <concept>
%   <concept_id>10010520.10010575.10010755</concept_id>
%   <concept_desc>Computer systems organization~Redundancy</concept_desc>
%   <concept_significance>300</concept_significance>
%  </concept>
%  <concept>
%   <concept_id>10010520.10010553.10010554</concept_id>
%   <concept_desc>Computer systems organization~Robotics</concept_desc>
%   <concept_significance>100</concept_significance>
%  </concept>
%  <concept>
%   <concept_id>10003033.10003083.10003095</concept_id>
%   <concept_desc>Networks~Network reliability</concept_desc>
%   <concept_significance>100</concept_significance>
%  </concept>
% </ccs2012>
% \end{CCSXML}

% \ccsdesc[500]{Computer systems organization~Embedded systems}
% \ccsdesc[300]{Computer systems organization~Redundancy}
% \ccsdesc{Computer systems organization~Robotics}
% \ccsdesc[100]{Networks~Network reliability}

%%
%% Keywords. The author(s) should pick words that accurately describe
%% the work being presented. Separate the keywords with commas.
\keywords{human-in-the-loop, interactive, uncertainty, concept learning, XAI}

% \received{20 February 2007}
% \received[revised]{12 March 2009}
% \received[accepted]{5 June 2009}

%%
%% This command processes the author and affiliation and title
%% information and builds the first part of the formatted document.
 \maketitle


% Our research question: What are the consequences of not taking into account human uncertainty in concept-based models? (at train- and test-time). 
% First highlight ``bread-and-butter'' of field: CUB. With \texttt{CheXpert}, also need to make a choice.  [details of comp exps to appendix]
% These challenges even propogate to interventions, which is core. Not helpful if users are uncertain!!! Selects things humans likely wrong. 
% ^Criticized dataset unc + what happens if you don't consider them.
% Well what if you have uncertainty??? Train w/ unc ---> generalize better. If you have it: beneficial! MNIST exps. ***Not just bad not to have, but can be good to have.
% Given all of this, call-to-action to collect better uncertainty info: to allow this, we provide a new interface. [note: not-visible label....]
% In this interface, we offer a relabeling of a subset of \texttt{CUB} for experimentation: CUB-S. Still a challenge! Espec with calibration. 



\section{Introduction}


% \textit{Note: the main prose of the intro is a bit out-dated. Needs editing for revised story structure.}

Human-in-the-loop machine learning (ML) systems are often framed as a promising way to reduce risks in settings where automated models cannot be solely relied upon to make decisions \citep{shneiderman2022human}. However, what if the humans themselves are unsure? Can such systems robustly rely on human interventions which may be inaccurate or uncertain? Concept-based models (e.g., Concept Bottleneck Models (CBMs) \citep{koh2020concept} and Concept Embedding Models (CEMs) \citep{cem22}), are ML models that enable users to improve their predictions via feedback in the form of human-interpretable ``concepts'', as opposed to feedback in the original feature space (e.g., pixels of an image). For instance, a radiologist can identify concepts like lung lesions or a fracture to aid a model which uses chest X-rays to predict diseases. Such human-in-the-loop systems typically assume that the intervening human is always correct and confident about their interventions; a so-called ``oracle'' whose predictions should always override those of the model (see Figure~\ref{fig:main-schematic}A). Yet, uncertainty is an integral component of the way humans reason about the world ~\citep{uncertainJudgments, Gha15, probBrain, lake_ullman_tenenbaum_gershman_2017}. If a doctor is unsure of whether a lung lesion is present, or a human cannot observe a feature in a bird due to occlusion (e.g. the tail of a bird is hidden from view), it may be safer to permit them to express this uncertainty \citep{medUncertainty, laidlaw2021uncertain, schneider2022effects}. Human-in-the-loop systems, which can take uncertainty into account when responding to human interventions, would mitigate the risks of both end-to-end automation and human error (see Figure~\ref{fig:main-schematic}B). 


% Supporting humans' ability to \textit{intervene} on the predictive process of machine learning (ML) systems is paramount for trustworthy deployment. Concept-based models are an exemplary system designed with this ease-of-intervention in mind. Concept-based models, e.g., Concept Bottleneck Models (CBMs) \citep{koh2020concept} and Concept Embedding Models (CEMs) \citep{cem22}, permit a user to edit predictions through human-interpretable ``concepts'' rather than in the raw example space (e.g., in the space of pixels). For instance, a doctor can ..... 

% For such systems to be used effectively in practice, users may not be able to intervene directly on each concept due to limited time, lack of experience, ...[something about fairness?] As such, there is a need for schemes designed to select the best next concept to query. \citep{interactiveconcept} take a step to address this challenge by proposing adaptive policies which can efficiently determine which concept would be most beneficial to query a human for. 

% However, these systems typically make a critical assumption: the human user is an oracle -- and specifically, a \textit{confident} oracle. Yet, uncertainty is an integral component of the way humans reason about the world ~\citep{uncertainJudgments, Gha15, probBrain, lake_ullman_tenenbaum_gershman_2017}. If a doctor is unsure of ...., or a human is unable to observe a concept due to occlusion (e.g.,....), it is sensible -- and ...[something about trust/transparency/safe?] to permit them to express this uncertainty. 
Just as machines ``knowing when they don't know'' has been emphasized for reliability \citep{bhatt2021uncertainty, liu2020simple, uncTrustModel, hendrycks2019benchmarking}, we emphasize \texttt{empowering humans to express when they do not know} as a way to improve trustworthy deployment and outcomes. Recent works have demonstrated the benefits of incorporating uncertainty over label spaces on predictive performance \citep{hinton2015distilling, muller2019labelSmoothHelp, peterson2019human, selfCiteSoftLabel,informativenessSoft,hillmixup,sandersambiguous}; we continue this tradition in the space of concept-based \textit{feedback}. Specifically, our contributions can be summarized as follows:

%While [cite Branson] do build a probabilistic model for human uncertainty in concepts --- here, we focus on richer, more extendable and flexible paradigms to handle human uncertainty in their feedback to ML systems. 

%\djcomment{I think the paragraphs below duplicate the contributions section and can be removed, removing now temporarily}

%To that end, here, we challenge this certainty assumption in concept-based models -- and consider the case where humans are uncertain of the concept interventions. We study existing concept-based models across a suite of simulated and in-the-wild uncertainty-laded datasets -- spanning a range of forms of uncertainty (discrete and categorical). We contribute two new datasets: 1) \texttt{UMNIST}, a controlled playground to study concept uncertainty, and 2) \texttt{CUB-S}, an exciting challenge dataset that relabels a subset of the popular concept dataset \texttt{CUB} \cite{WahCUB_200_2011}. These datasets are designed to address core limitations of existing concept datasets with uncertainty (e.g., CUB), i.e., their lack of reliability (CUB is highly noisy [cite]) and their limited richness (only coarse-grained uncertainty is elicited).

%We highlight through several empirical studies that whether, and how, you permit uncertainty to be expressed at test-time, has an effect on intervention efficacy. These findings hold practical implications for developers of concept-based systems (it is important to ensure robustness to a variety of possible forms of human uncertainty) and designers of interactive systems (the \textit{form} of uncertainty you permit users to express may change performance). Further, we demonstrate early indications that \textit{training} with even simulated levels of uncertainty may improve test-time robustness: if you a priori know that users will be uncertain, it is sensible to incorporate uncertainty into the train set.

%However, we emphasize much remains on the methodological front when it comes to handling human uncertainty. More work is needed -- necessitating expanded, quality collection of human uncertainty datasets in the ML community. To facilitate the collection of further uncertain feedback from humans over concepts in this quest for safer, more robust interactive systems, we develop and offer our new elicitation platform (\texttt{UElic}) open-source.

%We develop and offer a new interface to support the study of human uncertainty over concepts and include a detailed 

% \subsection{Contributions}

% \begin{itemize}
%     \item Identification of challenges of 
% \end{itemize}

\begin{itemize}
    \item We introduce the safety-critical problem of human uncertainty in interactive, concept-based models. 
    \item We reveal failure modes of existing concept-based models when handling user uncertainty over concepts.
    \item We empirically demonstrate the value of training with uncertainty as a mitigation strategy for better handling test-time uncertainty. 
    \item We develop \texttt{UElic}, an extensible platform to facilitate the collection of rich, real-world human uncertainty over concepts. 
    \item We use \texttt{UElic} to curate a novel relabeling of \texttt{CUB} (called \texttt{CUB-S}) designed to address limitations in the present dataset. Furthermore, we illustrate how \texttt{CUB-S} can serve as a challenge dataset to study uncertain human interventions. 
    % \item Develop a new controlled, dataset to study uncertainty over concepts (UMNIST), as well as a rigorous empirical investigation into impact of design choices when leveraging the uncertainty annotations in CUB.
\end{itemize}

% \begin{itemize}
%     \item Identify the problem of human uncertainty over concepts in concept-based models 
%     \item Highlight limitations with popular concept datasets when considering human uncertainty
%     \item Demonstrate the value of considering human uncertainty at train-time \kccomment{via a new controlled simulated uncertainty dataset, \texttt{UMNIST}.}
%     \item Offer an extensible platform, \texttt{UElic}, to elicit human uncertainty over concepts. 
%     \item Use \texttt{UElic} to curate a novel relabeling of CUB: CUB-S, and illustrate how CUB-S can serve as a compelling challenge dataset to study uncertain human interventions. 
% \end{itemize}

% \kccomment{
% % Current story
% % \begin{itemize}
% %     \item Interactive systems on the rise. One particular class are concept-based models, which let people interact via high-level concepts.
% %     \item However, these systems usually assume that humans are oracles: and certain oracles. I.e., human uncertainty is not considered. 
% %     \item In this work, we address this assumption head-on -- analyzing empirically how existing concept-based models fare when we permit human users to express their uncertainty at intervention-time.
% %     \item However, in order to run these analyses -- we need datasets of human uncertainty over concepts. Popular concept-based datasets are rarely annotated with uncertainty, and if they are, such uncertainty scores are typically discrete -- lacking richness -- and do not include annotator-level ids, prohibiting the study of individual annotator calibration on performance. 
% %     \item We demonstrate how decisions that researchers make about how to use the uncertainty scores in existing concept datasets matters -- and more broadly, that allowing uncertainty at test-time can impact intervention efficacy. \textit{TBD on whether we also note this impact extends regardless of test-time policy, e.g. CooP, Skyline, Random.}
% %     \item We've focused on the issues that arise when handling human uncertainty in existing systems (i.e., those trained on deterministic concepts). But, there's a wealth of recent work demonstrating the value of learning with human uncertainty. 
% %     \item We next develop a novel controlled setting to study training with uncertainty: \texttt{UMNIST}. We find that training with uncertainty improves performance -- even under distribution shift -- but not too much uncertainty... 
% %     \item Seeing as there are not enough datasets with good uncertainty annotations, uncertainty does impact performance, and training with uncertainty can help: we strongly encourage more collection of real-world human uncertainty. To facilitate such work, we develop and offer a new elicitation methodology for human uncertainty over concepts: UElic. 
% %     \item We use UElic to relabel a subset of \texttt{CUB}, releasing the soft labels as CUB-S. 
% %     \item We demonstrate that CUB-S is a compelling challenge dataset (CooP breaksdown), which we encourage researchers to explore. We include several observations which we think are driving such failures, which we see as promising for future work.
% % \end{itemize}

% % Alternative story
% % \begin{itemize}
% %      \item Interactive systems on the rise. One particular class are concept-based models, which let people interact via high-level concepts.
% %     \item However, these systems usually assume that humans are oracles: and certain oracles. I.e., human uncertainty is not considered.
% %     \item This is a safety-risk! If someone is uncertain, they should not be forced to express certainty. 
% %     \item In this work, we address this safety-critical assumption head-on 
% %     \item To do so, we first study how concept-based models handle uncertainty in a simulated, controlled setting. We develop a new plug-and-play dataset for such study: \texttt{UMNIST}. 
% %     \item We see that intervening with uncertainty can break concept-based models which were not trained with uncertain information. CEMs are more robust than CBMs, but still imperfect.
% %     \item What can be done? There's a wealth of recent work demonstrating the value of learning with human uncertainty. We find that training with a (reasonable) amount of uncertainty is a compelling salve / mitigation approach. 
% %     \item However, this is in a very controlled setting. What happens as we build-up closer to the real-world? 
% %     \item We next turn to the uncertainty annotations in CUB. 
% %     \item Howevever, how do systems fare under real-world uncertainty? 
    
% %     -- analyzing empirically how existing concept-based models fare when we permit human users to express their uncertainty at intervention-time.
% % \end{itemize}


% \textbf{Notes on Updated story with Ilia (7pm GMT Thurs)} 

% \textit{Katie to edit paper for new story!}

%     spectrum: \texttt{UMNIST} entirely simulated ---> population uncertainty, but can't easily access individ unc ---> cub-s = individ unc. gives you unc at pop-level. most real-world.
%     spectrum: coarse --> soft. latter lets you build individ level models
%     simulating uncertainty 
%     real-world uncertainty 



%     umnist: need to be able to model uncertainty in general. population-level / some kind of uncertainty.
%     move to CUB: we have population-level uncertainty, and some degree of unc per individ. but a) not fine-grained, and b) can't model the individs in it. show downsides here. it matters what you choose!
%     so, final real-world case. access at this deep level to individ uncertainty. we develop a method for collecting this. existing methods not equipped to deal with this real-world case.
%     future work: collect more data of this type + develop models that work with it. 



%     at each level: blindly training with uncertainty is helpful. but increasingly becomes less impactful. future solution needed. 
%     method section at the beginning where we introduce the dataset to the models and the policies. 

%     in intro: pitch why we move from \texttt{UMNIST} -> \texttt{CUB} -> cub-s. we develop \texttt{UMNIST} has these qualities, but lack these others.... \texttt{CUB} has these qualities but not others, so we also collect this third dataset using a novel elicitation method. 

%     study across with state-of-the-art models and policies to study the effect of diff types of uncertainty: individ and population level. 


%     even a little bit of uncertainty plays a big role there -- see appendix for detals. 

% }


% \begin{itemize}
%     \item \emph{Mechanisms for uncertainty elicitation}: We develop and offer an extensible platform [add name] to elicit human uncertainty, designed with human cognitive load in-mind.
%     \item \emph{Dataset curation}: We use [add name] to curate a novel version of the Caltech UCSD Birds Dataset [cite] we call \texttt{CUB-S} that contains rich human annotations on concept labels along with associated uncertainties.  
%     \item \emph{Taxonomy of challenges when handling human uncertainty:} We offer a taxonomy of several challenges worth considering when building systems that incorporate human uncertain feedback. We demonstrate challenges with exposing existing concept-based models, and policies designed over such models, to human uncertainty at intervention-time. 
%     \item \emph{Mitigation strategies}: We explore several possible mitigation strategies to improve the ability of concept-based systems to handle human uncertainty across two settings: test-time only, and during train-time. 
       

%     % \item \emph{Mechanisms for uncertainty elicitation and dataset curation:} We develop mechanisms to elicit uncertainty feedback from humans, and curate a novel version of the Caltech UCSD Birds Dataset we call \texttt{CUB-S} that contains rich human annotations on concept labels along with associated uncertainties. 
%     % \item \emph{Benchmarking:} We demonstrate that existing concept based models \citet{koh2020concept,cem22} perform poorly when exposed to test time interventions based on uncertain feedback on concept labels, both on simulated uncertainty and real uncertain feedback (\texttt{CUB-S}). \kccomment{Dj: I'm realizing we should be careful with this framing. CEMs are actually quite robust with the simulated uncertainty, as are the CooP varities (but there is a wider gap w/ Skyline -- suggesting generalization gap...) CUB-S is where things, atm, really fall apart though.}
%     % \item \emph{Test-time only mitigations:} We show that for the simluated uncertainty where the uncertainty levels are predictable and well-calibrated, it is possible to modify the policies developed in \citet{interactiveconcept} to handle uncertainty better. However, these fail on actual human uncertainty which is often miscalibrated and differs between humans.
%     % \item \emph{Training-based mitigations:} We develop a novel training method for training concept models with uncertainty concept annotations and show that models trained this way are more resilient to uncertain feedback and test time and can systematically improve performance with a modest number of interventions despite uncertain feedback.
% \end{itemize}

% \begin{figure}[!htb]
%     \centering
%     \includegraphics[width=1.0\linewidth]{figures/overview.tex}
%     \caption{Schematic of uncertainty in test-time interventions in concept-based models. When presented with features and a concept to annotate, a human user may be uncertain. We empower the user to express this uncertainty when intervening on concepts.}%\kccomment{We may want to have a diff central figure, given less emphasis now on intervention policies.}}
%     \label{fig:main-schematic}
% \end{figure}

% \begin{figure*}[!htb]
%     \centering
%     \includegraphics[width=1.0\linewidth]{figures/UElic-CUB.pdf}
%     \caption{Schematic of uncertainty in test-time interventions in concept-based models. When presented with features and a concept to annotate, a human user may be uncertain. We empower the user to express this uncertainty when intervening on concepts: to make the concept-based systems \textit{aware of their uncertainty}.}%We find that training over uncertain concepts (i.e., making concept-based models ``uncertainty-aware'' improves flexibility to handling uncertainty at test-time.}%\kccomment{We may want to have a diff central figure, given less emphasis now on intervention policies.}}
%     \label{fig:main-schematic}
% \end{figure*}

\begin{figure*}[!htb]
    \centering
    \includegraphics[width=1.0\linewidth]{figures/UElic-Chexpert.pdf}
    \caption{Schematic of uncertainty in test-time interventions in concept-based models. When presented with features and a concept to annotate, a human user may be uncertain. We empower the user to express this uncertainty when intervening on concepts: to make the concept-based systems \textit{aware of their uncertainty}. We demonstrate the set-up in a hypothesized safety-critical setting of medical diagnosis; X-ray images are depicted from CheXpert \cite{irvin2019chexpert}.}%We find that training over uncertain concepts (i.e., making concept-based models ``uncertainty-aware'' improves flexibility to handling uncertainty at test-time.}%\kccomment{We may want to have a diff central figure, given less emphasis now on intervention policies.}}
    \label{fig:main-schematic}
\end{figure*}


% \section{Why Care about Human Uncertainty?} 

% Broad problem statement about human uncertainty and why it matters for an AIES audience. Safety-critical. Dangerous to force an annotator to express certainty when they're unsure, e.g., in medical setting. 

% Some simple, principled example to reveal issues w/ not considering uncertainty \textit{in interventions}, \textit{over concepts}. 

% \section{Primer on Concept-Based Models} 

% First, we introduce concept-based models.  [...overview for an AIES audience...]

% \subsection{Why Care about Concept-Based Models?}

% % \section{Uncertainty in Concept-Based Models} 

% % Where does human uncertainty matter in concept-based models? 

% \subsection{Existing Critiques} Concept-based models, and the broader ecosystem in which they are deployed, have ..... several critiques, e.g., .... However, to our knowledge, we are the first work which directly considers \textit{uncertainty in the human user} with concept-based models.


\section{Primer on Concept-Based Systems}

In this section, we introduce concept-based models and discuss how their design enables concept interventions. Concept-based models use human-interpretable values (concepts) as intermediate representations when predicting a task label~\citep{koh2020concept}. The aim of such models is to improve the interpretability of the outputs and to facilitate human interventions which correct model mistakes~\citep{koh2020concept,chen2020concept, barbiero2022entropy, cem22, chauhan2022interactive}.

\subsection{Notation}
We consider the supervised case where each datapoint consists of (input $\mathbf{x} \in \mathcal{X}$, concepts $\mathbf{c} \in \mathcal{C}$, output $\mathbf{y} \in \mathcal{Y}$). Typically, concepts $\mathbf{c} = [c_1, c_2, \cdots, c_k]^T$ are binary (indicating that concept is ``on'' or ``off''; e.g., oedema is or is not present), or categorical (e.g., different wing colors). Notice, however, that categorical concepts can be converted into binary concepts (e.g., wing color is or is not blue). Typically, concept presence is annotated as being ``on'' or ``off'' ($c_i \in \{0, 1\}$); however, there may be \textit{uncertainty} over a concept's presence, which necessitates a continuous value. For that reason, in this work, we let concepts live $\in [0,1]$, representing $p(c_i | x)$.  

\subsection{Models}
Concept-based models predict the concepts from an intermediate layer in a neural network. Although a plethora of such systems have been developed \citep{koh2020concept, cem22, postHocCBMs, conceptSidecar, languageCBM, oikarinenlabel}, in this work we focus on Concept Embedding Models (CEMs) \citep{cem22} as they represent a recent extension of the popular Concept Bottleneck Models (CBMs) \citep{koh2020concept}. 

CBMs learn two mappings, one from the input to the concepts $g: \mathcal{X} \rightarrow \mathcal{C}$, and another from the concepts to the outputs $f: \mathcal{C} \rightarrow \mathcal{Y}$. The overall prediction is given by:
\begin{equation}
    \hat{y} = f(\mathbf{\hat{c}}) = f(g(\mathbf{x}))
\end{equation}
There are many ways of learning $g$ and $f$; here, we focus on the joint bottleneck, which learns $g$ and $f$ at the same time, simultaneously minimizing the concept prediction loss and the output prediction loss. In this work we focus on CBMs with sigmoidal activations in their concept layers whose output can be interpreted as a concept's probability of activation. CEMs further extend CBMs by learning supervised embeddings for each concept, representing concepts as high dimensional vectors while still learning to predict their values as an intermediate step~\citep{cem22}. This allows CEMs to better leverage their capacity when trained on datasets with an ``incomplete'' set of concept annotations~\citep{cem22, yeh2020completeness}. We use \textbf{training with uncertainty} to refer to models trained with concepts represented as probabilities $\in [0,1]$, rather than as binary concepts. The target $y$ is left unchanged in this work. 

% Experimentally, we discover non-sigmoidal activation functions in a CBM's concept layer (e.g., logits) perform poorly under uncertainty (see Supplement). Therefore, in this work we focus on CBMs with sigmoidal activations in their concept layers whose output can be interpreted as a concept's probability of activation. 


% \citeauthor{koh2020concept} describe three ways of learning $\hat{g}$ and $\hat{f}$:
% \begin{enumerate}
%     \item \emph{Independent bottleneck} which learns $\hat{g}$ and $\hat{f}$ separately, with the true concepts used as targets for $\hat{g}$ and inputs for $\hat{f}$.
%     \item \emph{Sequential bottleneck} which learns $\hat{g}$, and then uses the predicted concepts $\hat{c}$ to train $\hat{f}$.
%     \item \emph{Joint bottleneck} which learns $\hat{g}$ and $\hat{f}$ at the same time, jointly minimizing the concept prediction loss and the output prediction loss.
% \end{enumerate}



%Representing concepts as high-dimensional vectors helps to address the ``accuracy-vs-interpretability tradeoff'' and we show that they are more robust under uncertainty.

\subsection{Interventions}

A prime motivation for employing concept-based systems is the ease of intervenability. If a user notices that the model is predicting a concept incorrectly (e.g., the X-ray scan shows bone spurs, yet the model predicted no bone spurs), a user (e.g., a medical professional) can directly edit said concept to (potentially) update the prediction. This involves updating a predicted concept $\hat{c}_i$ with the concept value returned by the human $\hat{c}_i \leftarrow c_i$ and recalculating our prediction $\hat{y} = f(\mathbf{\hat{c}})$. Because these interventions edit the \textit{model's predicted probability of a given concept}, we can readily permit the user to edit \textit{with their own predicted probability of that concept}. When we refer to \textbf{testing with uncertainty}, we let the human edited concept be a probability, analogous to our ``training with uncertainty'' setting.

In this work, we consider two policies to select the concept to intervene on: 1) \textit{Random}: selecting the next concept to query randomly of concepts, and 2) \textit{Skyline:} an approximate oracle policy following \citeauthor{chauhan2022interactive}, which selects the next concept to query that will best impact performance (as if it were possible to know, simulating an upper bound on intervention efficacy; see Supplement for further details). While other works have been developed with more advanced policies \citep{chauhan2022interactive, sheth2022learning, shin2023closer}, we select Random and Skyline because they illustrate the \textit{bounds} on achievable performance; Random being the most naive policy and Skyline being the optimal policy. Unless otherwise noted, concepts are chosen via Random. 

\subsection{Critiques and Common Assumptions} Concept-based models, and the broader ecosystem in which they are deployed, have been shown to exhibit information leakage~\citep{mahinpei2021promises} or impurities distributed across concept representations~\citep{zarlenga2023towards}, spurious input saliency maps \citep{margeloiu2021concept}, bloated, hard-to-learn concept definitions \citep{ramaswamy2022overlooked}, and propensity to be influenced by correlations amongst concepts \citep{heidemann2023concept}. To our knowledge, we are the first work which directly considers \textit{uncertainty in the human user} with concept-based models.

% \djcomment{Is the paragraph below still accurate given we no longer use CooP?}

% selecting the most informative concept, as if you were an 

% In this work, we employ two intervention policies: 1) random selection of concepts, following \citep{koh2020concept} and \citep{cem22}, and 2) ``Skyline'' as proposed in \citeauthor{chauhan2022interactive} to model the \textit{best possible} intervention that can be achieved.

% \paragraph{Why Care about Concept-Based Models?}

% A core asset of concept-based models is their ease of intervention [cites]. 

% \section{Uncertainty in Concept-Based Models} 

% Where does human uncertainty matter in concept-based models? 



% \subsection{Intervention Policies}

% A core asset of concept-based models is their ease of intervention [cites]. 

% In this work, we focus on three different policies to select the concept to query: 

% \begin{itemize}
%     \item \textit{Skyline:}
%     \item \textit{Random:}
%     \item \textit{CooP:} 
% \end{itemize}

\section{Research Questions}

In this work, we address the following research questions: 

\begin{itemize}
    \item \textbf{RQ1}: How do existing concept-based systems handle the introduction of human uncertainty at test time?
    \item \textbf{RQ2}: How can systems be bolstered to better support human uncertainty at test time? 
    \item \textbf{RQ3}: How does the level and form of the uncertainty (e.g., whether the uncertainty is expressed through discrete annotations, or rich, continuous probabilities) impact performance? %Does the form of uncertainty matter? 
\end{itemize}

We investigate these questions across a \textit{spectrum of degrees and forms of uncertainty}. First, we study controlled, simulated uncertainty in \texttt{UMNIST}, our newly proposed addition dataset based on the \texttt{MNIST} handwritten digit recognition, as well as over the popular medical dataset \texttt{CheXpert} \citep{irvin2019chexpert}. We then depart from considering simulated uncertainty -- moving moving to the real, human-elicited in-the-wild uncertainty; first, coarse-grained uncertainty scores collected in \texttt{CUB} \citep{WahCUB_200_2011}, and then richer uncertainty which we collect in our new real-world dataset of human uncertainty: \texttt{CUB-S}.  

For each dataset, we study the test-time performance of models trained on binary, certain concepts, but faced with uncertainty at test-time. Then, we explore how this performance is affected when the same models are trained with uncertainty estimations in concept labels. 

% \subsection{Experimental Set-Up}

% In this work, we consider two policies to select the concept to intervene on: 1) random selection of concepts, and 2) an approximate oracle policy  -- ``Skyline'' -- following \citeauthor{chauhan2022interactive}, which selects the next concept to query that will best impact performance (as if it were possible to know, simulating an upper bound on intervention efficacy; see Supplement for further details). While other works have been developed with more clever policies \citep{chauhan2022interactive, sheth2022learning, shin2023closer} --- we select Random and Skyline as they illustrate the \textit{bounds} on achievable performance, Random being the most naive policy and Skyline being the optimal policy. 

% Our datasets cover a spectrum of types of uncertainty [add fig showing spectrum of ``simulatedness'' / axes of uncertainty (discrete vs. continuous, simulated vs. real); or as a table].

% \subsection{Data}

% In this work, we consider ... datasets. Our datasets cover a spectrum of types of uncertainty [add fig showing spectrum of ``simulatedness'' / axes of uncertainty (discrete vs. continuous, simulated vs. real); or as a table].

% \subsubsection{UMNIST} We introduce a new dataset for the controlled study of uncertainty over concepts... 

% \subsubsection{CheXpert} We explore the original uncertain annotations in \texttt{CheXpert} \citep{irvin2019chexpert}, which contains concept annotations from chest x-rays. The dataset is marked with four labels: positive, negative, unknown, and uncertain. For our experiments, we vary the value taken by uncertain labels, both at train and test time, and investigate its impact on intervention performance. \kccomment{To update for the simulated \texttt{CheXpert} uncertainty. ``Real'' uncertainty description for Appendix.}%[likely in appendix; but may go in main if we include with simulated uncertainty]

% \subsubsection{CUB} We leverage the uncertainty annotations provided in the original \texttt{CUB} dataset \citep{WahCUB_200_2011}, which .... 

% \subsubsection{CUB-S} Given the limitations of prior datasets (i.e., lack of richness, ....), we collect and release a relabeling of CUB. 

% % \subsubsection{Eliciting Human Uncertainty} To address the gap in human uncertainty-annotated datasets -- we encourage researchers to expand beyond the collection of CUB-S. We release our interface as a standalone platform, \texttt{UElic} to facilitate the collection of other such datasets. \kccomment{Not clear where to put section on our elicitation methodology, UElic. Can add here, but it goes beyond CUB-S. Either before? Or at the very end wrt expanding-beyond-this work}.  

% % \subsection{Evaluation Metrics}

% % We focus on three core metrics when measuring model efficacy: accuracy, AUC, and the difference beteween the probability the model assigns to the true class and the probability assigned to the top incorrect class.

% % We also measure calibration via expected calibration error (ECE) [cite]. 

% \section{Experiments}

% % We next address our dual research questions: 1) how do existing concept-based systems handle the introduction of human uncertainty at test-time?, and 2) if they fail, how can systems be bolstered to better support human uncertainty? 

% We next address our research questions: 1) how do existing concept-based systems handle the introduction of human uncertainty at test-time?, and 2) if they fail, how can systems be bolstered to better support human uncertainty at test-time? 

% %1) how do existing concept-based systems handle the introduction of human uncertainty at test-time?, and 2) if they fail, how can systems be bolstered to better support human uncertainty at test-time? 

% We investigate these questions across a \textit{spectrum of degrees and forms of uncertainty}. First, we study controlled, simulated uncertainty in our newly proposed variant of MNIST: \texttt{UMNIST}, \kccomment{\textit{as well over the popular medical dataset, \texttt{CheXpert} \citep{irvin2019chexpert}.}} We then increase the degree of realism, moving to the in-the-wild coarse-grained uncertainty scores collected in CUB. We then close with analyses into our new, real-world dataset of human uncertainty: \texttt{CUB-S}.  

% % We investigate these questions across a \textit{spectrum of degrees and forms of uncertainty}. First, we study controlled, simulated uncertainty in our newly proposed variant of MNIST: \texttt{UMNIST}, \kccomment{\textit{as well as simulated over the popular concept datasets \texttt{CUB} \citep{WahCUB_200_2011} and \texttt{CheXpert} \citep{irvin2019chexpert}.}} We then increase the degree of realism, moving to the in-the-wild coarse-grained uncertainty scores collected in CUB. We then close with analyses into our new, real-world dataset of human uncertainty: \texttt{CUB-S}.  

% For each dataset, we study test-time performance of models trained on only determinstic concepts, but faced with uncertainty at test-time -- as well as how training with uncertainty impacts handling test-time uncertainty. 

\section{Simulated Uncertainty}%Controlled, Simulated Uncertainty Reveals Blindspots} 

We first investigate the performance of concept-based models on simulated uncertainty.

% our controlled dataset of uncertain concepts, \texttt{UMNIST}, and CheXpert. Here, uncertainty is applied uniformly over concepts.

\subsection{Experimental Set-Up}

\subsubsection{Data} We consider two datasets with varying degrees of simulated uncertainty: \texttt{CheXpert} and a newly constructed, controllable dataset of uncertainty, \texttt{UMNIST}. \texttt{CheXpert} is a visual dataset containing chest X-rays that are annotated with a set of 14 concepts. In our scenario, we aim to predict the ``No Finding'' concept based on the other 13 concepts. We incorporate simulated uncertainty by each concept's label by setting uncertain values to 0.5 and unknown values to 0 (\texttt{CheXpert} comes with annotations indicating which concepts are uncertain/unknown). \texttt{UMNIST}'s samples are formed by a mixture of MNIST digits (e.g., zeros or ones), where the task is to compute the sum of all digits and each sample is given the number represented by each digit as a concept annotation (see Supplement for more details). \texttt{UMNIST} is parameterized with parameter $\delta \in [0, 1]$ which controls the amount of uncertainty/noise in each sample's concept annotations. Intuitively, $\delta =0$ represents fully certain concept labels and no mixing of each sample's digits while $\delta = 1$ represents a dataset with random concept annotations. Uncertainty in the \texttt{UMNIST} dataset, therefore, is applied uniformly over concepts, at varying levels. We apply the same $\delta$-smoothing to \texttt{CheXpert}.

% In this work, we consider two policies to select the concept to intervene on: 1) random selection of concepts, and 2) an approximate oracle policy  -- ``Skyline'' -- following \citeauthor{chauhan2022interactive}, which selects the next concept to query that will best impact performance (as if it were possible to know, simulating an upper bound on intervention efficacy; see Supplement for further details). While other works have been developed with more clever policies \citep{chauhan2022interactive, sheth2022learning, shin2023closer} --- we select Random and Skyline as they illustrate the \textit{bounds} on achievable performance, Random being the most naive policy and Skyline being the optimal policy. 

\subsubsection{Evaluation} 

We study the performance of the concept-based systems on the task of interest (e.g., abnormality detection in chest X-rays, predicting the sum of digits in an image) as a function of the number of concepts intervened. For both \texttt{CheXpert}, following \citeauthor{chauhan2022interactive}, we evaluate the Area under the ROC curve (\textit{AUC}). For \texttt{UMNIST}, given its multi-class setting, we evaluate accuracy instead. Finally, as we are interested in how uncertain interventions affect concept-based models rather than how to best take into account uncertainty at intervention time, an interesting yet different research question, in our evaluation we randomly choose which concepts to intervene on rather than deploying more principled intervention policies.

\subsection{Intervening with Uncertainty}

\begin{figure}[!htb]
    \centering
    \includegraphics[width=1.0\linewidth]{figures/cem_intervention_omega_chexpert.pdf}
        \caption{Effect of simulated uncertainty in \texttt{CheXpert} on test-time efficacy (AUC) in CME as the number of concepts intervened on increases. Standard error depicted over 3 random seeds.}%In this scenario, the train-time uncertainty value is $1.0$, so matching the test-time and train-time uncertainty maximizes intervention performance.}
    % \caption{We find that, for simulated uncertainty, having a larger test-time uncertainty value improves intervention performance. In this scenario, the train-time uncertainty value is $1.0$, so matching the test-time and train-time uncertainty maximizes intervention performance.}
    \label{fig:chexpert_sim_intervention}
\end{figure}

\begin{figure*}[!htb]
    \centering
    \includegraphics[width=0.47\linewidth]{UMNIST/umnist_cem_vs_cbm_comp.pdf}
    \includegraphics[width=0.43\linewidth]{UMNIST/umnist_cem_train_grid.pdf}
    % \caption{\textbf{Left}: Mean test accuracies of random interventions on CBMs and CEMs, together with their standard error computed across 5 different random initializations, as we increase the number of concepts we intervene on. We observe that concept-based systems (CBMs and CEMs) that have not been trained on uncertainty struggle to handle test-time uncertainty, even when both models achieved similarly high concept accuracies of $\sim98\%$.  \textbf{Right}: Heatmap showing the task accuracy (\%) of a CEM trained in \texttt{UMNIST} (with train-time $\delta$ varying across the y-axis) after intervening in 50\% of its concepts with possibly uncertain test-time concept labels (controlled by the test dataset's $\delta$ value in the x-axis). We observe that training with uncertainty in \texttt{UMNIST} improves robustness under distribution shift at intervention time (compare bottom row when test time $\delta \in \{04, 0.6\}$ vs CEMs trained with samples generated with $\delta \in \{ 0.2, 0.4 \}$), provided the training level of uncertainty is not too high. 
    % }
    \caption{\textbf{Left}: Mean test accuracies of random interventions on CBMs and CEMs, together with their standard error computed across 5 different random initializations, as we increase the number of concepts we intervene on. Concept-based systems (CBMs and CEMs) that have not been trained on uncertainty struggle to handle test-time uncertainty, even when both models achieved similarly high concept accuracies.  \textbf{Right}: Heatmap showing the task accuracy (\%) of a CEM trained in \texttt{UMNIST} (with train-time $\delta$ varying across the y-axis) after intervening in 50\% of its concepts with possibly uncertain test-time concept labels (controlled by the test dataset's $\delta$ value in the x-axis). Training with uncertainty in \texttt{UMNIST} improves robustness under distribution shift at intervention time (compare bottom row when test time $\delta \in \{0.4, 0.6\}$ vs CEMs trained with samples generated with $\delta \in \{ 0.2, 0.4 \}$), provided the training level of uncertainty is not too high. 
    }
    \label{fig:umnist_res}
\end{figure*}

We first benchmark how well models \textit{not trained with uncertain concepts} cope with uncertainty at intervention during testing. This setting best captures what a user facing uncertainty may experience when deploying pre-trained concept-based models, which are rarely trained on uncertainty. Specifically, we explore varying the total amount of concept uncertainty in the testing data and observe that, even with low simulated uncertainty, both CEMs and CBMs suffer from significant drops in intervention performance when dealing with uncertain samples (see Figures~\ref{fig:chexpert_sim_intervention}~and~ \ref{fig:umnist_res}; see Supplement). We can see that this drop is particularly sharp as the amount of uncertainty grows, as seen in the performance of CEMs when $\delta = 0.6$. This suggests that these models, although accurate and high performing when receiving fully certain interventions, are unable to generalize to settings in which the intervening user is uncertain of the nature of some of the concepts.
% Nevertheless, we observe that the absolute drop in performance between fully certain interventions and relatively uncertain interventions (e.g., $\delta = 0.4$) is higher in CBMs than in CBMs. For example, when intervening 50\% of all concepts in \texttt{UMNIST} the performance in CEM drops to about $65.03\%$ of its performance with the same amount of certain interventions while in CBM it drops to TODO\% of its certain intervention performance. This robustness brings further evidence that CEM's embeddings may enable a self-correcting mechanism in the model, as observed by~\citet{cem22}, indicating that they may be safer to deploy than traditional CBMs in uncertain setups.


\subsection{Training with Uncertainty Can Improve Robustness} 

While we observe that exposing models not trained with concept uncertainty to uncertain concepts leads to the breakdown of intervention efficacy --- we hypothesize that training with uncertainty can boost the ability to cope. We indeed observe in Figures \ref{fig:umnist_res} (Right) and \ref{fig:chexpert_sim_intervention} that by \textit{training} with uncertainty, we can salvage the efficacy of interventions -- particularly under \textit{distribution shift}. That is, if we train on an uncertainty level that differs from the level expressed by a user, we may be better equipped to handle that user's uncertainty than if we did not train with uncertainty at all. Notably, however, we observe a ``sweet spot'' in the level of uncertainty that is helpful to the model. 

%However, as demonstrated, existing concept datasets do not readily permit study of human uncertainty; as such, we construct a controlled dataset with simulated uncertainty to better analyze our hypothesis. We indeed affirm benefits to training with uncertainty.

\subsection{Implications} Even in controlled settings, existing concept-based systems struggle to adequately handle concept uncertainty at inference time. Training with concept uncertainty proves a reasonable salve for capturing value from the uncertain interventions, particularly affording robustness under distribution shifts. However, our results suggest that training with too much concept ``softness'' can be harmful.  



\section{Real human uncertainty}%Real, Coarse-Grained Uncertainty Poses Challenges}

% \djcomment{It is a bit strange to have this section and not a corresponding one above on "simulated uncertainty", the two should be parallel and structured as similarly as possible, so the reader can go through and constrast}


% While promising towards indicating training with uncertainty as a potential remediation strategy for test-time uncertainty, our \texttt{UMNIST} results are on contrived uncertainty. How do existing systems fare on real-world uncertainty? We first explore this question through the human uncertainty annotations provided in \texttt{CUB} \citep{WahCUB_200_2011}.

We see in our simulations that permitting test-time uncertainty can impact model performance --- and that training with uncertainty offers a potential remediation strategy when handling test-time uncertainty. However, these investigations are on contrived uncertainty, begging to ask how existing systems fare with real-world uncertainty.

\subsection{Taxonomy of Forms of Uncertainty}

Real human uncertainty can come in many forms. This uncertainty may be \textbf{epistemic}, representing lack of knowledge, or \textbf{aleotoric}, due to (potentially) inevitable randomness \citep{hullermeier2021aleatoric}. Further, this uncertainty can either be \textbf{heteroschedastic}, i.e., dependent on the input, or \textbf{homeoschedastic}, independent of the input \citep{books/lib/RasmussenW06}. Thus far, we have focused on \textit{regular} uncertainty -- simulating the same level of uncertainty $\delta$ across all concepts. 

However, in-the-wild uncertainty, elicited from humans, is not so simple. The method by which uncertainty is elicited can have a sizeable impact on the quality of the elicitation \citep{uncertainJudgments, goldstein2014lay, keren1991calibration, expertElicitation}. As researchers may use a variety of elicitation practices, we believe it is \textbf{important to understand how concept-based systems handle different forms of elicited human uncertainty}. %and the design choices made 
%, and how design choices around  to consider when faced with varied uncertainty measures. 

We focus on two flavors of uncertainty \textbf{coarse-grained} (elicited from a few-option discrete scale) and \textbf{fine-grained} (probabilities extracted over each possible attribute in a concept group). In the coarse-grained setting, humans provide both binary concept annotations, $c_i \in \{0, 1\}$, and a discrete measure of confidence $\omega$, e.g., $\omega \in \{$``Guessing'', ``Probably'', ``Definitely''\}. In this setup, we need to construct a map from $c_i \times \omega$ to the probability distribution of interest $p(c_i | x)$. In contrast, in the fine-grained setting, humans directly provide $p(c_i | x)$. 

While we do not consider \textit{all} forms of uncertainty expression, e.g., humans may prefer to express uncertainty flexibly through language \citep{DHAMI2022514, zhou2023navigating}, we see our study as a promising first step into a deeper investigation of the impact of different forms of \textit{real human uncertainty} on concept-based system performance. 
%\kccomment{Todo: write out formulation in (simple) prob maths; e.g. $P(c | x)$ over categories compared to just using coarse-grained discrete uncertainty, say $\omega \in \{$Guessing, Prob Sure, Definitely\}} 


% In our Simulated Uncertainty experiments, we explore regular uncertainty -- where the same level was applied over all training examples; here, we consider two different flavors of uncertainty applied less regularly, i.e., at a per-instance or per-class basis: coarse-grained (elicited from a few-option discrete scale) and fine-grained (probabilities extracted over each possible attribute in a concept group). 



\subsection{Coarse-Grained Uncertainty}

We first consider these questions over \textit{coarse-grained} human uncertainty; i.e., a single discrete annotation of whether the user is uncertain. The limited, discrete nature of the uncertainty variable $\omega$ raises important design considerations when considering how to use this uncertainty. For instance, if a user marks that they are uncertain, how can we know \textit{how} uncertain are they? And are they only uncertain over parts or the entirety of the concept space? We next study how design choices to impute these ambiguities at train- and test-time can impact the intervention efficacy. We address these questions through the uncertainty annotations provided in \citep{WahCUB_200_2011}.\footnote{We include analyses over the ``real''  coarse-grained uncertainty annotations in \texttt{CheXpert} in the Supplement. In contrast to \texttt{CUB}, which has uncertainty annotations for each image and attribute, only some concepts are labeled with an uncertainty score in CheXpert. Moreover, the score obfuscates whether the label is deemed uncertain due to human uncertainty versus annotation-scraping uncertainty \citep{irvin2019chexpert}. As such, we place less emphasis on CUB in this work.}.


\subsubsection{Experimental Set-Up}

\paragraph{Data}

CUB is a highly popular benchmark dataset for concept prediction that includes images of birds, annotated with 28 different concept groups (e.g., wing color, beak shape) \citep{WahCUB_200_2011}. Each concept can take on many different values. The task is to predict one of two hundred different bird species. \citeauthor{WahCUB_200_2011} elicited humans' uncertainty when collecting the original annotations; however, these annotations are highly coarse (a simple: ``Guessing,'' ``Probably'', or ``Definitely'' mark over each concept group's annotations). There are 311 total binary concepts that can be extracted from the 28 categorical concepts; we follow the common practice proposed in \citeauthor{koh2020concept} by filtering these down to 112 concepts. We study how intervening with, and learning with, these coarse-grained annotations impacts performance. 

% Additionally, we address the lack of richness in the original \texttt{CUB} dataset by collecting a new relabeling of CUB: which we call CUB-S (S for Soft Labels).  

\paragraph{Evaluation}

We follow similar evaluation protocols to our Simulated Uncertainty experiments, focusing on the measure of task accuracy (where the task is bird species classification). We include Skyline interventions to help demonstrate the best possible intervention policy that can be achieved to further highlight the impact of the types of uncertainty on performance. 

% We find similar patterns hold in another popular concept dataset with coarse-grained uncertainty annotations, \texttt{CheXpert} \citep{irvin2019chexpert} (see Supplement). 

% Through these limitations, however, the trends of \texttt{UMNIST} analyses persists: uncertainty at test-time \textit{matters} (and can break exisiting systems) and while training with soft concepts may provide value, the strength of the effect is weaker -- suggesting promising avenues for new methodological advances to better mitigate test-time user uncertainty. We find similar patterns hold in another popular concept dataset with coarse-grained uncertainty annotations, \texttt{CheXpert} \citep{irvin2019chexpert} (see Supplement). 

% We first explore these questions through the human uncertainty annotations provided in \texttt{CUB} \citep{WahCUB_200_2011}. However, as mentioned in Section [...], \texttt{CUB} encompasses only \textit{coarse-grained} uncertainty. The limited, discrete nature of the uncertainty raises important design considerations when considering how to use this uncertainty. 

% The collection of discrete uncertainty scores is popular [cite]. 

%We begin to explore this question in previously released, popular concept datasets annotated with uncertainty: \texttt{CUB} \citep{WahCUB_200_2011} and \texttt{CheXpert} [cite]. 

%While we take a primarily negative critique on the uncertainty in these datasets, we emphasize that the datasets remain highly valuable for concept-based studies; we see our analyses as an extension of the valuable critiques offered by \citeauthor{ramaswamy2022overlooked} to characterize the limitations as they relate to downstream use patterns of real humans with such models.

% Through these limitations, however, the trends of \texttt{UMNIST} analyses persists: uncertainty at test-time \textit{matters} (and can break exisiting systems) and while training with soft concepts may provide value, the strength of the effect is weaker -- suggesting promising avenues for new methodological advances to better mitigate test-time user uncertainty. We find similar patterns hold in another popular concept dataset with coarse-grained uncertainty annotations, \texttt{CheXpert} \citep{irvin2019chexpert} (see Supplement). 

\subsubsection{How to Use Discrete Uncertainty Scores?} 

The first question raised with the real-world uncertainty of the form elicited in \texttt{CUB} is how to leverage the scores at intervention time. Uncertain annotations are only provided in the form of a single, discrete measure of uncertainty: \texttt{CUB} annotators provided \textit{coarse-grained}, discretized approximations of their confidence in said annotations (i.e., specifying whether they were Guessing, Probably Sure, or Definitely Sure in their annotations). 

However, concept-based systems typically necessitate interventions to be specified in continuous space; as such, we need to define a custom mapping from discretely expressed uncertainty to continuous values. The choice of such a mapping impacts downstream performance. Second, for categorical concepts like those in \texttt{CUB}, a single measure of uncertainty does not permit a nuanced assignment of uncertainty to individual concepts. There is ambiguity around what the human user intended to express; i.e., if the user says they are ``Probably'' we do not know over which concepts and \textit{how} unsure they are.

We highlight the ramification of this ambiguity in two ways. First, we demonstrate that imputing the coarse-grained uncertainty with different continuous values can -- at times dramatically -- impact performance. Second, we demonstrate that the degree of softness assumed when leveraging uncertainty over \textit{categorical} concept spaces matters.   

\paragraph{Imputing the ``Probably'' Probability}

\begin{figure}[!htb]
    \centering
    \includegraphics[width=0.9\linewidth]{figures/vary_probs_grid.pdf}
    \caption{ Impact of different levels of uncertainty on intervention efficacy (task accuracy) in CEMs as the number of concepts intervened on increases, across both Random and Skyline policies. Colors correspond to the different imputations of the probability someone intends when they say they are ``Probably'' sure that are used at intervention-time. Mean performance when intervening over all test set examples in \texttt{CUB}.}
    \label{fig:vary_prob}
\end{figure}

We focus on the concept annotations where humans expressed they were ``Probably'' sure of the annotations. Here, we do not know \textit{how} certain the annotators were in their labeling. We vary the level of uncertainty we assume annotators were in such annotations when intervening and apply the same imputed probably to the ``on'' (e.g., blue wing present) and ``off'' concepts (e.g., wing color is not yellow); for the latter, we flip the assigned probability. We observe in Figure \ref{fig:vary_prob} that the imputed probability can have a dramatic impact. The imputation matters -- demonstrating both limitations of insufficient richness in annotation (we do not know what the original annotators intended) and further brittleness of these systems to test-time uncertainty when they have been trained exclusively on deterministic concepts.

\paragraph{Distribution of Uncertainty over Categorical Concepts} 

\begin{figure}[!htb]
    \centering
    \includegraphics[width=0.9\linewidth]{figures/redist_diff_skyline.pdf}
    \caption{Impact on task accuracy of different ways of distributing the discrete uncertainty over categorical concept groups selected using Skyline.}
    
    % The method of distributing discrete uncertainty over categorical concept groups matters. The upper bound on intervention effectiveness (Skyline) is heavily impacted whether we smooth all concepts (broad uncertainty) or just those the annotator said are present (narrow).}
    % \caption{The method of distributing discrete uncertainty over categorical concept groups matters. The upper bound on intervention effectiveness (Skyline) is heavily impacted whether we smooth all concepts (broad uncertainty) or just those the annotator said are present (narrow).}
    \label{fig:skyline_smooth_off}
\end{figure}
% \begin{figure}[!htb]
%     \centering
%     \includegraphics[width=0.9\linewidth]{figures/redist_diff_coop.png}
%     \caption{For categorical concept groups, the method of distributing uncertainty through discrete confidence scores matters. Here, we demonstrate that CooP's effectiveness is substantially impacted by whether we apply the discrete uncertainty scores to smooth the ``off'' concepts or not -- the application of which is unclear from the provided annotations. \kccomment{Replace with random intervention (similar trends to CooP).}}
%     \label{fig:coop_smooth_off}
% \end{figure}

Not only does insufficient richness in uncertain annotations pose a challenge when determining what level of certainty to assign: it is also ambiguous \textit{which} concepts the annotator was uncertain in when they said they were ``probably'' sure. We refer to this phenomenon as whether the annotator's uncertainty is \textbf{broad} (over all possible concept values) or \textbf{narrow} (just over a few of the possible concept values). For instance, when annotating beak shape, the annotator may be very certain the shape is not rounded -- but unsure whether to classify the shape as dagger-like or pointed: ``narrow'' uncertainty. In that case, the intervention on rounded should be left fully ``off'' (i.e., 0\%), but the mass should be spread on the possible ``on'' values (perhaps 70\% dagger, 30\% pointed). We demonstrate in Figures \ref{fig:skyline_smooth_off} and in the Supplement that these choices also matter. Assuming that an annotator's uncertainty is broad, and only over aspects of the concept space, can substantially impair intervention quality (likely because the converse was oversmoothing -- i.e., falsely miscalibrating to be underconfident). The sensitivity of the models and policies to these varied degrees of uncertainty highlights the brittleness of systems to such design choices and possible spectra of human uncertainty expression. 

\subsubsection{Instance- vs. Population-Level Uncertainty?}
% CUB-Specific Challenges}


Another question raised by in-the-wild human uncertainty is how to handle individual differences versus group-level uncertainty \citep{peterson2019human, selfCiteSoftLabel}. This question is particularly pertinent in \texttt{CUB}, as the annotations are both sparse and noisy. Several concepts have few annotations, and many annotations may be low-quality. As such, it may make sense to consider \textit{population-level} uncertainty rather than individual uncertainty. Here, we refer to population-level uncertainty as the class-level labels used by \citeauthor{koh2020concept}. We form soft labels by aggregating all annotators' individual-level soft labels for a given category. To ``upper bound'' the differences in population vs. individual-level uncertainty, we consider the possibility that annotators are unsure over \textit{both} ``on'' and ``off'' annotations (i.e., that they possess broad uncertainty). We see that whether or not to intervene with population-level uncertainty matters --- test-time performance is markedly higher when using population-level labels (see Supplement). 

\subsubsection{Training with Uncertainty}

% help from: https://tex.stackexchange.com/questions/10245/how-to-order-3-images-horizontally
\begin{figure*}[!htb]
\minipage{0.32\textwidth}
  \includegraphics[width=\linewidth]{figures/inst_unc_train_cub_cem.pdf}
  \subcaption{Training on instance-level (broad) uncertain concept annotations.}\label{fig:smooth_train_inst_test}
\endminipage\hfill
\minipage{0.32\textwidth}
  \includegraphics[width=\linewidth]{figures/nosmooth_unc_train_cub_cem.pdf}
  \subcaption{Training on instance-level (narrow) uncertain concept annotations.}\label{fig:nosmooth_train_inst_test}
\endminipage\hfill
\minipage{0.32\textwidth}%
  \includegraphics[width=\linewidth]{figures/agg_unc_train_cub_cem.pdf}
  \subcaption{Training on population-level (broad) uncertain concept annotations.}\label{fig:agg_train_inst_test}
\endminipage
\caption{Training on uncertain concept labels improves generalization to instance-level (broad) uncertainty at test-time -- the most challenging of the in-the-wild coarse-grained varieties. Heatmap colors depict generalization efficacy operationalized as the AUC between the intervention-accuracy curve. Uncertainty here is expressed by varying the imputed ``Probably'' probability at train and test time; decreasing probability (e.g., left-to-right on the x-axis) corresponds to increasing uncertainty.}
\label{fig:compare_training_labels_cub}
\end{figure*}

Likewise, the question of the form of uncertainty and whether or not to leverage aggregate uncertainty matters at train time. Training on aggregated uncertainty not only performance on similarly population-level uncertainty (see Supplement), but also over softer, potentially noisier individual-level uncertainty -- across gradations of uncertainty (see Figure \ref{fig:compare_training_labels_cub}). Further, whether uncertainty is assumed to be broad or narrow at an individual-level can also impact training label efficacy (see Left and Middle panels of Figure \ref{fig:compare_training_labels_cub}). 

% Training on aggregated uncertainty can improve performance on aggregate \textit{and} individual-level uncertainty the Supplement, across various levels of smoothing, at test time. However, we interestingly find that training over the individual \texttt{CUB} annotators' uncertainty separately can hamper test-time performance. We reason that this is due to the conflation of annotator noise and too much uncertainty (moving the softness into the deleterious regime uncovered in our simulated uncertainty experiments). 

% Likewise, the question of the form of uncertainty and whether or not to leverage aggregate uncertainty matters at train-time (Figure \ref{fig:cub_train_uncertainty}). Training on aggregated uncertainty can improve performance on aggregate \textit{and} individual-level uncertainty, across various levels of smoothing, at test-time. However, we interestingly find that training over the individual \texttt{CUB} annotators' uncertainty separately can hamper test-time performance. We reason that this is due to the conflation of annotator noise and too much uncertainty (moving the softness into the deletrious regime uncovered in our \texttt{UMNIST} simulations). 



% Interestingly, we see that training with uncertainty assumed over both the ``on'' and ``off'' values does \textit{not} improve test-time robustness; however, limiting smoothing to only ``on'' values does (see Figure ....). We hypothesize that the former is pushing the level of uncertainty into the domain beyond the ``sweet spot'' as identified in \texttt{UMNIST}. 




% \subsubsection{To Train or Not to Train with Uncertainty}

% % \kccomment{This may make sense to flip with next section, and repitch population-level uncertainty.}

% Seeing as the form of uncertainty at test-time matters, we next explore whether training with uncertainty can similarly improve test-time robustness as in \texttt{UMNIST}. We study training with uncertainty with varied levels of imputed uncertainty over discrete scores. Interestingly, we see that training with uncertainty assumed over both the ``on'' and ``off'' values does \textit{not} improve test-time robustness; however, limiting smoothing to only ``on'' values does (see Figure ....). We hypothesize that the former is pushing the level of uncertainty into the domain beyond the ``sweet spot'' as identified in \texttt{UMNIST}. 

% \begin{figure}[!htb]
%     \centering
%     \includegraphics[width=0.9\linewidth]{figures/diff_auc.png}
%     \caption{It matters whether or not we use \textit{individ} annotator uncertainty, or average over many individuals' uncertainty. Averaging improves stability of intervention; but in practice, we may only have a single individ who can provide their uncertainty. We find sizeable differences in the intervention efficacy when using averaged uncertainty for both Skyline and CooP. Complete intervention curves can be seen in Figure \ref{fig:complete_avgd_unc}}. %Will add CBMs. Note: this particular observation may be original-CUB-dependent to some degree; original \texttt{CUB} is very noisy [cite].}}
%     \label{fig:avgd_unc}
% \end{figure}

% However, we highlight a further limitation of \texttt{CUB} being the lack of \textit{individual-level} attributions of annotations. Without knowing which (anonymized) individual contributed which label, we are unable to study whether some annotators are simply miscalibrated, or spamming. 


\subsubsection{Implications}

While we focus here on \texttt{CUB} -- as the dataset is a highly popular concept benchmark, and therefore necessary to deeply understand -- the elicitation of discrete uncertainty is lightweight and popular in crowdsourcing \citep{uncertainJudgments} (see further investigations with \texttt{CheXpert} in the Supplement); as such, our investigations may be broadly applicable to researchers leveraging elicited discrete uncertainty. The wide impact design choices can have serves as a caution -- if we want safe systems which are robust, we ought to be able to handle the array of intended meaning expressed by humans through discrete uncertainty. Decisions around how to treat discrete uncertainty over concepts persist across train and test time. % our results highlight particular considerations necessary of CUB. (?)

% These results all highlight that concept-based models \textit{are} sensitive to test-time uncertainty in human uncertainty. And the form of this uncertainty matters; we need better datasets for studying human uncertainty.

% \subsection{Rich Human Uncertainty in CUB-S}

% Seeing the challenges with existing uncertainty datasets -- we next turn to our new uncertainty-annotated dataset, CUB-S. 

% \section{Eliciting Human Uncertainty} 

% Seeing as 1) existing concept datasets are not annotated richly with human uncertainty, 2) permitting human uncertainty at intervention-time can lead existing systems to go astray, and 3) training with concept uncertainty can boost test-time robustness --- we argue that there is immense value in \textit{collecting new concept datasets} enriched with human uncertainty.

% To facilitate this research, we build a new platform for uncertainty elicitation over concepts, which we call \texttt{UElic}. 

% % [...details on interface...]

% \subsection{Uncertainty Elicitation Mechanism Design} Our interface, \texttt{UElic}, offers a lightweight paradigm for users to express uncertainty. Users are presented with the features of interest (e.g., an image), the concept to be annotated, and all available options. To reduce the cognitive load of expressing uncertainty over \textit{all} options per concept, we request users select only the attributes they think are plausible and express their uncertainty over these attributes by dragging an interactive bar chart to represent their perceived probability. An example interface screen is depicted in Figure \ref{fig:interface-unc}. 

% \begin{figure}[!htb]
%     \centering
%     \includegraphics[width=0.9\linewidth]{figures/elic_interface_unc.png}
%     \caption{Example screen of \texttt{UElic} for CUB. Participants select the concept attributes they think are plausible, and drag bars to express said uncertainty. Here, the back is not visible; users must be uncertain in their annotation. We empower annotators to \textit{richly} express this belief distribution, in contrast to the original \texttt{CUB} dataset.}
%     \label{fig:interface-unc}
% \end{figure}


\subsection{Fine-Grained Uncertainty}%Richer, In-the-Wild Uncertainty}%Rich, Real-World Human Uncertainty through CUB-S}

Next, we turn to more fine-grained uncertainty. When faced with many options (e.g., multiple different possible colors for the wing, or different gradations of severity in a medical phenotype), a human may have different levels of uncertainty over each option. We now consider this form of categorical uncertainty \textit{explicitly}, rather than inferring from an ambiguous single measure of ``uncertainty.''

However, there is a paucity of datasets available with such richly annotated labelings over concept space. As such, to facilitate this research, \textbf{we build a new platform for uncertainty elicitation over concepts}, which we call \texttt{UElic} and offer a first application of \texttt{UElic} to relabel a subset of \texttt{CUB} with human soft labels over all concepts. \textbf{We release our dataset as \texttt{CUB-S}, replete with nearly 5,000 rich uncertainty-labeled concept groups}. %(S for soft labels). 

In this Section, we begin by introducing our new elicitation interface for rich human uncertainty, and offer several insights into the character of human uncertainty we elicit. We then highlight how concept-based systems crumble under the nuances of the fine-grained uncertainty we elicit. We believe \texttt{CUB-S} can serve as a formative dataset to further study human uncertainty in concept-based models. 


% When posed with multiple possible options, a user may have differential uncertainty over these options. 

% Seeing as 1) existing concept datasets are not annotated richly with human uncertainty, 2) permitting human uncertainty at intervention-time can lead existing systems to go astray, and 3) training with concept uncertainty may boost test-time robustness --- we argue that there is immense value in \textit{collecting new concept datasets} enriched with human uncertainty.

% To facilitate this research, we build a new platform for uncertainty elicitation over concepts, which we call \texttt{UElic} and offer a first application of \texttt{UElic} to relabel a subset of \texttt{CUB} with human soft labels over all concepts. We release our dataset as CUB-S (S for soft labels). 

% We believe CUB-S can serve as a formative dataset to further study human uncertainty in concept-based models. CUB-S is replete with nuances of handling real human uncertainty. In this Section, we offer several insights into the character of human uncertainty we elicit. We then highlight how state-of-the-art algorithms for working with concept models, e.g., adaptive intervention policies like CooP, struggle to handle real human uncertainty -- offering exciting grounds for future methodological advances. 

% \subsubsection{Experimental Set-Up}

% We follow the same experimental set-up as in our \texttt{CUB} coarse-grained experiments, however, we focus our test-time study onto our rich relabeling of CUB: CUB-S. 

\subsubsection{Eliciting Human Uncertainty} 

We offer a new platform to streamline the elicitation of human uncertainty. Our interface, \texttt{UElic}, offers a lightweight paradigm for users to express uncertainty. Users are presented with the features of interest (e.g., an image), the concept to be annotated, and all available options. To reduce the cognitive load of expressing uncertainty over \textit{all} options per concept, we request users select only the attributes they think are plausible and express their uncertainty over these attributes by dragging an interactive bar chart to represent their perceived probability, inspired by \citep{goldstein2014lay}. An example interface screen is depicted in Figure \ref{fig:interface-unc}. 

\begin{figure}[!htb]
    \centering
    \includegraphics[width=0.9\linewidth]{figures/elic_interface_unc.png}
    \caption{Example screen of \texttt{UElic} for \texttt{CUB}. Participants select the concept attributes they think are plausible, and drag bars to express said uncertainty. Here, the back is not visible; users must be uncertain in their annotation. We empower annotators to \textit{richly} express this belief distribution, in contrast to the original \texttt{CUB} dataset.}
    \label{fig:interface-unc}
\end{figure}

% \subsection{Collecting CUB-S} We recruit ..... through an Ethics Reviewed study. ...  Further details can be found in the Appendix. 

% \subsubsection{How CUB-S Addresses Limitations in CUB} We raised several issues around the uncertainty annotations in \texttt{CUB} in Section [...]. We detail how CUB-S addresses each of those limitations. 

% \begin{enumerate}
%     \item \textit{Missing richness:}
%     \item \textit{Studying individual annotator certainty:} Unlike \texttt{CUB}, we release (anonymized) individual annotator IDs, permitting the study of individual-level adjustments.
%     \item \textit{Reliability:}
% \end{enumerate}
\subsubsection{Collecting CUB-S} We recruit 89 participants from the crowdsourcing platform, Prolific \cite{palan2018prolific}. Participants annotate \textit{all 28 concepts} for two different bird images: totalling \textbf{4984 soft categorical concept group annotations}. Within each soft concept group annotation, participants provide their uncertainty over each of the possible attributes for that concept (e.g., possible wing colors, beak shapes, etc). Stimuli are selected from the \texttt{CUB} test set, and preferentially subsampled to include images which CEMs and CBMs both typically get wrong\footnote{Approximately 50\% of the images shown to participants are those which four different seeds of both CEMs and CBMs got incorrect, rendering them more interesting - and challenging - to study at intervention-time}. Participants are paid at a base rate of \$9/hr, with an optional bonus paid up to \$10/hr to encourage quality predictions; the bonus is applied to all participants.

\subsubsection{Richness in CUB-S} 
\label{sec:cubs_richness}

Our elicited soft labels demonstrate that humans indeed can starkly depart from a uniform distribution of uncertainty over concepts (see Figure \ref{fig:cubs_concept_examples}). Humans possess rich approximations of uncertainty. Eliciting this uncertainty directly can resolve some of the ambiguities mentioned with discrete uncertainty. 

\begin{figure*}[!htb]
    \centering
    \includegraphics[width=1.0\linewidth]{CUB-S-figures/examples_annotations.png}
    \caption{Example soft concept annotations elicited in CUB-S compared to \texttt{CUB} class labels. Far left: well-calibrated annotations for the "tail shape" concept, expressing appropriate uncertainties which sum to 100. Center left: annotations rarely included the obscure "buff" color, even when it was appropriate. Center right: richer annotations for the ``upperparts color'' provide more information than the certain \texttt{CUB} annotations. Far right: uncalibrated uncertainty of the ``wing shape'' concept under occlusion.}
    \label{fig:cubs_concept_examples}
\end{figure*}

Further, by tracking which concepts were annotated by particular individuals (information which is not stored in the original \texttt{CUB} annotations), we identify a wide spectrum in the calibration of annotators. This is not entirely unexpected, given different levels of uncertainty calibration in humans broadly \citep{KLAYMAN1999216, keren1987facing}. We use the Expected Calibration Error (ECE) \cite{naeini2015obtaining} as a metric to evaluate the accuracy of annotators when estimating their confidence. Intuitively, the metric is the expected absolute difference between the fraction of correct predictions (accuracy), and the probabilities provided by the annotators (confidence). The ``correct'' concepts for a given bird are determined from the original \texttt{CUB} annotations averaged over all birds of the same species. These ``correct'' concepts are a suitable approximation to ground truth, and are significantly less noisy than the \texttt{CUB-S} annotations; however, we emphasize that they are \textit{not definitive ground truth}. 

\begin{figure}
    \centering
    \includegraphics[width=0.9\linewidth]{CUB-S-figures/ECE_annotators.png}
    \caption{Distribution of Expected Calibration Error for annotators in \texttt{CUB-S}. The positive skew shows most annotators are well-calibrated, with a few who are very poorly calibrated.}
    \label{fig:cubs_miscalib}
\end{figure}

Figure \ref{fig:cubs_miscalib} shows that the majority of annotators are reasonably calibrated, although this value is positively skewed by the large number of (correct) zero probabilities provided for rare concepts (such as the color ``purple''). There are some annotators who are poorly calibrated and it mitigating this issue remains an open question. Some calibration ``error'' is a result of the additional richness in the CUB-S annotations not present in CUB. However, there are also genuine annotation errors which we observe when manually checking the annotations. Illustrative examples comparing soft CUB-S annotations to hard \texttt{CUB} annotations are shown in Figure \ref{fig:cubs_concept_examples}. Humans who intervene at test time will also suffer from calibration errors, challenging the common assumption that human experts are perfect ``oracles''.

\begin{figure}
    \centering
    \includegraphics[width=0.9\linewidth]{CUB-S-figures/calibration_curve.png}
    \caption{Calibration curve for CUB-S annotators, showing consistent underestimates of small probabilities and overestimates of large probabilities.}
    \label{fig:cubs_calibration_curve}
\end{figure}

On average, we observe that annotators consistently underestimate small probabilities but overestimate large probabilities (Figure \ref{fig:cubs_calibration_curve}). When several concepts are possible, it is likely that annotators attempt to reduce their cognitive load by only selecting a few to have a nonzero probability. Conversely, when a concept is highly probable, annotators may incorrectly round an annotation to 100 (i.e. absolute certainty). Figure \ref{fig:cubs_annotation_hist} shows that 0 and 100 are the most popular uncertain annotation values, due to the presence of these two effects. We emphasize that some errors are predictable, and thus have the potential to be corrected when training an uncertainty-aware model.

It is unclear whether the poor calibration is a result of our interface, or simply an unavoidable issue when eliciting uncertainties in a crowdsourcing setting; humans have limited cognitive resources at any given time -- they may not be willing to endorse several related concepts (e.g., orange and red), while providing detailed uncertainty over each. However, the fact that we \textit{do} encounter such challenges is an important consideration in the deployment of systems in which \textit{receive} such uncertainty estimates. It is essential that \textbf{systems be robust to these nuances and peculiarities in elicited human uncertainty, or else they may fail at deployment time}. 

 %Humans are likely using flawed heuristics when annotating rich concepts, limiting the information we can extract for each concept.

\subsubsection{Intervening at Test-Time with \texttt{CUB-S}} 

We next apply the same computational investigations as in our prior experiments to \texttt{CUB-S}; the same Experimental Set-Up is applied, now, only varying the labels used at test time. We use models trained on population-level broad uncertainty derived from coarse-grained \texttt{CUB} as in the prior section. %-laden training labels are derived from broad-uncertainty population-level labels.

We find in Figure \ref{fig:cubs_policies} that the richness of \texttt{CUB-S} poses a substantial challenge for concept-based models. While we find that using models trained on the coarse-grained uncertainty in \texttt{CUB} can mitigate some of the failures under test-time uncertainty, they are not a perfect salve.

The development of better mitigation strategies to handle the nuances of in-the-wild categorical uncertainty over concepts is exciting ground for future work. We observe that some concepts are preferable to elicit interventions over; sometimes human uncertainty is helpful, other times it \textit{harms} model performance (see Figure \ref{fig:model_v_human_uncs}). Further, these differences persist across methods of training the models (i.e., the level of uncertainty in the training data, see Supplement), underscoring the need for adaptive, query procedures personalized to individual- and model uncertainty. We argue multi-disciplinary methodological advances to handle in-the-wild, rich human uncertainty over concept annotations are essential. 

%Not only do random interventions fail to find a good selection of concepts --- even under an ``approximate'' oracle. 


% How well do state-of-the-art policies for selecting which concept to query humans perform when we assume the interventions returned by human users are those of CUB-S? We compare the approximate oracle policy from \citeauthor{chauhan2022interactive} (Skyline) against their adaptive policy, CooP. We find that on CUB-S, CooP is no better than a random intervention scheme. 

% What underlies this failure? Inspecting the CooP scores reveals that high CooP scores (those which are preferentially selected early) are frequently also the concepts which humans struggle to annotate with calibrated certainty (see Figure \ref{fig:coop_failure_analyses}). Skyline preferentially selects concepts which are valuable to intervene on the model (medium entropy), yet which humans valuably annototate; In contrast, we observe that the top Skyline selections tend to be those where the model has moderately high entropy, but the human is fairly confident [confirm]. Here, we see the consequences of breaking the human-certain-oracle assumption typical of many concept-based model works. We see adaptive policies which can better anticipate where humans may be poor annotators (e.g., uncertain and/or miscalibrated) as promising grounds for future work. 

\begin{figure}[!htb]
    \centering
    \includegraphics[width=0.9\linewidth]{figures/cubs_compare_revs.pdf}
    \caption{CEMs struggle to handle real, human uncertainty. While Skyline is still able to pick up \textit{some} signal to leverage in the data, not all incorporated concept annotations help model performance: some may hurt. Using models trained on human uncertainty information may mitigate some of the drop.}
    %-- but size of effect, if any, is small further highlighting the need for multi-discplinary methodological advances to handle in-the-wild, rich human uncertainty over concept annotations.}
    \label{fig:cubs_policies}
\end{figure}


\begin{figure}[!htb]
    \centering
    \includegraphics[width=0.9\linewidth]{figures/skyline_good_bad_selections.png}
    \caption{Model versus human distributions over concepts at the time of selection by Skyline. The first column of distributions are selections which boosted the model's classification (from incorrect to correct); humans' uncertainty was helpful to intervene with. The second column of distributions depicts the human uncertainty at intervention time which \textit{hurt} model performance (the classification went from correct to incorrect). Model trained on uncertain concepts (``Probably'' probability = 0.7).}
    \label{fig:model_v_human_uncs}
\end{figure}


% \begin{figure}[!htb]
%     \centering
%     \includegraphics[width=0.9\linewidth]{figures/unc_diffs_cubs_coop.png}
%     \caption{Humans are not always good annotators of high CooP-scored concepts, rendering the queries less effective. Left: CooP scores for the first three selected concepts may be those that humans are uncertain in; here, depicting the maximum probability assigned by the human over the selected concept group. Right: Skyline preferentially selects concepts which humans have lower overall entropy in than CooP in the first three selections. \kccomment{To replace with a figure showing some examples of concepts that Skyline selected which led to good performance -- to highlight opportunities for policy design (indep of CooP).}}
%     %High CooP scores are related to cases of high human uncertainty (left) and miscalibrated labelings (right), rendering the interventions ineffective, and actually harmful. \kccomment{To run on full CUB-S and update formatting}}
%     \label{fig:coop_failure_analyses}
% \end{figure}

% Cognitive burden of selecting diff vals in our checkboxes (click \textit{both} blue and purple? or just one b/c of cog load); could lead to false ``zero-prob''. Impact of false zero prob on interventions; to show: reason we get the breakdown with color.

% \subsubsection{Training with Uncertainty Can Help}

% We demonstrate that training on the coarse-grain adjusted \texttt{CUB} uncertainty \textit{can} salvage some of test-time performance; however, a core gap still persists between CooP and Skyline: exciting grounds for future work. 

% \subsubsection{Limitations}

% While we find \texttt{CUB-S} ...., we emphasize that several limitations in our data collection and present analyses are worth noting. To add: cognitive burden of selecting multiple of the same concepts -- leads to ``false zero'' probability? \kccomment{This subsection is in progress; thinking of adding a limitations here, but perhaps best broadly (before / after open challenges?)}

\subsubsection{Implications}

Humans interpret and reason in the world with richly structured uncertainty. Our CUB-S elicitation demonstrates this richness. However, we find that concept-based systems struggle to handle this level of richness. Given humans \textit{are capable and do} express fine-grained uncertainty, it is sensible that our systems ought to be equipped to handle the nuances of in-the-wild uncertainty.  


\section{Open Challenges}

We emphasize the importance of considering human uncertainty in concept-based models, and the need for richer datasets of human uncertainty to study these challenges. \texttt{CUB-S} is a promising initial playground to study the nuances faced with real human uncertainty\footnote{All code and data will be hosted at our \href{https://github.com/collinskatie/uncertainty-concepts}{repository}.}. Our work raises several open challenges.

% \subsection{Closing the Gap to Skyline: Better Adaptive Query Policies under Human Uncertainty}

% \subsection{Human-Machine Teaming, Complementarity, and Better Query Policies}
\subsection{Complementarity of Human and Machine Uncertainty}%Designing for Human-Machine Teaming?}

Considering human uncertainty in interventions opens up exciting opportunities in the study of human-machine complementarity \cite{branson20questions, wilder2020learning, bondi2022role, steyvers2022bayesian}. 
When we break the assumption that humans are confident oracles, it becomes especially important to consider whether cases which are hard for the model to annotate are also those that a human struggles with; in that case, selecting such a concept is not ideal. Learning models and intervention policies which can complement humans' strengths and weaknesses, accounting for their expertise and confidence, are promising grounds for further study with \texttt{CUB-S} and beyond. We see that varied models may prefer different forms of uncertainty (see Supplement); further, even though we see that Random interventions fail disastrously, there is a signal that Skyline picks up on the uncertain concepts -- how can we predict where and when to ask people for their uncertainty? And when we do receive their uncertainty, it is not immediately apparent whether we \textit{should} take the human intervention as ``truth.'' As we demonstrate, real humans are \textit{not} consistent oracles -- and in some settings (e.g., occlusion), \textit{no} human may be an oracle, even if they are an expert. Models which can learn whether or not to trust human interventions, e.g. \citep{dvijotham2022enhancing, conceptSidecar}, are promising grounds for future study.

% Considering human uncertainty in interventions opens up exciting opportunities in the study of human-machine complementarity \cite{branson20questions, wilder2020learning, bondi2022role, steyvers2022bayesian}. We find that the concepts preferred by CooP may be those which humans also struggle to annotate; learning models which can complement humans' strengths and weaknesses, accounting for their expertise and confidence, are promising grounds for further study with CUB-S and beyond. We observe a sizeable gap between Skyline and CooP performance; there is signal which Skyline picks up in the uncertain concepts -- how can we predict where and when to ask people for their uncertainty? And when we do receive their uncertainty, it is not immediately apparent whether we \textit{should} take the human intervention as ``truth.'' As we demonstrate, real humans are \textit{not} consistent oracles -- and in some settings (e.g., occlusion), \textit{no} human may be an oracle, even if they are an expert. Models which can learn whether or not to trust the humans intervention, e.g. \citep{dvijotham2022enhancing, conceptSidecar}, are promising grounds for future study.


\subsection{Treating Human (Mis)Calibration}

A core factor in whether or not to trust a \textit{human's} intervention, and determinant of which concepts to query, may depend on the expected calibration of the user. We observe wide variation in individuals' level of calibration in their uncertainty expression, a finding that resonates with a wealth of cognitive science literature \citep{lichtenstein1977calibration, KLAYMAN1999216, keren1991calibration, keren1987facing, kahneman1996reality, sharot2011optimism, uncertainJudgments}. However, we emphasize that calibration need not be a turn-off from collecting uncertainty in the first place; not only are some humans highly calibrated -- but forcing someone to express certainty when they are not (and when it is not possible to be certain; e.g., occlusion), we argue may be worse. Future work for post-hoc calibration in a \textit{few-shot} manner, i.e., from limited individual-level user data, provided in an online fashion, is further promising ground for new methodological advances. Additionally, we encourage further experimentation with \texttt{UElic} to encourage better calibration from humans -- perhaps through the use of a carefully designed teaching curriculum \citep{keren1991calibration, keren1990cognitive}. We see calibration as an exciting nexus for a multi-disciplinary study spanning ML, cognitive science, UX design, and psychology.

% Our study highlights calibration as a prime target for new algorithms; we encourage work to post-hoc calibrate in a few-shot manner, particularly in the face of likely limited data. Likewise, we encourage further experimentation with \texttt{UElic} to encourage better calibration from humans. 


% As we highlight, the degree of miscalibration in elicited uncertainty may be highly sensitive to the elicitation methodology [cogsci cites]. 
% \subsection{Scaling Uncertainty Elicitation}
\subsection{Scaling Uncertainty Elicitation}

Further, we recognize that the annotation of large-scale datasets with human uncertainty may be practically challenging. It is costly to elicit human uncertainty: annotators take substantially more time \citep{selfCiteSoftLabel}. There is a need for more scalable elicitation techniques, and better simulators of human uncertainty to permit the study of softness at train time. We observe substantial differences in model performance depending on the form of uncertainty used; more data is needed to further characterize these differences and determine when one form of uncertainty is better to elicit than another, such that when we deploy systems in the world -- they can handle a variety of forms of uncertainty expression. 

% and close the gap between simulated- and real-world uncertainty so that we can be sure that when we deploy systems in the world -- they can handle a variety of forms of uncertainty expression. 
%simulators ought to match the characteristics of human-uncertainty in which uncertainty is used; expanded datasets are needed to close our observed sim-to-real gap in the study of human uncertainty in ML systems.

% \subsection{Human-Machine Teaming,  Complementarity, and Better Query Policies}

% % Costs to annotate (more time). Leads to less data**. Highlight challenges posed by limited data (need for simulators: but can run into a sim2real gap when it comes to the richness, and irregularities/inconsistencies, of real human uncertainty -- lead into calibration section).  \kccomment{Key: emphasize that, while we see that averaging over individs' uncertainty can help smooth out errors in individs' uncertainty expression; it's unlikely that we'll be able to get enough intervention data to do such averaging at test-time! In real-world, we do have a \textit{single} human interacting with the system to make a decision. See Fig. \ref{fig:avgd_unc}}.


% % New algorithms, which may be able to post-hoc calibrate in a few-shot manner. Alterations to the interface to encourage better calibration from humans [cogsci cite]. Exciting opportunities across fields (ML, CogSci, UX Design, Psych, etc).  

% Additionally, considering human uncertainty in interventions opens up exciting opportunities in the study of human-machine complementarity [cites]. We find that the concepts preferred by CooP may be those which humans also struggle to annotate; learning models which can complement humans' strengths and weaknesses, accounting for their expertise and confidence, are promising grounds for further study with CUB-S and beyond. 

\section{Conclusion}

We highlight the importance of considering human uncertainty in concept-based models in order to improve reliable performance for safe applications in deployment across society. Humans in the real world are not certain oracles. We make mistakes and may be unsure. Even though humans may be miscalibrated in their uncertainty, we believe the study of tools to elicit and work with human uncertainty has great potential to improve human-in-the-loop systems. Through a mixture of simulated and in-the-wild experiments with uncertainty, we demonstrate failure modes of popular concept-based systems to handle both coarse- and fine-grained uncertain feedback. We offer a new interface, \texttt{UElic}, and a new challenge dataset, \texttt{CUB-S}, to support further study into human uncertainty in interventions. Modeling human uncertainty at train- and test-time has the potential to greatly improve the reliability and trustworthiness of concept-based models when deployed safely in the wild.



\section*{Acknowledgments}

We thank Yanzhi Chen, Isaac Reid, Kris Jensen, Richard Turner, Carl Henrik Ek, and Josh Tenenbaum for helpful conversations. We also thank Steve Branson, Catherine Wah, Kushal Chauhan, and Rishabh Tiwari for helpful clarifications on their work. Thank you to the participants from Prolific who took part in our annotation.

KMC gratefully acknowledges support from the Marshall Commission and the Cambridge Trust. MEZ acknowledges support from the Gates Cambridge Trust via the Gates Cambridge Scholarship. NR is supported by a Churchill Scholarship. UB  acknowledges  support  from  DeepMind  and  the  Leverhulme Trust  via  the  Leverhulme  Centre  for  the  Future  of  Intelligence  (CFI),  and  from  the  Mozilla  Foundation.  MJ is supported by the EPSRC grant EP/T019603/1. AW  acknowledges  support  from  a  Turing  AI  Fellowship  under grant  EP/V025279/1,  The  Alan  Turing  Institute,  and  the Leverhulme Trust via CFI. IS is supported by an NSERC fellowship (567554-2022). 














% % \section{Challenges with Uncertainty in Existing Concept Datasets}
% \section{Limitations of Popular Concept Datasets for Studying Human Intervention Uncertainty} 

% Having raised the problem of human uncertainty in concept-based models, we desire quality datasets to study this problem. However, popular concept datasets rarely include uncertainty in human annotations, and if they do (e.g., \texttt{CUB} \citep{WahCUB_200_2011} and \texttt{CheXpert} [cite]), the uncertainty suffers from insufficient richness, consistency, and reliability. 

% In this Section, we analyze the uncertain annotations provided in \texttt{CUB} and \texttt{CheXpert}, highlighting limitations of using such datasets to study the task of human uncertainty. Through the investigations, we identify several overarching challenges researchers ought to take into account when considering human uncertainty in interventions. While we take a primarily negative critique on the uncertainty in these datasets, we emphasize that the datasets remain highly valuable for concept-based studies; we see our analyses as an extension of the valuable critiques offered by \citeauthor{ramaswamy2022overlooked} to characterize the limitations as they relate to downstream use patterns of real humans with such models. 

% % \subsection{Set-up}

% % \subsection{Datasets} There are a limited suite of available datasets annotated with high-level concepts for study with concept-based models, as highlighted by \citeauthor{ramaswamy2022overlooked}. The paucity of datasets is even worse when considering uncertainty; only two datasets, to our awareness, popular in the concept-based literature have human uncertainty annotations: \texttt{CUB} \citep{WahCUB_200_2011} and \texttt{CheXpert} [cite]. [... add sentence on each dataset, with cites on works using them ...] 

% % \subsection{Considerations}

% % We focus on three core considerations 

% \subsection{Insufficient Richness}

% The first core limitation of the uncertainty annotations included in \texttt{CUB} and \texttt{CheXpert} is their lack of richness. Uncertain annotations are only provided in the form of a single, discrete measure of uncertainty: \texttt{CUB} annotators provided \textit{coarse-grained}, discretized approximations of their confidence in said annotations (i.e., specifying whether they were Guessing, Probably Sure, or Definitely Sure in their annotations) [cite] and [....\texttt{CheXpert} details: can't tell whether uncertainty is in extraction process, or from the human; also only a very small proportion of the entire dataset, approx 5\%... Granted, choice of how to use uncertainty still has a non-negligble impact!]. 

% % todo: add histogram of counts of each form of uncertainty. if probably prob is always higher --- demonstrates that more flexibility in elic may be helpful??

% This lack of richness has several practical ramifications. First, existing concept-based models typically necessitate interventions to be specified in continuous space; as such, we need to define a custom mapping from discretely expressed uncertainty to continuous values. The choice of such a mapping impacts downstream performance. Second, for categorical concepts like those in \texttt{CUB}, a single measure of uncertainty does not permit a nuanced assignment of uncertainty to individual concepts. %Third, .... 

% We design computational experiments to affirm these challenges...

% \subsubsection{Set-up}

% \subsubsection{Results}

% \begin{figure}[!htb]
%     \centering
%     \includegraphics[width=0.9\linewidth]{figures/vary_probs_grid.png}
%     \caption{Form of uncertainty matters. If the uncertainty is discrete, we need to decide what continuous value to assign: when someone says they're ``Probably Sure'', what do they really mean? There \textit{is} a difference between 60\% sure versus 90\% sure when intervening. \kccomment{Adding all policies for now. TBD on whether we just discuss Skyline / random here.}}
%     \label{fig:vary_prob}
% \end{figure}

% We demonstrate these practical challenges .... 

% % Yet, discretized uncertainty is a lossy expression; when an annotator says they are ``Probably Sure'' that the wing is blue, we have no way of knowing whether they mean they are 90\% sure, or only 70\% sure. Further, one person's definition of ``Probably'' may be different from another's, and could change depending on context [cites]. As a result, we need to define a custom mapping from discretely expressed uncertainty to continuous values. We construct a set of such mappings, with varying levels of uncertainty, to explore model and policy performance under different levels of uncertainty. We refer to this flavor of \texttt{CUB} as .... and reiterate that these uncertainty are \textit{real} human uncertainty expressions, but the continuous values are not directly elicited from people.

% % First, we note that these datasets each focus on a single, discrete measure of uncertainty. 

% % [..... \texttt{CUB} and \texttt{CheXpert} analyses ..... ]

% % \begin{itemize}
% %     \item Form of uncertainty --- need to make a choice about probably prob, which impacts performance 
% %     \item Cannot study human calibration b/c lack annotator ids. And this is important b/c [cite cogsci works re: calib]. 
% %     \item Group averaging? (CUB-specific) 
% %     \item How to spread extra mass? 
% % \end{itemize}

% \subsection{Inability to Study Individual Annotator Calibration}

% Further, these datasets do not permit the study of individual annotators' level of calibration. Human expression of uncertainty may be miscalibrated... 

% \begin{figure}[!htb]
%     \centering 
%     \includegraphics[width=0.9\linewidth]{figures/chexpert_test_time.png}
%     \caption{Test-time uncertainty values have a large impact on intervention performance. Setting it to $0$ prevents models from differentiating between negative concepts and uncertain concepts, leading to a decrease in performance. However, setting it to non-zero values allows models to pick up on this difference and improve intervention performance.  
%     } 
%     \label{fig:chexpert_test} 
% \end{figure}


% \begin{figure}[!htb]
%     \centering
%     \includegraphics[width=0.9\linewidth]{figures/redist_diff_skyline.png}
%     \caption{For categorical concept groups, the method of distributing uncertainty through discrete confidence scores matters. Here, we demonstrate that Skyline's effectiveness is substantially impacted by whether we apply the discrete uncertainty scores to smooth the ``off'' concepts or not -- the application of which is unclear from the provided annotations. Here, we see that the upper bound on intervention effectiveness is heavily impacted by the way human uncertainty is employed.}
%     \label{fig:redist_cub}
% \end{figure}


% \subsection{Limited Reliability}

% In addition, we highlight a CUB-specific challenge: the data is immensely noisy. As already highlighted in [...cite...], \texttt{CUB} annotations are... These issues persist into uncertainty annotations. 

% ...original paper did group-averaging, highlight that it's not clear how we should do this with uncertainty -- and the choice has a big impact on performance. 

% \begin{figure}[!htb]
%     \centering
%     \includegraphics[width=0.9\linewidth]{figures/diff_auc.png}
%     \caption{It matters whether or not we use \textit{individ} annotator uncertainty, or average over many individuals' uncertainty. Averaging improves stability of intervention; but in practice, we may only have a single individ who can provide their uncertainty. We find sizeable differences in the intervention efficacy when using averaged uncertainty for both Skyline and CooP. Complete intervention curves can be seen in Figure \ref{fig:complete_avgd_unc}}. %Will add CBMs. Note: this particular observation may be original-CUB-dependent to some degree; original \texttt{CUB} is very noisy [cite].}}
%     \label{fig:avgd_unc}
% \end{figure}




% \subsection{Implications}

% These results all highlight that concept-based models \textit{are} sensitive to test-time uncertainty in human uncertainty. And the form of this uncertainty matters; we need better datasets for studying human uncertainty.

% \kccomment{Do we introduce CooP here? Later? Start of this section with models?}


% \section{Benefits of Considering Human Uncertainty} 

% Having highlighted challenges with using existing datasets for studying human uncertainty -- and demonstrating how failing to account for human uncertainty can lead model performance to go awry -- we next consider whether considering human uncertainty at \textit{train-time} can help mitigate some of these consequences. Several recent works [cite] have found that training on labels which capture human uncertainty can boost generalization. 

% We hypothesize that training with uncertainty can boost performance. However, as demonstrated, existing concept datasets do not readily permit study of human uncertainty; as such, we construct a controlled dataset with simulated uncertainty to better analyze our hypothesis. We indeed affirm benefits to training with uncertainty.
% %We then return to \texttt{CUB} and \texttt{CheXpert}, and demonstrate that such findings appear to mitigate some of the challenges found, though more work is needed to fully confirm such findings. 

% \subsection{Set-up} [Mateo to add details on UMNIST]

% \subsection{Results} 

% [Mateo to add grid of train/test uncertainty perf.]

% \begin{figure}[!htb]
%     \centering
%     \includegraphics[width=0.9\linewidth]{figures/umnist_dist_shift.png}
%     \caption{Training with uncertainty in \texttt{UMNIST} improves robustnesss under distribution shift, provided training level of uncertainty is not too high. \kccomment{Details forthcoming}}
%     \label{fig:umnist_dist}
% \end{figure}

% \begin{itemize}
%     \item Training with uncertainty leads to better performance not only when assuming humans intervene with that same level of uncertainty --- but even when there is distribution shift in the level of uncertainty.
%     \item However, too much ``softness'' at train-time may not be helpful. There is a sweet spot. 
% \end{itemize}

% \section{Eliciting Human Uncertainty} 

% Seeing as 1) existing concept datasets are not annotated richly with human uncertainty, 2) permitting human uncertainty at intervention-time can lead existing systems to go astray, and 3) training with concept uncertainty can boost test-time robustness --- we argue that there is immense value in \textit{collecting new concept datasets} enriched with human uncertainty.

% To facilitate this research, we build a new platform for uncertainty elicitation over concepts, which we call \texttt{UElic}. 

% % [...details on interface...]

% \subsection{Uncertainty Elicitation Mechanism Design} Our interface, \texttt{UElic}, offers a lightweight paradigm for users to express uncertainty. Users are presented with the features of interest (e.g., an image), the concept to be annotated, and all available options. To reduce the cognitive load of expressing uncertainty over \textit{all} options per concept, we request users select only the attributes they think are plausible and express their uncertainty over these attributes by dragging an interactive bar chart to represent their perceived probability. An example interface screen is depicted in Figure \ref{fig:interface-unc}. 

% \begin{figure}[!htb]
%     \centering
%     \includegraphics[width=0.9\linewidth]{figures/elic_interface_unc.png}
%     \caption{Example screen of \texttt{UElic} for CUB. Participants select the concept attributes they think are plausible, and drag bars to express said uncertainty. Here, the back is not visible; users must be uncertain in their annotation. We empower annotators to \textit{richly} express this belief distribution, in contrast to the original \texttt{CUB} dataset.}
%     \label{fig:interface-unc}
% \end{figure}


% \section{Relabeling of \texttt{CUB} with Rich Human Uncertainty: CUB-S}

% We offer a first application of \texttt{UElic} to relabel a subset of \texttt{CUB} with human soft labels over all concepts. We release our dataset as CUB-S (S for soft labels). 

% We believe CUB-S can serve as a formative dataset to further study human uncertainty in concept-based models. CUB-S is replete with nuances of handling real human uncertainty. In this Section, we offer several insights into the character of human uncertainy we elicit. We then highlight how state-of-the-art algorithms for working with concept models, e.g., adaptive intervention policies like CooP, struggle to handle real human uncertainty -- offering exciting grounds for future methodological advances. 

% \subsection{Collecting CUB-S} We recruit ..... through an Ethics Reviewed study. ...  Further details can be found in the Appendix. 

% \subsection{How CUB-S Addresses Limitations in CUB} We raised several issues around the uncertainty annotations in \texttt{CUB} in Section [...]. We detail how CUB-S addresses each of those limitations. 

% \begin{enumerate}
%     \item \textit{Missing richness:}
%     \item \textit{Studying individual annotator certainty:}
%     \item \textit{Reliability:}
% \end{enumerate}


% \subsection{Observations} [...Matthew to add some findings...]


% \begin{figure}[!htb]
%     \centering
%     \includegraphics[width=0.9\linewidth]{example-image-a}
%     \caption{Example indicative soft concept annotations elicited in CUB-S. \kccomment{Matthew to add: 3-4 examples, each demonstrating some key feature in the data pls (richness, odd calib, sensible uncertainty under occlusion). Can likely make this a column-spanning fig.}}
%     \label{fig:soft_concept_examples}
% \end{figure}


% We use the Expected Calibration Error (ECE) \cite{naeini2015obtaining} as a metric to evaluate the accuracy of annotators when estimating their confidence. Intuitively, the metric is the expected absolute difference between the fraction of correct predictions (accuracy), and the probabilities provided by the annotators (confidence). Majority of annotators are reasonably calibrated, although this value is skewed by the large number of (correct) zero probabilites provided.
% \begin{figure}
%     \centering
%     \includegraphics[width=1.0\linewidth]{CUB-S-figures/ECE_annotators.png}
%     \includegraphics[width=1.0\linewidth]{CUB-S-figures/calibration_curve.png}
%     \caption{Top: Distribution of Expected Calibration Error for annotators in CUB-S. Bottom: Calibration curve for CUB-S annotators, showing consistent underestimates of small probabilities and overestimates of large probabilities.}
%     \label{fig:miscalib_cubs}
% \end{figure}
% % \begin{figure}[!htb]
% %     \centering
% %     \includegraphics[width=0.9\linewidth]{figures/calibration_curve.png}
% %     \caption{Miscalibration in CUB-S.\kccomment{Matthew: pls add your awesome details!}}
% %     \label{fig:miscalib_cubs}
% % \end{figure}


% \subsection{Existing Methods Ill-Equipped to Handle CUB-S} 

% How well do state-of-the-art policies for selecting which concept to query humans perform when we assume the interventions returned by human users are those of CUB-S? We compare the approximate oracle policy from \citeauthor{chauhan2022interactive} (Skyline) against their adaptive policy, CooP. We find that on CUB-S, CooP is no better than a random intervention scheme. 

% What underlies this failure? Inspecting the CooP scores reveals that high CooP scores (those which are preferentially selected early) are frequently also the concepts which humans struggle to annotate with calibrated certainty (see Figure \ref{fig:coop_failure_analyses}). Skyline preferentially selects concepts which are valuable to intervene on the model (medium entropy), yet which humans valuably annototate; In contrast, we observe that the top Skyline selections tend to be those where the model has moderately high entropy, but the human is fairly confident [confirm]. Here, we see the consequences of breaking the human-certain-oracle assumption typical of many concept-based model works. We see adaptive policies which can better anticipate where humans may be poor annotators (e.g., uncertain and/or miscalibrated) as promising grounds for future work. 

% \begin{figure}[!htb]
%     \centering
%     \includegraphics[width=0.9\linewidth]{figures/cub_s_policies.png}
%     \caption{CooP, a state-of-the-art adaptative intervention policy struggles to handle real, human uncertainty. \kccomment{Placeholder on subset of CUB-S: to run on full CUB-S, and update viz. Trend in results expected to persist.}}
%     \label{fig:cubs_policies}
% \end{figure}


% \begin{figure}[!htb]
%     \centering
%     \includegraphics[width=0.9\linewidth]{figures/unc_diffs_cubs_coop.png}
%     \caption{Humans are not always good annotators of high CooP-scored concepts, rendering the queries less effective. Left: CooP scores for the first three selected concepts may be those that humans are uncertain in; here, depicting the maximum probability assigned by the human over the selected concept group. Right: Skyline preferentially selects concepts which humans have lower overall entropy in than CooP in the first three selections.}
%     %High CooP scores are related to cases of high human uncertainty (left) and miscalibrated labelings (right), rendering the interventions ineffective, and actually harmful. \kccomment{To run on full CUB-S and update formatting}}
%     \label{fig:coop_failure_analyses}
% \end{figure}

% % Cognitive burden of selecting diff vals in our checkboxes (click \textit{both} blue and purple? or just one b/c of cog load); could lead to false ``zero-prob''. Impact of false zero prob on interventions; to show: reason we get the breakdown with color.


% \subsection{Limitations}

% While we find \texttt{CUB-S} ...., we emphasize that several limitations in our data collection and present analyses are worth noting. 

% \section{Looking Ahead}

% We emphasize the importance of considering human uncertainty in concept-based models, and the need for richer datasets of human uncertainty to study these challenges. CUB-S is a promising initial playground to study the nuances faced with real human uncertainty.

% However, we recognize that the annotation of large-scale datasets with human uncertainty may be practically challenging. It is costly to elicit human uncertainty: annotators take substantially more time \citep{selfCiteSoftLabel}. There is a need for more scalable elicitation techniques, and better simulators of human uncertainty to permit the study of softness at train-time. 

% % Costs to annotate (more time). Leads to less data**. Highlight challenges posed by limited data (need for simulators: but can run into a sim2real gap when it comes to the richness, and irregularities/inconsistencies, of real human uncertainty -- lead into calibration section).  \kccomment{Key: emphasize that, while we see that averaging over individs' uncertainty can help smooth out errors in individs' uncertainty expression; it's unlikely that we'll be able to get enough intervention data to do such averaging at test-time! In real-world, we do have a \textit{single} human interacting with the system to make a decision. See Fig. \ref{fig:avgd_unc}}.

% Moreover, as we highlight, the degree of miscalibration in elicited uncertainty may be highly sensitive to the elicitation methodology [cogsci cites]. Our study highlights calibration as a prime target for new algorithms; we encourage work to post-hoc calibrate in a few-shot manner, particularly in the face of likely limited data. Likewise, we encourage further experimentation with \texttt{UElic} to encourage better calibration from humans. 

% % New algorithms, which may be able to post-hoc calibrate in a few-shot manner. Alterations to the interface to encourage better calibration from humans [cogsci cite]. Exciting opportunities across fields (ML, CogSci, UX Design, Psych, etc).  

% Additionally, considering human uncertainty in interventions opens up exciting opportunities in the study of human-machine complementarity [cites]. We find that the concepts preferred by CooP may be those which humans also struggle to annotate; learning models which can complement humans' strengths and weaknesses, accounting for their expertise and confidence, are promosing grounds for further study with CUB-S and beyond. 

% \section{Conclusion}

% We highlight the importance of considering human uncertainty in concept-based models. Humans in the real-world are not certain oracles. We make mistakes and may be unsure. Algorithms designed to support people.... Even though humans may be miscalibrated in their uncertainty, we believe the study of tools to elicit and work with human uncertainty are ripe. Existing concept datasets are inadequate for such study. We offer a new interface, \texttt{UElic}, and a new challenge dataset, \texttt{CUB-S}, to support further study into human uncertainty in interventions. Modeling human uncertainty at train- and test-time have the potential to greatly improve the reliability and trustworthiness of concept-based models when deployed in-the-wild.

\bibliography{main}

% todo: this will need to be an entirely separate document!
\newpage
% ~
% \newpage

\section*{Supplement}

\subsection*{Constructing \texttt{UMNIST}}

We provide further clarity on how we constructed \texttt{UMNIST}. Each sample of the \texttt{UMNIST} dataset is formed by $p$ $28\times28$ grey-scale images of handwritten zeros or ones, given as a normalized sample with shape $\mathbf{x} \in [0, 1]^{28 \times 28 \times p}$. We annotate each sample with $p$ binary concept annotations $\mathbf{c} \in \{0, 1\}^p$, where $c_i$ indicates whether the $i$-th image is a one or a zero, and a task label $y \in \{0, \cdots, p\}$ corresponding to the number of ones in its digits, i.e., $y = \sum_i c_i$. To introduce uncertainty in this dataset's samples and concepts, we update concept $c_i$ corresponding to the $i$-th image as follows:
\[
    c_i := \begin{cases}
        \text{Randomly sample from Unif}(0, \delta) & \text{if i-th digit is 0}\\
        \text{Randomly sample from Unif}(1 - \delta, 1) & \text{if i-th digit is 1}\\
    \end{cases}
\]
where $\delta \in [0, 1]$ is a user-provided hyperparameter controlling the amount of dataset uncertainty. Furthermore, in order for this concept annotation uncertainty to be reflected as part of the input digits $\mathbf{x}$, we mix a concept's corresponding digit, akin to~\citet{mixup}, with a randomly selected MNIST training example of the opposite digit using $c_i$ as the mixing ratio. In other words, after generating a sample's uncertain concept annotations $\mathbf{c}$ we update its $i$-th input digit $\mathbf{x}_{(:, :, i)}$ as follows:
\[
    \mathbf{x}_{(:, :, i)} := \begin{cases}
        (1 - c_i) \mathbf{x}_{(:, :, i)} + c_i \mathbf{z} \text{ with } \mathbf{z} \sim p_\text{M}(\mathbf{x} | y = 1) & \text{if } \mathbf{x}_{(:, :, i)} \text{ is 0}\\
        c_i \mathbf{x}_{(:, :, i)} + (1 - c_i) \mathbf{z} \text{ with } \mathbf{z} \sim p_\text{M}(\mathbf{x} | y = 0) & \text{if } \mathbf{x}_{(:, :, i)} \text{ is 1}\\
    \end{cases}
\]
where $p_\text{M}(\mathbf{x} | y)$ is the empirical training distribution of MNIST samples whose label is $y$. For this paper, we focus on using only $p = 10$ digits per sample. See Figure \ref{fig:umnist_example_data} for some examples of this dataset as we vary $\delta$. 

\begin{figure}[!htb]
    \centering
    \includegraphics[width=1.0\linewidth]{UMNIST/umnist_examples.pdf}
    \caption{Example datapoints in \texttt{UMNIST} as we vary the value of $\delta$ (rows). Each row represents a single sample, with each column representing one of the $p=5$ digits forming that sample. We include each concept's annotation, as well as the datapoint's label, underneath each digit and to the left of each datapoint, respectively.}
    \label{fig:umnist_example_data}
\end{figure}

\subsection*{Computational experiment details} 

We next include additional details on how models were trained and run on the various probe datasets, as well as the intervention methods considered.

\subsubsection*{Training Details for \texttt{UMNIST} Experiments}
For all \texttt{UMNIST} experiments, we train both CBMs and CEMs using a concept extractor whose architecture consisted of four 3-by-3 convolutional layers with filters $\{5, 10, 20, 40\}$ followed by a linear layer with $20$ activations and an output layer with $pm$ output activations, where $m$ is the embedding size used for CEM (one can think of CBM as having $m=1$). In practice, we set $m$ to $8$ following the recommendations from~\citet{cem22}. Between all non-output layers, we include leaky-ReLU nonlinear activations and we apply batch normalization after each nonlinearity that follows a convolutional layer. Similarly, for both CEMs and CBMs, we use a simple ReLU two-layer MLP as its concept-to-label map with layers sizes $\{20, p\}$ and train each end-to-end CBM/CEM by weighting the concept loss as much as the task loss (i.e., the joint training hyperparameter $\alpha$ was set to $\alpha = 1$ for both methods). Finally, to avoid each model learning to simply predict the most common class to minimize its error, we weight each sample's task loss according to the empirical label distribution of its corresponding label to encourage our models.

All models are trained by sampling a total of $20,000$ training \texttt{UMNIST} samples, of which 20\% were used as a validation set, and tested by sampling $5,000$ \texttt{UMINST} testing samples from MNIST's testing set (so no digit in the testing set is ever used to construct \texttt{UMNIST}'s training set). We train all models using a standard Adam \cite{kingma2014adam} optimizer with a learning rate $10^{-3}$ and a batch size of $256$ for a maximum of $50$ epochs, stopping earlier if the validation loss has not improved for $15$ epochs. For each method in \texttt{UMNIST}, we run 5 models from different seeds.
%All results shown in this paper were obtained by training 5 different models, for each method, from 5 different initialization seeds.

\subsubsection*{Training Details for \texttt{CheXpert} Experiments} 
For the \texttt{CheXpert} dataset \citep{irvin2019chexpert}, we train all models for 25 epochs, subsampling the dataset to use only 25\% of the training dataset when training due to the large size of the dataset. 
Because the test split for \texttt{CheXpert} does not have the ``uncertain'' concept label, we perform an 80-10-10 split of the train split into the train, validation, and test folds. Results for \texttt{CheXpert} are averaged over 5 trials, and we use a learning rate of 0.001 across all trials. 

\subsubsection*{Training Details for CUB-Based Experiments} Models trained on \texttt{CUB} followed the same training settings as~\citet{cem22}; we employ a single model run for each seed due to computational complexity.

\subsubsection*{Details on Intervention Policies} The interaction policies we consider in this work (Random and Skyline) consider the setting where a user can be queried to intervene, or edit, a single concept (e.g., wing color) at a time. \textit{Skyline} assumes access to the true label $y$ and how the human would intervene (e.g., assumes access to the \texttt{CUB-S} elicited soft concept annotations), and ``tests'' intervening with each of the remaining concepts to see which yields the highest predicted probability of the model on the true label. In that way, this mimics an ``Oracle'' policy, which can greedily select the best of the available next concept interventions, following \citeauthor{chauhan2022interactive}. However, the assumption of knowing the humans' interventions in advance, and the true label, are not realistic (and defeat the purpose of an intervention policy) in practice; hence, this method is meant to capture the ``best possible'' amount of information that can be gleaned by a single-step direct intervention policy alone. \textit{Random} simply selects the next concept to query by randomly choosing amongst the available concepts which have not yet been queried.
% \kccomment{Todo: add details on int policies}

% \subsubsection*{Intervention Policy: Skyline} We also provide further details on the Skyline. 
\subsection*{Additional Results on Concept-Incomplete Variant of \texttt{UMNIST}}
As discussed by~\citet{cem22}, CBMs have a significant failure mode when the set of concept annotations available at training time is not fully predictive, or complete, with respect to the task of interest. Similarly to our \texttt{UMNIST} experiments summarized in Figure~\ref{fig:umnist_res}, this section explores how test-time uncertainty affects CBMs and CEMs when the dataset we are working with does not have a complete set of concept annotations. For this, we use our defined \texttt{UMNIST} dataset but only provide $50\%$ of its concept annotations at training time. We train a CBM and CEM using the same configuration and architecture as that described for our \texttt{UMNIST} experiments, with the exception that the concept weight loss $\alpha$ was changed to $0.1$. We apply such a change to improve CBM's performance, as otherwise, it was unable to achieve a moderately high task accuracy.

Our results in Figure~\ref{fig:umnist_res_incomplete} demonstrate that both CBMs and CEMs significantly drop their performance when test-time uncertainty increases (as we saw in Figure~\ref{fig:umnist_res} before). Nevertheless, in contrast with Figure~\ref{fig:umnist_res}, we see that interventions in CBMs actually decrease their test accuracy, with uncertainty exacerbating this effect even further. Therefore, these experiments suggest that in concept-incomplete setups, which tend to be what we would expect in real-world datasets given the cost of acquiring all possible concept annotations, CEMs are relatively safer to use regardless of the user's uncertainty at intervention time.

\begin{figure}[!htb]
    \centering
    \includegraphics[width=\linewidth]{UMNIST/umnist_cem_vs_cbm_comp_incomplete.pdf}
    
    \caption{Mean test accuracies of random interventions on CBMs and CEMs, and standard errors across 5 different random initializations, as we increase the number of concepts we intervene on. These models are trained on a variant of \texttt{UMNIST} where we only provide $50\%$ of its concepts at training time. 
    }
    \label{fig:umnist_res_incomplete}
\end{figure}

\subsection*{Additional \texttt{CheXpert} Investigations}

We include training with simulated uncertainty in Figure \ref{fig:chexpert_sim_unc}.

\begin{figure}[!htb]
    \centering
    \includegraphics[width=1.0\linewidth]{figures/cem_heatmap_chexpert_sim.pdf}
    \caption{Comparing CEMs trained and tested on differing levels of uncertainty in \texttt{CheXpert}. Heatmap colors depict AUC of the different variants.}%Across all levels of training uncertainty, increasing the level of test uncertainty improves intervention performance in \texttt{CheXpert}. Not all uncertainty is beneficial.}
    %Note the jump in performance once training and testing uncertainty are equal; for example when testing uncertainty is $0.5$ for $0.5$ training uncertainty, the intervention performance jumps from 84 to 89. We finally note that having some training uncertainty, at 0.95 rather than 1, helps across values of test uncertainty.}
    \label{fig:chexpert_sim_unc}
\end{figure}


We also explore the original uncertain annotations in \texttt{CheXpert} \citep{irvin2019chexpert}, which contains concept annotations from chest x-rays. The dataset is marked with four labels: positive, negative, unknown, and uncertain. For our experiments, we vary the value taken by uncertain labels, both at train and test time, and investigate its impact on intervention performance. In Figures and \ref{fig:chexpert_test} and \ref{fig:heatmap_chexpert}, we find that test-time uncertainty improves intervention performance, while train-time uncertainty has minimal impact. 
This is partially because of the sparsity of uncertain labels in the dataset; only 5\% of annotations are marked as uncertain, capping the total effect of train-time uncertainty. 
For test-time uncertainty, wen find that non-zero values improve intervention accuracy, because models are able to distinguish between ``uncertain'' labels and ``negative'' labels. 
%[likely in appendix; but may go in main if we include with simulated uncertainty]

\begin{figure}[!htb]
    \centering 
    \includegraphics[width=0.9\linewidth]{figures/chexpert_test_time.pdf}
    \caption{Test-time uncertainty values have a large impact on intervention performance, when using random concept interventions. Setting it to $0$ prevents models from differentiating between negative concepts and uncertain concepts, leading to a decrease in performance. However, setting it to non-zero values allows models to pick up on this difference and improve intervention performance. } 
    \label{fig:chexpert_test} 
\end{figure}

\begin{figure}[!htb]
    \centering
    \includegraphics[width=1.0\linewidth]{figures/cem_heatmap_chexpert.pdf}
    \caption{Intervention performance (i.e., random interventions) when using 8 out of 13 concepts to intervene across training and testing uncertainty values. Test uncertainty values have a much larger impact than train uncertainty values, and in general, training with uncertainty seems to have little impact on test-time uncertainty performance.}
    \label{fig:heatmap_chexpert}
\end{figure}

\subsection*{Additional Details on CUB-S}

% We next include additional details on the way we collected CUB-S, as well as further qualitative observations into the labels collected.
We next include further qualitative observations into the CUB-S soft concept labels collected. We observe in Figure \ref{fig:cubs_annotation_hist} that distribution of provided uncertain annotations is highly irregular, with heavy tails at 0 and 100, and a peak at 50. We hypothesize that heavy tails may be explained by humans rounding values to reduce their cognitive load; \citeauthor{selfCiteSoftLabel} found similar rounding effects in free-form uncertainty expression. 50 is the default value provided by the interface, likely underlying the large number of annotations at 50. This suggests there is scope for improving the interface to extract a more accurate distribution of uncertainties, potentially striking a more naunced balance in granularity of information elicited, e.g., \citep{efficientElic}.

As observed in Section \ref{sec:cubs_richness}, the calibration of individual annotators varies significantly. Figure \ref{fig:cubs_annotation_hist} shows that most annotators consistently assign approximately 100 probability mass for each concept, as one would expect. However, the distribution is positively skewed, with a significant number of annotators consistently over-assigning probability mass acoss the concept groups (for any individual concept, the annotator can endorse at most 100 ``probability units''). This is partly explained by concept groups where more than one concept is relevant (such as color), although it is also likely that annotators are overestimating their confidence. 

% Since each concept in \texttt{CUB-S}\,is unique, variation in the annotations between concepts is expected. 

Further, we investigate the variance in flavor of uncertainty expressed between different concepts. In Figure \ref{fig:CUBS_prob_mass_concept} we plot the distribution of probability mass assigned for each concept. We observe significant variations between concepts, in terms of their mean, variance and skew. Some concepts such as ``eye color'' have a very tight distribution around 100, suggesting those concepts are ``easy'' to annotate. In contrast, some concepts such as ``upperparts color'' show greater variation in the probability mass assigned. These concepts tend to either be color concepts, which can have several correct annotations, or ambiguous concepts like ``size'' which may be harder to annotate correctly.

These observations highlight nuances in the \texttt{CUB-S}\,dataset which aren't present in the original hard annotations. Soft annotations give insights into how humans interpret concepts when labeling and the variation in individual calibration of annotators \citep{selfCiteSoftLabel}. We hope to encourage future work to design ML models and datasets which account for the idiosyncrasies of human uncertain annotations.

% \subsubsection*{Elicitation Details} We recruit 89 participants from the crowdsourcing platform, Prolific \cite{palan2018prolific}. Participants annotate \textit{all 28 concepts} for two different bird images: totalling \textbf{4984} soft categorical concept group annotations. Within each soft concept group annotation, participants provide their uncertainty over each of the possible attributes for that concept (e.g., possible wing colors, beak shapes, etc). Stimuli are selected from the \texttt{CUB} test set, and preferentially subsampled to include images which CEMs and CBMs both typically get wrong\footnote{Approximately 50\% of the images shown to participants are those which four different seeds of both CEMs and CBMs got incorrect, rendering them more interesting - and challenging - to study at intervention-time}. Participants are paid at a base rate of \$9/hr, with an optional bonus paid up to \$10/hr to encourage quality predictions; the bonus is applied to all participants.


Additionally, our labels demonstrate potential issues with the concept filtering typically applied on CUB. \citeauthor{koh2020concept} propose a filtering scheme to avoid overly sparse annotations; however, we note that our annotators assign a substantial amount of probability mass to concept attributes which are \textit{filtered out} (see Figure \ref{fig:cubs_discarded_mass}). These data highlight that the filtered out attributes could indeed be missing critical information from people as to what is in the image. 

% \begin{figure*}[!htb]
%   \centering
%   \includegraphics[width=0.9\linewidth]{Sample UAI 2023 paper/figures/elic_interface_color.png}
%   \caption{An additional example interface screen. Participants only select attributes they think are plausible.}\label{fig:interface}
% \end{figure*}

\begin{figure}[!htb]
    \centering
    \includegraphics[width=0.9\linewidth]{CUB-S-figures/annotation_hist.png}
    \caption{Distribution of uncertainty values for all annotations in \texttt{CUB-S}. Annotators favor certain annotations (0 or 100) and the default value of 50 provided by the interface.}
    \label{fig:cubs_annotation_hist}
\end{figure}

\begin{figure}[!htb]
    \centering
    \includegraphics[width=0.9\linewidth]{CUB-S-figures/annotator_prob_mass.png}
    \caption{Histogram showing the distribution of mean total probability mass for each concept assigned by each annotator. Most annotators assign approximately 100 probability mass, although there are a significant number which over-assign probability mass.} 
    \label{fig:CUBS_annotator_mass}
\end{figure}

\begin{figure}[!htb]
    \centering
    \includegraphics[width=0.9\linewidth]{CUB-S-figures/probability_mass.png}
    \caption{Distribution across images of total probability mass assigned for each concept. There is significant variation in the mean, skew and variance of distributions, showing that different concepts are annotated differently by human annotators.}
    \label{fig:CUBS_prob_mass_concept}
\end{figure}

\begin{figure}[!htb]
    \centering
    \includegraphics[width=0.9\linewidth]{figures/avg_discarded_mass.pdf}
    \caption{Amount of assigned probability mass discarded per individual when using the popular \citeauthor{koh2020concept} concept filtering (averaged over concept groups). }
    \label{fig:cubs_discarded_mass}
\end{figure}

\begin{figure*}
    \centering
    \includegraphics[width=1\linewidth]{CUB-S-figures/examples_annotations_2.png}
    \caption{Additional examples showing rich annotations for \texttt{CUB-S} compared to hard assignments in \texttt{CUB}.}
    \label{fig:cubs_examples_2}
\end{figure*}

\begin{figure*}
    \centering
    \includegraphics[width=1\linewidth]{CUB-S-figures/examples_compare_annotations.png}
    \caption{\texttt{CUB-S} Examples where multiple annotators labelled the same image. Each bar color represents a unique annotator for each image. The annotated concepts vary significantly between annotators, especially for challenging concepts such as ``wing shape'' and ``wing pattern''.}
    \label{fig:cubs_examples_annotators}
\end{figure*}

\subsection*{Additional \texttt{CUB} Uncertainty Computational Experiments}

We next include further observations from our computational experiments in \texttt{CUB} and \texttt{CUB-S}.

\subsubsection*{Broad vs. Narrow Uncertainty} We demonstrate the sensitivity of concept-based systems to broad versus narrow uncertainty under the Random policy (see Figure \ref{fig:random_smooth_off}), further highlighting that the method of distributing uncertainty through discrete confidence scores matters impacts intervention efficacy.

\begin{figure}[!htb]
    \centering
    \includegraphics[width=0.9\linewidth]{figures/redist_diff_random.pdf}
    \caption{ the efficacy of random interventions on CEMs.}
    \caption{Impact of different ways of distributing the discrete uncertainty over categorical concept groups, selected using Random intervention policies on CEMs. }
    \label{fig:random_smooth_off}
\end{figure}

\subsubsection*{Individual- vs Population-Level Uncertainty} As noted, whether or not we intervene with individual or population-level annotations matters (see Section 5.2.3), and we see in Figure \ref{fig:cub_train_uncertainty} that training and then intervening with population-level annotations yields the best performance. These observations are relevant not only to ML practioners who work with \texttt{CUB}, but broadly in annotation-design and questions around who and how many annotators should we elicit from.


\begin{figure}[!htb]
    \centering
    \includegraphics[width=0.9\linewidth]{figures/diffs_in_auc_agg.pdf}
    \caption{It matters whether or not we use \textit{instance-level, individual} annotator uncertainty, or average over many individuals' uncertainty. Averaging improves the stability of interventions; but in practice, we may only have a single individual who can provide their uncertainty. We find sizeable differences in the intervention efficacy when using averaged uncertainty for both Skyline and Random.}%Complete intervention curves can be seen in Figure \ref{fig:complete_avgd_unc}.} %Will add CBMs. Note: this particular observation may be original-CUB-dependent to some degree; original \texttt{CUB} is very noisy [cite].}}
    \label{fig:avgd_unc}
\end{figure}


\begin{figure}[!htb]
    \centering
    \includegraphics[width=0.9\linewidth]{figures/agg_unc_train_cub_cem_test_agg.pdf}
    \caption{Training with a moderate level of (aggregate/population-level) uncertainty improves robustness under test-time uncertainty; as measured by AUC between intervention-accuracy curve. Higher is better.} %And holds across a variety of imputed forms of test-time uncertainty, at both instance- and aggregate-/class-level. }%Training on the instance-level uncertainty hampers performance.}
    \label{fig:cub_train_uncertainty}
\end{figure}

\subsubsection*{CBMs and Simulated Uncertainty}
Further, we concretize why we focus on CEMs in the bulk of this work. CBMs severely struggle under test-time uncertainty when dealing with concept-incomplete datasets (see the \texttt{UMNIST} section of this Supplement) and in-the-wild uncertainty (see Figure \ref{fig:cbm_cub_unc}). % Interestingly, we observe that different concepts are preferred for different models (see Figures \ref{fig:skyline_cem_sels} and \ref{fig:skyline_cbm_sels}). 

\begin{figure}[!htb]
    \centering
    \includegraphics[width=0.9\linewidth]{figures/cbmvary_probs_grid.pdf}
    \caption{CBMs struggle to handle uncertainty in CUB as well and are comparatively worse than CEMs.}
    \label{fig:cbm_cub_unc}
\end{figure}

\subsubsection*{Skyline Selections Reveal ``Helpful'' and ``Harmful'' \texttt{CUB-S} Annotations} 

 As seen in Figure \ref{fig:cubs_policies}, Skyline rapidly improves by selecting ``good'' uncertain annotations; however, the final selections hamper performance. We depict the proportion of selections for each concept being in the first or last 5 selections by Skyline. Avoiding selecting the examples in the last 5, e.g., ``upperparts'' color, offer promising directions for future policy design and investigation into when and why humans are good uncertain annotators. Interestingly, we observe differences in which concepts are preferred depending on whether the model was trained without (Figure \ref{fig:skyline_cem_sels_none}) or with uncertainty in the concepts at training time (i.e., Figures \ref{fig:skyline_cem_sels0.5}, \ref{fig:skyline_cem_sels0.7}, \ref{fig:skyline_cem_sels0.9}). 
 %Differences across models, e.g., compared to selections for a CBM in Figure \ref{fig:skyline_cbm_sels} demonstrate different model profiles may benefit from different levels of human uncertainty over different queries.
 
\begin{figure*}[!htb]
    \centering
    \includegraphics[width=0.9\linewidth]{figures/skyline_selections_cem_None.png}
    \caption{Skyline selections for CEM run on \texttt{CUB-S} reveal when human uncertainty elicitation is helpful (versus harmful). Proportion of selections for each concept being in the first or last 5 selections by Skyline. CEM trained on certain concepts.}
    % \caption{Skyline selections for CEM reveal when human uncertainty elicitation is helpful (versus harmful). As seen in Figure \ref{fig:cub-s-policies}, Skyline rapidly improves by selecting ``good'' uncertain annotations; however, the final selections hamper performance. Here, we depict the proportion of selections for each concept being in the first or last 5 selections by Skyline. Avoiding selecting the examples in the last 5, e.g., ``upperparts'' color, offer promising directions for future policy design and investigation into when and why humans are good uncertain annotators. Differences across models, e.g., compared to selections for a CBM in Figure \ref{fig:skyline_cbm_sels} demonstrate different model profiles may benefit from different levels of human uncertainty over different queries.}
    \label{fig:skyline_cem_sels_none}
\end{figure*}

\begin{figure*}[!htb]
    \centering
    \includegraphics[width=0.9\linewidth]{figures/skyline_selections_cem_agg0.5.png}
    \caption{Skyline selections for CEM trained on uncertain concepts (where the imputed ``Probably'' probability is set to 0.5). Population-level broad uncertainty labels used.}
    % \caption{Skyline selections for CEM reveal when human uncertainty elicitation is helpful (versus harmful). As seen in Figure \ref{fig:cub-s-policies}, Skyline rapidly improves by selecting ``good'' uncertain annotations; however, the final selections hamper performance. Here, we depict the proportion of selections for each concept being in the first or last 5 selections by Skyline. Avoiding selecting the examples in the last 5, e.g., ``upperparts'' color, offer promising directions for future policy design and investigation into when and why humans are good uncertain annotators. Differences across models, e.g., compared to selections for a CBM in Figure \ref{fig:skyline_cbm_sels} demonstrate different model profiles may benefit from different levels of human uncertainty over different queries.}
    \label{fig:skyline_cem_sels0.5}
\end{figure*}

\begin{figure*}[!htb]
    \centering
    \includegraphics[width=0.9\linewidth]{figures/skyline_selections_cem_agg0.7.png}
    \caption{Skyline selections for CEM trained as in Figure \ref{fig:skyline_cem_sels0.5}, but with the imputed ``Probably'' probability set to 0.7.}
    % \caption{Skyline selections for CEM reveal when human uncertainty elicitation is helpful (versus harmful). As seen in Figure \ref{fig:cub-s-policies}, Skyline rapidly improves by selecting ``good'' uncertain annotations; however, the final selections hamper performance. Here, we depict the proportion of selections for each concept being in the first or last 5 selections by Skyline. Avoiding selecting the examples in the last 5, e.g., ``upperparts'' color, offer promising directions for future policy design and investigation into when and why humans are good uncertain annotators. Differences across models, e.g., compared to selections for a CBM in Figure \ref{fig:skyline_cbm_sels} demonstrate different model profiles may benefit from different levels of human uncertainty over different queries.}
    \label{fig:skyline_cem_sels0.7}
\end{figure*}

\begin{figure*}[!htb]
    \centering
    \includegraphics[width=0.9\linewidth]{figures/skyline_selections_cem_agg0.9.png}
    \caption{Skyline selections for CEM trained as in Figure \ref{fig:skyline_cem_sels0.5}, but with imputed ``Probably'' probability set to 0.9.}
    % \caption{Skyline selections for CEM reveal when human uncertainty elicitation is helpful (versus harmful). As seen in Figure \ref{fig:cub-s-policies}, Skyline rapidly improves by selecting ``good'' uncertain annotations; however, the final selections hamper performance. Here, we depict the proportion of selections for each concept being in the first or last 5 selections by Skyline. Avoiding selecting the examples in the last 5, e.g., ``upperparts'' color, offer promising directions for future policy design and investigation into when and why humans are good uncertain annotators. Differences across models, e.g., compared to selections for a CBM in Figure \ref{fig:skyline_cbm_sels} demonstrate different model profiles may benefit from different levels of human uncertainty over different queries.}
    \label{fig:skyline_cem_sels0.9}
\end{figure*}



% \begin{figure}[!htb]
%     \centering
%     \includegraphics[width=0.9\linewidth]{figures/skyline_selections_cbm_sig.png}
%     \caption{Skyline selection preferences for CBM (sigmoid variant) further reveal how humans' strengths and weaknesses may complement models differently.}
%     \label{fig:skyline_cbm_sels}
% \end{figure}





% % help from: https://tex.stackexchange.com/questions/10245/how-to-order-3-images-horizontally
% \begin{figure}[!htb]
% \minipage{0.32\textwidth}
%   \includegraphics[width=\linewidth]{figures/inst_unc_train_cub_cem.pdf}
%   \caption{Training on instance-level smoothed, testing on instance-level smoothed.}\label{fig:smooth_train_inst_test}
% \endminipage\hfill
% \minipage{0.32\textwidth}
%   \includegraphics[width=\linewidth]{figures/nosmooth_unc_train_cub_cem.pdf}
%   \caption{Training on instance-level, unsmoothed labels.}\label{fig:nosmooth_train_inst_test}
% \endminipage\hfill
% \minipage{0.32\textwidth}%
%   \includegraphics[width=\linewidth]{figures/agg_unc_train_cub_cem.pdf}
%   \caption{Training on population-level labels, testing on instance-level smoothed.}\label{fig:agg_train_inst_test}
% \endminipage
% \label{fig:compare_training_labels_cub}
% \end{figure}




% \begin{figure}[!htb]
%     \centering
%     \includegraphics[width=0.9\linewidth]{example-image-a}
%     \caption{Placeholder: CBM fails on \texttt{CUB} uncertainty, too. Logit model is particularly bad.}
%     \label{fig:cbm_cub}
% \end{figure}


% \begin{figure}[!htb]
%     \centering
%     \includegraphics[width=0.9\linewidth]{figures/simulated_uncertainty_cub.png}
%     \caption{Simulating uncertainty in CEMs during training interestingly does not induce robustness to test-time uncertainty, as measured by AUC under the intervention-accuracy curve (higher is better). \kccomment{This may be a pecularity of the class-based nature of \texttt{CUB} and/or CEMs....}}
%     \label{fig:cem_cub_unc_sim}
% \end{figure}

% \subsubsection*{Other Models}

% We find in Figure \ref{fig:random_smooth_off} that whether we smooth the ``off'' concepts using uncertainty scores impacts efficacy of random selections as well. 




% \begin{figure}[!htb]
%     \centering
%     \includegraphics[width=0.9\linewidth]{figures/redist_diff_skyline.png}
%     \caption{The method of distributing uncertainty through discrete confidence scores matters impacts Skyline efficacy.}
%     \label{fig:skyline_smooth_off}
% \end{figure}

% \begin{figure}[!htb]
%     \centering
%     \includegraphics[width=0.9\linewidth]{figures/redist_diff_skyline.png}
%     \caption{For categorical concept groups, the method of distributing uncertainty through discrete confidence scores matters. Here, we demonstrate that Skyline's effectiveness is substantially impacted by whether we apply the discrete uncertainty scores to smooth the ``off'' concepts or not -- the application of which is unclear from the provided annotations. Here, we see that the upper bound on intervention effectiveness is heavily impacted by the way human uncertainty is employed.}
%     \label{fig:redist_cub}
% \end{figure}

% \begin{figure}[!htb]
%     \centering
%     \includegraphics[width=0.9\linewidth]{figures/vary_probs_avg.png}
%     \caption{It matters whether or not we use \textit{individ} annotator uncertainty, or average over many individuals' uncertainty. Averaging improves stability of intervention; but in practice, we may only have a single individ who can provide their uncertainty. \kccomment{Lot of lines atm... Showing with and without class-avging at intervention-time. Can adjust for clarity. Replace CooP with Random.}} %Will add CBMs. Note: this particular observation may be original-CUB-dependent to some degree; original \texttt{CUB} is very noisy [cite].}}
%     \label{fig:complete_avgd_unc}
% \end{figure}


% \subsection{Additional Training with Uncertainty Explorations}



\end{document}
\endinput
%%
%% End of file `sample-acmtog.tex'.
