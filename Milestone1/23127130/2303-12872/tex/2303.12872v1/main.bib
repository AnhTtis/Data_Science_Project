@article{mahinpei2021promises,
  title={Promises and pitfalls of black-box concept learning models},
  author={Mahinpei, Anita and Clark, Justin and Lage, Isaac and Doshi-Velez, Finale and Pan, Weiwei},
  journal={arXiv preprint arXiv:2106.13314},
  year={2021}
}

@book{shneiderman2022human,
  title={Human-centered AI},
  author={Shneiderman, Ben},
  year={2022},
  publisher={Oxford University Press}
}


@Inbook{Lichtenstein1977,
author="Lichtenstein, Sarah
and Fischhoff, Baruch
and Phillips, Lawrence D.",
editor="Jungermann, Helmut
and De Zeeuw, Gerard",
title="Calibration of Probabilities: The State of the Art",
bookTitle="Decision Making and Change in Human Affairs: Proceedings of the Fifth Research Conference on Subjective Probability, Utility, and Decision Making, Darmstadt, 1--4 September, 1975",
year="1977",
publisher="Springer Netherlands",
address="Dordrecht",
pages="275--324",
abstract="From the subjectivist point of view (de Finetti, 1937) a probability is a degree of belief in a proposition whose truth has not been ascertained. A probability expresses a purely internal state; there is no ``right'' or ``correct'' probability that resides somewhere ``in reality'' against which it can be compared. However, in many circumstances, it may become possible to verify the truth o{\textsterling} falsity of the proposition to which a probability was attached. Today, we assess the probability of the proposition``it will rain tomorrow''. Tomorrow, we go outside and look at the rain gauge to see whether or not it has rained. When verification is possible, we can use it to gauge the adequacy of our probability assessments.",
isbn="978-94-010-1276-8",
doi="10.1007/978-94-010-1276-8_19",
url="https://doi.org/10.1007/978-94-010-1276-8_19"
}



@article{KLAYMAN1999216,
title = {Overconfidence: It Depends on How, What, and Whom You Ask},
journal = {Organizational Behavior and Human Decision Processes},
volume = {79},
number = {3},
pages = {216-247},
year = {1999},
issn = {0749-5978},
doi = {https://doi.org/10.1006/obhd.1999.2847},
url = {https://www.sciencedirect.com/science/article/pii/S0749597899928479},
author = {Joshua Klayman and Jack B. Soll and Claudia González-Vallejo and Sema Barlas},
abstract = {Many studies have reported that the confidence people have in their judgments exceeds their accuracy and that overconfidence increases with the difficulty of the task. However, some common analyses confound systematic psychological effects with statistical effects that are inevitable if judgments are imperfect. We present three experiments using new methods to separate systematic effects from the statistically inevitable. We still find systematic differences between confidence and accuracy, including an overall bias toward overconfidence. However, these effects vary greatly with the type of judgment. There is little general overconfidence with two-choice questions and pronounced overconfidence with subjective confidence intervals. Over- and underconfidence also vary systematically with the domain of questions asked, but not as a function of difficulty. We also find stable individual differences. Determining why some people, some domains, and some types of judgments are more prone to overconfidence will be important to understanding how confidence judgments are made.}
}

@article{yeh2020completeness,
  title={On completeness-aware concept-based explanations in deep neural networks},
  author={Yeh, Chih-Kuan and Kim, Been and Arik, Sercan and Li, Chun-Liang and Pfister, Tomas and Ravikumar, Pradeep},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={20554--20565},
  year={2020}
}

@article{kingma2014adam,
  title={Adam: A method for stochastic optimization},
  author={Kingma, Diederik P and Ba, Jimmy},
  journal={arXiv preprint arXiv:1412.6980},
  year={2014}
}

@article{probBrain,
author = {Nick Chater and Jian-Qiao Zhu and Jake Spicer and Joakim Sundh and Pablo León-Villagrá and Adam Sanborn},
title ={Probabilistic Biases Meet the Bayesian Brain},
journal = {Current Directions in Psychological Science},
volume = {29},
number = {5},
pages = {506-512},
year = {2020},
doi = {10.1177/0963721420954801},

URL = { 
        https://doi.org/10.1177/0963721420954801
    
},
eprint = { 
        https://doi.org/10.1177/0963721420954801
    
}
,
    abstract = { In Bayesian cognitive science, the mind is seen as a spectacular probabilistic-inference machine. But judgment and decision-making (JDM) researchers have spent half a century uncovering how dramatically and systematically people depart from rational norms. In this article, we outline recent research that opens up the possibility of an unexpected reconciliation. The key hypothesis is that the brain neither represents nor calculates with probabilities but approximates probabilistic calculations by drawing samples from memory or mental simulation. Sampling models diverge from perfect probabilistic calculations in ways that capture many classic JDM findings, which offers the hope of an integrated explanation of classic heuristics and biases, including availability, representativeness, and anchoring and adjustment. }
}


@article{margeloiu2021concept,
  title={Do concept bottleneck models learn as intended?},
  author={Margeloiu, Andrei and Ashman, Matthew and Bhatt, Umang and Chen, Yanzhi and Jamnik, Mateja and Weller, Adrian},
  journal={arXiv preprint arXiv:2105.04289},
  year={2021}
}

@incollection{keren1990cognitive,
  title={Cognitive aids and debiasing methods: can cognitive pills cure cognitive ills?},
  author={Keren, Gideon},
  booktitle={Advances in psychology},
  volume={68},
  pages={523--552},
  year={1990},
  publisher={Elsevier}
}

@misc{histLS,
  doi = {10.48550/ARXIV.2201.11866},
  
  
  
  author = {Wei, Jerry and Torresani, Lorenzo and Wei, Jason and Hassanpour, Saeed},
  
  keywords = {Image and Video Processing (eess.IV), Computer Vision and Pattern Recognition (cs.CV), FOS: Electrical engineering, electronic engineering, information engineering, FOS: Electrical engineering, electronic engineering, information engineering, FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Calibrating Histopathology Image Classifiers using Label Smoothing},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {Creative Commons Attribution 4.0 International}
}



@techreport{WahCUB_200_2011,
	Title = {The {Caltech-UCSD Birds}-200-2011 Dataset},
	Author = {Wah, C. and Branson, S. and Welinder, P. and Perona, P. and Belongie, S.},
	Year = {2011},
	Institution = {California Institute of Technology},
	Number = {CNS-TR-2011-001}
}



@article{brundage2018malicious,
  title={The malicious use of artificial intelligence: Forecasting, prevention, and mitigation},
  author={Brundage, Miles and Avin, Shahar and Clark, Jack and Toner, Helen and Eckersley, Peter and Garfinkel, Ben and Dafoe, Allan and Scharre, Paul and Zeitzoff, Thomas and Filar, Bobby and others},
  journal={arXiv preprint arXiv:1802.07228},
  year={2018}
}

@inproceedings{ho2020denoising,
  title={Denoising diffusion probabilistic models},
  author={Ho, Jonathan and Jain, Ajay and Abbeel, Pieter},
  booktitle={NeurIPS},
  year={2020}
}

@inproceedings{sandersambiguous,
  title={Ambiguous Images With Human Judgments for Robust Visual Event Classification},
  author={Sanders, Kate and Kriz, Reno and Liu, Anqi and Van Durme, Benjamin},
booktitle={NeurIPS},
  year={2022}
}

@inproceedings{dhariwal2021diffusion,
  title={Diffusion models beat {GAN}s on image synthesis},
  author={Dhariwal, Prafulla and Nichol, Alexander},
  booktitle={NeurIPS},
  year={2021}
}

@article{gruber2018perceptual,
  title={Perceptual dominance in brief presentations of mixed images: Human perception vs. deep neural networks},
  author={Gruber, Liron Z and Haruvi, Aia and Basri, Ronen and Irani, Michal},
  journal={Frontiers in Computational Neuroscience},
  volume={12},
  pages={57},
  year={2018},
  publisher={Frontiers Media SA}
}


@article{tong2006neural,
  title={Neural bases of binocular rivalry},
  author={Tong, Frank and Meng, Ming and Blake, Randolph},
  journal={Trends in cognitive sciences},
  volume={10},
  number={11},
  pages={502--511},
  year={2006},
  publisher={Elsevier}
}


@article{blake2002visual,
  title={Visual competition},
  author={Blake, Randolph and Logothetis, Nikos K},
  journal={Nature Reviews Neuroscience},
  volume={3},
  number={1},
  pages={13--21},
  year={2002},
  publisher={Nature Publishing Group}
}


@article{hybridImages,
author = {Oliva, Aude and Torralba, Antonio and Schyns, Philippe G.},
title = {Hybrid Images},
year = {2006},
volume = {25},
number = {3},
journal = {ACM Transactions on Graphics},
pages = {527–532},
numpages = {6}
}

  




@article{humanStatisticians,
title = {Are humans good intuitive statisticians after all? Rethinking some conclusions from the literature on judgment under uncertainty},
journal = {Cognition},
volume = {58},
number = {1},
pages = {1-73},
year = {1996},
issn = {0010-0277},
doi = {https://doi.org/10.1016/0010-0277(95)00664-8},

author = {Leda Cosmides and John Tooby},
abstract = {Professional probabilists have long argued over what probability means, with, for example, Bayesians arguing that probabilities refer to subjective degrees of confidence and frequentists arguing that probabilities refer to the frequencies of events in the world. Recently, Gigerenzer and his colleagues have argued that these same distinctions are made by untutored subjects, and that, for many domains, the human mind represents probabilistic information as frequencies. We analyze several reasons why, from an ecological and evolutionary perspective, certain classes of problem-solving mechanisms in the human mind should be expected to represent probabilistic information as frequencies. Then, using a problem famous in the “heuristics and biases” literature for eliciting base rate neglect, we show that correct Bayesian reasoning can be elicited in 76% of subjects - indeed, 92% in the most ecologically valid condition - simply by expressing the problem in frequentist terms. This result adds to the growing body of literature showing that frequentist representations cause various cognitive biases to disappear, including overconfidence, the conjunction fallacy, and base-rate neglect. Taken together, these new findings indicate that the conclusion most common in the literature on judgment under uncertainty - that our inductive reasoning mechanisms do not embody a calculus of probability - will have to be re-examined. From an ecological and evolutionary perspective, humans may turn out to be good intuitive statisticians after all.}
}

@article{zhang2021delvingLS,
  title={Delving deep into label smoothing},
  author={Zhang, Chang-Bin and Jiang, Peng-Tao and Hou, Qibin and Wei, Yunchao and Han, Qi and Li, Zhen and Cheng, Ming-Ming},
  journal={IEEE Transactions on Image Processing},
  volume={30},
  pages={5984--5996},
  year={2021},
  publisher={IEEE}
}


@article{lackofuncertaintyCV,
  author    = {Matias Valdenegro{-}Toro},
  title     = {I Find Your Lack of Uncertainty in Computer Vision Disturbing},
  journal   = {CoRR},
  volume    = {abs/2104.08188},
  year      = {2021},
  
  eprinttype = {arXiv},
  eprint    = {2104.08188},
  timestamp = {Mon, 19 Apr 2021 16:45:47 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2104-08188.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}



    @inproceedings{labelRedundancy,
    author = {Shimizu, Ayame and Wakabayashi, Kei},
    title = {Examining Effect of Label Redundancy for Machine Learning Using Crowdsourcing},
    year = {2021},
    isbn = {9781450f395564},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    
    doi = {10.1145/3487664.3487677},
    abstract = {Crowdsourcing is widely used in making annotated examples for supervised learning, but these labels from workers are often noisy. A practical solution for this problem is to collect multiple labels per examples and aggregate them. Although gathering repeated labels improves data quality, concerning the fact that there’s a limit on the budget we can use for crowdsourcing, it can also lead to decreasing the amount of training data. This paper empirically examines to what extent the repeated labeling contribute to the accuracy of machine learning models for image classification and text information extraction in various conditions varying budgets and worker qualities. Experiments are designed to test 4 hypotheses we made each determining the effect of budgets, worker quality, task difficulty, and redundancy on crowdsourcing. The experimental results on image classification and named entity recognition supported all 4 hypotheses and surprisingly suggests, that the repeated labeling almost always brings a negative impact on machine learning accuracy.},
    booktitle = {The 23rd International Conference on Information Integration and Web Intelligence},
    pages = {87–94},
    numpages = {8},
    keywords = {Crowdsourcing, Supervised labeling, Multiple labeling},
    location = {Linz, Austria},
    series = {iiWAS2021}
    }

      




@article{goodfellow2014explaining,
  title={Explaining and harnessing adversarial examples},
  author={Goodfellow, Ian J and Shlens, Jonathon and Szegedy, Christian},
  journal={arXiv preprint arXiv:1412.6572},
  year={2014}
}

@inproceedings{selfCiteSoftLabel,
  title={Eliciting and Learning with Soft Labels from Every Annotator},
  author={Collins, Katherine M and Bhatt, Umang and Weller, Adrian},
  booktitle={HCOMP},
  year={2022}
}


@article{chen2022perspectives,
  title={Perspectives on Incorporating Expert Feedback into Model Updates},
  author={Chen, Valerie and Bhatt, Umang and Heidari, Hoda and Weller, Adrian and Talwalkar, Ameet},
  journal={arXiv preprint arXiv:2205.06905},
  year={2022}
}



@article{kurakin2016adversarial,
  title={Adversarial machine learning at scale},
  author={Kurakin, Alexey and Goodfellow, Ian and Bengio, Samy},
  journal={arXiv preprint arXiv:1611.01236},
  year={2016}
}

@inproceedings{he2016deep,
  title={Deep residual learning for image recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={CVPR},
  pages={770--778},
  year={2016}
}



@inproceedings{diaz2022crowdworksheets,
  title={CrowdWorkSheets: Accounting for Individual and Collective Identities Underlying Crowdsourced Dataset Annotation},
  author={D{\'\i}az, Mark and Kivlichan, Ian and Rosen, Rachel and Baker, Dylan and Amironesei, Razvan and Prabhakaran, Vinodkumar and Denton, Emily},
  booktitle={FAccT},
  year={2022}
}

@article{palan2018prolific,
  title={Prolific. ac—A subject pool for online experiments},
  author={Palan, Stefan and Schitter, Christian},
  journal={Journal of Behavioral and Experimental Finance},
  volume={17},
  pages={22--27},
  year={2018},
  publisher={Elsevier}
}
@article{hinton2012improving,
  title={Improving neural networks by preventing co-adaptation of feature detectors},
  author={Hinton, Geoffrey E and Srivastava, Nitish and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan R},
  journal={arXiv preprint arXiv:1207.0580},
  year={2012}
}

@article{folstein2013category,
  title={Category learning increases discriminability of relevant object dimensions in visual cortex},
  author={Folstein, Jonathan R and Palmeri, Thomas J and Gauthier, Isabel},
  journal={Cerebral Cortex},
  volume={23},
  number={4},
  pages={814--823},
  year={2013},
  publisher={Oxford University Press}
}

@incollection{harnad2003categorical,
  title={Categorical Perception},
  author={Harnad, Stevan},
  booktitle={Encyclopedia of Cognitive Science},
  volume={67},
  number={4},
  year={2003},
  publisher={MacMillan: Nature Publishing Group}
}


@article{shrager2019cancer,
  title={Is cancer solvable? Towards efficient and ethical biomedical science},
  author={Shrager, Jeff and Shapiro, Mark and Hoos, William},
  journal={The Journal of Law, Medicine \& Ethics},
  volume={47},
  number={3},
  pages={362--368},
  year={2019},
  publisher={SAGE Publications Sage CA: Los Angeles, CA}
}



@article{xu2020computation,
  title={Computation-efficient knowledge distillation via uncertainty-aware mixup},
  author={Xu, Guodong and Liu, Ziwei and Loy, Chen Change},
  journal={arXiv preprint arXiv:2012.09413},
  year={2020}
}

@article{pinto2021mix,
  title={Mix-MaxEnt: improving accuracy and uncertainty estimates of deterministic neural networks},
  author={Pinto, Francesco and Yang, Harry and Lim, Ser-Nam and Torr, Philip HS and Dokania, Puneet K},
  year={2021},
  publisher={Neural Information Processing Systems Foundation}
}

@article{vodrahalli2022uncalibrated,
  title={Uncalibrated Models Can Improve Human-AI Collaboration},
  author={Vodrahalli, Kailas and Gerstenberg, Tobias and Zou, James},
  journal={arXiv preprint arXiv:2202.05983},
  year={2022}
}


@article{vodrahalli2021humans,
  title={Do Humans Trust Advice More if it Comes from {AI}? An Analysis of Human-AI Interactions},
  author={Vodrahalli, Kailas and Gerstenberg, Tobias and Zou, James},
  journal={arXiv preprint arXiv:2107.07015},
  year={2021}
}


@article{feldmanCategoricalPerception,
author = {Jacob Feldman},
title ={Mutual Information and Categorical Perception},
journal = {Psychological Science},
volume = {32},
number = {8},
pages = {1298-1310},
year = {2021},
}


@article{morphSpacesShape,
title = {Shape discrimination along morph-spaces},
journal = {Vision Research},
volume = {158},
pages = {189-199},
year = {2019},
author = {Nathan Destler and Manish Singh and Jacob Feldman},
}




@article{perceptualMagnetStats,
  title={The influence of categories on perception: explaining the perceptual magnet effect as optimal statistical inference.},
  author={Feldman, Naomi H and Griffiths, Thomas L and Morgan, James L},
  journal={Psychological review},
  volume={116},
  number={4},
  pages={752},
  year={2009},
  publisher={American Psychological Association}
}

@INPROCEEDINGS {cutMix,
author = {S. Yun and D. Han and S. Chun and S. Oh and Y. Yoo and J. Choe},
booktitle = {ICCV},
title = {CutMix: Regularization Strategy to Train Strong Classifiers With Localizable Features},
year = {2019}
}


@article{perceptionAlignment,
  title={Exploring Alignment of Representations with Human Perception},
  author={Nanda, Vedant and Majumdar, Ayan and Kolling, Camila and Dickerson, John P and Gummadi, Krishna P and Love, Bradley C and Weller, Adrian},
  journal={arXiv preprint arXiv:2111.14726},
  year={2021}
}

@inproceedings{bcLearning,
  title={Between-class learning for image classification},
  author={Tokozume, Yuji and Ushiku, Yoshitaka and Harada, Tatsuya},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={5486--5494},
  year={2018}
}

@inproceedings{goodfellowAdversarial,
title	= {Explaining and Harnessing Adversarial Examples},
author	= {Ian Goodfellow and Jonathon Shlens and Christian Szegedy},
year	= {2015},
booktitle	= {ICLR}
}


@article{zhou2023navigating,
  title={Navigating the Grey Area: Expressions of Overconfidence and Uncertainty in Language Models},
  author={Zhou, Kaitlyn and Jurafsky, Dan and Hashimoto, Tatsunori},
  journal={arXiv preprint arXiv:2302.13439},
  year={2023}
}


@misc{madryAdversarial,
  doi = {10.48550/ARXIV.1706.06083},
  
  
  
  author = {Madry, Aleksander and Makelov, Aleksandar and Schmidt, Ludwig and Tsipras, Dimitris and Vladu, Adrian},
  
  keywords = {Machine Learning (stat.ML), Machine Learning (cs.LG), Neural and Evolutionary Computing (cs.NE), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Towards Deep Learning Models Resistant to Adversarial Attacks},
  
  publisher = {arXiv},
  
  year = {2017},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}




@inproceedings{mixupRobustness,
  title={How Does Mixup Help With Robustness and Generalization?},
  author={Zhang, Linjun and Deng, Zhun and Kawaguchi, Kenji and Ghorbani, Amirata and Zou, James},
  booktitle={ICLR},
  year={2020}
}




@article{counterfactuallyAugmentedHuman,
  title={Learning the difference that makes a difference with counterfactually-augmented data},
  author={Kaushik, Divyansh and Hovy, Eduard and Lipton, Zachary C},
  journal={arXiv preprint arXiv:1909.12434},
  year={2019}
}
  
  eprinttype = {arXiv},
  eprint    = {1909.12434},
  timestamp = {Mon, 06 Jan 2020 07:44:53 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1909-12434.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{schneider2022effects,
  title={The effects of communicating scientific uncertainty on trust and decision making in a public health context},
  author={Schneider, Claudia R and Freeman, Alexandra LJ and Spiegelhalter, David and van der Linden, Sander},
  journal={Judgment and Decision Making},
  volume={17},
  number={4},
  pages={849--882},
  year={2022},
  publisher={Cambridge University Press}
}


@misc{informativenessSoft,
  doi = {10.48550/ARXIV.2211.01407},
  
  url = {https://arxiv.org/abs/2211.01407},
  
  author = {Sucholutsky, Ilia and Marjieh, Raja and Jacoby, Nori and Griffiths, Thomas L.},
  
  keywords = {Machine Learning (cs.LG), Artificial Intelligence (cs.AI), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {On the Informativeness of Supervision Signals},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@article{dvijotham2022enhancing,
  title={Enhancing the reliability and accuracy of AI-enabled diagnosis via complementarity-driven deferral to clinicians (CoDoC)},
  author={Dvijotham, Krishnamurthy and Winkens, Jim and Barsbey, Melih and Ghaisas, Sumedh and Pawlowski, Nick and Stanforth, Robert and MacWilliams, Patricia and Ahmed, Zahra and Azizi, Shekoofeh and Bachrach, Yoram and others},
  year={2022}
}


@misc{conceptSidecar,
  doi = {10.48550/ARXIV.2211.11690},
  
  url = {https://arxiv.org/abs/2211.11690},
  
  author = {Lockhart, Joshua and Magazzeni, Daniele and Veloso, Manuela},
  
  keywords = {Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Learn to explain yourself, when you can: Equipping Concept Bottleneck Models with the ability to abstain on their concept predictions},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@misc{hillmixup,
  doi = {10.48550/ARXIV.2211.01202},
  
  url = {https://arxiv.org/abs/2211.01202},
  
  author = {Collins, Katherine M. and Bhatt, Umang and Liu, Weiyang and Piratla, Vihari and Sucholutsky, Ilia and Love, Bradley and Weller, Adrian},
  
  keywords = {Machine Learning (cs.LG), Computer Vision and Pattern Recognition (cs.CV), Human-Computer Interaction (cs.HC), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Human-in-the-Loop Mixup},
  
  publisher = {arXiv},
  
  year = {2023},
  
  copyright = {Creative Commons Attribution 4.0 International}
}


@misc{decoupledmixup, 
  doi = {10.48550/ARXIV.2203.10761},
  
  
  
  author = {Liu, Zicheng and Li, Siyuan and Wang, Ge and Tan, Cheng and Wu, Lirong and Li, Stan Z.},
  
  keywords = {Machine Learning (cs.LG), Artificial Intelligence (cs.AI), Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Decoupled Mixup for Data-efficient Learning},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}



@article{propMix,
  author    = {Filipe R. Cordeiro and
               Vasileios Belagiannis and
               Ian D. Reid and
               Gustavo Carneiro},
  title     = {PropMix: Hard Sample Filtering and Proportional MixUp for Learning
               with Noisy Labels},
  journal   = {CoRR},
  volume    = {abs/2110.11809},
  year      = {2021},
  
  eprinttype = {arXiv},
  eprint    = {2110.11809},
  timestamp = {Thu, 28 Oct 2021 15:25:31 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2110-11809.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc{gp_slides,
  author={Rasmussen, Carl Edward},
  title = {Posterior Gaussian Process (Lecture Slides)},
  howpublished = {\url{https://www.vle.cam.ac.uk/pluginfile.php/18575811/mod_resource/content/1/gp\%20and\%20data.pdf}},
  year = {2016},
  note = {Last Accessed: {2021–03-11}}
}

@misc{ustyuzhaninov2020compositional,
      title={Compositional uncertainty in deep Gaussian processes}, 
      author={Ivan Ustyuzhaninov and Ieva Kazlauskaite and Markus Kaiser and Erik Bodin and Neill D. F. Campbell and Carl Henrik Ek},
      year={2020},
      eprint={1909.07698},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}


@InProceedings{pmlr-v31-damianou13a,
  title = 	 {Deep {G}aussian Processes},
  author = 	 {Damianou, Andreas and Lawrence, Neil D.},
  booktitle = 	 {Proceedings of the Sixteenth International Conference on Artificial Intelligence and Statistics},
  pages = 	 {207--215},
  year = 	 {2013},
  editor = 	 {Carvalho, Carlos M. and Ravikumar, Pradeep},
  volume = 	 {31},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Scottsdale, Arizona, USA},
  month = 	 {29 Apr--01 May},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v31/damianou13a.pdf},
  
  abstract = 	 {In this paper we introduce deep Gaussian process (GP) models. Deep GPs are a deep belief network based on Gaussian process mappings. The data is modeled as the output of a multivariate GP. The inputs to that Gaussian process are then governed by another GP. A single layer model is equivalent to a standard GP or the GP latent variable model (GP-LVM). We perform inference in the model by approximate variational marginalization. This results in a strict lower bound on the marginal likelihood of the model which we use for model selection (number of layers and nodes per layer). Deep belief networks are typically applied to relatively large data sets using stochastic gradient descent for optimization. Our fully Bayesian treatment allows for the application of deep models even when data is scarce. Model selection by our variational bound shows that a five layer hierarchy is justified even when modelling a digit data set containing only 150 examples.}
}

@misc{hutchinson2021vectorvalued,
      title={Vector-valued Gaussian Processes on Riemannian Manifolds via Gauge Independent Projected Kernels}, 
      author={Michael Hutchinson and Alexander Terenin and Viacheslav Borovitskiy and So Takao and Yee Whye Teh and Marc Peter Deisenroth},
      year={2021},
      eprint={2110.14423},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}


@InProceedings{pmlr-v139-hutchinson21a,
  title = 	 {LieTransformer: Equivariant Self-Attention for Lie Groups},
  author =       {Hutchinson, Michael J and Lan, Charline Le and Zaidi, Sheheryar and Dupont, Emilien and Teh, Yee Whye and Kim, Hyunjik},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {4533--4543},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/hutchinson21a/hutchinson21a.pdf},
  
  abstract = 	 {Group equivariant neural networks are used as building blocks of group invariant neural networks, which have been shown to improve generalisation performance and data efficiency through principled parameter sharing. Such works have mostly focused on group equivariant convolutions, building on the result that group equivariant linear maps are necessarily convolutions. In this work, we extend the scope of the literature to self-attention, that is emerging as a prominent building block of deep learning models. We propose the LieTransformer, an architecture composed of LieSelfAttention layers that are equivariant to arbitrary Lie groups and their discrete subgroups. We demonstrate the generality of our approach by showing experimental results that are competitive to baseline methods on a wide range of tasks: shape counting on point clouds, molecular property regression and modelling particle trajectories under Hamiltonian dynamics.}
}

@article{ullman2016atoms,
  title={Atoms of recognition in human and computer vision},
  author={Ullman, Shimon and Assif, Liav and Fetaya, Ethan and Harari, Daniel},
  journal={Proceedings of the National Academy of Sciences},
  volume={113},
  number={10},
  pages={2744--2749},
  year={2016},
  publisher={National Acad Sciences}
}


@InProceedings{pmlr-v139-holderrieth21a,
  title = 	 {Equivariant Learning of Stochastic Fields: Gaussian Processes and Steerable Conditional Neural Processes},
  author =       {Holderrieth, Peter and Hutchinson, Michael J and Teh, Yee Whye},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {4297--4307},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/holderrieth21a/holderrieth21a.pdf},
  
  abstract = 	 {Motivated by objects such as electric fields or fluid streams, we study the problem of learning stochastic fields, i.e. stochastic processes whose samples are fields like those occurring in physics and engineering. Considering general transformations such as rotations and reflections, we show that spatial invariance of stochastic fields requires an inference model to be equivariant. Leveraging recent advances from the equivariance literature, we study equivariance in two classes of models. Firstly, we fully characterise equivariant Gaussian processes. Secondly, we introduce Steerable Conditional Neural Processes (SteerCNPs), a new, fully equivariant member of the Neural Process family. In experiments with Gaussian process vector fields, images, and real-world weather data, we observe that SteerCNPs significantly improve the performance of previous models and equivariance leads to improvements in transfer learning tasks.}
}

@article{newell2002categorical,
  title={Categorical perception of familiar objects},
  author={Newell, Fiona N and B{\"u}lthoff, Heinrich H},
  journal={Cognition},
  volume={85},
  number={2},
  pages={113--143},
  year={2002},
  publisher={Elsevier}
}

@inproceedings{naeini2015obtaining,
  title={Obtaining well calibrated probabilities using bayesian binning},
  author={Naeini, Mahdi Pakdaman and Cooper, Gregory and Hauskrecht, Milos},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={29},
  number={1},
  year={2015}
}


@article{folsteinFactorizedvBlended,
  title={How category learning affects object representations: not all morphspaces stretch alike.},
  author={Jonathan R. Folstein and Isabel Gauthier and Thomas J. Palmeri},
  journal={Journal of experimental psychology. Learning, memory, and cognition},
  year={2012},
  volume={38 4},
  pages={
          807-20
        }
}

@misc{miao2021intelvaes,
      title={InteL-VAEs: Adding Inductive Biases to Variational Auto-Encoders via Intermediary Latents}, 
      author={Ning Miao and Emile Mathieu and N. Siddharth and Yee Whye Teh and Tom Rainforth},
      year={2021},
      eprint={2106.13746},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@misc{chau2021bayesimp,
      title={BayesIMP: Uncertainty Quantification for Causal Data Fusion}, 
      author={Siu Lun Chau and Jean-François Ton and Javier González and Yee Whye Teh and Dino Sejdinovic},
      year={2021},
      eprint={2106.03477},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@misc{bloemreddy2020probabilistic,
      title={Probabilistic symmetries and invariant neural networks}, 
      author={Benjamin Bloem-Reddy and Yee Whye Teh},
      year={2020},
      eprint={1901.06082},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@misc{mikheeva2021aligned,
      title={Aligned Multi-Task Gaussian Process}, 
      author={Olga Mikheeva and Ieva Kazlauskaite and Adam Hartshorne and Hedvig Kjellström and Carl Henrik Ek and Neill D. F. Campbell},
      year={2021},
      eprint={2110.15761},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}


@InProceedings{pmlr-v108-ustyuzhaninov20a,
  title = 	 {Monotonic Gaussian Process Flows},
  author =       {Ustyuzhaninov, Ivan and Kazlauskaite, Ieva and Ek, Carl Henrik and Campbell, Neill},
  booktitle = 	 {Proceedings of the Twenty Third International Conference on Artificial Intelligence and Statistics},
  pages = 	 {3057--3067},
  year = 	 {2020},
  editor = 	 {Chiappa, Silvia and Calandra, Roberto},
  volume = 	 {108},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {26--28 Aug},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v108/ustyuzhaninov20a/ustyuzhaninov20a.pdf},
  
  abstract = 	 {We propose a new framework for imposing monotonicity constraints in a Bayesian non-parametric setting based on numerical solutions of stochastic differential equations. We derive a nonparametric model of monotonic functions that allows for interpretable priors and principled quantification of hierarchical uncertainty. We demonstrate the efficacy of the proposed model by providing competitive results to other probabilistic monotonic models on a number of benchmark functions. In addition, we consider the utility of a monotonic random process as a part of a hierarchical probabilistic model; we examine the task of temporal alignment of time-series data where it is beneficial to use a monotonic random process in order to preserve the uncertainty in the temporal warpings. }
}

@misc{le2021hybrid,
      title={Hybrid Memoised Wake-Sleep: Approximate Inference at the Discrete-Continuous Interface}, 
      author={Tuan Anh Le and Katherine M. Collins and Luke Hewitt and Kevin Ellis and Siddharth N and Samuel J. Gershman and Joshua B. Tenenbaum},
      year={2021},
      eprint={2107.06393},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@article{CAMACHO20181581,
title = {Next-Generation Machine Learning for Biological Networks},
journal = {Cell},
volume = {173},
number = {7},
pages = {1581-1592},
year = {2018},
issn = {0092-8674},
doi = {https://doi.org/10.1016/j.cell.2018.05.015},

author = {Diogo M. Camacho and Katherine M. Collins and Rani K. Powers and James C. Costello and James J. Collins},
keywords = {Machine leaning, deep learning, systems biology, synthetic biology, network biology, neural networks},
abstract = {Machine learning, a collection of data-analytical techniques aimed at building predictive models from multi-dimensional datasets, is becoming integral to modern biological research. By enabling one to generate models that learn from large datasets and make predictions on likely outcomes, machine learning can be used to study complex cellular systems such as biological networks. Here, we provide a primer on machine learning for life scientists, including an introduction to deep learning. We discuss opportunities and challenges at the intersection of machine learning and network biology, which could impact disease biology, drug discovery, microbiome research, and synthetic biology.}
}

@article{du2021learning,
  title={Learning Signal-Agnostic Implicit Manifolds},
  author={Du, Yilun and Collins, Katherine and Tenenbaum, Josh and Sitzmann, Vincent},
  journal={NeurIPS},
  volume={34},
  year={2021}
}


@InProceedings{pmlr-v119-bodin20a,
  title = 	 {Modulating Surrogates for {B}ayesian Optimization},
  author =       {Bodin, Erik and Kaiser, Markus and Kazlauskaite, Ieva and Dai, Zhenwen and Campbell, Neill and Ek, Carl Henrik},
  booktitle = 	 {Proceedings of the 37th International Conference on Machine Learning},
  pages = 	 {970--979},
  year = 	 {2020},
  editor = 	 {III, Hal Daumé and Singh, Aarti},
  volume = 	 {119},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {13--18 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v119/bodin20a/bodin20a.pdf},
  
  abstract = 	 {Bayesian optimization (BO) methods often rely on the assumption that the objective function is well-behaved, but in practice, this is seldom true for real-world objectives even if noise-free observations can be collected. Common approaches, which try to model the objective as precisely as possible, often fail to make progress by spending too many evaluations modeling irrelevant details. We address this issue by proposing surrogate models that focus on the well-behaved structure in the objective function, which is informative for search, while ignoring detrimental structure that is challenging to model from few observations. First, we demonstrate that surrogate models with appropriate noise distributions can absorb challenging structures in the objective function by treating them as irreducible uncertainty. Secondly, we show that a latent Gaussian process is an excellent surrogate for this purpose, comparing with Gaussian processes with standard noise distributions. We perform numerous experiments on a range of BO benchmarks and find that our approach improves reliability and performance when faced with challenging objective functions.}
}

@misc{cutajar2019deep,
      title={Deep Gaussian Processes for Multi-fidelity Modeling}, 
      author={Kurt Cutajar and Mark Pullin and Andreas Damianou and Neil Lawrence and Javier González},
      year={2019},
      eprint={1903.07320},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@misc{lawrence2019data,
      title={Data Science and Digital Systems: The 3Ds of Machine Learning Systems Design}, 
      author={Neil D. Lawrence},
      year={2019},
      eprint={1903.11241},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{paleyes2021emulation,
      title={Emulation of physical processes with Emukit}, 
      author={Andrei Paleyes and Mark Pullin and Maren Mahsereci and Cliff McCollum and Neil D. Lawrence and Javier Gonzalez},
      year={2021},
      eprint={2110.13293},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{DBLP:journals/corr/LakeLT16,
  author    = {Brenden M. Lake and
               Neil D. Lawrence and
               Joshua B. Tenenbaum},
  title     = {The Emergence of Organizing Structure in Conceptual Representation},
  journal   = {CoRR},
  volume    = {abs/1611.09384},
  year      = {2016},
  
  eprinttype = {arXiv},
  eprint    = {1611.09384},
  timestamp = {Mon, 13 Aug 2018 16:49:16 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/LakeLT16.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc{niu2018intrinsic,
      title={Intrinsic Gaussian processes on complex constrained domains}, 
      author={Mu Niu and Pokman Cheung and Lizhen Lin and Zhenwen Dai and Neil Lawrence and David Dunson},
      year={2018},
      eprint={1801.01061},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@misc{flennerhag2019transferring,
      title={Transferring Knowledge across Learning Processes}, 
      author={Sebastian Flennerhag and Pablo G. Moreno and Neil D. Lawrence and Andreas Damianou},
      year={2019},
      eprint={1812.01054},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{DBLP:journals/corr/abs-2011-07586,
  author    = {Umang Bhatt and
               Yunfeng Zhang and
               Javier Antor{\'{a}}n and
               Q. Vera Liao and
               Prasanna Sattigeri and
               Riccardo Fogliato and
               Gabrielle Gauthier Melan{\c{c}}on and
               Ranganath Krishnan and
               Jason Stanley and
               Omesh Tickoo and
               Lama Nachman and
               Rumi Chunara and
               Adrian Weller and
               Alice Xiang},
  title     = {Uncertainty as a Form of Transparency: Measuring, Communicating, and
               Using Uncertainty},
  journal   = {CoRR},
  volume    = {abs/2011.07586},
  year      = {2020},
  
  eprinttype = {arXiv},
  eprint    = {2011.07586},
  timestamp = {Wed, 18 Nov 2020 16:48:35 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2011-07586.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@ARTICLE{Kennedy00bayesiancalibration,
    author = {Marc C. Kennedy and Anthony O'Hagan},
    title = {Bayesian Calibration of Computer Models},
    journal = {Journal of the Royal Statistical Society, Series B, Methodological},
    year = {2000},
    volume = {63},
    pages = {425--464}
}

@misc{augMix,
  doi = {10.48550/ARXIV.1912.02781},
  
  
  
  author = {Hendrycks, Dan and Mu, Norman and Cubuk, Ekin D. and Zoph, Barret and Gilmer, Justin and Lakshminarayanan, Balaji},
  
  keywords = {Machine Learning (stat.ML), Computer Vision and Pattern Recognition (cs.CV), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {AugMix: A Simple Data Processing Method to Improve Robustness and Uncertainty},
  
  publisher = {arXiv},
  
  year = {2019},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@inproceedings{disagreement,
author = {Gordon, Mitchell L. and Zhou, Kaitlyn and Patel, Kayur and Hashimoto, Tatsunori and Bernstein, Michael S.},
title = {The Disagreement Deconvolution: Bringing Machine Learning Performance Metrics In Line With Reality},
year = {2021},
isbn = {9781450380966},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},

doi = {10.1145/3411764.3445423},
abstract = { Machine learning classifiers for human-facing tasks such as comment toxicity and misinformation often score highly on metrics such as ROC AUC but are received poorly in practice. Why this gap? Today, metrics such as ROC AUC, precision, and recall are used to measure technical performance; however, human-computer interaction observes that evaluation of human-facing systems should account for people’s reactions to the system. In this paper, we introduce a transformation that more closely aligns machine learning classification metrics with the values and methods of user-facing performance measures. The disagreement deconvolution takes in any multi-annotator (e.g., crowdsourced) dataset, disentangles stable opinions from noise by estimating intra-annotator consistency, and compares each test set prediction to the individual stable opinions from each annotator. Applying the disagreement deconvolution to existing social computing datasets, we find that current metrics dramatically overstate the performance of many human-facing machine learning tasks: for example, performance on a comment toxicity task is corrected from .95 to .73 ROC AUC. },
booktitle = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems},
articleno = {388},
numpages = {14},
location = {Yokohama, Japan},
series = {CHI '21}
}

@article{Gha15,
  added-at = {2018-02-28T15:15:47.000+0100},
  author = {Ghahramani, Zoubin},
  
  ee = {http://dx.doi.org/10.1038/nature14541},
  interhash = {5b6101000ce961c382a70c8bfa25a610},
  intrahash = {3ea1936269f6deb2eb1c5e5aab55e349},
  journal = {Nature},
  keywords = {bayes machine_learning overview},
  number = 7553,
  pages = {452-459},
  timestamp = {2018-02-28T15:16:04.000+0100},
  title = {Probabilistic machine learning and artificial intelligence.},
  
  volume = 521,
  year = 2015
}

@article{lake_ullman_tenenbaum_gershman_2017, title={Building machines that learn and think like people}, volume={40}, DOI={10.1017/S0140525X16001837}, journal={Behavioral and Brain Sciences}, publisher={Cambridge University Press}, author={Lake, Brenden M. and Ullman, Tomer D. and Tenenbaum, Joshua B. and Gershman, Samuel J.}, year={2017}, pages={e253}}

@book{uncertainJudgments,
           title = {Uncertain Judgements: Eliciting Expert Probabilities},
          author = {A. O'Hagan and C. E. Buck and A. Daneshkhah and J. R. Eiser and P. H. Garthwaite and D. J. Jenkinson and J. E. Oakley and T. Rakow},
         address = {Chichester},
       publisher = {John Wiley},
            year = {2006},
             
        abstract = {Elicitation is the process of extracting expert knowledge about some unknown quantity or quantities, and formulating that information as a probability distribution. Elicitation is important in situations, such as modelling the safety of nuclear installations or assessing the risk of terrorist attacks, where expert knowledge is essentially the only source of good information. It also plays a major role in other contexts by augmenting scarce observational data, through the use of Bayesian statistical methods. However, elicitation is not a simple task, and practitioners need to be aware of a wide range of research findings in order to elicit expert judgements accurately and reliably. Uncertain Judgements introduces the area, before guiding the reader through the study of appropriate elicitation methods, illustrated by a variety of multi-disciplinary examples.}
}

@article{fairmixup,
  author    = {Ching{-}Yao Chuang and
               Youssef Mroueh},
  title     = {Fair Mixup: Fairness via Interpolation},
  journal   = {CoRR},
  volume    = {abs/2103.06503},
  year      = {2021},
  
  eprinttype = {arXiv},
  eprint    = {2103.06503},
  timestamp = {Tue, 16 Mar 2021 11:26:59 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2103-06503.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@inproceedings{medImageDatasetGAN,
author = {Zong Fan and Varun Kelkar and Mark A. Anastasio and Hua Li},
title = {{Application of DatasetGAN in medical imaging: preliminary studies}},
volume = {12032},
booktitle = {Medical Imaging 2022: Image Processing},
editor = {Olivier Colliot and Ivana Išgum},
organization = {International Society for Optics and Photonics},
publisher = {SPIE},
pages = {452 -- 458},
keywords = {Deep learning, GAN, DatasetGAN, Medical imaging analysis, Image segmentation},
year = {2022},
doi = {10.1117/12.2611191},
}

@article{griffithsDiffusion,
  title={Analyzing Diffusion as Serial Reproduction},
  author={Marjieh, Raja and Sucholutsky, Ilia and Langlois, Thomas A and Jacoby, Nori and Griffiths, Thomas L},
  journal={arXiv preprint arXiv:2209.14821},
  year={2022}
}


@InProceedings{chandra2022designing,
  title = {Designing Perceptual Puzzles by Differentiating Probabilistic Programs},
  author = {Kartik Chandra and Tzu-Mao Li and Joshua Tenenbaum and Jonathan Ragan-Kelley},
  booktitle = {SIGGRAPH},
  year = {2022}
}

@article{simsSyntheticData,
title = {Next-generation deep learning based on simulators and synthetic data},
journal = {Trends in Cognitive Sciences},
volume = {26},
number = {2},
pages = {174-187},
year = {2022},
author = {Celso M. {de Melo} and Antonio Torralba and Leonidas Guibas and James DiCarlo and Rama Chellappa and Jessica Hodgins},
}


@InProceedings{simclr,
  title = 	 {A Simple Framework for Contrastive Learning of Visual Representations},
  author =       {Chen, Ting and Kornblith, Simon and Norouzi, Mohammad and Hinton, Geoffrey},
  booktitle = 	 {Proceedings of the 37th International Conference on Machine Learning},
  pages = 	 {1597--1607},
  year = 	 {2020},
  editor = 	 {III, Hal Daumé and Singh, Aarti},
  volume = 	 {119},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {13--18 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v119/chen20j/chen20j.pdf},
  
  abstract = 	 {This paper presents SimCLR: a simple framework for contrastive learning of visual representations. We simplify recently proposed contrastive self-supervised learning algorithms without requiring specialized architectures or a memory bank. In order to understand what enables the contrastive prediction tasks to learn useful representations, we systematically study the major components of our framework. We show that (1) composition of data augmentations plays a critical role in defining effective predictive tasks, (2) introducing a learnable nonlinear transformation between the representation and the contrastive loss substantially improves the quality of the learned representations, and (3) contrastive learning benefits from larger batch sizes and more training steps compared to supervised learning. By combining these findings, we are able to considerably outperform previous methods for self-supervised and semi-supervised learning on ImageNet. A linear classifier trained on self-supervised representations learned by SimCLR achieves 76.5% top-1 accuracy, which is a 7% relative improvement over previous state-of-the-art, matching the performance of a supervised ResNet-50. When fine-tuned on only 1% of the labels, we achieve 85.8% top-5 accuracy, outperforming AlexNet with 100X fewer labels.}
}




@article{distillMixup,
  author    = {Dongdong Wang and
               Yandong Li and
               Liqiang Wang and
               Boqing Gong},
  title     = {Neural Networks Are More Productive Teachers Than Human Raters: Active
               Mixup for Data-Efficient Knowledge Distillation from a Blackbox Model},
  journal   = {CoRR},
  volume    = {abs/2003.13960},
  year      = {2020},
  
  eprinttype = {arXiv},
  eprint    = {2003.13960},
  timestamp = {Thu, 14 Oct 2021 09:15:56 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2003-13960.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@article{cifar10,
  title={Learning multiple layers of features from tiny images},
  author={Krizhevsky, Alex and others},
  year={2009}
}

@article{doi:10.1080/00031305.2018.1529623,
author = {Naomi C. Brownstein and Thomas A. Louis and Anthony O’Hagan and Jane Pendergast},
title = {The Role of Expert Judgment in Statistical Inference and Evidence-Based Decision-Making},
journal = {The American Statistician},
volume = {73},
number = {sup1},
pages = {56-68},
year  = {2019},
publisher = {Taylor & Francis},
doi = {10.1080/00031305.2018.1529623},
    note ={PMID: 31057338},

eprint = { 
        https://doi.org/10.1080/00031305.2018.1529623
    
}

}


@article{10.1371/journal.pone.0229132,
    doi = {10.1371/journal.pone.0229132},
    author = {Okamura, Kazuo AND Yamada, Seiji},
    journal = {PLOS ONE},
    publisher = {Public Library of Science},
    title = {Adaptive trust calibration for human-AI collaboration},
    year = {2020},
    month = {02},
    volume = {15},
    
    pages = {1-20},
    abstract = {Safety and efficiency of human-AI collaboration often depend on how humans could appropriately calibrate their trust towards the AI agents. Over-trusting the autonomous system sometimes causes serious safety issues. Although many studies focused on the importance of system transparency in keeping proper trust calibration, the research in detecting and mitigating improper trust calibration remains very limited. To fill these research gaps, we propose a method of adaptive trust calibration that consists of a framework for detecting the inappropriate calibration status by monitoring the user’s reliance behavior and cognitive cues called “trust calibration cues” to prompt the user to reinitiate trust calibration. We evaluated our framework and four types of trust calibration cues in an online experiment using a drone simulator. A total of 116 participants performed pothole inspection tasks by using the drone’s automatic inspection, the reliability of which could fluctuate depending upon the weather conditions. The participants needed to decide whether to rely on automatic inspection or to do the inspection manually. The results showed that adaptively presenting simple cues could significantly promote trust calibration during over-trust.},
    number = {2},

}

@article{TOMSETT2020100049,
title = {Rapid Trust Calibration through Interpretable and Uncertainty-Aware AI},
journal = {Patterns},
volume = {1},
number = {4},
pages = {100049},
year = {2020},
issn = {2666-3899},
doi = {https://doi.org/10.1016/j.patter.2020.100049},

author = {Richard Tomsett and Alun Preece and Dave Braines and Federico Cerutti and Supriyo Chakraborty and Mani Srivastava and Gavin Pearson and Lance Kaplan},
keywords = {DSML 1: Concept: Basic principles of a new data science output observed and reported},
abstract = {Artificial intelligence (AI) systems hold great promise as decision-support tools, but we must be able to identify and understand their inevitable mistakes if they are to fulfill this potential. This is particularly true in domains where the decisions are high-stakes, such as law, medicine, and the military. In this Perspective, we describe the particular challenges for AI decision support posed in military coalition operations. These include having to deal with limited, low-quality data, which inevitably compromises AI performance. We suggest that these problems can be mitigated by taking steps that allow rapid trust calibration so that decision makers understand the AI system's limitations and likely failures and can calibrate their trust in its outputs appropriately. We propose that AI services can achieve this by being both interpretable and uncertainty-aware. Creating such AI systems poses various technical and human factors challenges. We review these challenges and recommend directions for future research.}
}

  @InProceedings{Liu2021LAST,
      title={Iterative Teaching by Label Synthesis},
      author={Liu, Weiyang and Liu, Zhen and Wang, Hanchen and Paull, Liam and Schölkopf, Bernhard and Weller, Adrian},
      booktitle = {NeurIPS},
      year={2021}}

@article{ho2016eliciting,
  title={Eliciting categorical data for optimal aggregation},
  author={Ho, Chien-Ju and Frongillo, Rafael and Chen, Yiling},
  journal={NeurIPS},
  volume={29},
  year={2016}
}

@paper{AAAI136451,
	author = {Goran Radanovic and Boi Faltings},
	title = {A Robust Bayesian Truth Serum for Non-Binary Signals},
	conference = {AAAI Conference on Artificial Intelligence},
	year = {2013},
	keywords = {Incentive Compatibility, Truth Elicitation, Mechanism Design},
	abstract = {Several mechanisms have been proposed for incentivizing truthful reports of a private signals owned by rational agents, among them the peer prediction method and the Bayesian truth serum. The robust Bayesian truth serum (RBTS) for small populations and binary signals is particularly interesting since it does not require a common prior to be known to the mechanism. We further analyze the problem of the common prior not known to the mechanism and give several results regarding the restrictions that need to be placed in order to have an incentive-compatible mechanism. Moreover, we construct a Bayes-Nash incentive-compatible scheme called multi-valued RBTS that generalizes RBTS to operate on both small populations and non-binary signals.},

	
}



@article{DBLP:journals/corr/abs-2011-11015,
  author    = {Brett D. Roads and
               Bradley C. Love},
  title     = {Enriching ImageNet with Human Similarity Judgments and Psychological
               Embeddings},
  journal   = {CoRR},
  volume    = {abs/2011.11015},
  year      = {2020},
  
  eprinttype = {arXiv},
  eprint    = {2011.11015},
  timestamp = {Wed, 25 Nov 2020 17:55:02 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2011-11015.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@INPROCEEDINGS{activeLearningMulti,

  author={Joshi, Ajay J. and Porikli, Fatih and Papanikolopoulos, Nikolaos},

  booktitle={2009 IEEE Conference on Computer Vision and Pattern Recognition}, 

  title={Multi-class active learning for image classification}, 

  year={2009},

  volume={},

  number={},

  pages={2372-2379},

  doi={10.1109/CVPR.2009.5206627}}





@article{russell2008labelme,
  title={LabelMe: a database and web-based tool for image annotation},
  author={Russell, Bryan C and Torralba, Antonio and Murphy, Kevin P and Freeman, William T},
  journal={International journal of computer vision},
  volume={77},
  number={1},
  pages={157--173},
  year={2008},
  publisher={Springer}
}


@article{noisyLabels,
  author    = {Jiaheng Wei and
               Zhaowei Zhu and
               Hao Cheng and
               Tongliang Liu and
               Gang Niu and
               Yang Liu},
  title     = {Learning with Noisy Labels Revisited: {A} Study Using Real-World Human
               Annotations},
  journal   = {CoRR},
  volume    = {abs/2110.12088},
  year      = {2021},
  
  eprinttype = {arXiv},
  eprint    = {2110.12088},
  timestamp = {Fri, 29 Oct 2021 12:48:32 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2110-12088.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{disagreementsMultiTask,
  author    = {Aida Mostafazadeh Davani and
               Mark D{\'{\i}}az and
               Vinodkumar Prabhakaran},
  title     = {Dealing with Disagreements: Looking Beyond the Majority Vote in Subjective
               Annotations},
  journal   = {CoRR},
  volume    = {abs/2110.05719},
  year      = {2021},
  
  eprinttype = {arXiv},
  eprint    = {2110.05719},
  timestamp = {Thu, 21 Oct 2021 16:20:08 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2110-05719.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@misc{plex,
  doi = {10.48550/ARXIV.2207.07411},
  
  
  
  author = {Tran, Dustin and Liu, Jeremiah and Dusenberry, Michael W. and Phan, Du and Collier, Mark and Ren, Jie and Han, Kehang and Wang, Zi and Mariet, Zelda and Hu, Huiyi and Band, Neil and Rudner, Tim G. J. and Singhal, Karan and Nado, Zachary and van Amersfoort, Joost and Kirsch, Andreas and Jenatton, Rodolphe and Thain, Nithum and Yuan, Honglin and Buchanan, Kelly and Murphy, Kevin and Sculley, D. and Gal, Yarin and Ghahramani, Zoubin and Snoek, Jasper and Lakshminarayanan, Balaji},
  
  keywords = {Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Plex: Towards Reliability using Pretrained Large Model Extensions},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {Creative Commons Attribution 4.0 International}
}


@article{releaseAnnotatorLevel,
  author    = {Vinodkumar Prabhakaran and
               Aida Mostafazadeh Davani and
               Mark D{\'{\i}}az},
  title     = {On Releasing Annotator-Level Labels and Information in Datasets},
  journal   = {CoRR},
  volume    = {abs/2110.05699},
  year      = {2021},
  
  eprinttype = {arXiv},
  eprint    = {2110.05699},
  timestamp = {Thu, 21 Oct 2021 16:20:08 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2110-05699.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{tsipras2018robustness,
  title={Robustness may be at odds with accuracy},
  author={Tsipras, Dimitris and Santurkar, Shibani and Engstrom, Logan and Turner, Alexander and Madry, Aleksander},
  journal={arXiv preprint arXiv:1805.12152},
  year={2018}
}



@article{audioUncertainty,
author = {M\'{e}ndez, Ana Elisa and Cartwright, Mark and Bello, Juan Pablo and Nov, Oded},
title = {Eliciting Confidence for Improving Crowdsourced Audio Annotations},
year = {2022},
issue_date = {April 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {CSCW1},

doi = {10.1145/3512935},
abstract = {In this work we explore confidence elicitation methods for crowdsourcing "soft" labels, e.g., probability estimates, to reduce the annotation costs for domains with ambiguous data. Machine learning research has shown that such "soft" labels are more informative and can reduce the data requirements when training supervised machine learning models. By reducing the number of required labels, we can reduce the costs of slow annotation processes such as audio annotation. In our experiments we evaluated three confidence elicitation methods: 1) "No Confidence" elicitation, 2) "Simple Confidence" elicitation, and 3) "Betting" mechanism for confidence elicitation, at both individual (i.e., per participant) and aggregate (i.e., crowd) levels. In addition, we evaluated the interaction between confidence elicitation methods, annotation types (binary, probability, and z-score derived probability), and "soft" versus "hard" (i.e., binarized) aggregate labels. Our results show that both confidence elicitation mechanisms result in higher annotation quality than the "No Confidence" mechanism for binary annotations at both participant and recording levels. In addition, when aggregating labels at the recording level, results indicate that we can achieve comparable results to those with 10-participant aggregate annotations using fewer annotators if we aggregate "soft" labels instead of "hard" labels. These results suggest that for binary audio annotation using a confidence elicitation mechanism and aggregating continuous labels we can obtain higher annotation quality, more informative labels, with quality differences more pronounced with fewer participants. Finally, we propose a way of integrating these confidence elicitation methods into a two-stage, multi-label annotation pipeline.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = {apr},
articleno = {88},
numpages = {25},
keywords = {crowdsourcing, audio annotation, machine learning}
}

@inproceedings{lease2011quality,
  title={On quality control and machine learning in crowdsourcing},
  author={Lease, Matthew},
  booktitle={AAAI Workshops},
  year={2011}
}

@inproceedings{futureOfCrowdWork2013,
author = {Kittur, Aniket and Nickerson, Jeffrey and Bernstein, Michael and Gerber, Elizabeth and Shaw, Aaron and Zimmerman, John and Lease, Matt and Horton, John},
year = {2013},
month = {02},
pages = {1301-1318},
title = {The Future of Crowd Work},
isbn = {1450313310},
journal = {Proceedings of the ACM Conference on Computer Supported Cooperative Work, CSCW},
doi = {10.1145/2441776.2441923}
}

@inproceedings{gadiraju2015understandingCrowdsource,
  title={Understanding malicious behavior in crowdsourcing platforms: The case of online surveys},
  author={Gadiraju, Ujwal and Kawase, Ricardo and Dietze, Stefan and Demartini, Gianluca},
  booktitle={CHI},
  year={2015}
}



@inproceedings{lei2016rationalizing,
  title={Rationalizing Neural Predictions},
  author={Lei, Tao and Barzilay, Regina and Jaakkola, Tommi},
  booktitle={Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing},
  pages={107--117},
  year={2016}
}

@article{pleiss2020identifyingMislabeled,
  title={Identifying mislabeled data using the area under the margin ranking},
  author={Pleiss, Geoff and Zhang, Tianyi and Elenberg, Ethan and Weinberger, Kilian Q},
  journal={NeurIPS},
  year={2020}
}

  
 @inproceedings{zhang2022when,
   title={When and How Mixup Improves Calibration},
   author={Zhang, Linjun and Deng, Zhun and Kawaguchi, Kenji and Zou, James},
   booktitle={ICML},
   year={2022}
}


@article{umangUncertainty,
  author    = {Umang Bhatt and
               Yunfeng Zhang and
               Javier Antor{\'{a}}n and
               Q. Vera Liao and
               Prasanna Sattigeri and
               Riccardo Fogliato and
               Gabrielle Gauthier Melan{\c{c}}on and
               Ranganath Krishnan and
               Jason Stanley and
               Omesh Tickoo and
               Lama Nachman and
               Rumi Chunara and
               Adrian Weller and
               Alice Xiang},
  title     = {Uncertainty as a Form of Transparency: Measuring, Communicating, and
               Using Uncertainty},
  journal   = {CoRR},
  volume    = {abs/2011.07586},
  year      = {2020},
  
  eprinttype = {arXiv},
  eprint    = {2011.07586},
  timestamp = {Wed, 18 Nov 2020 16:48:35 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2011-07586.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}



@article{learningFromImperfectAnnotations,
  author    = {Emmanouil Antonios Platanios and
               Maruan Al{-}Shedivat and
               Eric P. Xing and
               Tom M. Mitchell},
  title     = {Learning from Imperfect Annotations},
  journal   = {CoRR},
  volume    = {abs/2004.03473},
  year      = {2020},
  
  eprinttype = {arXiv},
  eprint    = {2004.03473},
  timestamp = {Wed, 08 Apr 2020 17:08:25 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2004-03473.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{zhang2018multi,
  title={Multi-label inference for crowdsourcing},
  author={Zhang, Jing and Wu, Xindong},
  booktitle={Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery \& Data Mining},
  pages={2738--2747},
  year={2018}
}

@article{dawid1979maximum,
  title={Maximum likelihood estimation of observer error-rates using the EM algorithm},
  author={Dawid, Alexander Philip and Skene, Allan M},
  journal={Journal of the Royal Statistical Society: Series C (Applied Statistics)},
  volume={28},
  number={1},
  pages={20--28},
  year={1979},
  publisher={Wiley Online Library}
}

@inproceedings{barbiero2022entropy,
  title={Entropy-based logic explanations of neural networks},
  author={Barbiero, Pietro and Ciravegna, Gabriele and Giannini, Francesco and Li{\'o}, Pietro and Gori, Marco and Melacci, Stefano},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={36},
  number={6},
  pages={6046--6054},
  year={2022}
}

@article{zarlenga2023towards,
  title={Towards Robust Metrics for Concept Representation Evaluation},
  author={Espinosa Zarlenga, Mateo and Barbiero, Pietro and Shams, Zohreh and Kazhdan, Dmitry and Bhatt, Umang and Weller, Adrian and Jamnik, Mateja},
  journal={arXiv preprint arXiv:2301.10367},
  year={2023}
}

@article{chen2020concept,
  title={Concept whitening for interpretable image recognition},
  author={Chen, Zhi and Bei, Yijie and Rudin, Cynthia},
  journal={Nature Machine Intelligence},
  volume={2},
  number={12},
  pages={772--782},
  year={2020},
  publisher={Nature Publishing Group UK London}
}

@article{whitehill2009whoseVote,
  title={Whose vote should count more: Optimal integration of labels from labelers of unknown expertise},
  author={Whitehill, Jacob and Wu, Ting-fan and Bergsma, Jacob and Movellan, Javier and Ruvolo, Paul},
  journal={NeurIPS},
  volume={22},
  year={2009}
}


@inproceedings{sheng2008get,
  title={Get another label? improving data quality and data mining using multiple, noisy labelers},
  author={Sheng, Victor S and Provost, Foster and Ipeirotis, Panagiotis G},
  booktitle={Proceedings of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining},
  pages={614--622},
  year={2008}
}

@inproceedings{patel2021manifoldDataAug,
  title={On-manifold adversarial data augmentation improves uncertainty calibration},
  author={Patel, Kanil and Beluch, William and Zhang, Dan and Pfeiffer, Michael and Yang, Bin},
  booktitle={2020 25th International Conference on Pattern Recognition (ICPR)},
  pages={8029--8036},
  year={2021},
  organization={IEEE}
}

@article{schmarje2022one,
  title={Is one annotation enough? A data-centric image classification benchmark for noisy and ambiguous label estimation},
  author={Schmarje, Lars and Grossmann, Vasco and Zelenka, Claudius and Dippel, Sabine and Kiko, Rainer and Oszust, Mariusz and Pastell, Matti and Stracke, Jenny and Valros, Anna and Volkmann, Nina and others},
  journal={arXiv preprint arXiv:2207.06214},
  year={2022}
}

@inproceedings{hillunknownunknowns,
  title={Iterative Human-in-the-Loop Discovery of Unknown Unknowns in Image Datasets},
  author={Han, Lei and Dong, Xiao and Demartini, Gianluca},
  booktitle={Proceedings of the AAAI Conference on Human Computation and Crowdsourcing},
  volume={9},
  pages={72--83},
  year={2021}
}


@inproceedings{collier2022transfer,
  title={Transfer and Marginalize: Explaining Away Label Noise with Privileged Information},
  author={Collier, Mark and Jenatton, Rodolphe and Kokiopoulou, Effrosyni and Berent, Jesse},
  booktitle={ICML},
  pages={4219--4237},
  year={2022},
  organization={PMLR}
}

@article{turing1950,
  title={Computing Machinery and Intelligence},
  author={Turing, Alan},
  journal={Mind},
  volume={59},
  number={236},
  pages={433},
  year={1950}
}

@TECHREPORT{Krizhevsky09learningmultiple,
    author = {Alex Krizhevsky},
    title = {Learning multiple layers of features from tiny images},
    institution = {University of Toronto},
    year = {2009}
}



@article{straitouri2022provably,
  title={Provably Improving Expert Predictions with Conformal Prediction},
  author={Straitouri, Eleni and Wang, Lequn and Okati, Nastaran and Rodriguez, Manuel Gomez},
  journal={arXiv preprint arXiv:2201.12006},
  year={2022}
}

@inproceedings{babbar2022utility,
  title={On the Utility of Prediction Sets in Human-AI Teams},
  author={Babbar, Varun and Bhatt, Umang and Weller, Adrian},
  year={2022},
  booktitle={IJCAI}
}


@article{ipeirotis2014repeated,
  title={Repeated labeling using multiple noisy labelers},
  author={Ipeirotis, Panagiotis G and Provost, Foster and Sheng, Victor S and Wang, Jing},
  journal={Data Mining and Knowledge Discovery},
  volume={28},
  number={2},
  pages={402--441},
  year={2014},
  publisher={Springer}
}

@inproceedings{smyth1994knowledgeUncImgs,
  title={Knowledge Discovery in Large Image Databases: Dealing with Uncertainties in Ground Truth.},
  author={Smyth, Padhraic and Burl, Michael C and Fayyad, Usama M and Perona, Pietro},
  booktitle={KDD workshop},
  pages={109--120},
  year={1994}
}

@incollection{doshi2018considerations,
  title={Considerations for evaluation and generalization in interpretable machine learning},
  author={Doshi-Velez, Finale and Kim, Been},
  booktitle={Explainable and interpretable models in computer vision and machine learning},
  pages={3--17},
  year={2018},
  publisher={Springer}
}

  
 @article{hendrycks2019robustness,
      title={Benchmarking Neural Network Robustness to Common Corruptions and Perturbations},
      author={Hendrycks, Dan and Dietterich, Thomas},
      journal={ICLR},
      year={2019}
    }
    
    @misc{gtNumLabelers,
  doi = {10.48550/ARXIV.2206.12041},
  
  
  
  author = {Cheng, Chen and Asi, Hilal and Duchi, John},
  
  keywords = {Statistics Theory (math.ST), Human-Computer Interaction (cs.HC), Machine Learning (cs.LG), FOS: Mathematics, FOS: Mathematics, FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {How many labelers do you have? A closer look at gold-standard labels},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

    
    @article{koller2022beyondOneHot,
  title={Going Beyond One-Hot Encoding in Classification: Can Human Uncertainty Improve Model Performance?},
  author={Koller, Christoph and Kauermann, G{\"o}ran and Zhu, Xiao Xiang},
  journal={arXiv preprint arXiv:2205.15265},
  year={2022}
}

@inproceedings{hendrycks2022pixmix,
  title={Pixmix: Dreamlike pictures comprehensively improve safety measures},
  author={Hendrycks, Dan and Zou, Andy and Mazeika, Mantas and Tang, Leonard and Li, Bo and Song, Dawn and Steinhardt, Jacob},
  booktitle={CVPR},
  year={2022}
}


@article{thomas2022relianceMetrics,
  title={Reliance on metrics is a fundamental challenge for AI},
  author={Thomas, Rachel L and Uminsky, David},
  journal={Patterns},
  volume={3},
  number={5},
  pages={100476},
  year={2022},
  publisher={Elsevier}
}



@inproceedings{weldRelabel,
  author    = {Christopher H. Lin and
               Mausam and
               Daniel S. Weld},
  editor    = {Jeffrey P. Bigham and
               David C. Parkes},
  title     = {To Re(label), or Not To Re(label)},
  booktitle = {Proceedings of the Seconf {AAAI} Conference on Human Computation and
               Crowdsourcing, {HCOMP} 2014, November 2-4, 2014, Pittsburgh, Pennsylvania,
               {USA}},
  publisher = {{AAAI}},
  year      = {2014},
  
  timestamp = {Wed, 10 Feb 2021 08:46:28 +0100},
  biburl    = {https://dblp.org/rec/conf/hcomp/LinMW14.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}



@article{softlossMed,
author = {Nguyen, Quang and Valizadegan, Hamed and Hauskrecht, Milos},
year = {2013},
month = {11},
title = {Learning classification models with soft-label information},
volume = {21},
journal = {Journal of the American Medical Informatics Association}
}

@inproceedings{kim2020co,
  title={Co-Mixup: Saliency Guided Joint Mixup with Supermodular Diversity},
  author={Kim, JangHyun and Choo, Wonho and Jeong, Hosan and Song, Hyun Oh},
  booktitle={ICLR},
  year={2020}
}

@inproceedings{coMixup,
  title={Co-Mixup: Saliency Guided Joint Mixup with Supermodular Diversity},
  author={Kim, JangHyun and Choo, Wonho and Jeong, Hosan and Song, Hyun Oh},
  booktitle={ICLR},
  year={2020}
}


@inproceedings{chuang2020fair,
  title={Fair Mixup: Fairness via Interpolation},
  author={Chuang, Ching-Yao and Mroueh, Youssef},
  booktitle={ICLR},
  year={2020}
}

@Inbook{Hartley2018,
author="Hartley, David
and French, Simon",
editor="Dias, Luis C.
and Morton, Alec
and Quigley, John",
title="Elicitation and Calibration: A Bayesian Perspective",
bookTitle="Elicitation: The Science and Art of Structuring Judgement",
year="2018",
publisher="Springer International Publishing",
address="Cham",
pages="119--140",
abstract="There are relatively few published perspectives on processes and procedures for organising the elicitation, aggregation and documentation of expert judgement studies. The few that exist emphasise different aggregation models, but none build a full Bayesian model to combine the judgements of multiple experts into the posterior distribution for a decision maker. Historically, Bayesian concepts have identified issues with current modelling approaches to aggregation, but have led to models that are difficult to implement. Recently Bayesian models have started to become more tractable, so it is timely to reflect on elicitation processes that enable the model to be applied. That is our purpose in this Chapter. In particular, the European Food Safety Authority have provided the most detailed and thorough prescription of the procedures and processes needed to conduct an expert judgement study. We critically review this from a Bayesian perspective, asking how it might need modifying if Bayesian models are included to analyse and aggregate the expert judgements.",
isbn="978-3-319-65052-4",
doi="10.1007/978-3-319-65052-4_6",

}

@misc{dataPerf,
  doi = {10.48550/ARXIV.2207.10062},
  
  
  
  author = {Mazumder, Mark and Banbury, Colby and Yao, Xiaozhe and Karlaš, Bojan and Rojas, William Gaviria and Diamos, Sudnya and Diamos, Greg and He, Lynn and Kiela, Douwe and Jurado, David and Kanter, David and Mosquera, Rafael and Ciro, Juan and Aroyo, Lora and Acun, Bilge and Eyuboglu, Sabri and Ghorbani, Amirata and Goodman, Emmett and Kane, Tariq and Kirkpatrick, Christine R. and Kuo, Tzu-Sheng and Mueller, Jonas and Thrush, Tristan and Vanschoren, Joaquin and Warren, Margaret and Williams, Adina and Yeung, Serena and Ardalani, Newsha and Paritosh, Praveen and Zhang, Ce and Zou, James and Wu, Carole-Jean and Coleman, Cody and Ng, Andrew and Mattson, Peter and Reddi, Vijay Janapa},
  
  keywords = {Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {DataPerf: Benchmarks for Data-Centric AI Development},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {Creative Commons Attribution 4.0 International}
}


@inproceedings{karpathy2015deep,
  title={Deep visual-semantic alignments for generating image descriptions},
  author={Karpathy, Andrej and Fei-Fei, Li},
  booktitle={CVPR},
  pages={3128--3137},
  year={2015}
}


@article{gebru2021datasheets,
  title={Datasheets for datasets},
  author={Gebru, Timnit and Morgenstern, Jamie and Vecchione, Briana and Vaughan, Jennifer Wortman and Wallach, Hanna and Iii, Hal Daum{\'e} and Crawford, Kate},
  journal={Communications of the ACM},
  volume={64},
  number={12},
  pages={86--92},
  year={2021},
  publisher={ACM New York, NY, USA}
}


@article{neilLawrenceData,
  author    = {Neil D. Lawrence},
  title     = {Data Science and Digital Systems: The 3Ds of Machine Learning Systems
               Design},
  journal   = {CoRR},
  volume    = {abs/1903.11241},
  year      = {2019},
  
  eprinttype = {arXiv},
  eprint    = {1903.11241},
  timestamp = {Tue, 02 Apr 2019 11:16:55 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1903-11241.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@inproceedings{guo2017calibration,
  title={On calibration of modern neural networks},
  author={Guo, Chuan and Pleiss, Geoff and Sun, Yu and Weinberger, Kilian Q},
  booktitle={ICML},
  pages={1321--1330},
  year={2017},
  organization={PMLR}
}


@article{umangTrust,
title = {How transparency modulates trust in artificial intelligence},
journal = {Patterns},
volume = {3},
number = {4},
pages = {100455},
year = {2022},
author = {John Zerilli and Umang Bhatt and Adrian Weller},
}

@InProceedings{pmlr-v139-wong21a,
  title = 	 {Leveraging Language to Learn Program Abstractions and Search Heuristics},
  author =       {Wong, Catherine and Ellis, Kevin M and Tenenbaum, Joshua and Andreas, Jacob},
  booktitle = 	 {ICML},
  pages = 	 {11193--11204},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/wong21a/wong21a.pdf},
  
  abstract = 	 {Inductive program synthesis, or inferring programs from examples of desired behavior, offers a general paradigm for building interpretable, robust, andgeneralizable machine learning systems. Effective program synthesis depends on two key ingredients: a strong library of functions from which to build programs, and an efficient search strategy for finding programs that solve a given task. We introduce LAPS (Language for Abstraction and Program Search), a technique for using natural language annotations to guide joint learning of libraries and neurally-guided search models for synthesis. When integrated into a state-of-the-art library learning system (DreamCoder), LAPS produces higher-quality libraries and improves search efficiency and generalization on three domains {–} string editing, image composition, and abstract reasoning about scenes {–} even when no natural language hints are available at test time.}
}


@article{jordon2022synthetic,
  title={Synthetic Data--what, why and how?},
  author={Jordon, James and Szpruch, Lukasz and Houssiau, Florimond and Bottarelli, Mirko and Cherubin, Giovanni and Maple, Carsten and Cohen, Samuel N and Weller, Adrian},
  journal={arXiv preprint arXiv:2205.03257},
  year={2022}
}

@article{avin2021filling,
  title={Filling gaps in trustworthy development of AI},
  author={Avin, Shahar and Belfield, Haydn and Brundage, Miles and Krueger, Gretchen and Wang, Jasmine and Weller, Adrian and Anderljung, Markus and Krawczuk, Igor and Krueger, David and Lebensold, Jonathan and others},
  journal={Science},
  volume={374},
  number={6573},
  pages={1327--1329},
  year={2021},
  publisher={American Association for the Advancement of Science}
}

@misc{beyondHardLabels,
  doi = {10.48550/ARXIV.2207.06224},
  
  
  
  author = {Grossmann, Vasco and Schmarje, Lars and Koch, Reinhard},
  
  keywords = {Computer Vision and Pattern Recognition (cs.CV), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Beyond Hard Labels: Investigating data label distributions},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}



@incollection{benHCAI,
    author = {Shneiderman, Ben},
    isbn = {9780192845290},
    title = "{}",
    booktitle = "{Human-Centered AI}",
    publisher = {Oxford University Press},
    year = {2022},
    month = {01},
    abstract = "{Researchers, developers, business leaders, policy-makers, and others are expanding the technology-centered scope of artificial intelligence (AI) to include human-centered AI (HCAI) ways of thinking. This expansion from an algorithm-focused view to embrace a human-centered perspective can shape the future of technology so as to better serve human needs. Educators, designers, software engineers, product managers, evaluators, and government agency staffers can build on AI-driven technologies to design products and services that make life better for the users, enabling people to care for each other. Humans have always been tool builders, and now they are supertool builders, whose inventions can improve our health, family life, education, business, the environment, and much more. The remarkable progress in algorithms for machine and deep learning during the past decade has opened the doors to new opportunities, and some dark possibilities. However, a bright future awaits AI researchers, developers, business leaders, policy-makers, and others who build on AI algorithms by including HCAI strategies of design and testing. This enlarged vision can shape the future of technology so as to better serve human values and needs. As many technology companies and thought leaders have said, the goal is not to replace people but to empower them by making design choices that give humans control over technology.}",
    doi = {10.1093/oso/9780192845290.001.0001},
    eprint = {https://academic.oup.com/book/0/chapter/350463911/chapter-ag-pdf/45210758/book\_41126\_section\_350463911.ag.pdf},
}



@article{SilverHuangEtAl16nature,
  author = {Silver, David and Huang, Aja and Maddison, Chris J. and Guez, Arthur and Sifre, Laurent and van den Driessche, George and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc and Dieleman, Sander and Grewe, Dominik and Nham, John and Kalchbrenner, Nal and Sutskever, Ilya and Lillicrap, Timothy and Leach, Madeleine and Kavukcuoglu, Koray and Graepel, Thore and Hassabis, Demis},
  journal = {Nature},
  number = 7587,
  pages = {484--489},
  title = {Mastering the Game of {Go} with Deep Neural Networks and Tree Search},
  volume = 529,
  year = 2016
}



@article{simonyan2014very,
  title={Very deep convolutional networks for large-scale image recognition},
  author={Simonyan, Karen and Zisserman, Andrew},
  journal={arXiv preprint arXiv:1409.1556},
  year={2014}
}

@inproceedings{wong2021leveraging,
  title={Leveraging language to learn program abstractions and search heuristics},
  author={Wong, Catherine and Ellis, Kevin M and Tenenbaum, Joshua and Andreas, Jacob},
  booktitle={ICML},
  pages={11193--11204},
  year={2021},
  organization={PMLR}
}


@article{wiegreffe2021reframing,
  title={Reframing human-ai collaboration for generating free-text explanations},
  author={Wiegreffe, Sarah and Hessel, Jack and Swayamdipta, Swabha and Riedl, Mark and Choi, Yejin},
  journal={arXiv preprint arXiv:2112.08674},
  year={2021}
}



}
@inproceedings{kim2012bayesian,
  title={Bayesian classifier combination},
  author={Kim, Hyun-Chul and Ghahramani, Zoubin},
  booktitle={Artificial Intelligence and Statistics},
  pages={619--627},
  year={2012},
  organization={PMLR}
}


@InProceedings{pmlr-v119-mozannar20b,
  title = 	 {Consistent Estimators for Learning to Defer to an Expert},
  author =       {Mozannar, Hussein and Sontag, David},
  booktitle = 	 {Proceedings of the 37th ICML},
  pages = 	 {7076--7087},
  year = 	 {2020},
  editor = 	 {III, Hal Daumé and Singh, Aarti},
  volume = 	 {119},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {13--18 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v119/mozannar20b/mozannar20b.pdf},
  
  abstract = 	 {Learning algorithms are often used in conjunction with expert decision makers in practical scenarios, however this fact is largely ignored when designing these algorithms. In this paper we explore how to learn predictors that can either predict or choose to defer the decision to a downstream expert. Given only samples of the expert’s decisions, we give a procedure based on learning a classifier and a rejector and analyze it theoretically. Our approach is based on a novel reduction to cost sensitive learning where we give a consistent surrogate loss for cost sensitive learning that generalizes the cross entropy loss. We show the effectiveness of our approach on a variety of experimental tasks.}
}

@misc{madras2018predict,
      title={Predict Responsibly: Improving Fairness and Accuracy by Learning to Defer}, 
      author={David Madras and Toniann Pitassi and Richard Zemel},
      year={2018},
      eprint={1711.06664},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}



@misc{joshi2021preemptive,
      title={Pre-emptive learning-to-defer for sequential medical decision-making under uncertainty}, 
      author={Shalmali Joshi and Sonali Parbhoo and Finale Doshi-Velez},
      year={2021},
      eprint={2109.06312},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{DBLP:journals/corr/abs-2004-13102,
  author    = {Gagan Bansal and
               Besmira Nushi and
               Ece Kamar and
               Eric Horvitz and
               Daniel S. Weld},
  title     = {Optimizing {AI} for Teamwork},
  journal   = {CoRR},
  volume    = {abs/2004.13102},
  year      = {2020},
  
  eprinttype = {arXiv},
  eprint    = {2004.13102},
  timestamp = {Sat, 02 May 2020 19:17:26 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2004-13102.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{doi:10.1146/annurev-devpsych-070120-014806,
author = {Chu, Junyi and Schulz, Laura E.},
title = {Play, Curiosity, and Cognition},
journal = {Annual Review of Developmental Psychology},
volume = {2},
number = {1},
pages = {317-343},
year = {2020},
doi = {10.1146/annurev-devpsych-070120-014806},
eprint = { 
        https://doi.org/10.1146/annurev-devpsych-070120-014806
    
}
,
    abstract = { Few phenomena in childhood are as compelling—and mystifying—as play. We review five proposals about the relationship between play and development. We believe each captures important aspects of play across species; however, we believe none of them accounts for the extraordinary richness of human play or its connection to distinctively human learning. In thinking about play, we are particularly struck by the profligacy with which children set seemingly arbitrary rewards and incur unnecessary costs. We suggest that researchers take the seeming inutility of play seriously and consider why it might be useful to engage in apparently useless behavior. We propose that humans’ ability to choose arbitrary costs and rewards allows us to pursue novel goals, discover unexpected information, and invent problems we would not otherwise encounter. Because problems impose constraints on search, these invented problems may help solve a big problem: how to generate new ideas and plans in an otherwise infinite search space. }
}

@article{new-facts,
author = {Schulz, Laura},
year = {2012},
month = {12},
pages = {269-94},
title = {Finding New Facts; Thinking New Thoughts},
volume = {43},
journal = {Advances in child development and behavior},
doi = {10.1016/B978-0-12-397919-3.00010-1}
}


@article{beale1995categorical,
  title={Categorical effects in the percxeption of faces},
  author={Beale, James M and Keil, Frank C},
  journal={Cognition},
  volume={57},
  number={3},
  pages={217--239},
  year={1995},
  publisher={Elsevier}
}


@article{kompa2021second,
  title={Second opinion needed: communicating uncertainty in medical machine learning},
  author={Kompa, Benjamin and Snoek, Jasper and Beam, Andrew L},
  journal={npj Digital Medicine},
  volume={4},
  number={1},
  pages={1--6},
  year={2021},
  publisher={Nature Publishing Group}
}

@misc{dutordoir2021deep,
      title={Deep Neural Networks as Point Estimates for Deep Gaussian Processes}, 
      author={Vincent Dutordoir and James Hensman and Mark van der Wilk and Carl Henrik Ek and Zoubin Ghahramani and Nicolas Durrande},
      year={2021},
      eprint={2105.04504},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@misc{baisero2015family,
      title={On a Family of Decomposable Kernels on Sequences}, 
      author={Andrea Baisero and Florian T. Pokorny and Carl Henrik Ek},
      year={2015},
      eprint={1501.06284},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{2019,
   title={A modern retrospective on probabilistic numerics},
   volume={29},
   ISSN={1573-1375},
   url={http://dx.doi.org/10.1007/s11222-019-09902-z},
   DOI={10.1007/s11222-019-09902-z},
   number={6},
   journal={Statistics and Computing},
   publisher={Springer Science and Business Media LLC},
   author={Oates, C. J. and Sullivan, T. J.},
   year={2019},
   month={Oct},
   pages={1335–1351}
}

@misc{hauberg2019bayes,
      title={Only Bayes should learn a manifold (on the estimation of differential geometric structure from data)}, 
      author={Søren Hauberg},
      year={2019},
      eprint={1806.04994},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@misc{gordon2020convolutional,
      title={Convolutional Conditional Neural Processes}, 
      author={Jonathan Gordon and Wessel P. Bruinsma and Andrew Y. K. Foong and James Requeima and Yann Dubois and Richard E. Turner},
      year={2020},
      eprint={1910.13556},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@misc{nguyen2018variational,
      title={Variational Continual Learning}, 
      author={Cuong V. Nguyen and Yingzhen Li and Thang D. Bui and Richard E. Turner},
      year={2018},
      eprint={1710.10628},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@inproceedings{zaidan2007using,
  title={Using “annotator rationales” to improve machine learning for text categorization},
  author={Zaidan, Omar and Eisner, Jason and Piatko, Christine},
  booktitle={NAACL},
  year={2007}
}


@INPROCEEDINGS{imagenet,

  author={Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Kai Li and Li Fei-Fei},

  booktitle={2009 IEEE Conference on Computer Vision and Pattern Recognition}, 

  title={ImageNet: A large-scale hierarchical image database}, 

  year={2009},

  volume={},

  number={},

  pages={248-255},

  doi={10.1109/CVPR.2009.5206848}}


@inproceedings{
xia2021the,
title={The Gaussian Process Latent Autoregressive Model},
author={Rui Xia and Richard E Turner and Wessel Bruinsma and William Tebbutt},
booktitle={Third Symposium on Advances in Approximate Bayesian Inference},
year={2021},

}

inproceedings{bronskill2021memory,
  title={Memory Efficient Meta-Learning with Large Images},
  author={Bronskill, John F and Massiceti, Daniela and Patacchiola, Massimiliano and Hofmann, Katja and Nowozin, Sebastian and Turner, Richard E},
  booktitle={Thirty-Fifth Conference on Neural Information Processing Systems},
  year={2021}
}

@article{DBLP:journals/corr/abs-1806-03836,
  author    = {Taesup Kim and
               Jaesik Yoon and
               Ousmane Dia and
               Sungwoong Kim and
               Yoshua Bengio and
               Sungjin Ahn},
  title     = {Bayesian Model-Agnostic Meta-Learning},
  journal   = {CoRR},
  volume    = {abs/1806.03836},
  year      = {2018},
  
  eprinttype = {arXiv},
  eprint    = {1806.03836},
  timestamp = {Mon, 13 Aug 2018 16:47:14 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1806-03836.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/abs-2101-07950,
  author    = {Anna Vaughan and
               William Tebbutt and
               J. Scott Hosking and
               Richard E. Turner},
  title     = {Convolutional conditional neural processes for local climate downscaling},
  journal   = {CoRR},
  volume    = {abs/2101.07950},
  year      = {2021},
  
  eprinttype = {arXiv},
  eprint    = {2101.07950},
  timestamp = {Sat, 23 Jan 2021 17:57:49 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2101-07950.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@unpublished{bas,
title= {Climate Science Challenges and Opportunities for Machine Learning
},
author = {J. Scott Hosking},
year = {2021},
note= {Machine Learning for the Physical World, Guest Lecture},
}

@inproceedings{word2vec,
 author = {Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg S and Dean, Jeff},
 booktitle = {NeurIPS},
 editor = {C. J. C. Burges and L. Bottou and M. Welling and Z. Ghahramani and K. Q. Weinberger},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Distributed Representations of Words and Phrases and their Compositionality},
 
 volume = {26},
 year = {2013}
}

@inproceedings{mixup,
  author    = {Hongyi Zhang and
               Moustapha Ciss{\'{e}} and
               Yann N. Dauphin and
               David Lopez{-}Paz},
  title     = {mixup: Beyond Empirical Risk Minimization},
  booktitle   = {ICLR},
  year      = {2018},
  
}


@Misc{
battleday2019improving,
title={Improving machine classification using human uncertainty measurements},
author={Ruairidh M. Battleday and Joshua C. Peterson and Thomas L. Griffiths},
year={2019},
howpublished={https://openreview.net/forum?id=rJl8BhRqF7},
}


@inproceedings{peterson2019human,
  title={Human uncertainty makes classification more robust},
  author={Peterson, Joshua C and Battleday, Ruairidh M and Griffiths, Thomas L and Russakovsky, Olga},
  booktitle={ICCV},
  year={2019}
}

@inproceedings{hendrycks2019augmix,
  title={AugMix: A Simple Data Processing Method to Improve Robustness and Uncertainty},
  author={Hendrycks, Dan and Mu, Norman and Cubuk, Ekin Dogus and Zoph, Barret and Gilmer, Justin and Lakshminarayanan, Balaji},
  booktitle={ICLR},
  year={2019}
}


@article{cascante2020curriculum,
  title={Curriculum labeling: Revisiting pseudo-labeling for semi-supervised learning},
  author={Cascante-Bonilla, Paola and Tan, Fuwen and Qi, Yanjun and Ordonez, Vicente},
  journal={arXiv preprint arXiv:2001.06001},
  year={2020}
}

@misc{pseudoVersusHumanASR,
  doi = {10.48550/ARXIV.2203.12668},
  
  
  
  author = {Hwang, Dongseong and Sim, Khe Chai and Huo, Zhouyuan and Strohman, Trevor},
  
  keywords = {Machine Learning (cs.LG), Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Pseudo Label Is Better Than Human Label},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {Creative Commons Attribution 4.0 International}
}


@InProceedings{datasetGAN,
    author    = {Zhang, Yuxuan and Ling, Huan and Gao, Jun and Yin, Kangxue and Lafleche, Jean-Francois and Barriuso, Adela and Torralba, Antonio and Fidler, Sanja},
    title     = {DatasetGAN: Efficient Labeled Data Factory With Minimal Human Effort},
    booktitle = {CVPR},
    year      = {2021}
}

@InProceedings{onmixuptraining,
  title={On mixup training: Improved calibration and predictive uncertainty for deep neural networks},
  author={Thulasidasan, Sunil and Chennupati, Gopinath and Bilmes, Jeff A and Bhattacharya, Tanmoy and Michalak, Sarah},
  booktitle={NeurIPS},
  year={2019}
}


@article{distillLabelNoise,
  author    = {Zizhao Zhang and
               Han Zhang and
               Sercan {\"{O}}mer Arik and
               Honglak Lee and
               Tomas Pfister},
  title     = {{IEG:} Robust Neural Network Training to Tackle Severe Label Noise},
  journal   = {CoRR},
  volume    = {abs/1910.00701},
  year      = {2019},
  
  eprinttype = {arXiv},
  eprint    = {1910.00701},
  timestamp = {Wed, 21 Oct 2020 12:10:27 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1910-00701.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@incollection{pytorch,
title = {PyTorch: An Imperative Style, High-Performance Deep Learning Library},
author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
booktitle = {NeurIPS 32},
editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
pages = {8024--8035},
year = {2019},
publisher = {Curran Associates, Inc.},

}

@article{automix,
  title={Unveiling the Power of Mixup for Stronger Classifiers},
  author={Liu, Zicheng and Li, Siyuan and Wu, Di and Chen, Zhiyuan and Wu, Lirong and Guo, Jianzhu and Li, Stan Z},
  journal={arXiv preprint arXiv:2103.13027},
  year={2021}
}

@inproceedings{gordon2021disagreement,
  title={The disagreement deconvolution: Bringing machine learning performance metrics in line with reality},
  author={Gordon, Mitchell L and Zhou, Kaitlyn and Patel, Kayur and Hashimoto, Tatsunori and Bernstein, Michael S},
  booktitle={Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems},
  pages={1--14},
  year={2021}
}


@article{uma2022scaling,
  title={Scaling and Disagreements: Bias, Noise, and Ambiguity},
  author={Uma, Alexandra and Almanea, Dina and Poesio, Massimo},
  journal={Frontiers in Artificial Intelligence},
  volume={5},
  year={2022},
  publisher={Frontiers Media SA}
}

@article{muller2019labelSmoothHelp,
  title={When does label smoothing help?},
  author={M{\"u}ller, Rafael and Kornblith, Simon and Hinton, Geoffrey E},
  journal={NeurIPS},
  volume={32},
  year={2019}
}



@article{pereyra2017regularizing,
  title={Regularizing neural networks by penalizing confident output distributions},
  author={Pereyra, Gabriel and Tucker, George and Chorowski, Jan and Kaiser, {\L}ukasz and Hinton, Geoffrey},
  journal={ICLR},
  year={2017}
}


@article{wei2022aggregate,
  title={To Aggregate or Not? Learning with Separate Noisy Labels},
  author={Wei, Jiaheng and Zhu, Zhaowei and Luo, Tianyi and Amid, Ehsan and Kumar, Abhishek and Liu, Yang},
  journal={arXiv preprint arXiv:2206.07181},
  year={2022}
}

@incollection{maniscalco2014signal,
  title={Signal detection theory analysis of type 1 and type 2 data: meta-d′, response-specific meta-d′, and the unequal variance SDT model},
  author={Maniscalco, Brian and Lau, Hakwan},
  booktitle={The cognitive neuroscience of metacognition},
  pages={25--66},
  year={2014},
  publisher={Springer}
}


@article{fleming2017hmeta,
  title={HMeta-d: hierarchical Bayesian estimation of metacognitive efficiency from confidence ratings},
  author={Fleming, Stephen M},
  journal={Neuroscience of consciousness},
  volume={2017},
  number={1},
  pages={nix007},
  year={2017},
  publisher={Oxford University Press}
}


@ARTICLE{metacogMetrics,
  
AUTHOR={Fleming, Stephen M. and Lau, Hakwan C.},   
	 
TITLE={How to measure metacognition},      
	
JOURNAL={Frontiers in Human Neuroscience},      
	
VOLUME={8},      
	
YEAR={2014},      
	  

	
DOI={10.3389/fnhum.2014.00443},      
	
ISSN={1662-5161},   
   
ABSTRACT={The ability to recognize one's own successful cognitive processing, in e.g., perceptual or memory tasks, is often referred to as metacognition. How should we quantitatively measure such ability? Here we focus on a class of measures that assess the correspondence between trial-by-trial accuracy and one's own confidence. In general, for healthy subjects endowed with metacognitive sensitivity, when one is confident, one is more likely to be correct. Thus, the degree of association between accuracy and confidence can be taken as a quantitative measure of metacognition. However, many studies use a statistical correlation coefficient (e.g., Pearson's r) or its variant to assess this degree of association, and such measures are susceptible to undesirable influences from factors such as response biases. Here we review other measures based on signal detection theory and receiver operating characteristics (ROC) analysis that are “bias free,” and relate these quantities to the calibration and discrimination measures developed in the probability estimation literature. We go on to distinguish between the related concepts of metacognitive bias (a difference in subjective confidence despite basic task performance remaining constant), metacognitive sensitivity (how good one is at distinguishing between one's own correct and incorrect judgments) and metacognitive efficiency (a subject's level of metacognitive sensitivity given a certain level of task performance). Finally, we discuss how these three concepts pose interesting questions for the study of metacognition and conscious awareness.}
}
@article{pereyra2017regularizingEnt,
  title={Regularizing neural networks by penalizing confident output distributions},
  author={Pereyra, Gabriel and Tucker, George and Chorowski, Jan and Kaiser, {\L}ukasz and Hinton, Geoffrey},
  journal={arXiv preprint arXiv:1701.06548},
  year={2017}
}


@InProceedings{Yuan_2020_CVPR_KD_LS,
author = {Yuan, Li and Tay, Francis EH and Li, Guilin and Wang, Tao and Feng, Jiashi},
title = {Revisiting Knowledge Distillation via Label Smoothing Regularization},
booktitle = {CVPR},
month = {June},
year = {2020}
} 

@article{combineConfusion,
  author    = {Gavin Kerrigan and
               Padhraic Smyth and
               Mark Steyvers},
  title     = {Combining Human Predictions with Model Probabilities via Confusion
               Matrices and Calibration},
  journal   = {CoRR},
  volume    = {abs/2109.14591},
  year      = {2021},
  
  eprinttype = {arXiv},
  eprint    = {2109.14591},
  timestamp = {Mon, 04 Oct 2021 17:22:25 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2109-14591.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{
hvarfner2022pibo,
title={\${\textbackslash}pi\${BO}: Augmenting Acquisition Functions with User Beliefs for Bayesian Optimization},
author={Carl Hvarfner and Danny Stoll and Artur Souza and Luigi Nardi and Marius Lindauer and Frank Hutter},
booktitle={ICLR},
year={2022},

}

@article{hullermeier2021aleatoric,
  title={Aleatoric and epistemic uncertainty in machine learning: An introduction to concepts and methods},
  author={H{\"u}llermeier, Eyke and Waegeman, Willem},
  journal={Machine Learning},
  volume={110},
  pages={457--506},
  year={2021},
  publisher={Springer}
}


@inproceedings{ru2020bayesian,
  title={Bayesian optimisation over multiple continuous and categorical inputs},
  author={Ru, Binxin and Alvi, Ahsan and Nguyen, Vu and Osborne, Michael A and Roberts, Stephen},
  booktitle={ICML},
  pages={8276--8285},
  year={2020},
  organization={PMLR}
}



@article{mozannar2021teaching,
  title={Teaching Humans When To Defer to a Classifier via Examplars},
  author={Mozannar, Hussein and Satyanarayan, Arvind and Sontag, David},
  journal={arXiv preprint arXiv:2111.11297},
  year={2021}
}

@inproceedings{szegedy2016rethinking,
  title={Rethinking the inception architecture for computer vision},
  author={Szegedy, Christian and Vanhoucke, Vincent and Ioffe, Sergey and Shlens, Jon and Wojna, Zbigniew},
  booktitle={CVPR},
  pages={2818--2826},
  year={2016}
}

@inproceedings{xie2016disturblabel,
  title={Disturblabel: Regularizing cnn on the loss layer},
  author={Xie, Lingxi and Wang, Jingdong and Wei, Zhen and Wang, Meng and Tian, Qi},
  booktitle={CVPR},
  pages={4753--4762},
  year={2016}
}

@article{hinton2015distilling,
  title={Distilling the knowledge in a neural network},
  author={Hinton, Geoffrey and Vinyals, Oriol and Dean, Jeff and others},
  journal={arXiv preprint arXiv:1503.02531},
  volume={2},
  number={7},
  year={2015}
}


@book{books/lib/RasmussenW06,
  added-at = {2020-07-17T00:00:00.000+0200},
  author = {Rasmussen, Carl Edward and Williams, Christopher K. I.},
  
  ee = {https://www.worldcat.org/oclc/61285753},
  interhash = {72c030472023000e0bdeeb06081c3764},
  intrahash = {670a576a21065048f7ddede17e09b6b4},
  isbn = {026218253X},
  keywords = {dblp},
  pages = {I-XVIII, 1-248},
  publisher = {MIT Press},
  series = {Adaptive computation and machine learning},
  timestamp = {2020-07-24T00:45:17.000+0200},
  title = {Gaussian processes for machine learning.},
  year = 2006
}


@article{perceptualMagnetEffect,
  title={Human adults and human infants show a “perceptual magnet effect” for the prototypes of speech categories, monkeys do not},
  author={Kuhl, Patricia K},
  journal={Perception \& psychophysics},
  volume={50},
  number={2},
  pages={93--107},
  year={1991},
  publisher={Springer}
}

@inproceedings{imagentRecht,
  title={Do imagenet classifiers generalize to imagenet?},
  author={Recht, Benjamin and Roelofs, Rebecca and Schmidt, Ludwig and Shankar, Vaishaal},
  booktitle={ICML},
  pages={5389--5400},
  year={2019},
  organization={PMLR}
}


@article{goldstone2010categorical,
  title={Categorical perception},
  author={Goldstone, Robert L and Hendrickson, Andrew T},
  journal={Wiley Interdisciplinary Reviews: Cognitive Science},
  volume={1},
  number={1},
  pages={69--78},
  year={2010},
  publisher={Wiley Online Library}
}


@inproceedings{puzzleMix,
  title={Puzzle mix: Exploiting saliency and local statistics for optimal mixup},
  author={Kim, Jang-Hyun and Choo, Wonho and Song, Hyun Oh},
  booktitle={ICML},
  year={2020}
}


@article{bcLearningAudio,
  title={Learning from between-class examples for deep sound recognition},
  author={Tokozume, Yuji and Ushiku, Yoshitaka and Harada, Tatsuya},
  journal={arXiv preprint arXiv:1711.10282},
  year={2017}
}



@misc{madryAdversarial,
  doi = {10.48550/ARXIV.1706.06083},
  
  
  
  author = {Madry, Aleksander and Makelov, Aleksandar and Schmidt, Ludwig and Tsipras, Dimitris and Vladu, Adrian},
  
  keywords = {Machine Learning (stat.ML), Machine Learning (cs.LG), Neural and Evolutionary Computing (cs.NE), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Towards Deep Learning Models Resistant to Adversarial Attacks},
  
  publisher = {arXiv},
  
  year = {2017},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}



@article{efficacyCounterfactual, 
  author    = {Divyansh Kaushik and
               Amrith Setlur and
               Eduard H. Hovy and
               Zachary C. Lipton},
  title     = {Explaining The Efficacy of Counterfactually-Augmented Data},
  journal   = {ICLR},
  year      = {2021},
  

}

@misc{decoupledmixup, 
  doi = {10.48550/ARXIV.2203.10761},
  
  
  
  author = {Liu, Zicheng and Li, Siyuan and Wang, Ge and Tan, Cheng and Wu, Lirong and Li, Stan Z.},
  
  keywords = {Machine Learning (cs.LG), Artificial Intelligence (cs.AI), Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Decoupled Mixup for Data-efficient Learning},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}



@article{propMix,
  author    = {Filipe R. Cordeiro and
               Vasileios Belagiannis and
               Ian D. Reid and
               Gustavo Carneiro},
  title     = {PropMix: Hard Sample Filtering and Proportional MixUp for Learning
               with Noisy Labels},
  journal   = {CoRR},
  volume    = {abs/2110.11809},
  year      = {2021},
  
  eprinttype = {arXiv},
  eprint    = {2110.11809},
  timestamp = {Thu, 28 Oct 2021 15:25:31 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2110-11809.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc{gp_slides,
  author={Rasmussen, Carl Edward},
  title = {Posterior Gaussian Process (Lecture Slides)},
  howpublished = {\url{https://www.vle.cam.ac.uk/pluginfile.php/18575811/mod_resource/content/1/gp\%20and\%20data.pdf}},
  year = {2016},
  note = {Last Accessed: {2021–03-11}}
}

@misc{ustyuzhaninov2020compositional,
      title={Compositional uncertainty in deep Gaussian processes}, 
      author={Ivan Ustyuzhaninov and Ieva Kazlauskaite and Markus Kaiser and Erik Bodin and Neill D. F. Campbell and Carl Henrik Ek},
      year={2020},
      eprint={1909.07698},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}


@InProceedings{pmlr-v31-damianou13a,
  title = 	 {Deep {G}aussian Processes},
  author = 	 {Damianou, Andreas and Lawrence, Neil D.},
  booktitle = 	 {Proceedings of the Sixteenth International Conference on Artificial Intelligence and Statistics},
  pages = 	 {207--215},
  year = 	 {2013},
  editor = 	 {Carvalho, Carlos M. and Ravikumar, Pradeep},
  volume = 	 {31},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Scottsdale, Arizona, USA},
  month = 	 {29 Apr--01 May},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v31/damianou13a.pdf},
  
  abstract = 	 {In this paper we introduce deep Gaussian process (GP) models. Deep GPs are a deep belief network based on Gaussian process mappings. The data is modeled as the output of a multivariate GP. The inputs to that Gaussian process are then governed by another GP. A single layer model is equivalent to a standard GP or the GP latent variable model (GP-LVM). We perform inference in the model by approximate variational marginalization. This results in a strict lower bound on the marginal likelihood of the model which we use for model selection (number of layers and nodes per layer). Deep belief networks are typically applied to relatively large data sets using stochastic gradient descent for optimization. Our fully Bayesian treatment allows for the application of deep models even when data is scarce. Model selection by our variational bound shows that a five layer hierarchy is justified even when modelling a digit data set containing only 150 examples.}
}

@misc{hutchinson2021vectorvalued,
      title={Vector-valued Gaussian Processes on Riemannian Manifolds via Gauge Independent Projected Kernels}, 
      author={Michael Hutchinson and Alexander Terenin and Viacheslav Borovitskiy and So Takao and Yee Whye Teh and Marc Peter Deisenroth},
      year={2021},
      eprint={2110.14423},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}


@InProceedings{pmlr-v139-hutchinson21a,
  title = 	 {LieTransformer: Equivariant Self-Attention for Lie Groups},
  author =       {Hutchinson, Michael J and Lan, Charline Le and Zaidi, Sheheryar and Dupont, Emilien and Teh, Yee Whye and Kim, Hyunjik},
  booktitle = 	 {ICML},
  pages = 	 {4533--4543},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/hutchinson21a/hutchinson21a.pdf},
  
  abstract = 	 {Group equivariant neural networks are used as building blocks of group invariant neural networks, which have been shown to improve generalisation performance and data efficiency through principled parameter sharing. Such works have mostly focused on group equivariant convolutions, building on the result that group equivariant linear maps are necessarily convolutions. In this work, we extend the scope of the literature to self-attention, that is emerging as a prominent building block of deep learning models. We propose the LieTransformer, an architecture composed of LieSelfAttention layers that are equivariant to arbitrary Lie groups and their discrete subgroups. We demonstrate the generality of our approach by showing experimental results that are competitive to baseline methods on a wide range of tasks: shape counting on point clouds, molecular property regression and modelling particle trajectories under Hamiltonian dynamics.}
}


@InProceedings{pmlr-v139-holderrieth21a,
  title = 	 {Equivariant Learning of Stochastic Fields: Gaussian Processes and Steerable Conditional Neural Processes},
  author =       {Holderrieth, Peter and Hutchinson, Michael J and Teh, Yee Whye},
  booktitle = 	 {ICML},
  pages = 	 {4297--4307},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/holderrieth21a/holderrieth21a.pdf},
  
  abstract = 	 {Motivated by objects such as electric fields or fluid streams, we study the problem of learning stochastic fields, i.e. stochastic processes whose samples are fields like those occurring in physics and engineering. Considering general transformations such as rotations and reflections, we show that spatial invariance of stochastic fields requires an inference model to be equivariant. Leveraging recent advances from the equivariance literature, we study equivariance in two classes of models. Firstly, we fully characterise equivariant Gaussian processes. Secondly, we introduce Steerable Conditional Neural Processes (SteerCNPs), a new, fully equivariant member of the Neural Process family. In experiments with Gaussian process vector fields, images, and real-world weather data, we observe that SteerCNPs significantly improve the performance of previous models and equivariance leads to improvements in transfer learning tasks.}
}

@misc{miao2021intelvaes,
      title={InteL-VAEs: Adding Inductive Biases to Variational Auto-Encoders via Intermediary Latents}, 
      author={Ning Miao and Emile Mathieu and N. Siddharth and Yee Whye Teh and Tom Rainforth},
      year={2021},
      eprint={2106.13746},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@misc{chau2021bayesimp,
      title={BayesIMP: Uncertainty Quantification for Causal Data Fusion}, 
      author={Siu Lun Chau and Jean-François Ton and Javier González and Yee Whye Teh and Dino Sejdinovic},
      year={2021},
      eprint={2106.03477},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@misc{bloemreddy2020probabilistic,
      title={Probabilistic symmetries and invariant neural networks}, 
      author={Benjamin Bloem-Reddy and Yee Whye Teh},
      year={2020},
      eprint={1901.06082},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@misc{mikheeva2021aligned,
      title={Aligned Multi-Task Gaussian Process}, 
      author={Olga Mikheeva and Ieva Kazlauskaite and Adam Hartshorne and Hedvig Kjellström and Carl Henrik Ek and Neill D. F. Campbell},
      year={2021},
      eprint={2110.15761},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}


@InProceedings{pmlr-v108-ustyuzhaninov20a,
  title = 	 {Monotonic Gaussian Process Flows},
  author =       {Ustyuzhaninov, Ivan and Kazlauskaite, Ieva and Ek, Carl Henrik and Campbell, Neill},
  booktitle = 	 {Proceedings of the Twenty Third International Conference on Artificial Intelligence and Statistics},
  pages = 	 {3057--3067},
  year = 	 {2020},
  editor = 	 {Chiappa, Silvia and Calandra, Roberto},
  volume = 	 {108},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {26--28 Aug},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v108/ustyuzhaninov20a/ustyuzhaninov20a.pdf},
  
  abstract = 	 {We propose a new framework for imposing monotonicity constraints in a Bayesian non-parametric setting based on numerical solutions of stochastic differential equations. We derive a nonparametric model of monotonic functions that allows for interpretable priors and principled quantification of hierarchical uncertainty. We demonstrate the efficacy of the proposed model by providing competitive results to other probabilistic monotonic models on a number of benchmark functions. In addition, we consider the utility of a monotonic random process as a part of a hierarchical probabilistic model; we examine the task of temporal alignment of time-series data where it is beneficial to use a monotonic random process in order to preserve the uncertainty in the temporal warpings. }
}

@misc{le2021hybrid,
      title={Hybrid Memoised Wake-Sleep: Approximate Inference at the Discrete-Continuous Interface}, 
      author={Tuan Anh Le and Katherine M. Collins and Luke Hewitt and Kevin Ellis and Siddharth N and Samuel J. Gershman and Joshua B. Tenenbaum},
      year={2021},
      eprint={2107.06393},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@article{CAMACHO20181581,
title = {Next-Generation Machine Learning for Biological Networks},
journal = {Cell},
volume = {173},
number = {7},
pages = {1581-1592},
year = {2018},
issn = {0092-8674},
doi = {https://doi.org/10.1016/j.cell.2018.05.015},

author = {Diogo M. Camacho and Katherine M. Collins and Rani K. Powers and James C. Costello and James J. Collins},
keywords = {Machine leaning, deep learning, systems biology, synthetic biology, network biology, neural networks},
abstract = {Machine learning, a collection of data-analytical techniques aimed at building predictive models from multi-dimensional datasets, is becoming integral to modern biological research. By enabling one to generate models that learn from large datasets and make predictions on likely outcomes, machine learning can be used to study complex cellular systems such as biological networks. Here, we provide a primer on machine learning for life scientists, including an introduction to deep learning. We discuss opportunities and challenges at the intersection of machine learning and network biology, which could impact disease biology, drug discovery, microbiome research, and synthetic biology.}
}

@article{du2021learning,
  title={Learning Signal-Agnostic Implicit Manifolds},
  author={Du, Yilun and Collins, Katherine and Tenenbaum, Josh and Sitzmann, Vincent},
  journal={NeurIPS},
  volume={34},
  year={2021}
}


@InProceedings{pmlr-v119-bodin20a,
  title = 	 {Modulating Surrogates for {B}ayesian Optimization},
  author =       {Bodin, Erik and Kaiser, Markus and Kazlauskaite, Ieva and Dai, Zhenwen and Campbell, Neill and Ek, Carl Henrik},
  booktitle = 	 {ICML},
  pages = 	 {970--979},
  year = 	 {2020},
  editor = 	 {III, Hal Daumé and Singh, Aarti},
  volume = 	 {119},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {13--18 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v119/bodin20a/bodin20a.pdf},
  
  abstract = 	 {Bayesian optimization (BO) methods often rely on the assumption that the objective function is well-behaved, but in practice, this is seldom true for real-world objectives even if noise-free observations can be collected. Common approaches, which try to model the objective as precisely as possible, often fail to make progress by spending too many evaluations modeling irrelevant details. We address this issue by proposing surrogate models that focus on the well-behaved structure in the objective function, which is informative for search, while ignoring detrimental structure that is challenging to model from few observations. First, we demonstrate that surrogate models with appropriate noise distributions can absorb challenging structures in the objective function by treating them as irreducible uncertainty. Secondly, we show that a latent Gaussian process is an excellent surrogate for this purpose, comparing with Gaussian processes with standard noise distributions. We perform numerous experiments on a range of BO benchmarks and find that our approach improves reliability and performance when faced with challenging objective functions.}
}

@misc{cutajar2019deep,
      title={Deep Gaussian Processes for Multi-fidelity Modeling}, 
      author={Kurt Cutajar and Mark Pullin and Andreas Damianou and Neil Lawrence and Javier González},
      year={2019},
      eprint={1903.07320},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@misc{lawrence2019data,
      title={Data Science and Digital Systems: The 3Ds of Machine Learning Systems Design}, 
      author={Neil D. Lawrence},
      year={2019},
      eprint={1903.11241},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{paleyes2021emulation,
      title={Emulation of physical processes with Emukit}, 
      author={Andrei Paleyes and Mark Pullin and Maren Mahsereci and Cliff McCollum and Neil D. Lawrence and Javier Gonzalez},
      year={2021},
      eprint={2110.13293},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{DBLP:journals/corr/LakeLT16,
  author    = {Brenden M. Lake and
               Neil D. Lawrence and
               Joshua B. Tenenbaum},
  title     = {The Emergence of Organizing Structure in Conceptual Representation},
  journal   = {CoRR},
  volume    = {abs/1611.09384},
  year      = {2016},
  
  eprinttype = {arXiv},
  eprint    = {1611.09384},
  timestamp = {Mon, 13 Aug 2018 16:49:16 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/LakeLT16.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc{niu2018intrinsic,
      title={Intrinsic Gaussian processes on complex constrained domains}, 
      author={Mu Niu and Pokman Cheung and Lizhen Lin and Zhenwen Dai and Neil Lawrence and David Dunson},
      year={2018},
      eprint={1801.01061},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@misc{flennerhag2019transferring,
      title={Transferring Knowledge across Learning Processes}, 
      author={Sebastian Flennerhag and Pablo G. Moreno and Neil D. Lawrence and Andreas Damianou},
      year={2019},
      eprint={1812.01054},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{DBLP:journals/corr/abs-2011-07586,
  author    = {Umang Bhatt and
               Yunfeng Zhang and
               Javier Antor{\'{a}}n and
               Q. Vera Liao and
               Prasanna Sattigeri and
               Riccardo Fogliato and
               Gabrielle Gauthier Melan{\c{c}}on and
               Ranganath Krishnan and
               Jason Stanley and
               Omesh Tickoo and
               Lama Nachman and
               Rumi Chunara and
               Adrian Weller and
               Alice Xiang},
  title     = {Uncertainty as a Form of Transparency: Measuring, Communicating, and
               Using Uncertainty},
  journal   = {CoRR},
  volume    = {abs/2011.07586},
  year      = {2020},
  
  eprinttype = {arXiv},
  eprint    = {2011.07586},
  timestamp = {Wed, 18 Nov 2020 16:48:35 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2011-07586.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@ARTICLE{Kennedy00bayesiancalibration,
    author = {Marc C. Kennedy and Anthony O'Hagan},
    title = {Bayesian Calibration of Computer Models},
    journal = {Journal of the Royal Statistical Society, Series B, Methodological},
    year = {2000},
    volume = {63},
    pages = {425--464}
}

@article{oakley2010shelf,
  title={SHELF: the Sheffield elicitation framework (version 2.0)},
  author={Oakley, Jeremy E and O’Hagan, Anthony},
  journal={School of Mathematics and Statistics, University of Sheffield, UK},
  year={2010}
}

@misc{augMix,
  doi = {10.48550/ARXIV.1912.02781},
  
  
  
  author = {Hendrycks, Dan and Mu, Norman and Cubuk, Ekin D. and Zoph, Barret and Gilmer, Justin and Lakshminarayanan, Balaji},
  
  keywords = {Machine Learning (stat.ML), Computer Vision and Pattern Recognition (cs.CV), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {AugMix: A Simple Data Processing Method to Improve Robustness and Uncertainty},
  
  publisher = {arXiv},
  
  year = {2019},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@inproceedings{disagreement,
author = {Gordon, Mitchell L. and Zhou, Kaitlyn and Patel, Kayur and Hashimoto, Tatsunori and Bernstein, Michael S.},
title = {The Disagreement Deconvolution: Bringing Machine Learning Performance Metrics In Line With Reality},
year = {2021},
isbn = {9781450380966},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},

doi = {10.1145/3411764.3445423},
abstract = { Machine learning classifiers for human-facing tasks such as comment toxicity and misinformation often score highly on metrics such as ROC AUC but are received poorly in practice. Why this gap? Today, metrics such as ROC AUC, precision, and recall are used to measure technical performance; however, human-computer interaction observes that evaluation of human-facing systems should account for people’s reactions to the system. In this paper, we introduce a transformation that more closely aligns machine learning classification metrics with the values and methods of user-facing performance measures. The disagreement deconvolution takes in any multi-annotator (e.g., crowdsourced) dataset, disentangles stable opinions from noise by estimating intra-annotator consistency, and compares each test set prediction to the individual stable opinions from each annotator. Applying the disagreement deconvolution to existing social computing datasets, we find that current metrics dramatically overstate the performance of many human-facing machine learning tasks: for example, performance on a comment toxicity task is corrected from .95 to .73 ROC AUC. },
booktitle = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems},
articleno = {388},
numpages = {14},
location = {Yokohama, Japan},
series = {CHI '21}
}



@article{sharot2011optimism,
  title={The optimism bias},
  author={Sharot, Tali},
  journal={Current biology},
  volume={21},
  number={23},
  pages={R941--R945},
  year={2011},
  publisher={Elsevier}
}

@article{lichtenstein1977calibration,
  title={Calibration of probabilities: The state of the art},
  author={Lichtenstein, Sarah and Fischhoff, Baruch and Phillips, Lawrence D},
  journal={Decision making and change in human affairs},
  pages={275--324},
  year={1977},
  publisher={Springer}
}
@inproceedings{pennington2014glove,
  author = {Jeffrey Pennington and Richard Socher and Christopher D. Manning},
  booktitle = {Empirical Methods in Natural Language Processing (EMNLP)},
  title = {GloVe: Global Vectors for Word Representation},
  year = {2014},
  pages = {1532--1543},
  
}

@electronic{jsPsych,
  added-at = {2016-01-22T16:32:01.000+0100},
  author = {de Leeuw, Josh},
  
  interhash = {a6d772cc484e07ed4e54fea52f753561},
  intrahash = {64e25d1ccc2173722fe8c93f325cdb80},
  keywords = {behavioral_science javascript webdevelopment},
  timestamp = {2016-01-22T16:32:01.000+0100},
  title = {jsPsych},
  
  year = 2016
}



@misc{sontagActive,
  doi = {10.48550/ARXIV.2207.09584},
  
  
  
  author = {Charusaie, Mohammad-Amin and Mozannar, Hussein and Sontag, David and Samadi, Samira},
  
  keywords = {Machine Learning (cs.LG), Artificial Intelligence (cs.AI), Human-Computer Interaction (cs.HC), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Sample Efficient Learning of Predictors that Complement Humans},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@article{chen2021goldilocks,
  title={Goldilocks: Consistent crowdsourced scalar annotations with relative uncertainty},
  author={Chen, Quan Ze and Weld, Daniel S and Zhang, Amy X},
  journal={Proceedings of the ACM on Human-Computer Interaction},
  volume={5},
  number={CSCW2},
  pages={1--25},
  year={2021},
  publisher={ACM New York, NY, USA}
}




@article{shorten2019survey,
  title={A survey on image data augmentation for deep learning},
  author={Shorten, Connor and Khoshgoftaar, Taghi M},
  journal={Journal of big data},
  volume={6},
  number={1},
  pages={1--48},
  year={2019},
  publisher={SpringerOpen}
}


@article{hendrycks2018deep,
  title={Deep anomaly detection with outlier exposure},
  author={Hendrycks, Dan and Mazeika, Mantas and Dietterich, Thomas},
  journal={arXiv preprint arXiv:1812.04606},
  year={2018}
}

@article{nguyen2015posterior,
  title={Posterior calibration and exploratory analysis for natural language processing models},
  author={Nguyen, Khanh and O'Connor, Brendan},
  journal={arXiv preprint arXiv:1508.05154},
  year={2015}
}



@article{northcutt2021pervasive,
  title={Pervasive label errors in test sets destabilize machine learning benchmarks},
  author={Northcutt, Curtis G and Athalye, Anish and Mueller, Jonas},
  journal={arXiv preprint arXiv:2103.14749},
  year={2021}
}


@article{steyversCrowd,
  title={The Wisdom of Crowds with Communication},
  author={Brent Miller and Mark Steyvers},
  journal={Cognitive Science},
  year={2011},
  volume={33}
}

@article{imagenetReaLH,
  author    = {Lucas Beyer and
               Olivier J. H{\'{e}}naff and
               Alexander Kolesnikov and
               Xiaohua Zhai and
               A{\"{a}}ron van den Oord},
  title     = {Are we done with ImageNet?},
  journal   = {CoRR},
  volume    = {abs/2006.07159},
  year      = {2020},
  
  eprinttype = {arXiv},
  eprint    = {2006.07159},
  timestamp = {Fri, 20 Nov 2020 14:04:05 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2006-07159.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{callForHumanStudies,
title={Request for Information (RFI) on Implementing the
Initial Findings and Recommendations of the National
Artificial Intelligence Research Resource Task Force:
Response},
author={Shavit, Yonadav and Kaushik, Divyansh and Lipton, Zachary C and Bowman, Samuel R. and Goldner, Kira},
journal={Federal Register Notice 87 FR 31914},
year={2022}}


@article{liptonHumans,
  title={Resolving the Human Subjects Status of Machine Learning's Crowdworkers},
  author={Kaushik, Divyansh and Lipton, Zachary C and London, Alex John},
  journal={arXiv preprint arXiv:2206.04039},
  year={2022}
}


@inproceedings{stochasticParrots,
author = {Bender, Emily M. and Gebru, Timnit and McMillan-Major, Angelina and Shmitchell, Shmargaret},
title = {On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?},
year = {2021},
isbn = {9781450383097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},

doi = {10.1145/3442188.3445922},
abstract = {The past 3 years of work in NLP have been characterized by the development and deployment of ever larger language models, especially for English. BERT, its variants, GPT-2/3, and others, most recently Switch-C, have pushed the boundaries of the possible both through architectural innovations and through sheer size. Using these pretrained models and the methodology of fine-tuning them for specific tasks, researchers have extended the state of the art on a wide array of tasks as measured by leaderboards on specific benchmarks for English. In this paper, we take a step back and ask: How big is too big? What are the possible risks associated with this technology and what paths are available for mitigating those risks? We provide recommendations including weighing the environmental and financial costs first, investing resources into curating and carefully documenting datasets rather than ingesting everything on the web, carrying out pre-development exercises evaluating how the planned approach fits into research and development goals and supports stakeholder values, and encouraging research directions beyond ever larger language models.},
booktitle = {Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency},
pages = {610–623},
numpages = {14},
location = {Virtual Event, Canada},
series = {FAccT '21}
}

  



@article{datasetUse,
  author    = {Amandalynne Paullada and
               Inioluwa Deborah Raji and
               Emily M. Bender and
               Emily Denton and
               Alex Hanna},
  title     = {Data and its (dis)contents: {A} survey of dataset development and
               use in machine learning research},
  journal   = {CoRR},
  volume    = {abs/2012.05345},
  year      = {2020},
  
  eprinttype = {arXiv},
  eprint    = {2012.05345},
  timestamp = {Sat, 02 Jan 2021 15:43:30 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2012-05345.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{scalingLLM,
  author    = {Jared Kaplan and
               Sam McCandlish and
               Tom Henighan and
               Tom B. Brown and
               Benjamin Chess and
               Rewon Child and
               Scott Gray and
               Alec Radford and
               Jeffrey Wu and
               Dario Amodei},
  title     = {Scaling Laws for Neural Language Models},
  journal   = {CoRR},
  volume    = {abs/2001.08361},
  year      = {2020},
  
  eprinttype = {arXiv},
  eprint    = {2001.08361},
  timestamp = {Wed, 03 Jun 2020 10:55:13 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2001-08361.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{marcus2018deep,
  title={Deep learning: A critical appraisal},
  author={Marcus, Gary},
  journal={arXiv preprint arXiv:1801.00631},
  year={2018}
}


@article{vote,
Author = {Levin, Jonathan and Nalebuff, Barry},
Title = {An Introduction to Vote-Counting Schemes},
Journal = {Journal of Economic Perspectives},
Volume = {9},
Number = {1},
Year = {1995},
Month = {March},
Pages = {3-26},
DOI = {10.1257/jep.9.1.3}}



@article{kahneman1996reality,
  title={On the reality of cognitive illusions},
  author={Tversky, Amos and Kahneman, Daniel},
  journal={Psychological Review},
  volume={103},
  number={3},
  pages={582--591},
  year={1996}
}



@article{hinton2012improving,
  title={Improving neural networks by preventing co-adaptation of feature detectors},
  author={Hinton, Geoffrey E and Srivastava, Nitish and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan R},
  journal={arXiv preprint arXiv:1207.0580},
  year={2012}
}


@article{lake2017building,
  title={Building machines that learn and think like people},
  author={Lake, Brenden M and Ullman, Tomer D and Tenenbaum, Joshua B and Gershman, Samuel J},
  journal={Behavioral and brain sciences},
  volume={40},
  year={2017},
  publisher={Cambridge University Press}
}



@article{fairmixup,
  author    = {Ching{-}Yao Chuang and
               Youssef Mroueh},
  title     = {Fair Mixup: Fairness via Interpolation},
  journal   = {CoRR},
  volume    = {abs/2103.06503},
  year      = {2021},
  
  eprinttype = {arXiv},
  eprint    = {2103.06503},
  timestamp = {Tue, 16 Mar 2021 11:26:59 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2103-06503.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@inproceedings{medImageDatasetGAN,
author = {Zong Fan and Varun Kelkar and Mark A. Anastasio and Hua Li},
title = {{Application of DatasetGAN in medical imaging: preliminary studies}},
volume = {12032},
booktitle = {Medical Imaging 2022: Image Processing},
editor = {Olivier Colliot and Ivana Išgum},
organization = {International Society for Optics and Photonics},
publisher = {SPIE},
pages = {452 -- 458},
keywords = {Deep learning, GAN, DatasetGAN, Medical imaging analysis, Image segmentation},
year = {2022},
doi = {10.1117/12.2611191},
}

@article{humanAnnotationsNeeded,
  title={On The State of Data In Computer Vision: Human Annotations Remain Indispensable for Developing Deep Learning Models},
  author={Emam, Zeyad and Kondrich, Andrew and Harrison, Sasha and Lau, Felix and Wang, Yushi and Kim, Aerin and Branson, Elliot},
  journal={arXiv preprint arXiv:2108.00114},
  year={2021}
}


@InProceedings{simclr,
  title = 	 {A Simple Framework for Contrastive Learning of Visual Representations},
  author =       {Chen, Ting and Kornblith, Simon and Norouzi, Mohammad and Hinton, Geoffrey},
  booktitle = 	 {ICML},
  pages = 	 {1597--1607},
  year = 	 {2020},
  editor = 	 {III, Hal Daumé and Singh, Aarti},
  volume = 	 {119},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {13--18 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v119/chen20j/chen20j.pdf},
  
  abstract = 	 {This paper presents SimCLR: a simple framework for contrastive learning of visual representations. We simplify recently proposed contrastive self-supervised learning algorithms without requiring specialized architectures or a memory bank. In order to understand what enables the contrastive prediction tasks to learn useful representations, we systematically study the major components of our framework. We show that (1) composition of data augmentations plays a critical role in defining effective predictive tasks, (2) introducing a learnable nonlinear transformation between the representation and the contrastive loss substantially improves the quality of the learned representations, and (3) contrastive learning benefits from larger batch sizes and more training steps compared to supervised learning. By combining these findings, we are able to considerably outperform previous methods for self-supervised and semi-supervised learning on ImageNet. A linear classifier trained on self-supervised representations learned by SimCLR achieves 76.5% top-1 accuracy, which is a 7% relative improvement over previous state-of-the-art, matching the performance of a supervised ResNet-50. When fine-tuned on only 1% of the labels, we achieve 85.8% top-5 accuracy, outperforming AlexNet with 100X fewer labels.}
}

@article{sohn2022genlabel,
  title={GenLabel: Mixup Relabeling using Generative Models},
  author={Sohn, Jy-yong and Shang, Liang and Chen, Hongxu and Moon, Jaekyun and Papailiopoulos, Dimitris and Lee, Kangwook},
  journal={arXiv preprint arXiv:2201.02354},
  year={2022}
}

@article{fel2022aligning,
  title={Harmonizing the object recognition strategies of deep neural networks with humans},
  author={Fel, Thomas and Felipe, Ivan and Linsley, Drew and Serre, Thomas},
  journal={Advances in Neural Information Processing Systems (NeurIPS)},
  year={2022}
}

@article{fel2022aligning,
  title={Harmonizing the object recognition strategies of deep neural networks with humans},
  author={Fel, Thomas and Felipe, Ivan and Linsley, Drew and Serre, Thomas},
  journal={Advances in Neural Information Processing Systems (NeurIPS)},
  year={2022}
}

@article{distillMixup,
  author    = {Dongdong Wang and
               Yandong Li and
               Liqiang Wang and
               Boqing Gong},
  title     = {Neural Networks Are More Productive Teachers Than Human Raters: Active
               Mixup for Data-Efficient Knowledge Distillation from a Blackbox Model},
  journal   = {CoRR},
  volume    = {abs/2003.13960},
  year      = {2020},
  
  eprinttype = {arXiv},
  eprint    = {2003.13960},
  timestamp = {Thu, 14 Oct 2021 09:15:56 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2003-13960.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}



@article{doi:10.1080/00031305.2018.1529623,
author = {Naomi C. Brownstein and Thomas A. Louis and Anthony O’Hagan and Jane Pendergast},
title = {The Role of Expert Judgment in Statistical Inference and Evidence-Based Decision-Making},
journal = {The American Statistician},
volume = {73},
number = {sup1},
pages = {56-68},
year  = {2019},
publisher = {Taylor & Francis},
doi = {10.1080/00031305.2018.1529623},
    note ={PMID: 31057338},
eprint = { 
        https://doi.org/10.1080/00031305.2018.1529623
    
}

}

@article{expertElicitation,
author = {Anthony O’Hagan},
title = {Expert Knowledge Elicitation: Subjective but Scientific},
journal = {The American Statistician},
volume = {73},
number = {sup1},
pages = {69-81},
year  = {2019},
publisher = {Taylor & Francis},
doi = {10.1080/00031305.2018.1518265},

eprint = { 
        https://doi.org/10.1080/00031305.2018.1518265
    
}
}

@inproceedings{augustin2017bayesian,
  title={Bayesian Aggregation of Categorical Distributions with Applications in Crowdsourcing.},
  author={Augustin, Alexandry and Venanzi, Matteo and Rogers, Alex and Jennings, Nicholas R},
  booktitle={IJCAI},
  pages={1411--1417},
  year={2017}
}

@article{prelec2004bayesian,
  title={A Bayesian truth serum for subjective data},
  author={Prelec, Drazen},
  journal={Science},
  volume={306},
  number={5695},
  pages={462--466},
  year={2004}
}
@article{yang2015multi,
  title={Multi-class active learning by uncertainty sampling with diversity maximization},
  author={Yang, Yi and Ma, Zhigang and Nie, Feiping and Chang, Xiaojun and Hauptmann, Alexander G},
  journal={International Journal of Computer Vision},
  volume={113},
  number={2},
  pages={113--127},
  year={2015},
  publisher={Springer}
}

@inproceedings{bhatt2021uncertainty,
  title={Uncertainty as a form of transparency: Measuring, communicating, and using uncertainty},
  author={Bhatt, Umang and Antor{\'a}n, Javier and Zhang, Yunfeng and Liao, Q Vera and Sattigeri, Prasanna and Fogliato, Riccardo and Melan{\c{c}}on, Gabrielle and Krishnan, Ranganath and Stanley, Jason and Tickoo, Omesh and others},
  booktitle={Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society},
  pages={401--413},
  year={2021}
}

@book{hastie2009rational,
  title={Rational choice in an uncertain world: The psychology of judgment and decision making},
  author={Hastie, Reid and Dawes, Robyn M},
  year={2009},
  publisher={Sage Publications}
}


@article{mellers1998judgment,
  title={Judgment and decision making},
  author={Mellers, Barbara A and Schwartz, Alan and Cooke, Alan DJ},
  journal={Annual review of psychology},
  volume={49},
  number={1},
  pages={447--477},
  year={1998}
}

@article{10.1371/journal.pone.0229132,
    doi = {10.1371/journal.pone.0229132},
    author = {Okamura, Kazuo AND Yamada, Seiji},
    journal = {PLOS ONE},
    publisher = {Public Library of Science},
    title = {Adaptive trust calibration for human-AI collaboration},
    year = {2020},
    month = {02},
    volume = {15},
    
    pages = {1-20},
    abstract = {Safety and efficiency of human-AI collaboration often depend on how humans could appropriately calibrate their trust towards the AI agents. Over-trusting the autonomous system sometimes causes serious safety issues. Although many studies focused on the importance of system transparency in keeping proper trust calibration, the research in detecting and mitigating improper trust calibration remains very limited. To fill these research gaps, we propose a method of adaptive trust calibration that consists of a framework for detecting the inappropriate calibration status by monitoring the user’s reliance behavior and cognitive cues called “trust calibration cues” to prompt the user to reinitiate trust calibration. We evaluated our framework and four types of trust calibration cues in an online experiment using a drone simulator. A total of 116 participants performed pothole inspection tasks by using the drone’s automatic inspection, the reliability of which could fluctuate depending upon the weather conditions. The participants needed to decide whether to rely on automatic inspection or to do the inspection manually. The results showed that adaptively presenting simple cues could significantly promote trust calibration during over-trust.},
    number = {2},

}

@article{TOMSETT2020100049,
title = {Rapid Trust Calibration through Interpretable and Uncertainty-Aware AI},
journal = {Patterns},
volume = {1},
number = {4},
pages = {100049},
year = {2020},
issn = {2666-3899},
doi = {https://doi.org/10.1016/j.patter.2020.100049},

author = {Richard Tomsett and Alun Preece and Dave Braines and Federico Cerutti and Supriyo Chakraborty and Mani Srivastava and Gavin Pearson and Lance Kaplan},
keywords = {DSML 1: Concept: Basic principles of a new data science output observed and reported},
abstract = {Artificial intelligence (AI) systems hold great promise as decision-support tools, but we must be able to identify and understand their inevitable mistakes if they are to fulfill this potential. This is particularly true in domains where the decisions are high-stakes, such as law, medicine, and the military. In this Perspective, we describe the particular challenges for AI decision support posed in military coalition operations. These include having to deal with limited, low-quality data, which inevitably compromises AI performance. We suggest that these problems can be mitigated by taking steps that allow rapid trust calibration so that decision makers understand the AI system's limitations and likely failures and can calibrate their trust in its outputs appropriately. We propose that AI services can achieve this by being both interpretable and uncertainty-aware. Creating such AI systems poses various technical and human factors challenges. We review these challenges and recommend directions for future research.}
}

@inproceedings{arazo2019unsupervised,
  title={Unsupervised label noise modeling and loss correction},
  author={Arazo, Eric and Ortego, Diego and Albert, Paul and O’Connor, Noel and McGuinness, Kevin},
  booktitle={ICML},
  pages={312--321},
  year={2019},
  organization={PMLR}
}



@article{fortuin2022priors,
  title={Priors in bayesian deep learning: A review},
  author={Fortuin, Vincent},
  journal={International Statistical Review},
  year={2022},
  publisher={Wiley Online Library}
}


@paper{AAAI136451,
	author = {Goran Radanovic and Boi Faltings},
	title = {A Robust Bayesian Truth Serum for Non-Binary Signals},
	conference = {AAAI Conference on Artificial Intelligence},
	year = {2013},
	keywords = {Incentive Compatibility, Truth Elicitation, Mechanism Design},
	abstract = {Several mechanisms have been proposed for incentivizing truthful reports of a private signals owned by rational agents, among them the peer prediction method and the Bayesian truth serum. The robust Bayesian truth serum (RBTS) for small populations and binary signals is particularly interesting since it does not require a common prior to be known to the mechanism. We further analyze the problem of the common prior not known to the mechanism and give several results regarding the restrictions that need to be placed in order to have an incentive-compatible mechanism. Moreover, we construct a Bayes-Nash incentive-compatible scheme called multi-valued RBTS that generalizes RBTS to operate on both small populations and non-binary signals.},

	
}



@article{DBLP:journals/corr/abs-2011-11015,
  author    = {Brett D. Roads and
               Bradley C. Love},
  title     = {Enriching ImageNet with Human Similarity Judgments and Psychological
               Embeddings},
  journal   = {CoRR},
  volume    = {abs/2011.11015},
  year      = {2020},
  eprinttype = {arXiv},
  eprint    = {2011.11015},
  timestamp = {Wed, 25 Nov 2020 17:55:02 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2011-11015.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@INPROCEEDINGS{activeLearningMulti,

  author={Joshi, Ajay J. and Porikli, Fatih and Papanikolopoulos, Nikolaos},

  booktitle={2009 IEEE Conference on Computer Vision and Pattern Recognition}, 

  title={Multi-class active learning for image classification}, 

  year={2009},

  volume={},

  number={},

  pages={2372-2379},

  doi={10.1109/CVPR.2009.5206627}}


@inproceedings{efficientElic,
author = {Chung, John Joon Young and Song, Jean Y. and Kutty, Sindhu and Hong, Sungsoo (Ray) and Kim, Juho and Lasecki, Walter S.},
title = {Efficient Elicitation Approaches to Estimate Collective Crowd Answers},
booktitle = {CSCW},
year = {2019},
}



@article{resnet,
  title={Deep residual learning for image recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={CVPR},
  pages={770--778},
  year={2016}
}

@inproceedings{he2016deep,
  title={Deep residual learning for image recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={CVPR},
  pages={770--778},
  year={2016}
}

@article{laidlaw2021uncertain,
  title={Uncertain Decisions Facilitate Better Preference Learning},
  author={Laidlaw, Cassidy and Russell, Stuart},
  journal={NeurIPS},
  volume={34},
  pages={15070--15083},
  year={2021}
}



@article{russell2008labelme,
  title={LabelMe: a database and web-based tool for image annotation},
  author={Russell, Bryan C and Torralba, Antonio and Murphy, Kevin P and Freeman, William T},
  journal={International journal of computer vision},
  volume={77},
  number={1},
  pages={157--173},
  year={2008},
  publisher={Springer}
}


@article{platts2020toleranceMedUnc,
  title={Tolerance of uncertainty and the practice of emergency medicine},
  author={Platts-Mills, Timothy F and Nagurney, Justine M and Melnick, Edward R},
  journal={Annals of emergency medicine},
  volume={75},
  number={6},
  pages={715--720},
  year={2020},
  publisher={Elsevier}
}


@article{medUncertainty,
  title={Reviewing intuitive decision-making and uncertainty: the implications for medical education},
  author={Hall, Katherine H},
  journal={Medical education},
  volume={36},
  number={3},
  pages={216--224},
  year={2002},
  publisher={Wiley Online Library}
}


@inproceedings{massiceti2021orbit,
  title={Orbit: A real-world few-shot dataset for teachable object recognition},
  author={Massiceti, Daniela and Zintgraf, Luisa and Bronskill, John and Theodorou, Lida and Harris, Matthew Tobias and Cutrell, Edward and Morrison, Cecily and Hofmann, Katja and Stumpf, Simone},
  booktitle={ICCV},
  pages={10818--10828},
  year={2021}
}

@inproceedings{heidemann2023concept,
  title={Concept Correlation and Its Effects on Concept-Based Models},
  author={Heidemann, Lena and Monnet, Maureen and Roscher, Karsten},
  booktitle={Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision},
  pages={4780--4788},
  year={2023}
}


@article{shin2023closer,
  title={A Closer Look at the Intervention Procedure of Concept Bottleneck Models},
  author={Shin, Sungbin and Jo, Yohan and Ahn, Sungsoo and Lee, Namhoon},
  journal={arXiv preprint arXiv:2302.14260},
  year={2023}
}

@inproceedings{sheth2022learning,
  title={Learning from uncertain concepts via test time interventions},
  author={Sheth, Ivaxi and Rahman, Aamer Abdul and Sevyeri, Laya Rafiee and Havaei, Mohammad and Kahou, Samira Ebrahimi},
  booktitle={Workshop on Trustworthy and Socially Responsible Machine Learning, NeurIPS 2022}, year={2022}
}


@article{cox2021diagnostic,
  title={Diagnostic uncertainty in primary care: what is known about its communication, and what are the associated ethical issues?},
  author={Cox, Caitr{\'\i}ona L and Miller, Benjamin M and Kuhn, Isla and Fritz, Zo{\"e}},
  journal={Family practice},
  volume={38},
  number={5},
  pages={654--668},
  year={2021},
  publisher={Oxford University Press UK}
}


@inproceedings{irvin2019chexpert,
  title={Chexpert: A large chest radiograph dataset with uncertainty labels and expert comparison},
  author={Irvin, Jeremy and Rajpurkar, Pranav and Ko, Michael and Yu, Yifan and Ciurea-Ilcus, Silviana and Chute, Chris and Marklund, Henrik and Haghgoo, Behzad and Ball, Robyn and Shpanskaya, Katie and others},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={33},
  number={01},
  pages={590--597},
  year={2019}
}



@article{noisyLabels,
  author    = {Jiaheng Wei and
               Zhaowei Zhu and
               Hao Cheng and
               Tongliang Liu and
               Gang Niu and
               Yang Liu},
  title     = {Learning with Noisy Labels Revisited: {A} Study Using Real-World Human
               Annotations},
  journal   = {CoRR},
  volume    = {abs/2110.12088},
  year      = {2021},
  eprinttype = {arXiv},
  eprint    = {2110.12088},
  timestamp = {Fri, 29 Oct 2021 12:48:32 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2110-12088.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{cifar10h,
  title={Capturing human categorization of natural images at scale by combining deep networks and cognitive models},
  author={Battleday, Ruairidh M and Peterson, Joshua C and Griffiths, Thomas L},
  journal={arXiv preprint arXiv:1904.12690},
  year={2019}
}


@InProceedings{Firman_2018_CVPR,
author = {Firman, Michael and Campbell, Neill D. F. and Agapito, Lourdes and Brostow, Gabriel J.},
title = {DiverseNet: When One Right Answer Is Not Enough},
booktitle = {CVPR},
month = {June},
year = {2018}
} 

@inproceedings{koh2020concept,
  title={Concept bottleneck models},
  author={Koh, Pang Wei and Nguyen, Thao and Tang, Yew Siang and Mussmann, Stephen and Pierson, Emma and Kim, Been and Liang, Percy},
  booktitle={ICML},
  pages={5338--5348},
  year={2020},
  organization={PMLR}
}

@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={NeurIPS},
  volume={33},
  pages={1877--1901},
  year={2020}
}

@article{sucholutsky2022informativeness,
  title={On the Informativeness of Supervision Signals},
  author={Sucholutsky, Ilia and Marjieh, Raja and Jacoby, Nori and Griffiths, Thomas L},
  journal={arXiv preprint arXiv:2211.01407},
  year={2022}
}

@article{chauhan2022interactive,
  title={Interactive Concept Bottleneck Models},
  author={Chauhan, Kushal and Tiwari, Rishabh and Freyberg, Jan and Shenoy, Pradeep and Dvijotham, Krishnamurthy},
  journal={arXiv preprint arXiv:2212.07430},
  year={2022}
}



@article{bommasani2021opportunities,
  title={On the opportunities and risks of foundation models},
  author={Bommasani, Rishi and Hudson, Drew A and Adeli, Ehsan and Altman, Russ and Arora, Simran and von Arx, Sydney and Bernstein, Michael S and Bohg, Jeannette and Bosselut, Antoine and Brunskill, Emma and others},
  journal={arXiv preprint arXiv:2108.07258},
  year={2021}
}

@article{peer2017beyond,
  title={Beyond the Turk: Alternative platforms for crowdsourcing behavioral research},
  author={Peer, Eyal and Brandimarte, Laura and Samat, Sonam and Acquisti, Alessandro},
  journal={Journal of Experimental Social Psychology},
  volume={70},
  pages={153--163},
  year={2017},
  publisher={Elsevier}
}


@inproceedings{branson20questions,
  title={Visual recognition with humans in the loop},
  author={Branson, Steve and Wah, Catherine and Schroff, Florian and Babenko, Boris and Welinder, Peter and Perona, Pietro and Belongie, Serge},
  booktitle={European Conference on Computer Vision},
  pages={438--451},
  year={2010},
  organization={Springer}
}

@article{griffiths2015manifesto,
  title={Manifesto for a new (computational) cognitive revolution},
  author={Griffiths, Thomas L},
  journal={Cognition},
  volume={135},
  pages={21--23},
  year={2015},
  publisher={Elsevier}
}


@inproceedings{bondi2022role,
  title={Role of Human-AI Interaction in Selective Prediction},
  author={Bondi, Elizabeth and Koster, Raphael and Sheahan, Hannah and Chadwick, Martin and Bachrach, Yoram and Cemgil, Taylan and Paquet, Ulrich and Dvijotham, Krishnamurthy},
  booktitle={AAAI},
  volume={36},
  number={5},
  pages={5286--5294},
  year={2022}
}

@article{collins2022structured,
  title={Structured, flexible, and robust: benchmarking and improving large language models towards more human-like behavior in out-of-distribution reasoning tasks},
  author={Collins, Katherine M and Wong, Catherine and Feng, Jiahai and Wei, Megan and Tenenbaum, Joshua B},
  journal={arXiv preprint arXiv:2205.05718},
  year={2022}
}



@article{ramaswamy2022overlooked,
  title={Overlooked factors in concept-based explanations: Dataset choice, concept salience, and human capability},
  author={Ramaswamy, Vikram V and Kim, Sunnie SY and Fong, Ruth and Russakovsky, Olga},
  journal={arXiv e-prints},
  pages={arXiv--2207},
  year={2022}
}



@InProceedings{goodfellow2014generative,
  title={Generative adversarial nets},
  author={Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
  booktitle={NeurIPS},
  year={2014}
}

@InProceedings{Sharmanska_2016_CVPR,
author = {Sharmanska, Viktoriia and Hernandez-Lobato, Daniel and Miguel Hernandez-Lobato, Jose and Quadrianto, Novi},
title = {Ambiguity Helps: Classification With Disagreements in Crowdsourced Annotations},
booktitle = {CVPR},
year = {2016}
} 

@article{murray2015posterior,
  title={Posterior probability matching and human perceptual decision making},
  author={Murray, Richard F and Patel, Khushbu and Yee, Alan},
  journal={PLoS computational biology},
  volume={11},
  number={6},
  pages={e1004342},
  year={2015},
  publisher={Public Library of Science San Francisco, CA USA}
}


@article{davani2022dealing,
  title={Dealing with disagreements: Looking beyond the majority vote in subjective annotations},
  author={Davani, Aida Mostafazadeh and D{\'\i}az, Mark and Prabhakaran, Vinodkumar},
  journal={Transactions of the Association for Computational Linguistics},
  volume={10},
  pages={92--110},
  year={2022},
  publisher={MIT Press}
}

@article{nguyen2014learning,
  title={Learning classification models with soft-label information},
  author={Nguyen, Quang and Valizadegan, Hamed and Hauskrecht, Milos},
  journal={Journal of the American Medical Informatics Association},
  volume={21},
  number={3},
  pages={501--508},
  year={2014},
  publisher={Oxford University Press}
}

@article{passonneau2014benefits,
  title={The benefits of a model of annotation},
  author={Passonneau, Rebecca J and Carpenter, Bob},
  journal={Transactions of the Association for Computational Linguistics},
  volume={2},
  pages={311--326},
  year={2014},
  publisher={MIT Press}
}

@article{fei2009imagenet,
  title={ImageNet: Constructing a large-scale image database},
  author={Fei-Fei, Li and Deng, Jia and Li, Kai},
  journal={Journal of vision},
  volume={9},
  number={8},
  pages={1037--1037},
  year={2009},
  publisher={The Association for Research in Vision and Ophthalmology}
}


@inproceedings{wang2021forecast,
  title={Forecast aggregation via peer prediction},
  author={Wang, Juntao and Liu, Yang and Chen, Yiling},
  booktitle={Proceedings of the AAAI Conference on Human Computation and Crowdsourcing},
  volume={9},
  pages={131--142},
  year={2021}
}


@article{sheng2017majority,
  title={Majority voting and pairing with multiple noisy labeling},
  author={Sheng, Victor S and Zhang, Jing and Gu, Bin and Wu, Xindong},
  journal={IEEE Transactions on Knowledge and Data Engineering},
  volume={31},
  number={7},
  pages={1355--1368},
  year={2017},
  publisher={IEEE}
}

@inproceedings{prabhakaran2021releasing,
  title={On Releasing Annotator-Level Labels and Information in Datasets},
  author={Prabhakaran, Vinodkumar and Davani, Aida Mostafazadeh and Diaz, Mark},
  booktitle={Proceedings of The Joint 15th Linguistic Annotation Workshop (LAW) and 3rd Designing Meaning Representations (DMR) Workshop},
  pages={133--138},
  year={2021}
}
  



@article{umangUncertainty,
  author    = {Umang Bhatt and
               Yunfeng Zhang and
               Javier Antor{\'{a}}n and
               Q. Vera Liao and
               Prasanna Sattigeri and
               Riccardo Fogliato and
               Gabrielle Gauthier Melan{\c{c}}on and
               Ranganath Krishnan and
               Jason Stanley and
               Omesh Tickoo and
               Lama Nachman and
               Rumi Chunara and
               Adrian Weller and
               Alice Xiang},
  title     = {Uncertainty as a Form of Transparency: Measuring, Communicating, and
               Using Uncertainty},
  journal   = {CoRR},
  volume    = {abs/2011.07586},
  year      = {2020},
  
  eprinttype = {arXiv},
  eprint    = {2011.07586},
  timestamp = {Wed, 18 Nov 2020 16:48:35 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2011-07586.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
  
 @article{hendrycks2019robustness,
      title={Benchmarking Neural Network Robustness to Common Corruptions and Perturbations},
      author={Hendrycks, Dan and Dietterich, Thomas},
      journal={ICLR},
      year={2019}
    }
    


@inproceedings{weldRelabel,
  author    = {Christopher H. Lin and
               Mausam and
               Daniel S. Weld},
  editor    = {Jeffrey P. Bigham and
               David C. Parkes},
  title     = {To Re(label), or Not To Re(label)},
  booktitle = {Proceedings of the Seconf {AAAI} Conference on Human Computation and
               Crowdsourcing, {HCOMP} 2014, November 2-4, 2014, Pittsburgh, Pennsylvania,
               {USA}},
  publisher = {{AAAI}},
  year      = {2014},
  
  timestamp = {Wed, 10 Feb 2021 08:46:28 +0100},
  biburl    = {https://dblp.org/rec/conf/hcomp/LinMW14.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{liu2017iterative,
  title={Iterative machine teaching},
  author={Liu, Weiyang and Dai, Bo and Humayun, Ahmad and Tay, Charlene and Yu, Chen and Smith, Linda B and Rehg, James M and Song, Le},
  booktitle={ICML},
  year={2017}
}

@inproceedings{oikarinenlabel,
  title={Label-free Concept Bottleneck Models},
  author={Oikarinen, Tuomas and Das, Subhro and Nguyen, Lam M and Weng, Tsui-Wei},
  booktitle={ICLR}, 
  year = {2023},
}

@article{hendrycks2019benchmarking,
  title={Benchmarking neural network robustness to common corruptions and perturbations},
  author={Hendrycks, Dan and Dietterich, Thomas},
  journal={arXiv preprint arXiv:1903.12261},
  year={2019}
}


@inbook{uncTrustModel,
author = {Ovadia, Yaniv and Fertig, Emily and Ren, Jie and Nado, Zachary and Sculley, D. and Nowozin, Sebastian and Dillon, Joshua V. and Lakshminarayanan, Balaji and Snoek, Jasper},
title = {Can You Trust Your Model's Uncertainty? Evaluating Predictive Uncertainty under Dataset Shift},
year = {2019},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Modern machine learning methods including deep learning have achieved great success in predictive accuracy for supervised learning tasks, but may still fall short in giving useful estimates of their predictive uncertainty. Quantifying uncertainty is especially critical in real-world settings, which often involve input distributions that are shifted from the training distribution due to a variety of factors including sample bias and non-stationarity. In such settings, well calibrated uncertainty estimates convey information about when a model's output should (or should not) be trusted. Many probabilistic deep learning methods, including Bayesian-and non-Bayesian methods, have been proposed in the literature for quantifying predictive uncertainty, but to our knowledge there has not previously been a rigorous large-scale empirical comparison of these methods under dataset shift. We present a large-scale benchmark of existing state-of-the-art methods on classification problems and investigate the effect of dataset shift on accuracy and calibration. We find that traditional post-hoc calibration does indeed fall short, as do several other previous methods. However, some methods that marginalize over models give surprisingly strong results across a broad spectrum of tasks.},
booktitle = {Proceedings of the 33rd International Conference on Neural Information Processing Systems},
articleno = {1254},
numpages = {12}
}

@article{liu2020simple,
  title={Simple and principled uncertainty estimation with deterministic deep learning via distance awareness},
  author={Liu, Jeremiah and Lin, Zi and Padhy, Shreyas and Tran, Dustin and Bedrax Weiss, Tania and Lakshminarayanan, Balaji},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={7498--7512},
  year={2020}
}




@misc{languageCBM,
  doi = {10.48550/ARXIV.2211.11158},
  
  url = {https://arxiv.org/abs/2211.11158},
  
  author = {Yang, Yue and Panagopoulou, Artemis and Zhou, Shenghao and Jin, Daniel and Callison-Burch, Chris and Yatskar, Mark},
  
  keywords = {Computer Vision and Pattern Recognition (cs.CV), Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Language in a Bottle: Language Model Guided Concept Bottlenecks for Interpretable Image Classification},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@misc{postHocCBMs,
  doi = {10.48550/ARXIV.2205.15480},
  
  url = {https://arxiv.org/abs/2205.15480},
  
  author = {Yuksekgonul, Mert and Wang, Maggie and Zou, James},
  
  keywords = {Machine Learning (cs.LG), Artificial Intelligence (cs.AI), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Post-hoc Concept Bottleneck Models},
  
  publisher = {ICLR},
  
  year = {2023},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}



@article{DHAMI2022514,
title = {Communicating uncertainty using words and numbers},
journal = {Trends in Cognitive Sciences},
volume = {26},
number = {6},
pages = {514-526},
year = {2022},
issn = {1364-6613},
doi = {https://doi.org/10.1016/j.tics.2022.03.002},
url = {https://www.sciencedirect.com/science/article/pii/S1364661322000602},
author = {Mandeep K. Dhami and David R. Mandel},
keywords = {uncertainty, probability, risk, communication, verbal probabilities, imprecise probabilities},
abstract = {Life in an increasingly information-rich but highly uncertain world calls for an effective means of communicating uncertainty to a range of audiences. Senders prefer to convey uncertainty using verbal (e.g., likely) rather than numeric (e.g., 75% chance) probabilities, even in consequential domains, such as climate science. However, verbal probabilities can convey something other than uncertainty, and senders may exploit this. For instance, senders can maintain credibility after making erroneous predictions. While verbal probabilities afford ease of expression, they can be easily misunderstood, and the potential for miscommunication is not effectively mitigated by assigning (imprecise) numeric probabilities to words. When making consequential decisions, recipients prefer (precise) numeric probabilities.}
}


@article{keren1987facing,
  title={Facing uncertainty in the game of bridge: A calibration study},
  author={Keren, Gideon},
  journal={Organizational Behavior and Human Decision Processes},
  volume={39},
  number={1},
  pages={98--114},
  year={1987},
  publisher={Elsevier}
}

@article{keren1991calibration,
  title={Calibration and probability judgements: Conceptual and methodological issues},
  author={Keren, Gideon},
  journal={Acta psychologica},
  volume={77},
  number={3},
  pages={217--273},
  year={1991},
  publisher={Elsevier}
}


@Inbook{Hartley2018,
author="Hartley, David
and French, Simon",
editor="Dias, Luis C.
and Morton, Alec
and Quigley, John",
title="Elicitation and Calibration: A Bayesian Perspective",
bookTitle="Elicitation: The Science and Art of Structuring Judgement",
year="2018",
publisher="Springer International Publishing",
address="Cham",
pages="119--140",
abstract="There are relatively few published perspectives on processes and procedures for organising the elicitation, aggregation and documentation of expert judgement studies. The few that exist emphasise different aggregation models, but none build a full Bayesian model to combine the judgements of multiple experts into the posterior distribution for a decision maker. Historically, Bayesian concepts have identified issues with current modelling approaches to aggregation, but have led to models that are difficult to implement. Recently Bayesian models have started to become more tractable, so it is timely to reflect on elicitation processes that enable the model to be applied. That is our purpose in this Chapter. In particular, the European Food Safety Authority have provided the most detailed and thorough prescription of the procedures and processes needed to conduct an expert judgement study. We critically review this from a Bayesian perspective, asking how it might need modifying if Bayesian models are included to analyse and aggregate the expert judgements.",
isbn="978-3-319-65052-4",
doi="10.1007/978-3-319-65052-4_6",

}


}
@inproceedings{kim2012bayesian,
  title={Bayesian classifier combination},
  author={Kim, Hyun-Chul and Ghahramani, Zoubin},
  booktitle={Artificial Intelligence and Statistics},
  pages={619--627},
  year={2012},
  organization={PMLR}
}


@InProceedings{pmlr-v119-mozannar20b,
  title = 	 {Consistent Estimators for Learning to Defer to an Expert},
  author =       {Mozannar, Hussein and Sontag, David},
  booktitle = 	 {Proceedings of the 37th International Conference on Machine Learning},
  pages = 	 {7076--7087},
  year = 	 {2020},
  editor = 	 {III, Hal Daumé and Singh, Aarti},
  volume = 	 {119},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {13--18 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v119/mozannar20b/mozannar20b.pdf},
  
  abstract = 	 {Learning algorithms are often used in conjunction with expert decision makers in practical scenarios, however this fact is largely ignored when designing these algorithms. In this paper we explore how to learn predictors that can either predict or choose to defer the decision to a downstream expert. Given only samples of the expert’s decisions, we give a procedure based on learning a classifier and a rejector and analyze it theoretically. Our approach is based on a novel reduction to cost sensitive learning where we give a consistent surrogate loss for cost sensitive learning that generalizes the cross entropy loss. We show the effectiveness of our approach on a variety of experimental tasks.}
}

@misc{madras2018predict,
      title={Predict Responsibly: Improving Fairness and Accuracy by Learning to Defer}, 
      author={David Madras and Toniann Pitassi and Richard Zemel},
      year={2018},
      eprint={1711.06664},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@misc{wilder2020learning,
      title={Learning to Complement Humans}, 
      author={Bryan Wilder and Eric Horvitz and Ece Kamar},
      year={2020},
      eprint={2005.00582},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}

@misc{joshi2021preemptive,
      title={Pre-emptive learning-to-defer for sequential medical decision-making under uncertainty}, 
      author={Shalmali Joshi and Sonali Parbhoo and Finale Doshi-Velez},
      year={2021},
      eprint={2109.06312},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{DBLP:journals/corr/abs-2004-13102,
  author    = {Gagan Bansal and
               Besmira Nushi and
               Ece Kamar and
               Eric Horvitz and
               Daniel S. Weld},
  title     = {Optimizing {AI} for Teamwork},
  journal   = {CoRR},
  volume    = {abs/2004.13102},
  year      = {2020},
  
  eprinttype = {arXiv},
  eprint    = {2004.13102},
  timestamp = {Sat, 02 May 2020 19:17:26 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2004-13102.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{doi:10.1146/annurev-devpsych-070120-014806,
author = {Chu, Junyi and Schulz, Laura E.},
title = {Play, Curiosity, and Cognition},
journal = {Annual Review of Developmental Psychology},
volume = {2},
number = {1},
pages = {317-343},
year = {2020},
doi = {10.1146/annurev-devpsych-070120-014806},
eprint = { 
        https://doi.org/10.1146/annurev-devpsych-070120-014806
    
}
,
    abstract = { Few phenomena in childhood are as compelling—and mystifying—as play. We review five proposals about the relationship between play and development. We believe each captures important aspects of play across species; however, we believe none of them accounts for the extraordinary richness of human play or its connection to distinctively human learning. In thinking about play, we are particularly struck by the profligacy with which children set seemingly arbitrary rewards and incur unnecessary costs. We suggest that researchers take the seeming inutility of play seriously and consider why it might be useful to engage in apparently useless behavior. We propose that humans’ ability to choose arbitrary costs and rewards allows us to pursue novel goals, discover unexpected information, and invent problems we would not otherwise encounter. Because problems impose constraints on search, these invented problems may help solve a big problem: how to generate new ideas and plans in an otherwise infinite search space. }
}

@article{new-facts,
author = {Schulz, Laura},
year = {2012},
month = {12},
pages = {269-94},
title = {Finding New Facts; Thinking New Thoughts},
volume = {43},
journal = {Advances in child development and behavior},
doi = {10.1016/B978-0-12-397919-3.00010-1}
}



@article{kompa2021second,
  title={Second opinion needed: communicating uncertainty in medical machine learning},
  author={Kompa, Benjamin and Snoek, Jasper and Beam, Andrew L},
  journal={npj Digital Medicine},
  volume={4},
  number={1},
  pages={1--6},
  year={2021},
  publisher={Nature Publishing Group}
}

@misc{dutordoir2021deep,
      title={Deep Neural Networks as Point Estimates for Deep Gaussian Processes}, 
      author={Vincent Dutordoir and James Hensman and Mark van der Wilk and Carl Henrik Ek and Zoubin Ghahramani and Nicolas Durrande},
      year={2021},
      eprint={2105.04504},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@misc{baisero2015family,
      title={On a Family of Decomposable Kernels on Sequences}, 
      author={Andrea Baisero and Florian T. Pokorny and Carl Henrik Ek},
      year={2015},
      eprint={1501.06284},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{2019,
   title={A modern retrospective on probabilistic numerics},
   volume={29},
   ISSN={1573-1375},
   url={http://dx.doi.org/10.1007/s11222-019-09902-z},
   DOI={10.1007/s11222-019-09902-z},
   number={6},
   journal={Statistics and Computing},
   publisher={Springer Science and Business Media LLC},
   author={Oates, C. J. and Sullivan, T. J.},
   year={2019},
   month={Oct},
   pages={1335–1351}
}

@misc{hauberg2019bayes,
      title={Only Bayes should learn a manifold (on the estimation of differential geometric structure from data)}, 
      author={Søren Hauberg},
      year={2019},
      eprint={1806.04994},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@misc{gordon2020convolutional,
      title={Convolutional Conditional Neural Processes}, 
      author={Jonathan Gordon and Wessel P. Bruinsma and Andrew Y. K. Foong and James Requeima and Yann Dubois and Richard E. Turner},
      year={2020},
      eprint={1910.13556},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@misc{nguyen2018variational,
      title={Variational Continual Learning}, 
      author={Cuong V. Nguyen and Yingzhen Li and Thang D. Bui and Richard E. Turner},
      year={2018},
      eprint={1710.10628},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@inproceedings{
xia2021the,
title={The Gaussian Process Latent Autoregressive Model},
author={Rui Xia and Richard E Turner and Wessel Bruinsma and William Tebbutt},
booktitle={Third Symposium on Advances in Approximate Bayesian Inference},
year={2021},

}

inproceedings{bronskill2021memory,
  title={Memory Efficient Meta-Learning with Large Images},
  author={Bronskill, John F and Massiceti, Daniela and Patacchiola, Massimiliano and Hofmann, Katja and Nowozin, Sebastian and Turner, Richard E},
  booktitle={Thirty-Fifth Conference on Neural Information Processing Systems},
  year={2021}
}

@article{DBLP:journals/corr/abs-1806-03836,
  author    = {Taesup Kim and
               Jaesik Yoon and
               Ousmane Dia and
               Sungwoong Kim and
               Yoshua Bengio and
               Sungjin Ahn},
  title     = {Bayesian Model-Agnostic Meta-Learning},
  journal   = {CoRR},
  volume    = {abs/1806.03836},
  year      = {2018},
  
  eprinttype = {arXiv},
  eprint    = {1806.03836},
  timestamp = {Mon, 13 Aug 2018 16:47:14 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1806-03836.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/abs-2101-07950,
  author    = {Anna Vaughan and
               William Tebbutt and
               J. Scott Hosking and
               Richard E. Turner},
  title     = {Convolutional conditional neural processes for local climate downscaling},
  journal   = {CoRR},
  volume    = {abs/2101.07950},
  year      = {2021},
  
  eprinttype = {arXiv},
  eprint    = {2101.07950},
  timestamp = {Sat, 23 Jan 2021 17:57:49 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2101-07950.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@unpublished{bas,
title= {Climate Science Challenges and Opportunities for Machine Learning
},
author = {J. Scott Hosking},
year = {2021},
note= {Machine Learning for the Physical World, Guest Lecture},
}

@inproceedings{word2vec,
 author = {Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg S and Dean, Jeff},
 booktitle = {NeurIPS},
 editor = {C. J. C. Burges and L. Bottou and M. Welling and Z. Ghahramani and K. Q. Weinberger},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Distributed Representations of Words and Phrases and their Compositionality},
 
 volume = {26},
 year = {2013}
}


@article{song2018active,
  title={Active learning with confidence-based answers for crowdsourcing labeling tasks},
  author={Song, Jinhua and Wang, Hao and Gao, Yang and An, Bo},
  journal={Knowledge-Based Systems},
  volume={159},
  pages={244--258},
  year={2018},
  publisher={Elsevier}
}


@inproceedings{manifoldMixUp,
  title={Manifold mixup: Better representations by interpolating hidden states},
  author={Verma, Vikas and Lamb, Alex and Beckham, Christopher and Najafi, Amir and Mitliagkas, Ioannis and Lopez-Paz, David and Bengio, Yoshua},
  booktitle={ICML},
  year={2019}
}

@misc{
battleday2019improving,
title={Improving machine classification using human uncertainty measurements},
author={Ruairidh M. Battleday and Joshua C. Peterson and Thomas L. Griffiths},
year={2019},

}


@article{battleday2020capturing,
  title={Capturing human categorization of natural images by combining deep networks and cognitive models},
  author={Battleday, Ruairidh M and Peterson, Joshua C and Griffiths, Thomas L},
  journal={Nature communications},
  volume={11},
  number={1},
  pages={1--14},
  year={2020},
  publisher={Nature Publishing Group}
}

@article{thangaratinam2005delphi,
  title={The delphi technique},
  author={Thangaratinam, Shakila and Redman, Charles WE},
  journal={The obstetrician \& gynaecologist},
  volume={7},
  number={2},
  pages={120--125},
  year={2005},
  publisher={Wiley Online Library}
}


@misc{pseudoVersusHumanASR,
  doi = {10.48550/ARXIV.2203.12668},
  
  
  
  author = {Hwang, Dongseong and Sim, Khe Chai and Huo, Zhouyuan and Strohman, Trevor},
  
  keywords = {Machine Learning (cs.LG), Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Pseudo Label Is Better Than Human Label},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {Creative Commons Attribution 4.0 International}
}



@inproceedings{fixmatch,
 author = {Sohn, Kihyuk and Berthelot, David and Carlini, Nicholas and Zhang, Zizhao and Zhang, Han and Raffel, Colin A and Cubuk, Ekin Dogus and Kurakin, Alexey and Li, Chun-Liang},
 booktitle = {NeurIPS},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 title = {FixMatch: Simplifying Semi-Supervised Learning with Consistency and Confidence},
 year = {2020}
}





@inproceedings{pseudoLabelMixup,
  title={Pseudo-labeling and confirmation bias in deep semi-supervised learning},
  author={Arazo, Eric and Ortego, Diego and Albert, Paul and O’Connor, Noel E and McGuinness, Kevin},
  booktitle={International Joint Conference on Neural Networks},
  year={2020}
}


@article{vapnik2015learning,
  title={Learning using privileged information: similarity control and knowledge transfer.},
  author={Vapnik, Vladimir and Izmailov, Rauf and others},
  journal={J. Mach. Learn. Res.},
  volume={16},
  number={1},
  pages={2023--2049},
  year={2015}
}


@article{distillLabelNoise,
  author    = {Zizhao Zhang and
               Han Zhang and
               Sercan {\"{O}}mer Arik and
               Honglak Lee and
               Tomas Pfister},
  title     = {{IEG:} Robust Neural Network Training to Tackle Severe Label Noise},
  journal   = {CoRR},
  volume    = {abs/1910.00701},
  year      = {2019},
  
  eprinttype = {arXiv},
  eprint    = {1910.00701},
  timestamp = {Wed, 21 Oct 2020 12:10:27 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1910-00701.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@incollection{pytorch,
title = {PyTorch: An Imperative Style, High-Performance Deep Learning Library},
author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
booktitle = {NeurIPS 32},
editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
pages = {8024--8035},
year = {2019},
publisher = {Curran Associates, Inc.},

}


@inproceedings{gordon2022jury,
  title={Jury learning: Integrating dissenting voices into machine learning models},
  author={Gordon, Mitchell L and Lam, Michelle S and Park, Joon Sung and Patel, Kayur and Hancock, Jeff and Hashimoto, Tatsunori and Bernstein, Michael S},
  booktitle={CHI Conference on Human Factors in Computing Systems},
  pages={1--19},
  year={2022}
}

@article{goldstein2014lay,
  title={Lay understanding of probability distributions},
  author={Goldstein, Daniel G and Rothschild, David},
  journal={Judgment and Decision making},
  volume={9},
  number={1},
  pages={1},
  year={2014},
  publisher={Society for Judgment \& Decision Making}
}

@article{krizhevsky2009learning,
  title={Learning multiple layers of features from tiny images},
  author={Krizhevsky, Alex and Hinton, Geoffrey and others},
  year={2009},
  publisher={Citeseer}
}


@article{steyvers2022bayesian,
  title={Bayesian modeling of human--AI complementarity},
  author={Steyvers, Mark and Tejeda, Heliodoro and Kerrigan, Gavin and Smyth, Padhraic},
  journal={Proceedings of the National Academy of Sciences},
  volume={119},
  number={11},
  pages={e2111547119},
  year={2022},
  publisher={National Acad Sciences}
}








@article{knowledgeDistill,
  title={Distilling the knowledge in a neural network},
  author={Hinton, Geoffrey and Vinyals, Oriol and Dean, Jeff and others},
  journal={arXiv preprint arXiv:1503.02531},
  volume={2},
  number={7},
  year={2015}
}


@article{gou2021knowledgeSurvey,
  title={Knowledge distillation: A survey},
  author={Gou, Jianping and Yu, Baosheng and Maybank, Stephen J and Tao, Dacheng},
  journal={International Journal of Computer Vision},
  volume={129},
  number={6},
  pages={1789--1819},
  year={2021},
  publisher={Springer}
}




@inproceedings{labelSmoothing,
  title={Rethinking the inception architecture for computer vision},
  author={Szegedy, Christian and Vanhoucke, Vincent and Ioffe, Sergey and Shlens, Jon and Wojna, Zbigniew},
  booktitle={CVPR},
  pages={2818--2826},
  year={2016}
}

@article{Qiu2022DHT,
    title={Iterative Teaching by Data Hallucination},
    author={Qiu, Zeju and Liu, Weiyang and Xiao, Tim Z and Liu, Zhen and Bhatt, Umang and Luo, Yucen and Weller, Adrian and Schölkopf, Bernhard},
    journal = {arXiv preprint arXiv:2210.17467},
    year={2022}
}

@InProceedings{Yuan_2020_CVPR_KD_LS,
author = {Yuan, Li and Tay, Francis EH and Li, Guilin and Wang, Tao and Feng, Jiashi},
title = {Revisiting Knowledge Distillation via Label Smoothing Regularization},
booktitle = {CVPR},
month = {June},
year = {2020}
} 



@article{Uma_Fornaciari_Hovy_Paun_Plank_Poesio_2020, title={A Case for Soft Loss Functions}, volume={8}, url={https://ojs.aaai.org/index.php/HCOMP/article/view/7478}, abstractNote={&lt;p class=&quot;abstract&quot;&gt;Recently, Peterson et al. provided evidence of the benefits of using probabilistic soft labels generated from crowd annotations for training a computer vision model, showing that using such labels maximizes performance of the models over unseen data. In this paper, we generalize these results by showing that training with soft labels is an effective method for using crowd annotations in several other ai tasks besides the one studied by Peterson &lt;em&gt;et al.&lt;/em&gt;, and also when their performance is compared with that of state-of-the-art methods for learning from crowdsourced data.&lt;/p&gt;}, number={1}, journal={Proceedings of the AAAI Conference on Human Computation and Crowdsourcing}, author={Uma, Alexandra and Fornaciari, Tommaso and Hovy, Dirk and Paun, Silviu and Plank, Barbara and Poesio, Massimo}, year={2020}, month={Oct.}, pages={173-177} }

@article{combineConfusion,
  author    = {Gavin Kerrigan and
               Padhraic Smyth and
               Mark Steyvers},
  title     = {Combining Human Predictions with Model Probabilities via Confusion
               Matrices and Calibration},
  journal   = {CoRR},
  volume    = {abs/2109.14591},
  year      = {2021},
  
  eprinttype = {arXiv},
  eprint    = {2109.14591},
  timestamp = {Mon, 04 Oct 2021 17:22:25 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2109-14591.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{paul2021deep,
  title={Deep learning on a data diet: Finding important examples early in training},
  author={Paul, Mansheej and Ganguli, Surya and Dziugaite, Gintare Karolina},
  journal={NeurIPS},
  volume={34},
  pages={20596--20607},
  year={2021}
}

@article{interactiveconcept, title={Interactive Concept Bottleneck Models}, volume={37}, number={5}, journal={Proceedings of the AAAI Conference on Artificial Intelligence}, author={Chuahuan, Kushal and Tiwari, Rishabh and Shenoy, Pradeep and Dvijotham, Krishnamurthy}, year={2023}}

@article{Bondi_Koster_Sheahan_Chadwick_Bachrach_Cemgil_Paquet_Dvijotham_2022, title={Role of Human-AI Interaction in Selective Prediction}, volume={36}, url={https://ojs.aaai.org/index.php/AAAI/article/view/20465}, DOI={10.1609/aaai.v36i5.20465}, number={5}, journal={Proceedings of the AAAI Conference on Artificial Intelligence}, author={Bondi, Elizabeth and Koster, Raphael and Sheahan, Hannah and Chadwick, Martin and Bachrach, Yoram and Cemgil, Taylan and Paquet, Ulrich and Dvijotham, Krishnamurthy}, year={2022}, month={Jun.}, pages={5286-5294} }

@misc{cem22,
  doi = {10.48550/ARXIV.2209.09056},
  url = {https://arxiv.org/abs/2209.09056},
  author = {
    Espinosa Zarlenga, Mateo and
    Barbiero, Pietro and
    Ciravegna, Gabriele and
    Marra, Giuseppe and
    Giannini, Francesco and
    Diligenti, Michelangelo and
    Shams, Zohreh and
    Precioso, Frederic and
    Melacci, Stefano and
    Weller, Adrian and
    Lio, Pietro and
    Jamnik, Mateja
  },
  keywords = {
    Machine Learning (cs.LG),
    Artificial Intelligence (cs.AI),
    FOS: Computer and information sciences,
    FOS: Computer and information sciences,
    I.2.6,
    68T07
  },
  title = {Concept Embedding Models},
  publisher = {NeurIPS},
  year = {2022},
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@inproceedings{
hvarfner2022pibo,
title={\${\textbackslash}pi\${BO}: Augmenting Acquisition Functions with User Beliefs for Bayesian Optimization},
author={Carl Hvarfner and Danny Stoll and Artur Souza and Luigi Nardi and Marius Lindauer and Frank Hutter},
booktitle={ICLR},
year={2022},

}

@inproceedings{ru2020bayesian,
  title={Bayesian optimisation over multiple continuous and categorical inputs},
  author={Ru, Binxin and Alvi, Ahsan and Nguyen, Vu and Osborne, Michael A and Roberts, Stephen},
  booktitle={International Conference on Machine Learning},
  pages={8276--8285},
  year={2020},
  organization={PMLR}
}


@article{mozannar2021teaching,
  title={Teaching Humans When To Defer to a Classifier via Examplars},
  author={Mozannar, Hussein and Satyanarayan, Arvind and Sontag, David},
  journal={arXiv preprint arXiv:2111.11297},
  year={2021}
}
