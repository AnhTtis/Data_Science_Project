{
    "arxiv_id": "2303.12872",
    "paper_title": "Human Uncertainty in Concept-Based AI Systems",
    "authors": [
        "Katherine M. Collins",
        "Matthew Barker",
        "Mateo Espinosa Zarlenga",
        "Naveen Raman",
        "Umang Bhatt",
        "Mateja Jamnik",
        "Ilia Sucholutsky",
        "Adrian Weller",
        "Krishnamurthy Dvijotham"
    ],
    "submission_date": "2023-03-22",
    "revised_dates": [
        "2023-03-24"
    ],
    "latest_version": 1,
    "categories": [
        "cs.HC",
        "cs.AI",
        "cs.LG"
    ],
    "abstract": "Placing a human in the loop may abate the risks of deploying AI systems in safety-critical settings (e.g., a clinician working with a medical AI system). However, mitigating risks arising from human error and uncertainty within such human-AI interactions is an important and understudied issue. In this work, we study human uncertainty in the context of concept-based models, a family of AI systems that enable human feedback via concept interventions where an expert intervenes on human-interpretable concepts relevant to the task. Prior work in this space often assumes that humans are oracles who are always certain and correct. Yet, real-world decision-making by humans is prone to occasional mistakes and uncertainty. We study how existing concept-based models deal with uncertain interventions from humans using two novel datasets: UMNIST, a visual dataset with controlled simulated uncertainty based on the MNIST dataset, and CUB-S, a relabeling of the popular CUB concept dataset with rich, densely-annotated soft labels from humans. We show that training with uncertain concept labels may help mitigate weaknesses of concept-based systems when handling uncertain interventions. These results allow us to identify several open challenges, which we argue can be tackled through future multidisciplinary research on building interactive uncertainty-aware systems. To facilitate further research, we release a new elicitation platform, UElic, to collect uncertain feedback from humans in collaborative prediction tasks.",
    "pdf_urls": [
        "http://arxiv.org/pdf/2303.12872v1"
    ],
    "publication_venue": null
}