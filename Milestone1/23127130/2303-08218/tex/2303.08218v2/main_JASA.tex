\documentclass[11pt]{article}
\usepackage{amsmath}
\usepackage{graphicx,psfrag,epsf}
\usepackage{enumerate}
\usepackage{natbib}
\usepackage{url} % not crucial - just used below for the URL 
\usepackage{times}
%\usepackage[compact]{titlesec}

% NOTE: To produce blinded version, replace "0" with "1" below.

\usepackage[margin=1in]{geometry}

\usepackage{color,xcolor}
\usepackage{float}
% \usepackage[subrefformat=parens,labelformat=parens]{subfig}
\usepackage[subrefformat=parens,labelformat=parens]{subcaption}
\usepackage{booktabs}
\usepackage{tikz}
\usepackage{multirow}
\usepackage{amsfonts}
\usepackage{amsthm}
\usepackage{bm}
\usepackage[inline]{enumitem}

\usetikzlibrary{positioning}
\usetikzlibrary{arrows.meta, matrix}
\usetikzlibrary{shapes,arrows,chains}
\usetikzlibrary{arrows,positioning,calc,topaths}

% \usepackage[compact]{titlesec}
\usepackage{titlesec}
\titleformat*{\section}{\large\bfseries}
\titleformat*{\subsection}{\normalsize\bfseries}
\titleformat{\subsubsection}[runin]
  {\normalfont\normalsize\bfseries}{\thesubsubsection}{1em}{}
\titleformat*{\paragraph}{\normalsize\bfseries}
\titleformat*{\subparagraph}{\normalsize\bfseries}


%\usepackage[nokeyprefix]{refstyle}
%\usepackage{varioref}
%\usepackage{xr-hyper}
\RequirePackage[citecolor=blue,colorlinks=true,linkcolor=blue]{hyperref}
\usepackage[capitalise,noabbrev]{cleveref}
\crefformat{equation}{(#2#1#3)}


\theoremstyle{definition}
%\newtheorem{lemma}{Lemma}
\newtheorem{proposition}{Proposition}
\newtheorem{cor}{Corollary}
%\newtheorem{definition}{Definition}
\newtheorem{assumption}{Assumption}
\newtheorem{remark}{Remark}

\newcommand{\commentG}[1]{{\color{purple} \noindent [GP: #1]}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\indep}{\perp \!\!\! \perp}
\newcommand\numberthis{\addtocounter{equation}{1}\tag{\theequation}}

\usepackage{xargs, xstring}
\newcommandx{\tor}[1][1=r]{{(#1)}}



\allowdisplaybreaks

\begin{document}

\def\spacingset#1{\renewcommand{\baselinestretch}%
{#1}\small\normalsize} \spacingset{1}


  \title{\bf Spatial causal inference in the presence of unmeasured confounding and interference}
  \author{ Georgia Papadogeorgou\thanks{
  This material is based on work supported by the National Science Foundation under Grant No 2124124. 
  %The authors would like to thank the attendees of the causal inference seminar organized by the Harvard Data Science Initiative for their feedback.
  } \hspace{.2cm}\\
  {\normalsize Department of Statistics, University of Florida} \\
  and \\
  Srijata Samanta \\
  {\normalsize Department of Biostatistics, The University of Texas M.D. Anderson Cancer Center}
}
\date{}
  \maketitle
\vspace{-25pt}

\bigskip


\begin{abstract}
Causal inference in spatial settings is met with unique challenges and opportunities. In spatial settings, a unitâ€™s outcome might be affected by the exposure at many locations and the confounders might be spatially structured.
Using causal diagrams, we investigate the complications that arise when investigating causal relationships from spatial data.
We illustrate that spatial confounding and interference can manifest as each other, meaning that investigating the presence of one can lead to wrongful conclusions in the presence of the other. We also show that statistical dependencies in the exposure can render standard analyses invalid, which can have crucial implications for understanding the effect of interventions on dependent units. 
Based on the conclusions from this investigation, we propose a parametric approach that simultaneously accounts for interference and mitigates bias from local and neighborhood unmeasured spatial confounding.
We show that incorporating an exposure model is necessary from a Bayesian perspective. Therefore, the proposed approach is based on modeling the exposure and the outcome simultaneously while accounting for the presence of common spatially-structured unmeasured predictors.
We illustrate our approach with a simulation study and with an analysis of the local and interference effects of sulfur dioxide emissions from power plants on cardiovascular mortality.
\end{abstract}

\noindent%
{\it Keywords:} 
causal graphs;
interference;
% network data;
spatial confounding;
spatial causal inference;
unmeasured confounding


\spacingset{1.2} % DON'T change the spacing!



\section{Introduction}

Many of the questions researchers are faced with are causal in nature, and methodology for drawing causal inferences from observational data has been flourishing in the past decades. Most of the methods and theory assume that the available data form a random sample from a superpopulation of interest. However, in many contexts, units in our data are not independent, and causal and statistical dependencies complicate both how causal effects are defined and how they are estimated.
Our focus is causal inference with spatial data, though our work readily extends to other structural dependencies such as network settings. When faced with spatial observational data, the data's inherent dependence structure offers unique challenges and opportunities.

One of the challenges pertains to the potential existence of spatial spillover effects: the outcome in one location might be driven by exposures in the same but also other locations. This phenomenon is often referred to in the literature as {\it interference}. When interference is present, the interpretation of estimates from estimators that ignore interference can be complicated \citep{savje2021average}, especially in observational settings \citep{Tchetgen2012}.
Interference has attracted a lot of attention in the last couple of decades \citep[e.g.][among many others]{Sobel2006, Hudgens2008, manski2013identification, aronow2017estimating} with some studies that focus explicitly on how interference manifests in spatial settings \citep{Verbitsky-savitz2012, wang2020design, zigler2020bipartite, antonelli2020heterogeneous, papadogeorgou2022causal, giffin2022generalized}.

Spatial data also present opportunities for causal inference that pertain specifically to their dependence structure. One causal assumption often employed to identify causal effects from observational data is that of no unmeasured confounding. If this assumption is not satisfied using only measured covariates, estimated quantities will lack a causal interpretation. If the unmeasured confounders are spatially varying in that nearby observations have similar values, recent developments have harvested this structure to mitigate bias from these unmeasured spatial confounders \citep{thaden2018structural, papadogeorgou2019adjusting, keller2020selecting, schnell2020mitigating, dupont2022spatial, christiansen2022toward, Guan2022spectral}. Therefore, the data's inherent dependence structure provides opportunities to relax the no-unmeasured confounding assumption, or mitigate bias due to missing structured confounders.

In this work, we provide an investigation of the challenges and opportunities in spatial causal inference that arise from interference, unmeasured confounding, and the data's inherent dependence structure. Our work achieves the following goals.
\begin{enumerate*}[label=(\alph*)]
\item We introduce causal diagrams for spatially dependent data.
\item We illustrate theoretically and practically that spatial confounding and spatial spillover effects can manifest as one another: If unmeasured spatial confounding is present and not accounted, investigators might misinterpret the spatial structures induced by the confounder as interference, which would lead to wrongful conclusions about the causal effect of a potential intervention. In reverse, if interference is present and not accounted, researchers might mis-attribute spatial dependencies induced by interference to spatial confounding.
\item We demonstrate that inherent spatial dependencies in the exposure variable can render standard analyses for estimating causal effects invalid. Therefore, establishing causality in spatial settings is faced with unique challenges compared to settings with independent observations.
\item Based on the introduced causal diagrams for spatial data, we formally establish that neighborhood exposure and confounder values must be incorporated in a spatial causal analysis in order to avoid the aforementioned pitfalls, an important guidance for practitioners.
\item We address this by introducing a Bayesian causal inference framework for spatial data which 
\begin{enumerate*}[label=\roman*)]
\item incorporates interference, 
\item mitigates bias in effect estimation from local and neighborhood confounding due to unmeasured spatial variables,
\item provides straightforward uncertainty quantification, and
\item establishes that modeling both the exposure and the outcome process is necessary in the presence of unmeasured spatial confounding.
\end{enumerate*}
%reiterating that unmeasured spatial confounding is \textit{not} properly addressed based on an outcome model only.
%\item We suggest an approach that allows us to address unmeasured spatial confounding and interference simultaneously. Specifically, we introduce a set of assumptions under which confounding by unmeasured spatial variables and interference can be accommodated within one unifying context.
\item Across a variety of dependency structures, we illustrate  in simulations that our approach reduces bias in the estimation of local and interference effects that arises due to unmeasured spatial confounding or inherent statistical dependencies.
\item Finally, we illustrate our approach by analyzing county-level data on the relationship between sulfur dioxide (SO$_2$) emissions from power plants and cardiovascular mortality. Our approach indicates the presence of some unmeasured spatial confounding that biases effect estimates downwards. 
% The approach that does not account for unmeasured confounding indicates a counter-intuitive protective effect of higher SO$_2$ emissions on health, whereas our approach returns substantively different conclusions that are more in-line with the current knowledge.}
\end{enumerate*}

Even though we focus on spatial areal data, extensions to point-referenced spatial data are readily available with appropriate adjustments. Furthermore, our work extends to dependent settings other than spatial data where similar challenges occur.


\subsection{Related literature}

The term spatial confounding has been used to represent drastically different notions in the spatial and causal literatures \citep{reich2021review, Gilbert2021approaches, papadogeorgou2022discussion}. Popularized in spatial statistics, spatial confounding was first used to describe collinearity between covariates and spatial random effects in regression models \citep{reich2006effects, Hodges2010, Paciorek2010, Hanks2015, Prates2019alleviating}. Here, we adopt the notion of spatial confounding encountered in causal inference \citep{papadogeorgou2019adjusting, Gilbert2021approaches}, where the measured variables do not suffice for confounding adjustment, but the missing confounders exhibit a spatial structure.

Even though unmeasured spatial confounding and spatial interference have been investigated previously, there exists limited work that addresses both of these issues simultaneously. In a study of area deprivation on pedestrian casualties, \cite{graham2013quantifying} adopted a modeling approach to spatial confounding and interference which included spatial predictors, spatial random effects, and functions of the neighboring areas' exposure in a Poisson regression. However, their approach does not explicitly clarify the causal quantities of interest, does not allow for confounding from unmeasured variables, and is susceptible to errors in fixed effects introduced by using spatial random effects \citep{Hodges2010}.
Within a causal inference framework, \cite{giffin2021instrumental} investigated an instrumental-variable approach in the case of spatial data, and they allowed for the effect of the exposure at other locations to affect the outcome. Their approach provides a promising direction forward, though it requires access to a valid instrument.

Particularly relevant is the work by \cite{ogburn2014causal} where they introduced causal diagrams for causal inference with interference, and use these graphs to determine identifiability of causal estimands. They find that neighbors' covariate values often need to be controlled for in order to block all back-door paths. This observation was also established by \cite{forastiere2021identification}, and \cite{tec2022weather2vec} developed an approach using neural networks for finding the appropriate adjustment form for the neighborhood covariates. However, the work by \cite{ogburn2014causal} does not directly address non-causal dependencies among observations that might naturally occur in spatial settings. Relatedly, \cite{vansteelandt2007confounding} considers the setting where treatments within a cluster can be correlated due to unmeasured cluster-level variables, but they do not investigate the interplay between statistical dependencies in confounders and exposures.
\cite{rosenbaum2007interference} states that ``Interference is distinct from statistical dependence produced by pretreatment clustering, although both may be present.'' 
Here, we address multiple types of spatial dependencies: spatial interference, spatial dependence occurring due to pre-treatment unmeasured confounders, and inherent spatial co-dependencies.


\section{Estimands and identifiability with paired spatial data}
\label{sec:pairs}

First, we focus on a population that is a collection of blocks with dependencies within a block but not across them. 
% CUT!!!
%This can be a reasonable approximation in situations where dependencies occur in small geographical distances, and available data include regions that are far from each other.
For simplicity, we focus on blocks of size two (pairs) though larger blocks or blocks of varying sizes are discussed in \cref{subsec:blocks_larger}. Network data are discussed in \cref{sec:one_network}.

\subsection{Causal estimands for paired spatial data with a binary treatment}

Consider the situation where there is a natural ordering within pairs that allows us to name them as Unit 1 and Unit 2.
% CUT!!!
%An example where such ordering is reasonable is the case of mother-child pairs. 
We use $i,j$ to denote the two units, and we drop the notation that corresponds to the block for simplicity. The units are assumed to have potential outcomes $Y_i(z_i, z_j)$ and $Y_j(z_j, z_i)$ for treatments $z_i, z_j \in \{0, 1\}$, where we write the individual's own treatment first.

We use $\lambda$ to denote {\it local} effects representing the effect on a unit's outcome for changes in its own treatment, and $\iota$ to denote {\it interference} effects representing the effect on a unit's outcome for changes in the neighbor's treatment. Define the local effects for unit $i$ as
\begin{equation}
\begin{aligned}
\lambda_i(0) &= \E[Y_i(z_i = 1, z_j = 0) - Y_i(z_i = 0, z_j= 0)] \equiv \E[ Y_i(1, 0) - Y_i(0, 0)],
\quad \text{and}\\
\lambda_i(1) &= \E[Y_i(z_i = 1, z_j = 1) - Y_i(z_i = 0, z_j= 1)] \equiv \E[ Y_i(1, 1) - Y_i(0, 1)],
\end{aligned}
\label{eq:local_effect}
\end{equation}
and the interference effects for unit $i$ as
\begin{equation}
\begin{aligned}
\iota_i(0) &= \E[Y_i(z_i = 0, z_j = 1) - Y_i(z_i = 0, z_j= 0)] \equiv \E[Y_i(0, 1) - Y_i(0, 0)],
\quad \text{and}\\
\iota_i(1) &= \E[Y_i(z_i = 1, z_j = 1) - Y_i(z_i = 1, z_j= 0)] \equiv \E[Y_i(1, 1) - Y_i(1, 0)],
\end{aligned}
\label{eq:interference_effect}
\end{equation}
where the expectations are over the blocks. In these definitions, the subscript $i$ represents the unit on whose outcome we focus, and the argument $z$ represents the level at which we fix the neighbor's treatment or the unit's own treatment for the local and interference effects, respectively.

Alternate definitions of local effects draw the treatments for other units from a pre-specified distribution. For $\pi \in [0, 1]$, we use $\lambda_i(\pi)$ to denote the local effect for unit $i$ when the treatment of unit $j$ is drawn from a Bernoulli distribution with probability $\pi$, and $\lambda_i(\pi) = \pi \lambda_i(1) + (1 - \pi) \lambda_i(0)$. Similarly, the interference effect of unit $i$ when their own treatment is drawn from a Bernoulli$(\pi)$ distribution is denoted by $\iota_i(\pi)$ and $\iota_i(\pi) = \pi \iota_i(1) + (1 - \pi) \iota_i(0)$. For $\pi \in \{0, 1\}$ these definitions revert back those in \cref{eq:local_effect} and \cref{eq:interference_effect}.
%
If there does not exist a natural ordering of the units within a block, then the local and interference causal effects could be defined as
\( \displaystyle
\lambda(z) = \left( \lambda_1(z) + \lambda_2(z) \right) / 2 \), and
$\iota(z) = \left(\iota_1(z) + \iota_2(z) \right) / 2$, respectively, for $z \in \{0, 1\}$.



\subsection{Spatial statistical and causal dependence through a graph lens}
\label{subsec:pairs_graphs}

We introduce graphs for paired spatial data to uncover the complications in identifying and estimating local and interference effects that arise under different causal and statistical dependency structures.
Causal graphs have been used to establish identifiability of causal estimands in a variety of settings \citep{pearl2000models} and in the presence of interference explicitly \citep{ogburn2014causal}. Here, we establish causal diagrams in scenarios where spatial confounding and interference might exist separately or simultaneously, and where the treatment itself exhibits inherent spatial statistical dependence. 
The term {\it spatial interference} is used to represent the situation where the treatment applied in one location affects the outcome in the other location. The term {\it spatial confounding} is used to represent the situation where a spatial variable confounds the relationship of interest, in that it leads to open back-door paths from the treatment to the outcome of interest \citep{pearl1995causal}. The treatment variable itself might have {\it inherent spatial dependence} in that it is correlated across locations, the value at one location is {\it not} causally driven by the value in other locations, and this dependence cannot be directly explained by conditioning on measured covariates. We return to the interpretation of the exposure's inherent dependence below when we discuss the interpretation of edges in our causal graphs.


Let $\bm Z = (Z_1, Z_2)$ and $\bm Y = (Y_1, Y_2)$ denote the block-level observed treatment and outcome for the two units. We assume that the observed outcomes are equal to the potential outcomes under the observed treatment, $Y_1 = Y_1(Z_1, Z_2)$ and $Y_2 = Y_2(Z_2, Z_1)$, and that ignorability holds conditional on a covariate $\bm U= (U_1, U_2)$. 
We denote the covariate with the letter ``U'' since this covariate will be considered unmeasured later in the manuscript. For simplicity of our causal graphs below, we refrain from including additional covariates until \cref{sec:one_network}. 
\begin{assumption}
% CUT!!!
%[Pair ignorability]
It holds that $\bm Z \indep Y_i(z_1, z_2) \mid \bm U$ for $i = 1, 2$ and $z_1, z_2\in \{0, 1\}$.
Also, for each $\bm u = (u_1, u_2)$ such that $P_{\bm U}(\bm u) > 0$, we have that $P_{\bm Z}(\bm z \mid \bm U = \bm u) > 0$, where $\bm z = (z_1, z_2) \in \{0, 1\}^2,$ and $P_{\bm U}, P_{\bm Z}$ denote the corresponding distribution across pairs.
%where $P_{\bm U}$ denotes the distribution of $\bm U$, and $P_{\bm Z}(\cdot \mid \bm U = \bm u)$ denotes the distribution of $\bm Z$ conditional on $\bm U = \bm u$.
\label{ass:paired_ignorability}
\end{assumption}

\cref{fig:dag_compact} depicts the directed acyclic graph (DAG)
% for the covariate, treatment, and outcome 
at the block level. An arrow between two variables represents that the variable on the tail of the arrow has an effect on the variable at the head of the arrow. 
% CUT!!!
% The covariate $\bm U$ blocks the back-door path from the block-level exposure $\bm Z$ to the block-level outcome $\bm Y$.
The graphical model associated with this DAG would specify that the block-level data have joint density $f(\bm u, \bm z, \bm y) = f(\bm u) f(\bm z \mid \bm u) f(\bm y \mid \bm z, \bm u)$.
This graph compacts the variables across the two units in the block which masks the underlying dependencies that are important for investigating identifiability of causal contrasts in different scenarios.

In \cref{fig:dag_expanded} we depict the two units separately using a chain graph. Informally, an undirected edge represents statistical dependence of the random variables on either side. We discuss the formal interpretation of an undirected edge below. Absence of an arrow or edge forces that the corresponding relationship is not present, whereas its presence does not necessarily mean that the depicted relationship takes place. The scenarios we consider in this manuscript are depicted in \cref{fig:dag_expanded_considered}: we do not delve into the situation where the outcome is inherently spatial ($Y_1, Y_2$ edge missing), and we ignore the case where $U$ in one location predicts $Z$ in a different location.
% CUT!!!
%, which would increase the number of scenarios to consider without providing many additional insights.
%Therefore, we consider the situation where the treatment assignment is driven by covariates only in the same location.

\begin{figure}[!t]
\spacingset{1.25}
\vspace{-40pt}
\centering
\begin{subfigure}[t]{.47\linewidth}
\centering
\begin{tikzpicture}
\node (1) {$\bm U$};
\node[right=of 1] (2) {$\bm Z$};
\node[right=of 2] (3) {$\bm Y$};
\draw[->] (1) to (2);
\draw[->] (1) to [out=30,in=150] (3);
\draw[->] (2) to (3);
\phantom{\draw[->] (1) to [out=315,in=315,looseness=2] (3);}
\phantom{\draw[->] (3) to [out=45,in=135,looseness=2] (1);}
\end{tikzpicture}
\vspace{-45pt}
\caption{Compact graph at the block level.}
\label{fig:dag_compact}
\end{subfigure}
\hfill
\begin{subfigure}[t]{.47\linewidth}
\centering
\begin{tikzpicture}
        \node (1) {$U_1$};
		\node[ right= of 1] (2) {$Z_1$};
		\node[ right= of 2] (3) {$Y_1$};
		\node[below = of 1] (12) {$U_2$};
		\node[ right= of 12] (22) {$Z_2$};
		\node[ right= of 22] (32) {$Y_2$};
		\draw[->] (1) to (2); 
		\draw[->] (12) to (2); 
		\draw[->] (1) to (22); 
		\draw[->] (12) to (22); 
		\draw[->] (2) to (3); 
		\draw[->] (2) to (32);
		\draw[->] (22) to (3);
		\draw[->] (12) to [out=330,in=210] (32);
		\draw[->] (1) to [out=45,in=45,looseness=1.35] (32);
\phantom{\draw[->] (32) to [out=45,in=135,looseness=1.35] (1);}
\phantom{\node [below left =0.6 and 0.6 of 1] (10) {$U$};}
		\draw[->] (12) to [out=315,in=315,looseness=1.35] (3);
		\draw[->] (1) to [out=30,in=150] (3);
		\draw[->] (22) to (32);
		\draw[-] (1) to (12);
		\draw[-] (2) to (22);
		\draw[-] (3) to (32);
	\end{tikzpicture}
\vspace{-26pt}
\caption{Two units depicted separately.}
\label{fig:dag_expanded}
\end{subfigure}
%
$\ $ \\[-20pt]
\begin{subfigure}[t]{.47\linewidth}
\centering
\begin{tikzpicture}
        \node (1) {$U_1$};
		\node[ right= of 1] (2) {$Z_1$};
		\node[ right= of 2] (3) {$Y_1$};
		\node[below = of 1] (12) {$U_2$};
		\node[ right= of 12] (22) {$Z_2$};
		\node[ right= of 22] (32) {$Y_2$};
		\draw[->] (1) to (2); 
		\draw[->] (12) to (22); 
		\draw[->] (2) to (3); 
		\draw[->] (2) to (32);
		\draw[->] (22) to (3);
		\draw[->] (12) to [out=330,in=210] (32);
		\draw[->] (1) to [out=45,in=45,looseness=1.35] (32);
\phantom{\draw[->] (32) to [out=45,in=135,looseness=1.35] (1);}
		\draw[->] (12) to [out=315,in=315,looseness=1.35] (3);
		\draw[->] (1) to [out=30,in=150] (3);
		\draw[->] (22) to (32);
		\draw[-] (1) to (12);
		\draw[-] (2) to (22);
	\end{tikzpicture}
\vspace{-30pt}
\caption{The relationships we consider in this work.}
\label{fig:dag_expanded_considered}
\end{subfigure}
\hfill 
\begin{subfigure}[t]{.47\linewidth}
\centering
\begin{tikzpicture}
        \node (1) {$U_1$};
        \node [below left =0.3 and 0.3 of 1] (10) {$U^u$};
        \node [below left =0.3 and 0.3 of 2] (20) {$Z^u$};
		\node[ right= of 1] (2) {$Z_1$};
		\node[ right= of 2] (3) {$Y_1$};
		\node[below = of 1] (12) {$U_2$};
		\node[ right= of 12] (22) {$Z_2$};
		\node[ right= of 22] (32) {$Y_2$};
		\draw[->] (1) to (2); 
		\draw[->] (12) to (22); 
		\draw[->] (2) to (3); 
		\draw[->] (2) to (32);
		\draw[->] (22) to (3);
		\draw[->] (12) to [out=330,in=210] (32);
		\draw[->] (1) to [out=45,in=45,looseness=1.35] (32);
\phantom{\draw[->] (32) to [out=45,in=135,looseness=1.35] (1);}
		\draw[->] (12) to [out=315,in=315,looseness=1.35] (3);
		\draw[->] (1) to [out=30,in=150] (3);
		\draw[->] (22) to (32);
		\draw[->] (10) to (1);
		\draw[->] (10) to (12); 
		\draw[->] (20) to (2);
		\draw[->] (20) to (22);
\end{tikzpicture}
\vspace{-30pt}
\caption{Statistical dependencies occur due to an unobservable common underlying trend.}
\label{fig:dag_underlying}
\end{subfigure}
\vspace{-5pt}
\caption{Block-level causal and statistical dependencies. 
% CUT!!!
%(a) Compacted across units. (b) The two units in the pair are depicted separately with all possible relationships allowed. (c) Subgraph we consider in this manuscript with some arrows and edges missing. (d) Statistical dependencies are conceived as occurring due an underlying unobservable trend component. 
}
\label{fig:dag}
\end{figure}

In all the cases we consider, we allow for spatial dependence within $\bm U$ and within $\bm Z$, depicted with edges between $U_1, U_2$ and between $Z_1, Z_2$. 
% CUT!!!
%Therefore, the exposure $\bm Z$ is spatial because the spatial variable $\bm U$ is a predictor of $\bm Z$ (illustrated through $Z_1 \leftarrow U_1 - U_2 \rightarrow Z_2$), and because $\bm Z$ is inherently spatially structured (illustrated through the direct connection $Z_1 - Z_2$). 
Chain graphs can have different causal interpretations \citep{lauritzen2002chain}. Here, undirected edges represent inherent statistical dependencies due to an underlying common trend that drives both variables. A DAG that represents this structure is shown in \cref{fig:dag_underlying}: $U^u$ induces correlation between $U_1$ and $U_2$, and similarly for $Z^u, Z_1$, and $Z_2.$ Therefore, the graphical model associated with the chain graph in \cref{fig:dag_expanded_considered} is the graphical model for the DAG in \cref{fig:dag_underlying}, and \( f(u^u, \bm u, z^u, \bm z, \bm y) \) can be written as
{ \(
%f(u^u, \bm u, z^u, \bm z, \bm y)
%&= 
f(u^u) f(u_1 \mid u^u) f(u_2 \mid u^u) f(z^u) f(z_1 \mid z^u, u_1) f(z_2 \mid z^u, z_2)
f(y_1 \mid z_1, z_2, u_1, u_2) f(y_2 \mid z_2, z_1, u_2, u_1).
\)}
%\label{eq:graphical_model}
The superscript $^u$ is used to stand for {\it u}nderlying variables that drive the spatial structure in the corresponding variable.
These underlying variables do not necessarily ``exist'' in the sense that they might be impossible to measure.
As an example, a vector $\bm Z = (Z_1, Z_2)$ that follows a bivariate normal distribution with variances 1 and correlation parameter $\rho$ can be equivalently conceived as arising from a generative process where $Z^u, \epsilon_1, \epsilon_2 \sim N(0, 1)$ independently, and $Z_i = \sqrt{\rho} Z^u + \sqrt{1 - \rho} \epsilon_i$. Here $Z^u$ drives the underlying common trend in $\bm Z$, but $Z^u$ is unobserv\textit{able}, and it can {\it never} be measured or conditioned on. 
Intuitively, such inherent spatial dependence in the treatment variable can occur, for example, by experimental design if different restrictions are applied (or resources are provided) to different geographical areas which would lead to spatially correlated treatment assignments.
Inherent spatial structure can also arise by exogenous processes that are possible to measure {\it in theory} but not in practice, such as the intricate atmospheric and pollution transport processes that dictate the spatially-correlated ambient air pollution levels.
Therefore, the exogenous $U^u, Z^u$ describe the inherent spatial structure in $\bm U$ and $\bm Z$ which cannot be ``adjusted away'' be conditioning on more covariates.

We present causal diagrams with spatial confounding and interference in \cref{fig:graphs}. These graphs correspond to subgraphs of \cref{fig:dag_expanded_considered} with different arrows missing, which allows us to directly investigate the complications in estimating causal effects that manifest {\it due to} the spatial dependence in the covariate and the treatment. 
Some of the identifiability statements below are based on viewing the statistical dependencies depicted in \cref{fig:graphs} within the realm of the underlying DAG in \cref{fig:dag_underlying} and using theory of graphical models \citep{spirtes1993causation, pearl1995causal, pearl2000models}.
If the identifiability criteria are not standard, we prove them in Supplement \ref{supp_sec:identifiability}.

%The presence of a link (arrow or dash) does not necessarily mean that the link exists, though the absence of a link means that the relationship is known to be absent.

\begin{figure}[!t]
\spacingset{1.1}
\centering
\small
\vspace{-10pt}
% DIRECT SPATIAL CONFOUNDING
\begin{subfigure}[t]{.475\linewidth}
\centering
\begin{tikzpicture}
		\node (1) {$U_1$};
		\node[ right= of 1] (2) {$Z_1$};
		\node[ right= of 2] (3) {$Y_1$};
		\node[below = of 1] (12) {$U_2$};
		\node[ right= of 12] (22) {$Z_2$};
		\node[ right= of 22] (32) {$Y_2$};
		\draw[->] (1) to (2);
		\draw[->] (1) to [out=30,in=150] (3);
		\draw[->] (2) to (3); 
		\draw[->] (12) to (22);
		\draw[->] (12) to [out=330,in=210] (32);
		\draw[->] (22) to (32);
		\draw[-] (1) to (12);
		\draw[-] (2) to (22);
	\end{tikzpicture}
 \vspace{-5pt}
\caption{{\bf Direct Spatial Confounding.}
The covariate predicts the exposure and outcome only locally. 
% CUT!!!
%Adjusting for the confounder is necessary for identifying both local and interference effects.
}
\label{fig:direct}
\end{subfigure}
\hfill
%
%\hspace{20pt}
% INTERFERENCE
\begin{subfigure}[t]{.485\linewidth}
\centering
\begin{tikzpicture}
		\node (1) {$U_1$};
		\node[ right= of 1] (2) {$Z_1$};
		\node[ right= of 2] (3) {$Y_1$};
		\node[below = of 1] (12) {$U_2$};
		\node[ right= of 12] (22) {$Z_2$};
		\node[ right= of 22] (32) {$Y_2$};
		\draw[->] (2) to (3); 
		\draw[->] (2) to (32);
		\draw[->] (22) to (3);
		\phantom{\draw[->] (12) to [out=330,in=210] (32);}
		\draw[->] (22) to (32);
		\draw[-] (1) to (12);
		\draw[-] (2) to (22);
	\end{tikzpicture}
 \vspace{-5pt}
\caption{{\bf Spatial interference.}
% CUT!!!
%Unmeasured spatial variables might exist, but they are not confounders.
One unit's treatment affects the another unit's outcome.}
\label{fig:interference}
\end{subfigure}
%
$\ $ \\[-2pt]
%
% DIRECT AND INDIRECT SPATIAL CONFOUNDING
\begin{subfigure}[t]{.485\linewidth}
\centering
\begin{tikzpicture}
		\node (1) {$U_1$};
		\node[ right= of 1] (2) {$Z_1$};
		\node[ right= of 2] (3) {$Y_1$};
		\node[below = of 1] (12) {$U_2$};
		\node[ right= of 12] (22) {$Z_2$};
		\node[ right= of 22] (32) {$Y_2$};
		\draw[->] (1) to (2);
		\draw[->] (1) to [out=30,in=150] (3);
		\draw[->] (2) to (3); 
		\draw[->] (12) to (22);
		\draw[->] (12) to [out=330,in=210] (32);
		\draw[->] (22) to (32);
		\draw[-] (1) to (12);
		\draw[-] (2) to (22);
	    \clip (0,-2.7) rectangle (4.3,1);
		\draw[->] (1) to [out=45,in=45,looseness=1.35] (32);
		\draw[->] (12) to [out=315,in=315,looseness=1.35] (3);
	\end{tikzpicture}
 \vspace{-5pt}
\caption{{\bf Direct \& Indirect Spatial Confounding.}
The covariate predicts the local and neighbor's outcome.
% CUT!!!
%Local and neighbor's covariate has to be adjusted for identifying local and interference effects.
}
\label{fig:general_spatial_conf}
\end{subfigure}
%
\hfill
%
% DIRECT SPATIAL CONFOUNDING AND INTERFERENCE
\begin{subfigure}[t]{.485\linewidth}
\centering
\begin{tikzpicture}
		\node (1) {$U_1$};
		\node[ right= of 1] (2) {$Z_1$};
		\node[ right= of 2] (3) {$Y_1$};
		\node[below = of 1] (12) {$U_2$};
		\node[ right= of 12] (22) {$Z_2$};
		\node[ right= of 22] (32) {$Y_2$};
		\draw[->] (1) to (2);
		\draw[->] (2) to (32);
		\draw[->] (22) to (3);
		\draw[->] (1) to [out=30,in=150] (3);
		\draw[->] (2) to (3); 
		\draw[->] (12) to (22);
		\draw[->] (12) to [out=330,in=210] (32);
		\draw[->] (22) to (32);
		\draw[-] (1) to (12);
		\draw[-] (2) to (22);
		\phantom{
		\clip (0,-2.7) rectangle (4.3,1);
		\draw[->] (1) to [out=45,in=45,looseness=1.35] (32);
		\draw[->] (12) to [out=315,in=315,looseness=1.35] (3);
		}
	\end{tikzpicture}
 \vspace{-5pt}
\caption{{\bf Direct Spatial Confounding and Interference.} 
% CUT!!!
% Conditioning on both exposure values and the local confounder is necessary for identifying local and interference effects due to the inherent spatial structure in $\bm U$ and $\bm Z$. 
}
\label{fig:direct_interference}
\end{subfigure}
%
$\ $ \\[-12pt]
%
% INTERFERENCE ONLY
\begin{subfigure}[t]{.485\linewidth}
\centering
\begin{tikzpicture}
		\node (1) {$U_1$};
		\node[ right= of 1] (2) {$Z_1$};
		\node[ right= of 2] (3) {$Y_1$};
		\node[below = of 1] (12) {$U_2$};
		\node[ right= of 12] (22) {$Z_2$};
		\node[ right= of 22] (32) {$Y_2$};
		\draw[->] (1) to (2);
		\draw[->] (2) to (32);
		\draw[->] (22) to (3);
		\draw[->] (2) to (3); 
		\draw[->] (12) to (22);
		\draw[->] (22) to (32);
		\draw[-] (1) to (12);
		\draw[-] (2) to (22);
		\phantom{
		\clip (0,-2.7) rectangle (4.3,1);
		\draw[->] (1) to [out=45,in=45,looseness=1.35] (32);
		\draw[->] (12) to [out=315,in=315,looseness=1.35] (3);
		}
	\end{tikzpicture}
 \vspace{-5pt}
\caption{{\bf Interference and a spatial predictor of the exposure.}
% CUT!!!
% If interference is not accounted for, methods that adjust and do not adjust for unmeasured spatial variables will return different estimates, both of which are wrong. Therefore, unaccounted interference would manifest as spatial confounding.
}
\label{fig:predictor_interference}
\end{subfigure}
%
%
%\hspace{20pt}
\hfill
% ALL SPATIAL CONFOUNDING AND INTERFERENCE
\begin{subfigure}[t]{.485\linewidth}
\centering
\begin{tikzpicture}
		\node (1) {$U_1$};
		\node[ right= of 1] (2) {$Z_1$};
		\node[ right= of 2] (3) {$Y_1$};
		\node[below = of 1] (12) {$U_2$};
		\node[ right= of 12] (22) {$Z_2$};
		\node[ right= of 22] (32) {$Y_2$};
		\draw[->] (1) to (2);
		\draw[->] (1) to [out=30,in=150] (3);
		\draw[->] (2) to (3); 
		\draw[->] (12) to (22);
		\draw[->] (12) to [out=330,in=210] (32);
		\draw[->] (22) to (32);
		\draw[->] (2) to (32);
		\draw[->] (22) to (3);
		\draw[-] (1) to (12);
		\draw[-] (2) to (22);
	    \clip (0,-2.7) rectangle (4.3,1);
		\draw[->] (1) to [out=45,in=45,looseness=1.35] (32);
		\draw[->] (12) to [out=315,in=315,looseness=1.35] (3);
	\end{tikzpicture}
 \vspace{-5pt}
\caption{{\bf Direct and Indirect Spatial Confounding with Interference.} The complete graph we consider. 
%CUT!!!
%Local and interference effects should be investigated simultaneously while also conditioning on the local and neighborhood covariates.
}
\label{fig:general_interference}
\end{subfigure}
% \vspace{10pt}
\caption{Graphical representation of spatial confounding and interference with a spatially correlated variable $\bm U = (U_1, U_2)$, a spatial exposure $\bm Z = (Z_1, Z_2)$, and outcome $\bm Y = (Y_1, Y_2)$.}
\label{fig:graphs}
\end{figure}



% \paragraph{Direct spatial confounding without interference}

The graph in \cref{fig:direct} corresponds to a scenario with {\bf direct spatial confounding and no interference}.
We refer to this confounding structure as direct because it is only the {\it local} value of $U$ that drives the local value for $Y$.
In this setting, there is no interference and $\iota_i(z) = 0$. To identify the local causal effects it suffices to control for the local value of the confounder.
%for Unit 1, $\lambda_1(z)$, there are two back-door paths. The first one is the classic back-door path in confounding through the unit's own confounder values, $Z_1 \leftarrow U_1 \rightarrow Y_1$. The second back-door path, $Z_1 - Z_2 \leftarrow U_2 - U_1 \rightarrow Y_1$, is present explicitly due to the inherent spatial dependence in both $\bm Z$ and $\bm U$. Controlling for the local value of the confounder would block both paths and suffice to identify local causal effects.
There are four potential back-door paths for the interference effect $\iota_1(z)$:
\begin{enumerate*}[label=(\arabic*)]
\item $Z_2 \leftarrow U_2 - U_1 \rightarrow Y_1$,
\item $Z_2 \leftarrow U_2 - U_1 \rightarrow Z_1 \rightarrow Y_1$,
\item $Z_2 - Z_1 \leftarrow U_1 \rightarrow Y_1$, and
\item $Z_2 - Z_1 \rightarrow Y_1$.
\end{enumerate*}
Paths (1) and (2) exist because $\bm U$ is spatial, and paths (3) and (4) exist due to the inherent spatial structure in $\bm Z$. If both the confounder and the exposure are {\it not} spatial, none of these paths exist, and one could estimate interference effects without any adjustments.
However, even when the confounder is not spatial, it is necessary to adjust for the local exposure $Z_1$ {\it and} the local confounder $U_1$ to identify the interference effect on unit 1, $\iota_1(z)$. That is because conditioning on $Z_1$ blocks path (4) but it opens path (3) on which $Z_1$ is a collider. To block path (3) again, one needs to condition on $U_1$.
% If $\bm Z$ or $\bm U$ were {\it not} spatial, no adjustment would be necessary to estimate interference effects, $\iota_i(z)$. However, 
Therefore, if the exposure variable is inherently spatial, adjusting for the local spatial confounder is necessary for identifying interference effects, and for avoiding mis-attributing spatial statistical dependencies to interference. Even in this simple scenario, the data's inherent spatial dependence ``breaks'' the analysis that would be valid for independent data, and failing to account for spatial dependencies could lead researchers to mis-identify spatial interference.

% \paragraph{Interference without spatial confounding}

\cref{fig:interference} represents a setting with {\bf interference and no spatial confounding}. 
%where the spatial covariates are not confounders, but interference might be present because a unit's outcome can be driven by the neighbor's treatment.
If the exposure variable $\bm Z$ is not spatially-structured (the edge $Z_1 - Z_2$ is missing), interpretable local and interference effects are identifiable without any adjustment. For example, the local effect $\lambda_i(\pi)$, for $\pi = P(Z_j = 1)$ is identifiable simply by comparing the outcomes of Unit $i$ among blocks with $Z_i = 1$ and $Z_i = 0$. Similarly, interference effects could be identified without explicitly controlling for the local effects of the treatment. However, when $\bm Z$ is inherently spatial, one would necessarily have to account for $Z_2$ when investigating the local effect for Unit 1, since the correlation of $Z_1, Z_2$ and the interference effect of $Z_2$ on $Y_1$ would lead to spurious associations for $Z_1$ and $Y_1$ locally, irrespective the sample size. Therefore, simple analyses that are justifiable for independent data are {\it not} applicable for spatially structured exposure variables.



%\paragraph{Direct and indirect spatial confounding without interference}

The graph in \cref{fig:general_spatial_conf} is a generalization of the one in \cref{fig:direct} that depicts {\bf direct and indirect spatial confounding without interference}. We refer to the situation where a local spatial predictor of the exposure ($U_j \rightarrow Z_j$) drives the outcome in a different location ($U_j \rightarrow Y_i$) as {\it indirect} spatial confounding.
When the exposure is spatial, estimating local effects for unit $i$ while adjusting for the neighbor's exposure would open the path $Z_i - Z_j \leftarrow U_j \rightarrow Y_i$ on which $Z_j$ is a collider, and would lead to bias.
Since this path requires that $\bm Z$ is spatially structured, this notion of confounding pertains solely to the setting with dependent data, and it is not met in settings with independent observations.
In this scenario, we would need to adjust for the local {\it and} the neighbor's confounder value in order to identify local and interference effects.
% At the same time, to identify the interference effect $\iota_i(z)$, one would need to adjust on the neighbor's confounder value $U_j$ in order to block the backdoor path $Z_j \leftarrow U_j \rightarrow Y_i$.
% The spatial confounder in location $j$, $U_j$, is not a confounder of the local causal effect at location $i$, $\lambda_i(z)$, in the classic sense since it is not a predictor of the exposure at location $i$ (missing arrow from $U_j$ to $Z_i$). Instead, $U_j$ indirectly confounds the $(Z_i, Y_i)$ relationship due to the $Z_i - Z_j \leftarrow U_j \rightarrow Y_i$ path.
% Indirect confounding can occur even if direct confounding does not exist, if for example the local predictor of the exposure is not a predictor of the local outcome ($U_i \rightarrow Z_i$, $U_i \rightarrow Y_j$, but $U_i \not\rightarrow Y_i$).


%\paragraph{Direct confounding with interference}

\cref{fig:direct_interference} shows a setting with {\bf direct spatial confounding and interference}, which combines the scenarios in Figures \ref{fig:direct} and \ref{fig:interference}. When $\bm Z$ is inherently spatial, it is necessary to condition on the local value of the confounder {\it and} the neighbor's exposure to identify local effects. In Supplement \ref{supp_sec:identifiability}, we show that if $\bm Z$ is not inherently spatial, interpretable local effects can be identified conditioning only on the local value of the confounder. Similar conclusions can be drawn about the identifiability of interference effects. We again see that the exposure's inherent spatial structure can lead to misleading conclusions if not properly accommodated.

%In the case of a spatial exposure ($Z_1-Z_2$), conditioning on the predictors of the exposure alone does not suffice to identify local effects due to the spurious association between $Z_i$ and $Y_i$ that flows through $Z_j$, and the neighbors' exposure value has to be conditioned on as well. Interference effects can be identified conditional only on the local confounder and exposure values.
%In the absence of spatial correlation in the exposure (no edge between $Z_1$ and $Z_2$), the neighbors' exposure will no longer be confounding one's exposure-outcome relationship, and local causal effects can be identified without adjustment for the neighbors' exposure.


The graph in 
\cref{fig:predictor_interference} is a generalization of the graph in \cref{fig:interference} to allow for {\bf interference and a spatial predictor of the exposure}. In this scenario, $\bm U$ is not a confounder for either the local or the interference effect. However, if interference is not accounted for, there is a back-door path from $Z_i$ to $Y_i$, through the unmeasured variable $\bm U$ and the neighbor's exposure $Z_j$. As a result, two methods that both ignore interference, but one adjusts and one does not adjust for the spatial covariate will return different values for the local effect estimate, both of which will be wrong since the path $Z_i - Z_j \rightarrow Y_i$ remains open regardless. Therefore, in this scenario, spatial interference could be mis-interpreted as spatial confounding.

%\paragraph{Direct and indirect confounding with interference}

Lastly, the graph in \cref{fig:general_interference} represents the situation also shown in \cref{fig:dag_expanded_considered} with {\bf direct, indirect spatial confounding, and interference}, of which the other graphs are special cases.
%
% It is demonstrated in the figures below that if we do not adjust for the neighbours' spatial confounders, it will lead to incorrect estimates of the neighbours' exposure effect on one's outcome.


\paragraph{}

When dependencies across locations are present due to missing conditioning variables, one could collect additional covariate information to ensure that data are as close to conditionally independent as possible. In spatial settings, variables might remain dependent irrespective of how many covariates we condition on in our analyses. Therefore, it is paramount that we better comprehend the complications created by the variables' dependence structures in order to separate statistical dependencies from causal relationships, and accurately attribute causal effects. 

Our investigations based on \cref{fig:graphs} illustrate that spatial settings are intrinsically different from settings with independent observations, in that confounding and causal dependencies can manifest as each other unless the spatial structure of the data is comprehensively accounted for.
These points are further illustrated with a motivating simulation study in Supplement \ref{subsec:illustrate_bias_pairs}, where we investigate biases of estimators that condition on different sets of variables. We see in practice how researchers might mis-attribute spatial confounding to interference, or mis-identify spatial confounding if interference is not accounted for.
We also see that the biases induced by the variables' spatial dependence are larger when the dependence is stronger.
Across all scenarios, we found that simultaneously estimating local and interference effects, and adjusting for local and neighborhood confounder values are necessary for eliminating biases due to spatial dependencies.
% Across all scenarios, adjusting for the local and neighborhood exposure and confounder value returns unbiased local and interference estimates.






% \paragraph{Simulation results}

% In this section we consider combinations of different true data generating models and fitted models to explore how the treatment effect estimators perform in each situation. Specifically we consider six different simulation scenarios as depicted in Figure 1 and fit linear models which has combinations of the treatment $Z$, the neighbour's treatment $\bar{Z}$, the spatial confounder $U$ and neighbour's spatial confounder $\bar{U}$. 

% In Table \ref{tab:combined} we consider 4 different simulation scenarios and in each scenario we fit different linear models and also the affine estimator. In Table \ref{tab:combined} we want to check that if the true model has only $Z$ and $\bar{Z}$ (and of course $C$) then how does the coefficient of $Z$ changes when we have only $C$ with $Z$ as opposed to having both $U$ and $C$ along with $Z$. For this experiment we have generated $U$ and $Z$ from their joint distribution as done for other rows in the table so as to preserve that dependence of $U$ and $Z$. We expect the coefficient of $Z$ to change when we add $U$ but it will not be better as we not adding the correct term ($\bar{Z}$). This might make us think that there is spatial confounding as we see a change in the coefficient of $Z$ but it is not the spatial confounding but interference in the true model which is the reason behind this change.



% \begin{table}[htbp]
%     \centering
%     \caption{Simulation Results}
%     % \rowcolors{5}{}{gray!10}
%     \resizebox{1.0\textwidth}{!}{%
%     \begin{tabular}{*{5}{c}}
%         \hline
%         True model & Fitted Model (Method) & bias & std error & estimate \\
%         \hline
%         $Y = 1 + 0.8 Z + U + \epsilon$, $\tau_U = 1$  & $Y \sim Z $ (OLS) & 0.6754955878 & 1.2834318 & 1.4754956 \\
%         & $Y \sim Z + \bar{Z} $ (OLS) &  0.5814788284 & 1.2675934 & 1.3814788 \\
%          & $Y \sim Z + U $ (OLS) &  0.0006709997 & 0.9769358 & 0.7993290 \\
%          & $Y \sim Z + \bar{Z} + U $ (OLS) &  -0.0018012268 & 0.9818240 & 0.7981988\\
%          & $Y \sim Z + U$ (affine) & 0.0027154422 & 0.9788643 & 0.8027154 \\
%       \hline
       
%          $Y = 1 + 0.8 Z + 0.4 \bar{Z} + U + \epsilon,$ $\tau_U = 1$  & $Y \sim Z $ (OLS) & 0.784380694 & 1.337471 & 1.5843807\\
%         & $Y \sim Z + \bar{Z} $ (OLS) &  0.581478828 & 1.267593 & 1.3814788 \\
%          & $Y \sim Z + U $ (OLS) &  0.108214107 & 1.002002 & 0.9082141 \\
%          & $Y \sim Z + \bar{Z} + U $ (OLS) &  -0.001801227 & 0.981824 & 0.7981988 \\
%          & $Y \sim Z + U$ (affine) &   0.072002281 & 1.003155 & 0.8720023 \\
        
%         \hline
%          $Y = 1 + 0.8 Z + U + \epsilon,$ $\tau_U = 0.05$  & $Y \sim Z $ (OLS) &  3.0232379077 & 3.7644575 & 3.8232379 \\
%          & $Y \sim Z + \bar{Z} $ (OLS) &  2.6067064798 & 3.6451792 & 3.4067065 \\
%          & $Y \sim Z + U $ (OLS) &  0.0006709997 & 0.9769358 & 0.7993290 \\
%          & $Y \sim Z + \bar{Z} + U $ (OLS) & -0.0018012268 & 0.9818240 & 0.7981988 \\
%          & $Y \sim Z + U$ (affine) &   0.0100933355 & 1.0123496 & 0.8100933 \\
         
%          \hline
         
%         $Y = 1 + 0.8 Z + U + \epsilon,$ $\tau_U = 10$  & $Y \sim Z $ (OLS) & 0.2131516497 & 1.0161609 & 1.0131516 \\
%         & $Y \sim Z + \bar{Z} $ (OLS) &  0.1826481220 & 1.0176842 & 0.9826481\\
%          & $Y \sim Z + U $ (OLS) &  0.0006709997 & 0.9769358 &  0.7993290 \\
%          & $Y \sim Z + \bar{Z} + U $ (OLS) & -0.0018012268 &  0.9818240 & 0.7981988\\
%          & $Y \sim Z + U$ (affine) &   0.0005958585 & 0.9771288 & 0.8005959 \\
     
%         \hline
%     \end{tabular}
%      }%
%      \label{exp1}
% \end{table}

% \begin{table}[htbp]
%     \centering
%     \caption{Simulation Results}
%     % \rowcolors{5}{}{gray!10}
%     \resizebox{0.75\textwidth}{!}{%
%     \begin{tabular}{*{7}{c}}
%         \hline
%         True model & Fitted Model (Method) & bias & std error & estimate of $\beta_z$ & estimate of $\beta_{\bar{z}}$\\
%         \hline
%         & & & & & \\
%          $Y = 1 + 0.8 Z + C + U + \epsilon$   & $Y \sim Z + U + C$ (affine) & -0.0057 & 1.0040 & 0.7943  \\
%           $\rho = 0.5$ & $Y \sim Z + \bar{Z} + U + C$ (affine) & -0.0214 &       1.0155 &  0.7786 & 0.0437  \\
%           & $Y \sim Z +  C$ (OLS) & 0.6604 & 1.2766 & 1.4604 \\
%           & $Y \sim Z + \bar{Z}  + C$ (OLS) & 0.5801 &  1.2651  &  1.3801  &    0.3275 \\
%           & & & & & \\
%          & & & & & \\
%          $Y = 1 + 0.8 Z + C + U + \epsilon$   & $Y \sim Z + U + C$ (affine) & -0.0037 & 0.9940 & 0.7963  \\
%           $\rho = 0.3$ & $Y \sim Z + \bar{Z} + U +C$ (affine) & -0.0216 & 1.0043 & 0.7784 & 0.0426  \\
%           & $Y \sim Z +  C$ (OLS) & 0.3647 & 1.2623 & 1.1648\\
%           & $Y \sim Z + \bar{Z}  + C$ (OLS) & 0.347 & 1.2643  &   1.147  &     0.1842\\
%           & & & & & \\
%          & & & & & \\
%          $Y = 1 + 0.8 Z + C + U + \epsilon$   & $Y \sim Z + U + C$ (affine) & 0.0008 & 0.9907 & 0.7992  \\
%           $\rho = 0.1$ & $Y \sim Z + \bar{Z} + U + C$ (affine) & -0.0222 &       1.0018 & 0.7778 & 0.0389  \\
%           & $Y \sim Z +  C$ (OLS) & 0.1175 & 1.2581 & 0.9175\\
%           & $Y \sim Z + \bar{Z}  + C$ (OLS) &  0.094 & 1.2694  & 0.894  &      0.1002 \\
%           & & & & & \\
%          \hline
%          & & & & & \\
%          $Y = 1 + 0.8 Z + 0.4 \bar{Z} + C + U + \epsilon$ & $Y \sim Z + U + C$ (affine) & 0.0608 & 1.0335 & 0.8608 \\
%          $\rho = 0.5$ & $Y \sim Z + \bar{Z} + U + C$ (affine) & -0.0214  &       1.0155 & 0.7786 & 0.4437 \\
%          & $Y \sim Z +  C$ (OLS) & 0.7638 & 1.3381 & 1.5638\\
%           & $Y \sim Z + \bar{Z}  + C$ (OLS) &  0.561 & 1.2707  & 1.361  &     0.7652\\
%          & & & & & \\
%          & & & & & \\
%          $Y = 1 + 0.8 Z + 0.4 \bar{Z} + C + U + \epsilon$   & $Y \sim Z + U + C$ (affine) & 0.0094 & 1.0143 & 0.8094  \\
%           $\rho = 0.3$ & $Y \sim Z + \bar{Z} + U + C$ (affine) & -0.0216  &       1.0043 &  0.7784  &  0.4426   \\
%           & $Y \sim Z +  C$ (OLS) & 0.4142 & 1.2946 & 1.2142 \\
%           & $Y \sim Z + \bar{Z}  + C$ (OLS) &  0.3258   & 1.2698  &  1.1258 &       0.6272\\
%           & & & & & \\
%           & & & & & \\
%          $Y = 1 + 0.8 Z + 0.4 \bar{Z} + C + U + \epsilon$   & $Y \sim Z + U + C$ (affine) & -0.0026 & 1.0078 & 0.7973\\
%           $\rho = 0.1$ & $Y \sim Z + \bar{Z} + U + C$ (affine) &  -0.0222 &         1.0018 &  0.7778 & 0.4389\\
%           & $Y \sim Z +  C$ (OLS) & 0.1515 & 1.2764 & 0.9515 \\
%           & $Y \sim Z + \bar{Z}  + C$ (OLS) & 0.094 & 1.2694 &  0.894 &      0.5002\\
%           & & & & & \\
%          \hline
%           & & & & & \\
%          $Y = 1 + 0.8 Z + C + U + \bar{U} + \epsilon$  & $Y \sim Z + U + C$ (affine) & 0.0274 & 1.1635 & 0.8274 \\
%          $\rho = 0.5$ & $Y \sim Z + \bar{Z} + U + C$ (affine) & 0.1406 &        1.1934 & 0.9406 & 0.7032 \\
%          & $Y \sim Z +  C$ (OLS) & 0.1564 & 1.4783 & 0.9564\\
%           & $Y \sim Z + \bar{Z}  + C$ (OLS) & 0.723 & 1.5029 & 1.523 & 1.0399 \\
%           & $Y \sim Z + U + \bar{U} + C$ (affine) & -0.0013 & 1.2695 & 0.7987 \\
%           & $Y \sim Z + \bar{Z} + U + \bar{U} +C$ (affine) &  -0.0138  &  1.2791  &  0.7862 &  0.0631  \\
%          & & & & & \\
%           & & & & & \\  
%         $Y = 1 + 0.8 Z + C +  U + \bar{U} + \epsilon$   & $Y \sim Z + U + C$ (affine) &  0.1047 & 1.1817 & 0.9048 \\
%         $\rho = 0.3$ & $Y \sim Z + \bar{Z} + U + C$ (affine) & 0.0733 &       1.1796 & 0.8733 &  0.4267 \\
%         & $Y \sim Z +  C$ (OLS) & 0.5039 & 1.5073 & 1.3038\\
%           & $Y \sim Z + \bar{Z}  + C$ (OLS) &  0.4218  &  1.4953  & 1.2218  &     0.6217\\
%           & $Y \sim Z + U + \bar{U} + C$ (affine) & 0.0021 & 1.1688 & 0.8021 \\
%           & $Y \sim Z + \bar{Z} + U + \bar{U} +C$ (affine) &   -0.0146 & 1.1778 & 0.7854 & 0.0576\\
%         & & & & & \\  
%         & & & & & \\
%          $Y = 1 + 0.8 Z + C + U + \bar{U} + \epsilon$  & $Y \sim Z + U + C$ (affine) & 0.0274 & 1.1635 & 0.8274\\
%          $\rho = 0.1$ & $Y \sim Z + \bar{Z} + U + C$ (affine) &  \\
%          & $Y \sim Z +  C$ (OLS) & 0.1506 & 1.4823 & 0.9506\\
%           & $Y \sim Z + \bar{Z}  + C$ (OLS) & 0.1405 & 1.4805 & 0.9405 & 0.1829 \\
%           & $Y \sim Z + U + \bar{U} + C$ (affine) & 0.0054 & 1.1450 & 0.8054  \\
%           & $Y \sim Z + \bar{Z} + U + \bar{U} +C$ (affine) & -0.0181 & 1.1531 & 0.7819 & 0.0463 \\
%          & & & & & \\
%           \hline  
%         & & & & & \\  
%         $Y = 1 + 0.8 Z + 0.4 \bar{Z} + C +  U + \bar{U} + \epsilon$   & $Y \sim Z + U + C$ (affine) & 0.3606 & 1.3518 & 1.1606 \\
%         $\rho = 0.5$ & $Y \sim Z + \bar{Z} + U + C$ (affine) & 0.1406 &       1.1934 & 0.9406 & 1.1032 \\
%         & $Y \sim Z +  C$ (OLS) & 1.0972 & 1.7193 & 1.8972 \\
%           & $Y \sim Z + \bar{Z}  + C$ (OLS) & 0.723 & 1.5029 & 1.523 & 1.4399 \\
%           & $Y \sim Z + U + \bar{U} + C$ (affine) &  \\
%           & $Y \sim Z + \bar{Z} + U + \bar{U} +C$ (affine) & -0.0138 & 1.2791 &       0.7862 &  0.4631\\
%         & & & & & \\
%         & & & & & \\  
%         $Y = 1 + 0.8 Z + 0.4 \bar{Z} + C +  U +  \bar{U} + \epsilon$   & $Y \sim Z + U + C$ (affine) &  \\
%         $\rho = 0.3$ & $Y \sim Z + \bar{Z} + U + C$ (affine) &  \\
%         & $Y \sim Z +  C$ (OLS) &  \\
%           & $Y \sim Z + \bar{Z}  + C$ (OLS) & \\
%           & $Y \sim Z + U + \bar{U} + C$ (affine) & \\
%           & $Y \sim Z + \bar{Z} + U + \bar{U} +C$ (affine) & \\
%         & & & & & \\
%         & & & & & \\  
%         $Y = 1 + 0.8 Z + 0.4 \bar{Z} + C +  U + \bar{U} + \epsilon$   & $Y \sim Z + U + C$ (affine) & \\
%         $\rho = 0.1$ & $Y \sim Z + \bar{Z} + U + C$ (affine) &  \\
%         & $Y \sim Z +  C$ (OLS) &  \\
%           & $Y \sim Z + \bar{Z}  + C$ (OLS) &  \\
%           & $Y \sim Z + U + \bar{U} + C$ (affine) & \\
%           & $Y \sim Z + \bar{Z} + U + \bar{U} +C$ (affine) & \\
%         & & & & & \\
%          \hline
%         \end{tabular}
%      }%
%      \label{exp2}
% \end{table}

% \begin{table}[htbp]
%     \centering
%     \caption{Simulation Results}
%     % \rowcolors{5}{}{gray!10}
%     \resizebox{0.8\textwidth}{!}{%
%     \begin{tabular}{*{7}{c}}
%         \hline
%         True model & Fitted Model (Method) & bias & std error & estimate of $\beta_z$ & estimate of $\beta_{\bar{z}}$\\
%         \hline
%         & & & & & \\
%          $Y = 1 + 0.8 Z + C + 0.5U + \epsilon$   & $Y \sim Z + U + C$ (affine) &   \\
%           $\rho = 0.5$ & $Y \sim Z + \bar{Z} + U + C$ (affine) &  \\
%           & $Y \sim Z +  C$ (OLS) &  \\
%           & $Y \sim Z + \bar{Z}  + C$ (OLS) &  \\
%           & & & & & \\
%          & & & & & \\
%          $Y = 1 + 0.8 Z + C + 0.5U + \epsilon$   & $Y \sim Z + U + C$ (affine) &  \\
%           $\rho = 0.3$ & $Y \sim Z + \bar{Z} + U +C$ (affine) &  \\
%           & $Y \sim Z +  C$ (OLS) &\\
%           & $Y \sim Z + \bar{Z}  + C$ (OLS) & \\
%           & & & & & \\
%          & & & & & \\
%          $Y = 1 + 0.8 Z + C + 0.5U + \epsilon$   & $Y \sim Z + U + C$ (affine) & \\
%           $\rho = 0.1$ & $Y \sim Z + \bar{Z} + U + C$ (affine) &  \\
%           & $Y \sim Z +  C$ (OLS) & \\
%           & $Y \sim Z + \bar{Z}  + C$ (OLS) &   \\
%           & & & & & \\
%          \hline
%          & & & & & \\
%          $Y = 1 + 0.8 Z + 0.4 \bar{Z} + C + 0.5U + \epsilon$ & $Y \sim Z + U + C$ (affine) &  \\
%          $\rho = 0.5$ & $Y \sim Z + \bar{Z} + U + C$ (affine) & \\
%          & $Y \sim Z +  C$ (OLS) & \\
%           & $Y \sim Z + \bar{Z}  + C$ (OLS) & \\
%          & & & & & \\
%          & & & & & \\
%          $Y = 1 + 0.8 Z + 0.4 \bar{Z} + C + 0.5U + \epsilon$   & $Y \sim Z + U + C$ (affine) &  \\
%           $\rho = 0.3$ & $Y \sim Z + \bar{Z} + U + C$ (affine) &  \\
%           & $Y \sim Z +  C$ (OLS) &  \\
%           & $Y \sim Z + \bar{Z}  + C$ (OLS) &  \\
%           & & & & & \\
%           & & & & & \\
%          $Y = 1 + 0.8 Z + 0.4 \bar{Z} + C + 0.5U + \epsilon$   & $Y \sim Z + U + C$ (affine) & \\
%           $\rho = 0.1$ & $Y \sim Z + \bar{Z} + U + C$ (affine) &  \\
%           & $Y \sim Z +  C$ (OLS) &  \\
%           & $Y \sim Z + \bar{Z}  + C$ (OLS) & \\
%           & & & & & \\
%          \hline
%           & & & & & \\
%          $Y = 1 + 0.8 Z + C + U + 0.5 \bar{U} + \epsilon$  & $Y \sim Z + U + C$ (affine) & 0.1301 & 1.0755 & 0.9301 \\
%          $\rho = 0.5$ & $Y \sim Z + \bar{Z} + U + C$ (affine) & 0.0597 &        1.0708 &  0.8597 & 0.3724 \\
%          & $Y \sim Z +  C$ (OLS) & 0.8271 & 1.4090 & 1.6271\\
%           & $Y \sim Z + \bar{Z}  + C$ (OLS) &  0.642 & 1.3625 & 1.442 &      0.7026\\
%          & & & & & \\
%           & & & & & \\  
%         $Y = 1 + 0.8 Z + C +  U +  0.5 \bar{U} + \epsilon$   & $Y \sim Z + U + C$ (affine) &   \\
%         $\rho = 0.3$ & $Y \sim Z + \bar{Z} + U + C$ (affine) & 0.0258  &       1.0591 & 0.8258 & 0.234 \\
%         & $Y \sim Z +  C$ (OLS) & \\
%           & $Y \sim Z + \bar{Z}  + C$ (OLS) & 0.3738 & 1.3589 & 1.1738 & 0.4245\\
%         & & & & & \\  
%         & & & & & \\
%          $Y = 1 + 0.8 Z + C + U + 0.5 \bar{U} + \epsilon$  & $Y \sim Z + U + C$ (affine) & \\
%          $\rho = 0.1$ & $Y \sim Z + \bar{Z} + U + C$ (affine) & -0.0053 &          1.056 & 0.7947 & 0.1055 \\
%          & $Y \sim Z +  C$ (OLS) & \\
%           & $Y \sim Z + \bar{Z}  + C$ (OLS) & 0.1111 & 1.3575  &  0.9111 &      0.1696 \\
%          & & & & & \\
%           \hline  
%         & & & & & \\  
%         $Y = 1 + 0.8 Z + 0.4 \bar{Z} + C +  U +  0.5 \bar{U} + \epsilon$   & $Y \sim Z + U + C$ (affine) & 0.2144 & 1.1518 & 1.0144 \\
%         $\rho = 0.5$ & $Y \sim Z + \bar{Z} + U + C$ (affine) & 0.0596  &       1.0707  & 0.8596 & 0.7722 \\
%         & $Y \sim Z +  C$ (OLS) & 0.9399 & 1.5043 & 1.7399 \\
%           & $Y \sim Z + \bar{Z}  + C$ (OLS) & 0.642 & 1.3625 & 1.442  &     1.1026 \\
%         & & & & & \\
%         & & & & & \\  
%         $Y = 1 + 0.8 Z + 0.4 \bar{Z} + C +  U +  0.5 \bar{U} + \epsilon$   & $Y \sim Z + U + C$ (affine) & 0.06512 & 1.0897 & 0.8651  \\
%         $\rho = 0.3$ & $Y \sim Z + \bar{Z} + U + C$ (affine) & 0.0258 &       1.059 & 0.8258 & 0.6339 \\
%         & $Y \sim Z +  C$ (OLS) & 0.4837 & 1.4043 & 1.2837 \\
%           & $Y \sim Z + \bar{Z}  + C$ (OLS) & 0.3738 &  1.3589 & 1.1738 &      0.8245\\
%         & & & & & \\
%         & & & & & \\  
%         $Y = 1 + 0.8 Z + 0.4 \bar{Z} + C +  U +  0.5 \bar{U} + \epsilon$   & $Y \sim Z + U + C$ (affine) & 0.0134 & 1.0663 & 0.8134 \\
%         $\rho = 0.1$ & $Y \sim Z + \bar{Z} + U + C$ (affine) & -0.0053 & 1.056 &    0.7947  &  0.5055 \\
%         & $Y \sim Z +  C$ (OLS) & 0.1709 & 1.3661 & 0.9709 \\
%           & $Y \sim Z + \bar{Z}  + C$ (OLS) & 0.1111 & 1.3575  &  0.9111 &      0.5696 \\
%         & & & & & \\
%          \hline
%         \end{tabular}
%      }%
%      \label{exp3}
% \end{table}

% \begin{table}[htbp]
%     \centering
%     \caption{Simulation Results}
%     % \rowcolors{5}{}{gray!10}
%     \resizebox{0.9\textwidth}{!}{%
%     \begin{tabular}{*{5}{c}}
%         \hline
%         True model & Fitted Model (Method) & bias & std error & estimate of $\beta_z$ \\
%         \hline
%         & & & &  \\
%         $Y = 1 + 0.8 Z + 0.4 \bar{Z} + C + \epsilon$   & $Y \sim Z + C$ (OLS) & -0.0021 & 0.9983 & 0.7979   \\
%           $\rho = 0.5$ & $Y \sim Z + U + C$ (affine) & -0.6203 & 0.7463 & 0.1797 \\
%           & & & &   \\
%           & & & &  \\
%          $Y = 1 + 0.8 Z + 0.4 \bar{Z} + C + \epsilon$   & $Y \sim Z + C$ (OLS) & 0.0005 & 0.9985 & 0.7995  \\
%           $\rho = 0.3$ & $Y \sim Z + U + C$ (affine) & -0.3413 & 0.7142 &  0.4587 \\
%           & & & &   \\
%           & & & &  \\
%          $Y = 1 + 0.8 Z + 0.4 \bar{Z} + C + \epsilon$   & $Y \sim Z + C$ (OLS) & 0.0012 & 0.9986 & 0.8012   \\
%           $\rho = 0.1$ & $Y \sim Z + U + C$ (affine) & -0.1077 &  0.7070 & 0.6923 \\
%           & & & &   \\
%           \hline
%         \end{tabular}
%      }%
%      \label{exp4}
% \end{table}

% \begin{table}[htbp]
%     \centering
%     \caption{Simulation Results}
%     % \rowcolors{5}{}{gray!10}
%     \resizebox{0.9\textwidth}{!}{%
%     \begin{tabular}{*{5}{c}}
%         \hline
%         True model & Fitted Model (Method) & bias & std error & estimate of $\beta_z$ \\
%         \hline
%         & & & &  \\
%         $Y = 1 + 0.8 Z + 0.4 \bar{Z} + C + \epsilon$   & $Y \sim Z + C$ (OLS) &   0.0352 & 1.0164 & 0.8352\\
%           $\rho = 0.5$ & $Y \sim Z + \bar{Z} + C$ (OLS) & 0.0034 & 0.9985 & 0.8034\\
%           & $Y \sim Z + U + C$ (affine) & -0.1264 & 0.7229 & 0.6736  \\
%           & & & &   \\
%           & & & &  \\
%          $Y = 1 + 0.8 Z + 0.4 \bar{Z} + C + \epsilon$   & $Y \sim Z + C$ (OLS) & 0.0345 & 1.0163 & 0.8345  \\
%           $\rho = 0.3$ & $Y \sim Z + \bar{Z} + C$ (OLS) &  0.0033 & 0.9985 & 0.8033\\
%           & $Y \sim Z + U + C$ (affine) & -0.0989 & 0.7211 & 0.7011 \\
%           & & & &   \\
%           & & & &  \\
%          $Y = 1 + 0.8 Z + 0.4 \bar{Z} + C + \epsilon$   & $Y \sim Z + C$ (OLS) & 0.0339 & 1.0163 & 0.8339   \\
%           $\rho = 0.1$ & $Y \sim Z + \bar{Z} + C$ (OLS) & 0.0031 & 0.9985 & 0.8032 \\
%           & $Y \sim Z + U + C$ (affine) & -0.0772 & 0.7205 & 0.7228 \\
%           & & & &   \\
%           \hline
%         \end{tabular}
%      }%
%      \label{exp4}
% \end{table}


% \begin{table}[htbp]
%     \centering
%     \caption{Simulation Results (same as above except $z = z+1$)}
%     % \rowcolors{5}{}{gray!10}
%     \resizebox{0.9\textwidth}{!}{%
%     \begin{tabular}{*{5}{c}}
%         \hline
%         True model & Fitted Model (Method) & bias & std error & estimate of $\beta_z$ \\
%         \hline
%         & & & &  \\
%         $Y = 1 + 0.8 Z + 0.4 \bar{Z} + C + \epsilon$   & $Y \sim Z + C$ (OLS) & 0.10880 & 1.03836 & 0.9088   \\
%           $\rho = 0.5$ & $Y \sim Z + \bar{Z} + C$ (OLS) & -0.0017 & 1.0082 & 0.7983\\
          
%           & & & &   \\
%           & & & &  \\
%          $Y = 1 + 0.8 Z + 0.4 \bar{Z} + C + \epsilon$   & $Y \sim Z + C$ (OLS) &  0.0592 & 1.03097 & 0.8592\\
%           $\rho = 0.3$ & $Y \sim Z + \bar{Z} + C$ (OLS) & 0.000359 & 1.0081 & 0.7996  \\
         
%           & & & &   \\
%           & & & &  \\
%          $Y = 1 + 0.8 Z + 0.4 \bar{Z} + C + \epsilon$   & $Y \sim Z + C$ (OLS) &   0.0440 & 1.02860 & 0.8440\\
%           $\rho = 0.1$ & $Y \sim Z + \bar{Z} + C$ (OLS) & 0.000067 & 1.0080 & 0.80007 \\
          
%           & & & &   \\
%           \hline
%         \end{tabular}
%      }%
%      \label{exp4}
% \end{table}

% \begin{table}[htbp]
%     \centering
%     \caption{Bias of the estimates of the model coefficients where the true model is given by $Y = 1 + 0.8 Z + 0.4 \bar{Z} + C + \epsilon$ and the spatial parameter $\rho$ varies over the set XX. The other spatial parameters ($\phi_U, \phi_Z, \tau_U, \tau_Z$) are fixed at $\phi_{U} = 0.6, \phi_{Z} = 0.6, \tau_U = \tau_Z = 1$}
%     % \rowcolors{5}{}{gray!10}
%     \resizebox{0.9\textwidth}{!}{%
%     \begin{tabular}{*{6}{c}}
%         \hline
%         & & & & & \\
%         Changes in spatial parameter &  $Y \sim Z + C$ & \multicolumn{2}{c}{ $Y \sim Z + U + C$}  & \multicolumn{2}{c}{  $Y \sim Z + \bar{Z} + C$} \\
%         & $Z$ & $Z$ & $U$ & $Z$ & $\bar{Z}$ \\
%         \hline
%         & & & & & \\
%         $\rho = 0.5$ & & & & & \\
%         $\rho = 0.3$ & 0.192 & 0.154 & 0.071 & -0.001 & 0.002 \\
%         $\rho = 0.2$ & 0.151 & 0.136 & 0.044 & -0.001 & 0.002 \\
%         $ \rho = 0.1$ & 0.132 & 0.127 & 0.024 & -0.001 & 0.002 \\
%           \hline
%         \end{tabular}
%      }%
%      \label{tab:bias:1e}
% \end{table}

% \begin{table}[htbp]
%     \centering
%     \caption{Bias of the estimates of the model coefficients where the true model is given by $Y = 1 + 0.8 Z + U + 0.5 \bar{U} + C + \epsilon$ and the spatial parameter $\phi_Z$ varies over the set XX. The other spatial parameters ($\phi_U, \rho, \tau_U, \tau_Z$) are fixed at $\phi_{U} = 0.6, \rho = 0.2 , \tau_U = \tau_Z = 1$}
%     % \rowcolors{5}{}{gray!10}
%     \resizebox{0.6\textwidth}{!}{%
%     \begin{tabular}{*{2}{c}}
%         \hline
%         &  \\
%         Changes in spatial parameter &  Bias of $\hat{\beta}_Z$\\
%         \hline
%         & \\
%         $\phi_Z = 0.9$ & -0.001\\
%         $\phi_Z = 0.6$ & -0.002 \\
%         $\phi_Z = 0.3$ & -0.001  \\
%         $\phi_Z = 0.2$ & -0.001 \\
%         $\phi_Z = 0.1$ & -0.001\\
%           \hline
%         \end{tabular}
%      }%
%      \label{exp4}
% \end{table}

% \begin{table}[htbp]
%     \centering
%     \caption{Simulation Results}
%     % \rowcolors{5}{}{gray!10}
%     \resizebox{1.0\textwidth}{!}{%
%     \begin{tabular}{*{7}{c}}
%         \hline
%         True model & Fitted Model (Method) & bias & std error & estimate of $\beta_z$ & estimate of $\beta_{\bar{z}}$\\
%         \hline
%         & & & & & \\
%         $Y = 1 + 0.8 Z + U + \bar{U} + C + \epsilon$   & $Y \sim Z + U + \bar{U} + C$ (affine) & -0.0013 & 1.2695 & 0.7987 \\
%           $\rho = 0.5$ & $Y \sim Z + \bar{Z} + U + \bar{U} +C$ (affine) &  -0.0138  &  1.2791  &  0.7862 &  0.0631  \\
%           & & & & &  \\
%           & & & & & \\
%          $Y = 1 + 0.8 Z + U + \bar{U} + C + \epsilon$   & $Y \sim Z + U + \bar{U} + C$ (affine) & 0.0021 & 1.1688 & 0.8021 \\
%           $\rho = 0.3$ & $Y \sim Z + \bar{Z} + U + \bar{U} +C$ (affine) &   -0.0146 & 1.1778 & 0.7854 & 0.0576\\
%           & & & & &  \\
%           & & & & & \\
%          $Y = 1 + 0.8 Z + U + \bar{U} + C + \epsilon$   & $Y \sim Z + U + \bar{U} + C$ (affine) & 0.0054 & 1.1450 & 0.8054  \\
%           $\rho = 0.1$ & $Y \sim Z + \bar{Z} + U + \bar{U} +C$ (affine) & -0.0181 & 1.1531 & 0.7819 & 0.0463 \\
%           & & & & &  \\
%           \hline
%         \end{tabular}
%      }%
%      \label{exp4}
% \end{table}




\subsection{Blocked interference with more than two units}
\label{subsec:blocks_larger}

For interference blocks that are larger than two units, estimands for local and interference effects can be defined to average over hypothetical distributions of the neighbors' treatments, in agreement to literature on partial interference \citep{Hudgens2008, Tchetgen2012}.
We define the average outcome for unit $i$ when its treatment is set to a fixed value $z \in \{0, 1\}$, and the treatment of the other units in the block, $\bm z_{-i}$, are independent draws from a Bernoulli distribution with probability of success $\pi \in [0, 1]$ as
\(
\overline Y_i(z, \pi) % & \equiv Y_i(z_i = z, \bm z_{-i} \sim \text{Bern}(\pi)) \\
= \sum_{\bm z_{-i} } Y_i(z_i = z, \bm z_{-i}) p(\bm z_{-i} ; \pi),
\)
where $p(\cdot ; \pi)$ is the joint probability mass function for independent Bernoulli trials with probability of success $\pi$. Then, the local effect for unit $i$ can be defined as
\(
\lambda_i(\pi) = \E \left[ \overline Y_i(1, \pi) - \overline Y_i(0, \pi) \right],
%\label{eq:local_effect_pi}
\)
and the interference effect on unit $i$ as
\(
\iota_i(\pi, \pi'; z) = \E \left[ \overline Y_i(z, \pi') - \overline Y_i(z, \pi) \right].
%\label{eq:interference_effect_pi}
\)
For $\pi, \pi' \in \{0, 1\}$ and for blocks with two units, these estimands revert back to the estimands in \cref{eq:local_effect} and \cref{eq:interference_effect}, respectively.

Spatial dependence in confounders and exposure values will lead to the same complications in identifying local and interference effects discussed for paired data. To avoid distraction, we do not delve into blocked data with more than two units further. Instead, in \cref{sec:one_network} we focus on data on a single spatial network.



\section{Causal inference with spatial dependencies on a single network}
\label{sec:one_network}

In the case of a single network of interconnected units, the population of interest cannot be partitioned in non-interacting groups \citep{aronow2017estimating, Tchetgen2017auto, forastiere2021identification, ogburn2022causal}, and as a result, defining and estimating causal effects is more challenging. Again, we will refer to a causal effect as a ``local effect'' if it corresponds to the effect of treating one location on the outcome at the same location, and to a causal effect as an ``interference effect'' if it corresponds to the effect on a location's outcome for a change in the neighbors' treatment, with formal definitions given below.

\subsection{Potential outcomes based on network connections and exposure mapping}
\label{subsec:po_network}

Let $Z_i \in \mathcal{Z}$ denote the treatment value of unit $i$, which can be binary or continuous. Continuous treatments are often referred to as exposures, so we use the two terms interchangeably. In full generality, a unit's potential outcomes depend on the treatment level of all units on the network. Let $Y_i(\bm z)$ denote the outcome for unit $i$ for exposure values on the $n$ units $\bm z \in \mathcal{Z}^n$. 
%By decomposing the treatment vector $\bm z$ in unit $i$'s treatment and the treatment of all others $\bm z = (z_i, \bm z_{-i})$, potential outcomes can also be denoted as $Y_i(z_i, \bm z_{-i})$.
We reduce the number of potential outcomes by assuming the presence of a known interference network and exposure mapping \citep{aronow2017estimating, zigler2020bipartite, forastiere2021identification}. Let $A$ denote a known adjacency matrix of dimension $n \times n$, where $A_{ij} = 1$ reflects that the outcome for unit $i$ might depend on unit $j$'s treatment level, $A_{ij} = 0$ otherwise, and all diagonal elements are 0.
%We set the diagonal elements, $A_{ii}$, to 0
%We use $A_{i(-i)}$ to denote the $i^{th}$ row of the matrix excluding the diagonal element, therefore reflecting unit $i$'s connections.
We assume that unit $i$'s potential outcomes depend on the treatment vector only through its own exposure, and the average exposure of units with which it is connected through $A$:

\begin{assumption}
% Let $g:\mathcal{Z}^{n - 1} \times \{0, 1\}^{n - 1} \rightarrow \mathcal{E}$ denote a known function that maps the treatment vector and vector of connections for $n - 1$ units to an exposure value in $\mathcal{E}$.
Let $\bm z, \bm z'$ be two treatment vectors in $\mathcal{Z}^n$ such that $z_i = z_i'$ and $\overline z_i = \overline z_i'$, where $\overline z_i = \sum_j A_{ij} z_j / \sum_j A_{ij},$ and similarly for $\overline z_i'$. Then, it holds that $Y_i(\bm z) = Y_i(\bm z')$, and potential outcomes can be denoted as $Y_i(z_i, \overline z_i).$
\label{ass:sutva}
\end{assumption}

For areal data, a binary adjacency matrix $A$ often makes sense. However, for the case of point-referenced data the adjacency matrix could be alternatively defined such that a non-diagonal element $(i, j)$ of $A$ is equal to $f(d_{ij})$ where $d_{ij}$ is the geographical distance of locations $i$ and $j$, and $f$ is a pre-specified decreasing function. Then, the average neighborhood exposure in \cref{ass:sutva} would be a weighted average of the exposures of other locations, with weights driven by the locations' geographic proximity.
Our discussion below would straightforwardly accommodate an adjacency matrix $A$ that is symmetric or not, or a definition of exposure mapping that is more complicated than the average neighborhood exposure in \cref{ass:sutva}. % We maintain these here for simplicity.


\subsection{Ignorability in terms of measured and unmeasured spatial covariates}

For identifying and estimating causal effects from observational data, a set of covariates that satisfies ignorability has to be conditioned on. When the necessary confounding adjustment set includes unmeasured covariates, biases for estimating local and interference effects when conditioning only on measured covariates persist. For the $n$ units in the network, let $\widetilde C_i = (C_{i1}, C_{i2}, \dots, C_{ip_c})^T$ denote unit $i$'s $p_c$ measured covariates, which include measured individual and neighborhood characteristics. However, these covariates are not a sufficient conditioning set for unconfoundedness of the treatment assignment, and 
\begin{equation}
(Z_i, \overline Z_i) \not\!\indep Y_i(z, \overline z) \mid \widetilde C_i.
\label{eq:not_ignorable}
\end{equation}
We assume however that unconfoundedness holds conditional on measured covariates, and the local and neighborhood value of an unmeasured covariate.
\begin{assumption}
%[Ignorability conditional on unmeasured local and neighborhood covariate]
There exists unmeasured covariate $\bm U = (U_1, U_2, \dots, U_n)$ such that
\( \displaystyle
(Z_i, \overline Z_i) \indep Y_i(\cdot) \mid \widetilde C_i, U_i, \overline U_i,
\)
where $Y_i(\cdot) = \{ Y_i(z, \overline z) \text{ for all } z, \overline z\}$ is the collection of unit $i$'s potential outcomes, and $\overline U_i$ is the average value of $U$ in the neighborhood of $i$, $\overline U_i = \sum_j A_{ij}U_j / \sum_j A_{ij}$.
Also, it holds that $f(Z_i = z, \overline Z_i = \overline z_i \mid \widetilde C_i, U_i, \overline U_i) > 0$.
\label{ass:network_ignorability}
\end{assumption}
%
\vspace{-10pt}
\paragraph{}
\cref{ass:sutva} on the potential outcomes and the ignorability \cref{ass:network_ignorability} are in-line with the definition of potential outcomes and the ignorability assumption for paired data discussed in \cref{sec:pairs}. In the case of paired data, the matrix $A$ is in the form of a blocked diagonal matrix with blocks of size two, the average neighborhood exposure $\overline z_i$ corresponds to the neighbor's treatment $z_j$, and the average neighborhood confounder $\overline u_i$ corresponds to the neighbor's covariate value $u_j$. Therefore, the potential outcomes $Y_i(z_i, \overline z_i)$ reduce to $Y_i(z_i, z_j)$, and the independence assumption in \cref{ass:network_ignorability} reduces to the independence statement in \cref{ass:paired_ignorability}, with the addition of measured covariates. Therefore, the estimands in \cref{subsec:sem_estimands} and method in \cref{sec:method} apply both to network and paired data, with the appropriate definition of the adjacency matrix $A$.



\subsection{Local and interference effects within a structural equation framework}
\label{subsec:sem_estimands}

We discuss estimands of interest within the realm of a structural equation model. 
Specifically, we assume that for known functions $f_1, f_2, f_3$, the potential outcomes arise according to
\begin{equation}
Y_i(z, \overline z) = f_1(z, \overline z) + f_2(\widetilde C_i) + f_3(U_i, \overline U_i) + \epsilon(z, \overline z)
\label{eq:sem}
\end{equation}
where $\epsilon(z, \overline z)$ are independent mean zero random variables. We assume that $f_1, f_2, f_3$ are linear, though non-linear functions could be easily accommodated, and \cref{eq:sem} reduces to
\begin{equation}
Y_i(z, \overline z) = \beta_0 + \beta_Z z + \beta_{\bar Z} \overline z + \widetilde C_i^T \bm \beta_C + \beta_U U_i + \beta_{\bar U} \overline U_i + \epsilon(z, \overline z).
\label{eq:linear_sem}
\end{equation}
According to this model, $\beta_Z$ and $\beta_{\bar Z}$ describe the local and interference effects of the exposure, respectively, with $\beta_Z$ representing the expected change in a unit's outcome for a unit increase in its own exposure when the neighborhood exposure remains fixed, and $\beta_{\bar Z}$ representing the expected change in a unit's outcome for a unit increase in its neighborhood exposure when its individual exposure remains fixed. 
% CUT!!!
% In the absence of interactions between $z, \overline z$, the local and interference effects do not vary with the level at which we fix the neighborhood and individual exposure, respectively, and in the absence of interactions between the exposure variables and the covariates, the causal effects are homogeneous across units.
The definitions of local and interference effects according to \cref{eq:linear_sem} agree with the corresponding definitions given in \cref{sec:pairs} for paired data with a binary treatment, and $\beta_Z = \lambda_i(z)$ and $\beta_{\bar Z} = \iota_i(z)$ for both $i$ and $z$.
Interaction terms between the local and neighborhood exposure, and the exposures and covariates could be straightforwardly incorporated in the structural model without additional complications.

Structural equation models have been previously employed for defining causal estimands in spatial settings with unmeasured confounders \citep{schnell2020mitigating, christiansen2022toward, papadogeorgou2022discussion}, interference \citep{giffin2022generalized}, or both \citep{giffin2021instrumental}, though model-free definitions using potential outcomes directly have also been employed \citep[e.g.][]{Verbitsky-savitz2012, Gilbert2021approaches}.

% \cite{sobel2014causal, vanderlaan2014causal}


\subsection{
%Confounding, interference and inherent spatial dependence induced bias in networks
The bias induced by spatial dependence in network data
}
\label{subsec:illustrate_bias_network}

We provide an example of how the variables' inherent spatial structure might occur in a network setting. This is merely an illustration, and it is not required below. We return to viewing the inherent spatial structure in $\bm U$ as driven from an underlying covariate $U^u$ as in \cref{fig:dag_underlying}. For $U^u = (U_1^u, U_2^u, \dots, U_n^u)$ vector of independent random variables, set $U_i = \sum_{j = 1}^n w_{ij} U_j^u + \epsilon_{i}$, for $w_{ij}$ not all zero and $\epsilon_i$ independent errors. Then the elements of $\bm U$ that share elements of $U^u$ are statistically dependent. If the weights $w_{ij}$ are based on the spatial proximity of $i$ and $j$, this dependence structure will be {\it spatially} driven. We can similarly conceive $Z^u$ and $\bm Z$.

Whether the coefficients in \cref{eq:linear_sem} are zero or not can be conceived in the same manner as to whether the corresponding arrows are missing or not in the graphs of \cref{fig:graphs}. 
% CUT!!! 
%For example, the structural model for $\beta_{\bar Z} = \beta_{\bar U} = 0$ is consistent with a graph resembling that in \cref{fig:direct}. 
Under the different scenarios of \cref{fig:graphs}, the ignorability \cref{ass:network_ignorability} might also hold conditional on $\widetilde C$ only, conditional on $\widetilde C$ and $U$, or might only hold conditional on all of $\widetilde C, U$ and $\overline U$.
%
In Supplement \ref{supp_sec:motivating_one_network}, we investigate the influence of spatial dependencies in learning local and interference effects from a single interconnected network. The conclusions are the same as the ones for paired data: \begin{enumerate*}[label=(\alph*)]
\item spatial confounding and interference can manifest as each other, 
\item inherent spatial dependencies complicate standard estimation strategies and can render them invalid even in simple settings, 
\item controlling for local and neighborhood covariates is crucial for adjusting for confounding and estimating causal effects unbiasedly, and 
\item local and interference effects should be investigated simultaneoulsy in the presence of spatial dependencies.
\end{enumerate*}



\section{Bayesian inference of local and interference effects with spatial dependencies and unmeasured spatial confounding}
\label{sec:method}

Until now we have focused on the complications of identifying causal quantities in spatial settings with confounding, interference, and inherent spatial structure. When the measured covariates do not suffice for confounding adjustment, biases in effect estimation will persist.
In what follows, we develop a Bayesian approach that aims to address these complications for estimating causal effects in the presence of spatial dependencies, while simultaneously mitigating bias due to local and neighborhood unmeasured spatial confounding. 
% CUT!!!
% Even if the unmeasured covariate is spatial, including a spatial random effect in the outcome regression will not necessarily reduce bias from unmeasured confounders \citep{schnell2020mitigating, reich2021review}.
In \cref{subsec:bayesian_treatment}, we show that the treatment assignment mechanism has to be incorporated in the Bayesian procedure in the presence of missing confounders.
%By viewing the inherent spatial structure in the confounder and the exposure as underlying, unobservable covariates as in \cref{fig:dag_underlying}, we investigate the impact of spatial structure and unmeasured confounding within the Bayesian framework for causal inference.
These derivations drive the assumptions on the relationship of the unmeasured confounder and the exposure in \cref{subsec:UZ_assumptions}. 
In \cref{subsec:priors}, we design sensible prior distributions for the hyperparameters of the spatial confounder by showing the relationship between the prior choice and the implied prior beliefs of the unmeasured covariate's confounding strength.



\subsection{The role of the treatment assignment mechanism in spatial settings}
\label{subsec:bayesian_treatment}


Bayesian causal inference views unobserved potential outcomes as missing data, and inference on causal effects is acquired from their posterior distribution \citep{rubin1978bayesian, imbens1997bayesian, ding2018causal, li2022bayesian}.
Let $\bm Z = (Z_1, Z_2, \dots, Z_n)$ denote the vector of realized exposures, $\overline{\bm Z} = (\overline Z_1, \overline Z_2, \dots, \overline Z_n)$ the vector of neighborhood exposures, $\bm C = (\widetilde C_1, \widetilde C_2, \dots, \widetilde C_n)$ the $n \times p_c$ matrix of measured covariates, and
$\bm Y(\cdot)  = \{Y_i (\cdot), \text{ for all } i \} $ the collection of all potential outcomes for all units.
Let also $\bm Y(\cdot) = \{ \bm Y, \bm Y^{\text{miss}} \}$ where  $\bm Y = (Y_1, Y_2, \dots, Y_n)$ are the observed outcomes and $\bm Y^{\text{miss}}$ is the collection of {\it un}observed potential outcomes.
% Here, it is helpful to return to \cref{fig:dag_underlying}, and think of the spatial structure in $\bm U, \bm Z$ as driven by underlying variables $U, Z$ that are unobservable. Therefore, all inherent spatial dependent in $\bm U, \bm Z$ is captured by $U, Z$. Conditional on $U, Z$ we assume unit-exchangeability \citep{de1937foresight}, which allows us to write
% \begin{align*}
% p(\bm Y(\cdot), \bm Z, \overline{\bm Z}, \bm C ) &=
% \int p(\bm Y(\cdot), \bm Z, \overline{\bm Z}, \bm C \mid U, Z) \ p(U, Z) \ \mathrm{d}U \mathrm{d}Z \\
% &= \int \prod_{i = 1}^n p(\bm Y_i(\cdot), Z_i, \overline Z_i, \widetilde C_i \mid U, Z, \theta) \ p(\theta \mid U, Z) \ \mathrm{d}\theta \ p(U, Z) \ \mathrm{d}U \mathrm{d}Z \\
% &= \int \prod_{i = 1}^n p(\bm Y_i(\cdot), Z_i, \overline Z_i, \widetilde C_i \mid U, Z, \theta) \ p(U, Z \mid \theta) \ p(\theta) \  \mathrm{d}U \mathrm{d}Z \mathrm{d}\theta
% \end{align*}
% for some vector of parameters $\theta$.
Bayesian inference proceeds by specifying
\(
p(\bm Y(\cdot), \bm Z, \overline{\bm Z}, \bm C \mid \theta)
\) 
and prior distribution \( p(\theta) \),
and imputing missing potential outcomes from
\begin{equation}
\begin{aligned}
p(\bm Y^{\text{miss}} \mid \bm Y, \bm Z, \overline{\bm Z}, \bm C, \theta)
%\propto & \ P(\bm Y(\cdot) , \bm Z, \overline{\bm Z}, \bm C \mid \theta) \\
\propto & \ P(\bm Z, \overline{\bm Z} \mid \bm Y(\cdot), \bm C, \theta) \  P(\bm Y(\cdot) \mid \bm C, \theta) \ P(\bm C \mid \theta).
\end{aligned}
\label{eq:impute_Ymiss}
\end{equation}
If ignorability {\it requires} that we condition on the unmeasured confounder (both \cref{eq:not_ignorable} and \cref{ass:network_ignorability} hold), then
\(
P(\bm Z, \overline{\bm Z} \mid \bm Y(\cdot), \bm C, \theta)
\neq
P(\bm Z, \overline{\bm Z} \mid \bm C, \theta)
\). 
As a result, the treatment assignment mechanism will not ``drop out'' from \cref{eq:impute_Ymiss}, and it will be informative for the imputation of missing potential outcomes \citep{mccandless2007bayesian, ricciardi2020bayesian}.
We return to the full data likelihood and write
\begin{align*}
p(\bm Y(\cdot), \bm Z, \overline{\bm Z}, \bm C)
&=
\int p(\bm Y(\cdot), \bm Z, \bm C, \bm U \mid \theta) \ \mathrm{d}\bm U \ p(\theta) \ \mathrm{d}\theta \\
&= \int
p(\bm Y(\cdot) \mid \bm Z, \bm C, \bm U, \theta) \ 
p(\bm Z \mid \bm C, \bm U, \theta) \ 
p(\bm U \mid \bm C, \theta) \ 
p(\bm C \mid \theta)
\ \mathrm{d} \bm U \ p(\theta) \ \mathrm{d}\theta 
\end{align*}
where $\overline{\bm Z}$ is excluded since it is uniquely defined based on $\bm Z$, and $\theta$ has been extended to include parameters governing $\bm U$. 
The structural model \cref{eq:sem} implies conditional independence among the outcomes of different units which can only depend on the vector of exposures and the unmeasured covariate through the local and neighborhood values. Therefore, we write
\begin{align*}
p(\bm Y(\cdot) \mid \bm Z, \bm C, \bm U, \theta) &= \prod_{i = 1}^n p(Y_i(\cdot) \mid Z_i, \overline Z_i, \widetilde C_i, U_i, \overline U_i, \theta)
= \prod_{i = 1}^n p(Y_i(\cdot) \mid \widetilde C_i, U_i, \overline U_i, \theta),
\end{align*}
where the last equality holds from \cref{ass:network_ignorability}. We can re-write the likelihood as
\begin{equation}
\int \Big[ \prod_{i = 1}^n p(Y_i(\cdot) \mid \widetilde C_i, U_i, \overline U_i, \theta) \Big] \ 
p(\bm Z \mid \bm C, \bm U, \theta) \ 
p(\bm U \mid \bm C, \theta) \ 
p(\bm C \mid \theta)
\ \mathrm{d} \bm U \ p(\theta) \ \mathrm{d}\theta.
\label{eq:bayesian_framework}
\end{equation}
Therefore, having access to $\bm U$ (in addition to $\bm C$) would in fact render the treatment assignment ignorable within the Bayesian framework. However, since $\bm U$ is unknown, and it plays a role in the distribution of the treatment assignment in \cref{eq:bayesian_framework}, the treatment assignment has to be incorporated in a valid Bayesian procedure for imputing the missing potential outcomes. 



These derivations provide important insights.
They show from a new perspective that simply including a spatial random effect in the outcome model can\textit{not} control for unmeasured spatial confounding, and adopting an exposure model is necessary for proper inference of causal effects within the Bayesian paradigm.
They also illuminate that it is necessary to specify joint distributions on the unmeasured and measured variables in order to proceed.

\subsection{Exposure-confounder assumptions}
\label{subsec:UZ_assumptions}

It is evident from \cref{eq:bayesian_framework} that one needs to specify the joint distribution of $\bm U, \bm Z$ given the measured covariates. For continuous exposures, we make the following assumption:
\begin{assumption}
% CUT!!!
%[Joint distribution of the spatial confounder and exposure]
The unmeasured spatial confounder and the spatial exposure have a joint normal distribution conditional on the measured covariates. Specifically,
\begin{equation}
\begin{pmatrix} \bm U \\ \bm Z \end{pmatrix} \Big| \  \bm C \sim N_{2n} \left(
\begin{pmatrix} \bm 0_n \\ \gamma_0 \bm 1_n + \bm C^T \bm \gamma_C \end{pmatrix} ,
\begin{pmatrix} G & Q \\ Q & H \end{pmatrix}^{-1}
\right),
\label{eq:UZ_normal}
\end{equation}
for $\bm \gamma_C$ vector of length $p_c$, and $G, H$ positive definite matrices. The matrix $Q$ is diagonal with elements $q_i = - \rho \sqrt{g_{ii}h_{ii}}$, where $g_{ii}, h_{ii}$ are the diagonal elements of $G, H$, respectively.
\label{ass:UZ_normal}
\end{assumption}
\noindent

The joint distribution of $\bm U, \bm Z$ is parameterized through its precision matrix, and elements of the precision matrix that are equal to zero indicate conditional independence of the corresponding variables. Under this light, the assumption that $Q$ is diagonal is a statistical representation of the absence of an arrow from $U_i$ to $Z_j$ in the graphs of Figures \ref{fig:dag} and \ref{fig:graphs}. That is because $Q$ being diagonal represents $Z_i \indep \bm U_{-i} \mid U_i, \bm Z_{-i}, \bm C$ for all $i$, where 
$\bm U_{-i} = (U_1, U_2, \dots, U_{i-1}, U_{i + 1}, \dots, U_n)$ and $\bm Z_{-i}$ is defined similarly.
% \cref{ass:UZ_normal} further specifies that the conditional correlation among the local confounder and exposure value is constant across locations.
Even though $\bm U$ does not depend on $\bm C$ in \cref{ass:UZ_normal}, it is reasonable to do so, since the part of the unmeasured variable that is correlated with measured covariates is already adjusted for. The joint distribution in \cref{ass:UZ_normal} has been previously adopted in a related setting \citep{schnell2020mitigating}. Our work illustrates two crucial parts with regards to this assumption: we have shown that adopting a joint distribution on $(\bm U, \bm Z)$ is necessary within the Bayesian framework 
(\cref{subsec:bayesian_treatment}), and we have linked a distributional assumption (diagonal $Q$ in the precision matrix) to the relationships of variables viewed through the causal graph representation. Therefore, we provide new insights on the role and interpretation of the distributional \cref{ass:UZ_normal} through the lens of Bayesian causal inference.


Since we only have one realization of the spatial exposure $\bm Z$, the conditional precision matrices $G$ and $H$ cannot be estimated without imposing some structure on their elements. We assume that $G$ and $H$ are known up to parameter vectors $\theta_U = (\tau_U, \phi_U)$ and $\theta_Z = (\tau_Z, \phi_Z)$, respectively.
To ease prior elicitation in \cref{subsec:priors} that is consistent for both areal and point-referenced data, we specify $G = \tau_U^2 (D - \phi_U A)$ in either case.
For areal data, the matrix $A$ is the binary adjacency matrix, whereas for point-referenced data, one could specify $A_{ij} = \exp(- d_{ij})$ for $i \neq j$ where $d_{ij}$ represents a measure of geographical distance of locations $i$ and $j$, and $A_{ii} = 0$. In both cases, $D$ is the diagonal matrix with entries $d_i = \sum_j A_{ij}$. Similarly for $H$.

The network's adjacency matrix is used for defining the neighborhood exposure and covariate in Assumptions \ref{ass:sutva} and \ref{ass:network_ignorability}, and in the joint precision matrix in \cref{ass:UZ_normal}.
% CUT!!!
% Since $H$ is the {\it conditional} precision matrix of the exposure given measured covariates and the unmeasured covariate $\bm U$, it represents the {\it inherent} spatial dependence in the exposure. 
% By viewing $G$ and $H$ under this light,
However, these two structures need not be the same, and researchers could specify the same or different adjacency matrices for these two components. 
Furthermore, the functional form with which the exposure is included in the outcome model in \cref{eq:linear_sem} can differ from the one in the joint distribution of \cref{ass:UZ_normal}. Therefore, the framework can easily allow for non-continuous exposures to be considered.
%In the case of binary exposures, one can use a probit or logistic model for the spatial treatment variable and impose \cref{ass:UZ_normal} on the underlying linear predictor.
We illustrate these points in our data analysis in \cref{sec:application}. 




\subsection{Prior distributions for confounding adjustment}
\label{subsec:priors}

In Bayesian settings, model performance often depends on the choice of hyperparameters, and non-informative priors can lead to poor performance in certain settings \citep{gelman2008weakly}. We adopt weakly informative prior distributions for model parameters. These parameters are the intercept, the coefficients of the measured covariates, the coefficients of the local and neighborhood exposure and confounder values, and the variance of the residual error in \cref{eq:linear_sem}, and the intercept, the coefficients of the measured covariates, and the parameters of the covariance matrix in \cref{eq:UZ_normal}.

We consider measured covariates, exposure and outcome that are standardized to have mean 0 and variance 1.
We adopt independent $N(0, \sigma^2_{\text{prior}})$ prior distributions for the coefficients of all measured covariates, $\bm \beta_C, \bm \gamma_C$ and for the intercepts $\beta_0, \gamma_0$, with $\sigma^2_{\text{prior}} = 2$. 
% Next, we focus on the coefficients of the local and neighborhood confounder values, $\beta_U, \beta_{\bar U}$.
The coefficients $\beta_U, \beta_{\bar U}$, and the parameter $\tau_U$ of the precision matrix $G$ are not identifiable up to scaling of the unmeasured confounder $\bm U$. To see this, consider $\bm U' = c \bm U$ for some $c \neq 0$. Then, setting $\beta_U' = \beta_U / c$, $\beta_{\bar U}' = \beta_{\bar U} / c$, and ${\tau_U^{2}}' = \tau_U^2 / c^2$ will lead to the same value of the likelihood for $(\bm Y, \bm Z, \bm U) \mid \bm C$. Therefore, without loss of generality, we set $\beta_U = 1$. Even though at first sight it might appear that we ``force'' $U$ in the outcome model by setting $\beta_U = 1$, we discuss below that our prior for $\tau_U$ ensures that this is not the case. We adopt a $N(0, \sigma^2_{\text{prior},\overline U})$ prior distribution for $\beta_{\bar U},$ and we set $\sigma^2_{\text{prior},\overline U} = 0.35^2$ to express the prior belief that the importance of the neighborhood value of the confounder is smaller than that of the local value of the confounder.

We specify $\sigma^2_Y \sim IG(\alpha_Y, \beta_Y)$, where $\sigma^2_Y$ is the residual variance of the outcome model in \cref{eq:linear_sem}. We follow a data-driven procedure for the hyperparameters $\alpha_Y, \beta_Y$. We regress the outcome on the measured local and neighborhood exposure and the measured covariates and acquire the estimated residual variance, $\widetilde \sigma^2_Y$. 
%It is reasonable to expect that including the additional predictors $U, \overline U$ in the outcome regression should reduce the residual variance. 
We set $\alpha_Y = 3$ and $\beta_Y = 3 \ \widetilde \sigma^2_Y / 4$, which leads to a prior distribution on the residual variance that puts most of its weight on values smaller than $\widetilde \sigma^2_Y$, and specifically $P(\sigma^2_Y < \widetilde \sigma^2_Y) \approx 0.98$.

Next, we specify prior distributions for the parameters of the joint precision matrix in \cref{eq:UZ_normal}. We specify flat priors for $\phi_Z, \rho$ on the $(-1, 1)$ interval. We assume that $\phi_U > 0$, and specify $\phi_U \sim \text{Beta}(6, 6)$ to encourage values that imply some spatial dependence ($\phi_U$ away from 0) while avoiding degenerate distributions ($\phi_U$ away from 1). We also require that $\phi_U < \phi_Z$ since the exposure should vary within levels of the confounder, in line with conclusions from \cite{Paciorek2010}, \cite{schnell2020mitigating} and \cite{dupont2022spatial}.

Lastly, we decide on prior distributions for $\tau_U, \tau_Z$. The priors for these parameters can have a large effect on model performance, and their choice requires careful considerations. For simplicity we discuss in detail the situation where all nodes have the same number of neighbors, and $D = dI_n$ for some $d > 0$ and $I_n$ being the $n \times n$ identity matrix.
We can show that $\Var(U_i) \geq (d \tau_U^2)^{-1}$, where equality holds for $\rho = 0$. Since $U_i$ is a priori centered at 0, if the marginal variance of $U_i$ is small for all $i$, the vector of $\bm U$ is almost indistinguishable from the vector of all zeros, and essentially drops out from the outcome model (even though we have set $\beta_U = 1$). 
% --- POTENTIAL TO DO --- % In sims can we show that U drops out when not important?
Reversely, if the marginal variance of $U_i$ is big a priori, then this prior distribution would imply a strong importance of $U$ in the outcome model (considering $\beta_U = 1$).
Therefore, our choice for the prior distribution of the hyperparameter $\tau_U$ should be informed by the implied prior distribution on the unmeasured covariate's confounding strength. We specify a prior distribution for $\tau_U$ which avoids the pathological situation that $U$ has an unrealistically high predictive accuracy for the outcome. Specifically,
we specify that $1 / \tau_U$ has a truncated mean-zero normal distribution with variance $d \sigma^2_{\text{prior}} / 2$, and truncated below at 0. 
The induced prior on $(d \tau_U^2)^{-1}$ ensures that the unmeasured variable's strength in the outcome model resembles, a priori, the measured covariates' strength in the outcome model specified by the $N(0, \sigma^2_{\text{prior}})$ prior distribution on their coefficients.

Similarly, the magnitude of $1 / (d \tau_Z^2)$ can be conceived as the variance in the exposure that cannot be explained by covariates. Therefore, $1 / \tau_Z^2$ should not be too small because we expect {\it some} inherent variability in the exposure. At the same time, it should not be too big in comparison to the residual variance of the regression of $Z$ on the measured covariates, denoted by $\widetilde \sigma^2_Z$. 
Let $\widetilde s^2_Z$ be the observed marginal variance in the exposure across locations. We specify that $1 / \tau_Z$ follows a truncated normal distribution centered at $\sqrt{d \ \widetilde \sigma^2_Z / 2}$ with standard deviation 1, truncated below at $\sqrt{d \ 0.01 \ \widetilde s^2_Z}$ and above at $\sqrt{d \ \widetilde \sigma^2_Z / 0.8}$. 

The prior distributions on $\tau_U$ and $\tau_Z$ are illustrated in Supplement \ref{supp_sec:priors}. When the degree is not constant across nodes (which is the case in most networks), we set $d$ to be the median network degree.
We sample from the posterior distribution of model parameters using Markov chain Monte Carlo (MCMC). The algorithm is described in Supplement \ref{supp_sec:mcmc}.


\section{Simulations}
\label{sec:sims}

We perform simulations to evaluate the extent to which the approach introduced in \cref{sec:method} mitigates the bias in estimating local and interference causal effects that is caused by direct and indirect unmeasured spatial confounding and the inherent spatial dependence in the exposure.
We simulate data under the data generative mechanisms in \cref{fig:graphs}. We consider observations on a network of interconnected units represented by a line graph, where each unit is connected with two others, except for the first and last units which only have one neighbor each. 
%The exact form of the adjacency matrix for this setting is given in Supplement \ref{supp_sec:motivating_one_network}. 
Simulations with pairs of interacting observations are deferred to Supplement \ref{supp_sec:sims_pairs}.


For number of units $n \in \{200, 350, 500\}$, we generated four measured covariates from independent $N(0, 1)$ distributions, the unmeasured confounder and the exposure of interest from \cref{eq:UZ_normal}, and calculated $\overline U, \overline Z$ for each observation. The outcome was generated according to the linear model in \cref{eq:linear_sem} with $\gamma_0 = \beta_0 = 0$. The coefficients of the measured covariates were generated randomly once and were fixed to $\bm \gamma_C = (-0.35, -0.64,  0.49,  0.06)$ and $\bm \beta_C = (0.06, 0.85, 0.02, 0.33)$ throughout our simulations. We specified spatial parameters $\phi_U = 0.6, \phi_Z = 0.4,$ and $\rho = 0.35$.
For the network data, for which median node degree is equal to 2, we set $\tau^2_U = \tau^2_Z = 1$. In all cases, we set the outcome residual error variance to one. The default outcome model coefficients of the local and neighborhood exposure and unmeasured confounder were set to $\beta_Z = 1,$ $\beta_{\bar Z} = 0.8,$ $\beta_U = 1$, and $\beta_{\bar U} = 0.5$, except for when the corresponding relationship does not exist in the scenarios of \cref{fig:graphs}. These specifications match exactly the motivating simulations for the network data discussed in \cref{subsec:illustrate_bias_network} which are presented in detail in Supplement \ref{supp_sec:motivating_one_network}.

We generate 500 data sets under each of the 36 different scenarios which are combinations of the six scenarios in \cref{fig:graphs}, the three sample sizes, and for network and paired data.
We compare the proposed approach to OLS conditional on the measured covariates for estimating local and interference effects.
In the absence of unmeasured spatial confounding, the OLS estimator is most efficient for estimating causal effects since it is based on the correctly specified outcome model. Therefore, this comparison informs us of potential efficiency loss when spatial confounding is considered but it is not truly present.
In the presence of unmeasured confounding, the OLS estimator incurs confounding bias. Therefore, comparing the two approaches in this setting illuminates the extent to which our approach can alleviate this bias.
We fit our method using two chains with 7,000 iterations as a burn-in period and thinning by 60, and evaluated MCMC convergence by examining whether the $\widehat R$ statistic for $\beta_Z$ and $\beta_{\bar Z}$ \citep{vehtari2021rank} was at most 1.02.



\begin{table}[!t]
    \centering
\spacingset{1.25}
    \caption{Simulation results with One Interconnected Network. Bias, root mean squared error (rMSE), and coverage of 95\% intervals based on OLS and the method of \cref{sec:method}, for the local and the interference effect. We show simulation results for the 6 settings in \cref{fig:graphs}, and for sample size equal to 200, 350 and 500. Coverage rates are reported as percentages.}
    % \rowcolors{5}{}{gray!10}
\spacingset{1}
    \resizebox{0.95\textwidth}{!}{%
    \begin{tabular}{*{15}{c}}
        % \hline
        % \\[-10pt]
        % & & \multicolumn{13}{c}{Network data} \\[-5pt] \\
        % \cmidrule{3-15}
       & & \multicolumn{6}{c}{Local effect} & &  \multicolumn{6}{c}{Interference effect} \\
       \cmidrule(lr){3-8} \cmidrule(lr){10-15}
        \multicolumn{2}{c}{True model \&} & \multicolumn{3}{c}{OLS} &  \multicolumn{3}{c}{Our approach} &
        & \multicolumn{3}{c}{OLS} &  \multicolumn{3}{c}{Our approach} \\
        \cmidrule(lr){3-5} \cmidrule(lr){6-8} \cmidrule(lr){10-12} \cmidrule(lr){13-15}  
        \multicolumn{2}{c}{sample size} & Bias & rMSE & Cover & Bias & rMSE & Cover &
        & Bias & rMSE & Cover & Bias & rMSE & Cover \\[5pt]
    
        \hline
        \\[-10pt]
        \multirow{4}{*}{\ref{fig:direct}} & & \multicolumn{13}{c}{$\beta_{\bar Z} = 0$ and $\beta_{\bar U}=0$} \\[0pt]
        \cmidrule{7-11}
        & 200 & 0.492 & 0.505 & 0.3 & -0.021 & 0.313 & 91.9 & & 0.154 & 0.186 & 72.3 & 0.028 & 0.140 & 91.9  \\ 
        & 350 & 0.499 & 0.507 & 0 & -0.097 & 0.264 & 95.7 & & 0.146 & 0.167 & 55\phantom{.0} & -0.004 & 0.095 & 96.6 \\
        & 500 & 0.510 & 0.516 & 0 & -0.109 & 0.230 & 97.3 & &  0.157 & 0.172 & 36\phantom{.0} & 0.003 & 0.086 & 93.8 \\[3pt]
        \hline
        \\[-10pt]

        
        \multirow{4}{*}{\ref{fig:interference}} & &
        \multicolumn{13}{c}{$\beta_{UZ} = 0$ and $\beta_U = \beta_{\bar U} = 0$}
        \\[0pt]
        \cmidrule{7-11}
        & 200 & 0.003 & 0.097 & 95.3 & -0.035 & 0.121 & 98.7 & & 0.004 & 0.086 & 95.7 & 0.001 & 0.090 & 95.6 \\
        & 350 & -0.002 & 0.072 & 96\phantom{.0} & -0.034 & 0.108 & 99.5 & & -0.007 & 0.066 & 94.3 & -0.013 & 0.068 & 95.2 \\
        & 500 & 0.001 & 0.067 & 92.7 & -0.023 & 0.098 & 99.3 & & -0.001 & 0.056 & 94\phantom{.0} & -0.006 & 0.058 & 94.7 \\[3pt]
        \hline
        \\[-10pt]
         
        \multirow{4}{*}{\ref{fig:general_spatial_conf}} & &
        \multicolumn{13}{c}{$\beta_{\bar Z} = 0$}
        \\[0pt]
        \cmidrule{7-11}
        & 200 & 0.616 & 0.630 & 0 & -0.165 & 0.301 & 96.9 & & 0.277 & 0.302 & 34.3 & 0.028 & 0.147 & 93.3 \\
        & 350 & 0.624 & 0.632 & 0 & -0.191 & 0.286 & 93.3 & & 0.273 & 0.288 & 15\phantom{.0} & 0.004 & 0.106 & 95.7  \\
        & 500 & 0.639 & 0.644 & 0 & -0.177 & 0.257 & 94.2 & & 0.289 & 0.299 & \phantom{0}3.7 & 0.011 & 0.093 & 94.8 \\[3pt]
        \hline
        \\[-10pt]
        
        \multirow{4}{*}{\ref{fig:direct_interference}} & &
        \multicolumn{13}{c}{$\beta_{\bar U} = 0$} \\[0pt]
        \cmidrule{7-11}
        & 200 & 0.492 & 0.505 & 0.3 & 0.057 & 0.304 & 88.4 & & 0.154 & 0.186 & 72.3 & 0.032 & 0.137 & 92.9 \\
        & 350 & 0.499 & 0.507 & 0 & -0.052 & 0.247 & 95\phantom{.0} & & 0.146 & 0.167 & 55\phantom{.0} & -0.003 & 0.101 & 95\phantom{.0} \\
        & 500 & 0.510 & 0.516 & 0 & -0.050 & 0.209 & 98.6 & & 0.157 & 0.172 & 36\phantom{.0} & 0.002 & 0.086 & 94.5 \\[3pt]
        \hline
        \\[-10pt]
         
        \multirow{4}{*}{\ref{fig:predictor_interference}} & &
        \multicolumn{13}{c}{$\beta_U = 0$ and $\beta_{\bar U} = 0$} \\[0pt]
        \cmidrule{7-11}
        & 200 & 0.002 & 0.085 & 96\phantom{.0} & -0.034 & 0.121 & 99.2 & & 0.003 & 0.081 & 96.3 & 0.004 & 0.086 & 95.3 \\
         & 350 & 0.000 & 0.064 & 95.7 & -0.036 & 0.122 & 97.4 & & -0.006 & 0.063 & 94\phantom{.0} & -0.016 & 0.068 & 95.6 \\ 
        & 500 & 0.001 & 0.060 & 92.7 & -0.044 & 0.142 & 96.5 & & -0.001 & 0.054 & 94\phantom{.0} & -0.003 & 0.055 & 94.4 \\[3pt]
        \hline
        \\[-10pt]
       
        \multirow{3}{*}{\ref{fig:general_interference}} 
        & 200 & 0.616 & 0.630 & 0 & -0.144 & 0.283 & 96.9 & & 0.277 & 0.302 & 34.3 & 0.018 & 0.144 & 94.4 \\ 
        & 350 & 0.624 & 0.632 & 0 & -0.154 & 0.254 & 94\phantom{.0} & & 0.273 & 0.288 & 15\phantom{.0} & 0.001 & 0.106 & 95.6 \\ 
        & 500 & 0.639 & 0.644 & 0 & -0.146 & 0.230 & 95.4 & & 0.289 & 0.299 & \phantom{0}3.7 & 0.007 & 0.092 & 95\phantom{.0} \\[1pt]
        \hline
        \end{tabular}
     }%
     \label{tab:sims_network}
\end{table}


Results for the network data simulations are shown in \cref{tab:sims_network}. We show bias, root mean squared error (rMSE), and coverage of 95\% intervals (confidence intervals for OLS, credible intervals for the Bayesian method).
When the unmeasured variable does not confound the relationship of interest (scenarios \ref{fig:interference} and \ref{fig:predictor_interference}) and OLS performs well, our method remains essentially unbiased for both the local and the interference effects. In these settings where our approach is not necessary for controlling for the unmeasured variable, it has larger rMSE than OLS for the local effect, but the two approaches have similar rMSE for estimating the interference effects. These results indicate that the proposed approach might avoid efficiency loss in estimating interference effects when accounting for unmeasured confounding, even when it is not present.
In all other cases where unmeasured confounding exists (scenarios \ref{fig:direct}, \ref{fig:general_spatial_conf}, \ref{fig:direct_interference}, and \ref{fig:general_interference}), our approach returns significantly lower bias and achieves substantially lower rMSE in comparison to OLS for all sample sizes, and for both local and interference effects. Furthermore, the proposed approach achieves close to nominal coverage in all scenarios and for both local and interference effects. Our simulations illustrate that our method can protect from biases arising due to unmeasured spatial variables, while ensuring proper inference.
Simulations for paired data are shown in Supplement \ref{supp_sec:sims_pairs}, and the conclusions are identical.



\section{Analyzing environmental health data}
\label{sec:application}

To illustrate our method, we investigate the relationship between sulfur dioxide (SO$_2$) emissions from power plants and cardiovascular health. SO$_2$ emissions contribute to particulate air pollution which has been linked to a number of adverse health outcomes \citep{dominici2014particulate}, and emissions from one location can travel and potentially affect the outcomes in other locations \citep{henneman2019accountability, Zigler2021}. We investigate the relationship between local and neighborhood SO$_2$ emissions from power plants on cardiovascular deaths among the elderly (65 years old or older) while aiming to mitigate bias from potentially missing spatial confounders.  Our data set includes the 445 counties in the continental US with local and neighborhood SO$_2$ emissions.
For a county, neighborhood SO$_2$ emissions are defined using a 2nd degree adjacency matrix: the total SO$_2$ emissions from neighboring counties, and neighbors of neighboring counties. 
A description of data compilation and visualizations are included in Supplement \ref{supp_sec:application}.

We analyzed the data using 
\begin{enumerate*}[label=(\alph*)]
\item OLS, and our approach based on a 1st degree, or a 2nd degree adjacency matrix for the spatial relationship of variables,
\item local and neighborhood values for power plant covariates only, or power plant and demographic covariates, and
\item including the exposure variable linearly or logarithmically in the outcome model.
\end{enumerate*}
These analyses illustrate a researcher's options for analyzing spatial data using our method. 
Note that the neighborhood exposure is defined based on a 2nd degree adjacency matrix. Therefore, our approach that uses a 1st degree adjacency matrix in the precision matrix of \cref{eq:UZ_normal} illustrates that a researcher can use different adjacency matrices for different aspects of the analysis. Also, for the analyses based on our method, the exposure is included as is in the joint distribution of \cref{eq:UZ_normal}. Therefore, our approach that uses a logarithmic transformation of the exposure in the outcome model illustrates that different functions of the exposure can be considered in different parts of the model specification.




% \begin{table}[!t]
% \spacingset{1.25}
% \caption{Local and neighborhood effect estimates from ordinary least squares with local exposure only, and with local and neighborhood exposure, and results from our approach that includes potential local and neighborhood unmeasured spatial variable in the outcome model. Estimates correspond to the maximum likelihood estimates and the posterior mean for the frequentist and Bayesian approaches, while CI corresponds to 95\% confidence and credible intervals, respectively.}
% \small
%     \centering
%     \begin{tabular}{ccccc}
%     & \multicolumn{2}{c}{Local effects} & \multicolumn{2}{c}{Interference effects} \\
%     \cmidrule(lr){2-3} \cmidrule(lr){4-5}
%     & Estimate & CI & Estimate & CI \\ \hline
%     %\cmidrule{2-2} \cmidrule{3-3} \cmidrule{4-4} \cmidrule{5-5}
%     OLS  & \phantom{(-17.27, 15.61)} & & \phantom{(-17.27, 15.61)} \\
%     local exposure & -0.83 &  (-17.27, 15.61) & -- & -- \\
%     only \\ \hline
%     OLS     &  \\
%     local \& neighborhood & -2.12 & (-18.50, 14.25) & -11.45 & (-20.53, -2.38) \\
%     exposure & \\
%     \hline
%     Our approach \\
%     First degree & -7.65 & (-36.86, 20.51) & -7.32 & (-14.52, 0.07) \\
%     adjacency matrix \\ \hline
%     Our approach \\
%     Second degree & -1.91 & (-45.80, 37.39) & -7.34 & (-14.61, -0.26) \\ 
%     adjacency matrix \\ \hline
%     \end{tabular}
%     \label{tab:app_results}
% \end{table}

\begin{figure}[!t]
\hspace{0.15\textwidth}
\includegraphics[width=0.25\textwidth,trim=170 145 270 10,clip]{app_results.pdf}
\hspace{0.14\textwidth}
\includegraphics[width=0.25\textwidth,trim=280 145 160 10,clip]{app_results.pdf} \\
%
\includegraphics[width=\textwidth,trim=17 0 0 25,clip]{app_results.pdf}
\vspace{-25pt}
\caption{Local and neighborhood effect estimates from OLS and the proposed approach based on 1st or 2nd degree neighbor specification for the spatial variables. The exposure is included in the outcome model linearly (left two panels) or logarithmically (right two panels). Estimates and 95\% intervals correspond to the maximum likelihood estimates and confidence interval for OLS, and the posterior mean and credible interval for our approach.}
    \label{fig:app_results}
\end{figure}

The results are shown in \cref{fig:app_results} as the estimated change in the number of deaths per 100,000 residents for a one standard deviation increase in the local or neighborhood exposure. The estimates from all analyses are comparable. Our approach almost always returns effect estimates that are more positive, towards the expected relationship between SO$_2$ emissions and cardiovascular mortality, illustrating that there is potential spatial confounding with a 1st or 2nd degree spatial structure. Our approach using a linear function of the exposure in the outcome model and a 2nd degree adjacency matrix for the unmeasured spatial confounder returns statistically significant effect estimates for the interference effect, with 51.9 (95\% CI: 3.3 to 101.1) deaths caused for a one standard deviation increase in the neighborhood SO$_2$ emissions. OLS analyses including weather variables returned similar estimates (see Supplement \ref{supp_sec:application}).




\section{Discussion}

In this manuscript, we have discussed the inherent challenges and opportunities that arise in causal inference with spatial data. We have illustrated the complications that arise from the data's inherent dependence structure, and discussed the interplay of spatial confounding and interference.  Unmeasured spatial confounding does not necessarily have to exist due to a completely missed covariate. Instead, it is possible that a confounder is mis-measured, or its functional form not correctly included in the outcome model. In these settings, employing a procedure that mitigates bias from unmeasured spatial covariates can improve estimation and inference.

Despite the potential merits of this work, important open questions remain in order to better understand causality in dependent settings.
In all scenarios we considered here, we assumed that the outcome is not inherently spatial, though it might exhibit spatial dependence when its spatial predictors are not conditioned on. We suspect that an inherently spatial outcome variable will create additional complications in defining and estimating local and interference effects. We believe that our advancements in drawing causal graphs in spatial settings in \cref{sec:pairs} and interpreting the spatial dependencies as a common underlying spatial trend in \cref{fig:dag_underlying} can provide a way forward in this setting. It is worth noting here that an inherently dependent outcome variable is entirely different from outcome dependencies that occur through contagion, and the interplay of the two in estimating causal effects is an interesting question for future research.

% when people include a smoothed version of the exposure, its trying to created Zu, Uu. -- I think this is also related to the group-level unconfoundedness in the Imbens paper "Fixed Effects and the Generalized Mundlak Estimator" and somehow also in including group-level indicators in the PS paper by fabri



\spacingset{1.35}
\bibliographystyle{plainnat}
\bibliography{Spatial-Interference}
\clearpage


\begin{center}
{\Large Supplementary Materials}
\end{center}


\appendix
\setcounter{page}{1}
\setcounter{section}{0}    
\renewcommand{\thesection}{\Alph{section}}
\setcounter{equation}{0}    
\renewcommand{\theequation}{S.\arabic{equation}}
\setcounter{table}{0}    
\renewcommand{\thetable}{S.\arabic{table}}
\setcounter{figure}{0}    
\renewcommand{\thefigure}{S.\arabic{figure}}
\setcounter{proposition}{0}    
\renewcommand{\theproposition}{S.\arabic{proposition}}
\titleformat{\section}
{\normalfont\bfseries\large\centering}{Supplement \thesection.}{1em}{}
\titleformat{\subsection}
{\normalfont\normalsize\bfseries}{\Alph{section}.\arabic{subsection}}{1em}{}

\normalsize

\section{Identifiability of quantities under different graphs}
\label{supp_sec:identifiability}

Many of the identifiability statements made in the manuscript are derived directly by existing theory on graphical models by viewing the edge dependencies in \cref{fig:graphs} in terms of the underlying spatial structure shown in \cref{fig:dag_underlying}. Here we prove any statements about identifiability of local and interference effects that are less obvious because they pertain specifically to the spatial structure of the observations or the presence of spatial interference.

\subsection{Graph \ref{fig:interference}: Spatial interference}

\begin{proposition}
If $\bm Z$ is not spatial, then $\lambda_1(\pi)$ for $\pi = P(Z_2 = 1)$ is identifiable based on the difference of averages of Unit 1 outcomes for blocks with $Z_1 = 1$ and blocks with $Z_1 = 0$. The case for $\lambda_2(\pi')$, for $\pi' = P(Z_1 = 1)$ is symmetric.
\label{supp_prop:2b_identifiability_local}
\end{proposition}

\begin{proof}
Note that 
\begin{align*}
\lambda_1(\pi)
&= \E [ \pi Y_1(1, 1) - \pi Y_1(0, 1) + (1 - \pi) Y_1(1, 0) - (1 - \pi) Y_1(0, 0)] \\
&= \E [ \pi Y_1(1, 1) + (1 - \pi) Y_1(1, 0)] - \E [ \pi Y_1(0, 1) + (1 - \pi) Y_1(0, 0)],
\end{align*}
so it suffices to identify $\E[ \pi Y_1(z, 1) + (1 - \pi) Y_1(z, 0)]$, for $z = 0, 1$.
The proof is similar to that on Page 566 of \cite{ogburn2014causal}.
\begin{align*}
\E & \left[ \pi Y_1(z, 1) + (1 - \pi) Y_1(z, 0) \right] \\
&= \pi  \E \left[ Y_1(z, 1) \right] + (1 - \pi) \E \left[ Y_1(z, 0) \right] \\
&= \pi  \E \left[ Y_1(z, 1) \mid Z_1 = z, Z_2 = 1 \right] +
(1 - \pi) \E \left[ Y_1(z, 0) \mid Z_1 = z, Z_2 = 0 \right]
\tag{Ignorability} \\
&= \pi  \E \left[ Y_1 \mid Z_1 = z, Z_2 = 1 \right] +
(1 - \pi) \E \left[ Y_1 \mid Z_1 = z, Z_2 = 0 \right]
\tag{Consistency of potential outcomes} \\
&= P(Z_2 = 1)  \E \left[ Y_1 \mid Z_1 = z, Z_2 = 1 \right] +
P(Z_2 = 0) \E \left[ Y_1 \mid Z_1 = z, Z_2 = 0 \right] \\
&= P(Z_2 = 1 \mid Z_1 = z)  \E \left[ Y_1 \mid Z_1 = z, Z_2 = 1 \right] +
P(Z_2 = 0 \mid Z_1 = z) \E \left[ Y_1 \mid Z_1 = z, Z_2 = 0 \right]
\tag{Treatment values are independent} \\
&= \E \left[ Y_1 \mid Z_1 = z \right]
\end{align*}
\end{proof}

\begin{proposition}
If $\bm Z$ is not spatial, then $\iota_1(\pi)$ for $\pi = P(Z_1 = 1)$ is identifiable based on the difference of averages of Unit 1 outcomes for blocks with $Z_2 = 1$ and blocks with $Z_2 = 0$. The case for $\iota_2(\pi')$, for $\pi' = P(Z_2 = 1)$ is symmetric.
\end{proposition}
\begin{proof}
Identical to the proof of \cref{supp_prop:2b_identifiability_local}, hence omitted.
\end{proof}


\subsection{Graph \ref{fig:direct_interference}: Direct spatial confounding and interference}

We define conditional average local effects. First, let
\[
\lambda_i(z; u_i) = \E[ Y_i(z_i = 1, z_j = z) - Y_i(z_i = 0, z_j = 0) \mid U_i = u_i]
\]
denote the expected change in unit $i$'s outcome for changes in its own treatment when the neighbor's treatment is set to $z$, among clusters with $U_i = u_i$. This is the equivalent to the local effects defined in \cref{eq:local_effect}, where we now also condition on the unit's covariate values.

We also consider expected conditional average local effects, where we average over a distribution for the neighbor's treatment. Specifically,
let $\pi(u_i) = P(Z_j = 1 \mid U_i = u_i)$. We define
\[
\lambda_i(\pi(u_i) ; u_i) = \pi(u_i) \lambda_i(1; u_i) + (1 - \pi(u_i)) \lambda_i(0; u_i),
\]
representing the average change in unit $i$'s outcome among clusters with $U_i = u_i$ for changes in unit $i$'s own treatment, and when the treatment of its neighbor is distributed according to $\pi(\cdot)$. These effects are the conditional equivalent to effects $\lambda_i(\pi)$ defined in the manuscript.

\begin{proposition}
If $\bm Z$ is not spatial, it holds that
\[
\E_{U_1} [\lambda_1(\pi(U_1); U_1)] = \E_{U_1} [ \E \left( Y_1 \mid Z_1 = 1, U_1 \right) - \E \left( Y_1 \mid Z_1 = 0, U_1 \right) ]
\]
The case for $\E_{U_2}[\lambda_2(\pi'(U_2); U_2)]$, for $\pi'(u_2) = P(Z_1 = 1 \mid U_2 = u_2)$ is symmetric.
\label{supp_prop:2d_identifiability_local}
\end{proposition}

\begin{proof}
We follow steps that are similar to those in the proof of  \cref{supp_prop:2b_identifiability_local}. However, here, we have to account for the fact that we average over a distribution of $\pi(U_1)$.
\begin{align*}
\E_{U_1} & \left[ \E \left( Y_1 \mid Z_1 = 1, U_1 \right) - \E \left( Y_1 \mid Z_1 = 0, U_1 \right) \right] = \\
&= \E_{U_1} \big\{ \big[
\E \left( Y_1 \mid Z_1 = 1, Z_2 = 1, U_1 \right) P(Z_2 = 1 \mid Z_1 = 1, U_1) + \\
& \hspace{100pt}
\E \left( Y_1 \mid Z_1 = 1, Z_2 = 0, U_1 \right) P(Z_2 = 0 \mid Z_1 = 1, U_1)
\big]  -  \\
& \hspace{60pt}
\big[
\E \left( Y_1 \mid Z_1 = 0, Z_2 = 1, U_1 \right) P(Z_2 = 1 \mid Z_1 = 0, U_1) + \\
& \hspace{160pt}
\E \left( Y_1 \mid Z_1 = 0, Z_2 = 0, U_1 \right) P(Z_2 = 0 \mid Z_1 = 0, U_1)
\big] \big\} \\
%
&= \E_{U_1} \big\{ \big[
\E \left( Y_1(1, 1) \mid Z_1 = 1, Z_2 = 1, U_1 \right) P(Z_2 = 1 \mid Z_1 = 1, U_1) + \\
& \hspace{100pt}
\E \left( Y_1(1, 0) \mid Z_1 = 1, Z_2 = 0, U_1 \right) P(Z_2 = 0 \mid Z_1 = 1, U_1)
\big]  -  \\
& \hspace{60pt}
\big[
\E \left( Y_1(0, 1) \mid Z_1 = 0, Z_2 = 1, U_1 \right) P(Z_2 = 1 \mid Z_1 = 0, U_1) + \\
& \hspace{160pt}
\E \left( Y_1(0, 0) \mid Z_1 = 0, Z_2 = 0, U_1 \right) P(Z_2 = 0 \mid Z_1 = 0, U_1)
\big] \big\}
\tag{Consistency of potential outcomes} \\
%
&= \E_{U_1} \big\{ \big[
\E \left( Y_1(1, 1) \mid U_1 \right) P(Z_2 = 1 \mid Z_1 = 1, U_1) + 
\E \left( Y_1(1, 0) \mid U_1 \right) P(Z_2 = 0 \mid Z_1 = 1, U_1)
\big]  -  \\
& \hspace{60pt}
\big[
\E \left( Y_1(0, 1) \mid U_1 \right) P(Z_2 = 1 \mid Z_1 = 0, U_1) +
\E \left( Y_1(0, 0) \mid U_1 \right) P(Z_2 = 0 \mid Z_1 = 0, U_1)
\big] \big\}
\tag{Ignorability $Z_1, Z_2 \indep Y_1(z_1, z_2) \mid U_1$ implied by the graph \ref{fig:direct_interference}} \\
%
&= \E_{U_1} \big\{ \big[
\E \left( Y_1(1, 1) \mid U_1 \right) P(Z_2 = 1 \mid U_1) + 
\E \left( Y_1(1, 0) \mid U_1 \right) P(Z_2 = 0 \mid U_1)
\big]  -  \\
& \hspace{60pt}
\big[
\E \left( Y_1(0, 1) \mid U_1 \right) P(Z_2 = 1 \mid U_1) +
\E \left( Y_1(0, 0) \mid U_1 \right) P(Z_2 = 0 \mid U_1)
\big] \big\}
\tag{$Z_1 \indep Z_2 \mid U_1$ according to the graph \ref{fig:direct_interference}} \\
%
&= \E_{U_1} \big\{ \big[
\E \left( Y_1(1, 1) \mid U_1 \right) \pi(U_1) + 
\E \left( Y_1(1, 0) \mid U_1 \right) (1 - \pi(U_1))
\big]  -  \\
& \hspace{60pt}
\big[
\E \left( Y_1(0, 1) \mid U_1 \right) \pi(U_1) +
\E \left( Y_1(0, 0) \mid U_1 \right) (1 - \pi(U_1))
\big] \big\} \\
%
&= \E_{U_1} \big[ \lambda_1(\pi(U_1); U_1) \big]
\end{align*}
\end{proof}

\cref{supp_prop:2d_identifiability_local} shows that, when $\bm Z$ is not spatial, one would need to adjust only for the local confounder in order to acquire interpretable local causal effects, even if interference is present.
We can define and identify interference effects similarly, without adjusting for the local exposure value.



\section{Motivating simulation studies}
\label{supp_sec:motivating_sims}


\subsection{Motivating simulation study with paired data}
\label{subsec:illustrate_bias_pairs}

To illustrate the points made in \cref{subsec:pairs_graphs} and show how interference and spatial confounding can manifest as each other and affect estimation of local and interference effects, we perform a small simulation study. We simulate pairs of $\bm U$ from a bivariate Normal distribution with mean 0, and covariance matrix $\Sigma_U = \left( \begin{smallmatrix} 1 & \phi_U \\ \phi_U & 1 \end{smallmatrix} \right)$. We also simulate a bivariate normal error term $\bm \epsilon_Z = (\epsilon_{Z,1}, \epsilon_{Z, 2})$ with marginal variances equal to 1 and correlation parameter $\phi_Z$. The binary exposure is generated from a Bernoulli distribution with a logistic link function and linear predictor $\beta_{UZ} U_i + \epsilon_{Z,i}$. Higher values of $\phi_U, \phi_Z$ correspond to stronger inherent spatial dependence for $\bm U$ and $\bm Z$.
The outcome is generated independently across locations from a normal distribution with mean $\beta_Z Z_i + \beta_{\bar Z} \overline Z_i + \beta_U U_i + \beta_{\bar U} \overline U_i$ and variance 1, where $\overline Z_i$ and $\overline U_i$ represent the value of the exposure and the covariate for the neighbor of location $i$, respectively.
Under this model, $\beta_Z$ and $\beta_{\bar{Z}}$ correspond to the local and interference effects, respectively.
We consider the six different scenarios presented in \cref{fig:graphs} by setting different parameters to zero. 
We simulate 300 data sets of 200 pairs each, and fit ordinary least squares (OLS) using different sets of predictor variables.
The data generating model and hyperparameters for each of these scenarios are listed in Table \ref{tab:pairdata}, along with the bias for the OLS estimators of $\beta_Z$ and $\beta_{\bar{Z}}$.



\begin{table}[p]
\spacingset{1}
\small
\centering
\caption{Motivating Simulation Study with Paired Data. For the graphs of \cref{fig:graphs}, we illustrate the induced biases in estimating local and interference causal effects due to spatial dependencies.
In these simulations, the parameters that drive the data generative mechanism are $\phi_U, \phi_Z, \beta_{UZ}, \beta_Z, \beta_{\bar Z}, \beta_U, \beta_{\bar U}.$ The different scenarios of \cref{fig:graphs} correspond to different set of parameters fixed at 0, shown below. Unless otherwise noted, the parameters are fixed at $\phi_U = 0.7$, $\phi_Z = 0.5, \beta_{UZ} = 1, \beta_Z = 1, \beta_{\bar Z} = 0.8, \beta_U = 1, \beta_{\bar U} = 0.5$. We generate 300 data sets of 200 pairs each. We regress the outcome on a different set of variables (columns), and report the bias of the OLS estimator for the local effect estimator, $\beta_Z$, and the interference effect estimator, $\beta_{\bar Z}$, when $\overline Z$ is included in the conditioning set. Values are rounded to the third decimal point, and those in {\bf bold} are discussed in the main text.
}
\spacingset{1}
    % \rowcolors{5}{}{gray!10}
    \resizebox{0.88\textwidth}{!}{%
    \begin{tabular}{*{10}{c}}
        \hline
        & & & & &  & \\[-12pt]
        & &  \multicolumn{8}{c}{Conditioning set} \\
        \multirow{2}{*}{\shortstack[c]{True \\[5pt] Model}} & \multirow{3}{*}{\shortstack[c]{Alternative \\[5pt] spatial \\[5pt] parameters}} & \multicolumn{8}{c}{\& estimated parameter} \\[2pt]
        \cmidrule{3-10} 
        & & \\[-15pt]
        & &  $(Z) $ & $(Z, U)$  & \multicolumn{2}{c}{  $(Z, \bar{Z})$} & \multicolumn{2}{c}{  $(Z, \bar{Z}, U)$} & \multicolumn{2}{c}{  $(Z, \bar{Z}, U, \bar{U})$}\\
        %\cmidrule(lr){3-3} \cmidrule(lr){4-4} \cmidrule(lr){5-6} \cmidrule(lr) {7-8} \cmidrule(lr) {9-10}
        %& & \\
        %\cmidrule{3-10} 
        \cmidrule(lr){3-3} \cmidrule(lr){4-4} \cmidrule(lr){5-6} \cmidrule(lr) {7-8} \cmidrule(lr) {9-10}
        & & $\beta_Z$ & $\beta_Z$ & $\beta_Z$ & $\beta_{\bar{Z}}$ & $\beta_Z$ & $\beta_{\bar{Z}}$ & $\beta_Z$ & $\beta_{\bar{Z}}$\\
        & & & & & & \\[-8pt]
        \hline \\[-8pt]
%
\multirow{2}{*}{\ref{fig:direct}} & & \multicolumn{8}{c}{$\beta_{\bar Z} = 0$ and $\beta_{\bar U}=0$} \\[2pt]
        \cmidrule{5-8}
        & & 0.726 & -0.003 & 0.660 & {\bf 0.406} & -0.003 & {\bf -0.002} & -0.002 & 0.000  \\
        \midrule \\[-8pt]
%
\multirow{4}{*}{\ref{fig:interference}} & & 
\multicolumn{8}{c}{$\beta_{UZ} = 0$ and $\beta_U = \beta_{\bar U} = 0$}
        \\[2pt]
        \cmidrule{5-8}
        & $\phi_z = 0.7$ & {\bf 0.152} & 0.087 & 0.001 & -0.002 & -0.001 & -0.003 & 0.000 & -0.002  \\
         & $\phi_z = 0.5$ & {\bf 0.129} & 0.060 & -0.001 & -0.001 & -0.003 & -0.002 & -0.002 & 0.000 \\
        & $\phi_z = 0.3$ & {\bf 0.105} & 0.032 & -0.002 & 0.001 & -0.003 & 0.000 & -0.003 & 0.001 \\
        \midrule \\[-8pt]
%
\multirow{2}{*}{\ref{fig:general_spatial_conf}} & & 
        \multicolumn{8}{c}{$\beta_{\bar Z} = 0$}
        \\[2pt]
        \cmidrule{5-8}
%        & $\phi_z = 0.7$ & 0.909 & -0.007 & 0.816 & 0.601 & -0.036 & 0.297 & -0.006 & 0.004 \\ 
         & %$\phi_z = 0.5$ 
         & 0.983 & 0.002 & 0.863 & 0.737 & -0.013 & {\bf 0.198} & -0.002 & {\bf 0.000}  \\ 
%         & $\phi_z = 0.3 $ & 0.904 & -0.008 & 0.839 & 0.636 & -0.020 & 0.295 & -0.005 & 0.004  \\
         \midrule \\[-8pt]
%
\multirow{3}{*}{\ref{fig:direct_interference}} & & 
\multicolumn{8}{c}{$\beta_{\bar U} = 0$} \\[2pt]
        \cmidrule{5-8}
& $\phi_Z = 0.5$ & 0.856 & {\bf 0.060} & 0.660 & 0.406 & -0.003 & -0.002 & -0.002 & 0.000 \\ 
& $\phi_Z = 0\phantom{.5}$ & 0.800 & {\bf -0.001} & 0.684 & 0.445 & 0.000 & -0.003 & 0.000 & -0.001 \\
\midrule \\[-8pt]
%
\multirow{4}{*}{\ref{fig:predictor_interference}} & 
& \multicolumn{8}{c}{$\beta_U = 0$ and $\beta_{\bar U} = 0$} \\[2pt]
\cmidrule{5-8}
        & $\beta_{UZ} = 1.5$ & {\bf 0.173} & {\bf 0.052} & 0.000 & 0.001 & -0.002 & 0.000 & -0.002 & 0.003 \\
        & $\beta_{UZ} = 1\phantom{.a}$ &  0.129 & 0.060 & -0.001 & -0.001 & -0.003 & -0.002 & -0.002 & 0.000  \\
        & $\beta_{UZ} = 0.5$ & 0.083 & 0.061 & -0.004 & -0.003 & -0.006 & -0.004 & -0.005 & -0.003 \\
\midrule \\[-8pt]
%
\ref{fig:general_interference}
         & & 1.113 & 0.064 & 0.863 & 0.737 & -0.013 & 0.198 & -0.002 & 0.000 \\
        & & & & & &  \\[-8pt]
          \hline
        \end{tabular}
     }%
\label{tab:pairdata}
\end{table}

In the presence of only direct spatial confounding (Scenario \ref{fig:direct}), we see that failing to adjust for the local spatial confounder returns biased interference effect estimates ($\beta_{\bar Z}$ in the model with $Z, \overline Z$ in \cref{tab:pairdata}). Therefore, in the presence of inherently spatial data, adjusting for spatial confounders is crucial for learning interference effects, even if spatial confounding is direct only. When spatial confounding is both direct and indirect (Scenario \ref{fig:general_spatial_conf}), adjusting only for the local spatial confounder and exposure values can still return misleading interference effects ($\beta_{\bar Z}$ in the model with $Z, \overline Z, U$), and it is necessary to also account for the neighbor's covariate value. In the presence of interference (Scenario \ref{fig:interference}) and when the exposure is inherently spatial, the local effect estimator is biased when the neighbor's exposure value is not conditioned on, and the bias is larger for stronger spatial dependence. Instead, local and interference effects can be unbiasedly estimated when they are considered simultaneously ($\beta_Z, \beta_{\bar Z}$ in the model with $Z, \overline Z$).
In Scenario \ref{fig:direct_interference}, we see that when the exposure is not inherently spatial ($\phi_Z = 0$), we can learn local effects without adjusting for the neighbor's exposure. However, this estimator is biased when the exposure has an inherent spatial structure, illustrating practically that spatial dependencies can hinder some analyses invalid if not properly taken into account.
In Scenario \ref{fig:predictor_interference}, the local effect of the exposure for unit $i$ is biased regardless of whether $U_i$ is adjusted for or not. At the same time, the estimates when $U_i$ is included in the model or not are substantially different, which could be interpreted as $U_i$ confounding the local effect. Therefore, in this scenario, the inherent spatial structure in the confounders and exposure could lead to  interference being mistakenly interpreted as spatial confounding. Of course, when all the possible dependencies are present in Scenario \ref{fig:general_interference}, one would need to condition on local and neighborhood covariates to properly estimate local and interference effects. The estimator that account for all of local and neighborhood exposure and confounding values returns unbiased effect estimates across all scenarios.



\subsection{Motivating simulation study in a setting with one spatial network of observations}
\label{supp_sec:motivating_one_network}

We consider a graph with $n$ nodes. We assume that this graph is a line graph, in that the first and last nodes are connected only to the second and second to last, respectively, and node $i$ is connected to nodes $i - 1$ and $i + 1$ for $i = 2, 3, \dots, n-1$. This implies the following adjacency and degree matrices:
\[
A = \begin{pmatrix}
0 & 1 & 0 & 0 & \dots & 0 & 0 & 0 \\
1 & 0 & 1 & 0 & \dots & 0 & 0 & 0 \\
0 & 1 & 0 & 1 & \dots & 0 & 0 & 0 \\
& & \vdots & & \dots & & \vdots &  \\
0 & 0 & 0 & 0 & \dots & 1 & 0 & 1 \\
0 & 0 & 0 & 0 & \dots & 0 & 1 & 0 
\end{pmatrix} \quad \text{and} \quad
D = \begin{pmatrix}
1 & 0 & 0 & \dots & 0 & 0 \\
0 & 2 & 0 & \dots & 0 & 0 \\
0 & 0 & 2 & \dots & 0 & 0 \\
& & \vdots & \dots & \vdots & \\
0 & 0 & 0 & \dots & 2 & 0 \\
0 & 0 & 0 & \dots & 0 & 1
\end{pmatrix}.
\]
We generate $\bm U = (U_1, U_2, \dots, U_n)$ and $\bm Z = (Z_1, Z_2, \dots, Z_n)$ simultaneously from a multivariate normal distribution as follows
\[
\begin{pmatrix} \bm U \\ \bm Z \end{pmatrix} \sim N_{2n} \left( \bm 0_{2n},
\begin{pmatrix} G & Q \\ Q & H \end{pmatrix}^{-1}
\right),
\]
where $\bm 0_{2n}$ is a vector of length $2n$ of all $0$s. We specify $G$ and $H$ according to a conditional autoregressive distribution as $G = \tau_U^2 (D - \phi_U A)$ and $H = \tau_Z ^2 (D - \phi_Z A)$. Then, $Q$ is specified to be diagonal with elements $Q_{ii} = - \rho \sqrt{G_{ii} H_{ii}}.$ Note that different values of $\phi$ for the same value of $\tau$ lead to different marginal variances for the entries of $\bm U$ and $\bm Z$.

We exclude measured covariates for simplicity. Once $\bm U$ and $\bm Z$ are generated, the outcome is generated according to the model \cref{eq:linear_sem}, where $\epsilon \sim N(0, 1)$ independent.
Unless otherwise specified, the hyperparameters for these simulations are set to the values reported in \cref{tab:motivating_network}.



\begin{table}[p]
\centering
\spacingset{1}
\caption{
Motivating Simulation Study with One Interconnected Network. Unless otherwise noted, the parameters are fixed at $\phi_U = 0.6$, $\phi_Z = 0.4$, $\tau_U = \tau_Z = 1$, $\rho = 0.35$, $\beta_Z = 1, \beta_{\bar Z} = 0.8, \beta_U = 1, \beta_{\bar U} = 0.5$. We generate 200 data sets with $n = 100$. We regress the outcome on a different set of variables (columns), and report the bias of the OLS estimator for the local effect estimator, $\beta_Z$, and the interference effect estimator, $\beta_{\bar Z}$, when $\overline Z$ is included in the conditioning set. Values are rounded to the third decimal point. We bold the entries corresponding to the same cells as in \cref{tab:pairdata}. The qualitative conclusions remain unchanged.}
    % \rowcolors{5}{}{gray!10}
    \resizebox{0.82\textwidth}{!}{%
    \begin{tabular}{*{10}{c}}
        \hline
        & & & & &  & \\[-5pt]
        & &  \multicolumn{8}{c}{Conditioning set} \\
        \multirow{2}{*}{\shortstack[c]{True \\[5pt] Model}} & \multirow{3}{*}{\shortstack[c]{Alternative \\[5pt] spatial \\[5pt] parameters}} & \multicolumn{8}{c}{\& estimated parameter} \\[5pt]
        \cmidrule{3-10} 
        & & \\[-5pt]
        & &  $(Z) $ & $(Z, U)$  & \multicolumn{2}{c}{  $(Z, \bar{Z})$} & \multicolumn{2}{c}{  $(Z, \bar{Z}, U)$} & \multicolumn{2}{c}{  $(Z, \bar{Z}, U, \bar{U})$}\\
        \cmidrule(lr){3-3} \cmidrule(lr){4-4} \cmidrule(lr){5-6} \cmidrule(lr) {7-8} \cmidrule(lr) {9-10}
        & & $\beta_Z$ & $\beta_Z$ & $\beta_Z$ & $\beta_{\bar{Z}}$ & $\beta_Z$ & $\beta_{\bar{Z}}$ & $\beta_Z$ & $\beta_{\bar{Z}}$\\
        & & & & & & \\[-5pt]
        \hline \\[-5pt]
%
\multirow{2}{*}{\ref{fig:direct}} & & \multicolumn{8}{c}{$\beta_{\bar Z} = 0$ and $\beta_{\bar U}=0$} \\[5pt]
        \cmidrule{5-8}
        & & 0.550 & -0.005 & 0.428 & {\bf 0.370} & -0.007 & {\bf 0.006} & -0.007 & -0.004  \\
        \midrule \\[-5pt]
%
\multirow{4}{*}{\ref{fig:interference}} & & 
\multicolumn{8}{c}{$\rho = 0$ and $\beta_U = \beta_{\bar U} = 0$}
        \\[5pt]
        \cmidrule{5-8}
        & $\phi_z = 0.6$ & {\bf 0.437} & 0.334 & -0.012 & 0.000 & -0.007 & 0.002 & -0.007 & 0.011 \\
         & $\phi_z = 0.4$ & {\bf 0.261} & 0.193 & -0.002 & -0.008 & 0.003 & -0.004 & 0.002 & -0.007 \\
        & $\phi_z = 0.2$ & {\bf 0.151} & 0.088 & 0.006 & -0.002 & 0.002 & -0.005 & 0.003 & 0.000 \\
        \midrule \\[-5pt]
%
\multirow{2}{*}{\ref{fig:general_spatial_conf}} & & 
        \multicolumn{8}{c}{$\beta_{\bar Z} = 0$}
        \\[5pt]
        \cmidrule{5-8}
%        & $\phi_z = 0.7$ & 0.909 & -0.007 & 0.816 & 0.601 & -0.036 & 0.297 & -0.006 & 0.004 \\ 
         & %$\phi_z = 0.5$ 
         & 0.683 & 0.045 & 0.489 & 0.595 & -0.008 & {\bf 0.237} & -0.013 & {\bf 0.030} \\ 
%         & $\phi_z = 0.3 $ & 0.904 & -0.008 & 0.839 & 0.636 & -0.020 & 0.295 & -0.005 & 0.004  \\
         \midrule \\[-5pt]
%
\multirow{3}{*}{\ref{fig:direct_interference}} & & 
\multicolumn{8}{c}{$\beta_{\bar U} = 0$} \\[5pt]
        \cmidrule{5-8}
& $\phi_Z = 0.4$ & 0.838 & {\bf 0.195} & 0.456 & 0.345 & 0.001 & 0.021 & 0.001 & 0.019 \\ 
& $\phi_Z = 0\phantom{.5}$ & 0.501 & {\bf -0.010} & 0.444 & 0.283 & 0.001 & -0.017 & 0.001 & -0.019 \\
\midrule \\[-5pt]
%
\multirow{4}{*}{\ref{fig:predictor_interference}} & 
& \multicolumn{8}{c}{$\beta_U = 0$ and $\beta_{\bar U} = 0$} \\[5pt]
\cmidrule{5-8}
        & $\rho = 0.15$ & 0.183 & 0.170 & 0.003 & 0.003 & 0.004 & 0.005 & 0.004 & 0.002 \\
        & $\rho = 0.35$ & 0.275 & 0.186 & 0.011 & -0.012 & 0.004 & -0.018 & 0.004 & -0.018  \\
        & $\rho = 0.45$ & {\bf 0.431} & {\bf 0.246} & 0.014 & -0.009 & 0.013 & -0.011 & 0.012 & -0.011 \\
        \midrule \\[-5pt]
%
\ref{fig:general_interference}
         & & 0.973 & 0.222 & 0.507 & 0.611 & -0.008 & 0.209 & -0.014 & -0.002 \\
        & & & & & &  \\
          \hline
          
        \end{tabular}
     }%
\label{tab:motivating_network}
\end{table}



\section{Illustration of prior distributions}
\label{supp_sec:priors}

As discussed in \cref{subsec:priors}, the prior specifications on the spatial parameters $\tau_U, \tau_Z$ have implications on the implied prior on the the confounding strength due to $U$ and the variance of the  exposure $Z$, respectively. Here, we provide a simulation-based illustration for the implied prior properties discussed in the manuscript.

Before we delve into this illustration, we discuss briefly the matrices $G, H$ in the joint precision matrix of \cref{ass:UZ_normal}, which in the absence of measured covariates states that
\begin{equation}
\begin{pmatrix} \bm U \\ \bm Z \end{pmatrix} \Big| \sim N_{2n} \left(
\bm 0_{2n} ,
\begin{pmatrix} G & Q \\ Q & H \end{pmatrix}^{-1}
\right).
\label{supp_eq:UZ_normal_no_covs}
\end{equation}
Since the joint distribution is parameterized through its precision matrix, $G^{-1}$ and $H^{-1}$ are the marginal covariance matrices of $\bm U$ and $\bm Z$, respectively, {\it only} when $\rho = 0$, and
%generating $\bm U$ from $N_n(\bm 0_n, G^{-1})$ is only in agreement with the joint distribution in \cref{supp_eq:UZ_normal_no_covs} only when $\rho = 0$, and 
the distribution of $\bm U$ when drawn from $N_n(\bm 0_n, G^{-1})$ is different from the distribution of $\bm U$ when drawn from \cref{supp_eq:UZ_normal_no_covs} for $\rho \neq 0$. In our illustrations below, we will consider vectors $\bm U$ and $\bm Z$ which are drawn from $N_n(\bm 0_n, G^{-1})$ and $N_n(\bm 0_n, H^{-1})$, respectively, or simultaneously from \cref{supp_eq:UZ_normal_no_covs} for $\rho \neq 0$.



\subsection{Prior distribution for $\tau_U$}

\begin{figure}[p]
\centering
\includegraphics[width = 0.8\textwidth,trim=0 57 0 0,clip]{figures/tauU_prior_phiU02.pdf} \\
\includegraphics[width = 0.8\textwidth,trim=0 57 0 0,clip]{figures/tauU_prior_phiU05.pdf} \\
\includegraphics[width = 0.8\textwidth,trim=0 57 0 0,clip]{figures/tauU_prior_phiU08.pdf}
\includegraphics[width = 0.9\textwidth,trim=0 0 0 160,clip]{figures/tauU_prior_phiU08.pdf}
\caption{Density plot for the implied prior distribution on the amount of outcome variability explained by a measured covariate and the unmeasured covariate $U$ based on the prior distribution for $\tau_U$. We consider network and pair data of sample sizes 200 and 400. $\bm U$ is generated from $N_n(\bm 0_n, G^{-1})$ for $\tau_U$ sampled from its prior distribution and $\phi_U \in \{0.2, 0.5, 0.8\}$.}
\label{supp_fig:prior_tauU}
\end{figure}

The strength of a measured covariate $C$ with variance 1 in the outcome model corresponds to the magnitude of its coefficient $\beta_C$, or (equivalently) the standard deviation of $\beta_C C_i$ across units $i$. Similarly, since the coefficient of $U_i$ is set to $\beta_U = 1$, the strength of the unmeasured $U_i$ in the outcome model can be measured by the standard deviation of the unmeasured confounder. For network and paired data of sizes $n \in \{200, 400\}$, we performed the following procedure 1,000 times:
\begin{enumerate*}[label=(\alph*)]
\item we drew $\beta_C \sim N(0, \sigma^2_{prior})$,
\item we drew $1 / \tau_U$ from the prior distribution described in \cref{subsec:priors},
\item we generated $\bm U = (U_1, U_2, \dots, U_n)$ from $N_n(\bm 0_n, G^{-1})$ where $G$ has a CAR structure with $\tau_U$ the one drawn at the previous step and $\phi_U \in \{0.2, 0.5, 0.8\}$.
\end{enumerate*}
Each time, we calculated the absolute value of $\beta_C$ and the standard deviation of $U_i$ across $i$. Their distributions are shown in \cref{supp_fig:prior_tauU}, using red for the measured covariate $C$ and blue for the unmeasured covariate $U$.
Considering that the outcome is standardized to have variance 1, the prior distribution for the strength of the unmeasured confounder in the outcome model allows for all reasonable values and it is relatively similar to the corresponding prior distribution for a measured covariate, across all configurations.

The two distributions are similar across all choices of $\sigma^2_{prior}$ we explored. Since prior distributions on model coefficients are well-explored and understood in the literature, the prior distribution for $\tau_U$ we designed can be used straightforwardly without requiring additional tuning. Specifically, a researcher can simply specify $\sigma^2_{prior}$ for the prior distribution of a coefficient in the outcome model, and our specification for the prior distribution of $\tau_U$ would automatically translate the choice of $\sigma^2_{prior}$ to an equivalent prior for the confounding strength of the unmeasured covariate.



\subsection{Prior distribution for $\tau_Z$}

\begin{figure}[p]
\centering
\includegraphics[width = 0.8\textwidth,trim=0 19 0 0,clip]{figures/tauZ_prior_phiZ02.pdf} \\[5pt]
\includegraphics[width = 0.8\textwidth,trim=0 19 0 0,clip]{figures/tauZ_prior_phiZ05.pdf} \\[5pt]
\includegraphics[width = 0.8\textwidth,trim=0 19 0 0,clip]{figures/tauZ_prior_phiZ08.pdf} \\[5pt]
\caption{Implied prior distribution on the exposure variability implied by the specified prior distribution for $\tau_Z$. We consider network and pair data of sample sizes 200 and 400, and $\bm Z$ is drawn from $N_n(\bm 0_n, H^{-1})$ where $H$ has a CAR structure with $\tau_Z$ sampled from its prior distribution and $\phi_Z \in \{0.2, 0.5, 0.8\}$.}
\label{supp_fig:prior_tauZ}
\end{figure}

We also investigated the prior on the exposure's variance as implied by the prior on $\tau_Z$ discussed in \cref{subsec:priors}. We set the hypothesized marginal variance of $\bm Z$ to $\widetilde s^2_Z = 1$ and the hypothesized residual variance of $\bm Z$ to $\widetilde \sigma^2_Z = 0.5^2$. We repeated the following procedure 2,000 times:
\begin{enumerate*}[label=(\alph*)]
\item we drew $\tau_Z$ from its prior distribution,
\item we generated $\bm Z$ from $N_n(\bm 0_n, H^{-1})$, where $H$ is specified as CAR with $\tau_Z$ the draw from the previous step and $\phi_Z \in \{0.2, 0.5, 0.8\}$, and
\item we calculated the exposure variance across locations.
\end{enumerate*}
We did so for network and paired data of sample sizes 200 and 400. The distribution of this variance is shown in \cref{supp_fig:prior_tauZ}, where the dashed vertical line represents the hypothesized residual variance of the exposure conditional on measured covariates, $\widetilde \sigma^2_Z$. We see that the implied exposure variability takes values in the neighborhood of $\widetilde \sigma^2_Z$, as expected.



\subsection{Implied prior distributions when $\rho \neq 0$}


Our prior distributions as described in \cref{subsec:priors} are designed based on approximations of the variability in the unmeasured covariate $\bm U$ and the exposure $\bm Z$ when the two variables are independent. Here, we illustrate using simulation that these prior distributions also imply reasonable prior distributions on the strength of confounding due to $U$ and the inherent exposure variability even when $\rho \neq 0$.


\begin{figure}[!t]
\centering
\includegraphics[width = 0.85\textwidth,trim=0 58 0 0,clip]{figures/tauU_prior_phiU05_rho03.pdf} \\
\includegraphics[width = 0.85\textwidth,trim=0 0 0 170,clip]{figures/tauU_prior_phiU05_rho03.pdf} \\
\includegraphics[width = 0.85\textwidth,trim=0 19 0 0,clip]{figures/tauZ_prior_phiZ05_rho03.pdf}  \\
\caption{Implied priors when the exposure and the unmeasured covariate are correlated according to \cref{ass:UZ_normal} with $\phi_U = \phi_Z = 0.5$ and $\rho = 0.3$. Top: Prior distribution of predictive strength of a measured and the unmeasured covariates (equivalent of \cref{supp_fig:prior_tauU}). Bottom: Prior distribution on the exposure variability (equivalent of \cref{supp_fig:prior_tauZ}).}
\label{supp_fig:prior_tauU_tauZ_rho03}
\end{figure}


We performed the following procedure 1,000 times:
\begin{enumerate*}[label=(\alph*)]
\item we drew $\tau_U$ and $\tau_Z$ from their prior distributions,
\item for these values and for $\phi_U = \phi_Z = 0.5$ and $\rho = 0.3$, we constructed the matrices $G, H$, and $Q$ and the precision matrix \cref{supp_eq:UZ_normal_no_covs},
\item we drew $(\bm U, \bm Z)$ from their joint distribution.
\end{enumerate*}
Based on the 1,000 samples from $(\bm U, \bm Z)$ we calculated the standard deviation of $\bm U$ across locations, and the standard deviation of $\bm Z$ across locations. \cref{supp_fig:prior_tauU_tauZ_rho03} is an equivalent to those in Figures \ref{supp_fig:prior_tauU} and \ref{supp_fig:prior_tauZ} for correlated exposure and unmeasured covariate. Specifically, at the top of \cref{supp_fig:prior_tauU_tauZ_rho03}, we compare the standard deviation of $U$ against the absolute value for draws from the $N(0, \sigma^2_{prior})$ distribution, and we find that the implied confounding strength for a measured and the unmeasured covariate have similar prior distributions. At the bottom of \cref{supp_fig:prior_tauU_tauZ_rho03}, we plotted the distribution of the exposure variance against the hypothesized residual variance $\widetilde \sigma^2_Z$, and we see that the implied prior still allow for a reasonable range of values.


\section{Posterior distribution sampling scheme}
\label{supp_sec:mcmc}

We describe the MCMC updates for approximating the posterior distribution. 
We write $p(\theta \mid \cdot)$ to denote the posterior distribution of $\theta$ conditional on all other parameters. 
We use the following definitions:
%
\begin{itemize}[leftmargin=*,label=-]
\item \underline{Exposure model residuals:} We use $\bm Z_{res}$ to denote the vector of length $n$ including the exposure residuals based on the current values of the parameters $\gamma_0, \bm \gamma_C$. Specifically, the $i^{th}$ entry of $\bm Z_{res}$ is
$Z_i - \gamma_0 - \widetilde C_i^T \bm \gamma_C$.

\item \underline{Outcome model residuals:} We consider three versions of outcome model residuals, conditional on all covariates, the measured ones only, and the unmeasured covariate only. We denote them by $\bm Y_{res}$, $\bm Y_{res}^C$, and $\bm Y_{res}^U$ with $i^{th}$ entries
\begin{align*}
Y_{res,i} &= Y_i - \beta_0 - \beta_Z Z_i - \beta_{\bar Z}\overline Z_i - \widetilde C_i \bm \beta_C - \beta_U U_i - \beta_{\bar U} \overline U_i \\
Y_{res,i}^C &= Y_i - \beta_0 - \beta_Z Z_i - \beta_{\bar Z}\overline Z_i - \widetilde C_i \bm \beta_C,
\quad \text{and} \\
Y_{res,i}^U &= Y_i - \beta_U U_i - \beta_{\bar U} \overline U_i,
\end{align*}
respectively.

\item \underline{The ``coefficient matrix'' of the unmeasured covariate in the outcome model:} If $A_U$ denotes the adjacency matrix that drives the neighborhood confounder values $\overline{\bm U}$ in terms of $\bm U$, and $D_U$ is the corresponding degree matrix, then we have that $\overline{\bm U} = D_U^{-1} A_U \bm U$. Therefore, the vector $\bm U$ is included in the outcome model through
\(
\beta_U \bm U + \beta_{\bar U} \overline{\bm U} = (I_n + \beta_{\bar U} D_U^{-1} A_U) \bm U
\),
when $\beta_U = 1$. We definite $M_U = I_n + \beta_{\bar U} D_U^{-1} A_U$ which will play a role for updating the values of the unmeasured covariate $\bm U$. The matrix $M_U$ depends on the current value of $\beta_{\bar U}$ so it is itself updated during the MCMC every time $\beta_{\bar U}$ is updated.

\item \underline{The design matrices:} the $n \times (p + 3)$  design matrix for the outcome model based on measured variables $\bm X = (\bm 1 \ \bm Z \ \overline{\bm Z} \ \bm C)$, and the $n \times (p + 1)$ design matrix for the exposure model  $\bm X_{-z} = (\bm 1 \ \bm C)$.

\end{itemize}

\noindent
The full list of parameters and the corresponding MCMC updates are described below. We use superscripts $\tor$ to denote the $r^{th}$ posterior sample of a given parameter. The updates below describe how the $\tor[r+1]^{th}$ sample is acquired. Most parameters are drawn using Gibbs updates, and Metropolis-Hastings is used for the spatial parameters.
%
\begin{enumerate}[leftmargin=*,label=(\alph*)]
\item $\bm U^{\tor[r+1]}$ is drawn from its full conditional posterior distribution which is a multivariate normal with mean $\mu_{new, \bm U}$ and variance $\Sigma_{new, \bm U}$ where
\begin{align*}
\Sigma_{new, \bm U} &= \left[ G^{\tor} + \big( M_U^{\tor} \big)^T M_U^{\tor} / \sigma^{2\tor}_Y \right]^{-1}, \quad \text{and}
\\
\mu_{new,\bm U} &= \Sigma_{new, \bm U} \left[ 
\big( M_U^{\tor} \big)^T \bm Y_{res}^{C,\tor} / \sigma^{2\tor}_Y
- Q^{\tor} \bm Z_{res}^{\tor}
\right].
\end{align*}
We update the values of $\overline{\bm U}$ based on $\bm U^{\tor[r+1]}$, and we calculate $\bm Y_{res}^{U\tor[r+1]}$.


\item We draw the intercept and the coefficients of the local exposure, neighborhood exposure, and the measured covariates in the outcome model, $(\beta_0, \beta_Z, \beta_{\bar Z}, \bm \beta_C)$, from their joint full conditional distribution which is a multivariate normal with mean $\mu_{new, \beta}$ and variance $\Sigma_{new, \beta}$, where
\begin{align*}
\Sigma_{new, \beta} &= \left[ \bm X^T \bm X / \sigma^{2\tor}_Y + I_{p + 3} / \sigma^2_{prior} \right]^{-1},
\quad \text{and} \\
\mu_{new, \beta} &= \Sigma_{new, \beta} \bm X^T \bm Y_{res}^{U,\tor[r+1]} / \sigma^{2\tor}_Y
\end{align*}
We calculate $\bm Y_{res}^{\tor[r+1]}$ and $\bm Y_{res}^{C,\tor[r+1]}$ based on the new $\beta$-values.


\item We draw the intercept and the coefficients of the measured covariates in the exposure model, $(\gamma_0, \bm \gamma_C)$, from their joint full conditional distribution which is a multivariate normal with mean $\mu_{new, \gamma}$ and variance $\Sigma_{new, \gamma}$, where
\begin{align*}
\Sigma_{new, \gamma} &= \left[ \bm X_{-z}^T H^{\tor} \bm X_{-z}  + I_{p + 1} / \sigma^2_{prior} \right]^{-1},
\quad \text{and} \\
\mu_{new, \gamma} &= \Sigma_{new, \gamma} \bm X_{-z}^T \left( H^{\tor} \bm Z + (Q^{\tor})^T \bm U^{\tor[r+1]} \right).
\end{align*}
We update the exposure residuals $\bm Z_{res}$ based on the new $\gamma-$values.


\item We draw the residual outcome model variance from an inverse gamma with shape parameter $\alpha_{new,Y} = \alpha_Y + n / 2$, and rate parameter $\beta_{new,Y} = 
\beta_Y + (\bm Y_{res}^{\tor[r+1]})^T \bm Y_{res}^{\tor[r+1]} / 2. $


\item We draw the coefficient of the neighborhood unmeasured covariate from a normal distribution with mean $\mu_{new,\bar U}$ and variance $\sigma^2_{new, \bar U}$ where
\begin{align*}
\sigma^2_{new, \bar U} &= \left[ 
\big( \overline{\bm U}^{\tor[r+1]} \big)^T
\overline{\bm U}^{\tor[r+1]} / \sigma^{2\tor[r+1]}_Y + 1 / \sigma^2_{prior, \bar U} \right]^{-1},
\quad \text{and} \\
\mu_{new, \bar U} &=  \sigma^2_{new, \bar U}
\big( \overline{\bm U}^{\tor[r+1]} \big)^T
\big( \bm Y_{res}^{C\tor[r+1]} - \beta_U \bm U^{\tor[r+1]} \big) / \sigma^{2\tor[r+1]}_Y.
\end{align*}
We update $\bm Y_{res}$ and $\bm Y_{res}^U$ based on the new value of $\beta_{\bar U}$.

\item We have specified CAR structure for $G, H$ with two parameters each ($\phi_U, \tau_U, \phi_Z, \tau_Z$) and one parameter ($\rho$) for their correlation. We update all parameters using a Metropolis-Hastings step. Consider the function $\text{dexpit}: \mathbb{R} \rightarrow (-1, 1)$ with $\text{dexpit}(x) = 2 / (1 + \exp(-x)) -1$ and its inverse $\text{dexpit}^{-1}: (-1, 1)  \rightarrow \mathbb{R}$ with $\text{dexpit}^{-1}(x) = \log(1 + x) - \log(1 - x).$ 
If  $\phi_U^{\tor}, \tau_U^{\tor}, \phi_Z^{\tor}, \tau_Z^{\tor}, \rho^{\tor}$ are the current values of the parameters, we propose values $\phi_U^{prop}, \tau_U^{prop}, \phi_Z^{prop}, \tau_Z^{prop}, \rho^{prop}$ as follows:
\begin{itemize}[leftmargin=*,label=-]
\item Draw $\epsilon_{\phi_U}$ from $N(0, 0.35^2 s^2)$ and set
$\phi_U^{prop} = \text{dexpit} ( \text{dexpit}^{-1} (\phi_U^{\tor}) + \epsilon_{\phi_U}) $.
\item Draw $\epsilon_{\tau_U}$ from $N(0, 0.2^2s^2)$ and set
$\tau_U^{prop} = \exp( \log( \tau_U^{\tor} ) + \epsilon_{\tau_U} )$.
\item Set $\phi_Z^{prop}$ and $\tau_Z^{prop}$ similarly.
\item  Draw $\epsilon_{\rho}$ from $N(0, 0.5^2 s^2)$ and set
$\rho^{prop} = \text{dexpit} ( \text{dexpit}^{-1} (\rho^{\tor}) + \epsilon_{\rho}) $.
\end{itemize}
Create matrices $G^{prop}, H^{prop}$ and $Q^{prop}$ based on the proposed values.

The acceptance probability for the joint move is given by the ratio of the posterior probabilities of the proposed values versus the current values:
\[
\frac{p( \phi_U^{prop}, \tau_U^{prop}, \phi_Z^{prop}, \tau_Z^{prop}, \rho^{prop} \mid \cdot)}
{p(\phi_U^{\tor}, \tau_U^{\tor}, \phi_Z^{\tor}, \tau_Z^{\tor}, \rho^{\tor} \mid \cdot)},
\]
where 
\(
p( \phi_U, \tau_U, \phi_Z, \tau_Z, \rho \mid \cdot) 
\)
is proportional to the likelihood of \cref{eq:UZ_normal} based on the current values $\gamma_0^{\tor[r+1]}, \bm \gamma_C^{\tor[r+1]}$ and $\bm U^{\tor[r+1]}$ times the prior distribution for these spatial parameters evaluated at the proposed (numerator) or current (denominator) values.
If $\phi_Z^{prop} > \phi_U^{prop}$, these values do not satisfy the prior constraint, and the proposal will be rejected. 


\end{enumerate}




\section{Simulation results on pairs of data}
\label{supp_sec:sims_pairs}

For pairs of observations, we specified the adjacency matrix as block diagonal, where each block was the $2\times 2$ matrix
$
\left(
\begin{smallmatrix}
0 & 1 \\ 1 & 0
\end{smallmatrix}
\right).
$
For the simulations on network data in \cref{sec:sims}, the network has median degree 2, and we set $\tau^2_U = \tau^2_Z = 1$. For the pair data, for which median node degree is equal to 1, we set $\tau^2_U = \tau^2_Z = 2$, in order to ensure similar marginal variability in the exposure and the unmeasured confounder in the network and paired data settings


\begin{table}[p]
\spacingset{1.15}
    \centering
    \vspace{20pt}
    \caption{Simulation results for paired data. Results show the bias, root mean squared error and coverage of 95\% intervals for the local and interference effects based on the OLS estimator and our approach.}
    % \rowcolors{5}{}{gray!10}
    \resizebox{0.98\textwidth}{!}{%
    \begin{tabular}{*{15}{c}}
        \hline
        \\[-5pt]
        % & & \multicolumn{13}{c}{Paired data} \\[-5pt] \\
        % \cmidrule{3-15}
       & & \multicolumn{6}{c}{Local effect} & &  \multicolumn{6}{c}{Interference effect} \\
       \cmidrule(lr){3-8} \cmidrule(lr){10-15}
        \multicolumn{2}{c}{True model \&} & \multicolumn{3}{c}{OLS} &  \multicolumn{3}{c}{Our approach} &
        & \multicolumn{3}{c}{OLS} &  \multicolumn{3}{c}{Our approach} \\
        \cmidrule(lr){3-5} \cmidrule(lr){6-8} \cmidrule(lr){10-12} \cmidrule(lr){13-15}  
        \multicolumn{2}{c}{sample size} & Bias & RMSE & Cover & Bias & RMSE & Cover &
        & Bias & RMSE & Cover & Bias & RMSE & Cover \\[10pt]
    
        \hline
        \\[-8pt]
        
        \multirow{4}{*}{\ref{fig:direct}} & & \multicolumn{13}{c}{$\beta_{\bar Z} = 0$ and $\beta_{\bar U}=0$} \\[2pt]
        \cmidrule{7-11}
        & 200 & 0.660 & 0.669 & 0 & -0.069 & 0.307 & 95.5 & & 0.151 & 0.172 & 55\phantom{.0} & 0.010 & 0.104 & 94.2 \\ 
  & 350 & 0.660 & 0.664 & 0 & -0.069 & 0.240 & 98.6 & & 0.144 & 0.155 & 38\phantom{.0} & 0.001 & 0.075 & 95.8 \\ 
  & 500 & 0.670 & 0.673 & 0 & -0.079 & 0.228 & 96.8 & & 0.147 & 0.155 & 17.7 & 0.001 & 0.063 & 97\phantom{.0} \\[3pt]
        \hline
        \\[-8pt]

        
        \multirow{4}{*}{\ref{fig:interference}} & & 
        \multicolumn{13}{c}{$\beta_{UZ} = 0$ and $\beta_U = \beta_{\bar U} = 0$}
        \\[2pt]
        \cmidrule{7-11}
        & 200 & 0.004 & 0.095 & 96\phantom{.0} & -0.037 & 0.129 & 99.3 & & 0.005 & 0.064 & 94.7 & 0.003 & 0.065 & 96.3 \\ & 350 & -0.003 & 0.069 & 95.7 & -0.029 & 0.114 & 99.5 & & -0.004 & 0.047 & 95.3 & -0.005 & 0.050 & 97\phantom{.0} \\ 
        & 500 & 0.000 & 0.065 & 92.3 & -0.031 & 0.116 & 99.2 & & 0.001 & 0.041 & 94.3 & -0.001 & 0.043 & 95.3 \\[3pt]
        \hline
        \\[-8pt]
         
        \multirow{4}{*}{\ref{fig:general_spatial_conf}} & &
        \multicolumn{13}{c}{$\beta_{\bar Z} = 0$}
        \\[2pt]
        \cmidrule{7-11}
        & 200 & 0.923 & 0.932 & 0 & -0.155 & 0.299 & 95.2 & & 0.269 & 0.285 & 21.7 & 0.020 & 0.123 & 96.1 \\ 
        & 350 & 0.920 & 0.925 & 0 & -0.163 & 0.261 & 94.7 & & 0.265 & 0.273 & \phantom{0}2.3 & 0.006 & 0.093 & 96.9 \\ 
        & 500 & 0.933 & 0.936 & 0 & -0.170 & 0.251 & 91.8 & & 0.270 & 0.276 & \phantom{0}0.7 & 0.004 & 0.079 & 96\phantom{.0} \\[3pt]
        \hline
        \\[-8pt]
        
        \multirow{4}{*}{\ref{fig:direct_interference}} & &
        \multicolumn{13}{c}{$\beta_{\bar U} = 0$} \\[2pt]
        \cmidrule{7-11}
        & 200 & 0.660 & 0.669 & 0 & 0.001 & 0.262 & 96.9 & & 0.151 & 0.172 & 55\phantom{.0} & 0.011 & 0.105 & 94.1 \\ 
  & 350 & 0.660 & 0.664 & 0 & -0.007 & 0.215 & 96.9 & & 0.144 & 0.155 & 38\phantom{.0} & 0.001 & 0.077 & 96.1 \\ 
  & 500 & 0.670 & 0.673 & 0 & -0.014 & 0.189 & 98.1 & & 0.147 & 0.155 & 17.7 & 0.002 & 0.061 & 97\phantom{.0} \\[3pt]
        \hline
        \\[-8pt]
         
        \multirow{4}{*}{\ref{fig:predictor_interference}} & & 
        \multicolumn{13}{c}{$\beta_U = 0$ and $\beta_{\bar U} = 0$} \\[2pt]
        \cmidrule{7-11}
        & 200 & 0.002 & 0.079 & 95.3 & -0.053 & 0.144 & 98.8 & & 0.005 & 0.061 & 94.7 & 0.002 & 0.064 & 95.6 \\ 
  & 350 & -0.002 & 0.057 & 95.7 & -0.043 & 0.141 & 98.4 & & -0.003 & 0.044 & 95.7 & -0.007 & 0.050 & 95.6 \\ 
  & 500 & 0.000 & 0.054 & 91.7 & -0.068 & 0.186 & 89.6 & & 0.001 & 0.039 & 93.7 & -0.007 & 0.047 & 93.6 \\[3pt]
        \hline
        \\[-8pt]
       
        \multirow{3}{*}{\ref{fig:general_interference}} 
        & 200 & 0.923 & 0.932 & 0 & -0.109 & 0.271 & 96.9 & & 0.269 & 0.285 & 21.7 & 0.014 & 0.126 & 95.400 \\ 
  & 350 & 0.920 & 0.925 & 0 & -0.119 & 0.230 & 95.4 & & 0.265 & 0.273 & \phantom{0}2.3 & 0.002 & 0.097 & 95.800 \\ 
  & 500 & 0.933 & 0.936 & 0 & -0.118 & 0.209 & 95\phantom{.0} & & 0.270 & 0.276 & \phantom{0}0.7 & 0.004 & 0.080 & 96.100 \\[3pt]
        \hline

        \end{tabular}
     }%
     \label{tab:sims_pairs}
\end{table}

\cref{tab:sims_pairs} shows the simulation results for pairs of data with 100, 175, and 250 pairs of observations (total number of observations 200, 350, and 500). We present bias, root mean squared error and coverage of 95\% intervals for the OLS estimator and for our approach, for the local and the interference effects. These results mirror the results for network data shown in \cref{tab:sims_network}, and the conclusions from the two settings are unaltered.


\section{Additional study information}
\label{supp_sec:application}

\subsection{The data set}

We assemble a data set on power plant emissions and characteristics, population demographics, weather, and information on cardiovascular mortality among the elderly, measured at the level of US counties. We briefly describe the data set here.

We acquire power plant emissions and characteristics for 2004 based on the publicly available data from \cite{papadogeorgou2019adjusting}. Power plant information includes the number of power plant units in the facility, whether the plant uses mostly natural gas or coal (an important predictor of SO$_2$ emissions), its total emissions, heat input and operating capacity, whether it has a technology installed for oxides of nitrogen control, and whether the plant participated in Phase II of the Acid Rain Program. Our data set includes 906 power plant facilities in 596 counties. We aggregate power plant information at the county level, and define the total SO$_2$ emissions from all power plants in the county as the exposure of interest.
We consider first and second degree county-level adjacency matrices. The first degree adjacency matrix $A^1$ has $(i,j)$ entry equal to 1 if counties $i$ and $j$ share a border, and 0 otherwise. Instead the $(i,j)$ entry of the second degree adjacency matrix $A^2$ is equal to 1 if $i$ and $j$ share a border or a first-degree neighbor. Considering the size of counties in the US and the potential long-distance pollution transport, we define the neighborhood exposure $\overline Z$ using the second degree adjacency matrix $A^2$, allowing neighbors of neighbors to contribute to potential interference effects. 

\begin{figure}[!t]
\includegraphics[width = 0.45\textwidth,trim=60 17 60 35, clip]{SO2_emissions.png}
\hfill
\includegraphics[width = 0.45\textwidth,trim=60 17 60 35, clip]{mortality.png}
\vspace{-3pt}
\caption{County-level exposure (left) and outcome (right) on the 445 counties in our data set.}
\label{fig:data}
\end{figure}

We considered demographic information as potential confounders. Specifically, we consider population characteristics such as percentages of urbanicity, of white and hispanic population, of population with at least a high school diploma, of population that lives below the poverty limit, of female population, of population having lived in the area for less than 5 years, of housing units that are occupied, and population per square mile from the 2000 Census, and also county-level smoking rates acquired using the CDC Behavioral Risk Factor Surveillance System data.

We downloaded county level weather data for 2004 from the National Oceanic and Atmospheric Administration's (NOAA) data base, available at \url{ftp://ftp.ncdc.noaa.gov//pub/data/cirs/climdiv/}. Specifically, we acquired data for each county describing the maximum, minimum and average temperature, and total precipitation for each month in 2004. We aggregated the data across the twelves months by considering the total yearly precipitation, the second most extreme of the monthly maximum and minimum temperatures, the average maximum and minimum temperatures, and the average, maximum and minimum of the average monthly temperatures. After examining the correlation matrix, we deduced that many covariates were highly correlated, and used only the three mentioned above (total precipitation, second maximum and minimum temperatures).

We acquire health information from the United States Centers for Disease Control and Prevention (CDC) WONDER query system. We consider deaths due to the diseases of the circulatory system (I-00 to I-99 codes) among population aged 65 years or older, and define the outcome of interest as the number of deaths per 100,000 residents in 2005.

We merge power plant, weather, health and demographic information. We only keep counties with at least one neighbor with SO$_2$ emissions from power plants, since the interference effect of changing neighborhood exposure would not be well-defined for a county without neighbors with emissions. The final data set includes 445 counties in 44 US states, illustrated in \cref{fig:data}. 
% We purposefully exclude meteorology information from our data set as these are spatial covariates that potentially confound the effect of interest, and we wish to investigate the extent to which the proposed methodology is able to account for them.


\subsection{Analysis including weather variables}

We found that OLS estimates for the local and interference effect are comparable when weather variables are included or not (\cref{supp_fig:app_OLS_weather}). Therefore, we have focused in our main text on the analyses excluding weather variables.

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{app_OLS_weather_results.pdf}
\caption{OLS estimates for the local and interference effect of exposure or the logarithmic transformation for exposure when adjusting for local and neighborhood values of power plant and demographic characteristics only, or also including weather information}
\label{supp_fig:app_OLS_weather}
\end{figure}



\end{document}
