%% %%%%%%%%%%%
%REVISED COPY
%%%%%%%%%%%%%%%%
\documentclass[final]{siamltex}

\usepackage{amsmath,amssymb,graphicx,graphics}
\usepackage[dvips]{epsfig}
\usepackage{showkeys}

\def \R{\mathbb R}
\def \C{\mathbb C}
\def \N{\mathbb N}
\def \Z{\mathbb Z}

\newcommand\qed{{\hspace*{\fill}Q.E.D.\vskip12pt plus 1pt}}
\newcommand\qad{{\hspace*{\fill}$\Box$\vskip12pt plus 1pt}}
\newcommand\Pic[1]{\hbox{\rm Pic(}#1\hbox{\rm )}}
\newcommand\genus{{geometric genus\ }}
\newcommand\length{\hbox{\rm length}}


\newcommand\sA{{\cal A}}
\newcommand\sB{{\cal B}}
\newcommand\sC{{\cal C}}
\newcommand\sD{{\cal D}}
\newcommand\sE{{\cal E}}
\newcommand\sF{{\cal F}}
\newcommand\sG{{\cal G}}
\newcommand\sH{{\cal H}}
\newcommand\sJ{{\cal J}}
\newcommand\sK{{\cal K}}
\newcommand\sL{{\cal L}}
\newcommand\sM{{\cal M}}
\newcommand\sN{{\cal N}}
\newcommand\sO{{\cal O}}
\newcommand\sP{{\cal P}}
\newcommand\sQ{{\cal Q}}
\newcommand\sR{{\cal R}}
\newcommand\sS{{\cal S}}
\newcommand\sT{{\cal T}}
\newcommand\sU{{\cal U}}
\newcommand\sW{{\cal W}}
\newcommand\sX{{\cal X}}
\newcommand\sV{{\cal V}}
\newcommand\sZ{{\cal Z}}


\newtheorem{remark}{Remark}[section]
\newtheorem{example}{Example}[section]



\newcommand\scc{{\mathscr C}}
\newcommand\scr{{\mathscr R}}
\newcommand\scf{{\mathscr F}}
\newcommand\sce{{\mathscr E}}
\newcommand\scg{{\mathscr G}}
\newcommand\scl{{\mathscr L}}
\newcommand\scd{{\mathscr D}}
\newcommand\sco{{\mathscr O}}
\newcommand\sch{{\mathscr H}}
\newcommand\scv{{\mathscr V}}
\newcommand\scy{{\mathscr Y}}
\newcommand\scj{{\mathscr J}}



\newcommand\gra{\alpha}
\newcommand\grb{\beta}
\newcommand\grg{\gamma}
\newcommand\grd{\delta}
\newcommand\gre{\epsilon}
\newcommand\grx{\chi}
\newcommand\grk{\kappa}
\newcommand\grl{\lambda}
\newcommand\vphi{\varphi}
\newcommand\grr{\rho}
\newcommand\grs{\sigma}
\newcommand\grt{\tau}
\newcommand\grz{\zeta}
\newcommand\gro{\omega}



\newcommand\rat{{\mathbb Q}}
\newcommand\reals{{\mathbb R}}
\newcommand\comp{{\mathbb C}}
\newcommand\zed{{\mathbb Z}}
\newcommand\eff{{\mathbb F}}
\newcommand\pn[1]{{\mathbb P}^{#1}}
\newcommand\proj[1]{{\mathbb P}({#1})}
\newcommand\pnc{{\mathbb P}_{\mathbb C}}
\newcommand\Gr{{\mathbb G}}


\newcommand\fm{{\frak m}}
\newcommand\fri{{\frak I}}
\newcommand\fj{{\frak K}}
\newcommand\ft{{\frak t}}
\newcommand\alb[1]{\rm{ALB}({#1})}



\newcommand\hatS{{\widehat{S}}}
\newcommand\hatd{{\widehat{d}}}


\title{Multi-scale CLEAN in hard X-ray solar imaging}

\author{A. Volpara\thanks{MIDA, Dipartimento di Matematica, Universit\`a di Genova, via Dodecaneso 35, 16146 Genova, Italy  ({\tt volpara@dima.unige.it})} 
\and
M. Piana\thanks{MIDA, Dipartimento di Matematica, Universit\`a di Genova, via Dodecaneso 35, 16146 Genova, Italy \& Istituto Nazionale di Astrofisica, Osservatorio Astrofisico di Torino, Torino, Italy ({\tt piana@dima.unige.it})}
\and
A. M.~Massone\thanks{MIDA, Dipartimento di Matematica, Universit\`a di Genova, via Dodecaneso 35, 16146 Genova, Italy ({\tt massone@dima.unige.it}) }}
%\and
%A. M.~Massone\thanks{CNR - SPIN, via Dodecaneso 33, 16146 Genova, Italy ({\tt annamaria.massone@cnr.it}) }}%\and
%Sara Giordano\thanks{Dipartimento di Matematica, Universit\`a di Genova, via Dodecaneso 35, 16146 Genova, Italy ({\tt giordano@dima.unige.it}) } \and
%R. A. Schwartz \thanks{Catholic University of America and NASA Goddard Space Science Center, Greenbelt, MD, USA ({\tt  richard schwartz <richard.a.schwartz@nasa.gov}).} }

\begin{document}

\maketitle

\begin{abstract}
Multi-scale deconvolution is an ill-posed inverse problem in imaging, with applications ranging from microscopy, through medical imaging, to astronomical remote sensing. In the case of high-energy space telescopes, multi-scale deconvolution algorithms need to account for the peculiar property of native measurements, which are sparse samples of the Fourier transform of the incoming radiation. The present paper proposes a multi-scale version of CLEAN, which is the most popular iterative deconvolution method in Fourier space imaging. Using synthetic data generated according to a simulated but realistic source configuration, we show that this multi-scale version of CLEAN performs better than the original one in terms of accuracy, photometry, and regularization. Further, the application to a data set measured by the NASA Reuven Ramaty High Energy Solar Spectroscopic Imager (RHESSI) shows the ability of multi-scale CLEAN to reconstruct rather complex topographies, characteristic of a real flaring event.
\end{abstract}

\begin{keywords} 
Multi-scale deconvolution; iterative algorithms; Fourier-based imaging; high-energy space telescopes
\end{keywords}

\begin{AMS}
45Q05; 47A52; 68U10; 8508
%convolution; inverse problems; ill-posedness, regularization; image processing; astronomy, computational methods
\end{AMS}

\pagestyle{myheadings}

\thispagestyle{plain}

\markboth{A. Volpara et al}{multi-scale CLEAN}


\section{Introduction}
Both on earth and from space astronomical data require a notable pre-processing effort before they can be transformed into meaningful images and then utilized for physical interpretation. In fact, the main mathematical challenge of astronomical image reconstruction \cite{pantin2017deconvolution} is the need of solving an ill-posed inverse problem whose instability issues can be addressed by means of regularization constraints \cite{bertero2021introduction}. In particular, modalities like on-earth radio \cite{kruger2012introduction} and from-space hard X-ray imaging \cite{piana2022hard} work by temporal source modulation rather than focusing, so that these telescopes measure a specific set of spatial Fourier components of the source, called visibilities, sampled in correspondence of specific spatial frequency pairs, called $(u,v)$ points. Image reconstruction from visibilities is, therefore, a Fourier transform inversion problem from limited data based on two steps:
\begin{itemize}
\item The gridding step transforms the visibility signal recorded in the $(u,v)$ plane into a (blurred) image.
\item The deconvolution step subtracts the effect of the Point Spread Function (PSF) of the instrument.
\end{itemize}
This paper focuses on this second deconvolution step in the case of hard X-ray space data, and introduces a multi-scale formulation of the most used deconvolution algorithm in this framework, i.e., the CLEAN algorithm \cite{hogbom1974aperture,dennis2009hard,li2011application,zhang2020parameterized,chu2019deconvolution,bose2002sequence}. In particular, X-ray collimators perform a sparse and limited sampling in the $(u,v)$ plane. This implies that inverse Fourier transforming X-ray visibilities yields notable artifacts in the image provided by gridding. In the CLEAN framework this image is named dirty map and the technical goal of this algorithm is to non-linearly and iteratively deconvolve the instrumental PSF (named dirty beam in this context) from the dirty map by
\begin{enumerate}
\item Identifying the maximum and its position.
\item Generating a CLEAN component, i.e. a $\delta$-Dirac, located in this position in a twin image called  the CLEAN components map.
\item Subtracting the PSF centered on the position of the maximum from the dirty map.
\item Repeating this scheme until the remaining dirty map  contains just noise.
\item Generating the CLEANed image as the convolution of the collection of CLEAN components with an idealized PSF, called CLEAN beam, obtained by fitting the dirty beam by means of a 2D Gaussian function.
\item Adding appropriate residuals to the CLEANed image.
\end{enumerate}
This algorithm is fast and effective in removing the sidelobes introduced by the PSF. However, CLEAN introduces significant photometric errors (the total flux is not conserved during the iterations) and, more importantly, in complex images is prone to misinterpret the spatial scales concurrently characterizing the different sources. 

This paper introduces a multi-scale version of CLEAN for deconvolving hard X-ray data which is partly inspired by an algorithm introduced in 2008 in radio imaging \cite{cornwell2008multiscale}. The formulation here presented is specifically tailored on a specific property of all the hard X-ray instruments of most recent conception and, as remarked in Section 3, it differs from its radio-interferometry counterpart at both a conceptual and a technical level. In fact, {\em{YohKoh}} (operating over 1991-2001) \cite{1992Sci...258..618A,ogawara1992status}, the Reuven Ramaty High Energy Solar Spectroscopic Imager (RHESSI) (launched by NASA in 2002 and decommissioned in 2018) \cite{lin2003reuven,fletcher2011observational} and the Spectrometer/Telescope for Imaging X-rays (STIX) (which has begun its travel to the Sun in February 2020 in the ESA Solar Orbiter payload) \cite{krucker2020spectrometer,massa2022first}, all sample the $(u,v)$ plane according to circles characterized by increasing radii. This configuration implies 
that the PSF of these telescopes is the sum of PSF components, each one associated to a specific circle of sampled visibilities in the $(u,v)$ plane. Accordingly, the dirty map corresponding to the measured hard X-ray visibilities can be straightforwardly interpreted as the sum of several dirty maps, each one corresponding to one PSF component. Finally, this property has a technical impact on two implementation steps: the way the dirty maps are rescaled in the pre-processing step and the way residuals are added to the CLEANed image in the final step of the algorithm. 

This formulation of the CLEAN method is genuinely multi-scale, since it is able to point out all different scales using, in one shot, the whole dataset and the total PSF. Further, its implementation is naturally biased in favor of the reconstruction of smaller sources, that, otherwise, could be obscured by sources with larger scales. We showed the effectiveness of our approach by first validating the algorithm against synthetic but realistically simulated visibility sets. Then, we applied the method to an experimental data set observed by RHESSI during an intense X-ray flare.

The plan of the paper is as follows. In Section 2 the well-known CLEAN algorithm for visibilities is recalled. Section 3 introduces the new multi-scale CLEAN approach for hard X-ray visibilities. Section 4 provides the numerical validation of the method against synthetic visibilities mimicking RHESSI acquisition. Section 5 describes an application to experimental RHESSI data. Our conclusions are offered in Section 6.

\section{The CLEAN algorithm: mathematical setup}
In radio and X-ray astronomical imaging, CLEAN is a numerical procedure for the solution of the convolution equation
\begin{equation}\label{prob}
I^D(x,y) = (K * I)(x,y) := \int\int K(x-x',y-y')I(x',y')dx'dy'\ \ ,
\end{equation}
where $I$ is the unknown source flux, $I^D$, the dirty map, is the input blurred and noisy image, and $K$ is the Point Spread Function (PSF) of the instrument. CLEAN models the solution $I(x,y)$ as the sum of $Q$ unknown point sources described by $\delta$-Dirac distributions plus background, i.e.:
\begin{equation}\label{prob-1}
I(x,y) = \sum_{q=1}^Q I_q \delta(x-x_q,y-y_q) + B(x,y) \ ,
\end{equation}
where $I_q$ and $(x_q,y_q)$ are respectively the amplitude and position of the $q$-th source and $B(x,y)$ models the background. Using (\ref{prob-1}) into (\ref{prob}) leads to
\begin{equation}\label{dirty}
%\begin{split}
I^D(x,y) 
%& = \int \int \sum_{q=1}^Q I_q \delta(x^{\prime}-x_q,y^{\prime}-y_q)K(x-x',y-y')dx'dy' + (K *B)(x,y) \\
         %& 
         = \sum_{q=1}^Q I_q K(x-x_q,y-y_q) + (K*B)(x,y).
%\end{split}
\end{equation}

The goal of CLEAN is then to iteratively estimate $I_q, x_q, y_q$ for $q=1,\ldots,Q$, as well as $Q$, given an estimate of the dirty map $I^D$ and armed with the instrument PSF $K$, here called dirty beam. Starting from the initialization
\begin{equation}\label{step-0}
I^{(0)}(x,y) := I^D(x,y) 
\end{equation}
the following CLEAN loop is at the base of such algorithm:
\vspace{0.25cm}

\begin{description}
\item{{\bf{Maximum Identification.}}} This is an easy optimization step that provides, at the $t$-th iteration of the loop $(t\ge1)$, the maximum and its position in the dirty image by computing
\begin{equation}\label{step-1}
(x_{max}^{(t)},y^{(t)}_{max}) = \arg\max_{(x,y)} I^{(t-1)}(x,y)~~~~~~~~~~~I^{(t)}_{max} = I^{(t-1)}(x_{max}^{(t)},y_{max}^{(t)}).
\end{equation}
\item{{\bf{CLEAN Components Update.}}} Starting from an empty map $CC^{(0)} (x,y)$, the {\it CLEAN components map} at the $t$-th iteration is
\begin{equation}\label{step-2}
CC^{(t)} (x,y) = CC^{(t-1)}(x,y) + \frac{\gamma I^{(t)}_{max}}{\max_{(x,y)} |K(x,y)|} \delta(x-x_{max}^{(t)},y-y_{max}^{(t)}),
\end{equation}
where $\gamma$ is the so-called {\em{gain factor}}.
\vspace{0.2cm}
\item{{\bf{Dirty Map Update.}}} The dirty beam centered at the maximum is subtracted from the actual dirty map (i.e., $I^{(t-1)}(x,y)$) to obtain
\begin{equation}\label{step-3}
I^{(t)}(x,y) = I^{(t-1)}(x,y) - \frac{\gamma I^{(t)}_{max}}{\max_{(x,y)} |K(x,y)|} K(x-x_{max}^{(t)},y-y_{max}^{(t)}).
\end{equation}
\item{{\bf{Update.}}} $t=t+1$ and then go back to the first step until a stopping rule is verified.
\end{description}

\vspace{0.25 cm}

The idea behind the CLEAN loop is the one to iteratively identify point sources in the actual dirty map and to save their intensity (scaled by the gain factor and the PSF peak) and position in the CLEAN components map. At the same time, in the Dirty Map Update step, the blurring effect of the PSF on the same point source is removed from the dirty map by subtracting the (rescaled) dirty beam.

The stopping rule for the loop applies when there are no more point sources in the updated dirty map, which occurs when this is embedded by the background map (i.e., when just positive and negative lobes of the same amplitude survive in the updated dirty map). Therefore, if $t={\overline{t}}$ is the iteration when this happens, then we define the residual map
\begin{equation}\label{residual-map}
R(x,y) := I^{({\overline{t}})}(x,y)
\end{equation}
and, from eq.~(\ref{dirty}), we have 
\begin{equation}\label{final-residual}
R(x,y) \simeq (K * B)(x,y),
\end{equation}
which inspires the following approximation for the background map:
\begin{equation}\label{background}
B(x,y) \simeq B^{\prime}(x,y):= \frac{R(x,y)}{T},
\end{equation}
where $T$ is an estimate of the integral of the PSF over the Field of View.

As the final step of CLEAN deconvolution, one constructs the CLEANed map $C(x,y)$ as 
\begin{equation}\label{CLEAN-final}
C(x,y) = (K^C * (CC^{({\overline{t}})}+B^{\prime}))(x,y)=(K^C * CC^{({\overline{t}})})(x,y) +  (K^C * B^{\prime})(x,y),
\end{equation}
where $K^C(x,y)$ is an idealized version of the instrumental, experimental PSF, here called CLEAN beam.

\vspace{0.25 cm}
\begin{remark}
The gain factor controls the fraction of each point source to be added in the CLEAN components map and subtracted from the dirty map. Typical values are in the range $[0.05,0.5]$.
\end{remark}
\vspace{0.25 cm}
\begin{remark} An estimate for $T$ is given by $T = \sum_{n}^{N_{pix}} K(x_n,y_n) \delta x \delta y$, where $(x_n,y_n)$ is the $n$-th pixel ($n=1,\dots,N_{pix}$) in the PSF image and $\delta x$, $\delta y$ are the pixel dimensions. 
\end{remark}

\subsection{The CLEAN algorithm in hard X-ray imaging}
In hard X-ray (and in radio astronomy) imaging, the experimental observations are given as a set of $N_V$ measured visibilities $\{V(u_l,v_l)\}_{l=1}^{N_V}$. Visibilities are samples of the Fourier transform of the source flux, i.e.
\begin{equation}\label{visibilities}
V(u_l,v_l) = \int_{\R^2} I(x,y) \exp[2 \pi i (u_l x  + v_l y)] dx dy~~~~l=1,\ldots,N_V.
\end{equation}
It follows that a straightforward estimate of the dirty map can be obtained by numerically computing the inverse Fourier transform of the set of sampled visibilities, i.e. 
\begin{equation}\label{dirty-1}
I^D(x,y) =  \sum_{l=1}^{N_V} V(u_l,v_l) \exp[-2 \pi i (x u_l + y v_l)] \delta u_l \delta v_l \ ,
\end{equation}
%where $m,n = 1,\ldots, N_I$ denote the $N_I \times N_I$ pixels in the source image and 
where $\delta u_l, \delta v_l$ are the weights in the numerical integration. 
%and we have normalized with respect to the number of visibilities $N_V$.
From (\ref{visibilities}) it follows that $I(x,y)=\delta(x,y)$ implies $V(x,y)=1$ for all sampled spatial frequencies. Therefore, the dirty beam can be computed as
\begin{equation}\label{dirty-beam}
K(x,y) =  \sum_{l=1}^{N_V}  \exp[-2 \pi i (x u_l + y v_l)] \delta u_l \delta v_l \ ,
\end{equation}
i.e., the dirty beam is the discretized inverse Fourier transform of the characteristic function of the sampled visibilities (i.e, the transfer function of the imaging system). 
Finally, the idealized version of the PSF, the CLEAN beam, is obtained by fitting $K(x,y)$ in (\ref{dirty-beam}) with a two-dimensional Gaussian function.

%Given these specific definitions for the dirty map, the dirty beam and the clean beam, the general mathematical setup for the CLEAN algorithm described in the previous section holds true also in the case of hard X-ray imaging. 


\section{The multi-scale CLEAN algorithm}
The differences between CLEAN as presented in the previous section and our multi-scale version of the method are at a both model and algorithm level. 

\begin{figure}[h]
\begin{center}
\includegraphics[width=11.cm]{Campionamento_line} 
\caption{Sampling of the $(u,v)$ plane as performed by three hard X-ray telescopes. Left: {\em{RHESSI}}. Right: {\em{STIX}}.}
\label{fig:uv-samples}
\end{center}
\end{figure}

From the modeling viewpoint, we exploit the fact that hard X-ray telescopes are characterized by a PSF that can be written, under some assumptions, as the sum of a finite number of PSF components, each one filtering a specific portion of the $(u,v)$ plane.  For example, 
%the Hard X-ray Telescope on-board the Yohkoh Japanese mission picked-up Fourier components over six semi-circles (see Figure \ref{fig:uv-samples}, left panel) \cite{kosugi1991hard}. 
the NASA satellite RHESSI measured visibilities by sampling them over nine circles in the $(u,v)$ plane with radii decreasing from $R_1 \sim 0.22$ arcsec$^{-1}$ to $R_9 \sim 0.0027$ arcsec$^{-1}$, according to a geometric progression with ratio $\frac{1}{\sqrt{3}}$ (see Figure \ref{fig:uv-samples}, left panel) \cite{hurford2003rhessi}, where the number of samples in each circle is not fixed but determined in an optimal way during the computational procedure of visibility generation (data stacking). The ESA instrument STIX samples the $(u,v)$ plane in $60$ points\footnote{STIX recording hardware is made of 30 sub-collimators, each one sampling the spatial frequency domain in one specific point. However, the property $V(-u,-v) = \overline{V(u,v)}$ allows the duplication of the 30 spatial frequency samples, where the overline indicates the complex conjugate.} placed over six spirals, but in such a way that these points lay over $10$ circles with radii decreasing from $\sim 0.07$ arcsec$^{-1}$ to $\sim 0.0028 $ arcsec$^{-1}$, according to a geometric progression with ratio $\sim 0.7$ (see Figure \ref{fig:uv-samples}, right panel) \cite{massa2023stix} (more than thirty years ago also the Yohkoh Japanese mission picked-up Fourier components over six semi-circles with uniformly increasing radii \cite{kosugi1991hard}). Therefore, if we group the circles in the $(u,v)$ plane in $N$ disjoint sets, we can define a number $N$ of experimental PSF components, i.e. of dirty beams $\{K_j(x,y)\}_{j=1}^N$, such that
\begin{equation}\label{psf-component}
K_j(x,y) = \sum_{l=1}^{N_j} \exp[-2 \pi i (x u^{(j)}_l + y v^{(j)}_l)] \delta u^{(j)}_l \delta v^{(j)}_l~~~~~j=1,\ldots,N,
\end{equation}
where $N_j$ and $\{u_l^{(j)},v_l^{(j)}\}_{l=1}^{N_j}$ are the number and the set of visibilities belonging to the $j$-th subset of sampled circles in the $(u,v)$ plane, respectively. For example, in RHESSI we may have maximum $N=9$ dirty beams, which happens when each subset is made of just one of the $9$ circles sampled by the RHESSI Rotation Modulation Collimators. Similarly, in STIX we may have maximum $N=10$ dirty beams. Further,  under the assumptions that $\sum_{j=1}^N N_j = N_V$, the sets $\{u_l^{(1)},v_l^{(1)}\}_{l=1}^{N_1},\ldots,\{u_l^{(N)},v_l^{(N)}\}_{l=1}^{N_N}$ are all disjoint, and that natural weighting is considered (i.e. $\delta u^{(j)}_l=\delta u$ and $\delta v^{(j)}_l=\delta v$ for each $j=1,\dots,N$ and $l=1,\dots,N_j$), 
then 
\begin{equation}\label{psf-sum}
K(x,y) = \sum_{j=1}^N K_j (x,y)
\end{equation} 
where the $j$-th dirty beam reads
\begin{equation}\label{psf-natural}
K_j(x,y) = \sum_{l=1}^{N_j} \exp[-2 \pi i (x u^{(j)}_l + y v^{(j)}_l)] \delta u\delta v~~~~~j=1,\ldots,N.
\end{equation}
\begin{remark}
In the following we will assume that $K_1(x,y)$ is the highest resolution PSF component, i.e. that the corresponding transfer function samples the portion of the $(u,v)$ plane characterized by the highest frequencies. We will also assume that this resolution decreases as $j$ increases.  
\end{remark}
\vspace{0.25 cm}

Using the same approach, we can define a set of $N$ dirty maps
\begin{equation}\label{dirty-multi-1}
I^D_j(x,y) =  \sum_{l=1}^{N_j} V(u^{(j)}_l,v^{(j)}_l) \exp[-2 \pi i (x u^{(j)}_l + y v^{(j)}_l)] \delta u \delta v~~~~~j=1,\ldots,N
\end{equation}
such that 
\begin{equation}\label{dirty-sum}
I^D(x,y) = \sum_{j=1}^N I^D_j (x,y).
\end{equation}


Coherently with the model \eqref{psf-sum} for the PSF, the new model for the source image reads as the sum of $N$ basis functions $\{m_i\}_{i=1}^N$ each one characterized by a specific scale (for example, the support in the case of paraboloids, the Full Width at Half Maximum or the standard deviation in the case of Gaussian functions). 

\vspace{0.25 cm}
\begin{remark}
The geometrical scales $\{m_i\}_{i=1}^N$ represent the different physical scales showing up in the radiation source. Accordingly to Remark 3.1, we assume that $m_1(x,y)$ corresponds to the smallest scale and that the scale dimension increases as $i$ increases.
\end{remark}
\vspace{0.25 cm}

The source image $I(x,y)$ can be modeled as the superposition of the basis functions, i.e. as
\begin{equation}\label{source-multi}
I(x,y) = \sum_{i=1}^N\sum_{q_i=1}^{Q_i}I_{q_i}m_i(x-x_{q_i},y-y_{q_i}) + B(x,y),
\end{equation}
which means that at scale $i$, for $i=1,\ldots,N$, there are $Q_i$ sources, each one placed at $(x_{q_i},y_{q_i})$ and with peak intensity $I_{q_i}$ for $q_i = 1,\dots,Q_i$ (we assume here that each basis function $m_i(x,y)$ is normalized in such a way that $\int_{\R^2} m_i(x,y) dx dy=1$). Inserting equation (\ref{source-multi}) into the model equation (\ref{prob}) leads to
\begin{equation}
%\begin{split}
I^D(x,y) 
%& = \int \int \sum_{i=1}^N \sum_{q_i=1}^{Q_i} I_{q_i} m_i(x^{\prime}-x_{q_i},y^{\prime}-y_{q_i})K(x-x',y-y')dx'dy' + (K *B)(x,y) \\
%& 
= \sum_{i=1}^N \sum_{q_i=1}^{Q_i} I_{q_i}  (m_i*K)(x-x_{q_i},y-y_{q_i}) + (K*B)(x,y).
%\end{split}
\end{equation}
We now exploit equations (\ref{psf-sum}) and (\ref{dirty-sum}) to obtain
%\begin{equation}\label{quasi-final}
%%\begin{split}
%\sum_j^N I_j^D(x,y) = \sum_{j=1}^N \sum_{i=1}^N \sum_{q_i=1}^{Q_i} I_{q_i}  (m_i * K_j)(x-x_{q_i},y-y_{q_i}) + \sum_{j=1}^N (K_j*B)(x,y).
%%\end{split}
%\end{equation}

\begin{equation}\label{quasi-final2}
 \sum_j^N I_j^D(x,y) = \sum_{j=1}^N \left( \sum_{i=1}^N \sum_{q_i=1}^{Q_i} I_{q_i}  (m_i * K_j)(x-x_{q_i},y-y_{q_i}) + (K_j*B)(x,y) \right)  
\end{equation}

%\begin{eqnarray}\label{quasi-final2}
%\begin{split}
%\hspace{-0.25cm} \sum_j^N I_j^D(x,y) & = & \sum_{j=1}^N \sum_{i=1}^N \sum_{q_i=1}^{Q_i} I_{q_i}  (m_i * K_j)(x-x_{q_i},y-y_{q_i}) + \sum_{j=1}^N (K_j*B)(x,y) \nonumber \\
% & = & \sum_{j=1}^N \left( \sum_{i=1}^N \sum_{q_i=1}^{Q_i} I_{q_i}  (m_i * K_j)(x-x_{q_i},y-y_{q_i}) + (K_j*B)(x,y) \right).
%\end{split}
%\end{eqnarray}
This equation inspires our multi-scale version of the CLEAN algorithm in which each dirty map $I_j^D(x,y)$ is identified as
\begin{equation}\label{final}
I_j^D(x,y) = \sum_{i=1}^N \sum_{q_i=1}^{Q_i} I_{q_i}  (m_i*K_j)(x-x_{q_i},y-y_{q_i}) + (K_j*B)(x,y),
\end{equation}
and CLEANed according to a parallel-wise procedure. The key idea is based on the interplaying properties of the basis functions and the PSF components. In fact,  equation (\ref{final}) shows that each dirty map contains cross-convolution products between the corresponding PSF component and all the basis functions. 

\vspace{0.25 cm}
\begin{remark}
Computing these cross-convolution products in the $(u,v)$ plane one immediately notes that  $m_i * K_j$ with $i>j$ are negligible. In fact, let us consider the Fourier Transforms $\widehat{m}_i(u,v)$ and $\widehat{K}_j(u,v)$ of $m_i(x,y)$ and $K_j(x,y)$ respectively; since the support of $\widehat{K}_j(u,v)$ is given by the set $\{u_l^{(j)},v_l^{(j)}\}_{l=1}^{N_j}$ of points sampled by the $j$-th set of detectors then if $i>j$ (i.e., the resolution of the basis function is lower then the resolution of the PSF) such points are outside the support of $\widehat{m}_i(u,v)$ and  $\widehat{m}_i(u,v)\cdot\widehat{K}_j(u,v)=0$. 
\end{remark}
\vspace{0.25 cm}

The strategy is therefore to identify small scale sources first and then to subtract their contribution to the whole set of dirty maps, provided that the identification of smaller scales requires a pre-processing step in which a bias towards these scales is applied.

%
%This implies that for the $j$-th dirty map the convolution products with all the lower resolution basis functions (i.e., $m_i * K_j$ with $i>j$) are negligible. The strategy is then to identify small scale sources first and then to subtract their contribution to the whole set of dirty maps, provided that the identification of smaller scales requires a pre-processing step in which a bias towards these scales is applied.


%Although the selection of $m_{\overline{j}}(x,y)$ in the Maximum Identification step (and, consequently, in equation (\ref{multi-step-2}) in the following CLEAN Components Update step) is a mathematical assumption, it has a motivation based on the interplaying properties of the basis functions and the PSF components. In fact equations (\ref{final}) and (\ref{multi-step-3}) show that each dirty map (included the updated ones) contains cross-convolution products between PSF components and basis functions. For small $t$, the scale bias factor emphasizes the role of $K_1(x,y)$, which is the highest resolution PSF component. 
%
%
%This implies that the convolution product with all the low resolution basis functions is negligible and just the component $(K_1 * m_1)(x,y)$ provides a significant contribution to the updated dirty maps. For higher $t$ values, the maximum is localized in (rescaled) dirty map components $(I^D_j)^{(t)}(x,y)$ characterized by $j>1$. However, all the components containing $m_i * K_j$ with $i<j$ have been eliminated by the Dirty Map Update step, while the components with $i>j$ are again negligible because the resolution of $K_j$ is higher than the resolution of $m_i$.

Starting from the initialization  
\begin{equation}\label{step-0-multi}
I_j^{(0)}(x,y) := I_j^D(x,y) ~~~~~~~~~~
M_j:=\max_{(x,y)} (I_j^{(0)}(x,y))
\end{equation}
where $j=1,\ldots,N$ and estimates for $I_j^D(x,y)$ can be obtained by using equation (\ref{dirty-multi-1}), the multi-scale CLEAN algorithm can be summarized as the following loop: 
\vspace{0.25cm}

\begin{description}
\item{{\bf{Rescaling.}}} At each iteration $t\ge 1$, all dirty maps are re-scaled in such a way that the re-scaled dirty map components 
${\mathcal{I}}_j^{(t)}$, $j=1,\ldots,N$ become 
\begin{equation}\label{rescaling}
{\mathcal{I}}_j^{(t)}(x,y) = \eta_j \frac{I_j^{(t-1)}(x,y)}{M_j}
\end{equation}
where $\eta_j$, $j=1,\ldots,N$ are scale bias factors decreasing as $j$ increases. In this way, small scales will be favored at the first iterations. This rescaling is just a technical fact and re-scaled maps are utilized just in the maximum search process.
\vspace{0.2cm}
\item{{\bf{Scale Identification}}} In this step we determine the ${\overline{j}}$-th re-scaled dirty map that presents the maximum intensity value (at $t=1$ we will have ${\overline{j}}=1$)
\begin{equation}\label{multi-step-0}
\overline{j} = \arg\max_{j} {\mathcal{I}}_j^{(t)}(x,y),
\end{equation}
and we select the corresponding basis function $m_{\overline{j}}(x,y)$. 
\vspace{0.2cm}
\item{{\bf{Maximum Identification.}}} 
This optimization step provides the maximum and its position in the ${\overline{j}}$-th dirty image, i.e. it computes
\begin{equation}\label{multi-step-1}
(x_{max}^{(t)},y^{(t)}_{max}) = \arg\max_{(x,y)} I_{\overline{j}}^{(t-1)}(x,y)~~~~~~~~~~~I^{(t)}_{max} = I_{\overline{j}}^{(t-1)}(x_{max}^{(t)},y_{max}^{(t)}).
\end{equation}

\item{{\bf{CLEAN Components Update.}}} 
%We now use the identification
%\begin{equation}\label{algorithm-1}
%I_{{\overline{j}}}^D(x,y) = \sum_{i=1}^N \sum_{q_i=1}^{Q_i} I_{q_i}  (m_i * K_{{\overline{j}}})(x-x_{q_i},y-y_{q_i}),
%\end{equation}
%which states that this dirty map is the superposition of terms including all possible basis function. However, the convolution products at the right hand side of (\ref{algorithm-1}) are all negligible with respect to $m_{{\overline{j}}} * K_{{\overline{j}}}$.
The multi-scale CLEAN components map at the $t$-th iteration is
\begin{equation}\label{multi-step-2}
CC^{(t)} (x,y) = CC^{(t-1)}(x,y) + \frac{ \gamma I^{(t)}_{max}}  {\max\limits_{ (x,y) }   |(m_{\overline{j}} * K_{\overline{j}})(x,y)|}m_{\overline{j}}(x-x_{max}^{(t)},y-y_{max}^{(t)}),
\end{equation}
where, again, $CC^{(0)}$ has all elements equal to zero and $\gamma$ is the {\em{gain factor}}.
\vspace{0.2cm}

\item{{\bf{Dirty Map Update.}}} At each scale $j=1,\ldots,N$, the dirty map component is then updated in such a way that
\begin{equation}\label{multi-step-3}
I_j^{(t)}(x,y) = I_j^{(t-1)}(x,y) - \frac{\gamma I^{(t)}_{max}}{\max\limits_{(x,y)} |(m_{\overline{j}} * K_{\overline{j}})(x,y)|} (m_{\overline{j}}*K_j)(x-x_{max}^{(t)},y-y_{max}^{(t)}).
\end{equation}
%where $(I_j^D)^{(0)}(x,y) = I_j^D(x,y)$.
\item{{\bf{Update.}}} $t=t+1$ and then go back to the first step until a stopping rule is verified.

\end{description}
\vspace{0.2cm}

The stopping rule for the loop applies when there are no more sources in all updated dirty map components, which occurs when these maps are embedded by the corresponding background maps. 
Therefore, if $t={\overline{t}}$ is the iteration when this happens, then the residual map is defined as
\begin{equation}\label{multi-final-residual}
R(x,y) := \sum_{j=1}^N I_j^{(\overline{t})}(x,y) \simeq \sum_{j=1}^N (K_j *B)(x,y) =(K*B)(x,y),
\end{equation}
which inspires the usual approximation for the background map
\begin{equation}\label{multi-background}
B(x,y) \simeq B^{\prime}(x,y):= \frac{R(x,y)}{T},
\end{equation}
where, as in the CLEAN algorithm, $T$ is an estimate of the integral of the full PSF over the Field of View, i.e. $T = \sum_{n}^{N_{pix}} K(x_n,y_n) \delta x \delta y$, being $(x_n,y_n)$ the $n$-th pixel ($n=1,\dots,N_{pix}$) in the full PSF image and $\delta x$, $\delta y$ the pixel dimensions.

In the final step of multi-scale CLEAN algorithm, no convolution with any CLEAN beam is needed and one simply constructs the CLEANed map $C(x,y)$ as 
\begin{equation}\label{multiCLEAN-final}
C(x,y) = CC^{({\overline{t}})}(x,y)+B^{\prime}(x,y).
\end{equation}


\vspace{0.25 cm}
\begin{remark}
The definition of the scale bias $\eta_j$ is crucial in order to emphasize smaller scales at first iterations. In \cite{cornwell2008multiscale}, the author suggests a linear relation assigning decreasing weights to each scale as they increase. Here we use a relation reflecting the geometric progression of the radii of the circles sampling the $(u,v)$ plane. In particular, if $r$ is the ratio of the geometric progression ($r=\frac{1}{\sqrt{3}}$ for RHESSI) 
and the $(u,v)$ points are
grouped in $N$ sets in such way that the $j$-th set contains the $N_j$ sampling points laying on $C_j$ circles whose radii sorted in decreasing order are $\{ R_{h^j_1}, R_{h^j_2}, \dots , R_{h^j_{C_j}}\} \subset \{ R_1, R_2, ... , R_C \} $ ($C=9$ for RHESSI), 
we have defined:
\begin{equation}\label{eq:eta}
\eta_j:= r^{h_1^{j}-1} ~~~~~~~~ j=1,\dots,N.
\end{equation}
In the RHESSI case, for example, the sampled visibilities can be grouped in $N=3$ sets, where the first set may contain visibilities belonging to $C_1=3$ circles, the second one may contain visibilities from $C_2=3$ circles and the last one from $C_3=3$ circles. This means that the radii of the circles drawn by the visibilities of the first set are $\{ R_1, R_2, R_3 \}$ (i.e., $h^1_1=1$,  $h^1_2=2$, $h^1_3=3$), the ones of the second set are  $\{ R_4, R_5,R_6 \}$ (i.e., $h^2_1=4$, $h^2_2=5$ and $h^2_3=6$) and finally the ones of the last set are $\{ R_7,R_8,R_9 \}$ (i.e., $h^3_1=7$, $h^3_2=8$, and $h^3_3=9$). According to (\ref{eq:eta}), just  $h^1_1$,  $h^2_1$ and $h^3_1$ are used for the computation of the three scale bias $\eta_j$, $j=1,2,3$, leading to $\eta_1=1$, $\eta_2=0.19$ and $\eta_3=0.04$.
\end{remark}

\vspace{0.25 cm}
\begin{remark}
As in the CLEAN algorithm, the gain factor controls the fraction of each point source to be added in the clean components map and subtracted from the dirty maps. Typical values are in the range $[0.05,0.5]$.
\end{remark}
\vspace{0.25 cm}

\vspace{0.25 cm}
\begin{remark}
This multi-scale version of CLEAN differs from the one introduced in \cite{cornwell2008multiscale} for radio-imaging contexts at both conceptual and technical levels. Conceptually, here we fully exploit the fact that the PSF of hard X-ray telescopes can be decomposed as the sum of several components. Each one of these components can be used to compute the cross-convolution product with the basis functions, without the need to approximate it as the convolution between each basis function and the global PSF as done in \cite{cornwell2008multiscale}. From a technical viewpoint, here we use two-dimensional Gaussian functions (and not two-dimensional paraboloids) as basis functions, which is better appropriate to the fact that in hard X-ray imaging the flaring sources do not have a compact support. Further, the scale bias is computed in a completely different way and, also, our scheme does not involve a final convolution step with the CLEAN beam as in the Cornwell's algorithm. Finally, we remark that this is the first time that a multi-scale CLEAN algorithm is applied to hard X-ray visibilities using both synthetic and experimental data.
\end{remark}
\vspace{0.25 cm}

%
%Given equations (\ref{psf-component}) and (\ref{source-multi}) the dirty map in a multi-scale framework is now
%\begin{equation}\label{dirty-image-multi}
%I^D(x,y) = \sum_{j=1}^N I_j^D(x,y),
%\end{equation}
%where
%\begin{equation}\label{dirty-component}
%I_j^D(x,y) = \sum_{i=1}^N\sum_{q_i=1}^{Q_i}I_{q_i}\left(m_i \ast Beam^D_j \right)(x-x_{q_i},y-y_{q_i}) \quad \forall j=1,\dots,N.
%\end{equation}

%We now consider the multi-scale modifications of the CLEAN algorithm described in the previous section. At the first step, maximum identification now involves all the $N$ dirty maps, i.e. at each iteration we need to compute
%\begin{equation}\label{maximum-multi}
%(x_j,y_j) = \arg\max_x I^D_j(x,y)~~~~~I^D_j (x_j,y_j)~~~~~j=1,\ldots,N.
%\end{equation}
%Also the CLEAN component update step now must account for the multi-scale nature of the problem. In fact we consider the set $(M_1,\ldots,M_N)$ of maxima determined in (\ref{maximum-multi}) and choose the index $l$ such that 
%\begin{equation}\label{update-multi-1}
%M_l = \max (M_1,\ldots,M_N).
%\end{equation}
%Denoting with $q_l$ the source with scale $l$ and position $(x_{q_l},y_{q_l})$, the CLEAN component update step becomes, at each iteration
%\begin{equation}\label{update-multi-2}
%CC(x,y) = CC(x,y) + \frac{\gamma M_l}{\max(m_l \ast K_l)}m_l(x-x_{q_l},y-y_{q_l})
%\end{equation}
%with $\gamma$ a new scaling factor. The final subtraction step involves now all dirty maps and must account for the scale of the subtracted CLEAN component. Therefore we have
%\begin{equation}\label{subtraction}
%I_j^D(x,y) = I_j^D(x,y) - \frac{\gamma M_l}{\max(m_l\ast K_l)}\left(m_l \ast K_l\right)(x-x_{\bar{q_l}},y-y_{\bar{q_l}})\quad \forall j=1,\dots,N.
%\end{equation}
%Implementation: $\gamma$; stopping rule; background


\section{Applications}
The validation of the multi-scale CLEAN reconstruction me\-thod performed in this section is concerned with the RHESSI imaging problem. This NASA instrument was operating until October 2018 and it is still the solar hard X-ray telescope with the best performances in terms of both spatial and spectral resolution. As said, the way its nine Rotating Modulation Collimators sampled the $(u,v)$ plane is described in Figure \ref{fig:uv-samples}, left panel, i.e. this telescope provided visibilities placed on nine circles with increasing radii in the spatial frequency plane. This implies that the RHESSI PSF can be written as the sum of up to nine PSF components, each one numerically computed as the inverse Fourier transform of the characteristic function of the set of $(u,v)$ points laying on each one of the sampled circles. As an example, in this paper the global PSF is decomposed as the sum of three PSF components (see Figure \ref{fig:PSFs-scale}, top row), the first one associated to the three circles with biggest radii, the second one associated to the three circles with intermedium radii, and the third one associated to the three circles with smallest radii. Correspondingly, we will consider three basis functions $m_1, m_2, m_3$ which are two-dimensional Gaussian functions whose standard deviations are determined by the utilized PSF components: specifically, we have computed the Full Width at Half Maximum (FWHM) of the central peak of each PSF component, we have exploited the relationship between FWHM and standard deviation for a Gaussian function ($FWHM=2\sqrt{2\ln 2} \sigma$), and used this standard deviation as the standard deviation of the corresponding scale (see Figure \ref{fig:PSFs-scale}, bottom row). All information characterizing this choice of the global PSF decomposition are contained in Table \ref{table:parameters}. Further, Figure \ref{fig:cross-conv} contains the nine cross-convolution products between the PSF components and the basis functions corresponding to the choices made in Figure \ref{fig:PSFs-scale}. As a numerical confirmation of Remark 3.3, the first and second colums of this figure contain two and one negligible cross-convolution products, respectively.


\begin{table}[h]
\begin{center}
\begin{tabular}{cccccc}
\hline\hline
 & detectors & $R_{max}$(arcsec$^{-1}$) & $\eta$ & resolution (arcsec) & FWHM (arcsec) \\
 \hline
 set 1 & $1,2,3$ & $0.221$ & $1$ & 2.26 & 2.8 \\
 \hline
 set 2 & $4,5,6$ & $0.0427$ & $0.19$ & 11.76 & 15.2\\
 \hline
 set 3 & $7,8,9$ & $0.029$ & $0.04$ & 61.08 & 76.8\\
 \hline\hline
\end{tabular}
\caption{Parameters describing the decomposition of the global RHESSI PSF for the experiment with synthetic data: 'detectors' denotes the circles composing each group of visibilities; $R_{max}$ is the radius of the circle with maximum radius in each group; $\eta$ is the scale bias factor; 'resolution' is the corresponding nominal spatial resolution as in \cite{hurford2003rhessi}, and FWHM is the Full Width at Half Maximum of the corresponding basis functions.}\label{table:parameters}
\end{center}
\end{table}


%\begin{figure}[h]
%\begin{center}
%\includegraphics[width=13.cm]{pannello_pixon_psf2} 
%\caption{The computational tools of multi-scale CLEAN. Top row: the three PSF components, obtained by using eq.~\eqref{psf-natural} with $N=3$ and sets of $(u,v)$ samples uniformly spaced along each circle. Bottom row: the three basis functions.}
%\label{fig:PSFs-scale}
%\end{center}
%\end{figure}

\begin{figure}[h]
\begin{center}
\includegraphics[width=7.6cm, angle=90]
{img_av/psf_basis_2.jpg} 
%{img_av/basis_psf.pdf} 
\caption{The computational tools of multi-scale CLEAN. Top row: the three PSF components, obtained by using Equation \eqref{psf-natural} with $N=3$ and according to the grouping of detectors as in Table \ref{table:parameters}. Bottom row: the three corresponding basis functions.}
\label{fig:PSFs-scale}
\end{center}
\end{figure}


%\begin{figure}[h]
%\begin{center}
%\includegraphics[width=13.cm]{pannello_cross_stessa_colorbar} 
%\caption{Cross-convolution products between PSF components and basis functions. In the panel, row $i$--column $j$ ($i,j=1,2,3$) contains cross-convolution $m_i*K_j$. In the first column ($j=1$) only the first image ($i=1$) is significantly different from zero, while in the second column ($j=2$) just the contribution of $i=3$ can be neglected.}
%\label{fig:cross-conv}
%\end{center}
%\end{figure}

\begin{figure}[h]
\begin{center}
\includegraphics[width=11.cm,angle=90]{img_av/cross_convolution3.pdf} 
\caption{Cross-convolution products between PSF components and basis functions. In the panel, row $i$--column $j$ ($i,j=1,2,3$) contains cross-convolution $m_i*K_j$. In the first column ($j=1$) only the first image ($i=1$) is significantly different from zero, while in the second column ($j=2$) just the contribution of $i=3$ can be neglected.}
\label{fig:cross-conv}
\end{center}
\end{figure}

\subsection{Validation against simulated visibilities}
In order to study the applicability conditions of our multi-scale CLEAN we performed a validation test based on the following two steps:
\begin{enumerate}
\item We invented a source configuration inspired by a specific real event observed by RHESSI, namely the one described in Figure \ref{fig:simulation-1}, occurred on December, 2 2003 in the time interval 22:54:00 - 22:58:00 UT, in the energy range 16-18 keV and showing an extended plus a compact source configuration. In the left panel of Figure \ref{fig:simulation-1}, this source is reconstructed from the RHESSI experimental visibilities by using a constrained maximum entropy algorithm \cite{massa2020mem_ge}. The right panel contains the synthetic source inspired by this reconstruction.
\item We then generated three synthetic 
data sets. Operationally, the Fourier transform has been applied three times from the flux distribution in the image plane to the set of visibilities sampled by RHESSI in the visibility space. Each time we considered a different level of statistics, i.e., a total flux per map equal to 1000, 10000, and 100000 photons cm$^{-2}$ sec$^{-1}$ arcsec$^{-2}$, respectively.
\item We randomly perturbed each visibility set $20$ times, by using a Gaussian distribution with constant standard deviation.
\end{enumerate}

For each level of statistics, we applied the CLEAN and multi-scale CLEAN algorithms on the $20$ random realizations of the visibility set, and provided one representative reconstruction provided by each algorithm in Figure \ref{fig:simulation-2}. Further, Table \ref{multivsclean} contains the average values and standard deviations of the reconstructed imaging parameters. These results show, first, that multi-scale CLEAN is more robust than CLEAN with respect to the level of the noise affecting the input visibilities. Further, both algorithms perform well in identifying the location of both the extended and the compact source. On the other hand, multi-scale CLEAN outperforms standard CLEAN as far as photometry is concerned, since both the flux ratio and the peak ratio characterizing the reconstructions provided by the multi-scale approach are significantly closer to the ones characterizing the ground-truth. Finally, from an iconographical perspective, the reconstructions obtained by CLEAN seem to be significantly over-regularized, the ones provided by the multi-scale algorithm being able to better distinguish the compact source from the extended background halo.
%These reconstructions show that the use of different scales allows the simultaneous recovery of both the extended and the compact sources, while standard, single-scale CLEAN fails in reconstructing the compact source. The outcome of this analysis is confirmed by Table \ref{multivsclean}, which compares some imaging parameters characterizing the reconstructions provided by CLEAN and multi-scale CLEAN to the ground-truth parameters for the three statistics levels.

\begin{figure}[h]
\begin{center}
\includegraphics[width=10cm]{img_av/groundtruth_memge02dec_2.png} 
%{img_av/groundtruth_memge.pdf}
\caption{Validation of multi-scale CLEAN against simulated {\em{RHESSI}} data: the synthetic configuration, inspired by the December 2 2003 event in the time interval 22:54:00 - 22:58:00 UT. Left panel: the real source reconstructed by means of MEM$\_$GE \cite{massa2020mem_ge}. Right panel: the corresponding synthetic ground-truth used in the simulation experiment.}
\label{fig:simulation-1}
\end{center}
\end{figure}


\begin{table}[h]
\begin{center}
\resizebox{\columnwidth}{!}{
\begin{tabular}{cccccccc}
\hline\hline
& \multicolumn{2}{c}{First peak}  &  & \multicolumn{2}{c}{Second peak}  & Peaks ratio & Fluxes ratio\\
\cline{2-3}
\cline{5-6}
&  x & y & & x & y &  & \\
& (arcsec) & (arcsec) & & (arcsec) & (arcsec)  & & \\
\hline
\multicolumn{8}{c}{HIGH STATISTICS} \\
\hline

Ground truth &  914.0 & -341.0 & & 918.6 & -334.4 & 2.29 & 0.19\\
Multi-scale Clean &  914.0 $\pm$ 0.0 & -341.0 $\pm$ 0.0 & & 918.4 $\pm$ 0.0 & -334.6 $\pm$ 0.0 & 2.59 $\pm$ 0.01 & 0.18 $\pm$ 0.01\\
Clean        &  914.6 $\pm$ 0.0 & -340.2 $\pm$ 0.0 & & 915.2 $\pm$ 0.0 & -337.8 $\pm$ 0.0 & 1.15 $\pm$ 0.01 & 0.14 $\pm$ 0.00\\
\hline
\multicolumn{8}{c}{MEDIUM STATISTICS} \\
\hline
Ground truth & 914.0 & -341.0 & & 918.6 & -334.4 & 2.29 & 0.19\\
Multi-scale Clean & 914.0 $\pm$  0.0 & -341.0 $\pm$  0.0 & & 918.4 $\pm$ 0.0 & -334.8 $\pm$ 0.2 & 2.54 $\pm$ 0.04 & 0.18 $\pm$ 0.03\\
Clean        & 914.5 $\pm$ 0.2 & -340.3 $\pm$ 0.2 & & 915.2 $\pm$ 0.1 & -337.8 $\pm$ 0.0 & 1.14 $\pm$ 0.01& 0.14 $\pm$ 0.01\\
\hline
\multicolumn{8}{c}{LOW STATISTICS} \\
\hline
Ground truth & 914.0 & -341.0 & & 918.6 & -334.4 & 2.29 & 0.19\\
Multi-scale Clean  & 914.2 $\pm$ 0.4 & -340.9 $\pm$ 0.5 & & 916.5 $\pm$ 1.6 & -336.7 $\pm$ 1.5 & 1.60 $\pm$ 0.34 & 0.15 $\pm$ 0.04\\
Clean        & 914.7 $\pm$ 0.2 & -340.3 $\pm$ 0.3 & & 915.4 $ \pm$ 1.3 & -337.4 $ \pm$ 1.4 & 1.14 $ \pm$ 0.08 & 0.13 $ \pm$ 0.01 \\
\hline\hline
\end{tabular}}
\caption{The parameters reconstructed by CLEAN and multi-scale CLEAN are compared to the ones associated to the ground truth images. The compact source is referred to as first source, while the elongated source is referred to as second source.}\label{multivsclean}
\end{center}
\end{table}


%\begin{table}[h]
%\begin{center}
%\begin{tabular}{cccccccc}
%\hline\hline
%& \multicolumn{2}{c}{First peak}  &  & \multicolumn{2}{c}{Second peak}  & Peaks ratio & Fluxes ratio\\
%\cline{2-3}
%\cline{5-6}
%&  x & y & & x & y &  & \\
%& (arcsec) & (arcsec) & & (arcsec) & (arcsec)  & & \\
%\hline
%\multicolumn{7}{c}{HIGH STATISTICS} \\
%\hline
%Ground truth &  914.0 & -341.0 & & 918.6 & -334.4 & 2.29 & 0.19\\
%Multi-scale Clean &  914.0 & -341.0 & & 918.4 & -334.6 & 2.58 & 0.18\\
%Clean        &  915.6 & -339.0 & & 916.0 & -337.8 & 1.01 & 0.12\\
%\hline
%\multicolumn{6}{c}{MEDIUM STATISTICS} \\
%\hline
%Ground truth & 914.0 & -341.0 & & 918.6 & -334.4 & 2.29 & 0.19\\
%Multi-scale Clean & 914.0 & -341.0 & & 918.4 & -334.6 & 2.58 & 0.18\\
%Clean        & 915.6 & -339.0 & & 916.0 & -337.8 & 1.01 & 0.12\\
%\hline
%\multicolumn{6}{c}{LOW STATISTICS} \\
%\hline
%Ground truth & 914.0 & -341.0 & & 918.6 & -334.4 & 2.29 & 0.19\\
%Multi-scale Clean  & 914.0 & -341.0 & & 918.4 & -334.6 & 2.58 & 0.18\\
%Clean        & 915.8 & -338.8 & & 916.0 & -337.8 & 1.01 & 0.12\\
%\hline\hline
%\end{tabular}
%\caption{\textbf{The parameters reconstructed by MULTI-SCALE CLEAN are compared with the ground truth and with the values provided by CLEAN. The compact source is referred to as first source, while the elongated source is referred to as second source.}}\label{multivsclean}
%\end{center}
%\end{table}

%\begin{figure}[h]
%\begin{center}
%\includegraphics[width=5cm]{simulation-1} 
%\caption{Validation of multi-scale CLEAN against simulated {\em{RHESSI}} data: the synthetic configuration, inspired by the December 2 2003 event in the time interval 22:54:00 - 22:58:00 UT.}
%\label{fig:simulation-1}
%\end{center}
%\end{figure}

%\begin{figure}[h]
%\begin{center}
%\includegraphics[width=12.cm]{simulation-2} 
%\caption{CLEAN vs multi-scale CLEAN for the reconstruction of the synthetic hard X-ray source represented in Figure \ref{fig:simulation-1}, right panel. Top row: reconstructions obtained by standard CLEAN in the case of low, medium and high statistics. Bottom row: reconstructions obtained by multi-scale CLEAN in the case of low, medium and high statistics. }
%\label{fig:simulation-2}
%\end{center}
%\end{figure}

%\begin{figure}[h]
%\begin{center}
%\includegraphics[width=8.cm, angle =90]
%{img_av/cleanVSmultiscale_2.jpg} 
%{img_av/clean_multiscale.pdf} 
%\caption{CLEAN vs multi-scale CLEAN for the reconstruction of the synthetic hard X-ray source represented in Figure \ref{fig:simulation-1}, right panel. Top row: reconstructions obtained by standard CLEAN in the case of low, medium and high statistics. Bottom row: reconstructions obtained by multi-scale CLEAN in the case of low, medium and high statistics. }
%\label{fig:simulation-2}
%\end{center}
%\end{figure}



\begin{figure}[h]
\begin{center}
\includegraphics[width=12.5cm]
{img_av/multiscaleVsclean_pert.png} 
%{img_av/clean_multiscale.pdf} 
\caption{CLEAN vs multi-scale CLEAN for the reconstruction of the synthetic hard X-ray source represented in Figure \ref{fig:simulation-1}, right panel. Top row: reconstructions obtained by standard CLEAN in the case of low, medium and high statistics. Bottom row: reconstructions obtained by multi-scale CLEAN in the case of low, medium and high statistics. }
\label{fig:simulation-2}
\end{center}
\end{figure}

\subsection{RHESSI experimental visibilities}

The standard model for solar flares \cite{tandberg1988physics,svestka2012solar} assumes that, at hard X-ray energies, a typical event is characterized by a low-energy extended source in the corona produced by thermal mechanisms, plus two high-energy compact sources down in the solar chromosphere produced by collisional bremsstrahlung \cite{warren2004thermal,prato2006regularized,saint2005thermal,hudson2011global}. However, on July 19 2012, RHESSI observed an M-class flare that, for the whole duration of the hard X-ray emission, was characterized by another coronal high-energy source over the thermal coronal loop-top. The analysis of this surprising observation has been performed in several papers \cite{oka2015electron,sun2014differential,gritsyk2016x,yuan2019compact,krucker2013particle,liu2013plasmoid}. In particular, the standard CLEAN algorithm used in \cite{liu2013plasmoid} was able to identify this high-energy coronal source, but not to clearly separate it from the close thermal emission. A better separation was obtained in \cite{krucker2013particle} by using standard CLEAN twice: first, using just the high-resolution detectors, and then using the coarser detectors after appropriate image subtraction. We applied our multi-scale CLEAN to RHESSI data corresponding to this event in two energy channels, ($6-8$ keV and $30-80$ keV). In this case we used all RHESSI detectors that were operational at that time (i.e., detectors from 3 through 9) according to two different grouping for the two energy channels: in the 6-8 keV channel, set 1 was made of detectors 3 and 4, set 2 of detectors 5 and 6, set 3 of detectors 7, 8, and 9; in the 30-80 keV channel, set 1 was made of detectors 3 and 4, set 2 of detectors 5, 6, and 7, set 3 of detectors 8 and 9. 

Figure \ref{fig:july-av} shows the behavior of the multi-scale algorithm in the two energy ranges. In the low-energy case (left panel), where just one scale is present, the method is capable of adapting to this single scale, although it is set in such a way to seek for three different scales (in other terms, multi-scale CLEAN nicely behaves as CLEAN when it has to reconstruct one scale). In the high energy scale (right panel), the algorithm is perfectly able to distinguish the coronal source from the two footpoints in one step. Further, it nicely reproduces the asymmetry of the two chromospheric footpoints, which is due to the fact that part of the emission from the flare ribbon is occulted by the solar disk, while the spatial resolution obtained for these compact sources is better with respect to standard CLEAN. 

\begin{figure}
%\begin{tabular}{cc}
%\includegraphics[width=5.7cm]{img_av/19july-1.jpg} &
%\includegraphics[width=6.cm]{img_av/19july-2.jpg}
%\end{tabular}
\includegraphics[width=12.7cm]{img_av/19Jul2012_1.png}
\caption{The X-ray emission recorded by RHESSI on July, 19 2012 in the time interval from 05:21:00 to 05:24:00 UT. Left panel: the thermal coronal emission in the energy range between 6 and 8 keV (55, 65, 70, and 90$\%$ green contour levels). Right panel: the chromospheric footpoints and the high-energy coronal source in the energy range between 30 and 80 keV (15, 20, 30, 50, 70, and 90$\%$ blue contour levels for the foot-points; 2, 2.5, 3, and 5 blue contour levels for the coronal source). The hard X-ray emissions are superimposed to the 193 \AA{} extreme ultra-violet map recorded by the Atmospheric Imaging Assembly on-board the Solar Dynamics Obervatory (SDO/AIA).}\label{fig:july-av}
\end{figure}



%\begin{figure}[h]\label{fig:july av}
%\begin{center}
%\begin{tabular}{cc}
%  \includegraphics[width=12.cm]{img_av/19july-1.jpg} &
%  \includegraphics[width=12.cm]{img_av/19july-2.jpg}
%  \end{tabular}
%  \end{center}
%  \caption{The X-ray emission recorded by RHESSI on July, 19 2012 in the time interval from 05:21:00 to 05:24:00 UT. Left column: reconstructions provided by multi-scale CLEAN at the energy range 6-8 keV (top panel) and 30-80 keV (bottom panel),  overlaid on the 193 \AA{} AIA map. The 55, 65, 70 and 90$\%$ contour levels of the reconstructed thermal X-ray emissions are plotted in green. The 15, 20, 30, 50, 70 and 90$\%$ contour levels of the reconstructed non-thermal X-ray emissions and 2, 2.5, 3, 5$\%$ contour levels of the extended coronal source are plotted in blue.} Right column: the EUV emission provided by AIA with superimposed the level curves provided by CLEAN using the same RHESSI experimental visibilities.}
%\end{figure}


%We then considered the set of experimental visibilities recorded by {\em{RHESSI}} during the July, 23 2002 flaring event. This flare is one of the largest event detected by this mission since its launch (it is an X4.8 class flare in the GOES classification) and, at hard X-ray wavelengths, it presents four main features: a bright, low energy source high in the corona, two localized high-energy footprints and a fourth bright feature near the top of a magnetic structure connecting the other two. The reconstructions in Figure \ref{fig:real} compare the performances of CLEAN and multi-scale CLEAN while processing three visibility sets associated to this event and corresponding to three energy ranges. In this case multi-scale CLEAN reaches a significantly higher resolution power and is systematically able to identity the multi-feature structure of this configuration.

%We have validated the reliability of multi-scale CLEAN for the reconstruction of flaring sources from experimental visibilities in the case of two events already studied in two papers published in 2013 and 2014, respectively. For obtaining their reconstructions, both those studies applied the CLEAN algorithm twice \cite{kretal11}: the more compact sources have been produced by using just the high-resolution RHESSI collimators; these components have been subtracted from the components produced by the low-resolution collimators, and then the CLEAN scheme has been allowed to finish its iterations.

%\subsubsection{The November 3, 2010 event}
%On november 3, 2010 RHESSI observed an impulsive flaring phase between 12:13:00 and 12:15:00 Universal Time (UT), which was gradually followed by a thermal, lower energy emission. This same event was observed by an EUV instrument, the Atmospheric Imaging Assembly (AIA) on-board the NASA Solar Dynamic Observatory. We applied our multi-scale CLEAN algorithm to the visibilities recorded by RHESSI at three time intervals with 30 sec duration, in the energy range between 18 and 40 keV. In Figure \ref{fig:november} we compared the results with the ones published by \cite{gletal13} for the same energy range and in similar time intervals. The level curves in the reconstructions clearly show that multi-scale CLEAN is able to identify, in one single shot, both the intense compact source at the solar limb, and the fainter, more extended emission at higher altitudes. Further, this latter source seems better resolved and more in accordance with the EUV emission with respect to the reconstructions obtained by CLEAN in its two-step application. 

%\subsubsection{The July 19, 2012 event}
%This event generated one of the most spectacular flares observed by RHESSI during its whole life time. Indeed, its complex topography was characterized by a thermal loop in the corona, two compact foot-points down into the chromosphere, and a seldom observed above-the-loop high-energy source. As done by \cite{kretal14}, we separately analyzed the visibilities recorded in the energy ranges 6-8 keV and 30-80 keV in the same time interval, i.e. from 05:21:00 to 05:24:00 UT. Once again, as shown in Figure \ref{fig:july} in one shot multi-scale CLEAN was able to reconstruct the whole complex topography with a space resolution higher than the one provided, in a two-step running, by standard CLEAN.

%\begin{figure}[h]\label{fig:november}
%\begin{center}
%  \includegraphics[width=14.cm]{november} 
%  \caption{The X-ray emission recorded by RHESSI on November, 3 2010 in the energy range between 18 keV and 40 keV at three consecutive time intervals from 12:13:00 UT and 12:14:40 UT. Left column: reconstructions provided by multi-scale CLEAN together with the corresponding level curves associated to the hard X-ray emission. Right column: hard X-ray level curves provided by CLEAN and superimposed on high resolution EUV images obtained by AIA [from \cite{gletal13})]. }
%\end{center}
%\end{figure}

%\begin{figure}[h]\label{fig: nov av}
%\begin{center}
%  \includegraphics[width=13.cm]{img_av/3nov2010.png} 
%  \caption{The X-ray emission recorded by RHESSI on July, 19 2012 in the time interval from 05:21:00 to 05:24:00 UT. Left column: reconstructions provided by multi-scale CLEAN at the energy range 6-8 keV (top panel) and 30-80 keV (bottom panel). Right column: the EUV emission provided by AIA with superimposed the level curves provided by CLEAN using the same RHESSI experimental visibilities [from \cite{kretal14})]. }
%\end{center}
%\end{figure}



%\begin{figure}[h]\label{fig:july}
%\begin{center}
%  \includegraphics[width=12.cm]{july.jpg} 
%  \caption{The X-ray emission recorded by RHESSI on July, 19 2012 in the time interval from 05:21:00 to 05:24:00 UT. Left column: reconstructions provided by multi-scale CLEAN at the energy range 6-8 keV (top panel) and 30-80 keV (bottom panel). Right column: the EUV emission provided by AIA with superimposed the level curves provided by CLEAN using the same RHESSI experimental visibilities [from \cite{kretal14})]. }
%\end{center}
%\end{figure}


%\begin{figure}[h]
%\begin{center}
%\includegraphics[width=12.cm]{real} 
%\caption{Application of multi-scale CLEAN against visibility sets recorded during the July 23 2003 flares. First row: CLEAN and multi-scale CLEAN reconstructions corresponding to the energy range $28-32$ keV. Second row: CLEAN and multi-scale CLEAN reconstructions corresponding to the energy range $32-36$ keV. Third row: CLEAN and multi-scale CLEAN reconstructions corresponding to the energy range $36-41$ keV.}
%\label{fig:real}
%\end{center}
%\end{figure}

\section{Conclusions}
Hard X-ray imaging is a crucial Fourier-based modality for investigating the physics of high-energy emissions from the Sun and in the last decades several missions have been realized and conceived to launch these kind of telescopes in space. From the image processing perspective, a large amount of scientific literature in this field utilizes CLEAN to deconvolve the effects of the instrumental PSF. In this paper we introduce a genuinely multi-scale version of the iterative CLEAN algorithm, in which the input data are sets of photon visibilities (i.e. spatial Fourier components of the emission distribution), the cleaning of the image is performed in parallel for all the dirty maps components, the scale bias factor is determined by accounting for the peculiar way visibilities are recorded by the instrument, and the background is added to the final image in a very natural way.  We have validated the method against some sets of realistically simulated visibilities and proved its effectiveness in the case of experimental observation provided by RHESSI. 

The accuracy of this multi-scale CLEAN depends on the way the dirty map components are computed, i.e., on the way scales are identified and detectors are grouped. Therefore, it would be important to implement an iterative scheme for the automated identification of the optimal grouping of the detectors. Further, this general approach to multi-scale deconvolution can be probably extended to other deconvolution methods, like Pixon, that are frequently used in hard X-ray imaging, and to other imaging problems in which the global PSF can be modelled as the sum of a finite number of components.

\thanks{MP and AMM are and will always be grateful to Richard A. Schwartz who first inspired this research activity on multi-scale algorithms for hard X-ray imaging. Richard passed away in 2019 and this paper is devoted to his memory. MP and AMM also thanks Sara Giordano for her preliminary work on this topic made during her PhD period at the Dipartimento di Matematica, Universit di Genova.}

%\thanks{This activity has been supported by the European Community Framework
%Programme 7, ``High Energy Solar Physics Data in Europe (HESPE)",
%grant agreement no.: 263086. We  thank our colleagues E. Carletti and L. Robbiano for many helpful discussions. We also thank our student
% C. Della Corte for a careful reading of the paper.}





%\begin{figure}[t]
%\begin{center}
%\includegraphics[scale=0.8]{pannello_quartiche_fin}
%\caption{\small{{\rm A)} Quartic curve with triple point, equation {\rm (\ref{tripleP1})} with $a=\frac{1}{4}$, $b=\frac{1}{32}$; \--- {\rm B)} Quartic curve with triple point, equation {\rm  (\ref{tripleP2})} with $a=\frac{1}{3}$, $b=\frac{10}{9}$; \--- {\rm C)} Quartic curve with a tacnode, equation {\rm  (\ref{GF})} with $a=1$, $b=8$;  \--- {\rm D)} Quartic curve with a tacnode, equation  {\rm (\ref{3D})} with $a=2$, $m=12$, $b=\frac{1}{2}$; \--- {\rm E)} Lima\c{c}on, equation {\rm (\ref{LC})} with $a=5$, $b=\frac{1}{2}$; \--- {\rm F)} Cardiod, equation {\rm (\ref{CARD})} with $a=\frac{1}{4}$, $b=\frac{1}{20}$.}}
%\label{Quartiche}
%\end{center}
%\end{figure}
%
%\begin{figure}[h]
%\begin{center}
%\includegraphics[scale=0.5]{Hough_Gallarati_noise99_few.eps} \\
%\includegraphics[scale=0.5]{Hough_lacci2_noise99_few}
%\caption{Recognition of the quartic curves, equations {\rm (\ref{GF})} and {\rm (\ref{tripleP2})}, when embedded in a noisy background (\,$99\%$ of noise points). Left panels: dataset and noise points represented with the same marker. Right panels: noise points (dots), dataset points (crosses) and recognized curve (solid).}
%\label{F5}
%\end{center}
%\end{figure}
%\begin{figure}[h]
%\begin{center}
%\includegraphics[scale=0.5]{Hough_Descartes_noise99_few} \\
%\includegraphics[scale=0.5]{Hough_Ellittica_noise99_few}
%\caption{Recognition of the Descartes Folium, equation {\rm (\ref{DF})},  and of the elliptic curve, equation {\rm (\ref{ECAppl})},  when embedded in a noisy background (\,$99\%$ of noise points). Left panels: dataset and noise points represented with the same marker. Right panels: noise points (dots), dataset points (crosses) and recognized curve (solid).}
%\label{F6}
%\end{center}
%\end{figure}
%
%
%\begin{table}
%\begin{center}
%\begin{tabular}{|l|c|c|c|c|}
%\hline \hline 
% & Quartic  curve & Quartic curve & Descartes & Elliptic \\
% & with tacnode & with triple point & Folium & curve \\
% \hline\hline 
% $N_1$ & 50 & 100 & 100 & 158 \\
% \hline 
% $N_2$ & 4950 & 9900 & 9900 & 15642 \\
% \hline 
% $N$ & 5000 & 10000 & 10000 & 15800 \\
%  \hline\hline
% \end{tabular}
%\end{center}
%\caption{Number of dataset points $(N_1)$, noise points $(N_2)$ and total number of points $(N)$ used in the robustness test for the pattern recognition method.}
%\label{T1}
%\end{table}
%
%
%\bigskip
%
%
%
%
%
%\clearpage
%
%\small 

 \bibliographystyle{unsrt}
\bibliography{mybiblio}

\end{document}

\begin{thebibliography}{10}

\bibitem{acton92} L.~Acton et al., \textit{The yohkoh mission for high-energy solar physics}, Science, 258, 5082, (1992) pp. 618--625.

\bibitem{be12} A. O. Benz et al., \textit{The spectrometer telescope for imaging x-rays on board the Solar Orbiter mission}, Proc. SPIE 8443, (2012), 8443, pp. 1--15.

\bibitem{bugr10} B. F. Burke and F. Graham-Smith, \textit{An Introduction to Radio Astronomy}, Cambridge University Press, Cambridge, 2010.

\bibitem{co08} T. J. Cornway, \textit{Multiscale CLEAN Deconvolution of Radio Synthesis Images}, IEEE J. Select. Top. Sign. Proc., (2008), 2, pp. 793--801.

\bibitem{depe09} B. R. Dennis and R. L. Pernak, \textit{Hard X-Ray Flare Source Sizes Measured with the Ramaty High Energy Solar Spectroscopic Imager}, Astrophys. J., (2009), 698, pp. 2131--2143.

\bibitem{giordano15} S.~Giordano, N.~Pinamonti, M.~Piana and A.M.~Massone, \textit{The process of data formation for the Spectrometer/Telescope for Imaging X-rays (STIX) in Solar Orbiter}, SIAM Journal on Imaging Sciences 8, 2, (2015), pp. 1315--1331.

\bibitem{gletal13} L. Glesener, S. Krucker, H. M. Bain and R. P. Lin, \textit{Observation of Heating by Flare-accelerated Electrons in a Solar Coronal Mass Ejection}, Astrophys. J. Lett., 779, (2013), L29--L34.

\bibitem{ho74} J. H\"ogbom, \textit{ Aperture Synthesis with a Non-Regular Distribution of Interferometer Baselines}, Astron. Astrophys. Suppl., (1974), 15, pp. 417--426.

\bibitem{huetal02} G. J. Hurford et al., \textit{The RHESSI Imaging Concept}, Solar Phys., 210, (2002), pp. 61--86.

\bibitem{kretal14} S. Krucker and M. Battaglia, \textit{Particle densities within the acceleartion region of a solar flare}, \textit{Astrophys. J.}, 780, (2014), 107.

\bibitem{kretal20} S. Krucker et al., \textit{The Spectrometer/Telescope for Imaging X-rays (STIX)}, Astron. Astrophys., 642, (2020) A15.

\bibitem{kretal11} S. Krucker, E. P. Kontar, S. Christe, L. Glesener and R. P. Lin, \textit{Electron Acceleration Associated with Solar Jets}, Astrophys. J., 742, (2011), pp. 82--91

\bibitem{lietal02} R.~P.~Lin et al., \textit{The Reuven Ramaty High-Energy Solar Spectroscopic Imager (RHESSI)}, Solar Phys., 210, (2002), pp. 3--32.

\bibitem{maetal09} A. M. Massone, A. G. Emslie, G. J. Hurford, M. Prato, E. P. Kontar and M. Piana, \textit{Hard X-ray imaging of solar flares using interpolated visibilities}, Astrophys. J., (2009), 703, pp. 2004--2016.

\bibitem{maetal22} P. Massa et al., \textit{First hard X-ray imaging results by Solar Orbiter STIX}, Solar Phys., 297, (2022), 93.

\bibitem{ok92} Y. Ogawara et al., \textit{The status of YOHKOH in orbit - an introduction to the initial scientific results}, Publ. Astron. Soc. Japan, (1992), 44, L41-L44.

\bibitem{pietal22} M. Piana, A. G. Emslie, A. M. Massone and B. R. Dennis, {\it{Hard X-ray Imaging of Solar Flares}}, Springer, Berlin, 2022.

\bibitem{pretal09} M. Prato, M. Piana, A. G. Emslie, G. J. Hurford, E. P. Kontar and A. M. Massone \textit{A regularized visibility-based approach to astronomical imaging spectroscopy}, SIAM J. Imag. Sci., (2009), 2, pp. 910--930.


\end{thebibliography}

\end{document}



\bibitem{as10} M.~J.~Aschwanden, \textit{Image Processing Techniques and Feature Recognition in Solar Physics},  Solar Phys, 262, (2010), pp. 235--275.

\bibitem{ballard} D.~H.~Ballard, \textit{Generalizing the Hough transform to detect arbitrary shapes}, Pattern Recognition, 11, 2, (1981),  pp. 111--122.

\bibitem{canny} J.~Canny, \textit{A Computational Approach to Edge Detection}, IEEE Trans. Pattern Analysis and Machine Intelligence, PAMI-8, 6, (1986),  pp. 679--698.

\bibitem{CH} R.~Courant and D.~Hilbert,  \textit{Methods of Mathematical Physics}; Vol. II, {\it Partial Differential Equations}, by R.~Courant, Interscience Publishers---a division of  J.~Wilew\&Sons, New York, 1962.

\bibitem{duda-hart} R.~O.~Duda and P.~E.~Hart, \textit{Use of the Hough Transformation to Detect Lines and Curves in Pictures}, Comm. ACM, 15, 1 (1972), pp. 11--15.

\bibitem{gletal13} L. Glesener, S. Krucker, H. M. Bain, R. P. Lin, \textit{Observation of heating by flare-accelerated electrons in a solar coronal mass ejection}, Astrophys. J. Lett., 779, (2013), L29.

\bibitem{God} R.~Godement,  \textit{Cours d'Alg\`ebre}, Enseignements de Sciences, Hermann, Paris, 1963.

\bibitem{houghp} P.~V.~C.~Hough, \textit{Method and means for recognizing complex patterns}, US Patent 3069654, December 18, 1962.

\bibitem{ja75} J. D. Jackson, \textit{Classical Electrodynamics}, Wiley (1975).

\bibitem{knapp} A.~W.~Knapp,  \textit{Elliptic Curves}, Mathematical Notes 40, Princeton University Press (1992).

\bibitem{kretal11}} S. Krucker, E. P. Kontar, S. Christe, L. Glesener and R. P. Lin, \textit{Electron acceleration associated with solar jets}, Astrophys. J., 742, (2011), 82

\bibitem{catalog} J.~D.~Lawrence,  \textit{A Catalog of Special Plane Curves}, Dover Publications Inc., New York (1972).

\bibitem{SDO_AIA} J.~R.~Lemen et al., \textit{The Atmospheric Imaging Assembly (AIA) on the Solar Dynamics Observatory (SDO)}, Solar Phys., 275, (2012), pp. 17--40.

\bibitem{lietal02} R.~P.~Lin at al., \textit{The Reuven Ramaty High-Energy Solar Spectroscopic Imager (RHESSI)}, Solar Phys., 210, (2002), pp. 3--32.

\bibitem{SDO_compvis} P.~C.~H.~Martens et al., \textit{Computer Vision for the Solar Dynamics Observatory (SDO)}, Solar Phys., 275, (2012), pp. 79--113.

\bibitem{SDO_gen} W.~D.~Pesnell, B.~J.~ Thompson and P.~C.~Chamberlin, \textit{The Solar Dynamics Observatory (SDO)}, Solar Phys., 275, (2012), pp. 3--15.

\bibitem{princen} J.~Princen, J.~Illingworth, and J.~Kittler,  \textit{A Formal Definition of the Hough Transform: Properties and Relationships}, Journal of Mathematical Imaging and Vision 1, (1992), pp. 153--168.

\bibitem{atlas} E.~V.~Shikin,  \textit{Handbook and Atlas of Curves}, CRP Press, Inc., Boca Raton (1995).

\bibitem{SDO_gallery} Solar Dynamics Observatory (SDO) gallery. http://sdo.gsfc.nasa.gov/gallery/main.php

\bibitem{taem88} E. Tandberg-Hanssen and A. G. Emslie, \textit{The physics of solar flares}, Cambridge (1988).

\bibitem{Wash} L.~C.~Washington,  \textit{Elliptic Curves---Number Theory and Cryptography}, Second Edition, Discrete Mathematics and its Applications Series, Editor Kenneth H. Rosen,  Chapman \& Hall/CRC, 2008.


