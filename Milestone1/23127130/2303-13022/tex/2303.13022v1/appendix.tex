\appendix

\section{Additional SDF Regularizations}
% Learning implicit neural surface representations is more challenging compared to learning neural volume representations. 
In addition to the Eikonal term \citep{gropp2020implicit} that is commonly used for constraining the learned SDF field,
neural surface methods also require special weight initialization or warm-up steps to stabilize the learning of implicit surfaces \citep{yariv2021volume, wang2021neus, Yu2022MonoSDF}.
However, unlike prior works that use a large fully implicit MLP \citep{yariv2021volume, wang2021neus} or attach spatial coordinates to the grid-interpolated geometry features \citep{Yu2022MonoSDF}, \model's geometry MLP uses an NGP-like \citep{mueller2022instant} model that uses multi-level hash encoding as the only input to the tiny geometry MLP. Therefore, the geometric initialization \citep{atzmon2020sal} that assumes spatial coordinates as MLP inputs cannot be applied to our model. Thus, our model requires new ways to initialize its learned geometry to better represent continuous and smooth surfaces. 

To demonstrate the importance of incorporating special constraints when learning implicit neural surfaces, we first show the results of models that do not include additional SDF constraints. Figure \ref{fig:ficus_curve} shows the surface normals of two such models, one trained solely with L1 photometric loss (``L1") and the other with L1 loss and Eikonal term (``L1+Eikonal"). 
Both of these models fail to capture accurate surface geometry on glossy regions, and their corresponding SDF curves oscillate around the zero-level SDF, which results in a compositing weight $w_i$ (obtained from Eqn. \ref{eq_radiance}) distribution with multiple peaks along the ray. 
These scattered compositing weights further cause the reconstructed surface to ``collapse" into the actual object.
To avoid this surface collapse, additional regularizations are needed on the initial SDF predictions.
% we can add extra regularizations to reduce the SDF oscillation and thus avoid surface collapse.

\begin{figure}[ht]
    \centering
    \includegraphics[width=\linewidth, trim={0.5cm, 0.1cm, 0.5cm, 0cm}, clip]{figures/ficus_wo_reg.pdf}
    \vspace{-10pt}
    \caption{The normals of the surfaces learned by models without additional constraints. The results are from models after only 20k training iterations.
    We also plot the curves of SDF and compositing weight $w_i$ of the sampled points along the ray for rendering the pixel shown on the left.}
    \label{fig:ficus_curve}
    \vspace{-12pt}
\end{figure}

In this work, we employ two SDF regularization terms in the early training steps to stabilize the learning of the initial SDF. 
The first regularization term is to force the SDF predictions of positions inside object surfaces to be away from zero-level SDF, which can suppress the multiple peaks on the compositing weight curves. 
To avoid over-suppression of the existing SDF predictions that are far away from the zero-level SDF, we employ a modified Cauchy loss \citep{hedman2021baking} on the SDF-converted (via Eqn. \ref{eq:conversion}) density values $\sigma_i$:
\begin{equation}
    \mathcal{L}_n = \frac{1}{N}\sum_{i} \log\left(1+ \frac{(1-\beta \sigma_i^2}{c^2}\right)
\label{eq:l_s}
\end{equation}
where $\beta$ is the parameter used in Eqn. \ref{eq:conversion} and $c$ is a hyperparameter that controls the loss scale which we set to 4. This regularization term is uniformly applied to all the sampled $N$ points per iteration. The effectiveness of this regularization is shown in Figure \ref{fig:ln_only}. Compared to the results in Figure \ref{fig:ficus_curve}, $\mathcal{L}_n$ is able to help our model to get a better initial surface structure.


The second regularization is to eliminate the fluctuations of SDF curve segments that are close to zero-level SDF. The zero level set of SDF denotes the actual position of the surface, and frequent fluctuations near the surface can significantly impact the quality of surface geometry as well as surface normals. 
To ensure that our estimated SDF has a stable decreasing curve when it hits the surface at ray-sampled points, we propose a back-face suppression regularization. This regularization penalizes SDF curve segments with positive slopes and high corresponding compositing weights, as shown in the following equation:
\begin{equation}
    \mathcal{L}_b = \sum_i w_i\max(\Delta s_i, 0)\frac{\Delta s_i}{\delta_i^2 + \Delta s_i^2}
\end{equation}
In this equation, $\Delta s_i = s({\mathbf{x}_{i+1}}) - s({\mathbf{x}_{i}})$ represents the difference in estimated SDF between two adjacent sampled points ${\mathbf{x}_{i}}$ and ${\mathbf{x}_{i+1}}$, $\delta_i$ is the actual distance between ${\mathbf{x}_{i}}$ and ${\mathbf{x}_{i+1}}$. 
The compositing weight $w_i$ for volume rendering can be obtained from Equation \ref{eq_radiance}.
Figure \ref{fig:lb_only} demonstrates the effectiveness of this regularization term. We can observe that the smoother estimated surface is achieved with the same number of training iterations compared to the results obtained using only $\mathcal{L}_n$. However, it should be noted that the object's surface shrinks slightly compared to the other results.

By combining the two regularization terms introduced above, we are able to achieve a more stable and accurate SDF estimation. Specifically, we formulate the combination of the two regularization terms as:
\begin{equation}
\mathcal{L}_{reg} =\lambda_n \mathcal{L}_n + \lambda_b \mathcal{L}_b
\end{equation}
where $\lambda_n$ and $\lambda_b$ are weight hyperparameters.
Setting overly large values of $\lambda_n$ and $\lambda_b$ may prohibit the model from learning fine-grained geometry details. However, they can be beneficial in providing smooth surfaces for the later training steps.
The effectiveness of the combined regularization can be seen in Figure \ref{fig:ln_lb}. By using these additional SDF regularizations during the early training steps, we can produce much better initial surfaces for accurate reconstruction and illumination. 


\begin{figure}
\centering
\small
\begin{subfigure}[c]{0.243\linewidth}
\includegraphics[width=\linewidth, trim={3cm 0 2.7cm 6cm},clip]{figures/ficus_initialization/box_normal_ficus_w_cauchy.png}
\caption{$\mathcal{L}_n$ Only}\label{fig:ln_only}
\end{subfigure}
\begin{subfigure}[c]{0.243\linewidth}
\includegraphics[width=\linewidth, trim={3cm 0 2.7cm 6cm},clip]{figures/ficus_initialization/box_normal_ficus_w_back.png}
\caption{$\mathcal{L}_b$ Only}\label{fig:lb_only}
\end{subfigure}
\begin{subfigure}[c]{0.243\linewidth}
\includegraphics[width=\linewidth, trim={3cm 0 2.7cm 6cm},clip]{figures/ficus_initialization/box_normal_ficus_w_back_w_cauchy.png}
\caption{$\mathcal{L}_n + \mathcal{L}_b$}\label{fig:ln_lb}
\end{subfigure}
\begin{subfigure}[c]{0.243\linewidth}
\includegraphics[width=\linewidth, trim={3cm 0 2.7cm 6cm},clip]{figures/ficus_initialization/ours.png}\label{fig:our_200k}
\caption{200k iter.}
\end{subfigure}
% $\mathcal{L}_n$ Only & $l^2$-Norm & Inst-Norm &  full, 200k iter.
        
    \caption{The estimated surface normal with our regularization terms. (a-c) show the results after 20k iterations, and (d) shows the results of the model with our full regularizations after 200k training iterations ($\mathcal{L}_n$ \& $\mathcal{L}_b$ stop at 40k).
    \label{fig:ficus_w_reg}}
\vspace{-6pt}
\end{figure}



% \subsection{Relighting}

\begin{table}[t]
    \centering
    \footnotesize
    \setlength\tabcolsep{0pt}
        \begin{tabularx}{\linewidth}%
        {p{1em}*{4}{>{\centering\arraybackslash}X}}
\hdashline
\rotatebox[origin=l]{90}{NVDiffRec} & \multicolumn{4}{c}{\includegraphics[width=0.98\linewidth, trim={0 0 0 0},clip]{figures/relight/helmet_rec.pdf}}\\
\rotatebox[origin=l]{90}{NVDiffMC} & \multicolumn{4}{c}{\includegraphics[width=0.98\linewidth, trim={0 0 0 0},clip]{figures/relight/helmet_mc.pdf}}\\
\rotatebox[origin=l]{90}{\quad\quad Ours} & \multicolumn{4}{c}{\includegraphics[width=0.98\linewidth, trim={0 0 0 0},clip]{figures/relight/helmet_our.pdf}}\\
\hdashline
\rotatebox[origin=l]{90}{NVDiffRec} & \multicolumn{4}{c}{\includegraphics[width=0.98\linewidth, trim={0 0 0 0},clip]{figures/relight/teapot_rec.pdf}}\\
\rotatebox[origin=l]{90}{NVDiffMC} & \multicolumn{4}{c}{\includegraphics[width=0.98\linewidth, trim={0 0 0 0},clip]{figures/relight/teapot_mc.pdf}}\\
\rotatebox[origin=l]{90}{\quad\quad Ours} & \multicolumn{4}{c}{\includegraphics[width=0.98\linewidth, trim={0 0 0 0},clip]{figures/relight/teapot_our_unfix.pdf}}\\
\hdashline
\rotatebox[origin=l]{90}{NVDiffRec} & \multicolumn{4}{c}{\includegraphics[width=0.98\linewidth, trim={0 0 0 0},clip]{figures/relight/toaster_rec.pdf}}\\
\rotatebox[origin=l]{90}{NVDiffMC} & \multicolumn{4}{c}{\includegraphics[width=0.98\linewidth, trim={0 0 0 0},clip]{figures/relight/toaster_mc.pdf}}\\
\rotatebox[origin=l]{90}{\quad\quad Ours} & \multicolumn{4}{c}{\includegraphics[width=0.98\linewidth, trim={0 0 0 0},clip]{figures/relight/toaster_our_unfix.pdf}}\\
\hdashline
\rotatebox{90}{Probes} & \multicolumn{4}{c}{\includegraphics[width=0.98\linewidth, trim={0 0 0 0},clip]{figures/relight/env.jpg}}\\
             & Relighting \#1 & Relighting \#2 & Relighting \#3 & Relighting \#4 
        \end{tabularx}%
        
    \makeatletter\def\@captype{figure}\makeatother
    \caption{\small
Additional relighting results for various scenes. Due to the unavailability of Blender files for these objects, reference relighting images are not provided for comparison. 
NVDIFFREC and NVDIFFRECMC are rendered by Blender Cycles. 
    \label{fig:add_relighting}}
    \vspace{-10pt}
\end{table}



\section{Implementation Details}
\textbf{Architecture and hyperparameters.} 
The geometry MLP $\bm{F}_g$ used in \model\ is similar to Instant-NGP \cite{mueller2022instant}. We employ a hash encoding with 16 grid levels where each level encodes a 2-d feature vector. The MLP $\bm{F}_g$ itself is a tiny 3-layer MLP with 64 neurons per hidden layer.
The output geometry feature $\mathbf{f}_{geo}$ is a 12-d feature vector. 
The environment MLP $\bm{E}$ is a relatively large  MLP with 4 layers and 256 neurons per layer (still smaller than a 1k HDR light probe image). The integrated directional encoding (IDE) \cite{verbin2022ref} utilized by $\bm{E}$ encodes unit directions using the first 5 bands of spherical harmonics. The output environment feature $\mathbf{f}_{env}$ is also a 12-d feature vector. Specular MLP $\bm{R}_s$ is a 3-layer MLP with 64 neurons per hidden layer, while Diffuse MLP $\bm{R}_d$ is a 2-layer MLP with 32 neurons per hidden layer. We implemented our model using a PyTorch version of Instant-NGP \footnote{\url{https://github.com/ashawkey/torch-ngp}}. 

\textbf{Training details.}
For the neural renderer's training, we use Filament PBR engine \cite{google2018filament} to randomly synthesize new frames on the fly, with varying materials and environment lights (we use 11 light probes images, provided by Filament's repository\footnote{\url{https://github.com/google/filament/tree/main/third\_party/environments}}). 
We train the neural renderer for 100k iterations, each with 32000 sampled rays, using the Adam optimizer with an initial learning rate of 0.001. This training takes about 3 hours on a single RTX3090 GPU.
For the training of representing general scenes, we train our model for 200k iterations with 4096 sampled rays per iteration, using the Adam optimizer with an initial learning rate of 0.0005. 
We first train the model only with a photometric loss for 4k iterations to obtain a coarse geometry for ray-sampling acceleration. We then apply our additional SDF regularizations for about 40k iterations with exponentially decaying loss weights. The Eikonal loss term is added to the training after the first 10k training iterations. If the indirect illumination module is used, we initiate the extra raymarching pass after the first 40k training iterations.
The training speed depends on the complexity of the target scene, but most scenes can be trained within 3 hours (5 hours if indirect illumination is used) on a single RTX3090 GPU.

\textbf{Runtime.}
The time required to render a single 800$\times$800 image is approximately between 0.5 to 1.2 seconds (without indirect illumination) on a single RTX3090 GPU. If the indirect illumination pass is enabled, rendering may take 1.6 times longer. Although our code base is not yet optimized for runtime performance, further optimizations are possible to achieve faster rendering.

\section{Additional Results}
In this section, we present additional visual results to demonstrate \model's ability to reconstruct and render glossy surfaces. We also provide a demo video on our \href{https://nexuslrf.github.io/ENVIDR/}{web page} to showcase results in motion.

\textbf{Relighting}. Additional relighting results on challenging synthetic shiny scenes are demonstrated in Figure \ref{fig:add_relighting}. \model\ is capable of synthesizing lighting effects that are comparable to Blender's path-tracing rendering.  while maintaining significantly better surface geometry compared to the two baseline models. However, these relighting results also reveal some limitations of our method. For instance, in the ``teapot" example, our current method cannot synthesize shadowing effects caused by surface occlusions. We plan to address this in our future work.

\textbf{Scene Decompositions}
The rendering decomposition of all evaluated synthetic scenes is shown in Figure \ref{fig:perscene}. \model~can effectively decompose both the view-independent diffuse color and view-dependent specular color from multiview training images. Moreover, it can successfully distinguish between different types of materials present in the scene, such as metallic materials in the "toaster" and "coffee" scenes. In addition, our model also captures high-fidelity environment light probes from these shiny objects. 

% \subsection{Scene Decompositions}

\input{appendices/per_scene}

% \subsection{Failure Cases}