
\begin{figure*}
    \centering
    \includegraphics[width=\linewidth]{figures/gardenspheres_blend.pdf}
    \vspace{-20pt}
    \caption{\small
    \emph{\model} achieves high-quality rendering (b) and reconstruction (c) of scenes with glossy/shiny surfaces. Our method represents diffuse, direct specular, and indirect specular colors separately (d), and enables various scene relighting and material editing (e-h) using our proposed neural renderer with decomposed rendering components. Ref-NeRF's results are reproduced based on the official code.
    }
    \label{fig:garden}
\vspace{-10pt}
\end{figure*}


\section{Preliminaries}
\subsection{Volume Rendering with Neural Surface} \label{sec:nerf_surface}
Instead of representing volume density like NeRF \citea{mildenhall2021nerf}, neural surface methods \citea{park2019deepsdf, yariv2020multiview} use implicit neural representation to represent scene geometry as signed distance fields (SDF).  
For a given 3D point $\mathbf x \in \mathbb R^3$, SDF returns the pointâ€™s distance to the closest surface $\mathbf x \mapsto s = \bm{F}_\theta(\mathbf{x})$, $\bm{F}_\theta$ denotes the neural spatial representation with learnable parameters $\theta$. $\bm{F}_\theta$ can be either a fully implicit MLP or a hybrid model containing voxel-based features \citea{mueller2022instant, Yu2022MonoSDF, wu2022voxurf}.

% Recent work \citea{oechsle2021unisurf, yariv2021volume,wang2021neus} optimizes such implicit surface representations via NeRF-like differentiable volume rendering. 
To render a pixel, a ray $\mathbf{r}:\mathbf{o} + t \mathbf{\hat{v}}$ is cast from the camera's origin $\mathbf{o}$ along its view direction $\mathbf{\hat{v}}$. 
The SDF value $s_i$ of sampled point $\mathbf{x}_i$ along the ray are then converted to density or opacity value for volume rendering. VolSDF \citea{yariv2021volume} demonstrates a density conversion method with the cumulative distribution function (CDF) of Laplace distribution:
\begin{equation}
    \sigma_\beta(s) = \begin{cases}
\frac{1}{2\beta}\exp(\frac{s}{\beta}) & \text{if } s \leq 0,\\
\frac{1}{\beta}(1 - \frac{1}{2}\exp(-\frac{s}{\beta})) & \text{if } s > 0
\end{cases}\label{eq:conversion}
\end{equation}
Where $\sigma$ is converted volume density, $\beta$ is a learnable parameter.
With the predicted color $\mathbf{c}(\mathbf{x}_i)$ of sampled points along the ray, the color ${\mathbf C}(\mathbf r)$ for the current ray $\mathbf r$ is integrated with volume rendering \citea{max1995optical}:
% \begin{align}
%     {\mathbf C}(\mathbf r) &= \sum_{i} w_i \mathbf c(\mathbf{x}_i) \notag \\
%     \text{with } w_i = \exp(-\sum_{j=1}^{i-1}\sigma&(\mathbf{x}_j) \delta_j)(1 - \exp(-\sigma(\mathbf{x}_j) \delta_j) ) \label{eq_radiance}
% \end{align}
\vspace{-8pt}
\begin{equation}
    {\mathbf C}(\mathbf r) = \sum_{i} \exp(-\sum_{j=1}^{i-1}\sigma_j \delta_j)(1 - \exp(-\sigma_j \delta_j) )  \mathbf c(\mathbf{x}_i)
\label{eq_radiance}
\vspace{-8pt}
\end{equation}
where $\delta_i$ denotes the distance between adjacent sampled positions along the ray.

% \subsection{View-dependent Color Prediction}
% The original NeRF uses directional MLP $\bm{R}(\mathbf f_{geo}, \mathbf{\hat{v}})$, conditioned on additional neural features $\mathbf{f}_{geo}$ from the spatial geometry model $\bm F$ and view direction $\mathbf{\hat{v}}$, to predict view-dependent color. 
% IDR \citea{yariv2020multiview} includes normal direction $\mathbf{\hat{n}}$, as the extra input to the color prediction MLP for the better modeling of the underlying bidirectional reflectance distribution function (BDRF) of the surface. The normal direction $\mathbf{\hat{n}}$ can be computed by the analytical gradient of neural field (SDF or density) $\bm{s}$ w.r.t. position $\mathbf x$: $\mathbf{\hat{n}} = \pm \frac{\nabla \bm{s}(\mathbf x)}{\|\nabla \bm{s}(\mathbf x)\|_2}$
% % \begin{equation}
% %     \mathbf{\hat{n}} = \pm \frac{\nabla \bm{s}(\mathbf x)}{\|\nabla \bm{s}(\mathbf x)\|_2}
% % \end{equation}
% % $c(\mathbf z, \mathbf{\hat{v}}, \mathbf{\hat{n}})$

% Some works \citea{zhang2021physg, boss2021neural, verbin2022ref} use the reflected view direction $\bm{\hat{\omega}}_r$ to help neural rendering model better learn continuous spatially-varying illumination, where %$\bm{\hat{\omega}}_r$ can be computed by 
% $\bm{\hat{\omega}}_r = \mathbf{\hat{v}} - 2(\mathbf{\hat{v}} \cdot \mathbf{\hat{n}})\mathbf{\hat{v}}$.
% \begin{equation}
%  \bm{\hat{\omega}}_r = \mathbf{\hat{v}} - 2(\mathbf{\hat{v}} \cdot \mathbf{\hat{n}})\mathbf{\hat{v}}   
% \end{equation}


\subsection{The Rendering Equation} \label{sec:re} 
Mathematically, the outgoing radiance of a surface point $\mathbf{x}$ with normal $\hat{\mathbf{n}}$ from outgoing direction $\hat{\bm{\omega}}_o$ can be described by the physically based rendering equation \citea{kajiya1986rendering}:
\vspace{-5pt}
\begin{equation}
% \vspace{-5pt}
    L_o(\mathbf{x}, \hat{\bm{\omega}}_o) = \int_{\Omega}  L_i(\mathbf{x},  \hat{\bm{\omega}}_i) f_r(\mathbf{x}, \hat{\bm{\omega}}_i, \hat{\bm{\omega}}_o) (\hat{\mathbf{n}}\cdot \hat{\bm{\omega}}_i)  d \hat{\bm{\omega}}_i \label{eq:re}
\end{equation}
where $\hat{\bm{\omega}}_i$ denotes incoming light direction, $\Omega$ denotes the hemisphere centered at $\hat{\mathbf{n}}$, $L_i(\mathbf{x},  \hat{\bm{\omega}}_i)$ is the incoming radiance of $\mathbf{x}$ from $\hat{\bm{\omega}}_i$, $f_r$ is the BRDF that describes the surface response of incoming lights. BDRF can be expressed as a function with diffuse $f_d$ and specular $f_s$ components \citea{walter2007microfacet}:
\begin{equation}
    f_r = f_d + f_s \label{eq:brdf_decompose}
\end{equation}
% Diffuse BRDF $f_d$ is view-independent, while $f_r$ depends on the view and surface normal directions. However, both components interact with the light denoted by $L_i$.

% \subsection{Multi-resolution Hash encoding.}
% Recently, Instant-NGP~\citea{mueller2022instant} proposes to represent the whole 3D space with multi-resolution grids stored in a hash table.
% Instead of using a fully implicit MLP, Instant-NGP uses a combination of implicit and explicit neural networks to represent the 3D scene. For each query point position $\mathbf{x}$, instant-NGP's hash encoding outputs its neural feature $\mathbf{h}_{\mathbf{x}}^{(l)}$ by interpolating the feature grids at each level $l$. Then the features from all resolution levels are concatenated together as the point's encoded neural features $\mathbf{h}_{\mathbf{x}}$. Then these features are fed into a shallow MLP for predicting density and radiance.
% Thanks to this efficient structure, Instant-NGP accelerates the training and rendering stage of NeRF by a large margin without obvious performance degradation.