\begin{figure*}[t!]
    \centering
    % \includesvg[width=0.9\textwidth,inkscapelatex=false]{figures/renderer.svg}
    \includegraphics[width=\linewidth, trim={0, 0, 1.5cm, 0}, clip]{figures/render_overview.pdf}
    \caption{Overview of our proposed neural renderer. Training images are synthesized by Filament PBR engine \citea{google2018filament} during runtime, and probes are from HDRI Haven (CC-0). The MLPs used in our neural renderer are some simple and tiny MLPs.}
    \label{fig:renderer}
    \vspace{-10pt}
\end{figure*}

\section{Related Work}
\textbf{Neural rendering and NeRF.}
Neural rendering is a class of reconstruction and rendering approaches that employ  neural methods to learn complex mappings from captured images to novel images \citea{tewari2020state}.
% The recent advances in neural rendering have shown promising results in various tasks \citea{thies2019deferred, lombardi2021mixture, chan2022efficient}.
% such as texture mapping \citea{thies2019deferred}, surface reconstruction \citea{yariv2020multiview}, image generation \citea{chan2022efficient}, etc. 
Neural radiance field (NeRF) \citea{mildenhall2020nerf} is one representative work that utilizes implicit neural representations and volume rendering for photo-realistic novel view synthesis.
% NeRF can render images from novel unobserved camera views by reconstructing the scene as a neural volume, represented as a coordinate-based multilayer perceptron (MLP).
% Given a set of sparsely-captured scene images, NeRF is able to render images from novel unobserved camera views through its 3D reconstruction of the scene as neural volume represented in the coordinate-based multilayer perceptron (MLP).
NeRF has inspired many follow-up works that achieve state-of-the-art performance in 3D rendering tasks \citea{barron2021mip, barron2022mip, verbin2022ref, tancik2022block}. 
Recent work also utilizes the hybrid neural representation to accelerate the training and rendering speed of NeRF models \citea{mueller2022instant, nerfstudio}, making it practical for real applications such as game and movie productions.
% One major issue is that the original NeRF method has low quality on the represented surface geometry because of the unconstrained volumetric representation. 
One major limitation of the original NeRF method is that its unconstrained volumetric representation leads to low-quality surface geometry.
Follow-up methods combine the implicit surface representation with NeRF \citea{yariv2021volume, wang2021neus} to enable volume rendering on neural surface representations for more accurate and continuous surface reconstruction.
% To improve the reconstructed surface quality, follow-up methods combine the implicit surface representation and NeRF \citea{yariv2021volume, wang2021neus} to enable the NeRF's volume rendering on the SDF-based neural surface representation. 
% These neural surface models can reconstruct accurate and continuous object surfaces through multiview images. 
% Recent work also shows that neural surface approaches can be employed in hybrid neural representations \citea{Yu2022MonoSDF, wang2022go, wu2022voxurf} for efficient and accurate surface reconstruction.
% While our model also utilizes a hybrid neural surface representation for more efficient learning, our primary focus is on accurately rendering and representing glossy surfaces.

\textbf{Rendering reflective and glossy surfaces.}
Rendering views in scenes with complex specular reflections has been challenging. Early methods use light field techniques \citea{gortler1996lumigraph, levoy1996light, wood2000surface, davis2012unstructured}, which require dense image capture. Recent approaches use learning-based methods to reconstruct the light field from a small set of images \citea{zhou2018stereo, flynn2019deepview, wizadwongsa2021nex}, but are limited by the number of available viewpoints.
Recent advances in neural rendering also show promising results in rendering reflective or glossy surfaces. 
NeRFReN \citea{guo2022nerfren} models planar reflections by learning a separate neural field. SNISR \citea{wu2022scalable} treats specular highlights as virtual lights underneath the surface with a reflection MLP.
Neural Catacaustics \citea{kopanas2022neural} uses a neural warp field to approximate the catacaustic through the virtual points for reflections.
% These neural methods synthesize the reflections through the virtual geometry (e.g., virtual lights/points) learned by specialized neural models. 
However, these methods do not model the physically based interaction between lighting and surface, limiting their ability to edit the lighting of represented scenes.
% Various neural methods \citea{guo2022nerfren, wu2022scalable, kopanas2022neural} model the reflections through virtual geometry learned by specialized neural models. However, these models do not account for the physically based interaction between environment light and surfaces, limiting their ability to edit scene lighting.
% 
Ref-NeRF \citea{verbin2022ref} conditions the view-dependent color on the reflected view direction and improves learning of surface normals, but it may still inaccurately learn virtual geometry from complex reflections (see Fig. \ref{fig:toaster_compare}).
% Furthermore, Ref-NeRF does not have a complete decomposition of environment light and surface color, which makes it challenging to do scene relighting.
% following the inspiration of physically based rendering \citea{ramamoorthi2002frequency, ramamoorthi2009precomputation}.
% Although Ref-NeRF improves the surface normal, it can still learn virtual geometry accounting for complex reflections (see Fig. \ref{fig:toaster_compare}).
% Another limitation of Ref-NeRF is that it does not have a complete decomposition of environment light and surface color, which disables its ability to relight scenes.
% Inspired by Ref-NeRF, we incorporate IDE in our decoupled environment light representation to generate pre-integrated environment feature vectors.
% Unlike Ref-NeRF, our model utilizes surface representation to constrain the surface normal, our model also decouples environment light from the directional MLP to achieve scene relighting.
In contrast, our model uses the surface-based representation to constrain the surface normal, and it decouples environment light from the directional MLP to achieve scene relighting.

\textbf{Neural inverse rendering.}
Inverse rendering aims to estimate surface geometry, material properties and lighting conditions from images \citea{marschner1998inverse}. 
% Prior works have shown accurate estimation of reflection properties, such as BRDF, under known lighting conditions \citea{matusik2003data,aittala2016reflectance,deschaintre2018single}, and estimation of environment light from objects with known geometry \citea{richter2016instant, legendre2019deeplight, park2020seeing}.
Recently, NeRF methods have been employed in inverse rendering tasks to learn scene lighting and material properties (e.g., BRDF). 
However, prior work such as \citea{bi2020neural, nerv2021} requires known lighting conditions to learn materials. 
More recent works such as \citea{zhang2021physg, boss2021neural, nerfactor, munkberg2022extracting, hasselgren2022shape} jointly estimate environment light and materials with images under unknown lighting conditions. These methods employ different representations to model the environment light, such as learnable spherical Gaussians \citea{boss2021nerd, zhang2021physg}, pre-integrated environment texture \citea{boss2021neural}, HDR light probes \citea{nerfactor, liang2022spidr, munkberg2022extracting, hasselgren2022shape}.
% Recent work also leverages NeRF methods to learn scene lighting and material reflectance. 
% Methods such as \citea{bi2020neural, nerv2021} attempt to learn material reflectance properties with NeRF, but require known lighting conditions.
% More recent work jointly estimates environment light and reflectance with images under unknown lighting conditions \citea{zhang2021physg, boss2021neural, nerfactor, munkberg2022extracting, hasselgren2022shape}.
% \citea{boss2021nerd, zhang2021physg} use learnable spherical Gaussian (SG) to represent the environment light.
% Neural-PIL \citea{boss2021neural} uses MLP to represent pre-integrated environment maps.
% \citea{nerfactor, liang2022spidr} directly learn the HDR light probes for the environment light.
% Other methods such as \citea{munkberg2022extracting, hasselgren2022shape} learn mesh-based 3D representations and optimize structured material textures and environment textures through a differentiable renderer.
With the estimated decomposed rendering parameters, previous neural inverse rendering methods rely on an approximated or simplified rendering equation \citea{kajiya1986rendering} to synthesize or edit the scene,  limiting their ability to achieve high-quality renderings comparable to top-performing NeRF models.
In contrast, our model uses a neural renderer to learn the physically based interaction between surface and environment through existing PBR renderers, without explicitly formulating the rendering equation. 
Similar to \citea{boss2021neural, gardner2022rotation, liang2022spidr}, our model also uses MLPs to represent environment lights, however, the output of our environment MLP is neural features instead of RGB colors.
% We represent the environment lights as neural feature vectors that are fused with surface features to synthesize the novel views.
Regarding indirect illumination, 
\citea{nerv2021, zhang2022modeling} incorporate indirect illumination in their model, but their approximation may not work well on shiny surfaces. We instead directly march the surface-reflected rays to synthesize indirect lighting on shiny surfaces.