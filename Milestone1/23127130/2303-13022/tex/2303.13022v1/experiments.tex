\section{Experiments}
We evaluate our method on various challenging shiny scenes and demonstrate the qualitative and quantitative results. We compare against prior methods based on view synthesis, scene relighting, and environment light estimation.

\noindent
\textbf{Datasets.} We use all 6 scenes in the Shiny Blender dataset proposed in \citea{verbin2022ref}, 2 shiny scenes (``ficus" and ``materials") from NeRF's Blender dataset \citea{mildenhall2021nerf}, and one real captured shiny scene (``garden spheres") from SNeRG \citea{hedman2021baking}.

\noindent
\textbf{Baselines.} We choose Ref-NeRF \citea{verbin2022ref} as the top-performing view synthesis model, NVDIFFREC \citea{munkberg2022extracting} and NVDIFFRECMC \citea{hasselgren2022shape} as two top-performing neural inverse rendering models. We also include VolSDF \citea{yariv2021volume} as a baseline neural surface model.

\subsection{Novel View Synthesis}
Following prior works, we use PSNR, SSIM, and LPIPS \citea{zhang2018unreasonable} to measure the view synthesis quality. Similar to \citea{verbin2022ref}, we use mean angular error (MAE) to evaluate the estimated surface normals.
We show the novel view synthesis results for all evaluated scenes in Table \ref{tab:nvs} and visual results in Figure \ref{fig:toaster_compare} and \ref{fig:garden}. 
Our model consistently shows better qualities in perceptually based metrics (SSIM and LPIPS).
\model\ significantly outperforms previous neural inverse rendering and neural surface methods. 
\model\ also performs on par with Ref-NeRF, and with higher PSNR scores in some scenes.
However, we should note that Ref-NeRF has a much lower surface quality (depicted by MAE) and does not support scene relighting.


\begin{table}[thb]
\centering
\footnotesize
\input{table/table}
\vspace{-3pt}
\caption{\small Quantitative comparison among evaluated models. ``NVDiffMC" is short for NVDIFFRECMC. Ref-NeRF's results are imported from their original paper \citea{verbin2022ref}.}
\label{tab:nvs}
\vspace{-15pt}
\end{table}

In terms of learned surface quality,
\model\ achieves the lowest MAEs on almost all evaluated synthetic scenes, indicating superior surface quality. 
We attribute this improvement primarily to the VolSDF-like neural surface representation employed in our model, as VolSDF also demonstrates competitive MAE values.
Combining our neural renderer and neural surface model can further enhance the quality of the learned surface geometry.

\subsection{Environment Estimation}
Although the environment MLP in \model\ does not directly represent RGB values of environment light, it encodes the environment light as neural features. Our learned neural renderer can convert these neural features into RGB colors on a metallic sphere. By unwrapping such a sphere, we can obtain a panorama view of the environment light, which is similar to a light-probe image. 
This approach allows us to extract the environment light and compare it against the results obtained from other methods. In this section, we choose NVDIFFREC as a strong baseline to evaluate the accuracy of our environment light estimation.

 %Therefore, 
% We show the visual comparisons\footnote{Indeterminable color scaling and incomplete estimation from limited views make it difficult to quantify the quality of estimated light probes.} of environment light estimation in Figure \ref{fig:env_compare}.
Figure \ref{fig:env_compare} visually compares our estimated environment light maps with those of NVDIFFREC\footnote{Indeterminable color scaling and incomplete estimation from limited views make it difficult to quantify the quality of estimated light probes.}.
Figure \ref{fig:env_compare} demonstrates that our model effectively captures high-frequency environment lighting through the training with multiview images. 
Both our approach and NVDIFFREC accurately capture the high-quality environment light from highly reflective objects like the ``toaster" and ``helmet".
However, NVDIFFREC struggles to capture the detailed patterns of environment light for less reflective objects such as the ``coffee" and ``teapot", whereas our model still captures these patterns with precision. 
% As demonstrated in Figure \ref{fig:env_compare}, our model is able to capture high-frequency environment lighting through the training of the differentiable rendering. 
% Both our model and NVDIFFREC are able to capture high-quality environment light from more reflective objects such as toaster and helmet. 
% For less reflective objects such as coffee and teapot, our model is still able to capture detailed patterns of environment light, while the NVDIFFREC fails to capture these patterns in its estimations.
% It is worth noting that our primary focus in this work is not on environment light estimation.


% \textbf{Baseline:} NVDIFFREC

% \textbf{Scenes:} Helmet, Teapot, Coffee

% \begin{figure}
%     \centering
%     \includegraphics[width=\linewidth]{figures/env_compare.pdf}
%     \caption{Caption}
%     \label{fig:my_label}
% \end{figure}

\begin{table}[t]
    % \vspace{-10pt}
    \centering
    \small
    \setlength\tabcolsep{0pt}
    % \settowidth\rotheadsize{abcd}
    \begin{tabularx}{\linewidth}%
    {>{\centering\arraybackslash}p{0.15\linewidth}*{3}{>{\centering\arraybackslash}X}}
    \multicolumn{4}{c}{\includegraphics[width=\linewidth]{figures/env_compare.pdf}}%
    \\
    Scenes & Reference & Ours & NVDiffRec \citea{munkberg2022extracting}
    \end{tabularx}%
    \vspace{-2pt}
    \makeatletter\def\@captype{figure}\makeatother
    \caption{\small The comparison of estimated environment light probes. HDR light probes are converted to sRGB by gamma correction. 
    % We manually adjust the roughness of our Env-MLP for the best visual quality. The scene images shown on the left are synthesized by our model.
    Note that NVDIFFRECMC's extracted probes have similar or worse qualities compared to NVDIFFREC.
    }
    \label{fig:env_compare}
    % \vspace{-2pt}
\end{table}


\begin{table}[t]
    \centering
    \footnotesize
    \setlength\tabcolsep{0pt}
        \begin{tabularx}{\linewidth}%
        {p{1em}*{4}{>{\centering\arraybackslash}X}}
\rotatebox{90}{Ref.} & \multicolumn{4}{c}{\includegraphics[width=0.98\linewidth, trim={0 0 8cm 0},clip]{figures/car_relight/gt.pdf}}\\
\rotatebox{90}{NV.Rec} & \multicolumn{4}{c}{\includegraphics[width=0.98\linewidth, trim={0 0 8cm 0},clip]{figures/car_relight/rec.pdf}}\\
\rotatebox{90}{NV.MC} & \multicolumn{4}{c}{\includegraphics[width=0.98\linewidth, trim={0 0 8cm 0},clip]{figures/car_relight/mc.pdf}}\\
\rotatebox{90}{Ours} & \multicolumn{4}{c}{\includegraphics[width=0.98\linewidth, trim={0 0 8cm 0},clip]{figures/car_relight/ours.pdf}}\\
\rotatebox{90}{Env.} & \multicolumn{4}{c}{\includegraphics[width=0.98\linewidth, trim={-4cm 0 35cm 0},clip]{figures/car_relight/env.jpg}}\\
             & Original & Relighting \#1 & Relighting \#2 & Relighting \#3 
        \end{tabularx}%
        
    \makeatletter\def\@captype{figure}\makeatother
    \caption{\small
    Relighting results for ``car". 
    Except for ours, other results are rendered by Blender.
    % NVDiffRec and NVDiffRecMC use Blender to render relighting, while our results are rendered through our neural model.
    Since our relighting is synthesized by a neural model, the intensity of environment light represented by our env MLP may not match the Blender rendering.
    \label{fig:relighting}}
    \vspace{-10pt}
\end{table}


% \vspace{}
\subsection{Relighting}\label{sec:relight}
Similar to the relighting process in the traditional PBR, we can relight the scene represented by our model by replacing the environment MLP with environment MLPs representing new environment lights (these pre-trained env. MLPs can be obtained from our neural renderer).  


In Figure \ref{fig:relighting}, we present a comparison between the scene relighting results obtained from our neural renderer and those obtained from NVDIFFREC and NVDIFFRECMC (rendered by Blender).
% we compare the scene relighting results obtained from our neural renderer with results from NVDIFFREC and NVDIFFRECMC rendered by Blender. 
Table \ref{tab:relighting} also provides a quantitative comparison. 
Our model outperforms NVDIFFREC since NVDIFFREC fails to accurately reconstruct surfaces on reflective regions (e.g., artifacts shown in Figure \ref{fig:relighting}).
Even though the baseline models directly use the same Blender rendering as the ground-truth reference, our model using a fully neural approach still provides comparable results with specular reflections on relit surfaces.
%
Artifacts from our approach are primarily due to: 1) the mismatch of rendering parameters (e.g., light intensity) used by our neural renderer and Blender; 
2) no synthesized shadowing effects on surfaces with occluded visibility due to the use of pre-integrated environment representation. We intend to address these limitations in our future work.
% Since our model uses the pre-integrated environment representation, which assumes full light visibility on the object surface, our neural renderer cannot synthesize the shadowing effects caused by surface occlusion. 


\begin{table}[t]
    \centering
    \input{table/tab_relight}
    \vspace{-6pt}
    \caption{Quantitative results (PSNR/SSIM) of relighting on three synthetic scenes with 4 light probe images, each with 50 uniformly sampled views}
    \label{tab:relighting}
\vspace{-13pt}
\end{table}

\vspace{-5pt}
\section{Ablation}\label{sec:ablation}

\subsection{The Design Choice of Neural Render}\label{sec:ablation_renderer}
Our proposed neural renderer is able to generalize to scenes with various shapes and materials for achieving reasonable relighting effects.
To justify the design choices, 
we conduct ablation studies from two aspects and present the visual comparisons in Figure \ref{fig:ficus_ablation}. 

\begin{table}[h]
    \centering
    \footnotesize
    \setlength\tabcolsep{1pt}
        \begin{tabularx}{\linewidth}%
        {*{4}{>{\centering\arraybackslash}X:}*{1}{>{\centering\arraybackslash}X}}
\includegraphics[width=\linewidth, trim={3cm 0 2.7cm 8cm},clip]{figures/ficus_ablation/gt.png} &
\includegraphics[width=\linewidth, trim={3cm 0 2.7cm 8cm},clip]{figures/ficus_ablation/un.png} &
\includegraphics[width=\linewidth, trim={3cm 0 2.7cm 8cm},clip]{figures/ficus_ablation/in.png} &
\includegraphics[width=\linewidth, trim={3cm 0 2.7cm 8cm},clip]{figures/ficus_ablation/no.png} &
\includegraphics[width=\linewidth, trim={3cm 0 2.7cm 8cm},clip]{figures/ficus_ablation/wo.png}
\\
Reference & $l^2$-Norm & Inst-Norm &  w/o Norm & w/o Pre-train
\end{tabularx}%
        
    \makeatletter\def\@captype{figure}\makeatother
    \caption{The effects of design choices on scene relighting. 
    Note that models trained with different design options have the same level of rendering quality before relighting.
    \label{fig:ficus_ablation}}
\vspace{-6pt}
\end{table}

\noindent
\textbf{The use of pre-trained rendering MLPs.}
The use of pre-trained diffuse/specular MLPs allows the model to enforce a consistent feature-color mapping across different scenes. Without pre-training, the model cannot synthesize accurate reflections during relighting (``w/o Pre-train" in Fig. \ref{fig:ficus_ablation}).

\noindent
\textbf{Feature normalizations.} The feature normalization (Sec. \ref{sec:normalization}) is another non-trivial part of our design. 
To demonstrate the effectiveness of our $l^2$-Norm, we train two additional models: one without any normalization (``w/o Norm") and one with instance normalization (``Inst-Norm"). 
Compared to our $l^2$-Norm, both w/o Norm and Inst-Norm fail to synthesize the accurate specular highlight in the relit scene (e.g., pot in Fig. \ref{fig:ficus_ablation}).


\subsection{The Effect of Indirect Illuminations.}
\vspace{-3pt}
To demonstrate the effectiveness of our modeling of indirect illuminations, we train our model without specific modeling of indirect illumination (``w/o Indir.") on 4 scenes that contain obvious inter-reflections. Table \ref{tab:indir_vs_dir} and Figure \ref{fig:indir_vs_dir} provide the quantitative and qualitative comparisons, respectively.
The results demonstrate that our additional modeling of indirect illumination can help improve model's rendering quality, as well as the accuracy of surface geometry.

\vspace{-4pt}
\begin{table}[h]
    \centering
    \input{table/tab_indir_ablation}
    % \vspace{-4pt}
    \caption{
    Comparison of PSNR/MAE scores for models trained with and without indirect illumination modeling.}
    \label{tab:indir_vs_dir}
    \vspace{-15pt}
\end{table}

\begin{table}[h]
    \centering
    \footnotesize
    \setlength\tabcolsep{0pt}
        \begin{tabularx}{\linewidth}%
        {*{2}{>{\centering\arraybackslash}X:}*{2}{>{\centering\arraybackslash}X}}
Ground Truth & w/o Indir. & \multicolumn{2}{c}{w/ Indirect Illumination} \\
\includegraphics[width=\linewidth, trim={0 0cm 0 0cm},clip]{figures/indir_vs_dir/gt.png} & 
\includegraphics[width=\linewidth, trim={0 0cm 0 0cm},clip]{figures/indir_vs_dir/wo_indir.png} & 
\includegraphics[width=\linewidth, trim={0 0cm 0 0cm},clip]{figures/indir_vs_dir/w_indir.png} & 
\includegraphics[width=\linewidth, trim={0 0cm 0 0cm},clip]{figures/indir_vs_dir/indir.png} %\\
            % Ground Truth & W/o Indir. & W/ Indir. & Indir. Specular
        \end{tabularx}%
    \vspace{-5pt}
    \makeatletter\def\@captype{figure}\makeatother
    \caption{
    The visual comparison between models trained with and without indirect illumination modeling, the rightmost figure shows our synthesized indirect specular color.
    \label{fig:indir_vs_dir}}
\vspace{-20pt}
\end{table}

% \noindent
% \textbf{The color blending options.}
% \ruofan{may put in supp.}