\vspace{-15pt}
\section{Introduction}
\vspace{-5pt}
Neural Radiance Fields (NeRF) \citea{mildenhall2021nerf} has emerged as a promising approach to many important 3D computer vision and graphics tasks. By integrating deep learning with traditional volume rendering techniques, NeRF enables high-quality 3D scene modeling and reconstruction with photo-realistic rendering quality with significant recent research that has achieved impressive results \citea{mueller2022instant, munkberg2022extracting, poole2022dreamfusion, lin2022magic3d}.
% Its strong ability in representing 3D scenes is favored by a wide range of related 3D vision or graphics tasks, including novel view synthesis \citea{barron2021mip,barron2022mip}, relighting \citea{nerfactor, boss2021neural, munkberg2022extracting}, 
% geometry editing \citea{Yuan22NeRFEditing, xu2022deforming, liang2022spidr}, and 3D content creation \citea{wang2022clip, poole2022dreamfusion, lin2022magic3d}.
While NeRF can synthesize novel views with photo-realistic quality, they often struggle to accurately represent surfaces with high {specular reflectance}. Instead of learning a solid, smooth surface for these regions, NeRF models tend to interpret the view-dependent specular reflections as virtual lights/images underneath the actual surfaces (Figure \ref{fig:artifacts}). This leads to learning inaccurate surface geometry in the shiny regions.
% because the fake internal light sources have to transmit through the internal volume to synthesize view-dependent color effects. 
These virtual lights can also interfere with normal directions and negatively affect performance in inverse rendering tasks such as relighting and environment estimation.
This challenge has also been observed and analyzed by prior work, Verbin \etal \citea{verbin2022ref}, but is yet to be fully addressed. 
%We refer readers to their paper for a more detailed analysis.

% \begin{figure}
%     \centering
%     \includegraphics[width = \linewidth]{figures/toaster_compare.pdf}
%     \caption{A visual comparison to show that \model\ has a more accurate surface reconstruction and a better rendering quality for this challenging shiny object with strong inter-reflection.
%     NVDIFFREC \citea{munkberg2022extracting} exhibits higher rendering quality but less accurate surface, whereas NVDIFFRECMC \citea{hasselgren2022shape} demonstrates better surface quality but lower rendering quality.
%     }
%     \label{fig:toaster_compare}
%     \vspace{-5pt}
% \end{figure}

% There are two approaches to addressing the challenge of learning reflection using neural rendering methods.
% The first approach aims to enhance rendering quality by modeling virtual images or lights underneath the object surface \citea{guo2022nerfren, wu2022scalable, kopanas2022neural}. Vanilla NeRF, like models in this line of research, tends to learn virtual lights underneath the real surface (Fig. \ref{fig:artifacts}).
% While models in this direction use additional MLPs or encodings for virtual lights/images to handle view-dependent color separately and improve rendering quality.
% %%%%%%
% The second approach incorporates more knowledge of the physically based (PB) rendering equation \citea{kajiya1986rendering} to model the actual interaction between light and surface \citea{yariv2020multiview, zhang2021physg, verbin2022ref, munkberg2022extracting}. This approach typically imposes additional constraints on the represented neural surface, such as using signed distance field (SDF) based representation \citea{yariv2021volume, wang2021neus} and surface normal constraints \citea{atzmon2020sal, verbin2022ref}, to ensure the correctness of the learned surface. By effectively separating scene geometry and appearance, methods in this direction also enable some physically based editing, including material editing and scene relighting.
% Prior work typically take two major approaches to address the challenge of learning reflection in neural rendering. The first approach separately represents virtual lights or images underneath the object surface \citea{guo2022nerfren, wu2022scalable, kopanas2022neural}. 
% Methods in this line improve rendering quality by using additional MLPs or encodings for virtual lights/images to handle view-dependent color. 
% The second approach incorporates knowledge of the inverse rendering to model the actual interaction between light and surface \citea{yariv2020multiview, zhang2021physg, munkberg2022extracting}. This also enables appearance editing, such as material editing and scene relighting.
% By effectively separating scene geometry and appearance, methods in this direction also enable some physically based editing, including material editing and scene relighting.
% Methods in this line also impose additional constraints on the represented field to ensure the correctness of the learned surface \citea{yariv2021volume, wang2021neus}. This approach enables physically based editing, including material editing and scene relighting, by effectively separating scene geometry and appearance.
%%%%%%%%%%


% Prior work typically take two major approaches to address the challenge of learning reflection in neural rendering. The first approach separately represents virtual lights or images underneath the object surface \citea{guo2022nerfren, wu2022scalable, kopanas2022neural}. 
% % Methods in this line improve rendering quality by using additional MLPs or encodings for virtual lights/images to handle view-dependent color. 
% The second approach incorporates knowledge of the inverse rendering to model the actual interaction between light and surface \citea{yariv2020multiview, zhang2021physg, munkberg2022extracting}. This also enables appearance editing, such as material editing and scene relighting.


Prior work largely takes one of two major approaches to address the challenge of learning reflection in neural rendering. 
The first approach involves explicitly representing virtual lights or images underneath the surface to account for complex view-dependent appearance \citea{guo2022nerfren, wu2022scalable, kopanas2022neural, tiwary2022orca}. 
The original NeRF \citea{mildenhall2021nerf} and its extensions such as \citea{liu2020neural, barron2021mip, yu2021plenoctrees} also synthesize complex reflections in this way (Figure \ref{fig:artifacts}). 
% More recent work use additional neural models to separately represent virtual lights/images for more higher quality rendering of reflections \citea{guo2022nerfren, wu2022scalable, kopanas2022neural, tiwary2022orca}. 
Although this approach at large can improve rendering quality, it often sacrifices the accuracy of the reconstructed surface and limits the ability to edit scenes, such as relighting.
%
Alternatively, the second approach incorporates knowledge of inverse rendering to model the  interaction between light and surface \citea{zhang2021physg, nerfactor, boss2021neural, munkberg2022extracting}.
By decomposing rendering parameters, these methods can achieve material editing and scene relighting. 
However, these methods often suffer from relatively low rendering quality compared to top-performing NeRF models without full decomposition. This is because the simplified or approximated rendering equation \citea{kajiya1986rendering} used in these models cannot account for all complex rendering effects. 
Ref-NeRF \citea{verbin2022ref} improves the rendering of glossy objects with some decomposition; however its editability (e.g., relighting) is still limited as it does not fully decompose surface color and environment lighting.
In this work, we aim to further improve the quality of neural rendering for glossy surfaces, while retaining accurate surface geometry and the ability to edit scenes. 

% Despite recent improvements in the neural rendering quality for challenging specular reflections, limitations still exist in \emph{two} aspects:
% \ruofan{first state what they do, then point out limitation}
% \textbf{1) View synthesis models that achieve top rendering quality often sacrifice the accuracy of surface geometry.}
% To maximize photo metrics such as PSNR, top-performing NeRF models tend to learn the virtual geometries caused by reflected virtual lights/images to account for the complex view-dependent appearance \citea{guo2022nerfren, wu2022scalable, kopanas2022neural}. 
% (see Figure \ref{fig:toaster_compare} \& \ref{fig:artifacts}).
% While counterintuitive, it is more difficult to learn high-frequency view-dependent color with an accurate surface compared to learning incorrect geometry for boosting the rendering quality. 
% Because 
% As compared to the correct surface, the incorrectly learned surface often has a larger surface area accounting for the changing color from different view directions. 
% As depicted in Figure \ref{fig:toaster_compare}, NVDIFFREC \citea{munkberg2022extracting} exhibits higher rendering quality but less accurate surface, whereas NVDIFFRECMC \citea{hasselgren2022shape} demonstrates better surface quality but lower rendering quality.
% \textbf{2) Inverse rendering models that achieve scene appearance editing have relatively lower rendering quality.} 
% Models with appearance editability often require estimating  decomposed rendering parameters (e.g., environment light, surface materials) to control the rendering results (also known as inverse rendering).
% Previous neural inverse rendering models 
% \citea{zhang2021physg, nerfactor, liang2022spidr, munkberg2022extracting} suffer from lower rendering quality compared to top-performing NeRF models that do not fully decompose rendering parameters. This is because these methods rely on a simplified or approximated PB rendering equation \citea{kajiya1986rendering} that cannot account for all the complex rendering effects.
% that rely on a simplified or approximated PB rendering equation cannot account for all rendering effects, 
% which leads to lower rendering quality compared to top-performing NeRF models that do not decompose rendering parameters fully.  
% \ruofan{Separately discuss RefNeRF}
% Although Ref-NeRF \citea{verbin2022ref}, a top-performing NeRF model for shiny objects, incorporates some rendering decompositions, its editability is limited as its environment light is still entangled with object color, thus it does not support scene relighting.
% Given the limitations in prior works, there is still room for improving the quality of neural rendering for glossy surfaces, while preserving accurate geometry and appearance editability. 
% In this work, we aim to improve the quality of neural rendering for glossy surfaces, while preserving the accurate surface geometry and the ability of scene relighting. 
% enhance the quality of neural rendering for shiny objects 
% and push the boundaries of trade-offs among rendering quality, surface geometry, and appearance editability.


\begin{figure}
    \centering
    \includegraphics[width = 0.9\linewidth]{figures/fake_emitter.pdf}
    \vspace{-10pt}
    \caption{\small Artifacts in rendering surfaces with specular reflections due to the inaccurate interpretation of virtual lights underneath object surfaces (results from mip-NeRF \citea{barron2021mip}).}
    \label{fig:artifacts}
    \vspace{-15pt}
\end{figure}







% Despite recent improvements in the neural rendering quality for challenging specular reflections, there are still limitations coming from \emph{two} major trade-offs:
% \textbf{1) Trade-off between rendering quality and surface geometry.}
% Models that achieve top rendering quality often sacrifice the accuracy of surface geometry.
% While counterintuitive, it is more difficult to learn high-frequency view-dependent color with an accurate surface compared to learning incorrect geometry for boosting the rendering quality. 
% % As compared to the correct surface, the incorrectly learned surface often has a larger surface area accounting for the changing color from different view directions. 
% As depicted in Figure \ref{fig:toaster_compare}, NVDIFFREC \citea{munkberg2022extracting} exhibits higher rendering quality but less accurate surface, whereas NVDIFFRECMC \citea{hasselgren2022shape} demonstrates better surface quality but lower rendering quality.
% %%%%%%%%%%%%
% \textbf{2) Trade-off between rendering quality and editability.}
% Models with more flexible editability often require estimating  decomposed rendering parameters (e.g., environment light, surface materials) to control the rendering results (also known as inverse rendering).
% However, previous neural inverse rendering models \citea{nerfactor, liang2022spidr, munkberg2022extracting, hasselgren2022shape} suffer from lower rendering quality compared to top-performing NeRF models that do not decompose rendering parameters fully. This is because these methods rely on a simplified or approximated PB rendering equation \citea{kajiya1986rendering} that cannot account for all the rendering effects.
% % that rely on a simplified or approximated PB rendering equation cannot account for all rendering effects, 
% % which leads to lower rendering quality compared to top-performing NeRF models that do not decompose rendering parameters fully.  
% Although Ref-NeRF \citea{verbin2022ref}, a top-performing model for shiny objects, incorporates some rendering decompositions, its editability is quite limited as its environment light is still entangled with object color.
% Given the limitations in prior works, there is still room for improving the quality of neural rendering for reflective or glossy surfaces, while preserving accurate geometry and editability. In this work, we aim to push the boundaries of these trade-offs and further enhance the quality of neural rendering for such objects.


% x
% with XX, we can ... (with fig
% how to learn.
% indirecti
% finally, 
% how to this get both.
In this work, we introduce \emph{\model}, a new rendering and modeling framework for high-quality reconstructing and rendering of 3D objects with challenging specular reflections. It comprises two major parts: 
1) a novel neural renderer and 
2) an SDF-based neural surface model that represents the scene and interacts with the neural renderer. %that approximates physically based rendering with 3 decomposed MLP components accounting for environment lighting, diffuse BRDF rendering, and specular BRDF rendering, respectively (Figure \ref{fig:renderer} \ruofan{better on page 2/3}).

Our neural renderer is different from prior works \citea{zhang2021physg, nerfactor, boss2021neural, munkberg2022extracting} that incorporate the rendering equations for inverse rendering as we do not use an explicit form of the rendering equation.
Instead, our neural renderer learns an approximation of physically based rendering (PBR) using 3 decomposed MLPs accounting for environment lighting, diffuse rendering, and specular rendering, respectively (Figure \ref{fig:renderer}). 
This neural renderer is trained using images with various materials and environments synthesized by existing PBR renderers. 
In our renderer, the environment MLP is a decoupled component that is trained to represent the pre-integrated lighting of a specific environment with neural features as output (different from prior methods \citea{boss2021neural, gardner2022rotation, liang2022spidr} that outputs RGB). Thus, our neural renderer can be used for scene relighting and material editing by simply swapping out the environment MLP with the one that is trained to represent the desired environment map.
% Unlike prior methods \citea{boss2021neural, gardner2022rotation, liang2022spidr} that output RGB color, our environment MLP outputs neural features fed into diffuse/specular rendering MLPs.

% The environment MLP in our renderer is a replaceable component.
% Each environment MLP is trained to learn a pre-integrated representation of a specific environment map with the output in the form of neural feature vectors (this is different from prior methods \citea{boss2021neural, gardner2022rotation, liang2022spidr} that output RGB pixel).
% Thus, our neural renderer can be used for scene relighting and material editing by replacing the environment MLP with the one representing the desired environment lighting.

To interact with this neural renderer, we present a new neural surface model that employs an SDF-based neural representation (similar to 
 \citea{yariv2021volume}). We, however, use the diffuse/specular MLPs from the neural renderer in place of the commonly used directional color MLP. 
During training, we only train this SDF model and a new environment MLP without changing the pre-trained diffuse/specular MLPs in the neural renderer.

Finally, shiny surfaces may have inter-reflections that cause apparent view-dependent indirect illumination. 
To model this, we approximate the incoming radiance from inter-reflections by marching rays along the surface-reflected view directions. We additionally propose a color blending model that converts the approximated incoming radiance into indirect illumination and blends it into \model's final rendered color.

% ... \ruofan{how raymarching, additionally, we propose to blend...} and convert it into indirect illumination through our neural renderer. 
% We then propose a color blending strategy that uses a neural blending factor to blend indirect illuminations from reflected rays into \model's final rendered results.

% that decouples the environment light representations from the neural scene representation to enable high rendering quality, accurate surface geometry, and flexible scene relighting.
% Instead, it learns a neural approximation of PB rendering with dedicated shading MLPs through the supervision of existing PB renderer. 
% Our neural renderer enables capturing more complex rendering effects compared to prior solutions.
% We represent the environment light as an MLP conditioned on light direction and material roughness, which allows for scene relighting.
% by simply replacing the environment MLP in our neural renderer.
%
% To account for view-dependent indirect illumination caused by inter-reflection, \model\  synthesizes one-bounce reflections from shiny surfaces by marching surface-reflected rays and uses a neural blending factor to combine direct and indirect lighting into the final specular color. This improves the rendering quality of shiny surfaces with indirect illuminations. 
% With the recent progress in NeRF acceleration \citea{mueller2022instant, fridovich2022plenoxels, sun2022direct}, we are able to make \model's training and rendering within acceptable time, even with these additional rendering modules.
% With recent advances in NeRF acceleration \citea{mueller2022instant}, Our model's training and rendering can be completed within acceptable timeframes, even with these additional modules.

% \noindent
% To summarize, we make the following contributions:
% \begin{enumerate}
%     \item An implicit neural renderer that approximates the physically based renderer with decomposed MLPs.
%     to enable high-quality, editable rendering.
%     \item \model, a neural surface model that uses our neural renderer to learn decomposed neural scenes, thus enabling material editing and scene relighting.
%     % leading to improved rendering quality and surface geometry,
%     % and thus enabling material editing and scene relighting.
%     \item A color blending strategy that employs raymarching along surface-reflected rays and a color blending model to synthesize indirect illumination. 
%     % and a neural blending factor to blend it into final rendered results.
% \end{enumerate}

% \begin{enumerate}
%     \item An implicit neural renderer that learns the prior from existing physically based renderer to produce high-quality renderings of view-dependent color and controllable scene editing.
%     \item \model, a neural surface model that employs our neural renderer to learn decomposed neural scenes with improved rendering quality and surface geometry, while enabling scene relighting and material editing.
%     \item A color blending strategy that synthesizes indirect illumination through additional raymarching along surface-reflected rays and blends synthesized indirect illumination via a neural blending factor.
% \end{enumerate}

We demonstrate the effectiveness of our proposed method on several challenging shiny scenes, and our results show that it is quantitatively and qualitatively on par with or superior to previous methods. Our method achieves this while preserving high-quality decomposed rendering components, including diffuse color, specular color, material roughness, and environment light, which enables physically based scene editing.