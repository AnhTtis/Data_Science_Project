@String(PAMI = {IEEE Trans. Pattern Anal. Mach. Intell.})
@String(IJCV = {Int. J. Comput. Vis.})
@String(CVPR= {IEEE Conf. Comput. Vis. Pattern Recog.})
@String(ICCV= {Int. Conf. Comput. Vis.})
@String(ECCV= {Eur. Conf. Comput. Vis.})
@String(NIPS= {Adv. Neural Inform. Process. Syst.})
@String(ICPR = {Int. Conf. Pattern Recog.})
@String(BMVC= {Brit. Mach. Vis. Conf.})
@String(TOG= {ACM Trans. Graph.})
@String(TIP  = {IEEE Trans. Image Process.})
@String(TVCG  = {IEEE Trans. Vis. Comput. Graph.})
@String(TMM  = {IEEE Trans. Multimedia})
@String(ACMMM= {ACM Int. Conf. Multimedia})
@String(ICME = {Int. Conf. Multimedia and Expo})
@String(ICASSP=	{ICASSP})
@String(ICIP = {IEEE Int. Conf. Image Process.})
@String(ACCV  = {ACCV})
@String(ICLR = {Int. Conf. Learn. Represent.})
@String(IJCAI = {IJCAI})
@String(PR   = {Pattern Recognition})
@String(AAAI = {AAAI})
@String(CVPRW= {IEEE Conf. Comput. Vis. Pattern Recog. Worksh.})
@String(CSVT = {IEEE Trans. Circuit Syst. Video Technol.})

@String(SPL	= {IEEE Sign. Process. Letters})
@String(VR   = {Vis. Res.})
@String(JOV	 = {J. Vis.})
@String(TVC  = {The Vis. Comput.})
@String(JCST  = {J. Comput. Sci. Tech.})
@String(CGF  = {Comput. Graph. Forum})
@String(CVM = {Computational Visual Media})


@String(PAMI  = {IEEE TPAMI})
@String(IJCV  = {IJCV})
@String(CVPR  = {CVPR})
@String(ICCV  = {ICCV})
@String(ECCV  = {ECCV})
@String(NIPS  = {NeurIPS})
@String(ICPR  = {ICPR})
@String(BMVC  =	{BMVC})
@String(TOG   = {ACM TOG})
@String(TIP   = {IEEE TIP})
@String(TVCG  = {IEEE TVCG})
@String(TCSVT = {IEEE TCSVT})
@String(TMM   =	{IEEE TMM})
@String(ACMMM = {ACM MM})
@String(ICME  =	{ICME})
@String(ICASSP=	{ICASSP})
@String(ICIP  = {ICIP})
@String(ACCV  = {ACCV})
@String(ICLR  = {ICLR})
@String(IJCAI = {IJCAI})
@String(PR = {PR})
@String(AAAI = {AAAI})
@String(CVPRW= {CVPRW})
@String(CSVT = {IEEE TCSVT})

@article{dosovitskiy2020image,
  title={An image is worth 16x16 words: Transformers for image recognition at scale},
  author={Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and others},
  journal={arXiv preprint arXiv:2010.11929},
  year={2020}
}


@inproceedings{TransMeetTracker,
  title={Transformer meets tracker: Exploiting temporal context for robust visual tracking},
  author={Wang, Ning and Zhou, Wengang and Wang, Jie and Li, Houqiang},
  booktitle={CVPR},
  pages={1571--1580},
  year={2021}
}
@article{zhao2021trtr,
  title={Trtr: Visual tracking with transformer},
  author={Zhao, Moju and Okada, Kei and Inaba, Masayuki},
  journal={arXiv preprint arXiv:2105.03817},
  year={2021}
}

@inproceedings{chen2021transformer,
  title={Transformer tracking},
  author={Chen, Xin and Yan, Bin and Zhu, Jiawen and Wang, Dong and Yang, Xiaoyun and Lu, Huchuan},
  booktitle={CVPR},
  pages={8126--8135},
  year={2021}
}


@inproceedings{stark,
  title={Learning spatio-temporal transformer for visual tracking},
  author={Yan, Bin and Peng, Houwen and Fu, Jianlong and Wang, Dong and Lu, Huchuan},
  booktitle={ICCV},
  pages={10448--10457},
  year={2021}
}

@inproceedings{li2017tracking,
  title={Tracking by natural language specification},
  author={Li, Zhenyang and Tao, Ran and Gavves, Efstratios and Snoek, Cees GM and Smeulders, Arnold WM},
  booktitle={CVPR},
  pages={6495--6503},
  year={2017}
}

@inproceedings{cui2022mixformer,
  title={MixFormer: End-to-End Tracking with Iterative Mixed Attention},
  author={Cui, Yutao and Jiang, Cheng and Wang, Limin and Wu, Gangshan},
  booktitle={CVPR},
  pages={13608--13618},
  year={2022}
}
@inproceedings{ostrack,
  title={Joint Feature Learning and Relation Modeling for Tracking: A One-Stream Framework},
  author={Ye, Botao and Chang, Hong and Ma, Bingpeng and Shan, Shiguang},
  booktitle={ECCV},
  pages={341--357},
  year={2022}
}
@article{wang2018describe,
  title={Describe and attend to track: Learning natural language guided structural representation and visual attention for object tracking},
  author={Wang, Xiao and Li, Chenglong and Yang, Rui and Zhang, Tianzhu and Tang, Jin and Luo, Bin},
  journal={arXiv preprint arXiv:1811.10014},
  year={2018}
}
@article{GTI,
  title={Grounding-tracking-integration},
  author={Yang, Zhengyuan and Kumar, Tushar and Chen, Tianlang and Su, Jingsong and Luo, Jiebo},
  journal={IEEE TCSVT},
  volume={31},
  number={9},
  pages={3433--3443},
  year={2020}
}
@inproceedings{feng2020real,
  title={Real-time visual object tracking with natural language description},
  author={Feng, Qi and Ablavsky, Vitaly and Bai, Qinxun and Li, Guorong and Sclaroff, Stan},
  booktitle={WACV},
  pages={700--709},
  year={2020}
}
@inproceedings{TNL2K,
  title={Towards more flexible and accurate object tracking with natural language: Algorithms and benchmark},
  author={Wang, Xiao and Shu, Xiujun and Zhang, Zhipeng and Jiang, Bo and Wang, Yaowei and Tian, Yonghong and Wu, Feng},
  booktitle={CVPR},
  pages={13763--13773},
  year={2021}
}
@inproceedings{li2022cross,
  title={Cross-Modal Target Retrieval for Tracking by Natural Language},
  author={Li, Yihao and Yu, Jun and Cai, Zhongpeng and Pan, Yuwen},
  booktitle={CVPR},
  pages={4931--4940},
  year={2022}
}
@inproceedings{feng2021siamese,
  title={Siamese natural language tracker: Tracking by natural language descriptions with siamese trackers},
  author={Feng, Qi and Ablavsky, Vitaly and Bai, Qinxun and Sclaroff, Stan},
  booktitle={CVPR},
  pages={5851--5860},
  year={2021}
}
@article{guo2022divert,
  title={Divert More Attention to Vision-Language Tracking},
  author={Guo, Mingzhe and Zhang, Zhipeng and Fan, Heng and Jing, Liping},
  journal={arXiv preprint arXiv:2207.01076},
  year={2022}
}

@article{fan2021lasot,
  title={Lasot: A high-quality large-scale single object tracking benchmark},
  author={Fan, Heng and Bai, Hexin and Lin, Liting and Yang, Fan and Chu, Peng and Deng, Ge and Yu, Sijia and Huang, Mingzhen and Liu, Juehuan and Xu, Yong and others},
  journal={IJCV},
  volume={129},
  number={2},
  pages={439--461},
  year={2021}
}
@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={NeurIPS},
  volume={30},
  year={2017}
}
@article{devlin2018bert,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018}
}
@inproceedings{SWIN,
  title={Swin transformer: Hierarchical vision transformer using shifted windows},
  author={Liu, Ze and Lin, Yutong and Cao, Yue and Hu, Han and Wei, Yixuan and Zhang, Zheng and Lin, Stephen and Guo, Baining},
  booktitle={ICCV},
  pages={10012--10022},
  year={2021}
}
@inproceedings{li2019siamrpn++,
  title={Siamrpn++: Evolution of siamese visual tracking with very deep networks},
  author={Li, Bo and Wu, Wei and Wang, Qiang and Zhang, Fangyi and Xing, Junliang and Yan, Junjie},
  booktitle={CVPR},
  pages={4282--4291},
  year={2019}
}
@inproceedings{2019onestagevg,
  title={A fast and accurate one-stage approach to visual grounding},
  author={Yang, Zhengyuan and Gong, Boqing and Wang, Liwei and Huang, Wenbing and Yu, Dong and Luo, Jiebo},
  booktitle={ICCV},
  pages={4683--4693},
  year={2019}
}

@article{yilmaz2006object,
  title={Object tracking: A survey},
  author={Yilmaz, Alper and Javed, Omar and Shah, Mubarak},
  journal={Acm computing surveys},
  volume={38},
  number={4},
  pages={13--es},
  year={2006}
}
@article{fasterrcnn,
  title={Faster r-cnn: Towards real-time object detection with region proposal networks},
  author={Ren, Shaoqing and He, Kaiming and Girshick, Ross and Sun, Jian},
  journal={NeurIPS},
  volume={28},
  year={2015}
}
@inproceedings{OTB2013,
  title={Online object tracking: A benchmark},
  author={Wu, Yi and Lim, Jongwoo and Yang, Ming-Hsuan},
  booktitle={CVPR},
  pages={2411--2418},
  year={2013}
}
@ARTICLE{OTB2015,
  author={Wu, Yi and Lim, Jongwoo and Yang, Ming-Hsuan},
  journal={IEEE TPAMI}, 
  title={Object Tracking Benchmark}, 
  year={2015},
  volume={37},
  number={9},
  pages={1834-1848}
}
@inproceedings{TrackingNet,
  title={Trackingnet: A large-scale dataset and benchmark for object tracking in the wild},
  author={Muller, Matthias and Bibi, Adel and Giancola, Silvio and Alsubaihi, Salman and Ghanem, Bernard},
  booktitle={ECCV},
  pages={300--317},
  year={2018}
}
@inproceedings{LASOT,
  title={Lasot: A high-quality benchmark for large-scale single object tracking},
  author={Fan, Heng and Lin, Liting and Yang, Fan and Chu, Peng and Deng, Ge and Yu, Sijia and Bai, Hexin and Xu, Yong and Liao, Chunyuan and Ling, Haibin},
  booktitle={CVPR},
  pages={5374--5383},
  year={2019}
}
@inproceedings{REFCOCO,
  title={Generation and comprehension of unambiguous object descriptions},
  author={Mao, Junhua and Huang, Jonathan and Toshev, Alexander and Camburu, Oana and Yuille, Alan L and Murphy, Kevin},
  booktitle={CVPR},
  pages={11--20},
  year={2016}
}
@inproceedings{transvg,
  title={Transvg: End-to-end visual grounding with transformers},
  author={Deng, Jiajun and Yang, Zhengyuan and Chen, Tianlang and Zhou, Wengang and Li, Houqiang},
  booktitle={ICCV},
  pages={1769--1779},
  year={2021}
}
@inproceedings{VLTVG,
  title={Improving Visual Grounding with Visual-Linguistic Verification and Iterative Reasoning},
  author={Yang, Li and Xu, Yan and Yuan, Chunfeng and Liu, Wei and Li, Bing and Hu, Weiming},
  booktitle={CVPR},
  pages={9499--9508},
  year={2022}
}
@article{swintrack,
  title={Swintrack: A simple and strong baseline for transformer tracking},
  author={Lin, Liting and Fan, Heng and Xu, Yong and Ling, Haibin},
  journal={arXiv preprint arXiv:2112.00995},
  year={2021}
}
@inproceedings{NMTree,
  title={Learning to assemble neural module tree networks for visual grounding},
  author={Liu, Daqing and Zhang, Hanwang and Wu, Feng and Zha, Zheng-Jun},
  booktitle={ICCV},
  pages={4673--4682},
  year={2019}
}
@inproceedings{LBYL-Net,
  title={Look before you leap: Learning landmark features for one-stage visual grounding},
  author={Huang, Binbin and Lian, Dongze and Luo, Weixin and Gao, Shenghua},
  booktitle={CVPR},
  pages={16888--16897},
  year={2021}
}
@inproceedings{ReSC-Large,
  title={Improving one-stage visual grounding by recursive sub-query construction},
  author={Yang, Zhengyuan and Chen, Tianlang and Wang, Liwei and Luo, Jiebo},
  booktitle={ECCV},
  pages={387--404},
  year={2020}
}

@inproceedings{DETR,
  title={End-to-end object detection with transformers},
  author={Carion, Nicolas and Massa, Francisco and Synnaeve, Gabriel and Usunier, Nicolas and Kirillov, Alexander and Zagoruyko, Sergey},
  booktitle={ECCV},
  pages={213--229},
  year={2020}
}
@inproceedings{AutoMatch,
  title={Learn to match: Automatic matching network design for visual tracking},
  author={Zhang, Zhipeng and Liu, Yihao and Wang, Xiao and Li, Bing and Hu, Weiming},
  booktitle={ICCV},
  pages={13339--13348},
  year={2021}
}
@inproceedings{keeptrack,
  title={Learning target candidate association to keep track of what not to track},
  author={Mayer, Christoph and Danelljan, Martin and Paudel, Danda Pani and Van Gool, Luc},
  booktitle={ICCV},
  pages={13444--13454},
  year={2021}
}
@InProceedings{SAOT,
    author    = {Zhou, Zikun and Pei, Wenjie and Li, Xin and Wang, Hongpeng and Zheng, Feng and He, Zhenyu},
    title     = {Saliency-Associated Object Tracking},
    booktitle = {ICCV},
    month     = {October},
    year      = {2021},
    pages     = {9866-9875}
}
@article{imagenet,
  title={Imagenet classification with deep convolutional neural networks},
  author={Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
  journal={Communications of the ACM},
  volume={60},
  number={6},
  pages={84--90},
  year={2017},
  publisher={AcM New York, NY, USA}
}

@inproceedings{GIOU,
  title={Generalized intersection over union: A metric and a loss for bounding box regression},
  author={Rezatofighi, Hamid and Tsoi, Nathan and Gwak, JunYoung and Sadeghian, Amir and Reid, Ian and Savarese, Silvio},
  booktitle={CVPR},
  pages={658--666},
  year={2019}
}

@inproceedings{GTELT,
  title={Global tracking via ensemble of local trackers},
  author={Zhou, Zikun and Chen, Jianqiu and Pei, Wenjie and Mao, Kaige and Wang, Hongpeng and He, Zhenyu},
  booktitle={CVPR},
  pages={8761--8770},
  year={2022}
}

@inproceedings{ToMP,
  title={Transforming model prediction for tracking},
  author={Mayer, Christoph and Danelljan, Martin and Bhat, Goutam and Paul, Matthieu and Paudel, Danda Pani and Yu, Fisher and Van Gool, Luc},
  booktitle={CVPR},
  pages={8731--8740},
  year={2022}
}

@inproceedings{CSWinTT,
  title={Transformer tracking with cyclic shifting window attention},
  author={Song, Zikai and Yu, Junqing and Chen, Yi-Ping Phoebe and Yang, Wei},
  booktitle={CVPR},
  pages={8791--8800},
  year={2022}
}

@inproceedings{ECO,
  title={Eco: Efficient convolution operators for tracking},
  author={Danelljan, Martin and Bhat, Goutam and Shahbaz Khan, Fahad and Felsberg, Michael},
  booktitle={CVPR},
  pages={6638--6646},
  year={2017}
}

@inproceedings{PrDiMP,
  title={Probabilistic regression for visual tracking},
  author={Danelljan, Martin and Gool, Luc Van and Timofte, Radu},
  booktitle={CVPR},
  pages={7183--7192},
  year={2020}
}

@inproceedings{DiMP,
  title={Learning discriminative model prediction for tracking},
  author={Bhat, Goutam and Danelljan, Martin and Gool, Luc Van and Timofte, Radu},
  booktitle={ICCV},
  pages={6182--6191},
  year={2019}
}


@inproceedings{DaSiam,
  title={Distractor-aware siamese networks for visual object tracking},
  author={Zhu, Zheng and Wang, Qiang and Li, Bo and Wu, Wei and Yan, Junjie and Hu, Weiming},
  booktitle={ECCV},
  pages={101--117},
  year={2018}
}

@inproceedings{Deformerble-DETR,
  title={Deformable detr: Deformable transformers for end-to-end object detection},
  author={Zhu, Xizhou and Su, Weijie and Lu, Lewei and Li, Bin and Wang, Xiaogang and Dai, Jifeng},
  booktitle={ICLR},
  year={2020}
}
