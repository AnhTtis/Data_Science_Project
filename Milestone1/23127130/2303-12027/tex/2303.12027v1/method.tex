In this section, we present the proposed joint visual grounding and tracking framework, which implements the two core steps, \ie~grounding and tracking, of tracking by natural language specification with a single end-to-end model. 
To this end, our framework is designed to accommodate the relation modeling between the test image and different input references of grounding and tracking. 

\subsection{Joint grounding and tracking framework}
Figure~\ref{Fig:framework} illustrates the proposed joint visual grounding and tracking framework, which mainly consists of the language and vision encoders, the multi-source relation modeling module, the target decoder, the semantic-guided temporal modeling module, and the localization head.

Given the input references and the test image, the language and vision encoders first embed them into the specific feature spaces, yielding the token embeddings of the input words or image patches. 
Then we adopt two linear projection layers to project the language and vision token embeddings to the latent spaces with the same dimension. 
After that, the projected embeddings are fed into the multi-source relation modeling module, which models the relations between the multi-source references and the test image to enhance the target information in the embeddings of the test image. 
On top of the enhanced embeddings of the test image, the target decoder followed by the localization head is used to predict the bounding box of the referred target. 
For the visual grounding task, in which no template is available, we use zero padding tokens as placeholders to fill in the missing embeddings corresponding to the template in the input of the multi-source relation modeling module. 
During visual tracking, the semantics-guided temporal modeling module will generate a temporal clue for the target decoder, enabling our model to exploit historical target states.

The tracking process with natural language specification using our framework can be summarized as: (1) At the first frame, our model takes as input the natural language, the zero padding tokens, and the test image (the first frame) to perform visual grounding, and then the target template can be obtained based on the grounding result; (2) At every subsequent frame, our model takes as input the natural language, the template, and the test image (also called the search image) to perform language-assisted visual tracking.

\subsection{Vision and language feature extraction}
\noindent\textbf{Language encoder.}
Since being proposed, the transformer~\cite{vaswani2017attention} model has achieved great success in natural language processing. 
Thus, we opt for the classic language transformer model BERT~\cite{devlin2018bert} as the language encoder in our framework.
Given a natural language query $L_q$, we first tokenize the sentences and append a CLS token and a SEP token at the beginning and end of the tokenized language query, respectively, yielding a sequence of tokens $T = \{ \text{CLS}, \bm{t}_1,\bm{t}_2, \cdots, \bm{t}_N,\text{SEP}\}$, where $N$ is the max length of the language query.
Then we feed the above sequence into the language encoder and obtain the language token embeddings $\bm{F}_l \in  \mathbb{R}^{C_l \times (N+2)}$, where $C_l=768$ is the dimension of the output embedding.

\noindent\textbf{Vision encoder.}
Similar to the language encoder, we also opt for the transformer model to learn the vision feature representation. 
In particular, we adopt the vanilla Swin-Transformer~\cite{SWIN} as our vision encoder for its excellent performance of image feature learning. 
Herein we only remain the first three stages of Swin-Transformer, as the output feature resolution of the last stage is too small. 
For the grounding process, given a test image $I_t \in \mathbb{R}^{3 \times H_t \times W_t}$, we feed it into the vision encoder and then flatten the output features to obtain the token embeddings $\bm{F}_t \in \mathbb{R}^{C_v \times L_g}$.
Similarly, for the tracking process, given the template image $I_z \in \mathbb{R}^{3 \times H_z \times W_z}$ and test image $I_t \in \mathbb{R}^{3 \times H_t \times W_t}$, we feed them into the vision encoder and the flatten the output features to get their token embeddings $\bm{F}_z \in \mathbb{R}^{C_v \times L_z}$ and $\bm{F}_t \in \mathbb{R}^{C_v \times L_t}$, respectively. Herein $C_v = 512$.

\subsection{Multi-source relation modeling}\label{Sec:MSRM}
Unifying the visual grounding and tracking sub-tasks together requires the algorithm to model the relations between the test image and the different references. 
Thus, we propose the multi-source relation modeling module, which accommodates the different references for grounding and tracking, to achieve cross-modality and cross-time relation modeling for tracking by the natural language specification. 
Considering the ability of the self-attention operation to capture the global dependencies, we exploit a transformer encoder that mainly stacks the self-attention layer to perform relation modeling instead of using complex cross-modality fusion or cross-correlation methods.

For visual grounding where the template embeddings are not available, we use zero padding tensors $\bm{P}_{o}$ as placeholders to fill the missing template embeddings. 
We also set the mask value for zero padding tensors as 1 to mask them during the computation of self-attention in case they pollute the other useful information. 
Before performing relation modeling, we first use the language and vision projection layers to process the corresponding token embeddings to unify the dimension of the token embeddings from different modalities. 
The projected language embeddings, template image embeddings, and test image embedding are denoted by $\bm{P}_l= [\bm{p}_l^1, \bm{p}_l^2, \dotsc, \bm{p}_l^{N_l}]$, $\bm{P}_z= [\bm{p}_z^1, \bm{p}_z^2, \dotsc, \bm{p}_z^{N_z}]$, $\bm{P}_t= [\bm{p}_t^1, \bm{p}_t^2, \dotsc, \bm{p}_t^{N_t}]$, respectively.
Then, we concatenate the embeddings of the references ($\bm{P}_l$ and $\bm{P}_{o}$ for grounding, and $\bm{P}_l$ and $\bm{P}_{z}$ for tracking) and the embeddings of the test image $\bm{P}_t$, and feed them into the transformer encoder to model the multi-source relation for grounding or tracking, which can be formulated as:
\begin{align}
\setlength{\abovedisplayskip}{3pt}
\setlength{\belowdisplayskip}{3pt}
\label{Eq: enc}
    [\bm{h}_l, \bm{h}_{o}, \bm{h}_t] &= \Phi_{enc}([\bm{P}_l,\bm{P}_{o},\bm{P}_{t}]);\\
    [\bm{h}_l, \bm{h}_z, \bm{h}_t] &= \Phi_{enc}([\bm{P}_l,\bm{P}_{z},\bm{P}_{t}]).
\end{align}
Herein the output embeddings corresponding to the test image, \ie~$\bm{h}_t$, will be further used for target decoding and bounding box prediction. 
Besides, we add learnable position embeddings to $[\bm{P}_l,\bm{P}_{o},\bm{P}_{t}]$ and $[\bm{P}_l,\bm{P}_{z},\bm{P}_{t}]$ to retain the positional information.

\subsection{Target decoder and localization head}
\noindent\textbf{Target decoder.}
To further enhance the target information in $\bm{h}_t$, we follow STARK~\cite{stark} and use a target decoder(TDec) and a Target Query(TQ) to learn the discriminative target embeddings. 
The target decoder takes $\bm{h}_t$ and the target query as inputs. 
For visual grounding, the target query is an offline learned embedding that contains the potential target information. 
For visual tracking, we add the temporal clue outputted by the semantics-guided temporal modeling module to the offline learned embedding to obtain a target query with recent target appearance priors.

\noindent\textbf{Localization head.}
To achieve the unification of grounding and tracking, we apply a shared localization head for bounding box prediction. 
Particularly, we use the localization head proposed in~\cite{stark} due to its great performance and efficiency. 
As shown in Figure~\ref{Fig:framework}, we first compute the similarity between the output of the target decoder and $\bm{h}_t$ token by token, and then we use a residual connection to enhance the target-related regions. Finally, the localization head is constructed to predict the target bounding box.
\input{Figures/fig_temporal_module.tex}
\subsection{Semantics-guided temporal modeling}\label{Sec:SGTM}
Although grounding and tracking share the same relation modeling process, there still exist some differences between the two tasks. 
More concretely, grounding is performed on a standalone image but tracking is executed on consecutive video which means there exist historical appearances of the target for exploitation. 
To improve the adaptability to the variations of the target using the temporal information, we propose a Semantics-Guided Temporal Modeling (SGTM) module. It is designed to learn the temporal clue about recent target appearances from historical target states with semantics from the natural language as guidance.
Figure~\ref{Fig:temporal_module} shows the architecture of the SGTM module, which mainly consists of a transformer encoder and decoder. 

After the tracking process of each frame, we perform region of interest (RoI) pooling~\cite{fasterrcnn} on $\bm{h}_t$ according to the predicted box to obtain the target region features which are further flattened to obtain the historical target patch embeddings.
Besides, we also take into account the global semantic representation $\bm{h}_l^1$ of the Natural Language (NL) outputted by the multi-source relation modeling module, which is called the NL CLS token denoted as $\bm{h}_l^{CLS}$.
We directly concatenate the NL CLS token with the historical target patch embeddings and feed them into the encoder.
Herein, the encoder is used to enhance the token embeddings corresponding to the target region and suppress those corresponding to the noise region with the guidance of NL CLS.
After that, we utilize a decoder to compute cross-attention between enhanced target patch embeddings and a temporal query which is a learnable vector.
The temporal query can attend to the language context and historical target embeddings, thus learning a robust historical target representation for future tracking.

\subsection{End-to-end learning and inference}
\vspace{1mm}
\noindent\textbf{End-to-end learning.}
Every training sample is composed of a language query and a pair of images (a grounding patch and a tracking patch). During training, the forward propagation for one sample is divided into two steps: grounding and tracking.
In the grounding step, the network takes as input the language query and the grounding patch, and outputs the predicted box.
Then we crop the target region according to the predicted box of the grounding process as the template for the tracking step.
In the tracking step, the model takes as input the language query, the template, and the tracking patch (\ie~the search patch) and outputs the predicted boxes.
For each step, we use the GIoU~\cite{GIOU} Loss and L1 Loss to supervise the learning of the model, and the losses of the two steps are directly added.

\noindent\textbf{Inference.}
During inference, we perform grounding on the first frame, then accordingly crop the target region as the template.
Besides, we also generate the token embeddings of the template via RoI pooling and flatten operations, which are further fed into the SGTM module.
In each subsequent frame, we conduct tracking based on the language query, the template, and the temporal clue that SGTM learns.
Specifically, our model also can be initialized by the natural language and bounding box specifications together, which is validated to achieve better tracking performance.


