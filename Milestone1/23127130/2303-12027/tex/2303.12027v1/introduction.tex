\begin{figure}[t]
\centering
\includegraphics[width=0.99\columnwidth]{Figures/Introduction_comparation.pdf}
\vspace{-2mm}
\caption{Illustration of two different frameworks for tracking by natural language specification. (a) The separated visual grounding and tracking framework, which consists of two independent models for visual grounding and tracking, respectively. (b) The proposed joint visual grounding and tracking framework, which employs a single model for both visual grounding and tracking.}
\label{Fig:introduction}
\vspace{-2mm}
\end{figure}
Tracking by natural language specification~\cite{li2017tracking} is a task aiming to locate the target in every frame of a sequence according to the state specified by the natural language.
Compared with the classical tracking task~\cite{yilmaz2006object, OTB2013, OTB2015, TrackingNet} using a bounding box to specify the target of interest, tracking by natural language specification provides a novel human-machine interaction manner for visual tracking. 
In addition, the natural language specification also has two advantages for the tracking task compared to the bounding box specification. 
First, the bounding box only provides a static representation of the target state, while the natural language can describe the variation of the target for the long term. 
Second, the bounding box contains no direct semantics about the target and even results in ambiguity~\cite{TNL2K}, but the natural language can provide clear semantics of the target used for assisting the tracker to recognize the target.
In spite of the above merits, tracking by natural language specification has not been fully explored.

Most existing solutions~\cite{li2017tracking, GTI, TNL2K, li2022cross} for this task could be generally divided into two steps: (1) localizing the target of interest according to the natural language description in the first frame, \ie~visual grounding; (2) tracking the localized target in the subsequent frames based on the target state predicted in the first frame, \ie~visual tracking. 
Accordingly, many algorithms~\cite{GTI, TNL2K, li2022cross} are designed to incorporate a grounding model and a tracking model, as shown in Figure~\ref{Fig:introduction}(a). 
Herein the grounding model performs relation modeling between the language and vision signal to localize the target, while the tracking model performs relation modeling between the template and search region to localize the target. 
The drawback of this framework is that the grounding model and the tracking model are two separate parts and work independently, ignoring the connections between the two steps. 
Besides, many of them~\cite{GTI, TNL2K, li2022cross} choose to adopt the off-the-shelf grounding model~\cite{2019onestagevg} or tracking model~\cite{li2019siamrpn++} to construct their framework, which means that the overall framework cannot be trained end-to-end.

The tracking model in most existing algorithms~\cite{GTI, TNL2K,li2022cross} predicts the target state only based on the template, overlooking the natural language description. 
By contrast, the tracking mechanism that considers both the target template and the natural language for predicting the target state has proven to have great potential~\cite{li2017tracking,wang2018describe, feng2021siamese, guo2022divert}. 
Such a tracking mechanism requires the tracking model to own the ability to simultaneously model the vision-language relation and the template-search region relation. 
Inspired by this tracking mechanism, we come up with the idea to build a joint relation modeling model to accomplish the above-mentioned two-step pipeline. 
Herein a joint relation modeling model can naturally connect visual grounding and tracking together and also can be trained end-to-end.
\input{Figures/fig_method.tex}

To this end, we propose a joint visual grounding and tracking framework for tracking by natural language specification, as shown in Figure~\ref{Fig:introduction}(b). Specifically, we look at these two tasks from a unified perspective and reformulate them as a unified one: localizing the referred target according to the given visual-language references. 
For visual grounding, the reference information is the natural language, while for visual tracking, the reference information is the natural language and historical target patch (usually called template). 
Thus, the crux of this unified task is to model the multi-source relations between the input references and the test image, which involve the cross-modality (visual and language) relation and the cross-time (historical target patch and current search image) relation. 
To deal with this issue, we introduce a transformer-based multi-source relation modeling module, which is flexible enough to accommodate the different references for grounding and tracking, to model the above relations effectively. It allows our method to switch between grounding and tracking according to different inputs. 

In addition, to improve the adaptability to the variations of the target, we resort to the historical prediction as they provide the temporal clue about the recent target appearance and propose a temporal modeling module to achieve this purpose. 
Considering that the natural language specification contains the global semantic information of the target, we use it as guidance to assist the temporal modeling module to focus on the target region instead of the noise in the previous prediction results. 

To conclude, we make the following contributions: (1) we propose a joint visual grounding and tracking framework for tracking by natural language specification, which unifies tracking and grounding as a unified task and can accommodate the different references of the grounding and tracking processes; (2) we propose a semantics-guided temporal modeling module to provide a temporal clue based on historical predictions for our joint model, which improves the adaptability of our method to the appearance variations of the target; (3) we achieve favorable performance against state-of-the-art algorithms on three natural language tracking datasets and one visual grounding dataset, which demonstrates the effectiveness of our approach.
