\documentclass{article}
\usepackage{structure}

\title{On the Gap between Hereditary Discrepancy and the Determinant Lower Bound}
\author{Lily Li\thanks{Department of Computer Science, University of
    Toronto, email:\href{mailto:xinyuan@cs.toronto.edu}{xinyuan@cs.toronto.edu}}, Aleksandar Nikolov\thanks{Department of Computer Science, University of
    Toronto, email:\href{mailto:anikolov@cs.toronto.edu}{anikolov@cs.toronto.edu}}}
\date{}

\begin{document}

\maketitle

\begin{abstract}
The determinant lower bound of Lovasz, Spencer, and Vesztergombi [European Journal of Combinatorics, 1986] is a powerful general way to prove lower bounds on the hereditary discrepancy of a set system. In their paper, Lovasz, Spencer, and Vesztergombi asked if hereditary discrepancy can also be bounded from above by a function of the hereditary discrepancy. This was answered in the negative by Hoffman, and the largest known multiplicative gap between the two quantities for a set system of $m$ substes of a universe of size $n$ is on the order of $\max\{\log n, \sqrt{\log m}\}$. On the other hand, building on work of Matou\v{s}ek [Proceedings of the AMS, 2013], recently Jiang and Reis [SOSA, 2022] showed that this gap is always bounded up to constants by $\sqrt{\log(m)\log(n)}$. This is tight when $m$ is polynomial in $n$, but leaves open what happens for large $m$. We show that the bound of Jiang and Reis is tight for nearly the entire range of $m$. Our proof relies on a technique of amplifying discrepancy via taking Kronecker products, and on discrepancy lower bounds for a set system derived from the discrete Haar basis. 
\end{abstract}



\section{Introduction}
Let $X$ be a finite universe of elements, $\mathcal{S}$ a collection of subsets of $X$ called a \emph{set system}, and $\chi: X \rightarrow \{\pm 1\}$ a $\pm 1$ coloring of the elements of $X$. The discrepancy of $\mathcal{S}$ with respect to $\chi$, denoted $\disc(\mathcal{S}, \chi)$, is defined as $\max_{S \in \mathcal{S}}||\chi^{-1}(1)\cap S| - |\chi^{-1}(-1)\cap S||$ i.e.\ the largest difference between the number of elements colored differently in any set $S \in \mathcal{S}$. The \emph{discrepancy of $\mathcal{S}$}, denoted $\disc(\mathcal{S})$, is then $\min_{\chi: X \rightarrow \{\pm 1\}}\disc(\mathcal{S}, \chi)$ i.e.\ among all $\pm 1$ colorings of the elements of $X$, the least unequal we can make the most unequal set of $\mathcal{S}$. If $|X| = n$ and $|\mathcal{S}| = m$, we will let $X = [n]$ and $\mathcal{S} = \{S_1, ..., S_m\}$ w.l.o.g. For each set system $\mathcal{S}$, we can construct an associated $m \times n$ incidence matrix $\mm{A}_{\mathcal{S}}$: the entry in row $i$ and column $j$ of $\mm{A}_{\mathcal{S}}$ is equal to one if element $j$ is in $S_i$ and zero otherwise. We define the \emph{discrepancy of a real-valued matrix \(\mm{A}\)} as 
\[\disc(\mm{A}) \coloneqq \min_{\mm{x} \in \{\pm 1\}^{n}} \norm{\mm{A}\mm{x}}_{\infty}\] 
where $\norm{\cdot}_{\infty}$ denotes the $L_\infty$-norm of a vector. Using this definition, we see that $\disc(\mm{A}_{\mathcal{S}}) = \disc(\mathcal{S})$. Throughout this exposition, we will use $\mathcal{S}$ and its indicator matrix $\mm{A}_{\mathcal{S}}$ interchangeably.

We would like discrepancy to be a robust quantity, but it can be sensitive to slight modifications to the incidence matrix e.g. the discrepancy of the $[\mm{A}, \mm{A}]$ is always zero regardless of $\disc(\mm{A})$. As an alternative, we define the hereditary discrepancy of a matrix $\mm{A}$ as
\[\herdisc(\mm{A}) \coloneqq \max_{\mm{B} \subseteq \mm{A}}\disc(\mm{B})\] 
where $\mm{B}$ is a sub-matrix of $\mm{A}$. In a sense, this definition generalizes total unimodularity. It is easy to show that totally unimodular matrices (TUM)\footnote{A matrix $\mm{A}$ is TUM if every square submatrix of $\mm{A}$ has determinant in $\{-1, 0, 1\}$. A linear systems of the form $\mm{A}\mm{x} \geq \mm{b}$ for TUM $\mm{A}$, integral $\mm{b}$, and $0 \le \mm{x}$ has an integral polyhedron as its feasible region.} have hereditary discrepancy at most one~\cite{schrijver1998theory}. Further, a result of \cite{ghouila1962caracterisation} states that the set of hereditary discrepancy one matrices with entries in $\{-1, 0, 1\}$ is exactly the set of TUM matrices. 

An important early work in discrepancy theory of~\cite{lovasz1986discrepancy} showed that the determinant lower bound of $\mm{A}$, defined as
\[\detlb(\mm{A}) \coloneqq \max_{k}\max_{[\mm{B}]_{k\times k}}\left|\mathrm{det}(\mm{B})\right|^{1/k}\]
where $\mm{B}$ is a $k\times k$ sub-matrix of $\mm{A}$, satisfies $2\cdot\herdisc(\mm{A}) \geq \detlb(\mm{A})$. This, again, generalizes what happens with totally unimodular matrices, for which both quantities are equal to $1$. The determinant lower bound has since become a powerful tool in proving nearly tight lower bounds on many natural and important set systems, e.g. axis-aligned boxes, and point-line incidences \cite{chazellelvov2001discrepancy, matousek18factorization}. Given that the determinant lower bound often implies nearly tight discrepancy lower bounds, it is natural to ask how far it can be from the hereditary discrepancy. The first result in this direction is due to \cite{matouvsek2013determinant} who showed that the ratio between hereditary discrepancy and the determinant lower bound is bounded from above as
\[\frac{\herdisc(\mm{A})}{\detlb(\mm{A})} \lesssim \log(2mn)\cdot\sqrt{\log(2n)}.\]
Here we used the notation $a \lesssim b$ to denote the existence of a universal constant $c$ such that $a \leq c\cdot b$. In the sequel, we also use $a \gtrsim b$ to denote the existence of a universal constant $c > 0$ such that $a \geq c\cdot b$, and $a\cong b$ when $a\lesssim b$ and $a\gtrsim b$.

Matou\v{s}ek's bound was not believed to be tight as the largest known
value of \(\frac{\herdisc(\mm{A})}{\detlb(\mm{A})}\) is on the order
of \(\max\{\sqrt{\log m},\log n\}\), where the first term is achieved
by the power set of the universe, and the second term is achieved on a few families of set systems (and their incidence matrices) e.g.\ the large discrepancy three permutations family of~\cite{newman2012beck}  (see also \citep{franks2018simplified}) and a construction due to~\cite{palvolgyi2010indecomposable}.
 Matou\v{s}ek's bound follows from a pair of inequalities
\begin{align}
    \herdisc(\mm{A}) \lesssim \log(2mn)\cdot\hervecdisc(\mm{A})\label{eq:matousek-eq1}\\
    \hervecdisc(\mm{A}) \lesssim \sqrt{\log 2n}\cdot\detlb(\mm{A})\label{eq:matousek-eq2}
\end{align}
where the first inequality is implied by the seminal result of~\cite{bansal2010constructive}, and the second inequality is proved using duality. %This was a surprising and important result because it constructed a coloring achieving discrepancy nearly matching Spencer's upper bound when such an algorithm was not thought to exist. Bansal's approach considered the feasibility of a particular SDP, the dual of which Matou\v{s}ek uses to obtain Equation~\ref{eq:matousek-eq2}. 
Note that $\vecdisc$ is the \emph{vector discrepancy} of a set system, or matrix. This quantity is similar to discrepancy but the elements of the universe are ``colored'' by vectors rather than by $\pm 1$. In particular, for an ${m\times n}$ matrix $\mm{A}$, 
\[\vecdisc(\mm{A}) = \min_{\mm{v}_1, \ldots, \mm{v}_n \in S^{n-1}}\max_{j \in [m]}\left\|\sum_{i \in [n]} A_{j,i}\cdot \mm{v}_i\right\|_2,
\]
where $S^{n-1}$ is the unit sphere in $\RR^n$.
Note that vector discrepancy is a lower bound on discrepancy since a coloring $\chi: X \rightarrow \{\pm 1\}$ can be interpreted as a set of vectors where all vectors are parallel to each other.
The \emph{hereditary vector discrepancy of $\mm{A}$}, denoted $\hervecdisc$, which appears in \eqref{eq:matousek-eq1}, is the maximum vector discrepancy of any subset of the columns of $\mm{A}$. %This variant of discrepancy appears in the result of~\cite{bansal2010constructive} mentioned above. 

Recently~\cite{jiang2022tighter} were able to
improve~\cite{matouvsek2013determinant}'s result by showing that
$\frac{\herdisc(\mm{A})}{\detlb(\mm{A})} \lesssim \sqrt{\log 2m\log
  2n})$. Their work left open whether the \(\sqrt{\log m}\) term can
be replaced by \(\sqrt{\log n}\) for large \(m\). 
In the present work, we show that this is mostly not possible, and that the factor of $\sqrt{\log m}$ is necessary for all $m$ in the range $n \leq m \leq 2^{n^{1-\epsilon}}$ for constant $\epsilon > 0$. Note that when $m > 2^n$, the matrix contains duplicated rows whose removal will not change the value of $\herdisc(\mm{A})$ nor $\detlb(\mm{A})$. Thus, our lower bound covers nearly the whole range of values for $m$.

Our main result is stated in the next theorem. Its proof appears at the end of Section~\ref{sec:haarbasis}.
\begin{theorem}\label{thm:main}
For any constant $\epsilon \in (0, 1)$, any integers $n \ge 2$ and $m \in \left[n, 2^{n^{1-\epsilon}}\right]$, there exists a matrix $\mm{A} \in \{0, 1\}^{m\times n}$ such that
\begin{equation}\label{eq:main}
    \frac{\herdisc(\mm{A})}{\detlb(\mm{A})} \gtrsim \sqrt{\log m \log n}.
\end{equation}
Further, for all positive integers $m$ and $n$, all matrices $\mm{A} \in \{0,1\}^{m\times n}$ satisfy $\frac{\herdisc(\mm{A})}{\detlb(\mm{A})}  \lesssim\sqrt{n}$. %Thus only when $m \gtrsim 2^{n/\log n}$ can the $O(\sqrt{\log m})$ term be improved slightly. 
\end{theorem}
The claim after ``further'' shows that the lower bound cannot be extended to $m = 2^{\omega(n/\log n)}$. 

Crucial to the proof of Theorem~\ref{thm:main} is a recursively
defined $2^{k} \times 2^k$ matrix $\mm{A}_k$, related to the discrete
Haar basis. We will also show that $\mm{A}_k$ is tight for Equation~\eqref{eq:matousek-eq2}. 
\begin{theorem}\label{thm:tight-matousek-example}
    With $n = 2^k$ for an integer $k\ge 1$, $\hervecdisc(\mm{A}_k) \gtrsim \sqrt{\log n}\cdot\detlb(\mm{A}_k)$.
\end{theorem}
Its proof appears in Section~\ref{sec:othernotableproperties}. It is yet unknown whether Equation~\eqref{eq:matousek-eq1} is tight as~\cite{jiang2022tighter} improved upon~\cite{matouvsek2013determinant}'s bound by circumventing the inequality altogether. The resolution of this problem via an efficient algorithm would imply new and old constructive bounds, for example, the constructive version of Banaszczyk's upper bound for the Beck-Fiala problem~\citep{banaszczyk1998balancing,bansal2016algorithm}, and a constructive version of Nikolov's upper bound for Tu\'snady's problem~\citep{nikolov17tusnady}.

Before we begin the proof proper, we define a few variants of discrepancy which will appear throughout this exposition. Just as $\disc(\mm{A})$ was defined in-terms of $L_{\infty}$, we can define discrepancy in-terms of other norms. In particular, for $L_1$,
\[\disc_1(\mm{A}) \coloneqq \min_{\mm{x} \in \{\pm 1\}^n}\frac{\norm{\mm{A}\mm{x}}_1}{m}\]
and generally for $L_p$,
\[\disc_p(\mm{A}) \coloneqq \min_{\mm{x} \in \{\pm 1\}^n} \left(\frac{\norm{\mm{A}\mm{x}}_p^p}{m}\right)^{1/p}.\]
Note that $\disc_p(\mm{A}) \leq \disc_q(\mm{A})$ when $p \leq q$.

\section{Proof Structure}
In order to prove Theorem~\ref{thm:main} we will find a family of matrices which satisfy equation~\eqref{eq:main}. Our candidates will have the form $\mm{P}_N \otimes \mm{A}$ where $\mm{P}_N$ is the $2^{N}\times N$ incidence matrix of the power set and $\mm{A}$ is some $p \times p$ matrix with a gap between $\detlb(\mm{A})$ and $\disc_1(\mm{A})$. In particular, we derive $\mm{A}$ from the Haar basis, drawing inspiration from~\cite{kunisky2021discrepancy}. 

We bound $\detlb(\mm{P}_N \otimes \mm{A})$ from above by showing that $\detlb(\mm{P}_N \otimes \mm{A}) \lesssim \sqrt{N}\cdot\detlb(\mm{A})$ using standard linear algebra and Lemma 4 from \cite{matouvsek2013determinant}. See Lemma~\ref{lem:detlb-amplification}. For our choice of $\mm{A}$, we will show that $\detlb(\mm{A}) \lesssim 1$. We also bound $\disc(\mm{P}_N \otimes \mm{A}) \gtrsim N\cdot \disc_1(\mm{A})$ using a discrepancy amplification argument. See Lemma~\ref{lem:disc-amplification}. By finding a tight lower bound on $\disc_1(\mm{A})$, we obtain the lower bound $\disc(\mm{P}_N \otimes \mm{A}) \gtrsim N\cdot\sqrt{\log p}$. Taken together, these bounds gives us a gap on the order of $\sqrt{N}\cdot \sqrt{\log p}$ between $\detlb(\mm{P}_N \otimes \mm{A})$ and $\disc(\mm{P}_N \otimes \mm{A})$.
%In practice, we do not know how to bound $\disc_1(\mm{P}_N \otimes \mm{A})$ for any of the matrices directly. Instead, we will bound $\disc_1(\mm{P}_N \otimes \mm{A})$ by finding an upper bound for $\disc_2(\mm{P}_N \otimes \mm{A})$. See Lemma~\ref{lem:lb-martingale}. Thus for each instance of $\mm{P}_N \otimes \mm{A}$ it suffices to show an upper bound on $\detlb(\mm{A})$, if it has not already been proven, and a lower bound on $\disc_2(\mm{A})$.  

\begin{lemma}\textup{(Determinant LB Amplification).}\label{lem:detlb-amplification} For the power matrix $\mm{P}_N$, and any real matrix $\mm{A}$, 
    \[\detlb\left(\mm{P}_N \otimes \mm{A}\right)\leq \sqrt{eN}\cdot\detlb(\mm{A}).\] 
\end{lemma}
\begin{proof}
    Let $\mm{u}_1, ..., \mm{u}_N$ be the columns of $\mm{P}_N$ and $\mm{A} \in \RR^{p\times p}$. Divide the columns of $\mm{P}_N \otimes \mm{A}$ into $N$ contiguous blocks of size $(2^{N}p)\times p$ each representing $\mm{u}_\ell \otimes \mm{A}$. Note that $\mm{u}_\ell \otimes \mm{A}$ consists of $2^N$ blocks of $\mm{A}$ or $\mm{0}$ stacked on top of one another. We claim that $\detlb(\mm{u}_\ell \otimes \mm{A}) \leq \detlb(\mm{A})$. Consider an $s\times s$ sub-matrix $\mm{B}$ of $\mm{u}_\ell \otimes \mm{A}$ with the rows indexed by $I$ and columns indexed by $J$. Note that if any row of $\mm{B}$ is zero, then $\det(\mm{B}) = 0$ so in order for the determinant to be non-zero, the rows of $\mm{B}$ must be parts of rows of $\mm{A}$ with columns indexed by $J$. If there are multiple copies of the same row of $\mm{A}$, then again $\det(\mm{B}) = 0$. Thus $\mm{B}$ must come from distinct rows of $\mm{A}$ with columns indexed by $J$. It follows that $\mm{B}$ is actually a sub-matrix of $\mm{A}$ up to rearrangement of the rows, so $\left|\det(\mm{B})\right|^{1/s} \leq \detlb(\mm{A})$. Since this is true for all choices of the submatrix $\mm{B}$, we have $\detlb\left(\mm{u}_\ell \otimes \mm{A}\right) \leq \detlb(\mm{A})$.

    %Next consider an $s\times s$ sub-matrix of $\mm{A}'$ of $\mm{P}_N \otimes \mm{A}$ where $I$ and $J$ are the size $s$ index sets of the rows and columns respectively.\sn{I don't understand why you take a submatrix here. Matou\v{s}ek's lemma is already in terms of detlb.} Note that $J$ can be decomposed into $J_1, ..., J_N$ where each $J_\ell$ comes from the columns of $\mm{u}_\ell \otimes \mm{A}$. 
    Recall from \cite{matouvsek2013determinant} Lemma 4, that for real matrices $\mm{B}_1, ..., \mm{B}_t$ each with the same number of columns and $D\coloneqq \max_{i=1,2,...t}\detlb(\mm{B}_i)$, any matrix $\mm{B}$ whose rows are copies of the rows of the matrices $\mm{B}_i$ satisfies $\detlb(\mm{B}) \leq D\sqrt{e t}$. By applying Lemma 4 to $\left(\mm{P}_N \otimes \mm{A}\right)^{\top}$ with $\mm{B}_i = \left(\mm{u}_i \otimes \mm{A}\right)^{\top}$, and we have that
    \[\detlb(\mm{P}_N \otimes \mm{A}) \leq \sqrt{e N}\cdot\max_{\ell \in [N]}\detlb(\mm{u}_\ell \otimes \mm{A}) \leq \sqrt{e N}\cdot\detlb(\mm{A}).\qedhere\]
\end{proof}

\begin{lemma}{\textup{(Discrepancy Amplification)}.}\label{lem:disc-amplification}
For the power matrix $\mm{P}_N$ and any real matrix $\mm{A}$, 
\[\disc(\mm{P}_N \otimes \mm{A}) \geq \frac{N\cdot \disc_1(\mm{A})}{2}.\]
\end{lemma}
\begin{proof}
Let $\mm{A} \in \RR^{p\times q}$ and $t \coloneqq \disc_1(\mm{A})$. Consider some vector $\mm{x}\in \{\pm 1\}^{qN}$ composed of vectors $\mm{x}^{(1)}, ..., \mm{x}^{(N)}$ stacked on top of each other containing $p$ entries each. We compute $\norm{(\mm{P}_N \otimes \mm{A})\mm{x}}_{\infty}$. Note that
\[\norm{(\mm{P}_N \otimes \mm{A})\mm{x}}_{\infty} = \max_{S\subseteq [N]}\left\|\sum_{i\in S}\mm{A}\mm{x}^{(i)}\right\|_{\infty} = \max_{S\subseteq[N]}\max_{j \in [p]}\left|\sum_{i \in S}\left(\mm{A}\mm{x}^{(i)}\right)_j\right|.\]
From the assumption, we have $\frac{1}{p}\norm{\mm{A}\mm{x}^{(i)}}_1 \geq t$ for every $i \in [N]$. Taking an average over all choices of $i$, 
\[t \le \frac{1}{pN}\sum_{i=1}^N\sum_{j=1}^p\left|\left(\mm{A}\mm{x}^{(i)}\right)_j\right| = \frac{1}{pN}\sum_{j=1}^p\sum_{i=1}^N\left|\left(\mm{A}\mm{x}^{(i)}\right)_j\right| \implies \sum_{i=1}^N\left|\left(\mm{A}\mm{x}^{(i)}\right)_j\right| \geq Nt\]
for some $j \in [p]$ by the pigeon hole principle. With $S^{+} = \{i: (\mm{A}\mm{x}^{(i)})_j > 0\}$ and $S^{-} = \{i: (\mm{A}\mm{x}^{(i)})_j < 0\}$, 
\begin{align*}
    Nt \leq \sum_{i=1}^{N}\left|\left(\mm{A}\mm{x}^{(i)}\right)_j\right| &= \sum_{i\in S^+} \left(\mm{A}\mm{x}^{(i)}\right)_j - \sum_{i \in S^-} \left(\mm{A}\mm{x}^{(i)}\right)_j\\
    &= \left|\sum_{i\in S^+} \left(\mm{A}\mm{x}^{(i)}\right)_j\right| + \left|\sum_{i \in S^-} \left(\mm{A}\mm{x}^{(i)}\right)_j\right|\\
    &\implies \max\left\{\left|\sum_{i\in S^+} \left(\mm{A}\mm{x}^{(i)}\right)_j\right|,  \left|\sum_{i \in S^-} \left(\mm{A}\mm{x}^{(i)}\right)_j\right| \right\} \geq \frac{Nt}{2}.\qedhere
\end{align*}
\end{proof}

Lemmas~\ref{lem:detlb-amplification}~and~\ref{lem:disc-amplification} together imply that
\[
\frac{\herdisc(\mm{P}_N \otimes \mm{A})}{\detlb(\mm{P}_N \otimes \mm{A})} \geq \frac{\sqrt{N}}{2\sqrt{e}} \cdot \frac{\disc_1(\mm{A})}{\detlb(\mm{A})}.
\]
Note that, if \(\mm{A} \in \RR^{p\times q}\), then \(\mm{P}_N \otimes \mm{A}\) is an \((2^N p)\times (Nq)\) matrix, so \(\sqrt{N}\) is roughly \(\sqrt{\log m}\) for small enough \(p\), where \(m \coloneqq 2^N p\) is the number of rows of \(\mm{P}_N \otimes \mm{A}\). To prove Theorem~\ref{thm:main}, we need to find a matrix \(\mm{A}\) that exhibits a large gap between \(\disc_1(\mm{A})\) and \(\detlb(\mm{A})\). In the next section, we show that a matrix whose columns are the discrete Haar basis vectors has this property.

\section{Discrete Haar Basis}\label{sec:haarbasis}
The $2^k\times 2^k$ discrete Haar basis matrix $\mm{A}_k$ is defined recursively with $\mm{A}_0 = [1]$ and 
\begin{equation}\label{eq:haardef}
\mm{A}_k = \begin{bmatrix}\mm{A}_{k-1} & \mm{I}_{k-1}\\\mm{A}_{k-1} & -\mm{I}_{k-1} \end{bmatrix}    
\end{equation}
where $\mm{I}_{k-1}$ is the $2^{k-1} \times 2^{k-1}$ identity matrix. This matrix arises from the following tree structure. Construct a depth $k$ perfect binary tree, and let $r$ be an additional node. We make the root of the perfect binary tree the left child of $r$, and $r$ becomes the root of our tree. Every non-leaf node represents a column in the matrix while every root-to-leaf path corresponds to a row in the matrix. Whenever the path proceeds down the left child from some node $i$, entry $i$ of the corresponding row will have value $+1$. If instead the path proceeds down the right child of $i$, entry $i$ of the corresponding row will have value $-1$. Thus every row will have $k$ non-zero entries. It is also not hard to show that, for any $\pm 1$ coloring of the columns, there is a row whose nonzero entries are equal to the corresponding column colors, and, therefore, $\disc(\mm{A}_k) = k$. \cite{kunisky2021discrepancy} describes this in detail.

In addition, we define the $\{0,1\}^{2^k\times 2^k}$ matrices $\mm{A}_k^+$ and $\mm{A}_k^-$ to be the indicator matrices of the positive and negatives elements of $\mm{A}_k$ respectively. Here an \emph{indicator matrix} will have a one in some entry if and only the corresponding entry of $\mm{A}_k$ is non-zero and is positive, in the case of $\mm{A}_k^+$, or negative, in the case of $\mm{A}_k^-$. Note that $\mm{A}_k = \mm{A}_k^{+} - \mm{A}_k^{-}$. Finally define, 
\begin{equation}\label{eq:haar-indicators}
    \mm{A}_k^{\pm}\coloneqq \begin{pmatrix}\mm{A}_k^{+}\\\mm{A}_k^{-}\end{pmatrix}.
\end{equation}
We bound the hereditary discrepancy to determinant lower bound ratio for both $\mm{P}_N \otimes \mm{A}_k$ and $\mm{P}_N \otimes \mm{A}_k^{\pm}$.

\begin{theorem}\label{thm:kunisky-ratio}
    For the power matrix $\mm{P}_N$, the discrete Haar basis $\mm{A}_k$, and the stacked indicator matrix $\mm{A}_k^{\pm}$ as defined in equation~\eqref{eq:haar-indicators},
    \begin{align}
        \frac{\herdisc\left(\mm{P}_{N}\otimes\mm{A}_k\right)}{\detlb\left(\mm{P}_{N}\otimes\mm{A}_k\right)} &\gtrsim \sqrt{N\cdot k}, \label{eq:kunisky-ratio}\\
        \frac{\herdisc\left(\mm{P}_{N}\otimes\mm{A}_k^{\pm}\right)}{\detlb\left(\mm{P}_{N}\otimes\mm{A}_k^{\pm}\right)} &\gtrsim \sqrt{N\cdot k} \label{eq:haar-indicator-ratio}.
    \end{align}
\end{theorem}
\begin{proof}
First we apply the proof structure described in the previous section to $\mm{A}_k$. In particular, we show that $\detlb(\mm{A}_k) = O(1)$ in Lemma~\ref{lem:ub-kunisky-detlb} and that $\disc_1(\mm{A}_k) \gtrsim \sqrt{k}$ in Lemma~\ref{lem:lb-kunisky-disc1}. Applying Lemma~\ref{lem:detlb-amplification} to the first result and Lemma~\ref{lem:disc-amplification} to the second, we have that $\detlb\left(\mm{P}_{N}\otimes\mm{A}_k\right) \lesssim \sqrt{N}$ and $\disc\left(\mm{P}_{N}\otimes\mm{A}_k\right) \gtrsim N\cdot\sqrt{k}$. It follows that 
\[\frac{\herdisc\left(\mm{P}_{N}\otimes\mm{A}_k\right)}{\detlb\left(\mm{P}_{N}\otimes\mm{A}_k\right)} \geq \frac{\disc\left(\mm{P}_{N}\otimes\mm{A}_k\right)}{\detlb\left(\mm{P}_{N}\otimes\mm{A}_k\right)} \gtrsim \sqrt{N\cdot k}.\]   
The process for $\mm{A}_k^{\pm}$ is similar. To show an upper bound on $\detlb(\mm{A}_k^{\pm})$, use Corollary~\ref{cor:haar-decomp} where $\mm{A}_k^+$ and $\mm{A}_k^-$ are shown to be TUM. Since the determinant of any square submatrix of either matrix is at most one in absolute value, we can apply Lemma 4 of~\cite{matouvsek2013determinant} to $\mm{A}_k^+$ and $\mm{A}_k^-$ to obtain $\detlb(\mm{A}_k^{\pm}) = O(1)$. To obtain the lower bound on $\disc_1(\mm{A}_k^{\pm})$, we will recall that $\disc_1(\mm{A}_k) \gtrsim \sqrt{k}$ from Lemma~\ref{lem:lb-kunisky-disc1}. Note that, for any \(\mm{x} \in \{-1, +1\}^{2^k}\), by the triangle inequality
\[
\frac{1}{2^k}\|\mm{A}_k\mm{x}\|_1 
= \frac{1}{2^k}\|(\mm{A}_k^+ - \mm{A}_k^-) \mm{x}\|_1 
\le 2\left(\frac{1}{2^{k+1}}\|\mm{A}_k^+\mm{x}\|_1 + \frac{1}{2^{k+1}}\|\mm{A}_k^-\mm{x}\|_1\right).
\]
Therefore, $\disc_1(\mm{A}_k^{\pm}) \gtrsim \sqrt{k}$ as well. Apply Lemma~\ref{lem:detlb-amplification} and Lemma~\ref{lem:disc-amplification} to $\detlb(\mm{A}_k^{\pm}) = O(1)$ and $\disc_1(\mm{A}_k^{\pm 1}) \gtrsim \sqrt{k}$ respectively to obtain equation~\eqref{eq:haar-indicator-ratio}.
\end{proof}

\begin{lemma}\label{lem:ub-kunisky-detlb}
    $\detlb(\mm{A}_k) \leq 2$.
\end{lemma}
\begin{proof}
We show that any $i \times i$ square submatrix $\mm{B}$ of $\mm{A}_k$ satisfies $|\det(\mm{B})| \leq 2^{i}$. First, define $M_{k}(i) \coloneqq \max_{\mm{B}}|\det(\mm{B})|$ where the maximum is taken over all $i\times i$ submatrices $\mm{B}$ of $\mm{A}_k$. We compute $M_{k}(i)$ recursively by considering the forms that all $i\times i$ submatrices of $\mm{A}_k$ can take:
\begin{enumerate}
    \item $\mm{B}$ only contain elements from the first $2^{k-1}$ columns of $\mm{A}_k$,
    \item $\mm{B}$ only contain elements from the second $2^{k-1}$ columns of $\mm{A}_k$, or
    \item $\mm{B}$ contain elements from both the first and second $2^{k-1}$ columns of $\mm{A}_k$.
\end{enumerate}
We use the recursive formula \eqref{eq:haardef} to analyze these cases.
In the first case the resulting submatrix is either entirely contained in $\mm{A}_{k-1}$ up to rearranging rows, or contains a duplicated row. The magnitude of the determinant of these submatrices can be bounded above by $M_{k-1}(i)$ and $0$, respectively. In the second case we note that the submatrix is TUM. To see this, recall that the second $2^{k-1}$ columns of $\mm{A}_k$ consist of an identity matrix and its negation stacked on top of one another. Any square submatrix is entirely contained in the identity matrix or contains duplicated (and negated) rows. Thus the absolute value of the determinant of this kind of submatrix is at most one. It remains to consider the third case. Let $\mm{B}$ be a submatrix of $\mm{A}_k$ with some $j$ columns coming from the second $2^{k-1}$ columns of $\mm{A}_k$ for $1 \leq j < i$. For any such column there is either one or two non-zero entries, equal to $1$ or $-1$. If there is only one non-zero entry, then this reduces to computing $M_k(i-1)$ since we can perform a co-factor expansion on this column. If there are two non-zero entries, then we can permute the rows so that they are adjacent. This only changes the sign of the resulting determinant. Notice that the two rows are identical except for the sign of the non-zero entries. When performing a co-factor expansion on these two entries, the $(i-1)\times(i-1)$ submatrix that results when removing either row and the column is identical. Thus $|\det(\mm{B})|$ is at most twice the absolute value of the determinant of this $(i-1)\times(i-1)$ submatrix. After removing all the columns and associated rows of $\mm{B}$ from the second half of $\mm{A}_k$ in this way, we see that $|\det(\mm{B})| \leq 2^{j}M_{k-1}(i-j)$. Since, in the base case, $M_0(1) = 1$, the claim follows.
\end{proof}
Using a similar argument as above, we can show that the matrices $\mm{A}_k^+$ and $\mm{A}_k^-$ are TUM. Note that, using \eqref{eq:haardef}, $\mm{A}_k^+$ and $\mm{A}_k^-$ can be recursively defined as $\mm{A}_0^+ = [1]$, $\mm{A}_0^- = [0]$, and
\begin{equation}\label{eq:recursive-def-signed-id}
    \mm{A}_k^+ = \begin{bmatrix}
        \mm{A}_{k-1}^+ & \mm{I}_{k-1}\\
        \mm{A}_{k-1}^+ & \mm{0}
    \end{bmatrix}
    \qquad
    \mm{A}_k^- = \begin{bmatrix}
        \mm{A}_{k-1}^- & \mm{0}\\
        \mm{A}_{k-1}^- & \mm{I}_{k-1}
    \end{bmatrix}
\end{equation}
where $\mm{0}$ is the all zeros matrix of appropriate dimension. 

\begin{corollary}\label{cor:haar-decomp}
    $\mm{A}_k^+$ and $\mm{A}_k^-$ are TUM matrices where $\mm{A}_k^+$ and $\mm{A}_k^-$ are indicators of the positive and negatives entries of $\mm{A}_k$, respectively.
\end{corollary}
\begin{proof}
    We only consider $\mm{A}_k^+$ as the proof that $\mm{A}_k^-$ is a TUM matrix is similar. The proof proceeds by induction on $k$. Consider some $i \times i$ submatrix $\mm{B}$ of $\mm{A}_k^+$. If $\mm{B}$ is entirely contained in first half of the columns of $\mm{A}_k^+$, then we are done by the inductive hypothesis; if $\mm{B}$ is entirely contained in the second half of the columns of $\mm{A}_k^+$, then $\mm{B}$ is a submatrix of the identity matrix or has a row of $0$'s, and the absolute value of its determinant is at most $1$. Thus it suffices to consider the case where $\mm{B}$ has some columns from the first half of $\mm{A}_k^+$ and some columns from the second half of $\mm{A}_k^+$. Since any column from the second half has only one non-zero entry, equal to $1$, performing  co-factor expansions on the columns in the second half shows that the absolute value of the determinant will only be as large as the absolute value of the determinant of some smaller square sub-matrix in $\mm{A}_{k-1}^+$. Note that in the base case, $|\det(\mm{A}_0^+)| = 1$.
\end{proof}

\begin{lemma}\label{lem:lb-kunisky-disc1}
$\disc_1(\mm{A}_k) \cong \sqrt{k}$.
\end{lemma}
\begin{proof}
The proof for $k = 0$ is trivial, so we focus on the case $k \ge 1$. Let $\tilde{\mm{A}}_k$ denote the $2^k \times (2^k - 1)$ matrix equal to $\mm{A}_k$ with the first column, all of whose entries are $1$, removed. Note that this is equivalent to removing the root node $r$ and keeping only the perfect binary tree of depth $k$ in the tree structure of the Haar basis, as described at the beginning of the section. Observe that the rows of $\tilde{\mm{A}}_k$ come in pairs: for every row $\tilde{\mm{a}}^\top$, $-\tilde{\mm{a}}^\top$ is also a row of $\tilde{\mm{A}}_k$. Therefore, writing $\tilde{\mm{a}}_1^\top, \ldots, \tilde{\mm{a}}_{2^k}^\top$ for the rows of $\tilde{\mm{A}}_k$,
\begin{align*}
\disc_1(\mm{A}_k,\mm{x}) &= 
\frac{1}{2^k}\sum_{i=1}^{2^k}{\frac{|1 + \tilde{\mm{a}}_i^\top\mm{x}| + |1 - \tilde{\mm{a}}_i^\top\mm{x}|}{2}}\\
&= \frac{1}{2^k}\sum_{i=1}^{2^k}{\frac{|1 + |\tilde{\mm{a}}_i^\top\mm{x}|| + |1 - |\tilde{\mm{a}}_i^\top\mm{x}||}{2}}\\
&= \frac{1}{2^k}\sum_{i=1}^{2^k}{\frac{| |\tilde{\mm{a}}_i^\top\mm{x}| + 1| + | |\tilde{\mm{a}}_i^\top\mm{x}|-1|}{2}}\\
&\ge 
\frac{1}{2^k}\sum_{i=1}^{2^k}{ |\tilde{\mm{a}}_i^\top\mm{x}|}
= \disc_1(\tilde{\mm{A}}_k,\mm{x}).
\end{align*}
On the other hand, it's also easy to see that $\disc_1({\mm{A}}_k,\mm{x})\le \disc_1(\tilde{\mm{A}}_k,\mm{x})+1$, so it suffices to compute $\disc_1(\tilde{\mm{A}}_k)$. Let $p_k = 2^k-1$ denote the number of columns in $\tilde{\mm{A}}_k$. To show a bound on $\disc_1(\tilde{\mm{A}}_k)$, we will show that for any $\mm{x} \in \{\pm 1\}^{p_k}$, $\disc_1(\tilde{\mm{A}}_k, \mm{x}) = \disc_1(\tilde{\mm{A}}_k, \mathbb{1})$ where $\mathbb{1}$ is the all ones vector of the appropriate dimension (see Claim~\ref{claim:haar-permute} below). Further, we will observe that $\disc_1(\tilde{\mm{A}}_k, \mathbb{1}) = \frac{1}{2^k}\sum_{\ell = 0}^{k}\binom{k}{\ell}|k - 2\ell|$. Then, by Lemma~\ref{lem:lb-kunisky-computation}, we have that $\sum_{\ell = 0}^{k}\binom{k}{\ell}|k-2\ell| = 2k\binom{k-1}{\floor{k/2}}$ which implies that $\disc_1(\mm{A}_k) \cong\sqrt{k}$ since $\binom{k-1}{\floor{k/2}} \cong 2^{k}/\sqrt{k}$ by Stirling's approximation.

\begin{claim}\label{claim:haar-permute}
    For any $\mm{x} \in \{\pm 1\}^{p_k}$ there exists a permutation which map the entries of $\tilde{\mm{A}}_k\mm{x}$ to those of $\tilde{\mm{A}}_k\mathbb{1}$. In particular, $\disc_1(\tilde{\mm{A}}_k, \mm{x}) = \disc_1(\tilde{\mm{A}}_k, \mathbb{1})$.
\end{claim}
\begin{proof}
Our proof is by induction on $k$. When $k = 1$,we have a root node with two children corresponding to the matrix
\[
\tilde{\mm{A}}_1 = \begin{bmatrix}
1\\
-1
\end{bmatrix}.
\]
 When $\mm{x} = [1]$, $\mm{A}_1\mm{x} = \mm{A}_1\mathbb{1}$ and the identity permutation suffices; when $\mm{x} = [-1]$, $\mm{A}_1\mm{x} = -\mm{A}_1\mathbb{1}$ and it suffices to swap the two entries.

Consider some height $k$ perfect binary tree corresponding to $\tilde{\mm{A}}_k$. Let $u$ be the root of the tree with left and right children $u_+$ and $u_-$ respectively. Since every root-to-leaf path must go through $u_+$ or $u_-$, this forms a partition of the rows of $\tilde{\mm{A}}_k$. In particular, we can rearrange $\tilde{\mm{A}}_k$ as
\[\tilde{\mm{A}}_k = \begin{bmatrix}
    \mathbb{1} & \tilde{\mm{A}}_{k-1} & \mm{0}\\
    -\mathbb{1} & \mm{0} & \tilde{\mm{A}}_{k-1} 
\end{bmatrix}.\]
Consider the $\tilde{\mm{A}}_{k-1}$ submatrices which appear in $\tilde{\mm{A}}_{k}$. The $\tilde{\mm{A}}_{k-1}$ submatrix in the first $2^{k-1}$ rows has rows which correspond to root-to-leaf paths with leaves in the subtree rooted at $u_+$. Its columns correspond to nodes in the same subtree. The $\tilde{\mm{A}}_{k-1}$ submatrix in the second $2^{k-1}$ rows is defined similarly on the subtree rooted at $u_-$. Write the vector $\mm{x}$ as $[x_u, \mm{x}_+, \mm{x}_-]^{\top}$ where $x_u$ is the color of the node $u$ and $\mm{x}_+$ and $\mm{x}_-$ are the colors of the nodes in the subtrees rooted at $u_+$ and $u_-$, respectively. Consider the value of $x_u$. If $x_u = 1$, then by the inductive hypothesis, there exists a permutation which takes the entries of $\tilde{\mm{A}}_{k-1}\mm{x}_+$ to the entries of $\tilde{\mm{A}}_{k-1}\mathbb{1}$ and another permutation which takes the entries of $\tilde{\mm{A}}_{k-1}\mm{x}_-$ to the entries of $\tilde{\mm{A}}_{k-1}\mathbb{1}$. These two permutations can be combined to form a permutation which maps the entries of $\tilde{\mm{A}}_k\mm{x}$ to the entries of $\tilde{\mm{A}}_k\mathbb{1}$. Otherwise $x_u = -1$. Again, there exists a permutation $\pi_1$ which takes $\tilde{\mm{A}}_{k-1}\mm{x}_+$ to $\tilde{\mm{A}}_{k-1}\mathbb{1}$ and another permutation $\pi_2$ which takes $\tilde{\mm{A}}_{k-1}\mm{x}_-$ to $\tilde{\mm{A}}_{k-1}\mathbb{1}$. We can construct a permutation which maps the elements of $\tilde{\mm{A}}_k\mm{x}$ to those of $\tilde{\mm{A}}_k\mathbb{1}$ by by first applying $\pi_1$ to the first $2^{k-1}$ entries of $\tilde{\mm{A}}_k\mm{x}$, and $\pi_2$ to the remaining $2^{k-1}$ entries, and then swapping the first $2^{k-1}$ entries with the second $2^{k-1}$ entries. \end{proof}

%To see that $\disc_1(\mm{A}_k) = \EE_{\chi}\left|\mathbb{1}_k^{\top}\chi\right|$, recall that $\disc_1(\mm{A}_k) = \min_{\chi \in \{\pm 1\}^q}\frac{\norm{\mm{A}_k\chi}_1}{p}$ which is equivalent to $\min_{\chi \in \{\pm 1\}^p} \EE\left|\mm{a}^{\top}\chi\right|$. In both cases were are computing the absolute value of the discrepancy of each row, summing them together, and dividing by the total number of rows. Notice that every variable appears the same number of times as a positive and as a negative literal.\sn{I don't see how this proves the claim. "Every variable appears the same number of times as a positive and as a negative literal"  only implies that $\EE \mm{a} = 0$, but it does not give independence between coordinates. Why don't you write the proof you showed me? Also I'd split this lemma into two lemmas, one for the first half of the proof, and the other for calculating the expected absolute value of the sum of $k$ Rademachers} Thus, regardless of coloring $\chi$, $\mm{a}^{\top}\chi$ is the sum of $k$ Rademacher random variables. Thus $\min_{\chi \in \{\pm 1\}^p}\EE_{\mm{a}}\left|\mm{a}^{\top}\chi\right| = \EE_{\chi}\left|\mathbb{1}_k^{\top}\chi\right|$.   
To see that $\disc_1(\tilde{\mm{A}}_k, \mathbb{1}) = \sum_{\ell = 0}^{k}\binom{k}{\ell}|k - 2\ell|$, recall that each row of $\tilde{\mm{A}}_k$ has exactly $k$ entries and every sign pattern for these $k$ entries appears exactly once (each row is a root-to-leaf path on a depth $k$ perfect binary tree). In particular there are exactly $\binom{k}{\ell}$ rows whose inner-product with $\mathbb{1}$ is $k-2\ell$, since such rows have exactly $n - \ell$ entries equal to $+1$ and $\ell$ entries equal to $-1$.
\end{proof}

The above proof also has a probabilistic interpretation. We show that, for a uniformly random row $\mm{a}^{\top}$ of $\tilde{\mm{A}}_k$ and a fixed coloring $\mm{x} \in \{\pm 1\}^{p_k}$, $\mm{a}^{\top}\mm{x}$ is distributed like $X_1 + \cdots + X_k$ where the $X_i$s are independent Rademacher random variables. Recall that uniformly choosing a row of $\tilde{\mm{A}}_k$ corresponds to uniformly choosing a root-to-leaf path in the depth $k$ perfect binary tree. Further the non-leaf nodes of the tree correspond to the columns of $\tilde{\mm{A}}_k$. Thus we know that exactly $k$ entries of the row will be non-zero. Wlog.\ assume these entries are $u_1, ..., u_k$ and note that $\mm{a}^{\top}\mm{x} = x_{u_1}a_{u_1} + \cdots + x_{u_k}a_{u_k}$. We show that the right hand side has the same distribution as a sum of $k$ independent Rademacher random variables by induction on $k$. In the base case, the root-to-leaf path is equally likely to end at the left or the right children of the root. Suppose $x_{u_1}a_{u_1} + \cdots + x_{u_{k-1}}a_{u_{k-1}}$ is distributed like $k-1$ independent Rademacher random variables. Note that the path is equally likely to do down the left or right child of $u_{k}$ so $x_{u_k}a_{u_k}$ is equally likely to be $1$ or $-1$, conditioned on the root-to-leaf path reaching the parent of $u_k$.  Taking expectation over the choice of $u_k$ finishes the proof.

%Note that $\disc_1(\tilde{\mm{A}}_k) = \min_{\mm{x} \in \{\pm 1\}^{p_k}}\frac{\norm{\tilde{\mm{A}}_k\mm{x}}_1}{2^k}$ which is equivalent to $\min_{\mm{x} \in \{\pm 1\}^{p_k}} \EE\left|\mm{a}^{\top}\mm{x}\right|$ where $\mm{a}^{\top}$ is a random row of $\tilde{\mm{A}}_k$. Fix coloring $\mm{x} \in \{\pm 1\}^{p_k}$ and choose a random row $\mm{a}^{\top}$. Let $X_1, ..., X_{p_k}$ be a sequence of random variables where $X_i$ is the value of entry $i$ of $\mm{a}^{\top}$.\sn{I don't think you want entry \(i\). You want the entry corresponding to the \(i\)-th node on the path represented by \(\mm{a}\). I guess that's the \(i\)-th nonzero entry in \(\mm{a}\)?} We will show that, conditioned on $X_1, ..., X_{i-1}$, $X_i$ is unbiased. It follows that $\chi_1X_1 + \cdots + \chi_{p_k}X_{p_k}$ is distributed like $X_1 + \cdots + X_{p_k}$ and $\disc_1(\tilde{\mm{A}}_k) = \EE_{\mm{a}}\left|\mm{a}^{\top}\mathbb{1}\right|$. To show that $X_i$ is unbiased, we will use induction on $k$. The base case where $k = 1$ is the same as before. Suppose that the random variables $X_1, ..., X_{p_{k-1}}$ are unbiased when conditioned on the variables of smaller index, we will show that $X_{p_{k-1}+1}, ..., X_{p_k}$ are similarly unbiased. Note that for any pair of distinct indices $s, t \in \left\{p_{k-1} + 1, ..., p_k\right\}$, $X_s$ is independent of $X_t$ since any row can have at most one non-zero entry in column $s$ and $t$.\sn{I don't understand this explanation, and I don't think they are independent. I also do not understand why you even need this.} Thus it suffices to consider $X_{p_k}$ and show that it is unbiased conditioned on $X_1, ..., X_{p_{k-1}}$. 

The next lemma is likely a well-known calculation. We include a proof, due to Lavrov, for completeness.

\begin{lemma}[\cite{lavrov2018}]\label{lem:lb-kunisky-computation}
    $\sum_{\ell=0}^{k}\binom{k}{\ell}|k-2\ell| = 2k\cdot\binom{k-1}{\floor{k/2}}$.
\end{lemma}
\begin{proof}
Recall the identity $\binom{k}{\ell}\ell = \binom{k-1}{\ell-1}k$. We write
\begin{align*}
    \sum_{\ell = 0}^k\binom{k}{\ell}|k-2\ell| &= \sum_{\ell < k/2}\binom{k}{\ell}(k-2\ell) - \sum_{\ell > k/2}\binom{k}{\ell}(k-2\ell)\\
    &= k\left(\sum_{\ell < k/2}\binom{k}{\ell} - \sum_{\ell > k/2}\binom{k}{\ell}\right) - 2\left(\sum_{\ell < k/2}\binom{k}{\ell}\ell - \sum_{\ell > k/2}\binom{k}{\ell}\ell\right)\\
    &= 2k\left(\sum_{\ell > k/2}\binom{k-1}{\ell-1} - \sum_{\ell < k/2}\binom{k-1}{\ell-1}\right)\\
    &= 2k\binom{k-1}{\floor{k/2}}.
\end{align*}
Here, the first equality follows since $\binom{k}{\ell}\left(k - 2(k/2)\right) = 0$ when $k$ is even. The last equality follows by consider the parity of $k$; when $k$ is even, we obtain a $\binom{k-1}{k/2}$ term after cancellation, and when $k$ is odd, we obtain a $\binom{k-1}{\floor{k/2}}$ term after cancellation.
\end{proof}
Note that when we divide the identity by $2^k$, we obtain the expectation of a sum of $k$ independent Rademacher random variables. The asymptotic version of this identity follows from Khintchine's inequality.
%\begin{claim}
%    For $1 \leq p \le k$, $\disc_p(\mm{A}_k) = %\end{claim}
%\begin{proof}
% From Claim~\ref{claim:haar-permute} it folows that $\disc_p(\tilde{\mm{A}}_k) = disc_p(\tilde{\mm{A}}_k, \mathbb{1})$. Either using the observation that each sign pattern of the non-zero entries in the a row of $\tilde{\mm{A}}_k$ appears exactly once, or using the alternate probabilistic proof presented after Lemma~\ref{lem:lb-kunisky-disc1}, we see that $disc_p(\tilde{\mm{A}}_k, \mathbb{1})$ is the $L_p$ norm of a sum of $k$ independent Rademacher random variables. Thus, by Khintchine's inequality, $\disc_p(\tilde{\mm{A}}_k) \leq \Theta(\sqrt{pk})$.
%\end{proof}

One final ingredient we need for the proof of Theorem~\ref{thm:main} is a connection between the hereditary discrepancy of a set system $\mathcal{S}$ on a universe $X$, and its VC dimension. Recall that the VC dimension, denoted $\dim(\mathcal{S})$, is the largest size of a set $Y\subseteq X$ such that the restriction $\mathcal{S}|_Y \coloneqq \{S \cap Y: S \in \mathcal{S}\}$ equals the powerset of $Y$. Equivalently, we can define the VC dimension $\dim(\mm{A})$ of an incidence matrix $\mm{A} \in \{0,1\}^{m\times n}$ as the largest $N$ for which the power matrix $\mm{P}_N$ can be found as a submatrix of $\mm{A}$. In the proof of Theorem~\ref{thm:main}, we use Lemma~\ref{lem:herdisc-vc-ub} which states that $\herdisc(\mm{A}) \lesssim \sqrt{n\cdot\mathrm{dim}(\mm{A})}$. This is a well know result whose proof is included in the Appendix for the sake of completeness. 

\begin{proof}[Proof of Theorem~\ref{thm:main}]
    Consider the range of $m$ in $[n, n^2]$ and $\left(n^2, 2^{n^{(1-\epsilon)}}\right]$ separately. In the first interval, we let $\mm{A}$ be the matrix $\mm{A}_k$ padded with $m - n$ rows of zeros. Here, $\frac{\herdisc(\mm{A})}{\detlb(\mm{A})} \cong \log n \cong \sqrt{\log m\cdot \log n}$. When $m \in \left(n^2, 2^{n^{(1-\epsilon)}}\right]$, we consider the matrix $\mm{P}_N\otimes \mm{A}_k$ where $N = \floor{\log_2(m/n)}$ and $k = \floor{\log_2 n^{\epsilon}}$. Observe that $\mm{P}_{N}\otimes\mm{A}_{k}$ is an $m' \times n'$ matrix where $m' = 2^{N+k} \leq m/n^{1 - \epsilon} < m$ and $n' = N\cdot2^k \leq \log_2(m/n) \cdot n^{\epsilon} \leq n - n^{\epsilon}\log n$ since $m \leq 2^{n^{(1-\epsilon)}}$. We obtain $\mm{A}$ by padding $\mm{P}_N\otimes\mm{A}_k$ with zero vectors so that it has exactly $m$ rows and $n$ columns. Note that $\log m \cong N$ and $\log n \cong k$. By Theorem~\ref{thm:kunisky-ratio}, $\frac{\herdisc(\mm{A})}{\detlb(\mm{A})}  \gtrsim\sqrt{Nk} \gtrsim \sqrt{\log m\log n}$, as required. 

    The reader might object that the matrix $\mm{A}_k$ has negative entries which would not occur for incidence matrices of a set system, but we can remedy this by considering $\mm{A}_k^{\pm}$ as defined in Theorem~\ref{thm:kunisky-ratio} instead.
    
    Finally, we will show that $\frac{\herdisc(\mm{A})}{\detlb(\mm{A})} \lesssim \sqrt{n}$ for every $m$ and $n$. If $\mm{A}$ is a constant matrix (i.e., all its entries are equal), the bound is trivial, so we assume otherwise. Let $\dim(\mm{A})$ be the $\mathrm{VC}$ dimension of the matrix $\mm{A}$. The upper bound arises from the pair of inequalities $\herdisc(\mm{A}) \lesssim \sqrt{n\dim(\mm{A})}$ and $\sqrt{n\dim(\mm{A})} \lesssim \sqrt{n}\cdot\detlb(\mm{A})$. The former inequality is achieved by a random coloring, and is recounted in Lemma~\ref{lem:herdisc-vc-ub} in the Appendix, while the latter inequality follows by taking the power matrix $\mm{P}_{\dim(\mm{A})}$, which is a submatrix of $\mm{A}$ by the definition of $\dim(\mm{A})$, and observing that it contains a Hadamard matrix as a submatrix.%~\cite{matousek93discrepancy}
\end{proof}

\subsection{Other Notable Properties}\label{sec:othernotableproperties}
We can make a few more observations about the properties of $\mm{A}_k$.
\begin{claim}
    $|\det(\mm{A}_{k})| = 2^{2^k - 1}$.
\end{claim}
\begin{proof}
    Note that the columns of $\mm{A}_{k}$ are orthogonal so $|\det(\mm{A}_k)|$ is equal to the product of the $\ell_2$-norms of columns. Since the $i$th column has magnitude $2^{2^{i-1}}$, $|\det(\mm{A}_k)| = 2^{2^{k-1} + 2^{k-2} + \cdots + 2^{0}} = 2^{2^k-1}$.
\end{proof}

Next we prove Theorem~\ref{thm:tight-matousek-example}, showing that $\mm{A}_k$ serves as an example that equation~\eqref{eq:matousek-eq2} of~\cite{matouvsek2013determinant} --- mentioned in the introduction --- is tight. 
\begin{proof}[Proof of Theorem~\ref{thm:tight-matousek-example}]
To see this, it suffices to show that $\vecdisc\left(\mm{A}_k\right)^2 = \Omega(k)$. Let $\mm{v}_0, \mm{v}_1, ..., \mm{v}_q$ be the vector colors assigned to the $2^k$ columns of $\mm{A}_k$. Recall that $\mm{A}_k$ corresponds to a tree with root node $r$, where $r$ has no right child, and the left child is the root of a perfect binary tree of depth $k$. The root to leaf paths of this tree represent rows in $\mm{A}_k$. For any path $r, t_1, ..., t_i$ from $r$ to a node $t_i$, let $\overline{\mm{v}}_{t_i} = \mm{v}_{r} + \sum_{j=1}^{i-1} a_{t_j}\mm{v}_{t_j}$ where $a_{t_j}$ is $1$ if $t_{j+1}$ is the left child of $t_{j}$, and $-1$ otherwise. We will show that there exists a root-to-leaf $t_k$ path $t_1, ..., t_k$ such that $\left\|\overline{\mm{v}}_{t_k}\right\|_2^2 \ge k$. In particular we show that at every internal node $t$, with children $t_+$ and $t_-$, must have $\norm{\overline{\mm{v}}_{t_+}}^2 \geq 1 + \norm{\overline{\mm{v}}_t}^2$ or $\norm{\overline{\mm{v}}_{t_-}}^2 \geq 1 + \norm{\overline{\mm{v}}_t}^2$. To see this, note that
\[\norm{\overline{\mm{v}}_{t_+}}^2 = \norm{\overline{\mm{v}}_{t} + \mm{v}_{t}}^2 = \norm{\overline{\mm{v}}_{t}}^2 + \norm{\mm{v}_{t}}^2 + 2\anglebrac{\overline{\mm{v}}_{t}, \mm{v}_{t}} = \norm{\overline{\mm{v}}_{t}}^2 + 1 + 2\anglebrac{\overline{\mm{v}}_{t}, \mm{v}_{t}}.\]
Similarly, we have that $\norm{\overline{\mm{v}}_{t_-}}^2 = \norm{\overline{\mm{v}}_{t}}^2 + 1 - 2\anglebrac{\overline{\mm{v}}_{t}, \mm{v}_{t}}$. The claim then follows since either $\anglebrac{\overline{\mm{v}}_{t}, \mm{v}_{t}} \geq 0$ or $-\anglebrac{\overline{\mm{v}}_{t}, \mm{v}_{t}} \geq 0$. The theorem then follows from the tree interpretation of $\mm{A}_k$
\end{proof}

In~\cite{lovasz1986discrepancy} there appears an open problem of So\'{s} which asks if the hereditary discrepancy of a union of two sets systems is bounded above by the discrepancy of each individual set system i.e.\ for set systems $(X, \mathcal{S}_1)$ and $(X, \mathcal{S}_2)$ is it true that $\herdisc(\mathcal{S}_1\cup \mathcal{S}_2) \leq f(\herdisc(\mathcal{S}_1), \herdisc(\mathcal{S}_2))$ for some function $f$? It turns out that no such bound exists. The Hoffman example\footnote{Hoffman's set system $\mathcal{F}$ is defined on a regular $k$-ary tree of depth $k$ and obtains $\frac{\herdisc(\mm{A}_{\mathcal{F}})}{\detlb(\mm{A_{\mathcal{F}}})} = \Theta\left(\frac{\log n}{\log\log n}\right)$. Let $T$ be a $k$-regular tree with height $k$. The universe consists of the nodes of $T$. Let $\mathcal{F}_1$ be the sets of all root-to-leaf paths in $T$, and $\mathcal{F}_2$ be the set of all sibling sets (all nodes with the same parent) of internal nodes in $T$. Then $\mathcal{F} = \mathcal{F}_1 \cup \mathcal{F}_2$. We have that $\herdisc(\mathcal{F}_1), \herdisc(\mathcal{F}_2) \leq 1$, $\detlb(\mathcal{F}) = O(1)$, and $\disc(\mathcal{F}) = \Omega(k/\log k)$. See~\cite{matousek2009geometric} Section 4.4.}, the example of~\cite{palvolgyi2010indecomposable}, and the three permutations family of~\cite{newman2012beck} are instances of such pairs of set systems whose individual hereditary discrepancies are at most $1$, but whose union on a universe of size $n$ has discrepancy $\Omega(\log n/\log \log n)$ (for the Hoffman example) or $\Omega(\log n)$ (for the other two).

We see that $\mm{A}_k$ --- with the decomposition into $\mm{A}_k^+$ and $\mm{A}_k^-$ --- is another similar counter-example of S\'{o}s' conjecture. While it matches the $\Omega(\log n)$ discrepancy lower bound of the \cite{palvolgyi2010indecomposable} and \cite{newman2012beck} constructions, it is simpler to analyze.
\begin{claim}\label{claim:haar-is-sos}
    With $\mm{A}_k^\pm$ as described above Claim~\ref{cor:haar-decomp}, 
    \[\disc\left(\mm{A}_k^{\pm}\right) \gtrsim k.\] 
\end{claim}
\begin{proof}
    Recall that $\disc(\mm{A}_k) = k$. We claim that $\disc(\mm{A}^\pm_k) \ge \frac12 \disc(\mm{A}_k)$, and this proves the claim. Indeed, take any coloring $\mm{x}$. Let $\mm{a}^{\top}$ be the row of $\mm{A}_k$ achieving $|\mm{a}^{\top}\mm{x}| = \disc(\mm{A}_k, \mm{x})$ and let $\mm{a}_+^{\top}$ and $\mm{a}_-^{\top}$ be the corresponding rows in the copy of $\mm{A}^{+}$ and $\mm{A}^-$ in $\mm{A}^{\pm}$ respectively. Since $\disc(\mm{A}_k) \leq |\mm{a}^{\top}\mm{x}| = |\mm{a}_+^{\top}\mm{x} - \mm{a}_-^{\top}\mm{x}|$, by the triangle inequality we have that either $|\mm{a}_+^{\top}\mm{x}| \geq \disc(\mm{A}_k)/2$ or $|\mm{a}_-^{\top}\mm{x}| \geq \disc(\mm{A}_k)/2$.
\end{proof}

%==============================
\bibliography{bibliography.bib}
\input{appendix.tex}
\end{document}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
