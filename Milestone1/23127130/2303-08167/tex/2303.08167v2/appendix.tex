\appendix
\section{Upper Bound on Hereditary Discrepancy for Set Systems}

\modified{Here we give a different proof of the special case of Theorem~\ref{thm:herdisc-ub} for matrices $\mm{A}$ with entries in $\{0,1\}$, i.e., incidence matrices of set systems. This proof is more elementary, and has the interesting property that the  upper bound on discrepancy is certified by a uniformly random coloring.}

We need is a connection between the hereditary discrepancy of a set system $\mathcal{S}$ on a universe $X$, and its VC dimension. Recall that the VC dimension, denoted $\dim(\mathcal{S})$, is the largest size of a set $Y\subseteq X$ such that the restriction $\mathcal{S}|_Y \coloneqq \{S \cap Y: S \in \mathcal{S}\}$ equals the powerset of $Y$. Equivalently, we can define the VC dimension $\dim(\mm{A})$ of an incidence matrix $\mm{A} \in \{0,1\}^{m\times n}$ as the largest $N$ for which the power matrix $\mm{P}_N$ can be found as a submatrix of $\mm{A}$. 

\subsection{Upper Bound in Terms of VC Dimension}

Let us introduce some terminology from stochastic processes that we use in our proof. For a metric space $(T, d)$, let $\mathcal{N}(T, d, \epsilon)$ be the \emph{covering number of $T$} i.e.\ $\mathcal{N}(T, d, \epsilon)$ is the smallest number of closed balls with centers in $T$ and radii $\epsilon$ whose union covers $T$. Further, let $\norm{Y}_{\psi_2}$ be the sub-gaussian norm of a real-valued random variable $Y$ where $\norm{Y}_{\psi_2} \coloneqq \inf\{t \geq 0: \EE \exp\left(Y^2/t^2\right) \leq 2\}$. Finally, for a class of real-valued functions $\mathcal{F}$ defined on a probability space $(\Omega, \mu)$, where \(\Omega\) is a finite set, we define the $L^2(\mu)$ norm by
\(
\|f\|_{L^2(\mu)} := \left(\sum_{\omega \in \Omega} |f(\omega)|^2\mu(\omega)\right)^{1/2}.
\)
If $\mu$ is the uniform measure on $\Omega$, then we simple write $L^2$ rather than $L^2(\mu)$. 

The following result is a consequence of well-known lemmas. We recount it here for completeness.


\begin{lemma}\label{lem:herdisc-vc-ub}
    \modified{For any non-constant matrix $\mm{A}\in \{0,1\}^{m\times n}$, 
    \[\herdisc(\mm{A}) \lesssim\sqrt{n\cdot\dim(\mm{A})}.\]}
\end{lemma}
\begin{proof}
    \modified{
    It is enough to prove that $\disc(\mm{A}) \lesssim \sqrt{n\cdot\dim(\mm{A})}$ since, applying this inequality to any submatrix $\mm{B}$ consisting of a subset of $k$ columns from $\mm{A}$ shows that, $\disc(\mm{B}) \lesssim \sqrt{k\cdot\dim(\mm{B})} \le \sqrt{n\cdot\dim(\mm{A})}$.}
    
    \modified{We prove that $\disc(\mm{A}, \mm{x}) \lesssim \sqrt{n\cdot\dim(\mm{A})}$ is satisfied in expectation by a uniformly random coloring $\mm{x} \in \{-1, +1\}^{n}$ with entries $X_1, X_2, ... X_{n}$ which are independent Rademacher random variables (i.e., uniform in \(\{-1,+1\}\)). Let $\mathcal{F}$ denote the class of indicator functions defined by the rows of $\mm{A}$, i.e., for every $i \in [m]$, we define a function $f_{i}: \left[n\right] \rightarrow \{0,1\}$ given by $f_i(j) = a_{i,j}$.} We will show that 
    \[\EE\sup_{f\in \mathcal{F}} \left|\frac{1}{\sqrt{n}}\sum_{i = 1}^{n}X_if\left(i\right)\right| \lesssim \sqrt{\dim(\mathcal{F})}.\]
    
    For each indicator function $f$, let the random variable $Z_f \coloneqq \left|\frac{1}{\sqrt{n}}\sum_{i=1}^{n} X_if(i)\right|$. Note that this differs from the discrepancy of the row indicated by $f$ by a multiple of $\sqrt{n}$, i.e., $\disc(\mm{A},\mm{x}) = \sqrt{n} \cdot \sup_{f \in \mathcal{F}} Z_f$. Consider the random process $(Z_f)_{f\in \mathcal{F}}$. We will apply Dudley's inequality (Lemma~\ref{lem:v-dudley}) to show that 
    \begin{equation}\label{eq:dudley}
        \EE\sup_{f \in \mathcal{F}} Z_f \lesssim \int_0^1\sqrt{\log \mathcal{N}\left(\mathcal{F}, L^2, \epsilon\right)}d\epsilon.
    \end{equation}
    In order to apply Dudley's inequality we must show that $(Z_f)_{f\in \mathcal{F}}$ has sub-gaussian increments. Note that, since $\norm{X_i}_{\psi_2} \lesssim 1$, we have 
    \begin{align*}
        \norm{Z_f - Z_g}_{\psi} &= \frac{1}{\sqrt{n}}\left\|\left|\sum_{i=1}^{n}X_if(i)\right|-\left|\sum_{i=1}^{n}X_ig(i)\right|\right\|_{\psi_2}\\ 
        &\leq \frac{1}{\sqrt{n}}\left\|\sum_{i=1}^{n}X_i(f-g)(i)\right\|_{\psi_2} \lesssim \left(\frac{1}{n}\sum_{i=1}^n(f-g)(i)^2\right)^{1/2},
    \end{align*}
    where the second step follows from the reverse triangle inequality, and the final step by Hoeffding's lemma. The right hand side is $\norm{f - g}_{L^2}$ so when we apply Dudley's inequality as shown in Lemma~\ref{lem:v-dudley}, we obtain Equation~\ref{eq:dudley}. 
    
    Using Theorem~\ref{thm:v-covering-number}, we can bound the covering number with respect to the normalized $L_2$ norm as 
    \[\log \mathcal{N}\left(\mathcal{F}, L^2, \epsilon\right) \lesssim \dim(\mathcal{F})\log \left(\frac{2}{\epsilon}\right).\]
    Plugging the right hand side into the integral in Equation~\ref{eq:dudley} and integrating, we have $\EE \sup_{f \in \mathcal{F}}Z_f \lesssim \sqrt{\dim(\mathcal{F})}$. Recall that the discrepancy of the row indicated by $f$ is $\sqrt{n}\cdot Z_f$, thus the hereditary discrepancy is bounded above as $\herdisc(\mm{A}) \lesssim\sqrt{n\dim(\mathcal{F})}$, as was our goal.
\end{proof}

\begin{lemma}{\textup{(Dudley's Inequality, Remark 8.1.5~\cite{vershynin2018high}).}}\label{lem:v-dudley}
    Let $(X_t)_{t\in T}$ be a random process on a metric space $(T, d)$ with sub-gaussian increments i.e.\ there exists a $K \geq 0$ such that $\norm{X_t - X_s}_{\psi_2} \leq Kd(t,s)$ for all $t, s \in T$. Then 
    \[\EE \sup_{t,s \in T}\left|X_t - X_s\right| \lesssim K\int_0^{\infty}\sqrt{\log \mathcal{N}(T, d, \epsilon)}d\epsilon.\]
\end{lemma}

\begin{theorem}\textup{(Covering Numbers via VC Dimension, 8.3.18~\cite{vershynin2018high}).}\label{thm:v-covering-number}
    Let $\mathcal{F}$ be a class of Boolean functions on a probability space $(\Omega, \Sigma, \mu)$. Then, for every $\epsilon \in (0, 1)$, we have
    \[\mathcal{N}\left(\mathcal{F}, L^2(\mu), e\right) \leq \left(\frac{2}{\epsilon}\right)^{C\cdot\dim(\mathcal{F})}\]
    for an absolute constant $C$.
\end{theorem}
We note that Lemma~\ref{lem:herdisc-vc-ub} can likely be improved
further, for example by following the techniques of
Matou\v{s}ek~\cite{matousek95tight}, and carefully tracking constants.


\subsection{Connection to the Determinant Lower Bound}

\modified{To finish the proof, it remains to show a connection between VC dimension and the determinant lower bound. To do so, we show that a matrix \(\mm{A}\in \{0,1\}^{m\times n}\) with large VC dimension must contain a submatrix with large determinant. This submatrix is a binary version of the Hadamard matrix, described next.}

\modified{Let the $0$-$1$ Hadamard matrix be the $\{0,1\}$ matrix obtained by applying the linear map $a \mapsto (a + 1)/2$ to all of the entries in the standard $\pm 1$ Hadamard matrix. Denote the $n\times n$ $0$-$1$ and standard Hadamard matrices by $\tilde{\mm{H}}_n$ and $\mm{H}_n$ respectively. We prove the following. 
\begin{claim}\label{claim:01-hadamard-determinant}
    $\left|\det\left(\tilde{H}_n\right)\right| \geq 2^{-n}\cdot n^{n/2}$.
\end{claim}
\begin{proof}
    Consider $\mm{H}_n$ and suppose w.l.o.g.\ that its first rows is the all ones row. Add this row to all the other rows. Observe that all the other rows now have entries in $\{0, 2\}$. Scale them down by a factor of two. Adding one row to another does not change the determinant. Scaling a row scales the determinant by the same amount. Since $\left|\det\left(\mm{H}_n\right)\right| = n^{n/2}$, $\left|\det\left(\tilde{\mm{H}}_n\right)\right| = 2^{-n}\cdot n^{n/2}$.   
\end{proof}}

We can now finish the proof of Theorem~\ref{thm:herdisc-ub} for \(\mm{A} \in \{0,1\}^{m\times n}\). If $\mm{A}$ is a constant matrix (i.e., all its entries are equal), the bound is trivial, so we assume otherwise. The upper bound arises from the pair of inequalities $\herdisc(\mm{A}) \lesssim \sqrt{n\dim(\mm{A})}$ and
$\sqrt{\dim(\mm{A})} \lesssim \detlb(\mm{A})$. The former inequality is achieved by a random coloring, as shown in Lemma~\ref{lem:herdisc-vc-ub}. \modified{The latter follows by considering the power matrix $\mm{P}_{\dim(\mm{A})}$, which is a submatrix of $\mm{A}$. Since every $\dim(\mm{A}) \times \dim(\mm{A})$ 0-1 matrix is a submatrix of $\mm{P}_{\dim(\mm{A})}$, we can find also find $\tilde{\mm{H}}_{\dim(\mm{A})}$ as a submatrix of $\mm{P}_{\dim(\mm{A})}$, and, therefore, of $\mm{A}$. By Claim~\ref{claim:01-hadamard-determinant} we know that $\left|\det\left(\tilde{\mm{H}}_{\dim(\mm{A})}\right)\right| \geq 2^{-\dim(\mm{A})}\cdot\left(\dim(\mm{A})\right)^{\dim(\mm{A})/2}$. It follows that
    \[\detlb(\mm{A})  \geq \left|\det\left(\tilde{\mm{H}}_{\dim(\mm{A})}\right)\right|^{1/\dim(\mm{A})} \gtrsim \sqrt{\dim(\mm{A})}.\]
    Thus, the two inequalities together give us $\detlb(\mm{A}) \gtrsim \herdisc(\mm{A})/\sqrt{n}$ as required.}


%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main"
%%% End:
