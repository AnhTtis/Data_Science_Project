% 1_introduction.tex

\begin{figure*}
    \centering
    \begin{subfigure}[b]{1.0\textwidth}
         \centering
        \includegraphics[width=.117\linewidth]{figure/lighting_variation/indoor1_105.jpg}
        \includegraphics[width=.117\linewidth]{figure/lighting_variation/indoor1_0.jpg}
        \includegraphics[width=.117\linewidth]{figure/lighting_variation/indoor1_17.jpg}
        \includegraphics[width=.117\linewidth]{figure/lighting_variation/indoor1_6.jpg}
        \hspace{.125cm}
        \includegraphics[width=.117\linewidth]{figure/lighting_variation/outdoor1_107.jpg}
        \includegraphics[width=.117\linewidth]{figure/lighting_variation/outdoor1_0.jpg}
        \includegraphics[width=.117\linewidth]{figure/lighting_variation/outdoor1_14.jpg}
        \includegraphics[width=.117\linewidth]{figure/lighting_variation/outdoor1_6.jpg}
        % \caption{3 Input Views}
        \vspace{.125cm}
        \includegraphics[width=.117\linewidth]{figure/lighting_variation/indoor2_108.jpg}
        \includegraphics[width=.117\linewidth]{figure/lighting_variation/indoor2_0.jpg}
        \includegraphics[width=.117\linewidth]{figure/lighting_variation/indoor2_14.jpg}
        \includegraphics[width=.117\linewidth]{figure/lighting_variation/indoor2_6.jpg}
        \hspace{.125cm}
        \includegraphics[width=.117\linewidth]{figure/lighting_variation/outdoor2_101.jpg}
        \includegraphics[width=.117\linewidth]{figure/lighting_variation/outdoor2_0.jpg}
        \includegraphics[width=.117\linewidth]{figure/lighting_variation/outdoor2_12.jpg}
        \includegraphics[width=.117\linewidth]{figure/lighting_variation/outdoor2_6.jpg}
     \end{subfigure}
     \vspace{-20pt}
    \caption{
        \textbf{Examples of NeRF Extreme dataset.}
        %An illumination variation of our proposed dataset. 
        Each scene consists of 10 test images with mild-lighting conditions and 30 train images with at least 3 lighting variations - including variations in color/direction/intensity of lights.}
    \label{fig:lightingvariation} \vspace{-10pt}
\end{figure*}

\section{Introduction}
\label{sec:intro}

Neural radiance fields (NeRF)~\cite{mildenhall2020nerf} have recently had significant impacts on 3D scene reconstruction and novel view synthesis, and the following works have steadily improved the performance of NeRF in various aspects, such as generalization ability~\cite{geonerf, yu2021pixelnerf, wang2021ibrnet, srf, chen2021mvsnerf, trevithick2021grf, MINE, reizenstein2021common}, representation ability~\cite{NERFindetail, hdrnerf, zhang2020nerf++, BakingNeRF, barron2021mip, shen2022conditional, jiang2022alignerf}, and practicality~\cite{park2021nerfies, reiser2021kilonerf, wang2021nerf--, garbin2021fastnerf, dietnerf, hu2022efficientnerf, yu2021pixelnerf, xu2022sinnerf, mueller2022instant, BlcokNeRF, lin2021barf, boss2022-samurai, zhang2021ners}. 

However, what if \textit{only a few images collected from the internet or mobile phones taken under unconstrained illumination conditions are available?} 
In most cases, NeRF-based novel view synthesis under such a practical environment is often limited since it 1) requires a massive amount of data for reliable synthesis results, and 2) assumes constrained illumination conditions among input views to encode a view-dependent color. These are key drawbacks for practical usage of NeRF, as they disable view synthesis on images that were casually collected or captured in daily life. 

Many studies~\cite{wang2021ibrnet, CLIPnerf, dsnerf, yu2021pixelnerf, niemeyer2022regnerf, xu2022sinnerf, shi20223daware, diffregnerf, li2022compnvs, infonerf, rgbdnerf, truong2022sparf} have succeeded in reducing the number of input images by using prior knowledge or by applying regularization techniques for the task, but they adhere to the assumption of constrained illumination. 
Fig.~\ref{fig:teaser} shows that the state-of-the-art few-shot view synthesis method~\cite{niemeyer2022regnerf} fails to synthesize a plausible result. In addition, some works~\cite{nerfinthewild, boss2022-samurai, boss2021nerd, srinivasan2021nerv, NeuralPIL} have attempted to overcome the constrained illumination assumption. 
However, all of the methods have limited practicality in that a sufficient amount of inputs ($80$ to $200$ images) or foreground masks for an object are required.

In this paper, we address the problem of few-shot view synthesis under unconstrained illumination conditions, whose application is not limited to object-centric scenes. %a topic that has not been fully discussed in conventional methods yet.
Our proposed method, dubbed ExtremeNeRF, can directly regularize the appearances of the unknown view by enforcing consistency among intrinsic components between input and rendered views, which should be independent of viewing direction and illumination. Since NeRF often struggles in rendering a large-size patch due to the complexity, it is challenging to infer intrinsic components from the rendered images that are largely dependent on global contexts~\cite{ye2022intrinsicnerf}. To overcome this, we first extract the global context-aware pseudo-albedo ground truth of the inputs in the offline process. By enforcing a patch-wise module to decompose the same albedo as the pseudo-ground-truth, we then achieve global context-aware intrinsic decomposition during NeRF's optimization with minimum computational costs in an end-to-end manner.
This albedo consistency loss is supported by the projective transformation-based geometric alignment and depth consistency loss, which provides correspondences between pixels to compare and encourages correct geometry synthesis.

A newly built NeRF Extreme dataset, which is the first in-the-wild multi-view dataset that has both indoor and outdoor scenes taken under varying illumination, is also proposed (see Fig.~\ref{fig:lightingvariation}).
Our ExtremeNeRF provides plausible few-shot view synthesis and video rendering results, given about $0.06$ times fewer inputs with varying illumination.