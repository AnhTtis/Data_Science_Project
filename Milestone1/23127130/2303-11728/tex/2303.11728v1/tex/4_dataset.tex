% 4_realillum_dataset.tex
\section{NeRF Extreme Dataset}
\label{sec:dataset}
In order to build a view synthesis benchmark that fully reflects in-the-wild environments such as occasionally collected mobile phone images or online photo collections, we collect multi-view images with different light sources such as multiple light bulbs and the sun.
For indoor scenes, illumination variations are obtained by turn-on/off light sources, and closing/opening curtains. For outdoor scenes, scenes are captured at different times with different sunlights~(see Fig.~\ref{fig:lightingvariation}). All the images are taken in the wild using the off-the-shelves camera in the mobile phone to reproduce casual image-capturing situations. 
Similar to LLFF~\cite{LLFF}, all the camera poses are obtained by COLMAP~\cite{colmap} structure-from-motion framework. All the images, corresponding camera poses, and depth map that is obtained by recent multi-view stereo method~\cite{li2022compnvs} will be publicly available.

Specifically, our dataset consists of $10$ scenes, of which $6$ are indoor and $4$ outdoor. A total of $40$ images were taken per scene with a resolution of $3,000\times4,000$ and $30$ images are for the train set and $10$ images were for the test set. The training images of each scene are captured with at least 3 different lighting conditions.
For wider usage, target test scenes are taken with mild-lighting conditions instead of challenging ones - i.e. taken without extremely low/high contrasts or intensities, similar to images of typical NeRF benchmark datasets\cite{LLFF, DTU}. 
Comparison between our newly built dataset and the related existing datasets are provided in Tab.~\ref{tab:comparison}. Among various datasets with multi-view or varying illumination, SAMURAI~\cite{boss2022-samurai} has met the other requirements except for the fact that it is an object-centric dataset whose backgrounds are diverse and are placed by humans. A detailed description of dataset statistics can be found in the supplementary material.


