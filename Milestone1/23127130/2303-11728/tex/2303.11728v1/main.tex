    \documentclass[10pt,twocolumn,letterpaper]{article}
    
\usepackage{iccv}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{verbatim}
\usepackage{wrapfig,lipsum,booktabs}
\usepackage[dvipsnames]{xcolor}
\usepackage{mathtools} 
\usepackage{multirow}
\usepackage{color}
\usepackage{subcaption}
\usepackage{colortbl}
\usepackage{listings}
\usepackage{comment}
\usepackage{algorithm}

% Include other packages here, before hyperref.
\newcommand{\figref}[1]{Fig. \ref{#1}}
\newcommand{\tabref}[1]{Table \ref{#1}}
\newcommand{\equref}[1]{(\ref{#1})}
\newcommand{\secref}[1]{Sec. \ref{#1}}
\newcommand{\algref}[1]{Alg. \ref{#1}}

\usepackage{pifont}
\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}


\iccvfinalcopy % *** Uncomment this line for the final submission


\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
\ificcvfinal\pagestyle{empty}\fi

\begin{document}

%%%%%%%%% TITLE
\title{ExtremeNeRF: Few-shot Neural Radiance Fields Under Unconstrained Illumination}

\author{\vspace{.05cm} SeokyYeong Lee$^{1,2}$ \quad JunYong Choi$^{1,2}$ \quad Seungryong Kim$^{2}$\\ \vspace{.2cm} Ig-Jae Kim$^{1,3,4}$ \quad Junghyun Cho$^{1,3,4}$ \\
\vspace{.05cm}$^1$Korea Institute of Science and Technology, Seoul \quad $^2$Korea University, Seoul \\
$^{3}$AI-Robotics, KIST School, University of Science and Technology \\
$^{4}$Yonsei-KIST Convergence Research Institute, Yonsei University \\
{\tt\small\{shapin94, happily, drjay, jhcho\}@kist.re.kr} \quad \tt\small seungryong\_kim@korea.ac.kr \\
}
\renewcommand\footnotemark{}
%\renewcommand\footnoterule{}
\thanks{This work was partly supported by Institute of Information \& communications Technology Planning \& Evaluation (IITP) grant funded by the Korea government(MSIT)(No.2020-0-00457, 50\%) and KIST Institutional Program(Project No.2E32301, 50\%).}


\maketitle
% Remove page # from the first page of camera-ready.
\ificcvfinal\thispagestyle{empty}\fi

\input{tex/0_abstract}
\input{tex/1_introduction}
\input{tex/2_related_works}
\input{tex/3_preliminary}
\input{tex/5_proposed_method}
\input{tex/6_experiments}
\input{tex/7_conclusion}

%%%%%%%%% TITLE
\vspace{15pt}
\hspace{-10pt}
\textbf{{\huge Appendix}}
\vspace{5pt}
\appendix

\section{Additional Experimental Results}\label{A}
\subsection{Comparisons with NeROIC}
As mentioned in the paper, we would like to emphasize that none of the previous works deal with the same problem as us, which is a few-shot views synthesis of a non-object-centric scene under unconstrained illumination.
In the paper, we provide comparisons with the baselines of Barron et al.~\cite{barron2021mip} and Niemeyer et al.~\cite{niemeyer2022regnerf}.
While NeROIC~\cite{kuang2022neroic} and other related works such as NeuralPIL~\cite{NeuralPIL}, NeRD~\cite{boss2021nerd}, and SAMURAI~\cite{boss2022-samurai} deal with view synthesis under varying illumination, they have several differences from us, which we describe as follows.

\begin{itemize}
    \item All of these works can only handle object-centric, 360 scenes paired with foreground masks, but our proposed method is targeting non-object-centric, forward-facing scenes without additional masks. 
    \item Previous works~\cite{kuang2022neroic, NeuralPIL, boss2021nerd, boss2022-samurai} are an inverse rendering framework, which aims to decompose images into their geometry, material, and illumination. Therefore, they require massive computational costs with multi-stage training. Our proposed work is the first end-to-end few-shot synthesis framework that handles inputs with unconstrained illuminations with minimum computational costs~($3.5$ hours in a $3$-view setting).  
    \item Boss et al.~\cite{boss2021nerd} and following works~\cite{NeuralPIL, boss2022-samurai} does not assume few-shot settings. Especially, SAMURAI~\cite{boss2022-samurai} assumes unknown camera poses of inputs, unlike other previous works and our proposed method.
\end{itemize}

In the following, we additionally provide comparisons with NeROIC, on our NeRF Extreme dataset. Note that NeROIC is the only baseline that performs few-shot view synthesis with varying illumination inputs, which shows better performance compared to other baselines (detailed experimental results are on their paper, ~\cite{kuang2022neroic}) under unconstrained illumination conditions.
Comparisons with NeRF-W~\cite{nerfinthewild} are not available since 1) it is targeting outdoor scenes and 2) neither the implementation nor the pre-processed data are publicly available. 
\vspace{-10pt}

\paragraph{Comparison on NeRF Extreme.}
We compare our model with NeROIC on our NeRF Extreme dataset in the 3-view setting,  Tab.~\ref{tab:neroic_extreme} and Fig.~\ref{fig:neroic_extreme} show the quantitative and qualitative comparison results, respectively. NeROIC-Geom refers to the geometry network of NeROIC while NeROIC-Full refers to the full rendering network of NeROIC. Both NeROIC-Geom and NeROIC-full show over-smoothed or diverged results on our dataset, while our proposed ExtremeNeRF shows plausible view synthesis results. The results demonstrate the fact that none of the previous works can successfully perform few-shot view synthesis under unconstrained illumination if the target scene is the non-object-centric one. 
See Sec.~\ref{B} for more details about NeROIC. 


\begin{table}
    \centering
    \resizebox{\linewidth}{!}
    {\input{table/table_iccv_neroic_extreme}}
    \vspace{-.2cm}
    \caption{
    \textbf{Quantitative comparison on NeRF Extreme.} Note that results from NeROIC-Full are not included for their diverged outputs.}
    \label{tab:neroic_extreme} \vspace{-10pt}
\end{table}

\begin{figure}
  \centering
  \begin{tabular}{>{\centering\arraybackslash}p{0.20\textwidth}>{\centering\arraybackslash}p{0.22\textwidth}}
      \scriptsize NeROIC~\cite{kuang2022neroic}-Geom & \scriptsize  \textbf{ExtremeNeRF~(Ours)} \\
    \end{tabular}
  \includegraphics[width=\linewidth]{figure/ICCV/neroic_extreme3.pdf}
  %\fbox{\rule{0pt}{2in} \rule{.9\linewidth}{0pt}}
  \vspace{-15pt}
   \caption{\textbf{Qualitative comparison with NeROIC~\cite{kuang2022neroic} on NeRF Extreme.} A synthesized novel view by NeROIC~\cite{kuang2022neroic}-Geom and our proposed method in 3-view setting. NeROIC-Geom shows unclear, distorted results compared to ours. Note that synthesized results of NeROIC-Full are not included since the network diverged.}
   \label{fig:neroic_extreme} \vspace{-10pt}
\end{figure}



\subsection{Ablation Studies}
As shown in Tab.~4 of the paper, our model with ablation on each component show a larger difference in the quality of the underlying depth. For better analysis, we provide additional qualitative comparison which visualizes the difference in the quality of the synthesized depth. Fig.~\ref{fig:supp_ablation} shows the synthesized color and depth map with and without cross-view albedo consistency regularization. Our model with albedo consistency regularization shows a better performance in synthesizing depth compared to the one without albedo consistency. It also supports the result described in Fig.~8 of the paper, that neglecting illumination can result in distorted depth in few-shot view synthesis under unconstrained illumination. 

\begin{figure}
  \centering
  \begin{tabular}{>{\centering\arraybackslash}p{0.20\textwidth}>{\centering\arraybackslash}p{0.22\textwidth}}
      \scriptsize Ours w/o $\mathcal{L}_{\mathrm{ac}}$ & \scriptsize  Ours \\
    \end{tabular}
  \includegraphics[width=\linewidth]{figure/ICCV/ablation_qual2.pdf}
  \vspace{-15pt}
   \caption{\textbf{Qualitative comparison on ablation studies.} Ablation on our cross-view albedo consistency loss results in degraded synthesizing performance, especially in the depth map.}
   \label{fig:supp_ablation} \vspace{-10pt}
\end{figure}







\subsection{Relighting}
Unlike the other previous works that tried to achieve inverse rendering of a scene given inputs with unconstrained illumination, our proposed method focuses on a few-shot view synthesis task for practical usage. The architectural choice results in better computational efficiency, however, has its limitation in that it cannot explicitly decompose the illumination of the target scene. In this supplementary material, we additionally provide relighting results of our proposed method, inspired by the image formation model we use during intrinsic decomposition. As described in Eq.~\ref{eq:imageformation}, our inputs and synthesized images can be decomposed by their illumination-invariant color~(albedo), illumination-variant shading, light color, and non-Lambertian residuals. Because our proposed ExtremeNeRF provides per-scene optimization, we can relight the synthesized image by replacing the shading image, as long as the synthesized image remains plausible. Fig.~\ref{fig:relit} shows the relighting results, achieved by replacing the shading image of the synthesized scene. Note that shiny, blurry effects in the scenes are caused by \cite{li2018iidww}, which is used as our full-image intrinsic decomposition method.

\begin{figure}
    \begin{tabular}{>{\centering\arraybackslash}p{0.20\textwidth}>{\centering\arraybackslash}p{0.22\textwidth}}
      \scriptsize Before & \scriptsize  After \\
    \end{tabular}
  \centering
  \includegraphics[width=\linewidth]{figure/ICCV/relit.pdf}
  \vspace{-15pt}
   \caption{\textbf{Relighting results.}}
   \label{fig:relit} \vspace{-10pt}
\end{figure}


\subsection{Failure Cases} % outdoor scene cases. 기존 work 보다는 훨씬 distortion 이 적지만, plausible 하지 못하다는 것. 
If the outdoor scenes have challenging illumination, such as the \textit{tent} scene in our dataset, our proposed method may struggle to synthesize plausible results. Figure \ref{fig:failure} shows the synthesized results for the \textit{tent} scene produced by our method and the baseline method from \cite{niemeyer2022regnerf}. The less-qualified results may be due to inadequate decomposition of the intrinsic components of the scene, which causes the cross-view regularization to fail. However, despite the difficulties in regularizing the intrinsic components, our proposed method still shows better synthesizing performance compared to the baseline, which produces a highly distorted color image.

\begin{figure}
  \centering
  \begin{tabular}{>{\centering\arraybackslash}p{0.20\textwidth}>{\centering\arraybackslash}p{0.22\textwidth}}
      \scriptsize RegNeRF~\cite{niemeyer2022regnerf} & \scriptsize  \textbf{ExtremeNeRF(Ours)} \\
    \end{tabular}
  \includegraphics[width=\linewidth]{figure/ICCV/failures.pdf}
  \vspace{-15pt}
   \caption{\textbf{Failure cases.}}
   \label{fig:failure} \vspace{-10pt}
\end{figure}

\begin{figure*}[t]
    \centering
     \begin{tabular}{>{\centering\arraybackslash}p{0.3\textwidth}>{\centering\arraybackslash}p{0.3\textwidth}>{\centering\arraybackslash}p{0.3\textwidth}}
      \scriptsize mip-NeRF~\cite{barron2021mip} & \scriptsize  RegNeRF~\cite{niemeyer2022regnerf} & \scriptsize  \textbf{ExtremeNeRF~(Ours)}\\
    \end{tabular}
     \begin{subfigure}[b]{1.0\textwidth}
         \centering
        \includegraphics[width=\linewidth]{figure/ICCV/extreme_supp_3.pdf}z
         \caption{3 Input Views}
     \end{subfigure}
     \begin{subfigure}[b]{1.0\textwidth}
         \centering
        \includegraphics[width=\linewidth]{figure/ICCV/extreme_supp_6.pdf}        %
         \caption{6 Input Views}
     \end{subfigure}
     \begin{subfigure}[b]{1.0\textwidth}
         \centering
        \includegraphics[width=\linewidth]{figure/ICCV/extreme_supp_9.pdf}        
         \caption{9 Input Views}
     \end{subfigure}
    \vspace{-15pt}
    \caption{
    \textbf{Additional qualitative results on NeRF Extreme.}
    }
    \label{fig:suppEx} \vspace{-10pt}
\end{figure*}

\begin{figure*}[t]
    \centering
     \begin{tabular}{>{\centering\arraybackslash}p{0.3\textwidth}>{\centering\arraybackslash}p{0.3\textwidth}>{\centering\arraybackslash}p{0.3\textwidth}}
      \scriptsize mip-NeRF~\cite{barron2021mip} & \scriptsize  RegNeRF~\cite{niemeyer2022regnerf} & \scriptsize  \textbf{ExtremeNeRF~(Ours)}\\
    \end{tabular}
     \begin{subfigure}[b]{1.0\textwidth}
         \centering
        \includegraphics[width=\linewidth]{figure/ICCV/dtu_add.pdf}
     \end{subfigure}
    \vspace{-20pt}
    \caption{
    \textbf{Additional qualitative results on light-varying DTU~\cite{DTU} for 3 input views.}
    }
    \label{fig:suppDTU} \vspace{-10pt}
\end{figure*}


\subsection{Additional Qualitative Results}
\paragraph{NeRF Extreme.}
Figure \ref{fig:suppEx} shows additional qualitative comparisons for our NeRF Extreme benchmark, using 3, 6, and 9 view scenarios. Our method performs better than the others, especially in synthesizing depth. This demonstrates that our albedo regularization pipeline successfully removes the distortions that can arise in an unconstrained illumination environment.
\vspace{-10pt}

\paragraph{Light-varying DTU.}
Figure \ref{fig:suppDTU} shows additional qualitative results for DTU~\cite{DTU} using 3 view scenarios. Although our proposed method sometimes produces competitive or inferior quantitative results compared to RegNeRF~\cite{niemeyer2022regnerf} (as shown in Table 3 in the paper), it often produces less distorted results for both color and depth maps. We would like to emphasize that the DTU dataset is not an ideal input for our method, due to its insufficient global context, which is important for successful intrinsic decomposition.

\begin{figure*}[t]
    \centering
    \begin{subfigure}[b]{1.0\textwidth}
    \centering
        \includegraphics[width=.15\linewidth]{figure/scene_list/bench.jpg}
        \includegraphics[width=.15\linewidth]{figure/scene_list/cafe.jpg}
        \includegraphics[width=.15\linewidth]{figure/scene_list/flower.jpg}
        \includegraphics[width=.15\linewidth]{figure/scene_list/houseplant.jpg}
        \includegraphics[width=.15\linewidth]{figure/scene_list/kitchen.jpg}
        % \caption{3 Input Views}
        \vspace{-3pt}
     \end{subfigure}
     \begin{tabular}{>{\centering\arraybackslash}p{0.15\textwidth}>{\centering\arraybackslash}p{0.12\textwidth}>{\centering\arraybackslash}p{0.15\textwidth}>{\centering\arraybackslash}p{0.12\textwidth}>{\centering\arraybackslash}p{0.15\textwidth}}
    \scriptsize Bench  & \scriptsize Cafe & \scriptsize Flower  & \scriptsize Houseplant & \scriptsize Kitchen  \\
    \end{tabular}
    \begin{subfigure}[b]{1.0\textwidth}
    \centering
       \includegraphics[width=.15\linewidth]{figure/scene_list/leaves.jpg}
       \includegraphics[width=.15\linewidth]{figure/scene_list/room.jpg}
       \includegraphics[width=.15\linewidth]{figure/scene_list/table.jpg}
       \includegraphics[width=.15\linewidth]{figure/scene_list/tent.jpg}
       \includegraphics[width=.15\linewidth]{figure/scene_list/tree.jpg}
        % \caption{3 Input Views}
        \vspace{-3pt}
    \end{subfigure}
    \begin{tabular}{>{\centering\arraybackslash}p{0.15\textwidth}>{\centering\arraybackslash}p{0.12\textwidth}>{\centering\arraybackslash}p{0.15\textwidth}>{\centering\arraybackslash}p{0.12\textwidth}>{\centering\arraybackslash}p{0.15\textwidth}}
    \scriptsize Leaves  & \scriptsize Room & \scriptsize Table & \scriptsize Tent & \scriptsize Tree \\
    \end{tabular}
    \vspace{-5pt}
    \caption{
        \textbf{NeRF Extreme dataset.} This dataset is newly built to provide multi-view images under varying illumination, which can be used to train and evaluate the robust NeRF. Exemplified images are selected from their test sets with mild-lighting conditions.}
    \label{fig:scenelist} \vspace{-10pt}
\end{figure*}



\section{Experimental Details}\label{B}
In this section, we provide details about the proposed methods and experiments.
\subsection{Datasets}
\paragraph{NeRF Extreme.}
As already mentioned in the paper, we would like to emphasize that our proposed NeRF Extreme dataset, is the first frontal-facing, in-the-wild multi-view dataset with varying illumination. A detailed explanation of the dataset statistics can be found in Sec.~\ref{C}.
Fig.~\ref{fig:scenelist} shows the indoor and outdoor scenes included in our dataset.
In the experiments, we use image IDs of (0, 14, 29), (0, 5, 12, 17, 28, 29), (0, 4, 7, 11, 14, 18, 22, 25, 29) for 3, 6, and 9 views scenarios, respectively.  All images are used in a resolution of $300 \times 400$. %More detailed information about the configuration will be provided with the code. 
\vspace{-10pt}

\paragraph{Light-varying DTU.}
DTU~\cite{DTU} consists of images taken under structured cameras and light sources. There exist 7 number of lighting variations per scene. Previous works which deal with view synthesis under consistent illumination~\cite{mildenhall2020nerf, dietnerf, yu2021pixelnerf, srf, barron2021mip, niemeyer2022regnerf, CLIPnerf, infonerf, chen2021mvsnerf, xu2022sinnerf} have used the dataset with fixed mild lighting condition.% - noted as 3, which refers to the mild lighting condition. 
In this paper, we randomly choose lighting conditions for each scene. Note that view synthesis performance may vary a lot depending on which viewing directions and lighting conditions are selected. Following the evaluation protocol used by the previous works~\cite{yu2021pixelnerf, niemeyer2022regnerf}, we use scan IDs (8, 21, 30, 31, 34, 38, 40, 41, 45, 55, 63, 82, 103, 110, 114) as the dataset, while using image IDs (25, 22, 28) as inputs. In the cases of the lighting condition, (4, 1, 5) are used for (25, 22, 28) images, respectively. All images are used in a resolution of $300 \times 400$, following the evaluation protocols of the previous work~\cite{niemeyer2022regnerf}. Note that all metrics are calculated without masks.




\subsection{Baselines}
In the cases of RegNeRF~\cite{niemeyer2022regnerf}, the official configurations for DTU~\cite{DTU} and LLFF~\cite{LLFF} are used for evaluations of light-varying DTU and NeRF Extreme, respectively, owing to their identical or similar data characteristics. 
For light-varying DTU, 44K train epochs are used for 3 view cases, for both RegNeRF and ours. For NeRF Extreme, 64K, 140K, and 21K train epochs are used for 3, 6, and 9 view cases, respectively. Our ExtremeNeRF shares the configurations with RegNeRF. Note that the official implementation of RegNeRF lacks color regularization done by the normalizing-flow model.
In the case of NeROIC~\cite{kuang2022neroic}, we use the publicly available instructions and code for the experiments. Since the authors have notified that the proposed method is not able to handle the frontal-facing scenes, we adjust the implementation code to be able to run on the frontal-facing scene. Instead of the foreground masks which should be provided to run the code, we enable all the image pixels to be regarded as foreground ones. It may result in degraded performance of the baseline, however, is the only way to make a comparison. We follow the provided configurations for the training options. 

\begin{table*}
    \centering
    \resizebox{\linewidth}{!}
    {\input{table/table_PIDNet_architecture}}
    \vspace{-.2cm}
    \caption{
    \textbf{Patch-wise intrinsic decomposition network(PIDNet) architecture. }}
    \label{tab:PIDNetarch} \vspace{-10pt}
\end{table*}

\subsection{Metrics}
\paragraph{Cross-Color-Ratio~(CCR). }
Cross-color-ratio (CCR)~\cite{gevers1999color} refers to the illumination-invariant image gradients that only depend on the albedo transitions of an image. For two adjacent $RGB$ pixels $x_1, x_2$, CCRs are defined as:
\begin{equation}
     M_{RG} = \cfrac{R_{x_1}G_{x_2}}{R_{x_2}G_{x_1}}, 
     M_{RB} = \cfrac{R_{x_1}B_{x_2}}{R_{x_2}B_{x_1}},
     M_{GB} = \cfrac{G_{x_1}B_{x_2}}{G_{x_2}B_{x_1}}, 
\label{eq:CCR}
\end{equation}
\vspace{-10pt}

\paragraph{Cross-Color-Ratio-Difference~(CCRD).}
According to Das et al.~\cite{das2022pie}, CCR reflects albedo properties sufficiently to improve the performance of intrinsic decomposition. By taking the logarithm of both side of Eq.~\ref{eq:CCR} as suggested by \cite{das2022pie}, our cross-color-ratio-difference (CCRD) metric can be formulated as:
\begin{equation}
     \text{CCRD} = \sum_{x \in \mathcal{P}} |\mathrm{ccr}(x) - \mathrm{ccr}_{\text{GT}}(x)|
\label{eq:CCRD}
\end{equation}
where $\mathrm{ccr}(x)$ and $\mathrm{ccr}_{\text{GT}}(x)$ indicate log-scale CCR values at a pixel $x$ of the synthesized image and the ground-truth image, respectively, and $\mathcal{P}$ denotes all pixels of the image.
\vspace{-10pt}

\paragraph{Absolute Relative error~(Abs Rel).} 
Absolute Relative error~(Abs Rel) is one of the most commonly used metrics for depth estimation tasks. In order to measure the scale-invariant plausibility of the synthesized depth, we adopted Abs Rel as:
\begin{equation}
     \text{Abs Rel} = \sum_{x \in \mathcal{P}} |\bar{d}(x) - \bar{d}_{\text{GT}}(x)|,
\label{eq:CCRD}
\end{equation}
where $\bar{d}(x)$ and $\bar{d}_{\text{GT}}(x)$ indicate the synthesized depth and the ground truth depth, which are normalized to scale 0 to 1, respectively. Since most of the view-synthesis datasets except DTU~\cite{DTU} are not providing depth information, we estimated the depth maps by ~\cite{giang2022curvatureguided} and used them as a pseudo-depth ground truth.


\subsection{Network Architectures} 
\paragraph{FIDNet architectures.}
IIDWW\cite{li2018iidww} uses a variant of UNet\cite{pix2pix,unet} architectures with a shared encoder and 2 decoders. A 3-dimensional light color $c$ is also predicted as a by-product of the network. The publicly available model takes $256 \times 384$ sized full-resolution images as inputs. Images are resized before and after intrinsic decomposition.
\vspace{-10pt}

\paragraph{PIDNet architectures.}
As a downsized model of FIDNet, PIDNet consists of 4 numbers of $4 \times 4$ sized convolution/deconvolution layers that are connected to each other using skip-connections, with additional FC layers for light color prediction. Tab.~\ref{tab:PIDNetarch} shows the details of PIDNet. Note that PIDNet takes $32 \times 32$ sized patch as an input. Decoders for albedo and shading are identical except for the last channel dimension ($3$ for the albedo and $1$ for the shading image).

\begin{algorithm}[t]
\caption{Depth Consistency, Pytorch-like}
\label{code:dcloss}
\definecolor{codeblue}{rgb}{0.25,0.5,0.5}
\definecolor{codekw}{rgb}{0.85, 0.18, 0.50}

\lstset{
  backgroundcolor=\color{white},
  basicstyle=\fontsize{7.5pt}{7.5pt}\ttfamily\selectfont,
  columns=fullflexible,
  breaklines=true,
  captionpos=b,
  commentstyle=\fontsize{7.5pt}{7.5pt}\color{codeblue},
  keywordstyle=\fontsize{7.5pt}{7.5pt}\color{codekw},
}
\begin{lstlisting}[language=python]
def get_corresponding_coords(K, coords, c2w_1, c2w_2, depth):
    im_coord = coords*depth
    cam_coord = np.linalg.inv(K) @ im_coord
    w2c = np.linalg.inv(c2w_2)
    cam_mat = w2c @ c2w_1
    return K @ cam_mat @ cam_coord

def init_mask_tg_rays(tg_coords, full_rays):
    lrc_mask = np.ones_like(tg_coords)
    lrc_mask = (0 if outside of image region else 1)
    tg_rays = full_rays[tg_coords]
    return lrc_mask, tg_rays

# src renderings from MLP
src_renderings = model.apply(src_rays)
depth = src_renderings['depth_pred']

# calc target coord & rays
uvd_target = get_corresponding_coords
          (K, src_coords, src_pose, tg_pose, depth)
tg_coords = uvd_target[:2,:]//uvd_target[2,:]

depth_tilde = uvd_target[2,:]
lrc_mask, tg_rays = init_mask_tg_rays
          (tg_coords, tg_full_rays)

# tg renderings from MLP
tg_renderings = model.apply(tg_rays)
depth_hat = tg_renderings['depth_pred']

error_proj = (depth_tilde - depth_hat)**2

loss_dc = np.mean(tv_norm(error_proj))
\end{lstlisting}
\end{algorithm}

\begin{algorithm}[t]
\caption{Albedo Consistency, Pytorch-like}
\label{code:acloss}
\definecolor{codeblue}{rgb}{0.25,0.5,0.5}
\definecolor{codekw}{rgb}{0.85, 0.18, 0.50}

\lstset{
  backgroundcolor=\color{white},
  basicstyle=\fontsize{7.5pt}{7.5pt}\ttfamily\selectfont,
  columns=fullflexible,
  breaklines=true,
  captionpos=b,
  commentstyle=\fontsize{7.5pt}{7.5pt}\color{codeblue},
  keywordstyle=\fontsize{7.5pt}{7.5pt}\color{codekw},
}
\begin{lstlisting}[language=python]

src_patch = src_renderings['rgb']
src_shading, src_albedo, src_rgb = pidnet.apply(src_patch)

# intrinsic smoothness loss
pid_loss = np.mean(tv_norm(src_albedo))

# get target albedo from FIDNet result
tg_albedo = tg_full_albedo[tg_coords]

tg_chrome = rgb_to_chromaticity(tg_albedo)
src_chrome = rgb_to_chromaticity(src_albedo)
src_patch_chrome = rgb_to_chromaticity(src_patch)

# chromaticity consistency loss
loss_chrom = np.mean((src_chrome - tg_chrome)**2 
            + (src_chrome - src_patch_chrome)**2)

# scale value using least square
tg_albedo = ls_scale_val(src_albedo, tg_albedo)

occ_weight = r_e * (1- error_proj/max(error_proj))

# albedo consistency loss
loss_ac = np.mean(occ_weight*(src_albedo - tg_albedo)**2)

src_grad = np.gradient(np.exp(lrc_mask*src_albedo))
tg_grad = np.gradient(np.exp(lrc_mask*tg_albedo))

# edge preserving loss
loss_edge = np.mean(occ_weight*(src_grad - tg_grad)**2)

\end{lstlisting}
\end{algorithm}

\subsection{Loss Functions}
\paragraph{Edge-preserving loss. }
Motivated by \cite{godard2017unsupervised}, we use the gradient-based edge-preserving loss, to enforce the input and the novel view patches to preserve geometric properties. Using an occlusion-aware weight term, $\omega_{occ}(x)$, which already has been discussed in the paper, our edge-preserving loss on the predicted albedo can be formulated as:
\begin{equation} 
\begin{split}
     \mathcal{L}_{\mathrm{edge}} =\sum_{x' \in \mathcal{P}'} \omega_{occ}(x)
     \|\partial(\hat{a}(x) - \hat{a}(x'))\|^2 ,
\end{split}
\label{eq:edge}
\end{equation}
where $\partial$ denotes the partial derivatives of the vertical and the horizontal directions, and $\mathcal{P'}$ are all the pixels in the target image. 
\vspace{-10pt}


\paragraph{Patch-wise intrinsic smoothness loss.}
Similar to the previous work~\cite{li2018iidww} which uses various kinds of smoothness terms to give constraints to the network, we give smoothness constraints to our patch-wise intrinsic decomposition network~(PIDNet). Our patch-wise intrinsic smoothness loss is formulated as:
\begin{equation}
\begin{split}
    \mathcal{L}_{\mathrm{pid}} = \sum_{x' \in \mathcal{P}'} \sum_{y \in \mathcal{N}(x')} \|\hat{a}(y)\!-\hat{a}(x')\|^2 ,
\end{split}
\label{eq:pid}
\end{equation}
where $y$ is one of the 4-neighbor adjacent pixels $\mathcal{N}(x')$ for $x'$, and $\mathcal{P}'$ denotes all the pixels of the target image.
\vspace{-10pt}

\paragraph{Chromaticity consistency loss.}
Similar to \cite{ye2022intrinsicnerf}, we adopted the chromaticity consistency loss to enforce the consistency between the input and novel view patches. Our chromaticity consistency loss is formulated as:
\begin{equation}
  \mathcal{L}_{\mathrm{chrom}} = \sum_{x' \in \mathcal{P'}} \|\hat{ch}(x) - \hat{ch}(x')\|^2_2,
  \label{eq:ac}
\end{equation}
where $\hat{ch}(x)$ and $\hat{ch}(x')$ indicate the extracted albedo at $x$ and $x'$ from the novel view and the input view, respectively.
$\mathcal{P}$ denotes all the pixels in the novel view.
\vspace{-10pt}

\paragraph{Total loss functions. }
For each $x$ and $x'$ in the batch-wisely sampled input and the novel view patch, the total loss of our proposed framework is
\begin{equation}
\begin{split}
    \mathcal{L}_{\mathrm{total}}  =  & \lambda_{\mathrm{c}}\mathcal{L}_{\mathrm{color}} + \lambda_{\mathrm{a}}\mathcal{L}_{\mathrm{ac}} + \lambda_{\mathrm{dc}}\mathcal{L}_{\mathrm{dc}} + \lambda_{\mathrm{ds}}\mathcal{L}_{\mathrm{ds}} \\
    &+ \lambda_{\mathrm{e}}\mathcal{L}_{\mathrm{edge}} + \lambda_{\mathrm{pid}}\mathcal{L}_{\mathrm{pid}} + \lambda_{\mathrm{chrom}}\mathcal{L}_{\mathrm{chrom}}, 
\end{split}
\label{eq:total}
\end{equation}
where $\lambda_{\mathrm{c}}, \lambda_{\mathrm{a}}, \lambda_{\mathrm{dc}}, \lambda_{\mathrm{ds}}, \lambda_{\mathrm{edge}}, \lambda_{\mathrm{pid}}, \lambda_{\mathrm{chrom}}$ are weights parameters for each loss, respectively. In our experiment, losses are weighted as: $\lambda_{\mathrm{c}} = 1.0$, $\lambda_{\mathrm{a}} = 1.0$, $\lambda_{\mathrm{dc}} = 1.0$, $\lambda_{\mathrm{ds}} = 0.1$, $\lambda_{\mathrm{edge}} = 0.01$, $\lambda_{\mathrm{pid}} = 1.0$,  $\lambda_{\mathrm{chrom}} = 0.01$. We provide detailed algorithms in Alg.~\ref{code:dcloss} and \ref{code:acloss}.

\begin{figure}
  \centering
  \includegraphics[width=\linewidth]{figure/projection.pdf}
  \vspace{-15pt}
   \caption{\textbf{Projection situation examples.} Blue and yellow arrows are the synthesized depths $\hat{d}(x)$ and $\hat{d}(x')$, respectively, while green arrow is $\Tilde{d}(x')$ obtained by projective transformation.}
   \label{fig:projection} \vspace{-10pt}
\end{figure}

\subsection{Occlusion Handling}
\paragraph{Problem situations.}
In Fig.~\ref{fig:projection}, we illustrate possible situations that may occur in the geometry alignment stage.
\begin{itemize}
    \item (a): Projection to $x_1$ shows a situation where projection error $\mathcal{E}_\mathrm{proj}(x_1)$ is caused by the self-occlusion. It refers to the expected occlusion error case, which does not relate to the ill-synthesized geometry. Using occlusion mask $m(x)$ can exclude the projection case since it should be maintained even after the end of the optimization.
    \item (b): Projection to $x_2$ shows an ideal projection case. The synthesized depth $\hat{d}(x_2)$ is identical to the depth $\Tilde{d}(x_2)$ obtained by projective-transformation - i.e. $\mathcal{E}_\mathrm{proj}(x_2) = 0$.
    \item (c): Projection to $x_3$ shows a projection error caused by the false-positive density measure of NeRF. By enforcing $\mathcal{E}_\mathrm{proj}(x_3)$ to become zero, the floating artifacts can be expected to be removed.
    \item (d): Projection to $x_4$ shows a zero projection error which cannot be optimized by enforcing $\mathcal{E}_\mathrm{proj} = 0$. It is because the projection error $\mathcal{E}_\mathrm{proj}(x_4)$ is already 0. By assuming that the projection errors caused by self-occlusions would have smooth projection errors while ill-synthesized artifacts are not, we can optimize the depth consistency loss $\mathcal{L}_{\mathrm{dc}}$ described in Eq. 8 in the paper.
\end{itemize}
\vspace{-10pt}
% occlusion handling 관련 내용 추가

\paragraph{Error rate coefficient $r_e$.}
As described earlier, our projection error $\mathcal{E}_\mathrm{proj}$ includes errors that result from both occlusions and ill-synthesized depth. To prevent over-smoothing of synthesized results on occluded areas, errors resulting from occlusion should be disregarded during training. However, our occlusion-aware weight term is formulated as Eq.~8 in the paper. If the network can effectively regularize the ill-synthesized depth, a portion of the occlusion pairs included as a regularization target will increase during training. To prevent the regularization of occlusion pairs, the error rate coefficient $r_e$ will decrease the contribution of the consistency regularization as training progresses.


\subsection{Image Formation Model}
Basic intrinsic decomposition methods are based on the Lambertian assumption. 
However, most real-world objects have surface characteristics whose reflectances vary upon viewing directions. Thus, we use the image formation model suggested by \cite{li2018iidww} as follows:
\begin{equation}
\begin{split}
    \log I = \log A + \log S + c + N ,
\end{split}
\label{eq:imageformation}
\end{equation}
which takes light color vector $c$ and non-Lambertian residuals $N$ into account.



\section{NeRF Extreme}\label{C}


\subsection{Pose Estimation}
Similar to LLFF~\cite{LLFF}, all the camera poses are obtained by COLMAP~\cite{colmap} structure from motion framework. All the images and corresponding camera poses will be publicly available. 


\subsection{Dataset Statistics}

\begin{figure}
  \centering
  \includegraphics[width=\linewidth]{figure/graph_2.png}
  \vspace{-20pt}
   \caption{\textbf{Intensity distribution of our NeRF Extreme and LLFF~\cite{LLFF}.} Our dataset shows a larger variance in per-image lighting intensity distribution than LLFF~\cite{LLFF}. Lighting intensities are obtained from the shading images extracted by \cite{das2022pie}.}
   \label{fig:graph} \vspace{-10pt}
\end{figure}

In order to verify the illumination diversity of our dataset, we extract per-image intensity distribution from shading images, which are obtained by the state-of-the-art intrinsic decomposition framework~\cite{das2022pie}. A shading image is suitable for evaluating the illumination diversity of a dataset since it provides environment-dependent information about the image. Fig.~\ref{fig:graph} shows an intensity distribution of ours and existing LLFF~\cite{LLFF}, which has similar dataset characteristics. Each dot in the distribution indicates per-image intensity characteristics. Our dataset shows more scattered distribution compared to LLFF, indicating a larger illumination diversity.

\subsection{Lighting Variations}
All of the images in our dataset were captured using off-the-shelf cameras. Galaxy z-flip 4 was used to reproduce the casual image-capturing setup. Fig.~\ref{fig:tentall} and \ref{fig:benchall} show all the training and the test images(2 rows from the last) belonging to our \textit{tent} and \textit{bench} scene, respectively. Outdoor scenes are captured at different times and in different sunlight, to be taken under varying illumination conditions. Fig.~\ref{fig:tableall} and \ref{fig:roomall} show all the training and test images belonging to our \textit{table} and \textit{room} scenes, respectively. Indoor scenes are captured with turned-on/off lights, and closed/open curtains, to get illumination variance. 




\clearpage


\begin{figure*}
    \centering
    \begin{subfigure}[b]{1.0\textwidth}
         \centering
        \includegraphics[width=.18\linewidth]{figure/table/000.jpg}
        \includegraphics[width=.18\linewidth]{figure/table/001.jpg}
        \includegraphics[width=.18\linewidth]{figure/table/002.jpg}
        \includegraphics[width=.18\linewidth]{figure/table/003.jpg}
        \includegraphics[width=.18\linewidth]{figure/table/004.jpg}
        \vspace{.125cm}
    \end{subfigure}
    \begin{subfigure}[b]{1.0\textwidth}
    \centering
        \includegraphics[width=.18\linewidth]{figure/table/005.jpg}
        \includegraphics[width=.18\linewidth]{figure/table/006.jpg}
        \includegraphics[width=.18\linewidth]{figure/table/007.jpg}
        \includegraphics[width=.18\linewidth]{figure/table/008.jpg}
        \includegraphics[width=.18\linewidth]{figure/table/009.jpg}
        \vspace{.125cm}
    \end{subfigure}
    \begin{subfigure}[b]{1.0\textwidth}
    \centering
        \includegraphics[width=.18\linewidth]{figure/table/010.jpg}
        \includegraphics[width=.18\linewidth]{figure/table/011.jpg}
        \includegraphics[width=.18\linewidth]{figure/table/012.jpg}
        \includegraphics[width=.18\linewidth]{figure/table/013.jpg}
        \includegraphics[width=.18\linewidth]{figure/table/014.jpg}
        \vspace{.125cm}
    \end{subfigure}
    \begin{subfigure}[b]{1.0\textwidth}
    \centering
        \includegraphics[width=.18\linewidth]{figure/table/015.jpg}
        \includegraphics[width=.18\linewidth]{figure/table/016.jpg}
        \includegraphics[width=.18\linewidth]{figure/table/017.jpg}
        \includegraphics[width=.18\linewidth]{figure/table/018.jpg}
        \includegraphics[width=.18\linewidth]{figure/table/019.jpg}
        \vspace{.125cm}
    \end{subfigure}
    \begin{subfigure}[b]{1.0\textwidth}
    \centering
        \includegraphics[width=.18\linewidth]{figure/table/020.jpg}
        \includegraphics[width=.18\linewidth]{figure/table/021.jpg}
        \includegraphics[width=.18\linewidth]{figure/table/022.jpg}
        \includegraphics[width=.18\linewidth]{figure/table/023.jpg}
        \includegraphics[width=.18\linewidth]{figure/table/024.jpg}
        \vspace{.125cm}
    \end{subfigure}
    \begin{subfigure}[b]{1.0\textwidth}
    \centering
        \includegraphics[width=.18\linewidth]{figure/table/025.jpg}
        \includegraphics[width=.18\linewidth]{figure/table/026.jpg}
        \includegraphics[width=.18\linewidth]{figure/table/027.jpg}
        \includegraphics[width=.18\linewidth]{figure/table/028.jpg}
        \includegraphics[width=.18\linewidth]{figure/table/029.jpg}
        \vspace{.125cm}
    \end{subfigure}
    \begin{subfigure}[b]{1.0\textwidth}
    \centering
        \includegraphics[width=.18\linewidth]{figure/table/101.jpg}
        \includegraphics[width=.18\linewidth]{figure/table/102.jpg}
        \includegraphics[width=.18\linewidth]{figure/table/103.jpg}
        \includegraphics[width=.18\linewidth]{figure/table/104.jpg}
        \includegraphics[width=.18\linewidth]{figure/table/105.jpg}
        \vspace{.125cm}
    \end{subfigure}
    \begin{subfigure}[b]{1.0\textwidth}
    \centering
        \includegraphics[width=.18\linewidth]{figure/table/106.jpg}
        \includegraphics[width=.18\linewidth]{figure/table/107.jpg}
        \includegraphics[width=.18\linewidth]{figure/table/108.jpg}
        \includegraphics[width=.18\linewidth]{figure/table/109.jpg}
        \includegraphics[width=.18\linewidth]{figure/table/110.jpg}
    \end{subfigure}
     \vspace{-20pt}
    \caption{
        \textbf{Illumination variation samples of the \textit{table} scene in NeRF Extreme. }}
    \label{fig:tableall} \vspace{-10pt}
\end{figure*}


\begin{figure*}
    \centering
    \begin{subfigure}[b]{1.0\textwidth}
         \centering
        \includegraphics[width=.18\linewidth]{figure/room/000.jpg}
        \includegraphics[width=.18\linewidth]{figure/room/001.jpg}
        \includegraphics[width=.18\linewidth]{figure/room/002.jpg}
        \includegraphics[width=.18\linewidth]{figure/room/003.jpg}
        \includegraphics[width=.18\linewidth]{figure/room/004.jpg}
        \vspace{.125cm}
    \end{subfigure}
    \begin{subfigure}[b]{1.0\textwidth}
    \centering
        \includegraphics[width=.18\linewidth]{figure/room/005.jpg}
        \includegraphics[width=.18\linewidth]{figure/room/006.jpg}
        \includegraphics[width=.18\linewidth]{figure/room/007.jpg}
        \includegraphics[width=.18\linewidth]{figure/room/008.jpg}
        \includegraphics[width=.18\linewidth]{figure/room/009.jpg}
        \vspace{.125cm}
    \end{subfigure}
    \begin{subfigure}[b]{1.0\textwidth}
    \centering
        \includegraphics[width=.18\linewidth]{figure/room/010.jpg}
        \includegraphics[width=.18\linewidth]{figure/room/011.jpg}
        \includegraphics[width=.18\linewidth]{figure/room/012.jpg}
        \includegraphics[width=.18\linewidth]{figure/room/013.jpg}
        \includegraphics[width=.18\linewidth]{figure/room/014.jpg}
        \vspace{.125cm}
    \end{subfigure}
    \begin{subfigure}[b]{1.0\textwidth}
    \centering
        \includegraphics[width=.18\linewidth]{figure/room/015.jpg}
        \includegraphics[width=.18\linewidth]{figure/room/016.jpg}
        \includegraphics[width=.18\linewidth]{figure/room/017.jpg}
        \includegraphics[width=.18\linewidth]{figure/room/018.jpg}
        \includegraphics[width=.18\linewidth]{figure/room/019.jpg}
        \vspace{.125cm}
    \end{subfigure}
    \begin{subfigure}[b]{1.0\textwidth}
    \centering
        \includegraphics[width=.18\linewidth]{figure/room/020.jpg}
        \includegraphics[width=.18\linewidth]{figure/room/021.jpg}
        \includegraphics[width=.18\linewidth]{figure/room/022.jpg}
        \includegraphics[width=.18\linewidth]{figure/room/023.jpg}
        \includegraphics[width=.18\linewidth]{figure/room/024.jpg}
        \vspace{.125cm}
    \end{subfigure}
    \begin{subfigure}[b]{1.0\textwidth}
    \centering
        \includegraphics[width=.18\linewidth]{figure/room/025.jpg}
        \includegraphics[width=.18\linewidth]{figure/room/026.jpg}
        \includegraphics[width=.18\linewidth]{figure/room/027.jpg}
        \includegraphics[width=.18\linewidth]{figure/room/028.jpg}
        \includegraphics[width=.18\linewidth]{figure/room/029.jpg}
        \vspace{.125cm}
    \end{subfigure}
    \begin{subfigure}[b]{1.0\textwidth}
    \centering
        \includegraphics[width=.18\linewidth]{figure/room/101.jpg}
        \includegraphics[width=.18\linewidth]{figure/room/102.jpg}
        \includegraphics[width=.18\linewidth]{figure/room/103.jpg}
        \includegraphics[width=.18\linewidth]{figure/room/104.jpg}
        \includegraphics[width=.18\linewidth]{figure/room/105.jpg}
        \vspace{.125cm}
    \end{subfigure}
    \begin{subfigure}[b]{1.0\textwidth}
    \centering
        \includegraphics[width=.18\linewidth]{figure/room/106.jpg}
        \includegraphics[width=.18\linewidth]{figure/room/107.jpg}
        \includegraphics[width=.18\linewidth]{figure/room/108.jpg}
        \includegraphics[width=.18\linewidth]{figure/room/109.jpg}
        \includegraphics[width=.18\linewidth]{figure/room/110.jpg}
    \end{subfigure}
     \vspace{-20pt}
    \caption{
        \textbf{Illumination variation samples of the \textit{room} scene in NeRF Extreme. }}
    \label{fig:roomall} \vspace{-10pt}
\end{figure*}


\begin{figure*}
    \centering
    \begin{subfigure}[b]{1.0\textwidth}
         \centering
        \includegraphics[width=.18\linewidth]{figure/tent/000.jpg}
        \includegraphics[width=.18\linewidth]{figure/tent/001.jpg}
        \includegraphics[width=.18\linewidth]{figure/tent/002.jpg}
        \includegraphics[width=.18\linewidth]{figure/tent/003.jpg}
        \includegraphics[width=.18\linewidth]{figure/tent/004.jpg}
        \vspace{.125cm}
    \end{subfigure}
    \begin{subfigure}[b]{1.0\textwidth}
    \centering
        \includegraphics[width=.18\linewidth]{figure/tent/005.jpg}
        \includegraphics[width=.18\linewidth]{figure/tent/006.jpg}
        \includegraphics[width=.18\linewidth]{figure/tent/007.jpg}
        \includegraphics[width=.18\linewidth]{figure/tent/008.jpg}
        \includegraphics[width=.18\linewidth]{figure/tent/009.jpg}
        \vspace{.125cm}
    \end{subfigure}
    \begin{subfigure}[b]{1.0\textwidth}
    \centering
        \includegraphics[width=.18\linewidth]{figure/tent/010.jpg}
        \includegraphics[width=.18\linewidth]{figure/tent/011.jpg}
        \includegraphics[width=.18\linewidth]{figure/tent/012.jpg}
        \includegraphics[width=.18\linewidth]{figure/tent/013.jpg}
        \includegraphics[width=.18\linewidth]{figure/tent/014.jpg}
        \vspace{.125cm}
    \end{subfigure}
    \begin{subfigure}[b]{1.0\textwidth}
    \centering
        \includegraphics[width=.18\linewidth]{figure/tent/015.jpg}
        \includegraphics[width=.18\linewidth]{figure/tent/016.jpg}
        \includegraphics[width=.18\linewidth]{figure/tent/017.jpg}
        \includegraphics[width=.18\linewidth]{figure/tent/018.jpg}
        \includegraphics[width=.18\linewidth]{figure/tent/019.jpg}
        \vspace{.125cm}
    \end{subfigure}
    \begin{subfigure}[b]{1.0\textwidth}
    \centering
        \includegraphics[width=.18\linewidth]{figure/tent/020.jpg}
        \includegraphics[width=.18\linewidth]{figure/tent/021.jpg}
        \includegraphics[width=.18\linewidth]{figure/tent/022.jpg}
        \includegraphics[width=.18\linewidth]{figure/tent/023.jpg}
        \includegraphics[width=.18\linewidth]{figure/tent/024.jpg}
        \vspace{.125cm}
    \end{subfigure}
    \begin{subfigure}[b]{1.0\textwidth}
    \centering
        \includegraphics[width=.18\linewidth]{figure/tent/025.jpg}
        \includegraphics[width=.18\linewidth]{figure/tent/026.jpg}
        \includegraphics[width=.18\linewidth]{figure/tent/027.jpg}
        \includegraphics[width=.18\linewidth]{figure/tent/028.jpg}
        \includegraphics[width=.18\linewidth]{figure/tent/029.jpg}
        \vspace{.125cm}
    \end{subfigure}
    \begin{subfigure}[b]{1.0\textwidth}
    \centering
        \includegraphics[width=.18\linewidth]{figure/tent/101.jpg}
        \includegraphics[width=.18\linewidth]{figure/tent/102.jpg}
        \includegraphics[width=.18\linewidth]{figure/tent/103.jpg}
        \includegraphics[width=.18\linewidth]{figure/tent/104.jpg}
        \includegraphics[width=.18\linewidth]{figure/tent/105.jpg}
        \vspace{.125cm}
    \end{subfigure}
    \begin{subfigure}[b]{1.0\textwidth}
    \centering
        \includegraphics[width=.18\linewidth]{figure/tent/106.jpg}
        \includegraphics[width=.18\linewidth]{figure/tent/107.jpg}
        \includegraphics[width=.18\linewidth]{figure/tent/108.jpg}
        \includegraphics[width=.18\linewidth]{figure/tent/109.jpg}
        \includegraphics[width=.18\linewidth]{figure/tent/110.jpg}
    \end{subfigure}
     \vspace{-20pt}
    \caption{
        \textbf{Illumination variation samples of the \textit{tent} scene in NeRF Extreme. }}
    \label{fig:tentall} \vspace{-10pt}
\end{figure*}

\begin{figure*}
    \centering
    \begin{subfigure}[b]{1.0\textwidth}
         \centering
        \includegraphics[width=.18\linewidth]{figure/bench/000.jpg}
        \includegraphics[width=.18\linewidth]{figure/bench/001.jpg}
        \includegraphics[width=.18\linewidth]{figure/bench/002.jpg}
        \includegraphics[width=.18\linewidth]{figure/bench/003.jpg}
        \includegraphics[width=.18\linewidth]{figure/bench/004.jpg}
        \vspace{.125cm}
    \end{subfigure}
    \begin{subfigure}[b]{1.0\textwidth}
    \centering
        \includegraphics[width=.18\linewidth]{figure/bench/005.jpg}
        \includegraphics[width=.18\linewidth]{figure/bench/006.jpg}
        \includegraphics[width=.18\linewidth]{figure/bench/007.jpg}
        \includegraphics[width=.18\linewidth]{figure/bench/008.jpg}
        \includegraphics[width=.18\linewidth]{figure/bench/009.jpg}
        \vspace{.125cm}
    \end{subfigure}
    \begin{subfigure}[b]{1.0\textwidth}
    \centering
        \includegraphics[width=.18\linewidth]{figure/bench/010.jpg}
        \includegraphics[width=.18\linewidth]{figure/bench/011.jpg}
        \includegraphics[width=.18\linewidth]{figure/bench/012.jpg}
        \includegraphics[width=.18\linewidth]{figure/bench/013.jpg}
        \includegraphics[width=.18\linewidth]{figure/bench/014.jpg}
        \vspace{.125cm}
    \end{subfigure}
    \begin{subfigure}[b]{1.0\textwidth}
    \centering
        \includegraphics[width=.18\linewidth]{figure/bench/015.jpg}
        \includegraphics[width=.18\linewidth]{figure/bench/016.jpg}
        \includegraphics[width=.18\linewidth]{figure/bench/017.jpg}
        \includegraphics[width=.18\linewidth]{figure/bench/018.jpg}
        \includegraphics[width=.18\linewidth]{figure/bench/019.jpg}
        \vspace{.125cm}
    \end{subfigure}
    \begin{subfigure}[b]{1.0\textwidth}
    \centering
        \includegraphics[width=.18\linewidth]{figure/bench/020.jpg}
        \includegraphics[width=.18\linewidth]{figure/bench/021.jpg}
        \includegraphics[width=.18\linewidth]{figure/bench/022.jpg}
        \includegraphics[width=.18\linewidth]{figure/bench/023.jpg}
        \includegraphics[width=.18\linewidth]{figure/bench/024.jpg}
        \vspace{.125cm}
    \end{subfigure}
    \begin{subfigure}[b]{1.0\textwidth}
    \centering
        \includegraphics[width=.18\linewidth]{figure/bench/025.jpg}
        \includegraphics[width=.18\linewidth]{figure/bench/026.jpg}
        \includegraphics[width=.18\linewidth]{figure/bench/027.jpg}
        \includegraphics[width=.18\linewidth]{figure/bench/028.jpg}
        \includegraphics[width=.18\linewidth]{figure/bench/029.jpg}
        \vspace{.125cm}
    \end{subfigure}
    \begin{subfigure}[b]{1.0\textwidth}
    \centering
        \includegraphics[width=.18\linewidth]{figure/bench/101.jpg}
        \includegraphics[width=.18\linewidth]{figure/bench/102.jpg}
        \includegraphics[width=.18\linewidth]{figure/bench/103.jpg}
        \includegraphics[width=.18\linewidth]{figure/bench/104.jpg}
        \includegraphics[width=.18\linewidth]{figure/bench/105.jpg}
        \vspace{.125cm}
    \end{subfigure}
    \begin{subfigure}[b]{1.0\textwidth}
    \centering
        \includegraphics[width=.18\linewidth]{figure/bench/106.jpg}
        \includegraphics[width=.18\linewidth]{figure/bench/107.jpg}
        \includegraphics[width=.18\linewidth]{figure/bench/108.jpg}
        \includegraphics[width=.18\linewidth]{figure/bench/109.jpg}
        \includegraphics[width=.18\linewidth]{figure/bench/110.jpg}
    \end{subfigure}
     \vspace{-20pt}
    \caption{
        \textbf{Illumination variation samples of the \textit{bench} scene in NeRF Extreme. }}
    \label{fig:benchall} \vspace{-10pt}
\end{figure*}


%#################################################################################

%#################################################################################



\clearpage

\clearpage
%%%%%%%%% REFERENCES
%%%%%%%%% REFERENCES
{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}


\end{document}