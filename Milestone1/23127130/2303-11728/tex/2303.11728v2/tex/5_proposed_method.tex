

\section{Methodology}
\label{sec:methods}
%\subsection{Problem Formulation}
\subsection{Overview}
The objective of this work is to build an illumination-robust few-shot view synthesis framework by regularizing intrinsic components that should be identical across multi-view images regardless of illumination.
However, there exist three major challenges in comparing the albedo extracted from the novel synthesized view and those of input images: 1) Since our proposed method requires pixel-to-pixel correspondence between different views, a geometric alignment is needed to select the pixel to compare. 2) There always exists a non-intersecting or occluded region between the novel view and the input view images, which should be considered during cross-view regularization. 3) The key feature for successful intrinsic decomposition is the global context of a scene. However, the NeRF-based structure has difficulties in rendering full-resolution images since continuous ray sampling upon the entire image requires massive computational and memory costs.

To address these problems, we suggest a few-shot view synthesis framework that utilizes an offline intrinsic decomposition network, providing global context-aware pseudo-albedo ground truth without the computational overhead. As illustrated in Fig.~\ref{fig:overallframework}, FIDNet~\cite{li2018iidww} provides pseudo-albedo ground truths for the input images before the start of the training. Then PIDNet learns to extract intrinsic components of the synthesized patch, given the pseudo-albedo ground truth of the corresponding patch depicting the same 3D surface. As a result, our NeRF learns illumination-robust few-shot view synthesis. 

In the following subsections, we introduce each component of our framework in detail.  





\subsection{Intrinsic Consistency Regularization}
\paragraph{Geometric alignment.}
As illustrated in Fig.~\ref{fig:overallframework}, a pair of images is selected from a set of inputs and randomly generated novel views, for every iteration.
In order to get a pixel correspondence between the two, we use a projective transformation, similar to several concurrent works~\cite{MVCGAN, xu2022sinnerf}. Given a pixel $x$ in the novel view, we need the corresponding image pixel $x'$ in the input view depicting the same 3D point. If the depth of a given image pixel is known, $x'$ can be obtained using projective transformation as follows:
\begin{equation}
     x' = (KT'^{-1}T)d(x)K^{-1}x ,
\label{eq:homography}
\end{equation}
where $K$ indicates camera intrinsics and $T$ and $T'$ indicate camera-to-world matrices of the novel view and input view. Both $K$ and $T$ are given for the calibrated images. \vspace{-10pt}


\paragraph{Albedo consistency.}
Based on the pixel correspondence obtained above, we can impose image consistency between inputs and novel views.
However, under varying illumination, Eq.~\ref{eq:colorreg} cannot regularize view-dependent color as it does under constrained illumination, for its different interactions within illumination (see Fig.~\ref{fig:intrinsics}).
To overcome this, we present $L2$ normalized albedo consistency loss $\mathcal{L}_{\mathrm{ac}}$ formulated as follows:
\begin{equation}
  \mathcal{L}_{\mathrm{ac}} = \sum_{x \in \mathcal{P}} \omega_{\mathrm{occ}}(x)\|\hat{a}(x) - \hat{a}(x')\|^2_2,
  \label{eq:ac}
\end{equation}
where $\hat{a}(x)$ and $\hat{a}(x')$ indicate the extracted albedo at $x$ and $x'$ from the novel view and input view, respectively.
$\mathit{w}_{\mathrm{occ}}(x)$ indicates the weight term to consider inaccurate correspondences coming from occlusions or out-of-region pixels, while $\mathcal{P}$ denotes all the pixels in the novel view. Details of $\mathit{w}_{\mathrm{occ}}(x)$ are in the following.
\vspace{-10pt}

\paragraph{Occlusion handling.}
Eq.~\ref{eq:homography} described above byproducts $\Tilde{d}(x')$, a depth value at pixel $x'$ in the input view. $\hat{d}(x')$, a synthesized depth value at $x'$ should be identical to $\Tilde{d}(x')$ if there exists neither self-occlusion nor ill-synthesized floating artifacts. 
For all cases, a projection error on $x'$, denote by $\mathcal{E}_\mathrm{proj}$ can by defined as
\begin{equation}    
\begin{array}{cc}
     \mathcal{E}_\mathrm{proj} = (\hat{d}(x') - \Tilde{d}(x'))^2.
\end{array}
  \label{eq:error}
\end{equation}
However, a problem exists in that both inaccurate correspondence and occlusion cause large projection errors.
In order to minimize $\mathcal{E}_\mathrm{proj}$ that came from inaccurate correspondences while protecting the pixel pairs with occlusion, we define an occlusion-aware weight term $\mathit{w}_{\mathrm{occ}}$, on a pixel $x'$ of input view, as
\begin{equation}    
\begin{array}{cc}
     \omega_{\mathrm{occ}} = \mathit{r}_{\mathrm{e}}(1 - (\mathcal{E}_\mathrm{proj}/\mathcal{M}_{\mathrm{proj}})), % 수식 정리 필요
\end{array}
  \label{eq:weight}
\end{equation}
where $\mathit{r}_{\mathrm{e}}$ and $\mathcal{M}_{\mathrm{proj}}$ indicates the error rate coefficient and the maximum value of $\mathcal{E}_\mathrm{proj}(x)$, respectively. By using $\mathit{w}_{\mathrm{occ}}$, the input-novel view pairs with occlusion that are likely to have large projection errors will not be enforced to have albedo consistency.  $\mathit{r}_{\mathrm{e}}$ decays from $1$ to the end criteria ($0.5$), reducing the number of pairs that are enforced to have the same albedo, since inaccurate correspondence pairs will decrease while training. 
\vspace{-10pt}

\begin{figure}
  \centering
  % \begin{tabular}{>{\centering\arraybackslash}p{.29\linewidth}>{\centering\arraybackslash}p{.27\linewidth}>{\centering\arraybackslash}p{.27 \linewidth}}
  %   \scriptsize Input & \scriptsize Albedo & \scriptsize Shading \\
  %  \end{tabular}
  \includegraphics[width=\linewidth]{figure/ICCV/intrinsics2.pdf}
  \vspace{-15pt}
\caption{ \textbf{Examples of intrinsic decomposition on NeRF Extreme.}
   %Despite the imperfect decomposition results, 
   Estimated albedo maps can provide appearances that are more illumination-invariant than input color images, as shown in a lower difference across multi-views.}
\label{fig:intrinsics} \vspace{-10pt}
\end{figure}

\paragraph{Depth consistency.}
Geometric alignment in our setup may utilize incorrect synthesized depth values that are commonly observed in novel view synthesis. 
In order to prevent the model to enforce consistency between pixels with inaccurate correspondence, and to efficiently correct ill-synthesized scene geometry, we present a depth consistency loss $\mathcal{L}_{\mathrm{dc}}$.
A direct minimization of $\mathcal{E}_\mathrm{proj}$, however, can be counterproductive due to occlusion, by smoothing two unrelated surface depths.
Through experimentation, we have discovered that total variation normalization on projection error can better regularize the scene geometry, successfully reducing floating artifacts without suffering adverse effects from occlusion.
Depth consistency loss $\mathcal{L}_{\mathrm{dc}}$ in the input view can be defined such that 
\begin{equation}
     \mathcal{L}_{\mathrm{dc}} = \sum_{x' \in \mathcal{P}'} \sum_{y \in \mathcal{N}(x')}(\mathcal{E}_\mathrm{proj}(y)-\mathcal{E}_\mathrm{proj}(x'))^2 ,
\label{eq:dc}
\end{equation}
where $y$ indicates one of the 4-neighbor adjacent pixels $\mathcal{N}(x')$ for $x'$. $\mathcal{P}'$ denotes all the pixels in the input view. Details of the projection error cases and depth regularization can be found in the supplementary materials.



\subsection{Albedo Estimation}
As discussed previously, a successful intrinsic decomposition inevitably requires the global context of a scene, while NeRF struggles with handling large-resolution inputs.
Moreover, intrinsic rendering methods~\cite{nerfinthewild, boss2021nerd, NeuralPIL, boss2022-samurai, ye2022intrinsicnerf} cannot be used in our case due to the lack of input data (uses 0.06 times fewer data).
To address these challenges without imposing a computational burden or requiring supervision, we propose a two-stage intrinsic decomposition pipeline: a full-image and patch-wise intrinsic decomposition network called FIDNet and PIDNet. Before training begins, FIDNet extracts the intrinsic components of the input images - pseudo-albedo ground truths - offline, in order to provide guidance to PIDNet with global contexts. During training, PIDNet extracts patch-wise intrinsic components ($\hat{a}(x)$) of the synthesized color patch at the novel view ($\hat{c}(x)$). Given the pseudo-albedo ground truth provided by FIDNet, PIDNet is trained to minimize $\mathcal{L}_{\mathrm{ac}}$ (Eq.~\ref{eq:ac}).
Specifically, FIDNet~\cite{li2018iidww} uses a shared encoder and $2$ decoders, one for log-scale albedo and the other for shading images. The network also predicts a 3-dimensional light color $c$ as a side output. PIDNet follows the architecture with a shallower structure. 
%Note that the official implementation and the pre-trained model of IIDWW~\cite{li2018iidww} have been used without fine-tuning for FIDNet. 


\subsection{Total Loss Functions} 
In addition to albedo consistency loss and depth consistency loss, an edge-preserving loss $\mathcal{L}_{\mathrm{edge}}$~\cite{godard2017unsupervised}, an intrinsic smoothness loss $\mathcal{L}_{\mathrm{pid}}$~\cite{li2018iidww}, and chromaticity consistency loss $\mathcal{L}_{\mathrm{chrom}}$~\cite{ye2022intrinsicnerf} are also used with color consistency loss $\mathcal{L}_{\mathrm{color}}$ and depth smoothness loss $\mathcal{L}_{\mathrm{ds}}$. An edge-preserving loss $\mathcal{L}_{\mathrm{edge}}$ gives the constraint that gradients of the novel synthesized view (i.e. edge) should be identical to the one of the input view. A patch-wise intrinsic smoothness loss $\mathcal{L}_{\mathrm{pid}}$ is formulated in the same way as depth consistency loss. A patch-wise chromaticity consistency loss $\mathcal{L}_{\mathrm{chrom}}$ gives the constraint that the chromaticity of the input patch and the extracted albedo is the same.

% chromaticity loss 내용 추가 / 