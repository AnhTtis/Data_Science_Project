% 7_conclusion.tex
\section{Conclusion}
In this paper, we propose ExtremeNeRF, which can synthesize a novel view image in more practical environments, where neither a massive amount of multi-view images nor constrained illumination is available. By regularizing intrinsic components which should be identical across different views, our method can directly regularize appearance instead of interpolating view-dependent color as vanilla-NeRF did. Supported by geometry alignment and depth consistency, our pipeline enables intrinsic decomposition while considering global contexts with marginal computational overhead. We have proved with our multi-view, varying-illumination dataset that the proposed method outperforms other previous works in a few-shot view synthesis under an unconstrained illumination environment, with extended applicability on non-object-centric scenes. 
\vspace{-10pt}
\paragraph{Limitations.}
More practical view synthesis under unconstrained environments requires the consideration of scenes with unknown camera poses and transient components. It should be addressed in the future, as it has rarely been discussed in few-shot, varying illumination settings.
