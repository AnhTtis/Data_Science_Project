% 3_preliminary.tex



\section{Preliminaries} 
\label{sec:pre}
\paragraph{Neural radiance fields (NeRF).} 
NeRF~\cite{mildenhall2020nerf} is a volume rendering-based view- synthesis framework that maps 5D inputs (3D coordinate and viewing direction of a ray) to color and volume density, denoted by $c$ and $\sigma$, respectively.
Specifically, with a ray $r_x(t) = o + td_x$, where $o$, $d_x$, and $t$ indicate camera origin, ray direction, and scene bound at pixel location $x$, respectively, a view-dependent color $\hat{c}(x)$ can be rendered such that
\begin{equation}
  \hat{c}(x) =\int_{t_n}^{t_f}T(t)\sigma(t)c(t)dt ,
  \label{eq:nerf}
\end{equation}
while $T(t) = \exp(-\int_{t_n}^{t}\sigma(s)ds)$ and $\sigma(\cdot),c(\cdot)$ are density and color predictions from the network, respectively. Similarly, a depth value $\hat{d}(x)$ at $x$ can also be rendered as
\begin{equation}
  \hat{d}(x) =\int_{t_n}^{t_f}T(t)\sigma(t)tdt .
  \label{eq:nerfdepth}
\end{equation}

In vanila-NeRF~\cite{mildenhall2020nerf}, view synthesis is done by optimizing mean squared error on synthesized color as
\begin{equation}
  \mathcal{L}_{\mathrm{color}} = \sum_{x \in \mathcal{S}} \| \hat{c}(x) - c_\mathrm{gt}(x)\|^2_2 ,
  \label{eq:colorreg} 
\end{equation}
where $\mathcal{S}$ indicates the set of sampled pixels, and $c_\mathrm{gt}(x)$ indicates ground-truth color at $x$.
\vspace{-10pt}


\paragraph{RegNeRF.} 
RegNeRF~\cite{niemeyer2022regnerf} uses a depth smoothness regularization of a novel view synthesized patch for few-shot view synthesis, defined as follows:
\begin{equation}
    \begin{split}
     \mathcal{L}_{\mathrm{ds}} = \sum_{x \in \mathcal{S}} \sum_{l \in \mathcal{N}(x)} \!\!\!{(\hat{d}(l)-\hat{d}(x))^2},
    \end{split}
    \label{eq:georeg}
\end{equation}
where $\hat{d}(x)$ indicates synthesized depth value at $x$, and $l$ indicates one of the 4-neighbor adjacent pixels $\mathcal{N}(x)$.

This regularization has succeeded in enhancing the ruined geometry of the synthesized scene.
However, RegNeRF~\cite{niemeyer2022regnerf} also relies on the assumption that input images should share consistent illumination conditions as ~\cite{mildenhall2020nerf}.

\begin{figure*}[t]
  \centering
  \includegraphics[width=\linewidth]{figure/ICCV/framework_final4.pdf}
  \vspace{-15pt}
   \caption{\textbf{Overall architecture of our ExtremeNeRF.} PIDNet extracts intrinsic components from the synthesized patch $\hat{c}(x)$ while optimizing albedo consistency $\mathcal{L}_{\mathrm{ac}}$, which enforces extracted albedo to be identical with the pseudo-albedo ground truth. An occlusion-aware weight term $\omega_{\mathrm{occ}(x)}$ and depth consistency loss $\mathcal{L}_{\mathrm{dc}}$ encourage proper correspondence matching between two views.  A bold, crimson arrow indicates the inference phase while green and black arrows indicate the offline and online training phases, respectively.}
   \label{fig:overallframework} \vspace{-10pt}
\end{figure*}