% 2_related_works.tex

\section{Related Work}
\label{sec:reference}
\paragraph{Few-shot NeRF.}
After Yu et al.~\cite{yu2021pixelnerf} has proved that leveraging knowledge priors leads to better few-shot view synthesis, the following works~\cite{wang2021ibrnet, dietnerf, CLIPnerf, infonerf, watson2022diffusion, deng2022nerdi} have suggested a variety of priors to improve the performance. Especially, Deng et al.~\cite{dsnerf} and Xu et al.~\cite{xu2022sinnerf} have presented depth prior based methods, as~\cite{roessle2022dense, dey2022mip, geonerf, attal2021torf, rgbdnerf}. Some of the other methods~\cite{trevithick2021grf, geonerf, srf, rematas2021sharf, chen2021mvsnerf, watson2022diffusion} utilize implicit geometry priors for the task. Similar to several previous works~\cite{xu2022sinnerf, MVCGAN, darmon2022improving, truong2022sparf}, our method is based on the projective transformation between different views, but we focus more on view synthesis under varying illumination, not covered by the aforementioned methods.
\vspace{-10pt}


\paragraph{NeRF under varying illumination.}
Unlike vanilla NeRF which requires images from strictly-controlled environments, some papers have attempted to handle images taken under challenging real-world environments, e.g., varying illumination. 
Some works~\cite{zhang2021nerfactor, boss2021nerd, srinivasan2021nerv, NeuralPIL, ye2022intrinsicnerf, boss2022-samurai, kuang2022neroic} have tried to factorize the scene components based on a NeRF-like framework, but exhaustive computational costs are required. NeRF-W~\cite{nerfinthewild} has succeeded in enabling view synthesis of internet photos taken under varying illumination using photometric embeddings. However, the target illumination is limited to the sun.
More recently, Tancik et al.~\cite{BlcokNeRF} presented a way to synthesize a large-scale city scene captured under varying lighting conditions. 
Boss et al.~\cite{boss2022-samurai, boss2021nerd, NeuralPIL} and Kuang et al.~\cite{kuang2022neroic}  suggest a method to decompose object-centric images for inverse-rendering, allowing for novel view synthesis and relighting. NeROIC~\cite{kuang2022neroic}, in particular, is applicable in few-shot environments but the application is still limited to object-centric images.

\vspace{-10pt}
\paragraph{Multi-view and/or multi-illumination datasets.}
Understanding the physical characteristics of a real-world scene requires multiple images with varying conditions. However, most of the existing datasets have their focus on varying single attributes like viewing direction or illumination instead of both. The well-known view synthesis benchmarks~\cite{LLFF, tremblay2022rtmv} have a rich variation of viewing directions yet lack lighting variation. On the other hand, the datasets that are collected for the purpose of lighting-estimation~\cite{MIW, elhelou2020vidit} or intrinsic decomposition~\cite{li2018iidww} have enough lighting variations but are not collected in a multi-view setup. Some works~\cite{DTU, nerfinthewild, boss2021nerd, boss2022-samurai} have built datasets with variation in both attributes, but still have limitations in that their application is limited to outdoor scene collections~\cite{phototourism} or object-centric scenes~\cite{DTU, boss2021nerd,boss2022-samurai}~(see Tab.~\ref{tab:comparison}). 

\begin{table}
    \resizebox{\linewidth}{!}{\input{table/table_realillum_comparison}}
    \vspace{-5pt}
    \caption{
    \textbf{Multi-view dataset comparison.}
    Our NeRF Extreme provides in-the-wild, non-object-centric, and varying illumination images taken from indoor and outdoor scenes.}
    \label{tab:comparison} \vspace{-10pt}
\end{table}


\vspace{-10pt}
\paragraph{Intrinsic image decomposition.}
As Weiss et al.~\cite{weiss2001deriving} have proved, multiple images with varying illumination can provide rich guidance to intrinsic decomposition. 
Li et al.~\cite{li2018iidww} has succeeded in disentangling intrinsic components without supervision, by enforcing multiple images with varying illumination to have identical albedo. Das et al.~\cite{das2022pie} has shown state-of-the-art performance by leveraging cross-color-ratio, the illumination-invariant image gradients in an intrinsic decomposition. We use \cite{li2018iidww} to extract the pseudo-albedo ground truth of the inputs before the start of the training phase.
Recently, Ye et al.~\cite{ye2022intrinsicnerf} have suggested a NeRF framework that enables intrinsic decomposition for a purpose of scene editing. However, did not address the problem of few-shot view synthesis under varying illumination. 