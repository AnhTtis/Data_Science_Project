% 6_experiments.tex



\section{Experiments}
In this section, we introduce our newly built dataset, NeRF Extreme, the first in-the-wild multi-view dataset with varying illumination, whose scenes are not limited to object-centric ones. After that, the experimental settings and results will follow. Details of the dataset statistics and experimental settings are in the supplementary material.

\subsection{NeRF Extreme Dataset}
To build a view synthesis benchmark that fully reflects unconstrained environments such as mobile phone images captured under casual conditions, we collected multi-view images with a variety of light sources such as multiple light bulbs and the sun.
For indoor scenes, we varied the illumination by turn-on/off light sources, and closing/opening curtains. For outdoor scenes, scenes are captured at different times with different sunlights~(see Fig.~\ref{fig:lightingvariation}). All the images are taken in the wild using the off-the-shelves camera on the mobile phone.
Similar to LLFF~\cite{LLFF}, all the camera poses are obtained using the COLMAP~\cite{colmap} structure-from-motion framework. Depth maps are also included in the dataset, which are obtained by the recent multi-view stereo method~\cite{li2022compnvs}.

%All the images, corresponding camera poses, and depth map that is obtained using a recent multi-view stereo method~\cite{li2022compnvs} will be publicly available.

Our dataset consists of $10$ scenes, $6$ of which are indoor and $4$ are outdoor. We took $40$ images per scene with a resolution of $3,000\times4,000$, with 30 images in the train set and 10 images in the test set. The training images of each scene are captured with at least 3 different lighting conditions.
To make the dataset more widely useful, we captured the test scenes under mild lighting conditions, rather than extremely low or high contrasts or intensities, similar to images in typical NeRF benchmark datasets~\cite{LLFF, DTU}. 
\begin{figure}[t]
    \centering
    \begin{tabular}{>{\centering\arraybackslash}p{.7\linewidth}>{\centering\arraybackslash}p{.19\linewidth}}
    \scriptsize Sparse inputs with varying illuminations  & \scriptsize Targets \\
    \end{tabular}
     \begin{subfigure}[b]{1.0\linewidth}
         \centering
        \includegraphics[width=\linewidth]{figure/ICCV/discussion.pdf}
     \end{subfigure}
    \vspace{-15pt}
    \caption{
        \textbf{Ill-posed color synthesis examples.} Given inputs with varying illuminations, surface characteristics and illuminations that are not observed in inputs ({\color{red}red boxes}) are impossible to be inferred by any few-shot NeRF methods.} 
    \label{fig:discussions} \vspace{-10pt}
\end{figure}

\begin{figure*}[t]
    \centering
    \begin{tabular}{>{\centering\arraybackslash}p{0.3\textwidth}>{\centering\arraybackslash}p{0.3\textwidth}>{\centering\arraybackslash}p{0.3\textwidth}}
      \scriptsize mip-NeRF~\cite{barron2021mip} & \scriptsize  RegNeRF~\cite{niemeyer2022regnerf} & \scriptsize  \textbf{ExtremeNeRF~(Ours)}\\
    \end{tabular}
     \begin{subfigure}[b]{1.0\textwidth} 
         \centering
        \includegraphics[width=\linewidth]{figure/ICCV/result.pdf}
     \end{subfigure}
    \vspace{-15pt}
    \caption{
    \textbf{Qualitative comparison on NeRF Extreme.}
    A synthesized novel view and corresponding depth map are generated by the baselines and our proposed method with $3$ view input images. Our proposed method shows plausible synthesis results compared to the other baseline methods (Best viewed in color). 
    }
    \label{fig:result_extreme} \vspace{-10pt}
\end{figure*}

\begin{figure*}[t]
    \centering
    \begin{tabular}{>{\centering\arraybackslash}p{0.3\textwidth}>{\centering\arraybackslash}p{0.3\textwidth}>{\centering\arraybackslash}p{0.3\textwidth}}
      \scriptsize mip-NeRF~\cite{barron2021mip} & \scriptsize  RegNeRF~\cite{niemeyer2022regnerf} & \scriptsize  \textbf{ExtremeNeRF~(Ours)}\\
    \end{tabular}
     \begin{subfigure}[b]{1.0\textwidth} 
         \centering
        \includegraphics[width=\linewidth]{figure/ICCV/dtu_result_full.pdf}
     \end{subfigure}
    \vspace{-15pt}
    \caption{
    \textbf{Qualitative comparison on light-varying DTU~\cite{DTU}.} Given light-varying 3 view input images, our proposed method shows reliable synthesis qualities both for the color and the depth maps, while RegNeRF~\cite{niemeyer2022regnerf} shows floating artifacts and ill-synthesized depths (Best viewed in color).
    }
    \label{fig:result_dtu} \vspace{-10pt}
\end{figure*}

\begin{figure}[t]
    \centering
    \begin{tabular}{>{\centering\arraybackslash}p{.45\linewidth}>{\centering\arraybackslash}p{.45 \linewidth}}
    \scriptsize RegNeRF~\cite{niemeyer2022regnerf} & \scriptsize \textbf{ExtremeNeRF (Ours)} \\
    \end{tabular}
     \begin{subfigure}[b]{1.0\linewidth}
         \centering
        \includegraphics[width=\linewidth]{figure/ICCV/depth_comparison.pdf}
     \end{subfigure}
    \vspace{-15pt}
    \caption{
        \textbf{Examples of rendered depths with corresponding rendered colors.} Even though a synthesized result of the previous work~\cite{niemeyer2022regnerf} often shows the competitive results as us in $6$ view settings, corresponding depth information is highly distorted compared to ours.}
    \label{fig:depthmap} \vspace{-10pt}
\end{figure}




\subsection{Experimental Settings}
\paragraph{Implementation details.}
Our framework is based on JAX~\cite{jax2018github} implementation of RegNeRF~\cite{niemeyer2022regnerf}. For FIDNet and albedo consistency measure, official code and publicly available model of IIDWW~\cite{li2018iidww} trained with BigTimes dataset~\cite{li2018iidww} is used without fine-tuning.
\vspace{-10pt}

\paragraph{Datasets.}
In addition to our proposed dataset, NeRF Extreme, the experimental results on DTU~\cite{DTU} are provided. For DTU, we adhere to the evaluation protocols that are used by the other previous works~\cite{niemeyer2022regnerf, dietnerf, yu2021pixelnerf} except for the lighting variations.
For NeRD~\cite{boss2021nerd} dataset, only weak comparisons are available for its inadequacy in few-shot view synthesis. More explanations and experimental results are in the supplementary material.
Similarly, experiments on SAMURAI are not available for its incapability of camera poses, as mentioned in their paper~\cite{boss2022-samurai}. In Sec.~\ref{sec:ablation}, we provide experimental results on LLFF~\cite{LLFF} dataset, in addition.

\vspace{-10pt}

\begin{table*}[t]
\centering
\resizebox{0.9\linewidth}{!}
{\input{table/table_iccv_extreme}}
    \vspace{-5pt}
    \caption{
    \textbf{Quantitative comparison on NeRF Extreme.}
    We compare our quantitative result with the few-shot view synthesis baselines that can be operated in the same experimental setting as ours. For every case, our model shows the best synthesizing performance for each metric.
    }
    \label{tab:realIllum} \vspace{-10pt}
\end{table*}


\begin{table}
    \resizebox{\linewidth}{!}{\input{table/table_iccv_dtu}}
    \vspace{-5pt}
    \caption{
    \textbf{Quantitative comparison on light-varying DTU~\cite{DTU} with 3 views.}
    %For DTU, our model shows relatively degraded performances for its texture-less backgrounds. However, 
    Our method has strength in maintaining the underlying color characteristics of the target scene, as shown in lower CCRD.}
    \label{tab:dtu} \vspace{-10pt}
\end{table}

\paragraph{Comparison algorithms.}
Since none of the previous works are dealing with the suggested problem, both few-shot view synthesis methods and NeRF-based inverse rendering methods, capable of view synthesis under varying illumination are weak baselines for us. 
For few-shot view synthesis, mip-NeRF~\cite{barron2021mip} and RegNeRF~\cite{niemeyer2022regnerf}, which are baseline and the state-of-the-art few-shot NeRF, respectively, are compared. 
In the case of view synthesis under varying illumination, however, comparisons with most of the baselines are unavailable. For NeRF-W~\cite{nerfinthewild}, neither the implementation nor the pre-processed data are publicly available. Plus, it can only handle outdoor scenes. 
NeROIC~\cite{kuang2022neroic} and the other works~\cite{boss2021nerd, NeuralPIL, boss2022-samurai} cannot deal with in-the-wild scenes like our NeRF Extreme, as their inputs are limited to object-centric scenes paired with foreground masks. This limitation only allows for weak comparisons.
In the supplementary material, we provide an additional comparison with the baselines.

\subsection{Evaluation Metrics}
The problem of evaluating a novel view synthesis under varying illumination has rarely been discussed since inferring consistent colors and synthesizing a view with unseen illumination is an ill-posed problem. 
Fig.~\ref{fig:discussions} illustrates the situations where scene appearances are impossible to be synthesized using the information provided by the inputs.
Although our method succeeds in synthesizing the plausible novel view, the physical characteristics like surface reflectance~(1st row) or unknown illumination~(2nd row) that are not observed in input images cannot be synthesized.
Similar works~\cite{nerfinthewild, boss2022-samurai} have dealt with the problem by evaluating typical metrics like PSNR and SSIM on the synthesized image after relighting to fit the target illumination.
In this paper, we suggest evaluation methods in addition to PSNR and SSIM, that can compare the underlying characteristics of the scene regardless of illumination.
\vspace{-10pt}

\paragraph{CCRD.}
Geverse et al.~\cite{gevers1999color} has presented the cross-color-ratio (CCR), an illumination-invariant image gradient that only depends on albedo transitions. Using the log-scale CCR image proposed by \cite{das2022pie}, we use the cross-color-ratio difference (CCRD), a metric that can measure the maintenance of underlying consistent color from the synthesized image to the inputs, as $L1$ difference between CCR images of the ground truth and the synthesized view.
\vspace{-10pt} % supple 에서 자세히 서술

\paragraph{Absolute relative error.} 
In order to compare the quality of the synthesized depth, we utilize Absolute Relative Error(Abs Rel), which is commonly used in the depth estimation task. For DTU~\cite{DTU}, provided ground-truth depth maps are used for the measurement. In the case of our NeRF Extreme, a depth map estimated by \cite{giang2022curvatureguided} is used as a pseudo-ground-truth. The estimated depth maps were publicly available within our dataset. 

% \begin{table} [t]
%     \centering
%     \resizebox{\linewidth}{!}
%     {\input{table/table_ablation_mask}}
%     \vspace{-.2cm}
%     \caption{
%     \textbf{Ablation study on occlusion mask. }
%     A quantitative results comparison between our proposed with/without occlusion-aware weight term ($\omega_{\mathrm{occ}}$). 
%     }
%     \label{tab:ablationmask} \vspace{-10pt}
% \end{table}

\begin{table}
    \centering
    \resizebox{\linewidth}{!}
    {\input{table/table_iccv_ablation}}
    \vspace{-.2cm}
    \caption{
    \textbf{Ablation study of our ExtremeNeRF.}
   %Large improvement in  CCRD and Abs Rel proves the fact that our loss terms effectively regularize the underlying physical characteristics of the scene.
    }
    \label{tab:ablation} \vspace{-10pt}
\end{table}

\begin{table}
    \centering
    \resizebox{\linewidth}{!}
    {\input{table/table_llff}}
    \vspace{-.2cm}
    \caption{
    \textbf{Ablation study for depth consistency term on LLFF~\cite{LLFF}.}
   %Improved CCRD and Abs Rel show that our depth consistency term is also helpful when images were taken under a constrained illumination. 
    }
    \label{tab:llff} \vspace{-10pt}
\end{table}

\subsection{Experimental Results}
\paragraph{Qualitative comparison.}
Fig.~\ref{fig:result_extreme} and Fig.~\ref{fig:result_dtu} show the qualitative comparison results between our ExtremeNeRF and other few-shot view synthesis baselines on NeRF Extreme and light-varying DTU~\cite{DTU} with $3$ input view setting. Our method outperforms other baseline methods~\cite{barron2021mip, niemeyer2022regnerf} in synthesizing geometry and eliminating distortions that come from varying illumination inputs.
Especially, for depth maps, our method shows a large improvement compared to the baselines, even when the baseline succeeds in synthesizing a color image with competitive quality~(Fig.~\ref{fig:depthmap}), enabling further applications such as video rendering.
\vspace{-10pt}



\paragraph{Quantitative comparison.}
Tab.~\ref{tab:realIllum} and \ref{tab:dtu} shows a quantitative comparison between the methods on $3$ view settings.
For all cases within NeRF Extreme, our method outperforms others. Our method shows the lowest CCRD and Abs Rel with a large difference indicating that our ExtremeNeRF can maintain the underlying physical properties of the target scene even with sparse inputs with varying illumination.
In the case of light-varying DTU~\cite{DTU}, performance degradation can be found, compared to the results of NeRF Extreme. 
%Note that an object-centric image with a texture-less background like DTU is unfavorable for our framework, as in the multi-view reconstruction task.
%Hence, the scenes that are captured under the experimental environments with discarded natural contexts like scenes in DTU, are difficult to handle.
However, despite the absence of image global contexts, our method shows the lowest CCRD, proving robustness against illumination changes.
More qualitative and quantitative results are in the supplementary material. 


\subsection{Ablation Studies}
\label{sec:ablation}
In this section, ablation studies on various components of our method are provided. We found that cross-view consistency regularization neglecting illumination has an adverse effect on the underlying color consistency and depth map. Further, the depth map shows improvement by not enforcing albedo consistency for cases suspected of occlusion.
\vspace{-10pt}
\paragraph{Depth consistency regularization.}
The 2nd rows of Tab.~\ref{tab:ablation} shows ablation on depth consistency term $\mathcal{L}_{\mathrm{dc}}$. The results prove that our depth consistency term is not only beneficial for the depth map but also for the underlying color consistency. This is due to a decrease in the number of ill-synthesized pixels. 
Additionally, we ablate our depth consistency term on the view synthesis benchmark with constrained illumination~\cite{LLFF} (Tab.~\ref{tab:llff}).
A lower CCRD proves that our depth consistency term helps to maintain underlying color consistency even when the unconstrained illumination assumption is absent. 
\vspace{-10pt}
\paragraph{Albedo consistency regularization.}
The ablation on $\mathcal{L}_{\mathrm{ac}}$ (the 3rd rows of Tab.~\ref{tab:ablation}) was performed by enforcing view-dependent color consistency across views instead of albedo, similar to Eq.~\ref{eq:colorreg}. Interestingly, the experimental results show the 2nd lowest CCRD, since the synthesized color neglects illumination and viewing direction - i.e. similar to albedo. However, the marginal decrease in Abs Rel proves that it fails to regularize depth compared to ours.
\vspace{-10pt}
\paragraph{Occlusion handling.}
Occlusion handling in our method is in the role of preventing consistency regularization between the pixels with occlusions.
The last $4$ rows in Tab.~\ref{tab:ablation} show ablation studies on occlusion handling. 
The ablation on $\omega_{\mathrm{occ}}$ proves that consistency regularization across views without $\omega_{\mathrm{occ}}$ can only bring minor changes in performance, especially for depth.
In the case of the error rate coefficient $r_e$, the Abs Rel of the synthesized depth has increased, if $r_e$ is kept at 1.0 without decreasing it. 
This is due to the enforced consistency between the pixel pairs with occlusions. 
Note that the best error rate coefficient $r_e$ may vary depending on the target scene geometry. In our experiments, decaying $r_e$ from $1$ to $0.5$ shows the best results on our NeRF Extreme dataset, which is likely to have many occlusions.
 
