\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{iccv}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{multicol}
\usepackage{graphicx}
\usepackage{amssymb}% http://ctan.org/pkg/amssymb
\usepackage{pifont}% http://ctan.org/pkg/pifont
\usepackage{booktabs}
\usepackage{mwe}
\usepackage{array}
\usepackage[dvipsnames]{xcolor}
% \usepackage{tikz}
\usepackage{courier}
\usepackage{float}
\usepackage{dblfloatfix}

\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%
% \newcommand{\dummyfigure}{\tikz \fill [NavyBlue] (0,0) rectangle node [black] {Figure} (2,2);}
\newcolumntype{M}[1]{>{\centering\arraybackslash}m{#1}}

% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}

\iccvfinalcopy % *** Uncomment this line for the final submission

\def\iccvPaperID{1408} % *** Enter the ICCV Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
\ificcvfinal\pagestyle{empty}\fi

\begin{document}

%%%%%%%%% TITLE
\title{Better Aligning Text-to-Image Models with Human Preference}

% \author{First Author\\
% Institution1\\
% Institution1 address\\
% {\tt\small firstauthor@i1.org}
% % For a paper whose authors are all at the same institution,
% % omit the following lines up until the closing ``}''.
% % Additional authors and addresses can be added with ``\and'',
% % just like the second author.
% % To save space, use either the email address or home page, not both
% \and
% Second Author\\
% Institution2\\
% First line of institution2 address\\
% {\tt\small secondauthor@i2.org}
% }

\author{
    Xiaoshi Wu$^{1}$, 
    Keqiang Sun$^{1}$, 
    Feng Zhu$^{2}$, 
    Rui Zhao$^{2, 3}$, 
    Hongsheng Li$^{1, 4}$
\vspace{0.1em}\\
    $^{1}$Multimedia Laboratory, The Chinese University of Hong Kong \\
    $^{2}$SenseTime Research \quad $^{3}$Qing Yuan Research Institute, Shanghai Jiao Tong University \\
    $^{4}$Centre for Perceptual and Interactive Intelligence (CPII) \\ 
    \texttt{\small \{wuxiaoshi@link, kqsun@link, hsli@ee\}.cuhk.edu.hk},~\texttt{\small \{zhufeng, zhaorui\}@sensetime.com}
}

\maketitle
% Remove page # from the first page of camera-ready.
\ificcvfinal\thispagestyle{empty}\fi


%%%%%%%%% ABSTRACT
\begin{abstract}
Recent years have witnessed a rapid growth of deep generative models, with text-to-image models gaining significant attention from the public. 
However, existing models often generate images that do not align well with human aesthetic preferences, such as awkward combinations of limbs and facial expressions. 
To address this issue, we collect a dataset of human choices on generated images from the Stable Foundation Discord channel. 
Our experiments demonstrate that current evaluation metrics for generative models do not correlate well with human choices.
Thus, we train a human preference classifier with the collected dataset and derive a Human Preference Score (HPS) based on the classifier. 
Using the HPS, we propose a simple yet effective method to adapt Stable Diffusion to better align with human aesthetic preferences. 
Our experiments show that the HPS outperforms CLIP in predicting human choices and has good generalization capability towards images generated from other models. 
By tuning Stable Diffusion with the guidance of the HPS, the adapted model is able to generate images that are more preferred by human users.
The project page is available here: \href{https://tgxs002.github.io/align\_sd\_web/}{https://tgxs002.github.io/align\_sd\_web/}.

\end{abstract}

%%%%%%%%% BODY TEXT
\section{Introduction}


% \begin{table*}[]
% \begin{tabular}{lccccc}
% \hline
% & \multicolumn{1}{l}{Pair Free} & \multicolumn{1}{l}{Rate Specific Image} & \multicolumn{1}{l}{Image} & \multicolumn{1}{l}{Text} & \multicolumn{1}{l}{Human Preference} \\
% \hline
% FID                    & \cmark        & \xmark                  & \cmark            & \xmark                   & \xmark               \\
% PSNR/SSIM              & \xmark        & \cmark                  & \cmark            & \xmark                   & \xmark               \\
% LPIPS                  & \xmark        & \cmark                  & \cmark            & \xmark                   & \xmark               \\
% IS                     & \cmark        & \cmark                  & \cmark            & \xmark                   & \xmark               \\
% KID                    & \cmark        & \xmark                  & \cmark            & \xmark                   & \xmark               \\
% CLIP Score             & \cmark        & \cmark                  & \xmark            & \cmark                   & \xmark               \\
% HPS & \cmark        & \cmark                  & \cmark            & \cmark                   & \cmark              \\
% \hline
% \end{tabular}
% \end{table*}

% 1. lead the authors to the problem
% 2. state the problems of the current diffusion model
% 3. say how this can be improved

\begin{figure}
    {
    \centering
    \includegraphics[width=1.\linewidth]{images/show_problem.pdf} 
    \caption{Generated images often do not align well with the human aesthetic preference and intention. Input prompts are shown below images.}
    \label{fig:seed}
    }
    {
    \centering
    \includegraphics[width=1.0\linewidth]{images/teaser.pdf}
    \caption{We show that Stable Diffusion v1.4 can be adapted to better align with the human aesthetic preference and intention when guided by the proposed human preference classifier. The input prompt is shown below images.}
    }
\end{figure}


The recent progress in diffusion models~\cite{glide, dalle2, imagen, stable_diffusion} has enabled impressive advancements in text-to-image generation, with many models now being deployed in real-world applications such as DALL·E~\cite{dalle2} and Stable Diffusion~\cite{stable_diffusion}. 
However, public attention has also highlighted new issues, such as the awkward combinations of limbs and facial expressions of generated persons as shown in Fig.~\ref{fig:seed}. 
The users usually need to cherry-pick results to avoid these artifacts.
In other words, the generated images are misaligned with the human aesthetic preference.

% It is worth noting that, in most cases, users can avoid these artifacts by cherry-picking the best image from a batch with different random seeds. 
% Therefore, the model is not incapable of generating good images. Rather, it is currently unaware of these specific artifacts, 

To further improve the quality of generated images, it is essential to track the ability of a model to generate human preferable images. 
However, it is uncertain whether the existing evaluation metrics, such as Inception Score (IS)~\cite{inception_score} and Fréchet inception distance (FID)~\cite{fid}, are sensitive to the aesthetic aspect of a generated image. 
These metrics perceive an image through a classification-based CNN trained on ImageNet~\cite{imagenet}, which has been shown to be biased towards image texture rather than general image contents ~\cite{Geirhos2018ImageNettrainedCA}, and thus may not align well with human perception. 
Also, both IS and FID are single modal evaluation metrics, which do not take user intention into account.
Some recent studies~\cite{glide, dalle2, instructpix2pix} use the CLIP~\cite{clip} model as a proxy for human judgement to evaluate the alignment between generated images and text prompts. 
The CLIP~\cite{clip} model is trained on a rich dataset and is believed to capture subtle aspects of human intention better. 
However, it is uncertain whether CLIP~\cite{clip} can measure the quality of generated synthetic images, which may not adhere to the same constraints as real images, such as the example shown in Fig.~\ref{fig:seed}.

% Despite the potential weaknesses of existing metrics, evaluating human preferences itself may not be a well-posed problem. This is due to two main reasons: (1) human preferences are highly diverse, with different people likely to have different criteria~\cite{lpips}, and (2) human preferences do not conform to a partial order relation. (i.g. comparing two unrelated images for human preference may not be reasonable)
% As a result, the most common method for evaluating human preferences is to conduct user studies~\cite{optimizeprompt, splicingvit}, which require significant human labor and are difficult to reproduce.

In this study, we investigate the problem of human preference using a novel, large-scale dataset of human choices on images generated by Stable Diffusion~\cite{stable_diffusion} using the same prompt.
The dataset comprises 98,807 diverse images generated from user-provided prompts, along with 25,205 human choices. 
By evaluating on this dataset, we find that the Inception Score (IS)~\cite{inception_score}, the Fréchet Inception distance (FID)~\cite{fid} and the CLIP score do not fully match the human choice, which means that the human preference is a missing dimension of image quality that is not well tracked by existing mainstream metrics.


% (2) Despite the highly diverse nature of human preferences, there are some common aesthetic criteria that can be learned to identify images with artifacts.
% (3) We find that human preference score (HPS) can be learned to better measure the  and the alignment with human intention of images generated from text-to-image models. 

We further train a human preference classifier on this dataset by fine-tuning the CLIP~\cite{clip} model, and define human preference score (HPS) based on it. We validate HPS's alignment with human choices and its generalization capability towards other generative models through user studies. 
% Using HPS, we are able to investigate several factors that can impact the aesthetic quality of generated images, including the choice of certain prompts, diffusion steps and the effect of ``low-quality'' images during training. 
% Surprisingly, we find that letting the model see ``low-quality'' images during training can improve the aesthetic quality estimation during inference, despite the common belief that higher-quality training images result in better generated images. User studies confirmed the quality gain resulting from this approach. 
% We also use HPS as a benchmark to evaluate the alignment of a range of text-to-image models with human aesthetic preference.
HPS can be utilized to guide generative models towards producing human-preferred images. 
To this end, we devise a simple yet effective method to adapt Stable Diffusion~\cite{stable_diffusion} by LoRA~\cite{lora} with aware of human preference.
We conduct user studies to validate the effectiveness of our approach.
The results show that the adapted model can better capture human intentions, and generate more preferable images, which significantly mitigates the kind of artifact shown in Fig.~\ref{fig:seed}.
% Additionally, our approach results in an improvement in FID scores when evaluated on ImageNet.

Our contributions are as follows:
(1) We create a large-scale dataset for studying human preferences.
To our best knowledge, this dataset is the first of its kind that contains massive human choices on images generated with the same prompt.
(2) We find that the human choice cannot be accurately predicted by the existing mainstream evaluation metrics, while it can be better predicted via fine-tuning CLIP on the proposed dataset.
(3) We propose a simple yet effective method to guide the Stable Diffusion model towards generating images with better aesthetic quality and better aligning with human intention.

\begin{figure*}
    \centering
    \includegraphics[width=0.75\linewidth]{images/example.pdf}
\vspace{-0.2cm}
\caption{Examples of the collected data. The images are generated by Stable Diffusion with the prompts shown below each row of images. The preferred images are highlighted with red borders. More examples can be found in the appendix.}
\label{fig:example}
\end{figure*}

\section{Related Works}
\noindent\textbf{Text-to-image generative models.}
Text-to-image generative model has long been an active research area.
Mansimov \etal~\cite{Mansimov2015GeneratingIF} show that Deep Recurrent Attention Writer (DRAW)~\cite{Gregor2015DRAWAR} can be conditioned on captions to generate novel scene compositions.
Generative Adversarial Networks (GANs) improve image fidelity by training a discriminator to provide supervision for the generative model.
DALL·E~\cite{dalle} firstly achieves open-domain text-to-image synthesis with the help of massive image-text pairs.

Diffusion model formulates the generative process as the inverse of the diffusion process~\cite{SohlDickstein2015DeepUL}, which was improved by Song and Ermon ~\cite{Song2019GenerativeMB} and Ho \etal ~\cite{Ho2020DenoisingDP}.
Dhariwal \etal firstly show the superiority of diffusion model over GANs on image generation.
Several following works, including DALL·E 2~\cite{dalle2}, GLIDE~\cite{glide}, Imagen~\cite{imagen}, ERNIE-ViLG~\cite{Feng2022ERNIEViLG2I, Zhang2021ERNIEViLGUG}, Stable Diffusion~\cite{stable_diffusion}, bring the magic of text-to-image generation to the public attention.
Among these models, Stable Diffusion is an open-source model with active user community.

Several recent works improve Stable Diffusion on different aspects.
DreamBooth~\cite{Ruiz2022DreamBoothFT} and ELITE~\cite{Wei2023ELITEEV} explore customizing Stable Diffusion to a certain object.
Feng \etal ~\cite{Feng2022TrainingFreeSD} propose a training-free method to guide diffusion model for better compositional capabilities.
It has been discovered that prompt engineering plays an important role in generating high quality images.
Hao \etal ~\cite{Hao2022OptimizingPF} devise an automatic prompt engineering scheme via reinforcement learning.
Our method focus on the misalignment between the generated image and human preference, which is orthogonal to the above mentioned topics.

\noindent\textbf{Datasets of generated images.}
Thanks to active user communities of text-to-image models, several databases of images generated by diffusion models have been introduced.
Lexica (\href{https://lexica.art/}{lexica.art}) is a large database of images generated by Stable Diffusion and Lexica Aperture.
It also provides related information about the image, such as the prompt and guidance scale.
However, the database is closed-source, and only allows online browsing.
DiffusionDB~\cite{diffusiondb} is a large-scale open-source database collected from the Stable Foundation Discord channel, containing the text prompt and parameters for each image.
SAC~\cite{sac} is a dataset of images generated from Stable Diffusion and GLIDE~\cite{glide}, along with user ratings from a aesthetic survey.
However, SAC only contains limited user choices compared to our dataset.

% \noindent\textbf{Evaluation metrics for generative models.}

\noindent\textbf{Learning from human feedback.}
Human feedback has long been used in a wide range of deep learning tasks.
Christiano \etal ~\cite{firstrlhf} and Arakawa \etal ~\cite{Arakawa2018DQNTAMERHR} incorporate human feedback into RL training, which is proved to accelerate the model convergence.
Krishna \etal ~\cite{Krishna2022SociallySA} propose ``socially situated AI'', which significantly improves image recognition performance via interacting with human users on Instagram.
InstructGPT~\cite{Ouyang2022TrainingLM} fine-tunes GPT via a reward function trained on human feedback, establishing the foundation for the success of ChatGPT.
~\cite{Hao2022OptimizingPF} and ~\cite{Lee2023AligningTM} use similar methodology to improve text-to-image models, which are highly related to our work.
In ~\cite{Hao2022OptimizingPF}, this is achieved by augmenting the text prompt.
~\cite{Lee2023AligningTM} is a concurrent work that focuses more on the exact alignment between text and image, while our work shows that the potential of human feedback is far beyond the exact alignment when the feedback takes into account the aesthetic preference of human.


\begin{figure}
    \centering
    \includegraphics[width=.7\linewidth]{images/datacollect.pdf}
    \vspace{0.2cm}
    \caption{Interactions in the Discord channel. The human choice is highlighted in orange.}
    \label{fig:data_collect}
\end{figure}

\section{Human Preference Dataset}
\label{sec:dataset}
In order to get a better understanding of the human preferences on the images generated from prompts, and to improve text-to-image generation quality, we start by collecting a dataset of human choices.

\noindent\textbf{Data collection.} 
We utilize the ``dreambot'' channel on the Stable Foundation Discord server to gather human choice data. The chat history of these channels is obtained using the \href{https://github.com/Tyrrrz/DiscordChatExporter}{DiscordChatExporter}~\cite{discord} tool, which downloads the full chat history of a Discord channel and stores it in JSON format. 
Among the chat messages, a discernible pattern of interaction is observed, as depicted in Fig.~\ref{fig:data_collect}, which reveals human preferences.
In this pattern, a user initiates a session by sending a text prompt to the bot, which generates several images in response. 
Then, the user selects a preferred image and sends it back to the bot, along with the original text prompt.
The bot will return several refined images.
This interaction follows a pre-defined grammar, which allows us to extract human choice and related images using simple pattern matching techniques.

\noindent\textbf{Data format and statistics.} 
Finally, we obtain a total of 98,807 images generated from 25,205 prompts. 
Each prompt corresponds to several images, among which one image is chosen by the user as the preferred one, while others are non-preferred negatives.
Each prompt corresponds with varying number of images.
23,722 prompts have four images, 953 prompts have three images, and 530 prompts have two images. 
The number of images for each prompt depends on the user's specifications in the generation request. Notably, the dataset exhibits a high level of diversity, with images generated across a broad range of themes. 
The dataset consists choices made by 2,659 different users, and each user contributes at most 267 choices.
Examples of the collected dataset can be found in Fig.~\ref{fig:example}.
For further details on the dataset, we refer the readers to Fig.~\ref{fig:more_example} in the appendix.


\noindent\textbf{Privacy and NSFW contents.} We observe that a small portion of images is generated with image condition (the condition image may be either generated or uploaded by the user).
Since user-uploaded images may contain sensitive information or privacy, we do not include them in our dataset.
For the images with potential NSFW contents, we use the channel bot's NSFW detector to filter them out.

In this work, we utilize this dataset to study the existing metrics' correlation with human preferences, which will be introduced in Sec.~\ref{sec:metrics}.
The dataset also serves as the training data for our human preference classifier, which is to be introduced in Sec.~\ref{sec:score}.

\section{Existing Metrics}
\label{sec:metrics}

In this section, we show that the current mainstream evaluation metrics are not well correlated with human preferences on our dataset.
% We categorize the existing metrics by their image encoder.

\input{tables/IS_FID}

\subsection{Metrics by Inception Net}

Inception Score (IS)~\cite{inception_score} and Fréchet inception distance (FID)~\cite{fid} are two popular metrics used to evaluate the quality of generated images. 
Both of them perceive an image through an Inception Net~\cite{inceptionnet} trained on ImageNet~\cite{imagenet}.
In this section, we investigate their correlation with human choices.

\noindent\textbf{Inception Score (IS)} 
measures the quality of generated images by computing the expected KL-divergence between the marginal class distribution over all generated images and the conditional distribution for a particular generated image, using the class probability predicted by the Inception Net. 
This metric is expected to capture both the fidelity and diversity of generated images. 
To determine the correlation between IS and human preferences, we compute IS for both the set of preferred and non-preferred images in our dataset. 
For each setting, we divide 20,000 images into 10 splits and reported the mean and standard deviation of IS computed on them. 
Our results, as shown in Tab.~\ref{tab:IS_FID}, indicate no significant difference between the preferred and non-preferred images.

\noindent\textbf{Fréchet Inception Distance (FID)} 
measures the similarity between the embedding feature of generated and real images. This is achieved by fitting the embedding features into a multivariate Gaussian distribution and computing their Fréchet distance. 
To define the target distribution, FID requires a set of real images. 
However, in the case of images generated from user-provided prompts, such as in our dataset, the target distribution is defined by users' intention, which can only be inferred from text prompts. 
To address this, we randomly sample 10,000 text prompts from our dataset, and for each prompt, we query the LAION~\cite{laion} dataset via the official \href{https://knn.laion.ai/knn-service}{api} to find the closest image, which is taken as a ``pseudo ground truth'' for that prompt. 
This provides a set of real images aligned with the users' intentions. 
We randomly sample 10,000 images from both the preferred and non-preferred split of the collected dataset to compute FID with the real images. 
Our results, as shown in Tab.~\ref{tab:IS_FID}, reveals no significant difference between the preferred and non-preferred images in terms of FID. 
This suggests that FID may not be a reliable metric for evaluating human preference.


\noindent\textbf{Discussion.} 
IS and FID may suffer from the following three issues when evaluating human preference.
Firstly, generated images often contain shape artifacts, as shown in Fig.~\ref{fig:seed}. 
However, classification-based CNNs tend to be biased towards image texture rather than shape~\cite{Geirhos2018ImageNettrainedCA}, making them be likely to ignore shape artifacts in generated images. 
Additionally, domain gap can pose a problem.
While the evaluation model is trained on real images from ImageNet~\cite{imagenet}, the generated images in our dataset exhibit a wide range of styles and themes, from oil painting portraits to digital art of cyborgs. 
As a result, the ImageNet-trained model may not have meaningful representations for these diverse images~\cite{inceptionscore}. 
Furthermore, these metrics are limited by their single-modal nature, which means that they cannot infer user intentions by accessing prompts, unless the target images are known or provided, like we do.

\begin{figure*}
    \centering
    \hspace{0.7cm}
    \includegraphics[width=0.85\linewidth]{images/overview.pdf}
% \vspace{cm}
\caption{Left: training human preference classifier to derive HPS. Right: adapting Stable Diffusion to generate preferable images. During training, the Stable Diffusion is tuned to associate the concept of non-prefer with the prompt prefix [Identifier]. During inference, [Identifier] is used as the negative prompt in classifier free guidance.}
\label{fig:overview}
\end{figure*}

\subsection{Metrics by CLIP}
\input{tables/CLIP}
Thanks to the large and diverse set of training data, CLIP is better at encoding images from various domains compared to ImageNet-trained models. Moreover, it can capture users' intention by encoding text prompts, making it a plausible choice for evaluating the alignment between a prompt and a generated image~\cite{glide, dalle2, instructpix2pix, stable_diffusion}.
\href{https://laion.ai/blog/laion-aesthetics/}{Aesthetic Score Predictor} ~\cite{aesthetic_classifier} is another CLIP-based tool for image quality evaluation, which has been utilized to filter the training data for Stable Diffusion~\cite{stable_diffusion}.
In this section, we evaluate the capability of these tools in predicting the human choice, which is done by counting the accuracy of the human choice prediction task conducted on a split of 5,000 samples from our dataset.

\noindent\textbf{CLIP score} is derived as the cosine similarity between the prompt embedding and the image embedding computed by CLIP. 
We evaluated the performance on ViT-L/14 and RN50x64 models, which are the largest open-source CLIP models for transformer and CNN architecture. 
Our results, presented in Tab.~\ref{tab:clip}, demonstrate that both CLIP models exhibit superior performance over random guessing. 
However, we will show in Sec.~\ref{sec:reliability} that the CLIP score is not sensitive to the aesthetic aspect of an image. 
Nevertheless, we will also show that it can be further fine-tuned on our dataset to better align with human preferences.

% add some comment on the human preference 

% However, a fatal problem of CLIP~\cite{clip} is that it is strongly biased towards synthesised images rather than real images.
% We validate this by letting CLIP choose between paired real and synthesised images with the same prompt.
% We sample 5,000 images from the COCO Captions~\cite{cococaptions} dataset, along with their corresponding captions.
% The synthesised images are conditioned on the captions to pair up with the real ones. 
% In Tab.~\ref{tab:clip}, we show that both ViT and ResNet versions of CLIP performs below random guess, sh owing a strong bias towards the generated images.

\noindent\textbf{Aesthetic score } is based on a pre-trained ViT-L/14 CLIP image encoder, which is adapted to the task of aesthetic score prediction by adding a MLP layer on top of the CLIP image encoder.
The MLP is trained on several aesthetic datasets, including both real images and generated images (e.g., AVA~\cite{ava}, SAC~\cite{sac}) to predict aesthetic scores ranging from 1 to 10.
Unlike CLIP, the aesthetic classifier does not condition on the prompt, so the image with the highest predicted score is taken as the model choice.
As shown in Tab.~\ref{tab:clip}, the aesthetic classifier also exhibits better-than-chance accuracy in predicting user choice, indicating the importance of the aesthetic aspect of an image in human decision-making.

% \begin{figure}
% \begin{multicols}{2}
%     {
%     \centering
%     \includegraphics[width=1.0\linewidth]{images/real_room.png} \\
%     \centering
%     CLIP similarity: 0.216 % \\
%     % Real Image \\
%     }
%     {
%     \centering
%     \includegraphics[width=1.0\linewidth]{images/syn_room.png} \\
%     \hspace{0.2cm}
%     CLIP similarity: 0.260 % \\
%     % Generated Image \\
%     }
% \end{multicols}
% \vspace{-0.2cm}
% \caption{Prompt: ``The child is waiting for his parents to come in.'' CLIP is biased towards generated images.}
% \label{fig:real_bias}
% \end{figure}

% \noindent\textbf{Discussion} 
% Our finding is consistent with the Concept Association Bias (CAB) of CLIP discovered by Yamada \etal~\cite{cabias}. 
% We hyposis that the bias is due to the ``extra'' objects in an image that the caption does not cover.
% For the example in Fig.~\ref{fig:real_bias}, the real image contains more objects than the text prompt, which will negatively affect its similarity with the prompt according to CAB, even though it corresponds better with the prompt, and visually more appealing.

% However, recent work suggests that the CLIP model tend to treat the input text and image as bag-of-concepts~\cite{fistofwords, cabias}, which indicates that the CLIP score is insensitive to the certain sequential or spatial combination of the input prompts and generated images.

\section{Human Preference Score}
\label{sec:score}
We firstly train a human preference classifier to predict the human choice based on the prompt text, and then derive HPS based on the trained classifier, which  complements image quality assessments by incorporating human aesthetic preferences. 

\noindent\textbf{Human preference classifier} 
We fine-tune the ViT-L/14 version of CLIP on our dataset to better align with human preferences.
Each sample in the training set contains one prompt along with $n \in \{2,3,4\}$ images, among which only one image is preferred by the user.
The model is trained to maximize the similarity between the embedding of the text prompt computed by the CLIP text encoder and the embedding of the preferred image computed by the CLIP visual encoder, while minimize the similarity for non-preferred images.
By fine-tuning on human choices of generated images, the model is encouraged to better align with human preferences.

\noindent\textbf{Human preference score (HPS)} is derived from the human preference classifier. 
We define HPS as:
$$ \mathrm{HPS}(\mathrm{img}, \mathrm{txt}) = 100 \cos(\mathit{enc}_v(\mathrm{img}), \mathit{enc}_t(\mathrm{txt})),$$
where $\mathit{enc}_v$ and $\mathit{enc}_t$ are the visual encoder and the text encoder of the human preference classifier.
We multiply the cosine similarity by a factor of 100 for better visualization.

\section{Better Aligning Stable Diffusion with Human Preferences}
HPS can be used to guide diffusion-based generative models to better align with human users.
We argue that the misalignment between generated images and human preferences is a problem of missing ``awareness'' rather than model capacity. 
To address this issue, we propose to adapt the generative model by explicitly distinguishing preferred images from non-preferred ones. 
Our solution is straightforward and intuitive.
We construct another dataset consisting of prompts and their newly generated images, which we categorize as either preferred or non-preferred using our previously trained human preference classifier. 
For the non-preferred images, we modify their corresponding prompts by prepending a special prefix. 
By adapting Stable Diffusion on this dataset via LoRA~\cite{lora}, we enhance the model's ability to learn the concept of non-preferred images, which can subsequently be avoided during inference.

\noindent\textbf{Constructing training data.} 
We construct the training data from the ``\texttt{large\_first\_1m}'' split of DiffusionDB~\cite{diffusiondb}, and a subset of the pre-train dataset of Stable Diffusion (LAION-5B) for regularization.
DiffusionDB~\cite{diffusiondb} is a large-scale dataset of generated images along with their text prompts.
For images from DiffusionDB, we firstly compute HPS for each image-prompt pair.
After that, we group the images by their prompts, and for each prompt $T$, we add the image $I^*$ with highest HPS into the training data if it passes the following criteria:
$$p > \frac{\alpha}{n},$$
where $n$ is the number of images with the same prompt, $\alpha$ is a hyper-parameter that controls the selectivity. 
$p$ is given by:
$$ p = \frac{\exp(HPS(I^*, T))}{\sum_{I\in B}\exp(HPS(I, T))},$$
where $B$ is the set of images with the same prompt.
Similarly, we construct the non-preferred subset by the same criteria, but using negative HPS.
Finally, we get a mixed dataset of generated images and real images, where the non-preferred generated images are identified by their prompt prefix.

\noindent\textbf{Adapting Stable Diffusion.} We adopt LoRA~\cite{lora} to adapt Stable Diffusion to the training data, in which the parameters of the original model are kept frozen, and the \{key, query, value, out\} projection matrices are augmented with a low-rank residual.
LoRA does not add new parameters to the model, since the learned projection matrices can be merged into the base model once trained.
During training, we use the prompt as caption for generated images.
For non-preferred images, we prepend a special identifier before each of their captions (we choose ``Weird image.'' as the special identifier in our case).
During inference, the special identifier is used as the negative prompt for classifier-free guidance~\cite{classifierfree} to avoid generating non-preferred images.

\begin{figure}
    \centering
    \includegraphics[width=0.75\linewidth]{images/visualization.pdf}
    % \vspace{0.1cm}
    \caption{Correlation between HPS and CLIP score. While the CLIP score emphasises more on the direct matching between the image content and the text prompt, HPS emphasises more on the aesthetic quality of images.}
    \label{fig:vis}
\end{figure}


\section{Experiments}

In this section we firstly validate the reliability of HPS in Sec.~\ref{sec:reliability}, and then in Sec.~\ref{sec:application}, we introduce our experiments of adapting Stable Diffusion.

\begin{figure*}
    \centering
    % \includegraphics[width=0.9\linewidth]{images/results.pdf}
    \includegraphics[width=0.9\linewidth]{images/results_new.pdf}
\caption{Comparison of images generated by the original model, the regularization only model, and our adapted model. ``Regularization Only'' refers to a head-to-head setting against ``Adapted Model'', where Stable Diffusion is adapted without HPS-labeled images. Images in the same row are generated with the same prompt and random seed. The prompts are sampled from DiffusionDB. The adapted model can better capture the user intention from the prompt, and generate more preferable images with less artifacts.}
\label{fig:quality}
\end{figure*}

\subsection{HPS}
\label{sec:reliability}
\noindent\textbf{Implementation details of human preference classifier. }
We use 20,205 samples from our dataset during training, which contains 20,205 prompts and 79,167 images.
We use the ViT-L/14 version of CLIP in our experiments.
We fine-tune the last 10 layers of the CLIP image encoder and the last 6 layers of the text encoder.
The model is trained by the AdamW optimizer~\cite{adam} with a learning rate of $1.7\times10^{-5}$ for 1 epoch.
The batch size is 5.
The learning rate decays with a cosine learning rate schedule.
Weight decay is set as $3.1\times10^{-3}$.
Instead of using the original data augmentation of random resized crop, we directly resize the longest edge of the image to 224, and then pad zeros to make the shorter edge increase to 224.
We empirically find that fixing the aspect ratio of the image is beneficial.
The hyper parameters are tuned via Bayesian optimization.

\input{tables/generalization}

\noindent\textbf{Alignment with human. }
As shown in Tab.~\ref{tab:clip}, the trained model significantly outperforms CLIP in the human choice prediction task.
Due to the strong diversity of human preference, the accuracy is even higher than our human participants.

\noindent\textbf{Generalization. } We evaluate HPS' generalization capability towards other generative models by user studies.
In this experiment, we let the human preference classifier and several human participants to evaluate 398 pairs of images.
In each pair, the images are generated by DALL·E~\cite{dalle2} and Stable Diffusion~\cite{stable_diffusion} with the same text prompt.
The prompts are randomly sampled from DiffusionDB~\cite{diffusiondb}, which is a large database of images and prompts sourced from the Stable Foundation Discord channel.
We filter out the NSFW prompts by the indicator provided in DiffusionDB~\cite{diffusiondb}.

In Tab.~\ref{tab:agreement}, we evaluate the agreement between the predictions from human, CLIP and HPS.
The agreement is computed by averaging the similarity of prediction of each participant.
HPS is better aligned with human preference compared to CLIP socre, and its agreement with human approaches the agreement between humans.
It shows that HPS can generalize towards images generated by other models.
We refer the readers to the supplementary material for a full list of images and choices made in this user study.

\noindent\textbf{Correlation with CLIP score. }
In Fig.~\ref{fig:vis}, we visualize the correlation between HPS and CLIP score.
The text prompts are randomly sampled from the COCO Captions~\cite{cococaptions} dataset, and the images are generated by Stable Diffusion~\cite{stable_diffusion}.
We can see that HPS has a positive correlation with CLIP score, but emphasises more on the aesthetic quality of an image.
However, HPS put less importance on the direct matching between image contents and text prompts, which can be interpreted as a visual analogy of ``alignment tax'' introduced in ~\cite{instructgpt}.

\subsection{Better Aligning Stable Diffusion with Human Preferences}
\label{sec:application}
\noindent\textbf{Implementation details. }
We use the Stable Diffusion~\cite{stable_diffusion} v1.4 for all our experiments.
$\alpha$ is set to 2.0 for both preferred images and non-preferred images when constructing the training set.
The constructed training set contains 37,572 preferred generated images, 21,108 non-preferred generated images.
The regularization images are from a 625k subset of LAION-5B filtered by the aesthetic score predictor with a threshold of 6.5.
200,231 regularization images participates training.
We only fine-tune the UNet of Stable Diffusion, while keeping the VAE and the text encoder frozen during training.
The rank is set to 32 in LoRA~\cite{lora}. 
The LoRA weights are trained for 10k iterations with the AdamW~\cite{adam} optimizer with a learning rate of $1\times10^{-5}$ and a weight decay of $1\times10^{-2}$, which is kept constant during training.
We use a batch size of 40 in our experiments.
For inference, we run the diffusion process by 50 steps for each image with PNDM~\cite{Liu2022PseudoNM} noise scheduler.
We use the default guidance scale of 7.5 for classifier-free guidance~\cite{classifierfree}.

\begin{figure}
    \centering
    \includegraphics[width=0.8\linewidth]{images/user_study.pdf}
\caption{Human evaluation results on images generated from 100 randomly sampled prompts. The color represents the number of positive votes received from 5 participants.}
\label{fig:user_study}
\end{figure}

\begin{figure}
\centering
\includegraphics[width=.95\linewidth]{images/negative_example.pdf}
\caption{The adapted model can hardly improve images that are already well aligned with human aesthetics. The left/right image is generated by the original/adapted model. Both images are generated using the same prompt and random seed.}
\label{fig:struggle}
\end{figure}

\noindent\textbf{Human evaluation. }
We compare our trained model with the original Stable Diffusion by conducting user studies.
In this study, we randomly sample 100 user-provided prompts from DiffusionDB~\cite{diffusiondb}.
% Prompts that may contain NSFW contents are excluded in advance.
For each prompt, we generate an image from both models with the same random seed for fair comparison.
By doing this, we get 100 pairs of generated images as the test set.
We ask 5 participants to read the prompt, and then choose between the image generated by our trained model and the original Stable Diffusion based on to what extent the generated image satisfies the prompt writers' intention.

In Fig.~\ref{fig:example}, we visualize our result by showing the percentages of images with different numbers of positive votes.
The adapted model significantly outperforms the original model.
72\% of the images generated by the adapted model has more than 3 votes, while the number is 28\% for the original model.
It should be noted that a large portion of images generated by the original model is already good enough, the adapted model struggles to further improve them, as shown in Fig.~\ref{fig:struggle}.
The image quality improves the most when there is obvious artifacts.
We provide a full list of images and user choices in this user study in the supplementary material.

\noindent\textbf{Qualitative Evaluation}
In Fig~\ref{fig:quality}, we show some typical cases of improvement.
We compare the original model, the regularization only model, and the adapted model.
The adapted model is trained with both real regularization images and generated images with HPS preference labels.
The regularization only model is a head-to-head comparison with the adapted model, which is trained by removing the generated images from the training set, and is trained exclusively on regularization images for the same number of steps.
The results show that the adapted model can better capture the user intention from the prompt, as shown in the first row.
The last three rows show that training with generated images mitigates the problem of unnatural limbs.
We refer the readers to Fig.~\ref{fig:quality} and Fig.~\ref{fig:artifacts} in the appendix for more examples.


\section{Limitations}
\label{sec:limitations}
There are several limitations about the dataset.
The collected dataset contains generated prompts and images of public figures. We choose to mark them out instead of removing them to keep the diversity of the dataset.
Despite the diversity of the dataset, we are also aware that it only represents the aesthetic preference of a small portion of people in the world, and it may be biased towards the certain group of people that are active in the Stable Foundation Discord channel.
Another potential bias about this dataset is that a large portion of text prompts are written by experienced Stable Diffusion users.
These prompts are very likely to be tweaked to activate the potential of Stable Diffusion, and deviates from normal language habits.

\section{Conclusion}
In this work, we study the human preference on a large-scale dataset of generated images.
We find that the previous evaluation metrics for generative models are not well aligned with human preferences, but the CLIP model can be fine-tuned into a human preference classifier to better align with the human choice.
Then, we show a simple yet effective method to adapt the generative model to generate more preferable images with the guidance of human preference score.
We hope our work can inspire the community to explore new possibilities of the human-aligned AI research.



% \input{tables/prompts}

% \subsection{Factors that influence human preference}
% The community has found that by putting certain phrases in prompts can make it easier to generate preferrable images.
% We explore the effect of these prompts on HPS.
% We randomly sample 500 captions from the COCO Captions dataset, and add the certain phrases are as prefix before generating images.
% When computing HPS, we use the original caption for fair comparison.
% We evaluate the average HPS over the 500 images generated for each caption.
% As shown in Tab.~\ref{tab:prompts}, the prompt of ``masterpiece''



{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\input{appendix/appendix}

\end{document}