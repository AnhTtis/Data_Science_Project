{
    "arxiv_id": "2303.14420",
    "paper_title": "Better Aligning Text-to-Image Models with Human Preference",
    "authors": [
        "Xiaoshi Wu",
        "Keqiang Sun",
        "Feng Zhu",
        "Rui Zhao",
        "Hongsheng Li"
    ],
    "submission_date": "2023-03-25",
    "revised_dates": [
        "2023-04-10"
    ],
    "latest_version": 1,
    "categories": [
        "cs.CV",
        "cs.AI"
    ],
    "abstract": "Recent years have witnessed a rapid growth of deep generative models, with text-to-image models gaining significant attention from the public. However, existing models often generate images that do not align well with human aesthetic preferences, such as awkward combinations of limbs and facial expressions. To address this issue, we collect a dataset of human choices on generated images from the Stable Foundation Discord channel. Our experiments demonstrate that current evaluation metrics for generative models do not correlate well with human choices. Thus, we train a human preference classifier with the collected dataset and derive a Human Preference Score (HPS) based on the classifier. Using the HPS, we propose a simple yet effective method to adapt Stable Diffusion to better align with human aesthetic preferences. Our experiments show that the HPS outperforms CLIP in predicting human choices and has good generalization capability towards images generated from other models. By tuning Stable Diffusion with the guidance of the HPS, the adapted model is able to generate images that are more preferred by human users.",
    "pdf_urls": [
        "http://arxiv.org/pdf/2303.14420v1"
    ],
    "publication_venue": "15 pages, 11 figures"
}