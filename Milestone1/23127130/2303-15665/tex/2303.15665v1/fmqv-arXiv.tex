\documentclass[prl, 10pt,twocolumn,groupedaddress,floatfix,showpacs]{revtex4-1}


%#####################################
%\usepackage[utf8]{inputenc}  
%\usepackage[T1]{fontenc}     %Output what you want e.g., é, ł, a, ü
%\usepackage[british]{babel}  %Do hyphenation according to british english
%\usepackage[sc,osf]{mathpazo}\linespread{1.05}  %Palatino font
\usepackage[scaled=0.86]{berasans}  % URL font that go well wtih palatino
\usepackage[colorlinks=true, allcolors=blue, urlcolor=blue]{hyperref}  %Hyperlinks (pink, green, blue)
\usepackage{graphicx} % Package to insert exteral figures
\usepackage[babel]{microtype}  %Improves text justification
\usepackage{amsmath,amssymb,amsthm,bm,amsfonts,mathrsfs,bbm} %Usefull math packages
\renewcommand{\qedsymbol}{\rule{0.7em}{0.7em}}
\usepackage{xspace}  %Useful to add space in macros
\usepackage{pgfplots}
\usepackage{xcolor,colortbl}
\usepackage{array}
\usepackage{bigstrut}
\usepackage{tcolorbox}
\usepackage{algorithm} % Package for create algorithm
\usepackage{algpseudocode} % Package for create algorithm
\usepackage{multirow} % Package for table 
\algnewcommand{\To}{\textbf{to }}
\algnewcommand\Input{\item[\textbf{input:}]}%
\algnewcommand\Output{\item[\textbf{output:}]}%
\let\oldemptyset\emptyset
\algdef{SE}[DOWHILE]{Do}{doWhile}{\algorithmicdo}[1]{\algorithmicwhile\ #1}%

%%%%%%%%%%%%%%%%%%bea

% ------------------------------------------------------------------------------
\newcommand{\ashu}{\color{blue}}
\newcommand{\bae}{\color{purple}}
\newcommand{\ji}{\color{gray}}


\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\E}{\mathbb{E}}

\newcommand{\tr}{\text{tr}}
\newcommand{\ket}[1]{| #1 \rangle}
\newcommand{\bra}[1]{\langle #1|}
\newcommand{\ip}[2]{\langle #1|#2 \rangle}
\newcommand{\bracket}[3]{\langle #1|#2|#3 \rangle}
\newcommand{\sm}[1]{\left( \begin{smallmatrix} #1 \end{smallmatrix} \right)}

\newcommand{\be}{\begin{equation}}
\newcommand{\ee}{\end{equation}}
\newcommand{\bea}{\begin{eqnarray}}
\newcommand{\eea}{\end{eqnarray}}
\newcommand{\bes}{\begin{equation*}}
\newcommand{\ees}{\end{equation*}}
\newcommand{\beas}{\begin{eqnarray*}}
	\newcommand{\eeas}{\end{eqnarray*}}

% ------------------------------------------------------------------------------

\newcommand{\x}{\mathrm{x}}
%\newcommand{\ket}[1]{|#1\rangle}
\newcommand{\ketbra}[1]{\ket{#1}\!\bra{#1}}
%\newcommand{\bra}[1]{\langle#1|}
\newcommand{\proj}[1]{\ket{#1}\!\bra{#1}}

\newcommand{\ten}{\otimes}
\newcommand{\Id}{\mathds{1}}
\newcommand{\zero}{\mathbf{0}}
\newcommand{\KBDS}{C^{s}}
\newcommand{\KBDSs}{C}
%\newcommand{\KBDS}{C^{\mbox{\tiny U}}}
\newcommand{\KTwo}{C^{2\times2}}
\renewcommand{\H}{\mathcal{H}}
\def\A{\mathcal{A}}
\def\B{\mathcal{B}}

\def\x{\mathrm{x}}
\def\y{\mathrm{y}}
\def\g{\mathrm{guess}}

\def\K{\widetilde{K}}
\def\v{\vec{v}}
\def\sv{\hat{s}}
\def\M{\widetilde{M}}

\def\sig{\widetilde{\sigma}}
\def\E{\mathrm{E}}
\def\N{\mathcal{N}}
\def\P{\mathrm{P}}
\def\tr{\mathrm{tr}}

\def\L{\mathcal{L}}
\def\Q{\mathcal{Q}}
\def\NL{\mathcal{NS}}


\newtheorem{thm}{Theorem}[section]
\newtheorem*{thm*}{Theorem}
\newtheorem{cor}[thm]{Corollary}
\newtheorem{con}[thm]{Conjecture}
\newtheorem{lem}{Lemma}
\newtheorem*{lem*}{Lemma}
\newtheorem{prop}[thm]{Proposition}
\newtheorem{defn}{Definition}
\newtheorem*{defn*}{Definition}
\newtheorem{rem}[thm]{Remark}
\newtheorem{eg}[thm]{Example}

\newtheorem*{lipschitzLem*}{Lemma \ref{lipschitz}}
\newtheorem*{lipschitzCubeLem*}{Lemma \ref{lipschitzCube}}
\newtheorem*{pgmNearlyOptimalThm*}{Theorem \ref{pgmNearlyOptimal}}

% ------------------------------------------------------------------------------




%#####################################

\begin{document}

\title{  Feature Map for Quantum Data: Probabilistic Manipulation}

% \title{Classical, quantum, and post-quantum capacity separation for a class of interference channels}


%#####################################
\author{Hyeokjea Kwon}
%\email{hyukjaekwon@kaist.ac.kr }
\affiliation{School of Electrical Engineering, Korea Advanced Institute of Science and Technology (KAIST), 291 Daehak-ro, Yuseong-gu, Daejeon 34141, Republic of Korea.}

\author{Hojun Lee}
%\email{hyukjaekwon@kaist.ac.kr }
\affiliation{School of Electrical Engineering, Korea Advanced Institute of Science and Technology (KAIST), 291 Daehak-ro, Yuseong-gu, Daejeon 34141, Republic of Korea.}


\author{Joonwoo Bae}
%\email{joonwoo.bae@kaist.ac.kr}
\affiliation{School of Electrical Engineering, Korea Advanced Institute of Science and Technology (KAIST), 291 Daehak-ro, Yuseong-gu, Daejeon 34141, Republic of Korea.}





%########################################

\begin{abstract}
The {\it kernel trick} in supervised learning signifies transformations of an inner product by a feature map, which then restructures training data in a larger Hilbert space according to an endowed inner product. A {\it quantum feature map} corresponds to an instance with a Hilbert space of quantum states by fueling quantum resources to ML algorithms. In this work, we point out that the quantum state space is specific such that a measurement postulate characterizes an inner product and that manipulation of quantum states prepared from classical data cannot enhance the distinguishability of data points. We present {\it a feature map for quantum data} as a probabilistic manipulation of quantum states to improve supervised learning algorithms. 
\end{abstract}
%\pacs{03.65.Ud, 02.50.Le, 03.67.Ac}

\maketitle

%\section{Introduction} 

In supervised learning, one aims to construct a model that makes predictions based on training data. Recently, the framework has begun to apply the laws of quantum mechanics, Quantum Machine Learning, to fuel nonclassical properties such as entanglement and superposition to machine learning (ML) algorithms for further advantages, see e.g., \cite{Biamonte:2017aa, schuldbook, PhysRevLett.117.130501}. One way to apply Quantum Information Theory (QIT) in ML is to process ML algorithms with quantum states prepared according to classical data. 

From the view of QIT, the state preparation rephrases embedding classical data to quantum systems. Technically, the space of quantum states is described by a Hilbert space where the measurement postulate, called the Born rule, specifies an inner product \cite{neubook}. From the view of ML, embedding data in a Hilbert space corresponds to a feature map: its quantum application is called a {\it quantum feature map} \cite{SK2019, https://doi.org/10.48550/arxiv.2101.11020}. Then, the resulting quantum states and the Hilbert space are referred to as feature vectors and a feature space, respectively. 

On the one hand, a quantum feature map allows one to exploit quantum resources such as entanglement and superposition existing in quantum states to enhance ML algorithms. As a result, one may envisage quantum advantages over classical counterparts. On the other hand, one notices that the quantum state space is a specific and restricted object. It is a Hilbert space entirely characterized by the postulates of quantum theory \cite{neubook}. 

The consequences show that once a feature map prepares quantum states, quantum operations are contractive, i.e., the norm of feature vectors does not increase \cite{Perez-Garcia:2006aa}. Moreover, a mathematical space describing quantum states is not hypothetical: the measurement postulate defines an inner product uniquely in the space, known as the Gleason theorem \cite{gleason}. The uniqueness implies limitations on the so-called {\it kernel tricks} in the quantum feature space. Apart from the fact the quantum state preparation corresponds to a feature map {\it per se}, little is known about how quantum principles can be incorporated to feature vectors to enhance ML algorithms.

In this work, we show that once quantum states are prepared for an ML algorithm, their distinguishability does not increase by a feature map. In contrast to classical ML algorithms, quantum data cannot be manipulated such that their distinguishability is enhanced. We then present a general manipulation of quantum data, namely a {\it feature map for quantum data}, by relaxing the trace-preserving condition, as a versatile tool to improve ML algorithms. We also develop a circuit construction of a feature map for quantum data and demonstrate its advantages in supervised learning for binary classification. 

% for sample data, denoted by $\{x_m\in \mathbbm{R}^N\}$. A feature map is

We first summarize a feature map as a general process of embedding data into a larger Hilbert space. For sample data $D = \{x_m\in \mathbbm{R}^N\}$, a feature map $\phi: \mathbbm{R}^N\rightarrow \H$ for $\dim (\H)\geq N$ induces an inner product, $\kappa (x_m, x_n ) = \langle {\phi}(x_m), {\phi}(x_{n }) \rangle_{\H}$ on a resulting Hilbert space, where $\kappa: (x_m,x_n) \mapsto \mathbbm{R}$ denotes a kernel function via an inner product on the feature space $\H$. Thus, a structure is introduced to data in terms of an inner product.

%\bea
%\kappa (x_m, x_n ) = \langle {\phi}(x_m), {\phi}(x_{n }) \rangle_{\H} 
%\eea

A {\it quantum feature space} is a Hilbert space of quantum states such that $n$ qubits define a $2^n$-dimensional space on which a quantum state could contain superposition or entanglement. A quantum feature map devises a quantum circuit $U_{\phi}$ that prepares states from sample data as follows,
\bea
\phi~: ~ x_m \in \mathbb{R}^N \mapsto %\phi(x^m) := 
\rho_{\phi}(x_m) \in S(\H) \label{eq:qfeature}
\eea
where $\rho_{\phi}(x_m)$ is a quantum state on a Hilbert space $\H$ and $S(\H)$ denotes the set of states. Consequently, a kernel is a mapping from a pair of datapoints to an inner product in the feature space via a feature map, 
\bea
\kappa (x_m, x_{ n })  =\tr [\rho_{\phi} (x_m) \rho_{\phi} (x_{ n })] ~~  \label{eq:ke}
\eea
which is in fact given by the measurement postulate. For instance, Eq. (\ref{eq:ke}) may be obtained as a probability of obtaining an outcome on positive-operator-valued-measure $\alpha \rho_{\phi} (x_{n })$ with some $\alpha>0$ for a state $\rho_{\phi} (x_m)$. Or, it corresponds to the estimation of the visibility of two states in an interferometric setup, e.g., the Hong-Ou-Mandel fringe for photonic qubits \cite{PhysRevLett.59.2044, PhysRevLett.89.127902}. 


%In Ref. \cite{}, various sets of quantum states are considered to realize a quantum feature space. A qubit state space can be exploited as a feature space \cite{}. Infinite-dimensional systems such as coherent states \cite{} could be used to introduce Gaussian kernels as well as squeezed states \cite{}. A quantum feature map is distinct to a classical one, i.e., when sample data are embedded in a Hilbert space, in that data are embedded to physical systems governed by quantum theory. 

%The Born rule from the measurement axiom uniquely identifies a quantum feature space 

The Hilbert space of quantum systems is specific in the sense that an inner product is introduced by a measurement postulate \cite{gleason, neubook}. It immediately implies limitations of a quantum feature map in that, contrasting to feature spaces by classical systems, quantum data cannot be repeatedly embedded in some other feature space such that they are structured with a higher distinguishability.  \\ %That is, {\it quantum states cannot be manipulated such that pairwise distinguishability in terms of the $L_1$ norm increases.}\

{\bf Proposition}. Quantum data cannot be embedded in a feature space with enhanced distinguishability. \\

{\it Proof.} We first consider cases that mapping quantum data directly back to a classical feature space by measurements. It is clear that non-orthogonal states cannot be perfectly distinguished, and thus the mapping introduces either an error \cite{Helstrom:1969aa} or ambiguous outcomes \cite{Ivanovic:1987aa, Dieks:1988aa, Peres:1988aa}, see also \cite{Bergou:2007aa, Bae_2015}. There are fundamental limitations in the bounds \cite{PhysRevA.77.012113, PhysRevLett.107.170403, doi:10.1063/1.3298647}

Or, one can consider {\it a feature map for quantum data} as embedding quantum data to high-dimensional quantum systems and then a measurement. A quantum channel, denoted by $\Lambda : \H \rightarrow \widetilde{H}$ for $\mathrm{dim}\H \leq \mathrm{dim}\widetilde{\H}$, corresponds to a positive and completely positive map for quantum states. Such a map does not increase pairwise distinguishability: for two states $\rho_1$ and $\rho_2$ appearing with probabilities $q_1$ and $q_2$, it holds that
\bea
\| \Lambda[X] \|_1 \leq \| X \|_1,~\mathrm{for}~X= q_1 \rho_1 - q_2 \rho_2 \ngeq 0
\eea
where $\|\cdot \|_1$ denotes the $L_1$ norm, i.e., $\|A \|_1 = \tr\sqrt{A^{\dagger}A}$. $\Box$\\

\begin{figure}[t]
	\begin{center}
		\includegraphics[angle=0, width=0.45 \textwidth]{scheme.pdf}
		\caption{ A quantum feature map transforms classical data (yellow) to quantum states (blue), where the mathematical structure is provided by a Hilbert space where an inner product is realized by a measurement axiom. A feature map defined by Kraus operators ($K,K_0$) for quantum states introduces two ensembles. One is by $K$ in which states are manipulated beyond the contractive properties (green), i.e., states can be even better distinguished. The other Kraus operator $K_0$ collects states whenever $K$ is unsuccessful. } \label{scheme}
	\end{center}	
\end{figure}

In the above, one can consider other measures of distinguishability such as von Neumann entropy or min-entropy and find the same conclusion \cite{PhysRevA.62.012301, Bae_2013}. 

We put a step forward to relaxing quantum channels to quantum filtering operations and present a general form of {\it a feature map for quantum data} to enhance ML algorithms with quantum states. To this end, we recall that a quantum channel can be described as a dynamics of a subsystem that interacts with an ancilla through a unitary transformation. When a state and an ancilla initialized in $\rho_{S}\otimes |a\rangle_A\langle a|$ result in $U_{SA} \rho_S \otimes |a \rangle_A\langle a | U_{SA}^{\dagger}$ for some interaction $U_{SA}$, a measurement on an ancilla in an orthonormal basis $\{|i\rangle_A \}$ finds the probability of having an outcome $i$,
\bea
p_A (i) = \tr[U_{SA} \rho_S \otimes |a \rangle_A\langle a | U_{SA}^{\dagger} |i \rangle_A\langle i |]. \nonumber
\eea
Then, the resulting state of a system can be described by
 \bea
\rho_S  (i) =\frac{1}{p_A (i)} {K_i} \rho K_{i}^{\dagger}~~\mathrm{where} ~K_i= _A\langle i | U_{SA} |a \rangle_A\label{eq:kraus}
\eea
where $\{ K_i\}$ are called Kraus operators, satisfying the relation $\sum_i K_{i}^{\dagger}K_i  =\mathbbm{I}$. 

It is worth mentioning that probabilistic manipulations of quantum states by exploiting Kraus operators have been used to resolve non-trivial problems in various contexts of quantum information applications. In entanglement theory, two-qubit entangled states can be transformed to a more entangled one with some probability by local operations and classical operations, called local filtering \cite{PhysRevA.64.010101, PhysRevLett.89.170401}. The protocol for distilling entanglement can be rephrased as a sequence of local filtering operations \cite{PhysRevLett.76.722, PhysRevLett.77.2818}. In fact, local filtering operations can also reveal hidden nonlocality existing in some entangled states \cite{PhysRevLett.74.2619}. In experiments, a post-selection technique can be described by Kraus operators. For instance, one way to demonstrate a quantum gate with photonic qubits which hardly interact with each other is to select particular measurement outcomes whenever photon-photon interactions were successful, see e.g., \cite{PhysRevLett.107.160401}. 

We now present {\it a feature map for quantum data} via Kraus operators as a probabilistic strategy of manipulating quantum states to enhance ML algorithms, see Fig. \ref{scheme}. Let $K $ and $K_{0}$ denote two Kraus operators such that $K $ describes a desired transformation of quantum data and $K_0  = (\mathbb{I} - K ^\dag K )^{1/2}$ otherwise. Then, quantum data previously prepared by a quantum feature map $\phi$ denoted by $D_{\phi} = \{  \rho_{\phi} (x_m) \in S(\H) \} $ can be transformed to $\widetilde{D}_{\phi} = \{  \widetilde{\rho}_{\phi} (x_m) \in S(\widetilde{\H}) \} $ such that
\bea
&& K~:~ {\rho}_{\phi} (x_m) \mapsto   \widetilde{\rho}_{\phi} (x_m) ~ \mathrm{where}~\label{eq:fq}\\
&& \widetilde{\rho}_{\phi} (x_m)  = \frac{  K \rho_{\phi} (x_m) K^{\dagger}}{p(x_m)}~\mathrm{with}~ p(x_m) = \tr[ K^{\dagger}K \rho_{\phi} (x_m) ]. \nonumber
\eea
From Proposition, we can safely restrict the consideration to the case $\dim ( \widetilde{\H}) = \dim ( {\H})$ due to no advantage of utilizing a larger Hilbert space of quantum states. It is straightforward to see that, once the transformation is successful, a Kraus operator also leads to a kernel: 
\bea
\kappa(  \rho_{\phi}(x_m) , \rho_{\phi} (x_n) ) = \tr [ \widetilde{\rho}_{\phi}(x_m) \widetilde{\rho}_{\phi}(x_{n }) ]
\eea
Therefore, the task to enhance ML algorithms for quantum data is to identify a Kraus operator $ K $ for manipulating quantum sample data beyond unitary transformations. If no enhancement occurs, one returns a trivial choice $K = \mathbbm{I}$.  


%Hence, a probabilistic feature map for quantum data can be characterized by Kraus operators. 

One may assert the weakness that a feature map in Eq. (\ref{eq:fq}) is probabilistic so that for a large number of datapoints, the probability of its realization $ p(x_1)\times p(x_2)\times \cdots \times p(x_m) \times\cdots $ quickly falls to zero. A collective interaction over blocks of quantum data can offer a prescription. That is, a Kraus operator is constructed for quantum states $D_{\phi} = \{  \rho_{\phi} (x_m) \in S(\H) \} $, 
\bea
K: \bigotimes_m {\rho}_{\phi} (x_m) \mapsto \bigotimes_m \widetilde{\rho}_{\phi} (x_m) \label{eq:kc}
\eea 
that prevents the overall success probability from dropping out to zero. Note that the success probability is given by $p(x_1,\cdots, x_m) = \tr[K^\dagger K \bigotimes_m {\rho}_{\phi} (x_m)]$, which could be low but does not depend on the size of data. Thus, the price to pay for a non-vanishing probability is to build a Kraus operator $K$ for interaction among all datapoints, see $V(\theta)$ in Fig. \ref{fig:fmqd}. 

In the following, we revisit supervised learning for binary classification: a training dataset is given as follows,
\bea
D = \{ (x_1, y_1), \dots, (x_M, y_M) : x_m\in\mathbbm{R}^N,~y_m =\pm 1 \}. 
\eea 
A linear hyperplane is sought to separate the set ${D}$ into two, one with $y_m=+1$ and the other with $y_m=-1$. A kernelized support vector machine applies
a kernel $\phi: \mathbbm{R}^N\rightarrow \mathbbm{R}^L$ for $L\geq N$ so that the decision boundary can be characterized in a higher-dimensional Hilbert space by a normal vector $w\in \mathbbm{R}^L$ such that $f(x) = w^{\top} \phi(x)$. A decision function is given as the sign of $f(x)$. The classification function can be obtained as,
\bea
f(x) =  \sum_{m }  y_m \kappa (x_m , x) \label{eq:cf}
\eea
where uniform weights are assumed and the kernel $\kappa$ depends on the choice of a feature map $\phi$. 

%The optimization problem is written as a dual problem
%\bea
%\max_{\{ \alpha_m\} }\left[  \sum_{m} \alpha_m y_m -\frac{1}{2} \sum_{m,n}\alpha_m \alpha_n y_m y_n \kappa( x_m, x_n)  \right]
%\eea
%with constraints that $\sum_m \alpha_m y_m =0$ and $\alpha_m\geq0$. A kernel has been used as $\kappa( x_m, x_n ) =   \phi(x_m)^{\top} \phi(x_n)$. The dual problem shows the optimization is up to a kernel. 

%After all, from the optimality conditions, the classification function can be written as,
%\bea
%f(x) = \mathrm{sign} \left( \sum_{m } \alpha_m y_m \kappa (x_m , x)\right) \label{eq:cf}
%\eea
%which is up to a feature map $\phi$. 



For simplicity, let us consider an amplitude encoding as a quantum feature map that prepares quantum states from sample data $D$,
\bea
\rho_{\phi}(x_m) = |\psi_{x_m}\rangle\langle\psi_{x_m}|~\mathrm{where} ~|\psi_{x_m}\rangle = \sum_{i=0}^{N-1}x_{m,i} |i\rangle.
\eea
To apply a SWAP-test based classifier \cite{Schuld_2017}, we write by $U$ a quantum circuit for the state preparation, 
\bea
U |0\rangle^{\otimes l} = \frac{1}{\sqrt{M}}\sum_{m=1}^{M} |m\rangle_L |\psi_{x_m}\rangle_T |\psi_{x} \rangle_t |0\rangle_S |s_m\rangle_C, ~~~\label{eq:InitState}
\eea
where $l =(2+ \log M+ 2\log N)$, see also Fig. \ref{fig:fmqd}. The first register (L) labels input data, the second one (T) collects training quantum data, the third one (t) contains a test state, the fourth one (S) is an ancilla needed for a SWAP test, and the last one (C) denotes a binary classification of training data: $|s_m\rangle = |0\rangle$ for $y_m=-1$ and $|s_m\rangle = |1\rangle$ for $y_m=1$. Registers (S) and (C) contain single qubits. 

\begin{figure}[t]
	\begin{center}
		\includegraphics[angle=0, width=.48 \textwidth]{fmqd2}
		\caption{  (A) A quantum feature map $U$ prepares quantum states from data, see the main text. (B) A feature map for quantum data implements a probabilistic transformation on both training and test data, i.e., when an outcome $0$ only is obtained in the register $F$. An interaction $V(\theta)$ may be trained to build an efficient feature map.  (C) A Hadamard classifier is implemented with a SWAP test.  } \label{fig:fmqd}
	\end{center}	
\end{figure}

Then, a Hadamard classifier, see the dotted box (C) in Fig. \ref{fig:fmqd}, applies a SWAP test for states in registers $T$ and $t$. Qubits in registers in $S$ and $C$ are measured in the computational basis $\{| 0\rangle_z, |1\rangle_z \}$, respectively. The classifier in Eq. (\ref{eq:cf}) can be constructed from outcomes in the registers. When an outcome $0$ $(1)$ is obtained in register $C$, states in registers $T$ are given by $\rho$ $(\sigma)$ where
\bea
\rho &=& \frac{1}{M_A}\sum_{y_m= - 1} \rho_{\phi}(x_m) ~ \mathrm{and}~ \sigma = \frac{1}{M_B} \sum_{y_m=1} \rho_{\phi}(x_m), ~~~\label{eq:ens}
\eea
where $M_A = |\{ x_m : y_m=-1 \}|$ and $M_B = |\{ x_m:y_m= 1 \}|$. Given an outcome $0$ in register $C$, a measurement is performed in register $S$: probabilities of $0$ and $1$ are given by $(1\pm \langle \psi_x | \rho |\psi_x\rangle )/2$, respectively. When an outcome in register $C$ is $1$, probabilities of $0$ and $1$ in register $S$ are given by $(1\pm \langle \psi_x | \sigma |\psi_x\rangle )/2$. Hence, from the probabilities, one can find the classification function in Eq. (\ref{eq:cf}) as  
\bea
f (x) =  \langle \psi_x | \rho - \sigma | \psi_x \rangle  \label{eq:qf}
\eea
The empirical risk with the loss function by $L( f (x), y) = yf(x) $ is given by
\bea
R_L &=& \frac{1}{M}\sum_{m }^{ } L( f (x_m), y_m) =  -\tr[(\rho-\sigma)^2], \label{eq:risk}
\eea
which also quantifies a classifier. Hence, a quantum feature map that maximizes the Hilbert-Schmidt distance between two ensembles $\rho$ and $\sigma$ builds a classifier with the least empirical risk \cite{https://doi.org/10.48550/arxiv.2001.03622}. 

\begin{figure}[t]
	\begin{center}
		\includegraphics[angle=0, width=0.48 \textwidth]{pqc}
		\caption{  An interaction that realizes a feature map for quantum data, see Fig. \ref{fig:fmqd}, is designed as $V_{TFt} (\theta) =  \widetilde{V}_{Ft} (\theta) \widetilde{V}_{TF} (\theta)$ and trained. A parameterized quantum circuit $\widetilde{V}(\theta)$ is chosen such that it contains arbitrary single qubit rotations and controlled-NOT gates. } \label{fig:pqc}
	\end{center}	
\end{figure}

A feature map in Eq. (\ref{eq:kc}) applies to quantum data in Eq. (\ref{eq:InitState}) to enhance quantum supervised learning for binary classification. The map is facilitated by a unitary transformation $V(\theta)$ over training and test data as well as an ancilla $F$,
\bea
 && V (\theta) \sum_{m=1}^{M}   |\psi_{x_m}\rangle_T |\psi_{x} \rangle_t |0\rangle_{F}    = \nonumber \\
  &&  \sum_{m=1}^{M}  K(\theta) |\psi_{x_m}\rangle_T |\psi_{x} \rangle_t |0\rangle_{F}  +   \sum_{m=1}^{M}   K_0(\theta) |\psi_{x_m}\rangle_T |\psi_{x} \rangle_t |1\rangle_{F}. ~~~ \nonumber
\eea
An interaction $V(\theta)$ is to be trained to minimize the empirical risk in Eq. (\ref{eq:risk}). Once a desired one $K(\theta)$ is realized with an ancilla state $|0\rangle_F$, ensembles of training data are transformed, denoted by $\widetilde{\rho}$ and $\widetilde{\sigma}$, respectively, similarly to ensembles in Eq. (\ref{eq:ens}), and the classifier is given by, ${f} (x) = \langle \widetilde{\psi}_x | \widetilde{\rho} - \widetilde{\sigma} | \widetilde{\psi}_x \rangle$. Resulting quantum data with $|1\rangle_F$ that is not of interest will be discarded. In practice, an efficient unitary $V(\theta)$ may be designed as a parameterized quantum circuit and trained to minimize the cost function ${C}(\theta)={R}_L({f}_{\mathrm{}})$, as it is shown in Fig. \ref{fig:pqc}. If a feature map by a Kraus operator $K(\theta)$ cannot make any advantage, an optimal parameter after training would return $V(\theta) =\mathbbm{I}$.

Let us illustrate the usefulness of a feature map for quantum data, for instance, the Iris dataset and the modified national institute of standard and technology (MNIST) dataset. In the former, Iris versicolor and Iris setosa are set as class $A$ and $B$, respectively. Then, sepal lengths and sepal widths are given as input data. We pick the $36$-th data in the Iris versicolor and the $34$-th data in the Iris setosa as the training dataset, and
the $29$-th data in the Iris versicolor for the test one,
\bea
\mathrm{Training ~data}~ &:& \{((0, 1), -1), ((0.796, 0.607), 1)\}, \nonumber \\
\mathrm{Test~data} &:&((-0.557, 0.83), -1) \nonumber
\eea
For the latter, we consider the images of $3$ and $6$ for binary classification. For convenience, we rescale each image from $28 \times 28$ pixels to $4 \times 4$ pixels  \cite{https://doi.org/10.48550/arxiv.1802.06002}. In both cases, we use a classical optimizer COBYLA to train the parameters.

%Also, the value of pixels are normalized for the amplitude encoding as shown in Fig. (\ref{MNISTData}). \\


\setlength{\tabcolsep}{3pt}
\renewcommand{\arraystretch}{1.4}
\begin{table}[H]
\centering
\begin{tabular}{|c||cc|cc|}
\hline
\multirow{2}{*}{} & \multicolumn{2}{c|}{Iris Dataset}    & \multicolumn{2}{c|}{MNIST Dataset}    \\ \cline{2-5} 
                  & \multicolumn{1}{c|}{$V(\theta)=\mathbbm{I}$ } & \multicolumn{1}{c|}{$V(\theta)\neq \mathbbm{I}$ } & \multicolumn{1}{c|}{$V(\theta)=\mathbbm{I}$ } & \multicolumn{1}{c|}{$V(\theta) \neq\mathbbm{I}$ } \\ \hline \hline
		\multicolumn{1}{|c||}{Empirical Risk} & \multicolumn{1}{c|}{$-1.251$} & \multicolumn{1}{c|}{$-2.666$} & \multicolumn{1}{c|}{$-1.534$} & \multicolumn{1}{c|}{$-1.963$} \\ \hline
		\multicolumn{1}{|c||}{Classifier} & \multicolumn{1}{c|}{$-0.708$} & \multicolumn{1}{c|}{$-0.996$} & \multicolumn{1}{c|}{$0.121$} & \multicolumn{1}{c|}{$0.056$} \\ \hline
		\multicolumn{1}{|c||}{Probability} & \multicolumn{1}{c|}{$-$} & \multicolumn{1}{c|}{$0.475$} & \multicolumn{1}{c|}{$-$} & \multicolumn{1}{c|}{$0.647$} \\ \hline
		\multicolumn{1}{|c||}{Decision} & \multicolumn{1}{c|}{$-1$} & \multicolumn{1}{c|}{$-1$} & \multicolumn{1}{c|}{$1$} & \multicolumn{1}{c|}{$1$} \\ \hline
\end{tabular}
\caption{  A feature map for quantum data, denoted by $V(\theta)$, see Fig. \ref{fig:pqc}, is trained for binary classification with datasets from Iris and MNIST. The case that a feature is not applied to quantum data is denoted by $V(\theta)=\mathbbm{I}$. The figure of merit is the empirical risk in Eq. (\ref{eq:risk}), which is smaller whenever $V(\theta) $ is trained for both datasets. The success probabilities are maintained as $0.457$ and $0.647$.}
\end{table}

Finally, we reiterate that a {\it feature map for quantum data} presents a strategy for enhancing an ML algorithm for quantum states prepared according to a quantum feature map. It could be compared with the  {\it quantum embedding} of classical data shown in Ref. \cite{https://doi.org/10.48550/arxiv.2001.03622}, which formulates training the state preparation to improve ML algorithms with quantum states. It is worth mentioning that, with some probability, our results always improve a quantum feature map or the quantum embedding, since a unitary transformation is an instance of a Kraus operator. Further enhancements of an ML algorithm may be envisaged by combining both strategies, i.e., with optimal embedding followed by filtering operations: in Fig. \ref{fig:fmqd} both $U$ and $V(\theta)$ are trained. Hence, a feature map for quantum data is a versatile tool to enhance existing ML algorithms. 

In conclusion, we have established a general framework for manipulating quantum data and then shown its applications to an ML algorithm for binary classification. We have also developed a circuit construction for a feature map for quantum data. In particular, our results shed light on the opportunity of restructuring quantum data on a Hilbert space beyond the constraint of contractivity. With some probability, quantum states can be distributed in a quantum feature space such that they are better distinguishable. We have applied the results to supervised learning for binary classification and demonstrated enhancements in the empirical risk. Our results present a versatile tool readily applicable to improve existing quantum ML algorithms.

%Our results are readily applied to improve existing quantum ML algorithms.

\section{Acknowledgement}
This work is supported by National Research Foundation of Korea (NRF-2019M3E4A1080001) and the ITRC (Information Technology Research Center) Program (IITP-2022-2018-0-01402). %and Samsung Research Funding $\&$ Incubation Center of Samsung Electronics (Project No. SRFC-TF2003-01).

%\bibliography{reff}
 
 %merlin.mbs apsrev4-1.bst 2010-07-25 4.21a (PWD, AO, DPC) hacked
%Control: key (0)
%Control: author (8) initials jnrlst
%Control: editor formatted (1) identically to author
%Control: production of article title (-1) disabled
%Control: page (0) single
%Control: year (1) truncated
%Control: production of eprint (0) enabled
\begin{thebibliography}{30}%
\makeatletter
\providecommand \@ifxundefined [1]{%
 \@ifx{#1\undefined}
}%
\providecommand \@ifnum [1]{%
 \ifnum #1\expandafter \@firstoftwo
 \else \expandafter \@secondoftwo
 \fi
}%
\providecommand \@ifx [1]{%
 \ifx #1\expandafter \@firstoftwo
 \else \expandafter \@secondoftwo
 \fi
}%
\providecommand \natexlab [1]{#1}%
\providecommand \enquote  [1]{``#1''}%
\providecommand \bibnamefont  [1]{#1}%
\providecommand \bibfnamefont [1]{#1}%
\providecommand \citenamefont [1]{#1}%
\providecommand \href@noop [0]{\@secondoftwo}%
\providecommand \href [0]{\begingroup \@sanitize@url \@href}%
\providecommand \@href[1]{\@@startlink{#1}\@@href}%
\providecommand \@@href[1]{\endgroup#1\@@endlink}%
\providecommand \@sanitize@url [0]{\catcode `\\12\catcode `\$12\catcode
  `\&12\catcode `\#12\catcode `\^12\catcode `\_12\catcode `\%12\relax}%
\providecommand \@@startlink[1]{}%
\providecommand \@@endlink[0]{}%
\providecommand \url  [0]{\begingroup\@sanitize@url \@url }%
\providecommand \@url [1]{\endgroup\@href {#1}{\urlprefix }}%
\providecommand \urlprefix  [0]{URL }%
\providecommand \Eprint [0]{\href }%
\providecommand \doibase [0]{http://dx.doi.org/}%
\providecommand \selectlanguage [0]{\@gobble}%
\providecommand \bibinfo  [0]{\@secondoftwo}%
\providecommand \bibfield  [0]{\@secondoftwo}%
\providecommand \translation [1]{[#1]}%
\providecommand \BibitemOpen [0]{}%
\providecommand \bibitemStop [0]{}%
\providecommand \bibitemNoStop [0]{.\EOS\space}%
\providecommand \EOS [0]{\spacefactor3000\relax}%
\providecommand \BibitemShut  [1]{\csname bibitem#1\endcsname}%
\let\auto@bib@innerbib\@empty
%</preamble>
\bibitem [{\citenamefont {Biamonte}\ \emph {et~al.}(2017)\citenamefont
  {Biamonte}, \citenamefont {Wittek}, \citenamefont {Pancotti}, \citenamefont
  {Rebentrost}, \citenamefont {Wiebe},\ and\ \citenamefont
  {Lloyd}}]{Biamonte:2017aa}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {J.}~\bibnamefont
  {Biamonte}}, \bibinfo {author} {\bibfnamefont {P.}~\bibnamefont {Wittek}},
  \bibinfo {author} {\bibfnamefont {N.}~\bibnamefont {Pancotti}}, \bibinfo
  {author} {\bibfnamefont {P.}~\bibnamefont {Rebentrost}}, \bibinfo {author}
  {\bibfnamefont {N.}~\bibnamefont {Wiebe}}, \ and\ \bibinfo {author}
  {\bibfnamefont {S.}~\bibnamefont {Lloyd}},\ }\href {\doibase
  10.1038/nature23474} {\bibfield  {journal} {\bibinfo  {journal} {Nature}\
  }\textbf {\bibinfo {volume} {549}},\ \bibinfo {pages} {195} (\bibinfo {year}
  {2017})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Maria~Schuld}(2018)}]{schuldbook}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {F.~P.}\ \bibnamefont
  {Maria~Schuld}},\ }\href@noop {} {\emph {\bibinfo {title} {Supervised
  Learning with Quantum Computers}}}\ (\bibinfo  {publisher} {Springer Cham},\
  \bibinfo {year} {2018})\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Dunjko}\ \emph {et~al.}(2016)\citenamefont {Dunjko},
  \citenamefont {Taylor},\ and\ \citenamefont
  {Briegel}}]{PhysRevLett.117.130501}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {V.}~\bibnamefont
  {Dunjko}}, \bibinfo {author} {\bibfnamefont {J.~M.}\ \bibnamefont {Taylor}},
  \ and\ \bibinfo {author} {\bibfnamefont {H.~J.}\ \bibnamefont {Briegel}},\
  }\href {\doibase 10.1103/PhysRevLett.117.130501} {\bibfield  {journal}
  {\bibinfo  {journal} {Phys. Rev. Lett.}\ }\textbf {\bibinfo {volume} {117}},\
  \bibinfo {pages} {130501} (\bibinfo {year} {2016})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {von Neumann}(1932)}]{neubook}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {J.}~\bibnamefont {von
  Neumann}},\ }\href@noop {} {\emph {\bibinfo {title} {Mathematical Foundations
  of Quantum Mechanics}}}\ (\bibinfo  {publisher} {Princeton University
  Press},\ \bibinfo {year} {1932})\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Schuld}\ and\ \citenamefont
  {Killoran}(2019)}]{SK2019}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {M.}~\bibnamefont
  {Schuld}}\ and\ \bibinfo {author} {\bibfnamefont {N.}~\bibnamefont
  {Killoran}},\ }\href {\doibase 10.1103/PhysRevLett.122.040504} {\bibfield
  {journal} {\bibinfo  {journal} {Phys. Rev. Lett.}\ }\textbf {\bibinfo
  {volume} {122}},\ \bibinfo {pages} {040504} (\bibinfo {year}
  {2019})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont
  {Schuld}(2021)}]{https://doi.org/10.48550/arxiv.2101.11020}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {M.}~\bibnamefont
  {Schuld}},\ }\href@noop {} {\enquote {\bibinfo {title} {Supervised quantum
  machine learning models are kernel methods},}\ }\bibinfo {howpublished}
  {arXiv:2101.11020} (\bibinfo {year} {2021})\BibitemShut {NoStop}%
\bibitem [{\citenamefont {P{\'e}rez-Garc{\'\i}a}\ \emph
  {et~al.}(2006)\citenamefont {P{\'e}rez-Garc{\'\i}a}, \citenamefont {Wolf},
  \citenamefont {Petz},\ and\ \citenamefont {Ruskai}}]{Perez-Garcia:2006aa}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {D.}~\bibnamefont
  {P{\'e}rez-Garc{\'\i}a}}, \bibinfo {author} {\bibfnamefont {M.~M.}\
  \bibnamefont {Wolf}}, \bibinfo {author} {\bibfnamefont {D.}~\bibnamefont
  {Petz}}, \ and\ \bibinfo {author} {\bibfnamefont {M.~B.}\ \bibnamefont
  {Ruskai}},\ }\href {\doibase 10.1063/1.2218675} {\bibfield  {journal}
  {\bibinfo  {journal} {Journal of Mathematical Physics}\ }\textbf {\bibinfo
  {volume} {47}},\ \bibinfo {pages} {083506} (\bibinfo {year}
  {2006})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Gleason}(1957)}]{gleason}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {A.}~\bibnamefont
  {Gleason}},\ }\href@noop {} {\bibfield  {journal} {\bibinfo  {journal}
  {Indiana Univ. Math. J.}\ }\textbf {\bibinfo {volume} {6}},\ \bibinfo {pages}
  {885} (\bibinfo {year} {1957})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Hong}\ \emph {et~al.}(1987)\citenamefont {Hong},
  \citenamefont {Ou},\ and\ \citenamefont {Mandel}}]{PhysRevLett.59.2044}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {C.~K.}\ \bibnamefont
  {Hong}}, \bibinfo {author} {\bibfnamefont {Z.~Y.}\ \bibnamefont {Ou}}, \ and\
  \bibinfo {author} {\bibfnamefont {L.}~\bibnamefont {Mandel}},\ }\href
  {\doibase 10.1103/PhysRevLett.59.2044} {\bibfield  {journal} {\bibinfo
  {journal} {Phys. Rev. Lett.}\ }\textbf {\bibinfo {volume} {59}},\ \bibinfo
  {pages} {2044} (\bibinfo {year} {1987})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Horodecki}\ and\ \citenamefont
  {Ekert}(2002)}]{PhysRevLett.89.127902}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {P.}~\bibnamefont
  {Horodecki}}\ and\ \bibinfo {author} {\bibfnamefont {A.}~\bibnamefont
  {Ekert}},\ }\href {\doibase 10.1103/PhysRevLett.89.127902} {\bibfield
  {journal} {\bibinfo  {journal} {Phys. Rev. Lett.}\ }\textbf {\bibinfo
  {volume} {89}},\ \bibinfo {pages} {127902} (\bibinfo {year}
  {2002})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Helstrom}(1969)}]{Helstrom:1969aa}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {C.~W.}\ \bibnamefont
  {Helstrom}},\ }\href {\doibase 10.1007/BF01007479} {\bibfield  {journal}
  {\bibinfo  {journal} {Journal of Statistical Physics}\ }\textbf {\bibinfo
  {volume} {1}},\ \bibinfo {pages} {231} (\bibinfo {year} {1969})}\BibitemShut
  {NoStop}%
\bibitem [{\citenamefont {Ivanovic}(1987)}]{Ivanovic:1987aa}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {I.~D.}\ \bibnamefont
  {Ivanovic}},\ }\href {\doibase https://doi.org/10.1016/0375-9601(87)90222-2}
  {\bibfield  {journal} {\bibinfo  {journal} {Physics Letters A}\ }\textbf
  {\bibinfo {volume} {123}},\ \bibinfo {pages} {257} (\bibinfo {year}
  {1987})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Dieks}(1988)}]{Dieks:1988aa}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {D.}~\bibnamefont
  {Dieks}},\ }\href {\doibase https://doi.org/10.1016/0375-9601(88)90840-7}
  {\bibfield  {journal} {\bibinfo  {journal} {Physics Letters A}\ }\textbf
  {\bibinfo {volume} {126}},\ \bibinfo {pages} {303} (\bibinfo {year}
  {1988})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Peres}(1988)}]{Peres:1988aa}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {A.}~\bibnamefont
  {Peres}},\ }\href {\doibase https://doi.org/10.1016/0375-9601(88)91034-1}
  {\bibfield  {journal} {\bibinfo  {journal} {Physics Letters A}\ }\textbf
  {\bibinfo {volume} {128}},\ \bibinfo {pages} {19} (\bibinfo {year}
  {1988})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Bergou}(2007)}]{Bergou:2007aa}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {J.~A.}\ \bibnamefont
  {Bergou}},\ }\href {\doibase 10.1088/1742-6596/84/1/012001} {\bibfield
  {journal} {\bibinfo  {journal} {Journal of Physics: Conference Series}\
  }\textbf {\bibinfo {volume} {84}},\ \bibinfo {pages} {012001} (\bibinfo
  {year} {2007})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Bae}\ and\ \citenamefont {Kwek}(2015)}]{Bae_2015}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {J.}~\bibnamefont
  {Bae}}\ and\ \bibinfo {author} {\bibfnamefont {L.-C.}\ \bibnamefont {Kwek}},\
  }\href {\doibase 10.1088/1751-8113/48/8/083001} {\bibfield  {journal}
  {\bibinfo  {journal} {Journal of Physics A: Mathematical and Theoretical}\
  }\textbf {\bibinfo {volume} {48}},\ \bibinfo {pages} {083001} (\bibinfo
  {year} {2015})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Croke}\ \emph {et~al.}(2008)\citenamefont {Croke},
  \citenamefont {Andersson},\ and\ \citenamefont
  {Barnett}}]{PhysRevA.77.012113}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {S.}~\bibnamefont
  {Croke}}, \bibinfo {author} {\bibfnamefont {E.}~\bibnamefont {Andersson}}, \
  and\ \bibinfo {author} {\bibfnamefont {S.~M.}\ \bibnamefont {Barnett}},\
  }\href {\doibase 10.1103/PhysRevA.77.012113} {\bibfield  {journal} {\bibinfo
  {journal} {Phys. Rev. A}\ }\textbf {\bibinfo {volume} {77}},\ \bibinfo
  {pages} {012113} (\bibinfo {year} {2008})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Bae}\ \emph {et~al.}(2011)\citenamefont {Bae},
  \citenamefont {Hwang},\ and\ \citenamefont {Han}}]{PhysRevLett.107.170403}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {J.}~\bibnamefont
  {Bae}}, \bibinfo {author} {\bibfnamefont {W.-Y.}\ \bibnamefont {Hwang}}, \
  and\ \bibinfo {author} {\bibfnamefont {Y.-D.}\ \bibnamefont {Han}},\ }\href
  {\doibase 10.1103/PhysRevLett.107.170403} {\bibfield  {journal} {\bibinfo
  {journal} {Phys. Rev. Lett.}\ }\textbf {\bibinfo {volume} {107}},\ \bibinfo
  {pages} {170403} (\bibinfo {year} {2011})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Hwang}\ and\ \citenamefont
  {Bae}(2010)}]{doi:10.1063/1.3298647}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {W.-Y.}\ \bibnamefont
  {Hwang}}\ and\ \bibinfo {author} {\bibfnamefont {J.}~\bibnamefont {Bae}},\
  }\href {\doibase 10.1063/1.3298647} {\bibfield  {journal} {\bibinfo
  {journal} {Journal of Mathematical Physics}\ }\textbf {\bibinfo {volume}
  {51}},\ \bibinfo {pages} {022202} (\bibinfo {year} {2010})},\ \Eprint
  {http://arxiv.org/abs/https://doi.org/10.1063/1.3298647}
  {https://doi.org/10.1063/1.3298647} \BibitemShut {NoStop}%
\bibitem [{\citenamefont {Jozsa}\ and\ \citenamefont
  {Schlienz}(2000)}]{PhysRevA.62.012301}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {R.}~\bibnamefont
  {Jozsa}}\ and\ \bibinfo {author} {\bibfnamefont {J.}~\bibnamefont
  {Schlienz}},\ }\href {\doibase 10.1103/PhysRevA.62.012301} {\bibfield
  {journal} {\bibinfo  {journal} {Phys. Rev. A}\ }\textbf {\bibinfo {volume}
  {62}},\ \bibinfo {pages} {012301} (\bibinfo {year} {2000})}\BibitemShut
  {NoStop}%
\bibitem [{\citenamefont {Bae}(2013)}]{Bae_2013}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {J.}~\bibnamefont
  {Bae}},\ }\href {\doibase 10.1088/1367-2630/15/7/073037} {\bibfield
  {journal} {\bibinfo  {journal} {New Journal of Physics}\ }\textbf {\bibinfo
  {volume} {15}},\ \bibinfo {pages} {073037} (\bibinfo {year}
  {2013})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Verstraete}\ \emph {et~al.}(2001)\citenamefont
  {Verstraete}, \citenamefont {Dehaene},\ and\ \citenamefont
  {DeMoor}}]{PhysRevA.64.010101}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {F.}~\bibnamefont
  {Verstraete}}, \bibinfo {author} {\bibfnamefont {J.}~\bibnamefont {Dehaene}},
  \ and\ \bibinfo {author} {\bibfnamefont {B.}~\bibnamefont {DeMoor}},\ }\href
  {\doibase 10.1103/PhysRevA.64.010101} {\bibfield  {journal} {\bibinfo
  {journal} {Phys. Rev. A}\ }\textbf {\bibinfo {volume} {64}},\ \bibinfo
  {pages} {010101} (\bibinfo {year} {2001})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Verstraete}\ and\ \citenamefont
  {Wolf}(2002)}]{PhysRevLett.89.170401}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {F.}~\bibnamefont
  {Verstraete}}\ and\ \bibinfo {author} {\bibfnamefont {M.~M.}\ \bibnamefont
  {Wolf}},\ }\href {\doibase 10.1103/PhysRevLett.89.170401} {\bibfield
  {journal} {\bibinfo  {journal} {Phys. Rev. Lett.}\ }\textbf {\bibinfo
  {volume} {89}},\ \bibinfo {pages} {170401} (\bibinfo {year}
  {2002})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Bennett}\ \emph {et~al.}(1996)\citenamefont
  {Bennett}, \citenamefont {Brassard}, \citenamefont {Popescu}, \citenamefont
  {Schumacher}, \citenamefont {Smolin},\ and\ \citenamefont
  {Wootters}}]{PhysRevLett.76.722}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {C.~H.}\ \bibnamefont
  {Bennett}}, \bibinfo {author} {\bibfnamefont {G.}~\bibnamefont {Brassard}},
  \bibinfo {author} {\bibfnamefont {S.}~\bibnamefont {Popescu}}, \bibinfo
  {author} {\bibfnamefont {B.}~\bibnamefont {Schumacher}}, \bibinfo {author}
  {\bibfnamefont {J.~A.}\ \bibnamefont {Smolin}}, \ and\ \bibinfo {author}
  {\bibfnamefont {W.~K.}\ \bibnamefont {Wootters}},\ }\href {\doibase
  10.1103/PhysRevLett.76.722} {\bibfield  {journal} {\bibinfo  {journal} {Phys.
  Rev. Lett.}\ }\textbf {\bibinfo {volume} {76}},\ \bibinfo {pages} {722}
  (\bibinfo {year} {1996})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Deutsch}\ \emph {et~al.}(1996)\citenamefont
  {Deutsch}, \citenamefont {Ekert}, \citenamefont {Jozsa}, \citenamefont
  {Macchiavello}, \citenamefont {Popescu},\ and\ \citenamefont
  {Sanpera}}]{PhysRevLett.77.2818}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {D.}~\bibnamefont
  {Deutsch}}, \bibinfo {author} {\bibfnamefont {A.}~\bibnamefont {Ekert}},
  \bibinfo {author} {\bibfnamefont {R.}~\bibnamefont {Jozsa}}, \bibinfo
  {author} {\bibfnamefont {C.}~\bibnamefont {Macchiavello}}, \bibinfo {author}
  {\bibfnamefont {S.}~\bibnamefont {Popescu}}, \ and\ \bibinfo {author}
  {\bibfnamefont {A.}~\bibnamefont {Sanpera}},\ }\href {\doibase
  10.1103/PhysRevLett.77.2818} {\bibfield  {journal} {\bibinfo  {journal}
  {Phys. Rev. Lett.}\ }\textbf {\bibinfo {volume} {77}},\ \bibinfo {pages}
  {2818} (\bibinfo {year} {1996})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Popescu}(1995)}]{PhysRevLett.74.2619}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {S.}~\bibnamefont
  {Popescu}},\ }\href {\doibase 10.1103/PhysRevLett.74.2619} {\bibfield
  {journal} {\bibinfo  {journal} {Phys. Rev. Lett.}\ }\textbf {\bibinfo
  {volume} {74}},\ \bibinfo {pages} {2619} (\bibinfo {year}
  {1995})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Lim}\ \emph {et~al.}(2011)\citenamefont {Lim},
  \citenamefont {Kim}, \citenamefont {Ra}, \citenamefont {Bae},\ and\
  \citenamefont {Kim}}]{PhysRevLett.107.160401}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {H.-T.}\ \bibnamefont
  {Lim}}, \bibinfo {author} {\bibfnamefont {Y.-S.}\ \bibnamefont {Kim}},
  \bibinfo {author} {\bibfnamefont {Y.-S.}\ \bibnamefont {Ra}}, \bibinfo
  {author} {\bibfnamefont {J.}~\bibnamefont {Bae}}, \ and\ \bibinfo {author}
  {\bibfnamefont {Y.-H.}\ \bibnamefont {Kim}},\ }\href {\doibase
  10.1103/PhysRevLett.107.160401} {\bibfield  {journal} {\bibinfo  {journal}
  {Phys. Rev. Lett.}\ }\textbf {\bibinfo {volume} {107}},\ \bibinfo {pages}
  {160401} (\bibinfo {year} {2011})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Schuld}\ \emph {et~al.}(2017)\citenamefont {Schuld},
  \citenamefont {Fingerhuth},\ and\ \citenamefont {Petruccione}}]{Schuld_2017}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {M.}~\bibnamefont
  {Schuld}}, \bibinfo {author} {\bibfnamefont {M.}~\bibnamefont {Fingerhuth}},
  \ and\ \bibinfo {author} {\bibfnamefont {F.}~\bibnamefont {Petruccione}},\
  }\href {\doibase 10.1209/0295-5075/119/60002} {\bibfield  {journal} {\bibinfo
   {journal} {Europhysics Letters}\ }\textbf {\bibinfo {volume} {119}},\
  \bibinfo {pages} {60002} (\bibinfo {year} {2017})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Lloyd}\ \emph {et~al.}(2021)\citenamefont {Lloyd},
  \citenamefont {Schuld}, \citenamefont {Ijaz}, \citenamefont {Izaac},\ and\
  \citenamefont {Killoran}}]{https://doi.org/10.48550/arxiv.2001.03622}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {S.}~\bibnamefont
  {Lloyd}}, \bibinfo {author} {\bibfnamefont {M.}~\bibnamefont {Schuld}},
  \bibinfo {author} {\bibfnamefont {A.}~\bibnamefont {Ijaz}}, \bibinfo {author}
  {\bibfnamefont {J.}~\bibnamefont {Izaac}}, \ and\ \bibinfo {author}
  {\bibfnamefont {N.}~\bibnamefont {Killoran}},\ }\href@noop {} {\enquote
  {\bibinfo {title} {Quantum embeddings for machine learning},}\ }\bibinfo
  {howpublished} {arXiv:2001.03622} (\bibinfo {year} {2021})\BibitemShut
  {NoStop}%
\bibitem [{\citenamefont {Farhi}\ and\ \citenamefont
  {Neven}(2018)}]{https://doi.org/10.48550/arxiv.1802.06002}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {E.}~\bibnamefont
  {Farhi}}\ and\ \bibinfo {author} {\bibfnamefont {H.}~\bibnamefont {Neven}},\
  }\href {\doibase 10.48550/ARXIV.1802.06002} {\enquote {\bibinfo {title}
  {Classification with quantum neural networks on near term processors},}\
  }\bibinfo {howpublished} {arXiv:1802.06002} (\bibinfo {year}
  {2018})\BibitemShut {NoStop}%
\end{thebibliography}%

\section{Appendix :  MNIST data }



The MNIST dataset contains pixelized $28 \times 28$ hand-writting images of numbers from $0$ to $9$. We rescale the  $28 \times 28$ images to $4 \times 4$ to encode input data into quantum states \cite{https://doi.org/10.48550/arxiv.1802.06002}. A pixel data with a value $0$ to $255$ are normalized such that they are mapped to amplitudes of a quantum state. We pick numbers $3$ and $6$. Then, the training dataset
\bea
\mathcal{D} = \{ (x_1, y_1), \cdots, (x_M, y_M) : x_m\in[0, 1]^{16}, y_m=\pm 1 \} \nonumber
\eea
are prepared by a quantum state $|\psi_x\rangle = \sum_{i=0}^{15} x_i |i\rangle$. %The quantum circuit that prepares the amplitude encoded quantum state requires multi-fold controlled rotation $R_y$ gates where a rotation angles are defined by the input data point $x$.

%The structure of feature map for quantum data depends on the dimension of input data. 

Unlike Iris dataset, a parameterized quantum circuit for the rescaled MNIST dataset requires $5$ qubits. We use single quantum layer, which means there are $10$ parameters to optimize and $10$ CNOT gates. COBYLA, a classical optimizer for the training, is used. 


\begin{figure}[h]
	\begin{center}
		\includegraphics[angle=0, width=0.49 \textwidth]{MNISTData}
		\caption{ (a) The original images of numbers $3$ and $6$ have $28 \times 28$ pixels. Each pixel has a value between $0$ to $255$. (b) The original images are rescaled to $4 \times 4$ pixels and then normalized. Thus, $16$ amplitudes are generated and used to prepare quantum states. } \label{MNISTData}
	\end{center}	
\end{figure}



\end{document}





\begin{figure*}[t]
	\begin{center}
		\includegraphics[angle=0, width=1.0 \textwidth]{fmqCirc}
		\caption{ The quantum circuit for the fidelity function in binary classification problem is shown. (a) The unitary matrix $U_{\mathrm{SI}}$ that depends on the training dataset prepares the initial state in Eq. (\ref{eq:InitState}). Then, the post-measure process and the SWAP test leads the fidelity function $f_{\mathrm{fid}}(x)$. The label of test data $x$ is decided by sign of the fidelity function: $y(x)  = \mathrm{sgn}(f_{\mathrm{fid}}(x))$. (b) It has same structure with (a), except data alteration step. The same $V(\theta)$ is applied to training dataset and test data. This quantum circuit results the fidelity function $\widetilde{f}_{\mathrm{fid}}(x)$ that minimizes the cost function $\widetilde{C}(\theta)$. (c) The PQC is prepared as full entanglement setting with $R_y$ and $R_z$ gates. The number of parameters $\theta$ depends on the dimension $N$ of input data $x$ and the number of PQC layer $n$.  } \label{fmqCirc}
	\end{center}	
\end{figure*}
