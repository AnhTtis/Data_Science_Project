\section{Method} \label{sec:method}

Motivated by the results from \cref{sec:motivation}, we adopt a conditional diffusion model for the task of inverting a face recognition (FR) model.
This results in a minimal formulation of the inverse problem compared to other methods that require complicated supervision~\cite{vec2face} or regularization~\cite{DBLP:journals/corr/ZhmoginovS16} signals.
The overall architecture of our method, named \underline{id}entity \underline{d}enoising \underline{d}iffusion \underline{p}robabilistic \underline{m}odel (ID3PM), is visualized in \cref{fig:architecture}.

\begin{figure*}[htbp]
    \centering
    \includegraphics[trim = 14mm 23mm 69mm 84mm, clip, width=0.98\linewidth]{images/architecture}
    \caption{Method architecture. Given an image of a source identity, the identity conditioning module first extracts the ID vector with a black-box, pre-trained face recognition network. This is projected with a fully connected layer and added to the time step embedding which are both injected into the residual blocks of a diffusion model. Starting with Gaussian noise $\mb{x}_T$, the diffusion model iteratively denoises the image to finally obtain the output image $\mb{x}_0$ in $64 \times 64$ resolution after $T$ diffusion time steps. Lastly, the image is upsampled to a resolution of $256 \times 256$ using an unconditional super-resolution model, which is also a diffusion model. The optional attribute conditioning module helps disentangle identity-specific from identity-agnostic features and allows intuitive control over attributes such as the pose. Note that the gray components are not learned during training.}
    \label{fig:architecture}
\end{figure*}

\subsection{Diffusion model formulation}  \label{sec:diff_form}

We build up on the diffusion model proposed by Dhariwal and Nichol~\cite{ddpm3}. Given a sample $\mb{x}_0$ from the image distribution, a sequence $\mb{x}_1, \mb{x}_2,..., \mb{x}_T$ of noisy images is produced by progressively adding Gaussian noise according to a variance schedule at each time step. At the final time step (say $T=1000$), $\mb{x}_T$ is assumed to be pure Gaussian noise: $\mathcal{N}(0,\mathbf{I})$.
A neural network is then trained to reverse this diffusion process. In each training iteration, an image $\mb{x}_0$ from the data set, and a time step $t \in [1, T]$ are sampled, and the corrupted images $\mb{x}_{t-1}$ and $\mb{x}_t$ are produced. The model predicts $\mb{x}_{t-1}$ (or rather a quantity from which $\mb{x}_{t-1}$ can be obtained) given the noisy image $\mb{x}_t$ and the time step $t$. 
For sampling a new image $\mb{x}_0$, we sample $\mb{x}_T \sim \mathcal{N}(0,\mathbf{I})$ and iteratively denoise it, producing a sequence $\mb{x}_T, \mb{x}_{T-1}, \ldots, \mb{x}_1, \mb{x}_0$. The final image, $\mb{x}_0$, should resemble the training data.

As \cite{ddpm3}, we assume that we can model $p_{\bm{\theta}}(\mb{x}_{t-1} | \mb{x}_t)$ as a Gaussian $\mathcal{N}(\mb{x}_{t-1}; \mb{\mu}_{\bm{\theta}}(\mb{x}_t,t), \bm{\Sigma}_{\bm{\theta}}(\mb{x}_t,t))$ whose mean $\mb{\mu}_{\bm{\theta}}(\mb{x}_t,t)$ can be calculated as a function of $\bm{\epsilon}_{\bm{\theta}}(\mb{x}_t,t)$, the noise that corresponds to the difference between $\mb{x}_t$ and $\mb{x}_{t-1}$. We extend this by conditioning on the ID vector $\mb{y}$ and thus predict $\bm{\epsilon}_{\bm{\theta}}(\mb{x}_t,\mb{y},t)$.
%
Extending \cite{ddpm2} to the conditional case, we elect to predict the noise $\bm{\epsilon}_{\bm{\theta}}(\mb{x}_t,\mb{y},t)$ and the variance $\bm{\Sigma}_{\bm{\theta}}(\mb{x}_t,\mb{y},t)$ from the image $\mb{x}_t$, the ID vector $\mb{y}$, and the time step $t$, using the objective
\begin{equation}
    \mathcal{L}_{\text{simple}} = \mathbb{E}_{t,\mb{x}_0,\mb{y},\bm{\epsilon}}[||\bm{\epsilon} - \bm{\epsilon}_{\bm{\theta}}(\mb{x}_t,\mb{y},t)||^2].
\end{equation}
For more details, refer to the diffusion model works~\cite{ddpm1, ddpm2, ddpm3}. Note that this objective is identical to the one theoretically derived in~\eqref{eq:err_loss}.  While some recent work has considered the application of diffusion models to inverse problems, they typically assume that $p(\mb{y}|\mb{x})$ is known \cite{song2020score}, while we make no such assumption.

Following Ramesh \etal~\cite{dalle2}, we adapt classifier-free guidance~\cite{classifierfree} by setting the ID vector to the $\mb{0}$-vector every $10$ batches during training. This effectively results in one model with two settings: conditional (ID vector) and unconditional ($0$-vector). During inference, we sample from both settings, and the model prediction $\hat{\bm{\epsilon}}_\theta$ becomes
\begin{equation}
    \hat{\bm{\epsilon}}_{\bm{\theta}}(\mb{x}_t, \mb{y}, t) = \bm{\epsilon}_{\bm{\theta}}(\mb{x}_t, \mb{0}, t) + s [\bm{\epsilon}_{\bm{\theta}}(\mb{x}_t, \mb{y}, t) - \bm{\epsilon}_{\bm{\theta}}(\mb{x}_t, \mb{0}, t)],
\end{equation}
where $s \geq 1$ is the guidance scale. Higher guidance scales cause the generation process to consider the identity conditioning more, effectively by re-weighting the balance of the conditional and unconditional scores.

\subsection{Architecture}

The architecture of the model is a U-net~\cite{unet} that takes the image $\mb{x}_t$, the ID vector $\mb{y}$, and the time step $t$ as input. The U-net architecture is adapted from \cite{ddpm3} and is described in detail in \cref{sec:add_implementation_details}.
To condition the diffusion model on the identity, we add an identity embedding to the residual connections of the ResNet blocks, as commonly done for class embeddings~\cite{ddpm3} and the CLIP~\cite{clip} embedding in text-to-image generation methods~\cite{dalle2, imagen}. The identity embedding is obtained by projecting the ID vector through a learnable fully connected layer such that it has the same size as the time step embedding and can be added to it.

\subsection{Controllability}\label{sec:method_controllability}

Due to its robustness and ability to pick a mode by setting the random seed during image generation, our method permits smooth interpolations and analyses in the ID vector latent space unlike other works that invert FR models. For example, we can smoothly interpolate between different identities as visualized in \cref{fig:teaser_small}. Furthermore, we can find meaningful directions in the latent spaces. Since the directions extracted automatically using principal component analysis (PCA) are generally difficult to interpret beyond the first dimension (see \cref{sec:pca}), we calculate custom directions using publicly available metadata~\cite{ffhq_metadata} for the FFHQ data set. For binary features (\eg glasses), we define the custom direction vector as the difference between the mean ID vectors of the two groups. For continuous features (\eg age), we map to the binary case by considering ID vectors with feature values below the $10$th percentile and values above the $90$th percentile for the two groups respectively. 
Examples of traveling along meaningful ID vector directions can be seen in \cref{fig:teaser_small}.

To better disentangle identity and non-identity information and obtain additional interpretable control, we can optionally extend our method by also conditioning the diffusion model on an attribute vector as done for the ID vector. We form several sets of attributes grouped by how much they can be considered as part of a person's identity (see \cref{table:attributes}). Set 1 only contains the emotion and head pose. Set 2 additionally contains glasses, makeup, and occlusions. Lastly, set 3 additionally contains age, facial hair, hair, and gender. As our method works with black-box vectors, it could be extended even more by simply adding more conditioning vectors.

\subsection{Implementation details} \label{sec:implementation_details}

As data set, we use FFHQ~\cite{stylegan} and split it into $69000$ images for training and $1000$ images for testing. Since we can only show images of individuals with written consent as explained in \cref{sec:ethics}, we use a proprietary data set of faces for the qualitative results in this paper. To condition our model, we use ID vectors from Tim Esler's PyTorch FaceNet implementation~\cite{facenet_pytorch} or the default InsightFace method~\cite{insightface}. To evaluate the generated images and thereby match the verification accuracy on real images shown in Vec2Face~\cite{vec2face} as closely as possible, we use the official PyTorch ArcFace implementation~\cite{arcface_torch} and David Sandberg's TensorFlow FaceNet implementation~\cite{facenet_tf}. A detailed description of the remaining implementation details and ID vectors is in \cref{sec:add_implementation_details}.
