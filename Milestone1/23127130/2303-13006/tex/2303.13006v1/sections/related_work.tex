\section{Related work} \label{sec:related_work}

\subsection{Face recognition}

While early deep learning works such as DeepFace~\cite{deepface} and VGG-Face~\cite{vggface} treated face recognition as a classification problem, with each class referring to one identity, FaceNet~\cite{facenet} introduced a new distance-based loss function: the triplet loss. 
The trend then shifted towards margin-based softmax methods that incorporate a margin penalty and perform sample-to-class comparisons rather than sample-to-sample comparisons~\cite{margin_based_1, margin_based_2, margin_based_3, arcface}. More recently, some FR methods tackle specific challenges such as robustness to different quality levels~\cite{adaface} and occlusions~\cite{mask_robustness, from}.

\subsection{Inversion of face recognition models}

Similar to gradient-based feature visualization techniques~\cite{feature_vis_1, feature_vis_2, feature_vis_3, deepdream}, Zhmoginov and Sandler~\cite{DBLP:journals/corr/ZhmoginovS16} perform gradient ascent steps using the gradient of a pre-trained FR model to generate images that approach the same ID vector as a target image. To avoid generating adversarial examples, strong image priors such as a total-variation loss and the use of a guiding image are necessary. Cole \etal~\cite{cole} transform the one-to-many task into a one-to-one task by mapping intermediate features of a FR model to frontal, neutral-expression images, enabling the use of an autoencoder architecture but requiring a difficult-to-obtain data set. Nitzan \etal~\cite{latent_space_mapping} map the identity features and attributes of images into the style space of a pre-trained StyleGAN~\cite{stylegan} to produce compelling results. However, their method struggles to encode real images since it is trained exclusively with images generated by StyleGAN. Furthermore, all of the above methods require white-box access to (the gradient of) a FR model, which is not always available in practice.

Many black-box methods view the problem from a security point-of-view, focusing on generating images that deceive a FR model rather than generating realistic faces with similar visual features. Early attempts using linear~\cite{mohanty2007scores} or radial basis function models~\cite{mignon2013reconstructing} lacked generative capacity to produce realistic images. NbNet~\cite{nbnet} introduces a neighborly de-convolutional neural network that can generate images with a reasonable resemblance to a given image, but it has line artifacts and relies on a huge data set augmented with a GAN. On the contrary, Razzhigaev \etal~\cite{gaussian_sampling} propose a data-set-free method using Gaussian blobs to produce images with ID vectors close to the target, but their results lack realism and take very long to generate due to querying the FR model hundreds of thousand times per image. Yang \etal~\cite{background_knowledge_alignment} rely on background knowledge to invert a model and only produce blurry images in the black-box setting. Vec2Face~\cite{vec2face} uses a new bijection metric and knowledge distillation from a black-box FR model to produce realistic identity-preserving faces; however, it requires a large data set (Casia-WebFace~\cite{casiawebface}) that contains multiple images of the same identity during training. Vendrow and Vendrow~\cite{stylegan-search} take advantage of a pre-trained StyleGAN~\cite{stylegan} generator to produce face images. They perform a random search in the StyleGAN latent space to find face images with an ID vector close to the target. While their method generates highly realistic images, the search strategy takes very long and often lands in local minima, resulting in images with completely different identities.

\Cref{table:comp} compares attributes of state-of-the-art FR model inversion methods. Our method is the only one that does not suffer from any of the common shortcomings: It consistently generates diverse, realistic, identity-preserving images in the black-box setting, can be trained with easy-to-obtain data, and does not require access to the FR model during inference other than to obtain the ID vector of the target image.

\begin{table*}[htbp]
\centering
    \mytablesize{
    \begin{tabular}{ llllllll }
    \toprule
    \multirow{2}{*}{Method} & \multirow{2}{*}{Black-box} & \multicolumn{2}{c}{\multirow{2}{*}{\begin{tabular}[l]{@{}l@{}}Number of FR model \\ queries during inference \end{tabular}}} & \multirow{2}{*}{Training data set} & \multirow{2}{*}{Realistic} & \multirow{2}{*}{Mapping} \\
    \\
    \midrule
    Zhmoginov and Sandler~\cite{DBLP:journals/corr/ZhmoginovS16} & \cellcolor{red!10}No & \cellcolor{red!10} $\sim$ 1000 $^*$ & \cellcolor{green!10}1  $^*$ & \cellcolor{green!10}Any images & \cellcolor{red!10}No & \cellcolor{red!10}One-to-one\\
    Cole \etal~\cite{cole} & \cellcolor{red!10}No & \cellcolor{green!10}1 & \cellcolor{green!10} & \cellcolor{red!10}Frontalized images & \cellcolor{green!10}Yes & \cellcolor{red!10}One-to-one \\
    % Nitzan \etal~\cite{latent_space_mapping} & \cellcolor{red!10}No & \cellcolor{green!10}1 & \cellcolor{green!10} & \cellcolor{green!10}Any images & \cellcolor{green!10}Yes & \cellcolor{green!10}One-to-many$^1$ \\
    Nitzan \etal~\cite{latent_space_mapping} & \cellcolor{red!10}No & \cellcolor{green!10}1 & \cellcolor{green!10} & \cellcolor{green!10}Any images & \cellcolor{green!10}Yes & \cellcolor{green!10}One-to-many \\
    NbNet~\cite{nbnet} & \cellcolor{green!10}Yes & \cellcolor{green!10}1 & \cellcolor{green!10} & \cellcolor{red!10}Huge data set & \cellcolor{red!10}No & \cellcolor{red!10}One-to-one \\
    Gaussian sampling~\cite{gaussian_sampling} & \cellcolor{green!10}Yes & \cellcolor{red!10}240000 & \cellcolor{red!10} & \cellcolor{green!10}Data-set-free & \cellcolor{red!10}No & \cellcolor{green!10}One-to-many \\
    Yang \etal~\cite{background_knowledge_alignment} & \cellcolor{green!10}Yes & \cellcolor{green!10}1 & \cellcolor{green!10} & \cellcolor{green!10}Any images & \cellcolor{red!10}No & \cellcolor{red!10}One-to-one \\
    Vec2Face~\cite{vec2face} & \cellcolor{green!10}Yes & \cellcolor{green!10}1 & \cellcolor{green!10} & \cellcolor{red!10}Multiple images per identity & \cellcolor{green!10}Yes & \cellcolor{green!10}One-to-many \\
    StyleGAN search~\cite{stylegan-search} & \cellcolor{green!10}Yes & \cellcolor{red!10}400 & \cellcolor{red!10} & \cellcolor{green!10}Data-set-free & \cellcolor{green!10}Yes & \cellcolor{green!10}One-to-many \\
    \midrule
    ID3PM (Ours) & \cellcolor{green!10}Yes & \cellcolor{green!10}1 & \cellcolor{green!10} & \cellcolor{green!10}Any images & \cellcolor{green!10}Yes & \cellcolor{green!10}One-to-many\\
    \bottomrule
    \\
    \end{tabular}
    }
\caption{Comparison of state-of-the-art face recognition (FR) model inversion methods. Our method does not have any of the common shortcomings, producing diverse, realistic images from black-box ID vectors with few requirements for the training data set or accessibility of the FR model during inference. 
$^*$ The authors propose two methods: one taking hundreds or thousands of queries and the second one doing it in one shot. 
}
\label{table:comp}
\end{table*}
