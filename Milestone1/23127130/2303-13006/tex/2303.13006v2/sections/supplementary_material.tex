\section{Additional implementation details} \label{sec:add_implementation_details}


\paragraph{ID vectors} \Cref{table:id_vectors} lists the face recognition methods used in this work. Note that we use two implementations for ArcFace~\cite{arcface} and FaceNet~\cite{facenet}, one for training and the other one for evaluation in each case. The methods used for training were chosen to match those used in Gaussian sampling~\cite{gaussian_sampling} and StyleGAN search~\cite{stylegan-search} respectively. The methods used for evaluation were chosen to match the verification accuracy on real images as closely as possible to the values shown in Vec2Face~\cite{vec2face} to enable a fair comparison. For both evaluation methods, we extract the identity embeddings for each image as well as its horizontally flipped version and then calculate the angular distance of the concatenated identity embeddings after subtracting the mean embedding, similar to \cite{facenet_tf}. In order to avoid having the face detector stage of different face recognition vectors influence the qualitative results, we manually confirmed that all shown test images were properly aligned. 

\begin{table}[h!]
\centering
    \small{
    \begin{tabular}{lllll} 
    \toprule
    Method & Usage & Alignment & Implementation & Checkpoint \\ 
    \midrule
    AdaFace~\cite{adaface} & Training $^1$ & Provided MTCNN~\cite{mtcnn} & Official GitHub repository & ``adaface\_ir50\_ms1mv2.ckpt'' \\
    ArcFace~\cite{arcface, arcface_torch} & Evaluation & RetinaFace~\cite{retinaface} from~\cite{insightface} & Official GitHub repository & ``ms1mv3\_arcface\_r100\_fp16'' \\
    ArcFace~\cite{arcface, gaussian_sampling} & Training $^1$ & MTCNN~\cite{mtcnn} from \cite{facenet_pytorch} & From Razzhigaev \etal~\cite{gaussian_sampling} & ``torchtest.pt'' \\
    FaceNet~\cite{facenet, facenet_tf} & Evaluation & Provided MTCNN~\cite{mtcnn} & From David Sandberg~\cite{facenet_tf} & ``20180402-114759'' \\
    FaceNet~\cite{facenet, facenet_pytorch} & Training & Provided MTCNN~\cite{mtcnn} & From Tim Esler~\cite{facenet_pytorch} & ``20180402-114759'' \\
    FROM~\cite{from} & Training $^1$ & MTCNN~\cite{mtcnn} from 
    \cite{facenet_pytorch} & Official GitHub repository & ``model\_p5\_w1\_9938\_9470\_6503.pth.tar'' \\
    InsightFace~\cite{insightface} & Training & Provided RetinaFace~\cite{retinaface} & InsightFace repository~\cite{insightface} & ``buffalo\_l'' \\
    \bottomrule
    \end{tabular}
}
\caption{Overview over the considered face recognition methods. $^1$ Only used in supplementary material.}
\label{table:id_vectors}
\end{table}

\paragraph{Model} We use the official U-net~\cite{unet} implementation by Dhariwal and Nichol~\cite{ddpm3, ddpm3_repo} and their recommended hyperparameters, whenever applicable, for the main $64 \times 64$ ID-conditioned face generation model and the $64 \rightarrow 256$ super-resolution model as listed in \cref{table:hyperparameters}. The U-net architecture is divided into several levels, with each level composed of ResNet~\cite{resnet} blocks and down- or upsampling layers. The U-net also contains global attention layers at $32 \times 32$, $16 \times 16$, and $8 \times 8$ resolutions. The time step $t$ is passed through a sinusoidal position embedding layer, known from transformers~\cite{transformer}, and is then added to the residual connection of the ResNet blocks. The most important additions to the baseline model are the identity conditioning module (identity\_cond) and introducing classifier-free guidance (classifier\_free) by setting the conditioning vector to the $0$-vector\footnote{For attribute conditioning, the $-1$-vector is used since the $0$-vector is a valid attribute vector (\eg age $0$) and the $-1$-vector performed better empirically.} for $10\%$ of the training samples to obtain an unconditional and conditional setting with just one trained model.

\paragraph{Training} The training set is composed of images along with their corresponding (pre-computed) ID vectors. We train the $64 \times 64$ ID-conditioned face generation model for $100000$ batches and the $64 \times 64$ unconditional upsampling model for $50000$ batches, both with a batch size of $64$, learning rate of $10^{-4}$, and from scratch. We use the weights with an exponential moving average rate of $0.9999$ because it generally leads to better results. Training takes around two days on one NVIDIA RTX 3090 GPU.

\paragraph{Inference} All models are trained with $T=1000$ but respaced to $250$ time steps during inference for computational reasons with a negligible decrease in quality. We use a classifier-free guidance scale of $2$ unless otherwise stated. Furthermore, we fix the randomness seeds whenever comparing different methods to ensure a fair comparison. Inference (main model + super-resolution to $256 \times 256$ resolution) takes around $15$ seconds per image when using batches of $16$ images on one NVIDIA RTX 3090 GPU. The inference time can be drastically reduced by using fewer respacing steps at a slight decrease in quality, as shown in \cref{fig:ablation_time_steps}. For example, when using $10$ respacing steps, the inference time decreases to around $1$ second per image with a comparable identity fidelity and only slightly fewer details (especially in the background).

\begin{table}[h!]
\centering
    \small{
    \begin{tabular}{lrr} 
    \toprule
    & $64 \times 64$ main model & $64 \times 64 \rightarrow 256 \times 256$ super-resolution model \\ 
    \midrule
    \underline{Diffusion parameters} \\
    diffusion\_steps & $1000$ & $1000$ \\ 
    noise\_schedule & cosine & linear \\
    \\
    \underline{Model parameters} \\
    attention\_resolutions & $32, 16, 8$ & $32, 16, 8$ \\ 
    classifier\_free & True & False \\
    dropout & $0.1$ & $0$ \\ 
    identity\_cond & True & False \\
    learn\_sigma & True & True \\
    num\_channels & $192$ & $192$ \\ 
    num\_heads & $3$ & $4$ \\ 
    num\_res\_blocks & $3$ & $2$ \\ 
    resblock\_updown & True & True \\
    use\_fp16 & True & True \\
    use\_new\_attention\_order & True & False \\
    use\_scale\_shift\_norm & True & True \\
    \\
    \underline{Training parameters} \\
    batch\_size & $64$ & $64$ \\
    ema\_rate & $0.9999$ & $0.9999$ \\
    lr (learning rate) & $10^{-4}$ & $10^{-4}$ \\
    total\_steps (batches) & $100000$ & $50000$ \\
    \bottomrule
    \end{tabular}
}
\caption{Hyperparameters of our diffusion models. We use one diffusion model to generate $64 \times 64$ resolution images and one super-resolution diffusion model to increase the resolution to $256 \times 256$. All other parameters are named as in the baseline implementation (where applicable).}
\label{table:hyperparameters}
\end{table}

\clearpage

\begin{figure*}[htbp]
\centering
\begin{subfigure}{\textwidth}
\centering
    \addtolength{\tabcolsep}{-5pt}
    \small{
    \begin{tabular}{cc}
    \raisebox{1.5\height}{\includegraphics[width=0.095\textwidth]{images/original/Martina.jpg}} & 
    \adjincludegraphics[width=0.855\textwidth, trim={0 0 0 0}, clip]{images/num_steps/Martina_64_selected_steps.jpg} \\ 
    Time (s) & \begin{tabularx}{0.855\textwidth}{ *{9}{Y} } $0.2$ & $0.2$ & $0.3$ & $0.6$ & $0.9$ & $1.8$ & $4.1$ & $8.3$ & $17.0$ \end{tabularx} \\
    \\
    Steps & \begin{tabularx}{0.855\textwidth}{ *{9}{Y} } $3$ & $5$ & $10$ & $25$ & $50$ & $100$ & $250$ & $500$ & $1000$ \end{tabularx} \\
    \end{tabular}
    }
    \addtolength{\tabcolsep}{5pt}
    \caption{$64 \times 64$ main model}
    \vspace{5mm}
\end{subfigure}
\begin{subfigure}{\textwidth}
\centering
    \addtolength{\tabcolsep}{-5pt}
    \small{
    \begin{tabular}{cc}
    \raisebox{1.5\height}{\includegraphics[width=0.095\textwidth]{images/original/Martina.jpg}} & 
    \adjincludegraphics[width=0.855\textwidth, trim={0 0 0 0}, clip]{images/num_steps/Martina_256_selected_steps.jpg} \\ 
    Time (s) & \begin{tabularx}{0.855\textwidth}{ *{9}{Y} } $0.6$ & $0.6$ & $0.9$ & $1.8$ & $3.3$ & $6.3$ & $15.1$ & $30.1$ & $60.4$ \end{tabularx} \\
    \\
    Steps & \begin{tabularx}{0.855\textwidth}{ *{9}{Y} } $3$ & $5$ & $10$ & $25$ & $50$ & $100$ & $250$ & $500$ & $1000$ \end{tabularx} \\
    \end{tabular}
    }
    \addtolength{\tabcolsep}{5pt}
    \caption{$64 \times 64$ main model + $64 \times 64 \rightarrow 256 \times 256$ super-resolution model}
\end{subfigure}
\caption{Qualitative evaluation of the effect of the number of respacing steps. For each ID vector extracted from the image on the left, we generate images for four seeds with different number of respacing steps, with and without super-resolution. We also report the inference time per image (when using batches of 16 images) in seconds on one NVIDIA RTX 3090 GPU. Note that the time listed for the images with super-resolution also includes the time to generate the $64 \times 64$ images.}
\label{fig:ablation_time_steps}
\end{figure*}

\clearpage

\section{Additional comparisons to state-of-the-art methods}

\subsection{Qualitative results}

\Cref{fig:comp} shows additional results of the qualitative comparison with the state-of-the-art black-box methods, whose code is available, and demonstrates the superiority of our method both in terms of image quality and identity preservation. 


\begin{figure*}[htpb]
    \centering
    \addtolength{\tabcolsep}{-5pt}
    \small{
    \begin{tabular}{cccccc}
        \includegraphics[width=0.13\textwidth]{images/original/Sally.jpg} & 
        \includegraphics[width=0.13\textwidth]{images/qualitative_comp/sally/nbnet.jpg} & 
        \includegraphics[width=0.13\textwidth]{images/qualitative_comp/sally/bbox.jpg} & 
        \includegraphics[width=0.13\textwidth]{images/qualitative_comp/sally/stylegan_0.jpg} & 
        \includegraphics[width=0.13\textwidth]{images/qualitative_comp/sally/ours_facenet.jpg} & 
        \includegraphics[width=0.13\textwidth]{images/qualitative_comp/sally/ours_insightface.jpg} \\
        \\[-0.46cm]   
        \includegraphics[width=0.13\textwidth]{images/original/Hayko.jpg} & 
        \includegraphics[width=0.13\textwidth]{images/qualitative_comp/hayko/nbnet.jpg} & 
        \includegraphics[width=0.13\textwidth]{images/qualitative_comp/hayko/bbox.jpg} & 
        \includegraphics[width=0.13\textwidth]{images/qualitative_comp/hayko/stylegan_0.jpg} & 
        \includegraphics[width=0.13\textwidth]{images/qualitative_comp/hayko/ours_facenet.jpg} & 
        \includegraphics[width=0.13\textwidth]{images/qualitative_comp/hayko/ours_insightface.jpg} \\
        \\[-0.46cm]   
        \includegraphics[width=0.13\textwidth]{images/original/Derek.jpg} & 
        \includegraphics[width=0.13\textwidth]{images/qualitative_comp/derek/nbnet.jpg} & 
        \includegraphics[width=0.13\textwidth]{images/qualitative_comp/derek/bbox.jpg} & 
        \includegraphics[width=0.13\textwidth]{images/qualitative_comp/derek/stylegan_0.jpg} & 
        \includegraphics[width=0.13\textwidth]{images/qualitative_comp/derek/ours_facenet.jpg} & 
        \includegraphics[width=0.13\textwidth]{images/qualitative_comp/derek/ours_insightface.jpg} \\
        \\[-0.46cm]
        \includegraphics[width=0.13\textwidth]{images/original/Sandra.jpg} & 
        \includegraphics[width=0.13\textwidth]{images/qualitative_comp/sandra/nbnet.jpg} & 
        \includegraphics[width=0.13\textwidth]{images/qualitative_comp/sandra/bbox.jpg} & 
        \includegraphics[width=0.13\textwidth]{images/qualitative_comp/sandra/stylegan_0.jpg} & 
        \includegraphics[width=0.13\textwidth]{images/qualitative_comp/sandra/ours_facenet.jpg} & 
        \includegraphics[width=0.13\textwidth]{images/qualitative_comp/sandra/ours_insightface.jpg} \\
        \\[-0.46cm]
        \includegraphics[width=0.13\textwidth]{images/original/Aziz.jpg} & 
        \includegraphics[width=0.13\textwidth]{images/qualitative_comp/aziz/nbnet.jpg} & 
        \includegraphics[width=0.13\textwidth]{images/qualitative_comp/aziz/bbox.jpg} & 
        \includegraphics[width=0.13\textwidth]{images/qualitative_comp/aziz/stylegan_0.jpg} & 
        \includegraphics[width=0.13\textwidth]{images/qualitative_comp/aziz/ours_facenet.jpg} & 
        \includegraphics[width=0.13\textwidth]{images/qualitative_comp/aziz/ours_insightface.jpg} \\
        \\[-0.46cm]  
        \includegraphics[width=0.13\textwidth]{images/original/Lucas.jpg} & 
        \includegraphics[width=0.13\textwidth]{images/qualitative_comp/lucas/nbnet.jpg} & 
        \includegraphics[width=0.13\textwidth]{images/qualitative_comp/lucas/bbox.jpg} & 
        \includegraphics[width=0.13\textwidth]{images/qualitative_comp/lucas/stylegan_0.jpg} & 
        \includegraphics[width=0.13\textwidth]{images/qualitative_comp/lucas/ours_facenet.jpg} & 
        \includegraphics[width=0.13\textwidth]{images/qualitative_comp/lucas/ours_insightface.jpg} \\
        \\[-0.46cm]  
        \includegraphics[width=0.13\textwidth]{images/original/Xianyao.jpg} & 
        \includegraphics[width=0.13\textwidth]{images/qualitative_comp/xianyao/nbnet.jpg} & 
        \includegraphics[width=0.13\textwidth]{images/qualitative_comp/xianyao/bbox.jpg} & 
        \includegraphics[width=0.13\textwidth]{images/qualitative_comp/xianyao/stylegan_0.jpg} & 
        \includegraphics[width=0.13\textwidth]{images/qualitative_comp/xianyao/ours_facenet.jpg} & 
        \includegraphics[width=0.13\textwidth]{images/qualitative_comp/xianyao/ours_insightface.jpg} \\
        
        \multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Original \\ image \end{tabular}} & \multirow{2}{*}{NbNet~\cite{nbnet}} & \multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Gaussian \\ sampling~\cite{gaussian_sampling} \end{tabular}} & \multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}StyleGAN \\ search~\cite{stylegan-search} \end{tabular}} & \multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}ID3PM (Ours, \\ FaceNet~\cite{facenet, facenet_pytorch}) \end{tabular}} & \multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}ID3PM (Ours, \\ InsightFace~\cite{insightface}) \end{tabular}} \\
        \\
    \end{tabular}
    }
    \addtolength{\tabcolsep}{5pt}
    \caption{Qualitative evaluation with state-of-the-art methods (additional results). The generated images of our method look realistic and resemble the identity of the original image more closely than any of the other methods. }
    \label{fig:comp}
\end{figure*}

\clearpage

In the main paper, we quantitatively compare the diversity of our method with that of StyleGAN search~\cite{stylegan-search}, which is the only competing method that can produce realistic images at a high resolution. 
\Cref{fig:comp_stylegan} qualitatively confirms that our method produces similarly diverse images but with better identity preservation. For fairness reasons, we use our model trained with FaceNet~\cite{facenet, facenet_pytorch} ID vectors since StyleGAN search~\cite{stylegan-search} uses the same FaceNet implementation~\cite{facenet_pytorch}. For the first two identities, the  StyleGAN search~\cite{stylegan-search} algorithm finds images that share facial features with the original face; however, the identity does not resemble the original face very closely. For the third identity, the search strategy often fails completely by landing in local minima. 

\begin{figure*}[htpb]
    \centering
    \addtolength{\tabcolsep}{-5pt}
    \small{
    \begin{tabular}{ccc}
        \raisebox{1.5\height}{\includegraphics[width=0.075\textwidth]{images/original/Derek.jpg}} & 
        \includegraphics[width=0.3\textwidth]{images/qualitative_comp/derek/stylegan_16.jpg} & 
        \includegraphics[width=0.3\textwidth]{images/qualitative_comp/derek/ours_16_facenet.jpg} \\
        \\[-0.46cm]
        \raisebox{1.5\height}{\includegraphics[width=0.075\textwidth]{images/original/Sandra.jpg}} & 
        \includegraphics[width=0.3\textwidth]{images/qualitative_comp/sandra/stylegan_16.jpg} & 
        \includegraphics[width=0.3\textwidth]{images/qualitative_comp/sandra/ours_16_facenet.jpg} \\
        \\[-0.46cm]
        \raisebox{1.5\height}{\includegraphics[width=0.075\textwidth]{images/original/Yang.jpg}} & 
        \includegraphics[width=0.3\textwidth]{images/qualitative_comp/yang/stylegan_16.jpg} & 
        \includegraphics[width=0.3\textwidth]{images/qualitative_comp/yang/ours_16_facenet.jpg} \\
        
        Original image & StyleGAN search~\cite{stylegan-search} & ID3PM (Ours, FaceNet~\cite{facenet, facenet_pytorch}) \\
        \\
    \end{tabular}
    }
    \addtolength{\tabcolsep}{5pt}
    \caption{Qualitative evaluation with StyleGAN search~\cite{stylegan-search}. The generated images of our method resemble the identity of the original image closely and more consistently. Note that StyleGAN search~\cite{stylegan-search} often fails completely for the third identity, whereas our method trained with the same face recognition method (FaceNet~\cite{facenet, facenet_pytorch}) reproduces the identity well.}
    \label{fig:comp_stylegan}
\end{figure*}

\clearpage

\subsection{User study}

To accompany the qualitative results and since we cannot show images from public data sets without the individuals' written consents (as explained in the ethics section of the main paper), we performed a user study with two parts. For the first part, we took the first 10 positive pairs with unique identities according to the LFW~\cite{lfw} protocol (same protocol as used for the quantitative evaluation of the identity preservation in the main paper) to compare the following methods: NbNet~\cite{nbnet}, Gaussian sampling~\cite{gaussian_sampling}, StyleGAN search~\cite{stylegan-search}, ours with FaceNet~\cite{facenet, facenet_pytorch}, and ours with InsightFace~\cite{insightface}. 

Specifically, we instructed the users to:
\begin{itemize}[itemsep=-2pt,topsep=2pt]
    \item Rank the generated images from most \textbf{similar to the person of the input image} to least.
    \item Rank the generated images from most \textbf{realistic} to least.
\end{itemize}

The results in \Cref{table:user_study} (left) provide further evidence of our method outperforming competitor methods, with the exception of StyleGAN search, which achieves better average realism at the expense of identity preservation. The user study also supports our realism labels in the related work section of the main paper.

\begin{table}[h!]
\centering
    \small{
    \subfloat{
    \begin{tabular}{lcc}
    \toprule
    \multirow{2}{*}{Method} & \multicolumn{2}{c}{LFW} \\
    \cmidrule{2-3}
    & {ID $\downarrow$} & {Real $\downarrow$} \\
    \midrule
    NbNet~\cite{nbnet} & 3.52 & 4.06 \\
    Gaussian sampling~\cite{gaussian_sampling} & 4.83 & 4.90 \\
    StyleGAN search~\cite{stylegan-search} & 2.53 & \textbf{1.39} \\
    \midrule
    ID3PM (Ours, FaceNet~\cite{facenet, facenet_pytorch}) & \textbf{1.90} & 2.05 \\
    ID3PM (Ours, InsightFace~\cite{insightface}) & 2.22 & 2.61 \\
    \bottomrule
    \end{tabular}}
    \quad
    \subfloat{
    \begin{tabular}{lcc}
    \toprule
    \multirow{2}{*}{Method} & \multicolumn{2}{c}{Vec2Face images} \\
    \cmidrule{2-3}
    & {ID $\downarrow$} & {Real $\downarrow$} \\
    \midrule
    Vec2Face~\cite{vec2face} & 3.51 & 3.80 \\
    \midrule
    ID3PM (Ours, FaceNet~\cite{facenet, facenet_pytorch}) & 2.665& \textbf{1.52} \\
    ID3PM (Ours, InsightFace~\cite{insightface}) & 3.50 & 3.23 \\
    ID3PM (Ours, FaceNet~\cite{facenet, facenet_pytorch}, CASIA-WebFace~\cite{casiawebface}) & 2.87 & 3.10 \\
    ID3PM (Ours, InsightFace~\cite{insightface}, CASIA-WebFace~\cite{casiawebface}) & \textbf{2.46} & 3.36 \\
    \bottomrule
    \end{tabular}}}
\caption{User study. The listed scores are the mean ranks (1 - 5) for realism (Real) and identity preservation (ID) of the different methods on LFW~\cite{lfw} images (left) and Vec2Face~\cite{vec2face} images (right).}
\label{table:user_study}
\end{table}

For the second part, we took screenshots of the 14 input and result images from Fig.\ 4 of the Vec2Face~\cite{vec2face} paper to compare our method to Vec2Face despite their code not being available. We then computed ID vectors for these faces and generated one image per ID vector with each variation of our method with a fixed random seed and ran a similar user study as above, again with 25 users. We also trained versions of our method on CASIA-WebFace~\cite{casiawebface}\footnote{Not upscaled from $64 \times 64$ to $256 \times 256$ for time reasons.} to have the same training data as Vec2Face. The results in \Cref{table:user_study} show that all variations of our method beat Vec2Face despite the experimental setup favoring Vec2Face (\eg low-quality screenshots as input for our method and using Vec2Face authors' chosen examples).

\subsection{Fairness of comparisons}

Our comparisons are fair or to the benefit of competing methods, and no retraining of competing methods was necessary.
Gaussian sampling~\cite{gaussian_sampling} does not have a training data set. 
StyleGAN search~\cite{stylegan-search} uses a StyleGAN2~\cite{stylegan2} trained on all 70000 images\footnote{See \url{https://tinyurl.com/ffhq70k}.} of FFHQ~\cite{stylegan}, so it saw the 1k test images used in the main paper during training (unlike our method that was trained only on the first 69000 images). 
NbNet~\cite{nbnet} and Vec2Face~\cite{vec2face} will likely not work well when trained only on FFHQ. NbNet reports significantly worse results in their Tab. 4 and section 4.2.1 when not augmenting their data set (which is already orders of magnitudes larger than FFHQ) with millions of images. Vec2Face uses CASIA-WebFace~\cite{casiawebface}, which is $\sim 7$ times bigger than FFHQ, and needs class information during training. One can thus consider Vec2Face as white-box with a slightly worse face recognition model (trained with knowledge distillation). When training our method with CASIA-WebFace instead of FFHQ, we obtain similar results and also match or outperform Vec2Face as seen in the above user study. Also, for fairness, we used Vec2Face's protocol for the quantitative comparison of the identity preservation. 
Lastly, note that no images visualized in the paper were included in any method's training data.


\clearpage

\section{Controllability}

To the best of our knowledge, our method is the first black-box face recognition model inversion method that offers intuitive control over the generation process. The mechanisms described in the following section enable the generation of data sets with control over variation and diversity of the identities as well as their attributes.

\subsection{Guidance scale}

As described in the main paper, the guidance scale of the classifier-free guidance controls the trade-off between the fidelity in terms of identity preservation (higher guidance) and the diversity of the generated faces (lower guidance). \Cref{fig:qualitative_guidance_full} shows examples of the generated images for different guidance scales $s$ ranging from $1.0$ to $5.0$. To improve the performance for high guidance scales, we adopt dynamic thresholding from Imagen~\cite{imagen} with a threshold of $0.99$.

\begin{figure*}[htbp]
\centering
\begin{subfigure}{\textwidth}
\centering
    \addtolength{\tabcolsep}{-5pt}
    \small{
    \begin{tabular}{cc}
    \raisebox{1.5\height}{\includegraphics[width=0.095\textwidth]{images/original/Sandra.jpg}} & 
    \adjincludegraphics[width=0.855\textwidth, trim={0 {.5\height} 0 {0.25\height}}, clip]{images/guidance_scale/guidance_sandra_facenet.jpg} \\ 
    $s$ & \begin{tabularx}{0.855\textwidth}{ *{9}{Y} } $1.0$ & $1.5$ & $2.0$ & $2.5$ & $3.0$ & $3.5$ & $4.0$ & $4.5$ & $5.0$ \end{tabularx} \\
    \end{tabular}
    }
    \addtolength{\tabcolsep}{5pt}
    \caption{FaceNet~\cite{facenet, facenet_pytorch}}
    \vspace{5mm}
\end{subfigure}
\begin{subfigure}{\textwidth}
\centering
    \addtolength{\tabcolsep}{-5pt}
    \small{
    \begin{tabular}{cc}
    \raisebox{1.5\height}{\includegraphics[width=0.095\textwidth]{images/original/Sandra.jpg}} & 
    \adjincludegraphics[width=0.855\textwidth, trim={0 {.5\height} 0 {0.25\height}}, clip]{images/guidance_scale/guidance_sandra_insightface.jpg} \\
    $s$ & \begin{tabularx}{0.855\textwidth}{ *{9}{Y} } $1.0$ & $1.5$ & $2.0$ & $2.5$ & $3.0$ & $3.5$ & $4.0$ & $4.5$ & $5.0$ \end{tabularx} \\
    \end{tabular}
    }
    \addtolength{\tabcolsep}{5pt}
    \caption{InsightFace~\cite{insightface}}
\end{subfigure}
\caption{Qualitative evaluation of the effect of the guidance scale. For each ID vector extracted from the image on the left, we generate images for four seeds at guidance scales $s$ ranging from $1.0$ to $5.0$.}
\label{fig:qualitative_guidance_full}
\end{figure*}

\clearpage

In the main paper, we evaluate the diversity in terms of the pairwise pose, expression, and LPIPS~\cite{lpips} feature distances among generated images as well as their identity embedding distances. To further quantify the effect of the guidance, we select the first $10000$ images of the FFHQ data set~\cite{stylegan}, extract their ID vectors, and generate one image for each ID vector\footnote{Note that we use the generated images of size $64 \times 64$ (rather than the upsampled images) for computational reasons.}. These $10000$ images are then compared to the corresponding $10000$ original images in terms of their FID scores~\cite{fid} as well as precision and recall~\cite{recall_precision}. The results are shown in \cref{table:fid}. The precision score assesses to which extent the generated samples fall into the distribution of the the real images. Guidance scales in the range $s=1.5$ to $s=2.0$ raise the precision score, implying a higher image quality of the generated images. Even larger guidance scales lead to lower precision scores, which could be explained by the saturated colors observed in \cref{fig:qualitative_guidance_full}. The recall score measures how much of the original distribution is covered by the generated samples and corresponds to the diversity. As the guidance scale increases, recall decreases. Similarly, the FID score gets worse with higher guidance scales, demonstrating the decrease in diversity among the generated images.

\begin{table}[htbp]
\centering
\small{
\begin{tabular}{lcS[table-format=2.3]S[table-format=1.3]S[table-format=1.3]}
    \toprule
    Method ID vector & Guidance scale $s$ & {FID ($\downarrow$)} & {Precision ($\uparrow$)} & {Recall ($\uparrow$)} \\
    \midrule
    \multirow{5}{*}{ID3PM (Ours, FaceNet~\cite{facenet, facenet_pytorch})} & $1.0$ &  \textbf{8.014} & 0.768 & \textbf{0.498} \\
    & $1.5$ &  9.141 & 0.782 & 0.490 \\
    & $2.0$ & 10.434 & \textbf{0.783} & 0.476 \\
    & $2.5$ & 11.659 & 0.775 & 0.452 \\
    & $3.0$ & 12.806 & 0.771 & 0.441 \\
    \midrule
    \multirow{5}{*}{ID3PM (Ours, InsightFace~\cite{insightface})} & $1.0$ &  \textbf{6.786} & 0.774 & \textbf{0.517} \\
    & $1.5$ &  7.442 & \textbf{0.782} & 0.516 \\
    & $2.0$ &  8.497 & 0.771 & 0.508 \\
    & $2.5$ &  9.286 & 0.763 & 0.506 \\
    & $3.0$ & 10.119 & 0.746 & 0.488 \\
    \bottomrule
\end{tabular}
}
\caption{Quantitative evaluation of the effect of the guidance scale on image quality and diversity. The best performing setting for each ID vector is marked in bold.}
\label{table:fid}
\end{table}

Additionally, we perform the face verification experiment from the main paper on LFW~\cite{lfw}, AgeDB-30~\cite{agedb}, and CFP-FP~\cite{cfpfp} with guidance scales between $1.0$ and $3.0$ for our models trained using FaceNet~\cite{facenet, facenet_pytorch} and InsightFace~\cite{insightface} ID vectors. As seen in \cref{table:face_recog_scores_guidance}, the face verification accuracy generally increases with higher guidance scales but saturates eventually, confirming our qualitative findings that the guidance aids in the identity preservation. 

\begin{table*}[htbp]
\centering
    \small{
    \begin{tabular}{lclcclcclcc}
    \toprule
    \multirow{2}{*}{Method} & \multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Guidance\\ scale $s$ \end{tabular}} &  & \multicolumn{2}{c}{LFW} &  & \multicolumn{2}{c}{AgeDB-30} &  & \multicolumn{2}{c}{CFP-FP} \\
    \cmidrule{4-5} \cmidrule{7-8} \cmidrule{10-11}
    & & & {ArcFace $\uparrow$} & {FaceNet $\uparrow$} & & {ArcFace $\uparrow$} & {FaceNet $\uparrow$} & & {ArcFace $\uparrow$} & {FaceNet $\uparrow$} \\
    \midrule
    Real images & {-} & & 99.83\% & 99.65\% & & 98.23\% & 91.33\% & & 98.86\% & 96.43\% \\
    \midrule
    \multirow{5}{*}{ID3PM (Ours, FaceNet~\cite{facenet, facenet_pytorch})} & 1.0 & & 95.60\% & 98.62\% & & 84.07\% & 86.20\% & & 91.83\% & 94.30\% \\
    & 1.5 & & 97.08\% & 99.00\% & & 87.55\% & 87.90\% & & 94.20\% & 95.04\% \\
    & 2.0 & & 97.65\% & 98.98\% & & 88.22\% & 88.00\% & & 94.47\% & \textbf{95.23\%} \\
    & 2.5 & & 97.92\% & 98.92\% & & \textbf{88.75\%} & \textbf{88.47\%} & & \textbf{94.61\%} & 95.19\% \\
    & 3.0 & & \textbf{98.03\%} & \textbf{99.07\%} & & 88.45\% & \textbf{88.47\%} & & 94.47\% & 95.03\% \\
    \midrule
    \multirow{5}{*}{ID3PM (Ours, InsightFace~\cite{insightface})} & 1.0 & & 98.38\% & 94.37\% & & 91.88\% & 75.60\% & & 93.50\% & 85.26\% \\
    & 1.5 & & 98.95\% & 95.62\% & & 93.88\% & 77.57\% & & 95.59\% & 86.81\% \\
    & 2.0 & & \textbf{99.20\%} & 96.02\% & & 94.53\% & 79.15\% & & 96.13\% & 87.43\% \\
    & 2.5 & & 98.97\% & \textbf{96.37\%} & & \textbf{94.88\%} & 79.20\% & & 96.03\% & 87.83\% \\
    & 3.0 & & 99.15\% & 96.30\% & & 94.78\% & \textbf{79.25\%} & & \textbf{96.16\%} & \textbf{87.97\%} \\
    \bottomrule
    \end{tabular}
    }
\caption{Quantitative evaluation similar to the main paper but with different values for the classifier-free guidance $s$ for our models trained using ID vectors from FaceNet~\cite{facenet, facenet_pytorch} and InsightFace~\cite{insightface}. The best performing setting for each ID vector is marked in bold.}
\label{table:face_recog_scores_guidance}
\end{table*}

\clearpage

\subsection{Identity vector latent space}

Our method is the first to our knowledge to enable smooth interpolations in the ID vector latent space. While we can condition other methods on an interpolated or adapted ID vector as well, their results lack realism and/or do not transition smoothly between images. This is demonstrated in the identity interpolations in \cref{fig:interpolation_competitors}. Note that spherical linear interpolation was used for all methods, but linear interpolation leads to a similar performance. The other one-to-many approaches, Gaussian sampling~\cite{gaussian_sampling} and StyleGAN search~\cite{stylegan-search}, were extended such that the seed of all random number generators is set before each image is generated to eliminate discontinuities due to the randomness of the generation process. Nevertheless, certain identity-agnostic characteristics, such as the the expression, pose, and background for StyleGAN search~\cite{stylegan-search}, change from one image to the next.

\begin{figure*}[htpb]
\centering
\begin{subfigure}{0.95\textwidth}
\centering
    \addtolength{\tabcolsep}{-2pt}
    \small{
    \begin{tabular}{lc}
    NbNet~\cite{nbnet} & \raisebox{-.5\height}{\adjincludegraphics[width=.85\textwidth, trim={0 0 0 0}, clip]{images/interpolations_competitors/sally_lucas/nbnet_slerp.jpg}} \\
    \\[-0.35cm]
    {\begin{tabular}[l]{@{}l@{}}Gaussian \\sampling~\cite{gaussian_sampling} \end{tabular}} & \raisebox{-.5\height}{\adjincludegraphics[width=.85\textwidth, trim={0 0 0 0}, clip]{images/interpolations_competitors/sally_lucas/bbox_slerp.jpg}} \\
    \\[-0.35cm]
    {\begin{tabular}[l]{@{}l@{}}StyleGAN \\search~\cite{stylegan-search} \end{tabular}} & \raisebox{-.5\height}{\adjincludegraphics[width=.85\textwidth, trim={0 0 0 0}, clip]{images/interpolations_competitors/sally_lucas/stylegan_slerp.jpg}} \\
    \\[-0.35cm]
    {\begin{tabular}[l]{@{}l@{}}ID3PM (Ours, \\ FaceNet~\cite{facenet, facenet_pytorch}) \end{tabular}} & \raisebox{-.5\height}{\adjincludegraphics[width=.85\textwidth, trim={0 {.75\height} 0 0}, clip]{images/interpolations/sally_lucas/facenet.jpg}} \\
    \\[-0.35cm]
    {\begin{tabular}[l]{@{}l@{}}ID3PM (Ours, \\ InsightFace~\cite{insightface}) \end{tabular}} & \raisebox{-.5\height}{\adjincludegraphics[width=.85\textwidth, trim={0 {.75\height} 0 0}, clip]{images/interpolations/sally_lucas/insightface.jpg}} \\
    \end{tabular}
    }
    \addtolength{\tabcolsep}{2pt}
    \caption{Identity 1 $\longleftrightarrow$ Identity 2}
    \vspace{5mm}
\end{subfigure}
\begin{subfigure}{0.95\textwidth}
\centering
    \addtolength{\tabcolsep}{-2pt}
    \small{
    \begin{tabular}{lc}
    NbNet~\cite{nbnet} & \raisebox{-.5\height}{\adjincludegraphics[width=.85\textwidth, trim={0 0 0 0}, clip]{images/interpolations_competitors/sandra_xianyao/nbnet_slerp.jpg}} \\
    \\[-0.35cm]
    {\begin{tabular}[l]{@{}l@{}}Gaussian \\sampling~\cite{gaussian_sampling} \end{tabular}} & \raisebox{-.5\height}{\adjincludegraphics[width=.85\textwidth, trim={0 0 0 0}, clip]{images/interpolations_competitors/sandra_xianyao/bbox_slerp.jpg}} \\
    \\[-0.35cm]
    {\begin{tabular}[l]{@{}l@{}}StyleGAN \\search~\cite{stylegan-search} \end{tabular}} & \raisebox{-.5\height}{\adjincludegraphics[width=.85\textwidth, trim={0 0 0 0}, clip]{images/interpolations_competitors/sandra_xianyao/stylegan_slerp.jpg}} \\
    \\[-0.35cm]
    {\begin{tabular}[l]{@{}l@{}}ID3PM (Ours, \\ FaceNet~\cite{facenet, facenet_pytorch}) \end{tabular}} & \raisebox{-.5\height}{\adjincludegraphics[width=.85\textwidth, trim={0 {.75\height} 0 0}, clip]{images/interpolations/sandra_xianyao/facenet.jpg}} \\
    \\[-0.35cm]
    {\begin{tabular}[l]{@{}l@{}}ID3PM (Ours, \\ InsightFace~\cite{insightface}) \end{tabular}} & \raisebox{-.5\height}{\adjincludegraphics[width=.85\textwidth, trim={0 {.75\height} 0 0}, clip]{images/interpolations/sandra_xianyao/insightface.jpg}} \\
    \end{tabular}
    }
    \addtolength{\tabcolsep}{2pt}
    \caption{Identity 3 $\longleftrightarrow$ Identity 4}
\end{subfigure}
    \caption{Identity interpolations for two pairs of identities using state-of-the-art methods. Our method is the only method that provides realistic, smooth interpolations.}
    \label{fig:interpolation_competitors}
\end{figure*}

\clearpage

As described in the main paper, we can find custom directions in the ID vector latent space. This allows us to change certain identity-specific features such as the age, glasses, beard, gender, and baldness during image generation by traversing along a given direction as visualized in \cref{{fig:id_vec_custom_dir}}. 

\begin{figure*}[htpb]
    \centering
    \addtolength{\tabcolsep}{-5pt}
    \begin{subfigure}{0.45\textwidth}
        \small{
        \begin{tabular}{lcccc}
            Older & & 
            \raisebox{-.5\height}{\includegraphics[width=0.3\textwidth]{images/custom_dir/rajesh/0.jpg}} & \raisebox{-.5\height}{\includegraphics[width=0.3\textwidth]{images/custom_dir/rajesh/older/2.jpg}} & \raisebox{-.5\height}{\includegraphics[width=0.3\textwidth]{images/custom_dir/rajesh/older/4.jpg}} \\
            \\[-0.32cm]
            Glasses & & 
            \raisebox{-.5\height}{\includegraphics[width=0.3\textwidth]{images/custom_dir/rajesh/0.jpg}} & \raisebox{-.5\height}{\includegraphics[width=0.3\textwidth]{images/custom_dir/rajesh/glasses/1.jpg}} & \raisebox{-.5\height}{\includegraphics[width=0.3\textwidth]{images/custom_dir/rajesh/glasses/2.jpg}} \\
            \\[-0.32cm]
            Beard & & 
            \raisebox{-.5\height}{\includegraphics[width=0.3\textwidth]{images/custom_dir/rajesh/0.jpg}} & \raisebox{-.5\height}{\includegraphics[width=0.3\textwidth]{images/custom_dir/rajesh/beard/2.jpg}} & \raisebox{-.5\height}{\includegraphics[width=0.3\textwidth]{images/custom_dir/rajesh/beard/3.jpg}} \\
        \end{tabular}
        }
        \caption{Identity 1}
    \end{subfigure}
    \hspace{1.0cm}
    \begin{subfigure}{0.45\textwidth}
        \small{
        \begin{tabular}{lcccc}
            Younger & & 
            \raisebox{-.5\height}{\includegraphics[width=0.3\textwidth]{images/custom_dir/sally/0.jpg}} & \raisebox{-.5\height}{\includegraphics[width=0.3\textwidth]{images/custom_dir/sally/younger/1.jpg}} & \raisebox{-.5\height}{\includegraphics[width=0.3\textwidth]{images/custom_dir/sally/younger/2.jpg}} \\
            \\[-0.32cm]
            Gender & & 
            \raisebox{-.5\height}{\includegraphics[width=0.3\textwidth]{images/custom_dir/sally/0.jpg}} & \raisebox{-.5\height}{\includegraphics[width=0.3\textwidth]{images/custom_dir/sally/gender/2.jpg}} & \raisebox{-.5\height}{\includegraphics[width=0.3\textwidth]{images/custom_dir/sally/gender/3.jpg}} \\
            \\[-0.32cm]
            Bald & & 
            \raisebox{-.5\height}{\includegraphics[width=0.3\textwidth]{images/custom_dir/sally/0.jpg}} & \raisebox{-.5\height}{\includegraphics[width=0.3\textwidth]{images/custom_dir/sally/bald/2.jpg}} & \raisebox{-.5\height}{\includegraphics[width=0.3\textwidth]{images/custom_dir/sally/bald/3.jpg}} \\
        \end{tabular}
        }
        \caption{Identity 2}
    \end{subfigure}
    \addtolength{\tabcolsep}{5pt}
    \caption{Controllable image generation through custom directions in the (InsightFace~\cite{insightface}) ID vector latent space.}
    \label{fig:id_vec_custom_dir}
\end{figure*}

\clearpage

\subsection{Attribute conditioning}

To help disentangle identity-specific from identity-agnostic features as well as to obtain additional intuitive control, we propose attribute conditioning in the main paper. We consider three sets of attributes from the FFHQ metadata~\cite{ffhq_metadata}\footnote{We ignore the following attributes from the metadata: smile because it correlates with emotion; and blur, exposure, and noise because we are not interested in them for the purposes of this experiment.} as shown in \cref{table:attributes} by grouping them by how much they contribute to a person's identity. For example, set 1 only contains attributes that are identity-agnostic whereas set 3 also contains attributes that are strongly correlated with identity. In practice, we recommend using set 1 (and thus use that in the main paper) but show sets 2 and 3 for completeness. Note that the attributes from the FFHQ metadata~\cite{ffhq_metadata} are in the form of JSON files. Since the neural networks used to extract the attributes are not publicly available, only black-box approaches such as ours that do not require the neural networks' gradients can be used. The attribute conditioning is thus an example of how our proposed conditioning mechanism can be extended to include information from non-differentiable sources.


\begin{table}[htbp]
\centering
\small{
\begin{tabular}{lcccc}
    \toprule
    \multirow{2}{*}{Attribute} & \multirow{2}{*}{Number of categories} & \multicolumn{3}{c}{Set} \\
    \cmidrule{3-5}
    & & 1 & 2 & 3 \\
    \midrule
    Age & 1 & \xmark & \xmark & \cmark \\
    Emotion & 8 & \cmark & \cmark & \cmark \\
    Facial hair & 3 & \xmark & \xmark & \cmark \\
    Hair & 8 & \xmark & \xmark & \cmark \\
    Head pose & 3 & \cmark & \cmark & \cmark \\
    Gender $^1$ & 1 & \xmark & \xmark & \cmark \\
    Glasses & 1 & \xmark & \cmark & \cmark \\
    Makeup & 2 & \xmark & \cmark & \cmark \\
    Occlusions & 3 & \xmark & \cmark & \cmark \\
    \bottomrule
\end{tabular}
}
\caption{Different attribute sets. The number of (potentially identity-correlated) features increases from left to right. $^1$ While gender arguably falls on a continuous, nonlinear spectrum, we treat it as a binary variable since only this information is available in the data set.}
\label{table:attributes}
\end{table}

By training our models with attribute conditioning, we can obtain images that recover more of the original data distribution when we sample conditioned on the identity but using the unconditional setting for the attributes (\ie $-1$-vector for the attribute vector), as shown in the main paper. 
However, the attributes can overpower the identity information if the attribute set contains attributes that are heavily correlated with the identity. This is visualized in \cref{fig:attribute_style} where we condition our model trained with InsightFace~\cite{insightface} ID vectors on the same ID vector but different attribute vectors, specifically the ones of the first $10$ images of the FFHQ test set ($\{69000, 69001, ..., 69009\}$). As we cannot show images from the FFHQ~\cite{stylegan} data set as mentioned in the ethics section of the main paper, we instead supply a table with their main attributes. For example, image $69000$ is of a happy 27-year old woman with brown/black hair and makeup whose head is turned slightly to the left. For attribute set 1, only the pose and emotion is copied. For attribute set 2, the makeup is also copied. For attribute 3, the gender is also copied, thus leading to an image of a woman for identity 2. In general, as we add more attributes, the original identity is changed increasingly more. Since attribute sets 2 and 3 can alter the identity significantly, we opt for attribute set 1 in most cases.

\begin{figure*}[htpb]
\centering
\begin{subfigure}{0.9\textwidth}
\centering
    \addtolength{\tabcolsep}{-2pt}
    \small{
    \begin{tabular}{lc}
    ID + Set 1 & \raisebox{-.5\height}{\adjincludegraphics[width=.9\textwidth, trim={0 0 0 0}, clip]{images/attribute_cond_style/eleftheria/notidentity.jpg}} \\
    \\[-0.35cm]
    ID + Set 2 & \raisebox{-.5\height}{\adjincludegraphics[width=.9\textwidth, trim={0 0 0 0}, clip]{images/attribute_cond_style/eleftheria/somewhatidentity.jpg}} \\
    \\[-0.35cm]
    ID + Set 3 & \raisebox{-.5\height}{\adjincludegraphics[width=.9\textwidth, trim={0 0 0 0}, clip]{images/attribute_cond_style/eleftheria/all.jpg}} \\
    Attributes \hspace{1mm} $\rightarrow$ & \begin{tabularx}{0.9\textwidth}{ *{10}{Y} } $69000$ & $69001$ & $69002$ & $69003$ & $69004$ & $69005$ & $69006$ & $69007$ & $69008$ & $69009$ \end{tabularx} \\
    \end{tabular}
    }
    \addtolength{\tabcolsep}{2pt}
    \caption{Identity 1}
    \vspace{5mm}
\end{subfigure}
\begin{subfigure}{0.9\textwidth}
\centering
    \addtolength{\tabcolsep}{-2pt}
    \small{
    \begin{tabular}{lc}
    ID + Set 1 & \raisebox{-.5\height}{\adjincludegraphics[width=.9\textwidth, trim={0 0 0 0}, clip]{images/attribute_cond_style/lucas/notidentity.jpg}} \\
    \\[-0.35cm]
    ID + Set 2 & \raisebox{-.5\height}{\adjincludegraphics[width=.9\textwidth, trim={0 0 0 0}, clip]{images/attribute_cond_style/lucas/somewhatidentity.jpg}} \\
    \\[-0.35cm]
    ID + Set 3 & \raisebox{-.5\height}{\adjincludegraphics[width=.9\textwidth, trim={0 0 0 0}, clip]{images/attribute_cond_style/lucas/all.jpg}} \\
    Attributes \hspace{1mm} $\rightarrow$ & \begin{tabularx}{0.9\textwidth}{ *{10}{Y} } $69000$ & $69001$ & $69002$ & $69003$ & $69004$ & $69005$ & $69006$ & $69007$ & $69008$ & $69009$ \end{tabularx} \\
    \end{tabular}
    }
    \addtolength{\tabcolsep}{2pt}
    \caption{Identity 2}
    \vspace{5mm}
\end{subfigure}
\begin{subfigure}{0.9\textwidth}
    \centering
    \small{
    \begin{tabular}{llS[table-format=2.0]llS[table-format=2.1]l}
    \toprule
    Image number & Gender & {Age} & Hair & Emotion & {Yaw angle} & Other \\
    \midrule
    69000 & Female & 27 & Brown/black & Happy & -21.9 & Makeup \\
    69001 & Male & 68 & Gray & Neutral & -13.9 & {-} \\
    69002 & Female & 50 & Not visible & Happy & 5.5 & Headwear + glasses \\
    69003 & Female & 20 & Brown/blond & Happy & -0.4 & {-} \\
    69004 & Male & 36 & Black & Neutral & 5.3 & {-} \\
    69005 & Female & 7 & Blond & Happy & -2.6 & {-} \\
    69006 & Female & 18 & Blond & Neutral & 9.0 & {-} \\
    69007 & Female & 21 & Brown & Happy & -24.3 & Makeup \\
    69008 & Female & 28 & Blond & Happy & 10.3 & Makeup \\
    69009 & Female & 43 & Black/brown & Happy & -16.6 & Makeup + glasses \\
    \bottomrule
    \end{tabular}
    }
    \caption{Attribute descriptions}
\end{subfigure}
    \caption{Attribute conditioning for two identities using different attribute sets. Images in each group have the same (InsightFace~\cite{insightface}) ID vector, but the attributes are chosen from images $\{69000, 69001, ..., 69009\}$ of the FFHQ data set.}
    \label{fig:attribute_style}
\end{figure*}

Interestingly, even when conditioning on attribute set 1 (only pose and emotion), the average identity distance (seen in the quantitative evaluation of the diversity and identity distances in the main paper) increases despite the visual results appearing similar in terms of identity preservation. We hypothesize that this is because most face recognition vectors (inadvertently) encode the pose and expression (see \cref{sec:analysis}) and are less robust to extreme poses and expressions. Therefore, for the identity distance, it is better to reconstruct a face with a similar pose and expression as the original image. Nevertheless, we argue for the attribute conditioning (using attribute set 1) for most use cases because it leads to more diverse results and allows for an intuitive control over attributes. 

\clearpage

Through attribute conditioning, we can simply set the values of desired attributes, such as the emotion and head pose, during inference time to control them, as seen in \cref{fig:attribute_cond_control_2}. Note that our method has no internal structure to enforce 3D consistency. The attribute conditioning alone suffices in generating images that preserve the identity and 3D geometry surprisingly well as we traverse the attribute latent space. This intuitive attribute control paves the way towards using our method to create and augment data sets.

\begin{figure*}[htpb]
\centering
\begin{subfigure}{0.9\textwidth}
\centering
    \addtolength{\tabcolsep}{-4pt}
    \small{
    \begin{tabular}{cc}
    1 & \raisebox{-.5\height}{\adjincludegraphics[width=.95\textwidth, trim={0 0 0 0}, clip]{images/attribute_cond_control/martina/yaw.jpg}} \\
    \\[-0.3cm]
    2 & \raisebox{-.5\height}{\adjincludegraphics[width=.95\textwidth, trim={0 0 0 0}, clip]{images/attribute_cond_control/xianyao/yaw.jpg}} \\
    \\[-0.3cm]
    3 & \raisebox{-.5\height}{\adjincludegraphics[width=.95\textwidth, trim={0 0 0 0}, clip]{images/attribute_cond_control/melissa/yaw.jpg}} \\
    \end{tabular}
    }
    \addtolength{\tabcolsep}{4pt}
    \caption{Yaw angle}
    \vspace{5mm}
\end{subfigure}
\begin{subfigure}{0.8\textwidth}
\centering
    \addtolength{\tabcolsep}{-4pt}
    \small{
    \begin{tabular}{cc}
    1 & \raisebox{-.5\height}{\adjincludegraphics[width=.95\textwidth, trim={0 0 0 0}, clip]{images/attribute_cond_control/martina/allemotions.jpg}} \\
    \\[-0.3cm]
    2 & \raisebox{-.5\height}{\adjincludegraphics[width=.95\textwidth, trim={0 0 0 0}, clip]{images/attribute_cond_control/xianyao/allemotions.jpg}} \\
    \\[-0.3cm]
    3 & \raisebox{-.5\height}{\adjincludegraphics[width=.95\textwidth, trim={0 0 0 0}, clip]{images/attribute_cond_control/melissa/allemotions.jpg}} \\
    & \begin{tabularx}{0.95\textwidth}{ *{8}{Y} } Anger & Contempt & Disgust & Fear & Happiness & Neutral & Sadness & Surprise \end{tabularx} \\
    \end{tabular}
    }
    \addtolength{\tabcolsep}{4pt}
    \caption{Eight emotions}
    \vspace{5mm}
\end{subfigure}
\begin{subfigure}{0.9\textwidth}
\centering
    \addtolength{\tabcolsep}{-4pt}
    \small{
    \begin{tabular}{cc}
    1 & \raisebox{-.5\height}{\adjincludegraphics[width=.95\textwidth, trim={0 0 0 0}, clip]{images/attribute_cond_control/martina/sadhappy.jpg}} \\
    \\[-0.3cm]
    2 & \raisebox{-.5\height}{\adjincludegraphics[width=.95\textwidth, trim={0 0 0 0}, clip]{images/attribute_cond_control/xianyao/sadhappy.jpg}} \\
    \\[-0.3cm]
    3 & \raisebox{-.5\height}{\adjincludegraphics[width=.95\textwidth, trim={0 0 0 0}, clip]{images/attribute_cond_control/melissa/sadhappy.jpg}} \\
    \end{tabular}
    }
    \addtolength{\tabcolsep}{4pt}
    \caption{Sad $\longleftrightarrow$ Happy}
\end{subfigure}
    \caption{Controllable image generation through attribute conditioning. We smoothly change three different attributes of three identities. All models were trained using InsightFace~\cite{insightface} ID vectors and attribute set 1.}
    \label{fig:attribute_cond_control_2}
\end{figure*}

\section{Failure case}

As described in the main paper, one limitation of our approach is that it inherits the biases of both the face recognition model and the data set used to train the diffusion model. This causes our method to sometimes lose identity fidelity of underrepresented groups in the data set, as seen in the example in \Cref{fig:failure_case}, where the method produces images quite different from the original identity.

\begin{figure*}[htpb]
    \centering
    \addtolength{\tabcolsep}{-2pt}
    \small{
    \begin{tabular}{cc}
        \raisebox{1.5\height}{\includegraphics[width=0.125\textwidth]{images/original/Prashanth.jpg}} & \includegraphics[width=0.5\textwidth]{images/qualitative_comp/prashanth/ours_16_facenet.jpg} \\
        Original image & ID3PM (Ours, FaceNet~\cite{facenet, facenet_pytorch}) \\
        \\
    \end{tabular}
    }
    \addtolength{\tabcolsep}{2pt}
    \caption{Failure case. Some underrepresented groups in the data sets might have lower identity fidelity for some ID vectors (here FaceNet~\cite{facenet, facenet_pytorch}).}
    \label{fig:failure_case}
\end{figure*}

\clearpage

\section{Application: Analysis of face recognition methods} \label{sec:analysis}

With the rise of deep learning and large data sets with millions of images, face recognition methods have reached or even surpassed human-level performance~\cite{deepface, deepid, facenet, vggface}. Nevertheless, face recognition systems have known issues in their robustness to different degradations and attacks~\cite{facerec_robustness1, facerec_robustness2} and their biases (\eg in terms of ethnic origin)~\cite{facerec_bias1, facerec_bias2, facerec_bias3}.

Due to its general nature with very low requirements for the data set and few assumptions compared to other methods, our method is very well suited for analyzing and visualizing the latent spaces of different face recognition (FR) models. Since our method does not require access to the internals of the pre-trained face recognition model (\emph{black-box} setting), we can analyze different face recognition methods by simply replacing the input ID vectors without worrying about different deep learning frameworks and the memory burden of adding more models. 

For the analysis in this section, we train multiple versions of the model with ID vectors extracted with the pre-trained face recognition models listed in \cref{table:id_vectors}. Note that since we specifically want to analyze what identity-agnostic features are contained in common face recognition models, we do not use attribute conditioning here since it would disentangle identity-specific and identity-agnostic features.

\subsection{Qualitative evaluation}

\Cref{fig:comp_diff_id_vectors} shows uncurated samples of generated images of the considered ID vectors for several identities. While all generated images appear of a similar quality in terms of realism, the identity preservation of different methods behaves quite differently. The relative performance of different ID vectors changes depending on the identity, but the results for FaceNet~\cite{facenet, facenet_pytorch} and InsightFace~\cite{insightface} seem most consistent on average~\footnote{Note that more images than the representative ones shown here were considered to make this statement and other statements in this section.}. As the inversion networks are trained with the same diffusion model architecture and the same data set, we hypothesize that these differences largely boil down to the biases of the respective face recognition methods and the data sets used to train them. 

\begin{figure*}[htpb]
    \centering
    \addtolength{\tabcolsep}{-5pt}
    \footnotesize{
    \begin{tabular}{cccccc}
        \includegraphics[width=0.09\textwidth]{images/original/Aziz.jpg} & 
        \includegraphics[width=0.09\textwidth]{images/qualitative_comp_diff_id_vectors/aziz/AdaFace.jpg} & 
        \includegraphics[width=0.09\textwidth]{images/qualitative_comp_diff_id_vectors/aziz/arcface.jpg} & 
        \includegraphics[width=0.09\textwidth]{images/qualitative_comp_diff_id_vectors/aziz/facenet.jpg} & 
        \includegraphics[width=0.09\textwidth]{images/qualitative_comp_diff_id_vectors/aziz/FROM.jpg} & 
        \includegraphics[width=0.09\textwidth]{images/qualitative_comp_diff_id_vectors/aziz/insightface.jpg} \\
        \\[-0.46cm]
        \includegraphics[width=0.09\textwidth]{images/original/Clara.jpg} & 
        \includegraphics[width=0.09\textwidth]{images/qualitative_comp_diff_id_vectors/clara/AdaFace.jpg} & 
        \includegraphics[width=0.09\textwidth]{images/qualitative_comp_diff_id_vectors/clara/arcface.jpg} & 
        \includegraphics[width=0.09\textwidth]{images/qualitative_comp_diff_id_vectors/clara/facenet.jpg} & 
        \includegraphics[width=0.09\textwidth]{images/qualitative_comp_diff_id_vectors/clara/FROM.jpg} & 
        \includegraphics[width=0.09\textwidth]{images/qualitative_comp_diff_id_vectors/clara/insightface.jpg} \\
        \\[-0.46cm]
        \includegraphics[width=0.09\textwidth]{images/original/Thomas.jpg} & 
        \includegraphics[width=0.09\textwidth]{images/qualitative_comp_diff_id_vectors/thomas/AdaFace.jpg} & 
        \includegraphics[width=0.09\textwidth]{images/qualitative_comp_diff_id_vectors/thomas/arcface.jpg} & 
        \includegraphics[width=0.09\textwidth]{images/qualitative_comp_diff_id_vectors/thomas/facenet.jpg} & 
        \includegraphics[width=0.09\textwidth]{images/qualitative_comp_diff_id_vectors/thomas/FROM.jpg} & 
        \includegraphics[width=0.09\textwidth]{images/qualitative_comp_diff_id_vectors/thomas/insightface.jpg} \\
        \\[-0.46cm]
        \includegraphics[width=0.09\textwidth]{images/original/Eleftheria.jpg} & 
        \includegraphics[width=0.09\textwidth]{images/qualitative_comp_diff_id_vectors/eleftheria/AdaFace.jpg} & 
        \includegraphics[width=0.09\textwidth]{images/qualitative_comp_diff_id_vectors/eleftheria/arcface.jpg} & 
        \includegraphics[width=0.09\textwidth]{images/qualitative_comp_diff_id_vectors/eleftheria/facenet.jpg} & 
        \includegraphics[width=0.09\textwidth]{images/qualitative_comp_diff_id_vectors/eleftheria/FROM.jpg} & 
        \includegraphics[width=0.09\textwidth]{images/qualitative_comp_diff_id_vectors/eleftheria/insightface.jpg} \\
        \\[-0.46cm]
        \includegraphics[width=0.09\textwidth]{images/original/Tunc.jpg} & 
        \includegraphics[width=0.09\textwidth]{images/qualitative_comp_diff_id_vectors/tunc/AdaFace.jpg} & 
        \includegraphics[width=0.09\textwidth]{images/qualitative_comp_diff_id_vectors/tunc/arcface.jpg} & 
        \includegraphics[width=0.09\textwidth]{images/qualitative_comp_diff_id_vectors/tunc/facenet.jpg} & 
        \includegraphics[width=0.09\textwidth]{images/qualitative_comp_diff_id_vectors/tunc/FROM.jpg} & 
        \includegraphics[width=0.09\textwidth]{images/qualitative_comp_diff_id_vectors/tunc/insightface.jpg} \\
        \\[-0.46cm]
        \includegraphics[width=0.09\textwidth]{images/original/Jakob.jpg} & 
        \includegraphics[width=0.09\textwidth]{images/qualitative_comp_diff_id_vectors_uncurated/jakob/AdaFace.jpg} & 
        \includegraphics[width=0.09\textwidth]{images/qualitative_comp_diff_id_vectors_uncurated/jakob/arcface.jpg} & 
        \includegraphics[width=0.09\textwidth]{images/qualitative_comp_diff_id_vectors_uncurated/jakob/facenet.jpg} & 
        \includegraphics[width=0.09\textwidth]{images/qualitative_comp_diff_id_vectors_uncurated/jakob/FROM.jpg} & 
        \includegraphics[width=0.09\textwidth]{images/qualitative_comp_diff_id_vectors_uncurated/jakob/insightface.jpg} \\
        \\[-0.46cm]
        \includegraphics[width=0.09\textwidth]{images/original/Julia.jpg} & 
        \includegraphics[width=0.09\textwidth]{images/qualitative_comp_diff_id_vectors_uncurated/julia/AdaFace.jpg} & 
        \includegraphics[width=0.09\textwidth]{images/qualitative_comp_diff_id_vectors_uncurated/julia/arcface.jpg} & 
        \includegraphics[width=0.09\textwidth]{images/qualitative_comp_diff_id_vectors_uncurated/julia/facenet.jpg} & 
        \includegraphics[width=0.09\textwidth]{images/qualitative_comp_diff_id_vectors_uncurated/julia/FROM.jpg} & 
        \includegraphics[width=0.09\textwidth]{images/qualitative_comp_diff_id_vectors_uncurated/julia/insightface.jpg} \\
        \\[-0.46cm]
        \includegraphics[width=0.09\textwidth]{images/original/Markus.jpg} & 
        \includegraphics[width=0.09\textwidth]{images/qualitative_comp_diff_id_vectors_uncurated/markus/AdaFace.jpg} & 
        \includegraphics[width=0.09\textwidth]{images/qualitative_comp_diff_id_vectors_uncurated/markus/arcface.jpg} & 
        \includegraphics[width=0.09\textwidth]{images/qualitative_comp_diff_id_vectors_uncurated/markus/facenet.jpg} & 
        \includegraphics[width=0.09\textwidth]{images/qualitative_comp_diff_id_vectors_uncurated/markus/FROM.jpg} & 
        \includegraphics[width=0.09\textwidth]{images/qualitative_comp_diff_id_vectors_uncurated/markus/insightface.jpg} \\
        \\[-0.46cm]
        \includegraphics[width=0.09\textwidth]{images/original/Martina.jpg} & 
        \includegraphics[width=0.09\textwidth]{images/qualitative_comp_diff_id_vectors_uncurated/martina/AdaFace.jpg} & 
        \includegraphics[width=0.09\textwidth]{images/qualitative_comp_diff_id_vectors_uncurated/martina/arcface.jpg} & 
        \includegraphics[width=0.09\textwidth]{images/qualitative_comp_diff_id_vectors_uncurated/martina/facenet.jpg} & 
        \includegraphics[width=0.09\textwidth]{images/qualitative_comp_diff_id_vectors_uncurated/martina/FROM.jpg} & 
        \includegraphics[width=0.09\textwidth]{images/qualitative_comp_diff_id_vectors_uncurated/martina/insightface.jpg} \\
        \\[-0.46cm]
        \includegraphics[width=0.09\textwidth]{images/original/Paulo.jpg} & 
        \includegraphics[width=0.09\textwidth]{images/qualitative_comp_diff_id_vectors_uncurated/paulo/AdaFace.jpg} & 
        \includegraphics[width=0.09\textwidth]{images/qualitative_comp_diff_id_vectors_uncurated/paulo/arcface.jpg} & 
        \includegraphics[width=0.09\textwidth]{images/qualitative_comp_diff_id_vectors_uncurated/paulo/facenet.jpg} & 
        \includegraphics[width=0.09\textwidth]{images/qualitative_comp_diff_id_vectors_uncurated/paulo/FROM.jpg} & 
        \includegraphics[width=0.09\textwidth]{images/qualitative_comp_diff_id_vectors_uncurated/paulo/insightface.jpg} \\
        \\[-0.46cm]
        \includegraphics[width=0.09\textwidth]{images/original/Prashanth.jpg} & 
        \includegraphics[width=0.09\textwidth]{images/qualitative_comp_diff_id_vectors_uncurated/prashanth/AdaFace.jpg} & 
        \includegraphics[width=0.09\textwidth]{images/qualitative_comp_diff_id_vectors_uncurated/prashanth/arcface.jpg} & 
        \includegraphics[width=0.09\textwidth]{images/qualitative_comp_diff_id_vectors_uncurated/prashanth/facenet.jpg} & 
        \includegraphics[width=0.09\textwidth]{images/qualitative_comp_diff_id_vectors_uncurated/prashanth/FROM.jpg} & 
        \includegraphics[width=0.09\textwidth]{images/qualitative_comp_diff_id_vectors_uncurated/prashanth/insightface.jpg} \\
        \\[-0.46cm]
        \includegraphics[width=0.09\textwidth]{images/original/Rajesh.jpg} & 
        \includegraphics[width=0.09\textwidth]{images/qualitative_comp_diff_id_vectors_uncurated/rajesh/AdaFace.jpg} & 
        \includegraphics[width=0.09\textwidth]{images/qualitative_comp_diff_id_vectors_uncurated/rajesh/arcface.jpg} & 
        \includegraphics[width=0.09\textwidth]{images/qualitative_comp_diff_id_vectors_uncurated/rajesh/facenet.jpg} & 
        \includegraphics[width=0.09\textwidth]{images/qualitative_comp_diff_id_vectors_uncurated/rajesh/FROM.jpg} & 
        \includegraphics[width=0.09\textwidth]{images/qualitative_comp_diff_id_vectors_uncurated/rajesh/insightface.jpg} \\
        \\[-0.46cm]
        \includegraphics[width=0.09\textwidth]{images/original/Xianyao.jpg} & 
        \includegraphics[width=0.09\textwidth]{images/qualitative_comp_diff_id_vectors_uncurated/xianyao/AdaFace.jpg} & 
        \includegraphics[width=0.09\textwidth]{images/qualitative_comp_diff_id_vectors_uncurated/xianyao/arcface.jpg} & 
        \includegraphics[width=0.09\textwidth]{images/qualitative_comp_diff_id_vectors_uncurated/xianyao/facenet.jpg} & 
        \includegraphics[width=0.09\textwidth]{images/qualitative_comp_diff_id_vectors_uncurated/xianyao/FROM.jpg} & 
        \includegraphics[width=0.09\textwidth]{images/qualitative_comp_diff_id_vectors_uncurated/xianyao/insightface.jpg} \\
        
        \multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Original \\ image \end{tabular}} & \multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Ada- \\ Face~\cite{adaface} \end{tabular}} & \multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Arc- \\ Face~\cite{arcface, gaussian_sampling} \end{tabular}} & \multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Face- \\ Net~\cite{facenet, facenet_pytorch} \end{tabular}} &  \multirow{2}{*}{FROM~\cite{from}} & \multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Insight- \\ Face~\cite{insightface} \end{tabular}} \\
        \\
    \end{tabular}
    }
    \addtolength{\tabcolsep}{5pt}
    \caption{Qualitative evaluation of ID vectors from different state-of-the-art face recognition models. Note that the same seed was used for all images to obtain the most fair results.}
    \label{fig:comp_diff_id_vectors}
\end{figure*}

\subsection{Robustness}

Our method can also be used to analyze and visualize the robustness of face recognition models in difficult scenarios such as varying expressions, poses, lighting, occlusions, and noise as seen in \cref{fig:robustness_2}. In line with our previous observations, FaceNet~\cite{facenet, facenet_pytorch} and InsightFace~\cite{insightface} appear the most robust. 

In this analysis, it is also relatively easy to tell which features are extracted by observing which features of a target identity's image are preserved. For example, ArcFace~\cite{arcface, gaussian_sampling} and FROM~\cite{from} seem to contain pose information as the generated images in the fourth and fifth columns have similar poses as the target identities' images for both identities. Similarly, AdaFace~\cite{adaface} and ArcFace~\cite{arcface, gaussian_sampling} seem to copy the expression for the third column of the first identity. InsightFace~\cite{insightface} also seems to contain expressions and pose for the second identity as seen in columns two to four. Another feature that is commonly copied is whether a person is wearing a hat or not even though this should arguably not be considered part of a person's identity. Interestingly, FROM, a method specifically aimed to mask out corrupted features, does not appear more robust for the tested occlusions (sunglasses, hat). Lastly, noise seems to affect most face recognition methods significantly.

\Cref{fig:robustness_2} also lists the angular distances of the identities for each generated image to the target identity for the same FR method. The distances for generated images that can be considered failure cases are in general higher than those of the images that worked better. However, they are all still below the optimal threshold\footnote{It is actually the mean threshold over the $10$ different splits. Note that the standard deviation across the splits is smaller than $0.005$.} calculated for each FR method for real images of the LFW~\cite{lfw} data set using the official protocol -- meaning that all generated images are considered to be of the same person. Therefore, we argue that the wrong ID reconstructions are mostly due to problems in the ID vectors rather than the inversion of our model.

\begin{figure*}[htpb]
\centering
\begin{subfigure}{1\textwidth}
\centering
    \addtolength{\tabcolsep}{-2pt}
    \scriptsize{
    \begin{tabular}{lcc}
    & \raisebox{-.5\height}{\adjincludegraphics[width=.8\textwidth, trim={0 {.835\height} 0 0}, clip]{images/robustness/Melissa_facenet.jpg}} & \begin{tabular}[c]{@{}c@{}}LFW \\ threshold \end{tabular} \\
    \\[-0.2cm]
    AdaFace~\cite{adaface} & \raisebox{-.5\height}{\adjincludegraphics[width=.8\textwidth, trim={0 {.667\height} 0 {0.167\height}}, clip]{images/robustness/Melissa_AdaFace.jpg}} & \\
    & \begin{tabularx}{0.8\textwidth}{ *{12}{Y} } $0.29$ & $0.24$ & $0.27$ & $0.24$ & $0.27$ & $0.24$ & $0.27$ & $0.35$ & $0.31$ & $0.26$ & $0.35$ & {-} \end{tabularx} & $0.43$ \\
    ArcFace~\cite{arcface, gaussian_sampling} & \raisebox{-.5\height}{\adjincludegraphics[width=.8\textwidth, trim={0 {.667\height} 0 {0.167\height}}, clip]{images/robustness/Melissa_arcface.jpg}} & \\
    & \begin{tabularx}{0.8\textwidth}{ *{12}{Y} } $0.28$ & $0.25$ & $0.25$ & $0.27$ & $0.30$ & $0.26$ & $0.23$ & $0.26$ & $0.28$ & $0.25$ & $0.28$ & {-} \end{tabularx} & $0.45$ \\
    FaceNet~\cite{facenet, facenet_pytorch} & \raisebox{-.5\height}{\adjincludegraphics[width=.8\textwidth, trim={0 {.667\height} 0 {0.167\height}}, clip]{images/robustness/Melissa_facenet.jpg}} & \\
    & \begin{tabularx}{0.8\textwidth}{ *{12}{Y} } $0.15$ & $0.19$ & $0.16$ & $0.16$ & $0.14$ & $0.12$ & $0.14$ & $0.17$ & $0.14$ & $0.14$ & $0.19$ & {-} \end{tabularx} & $0.37$ \\
    FROM~\cite{from} & \raisebox{-.5\height}{\adjincludegraphics[width=.8\textwidth, trim={0 {.667\height} 0 {0.167\height}}, clip]{images/robustness/Melissa_FROM.jpg}} & \\
    & \begin{tabularx}{0.8\textwidth}{ *{12}{Y} } $0.17$ & $0.16$ & $0.19$ & $0.19$ & $0.23$ & $0.23$ & $0.19$ & $0.18$ & $0.20$ & $0.18$ & $0.22$ & {-} \end{tabularx} & $0.42$ \\
    InsightFace~\cite{insightface} & \raisebox{-.5\height}{\adjincludegraphics[width=.8\textwidth, trim={0 {.667\height} 0 {0.167\height}}, clip]{images/robustness/Melissa_insightface.jpg}} & \\
    & \begin{tabularx}{0.8\textwidth}{ *{12}{Y} } $0.23$ & $0.24$ & $0.21$ & $0.23$ & $0.24$ & $0.22$ & $0.21$ & $0.25$ & $0.23$ & $0.25$ & $0.24$ & {-} \end{tabularx} & $0.42$ \\
    \end{tabular}
    }
    \addtolength{\tabcolsep}{2pt}
    \caption{Identity 1}
    \vspace{5mm}
\end{subfigure}
\begin{subfigure}{\textwidth}
\centering
    \addtolength{\tabcolsep}{-2pt}
    \scriptsize{
    \begin{tabular}{lcc}
    & \raisebox{-.5\height}{\adjincludegraphics[width=.8\textwidth, trim={0 {.801\height} 0 0}, clip]{images/robustness/Lucas_facenet.jpg}} & \begin{tabular}[c]{@{}c@{}}LFW \\ threshold \end{tabular} \\
    \\[-0.2cm]
    AdaFace~\cite{adaface} & \raisebox{-.5\height}{\adjincludegraphics[width=.8\textwidth, trim={0 {.2\height} 0 {0.6\height}}, clip]{images/robustness/Lucas_AdaFace.jpg}} & \\
    & \begin{tabularx}{0.8\textwidth}{ *{12}{Y} } $0.28$ & $0.29$ & $0.29$ & $0.28$ & $0.27$ & $0.28$ & $0.28$ & $0.26$ & $0.25$ & $0.40$ & $0.32$ & {-} \end{tabularx} & $0.43$ \\
    ArcFace~\cite{arcface, gaussian_sampling} & \raisebox{-.5\height}{\adjincludegraphics[width=.8\textwidth, trim={0 {.2\height} 0 {0.6\height}}, clip]{images/robustness/Lucas_arcface.jpg}} & \\
    & \begin{tabularx}{0.8\textwidth}{ *{12}{Y} } $0.23$ & $0.25$ & $0.22$ & $0.30$ & $0.24$ & $0.22$ & $0.24$ & $0.23$ & $0.27$ & $0.24$ & $0.23$ & {-} \end{tabularx} & $0.45$ \\
    FaceNet~\cite{facenet, facenet_pytorch} & \raisebox{-.5\height}{\adjincludegraphics[width=.8\textwidth, trim={0 {.2\height} 0 {0.6\height}}, clip]{images/robustness/Lucas_facenet.jpg}} & \\
    & \begin{tabularx}{0.8\textwidth}{ *{12}{Y} } $0.18$ & $0.17$ & $0.19$ & $0.16$ & $0.15$ & $0.17$ & $0.17$ & $0.17$ & $0.18$ & $0.13$ & $0.21$ & {-} \end{tabularx} & $0.37$ \\
    FROM~\cite{from} & \raisebox{-.5\height}{\adjincludegraphics[width=.8\textwidth, trim={0 {.2\height} 0 {0.6\height}}, clip]{images/robustness/Lucas_FROM.jpg}} & \\
    & \begin{tabularx}{0.8\textwidth}{ *{12}{Y} } $0.15$ & $0.12$ & $0.17$ & $0.23$ & $0.21$ & $0.15$ & $0.16$ & $0.17$ & $0.22$ & $0.16$ & $0.18$ & {-} \end{tabularx} & $0.42$ \\
    InsightFace~\cite{insightface} & \raisebox{-.5\height}{\adjincludegraphics[width=.8\textwidth, trim={0 {.2\height} 0 {0.6\height}}, clip]{images/robustness/Lucas_insightface.jpg}} & \\
    & \begin{tabularx}{0.8\textwidth}{ *{12}{Y} } $0.21$ & $0.22$ & $0.21$ & $0.21$ & $0.22$ & $0.23$ & $0.22$ & $0.23$ & $0.23$ & $0.23$ & $0.24$ & {-} \end{tabularx} & $0.42$ \\
    \end{tabular}
    }
    \addtolength{\tabcolsep}{2pt}
    \caption{Identity 2}
\end{subfigure}
    \caption{Robustness experiment. The first row shows images of a source identity in challenging scenarios whereas the remaining rows show the results when using different ID vectors. The last image column shows images generated from the mean ID vector of all of the source identity images (for which no source image exists). The numbers under each line are the angular distances between the identity of the generated images and the target identity for the same face recognition method. The numbers in the last column are the optimal thresholds for that method calculated using real images of the LFW~\cite{lfw} data set and official protocol.}
    \label{fig:robustness_2}
\end{figure*}


We further experiment with out-of-distribution samples such as drawings and digital renders as shown in \cref{fig:out_of_distr}. Interestingly, despite the extremely difficult setup, some of the resulting images resemble the identity fairly well, especially for FaceNet~\cite{facenet, facenet_pytorch}, demonstrating that some face recognition models can extract reasonable identity-specific features even from faces that are out of distribution. Furthermore, this experiment shows that our method can generate extreme features, such as the large nose of the man in the fifth row with FaceNet~\cite{facenet, facenet_pytorch}, that are likely not in the training data set.

\begin{figure*}[htpb]
    \centering
    \addtolength{\tabcolsep}{-5pt}
    \small{
    \begin{tabular}{cccccc}
        \includegraphics[width=0.13\textwidth]{images/original/MyDrawing.jpg} & 
        \includegraphics[width=0.13\textwidth]{images/out_of_distr/mydrawing/AdaFace.jpg} & 
        \includegraphics[width=0.13\textwidth]{images/out_of_distr/mydrawing/arcface.jpg} & 
        \includegraphics[width=0.13\textwidth]{images/out_of_distr/mydrawing/facenet.jpg} & 
        \includegraphics[width=0.13\textwidth]{images/out_of_distr/mydrawing/FROM.jpg} & 
        \includegraphics[width=0.13\textwidth]{images/out_of_distr/mydrawing/insightface.jpg} \\
        \\[-0.46cm]
        \includegraphics[width=0.13\textwidth]{images/original/Violaine6.jpg} & 
        \includegraphics[width=0.13\textwidth]{images/out_of_distr/violaine6/AdaFace.jpg} & 
        \includegraphics[width=0.13\textwidth]{images/out_of_distr/violaine6/arcface.jpg} & 
        \includegraphics[width=0.13\textwidth]{images/out_of_distr/violaine6/facenet.jpg} & 
        \includegraphics[width=0.13\textwidth]{images/out_of_distr/violaine6/FROM.jpg} & 
        \includegraphics[width=0.13\textwidth]{images/out_of_distr/violaine6/insightface.jpg} \\
        \\[-0.46cm]
        \includegraphics[width=0.13\textwidth]{images/original/Violaine4.jpg} & 
        \includegraphics[width=0.13\textwidth]{images/out_of_distr/violaine4/AdaFace.jpg} & 
        \includegraphics[width=0.13\textwidth]{images/out_of_distr/violaine4/arcface.jpg} & 
        \includegraphics[width=0.13\textwidth]{images/out_of_distr/violaine4/facenet.jpg} & 
        \includegraphics[width=0.13\textwidth]{images/out_of_distr/violaine4/FROM.jpg} & 
        \includegraphics[width=0.13\textwidth]{images/out_of_distr/violaine4/insightface.jpg} \\
        \\[-0.46cm]
        \includegraphics[width=0.13\textwidth]{images/original/Ifemelu.jpg} & 
        \includegraphics[width=0.13\textwidth]{images/out_of_distr/ifemelu/AdaFace.jpg} & 
        \includegraphics[width=0.13\textwidth]{images/out_of_distr/ifemelu/arcface.jpg} & 
        \includegraphics[width=0.13\textwidth]{images/out_of_distr/ifemelu/facenet.jpg} & 
        \includegraphics[width=0.13\textwidth]{images/out_of_distr/ifemelu/FROM.jpg} & 
        \includegraphics[width=0.13\textwidth]{images/out_of_distr/ifemelu/insightface.jpg} \\
        \\[-0.46cm]
        \includegraphics[width=0.13\textwidth]{images/original/Render.jpg} & 
        \includegraphics[width=0.13\textwidth]{images/out_of_distr/render/AdaFace.jpg} & 
        \includegraphics[width=0.13\textwidth]{images/out_of_distr/render/arcface.jpg} & 
        \includegraphics[width=0.13\textwidth]{images/out_of_distr/render/facenet.jpg} & 
        \includegraphics[width=0.13\textwidth]{images/out_of_distr/render/FROM.jpg} & 
        \includegraphics[width=0.13\textwidth]{images/out_of_distr/render/insightface.jpg} \\
        \\[-0.46cm]
        \includegraphics[width=0.13\textwidth]{images/original/Celia.jpg} & 
        \includegraphics[width=0.13\textwidth]{images/out_of_distr/celia/AdaFace.jpg} & 
        \includegraphics[width=0.13\textwidth]{images/out_of_distr/celia/arcface.jpg} & 
        \includegraphics[width=0.13\textwidth]{images/out_of_distr/celia/facenet.jpg} & 
        \includegraphics[width=0.13\textwidth]{images/out_of_distr/celia/FROM.jpg} & 
        \includegraphics[width=0.13\textwidth]{images/out_of_distr/celia/insightface.jpg} \\
        
        \multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Original \\ image \end{tabular}} & \multirow{2}{*}{AdaFace~\cite{adaface}} & \multirow{2}{*}{ArcFace~\cite{arcface, gaussian_sampling}} & \multirow{2}{*}{FaceNet~\cite{facenet, facenet_pytorch}} &  \multirow{2}{*}{FROM~\cite{from}} & \multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Insight- \\ Face~\cite{insightface} \end{tabular}} \\
        \\
    \end{tabular}
    }
    \addtolength{\tabcolsep}{5pt}
    \caption{Robustness of ID vectors from different state-of-the-art face recognition models for out-of-distribution samples. Note that the same seed was used for all images to obtain the most fair results.}
    \label{fig:out_of_distr}
\end{figure*}

\clearpage

\subsection{Identity interpolations}

By interpolating between two ID vectors, our method can produce new, intermediate identities, as shown in the main paper and in \cref{fig:interpolation}. This empirically demonstrates that the latent spaces of most face recognition methods are fairly well-structured. Note that we use spherical linear interpolation because it produces slightly smoother results compared to linear interpolation.


\begin{figure*}[htpb]
\centering
\begin{subfigure}{0.95\textwidth}
\centering
    \addtolength{\tabcolsep}{-2pt}
    \small{
    \begin{tabular}{lc}
    AdaFace~\cite{adaface} & \raisebox{-.5\height}{\adjincludegraphics[width=.85\textwidth, trim={0 {.75\height} 0 0}, clip]{images/interpolations/sally_lucas/AdaFace.jpg}} \\
    \\[-0.35cm]
    ArcFace~\cite{arcface, gaussian_sampling} & \raisebox{-.5\height}{\adjincludegraphics[width=.85\textwidth, trim={0 {.75\height} 0 0}, clip]{images/interpolations/sally_lucas/arcface.jpg}} \\
    \\[-0.35cm]
    FaceNet~\cite{facenet, facenet_pytorch} & \raisebox{-.5\height}{\adjincludegraphics[width=.85\textwidth, trim={0 {.75\height} 0 0}, clip]{images/interpolations/sally_lucas/facenet.jpg}} \\
    \\[-0.35cm]
    FROM~\cite{from} & \raisebox{-.5\height}{\adjincludegraphics[width=.85\textwidth, trim={0 {.75\height} 0 0}, clip]{images/interpolations/sally_lucas/FROM.jpg}} \\
    \\[-0.35cm]
    InsightFace~\cite{insightface} & \raisebox{-.5\height}{\adjincludegraphics[width=.85\textwidth, trim={0 {.75\height} 0 0}, clip]{images/interpolations/sally_lucas/insightface.jpg}} \\
    \end{tabular}
    }
    \addtolength{\tabcolsep}{2pt}
    \caption{Identity 1 $\longleftrightarrow$ Identity 2}
    \vspace{5mm}
\end{subfigure}
\begin{subfigure}{0.95\textwidth}
\centering
    \addtolength{\tabcolsep}{-2pt}
    \small{
    \begin{tabular}{lc}
    AdaFace~\cite{adaface} & \raisebox{-.5\height}{\adjincludegraphics[width=.85\textwidth, trim={0 {.75\height} 0 0}, clip]{images/interpolations/sandra_xianyao/AdaFace.jpg}} \\
    \\[-0.35cm]
    ArcFace~\cite{arcface, gaussian_sampling} & \raisebox{-.5\height}{\adjincludegraphics[width=.85\textwidth, trim={0 {.75\height} 0 0}, clip]{images/interpolations/sandra_xianyao/arcface.jpg}} \\
    \\[-0.35cm]
    FaceNet~\cite{facenet, facenet_pytorch} & \raisebox{-.5\height}{\adjincludegraphics[width=.85\textwidth, trim={0 {.75\height} 0 0}, clip]{images/interpolations/sandra_xianyao/facenet.jpg}} \\
    \\[-0.35cm]
    FROM~\cite{from} & \raisebox{-.5\height}{\adjincludegraphics[width=.85\textwidth, trim={0 {.75\height} 0 0}, clip]{images/interpolations/sandra_xianyao/FROM.jpg}} \\
    \\[-0.35cm]
    InsightFace~\cite{insightface} & \raisebox{-.5\height}{\adjincludegraphics[width=.85\textwidth, trim={0 {.75\height} 0 0}, clip]{images/interpolations/sandra_xianyao/insightface.jpg}} \\
    \end{tabular}
    }
    \addtolength{\tabcolsep}{2pt}
    \caption{Identity 3 $\longleftrightarrow$ Identity 4}
\end{subfigure}
    \caption{Identity interpolations for two pairs of identities using different ID vectors.}
    \label{fig:interpolation}
\end{figure*}

\clearpage

\subsection{Principal component analysis}

To analyze the most prominent axes within the latent space, we perform principal component analysis (PCA) on the ID vectors of all $70000$ images of the FFHQ data set for all considered face recognition models. As seen in \cref{fig:pca}, the first principal component appears to mainly encode a person's age while the subsequent components are more entangled and thus less interpretable. Note that we normalize the size of the steps along the PCA directions by the $L_2$ norm of the ID vectors to ensure a similar relative step size for the different ID vectors. Further note that large steps along any direction can cause the resulting latent vector to leave the distribution of plausible ID vectors and can cause artifacts, which is expected.

\begin{figure*}[htpb]
\centering
\begin{subfigure}{0.45\textwidth}
    \addtolength{\tabcolsep}{-4pt}
    \small{
    \begin{tabular}{llc}
    AdaFace~\cite{adaface} & \raisebox{-.5\height}{\adjincludegraphics[width=0.7\textwidth, trim={0 {0.7\height} 0 0}, clip]{images/pca_dir/prashanth/AdaFace.jpg}} & \begin{tabular}[c]{@{}l@{}} \\[-0.2cm] 1 \\[0.7cm] 2 \\[0.7cm] 3\end{tabular} \\
    \\[-0.32cm]
    ArcFace~\cite{arcface, gaussian_sampling} & \raisebox{-.5\height}{\adjincludegraphics[width=0.7\textwidth, trim={0 {0.7\height} 0 0}, clip]{images/pca_dir/prashanth/arcface.jpg}} & \begin{tabular}[c]{@{}l@{}} \\[-0.2cm] 1 \\[0.7cm] 2 \\[0.7cm] 3\end{tabular} \\
    \\[-0.32cm]
    FaceNet~\cite{facenet, facenet_pytorch} & \raisebox{-.5\height}{\adjincludegraphics[width=0.7\textwidth, trim={0 {0.7\height} 0 0}, clip]{images/pca_dir/prashanth/facenet.jpg}} & \begin{tabular}[c]{@{}l@{}} \\[-0.2cm] 1 \\[0.7cm] 2 \\[0.7cm] 3\end{tabular} \\
    \\[-0.32cm]
    FROM~\cite{from} & \raisebox{-.5\height}{\adjincludegraphics[width=0.7\textwidth, trim={0 {0.7\height} 0 0}, clip]{images/pca_dir/prashanth/FROM.jpg}} & \begin{tabular}[c]{@{}l@{}} \\[-0.2cm] 1 \\[0.7cm] 2 \\[0.7cm] 3\end{tabular} \\
    \\[-0.32cm]
    InsightFace~\cite{insightface} \hspace{0.2cm} & \raisebox{-.5\height}{\adjincludegraphics[width=0.7\textwidth, trim={0 {0.7\height} 0 0}, clip]{images/pca_dir/prashanth/insightface.jpg}} & \begin{tabular}[c]{@{}l@{}} \\[-0.2cm] 1 \\[0.7cm] 2 \\[0.7cm] 3\end{tabular} \\
    \end{tabular}
    }
    \addtolength{\tabcolsep}{4pt}
    \caption*{\hspace{2.3cm} (a) Identity 1}
\end{subfigure}
\hspace{1cm}
\begin{subfigure}{0.45\textwidth}
    \addtolength{\tabcolsep}{-4pt}
    \small{
    \begin{tabular}{lc}
    \raisebox{-.5\height}{\adjincludegraphics[width=0.7\textwidth, trim={0 {0.7\height} 0 0}, clip]{images/pca_dir/sally/AdaFace.jpg}} & \begin{tabular}[c]{@{}l@{}} \\[-0.2cm] 1 \\[0.7cm] 2 \\[0.7cm] 3\end{tabular} \\
    \\[-0.32cm]
    \raisebox{-.5\height}{\adjincludegraphics[width=0.7\textwidth, trim={0 {0.7\height} 0 0}, clip]{images/pca_dir/sally/arcface.jpg}} & \begin{tabular}[c]{@{}l@{}} \\[-0.2cm] 1 \\[0.7cm] 2 \\[0.7cm] 3\end{tabular} \\
    \\[-0.32cm]
    \raisebox{-.5\height}{\adjincludegraphics[width=0.7\textwidth, trim={0 {0.7\height} 0 0}, clip]{images/pca_dir/sally/facenet.jpg}} & \begin{tabular}[c]{@{}l@{}} \\[-0.2cm] 1 \\[0.7cm] 2 \\[0.7cm] 3\end{tabular} \\
    \\[-0.32cm]
    \raisebox{-.5\height}{\adjincludegraphics[width=0.7\textwidth, trim={0 {0.7\height} 0 0}, clip]{images/pca_dir/sally/FROM.jpg}} & \begin{tabular}[c]{@{}l@{}} \\[-0.2cm] 1 \\[0.7cm] 2 \\[0.7cm] 3\end{tabular} \\
    \\[-0.32cm]
    \raisebox{-.5\height}{\adjincludegraphics[width=0.7\textwidth, trim={0 {0.7\height} 0 0}, clip]{images/pca_dir/sally/insightface.jpg}} & \begin{tabular}[c]{@{}l@{}} \\[-0.2cm] 1 \\[0.7cm] 2 \\[0.7cm] 3\end{tabular} \\
    \end{tabular}
    }
    \addtolength{\tabcolsep}{4pt}
    \caption*{\hspace{-2.3cm} (b) Identity 2}
\end{subfigure}
    \caption{Visualization of the first three principal component analysis axes for two identities using different ID vectors.}
    \label{fig:pca}
\end{figure*}

\clearpage

Rather than traversing along PCA directions, \cref{fig:pca_red} shows how the images change when projecting the ID vectors onto the first $\{1, 2, 4, 8, 16, 32, 64, 128, 256, 512\}$ PCA axes. The main insight from this experiment is that the ID vectors from some face recognition models, such as FaceNet~\cite{facenet, facenet_pytorch}, can be compressed to as few as 64 dimensions without changing the perceived identity while others, such as AdaFace~\cite{adaface}, require all 512 dimensions.

\begin{figure*}[htpb]
\centering
\begin{subfigure}{0.9\textwidth}
\centering
    \addtolength{\tabcolsep}{-2pt}
    \small{
    \begin{tabular}{lc}
    AdaFace~\cite{adaface} & \raisebox{-.5\height}{\adjincludegraphics[width=.85\textwidth, trim={0 {.5\height} 0 {0.25\height}}, clip]{images/pca_red/jakob/AdaFace.jpg}} \\
    \\[-0.32cm]
    ArcFace~\cite{arcface, gaussian_sampling} & \raisebox{-.5\height}{\adjincludegraphics[width=.85\textwidth, trim={0 {.5\height} 0 {0.25\height}}, clip]{images/pca_red/jakob/arcface.jpg}} \\
    \\[-0.32cm]
    FaceNet~\cite{facenet, facenet_pytorch} & \raisebox{-.5\height}{\adjincludegraphics[width=.85\textwidth, trim={0 {.5\height} 0 {0.25\height}}, clip]{images/pca_red/jakob/facenet.jpg}} \\
    \\[-0.32cm]
    FROM~\cite{from} & \raisebox{-.5\height}{\adjincludegraphics[width=.85\textwidth, trim={0 {.5\height} 0 {0.25\height}}, clip]{images/pca_red/jakob/FROM.jpg}} \\
    \\[-0.32cm]
    InsightFace~\cite{insightface} & \raisebox{-.5\height}{\adjincludegraphics[width=.85\textwidth, trim={0 {.5\height} 0 {0.25\height}}, clip]{images/pca_red/jakob/insightface.jpg}} \\
    Number of axes $\rightarrow$ & \begin{tabularx}{0.85\textwidth}{ *{10}{Y} } $1$ & $2$ & $4$ & $8$ & $16$ & $32$ & $64$ & $128$ & $256$ & $512$ \end{tabularx} \\
    \end{tabular}
    }
    \addtolength{\tabcolsep}{2pt}
    \caption{Identity 1}
    \vspace{5mm}
\end{subfigure}
\begin{subfigure}{0.9\textwidth}
\centering
    \addtolength{\tabcolsep}{-2pt}
    \small{
    \begin{tabular}{lc}
    AdaFace~\cite{adaface} & \raisebox{-.5\height}{\adjincludegraphics[width=.85\textwidth, trim={0 {.5\height} 0 {0.25\height}}, clip]{images/pca_red/julia/AdaFace.jpg}} \\
    \\[-0.32cm]
    ArcFace~\cite{arcface, gaussian_sampling} & \raisebox{-.5\height}{\adjincludegraphics[width=.85\textwidth, trim={0 {.5\height} 0 {0.25\height}}, clip]{images/pca_red/julia/arcface.jpg}} \\
    \\[-0.32cm]
    FaceNet~\cite{facenet, facenet_pytorch} & \raisebox{-.5\height}{\adjincludegraphics[width=.85\textwidth, trim={0 {.5\height} 0 {0.25\height}}, clip]{images/pca_red/julia/facenet.jpg}} \\
    \\[-0.32cm]
    FROM~\cite{from} & \raisebox{-.5\height}{\adjincludegraphics[width=.85\textwidth, trim={0 {.5\height} 0 {0.25\height}}, clip]{images/pca_red/julia/FROM.jpg}} \\
    \\[-0.32cm]
    InsightFace~\cite{insightface} & \raisebox{-.5\height}{\adjincludegraphics[width=.85\textwidth, trim={0 {.5\height} 0 {0.25\height}}, clip]{images/pca_red/julia/insightface.jpg}} \\
    Number of axes $\rightarrow$ & \begin{tabularx}{0.85\textwidth}{ *{10}{Y} } $1$ & $2$ & $4$ & $8$ & $16$ & $32$ & $64$ & $128$ & $256$ & $512$ \end{tabularx} \\
    \end{tabular}
    }
    \addtolength{\tabcolsep}{2pt}
    \caption{Identity 2}
\end{subfigure}
    \caption{Visualization of the projections onto the first $\{1, 2, 4, 8, 16, 32, 64, 128, 256, 512\}$ principal component analysis (PCA) axes for two identities using different ID vectors.}
    \label{fig:pca_red}
\end{figure*}

\clearpage

\subsection{Custom directions}

Since the PCA axes are difficult to interpret, we calculate custom directions for each face recognition model as described in the main paper. As the biases of the FFHQ data set used to train our inversion models are the same for all ID vectors (\eg glasses appearing when increasing the age direction), the presence or absence of certain directions in the latent space along which a given feature can be changed give insights about what information is extracted by a given face recognition model. As seen in the main paper and \cref{fig:custom_dir_identity}, directions that are expected to be contained in the ID vector, such as the age and the gender, can be traversed smoothly. Furthermore, directions that may or may not be considered as part of the identity, such as the current look of a person (\eg glasses, hair style, facial hair style), are also commonly contained as seen in the examples with the blond hair color in \cref{fig:custom_dir_identity}. 


\begin{figure*}[htpb]
\centering
\begin{subfigure}{0.55\textwidth}
\centering
    \addtolength{\tabcolsep}{-4pt}
    \small{
    \begin{tabular}{lc}
    AdaFace~\cite{adaface} & \raisebox{-.5\height}{\adjincludegraphics[width=0.7\textwidth, trim={0 {0.875\height} 0 0}, clip]{images/custom_dir/martina/AdaFace.jpg}} \\
    \\[-0.35cm]
    ArcFace~\cite{arcface, gaussian_sampling} & \raisebox{-.5\height}{\adjincludegraphics[width=0.7\textwidth, trim={0 {0.875\height} 0 0}, clip]{images/custom_dir/martina/arcface.jpg}} \\
    \\[-0.35cm]
    FaceNet~\cite{facenet, facenet_pytorch} & \raisebox{-.5\height}{\adjincludegraphics[width=0.7\textwidth, trim={0 {0.875\height} 0 0}, clip]{images/custom_dir/martina/facenet.jpg}} \\
    \\[-0.35cm]
    FROM~\cite{from} & \raisebox{-.5\height}{\adjincludegraphics[width=0.7\textwidth, trim={0 {0.875\height} 0 0}, clip]{images/custom_dir/martina/FROM.jpg}} \\
    \\[-0.35cm]
    InsightFace~\cite{insightface} \hspace{0.2cm} & \raisebox{-.5\height}{\adjincludegraphics[width=0.7\textwidth, trim={0 {0.875\height} 0 0}, clip]{images/custom_dir/martina/insightface.jpg}} \\
    \end{tabular}
    }
    \addtolength{\tabcolsep}{4pt}
    \caption*{\hspace{2.0cm} (a) Identity 1 $|$ Age}
    \vspace{5mm}
\end{subfigure}
\hspace{0.5cm}
\begin{subfigure}{0.385\textwidth}
\centering
    \addtolength{\tabcolsep}{-4pt}
    \small{
    \begin{tabular}{c}
    \raisebox{-.5\height}{\adjincludegraphics[width=\textwidth, trim={0 {.125\height} 0 {0.75\height}}, clip]{images/custom_dir/martina/AdaFace.jpg}} \\
    \\[-0.35cm]
    \raisebox{-.5\height}{\adjincludegraphics[width=\textwidth, trim={0 {.125\height} 0 {0.75\height}}, clip]{images/custom_dir/martina/arcface.jpg}} \\
    \\[-0.35cm]
    \raisebox{-.5\height}{\adjincludegraphics[width=\textwidth, trim={0 {.125\height} 0 {0.75\height}}, clip]{images/custom_dir/martina/facenet.jpg}} \\
    \\[-0.35cm]
    \raisebox{-.5\height}{\adjincludegraphics[width=\textwidth, trim={0 {.125\height} 0 {0.75\height}}, clip]{images/custom_dir/martina/FROM.jpg}} \\
    \\[-0.35cm]
    \raisebox{-.5\height}{\adjincludegraphics[width=\textwidth, trim={0 {.125\height} 0 {0.75\height}}, clip]{images/custom_dir/martina/insightface.jpg}} \\
    \end{tabular}
    }
    \addtolength{\tabcolsep}{4pt}
    \caption*{(b) Identity 1 $|$ Blond hair color}
    \vspace{5mm}
\end{subfigure}
\begin{subfigure}{0.55\textwidth}
\centering
    \addtolength{\tabcolsep}{-4pt}
    \small{
    \begin{tabular}{lc}
    AdaFace~\cite{adaface} & \raisebox{-.5\height}{\adjincludegraphics[width=0.7\textwidth, trim={0 {0.875\height} 0 0}, clip]{images/custom_dir/sandra/AdaFace.jpg}} \\
    \\[-0.35cm]
    ArcFace~\cite{arcface, gaussian_sampling} & \raisebox{-.5\height}{\adjincludegraphics[width=0.7\textwidth, trim={0 {0.875\height} 0 0}, clip]{images/custom_dir/sandra/arcface.jpg}} \\
    \\[-0.35cm]
    FaceNet~\cite{facenet, facenet_pytorch} & \raisebox{-.5\height}{\adjincludegraphics[width=0.7\textwidth, trim={0 {0.875\height} 0 0}, clip]{images/custom_dir/sandra/facenet.jpg}} \\
    \\[-0.35cm]
    FROM~\cite{from} & \raisebox{-.5\height}{\adjincludegraphics[width=0.7\textwidth, trim={0 {0.875\height} 0 0}, clip]{images/custom_dir/sandra/FROM.jpg}} \\
    \\[-0.35cm]
    InsightFace~\cite{insightface} \hspace{0.2cm} & \raisebox{-.5\height}{\adjincludegraphics[width=0.7\textwidth, trim={0 {0.875\height} 0 0}, clip]{images/custom_dir/sandra/insightface.jpg}} \\
    \end{tabular}
    }
    \addtolength{\tabcolsep}{4pt}
    \caption*{\hspace{2.0cm} (c) Identity 2 $|$ Age}
\end{subfigure}
\hspace{0.5cm}
\begin{subfigure}{0.385\textwidth}
\centering
    \addtolength{\tabcolsep}{-4pt}
    \small{
    \begin{tabular}{c}
    \raisebox{-.5\height}{\adjincludegraphics[width=\textwidth, trim={0 {.125\height} 0 {0.75\height}}, clip]{images/custom_dir/sandra/AdaFace.jpg}} \\
    \\[-0.35cm]
    \raisebox{-.5\height}{\adjincludegraphics[width=\textwidth, trim={0 {.125\height} 0 {0.75\height}}, clip]{images/custom_dir/sandra/arcface.jpg}} \\
    \\[-0.35cm]
    \raisebox{-.5\height}{\adjincludegraphics[width=\textwidth, trim={0 {.125\height} 0 {0.75\height}}, clip]{images/custom_dir/sandra/facenet.jpg}} \\
    \\[-0.35cm]
    \raisebox{-.5\height}{\adjincludegraphics[width=\textwidth, trim={0 {.125\height} 0 {0.75\height}}, clip]{images/custom_dir/sandra/FROM.jpg}} \\
    \\[-0.35cm]
    \raisebox{-.5\height}{\adjincludegraphics[width=\textwidth, trim={0 {.125\height} 0 {0.75\height}}, clip]{images/custom_dir/sandra/insightface.jpg}} \\
    \end{tabular}
    }
    \addtolength{\tabcolsep}{4pt}
    \caption*{(d) Identity 2 $|$ Blond hair color}
\end{subfigure}
    \caption{Visualization of custom direction modifications for two identities using different ID vectors for two directions that can be considered to belong to a person's identity.}
    \label{fig:custom_dir_identity}
\end{figure*}

\clearpage

Most interestingly, our method reveals that some directions, such as the pose and emotion of a person, that arguably do not belong to a person's identity can be found for some face recognition models as seen in \cref{fig:custom_dir_nonidentity}. For example, ArcFace~\cite{arcface, gaussian_sampling}, FROM~\cite{from}, and InsightFace~\cite{insightface} seem to (inadvertently) extract pose information as the yaw angle can be controlled somewhat by moving along the corresponding direction in the ID vector latent space. Similarly, the smile appears to be controllable in some small region for all considered ID vectors. Note that the goal of looking at these identity-agnostic directions in the ID vector latent space is not necessarily to control this specific dimension cleanly (this can be achieved with attribute conditioning), but rather to analyze what information is extracted by a given FR method. Thus, our method can be used as a tool to reveal and visualize problems of FR methods that we might not even have been aware of and thus suggest hypotheses for further quantitative experiments.

\begin{figure*}[htpb]
\centering
\begin{subfigure}{0.55\textwidth}
\centering
    \addtolength{\tabcolsep}{-4pt}
    \small{
    \begin{tabular}{lc}
    AdaFace~\cite{adaface} & \raisebox{-.5\height}{\adjincludegraphics[width=0.7\textwidth, trim={0 0 0 {0.875\height}}, clip]{images/custom_dir/martina/AdaFace.jpg}} \\
    \\[-0.35cm]
    ArcFace~\cite{arcface, gaussian_sampling} & \raisebox{-.5\height}{\adjincludegraphics[width=0.7\textwidth, trim={0 0 0 {0.875\height}}, clip]{images/custom_dir/martina/arcface.jpg}} \\
    \\[-0.35cm]
    FaceNet~\cite{facenet, facenet_pytorch} & \raisebox{-.5\height}{\adjincludegraphics[width=0.7\textwidth, trim={0 0 0 {0.875\height}}, clip]{images/custom_dir/martina/facenet.jpg}} \\
    \\[-0.35cm]
    FROM~\cite{from} & \raisebox{-.5\height}{\adjincludegraphics[width=0.7\textwidth, trim={0 0 0 {0.875\height}}, clip]{images/custom_dir/martina/FROM.jpg}} \\
    \\[-0.35cm]
    InsightFace~\cite{insightface} \hspace{0.2cm} & \raisebox{-.5\height}{\adjincludegraphics[width=0.7\textwidth, trim={0 0 0 {0.875\height}}, clip]{images/custom_dir/martina/insightface.jpg}} \\
    \end{tabular}
    }
    \addtolength{\tabcolsep}{4pt}
    \caption*{\hspace{2.0cm} (a) Identity 1 $|$ Yaw angle}
    \vspace{5mm}
\end{subfigure}
\hspace{0.5cm}
\begin{subfigure}{0.385\textwidth}
\centering
    \addtolength{\tabcolsep}{-4pt}
    \small{
    \begin{tabular}{c}
    \raisebox{-.5\height}{\adjincludegraphics[width=\textwidth, trim={0 {.5\height} 0 {0.375\height}}, clip]{images/custom_dir/martina/AdaFace.jpg}} \\
    \\[-0.35cm]
    \raisebox{-.5\height}{\adjincludegraphics[width=\textwidth, trim={0 {.5\height} 0 {0.375\height}}, clip]{images/custom_dir/martina/arcface.jpg}} \\
    \\[-0.35cm]
    \raisebox{-.5\height}{\adjincludegraphics[width=\textwidth, trim={0 {.5\height} 0 {0.375\height}}, clip]{images/custom_dir/martina/facenet.jpg}} \\
    \\[-0.35cm]
    \raisebox{-.5\height}{\adjincludegraphics[width=\textwidth, trim={0 {.5\height} 0 {0.375\height}}, clip]{images/custom_dir/martina/FROM.jpg}} \\
    \\[-0.35cm]
    \raisebox{-.5\height}{\adjincludegraphics[width=\textwidth, trim={0 {.5\height} 0 {0.375\height}}, clip]{images/custom_dir/martina/insightface.jpg}} \\
    \end{tabular}
    }
    \addtolength{\tabcolsep}{4pt}
    \caption*{(b) Identity 1 $|$ Smile}
    \vspace{5mm}
\end{subfigure}
\begin{subfigure}{0.55\textwidth}
\centering
    \addtolength{\tabcolsep}{-4pt}
    \small{
    \begin{tabular}{lc}
    AdaFace~\cite{adaface} & \raisebox{-.5\height}{\adjincludegraphics[width=0.7\textwidth, trim={0 0 0 {0.875\height}}, clip]{images/custom_dir/sandra/AdaFace.jpg}} \\
    \\[-0.35cm]
    ArcFace~\cite{arcface, gaussian_sampling} & \raisebox{-.5\height}{\adjincludegraphics[width=0.7\textwidth, trim={0 0 0 {0.875\height}}, clip]{images/custom_dir/sandra/arcface.jpg}} \\
    \\[-0.35cm]
    FaceNet~\cite{facenet, facenet_pytorch} & \raisebox{-.5\height}{\adjincludegraphics[width=0.7\textwidth, trim={0 0 0 {0.875\height}}, clip]{images/custom_dir/sandra/facenet.jpg}} \\
    \\[-0.35cm]
    FROM~\cite{from} & \raisebox{-.5\height}{\adjincludegraphics[width=0.7\textwidth, trim={0 0 0 {0.875\height}}, clip]{images/custom_dir/sandra/FROM.jpg}} \\
    \\[-0.35cm]
    InsightFace~\cite{insightface} \hspace{0.2cm} & \raisebox{-.5\height}{\adjincludegraphics[width=0.7\textwidth, trim={0 0 0 {0.875\height}}, clip]{images/custom_dir/sandra/insightface.jpg}} \\
    \end{tabular}
    }
    \addtolength{\tabcolsep}{4pt}
    \caption*{\hspace{2.0cm} (c) Identity 2 $|$ Yaw angle}
\end{subfigure}
\hspace{0.5cm}
\begin{subfigure}{0.385\textwidth}
\centering
    \addtolength{\tabcolsep}{-4pt}
    \small{
    \begin{tabular}{lc}
    \raisebox{-.5\height}{\adjincludegraphics[width=\textwidth, trim={0 {.5\height} 0 {0.375\height}}, clip]{images/custom_dir/sandra/AdaFace.jpg}} \\
    \\[-0.35cm]
    \raisebox{-.5\height}{\adjincludegraphics[width=\textwidth, trim={0 {.5\height} 0 {0.375\height}}, clip]{images/custom_dir/sandra/arcface.jpg}} \\
    \\[-0.35cm]
    \raisebox{-.5\height}{\adjincludegraphics[width=\textwidth, trim={0 {.5\height} 0 {0.375\height}}, clip]{images/custom_dir/sandra/facenet.jpg}} \\
    \\[-0.35cm]
    \raisebox{-.5\height}{\adjincludegraphics[width=\textwidth, trim={0 {.5\height} 0 {0.375\height}}, clip]{images/custom_dir/sandra/FROM.jpg}} \\
    \\[-0.35cm]
    \raisebox{-.5\height}{\adjincludegraphics[width=\textwidth, trim={0 {.5\height} 0 {0.375\height}}, clip]{images/custom_dir/sandra/insightface.jpg}} \\
    \end{tabular}
    }
    \addtolength{\tabcolsep}{4pt}
    \caption*{(d) Identity 2 $|$ Smile}
\end{subfigure}
    \caption{Visualization of custom direction modifications for two identities using different ID vectors for two directions that arguably do not belong to a person's identity. Our method reveals that many face recognition methods inadvertently extract identity-agnostic information such as the pose and emotion.}
    \label{fig:custom_dir_nonidentity}
\end{figure*}

\clearpage
