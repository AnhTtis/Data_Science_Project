\section{Experiments and results} \label{sec:experiments}

\subsection{Comparison to state-of-the-art methods}

We mainly compare our model with the three methods that generate faces from black-box features whose code is available online: NbNet~\cite{nbnet} (``vgg-percept-nbnetb'' parameters), Gaussian sampling~\cite{gaussian_sampling}, and StyleGAN search~\cite{stylegan-search}. 

\Cref{fig:qualitative_comp} compares the outputs of our method with those of current state-of-the-art methods. While capturing the identity of the input face well in some cases, NbNet~\cite{nbnet} and Gaussian sampling~\cite{gaussian_sampling} both fail to produce realistic faces. In contrast, StyleGAN search~\cite{stylegan-search} always generates high-quality images, but they are not always faithful to the original identity, sometimes failing completely 
as seen in the last row. Our method is the only method that produces high-quality, realistic images that consistently resemble the original identity. Our observations are supported by the user study in the supplementary material.


\begin{figure}[htpb]
    \centering
    \addtolength{\tabcolsep}{-5.5pt}
    \footnotesize{
    \begin{tabular}{ccccc}
        \includegraphics[width=0.09\textwidth]{images/original/Jakob.jpg} & 
        \includegraphics[width=0.09\textwidth]{images/qualitative_comp/jakob/nbnet.jpg} & 
        \includegraphics[width=0.09\textwidth]{images/qualitative_comp/jakob/bbox.jpg} & 
        \includegraphics[width=0.09\textwidth]{images/qualitative_comp/jakob/stylegan_0.jpg} & 
        \includegraphics[width=0.09\textwidth]{images/qualitative_comp/jakob/ours.jpg} \\
        \\[-0.4cm]
        \includegraphics[width=0.09\textwidth]{images/original/Martina.jpg} & 
        \includegraphics[width=0.09\textwidth]{images/qualitative_comp/martina/nbnet.jpg} & 
        \includegraphics[width=0.09\textwidth]{images/qualitative_comp/martina/bbox.jpg} & 
        \includegraphics[width=0.09\textwidth]{images/qualitative_comp/martina/stylegan_0.jpg} & 
        \includegraphics[width=0.09\textwidth]{images/qualitative_comp/martina/ours.jpg} \\
        \\[-0.4cm]
        \includegraphics[width=0.09\textwidth]{images/original/Prashanth.jpg} & 
        \includegraphics[width=0.09\textwidth]{images/qualitative_comp/prashanth/nbnet.jpg} & 
        \includegraphics[width=0.09\textwidth]{images/qualitative_comp/prashanth/bbox.jpg} & 
        \includegraphics[width=0.09\textwidth]{images/qualitative_comp/prashanth/stylegan_0.jpg} & 
        \includegraphics[width=0.09\textwidth]{images/qualitative_comp/prashanth/ours.jpg} \\
        \\[-0.4cm]
        \includegraphics[width=0.09\textwidth]{images/original/Julia.jpg} & 
        \includegraphics[width=0.09\textwidth]{images/qualitative_comp/julia/nbnet.jpg} & 
        \includegraphics[width=0.09\textwidth]{images/qualitative_comp/julia/bbox.jpg} & 
        \includegraphics[width=0.09\textwidth]{images/qualitative_comp/julia/stylegan_0.jpg} & 
        \includegraphics[width=0.09\textwidth]{images/qualitative_comp/julia/ours.jpg} \\
        \\[-0.4cm]
        \includegraphics[width=0.09\textwidth]{images/original/Yang.jpg} & 
        \includegraphics[width=0.09\textwidth]{images/qualitative_comp/yang/nbnet.jpg} & 
        \includegraphics[width=0.09\textwidth]{images/qualitative_comp/yang/bbox.jpg} & 
        \includegraphics[width=0.09\textwidth]{images/qualitative_comp/yang/stylegan_0.jpg} & 
        \includegraphics[width=0.09\textwidth]{images/qualitative_comp/yang/ours.jpg} \\

        \multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Original \\ image \end{tabular}} & \multirow{2}{*}{NbNet~\cite{nbnet}} & \multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Gaussian \\ sampling~\cite{gaussian_sampling} \end{tabular}} & \multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}StyleGAN \\ search~\cite{stylegan-search} \end{tabular}} & \multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}ID3PM \\ (Ours) \end{tabular}} \\
        \\
    \end{tabular}
    }
    \addtolength{\tabcolsep}{5.5pt}
    \caption{Qualitative evaluation with state-of-the-art methods. The generated images of our method (with InsightFace~\cite{insightface} ID vectors) look realistic and resemble the identity of the original image more closely than other methods. Note that the second-best performing method, StyleGAN search~\cite{stylegan-search}, often fails completely as seen in the last row.}
    \label{fig:qualitative_comp}
\end{figure}

For the quantitative evaluation of the identity preservation, we generate one image from each ID vector of all 1000 images of the FFHQ~\cite{stylegan} test set for each method. We then calculate the distances according to the ArcFace~\cite{arcface, arcface_torch} and FaceNet~\cite{facenet, facenet_tf} face recognition methods for the $1000$ respective pairs. The resulting distance distributions are plotted in \cref{fig:plot_ffhq}. Note that StyleGAN search~\cite{stylegan-search} optimizes the FaceNet distance during the image generation and thus performs well when evaluated with FaceNet but poorly when evaluated with ArcFace. The opposite effect can be seen for Gaussian sampling, which optimizes ArcFace during image generation. Despite not optimizing the ID vector distance directly (neither during training nor inference), our method outperforms all other methods, producing images that are closer to the original images' identities.

\input{sections/ffhq_dist_plots_merged}


\begin{table*}[htbp]
\centering
    \small{
    \begin{tabular}{llcclcclcc}
    \toprule
    \multirow{2}{*}{Method} &  & \multicolumn{2}{c}{LFW} &  & \multicolumn{2}{c}{AgeDB-30} &  & \multicolumn{2}{c}{CFP-FP} \\
    \cmidrule{3-4} \cmidrule{6-7} \cmidrule{9-10}
    & & {ArcFace $\uparrow$} & {FaceNet $\uparrow$} & & {ArcFace $\uparrow$} & {FaceNet $\uparrow$} & & {ArcFace $\uparrow$} & {FaceNet $\uparrow$} \\
    \midrule
    Real images & & 99.83\% & 99.65\% & & 98.23\% & 91.33\% & & 98.86\% & 96.43\% \\
    \midrule
    NbNet~\cite{nbnet} & & 87.32\% & 92.48\% & & 81.83\% & 82.25\% & & 87.36\% & 89.89\% \\
    Gaussian sampling~\cite{gaussian_sampling} & & 89.10\% & 75.07\% & & 80.43\% & 63.42\% & & 61.39\% & 55.26\% \\
    StyleGAN search~\cite{stylegan-search} & & 82.43\% & 95.45\% & & 72.70\% & 85.22\% & & 80.83\% & 92.54\% \\
    Vec2Face~\cite{vec2face} $^1$ & & 99.13\% & 98.05\% & & 93.53\% & \textbf{89.80\%} & & 89.03\% & 87.19\% \\
    \midrule
    ID3PM (Ours, FaceNet~\cite{facenet, facenet_pytorch}) & & 97.65\% & \textbf{98.98\%} & & 88.22\% & 88.00\% & & 94.47\% & \textbf{95.23\%} \\
    ID3PM (Ours, InsightFace~\cite{insightface}) & & \textbf{99.20\%} & 96.02\% & & \textbf{94.53\%} & 79.15\% & & \textbf{96.13\%} & 87.43\% \\
    \bottomrule
    \end{tabular}
    }
\caption{Quantitative evaluation of the identity preservation with state-of-the-art methods. The scores depict the matching accuracy when replacing one image of each positive pair with the image generated from its ID vector for the protocols of the LFW~\cite{lfw}, AgeDB-30~\cite{agedb}, and CFP-FP~\cite{cfpfp} data sets. The best performing method per column is marked in bold. 
$^1$ Values taken from their paper.}
\label{table:face_recog_scores}
\end{table*}

To further evaluate the identity preservation and to compare to Vec2Face~\cite{vec2face} despite their code not being available online, we follow the procedure used in Vec2Face~\cite{vec2face}. Specifically, we use the official validation protocols of the LFW~\cite{lfw}, AgeDB-30~\cite{agedb}, and CFP-FP~\cite{cfpfp} data sets and replace the first image in each positive pair with the image reconstructed from its ID vector, while keeping the second image as the real reference face. The face matching accuracies for ArcFace~\cite{arcface, arcface_torch} and FaceNet~\cite{facenet, facenet_tf} are reported in \cref{table:face_recog_scores}. Our method outperforms NbNet~\cite{nbnet}, Gaussian sampling~\cite{gaussian_sampling}, and StyleGAN search~\cite{stylegan-search} in almost all tested configurations and performs on-par with or better than Vec2Face~\cite{vec2face}. 
Note that our method has fewer requirements for the training data set ($70000$ images vs. $490000$ images grouped into $10000$ classes) and produces visually superior results compared to Vec2Face~\cite{vec2face}, as confirmed in the user study in the supplementary material.

To evaluate the diversity of the generated results, we generate $100$ images for the first $50$ identities of the FFHQ~\cite{stylegan} test set. Motivated by the diversity evaluation common in unpaired image-to-image translation literature~\cite{starganv2, drit++}, we calculate the mean pairwise LPIPS~\cite{lpips} distances among all images of the same identity. We further calculate the mean pairwise pose and expressions extracted using 3DDFA\_V2~\cite{3ddfa}. We additionally calculate the mean identity vector distances according to ArcFace~\cite{arcface, arcface_torch} and FaceNet~\cite{facenet, facenet_tf} to measure the identity preservation. We report these values in \cref{table:diversity_eval}.

Since NbNet~\cite{nbnet} is a one-to-one method and Gaussian sampling~\cite{gaussian_sampling} produces faces that often fail to be detected by 3DDFA\_V2~\cite{3ddfa}, we only compare with StyleGAN search~\cite{stylegan-search}. In our default configuration (marked with $^*$ in \cref{table:diversity_eval}), we obtain similar diversity scores as StyleGAN search~\cite{stylegan-search}, while preserving the identity much better. 
Note that the diversity scores are slightly skewed in favor of methods whose generated images do not match the identity closely since higher variations in the identity also lead to more diversity in the LPIPS~\cite{lpips} features. 

\begin{table*}[htbp]
\centering
    \addtolength{\tabcolsep}{-2pt}
    \small{
    \begin{tabular}{lllS[table-format=2.2]S[table-format=1.2]S[table-format=1.3]lS[table-format=1.3]S[table-format=1.3]}
    \toprule
    \multirow{2}{*}{Method} & \multirow{2}{*}{Setting} & & \multicolumn{3}{c}{Diversity} &  & \multicolumn{2}{c}{Identity distance} \\ 
    \cmidrule{4-6} \cmidrule{8-9}
    & & & {Pose $\uparrow$} & {Expression $\uparrow$} & {LPIPS $\uparrow$} & & {ArcFace $\downarrow$} & {FaceNet $\downarrow$} \\
    \midrule
    StyleGAN search~\cite{stylegan-search} & {-} & & 12.57 & \textbf{1.57} & \textbf{0.317} & & 0.417 & 0.215 \\
    \midrule
    \multirow{5}{*}{\begin{tabular}[l]{@{}l@{}}ID3PM (Ours) \end{tabular}} & Guidance scale = 1.0 & & \textbf{17.36} & 1.35 & 0.315 & & 0.291 & 0.234 \\
    & \phantom{Guidance scale =} 1.5 & & 16.69 & 1.18 & 0.301 & & 0.260 & 0.211 \\
    & \phantom{Guidance scale =} 2.0 $^*$ & & 16.24 & 1.10 & 0.290 & & 0.247 & 0.203 \\
    & \phantom{Guidance scale =} 2.5 & & 15.88 & 1.05 & 0.282 & & 0.242 & 0.201 \\
    & \phantom{Guidance scale =} 3.0 & & 15.55 & 1.01 & 0.274 & & \textbf{0.239} & \textbf{0.200} \\
    \midrule
    \multirow{1}{*}{\begin{tabular}[l]{@{}l@{}}ID3PM (Ours) \end{tabular}} & Attribute conditioning & & 16.93 & 1.45 & 0.306 & & 0.302 & 0.252\\
    \bottomrule
    \end{tabular}
    }
    \addtolength{\tabcolsep}{2pt}
\caption{Quantitative evaluation of the diversity and identity distances of $100$ generated images for $50$ identities with StyleGAN search~\cite{stylegan-search}, different guidance scales, and attribute conditioning (set 1). InsightFace~\cite{insightface} ID vectors are used for our methods in this experiment. The best performing method per column is marked in bold.
$^*$ Indicates the default setting used in this paper and also for the run with attribute conditioning.}
\label{table:diversity_eval}
\end{table*}

\subsection{Controllability}

\subsubsection{Guidance scale}

The classifier-free guidance scale offers control over the trade-off between the fidelity and diversity of the generated results. As seen in \cref{fig:qualitative_guidance}, by increasing the guidance, the generated faces converge to the same identity, resemble the original face more closely, and contain fewer artifacts. At the same time, higher guidance values reduce the diversity of identity-agnostic features such as the background and expressions and also increase contrast and saturation. 

\begin{figure}[htbp]
\centering
    \addtolength{\tabcolsep}{-5pt}
    \small{
    \begin{tabular}{cc}
    \raisebox{1.1\height}{\includegraphics[width=0.075\textwidth]{images/original/Sandra.jpg}} & 
    \adjincludegraphics[width=0.4\textwidth, trim={0 0 0 {0.25\height}}, clip]{images/guidance_scale/guidance_sandra_insightface_compressed.jpg} \\
    $s$ & \begin{tabularx}{0.375\textwidth}{ *{5}{Y} } $1.0$ & $2.0$ & $3.0$ & $4.0$ & $5.0$ \end{tabularx} \\
    \end{tabular}
    }
    \addtolength{\tabcolsep}{5pt}
\caption{Effect of the guidance scale on the generated images. For the (InsightFace~\cite{insightface}) ID vector extracted from the image on the left, we generate images for four seeds at guidance scales $s$ ranging from $1.0$ to $5.0$.}
\label{fig:qualitative_guidance}
\end{figure}

To measure this effect quantitatively, we perform the same evaluation as in the previous section and report the results in \cref{table:diversity_eval}. As the guidance scale increases, the identity preservation improves as indicated by the decreasing identity distances, but the diversity in terms of poses, expressions, and LPIPS~\cite{lpips} features decreases. In practice, we choose a guidance scale of $2.0$ for all experiments unless stated otherwise because that appears to be the best compromise between image quality and diversity. In the supplementary material, we further show FID~\cite{fid} as well as precision and recall~\cite{recall_precision} values that measure how well the image distribution is preserved as the guidance scale varies.

\subsubsection{Identity vector latent space}

As described in \cref{sec:method}, we can find custom directions in the ID vector latent space that enable us to smoothly interpolate identities as well as change features such as the age or hair color as seen in \cref{fig:teaser_small} and in the supplementary material. Note that we refer to these features as \emph{identity-specific} because they exist in the ID vector latent space. In theory, this space should not contain any identity-agnostic information such as the pose. In practice, however, some FR methods inadvertently do extract this information. This is shown in great detail in the supplementary material, where we show an interesting application of our method to analyze pre-trained face recognition methods.

\subsubsection{Attribute conditioning}

By additionally conditioning our method on attributes, we can disentangle identity-specific and identity-agnostic features. As seen in \cref{fig:attribute_cond_diversity}, the additional attribute conditioning allows us to recover more of the original data distribution in terms of head poses and expressions whereas a model conditioned only on the ID vector is more likely to overfit and learn biases from the training data set. This is also shown in \cref{table:diversity_eval}, where the diversity increases with attribute conditioning at the expense of worse identity preservation compared to the base configuration. The attribute conditioning also enables intuitive control over the generated images by simply selecting the desired attribute values as shown in \cref{fig:teaser_small} and in the supplementary material.

\begin{figure}[htpb]
    \centering
    \addtolength{\tabcolsep}{-4pt}
    \small{
    \begin{tabular}{lc}
    ID & \raisebox{-.45\height}{\adjincludegraphics[width=.38\textwidth, trim={0 0 0 0}, clip]{images/attribute_cond_diversity/Markus_256_sorted_noatt.jpg}} \\
    \\[-0.35cm]
    ID + Set 1 & \raisebox{-.45\height}{\adjincludegraphics[width=.38\textwidth, trim={0 0 0 0}, clip]{images/attribute_cond_diversity/Markus_256_sorted_notidentity.jpg}} \\
    \end{tabular}
    }
    \addtolength{\tabcolsep}{4pt}
    \caption{Attribute conditioning diversity. Through additional attribute conditioning, we can disentangle identity-specific and identity-agnostic features. As a result, we obtain more diverse results when using both (InsightFace~\cite{insightface}) ID vector and attribute vector conditioning (set 1) compared to when only using ID vector conditioning.}
    \label{fig:attribute_cond_diversity}
\end{figure}
