\section{Motivation} \label{sec:motivation}

\subsection{Inverse problems}

In a system under study, we often have a \emph{forward problem} or function $f$ that corresponds to a set of observations $\mb{y} \sim \mathcal{Y}$.  The function $f$ has input arguments $\mb{x}$ and a set of parameters $\bm{\theta}$, such that $f(\mb{x}; \bm{\theta}) = \mb{y}$.  An \emph{inverse problem} seeks to reverse this process and make inferences about the values of $\mb{x}$ or $\bm{\theta}$ given the observations $\mb{y}$. For the application explored in this work, $f$ is a face recognition model that takes an image $\mb{x}$ as input and produces an ID vector $\mb{y}$.

When the function $f$ is not bijective, no inverse exists in the traditional mathematical sense.  However, it is possible to generalize our concept of what an inverse is to accommodate the problem of model inversion, namely by considering an inverse to be the set of pre-images of the function $f$ that map $\epsilon$-close to the target $\mb{y}$.  For bijective $f$, this corresponds to the traditional inverse for $\epsilon = 0$.


\begin{figure*}[htbp]
    \centering
    \includegraphics[trim = 14mm 23mm 69mm 84mm, clip, width=0.98\linewidth]{images/architecture}
    \caption{Method architecture. Given an image of a source identity, the identity conditioning module extracts the ID vector with a black-box, pre-trained face recognition network. This is projected with a fully connected layer and added to the time step embedding which is injected into the residual blocks of a diffusion model (DM). Starting with Gaussian noise $\mb{x}_T$, the DM iteratively denoises the image to finally obtain the output image $\mb{x}_0$ in $64 \times 64$ resolution. Lastly, the image is upsampled to a resolution of $256 \times 256$ using an unconditional super-resolution DM. The optional attribute conditioning module helps disentangle features and allows intuitive control over attributes such as the pose. Gray components are frozen during training.}
    \label{fig:architecture}
\end{figure*}

\subsubsection{Model inversion with model access} \label{sec:inv_access}

One way to handle the model-inversion problem when $f$ is not bijective is to treat it pointwise, defining a loss, such as 
\begin{equation}
	\mathcal{L} = \frac{1}{2} \| \mb{y} - f(\mb{x}) \|^2, \label{eq:loss}
\end{equation}
and minimizing it via gradient descent on $\mb{x}$ from some starting point $\mb{x}_0$ 
according to
\begin{equation}
	\Delta \mb{x}_t = - \nabla_{\mb{x}} \mathcal{L} = \left( \frac{\partial f}{\partial \mb{x}} \right)^\top \left( \mb{y} - f(\mb{x}) \right). \label{eq:update}
\end{equation}

In common cases where the inverse problem is one-to-many, we can take a statistical approach.  Here we want to sample from $p(\mb{x}|\mb{y})$, which is equivalent to drawing from the pre-image set that defines the inverse $f^{-1}(\mb{y})$.  

However, if we assume a Gaussian observation model 
\begin{equation}
	p(\mb{y} |\mb{x}) = \mathcal{N}(\mb{y}; f(\mb{x}), \sigma^2 \mb{I}) \propto \exp \left( -\frac{\mathcal{L}}{\sigma^2} \right), \label{eq:observation}
\end{equation}
where  the last term follows from \eqref{eq:loss}, then 
we can rewrite equation \eqref{eq:update} as $\Delta \mb{x}_t \propto \sigma^2 \nabla_{\mb{x}} \log p(\mb{y} |\mb{x}_t)$.

This shows that traditional model inversion via gradient descent performs a type of deterministic sampling from $p(\mb{y}|\mb{x})$---and not the distribution we want, $p(\mb{x}|\mb{y})$---by pushing toward modes of $p(\mb{y}|\mb{x})$ close to the initialization point $\mb{x}_0$, regardless of whether it possesses the desired characteristics of the data $p(\mb{x})$.  This can lead to results, such as adversarial examples~\cite{goodfellow2014explaining}, that, while technically satisfying the mathematical criteria of inversion, do not appear to come from $p(\mb{x})$.  

Various types of regularization exist to attempt to avoid this issue, which are most often \emph{ad hoc} methods geared toward the specific problem at hand~\cite{DBLP:journals/corr/ZhmoginovS16, mahendran2015understanding, creswell2018inverting, xia2022gan}.  A more general approach is to introduce regularization terms proportional to the \emph{(Stein) score}, $\nabla_{\mb{x}} \log p(\mb{x})$, since $$\nabla_{\mb{x}} \log p(\mb{x} |\mb{y}) = \nabla_{\mb{x}} \log p(\mb{y} |\mb{x}) + \nabla_{\mb{x}} \log p(\mb{x})$$ provides the \emph{conditional} score needed to sample from $p(\mb{x}|\mb{y})$, the distribution we are actually interested in.

Previous work has shown that diffusion models (DMs) effectively learn the score $\nabla_{\mb{x}} \log p(\mb{x})$, which allows them to be used alongside model gradients to guide sampling~\cite{song2020score, ddpm1, ddpm2, ddpm3, edm}.  When those models are classifiers, the procedure is known as \emph{classifier guidance}~\cite{ddpm3}.  However, this imposes an additional computational burden on sampling and also requires that the model $f$ be differentiable.


\subsubsection{Model inversion without full model access} \label{sec:inv_no_access}

In the case we focus on in this work, we assume to have access only to the values of the function $f$ via some oracle or a lookup table of $(\mb{x}, \mb{y})$ pairs but not its gradient $\nabla f$.  In this case, also referred to as \emph{black-box} setting, we may wish to train a function $g_{\bm{\psi}}$ to learn the inverse by minimizing  
\begin{equation}
	\mathcal{J} = \| \mb{x} - g_{\bm{\psi}}(\mb{y}) \|^2 \label{eq:J-loss}
\end{equation}
across all observed $\{ (\mb{x}, \mb{y}) \}$.  Recalling that $\mb{y} = f(\mb{x})$, we have essentially described an encoder-decoder setup with the encoder frozen and only the decoder being trained, which requires no gradients from the ``encoder'' $f$.

If we consider perturbed data $\tilde{\mb{x}} = \mb{x} + \bm{\epsilon}$, where $\bm{\epsilon} \sim \mathcal{N}(\bm{0}, \sigma^2_t \mb{I})$.  Then \eqref{eq:J-loss} is equivalent to 
\begin{equation}
\begin{split}
	\tilde{\mathcal{J}} &= \| (\tilde{\mb{x}} - \mb{x}) - (\tilde{\mb{x}} - g_{\bm{\psi}}(\mb{y})) \|^2 \\
	&= \| \bm{\epsilon}- \bm{\epsilon}_{\bm{\theta}}(\tilde{\mb{x}}, \mb{y}, t)  \|^2, \label{eq:err_loss}
\end{split}	
\end{equation}
and we are now training a conditional model $\bm{\epsilon}_{\bm{\theta}}$ to learn the noise added to $\mb{x}$ instead of a model $g$ to reconstruct $\mb{x}$.  This new task is exactly the one facing conditional diffusion models (\Cref{sec:diff_form}).

Although we cannot \emph{force} the model to leverage the conditioning on $\mb{y}$ or $t$, if it is to successfully minimize the loss $\tilde{\mathcal{J}}$, it should learn a function proportional to the conditional score.  That is because, by Tweedie's formula~\cite{efron2011tweedie, kim2021noise2score}, 
\begin{equation}
    \begin{split}
        \mathbb{E}[\mb{x}|\tilde{\mb{x}}, \mb{y}] &= \tilde{\mb{x}} + \sigma^2_t \nabla_{\tilde{\mb{x}}} \log p(\tilde{\mb{x}} | \mb{y}) \\
        &\approx \tilde{\mb{x}} - \bm{\epsilon}_{\bm{\theta}}(\tilde{\mb{x}}, \mb{y}, t). \\ 
    \end{split}
\end{equation}
As a result, we can effectively sample from the ``inverse distribution'' $p(\tilde{\mb{x}}|\mb{y})$ via $\bm{\epsilon}_{\bm{\theta}}(\tilde{\mb{x}}, \mb{y}, t)$ using Langevin dynamics~\cite{bussi2007accurate, welling2011bayesian} without having access to the gradients of the model $f$ or any other model-specific loss terms.  %

Intuitively, during training, especially in early denoising steps, it is difficult for the DM to both denoise an image to get a realistic face \underline{and} match the specific training image. The ID vector contains information (\eg face shape) that the DM is incentivized to use ($\rightarrow$ lower loss) to get closer to the training image. 
During inference, the random seed determines identity-agnostic features ($\rightarrow$ many results), and the ID conditioning pushes the DM to generate an image that resembles the target identity.





