\section{Method} \label{sec:method}

Motivated by the results from \cref{sec:motivation}, we adopt a conditional diffusion model (DM) for the task of inverting a face recognition (FR) model.
Since conditional DMs have inherent advantages for one-to-many and inversion tasks, this results in a minimal problem formulation compared to task-specific methods that require complicated supervision~\cite{vec2face} or regularization~\cite{DBLP:journals/corr/ZhmoginovS16} signals.
The overall architecture of our method is visualized in \cref{fig:architecture}.


\subsection{Diffusion model formulation}  \label{sec:diff_form}


We build up on the diffusion model proposed by Dhariwal and Nichol~\cite{ddpm3}. Given a sample $\mb{x}_0$ from the image distribution, a sequence $\mb{x}_1, \mb{x}_2,..., \mb{x}_T$ of noisy images is produced by progressively adding Gaussian noise according to a variance schedule. At the final time step, $\mb{x}_T$ is assumed to be pure Gaussian noise: $\mathcal{N}(0,\mathbf{I})$.
A neural network is then trained to reverse this diffusion process in order to predict $\mb{x}_{t-1}$ from the noisy image $\mb{x}_t$ and the time step $t$. 
To sample a new image, we sample $\mb{x}_T \sim \mathcal{N}(0,\mathbf{I})$ and iteratively denoise it, producing a sequence $\mb{x}_T, \mb{x}_{T-1}, \ldots, \mb{x}_1, \mb{x}_0$. The final image, $\mb{x}_0$, should resemble the training data.

As \cite{ddpm3}, we assume that we can model $p_{\bm{\theta}}(\mb{x}_{t-1} | \mb{x}_t)$ as a Gaussian $\mathcal{N}(\mb{x}_{t-1}; \mb{\mu}_{\bm{\theta}}(\mb{x}_t,t), \bm{\Sigma}_{\bm{\theta}}(\mb{x}_t,t))$ whose mean $\mb{\mu}_{\bm{\theta}}(\mb{x}_t,t)$ can be calculated as a function of $\bm{\epsilon}_{\bm{\theta}}(\mb{x}_t,t)$, the (unscaled) noise component of $\mb{x}_t$. We extend this by conditioning on the ID vector $\mb{y}$ and thus predict $\bm{\epsilon}_{\bm{\theta}}(\mb{x}_t,\mb{y},t)$.
Extending \cite{ddpm2} to the conditional case, we predict the noise $\bm{\epsilon}_{\bm{\theta}}(\mb{x}_t,\mb{y},t)$ and the variance $\bm{\Sigma}_{\bm{\theta}}(\mb{x}_t,\mb{y},t)$ from the image $\mb{x}_t$, the ID vector $\mb{y}$, and the time step $t$, using the objective
\begin{equation}
    \mathcal{L}_{\text{simple}} = \mathbb{E}_{t,\mb{x}_0,\mb{y},\bm{\epsilon}}[||\bm{\epsilon} - \bm{\epsilon}_{\bm{\theta}}(\mb{x}_t,\mb{y},t)||^2].
\end{equation}
For more details, refer to the diffusion model works~\cite{ddpm1, ddpm2, ddpm3}. Note that this objective is identical to the one theoretically derived in~\eqref{eq:err_loss}.  While some recent work has considered the application of diffusion models to inverse problems, they typically assume $p(\mb{y}|\mb{x})$ is known~\cite{kadkhodaie2021stochastic, DDRM, PiGDM, DPS, MCG, plugandplay, BlindDPS, CCDF, ILVR, RED_Diff, song2020score}, while we make no such assumption.

Following Ramesh \etal~\cite{dalle2}, we adapt classifier-free guidance~\cite{classifierfree} by setting the ID vector to the $\mb{0}$-vector with $10\%$ probability during training (unconditional setting). During inference, we sample from both settings, and the model prediction $\hat{\bm{\epsilon}}_\theta$ becomes
\begin{equation}
    \hat{\bm{\epsilon}}_{\bm{\theta}}(\mb{x}_t, \mb{y}, t) = \bm{\epsilon}_{\bm{\theta}}(\mb{x}_t, \mb{0}, t) + s [\bm{\epsilon}_{\bm{\theta}}(\mb{x}_t, \mb{y}, t) - \bm{\epsilon}_{\bm{\theta}}(\mb{x}_t, \mb{0}, t)],
\end{equation}
where $s \geq 1$ is the guidance scale. Higher guidance scales cause the generation process to consider the identity conditioning more.

\subsection{Architecture}

The model is a U-net~\cite{unet} that takes the image $\mb{x}_t$, the ID vector $\mb{y}$, and the time step $t$ as input. The U-net architecture is adapted from \cite{ddpm3} and is described in detail in the supplementary material.
To condition the diffusion model on the identity, we add an identity embedding to the residual connections of the ResNet blocks, as commonly done for class embeddings~\cite{ddpm3} and the CLIP~\cite{clip} embedding in text-to-image generation methods~\cite{dalle2, imagen}. The identity embedding is obtained by projecting the ID vector through a learnable fully connected layer such that it has the same size as the time step embedding and can be added to it.

\subsection{Controllability}

Due to its robustness and ability to pick a mode by setting the random seed during image generation, our method permits smooth interpolations and analyses in the ID vector latent space unlike other works that invert FR models. For example, we can smoothly interpolate between different identities as visualized in \cref{fig:teaser_small}. Furthermore, we can find meaningful directions in the latent spaces. Since the directions extracted automatically using principal component analysis (PCA) are generally difficult to interpret beyond the first dimension (see supplementary material), we calculate custom directions using publicly available metadata~\cite{ffhq_metadata} for the FFHQ data set. For binary features (\eg glasses), we define the custom direction vector as the difference between the mean ID vectors of the two groups. For continuous features (\eg age), we map to the binary case by considering ID vectors with feature values below the $10$th percentile and values above the $90$th percentile for the two groups respectively. 
Examples of traveling along meaningful ID vector directions can be seen in \cref{fig:teaser_small}.


To better disentangle identity-specific and identity-agnostic information and obtain additional interpretable control, we can optionally extend our method by also conditioning the DM on an attribute vector as done for the ID vector. In practice, we recommend using only \underline{identity-agnostic} attributes (referred to as set 1) along with the identity. In the supplementary material, we also show attribute sets that overlap more with identity (sets 2 \& 3) for completeness. 


\subsection{Implementation details} \label{sec:implementation_details}

As data set, we use FFHQ~\cite{stylegan}
and split it into $69000$ images for training and $1000$ for testing. As we can only show images of individuals with written consent (see \cref{sec:ethics}), we use a proprietary data set of faces for the qualitative results in this paper. To condition our model, we use ID vectors from a PyTorch FaceNet implementation~\cite{facenet, facenet_pytorch} or the default InsightFace method~\cite{insightface}. To evaluate the generated images and thereby match the verification accuracy on real images shown in Vec2Face~\cite{vec2face} as closely as possible, we use the official PyTorch ArcFace implementation~\cite{arcface, arcface_torch} and a TensorFlow FaceNet implementation~\cite{facenet, facenet_tf}. A detailed description of the remaining implementation details and ID vectors is in the supplementary material.
