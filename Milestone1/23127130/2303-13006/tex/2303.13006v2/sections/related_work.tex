\section{Related work} \label{sec:related_work}

\begin{table*}[htbp]
\centering
    \footnotesize{
    \begin{tabular}{ llllllll }
    \toprule
    Method & Black-box & \multicolumn{2}{c}{FR model queries (inference)} & Training data set & Realistic $^1$ & Mapping \\
    \midrule
    Zhmoginov and Sandler~\cite{DBLP:journals/corr/ZhmoginovS16} & \cellcolor{red!10}No & \cellcolor{red!10} $\sim$ 1000 $^2$ & \cellcolor{green!10}1  $^2$ & \cellcolor{green!10}Any images & \cellcolor{red!10}No & \cellcolor{red!10}One-to-one\\
    Cole \etal~\cite{cole} & \cellcolor{red!10}No & \cellcolor{green!10}1 & \cellcolor{green!10} & \cellcolor{red!10}Frontalized images & \cellcolor{green!10}Yes & \cellcolor{red!10}One-to-one \\
    Nitzan \etal~\cite{latent_space_mapping} & \cellcolor{red!10}No & \cellcolor{green!10}1 & \cellcolor{green!10} & \cellcolor{green!10}Any images & \cellcolor{green!10}Yes & \cellcolor{green!10}One-to-many \\
    NbNet~\cite{nbnet} & \cellcolor{green!10}Yes & \cellcolor{green!10}1 & \cellcolor{green!10} & \cellcolor{red!10}Huge data set & \cellcolor{red!10}No & \cellcolor{red!10}One-to-one \\
    Gaussian sampling~\cite{gaussian_sampling} & \cellcolor{green!10}Yes & \cellcolor{red!10}240000 & \cellcolor{red!10} & \cellcolor{green!10}Data-set-free & \cellcolor{red!10}No & \cellcolor{green!10}One-to-many \\
    Yang \etal~\cite{background_knowledge_alignment} & \cellcolor{green!10}Yes & \cellcolor{green!10}1 & \cellcolor{green!10} & \cellcolor{green!10}Any images & \cellcolor{red!10}No & \cellcolor{red!10}One-to-one \\
    Vec2Face~\cite{vec2face} & \cellcolor{green!10}Yes & \cellcolor{green!10}1 & \cellcolor{green!10} & \cellcolor{red!10}Multiple images per identity & \cellcolor{green!10}Yes & \cellcolor{green!10}One-to-many \\
    StyleGAN search~\cite{stylegan-search} & \cellcolor{green!10}Yes & \cellcolor{red!10}400 & \cellcolor{red!10} & \cellcolor{green!10}Data-set-free & \cellcolor{green!10}Yes & \cellcolor{green!10}One-to-many \\
    \midrule
    ID3PM (Ours) & \cellcolor{green!10}Yes & \cellcolor{green!10}1 & \cellcolor{green!10} & \cellcolor{green!10}Any images & \cellcolor{green!10}Yes & \cellcolor{green!10}One-to-many\\
    \bottomrule
    \end{tabular}
    }
\caption{Comparison of state-of-the-art face recognition (FR) model inversion methods. Our method does not have any of the common shortcomings, producing diverse, realistic images from black-box ID vectors with few requirements for the training data set or accessibility of the FR model during inference. 
$^1$~By visual inspection of the results of the respective papers.
$^2$~The authors propose two methods: one taking hundreds or thousands of queries and the second one doing it in one shot. 
}
\label{table:comp}
\end{table*}

\subsection{Face recognition}

While early deep learning works such as DeepFace~\cite{deepface} and VGG-Face~\cite{vggface} treated FR as a classification problem (one class per identity), FaceNet~\cite{facenet} introduced the triplet loss, a distance-based loss function. 
The trend then shifted towards margin-based softmax methods~\cite{margin_based_1, margin_based_2, margin_based_3, arcface} that incorporate a margin penalty and perform sample-to-class rather than sample-to-sample comparisons. More recently, some FR methods tackle specific challenges such as robustness to different quality levels~\cite{adaface} and occlusions~\cite{mask_robustness, from}.



\subsection{Inversion of face recognition models}

Similar to gradient-based feature visualization techniques~\cite{feature_vis_1, feature_vis_2, feature_vis_3, deepdream}, Zhmoginov and Sandler~\cite{DBLP:journals/corr/ZhmoginovS16} perform gradient ascent steps using the gradient of a pre-trained FR model to generate images that approach the same ID vector as a target image. To avoid generating adversarial examples, strong image priors such as a total-variation loss and a guiding image are necessary. Cole \etal~\cite{cole} transform the one-to-many task into a one-to-one task by mapping features of an FR model to frontal, neutral-expression images, which requires a difficult-to-obtain data set. Nitzan \etal~\cite{latent_space_mapping} map the identity features and attributes of images into the style space of a pre-trained StyleGAN~\cite{stylegan} to produce compelling results. However, their method struggles to encode real images since it is trained exclusively with images generated by StyleGAN. Furthermore, all of the above methods require white-box access to (the gradient of) an FR model, which is not always available in practice.

Many black-box methods view the problem from a security lens, focusing on generating images that deceive an FR model rather than appearing realistic. Early attempts using linear~\cite{mohanty2007scores} or radial basis function models~\cite{mignon2013reconstructing} lacked generative capacity to produce realistic images. NbNet~\cite{nbnet} introduces a neighborly de-convolutional neural network that can generate images with a reasonable resemblance to a given image, but it has line artifacts and relies on a huge data set augmented with a GAN. On the contrary, Razzhigaev \etal~\cite{gaussian_sampling} propose a data-set-free method using Gaussian blobs (which we call ``Gaussian sampling'' for simplicity), but they need  thousands of FR model queries ($10$-$15$ minutes) per image, and their results lack realism. Yang \etal~\cite{background_knowledge_alignment} rely on background knowledge to invert a model and only produce blurry images in the black-box setting. Vec2Face~\cite{vec2face} uses a bijection metric and knowledge distillation from a black-box FR model to produce realistic identity-preserving faces; however, it requires a large data set with multiple images per identity (Casia-WebFace~\cite{casiawebface}) during training. 
The method by Vendrow and Vendrow~\cite{stylegan-search} (which we call ``StyleGAN search'') searches the latent space of a pre-trained StyleGAN2~\cite{stylegan2} to find images with an ID vector close to the target.
While their search strategy generates highly realistic images, it needs hundreds of FR model queries ($5$-$10$ minutes) per image and often lands in local minima, resulting in completely 
different identities.

\Cref{table:comp} compares attributes of state-of-the-art FR model inversion methods. Ours is the only one that generates diverse, realistic, identity-preserving images in the black-box setting, can be trained with easy-to-obtain data, and only requires one FR model query during inference. 


\subsection{Diffusion models for inverse problems}

A number of approaches for solving inverse problems in a more general setting using conditional~\cite{SR3, palette} and unconditional~\cite{kadkhodaie2021stochastic, DDRM, PiGDM, DPS, MCG, plugandplay, BlindDPS, CCDF, ILVR, RED_Diff, song2020score} exist; however, they mostly focus on image-to-image tasks such as inpainting and super-resolution whereas we focus on a vector-to-image task. The method by Graikos \etal~\cite{plugandplay} can generate images from low-dimensional, nonlinear constraints such as attributes, but it requires the gradient of the attribute classifier during inference whereas ours does not. Thus, conditional diffusion models with vectors as additional input~\cite{ddpm3, dalle2, stable_diffusion, imagen}, while not directly geared towards inversion, are conceptually more similar to our approach.



