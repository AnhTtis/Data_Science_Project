% \newpage
\clearpage
% \twocolumn[
% \centering
% \Large
% \textbf{360BEV: Panoramic Semantic Mapping for Indoor Bird’s-Eye View} \\
% \vspace{1.0em} 
% \Large \textbf{(Supplementary Material)} \\
% \vspace{2.0em}
% ]

\appendix

\section{Data Generation}
To perform the data generation, we use an open-source tool\footnote{\url{https://github.com/atlantis-ar/matterport_utils}} to convert the 3D mesh semantic labels in Matterport3D~\cite{Matterport3D} into $194,400$ pinhole images with semantic labels. Then, every $18$ semantic label pairs are concatenated via a corresponding rotation-translation matrix, yielding $10,800$ panoramic semantic ground truth, which is referred to as 360FV-Matterport by us. These panoramic semantic images are originally annotated with $40$ object categories. 
Because many of them are only a small percentage (${\ll}0.1\%$), we merges some uncommon classes and maintains the $20$ most common object categories: \texttt{wall}, \texttt{floor}, \texttt{chair}, \texttt{door}, \texttt{table}, \texttt{picture}, \texttt{furniture}, \texttt{objects}, \texttt{window}, \texttt{sofa}, \texttt{bed}, \texttt{sink}, \texttt{stairs}, \texttt{ceiling}, \texttt{toilet}, \texttt{mirror}, \texttt{shower}, \texttt{bathtub}, \texttt{counter}, and \texttt{shelving}. 
For another front-view semantic segmentation dataset, Stanford2D3D~\cite{stanford2d3d}, we keep the original object classes: \texttt{beam},  \texttt{board},  \texttt{bookcase}, \texttt{ceiling},  \texttt{chair},  \texttt{clutter}, \texttt{column},  \texttt{door},  \texttt{floor},  \texttt{sofa},  \texttt{table}, \texttt{wall},  \texttt{window}.

For the presented 360BEV-Stanford dataset, we follow the data split method of Fold-1 of the Stanford2D3D~\cite{stanford2d3d} dataset. On the BEV dataset, we use the \textit{area1}, \textit{area2}, \textit{area3}, \textit{area4} and \textit{area6} as the training data for the proposed 360BEV task, and we use the \textit{area5a} and \textit{area5b} as the validation set to evaluate the panoramic semantic mapping performance of models. The results of training and evaluation with the Fold-1 data split is similar the average scores which are calculated by using three-fold cross-validation. Besides, the validation set from Fold-1 is sufficient to evaluate the model performance on panoramic semantic mapping.

For 360BEV-Matterport, we use a different data split compared to Wijmans~\etal~\cite{wijmans2019embodied}. Instead of using synthetic simulators, all samples on our dataset are converted from the real images and labels of Matterport3D~\cite{Matterport3D} dataset, where there are $86$ unique floors on our dataset, including $61$ for training, $7$ for validations, and $18$ for testing.

\begin{figure*}[t]
    \centering
    \subfloat[360BEV-Stanford\label{subfig-1:class_iou_s2d3d}]{%
    \includegraphics[trim={2 2 2 5},clip,width=0.40\textwidth]{Figures/360BEV_s2d3d/s2d3d.pdf}
    }
    \subfloat[360BEV-Matterport\label{subfig-2:class_iou_mp3d}]{%
      \includegraphics[trim={2 2 2 5},clip, width=0.55\textwidth]{Figures/360BEV_mp3d/mp3d.pdf}
    }
    \caption{\textbf{Distribution of per-class semantic mapping results} (per-class IoU in $\%$) on the 360BEV-Stanford and the 360BEV-Matterport datasets. On the 360BEV-Matterport dataset, the results are compared on respective validation and test sets. Compared to the baseline model Trans4Map~\cite{chen2022trans4map}, our 360Mapper models achieve overall better 360BEV results.}
    \label{fig:360bev_s2d3d_per_class}
\end{figure*}

\section{More Quantitative Analysis}
\subsection{Results on Stanford2D3D}
In Table~\ref{tab:s2d3d_front_every_class}, we present the per-class IoU results of front-view semantic segmentation on the Stanford2D3D dataset. The average (Avg.) scores are calculated with three folds~\cite{stanford2d3d} of cross validation, where Fold-2 is the most challenging split on the Stanford2D3D dataset. Compared to previous state-of-the-art Trans4PASS~\cite{trans4pass}, our proposed 360Mapper achieves $47.97\%$ mIoU in Fold-2 split. 
Besides, our 360Mapper model has overall better performance ($54.34\%$ in mIoU) in the average result calculated by three folds evaluation, surpassing the previous Trans4PASS model with ${+}2.24\%$ in mIoU. Furthermore, our model achieves the highest scores in $11$ of $13$ categories, including \textit{board},  \textit{bookcase}, \textit{ceiling},  \textit{chair},  \textit{clutter}, \textit{door},  \textit{floor},  \textit{sofa},  \textit{table}, \textit{wall}, and  \textit{window}. Improvements in these categories demonstrate the effectiveness of our 360Mapper model in combating distortions of 360{\textdegree} front-view images by incorporating distortion-aware 360Attention.

\input{Tables/s_tables/s2d3d_fv_per_class}

\subsection{Results on 360FV-Matterport}
\input{Tables/tab8_matterport3d_fv_test}
As shown in Table~\ref{tab:mp3d_front_test}, we present the front-view semantic segmentation results on the \texttt{test} set of 360FV-Matterport dataset. We compare our approaches with SegFormer~\cite{xie2021segformer}, Trans4PASS~\cite{trans4pass}, Trans4PASS+~\cite{trans4passplus}, HoHoNet~\cite{hohonet} with RGB and RGB-D, where HoHoNet uses ResNet-101 as backbone and the others use MiT-B2 as backbone. Compared with the well-established existing work SegFormer, our approach obtains a higher mIoU score with $43.16\%$, having a performance improvement of ${+}0.67\%$ mIoU on the test set. The test set is much more challenging than the validation set of 360FV-Matterport dataset, the results in Table~\ref{tab:mp3d_front_test} show the superiority of the proposed approach on extracting the underlying cues for the proposed task.

\input{Tables/s_tables/mp3d_fv_per_class}
Apart from that, per-class IoU scores on 360FV-Matterport in Table~\ref{tab:mp3d_front_every_class}. The performance of 360Mapper on both test and validation sets are demonstrated. 360Mapper delivers $46.35\%$ and $43.16\%$ mIoU performance on validation and test sets of 360FV-Matterport dataset respectively. For per-class IoUs, our model has better performance of challenging class, \eg, \textit{sink} with $25.12\%$ and $28.24\%$ on validation and test sets, surpassing Trans4PASS+~\cite{trans4passplus} with large margins. 
It notes that the small objects, \eg, \textit{furniture}, \textit{mirror}, \textit{toilet} on the test set, are still challenging for both methods. Apart from these, our models have better semantic segmentation results on $17$ of $20$ classes on the 360FV-Matterport dataset.

\subsection{Results on 360BEV-Stanford}
Per-class IoU scores on 360BEV-Stanford are shown in Table~\ref{tab:s2d3d_BEV_every_class}. 
On the 360BEV task, 360Mapper can achieve $45.78\%$ score of mIoU, outperforming the previous Trans4Map~\cite{chen2022trans4map} method with ${+}9.7\%$. Specifically, our 360Mapper achieves per-class IoU with $93.33\%$, $42.52\%$, $59.14\%$, $5.06\%$, $62.66\%$, $39.75\%$, $5.48\%$, $38.74\%$, $97.76\%$, $48.92\%$, $76.76\%$, $45.86\%$ and $24.89\%$ for \textit{void}, \textit{board}, \textit{bookcase}, \textit{ceiling}, \textit{chair}, \textit{clutter}, \textit{column}, \textit{door}, \textit{floor}, \textit{sofa}, \textit{table}, \textit{wall} and \textit{window}, respectively. Especially, the challenging objects that appear thin lines in bird's-eye views, such as \textit{doors} and \textit{walls}, can be more stably recognized by our method, which improves both IoUs with $10.23\%{\rightarrow}38.74\%$ and $29.56\%{\rightarrow}45.86\%$. The \textit{beam} class is not successfully recognized by both methods, because this BEV mechanism directly ignores objects on the ceiling. Different from the front-view semantic segmentation task, the \textit{void} class is included on the 360BEV task, because this class can be used to indicate the invisible area on the BEV semantic maps, which is important for the downstream task, such as path planing.
Based on the per-class scores shown  in Figure \ref{subfig-1:class_iou_s2d3d}, 360Mapper shows overall promising performance on the presented 360BEV-Stanford dataset, indicating the effectiveness of our methods in addressing the 360BEV task.
\input{Tables/s_tables/s2d3d_bev_per_class}


\subsection{Results on 360BEV-Matterport}
\input{Tables/tab11_matterport3d_bev_test}
The 360BEV results on the \texttt{test} set of 360BEV-Matterport are demonstrated in Table~\ref{tab:mp3d_topdown_test}.
We further compare our approach with three backbones, \eg, MiT-B0, MiT-B2 from SegFormer~\cite{xie2021segformer} and MSCA-B from SegNeXt~\cite{guo2022segnext} on the test set of the 360BEV-Matterport for the panoramic semantic mapping task. Methods based on intermediate projection show the most promising results compared with those based on early projection and late projection. The result is consistent compared with the ones demonstrated on the validation set of 360BEV-Matterport dataset. 360Mapper still delivers the state-of-the art results for the proposed 360BEV task on the test set, indicating the effectiveness of the proposed architecture. Especially, our 360Mapper with MiT-B2 backbone ($38.78\%$) can surpass Trans4Map with MiT-B2 ($31.08\%$) as well as the one with MiT-B4 ($31.79\%$). Besides, the proposed method based on MSCA-B backbone achieves the best result with $40.27\%$ in mIoU. 

Per-class IoU scores on the 360BEV-Matterport dataset are presented in Table~\ref{tab:mp3d_BEV_every_class}. The performance of 360Mapper under MiT-B2 from SegFormer~\cite{xie2021segformer} and MSCA-B from SegNeXt~\cite{guo2022segnext}  are included, which achieves promising performance for the 360BEV task. Compared to Trans4Map~\cite{chen2022trans4map}, our 360Mapper with the same MiT-B2 backbone can achieve respective $44.32\%$ and $38.78\%$ in mIoU on the validation set and the test set. The \textit{void} class is also included on the 360BEV-Matterport dataset. From the Figure~\ref{subfig-2:class_iou_mp3d} presented, it is readily apparent that our model can better recognize the \textit{chairs} and \textit{tables} on the bird's-eye-view semantic maps, yielding more that $6\%$ IoU gains compared to Trans4Map~\cite{chen2022trans4map}. On the test set, 360Mapper with MiT-B2 obtain IoU gains with ${>}12\%$ and ${>}15\%$ on the \textit{sink} and \textit{toilet} clasess, as compared to Trans4Map. Besides, if using a stronger backbone, \eg, MSCA-B~\cite{guo2022segnext}, our proposed mehods can achieve higher semantic mapping results on both of validation and test sets of 360BEV-Matterport dataset, which are $46.31\%$ and $40.27\%$ in mIoU, respectively.


\input{Tables/s_tables/mp3d_bev_per_class}

\section{More Qualitative Analysis}
\subsection{Analysis on Stanford2D3D}
The visualization of front-view semantic segmentation (360FV) on the Stanford2D3D dataset is shown in Fig.~\ref{fig:s2d3d_fv}, where the RGB input, the prediction of the baseline, the prediction of our model and the ground truth are depicted from left to the right. The corresponding color map is showcased at the top of Fig.~\ref{fig:s2d3d_fv}. Compared with the baseline Trans4Pass\cite{trans4pass}, the panoramic semantic segmentation results of our model have clear boundaries among different objects which is much more similar to the ground truth, \eg, the \textit{door} and the \textit{clutter} of the second sample. Our method also show promising performance on the objects with small spatial size, \eg, \textit{chairs}, compared with the baseline in the last sample, indicating that our 360Attention approach is good at grasping underlying context feature and cues through the deformable sampling locations. 

\subsection{Analysis on 360FV-Matterport}
Fig.~\ref{fig:mp3d_fv} is the front-view semantic segmentation visualization of the presented 360FV-Matterport dataset, providing a detailed depiction of the spatial distribution of different semantic classes. Compared with the baseline method Trans4Pass\cite{trans4pass}, our model produces segmentation results exhibit more precise contours and clearer boundaries between different objects, which closely resemble the ground truth segmentation labels, \eg, the \textit{toilet} and the \textit{door} of the first sample. In the second row, the \textit{door} on the right side is not recognized by the baseline model. In contrast to the baseline method, our model is able to accurately distinguish the \textit{door} class from its surrounding \textit{object} and \textit{wall} classes, despite its small size and low contrast with the surrounding environment. The \textit{table} in the center of the third sample are correctly predicted by our model while it is erroneously segmented by the baseline as \textit{furniture}. This highlights the superior performance of 360Mapper in panoramic semantic segmentation under challenging conditions. In the last two rows, the small \textit{chair} by the wall and the \textit{door} are correctly recognized by our model.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure*}[t]
    \footnotesize
    \setlength\tabcolsep{1pt}
    {
    \newcolumntype{P}[1]{>{\centering\arraybackslash}p{#1}}
    \begin{tabular}{@{}*{13}{P{0.1475\columnwidth}}@{}}
     {\cellcolor[rgb]{1,   0,   0.16}}\textcolor{white}{beam} 
    &{\cellcolor[rgb]{1,   0.28, 0}}\textcolor{white}{board}
    &{\cellcolor[rgb]{1,   0.73, 0}}\textcolor{black}{bookcase}
    &{\cellcolor[rgb]{0.8,  1,   0}}\textcolor{black}{ceiling}
    &{\cellcolor[rgb]{0.36, 1,   0}}\textcolor{black}{chair} 
    &{\cellcolor[rgb]{0,    1,  0.08}}\textcolor{black}{clutter} 
    &{\cellcolor[rgb]{0,    1,  0.55}}\textcolor{black}{column}
    &{\cellcolor[rgb]{0,    1,  0.99}}\textcolor{black}{door}
    &{\cellcolor[rgb]{0,   0.56, 1 }}\textcolor{white}{floor}
    &{\cellcolor[rgb]{0,   0.09, 1 }}\textcolor{white}{sofa} 
    &{\cellcolor[rgb]{0.35, 0,   1  }}\textcolor{white}{table}
    & {\cellcolor[rgb]{0.8,  0,   1 }}\textcolor{white}{wall}
    &{\cellcolor[rgb]{1,    0,   0.75}}\textcolor{white}{window}\\
    \end{tabular}
    }
    \centering
    \begin{tabular}{c c c c}
        \vspace{1pt}
        \raisebox{-0.5\height}{\includegraphics[width=0.24\textwidth]{Figures/s_fv_s2d3d/rgb_1ce1.png}} &
        \raisebox{-0.5\height}{\includegraphics[width=0.24\textwidth]{Figures/s_fv_s2d3d/baseline_1ce1.png}} &
        \raisebox{-0.5\height}{\includegraphics[width=0.24\textwidth]{Figures/s_fv_s2d3d/detr_1ce1.png}} &
        \raisebox{-0.5\height}{\includegraphics[width=0.24\textwidth]{Figures/s_fv_s2d3d/gt_1ce1.png}}\\
         % \vspace{0.5cm}

        \vspace{1pt}

        \raisebox{-0.5\height}{\includegraphics[width=0.24\textwidth]{Figures/s_fv_s2d3d/rgb_01d2.png}} &
        \raisebox{-0.5\height}{\includegraphics[width=0.24\textwidth]{Figures/s_fv_s2d3d/baseline_01d2.png}} &
        \raisebox{-0.5\height}{\includegraphics[width=0.24\textwidth]{Figures/s_fv_s2d3d/detr_01d2.png}} &
        \raisebox{-0.5\height}{\includegraphics[width=0.24\textwidth]{Figures/s_fv_s2d3d/gt_01d2.png}}\\
        
        \vspace{1pt}

        \raisebox{-0.5\height}{\includegraphics[width=0.24\textwidth]{Figures/s_fv_s2d3d/rgb_6a82.png}} &
        \raisebox{-0.5\height}{\includegraphics[width=0.24\textwidth]{Figures/s_fv_s2d3d/baseline_6a82.png}} &
        \raisebox{-0.5\height}{\includegraphics[width=0.24\textwidth]{Figures/s_fv_s2d3d/detr_6a82.png}} &
        \raisebox{-0.5\height}{\includegraphics[width=0.24\textwidth]{Figures/s_fv_s2d3d/gt_6a82.png}}\\
        
        \vspace{1pt}

        \raisebox{-0.5\height}{\includegraphics[width=0.24\textwidth]{Figures/s_fv_s2d3d/rgb_9ab5.png}} &
        \raisebox{-0.5\height}{\includegraphics[width=0.24\textwidth]{Figures/s_fv_s2d3d/baseline_9ab5.png}} &
        \raisebox{-0.5\height}{\includegraphics[width=0.24\textwidth]{Figures/s_fv_s2d3d/detr_9ab5.png}} &
        \raisebox{-0.5\height}{\includegraphics[width=0.24\textwidth]{Figures/s_fv_s2d3d/gt_9ab5.png}}\\
        
        \vspace{1pt}
        RGB Input&Baseline&Ours&Ground Truth
    \end{tabular}
    \vskip -2ex
    \caption{\textbf{360FV visualization and qualitative analysis} on the Stanford2D3D dataset.}%Black regions are \texttt{void}.}
    \label{fig:s2d3d_fv}
    \vskip -1ex
\end{figure*}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure*}[t]
    \footnotesize
    \setlength\tabcolsep{1pt}
    {
    \newcolumntype{P}[1]{>{\centering\arraybackslash}p{#1}}
    \begin{tabular}{@{}*{20}{P{0.093\columnwidth}}@{}}
    {\cellcolor[rgb]{0.68,0.78,0.91}}\textcolor{white}{wall} 
    &{\cellcolor[rgb]{0.44,0.50,0.56}}\textcolor{white}{floor}
    &{\cellcolor[rgb]{0.60,0.87,0.54}}\textcolor{black}{chair}
    &{\cellcolor[rgb]{0.77,0.69,0.84}}\textcolor{white}{door}
    &{\cellcolor[rgb]{1.00,0.50,0.05}}\textcolor{white}{table} 
    &{\cellcolor[rgb]{0.84,0.15,0.16}}\textcolor{white}{pictu.} 
    &{\cellcolor[rgb]{0.12,0.47,0.71}}\textcolor{white}{furni.}
    &{\cellcolor[rgb]{0.74,0.74,0.13}}\textcolor{black}{objec.}
    &{\cellcolor[rgb]{1.00,0.60,0.59}}\textcolor{black}{windo.}
    &{\cellcolor[rgb]{0.17,0.63,0.17}}\textcolor{white}{sofa} 
    &{\cellcolor[rgb]{0.89,0.47,0.76}}\textcolor{black}{bed}
    & {\cellcolor[rgb]{0.87,0.62,0.84}}\textcolor{black}{sink}
    &{\cellcolor[rgb]{0.58,0.40,0.74}}\textcolor{white}{stairs} 
    &{\cellcolor[rgb]{0.55,0.64,0.32}}\textcolor{white}{ceil.} 
    &{\cellcolor[rgb]{0.52,0.24,0.22}}\textcolor{white}{toilet} 
    &{\cellcolor[rgb]{0.62,0.85,0.90}}\textcolor{black}{mirror} 
    &{\cellcolor[rgb]{0.61,0.62,0.87}}\textcolor{black}{show.}
    &{\cellcolor[rgb]{0.91,0.59,0.61}}\textcolor{black}{batht.}
    &{\cellcolor[rgb]{0.39,0.47,0.22}}\textcolor{white}{count.} 
    &{\cellcolor[rgb]{0.55,0.34,0.29}}\textcolor{white}{shelv.} \\
    \end{tabular}
    }
    \centering
    \begin{tabular}{c c c c}
        \vspace{1pt}
        \raisebox{-0.5\height}{\includegraphics[width=0.24\textwidth]{Figures/s_fv_mp3d/rgb_02e0.png}} &
        \raisebox{-0.5\height}{\includegraphics[width=0.24\textwidth]{Figures/s_fv_mp3d/baseline_02e0.png}} &
        \raisebox{-0.5\height}{\includegraphics[width=0.24\textwidth]{Figures/s_fv_mp3d/detr_02e0.png}} &
        \raisebox{-0.5\height}{\includegraphics[width=0.24\textwidth]{Figures/s_fv_mp3d/gt_02e0.png}}\\
         % \vspace{0.5cm}

        \vspace{1pt}

        \raisebox{-0.5\height}{\includegraphics[width=0.24\textwidth]{Figures/s_fv_mp3d/rgb_0d15.png}} &
        \raisebox{-0.5\height}{\includegraphics[width=0.24\textwidth]{Figures/s_fv_mp3d/baseline_0d15.png}} &
        \raisebox{-0.5\height}{\includegraphics[width=0.24\textwidth]{Figures/s_fv_mp3d/detr_0d15.png}} &
        \raisebox{-0.5\height}{\includegraphics[width=0.24\textwidth]{Figures/s_fv_mp3d/gt_0d15.png}}\\
        
        \vspace{1pt}

        \raisebox{-0.5\height}{\includegraphics[width=0.24\textwidth]{Figures/s_fv_mp3d/rgb_8bc7.png}} &
        \raisebox{-0.5\height}{\includegraphics[width=0.24\textwidth]{Figures/s_fv_mp3d/baseline_8bc7.png}} &
        \raisebox{-0.5\height}{\includegraphics[width=0.24\textwidth]{Figures/s_fv_mp3d/detr_8bc7.png}} &
        \raisebox{-0.5\height}{\includegraphics[width=0.24\textwidth]{Figures/s_fv_mp3d/gt_8bc7.png}}\\
        
        \vspace{1pt}

        \raisebox{-0.5\height}{\includegraphics[width=0.24\textwidth]{Figures/s_fv_mp3d/rgb_2cda.png}} &
        \raisebox{-0.5\height}{\includegraphics[width=0.24\textwidth]{Figures/s_fv_mp3d/baseline_2cda.png}} &
        \raisebox{-0.5\height}{\includegraphics[width=0.24\textwidth]{Figures/s_fv_mp3d/detr_2cda.png}} &
        \raisebox{-0.5\height}{\includegraphics[width=0.24\textwidth]{Figures/s_fv_mp3d/gt_2cda.png}}\\
        \vspace{1pt}
        
        \raisebox{-0.5\height}{\includegraphics[width=0.24\textwidth]{Figures/s_fv_mp3d/rgb_bd46.png}} &
        \raisebox{-0.5\height}{\includegraphics[width=0.24\textwidth]{Figures/s_fv_mp3d/baseline_bd46.png}} &
        \raisebox{-0.5\height}{\includegraphics[width=0.24\textwidth]{Figures/s_fv_mp3d/detr_bd46.png}} &
        \raisebox{-0.5\height}{\includegraphics[width=0.24\textwidth]{Figures/s_fv_mp3d/gt_bd46.png}}\\
        
        \vspace{1pt}
        RGB Input&Baseline&Ours&Ground Truth
    \end{tabular}
    \vskip -2ex
    \caption{\textbf{360FV visualization and qualitative analysis} on the 360FV-Matterport dataset.}
    \label{fig:mp3d_fv}
    \vskip -3ex
\end{figure*}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% \subsection{360BEV Visualization}
\subsection{Analysis on 360BEV-Stanford}
We further introduce the qualitative results of 360BEV task on the 360BEV-Standford dataset in Fig.~\ref{fig:s2d3d_BEV}. The RGB input, the BEV semantic mapping results of the baseline and 360 Mapper, the BEV semantic mapping ground truth are depicted from left to right, where the color map is shown at the top of Fig.~\ref{fig:s2d3d_BEV}. The \textit{chairs} of the first and the second sample are correctly predicted by our method while they are partially or entirely missed by the baseline. Compared with the 360Mapper, the baseline shows more false prediction especially regarding some furniture, \eg, the false predicted \textit{bookcase} at the third sample, which should be predicted as \textit{chairs}. At the last row of Fig.~\ref{fig:s2d3d_BEV}, the challenging \textit{door} is not recognized by the baseline model, while our 360Mapper can provide accurate \textit{door} segmentation result, even it is a thin line in the BEV map. Our method shows overall superior performance on the proposed task compared with the baseline in terms of the semantic segmentation performance on small objects, which further illustrates the strength by using 360Attention.

\subsection{Analysis on 360BEV-Matterport}
Fig.~\ref{fig:mp3d_360bev} presents qualitative results for the 360BEV task on the 360BEV-Matterport dataset. We observe that our 360Mapper outperforms the baseline method Trans4Map~\cite{chen2022trans4map} in terms of accurately segmenting small objects. In particular, the baseline method exhibits more false predictions, such as the misclassified \textit{chair} in the first sample and \textit{object} misidentified as \textit{table} in the second sample. Surprisingly, the different steps of \textit{stairs} in the third and the fourth sample are recognized correctly by both methods.
However, we find the fifth sample to be particularly challenging, as both the baseline and our 360Mapper recognize the object in the center of the image as a \textit{counter}, which is a \textit{table} as shown in the ground truth. This failure case shows the difficulty of accurately distinguishing between similar object categories from the context of panoramic images to the bird's-eye-view semantic maps. 

\section{Acknowledgments}
This work was supported in part by Helmholtz Association of German Research Centers, 
in part by the Ministry of Science, Research and the Arts of Baden-Württemberg (MWK) through the Cooperative Graduate School Accessibility through AI-based Assistive Technology (KATE) under Grant BW6-03, in part by the University of Excellence through the ``KIT Future Fields'' project, in part by Hangzhou SurImage Technology Company Ltd., and in part by the Helmholtz Association Initiative and Networking Fund on the HAICORE@KIT partition.
This work was partially performed on the HoreKa supercomputer funded by the MWK and by the Federal Ministry of Education and Research.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{figure*}[t]
    \footnotesize
    \setlength\tabcolsep{1pt}
    \begin{center}
    {
    \newcolumntype{P}[1]{>{\centering\arraybackslash}p{#1}}
    \begin{tabular}{@{}*{14}{P{0.135\columnwidth}}@{}}
    {\cellcolor[rgb]{0,   0,   0}}\textcolor{white}{void} 
    &{\cellcolor[rgb]{1,   0,   0.16}}\textcolor{white}{beam} 
    &{\cellcolor[rgb]{1,   0.28, 0}}\textcolor{white}{board}
    &{\cellcolor[rgb]{1,   0.73, 0}}\textcolor{black}{bookcase}
    &{\cellcolor[rgb]{0.8,  1,   0}}\textcolor{black}{ceiling}
    &{\cellcolor[rgb]{0.36, 1,   0}}\textcolor{black}{chair} 
    &{\cellcolor[rgb]{0,    1,  0.08}}\textcolor{black}{clutter} 
    &{\cellcolor[rgb]{0,    1,  0.55}}\textcolor{black}{column}
    &{\cellcolor[rgb]{0,    1,  0.99}}\textcolor{black}{door}
    &{\cellcolor[rgb]{0,   0.56, 1 }}\textcolor{white}{floor}
    &{\cellcolor[rgb]{0,   0.09, 1 }}\textcolor{white}{sofa} 
    &{\cellcolor[rgb]{0.35, 0,   1  }}\textcolor{white}{table}
    & {\cellcolor[rgb]{0.8,  0,   1 }}\textcolor{white}{wall}
    &{\cellcolor[rgb]{1,    0,   0.75}}\textcolor{white}{window}\\
    \end{tabular}
    }
    \end{center}
    \vskip -3mm
    \centering
    \begin{tabular}{c c c c}
        \vspace{1pt}
        \raisebox{-0.5\height}{\includegraphics[width=0.38\textwidth]{Figures/360BEV_s2d3d/rgb_1a2b.png}} &
        \raisebox{-0.5\height}{\includegraphics[width=0.19\textwidth]{Figures/360BEV_s2d3d/baseline_1a2b.png}} &
        \raisebox{-0.5\height}{\includegraphics[width=0.19\textwidth]{Figures/360BEV_s2d3d/detr_1a2b.png}} &
        \raisebox{-0.5\height}{\includegraphics[width=0.19\textwidth]{Figures/360BEV_s2d3d/gt_1a2b.png}}\\
         % \vspace{0.5cm}

        \vspace{1pt}

        \raisebox{-0.5\height}{\includegraphics[width=0.38\textwidth]{Figures/360BEV_s2d3d/rgb_6a82.png}} &
        \raisebox{-0.5\height}{\includegraphics[width=0.19\textwidth]{Figures/360BEV_s2d3d/baseline_6a82.png}} &
        \raisebox{-0.5\height}{\includegraphics[width=0.19\textwidth]{Figures/360BEV_s2d3d/detr_6a82.png}} &
        \raisebox{-0.5\height}{\includegraphics[width=0.19\textwidth]{Figures/360BEV_s2d3d/gt_6a82.png}}\\
        
        \vspace{1pt}

        \raisebox{-0.5\height}{\includegraphics[width=0.38\textwidth]{Figures/360BEV_s2d3d/rgb_3d0b.png}} &
        \raisebox{-0.5\height}{\includegraphics[width=0.19\textwidth]{Figures/360BEV_s2d3d/baseline_3d0b.png}} &
        \raisebox{-0.5\height}{\includegraphics[width=0.19\textwidth]{Figures/360BEV_s2d3d/detr_3d0b.png}} &
        \raisebox{-0.5\height}{\includegraphics[width=0.19\textwidth]{Figures/360BEV_s2d3d/gt_3d0b.png}}\\
        
        \vspace{1pt}

        \raisebox{-0.5\height}{\includegraphics[width=0.38\textwidth]{Figures/360BEV_s2d3d/rgb_a233.png}} &
        \raisebox{-0.5\height}{\includegraphics[width=0.19\textwidth]{Figures/360BEV_s2d3d/baseline_a233.png}} &
        \raisebox{-0.5\height}{\includegraphics[width=0.19\textwidth]{Figures/360BEV_s2d3d/detr_a233.png}} &
        \raisebox{-0.5\height}{\includegraphics[width=0.19\textwidth]{Figures/360BEV_s2d3d/gt_a233.png}}\\
        
        \noalign{\vskip 2mm}
        RGB Input&Baseline&360Mapper&Ground Truth
    \end{tabular}
    \caption{\textbf{360BEV visualization and qualitative analysis} on the 360BEV-Stanford dataset. Black regions are the \texttt{void} class, indicating the invisible areas in BEV semantic maps. Zoom in for better view.}
    \label{fig:s2d3d_BEV}
    \vskip -3ex
\end{figure*}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure*}[t]
    \footnotesize
    \setlength\tabcolsep{1pt}
    \begin{center}
    {
    \newcolumntype{P}[1]{>{\centering\arraybackslash}p{#1}}
    \begin{tabular}{@{}*{21}{P{0.087\columnwidth}}@{}}
     {\cellcolor[rgb]{0,   0,   0}}\textcolor{white}{void} 
    &{\cellcolor[rgb]{0.68,0.78,0.91}}\textcolor{white}{wall} 
    &{\cellcolor[rgb]{0.44,0.50,0.56}}\textcolor{white}{floor}
    &{\cellcolor[rgb]{0.60,0.87,0.54}}\textcolor{black}{chair}
    &{\cellcolor[rgb]{0.77,0.69,0.84}}\textcolor{white}{door}
    &{\cellcolor[rgb]{1.00,0.50,0.05}}\textcolor{white}{table} 
    &{\cellcolor[rgb]{0.84,0.15,0.16}}\textcolor{white}{pictu.} 
    &{\cellcolor[rgb]{0.12,0.47,0.71}}\textcolor{white}{furni.}
    &{\cellcolor[rgb]{0.74,0.74,0.13}}\textcolor{black}{objec.}
    &{\cellcolor[rgb]{1.00,0.60,0.59}}\textcolor{black}{windo.}
    &{\cellcolor[rgb]{0.17,0.63,0.17}}\textcolor{white}{sofa} 
    &{\cellcolor[rgb]{0.89,0.47,0.76}}\textcolor{black}{bed}
    & {\cellcolor[rgb]{0.87,0.62,0.84}}\textcolor{black}{sink}
    &{\cellcolor[rgb]{0.58,0.40,0.74}}\textcolor{white}{stairs} 
    &{\cellcolor[rgb]{0.55,0.64,0.32}}\textcolor{white}{ceil.} 
    &{\cellcolor[rgb]{0.52,0.24,0.22}}\textcolor{white}{toilet} 
    &{\cellcolor[rgb]{0.62,0.85,0.90}}\textcolor{black}{mirror} 
    &{\cellcolor[rgb]{0.61,0.62,0.87}}\textcolor{black}{show.}
    &{\cellcolor[rgb]{0.91,0.59,0.61}}\textcolor{black}{batht.}
    &{\cellcolor[rgb]{0.39,0.47,0.22}}\textcolor{white}{count.} 
    &{\cellcolor[rgb]{0.55,0.34,0.29}}\textcolor{white}{shelv.} \\
    \end{tabular}
    }
    \end{center}
    \vskip -3mm
    \centering
    \begin{tabular}{c c c c}
        \vspace{1pt}
        \raisebox{-0.5\height}{\includegraphics[width=0.38\textwidth]{Figures/360BEV_mp3d/rgb_1.png}} &
        \raisebox{-0.5\height}{\includegraphics[width=0.19\textwidth]{Figures/360BEV_mp3d/baseline_1.png}} &
        \raisebox{-0.5\height}{\includegraphics[width=0.19\textwidth]{Figures/360BEV_mp3d/detr_1.png}} &
        \raisebox{-0.5\height}{\includegraphics[width=0.19\textwidth]{Figures/360BEV_mp3d/gt_1.png}}\\
         % \vspace{0.5cm}
        \vspace{1pt}

        \raisebox{-0.5\height}{\includegraphics[width=0.38\textwidth]{Figures/360BEV_mp3d/rgb_2.png}} &
        \raisebox{-0.5\height}{\includegraphics[width=0.19\textwidth]{Figures/360BEV_mp3d/baseline_2.png}} &
        \raisebox{-0.5\height}{\includegraphics[width=0.19\textwidth]{Figures/360BEV_mp3d/detr_2.png}} &
        \raisebox{-0.5\height}{\includegraphics[width=0.19\textwidth]{Figures/360BEV_mp3d/gt_2.png}}\\
        
        \vspace{1pt}

        \raisebox{-0.5\height}{\includegraphics[width=0.38\textwidth]{Figures/360BEV_mp3d/rgb_3.png}} &
        \raisebox{-0.5\height}{\includegraphics[width=0.19\textwidth]{Figures/360BEV_mp3d/baseline_3.png}} &
        \raisebox{-0.5\height}{\includegraphics[width=0.19\textwidth]{Figures/360BEV_mp3d/detr_3.png}} &
        \raisebox{-0.5\height}{\includegraphics[width=0.19\textwidth]{Figures/360BEV_mp3d/gt_3.png}}\\
        
        \vspace{1pt}

        \raisebox{-0.5\height}{\includegraphics[width=0.38\textwidth]{Figures/360BEV_mp3d/rgb_4.png}} &
        \raisebox{-0.5\height}{\includegraphics[width=0.19\textwidth]{Figures/360BEV_mp3d/baseline_4.png}} &
        \raisebox{-0.5\height}{\includegraphics[width=0.19\textwidth]{Figures/360BEV_mp3d/detr_4.png}} &
        \raisebox{-0.5\height}{\includegraphics[width=0.19\textwidth]{Figures/360BEV_mp3d/gt_4.png}}\\
        \vspace{1pt}

        \raisebox{-0.5\height}{\includegraphics[width=0.38\textwidth]{Figures/360BEV_mp3d/rgb_5.png}} &
        \raisebox{-0.5\height}{\includegraphics[width=0.19\textwidth]{Figures/360BEV_mp3d/baseline_5.png}} &
        \raisebox{-0.5\height}{\includegraphics[width=0.19\textwidth]{Figures/360BEV_mp3d/detr_5.png}} &
        \raisebox{-0.5\height}{\includegraphics[width=0.19\textwidth]{Figures/360BEV_mp3d/gt_5.png}}\\
        
        \noalign{\vskip 2mm}
        RGB Input&Baseline&360Mapper&Ground Truth
    \end{tabular}
    \caption{\textbf{360BEV visualization and qualitative analysis} on the 360BEV-Matterport dataset. Black regions are the \texttt{void} class, indicating the invisible areas in BEV semantic maps. Zoom in for better view.}
    \label{fig:mp3d_360bev}
    \vskip -3ex
\end{figure*}

