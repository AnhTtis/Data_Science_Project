\documentclass[10pt,twocolumn,letterpaper]{article}
% \usepackage{cvpr}
\usepackage[pagenumbers]{cvpr} % To force page numbers, e.g. for an arXiv version

\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}

\usepackage{float}
\usepackage{lipsum}
\usepackage{stfloats}
\usepackage{multicol}
\usepackage{multirow}
\usepackage{bm}
\usepackage{etoolbox}

\usepackage{array}
\usepackage{tabulary}
\usepackage[table,dvipsnames]{xcolor} % for rowcolor
\usepackage{paralist}
\usepackage{booktabs}
\usepackage{adjustbox}
\usepackage[shortlabels]{enumitem}
\usepackage{tabu} % color table rowfont

% === for table caption
\usepackage{caption}
\captionsetup[table]{format=plain,labelformat=simple,labelsep=period}%
% === for figure subcaption
\usepackage{subcaption}
\usepackage[pagebackref=true,breaklinks=true,colorlinks,bookmarks=false,citecolor=blue,linkcolor=red]{hyperref}
\usepackage{icomma} % for comma
\usepackage{arydshln}
\newcommand\ver[1]{\rotatebox[origin=c]{90}{#1}}
\newcommand{\fl}[1]{\multicolumn{1}{c}{#1}}
\definecolor{gray}{rgb}{0.3,0.3,0.3}
\definecolor{blue}{rgb}{0,0.5,1}
\definecolor{mask_red}{rgb}{1,0,0.8}
\definecolor{green}{rgb}{0.2,1,0.2}
\definecolor{rblue}{rgb}{0,0,1}
\definecolor{lightblue}{HTML}{6495ed}
\definecolor{lightred}{HTML}{F19C99}
\newcommand{\gray}[1]{\textcolor{gray}{#1}}
\newcommand{\green}[1]{\textcolor[RGB]{96,177,87}{#1}}
\newcommand{\lightblue}[1]{\textcolor{lightblue}{#1}}
\newcommand{\fn}[1]{\footnotesize{#1}}
\newcommand{\gbf}[1]{\green{\bf{\fn{(#1)}}}}
\newcommand{\bbf}[1]{\lightblue{\bf{\fn{(#1)}}}}
\newcommand{\rbf}[1]{\gray{\bf{\fn{(#1)}}}}
\newcommand{\obf}[1]{\textcolor{orange}{\bf{\fn{(#1)}}}}
\definecolor{graytablerow}{gray}{0.6}
\newcommand{\grow}[1]{\textcolor{graytablerow}{#1}}


% \usepackage{tikz}
\makeatletter
\@namedef{ver@everyshi.sty}{}
\makeatother
\usepackage{tikz}
\newcommand*\circled[1]{\tikz[baseline=(char.base)]{
\node[shape=circle,fill=gray,inner sep=0.5pt] (char) {\textcolor{white}{\footnotesize \textbf{#1}}};}}

\begin{document}

%%%%%%%%% TITLE
\title{360BEV: Panoramic Semantic Mapping for Indoor Bird's-Eye View}
\author{
Zhifeng Teng$^{1,}$\thanks{Equal contribution.},
~~Jiaming Zhang$^{1,*,}$\thanks{Corresponding author (e-mail: {\tt jiaming.zhang@kit.edu}).},
~~Kailun Yang$^2$,
~~Kunyu Peng$^1$,\\
~~Hao Shi$^3$,
~~Simon Rei√ü$^{1}$,
~~Ke Cao$^{1}$,
~~Rainer Stiefelhagen$^1$\\
\normalsize
$^1$Karlsruhe Institute of Technology,
\normalsize
~$^2$Hunan University,
\normalsize
~$^3$Zhejiang University
}

\maketitle

%%%%%%%%% ABSTRACT
\input{TexContents/abstract}

%%%%%%%%% BODY TEXT
\begin{figure}[t]
    \centering
    \includegraphics[width=0.99\linewidth]{Figures/fig1_diagram_2.pdf}
    \begin{minipage}[t]{.5\columnwidth}
        \vskip-3ex
        \subcaption{Narrow BEV}\label{fig1_narrowbev}
    \end{minipage}%
    \begin{minipage}[t]{.5\columnwidth}
        \vskip-3ex
        \subcaption{360BEV}\label{fig1_360bev}
    \end{minipage}%
    \vskip-1ex
    \caption{Semantic mapping from egocentric front-view images to allocentric BEV semantics. While (a) the narrow-BEV method has limited perception and map range, (b) 360BEV has an omnidirectional \textcolor[HTML]{78eaf8}{Field of View}, yielding a more complete BEV map by using our 360Mapper model.}
    \label{fig:360bev}
    \vskip-3ex
\end{figure}
\section{Introduction}
\label{sec:intro}
Semantic scene understanding has achieved remarkable performance on indoor- and outdoor scenes via pixel-wise semantic segmentation~\cite{mivcuvslik2009semantic}. %. It is able to offer a comprehensive interpretation of the surrounding environment 
It can be utilized directly on a wide range of downstream applications, such as autonomous driving~\cite{feng2020deep,janai2020cv4auto}, navigation in robotics~\cite{SMNet,chen2022trans4map} or in assistive technologies~\cite{zhang2021trans4trans} to name a few.
Recently, Bird's-Eye-View (BEV) semantic perception~\cite{bevformer} can be a solution for enabling a straightforward understanding of the environment and objects therein.
While BEV semantic segmentation has gained traction in outdoor scenes for autonomous driving~\cite{bevformer}, BEV perception has not yet been extensively explored for indoor scenes, which are often characterized by complex and varied structures, objects, and challenging lighting conditions.
For semantically mapping these indoor scenes, sequence-based methods~\cite{SMNet,chen2022trans4map} were proposed, which have to process whole videos and entail a moving camera. As shown in Fig.~\ref{fig1_narrowbev}, (1) these methods rely on computationally expensive processing of entire sequences of video-frames due to the narrow \textcolor[HTML]{78eaf8}{Field of View} of the pinhole camera, and (2) they are constrained to explore indoor mapping on synthetic simulators~\cite{savva2019habitat,xia2018gibson}, due to the lack of real indoor datasets.
These drawbacks limit their applicability to real-world indoor semantic mapping.

To solve these limitations, in this work we introduce \textbf{360BEV} to achieve panoramic semantic mapping for indoor BEV, which is illustrated in Fig.~\ref{fig1_360bev}.
Our considerations are twofold:
(1) To unleash the potential of indoor semantic mapping in real-world scenarios, real indoor databases with BEV semantic labels are crucial and (2) to reduce computational complexity of narrow-FoV sequence methods~\cite{SMNet} (more than $20$ video-frames to process) or the complexity of multi-camera setups~\cite{bevformer} (over $6$ camera views needed) we leverage a single-frame 360{\textdegree} image and thus bypass multi-sensor calibration, synchronization, and data fusion procedures.
With this in mind and to enable 360BEV segmentation we present two real indoor BEV datasets, which are extended from the Matterport3D~\cite{Matterport3D} and Stanford2D3D~\cite{stanford2d3d} datasets. First, the Front-View images captured by pinhole cameras from Matterport3D are extended to 360{\textdegree} panoramas for benchmarking on \textbf{360FV-Matterport}.
Furthermore, to enable bird's-eye view mapping, two BEV datasets, \textbf{360BEV-Matterport} and \textbf{360BEV-Stanford} are established by transforming front-view semantic labels to top-down views.
These two datasets pave the way to -- for the first time -- predicting a complete BEV semantic map from a single-frame 360{\textdegree} image.
Moreover, by decoupling the computationally expensive processing of sequences or multiple views, our direct 360BEV semantic mapping is more streamlined for generating indoor semantic maps. 

Spatial distortions and object deformations in panoramic images~\cite{trans4pass} severely harm the performance of methods proposed for narrow-range image~\cite{guo2022segnext,xie2021segformer} or multi-view perception~\cite{bevformer}.
Thus, to comprehensively investigate the established 360BEV task, we first revisit possible projection paradigms for 360BEV, including: (1) \textit{Early projection} (2) \textit{Late projection} and (3) \textit{Intermediate projection}. Based on our observation that intermediate features maintain dense information, we explore the intermediate projection paradigm and propose a dedicated solution for 360BEV mapping, which we call \textbf{360Mapper}. The challenge in this scheme resides in the feature conversion. While the prior BEVFormer~\cite{bevformer} relied on multi-view perception and SMNet~\cite{SMNet} projects the extracted feature directly via the depth-based transformation index, which are not appropriate for panoramic imagery due to its distortions and deformations, we propose a new transformation method, the \textbf{Inverse Radial Projection (IRP)}, to project features from 2D to 3D representations using only depth information. An additional benefit is that the depth information helps maintain object shape and space layout after being transferred to top-down views, rendering the 2D reference index for feature map as well as the BEV representation more accurate and consistent. Besides, unlike the deformable attention~\cite{bevformer,zhu2020deformabledetr} using multi-scale layers and fusion from multi-view cameras, we adopt \textbf{360Attention} with adaptive sampling offsets to extract information on a single panoramic feature map, yielding the bird's-eye-view feature with less distortion in an adaptive manner. These are combined with the 2D index obtained by IRP to include a deformation-aware mechanism in 360 scenes, which in turn serves to compensate for the adverse effects of distortion. With these designs, our 360Mapper model represents a step towards a more complete and accurate indoor semantic mapping, which has important implications for down-stream applications such as indoor navigation and scene understanding.

Through extensive experiments the new 360BEV task is thoroughly benchmarked with two real indoor BEV datasets, three projection paradigms, and more than ten methods, respectively. Compared to the semantic mapping counterparts, our 360Mapper models achieve state-of-the-art performance, with mean intersection-over-union (mIoU) gains of over $7\%$ on the 360BEV-Matterport dataset and over $9\%$ on the 360BEV-Stanford dataset. 

To summarize, we present the following contributions:
\begin{compactitem}
\item A new \textit{360BEV} task is introduced for the first time to address indoor semantic mapping via a single-frame panoramic image, bypassing complex multi-view perception and narrow-view sequence generation.

\item Two real-world indoor BEV datasets, \ie, \textit{360BEV-Matterport} and \textit{360BEV-Stanford}, are presented to include front-view panoramic images and BEV semantic labels, enabling end-to-end training.

\item \textit{360Mapper} -- a dedicated solution for interior panoramic semantic mapping is proposed,  exhibiting mIoU gains of over $7\%$ as compared to the baseline. 
\end{compactitem}

\section{Related Work}
\label{sec:related_work}
\subsection{Panoramic Semantic Segmentation}
Image semantic segmentation~\cite{wang2021hrnet,xie2021segformer,yuan2020ocr,zhao2017pspnet} has achieved great progress.
In contrast to narrow-FoV perception, panoramic semantic segmentation~\cite{gauge_equivariant,tangent,esteves2020spin_weighted_spherical,spherical_unstructured_grids,hohonet,distortion_aware,orientation,trans4pass}, yielding holistic scene understanding by using a single 360{\textdegree} front-view image, has received increasing attention in recent years. In the field of indoor panorama segmentation, there are some benchmarks that provide synthetic~\cite{InteriorNet18,structured3d} and real~\cite{stanford2d3d} panoramic images and labels for training. Matterport3D~\cite{Matterport3D} has large-scale panoramic images collected from $90$ indoor buildings, yet, it has not been benchmarked due to the lack of corresponding semantic labels.
To enable this, we generate the panoramic semantic segmentation labels through combining original $18$ pinhole camera labels regarding their camera transformation matrices. Therefore, a 360{\textdegree} Front-View (FV) dataset, \textit{360FV-Matterport}, with large-scale real indoor scenes, is provided to facilitate panoramic semantic segmentation. Besides, the 360FV-Matterport dataset is required to perform the late-projection paradigm of BEV semantic mapping.

\subsection{BEV Semantic Mapping}
Apart from front-view image semantic segmentation, some previous work explored top-view semantic segmentation, known as semantic mapping~\cite{chen2022trans4map,mivcuvslik2009semantic} in indoor scenes and bird's-eye-view semantic segmentation~\cite{bevformer,peng2023bevsegformer} in outdoor driving scenes. 
The indoor semantic mapping methods can be divided into three categories according to the level of projection from the front view to the top-down view: \textit{Early-projection} approaches~\cite{mattyus2015enhancing,singh2018self} are performed via general semantic segmentation methods, which first construct the BEV views from perspective images and then apply segmentation. Unfortunately, these pipelines lose fine-grained visual cues during the projection and thus result in unsatisfactory performance for small object segmentation.
\textit{Intermediate-projection} methods~\cite{SMNet,chen2022trans4map} directly take front views as input for holistic indoor scene understanding,
however, they work on synthetic data generated from Gibson~\cite{xia2018gibson} or Habitat~\cite{savva2019habitat} simulators, and rely on time-consuming image sequences. For example, SMNet~\cite{SMNet} gradually captures an average of $2,500$ view-points for each floor to generate a semantic map for indoor scenes. Instead, we explore to achieve efficient allocentric scene understanding via a single panorama image.
The \textit{Late-projection} pipeline~\cite{anderson2018vision,grinvald2019volumetric,maturana2018real,ran2021rs,sengupta2012automatic} performs egocentric semantic segmentation and project labels to top-down views, which are sensitive to depth map and agent pose information, inevitably facing the projection error and under-fitting of model training, thus remaining a suboptimal solution.
There are some BEV-related methods which leverage multiple perspective view sensors or LiDAR sensors and focus on outdoor object detection~\cite{li2022bevformer,liu2022petr,yang2022bevformer}, optical flow estimation~\cite{lee2020pillarflow,luo2021self}, and semantic segmentation~\cite{pan2020cross,zhou2022cross}. Different from previous methods, our 360Mapper is carefully designed for learning indoor holistic representations by forwarding a single panorama without using multi-view images, image sequences, or point clouds.

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.99\linewidth]{Figures/fig2_paradigms.pdf}
    \begin{minipage}[t]{.2\linewidth}
        \vskip-3ex
        \subcaption{Multi-view}\label{paradimgs_1}
    \end{minipage}%
    \begin{minipage}[t]{.2\linewidth}
        \vskip-3ex
        \subcaption{Sequence-based}\label{paradimgs_2}
    \end{minipage}%
    \begin{minipage}[t]{.2\linewidth}
        \vskip-3ex
        \subcaption{Early-projection}\label{paradimgs_3}
    \end{minipage}%
    \begin{minipage}[t]{.2\linewidth}
        \vskip-3ex
        \subcaption{Late-projection}\label{paradimgs_4}
    \end{minipage}%
    \begin{minipage}[t]{.2\linewidth}
        \vskip-3ex
        \subcaption{Intermediate-projection}\label{paradimgs_5}
    \end{minipage}%
    \vskip-1ex
    \caption{\textbf{Paradigms of semantic mapping.} While the narrow-FoV (a) multi-view and (b) sequence-based methods rely on $V{\geq}6$ and $N{\geq}20$ views, the 360{\textdegree}-BEV (c) Early-, (d) Late-, and (e) Intermediate-projection methods use a single panorama.}
    \label{fig:paradigms}
\end{figure*}

\section{Panorama Semantic Mapping (360BEV)}
\label{sec:methodology}
To investigate the 360BEV task, we analyze potential panoramic projection paradigms in Sec.~\ref{sec:3_1_paradigm}. The generation and data statistic of the dataset are detailed in Sec.~\ref{sec:3_3_dataset}. To tackle the challenging panoramic semantic mapping, in Sec.~\ref{sec:3_4_model} we present our solution \textbf{360Mapper} with the \textbf{Inverse Radial Projection} method and \textbf{360Attention} module, which enable distortion-aware feature processing.

\subsection{360 Projection Paradigms}~\label{sec:3_1_paradigm}
As shown in Fig.~\ref{fig:paradigms}, unlike multi-view methods relying on more than six views $V$ (Fig.~\ref{paradimgs_1}) and sequence-based methods using mor than $20$ narrow views $N$ (Fig.~\ref{paradimgs_2}), panoramic semantic mapping uses a single image.
We investigate the latter scenario with three projection paradigms, \ie, \textit{how to process data from front-view panoramas to birds'-eye-view semantics}, which are:
\begin{compactitem}
    \item[(1)] \textit{Early projection: \textbf{Proj.}${\rightarrow}$Enc.${\rightarrow}$Seg.} in Fig.~\ref{paradimgs_3}. This way of processing might harm the original visual information and the spatial relationship of indoor objects, leading to lower performance of semantic mapping. 
    \item[(2)] \textit{Late projection: Enc.${\rightarrow}$Seg.${\rightarrow}$\textbf{Proj.}} in Fig.~\ref{paradimgs_4}. The front-view segmentation errors caused by distortion and deformation of panoramas accumulate and affect the completeness of object masks in the BEV map. 
    \item[(3)] \textit{Intermediate projection: Enc.${\rightarrow}$\textbf{Proj.}${\rightarrow}$Seg.} in Fig.~\ref{paradimgs_5}. In this manner, the encoded feature maintains dense and representative information, which is crucial for view projection. Besides, the projected features are further parsed by the subsequent BEV decoder. 
\end{compactitem}
Based on to these properties, we mainly explore 360BEV with intermediate projections, in which we identify the following challenges: In the feature extraction stage, spatial distortions and object deformations severely hinder the encoder from extracting representative features from the front-view panoramic image. For the intermediate feature projection, only depth information is utilized to consistent view transformation of high-dimensional features. In addition, many large objects in the front view (\eg, \textit{walls}) are projected to thin objects in the top-down view, which greatly impedes capturing wide-range features during projection. 

\subsection{360FV and 360BEV Datasets}~\label{sec:3_3_dataset}
To benchmark the proposed 360FV and 360BEV tasks, in addition to the extended \textit{360FV-Matterport} dataset for Front-View (FV) panoramic semantic segmentation, two Bird's-Eye-View (BEV) datasets, \ie, \textit{360BEV-Matterport} and \textit{360BEV-Stanford}, are put forward for top-down panoramic semantic mapping. 

\noindent\textbf{360FV-Matterport.} 
Compared to Stanford2D3D, which was sampled on the campus, Matterport3D~\cite{Matterport3D} was collected from $90$ residential buildings with diverse indoor scenes. As shown in Fig.~\ref{fig:data_gen}, we convert the $18$ annotations in the multiple narrow FoV images into a single 360{\textdegree} semantic ground truth, according to the corresponding rotation-translation matrices. As a result, our data processing yields in total $10,800$ panoramic semantic labels for front-view panoramic semantic segmentation. 
We believe that such a front-view dataset with multiple complex scenes and diverse object categories can also foster progress in panoramic semantic segmentation. 

\begin{figure}[t]
    \centering
    \includegraphics[width=0.99\linewidth]{Figures/fig3_18_to_1.pdf}
    \vskip-1ex
    \caption{\textbf{360FV semantics generation} from $18$ narrow (60{\textdegree}) views to a panoramic (360{\textdegree}) view on the 360FV-Matterport dataset. $H$, $M$, $L$ represent high, medium, and low positions, respectively. }
    \label{fig:data_gen}
    \vskip-2ex
\end{figure}

\noindent \textbf{360BEV-Stanford.} 
The Stanford2D3D dataset~\cite{stanford2d3d} contains $1,413$ panoramic images, along with the corresponding depths, semantic annotations, and panoramic global XYZ images, where each pixel in panoramic images corresponds to an identified $(x, y, z)$ coordinate. As presented in Fig.~\ref{fig:data_gen_bev}, we can obtain spatial semantic information regarding the corresponding global XYZ image. Based on that, we generate $1,413$ BEV semantic images with $13$ annotated categories within a visible range as our ground truth by applying orthographic projection from generated spatial semantics.
Unlike the projection method in SMNet~\cite{SMNet}, where the challenging objects are removed due to accumulated errors in time series, \ie, \textit{wall}, \textit{door}, \textit{window}, and \textit{floor} classes, our single-frame method can accurately preserve them in our 360BEV dataset, as they are crucial for downstream indoor tasks, \eg, robot navigation. 

\noindent \textbf{360BEV-Matterport.} 
The Matterport3D dataset~\cite{Matterport3D} does not provide the global XYZ modality as Stanford2D3D. Further, the method of generating BEV semantic ground truth from 3D annotated meshes according to dividing room blocks cannot reflect the ability of the panoramic view to cope with flexible scenarios such as crossing bedroom and toilet, hallway and living room.
Inspired by the global XYZ modality~\cite{stanford2d3d}, we generate a global XYZ for each panoramic image by using the provided depth ground truth. In order to generate BEV semantic ground truth corresponding to the panoramic view, several key steps must be considered. Firstly, a panoramic image can be processed as a sphere with rays shooting from the centre of the sphere, where the camera is located. The image pixel $(i,j)$ can be transformed here into the spherical coordinate system, as depicted in Eq.~\eqref{eq:1}, representing as $\Theta$, and $\Phi$. 

\begin{figure}[t]
    \centering
    \includegraphics[width=0.99\linewidth]{Figures/fig4_global_xyz_to_gt3.pdf}
    \vskip-1ex
    \caption{\textbf{360BEV semantics generation} by orthographic projection, from (a) the front-view semantic image and (b) the global XYZ image, to (c) the 360BEV semantic map. }
    \label{fig:data_gen_bev}
    \vskip-2ex
\end{figure}

\begin{equation}\label{eq:1}
\begin{aligned}
\Theta_{i,j} &= \frac{i\pi}{H} + \frac{\pi}{2H}, \\  &i =\{0,\ldots,H{-}1\}, \ j=\{0,\ldots,W{-}1\}, \\
\Phi_{i,j} &= - \frac{2\pi j}{W} + \pi - \frac{\pi}{W},\\ &i=\{0,\ldots,H{-}1\}, \ j=\{0,\ldots,W{-}1\}, 
\end{aligned}
\end{equation}
Here, $\Theta$ and $\Phi$ are angle matrices of panoramic images with size $H {\times} W$, which consist of two dimensional Euler angular equivariant series. 
Given the representation in spherical coordinate systems, each 3D point $(X_{i,j}, Y_{i,j}, Z_{i,j})$ in the camera coordinate system will be obtained through the calculation in Eq.~\eqref{eq:2},
\begin{equation}\label{eq:2}
\begin{aligned}
X_{i,j} &= D_{i,j} \cdot \sin(\Theta_{i,j}) \cdot \sin(\Phi_{i,j}), \\
Y_{i,j} &= D_{i,j} \cdot \cos(\Theta_{i,j}), \\
Z_{i,j} &= D_{i,j} \cdot \sin(\Theta_{i,j}) \cdot \cos(\Phi_{i,j}), 
\end{aligned}
\end{equation}
where $D$ is the panoramic depth information.
After obtaining 3D points, the orthographic projection matrix $P_{v}$ is applied to transform 3D coordinates to 2D panoramic BEV indices $(u, v)$, which is presented in Eq.~\eqref{eq:3}, where $[\mathbf{R}|\mathbf{t}]$ is the transformation matrix.
\begin{equation}\label{eq:3}
\begin{aligned}
\left[\begin{array}{c}
x \\ y \\ z
\end{array}\right] = \mathbf{R}^{-1} \left[\begin{array}{l}
X_{i,j} \\  Y_{i,j} \\ Z_{i,j}
\end{array}\right]-\mathbf{t}, \\ 
% \quad
\underbrace{\left[\begin{array}{l}
u \\ v \\ 0 \\ 1
\end{array}\right] = P_{v}\left[\begin{array}{l}
x \\ y \\ z \\ 1
\end{array}\right]}_{\text {Orthographic projection}}.
\end{aligned}
\end{equation}

\noindent\textbf{Dataset statistics.}
After data transformation, two BEV datasets for panoramic semantic mapping are obtained. The overall statistical information of the two datasets is shown in Table~\ref{tab:datasets}. 
The Matterport3D dataset~\cite{Matterport3D} consists of reconstructed 3D meshes of $90$ indoor environments, such as homes, offices, and churches. There are $40$ object categories in the dense annotation. However, many of them are relatively rare in the original dataset, \eg, \textit{TV} and \textit{beam} (${\ll}0.1\%$), which are excluded. Nonetheless, in order to produce dense BEV semantic map, we keep classes such as \textit{walls} and \textit{doors} which appear as thin lines and which are especially challenging in BEV tasks. 
Thus, 360BEV-Matterport maintains the $20$ most common object categories and merges some uncommon classes, ranked by the number of object instances: \texttt{wall}, \texttt{floor}, \texttt{chair}, \texttt{door}, \texttt{table}, \texttt{picture}, \texttt{furniture}, \texttt{objects}, \texttt{window}, \texttt{sofa}, \texttt{bed}, \texttt{sink}, \texttt{stairs}, \texttt{ceiling}, \texttt{toilet}, \texttt{mirror}, \texttt{shower}, \texttt{bathtub}, \texttt{counter}, and \texttt{shelving}. The 360BEV-Matterport dataset is divided into three subsets, consisting of $61$ scenes for training, $7$ scenes for validation, and $18$ scenes for testing. Besides, we further calculate the per-class pixel number and per-class frequency of both BEV datasets in Fig.~\ref{fig:dataset_stat}. It is worth noting that the $floor$ class has a much higher frequency on both datasets. This category is important for tasks that rely on complete maps, such indoor navigation and is therefore also retained.

\input{Tables/tab1_datasets}

\subsection{Proposed Model: 360Mapper}~\label{sec:3_4_model}

\noindent\textbf{Overall Architecture.} After introducing the 360BEV task and datasets, in this section, we describe our \textbf{360Mapper} model, which considers the paradigm of intermediate projection. As shown in Fig.~\ref{fig:model}, our end-to-end 360Mapper framework includes four steps:
(1) The panoramic images are fed into the transformer-based backbone, which extracts contextual features from the front view. (2) The \textbf{Inverse Radial Projection (IRP)} module obtains a 2D index by projecting reference points which are generated by panoramic depth images. (3) The \textbf{360Attention} module enhances the feature from the front-view encoder by using the index from IRP and generates offsets from BEV queries to eliminate the effects of distortion. (4) The lightweight decoder parses the projected feature map and predicts the semantic maps in top-down view.

\begin{figure}[t]
    \centering
    \begin{minipage}[t]{.99\linewidth}
        \frame{\includegraphics[width=0.99\linewidth]{Figures/fig5_s2d3d_stat.pdf}}
        % \vskip-1ex
        \subcaption{Class distribution of 360BEV-Stanford dataset }\label{stat_s2d3d}
    \end{minipage}%
    \hfill
    \begin{minipage}[t]{.99\linewidth}
        \frame{\includegraphics[width=0.99\linewidth]{Figures/fig5_mp3d_stat.pdf}}
        % \vskip-1ex
        \subcaption{Class distribution of 360BEV-Matterport dataset}\label{stat_mp3d}
    \end{minipage}%
    \caption{\textbf{Per-class pixel number (logarithmic) and frequency ($\%$) distribution} of two 360BEV datasets.}
    \label{fig:dataset_stat}
    \vskip-3ex
\end{figure}

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.99\linewidth]{Figures/fig6_structure.pdf}
    \begin{minipage}[t]{.5\linewidth}
        \vskip-3ex
        \subcaption{360Mapper model}\label{model_1}
    \end{minipage}%
    \begin{minipage}[t]{.4\linewidth}
        \vskip-3ex
        \subcaption{360Attention module}\label{fig:modelb}
    \end{minipage}%
    \vskip-1ex
    \caption{\textbf{Architecture of 360Mapper and the 360Attention module.} The 360Mapper model includes the encoder for extracting features from the front-view panoramic image, the 360Attention module for feature projection, and the decoder for parsing the projected feature to the BEV semantic map. The offsets are obtained by a linear layer and added with the 2D index that is obtained by Inverse Radial Projection (IRP), yielding the sampling locations for 360BEV feature projection.}
    \label{fig:model}
    \vskip-2ex
\end{figure*}

\noindent \textbf{Inverse Radial Projection.} 
Next, we propose a flexible projection method, the Inverse Radial Projection (IRP), for which the input of panoramic depth is included. We can easily obtain a top-view mask map by projecting from depth information. This mask map is then used to generate 3D reference points with the corresponding map height. 3D reference points are projected onto the sphere to generate 2D reference indexes, as shown in Eq.~\eqref{eq:4}, where $ID_h$ and $ID_w$ represent the index values of the 2D reference for the height and width of the feature map, respectively. The 2D reference indexes are then used to locate the corresponding feature points on the encoded front-view feature map. 
\begin{equation}\label{eq:4}
\begin{aligned}
\Phi  &= \tan^{-1} \frac{y}{x},\\
\Theta &= \tan^{-1} \left(\frac{x}{z} \cdot \frac{1}{\cos(\Phi)} \right) ,\\
ID_h &= \left\lceil\frac{H \Theta}{\pi}\right\rceil ,\\
ID_w &= \left\lceil\left(\frac{\Phi}{\pi} - \frac{1}{W}\right) \cdot \frac{W}{2}\right\rceil.
\end{aligned}
\end{equation}
Due to the distortions in the stitching process of the panorama, it is hard to project the 3D reference points exactly onto the 2D front-view plane by rotation and translation. Thus, we use the depth map to generate a map mask that better describes the shape of the map, so that the accurate projection with the mask not only makes the amount of data entering the 360Attention much smaller, which is conducive to the fast convergence of the model, but also facilitates the use of sampling offsets for 360Attention.

\noindent \textbf{360Attention.}
In Fig.~\ref{fig:modelb}, the proposed 360Attention generates sampling offsets through the linear layer in an adaptive manner.
Given the BEV query $\boldsymbol{q} \in \mathbb{R}^{N \times C_{Emb}}$ as input, where $N{=}{h{\times}w}$ is the length of query, a $mask{(\cdot)}$ operation is applied on $\boldsymbol{q}$ and $\boldsymbol{p}$ to mask out irrelevant points and 2D indexes according to the mask map $M_{map}$ from IRP, which is crucial to keep $\boldsymbol{q}$ and $\boldsymbol{p}$ efficient and reducing computation of 360Attention ($\sum M_{map} {<} N$). 
The sampling offset $\Delta \boldsymbol{p}_{q,ij}$ and attention weight $\mathcal{A}_{ij}{\in}[0, 1]$ are predicted through BEV query by linear layers respectively. The adaptive sampling offsets are then added to the extended 2D index $\boldsymbol{p}$ to obtain distortion-aware sampling locations.
The 360Attention module can be denoted as:
\begin{equation}\label{eq:5}
\begin{aligned}
&\operatorname{360Attn}(\boldsymbol{q}, \boldsymbol{p}, \boldsymbol{f}_{360}) =\\ &\sum_{i = 1}^{N_{\text {head}}}\mathcal{W}_{i} \sum_{j = 1}^{N_{\text{point}}}\mathcal{A}_{ij}{\cdot} \boldsymbol{f}_{360}\left(mask\left(\boldsymbol{p}\right)+\Delta \boldsymbol{p}_{q,ij}\right),
\end{aligned}
\end{equation}
where $\boldsymbol{q}$, $\boldsymbol{p}$, and $\boldsymbol{f}_{360}$ indicate the query, the extended 2D index, and panoramic feature map, respectively. The linear layer $\mathcal{W}_{i} {\in} \mathbb{R}^{C\times (C/N_{head})}$ is specific to each attention head $i$, where $C$ is the feature dimension and $N_{head}$ is the number of heads.
The attention weight $\mathcal{A}_{ij}$ represents the importance of the sampled points $j$, where $\sum \mathcal{A}_{ij} {=}1$. The panoramic features $\boldsymbol{f}_{360}$ and the adaptive sampling locations $(mask(\boldsymbol{p}){+}\Delta \boldsymbol{p}_{q,ij})$ are aggregated using attention weights $\mathcal{A}_{ij}$ to produce a BEV output. Afterwards, the mask map $M_{map}$ is applied to assemble the BEV output as $\boldsymbol{q}' {\in} \mathbb{R}^{N \times C_{Emb}}$. After added a residual term of $\boldsymbol{q}$, the BEV result $\boldsymbol{q}{+}\boldsymbol{q}'$ is forwarded to the next 360Attention module.

Compared to Spatial Cross-Attention module in BEVFormer~\cite{bevformer}, the difference lies in: (1) Instead of relying on multi-view features across multiple cameras, our 360Attention module is designed to directly adopt adaptive sampling offsets to extract feature from a single panoramic feature map. (2) Our module gets rid of the projection of 3D reference points to different image views using the projection matrix, thus compensating for the lack of front-view perception. (3) The mask operation is applied to maintain the BEV query efficient and adaptive to front-view panoramic features by using depth information as a bridge. Through these non-trivial designs, the BEV feature map generated by 360Attention is able to effectively neutralise the effects of front-view distortion.  

\section{Experiments}
\label{sec:experiments}

\subsection{Implementation Details}
We train 360Mapper models with 4 A100 GPUs with an initial learning rate of $6\textrm{e}^{-5}$, scheduled by the  step strategy over $50$ epochs. AdamW is the optimizer with epsilon $1\textrm{e}^{-8}$, weight decay is $0.01$ and batch size is $4$ on each GPU. The panoramic image size of 360FV-Matterport and Stanford2D3D~\cite{stanford2d3d} are both $512{\times}1024$. The resolution of panoramic images on both 360BEV-Stanford and 360BEV-Matterport datasets are $512{\times}1024$ as input for 360Mapper training, while the output BEV maps are set to $500{\times}500$, which correspond to a perception range of $10m{\times}10m$. Following~\cite{SMNet,chen2022trans4map}, evaluation metrics are pixel-wise accuracy~(Acc), pixel recall~(mRecall), precision~(mPrecision), and mean Intersection-over-Union~(mIoU).

\subsection{Panorama Semantic Segmentation (360FV)}

\noindent\textbf{Results on Stanford2D3D.}
\input{Tables/tab2_stanford2d3d_fv}
The front-view panoramic semantic segmentation results on Stanford2D3D dataset are presented in Table~\ref{tab:s2d3d_front}.
All results are averaged over $3$ cross validation folds. Thanks to the proposed 360Attention module, our 360Mapper model is better capable of handling deformations in panoramas, yielding $54.3\%$ in mIoU, with ${>}2\%$ performance gains as compared to the previous state-of-the-art Trans4PASS~\cite{trans4pass} and CFCB~\cite{zheng2023complementary}. The promising result in front-view panoramas has initially revealed the potential of our model in extracting 360{\textdegree} front-view features, which is crucial for the BEV semantic mapping task as well.

\noindent\textbf{Results on 360FV-Matterport.}
\input{Tables/tab3_matterport3d_fv_val.tex}
Due to the limited amount of data on Stanford2D3D, for the first time, a large-scale benchmark, 360FV-Matterport, is brought to the community of front-view panoramic semantic segmentation. To conduct a comprehensively comparison on this new benchmark, in Table~\ref{tab:mp3d_front_val}, four state-of-the-art methods are selected and reproduced.
Compared to the Trans4PASS~\cite{trans4pass} and Trans4PASS+~\cite{trans4passplus} models, our model has respective ${+}4.44\%$ and ${+}3.75\%$ improvements. Furthermore, our model surpasses RGB-D HoHoNet~\cite{hohonet} and SegFormer~\cite{xie2021segformer} with ${+}1.50\%$ and ${+}0.82\%$ mIoU gains. The results indicate that our model can consistently achieve the state-of-the-art performance on large-scale datasets for panoramic semantic segmentation. 

\subsection{Panorama Semantic Mapping (360BEV)}

\input{Tables/tab4_stanford2d3d_bev}
\noindent\textbf{Results on 360BEV-Stanford.} 
In Table~\ref{tab:s2d3d_topdown}, to study the Early projection mode, SegFormer~\cite{xie2021segformer} and SegNeXt~\cite{guo2022segnext} with different backbones, are selected, which merely reach unsatisfactory results. The results indicate that the pre-projected RGB maintains less rich spatial and visual information of front-view images. Using Late projection, SegFormer with the same MiT-B2 backbone achieves $18.65\%$ mIoU and surpasses the one using Early projection, still yielding sub optimal semantic mapping results. Interestingly, all methods using Intermediate projection obtain more than $30\%$ mIoU. While using the same MiT-B2 backbone and our proposed 360Mapper achieves $45.78\%$ with ${+}9.70\%$ gains compared to the baseline Trans4Map~\cite{chen2022trans4map}. Further, our efficient model~(MiT-B0) outperforms Trans4Map~(MiT-B4) with ${+}05.73\%$ mIoU gains. With a stronger CNN backbone MSCA-B from SegNeXt~\cite{guo2022segnext}, our method reaches the best score with $46.44\%$ in mIoU, which indicates 360Mapper is flexible to both CNN- and Transformer-based backbones.


\noindent\textbf{Results on 360BEV-Matterport.}
\input{Tables/tab5_matterport3d_bev_val}
To investigate 360BEV on a large-scale dataset, in Table~\ref{tab:mp3d_topdown_val}, we conduct the same experiments on the 360BEV-Matterport dataset. SegFormer~\cite{xie2021segformer} and SegNeXt~\cite{guo2022segnext} adopt Early projection and show better performance than the approaches which adopt Late projection.
The reason for this is Late projection methods are constrained by their lower performance in front-view semantic segmentation, which affects the projected BEV semantic maps. In contrast, using Intermediate projection, our 360Mapper models based on two different model scales, \ie, MiT-B0 and MiT-B2, show overall promising performance with $36.98\%$ and $44.32\%$ mIoU, respectively. Compared to the previous state-of-the-art Trans4Map~\cite{chen2022trans4map} (MiT-B2), our approach with MiT-B2 has improvements by ${+}5.52\%$ in accuracy, ${+}7.94\%$ in mRecall, ${+}6.95\%$ in mPrecision, and ${+}7.60\%$ in mIoU. Surprisingly, our 360Mapper with MiT-B2 outperforms Trans4Map with MiT-B4 with ${+}6.28\%$ in mIoU. Besides, to compare multi-view methods, we reproduce BEVFormer~\cite{bevformer} by using a single panorama instead of six views of pinhole cameras. Our 360Mapper outperforms BEVFormer (MiT-B2) with ${+}11.81\%$ mIoU. Furthermore, we verify the flexibility of 360Mapper by using a CNN-based MSCA-B backbone~\cite{guo2022segnext}, which obtains the highest mIoU score with $46.31\%$. All results are in line with our observation that Intermediate projection can preserve dense visual cues and long-range information from front-view panoramas, and deliver more valuable context for BEV semantic mapping, leading to this superiority of 360Mapper, as compared to the other paradigms.  

\input{Tables/tab6_analysis}
\noindent\textbf{Analysis of 360Attention.}
To better understand 360Attention, we further conduct an analysis of the offset mechanisms in 360Attention and the backbone selection, in Table~\ref{tab:analysis}. First, in \circled{1}\circled{2}\circled{3}, we select three model scales, \ie, MiT-B0, MiT-B2, and MiT-B4, to verify the effect of model capacity in 360Attention. The three models obtain good performance, showing that 360Attention has positive effects in different model scales. Besides, different offset schemes are compared among \circled{2}\circled{4}\circled{5}\circled{6}, which are deformable, multi-scale, fixed-range, and separate offset.
All of them have the same MiT-B2 backbone.
Here, \circled{2} shows the superiority of deformable offset which has a better performance ($44.32\%$). However, these comparable results prove that our 360Attention design is robust to offset mechanisms.
Further, to analyze the effect of backbone selection, we choose transformer-based MiT-B2~\cite{xie2021segformer} and CNN-based MSCA-B~\cite{guo2022segnext} as in \circled{2}\circled{7}. A stronger backbone~\cite{guo2022segnext} shows a further improvement of mIoU (${+}1.99\%$), which shows the flexibility of our approach regarding the backbone variants.

\subsection{Qualitative Analysis}
To analyze the predicted semantic maps, we visualise the results from the validation set of the 360BEV-Matterport dataset. In Fig.~\ref{fig:vis}, from left to right are input images, results of baseline~\cite{chen2022trans4map}, results of our 360Mapper, and ground truth. Thanks to the IRP projection and 360Attention, the segmentation results of 360Mapper are much better.
In the first scene in Fig.~\ref{fig:vis}, 360Mapper is able to successfully classify \textit{chairs}, while the baseline model fails, predicting several \textit{tables} and misclassifying the distant ground as another \textit{table}. In the second scene, the segmentation of the \textit{tables} derived by the baseline is incomplete. Furthermore, in the last zoomed-in scene, 360Mapper provides accurate semantic maps, such as in \textit{counter}, \textit{chair}, and \textit{wall} categories, whereas the baseline Trans4Map~\cite{chen2022trans4map} misclassifies them as \textit{tables} and \textit{doors}. Based on the qualitative analysis, our 360Mapper can effectively handle object deformations and image distortions, yielding better BEV semantic maps.


\begin{figure}[t]
    \footnotesize
    \setlength\tabcolsep{1pt}
    {
    \newcolumntype{P}[1]{>{\centering\arraybackslash}p{#1}}
    \begin{tabular}{@{}*{10}{P{0.089\columnwidth}}@{}}
    {\cellcolor[rgb]{0.68,0.78,0.91}}\textcolor{white}{wall} 
    &{\cellcolor[rgb]{0.44,0.50,0.56}}\textcolor{white}{floor}
    &{\cellcolor[rgb]{0.60,0.87,0.54}}\textcolor{black}{chair}
    &{\cellcolor[rgb]{0.77,0.69,0.84}}\textcolor{white}{door}
    &{\cellcolor[rgb]{1.00,0.50,0.05}}\textcolor{white}{table} 
    &{\cellcolor[rgb]{0.84,0.15,0.16}}\textcolor{white}{pictu.} 
    &{\cellcolor[rgb]{0.12,0.47,0.71}}\textcolor{white}{furni.}
    &{\cellcolor[rgb]{0.74,0.74,0.13}}\textcolor{black}{objec.}
    &{\cellcolor[rgb]{1.00,0.60,0.59}}\textcolor{black}{windo.}
    &{\cellcolor[rgb]{0.17,0.63,0.17}}\textcolor{white}{sofa} \\
    {\cellcolor[rgb]{0.89,0.47,0.76}}\textcolor{black}{bed}
    & {\cellcolor[rgb]{0.87,0.62,0.84}}\textcolor{black}{sink}
    &{\cellcolor[rgb]{0.58,0.40,0.74}}\textcolor{white}{stairs} 
    &{\cellcolor[rgb]{0.55,0.64,0.32}}\textcolor{white}{ceil.} 
    &{\cellcolor[rgb]{0.52,0.24,0.22}}\textcolor{white}{toilet} 
    &{\cellcolor[rgb]{0.62,0.85,0.90}}\textcolor{black}{mirror} 
    &{\cellcolor[rgb]{0.61,0.62,0.87}}\textcolor{black}{show.}
    &{\cellcolor[rgb]{0.91,0.59,0.61}}\textcolor{black}{batht.}
    &{\cellcolor[rgb]{0.39,0.47,0.22}}\textcolor{white}{count.} 
    &{\cellcolor[rgb]{0.55,0.34,0.29}}\textcolor{white}{shelv.} \\
    \end{tabular}
    }
    \centering
    \begin{tabular}{c c c c}
        \vspace{1pt}
        \raisebox{-0.5\height}{\includegraphics[width=0.18\textwidth]{Figures/fig7_qualitative_analysis/pano1.png}} &
        \raisebox{-0.5\height}{\includegraphics[width=0.09\textwidth]{Figures/fig7_qualitative_analysis/1_baseline.png}} &
        \raisebox{-0.5\height}{\includegraphics[width=0.09\textwidth]{Figures/fig7_qualitative_analysis/1_360Mapper.png}} &
        \raisebox{-0.5\height}{\includegraphics[width=0.09\textwidth]{Figures/fig7_qualitative_analysis/1_groundtruth.png}}\\
         % \vspace{0.5cm}

        \vspace{1pt}

        \raisebox{-0.5\height}{\includegraphics[width=0.18\textwidth]{Figures/fig7_qualitative_analysis/pano2.png}} &
        \raisebox{-0.5\height}{\includegraphics[width=0.09\textwidth]{Figures/fig7_qualitative_analysis/2_baseline.png}} &
        \raisebox{-0.5\height}{\includegraphics[width=0.09\textwidth]{Figures/fig7_qualitative_analysis/2_360Mapper.png}} &
        \raisebox{-0.5\height}{\includegraphics[width=0.09\textwidth]{Figures/fig7_qualitative_analysis/2_groundtruth.png}}\\
        \vspace{1pt}

        \raisebox{-0.5\height}{\includegraphics[width=0.18\textwidth]{Figures/fig7_qualitative_analysis/last_row_pano.png}} &
        \raisebox{-0.5\height}{\includegraphics[width=0.09\textwidth]{Figures/fig7_qualitative_analysis/last_row_baseline.png}} &
        \raisebox{-0.5\height}{\includegraphics[width=0.09\textwidth]{Figures/fig7_qualitative_analysis/last_row_360Mapper.png}} &
        \raisebox{-0.5\height}{\includegraphics[width=0.09\textwidth]{Figures/fig7_qualitative_analysis/last_row_gt.png}}\\
        \noalign{\vskip 2mm}   
        Input&Baseline&360Mapper&Ground Truth
    \end{tabular}
    \vskip -2ex
    \caption{\textbf{Qualitative analysis} on the 360BEV-Matterport dataset. Black regions are \texttt{void}. Zoom-in for better view.}
    \label{fig:vis}
    \vskip -2ex
\end{figure}

\section{Conclusion}
\label{sec:conclusion}
In this paper, we introduce 360BEV, a new task to conduct panoramic semantic mapping in indoor scenes, \ie, from a single panoramic image to a holistic BEV semantic map.
To enable this, we present 360BEV-Matterport and 360BEV-Stanford, extending off-the-shelf datasets for the 360BEV task. 
We revisit existing transformation paradigms and propose 360Mapper, a novel end-to-end architecture specifically designed for panoramic semantic mapping. As a result, 360Mapper outperforms state-of-the-art counterparts by a clear margin.

\noindent\textbf{Limitations.}
The 360BEV task is first investigated on indoor datasets. In future work, 360BEV on outdoor driving scenes will be explored. Using one panoramic camera to construct a holistic map has the potential to solve the bottleneck of sensor fusion and calibration caused by the multi-view BEV counterparts. Semantic scene completion is another promising direction, yielding semantic map anticipation beyond the visible range for downstream tasks. 

\clearpage
{\small
\bibliographystyle{ieee_fullname}
\bibliography{main}
}


%%%%%%%%% 

\input{TexContents/appendix}


\end{document}