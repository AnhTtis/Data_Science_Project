% WACV 2024 Paper Template
% based on the CVPR 2023 template (https://media.icml.cc/Conferences/CVPR2023/cvpr2023-author_kit-v1_1-1.zip) with 2-track changes from the WACV 2023 template (https://github.com/wacv-pcs/WACV-2023-Author-Kit)
% based on the CVPR template provided by Ming-Ming Cheng (https://github.com/MCG-NKU/CVPR_Template)
% modified and extended by Stefan Roth (stefan.roth@NOSPAMtu-darmstadt.de)

\documentclass[10pt,twocolumn,letterpaper]{article}

%%%%%%%%% PAPER TYPE  - PLEASE UPDATE FOR FINAL VERSION
\usepackage[final,algorithms]{wacv} 
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}

\usepackage{float}
\usepackage{lipsum}
\usepackage{stfloats}
\usepackage{multicol}
\usepackage{multirow}
\usepackage{bm}
\usepackage{etoolbox}

\usepackage{array}
\usepackage{tabulary}
\usepackage[table,dvipsnames]{xcolor}
\usepackage{paralist}
\usepackage{booktabs}
\usepackage{adjustbox}
\usepackage[shortlabels]{enumitem}
\usepackage{tabu}
\usepackage{caption}
\captionsetup[table]{format=plain,labelformat=simple,labelsep=period}%
\usepackage{subcaption}
\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false,citecolor=blue,linkcolor=red]{hyperref}
\usepackage{icomma}
\usepackage{arydshln}
\newcommand\ver[1]{\rotatebox[origin=c]{90}{#1}}
\newcommand{\fl}[1]{\multicolumn{1}{c}{#1}}
\definecolor{gray}{rgb}{0.3,0.3,0.3}
\definecolor{blue}{rgb}{0,0.5,1}
\definecolor{mask_red}{rgb}{1,0,0.8}
\definecolor{green}{rgb}{0.2,1,0.2}
\definecolor{rblue}{rgb}{0,0,1}
\definecolor{lightblue}{HTML}{6495ed}
\definecolor{lightred}{HTML}{F19C99}
\newcommand{\gray}[1]{\textcolor{gray}{#1}}
\newcommand{\green}[1]{\textcolor[RGB]{96,177,87}{#1}}
\newcommand{\lightblue}[1]{\textcolor{lightblue}{#1}}
\newcommand{\fn}[1]{\footnotesize{#1}}
\newcommand{\gbf}[1]{\green{\bf{\fn{(#1)}}}}
\newcommand{\bbf}[1]{\lightblue{\bf{\fn{(#1)}}}}
\newcommand{\rbf}[1]{\gray{\bf{\fn{(#1)}}}}
\newcommand{\obf}[1]{\textcolor{orange}{\bf{\fn{(#1)}}}}
\definecolor{graytablerow}{gray}{0.6}
\newcommand{\grow}[1]{\textcolor{graytablerow}{#1}}
\usepackage[accsupp]{axessibility}  % Improves PDF readability for those with disabilities.

\usepackage{tikz}
\newcommand*\circled[1]{\tikz[baseline=(char.base)]{
\node[shape=circle,fill=gray,inner sep=0.5pt] (char) {\textcolor{white}{\footnotesize \textbf{#1}}};}}

\begin{document}

%%%%%%%%% TITLE
\title{360BEV: Panoramic Semantic Mapping for Indoor Bird's-Eye View}


\author{
Zhifeng Teng$^{1,4,}$\thanks{Equal contribution.},
~~Jiaming Zhang$^{1,*,}$\thanks{Corresponding author (e-mail: {\tt jiaming.zhang@kit.edu}).},
~~Kailun Yang$^2$,
~~Kunyu Peng$^1$,\\
~~Hao Shi$^3$,
~~Simon ReiÃŸ$^{1}$,
~~Ke Cao$^{1}$,
~~Rainer Stiefelhagen$^1$\\
\normalsize
$^1$Karlsruhe Institute of Technology,
\normalsize
~$^2$Hunan University,
\normalsize
~$^3$Zhejiang University,
\normalsize
~$^4$Agile Robots AG
}

\maketitle
% Remove page # from the first page of camera-ready.
% \ificcvfinal\thispagestyle{empty}\fi


%%%%%%%%% ABSTRACT
\input{TexContents/abstract}

%%%%%%%%% BODY TEXT
\begin{figure}[t]
    \centering
    \includegraphics[width=0.99\linewidth]{Figures/first_diagram_2.pdf}
    \begin{minipage}[t]{.5\columnwidth}
        \vskip-3ex
        \subcaption{Narrow BEV}\label{fig1_narrowbev}
    \end{minipage}%
    \begin{minipage}[t]{.5\columnwidth}
        \vskip-3ex
        \subcaption{360BEV}\label{fig1_360bev}
    \end{minipage}%
    \vskip-1ex
    \caption{Semantic mapping from egocentric front-view images to allocentric BEV semantics. While (a) the narrow-BEV method has limited perception and map range, (b) 360BEV has an omnidirectional \textcolor[HTML]{3cc7fa}{Field of View}, yielding a more complete BEV map by using our 360Mapper model.}
    \label{fig:360bev}
    \vskip-3ex
\end{figure}

\section{Introduction}
\label{sec:intro}
Semantic scene understanding has achieved remarkable performance on indoor- and outdoor scenes via pixel-wise semantic segmentation~\cite{mivcuvslik2009semantic}. It can be utilized directly on a wide range of downstream applications, such as autonomous driving~\cite{feng2020deep,janai2020cv4auto}, navigation in robotics~\cite{SMNet,chen2022trans4map} or in assistive technologies~\cite{zhang2021trans4trans} to name a few.
Recently, Bird's-Eye-View (BEV) semantic perception~\cite{bevformer} can be a solution for enabling a straightforward understanding of the environment and objects therein.
While BEV semantic segmentation has gained traction in outdoor scenes for autonomous driving~\cite{bevformer}, BEV perception has not yet been extensively explored for indoor scenes, which are often characterized by complex and varied structures, objects, and challenging lighting conditions.
For semantically mapping these indoor scenes, sequence-based methods~\cite{SMNet,chen2022trans4map} were proposed, which have to process whole videos and entail a moving camera. As shown in Fig.~\ref{fig1_narrowbev}, (1) these methods rely on computationally expensive processing of entire sequences of video-frames due to the narrow \textcolor[HTML]{3cc7fa}{Field of View} of the pinhole camera, and (2) they are constrained to explore indoor mapping on synthetic simulators~\cite{savva2019habitat,xia2018gibson}, due to the lack of real indoor datasets.
These drawbacks limit their applicability to real-world indoor semantic mapping.

To solve these limitations, in this work, we introduce \textbf{360BEV} to achieve panoramic semantic mapping for indoor BEV, which is illustrated in Fig.~\ref{fig1_360bev}.
Our considerations are twofold:
(1) To unleash the potential of indoor semantic mapping in real-world scenarios, real indoor databases with BEV semantic labels are crucial; (2) To reduce the computational complexity of narrow-FoV sequence methods~\cite{SMNet} (${\geq}20$ video-frames to process) %\#frame${\geq}$20)
or the complexity of multi-camera setups~\cite{bevformer} (${\geq}6$ camera views needed), %\#view${\geq}$6) 
we leverage a single-frame 360{\textdegree} image with depth information and thus bypass multi-sensor calibration, synchronization, and data fusion procedures.
With this in mind and to enable 360BEV segmentation we present two real indoor BEV datasets, which are extended from the Matterport3D~\cite{Matterport3D} and Stanford2D3D~\cite{stanford2d3d} datasets. First, the Front-View images captured by pinhole cameras from Matterport3D are extended to 360{\textdegree} panoramas for benchmarking on \textbf{360FV-Matterport}.
Furthermore, for the first time, two BEV datasets, \textbf{360BEV-Matterport} and \textbf{360BEV-Stanford} are established to enable bird's-eye view panoramic semantic mapping, \ie, predicting a complete BEV semantic map from a single-frame 360{\textdegree} image with depth.
Moreover, by decoupling the computationally expensive processing of sequences or multiple views, our direct 360BEV semantic mapping is more streamlined for generating indoor semantic maps.  

However, spatial distortions and object deformations in panoramic images~\cite{trans4pass} severely harm the performance of methods proposed for narrow-range image~\cite{guo2022segnext,xie2021segformer} or multi-view perception~\cite{bevformer}.
Thus, to comprehensively investigate the established 360BEV task, we first revisit three possible projection paradigms, including: (1) \textit{Early projection}, (2) \textit{Late projection}, and (3) \textit{Intermediate projection}. Based on our observation that intermediate features maintain dense information, we explore the intermediate projection paradigm and propose a dedicated solution for 360BEV mapping, which we call \textbf{360Mapper}. The challenge in this scheme resides in the feature conversion. While the prior BEVFormer~\cite{bevformer} relied on multi-view perception and SMNet~\cite{SMNet} projects the extracted feature directly via the depth-based transformation index, which is not appropriate for panoramic imagery due to its distortions and deformations, we propose a new transformation method, the \textbf{Inverse Radial Projection (IRP)}, to project features from 2D to 3D representations using only depth information. 
An additional benefit is that the depth information helps maintain object shape and space layout after being transferred to top-down views, rendering the 2D reference index for the feature map as well as the BEV representation more accurate and consistent. Besides, unlike the deformable attention~\cite{bevformer,zhu2020deformabledetr} using multi-scale layers and fusion from multi-view cameras, we adopt \textbf{360Attention} with adaptive sampling offsets to extract information from omnidirectional feature maps, yielding the bird's-eye-view feature with less distortion in an adaptive manner. 
These are combined with the 2D index obtained by IRP to include a deformation-aware mechanism in 360 scenes, which in turn serves to compensate for the adverse effects of distortion. 
With these designs, our 360Mapper model represents a step towards a more complete and accurate indoor semantic mapping, which has important implications for downstream applications such as indoor navigation and scene understanding.

Through extensive experiments, the new 360BEV task is thoroughly benchmarked with two real indoor BEV datasets, three projection paradigms, and more than ten methods, respectively. Compared to the semantic mapping counterparts, our 360Mapper models achieve state-of-the-art performance, with mean intersection-over-union (mIoU) gains of ${>}7\%$ on the 360BEV-Matterport dataset and ${>}9\%$ on the 360BEV-Stanford dataset. 

To summarize, we present the following contributions:
\begin{compactitem}
\item A new \textit{360BEV} task is introduced for the first time to address indoor semantic mapping via a single-frame panoramic image, decoupling complex processing of multi-view or sequence inputs. 

\item Two indoor BEV datasets, \ie, \textit{360BEV-Matterport} and \textit{360BEV-Stanford}, are extended with front-view panoramic images and BEV semantic labels, thoroughly benchmarking panoramic semantic mapping. 

\item \textit{360Mapper} model -- addressing spatial distortions and object deformations in panoramas -- is proposed as a dedicated solution for interior panoramic semantic mapping and achieves state-of-the-art performance.

\end{compactitem}

\section{Related Work}
\label{sec:related_work}
\subsection{Panoramic Semantic Segmentation}
Image semantic segmentation~\cite{wang2021hrnet,xie2021segformer,yuan2020ocr,zhao2017pspnet} has achieved great progress.
In contrast to narrow-FoV perception, panoramic semantic segmentation~\cite{gauge_equivariant,tangent,esteves2020spin_weighted_spherical,spherical_unstructured_grids,hohonet,distortion_aware,orientation,trans4pass}, yielding holistic scene understanding by using a single 360{\textdegree} front-view image, has received increasing attention in recent years. Besides, 3D60~\cite{zioulis20193D60} and Pano3D~\cite{albanis2021pano3d} datasets are generated for depth estimation from 360{\textdegree} images, but lack semantic labels.
In indoor panorama segmentation, there are some benchmarks that provide synthetic~\cite{InteriorNet18,structured3d} and real~\cite{stanford2d3d} panoramic images and labels for training. Matterport3D~\cite{Matterport3D} has large-scale panoramic images collected from $90$ indoor buildings, yet, it has not been benchmarked due to the lack of corresponding panoramic semantic labels.
To enable this, we generate the panoramic semantic segmentation labels by combining the original $18$ pinhole camera labels regarding their camera transformation matrices. Therefore, a 360{\textdegree} Front-View (FV) dataset, \textit{360FV-Matterport}, with large-scale real indoor scenes, is provided to facilitate panoramic semantic segmentation. Besides, the 360FV-Matterport dataset is required to perform the late-projection paradigm of BEV semantic mapping.


\subsection{BEV Semantic Mapping}
Apart from front-view image semantic segmentation, some previous work explored top-view semantic segmentation, known as semantic mapping~\cite{chen2022trans4map,mivcuvslik2009semantic} in indoor scenes and bird's-eye-view semantic segmentation~\cite{bevformer,peng2023bevsegformer} in outdoor driving scenes. 
The indoor semantic mapping methods can be divided into three categories according to the level of projection from the front view to the top-down view: \textit{Early-projection} approaches~\cite{mattyus2015enhancing,singh2018self} are performed via general semantic segmentation methods, which first construct the BEV views from perspective images and then apply segmentation. Unfortunately, these pipelines lose fine-grained visual cues during the projection and thus result in unsatisfactory performance for small object segmentation.
\textit{Intermediate-projection} methods~\cite{SMNet,chen2022trans4map} directly take front views as input for holistic indoor scene understanding,
however, they work on synthetic data generated from Gibson~\cite{xia2018gibson} or Habitat~\cite{savva2019habitat} simulators and rely on time-consuming image sequences. For example, SMNet~\cite{SMNet} gradually captures an average of $2,500$ view-points for each floor to generate a semantic map for indoor scenes.  
Instead, we explore achieving efficient allocentric scene understanding via a single panorama image.
The \textit{Late-projection} pipeline~\cite{anderson2018vision,grinvald2019volumetric,maturana2018real,ran2021rs,sengupta2012automatic} performs egocentric semantic segmentation and project labels to top-down views, which are sensitive to depth map and agent pose information, inevitably facing the projection error and under-fitting of model training, thus remaining a suboptimal solution.
There are some BEV-related methods that leverage multiple perspective view sensors or LiDAR sensors and focus on outdoor object detection~\cite{bevformer,liu2022petr,yang2022bevformer}, optical flow estimation~\cite{lee2020pillarflow,luo2021self}, and semantic segmentation~\cite{pan2020cross,zhou2022cross}. Different from previous methods, our 360Mapper is carefully designed for learning indoor holistic representations by forwarding a single panorama without using multi-view images, image sequences, or point clouds.


\begin{figure*}[t]
    \centering
    \includegraphics[width=0.99\linewidth]{Figures/paradigms.pdf}
    \begin{minipage}[t]{.2\linewidth}
        \vskip-3ex
        \subcaption{Multi-view}\label{paradimgs_1}
    \end{minipage}%
    \begin{minipage}[t]{.2\linewidth}
        \vskip-3ex
        \subcaption{Sequence-based}\label{paradimgs_2}
    \end{minipage}%
    \begin{minipage}[t]{.2\linewidth}
        \vskip-3ex
        \subcaption{Early-projection}\label{paradimgs_3}
    \end{minipage}%
    \begin{minipage}[t]{.2\linewidth}
        \vskip-3ex
        \subcaption{Late-projection}\label{paradimgs_4}
    \end{minipage}%
    \begin{minipage}[t]{.2\linewidth}
        \vskip-3ex
        \subcaption{Intermediate-projection}\label{paradimgs_5}
    \end{minipage}%
    \vskip-1ex
    \caption{\textbf{Paradigms of semantic mapping.} While the narrow-FoV (a) multi-view and (b) sequence-based methods rely on $V{\geq}6$ and $N{\geq}20$ views, the 360{\textdegree}-BEV (c) Early-, (d) Late-, and (e) Intermediate-projection methods use a single panorama.}
    \label{fig:paradigms}
    \vskip-2ex
\end{figure*}

\section{Panorama Semantic Mapping (360BEV)}
\label{sec:methodology}
To investigate the 360BEV task, we analyze potential panoramic projection paradigms in Sec.~\ref{sec:3_1_paradigm}. The generation and data statistics of the dataset are detailed in Sec.~\ref{sec:3_3_dataset}. To tackle the challenging panoramic semantic mapping, in Sec.~\ref{sec:3_4_model} we present our solution \textbf{360Mapper} with the \textbf{Inverse Radial Projection} method and \textbf{360Attention} module, which enable distortion-aware feature processing.

\subsection{360 Projection Paradigms}~\label{sec:3_1_paradigm}
As shown in Fig.~\ref{fig:paradigms}, unlike multi-view methods relying on more than six views ($V$ in Fig.~\ref{paradimgs_1}) and sequence-based methods using more than $20$ narrow views ($N$ in Fig.~\ref{paradimgs_2}), panoramic semantic mapping uses a single image with depth.
We investigate three projection paradigms, \ie, \textit{how to process data from front-view panoramas to bird's-eye-view semantics}, which are:
\begin{compactitem}
    \item[(1)] \textit{Early projection: \textbf{Proj.}${\rightarrow}$Enc.${\rightarrow}$Seg.} in Fig.~\ref{paradimgs_3}. 
    \item[(2)] \textit{Late projection: Enc.${\rightarrow}$Seg.${\rightarrow}$\textbf{Proj.}} in Fig.~\ref{paradimgs_4}.
    \item[(3)] \textit{Intermediate projection: Enc.${\rightarrow}$\textbf{Proj.}${\rightarrow}$Seg.} in Fig.~\ref{paradimgs_5}. 
\end{compactitem}
Based on these properties, we mainly explore 360BEV with intermediate projections, in which we identify the following challenges: In the feature extraction stage, spatial distortions and object deformations severely hinder the encoder from extracting representative features from the front-view panoramic image. For the intermediate feature projection, only depth information is utilized for consistent view transformation of high-dimensional features. In addition, many large objects in the front view (\eg, \textit{walls}) are projected to thin objects in the top-down view, which greatly impedes capturing wide-range features during projection. 


\subsection{360FV and 360BEV Data Generation}~\label{sec:3_3_dataset}

\noindent\textbf{360FV-Matterport.} 
The original Matterport3D~\cite{Matterport3D} was collected via narrow-FoV cameras. As shown in Fig.~\ref{fig:data_gen}, we convert the $18$ narrow-view images and annotations into the 360{\textdegree} format by using rotation-translation matrices. 

\noindent \textbf{360BEV-Stanford.} 
The Stanford2D3D dataset~\cite{stanford2d3d} has front-view panoramic images and semantic labels. However, it lacks BEV semantic labels. As presented in Fig.~\ref{fig:data_gen_bev}, we utilize the spatial semantic information from the global XYZ image to generate the corresponding BEV semantic map. 
By applying orthographic projection, we generate the BEV semantic maps within a visible range as BEV ground truth, enabling end-to-end training from font-view images to top-down semantics. 

\begin{figure}[t]
    \centering
    \includegraphics[width=0.99\linewidth]{Figures/18_to_1.pdf}
    \vskip-1ex
    \caption{\textbf{360FV semantics generation} from $18$ narrow views to a panoramic view on the 360FV-Matterport dataset. $H$, $M$, and $L$ represent high, medium, and low positions, respectively. } 
    \label{fig:data_gen}
    \vskip-1ex
\end{figure}
\begin{figure}[t]
    \centering
    \includegraphics[width=0.99\linewidth]{Figures/Global_xyz_to_gt3.pdf}
    \vskip-1ex
    \caption{\textbf{360BEV semantics generation} by orthographic projection, from (a) the front-view semantic image and (b) the global XYZ image, to (c) the 360BEV semantic map. }
    \label{fig:data_gen_bev}
    \vskip-2ex
\end{figure}
\noindent \textbf{360BEV-Matterport.} 
Inspired by the global XYZ modality~\cite{stanford2d3d}, we generate a global XYZ for each panoramic image by using the provided depth ground truth. In order to generate BEV semantic ground truth corresponding to the panoramic view, several key steps must be considered. Firstly, a panoramic image can be processed as a sphere with rays shooting from the center of the sphere, where the camera is located. 
\begin{equation}\label{eq:1}
\begin{aligned}
\Theta_{i,j} &= \frac{i\pi}{H} + \frac{\pi}{2H}, \\  &i =\{0,\ldots,H{-}1\}, \ j=\{0,\ldots,W{-}1\}, \\
\Phi_{i,j} &= - \frac{2\pi j}{W} + \pi - \frac{\pi}{W},\\ &i=\{0,\ldots,H{-}1\}, \ j=\{0,\ldots,W{-}1\}.
\end{aligned}
\end{equation}
Here, $\Theta$ and $\Phi$ are angle matrices of panoramic images with size $H {\times} W$, which consist of two dimensional Euler angular equivariant series.
Given the representation in spherical coordinate systems, each 3D point $(X_{i,j}, Y_{i,j}, Z_{i,j})$ in the camera coordinate system will be obtained through the calculation in Eq.~\eqref{eq:2},
\begin{equation}\label{eq:2}
\begin{aligned}
X_{i,j} &= D_{i,j} \cdot \sin(\Theta_{i,j}) \cdot \sin(\Phi_{i,j}), \\
Y_{i,j} &= D_{i,j} \cdot \cos(\Theta_{i,j}), \\
Z_{i,j} &= D_{i,j} \cdot \sin(\Theta_{i,j}) \cdot \cos(\Phi_{i,j}), 
\end{aligned}
\end{equation}
where $D$ is the panoramic depth information.
After obtaining 3D points, the orthographic projection matrix $P_{v}$ is applied to transform 3D coordinates to 2D panoramic BEV indices $(u, v)$, which is presented in Eq.~\eqref{eq:3}, where $[\mathbf{R}|\mathbf{t}]$ is the transformation matrix.
\begin{equation}\label{eq:3}
\begin{aligned}
\left[\begin{array}{c}
x \\ y \\ z
\end{array}\right] = \mathbf{R}^{-1} \left[\begin{array}{l}
X_{i,j} \\  Y_{i,j} \\ Z_{i,j}
\end{array}\right]-\mathbf{t}, \\ 
% \quad
\underbrace{\left[\begin{array}{l}
u \\ v \\ 0 \\ 1
\end{array}\right] = P_{v}\left[\begin{array}{l}
x \\ y \\ z \\ 1
\end{array}\right]}_{\text {Orthographic projection}}.
\end{aligned}
\end{equation}


\input{Tables/datasets}

\begin{figure}[t]
    \centering
    \begin{minipage}[t]{.99\linewidth}
        \frame{\includegraphics[width=0.99\linewidth]{Figures/s2d3d_stat.pdf}}
        % \vskip-1ex
        \subcaption{Class distribution of 360BEV-Stanford dataset }\label{stat_s2d3d}
    \end{minipage}%
    \hfill
    \begin{minipage}[t]{.99\linewidth}
        \frame{\includegraphics[width=0.99\linewidth]{Figures/mp3d_stat.pdf}}
        % \vskip-1ex
        \subcaption{Class distribution of 360BEV-Matterport dataset}\label{stat_mp3d}
    \end{minipage}%
    \vskip-1ex
    \caption{\textbf{Per-class pixel number (logarithmic) and frequency ($\%$) distribution} of two 360BEV datasets.}
    \label{fig:dataset_stat}
    \vskip-3ex
\end{figure}

\noindent\textbf{Dataset statistics.}
As a result, two BEV datasets for panoramic semantic mapping are obtained. The detailed data statistics of 360BEV-Stanford and 360BEV-Matterport datasets are shown in Table~\ref{tab:datasets}. While the 360BEV-Stanford dataset has $13$ classes and $1,413$ images, the 360BEV-Matterport dataset includes $20$ classes and $10,615$ samples. 
In the Matterport3D dataset~\cite{Matterport3D}, there are $40$ object categories in the dense annotation. However, many of them are relatively rare in the original dataset, \eg, \textit{TV} and \textit{beam} (${\ll}0.1\%$), which are excluded. 
Thus, 360BEV-Matterport maintains the $20$ most common object categories and merges some uncommon classes.
Besides, we further present the per-class pixel number and per-class frequency in Fig.~\ref{fig:dataset_stat}. It is worth noting that the $floor$ class has a much higher frequency on both datasets. This category is important for tasks that rely on complete maps, such as indoor navigation and is therefore also retained.


\subsection{Proposed Model: 360Mapper}~\label{sec:3_4_model}

\noindent\textbf{Overall Architecture.} 
As shown in Fig.~\ref{fig:model}, our end-to-end {360Mapper} framework includes four steps:
(1) The transformer-based backbone extracts features from the panoramic image.
(2) The \textbf{Inverse Radial Projection (IRP)} module obtains a 2D index by projecting reference points generated by depth. (3) The \textbf{360Attention} module enhances the front-view feature by 2D index and generates offsets from BEV queries to eliminate the effects of distortion. (4) The lightweight decoder parses the projected feature map and predicts the semantic BEV map.

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.99\linewidth]{Figures/Structure-crop.pdf}
    \begin{minipage}[t]{.5\linewidth}
        \vskip-3ex
        \subcaption{360Mapper model}\label{fig:modela}
    \end{minipage}%
    \begin{minipage}[t]{.4\linewidth}
        \vskip-3ex
        \subcaption{360Attention module}\label{fig:modelb}
    \end{minipage}%
    \vskip-1ex
    \caption{\textbf{Architecture of 360Mapper and the 360Attention module.} The 360Mapper model includes the encoder for extracting features from the front-view panoramic image, the 360Attention module for feature projection, and the decoder for parsing the projected feature to the BEV semantic map. The offsets are obtained by a linear layer and added with the 2D index that is obtained by Inverse Radial Projection (IRP), yielding the sampling locations for 360BEV feature projection.}
    \label{fig:model}
    \vskip-2ex
\end{figure*}

\noindent \textbf{Inverse Radial Projection.} 
Next, we propose a flexible projection method, the Inverse Radial Projection (IRP), for which the input of panoramic depth is included. We can easily obtain a top-view mask map by projecting from depth information. This mask map is then used to generate 3D reference points with the corresponding map height. 3D reference points are projected onto the sphere to generate 2D reference indexes, as shown in Eq.~\eqref{eq:4}, where $ID_h$ and $ID_w$ represent the index values of the 2D reference for the height and width of the feature map, respectively. The 2D reference indexes are then used to locate the corresponding feature points on the encoded front-view feature map. 
\begin{equation}\label{eq:4}
\begin{aligned}
\Phi  &= \tan^{-1} \frac{y}{x},\\
\Theta &= \tan^{-1} \left(\frac{x}{z} \cdot \frac{1}{\cos(\Phi)} \right) ,\\
ID_h &= \left\lceil\frac{H \Theta}{\pi}\right\rceil ,\\
ID_w &= \left\lceil\left(\frac{\Phi}{\pi} - \frac{1}{W}\right) \cdot \frac{W}{2}\right\rceil.
\end{aligned}
\end{equation}
Due to the distortions in the stitching process of the panorama, it is hard to project the 3D reference points exactly onto the 2D front-view plane by rotation and translation. Thus, we use the depth map to generate a map mask that better describes the shape of the map, so that the accurate projection with the mask not only makes the amount of data entering the 360Attention much smaller, which is conducive to the fast convergence of the model but also facilitates the use of sampling offsets for 360Attention.

\noindent \textbf{360Attention.}
In Fig.~\ref{fig:modelb}, the proposed 360Attention generates sampling offsets through the linear layer in an adaptive manner.
Given the BEV query $\boldsymbol{q} \in \mathbb{R}^{N \times C_{Emb}}$ as input, where $N{=}{h{\times}w}$ is the length of query, a $mask{(\cdot)}$ operation is applied on $\boldsymbol{q}$ and $\boldsymbol{p}$ to mask out irrelevant points and 2D indexes according to the mask map $M_{map}$ from IRP, which is crucial to keep $\boldsymbol{q}$ and $\boldsymbol{p}$ efficient and reducing computation of 360Attention ($\sum M_{map} {<} N$). 
The sampling offset $\Delta \boldsymbol{p}_{q,ij}$ and attention weight $\mathcal{A}_{ij}{\in}[0, 1]$ are predicted through BEV query by linear layers respectively. The adaptive sampling offsets are then added to the extended 2D index $\boldsymbol{p}$ to obtain distortion-aware sampling locations.
The 360Attention module can be denoted as:
\begin{equation}\label{eq:5}
\begin{aligned}
&\operatorname{360Attn}(\boldsymbol{q}, \boldsymbol{p}, \boldsymbol{f}_{360}) =\\ &\sum_{i = 1}^{N_{\text {head}}}\mathcal{W}_{i} \sum_{j = 1}^{N_{\text{point}}}\mathcal{A}_{ij}{\cdot} \boldsymbol{f}_{360}\left(mask\left(\boldsymbol{p}\right)+\Delta \boldsymbol{p}_{q,ij}\right),
\end{aligned}
\end{equation}
where $\boldsymbol{q}$, $\boldsymbol{p}$, and $\boldsymbol{f}_{360}$ indicate the query, the extended 2D index, and panoramic feature map, respectively. The linear layer $\mathcal{W}_{i} {\in} \mathbb{R}^{C\times (C/N_{head})}$ is specific to each attention head $i$, where $C$ is the feature dimension and $N_{head}$ is the number of heads.
The attention weight $\mathcal{A}_{ij}$ represents the importance of the sampled points $j$, where $\sum \mathcal{A}_{ij} {=}1$. 
The panoramic features $\boldsymbol{f}_{360}$ and the adaptive sampling locations $(mask(\boldsymbol{p}){+}\Delta \boldsymbol{p}_{q,ij})$ are aggregated using attention weights $\mathcal{A}_{ij}$ to produce a BEV output. Afterwards, the mask map $M_{map}$ is applied to assemble the BEV output as $\boldsymbol{q}' {\in} \mathbb{R}^{N \times C_{Emb}}$. After being added with a residual term of $\boldsymbol{q}$, the BEV result from $\boldsymbol{q}{+}\boldsymbol{q}'$ is forwarded to the next 360Attention module.

Compared to the Spatial Cross-Attention module in BEVFormer~\cite{bevformer}, the difference lies in (1) Instead of relying on multi-view features across multiple cameras, our 360Attention module is designed to directly adopt adaptive sampling offsets to extract features from a single panoramic feature map. (2) Our module gets rid of the projection of 3D reference points to different image views using the projection matrix, thus compensating for the lack of front-view perception. (3) The mask operation is applied to maintain the BEV query efficient and adaptive to front-view panoramic features by using depth information as a bridge. Through these non-trivial designs, the BEV feature map generated by 360Attention is able to effectively neutralize the effects of front-view distortion.  

\section{Experiments}
\label{sec:experiments}

\subsection{Implementation Details}
We train 360Mapper models with 4 A100 GPUs with an initial learning rate of $6\textrm{e}^{-5}$, scheduled by the step strategy over $50$ epochs. AdamW is the optimizer with epsilon $1\textrm{e}^{-8}$, weight decay is $0.01$ and batch size is $4$ on each GPU. The panoramic image size of 360FV-Matterport and Stanford2D3D~\cite{stanford2d3d} are both $512{\times}1024$. The resolution of panoramic images on both 360BEV-Stanford and 360BEV-Matterport datasets are $512{\times}1024$ as input for 360Mapper training, while the output BEV maps are set to $500{\times}500$, which correspond to a perception range of $10m{\times}10m$. Following~\cite{SMNet,chen2022trans4map}, evaluation metrics are pixel-wise accuracy~(Acc), pixel recall~(mRecall), precision~(mPrecision), and mean Intersection-over-Union~(mIoU).

\subsection{Panorama Semantic Segmentation (360FV)}

\input{Tables/stanford2d3d_front_view}
\noindent\textbf{Results on Stanford2D3D.}
To verify the capacity to handle object deformations and image distortions, we first evaluate our method on front-view panoramic semantic segmentation. The results on the Stanford2D3D dataset are presented in Table~\ref{tab:s2d3d_front}.
All results are averaged over $3$ cross-validation folds. Thanks to the proposed 360Attention module, our 360Mapper model is better capable of handling deformations in panoramas, yielding $54.3\%$ in mIoU, with ${>}2\%$ performance gains as compared to the previous state-of-the-art Trans4PASS~\cite{trans4pass} and 	
CBFC~\cite{zheng2023complementary}. 
The promising result in front-view panoramas has initially revealed the potential of our model in extracting 360{\textdegree} front-view features, which is crucial for the BEV semantic mapping task as well.

\input{Tables/matterport3d_front_view_val.tex}
\noindent\textbf{Results on 360FV-Matterport.}
For the first time, a large-scale 360FV-Matterport is brought to the community of front-view panoramic semantic segmentation.
In Table~\ref{tab:mp3d_front_val}, four state-of-the-art methods are selected and reproduced.
Compared to the Trans4PASS~\cite{trans4pass} and Trans4PASS+~\cite{trans4passplus} models, our model has respective ${+}4.44\%$ and ${+}3.75\%$ improvements. Furthermore, our model surpasses RGB-D HoHoNet~\cite{hohonet} and SegFormer~\cite{xie2021segformer} with ${+}1.50\%$ and ${+}0.82\%$ mIoU gains. The results indicate that our model can consistently achieve state-of-the-art performance on large-scale datasets for panoramic semantic segmentation. 

\subsection{Panorama Semantic Mapping (360BEV)}
To thoroughly investigate the 360BEV task, we consistently analyze the early-, late-, and intermediate projections, as well as compare their state-of-the-art methods in both 360BEV benchmarks.


\noindent\textbf{Results on 360BEV-Stanford.} 
In Table~\ref{tab:s2d3d_topdown}, to study the Early projection mode, SegFormer~\cite{xie2021segformer} and SegNeXt~\cite{guo2022segnext} with different backbones, are selected, which merely reach unsatisfactory results. The results indicate that the pre-projected RGB maintains less rich spatial and visual information of front-view images. Using Late projection, SegFormer with the same MiT-B2 backbone achieves $18.65\%$ mIoU and surpasses the one using Early projection, still yielding sub-optimal semantic mapping results. Interestingly, all methods using Intermediate projection obtain more than $30\%$ mIoU. While using the same MiT-B2 backbone, our proposed 360Mapper achieves $45.78\%$ with ${+}9.70\%$ gains compared to the baseline Trans4Map~\cite{chen2022trans4map}. Further, our efficient model~(MiT-B0) outperforms Trans4Map~(MiT-B4) with ${+}05.73\%$ mIoU gains. With a stronger CNN backbone MSCA-B from SegNeXt~\cite{guo2022segnext}, our method reaches the best score with $46.44\%$ in mIoU, which indicates 360Mapper is flexible to both CNN- and Transformer-based backbones.


\input{Tables/stanford2d3d}
\input{Tables/matterport3d_val} 
% \input{Tables/matterport3d_test}
\noindent\textbf{Results on 360BEV-Matterport.}
In Table~\ref{tab:mp3d_topdown_val}, we further present the results on the 360BEV-Matterport dataset. SegFormer~\cite{xie2021segformer} and SegNeXt~\cite{guo2022segnext} adopt Early projection and show better performance than the Late projection ones.
The reason for this is Late projection methods are constrained by their lower performance in front-view semantic segmentation, which affects the projected BEV semantic maps.
In contrast, using Intermediate projection, our 360Mapper models based on two different model scales, \ie, MiT-B0 and MiT-B2, show overall promising performance with $36.98\%$ and $44.32\%$ in mIoU, respectively. Compared to the previous state-of-the-art Trans4Map~\cite{chen2022trans4map} (MiT-B2), our approach with MiT-B2 has improvements by ${+}5.52\%$ in accuracy, ${+}7.94\%$ in mRecall, ${+}6.95\%$ in mPrecision, and ${+}7.60\%$ in mIoU. Surprisingly, our 360Mapper with MiT-B2 outperforms Trans4Map with MiT-B4 with ${+}6.28\%$ in mIoU. Besides, to compare multi-view methods, we reproduce BEVFormer~\cite{bevformer} by using a single panorama instead of six views of pinhole cameras. Our 360Mapper outperforms BEVFormer (MiT-B2) with ${+}11.81\%$ mIoU. Furthermore, we verify the flexibility of 360Mapper by using a CNN-based MSCA-B backbone~\cite{guo2022segnext}, which obtains the highest mIoU score with $46.31\%$. All results are in line with our observation that Intermediate projection can preserve dense visual cues and long-range information from front-view panoramas, and deliver more valuable context for BEV semantic mapping, leading to this superiority of 360Mapper, as compared to the other paradigms.


\noindent\textbf{Per-class Results.}
To study the per-class performance on both 360BEV datasets, we present the comparison results in Fig.~\ref{fig:360bev_s2d3d_per_class}. For comparison, both the baseline Trans4Map and our 360Mapper model are based on the same backbone, \ie, MiT-B2. On the 360BEV-Stanford dataset (Fig.~\ref{subfig-1:class_iou_s2d3d}), our 360Mapper model has significant gains on most of categories, such as \textit{board}~(${>}14\%$), \textit{wall}~(${>16\%}$), \textit{door}~(${>}28\%$), etc. On the 360BEV-Matterport dataset (Fig.~\ref{subfig-2:class_iou_mp3d}), it is readily apparent that our model can better recognize the \textit{chairs} and \textit{tables}, yielding ${>}6\%$ IoU gains compared to Trans4Map~\cite{chen2022trans4map}.
On the test set of the 360BEV-Matterport dataset, our 360Mapper obtains IoU gains with ${>}12\%$ and ${>}15\%$ on the \textit{sink} and \textit{toilet} classes, as compared to Trans4Map. Overall, the consistent improvements on both datasets show the superiority of our 360Mapper on panoramic semantic mapping. 
\begin{figure*}[t]
    \centering
    \subfloat[360BEV-Stanford\label{subfig-1:class_iou_s2d3d}]{%
    \includegraphics[trim={2 2 2 5},clip,width=0.40\textwidth]{Figures/360BEV_s2d3d/s2d3d.pdf}
    }
    % \hfill
    \subfloat[360BEV-Matterport\label{subfig-2:class_iou_mp3d}]{%
      \includegraphics[trim={2 2 2 5},clip, width=0.55\textwidth]{Figures/360BEV_mp3d/mp3d.pdf}
    }
    \vskip-1ex
    \caption{\textbf{Distribution of per-class semantic mapping results} (per-class IoU in $\%$) on the 360BEV-Stanford and the 360BEV-Matterport datasets. Compared to the baseline model Trans4Map~\cite{chen2022trans4map}, our 360Mapper models achieve overall better 360BEV results.}
    \label{fig:360bev_s2d3d_per_class}
    \vskip-1ex
\end{figure*}

\input{Tables/analysis}
\noindent\textbf{Analysis of 360Attention.}
To better understand 360Attention, we further conduct an analysis of the offset mechanisms in 360Attention and the backbone selection, in Table~\ref{tab:analysis}.
First, in \circled{1}\circled{2}\circled{3}, we select three model scales, \ie, MiT-B0, MiT-B2, and MiT-B4, to verify the effect of model capacity in 360Attention. The three models obtain good performance, showing that 360Attention has positive effects in different model scales. 
Besides, different offset schemes are compared among \circled{2}\circled{4}\circled{5}\circled{6}, which are deformable, multi-scale, fixed-range, and separate offset.
All of them have the same MiT-B2 backbone.
Here, \circled{2} shows the superiority of deformable offset which has a better performance ($44.32\%$). However, these comparable results prove that our 360Attention design is robust to offset mechanisms.
Further, to analyze the effect of backbone selection, we choose transformer-based MiT-B2~\cite{xie2021segformer} and CNN-based MSCA-B~\cite{guo2022segnext} as in \circled{2}\circled{7}. A stronger backbone~\cite{guo2022segnext} shows a further improvement of mIoU (${+}1.99\%$), which shows the flexibility of our approach regarding the backbone variants.


\begin{figure}[t]
    \footnotesize
    \setlength\tabcolsep{1pt}
    {
    \newcolumntype{P}[1]{>{\centering\arraybackslash}p{#1}}
    \begin{tabular}{@{}*{10}{P{0.089\columnwidth}}@{}}
    {\cellcolor[rgb]{0.68,0.78,0.91}}\textcolor{white}{wall} 
    &{\cellcolor[rgb]{0.44,0.50,0.56}}\textcolor{white}{floor}
    &{\cellcolor[rgb]{0.60,0.87,0.54}}\textcolor{black}{chair}
    &{\cellcolor[rgb]{0.77,0.69,0.84}}\textcolor{white}{door}
    &{\cellcolor[rgb]{1.00,0.50,0.05}}\textcolor{white}{table} 
    &{\cellcolor[rgb]{0.84,0.15,0.16}}\textcolor{white}{pictu.} 
    &{\cellcolor[rgb]{0.12,0.47,0.71}}\textcolor{white}{furni.}
    &{\cellcolor[rgb]{0.74,0.74,0.13}}\textcolor{black}{objec.}
    &{\cellcolor[rgb]{1.00,0.60,0.59}}\textcolor{black}{windo.}
    &{\cellcolor[rgb]{0.17,0.63,0.17}}\textcolor{white}{sofa} \\
    {\cellcolor[rgb]{0.89,0.47,0.76}}\textcolor{black}{bed}
    & {\cellcolor[rgb]{0.87,0.62,0.84}}\textcolor{black}{sink}
    &{\cellcolor[rgb]{0.58,0.40,0.74}}\textcolor{white}{stairs} 
    &{\cellcolor[rgb]{0.55,0.64,0.32}}\textcolor{white}{ceil.} 
    &{\cellcolor[rgb]{0.52,0.24,0.22}}\textcolor{white}{toilet} 
    &{\cellcolor[rgb]{0.62,0.85,0.90}}\textcolor{black}{mirror} 
    &{\cellcolor[rgb]{0.61,0.62,0.87}}\textcolor{black}{show.}
    &{\cellcolor[rgb]{0.91,0.59,0.61}}\textcolor{black}{batht.}
    &{\cellcolor[rgb]{0.39,0.47,0.22}}\textcolor{white}{count.} 
    &{\cellcolor[rgb]{0.55,0.34,0.29}}\textcolor{white}{shelv.} \\
    \end{tabular}
    }
    \centering
    \begin{tabular}{c c c c}
        \vspace{1pt}
        \raisebox{-0.5\height}{\includegraphics[width=0.18\textwidth]{Figures/Qualitative_analysis/pano1.png}} &
        \raisebox{-0.5\height}{\includegraphics[width=0.09\textwidth]{Figures/Qualitative_analysis/1_baseline.png}} &
        \raisebox{-0.5\height}{\includegraphics[width=0.09\textwidth]{Figures/Qualitative_analysis/1_360Mapper.png}} &
        \raisebox{-0.5\height}{\includegraphics[width=0.09\textwidth]{Figures/Qualitative_analysis/1_groundtruth.png}}\\
         % \vspace{0.5cm}

        \vspace{1pt}

        \raisebox{-0.5\height}{\includegraphics[width=0.18\textwidth]{Figures/Qualitative_analysis/pano2.png}} &
        \raisebox{-0.5\height}{\includegraphics[width=0.09\textwidth]{Figures/Qualitative_analysis/2_baseline.png}} &
        \raisebox{-0.5\height}{\includegraphics[width=0.09\textwidth]{Figures/Qualitative_analysis/2_360Mapper.png}} &
        \raisebox{-0.5\height}{\includegraphics[width=0.09\textwidth]{Figures/Qualitative_analysis/2_groundtruth.png}}\\
        \vspace{1pt}

        \raisebox{-0.5\height}{\includegraphics[width=0.18\textwidth]{Figures/Qualitative_analysis/last_row_pano.png}} &
        \raisebox{-0.5\height}{\includegraphics[width=0.09\textwidth]{Figures/Qualitative_analysis/last_row_baseline.png}} &
        \raisebox{-0.5\height}{\includegraphics[width=0.09\textwidth]{Figures/Qualitative_analysis/last_row_360Mapper.png}} &
        \raisebox{-0.5\height}{\includegraphics[width=0.09\textwidth]{Figures/Qualitative_analysis/last_row_gt.png}}\\
        \noalign{\vskip 2mm}   
        Input&Baseline&360Mapper&Ground Truth
    \end{tabular}
    \caption{\textbf{Qualitative analysis} on the 360BEV-Matterport dataset. Black regions are \texttt{void}. Zoom in for a better view.}
    \label{fig:vis}
    \vskip -3ex
\end{figure}

\subsection{Qualitative Analysis}
To analyze the predicted semantic maps, we visualize the results from the validation set of the 360BEV-Matterport dataset. In Fig.~\ref{fig:vis}, from left to right are input images, results of baseline~\cite{chen2022trans4map}, results of our 360Mapper, and ground truth. Thanks to the IRP projection and 360Attention, the segmentation results of 360Mapper are much better.
In the first scene in Fig.~\ref{fig:vis}, 360Mapper is able to successfully classify \textit{chairs}, while the baseline model fails, predicting several \textit{tables} and misclassifying the distant ground as another \textit{table}. In the second scene, the segmentation of the \textit{tables} derived by the baseline is incomplete. Furthermore, in the last zoomed-in scene, 360Mapper provides accurate semantic maps, such as in \textit{counter}, \textit{chair}, and \textit{wall} categories, whereas the baseline Trans4Map~\cite{chen2022trans4map} misclassifies them as \textit{tables} and \textit{doors}. Based on the qualitative analysis, our 360Mapper can effectively handle object deformations and image distortions, yielding better BEV semantic maps.

\section{Conclusion}
\label{sec:conclusion}
In this paper, we introduce 360BEV, a novel task to conduct panoramic semantic mapping in indoor environments, \ie, from a single panoramic image to a holistic BEV semantic map.
To enable this, we present 360BEV-Matterport and 360BEV-Stanford, extending off-the-shelf datasets for the presented 360BEV task. 
We revisit existing transformation paradigms and propose 360Mapper, a novel end-to-end architecture specifically designed for panoramic semantic mapping.
As a consequence, 360Mapper outperforms state-of-the-art counterparts by clear margins.

\noindent \footnotesize{\\ \textbf{Acknowledgement.}
This work was supported
in part by the ``KIT Future Fields'' project, 
in part by the Ministry of Science, Research and the Arts of Baden-Wurttemberg (MWK) through the Cooperative Graduate School Accessibility through AI-based Assistive Technology (KATE) under Grant BW6-03, in part by the Federal Ministry of Education and Research (BMBF) through a fellowship within the IFI program of the German Academic Exchange Service (DAAD), in part by Hangzhou SurImage Technology Company Ltd, and in part by Agile Robots AG. We thank HoreKA@KIT, HAICORE@KIT, and bwHPC supercomputer partitions.
}


\clearpage
{\small
\bibliographystyle{ieee_fullname}
\bibliography{main}
}

\input{TexContents/appendix}

\end{document}