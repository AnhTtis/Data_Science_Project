{
    "arxiv_id": "2303.11910",
    "paper_title": "360BEV: Panoramic Semantic Mapping for Indoor Bird's-Eye View",
    "authors": [
        "Zhifeng Teng",
        "Jiaming Zhang",
        "Kailun Yang",
        "Kunyu Peng",
        "Hao Shi",
        "Simon Reiß",
        "Ke Cao",
        "Rainer Stiefelhagen"
    ],
    "submission_date": "2023-03-21",
    "revised_dates": [
        "2023-08-28"
    ],
    "latest_version": 3,
    "categories": [
        "cs.CV"
    ],
    "abstract": "Seeing only a tiny part of the whole is not knowing the full circumstance. Bird's-eye-view (BEV) perception, a process of obtaining allocentric maps from egocentric views, is restricted when using a narrow Field of View (FoV) alone. In this work, mapping from 360° panoramas to BEV semantics, the 360BEV task, is established for the first time to achieve holistic representations of indoor scenes in a top-down view. Instead of relying on narrow-FoV image sequences, a panoramic image with depth information is sufficient to generate a holistic BEV semantic map. To benchmark 360BEV, we present two indoor datasets, 360BEV-Matterport and 360BEV-Stanford, both of which include egocentric panoramic images and semantic segmentation labels, as well as allocentric semantic maps. Besides delving deep into different mapping paradigms, we propose a dedicated solution for panoramic semantic mapping, namely 360Mapper. Through extensive experiments, our methods achieve 44.32% and 45.78% in mIoU on both datasets respectively, surpassing previous counterparts with gains of +7.60% and +9.70% in mIoU. Code and datasets are available at the project page: https://jamycheung.github.io/360BEV.html.",
    "pdf_urls": [
        "http://arxiv.org/pdf/2303.11910v1",
        "http://arxiv.org/pdf/2303.11910v2",
        "http://arxiv.org/pdf/2303.11910v3"
    ],
    "publication_venue": "Code and datasets are available at the project page: https://jamycheung.github.io/360BEV.html. Accepted to WACV 2024"
}