{
    "arxiv_id": "2303.14568",
    "paper_title": "Measuring Classification Decision Certainty and Doubt",
    "authors": [
        "Alexander M. Berenbeim",
        "Iain J. Cruickshank",
        "Susmit Jha",
        "Robert H. Thomson",
        "Nathaniel D. Bastian"
    ],
    "submission_date": "2023-03-25",
    "revised_dates": [
        "2023-03-28"
    ],
    "latest_version": 1,
    "categories": [
        "stat.ML",
        "cs.AI",
        "cs.LG",
        "math.DG",
        "math.PR"
    ],
    "abstract": "Quantitative characterizations and estimations of uncertainty are of fundamental importance in optimization and decision-making processes. Herein, we propose intuitive scores, which we call \\textit{certainty} and \\textit{doubt}, that can be used in both a Bayesian and frequentist framework to assess and compare the quality and uncertainty of predictions in (multi-)classification decision machine learning problems.",
    "pdf_urls": [
        "http://arxiv.org/pdf/2303.14568v1"
    ],
    "publication_venue": "4 pages"
}