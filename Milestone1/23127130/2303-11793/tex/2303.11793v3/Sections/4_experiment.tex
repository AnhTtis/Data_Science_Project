
\section{Experimental Results}
\label{sec:experiment}
%To evaluate our approach, we conducted various adversarial attacks on different datasets and two backbone networks. 
%  We first compare our proposed~\SystemName~with well-known AT baseline frameworks. Next, we show the compatibility of our \SystemName\ with different networks and high-quality datasets. Finally, we demonstrate how our \SystemName\ helps a defense model to boost its adversarial robustness via extensive ablation studies.




\subsection{Experiment Settings} 
\label{sec:exp_settings}
 We employ WideResNet34-10 \cite{zagoruyko2016wide} as our backbone architecture on two datasets~\CIFAR-10 and~\CIFAR-100 \cite{krizhevsky2009learning}. The model are trained for 100 epochs with the momentum SGD \cite{qian1999momentum} optimizer, whereas its initial learning rate is set to 0.1 and decayed by $10$ at $75^{th}$ and $90^{th}$ epoch, respectively. Adversarial samples in the training phase are generated by $L_\infty$-PGD \cite{madry2017towards} in 10 iterations with the maximal perturbation $\epsilon=8 /255$ and the perturbation step size $\eta=2/255$. For a fair comparison with different approaches \cite{pang2020bag}, we use the above settings throughout our experiments without early stopping or modifying models' architecture, and report their performances on the last epoch. For our~\SystemName~based defense models, we use the following hyper-parameter settings: $\{K=32, \lambda_J =0.002, \lambda_{SW}=64 \}$ and $\{K=128, \lambda_J =0.001, \lambda_{SW}=64 \}$ for~\CIFAR-10 and~\CIFAR-100, respectively. An ablation study of the hyper-parameters' impact on model performance is provided in Sec. \ref{abl:hyper_study} in the Appendix. In addition, we perform the basic sanity tests \cite{carlini2019evaluating} in  Sec.\ref{abl:sanity_test} to ensure that our proposed~\SystemName~does not rely on gradient obfuscation. Overall, we compare our method with four different recent SOTA AT frameworks: TRADES~\cite{zhang2019theoretically},  ALP~\cite{kannan2018adversarial}, PGD-AT~\cite{madry2017towards}, and SAT~\cite{bouniot2021optimal}. The experiments are conducted on one GeForce RTX 3090 24GB GPU with Intel Xeon Gold 6230R CPU @ 2.10GHz.

 
% \begin{table}[!t]
% \caption{Online fool rate (\%) \cite{mladenovic2021online} of various defense models with \CIFAR-10 and our downloaded ~\CIFAR-10-\WEB~ datasets.} 
%      \centering
%      \resizebox{0.48\textwidth}{!}{%
%      \begin{tabular}{l | c  c  c | c  c c } 
%     \Xhline{3\arrayrulewidth}
%     \multirow{2}{*}{\centering Loss}   &\multicolumn{3}{c|}{ \CIFAR-10}   &\multicolumn{3}{c}{ \CIFAR-10-\WEB}  \\
%     & {$k=100$} & {$k=200$} & {$k=500$}  & {$k=100$} & {$k=200$} & {$k=500$} \\
%     \Xhline{2\arrayrulewidth}
%     XE & $88.9_{\pm3.4}$ & $89.4_{\pm2.3}$ & $88.8_{\pm1.4}$ & $93.3_{\pm1.4}$ & $92.5_{\pm1.4}$ &$85.0_{\pm1.4}$ \Tstrut\\
%     TRADES &$74.4_{\pm4.5}$ & \textbf{73.2}$_{\pm4.3}$ & $74.0_{\pm2.0}$ &$ 78.1_{3.0}$ & $75.6 _{\pm1.4 }$ &$ 46.9_{1.4}$\Tstrut\\
%     ALP &$73.7_{\pm4.8}$ & $73.3_{\pm3.7}$ & $74.0_{\pm1.2}$ &$78.6 _{2.9}$ & $77.1 _{2.0}$ &$49.1 _{1.4}$\Tstrut\\
%     PGD-AT & $76.7_{\pm4.1}$ & $76.9_{\pm2.1}$ & $75.7_{\pm1.6}$ & $79.2 _{2.6}$ & $79.8 _{2.1}$ &$51.4 _{1.6}$\Tstrut\\
%     SAT &$75.2_{\pm3.3}$ & $74.1_{\pm2.4}$ & $73.4_{1.3}$ & $77.7 _{\pm5.4 }$ & $72.8 _{3.2}$ &$48.6 _{2.1}$\Tstrut\\
%     \rowcolor{backcolour}\SystemName &\textbf{72.9}$_{\pm2.5}$ & ${73.3}_{\pm2.3}$ & $\mathbf{73.3}_{\pm1.0}$ & \textbf{76.7}$_{3.7}$ & \textbf{72.6}$_{2.3}$ &\textbf{45.3}$_{1.6}$ \Bstrut\\
%     \Xhline{3\arrayrulewidth}
%     \end{tabular}%
%     }
%     \label{tb:online_attack}
% \end{table}


\begin{table}[!t]
\caption{Online fool rate (\%) \cite{mladenovic2021online} of various defense models with   our downloaded ~\CIFAR-10-\WEB~ and  ~\CIFAR-100-\WEB~ datasets.} 
     \centering
     \resizebox{0.46\textwidth}{!}{%
     \begin{tabular}{l | c  c  c | c  c c } 
    \Xhline{3\arrayrulewidth}
    \multirow{2}{*}{\centering Defense}   &\multicolumn{3}{c|}{ \CIFAR-10-\WEB}   &\multicolumn{3}{c}{ \CIFAR-100-\WEB}  \\
    & {$k=100$} & {$k=200$} & {$k=500$}  & {$k=100$} & {$k=200$} & {$k=500$} \\
    \Xhline{2\arrayrulewidth}
     
    TRADES   &  78.1$_{3.0}$&  75.6$_{2.1}$&  46.9$_{1.4}$ &  84.7$_{3.4}$&  85.5$_{2.2}$&  84.9$_{1.5}$
\Tstrut\\
    ALP &  78.6$_{2.9}$&  77.1$_{2.0}$&  49.1$_{1.4}$ &  85.1$_{4.6}$&  85.6$_{2.3}$&  85.3$_{1.7}$\Tstrut\\
    PGD-AT &  79.2$_{2.6}$&  79.8$_{2.1}$&  51.4$_{1.6}$ &  84.9$_{3.6}$&  86.2$_{2.9}$&  86.3$_{0.9}$\Tstrut\\
    SAT  &  77.7$_{5.4}$&  72.8$_{3.2}$&  48.6$_{2.1}$ &  86.9$_{2.6}$&  87.2 $_{1.8}$&  87.2$_{0.6}$ \Tstrut\\
    \hline
    \textit{Random JR} &  82.3$_{3.9}$&  82.5$_{2.8}$&  63.7$_{1.3}$ &  90.5$_{2.9}$&  90.1$_{1.7}$&  90.0$_{1.4}$\Tstrut\\
    \textit{SW} &  77.6$_{4.2}$&  72.0$_{2.3}$&  46.0$_{1.6}$ &  \textbf{83.9}$_{4.0}$&  85.5$_{2.6}$&  85.5$_{1.2}$\Tstrut\\
    \rowcolor{backcolour}\SystemName   &  \textbf{74.0}$_{4.3}$&  \textbf{71.3}$_{3.1}$&  \textbf{46.0}$_{1.9}$ &  84.9$_{3.8}$&  \textbf{84.6}$_{2.5}$&  \textbf{84.1}$_{1.4}$\Bstrut\\
    \Xhline{3\arrayrulewidth}
    \end{tabular}%
    }
    \label{tb:online_attack}
\end{table}
\begin{table}[!t]
\caption{Classification accuracy (\%) from defense losses integrated AWP, LBGAT, and UDR, respectively, on WRN34 with \CIFAR-100. $\ast$ indicates baseline results obtained from downloaded models. $\dagger$ indicates re-running original paper settings.} 
     \centering
     \resizebox{0.46\textwidth}{!}{%
     \begin{tabular}{l | c  c | c  c | c c } 
    \Xhline{3\arrayrulewidth}
    \multirow{2}{*}{\centering Defense}   &\multicolumn{2}{c|}{AWP$^\ast$}   &\multicolumn{2}{c|}{LBGAT$^\ast$} &\multicolumn{2}{c}{UDR$^{\dagger}$} \\
    & \textit{Clean} & AutoAtt & \textit{Clean} & AutoAtt & \textit{Clean} & AutoAtt \\
    \Xhline{2\arrayrulewidth}
    TRADES &\textit{60.17} & 28.80 & \textit{60.43}   &29.34 & 68.04 & 47.87 \Tstrut\\ % &58.17 &27.01
    \rowcolor{backcolour}\SystemName &\textit{\textbf{60.55}} & \textbf{29.79} & \textit{\textbf{62.15}}   &\textbf{29.64} &\textbf{68.31} &\textbf{49.34} \Bstrut\\ % &\textbf{58.91} &\textbf{27.12}
    \Xhline{3\arrayrulewidth}
    \end{tabular}%
    }
    \label{tb:proxy_framwork}
\end{table}
% \begin{table}[!t]
% \caption{Classification accuracy (\%) of PreActResNet18 and ResNet50 with~\ImageNet100 and~\Intel\ dataset, respectively,  under \textit{white-} and \textit{black-box} attacks.} 
% \centering
% \resizebox{0.42\textwidth}{!}{%
% \setlength{\tabcolsep}{0.5em}
% {\renewcommand{\arraystretch}{1.1}
%     \begin{tabular}{ l | c | c c | c | c } 
%     \Xhline{3\arrayrulewidth}
%     {Defense}  &{\textit{Clean}}   &MIM   &CW &Square  & AutoAtt \\
%     \Xhline{2\arrayrulewidth}
%     \multicolumn{6}{c}{~\ImageNet100 - PreActResNet18}\\
%     \hline
%     TRADES    &\textit{62.64}    &30.66   &56.95    &46.52    &24.86 \\
%     SAT$^{400}$ &\textit{45.08}    &29.08   &41.56    &36.14    &23.96 \\
%     \cellcolor{backcolour} {\SystemName}   &\cellcolor{backcolour} \textit{61.97}     &\cellcolor{backcolour} \textbf{33.20}    &\cellcolor{backcolour} \textbf{57.42}    &\cellcolor{backcolour} \textbf{49.00}    &\cellcolor{backcolour} \textbf{27.54}\\
%     \Xhline{2\arrayrulewidth}
%     \multicolumn{6}{c}{\Intel - ResNet50}\\
%     \hline
%     TRADES    &\textit{54.43}    &3.73   &{2.47}    &5.57     &0.07\\
%     SAT &\textit{50.17}    &5.93   &4.73  &6.77     &1.60\\
%     \rowcolor{backcolour} {\SystemName} &\textit{57.80}    &\textbf{8.27}  &\textbf{6.77} &\textbf{9.37}    &\textbf{3.74}\\
%     \Xhline{3\arrayrulewidth}
%     \end{tabular}%
%     }
% }
% \label{tb:appli_attack}
% \end{table}
\begin{table}[!t]
\caption{Classification accuracy (\%) of PreActResNet18 with Tiny-\ImageNet\ dataset,  under \textit{white-} and \textit{black-box} attacks.} 
\centering
\resizebox{0.46\textwidth}{!}{%
\setlength{\tabcolsep}{0.5em}
{\renewcommand{\arraystretch}{1.1}
    \begin{tabular}{ l | c  c c  c  c c |c} 
    \Xhline{3\arrayrulewidth}
    {Defense}  &{\textit{Clean}}   &MIM   &CW & FAB &Square  & AutoAtt & Avg.\\
    \Xhline{2\arrayrulewidth}
    
    TRADES   & \textit{35.91$_{\text{.10}}$}	& 11.82$_\text{.08}$	& 8.92$_\text{.13}$	&10.88$_\text{.24}$  & 15.86$_\text{.04}$	& 8.28$_\text{.09}$ & 11.15$_\text{3.00}$\\
    ALP	&\textit{39.69$_\text{.37}$}	& 8.13$_\text{.06}$	& 7.66$_\text{.08}$	&9.92$_\text{.16}$& 15.98$_\text{.34}$	& 6.48$_\text{.10}$	& 9.63$_\text{3.75}$\\
    PGD	&\textit{\underline{33.81}$_\text{.22}$}	& 11.49$_\text{.33}$	& \textbf{10.14}$_\text{.24}$	& 11.29$_\text{.32}$& 16.20$_\text{.43}$	& \textbf{8.98}$_{\text{.16}}$ & 11.60$_\text{2.77}$ \\
    SAT$^{400}$ &\multicolumn{6}{c|}{\textit{Not converge}} & -\\
    \hline
     Random JR &\textit{47.65$_\text{.19}$} &0.29$_\text{.02}$   &0.44$_\text{.04}$   &3.79$_\text{.13}$   & 5.87$_\text{.03}$   & 0.19$_\text{.01}$ & 2.12$_\text{2.59}$\\
    SW &\textit{37.05$_\text{.09}$} &12.43$_\text{.21}$  & 9.35$_\text{.19}$  & 11.19$_\text{.14}$  & 16.77$_\text{.31}$   & 8.72$_\text{.14}$ & 11.19$_\text{3.20}$\\
    \cellcolor{backcolour} {\SystemName}   &\cellcolor{backcolour} \textit{37.97$_\text{.11}$}     &\cellcolor{backcolour} \textbf{12.57}$_\text{.08}$    &\cellcolor{backcolour} {9.55}$_\text{.04}$  &\cellcolor{backcolour} \textbf{11.30}$_\text{.09}$ &\cellcolor{backcolour} \textbf{17.33}$_\text{.12}$    &\cellcolor{backcolour} {8.91}$_\text{.08}$ &\cellcolor{backcolour} \textbf{11.97}$_\text{3.34}$\\
    \Xhline{3\arrayrulewidth}
    \end{tabular}%
    }
}
\label{tb:appli_attack}
\end{table}
\subsection{Popular Adversarial Attacks}
\label{sec:adv_attacks}
 Follow \citeauthor{zhang2019theoretically}, we assess defense methods against a wide range \textit{white-box} attacks (20 iterations)\footnote{\url{https://github.com/Harry24k/adversarial-attacks-pytorch.git}}:
FGSM \cite{goodfellow2014explaining}, PGD \cite{madry2017towards}, MIM \cite{dong2018boosting}, CW$_{\infty}$ \cite{carlini2017towards}, DeepFool \cite{moosavi2016deepfool},  and FAB \cite{croce2020minimally}; and \textit{black-box} attacks (1000 iterations): Square \cite{andriushchenko2020square} and Simba \cite{guo2019simple}. We use the same parameters as \citeauthor{rice2020overfitting} for our experiments: for the $L_{\infty}$ threat model, the values of epsilon and step size are $8/255$ and  $2/255$ for CIFAR-10 and CIFAR-100, respectively. For the $L_2$ threat model, the values of epsilon and step size are 128/255 and 15/255 for all datasets. Additionally, we include AutoAttack \cite{croce2020reliable} which is a reliable adversarial evaluation framework and an ensemble of four distinct  attacks: APGD-CE, APGD-DLR \cite{croce2020reliable}, FAB \cite{croce2020minimally}, and Square \cite{andriushchenko2020square}.
The results of this experiment are presented in~Table~\ref{tb:white_attack}, where the best results are highlighted in bold. Evidently, our proposed~\SystemName~method demonstrates its superior performance across different attack paradigms. The improvement is considerable on AutoAttack, by more than $0.51\%$ and $1.21\%$ on average compared to other methods on~\CIFAR-10 and~\CIFAR-100, respectively. In addition, we include two primary components of our proposed~\SystemName, \textit{i.e.,} SW and random JR  in~Table~\ref{tb:white_attack}. While the random JR, as expected, is highly vulnerable to most of the \textit{white-box} attacks due to its adversarial-agnostic defense strategy, the SW approach is on par with other AT methods.

% \begin{figure}[t]
%     \centering
%     \subfloat[\centering ~\CIFAR-10]{{\includegraphics[width=0.46\textwidth]{  Figures/pgd_wrn28_~\CIFAR-10.pdf} }}%
%     % \qquad
%     \hfill
%     \subfloat[\centering ~\CIFAR-100]{{\includegraphics[width=0.46\textwidth]{  Figures/pgd_wrn28_~\CIFAR-100.pdf} }}%
%     \caption{  Classification accuracy (\%) of defensive WRN28 models under different number of $L_{\infty}$-PGD iterations.}%
%     \label{fig:pgd_wrn28}
% \end{figure}
 \subsection{Online Adversarial Attacks}
 \label{sec:online_attack}
In order to validate the applicability of our defense model in real-world systems, we employ the \textit{stochastic virtual} method \cite{mladenovic2021online}. This method is designed as an online attack algorithm with a transiency property; that is, an attacker makes an \textbf{irrevocable decision} regarding whether to initiate an attack or not.

For our experiment,  we curate a dataset by downloading 1,000 images and categorizing them into 10 distinct classes, mirroring the structure of the \CIFAR-10 dataset. We refer to this new subset as \CIFAR-10-\WEB. In a similar vein, we assemble a \CIFAR-100-\WEB~ dataset comprising 5,000 images on the Internet, distributed across 100 classes analogous to the \CIFAR-100 dataset. To ensure the reliability of our findings, we replicate the experiment ten times. The outcomes of these trials are detailed in Table \ref{tb:online_attack}. Notably, our novel system, \SystemName, persistently showcases the most minimal fooling rate in comparison to other methods in the \textit{k-secretary} settings \cite{mladenovic2021online}, underscoring its robustness for real-world applications.

 \subsection{Compatibility with Different Frameworks and Datasets}
 \label{sec:compatibility}

   \subsubsection{Frameworks with surrogate models and relaxed perturbations} AWP \cite{wu2020adversarial} and LBGAT \cite{cui2021learnable}  are two SOTA benchmarks boosting adversarial robustness using surrogate models during training, albeit rather computationally expensive. Furthermore, they utilized existing AT losses, such as TRADES. We integrate our proposed optimization loss into AWP and LBGAT, respectively. Additionally, we also include the results of UDR \cite{bui2022unified} that used for creating relaxed perturbed noises upon its entire distribution. We selected TRADES as the best existing loss function that was deployed with these frameworks and experimented with~\CIFAR-100-WRN34 for comparison. For consistent comparison, we use TRADES loss as our $\mathcal{L}_{\text{AT}}$ in Eq. \ref{eqn:overall}. As shown in Table~\ref{tb:proxy_framwork}, our~\SystemName\ is compatible with the three frameworks and surpasses the baseline performance by a significant margin.
  
 \subsubsection{Large scale dataset} We conducted further experiments on Tiny-\ImageNet\ with PreactResnet18, comparing our method against competitive AT methods under most challenging attacks, including MIM, CW, FAB, Square, and AutoAttack, in Table \ref{tb:appli_attack}. All defensive methods are trained in 150 epochs with same settings as in Sec. \ref{sec:exp_settings}. We note that SAT unable to converge within 400 epochs. As observed, our method not only sustains competitive performance on clean sample but also displays superior robustness against adversarial perturbations, while the second best robust model, PGD, has to sacrifice substantially its clean accuracy compared to ours (33.81\% \textit{vs.} 37.97\%). In addition, this experiment illustrates that our proposed approach does not hinder the convergence process, unlike certain alternative optimum transport methods such as SAT, when applied to datasets of significant size.

 
 % \subsubsection{High-quality datasets with different backbones} We extend the evaluation of \SystemName's robustness, comparing it against two leading adversarial training (AT) frameworks: TRADES and SAT, across high-quality datasets and diverse backbone architectures. Specifically, we employ the \ImageNet100 \cite{5206848} dataset with PreActResNet18 \cite{he2016identity} and the \Intel\ dataset \cite{intelanalytics} with ResNet50 \cite{he2016deep}. For the \Intel\ dataset, the experimental settings are analogous to the \CIFAR-10 configuration, albeit with an adjusted learning rate of $1e^{-3}$—decaying by $0.1$ every 7 epochs—and training spanned 50 epochs. For \ImageNet100, settings are aligned with \CIFAR-100, but with SAT's training extended to 400 epochs.  In~Table~\ref{tb:appli_attack}, we report the results with strongest attacks from~Table~\ref{tb:white_attack}, MIM, CW (\textit{white-box}) Square attack (\textit{black-box}), and AutoAttack. Remarkably, under the \SystemName\ framework, our model not only displays superior robustness against adversarial perturbations but also sustains competitive performance on clean samples. This affirms \SystemName's adaptability to various network architectures and diverse classification scenarios.
 

\begin{table}[!t]
\caption{ \textbf{Basic sanity tests for our ~\SystemName~ method} with \textit{white-box} PGD attack.}
\centering
\resizebox{0.96\linewidth}{!}{%
    \begin{tabular}{c|c|c|c|c|c } 
    \Xhline{3\arrayrulewidth}
    \multicolumn{6}{c }{Number of step}\\
    \hline
    \textit{Clean} &1   &10   &20  &40  &50\\
    \hline
    \textit{84.01}$_\text{.53}$  &78.86$_\text{.69}$ &56.26$_\text{.24}$ &55.38$_\text{.29}$ &55.1$_\text{.40}$ &55.08$_\text{.40}$\\
    \hline \hline
    \multicolumn{6}{c }{ Perturbation budget $\epsilon$ w/ PGD-20}\\
    \hline
    \textit{Clean} &8/255   &16/255   &24/255   &64/255  &128/255\\
    \hline
    \textit{84.01}$_\text{.53}$  &55.38$_\text{.29}$ &24.57$_\text{1.00}$ &9.66$_\text{.54}$ &0.57$_\text{.01}$ &0.00$_\text{.01}$\\
    \Xhline{3\arrayrulewidth}
    \end{tabular}%
 }
\label{tb:sanity_test}
\end{table}


\subsection{Ablation Studies and Discussions}
\subsubsection{ Sanity Tests}
\label{abl:sanity_test}
The phenomenon of gradient obfuscation arises, when a defense method is tailored such that its gradients prove ineffective for generating adversarial samples \cite{athalye2018obfuscated}. However, the method designed in that manner can be an incomplete defense to adversarial examples \cite{athalye2018obfuscated}.  Adhering to guidelines from \cite{carlini2019evaluating}, we evaluate our pre-trained model on \CIFAR10 with WRN34  to affirm  our proposed ~\SystemName~ does not lean on gradient obfuscation. As detailed in Table~\ref{tb:sanity_test}, iterative attacks
are strictly more powerful than single-step attacks, whereas when increasing perturbation budget $\epsilon$ can also raise attack successful rate. Finally,  the PGD attack attains a 100\% success rate when $\epsilon=128/255$.


\subsubsection{Loss's derivative. }In continuation with our preliminary analysis, we highlight the disparities in defense model gradients across layers between our \SystemName\ and SAT.  Throughout intermediate layers in an attacked model, both frameworks provide stable ratios between perturbed and clean sample's gradients as shown in Fig.~\ref{fig:model_der_sat_JR_vs_ours}. It is worth noting that the gradients are derived on unobserved samples in the test set.  In the forward path, our \SystemName\ adeptly equilibrates gradients of adversarial and clean samples, with the majority of layers presenting ratio values approximating 1.   Moreover, in the backward path, since the victim model's gradients are deployed to generate more perturbations, our\ \SystemName\  model achieves better robustness when it can produce smaller gradients \textit{w.r.t.} its inputs. 
\begin{figure}[!t]
\centering
\includegraphics[width=0.48\textwidth]{ Figures/SAT_JR_vs_ours_model_der.pdf}
\caption{ \textbf{Average gradient norm ratios between adversarial and clean samples.} The ratios of $\mathbb{E}(||\nabla_{\theta_i}\mathcal{L}(\tilde{x})|| / ||\nabla_{\theta_i}\mathcal{L}({x})||)$ with respect to the model's parameters $\theta_{i}$ on~\CIFAR-10 dataset.}
\label{fig:model_der_sat_JR_vs_ours}
\end{figure}
\begin{table}[!t]
\caption{\textbf{Average derivative of $\mathcal{XE}$ loss} \textit{w.r.t} input ${x}$ {at the} pixel level after various PGD iterations. The lower the derivative is, the less perturbed the adversarial samples are when the model is abused.  Best results are in \textbf{bold}.}
\centering
\resizebox{0.47\textwidth}{!}{%
    \begin{tabular}{l | r | r |r | r | r |r} 
    \Xhline{3\arrayrulewidth}
    \multirow{2}{*}{\textit{Method}}   &\multicolumn{5}{c }{$\mathbb{E}(||\nabla_{{x}} \mathcal{L})||_1)$}\\
     & \textit{Clean}  & $PGD^1$ & $PGD^5$  & $PGD^{10}$ & $PGD^{15}$ & $PGD^{20}$  \Tstrut\Bstrut\\
    \Xhline{2\arrayrulewidth}
    $\mathcal{XE}$& 854$\cdot e^{-4}$  &5492$\cdot e^{-4}$  &4894$\cdot e^{-4}$  &4852$\cdot e^{-4}$  &4975$\cdot e^{-4}$  &4900$\cdot e^{-4}$\Tstrut\\
    Random JR & 105$\cdot e^{-4}$  &149$\cdot e^{-4}$  &272$\cdot e^{-4}$  &335$\cdot e^{-4}$  &360$\cdot e^{-4}$  &370$\cdot e^{-4}$\\
    PGD & 158$\cdot e^{-4}$  &232$\cdot e^{-4}$  &420$\cdot e^{-4}$  &520$\cdot e^{-4}$  &568$\cdot e^{-4}$  &586$\cdot e^{-4}$\\
    ALP & 118$\cdot e^{-4}$  &161$\cdot e^{-4}$  &249$\cdot e^{-4}$  &298$\cdot e^{-4}$  &322$\cdot e^{-4}$  &332$\cdot e^{-4}$\\
    TRADES & 45$\cdot e^{-4}$  &53$\cdot e^{-4}$  &73$\cdot e^{-4}$  &84$\cdot e^{-4}$  &89$\cdot e^{-4}$  &90$\cdot e^{-4}$\\
   SAT & 48$\cdot e^{-4}$  &54$\cdot e^{-4}$  &65$\cdot e^{-4}$  &75$\cdot e^{-4}$  &81$\cdot e^{-4}$  &84$\cdot e^{-4}$\\
   \rowcolor{backcolour}\textbf{{\SystemName~ (\textit{Ours})}}& \textbf{43}$\cdot e^{-4}$  &\textbf{48}$\cdot e^{-4}$  &\textbf{61}$\cdot e^{-4}$  &\textbf{70}$\cdot e^{-4}$  &\textbf{73}$\cdot e^{-4}$  &\textbf{75}$\cdot e^{-4}$\Bstrut\\
    \Xhline{3\arrayrulewidth}
    \end{tabular}%
}
\label{tb:prog_input_der}
\end{table}

\begin{table*}[!t]
\caption{Comparison between ours and SAT$+$\textit{Random JR} on ~\CIFAR-10  and ~\CIFAR-100 with  WRN34.}
\centering
\resizebox{0.94\textwidth}{!}{%
    \begin{tabular}{l | l | c | c c c c c c c | c c | c } 
    \Xhline{3\arrayrulewidth}
Dataset & Defense    &\textit{Clean}    &PGD$^{20}$    &PGD$^{100}$    &$L_{2}$-PGD      &MIM &FGSM   &CW     &FAB   &Square & SimBa   &\emph{AutoAtt}\Tstrut\Bstrut\\
    \Xhline{2\arrayrulewidth}

    
\multirow{2}{*}{\small{~\CIFAR-10}} &   \textit{SAT+JR}  &  \textit{83.75}$_\text{.48}$	& 54.15$_\text{.17}$	& 53.87$_\text{.07}$	& 62.37$_\text{.33}$	& 54.12$_\text{.16}$	& 60.25$_\text{.22}$	& 52.22$_\text{.31}$	& 52.63$_\text{.53}$	& 61.72$_\text{.39}$	& 71.03$_\text{.65}$	& 51.15$_\text{.37}$\Tstrut\\
    &\cellcolor{backcolour} \SystemName~(\emph{\textbf{ours}}) & \cellcolor{backcolour}\textit{84.01}$_\text{.53}$ & \cellcolor{backcolour}\textbf{55.38}$_\text{.29}$ & \cellcolor{backcolour}\textbf{55.08}$_\text{.36}$ & \cellcolor{backcolour}\textbf{63.87}$_\text{.09}$ & \cellcolor{backcolour}\textbf{55.31}$_\text{.29}$ & \cellcolor{backcolour}\textbf{61.03}$_\text{.18}$ & \cellcolor{backcolour}\textbf{54.09}$_\text{.12}$ & \cellcolor{backcolour}{54.17}$_\text{.07}$ & \cellcolor{backcolour}\textbf{63.11}$_\text{.21}$ & \cellcolor{backcolour}\textbf{72.04}$_\text{.68}$ & \cellcolor{backcolour}\textbf{52.57}$_\text{.12}$\Bstrut \\
    \Xhline{3\arrayrulewidth}
\multirow{2}{*}{\small{~\CIFAR-100}} &   \textit{SAT+JR$^{400}$}  &   \textit{52.13}$_{1.55}$	& 26.18$_\text{.46}$	& 25.99$_\text{.40}$	& 32.64$_\text{.90}$	& 26.11$_\text{.43}$	& 30.14$_\text{.84}$	& 25.10$_\text{.47}$	& 25.19$_\text{.85}$	& 30.39$_{1.02}$	& 39.71$_{1.14}$	& 23.79$_\text{.42}$\Tstrut\\
     &\cellcolor{backcolour} \SystemName~(\emph{\textbf{ours}}) & \cellcolor{backcolour}\textit{58.20}$_\text{.13}$ & \cellcolor{backcolour}\textbf{32.11}$_\text{.21}$ & \cellcolor{backcolour}\textbf{32.01}$_\text{.18}$ & \cellcolor{backcolour}\textbf{43.13}$_\text{.12}$ & \cellcolor{backcolour}\textbf{32.07}$_\text{.19}$ & \cellcolor{backcolour}\textbf{34.26}$_\text{.30}$ & \cellcolor{backcolour}\textbf{29.71}$_\text{.06}$ & \cellcolor{backcolour}\textbf{29.24}$_\text{.08}$ & \cellcolor{backcolour}\textbf{36.27}$_\text{.05}$ & \cellcolor{backcolour}\textbf{49.92}$_\text{.23}$ & \cellcolor{backcolour}\textbf{28.36}$_\text{.10}$ \Bstrut \\
    \Xhline{3\arrayrulewidth}
    \end{tabular}%
}
\label{tb:naive_combine}
\end{table*}

\begin{table*}[!t]
\caption{Accuracy ($\%$) of WRN34 model trained with JR having random projections, and informative projections respectively.}
\centering
\resizebox{0.94\textwidth}{!}{%
    \begin{tabular}{l | l | c | c c c c c c c | c c | c } 
    \Xhline{3\arrayrulewidth}
Dataset & Defense    &\textit{Clean}    &PGD$^{20}$    &PGD$^{100}$    &$L_{2}$-PGD       &MIM &FGSM   &CW     &FAB   &Square & SimBa   &\emph{AutoAtt}\Tstrut\Bstrut\\
    \Xhline{2\arrayrulewidth}
\multirow{2}{*}{\small{~\CIFAR-10}} &   \textit{Random JR}     & \textit{84.99}$_\text{.14}$ & 22.67$_\text{.15}$ & 21.89$_\text{.14}$ & 60.98$_\text{.19}$ & 22.49$_\text{.18}$ & 32.99$_\text{.21}$ & 22.00$_\text{.06}$ & 21.74$_\text{.11}$ & 45.86$_\text{.20}$ & 71.20$_\text{.31}$ & 20.54$_\text{.14}$ \Tstrut\\
    &\cellcolor{backcolour} \textit{Optimal JR} &\cellcolor{backcolour} \textit{84.47}$_\text{.15}$    &\cellcolor{backcolour} \textbf{24.81}$_\text{.56}$
    &\cellcolor{backcolour} \textbf{23.97}$_\text{.48}$
   &\cellcolor{backcolour} \textbf{61.67}$_\text{.50}$
    &\cellcolor{backcolour} \textbf{24.62}$_\text{.49}$
  &\cellcolor{backcolour} \textbf{34.22}$_\text{.50}$
   &\cellcolor{backcolour} \textbf{23.91}$_\text{.70}$
   &\cellcolor{backcolour} \textbf{24.74}$_\text{.49}$
  &\cellcolor{backcolour} \textbf{47.18}$_\text{.54}$
   &\cellcolor{backcolour} \textbf{73.59}$_\text{.15}$
  &\cellcolor{backcolour} \textbf{22.84}$_\text{.64}$\Bstrut \\
    \Xhline{3\arrayrulewidth}
\multirow{2}{*}{\small{~\CIFAR-100}} &   \textit{Random JR}    & \textit{66.58}$_\text{.17}$ & 9.41$_\text{.44}$ & 8.87$_\text{.42}$ & 37.79$_\text{.31}$ & 9.27$_\text{.49}$ & 16.38$_\text{.33}$ & 10.27$_\text{.45}$ & 9.26$_\text{.23}$ & 23.53$_\text{.15}$ & 48.86$_\text{.55}$ & 8.10$_\text{.57}$ \Tstrut\\
     &\cellcolor{backcolour} \textit{Optimal JR} &\cellcolor{backcolour} \textit{65.16}$_\text{.27}$     &\cellcolor{backcolour} \textbf{10.90}$_\text{.29}$     &\cellcolor{backcolour} \textbf{10.32}$_\text{.35}$     &\cellcolor{backcolour} \textbf{38.73}$_\text{.20}$     &\cellcolor{backcolour} \textbf{10.78}$_\text{.32}$     &\cellcolor{backcolour} \textbf{17.30}$_\text{.29}$     &\cellcolor{backcolour} \textbf{11.29}$_\text{.43}$     &\cellcolor{backcolour} \textbf{10.43}$_\text{.34}$     &\cellcolor{backcolour} \textbf{24.23}$_\text{.41}$     &\cellcolor{backcolour} \textbf{50.31}$_\text{.13}$    &\cellcolor{backcolour} \textbf{9.14}$_\text{.34}$ \Bstrut \\
    \Xhline{3\arrayrulewidth}
    \end{tabular}%
}
\label{tb:randJac_vs_WDJac}
\end{table*}
In addition, in Table~\ref{tb:prog_input_der}, we present the average magnitude of cross-entropy loss's derivative \textit{w.r.t.} the input images from~\CIFAR-10 dataset with different PGD white-box attack iterations on WRN34.  Notably, as the number of attack iterations increases, the perturbation noise induced by the output loss derivatives intensifies. However, our proposed framework consistently exhibits the lowest magnitude across all iterations. This characteristic underscores the superior robustness performance of \SystemName.

% \begin{figure}[!t]
% \includegraphics[width=0.36\textwidth]{  Figures/convergence_sw_sat.pdf}
% \caption{ Convergence rates of SW and SAT on ~\CIFAR-10 and ~\CIFAR-100, respectively. While both algorithms exhibit a similar convergence on ~\CIFAR-10, SAT fails to converge on ~\CIFAR-100 in 100 epochs.} 
% \label{fig:converg}

% \end{figure}
 \subsubsection{SW distance vs. Sinkhorn divergence} Bouniot \textit{et al.} propose the SAT algorithm that deploys Sinkhorn divergence to push the distributions of clean and perturbed samples towards each other \cite{bouniot2021optimal}.
While SAT can achieve comparable results to previous research, its limitations become pronounced in high-dimensional space, i.e., datasets with a large number of classes exhibit slower convergence, as demonstrated in other research~\cite{meng2021large,petrovich2020feature}. Our empirical results indicate that \textbf{SAT struggles to converge within 100 epochs} for the \CIFAR-100 dataset. 
% as shown in Fig.~\ref{fig:converg}. 
Meanwhile, we observed that hyper-parameter settings highly affect the adversarial training~\cite{pang2020bag}, and the performance improvement by SAT can be partly achieved with additional training epochs. Nevertheless, we still include results from SAT trained in 400 epochs on ~\CIFAR-100 in \cite{bouniot2021optimal}.





 \subsubsection{Our~\SystemName~vs. Naive Combination of SAT \& random JR} To discern the impact of Jacobian regularization and distinguish our method from the naive combination of SAT and JR, we report their robustness under wide range of \textit{white-box} PGD attack in Table~\ref{tb:naive_combine}. The experiments are conducted with ~\CIFAR-10 and ~\CIFAR-100 with WRN34. Our optimal approach attains slightly better robustness on small dataset (~\CIFAR-10). On the large dataset (~\CIFAR-100), however, our  ~\SystemName~ achieves significant improvement. This phenomenon is explained by the fact that regularizing the input-output Jacobian matrix increases the difficulty of the SAT algorithm's convergence, which results in a slower convergence. Therefore, naively combining AT and random Jacobian regularization can restrain the overall optimization process.  
 
\subsubsection{Optimal vs. random projections}To verify the efficiency of the optimal Jacobian regularization, WRN34 is trained on ~\CIFAR-10 using {$\mathcal{XE}$} loss with clean samples and the regularization term using~Eq.~\ref{eqn:one_jac} and~Eq.~\ref{eqn:opt_jac} respectively, as  follow:\\
\begin{equation}
    \mathcal{L} = \sum^{\mathcal{B}}_{i=1} \left (\mathcal{L_{XE}}(x_i, y_i) + \lambda_{J}||J(x_i)||_{F}^{2} \right),
\end{equation}
where $\lambda_J$ is a hyper-parameter to balance the regularization term and {$\mathcal{XE}$} loss that is set to 0.02 for this experiment. As we can observe from Table~\ref{tb:randJac_vs_WDJac}, the Jacobian regularized model trained with SW-supported projections consistently achieves higher robustness, compared to the random projections. As shown, our proposed optimal regularization consistently achieves up to 2.5\% improvement in accuracy under \textit{AuttoAttack} compared to the random one.

In addition, we highlight the advantages of optimal Jacobian regularization on decision boundaries in Fig.\ref{fig:more_cross_section}. Models trained without this regularizer are notably susceptible to perturbations. However, integrating the Jacobian regularizer augments robustness by broadening the decision boundaries, evidenced by an \textbf{enlarged black circle}. Our optimal Jacobian regularizer further extends the decision boundaries, amplifying model resilience. The rationale behind this enhancement lies in the informative directions showcased in Fig.\ref{fig:opt_move_toy}, guiding the model to achieve optimal projections within the input-output Jacobian regularization framework. 

\begin{figure}[!t]
     \includegraphics[width=0.52\textwidth]{Figures/cross_decision/cross_decision_group2.pdf}
    \caption{\textbf{Cross sections of decision boundaries in the input space}. 1st row: Sample input images. 2nd row: Model trained without regularization. 3rd row: Model train with Jacobian regularization having random projection. 4th row: Model trained with Jacobian regularization having informative projections. Our optimal JR benefits from the informative directions obtained by SW distance, and thus helps the model to regularize these sensitive direction of clean samples and produce larger decision cells.} %
    \label{fig:more_cross_section}
\end{figure}

% \begin{figure*}[!t]
%   \centering
% \includegraphics[width=0.78\linewidth]{Figures/main_magnitude_activation_group.pdf}
% \caption{{Magnitude of activation} at the penultimate layer for models trained with different defense methods. Our \SystemName~ can regulate adversarial samples' magnitudes similar to clean samples' while well suppressing both of them. }%
% \label{fig:mag_activation_all}
% \end{figure*}





% \subsubsection{Activation Magnitude}
% \label{app:magnitude_activation}
% Figure ~\ref{fig:mag_activation_all} depicts the activation magnitudes at the penultimate layer of WRN34 across various AT frameworks. Although AT methods manage to bring the adversarial magnitudes closer to their clean counterparts, the magnitudes generally remain elevated, with PGD-AT being especially prominent. Through a balanced integration of the input Jacobian matrix and output distributions, our proposed method effectively mitigates the model's susceptibility to perturbed samples.



