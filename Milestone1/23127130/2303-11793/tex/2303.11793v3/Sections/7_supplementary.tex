
% \setcounter{page}{1}
% \newcount\cvprrulercount
% \setcounter{cvprrulercount}{1}
% \begin{center}
% \vspace{30pt}Supplementary Material for\\\textbf{OTJR: Optimal Transport Meets Optimal Jacobian Regularization for Adversarial Robustness\\}\vspace{16pt} 
% \end{center}

{\huge\textbf{Appendix}}
\appendix
\renewcommand{\thesection}{\Alph{section}}

\section{Threat Model and Adversarial Sample}
In this section, we summarize essential terminologies of adversarial settings related to our work.  We first define a threat model, which consists of a set of assumptions about the adversary. Then, we describe the generation mechanism of adversarial samples in AT frameworks for the threat model defending against adversarial attacks.

\subsection{Threat Model}
Adversarial perturbation was firstly discovered by~\citeauthor{szegedy2013intriguing}, and it instantly strikes an array of studies in both adversarial attack and adversarial robustness. ~\citeauthor{carlini2019evaluating} specifies a threat model for evaluating a defense method including a set of assumptions about the adversary's goals, capabilities, and knowledge, which are briefly delineated as follows:
 \begin{itemize}
     \item Adversary's goals could be either simply deceiving a model to make the wrong prediction to any classes from a perturbed input or making the model misclassify a specific class to an intended class. They are known as \textit{untargeted} and \textit{targeted} modes, respectively.
     \item Adversary's capabilities define reasonable constraints imposed on the attackers. For instance, a $L_p$ certified robust model is determined with the worst-case loss function $\mathcal{L}$ for a given perturbation budget $\epsilon$:
     \begin{equation}
         \mathbb{E}_{(x,y)\sim \mathcal{D}} \Bigg[\max_{ \tilde{x}\in {B}_{p} (x, \epsilon)} \mathcal{L}(f(\tilde{x}), y) \Bigg],
     \end{equation}
     where ${B}_{p} (x, \epsilon)=\{u \in \mathbb{R}^{\mathcal{I}}: ||u-x||_{p} \leq  \epsilon\}$.
     \item Adversary's knowledge indicates what knowledge of the threat model that an attacker is assumed to have. Typically, \textit{white-box} and \textit{black-box} attacks are two most popular scenarios studied. The white-box settings assume that attackers have full knowledge of the model's parameters and its defensive scheme. In contrast, the black-box settings have varying degrees of access to the model's parameter or the defense.
 \end{itemize}
 
Bearing these assumptions about the adversary, we describe how a defense model generates adversarial samples for its training in the following section.   
% \begin{algorithm}[!t]
% \caption{\SystemName: AT with SW and optimal Jacobian regularization.}

% \begin{algorithmic}[!b]
% \Require   DNN $f$
% parameterized by $\theta$, training dataset $\mathcal{D}$. Number of projections $K$. Maximum perturbation $\epsilon$, step size $\eta$, number of adversarial iteration $P$. Loss' hyper-parameters $\lambda_J$ and $\lambda_{SW}$. Learning rate $\alpha$ and batch size of $\mathcal{B}$.
%     \While{not converged}
%         \For{$\{(x_{i},\textrm{ } y_{i})\}_{\mathcal{B}} \in \mathcal{D}$}
%             \State $\nu :  = \{z_{i}\}_{\mathcal{B}} = f(x_i; {\theta})| _{i=1,..,\mathcal{B}}$ 
%             \State  \texttt{\# Generate adv. samples}
%             \For{iteration $t \gets 1$ to $P$}
%                 \State $\tilde{x_i} = \Pi_{x}^{\epsilon}(\tilde{x_i} +\eta \cdot \text{sgn} (\nabla _{\tilde{x_i}} \mathcal{L}(\tilde{x_i}, y_i)))| _{i=1,..,\mathcal{B}}$  
%             \EndFor
%             \State $\mu :  = \{\tilde{z_i}\}_{\mathcal{B}} = f(\tilde{x_i}; {\theta})| _{i=1,..,\mathcal{B}}$  
            
%             \State $SW \gets 0$ 
%             \State $\sigma_i \gets 0|_{i=1,..,\mathcal{B}}$ 
%             \For{iteration $k \gets 1$ to $K$}
%                 \State \texttt{\# Sample $\hat{v}_k$ from $\mathcal{S}^{C-1}$}           \State $\hat{v}_k$ $\gets$  $\mathcal{U}(\mathcal{S}^{C-1})$ 
%                 \State \texttt{\# Add SW under projection $\hat{v}_k$}
%                 \State $SW \gets SW + \psi  \left(\tau_1 \circ \mathcal{R}_{\hat{v}_k}\mu, \tau_2 \circ\mathcal{R}_{\hat{v}_k}\nu \right)$ 
%                 \State \texttt{\# Calculate samples' movements under $\hat{v}_k$}
%                 \State $m_k \gets \left(\tau_1^{-1} \circ \tau_2 \circ \mathcal{R}_{\hat{v}_k}\nu - \mathcal{R}_{\hat{v}_k}\mu \right) \otimes \hat{v}_k$ 
%                 \State $\sigma_i \gets \sigma_i + m_{k,i}|_{i=1,..,\mathcal{B}}$
%             \EndFor
%             \State $\sigma_i \gets \sigma_i / ||\sigma_i||_2 $  $ |_{i=1,..,\mathcal{B}}$
%             \State $\mathcal{L} \gets \sum^{\mathcal{B}} \left( \mathcal{L_{XE}}(\tilde{x_i}, y_i) + \lambda_J ||J(x_i | \sigma_i)||_{F}^{2} \right) $  \\\hspace{3cm}  $ +\quad \lambda_{SW}SW(\mu, \nu) $
%             \State $\theta \gets \theta - \alpha \cdot \nabla _{\theta} \mathcal{L}$ 
%         \EndFor
%     \EndWhile
% \end{algorithmic}

% \label{alg:opt_move}
% \end{algorithm}

 \subsection{Adversarial Sample in AT}
 Among multiple attempts to defend against adversarial perturbed samples, adversarial training (AT) is known as the most successful defense method. In fact, AT is an data-augmenting training method that originates  from the work of \cite{goodfellow2014explaining}, where crafted adversarial samples are created by the fast gradient sign method (FGSM), and mixed into the mini-batch training data. Subsequently, a wide range of studies focus on developing powerful attacks \cite{kurakin2016adversarial,dong2018boosting,carlini2017towards,croce2020minimally}. Meanwhile, in the opposite direction to the adversarial attack, there are also several attempts to resist against adversarial examples \cite{kannan2018adversarial,zhang2019defense,shafahi2019adversarial}. In general, a defense model is optimized by solving a minimax problem:
  \begin{equation}
      \min _{\theta} \Big[\max_{\tilde{x} \in {B}_{p}(x, \epsilon)} \mathcal{L_{XE}}(\tilde{x}, y; \theta) \Big ],
  \end{equation}
where the inner maximization tries to successfully create perturbation samples subjected to an $\epsilon$-radius ball $B$ around the clean sample $x$ in $L_p$ space. The outer minimization tries to adjust the model's parameters to minimize the loss caused by the inner attacks. Among existing defensive AT, PGD-AT \cite{madry2017towards} becomes the most popular one, in which the inner maximization is approximated by the multi-step projected gradient (PGD) method:
 \begin{equation}
 \label{eqn:pgd_gen}
     \tilde{x}_{t+1} = \Pi_{x}^{\epsilon}(\tilde{x_{t}} +\eta \cdot \text{sgn} (\nabla _{\tilde{x_{t}}} \mathcal{L}(\tilde{x_{t}}, y))),
 \end{equation}
 where $\Pi_{x}^{\epsilon}$ is an operator that projects its input into the feasible region ${B}_{\infty}(x, \epsilon)$, and $\eta \in \mathbb{R}$ is called step size. The loss function in Eq.~\ref{eqn:pgd_gen} can be modulated to derive different variants of generation mechanism for adversarial samples in AT. For example, Zhang \textit{et al.} \cite{zhang2019theoretically} utilizes the loss between the likelihood of clean and adversarial samples for updating the adversarial samples. In our work, we use Eq.~\ref{eqn:pgd_gen} as our generation mechanism for our AT framework.
 

%  \subsection{Adversarial attacks}
%  \label{supp:adv_attacks}
%  For the \textit{white-box} attacks, we deploy the followings in our experiement:
%  \begin{itemize}
%     \item \textbf{\textit{FGSM}}\cite{goodfellow2014explaining}: Adversarial samples are generated under the $L_{\infty}$ norm as follows:
%     \begin{equation}
%         x^{adv} = x + \epsilon \cdot \text{sgn}(\nabla_{x}\mathcal{L_{XE}}(x, y)).
%     \end{equation}
%     % \item \textbf{\textit{BIM}}~\cite{kurakin2016adversarial}: BIM with $L_{\infty}$ constraint creates adversarial samples by iteratively taking multiple gradient updates as follows:
%     % \begin{equation}
%     %     x_{t+1}^{adv} = \text{clip}_{x,\epsilon}\left ( x_{t}^{adv} + \eta \cdot \text{sgn}(\nabla_{x}\mathcal{L_{XE}}(x_{t}^{adv}, y))\right),
%     % \end{equation}
%     % where $\eta$ is a perturbation step size. 
%     \item \textbf{\textit{PGD}}~\cite{madry2017towards}: Its initial point $x_0^{adv}$ is uniformly sampled from the neighbor around $x$.
%     \item \textbf{\textit{MIM}}~\cite{dong2018boosting}: It is the integration of BIM and a momentum with decay factor $\mu$ as:
%     \begin{equation}
%         g_{t+1} = \mu\cdot g_{t} + \frac{\nabla_{x}\mathcal{L_{XE}}(x_{t}^{adv}, y)}{|| \nabla_{x}\mathcal{L_{XE}}(x_{t}^{adv}, y)|| _{1}},
%     \end{equation}
%     and the adversarial samples are then updated by:
%     \begin{equation}
%         x_{t+1}^{adv} = \text{clip}_{x,\epsilon}\left ( x_{t}^{adv} + \eta \cdot \text{sgn}(g_{t+1})\right),
%     \end{equation}
%     \item \textbf{\textit{C\&W}}~\cite{carlini2017towards}: Adversarial samples are generated under $L_2$ norm by solving:
%     \begin{equation}
%         x^{adv}  = \argmin _{x^{\prime}}\big[ c \cdot \max(f(x^{\prime})_{y} - \max _{i\neq y}f(x^{\prime})_{i}, 0)  + ||x^{\prime}-x ||_{2} \big],
%     \end{equation}
%     where $c$ is a variable that found by binary search.
%     \item \textbf{\textit{FAB}}~\cite{croce2020minimally}: It iteratively generates adversarial samples lie on the decision boundaries of a classifier with minimal perturbation. We implement both targeted and untargeted FAB attacks.
%     % \item \textbf{FAB} with a relative magnitude parameter $\alpha$ generate an adversarial samples lie on the decision boundaries with minimal perturbation as follows:
%     % \begin{equation}
%     %     x^{adv}_{t+1} = \text{proj}_{C}\big( (1-\alpha)(x^{adv}_{t} + \eta\sigma_{t}) + \alpha(x+\eta\sigma^{\text{orig}}_{t})\big),
%     % \end{equation}
%     % where proj$_{C}$ is a projection on the hyperplane $\pi$ with constraint set $C$, and $\sigma_{t}$ and $\sigma^{\text{orig}}_{t}$ indicate the distances of $x_{t}^{adv}$ and $x$ to $\pi$ inside $C$. 
% \end{itemize}

% Meanwhile, we utilize the following \textit{black-box} attacks:
% \begin{itemize}
%     \item \textbf{\textit{Square}}~\cite{andriushchenko2020square}. It is an efficient random search-based adversarial attack that generates $L_{\infty}$ adversarial sample by continuously adding small windows of perturbation $h$ to the clean image, which has size of $w \times w \times c$ until success:
%     \begin{equation}
%         x^{adv}_{t+1} = x^{adv}_{t} + \sigma, \sigma \sim P(\epsilon, h_{t+1}, w, c,x^{adv}_{t}, x),
%     \end{equation}
%     where $P$ is the uniform distribution with values are in $\{-2\epsilon, 2\epsilon\}$.
%     \item \textbf{\textit{SimBA}}~\cite{guo2019simple}. This approach iteratively chooses a random vector  $q$ from an orthonormal input space $Q$ and adds this vector to the clean image as follows:
%     \begin{equation}
%         x^{adv}_{t+1} = x^{adv}_{t} + \alpha \cdot q, \alpha \in \{\epsilon, -\epsilon \}.
%     \end{equation}
% \end{itemize}

%  \section{Detailed Implementation of ~\SystemName}
%  \label{app:implementation}
 


% \begin{lstlisting}[language=Python, caption={OTJR Python Code Implementation}, label={listing:pseudocode}]
% import torch
% from torch.autograd import grad
% """
% Arguments:
%     x: (batch_size, 3, H, W)
%     lb: (batch_size, )
%     src, tgt: (batch_size, C)
%     K: number of projection
% """
% # generate random projections
% projs = torch.randn(size=(dim, K))
% l2 = projs.pow(2).sum(0, True).pow(0.5)
% # normalize projections
% projs = projs.div(l2) 
% # project `src` and `tgt` distributions
% src_p = src.matmul(projs).transpose(0,1)
% tgt_p = tgt.matmul(projs).transpose(0,1)
% # ascending sort
% tgt_sort, _ = torch.sort(tgt_p,1)
% src_rank = torch.argsort(
%                 torch.argsort(src_p), 1)
% # tau_1^(-1) o tau_2 in Eqn.10
% tgt2src =  torch.gather(input=tgt_sort, 
%                 dim=1, index=src_rank)
% # movement
% src_move = tgt2src - src_p
% sw = src_move.pow(2).sum() # Eqn.9
% m = torch.zeros_like(src)
% for i in range(K):
%   m += torch.outer(torch.squeeze(
%         src_move[i]), projs[:,i]) # Eqn. 10
% l2 = m.pow(2).sum(1, True).pow(0.5)
% delta = m.div(l2) # Eqn. 11
% J = grad(x.reshape(-1),
%          delta.reshape(-1),
%          retain_graph=True,
%          create_graph=True
%          ).norm().pow(2) # Eqn. 12
% \end{lstlisting}
 




% % \subsection{\SystemName\ Implementation in Python}
%  \label{app:implementation_python}
%  We include the pseudo-code as an example, implemented with PyTorch, of how the $SW$ distance and optimal Jacobian regularization are calculated (List \ref{listing:pseudocode}). We use $\psi$ function as the square of $l_2$ loss. Note that $\psi  \left(\tau_1 \circ \mathcal{R}_{\hat{v}_k}\mu, \tau_2 \circ\mathcal{R}_{\hat{v}_k}\nu \right) = \psi  \left(\tau_1^{-1} \circ \tau_2 \circ\mathcal{R}_{\hat{v}_k}\nu, \mathcal{R}_{\hat{v}_k}\mu \right)$, since $l_2$ loss is applied in element-wise, and $\tau_1^{-1}$ now becomes the rank of the values in $\mathcal{R}_{\hat{v}_k}\mu$.
 






% As an simple illustration, Fig.~\ref{fig:opt_move_toy} provide a toy example of the optimal movement directions in 2 dimensional space obtained from ~Eq.~\ref{eqn:opt_move}.

% jac_mat = torch.autograd.grad(lb.reshape(-1), tgt.reshape(-1), delta.reshpae(-1), retain_graph=True, create_graph=True)
% J = torch.norm(jac_mat)**2 # Eqn. 12

\begin{algorithm*}[t!]
\caption{\SystemName: AT with SW and optimal Jacobian regularization}
\label{alg:at_infor_reg}
\begin{algorithmic}[1]
\Require DNN $f$
parameterized by $\theta$, training dataset $\mathcal{D}$. Number of projection $K$. Maximum perturbation $\epsilon$, step size $\eta$, number of adversarial iteration $P$. Loss' hyper-parameters $\lambda_J$ and $\lambda_{SW}$. Learning rate $\alpha$ and a mini-batch size of $\mathcal{B}$.
\While{not converged}
    \For{$\{(x_{i},\textrm{ } y_{i})\}_{\mathcal{B}} \in \mathcal{D}$}
        \State $\nu :  = z_{i} = f_{\theta}(x_i)| _{i=1,..,\mathcal{B}}$ \Comment{\textit{forward a batch of clean samples through the model}}
        \For{iteration $t \gets 1$ to $P$}
            \State $\tilde{x_i} = \Pi_{x}^{\epsilon}(\tilde{x_i} +\eta \cdot \text{sgn} (\nabla _{\tilde{x_i}} \mathcal{L}(\tilde{x_i}, y_i)))| _{i=1,..,\mathcal{B}}$  \Comment{{\emph{generate adv. samples by $L_{\infty}$-PGD in $P$ iterations}}}
        \EndFor
        \State $\mu :  = \tilde{z_i} = f_{\theta}(\tilde{x_i})| _{i=1,..,\mathcal{B}}$  \Comment{\textit{forward a batch of adv. samples through the model}}
        
        \State $SW \gets 0$ \Comment{\emph{initialize SW loss}}
        \State $\sigma_i \gets 0|_{i=1,..,\mathcal{B}}$ \Comment{\emph{initialize $\mathcal{B}$ Jacobian projections}}
        \For{iteration $k \gets 1$ to $K$}
            \State $\hat{v}_k$ $\gets$  $\mathcal{U}(\mathcal{S}^{C-1})$ \Comment{{\emph{uniformly sample $\hat{v}_k$ from $\mathcal{S}^{C-1}$}}}
            
            \State $SW \gets SW + \psi  \left(\tau_1 \circ \mathcal{R}_{\hat{v}_k}\mu, \tau_2 \circ\mathcal{R}_{\hat{v}_k}\nu \right)$ \Comment{{\emph{add SW under projection $\hat{v}_k$}}}
            \State $m_k \gets \left(\tau_1^{-1} \circ \tau_2 \circ \mathcal{R}_{\hat{v}_k}\nu - \mathcal{R}_{\hat{v}_k}\mu \right) \otimes \hat{v}_k$ \Comment{{\emph{calculate samples' movements under $\hat{v}_k$}}}
            \State $\sigma_i \gets \sigma_i + m_{k,i}|_{i=1,..,\mathcal{B}}$
        \EndFor
        \State $\sigma_i \gets \sigma_i / ||\sigma_i||_2 $  $ |_{i=1,..,\mathcal{B}}$
        \State $\mathcal{L} \gets \sum^{\mathcal{B}} \left( \mathcal{L_{XE}}(\tilde{x_i}, y_i) + \lambda_J ||J(x_i | \sigma_i)||_{F}^{2} \right)  $  $ +\quad \lambda_{SW}SW(\mu, \nu) $ \Comment{{\emph{overall loss}}}
        \State $\theta \gets \theta - \alpha \cdot \nabla _{\theta} \mathcal{L}$ \Comment{{\emph{update model's parameters $\theta$}}}
    \EndFor
\EndWhile
\end{algorithmic}
\label{alg:opt_move}
\end{algorithm*}
 
  
% \section{Appendix}
% In this section, we first briefly present our algorithm for training our \SystemName framework. Next, we conduct intensive experiments on various AT methods. 
\section{Training Algorithm for \SystemName}

Our end-to-end algorithm for optimizing ~Eq.~\ref{eqn:overall} is provided in  Algorithm \ref{alg:opt_move}. As mentioned, in practice, deep learning libraries allow for the simultaneous sampling of $K$ uniform vectors, denoted as $\hat{v}_k$. Consequently, the computation of random projections and the determination of optimal movement steps can be effectively vectorized and executed concurrently.

\section{Hyper-parameter sensitivity}
\label{abl:hyper_study}
In Table~\ref{tb:ab_lambda_sw}, we present ablation studies focusing on hyper-parameter sensitivities, namely, $\lambda_J$, $\lambda_{SW}$, and $K$, using the \CIFAR-10 dataset and WRN34 architecture. We observe that excessive $\lambda_J$ values compromise accuracy and robustness, a result of the loss function gradients inducing adversarial perturbations during the AT step. While $\lambda_{SW}$ offers flexibility in selection, models with minimal $\lambda_{SW}$ values inadequately address adversarial samples, and high values risk eroding clean accuracy. For the slice count $K$, a lower count fails to encapsulate transportation costs across latent space distributions; conversely, an overly large $K$ brings marginal benefits at the expense of extended training times. We acknowledge potential gains from further hyper-parameter optimization.
\section{Training Time}
Table~\ref{tb:training_time} indicates the average training time per epoch of all AT methods on our machine architecture using WRN34 model on ~\CIFAR-100 dataset. Notably, although the SAT algorithm demonstrates a commendable per-epoch training duration, its convergence necessitates up to four times more epochs than alternative methods, especially on large scale datasets such as CIFAR-100. Despite our method delivering notable enhancements over prior state-of-the-art frameworks, its computational demand during training remains within acceptable bounds.
\begin{table}[th!]
\caption{\textbf{Hyper-parameter tuning.} The sensitivities of hyper-parameters: $\lambda_J$, $\lambda_{SW}$ and $K$. Without Jacobian regularization, the model cannot achieve the best performance. Trade-off between model's accuracy vs. robustness is shown via $\lambda_{SW}$.}
\centering
\resizebox{0.44\textwidth}{!}{%
    \begin{tabular}{c | c | c | c | c | c | c } 
    \Xhline{3\arrayrulewidth}
    \multicolumn{3}{c | }{Hyper-parameters}    &\multicolumn{4}{c}{Robustness}\\
    \Xhline{1\arrayrulewidth}
    $\lambda_J$    &$\lambda_{SW}$ &$K$   &\textit{Clean}   &PGD$^{20}$    &PGD$^{100}$    &AutoAttack \\
    \hline \hline
     \rowcolor{backcolour}  0.002    &64    &32     &\textit{84.53}    &55.07    &54.69    &52.41\\
     \hline
    \textcolor{blue}{\underline{0.01}}    &64    &32  &\textit{84.75}    &54.37    &54.06    &52.13 \\
    \textcolor{blue}{\underline{0.05}}    &64    &32  &\textit{82.82}    &54.98    &54.72    &52.00\\
    \Xhline{2\arrayrulewidth}
    0.002    &\textcolor{blue}{\underline{32}} &32     &\textit{85.47}    &54.85    &54.46    &52.23\\
    0.002    &\textcolor{blue}{\underline{72}}    &32   &\textit{83.19}    &55.70    &55.40    &53.04\\
    \Xhline{2\arrayrulewidth}
    0.002    &64    &\textcolor{blue}{\underline{16}}  &\textit{81.47}    &55.10    &54.98    &51.82\\
    0.002    &64   &\textcolor{blue}{\underline{64}}  &\textit{85.79}    &53.80    &53.36    &51.83\\
    \Xhline{3\arrayrulewidth}
    \end{tabular}%
}
\label{tb:ab_lambda_sw}
\end{table}


\begin{table}[th!]
\caption{\textbf{Training time per epoch} of AT methods. Even though our method's training time/epoch is slightly slower than the SAT's as the additional Jacobian regularization, it can achieve faster convergence on large-scale datasets. }
\centering
\resizebox{0.86\linewidth}{!}{%
    \begin{tabular}{l | c || l | c } 
    \Xhline{2\arrayrulewidth}
    Method & Time (\textit{mins}) &Method & Time (\textit{mins})
     \\
    \hline \hline
    $\mathcal{XE}$ &1.63$_\text{.00}$ &PGD-AT &12.32$_\text{.03}$ \\
    ALP & 13.56$_\text{.03}$  & TRADES &16.42$_\text{.06}$ \\
      SAT &14.68$_\text{.01}$ &{\SystemName~(Ours)} &18.02$_\text{.12}$ \\
    \Xhline{2\arrayrulewidth}
    \end{tabular}%
}
\label{tb:training_time}
\end{table}


% \subsection{Smaller backbone experiment}
% Similar to our main experiment, we provide a comparison between our OTJR training framework with different defense methods  in Table~\ref{tb:white_attack_wrn28} with a smaller backbone - WRN28. Hyper-parameters are kept the same as we used. We note that our proposed framework achieves significant improvements across various while-box and black-box attacks. 

% \begin{table*}[!t]
% \caption{\textbf{Classification accuracy ($\%$) under \textit{white-box}, \textit{black-box} attacks and \textit{AutoAttack}.} Different defense methods trained on~\CIFAR-10 and~\CIFAR-100 datasets using WRN28, where underscript indicates training epochs.  Best results are in \textbf{bold}.}
% \centering
% \resizebox{0.92\textwidth}{!}{%
%     \begin{tabular}{c | l | c | c c c c c c c | c c | c } 
%     \Xhline{3\arrayrulewidth}
%     Dataset & Defense    &\textit{Clean}    &PGD$^{20}$    &PGD$^{100}$    &$L_{2}$-PGD       &MIM &FGSM   &CW     &FAB   &Square & SimBa  &\emph{AutoAtt}\Tstrut\Bstrut\\
%     \Xhline{2\arrayrulewidth}
%     \multirow{5}{*}{\rotatebox[origin=c]{90}{\centering \small{~\CIFAR-10}}}    
%     &TRADES    &\textit{84.52}    &54.06    &53.85    &61.07    &54.10    &60.27    &53.08    &53.43    &62.35    &70.84    &51.91\Tstrut\\
% &ALP    &\textit{85.90}    &47.31    &46.77    &55.98    &47.16    &57.96    &47.73    &53.35    &59.43    &68.82    &46.46\\
% &PGD-AT   &\textit{86.39}    &46.69    &46.22    &56.33    &46.61    &56.36    &47.44    &48.12    &58.96    &68.40    &45.98\\
% &SAT    &\textit{82.85}    &53.34    &53.06    &60.49    &53.34    &59.57    &51.69    &52.22    &60.73    &69.79    &50.50\Bstrut\\ 
%     &\SystemName~(\emph{Ours}) & \textit{83.63}    &\textbf{55.25}    &\textbf{54.87}    &\textbf{64.16}    &\textbf{55.08}    &\textbf{60.48}    &\textbf{53.41}    &\textbf{53.58}    &\textbf{62.61}    &\textbf{71.73}    &\textbf{52.03}\Bstrut \\
%     \Xhline{3\arrayrulewidth}
%     \multirow{5}{*}{\rotatebox[origin=c]{90}{\centering \small{~\CIFAR-100}}}    
%     &TRADES       &\textit{57.52}    &30.27    &30.07    &36.10    &30.19    &32.78    &28.21    &27.47    &33.41    &44.34    &26.46\Tstrut\\
% &ALP    &\textit{61.08}    &25.83    &25.40    &33.55    &25.74    &30.97    &25.59    &25.14    &32.57    &43.78    &23.89\\
% &PGD-AT   &\textit{59.73}    &23.45    &23.11    &31.08    &23.47    &28.92    &24.62    &23.86    &31.07    &41.27    &22.57\\
% &SAT    &\textit{53.27}    &26.82    &26.63    &31.99    &26.67    &30.96    &25.30    &26.26    &30.83    &40.45    &24.31\Bstrut\\ 


%     &\SystemName~(\emph{Ours})   &\textit{57.82}    &\textbf{31.84}    &\textbf{31.71}    &\textbf{42.87}    &\textbf{31.84}    &\textbf{34.08}    &\textbf{29.65}    &\textbf{28.25}    &\textbf{36.06}    &\textbf{49.44}    &\textbf{27.78}\Bstrut \\
%     \Xhline{3\arrayrulewidth}
%     \end{tabular}%
%   }
% \label{tb:white_attack_wrn28}

% \end{table*}



% \begin{figure}[t]
% \centering
% \includegraphics[width=0.44\textwidth]{  Figures/ranJac_vs_swJac.pdf}
% \caption{ Accuracy ($\%$) of WRN34 model trained with JR having random projections, and informative projections respectively, under different number of $L_\infty$-PGD iteration steps on ~\CIFAR-10 test dataset. \textcolor{red}{(update needed)}} 
% \label{fig:randJac_vs_WDJac}
% \end{figure}







% \begin{figure*}[!t]
%     \begin{minipage}[t]{.3\textwidth}
%       \centering
%         \includegraphics[width=1\textwidth]{Figures/ranJac_vs_swJac.pdf}
%         \caption{ Accuracy ($\%$) of WRN34 model trained with JR having random projections, and informative projections respectively, under different number of $L_\infty$-PGD iteration steps on ~\CIFAR-10 test dataset.} 
%         \label{fig:randJac_vs_WDJac}
%   \end{minipage}%
%   \hfill
%     \begin{minipage}[t]{.60\textwidth}
%         \centering
%     % \includegraphics[width=11.8cm, height=3.8cm]{  Figures/main_cross_section.png}
%     \subfloat[\centering \small Without regularization]{{\includegraphics[width=3.0cm]{Figures/cross_decision/00375_xe_r13.0.png}}}%
%     \hfill
%     \subfloat[\centering \small JR with random projections]{{\includegraphics[width=3.0cm]{Figures/cross_decision/00375_rand_jac_r119.5.png} }}%
%     \hfill
%     \subfloat[\centering \small JR with informative projections]{{\includegraphics[width=3.0cm]{Figures/cross_decision/00375_sw_jac_r155.6.png} }}%
%     \hfill
%     \subfloat{{\includegraphics[ height=3.4cm]{Figures/cross_decision/cross_section_legend.png} }}%

%     \caption{ \textbf{Cross sections of decision boundaries in the input space}. To generate the cross section cells, a test sample (central black dot) and two chosen random vectors $\in \mathbb{R}^{32 \times 32 \times 3}$ (vertical and horizontal axes), which are then orthonormalized to form a cross section plane. Different colors indicate different classes in ~\CIFAR-10, whereas only closely related classes are shown. The contours are formed by softmax values, and the circles are drawn by the distance from test samples to the closest decision boundaries. (a) Without regularization, the decision cell is tiny and the model is vulnerable to perturbations. (b) Jacobian regularization with random projections increases the classification margins. And (c) Jacobian regularization with our informative projections further expand the decision boundaries, ensuring the stability of the trained model.} %
%     \label{fig:cross_section}
%     \end{minipage}%
%   \end{figure*}

% \begin{figure*}[t!]
%     \centering
%     % \includegraphics[width=11.8cm, height=3.8cm]{  Figures/main_cross_section.png}
%     \subfloat[\centering \small Without regularization]{{\includegraphics[width=3.6cm]{Figures/cross_decision/00375_xe_r13.0.png}}}%
%     \hfill
%     \subfloat[\centering \small JR with random projections]{{\includegraphics[width=3.6cm]{Figures/cross_decision/00375_rand_jac_r119.5.png} }}%
%     \hfill
%     \subfloat[\centering \small JR with informative projections]{{\includegraphics[width=3.6cm]{Figures/cross_decision/00375_sw_jac_r155.6.png} }}%
%     \hfill
%     \subfloat{{\includegraphics[ height=3.4cm]{Figures/cross_decision/cross_section_legend.png} }}%

%     \caption{ \textbf{Cross sections of decision boundaries in the input space}. To generate the cross section cells, a test sample (central black dot) and two chosen random vectors $\in \mathbb{R}^{32 \times 32 \times 3}$ (vertical and horizontal axes), which are then orthonormalized to form a cross section plane. Different colors indicate different classes in ~\CIFAR-10, whereas only closely related classes are shown. The contours are formed by softmax values, and the circles are drawn by the distance from test samples to the closest decision boundaries. (a) Without regularization, the decision cell is tiny and the model is vulnerable to perturbations. (b) Jacobian regularization with random projections increases the classification margins. And (c) Jacobian regularization with our informative projections further expand the decision boundaries, ensuring the stability of the trained model.} %
%     \label{fig:cross_section}
% \end{figure*}








% \subsection{Progression of input derivatives}
% \label{app:prog_input_der}
% Table~\ref{tb:prog_input_der} shows the progression of input derivatives after different PGD white-box attack iterations. We note that the perturbation noises created by the output loss derivatives increase when more attack iterations are applied. Nevertheless, our proposed framework maintains the smallest silence throughout the iterations. This experiment further proves our ~\SystemName\'s robustness performance.\\


\section{Activation Magnitude}
\label{app:magnitude_activation}
Figure ~\ref{fig:mag_activation_all} depicts the activation magnitudes at the penultimate layer of WRN34 across various AT frameworks. Although AT methods manage to bring the adversarial magnitudes closer to their clean counterparts, the magnitudes generally remain elevated, with PGD-AT being especially prominent. Through a balanced integration of the input Jacobian matrix and output distributions, our proposed method effectively mitigates the model's susceptibility to perturbed samples. \\

\begin{figure*}[t]
    \centering
    \subfloat[\centering $\mathcal{XE}$ ]{{\includegraphics[width=0.42\linewidth]{ Figures/magnitude_activation_xe_pgd20.pdf}}}%
    \hfill
    \subfloat[\centering TRADES ]{{\includegraphics[width=0.42\linewidth]{ Figures/magnitude_activation_trades_pgd20.pdf}}}%
    \hfill
    \subfloat[\centering ALP]{{\includegraphics[width=0.42\linewidth]{ Figures/magnitude_activation_alp_pgd20.pdf}}}%
    \hfill
    \subfloat[\centering PGD-AT]{{\includegraphics[width=0.42\linewidth]{ Figures/magnitude_activation_pgd_pgd20.pdf}}}%
    \hfill
    \subfloat[\centering SAT  ]{{\includegraphics[width=0.42\linewidth]{ Figures/magnitude_activation_sink_pgd20.pdf} }}%
    \hfill
    \subfloat[\centering~\SystemName~(\textit{Ours}) ]{{\includegraphics[width=0.42\linewidth]{ Figures/magnitude_activation_trans_swdj_pgd20.pdf}}}%
    \hfill
    \caption{\textbf{Magnitude of activation} at the penultimate layer for models trained with different defense methods. Our \SystemName~ can regulate adversarial samples' magnitudes similar to clean samples' while well suppressing both of them.}%
    \label{fig:mag_activation_all}
\end{figure*}

% \begin{table*}[t]
%     \centering
%     \subfloat[\centering $\mathcal{XE}$ ]{{\includegraphics[width=0.42\linewidth]{ Figures/magnitude_activation_xe_pgd20.pdf}}}%
%     \hfill
%     \subfloat[\centering TRADES ]{{\includegraphics[width=0.42\linewidth]{ Figures/magnitude_activation_trades_pgd20.pdf}}}%
%     \hfill
%     \subfloat[\centering ALP]{{\includegraphics[width=0.42\linewidth]{ Figures/magnitude_activation_alp_pgd20.pdf}}}%
%     \hfill
%     \subfloat[\centering PGD-AT]{{\includegraphics[width=0.42\linewidth]{ Figures/magnitude_activation_pgd_pgd20.pdf}}}%
%     \hfill
%     \subfloat[\centering SAT  ]{{\includegraphics[width=0.42\linewidth]{ Figures/magnitude_activation_sink_pgd20.pdf} }}%
%     \hfill
%     \subfloat[\centering~\SystemName~(\textit{Ours}) ]{{\includegraphics[width=0.42\linewidth]{ Figures/magnitude_activation_trans_swdj_pgd20.pdf}}}%
%     \hfill
%     \caption{\textbf{Magnitude of activation} at the penultimate layer for models trained with different defense methods. Our \SystemName~ can regulate adversarial samples' magnitudes similar to clean samples' while well suppressing both of them.}%
%     \label{fig:abs}
% \end{table*}

% \begin{comment}
% \begin{figure*}[th!]
% \centering
% \includegraphics[width=0.8\linewidth]{ Figures/SAT_vs_ours_model_der.pdf}
% \caption{\textbf{Average gradient norm ratios between adversarial and clean samples.} The ratios of $\mathbb{E}(||\nabla_{\theta_i}\mathcal{L}(\tilde{x})|| / ||\nabla_{\theta_i}\mathcal{L}({x})||)$ with respect to the model's parameters $\theta_{i}$ on ~\CIFAR-10 dataset. While both methods can achieve similar variance at middle layers, our~\SystemName~ successfully learns to balance clean and adversarial samples, shown by ratios' values mainly close to 1, and obtains smaller ratios at beginning layers, which benefits for constraining generating perturbation.}
% \label{fig:model_der_sat_vs_ours}
% \end{figure*}
% \begin{figure}[t]
% \centering
% \includegraphics[width=0.80\linewidth]{ Figures/SAT_vs_ours_input_der.pdf}
% \caption{\textbf{Magnitude of $||\nabla_{x} \mathcal{L}_{\mathcal{XE}}||_1$} for WRN34 trained with SAT and our ~\SystemName. The red and green-filled areas range from minimum to maximum values of each sample. Our~\SystemName~can mostly achieve lower derivative's norm across samples, which means constraint perturbation's magnitude added to clean samples for attacking the model.} 
% \label{fig:input_der_sat_vs_ours}
% \end{figure}

% \subsection{~\SystemName~ vs. Sinkhorn Divergence}
% \label{app:sw-jac_vs_sink}
% Similar to \cref{subsec:exp_analysis}, we describe the differences between our proposed~\SystemName~and SAT in terms of the defense model's gradients at different layers.  At every intermediate layer in an attacked model, our framework provides a smaller ratio between perturbed and clean sample's gradients than SAT training as shown in \ref{fig:model_der_sat_vs_ours}. It is worth noting that the gradients are derived on unobserved samples in the test set. Therefore, in the forward path, perturbations in images have fewer effects on the model's prediction, indicated by smaller ratios at most of the layers.  Moreover, in the backward path, since the abused model's gradients are deployed to generate more perturbations, the model achieves better robustness when it can produce smaller gradients with respect to its inputs. Additionally, our proposed ~\SystemName~ successfully balances the adversarial samples and the clean samples, showing ratios' values in most of the layers closed to 1. Finally, \ref{fig:input_der_sat_vs_ours} more justify our framework's benefits while it can remarkably suppress the input gradient compared to barely using Sinkhorn divergence on the entire ~\CIFAR-10 test set.
% \end{comment}

% \subsection{Gallery of Cross Decision Cells}
% This section provides more illustrations about the benefit of optimal Jacobian regularization in increasing decision boundaries for a defense model. As shown in Fig. \ref{fig:more_cross_section}, models trained without this regularizer are notably susceptible to perturbations. However, integrating the Jacobian regularizer augments robustness by broadening the decision boundaries, evidenced by an \textbf{enlarged black circle}. Our optimal Jacobian regularizer further extends the decision boundaries, amplifying model resilience. The rationale behind this enhancement lies in the informative directions showcased in Fig.\ref{fig:opt_move_toy}, guiding the model to achieve optimal projections within the input-output Jacobian regularization framework. 


\section{Broader Impact}
Utilizing machine learning models in real-world systems necessitates not only high accuracy but also robustness across diverse environmental scenarios. The central motivation of this study is to devise a training framework that augments the robustness of Deep Neural Network (DNN) models in the face of various adversarial attacks, encompassing both white-box and black-box methodologies. To realize this objective, we introduce the \SystemName~framework, an innovative approach that refines traditional Jacobian regularization techniques and aligns output distributions. This research represents a significant stride in synergizing adversarial training with input-output Jacobian regularization—a combination hitherto underexplored—to {construct} a more resilient model.
% \clearpage
% \onecolumn

% \label{app:gallery_decision}
% \begin{figure*}[h!]
%     \centering
%     \subfloat{{\includegraphics[width= 4.5cm]{ Figures/cross_decision/00232_xe_r19.1.png}}}%
%     \quad
%     \subfloat{{\includegraphics[width= 4.5cm]{ Figures/cross_decision/00232_rand_jac_r55.3.png} }}%
%     \quad
%     \subfloat{{\includegraphics[width= 4.5cm]{ Figures/cross_decision/00232_sw_jac_r94.9.png} }}%
%     \quad
%     \subfloat{{\includegraphics[width= 4.5cm]{ Figures/cross_decision/00396_xe_r8.1.png}}}%
%     \quad
%     \subfloat{{\includegraphics[width= 4.5cm]{ Figures/cross_decision/00396_rand_jac_r87.0.png} }}%
%     \quad
%     \subfloat{{\includegraphics[width= 4.5cm]{ Figures/cross_decision/00396_sw_jac_r125.2.png} }}%
%     \quad
%     \subfloat{{\includegraphics[width= 4.5cm]{ Figures/cross_decision/00339_xe_r27.9.png}}}%
%     \quad
%     \subfloat{{\includegraphics[width= 4.5cm]{ Figures/cross_decision/00339_rand_jac_r50.8.png} }}%
%     \quad
%     \subfloat{{\includegraphics[width= 4.5cm]{ Figures/cross_decision/00339_sw_jac_r128.3.png} }}%
%     \quad
%     \subfloat{{\includegraphics[width= 4.5cm]{ Figures/cross_decision/00386_xe_r31.6.png}}}%
%     \quad
%     \subfloat{{\includegraphics[width= 4.5cm]{ Figures/cross_decision/00386_rand_jac_r147.7.png} }}%
%     \quad
%     \subfloat{{\includegraphics[width= 4.5cm]{ Figures/cross_decision/00386_sw_jac_r175.0.png} }}%
%     \caption{\textbf{Cross sections of decision boundaries in the input space}. \textbf{Left column:} Model trained without regularization. \textbf{Middle column:} Model train with Jacobian regularization having random projection. \textbf{Right column:} Model trained with Jacobian regularization having informative projections. Our optimal JR benefits from the informative directions obtained by SW distance, and thus helps the model to regularize these sensitive direction of clean samples and produce larger decision cells.} %
%     \label{fig:more_cross_section}
% \end{figure*}


\clearpage
