% \section{Reproducibility of Our Work}
% For reproducibility of the results presented in the paper, we share and release our source code here\footnote{https://anonymous.4open.science/r/OTJR-2024}. Upon acceptance of the paper, we plan to release the entire code. In addition, we provide more experimental results in the Appendix.


\section{Conclusion}
% We are the first to theoretically and empirically show and analyze the impacts of AT and Jacobian regularization on the robustness of DNN models, to defend against adversarial attacks. 
% In the intricate tapestry of the Web, where deep learning drives myriad services, the integrity of information is paramount. 
In the realm of deep learning, which powers a vast array of applications, maintaining the integrity of information is paramount.
In this dynamic, our research stands as the inaugural comprehensive exploration into the interplay between Adversarial Training (AT) and Jacobian regularization, especially in bolstering the robustness of Deep Neural Networks (DNNs) against adversarial forays. We show that the AT pays more attention to meaningful input samples' pixels, whereas the Jacobian regularizer agnostically silences the DNN's gradients under any perturbation from its output to input layers. Based on these characterizations, we effectively augment the AT framework by integrating input-output Jacobian matrix in order to more effectively improve the DNN's robustness. Using the optimal transport theory, our work is the first to jointly minimize the difference between the distributions of original and adversarial samples with much faster convergence. Also, the proposed SW distance produces the optimal projections for the Jacobian regularization, which can further increase the decision boundaries of a sample under perturbations, and achieves much higher performance through optimizing the best of both worlds. Our rigorous empirical evaluations, pitted against four state-of-the-art defense mechanisms across both controlled and real-world datasets, underscore the supremacy of our methodology.
% With extensive experiments comparing with four SOTA defense methods on both controlled and real-world datasets, we show that our approach outperform others. 
