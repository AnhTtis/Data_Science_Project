
\section{Our Approach}
Our approach explores the Sliced Wasserstein  distance in order to push the adversarial distribution closer to the natural distribution with a faster convergence rate. Next, \textit{\textbf{the Sliced Wasserstein distance results in optimal movement directions to sufficiently minimize the spectrum of input-output Jacobian matrix.}}

% \begin{algorithm}[!t]
% \caption{\SystemName: AT with SW and optimal Jacobian regularization.}
% \begin{algorithmic}[!b]
% \Require   DNN $f$
% parameterized by $\theta$, training dataset $\mathcal{D}$. Number of projections $K$. Maximum perturbation $\epsilon$, step size $\eta$, number of adversarial iteration $P$. Loss' hyper-parameters $\lambda_J$ and $\lambda_{SW}$. Learning rate $\alpha$ and batch size of $\mathcal{B}$.
%     \While{not converged}
%         \For{$\{(x_{i},\textrm{ } y_{i})\}_{\mathcal{B}} \in \mathcal{D}$}
%             \State $\nu :  = \{z_{i}\}_{\mathcal{B}} = f(x_i; {\theta})| _{i=1,..,\mathcal{B}}$ 
%             \State  \texttt{\# Generate adv. samples}
%             \For{iteration $t \gets 1$ to $P$}
%                 \State $\tilde{x_i} = \Pi_{x}^{\epsilon}(\tilde{x_i} +\eta \cdot \text{sgn} (\nabla _{\tilde{x_i}} \mathcal{L}(\tilde{x_i}, y_i)))| _{i=1,..,\mathcal{B}}$  
%             \EndFor
%             \State $\mu :  = \{\tilde{z_i}\}_{\mathcal{B}} = f(\tilde{x_i}; {\theta})| _{i=1,..,\mathcal{B}}$  
            
%             \State $SW \gets 0$ 
%             \State $\sigma_i \gets 0|_{i=1,..,\mathcal{B}}$ 
%             \For{iteration $k \gets 1$ to $K$}
%                 \State \texttt{\# Sample $\hat{v}_k$ from $\mathcal{S}^{C-1}$}           \State $\hat{v}_k$ $\gets$  $\mathcal{U}(\mathcal{S}^{C-1})$ 
%                 \State \texttt{\# Add SW under projection $\hat{v}_k$}
%                 \State $SW \gets SW + \psi  \left(\tau_1 \circ \mathcal{R}_{(t,\hat{v}_k)}\mu, \tau_2 \circ\mathcal{R}_{(t,\hat{v}_k)}\nu \right)$ 
%                 \State \texttt{\# Optimal movements}
%                 % \State \texttt{\# Calculate samples' movements under $\hat{v}_k$}
%                 \State $m_k \gets \left(\tau_1^{-1} \circ \tau_2 \circ \mathcal{R}_{(t,\hat{v}_k)}\nu - \mathcal{R}_{(t,\hat{v}_k)}\mu \right) \otimes \hat{v}_k$ 
%                 \State $\sigma_i \gets \sigma_i + m_{k,i}|_{i=1,..,\mathcal{B}}$
%             \EndFor
%             \State $\sigma_i \gets \sigma_i / ||\sigma_i||_2 $  $ |_{i=1,..,\mathcal{B}}$
%             \State $\mathcal{L} \gets \sum^{\mathcal{B}} \left( \mathcal{L_{XE}}(\tilde{x_i}, y_i) + \lambda_J ||J(x_i | \sigma_i)||_{F}^{2} \right) $  \\\hspace{3cm}  $ +\quad \lambda_{SW}SW(\mu, \nu) $
%             \State $\theta \gets \theta - \alpha \cdot \nabla _{\theta} \mathcal{L}$ 
%         \EndFor
%     \EndWhile
% \end{algorithmic}

% \label{alg:opt_move}
% \end{algorithm}

\begin{figure}[!b]
    \centering
    \includegraphics[width=0.42\textwidth]{ Figures/optimal_movement.png}
    \caption{ Illustration of optimal latent trajectories. \textbf{Top row} ((a) \& (b)): Random movement directions  (green arrows), which is non-informative, uniformly sampled from two-dimensional unit sphere $\mathcal{S}^{1}$. \textbf{Bottom row} ((c) \& (d)): The optimal trajectories from the SW distance between source distribution (\textcolor{green!65!black}{green}) and target distribution (\textcolor{orange}{orange}) obtained from ~Eq.~\ref{eqn:opt_move}.}
    \label{fig:opt_move_toy}
\end{figure}

\begin{table*}[!t]
\centering
\caption{\textbf{Classification accuracy (\% ) under \textit{white-box}, \textit{black-box} attacks and \textit{AutoAttack}.} Different defense methods trained on~\CIFAR-10 and~\CIFAR-100 datasets using WRN34 in 100 epochs, except that SAT$^{400}$ was trained in 400 epochs.  Each experiment was conducted three times to ensure reliability. Standard deviations are denoted with subscripts. \underline{Numbers underlined} represent the greatest reduction in performance on clean images to trade-off for robustness. Best performances are highlighted in bold. }
\resizebox{0.95\textwidth}{!}{%
    \begin{tabular}{c | l | c | c c c c c c c | c c | c } 
    \Xhline{3\arrayrulewidth}
    Dataset & Defense    &\textit{Clean}    &PGD$^{20}$    &PGD$^{100}$    &$L_{2}$-PGD       &MIM &FGSM   &CW     &FAB   &Square & SimBa  &\emph{AutoAtt}\Tstrut\Bstrut\\
    \Xhline{2\arrayrulewidth}
    \multirow{8}{*}{\rotatebox[origin=c]{90}{\centering \small{~\CIFAR-10}}}    
&TRADES & \textit{84.71}$_{.15}$ & 54.23$_{.07}$ & 53.91$_{.11}$ & 61.04$_{.21}$ & 54.13$_{.11}$ & 60.46$_{.15}$ & 53.17$_{.22}$ & 53.65$_{.39}$ & 62.25$_{.26}$ & 70.66$_{.52}$ & 52.06$_{.15}$ \Tstrut\\
&ALP & \textit{86.63}$_{.23}$ & 46.99$_{.25}$ & 46.48$_{.25}$ & 55.69$_{.33}$ & 46.88$_{.23}$ & 58.14$_{.21}$ & 47.50$_{.30}$ & \textbf{55.61}$_{.27}$ & 59.26$_{.28}$ & 68.28$_{.31}$ & 46.28$_{.25}$ \\
&PGD-AT & \textit{86.54}$_{.20}$ & 46.67$_{.11}$ & 46.23$_{.12}$ & 55.93$_{.10}$ & 46.66$_{.09}$ & 56.83$_{.23}$ & 47.56$_{.15}$ & 48.39$_{.22}$ & 58.92$_{.12}$ & 68.66$_{.14}$ & 45.77$_{.07}$ \\
&SAT & \textit{\underline{83.19}}$_{.33}$ & 53.52$_{.15}$ & 53.23$_{.23}$ & 60.37$_{.06}$ & 52.46$_{1.34}$ & 60.36$_{.13}$ & 52.15$_{.20}$ & 52.54$_{.36}$ & 61.50$_{.19}$ & 69.70$_{.29}$ & 50.73$_{.20}$ \Bstrut\\
    \cline{2-13}
    &\textit{Random JR}   & \textit{84.99}$_{.14}$ & 22.67$_{.15}$ & 21.89$_{.14}$ & 60.98$_{.19}$ & 22.49$_{.18}$ & 32.99$_{.21}$ & 22.00$_{.06}$ & 21.74$_{.11}$ & 45.86$_{.20}$ & 71.20$_{.31}$ & 20.54$_{.14}$ \Tstrut\\
&\textit{SW}  &  \textit{84.26}$_{.80}$ & 54.51$_{.33}$ & 54.20$_{.37}$ & 61.08$_{.12}$ & 54.46$_{.27}$ & 61.32$_{.12}$ & 53.78$_{.05}$ & 54.90$_{.29}$ & 62.95$_{.43}$ & 70.74$_{.65}$ & 51.97$_{.06}$ \\
    &\cellcolor{backcolour}\SystemName~(\emph{\textbf{ours}}) & \cellcolor{backcolour}\textit{84.01}$_{.53}$ & \cellcolor{backcolour}\textbf{55.38}$_{.29}$ & \cellcolor{backcolour}\textbf{55.08}$_{.36}$ & \cellcolor{backcolour}\textbf{63.87}$_{.09}$ & \cellcolor{backcolour}\textbf{55.31}$_{.29}$ & \cellcolor{backcolour}\textbf{61.03}$_{.18}$ & \cellcolor{backcolour}\textbf{54.09}$_{.12}$ & \cellcolor{backcolour}{54.17}$_{.07}$ & \cellcolor{backcolour}\textbf{63.11}$_{.21}$ & \cellcolor{backcolour}\textbf{72.04}$_{.68}$ & \cellcolor{backcolour}\textbf{52.57}$_{.12}$ \Bstrut \\
    \Xhline{3\arrayrulewidth}
    \multirow{8}{*}{\rotatebox[origin=c]{90}{\centering \small{~\CIFAR-100}}}    
    &TRADES   & 57.46$_{.16}$ & 30.42$_{.05}$ & 30.36$_{.09}$ & 35.85$_{.10}$ & 30.37$_{.08}$ & 33.04$_{.16}$ & 27.97$_{.13}$ & 27.93$_{.15}$ & 33.70$_{.06}$ & 44.28$_{.14}$ & 27.15$_{.09}$\Tstrut\\
&ALP    & \textit{60.61}$_{.05}$ & 26.23$_{.18}$ & 25.87$_{.17}$ & 33.75$_{.09}$ & 26.14$_{.07}$ & 31.40$_{.08}$ & 25.78$_{.22}$ & 25.69$_{.15}$ & 33.09$_{.15}$ & 43.25$_{.22}$ & 24.57$_{.22}$ \\
&PGD-AT &  \textit{59.7}7$_{.21}$ & 24.05$_{.03}$ & 23.74$_{.03}$ & 31.41$_{.16}$ & 24.01$_{.06}$ & 29.21$_{.18}$ & 24.67$_{.04}$ & 24.47$_{.06}$ & 31.47$_{.11}$ & 41.15$_{.21}$ & 23.28$_{.01}$ \\
&SAT$^{400}$    & \textit{\underline{53.61}}$_{.52}$ & 26.63$_{.14}$ & 26.42$_{.16}$ & 32.34$_{.25}$ & 26.57$_{.19}$ & 31.04$_{.21}$ & 25.22$_{.06}$ & 26.63$_{.11}$ & 31.32$_{.34}$ & 39.64$_{.90}$ & 24.32$_{.05}$ \Bstrut\\ 
    \cline{2-13}
    &\textit{Random JR}   & \textit{66.58}$_{.17}$ & 9.41$_{.44}$ & 8.87$_{.42}$ & 37.79$_{.31}$ & 9.27$_{.49}$ & 16.38$_{.33}$ & 10.27$_{.45}$ & 9.26$_{.23}$ & 23.53$_{.15}$ & 48.86$_{.55}$ & 8.10$_{.57}$ \Tstrut\\
&\textit{SW}     & \textit{57.69}$_{.28}$ & 26.01$_{.16}$ & 25.82$_{.24}$ & 31.37$_{.17}$ & 25.97$_{.18}$ & 30.78$_{.34}$ & 25.48$_{.23}$ & 26.11$_{.07}$ & 31.34$_{.26}$ & 40.68$_{.43}$ & 24.35$_{.29}$ \\
    &\cellcolor{backcolour}\SystemName~(\emph{\textbf{ours}}) & \cellcolor{backcolour}\textit{58.20}$_{.13}$ & \cellcolor{backcolour}\textbf{32.11}$_{.21}$ & \cellcolor{backcolour}\textbf{32.01}$_{.18}$ & \cellcolor{backcolour}\textbf{43.13}$_{.12}$ & \cellcolor{backcolour}\textbf{32.07}$_{.19}$ & \cellcolor{backcolour}\textbf{34.26}$_{.30}$ & \cellcolor{backcolour}\textbf{29.71}$_{.06}$ & \cellcolor{backcolour}\textbf{29.24}$_{.08}$ & \cellcolor{backcolour}\textbf{36.27}$_{.05}$ & \cellcolor{backcolour}\textbf{49.92}$_{.23}$ & \cellcolor{backcolour}\textbf{28.36}$_{.10}$ \Bstrut \\
    \Xhline{3\arrayrulewidth}
    \end{tabular}%
}
\label{tb:white_attack}
\end{table*}

\subsection{Sliced Wasserstein Distance} The \emph{p}-Wasserstein distance between two probability distributions $\mu$ and $\nu$ \cite{villani2008optimal} in a general \emph{d}-dimensional space $\Omega$, to search for an optimal transportation cost between $\mu$ and $\nu$, is defined as follows:
\begin{equation}
\label{eqn:p-wasseitern}
\small
    W_p(P_{\mu},P_{\nu}) = \Big(\inf_{\pi \in \Pi(\mu, \nu)}  \int_{\Omega \times \Omega}\psi(x,y)^p d\pi(x,y) \Big)^{1/p},
\end{equation}
where $\psi: \Omega \times \Omega \rightarrow \mathbb{R}^{+}$ is a transportation cost function, and $\Pi(\mu, \nu)$ is a collection of all possible transportation plans. The Sliced \emph{p}-Wasserstein distance ($SW_p$), which is inspired by the $W_p$ in one-dimensional, calculates the \emph{p}-Wasserstein distance by projecting $\mu$ and $\nu$ onto multiple one-dimensional marginal distributions using Radon transform \cite{helgason2010integral}. The $SW_p$ is defined as follows:
\begin{equation}
\small
    SW_{p}(\mu, \nu) = \int_{\mathcal{S}^{d-1}}W_{p}(\mathcal{R}_{(t,\hat{v})}\mu, \mathcal{R}_{(t,\hat{v})}\nu) d\hat{v},
\end{equation}
where $\mathcal{R}_{(t,\hat{v})}\mu$ is the Radon transform as follows:
\begin{equation}
\small
    \mathcal{R}_{(t,\hat{v})}\mu = \int_{\Omega} \mu (x) \sigma (t-\langle \hat{v}, x \rangle) dx, \forall \hat{v} \in \mathcal{S}^{d-1}, \forall t \in \mathbb{R},
\end{equation}
% \begin{figure}{r}
%     \centering
%     \includegraphics[width=0.38\textwidth]{ Figures/optimal_movement.png}
%     \caption{ Illustration of optimal movement directions. \textbf{Top row} ((a) \& (b)): Random movement directions  (green arrows), which is non-informative, uniformly sampled from two-dimensional unit sphere $\mathcal{S}^{1}$. \textbf{Bottom row} ((c) \& (d)): The optimal movement directions from the SW distance between source distribution (\textcolor{green!65!black}{green}) and target distribution (\textcolor{orange}{orange}) obtained from ~Eq.~\ref{eqn:opt_move}.}
%     \label{fig:opt_move_toy}
% \end{figure}
where $\langle \cdot, \cdot \rangle$ denote the Euclidean inner product, and $\sigma$ is the Dirac delta function.

Next, let $\mathcal{B}$ denote the size of a mini-batch of samples, and $C$ denote the number of classes. To calculate the transportation cost between adversarial samples' representations $\mu :  = \{\tilde{z}_{i}\}_{\mathcal{B}}$ and the corresponding original samples' representations $\nu :  = \{z_i\}_{\mathcal{B}}$, the integration of $\hat{v}$ over the unit sphere $\mathcal{S}^{C-1}$ is approximated via Monte Carlo method with $K$ uniformly sampled random vector $\hat{v}_i \in \mathcal{S}^{C-1}$. In particular, $\mathcal{R}_{(t,\hat{v})}\mu$ and $\mathcal{R}_{(t,\hat{v})}\nu$ are sorted in ascending order using two permutation operators $\tau_1$  and $\tau_2$, respectively, and the approximation of Sliced \textit{1}-Wasserstein is expressed as follows:
\begin{equation}
\label{eqn:swd_approx}
\small
    SW(\mu, \nu) \simeq  \sum_{k=1}^{K}  \psi  \left(\tau_1 \circ \mathcal{R}_{(t,\hat{v}_k)}\mu, \tau_2 \circ\mathcal{R}_{(t,\hat{v}_k)}\nu \right).
\end{equation}

 \subsection{Optimal Latent Trajectory} Using~Eq.~\ref{eqn:swd_approx}, we can straightforwardly compute the trajectory of each $\tilde{z}_{i}$ in the latent space $\mathbb{R}^{C}$ in order to minimize the $SW(\mu, \nu)$. We refer to the directions of these movements as \textit{optimal latent trajectories}, because the SW distance produces the lowest transportation cost between the source and target distributions. Particularly, let the trajectories of $\{\tilde{z}_i \}_{\mathcal{B}}$ in each single projection $\hat{v}_k$ be:
\begin{equation}
    m_k = (\tau_1^{-1} \circ \tau_2 \circ \mathcal{R}_{(t,\hat{v}_k)}\nu - \mathcal{R}_{(t,\hat{v}_k)}\mu ) \otimes \hat{v}_k,
\end{equation}
where $\otimes$ denotes the outer product. Then, the overall optimal trajectory direction of each $\tilde{z}_{i}$ is expressed as follows:
\begin{equation}
\label{eqn:opt_move}
    \sigma_{i} =  \frac{\sum_{k=1}^{K} m_{k,i}}{||\sum_{k=1}^{K} m_{k,i}||_2}.
\end{equation}

% \begin{figure}[t!]
%     \centering
%     \includegraphics[width=2.0in]{  Figures/optimal_movement.png}
%     % \subfloat[]{{\includegraphics[width=3.4cm]{Figures/movement_rand.circles.pdf}}}%
%     % % \qquad
%     % \subfloat[]{{\includegraphics[width=3.4cm]{Figures/movement_rand.moon.pdf}}}%
%     % % \dotfill\par
%     % \quad
%     % \subfloat[]{{\includegraphics[width=3.4cm]{Figures/movement_dir.circles.pdf}}}%
%     % % \qquad
%     % \subfloat[]{{\includegraphics[width=3.4cm]{Figures/movement_dir.moon.pdf} }}%
%     \caption{Illustration of optimal movement directions. \textbf{Top row} ((a) \& (b)): Random movement directions  (green arrows), which is non-informative, uniformly sampled from two-dimensional unit sphere $\mathcal{S}^{1}$. \textbf{Bottom row} ((c) \& (d)): The optimal movement directions from SW distance between source distribution (\textcolor{green!25!black}{green}) and target distribution (\textcolor{orange}{orange}) obtained from ~Eq.~\ref{eqn:opt_move}. The movement directions are passed into the Jacobian regularization on clean samples in ~Eq.~\ref{eqn:opt_jac}.}
%     \label{fig:opt_move}
%     \vspace{-10pt}
% \end{figure}
A demonstrative example is provided in Figure~\ref{fig:opt_move_toy} to illustrate the concept of optimal trajectories. This example highlights the trajectory direction of an adversarial sample, denoted as $\tilde{x}$, within the latent space. In particular, this direction is significant, as it represents the most sensitive axis of perturbation for $x$ when exposed to adversarial interference. To address this, our approach diverges from the traditional method of random projection, as previously proposed by Hoffman et al. (2019) \cite{hoffman2019robust}. Instead, we propose to regularize the Jacobian matrix of $x$ specifically along this identified optimal trajectory. Utilizing Eq.~\ref{eqn:one_jac}, we are able to reformulate the input-output Jacobian regularization, incorporating these strategically derived projections for each sample.  The formula for this new regularization approach is presented below:  
  % For illustration, we present a toy example of optimal trajectories obtained by~Eq.~\ref{eqn:opt_move} in Fig.~\ref{fig:opt_move_toy}. The trajectory direction of an adversarial sample $\tilde{x}$ in the latent space indicates the most sensitive direction of $x$ under adversarial perturbations. Therefore, we aim to regularize the Jacobian matrix of $x$ under this optimal direction, in contrast to a random projection as was proposed in \cite{hoffman2019robust}. Using ~Eq.~\ref{eqn:one_jac}, we can rewrite the optimal input-output Jacobian regularization with previously obtained informative projections for a sample as follows:
\begin{equation}
\label{eqn:opt_jac}
    ||J(x_i | \sigma_i)||_{F}^{2} \simeq \left[ \frac{\partial (\sigma_i \cdot z_i)}{\partial x_i}\right]^{2}.
\end{equation}

Then, our overall loss function for a batch of samples $\{(x_i, y_i)\}_{\mathcal{B}}$ is expressed in the following way:
\begin{equation}
\label{eqn:overall}
 \mathcal{L} = \sum^{\mathcal{B}}_{i=1} \left( \mathcal{L_{\text{AT}}}(\tilde{x_i}, y_{i}) + \lambda_{J} ||J(x_i | \sigma_i)||_{F}^{2} \right) + \lambda_{SW}SW(\mu, \nu), 
\end{equation}
where $\mathcal{L}_{\text{AT}}$, unless stated otherwise, is cross-entropy loss of adversarial samples. In practice, sampling $K$ uniform vectors $\hat{v}_k$ can be performed simultaneously thanks to deep learning libraries. Then, the calculation of random projections and optimal trajectory steps can be vectorized and performed simultaneously.




  
