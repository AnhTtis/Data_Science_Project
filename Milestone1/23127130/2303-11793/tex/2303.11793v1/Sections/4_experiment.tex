
\section{Experimental Results}
\label{sec:experiment}
%To evaluate our approach, we conducted various adversarial attacks on different datasets and two backbone networks. 
%  We first compare our proposed~\SystemName~with well-known AT baseline frameworks. Next, we show the compatibility of our \SystemName\ with different networks and high-quality datasets. Finally, we demonstrate how our \SystemName\ helps a defense model to boost its adversarial robustness via extensive ablation studies.



\subsection{Experiment Settings} 
 We employ WideResNet34-10 \cite{zagoruyko2016wide} as our backbone architecture on two datasets~\CIFAR-10 and~\CIFAR-100 \cite{krizhevsky2009learning}. The model are trained for 100 epochs with the momentum SGD \cite{qian1999momentum} optimizer, whereas its initial learning rate is set to 0.1 and decayed by a factor of $10$ at $75^{th}$ and $90^{th}$ epoch, respectively. Adversarial samples in the training phase are generated by $L_\infty$-PGD \cite{madry2017towards} in 10 iterations with the maximal perturbation $\epsilon=8 /255$ and the perturbation step size $\eta=2/255$. For a fair comparison with different approaches \cite{pang2020bag}, we use the above settings throughout our experiments without early stopping or modifying models' architecture, and report their performances on the last epoch. For our~\SystemName~based defense models, we use the following hyper-parameter settings: $\{K=32, \lambda_J =0.002, \lambda_{SW}=64 \}$ and $\{K=128, \lambda_J =0.001, \lambda_{SW}=64 \}$ for~\CIFAR-10 and~\CIFAR-100, respectively. An ablation study of the hyper-parameters' impact on model performance is provided in \textit{Supp. Material}. In addition, we perform the basic sanity tests \cite{carlini2019evaluating} in \textit{Supp. Material} to ensure that our proposed~\SystemName~does not rely on gradient obfuscation. Overall, we compare our method with four different recent SOTA AT frameworks: TRADES~\cite{zhang2019theoretically},  ALP~\cite{kannan2018adversarial}, PGD-AT~\cite{madry2017towards}, and SAT~\cite{bouniot2021optimal}. The experiments are conducted on one GeForce RTX 3090 24GB GPU with Intel Xeon Gold 6230R CPU @ 2.10GHz.

 

\subsection{Adversarial Attacks}
\label{sec:adv_attacks}

 Follow \citeauthor{zhang2019theoretically}, we evaluate defense methods under a wide range \textit{white-box} attacks (20 iterations)\footnote{\url{https://github.com/Harry24k/adversarial-attacks-pytorch.git}}:
FGSM \cite{goodfellow2014explaining}, PGD \cite{madry2017towards}, MIM \cite{dong2018boosting}, CW$_{\infty}$ \cite{carlini2017towards}, DeepFool \cite{moosavi2016deepfool},  and FAB \cite{croce2020minimally}; and a \textit{black-box} attacks (1000 iterations): Square \cite{andriushchenko2020square}. We use the same parameters as \citeauthor{rice2020overfitting} for our experiments: for the $L_{\infty}$ threat model, the values of epsilon and step size are $8/255$ and  $2/255$ for CIFAR-10 and CIFAR-100, respectively. For the $L_2$ threat model, the values of epsilon and step size are 128/255 and 15/255 for all datasets. Additionally, we include AutoAttack \cite{croce2020reliable} which is a reliable adversarial evaluation framework and an ensemble of four diverse attacks: APGD-CE, APGD-DLR \cite{croce2020reliable}, FAB \cite{croce2020minimally}, and Square \cite{andriushchenko2020square}.
The results of this experiment are presented in~Table~\ref{tb:white_attack}, where the best results are highlighted in bold. As shown, our proposed~\SystemName~method demonstrates its superior performance across different attack methods. The improvement is considerable, by more than $1.0\%$ and $2.3\%$ on average compared to other methods on~\CIFAR-10 and~\CIFAR-100, respectively. In addition, we include two primary components of our proposed~\SystemName, \textit{i.e.,} SW and random JR  in~Table~\ref{tb:white_attack}. While the random JR, as expected, is highly vulnerable to most of the \textit{white-box} attacks due to its adversarial-agnostic defense strategy, the SW approach is on par with other AT methods.

% \begin{figure}[t]
%     \centering
%     \subfloat[\centering ~\CIFAR-10]{{\includegraphics[width=0.46\textwidth]{  Figures/pgd_wrn28_~\CIFAR-10.pdf} }}%
%     % \qquad
%     \hfill
%     \subfloat[\centering ~\CIFAR-100]{{\includegraphics[width=0.46\textwidth]{  Figures/pgd_wrn28_~\CIFAR-100.pdf} }}%
%     \caption{  Classification accuracy (\%) of defensive WRN28 models under different number of $L_{\infty}$-PGD iterations.}%
%     \label{fig:pgd_wrn28}
% \end{figure}

\begin{table}[!t]
     \centering
     \resizebox{0.38\textwidth}{!}{%
     \begin{tabular}{l | c  c | c  c } 
    \Xhline{3\arrayrulewidth}
    \multirow{2}{*}{\centering Loss}   &\multicolumn{2}{c|}{AWP}   &\multicolumn{2}{c}{LBGAT} \\
    & \textit{Clean} & AutoAtt & \textit{Clean} & AutoAtt \\
    \Xhline{2\arrayrulewidth}
    TRADES &\textit{60.17} & 28.80 & \textit{60.43}   &29.34\Tstrut\\
    \SystemName &\textit{\textbf{60.55}} & \textbf{29.79} & \textit{\textbf{62.15}}   &\textbf{29.64}\Bstrut\\
    \Xhline{3\arrayrulewidth}
    \end{tabular}%
    }
    
    \caption{Classification accuracy ($\%$) from defense losses integrated AWP and LBGAT.  Best results are in \textbf{bold}.} 
    \label{tb:proxy_framwork}
   
  \end{table}

\begin{table}[t]
\centering
\resizebox{0.42\textwidth}{!}{%
\setlength{\tabcolsep}{0.5em}
{\renewcommand{\arraystretch}{1.1}
    \begin{tabular}{ l | c | c c | c | c } 
    \Xhline{3\arrayrulewidth}
    {Defense}  &{\textit{Clean}}   &MIM   &CW &Square  & AutoAtt \\
    \Xhline{2\arrayrulewidth}
    \multicolumn{6}{c}{~\ImageNet100 - PreActResNet18}\\
    \hline
    TRADES    &\textit{62.64}    &30.66   &56.95    &46.52    &24.86 \\
    SAT$^{400}$ &\textit{45.08}    &29.08   &41.56    &36.14    &23.96 \\
    {\SystemName}   &\textit{61.97}     &\textbf{33.20}    &\textbf{57.42}    &\textbf{49.00}    &\textbf{27.54}\\
    \Xhline{2\arrayrulewidth}
    \multicolumn{6}{c}{\Intel - ResNet50}\\
    \hline
    TRADES    &\textit{54.43}    &3.73   &{2.47}    &5.57     &0.07\\
    SAT &\textit{50.17}    &5.93   &4.73  &6.77     &1.60\\
    {\SystemName} &\textit{57.80}    &\textbf{8.27}  &\textbf{6.77} &\textbf{9.37}    &\textbf{3.74}\\
    \Xhline{3\arrayrulewidth}
    \end{tabular}%
    }
}
\caption{Classification accuracy ($\%$) of PreActResNet18 and ResNet50 with~\ImageNet100 and~\Intel\ dataset, respectively,  under \textit{white-} and \textit{black-box} attacks.  Best results are in \textbf{bold}.} 
\label{tb:appli_attack}

\end{table}




 \subsection{Compatibility with different frameworks and datasets.}
 \label{sec:compatibility}

  \textbf{Frameworks with surrogate models}. AWP \cite{wu2020adversarial} and LBGAT \cite{cui2021learnable}  are two SOTA benchmarks boosting adversarial robustness using surrogate models during training, albeit rather computationally expensive. Furthermore, they utilized existing AT losses, such as TRADES. We integrate our proposed optimization loss into AWP and LBGAT, respectively, and present its performance at the \textit{final training epoch}. We selected TRADES as the best existing loss function that was deployed with these frameworks and experimented with~\CIFAR-100-WRN34 for comparison. As shown in Table~\ref{tb:proxy_framwork}, our~\SystemName\ is compatible with the two frameworks and surpasses the baseline performance by a significant margin.
  
 
 \textbf{High-quality datasets with different backbones}. We further explore the robustness of the models trained with our~\SystemName~and the two most competitive AT frameworks, namely TRADES and SAT, on high quality datasets. Particularly, we experiment on~\ImageNet100~\cite{5206848}, and~\Intel\ image dataset \cite{intelanalytics}, using PreActResNet18 \cite{he2016identity} and ResNet50  \cite{he2016deep}, respectively. For~\Intel\ dataset, the hyper-parameter settings are kept the same as for~\CIFAR-10 dataset, except that the learning rate is $1e^{-3}$ decayed by $0.1$ every 7 epochs, and the model is trained in 50 epochs. Meanwhile,~\ImageNet100 is trained with the same settings as ~\CIFAR-100, except that SAT is trained in 400 epochs. In~Table~\ref{tb:appli_attack}, we report the results with strongest attacks from~Table~\ref{tb:white_attack}, MIM, CW (\textit{white-box}) Square attack (\textit{black-box}), and AutoAttack. Under our defense framework, our model achieves much higher robustness than others, while maintaining comparative accuracy on clean images. The results demonstrate that our~\SystemName~is compatible to various backbone networks as well as different classification tasks. 

   \begin{figure}
\centering
\includegraphics[width=0.50\textwidth]{ Figures/SAT_JR_vs_ours_model_der.pdf}

\caption{ \textbf{Average gradient norm ratios between adversarial and clean samples.} The ratios of $\mathbb{E}(||\nabla_{\theta_i}\mathcal{L}(\tilde{x})|| / ||\nabla_{\theta_i}\mathcal{L}({x})||)$ with respect to the model's parameters $\theta_{i}$ on~\CIFAR-10 dataset.}
\label{fig:model_der_sat_JR_vs_ours}
\end{figure}

\begin{table}
\centering
\resizebox{0.42\textwidth}{!}{%
    \begin{tabular}{l | c || l | c } 
    \Xhline{3\arrayrulewidth}
    Method & $\mathbb{E}(||\nabla_{{x}} \mathcal{L}||_1)$ &Method & $\mathbb{E}(||\nabla_{{x}} \mathcal{L})||_1)$\Tstrut\Bstrut\\
    \Xhline{2\arrayrulewidth}
    $\mathcal{XE}$ &$854e^{-4}$ &PGD-AT &$158e^{-4}$\Tstrut\\
    ALP &$118e^{-4}$ & TRADES &$45e^{-4}$ \\
   SAT &$48e^{-4}$  &{\SystemName~ (\textit{Ours})} &\textbf{43\textit{e}$^{-4}$}\Bstrut\\
    \Xhline{3\arrayrulewidth}
    \end{tabular}%
}
\caption{\textbf{Average derivative of $\mathcal{XE}$ loss} \textit{w.r.t} input ${x}$ {at the} pixel level. The lower the derivative is, the less perturbed the adversarial samples are when the model is abused. Best results are in \textbf{bold}. }
\label{tb:e_of_nabla}
\end{table}

 
\subsection{Ablation Studies \& Discussion}
\label{sec:abl_study}
\textbf{Loss's derivative. } Similar to the preliminary experiment, we describe the differences between our proposed \SystemName\ and SAT in terms of the defense model's gradients at different layers. Throughout intermediate layers in an attacked model, both frameworks provide stable ratios between perturbed and clean sample's gradients as shown in Fig.~\ref{fig:model_der_sat_JR_vs_ours}. It is worth noting that the gradients are derived on unobserved samples in the test set. As shown, in the forward path, our proposed \SystemName\ successfully balances the adversarial samples and the clean samples, with ratio values in the majority of layers near to 1.  Moreover, in the backward path, since the victim model's gradients are deployed to generate more perturbations, our\ \SystemName\  model achieves better robustness when it can produce smaller gradients \textit{w.r.t.} its inputs. 

Next, Table~\ref{tb:e_of_nabla} shows the average magnitude of cross-entropy loss's derivative \textit{w.r.t.} the input images from~\CIFAR-10 dataset with WRN34. While all the AT methods can significantly reduce the impact of the gradient compared to $\mathcal{XE}$ loss, our proposed framework introduces the lowest value at $43e^{-4}$. This finding explains why the \SystemName\ successfully ensures enhanced resilience against the same adversary.

\begin{figure}[!t]
\includegraphics[width=0.44\textwidth]{  Figures/convergence_sw_sat.pdf}
\caption{ The $\mathcal{XE}$ loss of WRN34 training on ~\CIFAR-10 and ~\CIFAR-100 with SW, and SAT, respectively. While both algorithms exhibit a similar convergence on ~\CIFAR-10, SAT fails to converge on ~\CIFAR-100 in 100 epochs.} 
\label{fig:converg}
\end{figure}
 \textbf{SW distance vs. Sinkhorn divergence.} Bouniot \textit{et al.} propose the SAT algorithm that deploys Sinkhorn divergence to push the distributions of clean and perturbed samples towards each other \cite{bouniot2021optimal}.
While SAT can achieve comparable results to previous research, it exposes a weakness in high-dimensional space, i.e., datasets with a large number of classes exhibit slower convergence, as demonstrated in other research~\cite{meng2021large,petrovich2020feature}. We also show that SAT fails to converge on~\CIFAR-100 in 100 epochs in our experiment, as shown in Fig.~\ref{fig:converg}. Meanwhile, we observed that hyper-parameter settings highly affect the adversarial training~\cite{pang2020bag}, and the performance improvement by SAT can be partly achieved with additional training epochs. Nevertheless, we still include results from SAT trained in 400 epochs on ~\CIFAR-100 in \cite{bouniot2021optimal}.


\begin{table*}[!t]
\centering
\resizebox{0.95\textwidth}{!}{%
    \begin{tabular}{l | l | c | c c c c c c c | c c | c } 
    \Xhline{3\arrayrulewidth}
Dataset & Defense    &\textit{Clean}    &PGD$^{20}$    &PGD$^{100}$    &$L_{2}$-PGD      &MIM &FGSM   &CW     &FAB   &Square & SimBa   &\emph{AutoAtt}\Tstrut\Bstrut\\
    \Xhline{2\arrayrulewidth}

    
\multirow{2}{*}{\small{~\CIFAR-10}} &   \textit{SAT+JR}    &\textit{84.43}    &54.31    &53.95    &62.82    &54.30    &60.55    &52.66    &53.36    &62.27    &91.94    &51.66\Tstrut\\
    &\SystemName~(\emph{Ours}) &\textit{84.53}    &\textbf{55.07}    &\textbf{54.69}    &\textbf{63.99}    &\textbf{55.03}    &\textbf{61.09}    &\textbf{53.92}    &\textbf{54.26}    &\textbf{63.27}    &\textbf{72.79}    &\textbf{52.41}\Bstrut \\
    \Xhline{3\arrayrulewidth}
\multirow{2}{*}{\small{~\CIFAR-100}} &   \textit{SAT+JR$_{400}$}    &\textit{53.77}    &26.07    &25.83    &33.06    &26.04    &30.69    &24.89    &26.21    &31.16    &40.93    &24.10\Tstrut\\
     &\SystemName~(\emph{Ours}) &\textit{58.14}    &\textbf{32.41}    &\textbf{32.26}    &\textbf{43.30}    &\textbf{32.34}    &\textbf{34.68}    &\textbf{29.65}    &\textbf{29.30}    &\textbf{36.34}    &\textbf{50.07}    &\textbf{28.49}\Bstrut \\
    \Xhline{3\arrayrulewidth}
    \end{tabular}%
}
\caption{Comparison between ours and SAT$+$\textit{Random JR} on ~\CIFAR-10  and ~\CIFAR-100 with  WRN34. Best results are in \textbf{bold}.}
\label{tb:naive_combine}
\end{table*}

\begin{table*}[!t]
\centering
\resizebox{0.95\textwidth}{!}{%
    \begin{tabular}{l | l | c | c c c c c c c | c c | c } 
    \Xhline{3\arrayrulewidth}
Dataset & Defense    &\textit{Clean}    &PGD$^{20}$    &PGD$^{100}$    &$L_{2}$-PGD       &MIM &FGSM   &CW     &FAB   &Square & SimBa   &\emph{AutoAtt}\Tstrut\Bstrut\\
    \Xhline{2\arrayrulewidth}

    
\multirow{2}{*}{\small{~\CIFAR-10}} &   \textit{Random JR}     &\textit{85.18}    &22.63    &21.91    &60.71    &22.40    &32.90    &{21.95}    &21.85    &46.14    &70.92    &20.38\Tstrut\\
    &\textit{Optimal JR} &\textit{84.42}    &\textbf{25.39}    &\textbf{24.62}   &\textbf{62.27}    &\textbf{25.22}    &\textbf{34.74}    &\textbf{24.63}    &\textbf{24.54}    &\textbf{46.98}    &\textbf{71.99}    &\textbf{22.90}\Bstrut \\
    \Xhline{3\arrayrulewidth}
\multirow{2}{*}{\small{~\CIFAR-100}} &   \textit{Random JR}    &\textit{66.64}    &9.54    &8.99    &37.98    &9.42    &16.58    &10.40    &9.28    &23.32    &49.48    &7.98\Tstrut\\
     &\textit{Optimal JR} &\textit{64.85}    &\textbf{11.20}    &\textbf{10.57}    &\textbf{38.91}    &\textbf{11.08}    &\textbf{17.59}    &\textbf{11.68}    &\textbf{10.58}    &\textbf{24.67}    &\textbf{50.46}    &\textbf{24.14} \Bstrut \\
    \Xhline{3\arrayrulewidth}
    \end{tabular}%
}
\caption{Accuracy ($\%$) of WRN34 model trained with JR having random projections, and informative projections respectively, under different number attacks on ~\CIFAR-10 test dataset. Best results are in \textbf{bold}.}
\label{tb:randJac_vs_WDJac}
\end{table*}


 \textbf{Our~\SystemName~vs. Naive Combination of SAT \& random JR.} In order to reveal the contribution of the Jacobian regularization and how the naive approach (SAT+JR) differs from ours, we report their robustness under wide range of \textit{white-box} PGD attack in Table~\ref{tb:naive_combine}. The experiments are conducted with ~\CIFAR-10 and ~\CIFAR-100 with WRN34. Our optimal approach attains slightly better robustness on small dataset (~\CIFAR-10). On the large dataset (~\CIFAR-100), however, our  ~\SystemName~ achieves significant improvement. This phenomenon is explained by the fact that regularizing the input-output Jacobian matrix increases the difficulty of the SAT algorithm's convergence, which results in a slower convergence. Therefore, naively combining AT and random Jacobian regularization can restrain the overall optimization process.  
 
\textbf{Optimal vs. random projections.}To verify the efficiency of the optimal Jacobian regularization, WRN34 is trained on ~\CIFAR-10 using {$\mathcal{XE}$} loss with clean samples and the regularization term using~Eq.~\ref{eqn:one_jac} and~Eq.~\ref{eqn:opt_jac} respectively, as  follow:
\begin{equation}
    \mathcal{L} = \sum^{\mathcal{B}}_{i=1} \left (\mathcal{L_{XE}}(x_i, y_i) + \lambda_{J}||J(x_i)||_{F}^{2} \right),
\end{equation}
where $\lambda_J$ is a hyper-parameter to balance the regularization term and {$\mathcal{XE}$} loss that is set to 0.02 for this experiment. As we can observe from Table~\ref{tb:randJac_vs_WDJac}, the Jacobian regularized model trained with SW-supported projections consistently achieves higher robustness, compared to the random projections. As shown, our proposed optimal regularization consistently achieves up to 2.5\% improvement in accuracy under \textit{AuttoAttack} compared to the random one.

In addition, we describe the benefits of the optimal Jacobian regularization in Fig.~\ref{fig:more_cross_section} (\textit{Supp. Material}). While the model trained without the regularizer is highly vulnerable to the perturbation, the Jacobian regularizer can enhance its robustness by extending the decision boundaries, which is indicated by a larger black
 circle. Our optimal Jacobian regularizer can further expand the decision cell, thus additionally improving the model robustness. This is explained by the informative directions, as illustrated in Fig.~\ref{fig:opt_move_toy}, which can guide our model to achieve optimal projections for the input-output Jacobian regularization.


\textbf{Hyper-parameter sensitivity}
\label{abl:hyper_study}
Our ablation studies about hyper-parameters' sensitivities, \textit{i.e.}, $\lambda_J$, $\lambda_{SW}$ and $K$, are included in~Table~\ref{tb:ab_lambda_sw}. The experiments are conducted on ~\CIFAR-10 with WRN34. First, we note that increasing $\lambda_J$ too much may decrease both accuracy and robustness. The reason comes from the fact that the gradients of the loss function creates adversarial perturbations in AT step.  Next, the values of $\lambda_{SW}$ can be selected from a wide range. Nevertheless, a trained model with small $\lambda_{SW}$ cannot force its parameters to pay more attention to adversarial samples, and a large $\lambda_{SW}$ can decrease the original accuracy. Regarding the number of slices $K$, we note that a small number of slices can not generalize the transportation cost between the distributions in the latent space, while a very large $K$ does not provide significant improvement, but can decelerate the training process. Finally, it is worth mentioning that there is still room for hyper-parameter tuning to achieve better results.

 % Finally, to show the superiority of our ~\SystemName~over SAT and other baselines,  we include extensive ablation studies regarding \textbf{models' performance}, \textbf{optimal projections' efficacy}, and \textbf{training time} in \textit{Supp. Material}.



 \begin{table}[!t]
\centering
\resizebox{0.46\textwidth}{!}{%
    \begin{tabular}{c | c | c | c | c | c | c } 
    \Xhline{3\arrayrulewidth}
    \multicolumn{3}{c | }{Hyper-parameters}    &\multicolumn{4}{c}{Robustness}\Tstrut\Bstrut\\
    \Xhline{1\arrayrulewidth}
    $\lambda_J$    &$\lambda_{SW}$ &$K$   &\textit{Clean}   &PGD$^{20}$    &PGD$^{100}$    &AutoAttack\Tstrut\Bstrut \\
    \hline \hline
    \rowcolor{lightgray} 0.002    &64    &32     &\textit{84.53}    &55.07    &54.69    &52.41\Tstrut\Bstrut\\
     \hline
    \underline{0.01}    &64    &32  &\textit{84.75}    &54.37    &54.06    &52.13 \Tstrut\\
    \underline{0.05}    &64    &32  &\textit{82.82}    &54.98    &54.72    &52.00\Bstrut\\
    \Xhline{2\arrayrulewidth}
    0.002    &\underline{32} &32     &\textit{85.47}    &54.85    &54.46    &52.23\Tstrut\\
    0.002    &\underline{72}    &32   &\textit{83.19}    &55.70    &55.40    &53.04\Bstrut\\
    \Xhline{2\arrayrulewidth}
    0.002    &64    &\underline{16}  &\textit{81.47}    &55.10    &54.98    &51.82\Tstrut\\
    0.002    &64   &\underline{64}  &\textit{85.79}    &53.80    &53.36    &51.83\Bstrut\\
    \Xhline{3\arrayrulewidth}
    \end{tabular}%
}
\caption{\textbf{Hyper-parameter tuning.} The sensitivities of hyper-parameters: $\lambda_J$, $\lambda_{SW}$ and $K$. Without Jacobian regularization, the model cannot achieve the best performance. Trade-off between model's accuracy vs. robustness is shown using different $\lambda_{SW}$. Default setting is \colorbox{lightgray}{highlighted}.}
\label{tb:ab_lambda_sw}
\end{table}





