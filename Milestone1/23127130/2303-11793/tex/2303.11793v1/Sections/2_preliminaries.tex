
\section{Comparisons between AT and JR} %Jacobian Regularization}
\subsection{Theoretical Preliminaries}
Let a function $f$ represent a deep neural network (DNN), which is parameterized by $\theta$, and $x \in \mathbb{R}^{I}$ be a clean input image. Its corresponding output vector is $z=f(x) \in \mathbb{R}^C$, where $z_{c}$ is proportional to the likelihood that $x$ is in the
$c$-th class. Also, let $\tilde{x} = x + \epsilon$ be an adversarial sample of $x$ generated by adding a small perturbation vector $\epsilon \in \mathbb{R}^{I}$. Then, the Taylor expansion of the mapped feature of the adversarial sample with respect to $\epsilon$ is derived as follows:
\begin{align}
    \tilde{z} = f(x + \epsilon) &= f(x) + J(x) \epsilon + O (\epsilon ^{2}) \nonumber \\
    &\simeq z + J(x) \epsilon , 
\label{eqn:taylor}
\end{align}
\begin{equation}
    ||\tilde{z} - z||_{q} \simeq ||J(x) \epsilon ||_{q}.
\label{eqn:key_eqn}
\end{equation}
\begin{table}[t!]
\centering
\resizebox{1\linewidth}{!}{%
\begin{tabular}{l | c }
\Xhline{2\arrayrulewidth}
AT Framework            & Training Objective       \\
\hline \hline
TRADES    & $\mathcal{L}_{\mathcal{XE}}(\mathbb{S}(z), y) + \lambda \mathcal{L}_{\mathcal{XE}}(\mathbb{S}(\tilde{z}), \mathbb{S}(z))$    \Tstrut\\
% MART    & $\mathcal{L}_{\mathcal{XE}}(\mathbb{S}(z), y) + \lambda \mathcal{L}_{\mathcal{XE}}(\mathbb{S}(\tilde{z}), \mathbb{S}(z))\cdot (1-\mathbb{S}_{y}(z))$    \Tstrut\\
PGD-AT    &   $ \mathcal{L}_{\mathcal{XE}}(\mathbb{S}(\tilde{z}), y)$         \Tstrut\\
ALP  & $\alpha \mathcal{L}_{\mathcal{XE}}(\mathbb{S}(\tilde{z}), y) +(1-\alpha) \mathcal{L}_{\mathcal{XE}}(\mathbb{S}(\tilde{z}), y) + \lambda ||\tilde{z}-z||
_2$ \Bstrut\Tstrut\\
\Xhline{2\arrayrulewidth}
\end{tabular}%
}
\caption{\small   Training objectives of the popular AT frameworks, where $\mathbb{S}$ denotes the softmax function.}
\label{tb:stategies}

\end{table}
Precisely, Eq.~\ref{eqn:key_eqn} is a basis to derive two primary schools of approaches, AT vs. Jacobian regularization, for mitigating adversarial perturbations and boosting the model robustness. In particular, each side of ~Eq.~\ref{eqn:key_eqn} targets the following two objectives, to improve the robustness: 

    \textbf{\textit{1) Aligning adversarial representation (AT)}.} Minimizing the left-hand side of Eq.~\ref{eqn:key_eqn} is to push the likelihood of an adversarial sample $\tilde{x}$ close to that of a clean sample $x$. For instance, the Kullback-Leibler divergence between two likelihoods is a popular AT framework such as TRADES~\cite{zhang2019theoretically}. More  broadly, the likelihood differences can include the cross-entropy ($\mathcal{XE}$) loss of adversarial samples such as ALP \cite{kannan2018adversarial}, PGD-AT \cite{madry2017towards}, or FreeAT \cite{shafahi2019adversarial}. These well-known AT frameworks are summarized in Table~\ref{tb:stategies}, to explain their core learning objective.

\begin{figure*}[!ht]
  \centering
\includegraphics[width=1\linewidth]{Figures/magnitude_activation_pgd20.pdf}
\caption{\small   The magnitude of activation at the penultimate layer for models trained with $\mathcal{XE}$ loss, PGD-AT adversarial training, and the input-output Jacobian regularization. The channels in the X-axis are sorted in descending order of the clean samples' magnitude. More visualizations are provided in \textit{Supp. Material}.}%
\label{fig:mag_activation}
\end{figure*}

\begin{figure}[t!]
 \centering
 \includegraphics[width=8.3cm]{Figures/Jac_vs_PGD_input_der.pdf}
 \caption{\small   Magnitude of $||\nabla_{\tilde{x}} \mathcal{L}_{\mathcal{XE}}||_1$ at the input layer for a model trained with PGD-AT and Jacobian regularization. Red and green-filled areas range from min. to max. values of each sample.}
 \label{fig:input_der}
\end{figure}
  
    \textbf{\textit{2) Regularizing input-output Jacobian matrix (JR)}.} Regularizing the right-hand side of Eq.~\ref{eqn:key_eqn}, which is independent of $\tilde{x}$, suppresses the spectrum of the input-output Jacobian matrix $J(x)$. Thus, the model becomes more stable with respect to input perturbation, as it was theoretically and empirically demonstrated in a line of recent research \cite{jakubovitz2018improving,hoffman2019robust,co2021jacobian}. Particularly, by observing $||J(x) \epsilon ||_{q} \leq ||J(x)||_F ||\epsilon || $, one can instead minimize the square of the Frobenius norm of the Jacobian matrix, which can be estimated as follows \cite{hoffman2019robust}:
    \begin{equation}
        ||J(x)||_{F}^{2} = C\mathbb{E}_{\hat{v}\sim\mathcal{S}^{C-1}} \left[ ||\hat{v} \cdot J ||^2\right],
    \label{eqn:estimator}
    \end{equation}
    where $\hat{v}$ is an uniform random vector drawn from a \textit{C}-dimensional unit sphere $\mathcal{S}^{C-1}$. Using Monte Carlo method to approximate the integration of $\hat{v}$ over the unit sphere,  Eq.~\ref{eqn:estimator} can be rewritten as follows:
    \begin{equation}
        ||J(x)||_{F}^{2} = \frac{1}{n_\text{proj}} \sum_{i=1}^{n_\text{proj}} \left[ \frac{\partial (\hat{v}_i \cdot z)}{\partial x}\right]^{2}.
    \end{equation}
    Considering a large number of samples in a mini-batch, $n_\text{proj}$ is usually set to $1$ for the efficient computation. Hence, the Jacobian regularization is expressed as follows:
    \begin{equation}
    \label{eqn:one_jac}
    ||J(x)||_{F}^{2} \simeq \left[ \frac{\partial (\hat{v} \cdot z)}{\partial x}\right]^{2},  \hat{v} \sim \mathcal{S}^{C-1}.
    \end{equation}


 \textbf{Summary.} As shown above, each approach takes different direction to tackle adversarial perturbations.
%follow different directions.
While AT targets aligning the likelihoods at the output end, Jacobian regularization tries to force the norm of the Jacobian matrix at the input to zero, as also pictorially described in Fig.~\ref{fig:diff_impact}. However, their distinct effects on the robustness of a DNN have not been fully compared and analyzed so far.
\subsection{Empirical Analysis}
% \begin{figure*}[ht!]

%     \centering
%     \includegraphics[width=12.0cm]{  Figures/main_magnitude_activation.png}
%     % \centering
%     % \subfloat[\centering \small $\mathcal{XE}$ loss]{{\includegraphics[width=3cm]{ Figures/magnitude_activateion_xe_pgd25_nolegend.pdf} }}\hfill
%     % \subfloat[\centering \small PGD-AT]{{\includegraphics[width=3cm]{ Figures/magnitude_activateion_pgd_pgd25.pdf} }}\hfill
%     % \subfloat[\centering \small JR ]{{\includegraphics[width=3cm]{ Figures/magnitude_activateion_rand_jac_pgd25.pdf} }}\hfill
%     % \subfloat{{\includegraphics[width=2.0cm]{ Figures/legend_fig2.pdf} }}\hfill
%     \caption{\small   The magnitude of activation at the penultimate layer for models trained with $\mathcal{XE}$ loss, PGD-AT adversarial training, and the input-output {Jacobian} regularization. The channels in the X-axis are sorted in descending order of the clean samples' magnitude. More visualizations are provided in \textit{Supp. Material}. \sh{ I think it will save space if we combine figure 2 and 3 like I did above.}}%
%     \label{fig:mag_activation}
% \end{figure*}
% \label{subsec:exp_analysis}
% \begin{figure}[t!]
% \centering
% \includegraphics[width=6.9cm]
% { Figures/Jac_vs_PGD_input_der.pdf}
% \caption{\small   The magnitude of $||\nabla_{x} \mathcal{L}_{\mathcal{XE}}||_1$ at the input layer for a model trained with PGD-AT and Jacobian regularization, respectively. The red and green-filled areas range from minimum to maximum values of each sample.}
% \label{fig:input_der}
% \end{figure}

We also conduct an experimental analysis to ascertain and characterize the distinct effects of the two approaches (AT vs. Jacobian regularization) on a defense DNN when it is trained with each of the objectives. We use wide residual network WRN34 as the preliminary baseline architecture on~\CIFAR-10 dataset and apply two canonical frameworks: PGD-AT \cite{madry2017towards} and input-output Jacobian regularization \cite{hoffman2019robust} to enhance the model's robustness. Details of training settings are provided in the experiment section.

\begin{figure}[t!]
\centering
\includegraphics[width=8.3cm]{ Figures/Jac_vs_PGD_model_der.pdf}
\caption{\small   Ratios of $\mathbb{E}(||\nabla_{\theta_i}\mathcal{L}(\tilde{x})|| / ||\nabla_{\theta_i}\mathcal{L}({x})||)$ {w.r.t.} the model's parameters $\theta_{i}$ on~\CIFAR-10. The lower the ratios are, the more emphasis the model puts on perturbations.}
\label{fig:model_der}
\end{figure}

In particular, we measure the magnitude of channel-wise activation to characterize the connection between adversarial defense methods and the intermediate layer's activation \cite{bai2021improving}. Fig.~\ref{fig:mag_activation} provides the average magnitude of penultimate activation of clean vs. adversarial samples created by PGD-20 attacks \cite{madry2017towards}. As shown in Fig.~\ref{fig:mag_activation}, not only AT~\cite{bai2021improving}, but also Jacobian regularization can effectively suppress the magnitude of the activation. Moreover, the Jacobian regularization typically achieves the lower magnitude value of the activation compared to that of PGD-AT. This observation serves as a clear counter-example to earlier results from ~\citeauthor{bai2021improving}, where they claim that adversarial robustness can be generally achieved via channel-wise activation suppressing. As such, it is worth noting that while a more effective defense strategy can produce lower activation, the inverse is not always true. In addition, Fig.~\ref{fig:input_der} represents the average gradient of $\mathcal{XE}$ loss with respect to the adversarial samples, at the input layer. This demonstrates that the model trained with Jacobian regularization suppresses input gradients more effectively than a typical AT framework, \textit{i.e.}, PGD-AT. 
In other words, when a defensive model is abused to generate adversarial samples, pre-training with Jacobian regularization can reduce the severity of perturbations, hindering the adversary's target.


On a layer-by-layer basis, we provide Fig.~\ref{fig:model_der} {to} depict the gradients of models trained with PGD-AT, and Jacobin regularization, respectively. Particularly, we compute the norm ratios of the loss gradient on the adversarial sample to the loss gradient on the clean samples for each layer of the model. As we can observe, the model trained with the Jacobian regularization produces higher ratio values, meaning it puts less emphasis on adversarial samples due to its agnostic defense mechanism. Meanwhile,  most of the ratio values at the middle layers from Jacobian training vary around 1.This is explained by the regularization applied to its first derivatives. In summary, we can also empirically conclude that the Jacobian regularization tends to silence the gradient of the model from output to input layers. Therefore, it \textit{agnostically} stabilizes the model under the changes of input samples, and produces low-magnitude adversarial perturbations, when the model is attacked. In contrast, by learning the meaningful pixels from input images, AT adjusts the model's parameters at every layer in such a way to reduce the impacts of adversarial perturbation on the model's outputs. 

As demonstrated by~\citeauthor{hoffman2019robust}, the increase of computational cost from a model training with Jacobian regularization is insignificant, compared to standard training. Therefore, a combination of AT and Jacobian regularization becomes an appealing approach for the adversarial robustness of a model. Furthermore, taking advantages from both approaches can effectively render a classifier to suppress the perturbation and adaptively learn crucial features from both clean and adversarial samples. However, merely adding both approaches together into the training loss is not the best option. Indeed it is insufficient, since the adversarial representations in the latent space can contain meaningful information for the Jacobian regularization, which we will discuss more in the next section.

%However, it is trivial and insufficient to barely add them together into the training loss since the adversarial representations in the latent space can embrace meaningful information for the Jacobian regularization. 

Therefore, in this work, we propose a novel optimization framework,~\SystemName, to leverage the movement direction information of adversarial samples in the latent space and optimize the Jacobian regularization. In this fashion, we can successfully establish a relationship and balance between silencing input's gradients and aligning output distributions, and significantly improve the overall model's robustness.
In addition, recent studies have proposed approaches utilizing a surrogate model \cite{wu2020adversarial} or teacher-student framework \cite{cui2021learnable} during training. Yet, while these methods improve the model's robustness, they also rely on previous training losses (such as TRADE or PGD). And they introduce additional computation for the AT, which are so far well-known for their slow training speed and computational overhead. In our experiment, we show that our novel training loss can be compatible with these frameworks and further improve the model's robustness by a significant margin compared to prior losses.








