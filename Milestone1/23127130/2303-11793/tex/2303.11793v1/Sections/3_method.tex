
\section{Our Approach}
Our approach explores the Sliced Wasserstein (SW) distance in order to push the adversarial distribution closer to the natural distribution with a faster convergence rate. Next, we propose optimal movement directions obtained from the SW distance to sufficiently minimize the spectrum of input-output Jacobian matrix.

\begin{algorithm}[!t]
\caption{\SystemName: AT with SW and optimal Jacobian regularization.}
\begin{algorithmic}[!b]
\Require   DNN $f$
parameterized by $\theta$, training dataset $\mathcal{D}$. Number of projections $K$. Maximum perturbation $\epsilon$, step size $\eta$, number of adversarial iteration $P$. Loss' hyper-parameters $\lambda_J$ and $\lambda_{SW}$. Learning rate $\alpha$ and batch size of $\mathcal{B}$.
    \While{not converged}
        \For{$\{(x_{i},\textrm{ } y_{i})\}_{\mathcal{B}} \in \mathcal{D}$}
            \State $\nu :  = \{z_{i}\}_{\mathcal{B}} = f(x_i; {\theta})| _{i=1,..,\mathcal{B}}$ 
            \State  \texttt{\# Generate adv. samples}
            \For{iteration $t \gets 1$ to $P$}
                \State $\tilde{x_i} = \Pi_{x}^{\epsilon}(\tilde{x_i} +\eta \cdot \text{sgn} (\nabla _{\tilde{x_i}} \mathcal{L}(\tilde{x_i}, y_i)))| _{i=1,..,\mathcal{B}}$  
            \EndFor
            \State $\mu :  = \{\tilde{z_i}\}_{\mathcal{B}} = f(\tilde{x_i}; {\theta})| _{i=1,..,\mathcal{B}}$  
            
            \State $SW \gets 0$ 
            \State $\Delta_i \gets 0|_{i=1,..,\mathcal{B}}$ 
            \For{iteration $k \gets 1$ to $K$}
                \State \texttt{\# Sample $\hat{v}_k$ from $\mathcal{S}^{C-1}$}           \State $\hat{v}_k$ $\gets$  $\mathcal{U}(\mathcal{S}^{C-1})$ 
                \State \texttt{\# Add SW under projection $\hat{v}_k$}
                \State $SW \gets SW + \psi  \left(\tau_1 \circ \mathcal{R}_{\hat{v}_k}\mu, \tau_2 \circ\mathcal{R}_{\hat{v}_k}\nu \right)$ 
                \State \texttt{\# Calculate samples' movements under $\hat{v}_k$}
                \State $m_k \gets \left(\tau_1^{-1} \circ \tau_2 \circ \mathcal{R}_{\hat{v}_k}\nu - \mathcal{R}_{\hat{v}_k}\mu \right) \otimes \hat{v}_k$ 
                \State $\Delta_i \gets \Delta_i + m_{k,i}|_{i=1,..,\mathcal{B}}$
            \EndFor
            \State $\Delta_i \gets \Delta_i / ||\Delta_i||_2 $  $ |_{i=1,..,\mathcal{B}}$
            \State $\mathcal{L} \gets \sum^{\mathcal{B}} \left( \mathcal{L_{XE}}(\tilde{x_i}, y_i) + \lambda_J ||J(x_i | \Delta_i)||_{F}^{2} \right) $  \\\hspace{3cm}  $ +\quad \lambda_{SW}SW(\mu, \nu) $
            \State $\theta \gets \theta - \alpha \cdot \nabla _{\theta} \mathcal{L}$ 
        \EndFor
    \EndWhile
\end{algorithmic}

\label{alg:opt_move}
\end{algorithm}


\textbf{Sliced Wasserstein distance.} The \emph{p}-Wasserstein distance between two probability distributions $\mu$ and $\nu$ \cite{villani2008optimal} in the \emph{d}-dimensional space $\Omega$, to search for an optimal transportation cost between $\mu$ and $\nu$, is defined as follows:
\begin{equation}
\label{eqn:p-wasseitern}
\small
    W_p(P_{\mu},P_{\nu}) = \Big(\inf_{\pi \in \Pi(\mu, \nu)}  \int_{\Omega \times \Omega}\psi(x,y)^p d\pi(x,y) \Big)^{1/p},
\end{equation}
where $\phi: \Omega \times \Omega \rightarrow \mathbb{R}^{+}$ is a transportation cost function, and $\Pi(\mu, \nu)$ is a collection of all possible transportation plans. The Sliced \emph{p}-Wasserstein distance ($SW_p$), which is inspired by the $W_p$ in one-dimensional, calculates the \emph{p}-Wasserstein distance by projecting $\mu$ and $\nu$ onto multiple one-dimensional marginal distributions using Radon transform \cite{helgason2010integral}. The $SW_p$ is defined as follows:
\begin{equation}
\small
    SW_{p}(\mu, \nu) = \int_{\mathcal{S}^{d-1}}W_{p}(\mathcal{R}_{\hat{v}}\mu, \mathcal{R}_{\hat{v}}\nu) d\hat{v},
\end{equation}
where $\mathcal{R}_{\hat{v}}\mu$ is the Radon transform as follows:
\begin{equation}
\small
    \mathcal{R}_{\hat{v}}\mu = \int_{\Omega} \mu (x) \delta (t-\langle \hat{v}, x \rangle) dx, \forall \hat{v} \in \mathcal{S}^{d-1}, \forall t \in \mathbb{R},
\end{equation}
% \begin{figure}{r}
%     \centering
%     \includegraphics[width=0.38\textwidth]{ Figures/optimal_movement.png}
%     \caption{\small Illustration of optimal movement directions. \textbf{Top row} ((a) \& (b)): Random movement directions  (green arrows), which is non-informative, uniformly sampled from two-dimensional unit sphere $\mathcal{S}^{1}$. \textbf{Bottom row} ((c) \& (d)): The optimal movement directions from the SW distance between source distribution (\textcolor{green!65!black}{green}) and target distribution (\textcolor{orange}{orange}) obtained from ~Eq.~\ref{eqn:opt_move}.}
%     \label{fig:opt_move_toy}
% \end{figure}
where $\langle \cdot, \cdot \rangle$ denote the Euclidean inner product, and $\delta$ is the Dirac delta function.

Next, let $\mathcal{B}$ denote the size of a mini-batch of samples. To calculate the transportation cost between adversarial samples' representations $\mu :  = \{\tilde{z}_{i}\}_{\mathcal{B}}$ and the corresponding original samples' representations $\nu :  = \{z_i\}_{\mathcal{B}}$, the integration of $\hat{v}$ over the unit sphere $\mathcal{S}^{C-1}$ is approximated via Monte Carlo method with $K$ uniformly sampled random vector $\hat{v}_i \in \mathcal{S}^{C-1}$. In particular, $\mathcal{R}_{\hat{v}}\mu$ and $\mathcal{R}_{\hat{v}}\nu$ are sorted in ascending order using two permutation operators $\tau_1$  and $\tau_2$, respectively, and the approximation of Sliced \textit{1}-Wasserstein is expressed as follows:
\begin{equation}
\label{eqn:swd_approx}
\small
    SW(\mu, \nu) \simeq  \sum_{k=1}^{K}  \psi  \left(\tau_1 \circ \mathcal{R}_{\hat{v}_k}\mu, \tau_2 \circ\mathcal{R}_{\hat{v}_k}\nu \right).
\end{equation}
\begin{table*}[!t]
\centering
\resizebox{0.92\textwidth}{!}{%
    \begin{tabular}{c | l | c | c c c c c c c | c c | c } 
    \Xhline{3\arrayrulewidth}
    Dataset & Defense    &\textit{Clean}    &PGD$^{20}$    &PGD$^{100}$    &$L_{2}$-PGD       &MIM &FGSM   &CW     &FAB   &Square & SimBa  &\emph{AutoAtt}\Tstrut\Bstrut\\
    \Xhline{2\arrayrulewidth}
    \multirow{8}{*}{\rotatebox[origin=c]{90}{\centering \small{~\CIFAR-10}}}    
    &TRADES    &\textit{84.65}    &54.33    &54.06    &61.23    &54.27    &60.59    &53.44    &54.20    &62.50    &71.29    &52.26\Tstrut\\
&ALP    &\textit{86.77}    &47.31    &46.72    &56.15    &47.12    &58.42    &47.80    &55.37    &59.64    &68.71    &46.55\\
&PGD-AT    &\textit{86.74}    &46.75    &46.26    &55.91    &46.66    &56.53    &47.49    &48.39    &59.01    &68.58    &45.81\\
&SAT    &\textit{82.96}    &53.60    &53.34    &60.44    &50.58    &60.36    &52.38    &53.05    &61.66    &69.90    &50.99\Bstrut\\ 
    \cdashline{2-13}
    &\textit{Random JR}    &\textit{85.18}    &22.63    &21.91    &60.71    &22.40    &32.90    &21.95    &21.85    &46.14    &70.92    &20.38\Tstrut\\
&\textit{SW}    &\textit{84.77}    &54.11    &53.78    &61.24    &54.19    &61.48    &53.84    &55.30    &63.11    &71.02    &51.89\\
    &\SystemName~(\emph{Ours}) &\textit{84.53}    &\textbf{55.07}    &\textbf{54.69}    &\textbf{63.99}    &\textbf{55.03}    &\textbf{61.09}    &\textbf{53.92}    &\textbf{54.26}    &\textbf{63.27}    &\textbf{72.79}    &\textbf{52.41}\Bstrut \\
    \Xhline{3\arrayrulewidth}
    \multirow{8}{*}{\rotatebox[origin=c]{90}{\centering \small{~\CIFAR-100}}}    
    &TRADES   &\textit{57.61}    &30.46    &30.46    &35.92    &30.43    &33.16    &28.14    &27.86    &33.75    &44.47    &27.13\Tstrut\\
&ALP    &\textit{60.60}    &26.10    &25.75    &33.66    &26.08    &31.51    &25.60    &25.70    &32.91    &43.17    &24.49\\
&PGD-AT    &\textit{60.05}    &24.07    &23.78    &31.56    &23.97    &29.45    &24.68    &24.55    &31.51    &41.30    &23.27\\
&SAT$_{400}$    &\textit{54.34}    &26.82    &26.64    &32.36    &26.84    &31.24    &25.30    &26.69    &31.35    &40.57    &24.38\Bstrut\\ 
    \cdashline{2-13}
    &\textit{Random JR}   &\textit{66.64}    &9.54    &8.99    &37.98    &9.42    &16.58    &10.40    &9.28    &23.32    &49.48    &7.98\Tstrut\\
&\textit{SW}     &\textit{58.06}    &26.22    &26.15    &31.56    &26.18    &31.11    &25.78    &26.20    &31.61    &41.24    &24.70\\
    &\SystemName~(\emph{Ours}) &\textit{58.14}    &\textbf{32.41}    &\textbf{32.26}    &\textbf{43.30}    &\textbf{32.34}    &\textbf{34.68}    &\textbf{29.65}    &\textbf{29.30}    &\textbf{36.34}    &\textbf{50.07}    &\textbf{28.49}\Bstrut \\
    \Xhline{3\arrayrulewidth}
    \end{tabular}%
}
\caption{\textbf{Classification accuracy ($\%$) under \textit{white-box}, \textit{black-box} attacks and \textit{AutoAttack}.} Different defense methods trained on~\CIFAR-10 and~\CIFAR-100 datasets using WRN34 in 100 epochs, where underscript indicates training epochs.  Best results are in \textbf{bold}.}
\label{tb:white_attack}
\end{table*}

 \textbf{Optimal movement direction.} Using~Eq.~\ref{eqn:swd_approx}, we can straightforwardly compute the trajectory of each $\tilde{z}_{i}$ in the latent space $\mathbb{R}^{C}$ in order to minimize the $SW(\mu, \nu)$. We refer to the directions of these trajectories as \textit{optimal movement directions}, because the SW distance produces the lowest transportation cost between the source and target distributions. Particularly, let the trajectories of $\{\tilde{z}_i \}_{\mathcal{B}}$ in each single projection $\hat{v}_k$ be:
\begin{equation}
    m_k = \left(\tau_1^{-1} \circ \tau_2 \circ \mathcal{R}_{\hat{v}_k}\nu - \mathcal{R}_{\hat{v}_k}\mu \right) \otimes \hat{v}_k,
\end{equation}
where $\otimes$ denotes the outer product. Then, the overall optimal movement direction of each $\tilde{z}_{i}$ is expressed as follows:
\begin{equation}
\label{eqn:opt_move}
    \Delta_{i} =  \frac{\sum_{k=1}^{K} m_{k,i}}{||\sum_{k=1}^{K} m_{k,i}||_2}.
\end{equation}

% \begin{figure}[t!]
%     \centering
%     \includegraphics[width=2.0in]{  Figures/optimal_movement.png}
%     % \subfloat[]{{\includegraphics[width=3.4cm]{Figures/movement_rand.circles.pdf}}}%
%     % % \qquad
%     % \subfloat[]{{\includegraphics[width=3.4cm]{Figures/movement_rand.moon.pdf}}}%
%     % % \dotfill\par
%     % \quad
%     % \subfloat[]{{\includegraphics[width=3.4cm]{Figures/movement_dir.circles.pdf}}}%
%     % % \qquad
%     % \subfloat[]{{\includegraphics[width=3.4cm]{Figures/movement_dir.moon.pdf} }}%
%     \caption{\smallIllustration of optimal movement directions. \textbf{Top row} ((a) \& (b)): Random movement directions  (green arrows), which is non-informative, uniformly sampled from two-dimensional unit sphere $\mathcal{S}^{1}$. \textbf{Bottom row} ((c) \& (d)): The optimal movement directions from SW distance between source distribution (\textcolor{green!25!black}{green}) and target distribution (\textcolor{orange}{orange}) obtained from ~Eq.~\ref{eqn:opt_move}. The movement directions are passed into the Jacobian regularization on clean samples in ~Eq.~\ref{eqn:opt_jac}.}
%     \label{fig:opt_move}
%     \vspace{-10pt}
% \end{figure}
\begin{figure}[!t]
    \centering
    \includegraphics[width=0.49\textwidth]{ Figures/optimal_movement.png}
    \caption{\small Illustration of optimal movement directions. \textbf{Top row} ((a) \& (b)): Random movement directions  (green arrows), which is non-informative, uniformly sampled from two-dimensional unit sphere $\mathcal{S}^{1}$. \textbf{Bottom row} ((c) \& (d)): The optimal movement directions from the SW distance between source distribution (\textcolor{green!65!black}{green}) and target distribution (\textcolor{orange}{orange}) obtained from ~Eq.~\ref{eqn:opt_move}.}
    \label{fig:opt_move_toy}
\end{figure}
  For illustration, we present a toy example of optimal movement directions obtained by~Eq.~\ref{eqn:opt_move} in Fig.~\ref{fig:opt_move_toy}. The movement direction of an adversarial sample $\tilde{x}$ in the latent space indicates the most sensitive direction of $x$ under adversarial perturbations. Therefore, we aim to regularize the Jacobian matrix of $x$ under this optimal direction, in contrast to a random projection as was proposed in \cite{hoffman2019robust}. Using ~Eq.~\ref{eqn:one_jac}, we can rewrite the optimal input-output Jacobian regularization with previously obtained informative projections for a sample as follows:
\begin{equation}
\label{eqn:opt_jac}
    ||J(x_i | \Delta_i)||_{F}^{2} \simeq \left[ \frac{\partial (\Delta_i \cdot z_i)}{\partial x_i}\right]^{2}.
\end{equation}

Then, our overall loss function for a batch of samples $\{(x_i, y_i)\}_{\mathcal{B}}$ is expressed in the following way:
\begin{equation}
\label{eqn:overall}
 \mathcal{L} = \sum^{\mathcal{B}}_{i=1} \left( \mathcal{L_{XE}}(\tilde{x_i}, y_{i}) + \lambda_{J} ||J(x_i | \Delta_i)||_{F}^{2} \right) + \lambda_{SW}SW(\mu, \nu). 
\end{equation}
%where $\mu :  = \{\tilde{z}_{i}\}_{\mathcal{B}}$ and $\nu :  = \{{z}_{i}\}_{\mathcal{B}}$ as stated earlier.

 In practice, sampling $K$ uniform vectors $\hat{v}_k$ can be performed simultaneously thanks to deep learning libraries. Then, the calculation of random projections and optimal movement steps can be vectorized. Our end-to-end algorithm for optimizing ~Eq.~\ref{eqn:overall} is provided in  Algorithm \ref{alg:opt_move} and List \ref{listing:pseudocode} (\textit{Supp. Material}). 




  
