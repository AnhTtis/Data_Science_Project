\section{Conclusion}
%In this paper, 
We first theoretically and empirically analyze the impacts of AT and Jacobian regularization on the robustness of DNN models. We show that the AT pays more attention to meaningful input samples' pixels, whereas the Jacobian regularizer agnostically silences the DNN's gradients under any perturbation from its output to input layers. Based on these characterizations, we effectively augment the AT framework by integrating input-output Jacobian matrix in order to more effectively improve the DNN's robustness. Using the optimal transport theory, our work is the first to jointly minimize the difference between the distributions of original and adversarial samples with much faster convergence. Also, the proposed SW distance produces the optimal projections for the Jacobian regularization, which can further increase the decision boundaries of a sample under perturbations, and achieves much higher performance through optimizing the best of both worlds. %\hl{Our future work will focus more on the effective projections for both SW and Jacobian regularization that can  save training costs, while further improving the performance.} \simon{i think we said optimum...there is additional stuff we can improve? or should we remove?} 