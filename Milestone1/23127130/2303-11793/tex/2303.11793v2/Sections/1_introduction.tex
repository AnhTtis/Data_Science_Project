
\section{Introduction}
\begin{figure}[t!]
\centering
\includegraphics[width=0.85\linewidth]{Figures/different_impacts.pdf}
\caption{    \textbf{Illustration of two popular approaches to boost a model's robustness: Adversarial Training (AT) vs. Jacobian regularization (JR).} The circled numbers indicate steps used in a typical AT framework. Jacobian regularization  tries to silence the Jacobian matrix at the input end (after $2^{nd}$ step). The AT adjusts the distribution of perturbed samples at the output end (after $4^{th}$ step). For simplicity, we depict the backward step of Jacobian regularization and AT with the common dashed arrow. Jacobian regularization backpropagates through random projections, whereas the AT via a loss function. Our proposed~\SystemName~integrates both, AT and Jacobian regularization, by the optimal transport theory.}
\label{fig:diff_impact}
\vspace{-8pt}
\end{figure}

 % Deep Neural Networks (DNNs) have established themselves as the de facto method for tackling challenging real-world machine learning problems.
 The contemporary landscape of the Web, driven by dynamic services and rich content, heavily leans on Deep Neural Networks (DNNs) as the cornerstone for a myriad of complex machine learning challenges. 
Their  applications cover a broad range of domains, such as image classification, object detection, and recommendation systems.
Nevertheless, recent research has revealed DNNs' severe vulnerability to adversarial examples ~\cite{szegedy2013intriguing,goodfellow2014explaining}, particularly in computer vision tasks. Small imperceptible perturbations added to the image can easily deceive the neural networks into making incorrect predictions with high confidence. Moreover, this unanticipated phenomenon raises social concerns about DNNs' safety and trustworthiness, as they can be abused to attack many sophisticated and practical machine learning systems putting human lives into danger, such as in autonomous car~\cite{deng2020analysis} or medical systems ~\cite{ma2021understanding,bortsova2021adversarial}.

Meanwhile, there are numerous studies that devote their efforts to enhance the robustness of various models against adversarial examples. Among the existing defenses, adversarial training (AT) \cite{goodfellow2014explaining,madry2017towards} and Jacobian regularization (JR)~\cite{jakubovitz2018improving,hoffman2019robust} are the two most predominant and popular defense approaches \footnote{Another line of research investigates distributional robustness (DR) \cite{shafieezadeh2015distributionally, duchi2021statistics, gao2022wasserstein,rahimian2019distributionally, kuhn2019wasserstein,bui2022unified}, which seeks the worst-case distribution of generated perturbations (\textbf{generation step}). Notably, several studies have utilized the Wasserstein distance \cite{shafieezadeh2015distributionally,kuhn2019wasserstein,bui2022unified}. However, these investigations are beyond the scope of our current study (\textbf{optimization step}). On their own, they struggle to robustify a model and often necessitate integration with an AT loss. In our experimental section, we further demonstrate the superiority of a state-of-the-art DR method, namely UDR \cite{bui2022unified}, when combined with our approach.}.
In AT, small perturbations are added to a clean image in its neighbor of $L_p$ norm ball to generate adversarial samples. Thus, an adversarially trained model can force itself to focus more on the most relevant image's pixels. On the other hand, the second approach, Jacobian regularization, mitigates the effect of the perturbation to the model's decision boundary by suppressing its gradients. However, AT and Jacobian regularization have not been directly compared in both theoretical and empirical settings.



In this study, we embark on a dual-path exploration, offering both theoretical and empirical comparisons between AT and Jacobian regularization. Our objective is to deepen our comprehension of the adversarial robustness inherent to DNN models and subsequently enhance their defensive capability. While a myriad of prior research has spotlighted defense, they predominantly adopt either an empirical or theoretical lens, rarely both. To bridge this gap, we introduce an innovative approach, integrating both Jacobian regularization and AT. This fusion seeks to augment the adversarial robustness and defensive efficacy of a model, as elucidated in Fig. \ref{fig:diff_impact}.
 %In this work, we theoretically revisit how AT and Jacobian regularization have been differently derived for providing the adversarial robustness {and defensive effectiveness} of a model. Secondly, we conduct an extensive empirical analysis to demonstrate the distinct impacts of Jacobian regularization and AT on different layers of a defensive DNN.
% In addition, we observe that not only AT but also Jacobian regularization can effectively suppress the channel-wise activation achieving even better results. This remarkably counteracts what was stated by Bai \textit{et al.} \cite{bai2021improving} that adversarial robustness can be improved through suppressing the activation magnitude.
% Thus, we dismantle the bi-directional relationship: while increased adversarial robustness can implies smaller activation magnitudes, the inverse is not always true. \binh{This is just a observation, not a contribution}
% Furthermore, we demonstrate that AT and Jacobian regularization have different impacts on the model's behavior when perturbed samples are encountered. While AT frameworks support a model to adjust its learning parameters to become more stable under adversarial attacks, Jacobian regularization produces a more silent gradient map and thus directly reduces the perturbations' impact on the model's outputs. \binh{replicate the second paragraph}

For AT, a plethora of studies have been proposed, presenting unique strategies to encourage the learning of robust classifiers. \cite{zhang2020geometry,zhang2019theoretically,rony2019decoupling,kannan2018adversarial,madry2017towards}. Notably, Sinkhorn Adversarial Training (SAT) \cite{bouniot2021optimal} resonates with our methodology, particularly in its objective to bridge the distributional gap between clean and adversarial samples using optimal transport theory.
%in which the geometric distance between adversarial samples' and \hl{natural?} \simon{clean} samples' distributions is minimized using optimal transport theory. 
However, the pillar of their algorithms mainly relies on the Sinkhorn algorithm \cite{cuturi2013sinkhorn} to utilize the space discretization property \cite{vialard2019elementary}. Therefore, their approach has several limitations in terms of handling high-dimensional data~\cite{meng2021large,petrovich2020feature}. Particularly, the Sinkhorn algorithm blurs the transport plan by adding an entropic penalty to ensure the optimizationâ€™s convexity. The entropic penalty encourages the randomness of the transportation map. However, in high-dimensional spaces, such randomness reduces the deterministic movement plan of one sample, causing ambiguity.
% As a result, %FeaScatter and SAT results in a slow convergence rate when training defensive models on a large scale dataset, which is unjustifiable due to additional training epochs with distinct learning schedules are introduced \cite{pang2020bag}. 
As a result, when training defense models on a large
scale dataset, SAT results in a slow convergence rate, which is unjustifiable due to the introduction of additional training epochs with distinct learning schedules~\cite{pang2020bag}. 




% To this end, we introduce the novel Optimal Transport with Jacobian regularization~\SystemName~method {for more effective defense against adversarial attacks}. 
To address the outlined challenges, we present our pioneering method, Optimal Transport with Jacobian Regularization, denoted as ~\SystemName, designed explicitly to bolster defenses against adversarial intrusions. We leverage the Sliced Wasserstein (SW) distance, which is more efficient for AT in high dimensional space with a faster convergence rate.
In addition, the SW distance provides us with other advantages due to optimal movement directions of adversarial samples in the embedding space, which is critical for designing an effective defense. We further integrate the input-output Jacobian regularization by substituting its random projections with the optimal movements and constructing the optimal Jacobian regularization. Our main contributions are summarized as follows\footnote{Our code is available at \url{https://anonymous.4open.science/r/OTJR-1120}, and we plan to release the entire code upon acceptance of the paper.}:
% \begin{itemize}
    % \item \textbf{Comprehensive Theoretical and Empirical Examination} We first revisit the theory of adversarial robustness from the perspective of AT and Jacobian regularization. We are the first {to theoretically and empirically} analyze and provide a side-by-side comparison, and {lay the foundation for characterizing} the difference in impacts of each approach on a defensive DNN  design.
    
    % \item \textbf{Innovative Utilization of the Sliced Wasserstein (SW) Distance.} We propose the application of the SW distance in our AT, called  \SystemName, which can rapidly improve the convergence of the training procedure compared to prior works. With the support of SW, we derive \textbf{the optimal movement directions} of adversarial samples in the latent space. Then, we integrate these optimal directions into the Jacobian regularization, which can further increase the decision boundaries of DNNs.
    
    % \item \textbf{White- and Black-box Attack and Defense Performance.} Through extensive experiments, we show that our proposed method achieves higher performance compared to the well-known SOTA defense mechanisms, demonstrating the effectiveness and competitiveness of optimizing Jacobian regularization in AT as an effective defense mechanism. 


% \end{itemize}
\textbf{(1) Comprehensive Theoretical and Empirical Examination.} Our research delves deep into the theoretical underpinnings of adversarial robustness, emphasizing the intricacies of AT and Jacobian Regularization. Distinctively, we pioneer a simultaneous theoretical and empirical analysis, offering an incisive, comparative exploration. This endeavor serves to elucidate the differential impacts that each methodology has on the design and efficacy of defensive DNNs.

\textbf{(2) Innovative Utilization of the Sliced Wasserstein (SW) Distance.} Charting new territory, we introduce the integration of the SW distance within our AT paradigm, denoted as \SystemName. This innovation promises a marked acceleration in the training convergence, setting it apart from extant methodologies. Harnessing the prowess of the SW distance, we discern the optimal trajectories for adversarial samples within the latent space. Subsequently, we weave these optimal vectors into the framework of Jacobian Regularization, augmenting the resilience of DNNs by expanding their decision boundaries.

\textbf{(3) Rigorous Evaluation against White- and Black-box Attacks.} Our exhaustive experimental assessments underscore the superiority of our methodology. Pitted against renowned state-of-the-art defense strategies, our approach consistently emerges preeminent, underscoring the potency of enhancing Jacobian Regularization within the AT spectrum as a formidable defensive arsenal.
