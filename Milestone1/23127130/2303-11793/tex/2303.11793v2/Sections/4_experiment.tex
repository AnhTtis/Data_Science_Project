
\section{Experimental Results}
\label{sec:experiment}
%To evaluate our approach, we conducted various adversarial attacks on different datasets and two backbone networks. 
%  We first compare our proposed~\SystemName~with well-known AT baseline frameworks. Next, we show the compatibility of our \SystemName\ with different networks and high-quality datasets. Finally, we demonstrate how our \SystemName\ helps a defense model to boost its adversarial robustness via extensive ablation studies.




\subsection{Experiment Settings} 
 We employ WideResNet34-10 \cite{zagoruyko2016wide} as our backbone architecture on two datasets~\CIFAR-10 and~\CIFAR-100 \cite{krizhevsky2009learning}. The model are trained for 100 epochs with the momentum SGD \cite{qian1999momentum} optimizer, whereas its initial learning rate is set to 0.1 and decayed by a factor of $10$ at $75^{th}$ and $90^{th}$ epoch, respectively. Adversarial samples in the training phase are generated by $L_\infty$-PGD \cite{madry2017towards} in 10 iterations with the maximal perturbation $\epsilon=8 /255$ and the perturbation step size $\eta=2/255$. For a fair comparison with different approaches \cite{pang2020bag}, we use the above settings throughout our experiments without early stopping or modifying models' architecture, and report their performances on the last epoch. For our~\SystemName~based defense models, we use the following hyper-parameter settings: $\{K=32, \lambda_J =0.002, \lambda_{SW}=64 \}$ and $\{K=128, \lambda_J =0.001, \lambda_{SW}=64 \}$ for~\CIFAR-10 and~\CIFAR-100, respectively. An ablation study of the hyper-parameters' impact on model performance is provided in Sec. \ref{abl:hyper_study}. In addition, we perform the basic sanity tests \cite{carlini2019evaluating} in  Sec.\ref{abl:sanity_test} to ensure that our proposed~\SystemName~does not rely on gradient obfuscation. Overall, we compare our method with four different recent SOTA AT frameworks: TRADES~\cite{zhang2019theoretically},  ALP~\cite{kannan2018adversarial}, PGD-AT~\cite{madry2017towards}, and SAT~\cite{bouniot2021optimal}. The experiments are conducted on one GeForce RTX 3090 24GB GPU with Intel Xeon Gold 6230R CPU @ 2.10GHz.

 
% \begin{table}[!t]
% \caption{Online fool rate (\%) \cite{mladenovic2021online} of various defense models with \CIFAR-10 and our downloaded ~\CIFAR-10-\WEB~ datasets.} 
%      \centering
%      \resizebox{0.48\textwidth}{!}{%
%      \begin{tabular}{l | c  c  c | c  c c } 
%     \Xhline{3\arrayrulewidth}
%     \multirow{2}{*}{\centering Loss}   &\multicolumn{3}{c|}{ \CIFAR-10}   &\multicolumn{3}{c}{ \CIFAR-10-\WEB}  \\
%     & {$k=100$} & {$k=200$} & {$k=500$}  & {$k=100$} & {$k=200$} & {$k=500$} \\
%     \Xhline{2\arrayrulewidth}
%     XE & $88.9_{\pm3.4}$ & $89.4_{\pm2.3}$ & $88.8_{\pm1.4}$ & $93.3_{\pm1.4}$ & $92.5_{\pm1.4}$ &$85.0_{\pm1.4}$ \Tstrut\\
%     TRADES &$74.4_{\pm4.5}$ & \textbf{73.2}$_{\pm4.3}$ & $74.0_{\pm2.0}$ &$ 78.1_{\pm 3.0}$ & $75.6 _{\pm1.4 }$ &$ 46.9_{\pm 1.4}$\Tstrut\\
%     ALP &$73.7_{\pm4.8}$ & $73.3_{\pm3.7}$ & $74.0_{\pm1.2}$ &$78.6 _{\pm 2.9}$ & $77.1 _{\pm 2.0}$ &$49.1 _{\pm 1.4}$\Tstrut\\
%     PGD-AT & $76.7_{\pm4.1}$ & $76.9_{\pm2.1}$ & $75.7_{\pm1.6}$ & $79.2 _{\pm 2.6}$ & $79.8 _{\pm 2.1}$ &$51.4 _{\pm 1.6}$\Tstrut\\
%     SAT &$75.2_{\pm3.3}$ & $74.1_{\pm2.4}$ & $73.4_{\pm 1.3}$ & $77.7 _{\pm5.4 }$ & $72.8 _{\pm 3.2}$ &$48.6 _{\pm 2.1}$\Tstrut\\
%     \rowcolor{backcolour}\SystemName &\textbf{72.9}$_{\pm2.5}$ & ${73.3}_{\pm2.3}$ & $\mathbf{73.3}_{\pm1.0}$ & \textbf{76.7}$_{\pm 3.7}$ & \textbf{72.6}$_{\pm 2.3}$ &\textbf{45.3}$_{\pm 1.6}$ \Bstrut\\
%     \Xhline{3\arrayrulewidth}
%     \end{tabular}%
%     }
%     \label{tb:online_attack}
% \end{table}


\begin{table}[!t]
\caption{Online fool rate (\%) \cite{mladenovic2021online} of various defense models with   our downloaded ~\CIFAR-10-\WEB~ and  ~\CIFAR-100-\WEB~ datasets.} 
     \centering
     \resizebox{0.48\textwidth}{!}{%
     \begin{tabular}{l | c  c  c | c  c c } 
    \Xhline{3\arrayrulewidth}
    \multirow{2}{*}{\centering Defense}   &\multicolumn{3}{c|}{ \CIFAR-10-\WEB}   &\multicolumn{3}{c}{ \CIFAR-100-\WEB}  \\
    & {$k=100$} & {$k=200$} & {$k=500$}  & {$k=100$} & {$k=200$} & {$k=500$} \\
    \Xhline{2\arrayrulewidth}
     
    TRADES   &  78.1$_{\pm 3.0}$&  75.6$_{\pm 2.1}$&  46.9$_{\pm 1.4}$ &  84.7$_{\pm 3.4}$&  85.5$_{\pm 2.2}$&  84.9$_{\pm 1.5}$
\Tstrut\\
    ALP &  78.6$_{\pm 2.9}$&  77.1$_{\pm 2.0}$&  49.1$_{\pm 1.4}$ &  85.1$_{\pm 4.6}$&  85.6$_{\pm 2.3}$&  85.3$_{\pm 1.7}$\Tstrut\\
    PGD-AT &  79.2$_{\pm 2.6}$&  79.8$_{\pm 2.1}$&  51.4$_{\pm 1.6}$ &  84.9$_{\pm 3.6}$&  86.2$_{\pm 2.9}$&  86.3$_{\pm 0.9}$\Tstrut\\
    SAT  &  77.7$_{\pm 5.4}$&  72.8$_{\pm 3.2}$&  48.6$_{\pm 2.1}$ &  86.9$_{\pm 2.6}$&  87.2 $_{\pm 1.8}$&  87.2$_{\pm 0.6}$ \Tstrut\\
    \hline
    \textit{Random JR} &  82.3$_{\pm 3.9}$&  82.5$_{\pm 2.8}$&  63.7$_{\pm 1.3}$ &  90.5$_{\pm 2.9}$&  90.1$_{\pm 1.7}$&  90.0$_{\pm 1.4}$\Tstrut\\
    \textit{SW} &  77.6$_{\pm 4.2}$&  72.0$_{\pm 2.3}$&  46.0$_{\pm 1.6}$ &  \textbf{83.9}$_{\pm 4.0}$&  85.5$_{\pm 2.6}$&  85.5$_{\pm 1.2}$\Tstrut\\
    \rowcolor{backcolour}\SystemName   &  \textbf{74.0}$_{\pm 4.3}$&  \textbf{71.3}$_{\pm 3.1}$&  \textbf{46.0}$_{\pm 1.9}$ &  84.9$_{\pm 3.8}$&  \textbf{84.6}$_{\pm 2.5}$&  \textbf{84.1}$_{\pm 1.4}$\Bstrut\\
    \Xhline{3\arrayrulewidth}
    \end{tabular}%
    }
    \label{tb:online_attack}
\end{table}
\begin{table}[!t]
\caption{Classification accuracy (\%) from defense losses integrated AWP, LBGAT, and UDR, respectively, on WRN34 with \CIFAR-100. $\dagger$ indicates original paper settings.} 
     \centering
     \resizebox{0.48\textwidth}{!}{%
     \begin{tabular}{l | c  c | c  c | c c } 
    \Xhline{3\arrayrulewidth}
    \multirow{2}{*}{\centering Defense}   &\multicolumn{2}{c|}{AWP}   &\multicolumn{2}{c|}{LBGAT} &\multicolumn{2}{c}{UDR$^{\dagger}$} \\
    & \textit{Clean} & AutoAtt & \textit{Clean} & AutoAtt & \textit{Clean} & AutoAtt \\
    \Xhline{2\arrayrulewidth}
    TRADES &\textit{60.17} & 28.80 & \textit{60.43}   &29.34 & 68.04 & 47.87 \Tstrut\\ % &58.17 &27.01
    \rowcolor{backcolour}\SystemName &\textit{\textbf{60.55}} & \textbf{29.79} & \textit{\textbf{62.15}}   &\textbf{29.64} &\textbf{68.31} &\textbf{49.34} \Bstrut\\ % &\textbf{58.91} &\textbf{27.12}
    \Xhline{3\arrayrulewidth}
    \end{tabular}%
    }
    \label{tb:proxy_framwork}
\end{table}
\begin{table}[!t]
\caption{Classification accuracy (\%) of PreActResNet18 and ResNet50 with~\ImageNet100 and~\Intel\ dataset, respectively,  under \textit{white-} and \textit{black-box} attacks.} 
\centering
\resizebox{0.42\textwidth}{!}{%
\setlength{\tabcolsep}{0.5em}
{\renewcommand{\arraystretch}{1.1}
    \begin{tabular}{ l | c | c c | c | c } 
    \Xhline{3\arrayrulewidth}
    {Defense}  &{\textit{Clean}}   &MIM   &CW &Square  & AutoAtt \\
    \Xhline{2\arrayrulewidth}
    \multicolumn{6}{c}{~\ImageNet100 - PreActResNet18}\\
    \hline
    TRADES    &\textit{62.64}    &30.66   &56.95    &46.52    &24.86 \\
    SAT$^{400}$ &\textit{45.08}    &29.08   &41.56    &36.14    &23.96 \\
    \cellcolor{backcolour} {\SystemName}   &\cellcolor{backcolour} \textit{61.97}     &\cellcolor{backcolour} \textbf{33.20}    &\cellcolor{backcolour} \textbf{57.42}    &\cellcolor{backcolour} \textbf{49.00}    &\cellcolor{backcolour} \textbf{27.54}\\
    \Xhline{2\arrayrulewidth}
    \multicolumn{6}{c}{\Intel - ResNet50}\\
    \hline
    TRADES    &\textit{54.43}    &3.73   &{2.47}    &5.57     &0.07\\
    SAT &\textit{50.17}    &5.93   &4.73  &6.77     &1.60\\
    \rowcolor{backcolour} {\SystemName} &\textit{57.80}    &\textbf{8.27}  &\textbf{6.77} &\textbf{9.37}    &\textbf{3.74}\\
    \Xhline{3\arrayrulewidth}
    \end{tabular}%
    }
}
\label{tb:appli_attack}
\end{table}

\subsection{Adversarial Attacks}
\label{sec:adv_attacks}

 Follow \citeauthor{zhang2019theoretically}, we assess defense methods against a wide range \textit{white-box} attacks (20 iterations)\footnote{\url{https://github.com/Harry24k/adversarial-attacks-pytorch.git}}:
FGSM \cite{goodfellow2014explaining}, PGD \cite{madry2017towards}, MIM \cite{dong2018boosting}, CW$_{\infty}$ \cite{carlini2017towards}, DeepFool \cite{moosavi2016deepfool},  and FAB \cite{croce2020minimally}; and a \textit{black-box} attacks (1000 iterations): Square \cite{andriushchenko2020square}. We use the same parameters as \citeauthor{rice2020overfitting} for our experiments: for the $L_{\infty}$ threat model, the values of epsilon and step size are $8/255$ and  $2/255$ for CIFAR-10 and CIFAR-100, respectively. For the $L_2$ threat model, the values of epsilon and step size are 128/255 and 15/255 for all datasets. Additionally, we include AutoAttack \cite{croce2020reliable} which is a reliable adversarial evaluation framework and an ensemble of four distinct  attacks: APGD-CE, APGD-DLR \cite{croce2020reliable}, FAB \cite{croce2020minimally}, and Square \cite{andriushchenko2020square}.
The results of this experiment are presented in~Table~\ref{tb:white_attack}, where the best results are highlighted in bold. Evidently, our proposed~\SystemName~method demonstrates its superior performance across different attack paradigms. The improvement is considerable, by more than $1.0\%$ and $2.3\%$ on average compared to other methods on~\CIFAR-10 and~\CIFAR-100, respectively. In addition, we include two primary components of our proposed~\SystemName, \textit{i.e.,} SW and random JR  in~Table~\ref{tb:white_attack}. While the random JR, as expected, is highly vulnerable to most of the \textit{white-box} attacks due to its adversarial-agnostic defense strategy, the SW approach is on par with other AT methods.

% \begin{figure}[t]
%     \centering
%     \subfloat[\centering ~\CIFAR-10]{{\includegraphics[width=0.46\textwidth]{  Figures/pgd_wrn28_~\CIFAR-10.pdf} }}%
%     % \qquad
%     \hfill
%     \subfloat[\centering ~\CIFAR-100]{{\includegraphics[width=0.46\textwidth]{  Figures/pgd_wrn28_~\CIFAR-100.pdf} }}%
%     \caption{  Classification accuracy (\%) of defensive WRN28 models under different number of $L_{\infty}$-PGD iterations.}%
%     \label{fig:pgd_wrn28}
% \end{figure}
 \subsection{Online adversarial attack}
 \label{sec:online_attack}
In order to validate the applicability of our defense model in real-world systems, we employ the \textit{stochastic virtual} method \cite{mladenovic2021online}. This method is designed as an online attack algorithm with a transiency property; that is, an attacker makes an \textbf{irrevocable decision} regarding whether to initiate an attack or not.

For our experiment,  we curate a dataset by downloading 1,000 images and categorizing them into 10 distinct classes, mirroring the structure of the \CIFAR-10 dataset. We refer to this new subset as \CIFAR-10-\WEB. In a similar vein, we assemble a \CIFAR-100-\WEB~ dataset comprising 5,000 images on the Internet, distributed across 100 classes analogous to the \CIFAR-100 dataset. To ensure the reliability of our findings, we replicate the experiment ten times. The outcomes of these trials are detailed in Table \ref{tb:online_attack}. Notably, our novel system, \SystemName, persistently showcases the most minimal fooling rate in comparison to other methods in the \textit{k-secretary} settings \cite{mladenovic2021online}, underscoring its robustness for real-world applications.

 \subsection{Compatibility with different frameworks and datasets.}
 \label{sec:compatibility}

  \subsubsection{Frameworks with surrogate models and relaxed perturbations} AWP \cite{wu2020adversarial} and LBGAT \cite{cui2021learnable}  are two SOTA benchmarks boosting adversarial robustness using surrogate models during training, albeit rather computationally expensive. Furthermore, they utilized existing AT losses, such as TRADES. We integrate our proposed optimization loss into AWP and LBGAT, respectively. Additionally, we also include the results of UDR \cite{bui2022unified} that used for creating relaxed perturbed noises upon its entire distribution. We selected TRADES as the best existing loss function that was deployed with these frameworks and experimented with~\CIFAR-100-WRN34 for comparison. As shown in Table~\ref{tb:proxy_framwork}, our~\SystemName\ is compatible with the three frameworks and surpasses the baseline performance by a significant margin.
  
 
 \subsubsection{High-quality datasets with different backbones} We extend the evaluation of \SystemName's robustness, comparing it against two leading adversarial training (AT) frameworks: TRADES and SAT, across high-quality datasets and diverse backbone architectures. Specifically, we employ the \ImageNet100 \cite{5206848} dataset with PreActResNet18 \cite{he2016identity} and the \Intel\ dataset \cite{intelanalytics} with ResNet50 \cite{he2016deep}. For the \Intel\ dataset, the experimental settings are analogous to the \CIFAR-10 configuration, albeit with an adjusted learning rate of $1e^{-3}$—decaying by $0.1$ every 7 epochs—and training spanned 50 epochs. For \ImageNet100, settings are aligned with \CIFAR-100, but with SAT's training extended to 400 epochs.  In~Table~\ref{tb:appli_attack}, we report the results with strongest attacks from~Table~\ref{tb:white_attack}, MIM, CW (\textit{white-box}) Square attack (\textit{black-box}), and AutoAttack. Remarkably, under the \SystemName\ framework, our model not only displays superior robustness against adversarial perturbations but also sustains competitive performance on clean samples. This affirms \SystemName's adaptability to various network architectures and diverse classification scenarios.
\begin{table}[!t]
\caption{ \textbf{Basic sanity tests for our ~\SystemName~ method} with \textit{white-box} PGD attack.}
\centering
\resizebox{0.9\linewidth}{!}{%
    \begin{tabular}{c|c|c|c|c|c } 
    \Xhline{3\arrayrulewidth}
    \multicolumn{6}{c }{Number of step}\\
    \hline
    \textit{Clean} &1   &10   &20  &40  &50\\
    \hline
    \textit{84.91} &79.45 &56.05 &55.07 &54.72 &54.72\\
    \hline \hline
    \multicolumn{6}{c }{ Perturbation budget $\epsilon$ w/ PGD-20}\\
    \hline
    \textit{Clean} &8/255   &16/255   &24/255   &64/255  &128/255\\
    \hline
    \textit{84.91}  &55.07 &23.66 &9.18 &0.57 &0.00\\
    \Xhline{3\arrayrulewidth}
    \end{tabular}%
}
\label{tb:sanity_test}
\vspace{-10pt}
\end{table}
   \begin{figure}
\centering
\includegraphics[width=0.46\textwidth]{ Figures/SAT_JR_vs_ours_model_der.pdf}
\vspace{-10pt}
\caption{ \textbf{Average gradient norm ratios between adversarial and clean samples.} The ratios of $\mathbb{E}(||\nabla_{\theta_i}\mathcal{L}(\tilde{x})|| / ||\nabla_{\theta_i}\mathcal{L}({x})||)$ with respect to the model's parameters $\theta_{i}$ on~\CIFAR-10 dataset.}
\label{fig:model_der_sat_JR_vs_ours}
\vspace{-8pt}
\end{figure}

% \begin{table}
% \centering
% \resizebox{0.42\textwidth}{!}{%
%     \begin{tabular}{l | c || l | c } 
%     \Xhline{3\arrayrulewidth}
%     Method & $\mathbb{E}(||\nabla_{{x}} \mathcal{L}||_1)$ &Method & $\mathbb{E}(||\nabla_{{x}} \mathcal{L})||_1)$\Tstrut\Bstrut\\
%     \Xhline{2\arrayrulewidth}
%     $\mathcal{XE}$ &$854e^{-4}$ &PGD-AT &$158e^{-4}$\Tstrut\\
%     ALP &$118e^{-4}$ & TRADES &$45e^{-4}$ \\
%    SAT &$48e^{-4}$  &{\SystemName~ (\textit{Ours})} &\textbf{43\textit{e}$^{-4}$}\Bstrut\\
%     \Xhline{3\arrayrulewidth}
%     \end{tabular}%
% }
% \caption{\textbf{Average derivative of $\mathcal{XE}$ loss} \textit{w.r.t} input ${x}$ {at the} pixel level. The lower the derivative is, the less perturbed the adversarial samples are when the model is abused. Best results are in \textbf{bold}. }
% \label{tb:e_of_nabla}
% \end{table}

\begin{table}[!t]
\caption{\textbf{Average derivative of $\mathcal{XE}$ loss} \textit{w.r.t} input ${x}$ {at the} pixel level after various PGD iterations. The lower the derivative is, the less perturbed the adversarial samples are when the model is abused.  Best results are in \textbf{bold}.}
\centering
\resizebox{0.486\textwidth}{!}{%
    \begin{tabular}{l | r | r |r | r | r |r} 
    \Xhline{3\arrayrulewidth}
    \multirow{2}{*}{\textit{Method}}   &\multicolumn{5}{c }{$\mathbb{E}(||\nabla_{{x}} \mathcal{L})||_1)$}\\
     & \textit{Clean}  & $PGD^1$ & $PGD^5$  & $PGD^{10}$ & $PGD^{15}$ & $PGD^{20}$  \Tstrut\Bstrut\\
    \Xhline{2\arrayrulewidth}
    $\mathcal{XE}$& 854$\cdot e^{-4}$  &5492$\cdot e^{-4}$  &4894$\cdot e^{-4}$  &4852$\cdot e^{-4}$  &4975$\cdot e^{-4}$  &4900$\cdot e^{-4}$\Tstrut\\
    Random JR & 105$\cdot e^{-4}$  &149$\cdot e^{-4}$  &272$\cdot e^{-4}$  &335$\cdot e^{-4}$  &360$\cdot e^{-4}$  &370$\cdot e^{-4}$\\
    PGD & 158$\cdot e^{-4}$  &232$\cdot e^{-4}$  &420$\cdot e^{-4}$  &520$\cdot e^{-4}$  &568$\cdot e^{-4}$  &586$\cdot e^{-4}$\\
    ALP & 118$\cdot e^{-4}$  &161$\cdot e^{-4}$  &249$\cdot e^{-4}$  &298$\cdot e^{-4}$  &322$\cdot e^{-4}$  &332$\cdot e^{-4}$\\
    TRADES & 45$\cdot e^{-4}$  &53$\cdot e^{-4}$  &73$\cdot e^{-4}$  &84$\cdot e^{-4}$  &89$\cdot e^{-4}$  &90$\cdot e^{-4}$\\
   SAT & 48$\cdot e^{-4}$  &54$\cdot e^{-4}$  &65$\cdot e^{-4}$  &75$\cdot e^{-4}$  &81$\cdot e^{-4}$  &84$\cdot e^{-4}$\\
   \rowcolor{backcolour}\textbf{{\SystemName~ (\textit{Ours})}}& \textbf{43}$\cdot e^{-4}$  &\textbf{48}$\cdot e^{-4}$  &\textbf{61}$\cdot e^{-4}$  &\textbf{70}$\cdot e^{-4}$  &\textbf{73}$\cdot e^{-4}$  &\textbf{75}$\cdot e^{-4}$\Bstrut\\
    \Xhline{3\arrayrulewidth}
    \end{tabular}%
}
\label{tb:prog_input_der}
\end{table}

 
\subsection{Ablation Studies \& Discussion}
\subsubsection{ Sanity Tests}
\label{abl:sanity_test}
The phenomenon of gradient obfuscation arises when a defense method is tailored such that its gradients prove ineffective for generating adversarial samples \cite{athalye2018obfuscated}. However, the method designed in that manner can be an incomplete defense to adversarial examples \cite{athalye2018obfuscated}.  Adhering to guidelines from \cite{carlini2019evaluating}, we evaluate our pre-trained model on \CIFAR10 with WRN34  to affirm  our proposed ~\SystemName~ doesn't lean on gradient obfuscation.   As detailed in Table~\ref{tb:sanity_test}, iterative attacks
are strictly more powerful than single-step attacks, whereas when increasing perturbation budget $\epsilon$ can also raise attack successful rate. Finally,  the PGD attack attains a 100\% success rate when $\epsilon=128/255$.


\subsubsection{Loss's derivative. }In continuation with our preliminary analysis, we highlight the disparities in defense model gradients across layers between our \SystemName\ and SAT.  Throughout intermediate layers in an attacked model, both frameworks provide stable ratios between perturbed and clean sample's gradients as shown in Fig.~\ref{fig:model_der_sat_JR_vs_ours}. It is worth noting that the gradients are derived on unobserved samples in the test set.  In the forward path, our \SystemName\ adeptly equilibrates gradients of adversarial and clean samples, with the majority of layers presenting ratio values approximating 1.   Moreover, in the backward path, since the victim model's gradients are deployed to generate more perturbations, our\ \SystemName\  model achieves better robustness when it can produce smaller gradients \textit{w.r.t.} its inputs. 
\begin{table*}[!t]
\caption{Comparison between ours and SAT$+$\textit{Random JR} on ~\CIFAR-10  and ~\CIFAR-100 with  WRN34. Best results are in \textbf{bold}.}
\centering
\resizebox{0.86\textwidth}{!}{%
    \begin{tabular}{l | l | c | c c c c c c c | c c | c } 
    \Xhline{3\arrayrulewidth}
Dataset & Defense    &\textit{Clean}    &PGD$^{20}$    &PGD$^{100}$    &$L_{2}$-PGD      &MIM &FGSM   &CW     &FAB   &Square & SimBa   &\emph{AutoAtt}\Tstrut\Bstrut\\
    \Xhline{2\arrayrulewidth}

    
\multirow{2}{*}{\small{~\CIFAR-10}} &   \textit{SAT+JR}    &\textit{84.43}    &54.31    &53.95    &62.82    &54.30    &60.55    &52.66    &53.36    &62.27    &91.94    &51.66\Tstrut\\
    &\cellcolor{backcolour} \SystemName~(\emph{\textbf{ours}}) &\cellcolor{backcolour} \textit{84.53}    &\cellcolor{backcolour} \textbf{55.07}    &\cellcolor{backcolour} \textbf{54.69}    &\cellcolor{backcolour} \textbf{63.99}    &\cellcolor{backcolour} \textbf{55.03}    &\cellcolor{backcolour} \textbf{61.09}    &\cellcolor{backcolour} \textbf{53.92}    &\cellcolor{backcolour} \textbf{54.26}    &\cellcolor{backcolour} \textbf{63.27}    &\cellcolor{backcolour} \textbf{72.79}    &\cellcolor{backcolour} \textbf{52.41}\Bstrut \\
    \Xhline{3\arrayrulewidth}
\multirow{2}{*}{\small{~\CIFAR-100}} &   \textit{SAT+JR$_{400}$}    &\textit{53.77}    &26.07    &25.83    &33.06    &26.04    &30.69    &24.89    &26.21    &31.16    &40.93    &24.10\Tstrut\\
     &\cellcolor{backcolour} \SystemName~(\emph{\textbf{ours}}) &\cellcolor{backcolour} \textit{58.14}    &\cellcolor{backcolour} \textbf{32.41}    &\cellcolor{backcolour} \textbf{32.26}    &\cellcolor{backcolour} \textbf{43.30}    &\cellcolor{backcolour} \textbf{32.34}    &\cellcolor{backcolour} \textbf{34.68}    &\cellcolor{backcolour} \textbf{29.65}    &\cellcolor{backcolour} \textbf{29.30}    &\cellcolor{backcolour} \textbf{36.34}    &\cellcolor{backcolour} \textbf{50.07}    &\cellcolor{backcolour} \textbf{28.49}\Bstrut \\
    \Xhline{3\arrayrulewidth}
    \end{tabular}%
}
\label{tb:naive_combine}
\end{table*}

\begin{table*}[!t]
\caption{Accuracy ($\%$) of WRN34 model trained with JR having random projections, and informative projections respectively, under different number attacks on ~\CIFAR-10 test dataset. Best results are in \textbf{bold}.}
\centering
\resizebox{0.86\textwidth}{!}{%
    \begin{tabular}{l | l | c | c c c c c c c | c c | c } 
    \Xhline{3\arrayrulewidth}
Dataset & Defense    &\textit{Clean}    &PGD$^{20}$    &PGD$^{100}$    &$L_{2}$-PGD       &MIM &FGSM   &CW     &FAB   &Square & SimBa   &\emph{AutoAtt}\Tstrut\Bstrut\\
    \Xhline{2\arrayrulewidth}
\multirow{2}{*}{\small{~\CIFAR-10}} &   \textit{Random JR}     &\textit{85.18}    &22.63    &21.91    &60.71    &22.40    &32.90    &{21.95}    &21.85    &46.14    &70.92    &20.38\Tstrut\\
    &\cellcolor{backcolour} \textit{Optimal JR} &\cellcolor{backcolour} \textit{84.42}    &\cellcolor{backcolour} \textbf{25.39}    &\cellcolor{backcolour} \textbf{24.62}   &\cellcolor{backcolour} \textbf{62.27}    &\cellcolor{backcolour} \textbf{25.22}    &\cellcolor{backcolour} \textbf{34.74}    &\cellcolor{backcolour} \textbf{24.63}    &\cellcolor{backcolour} \textbf{24.54}    &\cellcolor{backcolour} \textbf{46.98}    &\cellcolor{backcolour} \textbf{71.99}    &\cellcolor{backcolour} \textbf{22.90}\Bstrut \\
    \Xhline{3\arrayrulewidth}
\multirow{2}{*}{\small{~\CIFAR-100}} &   \textit{Random JR}    &\textit{66.64}    &9.54    &8.99    &37.98    &9.42    &16.58    &10.40    &9.28    &23.32    &49.48    &7.98\Tstrut\\
     &\cellcolor{backcolour} \textit{Optimal JR} &\cellcolor{backcolour} \textit{64.85}    &\cellcolor{backcolour} \textbf{11.20}    &\cellcolor{backcolour} \textbf{10.57}    &\cellcolor{backcolour} \textbf{38.91}    &\cellcolor{backcolour} \textbf{11.08}    &\cellcolor{backcolour} \textbf{17.59}    &\cellcolor{backcolour} \textbf{11.68}    &\cellcolor{backcolour} \textbf{10.58}    &\cellcolor{backcolour} \textbf{24.67}    &\cellcolor{backcolour} \textbf{50.46}    &\cellcolor{backcolour} \textbf{24.14} \Bstrut \\
    \Xhline{3\arrayrulewidth}
    \end{tabular}%
}
\label{tb:randJac_vs_WDJac}
\end{table*}
Next, Table~\ref{tb:prog_input_der} shows the average magnitude of cross-entropy loss's derivative \textit{w.r.t.} the input images from~\CIFAR-10 dataset with different PGD white-box attack iterations on WRN34.  Notably, as the number of attack iterations increases, the perturbation noise induced by the output loss derivatives intensifies. However, our proposed framework consistently exhibits the lowest magnitude across all iterations. This characteristic underscores the superior robustness performance of \SystemName.

\begin{figure}[!t]
\includegraphics[width=0.40\textwidth]{  Figures/convergence_sw_sat.pdf}
\caption{ The $\mathcal{XE}$ loss of WRN34 training on ~\CIFAR-10 and ~\CIFAR-100 with SW, and SAT, respectively. While both algorithms exhibit a similar convergence on ~\CIFAR-10, SAT fails to converge on ~\CIFAR-100 in 100 epochs.} 
\label{fig:converg}
\vspace{-14pt}
\end{figure}
 \subsubsection{SW distance vs. Sinkhorn divergence} Bouniot \textit{et al.} propose the SAT algorithm that deploys Sinkhorn divergence to push the distributions of clean and perturbed samples towards each other \cite{bouniot2021optimal}.
While SAT can achieve comparable results to previous research, its limitations become pronounced in high-dimensional space, i.e., datasets with a large number of classes exhibit slower convergence, as demonstrated in other research~\cite{meng2021large,petrovich2020feature}. Our empirical results indicate that SAT struggles to converge within 100 epochs for the \CIFAR-100 dataset, as shown in Fig.~\ref{fig:converg}. Meanwhile, we observed that hyper-parameter settings highly affect the adversarial training~\cite{pang2020bag}, and the performance improvement by SAT can be partly achieved with additional training epochs. Nevertheless, we still include results from SAT trained in 400 epochs on ~\CIFAR-100 in \cite{bouniot2021optimal}.





 \subsubsection{Our~\SystemName~vs. Naive Combination of SAT \& random JR} To discern the impact of Jacobian regularization and distinguish our method from the naive combination of SAT and JR, we report their robustness under wide range of \textit{white-box} PGD attack in Table~\ref{tb:naive_combine}. The experiments are conducted with ~\CIFAR-10 and ~\CIFAR-100 with WRN34. Our optimal approach attains slightly better robustness on small dataset (~\CIFAR-10). On the large dataset (~\CIFAR-100), however, our  ~\SystemName~ achieves significant improvement. This phenomenon is explained by the fact that regularizing the input-output Jacobian matrix increases the difficulty of the SAT algorithm's convergence, which results in a slower convergence. Therefore, naively combining AT and random Jacobian regularization can restrain the overall optimization process.  
 
\subsubsection{Optimal vs. random projections}To verify the efficiency of the optimal Jacobian regularization, WRN34 is trained on ~\CIFAR-10 using {$\mathcal{XE}$} loss with clean samples and the regularization term using~Eq.~\ref{eqn:one_jac} and~Eq.~\ref{eqn:opt_jac} respectively, as  follow:
\begin{equation}
    \mathcal{L} = \sum^{\mathcal{B}}_{i=1} \left (\mathcal{L_{XE}}(x_i, y_i) + \lambda_{J}||J(x_i)||_{F}^{2} \right),
\end{equation}
where $\lambda_J$ is a hyper-parameter to balance the regularization term and {$\mathcal{XE}$} loss that is set to 0.02 for this experiment. As we can observe from Table~\ref{tb:randJac_vs_WDJac}, the Jacobian regularized model trained with SW-supported projections consistently achieves higher robustness, compared to the random projections. As shown, our proposed optimal regularization consistently achieves up to 2.5\% improvement in accuracy under \textit{AuttoAttack} compared to the random one.

In addition, in Fig.\ref{fig:more_cross_section}, we highlight the advantages of optimal Jacobian regularization. Models trained without this regularizer are notably susceptible to perturbations. However, integrating the Jacobian regularizer augments robustness by broadening the decision boundaries, evidenced by an \textbf{enlarged black circle}. Our optimal Jacobian regularizer further extends the decision boundaries, amplifying model resilience. The rationale behind this enhancement lies in the informative directions showcased in Fig.\ref{fig:opt_move_toy}, guiding the model to achieve optimal projections within the input-output Jacobian regularization framework. 

\begin{figure}[!t]
     \includegraphics[width=0.49\textwidth]{Figures/cross_decision/cross_decision_group2.pdf}
    \caption{\textbf{Cross sections of decision boundaries in the input space}. 1st row: Sample input images. 2nd row: Model trained without regularization. 3rd row: Model train with Jacobian regularization having random projection. 4th row: Model trained with Jacobian regularization having informative projections. Our optimal JR benefits from the informative directions obtained by SW distance, and thus helps the model to regularize these sensitive direction of clean samples and produce larger decision cells.} %
    \label{fig:more_cross_section}
    \vspace{-14pt}
\end{figure}

\begin{figure*}[!t]
  \centering
\includegraphics[width=0.78\linewidth]{Figures/main_magnitude_activation_group.pdf}
\caption{{Magnitude of activation} at the penultimate layer for models trained with different defense methods. Our \SystemName~ can regulate adversarial samples' magnitudes similar to clean samples' while well suppressing both of them. }%
\label{fig:mag_activation_all}
\end{figure*}

\subsubsection{Hyper-parameter sensitivity}
\label{abl:hyper_study}
% Our ablation studies about hyper-parameters' sensitivities, \textit{i.e.}, $\lambda_J$, $\lambda_{SW}$ and $K$, are included in~Table~\ref{tb:ab_lambda_sw}. The experiments are conducted on ~\CIFAR-10 with WRN34. First, we note that increasing $\lambda_J$ too much may decrease both accuracy and robustness. The reason comes from the fact that the gradients of the loss function creates adversarial perturbations in AT step.  Next, the values of $\lambda_{SW}$ can be selected from a wide range. Nevertheless, a trained model with small $\lambda_{SW}$ cannot force its parameters to pay more attention to adversarial samples, and a large $\lambda_{SW}$ can decrease the original accuracy. Regarding the number of slices $K$, we note that a small number of slices can not generalize the transportation cost between the distributions in the latent space, while a very large $K$ does not provide significant improvement, but can decelerate the training process. Finally, it is worth mentioning that there is still room for hyper-parameter tuning to achieve better results.
In Table~\ref{tb:ab_lambda_sw}, we present ablation studies focusing on hyper-parameter sensitivities, namely, $\lambda_J$, $\lambda_{SW}$, and $K$, using the \CIFAR-10 dataset and WRN34 architecture. We observe that excessive $\lambda_J$ values compromise accuracy and robustness, a result of the loss function gradients inducing adversarial perturbations during the AT step. While $\lambda_{SW}$ offers flexibility in selection, models with minimal $\lambda_{SW}$ values inadequately address adversarial samples, and high values risk eroding clean accuracy. For the slice count $K$, a lower count fails to encapsulate transportation costs across latent space distributions; conversely, an overly large $K$ brings marginal benefits at the expense of extended training times. We acknowledge potential gains from further hyper-parameter optimization.
 % Finally, to show the superiority of our ~\SystemName~over SAT and other baselines,  we include extensive ablation studies regarding \textbf{models' performance}, \textbf{optimal projections' efficacy}, and \textbf{training time} in \textit{Supp. Material}.




\begin{table}[!t]
\caption{\textbf{Hyper-parameter tuning.} The sensitivities of hyper-parameters: $\lambda_J$, $\lambda_{SW}$ and $K$. Without Jacobian regularization, the model cannot achieve the best performance. Trade-off between model's accuracy vs. robustness is shown via $\lambda_{SW}$.}
\centering
\resizebox{0.44\textwidth}{!}{%
    \begin{tabular}{c | c | c | c | c | c | c } 
    \Xhline{3\arrayrulewidth}
    \multicolumn{3}{c | }{Hyper-parameters}    &\multicolumn{4}{c}{Robustness}\Tstrut\Bstrut\\
    \Xhline{1\arrayrulewidth}
    $\lambda_J$    &$\lambda_{SW}$ &$K$   &\textit{Clean}   &PGD$^{20}$    &PGD$^{100}$    &AutoAttack\Tstrut\Bstrut \\
    \hline \hline
     \rowcolor{backcolour}  0.002    &64    &32     &\textit{84.53}    &55.07    &54.69    &52.41\Tstrut\Bstrut\\
     \hline
    \underline{0.01}    &64    &32  &\textit{84.75}    &54.37    &54.06    &52.13 \Tstrut\\
    \underline{0.05}    &64    &32  &\textit{82.82}    &54.98    &54.72    &52.00\Bstrut\\
    \Xhline{2\arrayrulewidth}
    0.002    &\underline{32} &32     &\textit{85.47}    &54.85    &54.46    &52.23\Tstrut\\
    0.002    &\underline{72}    &32   &\textit{83.19}    &55.70    &55.40    &53.04\Bstrut\\
    \Xhline{2\arrayrulewidth}
    0.002    &64    &\underline{16}  &\textit{81.47}    &55.10    &54.98    &51.82\Tstrut\\
    0.002    &64   &\underline{64}  &\textit{85.79}    &53.80    &53.36    &51.83\Bstrut\\
    \Xhline{3\arrayrulewidth}
    \end{tabular}%
}
\label{tb:ab_lambda_sw}
\vspace{-10pt}
\end{table}


\subsubsection{Training Time}
Table~\ref{tb:training_time} indicates the average training time per epoch of all AT methods on our machine architecture using WRN34 model on ~\CIFAR-100 dataset. Notably, although the SAT algorithm demonstrates a commendable per-epoch training duration, its convergence necessitates up to four times more epochs than alternative methods, especially on large scale datasets such as CIFAR-100. Despite our method delivering notable enhancements over prior state-of-the-art frameworks, its computational demand during training remains within acceptable bounds.
\begin{table}[!t]
\caption{\textbf{Training time per epoch} of AT methods. Even though our method's training time/epoch is slightly slower than the SAT's as the additional Jacobian regularization, it can achieve faster convergence on large-scale datasets. }
\centering
\resizebox{0.86\linewidth}{!}{%
    \begin{tabular}{l | c || l | c } 
    \Xhline{2\arrayrulewidth}
    Method & Time(\textit{mins}) &Method & Time(\textit{mins})
     \\
    \hline \hline
    $\mathcal{XE}$ &1.27 &PGD-AT &13.67 \\
    ALP & 14.67 & TRADES &18.15 \\
      SAT &13.97 &{\SystemName~(Ours)} &18.68 \\
    \Xhline{2\arrayrulewidth}
    \end{tabular}%
}
\label{tb:training_time}
\vspace{-10pt}
\end{table}
\subsubsection{Activation Magnitude}
\label{app:magnitude_activation}
Figure ~\ref{fig:mag_activation_all} depicts the activation magnitudes at the penultimate layer of WRN34 across various AT frameworks. Although AT methods manage to bring the adversarial magnitudes closer to their clean counterparts, the magnitudes generally remain elevated, with PGD-AT being especially prominent. Through a balanced integration of the input Jacobian matrix and output distributions, our proposed method effectively mitigates the model's susceptibility to perturbed samples.



