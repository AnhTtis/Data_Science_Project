
% \setcounter{page}{1}
% \newcount\cvprrulercount
% \setcounter{cvprrulercount}{1}
\begin{center}
\vspace{30pt}Supplementary Material for\\\textbf{OTJR: Optimal Transport Meets Optimal Jacobian Regularization for Adversarial Robustness\\}\vspace{16pt} 
\end{center}


% \section{Threat Model and Adversarial Sample}
% In this section, we summarize essential terminologies of adversarial settings related to our work.  We first define a threat model, which consists of a set of assumptions about the adversary. Then, we describe the generation mechanism of adversarial samples in AT frameworks for the threat model defending against adversarial attacks.

% \subsection{Threat Model}
% Adversarial perturbation was firstly discovered by~\citeauthor{szegedy2013intriguing}, and it instantly strikes an array of studies in both adversarial attack and adversarial robustness. ~\citeauthor{carlini2019evaluating} specifies a threat model for evaluating a defense method including a set of assumptions about the adversary's goals, capabilities, and knowledge, which are briefly delineated as follows:
%  \begin{itemize}
%      \item Adversary's goals could be either simply deceiving a model to make the wrong prediction to any classes from a perturbed input or making the model misclassify a specific class to an intended class. They are known as \textit{untargeted} and \textit{targeted} modes, respectively.
%      \item Adversary's capabilities define reasonable constraints imposed on the attackers. For instance, a $L_p$ certified robust model is determined with the worst-case loss function $\mathcal{L}$ for a given perturbation budget $\epsilon$:
%      \begin{equation}
%          \mathbb{E}_{(x,y)\sim \mathcal{D}} \Bigg[\max_{ \tilde{x}\in {B}_{p} (x, \epsilon)} \mathcal{L}(f(\tilde{x}), y) \Bigg],
%      \end{equation}
%      where ${B}_{p} (x, \epsilon)=\{u \in \mathbb{R}^{\mathcal{I}}: ||u-x||_{p} \leq  \epsilon\}$.
%      \item Adversary's knowledge indicates what knowledge of the threat model that an attacker is assumed to have. Typically, \textit{white-box} and \textit{black-box} attacks are two most popular scenarios studied. The white-box settings assume that attackers have full knowledge of the model's parameters and its defensive scheme. In contrast, the black-box settings have varying degrees of access to the model's parameter or the defense.
%  \end{itemize}
 
% Bearing these assumptions about the adversary, we describe how a defense model generates adversarial samples for its training in the following section.   
% \begin{algorithm}[!t]
% \caption{\SystemName: AT with SW and optimal Jacobian regularization.}

% \begin{algorithmic}[!b]
% \Require   DNN $f$
% parameterized by $\theta$, training dataset $\mathcal{D}$. Number of projections $K$. Maximum perturbation $\epsilon$, step size $\eta$, number of adversarial iteration $P$. Loss' hyper-parameters $\lambda_J$ and $\lambda_{SW}$. Learning rate $\alpha$ and batch size of $\mathcal{B}$.
%     \While{not converged}
%         \For{$\{(x_{i},\textrm{ } y_{i})\}_{\mathcal{B}} \in \mathcal{D}$}
%             \State $\nu :  = \{z_{i}\}_{\mathcal{B}} = f(x_i; {\theta})| _{i=1,..,\mathcal{B}}$ 
%             \State  \texttt{\# Generate adv. samples}
%             \For{iteration $t \gets 1$ to $P$}
%                 \State $\tilde{x_i} = \Pi_{x}^{\epsilon}(\tilde{x_i} +\eta \cdot \text{sgn} (\nabla _{\tilde{x_i}} \mathcal{L}(\tilde{x_i}, y_i)))| _{i=1,..,\mathcal{B}}$  
%             \EndFor
%             \State $\mu :  = \{\tilde{z_i}\}_{\mathcal{B}} = f(\tilde{x_i}; {\theta})| _{i=1,..,\mathcal{B}}$  
            
%             \State $SW \gets 0$ 
%             \State $\Delta_i \gets 0|_{i=1,..,\mathcal{B}}$ 
%             \For{iteration $k \gets 1$ to $K$}
%                 \State \texttt{\# Sample $\hat{v}_k$ from $\mathcal{S}^{C-1}$}           \State $\hat{v}_k$ $\gets$  $\mathcal{U}(\mathcal{S}^{C-1})$ 
%                 \State \texttt{\# Add SW under projection $\hat{v}_k$}
%                 \State $SW \gets SW + \psi  \left(\tau_1 \circ \mathcal{R}_{\hat{v}_k}\mu, \tau_2 \circ\mathcal{R}_{\hat{v}_k}\nu \right)$ 
%                 \State \texttt{\# Calculate samples' movements under $\hat{v}_k$}
%                 \State $m_k \gets \left(\tau_1^{-1} \circ \tau_2 \circ \mathcal{R}_{\hat{v}_k}\nu - \mathcal{R}_{\hat{v}_k}\mu \right) \otimes \hat{v}_k$ 
%                 \State $\Delta_i \gets \Delta_i + m_{k,i}|_{i=1,..,\mathcal{B}}$
%             \EndFor
%             \State $\Delta_i \gets \Delta_i / ||\Delta_i||_2 $  $ |_{i=1,..,\mathcal{B}}$
%             \State $\mathcal{L} \gets \sum^{\mathcal{B}} \left( \mathcal{L_{XE}}(\tilde{x_i}, y_i) + \lambda_J ||J(x_i | \Delta_i)||_{F}^{2} \right) $  \\\hspace{3cm}  $ +\quad \lambda_{SW}SW(\mu, \nu) $
%             \State $\theta \gets \theta - \alpha \cdot \nabla _{\theta} \mathcal{L}$ 
%         \EndFor
%     \EndWhile
% \end{algorithmic}

% \label{alg:opt_move}
% \end{algorithm}

%  \subsection{Adversarial Sample in AT}
%  Among multiple attempts to defend against adversarial perturbed samples, adversarial training (AT) is known as the most successful defense method. In fact, AT is an data-augmenting training method that originates  from the work of \cite{goodfellow2014explaining}, where crafted adversarial samples are created by the fast gradient sign method (FGSM), and mixed into the mini-batch training data. Subsequently, a wide range of studies focus on developing powerful attacks \cite{kurakin2016adversarial,dong2018boosting,carlini2017towards,croce2020minimally}. Meanwhile, in the opposite direction to the adversarial attack, there are also several attempts to resist against adversarial examples \cite{kannan2018adversarial,zhang2019defense,shafahi2019adversarial}. In general, a defense model is optimized by solving a minimax problem:
%   \begin{equation}
%       \min _{\theta} \Big[\max_{\tilde{x} \in {B}_{p}(x, \epsilon)} \mathcal{L_{XE}}(\tilde{x}, y; \theta) \Big ],
%   \end{equation}
% where the inner maximization tries to successfully create perturbation samples subjected to an $\epsilon$-radius ball $B$ around the clean sample $x$ in $L_p$ space. The outer minimization tries to adjust the model's parameters to minimize the loss caused by the inner attacks. Among existing defensive AT, PGD-AT \cite{madry2017towards} becomes the most popular one, in which the inner maximization is approximated by the multi-step projected gradient (PGD) method:
%  \begin{equation}
%  \label{eqn:pgd_gen}
%      \tilde{x}_{t+1} = \Pi_{x}^{\epsilon}(\tilde{x_{t}} +\eta \cdot \text{sgn} (\nabla _{\tilde{x_{t}}} \mathcal{L}(\tilde{x_{t}}, y))),
%  \end{equation}
%  where $\Pi_{x}^{\epsilon}$ is an operator that projects its input into the feasible region ${B}_{\infty}(x, \epsilon)$, and $\eta \in \mathbb{R}$ is called step size. The loss function in Eq.~\ref{eqn:pgd_gen} can be modulated to derive different variants of generation mechanism for adversarial samples in AT. For example, Zhang \textit{et al.} \cite{zhang2019theoretically} utilizes the loss between the likelihood of clean and adversarial samples for updating the adversarial samples. In our work, we use Eq.~\ref{eqn:pgd_gen} as our generation mechanism for our AT framework.
 

%  \subsection{Adversarial attacks}
%  \label{supp:adv_attacks}
%  For the \textit{white-box} attacks, we deploy the followings in our experiement:
%  \begin{itemize}
%     \item \textbf{\textit{FGSM}}\cite{goodfellow2014explaining}: Adversarial samples are generated under the $L_{\infty}$ norm as follows:
%     \begin{equation}
%         x^{adv} = x + \epsilon \cdot \text{sgn}(\nabla_{x}\mathcal{L_{XE}}(x, y)).
%     \end{equation}
%     % \item \textbf{\textit{BIM}}~\cite{kurakin2016adversarial}: BIM with $L_{\infty}$ constraint creates adversarial samples by iteratively taking multiple gradient updates as follows:
%     % \begin{equation}
%     %     x_{t+1}^{adv} = \text{clip}_{x,\epsilon}\left ( x_{t}^{adv} + \eta \cdot \text{sgn}(\nabla_{x}\mathcal{L_{XE}}(x_{t}^{adv}, y))\right),
%     % \end{equation}
%     % where $\eta$ is a perturbation step size. 
%     \item \textbf{\textit{PGD}}~\cite{madry2017towards}: Its initial point $x_0^{adv}$ is uniformly sampled from the neighbor around $x$.
%     \item \textbf{\textit{MIM}}~\cite{dong2018boosting}: It is the integration of BIM and a momentum with decay factor $\mu$ as:
%     \begin{equation}
%         g_{t+1} = \mu\cdot g_{t} + \frac{\nabla_{x}\mathcal{L_{XE}}(x_{t}^{adv}, y)}{|| \nabla_{x}\mathcal{L_{XE}}(x_{t}^{adv}, y)|| _{1}},
%     \end{equation}
%     and the adversarial samples are then updated by:
%     \begin{equation}
%         x_{t+1}^{adv} = \text{clip}_{x,\epsilon}\left ( x_{t}^{adv} + \eta \cdot \text{sgn}(g_{t+1})\right),
%     \end{equation}
%     \item \textbf{\textit{C\&W}}~\cite{carlini2017towards}: Adversarial samples are generated under $L_2$ norm by solving:
%     \begin{equation}
%         x^{adv}  = \argmin _{x^{\prime}}\big[ c \cdot \max(f(x^{\prime})_{y} - \max _{i\neq y}f(x^{\prime})_{i}, 0)  + ||x^{\prime}-x ||_{2} \big],
%     \end{equation}
%     where $c$ is a variable that found by binary search.
%     \item \textbf{\textit{FAB}}~\cite{croce2020minimally}: It iteratively generates adversarial samples lie on the decision boundaries of a classifier with minimal perturbation. We implement both targeted and untargeted FAB attacks.
%     % \item \textbf{FAB} with a relative magnitude parameter $\alpha$ generate an adversarial samples lie on the decision boundaries with minimal perturbation as follows:
%     % \begin{equation}
%     %     x^{adv}_{t+1} = \text{proj}_{C}\big( (1-\alpha)(x^{adv}_{t} + \eta\delta_{t}) + \alpha(x+\eta\delta^{\text{orig}}_{t})\big),
%     % \end{equation}
%     % where proj$_{C}$ is a projection on the hyperplane $\pi$ with constraint set $C$, and $\delta_{t}$ and $\delta^{\text{orig}}_{t}$ indicate the distances of $x_{t}^{adv}$ and $x$ to $\pi$ inside $C$. 
% \end{itemize}

% Meanwhile, we utilize the following \textit{black-box} attacks:
% \begin{itemize}
%     \item \textbf{\textit{Square}}~\cite{andriushchenko2020square}. It is an efficient random search-based adversarial attack that generates $L_{\infty}$ adversarial sample by continuously adding small windows of perturbation $h$ to the clean image, which has size of $w \times w \times c$ until success:
%     \begin{equation}
%         x^{adv}_{t+1} = x^{adv}_{t} + \delta, \delta \sim P(\epsilon, h_{t+1}, w, c,x^{adv}_{t}, x),
%     \end{equation}
%     where $P$ is the uniform distribution with values are in $\{-2\epsilon, 2\epsilon\}$.
%     \item \textbf{\textit{SimBA}}~\cite{guo2019simple}. This approach iteratively chooses a random vector  $q$ from an orthonormal input space $Q$ and adds this vector to the clean image as follows:
%     \begin{equation}
%         x^{adv}_{t+1} = x^{adv}_{t} + \alpha \cdot q, \alpha \in \{\epsilon, -\epsilon \}.
%     \end{equation}
% \end{itemize}

%  \section{Detailed Implementation of ~\SystemName}
%  \label{app:implementation}
 


% \begin{lstlisting}[language=Python, caption={OTJR Python Code Implementation}, label={listing:pseudocode}]
% import torch
% from torch.autograd import grad
% """
% Arguments:
%     x: (batch_size, 3, H, W)
%     lb: (batch_size, )
%     src, tgt: (batch_size, C)
%     K: number of projection
% """
% # generate random projections
% projs = torch.randn(size=(dim, K))
% l2 = projs.pow(2).sum(0, True).pow(0.5)
% # normalize projections
% projs = projs.div(l2) 
% # project `src` and `tgt` distributions
% src_p = src.matmul(projs).transpose(0,1)
% tgt_p = tgt.matmul(projs).transpose(0,1)
% # ascending sort
% tgt_sort, _ = torch.sort(tgt_p,1)
% src_rank = torch.argsort(
%                 torch.argsort(src_p), 1)
% # tau_1^(-1) o tau_2 in Eqn.10
% tgt2src =  torch.gather(input=tgt_sort, 
%                 dim=1, index=src_rank)
% # movement
% src_move = tgt2src - src_p
% sw = src_move.pow(2).sum() # Eqn.9
% m = torch.zeros_like(src)
% for i in range(K):
%   m += torch.outer(torch.squeeze(
%         src_move[i]), projs[:,i]) # Eqn. 10
% l2 = m.pow(2).sum(1, True).pow(0.5)
% delta = m.div(l2) # Eqn. 11
% J = grad(x.reshape(-1),
%          delta.reshape(-1),
%          retain_graph=True,
%          create_graph=True
%          ).norm().pow(2) # Eqn. 12
% \end{lstlisting}
 




% % \subsection{\SystemName\ Implementation in Python}
%  \label{app:implementation_python}
%  We include the pseudo-code as an example, implemented with PyTorch, of how the $SW$ distance and optimal Jacobian regularization are calculated (List \ref{listing:pseudocode}). We use $\psi$ function as the square of $l_2$ loss. Note that $\psi  \left(\tau_1 \circ \mathcal{R}_{\hat{v}_k}\mu, \tau_2 \circ\mathcal{R}_{\hat{v}_k}\nu \right) = \psi  \left(\tau_1^{-1} \circ \tau_2 \circ\mathcal{R}_{\hat{v}_k}\nu, \mathcal{R}_{\hat{v}_k}\mu \right)$, since $l_2$ loss is applied in element-wise, and $\tau_1^{-1}$ now becomes the rank of the values in $\mathcal{R}_{\hat{v}_k}\mu$.
 






% As an simple illustration, Fig.~\ref{fig:opt_move_toy} provide a toy example of the optimal movement directions in 2 dimensional space obtained from ~Eq.~\ref{eqn:opt_move}.

% jac_mat = torch.autograd.grad(lb.reshape(-1), tgt.reshape(-1), delta.reshpae(-1), retain_graph=True, create_graph=True)
% J = torch.norm(jac_mat)**2 # Eqn. 12
% \begin{algorithm*}[t!]
% \caption{AT with SW and informative Jacobian regularization}
% \label{alg:at_infor_reg}
% \begin{algorithmic}[1]
% \Require DNN $f$
% parameterized by $\theta$, training dataset $\mathcal{D}$. Number of projection $K$. Maximum perturbation $\epsilon$, step size $\eta$, number of adversarial iteration $P$. Loss' hyper-parameters $\lambda_J$ and $\lambda_{SW}$. Learning rate $\alpha$ and a mini-batch size of $\mathcal{B}$.
% \While{not converged}
%     \For{$\{(x_{i},\textrm{ } y_{i})\}_{\mathcal{B}} \in \mathcal{D}$}
%         \State $\nu :  = z_{i} = f_{\theta}(x_i)| _{i=1,..,\mathcal{B}}$ \Comment{\textit{forward a batch of clean samples through the model}}
%         \For{iteration $t \gets 1$ to $P$}
%             \State $\tilde{x_i} = \Pi_{x}^{\epsilon}(\tilde{x_i} +\eta \cdot \text{sgn} (\nabla _{\tilde{x_i}} \mathcal{L}(\tilde{x_i}, y_i)))| _{i=1,..,\mathcal{B}}$  \Comment{{\emph{generate adv. samples by $L_{\infty}$-PGD in $P$ iterations}}}
%         \EndFor
%         \State $\mu :  = \tilde{z_i} = f_{\theta}(\tilde{x_i})| _{i=1,..,\mathcal{B}}$  \Comment{\textit{forward a batch of adv. samples through the model}}
        
%         \State $SW \gets 0$ \Comment{\emph{initialize SW loss}}
%         \State $\Delta_i \gets 0|_{i=1,..,\mathcal{B}}$ \Comment{\emph{initialize $\mathcal{B}$ Jacobian projections}}
%         \For{iteration $k \gets 1$ to $K$}
%             \State $\hat{v}_k$ $\gets$  $\mathcal{U}(\mathcal{S}^{C-1})$ \Comment{{\emph{uniformly sample $\hat{v}_k$ from $\mathcal{S}^{C-1}$}}}
            
%             \State $SW \gets SW + \psi  \left(\tau_1 \circ \mathcal{R}_{\hat{v}_k}\mu, \tau_2 \circ\mathcal{R}_{\hat{v}_k}\nu \right)$ \Comment{{\emph{add SW under projection $\hat{v}_k$}}}
%             \State $m_k \gets \left(\tau_1^{-1} \circ \tau_2 \circ \mathcal{R}_{\hat{v}_k}\nu - \mathcal{R}_{\hat{v}_k}\mu \right) \otimes \hat{v}_k$ \Comment{{\emph{calculate samples' movements under $\hat{v}_k$}}}
%             \State $\Delta_i \gets \Delta_i + m_{k,i}|_{i=1,..,\mathcal{B}}$
%         \EndFor
%         \State $\Delta_i \gets \Delta_i / ||\Delta_i||_2 $  $ |_{i=1,..,\mathcal{B}}$
%         \State $\mathcal{L} \gets \sum^{\mathcal{B}} \left( \mathcal{L_{XE}}(\tilde{x_i}, y_i) + \lambda_J ||J(x_i | \Delta_i)||_{F}^{2} \right)  $  $ +\quad \lambda_{SW}SW(\mu, \nu) $ \Comment{{\emph{overall loss}}}
%         \State $\theta \gets \theta - \alpha \cdot \nabla _{\theta} \mathcal{L}$ \Comment{{\emph{update model's parameters $\theta$}}}
%     \EndFor
% \EndWhile
% \end{algorithmic}
% \label{alg:opt_move}
% \end{algorithm*}
 
  
\section{Further Empirical Analyses}
In this section, we conduct intensive experiments on various AT methods. 
\subsection{Smaller backbone experiment}
Similar to our main experiment, we provide a comparison between our OTJR training framework with different defense methods  in Table~\ref{tb:white_attack_wrn28} with a smaller backbone - WRN28. Hyper-parameters are kept the same as we used. We note that our proposed framework achieves significant improvements across various while-box and black-box attacks. 

\begin{table*}[!t]
\label{tb:white_attack_wrn28}
\caption{\textbf{Classification accuracy ($\%$) under \textit{white-box}, \textit{black-box} attacks and \textit{AutoAttack}.} Different defense methods trained on~\CIFAR-10 and~\CIFAR-100 datasets using WRN28, where underscript indicates training epochs.  Best results are in \textbf{bold}.}

\centering
\resizebox{0.92\textwidth}{!}{%
    \begin{tabular}{c | l | c | c c c c c c c | c c | c } 
    \Xhline{3\arrayrulewidth}
    Dataset & Defense    &\textit{Clean}    &PGD$^{20}$    &PGD$^{100}$    &$L_{2}$-PGD       &MIM &FGSM   &CW     &FAB   &Square & SimBa  &\emph{AutoAtt}\Tstrut\Bstrut\\
    \Xhline{2\arrayrulewidth}
    \multirow{5}{*}{\rotatebox[origin=c]{90}{\centering \small{~\CIFAR-10}}}    
    &TRADES    &\textit{84.52}    &54.06    &53.85    &61.07    &54.10    &60.27    &53.08    &53.43    &62.35    &70.84    &51.91\Tstrut\\
&ALP    &\textit{85.90}    &47.31    &46.77    &55.98    &47.16    &57.96    &47.73    &53.35    &59.43    &68.82    &46.46\\
&PGD-AT   &\textit{86.39}    &46.69    &46.22    &56.33    &46.61    &56.36    &47.44    &48.12    &58.96    &68.40    &45.98\\
&SAT    &\textit{82.85}    &53.34    &53.06    &60.49    &53.34    &59.57    &51.69    &52.22    &60.73    &69.79    &50.50\Bstrut\\ 
    &\SystemName~(\emph{Ours}) & \textit{83.63}    &\textbf{55.25}    &\textbf{54.87}    &\textbf{64.16}    &\textbf{55.08}    &\textbf{60.48}    &\textbf{53.41}    &\textbf{53.58}    &\textbf{62.61}    &\textbf{71.73}    &\textbf{52.03}\Bstrut \\
    \Xhline{3\arrayrulewidth}
    \multirow{5}{*}{\rotatebox[origin=c]{90}{\centering \small{~\CIFAR-100}}}    
    &TRADES       &\textit{57.52}    &30.27    &30.07    &36.10    &30.19    &32.78    &28.21    &27.47    &33.41    &44.34    &26.46\Tstrut\\
&ALP    &\textit{61.08}    &25.83    &25.40    &33.55    &25.74    &30.97    &25.59    &25.14    &32.57    &43.78    &23.89\\
&PGD-AT   &\textit{59.73}    &23.45    &23.11    &31.08    &23.47    &28.92    &24.62    &23.86    &31.07    &41.27    &22.57\\
&SAT    &\textit{53.27}    &26.82    &26.63    &31.99    &26.67    &30.96    &25.30    &26.26    &30.83    &40.45    &24.31\Bstrut\\ 


    &\SystemName~(\emph{Ours})   &\textit{57.82}    &\textbf{31.84}    &\textbf{31.71}    &\textbf{42.87}    &\textbf{31.84}    &\textbf{34.08}    &\textbf{29.65}    &\textbf{28.25}    &\textbf{36.06}    &\textbf{49.44}    &\textbf{27.78}\Bstrut \\
    \Xhline{3\arrayrulewidth}
    \end{tabular}%
  }

\vspace{-2pt}
\end{table*}



% \begin{figure}[t]
% \centering
% \includegraphics[width=0.44\textwidth]{  Figures/ranJac_vs_swJac.pdf}
% \caption{ Accuracy ($\%$) of WRN34 model trained with JR having random projections, and informative projections respectively, under different number of $L_\infty$-PGD iteration steps on ~\CIFAR-10 test dataset. \textcolor{red}{(update needed)}} 
% \label{fig:randJac_vs_WDJac}
% \end{figure}







% \begin{figure*}[!t]
%     \begin{minipage}[t]{.3\textwidth}
%       \centering
%         \includegraphics[width=1\textwidth]{Figures/ranJac_vs_swJac.pdf}
%         \caption{ Accuracy ($\%$) of WRN34 model trained with JR having random projections, and informative projections respectively, under different number of $L_\infty$-PGD iteration steps on ~\CIFAR-10 test dataset.} 
%         \label{fig:randJac_vs_WDJac}
%   \end{minipage}%
%   \hfill
%     \begin{minipage}[t]{.60\textwidth}
%         \centering
%     % \includegraphics[width=11.8cm, height=3.8cm]{  Figures/main_cross_section.png}
%     \subfloat[\centering \small Without regularization]{{\includegraphics[width=3.0cm]{Figures/cross_decision/00375_xe_r13.0.png}}}%
%     \hfill
%     \subfloat[\centering \small JR with random projections]{{\includegraphics[width=3.0cm]{Figures/cross_decision/00375_rand_jac_r119.5.png} }}%
%     \hfill
%     \subfloat[\centering \small JR with informative projections]{{\includegraphics[width=3.0cm]{Figures/cross_decision/00375_sw_jac_r155.6.png} }}%
%     \hfill
%     \subfloat{{\includegraphics[ height=3.4cm]{Figures/cross_decision/cross_section_legend.png} }}%

%     \caption{ \textbf{Cross sections of decision boundaries in the input space}. To generate the cross section cells, a test sample (central black dot) and two chosen random vectors $\in \mathbb{R}^{32 \times 32 \times 3}$ (vertical and horizontal axes), which are then orthonormalized to form a cross section plane. Different colors indicate different classes in ~\CIFAR-10, whereas only closely related classes are shown. The contours are formed by softmax values, and the circles are drawn by the distance from test samples to the closest decision boundaries. (a) Without regularization, the decision cell is tiny and the model is vulnerable to perturbations. (b) Jacobian regularization with random projections increases the classification margins. And (c) Jacobian regularization with our informative projections further expand the decision boundaries, ensuring the stability of the trained model.} %
%     \label{fig:cross_section}
%     \end{minipage}%
%   \end{figure*}

% \begin{figure*}[t!]
%     \centering
%     % \includegraphics[width=11.8cm, height=3.8cm]{  Figures/main_cross_section.png}
%     \subfloat[\centering \small Without regularization]{{\includegraphics[width=3.6cm]{Figures/cross_decision/00375_xe_r13.0.png}}}%
%     \hfill
%     \subfloat[\centering \small JR with random projections]{{\includegraphics[width=3.6cm]{Figures/cross_decision/00375_rand_jac_r119.5.png} }}%
%     \hfill
%     \subfloat[\centering \small JR with informative projections]{{\includegraphics[width=3.6cm]{Figures/cross_decision/00375_sw_jac_r155.6.png} }}%
%     \hfill
%     \subfloat{{\includegraphics[ height=3.4cm]{Figures/cross_decision/cross_section_legend.png} }}%

%     \caption{ \textbf{Cross sections of decision boundaries in the input space}. To generate the cross section cells, a test sample (central black dot) and two chosen random vectors $\in \mathbb{R}^{32 \times 32 \times 3}$ (vertical and horizontal axes), which are then orthonormalized to form a cross section plane. Different colors indicate different classes in ~\CIFAR-10, whereas only closely related classes are shown. The contours are formed by softmax values, and the circles are drawn by the distance from test samples to the closest decision boundaries. (a) Without regularization, the decision cell is tiny and the model is vulnerable to perturbations. (b) Jacobian regularization with random projections increases the classification margins. And (c) Jacobian regularization with our informative projections further expand the decision boundaries, ensuring the stability of the trained model.} %
%     \label{fig:cross_section}
% \end{figure*}








% \subsection{Progression of input derivatives}
% \label{app:prog_input_der}
% Table~\ref{tb:prog_input_der} shows the progression of input derivatives after different PGD white-box attack iterations. We note that the perturbation noises created by the output loss derivatives increase when more attack iterations are applied. Nevertheless, our proposed framework maintains the smallest silence throughout the iterations. This experiment further proves our ~\SystemName\'s robustness performance.\\


\subsection{Activation Magnitude}
\label{app:magnitude_activation}
Fig.~\ref{fig:mag_activation_all} illustrates activation magnitudes at the penultimate layer of WRN34 for all AT frameworks. While AT methods can adjust the adversarial magnitudes closed to their clean, their overall magnitudes keep high, especially PGD-AT.  By balancing the input Jacobian matrix and output distributions, our proposed method can reduce the model's surprises when encountering perturbed samples. \\

\begin{figure*}[t]
    \centering
    \subfloat[\centering $\mathcal{XE}$ ]{{\includegraphics[width=0.42\linewidth]{ Figures/magnitude_activation_xe_pgd20.pdf}}}%
    \hfill
    \subfloat[\centering TRADES ]{{\includegraphics[width=0.42\linewidth]{ Figures/magnitude_activation_trades_pgd20.pdf}}}%
    \hfill
    \subfloat[\centering ALP]{{\includegraphics[width=0.42\linewidth]{ Figures/magnitude_activation_alp_pgd20.pdf}}}%
    \hfill
    \subfloat[\centering PGD-AT]{{\includegraphics[width=0.42\linewidth]{ Figures/magnitude_activation_pgd_pgd20.pdf}}}%
    \hfill
    \subfloat[\centering SAT  ]{{\includegraphics[width=0.42\linewidth]{ Figures/magnitude_activation_sink_pgd20.pdf} }}%
    \hfill
    \subfloat[\centering~\SystemName~(\textit{Ours}) ]{{\includegraphics[width=0.42\linewidth]{ Figures/magnitude_activation_trans_swdj_pgd20.pdf}}}%
    \hfill
    \caption{\textbf{Magnitude of activation} at the penultimate layer for models trained with different defense methods. Our \SystemName~ can regulate adversarial samples' magnitudes similar to clean samples' while well suppressing both of them.}%
    \label{fig:mag_activation_all}
\end{figure*}

% \begin{table*}[t]
%     \centering
%     \subfloat[\centering $\mathcal{XE}$ ]{{\includegraphics[width=0.42\linewidth]{ Figures/magnitude_activation_xe_pgd20.pdf}}}%
%     \hfill
%     \subfloat[\centering TRADES ]{{\includegraphics[width=0.42\linewidth]{ Figures/magnitude_activation_trades_pgd20.pdf}}}%
%     \hfill
%     \subfloat[\centering ALP]{{\includegraphics[width=0.42\linewidth]{ Figures/magnitude_activation_alp_pgd20.pdf}}}%
%     \hfill
%     \subfloat[\centering PGD-AT]{{\includegraphics[width=0.42\linewidth]{ Figures/magnitude_activation_pgd_pgd20.pdf}}}%
%     \hfill
%     \subfloat[\centering SAT  ]{{\includegraphics[width=0.42\linewidth]{ Figures/magnitude_activation_sink_pgd20.pdf} }}%
%     \hfill
%     \subfloat[\centering~\SystemName~(\textit{Ours}) ]{{\includegraphics[width=0.42\linewidth]{ Figures/magnitude_activation_trans_swdj_pgd20.pdf}}}%
%     \hfill
%     \caption{\textbf{Magnitude of activation} at the penultimate layer for models trained with different defense methods. Our \SystemName~ can regulate adversarial samples' magnitudes similar to clean samples' while well suppressing both of them.}%
%     \label{fig:abs}
% \end{table*}

% \begin{comment}
% \begin{figure*}[th!]
% \centering
% \includegraphics[width=0.8\linewidth]{ Figures/SAT_vs_ours_model_der.pdf}
% \caption{\textbf{Average gradient norm ratios between adversarial and clean samples.} The ratios of $\mathbb{E}(||\nabla_{\theta_i}\mathcal{L}(\tilde{x})|| / ||\nabla_{\theta_i}\mathcal{L}({x})||)$ with respect to the model's parameters $\theta_{i}$ on ~\CIFAR-10 dataset. While both methods can achieve similar variance at middle layers, our~\SystemName~ successfully learns to balance clean and adversarial samples, shown by ratios' values mainly close to 1, and obtains smaller ratios at beginning layers, which benefits for constraining generating perturbation.}
% \label{fig:model_der_sat_vs_ours}
% \end{figure*}
% \begin{figure}[t]
% \centering
% \includegraphics[width=0.80\linewidth]{ Figures/SAT_vs_ours_input_der.pdf}
% \caption{\textbf{Magnitude of $||\nabla_{x} \mathcal{L}_{\mathcal{XE}}||_1$} for WRN34 trained with SAT and our ~\SystemName. The red and green-filled areas range from minimum to maximum values of each sample. Our~\SystemName~can mostly achieve lower derivative's norm across samples, which means constraint perturbation's magnitude added to clean samples for attacking the model.} 
% \label{fig:input_der_sat_vs_ours}
% \end{figure}

% \subsection{~\SystemName~ vs. Sinkhorn Divergence}
% \label{app:sw-jac_vs_sink}
% Similar to \cref{subsec:exp_analysis}, we describe the differences between our proposed~\SystemName~and SAT in terms of the defense model's gradients at different layers.  At every intermediate layer in an attacked model, our framework provides a smaller ratio between perturbed and clean sample's gradients than SAT training as shown in \ref{fig:model_der_sat_vs_ours}. It is worth noting that the gradients are derived on unobserved samples in the test set. Therefore, in the forward path, perturbations in images have fewer effects on the model's prediction, indicated by smaller ratios at most of the layers.  Moreover, in the backward path, since the abused model's gradients are deployed to generate more perturbations, the model achieves better robustness when it can produce smaller gradients with respect to its inputs. Additionally, our proposed ~\SystemName~ successfully balances the adversarial samples and the clean samples, showing ratios' values in most of the layers closed to 1. Finally, \ref{fig:input_der_sat_vs_ours} more justify our framework's benefits while it can remarkably suppress the input gradient compared to barely using Sinkhorn divergence on the entire ~\CIFAR-10 test set.
% \end{comment}


\section{Broader Impact}
Using machine learning models in a practical system requires sufficient accuracy and robustness when encountering different environment scenarios. Our primary aim behind this work is to develop a training framework that can enhance a DNN model's robustness under a wide range of adversarial attacks, including white-box and black-box settings. To this end, we propose our novel~\SystemName~framework that can optimize the conventional Jacobian regularization and align the output distributions. We believe our work makes essential progress in combining both adversarial training and input-output Jacobian regularization, which have been so far overlooked, for achieving a more reliable model.

\clearpage
\onecolumn

\section{Gallery of Cross Decision Cells}
This section provides more illustrations about the benefit of optimal Jacobian regularization in increasing decision boundaries for a defense model.
\label{app:gallery_decision}
% \begin{figure*}[h!]
%     \centering
%     \subfloat{{\includegraphics[width= 3.0cm]{ Figures/cross_decision/00232_xe_r19.1.png}}}%
%     \quad
%     \subfloat{{\includegraphics[width= 3.0cm]{ Figures/cross_decision/00232_rand_jac_r55.3.png} }}%
%     \quad
%     \subfloat{{\includegraphics[width= 3.0cm]{ Figures/cross_decision/00232_sw_jac_r94.9.png} }}%
%     \quad
%     \subfloat{{\includegraphics[width= 3.0cm]{ Figures/cross_decision/00396_xe_r8.1.png}}}%
%     \quad
%     \subfloat{{\includegraphics[width= 3.0cm]{ Figures/cross_decision/00396_rand_jac_r87.0.png} }}%
%     \quad
%     \subfloat{{\includegraphics[width= 3.0cm]{ Figures/cross_decision/00396_sw_jac_r125.2.png} }}%
%     \quad
%     \subfloat{{\includegraphics[width= 3.0cm]{ Figures/cross_decision/00339_xe_r27.9.png}}}%
%     \quad
%     \subfloat{{\includegraphics[width= 3.0cm]{ Figures/cross_decision/00339_rand_jac_r50.8.png} }}%
%     \quad
%     \subfloat{{\includegraphics[width= 3.0cm]{ Figures/cross_decision/00339_sw_jac_r128.3.png} }}%
%     \quad
%     \subfloat{{\includegraphics[width= 3.0cm]{ Figures/cross_decision/00386_xe_r31.6.png}}}%
%     \quad
%     \subfloat{{\includegraphics[width= 3.0cm]{ Figures/cross_decision/00386_rand_jac_r147.7.png} }}%
%     \quad
%     \subfloat{{\includegraphics[width= 3.0cm]{ Figures/cross_decision/00386_sw_jac_r175.0.png} }}%
%     \caption{\textbf{Cross sections of decision boundaries in the input space}. \textbf{Left column:} Model trained without regularization. \textbf{Middle column:} Model train with Jacobian regularization having random projection. \textbf{Right column:} Model trained with Jacobian regularization having informative projections. Our optimal JR benefits from the informative directions obtained by SW distance, and thus helps the model to regularize these sensitive direction of clean samples and produce larger decision cells.} %
%     \label{fig:more_cross_section}
% \end{figure*}
\vspace{-14pt}

\clearpage
