\section{Methodology}
\subsection{Overview}
We present \model{}, a self-supervised learning method that ensures both group and counterfactual fairness criteria while maintaining high representation quality with the design of two special losses. We use a pictorial sketch in Figure~\ref{fig:main_training} to describe them.

First is the \textbf{fairness-aware contrastive loss}, which treats individuals in \textit{counterfactual relationships alike} (i.e., counterfactual fairness) and ensures \textit{non-distinguishable embeddings} over sensitive attributes (i.e., group fairness). A key to this loss is the design of a sample generator that produces a counterfactual version $\mathbf{x}_{\text{cnt}}^{s'}$ of an input item $\mathbf{x}^{s}$ by maintaining its latent characteristics but flipping the sensitive attribute $s \rightarrow s'$ (Sec. 3.2). In a job interview, for instance, this corresponds to a hypothetical decision if the applicant's gender were to change, with all other latent traits, such as education and work experience, remaining unchanged~\cite{kusner2017counterfactual}. Given a dataset $\mathcal{D}$ with sensitive attributes $S$ like gender, the fairness-aware contrastive loss is defined on the batch $\mathcal{B}_{s}$, whose data instances share the same sensitive attribute $s\in S$ (e.g., gender is female). The loss maximizes agreements between the original item and its counterfactual version generated by our generator (i.e., ensuring counterfactual fairness) and minimizes agreements between items with the same sensitive attribute (i.e., ensuring group fairness) (Sec. 3.3). \looseness=-1



\begin{figure*}[t!]
\centering
\begin{minipage}[t]{0.68\textwidth}
       \centering\includegraphics[width=\textwidth]{figure/vae_renewal_v2.pdf}
      \caption{Design of our counterfactual sample generator. Adversarial training over VAE eliminates sensitive information $s$ from $\mathbf{u}$. Cyclic consistency loss makes training more stable by ensuring the double-flipped sample ($\mathbf{x}^s_{\text{cyc}}$) to be same as the original ($\mathbf{x}^s$). \looseness=-1}
      \label{fig:vae}
\end{minipage}
\hspace{5mm}
\begin{minipage}[t]{0.25\textwidth}
       \centering\includegraphics[width=\textwidth]{figure/tabmix_renewal.pdf}
      \caption{Illustration of \tabmix{} operation for self-knowledge distillation ($m$ denotes a mask). \looseness=-1}
      \label{fig:tabmix}
\end{minipage}
\end{figure*}

Second is the \textbf{self-knowledge distillation loss} for extracting semantic information from the instance and maintaining the performance of downstream tasks (Sec. 3.4). Consider a Siamese network that has two heads: one for the student branch and the other for the teacher branch. We impose the perturbed instance's embedding from the student branch to be similar to that of the original instance from the teacher branch. The rationale behind this treatment is that a perturbation that does not largely deform the original content should not change the learned representation. We introduce our unique perturbation module \tabmix{} for an efficient self-knowledge distillation process. \looseness=-1


\subsection{Counterfactual Sample Generation}
Our sample generator creates data following the definition of a counterfactual relationship in~\cite{kusner2017counterfactual}. Let us denote three variables $V, U, R$, where $V$ is a set of observable variables, including the sensitive attribute $S$ and other attributes $X$. $U$ is a set of latent variables independent of $S$ or $V$. $R$ is a set of functions representing causal relationships from $U$ to $V$, or between $V$. Considering a causal model with ($V, U, R$), counterfactual inference works in three steps: \looseness=-1
\begin{enumerate}[leftmargin=*]
\item Calculate the posterior distribution of latent variables $U$ from the input instance.
\item Choose a target sensitive attribute $s\in S$ (e.g., gender) and reformulate a set of functions $R$ assuming the sensitive attribute $s \in S$ is fixed in the causal graph.
\item Infer $V$ from $U$ and $s \in S$ using re-formulated $R$. 
\end{enumerate}
Extracting the latent variable $U$ is non-trivial because it should explain observable variables sufficiently without revealing information about sensitive attributes. 


For this task, we introduce the cyclic variational autoencoder (C-VAE) depicted in Figure~\ref{fig:vae}. Our counterfactual sample generator is trained in the form of $p(\mathbf{x}^s, \mathbf{u}, s) = p(\mathbf{u})p(s)p(\mathbf{x}^s|\mathbf{u}, s)$, where $\mathbf{u}$ and $s$ represent the latent variable and the sensitive attribute in the input $\mathbf{x}^s$. $p(\mathbf{u})$ and $p(s)$ represent the prior distribution of the latent variable $\mathbf{u}$ from the encoder and the sensitive attribute $s$. $p(\mathbf{u})$ is assumed to follow a standard normal distribution~\cite{kingma2013auto}. Then $p(\mathbf{x}^s|\mathbf{u}, s)$ is a likelihood function from the decoder that reconstructs the input sample $\mathbf{x}^s$ from $\mathbf{u}$ and $s$. The encoder and decoder networks are denoted as $q_\phi$ and $p_\theta$. We formulate $L_{\text{vae}}(\mathbf{x}^s)$ from the variational lower bound of $\log p(\mathbf{x})$ as follows: \looseness=-1
\begin{align}
    L_{\text{vae}}(\mathbf{x}^s)=\text{KL}(q_\phi (\mathbf{u}  | \mathbf{x}^s) || p(\mathbf{u}))-\mathbb{E}_{q_\phi (\mathbf{u}|\mathbf{x}^s)}[\log p_\theta (\mathbf{x}^s | \mathbf{u},s)]. \label{eq:vae_loss}
\end{align}
Latent variable $\mathbf{u}$ from the encoder $q_\phi$ and sensitive attribute $s$ are not guaranteed to be independent even if they correspond to $U$ and $S$ in the causal model that are independent. To ensure independence, we introduce an additional adversarial objective and lead the model to eliminate unnecessary sensitive information from $\mathbf{u}$. A discriminator $r_\psi$ is trained with a cross-entropy loss to predict the sensitive attribute $s$ from the latent representation $\mathbf{u}$:
\begin{align}
    L_{\text{adv}}(\mathbf{x}^s) = - \mathbb{E}_{q_\phi(\mathbf{u}|\mathbf{x}^s)}[\log r_\psi(s | \mathbf{u})].
\end{align}
We negate the cross-entropy loss to train the sample generator to prevent the discriminator $r_\psi$ from predicting sensitive attributes. 

We further introduce the property of \textbf{cyclic consistency} to enhance training stability and improve generation quality. Our loss ensures that the counterfactual-counterfactual sample (i.e., double-flipped sample created by our generator) is similar to the original item.
We produce a counterfactual sample $\mathbf{x}_{\text{cnt}}^{s'}$ of the input instance $\mathbf{x}^s$, repeating the three steps below: \looseness=-1
\begin{enumerate}[leftmargin=*]
\item Compute a latent representation $\mathbf{u}$ from the encoder $q_\phi$.
\item Choose the target sensitive attribute $s' (\neq s)$.
\item Reconstruct the counterfactual sample from the decoder $p_\theta$ given the latent variable $\mathbf{u}$ and the target sensitive attribute $s'$. 
\end{enumerate}
The reconstructed sample $\mathbf{x}_{\text{cnt}}^{s'}$ goes back to the generator to produce a counterfactual-counterfactual (or double-flipped) sample $\mathbf{x}_{\text{cyc}}^s$ that shares the identical sensitive attribute with the original input. Our cyclic consistency loss in C-VAE maximizes the likelihood between two instances; $\mathbf{x}^s$, $\mathbf{x}_{\text{cyc}}^s$ (Eq.~\ref{eq:cyc_loss}).
\begin{align}
    convert(\mathbf{x}^{s'}_{\text{cnt}} | \mathbf{x}^s, s') = p_\theta (\mathbf{x}^{s'}_{\text{cnt}} | q_\phi (\mathbf{u} | \mathbf{x}^s), s'), \nonumber \\
    L_{\text{cyc}}(\mathbf{x}^s) = -\mathbb{E}_{convert(\mathbf{x}^{s'}_{\text{cnt}} | \mathbf{x}^s, s')}[\log convert(\mathbf{x}^{s} | \mathbf{x}^{s'}_{\text{cnt}}, s)] \label{eq:cyc_loss}
\end{align}
%
%
The total loss to train the sample generator is as follows:
\begin{align}
    L_{\text{c-vae}}(\mathbf{x}^s) = L_{\text{vae}}(\mathbf{x}^s) - L_{\text{adv}}(\mathbf{x}^s) + L_{\text{cyc}}(\mathbf{x}^s).
\end{align}


\subsection{Fairness-aware Contrastive Learning}
The next objective, given the learned sample generator, is to train an encoder that satisfies both group and counterfactual fairness. We use contrastive learning for this task. Let us denote an input query as $\mathbf{x}\in\mathbb{R}^{d}$, and a positive sample and a set of negative samples for contrastive learning as $\mathbf{x}_+$ and $\mathcal{X}_-$, respectively. Typically, positive samples and negative samples are determined by predefined rules. For example, a rule can set a single data instance and its augmented versions as positive, whereas set all others as negative (see description in $\S$\ref{ref:2}). In this work, we define InfoNCE loss in contrastive learning to train the model $F$ (i.e., $F := h_1 \circ g \circ f$ in Figure~\ref{fig:main_training}) as follows: \looseness=-1
\begin{align}
L_{c}(\mathbf{x}, \mathbf{x}_{+}, \mathcal{X}_{-}) &=  -\log {{\text{exp}({\text{sim}(F(\mathbf{x}), F(\mathbf{x}_+)) / \tau}})  \over {\sum_{\mathbf{x}' \in \{\mathbf{x}_{+}\} \cup \mathcal{X}_{-}} \text{exp}(\text{sim}(F(\mathbf{x}), F(\mathbf{x}')) / \tau)}} \label{eq:contrastive_loss} \\
&= - \text{sim}(F(\mathbf{x}), F(\mathbf{x}_+)) / \tau \nonumber \\
&\ \ \ \  + \log{\sum_{\mathbf{x}' \in \{\mathbf{x}_{+}\} \cup \mathcal{X}_{-}} \text{exp}(\text{sim}(F(\mathbf{x}), F(\mathbf{x}')) / \tau)} \\
&= -L_{\text{align}}(\mathbf{x}, \mathbf{x}_+) + L_{\text{distribution}}(\mathbf{x}, \mathbf{x}_{+}, \mathcal{X}_{-}),
\label{eq:simclr_decompose}
\end{align}
where sim($\cdot$) is the function to measure two embeddings' similarity, and $\tau$ is the temperature parameter.

This contrastive loss in Eq.~\ref{eq:simclr_decompose} comprises two terms. First, the alignment loss ($L_{\text{align}}$) encourages the embedding positions of positive pairs to be placed closer. Second, in contrast, the distribution loss ($L_{\text{distribution}}$) matches all instances' embeddings into the prior distribution with a high entropy value. Our model uses the generalized contrastive objective proposed in~\cite{wang2020understanding}, which supports the diverse choice of prior distributions by introducing the optimal transport theory~\cite{bonneel2015sliced} and by changing the distribution loss with Sliced Wasserstein Distance (SWD)~\cite{kolouri2019generalized}. Given the prior distribution over the embedding space $\mathcal{Z}_{prior}$ and the set of embeddings $\mathcal{\tilde{Z}}=\{ F(\mathbf{x}) | \mathbf{x} \in \mathcal{X}_{-}\}$, the loss is formulated as the following equation: \looseness=-1
\begin{align}
L_{\text{gen-}c}(\mathbf{x}, \mathbf{x}_{+}, \mathcal{X}_{-}) = -L_{\text{align}}(\mathbf{x}, \mathbf{x}_{+}) + \text{SWD}(\mathcal{\tilde{Z}}, \mathcal{Z}_{prior}). \label{eq:gen_infonce}
\end{align}



We modify the contrastive loss to jointly meet two fairness criteria. Assume that all instances in $\mathcal{X}_{-}$ are sampled to have the same sensitive attribute $s$ (e.g., gender $=$ female). Then minimizing the distribution loss on $\mathcal{X}_{-}$ (i.e., $\text{SWD}(\mathcal{\tilde{Z}}, \mathcal{Z}_{prior}$) in Eq.~\ref{eq:gen_infonce}) will cause the sensitive group $s$ in embeddings to match the predefined prior distribution $\mathcal{Z}_{prior}$. By iterating this process for every sensitive group (e.g., female and male), our model can produce embeddings of groups that follow the same distribution, $\mathcal{Z}_{prior}$. As a result, embeddings 
become no longer distinguishable by sensitive attributes. Given a batched set of instances $\mathcal{B}_s$ and an input $\mathbf{x}^s$, negative samples are defined as follows: \looseness=-1
\begin{align}
    \mathcal{X}^s_{-} = \{ \mathbf{x} | \mathbf{x} \in \mathcal{B}_s\setminus\{\mathbf{x}^s\} \}.
\end{align}


To ensure counterfactual fairness, our model also considers a counterfactual version of the sample as positive in contrastive learning. Given an input sample $\mathbf{x}^s$, we flip the sensitive attribute $s\rightarrow s'$ (e.g., female to male) and generate the counterfactual sample $\mathbf{x}^{s'}_{\text{cnt}}$ from our C-VAE based sample generator. The alignment loss in Eq.~\ref{eq:gen_infonce} then minimizes the embedding discrepancy between the original and counterfactual data instances. From the positive sample and the set of negative samples ($\mathbf{x}^{s'}_{\text{cnt}}$, $\mathcal{X}^s_{-}$), the fairness-aware contrastive loss for the given input $\mathbf{x}^s$ is defined as follows:
\begin{align}
    L_{\text{fair-cl}}(\mathbf{x}^s) =  L_{\text{gen-}c}(\mathbf{x}^s, \mathbf{x}^{s'}_{\text{cnt}}, \mathcal{X}^s_{-}). \label{eq:contrast_loss}
\end{align}
We train the embedding on top of the Euclidean space with Gaussian prior $\mathcal{Z}_{prior}$. This is different from other contrastive approaches where embeddings are learned over the L2-normalized space~\cite{chen2020simple,he2020momentum}. L2-normalized space used in those approaches does not account for the norm of embeddings during training, which can also be an important clue for sensitive attributes in downstream tasks. However, our choice of the Euclidean space regularizes the norm distribution and removes its dependency on sensitive attributes. The alignment loss $L_{\text{align}}$ is then defined with a negative Euclidean distance as a similarity measure between the original and positive instance. \looseness=-1


\subsection{Self-knowledge Distillation}
The final objective is maintaining the representation quality. We design the self-knowledge distillation loss to reduce the embedding discrepancy between the original and perturbed instances. Inspired by the original literature~\cite{grill2020bootstrap}, we present a Siamese network with two different heads, where each head becomes the student and teacher branches. Then, the model is trained through a prediction task so that the original instance from the teacher branch is highly predictive of the perturbed instances from the student branch. This process is called \textit{self-knowledge distillation}, since the knowledge extracted from the teacher branch is progressively transferred back to the student branch~\cite{kim2021self,tejankar2021isd}. To prevent the model from collapsing into a naive solution (e.g., representation becomes constant for every instance), we let the architecture of student and teacher be asymmetric, restricting the gradient flows of the teacher branch~\cite{chen2021exploring,grill2020bootstrap} (see the bottom part of Figure~\ref{fig:main_training}). Let us denote $f$, $g$, and $h_2$ as the backbone network, the projection head, and the prediction head, respectively. The self-knowledge distillation loss is defined as follows:
\begin{align}
\mathbf{p}_{\text{student}} &= h_2 \circ g \circ f(\mathbf{x}^s_{\text{pert}}), \ \ \ \ \ \mathbf{z}_{\text{teacher}} = sg \circ g \circ f(\mathbf{x}^s)  \nonumber \\
&L_{\text{self-kd}}(\mathbf{x}^s) = - {\mathbf{p}_{\text{student}} \over || \mathbf{p}_{\text{student}} ||_2} \cdot {\mathbf{z}_{\text{teacher}}  \over || \mathbf{z}_{\text{teacher}} ||_2}, \label{eq:self-kd}
\end{align}
where $\mathbf{x}^s_{\text{pert}}$ is a perturbed version of original instance $\mathbf{x}^s$ and $sg$ represents the stop-gradient operation.


When perturbing the instance, we propose \tabmix{} for our augmentation strategy. Given an input sample $\mathbf{x}^s$, the generator randomly masks $k$ features and replaces their values from other instances, as shown in Figure~\ref{fig:tabmix}.\footnote{We mask 50\% of the non-sensitive attributes. The masking ratio is not critical to the downstream task.} Let $\odot$ denote an element-wise multiplicator and $\mathbf{m} \in \{0,1\}^{d}$ a binary mask vector indicating which feature to replace. The mixing operation is defined as:
\begin{align}
    \text{TabMix}(\mathbf{x}_i, \mathbf{x}_j) = \mathbf{m} \odot \mathbf{x}_i + (1 - \mathbf{m}) \odot \mathbf{x}_j. \label{eq:tabmix}
\end{align}
\tabmix{} does not need to consider the scale of numeric variables; hence it is easily applicable to various datasets. The perturbed instance $\mathbf{x}^s_{\text{pert}}$ is obtained as follows: 
\begin{align}
    \mathbf{x'} &\sim \text{Sample}(\mathcal{D} \setminus \mathbf{x}^s) \\
    \mathbf{x}^s_{\text{pert}} &= \text{TabMix}(\mathbf{x}^s, \mathbf{x'}).
\end{align}

Finally, the loss function of the entire process is the sum of two losses: fairness-aware contrastive loss ($L_{\text{fair-cl}}$) and self-knowledge distillation loss ($L_{\text{self-kd}}$) as in Eq.~\ref{eq:final_loss}.
\begin{align}
    L_{\text{total}} = {1 \over |\mathcal{D}|} \sum_{\mathbf{x}^s \in \mathcal{D}}(L_{\text{fair-cl}}(\mathbf{x}^s) + L_{\text{self-kd}}(\mathbf{x}^s)) \label{eq:final_loss}
\end{align}


