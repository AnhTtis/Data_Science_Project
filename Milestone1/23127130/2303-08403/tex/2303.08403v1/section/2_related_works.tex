
\section{Related Works}

\subsection{Fairness in Machine Learning}
\textit{Fairness} is a conceptual term, and cannot be measured in a straightforward manner. Instead, there are several criteria to observe it from different perspectives: unawareness, group fairness, and individual fairness~\cite{gajane2017formalizing,verma2018fairness}. Removing sensitive attributes from data is a simple way to achieve fairness through unawareness~\cite{chen2019fairness}.
However, it can be brittle if some hidden features are highly correlated with sensitive attributes. Group fairness states that subjects in different groups (e.g., gender) should have an equal probability of being assigned to the predicted class~\cite{conitzer2019group,gajane2017formalizing}. Demographic parity and equalized odds are two measures of group fairness~\cite{hardt2016equality,zafar2017fairness}. Individual fairness is a fine-grained criterion that treats similar individuals as similarly as possible ~\cite{dwork2012innovations}. Counterfactual fairness is one alternative to individual fairness that assumes a counterfactual sample by flipping the sensitive attributes and treats it similarly to the original one~\cite{russel2017when}. \looseness=-1


Researchers also focus on diverse learning steps to ensure fairness. Elazar et al.~\cite{elazar2018advesarial} focus on fairness of the input dataset by noting that  a fair decision is made with the model trained with an unbiased dataset. However, Wang et al.~\cite{wang2019balanced} demonstrate that input-wise fairness does not entirely support fair decisions in large-scale datasets. Therefore, post-processing techniques are proposed to secure fairness, such as hiding the sensitive attribute information from trained representations by null space projection~\cite{ravfogel2020null} or identifying the subspace of sensitive attributes~\cite{bolukbasi2016man}.

With the advent of representation learning, recent self-supervised learning approaches aim to produce a fair representation of individual instances without knowing downstream tasks (i.e., treatment-level fairness)~\cite{kose2021fairness}.
These fair representation learning approaches add fairness-related objectives in training steps, or apply adversarial learning objectives to obtain fair representations~\cite{li2018towards, zhang2018mitigating, barrett2019adversarial, han2021mitigating}. For example, LAFTR~\cite{madras2018learning} employs a discriminator that detects sensitive attribute information, while generators make indistinguishable representations against discriminators. VFAE~\cite{louizos2016variational} proposes a variational autoencoder with regularization using Maximum Mean Discrepancy (MMD) to learn fair representations. 


\begin{figure*}[t!]
\centering
\includegraphics[width=0.93\textwidth]{figure/model_figure_renewal3.pdf}
\caption{The overall architecture of \model{}, where $f$, $g$, ($h_1$, $h_2$) denote the backbone network, the projection head, and two prediction heads, respectively. \model{} aims to achieve both fairness and representation quality by jointly optimizing the fairness-aware contrastive loss and the self-knowledge distillation loss.}
\label{fig:main_training}
\end{figure*}

Song et al.~\cite{song2019learning} proposes a user-centric approach that allows users to control the level of fairness and maximize the expressiveness of representations in the form of conditional mutual information with the preset fairness threshold. Tsai et al.~\cite{tsai2021conditional} aim to optimize the same objective but maximize the lower bound of conditional mutual information via InfoNCE objective instead. However, these methods only consider the single group fairness objective and often lose the expressiveness during the training~\cite{burke2017multisided,burke2018balanced}. Specifically, group fairness helps achieve anti-discrimination for protected groups, but individual justice is not guaranteed~\cite{binns2020apparent}.
Our research objective is to implement multiple algorithmic fairness concepts on a single deep model, a topic that is explored less in the literature. We seek to achieve both group fairness at a coarse-grained level and counterfactual fairness at an individual level while preserving a high level of representational quality. \looseness=-1


\subsection{Contrastive Self-Supervised Learning} \label{ref:2}
The fundamental idea of contrastive learning is to minimize the distance between similar (i.e., positive) instances while maximizing the distance among dissimilar (i.e., negative) instances~\cite{chen2020simple,he2020momentum}. SimCLR~\cite{chen2020simple} utilizes augmented images as positives while the other images in the same batch as negatives. Maintaining a similar contrastive concept, MoCo~\cite{he2020momentum} exploits a momentum encoder and proposes a dynamic dictionary with a queue to handle negative samples efficiently in both performance and memory perspectives. InfoNCE loss~\cite{oord2018representation} is often used in contrastive learning. Minimizing this loss increases mutual information between positive pairs so that the model can extract the consistent features between the original and augmented samples. 
