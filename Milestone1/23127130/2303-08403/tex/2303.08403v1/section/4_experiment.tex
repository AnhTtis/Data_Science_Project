\section{Experiment}

We compared \model{} with the latest models on multiple fairness-aware tabular datasets. Component analyses were performed to test the effect of each module. We also present qualitative analyses and case studies on how the model handles sensitive attributes. 

\subsection{Performance Evaluation}
\textbf{Datasets:} We use the following datasets: (1) UCI Adult~\cite{asuncion2007uci} contains 48,842 samples along with label information that indicates whether a given individual makes over 50K per year as a downstream task; (2) UCI German Credit~\cite{asuncion2007uci} includes 1,000 samples with 20 attributes and aims to predict credit approvals; (3) COMPAS~\cite{compas2016} includes 6,172 samples and is used to predict recidivism risk (i.e., the risk of a criminal defendant committing a crime again) of given individuals; (4) LSAC~\cite{wightman1998lsac} contains 22,407 samples and estimates whether a given individual will pass the law school admission; (5) Students Performance (Students) ~\cite{cortez2008using} consists of 649 samples and predicts the grade in exam; (6) Communities~\cite{redmond2002data} consists of 1,994 samples and estimates the number of violent crimes per 100K population.

We split each dataset into disjoint training and test sets. Embedding learning and counterfactual sample generation are based on the knowledge of the training set; evaluations are based on the test set. Table~\ref{tab:data_description} summarizes statistics of each dataset. \smallskip

\input{table/data.tex}

\noindent
\textbf{Evaluation: } The evaluation uses embeddings learned from the last ten epochs, and the averaged results of five metrics below:
\begin{itemize}[leftmargin=5mm]
    \item \textbf{\textsf{AUC}}, the area under the receiver operating characteristics, measures the prediction performance of the downstream classification task (i.e., the performance of the binary classifier to distinguish between cases and non-cases). If this value is 1, the model distinguishes the target variable from input instances with absolute precision.

    \item \textbf{\textsf{RMSE}}, the root mean squared error, measures the deviance between the prediction and ground-truth for the regression task. \looseness=-1
    
    \item \textbf{\textsf{Demographic Parity Distance ($\Delta DP$)}} is a group fairness metric and is defined as the expected absolute difference between the predictions of protected groups. Given a set of sensitive attributes $\mathcal{S}$, the definition of $\Delta DP$ is as follows ($s\neq s'$):
    \begin{align}
        \Delta DP = \mathbb{E}_{s,s' \in S}[|P(\hat{Y}=1 | s) - P(\hat{Y}=1 | s')|].
    \end{align}
    
    \item \textbf{\textsf{Equalized Odds ($\Delta EO$)}}, also known as equality of opportunity, is a group fairness metric based on the expected difference between the estimated positive rates for two protected groups. Given the set of sensitive attributes $\mathcal{S}$, $\Delta EO$ is defined as follows ($y \in \{0, 1\}$, $s\neq s'$):
    \begin{align}
        \Delta EO = \mathbb{E}_{s,s' \in S}[|P(\hat{Y}=1 | s, Y=y) - P(\hat{Y}=1 | &s', Y=y)|].
    \end{align}
    
    \item \textbf{\textsf{Counterfactual Parity Distance ($\Delta CP$)}} is a metric for counterfactual fairness and measures the prediction parity between two instances in counterfactual pair relationships. Assume that $(\mathbf{x}, \hat{\mathbf{x}})$ are from a set of counterfactual pairs $\mathcal{D} \times \hat{\mathcal{D}}$. Then $\Delta CP$ is defined as follows:
    \begin{align}
        \Delta CP = \mathbb{E}_{(\mathbf{x}, \hat{\mathbf{x}}) \in \mathcal{D} \times \hat{\mathcal{D}}}[|P(\hat{Y}=1 | \mathbf{x}) - P(\hat{Y}=1 | \hat{\mathbf{x}}) |].
    \end{align}
\end{itemize}
\noindent
\textbf{Baselines: } A total of nine baselines are employed. The first two directly utilize raw datasets to classify target variables, given that the downstream task is already known. They differ by input structure: (1) original data (LR) and (2) original data concatenated with the synthetic counterfactual samples (C-LR). The next one is (3) SCARF~\cite{bahri2021scarf}, an unsupervised contrastive learning method that does not consider any fairness requirement during training. The remaining six learn fair representations from data without any information on downstream tasks. They utilize different unsupervised representation learning techniques with fairness-aware objectives. (4) VFAE~\cite{louizos2016variational} introduces the maximum mean discrepancy term on top of the variational autoencoder to produce fair representations. (5) LAFTR~\cite{madras2018learning} adopts an adversarial approach to avoid unfair predictions from embeddings. (6,7) MIFR and L-MIFR~\cite{song2019learning} learn the controllable fair representation through mutual information. The two differ in the use of the Lagrangian dual optimization method. (8,9) C-InfoNCE and WeaC-InfoNCE~\cite{tsai2021conditional} maximize the conditional mutual information within representations. The two differ in how they introduce the sensitive variable in the InfoNCE objective. For all methods, we use a logistic regression model for the downstream classification task and a Random Forest regression model for the downstream regression task as a base predictor. The predictor is learned on top of either raw dataset (1--2) or learned embeddings (3--9). \looseness=-1 \smallskip


\noindent
\textbf{Implementation details: } We trained the multi-layer perceptron (MLP) model. A three-layer MLP was used for the backbone network $f$, and a two-layer MLP for both projection and prediction heads $g$, ($h_1$, $h_2$). ReLU activation function was used for all architectures, with the training of 200 epochs and a batch size of 128. Adam optimizer with a learning rate of 1e-3 and a weight decay factor of 1e-6 was utilized. For the counterfactual sample generator, three-layer, two-layer, and two-layer MLPs were utilized for the encoder, decoder, and discriminator, respectively. The sample generator was trained for 600 epochs with the Adam optimizer. Mode-specific normalization~\cite{xu2019modeling} was used for preprocessing categorical variables. \looseness=-1 \smallskip

\input{table/result_summary.tex}
\input{table/classification.tex}
\input{table/regression.tex}

\noindent
\textbf{Results: } 
\model{} outperforms other baselines in terms of the averaged rank for each evaluation metric. Table~\ref{table:summary} shows that our model's embedding maintains its prediction performance (i.e., AUC and RMSE) while successfully removing any bias related to sensitive information from the embedding. \looseness=-1

Tables~\ref{table:classificationresults} and \ref{table:regressionresults} report the detailed performance of \model{} and other baselines on downstream classification and regression tasks. According to the results, naively adding the counterfactual samples (i.e., C-LR) is insufficient to handle bias from sensitive information. Similarly, the fair embedding learning baselines such as VFAE, L-MIFR, or WeaC-InfoNCE fail to remove sensitive information entirely nor generate a fair embedding without losing critical information for the downstream task. These findings suggest that \model{} achieves both the performance and fairness requirements in a single training.


\subsection{Component Analyses}
We performed an ablation study by repeatedly assessing and comparing the models after removing each component. We also examined the counterfactual sample quality.  \smallskip

\noindent
\textbf{Ablation study: } \model{} utilizes two learning objectives: fairness-aware contrastive loss to ensure both group and counterfactual fairness, and self-knowledge distillation loss to ensure the representation quality. Ablations remove each loss objective from the full model to assess the unique contribution. Table~\ref{Tab:Ablation} reports the results based on the UCI Adult dataset. The full model achieves the best balance between fairness and prediction performance, implying that each loss plays a unique role in designing fair representations. The result without self-knowledge distillation shows that our fairness-aware contrastive loss effectively improves fairness while there is a trade-off for prediction performance. On the other hand, the experiment only with self-knowledge distillation loss (i.e., w/o $L_{\text{align}}$ and $L_{\text{distribution}}$) produces opposite result. The result without only alignment loss, which is in charge of counterfactual fairness, contrasts the role of two losses in fairness-aware contrastive objective; Counterfactual fairness does not have any improvement while group fairness has been reduced. 
%
Additionally, we experimented using Gaussian noise and dropout as alternative augmentation strategies for TabMix. This ablation reduced the prediction accuracy (AUC: 0.80$\rightarrow$0.78, 0.75) while maintaining fairness (DP: 0.03$\rightarrow$0.03, 0.02). \smallskip

\input{table/ablation.tex}
\input{table/counterfactual.tex}


\noindent
\textbf{Counterfactual samples: }
To test the quality of the generated counterfactual samples, we examined if the target variable is predictable, even if the model is trained only with the synthetic counterfactual samples. Table~\ref{Tab:counterfactual-only} reports the logistic regression performance that changes the training set from the original data. The latter model is on par with the model trained with the original dataset. \looseness=-1

We next examined if feature correlations are maintained for counterfactual samples. Figure~\ref{fig:corr_all} shows the correlation matrix between features in the original UCI adult dataset and the counterfactual dataset. Pearson correlation is used between continuous variables, and Cramer's V value is utilized between categorical variables. To measure the correlation between categorical and continuous variables, we label-encode the categorical variables to their continuous counterparts and compute the Pearson correlation with original continuous values. Two matrices show a remarkable resemblance, indicating that the relationship between features in the counterfactual samples is well-maintained.

\input{table/correlation.tex}

%Finally, to check whether the tendency between factual samples and counterfactual samples is consistent, we measured the Spearman correlation of each feature value between the two sets. In the Adult dataset, the correlation of continuous features—age, capital-loss, and education-num—were 0.91, 0.98, and 0.93, respectively. This demonstrates that each user's past experience does not drastically change in counterfactual sample generation.

\begin{figure}[t!]
\centering
\begin{subfigure}[t]{0.23\textwidth}
\captionsetup{justification=centering}
       \centering\includegraphics[height=3.2cm]{figure/density_original.jpg}
      \caption{Standard ($\Delta DP$=0.19)}
      \label{fig:density_scarf}
\end{subfigure}
\begin{subfigure}[t]{0.23\textwidth}
\captionsetup{justification=centering}
       \centering\includegraphics[height=3.2cm]{figure/density_ours.jpg}
      \caption{\model{} ($\Delta DP$=0.02)}
      \label{fig:density_ours}
\end{subfigure}
\caption{Income prediction results between the male and female groups are similar when we use \model{}'s embeddings. We compared two models, (a) standard logistic regression on top of original data and (b) logistic regression on top of \model{} embedding, on the UCI Adult dataset.
}
\label{fig:density}
\end{figure}




\subsection{Qualitative Analysis}
We visually check the learned representations to investigate how well our model handles sensitive information. Figure~\ref{fig:density} shows how debiasing achieves group fairness in downstream predictions. It compares the prediction results on UCI Adult for two models: standard logistic regression applied on the raw dataset and the same regression model using the \model{} embeddings. After debiasing, there is almost no difference in the distribution of predicted income between male and female groups, as depicted in Figure~\ref{fig:density_ours}. However, two probability density functions in Figure~\ref{fig:density_scarf} appear substantially different in the standard model. These results confirm the outstanding debiasing potential of the proposed model. \looseness=-1

We also investigate how well \model{} achieves counterfactual fairness by examining the difference in the $\Delta CP$ values of the proposed model and its ablation \textsf{GroupFair}, which omits the alignment loss between counterfactual pairs in fairness-aware contrastive loss (Eq.~\ref{eq:contrast_loss}). Experimental comparison over UCI Adult in Figure~\ref{fig:case_study1} shows a smaller prediction difference for the full model, indicating that the omitted loss is effective in debiasing sensitive attributes at the individual level. One of the counterfactual pairs from the model is illustrated in Figure~\ref{fig:case_study2}. Note that in generating counterfactual examples, we do not simply flip sensitive attributes (i.e., gender) but also observable variables (i.e., age, relationship) change together. When we compute $\Delta CP$ for the example case from two models, we confirm that the proposed \model{} satisfies the fairness concerns to some extent (prediction probability for original: 0.22 vs. counterparts: 0.24). Meanwhile, \textsf{GroupFair} fails to debias gender information and gives a higher score to the male counterparts (0.24 vs. 0.45). \looseness=-1

\begin{figure}[t!]
\centering
\begin{subfigure}[t]{0.23\textwidth}
\captionsetup{justification=centering}
       \centering\includegraphics[height=3.2cm]{figure/case_study1.jpg}
      \caption{Histogram of $\Delta CP$}
      \label{fig:case_study1}
\end{subfigure}
\begin{subfigure}[t]{0.23\textwidth}
\captionsetup{justification=centering}
       \centering\includegraphics[height=3.2cm]{figure/case_study2.jpg}
      \caption{Counterfactual example}
      \label{fig:case_study2}
\end{subfigure}
\caption{Additional findings on the counterfactual model: (a) Comparison of the $\Delta CP$ value between \model{} and its ablation \textsf{GroupFair} which omits the alignment loss between counterfactual pairs. (b) Observable variables affected by sensitive attributes are also changed in the counterfactual example. \looseness=-1}
\label{fig:case_study}
\end{figure}
 