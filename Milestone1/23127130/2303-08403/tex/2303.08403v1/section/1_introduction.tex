\section{Introduction}

Machine learning techniques are being used in many mission-critical real-world Web applications, such as recommendation~\cite{an2019neural}, hiring~\cite{hoffman2018discretion}, and advertisement of profiles~\cite{sweeney2013discrimination}. Many tasks are subject to stereotypes or societal biases embedded in data, leading models to treat individuals of particular attributes unfairly. Notable examples include facial recognition models that fail to recognize people with darker skin~\cite{buolamwini2018gender} or recruiting models that favor men over women candidates with comparable work experience~\cite{kiritchenko2018examining}. Lacking fairness in these tasks aggravates social inequity and even harms individuals and society. As a response, a growing number of studies are dedicated to algorithmic fairness~\cite{dwork2012fairness,wu2021fairness}. 


Fairness is also important in representation learning. Fair representation learning is useful when data needs to be shared without any prior knowledge of the downstream task. 
Previous research has focused on finding data representations that apply to diverse tasks while considering fairness. For example, LAFTR~\cite{madras2018learning} and VFAE~\cite{louizos2016variational} introduce an adversarial concept, and studies like~\cite{song2019learning} and~\cite{tsai2021conditional} maximize the conditional mutual information with fairness constraints to reduce the potential bias. These approaches generate fairer embeddings, albeit at the cost of performance degradation in downstream tasks. They also consider only group-level fairness, missing out on individual-level fairness~\cite{binns2020apparent}.

We propose \model{}, a self-supervised learning method that debiases sensitive information through fairness-aware contrastive learning while preserving rich expressiveness (i.e., representation quality) through self-knowledge distillation. 
\model{} can learn data representations satisfying two types of fairness, i.e., \textit{group fairness} and 
\textit{counterfactual fairness}.
The former (a.k.a. demographic parity) requires every protected group be treated in the same way as any advantaged group, and the latter requires the model to treat individuals in a counterfactual relationship (i.e., those who share similar traits except for the sensitive attribute) alike~\cite{kusner2017counterfactual}. 
Counterfactual fairness is a type of fairness defined at the individual level. It removes bias from sensitive information by comparing against synthetic individuals from the counterfactual world. \looseness=-1

 

Because the model is unaware of downstream tasks during training, adding fairness-related regularization terms (e.g., minimizing demographic parity from model predictions~\cite{kamishima2012fairness}) or fairness constraints (e.g., limiting the prediction difference among groups below the threshold~\cite{zafar2019fairness}) as in other research is not feasible. Instead, we propose the following alternative goals for our loss design, which are illustrated in Figure~\ref{fig:motivate}:
\begin{enumerate}
\item
\textbf{\textsf{Group fairness}}: Data points from every sensitive group have the same distribution across the embedding space; thus, their group membership cannot be identified. 

\item \textbf{\textsf{Counterfactual fairness}}: 
Data points from those in a counterfactual relationship are located close in the embedding space. This means the embedding distance between an individual and its counterfactual version should be minimized.
\looseness=-1 
\end{enumerate}
These goals can apply universally to any downstream task.
By making sensitive attributes indistinguishable in the embedding space, a downstream classifier cannot determine which data point belongs to the protected group and consequently produce unbiased predictions. The same applies to embeddings of counterfactual pairs.\looseness=-1

 


To implement these fairness objectives, we first propose a cyclic variational autoencoder (C-VAE) model to generate counterfactual samples (Sec.~3.2). This sample generation task is non-trivial due to the high correlation between data features and sensitive attributes. Next is to create fairness-aware embeddings from the input data. We employ contrastive learning, which learns representations based on the similarity between instances by modifying the selection of positive and negative samples. We select negative samples from the same protected group and select positive samples from counterfactual versions (Sec.~3.3).
In addition, we use self-knowledge distillation to extract semantic features and enforce consistent embeddings between the original and its augmentation to maintain a high representation quality (Sec.~3.4). 
Experiments demonstrate that the proposed framework can generate data embeddings that satisfy both fairness criteria while preserving prediction accuracy for a variety of downstream tasks. 
The main contributions of this paper are summarized below. \smallskip \looseness=-1
\begin{itemize}[nosep,leftmargin=1em,labelwidth=*,align=left]
    \item We propose a \textbf{self-supervised representation learning framework (\model{})} that simultaneously debiases sensitive attributes at both group and individual levels.
    
    \item We introduce the C-VAE model to generate counterfactual samples and propose \textbf{fairness-aware contrastive loss} to meet the two fairness criteria jointly.
    
    \item We design the \textbf{self-knowledge distillation loss} to maintain representation quality by minimizing the embedding discrepancy between original and perturbed instances.
    
    \item Experimental results on six real-world datasets confirm that \model{} generates a fair embedding for sensitive attributes while maintaining high representation quality. The ablation study further shows a synergistic effect of the two fairness criteria.
\end{itemize}
Codes are released at a GitHub repository.\footnote{https://github.com/Sungwon-Han/DualFair} 
% [nosep,leftmargin=1em,labelwidth=*,align=left]


\begin{figure}[t!]
\centering
\begin{subfigure}[t]{0.40\textwidth}
      \centering\includegraphics[width=0.9\textwidth]{figure/introduction_1.pdf}
      \subcaption{Group fairness: Embeddings of different member groups (e.g., by gender) should not be distinguished. This makes unbiased predictions possible at the group level.}
      \label{fig:intro1}
      \vspace{1mm}
\end{subfigure}
\begin{subfigure}[t]{0.40\textwidth}
      \centering\includegraphics[width=1\textwidth]{figure/introduction_2.pdf} 
      \subcaption{Counterfactual fairness (i.e., individual-level fairness): Individuals of similar traits should have similar embeddings irrespective of their sensitive attributes. This makes unbiased predictions possible at the individual level.}
      \label{fig:intro2}
      \vspace{-1mm}
\end{subfigure}
\caption{Illustration of two fairness criteria. %  via a self-supervised representation learning framework
\vspace{-3mm}} 
\label{fig:motivate}
\end{figure}


 