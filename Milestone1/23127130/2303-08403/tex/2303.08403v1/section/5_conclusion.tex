\section{Conclusion}
We presented \model{}, a self-supervised embedding learning model that de-biases sensitive data attributes without any prior information on downstream tasks. Its design includes a unique fairness-aware contrastive loss and self-knowledge distillation technique.
% This model can be used in any downstream task and ensure fairness by the following methods.
% First, we modified the contrastive objective to achieve both group and counterfactual fairness. We developed a counterfactual sample generator to reduce the embedding discrepancy between counterfactual pairs. Second, to improve representation quality, we introduced self-knowledge distillation techniques with our augmentation technique \tabmix{}, and guided the model to learn semantic information from data. 
%
Experiments confirm that \model{} generates rich data representations that ensure both group fairness and counterfactual fairness.
%
%For example, in the case of an online recommendation, user and item embeddings that satisfy the group and counterfactual fairness concerning the protected groups (i.e., gender or race) can provide fairer item recommendations.
% We believe self-supervised learning with debiasing techniques can become a building block to advance performance and fairness requirements for real-world applications. 
Our model is applicable to various Web applications, including classification, ranking, recommendation, and text generation tasks.
% Algorithmic fairness that we aimed in this research is gaining importance in a variety of fields.

The algorithmic bias observed in search engine results and social media platforms has reinforced the need for a clear policy for protecting sensitive attributes. However, the problem is more complex because bias can also exist in other domains, such as in natural language processing (e.g., Q\&A generation, chatbot). The recently released `AI Bill of Rights' blueprint from the White House states that ``you should not face discrimination by algorithms and systems should be used and designed in an equitable way.'' We believe that our self-supervised learning with debiasing techniques can serve as a building block for advancing the performance and fairness requirements of real-world web applications.
\looseness=-1

\section*{Acknowledgement}
This research was supported by the Institute for Basic Science (IBS-R029-C2, IBS-R029-Y4), the Potential Individuals Global Training Program (2022-00155958) by the Ministry of Science and ICT in Korea, and Microsoft Research Asia.