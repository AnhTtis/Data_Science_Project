\section{Experiments}

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.9\linewidth]{anchor_expdemon.pdf}
    \vspace{-3mm}
    \caption{\textbf{Expert Demonstration and \textit{\acronym{}} exploration visualization}. \textbf{A}: all potential anchor points in the Gibson Swormville scene. \textbf{B}: expert selectively traverses a subset of the anchor points~(by merging spatial close anchor points) by iteratively reaching the next unvisited closest anchor points so as to create expert demonstrations. \textbf{C}: \textit{\acronym{}} exploration trajectory with a 1000-step budget, with the model learned from expert demonstrations.}
    \label{fig:expert_explore}
    \vspace{-3mm}
\end{figure*}

We test \textit{\acronym{}} on two datasets: Gibson~\cite{gibson_env} and Matterport3D~(MP3D)~\cite{Matterport3D} dataset on Habitat-lab platform~\cite{habitat19iccv}. The two datasets are collected in real indoor spaces by 3D scanning and reconstruction methods. The agents can be equipped with multi-modality sensors to perform various robotic tasks. The average room size of MP3D~(100 $m^2$) is much larger than that of Gibson~($[14 m^2, 85m^2]$).

\begin{table*}[t]
\scriptsize
  \centering
    \caption{Coverage Ratio over 1000-step budget. Top three performances are highlighted by \textbf{\textcolor{red}{red}}, \textbf{\textcolor{Green}{green}}, and \textbf{\textcolor{blue}{blue}} color, respectively.}
  \begin{tabular}{c|c|c|c|cc|cc}
  \hline
    \multirow{2}{*}{Method Description}&\multirow{2}{*}{Method} & \multirow{2}{*}{Sensor Used} & \multirow{2}{*}{\#Train Imgs}  & \multicolumn{2}{c|}{Gibson Val} & \multicolumn{2}{c}{\shortstack{Domain Generalization \\ MP3D Test}}  \\
    \cline{5-8}
   & & & & \%Cov. & Cov. ($m^2$)~& \%Cov. & Cov. ($m^2$) \\
\hline
Non-learning Based & RandomWalk~(used by SPTM~\cite{savinov2018semiparametric}) & No & No & 0.501 & 22.268 & 0.301 & 40.121   \\
\hline
\multirow{5}{*}{RL w/ Metric Input/Estimates} & RL + 3LConv~\cite{eval_metric} & \multirow{6}{*}{RGB, Depth, Pose} & 10~M & 0.737 & 22.838 & 0.332 & 47.758 \\
&RL+ResNet18 &  & 10~M & 0.747 & 23.188 & 0.341 & 49.175\\
&RL+ResNet18+AuxDepth~\cite{learning2complex} &  & 10~M & 0.779 & 24.467 & 0.356 & 51.959\\
&RL+ResNet18+ProjDepth~\cite{chen2018learning} &  & 10~M & 0.789 & 24.863 & 0.378 & 54.775\\
&OccAnt~\cite{ramakrishnan2020occant} &  & 1.5-2~M & \textbf{\textcolor{Green}{0.935}} & \textbf{\textcolor{blue}{31.712}} & 0.500 & 71.121 \\
&ANS~\cite{learn2explore_iclr20} &  & 10~M & \textbf{\textcolor{red}{0.948}} & \textbf{\textcolor{Green}{32.701}} & 0.521 & 73.281 \\
\hline
\multirow{4}{*}{\acronym{} Model Variants} & \acronym\_NoDeepSup & \multirow{6}{*}{RGB only} & \multirow{6}{*}{0.45~M} & 0.768 & 26.671 & 0.292 & 37.163 \\
 & \acronym\_NoFeatDeepSup &  &  & 0.912 & 35.151 & 0.620 & 104.499 \\
 & \acronym\_NoActDeepSup  &  &  & 0.900 & 33.922 & 0.600 & 102.122 \\
 & \acronym\_LSTMActRegu &  &  & 0.914 & 35.238 & 0.610 & 101.734 \\
 & \acronym\_withHistory &  &  & 0.917 & 35.331 & \textbf{\textcolor{blue}{0.618}} & \textbf{\textcolor{blue}{102.302}} \\
 & \acronym\_NoFeatHallu &  &  & 0.907 & 34.563 & 0.589 & 99.091 \\
 \cline{1-2}\cline{5-8}
\multirow{2}{*}{Deeply Supervised Imitation}&\textbf{\acronym} &  &  &  0.918 &35.274 & \textbf{\textcolor{Green}{0.642}} & \textbf{\textcolor{Green}{109.057}} \\
&\textbf{\acronym~(0.30m/$30^\circ$)} & &  & \textbf{\textcolor{blue}{0.927}} & \textbf{\textcolor{red}{37.731}} & \textbf{\textcolor{red}{0.656}} & \textbf{\textcolor{red}{117.993}} \\
\hline
  \end{tabular}
  \label{tab:coverage_ratio}
\end{table*}

We run experiments on two tasks: (1) \textbf{autonomous exploration} proposed by Chen \textit{et al.}~\cite{chen2018learning}, in which the target is to maximize an environment coverage within a fixed step budget~(1000-step budget following~\cite{learn2explore_iclr20}), and (2) \textbf{image-goal navigation} where the agent uses the constructed topological map to navigate from current observation to target observation. Regarding the exploration, we employ two evaluation metrics: (1) coverage ratio which is the percentage of the covered area over all navigable area, and (2) absolute covered area~($m^2$). We exactly follow the setting by ANS~\cite{learn2explore_iclr20} that a point is covered by the agent if it lies within the agent's field-of-view and is less than $3.2m$ away. Regarding the navigation, we adopt two evaluation metrics: shortest path length~(SPL) and success rate~(Succ. Rate)~\cite{eval_metric}. We again follow ANS~\cite{learn2explore_iclr20} to train \acronym{} on Gibson training dataset~(72 scenes), and test \acronym{} on Gibson validation dataset~(14 scenes) and MP3D test dataset~(18 scenes). Testing on the MP3D dataset helps to show \acronym's generalizability.

\subsection{Experiment Configuration}
\label{expconfig_sec}
\textbf{Exploration setup.} In exploration, we independently explore each scene 71 times, each time assigning the agent a random starting point~(created by a random seed number). We keep track of all the random seed numbers for result reproduction. We use the Habitat-lab sliding function so that the agent does not stop when it collides with a wall but instead slides along it. In order to generate the initial 10 steps required by \textit{\acronym}, we constantly let the agent execute \texttt{move\_forward} action. Once it collides with the wall, it randomly chooses \texttt{turn\_left} or \texttt{turn\_right} action to continue to explore. Afterward, we iteratively call \textit{TaskPlanner} and \textit{MotionPlanner} to efficiently explore the environment. During \textit{\acronym}-guided exploration, we allow the agent to actively detect its distance with surrounding obstacles or walls~(by using a distance sensor). When the agent's forward-looking distance to the closest obstacle or wall is less than 2-step distances and the \textit{\acronym{}} predicted action is \texttt{move\_forward}, we randomly choose either \texttt{turn\_left} and \texttt{turn\_right} to execute so as to avoid colliding with an obstacle. It is worth noting that using an obstacle avoidance scheme does not lead to unfair comparison because the comparing metric-based methods internally preserve a global metric map, which serves a similar purpose to help the agent avoid obstacles.

We experiment with two locomotion setups: the first one is with step-size 0.25~m and turning angle $10^{\circ}$, which follows the same setting established in~\cite{learn2explore_iclr20} for comparing with baseline methods in the exploration task. The second one is with step-size 0.30~m and turn-angle $30^{\circ}$. This setting helps us test \textit{\acronym}'s generalization capability under different locomotion configurations.

\textbf{Navigation setup.} In navigation, we encourage the agent to visit enough positions for each room scene. Specifically, the agent has collected 2,000 images per room on Gibson and 5,000 images per room on MP3D (2,000/5,000-step \textit{\acronym}-guided exploration).

\textbf{Expert demonstration generation.} For each room scene, we first sample multiple anchor points across the whole navigable area for each room scene. Then the agent starts at a random anchor point and iteratively walks to the next unvisited closest anchor point with minimal steps~(by calling Habitat \texttt{PathFollower} API) until all anchor points are traversed. At each step, we record the agent's action and panoramic RGB image. Please refer to Fig.~\ref{fig:expert_explore} for a visualization of this process. It is worth noting that our expert demonstration does not necessarily guarantee globally optimal exploration. The way we obtain the expert demonstration can be easily automated and scaled.

\textbf{Training details.} The network architectures for both \textit{TaskPlanner} and \textit{MotionPlanner} are given in Table~\ref{atm_network},\ref{actionassigner_network} in Appendix . In our implementation, the local observation sequence length is 10 (m=10) because we empirically found it to achieve a good performance-memory trade-off. We experimentally tested $m=20$ and got inferior performance. \textit{\acronym{}} network architecture is illustrated in Appendix~(parameter size is just 16~M). We train \textit{\acronym{}} with PyTorch~\cite{pytorch_package}. The optimizer is Adam~\cite{adam_optimizer} with an initial learning rate of 0.0005, but decays every 40 epochs with a decaying rate of 0.5. In total, we train 70 epochs. We train all the \textit{\acronym{}} variants with the same hyperparameter setting for a fair comparison.

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.90\linewidth]{gibson_expcompare.pdf}
    \vspace{-3mm}
    \caption{\textbf{Exploration trajectories Visualization}. Top row: various \textit{\acronym{}} variants exploration result~(1000-step budget) on Gibson \texttt{Mosquito} scene. Bottom row: exploration with different start positions~(E, F, G, 500-step budget, with agent step-size 0.25~m and turn-angle $10^\circ$). An agent with larger step size and turn angle~(0.3~m/$30^\circ$) achieves a similar coverage ratio with much smaller steps~(200 steps, F). The trajectory color evolving from cold\,(blue) to warm~(yellow) indicates the exploration chronological order.}
    \vspace{-4mm}
    \label{fig:traj_vis}
\end{figure*}

\begin{figure*}[h]
    \centering
    \includegraphics[width=0.95\linewidth]{MP3D_allvisexp.pdf}
    \caption{DeepExplorer exploration result on MP3D~\cite{Matterport3D} room scene \texttt{q9vSo1VnCiC}. We show both efficient exploration results ~(top row, sub-figure A, B, C). We also show relatively less-efficient exploration in sub-figure E and F, which are mainly due to local repetitive exploration. We further show an inefficient exploration example in sub-figure D. Two-room scene 3D visualization is given in the left-most subfigures. The agent exploration starting position is marked by a red rectangle patch.}
    \label{fig:mp3d_explore_rst}
\end{figure*}
\subsection{Comparison Methods}

For exploration task, we compare \textit{\acronym{}} with six RL-based methods: 1. \textbf{RL + 3LConv}: An RL Policy with 3 layer convolutional network~\cite{habitat19iccv}; 2. \textbf{RL + Res18}: RL Policy initialized with ResNet18~\cite{resnet18} and followed by GRU~\cite{GRU}; 3. \textbf{RL + Res18 + AuxDepth}: adapted from~\cite{learning2complex} which uses depth map prediction as an auxiliary task. The network architecture is the same as ANS~\cite{active_slam} with one extra deconvolution layer for depth prediction; 4. \textbf{RL + Res18 + ProjDepth} adapted from Chen \textit{et al.}~\cite{chen2018learning} who project the depth
image in an egocentric top-down in addition to the RGB image as input to the RL policy. 5. \textbf{ANS}~(Active Neural SLAM~\cite{learn2explore_iclr20}) jointly learns a local and global policy network to guide the agent to explore; 6. \textbf{OccAnt}~\cite{ramakrishnan2020occant}: takes RGB, depth, and camera as inputs to learn a 2D top-down occupancy map to help exploration. For ablation studies, we have following \textit{\acronym{}} variants:

\begin{enumerate}
    \item \textbf{RandomWalk} The agent randomly chooses an action to execute at each step. It serves as a baseline and helps us to know agent exploration capability without any active learning process. Please note that RandomWalk is also the SPTM~\cite{savinov2018semiparametric} exploration strategy.
    \item \textbf{\acronym\_NoDeepSup.} \textit{\acronym{}} without deeply-supervised learning. We remove LSTM per-step feature supervision in \textit{TaskPlanner} and neighboring frame action supervision in \textit{MotionPlanner}. In other words, we just keep the feature prediction and action classification between the latest step and the future step. It helps to test the necessity of involving a deeply-supervised learning strategy.
    \item \textbf{\acronym\_NoFeatDeepSup.} \textit{\acronym{}} without deeply-supervised learning in the feature space. We remove LSTM per-step feature supervision in \textit{TaskPlanner} but keep the neighboring frame action supervision in \textit{MotionPlanner}. This means no $\mathcal{L}_T$ but only $\mathcal{L}_M$. Together with \textbf{\acronym\_NoDeepSup} and \textbf{\acronym\_NoActDeepSup}, it helps to test the necessity of deploying deep supervision in both task and motion planning.
    \item \textbf{\acronym\_NoActDeepSup.} \textit{\acronym{}} without deeply-supervised learning regarding action prediction. We remove the neighboring frame action supervision in \textit{MotionPlanner} but keep the LSTM per-frame feature supervision in \textit{TaskPlanner}. In other words, there is no $\mathcal{L}_M$ but only $\mathcal{L}_T$. Together with \textbf{\acronym\_NoDeepSup} and \textbf{\acronym\_NoFeatDeepSup} It helps to test the necessity of deploying deep supervision in both task and motion planning.
    \item \textbf{\acronym\_LSTMActRegu.} \textit{TaskPlanner} hallucinates the next-best feature at each step to deeply supervise the whole framework in the feature space. As an alternative, we can instead predict action instead at each step in \textit{TaskPlanner}. This \acronym{} variant helps us to figure out whether supervising each step of \textit{TaskPlanner} LSTM in feature space is helpful.
    \item \textbf{\acronym\_withHistory.} \textit{\acronym{}} is trained with only a short-memory\,(the latest $m$ steps observations). To validate the influence of long-term memory, we train a new \textit{\acronym{}} variant by adding extra historical information: we evenly extract 10 observations among all historically explored observations excluding the latest $m$ steps. After feeding them to ResNet18\,\cite{resnet18} to get their embedding, we simply use average pooling to get one 512-dimensional vector and feed it to \textit{TaskPlanner} LSTM as the hidden state input.
    \item \textbf{\acronym\_noFeatHallu.} We use the architecture of \textit{TaskPlanner} to directly predict the next action. It discards task planning in feature space but instead plans directly in action space. Its performance helps us to understand if the hallucinated feature is truly necessary.
    \item \textbf{\acronym~(0.30m/$30^\circ$).} This variant adopts a different locomotion protocol than the one used in ANS \cite{learn2explore_iclr20} and all other variants to demonstrate \textit{\acronym{}}'s robustness under different locomotion setups.
    
    
\end{enumerate}
Some visualizations of different \textit{\acronym} variants' exploration results can be found in Fig.~\ref{fig:traj_vis}.

\subsection{Evaluation Results on Exploration}
The quantitative results of the exploration task are shown in Table~\ref{tab:coverage_ratio}. We can observe from this table that \textit{\acronym{}} achieves comparable performance on the Gibson dataset with the best RL-based methods and best-performing result on the MP3D dataset by outperforming all RL-based methods significantly~(about $13\%$ coverage ratio and $40 m^2$ area improvement). Since the comparing RL-based methods~\cite{eval_metric,learning2complex,chen2018learning,learn2explore_iclr20} build the map in metric space and requires millions of training images, \acronym{} is desirable because (1) it provides a metric-free option for exploration, and (2) it is lightweight~(in terms of parameter size 16~M) and requires much less training data~(just about 0.45 million images, in contrast with 10 million images required by most RL-based methods). The room scenes in MP3D dataset are much more complex and larger than those in the Gibson dataset. They contain various physical impediments~(e.g., complex layout, furniture), and some rooms contain outdoor scenarios. Hence, \textbf{\acronym{} exhibits stronger zero-shot sim2sim generalizability in exploring novel scenes than RL-based methods}. Moreover, the performance gain is more obvious on both Gibson and MP3D datasets when we change the agent to a different locomotion setup~(from 0.25/$10^\circ$ to 0.30/$30^\circ$), which also shows \acronym{} is robust to different locomotion setups.

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.80\linewidth]{vis_featspace.pdf}
    \vspace{-4mm}
    \caption{\textbf{Feature Visualization} \textbf{A}. The exploration trajectory~(blue to yellow, step size 0.25~m and turn-angle $30^\circ$) with a 500-step budget overlaid on top of the floor plan map. \textbf{B}. The spatially-adjacent panoramic images are connected~(purple lines) via VPR. \textbf{C}. The difference~(Euclidean distance in 512-d feature space) between real-observed features and hallucinated features. The darker the color, the lower the difference.}
    \label{fig:feat_vis}
\end{figure*}

On the Gibson dataset, \textit{\acronym{}} achieves a slightly lower coverage ratio than ANS~\cite{learn2explore_iclr20} but a higher average covered area. We find such performance difference is mainly caused by \textit{\acronym{}} stronger capability in exploring large areas than RL-based methods. In most cases, \textit{\acronym{}} actively reaches new areas within limited steps.

\textbf{Comparison with random exploration.} \textit{RandomWalk} serves as the baseline for our framework. It is also adopted by SPTM~\cite{savinov2018semiparametric} to build a topological map. It involves no learning procedure, and the agent randomly takes action at each step to explore an environment. From Table\,\ref{tab:coverage_ratio}, we can see that \textit{RandomWalk} dramatically reduces the exploration performance in terms of both coverage ratio and average coverage area. The inferior performance of \textit{RandomWalk} verifies the necessity of learning active exploration strategy in order to help the agent efficiently explore an environment. Figure~\ref{fig:traj_vis} demonstrates the qualitative comparison between \textit{RandomWalk} and \textit{\acronym{}} exploration result.

\textbf{Feature regularization and with history memory}. If we replace feature regularization involved in \textit{TaskPlanner} with action regularization~(\textit{\acronym\_LSTMActRegu}), we have observed more performance drop on MP3D than on Gibson dataset~($3\%$ versus $0.2\%$), which shows adopting feature regularization improves the generalizability  compared with action regularization. Moreover, introducing full history memory~(\textit{\acronym\_FullHistory}) to \textit{TaskPlanner}~(used as LSTM hidden state input) produces very similar results on the Gibson dataset, but significantly reduces the performance on MP3D dataset~(more than $2\%$ drop). It thus shows using historical memory tends to encourage \textit{\acronym{}} to overfit training data so that its generalizability is inevitably reduced.  We argue that such generalizability drop might lie in our over-simplified history memory modeling because we just evenly sample 10 nodes~(image observations) from all historically visited nodes, which might be too simple to represent the whole history memory, or even confuses \textit{TaskPlanner} if the agent has already explored many steps. A more elegant long-term history memory model remains to be explored.

\textbf{Deeply-supervised learning and joint task and motion imitation.} Removing deeply-supervised learning~(\textit{\acronym\_noDeepSup}, \textit{\acronym\_noFeatDeepSup}, \textit{\acronym\_noActDeepSup}) leads to performance drop on both Gibson and MP3D dataset, especially when both deep supervisions in \textit{TaskPlanner} and \textit{MotionPlanner} are both dropped. In the MP3D dataset, it can even lead to worse performance than \textit{RandomWalk}. It thus shows the necessity of deep supervision in both feature space~(\textit{TaskPlanner}) and action space~(\textit{MotionPlanner}). Meanwhile, \textit{\acronym\_noFeatHallu} leads to a significant performance drop on both Gibson and MP3D datasets. It thus attests to the advantage of our feature-space task and motion imitation strategy which jointly optimize \textit{TaskPlanner} for high-level task allocation and \textit{MotionPlanner} for low-level motion control.

We also visualize the comparison between \textit{\acronym{}} hallucinated next-step future feature and truly observed feature in Fig.~\ref{fig:feat_vis}~(C). We see that the hallucinated feature is more similar to the observed real feature when the agent is walking through a spacious area~(in other words, the agent mostly takes \texttt{move\_forward} action) than when the agent is walking along a room corner, against the wall or through a narrow pathway. This may be due to the learned \textit{TaskPlanner} most likely hallucinates feature moving the agent forward if the temporary egocentric environment allows. This also matches expert exploration experience because experts mostly prefer moving forward so as to explore as many areas as possible.

\begin{table*}[t]
  \centering
    \caption{Navigation Result. Top three performances are highlighted in \textbf{\textcolor{red}{red}}, \textbf{\textcolor{Green}{green}}, and \textbf{\textcolor{blue}{blue}} color, respectively. We collect 2000~(Gibson)/5000(MP3D) images with agent setup~(0.25m/$10^\circ$). `N/A' means `not available.'}
    \scriptsize
    \label{table:nav_rst}
    \vspace{-1mm}
  \begin{tabular}{c|cc|cc}
  \hline
    \multirow{2}{*}{Method}  & \multicolumn{2}{c|}{Gibson Val} & \multicolumn{2}{c}{\shortstack{Domain Generalization on MP3D Testset}} \\
    \cline{2-5}
    & Succ. Rate~($\uparrow$) & SPL~($\uparrow$) & Succ. Rate~($\uparrow$) & SPL~($\uparrow$)\\
\hline
RandomWalk & 0.027 & 0.021 & 0.010 & 0.010 \\
RL + Blind & 0.625 & 0.421 & 0.136 & 0.087 \\
RL + 3LConv + GRU\,\cite{eval_metric} & 0.550 & 0.406 & 0.102 & 0.080 \\
RL + ResNet18 + GRU & 0.561 & 0.422 & 0.160 & 0.125 \\
RL + ResNet18 + GRU + AuxDepth\,\cite{learning2complex} & 0.640 & 0.461 & 0.189 & 0.143 \\
RL + ResNet18 + GRU + ProjDepth\,\cite{chen2018learning} & 0.614 & 0.436 & 0.134 & 0.111 \\
IL + ResNet18 + GRU & 0.823 & 0.725 & \textbf{\textcolor{blue}{0.365}} & \textbf{\textcolor{blue}{0.318}} \\
SPTM~\cite{savinov2018semiparametric} & 0.510 & 0.381 & 0.240 & 0.203 \\
CMP~\cite{CMP} & 0.827 & \textbf{\textcolor{blue}{0.730}} & 0.320 & 0.270 \\
OccAnt~(RGB)~\cite{ramakrishnan2020occant} & \textbf{\textcolor{blue}{0.882}} & 0.712 & N/A & N/A \\
ANS~\cite{learn2explore_iclr20} & \textbf{\textcolor{Green}{0.951}} & \textbf{\textcolor{Green}{0.848}} & \textbf{\textcolor{Green}{0.593}} & \textbf{\textcolor{Green}{0.496}} \\
\hline
\acronym{}& \textbf{\textcolor{red}{0.957}} & \textbf{\textcolor{red}{0.859}} & \textbf{\textcolor{red}{0.733}} & \textbf{\textcolor{red}{0.619}} \\
\hline
  \end{tabular}
  \label{tab:navigation}
\end{table*}

\subsection{Evaluation Results on Navigation}

For the visual navigation task, we compare \textit{\acronym{}} with most of the methods compared in the exploration task. CMP~\cite{CMP} builds up a top-down belief map for joint planning and mapping. For OccAnt~\cite{ramakrishnan2020occant}, we just report its result with the model trained with RGB image~(so as to be directly comparable with \textit{\acronym}). For SPTM~\cite{savinov2018semiparametric}, we train all its navigation-relevant models on data obtained by \textit{\acronym}. The navigation result is given in Table~\ref{table:nav_rst}. We can see that \textit{\acronym{}} outperforms all comparing methods on the two datasets, with the largest performance gain on the MP3D dataset~(about $14\%$ Succ. Rate, $12\%$ SPL improvement). Hence, we can see that our \textit{\acronym}-built topological map can be used for image-goal-based visual navigation. More importantly, \textit{\acronym{}} shows satisfactory zero-shot sim2sim generalizability in navigation as well. In Fig.~\ref{fig:feat_vis}~(B), we can see VPR and \textit{ActionAssigner} successfully add new edges~(purple lines) for loop closing. The resulting topological map, after topological mapping, fully reflects environment connectivity and traversability.

\subsection{Zero-Shot Sim2Real Real-World Exploration}

 \textit{\acronym{}} is deployed and verified on a customized real-world robot. We set an Insta360 Pro 2 camera\footnote{\url{https://www.insta360.com/cn/product/insta360-pro2}} on an iRobot Create 2 robot\footnote{\url{https://edu.irobot.com/what-we-offer/create-robot}}~(the camera height is around $1.5~m$). Nvidia Jetson TX2\footnote{\url{https://www.nvidia.com/en-gb/autonomous-machines/embedded-systems/jetson-tx2/}} platform is used to launch the \textit{\acronym{}} model and control the robot. We directly deploy the model~(with step size $0.25m$ and turn-angle $10^{\circ}$) trained on the Gibson simulation dataset without any fine-tuning on the real-world dataset. The robot's physical configuration is as close to that of the simulation as possible. We adopt a LiDAR scanner for obstacle avoidance. The experiment environment is a large indoor multi-functional office building.

We find that \textit{\acronym{}} demonstrates strong zero-shot sim2real exploration results: the robot is capable of identifying obstacles and actively reaching the open navigable areas. The robot can traverse the entire hallway and enter the only open door (marked with a star in Fig.~\ref{fig:teasingfig}) and manage to exit it through the door after exploring it. The exploration trajectory is shown in Fig.~\ref{fig:teasingfig} and Fig.~\ref{fig:realworld_traj_redstar}~(in Appendix). The corresponding video can be found on the Github repository.

\subsection{Limitations}
In our experiment on the simulation datasets, we find that \textit{\acronym{}} sometimes leads to inefficient exploration in complex room environments as is shown in Fig.~\ref{fig:mp3d_explore_rst}~(bottom row), especially when the room layout is sophisticated and the navigable area is narrow. We hypothesize that this is partly due to the lack of full history memory of \textit{\acronym{}} that can steer the agent away from already-covered areas. Although we have tried one simple history memory mechanism~(\acronym\_withHistory), it still remains a future research topic to design a better history memory framework. 

In the zero-shot sim2real exploration experiment, we find that the agent often mistakes large glass walls for open doorways. We speculate that the lack of relevant data in the Gibson training dataset, which is entirely made of the household environment, leads to this failure. Extra measurement should be considered to handle such cases.