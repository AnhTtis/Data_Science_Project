\section*{Appendix}

\subsection{DeepExplorer Neural Network Architecture}
\label{nn_architecture}
\textit{DeepExplorer} neural network architecture is given in Table~\ref{atm_network}, it consists of ResNet18~(for image observation embedding), LSTM layer~\cite{LSTM}~(for \textit{TaskPlanner}) and  multi-layer perceptron~(MLP)~(for \textit{MotionPlanner}). Please note that \textit{DeepExplorer} is lightweight, its parameter size is just 16~M~(where M is million).

\begin{table}[h]
    \centering
    \small
    \caption{DeepExplorer network architecture illustration. The network consists of basic 2D image convolution layers, such as ResNet18, LSTM, and FC. The network is lightweight, the parameter size is just 16~M.}
    \scriptsize
    \begin{tabular}{|c|c|c|}
        \hline
        Layer Name & Filter Num  & Output Size \\ 
        \hline
        \multicolumn{3}{|c|}{\textbf{Image Embedding Layer}}\\
        \hline
        \multicolumn{3}{|c|}{Input: [10, 3, 256, 512]}\\
        \multicolumn{3}{|c|}{Embedding Network: ResNet18} \\
        \multicolumn{3}{|c|}{Embedding Size: [10, 512]}\\
        \hline
        \multicolumn{3}{|c|}{\textbf{Task Planner Network}}\\
        \hline
        LSTM & layers = 2, hidden size = 512 & [10, 512]\\
        % \hline
        Feat Prediction FC & in feat = 512, out feat = 512 & [10, 512] \\
        \hline
        \multicolumn{3}{|c|}{\textbf{Motion Planner Network}} \\
        \hline
        \multicolumn{3}{|c|}{Input: Feat [10, 1024], Action: [10]} \\
        \hline
        Feat Merge FC & in feat = 1024, out feat = 512 & [10, 512]\\
        Action Classification FC & in feat = 512, out feat = 3 & [10, 3] \\
        \hline
    \end{tabular}
    \label{atm_network}
\end{table}

\begin{table}[h]
    \centering
    \small
    \caption{\textit{ActionAssigner} Neural Network Architecture. The whole parameter size is $13.5$~M.}
    \scriptsize
    \begin{tabular}{|c|c|c|}
        \hline
        Layer Name & Filter Num  & Output Size \\ 
        \hline
        \multicolumn{3}{|c|}{\textbf{Image Embedding Layer}}\\
        \hline
        \multicolumn{3}{|c|}{Input: [2, 3, 256, 512]}\\
        \multicolumn{3}{|c|}{Embedding Network: ResNet18} \\
        \hline
        \multicolumn{3}{|c|}{\textbf{Feat. Merge Layer}}\\
        \hline
        \multicolumn{3}{|c|}{Concat. Size: [1, 1024]}\\
        \hline
        FC & in feat = 1024, out feat = 512 & [1, 512]\\
        \hline
        \multicolumn{3}{|c|}{\textbf{Action Predict Branch}}\\
        \hline
        head1 FC & in feat = 512, out feat = 128 & [1, 128] \\
        head2 FC & in feat = 512, out feat = 128 & [1, 128] \\
        head3 FC & in feat = 512, out feat = 128 & [1, 128] \\
        head4 FC & in feat = 512, out feat = 128 & [1, 128] \\
        head5 FC & in feat = 512, out feat = 128 & [1, 128] \\
        head6 FC & in feat = 512, out feat = 128 & [1, 128] \\
        \hline
        \multicolumn{3}{|c|}{\textbf{Action Predict}} \\
        \hline
        \multicolumn{3}{|c|}{Concat. Size: [1, 6, 128]} \\
        \hline
        BiLSTM & layers = 1, out feat = 128 & [1, 6, 128]\\
        Action Classify FC & in feat = 128, out feat = 3 & [1, 6, 3] \\
        \hline
    \end{tabular}
    \label{actionassigner_network}
\end{table}

\subsection{ActionAssigner Network Architecture}

The \textit{ActionAssigner} neural network is given in Table~\ref{actionassigner_network}. The \textit{ActionAssigner} also uses ResNet18~\cite{resnet18} as the image embedding module. Then it uses a sequence of multi-layer perceptron~(MLP) to predict multi-step actions separately~(step length is 6), each step independently predicts one action. So \textit{ActionAssigner} is a multi-label classification neural network. Bidirectional LSTM is applied to model mutual action dependency among different steps. The parameter size is $13.5$~M. We train \textit{ActionAssigner} with the same parameter setting as of \textit{TaskPlanner} and \textit{MotionPlanner}~(the network in Table~\ref{atm_network}). During training data preparation, if the action list length is smaller than 6, we pad \texttt{STOP} action to fill the length.

\subsection{Coverage Ratio Progression Comparison}

We further provide the coverage ratio progression variation w.r.t. exploring steps comparison between \textit{DeepExplorer} and ANS~\cite{learn2explore_iclr20}, one RL-based method~(RL+ProjDepth) in Fig.~\ref{fig:coverage_ratio}. The comparison is based on Gibson validation dataset~\cite{gibson_env}, and we divide the room into \textit{Large}, \textit{Small}, and \textit{Overall} according to the room size. From this table, we can see that \textit{DeepExplorer} is capable of covering more area during the first 200 steps than ANS~\cite{learn2explore_iclr20} on large rooms~(which is verified by the more steep curve of \textit{DeepExplorer} over ANS~\cite{learn2explore_iclr20} and RL+ProjDepth, in the middle sub-figure).

\begin{figure}[t]
    \centering
    \includegraphics[width=0.98\linewidth]{coverage_ratio.pdf}
    \caption{Coverage ratio variation curve comparison over 1000-step budget over large $> 50 m^2$, small $< 50 m^2$ and all~(average) room size, respectively.}
    \label{fig:coverage_ratio}
\end{figure}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.90\linewidth]{real_world_traj.png}
    \caption{DeepExplorer exploration trajectory on zero-shot sim2real experiment. We can see that the agent can successfully find the navigable area along the corridor to efficiently explore more areas. It can also manage to enter and exit one conference room~(the area marked with a red star).}
    \label{fig:realworld_traj_redstar}
\end{figure}

\subsection{Zero-Shot Sim2Real Exploration Discussion}

We provide two exploration videos in the supplementary folder. One video demonstrates the successful exploration~(with the exploration trajectory shown in Fig.~\ref{fig:realworld_traj_redstar}), and the other video shows one unsuccessful exploration case in which the agent mixes the glass walls with an open area.

The agent we used for real-world exploration contains large actuation noise, so the actual angle it has turned may be different from our configuration~($10^\circ$). Sometimes its actual executed turn angle can be as large as $20\circ$ or even $30^\circ$, especially in the conference room where the floor is overlaid with carpet~(yellow color and red star marked area in Fig.~\ref{fig:realworld_traj_redstar}). The existence of actuation noise explains the non-smoothness between adjacent frames in our provided video, especially when the agent entered the conference room.

Although the actuation noise is caused by the agent, we find our trained \textit{DeepExplorer} model can predict the appropriate actions to mitigate the actuation noise impact. For example, when the agent has turned a larger angle than the configuration~(e.g. turn left), \textit{DeepExplorer} can predict a contrary action~(e.g. turn right) to the correct agent.


