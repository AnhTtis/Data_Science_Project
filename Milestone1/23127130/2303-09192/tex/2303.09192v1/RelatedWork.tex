\section{Related Work}

\textbf{Topological Map in Exploration and Navigation.} Inspired by the animal and human psychology~\cite{tolman1948cognitive}, a large amount of work has recently proposed to build topological map to represent an environment~\cite{graphtopoexplore,Murphy08ICRA,neural_topomap,beeching2020learning,savinov2018semiparametric,VisualGraphMem_ICCV21,MRTopoMap,Savarese-RSS-19}. They use the topological map for tasks such as navigation~\cite{neural_topomap,learn2explore_iclr20,savinov2018semiparametric,VisualGraphMem_ICCV21,Savarese-RSS-19}, exploration~\cite{learn2explore_iclr20,graphtopoexplore,Murphy08ICRA,savinov2018semiparametric,MRTopoMap,TSGM} and planning~\cite{beeching2020learning}. To build the topological map, they combine various sensors such as RGB image, depth map~\cite{TSGM,Savarese-RSS-19}, pose~\cite{neural_topomap,learn2explore_iclr20,beeching2020learning} and even LiDAR scanner~\cite{MRTopoMap,graphtopoexplore}. Some of them further adopt data-hungry and computation-demanding Reinforcement Learning~(RL) techniques to train the model to construct the topological map~\cite{neural_topomap,learn2explore_iclr20,VisualGraphMem_ICCV21}. Kwon \textit{et al.}~\cite{VisualGraphMem_ICCV21} combine imitation learning~(IL) and RL to train the model. Some of these methods~\cite{neural_topomap,learn2explore_iclr20,beeching2020learning} involve metric information to construct the topological map. N.~Savinov~\textit{et. al.}~\cite{savinov2018semiparametric} use the random walk to construct the topological map, which inevitably leads to an inefficient topological map. TSGM~\cite{TSGM} jointly adds surrounding objects during topological map construction. Unlike these prior works, our \textit{\acronym{}} is completely metric-free and simple in experimental configuration~(just RGB image, much smaller expert demonstration size).

\textbf{Hallucinating Future Feature.} The idea of hallucinating future latent features has been discussed in other application domains. Previous work has utilized this idea of visual anticipation in video prediction/human action prediction~\cite{16Vondrick,17Zeng,20Chang,21Fernando,Suris2021LearningTP}, and researchers have applied similar ideas to robot motion and path planning~\cite{Jain2016RecurrentNN, Koppula2016, Carlone2019, Park2016}. As stated in~\cite{16Vondrick,17Zeng,Suris2021LearningTP}, visual features in the latent space provide an efficient way to encode semantic/high-level information of scenes, allowing us to do planning in the latent space, which is considered more computationally efficient when dealing with high-dimensional data as input~\cite{Lippi2020,Ichter2019}. Different from previous robotics work, we take advantage of this efficient representation by adding deep supervision when anticipating the next visual feature, which was computationally intractable if we were to operate at the pixel level.

\textbf{Deeply-Supervised Learning} has been extensively explored~\cite{deeply_supervised_nets,knowledge_synergy,li2017deep,li2018deep} during the past several years. The main idea is to add extra supervision to various intermediate layers of a deep neural network in order to more effectively train deeper neural networks. In our work, we adopt a similar idea to deeply supervise the training of feature hallucination and action generation.


\begin{figure*}[t]
    \centering
    \includegraphics[width=0.8\linewidth]{Topo_map.pdf}
    \caption{\textbf{Training and inference for task and motion imitation.} Feature extractor $g_\psi$ takes image $I_t$ as input and generates the corresponding feature vector $f_t$. \textit{TaskPlanner} $\pi_{\theta_T}$ is a recurrent neural network (RNN) consuming a sequence of features $\{ f_{t-10}, \cdots, f_t\}$ to hallucinate the next best feature to visit $\hat{f}_{t+1}$. \textit{MotionPlanner} $\pi_{\theta_M}$ consumes the concatenation (denoted by $\bigoplus$) of ${f}_{t}$ and $\hat{f}_{t+1}$ and generates the action to move the agent towards the hallucinated feature. During training, we supervise all the intermediate outputs including the intermediate hallucinated features $\{ \hat{f}_{t-9}, \cdots, \hat{f}_{t} \}$ and the intermediate actions $\{ \hat{a}_{t-10}, \cdots, \hat{a}_{t-1} \}$, in addition to the final output $\hat{f}_{t+1}$ and $\hat{a}_t$. During inference, current observation $I_t$ is firstly encoded and fed into $\pi_{\theta_T}$ to hallucinate $\hat{f}_{t+1}$, and then $\hat{f}_{t+1}$ combined with the ${f}_{t}$ is fed into $\pi_{\theta_M}$ for motion planning. $\mathcal{L}_T$ is $L_2$ loss and $\mathcal{L}_M$ is cross entropy loss (the subscripts $T$ and $M$ denote \textbf{T}ask and \textbf{M}otion respectively). $h_t$ denotes the hidden state of RNN.} 
    \label{fig:pipeline}
    \vspace{-5mm}
\end{figure*}

\textbf{Task and Motion Planning.} Task and motion planning (TAMP) divides a robotic planning problem into high-level task allocation (task planning) and low-level action for task execution (motion planning). This hierarchical framework is adopted in many robotic tasks such as manipulation \cite{chitnis2016guided,mcdonald2022guided} exploration~\cite{Cao-RSS-21} and navigation \cite{lo2018petlon,thomas2021mptp}. Such a framework allows us to leverage high-level information about the scenes to tackle challenges in local control techniques~\cite{bansal2019-lb-wayptnav}. In this work, to perform active topological mapping of a novel environment, the agent firstly reasons at the highest level about the regions to navigate: hallucinate the next best feature point to visit. Afterward, the agent takes an action to get to the target feature. The whole procedure is totally implemented in feature space without any metric information.

\textbf{Imitation Learning} aims to mimic human behavior or expert demonstrations for a given specific task~\cite{imitation_learning,il_legged,il_planning}. The agent is trained to perform tasks by directly observing demonstrations~\cite{il_legged,il_planning}. In our work, the expert demonstration is a set of image-action pair sequences that an agent would observe along a route that efficiently covers an environment. It is widely accessible in either real-world or simulated environments~(e.g. from human experts or maps of environments). 