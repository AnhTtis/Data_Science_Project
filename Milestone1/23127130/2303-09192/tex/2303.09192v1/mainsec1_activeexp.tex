\section{Topological Exploration in Feature Space}
\label{active_explore}

Our topological map is represented by a graph $\mathcal{G} = (\mathcal{I}, \mathcal{A})$, where the graph nodes denoted by $\mathcal{I}$ is a set of RGB panoramic image observations collected by the agent at different locations $\mathcal{I} = \{I_1, I_2, \cdots, I_N\}$ (where $N$ denotes the number of nodes), and the edges denoted by $\mathcal{A}$  is composed of a set of actions $a_{(I_i, I_j)} \in \mathcal{A}$ which moves an agent between the two spatially adjacent observations $I_i$ and $I_j$. Each RGB panoramic image is of size $256\times 512$, and the action space consists of three basic actions: \texttt{move\_forward}, \texttt{turn\_left}, and \texttt{turn\_right}. Our visual exploration aims at maximizing the topological map coverage over an environment given a certain step budget $N$. The coverage of the topological map denoted by $\mathcal{C}$ is defined as the total area in the map that is known to be traversable or non-traversable. Mathematically, let $\pi_\theta$ denote the policy network parameterized by $\theta$, $a_t$ denote the action taken at step $t$, and $\Delta \mathcal{C} (a_t)$ denote the gain in coverage introduced by taking action $a_t$, the following objective function is optimized to obtain the optimal exploration policy $\pi^*$:
\vspace{-2mm}
\begin{equation}\label{eq:general_objective}
    \pi^* = \arg\max_{\pi_{\theta}} \underset{a_t \sim \pi_{\theta}}{\mathbb{E}}(\sum_{t=1}^{N}\Delta \mathcal{C}(a_t)) \ .
\end{equation}

\textbf{Learning from expert demonstrations.} In literature, most works solve Eq.~(\ref{eq:general_objective}) by reinforcement learning to maximize the reward~\cite{chen2018learning,active_slam}, such solutions are not only data-hungry but also require complicated training involving metric information. Differently, we adopt imitation learning~\cite{imitation_learning} to let our policy network $\pi_\theta$ mimic the output of the expert policy $\tilde{\pi}$ which could come from either an \textit{oracle policy} having full access to virtual environments or simply a \textit{human expert} in real world~(more discussion is in Sec.~\ref{expconfig_sec}). Hence, our objective is to minimize the difference between our policy network and the expert policy:
\begin{equation}\label{eq:our_objective}
    \pi^* = \arg\min_{\pi_{\theta}} \mathcal{L}(\pi_\theta,\tilde{\pi}) \ ,
\end{equation}
where $\mathcal{L}$ measures the discrepancy between two policies. We propose the task and motion imitation in feature space to solve Eq.~(\ref{eq:our_objective}) which will be introduced in the following (see Fig.\,\ref{fig:pipeline}). We respectively introduce the feature extraction (\ref{subsec:feature_extraction}), the policy network $\pi_\theta$ composed of a \textit{TaskPlanner} denoted by $\pi_{\theta_T}$ (\ref{subsec:task_planner}) as well as a \textit{MotionPlanner} denoted by $\pi_{\theta_M}$ (\ref{subsec:motion_planner}), and the deeply-supervised learning strategy (\ref{subsec:learning}).



\subsection{Image Feature Extraction}
\label{subsec:feature_extraction}
We firstly encode each visual observation $I_t \in \mathcal{I} (t=1,2,...,N)$ with a feature extractor $g_\psi$ parameterized by $\psi$ which uses the ImageNet\,\cite{ILSVRC15} pre-trained ResNet18 backbone\,\cite{resnet18}. The feature embedding $f_t \in \mathbb{R}^{d} (d=512)$ is obtained by $f_t = g_\psi (I_t)$, (see Fig.\,\ref{fig:pipeline}). Note that $g_\psi$ is jointly optimized with the task planner $\pi_{\theta_T}$ as well as the \textit{MotionPlanner} $\pi_{\theta_M}$ via imitation learning.

\subsection{Task Planner for Next Best Feature Hallucination}\label{subsec:task_planner}
\textit{TaskPlanner} $\pi_{\theta_T}$ parameterized by $\theta_T$  takes the most recent $m$-step visual features $\mathcal{F}=\{f_{t-m}, \cdots, f_{t}\}$ as input, and learns to hallucinate the next best feature to visit which is denoted by $\hat{f}_{t+1}$, see Fig.~\ref{fig:pipeline}. In specific, $\pi_{\theta_T}$ is a two-layer LSTM\,\cite{LSTM}: 
\begin{equation}\label{eq:task_planner}
    \hat{f}_{t+1} = \pi_{\theta_T}(f_{t-m}, \cdots, f_{t} | \theta_T) \ .
\end{equation}

To save computation, $\pi_{\theta_T}$ only takes the most recent $m$-step features as input and we empirically find that $m=10$ achieves good performance. In other words, \textit{TaskPlanner} is only equipped with a short-term scene memory, and it tries to extend the feature space as quickly as possible in order to guide the agent to perform efficient exploration.  Essentially, \textit{TaskPlanner} is planning in the feature space. This efficient representation of the environment enables us to deploy deep supervision strategy introduced in Section \ref{subsec:learning}.




\subsection{Motion Planner for Action Generation}\label{subsec:motion_planner}
\textit{MotionPlanner} $\pi_{\theta_M}$ parameterized by $\theta_M$ takes the hallucinated feature $\hat{f}_{t+1}$ and the current feature $f_{t}$ as input, and outputs the action taking the agent towards the hallucinated goal~(see Fig.~\ref{fig:pipeline}). Specifically, $\pi_{\theta_M}$ is a multi-layer-perceptron (MLP) taking the concatenation of two features as input to classify the action:
\begin{equation}\label{eq:motion_planner}
    \hat{a}_{t} = \pi_{\theta_M}(\hat{f}_{t+1}, f_{t} | \theta_M) \ .
\end{equation}


\subsection{Deeply-Supervised Imitation Learning Strategy}\label{subsec:learning}


Our imitation pipeline is shown in Fig.\,\ref{fig:pipeline}. Given an expert exploration demonstration including a sequence of images and the corresponding expert actions $\mathcal{{E}}=\{\{{I}_1, {a}_1\}, \{{I}_2, {a}_2\}, \cdots, \{{I}_N, {a}_N\}\}$, we adopt the deeply-supervised learning strategy~\cite{deeply_supervised_nets} to jointly optimize the feature extractor $g_\psi$, task planner $\pi_{\theta_T}$, and \textit{MotionPlanner} $\pi_{\theta_M}$. Ultimately, our objective in Eq.~(\ref{eq:our_objective}) becomes,
\begin{equation}\label{eq:our_objective1}
    \min_{\psi,\theta_T,\theta_M} \sum_{t=1}^{N-1}\mathcal{L}_T(\hat{f}_{t+1},{f}_{t+1}) + \sum_{t=1}^{N}\mathcal{L}_M(\hat{a}_t, {a}_t) \ ,
\end{equation}
where $\mathcal{L}_T$ is $L_2$ loss to measure the discrepancy between two features, and $\mathcal{L}_M$ is cross-entropy loss to make the model imitate the expert action. The desired target feature ${f}_{t+1}$ is obtained by ${f}_{t+1} = g_\psi({I}_{t+1})$ (${I}_{t+1}$ is obtained from the expert demonstration  $\mathcal{E}$), the desired action ${a}_{t}$ is also read from $\mathcal{E}$, the hallucinated feature $\hat{f}_{t+1}$ is calculated by Eq.~(\ref{eq:task_planner}), and the generated action $\hat{a}_t$ is computed by Eq.~(\ref{eq:motion_planner}). For each training iteration, we randomly clip  $m+1$ observations and the corresponding $m$ actions from an expert exploration\,($m=10$ and $N\gg m$), and feed them to $g_\psi$, $\pi_{\theta_T}$, and $\pi_{\theta_M}$. During exploration, we iteratively take the latest $m$ image observations as input, after which we first call task planner $\pi_{\theta_T}$ to hallucinate the next best feature and then motion planner $\pi_{\theta_M}$ to predict the next best action taking the agent to the hallucinated feature accordingly. By constantly executing the predicted action, the agent efficiently explores an environment. 

The whole pipeline is shown in Fig.~\ref{fig:pipeline}, in which we deeply supervise all intermediate output. Specifically, in \textit{TaskPlanner}, instead of simply hallucinating the next best feature, we simultaneously hallucinate all intermediate feature for each step and supervise all hallucinations by truly image observations. In \textit{MotionPlanner}, we deeply supervise the action prediction in a similar fashion. We show by experiment that such deeply-supervised learning strategy~\cite{deeply_supervised_nets} endows the agent with more powerful exploration capability.