learn something about the training dynamics of neural networks by developing a theory explaining the behavioral pattern rooted in the computational mechanisms within the network and their evolution over time.
In this work, we introduce two broad hypotheses about the mechanisms \emph{inside} the network that could explain grokking. We then attempt to distinguish which hypothesis more likely to explain grokking in two toy settings via post-hoc empirical analysis. Our two hypotheses are as follows:

\begin{hypothesis}[Compression] \label{hypo:compression}
Grokking arises when a large subnetwork initially memorizes the training set, and then this large subnetwork is eventually compressed into a sparser subnetwork, leading to the generalization phase transition.
\end{hypothesis}

\Cref{hypo:compression} resembles the information bottleneck hypothesis where learning can be explained by initially memorize the data, and then compressing the representation of the data such that the predictor generalizes \will{cite}. In other words, it suggests the network is learning by first memorizing the data, and then compressing it.
One of the consequences of \Cref{hypo:compression} is that initially learning to memorize the data may be a causal step towards finding a generalizing solution. Empirically, \Cref{hypo:compression} predicts that we should observe that most neurons active in the generalization phase should also have been active during the compression phase.

\begin{hypothesis}[Competition] \label{hypo:competition}
Grokking arises from competition between two distinct subnetworks: a dense one that memorizes the training set, and a sparse one that slowly learns to represent the generalizing solution. We observe grokking because model predictions are initially dominated by the memorizing subnetwork, but, eventually, the full network predictions transition to depending mostly on the sparse subnetwork.
\end{hypothesis}

In contrast to the compression hypothesis, the competition hypothesis predicts the neurons activated during the generalization phase should not have high overlap with the neurons activated during the compression phase. It also predicts that the memorization transition is not causally important for achieving generalization: if we can manipulate the network or problem to make the generalization transition happen earlier, we may even be able to immediately jump to generalization without going through the memorization phase. That is, under \Cref{hypo:competition}, the memorization phase is not a part of learning to solve the task, but an artifact of the fact that, for certain computational tasks, learning to memorize the training data is less hard than learning the underlying task, and the network is implementing these two strategies in parallel.

\paragraph{Contributions.} Overall, our results suggest that a competition-based mechanistic explanation of grokking is more plausible than a compression-based explanation. Specifically, analyzing two key case studies of grokking, we make the following observations:
\begin{enumerate}
    \item The neurons making up the active circuit during the memorization phase are largely distinct from the neurons active during the generalization phase. The generalization phase transition corresponds to the generalization neurons overtaking the memorization neurons in explaining network behavior.
    \item The generalization-phase subnetwork is more sparse than the memorization-phase subnetwork.
    \item The memorization-phase subnetwork is not, strictly speaking memorizing. Rather, it is a low-order hypothesis consistent with the training set that in certain cases can make nontrivial predictions for the generalization set. 
\end{enumerate}