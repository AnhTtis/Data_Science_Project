\vspace{-4mm}
\section{Entropy-dissipation Informed Neural Network}
\vspace{-3mm}
In this section, we present the proposed \EINN\ framework for the MVE.
To understand the intuition behind our design, we first write the continuity equation (\ref{eqn_CE}) in a similar form as the MVE (\ref{eqn_MVE_CE}):
\vspace{-1mm}
\begin{equation} \label{eqn_CE_as_MVE}
	\partial_t \rho^f_t(\vx) + \udiv \bigg(\rho_t^f(\vx) \Big(\gA[\rho_t^f](\vx) + \delta_t(\vx) \Big) \bigg) = 0,
\end{equation}
where $f$ is the hypothesis velocity (recall that $f_t(\cdot) = f(t, \cdot)$) and 
\begin{equation} \label{eqn_perturbation}
    \text{(Perturbation)}\quad 
	\delta_t(\vx) \defi f_t(\vx) - \gA[\rho_t^f](\vx)
\end{equation}
can be regarded as a perturbation to the original MVE system.
Consequently, it is natural to study the deviation of the hypothesis solution $\rho^f_t$ from the true solution $\bar \rho_t$ using an appropriate Lyapunov function $L(\rho_t^f, \bar \rho_t)$. 
The functional relation between this deviation and the perturbation is termed as the {\emph{stability}} of the underlying dynamical system, which allows us to derive the \EINN\ loss (\ref{eqn_self_consistency_potential}). 
Following this idea, the design of the \EINN\ loss can be determined by the choice of the Lyapunov function $L$ used in the stability analysis. 
In the following, we describe the Lyapunov function used for the MVE with the Coulomb interaction and the 2D NSE (MVE with Biot-Savart interaction). The proof of the following results are the major theoretical contributions of this paper and will be elaborated in the analysis section \ref{section_analysis}.
\begin{itemize}[leftmargin=*]
	\item For the MVE with the Coulomb interaction, we choose $L$ to be the \emph{modulated free energy} $E$ (defined in equation (\ref{DefModFree})) which is originally proposed in \citep{bresch2019mean} to establish the mean-field limit of a corresponding interacting particle system. We have (setting $L = E$)
	{
	\begin{equation}
		\frac{\ud}{\ud t } E(\rho_t^f, \bar \rho_t) \leq \frac 1 2  \int_{\mathcal{X}} \rho_t^f(\vx) \|\delta_t(\vx) \|^2 \ud \vx  + C\, E(\rho_t^f, \bar \rho_t),
	\end{equation}
	}
	where $C$ is a universal constant depending on $\nu $ and $(\bar\rho_t)_{t \in [0, T]}$. 
	\item For the 2D NSE (MVE with the  Biot-Savart interaction), we choose $L$ as the KL divergence. Our analysis is inspired by \citep{jabin2018quantitative} which for the first time establishes the quantitative mean-field limit of the stochastic interacting particle systems where the interaction kernel can be  in some negative Sobolev space. We have 
	{
	\begin{equation}
		\frac{\ud }{\ud  t} \mathbf{KL}(\rho_t^f, \bar \rho_t) \leq - \frac \nu  2 \int_\X \rho^f_t(\vx) \|\nabla \log \frac{\rho^f_t}{\bar \rho_t}(\vx)\|^2 +C\, \mathbf{KL}(\rho_t^f, \bar \rho_t)+ \frac{1}{\nu } \int_\X \rho_t^f(\vx)  \|\delta_t(\vx)\|^2 \ud \vx, 
	\end{equation}
	}
where again $C$ is a universal constant depending on $\nu $ and $(\bar\rho_t)_{t \in [0, T]}$. 
\end{itemize}
After applying Gr\"onwall's inequality on the above results, we can see that the \EINN\ loss (\ref{eqn_self_consistency_potential}) is precisely the term derived by stability analysis of the MVE system with an appropriate Lyapunov function.
In the next section, we elaborate on how a stochastic approximation of $\nabla_\theta R(f_\theta)$ can be efficiently computed for a parameterized hypothesis velocity field $f = f_\theta$ so that stochastic optimization methods can be utilized to minimize $R(f_\theta)$.
% However, the \EINN\ loss remains elusive from a computational perspective. Moreover, when the hypothesis velocity field is parameterized as $f = f_\theta$, we must be able to compute (a stochastic approximation of) $\nabla_\theta R(f_\theta)$ so that stochastic optimization methods can be utilized. This is elaborated in the next section.


\subsection{Stochastic Gradient Computation with Neural Network Parameterization} \label{section_NN_parameterization}
While the choice of the \EINN\ loss (\ref{eqn_self_consistency_potential}) is theoretically justified through the above stability study, in this section, we show that it admits an estimator which can be efficiently computed. 
Define the flow map $X_t$ via the ODE $\ud \vx(t) = f_t(\vx(t); \theta)\ud t$ with $\vx(0) = \vx_0$  such that $\vx(t) = X_t(\vx_0)$.
% (suppose that $f_t$ is Lipschitz continuous for all $t\in[0, T]$ so the trajectory exists and is unique). 
From the definition of the push-forward measure, one has $\rho^f_t = X_t\sharp\bar\rho_0$.
Recall the definitions of the \EINN\ loss $R(f)$ in equation (\ref{eqn_self_consistency_potential}) and the perturbation $\delta_t$ in equation (\ref{eqn_perturbation}). Use the change of variable formula of the push-forward measure in (a) and Fubini's theorem in (b). We have
\begin{equation} \label{eqn_pathwise_reformulation}
	R(f) = \int_0^T \|\delta_t\|_{\rho_t^f}^2 d t \stackrel{(a)}{=} \int_0^T \|\delta_t\circ X_t\|_{\rho_0}^2 d t \stackrel{(b)}{=} \int \int_0^T \|\delta_t\circ X_t(\vx_0)\|^2 d t d \bar \rho_0(\vx_0).
\end{equation}
Consequently, by defining the trajectory-wise loss (recall $\vx(t) = X_t(\vx_0)$)
\begin{equation} \label{eqn_trajectory_wise_loss}
	R(f; \vx_0) = \int_0^T \|\delta_t\circ X_t(\vx_0)\|^2 d t = \int_0^T \|\delta_t(\vx(t))\|^2 d t,
\end{equation}
we can write the potential function (\ref{eqn_self_consistency_potential}) as an expectation $R(f) = \E_{\vx_0\sim \bar \rho_0}[R(f; \vx_0)]$.
Similarly, when $f$ is parameterized as $f = f_\theta$, we obtain the expectation form $\nabla_\theta R(f_\theta) = \E_{\vx_0\sim\bar \rho_0} [\nabla_\theta R(f_\theta; \vx_0)]$.

We show $\nabla_\theta R(f_\theta; \vx_0)$ can be computed accurately, via the adjoint method (for completeness see the derivation of the adjoint method in appendix \ref{appendix_adjoint_method}). 
As a recap, suppose that we can write $R(f_\theta; \vx_0)$ in a standard ODE-constrained form
$R(f_\theta; \vx_0) = \ell(\theta) = \int_0^T g(t, \vs(t), \theta) d t$,
where $\{\vs(t)\}_{t\in[0, T]}$ is the solution to the ODE $\frac{d }{d t} \vs(t) = \psi(t, \vs(t); \theta)$ with $\vs(0) = \vs_0$, and $\psi$ is a known transition function.
The adjoint method states that the gradient $\frac{d}{d \theta} \ell(\theta)$ can be computed as
% \footnote{This implies that we can obtain an unbiased estimator of $\frac{d \ell}{d \theta}$ by first sample $t\sim \mathrm{Uniform}[0, T]$ and simply compute $T * \left(a(t)^\top\frac{\partial \psi}{\partial \theta}(t, \vs(t); \theta) + \frac{\partial g}{\partial \theta}(t, \vs(t); \theta)\right)$. 
% 	In practice, we sample multiple $t$ uniformly from $[0, T]$ to reduce the variance.
% 	Note that to obtain $\vs(t)$ and $\va(t)$ for all sampled time stamp $t$, we still need to solve the ODEs involved in the definition of $\vx(t)$ and $\va(t)$ once, however their dimensions are now independent of the size of the neural network. }
\begin{equation}
    \text{(Adjoint Method)}\quad 
	\frac{d \ell}{d \theta} = \int_0^T a(t)^\top\frac{\partial \psi}{\partial \theta}(t, \vs(t); \theta) + \frac{\partial g}{\partial \theta}(t, \vs(t); \theta) \ud t.
\end{equation}
where $a(t)$ solves the final value problems $\frac{d}{d t} a(t)^\top + a(t)^\top \frac{\partial  \psi}{\partial  s}(t, \vs(t); \theta) + \frac{\partial g}{\partial s}(t, \vs(t); \theta) = 0, a(T) = 0$.
In the following, we focus on how $R(f_\theta; \vx_0)$ can be written in the above ODE-constrained form.


\paragraph{Write $R(f_\theta; \vx_0)$ in  ODE-constrained form}
Expanding the definition of $\delta_t$ in equation (\ref{eqn_perturbation}) gives
\begin{equation} \label{eqn_delta_x_t}
	\delta_t(\vx(t)) = f_t(\vx(t)) - \left( - \nabla V(\vx(t)) + K\ast \rho_t^f(\vx(t)) - \nu \nabla \log \rho_t^f(\vx(t))\right).
\end{equation}
Note that in the above quantity, $f$ and $V$ are known functions. 
Moreover, it is known that $\nabla \log \rho_t^f(\vx(t))$ admits a closed form dynamics (e.g. see Proposition 2 in \citep{shen22a})
\begin{equation} \label{eqn_dynamics_of_score}
	\frac{d }{d t}\nabla \log \rho_{t}^{f}(\vx(t)) = - \nabla  \left(\udiv f_{t}(\vx(t); \theta)\right) -   \left(\gJ_{f_{t}}(\vx(t); \theta)\right)^\top \nabla \log \rho_{t}^{f}(\vx(t)),
\end{equation}
which allows it to be explicitly computed by starting from $\nabla \log \bar \rho_0(x_0)$ and integrating over time (recall that $\bar \rho_0$ is known).
Here $\gJ_{f_{t}}$ denotes the Jacobian matrix of $f_t$.
Consequently, all we need to handle is the convolution term $K\ast \rho_t^f(\vx(t))$.
% , which in general cannot be exactly computed since it depends on the global configuration of the hypothesis distribution $\rho_t^f$. 

A common choice to approximate the convolution operation is via Monte-Carlo integration:
Let $\vy_{i}(t) \overset{\mathrm{iid}}{\sim} \rho_t^f$ for $i = 1, \ldots, N$ and denote an empirical approximation of $\rho_t^f$ by $\mu_N^{\rho_t^f} = \frac{1}{N}\sum_{i=1}^{N} \delta_{\vy_{i}(t)}$, where $\delta_{\vy_{i}(t)}$ denotes the Dirac measure at $\vy_{i}(t)$. 
We approximate the convolution term in equation (\ref{eqn_delta_x_t}) in different ways for the Coulomb and the  Biot-Savart interactions:
\vspace{-2mm}
\begin{enumerate}[wide, labelwidth=!, labelindent=0pt]
%\begin{itemize}
	\item For the Coulomb type kernel (\ref{eqn_coulomb_interaction}), we first approximate $K\ast \rho_t^f$ with $K_c\ast \rho_t^f$, where 
\begin{equation} \label{eqn_Coulomb_kernel_cutoff}
    K_c(\vx) \defi \begin{cases}
        K(\vx)     \quad &\text{if}\ \|\vx\|> c,\\
        0 \quad &\text{if}\ \|\vx\|\leq c.
    \end{cases}
\end{equation}
If $\rho_t^\vf$ is bounded in $\X$, we have
% Under the mild assumption that $\rho_0 \in \mathcal{L}^\infty$, we can \red{propagate this regularity over time} to obtain $\rho_t^\vf \in \mathcal{L}^\infty(\X)$ and we obtain
\begin{align*}
   \sup_{\vx\in\X} \|(K - K_c)\ast \rho_t^f(\vx)\| = \sup_{\vx\in\X} \|\int_{\|\vx -\vy\|\leq c} \frac{\vx - \vy}{\|\vx - \vy\|^{d}} \rho_t^f(\vy)\ud \vy \|
   \leq \|\rho_t^f\|_{\gL^\infty(\X)} \int_{\|\vy\|\leq c}\frac{1}{\|\vy\|^{d-1}}\ud \vy.
\end{align*}
To compute the integral on the right-hand side, we will switch to polar coordinates $(r, \psi)$:
\begin{equation} \label{eqn_polar_coordinate}
\int_{|\vy|\leq c}\frac{1}{|\vy|^{d-1}}\ud \vy = \int_0^c \ud r \frac{1}{r^{d-1}} \int_\Psi \ud\psi\ J_{(r, \psi)} \leq \int_0^c \ud r = c.
\end{equation}
Here, $J_{(r, \psi)}$ denotes the determinant of the Jacobian matrix resulting from the transformation from the Cartesian system to the polar coordinate system.
In inequality (\ref{eqn_polar_coordinate}), we utilize the fact that $J_{(r, \psi)} \leq r^{d-1}$, which allows us to cancel out the factor ${1}/{r^{d-1}}$.
Now that $K_c$ is bounded by $c^{-d+1}$, we can further approximate $K_c\ast \rho_t^\vf$ using $K_c\ast \mu_N^{\rho_t^\vf}$ with error of the order $O(c^{-d+1}/\sqrt{N})$. Altogether, we have $\sup_{\vx\in\X} \|K\ast \rho_t^f(\vx) - K_c\ast \mu_N^{\rho_t^\vf}(\vx)\| = O(c + c^{-d+1}/\sqrt{N})$ which can be made arbitrarily small for a sufficiently small $c$ and a sufficiently large $N$.
 % For the Coulomb interaction, we directly approximate the convolution term in \eqref{eqn_delta_x_t} by $K \ast \mu_N^{\rho_t^f}(\vx(t)) = \frac{1}{N} \sum_{i=1}^N K(\vx(t) -  \vy_i(t))$. 
 %    In practice, we choose $N$ sufficiently large so that the above empirical approximation is accurate. 
	% Indeed, at least for the whole space case, i.e. the underlying space  $\mathcal{X}$ is $\mathbb{R}^d$, one has that 
	% \vspace{-2mm}
	% \[
	% \int_{\mathbb{R}^d} |K * \mu_N^{\rho^{f}} (x)- K * \rho^f (x)|^2 \ud x = \int_{x \ne y } g(x - y ) \ud ( \mu_N^{\rho^f} -  \rho^f  )^{\otimes 2}(x, y) = F(\mu_N^{\rho^f}, \rho^f), 
	% \]
	% where $F(\mu_N^{\rho^f}, \rho^f)$ is the modulated (interaction) energy defined as in \cite{serfaty2020mean}. 
	% We expect $F(\mu_N^{\rho^f}, \rho^f)\rightarrow 0$ almost surely as $N\rightarrow\infty$.
	\item For Biot-Savart interaction (2D Navier-Stokes equation), there are more structures to exploit and we can completely avoid the singularity: As noted by \cite{jabin2018quantitative}, the convolution kernel $K$ can be written in a divergence form:
	\begin{equation} \label{eqn_K_as_divergence}
		K = \nabla \cdot U, \text{ with } U(\vx) = \frac{1}{2\pi}\begin{bmatrix}
			-\arctan(\frac{\vx_1}{\vx_2}),& 0\\
			0,& \arctan(\frac{\vx_2}{\vx_1})
		\end{bmatrix},
	\end{equation}
	where the divergence of a matrix function is applied row-wise, i.e. $[K(\vx)]_i = \udiv\ U_i(\vx)$.
	Using integration by parts and noticing that the boundary integration vanishes on the torus, one has
	\begin{align*}
		K\ast \rho_t^f (\vx) =&\ \int K(\vy) \rho_t^f(\vx - \vy) d \vy = \int \nabla\cdot U(\vy) \rho_t^f(\vx - \vy) d \vy = \int U(\vy) \nabla \rho_t^f(\vx - \vy) d \vy \\
		=&\ \int U(\vx - \vy) \rho_t^f(\vy) \nabla \log \rho_t^f(\vy) d \vy = \E_{\vy\sim \rho_t^f(\vy)}[U(\vx - \vy) \nabla \log \rho_t^f(\vy)].
	\end{align*}
	If the score function $\nabla \log \rho_t^f$ is bounded, then the integrand in the expectation is also bounded. Therefore, we can avoid integrating singular functions and the Monte Carlo-type estimation $\frac{1}{N} \sum_{i=1}^N U(\vx - \vy_i(t)) \nabla \log \rho_t^f(\vy_i(t))$ is accurate for a sufficiently large value of N.
%\end{itemize}
\end{enumerate}
With the above discussion, we can write $R(f_\theta; \vx_0)$ in an ODE-constrained form in a standard way, which due to space limitation is deferred to Appendix \ref{appendix_ODE_constrained_form}.
\begin{remark}
    Let $\ell_N(\theta)$ be the function we obtained using the above approximation of the convolution, where $N$ is the number of Monte-Carlo samples.
    The above discussion shows that $\ell_N(\theta)$ and $R(f_\theta; \vx_0)$ are close in the $\gL^\infty$ sense, which is hence sufficient when the \EINN\ loss is used as error quantification since only function value matters.
    When both $\ell_N(\theta)$ and $R(f_\theta; \vx_0)$ are in $C^2$, one can translate the closeness in function value to the closeness of their gradients.
    In our experiments, using $\nabla \ell_N(\theta)$ as an approximation of $\nabla_\theta R(f_\theta; \vx_0)$ gives very good empirical performance already. 
\end{remark}
%\paragraph{Removing the Integration over the Parameter Space} If we directly evaluate the integration in \eqref{eqn_adjoint_method_integration}, we need to solve an ODE numerically in stats space with its dimension at least as large as the number of parameters in the neural network. Consequently, the computational complexity will drastically increase if a complicated neural network structure is used. To alleviate this burden, observe that the gradient admits the equivalent expectation form
%\begin{equation}
%	\frac{d \ell}{d \theta} = \E_{t\sim \Uniform[0, T]}\left[a(t)^\top\frac{\partial \psi}{\partial \theta}(t, \vs(t); \theta) + \frac{\partial g}{\partial \theta}(t, \vs(t); \theta)\right].
%\end{equation}