\section{Classical Methods for Solving MVEs} \label{appendix_related_work}
Solving partial differential equations (PDEs) is a key aspect of scientific research, with a wealth of literature in the field {\citep{evans2022partial}}. 
For the interest of this paper, we will only consider the methods that can be used to solve the MVE under consideration.

\paragraph{Categorize PDE solvers via solution representation.} To better understand the benefits of neural network (NN) based PDE solvers and to compare our approach with others, we categorize the literature based on the representation of the solution to the PDE. These representations can be roughly grouped into four categories:
\begin{itemize}[leftmargin=*]
    \item \textbf{1. Discretization-based representation:} The solution to the PDE is represented as discrete function values at grid points, finite-size cells, or finite-element meshes.
    \item \textbf{2. Representation as a combination of basis functions:} The solution to the PDE is approximated as a sum of basis functions, e.g. Fourier series, Legendre polynomials, or Chebyshev polynomials.
    \item \textbf{3. Representation using a collection of particles:} The solution to the PDE is represented as a collection of particles, each described by its weight, position, and other relevant information. 
    \item \textbf{4. NN-based representation:} NNs offer many strategies for representing the solution to the PDE, such as using the NN directly to represent the solution, using normalizing flow or GAN-based parameterization to ensure the non-negativity and conservation of mass of the solution or using the NN to parameterize the underlying dynamics of the PDE, such as the time-varying velocity field that drives the evolution of the system.
\end{itemize}


The drawback of the first three strategies is that a sparse representation\footnote{For example, sparser grid, cell or mesh with less granularity, fewer basis functions, fewer particles.} leads to reduced solution accuracy, while a dense representation results in increased computational and memory cost. 
NNs, as powerful function approximation tools, are expected to surpass these strategies by being able to handle higher-dimensional, less regular, and more complicated systems \citep{weinan2021algorithms}.

Given a representation strategy of the solution, an effective solver must exploit the underlying properties of the system to find the best candidate solution. Three-= notable properties that are utilized to design solvers are 
\begin{enumerate}[leftmargin=*,label=(\Alph*)]
    \item the PDE definition or weak formulation of the system,
    \item the SDE interpretation of the system,
    \item the variational interpretation, particularly the Wasserstein gradient flow interpretation.
\end{enumerate}

These properties are combined with the solution representations mentioned earlier to form different methods.
For example, the Finite Difference method \citep{smith1985numerical}, Finite Volume method \citep{moukalled2016finite}, and Finite Element method \citep{johnson2012numerical} represent the solution of partial differential equations (PDEs) by discretizing the solution and utilize the property (A), at least in their original form. On the other hand, recent work by \citet{carrillo2022primal} solves PDEs admitting a Wasserstein gradient flow structure using the classic JKO scheme \citep{jordan1998variational}, which is based on the property (C), and the solution is also represented via discretization. The Spectral method \citep{shen2011spectral} is a class of methods that exploits property (A) by representing the solution as a combination of basis functions.
The Random Vortex Method \citep{long1988convergence} is a highly successful method for solving the vorticity formulation of the 2D Navier-Stokes equation by exploiting property (C) and representing the solution with particles. The Blob method from \citet{carrillo2019blob} is another particle-based method for solving PDEs that describe diffusion processes, which also exploits property (C).
% In the following, we will focus on the methods that use NN for solution representation.


% Another line of research is to solve PDEs that admit the Wasserstein gradient flow interpretation using the minimizing movement scheme, also referred to as the JKO scheme \citep{jordan1998variational}. \citet{mokrov2021large} and \citet{fan2022variational} focus on the Fokker-Planck equation (FPE), a specific instance of MVE without the interaction term $K\equiv 0$, and learn a sequence of transport maps parameterized by NN. Each map is trained to solve an iteration of the JKO scheme. These learned maps can be used to reconstruct the solution to the Fokker-Planck equation. 
% While the MVE with Coulomb interaction also admits a Wasserstein gradient flow interpretation, it is unclear how methods along this line can be generalized to MVE. We plan to investigate this in the future.
\section{Comparison with Neural Operator}
We thank the anonymous reviewers for pointing out the interesting research direction of neural operators \citep{xiao2023coupled,gupta2021multiwavelet,li2020neural,kovachki2021neural,li2020fourier}.
However, to highlight the major difference between EINN and the approach of the Neural Operator, it's worth noting that they consider completely different problem settings: EINN requires \emph{no pre-existing data} and the goal is to obtain the solution to a PDE by solely exploiting the structure of the equation itself. In contrast, the neural operator approach is \emph{data-driven}, i.e. it relies on the existence of configuration-solution pairs. Here, by configuration-solution pairs, we mean the correspondence between some configurations that determine the PDE, e.g. the initial condition or the viscosity parameter in the fluid dynamics problems, and the pre-existing solution to the PDE given the aforementioned configurations. Consequently, the neural operator approach is more like a regression problem where a neural network is trained to learn the abstract map between the configuration and the solution. In contrast, EINN is more like a numerical PDE solver.

Consequently, EINN and the approach of neural operator are two related but quite distinct research directions. They are related in the sense that EINN can provide the data (configuration-solution pairs) required by the neural operator approach. They are distinct since EINN requires no data a priori, while the neural operator approach is built on the supervised learning paradigm.