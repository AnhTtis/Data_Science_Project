%\documentclass[anon,12pt]{colt2023} % Anonymized submission
%\documentclass[final,12pt]{colt2023} % Include author names

%\usepackage{showkeys}

% The following packages will be automatically loaded:
% amsmath, amssymb, natbib, graphicx, url, algorithm2e
\documentclass{article}
\usepackage[final]{neurips_2023}


\title{Entropy-dissipation Informed Neural Network\\ for McKean-Vlasov Type PDEs}
\usepackage{times}
% Use \Name{Author Name} to specify the name.
% If the surname contains spaces, enclose the surname
% in braces, e.g. \Name{John {Smith Jones}} similarly
% if the name has a "von" part, e.g \Name{Jane {de Winter}}.
% If the first letter in the forenames is a diacritic
% enclose the diacritic in braces, e.g. \Name{{\'E}louise Smith}

% Two authors with the same address
% \coltauthor{\Name{Author Name1} \Email{abc@sample.com}\and
%  \Name{Author Name2} \Email{xyz@sample.com}\\
%  \addr Address}

% Three or more authors with the same address:
% \coltauthor{\Name{Author Name1} \Email{an1@sample.com}\\
%  \Name{Author Name2} \Email{an2@sample.com}\\
%  \Name{Author Name3} \Email{an3@sample.com}\\
%  \addr Address}

% Authors with different addresses:
\author{
	Zebang Shen\thanks{Authors are listed in alphabetic order.}\\
	ETH Z\"urich \\
	\texttt{zebang.shen@inf.ethz.ch} \\
	%% examples of more authors
	\And
	Zhenfu Wang$^*$ \\
	Peking University \\
	\texttt{zwang@bicmr.pku.edu.cn}
	%% \AND
	%% Coauthor \\
	%% Affiliation \\
	%% Address \\
	%% \texttt{email} \\
	%% \And
	%% Coauthor \\
	%% Affiliation \\
	%% Address \\
	%% \texttt{email} \\
	%% \And
	%% Coauthor \\
	%% Affiliation \\
	%% Address \\
	%% \texttt{email} \\
}
\usepackage{amssymb,amsmath,amsthm}
\usepackage{color}
\usepackage{makecell}
\usepackage{enumitem}
\usepackage{natbib}
\usepackage{graphicx}
\usepackage{hyperref}       % hyperlinks
\usepackage{MnSymbol}

\input{math_commands.tex}

\newtheorem{theorem}{Theorem}
\newtheorem{proposition}{Proposition}
\newtheorem{assumption}{Assumption}
\newtheorem{remark}{Remark}
\newtheorem{lemma}{Lemma}
\newcommand{\Law}{\mathrm{Law}}
\newcommand{\red}[1]{\textcolor{red}{#1}}
\newcommand{\ud}{\mathrm{d}}
\newcommand{\X}{\mathcal{X}}
%\newcommand{\ud}{\,\mathrm{d}}
\newcommand{\udiv}{\, \mathrm{div}}
\newcommand{\Uniform}{\mathrm{Uniform}}
\let\KL\relax
\newcommand{\KL}{\mathbf{KL}}
\newcommand{\EINN}{\texttt{EINN}}
\newcommand{\defi}{\overset{\operatorname{def}}{=}}
\newcommand{\Lip}{\mathrm{Lip}}

\begin{document}

\maketitle

\begin{abstract}%
    The McKean-Vlasov equation (MVE) describes the collective behavior of particles subject to drift, diffusion, and mean-field interaction. In physical systems, the interaction term can be singular, i.e. it diverges when two particles collide. Notable examples of such interactions include the Coulomb interaction, fundamental in plasma physics, and the Biot-Savart interaction, present in the vorticity formulation of the 2D Navier-Stokes equation (NSE) in fluid dynamics.
    Solving MVEs that involve singular interaction kernels presents a significant challenge, especially when aiming to provide rigorous theoretical guarantees. In this work, we propose a novel approach based on the concept of entropy dissipation in the underlying system. We derive a potential function that effectively controls the KL divergence between a hypothesis solution and the ground truth.
    Building upon this theoretical foundation, we introduce the Entropy-dissipation Informed Neural Network (\EINN) framework for solving MVEs. In \EINN, we utilize neural networks (NN) to approximate the underlying velocity field and minimize the proposed potential function. By leveraging the expressive power of NNs, our approach offers a promising avenue for tackling the complexities associated with singular interactions.
    To assess the empirical performance of our method, we compare \EINN\ with SOTA NN-based MVE solvers. The results demonstrate the effectiveness of our approach in solving MVEs across various example problems.
\end{abstract}

%\begin{keywords}%
%  Entropy dissipation, McKean-Vlasov equation, 2D Navier-Stokes equation%
%\end{keywords}
\input{Introduction.tex}


%\section{Preliminaries}
%\subsection{Periodic Boundary Condition}
%
%\subsection{Neural Ordinary Differential Equation}

\input{methodology.tex}
\input{Analysis.tex}

\input{related_works.tex}

\input{experiment.tex}

\input{Conclusion.tex}
% \acks{Zebang Shen's work is supported by ETH research grant and Swiss National Science Foundation (SNSF) Project Funding No. 200021-207343.}

% Acknowledgments---Will not appear in anonymized version
% \acks{We thank a bunch of people and funding agency.}
\begin{ack}
Zhenfu Wang is supported by the National Key R\&D Program of China, Project Number 2021YFA1002800, NSFC grant No.12171009, Young Elite Scientist Sponsorship Program by China Association for Science and Technology (CAST) No. YESS20200028 and the Fundamental Research Funds for the Central Universities (the start-up fund), Peking University.
Zebang Shen's work is supported by ETH research grant and Swiss National Science Foundation (SNSF) Project Funding No. 200021-207343.
\end{ack}
\bibliographystyle{abbrvnat}  
\bibliography{MVE}

\clearpage
\appendix
\input{appendix/appendix_related_work.tex}
\input{appendix/appendix_experiments.tex}
\clearpage
\input{appendix/appendix_adjoint.tex}
\clearpage
\input{appendix/detailed_proofs.tex} 
\clearpage
\input{appendix/approximation_error_of_NN.tex}
\clearpage
\input{appendix/appendix_unbounded.tex}

%\clearpage
%\input{math_command_information.tex}
\end{document}
