\vspace{-.4cm}
\section{Introduction}
% Researchers are utilizing AI to push the boundaries of science research, following their success in the fields of computer vision and natural language processing. 
\vspace{-.2cm}
Scientists use Partial Differential Equations (PDEs) to describe natural laws and predict the dynamics of real-world systems. As PDEs are of fundamental importance, a growing area in machine learning is the use of neural networks (NN) to solve these equations {\citep{han2018solving,zhang2018deep,raissi2020hidden,cai2021physics,karniadakis2021physics,cuomo2022scientific}}.
An important category of PDEs is the McKean-Vlasov equation (MVE), which models the dynamics of a stochastic particle system with mean-field interactions
\begin{equation} \label{eqn_MVE_particle}
		\ud \rmX_t = - \nabla V(\rmX_t) \ud t  + K \ast \bar \rho_t(\rmX_t) \ud t  + \sqrt{2\nu}  \ud \rmB_t,  \quad
		\bar \rho_t = \Law(\rmX_t).  
\end{equation}
Here $\rmX_t \in \X$ denotes a random particle' position, $\X$ is either $\mathbb{R}^d$ or the torus $\Pi^d$ (a cube $[-L, L]^d$ with periodic boundary condition), $V: \sR^d\rightarrow\sR$ denotes a {\em known} potential, $K: \sR^d \rightarrow \sR^d$ denotes some interaction kernel and the convolution operation is defined as $h \ast \phi = \int_{\X} h(\vx-\vy) \phi(\vy) d \vy$, $\{\rmB_t\}_{t\geq 0}$ is the standard  $d$-dimensional Wiener process with $\nu\geq 0$ being the diffusion coefficient, and $\bar \rho_t:\X \rightarrow \sR$ is the law or the probability density function of the random variable $\rmX_t$ and the initial data $\bar \rho_0$ is given.
Under mild regularity conditions, the density function $\bar \rho_t$ satisfies the MVE
\begin{equation} \label{eqn_MVE}
    \text{(MVE)}\quad 
	\partial_t \bar \rho_t(\vx) + \udiv \left( \bar \rho_t (- \nabla V(\vx) + K \ast \bar \rho_t(\vx))\right) = \nu \Delta  \bar \rho_t(\vx),
\end{equation}
where $\udiv$ denotes the divergence operator, $\udiv\ h(\vx) = \sum_{i=1}^d {\partial h_i}/{\partial x_i}$ for a velocity field $h:\sR^d\rightarrow\sR^d$, $\Delta$ denotes the Laplacian operator defined as $\Delta \phi = \udiv( \nabla \phi)$,  where $\nabla \phi$ denotes the gradient of a scalar function $\phi:\sR^d\rightarrow\sR$. Note that all these operators are applied only on the spatial variable $\vx$.

In order to describe dynamics in real-world phenomena such as electromagnetism \citep{golse2016dynamics} and fluid mechanics {\citep{majda2002vorticity}}, the interaction kernels $K$ in the MVE can be highly \emph{singular}, i.e. $\|K(\vx)\| \rightarrow \infty$ when  $\|\vx\|\rightarrow 0$.
Two of the most notable examples are the Coulomb interactions 
\begin{equation} \label{eqn_coulomb_interaction}
    \text{(Coulomb Kernel)}\quad 
	K(\vx) = -\nabla g(\vx),\ \mathrm{with}\ g(\vx) = \begin{cases}
		\left((d-2)S_{d-1}(1)\right)^{-1}\|\vx\|^{-(d-2)}, & d\geq 3,\\
		-(2\pi)^{-1}\log \|\vx\|,  & d = 2,
	\end{cases}
\end{equation}
with $S_{d-1}(1)$ denoting the surface area of the unit sphere in $\sR^d$,
and the vorticity formulation of the 2D Navier-Stokes equation (NSE) where the interaction kernel $K$ is given by the Biot-Savart law
\begin{equation} \label{eqn_nse}
    \text{(Biot-Savart Kernel)}\quad 
	K(\vx) = \frac{1}{2\pi} \frac{\vx^\perp}{\|\vx\|^2} = \frac{1}{2\pi} \left(-\frac{x_2}{\|\vx\|^2}, \frac{x_1}{\|\vx\|^2}\right), 
\end{equation}
where $\vx= (x_1, x_2)$ and  $\|\vx\|$ denotes the Euclidean norm of a vector.\vspace{1mm}\\
Classical methods for solving MVEs, including finite difference, finite volume, finite element, spectral methods, and particle methods, have been developed over time. A common drawback of these methods lies in the constraints of their solution representation: Sparse representations, such as less granular grids, cells, meshes, fewer basis functions, or particles, may lead to an inferior solution accuracy; On the other hand, dense representations incur higher computational and memory costs. \vspace{1mm}\\
As a potent tool for function approximation, NNs are anticipated to overcome these hurdles and handle higher-dimensional, less regular, and more complex systems efficiently \citep{weinan2021algorithms}.
The most renowned NN-based algorithm is the Physics Informed Neural Network (PINN) \citep{raissi2019physics}.
The philosophy behind the PINN method is that solving a PDE system is equivalent to finding the root of the corresponding differential operators. PINN tackles the latter problem by directly parameterizing the hypothesis solution with an NN and training it to minimize the $\gL^2$ functional residual of the operators. 
As a versatile PDE solver, PINN may fail to exploit the underlying dynamics of the PDE, which possibly leads to inferior performance on task-specific solvers. For example, on the 2D NSE problem, a recent NN-based development \citet{zhang2022drvn} surpasses PINN and sets a new SOTA empirical performance, which however lacks rigorous theoretical substantiation.
Despite the widespread applications of PINN, rigorous error estimation guarantees are scarce in the literature. While we could not find results on the MVE with the Coulomb interaction, only in a very recent paper \citep{deerror}, the authors establish for NSE that the PINN loss controls the discrepancy between a candidate solution and the ground truth. 
We highlight that their result holds \emph{average-in-time}, meaning that at a particular timestamp $t \in [0, T]$, a candidate solution with small PINN loss may still significantly differ from the true solution. In contrast, all guarantees in this paper are \emph{uniform-in-time}. Moreover, there is a factor in the aforementioned guarantee that \emph{exponentially depends} on the total evolving time $T$, while the factor in our guarantee for the NSE is \emph{independent} of $T$.
We highlight that these novel improvements are achieved for the proposed \EINN\ framework since we take a completely different route from PINN: Our approach is explicitly designed to exploit the underlying dynamics of the system, as elaborated below.


% are possible since the proposed method exploits the underlying dynamics of the system and takes a fundamentally different principle.





% The residual minimization scheme of PINN fundamentally limits 






% Through an alternative formulation detailed in Appendix \ref{appendix_implementation_of_baselines}, PINN transforms NSE into a system of PDEs. It then solves these PDEs by minimizing a weighted sum of the $\gL^2$ residuals from a candidate solution over the entire space-time domain (elaborated in Section \ref{section_related_work}). 


% ===============
% For the best performance, PINN must carefully balance multiple error terms in its objective, a process which necessitates meticulous hyperparameter tuning of the respective weights. 
% ===============
% In terms of theoretical understanding, for the NSE a recent study \citep{deerror} demonstrates that a candidate solution with a diminishing PINN loss can retrieve the true solution, given that various regularity assumptions are met. However, this result applies only in \emph{a time-average sense}, meaning that at a particular timestamp $t \in [0, T]$, the candidate solution may significantly differ from the true solution. 
% In contrast, in our paper, we aim to provide \emph{uniform-in-time} type results (see Theorem \ref{thm_informal}).
% Furthermore, this result is specific to a certain function class of NN, thus preventing direct use of PINN loss as an error quantification for generic candidate solutions. In a recent development, \citet{zhang2022drvn} introduce the Random Deep Vortex Network (RDVN) method for solving 2D NSE, which surpasses PINN and sets a new SOTA empirical performance for this task. Nonetheless, the lack of rigorous theoretical substantiation for its empirical success remains a concern.

% \vspace{-.2cm}
% It is crucial to underline that, until our research, no \emph{provable} method existed for assessing the quality of a \emph{generic} candidate solution to the MVE, especially in the presence of singular interaction kernels. Our work addresses this lacuna by proposing an easily computable metric that facilitates the training of NN-based candidate solutions and aids in determining solution quality in such scenarios.
\vspace{-.3cm}
\paragraph{Our approach} Define the operator
\vspace{-.1cm}
\begin{equation} \label{eqn_operator_A}
    \mathcal{A}[\rho] \defi - \nabla V + K \ast \rho -  \nu \nabla \log \rho.
\end{equation}
By noting $\Delta  \bar \rho_t = \udiv (\bar \rho_t \nabla \log \bar  \rho_t)$, we can rewrite the MVE in the form of a continuity equation
\begin{equation} \label{eqn_MVE_CE}
	\partial_t \bar  \rho_t(\vx)  + \udiv \Big( \bar \rho_t(\vx) \gA[\bar \rho_t](\vx)\Big) = 0.
\end{equation}
For simplicity, we will refer to $\mathcal{A}[\bar \rho_t]$ as the \emph{underlying velocity}.
Consider another time-varying \emph{hypothesis velocity} field $f:\sR\times \sR^d\rightarrow\sR$ and let $\rho^f_t$ be the solution to the continuity equation
\vspace{-1mm}
\begin{equation} \label{eqn_CE}
    \text{(hypothesis solution)}\quad 
	\partial_t \rho^f_t(\vx)  + \udiv (\rho_t^f(\vx) f(t, \vx) ) = 0,\ \rho^f_0 = \bar \rho_0
\end{equation}
for $t \in [0, T]$, where we recall that the initial law  $\bar \rho_0$ is known.
We will refer to $\rho_t^f$ as the \emph{hypothesis solution} and use the superscript to emphasize its dependence on the hypothesis velocity field $f$.
We propose an Entropy-dissipation Informed Neural Network framework (\EINN), which trains an NN parameterized hypothesis velocity field $f_\theta$ by minimizing the following \EINN\ loss
\begin{equation} \label{eqn_self_consistency_potential}
\text{(\EINN\ loss)}\quad 
        R(f_\theta) \defi \int_0^T \int_\X \|f_\theta(t,\vx) - \mathcal{A}[\rho_t^{f_\theta}](\vx) \|^2 {\rho_t^{f_\theta}}(\vx)\ud \vx \ud t.    
\end{equation}
% Here $\|h\|_\phi$ denotes the weighted $\gL^2$ norm for the function $h$, $\|h\|_\phi^2 = \int_\X \|h(\vx)\|^2 \phi(\vx) d \vx$.
The objective (\ref{eqn_self_consistency_potential}) is obtained by studying the stability of carefully constructed Lyapunov functions. These Lyapunov functions draw inspiration from the concept of entropy dissipation in the system, leading to the name of our framework.
We highlight that we provide a rigorous error estimation guarantee for our framework for MVEs with singular kernels (\ref{eqn_coulomb_interaction}) and (\ref{eqn_nse}), showing that when $R(f_\theta)$ is sufficiently small, $\rho_t^{f_\theta}$ recovers the ground truth $\bar \rho_t$ in the KL sense, uniform in time.

% The goal of this paper is to show that, given a \emph{fixed} time interval $[0, T]$, when $f(t, \cdot)$ is sufficiently close to $\mathcal{A}[\bar \rho_t]$ in an appropriate sense, one can recover $\{ \bar \rho_t\}_{t\in[0, T]}$, i.e. the solution to the MVE in the time interval $[0, T]$, by solving the following 
% We will refer to $\rho_t^f$ as the \emph{hypothesis solution} and use the superscript to emphasize its dependence on the hypothesis velocity field $f$.
% Our results can be informally summarized as follows.
\begin{theorem}[Informal] \label{thm_informal}
	Suppose that the initial density function $\bar \rho_0$ is sufficiently regular and the hypothesis velocity field  $f_t(\cdot) = f(t, \cdot)$ is at least three times continuously differentiable both in $t$ and $x$. 
	We have for the MVE with a bounded interaction kernel $K$ or with the singular Coulomb (\ref{eqn_coulomb_interaction}) or Biot-Savart (\ref{eqn_nse}) interaction, the KL divergence between the hypothesis solution $\rho_t^f$ and the ground truth $\bar \rho_t$ is controlled by the \EINN\ loss for any $t\in[0, T]$, i.e. there exists some constant $C>0$, 
	\begin{equation}
		\sup_{t \in [0, T]}	\KL(\rho_t^f, \bar \rho_t) \leq C R(f).
	\end{equation} 
\end{theorem}
\vspace{-3mm}
Having stated our main result, we elaborate on the difference between \EINN\ and PINN in terms of information flow over time, which explains why \EINN\ achieves better theoretical guarantees:
In PINN, the residuals at different time stamps are independent of each other and hence there is no information flow from the residual at time $t_1$ to the one at time $t_2 (> t_1)$. In contrast, in the \EINN\ loss (\ref{eqn_self_consistency_potential}), incorrect estimation made in $t_1$ will also affect the error at $t_2$ through the hypothesis solution $\rho_t^f$. Such an information flow gives a stronger gradient signal when we are trying to minimize the \EINN\ loss, compared to the PINN loss. It partially explains why we can obtain the novel uniform-in-time estimation as opposed to the average-in-time estimation for PINN and why the constant $C$ in the NSE case is independent of $T$ for \EINN\ (Theorem \ref{NSMainEstimate}), but exponential in $T$ for PINN. 

\vspace{-1mm}

% \citet{shen22a} further propose that the self-consistency of a PDE can be used to test the quality of an existing solution and to guide the design of some objective functions for a neural network parameterization of the PDE solution.
% Inspired by their work, we generalize their framework to the more complicated MVE and provide algorithms that can solve the MVE with singular Coulomb interaction and the 2D Navier-Stokes equation. \\
%The computational limitations of \citep{shen22a} include the requirement of solving an ODE over a state space with dimensions that are at least as large as the size of the neural network when computing the gradient with respect to the neural network's parameters. By closely examining the adjoint method used for gradient computation, we demonstrate that this computational bottleneck can be eliminated.
\textbf{Contributions.} In summary, we present a novel NN-based framework for solving the MVEs. Our method capitalizes on the entropy dissipation property of the underlying system, ensuring robust theoretical guarantees even when dealing with singular interaction kernels. We elaborate on the contributions of our work from theory, algorithm, and empirical perspectives as follows.
\vspace{-2mm}
\begin{enumerate}[wide, labelwidth=!, labelindent=0pt]
%\begin{itemize}
\vspace{-1mm}
	\item (Theory-wise) By studying the stability of the MVEs with bounded interaction kernels or with singular interaction kernels in the Coulomb (\ref{eqn_coulomb_interaction}) and the Biot-Savart case (\ref{eqn_nse}) (the 2D NSE) via entropy dissipation, we establish the error estimation guarantee for the \EINN\ loss on these equations. Specifically, we design a potential function $R(f)$ of a hypothesis velocity $f$ such that $R(f)$ controls the KL divergence between the hypothesis solution $\rho_t^f$ (defined in equation (\ref{eqn_CE})) and the ground truth solution $\bar \rho_t$ for any time stamp within a given time interval $[0, T]$. 
    A direct consequence of this result is that $R(f)$ can be used to assess the quality of a generic hypothesis solution to the above MVEs and $\rho_t^f$ exactly recovers $\bar \rho_t$ in the KL sense given that $R(f) = 0$.
    \vspace{-1mm}
    % \red{when K is bound? large time estimation of NSE.}
	\item (Algorithm-wise) When the hypothesis velocity field is parameterized by an NN, i.e. $f = f_\theta$ with $\theta$ being some finite-dimensional parameters, the \EINN\ loss $R(f_\theta)$ can be used as the loss function of the NN parameters $\theta$. We discuss in detail how an estimator of the gradient $\nabla_\theta R(f_\theta)$ can be computed so that stochastic gradient-based optimizers can be utilized to train the NN. In particular, for the 2D NSE (the Biot-Savart case (\ref{eqn_nse})), we show that the singularity in the gradient computation can be removed   
	by exploiting the anti-derivative of the Biot-Savart kernel.
    \vspace{-1mm}
    % [Always remain a valid probability distribution.]
	\item (Empirical-wise) We compare the proposed approach, derived from our novel theoretical guarantees, with SOTA NN-based algorithms for solving the MVE with the Coulomb interaction and the 2D NSE (the Biot-Savart interaction). We pick specific instances of the initial density $\bar \rho_0$, under which explicit solutions are known and can be used as the ground truth to test the quality of the hypothesis ones.
	Using NNs with the same complexity (depth, width, and structure), we observe that the proposed method significantly outperforms the included baselines.
%\end{itemize}
\end{enumerate}