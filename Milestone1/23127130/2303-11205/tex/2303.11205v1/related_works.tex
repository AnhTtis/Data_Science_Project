\section{Related Works}
Solving partial differential equations (PDEs) is a key aspect of scientific research, with a wealth of literature in the field {\citep{evans2022partial}}. There are general purpose PDE solvers as well as algorithms specifically designed for MVEs. 
For the interest of this paper, for general purpose PDE solver, we will only consider the instance that can be used to solve the MVE under consideration.

\paragraph{Categorize PDE solvers via solution representation.} To better understand the benefits of neural network (NN) based PDE solvers and to compare our approach with others, we categorize the literature based on the representation of the solution to the PDE. These representations can be roughly grouped into four categories:\\
\noindent\textbf{1. Discretization-based representation:} The solution to the PDE is represented as discrete function values at grid points, finite-size cells, or finite-element meshes.\\
\noindent\textbf{2. Representation as a combination of basis functions:} The solution to the PDE is approximated as a sum of basis functions, e.g. Fourier series, Legendre polynomials, or Chebyshev polynomials.\\
\noindent\textbf{3. Representation using a collection of particles:} The solution to the PDE is represented as a collection of particles, each described by its weight, position, and other relevant information.\\
\noindent\textbf{4. NN-based representation:} NNs offer many strategies for representing the solution to the PDE, such as using the NN directly to represent the solution, using normalizing flow or GAN-based parameterization to ensure the non-negativity and conservation of mass of the solution, or using the NN to parameterize the underlying dynamics of the PDE, such as the time-varying velocity field that drives the evolution of the system.


The drawback of the first three strategies is that a sparse representation\footnote{For example, sparser grid, cell or mesh with less granularity, less basis functions, less particles.} leads to reduced solution accuracy, while a dense representation results in increased computational and memory cost. NNs, as powerful function approximation tools, are expected to surpass these strategies by being able to handle higher-dimensional, less regular, and more complicated systems \citep{weinan2021algorithms}.

Given a representation strategy of the solution, an effective solver must exploit the underlying properties of the system to find the best candidate solution. Four notable properties that are utilized to design solvers are: (A) the PDE definition or weak formulation of the system, (B) the SDE interpretation of the system, (C) the variational interpretation, particularly the Wasserstein gradient flow interpretation, and (D) the {stability} property of the system. These properties are combined with the solution representations mentioned earlier to form different methods.
For example, the Finite Difference method \citep{smith1985numerical}, Finite Volume method \citep{moukalled2016finite}, and Finite Element method \citep{johnson2012numerical} represent the solution of partial differential equations (PDEs) by discretizing the solution and utilize property (A), at least in their original form. On the other hand, a recent work by \citet{carrillo2022primal} solves PDEs admitting a Wasserstein gradient flow structure using the classic JKO scheme \citep{jordan1998variational}, which is based on property (C), and the solution is also represented via discretization. The Spectral method \citep{shen2011spectral} is a class of methods that exploits property (A) by representing the solution as a combination of basis functions.
The Random Vortex Method \citep{long1988convergence} is a highly successful method for solving the vorticity formulation of the 2D Navier-Stokes equation by exploiting property (C) and representing the solution with particles. The Blob method from \citet{carrillo2019blob} is another particle-based method for solving PDEs that describe diffusion processes, which also exploits property (C).
In the following, we will focus on the methods that uses NN for solution representation.
%\begin{table}	
%	\caption{A the PDE definition of the system (or weak formulation), B the SDE interpretation of the system, C the variational interpretation, D stability property of the system; 1 Discretization based representation, 2 Representation the solution a combination of basis functions, 3 Representing solutions as a collection of particles, 4 Neural network based representation}
%	\begin{tabular}{c | c | c | c | c}
%		  & A & B & C & D \\ \hline 
%		1 & \makecell{Finite Difference \\Finite Volume\\Finite Element}  & -  & \citep{carrillo2022primal}  & -  \\ \hline 
%		2 & \makecell{Spectral Method \\ Galerkin Method}  & -  & -  & -  \\ \hline 
%		3 & -  & \makecell{Random Vortex Method \\ Blob Method}  & -   & -  \\ \hline 
%		4 & PINN \citep{raissi2019physics}  & DRVN \citep{zhang2022drvn} & \citep{fan2022variational}  & \citep{shen22a}
%	\end{tabular}
%	\label{table_review}
%\end{table}
\paragraph{Comparison with NN-based solvers}
The Physics-Informed Neural Network (PINN) is a widely used neural network-based solver that leverages property (A) \citep{raissi2019physics,yang2019adversarial}. By expressing a PDE as $\gA(\vg) = 0$ and its time and space boundary conditions as $\gB(\vg) = 0$, where $\vg$ is a candidate solution and $\gA$ and $\gB$ are operators acting on $\vg$, PINN parameterizes $\vg$ using a neural network $\vg^\theta$ and optimizes its parameters $\theta$ by minimizing the functional $\gL^2$ norms $L(\theta) = \|\gA(\vg^\theta)\|_{\gL^2}^2 + \lambda\|\gB(\vg^\theta)\|_{\gL^2}^2$. The hyperparameter $\lambda$ balances the residuals of $\gA$ and $\gB$ and must be adjusted for optimal performance.
PINN is versatile and can be applied to a wide range of PDEs, but its performance may not be as good as other neural network-based solvers specifically designed for a particular class of PDEs, as it does not take into account other in-depth properties of the system \citep{krishnapriyan2021characterizing,wang2}.
In general, there is no theoretical guarantee on how the loss $L(\theta)$ controls the discrepancy between the candidate solution $\vg^\theta$ and the ground truth.

A recent work from \citet{zhang2022drvn} exploits the property (B) to design the Random Deep Vortex Network (RDVN) method for solving the 2D Navier-Stokes equation and achieves SOTA performance for this task. 
Let $\vu_t^\theta$ be an estimation of the interaction term $K\ast \rho_t$ in the SDE \ref{eqn_MVE_particle} and use $\rho_t^\theta$ to denote the law of the particle driven by the SDE $\ud \rmX_t = V(\rmX_t) \ud t  + \vu_t^\theta(\rmX_t) \ud t  + \sqrt{2\nu}  \ud \rmB_t$. The idea of RDVN is to minimize the time average of the $\gL^2$ norms $L(\theta) = \int_0^T\|\vu_t^\theta - K\ast \rho_t^\theta\|^2_{\gL^2}\ud t$. Note that in order to simulate the SDE, one needs to discretize the time variable in loss function $L$. After training $\theta$, $\rho_t^\theta$ is output as the estimated solution of the 2D Navier-Stokes equation. However, no convergence guarantee is given for the relationship between $L$ and the discrepancy between $\rho_t^\theta$ and the ground truth $\rho_t$. In our experiment section, we will extend RDVN to solve the MVE with Coulomb interaction as a baseline.

In another line of research, \citet{fan2022variational} exploits property (C) to propose the Primal-dual Gradient Flow (PDGF) method for the Fokker-Planck equation. The solution to the Fokker-Planck equation coincides with the Wasserstein gradient flow of the free energy functional. PDGF iteratively creates a sequence of transport maps by approximately solving the minimizing movement scheme, also referred to as the JKO scheme \citep{jordan1998variational}. 
These learned maps can be used to reconstructed the solution to the Fokker-Planck equation.
Unlike its previous work \citep{mokrov2021large}, PDGF uses the variational formulation of the $f$-divergence, allowing the gradient flow to be directly constructed from empirical distributions.
However, the variational formulation that PDGF is based on involves an extra dual variable, making each iteration of PDGF a non-convex non-concave minimization-maximization problem, which can be difficult to solve.
While the MVE with Coulomb interaction also has a Wasserstein gradient flow interpretation, unlike the 2D Navier-Stokes equation, it is unclear how PDGF can be generalized to solve this problem. As a result, we did not compare with PDGF in our experiments.


\citet{shen22a} propose the concept of self-consistency for the Fokker-Planck equation (a specific instance of MVE without the interaction term $K\equiv 0$), which most related to our research (as mentioned in the introduction). Unlike our work where the self-consistency potential is derived via the principle of stability analysis, this previous work constructs the self-consistency potential $R(f)$ for the hypothesis velocity field $f$ by observing that the underlying velocity field $f^*$ is the fixed point of some velocity-consistent transformation $\gA$ and they construct $R(f)$ to be a more complicated Sobolev norm of the residual $f - \gA(f)$. In their result, they bound the Wasserstein distance between $\rho^f$ and $\rho$ by $R(f)$, which is weaker than our KL type control. The improved KL type control for the Fokker-Planck equation has also been discussed in \citep{boffi2022probability}. A very recent work \citep{li2023self} extends the self-consistency approach to compute the general Wasserstein gradient flow numerically, without providing further theoretical justification.

