\section{Introduction}
Researchers are utilizing AI to push the boundaries of science research, following their success in the fields of computer vision and natural language processing. For centuries, scientists have discovered natural laws through real-world observations, using these laws to describe and predict the dynamics of real-world systems through Partial Differential Equations (PDEs). As PDEs are of fundamental importance, a growing area in machine learning is the use of neural networks (NN) to solve these equations {\citep{han2017deep,han2018solving,zhang2018deep,wang2018deepmd,raissi2020hidden,cai2021physics,karniadakis2021physics,lifourier,cuomo2022scientific}}.

An important category of PDEs is the McKean-Vlasov equation (MVE), which models the dynamics of a stochastic particle system with mean-field interactions
\begin{equation} \label{eqn_MVE_particle}
		\ud \rmX_t = - \nabla V(\rmX_t) \ud t  + K \ast \bar \rho_t(\rmX_t) \ud t  + \sqrt{2\nu}  \ud \rmB_t,  \quad
		\bar \rho_t = \Law(\rmX_t).  
\end{equation}
Here $\rmX_t \in \X$ denotes the (phase) space position of  a random particle, $\X$ is  either $\mathbb{R}^d$ or the  torus $\Pi^d = [-L, L]^d$ (a cube with periodic boundary condition), $V: \sR^d\rightarrow\sR$ denotes a {\em known} exterior potential, $K: \sR^d \rightarrow \sR^d$ denotes some interaction kernel function and the convolution operation $\ast$ is defined as $h \ast \phi = \int_{\X} h(\vx-\vy) \phi(\vy) d \vy$, $\{\rmB_t\}_{t\geq 0}$ is the standard  $d$-dimensional Wiener process with $\nu\geq 0$ being some diffusion coefficient, and $\bar \rho_t:\X \rightarrow \sR$ is the law or the probability density function of the random variable $\rmX_t$ and the initial data $\bar \rho_0$ is given.

Under mild regularity conditions, the density function $\bar \rho_t$ satisfies the MVE
\begin{equation} \label{eqn_MVE}
	\partial_t \bar \rho_t + \udiv \left( \bar \rho_t (- \nabla V + K \ast \bar \rho_t)\right) = \nu \Delta  \bar \rho_t,
\end{equation}
where $\udiv$ denotes the divergence operator, i.e. $\udiv\ h(\vx) = \sum_{i=1}^d \frac{\partial h_i}{\partial x_i}$ for a velocity field $h:\sR^d\rightarrow\sR^d$, $\Delta$ denotes the Laplacian operator on the spatial variables defined as $\Delta \phi = \udiv( \nabla \phi)$,  where $\nabla \phi$ denotes the gradient of a scalar function $\phi:\sR^d\rightarrow\sR$. Note that all these operators are applied only on the spatial variable $\vx$.

It is important to note that in order to accurately describe dynamics in real-world phenomena such as electromagnetism \citep{golse2016dynamics} and fluid mechanics {\citep{majda2002vorticity}}, the interaction kernels $K$ in the MVE are usually chosen to be highly \emph{singular}.
Two of the most notable examples are the MVE with Coulomb interactions where
\begin{equation} \label{eqn_coulomb_interaction}
	K(\vx) = -\nabla g(\vx),\ \mathrm{with}\ g(\vx) = \begin{cases}
		\left((d-2)S_{d-1}(1)\right)^{-1}\|\vx\|^{-(d-2)}, & d\geq 3,\\
		-(2\pi)^{-1}\log \|\vx\|,  & d = 2,
	\end{cases}
\end{equation}
with $S_{d-1}(1)$ denoting the surface area of the unit sphere in $\sR^d$,
and the vorticity formulation of the 2D Navier-Stokes equation ($d=2$) where the interaction kernel $K$ is given by the Biot-Savart law
\begin{equation} \label{eqn_nse}
	K(\vx) = \frac{1}{2\pi} \frac{\vx^\perp}{\|\vx\|^2} = \frac{1}{2\pi} \left(-\frac{x_2}{\|\vx\|^2}, \frac{x_1}{\|\vx\|^2}\right), 
\end{equation}
where $\vx= (x_1, x_2)$ and  $\|\vx\|$ denotes the Euclidean norm of a vector.
These choices of $K$ are called singular since $\|K(\vx)\| \rightarrow \infty$ when  $\|\vx\|\rightarrow 0$. 
The presence of singularity in the interaction kernel poses significant difficulties in solving the MVE using NN-based approaches. Furthermore, there is a significant lack of theoretical guarantees in the literature for NN-based algorithms. Specifically, prior to our research, there was no established method for determining the quality of a candidate solution through an easily computable quantity.




By noting $\Delta  \bar \rho_t = \udiv (\bar \rho_t \nabla \log \bar  \rho_t)$, we can rewrite the MVE in the form of a continuity equation
\begin{equation} \label{eqn_MVE_CE}
	\partial_t \bar  \rho_t  + \udiv \Big( \bar \rho_t \Big(- \nabla V + K \ast \rho_t -  \nu \nabla \log \bar \rho_t \Big)\Big) = 0,
\end{equation}
and for simplicity  throughout this paper we will denote $\mathcal{A}[\bar \rho_t](x) = - \nabla V + K \ast  \bar \rho_t -  \nu \nabla \log \bar \rho_t$
and refer to $\mathcal{A}[\bar \rho_t]$ as the \emph{underlying velocity}.
Consider another time-varying \emph{hypothesis velocity} field $f:\sR\times \sR^d\rightarrow\sR$. 
The goal of this paper is to show that, given some time interval $[0, T]$ with  $T>0$, when $f(t, \cdot)$ is sufficiently close to $\mathcal{A}[\bar \rho_t]$ in an appropriate sense, one can recover $\{ \bar \rho_t\}_{t\in[0, T]}$, i.e. the solution to the MVE in the time interval $[0, T]$, by solving the following continuity equation
\begin{equation} \label{eqn_CE}
	\partial_t \rho^f_t  + \nabla \cdot (\rho_t^f\,  f ) = 0,
\end{equation}
for $t \in [0, T]$ with $\rho^f_0 = \bar \rho_0$ (recall that  we assume the initial law  $\bar \rho_0$ to be given).
The superscript in $\rho_t^f$ is to emphasize its dependence on the hypothesis velocity field $f$ and we will refer to $\rho_t^f$ as the \emph{hypothesis solution}.
Our results can be informally summarized as follows.
\begin{theorem}
	Suppose that the initial density function $\bar \rho_0$ is sufficiently regular and the hypothesis velocity field  $f_t(\cdot) = f(t, \cdot)$ is at least three-times continuously differentiable both in $t$ and $x$. Define the \emph{self-consistency} potential/loss function  to be
	\begin{equation} \label{eqn_self_consistency_potential}
		\begin{split}
			R(f) 
%			& = \int_0^T \int_{\mathcal{X}} \Big\| f(t, x) - (- \nabla V (x) + K * \rho_t^f(x) - \nu \nabla \log \rho_t^f(x)) \Big\|^2  \rho_t^f (x) \ud x \ud t   \\
			= \int_0^T \Big\|f_t - (- \nabla V + K \ast \rho_t^f - \nu \nabla \log \rho_t^f) \Big\|_{\rho_t^f}^2 \ud t.
		\end{split}
	\end{equation}
	Here $\|h\|_\phi$ denotes the weighted $\gL^2$ norm for the function $h$, $\|h\|_\phi^2 = \int \|h(\vx)\|^2 \phi(\vx) d \vx$.
	We have for the MVE with the Coulomb interactions (\ref{eqn_coulomb_interaction}) or the Biot-Savart law (\ref{eqn_nse}), the KL divergence between the hypothesis solution $\rho_t^f$ and the ground truth solution $\bar \rho_t$ is controlled by the self-consistency potential for any $t\in[0, T]$, i.e. for some constant $C>0$, 
	\begin{equation}
		\sup_{t \in [0, T]}	\KL(\rho_t^f, \rho_t) \leq C R(f).
	\end{equation} 
\end{theorem}

Our work generalizes the result in \citep{shen22a}, where the authors consider the much simpler Fokker-Planck equation, an specific instance of our work (simply take $K \equiv 0$).
Consequently, as a by-product, we improve the Wasserstein-type bound therein to the KL type bound, and remove the requirement of the complicated Sobolev norm in the definition of self-consistency therein.
\citet{shen22a} further propose that the self-consistency of a PDE can be used to test the quality of an existing solution and to guide the design of some objective functions for a neural network parameterization of the PDE solution.
Inspired by their work, we generalize their framework to the more complicated MVE and provide algorithms that can solve the MVE with singular Coulomb interaction and the 2D Navier-Stokes equation. \\
%The computational limitations of \citep{shen22a} include the requirement of solving an ODE over a state space with dimensions that are at least as large as the size of the neural network when computing the gradient with respect to the neural network's parameters. By closely examining the adjoint method used for gradient computation, we demonstrate that this computational bottleneck can be eliminated.
\textbf{Contributions} In summary, we propose a novel neural network (NN) based MVE solver by exploiting the stability property of the corresponding system via entropy dissipation, which admits rigorous theoretical guarantees even in the presence of singular interaction kernels. We elaborate the contributions of our work from theory, algorithm and empirical perspectives as follows.
\vspace{-2mm}
\begin{enumerate}[wide, labelwidth=!, labelindent=0pt]
%\begin{itemize}
	\item (Theory-wise) By studying the stability of the MVE with singular interaction kernels in the Coulomb case (\ref{eqn_coulomb_interaction}) and the 2D Navier-Stokes equation (the Biot-Savart case (\ref{eqn_nse})) via entropy dissipation, we establish the self-consistency of the MVE. Specifically, we design a potential function $R(f)$ of a hypothesis velocity $f$ such that $R(f)$ controls the KL divergence between the hypothesis solution $\rho_t^f$ (defined in \eqref{eqn_CE}) and the ground truth solution $\bar \rho_t$ for any time stamp within a given time interval $[0, T]$. A direct consequence of this result is that $\rho_t^f$ exactly recovers $\bar \rho_t$ in the KL sense given that $R(f) = 0$.
	\item (Algorithm-wise) When the hypothesis velocity field is parameterized by a neural network (NN), i.e. $f = f_\theta$ with $\theta$ being some finite-dimensional parameters, the self-consistency potential $R(f_\theta)$ can be used as the loss function of the neural network parameters $\theta$. We discuss in details on how an estimator of the gradient $\nabla_\theta R(f_\theta)$ can be computed, so that gradient-based optimizers can be utilized to train the NN. In particular, for the 2D Navier-Stokes equation (the Biot-Savart case (\ref{eqn_nse})), we show that the singularity in the gradient computation can be removed   
	by exploiting an anti-derivative of the Biot-Savart kernel.
	\item (Empirical-wise) We compare the proposed approach, derived from our novel theoretical guarantees, with SOTA NN-based algorithms for solving MVE with the Coulomb interaction and the 2D Navier-Stokes equation (the Biot-Savart interaction). We pick specific instances of the initial density $\bar \rho_0$, under which explicit solutions are known and can be used as the ground truth to test the quality of the hypothesis ones.
	Using NNs with the same complexity (depth, width, and structure), we observe that the proposed method significantly outperforms the included baselines.
%\end{itemize}
\end{enumerate}