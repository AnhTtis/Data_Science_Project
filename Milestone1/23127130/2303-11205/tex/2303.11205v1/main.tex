%\documentclass[anon,12pt]{colt2023} % Anonymized submission
%\documentclass[final,12pt]{colt2023} % Include author names

%\usepackage{showkeys}

% The following packages will be automatically loaded:
% amsmath, amssymb, natbib, graphicx, url, algorithm2e
\documentclass{article}
\usepackage{PRIMEarxiv}


\title{Entropy-dissipation Informed Neural Network\\ for McKean-Vlasov Type PDEs}
\usepackage{times}
% Use \Name{Author Name} to specify the name.
% If the surname contains spaces, enclose the surname
% in braces, e.g. \Name{John {Smith Jones}} similarly
% if the name has a "von" part, e.g \Name{Jane {de Winter}}.
% If the first letter in the forenames is a diacritic
% enclose the diacritic in braces, e.g. \Name{{\'E}louise Smith}

% Two authors with the same address
% \coltauthor{\Name{Author Name1} \Email{abc@sample.com}\and
%  \Name{Author Name2} \Email{xyz@sample.com}\\
%  \addr Address}

% Three or more authors with the same address:
% \coltauthor{\Name{Author Name1} \Email{an1@sample.com}\\
%  \Name{Author Name2} \Email{an2@sample.com}\\
%  \Name{Author Name3} \Email{an3@sample.com}\\
%  \addr Address}

% Authors with different addresses:
\author{
	Zebang Shen\thanks{Authors are listed in an alphabetic order.}\\
	ETH Z\"urich \\
	\texttt{zebang.shen@inf.ethz.ch} \\
	%% examples of more authors
	\And
	Zhenfu Wang \\
	Peking University \\
	\texttt{zwang@bicmr.pku.edu.cn}
	%% \AND
	%% Coauthor \\
	%% Affiliation \\
	%% Address \\
	%% \texttt{email} \\
	%% \And
	%% Coauthor \\
	%% Affiliation \\
	%% Address \\
	%% \texttt{email} \\
	%% \And
	%% Coauthor \\
	%% Affiliation \\
	%% Address \\
	%% \texttt{email} \\
}
\usepackage{amssymb,amsmath,amsthm}
\newtheorem{theorem}{Theorem}
\newtheorem{proposition}{Proposition}
\newtheorem{remark}{Remark}
\newtheorem{lemma}{Lemma}

\input{math_commands.tex}
\newcommand{\Law}{\mathrm{Law}}
\usepackage{color}
\newcommand{\red}[1]{\textcolor{red}{#1}}
\newcommand{\ud}{\mathrm{d}}
\newcommand{\X}{\mathcal{X}}
%\newcommand{\ud}{\,\mathrm{d}}
\newcommand{\udiv}{\, \mathrm{div}}
\newcommand{\Uniform}{\mathrm{Uniform}}
\usepackage{makecell}
\usepackage{enumitem}
\let\KL\relax
\newcommand{\KL}{\mathbf{KL}}
\usepackage{natbib}
\usepackage{graphicx}
\usepackage{hyperref}       % hyperlinks


\begin{document}

\maketitle

\begin{abstract}%
  We extend the concept of {self-consistency} for the Fokker-Planck equation (FPE) \citep{shen22a} to the more general McKean-Vlasov equation (MVE).
  While FPE describes the macroscopic behavior of particles under drift and diffusion, MVE accounts for the additional inter-particle interactions, which are often highly singular in physical systems.
  Two important examples considered in this paper are the MVE with Coulomb interactions and the vorticity formulation of the 2D Navier-Stokes equation.
  We show that a generalized self-consistency potential controls the KL-divergence between a hypothesis solution to the ground truth, through entropy dissipation.
  Built on this result, we propose to solve the MVEs by minimizing this potential function, while utilizing the neural networks for function approximation.
  We validate the empirical performance of our approach by comparing with state-of-the-art NN-based PDE solvers on several example problems. 
\end{abstract}

%\begin{keywords}%
%  Entropy dissipation, McKean-Vlasov equation, 2D Navier-Stokes equation%
%\end{keywords}
\input{Introduction.tex}


%\section{Preliminaries}
%\subsection{Periodic Boundary Condition}
%
%\subsection{Neural Ordinary Differential Equation}


\section{Self-consistency of the McKean-Vlasov Equation}
In this section, we present the generalized self-consistency potential for the MVE.
To understand the intuition behind our design, we first write the continuity equation \ref{eqn_CE} in a similar form as the MVE:
\begin{equation} \label{eqn_CE_as_MVE}
	\partial_t \rho^f_t + \udiv \bigg(\rho_t^f \Big(- \nabla V + K \ast \rho_t^f - \nu \log \rho_t^f + \delta_t \Big) \bigg) = 0,
\end{equation}
where $f$ is the hypothesis velocity (recall that $f_t(\cdot) = f(t, \cdot)$) and 
\begin{equation} \label{eqn_perturbation}
	\delta_t = f_t - (- \nabla V + K \ast \rho_t^f - \nu \log \rho_t^f),
\end{equation}
can be regarded as a perturbation to the original MVE system.
Taking this perturbation perspective, it is natural to study the deviation of the hypothesis solution $\rho^f_t$ from the true solution $\bar \rho_t$ using an appropriate Lyapunov function $L(\rho_t^f, \bar \rho_t)$. 
Clearly this deviation will depend on the perturbation $\delta_t$, and such a dependence is often termed as the {\emph{stability}} of the underlying dynamical system. 
Moreover, the aforementioned relation between the perturbation and the deviation allows us to derive the self-consistency potential of the MVE. 
%From its construction, the potential function designed in this way should automatically control the deviation of the hypothesis from the ground truth.

Following this idea, the design of the self-consistency potential can be determined by the choice of the Lyapunov function $L$ used in the stability analysis. 
In the following, we describe the Lyapunov function used for the MVE with the Coulomb interaction and the vorticity formulation of the 2D Navier-Stokes equation (MVE with Biot-Savart interaction). The proof of the following results are the major theoretical contributions of this paper and will be elaborated in the analysis section \ref{section_analysis}.
\begin{itemize}
	\item For the MVE with the Coulomb interaction, we choose $L$ to be the \emph{modulated free energy} (defined in \eqref{DefModFree}) which is originally proposed in \citep{bresch2019mean} to establish the mean-field limit of a corresponding interacting particle system. We have (setting $L = E$)
	{
	\begin{equation}
		\frac{\ud}{\ud t } E(\rho_t^f, \bar \rho_t) \leq \frac 1 2  \int_{\mathcal{X}} \rho_t^f \, |\delta_t |^2 \ud x  + C E(\rho_t^f, \bar \rho_t),
	\end{equation}
	}
	where $C$ is a universal constant depending on $\nu $ and $(\bar\rho_t)_{t \in [0, T]}$. 
	\item For the 2D Navier-Stokes equation (MVE with the  Biot-Savart interaction), we choose $L$ to be the KL divergence. Our analysis is inspired by \citep{jabin2018quantitative} which for the first time establishes the quantitative mean-field limit of the stochastic interacting particle systems where the interaction kernel can be  in some negative Sobolev space. We have 
	{
	\begin{equation}
		\frac{\ud }{\ud  t} \mathbf{KL}(\rho_t^f, \bar \rho_t) \leq - \frac \nu  2 \int \rho_t |\nabla \log \frac{\rho_t}{\bar \rho_t}|^2 +C \mathbf{KL}(\rho_t^f, \bar \rho_t)+ \frac{1}{\nu } \int \rho_t^f  |\delta_t|^2, 
	\end{equation}
	}
where again $C$ is a universal constant depending on $\nu $ and $(\bar\rho_t)_{t \in [0, T]}$. 
\end{itemize}
%\textbf{TODO: We should comment on the relation between our work and the previous work.}
%Note that the Lyapunov function $L$ used in these previous works take different form which are designed for the analysis of mean-field limit. Specifically, 

From the above discussion, we can see that the self-consistency potential \ref{eqn_self_consistency_potential} is exactly the term derived by stability analysis of the MVE system with an appropariate Lyapunov function, after applying the Gr\"onwall's inequality.
However, the potential function \ref{eqn_self_consistency_potential} remains elusive from a computational perspective. Moreover, when utilized as the objective loss for training a parameterized hypothesis velocity field, we must be able to compute the gradient w.r.t. the parameters, so that gradient-based optimizer can be utilized. This is elaborated in the next section.


\subsection{Stochastic Gradient Computation with Neural Network Parameterization}
While the choice of self-consistency potential \ref{eqn_self_consistency_potential} is theoretically justified through the above stability study, in this section we show that it admits an estimator which can be efficiently computed. 
Given an initial data point $\vx_0$, define the trajectory $\{\vx(t)\}_{t=0}^T$ via the initial value problem $\frac{d \vx(t)}{d t} = f_t(\vx(t); \theta)$, $\vx(0) = \vx_0$ (suppose that $f_t$ is Lipschitz continuous for all $t\in[0, T]$ so the trajectory exists and is unique). 
Define the map $X_t$ such that $\vx(t) = X_t(\vx_0)$.
From the definition of the push-forward measure, one has $\rho^f_t = X_t\sharp\bar\rho_0$, where $\rho^t_t$ is defined in \eqref{eqn_CE}.
Recall the definitions of the potential $R(f)$ in \eqref{eqn_self_consistency_potential} and the perturbation $\delta_t$ in \eqref{eqn_perturbation}. Use the change of variable formula of the push-forward measure in (a) and the Fubini's theorem in (b). We have
\begin{equation}
	R(f) = \int_0^T \|\delta_t\|_{\rho_t^f}^2 d t \stackrel{(a)}{=} \int_0^T \|\delta_t\circ X_t\|_{\rho_0}^2 d t \stackrel{(b)}{=} \int \int_0^T \|\delta_t\circ X_t(\vx_0)\|^2 d t d \bar \rho_0(\vx_0).
\end{equation}
Consequently, by defining the trajectory-wise loss
\begin{equation}
	R(f; \vx_0) = \int_0^T \|\delta_t\circ X_t(\vx_0)\|^2 d t,
\end{equation}
we can write the potential function \ref{eqn_self_consistency_potential} as an expectation $R(f) = \E_{\vx_0\sim \bar \rho_0}[R(f; \vx_0)]$.
Suppose that the hypothesis velocity field is parameterized by a neural network $f = f_\theta$.
Using the above expectation formulation, we obtain an unbiased estimation of $\nabla_\theta R(f_\theta)$ via the Monte-Carlo integration w.r.t. $\vx_0 \sim \bar  \rho_0$, given that we can compute $\nabla_\theta R(f_\theta; \vx_0)$.

We show $\nabla_\theta R(f_\theta; \vx_0)$ can be computed, at least up to a high accuracy, via the adjoint method (for completeness see the derivation of the adjoint method in appendix \ref{appendix_adjoint_method}). 
As a recap, suppose that we can write $R(f_\theta; x_0)$ in a standard ODE-constrained form
\begin{equation} \label{eqn_trajectory_wise_inconsistency_ODE_loss}
	R(f_\theta; \vx_0) = \ell(\theta)\ = \int_0^T g(t, \vs(t), \theta) d t,
\end{equation}
where $\{\vs(t)\}_{t\in[0, T]}$ is the solution to the initial value problem $\frac{d }{d t} \vs(t) = \psi(t, \vs(t); \theta)$ with $\vs(0) = \vs_0$, and $\psi$ is a known transition function.
The adjoint method states that the gradient $\frac{d}{d \theta} \ell(\theta)$ can be computed as\footnote{This implies that we can obtain an unbiased estimator of $\frac{d \ell}{d \theta}$ by first sample $t\sim \mathrm{Uniform}[0, T]$ and simply compute $T * \left(a(t)^\top\frac{\partial \psi}{\partial \theta}(t, \vs(t); \theta) + \frac{\partial g}{\partial \theta}(t, \vs(t); \theta)\right)$. 
	In practice, we sample multiple $t$ uniformly from $[0, T]$ to reduce the variance.
	Note that to obtain $\vs(t)$ and $\va(t)$ for all sampled time stamp $t$, we still need to solve the ODEs involved in the definition of $\vx(t)$ and $\va(t)$ once, however their dimensions are now independent of the size of the neural network. }
\begin{equation}
	\frac{d \ell}{d \theta} = \E_{t\sim \Uniform[0, T]}\left[a(t)^\top\frac{\partial \psi}{\partial \theta}(t, \vs(t); \theta) + \frac{\partial g}{\partial \theta}(t, \vs(t); \theta)\right].
\end{equation}
where $a(t)$ is solution to the final value problems $\frac{d}{d t} a(t)^\top + a(t)^\top \frac{\partial  \psi}{\partial  s}(t, \vs(t); \theta) + \frac{\partial g}{\partial s}(t, \vs(t); \theta) = 0, a(T) = 0$.
In the following, we focus on how $R(f_\theta; \vx_0)$ can be written in the above ODE-constrained form.


\paragraph{Write $R(f_\theta; \vx_0)$ in ODE-constrained Form}
Expanding the definition of $\delta_t$ in \eqref{eqn_perturbation} gives
\begin{equation} \label{eqn_delta_x_t}
	\delta_t\circ X_t(\vx_0) = \delta_t(\vx(t)) = f_t(\vx(t)) - \left( - \nabla V(\vx(t)) + K\ast \rho_t^f(\vx(t)) - \nu \nabla \log \rho_t^f(\vx(t))\right).
\end{equation}
Note that in the above quantity, $f$ and $V$ are known functions. 
Moreover, it is known that $\nabla \log \rho_t^f(\vx(t))$ admits a closed form dynamics (e.g. see Proposition 2 in \citep{shen22a})
\begin{equation} \label{eqn_dynamics_of_score}
	\frac{d }{d t}\nabla \log \rho_{t}^{f}(\vx(t)) = - \nabla  \left(\nabla \cdot f_{t}(\vx(t); \theta)\right) -   \left(\gJ_{f_{t}}(\vx(t); \theta)\right)^\top \nabla \log \rho_{t}^{f}(\vx(t)),
\end{equation}
which allows it to be explicitly computed by starting from $\nabla \log \bar \rho_0(x_0)$ and integrating over time (recall that $\bar \rho_0$ is known).
Here $\gJ_{f_{t}}$ denotes the Jacobian matrix of $f_t$.
Consequently, all we need to handle is the convolution term $K\ast \rho_t^f(\vx(t))$, which in general cannot be exactly computed since it depends on the global configuration of the hypothesis distribution $\rho_t^f$. 

A common choice to avoid the difficulty of the convolution operation is via empirical approximation:
Let $\{\vy_{i}(t)\}_{i=1}^N$ be a batch of i.i.d. samples distributed according to $\rho_t^f$ and denote an empirical approximation of $\rho_t^f$ by $\mu_N^{\rho_t^f} = \frac{1}{N}\sum_{i=1}^{N} \delta_{\vy_{i}(t)}$, where $\delta_{\vy_{i}(t)}$ denotes the Dirac measure at $\vy_{i}(t)$. 
We approximate the convolution term in \eqref{eqn_delta_x_t} in different ways for the Coulomb and the  Biot-Savart interactions:
\vspace{-2mm}
\begin{enumerate}[wide, labelwidth=!, labelindent=0pt]
%\begin{itemize}
	\item For the Coulomb interaction, we directly approximate the convolution term in \eqref{eqn_delta_x_t} by $K \ast \mu_N^{\rho_t^f}(\vx(t)) = \frac{1}{N} \sum_{i=1}^N K(\vx(t) -  \vy_i(t))$. In practice, we choose $N$ sufficiently large so that the above empirical approximation is accurate. 
	Indeed, at least for the whole space case, i.e. the underlying space  $\mathcal{X}$ is $\mathbb{R}^d$, one has that 
	\vspace{-2mm}
	\[
	\int_{\mathbb{R}^d} |K * \mu_N^{\rho^{f}} (x)- K * \rho^f (x)|^2 \ud x = \int_{x \ne y } g(x - y ) \ud ( \mu_N^{\rho^f} -  \rho^f  )^{\otimes 2}(x, y) = F(\mu_N^{\rho^f}, \rho^f), 
	\]
	where $F(\mu_N^{\rho^f}, \rho^f)$ is the modulated (interaction) energy defined as in \cite{serfaty2020mean}. 
	We expect $F(\mu_N^{\rho^f}, \rho^f)\rightarrow 0$ almost surely as $N\rightarrow\infty$.
	\item For Biot-Savart interaction (2D Navier-Stokes equation), there are more structure to exploit and we can completely avoid the singularity: As noted by \cite{jabin2018quantitative}, the convolution kernel $K$ can be written in a divergence form:
	\vspace{-2mm}
	\begin{equation} \label{eqn_K_as_divergence}
		K = \nabla \cdot U, \text{ with } U(\vx) = \frac{1}{2\pi}\begin{bmatrix}
			-\arctan(\frac{\vx_1}{\vx_2}),& 0\\
			0,& \arctan(\frac{\vx_2}{\vx_1})
		\end{bmatrix},
	\end{equation}
	where the divergence of a matrix function is applied row-wisely, i.e. $[K(\vx)]_i = \udiv\ U_i(\vx)$.
	Using integration by parts, one has (assuming that the boundary integration vanishes, e.g. when the underlying space is a torus)
	\vspace{-2mm}
	\begin{align*}
		K\ast \rho_t^f (\vx) =&\ \int K(\vy) \rho_t^f(\vx - \vy) d \vy = \int \nabla\cdot U(\vy) \rho_t^f(\vx - \vy) d \vy = \int U(\vy) \nabla \rho_t^f(\vx - \vy) d \vy \\
		=&\ \int U(\vx - \vy) \rho_t^f(\vy) \nabla \log \rho_t^f(\vy) d \vy = \E_{\vy\sim \rho_t^f(\vy)}[U(\vx - \vy) \nabla \log \rho_t^f(\vy)].
	\end{align*}
	If the score function $\nabla \log \rho_t^f$ is bounded, then the integrand in the expectation is also bounded. Therefore, we can avoid integrating singular functions and the Monte Carlo-type estimation $\frac{1}{N} \sum_{i=1}^N U(\vx - \vy_i(t)) \nabla \log \rho_t^f(\vy_i(t))$ is accurate for a sufficiently large value of N.
%\end{itemize}
\end{enumerate}
\vspace{-2mm}
With the above discussion, we are now ready to write $R(f_\theta; \vx_0)$ in an ODE-constrained form. Define the state $\vs(t)$, the initial condition $\vs_0$ and the transition function $\psi$ as follows: Let
\begin{equation}
	\vs(t) = \left[\vx(t), \xi(t), \{\vy_i(t)\}_{i=1}^N, \{\zeta_i(t)\}_{i=1}^N\right],
\end{equation}
with $\xi(t) = \nabla\log\rho_t^f(\vx(t))$ and $\zeta_i(t) = \nabla\log\rho_t^f(\vy_i(t))$.  Take  the initial condition 
\begin{equation}
	\vs_0 = \left[\vx_0, \xi_0, \{\vy_i(0)\}_{i=1}^N, \{\zeta_i(0)\}_{i=1}^N\right]
\end{equation}
with $\xi_0 = \nabla\log \bar \rho_0(\vx_0)$, $\zeta_i(0) = \nabla\log\bar\rho_0(\vy_i(0))$, and $\vy_i(0) \stackrel{iid}{\sim} \bar \rho_0$;
and define the function
\begin{equation}
	\psi(t, s(t); \theta) = [f_t(\vx(t); \theta), h_t(\vx(t), \xi(t); \theta), \{f_t(\vy_i(t); \theta)\}_{i=1}^N, \{h_t(\vy_i(t), \zeta_i(t); \theta)\}_{i=1}^N],
\end{equation}
where $h(\va, \vb; \theta) = - \nabla  \left(\nabla \cdot f_{t}(\va; \theta)\right) -   \gJ^\top_{f_{t}}(\va; \theta) \vb$ (derived from \eqref{eqn_dynamics_of_score}).
Finally, define
\begin{equation}
	g(t, \vs(t); \theta) = \|f(t, \vx(t); \theta) - \left(-\nabla V(\vx(t)) + E(t, \vs(t)) - \nu \xi(t)\right)\|^2,
\end{equation}
where the estimator $E(t, \vs)$  of the convolution term is defined as
\begin{equation}
	E(t, \vs(t)) = \begin{cases}
		\frac{1}{N} \sum_{i=1}^N K(\vx(t) -  \vy_i(t)) & \text{ the Coulomb case}, \\
		\frac{1}{N} \sum_{i=1}^N U(\vx - \vy_i(t)) \zeta_i(t) & \text{the Biot-Savart case}.
	\end{cases}
\end{equation}
We recall the definition of $U$ in \eqref{eqn_K_as_divergence}.

%\paragraph{Removing the Integration over the Parameter Space} If we directly evaluate the integration in \eqref{eqn_adjoint_method_integration}, we need to solve an ODE numerically in stats space with its dimension at least as large as the number of parameters in the neural network. Consequently, the computational complexity will drastically increase if a complicated neural network structure is used. To alleviate this burden, observe that the gradient admits the equivalent expectation form
%\begin{equation}
%	\frac{d \ell}{d \theta} = \E_{t\sim \Uniform[0, T]}\left[a(t)^\top\frac{\partial \psi}{\partial \theta}(t, \vs(t); \theta) + \frac{\partial g}{\partial \theta}(t, \vs(t); \theta)\right].
%\end{equation}
\input{Analysis.tex}
























\input{related_works.tex}

\input{experiment.tex}

\input{Conclusion.tex}

% Acknowledgments---Will not appear in anonymized version
%\acks{We thank a bunch of people and funding agency.}
\bibliographystyle{abbrvnat}  
\bibliography{MVE}


\appendix
\input{appendix_experiments.tex}
\input{appendix_adjoint.tex}
\input{detailed_proofs.tex} 


%\clearpage
%\input{math_command_information.tex}
\end{document}
