\section{Methods}



We explored relationships between off-chain ETH/USD and CELO/USD\footnote{ETH/USD is the focus as it is the main oracle input for DeFi protocols and is explored for PoW Ethereum data. CELO/USD is included as a first look at PoS data.} prices and feature variables from PoW mining for Ethereum and Bitcoin (alternatively PoS validation for Celo), block space markets, network decentralization (e.g., burden on running a full node), usage and monetary velocity, and DeFi liquidity pools and AMMs, including activity on Bitcoin, Ethereum, and Celo networks. We obtained raw block and transaction data from Google Cloud Bigquery, Uniswap v1 and v2 data from the Graph, and off-chain USD price data from the Coinbase Pro API. We then derived the following types of on-chain data features:
\begin{itemize}
	\item \emph{Basic network features} that can be straightforwardly derived from Ethereum block and transaction data, covering information related to Ethereum’s network utility, ether supply,  transaction cost and the network’s computational consumption (i.e. the gas market).
	\item \emph{Uniswap features} on participation in DEX pools involving ETH and stablecoins (DAI, USDC, USDT). For the most part, we intentionally do not focus on DEX prices, as those measures would equivalently treat the stablecoin issuer as a sort of trusted oracle. We instead mainly focus on a measure of liquidity moving in and out of DEX pools.
	\item \emph{Economic features} that are suggested by fundamental economic models of decentralized networks and can also be derived from on-chain data (as described in the next subsection).
\end{itemize}
Data was collected spanning from July 1 2016 to May 1 2022 and was aggregated to the hourly level. We include Bitcoin data along with Ethereum data in the dataset for the sake of exploring relationships as in principle it can also be verified on-chain to varying degrees and discuss the connections further later.

Some further details on data and features are provided in the appendix. Precise methods are available in the project github repo: \url{https://github.com/tamamatammy/oracle-counterpoint}.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Fundamental Economic Features from On-chain Markets}

In addition to the above raw on-chain features, we also considered transformations of these features informed by fundamental economic models of on-chain markets, including PoW mining, PoS validation, block space markets, network decentralization costs of running full nodes, usage and monetary velocity, and on-chain liquidity pools (e.g., \cite{Huberman2019AnSystem,Prat2017AnMining,SusanAtheyIvaParashkevovVishnuSarukkai2016BitcoinUsage,Buterin2018BlockchainPricing,Fanti2019EconomicsINCOMPLETE,Easley2018FromHttps://ssrn.com/abstract=3055380}).
We analyzed the structure of these models to extract features that should economically be connected to price.



For example, \cite{Huberman2019AnSystem} models a block space market and finds that the ratio of average demand to capacity $\rho = \frac{\lambda}{\mu K}$ plays an important role in linking users' waiting costs to transaction fees pricing. Here $\lambda$ is the transaction volume, $K$ is the maximum number of transactions in a block, and $\mu$ is the block adding rate. A function emerges, called $F(\rho)$ that describes the relationship between fee pricing and congestion (i.e., amount of demand compared to supply for block space), which can be translated as
$$\text{tx fees in USD} = (\text{tx fees in ETH}) * \text{price}_{ETH} = F(\rho).$$
While $F(\rho)$ is nontrivial to work with, various pieces of the results in \cite{Huberman2019AnSystem} can be incorporated into useful features for the task of recovering $\text{price}_{ETH}$ (i.e., ETH/USD), including $\rho$, $\rho^2$, and the empirical finding that $\rho=0.8$ represents a phase transition in fee market pricing.

We also used the model in \cite{SusanAtheyIvaParashkevovVishnuSarukkai2016BitcoinUsage}, which modeled cryptocurrency price based on market fundamentals. A key feature in their model was currency velocity, which is defined as the ratio of transaction volume to money supply:
$$ \text{Velocity} = \frac{\text{Transaction Volume (USD)}}{\text{Exchange Rate} * \text{Supply of Currency}}.  $$
In their model, they show a unique equilibrium cryptocurrency exchange rate based on supply and demand with steady state expected exchange rate equal to the ratio of expected transaction volume and cryptocurrency supply.
Based on this model, we also incorporated the ratio of transaction volume (in cryptocurrency units since USD value is not given) and cryptocurrency supply, measured over one hour time steps, as an additional feature variable in our analysis.

We formulate other factors related to mining payoff, computational burden, and congestion as reviewed in the appendix.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Data-driven Feature Analysis}\label{sec:feature-analysis}

We analyse empirical relationships between features using graphical models and mutual information to study which features are most related to USD prices.

We use probabilistic models of Markov random fields, generated through sparse inverse covariance estimation with graphical lasso regularisation over normalized data, to express the partial/pair-wise relations between the time series of on-chain feature variables and off-chain prices.
For example, if the true underlying structure is Gaussian, then entries of the inverse covariance matrix are zero if and only if variables are conditionally independent (if the underlying structure is not Gaussian, then in general the inverse covariance matrix gives partial correlations).\footnote{In comparison, covariance represents relations between all variables as opposed to partial/pairwise relations.}
Sparse inverse covariance estimation is a method for learning the inverse covariance structure from limited data by applying a form of L1 regularization (see \cite{friedman2008sparse}).

The output of this technique helps to uncover strong empirical dependencies within the data, suggesting features that are strongly related to price and others that replicate similar information as others. We find that the method is often sensitive to the precise dataset used, which we adjust for by smoothing over the outputs of many $k$-fold subsets. 


We also consider mutual information between features in the dataset, which describes the amount of information measured (in information entropy terms), measured in reduction of uncertainty, obtained about price by observing the on-chain features. In information theory, entropy measures how surprising the typical outcome of a variable is, and hence the `information value'. This is helpful both in identifying strong relationships and evaluating different smoothing factors considering noisy on-chain signals. In this analysis, we consider smoothed versions of the feature set based on exponential moving averages with memory parameters $\alpha$, i.e., for feature value $b_t$ at time $t$, the smoothed measure is
$$ \tilde b_t = (1-\alpha) b_t + \alpha \tilde b_{t-1}.$$



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Modeling Off-chain Prices}


We apply supervised machine learning methods to explore the degree to which off-chain pricing information can be recovered from information that is entirely on-chain. We apply a few select simple and ensemble supervised machine learning methods on a rolling basis: basic regression, single decision tree, random forest, and gradient boost. 
A decision tree is a non-parametric method that breaks down the regression into subcases of the feature variables following a tree structure. A tree ensemble method constructs many tree models and uses the average of the trees as the output.
The motivation for using tree-based ensemble methods is the non-parametric nature of the dataset and success of similar methods in analyzing other market microstructure settings \cite{easley2021microstructure}.
In our results, we focus on random forest, which trains many trees to different samples of the empirical distribution of training points, as this method tends to be resilient to overfitting.

We run these models on the data set and evaluate performance using out-of-sample testing data on a rolling basis. The rolling training-testing data split, as depicted in Figure~\ref{fig:model-training}, is applied to boost model performance. For a given set of time series data with time duration of time t + time c = time t+c, where time series before time t were used for model training and time series between time t and time t + c were used for model testing. The benefit of this split is to test how good the model is in proxying ETH USD price for a fix period in the future, with all the information available in the past.

\begin{figure}
	\centering
	\includegraphics[width=\textwidth]{figures/model-training-rolling-forest}
	\caption{Rolling training-testing data split}
	\label{fig:model-training}
\end{figure}







