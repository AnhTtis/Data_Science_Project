\documentclass{midl}

\setlength{\tabcolsep}{4pt}

\usepackage{mwe} 
\usepackage{siunitx}

\title[Diffusion Models for Contrast Harmonization]{Diffusion Models for Contrast Harmonization
\newline of Magnetic Resonance Images}

\midlauthor{\Name{Alicia Durrer\nametag{$^{1}$}} \Email{alicia.durrer@unibas.ch}\\
\Name{Julia Wolleb\nametag{$^{1}$}}\Email{julia.wolleb@unibas.ch}\\
\Name{Florentin Bieder\nametag{$^{1}$}}\Email{florentin.bieder@unibas.ch}\\
\Name{Tim Sinnecker\nametag{$^{1,2}$}}\Email{tim.sinnecker@usb.ch}\\
\Name{Matthias Weigel\nametag{$^{1,2}$}}\Email{matthias.weigel@unibas.ch}\\
\Name{Robin Sandk{\"u}hler\nametag{$^{1}$}}\Email{robin.sandkuehler@unibas.ch}\\
\Name{Cristina Granziera\nametag{$^{1,2}$}}\Email{cristina.granziera@usb.ch}\\
\Name{{\"O}zg{\"u}r Yaldizli\nametag{$^{1,2}$}}\Email{Oezguer.Yaldizli@usb.ch}\\
\Name{Philippe C. Cattin\nametag{$^{1}$}}\Email{philippe.cattin@unibas.ch}\\
\addr $^{1}$ Department of Biomedical Engineering, University of Basel, Allschwil, Switzerland\\
\addr $^{2}$ University Hospital Basel, Switzerland}


\begin{document}

\maketitle

\begin{abstract}
Magnetic resonance (MR) images from multiple sources often show differences in image contrast related to acquisition settings or the used scanner type. For long-term studies, longitudinal comparability is essential but can be impaired by these contrast differences, leading to biased results when using automated evaluation tools. This study presents a diffusion model-based approach for contrast harmonization. We use a data set consisting of scans of 18 Multiple Sclerosis patients and 22 healthy controls. Each subject was scanned in two MR scanners of different magnetic field strengths ($\SI{1.5}{\tesla}$ and $\SI{3}{\tesla}$), resulting in a paired data set that shows scanner-inherent differences. We map images from the source contrast to the target contrast for both directions, from $\SI{3}{\tesla}$ to $\SI{1.5}{\tesla}$ and from $\SI{1.5}{\tesla}$ to $\SI{3}{\tesla}$. As we only want to change the contrast, not the anatomical information, our method uses the original image to guide the image-to-image translation process by adding structural information. The aim is that the mapped scans display increased comparability with scans of the target contrast for downstream tasks. We evaluate this method for the task of segmentation of cerebrospinal fluid, grey matter and white matter. Our method achieves good and consistent results for both directions of the mapping.
\end{abstract}
\begin{keywords}
Diffusion models, contrast harmonization, image-to-image translation
\end{keywords}
\section{Introduction}
In medical studies using magnetic resonance (MR) images, data acquisition from multiple centers and different scanners is a common scenario, especially regarding comprehensive or long-term studies \cite{kruger2020fully}. However, challenges arise when we compare data acquired with different MR scanners since the obtained MR images often display differences related to the acquisition settings and scanner variability \cite{dadar2020reliability}. 
\begin{figure}[ht]
	\floatconts
	{fig:overview}
	{\caption{Overview of our contrast harmonization method. We train a diffusion model using paired data from source contrast $\mathcal{S}$ and target contrast $\mathcal{T}$. We translate scan $B \in \mathcal{S}$ to scan $B_{transformed}$ that appears in contrast $\mathcal{T}$, allowing better comparability with $\hat{B} \in \mathcal{T}$ in subsequent tasks, such as segmentation.}}
	{\includegraphics[width=0.71\linewidth]{overview.png}}
\end{figure}
For instance, as the magnetic field strength alters the level of contrast between different tissue types \cite{maubon1999effect, ba2003effect}, the location of borders between different tissues can vary in images acquired with MR scanners of different field strength \cite{keihaninejad2010robust}. Therefore, direct comparison of scans from different scanners with automated tools such as SIENA \cite{smith2002accurate, smith2004advances} is highly difficult. Consequently, a scanner change in the middle of a long-term study affects the automatic evaluation and longitudinal comparability of scans \cite{sinnecker2022brain}. Longitudinal studies are important to monitor progressive diseases such as Multiple Sclerosis (MS), a demyelinating central nervous system disease \cite{mahad2015pathological}. It is essential that subsequent scans allow reliable interpretation of the disease progression without a bias related to a scanner change between the image acquisitions. In this work, we focus on a scanner change from $\SI{1.5}{\tesla}$ to $\SI{3}{\tesla}$ magnetic field strength that took place within a longitudinal MS study \cite{disanto2016swiss}. The goal of this work is to restore the comparability of the images by mapping all images to the same target contrast. A data set was acquired by \cite{sinnecker2022brain}, for which healthy subjects as well as MS patients were scanned in the $\SI{1.5}{\tesla}$ and the $\SI{3}{\tesla}$ scanner within approximately 3.5 months. Due to the short time span between the acquisitions, we assume that the differences in the images of the same participant are only related to the different scanner types. We therefore have paired $\SI{1.5}{\tesla}$ and $\SI{3}{\tesla}$ data for each participant.
\paragraph{Contribution}
By adapting a Denoising Diffusion Probabilistic Model (DDPM) \cite{ho2020denoising, wolleb2022diffusion} for contrast harmonization, we translate images from a source contrast $\mathcal{S}$ to a target contrast $\mathcal{T}$. Considering a pair $B \in \mathcal{S}$, $\hat{B} \in \mathcal{T}$, we map scan $B$ slice-by-slice to scan $B_{transformed}$, appearing in the contrast of $\mathcal{T}$, as shown in Figure \ref{fig:overview}. We generate consistent three-dimensional (3D) volumes by stacking two-dimensional (2D) slices, allowing us to save memory during image-to-image translation. Compared to the original $B$, $B_{transformed}$ presents better comparability with $\hat{B}$ from $\mathcal{T}$, with respect to downstream tasks such as segmentation of grey matter (GM), white matter (WM) and cerebrospinal fluid (CSF) using FAST \cite{zhang2001segmentation}. We achieve good results for both directions of the mapping, i.e., from  $\SI{3}{\tesla}$ to $\SI{1.5}{\tesla}$ and from $\SI{1.5}{\tesla}$ to $\SI{3}{\tesla}$.
\paragraph{Related Work}
Tracking brain volume changes over different scanners is prone to an error related to the scanner change \cite{lee2019estimating}. \cite{sinnecker2022brain} showed that given a paired data set, a corrective term for the volume computation can be calculated. To achieve a higher level of generalizability, images should be mapped to the target contrast. DeepHarmony \cite{dewey2019deepharmony}, builds on the U-Net architecture \cite{ronneberger2015u} and was developed for MR image contrast harmonization across protocol or scanner changes.
For almost a decade, GANs \cite{NIPS2014_5ca3e9b1} have been the state of the art for image generation and image-to-image translation \cite{karras2021alias, emami2020spa, zhu2017unpaired, isola2017image}. GANs such as \cite{dar2019image, liu2020multimodal, luo2021edge, peng2021multi} were created for multi-modal MR image synthesis, for instance T1- to T2-contrast. \cite{nie2017medical, nie2018medical} performed cross-modal and cross-scanner image synthesis using GANs. Lately, DDPMs \cite{sohl2015deep, ho2020denoising} became the focus of attention. \cite{nichol2021improved, dhariwal2021diffusion} further improved DDPMs, resulting in a transition of the state of the art for image generation from GANs to diffusion models. Their application includes text-to-image generation as in \cite{rombach2022high, saharia2022photorealistic}, image-to-image translation \cite{saharia2022palette, seo2022midms, wolleb2022swiss}, inpainting \cite{saharia2022palette, lugmayr2022repaint, wolleb2022swiss} and deformable image registration \cite{kim2022diffusemorph}. The recently introduced diffusion models are also used in medical image analysis. For instance for cross-modal \cite{lyu2022conversion} and multi-modal MR image synthesis \cite{ozbey2022unsupervised}, anomaly detection \cite{10.1007/978-3-031-16452-1_4} and synthetic image generation \cite{pinaya2022brain}. In medical studies, processing of 3D data is often required. \cite{dorjsembe2022three} showed the applicability of diffusion models to 3D data, but frequently, memory restrictions affect the processing of large volumes.
\section{Method}
DDPMs as described in \cite{nichol2021improved} form the basis for the proposed method. They are a class of generative models based on an iterative noising process $q$ and denosing process $p_{\theta}$. In the forward process $q$, Gaussian noise is added to an input image $x$ for $T$ time steps $t$. As the noise level is increased from a minimum at $t=0$ to a maximum at $t=T$, each image ${x_0, x_1, ..., x_T}$ displays a higher amount of noise compared to the previous one. The forward noising process $q$ is defined as
\begin{equation}\label{eq:forward}
q(x_{t}|x_{t-1}):=\mathcal{N}(x_{t};\sqrt{1-\beta _{t}}x_{t-1},\beta _{t}\mathbf{I}),
\end{equation}
where $\mathbf{I}$ is the identity matrix and $\beta_{1},...,\beta_{T}$ are the forward process variances. With \newline
$\alpha _{t}:=1-\beta _{t}$ and $\overline{\alpha}_{t}:=\prod_{s=1}^t \alpha _{s}$ and using the reparametrization trick, $x_t$ can be written as
\begin{equation}\label{eq:property}
x_{t}=\sqrt[]{\overline{\alpha} _{t}}x_{0}+\sqrt[]{1-\overline{\alpha} _{t}}\epsilon, \quad \mbox{with } \epsilon \sim \mathcal{N}(0,\mathbf{I}).
\end{equation}
For the denoising process $p_{\theta}$, the aim is to reverse the forward process, hence to predict $x_{t-1}$ from $x_t$ for $t \in \{T,...,1\}$. The learned model parameters $\theta$ define the reverse process
\begin{equation}\label{eq:reverse}
p_{\theta}(x_{t-1}\vert x_t):= \mathcal{N}\bigl(x_{t-1};\mu_{\theta}(x_t, t), \Sigma_\theta(x_t,t)\bigr).
\end{equation}
\newline To map images of the source contrast $\mathcal{S}$ to the target contrast $\mathcal{T}$, we use the generative process of DDPMs. We adapt the method of  \cite{wolleb2022diffusion}, originally created for DDPM-based image segmentation using paired data, to translate an image $B \in \mathcal{S}$ to $B_{transformed}$, which appears in the target contrast $\mathcal{T}$. The used data set provides for each of the $m$ participants one 3D scan $B$ of the source contrast $\mathcal{S}$ and one corresponding 3D scan $\hat{B}$ of the target contrast $\mathcal{T}$. Due to memory restrictions, we implement our model in 2D and slice each of the $m$ scans of both contrasts into $n$ slices and obtain for each participant $B$ = $\{b_i\}^{n}_{i=1}$ and $\hat{B} = \{\hat{b}_i\}^{n}_{i=1}$ slices. We translate the slices $\{b_i\}^{n}_{i=1}$ originating from a scan $B \in \mathcal{S}$ such that they resemble the contrast of the slices $\{\hat{b}_i\}^{n}_{i=1}$ originating from $\hat{B} \in  \mathcal{T}$, whereby each baseline slice $b_i$ has its corresponding ground truth $\hat{b}_{i}$. During training, depicted in Figure \ref{fig:training_method}, we pick a random timestep $t \in \{1,...,T\}$ and apply Equation \ref{eq:property} with $x_0$ = $\hat{b_i}$ to compute a noisy image $x_{b_i,t}$ from $\hat{b_i}$.
\begin{figure}[h]
	\floatconts
	{fig:training_method}
	{\caption{Overview of the training. Anatomical information is given through the concatenation of image $b_i$ from $B \in \mathcal{S}$ with noisy image $x_{b_i,t}$. $X_t$ is used by the diffusion model to predict a slightly denoised image $x_{b_i,t-1}$ from $x_{b_i,t}$ using Equation \ref{eq:sampling2}.}}
	{\includegraphics[width=0.75\linewidth]{training.png}}
\end{figure}
Since we only want to change the scanner-related image contrast and not any anatomical features, we add anatomical information of our baseline image $b_i$ through concatenation. We define the concatenated image as $X_t:=b_i \oplus x_{b_i,t}$ which serves as input for our diffusion model. 
We can compute $x_{b_i,t-1}$ using Equation \ref{eq:sampling2}, which summarizes a denoising step as
\begin{equation}\label{eq:sampling2}
x_{b_i,t-1}=\frac{1}{\sqrt[]{\alpha _{t}}}\left(x_{b_i,t}-\frac{1-\alpha _{t}}{\sqrt[]{1-\overline{\alpha }_{t}}}\epsilon _{\theta }(X_{t},t)\right)+\sigma_{t}\mathbf{z}, \quad \mbox{with } \mathbf{z} \sim \mathcal{N}(0,\mathbf{I}),
\end{equation}
whereby $\epsilon _{\theta }(X_{t},t)$ is the output of the diffusion model at time step $t$, $\sigma_t$ describes the variance scheme and $\mathbf{z}$ denotes the stochastic component of the process. The loss used to train the diffusion model $\epsilon_{\theta}$ can be written as
\begin{equation}\label{eq:mse}
{\mid\mid\epsilon-\epsilon_{\theta }(X_{t},t)\mid\mid}^{2} = {\mid\mid\epsilon-\epsilon_{\theta }(b_i \oplus(\sqrt{\bar{a{_t}}}\hat{b_i}+\sqrt{(1-\bar{a{_t}})}\epsilon),t)\mid\mid}^{2}, \quad \mbox{with } \epsilon \sim \mathcal{N}(0,\mathbf{I}).
\end{equation}
For the two directions of the image mapping, $\SI{3}{\tesla}$ to $\SI{1.5}{\tesla}$ and $\SI{1.5}{\tesla}$ to $\SI{3}{\tesla}$, two separate models need to be trained, as source and target contrast change.
To translate a scan volume $B = \{b_i\}^{n}_{i=1} \in \mathcal{S}$ to the target contrast $\mathcal{T}$ during sampling, we translate every slice $b_i$ to the synthetic slice $x_{b_i, 0}$ in the contrast of $\mathcal{T}$ . Figure \ref{fig:sampling_method} summarizes the sampling process starting from $x_{b_i,T} \sim \mathcal{N}(0,\mathbf{I})$. The previously trained denoising model is now applied for every denoising step $t \in \{T,...,1\}$ using Equation \ref{eq:sampling2}. Anatomical information is also added for every step $t$ of the denoising process through the concatenation of $b_i$ and $x_{b_i,t}$. We then stack the $n$ output slices $x_{b_i,0}$ to create our final 3D output volume $B_{transformed} = \{x_{b_i,0}\}^{n}_{i=1}$, displaying the whole brain in target contrast.
\begin{figure}[ht]
	\floatconts
	{fig:sampling_method}
	{\caption{Translation from $B \in \mathcal{S}$ to $B_{transformed}$. Each slice $b_i$ of $B \in \mathcal{S}$ is iteratively denoised by applying Equation \ref{eq:sampling2} for steps $t \in \{T,...,1\}$, whereby slice $b_i$ is used to add anatomical information through concatenation. The 2D output slices $\{x_{b_i,0}\}^{n}_{i=1}$ get stacked to $B_{transformed}$, showing the input scan $B$ translated to $\mathcal{T}$.}}
	{\includegraphics[width=0.85\linewidth]{sampling.png}}
\end{figure}
\section{Data Set and Training Details}
We used a data set exclusively created to track a scanner change from a $\SI{1.5}{\tesla}$ Siemens Magnetom Avanto to a $\SI{3}{\tesla}$ Siemens Magnetom Skyra$^{fit}$ whole-body scanner in 2016 \cite{sinnecker2022brain}. Scanner details can be found in Appendix \ref{scanners}. The participants' data is not public due to data privacy protection. Written consent was obtained from all participants. The data was coded (i.e., pseudoanonymized) at the time of the enrollement of the patients and includes scans of 22 healthy controls and 18 MS patients. All participants were scanned first in the $\SI{1.5}{\tesla}$ scanner and after a median time interval of 3.5 months in the $\SI{3}{\tesla}$ scanner. The relatively short time span between the scans ensures that no major disease progression happened in the MS patients, allowing a direct comparison of the scans and thereby forming a paired data set. Data set details can be found in Appendix \ref{dataset}. Pre-processing of the original 3D data includes skull-stripping using HD-BET \cite{isensee2019automated}, biasfield correction \cite{tustison2010n4itk}, resampling of voxel spacing to 1 x 1 x 1 {mm}$^3$ removal of the top and bottom 0.1 percentile of voxel intensities and normalization to voxel values between 0 and 1. With the ANTsPyx package, the paired images were registered based on affine and deformable transformation, using mutual information as similarity metric and an elastic regularization \cite{avants2014insight}. We sliced each pre-prepocessed 3D scan into 160 sagittal slices of shape [246, 262] that were then cropped to a size of [224, 224]. The cropping only affected background pixels. We trained our models for 300,000 iterations with a batch size of four on a NVIDIA GeForce RTX 2080 Ti GPU, taking about 60 hours. As in \cite{wolleb2022diffusion}, the number of channels in the first layer of the model is 128, and one attention head is used at resolution 16, resulting in 11,402,370 model parameters. The learning rate used is ${10^{-4}}$ for the Adam optimizer. $T$ is set as 1000. Details on hyperparameters and architecture can be found in \cite{nichol2021improved}. In addition to MSE calculation and histogram analysis, we evaluated CSF, GM and WM segmentations using FAST \cite{zhang2001segmentation} for the original images and the generated images. We did a four-fold cross-validation, combining data of healthy controls and MS patients in each fold. For each fold, the slices of 30 scans were used for training and those of 10 scans for testing.
\section{Results and Discussion}
For the evaluation, we compare our diffusion model (\textit{DM}) with DeepHarmony (\textit{DH}) \cite{dewey2019deepharmony} and \textit{pGAN} \cite{dar2019image}. Implementation details of the comparing methods can be found in Appendix \ref{implementation}. Each method takes an image $B \in \mathcal{S}$ and generates the image $B_{transformed}$ appearing in target contrast. The direct comparison of $B$ versus $\hat{B}$ is denoted as \textit{Original}. All tables in this chapter show the average scores on the test set over a four-fold cross-validation, the variances are listed in Appendix \ref{var}.
\begin{figure}[ht]
	\floatconts
	{fig:exampleimgs}
	{\caption{An exemplary coronal slice of a scan  $B \in \mathcal{S}$, the corresponding ground truth (GT) slice of $\hat{B} \in \mathcal{T}$ and slices of its mappings $B_{transformed}$ in contrast $\mathcal{T}$ generated by \textit{DH}, \textit{pGAN} and our \textit{DM} are shown for both mapping directions. The red circles indicate hyperintense regions generated by \textit{DH}. The blue arrows point at stripe artifacts produced by \textit{pGAN}. Further examples are provided in Appendix \ref{imgexamples}.}}
	{\includegraphics[width=\linewidth]{example_imgs.png}}
\end{figure}
\newline  Figure \ref{fig:exampleimgs} contains examples of generated images and the corresponding input and ground truth slices. The original $\SI{1.5}{\tesla}$ and $\SI{3}{\tesla}$ images differ in contrast. While \textit{pGAN} and our \textit{DM} generate convincing mappings of the input images to target contrast, \textit{DH} blurs the image and increases the brightness excessively for both directions of the image mapping. The examples shown are coronal slices from a 3D volume that was built by stacking sagittal slices. While images mapped to $\SI{3}{\tesla}$ contrast by \textit{pGAN} contain some stripe artifacts, mostly in the border regions of the brain, our \textit{DM} does not create any stripe artifacts, indicating that processing the data in a slice-wise fashion is a valuable simplification.
\begin{table}[ht]
	\floatconts
	{tab:mse_histo}%
	{\caption{MSE and AHD scores (formulas in Appendix \ref{var}) for both directions of the mapping.}}%
	{\begin{tabular}{c|cc|cc|}
&  \multicolumn{2}{c|}{{$\SI{3}{\tesla}$ to $\SI{1.5}{\tesla}$}} & \multicolumn{2}{c|}{{$\SI{1.5}{\tesla}$ to $\SI{3}{\tesla}$}} \\
			\bfseries Method & \textbf{MSE} & \textbf{AHD} & \textbf{MSE}& \textbf{AHD}\\
		    \textit{Original} & $1.85 \times 10^{-3}$ & $6.15 \times 10^{5}$ & $1.85 \times 10^{-3}$ & $6.15 \times 10^{5}$ \\
	        \textit{DH} & $2.30 \times 10^{-3}$  & $7.49 \times 10^{5}$ & $1.73 \times 10^{-3}$ & $8.22 \times 10^{5}$\\
			\textit{pGAN} & $\mathbf{1.50 \times 10^{-3}}$ & $\mathbf{1.27 \times 10^{5}}$ & $0.94 \times 10^{-3}$ & $2.07 \times 10^{5}$ \\
			\textit{DM} (ours) & $1.61 \times 10^{-3}$ &  $1.31 \times 10^{5}$ & $\mathbf{0.76 \times 10^{-3}}$ &  $\mathbf{1.48 \times 10^{5}}$ \\
	\end{tabular}}
\end{table}
\newline To compare the different methods, we compute the Mean Squared Error (MSE) between the ground truth images $\hat{B}$ and the translated images $B_{transformed}$. In Table \ref{tab:mse_histo} we report the MSE as well as the sum of the bin-wise absolute difference between the histograms (AHD), whereby each histogram consists of 255 bins. Mapping images of $\SI{3}{\tesla}$ to $\SI{1.5}{\tesla}$ contrast, our \textit{DM} and the \textit{pGAN} manage to decrease the MSE and to improve the AHD compared to \textit{Original}. \textit{DH} cannot compete and produces a higher MSE and AHD than the \textit{Original}. Mapping $\SI{1.5}{\tesla}$ to $\SI{3}{\tesla}$ contrast, our \textit{DM} outperforms \textit{Original}, \textit{DH} and \textit{pGAN} regarding MSE and AHD. 
The results show that the contrast harmonization shifts the voxel distributions characteristic for $\mathcal{S}$ towards the distributions of $\mathcal{T}$. For both directions of the mapping, exemplary histograms can be found in Appendix \ref{histogram_examples}. The histograms of the samples generated by \textit{pGAN} and our \textit{DM} better align with the histogram of the ground truth $\hat{B} \in \mathcal{T}$ than the histogram of $B \in \mathcal{S}$, for both directions of the mapping.
\begin{table}[ht]
	\floatconts
	{tab:seg_diff}%
	{\caption{Absolute volume differences in {mm}$^3$ of the segmentations of CSF, GM and WM of all original images $B \in \mathcal{S}$ and generated images $B_{transformed}$ compared to ground truths $\hat{B} \in \mathcal{T}$ for both directions of the mapping. The best result per class is bold.}}%
	{\begin{tabular}{c|ccc|ccc}
&  \multicolumn{3}{c|}{{$\SI{3}{\tesla}$ to $\SI{1.5}{\tesla}$}} & \multicolumn{3}{c}{{$\SI{1.5}{\tesla}$ to $\SI{3}{\tesla}$}} \\
			\bfseries Method & \textbf{CSF} & \textbf{GM}& \textbf{WM} & \textbf{CSF} & \textbf{GM}& \textbf{WM}\\
			\textit{Original} & $3.72 \times 10^{4}$ & $9.32 \times 10^{4}$ & $8.88 \times 10^{4}$ & $3.72 \times 10^{4}$ & $9.32 \times 10^{4}$ & $8.88 \times 10^{4}$ \\
			\textit{pGAN} &  $\mathbf{1.12 \times 10^{4}}$ &  $\mathbf{1.14 \times 10^{4}}$ & $1.91 \times 10^{4}$ & $3.11 \times 10^{4}$ & $6.15 \times 10^{4}$ & $2.09 \times 10^{4}$  \\
			\textit{DM} (ours) & $1.95 \times 10^{4}$ & $1.54 \times 10^{4}$ &  $\mathbf{1.59 \times 10^{4}}$ &  $\mathbf{1.79 \times 10^{4}}$ &  $\mathbf{3.91 \times 10^{4}}$ &  $\mathbf{1.49 \times 10^{4}}$  \\
	\end{tabular}}
\end{table}
\newline To obtain further insight about the increased comparability of $B_{transformed}$ with $\hat{B} \in \mathcal{T}$ for downstream tasks, we segmented the original 3D images as well as the 3D images generated by our \textit{DM} and \textit{pGAN} into the three classes CSF, GM and WM using FAST \cite{zhang2001segmentation}. \textit{DH} could not be considered for this comparison, as the quality of the generated images was not high enough to create meaningful segmentations using FAST. Examples of the segmentations can be found in Appendix \ref{segmentationsFAST}. As the voxel size is 1 x 1 x 1 {mm}$^3$, we calculate the volume of each class by counting the voxels attributed to each class. For both directions of the image mapping we then compute the volume differences for each class between the ground truth segmentation of $\hat{B}$ and the segmentations of $B$ or $B_{transformed}$, respectively.
Table \ref{tab:seg_diff} shows that \textit{pGAN} and our \textit{DM} decrease the differences between the segmentation volumes of $B_{transformed}$ and $\hat{B}$ compared to the segmentation volume difference between $B$ and $\hat{B}$ for both directions of the mapping. \textit{pGAN} and our \textit{DM} show similar performance for the mapping from $\SI{3}{\tesla}$ to $\SI{1.5}{\tesla}$ but our model performs better for the mapping from $\SI{1.5}{\tesla}$ to $\SI{3}{\tesla}$. We conclude that harmonizing contrasts before segmenting allows more coherent assignment of voxels to classes, enabling better comparison of tissue volumes between scans. We use the Dice score and the Hausdorff distance (HD) to assess that the contrast harmonization did not negatively affect the location of the segmented volumes, whereby the ground truth is given by the segmentation of $\hat{B}$. Table \ref{tab:hausdorff} shows that our \textit{DM} achieves better HD scores than \textit{pGAN} and the \textit{Original} for all classes. The Dice scores for our \textit{DM} remain in the ranges of the \textit{Original} for the mapping from $\SI{3}{\tesla}$ to $\SI{1.5}{\tesla}$. For the mapping from $\SI{1.5}{\tesla}$ to $\SI{3}{\tesla}$, however, our \textit{DM} improves the Dice scores of GM and WM considerably compared to the \textit{Original}. According to Tables \ref{tab:mse_histo} and \ref{tab:seg_diff}, \textit{pGAN} performed better than our \textit{DM} for the mapping from $\SI{3}{\tesla}$ to $\SI{1.5}{\tesla}$, but regarding the HD, our \textit{DM} seems more reliable for both directions of the mapping. The results indicate that using our \textit{DM}, the contrast harmonization and the resulting change in voxel distribution and segmentation volumes occurs at the desired regions. 
\begin{table}[ht]
	\floatconts
	{tab:hausdorff}%
	{\caption{Dice scores and HD of the segmentations of CSF, GM and WM of all original images $B \in \mathcal{S}$ and generated images $B_{transformed}$ compared to the ground truth segmentations of $\hat{B} \in \mathcal{T}$ for both directions of the mapping. The best result per class is bold.}}%
	{\begin{tabular}{c|cccccc|cccccc}
&  \multicolumn{6}{c|}{{$\SI{3}{\tesla}$ to $\SI{1.5}{\tesla}$}} & \multicolumn{6}{c}{{$\SI{1.5}{\tesla}$ to $\SI{3}{\tesla}$}} \\
&  \multicolumn{3}{c|}{\textbf{Dice}} & \multicolumn{3}{c|}{\textbf{HD}} & \multicolumn{3}{c|}{\textbf{Dice}} & \multicolumn{3}{c}{\textbf{HD}} \\
			\bfseries Method & \textbf{CSF} & \textbf{GM}& \textbf{WM} & \textbf{CSF} & \textbf{GM}& \textbf{WM} & \textbf{CSF} & \textbf{GM}& \textbf{WM} & \textbf{CSF} & \textbf{GM}& \textbf{WM}\\
			\textit{Original} & $0.82$ & $\mathbf{0.82}$ & $0.87$ & $10.21$ & $8.68$ & $11.16$ & $\mathbf{0.82}$ & $0.82$ & $0.87$ & $10.21$ & $8.68$ & $11.16$ \\
			\textit{pGAN} & $\mathbf{0.82}$ & $0.79$ & $0.87$ & $21.48$ & $13.54$ & $12.14$ & $0.76$ & $0.85$ & $0.92$ & $8.67$ & $7.24$ & $9.48$ \\
			\textit{DM} (ours) & $0.80$ & $0.79$ & $\mathbf{0.88}$ & $\mathbf{9.11}$ & $\mathbf{6.73}$ & $\mathbf{7.67}$ & $0.81$ & $\mathbf{0.88} $ & $\mathbf{0.92}$ & $\mathbf{8.65}$ & $\mathbf{7.01}$ & $\mathbf{9.30}$ \\
	\end{tabular}}
\end{table}
\section{Conclusion}
We present a novel method for contrast harmonization based on DDPMs. Using paired data from a source contrast $\mathcal{S}$ and a target contrast $\mathcal{T}$, our method allows us to translate a scan $B$ of the source contrast $\mathcal{S}$ to scan $B_{transformed}$ appearing in the contrast of $\mathcal{T}$. For the image-to-image translation, our diffusion model receives information from the source image $B$, as we only want to adjust the contrast while keeping the anatomical information. The translation improves comparability between scans from different contrasts for further evaluation and downstream tasks such as tissue segmentation. Our model outperforms the comparing methods for the mapping from $\SI{1.5}{\tesla}$ to $\SI{3}{\tesla}$ and generates great results for the opposite image mapping. Compared to GANs, diffusion models are not trained in an adversarial manner, making the training straightforward. The input and output of our model are 2D slices, allowing us to save memory compared to models trained on 3D volumes. Stacking the 2D images to a 3D volume does not generate any stripe artifacts, showing us that our method only changes the contrast and not the anatomical structure. Due to the image generation characteristics of DDPMs, our method has a long sampling time compared to the other methods, which could be improved by using other sampling schemes \cite{song2021denoising}. So far, our model was only trained on skull-stripped data sets, limiting a more in-depth temporal analysis of brain-volume changes. As our next step, we will omit the skull-stripping during pre-processing, enabling observing brain-volume changes relative to the fixed skull size.
\midlacknowledgments{This project was partially funded by the Research Fund for Junior Researchers of the University of Basel.}

~
\newpage

\bibliography{contrast_harmonization_bibliography}

\appendix

~
\newpage

\section{Scanner Details}\label{scanners}

\begin{table}[!ht]
\floatconts
  {tab:scanners}%
  {\caption{MR Scanners used for the data acquisition \cite{sinnecker2022brain}}}
  {\begin{tabular}{lll}
  \bfseries & \bfseries Previous Scanner & \bfseries Current Scanner\\
  Manufacturer & Siemens Healthineers & Siemens Healthineers \\
  &  Erlangen, Germany & Erlangen, Germany \\
  Model Name & Magnetom Avanto & Magnetom Skyra$^{fit}$ \\
  Magnetic Field Strength & $\SI{1.5}{\tesla}$ & $\SI{3}{\tesla}$ \\
  Repetition Time & 2080 $ms$ & 2300 $ms$ \\
  Inversion Time & 1100 $ms$ & 900 $ms$ \\
  Echo Time & 3.1 $ms$ & 2.94 $ms$ \\
  Imaging Matrix & 240 x 256 & 240 x 256 \\
  Field of View (FOV) & 234 x 250 {mm}$^2$ & 240 x 256 {mm}$^2$ \\
  Pixel Bandwidth & 130 $Hz$ per Pixel & 240 $Hz$ per Pixel \\
  Flip Angle & 15 $degrees$ & 9 $degrees$ \\
  Scanner Image Filter & Prescan-Normalization & Prescan-Normalization and \\
  & & Distortion Correction in 3D \\ 
  Software Version & syngo MR B17 & syngo MR VE11C \\
  Sequence used for Data Set & MPRAGE & MPRAGE \\
  Scan Period & 2012 - 2016 & 2016 - present \\
  \end{tabular}}
\end{table}
\FloatBarrier

\section{Data Set Details}\label{dataset}
The MS patients are part of a longitudinal MS study \cite{disanto2016swiss}. %the Swiss MS cohort study. 
Both, MS patients and healthy controls were scanned first in the $\SI{1.5}{\tesla}$ scanner and after the scanner change in the $\SI{3}{\tesla}$ scanner (median time interval 3.5 months, range 1.7 â€“ 5.2 months).
\begin{table}[!ht]
\floatconts
  {tab:dataset_MS}%
  {\caption{Details about the participants included in the data set, provided by \cite{sinnecker2022brain}}}
  {\begin{tabular}{lll}
  \bfseries  & \bfseries Multiple Sclerosis & \bfseries Healthy Controls\\
  Number of Female Participants & 14 & 11 \\
  Number of Male Participants & 4 & 11 \\
  Age in Years, Mean [SD] & 51.7 [$\pm 12.7$]  & 28.9 [$\pm 7.6$] 
  \end{tabular}}
\end{table}
\FloatBarrier
\newpage
~
%\newpage
\section{Implementation Details}\label{implementation}
We compared our models against \textit{DeepHarmony} (\textit{DH})  and \textit{pGAN}. For both comparing methods, we used the same four train and test folds as for our model to perform cross-validation. \\
\\
Implementation details:
\begin{itemize}
\item \textit{DH}: to make it more comparable to our method, we used 2D sagittal slices with shape [256, 256] as input, instead of the 2.5 dimensional implementation proposed in the paper. We cropped the output to [224, 224] and stacked the slices to a 3D volume of shape [160, 224, 224]. We trained the model for 300 epochs. We used a batch size of eight as in the original implementation and adjusted the learning rate to ${10^{-4}}$ as the original learning rate was not compatible with our images. For further implementation details refer to \cite{dewey2019deepharmony}.
\item \textit{pGAN}: we trained the models for 300 epochs (150 with normal learning rate, 150 with the learning rate decayed to zero) with a batch size of four. We used no neighbouring slices. The cycle loss weight as well as the perceptual loss weight were set to 100. The processed slices were of shape [256, 256], we cropped and stacked the output slices to shape [160, 224, 224] for comparison with our model. For further implementation details refer to \cite{dar2019image} and \url{https://github.com/icon-lab/pGAN-cGAN}.
\end{itemize}
\FloatBarrier
\newpage
~
%\newpage
\section{Detailed Metrics}\label{var}
The MSE was calculated using
\begin{equation}\label{eq:mseapp}
MSE = \sum_{i=1}^{N}(a_i-b_i)^2
\end{equation}
with $a$ and $b$ being the images to compare and $i$ iterating over all voxels $N$. The squared differences are summed.
To calculate the AHD we used the following formula
\begin{equation}\label{eq:adhapp}
AHD = \sum_{i=1}^{bins} \mid h_1(x_i) - h_2(x_i) \mid
\end{equation}
with $h_1$ and $h_2$ being the histograms to compare for all bins $x_i$, with $i$ ranging from one to the total number of bins.
\begin{table}[h]
\floatconts
  {tab:mse_var_BtoA}%
  {\caption{MSE and AHD averages including variances over four-fold cross-validation for the mapping from $\SI{3}{\tesla}$ to $\SI{1.5}{\tesla}$ contrast.}}%
  {\begin{tabular}{lll}
  \bfseries Method & \textbf{MSE} & \textbf{AHD} \\
		    Original & $1.85 [\pm 0.03] \times 10^{-3}$ & $6.15 [\pm 0.12] \times 10^{5}$ \\
	        DH & $2.30 [\pm 0.07] \times 10^{-3}$  & $7.49 [\pm 0.09] \times 10^{5}$\\
			pGAN & $\mathbf{1.50 [\pm 0.03] \times 10^{-3}}$ & $\mathbf{1.27 [\pm 0.07] \times 10^{5}}$ \\
			DM (ours) & $1.61 [\pm 0.03] \times 10^{-3}$ &  $1.31 [\pm 0.16] \times 10^{5}$ \\
  \end{tabular}}
\end{table}
\begin{table}[h]
\floatconts
  {tab:mse_var_AtoB}%
  {\caption{MSE and AHD averages including variances over four-fold cross-validation for the mapping from $\SI{1.5}{\tesla}$ to $\SI{3}{\tesla}$ contrast.}}%
  {\begin{tabular}{lll}
  \bfseries Method & \textbf{MSE} & \textbf{AHD}\\
		    Original & $1.85 [\pm 0.03] \times 10^{-3}$ & $6.15 [\pm 0.12] \times 10^{5}$ \\
	        DH & $1.73 [\pm 0.07] \times 10^{-3}$ & $8.22 [\pm 0.14] \times 10^{5}$\\
			pGAN & $0.94 [\pm 0.11] \times 10^{-3}$ & $2.07 [\pm 0.34] \times 10^{5}$ \\
			DM (ours) & $\mathbf{0.76 [\pm 0.008] \times 10^{-3}}$ &  $\mathbf{1.48 [\pm 0.16] \times 10^{5}}$ \\
  \end{tabular}}
\end{table}
\begin{table}[h]
\floatconts
  {tab:diff_var_BtoA}%
  {\caption{Average CSF, GM and WM segmentation differences and variances of the four-fold cross-validation. Differences between $\hat{B}$ and $B$ as well as between $\hat{B}$ and $B_{transformed}$ for the mapping from $\SI{3}{\tesla}$ to $\SI{1.5}{\tesla}$ contrast.}}%
  {\begin{tabular}{llll}
		    \bfseries Method & \textbf{CSF} & \textbf{GM}& \textbf{WM} \\
			Original & $3.72 [\pm 0.43] \times 10^{4}$ & $9.32 [\pm 0.93] \times 10^{4}$ & $8.88 [\pm 0.58] \times 10^{4}$ \\
			pGAN &  $\mathbf{1.12 [\pm 0.17] \times 10^{4}}$ &  $\mathbf{1.41 [\pm 0.23] \times 10^{4}}$ & $1.91 [\pm 0.19] \times 10^{4}$ \\
			DM (ours) & $1.95 [\pm 0.24] \times 10^{4}$ & $1.54 [\pm 0.40] \times 10^{4}$ &  $\mathbf{1.59 [\pm 0.24] \times 10^{4}}$  \\
  \end{tabular}}
\end{table}
\begin{table}[h]
\floatconts
  {tab:diff_var_AtoB}%
  {\caption{Average CSF, GM and WM segmentation differences and variances of the four-fold cross-validation. Differences between $\hat{B}$ and $B$ as well as between $\hat{B}$ and $B_{transformed}$ for the mapping from $\SI{1.5}{\tesla}$ to $\SI{3}{\tesla}$ contrast.}}%
  {\begin{tabular}{llll}
		    \bfseries Method & \textbf{CSF} & \textbf{GM}& \textbf{WM} \\
			Original & $3.72 [\pm 0.43] \times 10^{4}$ & $9.32 [\pm 0.93] \times 10^{4}$ & $8.88 [\pm 0.58] \times 10^{4}$ \\
			pGAN & $3.11 [\pm 0.55] \times 10^{4}$ & $6.15 [\pm 0.51] \times 10^{4}$ & $2.10 [\pm 0.27] \times 10^{4}$  \\
			DM (ours) & $\mathbf{1.79 [\pm 0.06] \times 10^{4}}$ &  $\mathbf{3.91 [\pm 0.19] \times 10^{4}}$ & $\mathbf{1.49 [\pm 0.65] \times 10^{4}}$ \\
  \end{tabular}}
\end{table}
\begin{table}[!ht]
	\floatconts
	{tab:dicevarBtoA}%
	{\caption{Average Dice scores and variances of the four-fold cross-validation of segmentations of CSF, GM and WM of $B$ and of $B_{transformed}$ compared to ground truth segmentation of $\hat{B}$ for the mapping from $\SI{3}{\tesla}$ to $\SI{1.5}{\tesla}$ contrast.}}%
	{\begin{tabular}{llll}
			\bfseries {Method} & \bfseries{CSF} & \bfseries{GM} & \bfseries{WM} \\
			Original & $0.8188 [\pm 0.0095]$ & $\mathbf{0.8180 [\pm 0.0041]}$ & $0.8715 [\pm 0.0031]$ \\
			pGAN & $\mathbf{0.8205 [\pm 0.0075]}$ & $0.7925 [\pm 0.0040]$ & $0.8745 [\pm 0.0029]$ \\
			DM (ours) & $0.7980 [\pm 0.0052]$ & $0.7918 [\pm 0.0029]$ & $\mathbf{0.8762 [\pm 0.0027]}$ \\
	\end{tabular}}
\end{table}
\begin{table}[!ht]
	\floatconts
	{tab:dicevarAtoB}%
	{\caption{Average Dice scores and variances of the four-fold cross-validation of segmentations of CSF, GM and WM of $B$ and of $B_{transformed}$ compared to ground truth segmentation of $\hat{B}$ for the mapping from $\SI{1.5}{\tesla}$ to $\SI{3}{\tesla}$ contrast.}}%
	{\begin{tabular}{llll}
			\bfseries {Method} & \bfseries{CSF} & \bfseries{GM} & \bfseries{WM} \\
			Original & $\mathbf{0.8188 [\pm 0.0095]}$ & $0.8180 [\pm 0.0041]$ & $0.8715 [\pm 0.0031]$ \\
			pGAN & $0.7607 [\pm 0.0113]$ & $0.8521 [\pm 0.0049]$ & $0.9163 [\pm 0.0034]$ \\
			DM (ours) & $0.8094 [\pm 0.0101]$ & $\mathbf{0.8760 [\pm 0.0045]}$ & $\mathbf{0.9232 [\pm 0.0031]}$ \\
	\end{tabular}}
\end{table}
\begin{table}[!ht]
	\floatconts
	{tab:HDvarBtoA}%
	{\caption{Average Hausdorff distances and variances of the four-fold cross validation of segmentations of CSF, GM and WM of $B$ and of $B_{transformed}$ compared to ground truth segmentation of $\hat{B}$ for the mapping from $\SI{3}{\tesla}$ to $\SI{1.5}{\tesla}$ contrast.}}%
	{\begin{tabular}{llll}
			\bfseries {Method} & \bfseries{CSF} & \bfseries{GM} & \bfseries{WM} \\
			Original & $10.21 [\pm 1.03]$ & $8.68 [\pm 0.83]$ & $11.16 [\pm 0.99]$ \\
			pGAN & $21.48 [\pm 5.88]$ & $13.54 [\pm 3.53]$ & $12.14 [\pm 2.98]$ \\
			DM (ours) & $\mathbf{9.11 [\pm 1.05]}$ & $\mathbf{6.73 [\pm 1.02]}$ & $\mathbf{7.67 [\pm 1.02]}$ \\
	\end{tabular}}
\end{table}
\begin{table}[!ht]
	\floatconts
	{tab:HDvarAtoB}%
	{\caption{Average Hausdorff distances and variances of the four-fold cross validation of segmentations of CSF, GM and WM of $B$ and of $B_{transformed}$ compared to ground truth segmentation of $\hat{B}$ for the mapping from $\SI{1.5}{\tesla}$ to $\SI{3}{\tesla}$ contrast.}}%
	{\begin{tabular}{llll}
			\bfseries {Method} & \bfseries{CSF} & \bfseries{GM} & \bfseries{WM} \\
			Original & $10.21 [\pm 1.03]$ & $8.68 [\pm 0.83]$ & $11.16 [\pm 0.99]$ \\
			pGAN & $8.67 [\pm 0.80]$ & $7.24 [\pm 0.90]$ & $9.48 [\pm 1.04]$ \\
			DM (ours) & $\mathbf{8.65 [\pm 0.91]}$ & $\mathbf{7.01 [\pm 0.81]}$ & $\mathbf{9.30 [\pm 1.33]}$ \\
	\end{tabular}}
\end{table}
\FloatBarrier
\newpage
~
%\newpage
\section{Histogram Examples}\label{histogram_examples}
In Figures \ref{fig:histograms_I}, \ref{fig:histograms_II}, \ref{fig:histograms_III} and \ref{fig:histograms_IV} we show exemplary histograms of both, healthy control and MS patient scans for both directions of the image mapping. Each histogram consists of 255 bins. Each figure contains the histograms of $B$, $\hat{B}$ and the generated volumes $B_{transformed}$. The histograms of \textit{DH} are shifted towards the brightest voxels for both directions of the mapping. Due to this comparatively high amount of white voxels, the histograms of the \textit{DH} samples are cropped in Figures \ref{fig:histograms_I} - \ref{fig:histograms_IV}. The initial $\SI{1.5}{\tesla}$ and $\SI{3}{\tesla}$ histograms show big differences. Both show two peaks, but for the $\SI{3}{\tesla}$ images, these are further apart, letting us perceive images of higher contrast compared to the $\SI{1.5}{\tesla}$ images. In Figures \ref{fig:histograms_I} - \ref{fig:histograms_IV}, our \textit{DM} and \textit{pGAN} manage to align the histograms of the generated samples much closer with these of the ground truths $\hat{B} \in \mathcal{T}$ than the initial histograms of $B \in \mathcal{S}$. Therefore we assume that for both methods, the contrast harmonization is proving effective, for healthy control as well as for MS patient scans.
\begin{figure}[ht]
	\floatconts
	{fig:histograms_I}
	{\caption{Exemplary histograms of one healthy control for the mapping from $\SI{3}{\tesla}$ to $\SI{1.5}{\tesla}$. The histograms of $B_{transformed}$ generated by \textit{pGAN} and our \textit{DM} align the ground truth histogram of $\hat{B}$ well compared to the histogram of the input image $B$.}}
	{\includegraphics[width=0.60\linewidth]{histogram_HC_3_T_to_1_5_T.png}}
\end{figure}
\begin{figure}[ht]
	\floatconts
	{fig:histograms_II}
	{\caption{Exemplary histograms of one healthy control for the mapping from $\SI{1.5}{\tesla}$ to $\SI{3}{\tesla}$. The histograms of $B_{transformed}$ generated by \textit{pGAN} and our \textit{DM} align the ground truth histogram of $\hat{B}$ well compared to the histogram of the input image $B$.}}
	{\includegraphics[width=0.70\linewidth]{histogram_HC_1_5_T_to_3_T.png}}
\end{figure}
\begin{figure}[ht]
	\floatconts
	{fig:histograms_III}
	{\caption{Exemplary histograms of one MS patient for the mapping from $\SI{3}{\tesla}$ to $\SI{1.5}{\tesla}$. The histograms of $B_{transformed}$ generated by \textit{pGAN} and our \textit{DM} align the ground truth histogram of $\hat{B}$ well compared to the histogram of the input image $B$.}}
	{\includegraphics[width=0.70\linewidth]{histogram_MS_3_T_to_1_5_T.png}}
\end{figure}
\begin{figure}[ht]
	\floatconts
	{fig:histograms_IV}
	{\caption{Exemplary histograms of one MS patient for the mapping from $\SI{1.5}{\tesla}$ to $\SI{3}{\tesla}$. The histogram of $B_{transformed}$ generated by \textit{pGAN} aligns the ground truth histogram of $\hat{B}$ well compared to the histogram of the input image $B$, while our \textit{DM} manages to almost perfectly match the peaks of $\hat{B}$.}}
	{\includegraphics[width=0.70\linewidth]{histogram_MS_1_5_T_to_3_T.png}}
\end{figure}
\FloatBarrier
\newpage
~
%\newpage
\section{Image Examples}\label{imgexamples}
\begin{figure}[h]
	\floatconts
	{fig:imgBtoA}
	{\caption{Exemplary images for the translation from $\SI{3}{\tesla}$ source contrast to $\SI{1.5}{\tesla}$ target contrast. The methods generate sagittal slices (right column), which are stacked to create a 3D volume. The red circles indicate hyperintense regions generated by \textit{DH}.}}
	{\includegraphics[width=0.96\linewidth]{example_imgs_3_T_to_1_5_T.png}}
\end{figure}
\begin{figure}[h]
	\floatconts
	{fig:imgAtoB}
	{\caption{Exemplary images for the translation from $\SI{1.5}{\tesla}$ source contrast to $\SI{3}{\tesla}$ target contrast. The methods generate sagittal slices (right column), which are stacked to create a 3D volume. The red circles indicate hyperintense regions generated by \textit{DH}. The blue arrows point at stripe artifacts due to the stacking of 2D slices produced by the \textit{pGAN}.}}
	{\includegraphics[width=1.0\linewidth]{example_imgs_1_5_T_to_3_T.png}}
\end{figure}
\FloatBarrier
\newpage
~
%\newpage
\section{Segmentation Examples}\label{segmentationsFAST}
\begin{figure}[!ht]
	\floatconts
	{fig:segBtoA}
	{\caption{Exemplary segmentations for the translation from $\SI{3}{\tesla}$ source contrast to $\SI{1.5}{\tesla}$ target contrast. Green indicates CSF, yellow is GM and red is WM.}}
	{\includegraphics[width=1.0\linewidth]{segmentations_BtoA.png}}
\end{figure}
\begin{figure}[!ht]
	\floatconts
	{fig:segAtoB}
	{\caption{Exemplary segmentations for the translation from $\SI{1.5}{\tesla}$ source contrast to $\SI{3}{\tesla}$ target contrast. Green indicates CSF, yellow is GM and red is WM.}}
	{\includegraphics[width=1.0\linewidth]{segmentations_AtoB.png}}
\end{figure}

\end{document}
