\section{Introduction \label{sec:introduction}}


E2E models \cite{Ryan19,CC18,Graves12,GravesMohamedHinton13,RaoSakPrabhavalkar17,Chan15,kim2017ctcattn,ChiuRaffel17} combine the acoustic (AM), pronunciation (PM) and language models (LM) of a conventional ASR system into a single neural network. This structure makes them significantly smaller than conventional models \cite{CC18,SainathPang19} and ideal for on-device ASR \cite{Ryan19}. However, the performance of E2E models on rare word recognition still lags behind conventional models. The performance gap is partially because they lack the ability to train using text-only data, which is abundant and often utilized by conventional LMs.

There have been multiple approaches for augmenting E2E models and training procedures to incorporate unpaired text data. Broadly speaking, these approaches use some combination of an LM trained on text data (shallow, cold, deep fusion \cite{Jan17,Sriram17,Anjuli18,inaguma2019fusion}) and a multi-stage training procedure that incorporates unpaired data (``weak distillation" \cite{Bo19}, ``back-translation" \cite{Sennrich16}, ``cycle-consistency" \cite{Hori19,DBLP:conf/icml/RenTQZZL19, Bai2019}). Each approach produces improvements in performance, but also increases some combination of model size, training and inference complexity, making it less desirable for on-device applications.

A recent approach, the joint acoustic and text decoder (JATD) \cite{sainath2020jatd, wang2020jatd} side-steps these issues. It utilizes unpaired data during E2E model training either directly or by generating the missing half of the data; using TTS to generate audio from text and ASR to generate text from audio. Previous methods using unpaired data in training have shown limited success in improving performance on real audio test sets \cite{Bo19,Ding19}. JATD produces stronger results by using a fixed context vector as a ``domain ID" to distinguish between paired and unpaired data during training. Paired data is processed as normal with the encoder computing an acoustic context vector that is fed to the decoder. For unpaired data (text-only or with synthesized audio), JATD bypasses the encoder network by using a fixed but learnable context vector in place of the encoder output, allowing the model to train on text-only data and avoiding the encoder training on synthesized audio.

The JATD architecture results in only a trivial increase in model size. It has been implemented within a LAS two-pass decoding framework and trained on both audio-text pair data and unpaired text-only data (by synthesizing into TTS utterances), showing substantial improvements in WER, especially on rare words \cite{sainath2020jatd}.

Beside data augmentation approaches, novel model architectures also show improvements in recognizing rare words. Recently, deliberation models \cite{xia2017deliberation, hu2020deliberation, hu2021deliberation} have used an attention-based two-pass design to achieve state-of-the-art performance on Google VoiceSearch test sets. Similar to other two-pass models, deliberation uses its first-pass decoder to produce streaming hypotheses and its second-pass decoder to attend to the first-pass hypotheses alongside the encoder outputs for redecoding or rescoring. The hypothesis attention allows deliberation to act like a spelling corrector on full-context first-pass hypotheses. This results in substantial performance gains, especially on rare words \cite{hu2020deliberation, hu2021deliberation}.

Our novel contribution is to combine the text-only training capabilities of JATD with the spelling-correction benefits of deliberation. Our approach, dubbed deliberation-JATD, augments deliberation's attention contexts to use JATD's fixed context vectors, enabling the architecture to train on text-only data. Experiments show deliberation-JATD improving rare word performance by at least 12\% relative to both LAS-JATD and deliberation without any degradation on VoiceSearch tasks.
