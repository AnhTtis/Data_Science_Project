\section{Methods} \label{sec:model}

In this section, we describe our baseline approaches, deliberation and LAS-JATD, as well as the proposed Deliberation-JATD method.

\subsection{Baseline Approaches}

\subsubsection{Deliberation}
\label{sec:methods_delib}


\begin{figure}[h!]
\begin{center}

\centering \scalebox{0.8}{
\input{figs/2pass_ext_color.tex}
}
\end{center}
\caption[]{\label{fig:delib} 2-pass model architecture. With ``Hyp Attn'' block: a deliberation model. Without "Hyp Attn" block: a 2-pass LAS model. Including the JATD block only on the right branch results in the ``partial'' variant of Deliberation-JATD. Including both JATD blocks results in the ``full'' variant.
}
\end{figure}

Deliberation networks (fig. \ref{fig:delib}) consist of a shared encoder, a first-pass RNN-T decoder, and a second-pass deliberation decoder. The shared encoder takes log-mel filterbank energies, $\vx=(\vx_1 \ldots \vx_T)$, where $T$ denotes the number of frames, and generates an encoding, $\ve$. This encoder output, $\ve$, is then fed to an RNN-T decoder to produce first-pass decoding results, $\vy^r$, in a streaming fashion. The deliberation decoder attends to both $\ve$ and $\vy^r$, producing two context vectors, $\vc_e^a$ and $\vc_b^a$, that are concatenated and passed as inputs to an attention-based LSTM decoder. This decoder produces the final probabilities, $\vy^d$, which can be written as $p(\vy^d | \vx, \vc_e^a, \vc_b^a , \vy_{u - 1:1}^d )$, where $\vy_{u - 1:1}^d = {\vy_{u - 1}^d , \ldots , \vy_1^d }$ indicates all previous decoded labels of a single hypothesis during inference.

Inference for deliberation models is done in two passes. First, the RNN-T decoder processes encoder outputs, $\ve$, to produce the first-pass sequence, $\vy^r$. Then, the deliberation decoder attends to $\ve$ and the complete first pass hypotheses, $\vy^r$, and performs a second beam search to generate $\vy^d$. This second pass acts as a spell-corrector, using the full context of the first pass hypothesis to substantially improve performance \cite{hu2020deliberation}.

Deliberation training requires audio-text pairs and does not offer a natural way to incorporate unpaired text data. In the following section, we describe JATD, which addresses this shortfall.
\subsubsection{LAS-JATD
\label{sec:methods_las_jatd}}

JATD was implemented \cite{sainath2020jatd} on a two-pass LAS model running beam search in the second-pass decoder. This two-pass LAS model can be succinctly described as a deliberation model where the second-pass LSTM decoder does not use the the first-pass RNN-T decoder outputs, $\vy^r$, and only attends to the shared encoder outputs, $\ve$. This is equivalent to fig. \ref{fig:delib} with the ``Hyp Attn'' block removed. The final log probabilities output by LAS can be written as $\log{p(\vy_u^d |\vx^a, \vc_e^a , \vy_{u - 1:1}^d )}$.

\begin{figure}[h!]
\begin{center}
\centering \scalebox{0.8}{
\input{figs/JATD.tex}
}
\end{center}
\caption[]{\label{fig:las_jatd} Details of JATD implementation. The ``JATD'' block can be added to the outputs of either attention block in fig. \ref{fig:delib}.
}
\end{figure}

LAS-JATD \cite{sainath2020jatd} augments LAS to enable training on paired audio-text data, as well as unpaired (i.e. text-only) data. This is done through the introduction of a new learnable fixed context vector, $\vc_e^l$, which is used as an alternative to the acoustic context vector $\vc_e^a$ (fig. \ref{fig:las_jatd}). During inference, two log probabilities are produced, one based on $\vc_e^a$ and the other based on $\vc_e^l$. These are interpolated using weight $\lambda$ to produce the final output log probabilities:

\begin{equation}
\label{eq:las_jatd_probs}
\lambda \log{p(\vy_u^d |\vx, \vc_e^a , \vy_{u - 1:1}^d )} + (1 - \lambda)\log{p(\vy_u^d | \vc_e^l , \vy_{u-1:1}^d )}
\end{equation}

The first term in this equation represents the familiar acoustic model (i.e. a regular two-pass LAS). The second term, $\log{p(\vy_u^d | \vc_e^l , \vy_{u-1:1}^d )}$, can be thought of as a language model since it does not depend on acoustic features, $\vx$.

LAS-JATD also provides a framework for incorporating unpaired data into training (eq. \ref{eq:las_jatd_loss}). It uses both acoustic and learnable context vectors when training on both paired and unpaired data. Acoustic context vectors are generated based on real audio, $\vx^a \in \vx$, for paired examples and ``created'' audio, $\vx^l \in \vx$, for unpaired examples. Importantly, it restricts training so only paired examples update the encoder attention parameters and only unpaired examples update the fixed context vector. This avoids biasing acoustic attention parameters towards unpaired data, and was found to be effective in \cite{sainath2020jatd}. In this work, we explore synthesizing audio based on text data to create $\vx^l$.

\begin{equation}
\label{eq:las_jatd_loss}
  \mathcal{L} =
    \begin{cases}
      \lambda \log{p(\vy_u^d |\vx^a, \vc_e^a , \vy_{u - 1:1}^d )} + (1 - \lambda)\log{p(\vy_u^d | \vc_e^l , \vy_{u-1:1}^d )}\\
      \text{if paired example}\\
      \lambda \log{p(\vy_u^d |\vx^l, \vc_e^a , \vy_{u - 1:1}^d )} + (1 - \lambda)\log{p(\vy_u^d | \vc_e^l , \vy_{u-1:1}^d )}\\
      \text{if unpaired example}\\
    \end{cases} 
\end{equation}

LAS-JATD improves performance through the addition of unpaired data to its training set, but misses out on gains from the spell-correcting capabilities of deliberation models.

\subsection{Proposed Method: Deliberation-JATD
\label{sec:methods_delib_jatd}}

We propose the Deliberation-JATD model, combining deliberation's spell-correcting benefits with JATD's ability to train on unpaired data. Similar to LAS-JATD, this model uses fixed context vectors as an alternative to attention context vectors, $c_e^a$ and $c_b^a$. This results in a new set of output log probabilities that act as a language model.

Given that deliberation models contain two attention contexts, we examine two Deliberation-JATD variations. The first, dubbed ``partial JATD'', uses the fixed context vector $\vc_e^l$ as an alternative to the encoder attention context, $\vc_e^a$, while continuing to use first-pass decoder context, $\vc_b^a$. This results in the LM log probabilities, $\log{p(\vy_u^d |\vx, \vc_e^l , \vc_b^a, \vy_{u - 1:1}^d )}$, and the following final model outputs used during inference:

\begin{equation}
\label{eq:deliberation_partial_jatd_probs}
\lambda \log{p(\vy_u^d |\vx, \vc_e^a , \vc_b^a, \vy_{u - 1:1}^d )} + (1 - \lambda)\log{p(\vy_u^d |\vx, \vc_e^l , \vc_b^a, \vy_{u - 1:1}^d )}
\end{equation}

The second variant, named ``full JATD'', goes one step further and adds a second fixed context vector, $\vc_b^l$, as an alternative to its first pass decoder attention context, $\vc_b^a$ (eq. \ref{eq:deliberation_full_jatd_probs}). This means that both attention contexts are replaced and the LM log probabilities having no dependence on the acoustic inputs, $\vx$.

\begin{equation}
\label{eq:deliberation_full_jatd_probs}
\lambda \log{p(\vy_u^d |\vx, \vc_e^a , \vc_b^a, \vy_{u - 1:1}^d )} + (1 - \lambda)\log{p(\vy_u^d | \vc_e^l , \vc_b^l, \vy_{u - 1:1}^d )}
\end{equation}

We use the ``joint'' training strategy \cite{sainath2020jatd} to train both variants on a mix of supervised audio and text-only data paired with TTS audio. The resulting training loss is similar to eq. \ref{eq:las_jatd_loss}, but incorporates eq. \ref{eq:deliberation_partial_jatd_probs} or eq. \ref{eq:deliberation_full_jatd_probs} for the unpaired text term, depending on the model variation used. Similar to LAS-JATD, we distinguish between real audio from paired data, $\vx^a$, and synthetic audio from text data, $\vx^l$, and only backpropagate some parameters for each type of data. Specifically, encoder attention parameters are held constant for synthetic audio and the fixed attention context vectors are held constant for real audio. The second-pass decoder parameters are updated for both types of data.

Full JATD's LM is similar to LAS-JATD's LM in that it replaces all attention contexts (from the encoder and first-pass decoder) with fixed vectors. This means that the LM component's second-pass decoder completely ignores the first-pass decoder and all acoustic information. As a result, the LM learns to spell-correct based purely on previous model outputs, $\vy_{u - 1:1}^d$. In contrast, partial JATD was designed to more frequently expose the second-pass decoder to the first-pass decoder during training. Its LM keeps the first-pass decoder attention context, allowing the second-pass decoder to train with it and the LM to make use of some indirect acoustic information.

\subsection{Training
\label{sec:methods_training}}

All models were initialized from an RNN-T trained with supervised audio-text paired data. The same model was also used as the baseline RNN-T model when analyzing performance. All training sets that included TTS used a mix of 90\% audio-text pairs and 10\% pure text data with corresponding TTS. We will refer to this training set as the ``mixed audio'' training set. For all JATD models, an interpolation weight of $\lambda = 0.1$ was found to work well in training.
