\section{Experiment Details \label{sec:experiments}}


\subsection{Training Sets}

We use the same paired audio-text training set as \cite{Arun19}. This dataset consists of approximately 180M multi-domain utterances spanning domains of search, farfield, telephony and YouTube English utterances. The search and farfield utterances are anonymized and hand-transcribed and are representative of Googleâ€™s voice search traffic.

Our unpaired data consisted of 4.6M samples of query text from anonymized Google Maps traffic. We paired this text with synthesized audio generated by a multi-speaker TTS system based on the architecture described in \cite{peyser2019numeric}. Our synthesized audio is generated in the voice of 98 English speakers covering American, Australian, British and Singaporean accents with a Google Assistant clean speaking style. Each utterance's audio was synthesized using a single voice assigned randomly during training set generation. 

Both real and synthesized audio training data were artificially corrupted using a room simulator. Various degrees of noise and reverberation were added such that the overall SNR is between 0dB and 30dB, with an average SNR of 12dB \cite{Chanwoo17}. The noise sources were from YouTube and noisy environmental recordings.

\subsection{Test Sets}
We use two primary evaluation sets: ``VS'' and ``SXS''. VS consists of ~14k anonymized hand-transcribed utterances from Google traffic. SXS is a sample of 1,200 real-audio utterances where the conventional model \cite{Golan16} outperformed the E2E LAS rescoring model \cite{SainathPang19}. This dataset consists largely of rare words and is useful for measuring how including text-only training data can improve some of these errors.


We used a corpus composed purely of rare word utterances \cite{peyser2020fusion} to specifically evaluate model performance on rare nouns. This corpus is a non-overlapping random subset of the same text data that was sampled to create the unpaired training data. It consists of utterances containing words that (1) occurred either once or not at all in the paired training set and (2) accounted for less than 1 in every million words in the text data. For privacy reasons, we only included words that occurred more than 1000 times in the text data.

We created two evaluation sets derived from this rare word corpus. The ``TTS'' set is a subset of 10,000 of these utterances combined with synthetic audio generated by the same system that synthesized audio for the training set in \cite{peyser2019numeric}. For this test set, the audio synthesizing system was configured to use a voice profile distinct from all those used in the training set. The ``Spoken Text'' evaluation set is a random sample of 200 utterances from the TTS set, but manually spoken and recorded by one of our team members. This set was used to verify that improvements in model performance on the TTS evaluation sets were matched on real audio.

\subsection{Architecture Details 
\label{sec:experiments_modeling}}
We used the same deliberation model configuration as \cite{hu2020deliberation}. All experiments used 128-dimensional log-Mel features, computed with a 32ms window and shifted every 10ms. Similar to \cite{Arun19}, features for each frame are stacked with 3 frames to the left and then downsampled by 3 to a 30ms frame rate.

The first-pass RNN-T network is similar to \cite{SainathPang19}, consisting of an 8 LSTM layer encoder and 2 LSTM layer prediction network. Each LSTM layer has 2,048 hidden units followed by a 640-dimensional projection layer. There is a factor of 2 time-reduction layer after the second encoder LSTM layer. The outputs of encoder and prediction network are fed to a 640 hidden unit joint network followed by a softmax layer predicting 4,096 lowercase wordpieces.

The first-pass RNN-T hypotheses are padded with end-of-sentence label $\langle \backslash \text{s} \rangle$ to a length of 120. Each subword in a hypothesis is mapped to a vector by a 96-dimensional embedding layer and encoded by a 2-layer bidirectional LSTM encoder, where each layer has 2,048 hidden units followed by a 320-dimensional projection. Both attention models use multi-headed attention \cite{Vaswani17} with four attention heads. The two output context vectors are concatenated and fed to a 2-layer LSTM decoder (2,048 hidden units followed by a 640-dimensional projection per layer). The second-pass attention decoder has a 4,096-dimensional softmax layer to predict the same mixed-case wordpieces \cite{schuster2012japanese} as the RNN-T. We use a similar architecture for our LAS models. The second-pass decoder in these models is the same as before (2 LSTM layers, each with 2,048 hidden units followed by a 640-dimensional projection).

The total size of the RNN-T model is 114M parameters, and the second-pass decoder has 33M parameters. All models are trained in Tensorflow \cite{AbadiAgarwalBarhamEtAl15} using the Lingvo \cite{shen2019lingvo} toolkit on a v2-128 Cloud TPU slice with a global batch size of 4,096.

For JATD models, the interpolation weight, $\lambda$, for inference is chosen (between 0.01, 0.025, and 0.05) to optimize WER for the VS test set. Results shown in table \ref{table:results_main} use $\lambda = 0.025$ for Deliberation-JATD and $\lambda = 0.01$ for LAS-JATD.

