\section{Related Works} \label{sec:Related}
\subsection{CTC-ATT Speech Transformer}
\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figs/model.png}
    \caption{CTC-ATT transformer with LR/LRA}
    \label{fig:model}
\end{figure}
The CTC-ATT Transformer-based \cite{nakatani2019improving} block-reusing mechanism could be described in Fig.\ref{fig:model}. The encoder, containing $M$ blocks, maps the sequence of input speech feature $X=\left\{\mathbf{x}_{t} \mid t=1, \cdots, T\right\}$ to a sequence of embedding $H=\left\{\mathbf{s}_{l} \mid l=1, \cdots, L\right\}$. Rather than a vanilla Transformer with a standard single encoder-attention-decoder structure, there are two decoders separately mapping embedding to the character.


The Connectionist Temporal Classification (CTC) decoder will map embedding $\textbf{H}$ into CTC topology sequence $C=\left\{c_{l} \in \mathcal{U} \mid l=1, \cdots, L\right\}$ with a set of distinct character, $\mathcal{U}$. It should be noted this character set not only contains a linguistic unit but also a "blank symbol" unit which explicitly denotes the letter boundary to handle the repetition of character symbols \cite{graves2006connectionist}. Meanwhile, the original attention decoder could simultaneously or later on generate one element at each time consuming on the embedding $H$, this auto-regressive decoder continues complete a sequence of token $Y=\left\{\mathbf{y}_{s} \in \mathcal{U} \mid s=1, \cdots, S\right\}$. These two decoders actually share the same character set $\mathcal{U}$. As a seq2seq model, letters boundary naturally exist within the sequence but the attention decoder should still have the "blank symbol" in its character set for later decoding. 


The multi-objective function of the original CTC-ATT Transformer is implemented in \cite{watanabe2017hybrid} and later on migrate into speech-Transformer for ASR \cite{nakatani2019improving}. Within the training stage of the CTC-ATT Transformer, rather than a data-driven attention training force model to learning alignment ignoring the monotonic speech-text natural properties, the CTC training plays a role as an auxiliary task to speed up the process of estimating the desired alignment. The optimized function of CTC-ATT is a simply logarithmic linear combination of CTC decoder and attention decoder objective:
\begin{equation}
    \mathcal{L}_{\mathrm{CTC-ATT}}=\lambda \log p_{\mathrm{ctc}}(C \mid X)+(1-\lambda) \log p_{\mathrm{att}}^{*}(C \mid X) \label{equa:loss_combine}
\end{equation}

There are several decoding methods for the CTC-ATT speech transformer. With these decoding algorithms, the decoders can generate token sequence $\textbf{C}$ consuming embedding $\textbf{H}$. In the attention decoding algorithm, the attention decoder could generate one element at each time. And this auto-regressive decoder continues a sequence of the token. In CTC greedy search decoding, just greedy searching is the best path in the CTC decoder's mapping. Usually, beam search has a precisely global path search.  Also, CTC prefix beam search merge sequences have the same non-blank token sub-sequence in each time step. The attention rescore decoding is designed for combining the n-best beam logarithmic probability with attention decoder logarithmic probability by text in the beam. The shared character set $\mathcal{U}$ in two decoders could easily linearly combine probability which is the reason attention still keeps the "blank symbol".


In vanilla CTC-ATT Transformer, the encoder, and attention decoders respectively contain $M$ transformer encoder blocks and $N$ transformer decoder blocks. And each block contains exclusive parameters which make the model large. It should be noted parameters get involved in forwarding propagation just once. 


In the BR strategy, within the encoder, each block actually shares the same set of parameters across blocks to make the block have high parameter efficiency. Similar strategies have been reported in \cite{dehghani2018universal}. Based on simple block reusing,  \cite{lan2019albert} also explore only block reusing, it shows that if only one share attention, the performance will drop a little, but only decrease the number of parameters by around one-third. And then \cite{gao2021extremely} explore the possibility migrate its application field from language to speech. It should be noted that \cite{zhao2021non} also points out that the transformer encoder blocks have high similarity. 