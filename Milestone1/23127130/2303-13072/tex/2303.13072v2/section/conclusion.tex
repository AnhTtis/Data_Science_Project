\section{Conclusion} \label{sec:Conclusion}
In this paper, we design a new block reusing method for transformer in E2E ASR system for memory and storage constrained device and enhance it with an adaptor module. It could achieve an extremely high parameters efficiency and decrease the number of parameters to one-third of the original one. This method also could prevent the number of model parameters from growing with the depth growing. The simple block reusing transformer block obtains an acceptable WER in AISHELL1. Then the adaptor module is imported to deliver more diversity to the model's representation which dramatically boosts the model performance. The model is evaluated in the AISHELL1 benchmark, obtaining a 6.63\% CER but only 8.5\%M parameters in the model. For understanding the degradation of WER in simple block reusing, the vertical and horizontal similarity is conducted. It not only shows that pure block reusing would trap the model in local optimization since simple block repeating lacks diversity, but also describe that the pure block reusing will lose non-linear representation as well.

In further work, we would like to focus on the parameter reusing method from block-wise to layer level, deep analysis of the layer in transformer block to realize a higher parameter efficiency, lower CER, and finally evaluate the methods in more scenarios.