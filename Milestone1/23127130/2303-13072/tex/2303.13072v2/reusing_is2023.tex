\documentclass{INTERSPEECH2023}
\usepackage{subcaption}
\usepackage{multirow} 

% 2023-01-06 modified by Simon King (Simon.King@ed.ac.uk)  

% **************************************
% *    DOUBLE-BLIND REVIEW SETTINGS    *
% **************************************
% Comment out \interspeechcameraready when submitting the 
% paper for review.
% If your paper is accepted, uncomment this to produce the
%  'camera ready' version to submit for publication.
\interspeechcameraready 


% **************************************
% *                                    *
% *      STOP !   DO NOT DELETE !      *
% *          READ THIS FIRST           *
% *                                    *
% * This template also includes        *
% * important INSTRUCTIONS that you    *
% * must follow when preparing your    *
% * paper. Read it BEFORE replacing    *
% * the content with your own work.    *
% **************************************

\title{Beyond Universal Transformer: block reusing with adaptor in Transformer for automatic speech recognition}
\name{Haoyu Tang$^1$, Zhaoyi Liu$^2$, Chang Zeng$^3$, Xinfeng Li $^4$}
%The maximum number of authors in the author list is 20. If the number of contributing authors is more than this, they should be listed in a footnote or the acknowledgement section.
\address{
  $^1$Department of Electronic Systems,
NTNU, Trondheim, Norway\\
  $^2$Computer Science, KU Leuven,  Leuven, Belgium $^3$National Institute of Informatics, Tokyo, Japan\\
  $^4$Electrical Engineering, Zhejiang University, Hangzhou, China 
  } 
\email{Charlie\_Tang\_1992@outlook.com, zhaoyi.liu@student.kuleuven.be, zengchang@nii.ac.jp, xinfengli@zju.edu.cn} 


\begin{document}

\maketitle
\begin{abstract}
% 1000 characters. ASCII characters only. No citations.
Transformer-based models have recently made significant achievements in the application of end-to-end (E2E) automatic speech recognition (ASR). It is possible to deploy the E2E ASR system on smart devices with the help of Transformer-based models. While these models 
%These Transformer-based models have enabled the deployment of E2E ASR on smart devices, but they 
still have the disadvantage of requiring a large number of model parameters. To overcome the drawback of universal Transformer models for the application of ASR on edge devices, we propose a solution that can reuse the block in Transformer models for the occasion of the small footprint ASR system, 
% a small footprint ASR new layer reusing solution 
which meets the objective of accommodating resource limitations without compromising recognition accuracy. Specifically, we design a novel block-reusing strategy for speech Transformer (BRST) to enhance the effectiveness of parameters and propose an adapter module (ADM) that can produce a compact and adaptable model with only a few additional trainable parameters accompanying each reusing block. We conducted an experiment with the proposed method on the public AISHELL-1 corpus, and the results show that 
% On the AISHELL-1 test, 
the proposed approach achieves the character  error rate (CER) of 9.3\%/6.63\% with only 7.6M/8.3M parameters without and with the ADM, respectively. In addition, we also make a deeper analysis to show the effect of ADM in the general block-reusing method.
\end{abstract}
\noindent\textbf{Index Terms}: speech recognition, Transformer, adapter module, layer-reusing

\input{section/introduction}
\input{section/related}
\input{section/method}
\input{section/exp}
\input{section/conclusion}

\bibliographystyle{IEEEtran}
\bibliography{mybib}

\end{document}
