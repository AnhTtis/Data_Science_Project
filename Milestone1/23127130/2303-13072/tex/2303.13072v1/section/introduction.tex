\section{Introduction}
There is significant interest in developing automatic speech recognition (ASR) systems on smart devices to meet privacy, security, and network capacity limits. The advancements in E2E ASR systems indicate that such systems are now strong candidates for such deployments. Instead of independently building acoustic, language, and pronunciation models in traditional hybrid ASR systems, the E2E ASR systems integrate these components into a single sequence-to-sequence (seq2seq) model. Thanks to Transformer's \cite{vaswani2017attention} superior performance in processing sequence-related tasks \cite{raffel2020exploring}, Transformer-based models have been widely adopted in various state-of-the-art (SOTA) ASR systems and brought a recent breakthrough in E2E ASR \cite{zhang2020pushing}. 


The advancements described in \cite{raffel2020exploring, gulati2020conformer} provide evidence that a large network is essential for obtaining SOTA performance \cite{ng2021pushing, guo2021recent}. However, the modeling ability of these approaches depends on a large number of parameters. The model with a large number of parameters makes it infeasible for deploying it on devices with limitations in memory and storage.


% The GPU/TPU overhead of the universal Transformer-based models \cite{dehghani2018universal} also grows as their size is scaled up, making it infeasible for deployment in resource-constrained scenarios or smart devices without connectivity and slowing down the training and inference processes.

%However, the computation and GPU/TPU memory costs of the large Transformer-based models scale quadratic ally with sequence length present a challenge for ASR and development in resource-constrained scenarios. For instance, just a 10-second speech equates to around 250 frames with a widely used window stride of 14ms.


Currently, the optimization of E2E ASR system for smart devices has received greater attention, including model parallelization \cite{shao2020pychain}, knowledge distillation \cite{yang2022knowledge}, and low-rank factorization \cite{winata2020lightweight}, etc. Although these efforts have made considerable headway, the limitations of both memory and computing resources still make it challenging to widely deploy an ASR system on smart devices.

In this paper, we propose an extremely low footprint E2E ASR system with the block-reusing strategy inspired by \cite{dehghani2018universal, lan2019albert} for Transformer-based models to more effectively balance the storage and the precision of speech recognition. Specifically, compared with using multiple attention-based blocks in the universal Transformer based-model \cite{dong2018speech}, we adopt a block-reusing strategy, which means the encoder and the decoder in the Transformer possess only one block but pass through it multiple times when in forwarding propagation of both training and inference stages. 
% This strategy is inspired by \cite{dehghani2018universal, lan2019albert}.
% the attention-based encoder and the decoder in the transformer are compressed into just one block model. This one block is repeated multiple times 
This strategy is equivalent to treating each repetition as an independent block in the original baseline, which means using one block with multiple repeats to replace multiple blocks. It could dramatically decrease the model size. This proposed model is named block reusing (BR). But our later block-level analysis shows that this repeating technology same within \cite{dehghani2018universal, lan2019albert} will drop in local optimization points. To jump out of the local optimization, then multiple ADMs are inserted into the module to rectify the performance degradation for BR. This rectification is named as BRA method in this paper. In the later similarity analysis, an interesting founding is reported by us. Pure block reusing method in \cite{dehghani2018universal, lan2019albert} will push the reusing block deteriorating into an almost linear process, which is main reason for performance degradation. And the ADMs could dramatically enhance the non-linearity of the model.

We report extensive experiments on the public AISHELL-1 benchmark \cite{bu2017aishell}. Experimental results show that block-reusing with the ADMs improves parameter efficiency without sacrificing model accuracy. Our proposed LRST method achieves the word error rate (WER) of 9.3\%/6.63\% with only 7.6M/8.3M parameters without and with the ADM, respectively.

The rest of this paper is organized as follows. Related works are discussed in Section ~\ref{sec:Related}. Our proposed method is described in Section~\ref{sec:Method}. Experimental setup, result and discussion appear in Section~\ref{sec:Experiments}. Finally, Section~\ref{sec:Conclusion} provides our conclusions.
