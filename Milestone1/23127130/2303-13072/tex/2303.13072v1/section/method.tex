\section{Method} \label{sec:Method}
In this section, the details of block-reusing will be introduced, to obtain a super low footprint E2E ASR system. How the adaptors in the encoder and attention decoder alleviate deterioration of WER is discussed likewise.

\subsection{Block-reusing Strategy in Transformer} \label{subsec:blockreuse}
In our design, this high parameter efficiency reusing also could be reviewed as a block reusing since actually there is only a block but repeat forward $M$ times. Basically, the total storage and training parameters in the encoder can be theoretically reduced into $1/M$ of the one without weight sharing. The blocks in the attention decoder also could share parameters across blocks and reduce the parameters to $1/N$. 

\subsection{Block-reusing Strategy with adapter module}
The function of the original CTC-ATT transformer encoder could be stated as:
\begin{equation}
\label{equa:encoder_compose1}
    H = f(X) = f_{M}(f_{M-1}(f_{M-2}(......f_{1}(X))) 
\end{equation}
In the equation, the $f_{m}$ represents the $m$-th block of the encoder, so the $f$ could represent the whole encoder.


As declared in the \cite{zhao2021non}, there is enormous similarity within the blocks in the encoder. And \cite{houlsby2019parameter} explores the adaptors in language transformer finetune. Inspired by the block similarity and adaptor, block function could be decomposed as a combination of public function and a unique function, and described as follows:
\begin{equation}
    f_m(x) = f_{m'}(f_{0}(x)) \label{equa:block_compose1}
\end{equation}
Within the equation, the $f_{0}$ is the public function for all blocks, and $f_{m'}$ is the ADM function of block $m$. 


If the $f_{m'}$ supposed as identification function, which means the all the block function is same, the function \ref{equa:block_compose1} can be view as:
\begin{equation}
    f_m(x) = f_{0}(x) \label{equa:block_compose2}
\end{equation}
Then replace the $M$ blocks with one block but $S_1$ is repeated. The equation \ref{equa:encoder_compose1} degenerate:
\begin{equation}
    H = f(X) = \underbrace{f_{0}(f_{0}(f_{0}(......f_{0}(X)))}_{S_1\ times} \label{equa:encoder_compose2}
\end{equation}
It is clear that equation \ref{equa:encoder_compose2} basically is BR described in subsection \ref{subsec:blockreuse}. Nevertheless, it is based on the assumption all the blocks are the same. Without this assumption, the blocks' function are still viewed as a similar function, the the equation \ref{equa:encoder_compose1}\ would be combined with equation \ref{equa:block_compose1}, then the whole encoder equation \ref{equa:encoder_compose2}  mapping is rewrite as:
% \begin{equation}
%     \begin{aligned} 
%         H = f(X) &= f_{M'}(f_{0}( \\
%         & f_{(M-1)'}(f_{0}(\\
%         & f_{(M-2)'}(f_{0}(...... \\
%         & f_{1'}(f_{0}(x)))))))
%         \label{equa:encoder_compose3}
%     \end{aligned}
% \end{equation}
\begin{equation}
        H = f(X) = f_{M'}(f_{0}( f_{(M-1)'}(f_{0}(f_{1'}(f_{0}(x))))
        \label{equa:encoder_compose3}
\end{equation}
Based on the equation \ref{equa:encoder_compose3}, the ADM will be inserted as function $f_{M'}$. As mentioned, the $f_{0}$ is the reused encoder block. So this function actually is the mathematical representation of the block reusing adaptor module (BRA). Meanwhile, ADM function could be proposed in the attention decoder as well with a reused encoder block with repeated $S_2$ times.