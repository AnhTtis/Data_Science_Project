\section{Experiments} \label{sec:Experiments}
\subsection{Experiments Setup}
There is speech recognition tasks for the evaluation proposed method, AISHELL-1 \cite{bu2017aishell}. 


The acoustics feature used in all experiments is 80-dimensional log-Mel filter bank (fbank) energies computed with 25ms windows width and 10ms windows shift. The characters set $\mathcal{U}$ in both datasets are Chinese characters simply generated by statistics all Chinese characters in its corresponding train text file. The baseline CTC-ATT model is composed of a 12 blocks encoder and 6 blocks decoder. Each block in encoder and attention and decoder outsize all are 256, with 4 heads in multi-head attention. And its corresponding BR(A) are showed in the table \ref{table:Model_hyper_parameters}. The BR/BRA models only contain 1 encoder block but are repeated 12 times and 1 decoder block is repeated 6 times shown in the table \ref{table:Model_hyper_parameters}.\\
\begin{table}[t!]
    \centering
    \caption{Model hyper-parameters}
    \begin{tabular}{|c|c|c|c|c|} \hline
         \textbf{Model} & \textbf{$M$} & \textbf{$N$} & \textbf{$S_{1}$} & \textbf{$S_2$} \\ \hline
         baseline & 12 & 6 & 1 & 1 \\ \hline
         BR(A) & 1 & 1 & 12 & 6 \\ \hline 
    \end{tabular}
    \label{table:Model_hyper_parameters}
\end{table}
In the training phase, the $\lambda$ in linear loss combine equation \ref{equa:loss_combine} is set as 0.3. The optimizer of the model is a warm-up Adam optimizer with a learning rate of 0.002 and 25,000 warm-up steps. The original waveform is augmented with speed-perturb with ratio 0.9 and 1.1 during training phase. Meanwhile, the train feature fbank is pre-processed by specaugment \cite{park2019specaugment, park2020specaugment}. There are two frequency-dimension masks and two time-dimension masks. The maximum mask in time and frequency respectively are 50 and 10 bins. 

\subsection{AISHELL Result}
\input{section/AISHELL_wer_table.tex}
After challenge, since the test set is not public, so we move to a public Mandarin dataset for further development and explore. Initially, we can implemented a BR model its size still just 1/3 of baseline showing in table \ref{table: CER_AISELL_test}, but CER increase much more from baseline if compared with last dataset. At this point, the ADM method is introduced. First of all, we found that the CER increasing at CTC greedy and CTC prefix is much more than others. Compared with these two path, encoder $\xrightarrow{}$ CTC decoder, and encoder $\rightarrow{}$ attention decoder, two path actually share a same encoder. Since CTC decoder is just a small linear output later, there are much more bias in encoder actually, but attention decoder rectify some of them. 

At next step, the ADM is implemented as a simple linear+ReLu and inserted into the encoder. This experiment named as "BRA-E" in table \ref{table: CER_AISELL_test}. Of course this insertion slightly gain the number of parameters. It is clear that it basically relieved the CER increasing in all decoding method since it push encoder to have a better feature extraction. Especially in attention rescore CER decrease around 26.3\% percent CER from BR experiment. Then the linear+ReLu ADM is inserted not only encoder and also attention decoder. In additional, there are two more experiments, "BRA-ED" and "BRA-D". Literally, the experiments with encoder and decoder both inserted ADM named as "BRA-ED" and with only decoder inserted ADM named as "BRA-D". In the "BRA-ED" have all most same experiment result with "BRA-E" which means the ADM in decoder don't have enough help. Within the "BRA-D", it is clear the ADM can not help if it only rectify at shallow block.


For verifying the gradient clip tuning, we also reduced gradient clip from 5 to 3 as well. Unfortunately, gradient clip experiment in "BRA-E" and "BRA-ED" don't evident improvement in this noiseless dataset. Inspired by \cite{gao2021extremely, lan2019albert}, the $S_1$ also is increase from 12 to 18 to explore the further improvement. Unfortunately, its CER also almost keeps same. But compared with the similar setup Exp6 ($M=N=1$) in \cite{gao2021extremely}, our proposed method "BRA-E" almost keeps the similar CER but just 65.4\% the number of parameters in model. 

\subsection{Similarity Analysis}
For analysis the mapping processing of ADM, we extract $m$th block's embedding in baseline and $m$th repeated reusing block embedding in BR/BRA to compare their similarity with AISHELL testset. The similarity metrics is linear centered kernel alignment (CKA) introduced in \cite{kornblith2019similarity} for compare representation in deep learning model. 
\begin{figure}[t!]
    \centering
    \begin{subfigure}[b]{0.45\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figs/Encoder_horizontal_similarity.pdf}
         \caption{Encoder} \label{subfig:CKA_horizontal_encoder}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.45\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figs/Decoder_horizontal_similarity.pdf}
         \caption{Decoder} \label{subfig:CKA_horizontal_decoder}
    \end{subfigure}
    \caption{Models horizontal similarity in AISHELL test}
    \label{fig:block_horizontal_CKA}
\end{figure}
Except the each proposed method models' embedding are compared with baseline model, BR and BRA-E models' embedding are contrasted. The compared metrics are showed in the figure \ref{fig:block_horizontal_CKA}. The horizontal axes "enc/dec-i"show the the $m$th similarity as mention in encoder or decoder. The different lines with colors show model comparison pair, more specifically, the dotted lines simple show similarity comparison between from $m$th block's embedding with $m$th repeating's embedding. 


More specifically the solid lines tailed with "after ADM" means similarity comparison of the $i$th block's embedding in baseline with $i$th ADM embedding. In the similarity analysis, define a AMD push-away number, as the difference between a solid line with its corresponding dotted line in each block. A large number of push-away number shows the ADMs indeed push repeating block's representation away from its original distributing. The ADMs indeed enhance the diversity of embedding for repeated blocks. Also it could be explained as the $f_{m'}$ equation \ref{equa:block_compose1} have a strong non-linear representation ability.

In encoder, the baseline - BR/BRA-D lines almost follows a same trend, slowly decreasing with model getting deeper, also reported in \cite{lan2019albert}. Within new introduced ADM, BRA-E(D) transformer block' similarity lines also drops in deeper blocks with a higher slope. 


Intuitively, if a lite model compressed from a large model, it will be generally think as its represent should as close as to original represent for each compressed part. This hypothesis could be demonstrate in the Figure \ref{fig:block_horizontal_CKA} as, the solid lines always have a much more higher value with its correspond dotted lines. But the Figures \ref{subfig:CKA_horizontal_encoder} shows that the BRA-E(D) models have a much bigger ADMs push-away number as the hypothesis's expectation, but actually the dotted line have a much more strong similarity with baseline. ADMs do enhance the diversity for each repeating's representation, but it do not make distribution close to baseline. However, even BRA-E(D) have a mush have different distribution from baseline with BR model, these model achieve a lower CER in the end. In the other hand, it could be reviewed as the ADMs push models out of local optimization points. 


In decoder, the baseline - BR/BRA-E lines drops slowly with deeper block as well, similar with corresponding BR/BRA-D in encoder. The funny things is, the AMD push-away number in BRA-(E)D actually is quite smell, which means the ADMs in decoder cannot enhance the diversity. That is might the reason there is improvement from BRA-E $\rightarrow{}$ BRA-D
 and BR $\rightarrow{}$ BRA-D experiments.
 

And this BR - BRA-E difference could be cross-valid in the green "linear CKA | BR- BRAE" for both encoder and decoder as well.

\begin{figure}[t!]
    \centering
    \begin{subfigure}[b]{0.45\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figs/Encoder_vertical_similarity.pdf}
         \caption{Encoder} \label{subfig:CKA_vertical_encoder}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.45\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figs/Decoder_vertical_similarity.pdf}
         \caption{Decoder} \label{subfig:CKA_vertical_decoder}
    \end{subfigure}
    \caption{Models vertical similarity in AISHELL test}
    \label{fig:block_vertical_CKA}
\end{figure}

Besides of horizontal similarity, we also want to know how each repeating and ADMs models. So the input and output of each block are compared shown in the figure \ref{fig:block_vertical_CKA} named with model vertical similarity analysis with same metrics. The transformer block process similarity is shown in the figure as dotted lines, and ADMs process similarity is shown as solid lines similar to figure \ref{fig:block_horizontal_CKA}. It should be pointed out that if there is a strong similarity between input and output of a block, which means input and output have strong linear similarity, and this block could be viewed as an approximate linear transformation.


In the encoder, it is clear that all the first blocks except BRA-E(D) transformer block have a strong non-linear. And then all lines keep a near 1 horizontal line, which means the actually non-linear process is mainly finished in the first block in all models. The reason the BRA-E(D) transformer first transformer block keeps strong linearity is the first AMD plays the role for it. It also could be a very important idea of why the BR model degrades so much from checkpoint, simple change blocks into one block repeating will makes this block the trade-off between in each repeating role. It will make this block deteriorate and lose non-linear modeling called \textbf{linear deterioration}. This discovery could explain the model degradation in the \cite{dehghani2018universal, lan2019albert, gao2021extremely} as well.

In the decoder, similar result within encoder, BR still shows linear deterioration, and just replace BRA-E(D) with BRA-(E)D. The biggest difference is the AMDs' in BRA-D show alternate linear and non-linear.

In the end, all ADMs ever in encoder and decoder show a stronger non-linearity than transformer block repeating.

\begin{table}[t!]
    \centering
    \caption{BRA initialized from BR}
    $^*$ means initialized from BR \\
    \begin{tabular}{|c|cccc|} \hline
        \multirow{2}{*}{Exp} & \multicolumn{4}{c|}{CER (\%)} \\ \cline{2-5} 
        & \multicolumn{1}{c|}{CG} & \multicolumn{1}{c|}{CP} & \multicolumn{1}{c|}{ATT} & ATT-RE \\ \hline
        BRA-E$^*$& \multicolumn{1}{c|}{31.36} & \multicolumn{1}{c|}{31.1}& \multicolumn{1}{c|}{26.9} & 33.01 \\ \hline
        BRA-ED$^*$& \multicolumn{1}{c|}{32.01} & \multicolumn{1}{c|}{31.69} & \multicolumn{1}{c|}{33.82} & 27.63 \\ \hline
    \end{tabular}
    \label{table: BRA_finetune_from_BR}
\end{table}

For valid similarity repeating blocks from BR and BRA, we train a BRA-E and BRA-ED model initialized with BR checkpoint showing in table \ref{table: BRA_finetune_from_BR}. It clearly shows there is much degrade in BR initialized BRA-E and BRA-ED models. This degrade cross verifies that actually the BRA's representation is much different from BR's representation.
