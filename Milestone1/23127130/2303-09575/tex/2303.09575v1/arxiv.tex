\documentclass[12pt]{article}

\usepackage{url}
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage{natbib}
\usepackage{amsfonts}
\usepackage{hyperref}
\usepackage{caption}
\usepackage{authblk}

\def\tblhead#1{\hline\\[-9pt]#1\\\hline\\[-9.75pt]}
\def\lastline{\hline}
\providecommand{\keywords}[1]{\textbf{\textit{Keywords:}} #1}
\setcounter{Maxaffil}{0}
\renewcommand\Affilfont{\itshape\small}

\begin{document}

% Title of paper
\title{Sample size determination via learning-type curves}

\author[$\dag$1]{Alimu Dayimu\thanks{Corresponding author Alimu Dayimu ad938@medschl.cam.ac.uk}}
\author[2]{Nikola Simidjievski\thanks{These authors contributed equally to this work}}
\author[3,4]{Nikolaos Demiris}
\author[4]{Jean Abraham}
\affil[1]{Cambridge Clinical Trials Unit, Department of Oncology, University of Cambridge, Cambridge, UK}
\affil[2]{Department of Computer Science and Technology, University of Cambridge, Cambridge, UK}
\affil[3]{Department of Statistics, Athens University of Economics and Business, Athens, Greece}
\affil[4]{Cambridge Clinical Trials Unit, Cambridge University Hospitals NHS Foundation Trust, Cambridge, UK}
\affil[5]{Cambridge Breast Unit, Cambridge University Hospitals NHS Foundation Trust, Addenbrooke's Hospital, Cambridge, UK}

\maketitle
% Add a footnote for the corresponding author if one has been
% identified in the author list


\begin{abstract}
{This paper is concerned with sample size determination methodology for prediction models. We propose combining the individual calculations via a learning-type curve. We suggest two distinct ways of doing so, a deterministic skeleton of a learning curve and a Gaussian process centred upon its deterministic counterpart. We employ several learning algorithms for modelling the primary endpoint and distinct measures for trial efficacy. We find that the performance may vary with the sample size, but borrowing information across sample size universally improves the performance of such calculations. The Gaussian process-based learning curve appears more robust and statistically efficient, while computational efficiency is comparable. We suggest that anchoring against historical evidence when extrapolating sample sizes should be adopted when such data are available. The methods are illustrated on binary and survival endpoints.
}
\end{abstract}
\keywords{Sample size estimation; Learning Curve; Gaussian process; Statistical design; Extrapolation.}

\section{Introduction}
\label{sec1}


Risk prediction models are routinely used in healthcare and medical research \citep{qrisk2,altman2007prognostic} to inform the diagnosis and/or prognosis of clinical events~\citep{steyerber2019, hemingway2013prognosis}. Constructing risk prediction models relies on different modelling approaches, ranging from well-established statistical methods to more recent machine learning algorithms. However, irrespective of the underlying modelling methodology, leveraging data with an appropriate sample size for developing and validating such models is imperative for achieving robust and accurate predictive performance on a given task, such as predicting binary, continuous or time-to-event outcomes. Therefore, accurate sample size calculations are necessary for the development phase, facilitating reliable and accurate prediction models.

Approaches to sample size calculation can differ based on the predictive task and modelling strategy as well as the underlying study design. In this paper, we focus on risk prediction models that address binary and time-to-event outcomes, but the techniques we develop are also directly applicable to continuous outcomes. This problem is distinct from sample size calculation where one is interested in estimating the accuracy of a diagnostic test since that target accuracy may be independent of the sample size. In contrast, when developing prediction models, performance may vary with sample size, possibly due to including covariates that may improve predicting performance as the sample size increases. 

A typical approach to determining an adequate sample size is factoring the number of predictor variables, such as ensuring at least ten events per predictor~\citep{moons2014critical}. Whilst simple, such criteria do not consider the predictors' type, magnitude and possible values (e.g. categorical variables may require more events) -- often leading to poorly fitted models that fail to generalise well to out-of-sample data~\citep{vansmeden2019, van2016no}. In response, recent simulation studies~\citep{vansmeden2019} point to additional and necessary requirements to inform the sample size estimation, which relate to the choice of the modelling strategy and their expected out-of-sample performance. \citet{riley2019minimum} incorporate the expected model performance, number of (candidate) predictors, and the outcome prevalence in the target population into the sample size calculation. 

In a clinical research setting, the statistician is typically interested in calculating the optimal sample size associated with the study design before the data collection stage. The performance of a prediction model varies depending on the modelling strategy and variables included in the model, such as relationships between predictors and response variable(s). Leveraging information from external data will inform the development of a modelling strategy and ultimately improve the predictive performance of the models. %The utility of these approaches is unclear in the presence of available study data, especially in small sample scenarios.

Our study focuses on estimating the predictive performance of a modelling strategy at the design stage, possibly utilising external data. Specifically, we are concerned with selecting a strategy and the associated sample size for developing a prediction model as well as assessing the feasibility of an ongoing study after a certain number of samples has been accrued. The technique used to develop a risk prediction model may differ depending on the method used and the variables included. For example, prediction effects and collinearity issues may affect the downstream performance of the resulting prediction model. To better assess the predictive operating characteristics of a strategy, we propose a learning curve approach that provides predictive performance estimates at different sample sizes and can leverage historical evidence.

A learning curve is often based on an inverse power law model that describes the performance of a modelling strategy as a function of the training sample size \citep{mukherjee2003,viering2021shape} used for developing a prediction model. Related work in different scenarios \citep{Figueroa2012,Beleites2013, christodoulou2021adaptive} estimate the expected performance of a linear classifier for different sample sizes. However, prediction models can be highly unstable, particularly when conditioned upon small sample sizes used at the modelling stage. Therefore, leveraging information from publicly available data from related studies to inform the operating characteristics of a strategy's performance for substantially larger sample sizes can provide additional benefits. In particular, during the study planning stage, this may help determine the optimal sample size needed to achieve a desired accuracy in a robust manner.

In this work, we study learning curve-based methods to estimate the expected predictive performance of a modelling strategy. We propose and evaluate approaches that utilise external data from other (similar) studies, leading to robust estimates for a given sample size. Specifically, we employ a bootstrap strategy on external data to derive model performance estimates at different (incremental) sample sizes. Using these estimates as data points, we learn the parameters of a learning-curve model, which in turn is applied for estimating and extrapolating the expected performance of the underlying model. We propose a frequentist and a Bayesian approach for constructing such learning curves based on four different statistical learning methods. We evaluate the proposed approaches on a series of real-world experiments, predicting binary and time-to-event outcomes, assessing their behaviour over the sample size. We show that in scenarios with limited available data, extrapolation may be unstable and incorporating external data using learning curves as an evidence synthesis tool can be highly beneficial at the design stage. The paper is structured as follows. Section 2 discusses the framework developed for estimating a learning curve that predicts sample size. In Section 3, we apply this method to publicly available data with binary and survival outcomes and Section 4 concludes with a discussion.


\section{Methods}
\label{sec2}

We introduce a learning curve meta-modelling framework to estimate the expected predictive performance of risk prediction models (and associated strategies) conditional on sample size. Specifically, we fit a learning curve using estimates of predictive performance derived from different modelling strategies over bootstraps of incremental sample sizes. The utility of this approach is beneficial in the designing stage to select the modelling strategy and the corresponding sample size or for an ongoing study. To achieve more stable performance, we leverage information from external data and anchor against those for extrapolating to larger sample sizes. The main elements of our proposed framework are presented below.

\subsection{Learning curves}
\label{learncurve}

Learning curves model the relationship between the predictive performance (PP), $Y_{pp}$, of a prediction model as a function of the sample size $n$ used for constructing it \citep{cortes1993learning}. Here we employ a \emph{regular} learning curve whose expected predictive performance is non-decreasing with sample size: $\mathbb{E} \left[ Y_{pp}(n+1) \right] \geq \mathbb{E} \left[ Y_{pp}(n) \right]$. While there are several curve types that can be utilised for modelling such behaviour, in this paper we focus on the power law model since it typically provides the best fit \citep{guModelling,brumen2014} while retaining a natural interpretation of its parameters. The functional form is given by:
\begin{equation} \label{eq:ln}
Y_{pp}(n)=f(n;a,b,c)=(1-a)-b n^{-c}
\end{equation}
where the parameter $a$ denotes the minimum achievable error (ranging from 0 to 1), $b$ denotes the learning rate and $c$ the decay rate with $c \in (0,1)$. For the predictive tasks considered in this paper $Y_{pp}$ is bounded in $[0,1]$ and the maximum is achieved at $(1-a)$. The estimated parameters generally depend on the data, the sampling method, the predictive tasks, and the modelling approach.

An essential component of developing risk prediction models is the choice of appropriate evaluation criteria that assess the predictive and/or discriminative ability of the developed models for clinical decision support/making. Commonly used metrics include $R^2$, the Brier score and the Area Under the Receiver Operating Characteristics Curve (AUC) among others. In this study, we measure the predictive performance $Y_{pp}$ using the C-statistic, commonly used in clinical studies, to evaluate the accuracy of risk prediction models \citep{tripod}. For binary outcomes, it is a proxy to the Area Under the Receiver Operating Characteristic Curve (AUC) \citep{bamber1975area}. In contrast, for time-to-event outcomes, we use the censoring-adjusted C-statistic \citep{unoC2011} as a discrimination measure.

Another major component in the models' development pipeline is the underlying modelling methodology used for constructing the predictive model, and as such crucial for its generalisability and downstream predictive performance. Risk prediction models typically build on well-established approaches from statistical modelling and machine learning such as: (group) lasso \citep{lasso,grouplasso}, support vector machines \citep{vapnik} as well as ensemble methods \citep{breimanRF, friedmanGBM}, which achieve accurate predictive performance by combining multiple predictors. However, the choice of an appropriate modelling methodology can rely on many different factors that relate to the study design, dataset properties and the predictive task at hand.

\subsection{Algorithm}

For a given predictive model, we seek to describe and estimate its predictive performance ($Y_{pp}$) as a function of the sample size used for developing it. Fitting a learning curve of a model's predictive performance as a function of sample size requires a series of performance estimates derived under different, but smaller, samples $n_i$ with sizes $m=\{s_1,s_2,\dots,s_m\}$, sampled from the total sample size $N$, where $i\in\{1,2,\dots,m\}$. The sample sizes $n_i$ are sampled randomly and vary in size, increasing from samples with sizes as small as $s=50$ to a maximum of samples with a total size $s=N$. In this paper, we sample $m=50$ such samples, which we then use for training and evaluating predictive models and, in turn, fitting the learning curve using the $m=50$ performance estimates. Note that, we perform $k$ repeats for every $n_i$ sampling from the set $m$. Specifically, for each $n_{ik}$ sample we estimate a set of model performance estimates $Y_{{pp}_{ik}}$, where $n_i$ denotes a particular sample with size $s_i$ from the set $m$ and $k$ denotes the bootstrap repeats performed within each $n_i$, $k \in \{1, 2, \dots, 100\}$. We develop and validate the modelling strategy via a stratified split of each $n_{ik}$ into $70\%$ training and $30\%$ validation data, the latter being used to obtain $Y_{{pp}_{ik}}$. Specifically, the procedure is as follows:


\begin{enumerate}
    \item Select the number of sample sizes $m$ and fit a learning curve.
    \item For each size $s_i$ of the set $m$:
    \begin{enumerate} 
    \item Randomly sample $n_{ik}$ of size $s_i$ from the complete data of size $N$ without replacement.
    \item Perform a randomly stratified 70/30 split of $n_{ik}$ into $D_{train}$ for training and $D_{test}$ for validating the modelling strategy, respectively.
    \item Fit a model with pre-specified modelling strategy using the training set $D_{train}$. 
    \item Validate the fitted model in the validation sample $D_{test}$ and calculate the predictive performance $Y_{{pp}_{ik}}$ on $D_{test}$.
    \item Repeat the previous steps for $k=100$ times for every size $s_i$ 
    \end{enumerate} 
   	\item Fit a learning curve to using the obtained $Y_{{pp}_{mk}}$ estimates.
\end{enumerate}

In this work, we employ two distinct but related approaches for estimating a learning curve: (i) a frequentist approach fitted via Nonlinear Least Squares (NLS), and (ii) a Bayesian approach where the learning curve is modelled as a Gaussian Processes (GP). The former is implemented using the Levenberg-Marquardt algorithm \citep{more1978levenberg} for weighted NLS to fit the learning curve given in \ref{eq:ln} using the normalised weights $n_i/N$. The $a$ and $c$ parameters are $logit$ transformed for stability.

While superficially there are similarities between our work and the methods of \citep{Figueroa2012} and \citep{Beleites2013} who also use NLS to construct a deterministic learning curve, our approach facilitates several additional methodological features mostly related to the Gaussian process and anchoring against external data. Extrapolating beyond the observed range of the data can be dangerous and should be done with caution. Anchoring upon an external study on similar data allows for robust extrapolation to large sample sizes in scenarios with limited data. Assuming a similar shape but a different scale for the learning curves of those related data suggests borrowing information on the learning rate $b$ and the decay rate $c$ from the external data while learning the increment on $a$ from the target data.

\subsection{Gaussian processes}

The Bayesian approach is based upon a Gaussian processes assuming the model performance is distributed as:

\begin{equation} \label{eq:gp}
Y_{pp}(n) \sim \mathcal{N}(f(n), \sigma_y^2)
\end{equation}
where $\sigma_y$ denotes the standard deviation and $f(n)$ is a Gaussian process $$f(n) \sim \mathcal{ GP }(\mu(n),\Sigma)$$ with mean $\mu(n) = (1-a)-b n^{-c}$, and covariance matrix $\Sigma$. We consider $\Sigma_{ij} = \phi^2\exp{( -\rho |n_i - n_j|^2)}$ and complete the model with \ref{eq:gp}. We place beta priors on $a$ and $c$ and a normal distribution to $b$. Note that, since $Y_{pp} \in [0, 1]$ we could have transformed $Y_{pp}$ to $\mathbb{R}$, but we found this to be unnecessary in our application.

The Bayesian approach naturally facilitates the synthesis of different sources of evidence. In our case this may involve two such processes, (i) borrowing information over sample sizes for accurately estimating the learning curve and (ii) anchoring on external data for robust extrapolation to larger sample sizes. For the latter we use the posterior of $b$, $c$ and $\phi$ estimated from external data as prior when fitting the learning curve for the target data. The predicted performance of $Y_{pp}$ at a sample $n_i$ is used to predict the accuracy of the chosen modelling strategy at a larger sample size for the target data. Given the marginal estimates of $\mu_p$ at $n_p$ and $\mu_o$ at $n_o$, the predicted value $y_p$ is given by:
$$
y_p|y_o\sim \mathcal{N}(\mu_p+\Sigma_{po}\Sigma_{oo}^{-1}(y_o-\mu_o),\Sigma_{pp}-\Sigma_{po}\Sigma_{oo}^{-1}\Sigma_{op})
$$
where $y_o$ denotes the observed performance and $\Sigma$ the covariance matrix. In contrast to NLS, using the GP means that the uncertainty around the predicted value can naturally be obtained via the covariance matrix.


\section{Experimental Design}
\label{sec3}
Our application focuses on two different clinical settings: 1) Estimate the sample size needed at the study design stage. This involves choosing a modelling strategy and determining the associated sample size. 2) For an ongoing study where limited samples have been accrued, assess the feasibility of reaching a particular performance. We argue that individual data from a similar study may be used in these situation. We illustrate out methodology on two different data sets. 

\subsection{Breast cancer examples}
Consider a common scenario in clinical trials where the investigator wishes to plan a study where different modelling strategies will be investigated in terms of their predictive ability. Here we consider predicting breast cancer outcomes and use the METABRIC study \citep{metabric2012a} as external data. For our limited (target) data we use a subset of MSK data \citep{msk2018} and extrapolate to large sample sizes using the METABRIC data. The clinical and genomics data of the METABRIC and MSK data were obtained from cBioPortal \citep{cerami2012cbio, gao2013integrative}. Both binary (5 years survival status) and time-to-event (overall survival) outcomes are considered. 

The variables reported by \citet{margolin2013systematic} were included in the models. For METABRIC they include age at diagnosis, tumour size, number of positive lymph node, tumour grade, ER status, PR status, HER2 status, radio therapy  received and hormone therapy received. The MSK clinical variables contain age at diagnosis, tumour stage, ER status, HER2 status and PR status. The METABRIC data had 1978 participants after removing missing observations. A total of 427 (21.6\%) death events were observed in the first 5 years and 1144 (57.8\%) in the complete follow-up period giving with a median survival of 13 years. The MSK data has a total 1640 participants after removing missing observations, 175 (10.7\%) died within 5 years and 343 (20.9\%) in total with median survival 14.8 years. Accuracy was evaluated by the C-statistics, AUC for binary outcome and Uno's C at 10 years for time-to-event outcomes. 

\subsection{Learning curve by sample size}
We used the METABRIC data to explore the baseline behaviour of the learning curve over different total sample sizes $N\in\{100, 150, 300, 450, 600, 900, 1200, 1500, 1978\}$. We used logistic regression and included the clinical variables and a total of $m=50$ data points with $s_1=30$ being the smallest sample size of $n_1$. Both NLS and GP were used to fit a learning curve. To further investigate the impact of number of data points on fitting learning curve, a series of $m\in\{5, 10, 20, 30, 40, 50\}$ were explored at different total sample sizes $N\in\{100, 300, 600\}$ with METABRIC data with the same modelling strategy described above.

\subsection{Leveraging multi-modal data and models}
In clinical research it is vital to select the modelling strategy that achieves best performance with as small a sample size as possible. This performance may be improved by integrating different data modalities collected on the same patients. We explore the accuracy of different models in predicting the METABRIC 5 year survival status using (1) clinical data only and (2) clinical data plus copy number alteration (CNA) variables. The clinical variables included in the prediction model were outlined in 3.1. In this setting we used all the METABRIC data and a total of $m=50$ data points with the smallest sample size $s_1=30$ of the sample $n_1$.

We evaluate prediction models developed using four different approaches: (1) Elastic net (ElNet)~\citep{ellasticnet} (2) Support vector machines (SVMs)~\citep{vapnik} (3) Random Forests (RF)~\citep{breimanRF} and (4) Gradient Boosted Trees (LGBM)~\citep{friedmanGBM,NIPS2017_LightGBM}. All models, from each of the four methods, have their hyper-parameters optimised and are calibrated. For the hyper-parameter optimisation procedure we use Bayesian optimisation search. Specifically, before performing $k=100$ training repetitions, we perform one extra repetition at each sample size, which we use for optimising the hyper-parameters of the given model. We use the resulting hyper-parameter values for the subsequent repetitions. Each model, at every repeat, is calibrated based on Platt's logistic model~\citep{Platt99} using internal validation. This is largely redundant for logistic regression but we retain the same approach for consistency.

\subsection{Sample size extrapolation}
We consider a scenario where limited data are available and a feasibility analysis is planned where the study may terminate early or recruit more patients if the performance of the prediction model seems appropriate. It is hard to fit a robust model with a small sample size so we may borrow information from external data as described above. We illustrate this task using a subset of the MSK study as target data and information from METABRIC as an anchor for extrapolation. Logistic regression and the Cox model were selected as prediction models using the available clinical variables. We assume an interim analysis to evaluate the feasibility of the proposed modelling strategy will be performed after limited samples become available from the MSK study. We choose $m=20$ data points and $n_1=30$ as the smallest sample size, assuming a total of $N\in\{50, 80, 100, 150, 300, 450\}$ were available from the MSK study. 

\section{Results}
\label{sec4}
Figure \ref{fig:fig1} presents the two (NLS and GP) C-statistic learning curves as a function of sample size. Both GP and NLS work well with most predicted C-statistics falling within the 95\% CIs across sample sizes. The GP model appears more robust, especially for sample sizes larger than 100 while the NLS method stabilises after 300 sample size. The shape of both learning curves was similar and the improvement in C-statistics was immaterial for sample sizes higher than 1000. Table \ref{tab:table1} presents the point estimates and standard error of the coefficients of both learning curves. The standard errors are higher for the NLS method and the point estimates stabilise at larger sample sizes suggesting that the GP is statistically more efficient. In this settings, a total of $m=50$ data points were used. Table S1 and Figure S1 of the Supplementary material reveals that the proposed method is reasonably robust to smaller sample sizes such as $m=5$ for example, with the GP method being more efficient.

\begin{center}
  \includegraphics[width=1\linewidth]{img/fig1.pdf}
  \captionof{figure}{Predicted C-statistics of learning curve of METABRIC with GP and NLS of logistic regression at different total sample size (lines) and 95\% confidence interval of raw bootstrap results using all samples (shaded area)}
  \label{fig:fig1}
\end{center}

\begin{center}
  \captionof{table}{Point estimation and standard error (SE) of GP and NLS at different total sample size
  \label{tab:table1}}
  \footnotesize
  \begin{tabular}{@{}lccccccc@{}}
  \tblhead{
  &\multicolumn{3}{c}{\textbf{Gaussian Process}} && \multicolumn{3}{c}{\textbf{Nonlinear Least Squares}} \\ \cline{2-4}\cline{6-8}
  \textbf{N} & \textbf{a (SE)}  & \textbf{b (SE)}  & \textbf{c (SE)} && \textbf{a (SE)}  & \textbf{b (SE)}  & \textbf{c (SE)}}
100 & 0.25 (0.013) & 2.07 (0.387) & 0.67 (0.050) && 0.28 (0.724) & 2.58 (11.838) & 0.82 (10.137) \\ 
  150 & 0.21 (0.023) & 1.48 (0.267) & 0.52 (0.068) && 0.00 (850735.729) & 0.82 (0.292) & 0.19 (3.085) \\ 
  300 & 0.20 (0.009) & 1.16 (0.107) & 0.45 (0.031) && 0.19 (0.291) & 1.04 (0.374) & 0.42 (0.588) \\ 
  450 & 0.20 (0.012) & 1.12 (0.150) & 0.44 (0.046) && 0.22 (0.093) & 1.41 (0.428) & 0.53 (0.372) \\ 
  600 & 0.24 (0.004) & 2.00 (0.185) & 0.64 (0.026) && 0.24 (0.035) & 2.22 (0.583) & 0.67 (0.311) \\ 
  900 & 0.22 (0.006) & 1.20 (0.160) & 0.50 (0.038) && 0.23 (0.029) & 1.51 (0.291) & 0.57 (0.204) \\ 
  1200 & 0.23 (0.003) & 1.72 (0.198) & 0.60 (0.031) && 0.24 (0.014) & 2.21 (0.345) & 0.66 (0.166) \\ 
  1500 & 0.24 (0.003) & 1.67 (0.175) & 0.60 (0.028) && 0.24 (0.008) & 2.41 (0.298) & 0.70 (0.133) \\ 
  1978 & 0.24 (0.002) & 2.26 (0.265) & 0.68 (0.028) && 0.24 (0.003) & 2.52 (0.164) & 0.71 (0.068) \\ 
  \lastline
  \end{tabular}
\end{center}

Figure \ref{fig:fig2} presents the learning curves with the predicted C-statistics of different modelling strategies using clinical data alone and clinical plus CNA data. The NLS and GP-based learning curves were similar across sample sizes. Including the CNA data in the prediction model did not considerably improve the accuracy in this example. All models performed similarly at large sample sizes while the LGBM slightly under-performed at smaller samples. The ElNet and SVM reached a plateau at 1000 samples while accuracy improvement was minimal when more data were added to the other models.

\begin{center}
  \includegraphics[width=1\linewidth]{img/fig2.pdf}
  \captionof{figure}{Predicted C-statistics of learning curve of METABRIC with GP and NLS of Elastic Net (ElNet), light gradient-boosting machine (LGBM), random forest (RF), support vector machine (SVM) modelled in clinical and clinical plus CNA data}
  \label{fig:fig2}
\end{center}

The NLS and GP learning curves with and without anchoring on the METABRIC data are shown in Figure \ref{fig:fig3}. The NLS method works effectively both with and without extrapolation if the sample size is larger than 50 samples while this is true for survival outcome with 300 samples. The GP approach on the other hand, works effectively with and without anchoring on the METABRIC data for both logistic and survival models, except without extrapolation with small sample size.

\begin{center}
  \includegraphics[width=1\linewidth]{img/fig3.pdf}
  \captionof{figure}{Predicted C-statistics of learning curve of MSK with GP and NLS with (solid line) and without (dashed line) extrapolating from METABRIC data for logistic and survival model. The shaded area is the 95\% confidence interval of C-statistics from bootstrap using all MSK samples}
  \label{fig:fig3}
\end{center}


\section{Discussion}
\label{sec5}
We proposed a flexible and robust approach to sample size prediction based on learning curves. The goal is to estimate the expected predictive performance of a prediction model (and a modelling strategy) at a given sample size in the design stage. Our approach has several practical benefits which can be employed for analysing the performance of different prediction models, and as such used in an ongoing study to inform their futility or feasibility. Specifically, we proposed two variants of a learning curve, showing that the Bayesian GP-based version can achieve better performance at small to moderate sample sizes. These differences, on the other hand, were negligible at large sample sizes. We also showed that the stability and robustness of the developed learning curves could be further improved by anchoring the fitting process against a larger study on a related area where available.  

We also explored the value of adding different data modalities, which in our experiments, including genetic data, led to only a small gain in the predictive performance. Implicitly and more broadly, our approach allows for assessing the gain and the cost of collecting distinct data modalities at the planning stage. As a result, it appears sensible for one to modify the prioritisation of the research question in order to maximise the expected benefit.  

The proposed approach is general, modular and flexible. We used the C-statistic for assessing the predictive performance, but different scoring rules, like the Brier score, may be used. Moreover, our approach allows for a more principled analysis of modelling strategies. As the sample size increases, different modelling approaches, both statistical and machine learning algorithms, may be investigated and used for model development. Within-model variable selection can also be explored in a similar manner. We employed a 70/30 train/test split when obtaining predictive performance estimates but different rules, such as 80/20 splits, may also be used.

Finally, in the presence of limited data, we suggest anchoring on a larger study of similar data with a GP approach. We acknowledge that it may be hard to select the external data and quantify the similarity to the target data. To this end, the choice of external data can rely on expert (e.g. clinical) opinion or by inspecting the definition of the populations, such as inclusion-exclusion criteria and the relatedness of the endpoints. Real-world data may also be useful in the absence of high-quality clinical trial data and this is the subject or current research.


\section{Software}
\label{sec6}
The code for reproducing the results of this work as well as using the presented learning curves framework is available at \url{https://github.com/adayim/samplesize-learning}. All statistical computing and analyses, except the machine learning models evaluated in this study, were performed using the software environment R version 4.2.1 and Jags 4.3.1. In this work, for training and evaluating the statistical methods and the machine learning models we used the the Scikit-learn~\citep{scikit-learn} and LightGBM~\citep{NIPS2017_LightGBM} Python libraries. Furthermore, the hyper-parameter optimisation was performed using the Scikit-optimize~\citep{sckiopt} Python library.



\section*{Acknowledgments}
Conflict of Interest: None declared.
We thank Simon Bond for thoughtful feedback on this work. NS and JA acknowledge financial support from Mark Foundation for Cancer Research and Cancer Research UK Cambridge Centre [C9685/A25177]. JA also acknowledges financial support from Cancer Research UK, the National Institute for Health and Care Research and from Cambridge Biomedical Research Centre.



\newpage
  
\small
\bibliographystyle{biorefs}
\bibliography{ref}

\newpage
\appendix
\input{supp.tex}

\end{document}