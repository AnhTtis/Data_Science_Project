
\section{Related Work}
\label{sec:related_work}
\vspace{3mm}



\mypar{Audio for spatial perception.} 
Recent works have explored the use of sound for spatial understanding.
Purushwalkam \etal~\cite{purushwalkam2021audio} reconstructed floor plans in simulated environments~\cite{chen2020soundspaces}. 
Chen \etal~\cite{chen2021structure} used ambient sounds from environments to learn about scene structures. 
Konno \etal~\cite{konnoaudio} integrated sound localization to visual SfM while do not jointly learn them. 
Other work learns representations for spatial audio-visual tasks. Yang \etal~\cite{yang2020telling} predicted whether stereo channels are swapped in a video, and Morgado \etal~\cite{morgado2020learning} solved a spatial alignment task. The learned representations are then used to improve localization, up-mixing, and segmentation models. In contrast, we learn camera pose and sound localization solely from self-supervision, obtaining angular predictions without labeled data. 
Other work uses echolocation sounds to learn representations~\cite{gao2020visualechoes,yang2022camera} and predict depth maps~\cite{christensen2020batvision,parida2021beyond} and estimate camera poses~\cite{yang2022camera} using labeled data.
In contrast, our proposed approach jointly learns binaural sound localization and camera pose through passive audio sensing, without supervision.

\input{floats/fig_method}


\mypar{Acoustic synthesis and spatialization.} 
Researchers have explored visually-guided sound synthesis~\cite{gan2020foley,ghose2020autofoley,iashin2021taming,du2023conditional} and text-guided audio synthesis~\cite{kreuk2022audiogen,yang2022diffsound,huang2023make}. Additionally, researchers have investigated generating realistic environmental acoustics using visual information~\cite{chen2022visual,singh2021image2reverb,chen2022soundspaces,majumder2022few}.
Chen \etal~\cite{chen2023novel} introduced the novel-view acoustic synthesis task, which synthesizes binaural sound at the target view using audio and visual information from a source view. Liang \etal~\cite{liang2023av} proposed an audio-visual neural field in real-world audio-visual scenes.
Many recent works have proposed to generate spatial audio from mono audio using visual cues~\cite{morgado2018self,gao20192,rachavarapu2021localize,xu2021visually,lin2021exploiting,zhou2020sep,garg2021geometry}, or the relative pose between sound sources and the receiver~\cite{richard2021neural,huang2022end}. Inspired by these works, our feature learning approach learns spatial representations through an audio prediction task. 

\mypar{Binaural sound localization.} Humans have the ability to localize sound sources from binaural sound~\cite{rayleigh1907perception}.  Traditional approaches estimate interaural time delays via cross-correlation using hand-crafted features~\cite{knapp1976Generalized}, factorization methods~\cite{schmidt1986multiple}, or loudness differences between ears~\cite{rayleigh1907xii,wang2006computational}. 
Chen \etal~\cite{chen2022sound} adapted methods from self-supervised visual tracking to the problem of binaural sound localization. Similarly, we estimate direction through self-supervision. However, we obtain our supervision through cross-modal supervision from vision instead of from correspondence cues. Moreover, we also obtain visual camera rotation estimation through our learning process. Francl~\cite{francl2022modeling} learned representations of sound location with a contrastive loss where positive and negative examples are selected based on the extent of head movements. Other works have used supervised learning techniques with labeled data to localize sound sources in reverberant environments~\cite{adavanne2018direction,vecchiotti2019end,yalta2017sound}. 
Unlike these methods, our model learns 3D sound localization without labels.



\mypar{Camera pose estimation.}
Traditional methods for camera pose estimation are based on finding correspondences between images and then solving an optimization problem using constraints from multi-view geometry~\cite{Hartley04}. These include structure from motion methods that estimate full  pose~\cite{schonberger2016structure} and camera rotation~\cite{brown2007automatic}.
Recent methods have directly predicted camera pose using neural networks, including methods that use photos~\cite{qian2020associative3d,kendall2015posenet,melekhov2017relative,jin2022perspective,ma2022virtual} or RGB-D scans~\cite{yang2020extreme,yang2019extreme,banani2021unsupervisedr,banani2021bootstrap}. 
Our setup is similar to work that learns relative camera poses from sparse views~\cite{jin2021planar,cai2021extreme}, and we use their network architectures. However, we learn the camera pose through cross-modal supervision from audio, rather than from labels.  Our work is also closely related to methods that learn structure from motion through self-supervision~\cite{zhou2017unsupervised,zou2020learning}, such as by jointly learning models that perform depth and camera pose estimation with photoconsistency constraints. In contrast, our visual model's learning signal comes solely from audio-based supervision, and we jointly learn audio localization.

\mypar{Audio-visual learning.} 
A number of works have focused on learning multimodal representations for audio and vision, taking into account semantic correspondence and temporal synchronization~\cite{owens2018audio, xiao2020audiovisual,asano2020labelling,morgado2021audio,owens2016visually,afouras2022self,mittal2022learning}. 
Other approaches study audio-visual sound localization~\cite{hu2022mix,mo2022closer,chen2021localizing,mo2022localizing}, source separation~\cite{majumder2021move2hear,gao2021visualvoice,majumder2022active,tzinis2022audioscopev2}, active speaker detection~\cite{afouras2020self,tao2021someone,alcazar2021maas},  navigation~\cite{chen2020soundspaces,chen2020learning,chen2021semantic} and forensics~\cite{zhou2021joint,haliassos2022leveraging,feng2023self}. Our focus, in contrast, is on utilizing multi-view audio-visual signals to learn geometry.

