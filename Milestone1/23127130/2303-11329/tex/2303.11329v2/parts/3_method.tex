

\section{Sound Localization from Motion}
\label{sec:method}







We address the {\em sound localization from motion}~(SLfM) task: predicting the azimuth of a sound source from binaural audio and camera rotation from two images. First, we present a self-supervised representation that can be used to solve this downstream task. Then, we show how the representation can be used to solve the task. 



\subsection{Learning representation via spatialization}

We learn an audio-visual representation that conveys spatial information by solving a {\em cross-view binauralization} task: 
converting mono sound to stereo for one viewpoint, given an audio-visual pair sampled from another viewpoint~(\cref{fig:method}a). In order to successfully solve the task, a model must implicitly estimate the sound direction in the source view and predict how the change in viewpoint will affect the sound in the target view. A key difference between this task and traditional, single-view binauralization~\cite{gao20192} is that the sound source {\em need not be visible} in any of the images. Hence, the task cannot be solved from the target audio-visual pair alone. 

We binauralize the sound $\ba_t$ at the target view through an audio predictor $\mathcal{F}_{\theta}$. To make our representation suitable for downstream tasks, we factorize it into visual features $f_v(\bv_s, \bv_t)$, which are intended to create features relevant to relative pose, and audio features $f_a(\ba_s)$, which capture sound localization cues. We predict the binauralized audio $\hat{\ba}_t$ at the target view from mono audio $\bar{\ba}_t$, the visual change of $(\bv_s, \bv_t)$ and binaural sound $\ba_s$ heard at the source viewpoint: 
\begin{equation}
    \hat{\ba}_t = \mathcal{F}_{\theta}\left(\bar{\ba}_t, f_v(\bv_s, \bv_t), f_a(\ba_s)\right).
    \label{eq:generative}
    \vspace{-1mm}
\end{equation}


We represent audio $\ba$ as a spectrogram $\bA$ using short-time Fourier transform~(STFT). Following Gao \etal~\cite{gao20192}, given the mix of stereo audio $\bar{\bA}_t = \text{STFT}(\ba^L_t + \ba^R_t)$, we predict the difference of two channels $\bA_t = \text{STFT}(\ba^L_t - \ba^R_t)$. We optimize the $L1$ loss between predicted spectrogram $\hat{\bA}_t$ and ground-truth spectrogram $\bA_t$:
  \vspace{-2mm}
\begin{equation}
     \mathcal{L}_{\mathrm{pretext}} = ||\hat{\bA}_t - \bA_t||_1. \\
  \label{eq:loss}
  \vspace{-2mm}
\end{equation}





\mypar{Multi-view binauralization.} 
Following Zhou~\etal~\cite{zhou2017unsupervised}, we improve our representations by binauralizing sounds at $N$ different target viewpoints, using observations from a single source viewpoint $s$.  We hypothesize that  jointly solving spatialization problems from a single viewpoint for multiple target viewpoints would require the model to make more accurate predictions of sound source locations, thereby improving the estimation of view changes:
\begin{equation}
    \mathcal{L}_{\mathrm{pretext}}  = \frac{1}{N} \sum_i^{N} ||\mathcal{F}_{\theta}\left(\bar{\bA}_{i}, f_v(\bv_s, \bv_{i}), f_a(\ba_s)\right) - \bA_{i}||_1.
 \label{eq:multi_binaural}  
 \vspace{-3mm}
\end{equation}


\subsection{Estimating pose and localizing sounds}

We now address the problem of learning models for sound localization and pose estimation, using our self-supervised audio-visual features. Given two views, we predict sound directions and relative rotation. We train the model to make these two predictions consistent with one another, while using simple binaural constraints to resolve ambiguities. 

We are given images $\bv_s$ and $\bv_t$ (rotated views recorded at the same position) and learned visual embedding $f_v$. We predict the scalar rotation angle $\phi_{s, t}$ via the encoder $g_v$: 
\begin{equation}
    \phi_{s, t} = g_v\left( f_v(\bv_s, 
    \bv_t)\right), \hspace{1mm} R_{s, t}=
    \begin{bmatrix}
        \cos \phi_{s, t} & - \sin \phi_{s, t} \\
        \sin \phi_{s,t} & \cos\phi_{s, t}
    \end{bmatrix},
\end{equation}
where $R_{s, t}$ is 2D rotation matrix of $\phi_{s, t}$.
Following common practice in indoor scene reconstruction, we give the camera a fixed downward tilt~\cite{zhi2021place,qian2021recognizing,zhang2017physically} and only estimate azimuth~\cite{jin2021planar}. This is also a common assumption in audio localization~\cite{adavanne2018direction,vecchiotti2019end}, since azimuth has strong binaural cues.



Similarly, we predict the azimuths of the sound sources using audio features  and  the encoder $g_a$: 
\begin{equation}
    \theta_{i} = g_a\left( f_a(\ba_i)\right), \quad \br_i = \begin{bmatrix}
        \cos \theta_{i} & \sin \theta_{i}
    \end{bmatrix}^\mathsf{T},
\end{equation}
where we represent the azimuth as a vector $\br_i$

\mypar{Cross-modal geometric consistency.}
When the camera is rotated, the sound source ought to rotate in the opposite direction~(\cref{fig:method}b). For example, a $30^\circ$ clockwise camera rotation should result in a $30^\circ$ counterclockwise rotation in sound direction.  Such a constraint could be converted into a loss:
\begin{equation}
    \mathcal{L}_{\mathrm{rot}} = \lVert\br_{s} -  R_{s, t}\br_{t}\rVert^2. %
    \label{eq:geometric}
\end{equation}

However, a well-known ambiguity called front-back confusion~\cite{rayleigh1907xii,fischer2020front} exists in binaural sound perception: one cannot generally tell whether a sound is in front of the view, or behind them.
To address that, we use permutation invariant training~\cite{yu2017permutation}~(PIT), and allow the model to use either the predicted sound direction or its reflection about the $x$ axis without penalty. This results in the loss:
\vspace{-1mm}
\begin{equation}
        \mathcal{L}_{\mathrm{geo}} = \min_{
        \substack{
        \hat{\mathbf{r}}_s \in \{\mathbf{r}_s, Q \mathbf{r}_s\} \\
        \hat{\mathbf{r}}_t \in \{\mathbf{r}_t, Q \mathbf{r}_t\}}}~\lVert\hat{\br}_{s} -  R_{s, t}\hat{\br}_{t}\rVert^2, \vspace{-1mm}
    \label{eq:per_geometric}
\end{equation}
\vspace{-1mm}
where $Q = \begin{bmatrix} 1 & 0 \\ 0 & {-1} \end{bmatrix}$ reflects the sound direction. 

\vspace{3mm}
As a consequence of this ambiguity, there are also two possible solutions for the visual rotation model, since the visual rotation matrices can be mirrored about the $x$ axis. For example, one can create a solution with equal loss by multiplying the rotations and sound directions by $Q$. We discuss this ambiguity in more depth in \cref{sec:pose}.


\mypar{Incorporating binaural observations.} 
Without additional constraints, the solution is ambiguous, and may collapse into a trivial solution (\eg, predicting zero for all three angles).\footnote{Work on self-supervised SfM has similar ambiguities~\cite{zhou2017unsupervised}, and deals with them by adding analogous constraints, such as photometric consistency.} To avoid this, we force the model to agree with a simple  binaural cue based on interaural intensity difference~(IID). We predict whether the sound is to the left or right of the viewer, based on whether it is louder in the left or right microphone: $d = \mathrm{sign}(\log\left\vert \frac{\bA^L}{\bA^R}\right\vert)$, where $\vert \bA \vert$ is the magnitude of the spectrogram $\bA$. 
We perform this left/right test at each timestep in the spectrogram and then pool via majority voting (see \supparxiv{supplementary}{\cref{appendix:implement}} for details).
We penalize predictions that are inconsistent with these ``left or right'' observations: %
\vspace{-2mm}
\begin{equation}
    \mathcal{L}_{\mathrm{binaural}} = \mathcal{L}_{\mathrm{BCE}}\left(\sin\theta_i, d_i\right),
    \label{eq:binaural}
\vspace{-1mm}
\end{equation}
where $\mathcal{L}_{\mathrm{BCE}}$ is binary cross entropy loss. 

\mypar{Encouraging symmetry.} To help regularize the model, we also add symmetry constraints. 
For sound localization, swapping the left and right channels of the audio ought to result in a prediction in the opposite direction since the binaural cues are reversed.
For rotation estimation, the relative pose between images $s$ and $t$ should invert the pose from $t$ to $s$. We encourage both constraints via a loss:
\begin{equation}
    \mathcal{L}_{\mathrm{sym}} = \lvert\theta + \theta_{\mathrm{flip}} \rvert + \lvert\phi_{s, t} + \phi_{t, s}\rvert,
    \label{eq:symmetric}
\end{equation}
where $\theta_{\mathrm{flip}}$ is the prediction of sound angle $\theta$ using audio with swapped audio channels, and $\phi_{s,t}$ and $\phi_{t,s}$ are the predicted rotations between cameras $s$ and $t$.

\mypar{Overall loss.} We combine these constraints to obtain an overall loss:
\vspace{-2mm}
\begin{equation}
    \mathcal{L} = \lambda \mathcal{L}_{\mathrm{geo}} + \mathcal{L}_{\mathrm{binaural}} + \mathcal{L}_{\mathrm{sym}},
    \label{eq:overall}
\end{equation}
\vspace{-1mm}
where $\lambda$ is the weight for the geometric loss. 


































