\section{Conclusion}
In this paper, we proposed the {\em sound localization from motion} (SLfM) problem, and provided a self-supervised method for solving it. We also presented a method for learning audio-visual features that convey sound directions and camera rotation, which we show are well-suited to solving the SLfM task. 
Despite learning our models solely from unlabeled audio-visual data, we obtain strong performance on a variety of benchmarks, including rotation estimation on the Stanford2D3D~\cite{armeni2017joint} dataset and ``in the wild'' sound direction estimation~\cite{chen2022sound}. Our results suggest that the subtle correlations between sights and binaural sounds that result from rotational motion provide a useful (and previously unused) learning signal. We see our work as opening new directions in self-supervised geometry estimation and feature learning that use sound as a complementary source of supervision. We will release code, data, and models upon acceptance.


\mypar{Limitations and Broader Impacts.} 
Our work has several limitations. 
First, while we evaluate our models on real images and sounds, we train on data from simulators, due to a lack of available relevant data. We note that this is common practice in visual 3D reconstruction~\cite{jin2021planar,wang2022neuris,lin2022neurmips}. %
Second, we assume that the 3D scene and sound sources are stationary.
Third, we do not evaluate our model on extreme viewpoint changes~\cite{ma2022virtual}, which requires reasoning about images that have little or no overlap.

