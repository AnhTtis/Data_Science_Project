\section{Experiments}
\label{sec:exp}

We have introduced a self-supervised method to learn camera pose and sound localization from audio-visual data.
In experiments, we first evaluate how well our learned representation captures spatial information.
We then evaluate how well our method learns camera pose and sound localization by comparing it with baselines. %
Finally, we show generalization to indoor panorama images Stanford2D3D~\cite{armeni2017joint} and in-the-wild binaural audio~\cite{chen2022sound}.



\input{floats/fig_arch}
\subsection{Implementations}
\vspace{3mm}
\mypar{Visual pose encoder.} We follow recent pose estimation work~\cite{cai2021extreme,jin2021planar} and build a Siamese-style visual pose network $f_v$ with ResNet-18~\cite{he2016} as the backbone. We compute dense 4D correlation volumes between the features from the third residual layer and then encode them by convolution layers. We resize images to $320 \times 240$ and encode a pair of images into 512-d features. We use an MLP $g_v$ to map visual features to 1-d logits for our SLfM models. 

\mypar{Binaural audio encoder.} We obtain binaural audio embeddings $f_a(\cdot)$ using ResNet-18~\cite{he2016} that operates on spectrograms. We covert the two-channel waveform of length $L$ to a spectrogram representation of size $256 \times 256 \times 4$ using short-time Fourier transform~(STFT), where we keep both the magnitude and phase of spectrograms. We extract 512-d features of binaural sound with $f_a$ and map them to 1-d logits using an MLP $g_a$.

\mypar{Audio prediction model.} We adopt the light-weighted audio-visual U-Net~\cite{gao20192} to perform binauralization. We feed in spectrograms of size $256 \times 256 \times 2$ and predict the target spectrograms. We concatenate the visual pose features $f_v(\cdot)$ and audio features $f_a(\cdot)$ at the bottleneck of U-Net. We show the architecture of our models in \cref{fig:arch}. Please see \supparxiv{supp.}{\cref{appendix:implement}} for more implementation details. 



\subsection{Dataset}
Since there is no public multi-view audio-visual dataset with camera poses and sound direction ground truth, we use the SoundSpaces 2.0 platform~\cite{chen2022soundspaces} to create a dataset. 
Our 3D scenes come from Habitat-Matterport 3D dataset (HM3D)~\cite{ramakrishnan2021hm3d}, which is a large dataset of real 3D scenes.
This setup allows us to have photorealistic images and high-quality spatial audio with real-world acoustics phenomenon (\eg, reverberation), as well as providing the ground-truth camera pose and sound directions that can be used for evaluation. 
We call this dataset HM3D-SS. %

We generate binaural Room Impulse Responses~(RIRs) and images with a $60^\circ$ field of view, using 100 scenes of HM3D~\cite{ramakrishnan2021hm3d}. For each audio-visual example, we randomly place sound sources in the scene with a height range of $(0.7, 1.7)$ meters, and sample 4 different rotated viewpoints at one location within 4 meters. The rotations are limited to $(10^\circ, 90^\circ)$ relative to the source viewpoints. 
We follow the standard practice to set the height to agents to be 1.5m and lock a downward tilt angle~\cite{jin2021planar,zhang2017physically,qian2021recognizing,zhi2021place}.
We render the binaural RIRs and images given the position of agents and sound sources.
We obtain binaural audio by convolving binaural RIRs with mono audio samples from LibriSpeech~\cite{panayotov2015librispeech} and Free Music Archive~\cite{defferrard2016fma}. To ensure that the evaluation tests the model's pose estimation abilities, rather than its ability to visually localize sound sources, the sound sources are not visible on screen. 

We create 50K audio-visual pairs from 200K viewpoints. The audio was rendered with average reverberation of $\text{RT}_{60}=0.4\text{s}$ (see \supparxiv{supp.}{\cref{appendix:implement}} for details).
We divided our data into 81/9/10 scenes for the train/val/test, respectively.


\input{floats/tab_downstream}

\subsection{Evaluating the learned representation}
First, we directly evaluate the quality of our learned features for rotation estimation and sound localization via linear probing with labeled data (rather than learning them jointly through self-supervision).



\mypar{Baselines and ablations.}
We compare our model with several baselines that use alternative pretext tasks: 1)~{\bf AVSA}~\cite{morgado2020learning}: it learns spatial cues by training a model to spatially align video and audio clips extracted from different viewing angles. We adapt this model to our dataset and train with 4 different views; 2)~{\bf RotNCE}~\cite{francl2022modeling}: it applies contrastive learning on the audio from different angles and uses annotations of the agent's rotation to select positive and negative samples, which results in learning audio spatial representation. For baselines, we use the same architecture for feature extractors to ensure fair comparisons. 

To determine if we utilize visual and audio features from different views to solve the binauralization task, we also study some variants of our models: 
1)~{\bf Ours-NoA}: we only provide visual features for the binauralization task; 
2)~{\bf Ours-NoV}: which only uses audio from the other view to spatialize sounds;
3)~{\bf Ours-GTRot}: we provide ground-truth rotation embedding instead of features from visual frames.

Besides the mono-to-binaural task, we also experiment with another objective: predicting the right channel from the left channel.
We train our {\bf L2R} model with the same setup as our M2B model. (Please see \supparxiv{supp.}{\cref{appendix:additional}} for pretext results.)



\input{floats/tab_pose}

\mypar{Downstream tasks.}
We assess the quality of spatial representations we learned from our pretext tasks in two downstream tasks: relative camera rotation and 3D sound localization. We formulate them as classification problems, where angles are categorized into 64 bins, and we use accuracy as the evaluation metric.
To evaluate the learned features, we freeze them and train a linear classifier on the downstream tasks. We compare the performance of our features with those learned from RotNCE~\cite{francl2022modeling}, AVSA~\cite{morgado2020learning}, ImageNet~\cite{he2015}, and random features, and report the results in \cref{tab:downstream}. Our approach outperforms the baselines in both tasks, indicating that we learn better spatial representations. 
Furthermore, our linear probe models show comparable performance against the supervised method which can be regarded as approximate upper bounds for our models, suggesting that our pretext tasks help learn a useful representation. 


\mypar{Emerging camera pose from audio prompting.}
To help understand the strong performance of our self-supervised features, we asked whether we could use the cross-view binauralization model {\em alone} to estimate camera rotation. Inspired by prompting in vision and language models~\cite{radford2021learning}, we obtain rough estimates of camera rotation by providing our model with carefully-provided inputs. 
Given a pair of images $(\bv_s, \bv_t)$, we create a synthetic binaural audio {\em prompt}, $\ba_s$. We then ask our model to generate the binaural sound $\hat{\ba}_t$ for the target viewpoint. By analyzing the IID cues in $\hat{\ba}_t$, we can estimate the model's implicitly predicted camera pose. To do this, we find the nearest neighbor of our generated audio $\hat{\ba}_t$, using a database of synthetically generated audio with known sound directions. Please refer to \supparxiv{supp.}{\cref{appendix:prompt}} for more details. Our method achieves the mean absolute error of $\mathbf{9.13^\circ}$ on HM3D-SS dataset where chance is $29.41^\circ$. Our approach can also generalize to Stanford2D3D~\cite{armeni2017joint} dataset, where we can achieve mean absolute error of $\mathbf{9.93^\circ}$. %



\input{floats/fig_realworld}



\subsection{Evaluating SLfM}
\label{sec:pose}
We conducted evaluations on our sound localization from motion~(SLfM) models on the HM3D-SS dataset. We use the mean absolute error of angle in degrees as evaluation metrics.  To avoid sound field ambiguity, we filtered out samples with sound angles outside of $(-90^\circ, 90^\circ)$ for the evaluation set.
When training our SLfM models, we use our best-performing features, \ie, pretext tasks trained with 3 views.  We freeze learned audio and visual features and only train multi-layer perceptrons on top of them and evaluate them independently on each modality.
\input{floats/fig_qualitative}







\mypar{Baselines and ablations.} 
For relative camera pose estimation, we compare our models with sparse feature matching using SIFT~\cite{lowe2004distinctive} and SuperGlue~\cite{sarlin2020superglue}, followed by rotation fitting.
We use those methods to detect key points, run Lowe's ratio test, and use RANSAC with five-point algorithm to recover the camera pose~\cite{lowe2004distinctive,Hartley04,nister2004efficient,fischler1981random}.
For the sound localization task, we compared with time delay estimation methods: the popular GCC-PHAT~\cite{knapp1976Generalized} and the recent self-supervised method StereoCRW~\cite{chen2022sound}. As far as we know, there are no other baselines that can estimate poses and sound source directions without labels. %

We also compared several variations of our method,
including {\bf Ours--Front}, where we filter out the samples with sound sources behind the viewers to remove binaural ambiguity during training, and an oracle model {\bf Ours--GTRot}, which uses ground-truth rotation angles instead.

\mypar{Results.} We show our results in \cref{tab:pose_estimation}. Our models can predict the azimuths of sound sources, obtaining strong performance without any labels. Without providing reflection invariance~(\cref{eq:per_geometric}), the model failed to learn reasonable geometric due to the audio ambiguity. Our self-supervised model achieves comparable performance against our oracle model~(Ours-GTRot) and supervised models, indicating we estimate camera poses and localize sound   accurately. 
We show some qualitative results on HM3D-SS in \cref{fig:qualitative} with LibriSpeech samples~\cite{panayotov2015librispeech}.

\input{floats/tab_realworld}


\mypar{Generalization to other datasets.}
We further demonstrate the generalization ability of our models by experimenting with out-of-distribution, real-world data. We evaluate our camera rotation model on Stanford2D3D~\cite{armeni2017joint} with real indoor RGB images (\cref{tab:real_world}). We obtain image pairs by cropping from panoramas. Although our model is trained on renderings of HM3D~\cite{ramakrishnan2021hm3d}, it obtains strong generalization ability. 
Compared with rotation estimation based on SIFT~\cite{lowe2004distinctive} and SuperGlue~\cite{sarlin2020superglue}, our model is significantly better on mean rotation error.
We also report median rotation error to be consistent with prior works~\cite{cai2021extreme,jin2021planar}.
However, SIFT~\cite{lowe2004distinctive} and Superglue~\cite{sarlin2020superglue} have a very low median error. This is likely due to the all-or-nothing nature of feature matching-based approaches, which either produce highly accurate predictions if the matches are correct (especially for ``easy'' cases with small amounts of rotation) or else produce gross errors.


We also evaluate our sound localization model on the in-the-wild binaural audio~\cite{chen2022sound}. We use binary accuracy as the metric for left-or-right direction classification accounting for the fact that microphone baselines are unknown for internet videos.
For an apples-to-apples comparison to prior work, we retrained our model using 0.51s length of audio for the binaural audio encoder $f_a(\cdot)$. 
As shown in \cref{tab:real_world}, our model obtains similar performance to StereoCRW, a state-of-the-art self-supervised time delay method, suggesting we have a strong capability for sound localization. We show qualitative results in \cref{fig:real_world}. We also perform both tasks on a self-recorded video~(\cref{fig:demo}) of a rotating camera and a binaural microphone. We show the mean and standard deviation of predictions in 1.0s windows.


\input{floats/fig_demo}


\mypar{Handling ambiguity.} 
In binaural sound perception, there is a fundamental ambiguity that whether the sound is in front of or behind us. It leads to multiple solutions with equal loss in our model, mirroring sound sources and negating rotation angles with flipped $z$ axis~(\cref{fig:ambiguity}). These two solutions differ in that a visual rotation angle either indicates a clockwise or counterclockwise rotation. 
For evaluation, we convert ``backwards'' counterclockwise predictions to clockwise predictions by simply providing the model with pairs of input frames, then negating the model's outputs if the angle is the opposite of the expected direction\footnote{Similar to ambiguities in SfM where reconstructions can be reoriented such that the sky is in the positive $y$ direction~\cite{Hartley04,Hartley98e,agarwal2011building}.}. 


\begin{figure}[b!]
\begin{minipage}{0.45\linewidth}
\upvspacefig
\input{floats/fig_ambiguity}
\end{minipage}
\hfill
\begin{minipage}{0.45\linewidth}
\upvspacefig
\vspace{-4.0mm}
\input{floats/fig_reverb_pose}
\end{minipage}

\end{figure}





\subsection{Generalization to more complex scenarios}
\label{sec:complex_scene}
We investigate whether our approach generalizes to more complex scenarios, such as with multiple sound sources or when translation is included in camera motion.

\input{floats/tab_complex.tex}

\mypar{Multiple sound sources.} 
We evaluate our representations and SLfM models on more complex scenes containing multiple sound sources. We train our models with two source sources placed in the scenes. One of them is dominant and our sound localization target. 
We report the results in \cref{tab:complex} and \cref{tab:complex_pose}. Our model learns better representations than baselines and achieves accurate predictions of the azimuth of the source and camera pose even in challenging scenarios with multiple sound sources.

\input{floats/tab_complex_pose} 


\mypar{Translation in camera motions.} 
To study how small translations in the camera motion could affect our models,  we generate 50K pairs of audio-visual data with both rotation and translation change, limiting the uniformly sampled translation to 0.5 meters. We train our models with LibriSpeech samples~\cite{panayotov2015librispeech}. 
For our linear probing evaluation, we measure the ability of features to handle complex examples by testing on the dataset that has translation. 
For our SLfM model, we study our ability to learn from noisy data, thus we evaluate it on the rotation-only examples.
As the results are shown in \cref{tab:complex} and \cref{tab:complex_pose}, we successfully learn useful features and obtain accurate rotation and sound direction predictions despite the presence of translation. Since we jointly learn the audio and visual representations, it can negatively impact the learning of one modality when another one becomes harder.



\subsection{Ablation study}
\vspace{3mm}

\mypar{Robustness to reverberation.} 
We study how our models perform under the different reverberation configurations. We used SoundSpaces~\cite{chen2022soundspaces} to create audio with average reverberation $\text{RT}_{60} \in \{0.1, 0.2, 0.3, 0.4, 0.5\}$ while keeping the visual signals the same.
We train our model on each setting. As shown in \cref{fig:reverb_pose}, our performance decreases as the level of reverberation increases, where audio becomes more challenging during both training and testing. 


\mypar{Losses for SLfM.} 
We study the necessity of our proposed loss functions in \cref{tab:loss_ablation}.
Our models fail to learn accurate pose estimation without binaural loss or symmetric loss.
It highlights the crucial role of these losses.

\input{floats/tab_loss_ablation}


