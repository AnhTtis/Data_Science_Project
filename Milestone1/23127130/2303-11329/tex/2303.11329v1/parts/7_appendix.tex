\supparxiv{
\setcounter{page}{1}
\twocolumn[{%
\renewcommand\twocolumn[1][]{#1}%
\begin{center}
    \vspace{-1.0em}
    {\bf \large Supplementary Material for Sound Localization from Motion:\\ Jointly Learning Sound Direction and Camera Rotation}
    \vspace{1.0em}
\end{center}

}]

}{}
\renewcommand{\thesection}{A.\arabic{section}}
\setcounter{section}{0}


\supparxiv{
\section{Video results}
We provide some qualitative results for our sound localization and camera pose estimation models on HM3D-SS dataset in {\tt qualitative.mp4} and self-recorded real videos in {\tt demo.mp4}. 
Videos are better watched with headphones, as it can be difficult to perceive the stereo sound without them.
}{}



\section{Camera pose from audio prompting}
\label{appendix:prompt}
We illustrate our prompting idea in \cref{fig:prompt}.
To create our audio prompts, we simulate 181 binaural RIRs at different angles from $[-90^\circ, 90^\circ]$ without reverberation using SoundSpaces~\cite{chen2022soundspaces} and render with audio signals from LibriSpeech~\cite{panayotov2015librispeech}. We use the sound with an angle of $0^\circ$ as the input prompt $\ba_s$~(the source view audio) and mix it into mono audio as the input at the target viewpoint. We calculate the interaural intensity difference (IID) cues for the audio prompts $\ba_i$ and generated audio $\hat{a}_t$. We use L1 distance between IID cues to find the nearest neighbors:
\begin{equation}
     \argmin_{\bA_i} \left\vert \log_{10}\frac{\hat{\bA}^L_t}{\hat{\bA}^R_t} - \log_{10}\frac{\bA^L_i}{\bA^R_i}  \right\vert,
    \label{eq:prompt}
\end{equation}
where $\bA_i=\text{STFT}(\ba_i)$. We use ground truth annotations of sound directions from the nearest prompts to predict the camera rotation angles. We first obtain rotation prediction votes from 1024 audio prompts and use a RANSAC-like mode estimation~\cite{fischler1981random,chen2022sound} to get the final prediction.

\input{floats/fig_prompt}










\section{Additional experimental results}
\label{appendix:additional}
\vspace{3mm}
\mypar{Evaluating pretext task.}
We also evaluate the performance of our model on the pretext task, which involves binauralizing sound at a novel microphone pose using sound from a different viewpoint and visual cues from both views as references.  We use the STFT distance between the predicted and ground-truth spectrogram to measure the audio reconstruction performance. As the results are shown in \cref{tab:pretext}, our model that incorporates both visual and audio features as input performs the best and is comparable to the model that receives ground truth rotation angles as inputs. 
This suggests that our model effectively uses the spatial information in both visual and audio signals to solve binauralization tasks, and encourages the network to learn useful representations. Moreover, the results show that training with more viewpoints improves the performance of the pretext task.

\input{floats/tab_pretext}


\mypar{SLfM without pretraining.} We further demonstrate the important role of the features learned from our cross-view binaural pretext task by training our SLfM model with random features. We show results in \cref{tab-appendix:random_pose}. We can see that the models perform better using our feature representations, which emphasizes the significance of our pretext task. Our SLfM model finetuned from random features achieves accurate predictions, highlighting that our proposed method successfully leverages the geometrically consistent changes between visual and audio signals.

\input{floats/tab_random_pose}



\section{Ablation study}
\label{appendix:ablation}
\vspace{3mm}

\renewcommand{\columnsep}{10pt}
\begin{wraptable}{r}{0.4\linewidth}
\vspace{-1.0em}
\captionsetup{type=figure}
\input{floats/fig_reverb_pretext.tex}
\vspace{-1.0em}
\end{wraptable} %
\mypar{Robustness to reverberation.} We also evaluate our representation under the influence of reverberation. We report linear probe performance on downstream tasks with average reverberation $\text{RT}_{60} \in \{0.1, 0.2, 0.3, 0.4, 0.5\}$. 
As shown in \cref{fig:reverb_pretext}, the results indicate a decrease in downstream performances as the level of reverberation increases, where audio becomes more challenging during both training and testing. 

\mypar{Audio prediction network.} We study how audio prediction architectures will influence representation learning from our proposed pretext task. We adapt the U-Net architecture with cross-attention modules for conditional feature inputs~\cite{rombach2022high,vaswani2017attention} and compare the pretext and downstream performance with U-Net~\cite{gao20192} we used for our main experiments. We train our models on the HM3D-SS dataset with a single sound source presented in the scenes and use LibriSpeech signals~\cite{panayotov2015librispeech}. We report results in \cref{tab:unet}. Interestingly, we found that ATTN U-Net can reconstruct better sounds for the pretext task while it does not learn the features as well as the 2.5D U-Net~\cite{gao20192}. We hypothesize that a more complex network may transfer the representation learning inside of the prediction networks rather than the feature extractors. 
\input{floats/tab_unet}








\section{Implementation details}
\label{appendix:implement}
\vspace{3mm}
\mypar{SLfM model.} We use separate multi-layer perceptrons $g_v$ and $g_a$~(\ie, FC~($512\rightarrow 256$)--ReLU--FC~($256\rightarrow 1$) layers) to predict scalar rotation and sound angles.


\mypar{Hyperparameters.}
For all experiments, we re-sample the audio to 16kHz and use 2.55s audio for the binauralization task. For pretext training, we use the AdamW optimizer~\cite{kingma2015adam,loshchilov2017decoupled} with a learning rate of $10^{-4}$, a cosine decay learning rate scheduler, a batch size of 96, and early stopping. During downstream tasks, we change the learning rate to $10^{-3}$ for linear probing experiments. To train our self-supervised pose estimation model, we set the weights $\lambda_1, \lambda_2$, and $\lambda_3$ to be $5,1,1$ for geometric, binaural, and symmetric losses respectively. For more complex scenarios~(\cref{sec:complex_scene}), we set the weights $\lambda_1, \lambda_2$, and $\lambda_3$ to be $3,1,1$ to avoid the geometric loss from dominating.

\mypar{IID cues.} We describe our implementation of predicting sound on the left or right using IID cues in detail here: we first compute the magnitude spectrogram $|\bA|$ from the binaural waveform $\ba$ and sum the magnitude over the frequency axis. Next, we calculate the log ratio between the left and right channels for each time frame. After this, we take the sign of log ratios and convert them into either +1 or -1. We sum over the votes and take the sign of it for final outputs.


\mypar{Dataset.} Due to the fact that SoundSpaces 2.0~\cite{chen2022soundspaces} does not support material configuration at the current time, we obtain binaural RIRs with different reverberation levels by scaling the indirect RIRs and add them up with direct RIRs. 
We render binaural sounds with random audio samples as augmentation during training.


