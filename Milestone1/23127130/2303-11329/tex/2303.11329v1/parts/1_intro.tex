\vspace{-2mm}
\section{Introduction}
\label{sec:intro}







As you rotate your head, the images and sounds that you perceive change in  geometrically consistent ways. For example, after turning to the right, a sound source that was directly in front of you will become louder in your left ear and quieter in your right, while simultaneously the visual scene will move right-to-left across your visual field (\cref{fig:teaser}). 



We hypothesize that these co-occurring audio and visual signals provide ``free'' supervision that captures geometry, including the motion made by a camera and the direction of sound sources. These  are each core problems in machine perception, but are largely studied separately, often using supervised methods that rely on difficult-to-acquire labeled training data, such as annotated sound directions. We take inspiration from self-supervised approaches to structure from motion~\cite{zhou2017unsupervised}, which learn to estimate 3D structure and camera pose by solving both tasks simultaneously. 




Analogously, we propose a problem we call {\em sound localization from motion} (SLfM): jointly estimating camera rotation from images and the sound direction from binaural audio. By solving both tasks simultaneously, we avoid the need for labeled training data. Our models provide each other with self-supervision: a visual model predicts the rotation angle between pairs of images, while an audio model predicts the azimuth of sound sources. We force their predictions to agree with one another, such that  changes in rotation are consistent with changes in sound direction and binaural cues. After training, the models can be deployed independently, without multimodal data at test time.






\input{floats/fig_teaser.tex}



This is a challenging task that requires perceiving motion in images and binaural cues in audio. Our second contribution is a method for learning representations that are well-suited to this task through {\em cross-view binauralization}. We train a network to convert mono to binaural sound for one viewpoint, given an audio-visual pair sampled from another viewpoint. Since the sound source is not necessarily visible in the images, the only way to successfully solve this pretext task is by analyzing the changes in the camera pose and predicting how they affect the sound direction. 

All components of our model are {entirely self-supervised} and are trained solely on unlabeled audio-visual data. Our results suggest that paired audio-visual data provides a useful and complementary signal for
learning about geometry. In contrast to other audio or visual self-supervised pose estimation methods, we obtain supervision from abundantly available audio data, thus avoiding the need of 3D ground truth or correspondences between pixels~\cite{zhou2017unsupervised,zou2020learning} or audio samples~\cite{chen2022sound}. %
Through experiments, we show:
\begin{itemize}[leftmargin=*,topsep=1pt, noitemsep]
\item Paired audio-visual data provides a supervisory signal for pose estimation tasks. 
\item We obtain competitive performance with state-of-the-art self-supervised sound localization methods~\cite{chen2022sound}.
\item We obtain strong rotation estimation performance, and our model generalizes to Stanford2D3D~\cite{armeni2017joint} dataset, where it is competitive with classic sparse feature matching methods.
\item The features we learn through our pretext task outperform other representations for our downstream tasks.
\end{itemize}


















 

 









 











 


















































































































