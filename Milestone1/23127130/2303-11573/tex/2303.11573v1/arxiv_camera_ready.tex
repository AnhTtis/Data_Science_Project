\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{iccv}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{xcolor}
\usepackage{mwe}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{color, colortbl}
\usepackage[export]{adjustbox}
\usepackage{balance}
\usepackage{enumerate}
\usepackage{enumitem}

% Include other packages here, before hyperref.

\newcommand{\grayrow}{\rowcolor[gray]{.9}}
\definecolor{rowgray}{gray}{0.5}

\newenvironment{Itemize}{
    \begin{itemize}[leftmargin=*]
    \setlength{\itemsep}{0pt}
    \setlength{\topsep}{0pt}
    \setlength{\partopsep}{0pt}
    \setlength{\parskip}{0pt}}
{\end{itemize}}
\setlength{\leftmargini}{9pt}

\newenvironment{Enumerate}{
    \begin{enumerate}
    \setlength{\itemsep}{0pt}
    \setlength{\topsep}{0pt}
    \setlength{\partopsep}{0pt}
    \setlength{\parskip}{0pt}}
{\end{enumerate}}
\setlength{\leftmargini}{9pt}

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[breaklinks=true, bookmarks=false, colorlinks]{hyperref}

\iccvfinalcopy % *** Uncomment this line for the final submission

\def\iccvPaperID{12039} % *** Enter the ICCV Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
\ificcvfinal\pagestyle{empty}\fi

\begin{document}

%%%%%%%%%%%%%%%%% TITLE %%%%%%%%%%%%%%%%%
%\title{BigSmall: An End-to-End Efficient multi-task Network \\ for Disparate Spatial and Temporal Tasks}
\title{BigSmall: Efficient Multi-Task Learning for Disparate Spatial and Temporal Physiological Measurements}


\author{Girish Narayanswamy$^1$, Yujia Liu$^1$, Yuzhe Yang$^2$, Chengqian Ma$^1$, \\
Xin Liu$^1$, Daniel McDuff$^1$, Shwetak Patel$^1$\\
$^1$University of Washington, $^2$Massachusetts Institute of Technology\\
\tt\small \{girishvn, nyjliu\}@uw.edu, yuzhe@mit.edu, cm74@uw.edu\\
\tt\small \{xliu0, dmcduff, shwetak\}@cs.washington.edu
}


\maketitle
% Remove page # from the first page of camera-ready.
\ificcvfinal\thispagestyle{empty}\fi



%%%%%%%%%%%%%%%%% ABSTRACT %%%%%%%%%%%%%%%%%
\begin{abstract}
Understanding of human visual perception has historically inspired the design of computer vision architectures. As an example, perception occurs at different scales both spatially and temporally, suggesting that the extraction of salient visual information may be made more effective by paying attention to specific features at varying scales. Visual changes in the body due to physiological processes also occur at different scales and with modality-specific characteristic properties. Inspired by this, we present BigSmall, an efficient architecture for physiological and behavioral measurement. We present the first joint camera-based facial action, cardiac, and pulmonary measurement model. We propose a multi-branch network with wrapping temporal shift modules that yields both accuracy and efficiency gains. We observe that fusing low-level features leads to suboptimal performance, but that fusing high level features enables efficiency gains with negligible loss in accuracy. Experimental results demonstrate that BigSmall significantly reduces the computational costs. Furthermore, compared to existing task-specific models, BigSmall achieves comparable or better results on multiple physiological measurement tasks simultaneously with a unified model.
\vspace{-0.3cm}
\end{abstract}


%%%%%%%%%%%%%%%%% FIGURE: BS Teaser Images %%%%%%%%%%%%%%%%%
\begin{figure}[t]
    \begin{center}
    % \includegraphics[width=3in]{figures/Teaser-BigSmall.pdf}
    \includegraphics[width=3in]{figures/Teaser-BigSmall_GVN.pdf}
    \end{center}
    \caption{\textbf{Overview of the Proposed BigSmall Model.} We present the first joint facial action, cardiac, and pulmonary measurement model from video. By leveraging a dual-branch architecture with wrapped temporal shift modules we achieve strong accuracy with an efficient multi-task implementation.}
    \label{fig:model_teaser}
\vspace{-0.3cm}
\end{figure}

%%%%%%%%%%%%%%%%% INTRODUCTION %%%%%%%%%%%%%%%%%
\section{Introduction}

Human visual perception occurs at both coarse and fine scales. Attending to coarse \emph{spatial} scales enables a quick estimate of the input to activate scene schemas in memory, while attending to fine information allows for further refinement~\cite{schyns1994blobs}. Motion perception is biased towards slower \emph{temporal} motions that are more likely to occur in nature than faster ones~\cite{weiss2002motion}. Many machine learned models are able to leverage relationships along both spatial and temporal axes when being trained to solve visual tasks. However, explicitly constructing networks that take advantage of different scales can still be effective~\cite{feichtenhofer2019slowfast,chunghierarchical}. For example, SlowFast the \cite{feichtenhofer2019slowfast} network uses a two branch architecture in which each branch models different temporal scales. While achieving strong results on action recognition, this architecture has the added advantage that the channel capacity of the fast branch can be reduced, leading to efficiency gains.

Multi-scale learning of this type has garnered attention, in part due to inspiration from neuroscience and neuromodulatory systems~\cite{mei2022informing}. Neural networks are often highly effective for specific tasks, but poor at generalizing across tasks or adapting to different domains~\cite{yang2022multi}.
Novel multi-scale approaches which can capture the latent hierarchical structure in sequences offer attractive properties and the potential to generalize across tasks~\cite{chunghierarchical}.
Global and local features have been successfully employed in video representation tasks for creating models that perform well at image object detection~\cite{xie2021detco}, sequence classification (e.g., action recognition)~\cite{yang2023simper}, and fine-grained temporal understanding (e.g., lip reading)~\cite{zeng2021contrastive}.


%\textcolor{red}{Informing deep neural networks by multiscale principles of neuromodulatory systems}~\cite{mei2022informing}
%\textcolor{red}{ (Despite the demonstrated effectiveness of deep neural networks for specific tasks, they remain relatively inflexible at generalizing across tasks or adapting to ever-changing behavioral demands. In this article, we provide an overview of neuromodulatory systems and their relationship to emerging pertinent principles in deep neural networks. )}
%Are similar advantages available for different spatial scales?  

The measurement of human physiology also requires understanding of processes with different spatial and temporal features and dynamics. For example, \textbf{\textit{facial actions}} (muscle movements) are idiosyncratic, localized and sporadic, whereas the \textbf{\textit{human pulse}} is almost invariably present in nearly all skin tissue while being highly periodic. \textbf{\textit{Respiration}} or \textbf{\textit{breathing}}, on the other hand, lies somewhere in between, being generally periodic but occasionally irregular, and is only measurable from certain parts of the body (e.g., chest or abdomen). It would seem that the optimal spatial and temporal features for measuring these signals would differ. However, facial expressions and cardio-pulmonary signals do have shared properties, they are all controlled in part by the autonomic nervous system~\cite{ekman1983autonomic}, they are all measured via analysis of the body, and more specifically can be captured through examination of the human face~\cite{martinez2017automatic,mcduff2021camera}. Thus, even though the low level feature representation of these tasks may seem dissimilar, it may be possible to concurrently learn all these features from a single input modality, and thus suggests that shared information at some scales might benefit performance. Despite these links, there is no empirical evidence to validate or invalidate this hypothesis in video measurement.

%%%%%%%%%%%%%%%%% FIGURE: BS MODEL ITERATIONS %%%%%%%%%%%%%%%%%
\begin{figure*}[h]
    \begin{center}
    \includegraphics[width=6.8in]{figures/big_small_iterations_3.pdf}
    \end{center}
    \caption{\textbf{Model Architecture Iterations of BigSmall.} During our research we designed several candidate networks for multi-task prediction of PPG, breathing, and facial action. These iterations are discussed in Section \ref{sec:methods} and in Section \ref{sec: ablation} (ablation studies).}
    \label{fig:model_its}
\vspace{-0.1cm}
\end{figure*}

%"To better exploit the temporal contextual and periodic rPPG clues, we also extend the PhysFormer to the two-pathway SlowFast based PhysFormer++ with temporal difference periodic and cross-attention transformers. ".~\cite{yu2023physformer++}

Concretely, remote measurement of the human pulse, via photoplethysmography (PPG), leverages aggressive spatial averaging to boost the signal-to-noise ratio of the subtle changes in blood flow present in video pixels~\cite{poh2010advancements,wang_algorithmic_2017}. Even neural networks apply this considerable spatial downsampling to the input frames~\cite{chen2018deepphys,lu_dual-gan_nodate}. Leveraging temporal information is very valuable with temporal models significantly out performing frame-based counterparts~\cite{yu2019remote,liu2020multi,yang2023simper}.
Computer vision-based facial action recognition on the other hand requires higher spatial resolution features and treats frames as uncorrelated. Much more modest gains have been observed in automated facial action recognition using temporal models. %In some cases, performance is even worse than a frame-by-frame model~\cite{}.

Although computer vision based facial action and physiological measurements have both received attention individually, there has been little exploration of multi-task models that predict multiple signals. This is surprising given the clear commonalities between the two tasks: they correspond to the same regions of the body (face), and there are some correlations between the two signals, such as in the case of emotional experiences that elicit expressions and physiological changes \cite{burzo2012towards,d2015review}.


% While computer vision measurement of facial action and physiological signals have received attention on their own there is little work exploring multi-task models that predict both. The is despite the clear commonalities between the two tasks: they correspond to the same regions of the body (face) and there are some correlations between the two signals (e.g., in the case of emotional experiences that elicit expressions and physiological changes)~\cite{burzo2012towards,d2015review}. 
%[CITE PAPERS THAT SHOW MULTIMODAL RECOGNITION IS BETTER THAN UNIMODAL] 

In this paper, we propose \textbf{BigSmall}, the first multi-task neural model for disparate spatial and temporal human physiological measurements.
% We present a two-path spatial and temporal scale network and evaluate this architecture on the tasks of vision-based facial action, respiration, and pulse measurements.
Specifically, BigSmall is comprised of a ``Big'' branch with high-resolution input for deriving \textit{\textbf{spatial}} texture features, and a ``Small'' branch, with extremely low-resolution inputs that compress noise from spatial features, which models \textit{\textbf{temporal}} dynamics. 
% We hypothesize that each task will benefit from a different balance of information at different temporal-spatial frequencies, and
We demonstrate empirically that leveraging such properties leads to both accuracy and efficiency gains via a unified model.
To reduce the compute overhead, we propose \textit{mixed spatial and temporal scales}, which leverages spatiotemporal properties of branch inputs to improve computational efficiency by more than 60\%.
Finally, we develop an efficient temporal modeling technique, \textit{{Wrapping Temporal Shift Module (WTSM)}} to improve temporal feature representation, particularly when only a limited number of frames are available.
Extensive evaluations on the tasks of vision-based facial action, respiration, and pulse measurements demonstrate both improved accuracy and efficiency of BigSmall compared to state-of-the-art (SOTA) methods.

To summarize, we make the following contributions:
\vspace{-0.15cm}
\begin{Itemize}
    \item We present BigSmall, the first \emph{multi-task model for disparate spatial \& temporal human physiological measurements}, using a unified two-path spatiotemporal network.
    \item We propose \emph{mixed spatial and temporal scales} for efficient spatiotemporal modeling while maintaining accuracy.
    \item We develop the \emph{Wrapping Temporal Shift Module} for effective temporal learning, especially when limited number of input frames are available.
    \item We evaluate BigSmall on three physiological vision tasks across multiple real-world video-based human physiology datasets and verify the effectiveness of BigSmall over SOTA methods.
\end{Itemize}
% \vspace{-0.1cm}
% 1) Introducing a novel efficient multi-task neural architecture called BigSmall that is comprised of a Big branch with high-resolution input for deriving spatial texture features, and a Small branch with extremely low-resolution input for compressing noise from spatial features and modeling temporal dynamics. 
% 2) Combining different temporal and spatial scales as inputs to improve computational efficiency by 66\%. 
% 3) Developing an efficient temporal modeling technique called Wrapping Temporal Shift Module (WTSM) to improve temporal representation modeling, particularly when only a limited number of frames are available.
% 4) Evaluating the effectiveness of our proposed method on three physiological vision tasks, including action unit detection, video-based pulse measurement, and video-based respiratory measurement.

We release our code, trained models, and a simple interface to simultaneously generate facial action unit (AU), heart rate, and breathing rate measurements from video.



%%%%%%%%%%%%%%%%% BACKGROUND %%%%%%%%%%%%%%%%%

\section{Background and Related Work}

%%%%%%%%%%%%%%%%% BACKGROUND: MULTISCALE MODELS %%%%%%%%%%%%%%%%%
\textbf{Multi-Scale Models.}
Scales in networks can take several forms. Global representations often refer to those for tasks such as classification or a whole video sequence, where as local representations refer to those for detection or localization of specific elements within video frames. Hjelm and Backman assume that information useful for action classification (i.e., global semantics) should be invariant across space and time within a given video~\cite{hjelm2018learning,hjelm2020representation}. 
The concept of leveraging global and local features has drawn attention~\cite{xie2021detco,zeng2021contrastive}. Zeng et al. argue that feature representations can be learnt that generalize to tasks which require global information and those that require local fine-grained spatio-temporal information (e.g., localization)~\cite{zeng2021contrastive}.
SlowFast takes an analogous approach in the temporal domain~\cite{feichtenhofer2019slowfast}, using two branches to model different frequency scales.  Exploiting temporal and spatial scales has been effective in the case of rPPG by implementing a SlowFast transformer network~\cite{yu2023physformer++} and leveraging global-local spatial features~\cite{zhao2023learning}. However, to our knowledge prior work has not applied these concepts to disparate multi-task physiological measurements.

%%%%%%%%%%%%%%%%% BACKGROUND: FACS %%%%%%%%%%%%%%%%%
\textbf{Facial Action Recognition.}
The Facial Action Unit Coding System (FACS)~\cite{Cohn2007,ekman1997face} decomposes facial movements into muscle activations called action units (AUs). This system of encoding features has been leveraged to correlate facial displays to expressions of human emotion (e.g., activation of AU6 - cheek raiser, and AU12 - lip corner puller, together result in a smile or an expression of happiness). Automating FACS using computer vision has a long history due to the laborious and time intensive nature of manual coding~\cite{martinez2017automatic, cohn2014automated}. Recent research has been focused on using deep neural networks for detecting AUs~\cite{gudi2015deep, jaiswal2016deep, benitez2017recognition, niu2019local}. These models are customized to the task, and make use of high spatial resolution inputs. Additionally, to achieve current state-of-the-art performance, most of the published methods make adaptations to their CNN architectures to utilize additional features for the representation learning \cite{corneanu2018deep, niu2019local, li2019self}.



%%%%%%%%%%%%%%%%% BACKGROUND: CAMERA BASE PHYS MEASUREMENT %%%%%%%%%%%%%%%%%
\textbf{Camera-based Physiological Measurement.}
Measurement of physiological parameters from video is possible as light reflected from the body is modulated by several physiologic processes~\cite{mcduff2021camera}. Remote photoplethysmography (rPPG) leverages subtle changes in light reflected from the body to measure the blood volume pulse~\cite{verkruysse2008remote,poh2010advancements,wang_algorithmic_2017}. Supervised neural networks are the current state-of-the-art for rPPG measurement~\cite{chen2018deepphys,yu2019remote,yu2021transrppg,liu2020multi,gideon2021way,yu2022physformer,yu2023physformer++}. There are some inductive biases that have informed the design of these models. Firstly, since the cardiac pulse is relatively invariant across neighboring skin regions, the input video frames can be aggressively spatially downsampled. This has the effect of boosting the pulse signal-to-noise ratio as camera quantization errors begin to average out. Secondly, the pulse signal has characteristic temporal structure and periodicity, therefore implying the benefit of modeling this temporal information~\cite{yu2019remote,nowara_benefit_2020,liu2020multi,liu2023efficientphys, yu2023physformer++}.






%%%%%%%%%%%%%%%%% METHODS %%%%%%%%%%%%%%%%%
\section{Methods}
\label{sec:methods}
\begin{figure*}[t!]
    \begin{center}
    \includegraphics[width=\textwidth]{figures/BSSF_WTSM_Implementation.pdf}
    \end{center}
    \caption{\textbf{BigSmall Model Architecture.} By mixing spatial and temporal scales and leveraging Wrapping Temporal Shifts we present an end-to-end efficient multi-task architecture for modeling disparate spatial and temporal physiological signals. }
    \label{fig:BSSFWTSM_architecture}
\vspace{-0.3cm}
\end{figure*}

%%%%%%%%%%%%%%%%% METHODS: MULTISCALE ANALYSIS %%%%%%%%%%%%%%%%%
\subsection{Modeling Disparate Spatiotemporal Signals}
\label{sec: model_spatialtemporal}
We explore the challenges of learning spatially and temporally disparate tasks, which are perceived at different spatiotemporal scales. For instance, learning periodic physiological signals, such as pulse, requires rich temporal information, relatively high frame rate, and relies on low image resolution to filter irrelevant high-frequency spatial noise~\cite{verkruysse2008remote,poh2010non}. Conversely, capturing muscle activation features, such as facial actions, demands high spatial resolution to detect subtle texture changes~\cite{lucey2007investigating}. These activations change more slowly meaning high temporal frequency information is less important. In fact, image-based classification tasks benefit from training with randomized mini-batches to maximize variance in the data and minimize correlation between individual frames, further underscoring the contrast between spatial and temporal tasks. Finally, breathing, traditionally approached through spatiotemporal methods such as optical flow, can be seen as a time-varying, often periodic task that leverages spatial information (e.g., body motion). Specifically, respiration models leverage higher spatial resolution inputs than rPPG (e.g., Chen et al. use 123$\times$123px for respiration and 36$\times$36px for rPPG~\cite{chen_deepmag_2020}).

To address the range of temporal and spatial scales needed to model these tasks, we propose a multi-task model architecture named ``BigSmall'' with a high-spatial-resolution branch to capture spatial texture and a low-spatial-resolution temporal branch to capture temporal dynamics. We leverage spatiotemporal scales to reduce the computation of the model, and introduce a novel and efficient technique called Wrapping Temporal Shift Modules (WTSM) to perform temporal modeling within limited amounts of temporal information. Below we describe the building blocks of this design and our technical contributions, which not only reduce computational costs but also improve temporal modeling.


%%%%%%%%%%%%%%%%% METHODS: BIG PATHWAY %%%%%%%%%%%%%%%%%
\subsection{Big: High Resolution Spatial Branch}

BigSmall's Big branch is designed to handle tasks that require high spatial fidelity, such as classifying facial actions. To preserve subtle facial edges and textures that comprise many action units, a large input frame resolution is necessary. Unlike the Small branch, the Big branch is not concerned with modeling temporals and generally treats frames as independent. The inputs to the Big pathway are high-resolution standardized raw frames of size $C\times H\times W$ = 3$\times H_{\text{big}}$$\times W_{\text{big}}$. In Fig. \ref{fig:model_its}A.i, we summarize the architecture of the Big pathway. The large, raw frames are passed through six convolutional layers with filter depths of [32, 32, 32, 64, 64, 64], and three average pooling layers with dropout, one after every other convolutional layer. 
% We also add fully connected layers for prediction. This network serves as a baseline for spatial tasks due to its ability to model spatial features.

%%%%%%%%%%%%%%%%% METHODS: SMALL PATHWAY %%%%%%%%%%%%%%%%%
\subsection{Small: Low Resolution Temporal Branch}

The small branch of the BigSmall model is optimized for tasks that rely on changes between frames rather than specific spatial details. Our proposed Small pathway leverages the fact that temporal tasks require extremely low spatial resolutions, which effectively filters out spatial noise. The Small pathway receives an input image of relatively low resolution defined by $C\times H\times W$ = 3$\times H_{\text{small}}$$\times W_{\text{small}}$. The input is provided in the form of ``normalized difference frames'', where each input frame represents the difference between a frame $K$ and the subsequent frame $K+1$. This type of input has been historically used in video-based physiological measurement networks \cite{chen2018deepphys, liu2020multi} to encode rich temporal information between adjacent time samples. In our evaluation, difference frames encode the pulse signal as changes in color, and motion caused by pulmonary function as edges, as shown in Fig. \ref{fig:model_its}A.ii. These downsampled difference-frames are fed through four convolutional layers with filter depths of [32, 32, 32, 64] in sequence.


% As the Small pathway is designed to best model temporal dynamics this architecure, with appended fully connected layers for prediction, will be used as a baseline model for more temporal tasks. 

%%%%%%%%%%%%%%%%% METHODS: IMPROVING COMPUTE / SLOWFAST %%%%%%%%%%%%%%%%%
\subsection{Mixing Temporal and Spatial Scales}

Although a model comprised of a fused \emph{Big} pathway and \emph{Small} pathway is able to learn disparate spatio-temporal signals in a multi-task fashion, it provides minimal computational benefit over separate task-optimized networks. For example, such a multi-task network used to predict AU, respiration, and PPG barely improves upon the parameters and floating point operations required to run a big spatial model (for AU) and two small temporal models (for respiration and PPG). Inspired by the SlowFast network ~\cite{feichtenhofer2019slowfast} which models slow and fast temporal scales in two branches, we propose incorporating different temporal scales on top of our spatial scales to help improve the computational footprint. 

Image-based classification models generally assign temporal independence between frames, as slow-changing spatial features result in a lack of interesting temporal dynamics. In the case of facial action, where changes occur over seconds, AU activations stay stagnant for a number of consecutive video frames. Thus, such spatial tasks can be reframed as \textit{slow} temporal tasks, where consecutive frames are highly correlated. On the other hand, PPG estimation and similar tasks rely on subtle changes of consecutive low-resolution frames and thus such spatial tasks can be reframed as \textit{fast} temporal tasks. 

In BigSmall, as input resolutions of the Big branch are far larger than that of the Small branch, computational load is dominated by convolutions in the Big branch. The ratio of floating point operations (FLOPs) between the Big and Small branches is approximately $( H_{\text{big}}W_{\text{big}})/( H_{\text{small}}W_{\text{small}})$ where $H$ and $W$ denote image height and width. If $ H_{\text{big}}$ and $W_{\text{big}}$ are much larger than $ H_{\text{small}}$ and $W_{\text{small}}$, the computational burden of the model is driven by the Big branch alone. To address this, we leverage the Big branch as a mechanism to model low-frequency high-spatial resolution signals, while using the small branch to model high-frequency low-resolution signals. By temporally downsampling the input frames to the Big pathway, we reduce the number of frames passed through the Big branch compared to the number of frames passed to the Small branch. Specifically, we reduce the frames seen by the Big branch to $N/M$ frames passed to Small branch where $M \in \mathbb{Z}^+ > 1$ is the reduction factor and $N$ is the number of frames in the Small branch. The comparison is illustrated in Figs. \ref{fig:model_its}C and F. As a result, we reduce the FLOPs executed by the model by approximately $M$ times on average for $N$ frames. This reduction in computational cost is particularly useful in situations where the high-resolution input data significantly increases the model's computational footprint.

More specifically, the Big pathway only receives a single frame ($M$=N) to predict AU activation's while the Small branch receives $N$ frames branch to predict PPG and respiration signals. The Big branch leverages the learned temporal representation and dynamics from the Small branch to infer minor changes in the spatial features of $N$ frames. Heavy reliance on the temporal pathway leads to a small drop in performance, but with the benefit of $1/N$ computation cost where $N$ is the number of frames passed to the Small branch as well as the reduction factor. 
 
 %%%%%%%%%%%%%%%% FIGURE: BIGSMALL ARCHITECTURE %%%%%%%%%%%%%%%%%
 % Additionally, we note that this architecture does not reduce the number of parameters in the model, which is only slightly smaller than the parameters used by independent task-optimized models. We further note, that downsampling the Big branch has the limitation of requiring a series of $N$ inputs before the model is able to produce $N$ frames worth of outputs. This inference latency can be alleviated by using sufficiently small $N$ consecutive frames.
 
 % it must make use of the Small branch to successfully predict spatial task outputs for all frames. It does so by using the modeled temporal dynamics to infer minor changes in spatial features. This heavy reliance on the temporal pathway leads to some small drop performance, but with the benefit of $1/N$ computation cost. Additionally, we note that this architecture does not reduce the number of parameters in the model, which is only slightly smaller than the parameters used by independent task-optimized models. We further note, that downsampling the Big branch has the limitation of requiring a series of $N$ inputs before the model is able to produce $N$ frames worth of outputs. This inference latency can be alleviated by using sufficiently small $N$ consecutive frames.

% The reduction in frames passed through the Big pathway results in a significant compute benefit.  Thus, by reducing the frames seen by the Big branch by a factor of $N$, we reduce the number of FLOPs executed by the model by a factor of almost $N$ (on average for $N$ frames). 

% Regarding spatial scale: in convolutional architectures, the number of floating-point operations (FLOPs) is driven by the convolutional layers, where the compute is proportional to the spatial resolution of the input. 


%%%%%%%%%%%%%%%%% METHODS: IMPROVING TEMPORAL MODELING / WTSM %%%%%%%%%%%%%%%%%
\subsection{Wrapping Temporal Shift Module}
\label{sec: wtsm}

% The BigSmall model is designed to learn and predict multiple spatiotemporally disparate signals. The Big pathway provides a robust spatial embedding based on high-resolution inputs. The Small pathway models temporal dynamics by leveraging low-resolution difference-frame inputs, which filter spatial features through aggressive downsampling and provide inter-frame temporal context. 


Modeling temporal dynamics and representing beyond consecutive frames are crucial for video-based physiological measurement tasks, as noted in previous works \cite{yu2019remote, liu2020multi}. However, the Small branch's difference-frame inputs only allow information sharing between adjacent frames. To address this, Liu et al. \cite{liu2020multi} leverage the temporal shift module (TSM) \cite{lin2019tsm} instead of 3D convolution to learn efficient spatial-temporal representation beyond adjacent frames. 


However, traditional TSM suffers when the number of input frames ($N$) is low, as the proportion of zeroed-features rises. As shown in Fig. \ref{fig:TSM_compare}, when traditional bi-directional TSMs operate on $N$ frame samples (indexed [1, $N$]), the first frame shifts a fold of channels right (forward in time) to the second frame. Since there is no previous frame, this fold is zero-padded in the first frame. Similarly, frame $N$ shifts a fold of channels left (backward in time) to frame $N-1$. That fold in frame $N$ is then zero-padded as there is no next frame. The proportion of zeroed features in $N$ frames is thus $2/(3N)$. As $N$ becomes small, the proportion of zeroed features grows rapidly, leading to a noticeable degradation in performance.

%%%%%%%%%%%%%%%%% FIGURE: TSM vs WTSM COMPARISON %%%%%%%%%%%%%%%%%
\begin{figure}[t!]
    \begin{center}
    \includegraphics[width=3.2in]{figures/WTSM_TSM_DJM.pdf}
    \end{center}
    \caption{\textbf{Wrapping Temporal Shift Modules.} A comparison of temporal shift modules (TSM) and wrapping temporal shift modules (WTSM). For modeling time variant signals with small window sizes, we find that WTSMs provide superior performance.}
    \label{fig:TSM_compare}
\vspace{-0.2cm}
\end{figure}

The problem of zero padding becomes particularly concerning when $N$ is \textbf{unavoidably small} due to strict latency requirements or when low inter-frame correlation is necessary for learning. In such cases, image-based classification tasks, like AU detection, benefit from high variance in mini-batches, which is achieved through frame randomization. However, as consecutive video frames tend to have highly similar spatial features and labels, using a large number of consecutive frames as input for training reduces batch variance and, therefore, degrades the model's performance.  

To address this challenge, we propose the Wrapping Temporal Shift Module (WTSM). In contrast to the TSM, the WTSM fills the zero-padded channels with channels shifted out during the temporal shift process. As illustrated in Fig. \ref{fig:TSM_compare}A, traditional TSM shifts out a fold of channels from both the first and last time frame, while simultaneously zero-padding a fold in the same frame. The WTSM resolves the problem of zero-padded channels by wrapping the shifted-out folds to fill the previously zero-padded folds. The WTSM, as shown in Fig. \ref{fig:TSM_compare}B, can leverage the inter-frame temporal benefits of TSM without increasing the proportion of zeroed features when using a small $N$, thereby avoiding a drop in performance. Furthermore, like TSM, WTSM does not increase parameter or FLOP count incurred by the model.

It is worth noting that, unlike long short-term memory networks (LSTMs), the temporal information added by WTSM is not dependent on time-series order. The WTSM helps 2D convolutions learn a time-invariant mapping that maps an input to an output relative to other input-output pairs. As a result, shared features between non-adjacent frames (such as the 1st and $Nth$ frames) do not disturb the temporal representation. Furthermore, wrapping features, as opposed to filling from intermediate frames, best balances the information represented for all $N$ frames.






% TSMs work by shifting a portion of the channels between adjacent samples along the time axis. More specifically, $\forall N$ frames, Bi-direction TSMs split the channels of a sample into 3 folds, shift the first fold of a sample $K$ to the previous sample $K-1$, shift the second fold of $K$ to the following sample $K+1$, and retain the final fold of at the sample $K$. These modules, placed before convolutional layers, help a model learn deeper temporal dynamics by exposing samples to information from additional frames. In this way, TSMs provide the spatio-temporal functionality of a 3D convolutional filter (which convolves across spatial and temporal dimensions), with out incurring overhead compute costs. TSMs add no additional parameters or floating point operations to a model (as verified in the original publication). The Bi-directional TSM module is visualized in Fig. \ref{fig:TSM_compare}A.

% Traditional TSMs suffer when the number of consecutive frames, $N$, is low, as the portion of zeroed-features rises. Assume a Bi-Direction TSM operating on $N$ frame samples (indexed [1,$N$]). Frame $1$ shifts a fold of channels right (forward in time) to frame $2$. This fold is zero-padded in frame $1$ as there is no previous frame. Similarly, frame N shifts a fold of channels left (backward in time) to frame $N-1$. That fold in frame $N$ is then zero padded as there is no next frame. The proportion of zeroed features in $N$ frames is thus $2/(3N)$. When the $N$ becomes small the proportion of zeroed features grows rapidly causing a noticeable degradation in performance. 




%%%%%%%%%%%%%%%%% METHODS: BIGSMALL %%%%%%%%%%%%%%%%%
\subsection{The BigSmall Model}

By combining the techniques proposed in Section \ref{sec: model_spatialtemporal} to Section \ref{sec: wtsm}, we present an end-to-end efficient multi-task architecture, called BigSmall, for disparate spatial and temporal signals (see Fig. \ref{fig:BSSFWTSM_architecture}). The proposed architecture leverages a dual pathway system consisting of 1) a Big branch to model fine-grain spatial features from raw high-resolution inputs, and 2) a Small branch optimized for modeling temporal dynamics from low-resolution difference-frames. To achieve computational efficiency, $N$ frames are passed into the Small branch while only $1$ frame is passed through the Big branch. This reduction in convolutions in the Big branch leads to a significant reduction in computation by almost a factor of $N$, as computation of the Small branch is negligible compared to that of the Big branch.

BigSmall benefits from the use of WTSM, which enables robust derivation of temporal information by passing features between frames. WTSMs are placed before convolutional layers in the Small branch and, when combined with difference-frame inputs, facilitate strong inter-frame feature mapping. This feature is particularly advantageous when training alongside spatial tasks that require high batch-variance or situations that demand low latency. Additionally, WTSM helps to alleviate the strain put on the temporal branch to infer missing spatial features resulting from temporal down-sampling in the Big branch, by augmenting the temporal representation.

% The BigSmall architecture presented in this paper is used to concurrently learn facial action (a spatial task), pulse via PPG (a temporal task), and breathing (a spatiotemporal task). An example of the BigSmall architecture designed for multi-tasking with AU and physiological signals is illustrated in Fig. \ref{fig:BSSFWTSM_architecture}.


 
%%%%%%%%%%%%%%%%% EXPERIMENTS %%%%%%%%%%%%%%%%%
\section{Experiments}
We evaluate our methods on the tasks of facial action, rPPG, and respiration. We run a series of ablation experiments on the BigSmall model to highlight individual contributions, and compare our model against previously published task-optimized architectures. We train and validate presented models using the BP4D+ dataset~\cite{zhang2013high, zhang2014bp4d, zhang2016multimodal}.

\textbf{Dataset.}
The BP4D+, a large multimodal emotion dataset, consists of face video (25fps) from 140 participants (82 female, 58 male). Each participant records 10 trials, each of which is meant elicit a specific emotional response: \textit{happiness}, \textit{surprise}, \textit{sadness}, \textit{startle}, \textit{skepticism}, \textit{embarrassment}, \textit{fear}, \textit{pain}, \textit{anger}, \textit{disgust}. These trials are labeled with the following signals: blood pressure (systolic/diastolic/mean/bp wave), heart rate, respiration (rate/wave), electrodermal activity. Trials 1/6/7/8 are FACs encoded for the most "facially expressive" portion. We refer to the portion of the dataset with AU labels as the AU subset (consisting of ~200k frames). This AU subset is the only portion of the dataset with concurrent AU, respiration, and PPG labels. We additionally evaluated BigSmall on two public rPPG datasets: UBFC \cite{bobbia2019unsupervised} and PURE \cite{stricker2014non}. Details regarding evaluations on additional datasets are included in the supplementary materials. 

\textbf{Experimental Details.}
Similar to \cite{kaili2016deep}, we use 3-fold cross validation, training on 2 folds and testing on the third, and report the average performance on the holdout-sets. Due to the sparsity of AU labels and the conflicting nature of the task gradients (explained in Section \ref{sec: result_multi_task}), networks are trained on folds from the AU Subset, and validated on the rest of the data from the entire BP4D+ dataset for PPG and breathing tasks. Folds are constructed as to not include subject overlap between train and test sets. Models are trained for 5 epochs, using video chunks of $N$ = 3 consecutive frames, a batch of 540 frames, an Adam optimizer, and a learning rate of 0.001. The AU multi-label classification task is trained using weighted Binary Cross Entropy Loss. Respiration and PPG are trained with Mean Squared Error Loss. The losses of all 3 tasks are equally weighted and summed to promote equal importance during training. We evaluate binary action unit performance on 12 commonly cited AUs \cite{kaili2016deep} using average F1 and accuracy. PPG and breathing metrics are based on the signal rate (beats/breaths per minute), and for each task we report Mean Average Error (MAE), Root Mean Square Error (RMSE), Mean Average Percent Error (MAPE), and Pearson Correlation ($\rho$). Additional information regarding training, metrics, and their derivation can be found in the supplementary material. We adapt our training pipeline from rPPG-Toolbox~\cite{liu2022rppgtoolbox}, a toolkit to standardize rPPG deep learning research. 

\begin{table*}[t!]
\setlength\tabcolsep{4pt}
\caption{\textbf{Ablation Studies of BigSmall.} The default BigSmall model is highlighted in \textcolor{rowgray}{gray}. Best results of each column are in \textbf{bold}.}
\vspace{-1.5em}
\small
\begin{center}
\resizebox{1\textwidth}{!}{
\begin{tabular}{lccccccccccccccccc}
\toprule[1.5pt]
\multicolumn{1}{l}{\multirow{2.5}{*}{\textbf{\begin{tabular}[c]{@{}c@{}}Model\\\end{tabular}}}} & 
\multicolumn{1}{c}{\multirow{2.5}{*}{\textbf{\begin{tabular}[c]{@{}c@{}}Big\\Branch\end{tabular}}}} & 
\multicolumn{1}{c}{\multirow{2.5}{*}{\textbf{\begin{tabular}[c]{@{}c@{}}Small\\Branch\end{tabular}}}} & 
\multicolumn{1}{c}{\multirow{2.5}{*}{\textbf{\begin{tabular}[c]{@{}c@{}}Temporal Shift\\Mechanism\end{tabular}}}} & 
\multicolumn{1}{c}{\multirow{2.5}{*}{\textbf{\begin{tabular}[c]{@{}c@{}}Big Branch Temporal\\ Down Sampling\end{tabular}}}} & 
\multicolumn{4}{c}{\textbf{Heart Rate}} & 
\multicolumn{4}{c}{\textbf{Breathing Rate}} & 
\multicolumn{2}{c}{\textbf{AU Avg.}} & 
\multicolumn{2}{c}{\textbf{Computation}} \\ 
\cmidrule(l){6-9} \cmidrule(l){10-13} \cmidrule(l){14-15} \cmidrule(l){16-17}
&&&& & MAE & RMSE & MAPE & $\rho$  & MAE & RMSE & MAPE & $\rho$  & F1 & Acc & FLOPS (M) & \# Params (M) \\ \midrule\midrule
\grayrow \textbf{BigSmall} & \checkmark & \checkmark& WTSM & \checkmark & \textbf{2.38} & \textbf{6.00} & \textbf{2.71} & \textbf{0.89} & \textbf{3.39} & \textbf{5.00} & \textbf{16.65} & \textbf{0.21} & 43.3 & 67.4 & {154.01} & 2.14 \\[1.2pt]
BigSmall &\checkmark & \checkmark &TSM & \checkmark & 3.03 & 7.27 & 3.50 & 0.85 & 3.59 & 5.20 & 17.63 & 0.17 & 43.0 & 67.3 & {154.01} & 2.14 \\[1.2pt]
BigSmall &\checkmark & \checkmark & $-$ & \checkmark & 2.47 & 6.16 & 2.81 & 0.88 & 3.65 & 5.21 & 17.80 & 0.16 & 40.3 & 62.3 & {154.01} & 2.14 \\[1.2pt]
BigSmall &\checkmark & \checkmark &WTSM & $-$ & 2.46 & 6.09 & 2.81 & 0.88 & 3.71 & 5.28 & 18.00 & 0.15 & 42.5 & 60.6 & 456.03 & 2.14 \\
Big Branch &\checkmark & $-$ & $-$ & $-$ &$-$ &$-$&$-$&$-$&$-$&$-$&$-$&$-$& \textbf{45.3} & \textbf{73.8} & 451.63 & 0.78 \\
Small Branch &$-$ & \checkmark & $-$ & $-$ & 2.57 & 6.57 & 2.90 & 0.87 & 3.86 & 5.39 & 18.79 & 0.12 & $-$ & $-$ & 3.73 & 0.70 \\
\bottomrule[1.5pt]
\end{tabular}
}
\end{center}
\vspace{-0.5cm}
\label{tab:ablation}
\end{table*}

\textbf{BigSmall Instantiation.}
The BigSmall model input dimension are chosen to highlight the different spatial scales of the two branches, and to further highlight the benefits of reducing the computation of the Big branch. The Big branch raw standardized input frames are of shape $C$$\times$$H$$\times$$W$ = 3$\times$144$\times$144. Small branch normalized difference frame inputs are of shape $C$$\times$$H$$\times$$W$ = 3$\times$9$\times$9. The pooling layers of the Big branch are of pool size [2$\times$2, 2$\times$2, 4$\times$4], in order. These pooling sizes are chosen such that the final convolutional output of the Big pathway matches that of the Small pathway, in an effort to balance the feature importance of the branches.

The Big and Small feature maps are combined through upsampling of the Big output and summation in order to prevent extremely large fully connected layers (an artifact of concatenating the Big and Small feature maps before the dense layers) and thus avoid additional mode complexity. We explore the use of lateral connections and alternative fusion techniques (discussed in the supplementary material), but find for our tasks, of AU, respiration, and PPG, that mixing high level features, or forcing the combination of low-level features results in performance degradation. As discussed in Section \ref{sec: result_multi_task}, this is due to the conflicting gradients of the spatial task (AU) with the temporal task (PPG and breathing). The combined Big and Small feature map is passed to fully connected layers for each learned task.

To match the inputs to the BigSmall model, PPG and respiration baseline models are fed 9$\times$9 difference-frame inputs, while AU baselines are trained with 144$\times$144 standardized raw inputs. 




%%%%%%%%% Comparison w/ literature table
\begin{table*}[h]
\setlength\tabcolsep{8pt}
\caption{\textbf{Comparisons of BigSmall \emph{vs.} SOTA Methods.} BigSmall enables both spatial and temporal human physiological learning simultaneously via a unified model. Supervised models are 3-fold cross validated on BP4D+. Best results of each column are in \textbf{bold}.}
\vspace{-0.5em}
\small
\begin{center}
\resizebox{0.99\textwidth}{!}{
\begin{tabular}{lccccccccccc}
\toprule[1.5pt]
\multicolumn{1}{l}{\multirow{2.5}{*}{\textbf{\begin{tabular}[c]{@{}c@{}}Method\\\end{tabular}}}} & 
\multicolumn{1}{c}{\multirow{2.5}{*}{\textbf{\begin{tabular}[c]{@{}c@{}}Task Type\\\end{tabular}}}} & 
\multicolumn{4}{c}{\textbf{Heart Rate}} & 
\multicolumn{4}{c}{\textbf{Breathing Rate}} & 
\multicolumn{2}{c}{\textbf{AU Avg.}} \\
\cmidrule(l){3-6} \cmidrule(l){7-10} \cmidrule(l){11-12}
& & MAE & RMSE & MAPE & $\rho$  & MAE & RMSE & MAPE & $\rho$  & F1 & Acc \\ 
\midrule\midrule
\grayrow \textbf{BigSmall} & \textbf{Spatial + Temporal} & 2.38 & 6.00 & \textbf{2.71} & \textbf{0.89} & \textbf{3.39} & \textbf{5.00} & \textbf{16.65} & \textbf{0.21} & 43.3 & 67.4 \\
\midrule
Small Branch & \multicolumn{1}{c}{\multirow{5}{*}{{\begin{tabular}[c]{@{}c@{}}Temporal\end{tabular}}}}  & 2.57 & 6.57 & 2.90 & 0.87 & 3.86 & 5.39 & 18.79 & 0.12 & $-$ & $-$\\[1.2pt]
MTTS-CAN~\cite{liu2020multi} & & 2.86 & 7.19 & 3.27 & 0.85 & 3.88  & 5.54 & 18.88 & 0.11 & $-$ & $-$\\[1.2pt]
DeepPhys~\cite{chen2018deepphys} & & \textbf{2.37} & \textbf{5.97} & 2.72 & 0.88 & $-$ & $-$ & $-$ & $-$ & $-$ & $-$\\[1.2pt]
POS~\cite{wang2016algorithmic} &  & 10.40 & 19.53 & 9.73 & 0.41 & $-$ & $-$ & $-$ & $-$ & $-$ & $-$\\[1.2pt]
CHROM~\cite{de2013robust} && 5.27 & 13.28 & 5.12 & 0.69 & $-$ & $-$ & $-$ & $-$ & $-$ & $-$\\
\midrule
Big Branch & \multicolumn{1}{c}{\multirow{3}{*}{{\begin{tabular}[c]{@{}c@{}}Spatial\end{tabular}}}}  & $-$ &$-$&$-$&$-$&$-$&$-$&$-$&$-$& \textbf{45.3} & 73.8 \\[1.2pt]
DRML~\cite{kaili2016deep} & & $-$ &$-$&$-$&$-$&$-$&$-$&$-$&$-$& 44.0 & \textbf{74.9} \\[1.2pt]
AlexNet~\cite{krizhevsky2017imagenet} & &  $-$ &$-$&$-$&$-$&$-$&$-$&$-$&$-$& 44.2 & 63.1 \\
\bottomrule[1.5pt]
\end{tabular}
}
\end{center}
\vspace{-0.2cm}
\label{tab:lit_compare}
\end{table*}



% %%%%%%%%%%%%%%%%% TABLE: BigSmall PURE/UBFC Eval %%%%%%%%%%%%%%%%%
\begin{table}
\setlength\tabcolsep{4pt}
\caption{\textbf{Evaluation on Public rPPG Datasets:} \textbf{UBFC} \cite{bobbia2019unsupervised} and \textbf{PURE} \cite{stricker2014non}. All (supervised) methods are trained on BP4D+. Best results of each column are in \textbf{bold}.}
\vspace{-1.2em}
\small
\begin{center}
\resizebox{.48\textwidth}{!}{
\begin{tabular}{lcccccccc}
\toprule[1.5pt]
\multicolumn{1}{l}{\multirow{2.5}{*}{\textbf{\begin{tabular}[c]{@{}c@{}}Method\\\end{tabular}}}} & 
\multicolumn{4}{c}{\textbf{PURE}} & 
\multicolumn{4}{c}{\textbf{UBFC}} \\
\cmidrule(l){2-5}\cmidrule(l){6-9}
& MAE & RMSE & MAPE & $\rho$  & MAE & RMSE & MAPE & $\rho$ \\ 
\midrule\midrule
\grayrow \textbf{BigSmall} & \textbf{1.97} & \textbf{6.48} & \textbf{2.56} & \textbf{0.93} & \textbf{1.03} & \textbf{2.55} & \textbf{1.14} & \textbf{0.99}\\[1.2pt]
MTTS-CAN \cite{liu2020multi} & 5.99 & 13.01 & 7.08 & 0.74 & 12.78 & 22.43 & 13.90 & 0.47  \\[1.2pt]
DeepPhys \cite{chen2018deepphys}& 4.73 & 11.83 & 5.81 & 0.78 & 3.36 & 12.86 & 3.37 & 0.69\\[1.2pt]
POS \cite{wang2016algorithmic} & 7.89 & 11.08 & 10.65 & 0.89 & 2.79 & 4.69 & 3.25 & 0.97\\[1.2pt]
CHROM \cite{de2013robust} & 7.29 & 10.33 & 10.06 & 0.90 & 3.13 & 5.11 & 3.68 & 0.97\\
\bottomrule[1.5pt]
\end{tabular}
}
\end{center}
\vspace{-0.4cm}
\label{tab:PURE_UBFC_Tests}
\end{table}




%%%%%%%%%%%%%%%%% FIGURE: SAMPLE OUTPUTS %%%%%%%%%%%%%%%%%
\begin{figure}[!t]
    \begin{center}
    \includegraphics[width=3.4in]{figures/prediction_outputs_v1_GVN_v2.pdf}
    \end{center}
    \caption{\textbf{Examples Outputs.} Attention masks, predicted signals, and relative training gradients. PPG and respiration share some gradient direction. The AU gradient conflicts with these tasks.}
    \label{fig:pred_out}
\vspace{-0.3cm}
\end{figure}

\section{Results and Discussion}

\subsection{Multi-Task AU and Physiological Measurement}
\label{sec: result_multi_task}
The BigSmall model is able to concurrently learn disparate spatiotemporal tasks. We show that the network enables multi-task measurement of facial action units, breathing, and PPG. Table~\ref{tab:ablation} illustrates that BigSmall performs comparatively to the baseline Big and Small single-task-optimized models, while reduing the computational load needed to run 3 task-specific models by $\sim$66\%. Fig.~\ref{fig:pred_out}-A/B/C show sample attention maps from the Big and Small branch, and PPG and respiration predicted waveforms plotted against the sensor ground truth. More examples are included in the supplementary material. 

Regression in AU results, as compared to the Big baseline, is explained by an analysis of the multi-tasked signals. Pulse and respiration signals are known to have shared information, in that respiration frequencies can be derived via respiratory sinus arrhythmia (RSA)~\cite{poh2010advancements}. Conversely, though AU may leverage some temporal dynamics modeled by the Small branch, the spatial feature representation is expected to share much less information with the PPG and respiration tasks. Indeed, we verify these hypotheses by observing the task-gradients during training~\cite{yu2020gradient, Pytorch-PCGrad}. The BigSmall task-gradient-vectors, calculated after the first training epoch, are shown in Fig.~\ref{fig:pred_out}-D. While PPG and respiration signal gradients project onto each other ($\angle \text{PPG}, \text{Resp}$ = 46.2$^{\circ}$), the gradients for the AU signal are much more different ($\angle \text{AU}, \text{Resp}$ = 106.3$^{\circ}$, $\angle \text{AU}, \text{PPG}$ = 100.3$^{\circ}$). This gradient conflict results in a degradation of AU results.
%We hypothesize that BS would show significant multi-task benefits when trained on more similar tasks and would better leverage more complex Big-Small fusion.

\subsection{Comparisons To SOTA Models}
We compare our BigSmall model against task-optimized models from the literature. Table \ref{tab:lit_compare} demonstrates that BigSmall is comparable to two common AU baselines \cite{kaili2016deep, krizhevsky2017imagenet}, and illustrates the performance of BigSmall over state-of-the-art rPPG, breathing multi-task models \cite{chen2018deepphys, liu2020multi}, and unsupervised methods \cite{wang2016algorithmic, de2013robust}.
As the table shows, existing methods are only capable of performing either the spatial task (i.e., AU detection) or the temporal tasks (i.e., heart rate and breathing rate) at one time. In contrast, BigSmall enables simultaneous spatiotemporal human physiological measurements with comparable or better performance.
% We further evaluate these trained PPG models and unsupervised methods, on two open source datasets \cite{bobbia2019unsupervised, stricker2014non}, and observe that BigSmall outperforms these methods. We further note that MTTS-CAN \cite{liu2020multi} under-performs on these two datasets. It is likely due to its use of TSM which highlights the importance of WTSM when training with short chunked data ($N$=3).


\subsection{Cross-Dataset Generalization}
We further evaluate the generalization ability of BigSmall on data that were not seen during training. We compare BigSmall with other baseline models which trained on BP4D+ and tested on two public rPPG datasets: \textbf{UBFC} \cite{bobbia2019unsupervised} and \textbf{PURE} \cite{stricker2014non}. Table~\ref{tab:PURE_UBFC_Tests} confirms that BigSmall outperforms the other competitors across all evaluated metrics with substantial performance gains. Moreover, these improvements are consistent on both datasets, indicating that BigSmall learns meaningful spatiotemporal information that can generalize to unseen datasets. We also provide results for generalization to other AU datasets in supplementary materials, and show benefits of BigSmall compared to existing methods.


\subsection{Computational Efficiency}

BigSmall benefits from improved computational efficiency by temporally downsampling the Big slow-spatial signal inputs. Since convolutions of the Big inputs dominate computation, temporal downsampling by a factor of $N$ reduces FLOPs by a factor of $N$. When paired with the augmenting temporal capacity of WTSM, which does not increase compute cost, BigSmall functions using only a fraction of the compute required to run three standalone task-optimized networks (BigSmall: 154M FLOPs vs Big + 2xSmall: 549M FLOPs), while producing comparable predictions as shown in Table \ref{tab:ablation}.



\subsection{Ablation Studies}
\label{sec: ablation}
\textbf{Leveraging Scales to Improve Performance.}
Table~\ref{tab:ablation} illustrates a 66\% reduction in FLOPs in BigSmall as compared to a similar model without Big input downsampling. Interestingly, such design also enjoys notable performance improvements over all considered metrics, indicating that BigSmall is both computationally efficient and achieves better multitask performance compared to other design choices (as illustrated in Fig. \ref{fig:model_its}).

\textbf{Improving Temporal Dynamics.}
The Wrapping Temporal Shift Module assists convolutional layers to better model temporal dynamics even when consecutive input frames are forcibly limited by latency or training conditions. Table \ref{tab:ablation} illustrates that for an input chunk of $N=3$  consecutive frames, BigSmall outperforms both a model without temporal shift and a model using traditional bi-direction TSM. Additionally, this demonstrates that the use of traditional TSMs, with small $N$, results in a high proportion of zeroed features and thus a drop in performance. We note that the performance of the spatial AU task improves with the use of WTSMs, suggesting that the Small branch temporal dynamics are leveraged to infer missing spatial features caused by temporal downsampling of Big input frames. The ablations of BigSmall are visualized in Fig. \ref{fig:model_its}.


\section{Limitations and Broader Impacts}

\textbf{Limitations.}
We acknowledge several limitations of our paper. First, the BP4D+ dataset only consists of videos with blank backgrounds. Future work might explore more diverse data with different backgrounds for experimenting with BigSmall network.
% Many cited AU models are trained and evaluated on BP4D (a subset of BP4D+). This dataset does not contain physiological labels and is thus infeasible to use for multitask training.
Moreover, we do not evaluate our model's performance on compute-limited platforms (e.g., embedded devices or micro-controllers). 


\textbf{Broader Impacts.}
Physiological sensing has a wide range of potentially positive applications in health sensing. However, there is also the potential for ``bad actors" to use these technologies in negative or negligent ways. Therefore, it is crucial to consider the implications of improving the accuracy, availability, and scalability of sensing methods of this kind. To mitigate negative outcomes, we have taken steps to license our models and code using responsible behavioral use licenses~\cite{contractor2022behavioral}.


\section{Conclusion}

We present the first example of a multi-task architecture for facial action unit, pulse, and respiration measurement from video. Our BigSmall model achieves this via an dual-branch design which utilizes representations on different temporal and spatial scales. The use of Big branch temporal downsampling results in a significant compute performance benefit. Wrapping Temporal Shift Modules assist the Small branch in better modeling temporal dynamics and alleviating strain cause by the downsampling of the Big branch. Finally, shared pooled representations reduce the number of parameters in the final layers of the network.  We show a model that is able to successfully and efficiently model spatially and temporally disparate signals with no significant drop in performance. 





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Appendicies %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\appendix{}

\section{Appendices Overview}

We include additional experiments and analysis regarding the facial action unit task and model architecture and ablation studies in Section \ref{sec: addtional_res}. Example waveforms are included in Section \ref{sec: example_waveform}. Details of pre and post processing are included in Section \ref{sec: preprocessing} and Section \ref{sec: postprocessing}. Details of SOTA methods and datasets are included in Section \ref{sec: SOTA_method_dataset}. Code and pretrained models can be found in the attached \texttt{BigSmall\_SM/code} folder. A video figure is also included along with the supplementary material. 

% This document contains additional ablation analysis on the BigSmall model, additional experiments on a public AU dataset (DISFA), sample plotted predictions, information regarding pre/post processing, and descriptions of baseline and state-of-the-art methods not included in the main publication due to space limitations. A walk through of the paper, its findings, and animated plots may be found in our video figure. 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Fusion Ablation %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{table}[t!]
\setlength\tabcolsep{5pt}
\caption{\textbf{Cross-AU-Dataset Generalization.} Models trained on BP4D+, evaluated on DISFA \cite{mavadati2013disfa}. Best results of each row are in \textbf{bold}.}
% \vspace{-1em}
\small
\begin{center}
\resizebox{.42\textwidth}{!}{
\begin{tabular}{cc | cccc }
\toprule[1.5pt]
\multicolumn{2}{c|}{\multirow{2}{*}{\textbf{Metrics}}} & 
\multicolumn{1}{c}{\multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}\textbf{AlexNet}\\\cite{krizhevsky2017imagenet}\end{tabular}}} & 
\multicolumn{1}{c}{\multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}\textbf{DRML}\\\cite{kaili2016deep}\end{tabular}}} & 
\multicolumn{1}{c}{\multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}\textbf{Big}\\\textbf{Pathway}\end{tabular}}} & 
\multicolumn{1}{c}{\multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}\textbf{BigSmall}\\(Ours)\end{tabular}}} \\
& & & & \\
\midrule \midrule
\textbf{AU (F1)} & AU01 & 10.7 & 10.9 & 9.1 & \textbf{11.9} \\[1.2pt]
& AU02 & 10.4 & \textbf{12.2} & 11.0 & 10.4 \\[1.2pt]
& AU04 & \textbf{33.0} & 26.0 & 32.7 & 20.5 \\[1.2pt]
& AU06 & 27.0 & 27.5 & 25.1 & \textbf{28.9} \\[1.2pt]
& AU12 & \textbf{39.0} & 35.6 & 36.4 & 38.9 \\[1.2pt]
& AU15 & 11.4 & \textbf{11.8} & 11.4 & 11.1 \\[1.2pt]
& AU17 & \textbf{18.6} & 17.2 & 16.7 & 17.9 \\[1.2pt]
\midrule
\textbf{AU (Avg)} & F1 & \textbf{21.4} & 20.2 & 20.3 & 19.9 \\[1.2pt]
& Acc. (\%) & 44.9 & 55.6 & \textbf{58.7} & 50.6 \\[1.2pt]
\bottomrule[1.5pt]
\end{tabular}
}
\end{center}
\vspace{-0.3cm}
\label{tab:AU_cross_dataset}
\end{table}







%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%% DISFA Dataset Generalization Ablation %%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%% Full AU Results: BS vs SOTA Dataset Generalization Ablation %%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{table}[t!]
\setlength\tabcolsep{5pt}
\caption{\textbf{AU comparisons of the BigSmall model \emph{vs.} literature baselines.} Models trained/tested on BP4D+. Best results of each row are in \textbf{bold}.}
% \vspace{-1em}
\small
\begin{center}
\resizebox{.42\textwidth}{!}{
\begin{tabular}{cc | cccc }
\toprule[1.5pt]
\multicolumn{2}{c|}{\multirow{2}{*}{\textbf{Metrics}}} & 
\multicolumn{1}{c}{\multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}\textbf{AlexNet}\\\cite{krizhevsky2017imagenet}\end{tabular}}} & 
\multicolumn{1}{c}{\multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}\textbf{DRML}\\\cite{kaili2016deep}\end{tabular}}} & 
\multicolumn{1}{c}{\multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}\textbf{Big}\\\textbf{Pathway}\end{tabular}}} & 
\multicolumn{1}{c}{\multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}\textbf{BigSmall}\\(Ours)\end{tabular}}} \\
& & & & \\
\midrule \midrule
\textbf{AU (F1)} & AU01 & \textbf{24.3} & 16.3 & 20.7 & 22.1 \\[1.2pt]
& AU02 & \textbf{19.5} & 12.0 & 16.5 & 18.6 \\[1.2pt]
& AU04 & 12.3 & 8.0 & 11.4 & \textbf{12.6} \\[1.2pt]
& AU06 & 72.4 & 73.9 & \textbf{75.6} & 70.2 \\[1.2pt]
& AU07 & \textbf{79.8} & 78.4 & 76.4 & 73.3 \\[1.2pt]
& AU10 & \textbf{82.0} & 80.9 & 81.6 & 74.7 \\[1.2pt]
& AU12 & 78.9 & 80.1 & \textbf{81.6} & 73.6 \\[1.2pt]
& AU14 & \textbf{72.8} & 70.9 & 68.5 & 67.7 \\[1.2pt]
& AU15 & 13.8 & 21.3 & 24.0	& \textbf{26.2} \\[1.2pt]
& AU17 & 24.3 & 32.6 & \textbf{34.4} & 29.6 \\[1.2pt]
& AU23 & 36.0 & 35.4 & 37.1	& \textbf{38.3} \\[1.2pt]
& AU24 & 14.3 & \textbf{18.4} & 15.6 & 12.1 \\[1.2pt]
\midrule
\textbf{AU (Avg)} & F1 & 44.2 & 44.0 & \textbf{45.3} & 43.3 \\[1.2pt]
& Acc. (\%) & 63.1 & \textbf{74.9} & 73.8 & 67.4 \\[1.2pt]
\bottomrule[1.5pt]
\end{tabular}
}
\end{center}
\vspace{-0.3cm}
\label{tab:full_AU_lit_baselines}
\end{table}





\begin{table*}
\setlength\tabcolsep{4pt}
\caption{\textbf{Ablation Studies of BigSmall Information Fusion and Sharing.} The default BigSmall model is highlighted in \textcolor{rowgray}{gray}. Best results of each column are in \textbf{bold}.}
\vspace{-1.5em}
\small
\begin{center}
\resizebox{1\textwidth}{!}{
\begin{tabular}{lccccccccccccccc}
\toprule[1.5pt]
\multicolumn{1}{l}{\multirow{2.5}{*}{\textbf{\begin{tabular}[c]{@{}c@{}}Model\\\end{tabular}}}} & 
\multicolumn{1}{c}{\multirow{2.5}{*}{\textbf{\begin{tabular}[c]{@{}c@{}}Fusion\\Method\end{tabular}}}} & 
\multicolumn{1}{c}{\multirow{2.5}{*}{\textbf{\begin{tabular}[c]{@{}c@{}}Lateral\\Connection\end{tabular}}}} & 
\multicolumn{4}{c}{\textbf{Heart Rate}} & 
\multicolumn{4}{c}{\textbf{Breathing Rate}} & 
\multicolumn{2}{c}{\textbf{AU Avg.}} & 
\multicolumn{2}{c}{\textbf{Computation}} \\ 
\cmidrule(l){4-7} \cmidrule(l){8-11} \cmidrule(l){12-13} \cmidrule(l){14-15}
&& & MAE & RMSE & MAPE & $\rho$  & MAE & RMSE & MAPE & $\rho$  & F1 & Acc & FLOPS (M) & \# Params (M) \\ \midrule\midrule
\grayrow \textbf{BigSmall} & Sum & $-$ & 2.38 & 6.00 & 2.71 & 0.89 & 3.39 & 5.00 & 16.65 & \textbf{0.21} & 43.3 & 67.4 & \textbf{154.01} & \textbf{2.14} \\[1.2pt]
BigSmall & Concat & $-$ & 2.28 & 5.68 & 2.58 & 0.90 & 3.72 & 5.28 & 17.94 & 0.15 & 43.5 & 67.3 & 156.00 & 4.13 \\[1.2pt]
BigSmall & Sum & Bi-Directional & \textbf{2.21} & \textbf{5.46} & \textbf{2.55} & \textbf{0.91} & 3.93 & 5.54 & 18.98 & 0.10 & \textbf{46.9} & \textbf{72.3} & 172.35 & 2.16 \\[1.2pt]
BigSmall & Sum & Big-To-Small & 2.32 & 5.84 & 2.62 & 0.89 & 3.80 & 5.39 & 18.42 & 0.12 & 46.0 & 69.5 & 154.76 & 2.15 \\[1.2pt]
BigSmall & Sum & Small-To-Big & 2.37 & 5.96 & 2.70 & 0.89 & \textbf{3.37} & \textbf{4.99} & \textbf{16.48} & 0.19 & 40.6 & 61.4 & 171.60 & 2.15 \\[1.2pt]
\bottomrule[1.5pt]
\end{tabular}
}
\end{center}
\vspace{-0.5cm}
\label{tab:fusion_ablation}
\end{table*}




\section{Additional Results}
\label{sec: addtional_res}

\subsection{Cross-Dataset Generalization in AU}
\label{sec:disfa}
We test the ability of BigSmall to generalize to other AU datasets, and benchmark its performance against state-of-the-art AU methods. To do so we train BigSmall, and other baseline models, on BP4D+, and evaluate on the DISFA \cite{mavadati2013disfa} dataset. We report results for AUs which exist in both the BP4D+ and DISFA dataset. We find that BigSmall performs comparatively to AU-task-optimized models in cross-dataset generalization. These results are found in Table \ref{tab:AU_cross_dataset}. These results are comparable to \cite{kaili2016deep} which trained with BP4D to generalize to the DISFA dataset.

\subsection{Individual AU Result in BP4D+}
\label{sec:individual_au}
Individual AU results for the comparisons between the BigSmall model and state-of-the-art AU models, trained and evaluated on BP4D+, are included in Table \ref{tab:full_AU_lit_baselines}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Gray Scale BS  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{table}
\setlength\tabcolsep{5pt}
\caption{\textbf{Comparison of BigSmall With Gray Scale Big Input.} Best results of each row are in \textbf{bold}.}
% \vspace{-1em}
\small
\begin{center}
\resizebox{.42\textwidth}{!}{
\begin{tabular}{cc | cccc }
\toprule[1.5pt]
\multicolumn{2}{c|}{\multirow{2}{*}{\textbf{Metrics}}} & 
\multicolumn{1}{c}{\multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}\textbf{BigSmall w/ Gray Scale} \\ \textbf{Big Pathway Input} \end{tabular}}} & 
\multicolumn{1}{c}{\multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}\textbf{BigSmall}\\(Ours)\end{tabular}}} \\
& & & & \\
\midrule \midrule

\textbf{Heart Rate} & MAE & \textbf{2.29} & 2.38\\[1.2pt]
& RMSE & \textbf{5.75} & 6.00\\[1.2pt]
& MAPE & \textbf{2.59} & 2.71\\[1.2pt]
& $\rho$ & \textbf{0.89} & \textbf{0.89}\\[1.2pt]
\midrule
\textbf{Resp. Rate} & MAE & 3.62 & \textbf{3.39}\\[1.2pt]
& RMSE & 5.26 & \textbf{5.00}\\[1.2pt]
& MAPE & 17.63 & \textbf{16.65}\\[1.2pt]
& $\rho$ & 0.18 & \textbf{0.21}\\[1.2pt]
\midrule
\textbf{AU (F1)} & AU01 & 19.6 & \textbf{22.1}\\[1.2pt]
& AU02 & 18.1 & \textbf{18.6}\\[1.2pt]
& AU04 & 11.5 & \textbf{12.6}\\[1.2pt]
& AU06 & 65.0 & \textbf{70.2}\\[1.2pt]
& AU07 & 71.3 & \textbf{73.3}\\[1.2pt]
& AU10 & 71.2 & \textbf{74.7}\\[1.2pt]
& AU12 & 68.9 & \textbf{73.6}\\[1.2pt]
& AU14 & \textbf{68.0} & 67.7\\[1.2pt]
& AU15 & 25.2 & \textbf{26.2}\\[1.2pt]
& AU17 & 24.8 & \textbf{29.6}\\[1.2pt]
& AU23 & 35.1 & \textbf{38.3}\\[1.2pt]
& AU24	& 8.7 & \textbf{12.1}\\[1.2pt]
\midrule
\textbf{AU (Avg)} & F1 & 40.6 & \textbf{43.3}\\[1.2pt]
& Acc. (\%) & 61.3 & \textbf{67.4}\\[1.2pt]
\bottomrule[1.5pt]
\end{tabular}
}
\end{center}
\vspace{-0.3cm}
\label{tab:gray_input_BS}
\end{table}

\subsection{Optimal Input Frame Number for Spatial Task}
As detailed in the main paper, spatial task performance degrades when trained with a high of number consecutive frames which reduces variance in the training mini batches. We train the AU task-optimized Big branch model using a number of chunked data lengths to empirically illustrate how performance degrades as the number of consecutive frames increases. We observe that there is significant degradation in AU task performance after $N$ exceeds 9. For our experiments we use $N=3$ to highlight the abilities of BigSmall and the Wrapping Temporal Shift Modules in situations that necessitate small $N$ due to training or latency considerations. This is highlighted in Fig.~\ref{fig:au_by_chunklen}.

%%%%%%%%%%%%%%%%% Chunk Length vs AU F1 %%%%%%%%%%%%%%%%%
\begin{figure}[t]
    \begin{center}
    \includegraphics[width=3.5in]{figures/variable_chunklen.pdf}
    \end{center}
    \caption{\textbf{Consecutive Frames $N$ vs Avg. 12 AU F1.} These 12 AU average F1 scores, from the Big branch model trained with a number of different consecutive frames $N$, shows that AU performance degrades as the N increases.}
    \label{fig:au_by_chunklen}
\vspace{-0.3cm}
\end{figure}

\begin{figure*}[t!]
    \begin{center}
    \includegraphics[width=6.8in]{figures/SM_samplewaveforms.pdf}
    \end{center}
    \caption{\textbf{Sample PPG and Respiration Waveforms.} BigSmall PPG ad respiration waveforms plotted against the sensor ground truth. Note that PPG predictions are plotted again the blood pressure waveform, the BP4D+ heart-signal ground truth.}
    \label{fig:sample_ppg_resp}
\vspace{-0.1cm}
\end{figure*}

\subsection{Fusion / Data Sharing Ablation Experiments}
We explore the type of connections used to fuse the Big and Small branches of BigSmall. These results are shown in Table \ref{tab:fusion_ablation}.

We first test the use of concatenation of the Big and Small feature maps (as opposed to summing). Concatenation of the features maps results in a negligible difference in performance while significantly increasing the number of parameters in the model due to large output dense layers. 

We further explore the use of lateral information sharing of high-level features between the Big and Small branches. These lateral connection occur after the first pooling layer of the Big branch and after the second convolutional layer of the Small branch. We test Big-to-Small, Small-to-Big, and bi-directional lateral connections. Big-to-Small lateral connections temporally upsample and spatial downsample the Big feature map to match the dimensions of the Small branch, and then concatenate these features with the Small branch feature map (along the channel dimension). Small-to-Big lateral connections temporally downsample and spatially upsample the Small branch feature map to match the dimensions of the Big branch, and then concatenate these features with the Big feature map (along the channel dimension). Bi-direction lateral connections utilize both the aforementioned Big-to-Small and Small-to-Big lateral connections.

We find that all methods of high-level information sharing benefit the PPG task. AU performance benefits from Big-to-Small fusion, but regresses considerably with Small-to-Big fusion. Respiration benefits from Small-to-Big fusion, but regresses considerably with Big-to-Small fusion. This suggests that though high level information sharing may benefit all tasks independently, the high level features of interest differ between respiration and AU, preventing a unified lateral connection system that benefits all tasks simultaneously.



\subsection{Gray Scale Big Input}
Some previous works \cite{kaili2016deep} train AU models using gray scale images which preserve texture information and reduce the number parameters which may cause overfitting. We find that using gray scale Big inputs results in reduced performance for BigSmall. This is likely as the Big branch of BigSmall is able to leverage color-channel-dependent variations embedded in the 3-color-channel Small input difference frames. Results in Table \ref{tab:gray_input_BS}.


\section{Example Waveforms}
\label{sec: example_waveform}
Fig.~\ref{fig:sample_ppg_resp} illustrates additional PPG and Respiration predictions from BigSmall plotted against the sensor ground truth. NOTE, PPG predictions are plotted against the Blood Pressure waveform (BP4D+ pulse ground truth). This accounts for the similar waveform frequency content but phase-misalignment. Similar animated waveform plots may be found in our video figure. 




\section{Preprocessing}
\label{sec: preprocessing}
\subsection{Video Frame Inputs}

Raw and normalized difference inputs are processed to match the preprocessing of \cite{liu2020multi}. The described transforms are performed per-video before the videos are chunked. Before each frame is transformed, the frames are center cropped, along the vertical axis, in order to produce square frames.

\textbf{Small Inputs (Normalized Difference Frames).}
Normalized difference frames are derived by taking the difference of a frame $k[n]$ and a frame $k[n+1]$ such that $k_{diffnorm}[n] = (k[n+1] - k[n]) / (k[n+1] + k[n])$. This denominator normalization factor helps to reduce dependence on per-frame-skin brightness and appearance \cite{liu2020multi}. The resulting frames are mean and standard deviation standardized. These frames are then downsampled to 9x9px. 

\textbf{Big Input (Raw Frames).} 
The raw frames are mean and standard deviation standardized. The resulting frames are then downsampled to 144x144px. 



\subsection{Data Labels}

\textbf{Label Preparation.} Following previous work \cite{liu2020multi, yang2023simper}, the respiration and PPG labels are difference noramlized, to match the format of the Small branch difference frame inputs. This is done such that for a sample $k[n]$, $k_{diffnorm}[n] = (k[n+1] - k[n]) / (k[n+1] + k[n])$. The resulting samples are mean and standard deviation standardized. AU labels are not difference normalized as the spatial branch (Big branch) inputs are not difference normalized.

\textbf{PPG Labels.} Early explorations indicated an ineptitude of BigSmall to effectively learn the PPG signal when trained on blood pressure waveform labels (the BP4D+ ground truth heart signal). Thus, we train the PPG task using ``pseudo'' PPG labels derived using the Plane Orthogonal-to-Skin (POS)~\cite{wang2016algorithmic} method. These POS-derived signals are then aggressively filtered using a 2nd Order Butterworth filter around the normal heart-rate frequency of [0.75, 2.5] Hz. The amplitude of the resulting signals are then normalized using the Hilbert envelope. Although these ``psuedo'' labels are used to train, all models are still evaluated against BP4D+'s ground truth blood pressure waveform which shares the PPG signal's heart rate frequency.

\textbf{AU Labels.}
BP4D+ has labels for 34 AU activations. We choose to use 12 of these AUs for training and evaluation based off previously published literature \cite{kaili2016deep} and as these 12 AUs have sufficient positive occurrences in the dataset. Some AU activations in both BP4D+ and DISFA are labeled as intensities [0-5], where 0 is no activation and 5 is maximum activation. Following previously published work we train and test using binarized AU activation (0 for inactive, 1 for activate regardless of intensity).

\textbf{Data Splits.}
We split the BP4D+ dataset into the following 3 subject-independent splits, used for cross validation. Note that all splits have approximately equal participants, and approximately equal subjects of each biological sex. ``F'' denotes female subjects, while ``M'' denotes male subjects.

\textbf{Split 1:} F003, F004, F005, F009, F017, F022, F028, F029, F031, F032, F033, F038, F044, F047, F048, F052, F053, F055, F061, F063, F067, F068, F074, F075, F076, F081, M003, M005, M006, M009, M012, M019, M025, M026, M031, M036, M037, M040, M046, M047, M049, M051, M054, M056		

\textbf{Split 2:} F001, F002, F008, F018, F021, F025, F026, F035, F036, F037, F039, F040, F041, F042, F046, F049, F057, F058, F060, F062, F064, F066, F070, F071, F072, F073, F077, M001, M002, M007, M013, M014, M022, M023, M024, M027, M029, M030, M034, M035, M041, M042, M043, M048, M055

\textbf{Split 3:} F078, M008, F080, M011, F014, M033, F020, M010, M052, M057, M017, M038, F030, F051, M032, F013, F011, F015, F016, F065, M015, M020, F007, F050, F010, M021, F012, F045, F059, M045, F023, M004, F069, M044, M053, M018, M058, M050, F019, F024, F034, F079, M039, F056, F054, F027, F043



\section{Postprocessing}
\label{sec: postprocessing}
\subsection{Heart and Respiration Rate From Waveform}

PPG and respiration waveform labels are difference normalized to match the temporal branch inputs. Thus predictions are also in a difference normalized form. PPG and respiration waveforms are derived from the difference normalized waveforms by taking the cumulative sum of the waveform at every sample and then detrending the resulting vector. 

Signal rates are then derived by applying a 2nd Order Butterworth filter with cut-off frequencies of [0.75, 2.5] Hz for heart rate and [0.08, 0.5] Hz for respiration rate to the signal waveforms and using a peak detection algorithm on the Fourier spectrum of the filtered signals. 

\subsection{AU Model Prediction Thresholding}
AU outputs from the final model layer are passed through a sigmoid function to bound the output (0,1). We use a threshold of 0.5 to binarize the output of the sigmoid such that AU sigmoid output $< 0.5 = 0$ (inactive) and AU sigmoid output $\ge 0.5 = 1$ (active). 




\subsection{Heart and Respiration Rate Evaluation Metrics}

\textbf{Mean Average Error (MAE).} The MAE as defined between the predicted signal rate $R_{pred}$ and the ground truth signal rate $R_{gt}$ for a total of $T$ instances: \[MAE = \frac{1}{T} \sum_{t=1}^{T} |R_{gt} - R_{pred}|\]

\textbf{Root Mean Square Error (RMSE).} The RMSE as defined between the predicted signal rate $R_{pred}$ and the ground truth signal rate $R_{gt}$ for a total of $T$ instances: \[RMSE = \sqrt{\frac{1}{T} \sum_{t=1}^{T} (R_{gt} - R_{pred})^2}\]

\textbf{Mean Average Percent Error (MAPE).} The MAPE as defined between the predicted signal rate $R_{pred}$ and the ground truth signal rate $R_{gt}$ for a total of $T$ instances: \[MAE = \frac{1}{T} \sum_{t=1}^{T} \bigg| \frac{R_{gt} - R_{pred}}{R_{gt}}\bigg|\]

\textbf{Pearson Correlation ($\rho$).} The Pearson correlation as defined between the predicted signal rate $R_{pred}$ and the ground truth signal rate $R_{gt}$ for a total of $T$ instances, and $\overline{R}$ the mean of $R$ over $T$ instances: 
\[\rho = \frac{\sum_{t=1}^{T}\bigg(R_{gt.t} - \overline{R_{gt}}\bigg)\bigg(R_{pred.t} - \overline{R_{pred}}\bigg)}{\sqrt{\bigg(\sum_{t=1}^{T}R_{gt.t} - \overline{R_{gt}}\bigg)^2\bigg(\sum_{t=1}^{T}R_{pred.t} - \overline{R_{pred}}\bigg)^2}}\]

\subsection{AU Evaluation Metrics}
\textbf{F1.} The F1 as defined between a list of predictions and ground truth labels, where $TP$ is the true positive count, $FP$ is the false positive count, and $FN$ is the false negative count: \[100*\frac{2TP}{2TP+FP+FN}\]

\textbf{Accuracy (\%).} The accuracy as defined between a list of predictions and ground truth labels, where $TP$ is the true positive count, $TN$ is the true negative count $FP$ is the false positive count, and $FN$ is the false negative count:  \[100*\frac{TP+TN}{TP+TN+FP+FN}\]



\section{SOTA Methods and Dataset Descriptions}
\label{sec: SOTA_method_dataset}


\subsection{Temporal Task Baselines}
An implementation of these rPPG baseline methods may be found in \cite{liu2022rppgtoolbox}.

\textbf{MTTS-CAN \cite{liu2020multi}.} An efficient dual pathway convolutional neural network for PPG and respiration multitasking. The network utilizes attention from the ``Appearance Branch'' which models the location of skin pixels, to assist the ``Motion Branch'' which models changes in skin color correlated to the pulse signal. The ``Motion Branch'' makes use of Temporal Shift Modules \cite{lin2019tsm} to share information between time samples. 

\textbf{DeepPhys \cite{chen2018deepphys}.} A dual pathway convolutional neural network for PPG estimation. The network utilizes attention from the ``Appearance Branch'' which models the location of skin pixels, to assist the ``Motion Branch'' which models changes in skin color correlated to the pulse signal.

\textbf{POS \cite{wang2016algorithmic}.}
A signal processing method that utilizes the  individual color channel (R, G, B) signals. These signals are split into overlapping window segments. For each window segment each color channel signal is normalized by its mean. The PPG signal for that window is then calculated through a relationship between the original color channel signals and mean signals. The final PPG signal is reconstructed by piecing together the overlapping window segments. 

\textbf{CHROM \cite{de2013robust}.}
A signal processing method that utilizes chrominance signals to derive the PPG signal. The method filters the individual color channel (R, G, B) signals around the normal heart rate frequency, and then windows the signals into overlapping segments. A relationship between the color-channel-based signals is then used to derive the PPG signal windows. The resulting segments are further Hanning-windowed and pieced together using an overlapping add technique to obtain the final PPG signal.



\subsection{Spatial Task Baselines}

\textbf{DRML \cite{kaili2016deep}.} Deep Region and Multi-Label Learning is a convolutional network that utilizes region learning to better isolate regions of the face in which different AUs activate. The use of a ``region layer'' helps the model learn spatial information regarding individual AU's without incurring the computational cost of needing to isolate individual pixels as is done by \cite{taigman2014deepface}.

\textbf{AlexNet \cite{krizhevsky2017imagenet}.} A convolutional network used to baseline image classification tasks. It consists of a number of convolutional and pooling layers before a number of fully connected layers. 



\subsection{PPG Datasets}

\textbf{PURE \cite{stricker2014non}.} A dataset comprised of RGB video recordings (30fps) from 10 participants (2 female, 8 male). Participants are seated and front lit with ambient light from a window. Each subject participates in 6 recordings, each with the individual performing different motion tasks. The dataset contains ground truth, contact-sensor-based, PPG and SpO2 measurements. 

\textbf{UBFC \cite{bobbia2019unsupervised}.} A dataset comprised of RGB video recordings (30fps). Participants are seated and lit with ambient light. The dataset contains ground truth, contact-sensor-based, PPG measurements. 

\subsection{AU Datasets}

\textbf{DISFA \cite{mavadati2013disfa}.} A dataset comprised of 4 minutes of RGB video recordings (20fps) per 27 subjects. Each frame of the dataset is manually FACS coded for 12 AUs (AU1, AU2, AU4, AU5, AU6, AU9, AU12, AU15, AU17, AU20, AU25, AU26) with an intensity measure [0-5].





\section{Broader Impacts and Future Work}

\textbf{Application To Other Domains.} Though BigSmall is evaluated on physiological sensing tasks, we believe that such a model may allow multitasking in other domains in which modeling disparate spatiotemporal signals may be of interest. We hypothesize that a BigSmall-esc. model may show significant benefit in situations where the modeled signals are more related (shared task-gradient direction) than those presented in this work. 

\textbf{COVID-19.} The COVID-19 pandemic has catalyzed interest in remote medicine and health sensing via ubiquitous technologies (e.g., a mobile phone) \cite{song2020role, smith2020telehealth}. However, the sensitive nature of biometrics often dictates that these models run on-device. Mobile sensing requires the use of efficient networks that can be run in near-real-time without significant computational limitations.

\textbf{Future Work.} Future work entails the evaluation of BigSmall on resource constrained platforms such as mobile devices and embedded processors. We also plan to train BigSmall on videos with dynamic backgrounds (as BP4D+ has blank background), and utilize additional data augmentation techniques to help build a more robust embedding. Finally, we intend to explore the use of different model backbones for both the Big and Small branches. 




\balance{}

{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\end{document}


