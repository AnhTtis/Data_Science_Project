\section{A Motivating Example}
\label{sec:motivation}
\noindent\textbf{What is \textit{Dummy Neurons}?} Literally, dummy neurons are defined to be those neurons which leave the prediction behavior of the original DNN intact after being inserted into the model. 
% %%% tell us about the story of inserting neurons of zero weight, the invalidation effect on some watermarks, and the limitation of the attack stealthiness (maybe easily removed)
% To prove the vulnerability of the existing white-box model watermarking schemes, we present a novel attack to invalidating the ownership verification based on neural structural obfuscation via inserting an arbitrary number of \textit{dummy neurons} into the victim model. 
% In this section, we first provide the definition of dummy neurons via a motivating example (\S\ref{sec:dn_concept}). Then, we explain our discovery on how dummy neuron injection is the core to invalidate existing white-box watermarks with almost no attack budget in the four dimensions (\S\ref{sec:how_dn_invalidate}).

% \subsection{Concept of Dummy Neurons}
% \label{sec:dn_concept}
\begin{definition}[Dummy Neurons]
When we insert a group of neurons $\{m_i\}^{M}_{i=1}$ into a given DNN $f$ to obtain a structurally obfuscated model $f^{'}$, we call the group of neurons \textit{dummy neurons} if for every input ${x} \in \mathcal{X}$, $f(x) = f^{'}(x)$. 


\end{definition}

%%%%%%%%%%%%%%%%%%%%%%% TODO %%%%%%%%%%%%%%%%%%%%%%%%%%%
% 四层FCN，DN加在相邻两层上符合uchida的攻击说明?
% 用cnn还是fcn？



%%%%%%%%%%%%% BEGIN OF naive dummy neuron 
\begin{figure}[t]
\begin{center}
\includegraphics[width=0.45\textwidth]{img/naive_dn.pdf}
\caption{A schematic diagram of NeuronZero on (a) fully-connected layers and (b) convolutional layers.}
\label{fig:naive_dn}
\end{center}
\end{figure}
%%%%%%%%%%%%%% END OF naive dummy neuron




%%%% BEGIN OVERVIEW
\begin{figure*}[t]
\begin{center}
\includegraphics[width=0.9\textwidth]{img/pipeline.pdf}
\caption{Overview of our proposed watermark removal attack by neural structural obfuscation with dummy neurons.}
\label{fig:atk_pipeline}
\end{center}
\end{figure*}
%%%% END OVERVIEW

We present motivating examples of dummy neurons in Fig.\ref{fig:naive_dn}. In Fig.\ref{fig:naive_dn}(a), given the target fully-connected neural network (FCN), we inject two additional neurons (i.e., $m_1$ and $m_3$) into each hidden layer of the model. By definition, each injected neuron $m_k$ is associated with vectors of incoming and outgoing weights, which connects the $k$-th dummy neuron with the $i$-th neuron in the precedent layer and $j$-th neuron in the successive layer (denoted as $u_{i,k}$ and $v_{k,j}$, respectively). According to the architecture specification, a neuron may optionally be associated with a bias $o^l_k$. 

To make the injected neurons dummy, a naive strategy is to set the incoming/outgoing weight of the neuron to be a vector of all $0$, the optional bias $0$ but leave the incoming/outgoing weights arbitrarily assigned. For example, the dummy neuron $m_1$ in Fig.\ref{fig:naive_dn}(a) has the outgoing weights of values $0$ (i.e., $v_{1,j} = 0$ for $j$). Therefore, its contribution of $m_1$ to any of the neurons in the successive layer is constantly $0$. Alternatively, the dummy neuron $m_3$ has the incoming weights of values $0$, which implies that the activation of $m_3$ is constantly $0$. Both cases provably ensure no impact on the next layer's output and finally leaves the utility of the model intact. As a fully connected layer in DNNs is a simplified form of convolutional layer, we can add the dummy neurons in convolutional neural network (CNN) in the same way (Fig.\ref{fig:naive_dn}(b)), i.e., setting the weights of all the incoming or outgoing filters to $0$. We refer to the above construction of the dummy neurons as \textit{NeuronZero}.


\noindent$\bullet$\textbf{ A Preliminary Attack.} With \textit{NeuronZero}, the adversary is ready to obfuscate the protected model by injecting an arbitrary number of dummy neurons with incoming/outgoing weights of values $0$ to every neural layer. The process requires no knowledge about whether and what type of watermark is embedded, and does not need to train the protected model. As we analyze below, the attack is also effective to invalidate the mainstream white-box model watermark schemes by perturbing the watermark-related parameters.



% \subsection{Invalidating White-Box Watermarking with Dummy Neurons}
% \label{sec:how_dn_invalidate}
\noindent\textbf{Procedural Vulnerability of White-Box Watermark} After dummy neurons are injected into each neural layer of the protected model, the parameters once embedded with the watermark information are now messed with the weights of the dummy neurons. Consequently, when the verification process invokes the watermark extraction algorithm on the tampered weights, the extracted watermark information is very likely to suffer a large distortion. Worse yet, most of the state-of-art white-box watermarks do not implement error-handling procedures to properly address the case when the size of the weights from the watermarked layer mismatches the expected size in the watermark extraction algorithm. Therefore, these watermark schemes are inexecutable for extracting valid watermarks from the suspect model and thus can no longer protect the intellectual property of the victim model. 

% n,c is the N_{l+1} and N_l
\noindent$\bullet$\textbf{ An Example.} Uchida et al. embed the watermark into the convolutional filter weights of a target layer, i.e., $W \in \mathbb{R}^{n\times c \times w \times h}$, where $n$ is the number of filters, $c$ is the number of channels, and $w$, $h$ are the width and height of these filters\cite{uchida2017embedding}. To extract the watermark, the verification process first averages these convolutional weights at channel level and then flattens the result to $\hat W \in \mathbb{R}^{c \cdot w \cdot h}$. Finally, a bit string with length of $b$ is obtained according to the signs of $\hat W$, i.e., $s' = T_h(A\cdot \hat W) = \{0,1\}^b$, where $T_h$ is a hard threshold function which output $1$ when the input is positive and $0$ otherwise, and $A\in \mathbb{R}^{b \times (c \cdot w \cdot h)}$ is a pre-defined transform matrix sampled from the normal distribution. Once we add a group of dummy neurons into the next layer of watermarked layer (e.g., Fig.\ref{fig:naive_dn}(b)), the weight $W'\in \mathbb{R}^{n' \times c' \times w \times h}$ in the watermarked layer now has an expanded shape, i.e., the numbers of filters and channels in the target layer increase to $n', c'$ due to the injected dummy neurons. After being flattened, the parameters become $\hat W' \in \mathbb{R}^{c' \cdot w \cdot h}$ in vector form. As a result, the verification process cannot be executed as the dimension of $\hat W'$ is incompatible with the second dimension of the transform matrix $A\in \mathbb{R}^{b \times (c \cdot w \cdot h)}$. By error-handling the flattened weight $\hat W'$ via, e.g., random sampling or max pooling, to satisfy the dimension consistency with $A$, a bit string can still be obtained from the watermarked layer. However, as validated in Section \ref{sec:eval_weight}, the extracted message remains almost at random unless the verifier were able to precisely eliminate the impact of the dummy neurons from $\hat W'$ to restore the watermark integrity. 



\noindent$\bullet$\textbf{ Limitation of \textit{NeuronZero}.}
 However, we notice that this preliminary attack suffers from the limitation of stealthiness: The dummy neurons can be easily detected due to the anomaly weight distribution. For example, to determine whether the protected models in Fig.\ref{fig:naive_dn} are maliciously injected with dummy neurons, the watermark verifier would first check all the neurons in the suspect model by inspecting the values of incoming and outgoing weights. Once the verification process finds out that the input or output parameters of a certain neuron all equal to value $0$, this neuron is likely to be dummy neurons and is provably has no contribution to the output of the watermarked model. Therefore, the watermark verifier can safely eliminate the detected dummy neurons and the associated weights before the ownership verification.
% extracting the watermark from the suspect model.


% At the core of our newly proposed attack is the concept of dummy neurons, literally a group of neurons that can be added to a target DNN model while provably leave the model behavior invariant (i.e., the model output remains the same under the same input). A naive example is neurons which have the input and output weights of values 0, which, if added to a given DNN model, will have no contribution to the model’s output. As a preliminary attack, the adversary may obfuscate the protected model by injecting a number of these neurons to every neural layer, which already inhibits half of the state-of-the-art white-box watermarks from being executed, but has clear limitation in its attack stealthiness (§4).