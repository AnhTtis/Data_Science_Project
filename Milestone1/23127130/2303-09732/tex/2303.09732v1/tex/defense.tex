\subsection{Stealthiness of Injected Dummy Neurons}
\label{sec:eval:stealthiness_dn}
Finally, we provide a preliminary study on potential adaptive approaches to detect and eliminate dummy neurons. Specifically, we consider two types of defenders, (a) a \textit{partially knowledgeable defender}, i.e., who knows the existence of dummy neurons but has rare knowledge about the detailed algorithm for generating the dummy neurons, (b) a \textit{skilled defender}, who has a perfect knowledge about our attack framework but does not have access to the original model (a common setting in existing watermarking protocols), and (c) a \textit{fully knowledgeable defender}, who also has the original model for reference. 

\noindent\textbf{(a) A Partially Knowledgeable Defender.} If knowing the existence of dummy neurons in the suspect model, the defender is likely to apply anomaly detection algorithms to detect and eliminate the suspicious neurons from the target layer. Specifically, by considering the incoming and outgoing weights of each neuron as the feature vector, we implement two representative anomaly detection algorithms, i.e., \textit{cluster-based} \cite{chen2018activation_clustering} and \textit{SVD-based} \cite{tran2018spectral}, to evaluate the stealthiness. 

\noindent$\bullet$\textbf{ Experimental Settings.}
We first inject the dummy neurons generated by NeuronZero, NeuronClique, and NeuronSplit into the watermarked models, respectively. Then, we concatenate the flattened incoming and outgoing weights of each neuron as its feature vector. The cluster-based detection leverages K-Means to separate the neurons from the same layer and assigns the abnormal cluster as dummy neurons \cite{chen2018activation_clustering}, while the SVD-based detection utilizes the covariance matrix of the neurons' feature representation to filter outliers \cite{tran2018spectral}.

\noindent$\bullet$\textbf{ Results \& Analysis.} 
As is shown in Fig.\ref{fig:detection}, the dummy neurons with vanishing values generated by NeuronZero are more likely to be recognized as abnormal neurons under both detection approaches, while the dummy neurons produced by NeuronSplit from the original neurons show stronger stealthiness compared to both NeuronZero and NeuronClique, as their weights have the same distribution to the normal ones.
%%%%%%%%%%%%% BEGIN of Detection Rate
\begin{figure}[t]
\begin{center}
\includegraphics[width=0.5\textwidth]{img/detection_rate.pdf}
\caption{Detection rate of anomaly detection algorithms on different types of dummy neurons.}
\label{fig:detection}
\end{center}
\end{figure}
%%%%%%%%%%%%%% END of Detection Rate


%%%%%%%%% BEGIN OF LINEAR ELIMINATION ALGORITHM 
\input{tex/algorithm/dn_elimination.tex}
%%%%%%%%% END OF LINEAR ELIMINATION ALGORITHM


\noindent\textbf{(b) A Skilled Defender.} Besides the above defense, we consider a more adaptive defender who has perfect knowledge about our proposed attack. From our construction in Section \ref{sec:dn_generation} (combined with the defense results above), the only exploitable information for dummy neuron elimination is in the first layer where the dummy neurons are injected. According to Eq.\ref{eq:NC_incoming}\&\ref{eq:Ns_incoming}, if there are no dummy neurons in the previous layer, then the dummy neurons belonging to the same group generated by \textit{NeuronClique} or \textit{NeuronSplit} would have their incoming weights, if viewed as vectors, proportional to each other. Based on this characteristic, the defender may implement the following procedures to detect and eliminate dummy neurons from each layer:
\begin{itemize}
    \item \textit{Step 1.} Normalize the incoming weights of each neuron in the current layer and move the neurons with the same normalized weight into the same hash bucket.
    \item \textit{Step 2.} Merge the neurons of the same hash bucket into one neuron: Its incoming weights take the normalized weights of either one of the neurons and its outgoing weights take the sum of these neurons.  
    \item \textit{Step 3.} After the merging, check the flattened outgoing weights of each neuron: If the weights are a zero vector, then remove the neuron and its associated weights. 
\end{itemize}

% %%%%%%%% BEGIN OF RESULTS SUMMARY 
% \input{tex/tables/removal_summary.tex}
% %%%%%%%% END OF RESULTS SUMMARY 


  By iterating the above procedure from the first hidden layer to the last one, the algorithm is expected to detect the dummy neurons and restore the original neural architecture from the obfuscated model. The detailed algorithm is shown in Algorithm \ref{alg:dn_elim}. Our experiments find, even though there is no dummy neurons after the elimination, the BER of the recognized watermark in the restored model remain over $50\%$ in Table \ref{tab:elim_table}, yielding no evidence for the claimed ownership. This is because, after the merging, the original scale of the parameter could still not be recovered, because the defender does not know how the attacker has rescaled the parameters of the dummy neurons during NeuronClique and NeuronSplit. Therefore, when the defender has no access to the original model, they could not adjust the parameter scale to cancel out the obfuscation effect. Hence, the original watermark is not recovered.
 
% To further improve the stealthiness of our injected neurons, we propose to remove the exploitable trace, i.e., the proportionality between the incoming weights of the dummy neurons, in an \textit{adaptive attack}. Specifically, we first generate their input weight randomly, then choose the bias of each injected neurons to keep them being activated under any valid inputs. Finally, we construct their outgoing weights by solving an under-determined linear equation system for preserving the original model behavior via vanishing the total contribution of the newly inserted dummy neurons. Appendix \ref{sec:app:stealthier_dn} provides the omitted technical details of the adaptive dummy neurons. In this design, the unparalleled incoming weights invalidate the elimination procedure above, which becomes ineffective in merging these stealthier DNs and thus leaves the size of relative parameters/activation maps unmatched for watermark extraction. Even after conducting the error-handling mechanisms in Section \ref{sec:eval}, Table \ref{tab:elim_table} shows the stealthier DN injection inhibits the verification procedures of most white-box watermarking schemes from neither successful detection of dummy neurons nor watermark recovery.

%%%%%%% BEGIN TABLE 
\input{tex/tables/elim_table}
%%%%%%% END TABLE 
\noindent\textbf{(c) A Fully Knowledgeable Defender.} Finally, we discuss the case when the defender also refers to the original watermarked model for watermark recovery. Then they can further compare the parameters of the original model with the obfuscated model in order to recover the order and the scale of the parameters. For the dummy neurons constructed in Section \ref{sec:dn_generation}, they can eliminate the dummy neurons and recover the watermark accuracy at the cost of some additional computing power. As the security research on white-box model watermark is an evolving game between the attacks and the defenses, we leave the study on more effective de-obfuscation approaches to future work. 

% In Appendix \ref{sec:app:stealthier_dn}, we develop a more stealthy dummy neuron construction which can adaptively confuse the defender when they try to determine which one of the neurons is from the original model. 
% In our preliminary results reported in Table \ref{tab:full_elim_table}, if the defender just merges the neurons with the same incoming weights as in Algorithm \ref{alg:dn_elim}, the watermark message is not recovered (i.e., BER > 50\%). 


% However, when using the more stealthy dummy neuron construction in the previous part, we find the attacker has the flexibility to set the incoming weights of the dummy neurons to be the same incoming weights of the original neurons. This then confuse the defender when he/she is trying to determine which one of the neurons is the original one. If the defender just merges the neurons with the same incoming weights, the watermark message could hardly be recovered. Our experimental results in the third row of Table \ref{tab:elim_table} validate this point. It is mainly because the outgoing weights of the injected dummy neurons in the new design has not to cancel each other out as for the original design.

% Nevertheless, we would also like to emphasize that referring to the original watermarked model is inconsistent with the basic settings of white-box watermarking. In fact, due to the concerns on opening forgery and ambiguity attack surfaces when an ownership claimer could submit a model for verification, the target model is usually not allowed to be involved in the watermark verification process. Moreover, we admit that the security research on white-box model watermark is again an evolving game between the attacks and the defenses. We leave the study on more effective de-obfuscation approaches as a potential future direction. 


% We present a case-by-case analysis on the stealthiness of preliminary and generalized DN generation algorithms when the attacker has the original watermarked model or not. Table \ref{tab:summary_stealthiness} summarizes our main findings. 


% \noindent$\bullet$\textbf{ Case A. Stealthiness of Preliminary DN.} We first apply the shuffling and scaling invariant transforms for the dummy neuron generation in Section 6.2 to construct Preliminary DN. As the DN elimination procedure shows above, to detect and eliminate the dummy neurons in all the neural layers incurs $O(nd)$ computational complexity. When the defender invests such computational costs to run the elimination algorithm, most of the dummy neurons injected by our approach in the original manuscript would be removed, which results in the following cases depending on whether the defender has access to the original watermarked model. Table \ref{tab:CaseA-12} summarizes the results of out evaluation.



% \noindent\textbf{Case A-1} \textit{(w/o. the original model).}
% Even if most of the dummy neurons are removed, the permutation and the scaling transformations on the weights are hardly canceled out when the defender does not have access to the original version of the watermarked model, which is a common setting in previous works on model watermarking. In the case, the identity information can hardly be recovered, resulting a failed watermark verification. The results in Table \ref{tab:CaseA-12} validate the above point. As is shown, all the nine white-box watermarking schemes is unable to extract the expected signature from the protected model as the scaled BER is higher than 50\% according to \cite{sokwatermark}.

% \noindent\textbf{Case A-2} \textit{(w/. the original model).}
% The situation becomes different when we further assume the defender has the original watermarked model at hand. In this case, after dummy neuron elimination, he/she can further compare the parameters of the original model with the obfuscated model in order to recover the order and the scale of the parameters. From our perspective, for the $i$-th layer, this can done via running the Hungarian algorithm based on the pairwise cosine distance of neurons between the two models. Table \ref{tab:CaseA-12} reports the BER of the obfuscated model after the additional defense steps above. According to the results, we admit that when a skilled defender also has access to the original model, he/she can almost fully eliminate the preliminary DN and recover the watermark accuracy at the cost of some additional computing power.

% However, we would also like to emphasize that referring to the original watermarked model is inconsistent with the basic settings of white-box watermarking. In fact, due to the concerns on opening forgery and ambiguity attack surfaces when an ownership claimer could submit a model for verification, the target model is usually not allowed to be involved in the watermark verification process. Nevertheless, in the following, we present a novel dummy neuron generation algorithm to 

% \noindent$\bullet$\textbf{ Case B. Stealthiness of Generalized DN.}

% However, the above defense algorithm has the major limitation as follows: 
% (i) The detection complexity is under the control of the adversary. In the worst case, the above procedure incurs $O(\sum_{l=1}^{H}(N_l+d_l)^2)$ floating-point operations for calculating the pairwise distance of neurons in all the hidden layers, where $d_l$ is the number of dummy neurons injected by the adversary. In other words, the adversary can inhibit the success of the fully knowledgeable defender by maliciously injecting a large number of dummy neurons. Therefore, the detection algorithm would suffer from an intractable computational cost for eliminating the dummy neurons. (ii) Moreover, we experimentally find 
% even after eliminating the dummy neurons generated by NeuronClique and merges the ones generated by NeuronSplit, the defender can hardly restore the original weights of the merged neurons due to the shuffling and scaling invariance imposed in NeuronSplit. Therefore, the BER of the restored DNN remains over $50\%$, yielding no evidence for existing model piracy.  
 

%%%%%%%%%%%%%%%% $O(\sum_{i=1}^{H}(N_{i=1}^{H})d_i)$ FLOPS

