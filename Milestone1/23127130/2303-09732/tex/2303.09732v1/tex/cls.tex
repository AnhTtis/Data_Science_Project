\section{Conclusion}
By thoroughly analyzing the protection mechanisms of the state-of-the-art white-box model watermarks, our work for the first time reveals their common and severe security flaw in the resilience against neural structural obfuscation. To validate this, we propose the novel notion of dummy neurons and implement an automatic framework to generate and inject dummy neurons into a given DNN model in a stealthy yet offensive way, which arbitrarily tampers the embedded watermarks while preserving the model utility. Through extensive experiments, we find all nine state-of-the-art white-box model watermarks with claimed robustness against existing removal attacks fail to recognize the original watermark in the protected model after being obfuscated via our neural structural obfuscation attack. As amendments, we discuss possible defenses to strengthen the verification procedures and recover the model from structural obfuscation.   

% be almost random under our attack when no utility loss or training costs are incurred, and no prior knowledge of the training data distribution or the watermark methods is required. 

% We urgently recommend future works to rethink the robustness of white-box model watermarks before deployment in real-world systems. 

% Moreover, it would be very meaningful to search for more robust and resilient internal model features, which should be at least invariant under our exploited three types of invariant transforms, for model watermarking.