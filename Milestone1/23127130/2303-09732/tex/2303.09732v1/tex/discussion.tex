% ----------------------- TODO -----------------------------
% (new) the ineffective of neuron scaling equivalence
% ----------------------------------------------------------

\section{Discussions}\label{sec:discussion}
%% Coverage of Victim Models 
% 1.1 Constrains of BN or ReLU => GN, IN (OK), LeakyReLU (OK), Sigmoid, tanh (**)

% 1.2 More complex DNNs, such as ResNet with shortcut, Inception and so on, where our algorithm need to adapt. (** Appendix) --> structural, future work


% In Section \ref{sec:dnn_invariance}, we make a few assumptions about the architecture of DNNs only for the simplicity of demonstrating the existence of invariant neuron transforms. 
\noindent$\bullet$\textbf{ Applicability of Our Attack.}
For simplicity, our methodology part uses the convolutional and fully-connected layers as the motivating example. Nevertheless, First, our proposed neural structure obfuscation with dummy neurons is applicable to many other neural network components, most of which are already involved in the covered victim models we evaluate in Section \ref{sec:eval}. We leave the technical details in Appendix \ref{sec:app:coverage}.

% We will show all the them have their own equivalent models, and are therefore vulnerable to our proposed attack. Meanwhile, we show the identical vulnerability of other works to emphasize our attack's wide applications, which goes beyond model watermarking.
% , including the normalization methods, the activation functions and the special connection modes between neurons





% For example, LayerShuffle would cause the outputs of the transformed model to be no longer be aligned with the outputs of the original victim model, which may therefore invalidate the copyright statement. We strongly suggest existing defensive works involving DNNs further rethink the robustness when invariant neuron transforms are available to attackers.

% 2. More about black-box watermark (No free lunch between accuracy and watermark utility; backdoor-based defense and removal attack; complement to watermark or attack; backdoor detection => real backdoor or watermark backdoor?) white-box is actually more vulnerable (**)


% Recently and concurrently with our work, Lukas et al. at S\&P'22 \cite{sokwatermark} also evaluate whether existing watermarking schemes are robust against known removal attacks. Our work reveals the vulnerability of the white-box watermarking schemes which claim high robustness, while Lukas et al. mainly focus on black-box ones. Although they demonstrate a preliminary feature permutation attack against DeepSigns \cite{darvish2019deepsigns}, they incorrectly claim that other white-box watermarks (e.g., \cite{uchida2017embedding}) are permutation-invariant. It is a false sense of robustness. We prove that shuffling the weights from the first two dimensions by blindly applying our proposed LayerShuffle still cracks the victim model (\S\ref{sec:case_study}). Based on a systematic study of invariant neuron transforms and their implication on the security of white-box watermarks, our work requires no prior knowledge to the watermark algorithms and training data distribution, while causing no utility loss. However, the requirements on prior information and a clear performance drop are proven to be inevitable in their evaluation. 

\noindent$\bullet$\textbf{ Coverage of Attack Targets.} Recently, an increasing number of white-box model watermarking schemes have been published at top-tier venues, some of which are authored by industrial research institutes (e.g., from Microsoft \cite{darvish2019deepsigns} and WeBank \cite{fan2021deepip}), and are cited in patents \cite{rouhani2021digitalpatent}. It implies that white-box model watermarking is an active research area. Considering the recent proposal of standards and laws on AI copyright \cite{EU_standards,EU_report}, we believe the need for research on model copyright protection is current and pressing. In this work, we covered nine published white-box watermarking schemes. We leave the evaluation on some more recent watermarking schemes to future work.

% In this sense, as model watermarking is not reported to be applied in real-world applications currently, we hope our vulnerability study is ahead of the time and can help existing watermarking schemes avoid common security pitfalls before they are widely implemented. In this work, we covered nine previously published white-box watermarking schemes which are sufficiently representative in reflecting the major technical branches. We leave the evaluation on some more recent watermarking schemes in future works.


\noindent$\bullet$\textbf{ More Threats of Neural Structural Obfuscation.}
During our extensive literature research, we surprisingly discover that not only the white-box watermarking schemes but also other works have strong dependence on the integrity of the neural structure. For example, Chen et al \cite{chen2021copy} present DEEPJUDGE, a testing platform based on non-invasive fingerprint for copyright protection of DNNs, which measures the similarity between the suspect model and victim model using multi-level testing metrics. Despite the authors claiming strong robustness against various attacks in their evaluation, we notice that four out of the six testing metrics in DEEPJUDGE measure the distance between the two models' hidden layer/neuron outputs in the white-box scenario, most of which are also sensitive to the existence of dummy neurons. In our preliminary results reported in Table \ref{tab:deepjudge}, the scheme fails to verify the stolen model under neural structure obfuscation when using the same threshold suggested in DEEPJUDGE \cite{chen2022deepjudge}. The additional results further 
highlight our revealed vulnerability is important for future research works on model copyright verification to circumvent this design pitfall. 


%%%%% BEGIN deepjudge TABLE
\input{tex/tables/deepjudge.tex}
%%%%% END deepjudge TABLE

% \noindent$\bullet$\textbf{ Limitation.}
% As an active research area, we have tried our best to cover the previously published white-box watermarking schemes. 
% Experiment results?

% %% watermark recovery 
% % 3. Potential Defense or Detection (during wwm algorithms; post-processing during verification)
% \subsection{Potential Mitigation Strategies}


% % 4. Future work (Comprehensive detection and defense; wwm and other works rely on the model intrinsic locality)

