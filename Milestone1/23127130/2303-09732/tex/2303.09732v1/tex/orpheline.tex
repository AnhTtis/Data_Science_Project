Compared with the previous hijacking attacks on object detectors, the new PoC only requires $2\sim{3}$ attack frames to achieve the same attack effectiveness (cf. $60$ required attack frames in previous works).


 the removal process usually incurs larger decrease in model performance for white-box watermarks than for black-box ones. For example, machine unlearning techniques help remove a subset of prediction rules from a suspect model with almost no utility loss, while similar attacks are not applicable to white-box watermarks thanks to the intrinsic relations between the parameters and the identity messages \cite{ shafieinejad2021robustofbackdoorbased, chen2021refit, aiken2021laundering, guo2021ftnotenough}. On the other hand,
 
 
 In fact, while model watermark research is still ongoing, copyright audit and protection for AI models, for example, is listed as an essential aspect of AI security in existing standards and copyright laws. Therefore, as an important and expanding branch of model copyright protection, white-box model watermarking is a promising option for future real-world applications. In this sense, our current work is of importance as it reveals the common design pitfalls of many mainstream white-box model watermarking schemes. 

 
 

% For the first time, we reveal exploiting \textit{invariant neuron transforms} \cite{geometryfeedforward, neyshabur2015pathsgd, ganju2018property, bui2020relunetworksequivalence} is a simple yet effective attack strategy against the mainstream white-box model watermarking schemes, most of which heavily rely on a set of fragile features, including the orders, the magnitudes and the signs of a local group of neurons for watermark embedding. By applying a chain of invariant neuron transforms, we are able to arbitrarily manipulate the message embedded in the local feature while provably preserving the normal functionality of the model. As Table \ref{tab:intro_table} shows, we successfully crack $9$ representative white-box DNN watermarks, which claim high robustness against some or all known removal attacks, reducing the embedded watermark to be almost random. In comparison, almost no known existing attacks can simultaneously crack even $1/3$ of them. Besides, our attack leaves the model utility provably intact after watermark removal. Moreover, as a \textit{blind} attack, our approach requires no knowledge on the training data distribution, the adopted watermark embedding process, or even the existence of a watermark.

% % indicating the enhancement to the connection between significant model parameters and watermark effectiveness gives the model owner a false sense of watermark security.



% %%%% more technical discussions, why white-box watermarks are hard to remove
% % Existing attacks on white-box model watermarks suffer from inherent restrictions. On the one hand, most existing attacks are based on model post-processing techniques (e.g., fine-tuning and pruning), which unavoidably sacrifices the normal model utility for fully removing the watermark. On the other hand, more advanced white-box watermarking techniques leverages the strong correlation to the significant parameters[] or stable selected activation maps[] to survive the . As a result, full access to the suspect model to acquire the inner weight matrices or hidden activation maps during ownership identification appears to bring higher credibility, as recently proposed white-box watermark methods have claimed the strong robustness against various attacks[].


% % TODO: Rename model intrinsic locality

% Technically, the core building blocks of our attack framework are three types of invariant neuron transforms, namely \textit{LayerShuffle}, \textit{NeuronScale}, and \textit{SignFlip}, which manipulate three types of local features, i.e., \textit{orders, magnitudes and signs}, of specific weights or neuron activation. We find these local features of model parameters are heavily utilized in existing white-box model watermarking schemes, but are highly fragile under the corresponding transforms. To crack the existing white-box watermark schemes, we construct a chain of invariant transforms on the model parameters which reduces the watermark embedded in one set of local features to randomness, while compensating for the malicious perturbations on watermark-related parameters by calibrating other related local features. This yields a functionally equivalent model to the original watermarked one, yet without the watermark.
% % As the hidden activations are determined by both model parameters and input tensors, we change their intrinsic locality by modifying related model parameters.

% Specifically, we first rearrange the local order through \textit{LayerShuffle}, which applies an arbitrary permutation to the neurons within each selected hidden layer to reshuffle the pertinent model parameters. Then, we alter the local magnitude by \textit{NeuronScale}, which scales up/down the incoming edges to each randomly chosen hidden neuron with a positive real number, and scales down/up the outgoing edges with the same ratio correspondingly. Finally, we perturb the local sign with \textit{SignFlip}, which targets at flipping the signs of model weights arbitrarily sampled from the watermarked model, while adjusting the signs of related parameters in adjacent layers (i.e., a linear/convolutional layer or a normalization layer\cite{ioffe2015batch, wu2018gn, ba2016layer, ulyanov2016instance}).
% As these three operations are not mutually exclusive and complementary to each other, we integrate them into a comprehensive attack framework to crack popular white-box watermarking verification \textit{blindly}, i.e., without knowing the adopted watermark scheme and even the existence of a watermark. Also, unlike previous removal attacks based on fine-tuning or retraining \cite{adi2018turning, chen2021refit, guo2021ftnotenough}, our attack has no requirement on training data throughout the attack by design. 


%% Local Neuron Invariance

% ################## overview ##################
% To prove WWM rely on MIL => attack evaluation
% our attack indeed remove the wm => ? indicate that ...
% remind the future work not rely on MIL

% On the contrary, our work for the first time discovers and systematizes the innate vulnerabilities in white-box model watermarking, i.e. the local neuron invariance in watermarked models. We present the first attack framework to fully remove the white-box model watermark while incurring certifiably no influence on the model functionality, blindly applicable to most existing white-box model watermarking schemes, and requiring no prior knowledge on the details of watermark schemes or the distribution of training data. Our finding strongly indicates that evaluating robustness against limited attacks in earlier work is insufficient.
% % corresponding to three requirements of a strong removal attack
% First, our removal attack has no effect on the model's functionality, whereas common removal attacks against white-box watermarking by perturbing the model parameters (e.g. fine-tuning or pruning) always cause a measurable utility loss, which is undesirable in some mission-critical applications. Second, as we validate in Section \ref{sec:case_study}, our removal attack is effective to all the known white-box watermarking schemes as a post-processing method, while the adversary is unaware of the details of employed watermarking technique. Third, the results of our extensive work demonstrate that the data and computing power for fine-tuning are unnecessary to invalidate the white-box model watermarking verification, where a underlying commonality overlooked by the most researchers results the root vulnerability of these white-box watermark schemes.



To trace illegally distributed DNN models in the wild, recent progress in IP protection proposes a number of model watermarking strategies for ownership verification, according to the existence of a unique watermark embedded in a suspect model. In this paper, we focus on the white-box watermarking schemes, where the model parameters and activation maps are accessible during the verification.

% 2. Further to develop a overwriting attack (*)
\noindent$\bullet$\textbf{ Extension of Our Attack.}
By converting random invariant neuron transforms to particular ones, our proposed attack is readily for \textit{watermark overwriting}, which implants the identity information of malicious users. For example, we could incorporate any binary signature into the specific scale parameters of a victim model watermarked by DeepIPR. Specifically, the adversary can force the signs of scale weights of selected layers to match the specified signature, while preventing the original model utility from any unexpected impairment by reversing the signs of other relative parameters to obtain a SignFlip equivalence of the victim model. As a result, the legitimate owner is unable to claim the ownership of the victim model inserted with the owner's private passports due to the unmatched poor accuracy and the high BER, whereas the adversary is able to extract his/her own signature from the suspect model, perfectly matching with the adversary's identity information. 

As a final remark, we highlight that the extension of our removal attack to an effective overwriting attack is a post-processing technique at almost no cost, while previous works \cite{wang2019overwrite} overwrite the existing watermark by running the watermark algorithm again to embed a new watermark. This requires unrealistic access to prior knowledge such as the original training set and details of the watermark schemes, along with additionally more


As our removal attack produces an equivalence of the victim model, a straightforward defense is to recover the original model before watermark verification by inverting the applied neuron transforms. However, our unified attack framework consists of a chain of different neuron transforms, which makes the reverse-engineering a non-trivial task. Moreover, our removal attack can combine with any post-processing techniques such as fine-tuning and pruning, which further increases the cost to recover the original model with owner-specific watermarks.  Considering the core vulnerability of most known white-box algorithms, future works on robust watermark algorithms should consider the robustness under invariant neuron transforms into account. For example, it is plausible to sort the chosen weights before calculating the watermark-related regularization loss during the watermark embedding phase. As a result, the owner can reorder the LayerShuffle equivalence of the victim model to detect their identity information. It is worth to note that a potential mitigation on the vulnerability should not sacrifice the utility of the original model and the reliability of the watermark algorithm, which indeed ensures minimal false positive in verification but to some degree intensifies the challenges of mitigating our attack. We hope future works would devise robust white-box algorithms against our removal attack.

Via parameter and structure obfuscation, existing removal attacks on white-box DNN watermarks are mainly four types. 



%%%% PREVIOUS DEFINITION About the watermarking 
if Given a deep neural network $f$ and a new deep neural network $f '$ obtained from $f$ by inserting a number of neurons $\{m_i\}^{N}_{i=1}$ into some layers, we call these neurons are \textit{dummy neurons} if $f(x) = f '(x)$ for every input $x$.



%%%%%%%%%%%%%% THE REVISED PARAGRAPH ON NORMALIZATOION LAYERs


\noindent\textbf{Normalization Layers.} Normalization layers are ubiquitous in modern industry-level DNN architectures \cite{he2016resnet, radford2015dcgan, szegedy2016inception, zagoruyko2016wrn, xu2015caption,  devlin2018bert}. Historically, a normalization layer is first designed for accelerating the training process \cite{ioffe2015batch}. Later, its unique benefit on enhancing the model performance are empirically proved. Typically, a normalization layer directly follows a convolutional or a fully-connected layer in modern DNN architectures to first normalize the data features by the running mean $\mu$ and variance $\sigma^2$ of the historical training data. Then, a linear transformation is conducted with a scale factor $\gamma$ and a bias shift $\beta$. Formally, the computing rule of a normalization layer writes: $
\hat{x} = \gamma \frac{x - \mu}{\sigma} + \beta,$
where $\gamma$, $\beta$ are learnable parameters. 

As one of the most widely-used components, Batch Normalization (BN) \cite{ioffe2015batch,wu2018gn,ba2016layer,ulyanov2016instance} reduces the internal covariate shift in input data by normalization along the batch dimension. In recent years, other normalization layers, including Layer Normalization (LN), Instance Normalization (IN) and Group Normalization (GN) further exploit the channel dimension for better performance. As a final remark, we emphasize that proper parameters in the normalization layers are essential to reproducing the reported accuracy of the model when deployed. Due to the other devision in hidden feature distribution, even slight perturbation to the parameters in the normalization layers would cause catastrophic damage to the model utility. This phenomenon lays the ground for a recent branch of white-box model watermarks to encode the secret information into the parameters of selected normalization layers \cite{ong2021iprgan,fan2021deepip,zhang2020passportaware}.

%%%%%%%%%% OHTER SENTENCES
 \mytodo{which can control the contribution of the dummy neurons combined with selected original neurons.} 
 
 
 
Similar to the case of batch normalization we can obtain a SignFlip equivalence of the target model with group normalization, instance normalization or layer normalization by only reversing the signs of learnable parameters in the normalization layer and the preceding convolution/linear layer. Although the statistic values of these normalization layers are calculated on the fly during inference and the adversary is unable to modify them directly, the modifications on the trainable parameters of the preceding layers already result in sign flips in these calculated statistic values to produce a equivalent model, which is also corroborated by our evaluation results in Section \ref{section:DeepIPR}. 

%%%%%%%%%%%%%%%%%%% ANALYSIS ON THE RECIPROCITY BETWEEN SUCCESSIVE LAYERS
In fact, we can further weaken the defense effectiveness by injecting additional dummy neurons to the layer previous to the target one. As the outgoing weights of the dummy neurons are also the incoming weights of the dummy neurons in the target layer, we can adversarially specify the weights, under the zero-sum condition, to further reduce the difference between the dummy and the original neurons. \mytodo{(we need this part if the defense is good)}


%没讲清楚groupnorm'是按顺序分组
\noindent\textbf{Applicable to Normalization layers.}
The output normalization layer follows a convolutional layer with $N_l$ neurons can be written as:
\begin{equation}\label{eq:norm}
y = \gamma \frac{W_c\odot x - \mu}{\sigma} + \beta = \frac{\gamma}{\sigma} \cdot W_c \odot x + (\beta - \frac{\gamma \cdot \mu}{\sigma}),
\end{equation}
where $W_c$ is the convolutional weight of the precedent layer, $\gamma, \beta, \mu, \sigma$ are the scale weights, bias, mean values, and variance of the normalization layer with size of $N_l$, respectively. 

For batch normalization, which is widely applied in the state-of-art models, we modify the Equation 


% Different to BN, other normalization layers (e.g., GN) compute the mean and variance in the channel dimension and update these statistic values on the fly on inference stage. 
For group normalization, the neurons in the $l^{th}$ layer (i.e., $\{n^{l}_i\}^{N_l}_{i = 1}$) are first divided into $G$ groups. Formally, the $k^{th}$ group can be denoted as  $\{n^{l}_i\}^{k* N_l / G}_{i = (k-1) * N_l / G+1}$. Different from BN, GN computes the statistic values for each group independently throughout both the training and testing phases. Similar to Eq. (\ref{eq:bn_combine}), the output of the $i^{th}$ neuron which belongs to the $k^{th}$ group can be formally written as:
\begin{equation}\label{eq:gn_combine}
\gamma^l_i \frac{w^l_{\cdot i}\odot x + b^l_i - \mu^l_k}{\sigma^l_k} + \beta^l_i = \frac{\gamma ^l_i w^l_{\cdot i}} {\sigma^l_k} \odot x + \frac{\gamma ^l_i(b^l_i - \mu ^l_k)}{\sigma^l_k} + \beta^l_i,
\end{equation}
where $\mu_k$ and $\sigma_k$ are the mean and variance of the $k^{th}$ group. 

To obtain a SignFlip equivalence, we only reverse the signs of $\gamma_i$, $w^l_{\cdot i}$ and $b^l_i$ in each neuron. Then, we have $\mu_k' = -\mu_k$ and $\sigma_k' = \sigma_k$ by definition. Finally, after our SignFlip operation, the output of each neuron with group normalization is exactly same as the original result.

To obtain a LayerShuffle equivalence, we can first shuffle the neurons within each group and then further reorder these $G$ groups within each layer. Specifically, for the $k^{th}$ group in the $l^{th}$ layer, we randomly generate permutation $p_{k,in}$ from $1$ to $N_l / G$ and permutation $p_{l,out}$ from $1$ to $G$. Then, we obtain the final permutation $p$ for the $l^{th}$ layer as $p(i) =(p_{l,out}(k)-1)N_l/G+ p_{k,in}(r)$, where the $i^{th}$ neuron belongs to the $k^{th}$ group and $i = (k-1)N_l/G+r$. With permutation $p$, we have $\mu_k' = -\mu_{p_{l,out}(k)}$ and $\sigma_k' = \sigma_{p_{l,out}(k)}$ by definition. As a result, we can transform the model with GN layers by $\mathcal{LS}(B^l, p)$ as is shown in Section \ref{sec:layershuffle} to obtain a LayerShuffle equivalence. 

To obtain a NeuronScale equivalence, we scale up/down the incoming/outgoing weights of each neuron which belongs to the $k^{th}$ group in the $l^{th}$ layer with the same positive factor $\alpha_k$. Then we leverage $\mathcal{NS}(n^l_i, \alpha_k)$ for the $i^{th}$ neuron in $k^{th}$ group to obtain a NeuronScale equivalence.

Moreover, instance normalization and layer normalization are both the simplified forms of group normalization, once we set the $G$ in group normalization to $N_l$ and $1$, respectively. As a result, our attack based on invariant neuron transforms can be further applied to the model with these three normalization layers, i.e., GN, IN and LN.



\noindent\textbf{Applicable to Other Complex Model Architectures.}