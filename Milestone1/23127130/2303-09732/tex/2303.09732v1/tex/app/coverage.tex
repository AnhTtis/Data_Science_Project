\section{Omitted Technical Details}
\subsection{Technical Details of Kernel Expansion}
\label{sec:app:kernel_expansion}
In Section \ref{sec:dn_camouflage}, we obfuscate the kernel shape of the dummy neurons in convolutional layers with vanishing weights for intuition. We provide the technical details of the kernel expansion with non-zero values in this section. Consider a convolutional layer with $N$ neurons, i.e., $\{n_i\}_{i=1}^{N}$, we can split each neuron $n_i$ into two substitute neurons as $n_i'$ and $m_i$, which have the same incoming weight and satisfy the replacement identity, i.e., $W_{i,in}' = U_{i,in} = W_{i,in}$ and $W_{i,out}' + V_{i,out} = W_{i,out}$. As a result, the expanded convolutional layer with $2\times N$ neurons can be denoted as $\{n_i'\}_{i=1}^{N} \cup \{m_i\}_{i=1}^{N}$. Then we can expand the kernel in the outgoing weight of replaced neuron $n_i'$ and $m_i$ with opposite values, while the extra features values in the next layer introduced by the expanded weights of $n_i'$ can be canceled out by the dummy neuron $m_i$, as their outputs in the current layer are exactly the same to each other. 

% \subsection{An Adaptive Attack on Dummy Neuron Detection}
% \label{sec:app:stealthier_dn}
% \subsubsection{Proposed Methodology}
% To further improve the stealthiness of our injected neurons, we propose to remove the exploitable trace, i.e., the proportionality between the incoming weights of the dummy neurons, in an \textit{adaptive attack}. Specifically, we first generate their input weight randomly, then choose the bias of each injected neurons to keep them being activated under any valid inputs. Finally, we construct their outgoing weights by solving an under-determined linear equation system for preserving the original model behavior via vanishing the total contribution of the newly inserted dummy neurons. We elaborate on the technical details as follows. For convenience, we take a three-layer fully-connected neural network with ReLU activation, i.e., $f(x) = W_2\sigma(W_1x+b_1)+b_2$ as an example. We suppose the input $x\in[a,b]^{d}$ (e.g., for normalized image inputs, we usually have $a=-1, b=1$), $W_1\in\mathbb{R}^{d_1\times{d}}, b_1\in\mathbb{R}^{d_1}, W_2\in\mathbb{R}^{K\times{d_1}},b_2\in\mathbb{R}^{K}$.

% %%%%%%%% BEGIN OF NEW ATTACK
% %%%%%%%%%%%%% BEGIN OF naive dummy neuron 
% \begin{figure}[h]
% \begin{center}
% \includegraphics[width=0.35\textwidth]{img/gdn.pdf}
% \caption{A schematic diagram of generalized dummy neuron generation algorithm.}
% \label{fig:gdn}
% \end{center}
% \end{figure}
% %%%%%%%%%%%%%% END OF naive dummy neuron
% %%%%%%%% END OF NEW ATTACK


% %%%%%%% BEGIN TABLE 
% % \input{tex/tables/full_elim_table.tex}
% %%%%%%% END TABLE 

% As in Fig.\ref{fig:gdn}, $N$ dummy neurons are injected in the hidden layer. Denote the incoming weights and biases related with the dummy neurons are $\tilde{W}_1 \in \mathbb{R}^{N\times{d}}$, $\tilde{b}_1 \in \mathbb{R}^{N}$, and the outgoing weights from the dummy neurons to the neurons in the last layer as $\tilde{W}_2\in\mathbb{R}^{K\times{N}}$. Therefore, to preserve the original model behavior requires the following equality to be satisfied for all $x\in[a,b]^{d}$:
% \begin{equation}
% \label{eq:vanish}
%     \tilde{W}_2\sigma(\tilde{W}_1x + \tilde{b}_1) = 0.
% \end{equation}
% First, we choose $\tilde{W}_1, \tilde{b}_1$ such that $\tilde{W}_1x + \tilde{b}_1 \succeq 0$ for all $x\in[a,b]^{d}$, i.e., the dummy neurons are always activated under the valid inputs. 
% % As we can see, such neurons are not rare. 
% Under this situation, Eq.\ref{eq:vanish} is reduced to $ \tilde{W}_2(\tilde{W}_1x + \tilde{b}_1) = 0$. To satisfy the equation for all valid inputs, $\tilde{W}_2$ is required to satisfy the following condition.  
% % \mytodo{The first layer of ResNet-18 trained on CIFAR10 has some neurons that activated by all the test data. Table \ref{} reports the ratio of neurons which satisfy the above condition in a normally trained neural network.}

% \begin{equation}
% \tilde{W}_2 A = 0,
% \end{equation}
% where $A = [\tilde{W}_1; b_1] \in \mathbb{R}^{N\times{(d+1)}}$. According to a basic theorem in linear algebra \cite[Theorem 18]{Magnus2019MatrixDC}, such a $\tilde{W}_2$ always exists, and can be constructed in the following form:
% \begin{equation}
% \label{eq:constr_W2}
%     \tilde{W}_2 = Q - QAA^{\dagger},
% \end{equation}
% where $Q \in \mathbb{R}^{K\times{N}}$ is an arbitrary matrix, and $A^{\dagger}$ denotes the pseudo-inverse of the matrix $A$. In summary, to construct the generalized DN, we first sample $\tilde{W}_1$ and $\tilde{b}_1$ to ensure the dummy neurons to be always active, and then sample a random matrix $Q$ to construct $\tilde{W}_2$ according to Eq.\ref{eq:constr_W2}. 

% % \subsection{Detailed Dummy Neuron Elimination Algorithm}
% % \label{sec:app:alg}

% % \subsubsection{Preliminary Results}
% % \noindent$\bullet$\textbf{ Evading the Skilled Defender.} In the above design, the unparalleled incoming weights invalidate the elimination procedure in Algorithm \ref{alg:dn_elim}, which becomes ineffective in merging these stealthier DNs and thus leaves the size of relative parameters/activation maps unmatched for watermark extraction. After conducting the error-handling mechanisms in Section \ref{sec:eval}, the first row of Table \ref{tab:full_elim_table} shows the stealthier DN injection still inhibits the verification procedures of most white-box watermarking schemes from recognizing the original watermark because $100\%$ of the inserted dummy neurons remain in the dummy neuron after the dummy neuron elimination.

% % \noindent$\bullet$\textbf{ Evading the Fully Knowledgeable Defender.} When using the more stealthy dummy neuron construction in the previous part, the attacker has the flexibility to set the incoming weights of the dummy neurons to be the same incoming weights of the original neurons. In this way, the defender has no additional knowledge to determine which one of the neurons is from the original model. If the defender just merges the neurons with the same incoming weights, the watermark message is not recovered according to our experimental results in the second row of Table \ref{tab:full_elim_table} (i.e., BER $>50\%$). It is mainly because the outgoing weights of the injected dummy neurons in the new design has not to cancel each other out as for the original design.

\subsection{On Attack Applicability}
\label{sec:app:coverage}
\noindent\textbf{Dealing with Normalization Layers.}
Similar to fully-connected layers, normalization layers such as batch normalization \cite{ioffe2015batch}, group normalization, and instance normalization implement an elementwise linear transformation on the input $x$: $\hat{x} = \gamma\odot\frac{x - \mu}{\sigma} + \beta,$
where $\gamma$, $\beta$ are learnable parameters and $\mu,\sigma$ are the statistics of the historical training data. Typically, a normalization layer follows a convolutional/fully-connected layer in modern DNN architectures. Therefore, to stay compatible with the dummy neurons injected in the preceding layers, our attack correspondingly expands the normalization layers by assigning the identical coefficients for the inputs from the dummy neurons in the same group.


% DN不存在该限制，因为消去过程都是：先过激活函数，再在outgoing weight和为0
% 加上scale equivalence就存在ReLU限制
% \noindent\textbf{Extension to More Activation Functions.} Similar to the case of ReLU, our attacks are applicable to the DNNs with other activation functions such as LeakReLU, PReLU, RReLU and \textit{tanh} \cite{he2015rectifiers,xu2015linearunit}, only if the activation function $\sigma$ satisfies $\sigma (ax) = a\sigma(x)$ with $a\in \mathbb{R}$ in testing. We admit some other activation functions, e.g., \textit{sigmoid}, would disable the equivalence under NeuronScale. Nevertheless, it has limited effect on the effectiveness of our unified attack framework when attacking DNN architectures with these special activation functions, as the other two transforms (i.e., SignFlip and LayerShuffle) are able to remove the potential watermark regardless of the choice of activation functions.

\noindent\textbf{Dealing with Other Complex Model Architectures.}
Besides, our removal attack can be easily applied to the watermarked DNNs with special connections between the neurons of different layers, e.g. ResNet with shortcuts \cite{he2016resnet} and Inception-V3 with parallel convolution \cite{szegedy2016inception} operations, which have much more complicated architecture than the simple convolutional neural networks. For example, we can obtain the equivalent branches in each Inception block separately to remove the embedded watermark in Inception-V3. More technical details can be found in Appendix \ref{sec:app:coverage}. Only if the adversary knows well about the forwarding computation in the target DNN, which is a common knowledge in white-box watermarking settings, he/she can readily extend our attack with small adjustments, which is left for future works. 


In Section \ref{sec:discussion}, we discuss the broad applicability of our attack on other watermarked models with various architecture, i.e., ResNet, not limited to simple convolutional neural network, by carefully setting the injection positions. We provide more detailed analysis and proofs below. First, we combine the $l$-th convolutional layer with possible normalization layer (e.g., batch normalization) as follows:
\begin{equation}
    y^l = \gamma \frac{W_c\odot x -\mu} {\sigma}+ \beta = (\frac {\gamma} {\sigma} \cdot W_c )\cdot x + (\frac {\mu} {\sigma} - \beta),
\end{equation}
as the adversary has full control over the victim model. We denote the weight of the combined convolutional layer as $W' = \frac {\gamma} {\sigma} \cdot W_c $ and $b' = \frac {\mu} {\sigma} - \beta$.

For ResNet \cite{he2016resnet}, we inject the same ratio of dummy neurons into the same position of every layer to confront with the existence of skip connections. As a result, the outputs of the dummy neurons generated by our attacks for a certain layer will produce the same feature maps, as we align the output of dummy neurons from different layers with the same size before the possible shortcut connections.

For Inception-V3 \cite{szegedy2016inception}, the inception modules apply multiple sizes of kernel filters to extract multiple representations, which usually consists of several branches. As a result, we generate the dummy neurons with the output weights which satisfy the cancel-out or replacement identity for each individual branch, and then inject these dummy neurons into each layer as proposed in Section \ref{sec:dn_injection}.


