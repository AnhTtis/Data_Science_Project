\section{Omitted Proofs}
\label{sec:app:proof}

\begin{proof}[Correctness of \textit{NeuronClique}.] Below, we prove the output of the obfuscated model is the same as the original one after this group of dummy neurons is injected. First, we denote the original outgoing weights and the output of $i$-th neuron in the $l$-th layer are $W^{l}_{i,\text{out}}$ and $h^{l}_i$, respectively. Then the output of the $(l+1)$-th layer after injecting dummy neurons in the $l$-th layer can be written as:
\begin{equation}
h^{l+1}{}' = \text{ReLU}( \sum_{i=1}^{N_l}(W^{l}_{i,\text{out}} \odot h^l_i) + \sum_{k=1}^d (V^{l}_{k,\text{out}} \odot g^l_k)), 
\end{equation}
where $g^l_k = \text{ReLU}(U^{l}_{k,in} \odot h^{l-1})$ is the output of the $k$-th dummy neuron added into the $l$-th layer. As we set the incoming weights for each dummy neuron as identical, we have $g^l_k = g^l_1$ for $k = 1, 2, ..., d$. Combined with other conditions, the contribution of the dummy neurons to the $(l+1)$-th layer is actually equal to $\textbf{0}$, i.e., $\sum _{k=1}^d (V^{l}_{k,out} \odot g^l_k) = \sum _{k=1}^d (V^{l}_{k,out}) \odot g^l_1 = \textbf{0} \odot g^l_1 = \textbf{0}$. As a result, we can further simplify the formulation of $h^{l+1'}$ as follows:
\begin{equation}
h^{l+1'} = \text{ReLU}(\sum _{i=1}^{N_l}(W^{l}_{i,out} \odot h^l_i)+ \textbf{0}) = h^{l+1},
\end{equation}
which indicates that the output of the victim model after we inject a group of dummy neurons generated by NeuronClique is exactly same as before.
\end{proof}


\begin{proof}[Correctness of \textit{NeuronSplit}]
Similar to NeuronClique, we prove that splitting the original neuron into several substitute neurons by NeuronSplit has no unexpected effect on the functionality of the target model. With the selected neuron replacement and dummy neuron injection, the output of the $(l+1)$-th layer can be written as:
$h^{l+1}{}' = \text{ReLU}( \sum _{i=2}^{N_l}(W^{l}_{i,out} \odot h^l_i) + \sum _{k=0}^d (V^{l}_{k,out} \odot g^l_k)), 
$
where $g^l_k = \text{ReLU}(U^{l}_{k,in} \odot h^{l-1}+ b^{l}_k)$ is the output of the $k$-th substitute neuron injected to the $l$-th layer. Because the values of the incoming weights for each substitute neuron are set as identical to the original neuron $n^l_1$, we have $g^l_k = h^l_1$ for $k = 0, 1, ..., d$. Combined with other identities, we formulate the contribution of these substitute neurons to the $(l+1)$-th layer as $\sum _{k=0}^d (V^{l}_{k,out} \odot g^l_k) = \sum _{k=0}^d (V^{l}_{k,out}) \odot h^l_1 = W^l_{1,out} \odot h^l_1$, which is equivalent to $n^l_1$ in the original model. As a result, we can further simplify the formulation of $h^{l+1'}$ as follows:
$
h^{l+1}{}' = \text{ReLU}(\sum _{i=2}^{N_l}(W^{l}_{i,out} \odot h^l_i) + W^l_{1,out} \odot h^l_1)  = h^{l+1},
$
which indicates that the output of the victim model after replacing the selected neuron and injecting a group of dummy neurons generated by NeuronSplit is provably the same as before.
\end{proof}