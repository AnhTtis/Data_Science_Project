%%%% The section of threat model 
\section{Security of White-Box DNN Watermark} 
% With increasingly more white-box watermarking techniques exploring the usage of the strong correlation to the significant parameters or selected stable activation maps to claim even stronger resilience than previous approaches\cite{ wang2021riga,liu2021greedyresiduals, chen2021lottery,fan2021deepip,lim2022ipcaption,zhang2020passportaware,ong2021iprgan}, recent research at the attack side starts to question for the robustness of existing watermarking schemes. However, to the best of our knowledge, \cite{sokwatermark} is the only existing systematic evaluation on the robustness of black-box DNN watermarks. In contrast, the robustness of white-box watermarks is mainly evaluated by the very scheme designers. 
\subsection{Security Settings}
% 段落整合，攻击分类攻击目标，ambiguity vs remove
%%% 介绍现有攻击的几种目标（ambiguity and removal (maybe others)）
\noindent\textbf{Attack Taxonomy.} According to the adversarial goal, we first categorize existing attacks on white-box model watermarking from previous works into \textit{ambiguity attacks} and \textit{removal attacks}. In the former attacks, the adversary aims at constructing a counterfeit watermark, when given the watermarked DNN \cite{zhang2020passportaware}, to pass the verification process. Instead, the removal attacks have a more straightforward goal: invalidating the verification process by removing the secret identity message from the protected model. Considering its severe influence on establishing the model ownership, our work concentrates on devising novel removal attacks to crack the state-of-the-art white-box model watermarks. Below, we formally describe the attack scenario. 
% leaving the secret watermark embedded intact
%  serves as a solid counter-evidence against the lawsuit filed by the owner.

\noindent\textbf{Attack Scenario.} In our threat model, the adversary has obtained an illegal copy of a watermarked model which allows full access to its model parameters. Such model piracy can be accomplished via either algorithmic attacks \cite{tramer2016StealViaApi, yu2020cloudleak} or system attacks exploiting software/hardware vulnerabilities \cite{jeong2021meltdown,yan2020cache}. To conceal the traces of model infringement, the attacker attempts to invalidate the model ownership verification by removing the existing watermarks. 

\noindent\textbf{Attack Budget.} As listed in Table \ref{tab:intro_table_comparison}, the attack budget of a removal attack is mainly measured in the following dimensions: \textit{utility loss}, \textit{training cost}, \textit{dataset access}, \textit{watermark knowledge} (similar to the ones on black-box watermark in \cite{sokwatermark}).

%%% 需要讨论
\begin{itemize}
\item \textbf{Utility Loss:} When removing the watermark, obfuscation on the parameters or the structure of the DNN model seems inevitable. In this case, the obfuscation should not incur a large decrease in the normal model utility, which is otherwise unacceptable because the attacker still requires the normal utility of the pirated model for profits.
\item \textbf{Training Cost:} For watermark removal, the adversary is usually unwilling to cost a similar scale of computing resources as retraining the DNN model from scratch. Typically, the adversary would avoid the expensive model training process, which usually involves the usage of high-end graphical cards for training industry-level models, but prefer to learning-free attacks.  

\item \textbf{Dataset Access:} As the training is usually a private asset of the model owner, the access to the original training data or even a public domain dataset brings an additional attack budget. The adversary would prefer to involve no real data inputs for conducting the attack.  

\item \textbf{Watermark Knowledge:} The adversary should have no knowledge about the adopted watermark embedding and extraction algorithms, which are usually exclusively known to the owner until the ownership verification is launched. 
% To obtain such knowledge, the attacker needs to invoke some precondition attacks to determine the specific location (e.g., the parameters in which layer) of the embedded watermark, which incurs additional attack budgets.   

\end{itemize}



%%%%%%%%%% TODO 
\subsection{Limitation of Existing Removal Attacks}
\label{sec:limitations}
Previous removal attacks
% based on parameter and structure obfuscation 
are all limited in one or more of the above dimensions for fully removing white-box watermarks from a protected model. 

\begin{itemize}

\item \textbf{Pruning} sets a proportion of redundant parameters in DNN to zero, under which previous white-box watermark is highly resistant. To fully remove the watermark, pruning has to remove a substantial amount of weights, which causes an unacceptable utility loss \cite{uchida2017embedding, darvish2019deepsigns}.

\item \textbf{Finetuning} continues the training operation for a few epochs without the watermark-related loss. This removal attack additionally requires a certain amount of domain data and computational resources, otherwise the model utility would degrade \cite{chen2021refit, guo2021ftnotenough}.

\item \textbf{Overwriting} is first proposed in \cite{wang2019overwrite} to show the vulnerability of  \cite{uchida2017embedding}. Considering the adversary has full knowledge about the watermarking process, he/she may confuse the verification by embedding his own identification information. However, the details of watermark schemes are always not available in real-world settings. Meanwhile, overwriting attacks usually fail to encode a new message into the target model following more advanced schemes \cite{fan2021deepip, ong2021iprgan, lim2022ipcaption, zhang2020passportaware}.

\item \textbf{Extraction} refers to an attack class which utilizes knowledge distillation techniques \cite{hinton2015distilling} on the pirated model to obtain an obfuscated model which usually has a different architecture. The extraction attack inevitably involves a substantial amount of training costs to distill a well-trained obfuscated model. Although some very recent works in knowledge distillation eliminate the assumption on dataset access \cite{Yin2020DreamingTD}, most mainstream extraction attacks still use the conventional knowledge distillation approaches and require the access to a domain dataset to reduce the utility loss.     
\end{itemize} 


% We do not consider the model extraction attacks (e.g., \cite{Jagielski2020HighAA}) in our work, as this branch of works mainly aim at stealing models from prediction APIs and also require a substantial amount of training cost to distill a well-trained surrogate models from the victim model. In summary, none of existing removal attacks could meet all the requirements in Section \ref{sec:threat_model} simultaneously. Meanwhile, these attacks are proven to be more effective on black-box model watermarking instead, probably because the black-box model watermarking only rely on the prediction results on a specific set of inputs and hence the behavior can be altered without concerns on the otherwise constraints on modifying some internal parameters which may be related with the white-box model watermarking \cite{ shafieinejad2021robustofbackdoorbased, chen2021refit, aiken2021laundering, guo2021ftnotenough, wang2019neuralcleanse}. With more novel white-box watermarking techniques proposed recently claiming even stronger robustness under limited evaluation, white-box model watermarking attracts increasing attention from the academy and industry as more robust and desirable for practical usages.

%  \item \textbf{No Access to Original Training Data:} As the training is usually a private asset of the model owner, the access to the original training data or even data from a similar domain, is not always practical in reality. Therefore, our work restricts the attacker from obtaining any knowledge of confidential distribution of the training data, since he/she otherwise may legally train his own model from scratch, which makes the white-box watermark removal attack unnecessary.

% \begin{tcolorbox}%[top=3pt]
% \textbf{Summary}: Existing attacks are all limited in one or more aspects for removing the state-of-the-art white-box watermarks from the protected model.   
% \end{tcolorbox}