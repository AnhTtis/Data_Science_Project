% ----------------------- TODO -----------------------------
% 0. Compare to the digital watermark in multimedia (related)
% 再强调下 computing cost; inference time cost (intro)
% 2. 已改：黑盒水印对准确率的影响更大
% 3. 传统方法和针对trigger的removal方法开头放一起，然后白盒水印直接避开了大部分removal attack？先后顺序
% 3. 这个example好奇怪，不如换成高比例pruning下白盒验证有效黑盒验证无效
% 3. finetuning 这些方法对于黑盒、白盒水印效果都不好；白盒的优点是即使pruning比例很高，acc降低了，但是水印依然能被提取
% 4. 正则化项
% 白盒水印的鲁棒性空间，在最后的sign函数提取水印
% ----------------------------------------------------------

% 黑盒攻击为什么不用在白盒水印上？1，2，3罗列

\section{Introduction}
Nowadays, the computational and engineering costs of training a giant DNN model increase faster than ever \cite{he2016resnet, xu2015caption,devlin2018bert, strubell2019energy}. As a critical asset of AI corporations, well-trained DNNs are exposed under the risk of model stealing attacks \cite{oh2019reverseNN, orekondy2019knockoff, wang2018stealhyper, yan2020cache,Jagielski2020HighAA,Zhu2021HermesAS}, which makes the need for model copyright protection current and pressing. As a rescue, the past few years witness the emergence of DNN watermarking \cite{uchida2017embedding, wang2021riga,liu2021greedyresiduals, fan2021deepip, zhang2020passportaware, ong2021iprgan,chen2021lottery, lim2022ipcaption,darvish2019deepsigns,adi2018turning, szyller2021dawn, jia2021entangled} for tracing illegal model copies in the wild \cite{sokwatermark}. 
Generally, a model watermarking scheme consists of \textit{watermark embedding} and \textit{verification}. At the former stage, a secret identity message, i.e., the \textit{watermark}, is first embedded into \textit{the target model} along with the training process. At the latter stage, the scheme verifies the ownership according to whether the same or a similar watermark is detected from \textit{a suspect model}. 

According to the location of the embedded message, existing DNN watermarks are categorized into \textit{black-box} and \textit{white-box}. Intuitively, a black-box watermark is embedded in the model's prediction behavior on a special set of inputs \cite {adi2018turning,zhang2018blackwatermark, szyller2021dawn, jia2021entangled}, while a white-box watermark is embedded in the model internals, including the model parameters \cite{uchida2017embedding,  wang2021riga,liu2021greedyresiduals, chen2021lottery,fan2021deepip, zhang2020passportaware, ong2021iprgan} and the neuron activation \cite{darvish2019deepsigns, lim2022ipcaption}. The difference above also determines the required access mode to the suspect model for verification. As suggested by Fan et al. \cite{fan2021deepip}, in a real-world copyright dispute, the owner may first collect evidence of model piracy via a black-box query and then attain the white-box access via law enforcement for ownership verification. 


%%%%% BEGIN INTRO TABLE
\input{tex/tables/intro_table_ccs.tex}
%%%%% END INTRO TABLE

Sacrificing less functionality and involving more information for verification, white-box model watermarks are widely considered more comprehensive compared with the black-box counterpart \cite{wang2021riga,liu2021greedyresiduals, fan2021deepip, jia2021entangled}, with increasingly more research efforts on top-tier AI/security/system venues and from industry leaders (e.g., Microsoft \cite{zhang2020passportaware, darvish2019deepsigns, chen2019deepattest}). In a typical attack scenario, the adversary with a stolen DNN would modify the parameters or the structure of the model to frustrate the success of watermark verification \cite{see2016compression, li2016filterpruning, hinton2015distilling, wang2019overwrite, yang2019distillremoval, shafieinejad2021robustofbackdoorbased, chen2021refit, aiken2021laundering, guo2021ftnotenough, wang2019neuralcleanse}. To achieve the attack goal, the primary constraints for the attacker are (i) the obfuscation process should not cost more \textit{resources} than training a DNN from scratch and (ii) the \textit{utility} of the obfuscated model should have no clear decrease. 


However, as summarized in Table \ref{tab:intro_table_comparison}, none of the existing approaches can balance well the cost on utility or computing resources for fully removing the embedded watermark. On the one hand, removal attacks by parameter modification  
inevitably encounter degradation in the normal model utility \cite{adi2018turning,zhang2020passportaware,fan2021deepip,liu2021greedyresiduals,ong2021iprgan,chen2021lottery}. Relying on the internals of the suspect model, the embedded identity messages in white-box watermarking are much strongly connected with the model performance. 
Therefore, attack attempts via conventional post-processing techniques  \cite{see2016compression, li2016filterpruning}, which show empirical success on black-box model watermarks, inevitably perturb the model parameters at an unacceptable scale to fully remove a white-box model watermark \cite{uchida2017embedding, wang2021riga,liu2021greedyresiduals, chen2021lottery, fan2021deepip, zhang2020passportaware, ong2021iprgan,darvish2019deepsigns, lim2022ipcaption}. On the other hand, existing structural modification attacks apply knowledge distillation on the target model to construct a substitute model with similar performance but of different neural architecture   \cite{hinton2015distilling,Gou2021KnowledgeDA}. However, they usually require additional computational resources for training the substitute model. Besides, some attacks further require the access to a domain dataset or require additional knowledge about the embedded watermark \cite{wang2021riga}, which are usually impractical for attacks in the wild.  
% and distillation yang2019distillremoval,

\noindent\textbf{Our Work.} We for the first time show, most of the state-of-the-art white-box DNN watermarks share common vulnerabilities in their verification procedures which assume the structural integrity of the suspect model after being obfuscated by the attacker. Our current work constructs a novel neural structural obfuscation attack which intensively modifies the architecture of the victim model to disable the verification procedures of nine previously published schemes. Meanwhile, our attack incurs no utility loss and training costs, and requires neither dataset access nor the knowledge about the embedded watermark. 
At the core of our newly proposed attack is the concept of \textit{dummy neurons}, literally a group of neurons which can be added to a target DNN model for intensively perturbing the embedded watermark while provably leaving the model behavior \textit{invariant} (i.e., the model output remains the same under the same input). A naive example is neurons which have the input and output weights of zero values, which, if added to a DNN model, have no contribution to its output. As a preliminary yet effective attack, the adversary obfuscates the protected model by injecting a number of these neurons to every neural layer, which already inhibits most of the state-of-the-art white-box watermarks from being executed, but has clear limitation in its attack stealthiness (\S\ref{sec:motivation}).    


 
 % Our discovery justifies a misconception by the inventors of existing white-box model watermarking schemes on the robustness.  


% It is believed that it is almost impossible for white-box watermarks to be removed from a DNN model without devastating its normal utility. \morinop{(how to support this statement?)}


% Besides, in a white-box verification, the suspect model is fully transport to the verification process, leaving the adversary almost no freedom for ownership obfuscation. It explains why black-box watermarking is usually viewed as  to the white-box watermarking.

%%%% THE CONCEPT OF DUMMY NEURONS and a simple case
%% Generation: (i) A group of neurons which have canceling effect; (ii) Split one existing neuron into two

Alternatively, we propose a more comprehensive attack framework to automatically generate and inject dummy neurons into a victim model, which implements by-design stealthiness of the injected dummy neurons when the obfuscated model is under inspection. For dummy neuron generation, we propose \textit{NeuronClique} and \textit{NeuronSplit}, two novel structural obfuscation primitives to construct groups of dummy neurons, where the neurons are associated with non-vanishing weights but still bring no change to the model output. Specifically, the \textit{NeuronClique} primitive directly generates an arbitrary number of neurons which are assigned with weights that can cancel the others' output out, while \textit{NeuronSplit} converts a neuron in the victim model into two substitute neurons which preserve the replaced neuron's functionality (\S\ref{sec:dn_generation}).

%% Injection: (i) Using shuffle to randomize the position of the neurons; (ii) Using scale to further randomize the distribution.
For dummy neuron injection, our proposed framework carefully designs the injection order and leverages the reciprocity between dummy neurons in successive layers to enhance the attack stealthiness (\S\ref{sec:dn_injection}). Furthermore, we leverage the scaling invariance in DNN \cite{neyshabur2015pathsgd} to provide the adversary with the flexibility to specify the weight distribution of the dummy neurons to follow the same distribution of the original neurons, and the shuffling invariance in DNN \cite{ganju2018property} to randomize the location of the injected dummy neurons among the original neurons. Finally, we also introduce the kernel expansion technique to further obfuscate the weight shape of the dummy neurons, which, as the final straw, turns the victim model into a structurally irrelevant model with its original self (\S\ref{sec:dn_camouflage}). In \S\ref{sec:eval:stealthiness_dn}, we discuss and experiment with the feasibility for a defender of different knowledge on our attack to attempt to remove the dummy neurons.  

% Our attack evaluation is featured with a systematic study on nine mainstream white-box watermarking schemes. Despite the claimed robustness \cite{uchida2017embedding, wang2021riga,liu2021greedyresiduals, chen2021lottery, fan2021deepip, zhang2020passportaware, ong2021iprgan,darvish2019deepsigns, lim2022ipcaption}, our case-by-case analysis shows most of the watermarking schemes are even not executable for validating the obfuscated victim model. For the survivors, the extracted identity messages are reduced to almost random after our obfuscation attack. Meanwhile, our attack incurs no model utility loss to achieve the above attack effectiveness. We strongly recommend future works to carefully rethink our discovered vulnerability of white-box model watermarking, and propose more advanced defense techniques to fix this severe security flaw.


% Besides, we also provide additional experiments to validate the stealthiness of the dummy neurons in the victim model. 

% This reminds the researchers to take the conduct more comprehensive evaluations to the robustness of white-box watermarking algorithms, so that future works relying on model intrinsic locality, not just white-box watermarking, can avoid falling vulnerable to these same attack approaches.
% More generally, we hope that future work will be able to avoid relying on obfuscated gradients (and other methods that only prevent gradient descent-based attacks) for perceived robustness, and use our evaluation approach to detect when this occurs. Defending against adversarial examples is an important area of research and we believe performing a careful, thorough evaluation is a critical step that can not be overlooked when designing defenses.
% Additionally, we hope to provide researchers with a common baseline of knowledge, description of attack techniques, and common evaluation pitfalls, so that future defenses 

\noindent\textbf{Our Contributions.} In summary, we mainly make the following contributions:
\begin{itemize}
\item We for the first time reveal the common vulnerability of the state-of-the-art white-box DNN watermarks to neural structural obfuscation with dummy neurons.

\item We devise a comprehensive attack framework which automatically generates groups of dummy neurons into a protected model with newly proposed attack primitives. 

% Besides, we also offensively exploit DNN invariant transforms to allow the dummy neurons to be indistinguishable from the original neurons.      
 
% TODO: BER等指标下降程度
\item We validate the success of our attack on a wide group of DNNs protected by nine published white-box watermarking schemes. Despite the claimed robustness, the success rate of watermark verification is reduced to random after our attack, while the normal model utility remains the same.

\item We also provide a study on the stealthiness of these dummy neurons and present a dummy neuron elimination algorithm. This possible defense eliminates the dummy neurons, while the original model watermark in the protected model is recovered only when the defender has access to the original watermarked model.


\end{itemize}

% We report the success of our attack framework on almost all the state-of-the-art white-box watermarking schemes, which completely invalidate the success of watermark verification: 