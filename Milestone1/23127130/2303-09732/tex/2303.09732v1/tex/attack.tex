\section{Neural Structural Obfuscation}
% To address the limitation of the preliminary attack, we present a more comprehensive attack framework to automatically generate and inject more stealthy dummy neurons to crack the white-box model watermarking schemes, which effectively invalidates the ownership verification with almost no attack budgets. 




\subsection{Attack Overview}
In Fig.\ref{fig:atk_pipeline}, our proposed attack consists of three major steps.
1) \textit{Dummy Neuron Generation}: Inspired by the preliminary attack in Section \ref{sec:motivation}, we propose two non-trivial dummy neuron generation primitives, i.e., \textit{NeuronClique} and \textit{NeuronSplit}, to construct dummy neurons associated with non-vanishing weights. This inhibits the direct detection based on the vanishing weights.
2) \textit{Dummy Neuron Injection}: The adversary will generate and inject the dummy neurons from the back to front considering the stealthiness of these injected neurons.
3) \textit{Further Camouflage}: The final step is to further camouflage the dummy neurons in terms of the scale, the location, and the shape of the associated weights via other invariant transforms on DNNs, aiming at transforming the original model into an the obfuscated model which has almost no structural similarity with its original self, while provably preserves the normal model utility. 

Without loss of generality, we present our methodology below with a $H$-layer CNN $f$, where the weights of the $l$-th convolutional layer is denoted as $W^l \in \mathbb R^{N_{l-1} \times N_l \times h \times w}$. Following the notations in Section \ref{sec:motivation}, we denote the input and the output weights of the dummy neuron $m^l_k$ injected in the $l$-th hidden layer as $U^l_{k,in} = \{u^l_{i,k}\}_{i = 1}^{N_{l-1}}$ and $V^l_{k,out} =\{v^{l+1}_{k,j}\}_{j = 1}^{N_{l+1}}$, respectively. We do not consider the bias term because modern DNN models with batch normalization usually have no bias terms \cite{torchvision}. Appendix \ref{sec:app:coverage} provides the technical details on applying our attack framework to more complicated neural architectures (e.g., ResNet \cite{he2016resnet} and Inception \cite{szegedy2016inception}).


\subsection{Dummy Neuron Generation}
\label{sec:dn_generation}
How to construct dummy neurons with non-vanishing input or output weights is challenging. It is mainly because, when the adversary has no knowledge on the input data to the victim model, the contribution of a newly added neuron with non-vanishing weights to the next layer is highly unpredictable, likely to cause a noticeable loss to the original model performance. To eliminate the negative impact of such dummy neurons on the victim model, we alternatively construct groups of \textit{dummy neurons} which work together to preserve the model's prediction behavior. We devise the following structural obfuscation primitives, i.e., \textit{NeuronClique} and \textit{NeuronSplit}. Appendix \ref{sec:app:proof} rigorously proves why these two primitives construct valid groups of dummy neurons.

\noindent$\bullet$\textbf{ \textit{NeuronClique}.}
%%%% neurons which cancel each other's output out
Our first proposed primitive, \textit{NeuronClique}, generates a group of dummy neurons assigned with the identical incoming weights and arbitrary outgoing weights which satisfies that they cancel the others' output out. In the following, we elaborate on the case where the adversary attempts to generates $d(\geq 2)$ dummy neurons, i.e., $\{m^l_1,m^l_2,...m^l_d\}$, for the $l$-th layer of the target model $f$ with \textit{NeuronClique}. Formally, the input and output weights of these dummy neurons are designed to satisfy the following conditions:

\begin{equation}
\label{eq:NC_incoming}
U^l_{k,in} = U^l_{1,in} \in \mathbb {R}^{N_{l-1} \times w \times h}, \text{for}\ k = 1,2,...,d,
\end{equation}

\begin{equation}
\label{eq:NC_outgoing}
\sum _{k=1}^d V^l_{k,out} = \textbf{0} \in \mathbb {R}^{N_{l+1} \times w \times h},
\end{equation}
where $U^l_{1,in}$ and $\{V^l_{k,out}\}_{k = 1}^{d-1}$ are randomly sampled from the parameter distribution of the normal neurons to implement by-design stealthiness. For example, in Fig.\ref{fig:neuron_clique}, we generate three dummy neurons by \textit{NeuronClique} and inject them into the $l$-th layer of the prototypical target model. 

%%%%%%%%%%%%% BEGIN OF dummy neuron clique
\begin{figure}[t]
\begin{center}
\includegraphics[width=0.4\textwidth]{img/neuronclique.pdf}
\caption{A schematic diagram of \textit{NeuronClique} combined with the parameter rescaling invariance.}
\label{fig:neuron_clique}
\end{center}
\vspace{-0.2in}
\end{figure}
%%%%%%%%%%%%%% END OF naive dummy clique



\noindent$\bullet$\textbf{ \textit{NeuronSplit}.}
%%%% split one neuron into two
Our second primitive for dummy neuron generation further aims at enhancing the connection between the outputs of the dummy neurons and the original neurons. Specifically, the primitive \textit{NeuronSplit} converts the normal neuron into a number of substitute neurons which work together to preserve functionality of the replaced neuron  for the following layers. Without loss of generality, we split the first neuron in $l$-th layer, i.e., $n^l_1$, into $d+1$ substitute neurons, i.e., $\{m^l_1,m^l_2,m^l_3,...m^l_{d+1}\}$, by which we replace the original neuron with $m^l_1$ and view others as $d$ dummy neurons. Formally, we construct these substitute neurons by setting the input and output weights to satisfy the conditions:
\begin{equation}
\label{eq:Ns_incoming}
U^l_{k,in} = W^l_{1,in} \in \mathbb {R}^{N_{l-1} \times w \times h}, \text{for}\ k = 1,2,...,d+1,
\end{equation}

% \begin{equation}
% \label{eq:Ns_bias}
% o^l_k = b^l_1 \in \mathbb {R}, for\ k = 0,1,...,d,
% \end{equation}

\begin{equation}
\label{eq:Ns_outgoing}
\sum _{k=1}^{d+1} V^l_{k,out} = W^l_{1,out} \in \mathbb {R}^{N_{l+1} \times w \times h},
\end{equation}
where $W^l_{1,in},W^l_{1,out}$ are the associated incoming weights and outgoing weights of the original neuron $n^l_1$ before the substitution, $\{V^l_{k,out}\}_{k = 1}^{d}$ are randomly sampled from the similar distribution of the normal neurons' weights. For example, as is shown in Fig.\ref{fig:neuron_split}, we can split the first neuron in $l$-th layer into three neurons by \textit{NeuronSplit}. Then we replace the original neuron with one substitute neuron and two dummy neurons into the same layer. 

% Below, we prove \text{NeuronSplit} constructs a valid group of dummy neurons.

%%%%%%%%%%%%% BEGIN OF dummy neuron clique
\begin{figure}[t]
\begin{center}
\includegraphics[width=0.45\textwidth]{img/neuronsplit.pdf}
\caption{A schematic diagram of \textit{NeuronSplit} combined with the parameter rescaling invariance.}
\label{fig:neuron_split}
\end{center}
\vspace{-0.3in}
\end{figure}
%%%%%%%%%%%%%% END OF naive dummy clique


\subsection{Dummy Neuron Injection}
\label{sec:dn_injection}
%%%%% (1) layers by layers injection
%%%%% (2) input-output dimension 
%%%%% ....
% After generating a group of dummy neurons by NeuronClique or NeuronSplit, we can inject them into arbitrary positions of the pre-defined layer, which can separate the original neurons to further perturb the embedded identity information. It worth nothing that we have to replace the target neuron for NeuronSplit first before adding other dummy neurons.  

Without knowing the specific layers where the watermark is embedded, our attack framework randomly generates and injects groups of dummy neurons via the two primitives (i.e., \textit{NeuronClique} and \textit{NeuronSplit})  \textit{from the last hidden layer to the first layer} of the victim model. Specifically, As is shown in Fig.\ref{fig:atk_pipeline}, we first inject dummy neurons into $l$-th layer and then insert the dummy neurons generated by \textit{NeuronSplit} into $(l-1)$-th layer. It turns out that each dummy neuron in the first group now has different incoming weights with one another, as the these weights of each dummy neuron in $l$-th layer are expanded with the randomly sampled output parameters of the dummy neurons in $(l-1)$-th layer, which does not satisfy Eq.(\ref{eq:NC_incoming}) any longer. This substantially increases the detection budget for a skilled defender in Section \ref{sec:eval:stealthiness_dn}.

As a final remark, generating each dummy neuron group only involves sampling random vectors and several floating-point operations, which incurs almost no additional computational costs than training/finetuning the protected model as in previous attacks. It is worth to note that, the injection process has no change on the dimension of the model's input and output dimension, leaving the prediction API of the victim model in its original form. 

% In the above process, we find the order of obfuscation operation is especially tricky if we want to further enhance the attack stealthiness. In our attack framework, we propose to generate and inject dummy neurons \textit{from the last hidden layer to the first layer}, considering the following reciprocal effect between dummy neurons in two successive layers. In fact, if we solely consider the insertion of dummy neurons in one specific layer, then according to Eq.(\ref{eq:NC_incoming})\&(\ref{eq:Ns_incoming}), the incoming weights of these dummy neurons generated by either \textit{NeuronClique} or \textit{NeuronSplit} are exactly proportional to one another. As is shown in Fig.\ref{fig:atk_pipeline}, to further obfuscate the incoming weights' distribution of these dummy neurons, we first inject dummy neurons into $l$-th layer and then insert the dummy neurons generated by \textit{NeuronSplit} into $(l-1)$-th layer. It turns out that each dummy neuron in the first group now has different incoming weights with one another, as the these weights of each dummy neuron in $l$-th layer are expanded with the randomly sampled output parameters of the dummy neurons in $(l-1)$-th layer. As a result, the incoming weights of the first group of injected neurons do not satisfy Eq.(\ref{eq:NC_incoming}) any longer, which increases the complexity of detection for the $l$-th layer's dummy neurons. \mytodo{(Can Delete)}

% More evaluation results can be found in Section \ref{sec:eval:stealthiness_dn}.




\subsection{Advanced Camouflage Techniques}
\label{sec:dn_camouflage}
To wind up, our proposed attack further camouflages the injected dummy neurons among the original neurons by obfuscating the location, the scale, the shape, and the distribution of the weights associated with the dummy neurons, which are stealthier against possible elimination attempts compared to the preliminary dummy neurons.
% layershuffle这里存在一个问题，单纯的shuffle就可以破坏水印验证，是否显得dummy neurons比较多余；改成插入随机位置？

\noindent$\bullet$\textbf{ Exploiting Shuffling and Scaling Invariance.}
We offensively exploit the usage of the shuffling and scaling invariance in DNN \cite{neyshabur2015pathsgd, ganju2018property}. With the scaling invariance, the adversary can obfuscate the weight scale by scaling up the incoming weights of every injected dummy neuron with a positive value $\lambda$ and scaling down its outgoing weights at the same ratio, which can prevent the weight distribution of the dummy neurons from being abnormal as these weights in scaling equivalence do not satisfy the conditions of \textit{NeuronClique} or \textit{NeuronSplit} any longer. 
% 是否需要进一步说明如何控制lambda使得dn weight分布正常
To randomize the fake weight's location, our attack framework leverages the permutation invariance of neural networks to disperse the dummy neurons among the original ones. We use random permutation on the neurons of every expanded layer to randomize the location of the injected dummy neurons, rather than injecting the dummy neurons as an independent module, or otherwise the location information would be exploited for neuron inspection.

% 是否引一下sok那篇说的shuffle无关性，我们这里直接破坏结构
\noindent$\bullet$\textbf{ Kernel Expansion.}
Furthermore, the adversary can further modify the architecture of the protected model by expanding the kernel size of every convolutional layers, which obfuscates the weight shape of the dummy neurons. For intuition, we can obtain an equivalent model by padding the kernel matrix with all $0$, while increasing the amount of the implicit padding of the input activation maps. Combined with the injected dummy neurons, our proposed attack can pad the kernels within the same layer with non-vanishing values, which not only improves the stealthiness of injected neurons but also introduces more perturbation to the verification process. More technical details on kernel expansion are in Appendix \ref{sec:app:kernel_expansion}. 

% \noindent$\bullet$\textbf{ Generalized Dummy Neuron.}
% To further improve the stealthiness of our injected neurons, we find the key is to remove the exploitable trace of the dummy neurons. Such a trace in our previous approach is \textit{the proportionality between the incoming weights of the dummy neurons}. To construct stealthier dummy neurons, we first generated their input weight randomly, than choose the bias of each injected neurons to keep them being activated under any valid inputs. Finally, we can elaborate their outgoing weights to preserve the original model behavior by canceling the output of newly inserted dummy neurons. Appendix \ref{sec:app:stealthier_dn} provides the missing technical details.
