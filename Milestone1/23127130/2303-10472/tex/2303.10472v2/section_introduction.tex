
\section{Introduction}
Variational inference (VI; \citealt{jordan_introduction_1999,blei_variational_2017,zhang_advances_2019}) algorithms are fast and scalable Bayesian inference methods widely applied in fields of statistics and machine learning.
In particular, black-box VI (BBVI; \citealt{ranganath_black_2014,titsias_doubly_2014}) leverages 
stochastic gradient descent (SGD; \citealt{robbins_stochastic_1951,bottou_online_1999}) for inference of non-conjugate probabilistic models.
With the development of bijectors~\citep{kucukelbir_automatic_2017,dillon_tensorflow_2017,fjelde_bijectors_2020}, most of the methodological advances in BBVI have now been abstracted out through various probabilistic programming frameworks~\citep{carpenter_stan_2017,ge_turing_2018,dillon_tensorflow_2017,bingham_pyro_2019,salvatier_probabilistic_2016}.

Despite the advances of BBVI, little is known about its theoretical properties.
Even when restricted to the location-scale family (\cref{def:family}), it is unknown whether BBVI is guaranteed to converge without having to modify the algorithms used in practice, for example, by enforcing bounded domains, bounded support, bounded gradients, and such.
This theoretical insight is necessary since BBVI methods are known to be less robust~\citep{yao_yes_2018,dhaka_robust_2020,welandawe_robust_2022,dhaka_challenges_2021,domke_provable_2020} compared to other inference methods such as Markov chain Monte Carlo.
Although progress has been made to formalize the theory of BBVI with some generality, the gap between our understanding of BBVI and the convergence guarantees of SGD remains open.
For example,~\citet{domke_provable_2019,domke_provable_2020} provided smoothness and gradient variance guarantees. 
Still, these results do not yet yield a full convergence guarantee and do not extend to \textit{nonlinear} covariance parameterizations used in practice.

%Currently, most convergence proofs of SGD require that the gradient variance (or, more precisely, the expected squared norm) is bounded by certain quantities.

In this work, we investigate whether recent progress in relaxing the gradient variance assumptions used in SGD~\citep{tseng_incremental_1998,vaswani_fast_2019,schmidt_fast_2013,bottou_optimization_2018,gower_sgd_2019,gower_stochastic_2021,nguyen_sgd_2018} apply to BBVI. These extensions have led to new insights that the structure of the gradient bounds can have non-trivial interactions with gradient-adaptive SGD algorithms~\citep{zhang_adam_2022}.
For example, when the ``interpolation assumption'' (the gradient noise converges to 0;~\citealt{schmidt_fast_2013,ma_power_2018,vaswani_fast_2019}) does not hold, ADAM~\citep{kingma_adam_2015} provably diverges with certain stepsize combinations~\citep{zhang_adam_2022}.
Until BBVI can be shown to conform to the assumptions used by these recent works, it is unclear how these results relate to BBVI.

While the variance of BBVI gradient estimators has been studied before~\citep{xu_variance_2019,domke_provable_2019,mohamed_monte_2020,fujisawa_multilevel_2021}, the connection with the conditions used in SGD has yet to be established.
As such, we answer the following question:
%
\vspace{-2ex}%
\begin{quote}
  \textit{Does the gradient variance of BBVI conform to the conditions assumed in convergence guarantees of SGD without modifying the implementations used in practice?}
\end{quote}
\vspace{-2ex}%
%
The answer is yes!
Assuming the target log joint distribution is smooth and quadratically growing, we show that the gradient variance of BBVI satisfies the \textit{ABC} condition (\cref{assumption:abc}) used by~\citet{polyak_pseudogradient_1973,khaled_better_2023,gower_stochastic_2021}.
Our analysis extends the previous result of \citet{domke_provable_2019} to covariance parameterizations involving nonlinear functions for conditioning the diagonal (see \cref{section:covariance_parameterization}), as commonly done in practice.
Furthermore, we prove that the gradient variance of the mean-field parameterization \citep{peterson_mean_1987,peterson_explorations_1989,hinton_keeping_1993} results in better dimensional dependence compared to full-rank ones.
%Overall, our results should act as stepping stones towards full convergence guarantees of BBVI.

Our contributions are summarized as follows:
\begin{itemize}
  \vspace{-2ex}
  \setlength\itemsep{-1.5ex}
  \item[\ding{182}] We provide upper bounds on the gradient variance of BBVI that matches the \textit{ABC condition} (\cref{assumption:abc}) used for analyzing SGD.
    \begin{itemize}[leftmargin=1.5em,]
      \item[\ding{228}] \cref{thm:gradient_upper_bound,thm:gradient_upper_bound_kl} do not require any modification of the algorithms used in practice.
      \item[\ding{228}] \cref{thm:gradient_upper_bound_bounded_entropy} achieves better constants under the stronger \textit{bounded entropy} assumption.
    \end{itemize}
    % 
    \item[\ding{183}] Our analysis applies to BBVI parameterizations (\cref{section:covariance_parameterization}) widely used in practice (\cref{table:parameterization_survey}).
    \begin{itemize}[leftmargin=1.5em,]
        \item[\ding{228}] \cref{thm:general_variational_gradient_norm_identity} enables the bounds to cover nonlinear covariance parameterizations.
        \item[\ding{228}] \cref{thm:meanfield_u_identity,remark:meanfield_superiority} shows that the gradient variance of the mean-field parameterization has superior dimensional scaling.
    \end{itemize}
    %
    \item[\ding{184}] We provide a matching lower bound (\cref{thm:gradient_lower_bound}) on the gradient variance, showing that, under the stated assumptions, the ABC condition is the weakest assumption applicable to BBVI.
\end{itemize}
  \vspace{-2ex}

%%% Local Variables:
%%% TeX-master: "main"
%%% End:
