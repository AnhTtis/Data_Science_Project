
\vspace{-.5ex}
\section{Discussions}
\vspace{-.5ex}
In this work, we have proven upper bounds on the gradient variance of BBVI with the location-scale family for smooth, quadratically-growing log-likelihoods.
Specifically, we have provided bounds for both the ELBO in entropy-regularized and KL-regularized forms.
Our guarantees work without a single modification to the algorithms used in practice, although stronger assumptions establish a tighter bound for the entropy-regularized form ELBO.

\vspace{-1ex}
\paragraph{Limitations}
Our results have the following limitations:
\begin{enumerate*}
    \item[\ding{182}] Our results only apply to smooth and quadratically-growing log likelihoods and
    \item[\ding{183}] the location-scale ADVI family. Also, 
    \item[\ding{184}] our bounds cannot distinguish the variance of the Cholesky and matrix square root parameterizations, 
    \item[\ding{185}] and empirically, the bounds for the mean-field parameterization appear loose. Furthermore, 
    \item[\ding{186}] our results only work with 1-Lipschitz diagonal conditioners such as the softplus function.
\end{enumerate*}
Unfortunately, assuming both smoothness and quadratic growth is quite restrictive, as it leaves a very small number of known distributions.
Also, in practice, non-Lipschitz conditioners such as the exponential functions are widely used.
While obtaining similar bounds with such conditioners would be challenging, constructing a theoretical framework that extends to such would be an important future research direction.
 
%% \paragraph{Signal-to-Noise Ratio}
%% To empirically quantify variance in VI, the gradient signal-to-noise ratio metric
%% \begin{align*}
%%   \mathrm{SNR}\left(\rvvg\right) =
%%   \frac{ \norm{\mathbb{E}\rvvg}^2_2 }{ \mathbb{E}\norm{\rvvg}^2_2 }
%%   =
%%   \frac{ \norm{ \nabla F }^2_2 }{ \mathrm{tr} \mathbb{V}\rvvg + \norm{ \nabla F }^2_2  } \leq 1,
%% \end{align*}
%% where the last equality holds since \(\rvvg\) is unbiased, has recently seen use~\citep{pmlr-v139-geffner21a, rainforth_tighter_2018, fujisawa_multilevel_2021}.
%% This metric can be though as quantifying the relative magnitude of the gradient noise with respect to the true gradient.
%% Under the ABC condition, as discussed in~\cref{section:abc}, each of the \(A\) and \(C\) term contribute to the convergence speed (\(A\)) and radius fo the stationary region (\(C\)), which will be unique to different estimators.
%% However, when using the SNR ratio for comparing different estimatosr, it is not possible to understand the effect of each term.
%% Thus, if one would go with the the ABC condition, analyzing the contribution of each term separately would be more informative.

%Under the ESG condition, the SNR is provides an estimate of the lower bound of \(B\).
%However, the ESG condition only holds for the interpolation regime where \(\mathbb{E}\norm{\rvvg}_2^2\).
%Thus, under the ABC condition, it is unsure how the SNR relates to the convergence aspects of SGD.

%% \begin{proposition}
%%   Let \(F\) be \(L\)-smooth and convex.
%%   Also, let \(\mathrm{SNR}\left(\rvvg\left(\vlambda\right)\right) > 0\) be the gradient SNR at \(\vlambda\).
%%   Then, 
%%   \[
%%      \mathbb{E}\norm{\rvvg\left(\vlambda\right)}_2^2
%%      \leq 
%%      L \left(\frac{1}{\mathrm{SNR}\left(\rvvg\left(\vlambda\right)\right)} - 1\right)
%%      \left(F\left(\vlambda\right) - F^*\right)
%%      +
%%      \norm{\mathbb{E} \rvvg }_2^2,
%%   \]
%%   where \(F^* = F\left(\vlambda^*\right)\) for the global minimum \(\vlambda^*\).
%% \end{proposition}
%% \begin{proof}
%%   From smoothness and convexity, it follows that
%%   \begin{align}
%%      \norm{ \nabla F\left(\vlambda\right) }_2^2 
%%        \leq
%%        L \inner{\nabla F\left(\vlambda\right)}{\vlambda - \bar{\vlambda}}
%%        \leq
%%       L \left( F\left(\vlambda\right) - F^* \right),\label{eq:snr_bound_eq1}
%%   \end{align}
%%   where the first inequality uses the co-coercivity of Lipschitz gradients, the last bound is the definition of convexity.
%%   From the definition of the SNR,
%%   \begin{align*}
%%     \mathbb{E}\norm{\rvvg\left(\vlambda\right)}_2^2 
%%     &=
%%     \frac{1}{\mathrm{SNR}\left(\rvvg\left(\vlambda\right)\right)} 
%%     \norm{ \nabla F\left(\vlambda\right)  }_2^2
%%     \\
%%     &=
%%     \left(
%%     \frac{1}{\mathrm{SNR}\left(\rvvg\left(\vlambda\right)\right)} 
%%     - 1
%%     \right)
%%     \norm{ \nabla F\left(\vlambda\right)  }_2^2
%%     +
%%     \norm{ \nabla F\left(\vlambda\right)  }_2^2.
%%   \end{align*}
%%   Plugging \cref{eq:snr_bound_eq1} to the first \( \norm{ \nabla F\left(\vlambda\right)  }_2^2 \) yields the result.
%% \end{proof}

%% \begin{remark}
%%   Let an unbiased estimator of \(\nabla F\), \(\rvvg\), satisfy \cref{assumption:abc} with \(B=1\).
%%   Then, the gradient SNR is bounded as
%%   \[
%%     \frac{ \norm{\nabla F\left(\vlambda\right)}_2^2 }{
%%       2 A \left( F\left(\vlambda\right) - F^* \right)
%%       +
%%       \norm{ \nabla F\left(\vlambda\right) }_2^2
%%       +
%%       C
%%     }
%%     \leq 
%%     \mathrm{SNR}\left(\rvvg\left(\vlambda\right)\right)
%%   \]
%%   where \(F^* = \inf_{\vlambda \in \mathbb{R}^p}\).
%% \end{remark}

%% \(A\) and \(C\) are determined by the unique properties of the estimator.
%% Thus, comparing the SNR of different estimators is an indirect way to compare the constants.

%%% Local Variables:
%%% TeX-master: "main"
%%% End:
