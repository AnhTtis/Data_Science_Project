

\prAtEndRestateii*

\makeatletter\Hy@SaveLastskip\label{proofsection:prAtEndii}\ifdefined\pratend@current@sectionlike@label\immediate\write\@auxout{\string\gdef\string\pratend@section@for@proofii{\pratend@current@sectionlike@label}}\fi\Hy@RestoreLastskip\makeatother\begin{proof}[Proof]\phantomsection\label{proof:prAtEndii}\begin {alignat}{2} &\norm {\nabla _{\vlambda } f\left ( \vt _{\vlambda }\left (\vu \right ) \right ) }_2^2 \nonumber \\ &\;= {\left ( \frac { \partial \vt _{\vlambda }\left (\vu \right ) }{ \partial \vlambda } \nabla f\left ( \vt _{\vlambda }\left (\vu \right ) \right ) \right )}^{\top } \frac { \partial \vt _{\vlambda }\left (\vu \right ) }{ \partial \vlambda } \nabla f\left ( \vt _{\vlambda }\left (\vu \right ) \right ) \nonumber \\ &\;= {\nabla f^{\top }\left ( \vt _{\vlambda }\left (\vu \right ) \right )} {\left ( \frac { \partial \vt _{\vlambda }\left (\vu \right ) }{ \partial \vlambda } \right )}^{\top } \frac { \partial \vt _{\vlambda }\left (\vu \right ) }{ \partial \vlambda } \nabla f\left ( \vt _{\vlambda }\left (\vu \right ) \right ) \nonumber \\ &\;= {\vg _{f}^{\top }} {\left ( \frac { \partial \vt _{\vlambda }\left (\vu \right ) }{ \partial \vlambda } \right )}^{\top } \frac { \partial \vt _{\vlambda }\left (\vu \right ) }{ \partial \vlambda } \vg _{f}.\label {eq:variational_gradient_norm_identity_eq1} \end {alignat} \par \paragraph {Proof for Cholesky} \par Let \(p\) denote the number of scalar variational parameters such that \(\vlambda = (\lambda _1, \ldots , \lambda _p)\). Then, \begin {alignat*}{2} &{\left ( \frac { \partial \vt _{\vlambda }\left (\vu \right ) }{ \partial \vlambda } \right )}^{\top } \frac { \partial \vt _{\vlambda }\left (\vu \right ) }{ \partial \vlambda } \\ &\;= \sum ^{d}_{i=1} \frac { \partial \vt _{\vlambda }\left (\vu \right ) }{ \partial m_i } {\left ( \frac { \partial \vt _{\vlambda }\left (\vu \right ) }{ \partial m_i } \right )}^{\top } + \sum ^{d}_{i=1} \sum ^{d}_{j \leq i} \frac { \partial \vt _{\vlambda }\left (\vu \right ) }{ \partial \lambda _{C_{ij}} } {\left ( \frac { \partial \vt _{\vlambda }\left (\vu \right ) }{ \partial \lambda _{C_{ij}} } \right )}^{\top }, \end {alignat*} where \(\lambda _{C_{ij}}\) denote the parameter responsible for the \(ij\)-th entry of \(\mC \), \(C_{ij}\). Notice that, unlike for the matrix square root parameterization~\citep {domke_provable_2019}, the sum for \(C_{ij}\) is only over the lower triangular section. \par For the derivatives with respect to \(m_i\) and \(C_{ij}\), \citet {domke_provable_2020, domke_provable_2019} show that \begin {alignat}{2} \frac {\partial \vt _{\vlambda }\left (\vu \right ) }{ \partial m_i } &= \ve _i \quad \frac {\partial \vt _{\vlambda }\left (\vu \right ) }{ \partial C_{ij} } &= \ve _i u_j,\label {eq:covariance_derivative} \end {alignat} where \(\ve _i\) is the unit basis of the \(i\)th component. \par Therefore, \begin {alignat}{2} &{\left ( \frac { \partial \vt _{\vlambda }\left (\vu \right ) }{ \partial \vlambda } \right )}^{\top } \frac { \partial \vt _{\vlambda }\left (\vu \right ) }{ \partial \vlambda } \nonumber \\ &\;= \sum ^{d}_{i=1} \ve _i \ve _i^{\top } + \sum ^{d}_{i=1} \sum _{j \leq i} \frac { \partial \vt _{\vlambda }\left (\vu \right ) }{ \partial \lambda _{C_{ij}} } {\left ( \frac { \partial \vt _{\vlambda }\left (\vu \right ) }{ \partial \lambda _{C_{ij}} } \right )}^{\top } \nonumber \\ &\;= \mI + \underbrace { \sum ^{d}_{i=1} \frac { \partial \vt _{\vlambda }\left (\vu \right ) }{ \partial \lambda _{C_{ii}} } {\left ( \frac { \partial \vt _{\vlambda }\left (\vu \right ) }{ \partial \lambda _{C_{ii}} } \right )}^{\top } }_{\text {diagonal of \(\mC \)}} \nonumber \\ &\qquad + \underbrace { \sum ^{d}_{i=1} \sum _{j < i} \frac { \partial \vt _{\vlambda }\left (\vu \right ) }{ \partial \lambda _{C_{ij}} } {\left ( \frac { \partial \vt _{\vlambda }\left (\vu \right ) }{ \partial \lambda _{C_{ij}} } \right )}^{\top }}_{\text {off-diagonal of \(\mC \)}}, \label {eq:thm:variational_gradient_norm_identity_eq2} \end {alignat} leaving us with the derivatives of the scale term. \par The gradient with respect to \(\lambda _{C_{ij}}\), however, depends on the parameterization. That is, \begin {alignat}{2} \frac { \partial \vt _{\vlambda }\left (\vu \right ) }{ \partial \lambda _{C_{ij}} } &= \frac { \partial \vt _{\vlambda }\left (\vu \right ) }{ \partial C_{ij} } \frac { \partial C_{ij} }{ \partial \lambda _{C_{ij}} } &= \ve _i u_j \frac { \partial C_{ij} }{ \partial \lambda _{C_{ij}} }. \label {eq:thm:variational_gradient_norm_identity_covderivative} \end {alignat} \par For the diagonal elements, \(\lambda _{C_{ii}} = d_i\). Thus, \begin {align} \frac { \partial C_{ii} }{ \partial s_i } = \frac { \partial \phi \left (s_i\right ) }{ \partial s_i } = \phi ^{\prime }\left (s_i\right ). \label {eq:thm:variational_gradient_norm_identity_diag} \end {align} And for the off-diagonal elements, \(\lambda _{L_{ij}} = L_{ij}\), and \begin {align} \frac { \partial C_{ij} }{ \partial L_{ij} } = 1. \label {eq:thm:variational_gradient_norm_identity_offdiag} \end {align} \par Plugging \cref {eq:thm:variational_gradient_norm_identity_diag,eq:thm:variational_gradient_norm_identity_offdiag,eq:thm:variational_gradient_norm_identity_covderivative} into \cref {eq:thm:variational_gradient_norm_identity_eq2}, \begin {alignat}{2} &{\left ( \frac { \partial \vt _{\vlambda }\left (\vu \right ) }{ \partial \vlambda } \right )}^{\top } \frac { \partial \vt _{\vlambda }\left (\vu \right ) }{ \partial \vlambda } \nonumber \\ &\;= \mI + \underbrace { \sum ^{d}_{i=1} {\left ( u_j \phi ^{\prime }\left (s_i\right ) \right )}^2 \ve _i \ve _i^{\top } }_{\text {diagonal of \(\mC \)}} + \underbrace { \sum ^{d}_{i=1} \sum _{j=1, j < i} u_j^2 \, \ve _i \ve _i^{\top } }_{\text {off-diagonal of \(\mC \)}} \nonumber \\ &\;= \mI + \underbrace { \sum ^{d}_{i=1} u_i^2 {\left (\phi ^{\prime }\left (s_i\right ) \right )}^2 \ve _i \ve _i^{\top } }_{\text {diagonal of \(\mC \)}} + \underbrace { \sum ^{d}_{i=1} \sum _{j \leq i} u_j^2 \, \ve _i \ve _i^{\top } - \sum ^{d}_{i=1} u_i^2 \, \ve _i \ve _i^{\top } }_{\text {off-diagonal of \(\mC \)}} \nonumber \\ &\;= \mI + \underbrace { \mU \, \mPhi }_{\text {diagonal of \(\mC \)}} + \underbrace { \mY - \mU }_{\text {off-diagonal of \(\mC \)}} \nonumber \\ &\;= \left ( \mI + \mY \right ) + \mU \left ( \mPhi - \mI \right ), \label {eq:variational_gradient_norm_identity_jacinner} \end {alignat} where \(\mU ,\dot {\Phi }\) are diagonal matrices defined as \begin {alignat*}{2} & \mPhi &&= \mathrm {diag}\left ( \left [ {\phi ^{\prime }\left (s_1\right )}^2, \ldots , {\phi ^{\prime }\left (s_d\right )}^2 \right ] \right ) \\ &\mU &&= \mathrm {diag}\left ( \left [ u_1^2, \ldots , u_d^2 \right ] \right ) \\ &\mY &&= \mathrm {diag}\left ( \left [ u_1^2, u_1^2 + u_2^2,\, \ldots \,, {\textstyle \sum ^{d}_{i=1} u_i^2} \right ] \right ). \end {alignat*} The major difference with the proof of \citet [Lemma 8]{domke_provable_2019} for the matrix square root case is that we sum of the \(u_j^2 \ve _i \ve _i^{\top }\) terms is over the \textit {lower diagonal elements} instead of all the elements. This is the variance reduction effect we get for using the Cholesky parameterization. \par Coming back to \cref {eq:variational_gradient_norm_identity_eq1}, \begin {alignat}{2} &\norm {\nabla _{\vlambda } f\left ( \vt _{\vlambda }\left (\vu \right ) \right )}_2^2 \nonumber \\ \;&= \vg _f^{\top } {\left ( \frac { \partial \vt _{\vlambda }\left (\vu \right ) }{ \partial \vlambda } \right )}^{\top } \frac { \partial \vt _{\vlambda }\left (\vu \right ) }{ \partial \vlambda } \vg \nonumber \\ &= \vg _{f}^{\top } \Big ( \left ( \mI + \mY \right ) + \mU \left ( \mPhi - \mI \right ) \Big ) \vg _{f} \nonumber \\ &= {\lVert \vg _{f} \rVert }_2^2 + \vg _{f}^{\top } \mY \vg _{f} + \vg _{f}^{\top } \mU \left ( \mPhi - \mI \right ) \vg _{f} \nonumber \\ &= {\lVert \vg _{f} \rVert }_2^2 + {\lVert \vg _{f} \rVert }_{\mY }^2 + \mathrm {tr}\left ( \vg _{f} \vg _{f}^{\top } \mU \left ( \mPhi - \mI \right ) \right ). \label {eq:variational_gradient_norm_identity_conclusion} \end {alignat} \par \paragraph {Proof for Mean-field} For the mean-field variational family, the covariance has only diagonal elements. Therefore, \cref {eq:variational_gradient_norm_identity_jacinner} becomes \begin {alignat}{2} {\left ( \frac { \partial \vt _{\vlambda }\left (\vu \right ) }{ \partial \vlambda } \right )}^{\top } \frac { \partial \vt _{\vlambda }\left (\vu \right ) }{ \partial \vlambda } &= \mI + \mU \mPhi , \nonumber \end {alignat} and \cref {eq:variational_gradient_norm_identity_conclusion} becomes \begin {alignat}{2} \norm {\nabla _{\vlambda } f\left ( \vt _{\vlambda }\left (\vu \right ) \right )} = \vg _{f}^{\top } \left ( \mI + \mU \mPhi \right ) \vg _{f} = {\lVert \vg _{f} \rVert }_2^2 + \mathrm {tr}\left ( \vg _{f} \vg _{f}^{\top } \, \mU \mPhi \right ). \nonumber \end {alignat}\end{proof}

\prAtEndRestateiii*

\makeatletter\Hy@SaveLastskip\label{proofsection:prAtEndiii}\ifdefined\pratend@current@sectionlike@label\immediate\write\@auxout{\string\gdef\string\pratend@section@for@proofiii{\pratend@current@sectionlike@label}}\fi\Hy@RestoreLastskip\makeatother\begin{proof}[Proof]\phantomsection\label{proof:prAtEndiii}The proof continues from the result of \cref {thm:general_variational_gradient_norm_identity}. \par \paragraph {Proof for Cholesky} \cref {thm:general_variational_gradient_norm_identity} shows that \begin {alignat*}{2} \norm { \nabla _{\vlambda } f\left ( \vt _{\vlambda }\left (\vu \right ) \right ) }_2^2 = {\lVert \vg _{f} \rVert }_2^2 + {\lVert \vg _{f} \rVert }_{\mY }^2 + \mathrm {tr}\left ( \vg _{f} \vg _{f}^{\top } \mU \left ( \mPhi - \mI \right ) \right ). \end {alignat*} \par By the 1-Lipschitz assumption, the entries of the diagonal matrix \(\Phi \) satisfy \begin {align*} \Phi _{ii} = {\phi ^{\prime }\left (d_i\right )}^2 \leq 1, \end {align*} which means \(\mPhi \preceq \mI \) and thus \(\left ( \mPhi - \mI \right ) \preceq 0\). Also, \(\mU \), \(\vg _{f} \vg _{f}^{\top }\), and \(\left ( \mPhi - \mI \right )\) are all Hermitian. Based on this fact, applying \cref {thm:generalized_ruhe_inequality} reveals that \begin {alignat}{2} \mathrm {tr}\left ( \vg _{f} \vg _{f}^{\top }\, \mU \left ( \mPhi - \mI \right ) \right ) &\leq \sum _{i=1}^d \sigma _{i}\left (\vg _{f} \vg _{f}^{\top } \mU \right ) \sigma _i\left (\mPhi - \mI \right ) \nonumber \\ &\leq 0. \nonumber \end {alignat} Therefore, for the full rank Cholesky parameterization and a 1-Lipschitz positive map \(\phi \), \begin {alignat*}{2} \norm {\nabla _{\vlambda } f\left ( \vt _{\vlambda }\left (\vu \right ) \right )} &= {\lVert \nabla f\left ( \vt _{\vlambda }\left (\vu \right ) \right ) \rVert }_2^2 + {\lVert \nabla f\left ( \vt _{\vlambda }\left (\vu \right ) \right ) \rVert }_{\mY }^2 \\ &\leq {\lVert \nabla f\left ( \vt _{\vlambda }\left (\vu \right ) \right ) \rVert }_2^2 + \norm { \mY }_{2,2} {\lVert \nabla f\left ( \vt _{\vlambda }\left (\vu \right ) \right ) \rVert }_2^2 \\ &= {\lVert \nabla f\left ( \vt _{\vlambda }\left (\vu \right ) \right ) \rVert }_2^2 + \left ( \sum ^{d}_{i=1} u_{i}^2 \right ) {\lVert \nabla f\left ( \vt _{\vlambda }\left (\vu \right ) \right ) \rVert }_2^2 \\ &= \left (1 + \norm {\vu }^2_2\right ) {\lVert \nabla f\left ( \vt _{\vlambda }\left (\vu \right ) \right ) \rVert }_2^2, \end {alignat*} where \(\norm {\mU }_{2,2}\) is the \(L_2\) operator norm of \(\mU \). This upper bound coincides with that of the matrix square root parameteration. Thus, unforunately, this bound will be loose for the Cholesky parameterization. \par \paragraph {Proof for Mean-field (\cref {def:meanfield})} For the mean-field parameterization,~\cref {thm:general_variational_gradient_norm_identity} shows that \begin {alignat*}{2} \norm { \nabla _{\vlambda } f\left ( \vt _{\vlambda }\left (\vu \right ) \right ) }_2^2 = {\lVert \vg _{f} \rVert }_2^2 + \mathrm {tr}\left ( \vg _{f} \vg _{f}^{\top } \mU \mPhi \right ), \end {alignat*} \par For the second term, \begin {alignat*}{2} \mathrm {tr}\left ( \vg _{f} \vg _{f}^{\top } \mU \mPhi \right ) &= \vg _{f}^{\top } \left ( \mU \mPhi \right ) \vg _{f} \\ &\leq {\lVert \mU \rVert }_{2,2} {\lVert \mPhi \rVert }_{2,2} {\lVert \vg _{f} \rVert }^2_2. \end {alignat*} By the \(1\)-Lipschitzness of \(\phi \), \[ {\lVert \mPhi \rVert }_{2,2} = \sigma _{\mathrm {max}}\left ( \mPhi \right ) = \max _{i = 1, \ldots , d} {\phi ^{\prime }\left ( s_i \right )}^2 \leq 1. \] Then, \begin {alignat}{2} \vg _{f}^{\top } \left ( \mU \mPhi \right ) \vg _{f} &\leq {\lVert \mU \rVert }_{2,2} \, {\lVert \vg _{f} \rVert }^2_2 \label {eq:variational_gradient_norm_identity_mf_eq1} \\ &\leq {\lVert \mU \rVert }_{\mathrm {F}} \, {\lVert \vg _{f} \rVert }^2_2, \label {eq:variational_gradient_norm_identity_mf_eq2} \end {alignat} which gives the result. Here, unlike the bounds on \(\mPhi \), the bounds in \cref {eq:variational_gradient_norm_identity_mf_eq1,eq:variational_gradient_norm_identity_mf_eq2} are quite loose, and become looser as the dimensionality increases. \par \end{proof}

\prAtEndRestateiv*

\makeatletter\Hy@SaveLastskip\label{proofsection:prAtEndiv}\ifdefined\pratend@current@sectionlike@label\immediate\write\@auxout{\string\gdef\string\pratend@section@for@proofiv{\pratend@current@sectionlike@label}}\fi\Hy@RestoreLastskip\makeatother\begin{proof}[Proof]\phantomsection\label{proof:prAtEndiv}The key idea is to prove a similar result as \cref {thm:reparam_u_identity}. For terms that zero-out during during the proof of \cref {thm:reparam_u_identity}, we apply \cref {thm:meanfield_superiority} to zero-out those same terms. \par \begin {alignat}{2} &\mathbb {E}\norm {\vt _{\vlambda }\left (\rvvu \right ) - \vz }_2^2 \, \left ( 1 + \norm {\mU }_{\mathrm {F}} \right ) \nonumber \\ &\;= \mathbb {E} \rvvu ^{\top } \mC ^{\top } \mC \rvvu \, \left ( 1 + \norm {\mU }_{\mathrm {F}} \right ) \nonumber \\ &\;\quad + 2\,\mathbb {E} \rvvu ^{\top } \mC ^{\top } \left ( \vm - \vz \right ) \left ( 1 + \norm {\mU }_{\mathrm {F}} \right ) \nonumber \\ &\;\quad + {\left ( \vm - \vz \right )}^{\top } \left ( \vm - \vz \right ) \left ( 1 + \mathbb {E} \norm {\mU }_{\mathrm {F}} \right ) \nonumber \shortintertext {invoking the trace trick,} &\;= \norm {\vm - \vz } \left ( 1 + \mathbb {E} \norm {\mU }_{\mathrm {F}}\right ). \nonumber \\ &\;\quad + 2\,\mathbb {E} \left ( 1 + \norm {\mU }_{\mathrm {F}} \right ) \, \rvvu ^{\top } \mC ^{\top } \left ( \vm - \vz \right ) \nonumber \\ &\;\quad + \mathrm {tr}\left ( \mC ^{\top } \mC \, \mathbb {E} \rvvu \rvvu ^{\top } \left ( 1 + \norm {\mU }_{\mathrm {F}}\right ) \right ) \nonumber \end {alignat} \par \begin {alignat*}{2} \mathbb {E} \norm {\mathbfsf {U}}_{\mathrm {F}} &\;= \mathbb {E} \sqrt { \mathrm {tr}\left ( \mathbfsf {U} \mathbfsf {U} \right ) } \\ &\;= \mathbb {E} \sqrt { \sum _{i=1}^d \rvu _i^4 }, \shortintertext {using Jensen's inequality,} &\;\leq \sqrt { \mathbb {E} \sum _{i=1}^d \rvu _i^4 } \\ &\;= \sqrt { d \kappa }. \end {alignat*} \par \begin {alignat*}{2} \mathbb {E} \left ( \norm {\mathbfsf {U}}_{\mathrm {F}} + 1\right ) \rvu _i &\;= \mathbb {E} \left ( \sqrt { \sum ^d_{i=1} \rvu _i^4 } + 1\right ) \rvu _i \\ &\;\leq \mathbb {E} \left ( \sum ^d_{i=1} \rvu _i^2 + 1\right ) \rvu _i \\ &\;= \mathbb {E} \, \rvu _i \, \sum _{j \neq i} \rvu _j^2 + \rvu _i^3 + \rvu _i \shortintertext {applying \cref {assumption:symmetric_standard},} &\;= 0. \end {alignat*} \par \begin {alignat*}{2} \mathbb {E} \rvu _i \rvu _j \norm {\mathbfsf {U}}_{\mathrm {F}} &\;= \mathbb {E} \rvu _i \rvu _j \left ( \sqrt { \sum _{k=1}^d \rvu _k^4 } \right ) \\ &\;\leq \mathbb {E} \rvu _i \rvu _j \sum _{k=1}^d \rvu _k^2 \\ &\;= \mathbb {E} \rvu _i^3 + \rvu _j^3 + \rvu _i \rvu _j \sum _{k \neq i, j} \rvu _k^2 \\ &\;= 0. \end {alignat*} \par \begin {alignat*}{2} \mathbb {E} \rvu _i \rvu _i \norm {\mathbfsf {U}}_{\mathrm {F}} &\;= \mathbb {E} \rvu _i^2 \left ( \sqrt { \sum _{k=1}^d \rvu _k^4 } \right ), \shortintertext {applying Cauchy-Schwarz's inequality for expectations,} &\;\leq \sqrt { \left ( \mathbb {E} \sum _{k=1}^d \rvu _k^4 \right ) \left ( \mathbb {E} \rvu _i^4 \right ) }, \shortintertext {and given \cref {assumption:symmetric_standard},} &\;= \sqrt { \left ( d \kappa \right ) \left ( \kappa \right ) } \\ &\;= \kappa \sqrt { d }. \end {alignat*} \par Combining all the results, \begin {alignat*}{2} &\mathbb {E}\norm {\vt _{\vlambda }\left (\rvvu \right ) - \vz }_2^2 \, \left ( 1 + \norm {\mU }_{\mathrm {F}} \right ) \\ &\;\leq \norm { \vm - \vz }_2^2 \left ( \vm - \vz \right ) \left ( 1 + \mathbb {E} \norm {\mU }_{\mathrm {F}}\right ) \\ &\;\quad + 2\,\mathbb {E} \left ( 1 + \norm {\mU }_{\mathrm {F}} \right ) \, \rvvu ^{\top } \mC ^{\top } \left ( \vm - \vz \right ) \\ &\;\quad + \mathrm {tr}\left ( \mC ^{\top } \mC \, \mathbb {E} \rvvu \rvvu ^{\top } \left ( 1 + \norm {\mU }_{\mathrm {F}}\right ) \right ) \\ &\;\leq \sqrt {d \kappa } \norm { \vm - \vz }_2^2 + \mathrm {tr}\left ( \mC ^{\top } \mC \left (\mI + \kappa \sqrt {d} \mI \right ) \right ) \\ &\;= \sqrt {d \kappa } \norm { \vm - \vz }_2^2 + \left (\kappa \sqrt {d} + 1\right ) \mathrm {tr}\left ( \mC ^{\top } \mC \right ) \\ &\;= \sqrt {d \kappa } \norm { \vm - \vz }_2^2 + \left (\kappa \sqrt {d} + 1\right ) \norm { \mC }_{\mathrm {F}}^2. \end {alignat*} \par \end{proof}

\prAtEndRestateix*

\makeatletter\Hy@SaveLastskip\label{proofsection:prAtEndix}\ifdefined\pratend@current@sectionlike@label\immediate\write\@auxout{\string\gdef\string\pratend@section@for@proofix{\pratend@current@sectionlike@label}}\fi\Hy@RestoreLastskip\makeatother\begin{proof}[Proof]\phantomsection\label{proof:prAtEndix}From the definition of variance, \begin {alignat}{2} &\mathbb {E} \norm {\vg _M }^2_2 \nonumber \\ &\;= \mathrm {tr}\,\V { \vg _M } + \norm {\mathbb {E} \vg }^2_2, \nonumber \shortintertext {following the definition in \cref {eq:def_gradient_M_est},} &\;= \mathrm {tr}\,\V { \frac {1}{M} \sum _{m=1}^M \vg _m } + \norm { \nabla F\left (\vlambda \right ) }^2_2, \nonumber \shortintertext {and then the definition in \cref {eq:def_gradient_m_est},} &\;= \mathrm {tr}\,\V { \frac {1}{M} \sum _{m=1}^M \nabla _{\vlambda } f\left (\vt _{\vlambda }\left (\rvvu _m\right )\right ) + \nabla h\left (\vlambda \right ) } + \norm { \nabla F\left (\vlambda \right ) }^2_2, \nonumber \shortintertext {by the linearity of variance,} &\;= \frac {1}{M} \mathrm {tr}\,\V { \nabla _{\vlambda } f\left (\vt _{\vlambda }\left (\rvvu \right )\right ) } + \norm { \nabla F\left (\vlambda \right ) }^2_2 \\ &\;= \frac {1}{M} \left ( \mathbb {E}{ \norm {\nabla _{\vlambda } f\left (\vt _{\vlambda }\left (\rvvu \right )\right )}_2^2 } - \norm { \mathbb {E}{ \nabla _{\vlambda } f\left (\vt _{\vlambda }\left (\rvvu \right )\right )} }_2^2 \right ) + \norm { \nabla F\left (\vlambda \right ) }^2_2 \nonumber \\ &\;\leq \frac {1}{M} \mathbb {E}{ \norm { \nabla _{\vlambda } f\left (\vt _{\vlambda }\left (\rvvu \right )\right ) }_2^2 } + \norm { \nabla F\left (\vlambda \right ) }^2_2. \nonumber \end {alignat} The results follow from applying \cref {thm:general_variational_gradient_norm_bound} accordingly. \par \todo [inline]{ I think it might be possible to do something about the \[ -\norm {\mathbb {E}{ \nabla _{\vlambda } f\left (\vt _{\vlambda }\left (\rvvu \right )\right )}}_2^2 \] term instead of just upper bounding it by 0. In fact, it is possible to generalize \cref {thm:general_variational_gradient_norm_identity} such that \begin {align*} &\inner { \nabla _{\vlambda } f\left ( \vt _{\vlambda }\left (\vu \right ) \right ) }{ \nabla _{\vlambda } f\left ( \vt _{\vlambda }\left (\vv \right ) \right ) } \\ &\;= \left (1 + \inner {\vu }{\vv }\right ) \inner {\vg _{f,\vu }}{\vg _{f,\vv }} + \mathrm {tr}\left ( \vg _{f,\vu } \vg _{f,\vv }^{\top }\, \vv \vu ^{\top } \left ( \Phi ^2 - \mI \right ) \right ), \end {align*} where \(\vg _{f,\vu } = \nabla f\left (\vt _{\vlambda }\left (\vu \right )\right )\). }\end{proof}
