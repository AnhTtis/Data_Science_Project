

\prAtEndRestatexviii*

\makeatletter\Hy@SaveLastskip\label{proofsection:prAtEndxviii}\ifdefined\pratend@current@sectionlike@label\immediate\write\@auxout{\string\gdef\string\pratend@section@for@proofxviii{\pratend@current@sectionlike@label}}\fi\Hy@RestoreLastskip\makeatother\begin{proof}[Proof]\phantomsection\label{proof:prAtEndxviii}\par The STL estimator is obtained by setting \begin {alignat*}{2} f\left ( \vz \right ) = -\log p\left (\vz , \vx \right ) + \log q_{\vgamma }\left ( \vz \right ) \lvert _{\vgamma = \vlambda }. \end {alignat*} We first consider the log joint likelihood of the model to be \begin {alignat*}{2} \log p\left (\vz , \vx \right ) = -\frac {1}{2} {\left (\vz - \vmu \right )}^{-1} \mSigma ^{-1} \left (\vz - \vmu \right ), \end {alignat*} which is the unnormalized log probability density of a multivariate normal distribution with mean \(\vmu \) and covariance \(\mSigma \). We also set the variational family to be multivariate normal with mean (location) \(\vm \) and covariance (scale) \(\mC \mC ^{\top }\). Given these, the objective function is \begin {alignat*}{2} F\left (\vlambda \right ) &= \mathbb {E} f\left (\vt _{\vlambda }\left (\rvvu \right )\right ) \\ &= \mathbb {E}\log p\left (\vt _{\vlambda }\left (\rvvu \right )\right ) - \mathbb {E}\log q_{\vgamma }\left (\vt _{\vlambda }\left (\rvvu \right )\right ) \big \lvert _{\vgamma = \vlambda } \end {alignat*} \par From~\cref {thm:general_quad_reparam}, the likelihood term is \begin {alignat*}{2} &\mathbb {E}\log p\left (\vt _{\vlambda }\left (\rvvu \right )\right ) \\ &\;= -\frac {1}{2} {\left (\vm - \vmu \right )}^{\top } \mSigma ^{-1} \left (\vm - \vmu \right ) - \frac {1}{2} \mathrm {tr}\left (\mSigma ^{-1} \mC \mC ^{\top } \right ), \end {alignat*} while from~\cref {thm:q_quad_reparam}, the entropy term is \begin {alignat*}{2} &\mathbb {E}\log q_{\vlambda }\left (\vt _{\vlambda }\left (\rvvu \right )\right ) \\ &\;= -\frac {1}{2} \mathbb {E} {\left ( \vt _{\vlambda }\left (\rvvu \right ) - \vm \right )}^{\top } \mS ^{-1} \left (\vt _{\vlambda }\left (\rvvu \right ) - \vm \right ) - \frac {1}{2}\log \abs {\mS } + Z, \\ &\;= -\frac {1}{2} d - \frac {1}{2}\log \abs {\mS } + Z, \end {alignat*} where, \(\mS = \mC \mC ^{\top }\) and \(Z\) is a constant independent of \(\vm \) and \(\mC \). The derivative of the \(f\) is given as \begin {alignat}{2} &\nabla f\left (\vt _{\vlambda }\left (\vu \right )\right ) \nonumber \\ &\;= -\nabla \log p\left (\vt _{\vlambda }\left (\vu \right )\right ) + \nabla \log q_{\vgamma }\left (\vt _{\vlambda }\left (\vu \right )\right ) \lvert _{\vgamma = \vlambda } \nonumber \\ &\;= \mSigma ^{-1} \left (\vt _{\vlambda }\left (\vu \right ) - \vmu \right ) - {\mS }^{-1} \left ( \vt _{\vgamma }\left (\vu \right ) - \vm \right ) \lvert _{\vgamma = \vlambda }.\label {eq:gradient_lower_bound_f_derivative} \end {alignat} \par \begin {alignat*}{2} \mathbb {E} \norm {\vg \left (\vlambda \right )}_2^2 = \frac {1}{M} \mathbb {E} \norm { \nabla _{\vlambda } f\left ( \vt _{\vlambda }\left (\rvvu \right ) \right ) }_2^2 + \frac {M-1}{M} \norm { \nabla F\left (\vlambda \right ) }_2^2 \end {alignat*} \par We now compute the expectation of the squared gradient norm. First, by starting from \cref {thm:variational_gradient_norm_identity}, \begin {alignat}{2} &\mathbb {E}\norm { \nabla _{\vlambda } f\left ( \vt _{\vlambda }\left (\rvvu \right ) \right ) }_2^2 \nonumber \\ &\;= \mathbb {E} \norm { \nabla f\left ( \vt _{\vlambda }\left (\rvvu \right ) \right ) }_2^2 \left ( 1 + \norm {\rvvu }_2^2 \right ), \nonumber \shortintertext {plugging in \cref {eq:gradient_lower_bound_f_derivative},} &\;= \mathbb {E} \norm {\, \mSigma ^{-1} \left (\vt _{\vlambda }\left (\vu \right ) - \vmu \right ) - {\mS }^{-1} \left ( \vt _{\vlambda }\left (\vu \right ) - \vm \right )\,}^2_2 \left ( 1 + \norm {\rvvu }_2^2 \right ) \nonumber \\ &\;= \mathbb {E} {\left (\vt _{\vlambda }\left (\vu \right ) - \vmu \right )}^{\top } \mSigma ^{-\top } \mSigma ^{-1} \left (\vt _{\vlambda }\left (\vu \right ) - \vmu \right ) \left ( 1 + \norm {\rvvu }_2^2 \right ) \nonumber \\ &\qquad - 2\,\mathbb {E} {\left (\vt _{\vlambda }\left (\vu \right ) - \vmu \right )}^{\top } \mSigma ^{-\top } \mS ^{-1} \left (\vt _{\vlambda }\left (\vu \right ) - \vm \right ) \left ( 1 + \norm {\rvvu }_2^2 \right ) \nonumber \\ &\qquad + \mathbb {E} {\left (\vt _{\vlambda }\left (\vu \right ) - \vm \right )}^{\top } \mS ^{-\top } \mS ^{-1} \left (\vt _{\vlambda }\left (\vu \right ) - \vm \right ) \left ( 1 + \norm {\rvvu }_2^2 \right ), \nonumber \shortintertext {applying \cref {thm:general_grad_norm_reparam},} &\;= \left (d+1\right ) \vmu ^{\top } \mSigma ^{-\top } \mSigma ^{-1} \vmu + \left (d+\kappa \right ) \mathrm {tr}\left (\mS \mSigma ^{-\top } \mSigma ^{-1}\right ) \nonumber \\ &\qquad - 2 \, \left (d+1\right ) \vmu ^{\top } \mSigma ^{-\top } \mS ^{-1} \vm - 2\,\left (d+\kappa \right ) \mathrm {tr}\left (\mS \mSigma ^{-\top } \mS ^{-1}\right ) \nonumber \\ &\qquad + \left (d+1\right ) \vm ^{\top } \mS ^{-\top } \mS ^{-1} \vm + \left (d+\kappa \right ) \mathrm {tr}\left (\mS ^{-1}\right ), \nonumber \\ \shortintertext {and after regroupping the terms,} &\;= \left (d+1\right ) \left ( \vmu ^{\top } \mSigma ^{-\top } \mSigma ^{-1} \vmu - 2 \vmu ^{\top } \mSigma ^{-\top } \mS ^{-1} \vm + \vm ^{\top } \mS ^{-\top } \mS ^{-1} \vm \right ) \nonumber \\ &\qquad + \left (d+\kappa \right ) \left ( \mathrm {tr}\left (\mS \mSigma ^{-\top } \mSigma ^{-1}\right ) -2 \mathrm {tr}\left (\mS \mSigma ^{-\top } \mS ^{-1}\right ) + \mathrm {tr}\left (\mS ^{-1}\right ) \right ). \nonumber \end {alignat} This expression itself is not very convenient to work with due to the inner product term of \(\vmu \) and \(\vm \). Thus, we focus on the special case \(\vmu = \mathbf {0}\). Then, \begin {alignat}{2} &\mathbb {E}\norm { \nabla _{\vlambda } f\left ( \vt _{\vlambda }\left (\rvvu \right ) \right ) }_2^2 \nonumber \\ &\;= \left (d+1\right )\vm ^{\top } \mS ^{-\top } \mS ^{-1} \vm \nonumber \\ &\quad + \left (d+\kappa \right ) \left ( \mathrm {tr}\left (\mS \mSigma ^{-\top } \mSigma ^{-1}\right ) -2 \, \mathrm {tr}\left (\mSigma ^{-\top }\right ) + \mathrm {tr}\left (\mS ^{-1}\right ) \right ) \nonumber \\ &\;= \left (d+1\right )\vm ^{\top } \mS ^{-\top } \mS ^{-1} \vm \nonumber \\ &\quad + \left (d+\kappa \right ) \left ( \mathrm {tr}\left (\mS \mSigma ^{-\top } \mSigma ^{-1}\right ) + \mathrm {tr}\left (\mS ^{-1}\right ) \right ) \nonumber \\ &\quad - \underbrace { 2 \left (d+\kappa \right ) \mathrm {tr}\left (\mSigma ^{-\top }\right ) }_{\text {constant with respect to \(\vlambda \)}} \end {alignat} This also simplifies the suboptimality gap as \begin {alignat}{2} & F\left (\vlambda \right ) - F\left (\vlambda ^{*}\right ) \nonumber \\ &\;= \frac {1}{2} \vm ^{\top } \mSigma ^{-1} \vm + \frac {1}{2} \left ( \mathrm {tr}\left (\mSigma ^{-1} \mS \right ) + \mathrm {tr}\left (\mSigma ^{-1} \mS ^* \right ) \right ) \nonumber \\ &\qquad + \frac {1}{2} \left ( -\log \abs {\mS } + \log \abs {\mS ^*} \right ). \nonumber \\ &\;= \frac {1}{2} \vm ^{\top } \mSigma ^{-1} \vm + \frac {1}{2} \left ( \mathrm {tr}\left (\mSigma ^{-1} \mS \right ) - \log \abs {\mS } \right ) \nonumber \\ &\qquad - \underbrace {\frac {1}{2} \,\left (\mathrm {tr}\left (\mSigma ^{-1} \mS ^* \right ) - \log \abs {\mS ^*}\right )}_{\text {constant with respect to \(\vlambda \)}}. \nonumber \end {alignat} \par Showing \begin {alignat}{2} &\vm ^{\top } \mS ^{-\top } \mS ^{-1} \vm \; &&\succeq \; \vm ^{\top } \mSigma ^{-1} \vm \\ &\mathrm {tr}\left (\mS \mSigma ^{-\top } \mSigma ^{-1}\right ) \; &&\succeq \; \mathrm {tr}\left (\mSigma ^{-1} \mS \right ) \\ &\mathrm {tr}\left (\mS ^{-1}\right ) \; &&\succeq \; -\log \abs {\mS } \end {alignat}for proves the statement.\end{proof}

\prAtEndRestatexix*
