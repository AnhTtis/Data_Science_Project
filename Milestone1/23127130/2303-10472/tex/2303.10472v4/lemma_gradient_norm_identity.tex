
\begin{theoremEnd}[\keylemmaproofoption,category=upperboundkeylemmagradientnormidentity]{lemma}\label{thm:general_variational_gradient_norm_identity}
  Let \(\vt_{\vlambda}: \mathbb{R}^d \rightarrow \mathbb{R}^d\) be a location-scale reparameterization function (\cref{def:reparam}) with some differentiable function \(f : \mathbb{R}^d \rightarrow \mathbb{R} \).
  Then, for \(\vg_{f} \triangleq \nabla f\left( \vt_{\vlambda}\left(\vu\right) \right)\), 
  \begin{enumerate}[label=(\roman*)]
    \vspace{-2ex}
    \setlength\itemsep{-1ex}
    \item Mean-Field
      \vspace{-1ex}
    {\small
    \setlength{\belowdisplayskip}{1ex} \setlength{\belowdisplayshortskip}{1ex}
    \setlength{\abovedisplayskip}{1ex} \setlength{\abovedisplayshortskip}{1ex}
    \begin{alignat*}{2}
      \hspace{-3em}
      \norm{ \nabla_{\vlambda} f\left( \vt_{\vlambda}\left(\vu\right) \right) }_2^2
      = 
      {\lVert \vg_{f} \rVert}_2^2
      +
      \vg_{f}^{\top}
      \mU \mPhi
      \vg_{f},
      \qquad\qquad
    \end{alignat*}
  }

    \item Cholesky
      {\small%
    {
    \setlength{\belowdisplayskip}{1ex} \setlength{\belowdisplayshortskip}{1ex}
    \setlength{\abovedisplayskip}{1ex} \setlength{\abovedisplayshortskip}{1ex}
    \begin{alignat*}{2}
      \norm{ \nabla_{\vlambda} f\left( \vt_{\vlambda}\left(\vu\right) \right) }_2^2
      &=
      {\lVert \vg_{f} \rVert}_2^2 + \vg_{f}^{\top} \mSigma \vg_{f}
      +
      \vg_{f}^{\top}
      \mU
      \left(
      \mPhi
      - 
      \boldupright{I}
      \right)
      \vg_{f},
    \end{alignat*}
      }%
      %\hspace{-2ex}
  }
  \end{enumerate}
  where \(\mU,\mPhi,\mSigma\) are diagonal matrices, which the diagonals are defined as 
  {
    \setlength{\belowdisplayskip}{.5ex} \setlength{\belowdisplayshortskip}{.5ex}
    \setlength{\abovedisplayskip}{.5ex} \setlength{\abovedisplayshortskip}{.5ex}
    \[
    U_{ii} = u_i^2,\quad
    \Phi_{ii} = {\phi^{\prime}\left(s_i\right)}^2,\quad
    \Sigma_{ii} = {\textstyle\sum^{i}_{j=1}} u_j^2,
    \]
  }
  %\vspace{-2ex}
  and \(\phi\) is a diagonal conditioner for the scale matrix.
\end{theoremEnd}
\vspace{-2ex}
\begin{proofEnd}
  The proof starts by applying the Chain Rule and then computing the quadratic norm of the gradient as
  \begin{alignat}{2}
    &\norm{\nabla_{\vlambda} f\left( \vt_{\vlambda}\left(\vu\right) \right) }_2^2
    \nonumber
    \\
    &\;= 
    {\left(
      \frac{
        \partial \vt_{\vlambda}\left(\vu\right)
      }{
        \partial \vlambda
      }
      \nabla f\left( \vt_{\vlambda}\left(\vu\right) \right)
    \right)}^{\top}
    \frac{
      \partial \vt_{\vlambda}\left(\vu\right)
    }{
      \partial \vlambda
    }
    \nabla f\left( \vt_{\vlambda}\left(\vu\right) \right)
    \nonumber
    \\
    &\;=
    {\nabla f^{\top}\left( \vt_{\vlambda}\left(\vu\right) \right)}
    {\left(
      \frac{
        \partial \vt_{\vlambda}\left(\vu\right)
      }{
        \partial \vlambda
      }
    \right)}^{\top}
    \frac{
      \partial \vt_{\vlambda}\left(\vu\right)
    }{
      \partial \vlambda
    }
    \nabla f\left( \vt_{\vlambda}\left(\vu\right) \right)
    \nonumber
    \\
    &\;=
    {\vg_{f}^{\top}}
    {\left(
      \frac{
        \partial \vt_{\vlambda}\left(\vu\right)
      }{
        \partial \vlambda
      }
    \right)}^{\top}
    \frac{
      \partial \vt_{\vlambda}\left(\vu\right)
    }{
      \partial \vlambda
    }
    \vg_{f}.\label{eq:variational_gradient_norm_identity_eq1}
  \end{alignat}
  Naturally, the derivative of the reparameterization function will depend on the specific parameterization used.

  \paragraph{Proof for Cholesky}

  Let \(p\) denote the number of scalar variational parameters such that \(\vlambda = (\lambda_1, \ldots, \lambda_p)\).
  Then,
  \begin{alignat*}{2}
    &{\left(
      \frac{
        \partial \vt_{\vlambda}\left(\vu\right)
      }{
        \partial \vlambda
      }
    \right)}^{\top}
    \frac{
      \partial \vt_{\vlambda}\left(\vu\right)
    }{
      \partial \vlambda
    }
    \\
    &\;=
    \sum^{d}_{i=1} 
    \frac{
      \partial \vt_{\vlambda}\left(\vu\right)
    }{
      \partial m_i
    }
    {\left(
    \frac{
      \partial \vt_{\vlambda}\left(\vu\right)
    }{
      \partial m_i
    }
    \right)}^{\top}
    +
    \sum^{d}_{i=1} 
    \sum^{d}_{j \leq i} 
    \frac{
      \partial \vt_{\vlambda}\left(\vu\right)
    }{
      \partial \lambda_{C_{ij}}
    }
    {\left(
    \frac{
      \partial \vt_{\vlambda}\left(\vu\right)
    }{
      \partial \lambda_{C_{ij}}
    }
    \right)}^{\top},
  \end{alignat*}
  where \(\lambda_{C_{ij}}\) denote the parameter responsible for the \(ij\)-th entry of \(\mC\), \(C_{ij}\).
  Notice that, unlike for the matrix square root parameterization~\citep{domke_provable_2019}, the sum for \(C_{ij}\) is only over the lower triangular section.

  For the derivatives with respect to \(m_i\) and \(C_{ij}\), \citet{domke_provable_2020, domke_provable_2019} show that
  \begin{alignat}{2}
    \frac{\partial \vt_{\vlambda}\left(\vu\right) }{ \partial m_i }   &= \boldupright{e}_i \quad
    \frac{\partial \vt_{\vlambda}\left(\vu\right) }{ \partial C_{ij} } &= \boldupright{e}_i u_j,\label{eq:covariance_derivative}
  \end{alignat}
  where \(\boldupright{e}_i\) is the unit basis of the \(i\)th component.

  Therefore,
  \begin{alignat}{2}
    &{\left(
      \frac{
        \partial \vt_{\vlambda}\left(\vu\right)
      }{
        \partial \vlambda
      }
    \right)}^{\top}
    \frac{
      \partial \vt_{\vlambda}\left(\vu\right)
    }{
      \partial \vlambda
    }
    \nonumber
    \\
    &\;=
    \sum^{d}_{i=1} 
    \boldupright{e}_i
    \boldupright{e}_i^{\top}
    +
    \sum^{d}_{i=1} 
    \sum_{j \leq i} 
    \frac{
      \partial \vt_{\vlambda}\left(\vu\right)
    }{
      \partial \lambda_{C_{ij}}
    }
    {\left(
    \frac{
      \partial \vt_{\vlambda}\left(\vu\right)
    }{
      \partial \lambda_{C_{ij}}
    }
    \right)}^{\top}
    \nonumber
    \\
    &\;=
    \boldupright{I}
    +
    \underbrace{
    \sum^{d}_{i=1} 
    \frac{
      \partial \vt_{\vlambda}\left(\vu\right)
    }{
      \partial \lambda_{C_{ii}}
    }
    {\left(
    \frac{
      \partial \vt_{\vlambda}\left(\vu\right)
    }{
      \partial \lambda_{C_{ii}}
    }
    \right)}^{\top}
    }_{\text{diagonal of \(\mC\)}}
    \nonumber
    \\
    &\qquad+
    \underbrace{
    \sum^{d}_{i=1} 
    \sum_{j < i} 
    \frac{
      \partial \vt_{\vlambda}\left(\vu\right)
    }{
      \partial \lambda_{C_{ij}}
    }
    {\left(
    \frac{
      \partial \vt_{\vlambda}\left(\vu\right)
    }{
      \partial \lambda_{C_{ij}}
    }
    \right)}^{\top}}_{\text{off-diagonal of \(\mC\)}},
    \label{eq:thm:variational_gradient_norm_identity_eq2}
  \end{alignat}
  leaving us with the derivatives of the scale term.

  The gradient with respect to \(\lambda_{C_{ij}}\), however, depends on the parameterization.
  That is,
  \begin{alignat}{2}
    \frac{
      \partial \vt_{\vlambda}\left(\vu\right)
    }{
      \partial \lambda_{C_{ij}}
    }
    &=
    \frac{
      \partial \vt_{\vlambda}\left(\vu\right)
    }{
      \partial C_{ij}
    }
    \frac{
      \partial C_{ij}
    }{
      \partial \lambda_{C_{ij}}
    }
    &=
    \boldupright{e}_i u_j
    \frac{
      \partial C_{ij}
    }{
      \partial \lambda_{C_{ij}}
    }. \label{eq:thm:variational_gradient_norm_identity_covderivative}
  \end{alignat}

  For the diagonal elements, \(\lambda_{C_{ii}} = s_i\).
  Thus,
  \begin{align}
    \frac{
      \partial C_{ii}
    }{
      \partial s_i
    }
    =
    \frac{
      \partial \phi\left(s_i\right)
    }{
      \partial s_i
    }
    =
    \phi^{\prime}\left(s_i\right). \label{eq:thm:variational_gradient_norm_identity_diag}
  \end{align}
  And for the off-diagonal elements, \(\lambda_{L_{ij}} = L_{ij}\), and
  \begin{align}
    \frac{
      \partial C_{ij}
    }{
      \partial L_{ij}
    }
    =
    1. \label{eq:thm:variational_gradient_norm_identity_offdiag}
  \end{align}

  Plugging \cref{eq:thm:variational_gradient_norm_identity_diag,eq:thm:variational_gradient_norm_identity_offdiag,eq:thm:variational_gradient_norm_identity_covderivative} into \cref{eq:thm:variational_gradient_norm_identity_eq2},
  \begin{alignat}{2}
    &{\left(
      \frac{
        \partial \vt_{\vlambda}\left(\vu\right)
      }{
        \partial \vlambda
      }
    \right)}^{\top}
    \frac{
      \partial \vt_{\vlambda}\left(\vu\right)
    }{
      \partial \vlambda
    }
    \nonumber
    \\
    &\;=
    \boldupright{I}
    +
    \underbrace{
    \sum^{d}_{i=1} 
    {\left( u_i \phi^{\prime}\left(s_i\right) \right)}^2
    \boldupright{e}_i \boldupright{e}_i^{\top}
    }_{\text{diagonal of \(\mC\)}}
    +
    \underbrace{
    \sum^{d}_{i=1} 
    \sum_{j=1, j < i} 
    u_j^2 \, \boldupright{e}_i \boldupright{e}_i^{\top}
    }_{\text{off-diagonal of \(\mC\)}}
    \nonumber
    \\
    &\;=
    \boldupright{I}
    +
    \underbrace{
    \sum^{d}_{i=1} 
    u_i^2 {\left(\phi^{\prime}\left(s_i\right) \right)}^2
    \boldupright{e}_i \boldupright{e}_i^{\top}
    }_{\text{diagonal of \(\mC\)}}
    +
    \underbrace{
    \sum^{d}_{i=1} 
    \sum_{j \leq i} 
    u_j^2 \, \boldupright{e}_i \boldupright{e}_i^{\top}
    -
    \sum^{d}_{i=1} 
    u_i^2 \, \boldupright{e}_i \boldupright{e}_i^{\top}
    }_{\text{off-diagonal of \(\mC\)}}
    \nonumber
    \\
    &\;=
    \boldupright{I}
    +
    \underbrace{
      \mU \, \mPhi
    }_{\text{diagonal of \(\mC\)}}
    +
    \underbrace{
    \mSigma
    -
    \mU
    }_{\text{off-diagonal of \(\mC\)}}
    \nonumber
    \\
    &\;=
    \left( \boldupright{I} + \mSigma \right)
    +
    \mU \left( \mPhi - \boldupright{I} \right), \label{eq:variational_gradient_norm_identity_jacinner}
  \end{alignat}
  where \(\mU,\mPhi,\mSigma\) are diagonal matrices defined as
  \begin{alignat*}{2}
    &
    \mPhi
    &&=
    \mathrm{diag}\left(
    \left[ {\phi^{\prime}\left(s_1\right)}^2, \ldots , {\phi^{\prime}\left(s_d\right)}^2 \right]
    \right)
    \\
    &\mU
    &&=
    \mathrm{diag}\left(
    \left[ u_1^2, \ldots, u_d^2 \right]
    \right)
    \\
    &\mSigma
    &&=
    \mathrm{diag}\left(
    \left[ u_1^2, u_1^2 + u_2^2,\, \ldots\,, {\textstyle\sum^{d}_{i=1} u_i^2} \right]
    \right).
  \end{alignat*}
  The major difference with the proof of \citet[Lemma 8]{domke_provable_2019} for the matrix square root case is that we only sum the \(u_j^2 \boldupright{e}_i \boldupright{e}_i^{\top}\) terms over the \textit{lower diagonal elements}.
  This is the variance reduction effect we get from using the Cholesky parameterization.

  Coming back to \cref{eq:variational_gradient_norm_identity_eq1}, 
  \begin{alignat}{2}
    &\norm{\nabla_{\vlambda} f\left( \vt_{\vlambda}\left(\vu\right) \right)}_2^2
    \nonumber
    \\
    \;&=
      \vg_f^{\top}
      {\left(
        \frac{
          \partial \vt_{\vlambda}\left(\vu\right)
        }{
          \partial \vlambda
        }
        \right)}^{\top}
      \frac{
        \partial \vt_{\vlambda}\left(\vu\right)
      }{
        \partial \vlambda
      }
      \vg
    \nonumber
    \\
    &=
      \vg_{f}^{\top}
      \Big(
        \left( \boldupright{I} + \mSigma \right)
        +
        \mU \left( \mPhi - \boldupright{I} \right)
      \Big)
      \vg_{f}
    \nonumber
    \\
    &=
    {\lVert \vg_{f} \rVert}_2^2
    +
    \vg_{f}^{\top} \mSigma \vg_{f}
    +
    \vg_{f}^{\top}
    \mU \left( \mPhi - \boldupright{I} \right)
    \vg_{f}.
    \label{eq:variational_gradient_norm_identity_conclusion}
  \end{alignat}

  \paragraph{Proof for Mean-field}
  For the mean-field variational family, the covariance has only diagonal elements.
  Therefore, \cref{eq:variational_gradient_norm_identity_jacinner} becomes
  \begin{alignat}{2}
    {\left(
      \frac{
        \partial \vt_{\vlambda}\left(\vu\right)
      }{
        \partial \vlambda
      }
    \right)}^{\top}
    \frac{
      \partial \vt_{\vlambda}\left(\vu\right)
    }{
      \partial \vlambda
    }
    &=
    \boldupright{I} + \mU \mPhi,
    \nonumber
  \end{alignat}
  and \cref{eq:variational_gradient_norm_identity_conclusion} becomes
  \begin{alignat}{2}
    \norm{\nabla_{\vlambda} f\left( \vt_{\vlambda}\left(\vu\right) \right)}_2^2
    =
      \vg_{f}^{\top}
      \left(
        \boldupright{I}
        +
        \mU \mPhi
      \right)
      \vg_{f}
    =
    {\lVert \vg_{f} \rVert}_2^2
    +
    \vg_{f}^{\top}
    \mU \mPhi
    \vg_{f}.
    \nonumber
  \end{alignat}
\end{proofEnd}

Note that the relationships in this lemma are all equalities, which can be bounded with known quantities, as done in the next lemma.
We note here that if any of our analyses were to be improved, this shall by done by obtaining tighter bounds on the equalities in \cref{thm:general_variational_gradient_norm_identity}.

\begin{theoremEnd}[\keylemmaproofoption,category=upperboundkeylemmagradientnormbound]{lemma}\label{thm:general_variational_gradient_norm_bound}
Let \(\vt_{\vlambda}: \mathbb{R}^d \rightarrow \mathbb{R}^d\) be a location-scale reparameterization function (\cref{def:reparam}), \(f : \mathbb{R}^d \rightarrow \mathbb{R} \) be a differentiable function, and let \(\phi\) satisfy \cref{assumption:phi_lipschitz}.
  \vspace{-5ex}
  \begin{enumerate}[label=(\roman*)]
    \setlength\itemsep{-1ex}
    \item Mean-Field
    {%
    \setlength{\belowdisplayskip}{1ex} \setlength{\belowdisplayshortskip}{1ex}%
    \setlength{\abovedisplayskip}{1ex} \setlength{\abovedisplayshortskip}{1ex}%
      \[
        \norm{\nabla_{\vlambda} f\left( \vt_{\vlambda}\left(\vu\right) \right)}_2^2
        \leq
        \left(1 + \norm{ \mU }_{\mathrm{F}} \right)
        {\lVert \nabla f\left( \vt_{\vlambda}\left(\vu\right) \right) \rVert}_2^2,
      \]
      where \(\mU\) is a diagonal matrix such that \(U_{ii} = u_i^2\).
    }%
    \item Cholesky
    {%
    \setlength{\belowdisplayskip}{1ex} \setlength{\belowdisplayshortskip}{1ex}%
    \setlength{\abovedisplayskip}{1ex} \setlength{\abovedisplayshortskip}{1ex}%
      \[
        \norm{\nabla_{\vlambda} f\left( \vt_{\vlambda}\left(\vu\right) \right)}_2^2
        \leq
        \left(1 + \norm{ \vu }_2^2 \right)
        {\lVert \nabla f\left( \vt_{\vlambda}\left(\vu\right) \right) \rVert}_2^2,
      \]
      }%
      where the equality holds for the matrix square root parameterization.
  \end{enumerate}
\end{theoremEnd}
\vspace{-2ex}
\begin{proofEnd}
  The proof continues from the result of \cref{thm:general_variational_gradient_norm_identity}.

  \paragraph{Proof for Cholesky}
  \cref{thm:general_variational_gradient_norm_identity} shows that
  \begin{alignat*}{2}
    \norm{ \nabla_{\vlambda} f\left( \vt_{\vlambda}\left(\vu\right) \right) }_2^2
    = 
    {\lVert \vg_{f} \rVert}_2^2
    +
    \vg_{f}^{\top} \mSigma \vg_{f}
    +
    \vg_{f}^{\top}
    \mU \left( \mPhi - \boldupright{I} \right)
    \vg_{f},
  \end{alignat*}
  where \(\vg_f = \nabla f\left(\vt_{\vlambda}\left(\vu\right)\right) \).

  By the 1-Lipschitz assumption, the entries of the diagonal matrix \(\Phi\) satisfy
  \begin{align*}
    \Phi_{ii} = {\phi^{\prime}\left(d_i\right)}^2 \leq 1,
  \end{align*}
  which means
  \begin{alignat*}{2}
    \mPhi \preceq \boldupright{I}
    \;\Rightarrow\;
    \mU \left( \mPhi - \boldupright{I} \right)
    \preceq
    0
    \;\Rightarrow\;
    {\vg_{f}}^{\top} \mU \left( \mPhi - \boldupright{I} \right) \vg_{f} \leq 0.
  \end{alignat*}
  Therefore, for the full-rank Cholesky parameterization and a 1-Lipschitz conditioner \(\phi\),
  \begin{alignat*}{2}
    &\norm{\nabla_{\vlambda} f\left( \vt_{\vlambda}\left(\vu\right) \right)}_2^2
    \\
    &\;=
    {\lVert \nabla f\left( \vt_{\vlambda}\left(\vu\right) \right) \rVert}_2^2
    +
    {\vg_{f}}^{\top}
    \mSigma
    \vg_{f}
    +
    {\vg_{f}}^{\top} \mU \left( \mPhi - \boldupright{I} \right) \vg_{f}
    \\
    &\;\leq
    {\lVert \nabla f\left( \vt_{\vlambda}\left(\vu\right) \right) \rVert}_2^2
    +
    {\vg_{f}}^{\top}
    \mSigma
    \vg_{f}
    \\
    &\;\leq
    {\lVert \nabla f\left( \vt_{\vlambda}\left(\vu\right) \right) \rVert}_2^2
    +
    \norm{ \mSigma }_{2,2} {\lVert \nabla f\left( \vt_{\vlambda}\left(\vu\right) \right) \rVert}_2^2
    \\
    &\;=
    {\lVert \nabla f\left( \vt_{\vlambda}\left(\vu\right) \right) \rVert}_2^2
    +
    \left( \sum^{d}_{i=1} u_{i}^2 \right)  {\lVert \nabla f\left( \vt_{\vlambda}\left(\vu\right) \right) \rVert}_2^2
    \\
    &\;=
    \left(1 + \norm{\vu}^2_2\right) {\lVert \nabla f\left( \vt_{\vlambda}\left(\vu\right) \right) \rVert}_2^2,
  \end{alignat*}
  where \(\norm{\mU}_{2,2}\) is the \(L_2\) operator norm of \(\mU\).
  This upper bound coincides with that of the matrix square root parameteration.
  Thus, unforunately, this bound fails to acknowledge the lower variance of the Cholesky parameterization, coinciding with that of the matrix square root parameterization.

  \paragraph{Proof for Mean-field (\cref{def:meanfield})}
  For the mean-field parameterization,~\cref{thm:general_variational_gradient_norm_identity} shows that
  \begin{alignat*}{2}
    \norm{ \nabla_{\vlambda} f\left( \vt_{\vlambda}\left(\vu\right) \right) }_2^2
    =
    {\lVert \vg_{f} \rVert}_2^2
    +
    \vg_{f}^{\top}
    \mU \mPhi
    \vg_{f}.
  \end{alignat*}

  For the second term,  
  \begin{alignat*}{2}
    \vg_{f}^{\top}
    \mU \mPhi
    \vg_{f}
    \leq
    {\lVert \mU \rVert}_{2,2} {\lVert \mPhi \rVert}_{2,2}
    {\lVert \vg_{f} \rVert}^2_2.
  \end{alignat*}
  By the \(1\)-Lipschitzness of \(\phi\),
  \[
    {\lVert \mPhi \rVert}_{2,2}
    = \sigma_{\mathrm{max}}\left( \mPhi \right)
    = \max_{i = 1, \ldots, d} {\phi^{\prime}\left( s_i \right)}^2
    \leq 1.
  \]
  Then,
  \begin{alignat}{2}
    \vg_{f}^{\top}
    \left( \mU \mPhi \right)
    \vg_{f}
    &\leq
    {\lVert \mU \rVert}_{2,2} \,
    {\lVert \vg_{f} \rVert}^2_2 \label{eq:variational_gradient_norm_identity_mf_eq1}
    \\
    &\leq
    {\lVert \mU \rVert}_{\mathrm{F}} \,
    {\lVert \vg_{f} \rVert}^2_2, \label{eq:variational_gradient_norm_identity_mf_eq2}
  \end{alignat}
  which gives the result.
  Here, unlike the bounds on \(\mPhi\), the bounds in \cref{eq:variational_gradient_norm_identity_mf_eq1,eq:variational_gradient_norm_identity_mf_eq2} are quite loose, and become looser as the dimensionality increases.

%%   We conclude as
%%   \begin{alignat*}{2}
%%     \norm{ \nabla_{\vlambda} f\left( \vt_{\vlambda}\left(\vu\right) \right) }_2^2
%%     \leq
%%     \norm{ \nabla f\left(\vt_{\vlambda}\left(\vu\right)\right) }_2^2
%%     + \frac{1}{2}{\lVert \vg_{f} \rVert}_2^2 + \frac{1}{2}\norm{\vu}_2^2
%%     =
%%     \frac{3}{2} \norm{ \nabla f\left(\vt_{\vlambda}\left(\vu\right)\right) }_2^2
%%     +
%%     \frac{1}{2} \norm{\vu}_2^2
%%   \end{alignat*}
\end{proofEnd}


%%% Local Variables:
%%% TeX-master: "main"
%%% End:
