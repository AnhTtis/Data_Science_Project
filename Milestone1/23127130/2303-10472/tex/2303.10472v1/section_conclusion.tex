
\vspace{-.5ex}
\section{Discussions}
\vspace{-.5ex}
In this work, we have proven upper bounds on the gradient variance of BBVI with the location-scale family for smooth, quadratically-growing log-likelihoods.
Specifically, we have provided bounds for both the ELBO in entropy-regularized and KL-regularized forms.
Our guarantees work without a single modification to the algorithms used in practice, although stronger assumptions establish a tighter bound for the entropy-regularized form ELBO.

\vspace{-1ex}
\paragraph{Limitations}
Our results have the following limitations:
\begin{enumerate*}
    \item[\ding{182}] Our results only apply to smooth and quadratically-growing log likelihoods and
    \item[\ding{183}] the location-scale ADVI family. Also, 
    \item[\ding{184}] our bounds cannot distinguish the variance of the Cholesky and matrix square root parameterizations, 
    \item[\ding{185}] and empirically, the bounds for the mean-field parameterization appear loose. Furthermore, 
    \item[\ding{186}] our results only work with 1-Lipschitz diagonal conditioners such as the softplus function.
\end{enumerate*}
In practice, non-Lipschitz conditioners such as the exponential functions are widely used.
While obtaining similar bounds with such conditioners would be challenging, constructing a theoretical framework that extends to such would be an important future research direction.
 
%% \paragraph{Signal-to-Noise Ratio}
%% To empirically quantify variance in VI, the gradient signal-to-noise ratio metric
%% \begin{align*}
%%   \mathrm{SNR}\left(\rvvg\right) =
%%   \frac{ \norm{\mathbb{E}\rvvg}^2_2 }{ \mathbb{E}\norm{\rvvg}^2_2 }
%%   =
%%   \frac{ \norm{ \nabla F }^2_2 }{ \mathrm{tr} \mathbb{V}\rvvg + \norm{ \nabla F }^2_2  } \leq 1,
%% \end{align*}
%% where the last equality holds since \(\rvvg\) is unbiased, has recently seen use~\citep{pmlr-v139-geffner21a, rainforth_tighter_2018, fujisawa_multilevel_2021}.
%% This metric can be though as quantifying the relative magnitude of the gradient noise with respect to the true gradient.
%% Under the ABC condition, as discussed in~\cref{section:abc}, each of the \(A\) and \(C\) term contribute to the convergence speed (\(A\)) and radius fo the stationary region (\(C\)), which will be unique to different estimators.
%% However, when using the SNR ratio for comparing different estimatosr, it is not possible to understand the effect of each term.
%% Thus, if one would go with the the ABC condition, analyzing the contribution of each term separately would be more informative.

%Under the ESG condition, the SNR is provides an estimate of the lower bound of \(B\).
%However, the ESG condition only holds for the interpolation regime where \(\mathbb{E}\norm{\rvvg}_2^2\).
%Thus, under the ABC condition, it is unsure how the SNR relates to the convergence aspects of SGD.

%% \begin{proposition}
%%   Let \(F\) be \(L\)-smooth and convex.
%%   Also, let \(\mathrm{SNR}\left(\rvvg\left(\vlambda\right)\right) > 0\) be the gradient SNR at \(\vlambda\).
%%   Then, 
%%   \[
%%      \mathbb{E}\norm{\rvvg\left(\vlambda\right)}_2^2
%%      \leq 
%%      L \left(\frac{1}{\mathrm{SNR}\left(\rvvg\left(\vlambda\right)\right)} - 1\right)
%%      \left(F\left(\vlambda\right) - F^*\right)
%%      +
%%      \norm{\mathbb{E} \rvvg }_2^2,
%%   \]
%%   where \(F^* = F\left(\vlambda^*\right)\) for the global minimum \(\vlambda^*\).
%% \end{proposition}
%% \begin{proof}
%%   From smoothness and convexity, it follows that
%%   \begin{align}
%%      \norm{ \nabla F\left(\vlambda\right) }_2^2 
%%        \leq
%%        L \inner{\nabla F\left(\vlambda\right)}{\vlambda - \bar{\vlambda}}
%%        \leq
%%       L \left( F\left(\vlambda\right) - F^* \right),\label{eq:snr_bound_eq1}
%%   \end{align}
%%   where the first inequality uses the co-coercivity of Lipschitz gradients, the last bound is the definition of convexity.
%%   From the definition of the SNR,
%%   \begin{align*}
%%     \mathbb{E}\norm{\rvvg\left(\vlambda\right)}_2^2 
%%     &=
%%     \frac{1}{\mathrm{SNR}\left(\rvvg\left(\vlambda\right)\right)} 
%%     \norm{ \nabla F\left(\vlambda\right)  }_2^2
%%     \\
%%     &=
%%     \left(
%%     \frac{1}{\mathrm{SNR}\left(\rvvg\left(\vlambda\right)\right)} 
%%     - 1
%%     \right)
%%     \norm{ \nabla F\left(\vlambda\right)  }_2^2
%%     +
%%     \norm{ \nabla F\left(\vlambda\right)  }_2^2.
%%   \end{align*}
%%   Plugging \cref{eq:snr_bound_eq1} to the first \( \norm{ \nabla F\left(\vlambda\right)  }_2^2 \) yields the result.
%% \end{proof}

%% \begin{remark}
%%   Let an unbiased estimator of \(\nabla F\), \(\rvvg\), satisfy \cref{assumption:abc} with \(B=1\).
%%   Then, the gradient SNR is bounded as
%%   \[
%%     \frac{ \norm{\nabla F\left(\vlambda\right)}_2^2 }{
%%       2 A \left( F\left(\vlambda\right) - F^* \right)
%%       +
%%       \norm{ \nabla F\left(\vlambda\right) }_2^2
%%       +
%%       C
%%     }
%%     \leq 
%%     \mathrm{SNR}\left(\rvvg\left(\vlambda\right)\right)
%%   \]
%%   where \(F^* = \inf_{\vlambda \in \mathbb{R}^p}\).
%% \end{remark}

%% \(A\) and \(C\) are determined by the unique properties of the estimator.
%% Thus, comparing the SNR of different estimators is an indirect way to compare the constants.

\vspace{-1ex}
\paragraph{Interpolation Property}
Our lower bound shows that some problems will not have the interpolation property.
For these types of problems,~\citet{zhang_adam_2022} have shown that ADAM~\citep{kingma_adam_2015} diverges with a broad range of stepsizes.
This strictly differs from overparameterized deep neural networks where the interpolation property is reasonable.
It is thus possible that some of the current optimization practices may not be optimal for BBVI, pointing towards new research directions.

%%% Local Variables:
%%% TeX-master: "main"
%%% End:
