

\begin{restatable}[]{lemma}{prAtEndRestateii}\label{thm:prAtEndii}\label {thm:general_variational_gradient_norm_identity} Let \(\vt _{\vlambda }: \mathbb {R}^d \rightarrow \mathbb {R}^d\) be a location-scale reparameterization function (\cref {def:reparam}) with some differentiable function \(f : \mathbb {R}^d \rightarrow \mathbb {R} \). Then, for \(\vg _{f} \triangleq \nabla f\left ( \vt _{\vlambda }\left (\vu \right ) \right )\), \begin {enumerate}[label=(\roman *)] \vspace {-2ex} \setlength \itemsep {-1ex} \item Mean-Field \vspace {-1ex} { \setlength {\belowdisplayskip }{1ex} \setlength {\belowdisplayshortskip }{1ex} \setlength {\abovedisplayskip }{1ex} \setlength {\abovedisplayshortskip }{1ex} \begin {alignat*}{2} \hspace {-3em} \norm { \nabla _{\vlambda } f\left ( \vt _{\vlambda }\left (\vu \right ) \right ) }_2^2 = {\lVert \vg _{f} \rVert }_2^2 + \vg _{f}^{\top } \mU \mPhi \vg _{f}, \qquad \qquad \end {alignat*} } \par \item Cholesky {{ \setlength {\belowdisplayskip }{1ex} \setlength {\belowdisplayshortskip }{1ex} \setlength {\abovedisplayskip }{1ex} \setlength {\abovedisplayshortskip }{1ex} \begin {alignat*}{2} \norm { \nabla _{\vlambda } f\left ( \vt _{\vlambda }\left (\vu \right ) \right ) }_2^2 &= {\lVert \vg _{f} \rVert }_2^2 + \vg _{f}^{\top } \mSigma \vg _{f} + \vg _{f}^{\top } \mU \left ( \mPhi - \boldupright {I} \right ) \vg _{f}. \qquad \qquad \end {alignat*} }} where \(\mU ,\mPhi ,\mSigma \) are diagonal matrices, which the diagonals are defined as { \setlength {\belowdisplayskip }{.5ex} \setlength {\belowdisplayshortskip }{.5ex} \setlength {\abovedisplayskip }{.5ex} \setlength {\abovedisplayshortskip }{.5ex} \[ U_{ii} = u_i^2,\quad \Phi _{ii} = {\phi ^{\prime }\left (s_i\right )}^2,\quad \Sigma _{ii} = {\textstyle \sum ^{i}_{j=1}} u_j^2, \] } and \(\phi \) is a diagonal conditioner for the scale matrix. \end {enumerate}\end{restatable}

\makeatletter\Hy@SaveLastskip\label{proofsection:prAtEndii}\ifdefined\pratend@current@sectionlike@label\immediate\write\@auxout{\string\gdef\string\pratend@section@for@proofii{\pratend@current@sectionlike@label}}\fi\Hy@RestoreLastskip\makeatother\begin{proof}[Proof]\phantomsection\label{proof:prAtEndii}The proof starts by applying the Chain Rule and then computing the quadratic norm of the gradient as \begin {alignat}{2} &\norm {\nabla _{\vlambda } f\left ( \vt _{\vlambda }\left (\vu \right ) \right ) }_2^2 \nonumber \\ &\;= {\left ( \frac { \partial \vt _{\vlambda }\left (\vu \right ) }{ \partial \vlambda } \nabla f\left ( \vt _{\vlambda }\left (\vu \right ) \right ) \right )}^{\top } \frac { \partial \vt _{\vlambda }\left (\vu \right ) }{ \partial \vlambda } \nabla f\left ( \vt _{\vlambda }\left (\vu \right ) \right ) \nonumber \\ &\;= {\nabla f^{\top }\left ( \vt _{\vlambda }\left (\vu \right ) \right )} {\left ( \frac { \partial \vt _{\vlambda }\left (\vu \right ) }{ \partial \vlambda } \right )}^{\top } \frac { \partial \vt _{\vlambda }\left (\vu \right ) }{ \partial \vlambda } \nabla f\left ( \vt _{\vlambda }\left (\vu \right ) \right ) \nonumber \\ &\;= {\vg _{f}^{\top }} {\left ( \frac { \partial \vt _{\vlambda }\left (\vu \right ) }{ \partial \vlambda } \right )}^{\top } \frac { \partial \vt _{\vlambda }\left (\vu \right ) }{ \partial \vlambda } \vg _{f}.\label {eq:variational_gradient_norm_identity_eq1} \end {alignat} Naturally, the derivative of the reparameterization function will depend on the specific parameterization used. \par \paragraph {Proof for Cholesky} \par Let \(p\) denote the number of scalar variational parameters such that \(\vlambda = (\lambda _1, \ldots , \lambda _p)\). Then, \begin {alignat*}{2} &{\left ( \frac { \partial \vt _{\vlambda }\left (\vu \right ) }{ \partial \vlambda } \right )}^{\top } \frac { \partial \vt _{\vlambda }\left (\vu \right ) }{ \partial \vlambda } \\ &\;= \sum ^{d}_{i=1} \frac { \partial \vt _{\vlambda }\left (\vu \right ) }{ \partial m_i } {\left ( \frac { \partial \vt _{\vlambda }\left (\vu \right ) }{ \partial m_i } \right )}^{\top } + \sum ^{d}_{i=1} \sum ^{d}_{j \leq i} \frac { \partial \vt _{\vlambda }\left (\vu \right ) }{ \partial \lambda _{C_{ij}} } {\left ( \frac { \partial \vt _{\vlambda }\left (\vu \right ) }{ \partial \lambda _{C_{ij}} } \right )}^{\top }, \end {alignat*} where \(\lambda _{C_{ij}}\) denote the parameter responsible for the \(ij\)-th entry of \(\mC \), \(C_{ij}\). Notice that, unlike for the matrix square root parameterization~\citep {domke_provable_2019}, the sum for \(C_{ij}\) is only over the lower triangular section. \par For the derivatives with respect to \(m_i\) and \(C_{ij}\), \citet {domke_provable_2020, domke_provable_2019} show that \begin {alignat}{2} \frac {\partial \vt _{\vlambda }\left (\vu \right ) }{ \partial m_i } &= \ve _i \quad \frac {\partial \vt _{\vlambda }\left (\vu \right ) }{ \partial C_{ij} } &= \ve _i u_j,\label {eq:covariance_derivative} \end {alignat} where \(\ve _i\) is the unit basis of the \(i\)th component. \par Therefore, \begin {alignat}{2} &{\left ( \frac { \partial \vt _{\vlambda }\left (\vu \right ) }{ \partial \vlambda } \right )}^{\top } \frac { \partial \vt _{\vlambda }\left (\vu \right ) }{ \partial \vlambda } \nonumber \\ &\;= \sum ^{d}_{i=1} \ve _i \ve _i^{\top } + \sum ^{d}_{i=1} \sum _{j \leq i} \frac { \partial \vt _{\vlambda }\left (\vu \right ) }{ \partial \lambda _{C_{ij}} } {\left ( \frac { \partial \vt _{\vlambda }\left (\vu \right ) }{ \partial \lambda _{C_{ij}} } \right )}^{\top } \nonumber \\ &\;= \boldupright {I} + \underbrace { \sum ^{d}_{i=1} \frac { \partial \vt _{\vlambda }\left (\vu \right ) }{ \partial \lambda _{C_{ii}} } {\left ( \frac { \partial \vt _{\vlambda }\left (\vu \right ) }{ \partial \lambda _{C_{ii}} } \right )}^{\top } }_{\text {diagonal of \(\mC \)}} \nonumber \\ &\qquad + \underbrace { \sum ^{d}_{i=1} \sum _{j < i} \frac { \partial \vt _{\vlambda }\left (\vu \right ) }{ \partial \lambda _{C_{ij}} } {\left ( \frac { \partial \vt _{\vlambda }\left (\vu \right ) }{ \partial \lambda _{C_{ij}} } \right )}^{\top }}_{\text {off-diagonal of \(\mC \)}}, \label {eq:thm:variational_gradient_norm_identity_eq2} \end {alignat} leaving us with the derivatives of the scale term. \par The gradient with respect to \(\lambda _{C_{ij}}\), however, depends on the parameterization. That is, \begin {alignat}{2} \frac { \partial \vt _{\vlambda }\left (\vu \right ) }{ \partial \lambda _{C_{ij}} } &= \frac { \partial \vt _{\vlambda }\left (\vu \right ) }{ \partial C_{ij} } \frac { \partial C_{ij} }{ \partial \lambda _{C_{ij}} } &= \ve _i u_j \frac { \partial C_{ij} }{ \partial \lambda _{C_{ij}} }. \label {eq:thm:variational_gradient_norm_identity_covderivative} \end {alignat} \par For the diagonal elements, \(\lambda _{C_{ii}} = s_i\). Thus, \begin {align} \frac { \partial C_{ii} }{ \partial s_i } = \frac { \partial \phi \left (s_i\right ) }{ \partial s_i } = \phi ^{\prime }\left (s_i\right ). \label {eq:thm:variational_gradient_norm_identity_diag} \end {align} And for the off-diagonal elements, \(\lambda _{L_{ij}} = L_{ij}\), and \begin {align} \frac { \partial C_{ij} }{ \partial L_{ij} } = 1. \label {eq:thm:variational_gradient_norm_identity_offdiag} \end {align} \par Plugging \cref {eq:thm:variational_gradient_norm_identity_diag,eq:thm:variational_gradient_norm_identity_offdiag,eq:thm:variational_gradient_norm_identity_covderivative} into \cref {eq:thm:variational_gradient_norm_identity_eq2}, \begin {alignat}{2} &{\left ( \frac { \partial \vt _{\vlambda }\left (\vu \right ) }{ \partial \vlambda } \right )}^{\top } \frac { \partial \vt _{\vlambda }\left (\vu \right ) }{ \partial \vlambda } \nonumber \\ &\;= \boldupright {I} + \underbrace { \sum ^{d}_{i=1} {\left ( u_i \phi ^{\prime }\left (s_i\right ) \right )}^2 \ve _i \ve _i^{\top } }_{\text {diagonal of \(\mC \)}} + \underbrace { \sum ^{d}_{i=1} \sum _{j=1, j < i} u_j^2 \, \ve _i \ve _i^{\top } }_{\text {off-diagonal of \(\mC \)}} \nonumber \\ &\;= \boldupright {I} + \underbrace { \sum ^{d}_{i=1} u_i^2 {\left (\phi ^{\prime }\left (s_i\right ) \right )}^2 \ve _i \ve _i^{\top } }_{\text {diagonal of \(\mC \)}} + \underbrace { \sum ^{d}_{i=1} \sum _{j \leq i} u_j^2 \, \ve _i \ve _i^{\top } - \sum ^{d}_{i=1} u_i^2 \, \ve _i \ve _i^{\top } }_{\text {off-diagonal of \(\mC \)}} \nonumber \\ &\;= \boldupright {I} + \underbrace { \mU \, \mPhi }_{\text {diagonal of \(\mC \)}} + \underbrace { \mSigma - \mU }_{\text {off-diagonal of \(\mC \)}} \nonumber \\ &\;= \left ( \boldupright {I} + \mSigma \right ) + \mU \left ( \mPhi - \boldupright {I} \right ), \label {eq:variational_gradient_norm_identity_jacinner} \end {alignat} where \(\mU ,\mPhi ,\mSigma \) are diagonal matrices defined as \begin {alignat*}{2} & \mPhi &&= \mathrm {diag}\left ( \left [ {\phi ^{\prime }\left (s_1\right )}^2, \ldots , {\phi ^{\prime }\left (s_d\right )}^2 \right ] \right ) \\ &\mU &&= \mathrm {diag}\left ( \left [ u_1^2, \ldots , u_d^2 \right ] \right ) \\ &\mSigma &&= \mathrm {diag}\left ( \left [ u_1^2, u_1^2 + u_2^2,\, \ldots \,, {\textstyle \sum ^{d}_{i=1} u_i^2} \right ] \right ). \end {alignat*} The major difference with the proof of \citet [Lemma 8]{domke_provable_2019} for the matrix square root case is that we only sum the \(u_j^2 \ve _i \ve _i^{\top }\) terms over the \textit {lower diagonal elements}. This is the variance reduction effect we get from using the Cholesky parameterization. \par Coming back to \cref {eq:variational_gradient_norm_identity_eq1}, \begin {alignat}{2} &\norm {\nabla _{\vlambda } f\left ( \vt _{\vlambda }\left (\vu \right ) \right )}_2^2 \nonumber \\ \;&= \vg _f^{\top } {\left ( \frac { \partial \vt _{\vlambda }\left (\vu \right ) }{ \partial \vlambda } \right )}^{\top } \frac { \partial \vt _{\vlambda }\left (\vu \right ) }{ \partial \vlambda } \vg \nonumber \\ &= \vg _{f}^{\top } \Big ( \left ( \boldupright {I} + \mSigma \right ) + \mU \left ( \mPhi - \boldupright {I} \right ) \Big ) \vg _{f} \nonumber \\ &= {\lVert \vg _{f} \rVert }_2^2 + \vg _{f}^{\top } \mSigma \vg _{f} + \vg _{f}^{\top } \mU \left ( \mPhi - \boldupright {I} \right ) \vg _{f}. \label {eq:variational_gradient_norm_identity_conclusion} \end {alignat} \par \paragraph {Proof for Mean-field} For the mean-field variational family, the covariance has only diagonal elements. Therefore, \cref {eq:variational_gradient_norm_identity_jacinner} becomes \begin {alignat}{2} {\left ( \frac { \partial \vt _{\vlambda }\left (\vu \right ) }{ \partial \vlambda } \right )}^{\top } \frac { \partial \vt _{\vlambda }\left (\vu \right ) }{ \partial \vlambda } &= \boldupright {I} + \mU \mPhi , \nonumber \end {alignat} and \cref {eq:variational_gradient_norm_identity_conclusion} becomes \begin {alignat}{2} \norm {\nabla _{\vlambda } f\left ( \vt _{\vlambda }\left (\vu \right ) \right )}_2^2 = \vg _{f}^{\top } \left ( \boldupright {I} + \mU \mPhi \right ) \vg _{f} = {\lVert \vg _{f} \rVert }_2^2 + \vg _{f}^{\top } \mU \mPhi \vg _{f}. \nonumber \end {alignat}\end{proof}
