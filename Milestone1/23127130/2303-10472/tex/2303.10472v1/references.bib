@article{_autonomic_2015,
  title = {An Autonomic Performance Environment for Exascale},
  year = {2015},
  month = jul,
  journal = {Supercomputing Frontiers and Innovations},
  volume = {2},
  number = {3}
}

@misc{_computational_,
  title = {The Computational Asymptotics of {{Gaussian}} Variational Inference and the {{Laplace}} Approximation | {{SpringerLink}}},
  howpublished = {https://link.springer.com/article/10.1007/s11222-022-10125-y},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\NPJXDJFH\\s11222-022-10125-y.html}
}

@misc{_detection_,
  title = {Detection of Signals by Information Theoretic Criteria: General Asymptotic Performance Analysis | {{IEEE Journals}} \& {{Magazine}} | {{IEEE Xplore}}},
  howpublished = {https://ieeexplore.ieee.org/document/995060},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\PLGFX5FB\\995060.html}
}

@misc{_directionofarrival_,
  title = {Direction-of-Arrival Estimation for Wide-Band Signals Using the {{ESPRIT}} Algorithm | {{IEEE Journals}} \& {{Magazine}} | {{IEEE Xplore}}},
  howpublished = {https://ieeexplore.ieee.org/document/103067},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\5GKI45NE\\103067.html}
}

@misc{_doa_,
  title = {{{DOA}} Estimation of Wideband Sources without Estimating the Number of Sources | {{Elsevier Enhanced Reader}}},
  howpublished = {https://reader.elsevier.com/reader/sd/pii/S0165168411003690?token=E0C354E602333AA84EAC578A33A49BCBFD81280F48C8A5A03255D0FA14B564A1A453DFFD05D6DC1D6E0EFB8B07337094\&originRegion=us-east-1\&originCreation=20230110031618},
  langid = {english},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\BY4LRXFB\\DOA estimation of wideband sources without estimat.pdf}
}

@misc{_doa_a,
  title = {{{DOA}} Estimation of Wideband Sources without Estimating the Number of Sources - {{ScienceDirect}}},
  howpublished = {https://www.sciencedirect.com/science/article/pii/S0165168411003690?ref=cra\_js\_challenge\&fr=RR-1},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\JFMU7M6X\\S0165168411003690.html}
}

@misc{_exact_,
  title = {Exact {{MCMC}} with Differentially Private Moves | {{SpringerLink}}},
  howpublished = {https://link.springer.com/article/10.1007/s11222-018-9847-x}
}

@incollection{_fdivergences_,
  title = {F-Divergences},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\4WEVGPFU\\f-divergences.pdf}
}

@misc{_global_,
  title = {On the Global Convergence Rate of the Gradient Descent Method for Functions with {{H\"older}} Continuous Gradients | {{SpringerLink}}},
  howpublished = {https://link.springer.com/article/10.1007/s11590-015-0936-x},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\6YCPMLZ8\\s11590-015-0936-x.html}
}

@techreport{_openmp_2015,
  title = {{{OpenMP}} Application Programming Interface},
  year = {2015},
  month = nov,
  number = {Version 4.5},
  institution = {{OpenMP Architecture Review Board}}
}

@article{:/content/journals/10.1049/ip-rsn_19990255,
  title = {Improved Particle Filter for Nonlinear Problems},
  author = {Carpenter, J. and Clifford, P. and Fearnhead, P.},
  year = {1999},
  month = feb,
  journal = {IEE Proceedings - Radar, Sonar and Navigation},
  volume = {146},
  number = {1},
  pages = {2-7(5)},
  abstract = {The Kalman filter provides an effective solution to the linear Gaussian filtering problem. However where there is nonlinearity, either in the model specification or the observation process, other methods are required. Methods known generically as `particle filters' are considered. These include the condensation algorithm and the Bayesian bootstrap or sampling importance resampling (SIR) filter. These filters represent the posterior distribution of the state variables by a system of particles which evolves and adapts recursively as new information becomes available. In practice, large numbers of particles may be required to provide adequate approximations and for certain applications, after a sequence of updates, the particle system will often collapse to a single point. A method of monitoring the efficiency of these filters is introduced which provides a simple quantitative assessment of sample impoverishment and the authors show how to construct improved particle filters that are both structurally efficient in terms of preventing the collapse of the particle system and computationally efficient in their implementation. This is illustrated with the classic bearings-only tracking problem.},
  copyright = {\textcopyright{} IEE},
  langid = {english},
  keywords = {approximations,Bayesian bootstrap,bearings-only tracking problem,condensation algorithm,efficiency,improved particle filter,monitoring,nonlinear problems,posterior distribution,sample impoverishment,sampling importance resampling filter,state variables,updates}
}

@inproceedings{10.1007/978-3-030-00934-2_56,
  title = {High-Dimensional Bayesian Optimization of Personalized Cardiac Model Parameters via an Embedded Generative Model},
  booktitle = {Medical Image Computing and Computer Assisted Intervention \textendash{} {{MICCAI}} 2018},
  author = {Dhamala, Jwala and Ghimire, Sandesh and Sapp, John L. and Hor{\'a}{\v c}ek, B. Milan and Wang, Linwei},
  year = {2018},
  pages = {499--507},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  abstract = {The estimation of patient-specific tissue properties in the form of model parameters is important for personalized physiological models. However, these tissue properties are spatially varying across the underlying anatomical model, presenting a significance challenge of high-dimensional (HD) optimization at the presence of limited measurement data. A common solution to reduce the dimension of the parameter space is to explicitly partition the anatomical mesh, either into a fixed small number of segments or a multi-scale hierarchy. This anatomy-based reduction of parameter space presents a fundamental bottleneck to parameter estimation, resulting in solutions that are either too low in resolution to reflect tissue heterogeneity, or too high in dimension to be reliably estimated within feasible computation. In this paper, we present a novel concept that embeds a generative variational auto-encoder (VAE) into the objective function of Bayesian optimization, providing an implicit low-dimensional (LD) search space that represents the generative code of the HD spatially-varying tissue properties. In addition, the VAE-encoded knowledge about the generative code is further used to guide the exploration of the search space. The presented method is applied to estimating tissue excitability in a cardiac electrophysiological model. Synthetic and real-data experiments demonstrate its ability to improve the accuracy of parameter estimation with more than 10x gain in efficiency.}
}

@article{10.1007/s11222-018-9809-3,
  title = {{{GPU-Accelerated}} Gibbs Sampling: {{A}} Case Study of the Horseshoe Probit Model},
  author = {Terenin, Alexander and Dong, Shawfeng and Draper, David},
  year = {2019},
  month = mar,
  journal = {Statistics and Computing},
  volume = {29},
  number = {2},
  pages = {301--310},
  publisher = {{Kluwer Academic Publishers}},
  address = {{USA}},
  issue_date = {March 2019},
  keywords = {Bayesian generalized linear models,Big data,Graphics processing units,High-dimensional statistical modeling,Markov chain Monte Carlo,Parallel computing}
}

@article{10.1023/A:1020711129064,
  title = {Conditional Simulation from Highly Structured Gaussian Systems, with Application to Blocking-Mcmc for the Bayesian Analysis of Very Large Linear Models},
  author = {Wilkinson, Darren J. and Yeung, Stephen K. H.},
  year = {2002},
  month = jul,
  journal = {Statistics and Computing},
  volume = {12},
  number = {3},
  pages = {287--300},
  publisher = {{Kluwer Academic Publishers}},
  address = {{USA}},
  issue_date = {July 2002},
  keywords = {block sampling,DAG propagation,linear Bayes models,local computation,nested hierarchical random effects}
}

@inproceedings{10.1145/1102351.1102369,
  title = {Preference Learning with Gaussian Processes},
  booktitle = {Proceedings of the 22nd International Conference on Machine Learning},
  author = {Chu, Wei and Ghahramani, Zoubin},
  year = {2005},
  series = {{{ICML}} '05},
  pages = {137--144},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}}
}

@article{10.1145/2010324.1964963,
  title = {Local {{Laplacian}} Filters: {{Edge-aware}} Image Processing with a {{Laplacian}} Pyramid},
  author = {Paris, Sylvain and Hasinoff, Samuel W. and Kautz, Jan},
  year = {2011},
  month = jul,
  journal = {ACM Transactions on Graphics},
  series = {{{SIGGRAPH}}'11},
  volume = {30},
  number = {4},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  abstract = {The Laplacian pyramid is ubiquitous for decomposing images into multiple scales and is widely used for image analysis. However, because it is constructed with spatially invariant Gaussian kernels, the Laplacian pyramid is widely believed as being unable to represent edges well and as being ill-suited for edge-aware operations such as edge-preserving smoothing and tone mapping. To tackle these tasks, a wealth of alternative techniques and representations have been proposed, e.g., anisotropic diffusion, neighborhood filtering, and specialized wavelet bases. While these methods have demonstrated successful results, they come at the price of additional complexity, often accompanied by higher computational cost or the need to post-process the generated results. In this paper, we show state-of-the-art edge-aware processing using standard Laplacian pyramids. We characterize edges with a simple threshold on pixel values that allows us to differentiate large-scale edges from small-scale details. Building upon this result, we propose a set of image filters to achieve edge-preserving smoothing, detail enhancement, tone mapping, and inverse tone mapping. The advantage of our approach is its simplicity and flexibility, relying only on simple point-wise nonlinearities and small Gaussian convolutions; no optimization or post-processing is required. As we demonstrate, our method produces consistently high-quality results, without degrading edges or introducing halos.},
  articleno = {68},
  issue_date = {July 2011},
  keywords = {edge-aware image processing,image pyramids}
}

@inproceedings{10.1145/2063384.2063405,
  title = {Parallel Random Numbers: {{As}} Easy as 1, 2, 3},
  booktitle = {Proceedings of {{International Conference}} for {{High Performance Computing}}, {{Networking}}, {{Storage}} and {{Analysis}}},
  author = {Salmon, John K. and Moraes, Mark A. and Dror, Ron O. and Shaw, David E.},
  year = {2011},
  series = {{{SC}} '11},
  publisher = {{ACM Press}},
  address = {{Seattle, WA, USA}},
  abstract = {Most pseudorandom number generators (PRNGs) scale poorly to massively parallel high-performance computation because they are designed as sequentially dependent state transformations. We demonstrate that independent, keyed transformations of counters produce a large alternative class of PRNGs with excellent statistical properties (long period, no discernable structure or correlation). These counter-based PRNGs are ideally suited to modern multi-core CPUs, GPUs, clusters, and special-purpose hardware because they vectorize and parallelize well, and require little or no memory for state. We introduce several counter-based PRNGs: some based on cryptographic standards (AES, Threefish) and some completely new (Philox). All our PRNGs pass rigorous statistical tests (including TestU01's BigCrush) and produce at least 264 unique parallel streams of random numbers, each with period 2128 or more. In addition to essentially unlimited parallel scalability, our PRNGs offer excellent single-chip performance: Philox is faster than the CURAND library on a single NVIDIA GPU.},
  articleno = {16}
}

@inproceedings{10.1145/258734.258887,
  title = {Design Galleries: {{A}} General Approach to Setting Parameters for Computer Graphics and Animation},
  booktitle = {Proc. {{Annu}}. {{Conf}}. {{Comput}}. {{Graph}}. {{Interact}}. {{Techn}}.},
  author = {Marks, J. and Andalman, B. and Beardsley, P. A. and Freeman, W. and Gibson, S. and Hodgins, J. and Kang, T. and Mirtich, B. and Pfister, H. and Ruml, W. and Ryall, K. and Seims, J. and Shieber, S.},
  year = {1997},
  series = {{{SIGGRAPH}}'97},
  pages = {389--400},
  publisher = {{ACM Press/Addison-Wesley Publishing Co.}},
  address = {{USA}},
  keywords = {animation,computer-aided design,image rendering,lighting,motion synthesis,particle systems,physical modeling,visualization,volume rendering}
}

@inproceedings{10.1145/2593882.2593900,
  title = {Probabilistic Programming},
  booktitle = {Future of Software Engineering Proceedings},
  author = {Gordon, Andrew D. and Henzinger, Thomas A. and Nori, Aditya V. and Rajamani, Sriram K.},
  year = {2014},
  series = {{{FOSE}} 2014},
  pages = {167--181},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  keywords = {Machine learning,Probabilistic programming,Program analysis}
}

@inproceedings{10.1145/2737924.2737969,
  title = {Autotuning Algorithmic Choice for Input Sensitivity},
  booktitle = {Proceedings of the 36th {{ACM SIGPLAN}} Conference on Programming Language Design and Implementation},
  author = {Ding, Yufei and Ansel, Jason and Veeramachaneni, Kalyan and Shen, Xipeng and O'Reilly, Una-May and Amarasinghe, Saman},
  year = {2015},
  series = {{{PLDI}} '15},
  pages = {379--390},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  keywords = {Algorithmic Selection,Autotuning,Input Sensitivity,Variable Accuracy}
}

@inproceedings{10.1145/2751205.2751214,
  title = {{{FAST}}: {{A}} Fast Stencil Autotuning Framework Based on an Optimal-Solution Space Model},
  booktitle = {Proceedings of the 29th {{ACM}} on International Conference on Supercomputing},
  author = {Luo, Yulong and Tan, Guangming and Mo, Zeyao and Sun, Ninghui},
  year = {2015},
  series = {{{ICS}} '15},
  pages = {187--196},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  keywords = {autotuning,oss,stencil}
}

@inproceedings{10.1145/2858036.2858111,
  title = {{{SelPh}}: {{Progressive}} Learning and Support of Manual Photo Color Enhancement},
  booktitle = {Proceedings of the {{CHI}} Conference on Human Factors in Computing Systems},
  author = {Koyama, Yuki and Sakamoto, Daisuke and Igarashi, Takeo},
  year = {2016},
  series = {{{CHI}} '16},
  pages = {2520--2532},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  keywords = {design support,photo enhancement,self-reinforcement}
}

@inproceedings{10.1145/2872362.2872411,
  title = {Architecture-Adaptive Code Variant Tuning},
  booktitle = {Proceedings of the Twenty-First International Conference on Architectural Support for Programming Languages and Operating Systems},
  author = {Muralidharan, Saurav and Roy, Amit and Hall, Mary and Garland, Michael and Rai, Piyush},
  year = {2016},
  series = {{{ASPLOS}} '16},
  pages = {325--338},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  keywords = {autotuning,cross-architectural tuning,device feature selection,input-adaptive,multi-task learning}
}

@inproceedings{10.1145/2939672.2939785,
  title = {{{XGBoost}}: {{A}} Scalable Tree Boosting System},
  booktitle = {Proceedings of the 22nd {{ACM SIGKDD}} International Conference on Knowledge Discovery and Data Mining},
  author = {Chen, Tianqi and Guestrin, Carlos},
  year = {2016},
  series = {{{KDD}} '16},
  pages = {785--794},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  keywords = {large-scale machine learning}
}

@article{10.1145/3072959.3073598,
  title = {Sequential Line Search for Efficient Visual Design Optimization by Crowds},
  author = {Koyama, Yuki and Sato, Issei and Sakamoto, Daisuke and Igarashi, Takeo},
  year = {2017},
  month = jul,
  journal = {ACM Transactions on Graphics},
  volume = {36},
  number = {4},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  abstract = {Parameter tweaking is a common task in various design scenarios. For example, in color enhancement of photographs, designers tweak multiple parameters such as "brightness" and "contrast" to obtain the best visual impression. Adjusting one parameter is easy; however, if there are multiple correlated parameters, the task becomes much more complex, requiring many trials and a large cognitive load. To address this problem, we present a novel extension of Bayesian optimization techniques, where the system decomposes the entire parameter tweaking task into a sequence of one-dimensional line search queries that are easy for human to perform by manipulating a single slider. In addition, we present a novel concept called crowd-powered visual design optimizer, which queries crowd workers, and provide a working implementation of this concept. Our single-slider manipulation microtask design for crowdsourcing accelerates the convergence of the optimization relative to existing comparison-based microtask designs. We applied our framework to two different design domains: photo color enhancement and material BRDF design, and thereby showed its applicability to various design domains.},
  articleno = {48},
  issue_date = {July 2017},
  keywords = {bayesian optimization,computational design,crowdsourcing,human computation}
}

@inproceedings{10.1145/3205289.3205321,
  title = {Bootstrapping Parameter Space Exploration for Fast Tuning},
  booktitle = {Proceedings of the 2018 International Conference on Supercomputing},
  author = {Thiagarajan, Jayaraman J. and Jain, Nikhil and Anirudh, Rushil and Gimenez, Alfredo and Sridhar, Rahul and Marathe, Aniruddha and Wang, Tao and Emani, Murali and Bhatele, Abhinav and Gamblin, Todd},
  year = {2018},
  series = {{{ICS}} '18},
  pages = {385--395},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  keywords = {autotuning,performance,sampling,semi-supervised learning}
}

@article{10.1145/3218823,
  title = {Design and Implementation of Adaptive {{SpMV}} Library for Multicore and Many-Core Architecture},
  author = {Tan, Guangming and Liu, Junhong and Li, Jiajia},
  year = {2018},
  month = aug,
  journal = {ACM Trans. Math. Softw.},
  volume = {44},
  number = {4},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  articleno = {46},
  issue_date = {August 2018},
  keywords = {auto-tuning,machine learning,multicore,Sparse matrix vector multiplication}
}

@inproceedings{10.1145/3243176.3243198,
  title = {Log({{Graph}}): {{A}} near-Optimal High-Performance Graph Representation},
  booktitle = {Proceedings of the 27th International Conference on Parallel Architectures and Compilation Techniques},
  author = {Besta, Maciej and Stanojevic, Dimitri and Zivic, Tijana and Singh, Jagpreet and Hoerold, Maurice and Hoefler, Torsten},
  year = {2018},
  series = {{{PACT}} '18},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  abstract = {Today's graphs used in domains such as machine learning or social network analysis may contain hundreds of billions of edges. Yet, they are not necessarily stored efficiently, and standard graph representations such as adjacency lists waste a significant number of bits while graph compression schemes such as WebGraph often require time-consuming decompression. To address this, we propose Log(Graph): a graph representation that combines high compression ratios with very low-overhead decompression to enable cheaper and faster graph processing. The key idea is to encode a graph so that the parts of the representation approach or match the respective storage lower bounds. We call our approach "graph logarithmization" because these bounds are usually logarithmic. Our high-performance Log(Graph) implementation based on modern bitwise operations and state-of-the-art succinct data structures achieves high compression ratios as well as performance. For example, compared to the tuned Graph Algorithm Processing Benchmark Suite (GAPBS), it reduces graph sizes by 20-35\% while matching GAPBS' performance or even delivering speedups due to reducing amounts of transferred data. It approaches the compression ratio of the established WebGraph compression library while enabling speedups of up to more than 2\texttimes. Log(Graph) can improve the design of various graph processing engines or libraries on single NUMA nodes as well as distributed-memory systems.},
  articleno = {7},
  keywords = {graph compression,graph layout,graph representation,ILP,parallel graph algorithms,succinct data structures}
}

@inproceedings{10.1145/3295500.3356181,
  title = {Red-Blue Pebbling Revisited: {{Near}} Optimal Parallel Matrix-Matrix Multiplication},
  booktitle = {Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis},
  author = {Kwasniewski, Grzegorz and Kabi{\'c}, Marko and Besta, Maciej and VandeVondele, Joost and Solc{\`a}, Raffaele and Hoefler, Torsten},
  year = {2019},
  series = {{{SC}} '19},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  abstract = {We propose COSMA: a parallel matrix-matrix multiplication algorithm that is near communication-optimal for all combinations of matrix dimensions, processor counts, and memory sizes. The key idea behind COSMA is to derive an optimal (up to a factor of 0.03\% for 10MB of fast memory) sequential schedule and then parallelize it, preserving I/O optimality. To achieve this, we use the red-blue pebble game to precisely model MMM dependencies and derive a constructive and tight sequential and parallel I/O lower bound proofs. Compared to 2D or 3D algorithms, which fix processor decomposition upfront and then map it to the matrix dimensions, it reduces communication volume by up to {$\surd$} times. COSMA outperforms the established ScaLAPACK, CARMA, and CTF algorithms in all scenarios up to 12.8x (2.2x on average), achieving up to 88\% of Piz Daint's peak performance. Our work does not require any hand tuning and is maintained as an open source implementation.},
  articleno = {24}
}

@article{10.1145/3386569.3392409,
  title = {Human-in-the-Loop Differential Subspace Search in High-Dimensional Latent Space},
  author = {Chiu, Chia-Hsing and Koyama, Yuki and Lai, Yu-Chi and Igarashi, Takeo and Yue, Yonghao},
  year = {2020},
  month = jul,
  journal = {ACM Transactions on Graphics},
  series = {{{SIGGRAPH}}'20},
  volume = {39},
  number = {4},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  abstract = {Generative models based on deep neural networks often have a high-dimensional latent space, ranging sometimes to a few hundred dimensions or even higher, which typically makes them hard for a user to explore directly. We propose differential subspace search to allow efficient iterative user exploration in such a space, without relying on domain- or data-specific assumptions. We develop a general framework to extract low-dimensional subspaces based on a local differential analysis of the generative model, such that a small change in such a subspace would provide enough change in the resulting data. We do so by applying singular value decomposition to the Jacobian of the generative model and forming a subspace with the desired dimensionality spanned by a given number of singular vectors stochastically selected on the basis of their singular values, to maintain ergodicity. We use our framework to present 1D subspaces to the user via a 1D slider interface. Starting from an initial location, the user finds a new candidate in the presented 1D subspace, which is in turn updated at the new candidate location. This process is repeated until no further improvement can be made. Numerical simulations show that our method can better optimize synthetic black-box objective functions than the alternatives that we tested. Furthermore, we conducted a user study using complex generative models and the results show that our method enables more efficient exploration of high-dimensional latent spaces than the alternatives.},
  articleno = {85},
  issue_date = {July 2020},
  keywords = {dimensionality reduction,generative models,human-in-the-loop optimization}
}

@article{10.1145/3414685.3417793,
  title = {Path Tracing Estimators for Refractive Radiative Transfer},
  author = {Pediredla, Adithya and Chalmiani, Yasin Karimi and Scopelliti, Matteo Giuseppe and Chamanzar, Maysamreza and Narasimhan, Srinivasa and Gkioulekas, Ioannis},
  year = {2020},
  month = nov,
  journal = {ACM Trans. Graph.},
  volume = {39},
  number = {6},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  abstract = {Rendering radiative transfer through media with a heterogeneous refractive index is challenging because the continuous refractive index variations result in light traveling along curved paths. Existing algorithms are based on photon mapping techniques, and thus are biased and result in strong artifacts. On the other hand, existing unbiased methods such as path tracing and bidirectional path tracing cannot be used in their current form to simulate media with a heterogeneous refractive index. We change this state of affairs by deriving unbiased path tracing estimators for this problem. Starting from the refractive radiative transfer equation (RRTE), we derive a path-integral formulation, which we use to generalize path tracing with next-event estimation and bidirectional path tracing to the heterogeneous refractive index setting. We then develop an optimization approach based on fast analytic derivative computations to produce the point-to-point connections required by these path tracing algorithms. We propose several acceleration techniques to handle complex scenes (surfaces and volumes) that include participating media with heterogeneous refractive fields. We use our algorithms to simulate a variety of scenes combining heterogeneous refraction and scattering, as well as tissue imaging techniques based on ultrasonic virtual waveguides and lenses. Our algorithms and publicly-available implementation can be used to characterize imaging systems such as refractive index microscopy, schlieren imaging, and acousto-optic imaging, and can facilitate the development of inverse rendering techniques for related applications.},
  articleno = {241},
  issue_date = {December 2020},
  keywords = {eikonal equation,gradient index,refractive radiative transfer},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\X3LZM3QI\\Pediredla et al. - 2020 - Path tracing estimators for refractive radiative t.pdf}
}

@article{10.1145/3478513.3480495,
  title = {Ships, Splashes, and Waves on a Vast Ocean},
  author = {Huang, Libo and Qu, Ziyin and Tan, Xun and Zhang, Xinxin and Michels, Dominik L. and Jiang, Chenfanfu},
  year = {2021},
  month = dec,
  journal = {ACM Trans. Graph.},
  volume = {40},
  number = {6},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  abstract = {The simulation of large open water surface is challenging using a uniform volumetric discretization of the Navier-Stokes equations. Simulating water splashes near moving objects, which height field methods for water waves cannot capture, necessitates high resolutions. Such simulations can be carried out using the Fluid-Implicit-Particle (FLIP) method. However, the FLIP method is not efficient for the long-lasting water waves that propagate to long distances, which require sufficient depth for a correct dispersion relationship. This paper presents a new method to tackle this dilemma through an efficient hybridization of volumetric and surface-based advection-projection discretizations. We design a hybrid time-stepping algorithm that combines a FLIP domain and an adaptively remeshed Boundary Element Method (BEM) domain for the incompressible Euler equations. The resulting framework captures the detailed water splashes near moving objects with the FLIP method, and produces convincing water waves with correct dispersion relationships at modest additional costs.},
  articleno = {203},
  issue_date = {December 2021},
  keywords = {boundary element method (BEM),fluid dynamics,fluid-implicit-particle (FLIP) method,hybrid discretization,multiscale modeling}
}

@article{10.1214/06-BA127,
  title = {Nested Sampling for General {{Bayesian}} Computation},
  author = {Skilling, John},
  year = {2006},
  journal = {Bayesian Analysis},
  volume = {1},
  number = {4},
  pages = {833--859},
  publisher = {{International Society for Bayesian Analysis}},
  keywords = {algorithm,annealing,Bayesian computation,evidence,marginal likelihood,Model selection,nest,phase change}
}

@article{10.1214/17-STS611,
  title = {Importance Sampling: {{Intrinsic}} Dimension and Computational Cost},
  author = {Agapiou, S. and Papaspiliopoulos, O. and {Sanz-Alonso}, D. and Stuart, A. M.},
  year = {2017},
  journal = {Statistical Science},
  volume = {32},
  number = {3},
  pages = {405--431},
  publisher = {{Institute of Mathematical Statistics}},
  keywords = {Absolute continuity,Filtering,importance sampling,Inverse problems,notions of dimension,small noise}
}

@article{10.2307/24308995,
  title = {Adaptively Scaling the {{Metropolis}} Algorithm Using Expected Squared Jumped Distance},
  author = {Pasarica, Cristian and Gelman, Andrew},
  year = {2010},
  journal = {Statistica Sinica},
  volume = {20},
  number = {1},
  pages = {343--364},
  publisher = {{Institute of Statistical Science, Academia Sinica}},
  abstract = {A good choice of the proposal distribution is crucial for the rapid convergence of the Metropolis algorithm. In this paper, given a family of parametric Markovian kernels, we develop an adaptive algorithm for selecting the best kernel that maximizes the expected squared jumped distance, an objective function that characterizes the Markov chain. We demonstrate the effectiveness of our method in several examples.}
}

@article{10.2307/25442663,
  title = {Variance Bounding {{Markov}} Chains},
  author = {Roberts, Gareth O. and Rosenthal, Jeffrey S.},
  year = {2008},
  journal = {The Annals of Applied Probability},
  volume = {18},
  number = {3},
  pages = {1201--1214},
  publisher = {{Institute of Mathematical Statistics}},
  abstract = {We introduce a new property of Markov chains, called variance bounding. We prove that, for reversible chains at least, variance bounding is weaker than, but closely related to, geometric ergodicity. Furthermore, variance bounding is equivalent to the existence of usual central limit theorems for all L{$^2$} functionals. Also, variance bounding (unlike geometric ergodicity) is preserved under the Peskun order. We close with some applications to Metropolis-Hastings algorithms.}
}

@article{10.2307/25662473,
  title = {{{DOES WASTE RECYCLING REALLY IMPROVE THE MULTI-PROPOSAL METROPOLIS}}\textendash{{HASTINGS ALGORITHM}}? {{AN ANALYSIS BASED ON CONTROL VARIATES}}},
  author = {Delmas, Jean-Fran{\c c}ois and Jourdain, Benjamin},
  year = {2009},
  journal = {Journal of Applied Probability},
  volume = {46},
  number = {4},
  pages = {938--959},
  publisher = {{Applied Probability Trust}},
  abstract = {The waste-recycling Monte Carlo (WRMC) algorithm introduced by physicists is a modification of the (multi-proposal) Metropolis\textendash Hastings algorithm, which makes use of all the proposals in the empirical mean, whereas the standard (multi-proposal) Metropolis\textendash Hastings algorithm uses only the accepted proposals. In this paper we extend the WRMC algorithm to a general control variate technique and exhibit the optimal choice of the control variate in terms of the asymptotic variance. We also give an example which shows that, in contradiction to the intuition of physicists, the WRMC algorithm can have an asymptotic variance larger than that of the Metropolis\textendash Hastings algorithm. However, in the particular case of the Metropolis\textendash Hastings algorithm called the Boltzmann algorithm, we prove that the WRMC algorithm is asymptotically better than the Metropolis\textendash Hastings algorithm. This last property is also true for the multiproposal Metropolis\textendash Hastings algorithm. In this last framework we consider a linear parametric generalization of WRMC, and we propose an estimator of the explicit optimal parameter using the proposals.}
}

@article{10.2307/27595854,
  title = {Coupling and Ergodicity of Adaptive Markov Chain Monte Carlo Algorithms},
  author = {Roberts, Gareth O. and Rosenthal, Jeffrey S.},
  year = {2007},
  journal = {Journal of Applied Probability},
  volume = {44},
  number = {2},
  pages = {458--475},
  publisher = {{Applied Probability Trust}},
  abstract = {We consider basic ergodicity properties of adaptive Markov chain Monte Carlo algorithms under minimal assumptions, using coupling constructions. We prove convergence in distribution and a weak law of large numbers. We also give counterexamples to demonstrate that the assumptions we make are not redundant.}
}

@article{10.2307/27821441,
  title = {Preference Uncertainty, Preference Learning, and Paired Comparison Experiments},
  author = {Kingsley, David C. and Brown, Thomas C.},
  year = {2010},
  journal = {Land Economics},
  volume = {86},
  number = {3},
  pages = {530--544},
  publisher = {{[Board of Regents of the University of Wisconsin System, University of Wisconsin Press]}},
  abstract = {Results from paired comparison experiments suggest that as respondents progress through a sequence of binary choices they become more consistent, apparently fine-tuning their preferences. Consistency may be indicated by the variance of the estimated valuation distribution measured by the error term in the random utility model. A significant reduction in the variance is shown to be consistent with a model of preference uncertainty allowing for preference learning. Respondents become more adept at discriminating among items as they gain experience considering and comparing them, suggesting that methods allowing for such experience may obtain more well founded values.}
}

@article{10.2307/41058939,
  title = {The Random Walk {{Metropolis}}: {{Linking}} Theory and Practice through a Case Study},
  author = {Sherlock, Chris and Fearnhead, Paul and Roberts, Gareth O.},
  year = {2010},
  journal = {Statistical Science},
  volume = {25},
  number = {2},
  pages = {172--190},
  publisher = {{Institute of Mathematical Statistics}},
  abstract = {The random walk Metropolis (RWM) is one of the most common Markov chain Monte Carlo algorithms in practical use today. Its theoretical properties have been extensively explored for certain classes of target, and a number of results with important practical implications have been derived. This article draws together a selection of new and existing key results and concepts and describes their implications. The impact of each new idea on algorithm efficiency is demonstrated for the practical example of the Markov modulated Poisson process (MMPP). A reparameterization of the MMPP which leads to a highly efficient RWM-within-Gibbs algorithm in certain circumstances is also presented.}
}

@inproceedings{10.5555/2074022.2074067,
  title = {Expectation Propagation for Approximate Bayesian Inference},
  booktitle = {Proceedings of the Conference on Uncertainty in Artificial Intelligence},
  author = {Minka, Thomas P.},
  year = {2001},
  pages = {362--369},
  publisher = {{Morgan Kaufmann Publishers Inc.}},
  address = {{San Francisco, CA, USA}},
  abstract = {This paper presents a new deterministic approximation technique in Bayesian networks. This method, "Expectation Propagation," unifies two previous techniques: assumed-density filtering, an extension of the Kalman filter, and loopy belief propagation, an extension of belief propagation in Bayesian networks. Loopy belief propagation, because it propagates exact belief states, is useful for a limited class of belief networks, such as those which are purely discrete. Expectation Propagation approximates the belief states by only retaining expectations, such as mean and varitmce, and iterates until these expectations are consistent throughout the network. This makes it applicable to hybrid networks with discrete and continuous nodes. Experiments with Gaussian mixture models show Expectation Propagation to be donvincingly better than methods with similar computational cost: Laplace's method, variational Bayes, and Monte Carlo. Expectation Propagation also provides an efficient algorithm for training Bayes point machine classifiers.}
}

@article{10.5555/235610.235641,
  title = {Exact Sampling with Coupled Markov Chains and Applications to Statistical Mechanics},
  author = {Propp, James Gary and Wilson, David Bruce},
  year = {1996},
  month = aug,
  journal = {Random Structures and Algorithms},
  volume = {9},
  number = {1\textendash 2},
  pages = {223--252},
  publisher = {{John Wiley \& Sons, Inc.}},
  address = {{USA}},
  issue_date = {Aug./Sept. 1996}
}

@inproceedings{10.5555/2540128.2540383,
  title = {Bayesian Optimization in High Dimensions via Random Embeddings},
  booktitle = {Proceedings of the International Joint Conference on Artificial Intelligence},
  author = {Wang, Ziyu and Zoghi, Masrour and Hutter, Frank and Matheson, David and De Freitas, Nando},
  year = {2013},
  series = {{{IJCAI}} '13},
  pages = {1778--1784},
  publisher = {{AAAI Press}},
  address = {{Beijing, China}},
  abstract = {Bayesian optimization techniques have been successfully applied to robotics, planning, sensor placement, recommendation, advertising, intelligent user interfaces and automatic algorithm configuration. Despite these successes, the approach is restricted to problems of moderate dimension, and several workshops on Bayesian optimization have identified its scaling to high dimensions as one of the holy grails of the field. In this paper, we introduce a novel random embedding idea to attack this problem. The resulting Random EMbedding Bayesian Optimization (REMBO) algorithm is very simple and applies to domains with both categorical and continuous variables. The experiments demonstrate that REMBO can effectively solve high-dimensional problems, including automatic parameter configuration of a popular mixed integer linear programming solver.}
}

@article{10.5555/2627435.2638586,
  title = {The No-u-Turn Sampler: {{Adaptively}} Setting Path Lengths in Hamiltonian Monte Carlo},
  author = {Homan, Matthew D. and Gelman, Andrew},
  year = {2014},
  month = jan,
  journal = {Journal of Machine Learning Research},
  volume = {15},
  number = {1},
  pages = {1593--1623},
  publisher = {{JMLR.org}},
  issue_date = {January 2014},
  keywords = {adaptive Monte Carlo,Bayesian inference,dual averaging,Hamiltonian Monte Carlo,Markov chain Monte Carlo}
}

@inproceedings{10.5555/3020751.3020755,
  title = {Accelerating {{MCMC}} via Parallel Predictive Prefetching},
  booktitle = {Proc. 30th {{Conf}}. {{Uncertainty Artif}}. {{Intell}}.},
  author = {Angelino, Elaine and Kohler, Eddie and Waterland, Amos and Seltzer, Margo and Adams, Ryan P.},
  year = {2014},
  series = {{{UAI}}'14},
  pages = {22--31},
  publisher = {{AUAI Press}},
  address = {{Arlington, Virginia, USA}},
  abstract = {Parallel predictive prefetching is a new framework for accelerating a large class of widely-used Markov chain Monte Carlo (MCMC) algorithms. It speculatively evaluates many potential steps of an MCMC chain in parallel while exploiting fast, iterative approximations to the target density. This can accelerate sampling from target distributions in Bayesian inference problems. Our approach takes advantage of whatever parallel resources are available, but produces results exactly equivalent to standard serial execution. In the initial burn-in phase of chain evaluation, we achieve speedup close to linear in the number of available cores.}
}

@inproceedings{10.5555/3020751.3020816,
  title = {Asymptotically Exact, Embarrassingly Parallel {{MCMC}}},
  booktitle = {Proceedings of the {{International Conference}} on {{Uncertainty}} in {{Artifical Intelligence}}},
  author = {Neiswanger, Willie and Wang, Chong and Xing, Eric P.},
  year = {2014},
  series = {{{UAI}}'14},
  pages = {623--632},
  publisher = {{AUAI Press}},
  address = {{Arlington, Virginia, USA}}
}

@inproceedings{10.5555/3020948.3020982,
  title = {Faster Stochastic Variational Inference Using Proximal-Gradient Methods with General Divergence Functions},
  booktitle = {Proceedings of the Thirty-Second Conference on Uncertainty in Artificial Intelligence},
  author = {Khan, Mohammad Emtiyaz and Babanezhad, Reza and Lin, Wu and Schmidt, Mark and Sugiyama, Masashi},
  year = {2016},
  series = {{{UAI}}'16},
  pages = {319--328},
  publisher = {{AUAI Press}},
  address = {{Arlington, Virginia, USA}},
  abstract = {Several recent works have explored stochastic gradient methods for variational inference that exploit the geometry of the variational-parameter space. However, the theoretical properties of these methods are not well-understood and these methods typically only apply to conditionally-conjugate models. We present a new stochastic method for variational inference which exploits the geometry of the variational-parameter space and also yields simple closed-form updates even for non-conjugate models. We also give a convergence-rate analysis of our method and many other previous methods which exploit the geometry of the space. Our analysis generalizes existing convergence results for stochastic mirror-descent on non-convex objectives by using a more general class of divergence functions. Beyond giving a theoretical justification for a variety of recent methods, our experiments show that new algorithms derived in this framework lead to state of the art results on a variety of problems. Further, due to its generality, we expect that our theoretical analysis could also apply to other applications.}
}

@inproceedings{10.5555/3042817.3043056,
  title = {Gaussian Process Kernels for Pattern Discovery and Extrapolation},
  booktitle = {Proceedings of the 30th International Conference on International Conference on Machine Learning - Volume 28},
  author = {Wilson, Andrew Gordon and Adams, Ryan Prescott},
  year = {2013},
  series = {{{ICML}}'13},
  pages = {III--1067--III--1075},
  publisher = {{JMLR.org}},
  address = {{Atlanta, GA, USA}}
}

@inproceedings{10.5555/3061507.3061534,
  title = {Monte-{{Carlo}} Ray-Tracing for Realistic Interactive Ultrasound Simulation},
  booktitle = {Proceedings of the Eurographics Workshop on Visual Computing for Biology and Medicine},
  author = {Mattausch, O. and Goksel, O.},
  year = {2016},
  series = {{{VCBM}} '16},
  pages = {173--181},
  publisher = {{Eurographics Association}},
  address = {{Goslar, DEU}},
  abstract = {Ray-based simulations have been shown to generate impressively realistic ultrasound images in interactive frame rates. Recent efforts used GPU-based surface ray-tracing to simulate complex ultrasound interactions such as multiple reflections and refractions. These methods are restricted to perfectly specular reflections (i.e., following only a single reflective/refractive ray), whereas real tissue exhibits roughness of varying degree at tissue interfaces, causing partly diffuse reflections and refractions. Such surface interactions are significantly more complex and can in general not be handled by such deterministic ray-tracing approaches. However, they can be efficiently computed by Monte-Carlo sampling techniques, where many ray paths are generated with respect to a probability distribution. In this paper we introduce Monte-Carlo ray-tracing for ultrasound. This enables the realistic simulation of ultrasound interactions such as soft shadows and fuzzy reflections. We discuss how to properly weight the contribution of each ray path in order to simulate the behavior of a beamformed ultrasound signal. Tracing many individual rays per transducer element is easily parallelizable on modern GPUs, as opposed to previous approaches based on recursive binary ray-tracing.}
}

@article{10.5555/3122009.3208005,
  title = {Robust and Scalable Bayes via a Median of Subset Posterior Measures},
  author = {Minsker, Stanislav and Srivastava, Sanvesh and Lin, Lizhen and Dunson, David B.},
  year = {2017},
  month = jan,
  journal = {Journal of Machine Learning Research},
  volume = {18},
  number = {1},
  pages = {4488--4527},
  publisher = {{JMLR.org}},
  issue_date = {January 2017},
  keywords = {big data,distributed computing,geometric median,parallel MCMC,Wasserstein distance}
}

@inproceedings{10.5555/3305381.3305515,
  title = {Measuring Sample Quality with Kernels},
  booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
  author = {Gorham, Jackson and Mackey, Lester},
  year = {2017},
  series = {{{ICML}}'17},
  pages = {1292--1301},
  publisher = {{JMLR.org}},
  address = {{Sydney, NSW, Australia}}
}

@article{10.5555/944919.944937,
  title = {Latent Dirichlet Allocation},
  author = {Blei, David M. and Ng, Andrew Y. and Jordan, Michael I.},
  year = {2003},
  month = mar,
  journal = {Journal of Machine Learning Research},
  volume = {3},
  number = {null},
  pages = {993--1022},
  publisher = {{JMLR.org}},
  issue_date = {3/1/2003}
}

@article{41849,
  title = {Bayes and Big Data: {{The}} Consensus Monte Carlo Algorithm},
  author = {Scott, Steven L. and Blocker, Alexander W. and Bonassi, Fernando V. and Chipman, Hugh A. and George, Edward I. and McCulloch, Robert E.},
  year = {2016},
  journal = {International Journal of Management Science and Engineering Management},
  volume = {11},
  pages = {78--88}
}

@inproceedings{6713531,
  title = {Modeling of Motion Artifacts in Contactless Heart Rate Measurements},
  booktitle = {Computing in Cardiology},
  author = {Wartzek, T and Br{\"u}ser, C and Schlebusch, T and Brendle, C and Santos, S and Kerekes, A and {Gerlach-Hahn}, K and Weyer, S and Lunze, K and Hoog Antink, C and Leonhardt, S},
  year = {2013},
  pages = {931--934},
  publisher = {{IEEE}},
  address = {{Zaragoza, Spain}}
}

@article{abd-elmoniem_realtime_2002,
  title = {Real-Time Speckle Reduction and Coherence Enhancement in Ultrasound Imaging via Nonlinear Anisotropic Diffusion},
  author = {{Abd-Elmoniem}, K.Z. and Youssef, A.-B.M. and Kadah, Y.M.},
  year = {2002},
  month = sep,
  journal = {IEEE Transactions on Biomedical Engineering},
  volume = {49},
  number = {9},
  pages = {997--1014},
  langid = {english},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\J7NM8HBL\\Abd-Elmoniem et al. - 2002 - Real-time speckle reduction and coherence enhancem.pdf}
}

@article{abdi_automatic_2017,
  title = {Automatic Quality Assessment of Echocardiograms Using Convolutional Neural Networks: Feasibility on the Apical Four-Chamber View},
  shorttitle = {Automatic {{Quality Assessment}} of {{Echocardiograms Using Convolutional Neural Networks}}},
  author = {Abdi, Amir H. and Luong, Christina and Tsang, Teresa and Allan, Gregory and Nouranian, Saman and Jue, John and Hawley, Dale and Fleming, Sarah and Gin, Ken and Swift, Jody and Rohling, Robert and Abolmaesumi, Purang},
  year = {2017},
  month = jun,
  journal = {IEEE Transactions on Medical Imaging},
  volume = {36},
  number = {6},
  pages = {1221--1230}
}

@article{adedinsewo_artificial_2020,
  title = {Artificial Intelligence-Enabled {{ECG}} Algorithm to Identify Patients with Left Ventricular Systolic Dysfunction Presenting to the Emergency Department with Dyspnea},
  author = {Adedinsewo, Demilade and Carter, Rickey E. and Attia, Zachi and Johnson, Patrick and Kashou, Anthony H. and Dugan, Jennifer L. and Albus, Michael and Sheele, Johnathan M. and Bellolio, Fernanda and Friedman, Paul A. and {Lopez-Jimenez}, Francisco and Noseworthy, Peter A.},
  year = {2020},
  month = aug,
  journal = {Circulation: Arrhythmia and Electrophysiology},
  volume = {13},
  number = {8},
  abstract = {Background:               Identification of systolic heart failure among patients presenting to the emergency department (ED) with acute dyspnea is challenging. The reasons for dyspnea are often multifactorial. A focused physical evaluation and diagnostic testing can lack sensitivity and specificity. The objective of this study was to assess the accuracy of an artificial intelligence-enabled ECG to identify patients presenting with dyspnea who have left ventricular systolic dysfunction (LVSD).                                         Methods:               We retrospectively applied a validated artificial intelligence-enabled ECG algorithm for the identification of LVSD (defined as LV ejection fraction {$\leq$}35\%) to a cohort of patients aged {$\geq$}18 years who were evaluated in the ED at a Mayo Clinic site with dyspnea. Patients were included if they had at least one standard 12-lead ECG acquired on the date of the ED visit and an echocardiogram performed within 30 days of presentation. Patients with prior LVSD were excluded. We assessed the model performance using area under the receiver operating characteristic curve, accuracy, sensitivity, and specificity.                                         Results:               A total of 1606 patients were included. Median time from ECG to echocardiogram was 1 day (Q1: 1, Q3: 2). The artificial intelligence-enabled ECG algorithm identified LVSD with an area under the receiver operating characteristic curve of 0.89 (95\% CI, 0.86\textendash 0.91) and accuracy of 85.9\%. Sensitivity, specificity, negative predictive value, and positive predictive value were 74\%, 87\%, 97\%, and 40\%, respectively. To identify an ejection fraction {$<$}50\%, the area under the receiver operating characteristic curve, accuracy, sensitivity, and specificity were 0.85 (95\% CI, 0.83\textendash 0.88), 86\%, 63\%, and 91\%, respectively. NT-proBNP (N-terminal pro-B-type natriuretic peptide) alone at a cutoff of {$>$}800 identified LVSD with an area under the receiver operating characteristic curve of 0.80 (95\% CI, 0.76\textendash 0.84).                                         Conclusions:               The ECG is an inexpensive, ubiquitous, painless test which can be quickly obtained in the ED. It effectively identifies LVSD in selected patients presenting to the ED with dyspnea when analyzed with artificial intelligence and outperforms NT-proBNP.                                         Graphic Abstract:                                A                 graphic abstract                 is available for this article.},
  langid = {english},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\8A84NK47\\Adedinsewo et al. - 2020 - Artificial Intelligence-Enabled ECG Algorithm to I.pdf}
}

@inproceedings{adve_influence_1993,
  title = {The Influence of Random Delays on Parallel Execution Times},
  booktitle = {Proc. 1993 {{ACM SIGMETRICS Conf}}. {{Meas}}. {{Model}}. {{Comp}}. {{Syst}}.},
  author = {Adve, Vikram S. and Vernon, Mary K.},
  year = {1993},
  series = {{{SIGMETRICS}}'93},
  pages = {61--73},
  publisher = {{ACM}},
  address = {{New York, NY, USA}}
}

@inproceedings{agakov_auxiliary_2004,
  title = {An Auxiliary Variational Method},
  booktitle = {Neural {{Information Processing}}},
  author = {Agakov, Felix V. and Barber, David},
  year = {2004},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {561--566},
  publisher = {{Springer}},
  address = {{Berlin, Heidelberg}},
  abstract = {An attractive feature of variational methods used in the context of approximate inference in undirected graphical models is a rigorous lower bound on the normalization constants. Here we explore the idea of using augmented variable spaces to improve on the standard mean-field bounds. Our approach forms a more powerful class of approximations than any structured mean field technique. Moreover, the existing variational mixture models may be seen as computationally expensive special cases of our method. A byproduct of our work is an efficient way to calculate a set of mixture coefficients for any set of tractable distributions that principally improves on a flat combination.},
  langid = {english},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\K3MWFJ7I\\Agakov and Barber - 2004 - An Auxiliary Variational Method.pdf}
}

@inproceedings{agrawal_advances_2020,
  title = {Advances in Black-Box {{VI}}: {{Normalizing}} Flows, Importance Weighting, and Optimization},
  shorttitle = {Advances in Black-Box {{VI}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Agrawal, Abhinav and Sheldon, Daniel R and Domke, Justin},
  year = {2020},
  volume = {33},
  pages = {17358--17369},
  publisher = {{Curran Associates, Inc.}},
  abstract = {Recent research has seen several advances relevant to black-box VI, but the current state of automatic posterior inference is unclear. One such advance is the use of normalizing flows to define flexible posterior densities for deep latent variable models. Another direction is the integration of Monte-Carlo methods to serve two purposes; first, to obtain tighter variational objectives for optimization, and second, to define enriched variational families through sampling. However, both flows and variational Monte-Carlo methods remain relatively unexplored for black-box VI. Moreover, on a pragmatic front, there are several optimization considerations like step-size scheme, parameter initialization, and choice of gradient estimators, for which there are no clear guidance in the existing literature. In this paper, we postulate that black-box VI is best addressed through a careful combination of numerous algorithmic components. We evaluate components relating to optimization, flows, and Monte-Carlo methods on a benchmark of 30 models from the Stan model library. The combination of these algorithmic components significantly advances the state-of-the-art "out of the box" variational inference.},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\2PXTCBFL\\Agrawal et al. - 2020 - Advances in Black-Box VI Normalizing Flows, Impor.pdf}
}

@article{ai-jun_bayesian_2010,
  title = {Bayesian Variable Selection for Disease Classification Using Gene Expression Data},
  author = {{Ai-Jun}, Yang and {Xin-Yuan}, Song},
  year = {2010},
  month = jan,
  journal = {Bioinformatics},
  volume = {26},
  number = {2},
  pages = {215--222},
  langid = {english},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\IXZMVMAF\\Ai-Jun and Xin-Yuan - 2010 - Bayesian variable selection for disease classifica.pdf}
}

@book{ainslie_principles_2010,
  title = {Principles of Sonar Performance Modelling},
  author = {Ainslie, Michael Anthony},
  year = {2010},
  series = {Springer {{Praxis}} Books in Geophysical Sciences},
  publisher = {{Springer Published in association with Praxis Pub}},
  address = {{Heidelberg [Germany] London New York Chichester, UK}},
  langid = {english},
  lccn = {551.465}
}

@article{aja-fernandez_estimation_2006,
  title = {On the Estimation of the Coefficient of Variation for Anisotropic Diffusion Speckle Filtering},
  author = {{Aja-Fernandez}, S. and {Alberola-Lopez}, C.},
  year = {2006},
  month = sep,
  journal = {IEEE Transactions on Image Processing},
  volume = {15},
  number = {9},
  pages = {2694--2701}
}

@article{akaike_new_1974,
  title = {A New Look at the Statistical Model Identification},
  author = {Akaike, H.},
  year = {1974},
  month = dec,
  journal = {IEEE Transactions on Automatic Control},
  volume = {19},
  number = {6},
  pages = {716--723},
  langid = {english},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\CG299KIG\\Akaike - 1974 - A new look at the statistical model identification.pdf}
}

@article{akyildiz_convergence_2021,
  title = {Convergence Rates for Optimised Adaptive Importance Samplers},
  author = {Akyildiz, {\"O}mer Deniz and M{\'i}guez, Joaqu{\'i}n},
  year = {2021},
  month = mar,
  journal = {Statistics and Computing},
  volume = {31},
  number = {2},
  pages = {12},
  abstract = {Abstract                            Adaptive importance samplers are adaptive Monte Carlo algorithms to estimate expectations with respect to some target distribution which               adapt               themselves to obtain better estimators over a sequence of iterations. Although it is straightforward to show that they have the same                                                   \$\$\textbackslash mathcal \{O\}(1/\textbackslash sqrt\{N\})\$\$                                                               O                       (                       1                       /                                                N                                              )                                                                                       convergence rate as standard importance samplers, where               N               is the number of Monte Carlo samples, the behaviour of adaptive importance samplers over the number of iterations has been left relatively unexplored. In this work, we investigate an adaptation strategy based on convex optimisation which leads to a class of adaptive importance samplers termed               optimised adaptive importance samplers               (OAIS). These samplers rely on the iterative minimisation of the                                                   \$\$\textbackslash chi \^2\$\$                                                               {$\chi$}                       2                                                                                       -divergence between an exponential family proposal and the target. The analysed algorithms are closely related to the class of adaptive importance samplers which minimise the variance of the weight function. We first prove non-asymptotic error bounds for the mean squared errors (MSEs) of these algorithms, which explicitly depend on the number of iterations and the number of samples together. The non-asymptotic bounds derived in this paper imply that when the target belongs to the exponential family, the                                                   \$\$L\_2\$\$                                                               L                       2                                                                                       errors of the optimised samplers converge to the optimal rate of                                                   \$\$\textbackslash mathcal \{O\}(1/\textbackslash sqrt\{N\})\$\$                                                               O                       (                       1                       /                                                N                                              )                                                                                       and the rate of convergence in the number of iterations are explicitly provided. When the target does               not               belong to the exponential family, the rate of convergence is the same but the asymptotic                                                   \$\$L\_2\$\$                                                               L                       2                                                                                       error increases by a factor                                                   \$\$\textbackslash sqrt\{\textbackslash rho \^\textbackslash star \} {$>$} 1\$\$                                                                                                                   {$\rho$}                           {$\star$}                                                                       {$>$}                       1                                                                                       , where                                                   \$\$\textbackslash rho \^\textbackslash star - 1\$\$                                                                                        {$\rho$}                         {$\star$}                                              -                       1                                                                                       is the minimum                                                   \$\$\textbackslash chi \^2\$\$                                                               {$\chi$}                       2                                                                                       -divergence between the target and an exponential family proposal.},
  langid = {english},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\6ZUPK5ZZ\\Akyildiz and Mguez - 2021 - Convergence rates for optimised adaptive importanc.pdf}
}

@article{al-awadhi_improving_2004,
  title = {Improving the Acceptance Rate of Reversible Jump {{MCMC}} Proposals},
  author = {{Al-Awadhi}, Fahimah and Hurn, Merrilee and Jennison, Christopher},
  year = {2004},
  month = aug,
  journal = {Statistics \& Probability Letters},
  volume = {69},
  number = {2},
  pages = {189--198},
  langid = {english}
}

@misc{alexanderfabisch_cmaespp_2011,
  title = {{{CMA-ESpp}}},
  author = {Alexander Fabisch},
  year = {2011},
  abstract = {A C++ implementation of the derivative-free optimization algorithm CMA-ES.}
}

@article{alhinai_deep_2021,
  title = {Deep Learning Analysis of Resting Electrocardiograms for the Detection of Myocardial Dysfunction, Hypertrophy, and Ischaemia: A Systematic Review},
  shorttitle = {Deep Learning Analysis of Resting Electrocardiograms for the Detection of Myocardial Dysfunction, Hypertrophy, and Ischaemia},
  author = {Al Hinai, Ghalib and Jammoul, Samer and Vajihi, Zara and Afilalo, Jonathan},
  year = {2021},
  month = aug,
  journal = {European Heart Journal - Digital Health},
  pages = {ztab048},
  abstract = {Abstract                            Aims               To assess the evidence for deep learning (DL) analysis of resting electrocardiograms (ECG) to predict structural cardiac pathologies such as left ventricular systolic dysfunction, myocardial hypertrophy, and ischaemic heart disease.                                         Methods and Results               A systematic review was conducted to identify published original articles on end-to-end DL analysis of resting ECG signals for the detection of structural cardiac pathologies. Studies were excluded if the ECG was acquired by ambulatory, stress, intracardiac, or implantable devices, and if the pathology of interest was arrhythmic in nature. After duplicate reviewers screened search results, 12 articles met the inclusion criteria and were included. Three articles used DL ECG to detect left ventricular systolic dysfunction, achieving an area under the curve (AUC) of 0.89-0.93 and accuracy of 98\%. One study used DL ECG to detect left ventricular hypertrophy, achieving an AUC of 0.87 and accuracy of 87\%. Six articles used DL ECG to detect acute myocardial infarction, achieving an AUC of 0.88-1.00 and accuracy of 83-99.9\%. Two articles used DL ECG to detect stable ischaemic heart disease, achieving an accuracy of 95-99.9\%. DL algorithms, particularly those that used convolutional neural networks, outperformed rules-based algorithms and other machine learning algorithms.                                         Conclusions               DL is a promising technique to analyze resting ECG signals for the detection of structural cardiac pathologies, which has clinical applicability for more effective screening of asymptomatic populations and expedited diagnostic work-up of symptomatic patients at risk for cardiovascular disease.},
  langid = {english},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\EIYAWAEP\\Al Hinai et al. - 2021 - Deep learning analysis of resting electrocardiogra.pdf}
}

@article{ali_local_2022,
  title = {Local Sound Speed Estimation for Pulse-Echo Ultrasound in Layered Media},
  author = {Ali, Rehman and Telichko, Arsenii V. and Wang, Huaijun and Sukumar, Uday K. and {Vilches-Moure}, Jose G. and Paulmurugan, Ramasamy and Dahl, Jeremy J.},
  year = {2022},
  month = feb,
  journal = {IEEE Transactions on Ultrasonics, Ferroelectrics, and Frequency Control},
  volume = {69},
  number = {2},
  pages = {500--511}
}

@inproceedings{alipourfard_cherrypick_2017,
  title = {{{CherryPick}}: Adaptively Unearthing the Best Cloud Configurations for Big Data Analytics},
  booktitle = {Proc. 14th {{USENIX Symp}}. {{Networked Syst}}. {{Des}}. {{Implementation}}},
  author = {Alipourfard, Omid and Liu, Hongqiang Harry and Chen, Jianshu and Venkataraman, Shivaram and Yu, Minlan and Zhang, Ming},
  year = {2017},
  series = {{{NSDI}}'17},
  pages = {469--482},
  publisher = {{USENIX Association}},
  address = {{Boston, MA}},
  file = {/home/msca8h/Documents/bayesian_optimization/Alipourfard et al. - 2017 - CherryPick Adaptively Unearthing the Best Cloud C.pdf}
}

@inproceedings{alquier_nonexponentially_2021,
  title = {Non-{{Exponentially Weighted Aggregation}}: {{Regret Bounds}} for {{Unbounded Loss Functions}}},
  shorttitle = {Non-{{Exponentially Weighted Aggregation}}},
  booktitle = {Proceedings of the {{International Conference}} on {{Machine Learning}}},
  author = {Alquier, Pierre},
  year = {2021},
  month = jul,
  series = {{{PMLR}}},
  volume = {193},
  pages = {207--218},
  publisher = {{ML Research Press}},
  abstract = {We tackle the problem of online optimization with a general, possibly unbounded, loss function. It is well known that when the loss is bounded, the exponentially weighted aggregation strategy (EWA) leads to a regret in T--{$\surd$}T\textbackslash sqrt\{T\} after TTT steps. In this paper, we study a generalized aggregation strategy, where the weights no longer depend exponentially on the losses. Our strategy is based on Follow The Regularized Leader (FTRL): we minimize the expected losses plus a regularizer, that is here a {$\phi\phi\backslash$}phi-divergence. When the regularizer is the Kullback-Leibler divergence, we obtain EWA as a special case. Using alternative divergences enables unbounded losses, at the cost of a worst regret bound in some cases.},
  langid = {english},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\GYHU659E\\Alquier - 2021 - Non-Exponentially Weighted Aggregation Regret Bou.pdf;C\:\\Users\\msca8h\\Zotero\\storage\\K4TX6DHE\\Alquier - 2021 - Non-Exponentially Weighted Aggregation Regret Bou.pdf}
}

@inproceedings{alquier_nonexponentially_2021a,
  title = {Non-Exponentially Weighted Aggregation: {{Regret}} Bounds for Unbounded Loss Functions},
  shorttitle = {Non-Exponentially Weighted Aggregation},
  booktitle = {Proceedings of the {{International Conference}} on {{Machine Learning}}},
  author = {Alquier, Pierre},
  year = {2021},
  month = jul,
  series = {{{PMLR}}},
  volume = {193},
  pages = {207--218},
  publisher = {{ML Research Press}},
  abstract = {We tackle the problem of online optimization with a general, possibly unbounded, loss function. It is well known that when the loss is bounded, the exponentially weighted aggregation strategy (EWA) leads to a regret in T--{$\surd$}T\textbackslash sqrt\{T\} after TTT steps. In this paper, we study a generalized aggregation strategy, where the weights no longer depend exponentially on the losses. Our strategy is based on Follow The Regularized Leader (FTRL): we minimize the expected losses plus a regularizer, that is here a {$\phi\phi\backslash$}phi-divergence. When the regularizer is the Kullback-Leibler divergence, we obtain EWA as a special case. Using alternative divergences enables unbounded losses, at the cost of a worst regret bound in some cases.},
  langid = {english},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\7Q54E9BP\\Alquier - 2021 - Non-Exponentially Weighted Aggregation Regret Bou.pdf;C\:\\Users\\msca8h\\Zotero\\storage\\D4AFG38R\\Alquier - 2021 - Non-Exponentially Weighted Aggregation Regret Bou.pdf}
}

@article{altekar_parallel_2004,
  title = {Parallel {{Metropolis}} Coupled {{Markov}} Chain {{Monte Carlo}} for {{Bayesian}} Phylogenetic Inference},
  author = {Altekar, G. and Dwarkadas, S. and Huelsenbeck, J. P. and Ronquist, F.},
  year = {2004},
  month = feb,
  journal = {Bioinformatics},
  volume = {20},
  number = {3},
  pages = {407--415},
  langid = {english},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\YQ92IHIC\\Altekar et al. - 2004 - Parallel Metropolis coupled Markov chain Monte Car.pdf}
}

@inproceedings{amdahl_validity_1967,
  title = {Validity of the Single Processor Approach to Achieving Large Scale Computing Capabilities},
  booktitle = {Proceedings of the {{April}} 18-20, 1967, {{Spring Joint Computer Conference}}},
  author = {Amdahl, Gene M.},
  year = {1967},
  pages = {483},
  publisher = {{ACM Press}},
  address = {{Atlantic City, New Jersey}},
  langid = {english}
}

@inproceedings{amiri_computation_2019,
  title = {Computation {{Scheduling}} for {{Distributed Machine Learning}} with {{Straggling Workers}}},
  booktitle = {{{ICASSP}} 2019 - 2019 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Amiri, Mohammad Mohammadi and Gunduz, Deniz},
  year = {2019},
  month = may,
  pages = {8177--8181},
  publisher = {{IEEE}},
  address = {{Brighton, United Kingdom}},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\9B8TBEGA\\Amiri and Gunduz - 2019 - Computation Scheduling for Distributed Machine Lea.pdf}
}

@article{amirsoleimani_wideband_2020,
  title = {Wideband Modal Orthogonality: {{A New}} Approach for Broadband {{DOA}} Estimation},
  shorttitle = {Wideband Modal Orthogonality},
  author = {Amirsoleimani, Shervin and Olfat, Ali},
  year = {2020},
  month = nov,
  journal = {Signal Processing},
  volume = {176},
  pages = {107696},
  langid = {english},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\NDDSL7N7\\Amirsoleimani and Olfat - 2020 - Wideband modal orthogonality A New approach for b.pdf}
}

@article{amirsoleimani_wideband_2020a,
  title = {Wideband Modal Orthogonality: {{A New}} Approach for Broadband {{DOA}} Estimation},
  shorttitle = {Wideband Modal Orthogonality},
  author = {Amirsoleimani, Shervin and Olfat, Ali},
  year = {2020},
  month = nov,
  journal = {Signal Processing},
  volume = {176},
  pages = {107696},
  langid = {english},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\XHAL6MIU\\Amirsoleimani and Olfat - 2020 - Wideband modal orthogonality A New approach for b.pdf}
}

@article{andrieu_ergodicity_2006,
  title = {On the Ergodicity Properties of Some Adaptive {{MCMC}} Algorithms},
  author = {Andrieu, Christophe and Moulines, {\'E}ric},
  year = {2006},
  month = aug,
  journal = {The Annals of Applied Probability},
  volume = {16},
  number = {3},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\BWPKTXKR\\Andrieu and Moulines - 2006 - On the ergodicity properties of some adaptive MCMC.pdf}
}

@article{andrieu_introduction_2003,
  title = {An Introduction to {{MCMC}} for Machine Learning},
  author = {Andrieu, Christophe and {de Freitas}, Nando and Doucet, Arnaud and Jordan, Michael I.},
  year = {2003},
  journal = {Machine Learning},
  volume = {50},
  number = {1/2},
  pages = {5--43},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\JRMGPMFB\\Andrieu et al. - 2003 - [No title found].pdf}
}

@article{andrieu_joint_1999,
  title = {Joint {{Bayesian}} Model Selection and Estimation of Noisy Sinusoids via Reversible Jump {{MCMC}}},
  author = {Andrieu, C. and Doucet, A.},
  year = {Oct./1999},
  journal = {IEEE Transactions on Signal Processing},
  volume = {47},
  number = {10},
  pages = {2667--2676}
}

@article{andrieu_model_2001,
  title = {Model Selection by {{MCMC}} Computation},
  author = {Andrieu, C. and Djuri{\'c}, P.M. and Doucet, A.},
  year = {2001},
  month = jan,
  journal = {Signal Processing},
  volume = {81},
  number = {1},
  pages = {19--37},
  langid = {english}
}

@article{andrieu_particle_2010,
  title = {Particle {{Markov}} Chain {{Monte Carlo}} Methods},
  shorttitle = {Particle {{Markov}} Chain {{Monte Carlo}} Methods},
  author = {Andrieu, Christophe and Doucet, Arnaud and Holenstein, Roman},
  year = {2010},
  month = jun,
  journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
  volume = {72},
  number = {3},
  pages = {269--342},
  langid = {english},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\UNBH88PQ\\Andrieu et al. - 2010 - Particle Markov chain Monte Carlo methods Particl.pdf}
}

@article{andrieu_tutorial_2008,
  title = {A Tutorial on Adaptive {{MCMC}}},
  author = {Andrieu, Christophe and Thoms, Johannes},
  year = {2008},
  month = dec,
  journal = {Statistics and Computing},
  volume = {18},
  number = {4},
  pages = {343--373},
  publisher = {{Kluwer Academic Publishers}},
  address = {{USA}},
  abstract = {We review adaptive Markov chain Monte Carlo algorithms (MCMC) as a mean to optimise their performance. Using simple toy examples we review their theoretical underpinnings, and in particular show why adaptive MCMC algorithms might fail when some fundamental properties are not satisfied. This leads to guidelines concerning the design of correct algorithms. We then review criteria and the useful framework of stochastic approximation, which allows one to systematically optimise generally used criteria, but also analyse the properties of adaptive MCMC algorithms. We then propose a series of novel adaptive algorithms which prove to be robust and reliable in practice. These algorithms are applied to artificial and high dimensional scenarios, but also to the classic mine disaster dataset inference problem.},
  issue_date = {December 2008},
  keywords = {Adaptive MCMC,Controlled Markov chain,MCMC,Stochastic approximation}
}

@article{andrieu_uniform_2018,
  title = {Uniform Ergodicity of the Iterated Conditional {{SMC}} and Geometric Ergodicity of Particle {{Gibbs}} Samplers},
  author = {Andrieu, Christophe and Lee, Anthony and Vihola, Matti},
  year = {2018},
  month = may,
  journal = {Bernoulli},
  volume = {24},
  number = {2},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\AAKZYF9Z\\Andrieu et al. - 2018 - Uniform ergodicity of the iterated conditional SMC.pdf}
}

@article{ang_optimal_1992,
  title = {Optimal Importance-sampling Density Estimator},
  author = {Ang, George L. and Ang, Alfredo H.-S. and Tang, Wilson H.},
  year = {1992},
  month = jun,
  journal = {Journal of Engineering Mechanics},
  volume = {118},
  number = {6},
  pages = {1146--1163},
  langid = {english}
}

@article{angelino_patterns_2016,
  title = {Patterns of Scalable {{Bayesian}} Inference},
  author = {Angelino, Elaine and Johnson, Matthew James and Adams, Ryan P.},
  year = {2016},
  journal = {Foundations and Trends\textregistered{} in Machine Learning},
  volume = {9},
  number = {2-3},
  pages = {119--247},
  langid = {english},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\K48JPUMZ\\Angelino et al. - 2016 - Patterns of Scalable Bayesian Inference.pdf}
}

@inproceedings{annangi_ai_2020,
  title = {{{AI}} Assisted Feedback System for Transmit Parameter Optimization in {{Cardiac Ultrasound}}},
  booktitle = {Proc. {{IEEE Int}}. {{Ultrason}}. {{Symp}}.},
  author = {Annangi, Pavan and Ravishankar, Hariharan and Patil, Rohan and Tore, Bjastaad and Aase, Svein Arne and Steen, Erik},
  year = {2020},
  month = sep,
  series = {{{IUS}}'20},
  pages = {1--4},
  publisher = {{IEEE}},
  address = {{Las Vegas, NV, USA}}
}

@inproceedings{anonymous2023on,
  title = {On the Nonconvex Convergence of {{SGD}}},
  booktitle = {Submitted to the Eleventh International Conference on Learning Representations},
  author = {{Anonymous}},
  year = {2023}
}

@inproceedings{ansel_opentuner_2014,
  title = {{{OpenTuner}}: An Extensible Framework for Program Autotuning},
  shorttitle = {{{OpenTuner}}},
  booktitle = {Proceedings of the 23rd International Conference on {{Parallel}} Architectures and Compilation - {{PACT}} '14},
  author = {Ansel, Jason and Kamil, Shoaib and Veeramachaneni, Kalyan and {Ragan-Kelley}, Jonathan and Bosboom, Jeffrey and O'Reilly, Una-May and Amarasinghe, Saman},
  year = {2014},
  pages = {303--316},
  publisher = {{ACM Press}},
  address = {{Edmonton, AB, Canada}},
  langid = {english}
}

@article{article,
  title = {{{JAGS}}: {{A}} Program for Analysis of Bayesian Graphical Models Using Gibbs Sampling},
  author = {Plummer, Martyn},
  year = {2003},
  month = apr,
  journal = {3rd International Workshop on Distributed Statistical Computing (DSC 2003); Vienna, Austria},
  volume = {124}
}

@article{asadzadeh_coherent_2020,
  title = {Coherent Wide-Band Signals {{DOA}} Estimation by the New {{CTOPS}} Algorithm},
  author = {Asadzadeh, Abbas and Alavi, Seyed Mohammad and Karimi, Mahmood and Amiri, Hadi},
  year = {2020},
  month = jul,
  journal = {Multidimensional Systems and Signal Processing},
  volume = {31},
  number = {3},
  pages = {1075--1089},
  abstract = {Direction of arrival estimation is one of the most important issues in array signals processing. In this paper, a new method is presented for direction estimation of coherent wide-band signals. First of all, signal eigenvalues which have information of the direction of coherent signals, are extracted as primary vectors in each frequency bin. Afterwards, by constructing the innovative matrix H from signal eigenvalues, the de-correlation process is performed and the linear independent vectors of sources are extracted from SVD decomposition. Finally, by choosing transfer matrix, the signal subspace of the reference frequency is transferred to other frequency bins and the direction estimation process is performed by using TOPS algorithm. The proposed method does not need any knowledge of the number of sources and initial estimation of arrival angles. According to the simulation results, the CTOPS new method has a better performance than TOPS method in the presence of correlated sources.},
  langid = {english},
  keywords = {Array,Coherent signals,CTOPS,DOA estimation,TOPS},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\H4SE9WF2\\Asadzadeh et al. - 2020 - Coherent wide-band signals DOA estimation by the n.pdf}
}

@article{ashikuzzaman_low_2020,
  title = {Low {{Rank}} and {{Sparse Decomposition}} of {{Ultrasound Color Flow Images}} for {{Suppressing Clutter}} in {{Real-Time}}},
  author = {Ashikuzzaman, Md and Belasso, Clyde and Kibria, Md. Golam and Bergdahl, Andreas and Gauthier, Claudine J. and Rivaz, Hassan},
  year = {2020},
  month = apr,
  journal = {IEEE Transactions on Medical Imaging},
  volume = {39},
  number = {4},
  pages = {1073--1084}
}

@article{ashouri_survey_2018,
  title = {A {{Survey}} on {{Compiler Autotuning}} Using {{Machine Learning}}},
  author = {Ashouri, Amir H. and Killian, William and Cavazos, John and Palermo, Gianluca and Silvano, Cristina},
  year = {2018},
  month = sep,
  journal = {ACM Computing Surveys},
  volume = {51},
  number = {5},
  pages = {1--42},
  langid = {english},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\K2MMSXTC\\Ashouri et al. - 2018 - A Survey on Compiler Autotuning using Machine Lear.pdf}
}

@article{asl_contrast_2011,
  title = {Contrast Enhancement and Robustness Improvement of Adaptive Ultrasound Imaging Using Forward-Backward Minimum Variance Beamforming},
  author = {Asl, B M and Mahloojifar, A},
  year = {2011},
  month = apr,
  journal = {IEEE Transactions on Ultrasonics, Ferroelectrics and Frequency Control},
  volume = {58},
  number = {4},
  pages = {858--867}
}

@article{asl_lowcomplexity_2012,
  title = {A Low-Complexity Adaptive Beamformer for Ultrasound Imaging Using Structured Covariance Matrix},
  author = {Asl, B. M. and Mahloojifar, A.},
  year = {2012},
  month = apr,
  journal = {IEEE Transactions on Ultrasonics, Ferroelectrics and Frequency Control},
  volume = {59},
  number = {4},
  pages = {660--667}
}

@inproceedings{ath_bayesian_2021,
  title = {How {{Bayesian}} Should {{Bayesian}} Optimisation Be?},
  booktitle = {Proc.  {{Genetic Evol}}. {{Comput}}. {{Conf}}. {{Companion}}},
  author = {George De Ath, Richard Everson, Jonathan Fieldsend},
  year = {2021},
  series = {{{GECCO}} '20},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}}
}

@article{attia_screening_2019,
  title = {Screening for Cardiac Contractile Dysfunction Using an Artificial Intelligence\textendash Enabled Electrocardiogram},
  author = {Attia, Zachi I. and Kapa, Suraj and {Lopez-Jimenez}, Francisco and McKie, Paul M. and Ladewig, Dorothy J. and Satam, Gaurav and Pellikka, Patricia A. and {Enriquez-Sarano}, Maurice and Noseworthy, Peter A. and Munger, Thomas M. and Asirvatham, Samuel J. and Scott, Christopher G. and Carter, Rickey E. and Friedman, Paul A.},
  year = {2019},
  month = jan,
  journal = {Nature Medicine},
  volume = {25},
  number = {1},
  pages = {70--74},
  langid = {english}
}

@article{aubry_fast_2014,
  title = {Fast Local {{Laplacian}} Filters: {{Theory}} and Applications},
  shorttitle = {Fast {{Local Laplacian Filters}}},
  author = {Aubry, Mathieu and Paris, Sylvain and Hasinoff, Samuel W. and Kautz, Jan and Durand, Fr{\'e}do},
  year = {2014},
  month = sep,
  journal = {ACM Transactions on Graphics},
  volume = {33},
  number = {5},
  pages = {1--14},
  abstract = {Multiscale manipulations are central to image editing but also prone to halos. Achieving artifact-free results requires sophisticated edge-aware techniques and careful parameter tuning. These shortcomings were recently addressed by the local Laplacian filters, which can achieve a broad range of effects using standard Laplacian pyramids. However, these filters are slow to evaluate and their relationship to other approaches is unclear. In this article, we show that they are closely related to anisotropic diffusion and to bilateral filtering. Our study also leads to a variant of the bilateral filter that produces cleaner edges while retaining its speed. Building upon this result, we describe an acceleration scheme for local Laplacian filters on gray-scale images that yields speedups on the order of 50\texttimes. Finally, we demonstrate how to use local Laplacian filters to alter the distribution of gradients in an image. We illustrate this property with a robust algorithm for photographic style transfer.},
  langid = {english},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\Y9QU44ZV\\Aubry et al. - 2014 - Fast Local Laplacian Filters Theory and Applicati.pdf}
}

@mastersthesis{austad_parallel_2007,
  title = {Parallel {{Multiple Proposal MCMC Algorithms}}},
  author = {Austad, Haakon Michael},
  year = {2007},
  month = jun,
  school = {Norwegian University of Science and Technology}
}

@inproceedings{austeng_use_2013,
  title = {Use of the Minimum Variance Beamformer in Synthetic Aperture Sonar Imaging},
  booktitle = {Proceedings of the {{European Conference}} on {{Underwater Acoustics}}},
  author = {Austeng, Andreas and Jensen, Are C. and Nilsen, Carl-Inge C. and Callow, Hayden J. and Hansen, Roy E.},
  year = {2013},
  series = {{{ECUA}}'12},
  pages = {070098},
  address = {{Edinburgh, Scotland}},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\G9GV37BU\\Austeng et al. - 2013 - Use of the minimum variance beamformer in syntheti.pdf}
}

@inproceedings{axelsen_evaluation_2010,
  title = {Evaluation of Automatic Time Gain Compensated In-Vivo Ultrasound Sequences},
  booktitle = {{{IEEE International Ultrasonics Symposium}}},
  author = {Axelsen, Martin Christian and Roeboe, Kristian Frostholm and Hemmsen, Martin Christian and Nikolov, Svetoslav Ivanov and Pedersen, Mads Moller and Nielsen, Michael Bachmann and Jensen, Jorgen Arendt},
  year = {2010},
  month = oct,
  pages = {1640--1643},
  publisher = {{IEEE}},
  address = {{San Diego, CA}},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\KWKSCNCV\\Axelsen et al. - 2010 - Evaluation of automatic time gain compensated in-v.pdf}
}

@inproceedings{aykin_efficient_2016,
  title = {Efficient Ray-Casting of Quadric Surfaces for Forward-Scan Sonars},
  booktitle = {{{OCEANS}} 2016 {{MTS}}/{{IEEE Monterey}}},
  author = {Aykin, Murat D. and Negahdaripour, Shahriar},
  year = {2016},
  month = sep,
  pages = {1--7},
  publisher = {{IEEE}},
  address = {{Monterey, CA, USA}}
}

@article{aykin_modeling_2016,
  title = {Modeling 2-{{D}} Lens-Based Forward-Scan Sonar Imagery for Targets with Diffuse Reflectance},
  author = {Aykin, Murat D. and Negahdaripour, Shahriar S.},
  year = {2016},
  month = jul,
  journal = {IEEE Journal of Oceanic Engineering},
  volume = {41},
  number = {3},
  pages = {569--582}
}

@article{ba-nguvo_sequential_2005,
  title = {Sequential {{Monte Carlo}} Methods for Multi-Target Filtering with Random Finite Sets},
  author = {{Ba-Ngu Vo} and Singh, S. and Boucet, A.},
  year = {2005},
  month = oct,
  journal = {IEEE Transactions on Aerospace and Electronic Systems},
  volume = {41},
  number = {4},
  pages = {1224--1245},
  langid = {english},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\LBNFNFL5\\Ba-Ngu Vo et al. - 2005 - Sequential monte carlo methods for multi-target fi.pdf}
}

@inproceedings{babanezhadharikandeh_stop_2015,
  title = {Stop Wasting My Gradients: {{Practical SVRG}}},
  shorttitle = {{{StopWasting My Gradients}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Babanezhad Harikandeh, Reza and Ahmed, Mohamed Osama and Virani, Alim and Schmidt, Mark and Kone{\v c}n{\'y}, Jakub and Sallinen, Scott},
  year = {2015},
  volume = {28},
  publisher = {{Curran Associates, Inc.}},
  abstract = {We present and analyze several strategies for improving the performance ofstochastic variance-reduced gradient (SVRG) methods. We first show that theconvergence rate of these methods can be preserved under a decreasing sequenceof errors in the control variate, and use this to derive variants of SVRG that usegrowing-batch strategies to reduce the number of gradient calculations requiredin the early iterations. We further (i) show how to exploit support vectors to reducethe number of gradient computations in the later iterations, (ii) prove that thecommonly\textendash used regularized SVRG iteration is justified and improves the convergencerate, (iii) consider alternate mini-batch selection strategies, and (iv) considerthe generalization error of the method.},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\8DGTHKY3\\Babanezhad Harikandeh et al. - 2015 - StopWasting My Gradients Practical SVRG.pdf}
}

@inproceedings{backstrom_group_2006,
  title = {Group Formation in Large Social Networks: Membership, Growth, and Evolution},
  booktitle = {Proc. 12th {{ACM SIGKDD Int}}. {{Conf}}. {{Knowl}}. {{Discovery Data Mining}}},
  author = {Backstrom, Lars and Huttenlocher, Dan and Kleinberg, Jon and Lan, Xiangyang},
  year = {2006},
  series = {{{KDD}}'06},
  pages = {44--54},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  keywords = {diffusion of innovations,on-line communities,social networks}
}

@article{badiu_variational_2017,
  title = {Variational {{Bayesian Inference}} of {{Line Spectra}}},
  author = {Badiu, Mihai-Alin and Hansen, Thomas Lundgaard and Fleury, Bernard Henri},
  year = {2017},
  month = may,
  journal = {IEEE Transactions on Signal Processing},
  volume = {65},
  number = {9},
  pages = {2247--2261},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\KZLT5JX9\\Badiu et al. - 2017 - Variational Bayesian Inference of Line Spectra.pdf}
}

@article{badri_fast_2015,
  title = {Fast Edge-Aware Processing via First Order Proximal Approximation},
  author = {Badri, Hicham and Yahia, Hussein and Aboutajdine, Driss},
  year = {2015},
  month = jun,
  journal = {IEEE Transactions on Visualization and Computer Graphics},
  volume = {21},
  number = {6},
  pages = {743--755},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\ZRD4PQFY\\Badri et al. - 2015 - Fast Edge-Aware Processing via First Order Proxima.pdf}
}

@article{baggenstoss_specular_2013,
  title = {Specular Decomposition of Active Sonar Returns Using Combined Waveforms},
  author = {Baggenstoss, Paul M.},
  year = {2013},
  month = oct,
  journal = {IEEE Transactions on Aerospace and Electronic Systems},
  volume = {49},
  number = {4},
  pages = {2509--2521}
}

@inproceedings{baggeroer_passive_1999,
  title = {Passive Sonar Limits upon Nulling Multiple Moving Ships with Large Aperture Arrays},
  booktitle = {Conf. {{Record Asilomar Conf}}. on {{Signals}}, {{Syst}}. {{Comput}}.},
  author = {Baggeroer, A.B. and Cox, H.},
  year = {1999},
  volume = {1},
  pages = {103--108},
  publisher = {{IEEE}},
  address = {{Pacific Grove, CA, USA}}
}

@article{bai_containment_2011,
  title = {On the Containment Condition for Adaptive Markov Chain Monte Carlo Algorithms},
  author = {Bai, Yan and Roberts, Gareth and Rosenthal, Jeffrey},
  year = {2011},
  month = jan,
  journal = {Adv. Appl. Stat.},
  volume = {21}
}

@inproceedings{bailey_nas_1991,
  title = {The {{NAS}} Parallel Benchmarks---Summary and Preliminary Results},
  booktitle = {Proceedings of the 1991 {{ACM}}/{{IEEE}} Conference on {{Supercomputing}}  - {{Supercomputing}} '91},
  author = {Bailey, D. H. and Schreiber, R. S. and Simon, H. D. and Venkatakrishnan, V. and Weeratunga, S. K. and Barszcz, E. and Barton, J. T. and Browning, D. S. and Carter, R. L. and Dagum, L. and Fatoohi, R. A. and Frederickson, P. O. and Lasinski, T. A.},
  year = {1991},
  pages = {158--165},
  publisher = {{ACM Press}},
  address = {{Albuquerque, New Mexico, United States}},
  langid = {english}
}

@incollection{bailey_nas_2011,
  title = {{{NAS}} Parallel Benchmarks},
  booktitle = {Encyclopedia of {{Parallel Computing}}},
  author = {Bailey, David H.},
  year = {2011},
  pages = {1254--1259},
  publisher = {{Springer US}},
  address = {{Boston, MA}}
}

@inproceedings{bak_optimized_2019,
  title = {Optimized Execution of Parallel Loops via User-Defined Scheduling Policies},
  booktitle = {Proc. 48th {{Int}}. {{Conf}}. {{Parallel Process}}.},
  author = {Bak, Seonmyeong and Guo, Yanfei and Balaji, Pavan and Sarkar, Vivek},
  year = {2019},
  month = aug,
  series = {{{ICPP}}'19},
  pages = {1--10},
  publisher = {{ACM}},
  address = {{Kyoto Japan}},
  langid = {english}
}

@article{balakrishnan_onepass_2006,
  title = {A One-Pass Sequential {{Monte Carlo}} Method for {{Bayesian}} Analysis of Massive Datasets},
  author = {Balakrishnan, Suhrid and Madigan, David},
  year = {2006},
  month = jun,
  journal = {Bayesian Analysis},
  volume = {1},
  number = {2},
  pages = {345--361},
  langid = {english},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\WVSSS4I2\\Balakrishnan and Madigan - 2006 - A one-pass sequential Monte Carlo method for Bayes.pdf}
}

@inproceedings{balaprakash_activelearningbased_2013,
  title = {Active-Learning-Based Surrogate Models for Empirical Performance Tuning},
  booktitle = {2013 {{IEEE International Conference}} on {{Cluster Computing}} ({{CLUSTER}})},
  author = {Balaprakash, Prasanna and Gramacy, Robert B. and Wild, Stefan M.},
  year = {2013},
  month = sep,
  pages = {1--8},
  publisher = {{IEEE}},
  address = {{Indianapolis, IN, USA}},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\JTYE3NWJ\\Balaprakash et al. - 2013 - Active-learning-based surrogate models for empiric.pdf}
}

@article{balaprakash_autotuning_2018,
  title = {Autotuning in {{High-Performance Computing Applications}}},
  author = {Balaprakash, Prasanna and Dongarra, Jack and Gamblin, Todd and Hall, Mary and Hollingsworth, Jeffrey K. and Norris, Boyana and Vuduc, Richard},
  year = {2018},
  month = nov,
  journal = {Proceedings of the IEEE},
  volume = {106},
  number = {11},
  pages = {2068--2083}
}

@inproceedings{bamler_perturbative_2017,
  title = {Perturbative Black Box Variational Inference},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Bamler, Robert and Zhang, Cheng and Opper, Manfred and Mandt, Stephan},
  year = {2017},
  volume = {30},
  publisher = {{Curran Associates, Inc.}},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\JSVVV2WU\\Bamler et al. - 2017 - Perturbative Black Box Variational Inference.pdf}
}

@inproceedings{banicescu_balancing_1995,
  title = {Balancing {{Processor Loads}} and {{Exploiting Data Locality}} in {{N-Body Simulations}}},
  booktitle = {Supercomputing '95:{{Proceedings}} of the 1995 {{ACM}}/{{IEEE Conference}} on {{Supercomputing}}},
  author = {Banicescu, I. and Hummel, S. F.},
  year = {1995},
  month = dec,
  pages = {43--43},
  keywords = {Delay,Distributed computing,Dynamic scheduling,exploiting locality,factoring scheduling,Fractals,Load management,multiprocessor load balancing,N-body simulations,Parallel machines,Performance gain,Permission,Processor scheduling,Scheduling algorithm,tiling},
  file = {/home/msca8h/Documents/parallel_scheduling/Banicescu and Hummel - 1995 - Balancing Processor Loads and Exploiting Data Loca.pdf}
}

@inproceedings{banicescu_load_2002,
  title = {Load Balancing Highly Irregular Computations with the Adaptive Factoring},
  booktitle = {Proc. {{Int}}. {{Parallel Distrib}}. {{Process}}. {{Symp}}.},
  author = {Banicescu, I. and Velusamy, V.},
  year = {2002},
  series = {{{IPDPS}}'02},
  pages = {12 pp},
  address = {{Ft. Lauderdale, FL}},
  file = {/home/msca8h/Documents/parallel_scheduling/Banicescu and Velusamy - 2002 - Load balancing highly irregular computations with .pdf}
}

@article{banterle_accelerating_2019,
  title = {Accelerating {{Metropolis-Hastings}} Algorithms by {{Delayed Acceptance}}},
  author = {Banterle, Marco and Grazian, Clara and Lee, Anthony and P. Robert, Christian},
  year = {2019},
  journal = {Foundations of Data Science},
  volume = {1},
  number = {2},
  pages = {103--128},
  langid = {english},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\EQTZFSHG\\Banterle et al. - 2019 - Accelerating Metropolis-Hastings algorithms by Del.pdf}
}

@article{baragatti_comments_2011,
  title = {Comments on '{{Bayesian}} Variable Selection for Disease Classification Using Gene Expression Data'},
  author = {Baragatti, M. C. and Pommeret, D.},
  year = {2011},
  month = apr,
  journal = {Bioinformatics},
  volume = {27},
  number = {8},
  pages = {1194--1194},
  langid = {english},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\27IVLZBH\\Baragatti and Pommeret - 2011 - Comments on 'Bayesian variable selection for disea.pdf}
}

@article{baragatti_study_2012,
  title = {A Study of Variable Selection Using $g$ -Prior Distribution with Ridge Parameter},
  author = {Baragatti, M. and Pommeret, D.},
  year = {2012},
  month = jun,
  journal = {Computational Statistics \& Data Analysis},
  volume = {56},
  number = {6},
  pages = {1920--1934},
  langid = {english}
}

@inbook{barbakh_cross_2009,
  title = {Cross Entropy Methods},
  booktitle = {Non-{{Standard Parameter Adaptation}} for {{Exploratory Data Analysis}}},
  author = {Barbakh, Wesam Ashour and Wu, Ying and Fyfe, Colin},
  year = {2009},
  volume = {249},
  pages = {151--174},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  collaborator = {Barbakh, Wesam Ashour and Wu, Ying and Fyfe, Colin}
}

@article{bardenet_markov_,
  title = {On {{Markov}} Chain {{Monte Carlo}} Methods for Tall Data},
  author = {Bardenet, Remi and Doucet, Arnaud and Holmes, Chris},
  pages = {43},
  abstract = {Markov chain Monte Carlo methods are often deemed too computationally intensive to be of any practical use for big data applications, and in particular for inference on datasets containing a large number n of individual data points, also known as tall datasets. In scenarios where data are assumed independent, various approaches to scale up the MetropolisHastings algorithm in a Bayesian inference context have been recently proposed in machine learning and computational statistics. These approaches can be grouped into two categories: divide-and-conquer approaches and, subsampling-based algorithms. The aims of this article are as follows. First, we present a comprehensive review of the existing literature, commenting on the underlying assumptions and theoretical guarantees of each method. Second, by leveraging our understanding of these limitations, we propose an original subsampling-based approach relying on a control variate method which samples under regularity conditions from a distribution provably close to the posterior distribution of interest, yet can require less than O(n) data point likelihood evaluations at each iteration for certain statistical models in favourable scenarios. Finally, we emphasize that we have only been able so far to propose subsampling-based methods which display good performance in scenarios where the Bernstein-von Mises approximation of the target posterior distribution is excellent. It remains an open challenge to develop such methods in scenarios where the Bernstein-von Mises approximation is poor.},
  langid = {english},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\H2JERCKI\\Bardenet et al. - On Markov chain Monte Carlo methods for tall data.pdf}
}

@article{bardenet_markov_2017,
  title = {On {{Markov}} Chain {{Monte Carlo}} Methods for Tall Data},
  author = {Bardenet, R{\'e}mi and Doucet, Arnaud and Holmes, Chris},
  year = {2017},
  journal = {Journal of Machine Learning Research},
  volume = {18},
  number = {47},
  pages = {1--43},
  abstract = {Markov chain Monte Carlo methods are often deemed too computationally intensive to be of any practical use for big data applications, and in particular for inference on datasets containing a large number  n n  of individual data points, also known as tall datasets. In scenarios where data are assumed independent, various approaches to scale up the Metropolis- Hastings algorithm in a Bayesian inference context have been recently proposed in machine learning and computational statistics. These approaches can be grouped into two categories: divide-and-conquer approaches and, subsampling-based algorithms. The aims of this article are as follows. First, we present a comprehensive review of the existing literature, commenting on the underlying assumptions and theoretical guarantees of each method. Second, by leveraging our understanding of these limitations, we propose an original subsampling-based approach relying on a control variate method which samples under regularity conditions from a distribution provably close to the posterior distribution of interest, yet can require less than  O(n) O(n)  data point likelihood evaluations at each iteration for certain statistical models in favourable scenarios. Finally, we emphasize that we have only been able so far to propose subsampling-based methods which display good performance in scenarios where the Bernstein-von Mises approximation of the target posterior distribution is excellent. It remains an open challenge to develop such methods in scenarios where the Bernstein-von Mises approximation is poor.},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\FDA6JZPI\\Bardenet et al. - 2017 - On Markov chain Monte Carlo methods for tall data.pdf}
}

@article{bares_noise_2010,
  title = {Noise Estimation in Long-Range Matched-Filter Envelope Sonar Data},
  author = {Bares, Robert and Evans, Dafydd and Long, Stephen},
  year = {2010},
  month = apr,
  journal = {IEEE Journal of Oceanic Engineering},
  volume = {35},
  number = {2},
  pages = {230--235}
}

@article{barker_monte_1965,
  title = {Monte {{Carlo}} Calculations of the Radial Distribution Functions for a Proton Electron Plasma},
  shorttitle = {Monte {{Carlo Calculations}} of the {{Radial Distribution Functions}} for a {{Proton}}?},
  author = {Barker, Anthony A.},
  year = {1965},
  journal = {Australian Journal of Physics},
  volume = {18},
  number = {2},
  pages = {119},
  abstract = {A general method is presented for computation of radial distribution functions for plasmas over a wide range of temperatures and densities. The method uses the Monte Carlo technique applied by Wood and Parker, and extends this to long-range forces using results borrowed from crystal lattice theory. The approach is then used to calculate the radial distribution functions for a proton-electron plasma of density 1018 electrons/cm3 at a temperature of 104 OK. The results show the usefulness of the method if sufficient computing facilities are available.},
  langid = {english},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\7TZDEJCY\\Barker - 1965 - Monte Carlo Calculations of the Radial Distributio.pdf}
}

@article{bartolucci_efficient_2006,
  title = {Efficient {{Bayes}} Factor Estimation from the Reversible Jump Output},
  author = {Bartolucci, Francesco and Scaccia, Luisa and Mira, Antonietta},
  year = {2006},
  month = mar,
  journal = {Biometrika},
  volume = {93},
  number = {1},
  pages = {41--52},
  abstract = {We propose a class of estimators of the Bayes factor which is based on an extension of the bridge sampling identity of Meng \&amp; Wong (1996) and makes use of the output of the reversible jump algorithm of Green (1995). Within this class we give the optimal estimator and also a suboptimal one which may be simply computed on the basis of the acceptance probabilities used within the reversible jump algorithm for jumping between models. The proposed estimators are very easily computed and lead to a substantial gain of efficiency in estimating the Bayes factor over the standard estimator based on the reversible jump output. This is illustrated through a series of Monte Carlo simulations involving a linear and a logistic regression model.},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\XT3WW3CV\\Bartolucci et al. - 2006 - Efficient Bayes factor estimation from the reversi.pdf;C\:\\Users\\msca8h\\Zotero\\storage\\V9YPS23F\\235857.html}
}

@article{bast_scheduling_2000,
  title = {On Scheduling Parallel Tasks at Twilight},
  author = {Bast, H.},
  year = {2000},
  month = dec,
  journal = {Theory of Computing Systems},
  volume = {33},
  number = {5-6},
  pages = {489--563},
  langid = {english},
  file = {/home/msca8h/Documents/parallel_scheduling/Bast - 2000 - On Scheduling Parallel Tasks at Twilight.pdf}
}

@phdthesis{basthannah_provably_2000,
  title = {Provably Optimal Scheduling of Similar Tasks},
  author = {{Bast, Hannah}},
  year = {2000},
  school = {Universit\"at des Saarlandes, Saarbr\"ucken},
  file = {/home/msca8h/Documents/parallel_scheduling/Bast, Hannah - 2000 - Provably Optimal Scheduling of Similar Tasks.pdf}
}

@article{basu_making_2013,
  title = {Towards Making Autotuning Mainstream},
  author = {Basu, Protonu and Hall, Mary and Khan, Malik and Maindola, Suchit and Muralidharan, Saurav and Ramalingam, Shreyas and Rivera, Axel and Shantharam, Manu and Venkat, Anand},
  year = {2013},
  month = nov,
  journal = {The International Journal of High Performance Computing Applications},
  volume = {27},
  number = {4},
  pages = {379--393},
  langid = {english}
}

@inproceedings{bayat_concurrent_2018,
  title = {Concurrent {{Clutter}} and {{Noise Suppression}} via {{Low Rank Plus Sparse Optimization}} for {{Non-Contrast Ultrasound Flow Doppler Processing}} in {{Microvasculature}}},
  booktitle = {2018 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Bayat, Mahdi and Fatemi, Mostafa},
  year = {2018},
  month = apr,
  pages = {1080--1084},
  publisher = {{IEEE}},
  address = {{Calgary, AB}}
}

@inproceedings{baydin_etalumis_2019,
  title = {Etalumis: Bringing Probabilistic Programming to Scientific Simulators at Scale},
  shorttitle = {Etalumis},
  booktitle = {Proc.  {{Int}}. {{Conf}}. {{High Perform}}. {{Comput}}. {{Networking}}, {{Storage}}, {{Anal}}.},
  author = {Baydin, Atilim G{\"u}ne{\c s} and Shao, Lei and Bhimji, Wahid and Heinrich, Lukas and Meadows, Lawrence and Liu, Jialin and Munk, Andreas and Naderiparizi, Saeid and {Gram-Hansen}, Bradley and Louppe, Gilles and Ma, Mingfei and Zhao, Xiaohui and Torr, Philip and Lee, Victor and Cranmer, Kyle and {Prabhat} and Wood, Frank},
  year = {2019},
  month = nov,
  series = {{{SC}}'19},
  pages = {1--24},
  address = {{Denver Colorado}},
  langid = {english},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\7KAL9GUM\\Baydin et al. - 2019 - Etalumis bringing probabilistic programming to sc.pdf}
}

@article{beamer_gap_2017,
  title = {The {{GAP}} Benchmark Suite},
  author = {Beamer, Scott and Asanovi{\'c}, Krste and Patterson, David},
  year = {2017},
  month = may,
  journal = {arXiv:1508.03619 [cs]},
  eprint = {1508.03619},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {We present a graph processing benchmark suite with the goal of helping to standardize graph processing evaluations. Fewer differences between graph processing evaluations will make it easier to compare different research efforts and quantify improvements. The benchmark not only specifies graph kernels, input graphs, and evaluation methodologies, but it also provides optimized baseline implementations. These baseline implementations are representative of state-of-the-art performance, and thus new contributions should outperform them to demonstrate an improvement. The input graphs are sized appropriately for shared memory platforms, but any implementation on any platform that conforms to the benchmark's specifications could be compared. This benchmark suite can be used in a variety of settings. Graph framework developers can demonstrate the generality of their programming model by implementing all of the benchmark's kernels and delivering competitive performance on all of the benchmark's graphs. Algorithm designers can use the input graphs and the baseline implementations to demonstrate their contribution. Platform designers and performance analysts can use the suite as a workload representative of graph processing.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Data Structures and Algorithms,Computer Science - Distributed; Parallel; and Cluster Computing},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\SLDCD2QR\\Beamer et al. - 2017 - The GAP Benchmark Suite.pdf;C\:\\Users\\msca8h\\Zotero\\storage\\YRQLAWQG\\1508.html}
}

@inproceedings{beckingsale_apollo_2017,
  title = {Apollo: {{Reusable Models}} for {{Fast}}, {{Dynamic Tuning}} of {{Input-Dependent Code}}},
  shorttitle = {Apollo},
  booktitle = {2017 {{IEEE International Parallel}} and {{Distributed Processing Symposium}} ({{IPDPS}})},
  author = {Beckingsale, David and Pearce, Olga and Laguna, Ignacio and Gamblin, Todd},
  year = {2017},
  month = may,
  pages = {307--316},
  publisher = {{IEEE}},
  address = {{Orlando, FL, USA}}
}

@inproceedings{beckingsale_raja_2019,
  title = {{{RAJA}}: {{Portable Performance}} for {{Large-Scale Scientific Applications}}},
  shorttitle = {{{RAJA}}},
  booktitle = {2019 {{IEEE}}/{{ACM International Workshop}} on {{Performance}}, {{Portability}} and {{Productivity}} in {{HPC}} ({{P3HPC}})},
  author = {Beckingsale, David A. and Scogland, Thomas RW and Burmark, Jason and Hornung, Rich and Jones, Holger and Killian, William and Kunen, Adam J. and Pearce, Olga and Robinson, Peter and Ryujin, Brian S.},
  year = {2019},
  month = nov,
  pages = {71--81},
  publisher = {{IEEE}},
  address = {{Denver, CO, USA}},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\QL5ZBNKA\\Beckingsale et al. - 2019 - RAJA Portable Performance for Large-Scale Scienti.pdf}
}

@phdthesis{beczkowski_control_2012,
  title = {Control and Driving Methods for {{LED}} Based Intelligent Light Sources},
  author = {Beczkowski, Szymon},
  year = {2012},
  school = {Aalborg Universitet}
}

@article{beheshti_number_2018,
  title = {Number of Source Signal Estimation by the Mean Squared Eigenvalue Error},
  author = {Beheshti, Soosan and Sedghizadeh, Saba},
  year = {2018},
  month = nov,
  journal = {IEEE Transactions on Signal Processing},
  volume = {66},
  number = {21},
  pages = {5694--5704}
}

@inproceedings{behzad_dynamic_2015,
  title = {Dynamic {{Model-Driven Parallel I}}/{{O Performance Tuning}}},
  booktitle = {2015 {{IEEE International Conference}} on {{Cluster Computing}}},
  author = {Behzad, Babak and Byna, Surendra and Wild, Stefan M. and {Prabhat} and Snir, Marc},
  year = {2015},
  month = sep,
  pages = {184--193},
  publisher = {{IEEE}},
  address = {{Chicago, IL, USA}}
}

@article{behzad_optimizing_2019,
  title = {Optimizing {{I}}/{{O Performance}} of {{HPC Applications}} with {{Autotuning}}},
  author = {Behzad, Babak and Byna, Surendra and {Prabhat} and Snir, Marc},
  year = {2019},
  month = mar,
  journal = {ACM Transactions on Parallel Computing},
  volume = {5},
  number = {4},
  pages = {1--27},
  langid = {english}
}

@inproceedings{belilovsky_greedy_2019,
  title = {Greedy {{Layerwise Learning Can Scale To ImageNet}}},
  booktitle = {Proceedings of the 36th {{International Conference}} on {{Machine Learning}}},
  author = {Belilovsky, Eugene and Eickenberg, Michael and Oyallon, Edouard},
  year = {2019},
  month = jun,
  series = {Proceedings of {{Machine Learning Research}}},
  volume = {97},
  pages = {583--593},
  publisher = {{PMLR}},
  address = {{Long Beach, California, USA}},
  abstract = {Shallow supervised 1-hidden layer neural networks have a number of favorable properties that make them easier to interpret, analyze, and optimize than their deep counterparts, but lack their representational power. Here we use 1-hidden layer learning problems to sequentially build deep networks layer by layer, which can inherit properties from shallow networks. Contrary to previous approaches using shallow networks, we focus on problems where deep learning is reported as critical for success. We thus study CNNs on image classification tasks using the large-scale ImageNet dataset and the CIFAR-10 dataset. Using a simple set of ideas for architecture and training we find that solving sequential 1-hidden-layer auxiliary problems lead to a CNN that exceeds AlexNet performance on ImageNet. Extending this training methodology to construct individual layers by solving 2-and-3-hidden layer auxiliary problems, we obtain an 11-layer network that exceeds several members of the VGG model family on ImageNet, and can train a VGG-11 model to the same accuracy as end-to-end learning. To our knowledge, this is the first competitive alternative to end-to-end training of CNNs that can scale to ImageNet. We illustrate several interesting properties of these models and conduct a range of experiments to study the properties this training induces on the intermediate layers.}
}

@article{bell_bayesian_2000,
  title = {A {{Bayesian}} Approach to Robust Adaptive Beamforming},
  author = {Bell, K.L. and Ephraim, Y. and Van Trees, H.L.},
  year = {Feb./2000},
  journal = {IEEE Transactions on Signal Processing},
  volume = {48},
  number = {2},
  pages = {386--398}
}

@inproceedings{bell_challenge_2020,
  title = {Challenge on {{Ultrasound Beamforming}} with {{Deep Learning}} ({{CUBDL}})},
  booktitle = {2020 {{IEEE International Ultrasonics Symposium}} ({{IUS}})},
  author = {Bell, Muyinatu A. Lediju and Huang, Jiaqi and Hyun, Dongwoon and Eldar, Yonina C. and {van Sloun}, Ruud and Mischi, Massimo},
  year = {2020},
  month = sep,
  pages = {1--5},
  publisher = {{IEEE}},
  address = {{Las Vegas, NV, USA}}
}

@article{bell_simulation_1997,
  title = {Simulation and Analysis of Synthetic Sidescan Sonar Images},
  author = {Bell, J.M. and Linnett, L.M.},
  year = {1997},
  journal = {IEE Proceedings - Radar, Sonar and Navigation},
  volume = {144},
  number = {4},
  pages = {219},
  langid = {english}
}

@article{bellettini_theoretical_2002,
  title = {Theoretical Accuracy of Synthetic Aperture Sonar Micronavigation Using a Displaced Phase-Center Antenna},
  author = {Bellettini, A. and Pinto, M.A.},
  year = {2002},
  month = oct,
  journal = {IEEE Journal of Oceanic Engineering},
  volume = {27},
  number = {4},
  pages = {780--789},
  langid = {english}
}

@article{ben-nun_demystifying_2018,
  title = {Demystifying {{Parallel}} and {{Distributed Deep Learning}}: {{An In-Depth Concurrency Analysis}}},
  shorttitle = {Demystifying {{Parallel}} and {{Distributed Deep Learning}}},
  author = {{Ben-Nun}, Tal and Hoefler, Torsten},
  year = {2018},
  month = feb,
  journal = {arXiv:1802.09941 [cs]},
  eprint = {1802.09941},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Deep Neural Networks (DNNs) are becoming an important tool in modern computing applications. Accelerating their training is a major challenge and techniques range from distributed algorithms to low-level circuit design. In this survey, we describe the problem from a theoretical perspective, followed by approaches for its parallelization. We present trends in DNN architectures and the resulting implications on parallelization strategies. We then review and model the different types of concurrency in DNNs: from the single operator, through parallelism in network inference and training, to distributed deep learning. We discuss asynchronous stochastic optimization, distributed system architectures, communication schemes, and neural architecture search. Based on those approaches, we extrapolate potential directions for parallelism in deep learning.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Distributed; Parallel; and Cluster Computing,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\FC6L9TTY\\Ben-Nun and Hoefler - 2018 - Demystifying Parallel and Distributed Deep Learnin.pdf;C\:\\Users\\msca8h\\Zotero\\storage\\SS9CEJVR\\1802.html}
}

@article{ben-nun_demystifying_2018a,
  title = {Demystifying {{Parallel}} and {{Distributed Deep Learning}}: {{An In-Depth Concurrency Analysis}}},
  shorttitle = {Demystifying {{Parallel}} and {{Distributed Deep Learning}}},
  author = {{Ben-Nun}, Tal and Hoefler, Torsten},
  year = {2018},
  month = feb,
  journal = {arXiv:1802.09941 [cs]},
  eprint = {1802.09941},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Deep Neural Networks (DNNs) are becoming an important tool in modern computing applications. Accelerating their training is a major challenge and techniques range from distributed algorithms to low-level circuit design. In this survey, we describe the problem from a theoretical perspective, followed by approaches for its parallelization. We present trends in DNN architectures and the resulting implications on parallelization strategies. We then review and model the different types of concurrency in DNNs: from the single operator, through parallelism in network inference and training, to distributed deep learning. We discuss asynchronous stochastic optimization, distributed system architectures, communication schemes, and neural architecture search. Based on those approaches, we extrapolate potential directions for parallelism in deep learning.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Distributed; Parallel; and Cluster Computing,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\P87CUQF8\\Ben-Nun and Hoefler - 2018 - Demystifying Parallel and Distributed Deep Learnin.pdf;C\:\\Users\\msca8h\\Zotero\\storage\\Y96D3EKI\\1802.html}
}

@incollection{benassi_bayesian_2012,
  title = {Bayesian {{Optimization Using Sequential Monte Carlo}}},
  booktitle = {Learning and {{Intelligent Optimization}}},
  author = {Benassi, Romain and Bect, Julien and Vazquez, Emmanuel},
  year = {2012},
  volume = {7219},
  pages = {339--342},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  langid = {english},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\PAI2I9NZ\\Benassi et al. - 2012 - Bayesian Optimization Using Sequential Monte Carlo.pdf}
}

@incollection{bengio_greedy_2007,
  title = {Greedy {{Layer-Wise Training}} of {{Deep Networks}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 19},
  author = {Bengio, Yoshua and Lamblin, Pascal and Popovici, Dan and Larochelle, Hugo},
  year = {2007},
  pages = {153--160},
  publisher = {{MIT Press}}
}

@article{benjamini_controlling_1995,
  title = {Controlling the False Discovery Rate: {{A}} Practical and Powerful Approach to Multiple Testing},
  shorttitle = {Controlling the {{False Discovery Rate}}},
  author = {Benjamini, Yoav and Hochberg, Yosef},
  year = {1995},
  journal = {Journal of the Royal Statistical Society: Series B (Methodological)},
  volume = {57},
  number = {1},
  pages = {289--300},
  abstract = {The common approach to the multiplicity problem calls for controlling the familywise error rate (FWER). This approach, though, has faults, and we point out a few. A different approach to problems of multiple significance testing is presented. It calls for controlling the expected proportion of falsely rejected hypotheses \textemdash{} the false discovery rate. This error rate is equivalent to the FWER when all hypotheses are true but is smaller otherwise. Therefore, in problems where the control of the false discovery rate rather than that of the FWER is desired, there is potential for a gain in power. A simple sequential Bonferronitype procedure is proved to control the false discovery rate for independent test statistics, and a simulation study shows that the gain in power is substantial. The use of the new procedure and the appropriateness of the criterion are illustrated with examples.},
  langid = {english},
  keywords = {bonferroni-type procedures,familywise error rate,multiple-comparison procedures,p-values},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.2517-6161.1995.tb02031.x},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\G8ENATJ7\\j.2517-6161.1995.tb02031.html}
}

@book{berger_likelihood_1988,
  title = {The Likelihood Principle: {{A}} Review, Generalizations, and Statistical Implications},
  author = {Berger, James O. and Wolpert, Robert L.},
  year = {1988},
  series = {Institute of {{Mathematical Statistics Lecture Notes}} - {{Monograph Series}}},
  number = {6}
}

@inproceedings{bergstra_machine_2012,
  title = {Machine Learning for Predictive Auto-Tuning with Boosted Regression Trees},
  booktitle = {2012 {{Innovative Parallel Computing}} ({{InPar}})},
  author = {Bergstra, James and Pinto, Nicolas and Cox, David},
  year = {2012},
  month = may,
  pages = {1--9},
  publisher = {{IEEE}},
  address = {{San Jose, CA, USA}}
}

@article{berkenkamp_noregret_2019,
  title = {No-{{Regret Bayesian Optimization}} with {{Unknown Hyperparameters}}},
  author = {Berkenkamp, Felix and Schoellig, Angela P. and Krause, Andreas},
  year = {2019},
  journal = {Journal of Machine Learning Research},
  volume = {20},
  number = {50},
  pages = {1--24}
}

@article{berkenkamp_noregret_2019a,
  title = {No-{{Regret Bayesian Optimization}} with {{Unknown Hyperparameters}}},
  author = {Berkenkamp, Felix and Schoellig, Angela P. and Krause, Andreas},
  year = {2019},
  journal = {Journal of Machine Learning Research},
  volume = {20},
  number = {50},
  pages = {1--24}
}

@article{berkenkamp_noregret_2019b,
  title = {No-{{Regret Bayesian Optimization}} with {{Unknown Hyperparameters}}},
  author = {Berkenkamp, Felix and Schoellig, Angela P and Krause, Andreas},
  year = {2019},
  journal = {Journal of Machine Learning Research},
  volume = {20},
  pages = {50}
}

@book{bernardo_bayesian_2004,
  title = {Bayesian Theory},
  author = {Bernardo, Jos{\'e} M. and Smith, Adrian F. M.},
  year = {2004},
  series = {Wiley Series in Probability and Statistics},
  edition = {Repr},
  publisher = {{Wiley}},
  address = {{Chichester Weinheim}}
}

@inproceedings{bernstein_differentially_2018,
  title = {Differentially Private {{Bayesian}} Inference for Exponential Families},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Bernstein, Garrett and Sheldon, Daniel R},
  year = {2018},
  volume = {31},
  publisher = {{Curran Associates, Inc.}},
  abstract = {The study of private inference has been sparked by growing concern regarding the analysis of data when it stems from sensitive sources. We present the first method for private Bayesian inference in exponential families that properly accounts for noise introduced by the privacy mechanism. It is efficient because it works only with sufficient statistics and not individual data. Unlike other methods, it gives properly calibrated posterior beliefs in the non-asymptotic data regime.},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\8J742KGC\\Bernstein and Sheldon - 2018 - Differentially Private Bayesian Inference for Expo.pdf}
}

@article{bernton_locally_2015,
  title = {Locally Weighted {{Markov}} Chain {{Monte Carlo}}},
  author = {Bernton, Espen and Yang, Shihao and Chen, Yang and Shephard, Neil and Liu, Jun S.},
  year = {2015},
  month = jun,
  journal = {arXiv:1506.08852 [stat]},
  eprint = {1506.08852},
  eprinttype = {arxiv},
  primaryclass = {stat},
  abstract = {We propose a weighting scheme for the proposals within Markov chain Monte Carlo algorithms and show how this can improve statistical efficiency at no extra computational cost. These methods are most powerful when combined with multi-proposal MCMC algorithms such as multiple-try Metropolis, which can efficiently exploit modern computer architectures with large numbers of cores. The locally weighted Markov chain Monte Carlo method also improves upon a partial parallelization of the Metropolis-Hastings algorithm via Rao-Blackwellization. We derive the effective sample size of the output of our algorithm and show how to estimate this in practice. Illustrations and examples of the method are given and the algorithm is compared in theory and applications with existing methods.},
  archiveprefix = {arXiv},
  keywords = {Statistics - Computation},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\M2W2VEPJ\\Bernton et al. - 2015 - Locally weighted Markov chain Monte Carlo.pdf;C\:\\Users\\msca8h\\Zotero\\storage\\ZPUC7D2R\\1506.html}
}

@article{beskos_convergence_2016,
  title = {On the Convergence of Adaptive Sequential {{Monte Carlo}} Methods},
  author = {Beskos, Alexandros and Jasra, Ajay and Kantas, Nikolas and Thiery, Alexandre},
  year = {2016},
  month = apr,
  journal = {The Annals of Applied Probability},
  volume = {26},
  number = {2},
  pages = {1111--1146},
  langid = {english},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\CVPC6354\\Beskos et al. - 2016 - On the convergence of adaptive sequential Monte Ca.pdf}
}

@article{beskos_optimal_2013,
  title = {Optimal Tuning of the Hybrid {{Monte Carlo}} Algorithm},
  author = {Beskos, Alexandros and Pillai, Natesh and Roberts, Gareth and {Sanz-Serna}, Jesus-Maria and Stuart, Andrew},
  year = {2013},
  month = nov,
  journal = {Bernoulli},
  volume = {19},
  number = {5A},
  pages = {1501--1534},
  langid = {english},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\TWGYDNM6\\Beskos et al. - 2013 - Optimal tuning of the hybrid Monte Carlo algorithm.pdf}
}

@misc{betancourt_bayes_2018,
  title = {Bayes {{Sparse Regression}}},
  author = {Betancourt, Michael},
  year = {2018},
  month = mar,
  abstract = {As complex measurements convolve meaningful phenomena with more and more extraneous phenomena, sparsity is becoming an increasingly prevalent objective in statistical analyses. Much of this zeitgeist has been driven by the success of frequentist methods like compressed sensing and LASSO regression, and it is often naively assumed that these properties immediately carry over to the corresponding Bayesian analyses. Sparsity in a Bayesian analysis, however, is induced by fundamentally different properties than those that induce sparsity in a frequentist analysis. In this case study I'll review how sparsity arises in frequentist and Bayesian analyses and discuss the often subtle challenges in implementing sparsity in practical Bayesian analyses.}
}

@article{betancourt_conceptual_2017,
  title = {A Conceptual Introduction to {{Hamiltonian Monte Carlo}}},
  author = {Betancourt, Michael},
  year = {2017},
  month = jan,
  journal = {arXiv:1701.02434 [stat]},
  eprint = {1701.02434},
  eprinttype = {arxiv},
  primaryclass = {stat},
  abstract = {Hamiltonian Monte Carlo has proven a remarkable empirical success, but only recently have we begun to develop a rigorous understanding of why it performs so well on difficult problems and how it is best applied in practice. Unfortunately, that understanding is confined within the mathematics of differential geometry which has limited its dissemination, especially to the applied communities for which it is particularly important. In this review I provide a comprehensive conceptual account of these theoretical foundations, focusing on developing a principled intuition behind the method and its optimal implementations rather of any exhaustive rigor. Whether a practitioner or a statistician, the dedicated reader will acquire a solid grasp of how Hamiltonian Monte Carlo works, when it succeeds, and, perhaps most importantly, when it fails.},
  archiveprefix = {arXiv},
  keywords = {Statistics - Methodology},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\P6BHN8X4\\Betancourt - 2017 - A Conceptual Introduction to Hamiltonian Monte Car.pdf;C\:\\Users\\msca8h\\Zotero\\storage\\KZ3MH7EI\\1701.html}
}

@article{betancourt_fundamental_2015,
  title = {The Fundamental Incompatibility of {{Hamiltonian Monte Carlo}} and Data Subsampling},
  author = {Betancourt, M. J.},
  year = {2015},
  month = feb,
  journal = {arXiv:1502.01510 [stat]},
  eprint = {1502.01510},
  eprinttype = {arxiv},
  primaryclass = {stat},
  abstract = {Leveraging the coherent exploration of Hamiltonian flow, Hamiltonian Monte Carlo produces computationally efficient Monte Carlo estimators, even with respect to complex and high-dimensional target distributions. When confronted with data-intensive applications, however, the algorithm may be too expensive to implement, leaving us to consider the utility of approximations such as data subsampling. In this paper I demonstrate how data subsampling fundamentally compromises the efficient exploration of Hamiltonian flow and hence the scalable performance of Hamiltonian Monte Carlo itself.},
  archiveprefix = {arXiv},
  keywords = {Statistics - Methodology},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\74MTU2DC\\Betancourt - 2015 - The Fundamental Incompatibility of Hamiltonian Mon.pdf;C\:\\Users\\msca8h\\Zotero\\storage\\UULTNRI9\\1502.html}
}

@misc{betancourt_hierarchical_2020,
  title = {Hierarchical {{Modeling}}},
  author = {Betancourt, Michael},
  year = {2020},
  month = nov
}

@article{betancourt_identifying_2016,
  title = {Identifying the Optimal Integration Time in {{Hamiltonian Monte Carlo}}},
  author = {Betancourt, Michael},
  year = {2016},
  month = jan,
  journal = {arXiv:1601.00225 [stat]},
  eprint = {1601.00225},
  eprinttype = {arxiv},
  primaryclass = {stat},
  abstract = {By leveraging the natural geometry of a smooth probabilistic system, Hamiltonian Monte Carlo yields computationally efficient Markov Chain Monte Carlo estimation. At least provided that the algorithm is sufficiently well-tuned. In this paper I show how the geometric foundations of Hamiltonian Monte Carlo implicitly identify the optimal choice of these parameters, especially the integration time. I then consider the practical consequences of these principles in both existing algorithms and a new implementation called \textbackslash emph\{Exhaustive Hamiltonian Monte Carlo\} before demonstrating the utility of these ideas in some illustrative examples.},
  archiveprefix = {arXiv},
  keywords = {Statistics - Computation,Statistics - Methodology},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\6I836MJZ\\Betancourt - 2016 - Identifying the Optimal Integration Time in Hamilt.pdf;C\:\\Users\\msca8h\\Zotero\\storage\\82GALAQV\\1601.html}
}

@misc{betancourt_identity_2020,
  title = {Identity Crisis},
  author = {Betancourt, Michael},
  year = {2020},
  month = jun,
  abstract = {Under ideal conditions only a small neighborhood of model configurations will be consistent with both the observed data and the domain expertise we encode in our prior model, resulting in a posterior distribution that strongly concentrates along each parameter. This not only yields precise inferences and but also facilitates accurate estimation of those inferences. Under more realistic conditions, however, our measurements and domain expertise can be much less informative, allowing our posterior distribution to stretch across more expansive, complex neighborhoods of the model configuration space. These intricate uncertainties complicate not only the utility of our inferences but also our ability to quantify those inferences computationally. In this case study we will explore the theoretical concept of identifiability and its more geometric counterpart degeneracy that better generalizes to applied statistical practice. We will also discuss the principled ways in which we can identify and then compensate for degenerate inferences before demonstrating these strategies in a series of pedagogical examples.}
}

@inproceedings{betancourt_nested_2011,
  title = {Nested Sampling with Constrained {{Hamiltonian Monte Carlo}}},
  booktitle = {Proceedings of the {{International Workshop}} on {{Bayesian Inference}} and {{Maximum Entropy Methods}} in {{Science}} and {{Engineering}}},
  author = {Betancourt, Michael and {Mohammad-Djafari}, Ali and Bercher, Jean-Fran{\c c}ois and Bessi{\'e}re, Pierre},
  year = {2011},
  series = {{{AIP Conference Proceedings}}},
  volume = {1305},
  pages = {165--172},
  address = {{Chamonix, (France)}},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\N7NTHL39\\Betancourt et al. - 2011 - Nested Sampling with Constrained Hamiltonian Monte.pdf}
}

@article{betancourt_optimizing_2015,
  title = {Optimizing the Integrator Step Size for {{Hamiltonian Monte Carlo}}},
  author = {Betancourt, M. J. and Byrne, Simon and Girolami, Mark},
  year = {2015},
  month = feb,
  journal = {arXiv:1411.6669 [math, stat]},
  eprint = {1411.6669},
  eprinttype = {arxiv},
  primaryclass = {math, stat},
  abstract = {Hamiltonian Monte Carlo can provide powerful inference in complex statistical problems, but ultimately its performance is sensitive to various tuning parameters. In this paper we use the underlying geometry of Hamiltonian Monte Carlo to construct a universal optimization criteria for tuning the step size of the symplectic integrator crucial to any implementation of the algorithm as well as diagnostics to monitor for any signs of invalidity. An immediate outcome of this result is that the suggested target average acceptance probability of 0.651 can be relaxed to \$0.6 \textbackslash lesssim a \textbackslash lesssim 0.9\$ with larger values more robust in practice.},
  archiveprefix = {arXiv},
  keywords = {Mathematics - Statistics Theory,Statistics - Methodology},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\KSFYXYRW\\Betancourt et al. - 2015 - Optimizing The Integrator Step Size for Hamiltonia.pdf;C\:\\Users\\msca8h\\Zotero\\storage\\6A8M6UH9\\1411.html}
}

@article{bezanson_julia_2017,
  title = {Julia: {{A}} Fresh Approach to Numerical Computing},
  author = {Bezanson, Jeff and Edelman, Alan and Karpinski, Stefan and Shah, Viral B},
  year = {2017},
  journal = {SIAM review},
  volume = {59},
  number = {1},
  pages = {65--98}
}

@article{bhatia_better_2000,
  title = {A Better Bound on the Variance},
  author = {Bhatia, Rajendra and Davis, Chandler},
  year = {2000},
  month = apr,
  journal = {The American Mathematical Monthly},
  volume = {107},
  number = {4},
  pages = {353--357},
  langid = {english}
}

@misc{bhatia_statistical_2022,
  title = {Statistical and Computational Trade-Offs in Variational Inference: A Case Study in Inferential Model Selection},
  shorttitle = {Statistical and Computational Trade-Offs in Variational Inference},
  author = {Bhatia, Kush and Kuang, Nikki Lijing and Ma, Yi-An and Wang, Yixin},
  year = {2022},
  month = jul,
  number = {arXiv:2207.11208},
  eprint = {2207.11208},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  institution = {{arXiv}},
  abstract = {Variational inference has recently emerged as a popular alternative to the classical Markov chain Monte Carlo (MCMC) in large-scale Bayesian inference. The core idea of variational inference is to trade statistical accuracy for computational efficiency. It aims to approximate the posterior, reducing computation costs but potentially compromising its statistical accuracy. In this work, we study this statistical and computational trade-off in variational inference via a case study in inferential model selection. Focusing on Gaussian inferential models (a.k.a. variational approximating families) with diagonal plus low-rank precision matrices, we initiate a theoretical study of the trade-offs in two aspects, Bayesian posterior inference error and frequentist uncertainty quantification error. From the Bayesian posterior inference perspective, we characterize the error of the variational posterior relative to the exact posterior. We prove that, given a fixed computation budget, a lower-rank inferential model produces variational posteriors with a higher statistical approximation error, but a lower computational error; it reduces variances in stochastic optimization and, in turn, accelerates convergence. From the frequentist uncertainty quantification perspective, we consider the precision matrix of the variational posterior as an uncertainty estimate. We find that, relative to the true asymptotic precision, the variational approximation suffers from an additional statistical error originating from the sampling uncertainty of the data. Moreover, this statistical error becomes the dominant factor as the computation budget increases. As a consequence, for small datasets, the inferential model need not be full-rank to achieve optimal estimation error. We finally demonstrate these statistical and computational trade-offs inference across empirical studies, corroborating the theoretical findings.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\PSQ7EZKD\\Bhatia et al. - 2022 - Statistical and Computational Trade-offs in Variat.pdf;C\:\\Users\\msca8h\\Zotero\\storage\\FLCPJC8U\\2207.html}
}

@inproceedings{bhattacharyya_pemogen_2014,
  title = {{{PEMOGEN}}: Automatic Adaptive Performance Modeling during Program Runtime},
  shorttitle = {{{PEMOGEN}}},
  booktitle = {Proceedings of the 23rd International Conference on {{Parallel}} Architectures and Compilation - {{PACT}} '14},
  author = {Bhattacharyya, Arnamoy and Hoefler, Torsten},
  year = {2014},
  pages = {393--404},
  publisher = {{ACM Press}},
  address = {{Edmonton, AB, Canada}},
  langid = {english}
}

@article{bi_image_2015,
  title = {An L_1 Image Transform for Edge-Preserving Smoothing and Scene-Level Intrinsic Decomposition},
  author = {Bi, Sai and Han, Xiaoguang and Yu, Yizhou},
  year = {2015},
  month = jul,
  journal = {ACM Transactions on Graphics},
  volume = {34},
  number = {4},
  pages = {1--12},
  langid = {english}
}

@inproceedings{bianchi_robust_2014,
  title = {Robust Beamforming under Uncertainties in the Loudspeakers Directivity Pattern},
  booktitle = {2014 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Bianchi, L. and Magalotti, R. and Antonacci, F. and Sarti, A. and Tubaro, S.},
  year = {2014},
  month = may,
  pages = {4448--4452},
  publisher = {{IEEE}},
  address = {{Florence, Italy}}
}

@article{bierkens_nonreversible_2016,
  title = {Non-Reversible {{Metropolis-Hastings}}},
  author = {Bierkens, Joris},
  year = {2016},
  month = nov,
  journal = {Statistics and Computing},
  volume = {26},
  number = {6},
  pages = {1213--1228},
  abstract = {The classical Metropolis-Hastings (MH) algorithm can be extended to generate non-reversible Markov chains. This is achieved by means of a modification of the acceptance probability, using the notion of vorticity matrix. The resulting Markov chain is non-reversible. Results from the literature on asymptotic variance, large deviations theory and mixing time are mentioned, and in the case of a large deviations result, adapted, to explain how non-reversible Markov chains have favorable properties in these respects. We provide an application of NRMH in a continuous setting by developing the necessary theory and applying, as first examples, the theory to Gaussian distributions in three and nine dimensions. The empirical autocorrelation and estimated asymptotic variance for NRMH applied to these examples show significant improvement compared to MH with identical stepsize.},
  langid = {english},
  keywords = {60J20,65C40,Asymptotic variance,Langevin sampling,Large deviations,Markov Chain Monte Carlo,MCMC,Metropolis-Hastings,Non-reversible Markov processes},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\4CUJ2I3E\\Bierkens - 2016 - Non-reversible Metropolis-Hastings.pdf}
}

@article{bingham_pyro_2019,
  title = {Pyro: {{Deep}} Universal Probabilistic Programming},
  shorttitle = {Pyro},
  author = {Bingham, Eli and Chen, Jonathan P. and Jankowiak, Martin and Obermeyer, Fritz and Pradhan, Neeraj and Karaletsos, Theofanis and Singh, Rohit and Szerlip, Paul and Horsfall, Paul and Goodman, Noah D.},
  year = {2019},
  journal = {Journal of Machine Learning Research},
  volume = {20},
  number = {28},
  pages = {1--6},
  abstract = {Pyro is a probabilistic programming language built on Python as a platform for developing advanced probabilistic models in AI research. To scale to large data sets and high-dimensional models, Pyro uses stochastic variational inference algorithms and probability distributions built on top of PyTorch, a modern GPU-accelerated deep learning framework. To accommodate complex or model-specific algorithmic behavior, Pyro leverages Poutine, a library of composable building blocks for modifying the behavior of probabilistic programs.},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\BFM4DYC7\\Bingham et al. - 2019 - Pyro Deep Universal Probabilistic Programming.pdf;C\:\\Users\\msca8h\\Zotero\\storage\\5QA3D2I7\\pyro.html}
}

@article{bini_despeckling_2014,
  title = {Despeckling Low {{SNR}}, Low Contrast Ultrasound Images via Anisotropic Level Set Diffusion},
  author = {Bini, A. A. and Bhat, M. S.},
  year = {2014},
  month = jan,
  journal = {Multidimensional Systems and Signal Processing},
  volume = {25},
  number = {1},
  pages = {41--65},
  langid = {english}
}

@article{birkeneslonmo_improving_2020,
  title = {Improving Swath Sonar Water Column Imagery and Bathymetry with Adaptive Beamforming},
  author = {Birkenes Lonmo, Tor Inge and Austeng, Andreas and Hansen, Roy Edgar},
  year = {2020},
  month = oct,
  journal = {IEEE Journal of Oceanic Engineering},
  volume = {45},
  number = {4},
  pages = {1552--1563},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\SRH9YXFU\\Birkenes Lonmo et al. - 2020 - Improving Swath Sonar Water Column Imagery and Bat.pdf}
}

@inproceedings{bishop_bayesian_1998,
  title = {Bayesian {{PCA}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Bishop, Christopher},
  year = {1998},
  volume = {11},
  publisher = {{MIT Press}},
  abstract = {The technique of principal component analysis (PCA) has recently been  expressed  as  the  maximum  likelihood  solution  for  a  generative  latent  variable  model.  In  this  paper  we  use  this  probabilistic  reformulation  as  the basis  for a  Bayesian treatment of PCA.  Our key  result  is  that  ef(cid:173) fective  dimensionality  of the  latent  space  (equivalent to  the  number of  retained principal components) can be determined automatically as  part  of the  Bayesian  inference  procedure.  An  important application  of this  framework  is  to  mixtures  of probabilistic  PCA  models,  in  which  each  component can determine its own effective complexity.},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\PMKWCJ8X\\Bishop - 1998 - Bayesian PCA.pdf}
}

@article{blei_variational_2006,
  title = {Variational Inference for {{Dirichlet}} Process Mixtures},
  author = {Blei, David M. and Jordan, Michael I.},
  year = {2006},
  month = mar,
  journal = {Bayesian Analysis},
  volume = {1},
  number = {1},
  pages = {121--143},
  publisher = {{International Society for Bayesian Analysis}},
  abstract = {Dirichlet process (DP) mixture models are the cornerstone of nonparametric Bayesian statistics, and the development of Monte-Carlo Markov chain (MCMC) sampling methods for DP mixtures has enabled the application of nonparametric Bayesian methods to a variety of practical data analysis problems. However, MCMC sampling can be prohibitively slow, and it is important to explore alternatives. One class of alternatives is provided by variational methods, a class of deterministic algorithms that convert inference problems into optimization problems (Opper and Saad 2001; Wainwright and Jordan 2003). Thus far, variational methods have mainly been explored in the parametric setting, in particular within the formalism of the exponential family (Attias 2000; Ghahramani and Beal 2001; Blei et al. 2003). In this paper, we present a variational inference algorithm for DP mixtures. We present experiments that compare the algorithm to Gibbs sampling algorithms for DP mixtures of Gaussians and present an application to a large-scale image analysis problem.},
  keywords = {Bayesian computation,Dirichlet processes,hierarchical models,image processing,variational inference},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\I2HEV4IF\\Blei and Jordan - 2006 - Variational inference for Dirichlet process mixtur.pdf;C\:\\Users\\msca8h\\Zotero\\storage\\WLX3SAXG\\06-BA104.html}
}

@article{blei_variational_2017,
  title = {Variational Inference: {{A}} Review for Statisticians},
  shorttitle = {Variational {{Inference}}},
  author = {Blei, David M. and Kucukelbir, Alp and McAuliffe, Jon D.},
  year = {2017},
  month = apr,
  journal = {Journal of the American Statistical Association},
  volume = {112},
  number = {518},
  pages = {859--877},
  publisher = {{Taylor \& Francis}},
  abstract = {One of the core problems of modern statistics is to approximate difficult-to-compute probability densities. This problem is especially important in Bayesian statistics, which frames all inference about unknown quantities as a calculation involving the posterior density. In this article, we review variational inference (VI), a method from machine learning that approximates probability densities through optimization. VI has been used in many applications and tends to be faster than classical methods, such as Markov chain Monte Carlo sampling. The idea behind VI is to first posit a family of densities and then to find a member of that family which is close to the target density. Closeness is measured by Kullback\textendash Leibler divergence. We review the ideas behind mean-field variational inference, discuss the special case of VI applied to exponential family models, present a full example with a Bayesian mixture of Gaussians, and derive a variant that uses stochastic optimization to scale up to massive data. We discuss modern research in VI and highlight important open problems. VI is powerful, but it is not yet well understood. Our hope in writing this article is to catalyze statistical research on this class of algorithms. Supplementary materials for this article are available online.},
  keywords = {Algorithms,Computationally intensive methods,Statistical computing},
  annotation = {\_eprint: https://doi.org/10.1080/01621459.2017.1285773},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\9ESTVJPQ\\Blei et al. - 2017 - Variational Inference A Review for Statisticians.pdf;C\:\\Users\\msca8h\\Zotero\\storage\\5PLSQZTG\\01621459.2017.html}
}

@article{blei_variational_2017a,
  title = {Variational Inference: {{A}} Review for Statisticians},
  shorttitle = {Variational {{Inference}}},
  author = {Blei, David M. and Kucukelbir, Alp and McAuliffe, Jon D.},
  year = {2017},
  month = apr,
  journal = {Journal of the American Statistical Association},
  volume = {112},
  number = {518},
  pages = {859--877},
  langid = {english},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\NM6WQLJH\\Blei et al. - 2017 - Variational Inference A Review for Statisticians.pdf}
}

@article{blok_comments_2013,
  title = {Comments on ``{{Closed}} Form Variable Fractional Time Delay Using {{FFT}}''},
  author = {Blok, M.},
  year = {2013},
  month = aug,
  journal = {IEEE Signal Processing Letters},
  volume = {20},
  number = {8},
  pages = {747--750}
}

@phdthesis{blomberg_adaptive_2012,
  type = {Doctoral {{Thesis}}},
  title = {Adaptive Beamforming for Active Sonar Imaging},
  author = {Blomberg, Ann E. A.},
  year = {2012},
  school = {University of Oslo},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\I3LFUWU5\\Blomberg - Adaptive beamforming for active sonar imaging.pdf}
}

@article{blomberg_adaptive_2013,
  title = {Adaptive Sonar Imaging Using Aperture Coherence},
  author = {Blomberg, Ann Elisabeth Albright and Nilsen, Carl-Inge Colombo and Austeng, Andreas and Hansen, Roy Edgar},
  year = {2013},
  month = jan,
  journal = {IEEE Journal of Oceanic Engineering},
  volume = {38},
  number = {1},
  pages = {98--108}
}

@inproceedings{blomberg_apes_2009,
  title = {{{APES}} Beamforming Applied to Medical Ultrasound Imaging},
  booktitle = {2009 {{IEEE International Ultrasonics Symposium}}},
  author = {Blomberg, Ann E.A. and Holfort, Iben Kraglund and Austeng, Andreas and Synnevag, Johan-Fredrik and Holm, Sverre and Jensen, Jorgen Arendt},
  year = {2009},
  month = sep,
  pages = {2347--2350},
  publisher = {{IEEE}},
  address = {{Rome}},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\JDLMSDK7\\Blomberg et al. - 2009 - APES beamforming applied to medical ultrasound ima.pdf}
}

@article{blomberg_improving_2013,
  title = {Improving Sonar Performance in Shallow Water Using Adaptive Beamforming},
  author = {Blomberg, Ann Elisabeth Albright and Austeng, Andreas and Hansen, Roy Edgar and Synnes, Stig Asle Vaksvik},
  year = {2013},
  month = apr,
  journal = {IEEE Journal of Oceanic Engineering},
  volume = {38},
  number = {2},
  pages = {297--307}
}

@article{bodnar_bayesian_2017,
  title = {Bayesian Estimation of the Global Minimum Variance Portfolio},
  author = {Bodnar, Taras and Mazur, Stepan and Okhrin, Yarema},
  year = {2017},
  month = jan,
  journal = {European Journal of Operational Research},
  volume = {256},
  number = {1},
  pages = {292--307},
  langid = {english},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\A6BGLH7W\\Bodnar et al. - 2017 - Bayesian estimation of the global minimum variance.pdf}
}

@article{bogu_local_2013,
  title = {Local Edge-Preserving Multiscale Decomposition for High Dynamic Range Image Tone Mapping},
  author = {{Bo Gu} and {Wujing Li} and {Minyun Zhu} and {Minghui Wang}},
  year = {2013},
  month = jan,
  journal = {IEEE Transactions on Image Processing},
  volume = {22},
  number = {1},
  pages = {70--79}
}

@article{botev_markov_2013,
  title = {Markov Chain Importance Sampling with Applications to Rare Event Probability Estimation},
  author = {Botev, Zdravko I. and L'Ecuyer, Pierre and Tuffin, Bruno},
  year = {2013},
  month = mar,
  journal = {Statistics and Computing},
  volume = {23},
  number = {2},
  pages = {271--285},
  langid = {english}
}

@article{bottenus_resolution_2021,
  title = {Resolution and Speckle Reduction in Cardiac Imaging},
  author = {Bottenus, Nick and LeFevre, Melissa and Cleve, Jayne and Crowley, Anna Lisa and Trahey, Gregg},
  year = {2021},
  month = apr,
  journal = {IEEE Transactions on Ultrasonics, Ferroelectrics, and Frequency Control},
  volume = {68},
  number = {4},
  pages = {1131--1143}
}

@incollection{bottou_online_1999,
  title = {On-Line Learning and Stochastic Approximations},
  booktitle = {On-{{Line Learning}} in {{Neural Networks}}},
  author = {Bottou, L{\'e}on},
  year = {1999},
  month = jan,
  edition = {First},
  pages = {9--42},
  publisher = {{Cambridge University Press}},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\F9CTXQGD\\Bottou - 1999 - On-line Learning and Stochastic Approximations.pdf}
}

@article{bottou_optimization_2018,
  title = {Optimization Methods for Large-Scale Machine Learning},
  author = {Bottou, L{\'e}on and Curtis, Frank E. and Nocedal, Jorge},
  year = {2018},
  month = jan,
  journal = {SIAM Review},
  volume = {60},
  number = {2},
  pages = {223--311},
  langid = {english},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\D8EBG3MP\\Bottou et al. - 2018 - Optimization Methods for Large-Scale Machine Learn.pdf}
}

@article{bottou_optimization_2018a,
  title = {Optimization Methods for Large-Scale Machine Learning},
  author = {Bottou, L{\'e}on and Curtis, Frank E. and Nocedal, Jorge},
  year = {2018},
  month = jan,
  journal = {SIAM Review},
  volume = {60},
  number = {2},
  pages = {223--311},
  langid = {english},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\B5Q4DBVW\\Bottou et al. - 2018 - Optimization Methods for Large-Scale Machine Learn.pdf}
}

@article{braak_markov_2006,
  title = {A {{Markov Chain Monte Carlo}} Version of the Genetic Algorithm {{Differential Evolution}}: Easy {{Bayesian}} Computing for Real Parameter Spaces},
  shorttitle = {A {{Markov Chain Monte Carlo}} Version of the Genetic Algorithm {{Differential Evolution}}},
  author = {Braak, Cajo J. F. Ter},
  year = {2006},
  month = sep,
  journal = {Statistics and Computing},
  volume = {16},
  number = {3},
  pages = {239--249},
  langid = {english}
}

@article{bratley_algorithm_1988,
  title = {{{ALGORITHM}} 659: Implementing {{Sobol}}'s Quasirandom Sequence Generator},
  shorttitle = {{{ALGORITHM}} 659},
  author = {Bratley, Paul and Fox, Bennett L.},
  year = {1988},
  month = mar,
  journal = {ACM Transactions on Mathematical Software},
  volume = {14},
  number = {1},
  pages = {88--100}
}

@article{breivik_realtime_2017,
  title = {Real-{{Time}} Nonlocal Means-Based Despeckling},
  author = {Breivik, Lars Hofsoy and Snare, Sten Roar and Steen, Erik Normann and Solberg, Anne H. Schistad},
  year = {2017},
  month = jun,
  journal = {IEEE Transactions on Ultrasonics, Ferroelectrics, and Frequency Control},
  volume = {64},
  number = {6},
  pages = {959--977}
}

@inproceedings{brekelmans_all_2020,
  title = {All in the Exponential Family: {{Bregman}} Duality in Thermodynamic Variational Inference},
  shorttitle = {All in the {{Exponential Family}}},
  booktitle = {Proceedings of the {{International Conference}} on {{Machine Learning}}},
  author = {Brekelmans, Rob and Masrani, Vaden and Wood, Frank and Steeg, Greg Ver and Galstyan, Aram},
  year = {2020},
  month = nov,
  pages = {1111--1122},
  publisher = {{PMLR}},
  abstract = {The recently proposed Thermodynamic Variational Objective (TVO) leverages thermodynamic integration to provide a family of variational inference objectives, which both tighten and generalize the ubiquitous Evidence Lower Bound (ELBO). However, the tightness of TVO bounds was not previously known, an expensive grid search was used to choose a ``schedule'' of intermediate distributions, and model learning suffered with ostensibly tighter bounds. In this work, we propose an exponential family interpretation of the geometric mixture curve underlying the TVO and various path sampling methods, which allows us to characterize the gap in TVO likelihood bounds as a sum of KL divergences. We propose to choose intermediate distributions using equal spacing in the moment parameters of our exponential family, which matches grid search performance and allows the schedule to adaptively update over the course of training. Finally, we derive a doubly reparameterized gradient estimator which improves model learning and allows the TVO to benefit from more refined bounds. To further contextualize our contributions, we provide a unified framework for understanding thermodynamic integration and the TVO using Taylor series remainders.},
  langid = {english},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\3FJAQQKS\\Brekelmans et al. - 2020 - All in the Exponential Family Bregman Duality in .pdf;C\:\\Users\\msca8h\\Zotero\\storage\\MQBHG4MI\\Brekelmans et al. - 2020 - All in the Exponential Family Bregman Duality in .pdf}
}

@inproceedings{briles_realtime_2005,
  title = {Real-Time Implementation of an Adaptive Bayesian Beamformer},
  booktitle = {Proceedings of the {{IEEE}}/{{SP Workshop}} on {{Statistical Signal Processing}}},
  author = {Briles, S.D. and Arrowood, J. and Cases, T. and Turcotte, D. and Fiset, E.},
  year = {2005},
  pages = {259--264},
  publisher = {{IEEE}},
  address = {{Bordeaux, France}}
}

@inproceedings{brochu_bayesian_2010,
  title = {A {{Bayesian Interactive Optimization Approach}} to {{Procedural Animation Design}}},
  booktitle = {Proc. {{ACM SIGGRAPH}}/{{Eurographics Symp}}. {{Comput}}. {{Animation}}},
  author = {Brochu, Eric and Brochu, Tyson and {de Freitas}, Nando},
  year = {2010},
  series = {{{SCA}} '10},
  pages = {103--112},
  publisher = {{Eurographics Assoc.}},
  address = {{Goslar, DEU}}
}

@article{brochu_tutorial_2010,
  title = {A Tutorial on {{Bayesian}} Optimization of Expensive Cost Functions, with Application to Active User Modeling and Hierarchical Reinforcement Learning},
  author = {Brochu, Eric and Cora, Vlad M. and de Freitas, Nando},
  year = {2010},
  journal = {CoRR},
  volume = {abs/1012.2599},
  file = {/home/msca8h/Documents/bayesian_optimization/Brochu et al. - 2010 - A Tutorial on Bayesian Optimization of Expensive C.pdf}
}

@inproceedings{brock_freezeout_2017,
  title = {{{FreezeOut}}: {{Accelerate Training}} by {{Progressively Freezing Layers}}},
  author = {Brock, Andrew and Lim, Theodore and Ritchie, James Millar and Weston, Nicholas J.},
  year = {2017},
  month = dec,
  abstract = {The early layers of a deep neural net have the fewest parameters, but take up themost computation. In this extended abstract, we propose to only train the hidden layers for a set portion of the training run, freezing them out one-by-one and excluding them from the backward pass. We empirically demonstrate that FreezeOut yields savings of up to 20\% wall-clock time during training with 3\% loss in accuracy for DenseNets on CIFAR.},
  langid = {english},
  keywords = {Machine learning,Neural network}
}

@article{brockwell_identification_2005,
  title = {Identification of {{Regeneration Times}} in {{MCMC Simulation}}, {{With Application}} to {{Adaptive Schemes}}},
  author = {Brockwell, Anthony E and Kadane, Joseph B},
  year = {2005},
  month = jun,
  journal = {Journal of Computational and Graphical Statistics},
  volume = {14},
  number = {2},
  pages = {436--458},
  langid = {english},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\PYU7V8YA\\Brockwell and Kadane - 2005 - Identification of Regeneration Times in MCMC Simul.pdf}
}

@article{brockwell_parallel_2006,
  title = {Parallel {{Markov}} Chain {{Monte Carlo}} Simulation by Pre-Fetching},
  author = {Brockwell, A. E},
  year = {2006},
  month = mar,
  journal = {Journal of Computational and Graphical Statistics},
  volume = {15},
  number = {1},
  pages = {246--261},
  langid = {english}
}

@book{brockwell_time_1991,
  title = {Time {{Series}}: {{Theory}} and {{Methods}}},
  shorttitle = {Time {{Series}}},
  author = {Brockwell, Peter J. and Davis, Richard A.},
  year = {1991},
  series = {Springer {{Series}} in {{Statistics}}},
  publisher = {{Springer New York}},
  address = {{New York, NY}}
}

@article{brooks_efficient_2003,
  title = {Efficient Construction of Reversible Jump {{Markov}} Chain {{Monte Carlo}} Proposal Distributions},
  shorttitle = {Efficient Construction of Reversible Jump {{Markov}} Chain {{Monte Carlo}} Proposal Distributions},
  author = {Brooks, S. P. and Giudici, P. and Roberts, G. O.},
  year = {2003},
  month = jan,
  journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
  volume = {65},
  number = {1},
  pages = {3--39},
  langid = {english},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\ADDPC5M9\\Brooks et al. - 2003 - Efficient construction of reversible jump Markov c.pdf}
}

@article{broquedis_forestgomp_2010,
  title = {{{ForestGOMP}}: {{An Efficient OpenMP Environment}} for {{NUMA Architectures}}},
  shorttitle = {{{ForestGOMP}}},
  author = {Broquedis, Fran{\c c}ois and Furmento, Nathalie and Goglin, Brice and Wacrenier, Pierre-Andr{\'e} and Namyst, Raymond},
  year = {2010},
  month = oct,
  journal = {International Journal of Parallel Programming},
  volume = {38},
  number = {5-6},
  pages = {418--439},
  langid = {english},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\H9IAWHUD\\Broquedis et al. - 2010 - ForestGOMP An Efficient OpenMP Environment for NU.pdf}
}

@article{brown_inference_2010,
  title = {Inference with Normal-Gamma Prior Distributions in Regression Problems},
  author = {Brown, Philip J. and Griffin, Jim E.},
  year = {2010},
  month = mar,
  journal = {Bayesian Analysis},
  volume = {5},
  number = {1},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\7JKH4MRS\\Brown and Griffin - 2010 - Inference with normal-gamma prior distributions in.pdf}
}

@article{brown_interpolation_2020,
  title = {Interpolation Kernels for Synthetic Aperture Sonar Along-Track Motion Estimation},
  author = {Brown, Daniel C. and Gerg, Isaac D. and Blanford, Thomas E.},
  year = {2020},
  month = oct,
  journal = {IEEE Journal of Oceanic Engineering},
  volume = {45},
  number = {4},
  pages = {1497--1505}
}

@article{bruel_autotuning_2017,
  title = {Autotuning {{CUDA}} Compiler Parameters for Heterogeneous Applications Using the {{OpenTuner}} Framework: {{WSCAD15}}},
  shorttitle = {Autotuning {{CUDA}} Compiler Parameters for Heterogeneous Applications Using the {{OpenTuner}} Framework},
  author = {Bruel, Pedro and Amar{\'i}s, Marcos and Goldman, Alfredo},
  year = {2017},
  month = nov,
  journal = {Concurrency and Computation: Practice and Experience},
  volume = {29},
  number = {22},
  pages = {e3973},
  langid = {english}
}

@inproceedings{bruel_autotuning_2019,
  title = {Autotuning {{Under Tight Budget Constraints}}: {{A Transparent Design}} of {{Experiments Approach}}},
  shorttitle = {Autotuning {{Under Tight Budget Constraints}}},
  booktitle = {2019 19th {{IEEE}}/{{ACM International Symposium}} on {{Cluster}}, {{Cloud}} and {{Grid Computing}} ({{CCGRID}})},
  author = {Bruel, Pedro and Quinito Masnada, Steven and Videau, Brice and Legrand, Arnaud and Vincent, Jean-Marc and Goldman, Alfredo},
  year = {2019},
  month = may,
  pages = {147--156},
  publisher = {{IEEE}},
  address = {{Larnaca, Cyprus}},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\YAP4WEYQ\\Bruel et al. - 2019 - Autotuning Under Tight Budget Constraints A Trans.pdf}
}

@article{buchholz_adaptive_2018,
  title = {Adaptive {{Tuning Of Hamiltonian Monte Carlo Within Sequential Monte Carlo}}},
  author = {Buchholz, Alexander and Chopin, Nicolas and Jacob, Pierre E.},
  year = {2018},
  month = aug,
  journal = {arXiv:1808.07730 [stat]},
  eprint = {1808.07730},
  eprinttype = {arxiv},
  primaryclass = {stat},
  abstract = {Sequential Monte Carlo (SMC) samplers form an attractive alternative to MCMC for Bayesian computation. However, their performance depends strongly on the Markov kernels used to re- juvenate particles. We discuss how to calibrate automatically (using the current particles) Hamiltonian Monte Carlo kernels within SMC. To do so, we build upon the adaptive SMC ap- proach of Fearnhead and Taylor (2013), and we also suggest alternative methods. We illustrate the advantages of using HMC kernels within an SMC sampler via an extensive numerical study.},
  archiveprefix = {arXiv},
  keywords = {Statistics - Computation},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\P74SRNF6\\Buchholz et al. - 2018 - Adaptive Tuning Of Hamiltonian Monte Carlo Within .pdf;C\:\\Users\\msca8h\\Zotero\\storage\\JYD9MGZA\\1808.html}
}

@article{buchholz_adaptive_2020,
  title = {Adaptive {{Tuning}} of {{Hamiltonian Monte Carlo Within Sequential Monte Carlo}}},
  author = {Buchholz, Alexander and Chopin, Nicolas and Jacob, Pierre E.},
  year = {2020},
  month = jul,
  journal = {Bayesian Analysis},
  langid = {english},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\ZSWJFAKX\\Buchholz et al. - 2020 - Adaptive Tuning of Hamiltonian Monte Carlo Within .pdf}
}

@inproceedings{buchholz_quasimonte_2018,
  title = {Quasi-{{Monte Carlo}} Variational Inference},
  booktitle = {Proceedings of the {{International Conference}} on {{Machine Learning}}},
  author = {Buchholz, Alexander and Wenzel, Florian and Mandt, Stephan},
  year = {2018},
  month = jul,
  series = {{{PMLR}}},
  volume = {80},
  pages = {668--677},
  publisher = {{ML Research Press}},
  abstract = {Many machine learning problems involve Monte Carlo gradient estimators. As a prominent example, we focus on Monte Carlo variational inference (MCVI) in this paper. The performance of MCVI crucially depends on the variance of its stochastic gradients. We propose variance reduction by means of Quasi-Monte Carlo (QMC) sampling. QMC replaces N i.i.d. samples from a uniform probability distribution by a deterministic sequence of samples of length N. This sequence covers the underlying random variable space more evenly than i.i.d. draws, reducing the variance of the gradient estimator. With our novel approach, both the score function and the reparameterization gradient estimators lead to much faster convergence. We also propose a new algorithm for Monte Carlo objectives, where we operate with a constant learning rate and increase the number of QMC samples per iteration. We prove that this way, our algorithm can converge asymptotically at a faster rate than SGD . We furthermore provide theoretical guarantees on qmc for Monte Carlo objectives that go beyond MCVI , and support our findings by several experiments on large-scale data sets from various domains.},
  langid = {english},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\QGJKICY6\\Buchholz et al. - 2018 - Quasi-Monte Carlo Variational Inference.pdf;C\:\\Users\\msca8h\\Zotero\\storage\\VPLTH34J\\Buchholz et al. - 2018 - Quasi-Monte Carlo Variational Inference.pdf}
}

@article{buchner_nested_2021,
  title = {Nested Sampling Methods},
  author = {Buchner, Johannes},
  year = {2021},
  month = jul,
  journal = {arXiv:2101.09675 [astro-ph, stat]},
  eprint = {2101.09675},
  eprinttype = {arxiv},
  primaryclass = {astro-ph, stat},
  abstract = {Nested sampling (NS) computes parameter posterior distributions and makes Bayesian model comparison computationally feasible. Its strengths are the unsupervised navigation of complex, potentially multi-modal posteriors until a well-defined termination point. A systematic literature review of nested sampling algorithms and variants is presented. We focus on complete algorithms, including solutions to likelihood-restricted prior sampling, parallelisation, termination and diagnostics. The relation between number of live points, dimensionality and computational cost is studied for two complete algorithms. A new formulation of NS is presented, which casts the parameter space exploration as a search on a tree. Previously published ways of obtaining robust error estimates and dynamic variations of the number of live points are presented as special cases of this formulation. A new on-line diagnostic test is presented based on previous insertion rank order work. The survey of nested sampling methods concludes with outlooks for future research.},
  archiveprefix = {arXiv},
  keywords = {Astrophysics - Instrumentation and Methods for Astrophysics,Statistics - Computation},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\8BI8JY7M\\Buchner - 2021 - Nested Sampling Methods.pdf;C\:\\Users\\msca8h\\Zotero\\storage\\P6NYEYIC\\2101.html}
}

@article{bucker_simple_1994,
  title = {A Simple 3-{{D Gaussian}} Beam Sound Propagation Model for Shallow Water},
  author = {Bucker, Homer P.},
  year = {1994},
  month = may,
  journal = {The Journal of the Acoustical Society of America},
  volume = {95},
  number = {5},
  pages = {2437--2440},
  langid = {english}
}

@article{buckley_broadband_1988,
  title = {Broad-Band Signal-Subspace Spatial-Spectrum ({{BASS-ALE}}) Estimation},
  author = {Buckley, K.M. and Griffiths, L.J.},
  year = {1988},
  month = jul,
  journal = {IEEE Transactions on Acoustics, Speech, and Signal Processing},
  volume = {36},
  number = {7},
  pages = {953--964},
  abstract = {An approach to spatial-spectrum estimation of broadband sources is described. The approach, termed broadband signal-subspace spatial spectrum (BASS-ALE) estimation, is based on the eigenstructure of a broadband spatial/temporal covariance matrix and is justified by identifying the low-rank character of spatial/temporal observations of broadband sources. BASS-ALE estimators are described which incorporate: source focusing, spatial/temporal noise decorrelating, a signal (or noise-only) subspace generated from the eigenstructure of the transformed spatial/temporal covariance matrix, and broadband source models. Through discussion and simulation, BASS-ALE estimators are compared to the coherent signal-subspace processor (a similar subspace method), and relative advantages of the two are identified.{$<>$}},
  keywords = {Covariance matrix,Decorrelation,Noise generators,Signal generators,Signal processing},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\ZGGXUICQ\\Buckley and Griffiths - 1988 - Broad-band signal-subspace spatial-spectrum (BASS-.pdf;C\:\\Users\\msca8h\\Zotero\\storage\\XLKBC4WD\\1617.html}
}

@book{buder_lapesd_,
  title = {{{LaPeSD LibGOMP}}},
  author = {Buder, Patrick A.}
}

@article{bugallo_adaptive_2017,
  title = {Adaptive Importance Sampling: The Past, the Present, and the Future},
  shorttitle = {Adaptive {{Importance Sampling}}},
  author = {Bugallo, Monica F. and Elvira, Victor and Martino, Luca and Luengo, David and Miguez, Joaquin and Djuric, Petar M.},
  year = {2017},
  month = jul,
  journal = {IEEE Signal Processing Magazine},
  volume = {34},
  number = {4},
  pages = {60--79}
}

@incollection{bui_streaming_2017,
  title = {Streaming {{Sparse Gaussian Process Approximations}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 30},
  author = {Bui, Thang D and Nguyen, Cuong and Turner, Richard E},
  year = {2017},
  pages = {3299--3307},
  publisher = {{Curran Associates, Inc.}}
}

@incollection{bull_feedback_1998,
  title = {Feedback Guided Dynamic Loop Scheduling: {{Algorithms}} and Experiments},
  shorttitle = {Feedback Guided Dynamic Loop Scheduling},
  booktitle = {Euro-{{Par}}'98 {{Parallel Processing}}},
  author = {Bull, J. Mark},
  year = {1998},
  volume = {1470},
  pages = {377--382},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  file = {/home/msca8h/Documents/parallel_scheduling/bull1998.pdf;C\:\\Users\\msca8h\\Zotero\\storage\\QSJV2E7S\\Bull - 1998 - Feedback guided dynamic loop scheduling Algorithm.pdf}
}

@inproceedings{bull_measuring_1999,
  title = {Measuring Synchronisation and Scheduling Overheads in {{OpenMP}}},
  booktitle = {Proc. 1st {{Eur}}. {{Workshop OpenMP}}},
  author = {Bull, J. M.},
  year = {1999},
  series = {{{IWOMP}}'99},
  pages = {99--105},
  file = {/home/msca8h/Documents/parallel_scheduling/Bull - 1999 - Measuring Synchronisation and Scheduling Overheads.pdf}
}

@inproceedings{bun_concentrated_2016,
  title = {Concentrated Differential Privacy: {{Simplifications}}, Extensions, and Lower Bounds},
  shorttitle = {Concentrated {{Differential Privacy}}},
  booktitle = {Theory of {{Cryptography}}},
  author = {Bun, Mark and Steinke, Thomas},
  year = {2016},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {635--658},
  publisher = {{Springer}},
  address = {{Berlin, Heidelberg}},
  abstract = {``Concentrated differential privacy'' was recently introduced by Dwork and Rothblum as a relaxation of differential privacy, which permits sharper analyses of many privacy-preserving computations. We present an alternative formulation of the concept of concentrated differential privacy in terms of the R\'enyi divergence between the distributions obtained by running an algorithm on neighboring inputs. With this reformulation in hand, we prove sharper quantitative results, establish lower bounds, and raise a few new questions. We also unify this approach with approximate differential privacy by giving an appropriate definition of ``approximate concentrated differential privacy''.},
  langid = {english},
  keywords = {Differential Privacy,Gaussian Mechanism,Lower Bound,Privacy Loss,Rothblum},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\S484UPZ5\\Bun and Steinke - 2016 - Concentrated Differential Privacy Simplifications.pdf}
}

@article{burckhardt_speckle_1978,
  title = {Speckle in Ultrasound {{B-mode}} Scans},
  author = {Burckhardt, Christoph B.},
  year = {1978},
  month = jan,
  journal = {IEEE Transactions on Sonics and Ultrasonics},
  volume = {25},
  number = {1},
  pages = {1--6}
}

@inproceedings{burda_importance_2015,
  title = {Importance Weighted Autoencoders},
  booktitle = {Proceedings of the {{International}} Conference on Learning Representations},
  author = {Burda, Yuri and Grosse, Roger and Salakhutdinov, Ruslan},
  year = {2015}
}

@article{burgess_understanding_2018,
  title = {Understanding Disentangling in \$\textbackslash beta\$-{{VAE}}},
  author = {Burgess, Christopher P. and Higgins, Irina and Pal, Arka and Matthey, Loic and Watters, Nick and Desjardins, Guillaume and Lerchner, Alexander},
  year = {2018},
  month = apr,
  journal = {arXiv:1804.03599 [cs, stat]},
  eprint = {1804.03599},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {We present new intuitions and theoretical assessments of the emergence of disentangled representation in variational autoencoders. Taking a rate-distortion theory perspective, we show the circumstances under which representations aligned with the underlying generative factors of variation of data emerge when optimising the modified ELBO bound in \$\textbackslash beta\$-VAE, as training progresses. From these insights, we propose a modification to the training regime of \$\textbackslash beta\$-VAE, that progressively increases the information capacity of the latent code during training. This modification facilitates the robust learning of disentangled representations in \$\textbackslash beta\$-VAE, without the previous trade-off in reconstruction accuracy.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\PGMMNJYJ\\Burgess et al. - 2018 - Understanding disentangling in $beta$-VAE.pdf;C\:\\Users\\msca8h\\Zotero\\storage\\QSU2LBVW\\1804.html}
}

@article{burke_learning_2020,
  title = {Learning Rewards for Robotic Ultrasound Scanning Using Probabilistic Temporal Ranking},
  author = {Burke, Michael and Lu, Katie and Angelov, Daniel and Strai{\v z}ys, Art{\=u}ras and Innes, Craig and Subr, Kartic and Ramamoorthy, Subramanian},
  year = {2020},
  month = may,
  journal = {arXiv:2002.01240 [cs]},
  eprint = {2002.01240},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {This paper addresses a common class of problems where a robot learns to perform a discovery task based on example solutions, or \textbackslash emph\{human demonstrations\}. As an example, this work considers the problem of ultrasound scanning, where a demonstration involves an expert adaptively searching for a satisfactory view of internal organs, vessels or tissue and potential anomalies while maintaining optimal contact between the probe and surface tissue. Such problems are often solved by inferring notional \textbackslash emph\{rewards\} that, when optimised for, result in a plan that mimics demonstrations. A pivotal assumption, that plans with higher reward should be exponentially more likely, leads to the de facto approach for reward inference in robotics. While this approach of maximum entropy inverse reinforcement learning leads to a general and elegant formulation, it struggles to cope with frequently encountered sub-optimal demonstrations. In this paper, we propose an alternative approach to cope with the class of problems where sub-optimal demonstrations occur frequently. We hypothesise that, in tasks which require discovery, successive states of any demonstration are progressively more likely to be associated with a higher reward. We formalise this \textbackslash emph\{temporal ranking\} approach and show that it improves upon maximum-entropy approaches to perform reward inference for autonomous ultrasound scanning, a novel application of learning from demonstration in medical imaging.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Robotics},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\FXSDY8YH\\Burke et al. - 2020 - Learning rewards for robotic ultrasound scanning u.pdf;C\:\\Users\\msca8h\\Zotero\\storage\\UQX3PSYE\\2002.html}
}

@article{burt_laplacian_1983,
  title = {The {{Laplacian}} Pyramid as a Compact Image Code},
  author = {Burt, P. and Adelson, E.},
  year = {1983},
  month = apr,
  journal = {IEEE Transactions on Communications},
  volume = {31},
  number = {4},
  pages = {532--540},
  langid = {english}
}

@article{buskenes_lowcomplexity_2016,
  title = {Low-Complexity Adaptive Sonar Imaging},
  author = {Buskenes, Jo Inge and Hansen, Roy Edgar and Austeng, Andreas},
  year = {2016},
  journal = {IEEE Journal of Oceanic Engineering},
  pages = {1--10}
}

@article{buskenes_optimized_2015,
  title = {An Optimized {{GPU}} Implementation of the {{MVDR}} Beamformer for Active Sonar Imaging},
  author = {Buskenes, Jo Inge and Asen, Jon Petter and Nilsen, Carl-Inge Colombo and Austeng, Andreas},
  year = {2015},
  month = apr,
  journal = {IEEE Journal of Oceanic Engineering},
  volume = {40},
  number = {2},
  pages = {441--451}
}

@inproceedings{byrd_parallelisation_2010,
  title = {On the Parallelisation of {{MCMC}} by Speculative Chain Execution},
  booktitle = {Proc. {{Int}}. {{Parallel Distrib}}. {{Process}}. {{Symp}}. {{Workshop}}  {{Phd Forum}}},
  author = {Byrd, Jonathan M R and Jarvis, Stephen A and Bhalerao, Abhir H},
  year = {2010},
  month = apr,
  series = {{{IPDPSW}}'10},
  pages = {1--8},
  publisher = {{IEEE}},
  address = {{Atlanta, GA}},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\AEW2NJKI\\Byrd et al. - 2010 - On the parallelisation of MCMC by speculative chai.pdf}
}

@inproceedings{byrd_reducing_2008,
  title = {Reducing the Run-Time of {{MCMC}} Programs by Multithreading on {{SMP}} Architectures},
  booktitle = {Proc. {{Int}}. {{Parallel Distrib}}. {{Process}}. {{Symp}}.},
  author = {Byrd, Jonathan M. R. and Jarvis, Stephen A. and Bhalerao, Abhir H.},
  year = {2008},
  month = apr,
  series = {{{IPDPS}}'08},
  pages = {1--8},
  publisher = {{IEEE}},
  address = {{Miami, FL, USA}},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\GTUHTAJZ\\Byrd et al. - 2008 - Reducing the run-time of MCMC programs by multithr.pdf}
}

@article{byrne_timedomain_2015,
  title = {Time-Domain Wideband Adaptive Beamforming for Radar Breast Imaging},
  author = {Byrne, Dallan and Craddock, Ian J.},
  year = {2015},
  month = apr,
  journal = {IEEE Transactions on Antennas and Propagation},
  volume = {63},
  number = {4},
  pages = {1725--1735}
}

@article{cadalli_wideband_1999,
  title = {Wideband Maximum Likelihood Direction Finding and Signal Parameter Estimation by Using the Tree-Structured {{EM}} Algorithm},
  author = {Cadalli, N. and Arikan, O.},
  year = {Jan./1999},
  journal = {IEEE Transactions on Signal Processing},
  volume = {47},
  number = {1},
  pages = {201--206},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\MVBNISRU\\Cadalli and Arikan - 1999 - Wideband maximum likelihood direction finding and .pdf}
}

@inproceedings{cadena_how_2019,
  title = {How Well Do Deep Neural Networks Trained on Object Recognition Characterize the Mouse Visual System?},
  booktitle = {{{NeurIPS Neuro AI Workshop}}},
  author = {Cadena, S. A. and Sinz, F. H. and Muhammad, T. and Froudarakis, E. and Cobos, E. and Walker, E. Y. and Reimer, J. and Bethge, M. and Tolias, A. and Ecker, A. S.},
  year = {2019},
  keywords = {deep neural networks,goal-driven modeling,hierarchical organization,mouse visual cortex,object recognition}
}

@article{cadzow_multiple_1990,
  title = {Multiple Source Location-the Signal Subspace Approach},
  author = {Cadzow, J.A.},
  year = {1990},
  month = jul,
  journal = {IEEE Transactions on Acoustics, Speech, and Signal Processing},
  volume = {38},
  number = {7},
  pages = {1110--1125},
  abstract = {The multiple source location problem is solved for a general array geometry using the principal of least-squares modeling. The incident wavefields may be a mixture of coherent and noncoherent sources that are broadband and/or narrowband. Furthermore, the wavefields may be plane waves and/or spherical waves. An effective algorithmic approach is developed for solving the multiple source location problem in the signal and covariance domains. In each domain, it is necessary to represent a set of vectors as a linear combination of array steering vectors. Using this property, a generic descent algorithmic approach is developed for estimating the wavefield directions of arrival. The success of such iterative approaches is critically dependent on the generation of good estimates to initialize the algorithm. An effective initialization procedure is developed for this purpose. Direction-of-arrival estimates which arise from the proposed procedure are empirically found to be unbiased and low variance.{$<>$}},
  keywords = {Array signal processing,Direction of arrival estimation,Geometry,Least squares methods,Narrowband,Position measurement,Robustness,Signal resolution,Spatial resolution,Vectors},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\SBKI2324\\Cadzow - 1990 - Multiple source location-the signal subspace appro.pdf;C\:\\Users\\msca8h\\Zotero\\storage\\F2XKEGTX\\57540.html}
}

@inproceedings{cain_offset_1995,
  title = {Offset Windowing for {{FIR}} Fractional-Sample Delay},
  booktitle = {International {{Conference}} on {{Acoustics}}, {{Speech}}, and {{Signal Processing}}},
  author = {Cain, G.D. and Yardim, A. and Henry, P.},
  year = {1995},
  volume = {2},
  pages = {1276--1279},
  publisher = {{IEEE}},
  address = {{Detroit, MI, USA}}
}

@article{calderhead_general_2014,
  title = {A General Construction for Parallelizing {{Metropolis}}-{{Hastings}} Algorithms},
  author = {Calderhead, Ben},
  year = {2014},
  month = dec,
  journal = {Proceedings of the National Academy of Sciences},
  series = {{{PNAS}}},
  volume = {111},
  number = {49},
  pages = {17408--17413},
  langid = {english}
}

@article{callow_motioncompensation_2009,
  title = {Motion-Compensation Improvement for Widebeam, Multiple-Receiver {{SAS}} Systems},
  author = {Callow, H.J. and Hayes, M.P. and Gough, P.T.},
  year = {2009},
  month = jul,
  journal = {IEEE Journal of Oceanic Engineering},
  volume = {34},
  number = {3},
  pages = {262--268}
}

@phdthesis{callow_signal_2003,
  type = {Doctoral {{Thesis}}},
  title = {Signal Processing for Synthetic Aperture Sonar Image Enhancement},
  author = {Callow, Hayden John},
  year = {2003},
  month = apr,
  school = {University of Canterbury},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\RMTRBYNR\\Callow - 2003 - Signal processing for synthetic aperture sonar ima.pdf}
}

@book{candy_bayesian_2016,
  title = {Bayesian Signal Processing: {{Classical}}, Modern, and Particle Filtering Methods},
  shorttitle = {Bayesian {{Signal Processing}}},
  author = {Candy, James V.},
  year = {2016},
  series = {Adaptive and Cognitive Dynamic Systems},
  edition = {Second edition},
  publisher = {{IEEE Press / Wiley \& Sons, Inc}},
  address = {{Hoboken, New Jersey}},
  lccn = {TK5102.9 .C3187 2016},
  keywords = {Bayesian statistical decision theory,Mathematics,Signal processing}
}

@inproceedings{canepa_comparison_2017,
  title = {Comparison of Computation Time and Accuracy of the Real Time Implementation of Two Beamforming Algorithms},
  booktitle = {Proceedings of the {{Underwater Acoustics Conference}} and {{Exhibition}}},
  author = {Canepa, Gaetano and Tesei, Alessandra and Troiano, Luigi and Biagini, Stefano and Aglietti, Federico and Dymond, Rodney and Mazzi, Marco and Vermeij, Arjan and Grandi, Vittorio},
  year = {2017},
  month = sep,
  series = {{{UACE}}'17},
  pages = {121--128},
  address = {{Skiathos, Greece}}
}

@article{cao_interactive_2016,
  title = {Interactive Sound Propagation with Bidirectional Path Tracing},
  author = {Cao, Chunxiao and Ren, Zhong and Schissler, Carl and Manocha, Dinesh and Zhou, Kun},
  year = {2016},
  month = nov,
  journal = {ACM Transactions on Graphics},
  volume = {35},
  number = {6},
  pages = {1--11},
  langid = {english},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\KZ4QVQGV\\Cao et al. - 2016 - Interactive sound propagation with bidirectional p.pdf}
}

@article{capon_highresolution_1969,
  title = {High-Resolution Frequency-Wavenumber Spectrum Analysis},
  author = {Capon, J.},
  year = {1969},
  journal = {Proceedings of the IEEE},
  volume = {57},
  number = {8},
  pages = {1408--1418}
}

@article{cappe_adaptive_2008,
  title = {Adaptive Importance Sampling in General Mixture Classes},
  author = {Capp{\'e}, Olivier and Douc, Randal and Guillin, Arnaud and Marin, Jean-Michel and Robert, Christian P.},
  year = {2008},
  month = dec,
  journal = {Statistics and Computing},
  volume = {18},
  number = {4},
  pages = {447--459},
  langid = {english},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\QI9CG3WZ\\Capp et al. - 2008 - Adaptive importance sampling in general mixture cl.pdf}
}

@article{cappe_overview_2007,
  title = {An {{Overview}} of {{Existing Methods}} and {{Recent Advances}} in {{Sequential Monte Carlo}}},
  author = {Capp{\'e}, Olivier and Godsill, Simon J and Moulines, Eric},
  year = {2007},
  journal = {Proceedings of the IEEE},
  volume = {95},
  number = {5},
  pages = {899--924},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\7BQ8YWN3\\Capp et al. - 2007 - An overview of existing methods and recent advance.pdf}
}

@article{cappe_overview_2007a,
  title = {An {{Overview}} of {{Existing Methods}} and {{Recent Advances}} in {{Sequential Monte Carlo}}},
  author = {Cappe, O. and Godsill, S. J. and Moulines, E.},
  year = {2007},
  month = may,
  journal = {Proceedings of the IEEE},
  volume = {95},
  number = {5},
  pages = {899--924},
  keywords = {and smoothing,Bayesian dynamical model,Bayesian dynamical models,Computer vision,Computerized monitoring,filtering,Filtering,filtering theory,hidden Markov models,Hidden Markov models,image processing,Monte Carlo methods,parameter estimation,particle filter,Pollution,prediction,Predictive models,Probability density function,sequential Monte Carlo,signal processing,Signal processing,Sliding mode control,SMC algorithms,state-space model,tracking},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\KQWQP35W\\Cappe et al. - 2007 - An Overview of Existing Methods and Recent Advance.pdf}
}

@article{cappe_population_2004,
  title = {Population {{Monte Carlo}}},
  author = {Capp{\'e}, O and Guillin, A and Marin, J. M and Robert, C. P},
  year = {2004},
  month = dec,
  journal = {Journal of Computational and Graphical Statistics},
  volume = {13},
  number = {4},
  pages = {907--929},
  langid = {english}
}

@inproceedings{carastan-santos_obtaining_2017,
  title = {Obtaining Dynamic Scheduling Policies with Simulation and Machine Learning},
  booktitle = {Proceedings of the {{International Conference}} for {{High Performance Computing}}, {{Networking}}, {{Storage}} and {{Analysis}} on - {{SC}} '17},
  author = {{Carastan-Santos}, Danilo and {de Camargo}, Raphael Y.},
  year = {2017},
  pages = {1--13},
  publisher = {{ACM Press}},
  address = {{Denver, Colorado}},
  langid = {english},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\LBXXK2IW\\Carastan-Santos and de Camargo - 2017 - Obtaining dynamic scheduling policies with simulat.pdf}
}

@article{cardoso_brsnis_2022,
  title = {{{BR-SNIS}}: {{Bias}} Reduced Self-Normalized Importance Sampling},
  shorttitle = {{{BR-SNIS}}},
  author = {Cardoso, Gabriel and Samsonov, Sergey and Thin, Achille and Moulines, Eric and Olsson, Jimmy},
  year = {2022},
  month = sep,
  journal = {Advances in Neural Information Processing Systems (to be presented)},
  eprint = {2207.06364},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  abstract = {Importance Sampling (IS) is a method for approximating expectations under a target distribution using independent samples from a proposal distribution and the associated importance weights. In many applications, the target distribution is known only up to a normalization constant, in which case self-normalized IS (SNIS) can be used. While the use of self-normalization can have a positive effect on the dispersion of the estimator, it introduces bias. In this work, we propose a new method, BR-SNIS, whose complexity is essentially the same as that of SNIS and which significantly reduces bias without increasing the variance. This method is a wrapper in the sense that it uses the same proposal samples and importance weights as SNIS, but makes clever use of iterated sampling--importance resampling (ISIR) to form a bias-reduced version of the estimator. We furnish the proposed algorithm with rigorous theoretical results, including new bias, variance and high-probability bounds, and these are illustrated by numerical examples.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Computation,Statistics - Machine Learning},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\G7RER2YB\\Cardoso et al. - 2022 - BR-SNIS Bias Reduced Self-Normalized Importance S.pdf;C\:\\Users\\msca8h\\Zotero\\storage\\4LE2BSW4\\2207.html}
}

@inproceedings{carino_dynamic_2002,
  title = {Dynamic Scheduling Parallel Loops with Variable Iterate Execution Times},
  booktitle = {Proceedings 16th {{International Parallel}} and {{Distributed Processing Symposium}}},
  author = {Carino, R.L. and Banicescu, I.},
  year = {2002},
  pages = {8 pp},
  publisher = {{IEEE}},
  address = {{Ft. Lauderdale, FL}},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\EKD9L7GR\\Carino and Banicescu - 2002 - Dynamic scheduling parallel loops with variable it.pdf}
}

@article{carino_dynamic_2008,
  title = {Dynamic Load Balancing with Adaptive Factoring Methods in Scientific Applications},
  author = {Cari{\~n}o, Ricolindo L. and Banicescu, Ioana},
  year = {2008},
  month = apr,
  journal = {The Journal of Supercomputing},
  volume = {44},
  number = {1},
  pages = {41--63},
  abstract = {To improve the performance of scientific applications with parallel loops, dynamic loop scheduling methods have been proposed. Such methods address performance degradations due to load imbalance caused by predictable phenomena like nonuniform data distribution or algorithmic variance, and unpredictable phenomena such as data access latency or operating system interference. In particular, methods such as factoring, weighted factoring, adaptive weighted factoring, and adaptive factoring have been developed based on a probabilistic analysis of parallel loop iterates with variable running times. These methods have been successfully implemented in a number of applications such as: N-Body and Monte Carlo simulations, computational fluid dynamics, and radar signal processing.},
  file = {/home/msca8h/Documents/parallel_scheduling/Cario and Banicescu - 2008 - Dynamic load balancing with adaptive factoring met.pdf}
}

@article{carlson_covariance_1988,
  title = {Covariance Matrix Estimation Errors and Diagonal Loading in Adaptive Arrays},
  author = {Carlson, B.D.},
  year = {1988},
  month = jul,
  journal = {IEEE Transactions on Aerospace and Electronic Systems},
  volume = {24},
  number = {4},
  pages = {397--401}
}

@article{carpenter_stan_2017,
  title = {Stan: {{A}} Probabilistic Programming Language},
  shorttitle = {{\emph{Stan}}},
  author = {Carpenter, Bob and Gelman, Andrew and Hoffman, Matthew D. and Lee, Daniel and Goodrich, Ben and Betancourt, Michael and Brubaker, Marcus and Guo, Jiqiang and Li, Peter and Riddell, Allen},
  year = {2017},
  journal = {Journal of Statistical Software},
  volume = {76},
  number = {1},
  langid = {english},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\5KD8YE7R\\Carpenter et al. - 2017 - Stan  A Probabilistic Programming Language.pdf}
}

@article{carvalho_particle_2010,
  title = {Particle {{Learning}} and {{Smoothing}}},
  author = {Carvalho, Carlos M. and Johannes, Michael S. and Lopes, Hedibert F. and Polson, Nicholas G.},
  year = {2010},
  month = feb,
  journal = {Statistical Science},
  volume = {25},
  number = {1},
  pages = {88--106},
  langid = {english},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\YLZTBG3Q\\Carvalho et al. - 2010 - Particle Learning and Smoothing.pdf}
}

@article{carvalho_particle_2010a,
  title = {Particle {{Learning}} and {{Smoothing}}},
  author = {Carvalho, Carlos M. and Johannes, Michael S. and Lopes, Hedibert F. and Polson, Nicholas G.},
  year = {2010},
  month = feb,
  journal = {Statistical Science},
  volume = {25},
  number = {1},
  pages = {88--106},
  langid = {english},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\QF2GT4LF\\Carvalho et al. - 2010 - Particle Learning and Smoothing.pdf}
}

@article{carvalho_particle_2010b,
  title = {Particle {{Learning}} and {{Smoothing}}},
  author = {Carvalho, Carlos M. and Johannes, Michael S. and Lopes, Hedibert F. and Polson, Nicholas G.},
  year = {2010},
  journal = {Statistical Science},
  volume = {25},
  number = {1},
  pages = {88--106},
  abstract = {Particle learning (PL) provides state filtering, sequential parameter learning and smoothing in a general class of state space models. Our approach extends existing particle methods by incorporating the estimation of static parameters via a fully-adapted filter that utilizes conditional sufficient statistics for parameters and/or states as particles. State smoothing in the presence of parameter uncertainty is also solved as a by-product of PL. In a number of examples, we show that PL outperforms existing particle filtering alternatives and proves to be a competitor to MCMC.}
}

@book{casella_statistical_1990,
  title = {Statistical Inference},
  author = {Casella, George and Berger, Roger L.},
  year = {1990},
  series = {The {{Wadsworth}} \& {{Brooks}}/{{Cole Statistics}}/{{Probability}} Series},
  publisher = {{Brooks/Cole Pub. Co}},
  address = {{Pacific Grove, Calif}},
  lccn = {QA276 .C37 1990},
  keywords = {Mathematical statistics,Probabilities}
}

@article{cassidy_bayesian_2002,
  title = {Bayesian Nonstationary Autoregressive Models for Biomedical Signal Analysis},
  author = {Cassidy, M.J. and Penny, W.D.},
  year = {2002},
  month = oct,
  journal = {IEEE Transactions on Biomedical Engineering},
  volume = {49},
  number = {10},
  pages = {1142--1152},
  langid = {english},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\U8U4IAPJ\\Cassidy and Penny - 2002 - Bayesian nonstationary autoregressive models for b.pdf}
}

@inproceedings{caterini_hamiltonian_2018,
  title = {Hamiltonian Variational Auto-Encoder},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Caterini, Anthony L and Doucet, Arnaud and Sejdinovic, Dino},
  year = {2018},
  volume = {31},
  publisher = {{Curran Associates, Inc.}},
  abstract = {Variational Auto-Encoders (VAE) have become very popular techniques to perform inference and learning in latent variable models as they allow us to leverage the rich representational power of neural networks to obtain flexible approximations of the posterior of latent variables as well as tight evidence lower bounds (ELBO). Com- bined with stochastic variational inference, this provides a methodology scaling to large datasets. However, for this methodology to be practically efficient, it is neces- sary to obtain low-variance unbiased estimators of the ELBO and its gradients with respect to the parameters of interest. While the use of Markov chain Monte Carlo (MCMC) techniques such as Hamiltonian Monte Carlo (HMC) has been previously suggested to achieve this [23, 26], the proposed methods require specifying reverse kernels which have a large impact on performance. Additionally, the resulting unbiased estimator of the ELBO for most MCMC kernels is typically not amenable to the reparameterization trick. We show here how to optimally select reverse kernels in this setting and, by building upon Hamiltonian Importance Sampling (HIS) [17], we obtain a scheme that provides low-variance unbiased estimators of the ELBO and its gradients using the reparameterization trick. This allows us to develop a Hamiltonian Variational Auto-Encoder (HVAE). This method can be re-interpreted as a target-informed normalizing flow [20] which, within our context, only requires a few evaluations of the gradient of the sampled likelihood and trivial Jacobian calculations at each iteration.},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\CYMU27CQ\\Caterini et al. - 2018 - Hamiltonian Variational Auto-Encoder.pdf}
}

@book{cemgil_bayesian_2018,
  title = {Bayesian Statistical Methods for Audio and Music Processing},
  author = {Cemgil, A. Taylan and Godsill, Simon and Peeling, Paul and Whiteley, Nick},
  year = {2018},
  month = aug,
  volume = {1},
  publisher = {{Oxford University Press}},
  abstract = {This article focuses on the use of Bayesian statistical methods in audio and music processing in the context of an application to multipitch audio and determining a musical `score' representation that includes pitch and time duration summary for a musical extract (the so-called `piano-roll' representation of music). It first provides an overview of mainstream applications of audio signal processing, the properties of musical audio, superposition and how to address it using the Bayesian approach, and the principal challenges facing audio processing. It then considers the fundamental audio processing tasks before discussing a range of Bayesian hierarchical models involving both time and frequency domain dynamic models. It shows that Bayesian analysis is applicable in audio signal processing in real environments where acoustical conditions and sound sources are highly variable, yet audio signals possess strong statistical structure.},
  langid = {english},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\37I4UB7G\\Cemgil et al. - 2018 - Bayesian statistical methods for audio and music p.pdf}
}

@article{cerqueira_novel_2017,
  title = {A Novel {{GPU-based}} Sonar Simulator for Real-Time Applications},
  author = {Cerqueira, R{\^o}mulo and Trocoli, Tiago and Neves, Gustavo and Joyeux, Sylvain and Albiez, Jan and Oliveira, Luciano},
  year = {2017},
  month = nov,
  journal = {Computers \& Graphics},
  volume = {68},
  pages = {66--76},
  langid = {english}
}

@article{cerqueira_novel_2017a,
  title = {A Novel {{GPU-based}} Sonar Simulator for Real-Time Applications},
  author = {Cerqueira, R{\^o}mulo and Trocoli, Tiago and Neves, Gustavo and Joyeux, Sylvain and Albiez, Jan and Oliveira, Luciano},
  year = {2017},
  month = nov,
  journal = {Computers \& Graphics},
  volume = {68},
  pages = {66--76},
  langid = {english}
}

@article{cerqueira_rasterized_2020,
  title = {A Rasterized Ray-Tracer Pipeline for Real-Time, Multi-Device Sonar Simulation},
  author = {Cerqueira, R{\^o}mulo and Trocoli, Tiago and Albiez, Jan and Oliveira, Luciano},
  year = {2020},
  month = sep,
  journal = {Graphical Models},
  volume = {111},
  pages = {101086},
  langid = {english},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\BMLQ4WHN\\Cerqueira et al. - 2020 - A rasterized ray-tracer pipeline for real-time, mu.pdf}
}

@article{cerqueira_rasterized_2020a,
  title = {A Rasterized Ray-Tracer Pipeline for Real-Time, Multi-Device Sonar Simulation},
  author = {Cerqueira, R{\^o}mulo and Trocoli, Tiago and Albiez, Jan and Oliveira, Luciano},
  year = {2020},
  month = sep,
  journal = {Graphical Models},
  volume = {111},
  pages = {101086},
  langid = {english},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\ZI6H9IHF\\Cerqueira et al. - 2020 - A rasterized ray-tracer pipeline for real-time, mu.pdf}
}

@article{chakrabarty_bayesian_2018,
  title = {A {{Bayesian}} Approach to Informed Spatial Filtering with Robustness against {{DOA}} Estimation Errors},
  author = {Chakrabarty, Soumitro and Habets, Emanuel A. P.},
  year = {2018},
  month = jan,
  journal = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  volume = {26},
  number = {1},
  pages = {145--160}
}

@article{challis_gaussian_2013,
  title = {Gaussian {{Kullback-Leibler}} Approximate Inference},
  author = {Challis, Edward and Barber, David},
  year = {2013},
  journal = {Journal of Machine Learning Research},
  volume = {14},
  number = {68},
  pages = {2239--2286},
  abstract = {We investigate Gaussian Kullback-Leibler (G-KL) variational approximate inference techniques for Bayesian generalised linear models and various extensions. In particular we make the following novel contributions: sufficient conditions for which the G-KL objective is differentiable and convex are described; constrained parameterisations of Gaussian covariance that make G-KL methods fast and scalable are provided; the lower bound to the normalisation constant provided by G-KL methods is proven to dominate those provided by local lower bounding methods; complexity and model applicability issues of G-KL versus other Gaussian approximate inference methods are discussed. Numerical results comparing G-KL and other deterministic Gaussian approximate inference methods are presented for: robust Gaussian process regression models with either Student- t t  or Laplace likelihoods, large scale Bayesian binary logistic regression models, and Bayesian sparse linear models for sequential experimental design.},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\UDBF2CQ7\\Challis and Barber - 2013 - Gaussian Kullback-Leibler Approximate Inference.pdf}
}

@inproceedings{chan_autotuning_2009,
  title = {Autotuning Multigrid with {{PetaBricks}}},
  booktitle = {Proceedings of the {{Conference}} on {{High Performance Computing Networking}}, {{Storage}} and {{Analysis}} - {{SC}} '09},
  author = {Chan, Cy and Ansel, Jason and Wong, Yee Lok and Amarasinghe, Saman and Edelman, Alan},
  year = {2009},
  pages = {1},
  publisher = {{ACM Press}},
  address = {{Portland, Oregon}},
  langid = {english},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\S8LD2X2V\\Chan et al. - 2009 - Autotuning multigrid with PetaBricks.pdf}
}

@inproceedings{chang_domainspecific_2019,
  title = {Domain-{{Specific Batch Normalization}} for {{Unsupervised Domain Adaptation}}},
  booktitle = {The {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Chang, Woong-Gi and You, Tackgeun and Seo, Seonguk and Kwak, Suha and Han, Bohyung},
  year = {2019},
  month = jun
}

@inproceedings{chang_multiple_2005,
  title = {Multiple Object Tracking with Kernel Particle Filter},
  booktitle = {Proceedings of the {{IEEE Computer Society Conference}} on {{Computer Vision}} and {{Pattern}}},
  author = {Chang, Cheng and Ansari, R. and Khokhar, A.},
  year = {2005},
  volume = {1},
  pages = {566--573},
  publisher = {{IEEE}},
  address = {{San Diego, CA, USA}}
}

@article{chang_usefulness_2021,
  title = {Usefulness of Machine Learning-Based Detection and Classification of Cardiac Arrhythmias with 12-Lead Electrocardiograms},
  author = {Chang, Kuan-Cheng and Hsieh, Po-Hsin and Wu, Mei-Yao and Wang, Yu-Chen and Chen, Jan-Yow and Tsai, Fuu-Jen and Shih, Edward S.C. and Hwang, Ming-Jing and Huang, Tzung-Chi},
  year = {2021},
  month = jan,
  journal = {Canadian Journal of Cardiology},
  volume = {37},
  number = {1},
  pages = {94--104},
  langid = {english}
}

@article{chatterjee_sample_2018,
  title = {The Sample Size Required in Importance Sampling},
  author = {Chatterjee, Sourav and Diaconis, Persi},
  year = {2018},
  month = apr,
  journal = {The Annals of Applied Probability},
  volume = {28},
  number = {2},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\B7CLIKC7\\Chatterjee and Diaconis - 2018 - The sample size required in importance sampling.pdf}
}

@inproceedings{che_characterization_2010,
  title = {A Characterization of the {{Rodinia}} Benchmark Suite with Comparison to Contemporary {{CMP}} Workloads},
  booktitle = {Proc. {{IEEE Int}}. {{Symp}}. {{Workload Characterization}}},
  author = {Che, Shuai and Sheaffer, Jeremy W. and Boyer, Michael and Szafaryn, Lukasz G. and {Liang Wang} and Skadron, Kevin},
  year = {2010},
  month = dec,
  series = {{{IISWC}}'10},
  pages = {1--11},
  publisher = {{IEEE}},
  address = {{Atlanta, GA, USA}},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\SFWW88D6\\Che et al. - 2010 - A characterization of the Rodinia benchmark suite .pdf}
}

@article{chen_accelerated_2016,
  title = {Accelerated {{Dimension-Independent Adaptive Metropolis}}},
  author = {Chen, Yuxin and Keyes, David and Law, Kody J. H. and Ltaief, Hatem},
  year = {2016},
  month = jan,
  journal = {SIAM Journal on Scientific Computing},
  volume = {38},
  number = {5},
  pages = {S539-S565},
  langid = {english},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\53VNYW45\\Chen et al. - 2016 - Accelerated Dimension-Independent Adaptive Metropo.pdf}
}

@article{chen_adaptive_2013,
  title = {Adaptive {{Cache Aware Bitier Work-Stealing}} in {{Multisocket Multicore Architectures}}},
  author = {Chen, Quan and Guo, Minyi and Huang, Zhiyi},
  year = {2013},
  month = dec,
  journal = {IEEE Transactions on Parallel and Distributed Systems},
  volume = {24},
  number = {12},
  pages = {2334--2343}
}

@article{chen_contention_2018,
  title = {Contention and {{Locality-Aware Work-Stealing}} for {{Iterative Applications}} in {{Multi-Socket Computers}}},
  author = {Chen, Quan and Guo, Minyi},
  year = {2018},
  month = jun,
  journal = {IEEE Transactions on Computers},
  volume = {67},
  number = {6},
  pages = {784--798},
  file = {/home/msca8h/Documents/parallel_scheduling/Chen and Guo - 2018 - Contention and Locality-Aware Work-Stealing for It.pdf;/home/msca8h/Documents/parallel_scheduling/Chen and Guo - 2018 - Contention and Locality-Aware Work-Stealing for It.pdf}
}

@article{chen_detection_1991,
  title = {Detection of the Number of Signals: A Predicted Eigen-Threshold Approach},
  shorttitle = {Detection of the Number of Signals},
  author = {Chen, W. and Wong, K.M. and Reilly, J.P.},
  year = {1991},
  month = may,
  journal = {IEEE Transactions on Signal Processing},
  volume = {39},
  number = {5},
  pages = {1088--1098}
}

@article{chen_fast_2019,
  title = {Fast Mixing of {{Metropolized Hamiltonian Monte Carlo}}: {{Benefits}} of Multi-Step Gradients},
  shorttitle = {Fast Mixing of {{Metropolized Hamiltonian Monte Carlo}}},
  author = {Chen, Yuansi and Dwivedi, Raaz and Wainwright, Martin J. and Yu, Bin},
  year = {2019},
  month = may,
  journal = {arXiv:1905.12247 [cs, stat]},
  eprint = {1905.12247},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Hamiltonian Monte Carlo (HMC) is a state-of-the-art Markov chain Monte Carlo sampling algorithm for drawing samples from smooth probability densities over continuous spaces. We study the variant most widely used in practice, Metropolized HMC with the St\textbackslash "\{o\}rmer-Verlet or leapfrog integrator, and make two primary contributions. First, we provide a non-asymptotic upper bound on the mixing time of the Metropolized HMC with explicit choices of stepsize and number of leapfrog steps. This bound gives a precise quantification of the faster convergence of Metropolized HMC relative to simpler MCMC algorithms such as the Metropolized random walk, or Metropolized Langevin algorithm. Second, we provide a general framework for sharpening mixing time bounds Markov chains initialized at a substantial distance from the target distribution over continuous spaces. We apply this sharpening device to the Metropolized random walk and Langevin algorithms, thereby obtaining improved mixing time bounds from a non-warm initial distribution.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Computation,Statistics - Machine Learning},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\BS8WR7KS\\Chen et al. - 2019 - Fast mixing of Metropolized Hamiltonian Monte Carl.pdf;C\:\\Users\\msca8h\\Zotero\\storage\\MPFZESKL\\1905.html}
}

@article{chen_maximumlikelihood_2002,
  title = {Maximum-Likelihood Source Localization and Unknown Sensor Location Estimation for Wideband Signals in the near-Field},
  author = {Chen, J.C. and Hudson, R.E. and {Kung Yao}},
  year = {2002},
  month = aug,
  journal = {IEEE Transactions on Signal Processing},
  volume = {50},
  number = {8},
  pages = {1843--1854},
  langid = {english}
}

@inproceedings{chen_pairwise_2013,
  title = {Pairwise Ranking Aggregation in a Crowdsourced Setting},
  booktitle = {Proceedings of the Sixth {{ACM}} International Conference on {{Web}} Search and Data Mining},
  author = {Chen, Xi and Bennett, Paul N. and {Collins-Thompson}, Kevyn and Horvitz, Eric},
  year = {2013},
  month = feb,
  series = {{{WSDM}} '13},
  pages = {193--202},
  publisher = {{Association for Computing Machinery}},
  address = {{Rome, Italy}},
  abstract = {Inferring rankings over elements of a set of objects, such as documents or images, is a key learning problem for such important applications as Web search and recommender systems. Crowdsourcing services provide an inexpensive and efficient means to acquire preferences over objects via labeling by sets of annotators. We propose a new model to predict a gold-standard ranking that hinges on combining pairwise comparisons via crowdsourcing. In contrast to traditional ranking aggregation methods, the approach learns about and folds into consideration the quality of contributions of each annotator. In addition, we minimize the cost of assessment by introducing a generalization of the traditional active learning scenario to jointly select the annotator and pair to assess while taking into account the annotator quality, the uncertainty over ordering of the pair, and the current model uncertainty. We formalize this as an active learning strategy that incorporates an exploration-exploitation tradeoff and implement it using an efficient online Bayesian updating scheme. Using simulated and real-world data, we demonstrate that the active learning strategy achieves significant reductions in labeling cost while maintaining accuracy.},
  keywords = {crowdsourcing,pairwise preference,ranking}
}

@article{chen_wideband_2004,
  title = {Wideband {{MVDR}} Beamforming for Acoustic Vector Sensor Linear Array},
  author = {Chen, H.-W. and Zhao, J.-W.},
  year = {2004},
  journal = {IEE Proceedings - Radar, Sonar and Navigation},
  volume = {151},
  number = {3},
  pages = {158},
  langid = {english}
}

@article{cheng-choulee_eigenspacebased_1997,
  title = {Eigenspace-Based Adaptive Array Beamforming with Robust Capabilities},
  author = {{Cheng-Chou Lee} and {Ju-Hong Lee}},
  year = {Dec./1997},
  journal = {IEEE Transactions on Antennas and Propagation},
  volume = {45},
  number = {12},
  pages = {1711--1716}
}

@inproceedings{cherief-abdellatif_generalization_2019,
  title = {A Generalization Bound for Online Variational Inference},
  booktitle = {Proceedings of the {{Asian Conference}} on {{Machine Learning}}},
  author = {{Ch{\'e}rief-Abdellatif}, Badr-Eddine and Alquier, Pierre and Khan, Mohammad Emtiyaz},
  year = {2019},
  month = oct,
  series = {{{PMLR}}},
  volume = {101},
  pages = {662--677},
  publisher = {{ML Research Press}},
  abstract = {Bayesian inference provides an attractive online-learning framework to analyze sequential data, and offers generalization guarantees which hold even with model mismatch and adversaries. Unfortunately, exact Bayesian inference is rarely feasible in practice and approximation methods are usually employed, but do such methods preserve the generalization properties of Bayesian inference ? In this paper, we show that this is indeed the case for some variational inference (VI) algorithms. We consider a few existing online, tempered VI algorithms, as well as a new algorithm, and derive their generalization bounds. Our theoretical result relies on the convexity of the variational objective, but we argue that the result should hold more generally and present empirical evidence in support of this. Our work in this paper presents theoretical justifications in favor of online algorithms relying on approximate Bayesian methods.},
  langid = {english},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\2I6LT4LW\\Chrief-Abdellatif et al. - 2019 - A Generalization Bound for Online Variational Infe.pdf;C\:\\Users\\msca8h\\Zotero\\storage\\MBTKSG5R\\Chrief-Abdellatif et al. - 2019 - A Generalization Bound for Online Variational Infe.pdf}
}

@inproceedings{cherief-abdellatif_generalization_2019a,
  title = {A Generalization Bound for Online Variational Inference},
  booktitle = {Proceedings of {{The Asian Conference}} on {{Machine Learning}}},
  author = {{Ch{\'e}rief-Abdellatif}, Badr-Eddine and Alquier, Pierre and Khan, Mohammad Emtiyaz},
  year = {2019},
  month = oct,
  series = {{{PMLR}}},
  volume = {101},
  pages = {662--677},
  publisher = {{ML Research Press}},
  abstract = {Bayesian inference provides an attractive online-learning framework to analyze sequential data, and offers generalization guarantees which hold even with model mismatch and adversaries. Unfortunately, exact Bayesian inference is rarely feasible in practice and approximation methods are usually employed, but do such methods preserve the generalization properties of Bayesian inference ? In this paper, we show that this is indeed the case for some variational inference (VI) algorithms. We consider a few existing online, tempered VI algorithms, as well as a new algorithm, and derive their generalization bounds. Our theoretical result relies on the convexity of the variational objective, but we argue that the result should hold more generally and present empirical evidence in support of this. Our work in this paper presents theoretical justifications in favor of online algorithms relying on approximate Bayesian methods.},
  langid = {english},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\6IK3P88L\\Chrief-Abdellatif et al. - 2019 - A Generalization Bound for Online Variational Infe.pdf;C\:\\Users\\msca8h\\Zotero\\storage\\Z6RJWLCT\\Chrief-Abdellatif et al. - 2019 - A Generalization Bound for Online Variational Infe.pdf}
}

@article{ching_transitional_2007,
  title = {Transitional {{Markov Chain Monte Carlo Method}} for {{Bayesian Model Updating}}, {{Model Class Selection}}, and {{Model Averaging}}},
  author = {Ching, Jianye and Chen, Yi-Chu},
  year = {2007},
  month = jul,
  journal = {Journal of Engineering Mechanics},
  volume = {133},
  number = {7},
  pages = {816--832},
  langid = {english}
}

@article{cho_bilateral_2014,
  title = {Bilateral Texture Filtering},
  author = {Cho, Hojin and Lee, Hyunjoon and Kang, Henry and Lee, Seungyong},
  year = {2014},
  month = jul,
  journal = {ACM Transactions on Graphics},
  volume = {33},
  number = {4},
  pages = {1--8},
  langid = {english},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\2LEQBACV\\Cho et al. - 2014 - Bilateral texture filtering.pdf}
}

@inproceedings{choi_modeldriven_2010,
  title = {Model-Driven Autotuning of Sparse Matrix-Vector Multiply on {{GPUs}}},
  booktitle = {Proceedings of the 15th {{ACM SIGPLAN}} Symposium on {{Principles}} and Practice of Parallel Programming - {{PPoPP}} '10},
  author = {Choi, Jee W. and Singh, Amik and Vuduc, Richard W.},
  year = {2010},
  pages = {115},
  publisher = {{ACM Press}},
  address = {{Bangalore, India}},
  langid = {english},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\TZTYX998\\Choi et al. - 2010 - Model-driven autotuning of sparse matrix-vector mu.pdf}
}

@article{choi_physicsbased_2021,
  title = {Physics-Based Modelling and Simulation of Multibeam Echosounder Perception for Autonomous Underwater Manipulation},
  author = {Choi, Woen-Sug and Olson, Derek R. and Davis, Duane and Zhang, Mabel and Racson, Andy and Bingham, Brian and McCarrin, Michael and Vogt, Carson and Herman, Jessica},
  year = {2021},
  journal = {Frontiers in Robotics and AI},
  volume = {8},
  abstract = {One of the key distinguishing aspects of underwater manipulation tasks is the perception challenges of the ocean environment, including turbidity, backscatter, and lighting effects. Consequently, underwater perception often relies on sonar-based measurements to estimate the vehicle's state and surroundings, either standalone or in concert with other sensing modalities, to support the perception necessary to plan and control manipulation tasks. Simulation of the multibeam echosounder, while not a substitute for in-water testing, is a critical capability for developing manipulation strategies in the complex and variable ocean environment. Although several approaches exist in the literature to simulate synthetic sonar images, the methods in the robotics community typically use image processing and video rendering software to comply with real-time execution requirements. In addition to a lack of physics-based interaction model between sound and the scene of interest, several basic properties are absent in these rendered sonar images\textendash notably the coherent imaging system and coherent speckle that cause distortion of the object geometry in the sonar image. To address this deficiency, we present a physics-based multibeam echosounder simulation method to capture these fundamental aspects of sonar perception. A point-based scattering model is implemented to calculate the acoustic interaction between the target and the environment. This is a simplified representation of target scattering but can produce realistic coherent image speckle and the correct point spread function. The results demonstrate that this multibeam echosounder simulator generates qualitatively realistic images with high efficiency to provide the sonar image and the physical time series signal data. This synthetic sonar data is a key enabler for developing, testing, and evaluating autonomous underwater manipulation strategies that use sonar as a component of perception.}
}

@article{chopin_central_2004,
  title = {Central Limit Theorem for Sequential {{Monte Carlo}} Methods and Its Application to {{Bayesian}} Inference},
  author = {Chopin, Nicolas},
  year = {2004},
  month = dec,
  journal = {The Annals of Statistics},
  volume = {32},
  number = {6},
  pages = {2385--2411},
  langid = {english},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\KZWFGFEG\\Chopin - 2004 - Central limit theorem for sequential Monte Carlo m.pdf}
}

@book{chopin_introduction_2020,
  title = {An {{Introduction}} to {{Sequential Monte Carlo}}},
  author = {Chopin, Nicolas and Papaspiliopoulos, Omiros},
  year = {2020},
  series = {Springer {{Series}} in {{Statistics}}},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  langid = {english}
}

@article{chopin_sequential_2002,
  title = {A Sequential Particle Filter Method for Static Models},
  author = {Chopin, Nicolas},
  year = {2002},
  month = aug,
  journal = {Biometrika},
  volume = {89},
  number = {3},
  pages = {539--552},
  abstract = {Particle filter methods are complex inference procedures, which combine importance sampling and Monte Carlo schemes in order to explore consistently a sequence of multiple distributions of interest. We show that such methods can also offer an efficient estimation tool in `static' set-ups, in which case {$\pi$}(\texttheta{} | y1, \ldots, yN) (n \&lt; N) is the only posterior distribution of interest but the preliminary exploration of partial posteriors {$\pi$}(\texttheta{} | y1, \ldots, yn) makes it possible to save computing time. A complete algorithm is proposed for independent or Markov models. Our method is shown to challenge other common estimation procedures in terms of robustness and execution time, especially when the sample size is important. Two classes of examples, mixture models and discrete generalised linear models, are discussed and illustrated by numerical results.}
}

@article{chow_review_2016,
  title = {Review of Medical Image Quality Assessment},
  author = {Chow, Li Sze and Paramesran, Raveendran},
  year = {2016},
  month = may,
  journal = {Biomedical Signal Processing and Control},
  volume = {27},
  pages = {145--154},
  langid = {english}
}

@article{christen_general_2010,
  title = {A General Purpose Sampling Algorithm for Continuous Distributions (the t-Walk)},
  author = {Christen, J. Andr{\'e}s and Fox, Colin},
  year = {2010},
  month = jun,
  journal = {Bayesian Analysis},
  volume = {5},
  number = {2},
  pages = {263--281},
  langid = {english},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\L24GLPZT\\Christen and Fox - 2010 - A general purpose sampling algorithm for continuou.pdf}
}

@inproceedings{christen_patus_2011,
  title = {{{PATUS}}: {{A Code Generation}} and {{Autotuning Framework}} for {{Parallel Iterative Stencil Computations}} on {{Modern Microarchitectures}}},
  shorttitle = {{{PATUS}}},
  booktitle = {2011 {{IEEE International Parallel}} \& {{Distributed Processing Symposium}}},
  author = {Christen, Matthias and Schenk, Olaf and Burkhart, Helmar},
  year = {2011},
  month = may,
  pages = {676--687},
  publisher = {{IEEE}},
  address = {{Anchorage, AK, USA}}
}

@article{christmas_robust_2011,
  title = {Robust Autoregression: {{Student-T}} Innovations Using Variational {{Bayes}}},
  shorttitle = {Robust {{Autoregression}}},
  author = {Christmas, Jacqueline and Everson, Richard},
  year = {2011},
  month = jan,
  journal = {IEEE Transactions on Signal Processing},
  volume = {59},
  number = {1},
  pages = {48--57},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\Z4KHKJIT\\Christmas and Everson - 2011 - Robust Autoregression Student-t Innovations Using.pdf}
}

@article{chung_broadband_2010,
  title = {Broadband {{ML}} Estimation under Model Order Uncertainty},
  author = {Chung, Pei-Jung and Viberg, Mats and Mecklenbr{\"a}uker, Christoph F.},
  year = {2010},
  month = may,
  journal = {Signal Processing},
  series = {Special {{Section}} on {{Statistical Signal}} \& {{Array Processing}}},
  volume = {90},
  number = {5},
  pages = {1350--1356},
  abstract = {The number of signals hidden in data plays a crucial role in array processing. When this information is not available, conventional approaches apply information theoretic criteria or multiple hypothesis tests to simultaneously estimate model order and parameter. These methods are usually computationally intensive, since ML estimates are required for a hierarchy of nested models. In this contribution, we propose a computationally efficient solution to avoid this full search procedure and address issues unique to the broadband case. Our max-search approach computes ML estimates only for the maximally hypothesized number of signals, and selects relevant components through hypothesis testing. Furthermore, we introduce a criterion based on the rank of the steering matrix to reduce indistinguishable components caused by overparameterization. Numerical experiments show that despite model order uncertainty, the proposed method achieves comparable estimation and detection accuracy as standard methods, but at much lower computational expense.},
  langid = {english},
  keywords = {Broadband signals,Direction of arrival,Maximum likelihood estimation,Overparameterized models,Unknown number of signals},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\5ZIXIMWP\\Chung et al. - 2010 - Broadband ML estimation under model order uncertai.pdf}
}

@inproceedings{chung_case_2006,
  title = {A {{Case Study Using Automatic Performance Tuning}} for {{Large-Scale Scientific Programs}}},
  booktitle = {2006 15th {{IEEE International Conference}} on {{High Performance Distributed Computing}}},
  author = {Chung, I.-H. and Hollingsworth, J.K.},
  year = {2006},
  pages = {45--56},
  publisher = {{IEEE}},
  address = {{Paris, France}}
}

@article{chung_comparative_2001,
  title = {Comparative Convergence Analysis of {{EM}} and {{SAGE}} Algorithms in {{DOA}} Estimation},
  author = {Chung, Pei Jung and Bohme, J.F.},
  year = {2001},
  month = dec,
  journal = {IEEE Transactions on Signal Processing},
  volume = {49},
  number = {12},
  pages = {2940--2949},
  abstract = {In this work, the convergence rates of direction of arrival (DOA) estimates using the expectation-maximization (EM) and space alternating generalized EM (SAGE) algorithms are investigated. The EM algorithm is a well-known iterative method for locating modes of a likelihood function and is characterized by simple implementation and stability. Unfortunately, the slow convergence associated with EM makes it less attractive for practical applications. The SAGE algorithm proposed by Fessler and Hero (1994), based on the same idea of data augmentation, has the potential to speed up convergence and preserves the advantage of simple implementation. We study both algorithms within the framework of array processing. Theoretical analysis shows that SAGE has faster convergence speed than EM under certain conditions on observed and augmented information matrices. The analytical results are supported by numerical simulations carried out over a wide range of signal-to-noise ratios (SNRs) and various source locations.},
  keywords = {Algorithm design and analysis,Array signal processing,Convergence,Direction of arrival estimation,Information analysis,Iterative algorithms,Iterative methods,Numerical simulation,Signal analysis,Stability},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\G2LMJDVV\\Chung and Bohme - 2001 - Comparative convergence analysis of EM and SAGE al.pdf}
}

@article{chung_detection_2007,
  title = {Detection of the Number of Signals Using the {{Benjamini-Hochberg}} Procedure},
  author = {Chung, Pei-Jung and Bohme, Johann F. and Mecklenbrauker, Christoph F. and Hero, Alfred O.},
  year = {2007},
  month = jun,
  journal = {IEEE Transactions on Signal Processing},
  volume = {55},
  number = {6},
  pages = {2497--2508},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\ZMCIRCTD\\Chung et al. - 2007 - Detection of the Number of Signals Using the Benja.pdf}
}

@incollection{chung_doa_2014,
  title = {{{DOA}} Estimation Methods and Algorithms},
  booktitle = {Array and {{Statistical Signal Processing}}},
  author = {Chung, Pei-Jung and Viberg, Mats and Yu, Jia},
  year = {2014},
  month = jan,
  series = {Academic {{Press Library}} in {{Signal Processing}}},
  number = {3},
  pages = {599--650},
  publisher = {{Elsevier}},
  abstract = {Estimation of direction of arrival (DOA) from data collected by sensor arrays is of fundamental importance to a variety of applications such as radar, sonar, wireless communications, geophysics and biomedical engineering. Significant progress in the development of algorithms has been made over the last three decades. This article provides an overview of DOA estimation methods that are relevant in theory and practice. We will present estimators based on beamforming, subspace and parametric approaches and compare their performance in terms of estimation accuracy, resolution capability and computational complexity. Methods for processing broadband data and signal detection will be discussed as well. Finally, a brief discussion will be given to application specific algorithms.},
  langid = {english},
  keywords = {Beamforming,Direction of arrival (DOA),Estimation,High resolution methods,Maximum likelihood,Sensor array processing,Signal detection,Subspace methods},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\PKXNM63S\\B978012411597200014X.html}
}

@article{churchill_synthetic_2020,
  title = {Synthetic {{Aperture Radar Image}} Formation with Uncertainty Quantification},
  author = {Churchill, Victor and Gelb, Anne},
  year = {2020},
  month = jul,
  journal = {arXiv:2007.06380 [eess, stat]},
  eprint = {2007.06380},
  eprinttype = {arxiv},
  primaryclass = {eess, stat},
  abstract = {Synthetic aperture radar (SAR) is a day or night any-weather imaging modality that is an important tool in remote sensing. Most existing SAR image formation methods result in a maximum a posteriori image which approximates the reflectivity of an unknown ground scene. This single image provides no quantification of the certainty with which the features in the estimate should be trusted. In addition, finding the mode is generally not the best way to interrogate a posterior. This paper addresses these issues by introducing a sampling framework to SAR image formation. A hierarchical Bayesian model is constructed using conjugate priors that directly incorporate coherent imaging and the problematic speckle phenomenon which is known to degrade image quality. Samples of the resulting posterior as well as parameters governing speckle and noise are obtained using a Gibbs sampler. These samples may then be used to compute estimates, and also to derive other statistics like variance which aid in uncertainty quantification. The latter information is particularly important in SAR, where ground truth images even for synthetically-created examples are typically unknown. An example result using real-world data shows that the sampling-based approach introduced here to SAR image formation provides parameter-free estimates with improved contrast and significantly reduced speckle, as well as unprecedented uncertainty quantification information.},
  archiveprefix = {arXiv},
  keywords = {Electrical Engineering and Systems Science - Image and Video Processing,Statistics - Applications},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\EAJ7A5G7\\Churchill and Gelb - 2020 - Synthetic Aperture Radar Image Formation with Unce.pdf;C\:\\Users\\msca8h\\Zotero\\storage\\2WX44B2A\\2007.html}
}

@inproceedings{ciorba_openmp_2018,
  title = {{{OpenMP}} Loop Scheduling Revisited: Making a Case for More Schedules},
  booktitle = {Evolving {{OpenMP}} for {{Evolving Architectures}}},
  author = {Ciorba, F. M. and Iwainsky, C. and Buder, P.},
  year = {2018},
  series = {{{IWOMP}}'18},
  pages = {21--36},
  publisher = {{Springer}},
  abstract = {In light of continued advances in loop scheduling, this work revisits the OpenMP loop scheduling by outlining the current state of the art in loop scheduling and presenting evidence that the existing OpenMP schedules are insufficient for all combinations of applications, systems, and their characteristics. A review of the state of the art shows that due to the specifics of the parallel applications, the variety of computing platforms, and the numerous performance degradation factors, no single loop scheduling technique can be a 'one-fits-all' solution to effectively optimize the performance of all parallel applications in all situations. The impact of irregularity in computational workloads and hardware systems, including operating system noise, on the performance of parallel applications, results in performance loss and has often been neglected in loop scheduling research, in particular, the context of OpenMP schedules. Existing dynamic loop self-scheduling techniques, such as trapezoid self-scheduling, factoring, and weighted factoring, offer an unexplored potential to alleviate this degradation in OpenMP due to the fact that they explicitly target the minimization of load imbalance and scheduling overhead. Through theoretical and experimental evaluation, this work shows that these loop self-scheduling methods provide a benefit in the context of OpenMP. In conclusion, OpenMP must include more schedules to offer a broader performance coverage of applications executing on an increasing variety of heterogeneous shared memory computing platforms.},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\G4QJBAKP\\OMP_scheduling_2018.pdf}
}

@article{claudio_space_2018,
  title = {Space Time {{MUSIC}}: {{Consistent}} Signal Subspace Estimation for Wideband Sensor Arrays},
  shorttitle = {Space {{Time MUSIC}}},
  author = {Claudio, Elio D. Di and Parisi, Raffaele and Jacovitti, Giovanni},
  year = {2018},
  month = may,
  journal = {IEEE Transactions on Signal Processing},
  volume = {66},
  number = {10},
  pages = {2685--2699},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\THGLI8PP\\Claudio et al. - 2018 - Space Time MUSIC Consistent Signal Subspace Estim.pdf}
}

@article{clintwhaley_automated_2001,
  title = {Automated Empirical Optimizations of Software and the {{ATLAS}} Project},
  author = {Clint Whaley, R. and Petitet, Antoine and Dongarra, Jack J.},
  year = {2001},
  month = jan,
  journal = {Parallel Computing},
  volume = {27},
  number = {1-2},
  pages = {3--35},
  langid = {english}
}

@article{cobb_scaling_2020,
  title = {Scaling {{Hamiltonian Monte Carlo}} Inference for {{Bayesian}} Neural Networks with Symmetric Splitting},
  author = {Cobb, Adam D. and Jalaian, Brian},
  year = {2020},
  month = oct,
  journal = {arXiv:2010.06772 [cs, stat]},
  eprint = {2010.06772},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Hamiltonian Monte Carlo (HMC) is a Markov chain Monte Carlo (MCMC) approach that exhibits favourable exploration properties in high-dimensional models such as neural networks. Unfortunately, HMC has limited use in large-data regimes and little work has explored suitable approaches that aim to preserve the entire Hamiltonian. In our work, we introduce a new symmetric integration scheme for split HMC that does not rely on stochastic gradients. We show that our new formulation is more efficient than previous approaches and is easy to implement with a single GPU. As a result, we are able to perform full HMC over common deep learning architectures using entire data sets. In addition, when we compare with stochastic gradient MCMC, we show that our method achieves better performance in both accuracy and uncertainty quantification. Our approach demonstrates HMC as a feasible option when considering inference schemes for large-scale machine learning problems.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\QLSNMQ5R\\Cobb and Jalaian - 2020 - Scaling Hamiltonian Monte Carlo Inference for Baye.pdf;C\:\\Users\\msca8h\\Zotero\\storage\\NNW94ZQ6\\2010.html}
}

@book{cochran_sampling_1977,
  title = {Sampling Techniques},
  author = {Cochran, William Gemmell},
  year = {1977},
  series = {Wiley Series in Probability and Mathematical Statistics},
  edition = {3d ed},
  publisher = {{Wiley}},
  address = {{New York}},
  lccn = {QA276.6 .C6 1977},
  keywords = {Sampling (Statistics)}
}

@inproceedings{cohen_progressive_1988,
  title = {A {{Progressive Refinement Approach}} to {{Fast Radiosity Image Generation}}},
  booktitle = {Proceedings of the 15th {{Annual Conference}} on {{Computer Graphics}} and {{Interactive Techniques}}},
  author = {Cohen, Michael F. and Chen, Shenchang Eric and Wallace, John R. and Greenberg, Donald P.},
  year = {1988},
  series = {{{SIGGRAPH}} '88},
  pages = {75--84},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  abstract = {A reformulated radiosity algorithm is presented that produces initial images in time linear to the number of patches. The enormous memory costs of the radiosity algorithm are also eliminated by computing form-factors on-the-fly. The technique is based on the approach of rendering by progressive refinement. The algorithm provides a useful solution almost immediately which progresses gracefully and continuously to the complete radiosity solution. In this way the competing demands of realism and interactivity are accommodated. The technique brings the use of radiosity for interactive rendering within reach and has implications for the use and development of current and future graphics workstations.},
  keywords = {adaptive subdivsion,backward ray tracing,global illumination,progressive refinement,radiosity,z-buffer},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\PCBUV2GR\\Cohen et al. - 1988 - A Progressive Refinement Approach to Fast Radiosit.pdf}
}

@incollection{cohen_pulse_1987,
  title = {Pulse Compression in Radar Systems},
  booktitle = {Principles of {{Modern Radar}}},
  author = {Cohen, Marvin N.},
  year = {1987},
  pages = {465--501},
  publisher = {{Springer US}},
  address = {{Boston, MA}},
  langid = {english},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\ZI3LDLGD\\Cohen - 1987 - Pulse compression in radar systems.pdf}
}

@article{cohen_randomout_2017,
  title = {{{RandomOut}}: {{Using}} a Convolutional Gradient Norm to Rescue Convolutional Filters},
  shorttitle = {{{RandomOut}}},
  author = {Cohen, Joseph Paul and Lo, Henry Z. and Ding, Wei},
  year = {2017},
  month = may,
  journal = {arXiv:1602.05931 [cs]},
  eprint = {1602.05931},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Filters in convolutional neural networks are sensitive to their initialization. The random numbers used to initialize filters are a bias and determine if you will "win" and converge to a satisfactory local minimum so we call this The Filter Lottery. We observe that the 28x28 Inception-V3 model without Batch Normalization fails to train 26\% of the time when varying the random seed alone. This is a problem that affects the trial and error process of designing a network. Because random seeds have a large impact it makes it hard to evaluate a network design without trying many different random starting weights. This work aims to reduce the bias imposed by the initial weights so a network converges more consistently. We propose to evaluate and replace specific convolutional filters that have little impact on the prediction. We use the gradient norm to evaluate the impact of a filter on error, and re-initialize filters when the gradient norm of its weights falls below a specific threshold. This consistently improves accuracy on the 28x28 Inception-V3 with a median increase of +3.3\%. In effect our method RandomOut increases the number of filters explored without increasing the size of the network. We observe that the RandomOut method has more consistent generalization performance, having a standard deviation of 1.3\% instead of 2\% when varying random seeds, and does so faster and with fewer parameters.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\IBJNGDRC\\Cohen et al. - 2017 - RandomOut Using a convolutional gradient norm to .pdf;C\:\\Users\\msca8h\\Zotero\\storage\\FRJ2TCAR\\1602.html}
}

@inproceedings{coiras_gpubased_2009,
  title = {{{GPU-based}} Simulation of Side-Looking Sonar Images},
  booktitle = {{{OCEANS}} 2009-{{EUROPE}}},
  author = {Coiras, E. and {Ramirez-Montesinos}, A. and Groen, J.},
  year = {2009},
  month = may,
  pages = {1--6},
  publisher = {{IEEE}},
  address = {{Bremen, Germany}}
}

@inproceedings{coker_wide_2022,
  title = {Wide Mean-Field {{Bayesian}} Neural Networks Ignore the Data},
  booktitle = {Proceedings of the {{International Conference}} on {{Artificial Intelligence}} and {{Statistics}}},
  author = {Coker, Beau and Bruinsma, Wessel P. and Burt, David R. and Pan, Weiwei and {Doshi-Velez}, Finale},
  year = {2022},
  month = may,
  pages = {5276--5333},
  publisher = {{PMLR}},
  abstract = {Bayesian neural networks (BNNs) combine the expressive power of deep learning with the advantages of Bayesian formalism. In recent years, the analysis of wide, deep BNNs has provided theoretical insight into their priors and posteriors. However, we have no analogous insight into their posteriors under approximate inference. In this work, we show that mean-field variational inference entirely fails to model the data when the network width is large and the activation function is odd. Specifically, for fully-connected BNNs with odd activation functions and a homoscedastic Gaussian likelihood, we show that the optimal mean-field variational posterior predictive (i.e., function space) distribution converges to the prior predictive distribution as the width tends to infinity. We generalize aspects of this result to other likelihoods. Our theoretical results are suggestive of underfitting behavior previously observered in BNNs. While our convergence bounds are non-asymptotic and constants in our analysis can be computed, they are currently too loose to be applicable in standard training regimes. Finally, we show that the optimal approximate posterior need not tend to the prior if the activation function is not odd, showing that our statements cannot be generalized arbitrarily.},
  langid = {english},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\PGHHDXVB\\Coker et al. - 2022 - Wide Mean-Field Bayesian Neural Networks Ignore th.pdf}
}

@inproceedings{coll-font_ecgbased_2017,
  title = {{{ECG-based}} Reconstruction of Heart Position and Orientation with {{Bayesian}} Optimization},
  booktitle = {2017 {{Computing}} in {{Cardiology Conference}}},
  author = {{Coll-Font}, Jaume and Brooks, Dana},
  year = {2017},
  month = sep,
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\BGBDC9EL\\Coll-Font and Brooks - 2017 - ECG-Based Reconstruction of Heart Position and Ori.pdf}
}

@article{consonni_prior_2018,
  title = {Prior Distributions for Objective {{Bayesian}} Analysis},
  author = {Consonni, Guido and Fouskakis, Dimitris and Liseo, Brunero and Ntzoufras, Ioannis},
  year = {2018},
  month = jun,
  journal = {Bayesian Analysis},
  volume = {13},
  number = {2},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\RA75BCQH\\Consonni et al. - 2018 - Prior Distributions for Objective Bayesian Analysi.pdf}
}

@article{contrerasortiz_ultrasound_2012,
  title = {Ultrasound Image Enhancement: A Review},
  shorttitle = {Ultrasound Image Enhancement},
  author = {Contreras Ortiz, Sonia H. and Chiu, Tsuicheng and Fox, Martin D.},
  year = {2012},
  month = sep,
  journal = {Biomedical Signal Processing and Control},
  volume = {7},
  number = {5},
  pages = {419--428},
  langid = {english}
}

@mastersthesis{cook_synthetic_2007,
  title = {Synthetic Aperture Sonar Motion Estimation and Compensation},
  author = {Cook, Daniel A.},
  year = {2007},
  school = {Georgia Institute of Technology},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\MTQ8F8RN\\Cook - 2007 - Synthetic aperture sonar motion estimation and com.pdf}
}

@article{copsey_bayesian_2002,
  title = {Bayesian Analysis of Generalized Frequency-Modulated Signals},
  author = {Copsey, K. and Gordon, N. and Marrs, A.},
  year = {2002},
  month = mar,
  journal = {IEEE Transactions on Signal Processing},
  volume = {50},
  number = {3},
  pages = {725--735}
}

@article{copsey_bayesian_2002a,
  title = {Bayesian Analysis of Generalized Frequency-Modulated Signals},
  author = {Copsey, K. and Gordon, N. and Marrs, A.},
  year = {2002},
  month = mar,
  journal = {IEEE Transactions on Signal Processing},
  volume = {50},
  number = {3},
  pages = {725--735}
}

@inproceedings{cornish_scalable_2019,
  title = {Scalable {{Metropolis-Hastings}} for Exact {{Bayesian}} Inference with Large Datasets},
  booktitle = {Proceedings of the 36th {{International Conference}} on {{Machine Learning}}},
  author = {Cornish, Rob and Vanetti, Paul and {Bouchard-Cote}, Alexandre and Deligiannidis, George and Doucet, Arnaud},
  year = {2019},
  month = may,
  pages = {1351--1360},
  publisher = {{PMLR}},
  abstract = {Bayesian inference via standard Markov Chain Monte Carlo (MCMC) methods such as Metropolis-Hastings is too computationally intensive to handle large datasets, since the cost per step usually scales like O(n)O(n)O(n) in the number of data points nnn. We propose the Scalable Metropolis-Hastings (SMH) kernel that only requires processing on average O(1)O(1)O(1) or even O(1/n--{$\surd$})O(1/n)O(1/\textbackslash sqrt\{n\}) data points per step. This scheme is based on a combination of factorized acceptance probabilities, procedures for fast simulation of Bernoulli processes, and control variate ideas. Contrary to many MCMC subsampling schemes such as fixed step-size Stochastic Gradient Langevin Dynamics, our approach is exact insofar as the invariant distribution is the true posterior and not an approximation to it. We characterise the performance of our algorithm theoretically, and give realistic and verifiable conditions under which it is geometrically ergodic. This theory is borne out by empirical results that demonstrate overall performance benefits over standard Metropolis-Hastings and various subsampling algorithms.},
  langid = {english},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\EVEPV35H\\Cornish et al. - 2019 - Scalable Metropolis-Hastings for Exact Bayesian In.pdf;C\:\\Users\\msca8h\\Zotero\\storage\\FTCFFDHY\\Cornish et al. - 2019 - Scalable Metropolis-Hastings for Exact Bayesian In.pdf}
}

@article{cornuet_adaptive_2012,
  title = {Adaptive Multiple Importance Sampling},
  shorttitle = {Adaptive {{Multiple Importance Sampling}}},
  author = {Cornuet, Jean-Marie and Marin, Jean-Michel and Mira, Antonietta and Robert, Christian P.},
  year = {2012},
  month = dec,
  journal = {Scandinavian Journal of Statistics},
  volume = {39},
  number = {4},
  pages = {798--812},
  langid = {english},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\9TMNYH7N\\Cornuet et al. - 2012 - Adaptive Multiple Importance Sampling Adaptive.pdf}
}

@article{coupe_nonlocal_2009,
  title = {Nonlocal Means-Based Speckle Filtering for Ultrasound Images},
  author = {Coupe, Pierrick and Hellier, Pierre and Kervrann, Charles and Barillot, Christian},
  year = {2009},
  month = oct,
  journal = {IEEE Transactions on Image Processing},
  volume = {18},
  number = {10},
  pages = {2221--2229},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\9IFRANXP\\Coupe et al. - 2009 - Nonlocal means-based speckle filtering for ultraso.pdf}
}

@article{craiu_learn_2009,
  title = {Learn from Thy Neighbor: {{Parallel-chain}} and Regional Adaptive {{MCMC}}},
  shorttitle = {Learn {{From Thy Neighbor}}},
  author = {Craiu, Radu V. and Rosenthal, Jeffrey and Yang, Chao},
  year = {2009},
  month = dec,
  journal = {Journal of the American Statistical Association},
  volume = {104},
  number = {488},
  pages = {1454--1466},
  langid = {english},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\63PRJ2PE\\Craiu et al. - 2009 - Learn From Thy Neighbor Parallel-Chain and Region.pdf}
}

@article{cuevas_bayesian_2021,
  title = {Bayesian Autoregressive Spectral Estimation},
  author = {Cuevas, Alejandro and L{\'o}pez, Sebasti{\'a}n and Mandic, Danilo and Tobar, Felipe},
  year = {2021},
  journal = {Proceedings of the IEEE Latin American Conference on Computational Intelligence},
  eprint = {2110.02156},
  eprinttype = {arxiv},
  primaryclass = {eess},
  publisher = {{arXiv}},
  abstract = {Autoregressive (AR) time series models are widely used in parametric spectral estimation (SE), where the power spectral density (PSD) of the time series is approximated by that of the \textbackslash emph\{best-fit\} AR model, which is available in closed form. Since AR parameters are usually found via maximum-likelihood, least squares or the method of moments, AR-based SE fails to account for the uncertainty of the approximate PSD, and thus only yields point estimates. We propose to handle the uncertainty related to the AR approximation by finding the full posterior distribution of the AR parameters to then propagate this uncertainty to the PSD approximation by \textbackslash emph\{integrating out the AR parameters\}; we implement this concept by assuming two different priors over the model noise. Through practical experiments, we show that the proposed Bayesian autoregressive spectral estimation (BASE) provides point estimates that follow closely those of standard autoregressive spectral estimation (ASE), while also providing error bars. BASE is validated against ASE and the Periodogram on both synthetic and real-world signals.},
  archiveprefix = {arXiv},
  keywords = {Electrical Engineering and Systems Science - Signal Processing},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\4CPI652D\\Cuevas et al. - 2021 - Bayesian autoregressive spectral estimation.pdf;C\:\\Users\\msca8h\\Zotero\\storage\\2LFPZPFM\\2110.html}
}

@article{cui_low_2019,
  title = {Low Complexity {{DOA}} Estimation for Wideband Off-Grid Sources Based on Re-Focused Compressive Sensing with Dynamic Dictionary},
  author = {Cui, Wei and Shen, Qing and Liu, Wei and Wu, Siliang},
  year = {2019},
  month = sep,
  journal = {IEEE Journal of Selected Topics in Signal Processing},
  volume = {13},
  number = {5},
  pages = {918--930},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\A4F66V4E\\Cui et al. - 2019 - Low Complexity DOA Estimation for Wideband Off-Gri.pdf}
}

@inproceedings{cutkosky_momentumbased_2019,
  title = {Momentum-Based Variance Reduction in Non-Convex {{SGD}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Cutkosky, Ashok and Orabona, Francesco},
  year = {2019},
  volume = {32},
  publisher = {{Curran Associates, Inc.}},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\4XNANAXY\\Cutkosky and Orabona - 2019 - Momentum-Based Variance Reduction in Non-Convex SG.pdf}
}

@article{dagum_openmp_1998,
  title = {{{OpenMP}}: An Industry Standard {{API}} for Shared-Memory Programming},
  shorttitle = {{{OpenMP}}},
  author = {Dagum, L. and Menon, R.},
  year = {Jan.-March/1998},
  journal = {IEEE Computational Science and Engineering},
  volume = {5},
  number = {1},
  pages = {46--55},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\6N6LSIYQ\\Dagum and Menon - 1998 - OpenMP an industry standard API for shared-memory.pdf}
}

@inproceedings{dai_bayesian_2019,
  title = {Bayesian {{Optimization Meets Bayesian Optimal Stopping}}},
  booktitle = {Proceedings of the 36th {{International Conference}} on {{Machine Learning}}},
  author = {Dai, Zhongxiang and Yu, Haibin and Low, Bryan Kian Hsiang and Jaillet, Patrick},
  year = {2019},
  month = jun,
  series = {Proceedings of {{Machine Learning Research}}},
  volume = {97},
  pages = {1496--1506},
  publisher = {{PMLR}},
  address = {{Long Beach, California, USA}},
  abstract = {Bayesian optimization (BO) is a popular paradigm for optimizing the hyperparameters of machine learning (ML) models due to its sample efficiency. Many ML models require running an iterative training procedure (e.g., stochastic gradient descent). This motivates the question whether information available during the training process (e.g., validation accuracy after each epoch) can be exploited for improving the epoch efficiency of BO algorithms by early-stopping model training under hyperparameter settings that will end up under-performing and hence eliminating unnecessary training epochs. This paper proposes to unify BO (specifically, Gaussian process-upper confidence bound (GP-UCB)) with Bayesian optimal stopping (BO-BOS) to boost the epoch efficiency of BO. To achieve this, while GP-UCB is sample-efficient in the number of function evaluations, BOS complements it with epoch efficiency for each function evaluation by providing a principled optimal stopping mechanism for early stopping. BO-BOS preserves the (asymptotic) no-regret performance of GP-UCB using our specified choice of BOS parameters that is amenable to an elegant interpretation in terms of the exploration-exploitation trade-off. We empirically evaluate the performance of BO-BOS and demonstrate its generality in hyperparameter optimization of ML models and two other interesting applications.}
}

@inproceedings{dai_bayesian_2019a,
  title = {Bayesian {{Optimization Meets Bayesian Optimal Stopping}}},
  booktitle = {Proceedings of the 36th {{International Conference}} on {{Machine Learning}}},
  author = {Dai, Zhongxiang and Yu, Haibin and Low, Bryan Kian Hsiang and Jaillet, Patrick},
  year = {2019},
  month = jun,
  series = {Proceedings of {{Machine Learning Research}}},
  volume = {97},
  pages = {1496--1506},
  publisher = {{PMLR}},
  address = {{Long Beach, California, USA}},
  abstract = {Bayesian optimization (BO) is a popular paradigm for optimizing the hyperparameters of machine learning (ML) models due to its sample efficiency. Many ML models require running an iterative training procedure (e.g., stochastic gradient descent). This motivates the question whether information available during the training process (e.g., validation accuracy after each epoch) can be exploited for improving the epoch efficiency of BO algorithms by early-stopping model training under hyperparameter settings that will end up under-performing and hence eliminating unnecessary training epochs. This paper proposes to unify BO (specifically, Gaussian process-upper confidence bound (GP-UCB)) with Bayesian optimal stopping (BO-BOS) to boost the epoch efficiency of BO. To achieve this, while GP-UCB is sample-efficient in the number of function evaluations, BOS complements it with epoch efficiency for each function evaluation by providing a principled optimal stopping mechanism for early stopping. BO-BOS preserves the (asymptotic) no-regret performance of GP-UCB using our specified choice of BOS parameters that is amenable to an elegant interpretation in terms of the exploration-exploitation trade-off. We empirically evaluate the performance of BO-BOS and demonstrate its generality in hyperparameter optimization of ML models and two other interesting applications.}
}

@article{dai_invitation_2020,
  title = {An Invitation to Sequential {{Monte Carlo}} Samplers},
  author = {Dai, Chenguang and Heng, Jeremy and Jacob, Pierre E. and Whiteley, Nick},
  year = {2020},
  month = jul,
  journal = {arXiv:2007.11936 [stat]},
  eprint = {2007.11936},
  eprinttype = {arxiv},
  primaryclass = {stat},
  abstract = {Sequential Monte Carlo samplers provide consistent approximations of sequences of probability distributions and of their normalizing constants, via particles obtained with a combination of importance weights and Markov transitions. This article presents this class of methods and a number of recent advances, with the goal of helping statisticians assess the applicability and usefulness of these methods for their purposes. Our presentation emphasizes the role of bridging distributions for computational and statistical purposes. Numerical experiments are provided on simple settings such as multivariate Normals, logistic regression and a basic susceptible-infected-recovered model, illustrating the impact of the dimension, the ability to perform inference sequentially and the estimation of normalizing constants.},
  archiveprefix = {arXiv},
  keywords = {Statistics - Computation,Statistics - Methodology},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\NTFKYZUH\\Dai et al. - 2020 - An invitation to sequential Monte Carlo samplers.pdf;C\:\\Users\\msca8h\\Zotero\\storage\\RIJC7RCL\\2007.html}
}

@article{dai_monte_2019,
  title = {Monte {{Carlo}} Fusion},
  author = {Dai, Hongsheng and Pollock, Murray and Roberts, Gareth},
  year = {2019},
  month = mar,
  journal = {Journal of Applied Probability},
  volume = {56},
  number = {01},
  pages = {174--191},
  abstract = {Abstract             In this paper we propose a new theory and methodology to tackle the problem of unifying Monte Carlo samples from distributed densities into a single Monte Carlo draw from the target density. This surprisingly challenging problem arises in many settings (for instance, expert elicitation, multiview learning, distributed `big data' problems, etc.), but to date the framework and methodology proposed in this paper (Monte Carlo fusion) is the first general approach which avoids any form of approximation error in obtaining the unified inference. In this paper we focus on the key theoretical underpinnings of this new methodology, and simple (direct) Monte Carlo interpretations of the theory. There is considerable scope to tailor the theory introduced in this paper to particular application settings (such as the big data setting), construct efficient parallelised schemes, understand the approximation and computational efficiencies of other such unification paradigms, and explore new theoretical and methodological directions.},
  langid = {english},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\H7DR4KCA\\Dai et al. - 2019 - Monte Carlo fusion.pdf}
}

@article{dai_sparse_2018,
  title = {Sparse {{Bayesian}} Learning Approach for Outlier-Resistant Direction-of-Arrival Estimation},
  author = {Dai, Jisheng and So, Hing Cheung},
  year = {2018},
  month = feb,
  journal = {IEEE Transactions on Signal Processing},
  volume = {66},
  number = {3},
  pages = {744--756}
}

@inproceedings{dai_usual_2020,
  title = {The Usual Suspects? {{Reassessing}} Blame for {{VAE}} Posterior Collapse},
  shorttitle = {The Usual Suspects?},
  booktitle = {Proceedings of the 37th {{International Conference}} on {{Machine Learning}}},
  author = {Dai, Bin and Wang, Ziyu and Wipf, David},
  year = {2020},
  month = nov,
  pages = {2313--2322},
  publisher = {{PMLR}},
  abstract = {In narrow asymptotic settings Gaussian VAE models of continuous data have been shown to possess global optima aligned with ground-truth distributions. Even so, it is well known that poor solutions whereby the latent posterior collapses to an uninformative prior are sometimes obtained in practice. However, contrary to conventional wisdom that largely assigns blame for this phenomena on the undue influence of KL-divergence regularization, we will argue that posterior collapse is, at least in part, a direct consequence of bad local minima inherent to the loss surface of deep autoencoder networks. In particular, we prove that even small nonlinear perturbations of affine VAE decoder models can produce such minima, and in deeper models, analogous minima can force the VAE to behave like an aggressive truncation operator, provably discarding information along all latent dimensions in certain circumstances. Regardless, the underlying message here is not meant to undercut valuable existing explanations of posterior collapse, but rather, to refine the discussion and elucidate alternative risk factors that may have been previously underappreciated.},
  langid = {english},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\3AISSGYC\\Dai et al. - 2020 - The Usual Suspects Reassessing Blame for VAE Post.pdf;C\:\\Users\\msca8h\\Zotero\\storage\\WSZYZ3E7\\Dai et al. - 2020 - The Usual Suspects Reassessing Blame for VAE Post.pdf}
}

@article{dalbey_gaussian_2014,
  title = {Gaussian {{Process Adaptive Importance Sampling}}},
  author = {Dalbey, Keith R and Swiler, Laura P},
  year = {2014},
  journal = {International Journal for Uncertainty Quantification},
  volume = {4},
  number = {2},
  pages = {133--149}
}

@inproceedings{dalibard_boat_2017,
  title = {{{BOAT}}: Building Auto-Tuners with Structured {{Bayesian}} Optimization},
  shorttitle = {{{BOAT}}},
  booktitle = {Proc. 26th {{Int}}. {{Conf}}. {{World Wide Web}}},
  author = {Dalibard, Valentin and Schaarschmidt, Michael and Yoneki, Eiko},
  year = {2017},
  series = {{{WWW}} '17},
  pages = {479--488},
  publisher = {{ACM Press}},
  address = {{Perth, Australia}},
  langid = {english},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\B3Q82T24\\Dalibard et al. - 2017 - BOAT Building Auto-Tuners with Structured Bayesia.pdf}
}

@article{dan_efficient_1996,
  title = {Efficient {{Coding}} of {{Natural Scenes}} in the {{Lateral Geniculate Nucleus}}: {{Experimental Test}} of a {{Computational Theory}}},
  shorttitle = {Efficient {{Coding}} of {{Natural Scenes}} in the {{Lateral Geniculate Nucleus}}},
  author = {Dan, Yang and Atick, Joseph J. and Reid, R. Clay},
  year = {1996},
  month = may,
  journal = {The Journal of Neuroscience},
  volume = {16},
  number = {10},
  pages = {3351--3362},
  langid = {english},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\H2Y44VPQ\\Dan et al. - 1996 - Efficient Coding of Natural Scenes in the Lateral .pdf}
}

@mastersthesis{danesh_real_2013,
  title = {Real Time Active Sonar Simulation in a Deep Ocean Environment},
  author = {Danesh, Sheida Anya},
  year = {2013},
  school = {Massachusetts Institute of Technology},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\SPAFDMXR\\Danesh - 2013 - Real time active sonar simulation in a deep ocean .pdf}
}

@incollection{danilova_recent_2022,
  title = {Recent {{Theoretical Advances}} in {{Non-Convex Optimization}}},
  booktitle = {High-{{Dimensional Optimization}} and {{Probability}}: {{With}} a {{View Towards Data Science}}},
  author = {Danilova, Marina and Dvurechensky, Pavel and Gasnikov, Alexander and Gorbunov, Eduard and Guminov, Sergey and Kamzolov, Dmitry and Shibaev, Innokentiy},
  year = {2022},
  series = {Springer {{Optimization}} and {{Its Applications}}},
  pages = {79--163},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  abstract = {Motivated by recent increased interest in optimization algorithms for non-convex optimization in application to training deep neural networks and other optimization problems in data analysis, we give an overview of recent theoretical results on global performance guarantees of optimization algorithms for non-convex optimization. We start with classical arguments showing that general non-convex problems could not be solved efficiently in a reasonable time. Then we give a list of problems that can be solved efficiently to find the global minimizer by exploiting the structure of the problem as much as it is possible. Another way to deal with non-convexity is to relax the goal from finding the global minimum to finding a stationary point or a local minimum. For this setting, we first present known results for the convergence rates of deterministic first-order methods, which are then followed by a general theoretical analysis of optimal stochastic and randomized gradient schemes, and an overview of the stochastic first-order methods. After that, we discuss quite general classes of non-convex problems, such as minimization of {$\alpha$}-weakly quasi-convex functions and functions that satisfy Polyak\textendash\L ojasiewicz condition, which still allow obtaining theoretical convergence guarantees of first-order methods. Then we consider higher-order and zeroth-order/derivative-free methods and their convergence rates for non-convex optimization problems.},
  langid = {english},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\MGBNP5A6\\Danilova et al. - 2022 - Recent Theoretical Advances in Non-Convex Optimiza.pdf}
}

@article{das_narrowband_2018,
  title = {Narrowband and Wideband Off-Grid Direction-of-Arrival Estimation via Sparse {{Bayesian}} Learning},
  author = {Das, Anup and Sejnowski, Terrence J.},
  year = {2018},
  month = jan,
  journal = {IEEE Journal of Oceanic Engineering},
  volume = {43},
  number = {1},
  pages = {108--118}
}

@article{das_realvalued_2021,
  title = {Real-Valued Sparse {{Bayesian}} Learning for off-Grid Direction-of-Arrival ({{DOA}}) Estimation in Ocean Acoustics},
  author = {Das, Anup},
  year = {2021},
  month = jan,
  journal = {IEEE Journal of Oceanic Engineering},
  volume = {46},
  number = {1},
  pages = {172--182}
}

@article{das_realvalued_2021a,
  title = {Real-Valued Sparse {{Bayesian}} Learning for off-Grid Direction-of-Arrival ({{DOA}}) Estimation in Ocean Acoustics},
  author = {Das, Anup},
  year = {2021},
  month = jan,
  journal = {IEEE Journal of Oceanic Engineering},
  volume = {46},
  number = {1},
  pages = {172--182}
}

@inproceedings{datta_stencil_2008,
  title = {Stencil {{Computation Optimization}} and {{Auto-Tuning}} on {{State-of-the-Art Multicore Architectures}}},
  booktitle = {Proceedings of the 2008 {{ACM}}/{{IEEE Conference}} on {{Supercomputing}}},
  author = {Datta, Kaushik and Murphy, Mark and Volkov, Vasily and Williams, Samuel and Carter, Jonathan and Oliker, Leonid and Patterson, David and Shalf, John and Yelick, Katherine},
  year = {2008},
  series = {{{SC}} '08},
  publisher = {{IEEE Press}}
}

@article{dau_wastefree_2020,
  title = {Waste-Free {{Sequential Monte Carlo}}},
  author = {Dau, Hai-Dang and Chopin, Nicolas},
  year = {2020},
  month = nov,
  journal = {arXiv:2011.02328 [stat]},
  eprint = {2011.02328},
  eprinttype = {arxiv},
  primaryclass = {stat},
  abstract = {A standard way to move particles in a SMC sampler is to apply several steps of a MCMC (Markov chain Monte Carlo) kernel. Unfortunately, it is not clear how many steps need to be performed for optimal performance. In addition, the output of the intermediate steps are discarded and thus wasted somehow. We propose a new, waste-free SMC algorithm which uses the outputs of all these intermediate MCMC steps as particles. We establish that its output is consistent and asymptotically normal. We use the expression of the asymptotic variance to develop various insights on how to implement the algorithm in practice. We develop in particular a method to estimate, from a single run of the algorithm, the asymptotic variance of any particle estimate. We show empirically, through a range of numerical examples, that waste-free SMC tends to outperform standard SMC samplers, and especially so in situations where the mixing of the considered MCMC kernels decreases across iterations (as in tempering or rare event problems).},
  archiveprefix = {arXiv},
  keywords = {Statistics - Computation},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\AXQGMSHV\\Dau and Chopin - 2020 - Waste-free Sequential Monte Carlo.pdf;C\:\\Users\\msca8h\\Zotero\\storage\\Q9GHV89K\\2011.html}
}

@article{davis_university_2011,
  title = {The University of Florida Sparse Matrix Collection},
  author = {Davis, Timothy A. and Hu, Yifan},
  year = {2011},
  month = dec,
  journal = {ACM Trans. Math. Softw.},
  volume = {38},
  number = {1},
  keywords = {Graph drawing,multilevel algorithms,performance evaluation,sparse matrices}
}

@article{davy_bayesian_2006,
  title = {Bayesian Analysis of Polyphonic Western Tonal Music},
  author = {Davy, Manuel and Godsill, Simon and Idier, J{\'e}r{\^o}me},
  year = {2006},
  month = apr,
  journal = {The Journal of the Acoustical Society of America},
  volume = {119},
  number = {4},
  pages = {2498--2517},
  langid = {english}
}

@article{davy_classification_2002,
  title = {Classification of Chirp Signals Using Hierarchical {{Bayesian}} Learning and {{MCMC}} Methods},
  author = {Davy, M. and Doncarli, C. and Tourneret, J.-Y.},
  year = {Feb./2002},
  journal = {IEEE Transactions on Signal Processing},
  volume = {50},
  number = {2},
  pages = {377--388},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\6CC7MJPM\\Davy et al. - 2002 - Classification of chirp signals using hierarchical.pdf}
}

@article{davy_classification_2002a,
  title = {Classification of Chirp Signals Using Hierarchical {{Bayesian}} Learning and {{MCMC}} Methods},
  author = {Davy, M. and Doncarli, C. and Tourneret, J.-Y.},
  year = {Feb./2002},
  journal = {IEEE Transactions on Signal Processing},
  volume = {50},
  number = {2},
  pages = {377--388}
}

@inproceedings{DBLP:conf/icml/RainforthNLPMDW16,
  title = {Interacting Particle Markov Chain Monte Carlo},
  booktitle = {{{ICML}}},
  author = {Rainforth, Tom and Naesseth, Christian A. and Lindsten, Fredrik and Paige, Brooks and {van de Meent}, Jan-Willem and Doucet, Arnaud and Wood, Frank D.},
  year = {2016},
  pages = {2616--2625},
  cdate = {1451606400000},
  crossref = {conf/icml/2016}
}

@inproceedings{DBLP:journals/corr/BornscheinB14,
  title = {Reweighted Wake-Sleep},
  booktitle = {Proceedings of the {{International}} Conference on Learning Representations},
  author = {Bornschein, J{\"o}rg and Bengio, Yoshua},
  year = {2015},
  month = may,
  address = {{San Diego, California, USA}},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl = {https://dblp.org/rec/journals/corr/BornscheinB14.bib},
  timestamp = {Wed, 17 Jul 2019 10:40:54 +0200}
}

@inproceedings{dean_large_2012,
  title = {Large {{Scale Distributed Deep Networks}}},
  booktitle = {Proceedings of the 25th {{International Conference}} on {{Neural Information Processing Systems}} - {{Volume}} 1},
  author = {Dean, Jeffrey and Corrado, Greg S. and Monga, Rajat and Chen, Kai and Devin, Matthieu and Le, Quoc V. and Mao, Mark Z. and Ranzato, Marc'Aurelio and Senior, Andrew and Tucker, Paul and Yang, Ke and Ng, Andrew Y.},
  year = {2012},
  series = {{{NIPS}}'12},
  pages = {1223--1231},
  publisher = {{Curran Associates Inc.}},
  address = {{USA}}
}

@article{debavelaere_convergence_2021,
  title = {On the Convergence of Stochastic Approximations under a Subgeometric Ergodic {{Markov}} Dynamic},
  author = {Debavelaere, Vianney and Durrleman, Stanley and Allassonni{\`e}re, St{\'e}phanie},
  year = {2021},
  month = jan,
  journal = {Electronic Journal of Statistics},
  volume = {15},
  number = {1},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\LYUQ74KS\\Debavelaere et al. - 2021 - On the convergence of stochastic approximations un.pdf}
}

@article{deboer_tutorial_2005,
  title = {A Tutorial on the Cross-Entropy Method},
  author = {{de Boer}, Pieter-Tjerk and Kroese, Dirk P. and Mannor, Shie and Rubinstein, Reuven Y.},
  year = {2005},
  month = feb,
  journal = {Annals of Operations Research},
  volume = {134},
  number = {1},
  pages = {19--67},
  langid = {english},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\XI565KQW\\de Boer et al. - 2005 - A Tutorial on the Cross-Entropy Method.pdf}
}

@incollection{defazio_ineffectiveness_2019,
  title = {On the {{Ineffectiveness}} of {{Variance Reduced Optimization}} for {{Deep Learning}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 32},
  author = {Defazio, Aaron and Bottou, Leon},
  year = {2019},
  pages = {1755--1765},
  publisher = {{Curran Associates, Inc.}}
}

@article{definelicht_transformations_2021,
  title = {Transformations of High-Level Synthesis Codes for High-Performance Computing},
  author = {{de Fine Licht}, Johannes and Besta, Maciej and Meierhans, Simon and Hoefler, Torsten},
  year = {2021},
  month = may,
  journal = {IEEE Transactions on Parallel and Distributed Systems},
  volume = {32},
  number = {5},
  pages = {1014--1029},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\5KS289A2\\de Fine Licht et al. - 2021 - Transformations of High-Level Synthesis Codes for .pdf}
}

@misc{defossez_simple_2020,
  title = {A Simple Convergence Proof of {{Adam}} and {{Adagrad}}},
  author = {D{\'e}fossez, Alexandre and Bottou, L{\'e}on and Bach, Francis and Usunier, Nicolas},
  year = {2020},
  month = oct,
  number = {arXiv:2003.02395},
  eprint = {2003.02395},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  institution = {{arXiv}},
  abstract = {We provide a simple proof of convergence covering both the Adam and Adagrad adaptive optimization algorithms when applied to smooth (possibly non-convex) objective functions with bounded gradients. We show that in expectation, the squared norm of the objective gradient averaged over the trajectory has an upper-bound which is explicit in the constants of the problem, parameters of the optimizer and the total number of iterations \$N\$. This bound can be made arbitrarily small: Adam with a learning rate \$\textbackslash alpha=1/\textbackslash sqrt\{N\}\$ and a momentum parameter on squared gradients \$\textbackslash beta\_2=1-1/N\$ achieves the same rate of convergence \$O(\textbackslash ln(N)/\textbackslash sqrt\{N\})\$ as Adagrad. Finally, we obtain the tightest dependency on the heavy ball momentum among all previous convergence bounds for non-convex Adam and Adagrad, improving from \$O((1-\textbackslash beta\_1)\^\{-3\})\$ to \$O((1-\textbackslash beta\_1)\^\{-1\})\$. Our technique also improves the best known dependency for standard SGD by a factor \$1 - \textbackslash beta\_1\$.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\AQQV89MP\\Dfossez et al. - 2020 - A Simple Convergence Proof of Adam and Adagrad.pdf;C\:\\Users\\msca8h\\Zotero\\storage\\H5QEKTZV\\2003.html}
}

@article{degooijer_conditional_2003,
  title = {On Conditional Density Estimation},
  author = {De Gooijer, Jan G. and Zerom, Dawit},
  year = {2003},
  month = may,
  journal = {Statistica Neerlandica},
  volume = {57},
  number = {2},
  pages = {159--176},
  langid = {english},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\LYHMFZMD\\De Gooijer and Zerom - 2003 - On Conditional Density Estimation.pdf}
}

@article{degroot_conversation_1986,
  title = {A Conversation with {{Charles Stein}}},
  author = {DeGroot, Morris H.},
  year = {1986},
  journal = {Statistical Science},
  volume = {1},
  number = {4},
  pages = {454--462},
  publisher = {{Institute of Mathematical Statistics}},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\V7KJIELJ\\DeGroot - 1986 - A Conversation with Charles Stein.pdf}
}

@book{delmoral_feynmankac_2004,
  title = {Feynman-{{Kac Formulae}}},
  author = {Del Moral, Pierre},
  year = {2004},
  series = {Probability and Its {{Applications}}},
  publisher = {{Springer New York}},
  address = {{New York, NY}}
}

@article{delmoral_sequential_2006,
  title = {Sequential {{Monte Carlo}} Samplers},
  author = {Del Moral, Pierre and Doucet, Arnaud and Jasra, Ajay},
  year = {2006},
  month = jun,
  journal = {Journal of the Royal Statistical Society Series B (Statistical Methodology)},
  volume = {68},
  number = {3},
  pages = {411--436},
  langid = {english},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\NNQC3GJ2\\Del Moral et al. - 2006 - Sequential Monte Carlo samplers.pdf}
}

@inproceedings{demarco_computationallyefficient_2015,
  title = {A Computationally-Efficient {{2D}} Imaging Sonar Model for Underwater Robotics Simulations in {{Gazebo}}},
  booktitle = {{{OCEANS}} 2015 - {{MTS}}/{{IEEE Washington}}},
  author = {DeMarco, Kevin J. and West, Michael E. and Howard, Ayanna M.},
  year = {2015},
  month = oct,
  pages = {1--7},
  publisher = {{IEEE}},
  address = {{Washington, DC}}
}

@misc{demetrescu_9th_2006,
  title = {9th {{DIMACS}} Implementation Challenge - Shortest Paths},
  author = {Demetrescu, Camil and Goldberg, Andrew and Johnson, David},
  year = {2006}
}

@inproceedings{demuth_frequency_1977,
  title = {Frequency Domain Beamforming Techniques},
  booktitle = {{{ICASSP}} '77. {{IEEE International Conference}} on {{Acoustics}}, {{Speech}}, and {{Signal Processing}}},
  author = {DeMuth, G.},
  year = {1977},
  volume = {2},
  pages = {713--715},
  publisher = {{Institute of Electrical and Electronics Engineers}},
  address = {{Hartford, CT, USA}}
}

@article{deng_speckle_2011,
  title = {Speckle Reduction of Ultrasound Images Based on {{Rayleigh-trimmed}} Anisotropic Diffusion Filter},
  author = {Deng, Yinhui and Wang, Yuanyuan and Shen, Yuzhong},
  year = {2011},
  month = oct,
  journal = {Pattern Recognition Letters},
  volume = {32},
  number = {13},
  pages = {1516--1525},
  langid = {english}
}

@inproceedings{desa_understanding_2017,
  title = {Understanding and {{Optimizing Asynchronous Low-Precision Stochastic Gradient Descent}}},
  booktitle = {Proceedings of the 44th {{Annual International Symposium}} on {{Computer Architecture}}},
  author = {De Sa, Christopher and Feldman, Matthew and R{\'e}, Christopher and Olukotun, Kunle},
  year = {2017},
  series = {{{ISCA}} '17},
  pages = {561--574},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  keywords = {asynchrony,FPGA,low precision,multicore,Stochastic gradient descent}
}

@incollection{detorakis_inherent_2019,
  title = {Inherent {{Weight Normalization}} in {{Stochastic Neural Networks}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 32},
  author = {Detorakis, Georgios and Dutta, Sourav and Khanna, Abhishek and Jerry, Matthew and Datta, Suman and Neftci, Emre},
  year = {2019},
  pages = {3291--3302},
  publisher = {{Curran Associates, Inc.}}
}

@incollection{detorakis_inherent_2019a,
  title = {Inherent {{Weight Normalization}} in {{Stochastic Neural Networks}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 32},
  author = {Detorakis, Georgios and Dutta, Sourav and Khanna, Abhishek and Jerry, Matthew and Datta, Suman and Neftci, Emre},
  year = {2019},
  pages = {3291--3302},
  publisher = {{Curran Associates, Inc.}}
}

@article{detrano_international_1989,
  title = {International Application of a New Probability Algorithm for the Diagnosis of Coronary Artery Disease},
  author = {Detrano, Robert and Janosi, Andras and Steinbrunn, Walter and Pfisterer, Matthias and Schmid, Johann-Jakob and Sandhu, Sarbjit and Guppy, Kern H. and Lee, Stella and Froelicher, Victor},
  year = {1989},
  month = aug,
  journal = {The American Journal of Cardiology},
  volume = {64},
  number = {5},
  pages = {304--310},
  langid = {english}
}

@article{devlin_nouturn_2021,
  title = {The {{No-U-Turn Sampler}} as a {{Proposal Distribution}} in a {{Sequential Monte Carlo Sampler}} with a {{Near-Optimal L-Kernel}}},
  author = {Devlin, Lee and Horridge, Paul and Green, Peter L. and Maskell, Simon},
  year = {2021},
  month = aug,
  journal = {arXiv:2108.02498 [stat]},
  eprint = {2108.02498},
  eprinttype = {arxiv},
  primaryclass = {stat},
  abstract = {Markov Chain Monte Carlo (MCMC) is a powerful method for drawing samples from non-standard probability distributions and is utilized across many fields and disciplines. Methods such as Metropolis-Adjusted Langevin (MALA) and Hamiltonian Monte Carlo (HMC), which use gradient information to explore the target distribution, are popular variants of MCMC. The Sequential Monte Carlo (SMC) sampler is an alternative sampling method which, unlike MCMC, can readily utilise parallel computing architectures and also has tuning parameters not available to MCMC. One such parameter is the L-kernel which can be used to minimise the variance of the estimates from an SMC sampler. In this letter, we show how the proposal used in the No-U-Turn Sampler (NUTS), an advanced variant of HMC, can be incorporated into an SMC sampler to improve the efficiency of the exploration of the target space. We also show how the SMC sampler can be optimized using both a near-optimal L-kernel and a Hamiltonian proposal},
  archiveprefix = {arXiv},
  keywords = {Statistics - Computation},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\8D9GRHF3\\Devlin et al. - 2021 - The No-U-Turn Sampler as a Proposal Distribution i.pdf;C\:\\Users\\msca8h\\Zotero\\storage\\AMEJ9MMA\\2108.html}
}

@inproceedings{dhaka_challenges_2021,
  title = {Challenges and Opportunities in High Dimensional Variational Inference},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Dhaka, Akash Kumar and Catalina, Alejandro and Welandawe, Manushi and Andersen, Michael R. and Huggins, Jonathan and Vehtari, Aki},
  year = {2021},
  volume = {34},
  pages = {7787--7798},
  publisher = {{Curran Associates, Inc.}}
}

@inproceedings{dhaka_robust_2020,
  title = {Robust, Accurate Stochastic Optimization for Variational Inference},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Dhaka, Akash Kumar and Catalina, Alejandro and Andersen, Michael R and ns Magnusson, M{\aa} and Huggins, Jonathan and Vehtari, Aki},
  year = {2020},
  volume = {33},
  pages = {10961--10973},
  publisher = {{Curran Associates, Inc.}},
  abstract = {We examine the accuracy of black box variational posterior approximations for parametric models in a probabilistic programming context. The performance of these approximations depends on (1) how well the variational family approximates the true posterior distribution, (2) the choice of divergence, and (3) the optimization of the variational objective. We show that even when the true variational family is used, high-dimensional posteriors can be very poorly approximated using common stochastic gradient descent (SGD) optimizers. Motivated by recent theory, we propose a simple and parallel way to improve SGD estimates for variational inference. The approach is theoretically motivated and comes with a diagnostic for convergence and a novel stopping rule, which is robust to noisy objective  functions evaluations. We show empirically, the new workflow works well on a diverse set of models and datasets, or warns if the stochastic optimization fails or if the used variational distribution is not good.},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\D84XV28K\\Dhaka et al. - 2020 - Robust, Accurate Stochastic Optimization for Varia.pdf}
}

@article{dhamala_embedding_2020,
  title = {Embedding High-Dimensional {{Bayesian}} Optimization via Generative Modeling: {{Parameter}} Personalization of Cardiac Electrophysiological Models},
  shorttitle = {Embedding High-Dimensional {{Bayesian}} Optimization via Generative Modeling},
  author = {Dhamala, Jwala and Bajracharya, Pradeep and Arevalo, Hermenegild J. and Sapp, John L. and Hor{\'a}cek, B. Milan and Wu, Katherine C. and Trayanova, Natalia A. and Wang, Linwei},
  year = {2020},
  month = may,
  journal = {Medical Image Analysis},
  volume = {62},
  pages = {101670},
  langid = {english}
}

@incollection{dhamala_highdimensional_2018,
  title = {High-Dimensional {{Bayesian}} Optimization of Personalized Cardiac Model Parameters via an Embedded Generative Model},
  booktitle = {Medical {{Image Computing}} and {{Computer Assisted Intervention}} \textendash{} {{MICCAI}} 2018},
  author = {Dhamala, Jwala and Ghimire, Sandesh and Sapp, John L. and Hor{\'a}{\v c}ek, B. Milan and Wang, Linwei},
  year = {2018},
  volume = {11071},
  pages = {499--507},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\A4UNBB42\\Dhamala et al. - 2018 - High-Dimensional Bayesian Optimization of Personal.pdf}
}

@article{diaconis_analysis_2000,
  title = {Analysis of a Nonreversible {{Markov}} Chain Sampler},
  author = {Diaconis, Persi and Holmes, Susan and Neal, Radford M.},
  year = {2000},
  month = aug,
  journal = {The Annals of Applied Probability},
  volume = {10},
  number = {3},
  pages = {726--752},
  publisher = {{Institute of Mathematical Statistics}},
  abstract = {We analyze the convergence to stationarity of a simple nonreversible Markov chain that serves as a model for several nonreversible Markov chain sampling methods that are used in practice. Our theoretical and numerical results show that nonreversibility can indeed lead to improvements over the diffusive behavior of simple Markov chain sampling schemes. The analysis uses both probabilistic techniques and an explicit diagonalization.},
  keywords = {60J10,65U05,Markov chain Monte Carlo,Metropolis algorithm,Nonreversible Markov chain},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\4XLGTGAM\\Diaconis et al. - 2000 - Analysis of a nonreversible Markov chain sampler.pdf}
}

@article{diaconis_analysis_2000a,
  title = {Analysis of a {{Nonreversible Markov Chain Sampler}}},
  author = {Diaconis, Persi and Holmes, Susan and Neal, Radford M.},
  year = {2000},
  journal = {The Annals of Applied Probability},
  volume = {10},
  number = {3},
  pages = {726--752},
  abstract = {We analyze the convergence to stationarity of a simple nonreversible Markov chain that serves as a model for several nonreversible Markov chain sampling methods that are used in practice. Our theoretical and numerical results show that nonreversibility can indeed lead to improvements over the diffusive behavior of simple Markov chain sampling schemes. The analysis uses both probabilistic techniques and an explicit diagonalization.}
}

@article{diaz_derivation_2009,
  title = {Derivation of Self-Scheduling Algorithms for Heterogeneous Distributed Computer Systems: {{Application}} to Internet-Based Grids of Computers},
  shorttitle = {Derivation of Self-Scheduling Algorithms for Heterogeneous Distributed Computer Systems},
  author = {D{\'i}az, Javier and Reyes, Sebasti{\'a}n and Ni{\~n}o, Alfonso and {Mu{\~n}oz-Caro}, Camelia},
  year = {2009},
  month = jun,
  journal = {Future Generation Computer Systems},
  volume = {25},
  number = {6},
  pages = {617--626},
  langid = {english}
}

@inproceedings{diaz_quadratic_2006,
  title = {A {{Quadratic Self-Scheduling Algorithm}} for {{Heterogeneous Distributed Computing Systems}}},
  booktitle = {2006 {{IEEE International Conference}} on {{Cluster Computing}}},
  author = {Diaz, J. and Reyes, S. and Nino, A. and {Munoz-Caro}, C.},
  year = {2006},
  month = sep,
  pages = {1--8},
  keywords = {Algorithm design and analysis,Application software,Clustering algorithms,Distributed computing,grid computing,Grid computing,heterogeneous computing systems,heterogeneous distributed computing systems,Heuristic algorithms,Internet,Processor scheduling,quadratic self-scheduling algorithm,resource allocation,scheduling,Scheduling algorithm,scheduling strategy,Testing,transatlantic connection},
  file = {/home/msca8h/Documents/parallel_scheduling/Diaz et al. - 2006 - A Quadratic Self-Scheduling Algorithm for Heteroge.pdf}
}

@article{diclaudio_robust_2003,
  title = {Robust {{ML}} Wideband Beamforming in Reverberant Fields},
  author = {Di Claudio, E.D. and Parisi, R.},
  year = {2003},
  month = feb,
  journal = {IEEE Transactions on Signal Processing},
  volume = {51},
  number = {2},
  pages = {338--349},
  langid = {english}
}

@article{diclaudio_waves_2001,
  title = {{{WAVES}}: Weighted Average of Signal Subspaces for Robust Wideband Direction Finding},
  shorttitle = {{{WAVES}}},
  author = {{di Claudio}, E.D. and Parisi, R.},
  year = {Oct./2001},
  journal = {IEEE Transactions on Signal Processing},
  volume = {49},
  number = {10},
  pages = {2179--2191}
}

@misc{dillon_tensorflow_2017,
  title = {{{TensorFlow}} Distributions},
  author = {Dillon, Joshua V. and Langmore, Ian and Tran, Dustin and Brevdo, Eugene and Vasudevan, Srinivas and Moore, Dave and Patton, Brian and Alemi, Alex and Hoffman, Matt and Saurous, Rif A.},
  year = {2017},
  month = nov,
  number = {arXiv:1711.10604},
  eprint = {1711.10604},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  institution = {{arXiv}},
  abstract = {The TensorFlow Distributions library implements a vision of probability theory adapted to the modern deep-learning paradigm of end-to-end differentiable computation. Building on two basic abstractions, it offers flexible building blocks for probabilistic computation. Distributions provide fast, numerically stable methods for generating samples and computing statistics, e.g., log density. Bijectors provide composable volume-tracking transformations with automatic caching. Together these enable modular construction of high dimensional distributions and transformations not possible with previous libraries (e.g., pixelCNNs, autoregressive flows, and reversible residual networks). They are the workhorse behind deep probabilistic programming systems like Edward and empower fast black-box inference in probabilistic models built on deep-network components. TensorFlow Distributions has proven an important part of the TensorFlow toolkit within Google and in the broader deep learning community.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Programming Languages,Statistics - Machine Learning},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\Y7A5F3UD\\Dillon et al. - 2017 - TensorFlow Distributions.pdf;C\:\\Users\\msca8h\\Zotero\\storage\\RZ3GJ6UG\\1711.html}
}

@article{dimitrakakis_differential_2017,
  title = {Differential Privacy for {{Bayesian}} Inference through Posterior Sampling},
  author = {Dimitrakakis, Christos and Nelson, Blaine and Zhang, Zuhe and Mitrokotsa, Aikaterini and Rubinstein, Benjamin I. P.},
  year = {2017},
  journal = {Journal of Machine Learning Research},
  volume = {18},
  number = {11},
  pages = {1--39},
  abstract = {Differential privacy formalises privacy-preserving mechanisms that provide access to a database. Can Bayesian inference be used directly to provide private access to data? The answer is yes: under certain conditions on the prior, sampling from the posterior distribution can lead to a desired level of privacy and utility. For a uniform treatment, we define differential privacy over arbitrary data set metrics, outcome spaces and distribution families. This allows us to also deal with non-i.i.d or non-tabular data sets. We then prove bounds on the sensitivity of the posterior to the data, which delivers a measure of robustness. We also show how to use posterior sampling to provide differentially private responses to queries, within a decision-theoretic framework. Finally, we provide bounds on the utility of answers to queries and on the ability of an adversary to distinguish between data sets. The latter are complemented by a novel use of Le Cam's method to obtain lower bounds on distinguishability. Our results hold for arbitrary metrics, including those for the common definition of differential privacy. For specific choices of the metric, we give a number of examples satisfying our assumptions.},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\VBHWWMFU\\Dimitrakakis et al. - 2017 - Differential Privacy for Bayesian Inference throug.pdf}
}

@inproceedings{dimitrakakis_robust_2014,
  title = {Robust and Private {{Bayesian}} Inference},
  booktitle = {Algorithmic {{Learning Theory}}},
  author = {Dimitrakakis, Christos and Nelson, Blaine and Mitrokotsa, Aikaterini and Rubinstein, Benjamin I. P.},
  year = {2014},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {291--305},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  abstract = {We examine the robustness and privacy of Bayesian inference, under assumptions on the prior, and with no modifications to the Bayesian framework. First, we generalise the concept of differential privacy to arbitrary dataset distances, outcome spaces and distribution families. We then prove bounds on the robustness of the posterior, introduce a posterior sampling mechanism, show that it is differentially private and provide finite sample bounds for distinguishability-based privacy under a strong adversarial model. Finally, we give examples satisfying our assumptions.},
  langid = {english},
  keywords = {Bayesian Inference,Distribution Family,Posterior Distribution,Posterior Sampling,Prior Distribution},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\352AVGH6\\Dimitrakakis et al. - 2014 - Robust and Private Bayesian Inference.pdf}
}

@article{dinan_scalable_2009,
  title = {Scalable Work Stealing},
  author = {Dinan, James and Larkins, D. Brian and Sadayappan, P. and Krishnamoorthy, Sriram and Nieplocha, Jarek},
  year = {2009},
  journal = {Proceedings of the Conference on High Performance Computing Networking, Storage and Analysis},
  pages = {1--11},
  keywords = {dynamic load balancing,pgas,task pools,work stealing},
  file = {/home/msca8h/Documents/parallel_scheduling/Dinan et al. - 2009 - Scalable work stealing.pdf}
}

@inproceedings{dinari_distributed_2019,
  title = {Distributed {{MCMC Inference}} in {{Dirichlet Process Mixture Models Using Julia}}},
  booktitle = {2019 19th {{IEEE}}/{{ACM International Symposium}} on {{Cluster}}, {{Cloud}} and {{Grid Computing}} ({{CCGRID}})},
  author = {Dinari, Or and Yu, Angel and Freifeld, Oren and Fisher, John},
  year = {2019},
  month = may,
  pages = {518--525},
  publisher = {{IEEE}},
  address = {{Larnaca, Cyprus}}
}

@article{dippel_multiscale_2002,
  title = {Multiscale Contrast Enhancement for Radiographies: {{Laplacian}} Pyramid versus Fast Wavelet Transform},
  shorttitle = {Multiscale Contrast Enhancement for Radiographies},
  author = {Dippel, S. and Stahl, M. and Wiemker, R. and Blaffert, T.},
  year = {2002},
  month = apr,
  journal = {IEEE Transactions on Medical Imaging},
  volume = {21},
  number = {4},
  pages = {343--353},
  langid = {english}
}

@techreport{doan_convergence_2020,
  title = {Convergence Rates of Accelerated {{Markov}} Gradient Descent with Applications in Reinforcement Learning},
  author = {Doan, Thinh T. and Nguyen, Lam M. and Pham, Nhan H. and Romberg, Justin},
  year = {2020},
  month = oct,
  number = {arXiv:2002.02873 [math]},
  eprint = {2002.02873},
  eprinttype = {arxiv},
  institution = {{ArXiv}},
  abstract = {Motivated by broad applications in machine learning, we study the popular accelerated stochastic gradient descent (ASGD) algorithm for solving (possibly nonconvex) optimization problems. We characterize the finite-time performance of this method when the gradients are sampled from Markov processes, and hence biased and dependent from time step to time step; in contrast, the analysis in existing work relies heavily on the stochastic gradients being independent and sometimes unbiased. Our main contributions show that under certain (standard) assumptions on the underlying Markov chain generating the gradients, ASGD converges at the nearly the same rate with Markovian gradient samples as with independent gradient samples. The only difference is a logarithmic factor that accounts for the mixing time of the Markov chain. One of the key motivations for this study are complicated control problems that can be modeled by a Markov decision process and solved using reinforcement learning. We apply the accelerated method to several challenging problems in the OpenAI Gym and Mujoco, and show that acceleration can significantly improve the performance of the classic temporal difference learning and REINFORCE algorithms.},
  archiveprefix = {arXiv},
  keywords = {Mathematics - Optimization and Control},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\WVUPZUP9\\Doan et al. - 2020 - Convergence Rates of Accelerated Markov Gradient D.pdf;C\:\\Users\\msca8h\\Zotero\\storage\\PXMZT3YD\\2002.html}
}

@techreport{doan_finitetime_2020,
  title = {Finite-Time Analysis of Stochastic Gradient Descent under {{Markov}} Randomness},
  author = {Doan, Thinh T. and Nguyen, Lam M. and Pham, Nhan H. and Romberg, Justin},
  year = {2020},
  month = apr,
  number = {arXiv:2003.10973},
  eprint = {2003.10973},
  eprinttype = {arxiv},
  institution = {{ArXiv}},
  abstract = {Motivated by broad applications in reinforcement learning and machine learning, this paper considers the popular stochastic gradient descent (SGD) when the gradients of the underlying objective function are sampled from Markov processes. This Markov sampling leads to the gradient samples being biased and not independent. The existing results for the convergence of SGD under Markov randomness are often established under the assumptions on the boundedness of either the iterates or the gradient samples. Our main focus is to study the finite-time convergence of SGD for different types of objective functions, without requiring these assumptions. We show that SGD converges nearly at the same rate with Markovian gradient samples as with independent gradient samples. The only difference is a logarithmic factor that accounts for the mixing time of the Markov chain.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Mathematics - Optimization and Control},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\9ELL9LC6\\Doan et al. - 2020 - Finite-Time Analysis of Stochastic Gradient Descen.pdf;C\:\\Users\\msca8h\\Zotero\\storage\\E8LFYS36\\2003.html}
}

@article{doi:10.1080/01621459.1951.10500768,
  title = {The Theory of Statistical Decision},
  author = {{L. J. Savage}},
  year = {1951},
  journal = {Journal of the American Statistical Association},
  volume = {46},
  number = {253},
  eprint = {https://www.tandfonline.com/doi/pdf/10.1080/01621459.1951.10500768},
  pages = {55--67},
  publisher = {{Taylor \& Francis}}
}

@article{doi:10.1080/01621459.2000.10473908,
  title = {The Multiple-Try Method and Local Optimization in Metropolis Sampling},
  author = {Liu, Jun S. and Liang, Faming and {Wing Hung Wong}},
  year = {2000},
  journal = {Journal of the American Statistical Association},
  volume = {95},
  number = {449},
  eprint = {https://www.tandfonline.com/doi/pdf/10.1080/01621459.2000.10473908},
  pages = {121--134},
  publisher = {{Taylor \& Francis}}
}

@inproceedings{domke_importance_2018,
  title = {Importance Weighting and Variational Inference},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Domke, Justin and Sheldon, Daniel R},
  year = {2018},
  volume = {31},
  publisher = {{Curran Associates, Inc.}},
  abstract = {Recent work used importance sampling ideas for better variational bounds on likelihoods. We clarify the applicability of these ideas to pure probabilistic inference, by showing the resulting Importance Weighted Variational Inference (IWVI) technique is an instance of augmented variational inference, thus identifying the looseness in previous work. Experiments confirm IWVI's practicality for probabilistic inference. As a second contribution, we investigate inference with elliptical distributions, which improves accuracy in low dimensions, and convergence in high dimensions.},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\BXTDVMBR\\Domke and Sheldon - 2018 - Importance Weighting and Variational Inference.pdf}
}

@inproceedings{domke_provable_2019,
  title = {Provable Gradient Variance Guarantees for Black-Box Variational Inference},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Domke, Justin},
  year = {2019},
  volume = {32},
  publisher = {{Curran Associates, Inc.}},
  abstract = {Recent variational inference methods use stochastic gradient estimators whose variance is not well understood. Theoretical guarantees for these estimators are important to understand when these methods will or will not work. This paper gives bounds for the common ``reparameterization'' estimators when the target is smooth and the variational family is a location-scale distribution. These bounds are unimprovable and thus provide the best possible guarantees under the stated assumptions.},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\QPGCDUST\\Domke - 2019 - Provable Gradient Variance Guarantees for Black-Bo.pdf}
}

@inproceedings{domke_provable_2020,
  title = {Provable Smoothness Guarantees for Black-Box Variational Inference},
  booktitle = {Proceedings of the International Conference on Machine Learning},
  author = {Domke, Justin},
  year = {2020},
  month = jul,
  series = {{{PMLR}}},
  volume = {119},
  pages = {2587--2596},
  publisher = {{ML Research Press}},
  abstract = {Black-box variational inference tries to approximate a complex target distribution through a gradient-based optimization of the parameters of a simpler distribution. Provable convergence guarantees require structural properties of the objective. This paper shows that for location-scale family approximations, if the target is M-Lipschitz smooth, then so is the ``energy'' part of the variational objective. The key proof idea is to describe gradients in a certain inner-product space, thus permitting the use of Bessel's inequality. This result gives bounds on the location of the optimal parameters, and is a key ingredient for convergence guarantees.},
  pdf = {http://proceedings.mlr.press/v119/domke20a/domke20a.pdf}
}

@article{doron_focusing_1992,
  title = {On Focusing Matrices for Wide-Band Array Processing},
  author = {Doron, M.A. and Weiss, A.J.},
  year = {1992},
  month = jun,
  journal = {IEEE Transactions on Signal Processing},
  volume = {40},
  number = {6},
  pages = {1295--1302},
  abstract = {A general class of transformation matrices for coherent signal-subspace processing is presented. These signal-subspace transformation (SST) matrices are shown to generate a sufficient statistic for maximum-likelihood bearing estimation. Two general forms for calculating SST matrices are presented, and the rotational signal-subspace (RSS) focusing matrices proposed by H. Hung and M. Kaveh (1988) are shown to be a special case of the SST matrices. An efficient procedure for computing a subset of the SST matrices, utilizing Householder transformations, is presented. The procedure reduces the computational load by a factor of 10, compared with that for the RSS matrices. The application of MUSIC to the coherently combined covariance matrix is also discussed, and Monte Carlo simulations comparing the performance of Householder SST matrices and RSS matrices are performed.{$<>$}},
  keywords = {Array signal processing,Covariance matrix,Direction of arrival estimation,Maximum likelihood detection,Maximum likelihood estimation,Multiple signal classification,Narrowband,Signal processing,Statistics,Wideband},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\IVR4G3MG\\Doron and Weiss - 1992 - On focusing matrices for wide-band array processin.pdf}
}

@article{doron_maximumlikelihood_1993,
  title = {Maximum-Likelihood Direction Finding of Wide-Band Sources},
  author = {Doron, M.A. and Weiss, A.J. and Messer, H.},
  year = {1993},
  month = jan,
  journal = {IEEE Transactions on Signal Processing},
  volume = {41},
  number = {1},
  pages = {411-},
  keywords = {Frequency estimation,Gaussian noise,Least squares methods,Maximum likelihood estimation,Sensor arrays,Signal processing,Signal processing algorithms,Signal resolution,Speech processing,Wideband},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\IJ33G9FJ\\Doron et al. - 1993 - Maximum-Likelihood Direction Finding of Wide-Band .pdf;C\:\\Users\\msca8h\\Zotero\\storage\\6C5ZLJQ3\\193166.html}
}

@inproceedings{dorta_openmp_2005,
  title = {The {{OpenMP Source Code Repository}}},
  booktitle = {13th {{Euromicro Conference}} on {{Parallel}}, {{Distributed}} and {{Network-Based Processing}}},
  author = {Dorta, A.J. and Rodriguez, C. and {de Sande}, F. and {Gonzalez-Escribano}, A.},
  year = {2005},
  pages = {244--250},
  publisher = {{IEEE}},
  address = {{Lugano, Switzerland}}
}

@inproceedings{dotsenko_autotuning_2011,
  title = {Auto-Tuning of Fast Fourier Transform on Graphics Processors},
  booktitle = {Proceedings of the 16th {{ACM}} Symposium on {{Principles}} and Practice of Parallel Programming - {{PPoPP}} '11},
  author = {Dotsenko, Yuri and Baghsorkhi, Sara S. and Lloyd, Brandon and Govindaraju, Naga K.},
  year = {2011},
  pages = {257},
  publisher = {{ACM Press}},
  address = {{San Antonio, TX, USA}},
  langid = {english}
}

@inproceedings{douc_comparison_2005,
  title = {Comparison of Resampling Schemes for Particle Filtering},
  booktitle = {{{ISPA}} 2005. {{Proceedings}} of the 4th {{International Symposium}} on {{Image}} and {{Signal Processing}} and {{Analysis}}, 2005.},
  author = {Douc, R. and Cappe, O.},
  year = {2005},
  pages = {64--69},
  publisher = {{IEEE}},
  address = {{Zagreb, Croatia}},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\KEZ55L2N\\Douc and Cappe - 2005 - Comparison of resampling schemes for particle filt.pdf}
}

@incollection{doucet_introduction_2001,
  title = {An {{Introduction}} to {{Sequential Monte Carlo Methods}}},
  booktitle = {Sequential {{Monte Carlo Methods}} in {{Practice}}},
  author = {Doucet, Arnaud and Freitas, Nando and Gordon, Neil},
  year = {2001},
  pages = {3--14},
  publisher = {{Springer New York}},
  address = {{New York, NY}}
}

@article{doucet_monte_2005,
  title = {Monte {{Carlo}} Methods for Signal Processing: A Review in the Statistical Signal Processing Context},
  shorttitle = {Monte {{Carlo}} Methods for Signal Processing},
  author = {Doucet, A. and Wang, Xiaodong},
  year = {2005},
  month = nov,
  journal = {IEEE Signal Processing Magazine},
  volume = {22},
  number = {6},
  pages = {152--170}
}

@article{Dua:2019,
  title = {{{UCI}} Machine Learning Repository},
  author = {Dua, Dheeru and Graff, Casey},
  year = {2017},
  publisher = {{University of California, Irvine, School of Information and Computer Sciences}}
}

@article{duan_tuning_2009,
  title = {Tuning Database Configuration Parameters with {{iTuned}}},
  author = {Duan, Songyun and Thummala, Vamsidhar and Babu, Shivnath},
  year = {2009},
  month = aug,
  journal = {Proceedings of the VLDB Endowment},
  volume = {2},
  number = {1},
  pages = {1246--1257},
  langid = {english},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\KYLPG32R\\Duan et al. - 2009 - Tuning database configuration parameters with iTun.pdf}
}

@article{duane_hybrid_1987,
  title = {Hybrid {{Monte Carlo}}},
  author = {Duane, Simon and Kennedy, Anthony D. and Pendleton, Brian J. and Roweth, Duncan},
  year = {1987},
  journal = {Physics Letters B},
  volume = {195},
  number = {2},
  pages = {216--222},
  abstract = {We present a new method for the numerical simulation of lattice field theory. A hybrid (molecular dynamics/Langevin) algorithm is used to guide a Monte Carlo simulation. There are no discretization errors even for large step sizes. The method is especially efficient for systems such as quantum chromodynamics which contain fermionic degrees of freedom. Detailed results are presented for four-dimensional compact quantum electrodynamics including the dynamical effects of electrons.}
}

@article{duarte-salazar_speckle_2020,
  title = {Speckle Noise Reduction in Ultrasound Images for Improving the Metrological Evaluation of Biomedical Applications: {{An}} Overview},
  shorttitle = {Speckle {{Noise Reduction}} in {{Ultrasound Images}} for {{Improving}} the {{Metrological Evaluation}} of {{Biomedical Applications}}},
  author = {{Duarte-Salazar}, Carlos A. and {Castro-Ospina}, Andres Eduardo and Becerra, Miguel A. and {Delgado-Trejos}, Edilson},
  year = {2020},
  journal = {IEEE Access},
  volume = {8},
  pages = {15983--15999},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\ZQVA8DQI\\Duarte-Salazar et al. - 2020 - Speckle Noise Reduction in Ultrasound Images for I.pdf}
}

@article{duchi_adaptive_2011,
  title = {Adaptive Subgradient Methods for Online Learning and Stochastic Optimization},
  author = {Duchi, John and Hazan, Elad and Singer, Yoram},
  year = {2011},
  journal = {Journal of Machine Learning Research},
  volume = {12},
  number = {Jul},
  pages = {2121--2159}
}

@article{duchi_ergodic_2012,
  title = {Ergodic Mirror Descent},
  author = {Duchi, John C. and Agarwal, Alekh and Johansson, Mikael and Jordan, Michael I.},
  year = {2012},
  month = jan,
  journal = {SIAM Journal on Optimization},
  volume = {22},
  number = {4},
  pages = {1549--1578},
  langid = {english}
}

@article{dudgeon_fundamentals_1977,
  title = {Fundamentals of Digital Array Processing},
  author = {Dudgeon, D.E.},
  year = {1977},
  journal = {Proceedings of the IEEE},
  volume = {65},
  number = {6},
  pages = {898--904}
}

@phdthesis{duersch_backprojection_,
  type = {Doctoral {{Thesis}}},
  title = {Backprojection for {{Synthetic Aperture Radar}}},
  author = {Duersch, Michael Israel},
  school = {Brigham Young University}
}

@inproceedings{dugas_incorporating_2000,
  title = {Incorporating Second-Order Functional Knowledge for Better Option Pricing},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Dugas, Charles and Bengio, Yoshua and B{\'e}lisle, Fran{\c c}ois and Nadeau, Claude and Garcia, Ren{\'e}},
  year = {2000},
  volume = {13},
  publisher = {{MIT Press}},
  abstract = {Incorporating prior knowledge of a particular task into the  architecture  of a learning algorithm can greatly improve generalization performance.  We  study  here  a case where we  know  that the  function  to be learned is  non-decreasing in two of its  arguments and convex in  one of them.  For  this purpose we propose a class of functions similar to multi-layer neural  networks but (1) that has those properties, (2) is a universal approximator  of continuous functions  with  these  and  other properties.  We  apply  this  new class  of functions  to  the task of modeling the price of call  options.  Experiments show improvements on regressing the price of call options  using the new types of function classes that incorporate the a priori con(cid:173) straints.},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\TTSZN9WS\\Dugas et al. - 2000 - Incorporating Second-Order Functional Knowledge fo.pdf}
}

@inproceedings{duplyakin_active_2016,
  title = {Active {{Learning}} in {{Performance Analysis}}},
  booktitle = {2016 {{IEEE International Conference}} on {{Cluster Computing}} ({{CLUSTER}})},
  author = {Duplyakin, Dmitry and Brown, Jed and Ricci, Robert},
  year = {2016},
  month = sep,
  pages = {182--191},
  publisher = {{IEEE}},
  address = {{Taipei, Taiwan}}
}

@article{durand_impact_1996,
  title = {Impact of Memory Contention on Dynamic Scheduling on {{NUMA}} Multiprocessors},
  author = {Durand, D. and Montaut, T. and Kervella, L. and Jalby, W.},
  year = {1996},
  month = nov,
  journal = {IEEE Transactions on Parallel and Distributed Systems},
  volume = {7},
  number = {11},
  pages = {1201--1214},
  keywords = {Analytical models,analytical performance models,BBN TC2000,dynamic scheduling,Dynamic scheduling,load balancing,load imbalance,Load management,memory contention,multiprocessing systems,Multiprocessor interconnection networks,nonuniform memory access machine,NUMA multiprocessors,parallel programming,parallel programs,Performance analysis,performance evaluation,Predictive models,processor scheduling,Processor scheduling,Programming profession,resource allocation,Scalability,scheduling overhead,self-scheduling,Shape,task duration distributions,task scheduling},
  file = {/home/msca8h/Documents/parallel_scheduling/Durand et al. - 1996 - Impact of memory contention on dynamic scheduling .pdf}
}

@article{durillo_multiobjective_2019,
  title = {Multi-{{Objective}} Region-{{Aware}} Optimization of Parallel Programs},
  author = {Durillo, Juan J. and Gschwandtner, Philipp and Kofler, Klaus and Fahringer, Thomas},
  year = {2019},
  month = apr,
  journal = {Parallel Computing},
  volume = {83},
  pages = {3--21},
  langid = {english},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\YS2TNN8I\\Durillo et al. - 2019 - Multi-Objective region-Aware optimization of paral.pdf}
}

@inproceedings{duvenaud_structure_2013,
  title = {Structure Discovery in Nonparametric Regression through Compositional Kernel Search},
  booktitle = {Proc. 30th {{Int}}. {{Conf}}. {{Mach}}. {{Learn}}.},
  author = {Duvenaud, David and Lloyd, James and Grosse, Roger and Tenenbaum, Joshua and Zoubin, Ghahramani},
  year = {2013},
  month = jun,
  series = {{{ICML}}'13},
  volume = {28},
  pages = {1166--1174},
  publisher = {{PMLR}},
  address = {{Atlanta, Georgia, USA}},
  abstract = {Despite its importance, choosing the structural form of the kernel in nonparametric regression remains a black art. We define a space of kernel structures which are built compositionally by adding and multiplying a small number of base kernels. We present a method for searching over this space of structures which mirrors the scientific discovery process. The learned structures can often decompose functions into interpretable components and enable long-range extrapolation on time-series datasets. Our structure search method outperforms many widely used kernels and kernel combination methods on a variety of prediction tasks.}
}

@article{dwork_algorithmic_2014,
  title = {The Algorithmic Foundations of Differential Privacy},
  author = {Dwork, Cynthia and Roth, Aaron},
  year = {2014},
  month = aug,
  journal = {Foundations and Trends\textregistered{} in Theoretical Computer Science},
  volume = {9},
  number = {3\textendash 4},
  pages = {211--407},
  publisher = {{Now Publishers, Inc.}},
  abstract = {The Algorithmic Foundations of Differential Privacy},
  langid = {english},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\9IRKQDHU\\Dwork and Roth - 2014 - The Algorithmic Foundations of Differential Privac.pdf;C\:\\Users\\msca8h\\Zotero\\storage\\XP2AMKVV\\TCS-042.html}
}

@article{dwork_differential_2010,
  title = {Differential Privacy for Statistics: {{What}} We Know and What We Want to Learn},
  shorttitle = {Differential {{Privacy}} for {{Statistics}}},
  author = {Dwork, Cynthia and Smith, Adam},
  year = {2010},
  month = apr,
  journal = {Journal of Privacy and Confidentiality},
  volume = {1},
  number = {2},
  abstract = {We motivate and review the definition of differential privacy, survey some results on differentially private statistical estimators, and outline a research agenda.  This survey is based on two presentations given by the authors at an NCHS/CDC sponsored workshop on data privacy in May 2008.},
  copyright = {Copyright (c) 2010 Cynthia Dwork, Adam Smith},
  langid = {english},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\SXRJG54M\\Dwork and Smith - 2010 - Differential Privacy for Statistics What we Know .pdf}
}

@techreport{eagerd.l_adaptive_1992,
  title = {Adaptive {{Guided Self-Scheduling}}},
  author = {{Eager, D.L} and {Zahorjan, J.}},
  year = {1992},
  number = {TR92-01-01,},
  institution = {{Department of Computer Science and Engineering, University of Washington}}
}

@article{eches_estimating_2010,
  title = {Estimating the Number of Endmembers in Hyperspectral Images Using the Normal Compositional Model and a Hierarchical {{Bayesian}} Algorithm},
  author = {Eches, Olivier and Dobigeon, Nicolas and Tourneret, Jean-Yves},
  year = {2010},
  month = jun,
  journal = {IEEE Journal of Selected Topics in Signal Processing},
  volume = {4},
  number = {3},
  pages = {582--591},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\FMXN8K9K\\Eches et al. - 2010 - Estimating the Number of Endmembers in Hyperspectr.pdf}
}

@incollection{edgar_introduction_2011,
  title = {Introduction to {{Synthetic Aperture Sonar}}},
  booktitle = {Sonar {{Systems}}},
  author = {Edgar, Roy},
  year = {2011},
  month = sep,
  publisher = {{InTech}},
  langid = {english},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\BMQL4KR9\\Edgar - 2011 - Introduction to Synthetic Aperture Sonar.pdf}
}

@article{efron_stein_1973,
  title = {Stein's Estimation Rule and Its Competitors--an Empirical {{Bayes}} Approach},
  author = {Efron, Bradley and Morris, Carl},
  year = {1973},
  month = mar,
  journal = {Journal of the American Statistical Association},
  volume = {68},
  number = {341},
  pages = {117}
}

@inproceedings{eggensperger_efficient_2015,
  title = {Efficient {{Benchmarking}} of {{Hyperparameter Optimizers}} via {{Surrogates}}},
  booktitle = {Proceedings of the {{Twenty-Ninth AAAI Conference}} on {{Artificial Intelligence}}},
  author = {Eggensperger, Katharina and Hutter, Frank and Hoos, Holger H. and {Leyton-Brown}, Kevin},
  year = {2015},
  series = {{{AAAI}}'15},
  pages = {1114--1120},
  publisher = {{AAAI Press}}
}

@inproceedings{eggensperger_efficient_2015a,
  title = {Efficient {{Benchmarking}} of {{Hyperparameter Optimizers}} via {{Surrogates}}},
  booktitle = {Proceedings of the {{Twenty-Ninth AAAI Conference}} on {{Artificial Intelligence}}},
  author = {Eggensperger, Katharina and Hutter, Frank and Hoos, Holger H. and {Leyton-Brown}, Kevin},
  year = {2015},
  series = {{{AAAI}}'15},
  pages = {1114--1120},
  publisher = {{AAAI Press}}
}

@inproceedings{eggensperger_empirical_2013,
  title = {Towards an {{Empirical Foundation}} for {{Assessing Bayesian Optimization}} of {{Hyperparameters}}},
  booktitle = {{{NIPS}} Workshop on {{Bayesian Optimization}} in {{Theory}} and {{Practice}}},
  author = {Eggensperger, K. and Feurer, M. and Hutter, F. and Bergstra, J. and Snoek, J. and Hoos, H. and {Leyton-Brown}, K.},
  year = {2013}
}

@inproceedings{egidi_are_2018,
  title = {Are {{Shots Predictive Of Soccer Results}}?},
  booktitle = {{{StanCon}} 2018},
  author = {Egidi, Leonardo and Pauli, Francesco and Torelli, Nicola},
  year = {2018},
  month = aug,
  publisher = {{Zenodo}},
  abstract = {A Stan model for soccer shooting ability.},
  copyright = {Creative Commons Attribution 4.0, Open Access},
  keywords = {Bayesian Data Analysis,Stan,StanCon}
}

@article{ehlers_adaptive_2008,
  title = {Adaptive Proposal Construction for Reversible Jump {{MCMC}}: {{Adaptive}} Proposal Construction for Reversible Jump {{MCMC}}},
  shorttitle = {Adaptive Proposal Construction for Reversible Jump {{MCMC}}},
  author = {Ehlers, Ricardo S. and Brooks, Stephen P.},
  year = {2008},
  month = jul,
  journal = {Scandinavian Journal of Statistics},
  volume = {35},
  number = {4},
  pages = {677--690},
  langid = {english}
}

@incollection{el-zehiry_learning_2013,
  title = {Learning the Manifold of Quality Ultrasound Acquisition},
  booktitle = {Med. {{Image Comput}}. {{Comput}}.-{{Assisted Intervention}}},
  author = {{El-Zehiry}, Noha and Yan, Michelle and Good, Sara and Fang, Tong and Zhou, S. Kevin and Grady, Leo},
  year = {2013},
  series = {{{MICCAI}}'13},
  volume = {7908},
  pages = {122--130},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}}
}

@article{ellis_bistatic_1991,
  title = {Bistatic Reverberation Calculations Using a Three-Dimensional Scattering Function},
  author = {Ellis, Dale D. and Crowe, D. Vance},
  year = {1991},
  month = may,
  journal = {The Journal of the Acoustical Society of America},
  volume = {89},
  number = {5},
  pages = {2207--2214},
  langid = {english},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\PTH4W6G5\\Ellis and Crowe - 1991 - Bistatic reverberation calculations using a three-.pdf}
}

@article{elvira_rethinking_2018,
  title = {Rethinking the {{Effective Sample Size}}},
  author = {Elvira, V{\'i}ctor and Martino, Luca and Robert, Christian P.},
  year = {2018},
  month = sep,
  journal = {arXiv:1809.04129 [stat]},
  eprint = {1809.04129},
  eprinttype = {arxiv},
  primaryclass = {stat},
  abstract = {The effective sample size (ESS) is widely used in sample-based simulation methods for assessing the quality of a Monte Carlo approximation of a given distribution and of related integrals. In this paper, we revisit and complete the approximation of the ESS in the specific context of importance sampling (IS). The derivation of this approximation, that we will denote as \$\textbackslash widehat\{\textbackslash text\{ESS\}\}\$, is only partially available in Kong [1992]. This approximation has been widely used in the last 25 years due to its simplicity as a practical rule of thumb in a wide variety of importance sampling methods. However, we show that the multiple assumptions and approximations in the derivation of \$\textbackslash widehat\{\textbackslash text\{ESS\}\}\$, makes it difficult to be considered even as a reasonable approximation of the ESS. We extend the discussion of the ESS in the multiple importance sampling (MIS) setting, and we display numerical examples. This paper does not cover the use of ESS for MCMC algorithms.},
  archiveprefix = {arXiv},
  keywords = {Statistics - Computation}
}

@article{elvira_rethinking_2022,
  title = {Rethinking the Effective Sample Size},
  author = {Elvira, V{\'i}ctor and Martino, Luca and Robert, Christian P.},
  year = {2022},
  month = apr,
  journal = {International Statistical Review},
  pages = {insr.12500},
  langid = {english},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\T9EGNTZY\\Elvira et al. - 2022 - Rethinking the Effective Sample Size.pdf}
}

@inproceedings{emmawang_demystifying_2019,
  title = {Demystifying {{Bayesian Inference Workloads}}},
  booktitle = {{{IEEE Int}}. {{Symp}}. {{Perform}}. {{Anal}}. {{Syst}}. {{Softw}}.},
  author = {Emma Wang, Yu and Zhu, Yuhao and Ko, Glenn G. and Reagen, Brandon and Wei, Gu-Yeon and Brooks, David},
  year = {2019},
  month = mar,
  series = {{{ISPASS}}'19},
  pages = {177--189},
  publisher = {{IEEE}},
  address = {{Madison, WI, USA}}
}

@techreport{evans_application_1982,
  type = {Technical {{Report}}},
  title = {Application of {{Advanced Signal Processing Techniques}} to {{Angle}} of {{Arrival Estimation}} in {{ATC Navigation}} and {{Surveillance Systems}}},
  author = {Evans, J. E. and Sun, D. F. and Johnson, J. R.},
  year = {1982},
  month = jun,
  number = {ADA118306},
  institution = {{MIT Lexington Lincoln Lab.}}
}

@article{fairbrother_gaussianprocesses_2018,
  title = {{{GaussianProcesses}}. Jl: {{A Nonparametric Bayes}} Package for the {{Julia Language}}},
  author = {Fairbrother, Jamie and Nemeth, Christopher and Rischard, Maxime and Brea, Johanni and Pinder, Thomas},
  year = {2018},
  journal = {arXiv preprint arXiv:1812.09064},
  eprint = {1812.09064},
  eprinttype = {arxiv},
  archiveprefix = {arXiv}
}

@inproceedings{fan_fast_2015,
  title = {Fast Second Order Stochastic Backpropagation for Variational Inference},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Fan, Kai and Wang, Ziteng and Beck, Jeff and Kwok, James and Heller, Katherine A},
  year = {2015},
  volume = {28},
  publisher = {{Curran Associates, Inc.}},
  abstract = {We propose a second-order (Hessian or Hessian-free) based optimization method for variational inference inspired by Gaussian backpropagation, and argue that quasi-Newton optimization can be developed as well.  This is accomplished by generalizing the gradient computation in stochastic backpropagation via a reparametrization trick with lower complexity. As an illustrative example, we apply this approach to the problems of Bayesian logistic regression and variational auto-encoder (VAE). Additionally, we compute bounds on the estimator variance of intractable expectations for the family  of Lipschitz continuous function. Our method is practical, scalable and model free. We demonstrate our method on several real-world datasets and provide comparisons with other stochastic gradient methods to show substantial enhancement in convergence rates.},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\KNDC6JA6\\Fan et al. - 2015 - Fast Second Order Stochastic Backpropagation for V.pdf}
}

@article{fan_tracking_1992,
  title = {Tracking of Conventional Beamforming with Hydrophone Array of Varying Geometry},
  author = {Fan, H. and Hu, X.},
  year = {1992},
  month = apr,
  journal = {IEEE Transactions on Aerospace and Electronic Systems},
  volume = {28},
  number = {2},
  pages = {335--354}
}

@article{fang_patterncoupled_2015,
  title = {Pattern-Coupled Sparse {{Bayesian}} Learning for Recovery of Block-Sparse Signals},
  author = {Fang, Jun and Shen, Yanning and Li, Hongbin and Wang, Pu},
  year = {2015},
  month = jan,
  journal = {IEEE Transactions on Signal Processing},
  volume = {63},
  number = {2},
  pages = {360--372},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\YLI8XZFS\\Fang et al. - 2015 - Pattern-Coupled Sparse Bayesian Learning for Recov.pdf}
}

@book{fang_symmetric_2018,
  title = {Symmetric {{Multivariate}} and {{Related Distributions}}},
  author = {Fang, Kaitai and Kotz, Samuel and Ng, Kai Wang},
  year = {2018},
  abstract = {"Since the publication of the by now classical Johnson and Kotz Continuous Multivariate Distributions (Wiley, 1972) there have been substantial developments in multivariate distribution theory especially in the area of non-normal symmetric multivariate distributions. The book by Fang, Kotz and Ng summarizes these developments in a manner which is accessible to a reader with only limited background (advanced real-analysis calculus, linear algebra and elementary matrix calculus). Many of the results in this field are due to Kai-Tai Fang and his associates and appeared in Chinese publications only.A thorough literature search was conducted and the book represents the latest work - as of 1988 - in this rapidly developing field of multivariate distributions. The authors are experts in statistical distribution theory."--Provided by publisher.},
  langid = {english},
  annotation = {OCLC: 1020790331}
}

@article{fann_intelligent_2000,
  title = {An {{Intelligent Parallel Loop Scheduling}} for {{Parallelizing Compilers}}},
  author = {Fann, Yun-Woei and Yang, Chao-Tung and Tseng, Shian-Shyong and Tsai, Chang-Jiun},
  year = {2000},
  journal = {J. Inf. Sci. Eng.},
  volume = {16},
  pages = {169--200},
  file = {/home/msca8h/Documents/parallel_scheduling/Fann et al. - 2000 - An Intelligent Parallel Loop Scheduling for Parall.pdf}
}

@inproceedings{fann_ipls_1998,
  title = {{{IPLS}}: {{An}} Intelligent Parallel Loop Scheduling for Multiprocessor Systems},
  booktitle = {Proceedings 1998 {{International Conference}} on {{Parallel}} and {{Distributed Systems}} ({{Cat}}. {{No}}. {{98TB100250}})},
  author = {Fann, Yun-Woei and Yang, Chao-Tung and Tsai, Chang-Jiun and Tseng, Shian-Shyong},
  year = {1998},
  pages = {775--782},
  publisher = {{IEEE}}
}

@article{farbman_edgepreserving_2008,
  title = {Edge-Preserving Decompositions for Multi-Scale Tone and Detail Manipulation},
  author = {Farbman, Zeev and Fattal, Raanan and Lischinski, Dani and Szeliski, Richard},
  year = {2008},
  month = aug,
  journal = {ACM Transactions on Graphics},
  volume = {27},
  number = {3},
  pages = {1--10},
  langid = {english}
}

@article{fattal_edgeavoiding_2009,
  title = {Edge-Avoiding Wavelets and Their Applications},
  author = {Fattal, Raanan},
  year = {2009},
  month = jul,
  journal = {ACM Transactions on Graphics},
  volume = {28},
  number = {3},
  pages = {1--10},
  abstract = {We propose a new family of second-generation wavelets constructed using a robust data-prediction lifting scheme. The support of these new wavelets is constructed based on the edge content of the image and avoids having pixels from both sides of an edge. Multi-resolution analysis, based on these new               edge-avoiding wavelets               , shows a better decorrelation of the data compared to common linear translation-invariant multi-resolution analyses. The reduced inter-scale correlation allows us to avoid halo artifacts in band-independent multi-scale processing without taking any special precautions. We thus achieve nonlinear data-dependent multi-scale edge-preserving image filtering and processing at computation times which are               linear               in the number of image pixels. The new wavelets encode, in their shape, the smoothness information of the image at every scale. We use this to derive a new edge-aware interpolation scheme that achieves results, previously computed by solving an inhomogeneous Laplace equation, through an               explicit               computation. We thus avoid the difficulties in solving large and poorly-conditioned systems of equations.                          We demonstrate the effectiveness of the new wavelet basis for various computational photography applications such as multi-scale dynamic-range compression, edge-preserving smoothing and detail enhancement, and image colorization.},
  langid = {english},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\94FW7E5D\\Fattal - 2009 - Edge-avoiding wavelets and their applications.pdf}
}

@article{fattal_multiscale_2007,
  title = {Multiscale Shape and Detail Enhancement from Multi-Light Image Collections},
  author = {Fattal, Raanan and Agrawala, Maneesh and Rusinkiewicz, Szymon},
  year = {2007},
  month = jul,
  journal = {ACM Transactions on Graphics},
  volume = {26},
  number = {3},
  pages = {51},
  langid = {english}
}

@article{fearnhead_adaptive_2013,
  title = {An Adaptive Sequential {{Monte Carlo}} Sampler},
  author = {Fearnhead, Paul and Taylor, Benjamin M.},
  year = {2013},
  month = jun,
  journal = {Bayesian Analysis},
  volume = {8},
  number = {2},
  pages = {411--438},
  langid = {english},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\QTCLZVNB\\Fearnhead and Taylor - 2013 - An Adaptive Sequential Monte Carlo Sampler.pdf}
}

@inproceedings{featherstone_novel_1997,
  title = {A Novel Method to Improve the Performance of {{Capon}}'s Minimum Variance Estimator},
  booktitle = {Proceedings of the {{International Conference}} on {{Antennas}} and {{Propagation}}},
  author = {Featherstone, W.},
  year = {1997},
  volume = {1997},
  pages = {v1-322-v1-322},
  publisher = {{IEE}},
  address = {{Edinburgh, UK}},
  langid = {english}
}

@inproceedings{feng_building_2007,
  title = {Building the Tree of Life on Terascale Systems},
  booktitle = {Proceedings of the 2007 {{IEEE International Parallel}} and {{Distributed Processing Symposium}}},
  author = {Feng, Xizhou and Cameron, Kirk W. and Sosa, Carlos P. and Smith, Brian},
  year = {2007},
  series = {{{IPDPS}}'07},
  pages = {1--10},
  publisher = {{IEEE}},
  address = {{Long Beach, CA, USA}},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\N44I829Q\\Feng et al. - 2007 - Building the Tree of Life on Terascale Systems.pdf}
}

@article{feng_parallel_2003,
  title = {Parallel Algorithms for {{Bayesian}} Phylogenetic Inference},
  author = {Feng, Xizhou and Buell, Duncan A. and Rose, John R. and Waddell, Peter J.},
  year = {2003},
  month = jul,
  journal = {Journal of Parallel and Distributed Computing},
  volume = {63},
  number = {7-8},
  pages = {707--718},
  langid = {english}
}

@inproceedings{feng_uniform_2018,
  title = {Uniform Convergence of Sample Average Approximation with Adaptive Multiple Importance Sampling},
  booktitle = {2018 {{Winter Simulation Conference}} ({{WSC}})},
  author = {Feng, M. Ben and Maggiar, Alvaro and Staum, Jeremy and Wachter, Andreas},
  year = {2018},
  month = dec,
  pages = {1646--1657},
  publisher = {{IEEE}},
  address = {{Gothenburg, Sweden}}
}

@article{ferrari_beta_2004,
  title = {Beta {{Regression}} for {{Modelling Rates}} and {{Proportions}}},
  author = {Ferrari, Silvia and {Cribari-Neto}, Francisco},
  year = {2004},
  month = aug,
  journal = {Journal of Applied Statistics},
  volume = {31},
  number = {7},
  pages = {799--815},
  langid = {english}
}

@article{filippone_pseudomarginal_2014,
  title = {Pseudo-Marginal {{Bayesian}} Inference for {{Gaussian}} Processes},
  author = {Filippone, Maurizio and Girolami, Mark},
  year = {2014},
  month = nov,
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {36},
  number = {11},
  pages = {2214--2226},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\QQ967QMI\\Filippone and Girolami - 2014 - Pseudo-Marginal Bayesian Inference for Gaussian Pr.pdf}
}

@article{filos_systematic_2019,
  title = {A Systematic Comparison of {{Bayesian}} Deep Learning Robustness in Diabetic Retinopathy Tasks},
  author = {Filos, Angelos and Farquhar, Sebastian and Gomez, Aidan N. and Rudner, Tim G. J. and Kenton, Zachary and Smith, Lewis and Alizadeh, Milad and {de Kroon}, Arnoud and Gal, Yarin},
  year = {2019},
  month = dec,
  journal = {arXiv:1912.10481 [cs, eess, stat]},
  eprint = {1912.10481},
  eprinttype = {arxiv},
  primaryclass = {cs, eess, stat},
  abstract = {Evaluation of Bayesian deep learning (BDL) methods is challenging. We often seek to evaluate the methods' robustness and scalability, assessing whether new tools give `better' uncertainty estimates than old ones. These evaluations are paramount for practitioners when choosing BDL tools on-top of which they build their applications. Current popular evaluations of BDL methods, such as the UCI experiments, are lacking: Methods that excel with these experiments often fail when used in application such as medical or automotive, suggesting a pertinent need for new benchmarks in the field. We propose a new BDL benchmark with a diverse set of tasks, inspired by a real-world medical imaging application on \textbackslash emph\{diabetic retinopathy diagnosis\}. Visual inputs (512x512 RGB images of retinas) are considered, where model uncertainty is used for medical pre-screening---i.e. to refer patients to an expert when model diagnosis is uncertain. Methods are then ranked according to metrics derived from expert-domain to reflect real-world use of model uncertainty in automated diagnosis. We develop multiple tasks that fall under this application, including out-of-distribution detection and robustness to distribution shift. We then perform a systematic comparison of well-tuned BDL techniques on the various tasks. From our comparison we conclude that some current techniques which solve benchmarks such as UCI `overfit' their uncertainty to the dataset---when evaluated on our benchmark these underperform in comparison to simpler baselines. The code for the benchmark, its baselines, and a simple API for evaluating new BDL tools are made available at https://github.com/oatml/bdl-benchmarks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Electrical Engineering and Systems Science - Image and Video Processing,Statistics - Machine Learning},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\JATV6WDU\\Filos et al. - 2019 - A Systematic Comparison of Bayesian Deep Learning .pdf;C\:\\Users\\msca8h\\Zotero\\storage\\PX9STZNM\\1912.html}
}

@phdthesis{finke_extended_2015,
  title = {On Extended State-Space Constructions for Monte Carlo Methods},
  author = {Finke, Axel},
  year = {2015},
  month = jul,
  abstract = {This thesis develops computationally efficient methodology in two areas. Firstly, we consider a particularly challenging class of discretely observed continuous-time point-process models. For these, we analyse and improve an existing filtering algorithm based on sequential Monte Carlo (smc) methods. To estimate the static parameters in such models, we devise novel particle Gibbs samplers. One of these exploits a sophisticated non-entred parametrisation whose benefits in a Markov chain Monte Carlo (mcmc) context have previously been limited by the lack of blockwise updates for the latent point process. We apply this algorithm to a L\'evy-driven stochastic volatility model. Secondly, we devise novel Monte Carlo methods \textendash{} based around pseudo-marginal and conditional smc approaches \textendash{} for performing optimisation in latent-variable models and more generally. To ease the explanation of the wide range of techniques employed in this work, we describe a generic importance-sampling framework which admits virtually all Monte Carlo methods, including smc and mcmc methods, as special cases. Indeed, hierarchical combinations of different Monte Carlo schemes such as smc within mcmc or smc within smc can be justified as repeated applications of this framework.},
  school = {University of Warwick}
}

@article{finn_echocardiographic_2011,
  title = {Echocardiographic Speckle Reduction Comparison},
  author = {Finn, S and Glavin, M and Jones, E},
  year = {2011},
  month = jan,
  journal = {IEEE Transactions on Ultrasonics, Ferroelectrics and Frequency Control},
  volume = {58},
  number = {1},
  pages = {82--101}
}

@article{finzi_generalizing_2020,
  title = {Generalizing Convolutional Neural Networks for Equivariance to Lie Groups on Arbitrary Continuous Data},
  author = {Finzi, Marc and Stanton, Samuel and Izmailov, Pavel and Wilson, Andrew Gordon},
  year = {2020},
  month = feb,
  journal = {arXiv:2002.12880 [cs, stat]},
  eprint = {2002.12880},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {The translation equivariance of convolutional layers enables convolutional neural networks to generalize well on image problems. While translation equivariance provides a powerful inductive bias for images, we often additionally desire equivariance to other transformations, such as rotations, especially for non-image data. We propose a general method to construct a convolutional layer that is equivariant to transformations from any specified Lie group with a surjective exponential map. Incorporating equivariance to a new group requires implementing only the group exponential and logarithm maps, enabling rapid prototyping. Showcasing the simplicity and generality of our method, we apply the same model architecture to images, ball-and-stick molecular data, and Hamiltonian dynamical systems. For Hamiltonian systems, the equivariance of our models is especially impactful, leading to exact conservation of linear and angular momentum.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\5WPY7SJX\\Finzi et al. - 2020 - Generalizing Convolutional Neural Networks for Equ.pdf;C\:\\Users\\msca8h\\Zotero\\storage\\5PDTUENJ\\2002.html}
}

@article{fishler_detection_2002,
  title = {Detection of Signals by Information Theoretic Criteria: General Asymptotic Performance Analysis},
  shorttitle = {Detection of Signals by Information Theoretic Criteria},
  author = {Fishler, E. and Grosmann, M. and Messer, H.},
  year = {2002},
  month = may,
  journal = {IEEE Transactions on Signal Processing},
  volume = {50},
  number = {5},
  pages = {1027--1036}
}

@book{fishman_discreteevent_2001,
  title = {Discrete-{{Event Simulation}}},
  author = {Fishman, George S.},
  year = {2001},
  publisher = {{Springer New York}},
  address = {{New York, NY}},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\W8M8QNTR\\Fishman - 2001 - Discrete-Event Simulation.pdf}
}

@inproceedings{fjelde_bijectors_2020,
  title = {Bijectors.Jl: {{Flexible}} Transformations for Probability Distributions},
  shorttitle = {Bijectors.Jl},
  booktitle = {Proceedings of {{The Symposium}} on  {{Advances}} in {{Approximate Bayesian Inference}}},
  author = {Fjelde, Tor Erlend and Xu, Kai and Tarek, Mohamed and Yalburgi, Sharan and Ge, Hong},
  year = {2020},
  month = feb,
  series = {{{PMLR}}},
  volume = {118},
  pages = {1--17},
  publisher = {{ML Research Press}},
  abstract = {Transforming one probability distribution to another is a powerful tool in Bayesian inference and machine learning. Some prominent examples are constrained-to-unconstrained transformations of distributions for use in Hamiltonian Monte Carlo and constructing exible and learnable densities such as normalizing ows. We present Bijectors.jl, a software package in Julia for transforming distributions, available at github.com/TuringLang/Bijectors.jl. The package provides a exible and composable way of implementing transformations of distributions without being tied to a computational framework. We demonstrate the use of Bijectors.jl on improving variational inference by encoding known statistical dependencies into the variational posterior using normalizing ows, providing a general approach to relaxing the mean-field assumption usually made in variational inference.},
  langid = {english},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\BPYAGMPA\\Fjelde et al. - 2020 - Bijectors.jl Flexible transformations for probabi.pdf}
}

@article{flaxman_estimating_2020,
  title = {Estimating the Effects of Non-Pharmaceutical Interventions on {{COVID-19}} in {{Europe}}},
  author = {Flaxman, Seth and Mishra, Swapnil and Gandy, Axel and Unwin, H. Juliette T. and Mellan, Thomas A. and Coupland, Helen and Whittaker, Charles and Zhu, Harrison and Berah, Tresnia and Eaton, Jeffrey W. and Monod, M{\'e}lodie and {Imperial College COVID-19 Response Team} and Ghani, Azra C. and Donnelly, Christl A. and Riley, Steven and Vollmer, Michaela A. C. and Ferguson, Neil M. and Okell, Lucy C. and {Samir Bhatt}},
  year = {2020},
  month = jun,
  journal = {Nature},
  langid = {english},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\CFWA2QPN\\Imperial College COVID-19 Response Team et al. - 2020 - Estimating the effects of non-pharmaceutical inter.pdf}
}

@techreport{flynn_scheduling_1990,
  title = {Scheduling Variable-Length Parallel Subtasks},
  author = {Flynn, L. E. and Hummel, S. F.},
  year = {1990},
  month = feb,
  institution = {{IBM Research T. J. Watson Research Center}},
  file = {/home/msca8h/Documents/parallel_scheduling/Flynn and Hummel - 1990 - Scheduling variable-length parallel subtasks.pdf}
}

@book{fort_deep_2020,
  title = {Deep Ensembles: {{A}} Loss Landscape Perspective},
  author = {Fort, Stanislav and Hu, Clara Huiyi and Lakshminarayanan, Balaji},
  year = {2020}
}

@phdthesis{fortune_phase_2005,
  type = {Doctoral {{Thesis}}},
  title = {Phase Error Estimation for Synthetic Aperture Imagery},
  author = {Fortune, Steven A.},
  year = {2005},
  school = {University of Canterbury},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\EI5HQ35R\\Fortune - 2005 - Phase error estimation for synthetic aperture imag.pdf}
}

@inproceedings{foulds_theory_2016,
  title = {On the Theory and Practice of Privacy-Preserving {{Bayesian}} Data Analysis},
  booktitle = {Proceedings of the {{Conference}} on {{Uncertainty}} in {{Artificial Intelligence}}},
  author = {Foulds, James and Geumlek, Joseph and Welling, Max and Chaudhuri, Kamalika},
  year = {2016},
  month = jun,
  series = {{{UAI}}'16},
  pages = {192--201},
  publisher = {{AUAI Press}},
  address = {{Arlington, Virginia, USA}},
  abstract = {Bayesian inference has great promise for the privacy-preserving analysis of sensitive data, as posterior sampling automatically preserves differential privacy, an algorithmic notion of data privacy, under certain conditions (Dimitrakakis et al., 2014; Wang et al., 2015b). While this one posterior sample (OPS) approach elegantly provides privacy "for free," it is data inefficient in the sense of asymptotic relative efficiency (ARE). We show that a simple alternative based on the Laplace mechanism, the workhorse of differential privacy, is as asymptotically efficient as non-private posterior inference, under general assumptions. This technique also has practical advantages including efficient use of the privacy budget for MCMC. We demonstrate the practicality of our approach on a time-series analysis of sensitive military records from the Afghanistan and Iraq wars disclosed by the Wikileaks organization.}
}

@article{fox_adapting_2003,
  title = {Adapting the {{Sample Size}} in {{Particle Filters Through KLD-Sampling}}},
  author = {Fox, Dieter},
  year = {2003},
  month = dec,
  journal = {The International Journal of Robotics Research},
  volume = {22},
  number = {12},
  pages = {985--1003},
  langid = {english}
}

@article{frankle_training_2020,
  title = {Training {{BatchNorm}} and {{Only BatchNorm}}: {{On}} the {{Expressive Power}} of {{Random Features}} in {{CNNs}}},
  shorttitle = {Training {{BatchNorm}} and {{Only BatchNorm}}},
  author = {Frankle, Jonathan and Schwab, David J. and Morcos, Ari S.},
  year = {2020},
  month = feb,
  journal = {arXiv:2003.00152 [cs, stat]},
  eprint = {2003.00152},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Batch normalization (BatchNorm) has become an indispensable tool for training deep neural networks, yet it is still poorly understood. Although previous work has typically focused on its normalization component, BatchNorm also adds two per-feature trainable parameters: a coefficient and a bias. However, the role and expressive power of these parameters remains unclear. To study this question, we investigate the performance achieved when training only these parameters and freezing all others at their random initializations. We find that doing so leads to surprisingly high performance. For example, a sufficiently deep ResNet reaches 83\% accuracy on CIFAR-10 in this configuration. Interestingly, BatchNorm achieves this performance in part by naturally learning to disable around a third of the random features without any changes to the training objective. Not only do these results highlight the under-appreciated role of the affine parameters in BatchNorm, but - in a broader sense - they characterize the expressive power of neural networks constructed simply by shifting and rescaling random features.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\IX8W55US\\Frankle et al. - 2020 - Training BatchNorm and Only BatchNorm On the Expr.pdf;C\:\\Users\\msca8h\\Zotero\\storage\\LIYI3EGL\\2003.html}
}

@article{frey_bayesian_2016,
  title = {Bayesian Shrinkage of Portfolio Weights},
  author = {Frey, Christoph},
  year = {2016},
  journal = {SSRN Electronic Journal},
  langid = {english}
}

@article{frigo_design_2005,
  title = {The {{Design}} and {{Implementation}} of {{FFTW3}}},
  author = {Frigo, M. and Johnson, S.G.},
  year = {2005},
  month = feb,
  journal = {Proceedings of the IEEE},
  volume = {93},
  number = {2},
  pages = {216--231},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\8Q8SPIXB\\Frigo and Johnson - 2005 - The Design and Implementation of FFTW3.pdf}
}

@incollection{frigola_bayesian_2013,
  title = {Bayesian {{Inference}} and {{Learning}} in {{Gaussian Process State-Space Models}} with {{Particle MCMC}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 26},
  author = {Frigola, Roger and Lindsten, Fredrik and Sch{\"o}n, Thomas B and Rasmussen, Carl Edward},
  year = {2013},
  pages = {3156--3164},
  publisher = {{Curran Associates, Inc.}}
}

@incollection{frigola_variational_2014,
  title = {Variational {{Gaussian Process State-Space Models}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 27},
  author = {Frigola, Roger and Chen, Yutian and Rasmussen, Carl Edward},
  year = {2014},
  pages = {3680--3688},
  publisher = {{Curran Associates, Inc.}}
}

@article{frost_algorithm_1972,
  title = {An Algorithm for Linearly Constrained Adaptive Array Processing},
  author = {Frost, O.L.},
  year = {1972},
  journal = {Proceedings of the IEEE},
  volume = {60},
  number = {8},
  pages = {926--935}
}

@article{fuentes_autonomous_2020,
  title = {Autonomous Ultrasonic Inspection Using {{Bayesian}} Optimisation and Robust Outlier Analysis},
  author = {Fuentes, R. and Gardner, P. and Mineo, C. and Rogers, T.J. and Pierce, S.G. and Worden, K. and Dervilis, N. and Cross, E.J.},
  year = {2020},
  month = nov,
  journal = {Mechanical Systems and Signal Processing},
  volume = {145},
  pages = {106897},
  langid = {english},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\WQB92MII\\Fuentes et al. - 2020 - Autonomous ultrasonic inspection using Bayesian op.pdf}
}

@article{fujisawa_multilevel_2021,
  title = {Multilevel {{Monte Carlo}} Variational Inference},
  author = {Fujisawa, Masahiro and Sato, Issei},
  year = {2021},
  journal = {Journal of Machine Learning Research},
  volume = {22},
  number = {278},
  pages = {1--44},
  abstract = {We propose a variance reduction framework for variational inference using the Multilevel Monte Carlo (MLMC) method. Our framework is built on reparameterized gradient estimators and ``recycles'' parameters obtained from past update history in optimization. In addition, our framework provides a new optimization algorithm based on stochastic gradient descent (SGD) that adaptively estimates the sample size used for gradient estimation according to the ratio of the gradient variance. We theoretically show that, with our method, the variance of the gradient estimator decreases as optimization proceeds and that a learning rate scheduler function helps improve the convergence. We also show that, in terms of the signal-to-noise ratio, our method can improve the quality of gradient estimation by the learning rate scheduler function without increasing the initial sample size. Finally, we confirm that our method achieves faster convergence and reduces the variance of the gradient estimator compared with other methods through experimental comparisons with baseline methods using several benchmark datasets.},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\DX7XHXVR\\Fujisawa and Sato - 2021 - Multilevel Monte Carlo Variational Inference.pdf}
}

@article{gabrie_adaptive_2022,
  title = {Adaptive {{Monte Carlo}} Augmented with Normalizing Flows},
  author = {Gabri{\'e}, Marylou and Rotskoff, Grant M. and {Vanden-Eijnden}, Eric},
  year = {2022},
  month = mar,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {119},
  number = {10},
  pages = {e2109420119},
  abstract = {Significance             Monte Carlo methods, tools for sampling data from probability distributions, are widely used in the physical sciences, applied mathematics, and Bayesian statistics. Nevertheless, there are many situations in which it is computationally prohibitive to use Monte Carlo due to slow ``mixing'' between modes of a distribution unless hand-tuned algorithms are used to accelerate the scheme. Machine learning techniques based on generative models offer a compelling alternative to the challenge of designing efficient schemes for a specific system. Here, we formalize Monte Carlo augmented with normalizing flows and show that, with limited prior data and a physically inspired algorithm, we can substantially accelerate sampling with generative models.           ,              Many problems in the physical sciences, machine learning, and statistical inference necessitate sampling from a high-dimensional, multimodal probability distribution. Markov Chain Monte Carlo (MCMC) algorithms, the ubiquitous tool for this task, typically rely on random local updates to propagate configurations of a given system in a way that ensures that generated configurations will be distributed according to a target probability distribution asymptotically. In high-dimensional settings with multiple relevant metastable basins, local approaches require either immense computational effort or intricately designed importance sampling strategies to capture information about, for example, the relative populations of such basins. Here, we analyze an adaptive MCMC, which augments MCMC sampling with nonlocal transition kernels parameterized with generative models known as normalizing flows. We focus on a setting where there are no preexisting data, as is commonly the case for problems in which MCMC is used. Our method uses 1) an MCMC strategy that blends local moves obtained from any standard transition kernel with those from a generative model to accelerate the sampling and 2) the data generated this way to adapt the generative model and improve its efficacy in the MCMC algorithm. We provide a theoretical analysis of the convergence properties of this algorithm and investigate numerically its efficiency, in particular in terms of its propensity to equilibrate fast between metastable modes whose rough location is known a priori but respective probability weight is not. We show that our algorithm can sample effectively across large free energy barriers, providing dramatic accelerations relative to traditional MCMC algorithms.},
  langid = {english},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\G6YUS2FS\\Gabri et al. - 2022 - Adaptive Monte Carlo augmented with normalizing fl.pdf}
}

@article{gadioli_margot_2019,
  title = {{{mARGOt}}: {{A Dynamic Autotuning Framework}} for {{Self-Aware Approximate Computing}}},
  shorttitle = {{{mARGOt}}},
  author = {Gadioli, Davide and Vitali, Emanuele and Palermo, Gianluca and Silvano, Cristina},
  year = {2019},
  month = may,
  journal = {IEEE Transactions on Computers},
  volume = {68},
  number = {5},
  pages = {713--728}
}

@article{gagnon_nonreversible_2021,
  title = {Nonreversible Jump Algorithms for {{Bayesian}} Nested Model Selection},
  author = {Gagnon, Philippe and Doucet, Arnaud},
  year = {2021},
  month = apr,
  journal = {Journal of Computational and Graphical Statistics},
  volume = {30},
  number = {2},
  pages = {312--323},
  langid = {english},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\EPXM7TWD\\Gagnon and Doucet - 2021 - Nonreversible Jump Algorithms for Bayesian Nested .pdf}
}

@misc{ganguly_amortized_2022,
  title = {Amortized Variational Inference: {{Towards}} the Mathematical Foundation and Review},
  shorttitle = {Amortized Variational Inference},
  author = {Ganguly, Ankush and Jain, Sanjana and Watchareeruetai, Ukrit},
  year = {2022},
  month = sep,
  number = {arXiv:2209.10888},
  eprint = {2209.10888},
  eprinttype = {arxiv},
  primaryclass = {cs, math, stat},
  institution = {{arXiv}},
  abstract = {The core principle of Variational Inference (VI) is to convert the statistical inference problem of computing complex posterior probability densities into a tractable optimization problem. This property enables VI to be faster than several sampling-based techniques. However, the traditional VI algorithm is not scalable to large data sets and is unable to readily infer out-of-bounds data points without re-running the optimization process. Recent developments in the field, like stochastic-, black box- and amortized-VI, have helped address these issues. Generative modeling tasks nowadays widely make use of amortized VI for its efficiency and scalability, as it utilizes a parameterized function to learn the approximate posterior density parameters. With this paper, we review the mathematical foundations of various VI techniques to form the basis for understanding amortized VI. Additionally, we provide an overview of the recent trends that address several issues of amortized VI, such as the amortization gap, generalization issues, inconsistent representation learning, and posterior collapse. Finally, we analyze alternate divergence measures that improve VI optimization.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Information Theory,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\2GZXXJPV\\Ganguly et al. - 2022 - Amortized Variational Inference Towards the Mathe.pdf;C\:\\Users\\msca8h\\Zotero\\storage\\UQ4DB6Q9\\2209.html}
}

@article{garbuno-inigo_gaussian_2016,
  title = {Gaussian Process Hyper-Parameter Estimation Using {{Parallel Asymptotically Independent Markov Sampling}}},
  author = {{Garbuno-Inigo}, A. and DiazDelaO, F.A. and Zuev, K.M.},
  year = {2016},
  month = nov,
  journal = {Computational Statistics \& Data Analysis},
  volume = {103},
  pages = {367--383},
  langid = {english},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\GRU9QYV8\\Garbuno-Inigo et al. - 2016 - Gaussian process hyper-parameter estimation using .pdf}
}

@inproceedings{gardner_gpytorch_2018,
  title = {{{GPyTorch}}: {{Blackbox}} Matrix-Matrix {{Gaussian}} Process Inference with {{GPU}} Acceleration},
  shorttitle = {{{GPyTorch}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Gardner, Jacob and Pleiss, Geoff and Weinberger, Kilian Q and Bindel, David and Wilson, Andrew G},
  year = {2018},
  volume = {31},
  publisher = {{Curran Associates, Inc.}},
  abstract = {Despite advances in scalable models, the inference tools used for Gaussian processes (GPs) have yet to fully capitalize on developments in computing hardware. We present an efficient and general approach to GP inference based on Blackbox Matrix-Matrix multiplication (BBMM). BBMM inference uses a modified batched version of the conjugate gradients algorithm to derive all terms for training and inference in a single call. BBMM reduces the asymptotic complexity of exact GP inference from O(n\^3) to O(n\^2). Adapting this algorithm to scalable approximations and complex GP models simply requires a routine for efficient matrix-matrix multiplication with the kernel and its derivative. In addition, BBMM uses a specialized preconditioner to substantially speed up convergence. In experiments we show that BBMM effectively uses GPU hardware to dramatically accelerate both exact GP inference and scalable approximations. Additionally, we provide GPyTorch, a software platform for scalable GP inference via BBMM, built on PyTorch.},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\VGFS9GSI\\Gardner et al. - 2018 - GPyTorch Blackbox Matrix-Matrix Gaussian Process .pdf}
}

@article{garthwaite_adaptive_2016,
  title = {Adaptive Optimal Scaling of {{Metropolis}}\textendash{{Hastings}} Algorithms Using the {{Robbins}}\textendash{{Monro}} Process},
  author = {Garthwaite, P. H. and Fan, Y. and Sisson, S. A.},
  year = {2016},
  month = sep,
  journal = {Communications in Statistics - Theory and Methods},
  volume = {45},
  number = {17},
  pages = {5098--5111},
  langid = {english},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\H5AFFK39\\Garthwaite et al. - 2016 - Adaptive optimal scaling of MetropolisHastings al.pdf}
}

@article{gast_new_2018,
  title = {A New Analysis of {{Work Stealing}} with Latency},
  author = {Gast, N. and Khatiri, M. and Trystram, D. and Wagner, F.},
  year = {2018},
  journal = {arXiv:1805.00857},
  eprint = {1805.00857},
  eprinttype = {arxiv},
  abstract = {We study in this paper the impact of communication latency on the classical Work Stealing load balancing algorithm. Our paper extends the reference model in which we introduce a latency parameter. By using a theoretical analysis and simulation, we study the overall impact of this latency on the Makespan (maximum completion time). We derive a new expression of the expected running time of a bag of tasks scheduled by Work Stealing. This expression enables us to predict under which conditions a given run will yield acceptable performance. For instance, we can easily calibrate the maximal number of processors to use for a given work/platform combination. All our results are validated through simulation on a wide range of parameters.},
  archiveprefix = {arXiv}
}

@inproceedings{gastal_domain_2011,
  title = {Domain Transform for Edge-Aware Image and Video Processing},
  booktitle = {{{ACM SIGGRAPH}} 2011 Papers on - {{SIGGRAPH}} '11},
  author = {Gastal, Eduardo S. L. and Oliveira, Manuel M.},
  year = {2011},
  pages = {1},
  publisher = {{ACM Press}},
  address = {{Vancouver, British Columbia, Canada}},
  langid = {english}
}

@inproceedings{gates_autotuning_2017,
  title = {Autotuning Batch {{Cholesky}} Factorization in {{CUDA}} with Interleaved Layout of Matrices},
  booktitle = {2017 {{IEEE International Parallel}} and {{Distributed Processing Symposium Workshops}} ({{IPDPSW}})},
  author = {Gates, Mark and Kurzak, Jakub and Luszczek, Piotr and {Yu Pei} and Dongarra, Jack},
  year = {2017},
  month = may,
  pages = {1408--1417},
  publisher = {{IEEE}},
  address = {{Lake Buena Vista, FL}}
}

@article{gaudes_robust_2007,
  title = {Robust Array Beamforming with Sidelobe Control Using Support Vector Machines},
  author = {Gaudes, Csar C. and Santamaria, Ignacio and Via, Javier and Masgrau Gomez, Enrique Masgrau and Ses Paules, Talesa},
  year = {2007},
  month = feb,
  journal = {IEEE Transactions on Signal Processing},
  volume = {55},
  number = {2},
  pages = {574--584}
}

@misc{gcc_gcc_2018,
  title = {{{GCC}}, the {{GNU}} Compiler Collection},
  author = {GCC},
  year = {2018},
  month = jul,
  abstract = {The GNU Compiler Collection includes front ends for C, C++, Objective-C, Fortran, Ada, Go, and D, as well as libraries for these languages (libstdc++,...)}
}

@inproceedings{ge_turing_2018,
  title = {Turing: A Language for Flexible Probabilistic Inference},
  booktitle = {Proceedings of the {{International Conference}} on {{Machine Learning}}},
  author = {Ge, Hong and Xu, Kai and Ghahramani, Zoubin},
  year = {2018},
  series = {{{PMLR}}},
  volume = {84},
  pages = {1682--1690},
  publisher = {{ML Research Press}},
  biburl = {https://dblp.org/rec/bib/conf/aistats/GeXG18}
}

@inproceedings{geffner_mcmc_2021,
  title = {{{MCMC}} Variational Inference via Uncorrected {{Hamiltonian}} Annealing},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Geffner, Tomas and Domke, Justin},
  year = {2021},
  volume = {34},
  pages = {639--651},
  publisher = {{Curran Associates, Inc.}}
}

@inproceedings{geffner_rule_2020,
  title = {A Rule for Gradient Estimator Selection, with an Application to Variational Inference},
  booktitle = {Proceedings of the International Conference on Artificial Intelligence and Statistics},
  author = {Geffner, Tomas and Domke, Justin},
  year = {2020},
  month = aug,
  series = {{{PMLR}}},
  volume = {108},
  pages = {1803--1812},
  publisher = {{ML Research Press}},
  abstract = {Stochastic gradient descent (SGD) is the workhorse of modern machine learning. Sometimes, there are many different potential gradient estimators that can be used. When so, choosing the one with the best tradeoff between cost and variance is important. This paper analyzes the convergence rates of SGD as a function of time, rather than iterations. This results in a simple rule to select the estimator that leads to the best optimization convergence guarantee. This choice is the same for different variants of SGD, and with different assumptions about the objective (e.g. convexity or smoothness). Inspired by this principle, we propose a technique to automatically select an estimator when a finite pool of estimators is given. Then, we extend to infinite pools of estimators, where each one is indexed by control variate weights. Empirically, automatically choosing an estimator performs comparably to the best estimator chosen with hindsight.},
  pdf = {http://proceedings.mlr.press/v108/geffner20a/geffner20a.pdf}
}

@inproceedings{geffner_using_2018,
  title = {Using Large Ensembles of Control Variates for Variational Inference},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Geffner, Tomas and Domke, Justin},
  year = {2018},
  volume = {31},
  publisher = {{Curran Associates, Inc.}},
  abstract = {Variational inference is increasingly being addressed with stochastic optimization. In this setting, the gradient's variance plays a crucial role in the optimization procedure, since high variance gradients lead to poor convergence. A popular approach used to reduce gradient's variance involves the use of control variates. Despite the good results obtained, control variates developed for variational inference are typically looked at in isolation. In this paper we clarify the large number of control variates that are available by giving a systematic view of how they are derived. We also present a Bayesian risk minimization framework in which the quality of a procedure for combining control variates is quantified by its effect on optimization convergence rates, which leads to a very simple combination rule. Results show that combining a large number of control variates this way significantly improves the convergence of inference over using the typical gradient estimators or a reduced number of control variates.},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\9MZRZEYV\\Geffner and Domke - 2018 - Using Large Ensembles of Control Variates for Vari.pdf}
}

@inproceedings{geffner2021empirical,
  title = {Empirical Evaluation of Biased Methods for Alpha Divergence Minimization},
  booktitle = {Proceedings of the Symposium on Advances in Approximate Bayesian Inference},
  author = {Geffner, Tomas and Domke, Justin},
  year = {2021}
}

@article{geirhos_shortcut_2020,
  title = {Shortcut {{Learning}} in {{Deep Neural Networks}}},
  author = {Geirhos, Robert and Jacobsen, J{\"o}rn-Henrik and Michaelis, Claudio and Zemel, Richard and Brendel, Wieland and Bethge, Matthias and Wichmann, Felix A.},
  year = {2020},
  month = apr,
  journal = {arXiv:2004.07780 [cs, q-bio]},
  eprint = {2004.07780},
  eprinttype = {arxiv},
  primaryclass = {cs, q-bio},
  abstract = {Deep learning has triggered the current rise of artificial intelligence and is the workhorse of today's machine intelligence. Numerous success stories have rapidly spread all over science, industry and society, but its limitations have only recently come into focus. In this perspective we seek to distil how many of deep learning's problem can be seen as different symptoms of the same underlying problem: shortcut learning. Shortcuts are decision rules that perform well on standard benchmarks but fail to transfer to more challenging testing conditions, such as real-world scenarios. Related issues are known in Comparative Psychology, Education and Linguistics, suggesting that shortcut learning may be a common characteristic of learning systems, biological and artificial alike. Based on these observations, we develop a set of recommendations for model interpretation and benchmarking, highlighting recent advances in machine learning to improve robustness and transferability from the lab to real-world applications.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Quantitative Biology - Neurons and Cognition},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\F59FZCPR\\Geirhos et al. - 2020 - Shortcut Learning in Deep Neural Networks.pdf;C\:\\Users\\msca8h\\Zotero\\storage\\FESPD5QZ\\2004.html}
}

@article{gelb_statistics_2010,
  title = {Statistics of Distinct Clutter Classes in Midfrequency Active Sonar},
  author = {Gelb, James M and Heath, Ross E and Tipple, George L},
  year = {2010},
  month = apr,
  journal = {IEEE Journal of Oceanic Engineering},
  volume = {35},
  number = {2},
  pages = {220--229}
}

@book{gelman_bayesian_2014,
  title = {Bayesian Data Analysis},
  author = {Gelman, Andrew and Carlin, John and Stern, Hal and Dunson, David and Vehtari, Aki and Rubin, Donald},
  year = {2014},
  series = {Chapman \& {{Hall}}/{{CRC}} Texts in Statistical Science},
  edition = {Third},
  publisher = {{CRC Press}},
  address = {{Boca Raton}},
  abstract = {"Preface This book is intended to have three roles and to serve three associated audiences: an introductory text on Bayesian inference starting from first principles, a graduate text on effective current approaches to Bayesian modeling and computation in statistics and related fields, and a handbook of Bayesian methods in applied statistics for general users of and researchers in applied statistics. Although introductory in its early sections, the book is definitely not elementary in the sense of a first text in statistics. The mathematics used in our book is basic probability and statistics, elementary calculus, and linear algebra. A review of probability notation is given in Chapter 1 along with a more detailed list of topics assumed to have been studied. The practical orientation of the book means that the reader's previous experience in probability, statistics, and linear algebra should ideally have included strong computational components. To write an introductory text alone would leave many readers with only a taste of the conceptual elements but no guidance for venturing into genuine practical applications, beyond those where Bayesian methods agree essentially with standard non-Bayesian analyses. On the other hand, we feel it would be a mistake to present the advanced methods without first introducing the basic concepts from our data-analytic perspective. Furthermore, due to the nature of applied statistics, a text on current Bayesian methodology would be incomplete without a variety of worked examples drawn from real applications. To avoid cluttering the main narrative, there are bibliographic notes at the end of each chapter and references at the end of the book"--},
  lccn = {QA279.5 .G45 2014},
  keywords = {Bayesian statistical decision theory,MATHEMATICS / Probability \& Statistics / General}
}

@misc{gelman_bayesian_2020,
  title = {Bayesian Workflow},
  author = {Gelman, Andrew and Vehtari, Aki and Simpson, Daniel and Margossian, Charles C. and Carpenter, Bob and Yao, Yuling and Kennedy, Lauren and Gabry, Jonah and B{\"u}rkner, Paul-Christian and Modr{\'a}k, Martin},
  year = {2020},
  month = nov,
  number = {arXiv:2011.01808},
  eprint = {2011.01808},
  eprinttype = {arxiv},
  primaryclass = {stat},
  institution = {{arXiv}},
  abstract = {The Bayesian approach to data analysis provides a powerful way to handle uncertainty in all observations, model parameters, and model structure using probability theory. Probabilistic programming languages make it easier to specify and fit Bayesian models, but this still leaves us with many options regarding constructing, evaluating, and using these models, along with many remaining challenges in computation. Using Bayesian inference to solve real-world problems requires not only statistical skills, subject matter knowledge, and programming, but also awareness of the decisions made in the process of data analysis. All of these aspects can be understood as part of a tangled workflow of applied Bayesian statistics. Beyond inference, the workflow also includes iterative model building, model checking, validation and troubleshooting of computational problems, model understanding, and model comparison. We review all these aspects of workflow in the context of several examples, keeping in mind that in practice we will be fitting many models for any given problem, even if only a subset of them will ultimately be relevant for our conclusions.},
  archiveprefix = {arXiv},
  keywords = {Statistics - Methodology},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\UEJEGY8R\\Gelman et al. - 2020 - Bayesian Workflow.pdf;C\:\\Users\\msca8h\\Zotero\\storage\\626Z9VLE\\2011.html}
}

@article{gelman_bayesian_2020a,
  title = {Bayesian Workflow},
  author = {Gelman, Andrew and Vehtari, Aki and Simpson, Daniel and Margossian, Charles C. and Carpenter, Bob and Yao, Yuling and Kennedy, Lauren and Gabry, Jonah and B{\"u}rkner, Paul-Christian and Modr{\'a}k, Martin},
  year = {2020},
  month = nov,
  journal = {arXiv:2011.01808 [stat]},
  eprint = {2011.01808},
  eprinttype = {arxiv},
  primaryclass = {stat},
  abstract = {The Bayesian approach to data analysis provides a powerful way to handle uncertainty in all observations, model parameters, and model structure using probability theory. Probabilistic programming languages make it easier to specify and fit Bayesian models, but this still leaves us with many options regarding constructing, evaluating, and using these models, along with many remaining challenges in computation. Using Bayesian inference to solve real-world problems requires not only statistical skills, subject matter knowledge, and programming, but also awareness of the decisions made in the process of data analysis. All of these aspects can be understood as part of a tangled workflow of applied Bayesian statistics. Beyond inference, the workflow also includes iterative model building, model checking, validation and troubleshooting of computational problems, model understanding, and model comparison. We review all these aspects of workflow in the context of several examples, keeping in mind that in practice we will be fitting many models for any given problem, even if only a subset of them will ultimately be relevant for our conclusions.},
  archiveprefix = {arXiv},
  keywords = {Statistics - Methodology},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\6PFKHPVW\\Gelman et al. - 2020 - Bayesian Workflow.pdf;C\:\\Users\\msca8h\\Zotero\\storage\\BXUK567W\\2011.html}
}

@book{gelman_data_2007,
  title = {Data Analysis Using Regression and Multilevel/Hierarchical Models},
  author = {Gelman, Andrew and Hill, Jennifer},
  year = {2007},
  series = {Analytical Methods for Social Research},
  publisher = {{Cambridge University Press}},
  address = {{Cambridge; New York}},
  lccn = {HA31.3 .G45 2007},
  keywords = {Multilevel models (Statistics),Regression analysis},
  annotation = {OCLC: ocm67375137}
}

@article{gelman_inference_1992,
  title = {Inference from Iterative Simulation Using Multiple Sequences},
  author = {Gelman, Andrew and Rubin, Donald B.},
  year = {1992},
  month = nov,
  journal = {Statistical Science},
  volume = {7},
  number = {4},
  pages = {457--472},
  langid = {english},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\ALGTLQQX\\Gelman and Rubin - 1992 - Inference from Iterative Simulation Using Multiple.pdf}
}

@incollection{gelman_inference_2011,
  title = {Inference from Simulations and Monitoring Convergence},
  booktitle = {Handbook of Markov Chain Monte Carlo},
  author = {Gelman, Andrew and Shirley, Kenneth},
  year = {2011},
  pages = {163--174},
  publisher = {{CRC Press}}
}

@article{gelman_simulating_1998,
  title = {Simulating Normalizing Constants: From Importance Sampling to Bridge Sampling to Path Sampling},
  shorttitle = {Simulating Normalizing Constants},
  author = {Gelman, Andrew and Meng, Xiao-Li},
  year = {1998},
  month = may,
  journal = {Statistical Science},
  volume = {13},
  number = {2},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\A6GUD9J4\\Gelman and Meng - 1998 - Simulating normalizing constants from importance .pdf}
}

@article{gelman_weak_1997,
  title = {Weak Convergence and Optimal Scaling of Random Walk {{Metropolis}} Algorithms},
  author = {Gelman, A. and Gilks, W. R. and Roberts, G. O.},
  year = {1997},
  month = feb,
  journal = {The Annals of Applied Probability},
  volume = {7},
  number = {1},
  pages = {110--120},
  publisher = {{Institute of Mathematical Statistics}},
  abstract = {This paper considers the problem of scaling the proposal distribution of a multidimensional random walk Metropolis algorithm in order to maximize the efficiency of the algorithm. The main result is a weak convergence result as the dimension of a sequence of target densities, n, converges to \$\textbackslash infty\$. When the proposal variance is appropriately scaled according to n, the sequence of stochastic processes formed by the first component of each Markov chain converges to the appropriate limiting Langevin diffusion process. The limiting diffusion approximation admits a straightforward efficiency maximization problem, and the resulting asymptotically optimal policy is related to the asymptotic acceptance rate of proposed moves for the algorithm. The asymptotically optimal acceptance rate is 0.234 under quite general conditions. The main result is proved in the case where the target density has a symmetric product form. Extensions of the result are discussed.},
  keywords = {60F05,65U05,Markov chain Monte Carlo,Metropolis algorithm,Optimal scaling,weak convergence},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\8HSKIMYZ\\Gelman et al. - 1997 - Weak convergence and optimal scaling of random wal.pdf;C\:\\Users\\msca8h\\Zotero\\storage\\KHTUM7ZZ\\1034625254.html}
}

@article{gemba_estimating_2018,
  title = {Estimating Relative Channel Impulse Responses from Ships of Opportunity in a Shallow Water Environment},
  author = {Gemba, Kay L. and Sarkar, Jit and Cornuelle, Bruce and Hodgkiss, William S. and Kuperman, W. A.},
  year = {2018},
  month = sep,
  journal = {The Journal of the Acoustical Society of America},
  volume = {144},
  number = {3},
  pages = {1231--1244},
  langid = {english},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\I5AVSL8X\\Gemba et al. - 2018 - Estimating relative channel impulse responses from.pdf}
}

@inproceedings{germain_pacbayesian_2016,
  title = {{{PAC-Bayesian}} Theory Meets {{Bayesian}} Inference},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Germain, Pascal and Bach, Francis and Lacoste, Alexandre and {Lacoste-Julien}, Simon},
  year = {2016},
  volume = {29},
  publisher = {{Curran Associates, Inc.}},
  abstract = {We exhibit a strong link between frequentist PAC-Bayesian bounds and the Bayesian marginal likelihood. That is, for the negative log-likelihood loss function, we show that the minimization of PAC-Bayesian generalization bounds maximizes the Bayesian marginal likelihood. This provides an alternative explanation to the Bayesian Occam's razor criteria, under the assumption that the data is generated by an i.i.d. distribution. Moreover, as the negative log-likelihood is an unbounded loss function, we motivate and propose a PAC-Bayesian theorem tailored for the sub-gamma loss family, and we show that our approach is sound on classical Bayesian linear regression tasks.},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\NAIIHYLB\\Germain et al. - 2016 - PAC-Bayesian Theory Meets Bayesian Inference.pdf}
}

@article{gerstoft_adaptive_2003,
  title = {Adaptive Beamforming of a Towed Array during a Turn},
  author = {Gerstoft, P. and Hodgkiss, W.S. and Kuperman, W.A. and {Heechun Song} and Siderius, M. and Nielsen, P.L.},
  year = {2003},
  month = jan,
  journal = {IEEE Journal of Oceanic Engineering},
  volume = {28},
  number = {1},
  pages = {44--54},
  langid = {english}
}

@article{gerstoft_multisnapshot_2016,
  title = {Multisnapshot Sparse {{Bayesian}} Learning for {{DOA}}},
  author = {Gerstoft, Peter and Mecklenbrauker, Christoph F. and Xenaki, Angeliki and Nannuru, Santosh},
  year = {2016},
  month = oct,
  journal = {IEEE Signal Processing Letters},
  volume = {23},
  number = {10},
  pages = {1469--1473},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\ZATZ3W5Z\\Gerstoft et al. - 2016 - Multisnapshot Sparse Bayesian Learning for DOA.pdf}
}

@inproceedings{geumlek_renyi_2017,
  title = {Renyi Differential Privacy Mechanisms for Posterior Sampling},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Geumlek, Joseph and Song, Shuang and Chaudhuri, Kamalika},
  year = {2017},
  volume = {30},
  publisher = {{Curran Associates, Inc.}},
  abstract = {With the newly proposed privacy definition of R\'enyi Differential Privacy (RDP) in (Mironov, 2017), we re-examine the inherent privacy of releasing a single sample from a posterior distribution. We exploit the impact of the prior distribution in mitigating the influence of individual data points. In particular, we focus on sampling from an exponential family and specific generalized linear models, such as logistic regression. We propose novel RDP mechanisms as well as offering a new RDP analysis for an existing method in order to add value to the RDP framework. Each method is capable of achieving arbitrary RDP privacy guarantees, and we offer experimental results of their efficacy.},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\6MNZJBIR\\Geumlek et al. - 2017 - Renyi Differential Privacy Mechanisms for Posterio.pdf}
}

@inproceedings{geyer_markov_1991,
  title = {Markov {{Chain Monte Carlo Maximum Likelihood}}},
  booktitle = {Proceedings of the 23rd {{Symposium}} on the {{Interface}}},
  author = {Geyer, Charles J.},
  year = {1991},
  pages = {pp. 156-163},
  publisher = {{Interface Foundation of North America}},
  abstract = {Markov chain Monte Carlo (e. g., the Metropolis algorithm and Gibbs sampler) is a general tool for simulation of complex stochastic processes useful in many types of statistical inference. The basics of Markov chain Monte Carlo are reviewed, including choice of algorithms and variance estimation, and some new methods are introduced. The use of Markov chain Monte Carlo for maximum likelihood estimation is explained, and its performance is compared with maximum pseudo likelihood estimation.}
}

@article{geyer_practical_1992,
  title = {Practical {{Markov}} Chain {{Monte Carlo}}},
  author = {Geyer, Charles J.},
  year = {1992},
  month = nov,
  journal = {Statistical Science},
  volume = {7},
  number = {4},
  pages = {473--483},
  langid = {english},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\FNT6APTW\\Geyer - 1992 - Practical Markov Chain Monte Carlo.pdf}
}

@incollection{geyer2011introduction,
  title = {Introduction to Markov Chain Monte Carlo},
  booktitle = {Handbook of Markov Chain Monte Carlo},
  author = {Geyer, Charles J},
  year = {2011},
  pages = {3--48},
  publisher = {{CRC Press}}
}

@article{ghosh_fast_2019,
  title = {Fast Scale-Adaptive Bilateral Texture Smoothing},
  author = {Ghosh, Sanjay and Gavaskar, Ruturaj G. and Panda, Debasisha and Chaudhury, Kunal N.},
  year = {2019},
  journal = {IEEE Transactions on Circuits and Systems for Video Technology},
  pages = {1--1}
}

@article{gilboa_image_2004,
  title = {Image Enhancement and Denoising by Complex Diffusion Processes},
  author = {Gilboa, G. and Sochen, N. and Zeevi, Y.Y.},
  year = {2004},
  month = aug,
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {26},
  number = {8},
  pages = {1020--1036},
  langid = {english}
}

@article{giordani_adaptive_2010,
  title = {Adaptive Independent {{Metropolis}}\textendash{{Hastings}} by Fast Estimation of Mixtures of Normals},
  author = {Giordani, Paolo and Kohn, Robert},
  year = {2010},
  month = jan,
  journal = {Journal of Computational and Graphical Statistics},
  volume = {19},
  number = {2},
  pages = {243--259},
  langid = {english},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\DG63WRM7\\Giordani and Kohn - 2010 - Adaptive Independent MetropolisHastings by Fast E.pdf}
}

@article{girolami_riemann_2011,
  title = {Riemann Manifold {{Langevin}} and {{Hamiltonian Monte Carlo}} Methods: {{Riemann Manifold Langevin}} and {{Hamiltonian Monte Carlo Methods}}},
  shorttitle = {Riemann Manifold {{Langevin}} and {{Hamiltonian Monte Carlo}} Methods},
  author = {Girolami, Mark and Calderhead, Ben},
  year = {2011},
  month = mar,
  journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
  volume = {73},
  number = {2},
  pages = {123--214},
  langid = {english}
}

@inproceedings{gitman_understanding_2019,
  title = {Understanding the Role of Momentum in Stochastic Gradient Methods},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Gitman, Igor and Lang, Hunter and Zhang, Pengchuan and Xiao, Lin},
  year = {2019},
  volume = {32},
  publisher = {{Curran Associates, Inc.}},
  abstract = {The use of momentum in stochastic gradient methods has become a widespread practice in machine learning. Different variants of momentum, including heavy-ball momentum, Nesterov's accelerated gradient (NAG), and quasi-hyperbolic momentum (QHM), have demonstrated success on various tasks. Despite these empirical successes, there is a lack of clear understanding of how the momentum parameters affect convergence and various performance measures of different algorithms. In this paper, we use the general formulation of QHM to give a unified analysis of several popular algorithms, covering their asymptotic convergence conditions, stability regions, and properties of their stationary distributions. In addition, by combining the results on convergence rates and stationary distributions, we obtain sometimes counter-intuitive practical guidelines for setting the learning rate and momentum parameters.},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\GV2HCXI5\\Gitman et al. - 2019 - Understanding the Role of Momentum in Stochastic G.pdf}
}

@misc{gleich_wikipedia20070206_2007,
  title = {Wikipedia-20070206},
  author = {Gleich, D},
  year = {2007}
}

@article{glynn_analysis_1992,
  title = {Analysis of {{Initial Transient Deletion}} for {{Parallel Steady-State Simulations}}},
  author = {Glynn, Peter W. and Heidelberger, Philip},
  year = {1992},
  month = jul,
  journal = {SIAM Journal on Scientific and Statistical Computing},
  volume = {13},
  number = {4},
  pages = {904--922},
  langid = {english}
}

@article{godsill_bayesian_1997,
  title = {Bayesian Enhancement of Speech and Audio Signals Which Can Be Modelled as {{ARMA}} Processes},
  author = {Godsill, Simon J.},
  year = {1997},
  month = apr,
  journal = {International Statistical Review / Revue Internationale de Statistique},
  volume = {65},
  number = {1},
  pages = {1}
}

@incollection{godsill_bayesian_2014,
  title = {Bayesian {{Computational Methods}} in {{Signal Processing}}},
  booktitle = {Array and {{Statistical Signal Processing}}},
  author = {Godsill, Simon},
  year = {2014},
  series = {Academic {{Press Library}} in {{Signal Processing}}},
  number = {3},
  pages = {143--185},
  publisher = {{Elsevier}},
  langid = {english}
}

@article{godsill_relationship_2001,
  title = {On the Relationship between {{Markov}} Chain {{Monte Carlo}} Methods for Model Uncertainty},
  author = {Godsill, Simon J},
  year = {2001},
  month = jun,
  journal = {Journal of Computational and Graphical Statistics},
  volume = {10},
  number = {2},
  pages = {230--248},
  langid = {english}
}

@article{godsill_statistical_1998,
  title = {Statistical Reconstruction and Analysis of Autoregressive Signals in Impulsive Noise Using the {{Gibbs}} Sampler},
  author = {Godsill, S.J. and Rayner, P.J.W.},
  year = {1998},
  month = jul,
  journal = {IEEE Transactions on Speech and Audio Processing},
  volume = {6},
  number = {4},
  pages = {352--372}
}

@article{goldstein_reducedrank_1997,
  title = {Reduced-Rank Adaptive Filtering},
  author = {Goldstein, J.S. and Reed, I.S.},
  year = {1997},
  month = feb,
  journal = {IEEE Transactions on Signal Processing},
  volume = {45},
  number = {2},
  pages = {492--496}
}

@article{Gopal2011RunningRM,
  title = {Running Regenerative Markov Chains in Parallel},
  author = {Gopal, Vikneswaran and Casella, George},
  year = {2011}
}

@inproceedings{gorbunov_unified_2020,
  title = {A Unified Theory of {{SGD}}: {{Variance}} Reduction, Sampling, Quantization and Coordinate Descent},
  shorttitle = {A {{Unified Theory}} of {{SGD}}},
  booktitle = {Proceedings of the {{International Conference}} on {{Artificial Intelligence}} and {{Statistics}}},
  author = {Gorbunov, Eduard and Hanzely, Filip and Richtarik, Peter},
  year = {2020},
  month = jun,
  pages = {680--690},
  publisher = {{PMLR}},
  abstract = {In this paper we introduce a unified analysis of a large family of variants of proximal stochastic gradient descent (SGD) which so far have required different intuitions, convergence analyses, have different applications, and which have been developed separately in various communities. We show that our framework includes methods with and without the following tricks, and their combinations: variance reduction, importance sampling, mini-batch sampling, quantization, and coordinate sub-sampling.  As a by-product, we obtain the first unified theory of SGD and randomized coordinate descent (RCD) methods,  the first unified theory of variance reduced and non-variance-reduced SGD methods, and the first unified theory of quantized and non-quantized methods. A key to our approach is a parametric assumption on the iterates and stochastic gradients. In a single theorem we establish a linear convergence result under this assumption and strong-quasi convexity of the loss function. Whenever we recover an existing method as a special case, our theorem gives the best known complexity result. Our approach can be  used to motivate the development of new useful methods, and offers pre-proved convergence guarantees. To illustrate the strength of our approach, we develop five new variants of SGD, and through numerical experiments demonstrate some of their properties.},
  langid = {english},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\SIJIQB8S\\Gorbunov et al. - 2020 - A Unified Theory of SGD Variance Reduction, Sampl.pdf;C\:\\Users\\msca8h\\Zotero\\storage\\X4GE6BXY\\Gorbunov et al. - 2020 - A Unified Theory of SGD Variance Reduction, Sampl.pdf}
}

@article{gordon_novel_1993,
  title = {Novel Approach to Nonlinear/Non-{{Gaussian Bayesian}} State Estimation},
  author = {Gordon, N.J. and Salmond, D.J. and Smith, A.F.M.},
  year = {1993},
  journal = {IEE Proceedings F Radar and Signal Processing},
  volume = {140},
  number = {2},
  pages = {107},
  langid = {english},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\458KJ7FI\\Gordon et al. - 1993 - Novel approach to nonlinearnon-Gaussian Bayesian .pdf}
}

@inproceedings{gorham_measuring_2015,
  title = {Measuring Sample Quality with {{Stein}}' s Method},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Gorham, Jackson and Mackey, Lester},
  year = {2015},
  volume = {28},
  publisher = {{Curran Associates, Inc.}},
  abstract = {To improve the efficiency of Monte Carlo estimation, practitioners are turning to biased Markov chain Monte Carlo procedures that trade off asymptotic exactness for computational speed.  The reasoning is sound: a reduction in variance due to more rapid sampling can outweigh the bias introduced.  However, the inexactness creates new challenges for sampler and parameter selection, since standard measures of sample quality like effective sample size do not account for asymptotic bias.  To address these challenges, we introduce a new computable quality measure based on Stein's method that bounds the discrepancy between sample and target expectations over a large class of test functions.  We use our tool to compare exact, biased, and deterministic sample sequences and illustrate applications to hyperparameter selection, convergence rate assessment, and quantifying bias-variance tradeoffs in posterior inference.},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\HFWZFM4R\\Gorham and Mackey - 2015 - Measuring Sample Quality with Stein' s Method.pdf}
}

@article{gorman_analysis_1988,
  title = {Analysis of Hidden Units in a Layered Network Trained to Classify Sonar Targets},
  author = {Gorman, R.Paul and Sejnowski, Terrence J.},
  year = {1988},
  month = jan,
  journal = {Neural Networks},
  volume = {1},
  number = {1},
  pages = {75--89},
  langid = {english}
}

@inproceedings{gower_sgd_2019,
  title = {{{SGD}}: {{General}} Analysis and Improved Rates},
  booktitle = {Proceedings of the International Conference on Machine Learning},
  author = {Gower, Robert Mansel and Loizou, Nicolas and Qian, Xun and Sailanbayev, Alibek and Shulgin, Egor and Richt{\'a}rik, Peter},
  year = {2019},
  month = jun,
  series = {{{PMLR}}},
  volume = {97},
  pages = {5200--5209},
  publisher = {{ML Research Press}},
  abstract = {We propose a general yet simple theorem describing the convergence of SGD under the arbitrary sampling paradigm. Our theorem describes the convergence of an infinite array of variants of SGD, each of which is associated with a specific probability law governing the data selection rule used to form minibatches. This is the first time such an analysis is performed, and most of our variants of SGD were never explicitly considered in the literature before. Our analysis relies on the recently introduced notion of expected smoothness and does not rely on a uniform bound on the variance of the stochastic gradients. By specializing our theorem to different mini-batching strategies, such as sampling with replacement and independent sampling, we derive exact expressions for the stepsize as a function of the mini-batch size. With this we can also determine the mini-batch size that optimizes the total complexity, and show explicitly that as the variance of the stochastic gradient evaluated at the minimum grows, so does the optimal mini-batch size. For zero variance, the optimal mini-batch size is one. Moreover, we prove insightful stepsize-switching rules which describe when one should switch from a constant to a decreasing stepsize regime.},
  pdf = {http://proceedings.mlr.press/v97/qian19b/qian19b.pdf}
}

@inproceedings{gower_sgd_2021,
  title = {{{SGD}} for Structured Nonconvex Functions: {{Learning}} Rates, Minibatching and Interpolation},
  shorttitle = {{{SGD}} for {{Structured Nonconvex Functions}}},
  booktitle = {Proceedings of {{The International Conference}} on {{Artificial Intelligence}} and {{Statistics}}},
  author = {Gower, Robert and Sebbouh, Othmane and Loizou, Nicolas},
  year = {2021},
  month = mar,
  series = {{{PMLR}}},
  volume = {130},
  pages = {1315--1323},
  publisher = {{ML Research Press}},
  abstract = {Stochastic Gradient Descent (SGD) is being used routinely for optimizing non-convex functions. Yet, the standard convergence theory for SGD in the smooth non-convex setting gives a slow sublinear convergence to a stationary point. In this work, we provide several convergence theorems for SGD showing convergence to a global minimum for non-convex problems satisfying some extra structural assumptions. In particular, we focus on two large classes of structured non-convex functions: (i) Quasar (Strongly) Convex functions (a generalization of convex functions) and (ii) functions satisfying the Polyak-\L ojasiewicz condition (a generalization of strongly-convex functions). Our analysis relies on an Expected Residual condition which we show is a strictly weaker assumption than previously used growth conditions, expected smoothness or bounded variance assumptions. We provide theoretical guarantees for the convergence of SGD for different step-size selections including constant, decreasing and the recently proposed stochastic Polyak step-size. In addition, all of our analysis holds for the arbitrary sampling paradigm, and as such, we give insights into the complexity of minibatching and determine an optimal minibatch size. Finally, we show that for models that interpolate the training data, we can dispense of our Expected Residual condition and give state-of-the-art results in this setting.},
  langid = {english},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\LBM9376G\\Gower et al. - 2021 - SGD for Structured Nonconvex Functions Learning R.pdf;C\:\\Users\\msca8h\\Zotero\\storage\\Y4P7R4IB\\Gower et al. - 2021 - SGD for Structured Nonconvex Functions Learning R.pdf}
}

@article{gower_stochastic_2021,
  title = {Stochastic Quasi-Gradient Methods: {{Variance}} Reduction via {{Jacobian}} Sketching},
  shorttitle = {Stochastic Quasi-Gradient Methods},
  author = {Gower, Robert M. and Richt{\'a}rik, Peter and Bach, Francis},
  year = {2021},
  month = jul,
  journal = {Mathematical Programming},
  volume = {188},
  number = {1},
  pages = {135--192},
  abstract = {We develop a new family of variance reduced stochastic gradient descent methods for minimizing the average of a very large number of smooth functions. Our method\textemdash JacSketch\textemdash is motivated by novel developments in randomized numerical linear algebra, and operates by maintaining a stochastic estimate of a Jacobian matrix composed of the gradients of individual functions. In each iteration, JacSketch efficiently updates the Jacobian matrix by first obtaining a random linear measurement of the true Jacobian through (cheap) sketching, and then projecting the previous estimate onto the solution space of a linear matrix equation whose solutions are consistent with the measurement. The Jacobian estimate is then used to compute a variance-reduced unbiased estimator of the gradient. Our strategy is analogous to the way quasi-Newton methods maintain an estimate of the Hessian, and hence our method can be seen as a stochastic quasi-gradient method. Our method can also be seen as stochastic gradient descent applied to a controlled stochastic optimization reformulation of the original problem, where the control comes from the Jacobian estimates. We prove that for smooth and strongly convex functions, JacSketch converges linearly with a meaningful rate dictated by a single convergence theorem which applies to general sketches. We also provide a refined convergence theorem which applies to a smaller class of sketches, featuring a novel proof technique based on a stochastic Lyapunov function. This enables us to obtain sharper complexity results for variants of JacSketch with importance sampling. By specializing our general approach to specific sketching strategies, JacSketch reduces to the celebrated stochastic average gradient (SAGA) method, and its several existing and many new minibatch, reduced memory, and importance sampling variants. Our rate for SAGA with importance sampling is the current best-known rate for this method, resolving a conjecture by Schmidt~et~al. (Proceedings of the eighteenth international conference on artificial intelligence and statistics, AISTATS 2015, San Diego, California, 2015). The rates we obtain for minibatch SAGA are also superior to existing rates and are sufficiently tight as to show a decrease in total complexity as the minibatch size increases. Moreover, we obtain the first minibatch SAGA method with importance sampling.},
  langid = {english},
  keywords = {65Kxx,90C15,90C25,Covariates,Sketching,Stochastic gradient descent,Variance reduction},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\VZ3K2FCS\\Gower et al. - 2021 - Stochastic quasi-gradient methods variance reduct.pdf}
}

@article{gower_variancereduced_2020,
  title = {Variance-Reduced Methods for Machine Learning},
  author = {Gower, Robert M. and Schmidt, Mark and Bach, Francis and Richtarik, Peter},
  year = {2020},
  month = nov,
  journal = {Proceedings of the IEEE},
  volume = {108},
  number = {11},
  pages = {1968--1983},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\7MIX8UTB\\Gower et al. - 2020 - Variance-Reduced Methods for Machine Learning.pdf}
}

@article{goyal_accurate_2017,
  title = {Accurate, {{Large Minibatch SGD}}: {{Training ImageNet}} in 1 {{Hour}}},
  shorttitle = {Accurate, {{Large Minibatch SGD}}},
  author = {Goyal, Priya and Doll{\'a}r, Piotr and Girshick, Ross and Noordhuis, Pieter and Wesolowski, Lukasz and Kyrola, Aapo and Tulloch, Andrew and Jia, Yangqing and He, Kaiming},
  year = {2017},
  month = jun,
  journal = {arXiv:1706.02677 [cs]},
  eprint = {1706.02677},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Deep learning thrives with large neural networks and large datasets. However, larger networks and larger datasets result in longer training times that impede research and development progress. Distributed synchronous SGD offers a potential solution to this problem by dividing SGD minibatches over a pool of parallel workers. Yet to make this scheme efficient, the per-worker workload must be large, which implies nontrivial growth in the SGD minibatch size. In this paper, we empirically show that on the ImageNet dataset large minibatches cause optimization difficulties, but when these are addressed the trained networks exhibit good generalization. Specifically, we show no loss of accuracy when training with large minibatch sizes up to 8192 images. To achieve this result, we adopt a hyper-parameter-free linear scaling rule for adjusting learning rates as a function of minibatch size and develop a new warmup scheme that overcomes optimization challenges early in training. With these simple techniques, our Caffe2-based system trains ResNet-50 with a minibatch size of 8192 on 256 GPUs in one hour, while matching small minibatch accuracy. Using commodity hardware, our implementation achieves \textasciitilde 90\% scaling efficiency when moving from 8 to 256 GPUs. Our findings enable training visual recognition models on internet-scale data with high efficiency.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Distributed; Parallel; and Cluster Computing,Computer Science - Machine Learning},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\TUKW27U7\\Goyal et al. - 2017 - Accurate, Large Minibatch SGD Training ImageNet i.pdf;C\:\\Users\\msca8h\\Zotero\\storage\\2XSTJVTS\\1706.html}
}

@article{gramacy_bayesian_2008,
  title = {Bayesian {{Treed Gaussian Process Models With}} an {{Application}} to {{Computer Modeling}}},
  author = {Gramacy, Robert B and Lee, Herbert K. H},
  year = {2008},
  month = sep,
  journal = {Journal of the American Statistical Association},
  volume = {103},
  number = {483},
  pages = {1119--1130},
  langid = {english},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\S58VXEAP\\Gramacy and Lee - 2008 - Bayesian Treed Gaussian Process Models With an App.pdf}
}

@article{gramacy_importance_2010,
  title = {Importance Tempering},
  author = {Gramacy, Robert and Samworth, Richard and King, Ruth},
  year = {2010},
  month = jan,
  journal = {Statistics and Computing},
  volume = {20},
  number = {1},
  pages = {1--7},
  langid = {english}
}

@article{gramacy_particle_2011,
  title = {Particle {{Learning}} of {{Gaussian Process Models}} for {{Sequential Design}} and {{Optimization}}},
  author = {Gramacy, Robert B. and Polson, Nicholas G.},
  year = {2011},
  month = jan,
  journal = {Journal of Computational and Graphical Statistics},
  volume = {20},
  number = {1},
  pages = {102--118},
  langid = {english},
  file = {/home/msca8h/Documents/bayesian_optimization/gramacy2011.pdf;C\:\\Users\\msca8h\\Zotero\\storage\\6YUF6AMF\\Gramacy and Polson - 2011 - Particle Learning of Gaussian Process Models for S.pdf}
}

@article{green_delayed_2001,
  title = {Delayed Rejection in Reversible Jump {{Metropolis}}\textendash{{Hastings}}},
  author = {Green, Peter J. and Mira, Antonietta},
  year = {2001},
  month = dec,
  journal = {Biometrika},
  volume = {88},
  number = {4},
  pages = {1035--1053},
  abstract = {In a Metropolis\textendash Hastings algorithm, rejection of proposed moves is an intrinsic part of ensuring that the chain converges to the intended target distribution. However, persistent rejection, perhaps in particular parts of the state space, may indicate that locally the proposal distribution is badly calibrated to the target. As an alternative to careful off-line tuning of state-dependent proposals, the basic algorithm can be modified so that, on rejection, a second attempt to move is made. A different proposal can be generated from a new distribution that is allowed to depend on the previously rejected proposal. We generalise this idea of delaying the rejection and adapting the proposal distribution, due to Tierney \&amp; Mira (1999), to generate a more flexible class of methods that applies in particular to a variable-dimension setting. The approach is illustrated by two pedagogical examples and a more realistic application to a changepoints analysis for point processes.},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\7YJGHQ3K\\Green and Mira - 2001 - Delayed rejection in reversible jump MetropolisHa.pdf;C\:\\Users\\msca8h\\Zotero\\storage\\N9NBE9EC\\2722143.html}
}

@article{green_increasing_2020,
  title = {Increasing the Efficiency of {{Sequential Monte Carlo}} Samplers through the Use of Approximately Optimal {{L-kernels}}},
  author = {Green, Peter L. and Moore, Robert E. and Jackson, Ryan J. and Li, Jinglai and Maskell, Simon},
  year = {2020},
  month = apr,
  journal = {arXiv:2004.12838 [stat]},
  eprint = {2004.12838},
  eprinttype = {arxiv},
  primaryclass = {stat},
  abstract = {By facilitating the generation of samples from arbitrary probability distributions, Markov Chain Monte Carlo (MCMC) is, arguably, \textbackslash emph\{the\} tool for the evaluation of Bayesian inference problems that yield non-standard posterior distributions. In recent years, however, it has become apparent that Sequential Monte Carlo (SMC) samplers have the potential to outperform MCMC in a number of ways. SMC samplers are better suited to highly parallel computing architectures and also feature various tuning parameters that are not available to MCMC. One such parameter - the `L-kernel' - is a user-defined probability distribution that can be used to influence the efficiency of the sampler. In the current paper, the authors explain how to derive an expression for the L-kernel that minimises the variance of the estimates realised by an SMC sampler. Various approximation methods are then proposed to aid implementation of the proposed L-kernel. The improved performance of the resulting algorithm is demonstrated in multiple scenarios. For the examples shown in the current paper, the use of an approximately optimum L-kernel has reduced the variance of the SMC estimates by up to 99 \% while also reducing the number of times that resampling was required by between 65 \% and 70 \%. Python code and code tests accompanying this manuscript are available through the Github repository \textbackslash url\{https://github.com/plgreenLIRU/SMC\_approx\_optL\}.},
  archiveprefix = {arXiv},
  keywords = {Statistics - Computation},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\QME9EYBA\\Green et al. - 2020 - Increasing the efficiency of Sequential Monte Carl.pdf;C\:\\Users\\msca8h\\Zotero\\storage\\V5GX63NC\\2004.html}
}

@article{green_increasing_2022,
  title = {Increasing the Efficiency of {{Sequential Monte Carlo}} Samplers through the Use of Approximately Optimal {{L-kernels}}},
  author = {Green, P.L. and Devlin, L.J. and Moore, R.E. and Jackson, R.J. and Li, J. and Maskell, S.},
  year = {2022},
  month = jan,
  journal = {Mechanical Systems and Signal Processing},
  volume = {162},
  pages = {108028},
  langid = {english},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\NNF9LQDW\\Green et al. - 2022 - Increasing the efficiency of Sequential Monte Carl.pdf}
}

@article{green_reversible_1995,
  title = {Reversible Jump {{Markov}} Chain {{Monte Carlo}} Computation and {{Bayesian}} Model Determination},
  author = {Green, Peter J.},
  year = {1995},
  journal = {Biometrika},
  volume = {82},
  number = {4},
  pages = {711--732},
  langid = {english}
}

@incollection{green_transdimensional_2003,
  title = {Trans-Dimensional {{Markov}} Chain {{Monte Carlo}}},
  booktitle = {Highly {{Structured Stochastic Systems}}},
  author = {Green, Peter J.},
  year = {2003},
  series = {Oxford {{Statistical Science Series}}},
  number = {27},
  pages = {179--198},
  publisher = {{Oxford University Press, Oxford}}
}

@article{grenander_representations_1994,
  title = {Representations of {{Knowledge}} in {{Complex Systems}}},
  author = {Grenander, Ulf and Miller, Michael I.},
  year = {1994},
  journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
  volume = {56},
  number = {4},
  pages = {549--603},
  abstract = {Modern sensor technologies, especially in biomedicine, produce increasingly detailed and informative image ensembles, many extremely complex. It will be argued that pattern theory can supply mathematical representations of subject-matter knowledge that can be used as a basis for algorithmic `understanding' of such pictures. After a brief survey of the basic principles of pattern theory we shall illustrate them by an application to a concrete situation: high magnification (greater than 15 000 \texttimes ) electron micrographs of cardiac muscle cells. The aim is to build algorithms for automatic hypothesis formation concerning the number, location, orientation and shape of mitochondria and membranes. For this we construct a pattern theoretic model in the form of a prior probability measure on the space of configurations describing these hypotheses. This measure is synthesized by solving sequentially a jump-diffusion equation of generalized Langevin form. The jumps occur for the creation-annihilation of hypotheses, corresponding to a jump from one continuum to another in configuration (hypothesis) space. These continua (subhypotheses) are expressed in terms of products of low dimensional Lie groups acting on the generators of a template. We use a modified Bayes approach to obtain the hypothesis formation, also organized by solving a generalized Langevin equation. To justify this it is shown that the resulting jump-diffusion process is ergodic so that the solution converges to the desired probability measure. To speed up the convergence we reduce the computation of the drift term in the stochastic differential equation analytically to a curvilinear integral, with the random term computed almost instantaneously. The algorithms thus obtained are implemented, both for mitochondria and membranes, on a 4000 processor parallel machine. Photographs of the graphics illustrate how automatic hypothesis formation is achieved. This approach is applied to deformable neuroanatomical atlases and tracking recognition from narrow band and high resolution sensor arrays.}
}

@inproceedings{griffiths_constrained_2017,
  title = {Constrained {{Bayesian}} Optimization for Automatic Chemical Design},
  booktitle = {Proceedings of the {{NIPS Workshop}} on {{Bayesian Optimization}} ({{BayesOpt}}'17)},
  author = {Griffiths, Ryan-Rhys and {Hern{\'a}ndez-Lobato}, Jos{\'e} Miguel},
  year = {2017},
  abstract = {Automatic Chemical Design provides a framework for generating novel molecules with optimized molecular properties. The current model suffers from the pathology that it tends to produce invalid molecular structures. By reformulating the search procedure as a constrained Bayesian optimization problem, we showcase improvements in both the validity and quality of the generated molecules. We demonstrate that the model consistently produces novel molecules ranking above the 90th percentile of the distribution over training set scores across a range of objective functions. Importantly, our method suffers no degradation in the complexity or the diversity of the generated molecules.},
  keywords = {Statistics - Machine Learning},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\S4WAADMA\\Griffiths and Hernndez-Lobato - 2017 - Constrained Bayesian Optimization for Automatic Ch.pdf;C\:\\Users\\msca8h\\Zotero\\storage\\ZB93RBZS\\1709.html}
}

@inproceedings{grubel_performance_2015,
  title = {The {{Performance Implication}} of {{Task Size}} for {{Applications}} on the {{HPX Runtime System}}},
  booktitle = {2015 {{IEEE International Conference}} on {{Cluster Computing}}},
  author = {Grubel, Patricia and Kaiser, Hartmut and Cook, Jeanine and Serio, Adrian},
  year = {2015},
  month = sep,
  publisher = {{IEEE}},
  file = {/home/msca8h/Documents/parallel_scheduling/Grubel et al. - 2015 - The Performance Implication of Task Size for Appli.pdf}
}

@article{gu_recent_2018,
  title = {Recent Advances in Convolutional Neural Networks},
  author = {Gu, Jiuxiang and Wang, Zhenhua and Kuen, Jason and Ma, Lianyang and Shahroudy, Amir and Shuai, Bing and Liu, Ting and Wang, Xingxing and Wang, Gang and Cai, Jianfei and Chen, Tsuhan},
  year = {2018},
  month = may,
  journal = {Pattern Recognition},
  volume = {77},
  pages = {354--377},
  langid = {english},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\6HSV9JLM\\Gu et al. - 2018 - Recent advances in convolutional neural networks.pdf;C\:\\Users\\msca8h\\Zotero\\storage\\CTXXIA74\\gu2017.pdf}
}

@article{gu_stochastic_1998,
  title = {A Stochastic Approximation Algorithm with {{Markov}} Chain {{Monte-Carlo}} Method for Incomplete Data Estimation Problems},
  author = {Gu, Ming Gao and Kong, Fan Hui},
  year = {1998},
  month = jun,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {95},
  number = {13},
  pages = {7270--7274},
  langid = {english},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\MWI2B2JX\\Gu and Kong - 1998 - A stochastic approximation algorithm with Markov c.pdf}
}

@inproceedings{gueriot_sonar_2007,
  title = {Sonar Data Simulation Based on Tube Tracing},
  booktitle = {{{OCEANS}} 2007 - {{Europe}}},
  author = {Gueriot, Didier and Sintes, Christophe and Garello, Rene},
  year = {2007},
  month = jun,
  pages = {1--6},
  publisher = {{IEEE}},
  address = {{Aberdeen, Scotland, UK}}
}

@article{gulamrazul_bayesian_2003,
  title = {Bayesian Model Selection and Parameter Estimation of Nuclear Emission Spectra Using {{RJMCMC}}},
  author = {Gulam Razul, S. and Fitzgerald, W.J. and Andrieu, C.},
  year = {2003},
  month = feb,
  journal = {Nuclear Instruments and Methods in Physics Research Section A: Accelerators, Spectrometers, Detectors and Associated Equipment},
  volume = {497},
  number = {2-3},
  pages = {492--510},
  langid = {english}
}

@misc{guo_novel_2022,
  title = {A Novel Convergence Analysis for Algorithms of the {{Adam}} Family and Beyond},
  author = {Guo, Zhishuai and Xu, Yi and Yin, Wotao and Jin, Rong and Yang, Tianbao},
  year = {2022},
  month = feb,
  number = {arXiv:2104.14840},
  eprint = {2104.14840},
  eprinttype = {arxiv},
  primaryclass = {cs, math},
  institution = {{arXiv}},
  abstract = {Why does the original analysis of Adam fail, but it still converges very well in practice on a broad range of problems? There are still some mysteries about Adam that have not been unraveled. This paper provides a novel non-convex analysis of Adam and its many variants to uncover some of these mysteries. Our analysis exhibits that an increasing or large enough "momentum" parameter for the first-order moment used in practice is sufficient to ensure Adam and its many variants converge under a mild boundness condition on the adaptive scaling factor of the step size. In contrast, the original problematic analysis of Adam uses a momentum parameter that decreases to zero, which is the key reason that makes it diverge on some problems. To the best of our knowledge, this is the first time the gap between analysis and practice is bridged. Our analysis also exhibits more insights for practical implementations of Adam, e.g., increasing the momentum parameter in a stage-wise manner in accordance with stagewise decreasing step size would help improve the convergence. Our analysis of the Adam family is modular such that it can be (has been) extended to solving other optimization problems, e.g., compositional, min-max and bi-level problems. As an interesting yet non-trivial use case, we present an extension for solving non-convex min-max optimization in order to address a gap in the literature that either requires a large batch or has double loops. Our empirical studies corroborate the theory and also demonstrate the effectiveness in solving min-max problems.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Mathematics - Optimization and Control},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\9HJ822AN\\Guo et al. - 2022 - A Novel Convergence Analysis for Algorithms of the.pdf;C\:\\Users\\msca8h\\Zotero\\storage\\4A85ZXS3\\2104.html}
}

@inproceedings{gupta_exploiting_2018,
  title = {Exploiting {{Strategy-Space Diversity}} for {{Batch Bayesian Optimization}}},
  booktitle = {Proceedings of the {{Twenty-First International Conference}} on {{Artificial Intelligence}} and {{Statistics}}},
  author = {Gupta, Sunil and Shilton, Alistair and Rana, Santu and Venkatesh, Svetha},
  year = {2018},
  month = apr,
  series = {Proceedings of {{Machine Learning Research}}},
  volume = {84},
  pages = {538--547},
  publisher = {{PMLR}},
  address = {{Playa Blanca, Lanzarote, Canary Islands}},
  abstract = {This paper proposes a novel approach to batch Bayesian optimisation using a multi-objective optimisation framework with exploitation and exploration forming two objectives. The key advantage of this approach is that it uses a suite of strategies to balance exploration and exploitation and thus can efficiently handle the optimisation of a variety of functions with small to large number of local extrema. Another advantage is that it automatically determines the batch size within a specified budget avoiding unnecessary function evaluations. Theoretical analysis shows that the regret not only reduces sub-linearly but also by an additional reduction factor determined by the batch size. We demonstrate the efficiency of our algorithm by optimising a variety of benchmark functions, performing hyperparameter tuning of support vector regression and classification, and finally heat treatment process of an Al-Sc alloy. Comparisons with recent baseline algorithms confirm the usefulness of our algorithm.}
}

@article{gupta_information_2009,
  title = {An Information Matrix Prior for {{Bayesian}} Analysis in Generalized Linear Models with High Dimensional Data},
  author = {Gupta, Mayetri and Ibrahim, Joseph G.},
  year = {2009},
  journal = {Statistica Sinica},
  volume = {19},
  number = {4},
  pages = {1641--1663},
  publisher = {{Institute of Statistical Science, Academia Sinica}},
  abstract = {An important challenge in analyzing high dimensional data in regression settings is that of facing a situation in which the number of covariates p in the model greatly exceeds the sample size n (sometimes termed the "p {$>$} n" problem). In this article, we develop a novel specification for a general class of prior distributions, called Information Matrix (IM) priors, for high-dimensional generalized linear models. The priors are first developed for settings in which p {$<$} n, and then extended to the p {$>$} n case by defining a ridge parameter in the prior construction, leading to the Information Matrix Ridge (IMR) prior. The IM and IMR priors are based on a broad generalization of Zellner's g-prior for Gaussian linear models. Various theoretical properties of the prior and implied posterior are derived including existence of the prior and posterior moment generating functions, tail behavior, as well as connections to Gaussian priors and Jeffreys' prior. Several simulation studies and an application to a nucleosomal positioning data set demonstrate its advantages over Gaussian, as well as g-priors, in high dimensional settings.},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\H6F7EH6I\\Gupta and Ibrahim - 2009 - AN INFORMATION MATRIX PRIOR FOR BAYESIAN ANALYSIS .pdf}
}

@article{gupta_variable_2007,
  title = {Variable Selection in Regression Mixture Modeling for the Discovery of Gene Regulatory Networks},
  author = {Gupta, Mayetri and Ibrahim, Joseph G},
  year = {2007},
  month = sep,
  journal = {Journal of the American Statistical Association},
  volume = {102},
  number = {479},
  pages = {867--880},
  langid = {english}
}

@article{gustafson_reevaluating_1988,
  title = {Reevaluating {{Amdahl}}'s Law},
  author = {Gustafson, John L.},
  year = {1988},
  month = may,
  journal = {Communications of the ACM},
  volume = {31},
  number = {5},
  pages = {532--533},
  langid = {english},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\4IL35GWU\\Gustafson - 1988 - Reevaluating Amdahl's law.pdf}
}

@article{gustafson_value_2004,
  title = {On the {{Value}} of Derivative Evaluations and Random Walk Suppression in {{Markov Chain Monte Carlo}} Algorithms},
  author = {Gustafson, Paul and MacNab, Ying C. and Wen, Sijin},
  year = {2004},
  month = jan,
  journal = {Statistics and Computing},
  volume = {14},
  number = {1},
  pages = {23--38},
  langid = {english}
}

@inproceedings{gustafsson_evaluating_2020,
  title = {Evaluating Scalable {{Bayesian}} Deep Learning Methods for Robust Computer Vision},
  booktitle = {2020 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition Workshops}} ({{CVPRW}})},
  author = {Gustafsson, Fredrik K. and Danelljan, Martin and Schon, Thomas B.},
  year = {2020},
  month = jun,
  pages = {1289--1298},
  publisher = {{IEEE}},
  address = {{Seattle, WA, USA}},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\N63YCCXS\\Gustafsson et al. - 2020 - Evaluating Scalable Bayesian Deep Learning Methods.pdf}
}

@article{gyori_nonasymptotic_2015,
  title = {Non-Asymptotic Confidence Intervals for {{MCMC}} in Practice},
  author = {Gyori, Benjamin M. and Paulin, Daniel},
  year = {2015},
  month = sep,
  journal = {arXiv:1212.2016 [math]},
  eprint = {1212.2016},
  eprinttype = {arxiv},
  primaryclass = {math},
  abstract = {Using concentration inequalities, we give non-asymptotic confidence intervals for estimates obtained by Markov chain Monte Carlo (MCMC) simulations, when using the approximation \$\textbackslash mathbb\{E\}\_\{\textbackslash pi\} f\textbackslash approx (1/(N-t\_0))\textbackslash cdot \textbackslash sum\_\{i=t\_0+1\}\^N f(X\_i)\$. To allow the application of non-asymptotic error bounds in practice, here we state bounds formulated in terms of the spectral properties of the chain and the properties of \$f\$ and propose estimators of the parameters appearing in the bounds, including the spectral gap, mixing time, and asymptotic variance. We introduce a method for setting the burn-in time and the initial distribution that is theoretically well-founded and yet is relatively simple to apply. We also investigate the estimation of \$\textbackslash mathbb\{E\}\_\{\textbackslash pi\}f\$ via subsampling and by using parallel runs instead of a single run. Our results are applicable to both reversible and non-reversible Markov chains on discrete as well as general state spaces. We illustrate our methods by simulations for three examples of Bayesian inference in the context of risk models and clinical trials.},
  archiveprefix = {arXiv},
  keywords = {65C05; 60J10; 62M05; 82B20; 68Q87; 68W20,Mathematics - Probability},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\2CNZD5IU\\Gyori and Paulin - 2015 - Non-asymptotic confidence intervals for MCMC in pr.pdf;C\:\\Users\\msca8h\\Zotero\\storage\\BI28PNCY\\1212.html}
}

@article{haario2001,
  title = {An Adaptive Metropolis Algorithm},
  author = {Haario, Heikki and Saksman, Eero and Tamminen, Johanna},
  year = {2001},
  month = apr,
  journal = {Bernoulli},
  volume = {7},
  number = {2},
  pages = {223--242},
  publisher = {{Bernoulli Society for Mathematical Statistics and Probability}},
  fjournal = {Bernoulli}
}

@inproceedings{habeck_nested_2015,
  title = {Nested Sampling with Demons},
  booktitle = {Proceedings of the {{International Workshop}} on {{Bayesian Inference}} and {{Maximum Entropy Methods}} in {{Science}} And},
  author = {Habeck, Michael},
  year = {2015},
  series = {{{AIP Conference Proceedings}}},
  volume = {1641},
  pages = {121--129},
  address = {{Clos Luc\'e, Amboise, France}}
}

@inproceedings{habib2018auxiliary,
  title = {Auxiliary Variational {{MCMC}}},
  booktitle = {Proceedings of the {{International}} Conference on Learning Representations},
  author = {Habib, Raza and Barber, David},
  year = {2019}
}

@article{hagerup_allocating_1997,
  title = {Allocating Independent Tasks to Parallel Processors: An Experimental Study},
  shorttitle = {Allocating {{Independent Tasks}} to {{Parallel Processors}}},
  author = {Hagerup, Torben},
  year = {1997},
  month = dec,
  journal = {Journal of Parallel and Distributed Computing},
  volume = {47},
  number = {2},
  pages = {185--197},
  langid = {english},
  file = {/home/msca8h/Documents/parallel_scheduling/Hagerup - 1997 - Allocating Independent Tasks to Parallel Processor.pdf}
}

@book{hairer_geometric_2006,
  title = {Geometric {{Numerical Integration}}},
  author = {Hairer, Ernst},
  year = {2006},
  series = {Springer {{Series}} in {{Computational Mathematics}}},
  volume = {31},
  publisher = {{Springer-Verlag}},
  address = {{Berlin/Heidelberg}},
  langid = {english},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\3ATR88KQ\\2006 - Geometric Numerical Integration.pdf}
}

@incollection{hairer_symplectic_2006,
  title = {Symplectic {{Integration}} of {{Hamiltonian Systems}}},
  booktitle = {Geometric {{Numerical Integration}}},
  author = {Hairer, Ernst and Wanner, Gerhard and Lubich, Christian},
  year = {2006},
  volume = {31},
  pages = {179--236},
  publisher = {{Springer-Verlag}},
  address = {{Berlin/Heidelberg}},
  langid = {english}
}

@inproceedings{ham_robust_2015,
  title = {Robust Image Filtering Using Joint Static and Dynamic Guidance},
  booktitle = {{{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Ham, Bumsub and Cho, Minsu and Ponce, Jean},
  year = {2015},
  month = jun,
  pages = {4823--4831},
  publisher = {{IEEE}},
  address = {{Boston, MA, USA}},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\LEK5ETZ5\\Ham et al. - 2015 - Robust image filtering using joint static and dyna.pdf}
}

@article{ham_robust_2018,
  title = {Robust Guided Image Filtering Using Nonconvex Potentials},
  author = {Ham, Bumsub and Cho, Minsu and Ponce, Jean},
  year = {2018},
  month = jan,
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {40},
  number = {1},
  pages = {192--207},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\WXG8ZSPC\\Ham et al. - 2018 - Robust Guided Image Filtering Using Nonconvex Pote.pdf}
}

@inproceedings{han_stein_2017,
  title = {Stein {{Variational Adaptive Importance Sampling}}},
  booktitle = {Proceedings of the {{Thirty-Third Conference}} on {{Uncertainty}} in {{Artificial Intelligence}}, {{UAI}} 2017, {{Sydney}}, {{Australia}}, {{August}} 11-15, 2017},
  author = {Han, Jun and Liu, Qiang},
  year = {2017}
}

@inproceedings{han_stein_2018,
  title = {Stein {{Variational Gradient Descent Without Gradient}}},
  booktitle = {Proceedings of the 35th {{International Conference}} on {{Machine Learning}}},
  author = {Han, Jun and Liu, Qiang},
  year = {2018},
  month = jul,
  series = {Proceedings of {{Machine Learning Research}}},
  volume = {80},
  pages = {1900--1908},
  publisher = {{PMLR}},
  address = {{Stockholmsm\"assan, Stockholm Sweden}},
  abstract = {Stein variational gradient decent (SVGD) has been shown to be a powerful approximate inference algorithm for complex distributions. However, the standard SVGD requires calculating the gradient of the target density and cannot be applied when the gradient is unavailable. In this work, we develop a gradient-free variant of SVGD (GF-SVGD), which replaces the true gradient with a surrogate gradient, and corrects the introduced bias by re-weighting the gradients in a proper form. We show that our GF-SVGD can be viewed as the standard SVGD with a special choice of kernel, and hence directly inherits all the theoretical properties of SVGD. We shed insights on the empirical choice of the surrogate gradient and further, propose an annealed GF-SVGD that consistently outperforms a number of recent advanced gradient-free MCMC methods in our empirical studies.}
}

@article{hannun_cardiologistlevel_2019,
  title = {Cardiologist-Level Arrhythmia Detection and Classification in Ambulatory Electrocardiograms Using a Deep Neural Network},
  author = {Hannun, Awni Y. and Rajpurkar, Pranav and Haghpanahi, Masoumeh and Tison, Geoffrey H. and Bourn, Codie and Turakhia, Mintu P. and Ng, Andrew Y.},
  year = {07/jan/2019},
  journal = {Nature Medicine},
  volume = {25},
  number = {1},
  pages = {65--69}
}

@incollection{hansen_cma_2006,
  title = {The {{CMA}} Evolution Strategy: A Comparing Review},
  shorttitle = {The {{CMA Evolution Strategy}}},
  booktitle = {Towards a {{New Evolutionary Computation}}},
  author = {Hansen, Nikolaus},
  year = {2006},
  volume = {192},
  pages = {75--102},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  langid = {english},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\3QQYYDLZ\\Hansen - 2006 - The CMA Evolution Strategy A Comparing Review.pdf}
}

@article{hansen_cma_2016,
  title = {The {{CMA}} Evolution Strategy: {{A}} Tutorial},
  author = {Hansen, Nikolaus},
  year = {2016},
  journal = {arXiv preprint arXiv:1604.00772},
  eprint = {1604.00772},
  eprinttype = {arxiv},
  archiveprefix = {arXiv}
}

@misc{hansen_cmaes_2019,
  title = {{{CMA-ES}}/Pycma: R2.7.0},
  author = {Hansen, Nikolaus and Akimoto, Youhei and Baudis, Petr},
  year = {2019},
  month = apr,
  howpublished = {Zenodo}
}

@article{hardt_gradient_2018,
  title = {Gradient {{Descent Learns Linear Dynamical Systems}}},
  author = {Hardt, Moritz and Ma, Tengyu and Recht, Benjamin},
  year = {2018},
  journal = {Journal of Machine Learning Research},
  volume = {19},
  number = {29},
  pages = {1--44},
  abstract = {We prove that stochastic gradient descent efficiently converges to the global optimizer of the maximum likelihood objective of an unknown linear time-invariant dynamical system from a sequence of noisy observations generated by the system. Even though the objective function is non-convex, we provide polynomial running time and sample complexity bounds under strong but natural assumptions. Linear systems identification has been studied for many decades, yet, to the best of our knowledge, these are the first polynomial guarantees for the problem we consider.},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\V4X7DYFL\\Hardt et al. - 2018 - Gradient Descent Learns Linear Dynamical Systems.pdf}
}

@inproceedings{hartono_annotationbased_2009,
  title = {Annotation-Based Empirical Performance Tuning Using {{Orio}}},
  booktitle = {2009 {{IEEE International Symposium}} on {{Parallel}} \& {{Distributed Processing}}},
  author = {Hartono, Albert and Norris, Boyana and Sadayappan, P.},
  year = {2009},
  month = may,
  pages = {1--11},
  publisher = {{IEEE}},
  address = {{Rome, Italy}},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\C5Z5CP4D\\Hartono et al. - 2009 - Annotation-based empirical performance tuning usin.pdf}
}

@article{hastie_model_2012,
  title = {Model Choice Using Reversible Jump {{Markov}} Chain {{Monte Carlo}}},
  author = {Hastie, David I. and Green, Peter J.},
  year = {2012},
  journal = {Statistica Neerlandica},
  volume = {66},
  number = {3},
  pages = {309--338},
  abstract = {We review the across-model simulation approach to computation for Bayesian model determination, based on the reversible jump Markov chain Monte Carlo method. Advantages, difficulties and variations of the methods are discussed. We also discuss some limitations of the ideal Bayesian view of the model determination problem, for which no computational methods can provide a cure.},
  langid = {english},
  keywords = {across-model sampling,Bayes factors,Bayesian model determination,posterior model probabilities,transdimensional inference,variable dimension problems},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1467-9574.2012.00516.x},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\5BAZ8P2K\\j.1467-9574.2012.00516.html}
}

@article{hastings_monte_1970,
  title = {Monte {{Carlo}} Sampling Methods Using {{Markov}} Chains and Their Applications},
  author = {Hastings, W. K.},
  year = {1970},
  month = apr,
  journal = {Biometrika},
  volume = {57},
  number = {1},
  pages = {97--109},
  langid = {english},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\YF7HGJJC\\Hastings - 1970 - Monte Carlo sampling methods using Markov chains a.pdf}
}

@article{hayashi_doa_2016,
  title = {{{DOA}} Estimation for Wideband Signals Based on Weighted {{Squared TOPS}}},
  author = {Hayashi, Hirotaka and Ohtsuki, Tomoaki},
  year = {2016},
  month = oct,
  journal = {EURASIP Journal on Wireless Communications and Networking},
  volume = {2016},
  number = {1},
  pages = {243},
  abstract = {This paper introduces a new direction-of-arrival (DOA) estimation method for wideband signal sources. The new method estimates the DOA of wideband signal sources based on squared test of orthogonality of projected subspaces (Squared TOPS) which is an improved method of TOPS. TOPS and Squared TOPS use the signal and noise subspaces of multiple frequency components of wideband signal sources. Although coherent wideband method, such as coherent signal subspace method (CSSM), performs high DOA estimation accuracy, it requires the initial estimate of signal source directions. On the other hand, TOPS and Squared TOPS can provide good performance of DOA estimation without the initial value of signal sources; however, some false peaks appear in spatial spectrum based on these methods. The proposed method, called weighted Squared TOPS (WS-TOPS), uses the modified squared matrix and selective weighted averaging process to improve DOA estimation performance. The performance of WS-TOPS is compared with those of TOPS, Squared TOPS, incoherent MUSIC, and test of orthogonality of frequency spaces (TOFS) through computer simulations. The simulation results show that WS-TOPS can suppress all false peaks in spatial spectrum and improve DOA estimation accuracy and also keep the same resolution performance as Squared TOPS.},
  langid = {english},
  keywords = {Array signal processing,Direction-of-arrival estimation,Wideband signals},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\X45G9RIZ\\Hayashi and Ohtsuki - 2016 - DOA estimation for wideband signals based on weigh.pdf}
}

@article{he_lagging_2019,
  title = {{{LAGGING INFERENCE NETWORKS AND POSTERIOR COLLAPSE IN VARIATIONAL AUTOENCODERS}}},
  author = {He, Junxian and Spokoyny, Daniel and Neubig, Graham and {Berg-Kirkpatrick}, Taylor},
  year = {2019},
  langid = {english},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\YVL7462R\\He et al. - 2019 - LAGGING INFERENCE NETWORKS AND POSTERIOR COLLAPSE .pdf}
}

@inproceedings{he_lagging_2022,
  title = {Lagging Inference Networks and Posterior Collapse in Variational Autoencoders},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {He, Junxian and Spokoyny, Daniel and Neubig, Graham and {Berg-Kirkpatrick}, Taylor},
  year = {2022},
  month = feb,
  abstract = {The variational autoencoder (VAE) is a popular combination of deep latent variable model and accompanying variational learning technique. By using a neural inference network to approximate the model's posterior on latent variables, VAEs efficiently parameterize a lower bound on marginal data likelihood that can be optimized directly via gradient methods. In practice, however, VAE training often results in a degenerate local optimum known as "posterior collapse" where the model learns to ignore the latent variable and the approximate posterior mimics the prior. In this paper, we investigate posterior collapse from the perspective of training dynamics. We find that during the initial stages of training the inference network fails to approximate the model's true posterior, which is a moving target. As a result, the model is encouraged to ignore the latent encoding and posterior collapse occurs. Based on this observation, we propose an extremely simple modification to VAE training to reduce inference lag: depending on the model's current mutual information between latent variable and observation, we aggressively optimize the inference network before performing each model update. Despite introducing neither new model components nor significant complexity over basic VAE, our approach is able to avoid the problem of collapse that has plagued a large amount of previous work. Empirically, our approach outperforms strong autoregressive baselines on text and image benchmarks in terms of held-out likelihood, and is competitive with more complex techniques for avoiding collapse while being substantially faster.},
  langid = {english},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\8Z9NWKVS\\He et al. - 2022 - Lagging Inference Networks and Posterior Collapse .pdf}
}

@inproceedings{he_powerful_2016,
  title = {A {{Powerful Generative Model Using Random Weights}} for the {{Deep Image Representation}}},
  booktitle = {Proceedings of the 30th {{International Conference}} on {{Neural Information Processing Systems}}},
  author = {He, Kun and Wang, Yan and Hopcroft, John},
  year = {2016},
  series = {{{NIPS}}'16},
  pages = {631--639},
  publisher = {{Curran Associates Inc.}},
  address = {{Red Hook, NY, USA}}
}

@inproceedings{hemmsen_ultrasound_2010,
  title = {Ultrasound Image Quality Assessment: A Framework for Evaluation of Clinical Image Quality},
  shorttitle = {Ultrasound Image Quality Assessment},
  booktitle = {Proc. {{SPIE Med}}. {{Imag}}.},
  author = {Hemmsen, Martin Christian and Petersen, Mads M{\o}ller and Nikolov, Svetoslav Ivanov and Nielsen, Michael Backmann and Jensen, J{\o}rgen Arendt},
  year = {2010},
  month = mar,
  pages = {76290C},
  address = {{San Diego, California, USA}},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\WXP9D6RE\\Hemmsen et al. - 2010 - Ultrasound image quality assessment a framework f.pdf}
}

@article{hemmsen_vivo_2012,
  title = {In {{Vivo Evaluation}} of {{Synthetic Aperture Sequential Beamforming}}},
  author = {Hemmsen, Martin Christian and Hansen, Peter M{\o}ller and Lange, Theis and Hansen, Jens Munk and Hansen, Kristoffer Lindskov and Nielsen, Michael Bachmann and Jensen, J{\o}rgen Arendt},
  year = {2012},
  month = apr,
  journal = {Ultrasound in Medicine \& Biology},
  volume = {38},
  number = {4},
  pages = {708--716},
  langid = {english}
}

@article{hennig_entropy_2012,
  title = {Entropy {{Search}} for {{Information-efficient Global Optimization}}},
  author = {Hennig, Philipp and Schuler, Christian J.},
  year = {2012},
  month = jun,
  journal = {J. Mach. Learn. Res.},
  volume = {13},
  number = {1},
  pages = {1809--1837},
  keywords = {expectation propagation,Gaussian processes,information,optimization,probability}
}

@inproceedings{henrandez-lobato_predictive_2014,
  title = {Predictive Entropy Search for Efficient Global Optimization of Black-Box Functions},
  booktitle = {Adv. {{Neural Inf}}. {{Process}}. {{Syst}}.},
  author = {{Henr{\'a}ndez-Lobato}, Jos{\'e} Miguel and Hoffman, Matthew W. and Ghahramani, Zoubin},
  year = {2014},
  series = {{{NIPS}}'14},
  volume = {27},
  pages = {918--926}
}

@article{hensman_gaussian_2013,
  title = {Gaussian Processes for Big Data},
  author = {Hensman, James and Fusi, Nicolo and Lawrence, Neil D},
  year = {2013},
  journal = {arXiv preprint arXiv:1309.6835},
  eprint = {1309.6835},
  eprinttype = {arxiv},
  archiveprefix = {arXiv}
}

@inproceedings{hensman_mcmc_2015,
  title = {{{MCMC}} for {{Variationally Sparse Gaussian Processes}}},
  booktitle = {Proceedings of the 28th {{International Conference}} on {{Neural Information Processing Systems}} - {{Volume}} 1},
  author = {Hensman, James and Matthews, Alexander G. de G. and Filippone, Maurizio and Ghahramani, Zoubin},
  year = {2015},
  series = {{{NIPS}}'15},
  pages = {1648--1656},
  publisher = {{MIT Press}},
  address = {{Cambridge, MA, USA}},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\63C6CTGI\\Hensman et al. - 2015 - MCMC for Variationally Sparse Gaussian Processes.pdf}
}

@inproceedings{hernandez-lobato_blackbox_2016,
  title = {Black-Box Alpha Divergence Minimization},
  booktitle = {Proceedings of the International Conference on Machine Learning},
  author = {{Hernandez-Lobato}, Jose and Li, Yingzhen and Rowland, Mark and Bui, Thang and {Hernandez-Lobato}, Daniel and Turner, Richard},
  year = {2016},
  month = jun,
  series = {{{PMLR}}},
  volume = {48},
  pages = {1511--1520},
  publisher = {{ML Research Press}},
  abstract = {Black-box alpha (BB-{$\alpha$}) is a new approximate inference method based on the minimization of {$\alpha$}-divergences. BB-{$\alpha$}scales to large datasets because it can be implemented using stochastic gradient descent. BB-{$\alpha$}can be applied to complex probabilistic models with little effort since it only requires as input the likelihood function and its gradients. These gradients can be easily obtained using automatic differentiation. By changing the divergence parameter {$\alpha$}, the method is able to interpolate between variational Bayes (VB) ({$\alpha\rightarrow$}0) and an algorithm similar to expectation propagation (EP) ({$\alpha$}= 1). Experiments on probit regression and neural network regression and classification problems show that BB-{$\alpha$}with non-standard settings of {$\alpha$}, such as {$\alpha$}= 0.5, usually produces better predictions than with {$\alpha\rightarrow$}0 (VB) or {$\alpha$}= 1 (EP).},
  pdf = {http://proceedings.mlr.press/v48/hernandez-lobatob16.pdf}
}

@article{hesterberg_weighted_1995,
  title = {Weighted Average Importance Sampling and Defensive Mixture Distributions},
  author = {Hesterberg, Tim},
  year = {1995},
  month = may,
  journal = {Technometrics},
  volume = {37},
  number = {2},
  pages = {185--194},
  langid = {english}
}

@article{hettinger_forward_2017,
  title = {Forward {{Thinking}}: {{Building}} and {{Training Neural Networks One Layer}} at a {{Time}}},
  shorttitle = {Forward {{Thinking}}},
  author = {Hettinger, Chris and Christensen, Tanner and Ehlert, Ben and Humpherys, Jeffrey and Jarvis, Tyler and Wade, Sean},
  year = {2017},
  month = jun,
  journal = {arXiv:1706.02480 [cs, stat]},
  eprint = {1706.02480},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {We present a general framework for training deep neural networks without backpropagation. This substantially decreases training time and also allows for construction of deep networks with many sorts of learners, including networks whose layers are defined by functions that are not easily differentiated, like decision trees. The main idea is that layers can be trained one at a time, and once they are trained, the input data are mapped forward through the layer to create a new learning problem. The process is repeated, transforming the data through multiple layers, one at a time, rendering a new data set, which is expected to be better behaved, and on which a final output layer can achieve good performance. We call this forward thinking and demonstrate a proof of concept by achieving state-of-the-art accuracy on the MNIST dataset for convolutional neural networks. We also provide a general mathematical formulation of forward thinking that allows for other types of deep learning problems to be considered.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\SR52EAE6\\Hettinger et al. - 2017 - Forward Thinking Building and Training Neural Net.pdf;C\:\\Users\\msca8h\\Zotero\\storage\\N36RZQSN\\1706.html}
}

@book{hildebrand_introduction_2003,
  title = {Introduction to {{Numerical Analysis}}:},
  author = {Hildebrand, Francis B.},
  year = {2003},
  month = mar,
  series = {Dover {{Books}} on {{Mathematics}}},
  publisher = {{Dover Publications}}
}

@inproceedings{hinder_nearoptimal_2020,
  title = {Near-Optimal Methods for Minimizing Star-Convex Functions and Beyond},
  booktitle = {Proceedings of {{Conference}} on {{Learning Theory}}},
  author = {Hinder, Oliver and Sidford, Aaron and Sohoni, Nimit},
  year = {2020},
  month = jul,
  series = {{{PMLR}}},
  volume = {125},
  pages = {1894--1938},
  publisher = {{ML Research Press}},
  abstract = {In this paper, we provide near-optimal accelerated first-order methods for minimizing a broad class of smooth nonconvex functions that are unimodal on all lines through a minimizer. This function class, which we call the class of smooth quasar-convex functions, is parameterized by a constant {$\gamma\in$}(0,1]{$\gamma\in$}(0,1]\textbackslash gamma \textbackslash in (0,1]: {$\gamma$}=1{$\gamma$}=1\textbackslash gamma = 1 encompasses the classes of smooth convex and star-convex functions, and smaller values of {$\gamma\gamma\backslash$}gamma indicate that the function can be "more nonconvex." We develop a variant of accelerated gradient descent that computes an {$\epsilon\epsilon\backslash$}epsilon-approximate minimizer of a smooth {$\gamma\gamma\backslash$}gamma-quasar-convex function with at most O({$\gamma-$}1{$\epsilon-$}1/2log({$\gamma-$}1{$\epsilon-$}1))O({$\gamma-$}1{$\epsilon-$}1/2log({$\gamma-$}1{$\epsilon-$}1))O(\textbackslash gamma\^\{-1\} \textbackslash epsilon\^\{-1/2\} \textbackslash log(\textbackslash gamma\^\{-1\} \textbackslash epsilon\^\{-1\})) total function and gradient evaluations. We also derive a lower bound of {$\Omega$}({$\gamma-$}1{$\epsilon-$}1/2){$\Omega$}({$\gamma-$}1{$\epsilon-$}1/2)\textbackslash Omega(\textbackslash gamma\^\{-1\} \textbackslash epsilon\^\{-1/2\}) on the worst-case number of gradient evaluations required by any deterministic first-order method, showing that, up to a logarithmic factor, no deterministic first-order method can improve upon ours.},
  langid = {english},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\PF2EMTMC\\Hinder et al. - 2020 - Near-Optimal Methods for Minimizing Star-Convex Fu.pdf}
}

@inproceedings{hinton_keeping_1993,
  title = {Keeping the Neural Networks Simple by Minimizing the Description Length of the Weights},
  booktitle = {Proceedings of the Annual Conference on {{Computational}} Learning Theory},
  author = {Hinton, Geoffrey E. and {van Camp}, Drew},
  year = {1993},
  pages = {5--13},
  publisher = {{ACM Press}},
  address = {{Santa Cruz, California, United States}},
  langid = {english}
}

@article{hochreiter_flat_1997,
  title = {Flat Minima},
  author = {Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
  year = {1997},
  month = jan,
  journal = {Neural Computation},
  volume = {9},
  number = {1},
  pages = {1--42},
  abstract = {We present a new algorithm for finding low-complexity neural networks with high generalization capability. The algorithm searches for a ``flat'' minimum of the error function. A flat minimum is a large connected region in weight space where the error remains approximately constant. An MDL-based, Bayesian argument suggests that flat minima correspond to ``simple'' networks and low expected overfitting. The argument is based on a Gibbs algorithm variant and a novel way of splitting generalization error into underfitting and overfitting error. Unlike many previous approaches, ours does not require gaussian assumptions and does not depend on a ``good'' weight prior. Instead we have a prior over input output functions, thus taking into account net architecture and training set. Although our algorithm requires the computation of second-order derivatives, it has backpropagation's order of complexity. Automatically, it effectively prunes units, weights, and input lines. Various experiments with feedforward and recurrent nets are described. In an application to stock market prediction, flat minimum search outperforms conventional backprop, weight decay, and ``optimal brain surgeon/optimal brain damage.''},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\YZEAP2FZ\\Hochreiter and Schmidhuber - 1997 - Flat Minima.pdf;C\:\\Users\\msca8h\\Zotero\\storage\\PCJIXP38\\Flat-Minima.html}
}

@inproceedings{hochreiter_simplifying_1994,
  title = {Simplifying Neural Nets by Discovering Flat Minima},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
  year = {1994},
  volume = {7},
  publisher = {{MIT Press}},
  abstract = {We present a new algorithm for finding low complexity networks  with high generalization capability. The algorithm searches for  large connected regions of so-called ''fiat'' minima of the error func(cid:173) tion. In the weight-space environment of a "flat" minimum, the  error remains approximately constant. Using an MDL-based ar(cid:173) gument, flat minima can be shown to correspond to low expected  overfitting. Although our algorithm requires the computation of  second order derivatives, it has backprop's order of complexity.  Experiments with feedforward and recurrent nets are described. In  an application to stock market prediction, the method outperforms  conventional backprop, weight decay, and "optimal brain surgeon" .},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\ENSIIZBR\\Hochreiter and Schmidhuber - 1994 - SIMPLIFYING NEURAL NETS BY DISCOVERING FLAT MINIMA.pdf}
}

@inproceedings{hoefler_performance_2011,
  title = {Performance Modeling for Systematic Performance Tuning},
  booktitle = {State of the {{Practice Reports}} on - {{SC}} '11},
  author = {Hoefler, Torsten and Gropp, William and Kramer, William and Snir, Marc},
  year = {2011},
  pages = {1},
  publisher = {{ACM Press}},
  address = {{Seattle, Washington}},
  langid = {english},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\CXPPFFCW\\Hoefler et al. - 2011 - Performance modeling for systematic performance tu.pdf}
}

@inproceedings{hoffeins_examining_2017,
  title = {Examining the {{Reproducibility}} of {{Using Dynamic Loop Scheduling Techniques}} in {{Scientific Applications}}},
  booktitle = {2017 {{IEEE International Parallel}} and {{Distributed Processing Symposium Workshops}} ({{IPDPSW}})},
  author = {Hoffeins, Franziska and Ciorba, Florina M. and Banicescu, Ioana},
  year = {2017},
  month = may,
  publisher = {{IEEE}},
  file = {/home/msca8h/Documents/parallel_scheduling/Hoffeins et al. - 2017 - Examining the Reproducibility of Using Dynamic Loo.pdf}
}

@incollection{hoffer_norm_2018,
  title = {Norm Matters: Efficient and Accurate Normalization Schemes in Deep Networks},
  booktitle = {Advances in {{Neural Information Processing Systems}} 31},
  author = {Hoffer, Elad and Banner, Ron and Golan, Itay and Soudry, Daniel},
  year = {2018},
  pages = {2160--2170},
  publisher = {{Curran Associates, Inc.}}
}

@inproceedings{hoffman_blackbox_2020,
  title = {Black-Box Variational Inference as a Parametric Approximation to {{Langevin}} Dynamics},
  booktitle = {Proceedings of the 37th {{International Conference}} on {{Machine Learning}}},
  author = {Hoffman, Matthew and Ma, Yian},
  year = {2020},
  month = nov,
  pages = {4324--4341},
  publisher = {{PMLR}},
  abstract = {Variational inference (VI) and Markov chain Monte Carlo (MCMC) are approximate posterior inference algorithms that are often said to have complementary strengths, with VI being fast but biased and MCMC being slower but asymptotically unbiased. In this paper, we analyze gradient-based MCMC and VI procedures and find theoretical and empirical evidence that these procedures are not as different as one might think. In particular, a close examination of the Fokker-Planck equation that governs the Langevin dynamics (LD) MCMC procedure reveals that LD implicitly follows a gradient flow that corresponds to a variational inference procedure based on optimizing a nonparametric normalizing flow. This result suggests that the transient bias of LD (due to the Markov chain not having burned in) may track that of VI (due to the optimizer not having converged), up to differences due to VI's asymptotic bias and parameterization. Empirically, we find that the transient biases of these algorithms (and their momentum-accelerated counterparts) do evolve similarly. This suggests that practitioners with a limited time budget may get more accurate results by running an MCMC procedure (even if it's far from burned in) than a VI procedure, as long as the variance of the MCMC estimator can be dealt with (e.g., by running many parallel chains).},
  langid = {english},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\5XALMR9Z\\Hoffman and Ma - 2020 - Black-Box Variational Inference as a Parametric Ap.pdf;C\:\\Users\\msca8h\\Zotero\\storage\\6IHTEK58\\Hoffman and Ma - 2020 - Black-Box Variational Inference as a Parametric Ap.pdf}
}

@inproceedings{hoffman_outputspace_2015,
  title = {Output-Space Predictive Entropy Search for Flexible Global Optimization},
  booktitle = {{{NIPS}} Workshop on {{Bayesian Optimization}}},
  author = {Hoffman, Matthew W and Ghahramani, Zoubin},
  year = {2015}
}

@inproceedings{hoffman_tuningfree_2022,
  title = {Tuning-Free Generalized {{Hamiltonian Monte Carlo}}},
  booktitle = {Proceedings of {{The}} 25th {{International Conference}} on {{Artificial Intelligence}} and {{Statistics}}},
  author = {Hoffman, Matthew D. and Sountsov, Pavel},
  year = {2022},
  month = may,
  pages = {7799--7813},
  publisher = {{PMLR}},
  abstract = {Hamiltonian Monte Carlo (HMC) has become a go-to family of Markov chain Monte Carlo (MCMC) algorithms for Bayesian inference problems, in part because we have good procedures for automatically tuning its parameters. Much less attention has been paid to automatic tuning of generalized HMC (GHMC), in which the auxiliary momentum vector is partially updated frequently instead of being completely resampled infrequently. Since GHMC spreads progress over many iterations, it is not straightforward to tune GHMC based on quantities typically used to tune HMC such as average acceptance rate and squared jumped distance. In this work, we propose an ensemble-chain adaptation (ECA) algorithm for GHMC that automatically selects values for all of GHMC's tunable parameters each iteration based on statistics collected from a population of many chains. This algorithm is designed to make good use of SIMD hardware accelerators such as GPUs, allowing most chains to be updated in parallel each iteration. Unlike typical adaptive-MCMC algorithms, our ECA algorithm does not perturb the chain's stationary distribution, and therefore does not need to be ``frozen'' after warmup. Empirically, we find that the proposed algorithm quickly converges to its stationary distribution, producing accurate estimates of posterior expectations with relatively few gradient evaluations per chain.},
  langid = {english},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\M7IKQ3GV\\Hoffman and Sountsov - 2022 - Tuning-Free Generalized Hamiltonian Monte Carlo.pdf}
}

@book{hol_resampling_2004,
  title = {Resampling in Particle Filters},
  author = {Hol, Jeroen D},
  year = {2004},
  publisher = {{Institutionen f\"or systemteknik}}
}

@article{holden_adaptive_2009,
  title = {Adaptive Independent {{Metropolis}}\textendash{{Hastings}}},
  author = {Holden, Lars and Hauge, Ragnar and Holden, Marit},
  year = {2009},
  month = feb,
  journal = {The Annals of Applied Probability},
  volume = {19},
  number = {1},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\XQC5UB3T\\Holden et al. - 2009 - Adaptive independent MetropolisHastings.pdf}
}

@inproceedings{holfort_investigation_2008,
  title = {Investigation of Sound Speed Errors in Adaptive Beamforming},
  booktitle = {{{IEEE Ultrasonics Symposium}}},
  author = {Holfort, Iben Kraglund and Gran, Fredrik and Jensen, Jorgen Arendt},
  year = {2008},
  month = nov,
  series = {{{IUS}}'08},
  pages = {1080--1083},
  publisher = {{IEEE}},
  address = {{Beijing, China}},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\XD9D4ERS\\Holfort et al. - 2008 - Investigation of sound speed errors in adaptive be.pdf}
}

@article{honeine_online_2012,
  title = {Online {{Kernel Principal Component Analysis}}: {{A Reduced-Order Model}}},
  shorttitle = {Online {{Kernel Principal Component Analysis}}},
  author = {Honeine, P.},
  year = {2012},
  month = sep,
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {34},
  number = {9},
  pages = {1814--1826},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\D7XI2LWP\\Honeine - 2012 - Online Kernel Principal Component Analysis A Redu.pdf}
}

@article{honeine_preimage_2011,
  title = {Preimage {{Problem}} in {{Kernel-Based Machine Learning}}},
  author = {Honeine, Paul and Richard, Cedric},
  year = {2011},
  month = mar,
  journal = {IEEE Signal Processing Magazine},
  volume = {28},
  number = {2},
  pages = {77--88},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\U7SHY2UD\\Honeine and Richard - 2011 - Preimage Problem in Kernel-Based Machine Learning.pdf}
}

@inproceedings{hong_efficient_2018,
  title = {Efficient Sparse-Matrix Multi-Vector Product on {{GPUs}}},
  booktitle = {Proceedings of the 27th {{International Symposium}} on {{High-Performance Parallel}} and {{Distributed Computing}} - {{HPDC}} '18},
  author = {Hong, Changwan and Sadayappan, P. and {Sukumaran-Rajam}, Aravind and Bandyopadhyay, Bortik and Kim, Jinsung and Kurt, S{\"u}reyya Emre and Nisa, Israt and Sabhlok, Shivani and {\c C}ataly{\"u}rek, {\"U}mit V. and Parthasarathy, Srinivasan},
  year = {2018},
  pages = {66--79},
  publisher = {{ACM Press}},
  address = {{Tempe, Arizona}},
  langid = {english}
}

@article{horowitz_generalized_1991,
  title = {A Generalized Guided {{Monte Carlo}} Algorithm},
  author = {Horowitz, Alan M.},
  year = {1991},
  month = oct,
  journal = {Physics Letters B},
  volume = {268},
  number = {2},
  pages = {247--252},
  langid = {english},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\82JGXGWZ\\Horowitz - 1991 - A generalized guided Monte Carlo algorithm.pdf}
}

@misc{horvath_textttstop_2022,
  title = {\texttt{StoP} and \texttt{GraD}: {{Adaptive}} Learning Rates for Faster Stochastic Gradient Methods},
  author = {Horv{\'a}th, Samuel and Mishchenko, Konstantin and Richt{\'a}rik, Peter},
  year = {2022},
  month = aug,
  number = {arXiv:2208.05287},
  eprint = {2208.05287},
  eprinttype = {arxiv},
  primaryclass = {cs, math},
  institution = {{arXiv}},
  abstract = {In this work, we propose new adaptive step size strategies that improve several stochastic gradient methods. Our first method (StoPS) is based on the classical Polyak step size (Polyak, 1987) and is an extension of the recent development of this method for the stochastic optimization-SPS (Loizou et al., 2021), and our second method, denoted GraDS, rescales step size by "diversity of stochastic gradients". We provide a theoretical analysis of these methods for strongly convex smooth functions and show they enjoy deterministic-like rates despite stochastic gradients. Furthermore, we demonstrate the theoretical superiority of our adaptive methods on quadratic objectives. Unfortunately, both StoPS and GraDS depend on unknown quantities, which are only practical for the overparametrized models. To remedy this, we drop this undesired dependence and redefine StoPS and GraDS to StoP and GraD, respectively. We show that these new methods converge linearly to the neighbourhood of the optimal solution under the same assumptions. Finally, we corroborate our theoretical claims by experimental validation, which reveals that GraD is particularly useful for deep learning optimization.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Mathematics - Optimization and Control},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\KUAAUGPB\\Horvth et al. - 2022 - Adaptive Learning Rates for Faster Stochastic Grad.pdf;C\:\\Users\\msca8h\\Zotero\\storage\\ZQYW47QT\\2208.html}
}

@inproceedings{hou_autotuning_2017,
  title = {Auto-{{Tuning Strategies}} for {{Parallelizing Sparse Matrix-Vector}} ({{SpMV}}) {{Multiplication}} on {{Multi-}} and {{Many-Core Processors}}},
  booktitle = {2017 {{IEEE International Parallel}} and {{Distributed Processing Symposium Workshops}} ({{IPDPSW}})},
  author = {Hou, Kaixi and Feng, Wu-chun and Che, Shuai},
  year = {2017},
  month = may,
  pages = {713--722},
  publisher = {{IEEE}},
  address = {{Orlando / Buena Vista, FL, USA}}
}

@inproceedings{hu_cluster_2016,
  title = {Cluster Driven Anisotropic Diffusion for Speckle Reduction in Ultrasound Images},
  booktitle = {{{IEEE International Conference}} on {{Image Processing}} ({{ICIP}})},
  author = {Hu, Zilong and Tang, Jinshan},
  year = {2016},
  month = sep,
  pages = {2325--2329},
  publisher = {{IEEE}},
  address = {{Phoenix, AZ, USA}}
}

@article{hu_underdetermined_2017,
  title = {Underdetermined {{DOA}} Estimation Method for Wideband Signals Using Joint Nonnegative Sparse {{Bayesian}} Learning},
  author = {Hu, Nan and Sun, Bing and Zhang, Yi and Dai, Jisheng and Wang, Jiajun and Chang, Chunqi},
  year = {2017},
  month = may,
  journal = {IEEE Signal Processing Letters},
  volume = {24},
  number = {5},
  pages = {535--539}
}

@article{hu_wideband_2012,
  title = {Wideband {{DOA}} Estimation from the Sparse Recovery Perspective for the Spatial-Only Modeling of Array Data},
  author = {Hu, Nan and Xu, Dongyang and Xu, Xu and Ye, Zhongfu},
  year = {2012},
  month = may,
  journal = {Signal Processing},
  volume = {92},
  number = {5},
  pages = {1359--1364},
  abstract = {This communication utilizes a sparse recovery technique named ``least absolute shrinkage and selection operator'' (LASSO) to formulate and solve the problem of direction-of-arrival (DOA) estimation for far-field wideband sources. The spatial-only modeling for wideband array output is employed when the sources have flat spectra, and the DOA estimation problem in this model can be transformed into a sparse recovery problem. Via the LASSO technique, an optimization problem is constructed and solved by second-order cone (SOC) programming to obtain the DOA estimates. Numerical simulations are also provided in contrast with those existing algorithms using the spatial-only model.},
  langid = {english},
  keywords = {Direction-of-arrival estimation,Least absolute shrinkage and selection operator,Second-order cone programming,Wideband source},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\5IU3ZSBU\\Hu et al. - 2012 - Wideband DOA estimation from the sparse recovery p.pdf}
}

@article{huang_new_2019,
  title = {New Designs on {{MVDR}} Robust Adaptive Beamforming Based on Optimal Steering Vector Estimation},
  author = {Huang, Yongwei and Zhou, Mingkang and Vorobyov, Sergiy A.},
  year = {2019},
  month = jul,
  journal = {IEEE Transactions on Signal Processing},
  volume = {67},
  number = {14},
  pages = {3624--3638},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\IRJH3W2J\\Huang et al. - 2019 - New Designs on MVDR Robust Adaptive Beamforming Ba.pdf}
}

@article{huang_temporally_2007,
  title = {Temporally Correlated Source Separation Using Variational {{Bayesian}} Learning Approach},
  author = {Huang, Qinghua and Yang, Jie and Wei, Shoushui},
  year = {2007},
  month = sep,
  journal = {Digital Signal Processing},
  volume = {17},
  number = {5},
  pages = {873--890},
  langid = {english}
}

@article{huang_variational_2007,
  title = {Variational {{Bayesian}} Learning for Speech Modeling and Enhancement},
  author = {Huang, Qinghua and Yang, Jie and Wei, Shoushui},
  year = {2007},
  month = sep,
  journal = {Signal Processing},
  volume = {87},
  number = {9},
  pages = {2026--2035},
  langid = {english}
}

@inproceedings{hughes_memoized_2013,
  title = {Memoized Online Variational Inference for {{Dirichlet}} Process Mixture Models},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Hughes, Michael C and Sudderth, Erik},
  year = {2013},
  volume = {26},
  publisher = {{Curran Associates, Inc.}}
}

@misc{huix_variational_2022,
  title = {Variational Inference of Overparameterized {{Bayesian}} Neural Networks: {{A}} Theoretical and Empirical Study},
  shorttitle = {Variational {{Inference}} of Overparameterized {{Bayesian Neural Networks}}},
  author = {Huix, Tom and Majewski, Szymon and Durmus, Alain and Moulines, Eric and Korba, Anna},
  year = {2022},
  month = jul,
  number = {arXiv:2207.03859},
  eprint = {2207.03859},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  institution = {{arXiv}},
  abstract = {This paper studies the Variational Inference (VI) used for training Bayesian Neural Networks (BNN) in the overparameterized regime, i.e., when the number of neurons tends to infinity. More specifically, we consider overparameterized two-layer BNN and point out a critical issue in the mean-field VI training. This problem arises from the decomposition of the lower bound on the evidence (ELBO) into two terms: one corresponding to the likelihood function of the model and the second to the Kullback-Leibler (KL) divergence between the prior distribution and the variational posterior. In particular, we show both theoretically and empirically that there is a trade-off between these two terms in the overparameterized regime only when the KL is appropriately re-scaled with respect to the ratio between the the number of observations and neurons. We also illustrate our theoretical results with numerical experiments that highlight the critical choice of this ratio.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\FBI72NSP\\Huix et al. - 2022 - Variational Inference of overparameterized Bayesia.pdf;C\:\\Users\\msca8h\\Zotero\\storage\\N2YWIHAR\\2207.html}
}

@article{hummel_factoring_1992,
  title = {Factoring: A Method for Scheduling Parallel Loops},
  author = {Hummel, Susan Flynn and Schonberg, Edith and Flynn, Lawrence E.},
  year = {1992},
  month = aug,
  journal = {Commun. of the ACM},
  volume = {35},
  number = {8},
  pages = {90--101},
  keywords = {chunking,dynamic scheduling,parallel loop scheduling,partitioning},
  file = {/home/msca8h/Documents/parallel_scheduling/Hummel et al. - 1992 - Factoring A Method for Scheduling Parallel Loops.pdf}
}

@inproceedings{hummel_loadsharing_1996,
  title = {Load-Sharing in Heterogeneous Systems via Weighted Factoring},
  booktitle = {Proceedings of the {{Eighth Annual ACM Symposium}} on {{Parallel Algorithms}} and {{Architectures}}},
  author = {Hummel, Susan Flynn and Schmidt, Jeanette and Uma, R. N. and Wein, Joel},
  year = {1996},
  series = {{{SPAA}} '96},
  pages = {318--328},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  file = {/home/msca8h/Documents/parallel_scheduling/Hummel et al. - 1996 - Load-sharing in Heterogeneous Systems via Weighted.pdf}
}

@article{hung_coherent_1990,
  title = {Coherent Wide-Band {{ESPRIT}} Method for Directions-of-Arrival Estimation of Multiple Wide-Band Sources},
  author = {Hung, H. and Kaveh, M.},
  year = {1990},
  month = feb,
  journal = {IEEE Transactions on Acoustics, Speech, and Signal Processing},
  volume = {38},
  number = {2},
  pages = {354--356},
  abstract = {A strategy extending the coherent signal-subspace method (CSM) is proposed for estimating directions of arrival of multiple wideband sources. The proposed method, coherent wideband ESPRIT, extends the ESPRIT algorithm to a framework based on the CSM. A simulation example is provided to illustrate the effectiveness of the proposed method.{$<>$}},
  keywords = {Direction of arrival estimation,Discrete Fourier transforms,Frequency,Multiple signal classification,Narrowband,Noise generators,Sensor arrays,Signal generators,Statistics,Wideband},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\V4HPP42H\\Hung and Kaveh - 1990 - Coherent wide-band ESPRIT method for directions-of.pdf}
}

@article{hung_focussing_1988,
  title = {Focussing Matrices for Coherent Signal-Subspace Processing},
  author = {Hung, H. and Kaveh, M.},
  year = {Aug./1988},
  journal = {IEEE Transactions on Acoustics, Speech, and Signal Processing},
  volume = {36},
  number = {8},
  pages = {1272--1281}
}

@phdthesis{hunter_underwater_2006,
  type = {Doctoral {{Thesis}}},
  title = {Underwater Acoustic Modelling for Synthetic Aperture Sonar},
  author = {Hunter, Alan J.},
  year = {2006},
  school = {University of Canterbury},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\L9DYKTSV\\Hunter - 2006 - Underwater acoustic modelling for synthetic apertu.pdf}
}

@inproceedings{huynh_streaming_2016,
  title = {Streaming Variational Inference for {{Dirichlet}} Process Mixtures},
  booktitle = {Proceedings of the {{Asian Conference}} on {{Machine Learning}}},
  author = {Huynh, Viet and Phung, Dinh and Venkatesh, Svetha},
  year = {2016},
  month = feb,
  pages = {237--252},
  publisher = {{PMLR}},
  abstract = {Bayesian nonparametric models are theoretically suitable to learn streaming data due to their complexity relaxation to the volume of observed data. However, most of the existing variational inference algorithms are not applicable to streaming applications since they require truncation on variational distributions. In this paper, we present two truncation-free variational algorithms, one for mix-membership inference called TFVB (truncation-free variational Bayes), and the other for hard clustering inference called TFME (truncation-free maximization expectation). With these algorithms, we further developed a streaming learning framework for the popular Dirichlet process mixture (DPM) models. Our experiments demonstrate the usefulness of our framework in both synthetic and real-world data.},
  langid = {english},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\8R6SLZAX\\Huynh et al. - 2016 - Streaming Variational Inference for Dirichlet Proc.pdf}
}

@article{hyder_directionofarrival_2010,
  title = {Direction-of-Arrival Estimation Using a Mixed $\ell _{2,0}$ Norm Approximation},
  author = {Hyder, M M and Mahata, K},
  year = {2010},
  month = sep,
  journal = {IEEE Transactions on Signal Processing},
  volume = {58},
  number = {9},
  pages = {4646--4655}
}

@article{iba_population_2001,
  title = {Population {{Monte Carlo}} Algorithms.},
  author = {Iba, Yukito},
  year = {2001},
  journal = {Transactions of the Japanese Society for Artificial Intelligence},
  volume = {16},
  pages = {279--286},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\H7935D92\\Iba - 2001 - Population Monte Carlo algorithms..pdf}
}

@article{iglberger_expression_2012,
  title = {Expression {{Templates Revisited}}: {{A Performance Analysis}} of {{Current Methodologies}}},
  shorttitle = {Expression {{Templates Revisited}}},
  author = {Iglberger, Klaus and Hager, Georg and Treibig, Jan and R{\"u}de, Ulrich},
  year = {2012},
  month = jan,
  journal = {SIAM Journal on Scientific Computing},
  volume = {34},
  number = {2},
  pages = {C42-C69},
  langid = {english}
}

@inproceedings{ioffe_batch_2015,
  title = {Batch {{Normalization}}: {{Accelerating Deep Network Training}} by {{Reducing Internal Covariate Shift}}},
  booktitle = {Proceedings of the 32nd {{International Conference}} on {{International Conference}} on {{Machine Learning}} - {{Volume}} 37},
  author = {Ioffe, Sergey and Szegedy, Christian},
  year = {2015},
  series = {{{ICML}}'15},
  pages = {448--456},
  publisher = {{JMLR.org}}
}

@article{izmailov_subspace_2019,
  title = {Subspace Inference for {{Bayesian}} Deep Learning},
  author = {Izmailov, Pavel and Maddox, Wesley J. and Kirichenko, Polina and Garipov, Timur and Vetrov, Dmitry and Wilson, Andrew Gordon},
  year = {2019},
  month = jul,
  journal = {arXiv:1907.07504 [cs, stat]},
  eprint = {1907.07504},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Bayesian inference was once a gold standard for learning with neural networks, providing accurate full predictive distributions and well calibrated uncertainty. However, scaling Bayesian inference techniques to deep neural networks is challenging due to the high dimensionality of the parameter space. In this paper, we construct low-dimensional subspaces of parameter space, such as the first principal components of the stochastic gradient descent (SGD) trajectory, which contain diverse sets of high performing models. In these subspaces, we are able to apply elliptical slice sampling and variational inference, which struggle in the full parameter space. We show that Bayesian model averaging over the induced posterior in these subspaces produces accurate predictions and well calibrated predictive uncertainty for both regression and image classification.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\F2WJBSZJ\\Izmailov et al. - 2019 - Subspace Inference for Bayesian Deep Learning.pdf;C\:\\Users\\msca8h\\Zotero\\storage\\MHZI3QHT\\1907.html}
}

@inproceedings{izmailov_what_2021,
  title = {What Are {{Bayesian}} Neural Network Posteriors Really Like?},
  booktitle = {Proceedings of the 38th International Conference on Machine Learning},
  author = {Izmailov, Pavel and Vikram, Sharad and Hoffman, Matthew D and Wilson, Andrew Gordon Gordon},
  year = {2021},
  month = jul,
  series = {Proceedings of Machine Learning Research},
  volume = {139},
  pages = {4629--4640},
  publisher = {{PMLR}},
  abstract = {The posterior over Bayesian neural network (BNN) parameters is extremely high-dimensional and non-convex. For computational reasons, researchers approximate this posterior using inexpensive mini-batch methods such as mean-field variational inference or stochastic-gradient Markov chain Monte Carlo (SGMCMC). To investigate foundational questions in Bayesian deep learning, we instead use full batch Hamiltonian Monte Carlo (HMC) on modern architectures. We show that (1) BNNs can achieve significant performance gains over standard training and deep ensembles; (2) a single long HMC chain can provide a comparable representation of the posterior to multiple shorter chains; (3) in contrast to recent studies, we find posterior tempering is not needed for near-optimal performance, with little evidence for a ``cold posterior'' effect, which we show is largely an artifact of data augmentation; (4) BMA performance is robust to the choice of prior scale, and relatively similar for diagonal Gaussian, mixture of Gaussian, and logistic priors; (5) Bayesian neural networks show surprisingly poor generalization under domain shift; (6) while cheaper alternatives such as deep ensembles and SGMCMC can provide good generalization, their predictive distributions are distinct from HMC. Notably, deep ensemble predictive distributions are similarly close to HMC as standard SGLD, and closer than standard variational inference.},
  pdf = {http://proceedings.mlr.press/v139/izmailov21a/izmailov21a.pdf}
}

@article{jackson_electrostrictive_2019,
  title = {Electrostrictive {{Cavitation}} in {{Water Induced}} by a {{SnO2 Nanoparticle}}},
  author = {Jackson, Shane and Nakano, Aiichiro and Vashishta, Priya and Kalia, Rajiv K.},
  year = {2019},
  month = dec,
  journal = {ACS Omega},
  volume = {4},
  number = {27},
  pages = {22274--22279},
  langid = {english},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\HCCPA52N\\Jackson et al. - 2019 - Electrostrictive Cavitation in Water Induced by a .pdf}
}

@article{jacob_unbiased_2020,
  title = {Unbiased {{Markov}} Chain {{Monte Carlo}} Methods with Couplings},
  author = {Jacob, Pierre E. and O'Leary, John and Atchad{\'e}, Yves F.},
  year = {2020},
  month = jul,
  journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
  volume = {82},
  number = {3},
  pages = {543--600},
  langid = {english},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\SBUQB749\\Jacob et al. - 2020 - Unbiased Markov chain Monte Carlo methods with cou.pdf}
}

@article{jacob_using_2011,
  title = {Using Parallel Computation to Improve Independent {{Metropolis}}\textendash{{Hastings}} Based Estimation},
  author = {Jacob, P. and Robert, C. P. and Smith, M. H.},
  year = {2011},
  month = jan,
  journal = {Journal of Computational and Graphical Statistics},
  volume = {20},
  number = {3},
  pages = {616--635},
  langid = {english},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\5S5JSC42\\Jacob et al. - 2011 - Using Parallel Computation to Improve Independent .pdf}
}

@article{jacobs_sparse_2018,
  title = {Sparse {{Bayesian}} Nonlinear System Identification Using Variational Inference},
  author = {Jacobs, William R. and Baldacchino, Tara and Dodd, Tony and Anderson, Sean R.},
  year = {2018},
  month = dec,
  journal = {IEEE Transactions on Automatic Control},
  volume = {63},
  number = {12},
  pages = {4172--4187},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\4T6KUV9V\\Jacobs et al. - 2018 - Sparse Bayesian Nonlinear System Identification Us.pdf}
}

@article{jalko_differentially_,
  title = {Differentially {{Private Variational Inference}} for {{Non-conjugate Models}}},
  author = {Jalko, Joonas and Dikmen, Onur and Honkela, Antti},
  pages = {10},
  langid = {english},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\ESHBQ56Y\\Jalko et al. - Differentially Private Variational Inference for N.pdf}
}

@article{jalko_differentially_2017,
  title = {Differentially Private Variational Inference for Non-Conjugate Models},
  author = {J{\"a}lk{\"o}, Joonas and Dikmen, Onur and Honkela, Antti},
  year = {2017},
  month = apr,
  journal = {Proceedings of the Conference on Uncertainty in Artificial Intelligence},
  eprint = {1610.08749},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  abstract = {Many machine learning applications are based on data collected from people, such as their tastes and behaviour as well as biological traits and genetic data. Regardless of how important the application might be, one has to make sure individuals' identities or the privacy of the data are not compromised in the analysis. Differential privacy constitutes a powerful framework that prevents breaching of data subject privacy from the output of a computation. Differentially private versions of many important Bayesian inference methods have been proposed, but there is a lack of an efficient unified approach applicable to arbitrary models. In this contribution, we propose a differentially private variational inference method with a very wide applicability. It is built on top of doubly stochastic variational inference, a recent advance which provides a variational solution to a large class of models. We add differential privacy into doubly stochastic variational inference by clipping and perturbing the gradients. The algorithm is made more efficient through privacy amplification from subsampling. We demonstrate the method can reach an accuracy close to non-private level under reasonably strong privacy guarantees, clearly improving over previous sampling-based alternatives especially in the strong privacy regime.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Cryptography and Security,Computer Science - Machine Learning,Statistics - Machine Learning,Statistics - Methodology},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\QQIEG3F8\\Jlk et al. - 2017 - Differentially Private Variational Inference for N.pdf;C\:\\Users\\msca8h\\Zotero\\storage\\FD5237BP\\1610.html}
}

@incollection{jamil_test_2013,
  title = {Test {{Functions}} for {{Global Optimization}}},
  booktitle = {Swarm {{Intelligence}} and {{Bio-Inspired Computation}}},
  author = {Jamil, Momin and Yang, Xin-She and Zepernick, Hans-J{\"u}rgen},
  year = {2013},
  pages = {193--222},
  publisher = {{Elsevier}},
  langid = {english}
}

@article{jarner_necessary_2003,
  title = {Necessary Conditions for Geometric and Polynomial Ergodicity of Random-Walk-Type},
  author = {Jarner, S{\o}ren F. and Tweedie, Richard L.},
  year = {2003},
  month = aug,
  journal = {Bernoulli},
  volume = {9},
  number = {4},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\Z93EL43K\\Jarner and Tweedie - 2003 - Necessary conditions for geometric and polynomial .pdf}
}

@article{jasra_inference_2011,
  title = {Inference for {{L\'evy-Driven Stochastic Volatility Models}} via {{Adaptive Sequential Monte Carlo}}: {{L\'evy-driven}} Stochastic Volatility},
  shorttitle = {Inference for {{L\'evy-Driven Stochastic Volatility Models}} via {{Adaptive Sequential Monte Carlo}}},
  author = {Jasra, Ajay and Stephens, David A. and Doucet, Arnaud and Tsagaris, Theodoros},
  year = {2011},
  month = mar,
  journal = {Scandinavian Journal of Statistics},
  volume = {38},
  number = {1},
  pages = {1--22},
  langid = {english}
}

@article{jasra_populationbased_2007,
  title = {On Population-Based Simulation for Static Inference},
  author = {Jasra, Ajay and Stephens, David A. and Holmes, Christopher C.},
  year = {2007},
  month = aug,
  journal = {Statistics and Computing},
  volume = {17},
  number = {3},
  pages = {263--279},
  langid = {english}
}

@article{jasra_populationbased_2007a,
  title = {Population-Based Reversible Jump {{Markov}} Chain {{Monte Carlo}}},
  author = {Jasra, A. and Stephens, D. A. and Holmes, C. C.},
  year = {2007},
  month = aug,
  journal = {Biometrika},
  volume = {94},
  number = {4},
  pages = {787--807},
  langid = {english},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\YKHUMFV9\\Jasra et al. - 2007 - Population-Based Reversible Jump Markov Chain Mont.pdf}
}

@book{jensen_computational_2011,
  title = {Computational Ocean Acoustics},
  author = {Jensen, Finn B. and Kuperman, William A. and Porter, Michael B. and Schmidt, Henrik},
  year = {2011},
  publisher = {{Springer New York}},
  address = {{New York, NY}},
  langid = {english},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\2F5FYG5W\\Jensen et al. - 2011 - Computational Ocean Acoustics.pdf}
}

@article{ji_parallelizing_2019,
  title = {Parallelizing {{Word2Vec}} in {{Shared}} and {{Distributed Memory}}},
  author = {Ji, Shihao and Satish, Nadathur and Li, Sheng and Dubey, Pradeep},
  year = {2019},
  journal = {IEEE Transactions on Parallel and Distributed Systems},
  pages = {1--1},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\J6XQ383P\\Ji et al. - 2019 - Parallelizing Word2Vec in Shared and Distributed M.pdf}
}

@inproceedings{jia_autotuning_2016,
  title = {Auto-Tuning {{Spark Big Data Workloads}} on {{POWER8}}: {{Prediction-Based Dynamic SMT Threading}}},
  shorttitle = {Auto-Tuning {{Spark Big Data Workloads}} on {{POWER8}}},
  booktitle = {Proceedings of the 2016 {{International Conference}} on {{Parallel Architectures}} and {{Compilation}} - {{PACT}} '16},
  author = {Jia, Zhen and Xue, Chao and Chen, Guancheng and Zhan, Jianfeng and Zhang, Lixin and Lin, Yonghua and Hofstee, Peter},
  year = {2016},
  pages = {387--400},
  publisher = {{ACM Press}},
  address = {{Haifa, Israel}},
  langid = {english}
}

@inproceedings{jiang_fantastic_2020,
  title = {Fantastic {{Generalization Measures}} and {{Where}} to {{Find Them}}},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {Jiang, Yiding and Neyshabur*, Behnam and Mobahi, Hossein and Krishnan, Dilip and Bengio, Samy},
  year = {2020}
}

@incollection{jiang_linear_2018,
  title = {A {{Linear Speedup Analysis}} of {{Distributed Deep Learning}} with {{Sparse}} and {{Quantized Communication}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 31},
  author = {Jiang, Peng and Agrawal, Gagan},
  year = {2018},
  pages = {2525--2536},
  publisher = {{Curran Associates, Inc.}}
}

@article{jiang_mcmc_2021,
  title = {{{MCMC}} Confidence Intervals and Biases},
  author = {Jiang, Yu Hang and Liu, Tong and Lou, Zhiya and Rosenthal, Jeffrey S. and Shangguan, Shanshan and Wang, Fei and Wu, Zixuan},
  year = {2021},
  month = jun,
  journal = {arXiv:2012.02816 [math, stat]},
  eprint = {2012.02816},
  eprinttype = {arxiv},
  primaryclass = {math, stat},
  abstract = {The recent paper "Simple confidence intervals for MCMC without CLTs" by J.S. Rosenthal, showed the derivation of a simple MCMC confidence interval using only Chebyshev's inequality, not CLT. That result required certain assumptions about how the estimator bias and variance grow with the number of iterations \$n\$. In particular, the bias is \$o(1/\textbackslash sqrt\{n\})\$. This assumption seemed mild. It is generally believed that the estimator bias will be \$O(1/n)\$ and hence \$o(1/\textbackslash sqrt\{n\})\$. However, questions were raised by researchers about how to verify this assumption. Indeed, we show that this assumption might not always hold. In this paper, we seek to simplify and weaken the assumptions in the previously mentioned paper, to make MCMC confidence intervals without CLTs more widely applicable.},
  archiveprefix = {arXiv},
  keywords = {60J10; 62E20,Mathematics - Statistics Theory},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\6HAE5WUU\\Jiang et al. - 2021 - MCMC Confidence Intervals and Biases.pdf;C\:\\Users\\msca8h\\Zotero\\storage\\X7FK8STD\\2012.html}
}

@article{jiang_robust_2014,
  title = {Robust Beamforming by Linear Programming},
  author = {Jiang, Xue and Zeng, Wen-Jun and Yasotharan, A. and So, Hing Cheung and Kirubarajan, Thiagalingam},
  year = {2014},
  month = apr,
  journal = {IEEE Transactions on Signal Processing},
  volume = {62},
  number = {7},
  pages = {1834--1849}
}

@article{jianli_adaptive_1996,
  title = {An Adaptive Filtering Approach to Spectral Estimation and {{SAR}} Imaging},
  author = {{Jian Li} and Stoica, P.},
  year = {1996},
  month = jun,
  journal = {IEEE Transactions on Signal Processing},
  volume = {44},
  number = {6},
  pages = {1469--1484}
}

@misc{jin_convergence_2020,
  title = {On the Convergence of First Order Methods for Quasar-Convex Optimization},
  author = {Jin, Jikai},
  year = {2020},
  month = oct,
  number = {arXiv:2010.04937},
  eprint = {2010.04937},
  eprinttype = {arxiv},
  primaryclass = {cs, math, stat},
  institution = {{arXiv}},
  abstract = {In recent years, the success of deep learning has inspired many researchers to study the optimization of general smooth non-convex functions. However, recent works have established pessimistic worst-case complexities for this class functions, which is in stark contrast with their superior performance in real-world applications (e.g. training deep neural networks). On the other hand, it is found that many popular non-convex optimization problems enjoy certain structured properties which bear some similarities to convexity. In this paper, we study the class of \textbackslash textit\{quasar-convex functions\} to close the gap between theory and practice. We study the convergence of first order methods in a variety of different settings and under different optimality criterions. We prove complexity upper bounds that are similar to standard results established for convex functions and much better that state-of-the-art convergence rates of non-convex functions. Overall, this paper suggests that \textbackslash textit\{quasar-convexity\} allows efficient optimization procedures, and we are looking forward to seeing more problems that demonstrate similar properties in practice.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Mathematics - Optimization and Control,Statistics - Machine Learning},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\KUCJSRAA\\Jin - 2020 - On The Convergence of First Order Methods for Quas.pdf;C\:\\Users\\msca8h\\Zotero\\storage\\A4D8GQQU\\2010.html}
}

@article{jin_openmp_1999,
  title = {The {{OpenMP}} Implementation of {{NAS}} Parallel Benchmarks and Its Performance},
  author = {Jin, Hao-Qiang and Frumkin, Michael and Yan, Jerry},
  year = {1999}
}

@inproceedings{jinbumkang_new_2014,
  title = {A New Feature-Enhanced Speckle Reduction Method Based on Multiscale Analysis and Synthesis for Ultrasound {{B-mode}} Imaging},
  booktitle = {2014 {{IEEE International Ultrasonics Symposium}}},
  author = {{Jinbum Kang} and Yoo, Yangmo},
  year = {2014},
  month = sep,
  pages = {1320--1323},
  publisher = {{IEEE}},
  address = {{Chicago, IL, USA}}
}

@article{JMLR:v15:hoffman14a,
  title = {The No-u-Turn Sampler: Adaptively Setting Path Lengths in {{Hamiltonian Monte Carlo}}},
  author = {Hoffman, Matthew D. and Gelman, Andrew},
  year = {2014},
  journal = {Journal of Machine Learning Research},
  volume = {15},
  number = {47},
  pages = {1593--1623}
}

@article{JMLR:v15:nishihara14a,
  title = {Parallel {{MCMC}} with Generalized Elliptical Slice Sampling},
  author = {Nishihara, Robert and Murray, Iain and Adams, Ryan P.},
  year = {2014},
  journal = {Journal of Machine Learning Research},
  volume = {15},
  number = {61},
  pages = {2087--2112}
}

@article{JMLR:v18:15-205,
  title = {On {{Markov}} Chain {{Monte Carlo}} Methods for Tall Data},
  author = {Bardenet, R{\'e}mi and Doucet, Arnaud and Holmes, Chris},
  year = {2017},
  journal = {Journal of Machine Learning Research},
  volume = {18},
  number = {47},
  pages = {1--43}
}

@article{JMLR:v18:15-205,
  title = {On {{Markov}} Chain {{Monte Carlo}} Methods for Tall Data},
  author = {Bardenet, R{\'e}mi and Doucet, Arnaud and Holmes, Chris},
  year = {2017},
  journal = {Journal of Machine Learning Research},
  volume = {18},
  number = {47},
  pages = {1--43}
}

@article{JMLR:v19:17-084,
  title = {Scalable {{Bayes}} via Barycenter in {{Wasserstein}} Space},
  author = {Srivastava, Sanvesh and Li, Cheng and Dunson, David B.},
  year = {2018},
  journal = {Journal of Machine Learning Research},
  volume = {19},
  number = {8},
  pages = {1--35}
}

@article{JMLR:v20:17-757,
  title = {Dependent Relevance Determination for Smooth and Structured Sparse Regression},
  author = {Wu, Anqi and Koyejo, Oluwasanmi and Pillow, Jonathan},
  year = {2019},
  journal = {Journal of Machine Learning Research},
  volume = {20},
  number = {89},
  pages = {1--43}
}

@article{JMLR:v20:18-403,
  title = {Pyro: {{Deep}} Universal Probabilistic Programming},
  author = {Bingham, Eli and Chen, Jonathan P. and Jankowiak, Martin and Obermeyer, Fritz and Pradhan, Neeraj and Karaletsos, Theofanis and Singh, Rohit and Szerlip, Paul and Horsfall, Paul and Goodman, Noah D.},
  year = {2019},
  journal = {Journal of Machine Learning Research},
  volume = {20},
  number = {28},
  pages = {1--6}
}

@article{JMLR:v20:19-306,
  title = {Log-Concave Sampling: {{Metropolis-Hastings}} Algorithms Are Fast},
  author = {Dwivedi, Raaz and Chen, Yuansi and Wainwright, Martin J. and Yu, Bin},
  year = {2019},
  journal = {Journal of Machine Learning Research},
  volume = {20},
  number = {183},
  pages = {1--42}
}

@article{JMLR:v7:demsar06a,
  title = {Statistical Comparisons of Classifiers over Multiple Data Sets},
  author = {Dem{\v s}ar, Janez},
  year = {2006},
  journal = {Journal of Machine Learning Research},
  volume = {7},
  number = {1},
  pages = {1--30}
}

@inproceedings{johannsson_imaging_2010,
  title = {Imaging Sonar-Aided Navigation for Autonomous Underwater Harbor Surveillance},
  booktitle = {Proceedings of the {{IEEE}}/{{RSJ International Conference}} on {{Intelligent Robots}} and {{Systems}}},
  author = {Johannsson, H and Kaess, M and Englot, B and Hover, F and Leonard, J},
  year = {2010},
  month = oct,
  series = {{{IROS}}'10},
  pages = {4396--4403},
  publisher = {{IEEE}},
  address = {{Taipei}},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\N96N5GQP\\Johannsson et al. - 2010 - Imaging sonar-aided navigation for autonomous unde.pdf}
}

@misc{johndrow_no_2020,
  title = {No Free Lunch for Approximate {{MCMC}}},
  author = {Johndrow, James E. and Pillai, Natesh S. and Smith, Aaron},
  year = {2020},
  month = oct,
  number = {arXiv:2010.12514},
  eprint = {2010.12514},
  eprinttype = {arxiv},
  primaryclass = {math, stat},
  institution = {{arXiv}},
  abstract = {It is widely known that the performance of Markov chain Monte Carlo (MCMC) can degrade quickly when targeting computationally expensive posterior distributions, such as when the sample size is large. This has motivated the search for MCMC variants that scale well to large datasets. One general approach has been to look at only a subsample of the data at every step. In this note, we point out that well-known MCMC convergence results often imply that these "subsampling" MCMC algorithms cannot greatly improve performance. We apply these generic results to realistic statistical problems and proposed algorithms, and also discuss some design principles suggested by the results.},
  archiveprefix = {arXiv},
  keywords = {Mathematics - Statistics Theory,Statistics - Computation},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\62EWPR75\\Johndrow et al. - 2020 - No Free Lunch for Approximate MCMC.pdf;C\:\\Users\\msca8h\\Zotero\\storage\\XIR4U8JN\\2010.html}
}

@misc{johndrow_optimal_2017,
  title = {Optimal Approximating {{Markov}} Chains for {{Bayesian}} Inference},
  author = {Johndrow, James E. and Mattingly, Jonathan C. and Mukherjee, Sayan and Dunson, David},
  year = {2017},
  month = aug,
  number = {arXiv:1508.03387},
  eprint = {1508.03387},
  eprinttype = {arxiv},
  primaryclass = {math, stat},
  institution = {{arXiv}},
  abstract = {The Markov Chain Monte Carlo method is the dominant paradigm for posterior computation in Bayesian analysis. It is common to control computation time by making approximations to the Markov transition kernel. Comparatively little attention has been paid to computational optimality in these approximating Markov Chains, or when such approximations are justified relative to obtaining shorter paths from the exact kernel. We give simple, sharp bounds for uniform approximations of uniformly mixing Markov chains. We then suggest a notion of optimality that incorporates computation time and approximation error, and use our bounds to make generalizations about properties of good approximations in the uniformly mixing setting. The relevance of these properties is demonstrated in applications to a minibatching-based approximate MCMC algorithm for large \$n\$ logistic regression and low-rank approximations for Gaussian processes.},
  archiveprefix = {arXiv},
  keywords = {Mathematics - Statistics Theory,Statistics - Computation},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\FYJ8SS9B\\Johndrow et al. - 2017 - Optimal approximating Markov chains for Bayesian i.pdf;C\:\\Users\\msca8h\\Zotero\\storage\\FNIF3SBW\\1508.html}
}

@book{johnson_nlopt_2011,
  title = {The {{NLopt}} Nonlinear-Optimization Package},
  author = {Johnson, Steven G.},
  year = {2011},
  keywords = {imported}
}

@article{jones_efficient_1998,
  title = {Efficient Global Optimization of Expensive Black-Box Functions},
  author = {Jones, Donald R. and Schonlau, Matthias and Welch, William J.},
  year = {1998},
  journal = {Journal of Global Optimization},
  volume = {13},
  number = {4},
  pages = {455--492}
}

@article{jones_lipschitzian_1993,
  title = {Lipschitzian Optimization without the {{Lipschitz}} Constant},
  author = {Jones, D. R. and Perttunen, C. D. and Stuckman, B. E.},
  year = {1993},
  month = oct,
  journal = {Journal of Optimization Theory and Applications},
  volume = {79},
  number = {1},
  pages = {157--181},
  langid = {english}
}

@article{jones_remark_1984,
  title = {Remark {{AS R50}}: {{A Remark}} on {{Algorithm AS}} 176. {{Kernal Density Estimation Using}} the {{Fast Fourier Transform}}},
  shorttitle = {Remark {{AS R50}}},
  author = {Jones, M. C. and Lotwick, H. W.},
  year = {1984},
  journal = {Applied Statistics},
  volume = {33},
  number = {1},
  pages = {120}
}

@article{jordan_introduction_1999,
  title = {An Introduction to Variational Methods for Graphical Models},
  author = {Jordan, Michael I. and Ghahramani, Zoubin and Jaakkola, Tommi S. and Saul, Lawrence K.},
  year = {1999},
  journal = {Machine Learning},
  volume = {37},
  number = {2},
  pages = {183--233},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\IG37GFMG\\Jordan et al. - 1999 - [No title found].pdf}
}

@inproceedings{jordan_multiobjective_2012,
  title = {A Multi-Objective Auto-Tuning Framework for Parallel Codes},
  booktitle = {2012 {{International Conference}} for {{High Performance Computing}}, {{Networking}}, {{Storage}} and {{Analysis}}},
  author = {Jordan, Herbert and Thoman, Peter and Durillo, Juan J. and Pellegrini, Simone and Gschwandtner, Philipp and Fahringer, Thomas and Moritsch, Hans},
  year = {2012},
  month = nov,
  pages = {1--12},
  publisher = {{IEEE}},
  address = {{Salt Lake City, UT}}
}

@inproceedings{jordan_multiobjective_2012a,
  title = {A Multi-Objective Auto-Tuning Framework for Parallel Codes},
  booktitle = {2012 {{International Conference}} for {{High Performance Computing}}, {{Networking}}, {{Storage}} and {{Analysis}}},
  author = {Jordan, Herbert and Thoman, Peter and Durillo, Juan J. and Pellegrini, Simone and Gschwandtner, Philipp and Fahringer, Thomas and Moritsch, Hans},
  year = {2012},
  month = nov,
  pages = {1--12},
  publisher = {{IEEE}},
  address = {{Salt Lake City, UT}}
}

@article{jospin_handson_2020,
  title = {Hands-on {{Bayesian}} Neural Networks -- a Tutorial for Deep Learning Users},
  author = {Jospin, Laurent Valentin and Buntine, Wray and Boussaid, Farid and Laga, Hamid and Bennamoun, Mohammed},
  year = {2020},
  month = jul,
  journal = {arXiv:2007.06823 [cs, stat]},
  eprint = {2007.06823},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Modern deep learning methods have equipped researchers and engineers with incredibly powerful tools to tackle problems that previously seemed impossible. However, since deep learning methods operate as black boxes, the uncertainty associated with their predictions is often challenging to quantify. Bayesian statistics offer a formalism to understand and quantify the uncertainty associated with deep neural networks predictions. This paper provides a tutorial for researchers and scientists who are using machine learning, especially deep learning, with an overview of the relevant literature and a complete toolset to design, implement, train, use and evaluate Bayesian neural networks.},
  archiveprefix = {arXiv},
  keywords = {62-02 (Primary),Computer Science - Machine Learning,G.3,I.2.6,Statistics - Machine Learning},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\MMVZ4S4W\\Jospin et al. - 2020 - Hands-on Bayesian Neural Networks -- a Tutorial fo.pdf;C\:\\Users\\msca8h\\Zotero\\storage\\QWAVED58\\2007.html}
}

@article{kadav_asap_2016,
  title = {{{ASAP}}: {{Asynchronous Approximate Data-Parallel Computation}}},
  shorttitle = {{{ASAP}}},
  author = {Kadav, Asim and Kruus, Erik},
  year = {2016},
  month = dec,
  journal = {arXiv:1612.08608 [cs]},
  eprint = {1612.08608},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Emerging workloads, such as graph processing and machine learning are approximate because of the scale of data involved and the stochastic nature of the underlying algorithms. These algorithms are often distributed over multiple machines using bulk-synchronous processing (BSP) or other synchronous processing paradigms such as map-reduce. However, data parallel processing primitives such as repeated barrier and reduce operations introduce high synchronization overheads. Hence, many existing data-processing platforms use asynchrony and staleness to improve data-parallel job performance. Often, these systems simply change the synchronous communication to asynchronous between the worker nodes in the cluster. This improves the throughput of data processing but results in poor accuracy of the final output since different workers may progress at different speeds and process inconsistent intermediate outputs. In this paper, we present ASAP, a model that provides asynchronous and approximate processing semantics for data-parallel computation. ASAP provides fine-grained worker synchronization using NOTIFY-ACK semantics that allows independent workers to run asynchronously. ASAP also provides stochastic reduce that provides approximate but guaranteed convergence to the same result as an aggregated all-reduce. In our results, we show that ASAP can reduce synchronization costs and provides 2-10X speedups in convergence and up to 10X savings in network costs for distributed machine learning applications and provides strong convergence guarantees.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Distributed; Parallel; and Cluster Computing,Computer Science - Machine Learning},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\5L8QTER2\\Kadav and Kruus - 2016 - ASAP Asynchronous Approximate Data-Parallel Compu.pdf;C\:\\Users\\msca8h\\Zotero\\storage\\YNYDDIFR\\1612.html}
}

@article{kadupitiya_machine_2020,
  title = {Machine Learning for Parameter Auto-Tuning in Molecular Dynamics Simulations: {{Efficient}} Dynamics of Ions near Polarizable Nanoparticles},
  shorttitle = {Machine Learning for Parameter Auto-Tuning in Molecular Dynamics Simulations},
  author = {Kadupitiya, Jcs and Fox, Geoffrey C and Jadhao, Vikram},
  year = {2020},
  month = jan,
  journal = {The International Journal of High Performance Computing Applications},
  pages = {109434201989945},
  abstract = {Simulating the dynamics of ions near polarizable nanoparticles (NPs) using coarse-grained models is extremely challenging due to the need to solve the Poisson equation at every simulation timestep. Recently, a molecular dynamics (MD) method based on a dynamical optimization framework bypassed this obstacle by representing the polarization charge density as virtual dynamic variables and evolving them in parallel with the physical dynamics of ions. We highlight the computational gains accessible with the integration of machine learning (ML) methods for parameter prediction in MD simulations by demonstrating how they were realized in MD simulations of ions near polarizable NPs. An artificial neural network\textendash based regression model was integrated with MD simulation and predicted the optimal simulation timestep and optimization parameters characterizing the virtual system with 94.3\% success. The ML-enabled auto-tuning of parameters generated accurate dynamics of ions for {$\approx$} 10 million steps while improving the stability of the simulation by over an order of magnitude. The integration of ML-enhanced framework with hybrid Open Multi-Processing / Message Passing Interface (OpenMP/MPI) parallelization techniques reduced the computational time of simulating systems with thousands of ions and induced charges from thousands of hours to tens of hours, yielding a maximum speedup of {$\approx$} 3 from ML-only acceleration and a maximum speedup of {$\approx$} 600 from the combination of ML and parallel computing methods. Extraction of ionic structure in concentrated electrolytes near oil\textendash water emulsions demonstrates the success of the method. The approach can be generalized to select optimal parameters in other MD applications and energy minimization problems.},
  langid = {english},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\Z8NXCT6I\\Kadupitiya et al. - 2020 - Machine learning for parameter auto-tuning in mole.pdf}
}

@article{kajiya_rendering_1986,
  title = {The Rendering Equation},
  author = {Kajiya, James T.},
  year = {1986},
  month = aug,
  journal = {ACM SIGGRAPH Computer Graphics},
  volume = {20},
  number = {4},
  pages = {143--150},
  abstract = {We present an integral equation which generalizes a variety of known rendering algorithms. In the course of discussing a monte carlo solution we also present a new form of variance reduction, called Hierarchical sampling and give a number of elaborations shows that it may be an efficient new technique for a wide variety of monte carlo procedures. The resulting rendering algorithm extends the range of optical phenomena which can be effectively simulated.},
  langid = {english}
}

@inproceedings{kaleem_adaptive_2014,
  title = {Adaptive Heterogeneous Scheduling for Integrated {{GPUs}}},
  booktitle = {2014 23rd {{International Conference}} on {{Parallel Architecture}} and {{Compilation Techniques}} ({{PACT}})},
  author = {Kaleem, R. and Barik, R. and Shpeisman, T. and Hu, C. and Lewis, B. T. and Pingali, K.},
  year = {2014},
  month = aug,
  pages = {151--162},
  keywords = {adaptive heterogeneous scheduling,asymmetric scheduling algorithm,C++ languages,CPU-GPU communication,graphics processing units,Graphics processing units,Heterogeneous computing,integrated CPU-GPU processors,integrated GPUs,Intel 4th generation core processor,Irregular applications,Kernel,load balancing,microprocessor chips,NVIDIA discrete GPU,online profiling-based scheduling algorithms,processor scheduling,Programming,scheduling,Scheduling algorithms}
}

@article{kalinnik_online_2014,
  title = {Online Auto-Tuning for the Time-Step-Based Parallel Solution of {{ODEs}} on Shared-Memory Systems},
  author = {Kalinnik, Natalia and Korch, Matthias and Rauber, Thomas},
  year = {2014},
  month = aug,
  journal = {Journal of Parallel and Distributed Computing},
  volume = {74},
  number = {8},
  pages = {2722--2744},
  langid = {english}
}

@inproceedings{kamil_autotuning_2010,
  title = {An Auto-Tuning Framework for Parallel Multicore Stencil Computations},
  booktitle = {2010 {{IEEE International Symposium}} on {{Parallel}} \& {{Distributed Processing}} ({{IPDPS}})},
  author = {Kamil, Shoaib and Chan, Cy and Oliker, Leonid and Shalf, John and Williams, Samuel},
  year = {2010},
  pages = {1--12},
  publisher = {{IEEE}},
  address = {{Atlanta, GA, USA}},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\HNYEDUZ2\\Kamil et al. - 2010 - An auto-tuning framework for parallel multicore st.pdf}
}

@incollection{kamp_efficient_2019,
  title = {Efficient {{Decentralized Deep Learning}} by {{Dynamic Model Averaging}}},
  booktitle = {Machine {{Learning}} and {{Knowledge Discovery}} in {{Databases}}},
  author = {Kamp, Michael and Adilova, Linara and Sicking, Joachim and H{\"u}ger, Fabian and Schlicht, Peter and Wirtz, Tim and Wrobel, Stefan},
  year = {2019},
  volume = {11051},
  pages = {393--409},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  langid = {english},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\PGYVH8C2\\Kamp et al. - 2019 - Efficient Decentralized Deep Learning by Dynamic M.pdf}
}

@inproceedings{kandasamy_high_2015,
  title = {High {{Dimensional Bayesian Optimisation}} and {{Bandits}} via {{Additive Models}}},
  booktitle = {Proceedings of the 32nd {{International Conference}} on {{International Conference}} on {{Machine Learning}} - {{Volume}} 37},
  author = {Kandasamy, Kirthevasan and Schneider, Jeff and P{\'o}czos, Barnab{\'a}s},
  year = {2015},
  series = {{{ICML}}'15},
  pages = {295--304},
  publisher = {{JMLR.org}}
}

@article{kang_new_2016,
  title = {A New Feature-Enhanced Speckle Reduction Method Based on Multiscale Analysis for Ultrasound {{B-mode}} Imaging},
  author = {Kang, Jinbum and Lee, Jae Young and Yoo, Yangmo},
  year = {2016},
  month = jun,
  journal = {IEEE Transactions on Biomedical Engineering},
  volume = {63},
  number = {6},
  pages = {1178--1191}
}

@mastersthesis{kaouri_leftright_2000,
  title = {Left-Right Ambiguity Resolution of a Towed Array Sonar},
  author = {Kaouri, Katerina},
  year = {2000},
  school = {University of Oxford},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\XUZKR4R9\\Kaouri - 2000 - Left-right ambiguity resolution of a towed array s.pdf}
}

@mastersthesis{kapfer_advanced_2005,
  title = {An Advanced Specular and Diffuse Bidirectional Reflectance Distribution Function Target Model for a Synthetic Aperture Ground Penetrating Radar},
  author = {Kapfer, Robert M.},
  year = {2005},
  month = jan,
  address = {{Rochester Institute of Technology}},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\KIQLEFKG\\Kapfer - 2005 - An advanced specular and diffuse bidirectional ref.pdf}
}

@article{kaplanberkaya_survey_2018,
  title = {A Survey on {{ECG}} Analysis},
  author = {Kaplan Berkaya, Selcan and Uysal, Alper Kursat and Sora Gunal, Efnan and Ergin, Semih and Gunal, Serkan and Gulmezoglu, M. Bilginer},
  year = {2018},
  month = may,
  journal = {Biomedical Signal Processing and Control},
  volume = {43},
  pages = {216--235},
  langid = {english}
}

@phdthesis{kapralos_sonel_2006,
  type = {Doctoral {{Thesis}}},
  title = {The Sonel Mapping Acoustical Modeling Method},
  author = {Kapralos, Bill},
  year = {2006},
  month = sep,
  address = {{Toronto, Ontario, Canada}},
  school = {York University},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\E4HTNDJR\\Kapralos - 2006 - The sonel mapping acoustical modeling method.pdf}
}

@article{kapralos_sonel_2008,
  title = {Sonel Mapping: {{A}} Probabilistic Acoustical Modeling Method},
  author = {Kapralos, Bill and Jenkin, Michael and Milios, Evangelos},
  year = {2008},
  journal = {Building Acoustics},
  volume = {15},
  number = {4},
  pages = {289--313},
  abstract = {Sonel mapping is a Monte-Carlo-based acoustical modeling technique that approximates the acoustics of an environment while accounting for diffuse and specular reflections as well as diffraction effects. Through the use of a probabilistic Russian roulette strategy to determine the type of interaction between a sound and any objects/surfaces it may encounter, sonel mapping avoids excessively large running times in contrast to deterministic techniques. Sonel mapping approximates many of the subtle interaction effects required for realistic acoustical modeling yet due to its probabilistic nature, can be incorporated into interactive virtual environments where accuracy is often substituted for efficiency. Experimental results demonstrate the efficacy of the approach.},
  annotation = {\_eprint: https://doi.org/10.1260/135101008786939973}
}

@article{karacan_structurepreserving_2013,
  title = {Structure-Preserving Image Smoothing via Region Covariances},
  author = {Karacan, Levent and Erdem, Erkut and Erdem, Aykut},
  year = {2013},
  month = nov,
  journal = {ACM Transactions on Graphics},
  volume = {32},
  number = {6},
  pages = {1--11},
  abstract = {Recent years have witnessed the emergence of new image smoothing techniques which have provided new insights and raised new questions about the nature of this well-studied problem. Specifically, these models separate a given image into its structure and texture layers by utilizing non-gradient based definitions for edges or special measures that distinguish edges from oscillations. In this study, we propose an alternative yet simple image smoothing approach which depends on covariance matrices of simple image features, aka the region covariances. The use of second order statistics as a patch descriptor allows us to implicitly capture local structure and texture information and makes our approach particularly effective for structure extraction from texture. Our experimental results have shown that the proposed approach leads to better image decompositions as compared to the state-of-the-art methods and preserves prominent edges and shading well. Moreover, we also demonstrate the applicability of our approach on some image editing and manipulation tasks such as image abstraction, texture and detail enhancement, image composition, inverse halftoning and seam carving.},
  langid = {english}
}

@article{karagiannis_annealed_2013,
  title = {Annealed Importance Sampling Reversible Jump {{MCMC}} Algorithms},
  author = {Karagiannis, Georgios and Andrieu, Christophe},
  year = {2013},
  month = jul,
  journal = {Journal of Computational and Graphical Statistics},
  volume = {22},
  number = {3},
  pages = {623--648},
  langid = {english},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\YE7D2DVD\\Karagiannis and Andrieu - 2013 - Annealed Importance Sampling Reversible Jump MCMC .pdf}
}

@article{karagiannis_annealed_2013a,
  title = {Annealed {{Importance Sampling Reversible Jump MCMC Algorithms}}},
  author = {Karagiannis, Georgios and Andrieu, Christophe},
  year = {2013},
  month = jul,
  journal = {Journal of Computational and Graphical Statistics},
  volume = {22},
  number = {3},
  pages = {623--648},
  langid = {english}
}

@inproceedings{karimi_linear_2016,
  title = {Linear Convergence of Gradient and Proximal-Gradient Methods under the {{Polyak-\L ojasiewicz}} Condition},
  booktitle = {Machine {{Learning}} and {{Knowledge Discovery}} in {{Databases}}},
  author = {Karimi, Hamed and Nutini, Julie and Schmidt, Mark},
  year = {2016},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {795--811},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  abstract = {In 1963, Polyak proposed a simple condition that is sufficient to show a global linear convergence rate for gradient descent. This condition is a special case of the \L ojasiewicz inequality proposed in the same year, and it does not require strong convexity (or even convexity). In this work, we show that this much-older Polyak-\L ojasiewicz (PL) inequality is actually weaker than the main conditions that have been explored to show linear convergence rates without strong convexity over the last 25 years. We also use the PL inequality to give new analyses of coordinate descent and stochastic gradient for many non-strongly-convex (and some non-convex) functions. We further propose a generalization that applies to proximal-gradient methods for non-smooth optimization, leading to simple proofs of linear convergence for support vector machines and L1-regularized least squares without additional assumptions.},
  langid = {english},
  keywords = {Boosting,Coordinate descent,Gradient descent,L1-regularization,Stochastic gradient,Support vector machines,Variance-reduction},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\YNEAZY6I\\Karimi et al. - 2016 - Linear Convergence of Gradient and Proximal-Gradie.pdf}
}

@article{karnopp_random_1963,
  title = {Random Search Techniques for Optimization Problems},
  author = {Karnopp, Dean C.},
  year = {1963},
  month = aug,
  journal = {Automatica},
  volume = {1},
  number = {2-3},
  pages = {111--121},
  langid = {english}
}

@article{kass_bayes_1995,
  title = {Bayes Factors},
  author = {Kass, Robert E. and Raftery, Adrian E.},
  year = {1995},
  month = jun,
  journal = {Journal of the American Statistical Association},
  volume = {90},
  number = {430},
  pages = {773--795},
  publisher = {{Taylor \& Francis}},
  abstract = {In a 1935 paper and in his book Theory of Probability, Jeffreys developed a methodology for quantifying the evidence in favor of a scientific theory. The centerpiece was a number, now called the Bayes factor, which is the posterior odds of the null hypothesis when the prior probability on the null is one-half. Although there has been much discussion of Bayesian hypothesis testing in the context of criticism of P-values, less attention has been given to the Bayes factor as a practical tool of applied statistics. In this article we review and discuss the uses of Bayes factors in the context of five scientific applications in genetics, sports, ecology, sociology, and psychology. We emphasize the following points: \textbullet{} From Jeffreys' Bayesian viewpoint, the purpose of hypothesis testing is to evaluate the evidence in favor of a scientific theory. \textbullet{} Bayes factors offer a way of evaluating evidence in favor of a null hypothesis. \textbullet{} Bayes factors provide a way of incorporating external information into the evaluation of evidence about a hypothesis. \textbullet{} Bayes factors are very general and do not require alternative models to be nested. \textbullet{} Several techniques are available for computing Bayes factors, including asymptotic approximations that are easy to compute using the output from standard packages that maximize likelihoods. \textbullet{} In ``nonstandard'' statistical models that do not satisfy common regularity conditions, it can be technically simpler to calculate Bayes factors than to derive non-Bayesian significance tests. \textbullet{} The Schwarz criterion (or BIC) gives a rough approximation to the logarithm of the Bayes factor, which is easy to use and does not require evaluation of prior distributions. \textbullet{} When one is interested in estimation or prediction, Bayes factors may be converted to weights to be attached to various models so that a composite estimate or prediction may be obtained that takes account of structural or model uncertainty. \textbullet{} Algorithms have been proposed that allow model uncertainty to be taken into account when the class of models initially considered is very large. \textbullet{} Bayes factors are useful for guiding an evolutionary model-building process. \textbullet{} It is important, and feasible, to assess the sensitivity of conclusions to the prior distributions used.},
  keywords = {Bayesian hypothesis tests,BIC,Importance sampling,Laplace method,Markov chain Monte Carlo,Model selection,Monte Carlo integration,Posterior model probabilities,Posterior odds,Quadrature,Schwarz criterion,Sensitivity analysis,Strength of evidence},
  annotation = {\_eprint: https://www.tandfonline.com/doi/pdf/10.1080/01621459.1995.10476572}
}

@article{ke_robust_2017,
  title = {Robust Adaptive {{Beamforming}} Using Noise Reduction Preprocessing-Based Fully Automatic Diagonal Loading and Steering Vector Estimation},
  author = {Ke, Yuxuan and Zheng, Chengshi and Peng, Renhua and Li, Xiaodong},
  year = {2017},
  journal = {IEEE Access},
  volume = {5},
  pages = {12974--12987},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\D9SN57QM\\Ke et al. - 2017 - Robust Adaptive Beamforming Using Noise Reduction .pdf}
}

@article{keith_adaptive_2008,
  title = {Adaptive Independence Samplers},
  author = {Keith, Jonathan M. and Kroese, Dirk P. and Sofronov, George Y.},
  year = {2008},
  month = dec,
  journal = {Statistics and Computing},
  volume = {18},
  number = {4},
  pages = {409--420},
  langid = {english},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\HS8SJGPP\\Keith et al. - 2008 - Adaptive independence samplers.pdf}
}

@incollection{kejariwal_efficient_2006,
  title = {An Efficient Approach for Self-Scheduling Parallel Loops on Multiprogrammed Parallel Computers},
  booktitle = {Languages and {{Compilers}} for {{Parallel Computing}}},
  author = {Kejariwal, Arun and Nicolau, Alexandru and Polychronopoulos, Constantine D.},
  year = {2006},
  volume = {4339},
  pages = {441--449},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\96DDIPPN\\Kejariwal et al. - 2006 - An Efficient Approach for Self-scheduling Parallel.pdf}
}

@inproceedings{kejariwal_historyaware_2006,
  title = {History-Aware Self-Scheduling},
  booktitle = {Proc. {{Int}}. {{Conf}}. {{Parallel Process}}.},
  author = {Kejariwal, A. and Nicolau, A. and Polychronopoulos, C. D.},
  year = {2006},
  series = {{{ICPP}}'06},
  publisher = {{IEEE}},
  file = {/home/msca8h/Documents/parallel_scheduling/Kejariwal et al. - 2006 - History-aware Self-Scheduling.pdf}
}

@article{kennedy_acceptances_1991,
  title = {Acceptances and Autocorrelations in Hybrid {{Monte Carlo}}},
  author = {Kennedy, A.D. and Pendleton, Brian},
  year = {1991},
  month = may,
  journal = {Nuclear Physics B - Proceedings Supplements},
  volume = {20},
  pages = {118--121},
  langid = {english}
}

@article{kennedy_acceptances_1991a,
  title = {Acceptances and Autocorrelations in Hybrid {{Monte Carlo}}},
  author = {Kennedy, A.D. and Pendleton, Brian},
  year = {1991},
  month = may,
  journal = {Nuclear Physics B - Proceedings Supplements},
  volume = {20},
  pages = {118--121},
  langid = {english}
}

@incollection{kermarrec_evaluating_2008,
  title = {Evaluating the {{Quality}} of a {{Network Topology}} through {{Random Walks}}},
  booktitle = {Distributed {{Computing}}},
  author = {Kermarrec, Anne-Marie and Le Merrer, Erwan and Sericola, Bruno and Tr{\'e}dan, Gilles},
  year = {2008},
  volume = {5218},
  pages = {509--511},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}}
}

@article{keskar_largebatch_2017,
  title = {On Large-Batch Training for Deep Learning: {{Generalization}} Gap and Sharp Minima},
  shorttitle = {On {{Large-Batch Training}} for {{Deep Learning}}},
  author = {Keskar, Nitish Shirish and Mudigere, Dheevatsa and Nocedal, Jorge and Smelyanskiy, Mikhail and Tang, Ping Tak Peter},
  year = {2017},
  month = feb,
  journal = {Proceedings of the International conference on learning representations},
  eprint = {1609.04836},
  eprinttype = {arxiv},
  primaryclass = {cs, math},
  abstract = {The stochastic gradient descent (SGD) method and its variants are algorithms of choice for many Deep Learning tasks. These methods operate in a small-batch regime wherein a fraction of the training data, say \$32\$-\$512\$ data points, is sampled to compute an approximation to the gradient. It has been observed in practice that when using a larger batch there is a degradation in the quality of the model, as measured by its ability to generalize. We investigate the cause for this generalization drop in the large-batch regime and present numerical evidence that supports the view that large-batch methods tend to converge to sharp minimizers of the training and testing functions - and as is well known, sharp minima lead to poorer generalization. In contrast, small-batch methods consistently converge to flat minimizers, and our experiments support a commonly held view that this is due to the inherent noise in the gradient estimation. We discuss several strategies to attempt to help large-batch methods eliminate this generalization gap.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Mathematics - Optimization and Control},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\N6HWCVWM\\Keskar et al. - 2017 - On Large-Batch Training for Deep Learning General.pdf;C\:\\Users\\msca8h\\Zotero\\storage\\5BSIG65U\\1609.html}
}

@inproceedings{key_side_2000,
  title = {Side Scan Sonar Technology},
  booktitle = {Proceedings of the {{MTS}}/{{IEEE Conference}} and {{Exhibition}}. {{Conference}}},
  author = {Key, W.H.},
  year = {2000},
  series = {{{OCEANS}}'00},
  volume = {2},
  pages = {1029--1033},
  publisher = {{IEEE}},
  address = {{Providence, RI, USA}}
}

@article{khaled_better_2022,
  title = {Better Theory for {{SGD}} in the Nonconvex World},
  author = {Khaled, Ahmed and Richt{\'a}rik, Peter},
  year = {2022},
  month = sep,
  journal = {Transactions of Machine Learning Research (in press)},
  eprint = {2002.03329},
  eprinttype = {arxiv},
  primaryclass = {cs, math, stat},
  abstract = {Large-scale nonconvex optimization problems are ubiquitous in modern machine learning, and among practitioners interested in solving them, Stochastic Gradient Descent (SGD) reigns supreme. We revisit the analysis of SGD in the nonconvex setting and propose a new variant of the recently introduced expected smoothness assumption which governs the behaviour of the second moment of the stochastic gradient. We show that our assumption is both more general and more reasonable than assumptions made in all prior work. Moreover, our results yield the optimal \$\textbackslash mathcal\{O\}(\textbackslash varepsilon\^\{-4\})\$ rate for finding a stationary point of nonconvex smooth functions, and recover the optimal \$\textbackslash mathcal\{O\}(\textbackslash varepsilon\^\{-1\})\$ rate for finding a global solution if the Polyak-\{\textbackslash L\}ojasiewicz condition is satisfied. We compare against convergence rates under convexity and prove a theorem on the convergence of SGD under Quadratic Functional Growth and convexity, which might be of independent interest. Moreover, we perform our analysis in a framework which allows for a detailed study of the effects of a wide array of sampling strategies and minibatch sizes for finite-sum optimization problems. We corroborate our theoretical results with experiments on real and synthetic data.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Mathematics - Optimization and Control,Statistics - Machine Learning},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\EFX4EMCL\\Khaled and Richtrik - 2020 - Better Theory for SGD in the Nonconvex World.pdf;C\:\\Users\\msca8h\\Zotero\\storage\\PR4G2FVB\\2002.html}
}

@article{khan_switchable_2020,
  title = {Switchable {{Deep Beamformer}}},
  author = {Khan, Shujaat and Huh, Jaeyoung and Ye, Jong Chul},
  year = {2020},
  month = sep,
  journal = {arXiv:2008.13646 [cs, eess, stat]},
  eprint = {2008.13646},
  eprinttype = {arxiv},
  primaryclass = {cs, eess, stat},
  abstract = {Recent proposals of deep beamformers using deep neural networks have attracted significant attention as computational efficient alternatives to adaptive and compressive beamformers. Moreover, deep beamformers are versatile in that image post-processing algorithms can be combined with the beamforming. Unfortunately, in the current technology, a separate beamformer should be trained and stored for each application, demanding significant scanner resources. To address this problem, here we propose a \{\textbackslash em switchable\} deep beamformer that can produce various types of output such as DAS, speckle removal, deconvolution, etc., using a single network with a simple switch. In particular, the switch is implemented through Adaptive Instance Normalization (AdaIN) layers, so that various output can be generated by merely changing the AdaIN code. Experimental results using B-mode focused ultrasound confirm the flexibility and efficacy of the proposed methods for various applications.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Electrical Engineering and Systems Science - Image and Video Processing,Statistics - Machine Learning},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\YIB7XIR8\\Khan et al. - 2020 - Switchable Deep Beamformer.pdf;C\:\\Users\\msca8h\\Zotero\\storage\\DKP47Y39\\2008.html}
}

@inproceedings{khatami_hpx_2017,
  title = {{{HPX}} Smart Executors},
  booktitle = {Proc. 3rd {{Int}}. {{Workshop Extreme Scale Program}}. {{Models Middleware}}},
  author = {Khatami, Z. and Troska, L. and Kaiser, H. and Ramanujam, J. and Serio, A.},
  year = {2017},
  series = {{{ESPM2}}'17},
  abstract = {The performance of many parallel applications depends on loop-level parallelism. However, manually parallelizing all loops may result in degrading parallel performance, as some of them cannot scale desirably to a large number of threads. In addition, the overheads of manually tuning loop parameters might prevent an application from reaching its maximum parallel performance. We illustrate how machine learning techniques can be applied to address these challenges. In this research, we develop a framework that is able to automatically capture the static and dynamic information of a loop. Moreover, we advocate a novel method by introducing HPX smart executors for determining the execution policy, chunk size, and prefetching distance of an HPX loop to achieve higher possible performance by feeding static information captured during compilation and runtime-based dynamic information to our learning model. Our evaluated execution results show that using these smart executors can speed up the HPX execution process by around 12\%-35\% for the Matrix Multiplication, Stream and \$2D\$ Stencil benchmarks compared to setting their HPX loop's execution policy/parameters manually or using HPX auto-parallelization techniques.},
  file = {/home/msca8h/Documents/parallel_scheduling/HPX_smart_executors.pdf;C\:\\Users\\msca8h\\Zotero\\storage\\3S6Z2YCI\\HPX_smart_executors.pdf}
}

@article{khu-raikim_evaluating_2020,
  title = {Evaluating the {{Strong Scalability}} of {{Parallel Markov-Chain Monte Carlo Algorithms}}},
  author = {{Khu-Rai Kim} and Maskell, Simon and Sungyong Park},
  year = {2020},
  publisher = {{Unpublished}},
  langid = {english}
}

@article{kim_automatic_2020,
  title = {Automatic Myocardial Segmentation in Dynamic Contrast Enhanced Perfusion {{MRI}} Using {{Monte Carlo}} Dropout in an Encoder-Decoder Convolutional Neural Network},
  author = {Kim, Yoon-Chul and Kim, Khu Rai and Choe, Yeon Hyeon},
  year = {2020},
  month = mar,
  journal = {Computer Methods and Programs in Biomedicine},
  volume = {185},
  pages = {105150},
  langid = {english}
}

@article{kim_evcmr_2019,
  title = {{{EVCMR}}: {{A}} Tool for the Quantitative Evaluation and Visualization of Cardiac {{MRI}} Data},
  shorttitle = {{{EVCMR}}},
  author = {Kim, Yoon-Chul and Kim, Khu Rai and Choi, Kwanghee and Kim, Minwoo and Chung, Younjoon and Choe, Yeon Hyeon},
  year = {2019},
  month = aug,
  journal = {Computers in Biology and Medicine},
  volume = {111},
  pages = {103334},
  langid = {english}
}

@article{kim_fast_2021,
  title = {Fast Calculation Software for Modified {{Look-Locker}} Inversion Recovery ({{MOLLI}}) {{T1}} Mapping},
  author = {Kim, Yoon-Chul and Kim, Khu Rai and Lee, Hyelee and Choe, Yeon Hyeon},
  year = {2021},
  month = dec,
  journal = {BMC Medical Imaging},
  volume = {21},
  number = {1},
  pages = {26},
  abstract = {Abstract                            Background               The purpose of this study was to develop a software tool and evaluate different T1 map calculation methods in terms of computation time in cardiac magnetic resonance imaging.                                         Methods               The modified Look-Locker inversion recovery (MOLLI) sequence was used to acquire multiple inversion time (TI) images for pre- and post-contrast T1 mapping. The T1 map calculation involved pixel-wise curve fitting based on the T1 relaxation model. A variety of methods were evaluated using data from 30 subjects for computational efficiency: MRmap, python Levenberg\textendash Marquardt (LM), python reduced-dimension (RD) non-linear least square, C++ single- and multi-core LM, and C++ single- and multi-core RD.                                         Results               Median (interquartile range) computation time was 126~s (98\textendash 141) for the publicly available software MRmap, 261~s (249\textendash 282) for python LM, 77~s (74\textendash 80) for python RD, 3.4~s (3.1\textendash 3.6) for C++ multi-core LM, and 1.9~s (1.9\textendash 2.0) for C++ multi-core RD. The fastest C++ multi-core RD and the publicly available MRmap showed good agreement of myocardial T1 values, resulting in 95\% Bland\textendash Altman limits of agreement of (-\,0.83 to 0.58~ms) and (-\,6.57 to 7.36~ms) with mean differences of -\,0.13~ms and 0.39~ms, for the pre- and post-contrast, respectively.                                         Conclusion               The C++ multi-core RD was the fastest method on a regular eight-core personal computer for pre- or post-contrast T1 map calculation. The presented software tool (fT1fit) facilitated rapid T1 map and extracellular volume fraction map calculations.},
  langid = {english},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\H54N7HXH\\Kim et al. - 2021 - Fast calculation software for modified Look-Locker.pdf}
}

@article{kim_probabilistic_2020,
  title = {A Probabilistic Machine Learning Approach to Scheduling Parallel Loops with {{Bayesian}} Optimization},
  author = {Kim, Khu-rai and Kim, Youngjae and Park, Sungyong},
  year = {2020},
  journal = {IEEE Transactions on Parallel and Distributed Systems},
  pages = {1--1}
}

@article{kim_robust_1993,
  title = {A Robust Adaptive Array Based on Signal Subspace Approach},
  author = {Kim, J.W. and Un, C.K.},
  year = {Nov./1993},
  journal = {IEEE Transactions on Signal Processing},
  volume = {41},
  number = {11},
  pages = {3166--3171}
}

@inproceedings{kim_robust_2019,
  title = {Towards Robust Data-Driven Parallel Loop Scheduling Using {{Bayesian}} Optimization},
  booktitle = {Proceedings of the {{IEEE International Symposium}} on {{Modeling}}, {{Analysis}}, and {{Simulation}} of {{Computer}} and {{Telecommunication Systems}}},
  author = {Kim, Khu-rai and Kim, Youngjae and Park, Sungyong},
  year = {2019},
  month = oct,
  pages = {241--248},
  publisher = {{IEEE}},
  address = {{Rennes, FR}}
}

@inproceedings{kim_robust_2019a,
  title = {Towards Robust Data-Driven Parallel Loop Scheduling Using {{Bayesian}} Optimization},
  booktitle = {{{IEEE}} 27th {{Int}}. {{Symp}}. {{Model}}., {{Anal}}. {{Simul}}. {{Comput}}. {{Telecommun}}. {{Syst}}.},
  author = {Kim, Khu-rai and Kim, Youngjae and Park, Sungyong},
  year = {2019},
  pages = {241--248},
  publisher = {{IEEE}},
  address = {{Rennes, FR}}
}

@inproceedings{kim_robust_2019b,
  title = {Towards {{Robust Data-Driven Parallel Loop Scheduling Using Bayesian Optimization}}},
  booktitle = {2019 {{IEEE}} 27th {{International Symposium}} on {{Modeling}}, {{Analysis}}, and {{Simulation}} of {{Computer}} and {{Telecommunication Systems}} ({{MASCOTS}})},
  author = {Kim, Khu-rai and Kim, Youngjae and Park, Sungyong},
  year = {2019},
  month = oct,
  pages = {241--248},
  publisher = {{IEEE}},
  address = {{Rennes, FR}}
}

@article{kim_stochastic_1998,
  title = {Stochastic {{Volatility}}: {{Likelihood Inference}} and {{Comparison}} with {{ARCH Models}}},
  shorttitle = {Stochastic {{Volatility}}},
  author = {Kim, Sangjoon and Shepherd, Neil and Chib, Siddhartha},
  year = {1998},
  month = jul,
  journal = {Review of Economic Studies},
  volume = {65},
  number = {3},
  pages = {361--393},
  langid = {english}
}

@inproceedings{kim2021adaptive,
  title = {Adaptive Strategy for Resetting a Non-Stationary Markov Chain during Learning via Joint Stochastic Approximation},
  booktitle = {Proceedings of the 3rd Symposium on Advances in Approximate Bayesian, to {{Appear}}},
  author = {Kim, Hyunsu and Lee, Juho and Yang, Hongseok},
  year = {2021}
}

@inproceedings{kingma_adam_2015,
  title = {Adam: {{A Method}} for {{Stochastic Optimization}}},
  booktitle = {Proceedings of the {{International}} Conference on Learning Representations},
  author = {Kingma, Diederik P. and Ba, Jimmy},
  year = {2015},
  eprint = {1412.6980},
  eprinttype = {arxiv},
  address = {{San Diego, California, USA}},
  abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\ICB6B4IV\\Kingma and Ba - 2017 - Adam A Method for Stochastic Optimization.pdf;C\:\\Users\\msca8h\\Zotero\\storage\\REWW72SC\\1412.html}
}

@inproceedings{klambauer_selfnormalizing_2017,
  title = {Self-{{Normalizing Neural Networks}}},
  booktitle = {Proceedings of the 31st {{International Conference}} on {{Neural Information Processing Systems}}},
  author = {Klambauer, G{\"u}nter and Unterthiner, Thomas and Mayr, Andreas and Hochreiter, Sepp},
  year = {2017},
  series = {{{NIPS}}'17},
  pages = {972--981},
  publisher = {{Curran Associates Inc.}},
  address = {{Red Hook, NY, USA}}
}

@article{klein_fast_2017,
  title = {Fast {{Bayesian}} Hyperparameter Optimization on Large Datasets},
  author = {Klein, Aaron and Falkner, Stefan and Bartels, Simon and Hennig, Philipp and Hutter, Frank},
  year = {2017},
  journal = {Electronic Journal of Statistics},
  volume = {11},
  number = {2},
  pages = {4945--4968},
  langid = {english},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\DAEJNAYW\\Klein et al. - 2017 - Fast Bayesian hyperparameter optimization on large.pdf}
}

@inproceedings{klein_robo_2017,
  title = {{{RoBO}}: {{A}} Flexible and Robust {{Bayesian}} Optimization Framework in {{Python}}},
  booktitle = {{{NIPS}} 2017 {{Bayesian Optimization Workshop}}},
  author = {Klein, Aaron and Falkner, Stefan and Mansur, Numair and Hutter, Frank},
  year = {2017}
}

@incollection{klug_autopin_2011,
  title = {Autopin \textendash{} {{Automated Optimization}} of {{Thread-to-Core Pinning}} on {{Multicore Systems}}},
  booktitle = {Transactions on {{High-Performance Embedded Architectures}} and {{Compilers III}}},
  author = {Klug, Tobias and Ott, Michael and Weidendorfer, Josef and Trinitis, Carsten},
  year = {2011},
  volume = {6590},
  pages = {219--235},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}}
}

@article{knight_digital_1981,
  title = {Digital Signal Processing for Sonar},
  author = {Knight, W.C. and Pridham, R.G. and Kay, S.M.},
  year = {1981},
  journal = {Proceedings of the IEEE},
  volume = {69},
  number = {11},
  pages = {1451--1506}
}

@article{knoblauch_optimizationcentric_2022,
  title = {An Optimization-Centric View on {{Bayes}}' Rule: {{Reviewing}} and Generalizing Variational Inference},
  shorttitle = {An Optimization-Centric View on Bayes' Rule},
  author = {Knoblauch, Jeremias and Jewson, Jack and Damoulas, Theodoros},
  year = {2022},
  journal = {Journal of Machine Learning Research},
  volume = {23},
  number = {132},
  pages = {1--109},
  abstract = {We advocate an optimization-centric view of Bayesian inference. Our inspiration is the representation of Bayes' rule as infinite-dimensional optimization (Csiszar, 1975; Donsker and Varadhan, 1975; Zellner, 1988). Equipped with this perspective, we study Bayesian inference when one does not have access to (1) well-specified priors, (2) well-specified likelihoods, (3) infinite computing power. While these three assumptions underlie the standard Bayesian paradigm, they are typically inappropriate for modern Machine Learning applications. We propose addressing this through an optimization-centric generalization of Bayesian posteriors that we call the Rule of Three (RoT). The RoT can be justified axiomatically and recovers Bayesian, PAC-Bayesian and VI posteriors as special cases. While the RoT is primarily a conceptual and theoretical device, it also encompasses a novel sub-class of tractable posteriors which we call Generalized Variational Inference (GVI) posteriors. Just as the RoT, GVI posteriors are specified by three arguments: a loss, a divergence and a variational family. They also possess a number of desirable properties, including modularity, Frequentist consistency and an interpretation as approximate ELBO. We explore applications of GVI posteriors, and show that they can be used to improve robustness and posterior marginals on Bayesian Neural Networks and Deep Gaussian Processes.},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\5H4T4WF5\\Knoblauch et al. - 2022 - An Optimization-centric View on Bayes' Rule Revie.pdf;C\:\\Users\\msca8h\\Zotero\\storage\\J2GHSAR7\\GVIPublic.html}
}

@article{knuth_bayesian_2015,
  title = {Bayesian Evidence and Model Selection},
  author = {Knuth, Kevin H. and Habeck, Michael and Malakar, Nabin K. and Mubeen, Asim M. and Placek, Ben},
  year = {2015},
  month = dec,
  journal = {Digital Signal Processing},
  volume = {47},
  pages = {50--67},
  langid = {english},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\WHHHX2IF\\Knuth et al. - 2015 - Bayesian evidence and model selection.pdf}
}

@article{koblents_population_2015,
  title = {A Population {{Monte Carlo}} Scheme with Transformed Weights and Its Application to Stochastic Kinetic Models},
  author = {Koblents, Eugenia and M{\'i}guez, Joaqu{\'i}n},
  year = {2015},
  month = mar,
  journal = {Statistics and Computing},
  volume = {25},
  number = {2},
  pages = {407--425},
  langid = {english},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\SI34WAQ6\\Koblents and Mguez - 2015 - A population Monte Carlo scheme with transformed w.pdf}
}

@inproceedings{kochanski_bayesian_2017,
  title = {Bayesian Optimization for a Better Dessert},
  booktitle = {Proceedings of the {{NIPS Workshop}} on {{Bayesian Optimization}} ({{BayesOpt}}'17)},
  author = {Kochanski, Greg and Golovin, Daniel and Karro, John and Solnik, Benjamin and Moitra, Subhodeep and Sculley, D},
  year = {2017},
  file = {/home/msca8h/Documents/bayesian_optimization/Kochanski et al. - 2017 - Bayesian Optimization for a Better Dessert.pdf}
}

@incollection{koivunen_model_2014,
  title = {Model Order Selection},
  booktitle = {Academic {{Press Library}} in {{Signal Processing}}},
  author = {Koivunen, Visa and Ollila, Esa},
  year = {2014},
  volume = {3},
  pages = {9--25},
  publisher = {{Elsevier}},
  langid = {english}
}

@incollection{koivunen_model_2014a,
  title = {Model Order Selection},
  booktitle = {Academic {{Press Library}} in {{Signal Processing}}},
  author = {Koivunen, Visa and Ollila, Esa},
  year = {2014},
  volume = {3},
  pages = {9--25},
  publisher = {{Elsevier}},
  langid = {english}
}

@techreport{kong_note_1992,
  type = {Technical {{Report}}},
  title = {A Note on Importance Sampling Using Standardized Weights},
  author = {Kong, Augustine},
  year = {1992},
  month = jul,
  number = {348},
  institution = {{Department of Statistics, University of Chicago}}
}

@article{kong_sequential_1994,
  title = {Sequential {{Imputations}} and {{Bayesian Missing Data Problems}}},
  author = {Kong, Augustine and Liu, Jun S. and Wong, Wing Hung},
  year = {1994},
  month = mar,
  journal = {Journal of the American Statistical Association},
  volume = {89},
  number = {425},
  pages = {278--288},
  langid = {english}
}

@article{korobilis_bayesian_2022,
  title = {Bayesian Approaches to Shrinkage and Sparse Estimation},
  author = {Korobilis, Dimitris and Shimizu, Kenichi},
  year = {2022},
  month = jun,
  journal = {Foundations and Trends\textregistered{} in Econometrics},
  volume = {11},
  number = {4},
  pages = {230--354}
}

@article{kovesi_image_1999,
  title = {Image Features from Phase Congruency},
  author = {Kovesi, Peter},
  year = {1999},
  journal = {Journal of Computer Vision Research},
  volume = {1},
  number = {3},
  pages = {1--26}
}

@book{koyama_computational_2018,
  title = {Computational {{Design}} with {{Crowds}}},
  author = {Koyama, Yuki and Igarashi, Takeo},
  year = {2018},
  month = mar,
  volume = {1},
  publisher = {{Oxford University Press}},
  abstract = {Computational design is aimed at supporting automating design processes using computational techniques. However, some classes of design tasks involve criteria that are difficult to handle only with computers. For example, visual design tasks seeking to fulfil aesthetic goals are difficult to handle purely with computers. One promising approach is to leverage human computation; that is, to incorporate human input into the computation process. Crowdsourcing platforms provide a convenient way to integrate such human computation into a working system. In this chapter, we discuss such computational design with crowds in the domain of parameter tweaking tasks in visual design. Parameter tweaking is often performed to maximize the aesthetic quality of designed objects. Computational design powered by crowds can solve this maximization problem by leveraging human computation. We discuss the opportunities and challenges of computational design with crowds with two illustrative examples: (1) estimating the objective function to facilitate the design exploration by a designer and (2) directly searching for the optimal parameter setting that maximizes the objective function.},
  langid = {english},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\EUB5G7JJ\\Koyama and Igarashi - 2018 - Computational Design with Crowds.pdf}
}

@article{koyama_sequential_2020,
  title = {Sequential Gallery for Interactive Visual Design Optimization},
  author = {Koyama, Yuki and Sato, Issei and Goto, Masataka},
  year = {2020},
  month = jul,
  journal = {ACM Transactions on Graphics},
  series = {{{SIGGRAPH}}'20},
  volume = {39},
  number = {4},
  pages = {88:1-88:12},
  langid = {english},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\ZV72YBV3\\Koyama et al. - 2020 - Sequential gallery for interactive visual design o.pdf}
}

@inproceedings{kraus_em_1993,
  title = {{{EM}} Dual Maximum Likelihood Estimation for Wideband Source Location},
  booktitle = {1993 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}}, and {{Signal Processing}}},
  author = {Kraus, D. and Dhaouadi, A. and Bohme, J.F.},
  year = {1993},
  month = apr,
  volume = {1},
  pages = {257-260 vol.1},
  abstract = {Approximate maximum likelihood estimates and approximate dual maximum likelihood estimates for locating wideband sources in the presence of partly unknown noise fields are investigated. An extended model of the spectral density matrix of the sensor array output for coherent sources (multipath propagation) is introduced. The authors derive an expectation maximization (EM) iteration scheme for distributions belonging to the exponential family and investigate the so-called dual maximum likelihood estimate. The EM procedure is used to compute approximate maximum likelihood and approximate dual maximum likelihood estimates for source locations, signal, and noise spectral parameters.{$<>$}},
  keywords = {Frequency,Geometry,Integrated circuit noise,Maximum likelihood estimation,Noise measurement,Position measurement,Sensor arrays,Wideband},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\FGCSV6TB\\Kraus et al. - 1993 - EM dual maximum likelihood estimation for wideband.pdf;C\:\\Users\\msca8h\\Zotero\\storage\\R6V7BM2L\\319104.html}
}

@article{krim_two_1996,
  title = {Two Decades of Array Signal Processing Research: The Parametric Approach},
  shorttitle = {Two Decades of Array Signal Processing Research},
  author = {Krim, H. and Viberg, M.},
  year = {1996},
  month = jul,
  journal = {IEEE Signal Processing Magazine},
  volume = {13},
  number = {4},
  pages = {67--94}
}

@article{krissian_oriented_2007,
  title = {Oriented Speckle Reducing Anisotropic Diffusion},
  author = {Krissian, Karl and Westin, Carl-Fredrik and Kikinis, Ron and Vosburgh, Kirby G.},
  year = {2007},
  month = may,
  journal = {IEEE Transactions on Image Processing},
  volume = {16},
  number = {5},
  pages = {1412--1424}
}

@article{kritchman_nonparametric_2009,
  title = {Non-Parametric Detection of the Number of Signals: Hypothesis Testing and Random Matrix Theory},
  shorttitle = {Non-{{Parametric Detection}} of the {{Number}} of {{Signals}}},
  author = {Kritchman, S. and Nadler, B.},
  year = {2009},
  month = oct,
  journal = {IEEE Transactions on Signal Processing},
  volume = {57},
  number = {10},
  pages = {3930--3941}
}

@inproceedings{krizhevsky_imagenet_2012,
  title = {{{ImageNet Classification}} with {{Deep Convolutional Neural Networks}}},
  booktitle = {Proceedings of the 25th {{International Conference}} on {{Neural Information Processing Systems}} - {{Volume}} 1},
  author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E.},
  year = {2012},
  series = {{{NIPS}}'12},
  pages = {1097--1105},
  publisher = {{Curran Associates Inc.}},
  address = {{USA}}
}

@article{krolik_focused_1990,
  title = {Focused Wide-Band Array Processing by Spatial Resampling},
  author = {Krolik, J. and Swingler, D.},
  year = {1990},
  month = feb,
  journal = {IEEE Transactions on Acoustics, Speech, and Signal Processing},
  volume = {38},
  number = {2},
  pages = {356--360},
  abstract = {The authors present a novel approach for coherently focusing wideband data received by a linear array of sensors. The proposed preprocessing consists of adjusting the spatial sampling rate, or spatially resampling the array outputs as a function of temporal frequency so that broadband sources are aligned in the spatial frequency domain. Spatial resampling has the advantage that it can reduce each wideband source in multigroup multiple-source scenarios to essentially a rank-one representation without preliminary estimates or a priori knowledge of the spatial distribution of the sources. Preliminary simulations performed using a well-known digital interpolation method to implement spatial resampling with the MUSIC spectral estimator have yielded promising results.{$<>$}},
  keywords = {Array signal processing,Direction of arrival estimation,Least squares methods,Narrowband,Sensor arrays,Signal processing,Signal resolution,Signal to noise ratio,Speech processing,Wideband},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\MNI8RBG6\\Krolik and Swingler - 1990 - Focused wide-band array processing by spatial resa.pdf}
}

@article{krolik_multiple_1989,
  title = {Multiple Broad-Band Source Location Using Steered Covariance Matrices},
  author = {Krolik, J. and Swingler, D.},
  year = {Oct./1989},
  journal = {IEEE Transactions on Acoustics, Speech, and Signal Processing},
  volume = {37},
  number = {10},
  pages = {1481--1494}
}

@article{kruizinga_planewave_2012,
  title = {Plane-Wave Ultrasound Beamforming Using a Nonuniform Fast Fourier Transform},
  author = {Kruizinga, P. and Mastik, F. and {de Jong}, N. and {van der Steen}, A. F. W. and {van Soest}, G.},
  year = {2012},
  month = dec,
  journal = {IEEE Transactions on Ultrasonics, Ferroelectrics and Frequency Control},
  volume = {59},
  number = {12},
  pages = {6373791}
}

@article{kruskal_allocating_1985,
  title = {Allocating Independent Subtasks on Parallel Processors},
  author = {Kruskal, C. P. and Weiss, A.},
  year = {1985},
  month = oct,
  journal = {IEEE Transactions on Software Engineering},
  volume = {SE-11},
  number = {10},
  pages = {1001--1016},
  file = {/home/msca8h/Documents/parallel_scheduling/Kruskal and Weiss - 1985 - Allocating Independent Subtasks on Parallel Proces.pdf}
}

@article{kucukelbir_automatic_2017,
  title = {Automatic Differentiation Variational Inference},
  author = {Kucukelbir, Alp and Tran, Dustin and Ranganath, Rajesh and Gelman, Andrew and Blei, David M.},
  year = {2017},
  journal = {Journal of Machine Learning Research},
  volume = {18},
  number = {14},
  pages = {1--45}
}

@article{kuhn_coupling_2004,
  title = {Coupling a Stochastic Approximation Version of {{EM}} with an {{MCMC}} Procedure},
  author = {Kuhn, Estelle and Lavielle, Marc},
  year = {2004},
  month = aug,
  journal = {ESAIM: Probability and Statistics},
  volume = {8},
  pages = {115--131},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\H87DQ6EA\\Kuhn and Lavielle - 2004 - Coupling a stochastic approximation version of EM .pdf}
}

@inproceedings{kumar_overview_2016,
  title = {An Overview of Modern Cache Memory and Performance Analysis of Replacement Policies},
  booktitle = {2016 {{IEEE International Conference}} on {{Engineering}} and {{Technology}} ({{ICETECH}})},
  author = {Kumar, S. and Singh, P. K.},
  year = {2016},
  month = mar,
  pages = {210--214},
  keywords = {associative cache,benchmark testing,cache memory,Cache memory,Cache Performance,cache replacement policy,cache storage,Clocks,Conferences,current generation processors,design parameter,energy consumption,FIFO,hard disk,Hardware,hit rate,Hit rate,LFU,LRU,memory architecture,memory hierarchy,microprocessor chips,Miss rate,Multicore processing,Organizations,performance analysis,performance evaluation,performance improvement,processor performance,RANDOM,registers,Registers,Replacement Policy,virtual memory,virtual storage}
}

@article{kung_discussions_1994,
  title = {Discussion: {{Markov}} Chains for Exploring Posterior Distributions},
  author = {Chan, Kung Sik and Geyer, Charles J.},
  year = {1994},
  journal = {The Annals of Statistics},
  volume = {22},
  number = {4},
  pages = {1747--1758},
  publisher = {{Institute of Mathematical Statistics}}
}

@inproceedings{kurth_exascale_2018,
  title = {Exascale Deep Learning for Climate Analytics},
  booktitle = {Proc.  {{Int}}. {{Conf}}. {{High Perform}}. {{Comput}}. {{Networking}}, {{Storage}}, {{Anal}}.},
  author = {Kurth, Thorsten and Treichler, Sean and Romero, Joshua and Mudigonda, Mayur and Luehr, Nathan and Phillips, Everett and Mahesh, Ankur and Matheson, Michael and Deslippe, Jack and Fatica, Massimiliano and {al}, et},
  year = {2018},
  series = {{{SC}} '18}
}

@article{kurzak_autotuning_2012,
  title = {Autotuning {{GEMM Kernels}} for the {{Fermi GPU}}},
  author = {Kurzak, Jakub and Tomov, Stanimire and Dongarra, Jack},
  year = {2012},
  month = nov,
  journal = {IEEE Transactions on Parallel and Distributed Systems},
  volume = {23},
  number = {11},
  pages = {2045--2057}
}

@inproceedings{kurzak_massively_2019,
  title = {Massively {{Parallel Automated Software Tuning}}},
  booktitle = {Proceedings of the 48th {{International Conference}} on {{Parallel Processing}}  - {{ICPP}} 2019},
  author = {Kurzak, Jakub and Tsai, Yaohung M. and Gates, Mark and Abdelfattah, Ahmad and Dongarra, Jack},
  year = {2019},
  pages = {1--10},
  publisher = {{ACM Press}},
  address = {{Kyoto, Japan}},
  langid = {english}
}

@inproceedings{kurzak_massively_2019a,
  title = {Massively {{Parallel Automated Software Tuning}}},
  booktitle = {Proceedings of the 48th {{International Conference}} on {{Parallel Processing}}  - {{ICPP}} 2019},
  author = {Kurzak, Jakub and Tsai, Yaohung M. and Gates, Mark and Abdelfattah, Ahmad and Dongarra, Jack},
  year = {2019},
  pages = {1--10},
  publisher = {{ACM Press}},
  address = {{Kyoto, Japan}},
  langid = {english}
}

@inproceedings{kwak_development_2015,
  title = {Development of Acoustic Camera-Imaging Simulator Based on Novel Model},
  booktitle = {2015 {{IEEE}} 15th {{International Conference}} on {{Environment}} and {{Electrical Engineering}} ({{EEEIC}})},
  author = {Kwak, Seungchul and Ji, Yonghoon and Yamashita, Atsushi and Asama, Hajime},
  year = {2015},
  month = jun,
  pages = {1719--1724},
  publisher = {{IEEE}},
  address = {{Rome, Italy}}
}

@article{laakso_splitting_1996,
  title = {Splitting the Unit Delay [{{FIR}}/All Pass Filters Design]},
  author = {Laakso, T.I. and Valimaki, V. and Karjalainen, M. and Laine, U.K.},
  year = {Jan./1996},
  journal = {IEEE Signal Processing Magazine},
  volume = {13},
  number = {1},
  pages = {30--60}
}

@inproceedings{laberge_scheduling_2019,
  title = {Scheduling Optimization of Parallel Linear Algebra Algorithms Using Supervised Learning},
  booktitle = {{{IEEE}}/{{ACM Workshop Mach}}. {{Learn}}. {{High Perform}}. {{Comput}}. {{Environ}}.},
  author = {Laberge, Gabriel and Shirzad, Shahrzad and Diehl, Patrick and Kaiser, Hartmut and Prudhomme, Serge and Lemoine, Adrian S.},
  year = {2019},
  month = nov,
  series = {{{MLHPC}}'19},
  pages = {31--43},
  publisher = {{IEEE}},
  address = {{Denver, CO, USA}},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\7IEGGCK8\\laberge et al. - 2019 - Scheduling Optimization of Parallel Linear Algebra.pdf}
}

@inproceedings{laberge_scheduling_2019a,
  title = {Scheduling {{Optimization}} of {{Parallel Linear Algebra Algorithms Using Supervised Learning}}},
  booktitle = {2019 {{IEEE}}/{{ACM Workshop}} on {{Machine Learning}} in {{High Performance Computing Environments}} ({{MLHPC}})},
  author = {{laberge}, gabriel and Shirzad, Shahrzad and Diehl, Patrick and Kaiser, Hartmut and Prudhomme, Serge and Lemoine, Adrian S.},
  year = {2019},
  month = nov,
  pages = {31--43},
  publisher = {{IEEE}},
  address = {{Denver, CO, USA}},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\28R5IL4D\\laberge et al. - 2019 - Scheduling Optimization of Parallel Linear Algebra.pdf}
}

@article{ladaycia_performance_2017,
  title = {Performance Bounds Analysis for Semi-Blind Channel Estimation in {{MIMO-OFDM}} Communications Systems},
  author = {Ladaycia, Abdelhamid and Mokraoui, Anissa and {Abed-Meraim}, Karim and Belouchrani, Adel},
  year = {2017},
  month = sep,
  journal = {IEEE Transactions on Wireless Communications},
  volume = {16},
  number = {9},
  pages = {5925--5938}
}

@inproceedings{lafond_dfw_2016,
  title = {D-{{FW}}: {{Communication}} Efficient Distributed Algorithms for High-Dimensional Sparse Optimization},
  shorttitle = {D-{{FW}}},
  booktitle = {2016 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Lafond, Jean and Wai, Hoi-To and Moulines, Eric},
  year = {2016},
  month = mar,
  pages = {4144--4148},
  publisher = {{IEEE}},
  address = {{Shanghai}},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\5TZAVVFE\\Lafond et al. - 2016 - D-FW Communication efficient distributed algorith.pdf}
}

@article{lam_bayesian_2006,
  title = {Bayesian Beamforming for {{DOA}} Uncertainty: {{Theory}} and Implementation},
  shorttitle = {Bayesian {{Beamforming}} for {{DOA Uncertainty}}},
  author = {Lam, C.J. and Singer, A.C.},
  year = {2006},
  month = nov,
  journal = {IEEE Transactions on Signal Processing},
  volume = {54},
  number = {11},
  pages = {4435--4445}
}

@inproceedings{lam_fast_2003,
  title = {Fast Adaptive Bayesian Beamforming Using the {{FFT}}},
  booktitle = {Proceedings of the {{IEEE Workshop}} on {{Statistical Signal Processing}}},
  author = {Lam, C.J. and Singer, A.C.},
  year = {2003},
  pages = {413--416},
  publisher = {{IEEE}},
  address = {{St. Louis, MO, USA}}
}

@article{lamberti_independent_2017,
  title = {Independent {{Resampling Sequential Monte Carlo Algorithms}}},
  author = {Lamberti, Roland and Petetin, Yohan and Desbouvries, Francois and Septier, Francois},
  year = {2017},
  month = oct,
  journal = {IEEE Transactions on Signal Processing},
  volume = {65},
  number = {20},
  pages = {5318--5333},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\79VIR5YR\\lamberti2017.pdf;C\:\\Users\\msca8h\\Zotero\\storage\\ICAIAK26\\Lamberti et al. - 2017 - Independent Resampling Sequential Monte Carlo Algo.pdf}
}

@article{lamberti_semiindependent_2018,
  title = {Semi-{{Independent Resampling}} for {{Particle Filtering}}},
  author = {Lamberti, Roland and Petetin, Yohan and Desbouvries, Francois and Septier, Francois},
  year = {2018},
  month = jan,
  journal = {IEEE Signal Processing Letters},
  volume = {25},
  number = {1},
  pages = {130--134},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\3U63MQ2P\\Lamberti et al. - 2018 - Semi-Independent Resampling for Particle Filtering.pdf;C\:\\Users\\msca8h\\Zotero\\storage\\5UCMWPG8\\lamberti2018.pdf}
}

@inproceedings{langer_building_1991,
  title = {Building Qualitative Elevation Maps from Side Scan Sonar Data for Autonomous Underwater Navigation},
  booktitle = {Proceedings. 1991 {{IEEE International Conference}} on {{Robotics}} and {{Automation}}},
  author = {Langer, D. and Hebert, M.},
  year = {1991},
  pages = {2478--2483},
  publisher = {{IEEE Comput. Soc. Press}},
  address = {{Sacramento, CA, USA}}
}

@article{larocque_reversible_2002,
  title = {Reversible Jump {{MCMC}} for Joint Detection and Estimation of Sources in Colored Noise},
  author = {Larocque, J.-R. and Reilly, J.P.},
  year = {Feb./2002},
  journal = {IEEE Transactions on Signal Processing},
  volume = {50},
  number = {2},
  pages = {231--240}
}

@article{larocque_reversible_2002a,
  title = {Reversible Jump {{MCMC}} for Joint Detection and Estimation of Sources in Colored Noise},
  author = {Larocque, J.-R. and Reilly, J.P.},
  year = {Feb./2002},
  journal = {IEEE Transactions on Signal Processing},
  volume = {50},
  number = {2},
  pages = {231--240}
}

@inproceedings{larsson_waveform_2004,
  title = {On Waveform Selection in a Time Varying Sonar Environment},
  booktitle = {Proceedings of the {{Annual Conference}} of the {{Australian Acoustical Society}}},
  author = {Larsson, Ashley I. and Gillard, Chris},
  year = {2004},
  month = nov,
  series = {{{ACOUSTICS}}'04},
  pages = {73--78},
  address = {{Queensland, Australia}},
  file = {C\:\\Users\\msca8h\\Projects\\BLUE\\papers\\zotero\\AC040043.PDF}
}

@article{lartillot_computing_2006,
  title = {Computing {{Bayes}} Factors Using Thermodynamic Integration},
  author = {Lartillot, Nicolas and Philippe, Herv{\'e}},
  year = {2006},
  month = apr,
  journal = {Systematic Biology},
  volume = {55},
  number = {2},
  pages = {195--207},
  langid = {english},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\HS3B5PUZ\\Lartillot and Philippe - 2006 - Computing Bayes Factors Using Thermodynamic Integr.pdf}
}

@inproceedings{le_revisiting_2020,
  title = {Revisiting Reweighted Wake-Sleep for Models with Stochastic Control Flow},
  booktitle = {Proceedings of the {{International Conference}} on {{Uncertainty}} in {{Artifical Intelligence}}},
  author = {Le, Tuan Anh and Kosiorek, Adam R. and Siddharth, N. and Teh, Yee Whye and Wood, Frank},
  year = {2020},
  month = jul,
  series = {{{PMLR}}},
  volume = {115},
  pages = {1039--1049},
  publisher = {{ML Research Press}}
}

@article{leclerc_deep_2019,
  title = {Deep Learning for Segmentation Using an Open Large-Scale Dataset in {{2D}} Echocardiography},
  author = {Leclerc, Sarah and Smistad, Erik and Pedrosa, Joao and Ostvik, Andreas and Cervenansky, Frederic and Espinosa, Florian and Espeland, Torvald and Berg, Erik Andreas Rye and Jodoin, Pierre-Marc and Grenier, Thomas and Lartizien, Carole and Dhooge, Jan and Lovstakken, Lasse and Bernard, Olivier},
  year = {2019},
  month = sep,
  journal = {IEEE Transactions on Medical Imaging},
  volume = {38},
  number = {9},
  pages = {2198--2210},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\A43RDL35\\Leclerc et al. - 2019 - Deep Learning for Segmentation Using an Open Large.pdf}
}

@incollection{lecun_efficient_2012,
  title = {Efficient {{BackProp}}},
  booktitle = {Neural {{Networks}}: {{Tricks}} of the {{Trade}}},
  author = {LeCun, Yann A. and Bottou, L{\'e}on and Orr, Genevieve B. and M{\"u}ller, Klaus-Robert},
  year = {2012},
  volume = {7700},
  pages = {9--48},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  langid = {english}
}

@article{ledoit_wellconditioned_2004,
  title = {A Well-Conditioned Estimator for Large-Dimensional Covariance Matrices},
  author = {Ledoit, Olivier and Wolf, Michael},
  year = {2004},
  month = feb,
  journal = {Journal of Multivariate Analysis},
  volume = {88},
  number = {2},
  pages = {365--411},
  langid = {english},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\CLJVPUNC\\Ledoit and Wolf - 2004 - A well-conditioned estimator for large-dimensional.pdf}
}

@inproceedings{lee_automatic_2006,
  title = {Automatic Time Gain Compensation and Dynamic Range Control in Ultrasound Imaging Systems},
  booktitle = {Proc. {{SPIE Med}}. {{Imag}}.},
  author = {Lee, Duhgoon and Kim, Yong Sun and Ra, Jong Beom},
  year = {2006},
  month = mar,
  pages = {614708},
  address = {{San Diego, CA}}
}

@article{lee_automatic_2015,
  title = {Automatic Dynamic Range Adjustment for Ultrasound {{B-mode}} Imaging},
  author = {Lee, Yeonhwa and Kang, Jinbum and Yoo, Yangmo},
  year = {2015},
  month = feb,
  journal = {Ultrasonics},
  volume = {56},
  pages = {435--443},
  langid = {english}
}

@article{lee_distributed_2017,
  title = {Distributed {{Stochastic Variance Reduced Gradient Methods}} by {{Sampling Extra Data}} with {{Replacement}}},
  author = {Lee, Jason D. and Lin, Qihang and Ma, Tengyu and Yang, Tianbao},
  year = {2017},
  journal = {Journal of Machine Learning Research},
  volume = {18},
  number = {122},
  pages = {1--43},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\DTSH3QKY\\Lee et al. - 2017 - Distributed Stochastic Variance Reduced Gradient M.pdf}
}

@article{lee_efficient_1994,
  title = {Efficient Wideband Source Localization Using Beamforming Invariance Technique},
  author = {Lee, Ta-Sung},
  year = {1994},
  month = jun,
  journal = {IEEE Transactions on Signal Processing},
  volume = {42},
  number = {6},
  pages = {1376--1387},
  abstract = {A novel scheme for wideband direction-of-arrival (DOA) estimation is proposed. The technique performs coherent signal subspace transformation by a set of judiciously constructed beamforming matrices. The beamformers are chosen to transform each of the narrowband array manifold vectors into the one corresponding to the reference frequency, regardless of the actual spatial distribution of the sources. The focused data correlation matrix can thus be obtained without any preliminary DOA estimation or iteration. A simplified version of the beamspace Root-MUSIC algorithm is developed and used in conjunction with the proposed method to efficiently localize multiple wideband sources with a linear, equally spaced array. Numerical simulations are conducted to demonstrate the efficacy of the new scheme.{$<>$}},
  keywords = {Array signal processing,Covariance matrix,Direction of arrival estimation,Frequency,Matrix decomposition,Narrowband,Numerical simulation,Position measurement,Sensor arrays,Wideband},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\SB2Z3XSJ\\Lee - 1994 - Efficient wideband source localization using beamf.pdf}
}

@inproceedings{lee_petascale_2013,
  title = {Petascale Direct Numerical Simulation of Turbulent Channel Flow on up to {{786K}} Cores},
  booktitle = {{{SC}} '13: {{Proceedings}} of the {{International Conference}} on {{High Performance Computing}}, {{Networking}}, {{Storage}} and {{Analysis}}},
  author = {Lee, M. and Malaya, N. and Moser, R. D.},
  year = {2013},
  month = nov,
  pages = {1--11},
  keywords = {Abstracts,Benchmark testing,boundary layer turbulence,channel flow,computational fluid dynamics,Data transpose,DNS,Equations,external flows,flow simulation,fluid flow equations,grid modeling,high Reynolds number,Mathematical model,Mechanical engineering,MPI alltoall,numerical analysis,Numerical simulation,Parallel FFT,performance optimization,Petascale,petascale direct numerical simulation,Turbulence,turbulent flow over walls,Vectors,wall bounded turbulent flow}
}

@article{lee_utility_2010,
  title = {On the Utility of Graphics Cards to Perform Massively Parallel Simulation of Advanced {{Monte Carlo}} Methods},
  author = {Lee, Anthony and Yau, Christopher and Giles, Michael B. and Doucet, Arnaud and Holmes, Christopher C.},
  year = {2010},
  month = jan,
  journal = {Journal of Computational and Graphical Statistics},
  volume = {19},
  number = {4},
  pages = {769--789},
  langid = {english},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\GEUTUAWS\\Lee et al. - 2010 - On the Utility of Graphics Cards to Perform Massiv.pdf}
}

@misc{leger_parametrization_2023,
  title = {Parametrization Cookbook: {{A}} Set of Bijective Parametrizations for Using Machine Learning Methods in Statistical Inference},
  shorttitle = {Parametrization Cookbook},
  author = {Leger, Jean-Benoist},
  year = {2023},
  month = jan,
  number = {arXiv:2301.08297},
  eprint = {2301.08297},
  eprinttype = {arxiv},
  primaryclass = {stat},
  publisher = {{arXiv}},
  abstract = {We present in this paper a way to transform a constrained statistical inference problem into an unconstrained one in order to be able to use modern computational methods, such as those based on automatic differentiation, GPU computing, stochastic gradients with mini-batch. Unlike the parametrizations classically used in Machine Learning, the parametrizations introduced here are all bijective and are even diffeomorphisms, thus allowing to keep the important properties from a statistical inference point of view, first of all identifiability. This cookbook presents a set of recipes to use to transform a constrained problem into a unconstrained one. For an easy use of parametrizations, this paper is at the same time a cookbook, and a Python package allowing the use of parametrizations with numpy, but also JAX and PyTorch, as well as a high level and expressive interface allowing to easily describe a parametrization to transform a difficult problem of statistical inference into an easier problem addressable with modern optimization tools.},
  archiveprefix = {arXiv},
  keywords = {Statistics - Computation,Statistics - Machine Learning},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\KXU93HVZ\\Leger - 2023 - Parametrization Cookbook A set of Bijective Param.pdf;C\:\\Users\\msca8h\\Zotero\\storage\\Z6FI8X9F\\2301.html}
}

@article{lei_stochastic_2020,
  title = {Stochastic Gradient Descent for Nonconvex Learning without Bounded Gradient Assumptions},
  author = {Lei, Yunwen and Hu, Ting and Li, Guiying and Tang, Ke},
  year = {2020},
  month = oct,
  journal = {IEEE Transactions on Neural Networks and Learning Systems},
  volume = {31},
  number = {10},
  pages = {4394--4400},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\DQK4DV2F\\Lei et al. - 2020 - Stochastic Gradient Descent for Nonconvex Learning.pdf}
}

@article{leimkuhler_ensemble_2018,
  title = {Ensemble Preconditioning for {{Markov}} Chain {{Monte Carlo}} Simulation},
  author = {Leimkuhler, Benedict and Matthews, Charles and Weare, Jonathan},
  year = {2018},
  month = mar,
  journal = {Statistics and Computing},
  volume = {28},
  number = {2},
  pages = {277--290},
  langid = {english},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\QVH4ZTA5\\Leimkuhler et al. - 2018 - Ensemble preconditioning for Markov chain Monte Ca.pdf}
}

@article{leimkuhler_ensemble_2018a,
  title = {Ensemble Preconditioning for {{Markov}} Chain {{Monte Carlo}} Simulation},
  author = {Leimkuhler, Benedict and Matthews, Charles and Weare, Jonathan},
  year = {2018},
  month = mar,
  journal = {Statistics and Computing},
  volume = {28},
  number = {2},
  pages = {277--290},
  langid = {english},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\JEH6GIMV\\Leimkuhler et al. - 2018 - Ensemble preconditioning for Markov chain Monte Ca.pdf}
}

@inproceedings{leskovec_graphs_2005,
  title = {Graphs over Time: Densification Laws, Shrinking Diameters and Possible Explanations},
  booktitle = {Proc. 11th {{ACM SIGKDD Int}}. {{Conf}}. {{Knowl}}. {{Discovery Data Mining}}},
  author = {Leskovec, Jure and Kleinberg, Jon and Faloutsos, Christos},
  year = {2005},
  series = {{{KDD}} '05},
  pages = {177--187},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  keywords = {densification power laws,graph generators,graph mining,heavy-tailed distributions,small-world phenomena}
}

@article{letham_constrained_2018,
  title = {Constrained {{Bayesian}} Optimization with Noisy Experiments},
  author = {Letham, Benjamin and Karrer, Brian and Ottoni, Guilherme and Bakshy, Eytan},
  year = {2018},
  month = aug,
  journal = {Bayesian Analysis},
  volume = {14},
  number = {2},
  pages = {495--519},
  file = {/home/msca8h/Documents/bayesian_optimization/Letham et al. - 2017 - Constrained Bayesian Optimization with Noisy Exper.pdf}
}

@inproceedings{lethunguyen_improving_2014,
  title = {Improving {{SMC}} Sampler Estimate by Recycling All Past Simulated Particles},
  booktitle = {2014 {{IEEE Workshop}} on {{Statistical Signal Processing}} ({{SSP}})},
  author = {Le Thu Nguyen, Thi and Septier, Francois and Peters, Gareth W. and Delignon, Yves},
  year = {2014},
  month = jun,
  pages = {117--120},
  publisher = {{IEEE}},
  address = {{Gold Coast, Australia}}
}

@inproceedings{li_almost_2020,
  title = {Almost Tune-Free Variance Reduction},
  booktitle = {Proceedings of the 37th {{International Conference}} on {{Machine Learning}}},
  author = {Li, Bingcong and Wang, Lingda and Giannakis, Georgios B.},
  year = {2020},
  month = nov,
  pages = {5969--5978},
  publisher = {{PMLR}},
  abstract = {The variance reduction class of algorithms including the representative ones, SVRG and SARAH, have well documented merits for empirical risk minimization problems. However, they require grid search to tune parameters (step size and the number of iterations per inner loop) for optimal performance. This work introduces `almost tune-free' SVRG and SARAH schemes equipped with i) Barzilai-Borwein (BB) step sizes; ii) averaging; and, iii) the inner loop length adjusted to the BB step sizes. In particular, SVRG, SARAH, and their BB variants are first reexamined through an `estimate sequence' lens to enable new averaging methods that tighten their convergence rates theoretically, and improve their performance empirically when the step size or the inner loop length is chosen large. Then a simple yet effective means to adjust the number of iterations per inner loop is developed to enhance the merits of the proposed averaging schemes and BB step sizes. Numerical tests corroborate the proposed methods.},
  langid = {english},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\8WKQNJXI\\Li et al. - 2020 - Almost Tune-Free Variance Reduction.pdf;C\:\\Users\\msca8h\\Zotero\\storage\\YLHMA4V9\\Li et al. - 2020 - Almost Tune-Free Variance Reduction.pdf}
}

@techreport{li_approximate_2017,
  title = {Approximate Inference with Amortised {{MCMC}}},
  author = {Li, Yingzhen and Turner, Richard E. and Liu, Qiang},
  year = {2017},
  month = may,
  number = {arXiv:1702.08343 [cs, stat]},
  eprint = {1702.08343},
  eprinttype = {arxiv},
  institution = {{ArXiv}},
  abstract = {We propose a novel approximate inference algorithm that approximates a target distribution by amortising the dynamics of a user-selected MCMC sampler. The idea is to initialise MCMC using samples from an approximation network, apply the MCMC operator to improve these samples, and finally use the samples to update the approximation network thereby improving its quality. This provides a new generic framework for approximate inference, allowing us to deploy highly complex, or implicitly defined approximation families with intractable densities, including approximations produced by warping a source of randomness through a deep neural network. Experiments consider image modelling with deep generative models as a challenging test for the method. Deep models trained using amortised MCMC are shown to generate realistic looking samples as well as producing diverse imputations for images with regions of missing pixels.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\YMZ2XQ9Q\\Li et al. - 2017 - Approximate Inference with Amortised MCMC.pdf;C\:\\Users\\msca8h\\Zotero\\storage\\YBE7ZLFP\\1702.html}
}

@incollection{li_communication_2014,
  title = {Communication {{Efficient Distributed Machine Learning}} with the {{Parameter Server}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 27},
  author = {Li, Mu and Andersen, David G and Smola, Alexander J and Yu, Kai},
  year = {2014},
  pages = {19--27},
  publisher = {{Curran Associates, Inc.}},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\BI58R6BL\\Li et al. - 2014 - Communication Efficient Distributed Machine Learni.pdf}
}

@inproceedings{li_compressing_2005,
  title = {Compressing and Companding High Dynamic Range Images with Subband Architectures},
  booktitle = {{{ACM SIGGRAPH}} 2005 {{Papers}} on   - {{SIGGRAPH}} '05},
  author = {Li, Yuanzhen and Sharan, Lavanya and Adelson, Edward H.},
  year = {2005},
  pages = {836},
  publisher = {{ACM Press}},
  address = {{Los Angeles, California}},
  langid = {english},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\7ZGC248T\\Li et al. - 2005 - Compressing and companding high dynamic range imag.pdf}
}

@inproceedings{li_connecting_2019,
  title = {On Connecting Stochastic Gradient {{MCMC}} and Differential Privacy},
  booktitle = {Proceedings of the {{Twenty-Second International Conference}} on {{Artificial Intelligence}} and {{Statistics}}},
  author = {Li, Bai and Chen, Changyou and Liu, Hao and Carin, Lawrence},
  year = {2019},
  month = apr,
  pages = {557--566},
  publisher = {{PMLR}},
  abstract = {Concerns related to data security and confidentiality have been raised when applying machine learning to real-world applications. Differential privacy provides a principled and rigorous privacy guarantee for machine learning models. While it is common to inject noise to design a model satisfying a required differential-privacy property, it is generally hard to balance the trade-off between privacy and utility. We show that stochastic gradient Markov chain Monte Carlo (SG-MCMC) \textendash{} a class of scalable Bayesian posterior sampling algorithms \textendash{} satisfies strong differential privacy, when carefully chosen stepsizes are employed. We develop theory on the performance of the proposed differentially-private SG-MCMC method. We conduct experiments to support our analysis, and show that a standard SG-MCMC sampler with minor modification can reach state-of-the-art performance in terms of both privacy and utility on Bayesian learning.},
  langid = {english},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\T5BMV8CP\\Li et al. - 2019 - On Connecting Stochastic Gradient MCMC and Differe.pdf;C\:\\Users\\msca8h\\Zotero\\storage\\XDBBGISV\\Li et al. - 2019 - On Connecting Stochastic Gradient MCMC and Differe.pdf}
}

@article{li_fusion_2019,
  title = {Fusion of Medical Sensors Using Adaptive Cloud Model in Local {{Laplacian}} Pyramid Domain},
  author = {Li, Weisheng and Du, Jiao and Zhao, Zhengmin and Long, Jinyi},
  year = {2019},
  month = apr,
  journal = {IEEE Transactions on Biomedical Engineering},
  volume = {66},
  number = {4},
  pages = {1172--1183}
}

@inproceedings{li_locality_1993,
  title = {Locality and Loop Scheduling on {{NUMA}} Multiprocessors},
  booktitle = {1993 {{International Conference}} on {{Parallel Processing}} - {{ICPP}}\textbackslash textquotesingle93 {{Vol2}}},
  author = {Li, Hui and Tandri, Sudarsan and Stumm, Michael and Sevcik, Kenneth C.},
  year = {1993},
  month = aug,
  publisher = {{IEEE}},
  file = {/home/msca8h/Documents/parallel_scheduling/Li et al. - 1993 - Locality and Loop Scheduling on NUMA Multiprocesso.pdf}
}

@inproceedings{li_machine_2009,
  title = {Machine Learning Based Online Performance Prediction for Runtime Parallelization and Task Scheduling},
  booktitle = {2009 {{IEEE International Symposium}} on {{Performance Analysis}} of {{Systems}} and {{Software}}},
  author = {Li, J. and Ma, X. and Singh, K. and Schulz, M. and de Supinski, B. R. and McKee, S. A.},
  year = {2009},
  month = apr,
  pages = {89--100},
  keywords = {Application software,Artificial Neural Networks,authoring languages,automatic performance prediction,Automatic Task Scheduling,Costs,Hardware,learning (artificial intelligence),Load management,machine learning,Machine learning,next-generation software,online performance prediction,online task partitioning,parallel programming,Parallel programming,Performance Prediction,pR framework,Predictive models,program execution,R language,Runtime,runtime parallelization,scheduling,Scheduling algorithm,scripting language,Scripting Languages,task analysis,task cost estimates,task scheduling,Testing},
  file = {/home/msca8h/Documents/parallel_scheduling/Li et al. - 2009 - Machine learning based online performance predicti.pdf}
}

@inproceedings{li_metis_2018,
  title = {Metis: {{Robustly Tuning Tail Latencies}} of {{Cloud Systems}}},
  booktitle = {2018 {{USENIX Annual Technical Conference}} ({{USENIX ATC}} 18)},
  author = {Li, Zhao Lucis and Liang, Chieh-Jan Mike and He, Wenjia and Zhu, Lianjie and Dai, Wenjun and Jiang, Jin and Sun, Guangzhong},
  year = {2018},
  month = jul,
  pages = {981--992},
  publisher = {{USENIX Association}},
  address = {{Boston, MA}}
}

@article{li_mixtures_2018,
  title = {Mixtures of $g$-{{Priors}} in {{Generalized Linear Models}}},
  author = {Li, Yingbo and Clyde, Merlise A.},
  year = {2018},
  month = oct,
  journal = {Journal of the American Statistical Association},
  volume = {113},
  number = {524},
  pages = {1828--1845},
  langid = {english},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\9MTIAIZ3\\Li and Clyde - 2018 - Mixtures of  g  -Priors in Generaliz.pdf}
}

@misc{li_neumann_2020,
  type = {Blog},
  title = {Von {{Neumann}}'s Trace Inequalities},
  author = {Li, Yingzhou},
  year = {2020},
  month = jul,
  journal = {Yingzhou Li}
}

@article{li_passive_2015,
  title = {Passive Multipath Time Delay Estimation Using {{MCMC}} Methods},
  author = {Li, Jing and Zhu, Jiandong and Feng, Zhihong and Zhao, Yongjun and Li, Donghai},
  year = {2015},
  month = dec,
  journal = {Circuits, Systems, and Signal Processing},
  volume = {34},
  number = {12},
  pages = {3897--3913},
  langid = {english},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\P5DHEIJ2\\Li et al. - 2015 - Passive Multipath Time Delay Estimation Using MCMC.pdf}
}

@article{li_performance_1998,
  title = {Performance Analysis of Forward-Backward Matched-Filterbank Spectral Estimators},
  author = {Li, H. and Li, J. and Stoica, P.},
  year = {1998},
  month = jul,
  journal = {IEEE Transactions on Signal Processing},
  volume = {46},
  number = {7},
  pages = {1954--1966}
}

@article{li_performance_2015,
  title = {Performance {{Analysis}} and {{Optimization}} for {{SpMV}} on {{GPU Using Probabilistic Modeling}}},
  author = {Li, Kenli and Yang, Wangdong and Li, Keqin},
  year = {2015},
  month = jan,
  journal = {IEEE Transactions on Parallel and Distributed Systems},
  volume = {26},
  number = {1},
  pages = {196--205}
}

@inproceedings{li_pipesgd_2018,
  title = {Pipe-{{SGD}}: {{A Decentralized Pipelined SGD Framework}} for {{Distributed Deep Net Training}}},
  booktitle = {Proceedings of the {{32Nd International Conference}} on {{Neural Information Processing Systems}}},
  author = {Li, Youjie and Yu, Mingchao and Li, Songze and Avestimehr, Salman and Kim, Nam Sung and Schwing, Alexander},
  year = {2018},
  series = {{{NIPS}}'18},
  pages = {8056--8067},
  publisher = {{Curran Associates Inc.}},
  address = {{USA}},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\C5KDQVMS\\Li et al. - 2018 - Pipe-SGD A Decentralized Pipelined SGD Framework .pdf}
}

@inproceedings{li_unified_2022,
  title = {A Unified Convergence Theorem for Stochastic Optimization Methods},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Li, Xiao and Milzarek, Andre},
  year = {2022},
  month = oct,
  abstract = {In this work, we provide a fundamental unified convergence theorem used for deriving expected and almost sure convergence results for a series of stochastic optimization methods. Our unified theorem only requires to verify several representative conditions and is not tailored to any specific algorithm. As a direct application, we recover expected and almost sure convergence results of the stochastic gradient method (SGD) and random reshuffling (RR) under more general settings. Moreover, we establish new expected and almost sure convergence results for the stochastic proximal gradient method (prox-SGD) and stochastic model-based methods for nonsmooth nonconvex optimization problems. These applications reveal that our unified theorem provides a plugin-type convergence analysis and strong convergence guarantees for a wide class of stochastic optimization methods.},
  langid = {english},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\8RDNUP9I\\Li and Milzarek - 2022 - A Unified Convergence Theorem for Stochastic Optim.pdf}
}

@article{li2018feature,
  title = {Feature Selection: {{A}} Data Perspective},
  author = {Li, Jundong and Cheng, Kewei and Wang, Suhang and Morstatter, Fred and Trevino, Robert P and Tang, Jiliang and Liu, Huan},
  year = {2018},
  journal = {ACM Computing Surveys},
  volume = {50},
  number = {6},
  pages = {94},
  publisher = {{ACM}}
}

@incollection{lian_asynchronous_2015,
  title = {Asynchronous {{Parallel Stochastic Gradient}} for {{Nonconvex Optimization}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 28},
  author = {Lian, Xiangru and Huang, Yijun and Li, Yuncheng and Liu, Ji},
  year = {2015},
  pages = {2737--2745},
  publisher = {{Curran Associates, Inc.}}
}

@incollection{lian_can_2017,
  title = {Can {{Decentralized Algorithms Outperform Centralized Algorithms}}? {{A Case Study}} for {{Decentralized Parallel Stochastic Gradient Descent}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 30},
  author = {Lian, Xiangru and Zhang, Ce and Zhang, Huan and Hsieh, Cho-Jui and Zhang, Wei and Liu, Ji},
  year = {2017},
  pages = {5330--5340},
  publisher = {{Curran Associates, Inc.}}
}

@article{lian_estimating_2010,
  title = {Estimating Unknown Clutter Intensity for {{PHD}} Filter},
  author = {Lian, Feng and Han, Chongzhao and Liu, Weifeng},
  year = {2010},
  month = oct,
  journal = {IEEE Transactions on Aerospace and Electronic Systems},
  volume = {46},
  number = {4},
  pages = {2066--2078}
}

@article{liang_cramerrao_2021,
  title = {Cram\'er-{{Rao}} Bound Analysis of Underdetermined Wideband {{DOA}} Estimation under the Subband Model via Frequency Decomposition},
  author = {Liang, Yibao and Cui, Wei and Shen, Qing and Liu, Wei and Wu, Siliang},
  year = {2021},
  journal = {IEEE Transactions on Signal Processing},
  volume = {69},
  pages = {4132--4148}
}

@article{liang_mixtures_2008,
  title = {Mixtures of $g$ {{Priors}} for {{Bayesian Variable Selection}}},
  author = {Liang, Feng and Paulo, Rui and Molina, German and Clyde, Merlise A and Berger, Jim O},
  year = {2008},
  month = mar,
  journal = {Journal of the American Statistical Association},
  volume = {103},
  number = {481},
  pages = {410--423},
  langid = {english}
}

@article{liang_sparse_2021,
  title = {Sparse {{Bayesian}} Learning Based Direction-of-Arrival Estimation under Spatially Colored Noise Using Acoustic Hydrophone Arrays},
  author = {Liang, Guolong and Shi, Zhibo and Qiu, Longhao and Sun, Sibo and Lan, Tian},
  year = {2021},
  month = jan,
  journal = {Journal of Marine Science and Engineering},
  volume = {9},
  number = {2},
  pages = {127},
  abstract = {Direction-of-arrival (DOA) estimation in a spatially isotropic white noise background has been widely researched for decades. However, in practice, such as underwater acoustic ambient noise in shallow water, the ambient noise can be spatially colored, which may severely degrade the performance of DOA estimation. To solve this problem, this paper proposes a DOA estimation method based on sparse Bayesian learning with the modified noise model using acoustic vector hydrophone arrays. Firstly, an applicable linear noise model is established by using the prolate spheroidal wave functions (PSWFs) to characterize spatially colored noise and exploiting the excellent performance of the PSWFs in extrapolating band-limited signals to the space domain. Then, using the proposed noise model, an iterative method for sparse spectrum reconstruction is developed under a sparse Bayesian learning (SBL) framework to fit the actual noise field received by the acoustic vector hydrophone array. Finally, a DOA estimation algorithm under the modified noise model is also presented, which has a superior performance under spatially colored noise. Numerical results validate the effectiveness of the proposed method.},
  langid = {english},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\IWTSGWH3\\Liang et al. - 2021 - Sparse Bayesian Learning Based Direction-of-Arriva.pdf}
}

@inproceedings{liao_fast_2006,
  title = {Fast and {{Adaptive Low-Pass Whitening Filters}} for {{Natural Images}}},
  booktitle = {Proceedings of the 13th {{International Conference}} on {{Neural Information Processing}} - {{Volume Part II}}},
  author = {Liao, Ling-Zhi and Luo, Si-Wei and Tian, Mei and Zhao, Lian-Wei},
  year = {2006},
  series = {{{ICONIP}}'06},
  pages = {343--352},
  publisher = {{Springer-Verlag}},
  address = {{Berlin, Heidelberg}}
}

@article{liao_sharpening_2019,
  title = {Sharpening {{Jensen}}'s Inequality},
  author = {Liao, J. G. and Berg, Arthur},
  year = {2019},
  month = jul,
  journal = {The American Statistician},
  volume = {73},
  number = {3},
  pages = {278--281},
  langid = {english},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\42BDW3KX\\Liao and Berg - 2019 - Sharpening Jensen's Inequality.pdf}
}

@article{lift-tensorforce,
  title = {{{LIFT}}: {{Reinforcement}} Learning in Computer Systems by Learning from Demonstrations},
  author = {Schaarschmidt, Michael and Kuhnle, Alexander and Ellis, Ben and Fricke, Kai and Gessert, Felix and Yoneki, Eiko},
  year = {2018},
  journal = {CoRR},
  volume = {abs/1808.07903},
  eprint = {1808.07903},
  eprinttype = {arxiv},
  archiveprefix = {arXiv}
}

@article{lilja_dynamic_2000,
  title = {Dynamic Task Scheduling Using Online Optimization},
  author = {Lilja, D.J. and {Lau Ying Kit} and Hamidzadeh, B.},
  year = {Nov./2000},
  journal = {IEEE Transactions on Parallel and Distributed Systems},
  volume = {11},
  number = {11},
  pages = {1151--1163},
  file = {/home/msca8h/Documents/parallel_scheduling/Lilja et al. - 2000 - Dynamic task scheduling using online optimization.pdf}
}

@inproceedings{lim_autotuning_2017,
  title = {Autotuning {{GPU Kernels}} via {{Static}} and {{Predictive Analysis}}},
  booktitle = {2017 46th {{International Conference}} on {{Parallel Processing}} ({{ICPP}})},
  author = {Lim, Robert and Norris, Boyana and Malony, Allen},
  year = {2017},
  month = aug,
  pages = {523--532},
  publisher = {{IEEE}},
  address = {{Bristol, United Kingdom}},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\6MMUNZHQ\\Lim et al. - 2017 - Autotuning GPU Kernels via Static and Predictive A.pdf}
}

@inproceedings{lin_deep_2018,
  title = {Deep {{Gradient Compression}}: {{Reducing}} the {{Communication Bandwidth}} for {{Distributed Training}}},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {Lin, Yujun and Han, Song and Mao, Huizi and Wang, Yu and Dally, Bill},
  year = {2018}
}

@article{lin_don_2018,
  title = {Don't {{Use Large Mini-Batches}}, {{Use Local SGD}}},
  author = {Lin, Tao and Stich, Sebastian U. and Patel, Kumar Kshitij and Jaggi, Martin},
  year = {2018},
  month = aug,
  journal = {arXiv:1808.07217 [cs, stat]},
  eprint = {1808.07217},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Mini-batch stochastic gradient methods (SGD) are state of the art for distributed training of deep neural networks. Drastic increases in the mini-batch sizes have lead to key efficiency and scalability gains in recent years. However, progress faces a major roadblock, as models trained with large batches often do not generalize well. Local SGD can offer the same communication vs. computation pattern as mini-batch SGD---thus is as efficient as mini-batch SGD from a systems perspective---but instead of performing a single large-batch update in each round, it performs several local parameter updates sequentially. We extensively study the communication efficiency vs. performance trade-offs associated with local SGD and provide a new variant, called \textbackslash emph\{post-local SGD\}. We show that it significantly improves the generalization performance compared to large-batch training and converges to flatter minima.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\2FMPE9SP\\Lin et al. - 2018 - Don't Use Large Mini-Batches, Use Local SGD.pdf;C\:\\Users\\msca8h\\Zotero\\storage\\M8RMGBTP\\1808.html}
}

@inproceedings{lin_online_2013,
  title = {Online Learning of Nonparametric Mixture Models via Sequential Variational Approximation},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Lin, Dahua},
  year = {2013},
  volume = {26},
  publisher = {{Curran Associates, Inc.}}
}

@inproceedings{liu_almost_2022,
  title = {On Almost Sure Convergence Rates of Stochastic Gradient Methods},
  booktitle = {Proceedings of  the {{Conference}} on {{Learning Theory}}},
  author = {Liu, Jun and Yuan, Ye},
  year = {2022},
  month = jun,
  series = {{{PMLR}}},
  volume = {178},
  pages = {2963--2983},
  publisher = {{ML Research Press}},
  abstract = {The vast majority of convergence rates analysis for stochastic gradient methods in the literature focus on convergence in expectation, whereas trajectory-wise almost sure convergence is clearly important to ensure that any instantiation of the stochastic algorithms would converge with probability one. Here we provide a unified almost sure convergence rates analysis for stochastic gradient descent (SGD), stochastic heavy-ball (SHB), and stochastic Nesterov's accelerated gradient (SNAG) methods.  We show, for the first time, that the almost sure convergence rates obtained for these stochastic gradient methods on strongly convex functions, are arbitrarily close to their optimal convergence rates possible. For non-convex objective functions, we not only show that a weighted average of the squared gradient norms converges to zero almost surely, but also the last iterates of the algorithms. We further provide last-iterate almost sure convergence rates analysis for stochastic gradient methods on weakly convex smooth functions, in contrast with most existing results in the literature that only provide convergence in expectation for a weighted average of the iterates.},
  langid = {english},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\VI9H7NQB\\Liu and Yuan - 2022 - On Almost Sure Convergence Rates of Stochastic Gra.pdf}
}

@article{liu_autotuning_2019,
  title = {An {{Autotuning Protocol}} to {{Rapidly Build Autotuners}}},
  author = {Liu, Junhong and Tan, Guangming and Luo, Yulong and Li, Jiajia and Mo, Zeyao and Sun, Ninghui},
  year = {2019},
  month = jan,
  journal = {ACM Transactions on Parallel Computing},
  volume = {5},
  number = {2},
  pages = {1--25},
  langid = {english}
}

@inproceedings{liu_blackbox_2016,
  title = {Black-Box {{Importance Sampling}}},
  booktitle = {The 19th {{International Conference}} on {{Artificial Intelligence}} and {{Statistics}}, {{AISTATS}}},
  author = {Liu, Qiang and Lee, Jason D.},
  year = {2016},
  month = oct,
  eprint = {1610.05247},
  eprinttype = {arxiv},
  abstract = {Importance sampling is widely used in machine learning and statistics, but its power is limited by the restriction of using simple proposals for which the importance weights can be tractably calculated. We address this problem by studying black-box importance sampling methods that calculate importance weights for samples generated from any unknown proposal or black-box mechanism. Our method allows us to use better and richer proposals to solve difficult problems, and (somewhat counter-intuitively) also has the additional benefit of improving the estimation accuracy beyond typical importance sampling. Both theoretical and empirical analyses are provided.},
  archiveprefix = {arXiv},
  keywords = {Statistics - Machine Learning},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\326KUNX3\\Liu and Lee - 2016 - Black-box Importance Sampling.pdf;C\:\\Users\\msca8h\\Zotero\\storage\\FSWKHSFB\\1610.html}
}

@article{liu_collapsed_1994,
  title = {The Collapsed {{Gibbs}} Sampler in Bayesian Computations with Applications to a Gene Regulation Problem},
  author = {Liu, Jun S.},
  year = {1994},
  month = sep,
  journal = {Journal of the American Statistical Association},
  volume = {89},
  number = {427},
  pages = {958--966},
  langid = {english}
}

@misc{liu_convergence_2022,
  title = {On the Convergence of {{AdaGrad}} on $\mathbb{R}^{d}$: {{Beyond}} Convexity, Non-Asymptotic Rate and Acceleration},
  shorttitle = {On the {{Convergence}} of {{AdaGrad}} on \$\textbackslash{{R}}\^\{d\}\$},
  author = {Liu, Zijian and Nguyen, Ta Duy and Ene, Alina and Nguyen, Huy L.},
  year = {2022},
  month = sep,
  number = {arXiv:2209.14827},
  eprint = {2209.14827},
  eprinttype = {arxiv},
  primaryclass = {cs},
  institution = {{arXiv}},
  abstract = {Existing analysis of AdaGrad and other adaptive methods for smooth convex optimization is typically for functions with bounded domain diameter. In unconstrained problems, previous works guarantee an asymptotic convergence rate without an explicit constant factor that holds true for the entire function class. Furthermore, in the stochastic setting, only a modified version of AdaGrad, different from the one commonly used in practice, in which the latest gradient is not used to update the stepsize, has been analyzed. Our paper aims at bridging these gaps and developing a deeper understanding of AdaGrad and its variants in the standard setting of smooth convex functions as well as the more general setting of quasar convex functions. First, we demonstrate new techniques to explicitly bound the convergence rate of the vanilla AdaGrad for unconstrained problems in both deterministic and stochastic settings. Second, we propose a variant of AdaGrad for which we can show the convergence of the last iterate, instead of the average iterate. Finally, we give new accelerated adaptive algorithms and their convergence guarantee in the deterministic setting with explicit dependency on the problem parameters, improving upon the asymptotic rate shown in previous works.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Data Structures and Algorithms,Computer Science - Machine Learning},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\VJZCE2IH\\Liu et al. - 2022 - On the Convergence of AdaGrad on $R^ d $ Beyond .pdf;C\:\\Users\\msca8h\\Zotero\\storage\\AY5B24SA\\2209.html}
}

@inproceedings{liu_differentiable_2019,
  title = {Differentiable {{Kernel Evolution}}},
  booktitle = {2019 {{IEEE}}/{{CVF International Conference}} on {{Computer Vision}} ({{ICCV}})},
  author = {Liu, Yu and Liu, Jihao and Wang, Xiaogang and Zeng, Ailing},
  year = {2019},
  month = oct,
  pages = {1834--1843},
  publisher = {{IEEE}},
  address = {{Seoul, Korea (South)}}
}

@article{liu_directionofarrival_2011,
  title = {Direction-of-Arrival Estimation of Wideband Signals via Covariance Matrix Sparse Representation},
  author = {Liu, Zhang-Meng and Huang, Zhi-Tao and Zhou, Yi-Yu},
  year = {2011},
  month = sep,
  journal = {IEEE Transactions on Signal Processing},
  volume = {59},
  number = {9},
  pages = {4256--4270}
}

@article{liu_directionofarrival_2011a,
  title = {Direction-of-Arrival Estimation of Wideband Signals via Covariance Matrix Sparse Representation},
  author = {Liu, Zhang-Meng and Huang, Zhi-Tao and Zhou, Yi-Yu},
  year = {2011},
  month = sep,
  journal = {IEEE Transactions on Signal Processing},
  volume = {59},
  number = {9},
  pages = {4256--4270}
}

@article{liu_efficient_2012,
  title = {An Efficient Maximum Likelihood Method for Direction-of-Arrival Estimation via Sparse {{Bayesian}} Learning},
  author = {Liu, Zhang-Meng and Huang, Zhi-Tao and Zhou, Yi-Yu},
  year = {2012},
  month = oct,
  journal = {IEEE Transactions on Wireless Communications},
  volume = {11},
  number = {10},
  pages = {1--11}
}

@article{liu_evolving_2020,
  title = {Evolving {{Normalization-Activation Layers}}},
  author = {Liu, Hanxiao and Brock, Andrew and Simonyan, Karen and Le, Quoc V.},
  year = {2020},
  month = apr,
  journal = {arXiv:2004.02967 [cs, stat]},
  eprint = {2004.02967},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Normalization layers and activation functions are critical components in deep neural networks that frequently co-locate with each other. Instead of designing them separately, we unify them into a single computation graph, and evolve its structure starting from low-level primitives. Our layer search algorithm leads to the discovery of EvoNorms, a set of new normalization-activation layers that go beyond existing design patterns. Several of these layers enjoy the property of being independent from the batch statistics. Our experiments show that EvoNorms not only excel on a variety of image classification models including ResNets, MobileNets and EfficientNets, but also transfer well to Mask R-CNN for instance segmentation and BigGAN for image synthesis, outperforming BatchNorm and GroupNorm based layers by a significant margin in many cases.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\EVDUUA7Q\\Liu et al. - 2020 - Evolving Normalization-Activation Layers.pdf;C\:\\Users\\msca8h\\Zotero\\storage\\EWGJGHGH\\2004.html}
}

@article{liu_generalized_2021,
  title = {A Generalized Framework for Edge-Preserving and Structure-Preserving Image Smoothing},
  author = {Liu, Wei and Zhang, Pingping and Lei, Yinjie and Huang, Xiaolin and Yang, Jie and Ng, Michael Kwok-Po},
  year = {2021},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  pages = {1--1},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\MMERAGRC\\Liu et al. - 2021 - A Generalized Framework for Edge-preserving and St.pdf}
}

@inproceedings{liu_improved_2020,
  title = {An Improved Analysis of Stochastic Gradient Descent with Momentum},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Liu, Yanli and Gao, Yuan and Yin, Wotao},
  year = {2020},
  volume = {33},
  pages = {18261--18271},
  publisher = {{Curran Associates, Inc.}},
  abstract = {SGD with momentum (SGDM) has been widely applied in many machine learning tasks, and it is often applied with dynamic stepsizes and momentum weights tuned in a stagewise manner. Despite of its empirical advantage over SGD, the role of momentum is still unclear in general since previous analyses on SGDM either provide worse convergence bounds than those of SGD, or assume Lipschitz or quadratic objectives, which fail to hold in practice. Furthermore, the role of dynamic parameters has not been addressed. In this work, we show that SGDM converges as fast as SGD for smooth objectives under both strongly convex and nonconvex settings. We also prove that multistage strategy is beneficial for SGDM compared to using fixed parameters. Finally, we verify these theoretical claims by numerical experiments.},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\YE78UIPG\\Liu et al. - 2020 - An Improved Analysis of Stochastic Gradient Descen.pdf}
}

@article{liu_limited_1989,
  title = {On the Limited Memory {{BFGS}} Method for Large Scale Optimization},
  author = {Liu, Dong C. and Nocedal, Jorge},
  year = {1989},
  month = aug,
  journal = {Mathematical Programming},
  volume = {45},
  number = {1-3},
  pages = {503--528},
  langid = {english},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\26UTS3KN\\Liu and Nocedal - 1989 - On the limited memory BFGS method for large scale .pdf}
}

@article{liu_metropolized_1996,
  title = {Metropolized Independent Sampling with Comparisons to Rejection Sampling and Importance Sampling},
  author = {Liu, Jun S.},
  year = {1996},
  month = jun,
  journal = {Statistics and Computing},
  volume = {6},
  number = {2},
  pages = {113--119},
  langid = {english}
}

@article{liu_quasimonte_2021,
  title = {Quasi-{{Monte Carlo}} Quasi-{{Newton}} in {{Variational Bayes}}},
  author = {Liu, Sifan and Owen, Art B.},
  year = {2021},
  journal = {Journal of Machine Learning Research},
  volume = {22},
  number = {243},
  pages = {1--23},
  abstract = {Many machine learning problems optimize an objective that must be measured with noise. The primary method is a first order stochastic gradient descent using one or more Monte Carlo (MC) samples at each step. There are settings where ill-conditioning makes second order methods such as limited-memory Broyden-Fletcher-Goldfarb-Shanno (L-BFGS) more effective. We study the use of randomized quasi-Monte Carlo (RQMC) sampling for such problems. When MC sampling has a root mean squared error (RMSE) of  O( n -1/2 ) O(n-1/2)  then RQMC has an RMSE of  o( n -1/2 ) o(n-1/2)  that can be close to  O( n -3/2 ) O(n-3/2)  in favorable settings. We prove that improved sampling accuracy translates directly to improved optimization. In our empirical investigations for variational Bayes, using RQMC with stochastic quasi-Newton method greatly speeds up the optimization, and sometimes finds a better parameter value than MC does.},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\G222TV3W\\Liu and Owen - 2021 - Quasi-Monte Carlo Quasi-Newton in Variational Baye.pdf}
}

@article{liu_safe_1994,
  title = {Safe Self-Scheduling: A Parallel Loop Scheduling Scheme for Shared-Memory Multiprocessors},
  author = {Liu, Jie and Saletore, Vikram A and Lewis, Ted G},
  year = {1994},
  journal = {International Journal of Parallel Programming},
  volume = {22},
  number = {6},
  pages = {589--616},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\VPWVZP4K\\Liu et al. - 1994 - Safe self-scheduling a parallel loop scheduling s.pdf}
}

@article{liu_sequential_1998,
  title = {Sequential {{Monte Carlo Methods}} for {{Dynamic Systems}}},
  author = {Liu, Jun S. and Chen, Rong},
  year = {1998},
  month = sep,
  journal = {Journal of the American Statistical Association},
  volume = {93},
  number = {443},
  pages = {1032--1044},
  langid = {english}
}

@inproceedings{liu_stein_2016,
  title = {Stein {{Variational Gradient Descent}}: {{A General Purpose Bayesian Inference Algorithm}}},
  booktitle = {Proceedings of the 30th {{International Conference}} on {{Neural Information Processing Systems}}},
  author = {Liu, Qiang and Wang, Dilin},
  year = {2016},
  series = {{{NIPS}}'16},
  pages = {2378--2386},
  publisher = {{Curran Associates Inc.}},
  address = {{USA}}
}

@article{liu_unbiased_2017,
  title = {An {{Unbiased MCMC FPGA-Based Accelerator}} in the {{Land}} of {{Custom Precision Arithmetic}}},
  author = {Liu, Shuanglong and Mingas, Grigorios and Bouganis, Christos-Savvas},
  year = {2017},
  month = may,
  journal = {IEEE Transactions on Computers},
  volume = {66},
  number = {5},
  pages = {745--758},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\Y83FNACA\\Liu et al. - 2017 - An Unbiased MCMC FPGA-Based Accelerator in the Lan.pdf}
}

@inproceedings{liu_understanding_2019,
  title = {Understanding and {{Accelerating Particle-Based Variational Inference}}},
  booktitle = {Proceedings of the 36th {{International Conference}} on {{Machine Learning}}},
  author = {Liu, Chang and Zhuo, Jingwei and Cheng, Pengyu and Zhang, Ruiyi and Zhu, Jun},
  year = {2019},
  month = jun,
  series = {Proceedings of {{Machine Learning Research}}},
  volume = {97},
  pages = {4082--4092},
  publisher = {{PMLR}},
  address = {{Long Beach, California, USA}},
  abstract = {Particle-based variational inference methods (ParVIs) have gained attention in the Bayesian inference literature, for their capacity to yield flexible and accurate approximations. We explore ParVIs from the perspective of Wasserstein gradient flows, and make both theoretical and practical contributions. We unify various finite-particle approximations that existing ParVIs use, and recognize that the approximation is essentially a compulsory smoothing treatment, in either of two equivalent forms. This novel understanding reveals the assumptions and relations of existing ParVIs, and also inspires new ParVIs. We propose an acceleration framework and a principled bandwidth-selection method for general ParVIs; these are based on the developed theory and leverage the geometry of the Wasserstein space. Experimental results show the improved convergence by the acceleration framework and enhanced sample accuracy by the bandwidth-selection method.}
}

@article{liu_when_2018,
  title = {When {{Gaussian Process Meets Big Data}}: {{A Review}} of {{Scalable GPs}}},
  author = {Liu, Haitao and Ong, Yew-Soon and Shen, Xiaobo and Cai, Jianfei},
  year = {2018},
  journal = {arXiv preprint arXiv:1807.01065},
  eprint = {1807.01065},
  eprinttype = {arxiv},
  archiveprefix = {arXiv}
}

@article{livingstone_geometric_2019,
  title = {On the Geometric Ergodicity of {{Hamiltonian Monte Carlo}}},
  author = {Livingstone, Samuel and Betancourt, Michael and Byrne, Simon and Girolami, Mark},
  year = {2019},
  month = nov,
  journal = {Bernoulli},
  volume = {25},
  number = {4A},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\CFXYBWN7\\Livingstone et al. - 2019 - On the geometric ergodicity of Hamiltonian Monte C.pdf}
}

@article{llorente_marginal_2022,
  title = {Marginal Likelihood Computation for Model Selection and Hypothesis Testing: An Extensive Review},
  shorttitle = {Marginal Likelihood Computation for Model Selection and Hypothesis Testing},
  author = {Llorente, Fernando and Martino, Luca and Delgado, David and {Lopez-Santiago}, Javier},
  year = {2022},
  month = jan,
  journal = {SIAM Review (in press)},
  eprint = {2005.08334},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {This is an up-to-date introduction to, and overview of, marginal likelihood computation for model selection and hypothesis testing. Computing normalizing constants of probability models (or ratio of constants) is a fundamental issue in many applications in statistics, applied mathematics, signal processing and machine learning. This article provides a comprehensive study of the state-of-the-art of the topic. We highlight limitations, benefits, connections and differences among the different techniques. Problems and possible solutions with the use of improper priors are also described. Some of the most relevant methodologies are compared through theoretical comparisons and numerical experiments.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Computation},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\LI9SHM45\\Llorente et al. - 2022 - Marginal likelihood computation for model selectio.pdf;C\:\\Users\\msca8h\\Zotero\\storage\\F3UBLQ98\\2005.html}
}

@article{llorente_safe_2022,
  title = {On the Safe Use of Prior Densities for {{Bayesian}} Model Selection},
  author = {Llorente, F. and Martino, L. and Curbelo, E. and {Lopez-Santiago}, J. and Delgado, D.},
  year = {2022},
  month = jun,
  journal = {WIREs Computational Statistics (in press)},
  eprint = {2206.05210},
  eprinttype = {arxiv},
  primaryclass = {stat},
  abstract = {The application of Bayesian inference for the purpose of model selection is very popular nowadays. In this framework, models are compared through their marginal likelihoods, or their quotients, called Bayes factors. However, marginal likelihoods depends on the prior choice. For model selection, even diffuse priors can be actually very informative, unlike for the parameter estimation problem. Furthermore, when the prior is improper, the marginal likelihood of the corresponding model is undetermined. In this work, we discuss the issue of prior sensitivity of the marginal likelihood and its role in model selection. We also comment on the use of uninformative priors, which are very common choices in practice. Several practical suggestions are discussed and many possible solutions, proposed in the literature, to design objective priors for model selection are described. Some of them also allow the use of improper priors. The connection between the marginal likelihood approach and the well-known information criteria is also presented. We describe the main issues and possible solutions by illustrative numerical examples, providing also some related code. One of them involving a real-world application on exoplanet detection.},
  archiveprefix = {arXiv},
  keywords = {Statistics - Computation,Statistics - Machine Learning,Statistics - Methodology},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\VY4KRGVV\\Llorente et al. - 2022 - On the safe use of prior densities for Bayesian mo.pdf;C\:\\Users\\msca8h\\Zotero\\storage\\95VLUVBK\\2206.html}
}

@inproceedings{loggins_comparison_2001,
  title = {A Comparison of Forward-Looking Sonar Design Alternatives},
  booktitle = {Proceedings of {{MTS}}/{{IEEE Oceans}} 2001},
  author = {Loggins, C.D.},
  year = {2001},
  series = {{{OCEANS}}'01},
  volume = {3},
  pages = {1536--1545},
  publisher = {{Marine Technol. Soc}},
  address = {{Honolulu, HI, USA}},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\EQZK2ACK\\Loggins - 2001 - A comparison of forward-looking sonar design alter.pdf}
}

@article{loizou_comparative_2005,
  title = {Comparative Evaluation of Despeckle Filtering in Ultrasound Imaging of the Carotid Artery},
  author = {Loizou, C.P. and Pattichis, C.S. and Christodoulou, C.I. and Istepanian, R.S.H. and Pantziaris, M. and Nicolaides, A.},
  year = {2005},
  month = oct,
  journal = {IEEE Transactions on Ultrasonics, Ferroelectrics and Frequency Control},
  volume = {52},
  number = {10},
  pages = {1653--1669},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\EHHSXC9U\\Loizou et al. - 2005 - Comparative evaluation of despeckle filtering in u.pdf}
}

@article{loizou_quality_2006,
  title = {Quality Evaluation of Ultrasound Imaging in the Carotid Artery Based on Normalization and Speckle Reduction Filtering},
  author = {Loizou, C. P. and Pattichis, C. S. and Pantziaris, M. and Tyllis, T. and Nicolaides, A.},
  year = {2006},
  month = may,
  journal = {Medical \& Biological Engineering \& Computing},
  volume = {44},
  number = {5},
  pages = {414--426},
  langid = {english},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\C778CPS3\\Loizou et al. - 2006 - Quality evaluation of ultrasound imaging in the ca.pdf}
}

@inproceedings{loizou_stochastic_2021,
  title = {Stochastic {{Polyak}} Step-Size for {{SGD}}: {{An}} Adaptive Learning Rate for Fast Convergence},
  shorttitle = {Stochastic Polyak Step-Size for {{SGD}}},
  booktitle = {Proceedings of {{The International Conference}} on {{Artificial Intelligence}} and {{Statistics}}},
  author = {Loizou, Nicolas and Vaswani, Sharan and Laradji, Issam Hadj and {Lacoste-Julien}, Simon},
  year = {2021},
  month = mar,
  series = {{{PMLR}}},
  pages = {1306--1314},
  publisher = {{ML Research Press}},
  abstract = {We propose a stochastic variant of the classical Polyak step-size (Polyak, 1987) commonly used in the subgradient method. Although computing the Polyak step-size requires knowledge of the optimal function values, this information is readily available for typical modern machine learning applications. Consequently, the proposed stochastic Polyak step-size (SPS) is an attractive choice for setting the learning rate for stochastic gradient descent (SGD). We provide theoretical convergence guarantees for SGD equipped with SPS in different settings, including strongly convex, convex and non-convex functions. Furthermore, our analysis results in novel convergence guarantees for SGD with a constant step-size. We show that SPS is particularly effective when training over-parameterized models capable of interpolating the training data. In this setting, we prove that SPS enables SGD to converge to the true solution at a fast rate without requiring the knowledge of any problem-dependent constants or additional computational overhead. We experimentally validate our theoretical results via extensive experiments on synthetic and real datasets. We demonstrate the strong performance of SGD with SPS compared to state-of-the-art optimization methods when training over-parameterized models.},
  langid = {english},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\64N4RJME\\Loizou et al. - 2021 - Stochastic Polyak Step-size for SGD An Adaptive L.pdf;C\:\\Users\\msca8h\\Zotero\\storage\\IJPX2R7I\\Loizou et al. - 2021 - Stochastic Polyak Step-size for SGD An Adaptive L.pdf}
}

@article{lorenz_neuroadaptive_2017,
  title = {Neuroadaptive {{Bayesian Optimization}} and {{Hypothesis Testing}}.},
  author = {Lorenz, R. and Hampshire, A. and Leech, R.},
  year = {2017},
  journal = {Trends in cognitive sciences},
  volume = {21},
  number = {3},
  pages = {155--167},
  abstract = {Cognitive neuroscientists are often interested in broad research questions, yet use overly narrow experimental designs by considering only a small subset of possible experimental conditions. This limits the generalizability and reproducibility of many research findings. Here, we propose an alternative approach that resolves these problems by taking advantage of recent developments in real-time data analysis and machine learning. Neuroadaptive Bayesian optimization is a powerful strategy to efficiently explore more experimental conditions than is currently possible with standard methodology. We argue that such an approach could broaden the hypotheses considered in cognitive science, improving the generalizability of findings. In addition, Bayesian optimization can be combined with preregistration to cover exploration, mitigating researcher bias more broadly and improving reproducibility.},
  keywords = {bayes theorem,cognition,generalization,humans,models,psychology,reproducibility of results,statistical},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\GYNTNE2H\\Neuroadaptive Bayesian Optimization.pdf}
}

@inproceedings{lorenz_tailoring_2016,
  title = {Towards Tailoring Non-Invasive Brain Stimulation Using Real-Time {{fMRI}} and {{Bayesian}} Optimization},
  booktitle = {2016 {{International Workshop}} on {{Pattern Recognition}} in {{Neuroimaging}} ({{PRNI}})},
  author = {Lorenz, Romy and Monti, Ricardo P and Hampshire, Adam and Koush, Yury and Anagnostopoulos, Christoforos and Faisal, Aldo A and Sharp, David and Montana, Giovanni and Leech, Robert and Violante, Ines R},
  year = {2016},
  month = jun,
  pages = {1--4},
  publisher = {{IEEE}},
  address = {{Trento, Italy}},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\N2VBCKZV\\Lorenz et al. - 2016 - Towards tailoring non-invasive brain stimulation u.pdf}
}

@article{lu_complex_2020,
  title = {Complex {{Convolutional Neural Networks}} for {{Ultrasound Image Reconstruction}} from {{In-Phase}}/{{Quadrature Signal}}},
  author = {Lu, Jingfeng and Millioz, Fabien and Garcia, Damien and Salles, Sebastien and Ye, Dong and Friboulet, Denis},
  year = {2020},
  month = sep,
  journal = {arXiv:2009.11536 [eess]},
  eprint = {2009.11536},
  eprinttype = {arxiv},
  primaryclass = {eess},
  abstract = {A wide variety of studies based on deep learning have recently been investigated to improve ultrasound (US) imaging. Most of these approaches were performed on radio frequency (RF) signals. However, inphase/quadrature (I/Q) digital beamformers (IQBF) are now widely used as low-cost strategies. In this work, we leveraged complex convolutional neural networks (CCNNs) for reconstructing ultrasound images from I/Q signals. We recently described a CNN architecture called ID-Net, which exploited an inception layer devoted to the reconstruction of RF diverging-wave (DW) ultrasound images. We derived in this work the complex equivalent of this network, i.e., the complex inception for DW network (CID-Net), operating on I/Q data. We provided experimental evidence that the CID-Net yields the same image quality as that obtained from the RF-trained CNNs; i.e., by using only three I/Q images, the CID-Net produced high-quality images competing with those obtained by coherently compounding 31 RF images. Moreover, we showed that the CID-Net outperforms the straightforward architecture consisting in processing separately the real and imaginary parts of the I/Q signal, indicating thereby the importance of consistently processing the I/Q signals using a network that exploits the complex nature of such signal.},
  archiveprefix = {arXiv},
  keywords = {Electrical Engineering and Systems Science - Image and Video Processing,Electrical Engineering and Systems Science - Signal Processing},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\8THCRWB8\\Lu et al. - 2020 - Complex Convolutional Neural Networks for Ultrasou.pdf;C\:\\Users\\msca8h\\Zotero\\storage\\9BT532PM\\2009.html}
}

@article{lu_robust_2011,
  title = {Robust Expectation\textendash Maximization Direction-of-Arrival Estimation Algorithm for Wideband Source Signals},
  author = {Lu, Lu and Wu, Hsiao-Chun},
  year = {2011},
  month = jun,
  journal = {IEEE Transactions on Vehicular Technology},
  volume = {60},
  number = {5},
  pages = {2395--2400},
  abstract = {Direction-of-arrival (DOA) estimation for wideband source signals using far-field acoustic sensors has recently drawn much research interest. A wide variety of DOA estimation approaches are based on the maximum-likelihood objective. In this paper, we tackle the DOA estimation problem based on the realistic assumption that the sources are corrupted by spatially nonwhite noise. We explore the respective limitations of two popular DOA methods to solve this problem-the stepwise-concentrated maximum-likelihood (SC-ML) and approximately concentrated maximum-likelihood (AC-ML) algorithms-and design a novel expectation-maximization (EM) algorithm. In addition, we provide the Cramer-Rao lower bound (CRLB) and the computational-complexity analyses for the aforementioned DOA estimation schemes. Through Monte Carlo simulations and our derived CRLB and computational-complexity analyses, it is demonstrated that our proposed EM algorithm outperforms the SC-ML and AC-ML methods in terms of the DOA estimation accuracy and computational complexity.},
  keywords = {Algorithm design and analysis,Direction of arrival estimation,Direction-of-arrival (DOA),expectationmaximization (EM) algorithm,maximum likelihood,Maximum likelihood estimation,Noise,Signal processing algorithms,Wideband},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\CJXZA3GZ\\5742729.html}
}

@article{lucas2019understanding,
  title = {Understanding Posterior Collapse in Generative Latent Variable Models},
  author = {Lucas, James and Tucker, George and Grosse, Roger and Norouzi, Mohammad},
  year = {2019}
}

@inproceedings{lucco_dynamic_1992,
  title = {A Dynamic Scheduling Method for Irregular Parallel Programs},
  booktitle = {Proc. {{ACM SIGPLAN}} 1992 {{Conf}}. {{Program}}. {{Lang}}. {{Des}}. {{Implementation}}},
  author = {Lucco, Steven},
  year = {1992},
  series = {{{PLDI}} '92},
  pages = {200--211},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  keywords = {Tapering},
  file = {/home/msca8h/Documents/parallel_scheduling/Lucco - 1992 - A Dynamic Scheduling Method for Irregular Parallel.pdf}
}

@article{luke_optimal_1998,
  title = {The Optimal Effectiveness Metric for Parallel Application Analysis},
  author = {Luke, Edward A. and Banicescu, Ioana and Li, Jin},
  year = {1998},
  month = jun,
  journal = {Information Processing Letters},
  volume = {66},
  number = {5},
  pages = {223--229},
  langid = {english},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\SW5CF2ZP\\Luke et al. - 1998 - The optimal effectiveness metric for parallel appl.pdf}
}

@inproceedings{luo_learning_2017,
  title = {Learning {{Deep Architectures}} via {{Generalized Whitened Neural Networks}}},
  booktitle = {Proceedings of the 34th {{International Conference}} on {{Machine Learning}}},
  author = {Luo, Ping},
  year = {2017},
  month = aug,
  series = {Proceedings of {{Machine Learning Research}}},
  volume = {70},
  pages = {2238--2246},
  publisher = {{PMLR}},
  address = {{International Convention Centre, Sydney, Australia}},
  abstract = {Whitened Neural Network (WNN) is a recent advanced deep architecture, which improves convergence and generalization of canonical neural networks by whitening their internal hidden representation. However, the whitening transformation increases computation time. Unlike WNN that reduced runtime by performing whitening every thousand iterations, which degenerates convergence due to the ill conditioning, we present generalized WNN (GWNN), which has three appealing properties. First, GWNN is able to learn compact representation to reduce computations. Second, it enables whitening transformation to be performed in a short period, preserving good conditioning. Third, we propose a data-independent estimation of the covariance matrix to further improve computational efficiency. Extensive experiments on various datasets demonstrate the benefits of GWNN.}
}

@inproceedings{lyu_multiobjective_2018,
  title = {Multi-Objective {{Bayesian}} Optimization for Analog/{{RF}} Circuit Synthesis},
  booktitle = {Proc. {{ACM}}/{{ESDA}}/{{IEEE Des}}. {{Automat}}. {{Conf}}.},
  author = {Lyu, Wenlong and Yang, Fan and Yan, Changhao and Zhou, Dian and Zeng, Xuan},
  year = {2018},
  month = jun,
  pages = {1--6},
  publisher = {{IEEE}},
  address = {{San Francisco, CA}}
}

@incollection{m._ray_2013,
  title = {Ray Trace Modeling of Underwater Sound Propagation},
  booktitle = {Modeling and {{Measurement Methods}} for {{Acoustic Waves}} and for {{Acoustic Microdevices}}},
  author = {M., Jens},
  year = {2013},
  month = aug,
  publisher = {{InTech}},
  langid = {english},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\6QYC7M2K\\M. - 2013 - Ray Trace Modeling of Underwater Sound Propagation.pdf}
}

@inproceedings{ma_power_2018,
  title = {The Power of Interpolation: {{Understanding}} the Effectiveness of {{SGD}} in Modern over-Parametrized Learning},
  shorttitle = {The Power of Interpolation},
  booktitle = {Proceedings of the {{International Conference}} on {{Machine Learning}}},
  author = {Ma, Siyuan and Bassily, Raef and Belkin, Mikhail},
  year = {2018},
  month = jul,
  series = {{{PMLR}}},
  volume = {80},
  pages = {3325--3334},
  publisher = {{ML Research Press}},
  abstract = {In this paper we aim to formally explain the phenomenon of fast convergence of Stochastic Gradient Descent (SGD) observed in modern machine learning. The key observation is that most modern learning architectures are over-parametrized and are trained to interpolate the data by driving the empirical loss (classification and regression) close to zero. While it is still unclear why these interpolated solutions perform well on test data, we show that these regimes allow for fast convergence of SGD, comparable in number of iterations to full gradient descent. For convex loss functions we obtain an exponential convergence bound for mini-batch SGD parallel to that for full gradient descent. We show that there is a critical batch size m{${_\ast}$}m{${_\ast}$}m\^* such that: (a) SGD iteration with mini-batch size m{$\leq$}m{${_\ast}$}m{$\leq$}m{${_\ast}$}m\textbackslash leq m\^* is nearly equivalent to mmm iterations of mini-batch size 111 (linear scaling regime). (b) SGD iteration with mini-batch m{$>$}m{${_\ast}$}m{$>$}m{${_\ast}$}m{$>$} m\^* is nearly equivalent to a full gradient descent iteration (saturation regime). Moreover, for the quadratic loss, we derive explicit expressions for the optimal mini-batch and step size and explicitly characterize the two regimes above. The critical mini-batch size can be viewed as the limit for effective mini-batch parallelization. It is also nearly independent of the data size, implying O(n)O(n)O(n) acceleration over GD per unit of computation. We give experimental evidence on real data which closely follows our theoretical analyses. Finally, we show how our results fit in the recent developments in training deep neural networks and discuss connections to adaptive rates for SGD and variance reduction.},
  langid = {english},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\A98DQQ6Y\\Ma et al. - 2018 - The Power of Interpolation Understanding the Effe.pdf}
}

@article{ma_why_2020,
  title = {Why Do Local Methods Solve Nonconvex Problems?},
  author = {Ma, Tengyu},
  year = {Dec-2020},
  journal = {Beyond the Worst-Case Analysis of Algorithms},
  eprint = {2103.13462},
  eprinttype = {arxiv},
  primaryclass = {cs, math, stat},
  pages = {465--485},
  publisher = {{Cambridge University Press}},
  abstract = {Non-convex optimization is ubiquitous in modern machine learning. Researchers devise non-convex objective functions and optimize them using off-the-shelf optimizers such as stochastic gradient descent and its variants, which leverage the local geometry and update iteratively. Even though solving non-convex functions is NP-hard in the worst case, the optimization quality in practice is often not an issue -- optimizers are largely believed to find approximate global minima. Researchers hypothesize a unified explanation for this intriguing phenomenon: most of the local minima of the practically-used objectives are approximately global minima. We rigorously formalize it for concrete instances of machine learning problems.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Data Structures and Algorithms,Computer Science - Machine Learning,Mathematics - Optimization and Control,Statistics - Machine Learning},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\LW3J4X7N\\Ma - 2021 - Why Do Local Methods Solve Nonconvex Problems.pdf;C\:\\Users\\msca8h\\Zotero\\storage\\2SKNEI26\\2103.html}
}

@techreport{mackay_local_2001,
  type = {Technical {{Report}}},
  title = {Local Minima, Symmetry-Breaking, and Model Pruning in Variational Free Energy Minimization},
  author = {MacKay, David J.C.},
  year = {2001},
  month = jun
}

@inproceedings{maclaurin_firefly_2015,
  title = {Firefly {{Monte Carlo}}: Exact {{MCMC}} with Subsets of Data},
  booktitle = {Proceedings of the {{International Joint Conference}} on {{Artificial Intelligence}}},
  author = {Maclaurin, Dougal and Adams, Ryan P.},
  year = {2015}
}

@article{mada_efficient_2009,
  title = {Efficient and Robust {{EM}} Algorithm for Multiple Wideband Source Localization},
  author = {Mada, K.K. and {Hsiao-Chun Wu} and Iyengar, S.S.},
  year = {2009},
  month = jul,
  journal = {IEEE Transactions on Vehicular Technology},
  volume = {58},
  number = {6},
  pages = {3071--3075}
}

@article{maddox_rethinking_2020,
  title = {Rethinking {{Parameter Counting}} in {{Deep Models}}: {{Effective Dimensionality Revisited}}},
  shorttitle = {Rethinking {{Parameter Counting}} in {{Deep Models}}},
  author = {Maddox, Wesley J. and Benton, Gregory and Wilson, Andrew Gordon},
  year = {2020},
  month = mar,
  journal = {arXiv:2003.02139 [cs, stat]},
  eprint = {2003.02139},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Neural networks appear to have mysterious generalization properties when using parameter counting as a proxy for complexity. Indeed, neural networks often have many more parameters than there are data points, yet still provide good generalization performance. Moreover, when we measure generalization as a function of parameters, we see double descent behaviour, where the test error decreases, increases, and then again decreases. We show that many of these properties become understandable when viewed through the lens of effective dimensionality, which measures the dimensionality of the parameter space determined by the data. We relate effective dimensionality to posterior contraction in Bayesian deep learning, model selection, double descent, and functional diversity in loss surfaces, leading to a richer understanding of the interplay between parameters and functions in deep models.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\KGB6NS8H\\Maddox et al. - 2020 - Rethinking Parameter Counting in Deep Models Effe.pdf;C\:\\Users\\msca8h\\Zotero\\storage\\K3JFRBDY\\2003.html}
}

@article{maddox_simple_2019,
  title = {A Simple Baseline for {{Bayesian}} Uncertainty in Deep Learning},
  author = {Maddox, Wesley and Garipov, Timur and Izmailov, Pavel and Vetrov, Dmitry and Wilson, Andrew Gordon},
  year = {2019},
  month = dec,
  journal = {arXiv:1902.02476 [cs, stat]},
  eprint = {1902.02476},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {We propose SWA-Gaussian (SWAG), a simple, scalable, and general purpose approach for uncertainty representation and calibration in deep learning. Stochastic Weight Averaging (SWA), which computes the first moment of stochastic gradient descent (SGD) iterates with a modified learning rate schedule, has recently been shown to improve generalization in deep learning. With SWAG, we fit a Gaussian using the SWA solution as the first moment and a low rank plus diagonal covariance also derived from the SGD iterates, forming an approximate posterior distribution over neural network weights; we then sample from this Gaussian distribution to perform Bayesian model averaging. We empirically find that SWAG approximates the shape of the true posterior, in accordance with results describing the stationary distribution of SGD iterates. Moreover, we demonstrate that SWAG performs well on a wide variety of tasks, including out of sample detection, calibration, and transfer learning, in comparison to many popular alternatives including MC dropout, KFAC Laplace, SGLD, and temperature scaling.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\Y2XGQUJ4\\Maddox et al. - 2019 - A Simple Baseline for Bayesian Uncertainty in Deep.pdf;C\:\\Users\\msca8h\\Zotero\\storage\\YNZSNCEI\\1902.html}
}

@article{magnusson_convergence_2016,
  title = {On the {{Convergence}} of {{Alternating Direction Lagrangian Methods}} for {{Nonconvex Structured Optimization Problems}}},
  author = {Magnusson, Sindri and Weeraddana, Pradeep Chathuranga and Rabbat, Michael G. and Fischione, Carlo},
  year = {2016},
  month = sep,
  journal = {IEEE Transactions on Control of Network Systems},
  volume = {3},
  number = {3},
  pages = {296--309},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\L93CB5AZ\\Magnusson et al. - 2016 - On the Convergence of Alternating Direction Lagran.pdf}
}

@inproceedings{mahendran_adaptive_2012,
  title = {Adaptive {{MCMC}} with {{Bayesian Optimization}}},
  booktitle = {Proceedings of the {{Fifteenth International Conference}} on {{Artificial Intelligence}} and {{Statistics}}},
  author = {Mahendran, Nimalan and Wang, Ziyu and Hamze, Firas and Freitas, Nando De},
  year = {2012},
  month = apr,
  series = {Proceedings of {{Machine Learning Research}}},
  volume = {22},
  pages = {751--760},
  publisher = {{PMLR}},
  address = {{La Palma, Canary Islands}},
  abstract = {This paper proposes a new randomized strategy for adaptive MCMC using Bayesian optimization. This approach applies to non-differentiable objective functions and trades off exploration and exploitation to reduce the number of potentially costly objective function evaluations. We demonstrate the strategy in the complex setting of sampling from constrained, discrete and densely connected probabilistic graphical models where, for each variation of the problem, one needs to adjust the parameters of the proposal mechanism automatically to ensure efficient mixing of the Markov chains.}
}

@inproceedings{mai_convergence_2020,
  title = {Convergence of a Stochastic Gradient Method with Momentum for Non-Smooth Non-Convex Optimization},
  booktitle = {Proceedings of the 37th {{International Conference}} on {{Machine Learning}}},
  author = {Mai, Vien and Johansson, Mikael},
  year = {2020},
  month = nov,
  pages = {6630--6639},
  publisher = {{PMLR}},
  abstract = {Stochastic gradient methods with momentum are widely used in applications and at the core of optimization subroutines in many popular machine learning libraries. However, their sample complexities have not been obtained for problems beyond those that are convex or smooth. This paper establishes the convergence rate of a stochastic subgradient method with a momentum term of Polyak type for a broad class of non-smooth, non-convex, and constrained optimization problems. Our key innovation is the construction of a special Lyapunov function for which the proven complexity can be achieved without any tuning of the momentum parameter. For smooth problems, we extend the known complexity bound to the constrained case and demonstrate how the unconstrained case can be analyzed under weaker assumptions than the state-of-the-art. Numerical results confirm our theoretical developments.},
  langid = {english},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\M7SZZUFW\\Mai and Johansson - 2020 - Convergence of a Stochastic Gradient Method with M.pdf;C\:\\Users\\msca8h\\Zotero\\storage\\X9S27ZML\\Mai and Johansson - 2020 - Convergence of a Stochastic Gradient Method with M.pdf}
}

@article{maire_informed_2019,
  title = {Informed Sub-Sampling {{MCMC}}: Approximate {{Bayesian}} Inference for Large Datasets},
  shorttitle = {Informed Sub-Sampling {{MCMC}}},
  author = {Maire, Florian and Friel, Nial and Alquier, Pierre},
  year = {2019},
  month = may,
  journal = {Statistics and Computing},
  volume = {29},
  number = {3},
  pages = {449--482},
  abstract = {This paper introduces a framework for speeding up Bayesian inference conducted in presence of large datasets. We design a Markov chain whose transition kernel uses an unknown fraction of fixed size of the available data that is randomly refreshed throughout the algorithm. Inspired by the Approximate Bayesian Computation literature, the subsampling process is guided by the fidelity to the observed data, as measured by summary statistics. The resulting algorithm, Informed Sub-Sampling MCMC, is a generic and flexible approach which, contrary to existing scalable methodologies, preserves the simplicity of the Metropolis\textendash Hastings algorithm. Even though exactness is lost, i.e~ the chain distribution approximates the posterior, we study and quantify theoretically this bias and show on a diverse set of examples that it yields excellent performances when the computational budget is limited. If available and cheap to compute, we show that setting the summary statistics as the maximum likelihood estimator is supported by theoretical arguments.},
  langid = {english},
  keywords = {65C60,Approximate Bayesian Computation,Bayesian inference,Big-data,noisy Markov chain Monte Carlo,Primary 65C40,Secondary 62F15},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\6L63CW5S\\Maire et al. - 2019 - Informed sub-sampling MCMC approximate Bayesian i.pdf}
}

@inproceedings{majo_memory_2011,
  title = {Memory {{Management}} in {{NUMA Multicore Systems}}: {{Trapped Between Cache Contention}} and {{Interconnect Overhead}}},
  booktitle = {Proceedings of the {{International Symposium}} on {{Memory Management}}},
  author = {Majo, Zoltan and Gross, Thomas R.},
  year = {2011},
  series = {{{ISMM}} '11},
  pages = {11--20},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  keywords = {memory allocation,multicore processors,numa,shared resource contention},
  file = {/home/msca8h/Documents/parallel_scheduling/Majo and Gross - 2011 - Memory Management in NUMA Multicore Systems Trapp.pdf}
}

@article{malik_bayesian_2014,
  title = {A {{Bayesian}} Framework for Blind Adaptive Beamforming},
  author = {Malik, Sarmad and Benesty, Jacob and Chen, Jingdong},
  year = {2014},
  month = may,
  journal = {IEEE Transactions on Signal Processing},
  volume = {62},
  number = {9},
  pages = {2370--2384}
}

@article{malioutov_sparse_2005,
  title = {A Sparse Signal Reconstruction Perspective for Source Localization with Sensor Arrays},
  author = {Malioutov, D. and Cetin, M. and Willsky, A.S.},
  year = {2005},
  month = aug,
  journal = {IEEE Transactions on Signal Processing},
  volume = {53},
  number = {8},
  pages = {3010--3022},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\6L77BAGA\\Malioutov et al. - 2005 - A sparse signal reconstruction perspective for sou.pdf}
}

@incollection{malkomes_automating_2018,
  title = {Automating {{Bayesian}} Optimization with {{Bayesian}} Optimization},
  booktitle = {Advances in {{Neural Information Processing Systems}} 31},
  author = {Malkomes, Gustavo and Garnett, Roman},
  year = {2018},
  pages = {5984--5994},
  publisher = {{Curran Associates, Inc.}}
}

@incollection{malkomes_automating_2018a,
  title = {Automating {{Bayesian}} Optimization with {{Bayesian}} Optimization},
  booktitle = {Advances in {{Neural Information Processing Systems}} 31},
  author = {Malkomes, Gustavo and Garnett, Roman},
  year = {2018},
  pages = {5984--5994},
  publisher = {{Curran Associates, Inc.}}
}

@article{mandt_stochastic_2017,
  title = {Stochastic Gradient Descent as Approximate {{Bayesian}} Inference},
  author = {Mandt, Stephan and Hoffman, Matthew D. and Blei, David M.},
  year = {2017},
  journal = {Journal of Machine Learning Research},
  volume = {18},
  number = {134},
  pages = {1--35},
  abstract = {Stochastic Gradient Descent with a constant learning rate (constant SGD) simulates a Markov chain with a stationary distribution. With this perspective, we derive several new results. (1) We show that constant SGD can be used as an approximate Bayesian posterior inference algorithm. Specifically, we show how to adjust the tuning parameters of constant SGD to best match the stationary distribution to a posterior, minimizing the Kullback-Leibler divergence between these two distributions. (2) We demonstrate that constant SGD gives rise to a new variational EM algorithm that optimizes hyperparameters in complex probabilistic models. (3) We also show how to tune SGD with momentum for approximate sampling. (4) We analyze stochastic-gradient MCMC algorithms. For Stochastic- Gradient Langevin Dynamics and Stochastic-Gradient Fisher Scoring, we quantify the approximation errors due to finite learning rates. Finally (5), we use the stochastic process perspective to give a short proof of why Polyak averaging is optimal. Based on this idea, we propose a scalable approximate MCMC algorithm, the Averaged Stochastic Gradient Sampler.},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\HJAGH22P\\M et al. - 2017 - Stochastic Gradient Descent as Approximate Bayesia.pdf}
}

@article{mangoubi_does_2018,
  title = {Does {{Hamiltonian Monte Carlo}} Mix Faster than a Random Walk on Multimodal Densities?},
  author = {Mangoubi, Oren and Pillai, Natesh S. and Smith, Aaron},
  year = {2018},
  month = sep,
  journal = {arXiv:1808.03230 [cs, math, stat]},
  eprint = {1808.03230},
  eprinttype = {arxiv},
  primaryclass = {cs, math, stat},
  abstract = {Hamiltonian Monte Carlo (HMC) is a very popular and generic collection of Markov chain Monte Carlo (MCMC) algorithms. One explanation for the popularity of HMC algorithms is their excellent performance as the dimension \$d\$ of the target becomes large: under conditions that are satisfied for many common statistical models, optimally-tuned HMC algorithms have a running time that scales like \$d\^\{0.25\}\$. In stark contrast, the running time of the usual Random-Walk Metropolis (RWM) algorithm, optimally tuned, scales like \$d\$. This superior scaling of the HMC algorithm with dimension is attributed to the fact that it, unlike RWM, incorporates the gradient information in the proposal distribution. In this paper, we investigate a different scaling question: does HMC beat RWM for highly \$\textbackslash textit\{multimodal\}\$ targets? We find that the answer is often \$\textbackslash textit\{no\}\$. We compute the spectral gaps for both the algorithms for a specific class of multimodal target densities, and show that they are identical. The key reason is that, within one mode, the gradient is effectively ignorant about other modes, thus negating the advantage the HMC algorithm enjoys in unimodal targets. We also give heuristic arguments suggesting that the above observation may hold quite generally. Our main tool for answering this question is a novel simple formula for the conductance of HMC using Liouville's theorem. This result allows us to compute the spectral gap of HMC algorithms, for both the classical HMC with isotropic momentum and the recent Riemannian HMC, for multimodal targets.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Mathematics - Probability,Statistics - Computation,Statistics - Machine Learning,Statistics - Methodology},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\A2MVFI4P\\Mangoubi et al. - 2018 - Does Hamiltonian Monte Carlo mix faster than a ran.pdf;C\:\\Users\\msca8h\\Zotero\\storage\\XF4ZLP2X\\1808.html}
}

@article{mao_marginal_2021,
  title = {Marginal Likelihood Maximization Based Fast Array Manifold Matrix Learning for Direction of Arrival Estimation},
  author = {Mao, Yiwen and Guo, Qinghua and Ding, Jinshan and Liu, Fei and Yu, Yanguang},
  year = {2021},
  journal = {IEEE Transactions on Signal Processing},
  volume = {69},
  pages = {5512--5522}
}

@article{maranda_efficient_1989,
  title = {Efficient Digital Beamforming in the Frequency Domain},
  author = {Maranda, Brian},
  year = {1989},
  month = nov,
  journal = {The Journal of the Acoustical Society of America},
  volume = {86},
  number = {5},
  pages = {1813--1819},
  langid = {english}
}

@inproceedings{marathe_performance_2017,
  title = {Performance {{Modeling}} under {{Resource Constraints Using Deep Transfer Learning}}},
  booktitle = {Proceedings of the {{International Conference}} for {{High Performance Computing}}, {{Networking}}, {{Storage}} and {{Analysis}}},
  author = {Marathe, Aniruddha and Anirudh, Rushil and Jain, Nikhil and Bhatele, Abhinav and Thiagarajan, Jayaraman and Kailkhura, Bhavya and Yeom, Jae-Seung and Rountree, Barry and Gamblin, Todd},
  year = {2017},
  series = {{{SC}} '17},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  keywords = {deep learning,parameter selection,performance prediction,transfer learning}
}

@article{marin_consistency_2019,
  title = {Consistency of Adaptive Importance Sampling and Recycling Schemes},
  author = {Marin, Jean-Michel and Pudlo, Pierre and Sedki, Mohammed},
  year = {2019},
  month = aug,
  journal = {Bernoulli},
  volume = {25},
  number = {3}
}

@article{markatos_using_1994,
  title = {Using Processor Affinity in Loop Scheduling on Shared-Memory Multiprocessors},
  author = {Markatos, E.P. and LeBlanc, T.J.},
  year = {1994},
  month = apr,
  journal = {IEEE Transactions on Parallel and Distributed Systems},
  volume = {5},
  number = {4},
  pages = {379--400},
  file = {/home/msca8h/Documents/parallel_scheduling/Markatos and LeBlanc - 1994 - Using processor affinity in loop scheduling on sha.pdf}
}

@book{marshall_inequalities_2011,
  title = {Inequalities: {{Theory}} of {{Majorization}} and {{Its Applications}}},
  shorttitle = {Inequalities},
  author = {Marshall, Albert W. and Olkin, Ingram and Arnold, Barry C.},
  year = {2011},
  series = {Springer {{Series}} in {{Statistics}}},
  edition = {Second},
  publisher = {{Springer}},
  langid = {english},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\XFH4WYEW\\978-0-387-68276-1.html}
}

@article{martinez-cantin_bayesopt_2014,
  title = {{{BayesOpt}}: A {{Bayesian}} Optimization Library for Nonlinear Optimization, Experimental Design and Bandits},
  author = {{Martinez-Cantin}, Ruben},
  year = {2014},
  journal = {Journal of Machine Learning Research},
  volume = {15},
  pages = {3915--3919}
}

@article{martinez-cantin_practical_2017,
  title = {Practical {{Bayesian}} Optimization in the Presence of Outliers},
  author = {{Martinez-Cantin}, Ruben and Tee, Kevin and McCourt, Michael},
  year = {2017},
  month = dec,
  journal = {arXiv:1712.04567 [cs, stat]},
  eprint = {1712.04567},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Inference in the presence of outliers is an important field of research as outliers are ubiquitous and may arise across a variety of problems and domains. Bayesian optimization is method that heavily relies on probabilistic inference. This allows outstanding sample efficiency because the probabilistic machinery provides a memory of the whole optimization process. However, that virtue becomes a disadvantage when the memory is populated with outliers, inducing bias in the estimation. In this paper, we present an empirical evaluation of Bayesian optimization methods in the presence of outliers. The empirical evidence shows that Bayesian optimization with robust regression often produces suboptimal results. We then propose a new algorithm which combines robust regression (a Gaussian process with Student-t likelihood) with outlier diagnostics to classify data points as outliers or inliers. By using an scheduler for the classification of outliers, our method is more efficient and has better convergence over the standard robust regression. Furthermore, we show that even in controlled situations with no expected outliers, our method is able to produce better results.},
  archiveprefix = {arXiv},
  keywords = {90C26; 62K25; 62F35,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\66IZF3TX\\Martinez-Cantin et al. - 2017 - Practical Bayesian optimization in the presence of.pdf;C\:\\Users\\msca8h\\Zotero\\storage\\ER5VPHMX\\1712.html}
}

@article{martinez-cantin_robust_2017,
  title = {Robust {{Bayesian}} Optimization with {{Student-t}} Likelihood},
  author = {{Martinez-Cantin}, Ruben and McCourt, Michael and Tee, Kevin},
  year = {2017},
  journal = {arXiv preprint arXiv:1707.05729},
  eprint = {1707.05729},
  eprinttype = {arxiv},
  archiveprefix = {arXiv}
}

@article{martino_adaptive_2015,
  title = {An Adaptive Population Importance Sampler: Learning from Uncertainty},
  shorttitle = {An {{Adaptive Population Importance Sampler}}},
  author = {Martino, Luca and Elvira, Victor and Luengo, David and Corander, Jukka},
  year = {2015},
  month = aug,
  journal = {IEEE Transactions on Signal Processing},
  volume = {63},
  number = {16},
  pages = {4422--4437},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\VUZC8DMX\\Martino et al. - 2015 - An Adaptive Population Importance Sampler Learnin.pdf}
}

@article{martino_issues_2017,
  title = {Issues in the {{Multiple Try Metropolis}} Mixing},
  author = {Martino, L. and Louzada, F.},
  year = {2017},
  month = mar,
  journal = {Computational Statistics},
  volume = {32},
  number = {1},
  pages = {239--252},
  langid = {english},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\8KAR68L2\\Martino and Louzada - 2017 - Issues in the Multiple Try Metropolis mixing.pdf}
}

@article{martino_layered_2017,
  title = {Layered Adaptive Importance Sampling},
  author = {Martino, L. and Elvira, V. and Luengo, D. and Corander, J.},
  year = {2017},
  month = may,
  journal = {Statistics and Computing},
  volume = {27},
  number = {3},
  pages = {599--623},
  langid = {english},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\BA9U4DD3\\Martino et al. - 2017 - Layered adaptive importance sampling.pdf}
}

@incollection{martino_metropolis_2017,
  title = {Metropolis {{Sampling}}},
  booktitle = {Wiley {{StatsRef}}: {{Statistics Reference Online}}},
  author = {Martino, Luca and Elvira, Victor},
  year = {2017},
  month = may,
  pages = {1--18},
  publisher = {{John Wiley \& Sons, Ltd}},
  address = {{Chichester, UK}},
  langid = {english}
}

@article{martino_orthogonal_2016,
  title = {Orthogonal Parallel {{MCMC}} Methods for Sampling and Optimization},
  author = {Martino, L. and Elvira, V. and Luengo, D. and Corander, J. and Louzada, F.},
  year = {2016},
  month = nov,
  journal = {Digital Signal Processing},
  volume = {58},
  pages = {64--84},
  langid = {english},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\ELWC3EB8\\Martino et al. - 2016 - Orthogonal parallel MCMC methods for sampling and .pdf}
}

@inproceedings{martino_parallel_2016,
  title = {Parallel Metropolis Chains with Cooperative Adaptation},
  booktitle = {2016 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Martino, L. and Elvira, V and Luengo, D. and Louzada, F},
  year = {2016},
  month = mar,
  pages = {3974--3978},
  publisher = {{IEEE}},
  address = {{Shanghai}},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\L88AUWPC\\Martino et al. - 2016 - Parallel metropolis chains with cooperative adapta.pdf}
}

@article{martino_review_2018,
  title = {A Review of Multiple Try {{MCMC}} Algorithms for Signal Processing},
  author = {Martino, Luca},
  year = {2018},
  month = apr,
  journal = {Digital Signal Processing},
  volume = {75},
  pages = {134--152},
  langid = {english},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\4Y9E6D3P\\Martino - 2018 - A review of multiple try MCMC algorithms for signa.pdf}
}

@article{martino_review_2018a,
  title = {A Review of Multiple Try {{MCMC}} Algorithms for Signal Processing},
  author = {Martino, Luca},
  year = {2018},
  month = apr,
  journal = {Digital Signal Processing},
  volume = {75},
  pages = {134--152},
  langid = {english},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\EZWARYVG\\Martino - 2018 - A review of multiple try MCMC algorithms for signa.pdf}
}

@inproceedings{martino_smelly_2015,
  title = {Smelly Parallel {{MCMC}} Chains},
  booktitle = {2015 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Martino, L. and Elvira, V. and Luengo, D. and {Artes-Rodriguez}, A. and Corander, J.},
  year = {2015},
  month = apr,
  pages = {4070--4074},
  publisher = {{IEEE}},
  address = {{South Brisbane, Queensland, Australia}}
}

@article{martis_ecg_2013,
  title = {{{ECG}} Beat Classification Using {{PCA}}, {{LDA}}, {{ICA}} and {{Discrete Wavelet Transform}}},
  author = {Martis, Roshan Joy and Acharya, U. Rajendra and Min, Lim Choo},
  year = {2013},
  month = sep,
  journal = {Biomedical Signal Processing and Control},
  volume = {8},
  number = {5},
  pages = {437--448},
  langid = {english}
}

@article{maruyama_fully_2011,
  title = {Fully {{Bayes}} Factors with a Generalized $g$ -Prior},
  author = {Maruyama, Yuzo and George, Edward I.},
  year = {2011},
  month = oct,
  journal = {The Annals of Statistics},
  volume = {39},
  number = {5},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\RFHEVJEA\\Maruyama and George - 2011 - Fully Bayes factors with a generalized g-prior.pdf}
}

@inproceedings{masrani_thermodynamic_2019,
  title = {The Thermodynamic Variational Objective},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Masrani, Vaden and Le, Tuan Anh and Wood, Frank},
  year = {2019},
  volume = {32},
  publisher = {{Curran Associates, Inc.}},
  abstract = {We introduce the thermodynamic variational objective (TVO) for learning in both continuous and discrete deep generative models. The TVO arises from a key connection between variational inference and thermodynamic integration that results in a tighter lower bound to the log marginal likelihood than the standard variational evidence lower bound (ELBO) while remaining as broadly applicable. We provide a computationally efficient gradient estimator for the TVO that applies to continuous, discrete, and non-reparameterizable distributions and show that the objective functions used in variational inference, variational autoencoders, wake sleep, and inference compilation are all special cases of the TVO. We use the TVO to learn both discrete and continuous deep generative models and empirically demonstrate state of the art model and inference network learning.},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\FGMICSGP\\Masrani et al. - 2019 - The Thermodynamic Variational Objective.pdf}
}

@article{matsaglia_equalities_1974,
  title = {Equalities and Inequalities for Ranks of Matrices},
  author = {Matsaglia, George and P. H. Styan, George},
  year = {1974},
  month = jan,
  journal = {Linear and Multilinear Algebra},
  volume = {2},
  number = {3},
  pages = {269--292},
  langid = {english}
}

@book{mcbook,
  title = {Monte {{Carlo}} Theory, Methods and Examples},
  author = {Owen, Art B.},
  year = {2013}
}

@techreport{mccammon_literature_2010,
  type = {Contract {{Report}}},
  title = {A Literature Survey of Reverberation Modeling: With Emphasis on {{Bellhop}} Compatibility for Operational Applications},
  author = {McCammon, Diana},
  year = {2010},
  month = sep,
  number = {DRDC Atlantic CR 2010-119},
  institution = {{Defence Research and Development Canada}},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\WCUG9T3E\\McCammon - 2010 - A literature survey of reverberation modeling wit.pdf}
}

@article{mcphail_robustness_2018,
  title = {Robustness Metrics: How Are They Calculated, When Should They Be Used and Why Do They Give Different Results?},
  shorttitle = {Robustness {{Metrics}}},
  author = {McPhail, C. and Maier, H. R. and Kwakkel, J. H. and Giuliani, M. and Castelletti, A. and Westra, S.},
  year = {2018},
  month = feb,
  journal = {Earth's Future},
  volume = {6},
  number = {2},
  pages = {169--191},
  langid = {english},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\URPWCJ8U\\McPhail et al. - 2018 - Robustness Metrics How Are They Calculated, When .pdf}
}

@article{mcphail_robustness_2018a,
  title = {Robustness {{Metrics}}: {{How Are They Calculated}}, {{When Should They Be Used}} and {{Why Do They Give Different Results}}?},
  shorttitle = {Robustness {{Metrics}}},
  author = {McPhail, C. and Maier, H. R. and Kwakkel, J. H. and Giuliani, M. and Castelletti, A. and Westra, S.},
  year = {2018},
  month = feb,
  journal = {Earth's Future},
  volume = {6},
  number = {2},
  pages = {169--191},
  langid = {english},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\KKKQVPMS\\McPhail et al. - 2018 - Robustness Metrics How Are They Calculated, When .pdf}
}

@article{mcsharry_dynamical_2003,
  title = {A Dynamical Model for Generating Synthetic Electrocardiogram Signals},
  author = {McSharry, P.E. and Clifford, G.D. and Tarassenko, L. and Smith, L.A.},
  year = {2003},
  month = mar,
  journal = {IEEE Transactions on Biomedical Engineering},
  volume = {50},
  number = {3},
  pages = {289--294},
  langid = {english},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\93DN66HR\\McSharry et al. - 2003 - A dynamical model for generating synthetic electro.pdf}
}

@article{mei_phase_2020,
  title = {Phase Asymmetry Ultrasound Despeckling with Fractional Anisotropic Diffusion and Total Variation},
  author = {Mei, Kunqiang and Hu, Bin and Fei, Baowei and Qin, Binjie},
  year = {2020},
  journal = {IEEE Transactions on Image Processing},
  volume = {29},
  pages = {2845--2859},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\F68MZU4A\\Mei et al. - 2020 - Phase Asymmetry Ultrasound Despeckling With Fracti.pdf}
}

@inproceedings{mei-na_joint_2008,
  title = {Joint Source Number Detection and {{DOA}} Estimation via Reversible Jump {{MCMC}}},
  booktitle = {2008 {{IEEE Pacific-Asia Workshop}} on {{Computational Intelligence}} and {{Industrial Application}}},
  author = {{Mei-na}, Jin and {Yong-jun}, Zhao and {Dong-hai}, Li},
  year = {2008},
  month = dec,
  pages = {859--863},
  publisher = {{IEEE}},
  address = {{Wuhan}}
}

@article{meister_survey_2021,
  title = {A Survey on Bounding Volume Hierarchies for Ray Tracing},
  author = {Meister, Daniel and Ogaki, Shinji and Benthin, Carsten and Doyle, Michael J. and Guthe, Michael and Bittner, Ji{\v r}{\'i}},
  year = {2021},
  month = may,
  journal = {Computer Graphics Forum},
  volume = {40},
  number = {2},
  pages = {683--712},
  langid = {english}
}

@article{mengersen_rates_1996,
  title = {Rates of Convergence of the Hastings and Metropolis Algorithms},
  author = {Mengersen, K. L. and Tweedie, R. L.},
  year = {1996},
  journal = {The Annals of Statistics},
  volume = {24},
  number = {1},
  pages = {101--121},
  publisher = {{Institute of Mathematical Statistics}},
  abstract = {We apply recent results in Markov chain theory to Hastings and Metropolis algorithms with either independent or symmetric candidate distributions, and provide necessary and sufficient conditions for the algorithms to converge at a geometric rate to a prescribed distribution {$\pi$}. In the independence case (in Rk) these indicate that geometric convergence essentially occurs if and only if the candidate density is bounded below by a multiple of {$\pi$}; in the symmetric case (in R only) we show geometric convergence essentially occurs if and only if {$\pi$} has geometric tails. We also evaluate recently developed computable bounds on the rates of convergence in this context: examples show that these theoretical bounds can be inherently extremely conservative, although when the chain is stochastically monotone the bounds may well be effective.}
}

@inproceedings{Menon2020AutotuningPC,
  title = {Auto-Tuning Parameter Choices in {{HPC}} Applications Using Bayesian Optimization},
  author = {Menon, Harshitha and Bhatele, Abhinav and Gamblin, Todd},
  year = {2020}
}

@inproceedings{meronen_periodic_2021,
  title = {Periodic Activation Functions Induce Stationarity},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Meronen, Lassi and Trapp, Martin and Solin, Arno},
  year = {2021},
  volume = {34},
  pages = {1673--1685},
  publisher = {{Curran Associates, Inc.}},
  abstract = {Neural network models are known to reinforce hidden data biases, making them unreliable and difficult to interpret. We seek to build models that `know what they do not know' by introducing inductive biases in the function space. We show that periodic activation functions in Bayesian neural networks establish a connection between the prior on the network weights and translation-invariant, stationary Gaussian process priors. Furthermore, we show that this link goes beyond sinusoidal (Fourier) activations by also covering triangular wave and periodic ReLU activation functions. In a series of experiments, we show that periodic activation functions obtain comparable performance for in-domain data and capture sensitivity to perturbed inputs in deep neural networks for out-of-domain detection.},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\MLS7XUWU\\Meronen et al. - 2021 - Periodic Activation Functions Induce Stationarity.pdf}
}

@article{mesquita_embarrassingly_2019,
  title = {Embarrassingly Parallel {{MCMC}} Using Deep Invertible Transformations},
  author = {Mesquita, Diego and Blomstedt, Paul and Kaski, Samuel},
  year = {2019},
  month = mar,
  journal = {arXiv:1903.04556 [cs, stat]},
  eprint = {1903.04556},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {While MCMC methods have become a main work-horse for Bayesian inference, scaling them to large distributed datasets is still a challenge. Embarrassingly parallel MCMC strategies take a divide-and-conquer stance to achieve this by writing the target posterior as a product of subposteriors, running MCMC for each of them in parallel and subsequently combining the results. The challenge then lies in devising efficient aggregation strategies. Current strategies trade-off between approximation quality, and costs of communication and computation. In this work, we introduce a novel method that addresses these issues simultaneously. Our key insight is to introduce a deep invertible transformation to approximate each of the subposteriors. These approximations can be made accurate even for complex distributions and serve as intermediate representations, keeping the total communication cost limited. Moreover, they enable us to sample from the product of the subposteriors using an efficient and stable importance sampling scheme. We demonstrate the approach outperforms available state-of-the-art methods in a range of challenging scenarios, including high-dimensional and heterogeneous subposteriors.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\BHJCXPKR\\Mesquita et al. - 2019 - Embarrassingly parallel MCMC using deep invertible.pdf;C\:\\Users\\msca8h\\Zotero\\storage\\MHP3YGCQ\\1903.html}
}

@article{mestre_finite_2006,
  title = {Finite Sample Size Effect on Minimum Variance Beamformers: Optimum Diagonal Loading Factor for Large Arrays},
  shorttitle = {Finite Sample Size Effect on Minimum Variance Beamformers},
  author = {Mestre, X. and Lagunas, M.A.},
  year = {2006},
  month = jan,
  journal = {IEEE Transactions on Signal Processing},
  volume = {54},
  number = {1},
  pages = {69--82}
}

@article{metropolis_equation_1953,
  title = {Equation of State Calculations by Fast Computing Machines},
  author = {Metropolis, Nicholas and Rosenbluth, Arianna W. and Rosenbluth, Marshall N. and Teller, Augusta H. and Teller, Edward},
  year = {1953},
  month = jun,
  journal = {The Journal of Chemical Physics},
  volume = {21},
  number = {6},
  pages = {1087--1092},
  langid = {english},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\DV7X38EW\\Metropolis et al. - 1953 - Equation of State Calculations by Fast Computing M.pdf}
}

@article{michel_clock_2019,
  title = {Clock {{Monte Carlo}} Methods},
  author = {Michel, Manon and Tan, Xiaojun and Deng, Youjin},
  year = {2019},
  month = jan,
  journal = {Physical Review E},
  volume = {99},
  number = {1},
  pages = {010105},
  langid = {english},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\YAVUQYGQ\\Michel et al. - 2019 - Clock Monte Carlo methods.pdf}
}

@misc{mikkola_prior_2021,
  title = {Prior Knowledge Elicitation: {{The}} Past, Present, and Future},
  shorttitle = {Prior Knowledge Elicitation},
  author = {Mikkola, Petrus and Martin, Osvaldo A. and Chandramouli, Suyog and Hartmann, Marcelo and Pla, Oriol Abril and Thomas, Owen and Pesonen, Henri and Corander, Jukka and Vehtari, Aki and Kaski, Samuel and B{\"u}rkner, Paul-Christian and Klami, Arto},
  year = {2021},
  month = dec,
  number = {arXiv:2112.01380},
  eprint = {2112.01380},
  eprinttype = {arxiv},
  primaryclass = {stat},
  institution = {{arXiv}},
  abstract = {Specification of the prior distribution for a Bayesian model is a central part of the Bayesian workflow for data analysis, but it is often difficult even for statistical experts. Prior elicitation transforms domain knowledge of various kinds into well-defined prior distributions, and offers a solution to the prior specification problem, in principle. In practice, however, we are still fairly far from having usable prior elicitation tools that could significantly influence the way we build probabilistic models in academia and industry. We lack elicitation methods that integrate well into the Bayesian workflow and perform elicitation efficiently in terms of costs of time and effort. We even lack a comprehensive theoretical framework for understanding different facets of the prior elicitation problem. Why are we not widely using prior elicitation? We analyze the state of the art by identifying a range of key aspects of prior knowledge elicitation, from properties of the modelling task and the nature of the priors to the form of interaction with the expert. The existing prior elicitation literature is reviewed and categorized in these terms. This allows recognizing under-studied directions in prior elicitation research, finally leading to a proposal of several new avenues to improve prior elicitation methodology.},
  archiveprefix = {arXiv},
  keywords = {Statistics - Methodology},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\M9YSBVI4\\Mikkola et al. - 2021 - Prior knowledge elicitation The past, present, an.pdf;C\:\\Users\\msca8h\\Zotero\\storage\\RQFU3JWT\\2112.html}
}

@article{milanfar_tour_2013,
  title = {A Tour of Modern Image Filtering: {{New}} Insights and Methods, Both Practical and Theoretical},
  shorttitle = {A {{Tour}} of {{Modern Image Filtering}}},
  author = {Milanfar, Peyman},
  year = {2013},
  month = jan,
  journal = {IEEE Signal Processing Magazine},
  volume = {30},
  number = {1},
  pages = {106--128},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\3KACBNPH\\Milanfar - 2013 - A Tour of Modern Image Filtering New Insights and.pdf}
}

@inproceedings{miller_reducing_2017,
  title = {Reducing Reparameterization Gradient Variance},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Miller, Andrew and Foti, Nick and D' Amour, Alexander and Adams, Ryan P},
  year = {2017},
  volume = {30},
  publisher = {{Curran Associates, Inc.}},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\MIXA5K6S\\Miller et al. - 2017 - Reducing Reparameterization Gradient Variance.pdf}
}

@article{min_fast_2014,
  title = {Fast Global Image Smoothing Based on Weighted Least Squares},
  author = {Min, Dongbo and Choi, Sunghwan and Lu, Jiangbo and Ham, Bumsub and Sohn, Kwanghoon and Do, Minh N.},
  year = {2014},
  month = dec,
  journal = {IEEE Transactions on Image Processing},
  volume = {23},
  number = {12},
  pages = {5638--5653}
}

@article{ming-yuenchan_perceptionbased_2009,
  title = {Perception-{{Based Transparency Optimization}} for {{Direct Volume Rendering}}},
  author = {{Ming-Yuen Chan} and {Yingcai Wu} and {Wai-Ho Mak} and {Wei Chen} and {Huamin Qu}},
  year = {2009},
  month = nov,
  journal = {IEEE Transactions on Visualization and Computer Graphics},
  volume = {15},
  number = {6},
  pages = {1283--1290}
}

@article{mingas_populationbased_2016,
  title = {Population-Based {{MCMC}} on Multi-Core {{CPUs}}, {{GPUs}} and {{FPGAs}}},
  author = {Mingas, Grigorios and Bouganis, Christos-Savvas},
  year = {2016},
  month = apr,
  journal = {IEEE Transactions on Computers},
  volume = {65},
  number = {4},
  pages = {1283--1296},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\UR5NSRXA\\Mingas and Bouganis - 2016 - Population-Based MCMC on Multi-Core CPUs, GPUs and.pdf}
}

@article{mingyihong_joint_2010,
  title = {Joint Model Selection and Parameter Estimation by Population {{Monte Carlo}} Simulation},
  author = {{Mingyi Hong} and Bugallo, Monica F and Djuric, Petar M},
  year = {2010},
  month = jun,
  journal = {IEEE Journal of Selected Topics in Signal Processing},
  volume = {4},
  number = {3},
  pages = {526--539}
}

@article{minh_understanding_2015,
  title = {Understanding the {{Hastings Algorithm}}},
  author = {Minh, David D. L. and Minh, Do Le (Paul)},
  year = {2015},
  month = feb,
  journal = {Communications in Statistics - Simulation and Computation},
  volume = {44},
  number = {2},
  pages = {332--349},
  langid = {english},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\23G7T4HA\\Minh and Minh - 2015 - Understanding the Hastings Algorithm.pdf}
}

@techreport{minka2005divergence,
  title = {Divergence Measures and Message Passing},
  author = {Minka, Tom},
  year = {2005},
  month = jan,
  number = {MSR-TR-2005-173},
  pages = {17},
  institution = {{Microsoft Research}},
  abstract = {This paper presents a unifying view of message-passing algorithms, as methods to approximate a complex Bayesian network by a simpler network with minimum information divergence. In this view, the difference between mean-field methods and belief propagation is not the amount of structure they model, but only the measure of loss they minimize (`exclusive' versus `inclusive' Kullback-Leibler divergence). In each case, message-passing arises by minimizing a localized version of the divergence, local to each factor. By examining these divergence measures, we can intuit the types of solution they prefer (symmetry-breaking, for example) and their suitability for different tasks. Furthermore, by considering a wider variety of divergence measures (such as alpha-divergences), we can achieve different complexity and performance goals.}
}

@article{mira_efficiency_2002,
  title = {Efficiency and Convergence Properties of Slice Samplers},
  author = {Mira, Antonietta and Tierney, Luke},
  year = {2002},
  journal = {Scandinavian Journal of Statistics},
  volume = {29},
  number = {1},
  pages = {1--12},
  abstract = {The slice sampler (SS) is a method of constructing a reversible Markov chain with a specified invariant distribution. Given an independence Metropolis\textendash Hastings algorithm (IMHA) it is always possible to construct a SS that dominates it in the Peskun sense. This means that the resulting SS produces estimates with a smaller asymptotic variance than the IMHA. Furthermore the SS has a smaller second-largest eigenvalue. This ensures faster convergence to the target distribution. A sufficient condition for uniform ergodicity of the SS is given and an upper bound for the rate of convergence to stationarity is provided.},
  langid = {english},
  keywords = {auxiliary variables,efficiency of MCMC,geometric ergodicity,Markov chain Monte Carlo,MetropolisHastings algorithm,Peskun ordering,uniform ergodicity},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/1467-9469.00267},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\CEGTG2D5\\Mira and Tierney - 2002 - Efficiency and convergence properties of slice sam.pdf;C\:\\Users\\msca8h\\Zotero\\storage\\HSNTHEGD\\1467-9469.html}
}

@article{mira_ordering_2001,
  title = {Ordering and Improving the Performance of {{Monte Carlo Markov}} Chains},
  author = {Mira, Antonietta},
  year = {2001},
  month = nov,
  journal = {Statistical Science},
  volume = {16},
  number = {4},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\FS5LQEF4\\Mira - 2001 - Ordering and Improving the Performance of Monte Ca.pdf}
}

@inproceedings{mishra_despeckling_2018,
  title = {Despeckling {{CNN}} with Ensembles of Classical Outputs},
  booktitle = {2018 24th {{International Conference}} on {{Pattern Recognition}} ({{ICPR}})},
  author = {Mishra, Deepak and Tyagi, Sarthak and Chaudhury, Santanu and Sarkar, Mukul and Singh Soin, Arvinder},
  year = {2018},
  month = aug,
  pages = {3802--3807},
  publisher = {{IEEE}},
  address = {{Beijing}}
}

@incollection{mishra_edge_2017,
  title = {Edge Aware Geometric Filter for Ultrasound Image Enhancement},
  booktitle = {Medical {{Image Understanding}} and {{Analysis}}},
  author = {Mishra, Deepak and Chaudhury, Santanu and Sarkar, Mukul and Soin, Arvinder Singh},
  year = {2017},
  volume = {723},
  pages = {109--120},
  publisher = {{Springer International Publishing}},
  address = {{Cham}}
}

@article{mishra_edge_2018,
  title = {Edge Probability and Pixel Relativity-Based Speckle Reducing Anisotropic Diffusion},
  author = {Mishra, Deepak and Chaudhury, Santanu and Sarkar, Mukul and Soin, Arvinder Singh and Sharma, Vivek},
  year = {2018},
  month = feb,
  journal = {IEEE Transactions on Image Processing},
  volume = {27},
  number = {2},
  pages = {649--664}
}

@inproceedings{mnih_asynchronous_2016,
  title = {Asynchronous {{Methods}} for {{Deep Reinforcement Learning}}},
  booktitle = {Proceedings of {{The}} 33rd {{International Conference}} on {{Machine Learning}}},
  author = {Mnih, Volodymyr and Badia, Adria Puigdomenech and Mirza, Mehdi and Graves, Alex and Lillicrap, Timothy and Harley, Tim and Silver, David and Kavukcuoglu, Koray},
  year = {2016},
  month = jun,
  series = {Proceedings of {{Machine Learning Research}}},
  volume = {48},
  pages = {1928--1937},
  publisher = {{PMLR}},
  address = {{New York, New York, USA}},
  abstract = {We propose a conceptually simple and lightweight framework for deep reinforcement learning that uses asynchronous gradient descent for optimization of deep neural network controllers. We present asynchronous variants of four standard reinforcement learning algorithms and show that parallel actor-learners have a stabilizing effect on training allowing all four methods to successfully train neural network controllers. The best performing method, an asynchronous variant of actor-critic, surpasses the current state-of-the-art on the Atari domain while training for half the time on a single multi-core CPU instead of a GPU. Furthermore, we show that asynchronous actor-critic succeeds on a wide variety of continuous motor control problems as well as on a new task of navigating random 3D mazes using a visual input.}
}

@article{mo_tracing_2016,
  title = {Tracing Analytic Ray Curves for Light and Sound Propagation in Non-Linear Media},
  author = {Mo, Qi and Yeh, Hengchin and Manocha, Dinesh},
  year = {2016},
  month = nov,
  journal = {IEEE Transactions on Visualization and Computer Graphics},
  volume = {22},
  number = {11},
  pages = {2493--2506},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\BPI5MNB7\\Mo et al. - 2016 - Tracing Analytic Ray Curves for Light and Sound Pr.pdf}
}

@article{mohamed_monte_,
  title = {Monte {{Carlo Gradient Estimation}} in {{Machine Learning}}},
  author = {Mohamed, Shakir and Rosca, Mihaela and Figurnov, Michael and Mnih, Andriy},
  abstract = {This paper is a broad and accessible survey of the methods we have at our disposal for Monte Carlo gradient estimation in machine learning and across the statistical sciences: the problem of computing the gradient of an expectation of a function with respect to parameters defining the distribution that is integrated; the problem of sensitivity analysis. In machine learning research, this gradient problem lies at the core of many learning problems, in supervised, unsupervised and reinforcement learning. We will generally seek to rewrite such gradients in a form that allows for Monte Carlo estimation, allowing them to be easily and efficiently used and analysed. We explore three strategies\textemdash the pathwise, score function, and measure-valued gradient estimators\textemdash exploring their historical development, derivation, and underlying assumptions. We describe their use in other fields, show how they are related and can be combined, and expand on their possible generalisations. Wherever Monte Carlo gradient estimators have been derived and deployed in the past, important advances have followed. A deeper and more widely-held understanding of this problem will lead to further advances, and it is these advances that we wish to support.},
  langid = {english},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\XW59MV6Y\\Mohamed et al. - Monte Carlo Gradient Estimation in Machine Learnin.pdf}
}

@article{mohamed_monte_2020,
  title = {Monte {{Carlo}} Gradient Estimation in Machine Learning},
  author = {Mohamed, Shakir and Rosca, Mihaela and Figurnov, Michael and Mnih, Andriy},
  year = {2020},
  journal = {Journal of Machine Learning Research},
  volume = {21},
  number = {132},
  pages = {1--62},
  abstract = {This paper is a broad and accessible survey of the methods we have at our disposal for Monte Carlo gradient estimation in machine learning and across the statistical sciences: the problem of computing the gradient of an expectation of a function with respect to parameters defining the distribution that is integrated; the problem of sensitivity analysis. In machine learning research, this gradient problem lies at the core of many learning problems, in supervised, unsupervised and reinforcement learning. We will generally seek to rewrite such gradients in a form that allows for Monte Carlo estimation, allowing them to be easily and efficiently used and analysed. We explore three strategies---the pathwise, score function, and measure-valued gradient estimators---exploring their historical development, derivation, and underlying assumptions. We describe their use in other fields, show how they are related and can be combined, and expand on their possible generalisations. Wherever Monte Carlo gradient estimators have been derived and deployed in the past, important advances have followed. A deeper and more widely-held understanding of this problem will lead to further advances, and it is these advances that we wish to support.},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\NF24DQAJ\\Mohamed et al. - 2020 - Monte Carlo Gradient Estimation in Machine Learnin.pdf;C\:\\Users\\msca8h\\Zotero\\storage\\TWINXLA9\\mc_gradients.html}
}

@article{mohamed_monte_2020a,
  title = {Monte {{Carlo}} Gradient Estimation in Machine Learning},
  author = {Mohamed, Shakir and Rosca, Mihaela and Figurnov, Michael and Mnih, Andriy},
  year = {2020},
  journal = {Journal of Machine Learning Research},
  volume = {21},
  number = {132},
  pages = {1--62},
  abstract = {This paper is a broad and accessible survey of the methods we have at our disposal for Monte Carlo gradient estimation in machine learning and across the statistical sciences: the problem of computing the gradient of an expectation of a function with respect to parameters defining the distribution that is integrated; the problem of sensitivity analysis. In machine learning research, this gradient problem lies at the core of many learning problems, in supervised, unsupervised and reinforcement learning. We will generally seek to rewrite such gradients in a form that allows for Monte Carlo estimation, allowing them to be easily and efficiently used and analysed. We explore three strategies---the pathwise, score function, and measure-valued gradient estimators---exploring their historical development, derivation, and underlying assumptions. We describe their use in other fields, show how they are related and can be combined, and expand on their possible generalisations. Wherever Monte Carlo gradient estimators have been derived and deployed in the past, important advances have followed. A deeper and more widely-held understanding of this problem will lead to further advances, and it is these advances that we wish to support.},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\RUH9UZVK\\Mohamed et al. - 2020 - Monte Carlo Gradient Estimation in Machine Learnin.pdf;C\:\\Users\\msca8h\\Zotero\\storage\\G7LNP5R6\\mc_gradients.html}
}

@techreport{moore_metropolishastings_2020,
  type = {Technical {{Report}}},
  title = {Metropolis-{{Hastings}} Forward Proposal},
  author = {Moore, Robert E.},
  year = {2020},
  month = may,
  institution = {{University of Liverpool}}
}

@article{morton_variational_2011,
  title = {Variational {{Bayesian}} Learning for Mixture Autoregressive Models with Uncertain-Order},
  author = {Morton, K D and Torrione, P A and Collins, L M},
  year = {2011},
  month = jun,
  journal = {IEEE Transactions on Signal Processing},
  volume = {59},
  number = {6},
  pages = {2614--2627}
}

@inproceedings{moshavegh_advanced_2015,
  title = {Advanced Automated Gain Adjustments for In-Vivo Ultrasound Imaging},
  booktitle = {{{IEEE International Ultrasonics Symposium}}},
  author = {Moshavegh, Ramin and Hemmsen, Martin Christian and Martins, Bo and Hansen, Kristoffer Lindskov and Ewertsen, Caroline and Brandt, Andreas Hjelm and Bechsgaard, Thor and Nielsen, Michael Bachmann and Jensen, Jorgen Arendt},
  year = {2015},
  month = oct,
  pages = {1--4},
  publisher = {{IEEE}},
  address = {{Taipei, Taiwan}},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\5WDSSY5J\\Moshavegh et al. - 2015 - Advanced automated gain adjustments for in-vivo ul.pdf}
}

@article{muller_latencyhiding_2016,
  title = {Latency-{{Hiding Work Stealing}}},
  author = {Muller, S. K. and Acar, U. A.},
  year = {2016},
  journal = {Proceedings of the 28th ACM Symposium on Parallelism in Algorithms and Architectures: SPAA '16},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\JRBMFTWY\\latency_hiding_WS.pdf}
}

@inproceedings{muralidharan_nitro_2014,
  title = {Nitro: {{A Framework}} for {{Adaptive Code Variant Tuning}}},
  shorttitle = {Nitro},
  booktitle = {2014 {{IEEE}} 28th {{International Parallel}} and {{Distributed Processing Symposium}}},
  author = {Muralidharan, Saurav and Shantharam, Manu and Hall, Mary and Garland, Michael and Catanzaro, Bryan},
  year = {2014},
  month = may,
  pages = {501--512},
  publisher = {{IEEE}},
  address = {{Phoenix, AZ, USA}},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\7C36F2UG\\Muralidharan et al. - 2014 - Nitro A Framework for Adaptive Code Variant Tunin.pdf}
}

@book{murphy_machine_2012,
  title = {Machine Learning: A Probabilistic Perspective},
  shorttitle = {Machine Learning},
  author = {Murphy, Kevin P.},
  year = {2012},
  series = {Adaptive Computation and Machine Learning Series},
  publisher = {{The MIT Press}},
  address = {{Cambridge, Massachusetts London, England}},
  langid = {english},
  annotation = {OCLC: 812073227},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\3XQPF7G8\\Murphy - 2012 - Machine learning a probabilistic perspective.pdf}
}

@article{murray_anytime_2017,
  title = {Anytime {{Monte Carlo}}},
  author = {Murray, Lawrence M. and Singh, Sumeetpal and Jacob, Pierre E. and Lee, Anthony},
  year = {2017},
  month = jun,
  journal = {arXiv:1612.03319 [stat]},
  eprint = {1612.03319},
  eprinttype = {arxiv},
  primaryclass = {stat},
  abstract = {A Monte Carlo algorithm typically simulates some prescribed number of samples, taking some random real time to complete the computations necessary. This work considers the converse: to impose a real-time budget on the computation, so that the number of samples simulated is random. To complicate matters, the real time taken for each simulation may depend on the sample produced, so that the samples themselves are not independent of their number, and a length bias with respect to compute time is apparent. This is especially problematic when a Markov chain Monte Carlo (MCMC) algorithm is used and the final state of the Markov chain---rather than an average over all states---is required. The length bias does not diminish with the compute budget in this case. It occurs, for example, in sequential Monte Carlo (SMC) algorithms. We propose an anytime framework to address the concern, using a continuous-time Markov jump process to study the progress of the computation in real time. We show that the length bias can be eliminated for any MCMC algorithm by using a multiple chain construction. The utility of this construction is demonstrated on a large-scale SMC-squared implementation, using four billion particles distributed across a cluster of 128 graphics processing units on the Amazon EC2 service. The anytime framework imposes a real-time budget on the MCMC move steps within SMC-squared, ensuring that all processors are simultaneously ready for the resampling step, demonstrably reducing wait times and providing substantial control over the total compute budget.},
  archiveprefix = {arXiv},
  keywords = {Statistics - Computation,Statistics - Machine Learning},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\Q2YJ6GBX\\Murray et al. - 2017 - Anytime Monte Carlo.pdf;C\:\\Users\\msca8h\\Zotero\\storage\\Q58ID2J8\\1612.html}
}

@inproceedings{murray_elliptical_2010,
  title = {Elliptical Slice Sampling},
  booktitle = {Proceedings of the {{Thirteenth International Conference}} on {{Artificial Intelligence}} and {{Statistics}}},
  author = {Murray, Iain and Adams, Ryan and MacKay, David},
  year = {2010},
  month = may,
  series = {Proceedings of {{Machine Learning Research}}},
  volume = {9},
  pages = {541--548},
  publisher = {{PMLR}},
  address = {{Chia Laguna Resort, Sardinia, Italy}},
  abstract = {Many probabilistic models introduce strong dependencies between variables using a latent multivariate Gaussian distribution or a Gaussian process. We present a new Markov chain Monte Carlo algorithm for performing inference in models with multivariate Gaussian priors. Its key properties are: 1) it has simple, generic code applicable to many models, 2) it has no free parameters, 3) it works well for a variety of Gaussian process based models. These properties make our method ideal for use while model building, removing the need to spend time deriving and tuning updates for more complex algorithms.}
}

@incollection{murray_slice_2010,
  title = {Slice Sampling Covariance Hyperparameters of Latent {{Gaussian}} Models},
  booktitle = {Advances in {{Neural Information Processing Systems}} 23},
  author = {Murray, Iain and Adams, Ryan P},
  year = {2010},
  pages = {1732--1740},
  publisher = {{Curran Associates, Inc.}}
}

@inproceedings{murray2010distributed,
  title = {Distributed {{Markov}} Chain {{Monte Carlo}}},
  booktitle = {Neural {{Information Processing Systems Workshop}} on {{Leaning}} on {{Cores}}, {{Clusters}}, and {{Clouds}}},
  author = {Murray, Lawrence},
  year = {2010}
}

@phdthesis{myers_traitement_2019,
  title = {Le Traitement, l'interpr\'etation et l'exploitation d'images Sonar \`a Antenne Synth\'etique Obtenues \`a Partir de Trajectoires R\'ep\'etitives},
  author = {Myers, Vincent},
  year = {2019},
  school = {\'Ecole nationale sup\'erieure de techniques avanc\'ees Bretagne},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\PKPY879L\\Myers - 2019 - Le traitement, l'interprtation et l'exploitation .pdf}
}

@inproceedings{mytkowicz_producing_2009,
  title = {Producing {{Wrong Data Without Doing Anything Obviously Wrong}}!},
  booktitle = {Proceedings of the 14th {{International Conference}} on {{Architectural Support}} for {{Programming Languages}} and {{Operating Systems}}},
  author = {Mytkowicz, Todd and Diwan, Amer and Hauswirth, Matthias and Sweeney, Peter F.},
  year = {2009},
  series = {{{ASPLOS XIV}}},
  pages = {265--276},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  keywords = {bias,measurement,performance},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\D2GNBCCN\\Mytkowicz et al. - 2009 - Producing Wrong Data Without Doing Anything Obviou.pdf}
}

@article{nadler_nonparametric_2010,
  title = {Nonparametric Detection of Signals by Information Theoretic Criteria: {{Performance}} Analysis and an Improved Estimator},
  shorttitle = {Nonparametric {{Detection}} of {{Signals}} by {{Information Theoretic Criteria}}},
  author = {Nadler, Boaz},
  year = {2010},
  month = may,
  journal = {IEEE Transactions on Signal Processing},
  volume = {58},
  number = {5},
  pages = {2746--2756},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\H5MB534R\\Nadler - 2010 - Nonparametric Detection of Signals by Information .pdf}
}

@article{naesseth_elements_2019,
  title = {Elements of {{Sequential Monte Carlo}}},
  author = {Naesseth, Christian A. and Lindsten, Fredrik and Sch{\"o}n, Thomas B.},
  year = {2019},
  journal = {Foundations and Trends\textregistered{} in Machine Learning},
  volume = {12},
  number = {3},
  pages = {307--392},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\IX9KJ4ZI\\Naesseth et al. - 2019 - Elements of Sequential Monte Carlo.pdf}
}

@article{nagare_multi_2017,
  title = {A Multi Directional Perfect Reconstruction Filter Bank Designed with 2-{{D}} Eigenfilter Approach: Application to Ultrasound Speckle Reduction},
  shorttitle = {A {{Multi Directional Perfect Reconstruction Filter Bank Designed}} with 2-{{D Eigenfilter Approach}}},
  author = {Nagare, Mukund B and Patil, Bhushan D and Holambe, Raghunath S},
  year = {2017},
  month = feb,
  journal = {Journal of Medical Systems},
  volume = {41},
  number = {2},
  pages = {31},
  langid = {english}
}

@article{nakarmi_kernelbased_2017,
  title = {A {{Kernel-Based Low-Rank}} ({{KLR}}) {{Model}} for {{Low-Dimensional Manifold Recovery}} in {{Highly Accelerated Dynamic MRI}}},
  author = {Nakarmi, Ukash and Wang, Yanhua and Lyu, Jingyuan and Liang, Dong and Ying, Leslie},
  year = {2017},
  month = nov,
  journal = {IEEE Transactions on Medical Imaging},
  volume = {36},
  number = {11},
  pages = {2297--2307},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\2T4DFE2H\\Nakarmi et al. - 2017 - A Kernel-Based Low-Rank (KLR) Model for Low-Dimens.pdf}
}

@article{nannuru_sparse_2018,
  title = {Sparse {{Bayesian}} Learning for Beamforming Using Sparse Linear Arrays},
  author = {Nannuru, Santosh and Koochakzadeh, Ali and Gemba, Kay L. and Pal, Piya and Gerstoft, Peter},
  year = {2018},
  month = nov,
  journal = {The Journal of the Acoustical Society of America},
  volume = {144},
  number = {5},
  pages = {2719--2729},
  langid = {english}
}

@article{natarovskii_quantitative_2021,
  title = {Quantitative Spectral Gap Estimate and {{Wasserstein}} Contraction of Simple Slice Sampling},
  author = {Natarovskii, Viacheslav and Rudolf, Daniel and Sprungk, Bj{\"o}rn},
  year = {2021},
  month = apr,
  journal = {The Annals of Applied Probability},
  volume = {31},
  number = {2},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\DQ24TAM4\\Natarovskii et al. - 2021 - Quantitative spectral gap estimate and Wasserstein.pdf}
}

@article{nayar_surface_1991,
  title = {Surface Reflection: Physical and Geometrical Perspectives},
  shorttitle = {Surface Reflection},
  author = {Nayar, S.K. and Ikeuchi, K. and Kanade, T.},
  year = {1991},
  month = jul,
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {13},
  number = {7},
  pages = {611--634}
}

@article{neal_annealed_2001,
  title = {Annealed Importance Sampling},
  author = {Neal, Radford M.},
  year = {2001},
  journal = {Statistics and Computing},
  volume = {11},
  number = {2},
  pages = {125--139}
}

@book{neal_bayesian_1996,
  title = {Bayesian {{Learning}} for {{Neural Networks}}},
  author = {Neal, Radford M.},
  year = {1996},
  series = {Lecture {{Notes}} in {{Statistics}}},
  volume = {118},
  publisher = {{Springer New York}},
  address = {{New York, NY}}
}

@inproceedings{neal_classification_2006,
  title = {Classification with {{Bayesian}} Neural Networks},
  booktitle = {Machine {{Learning Challenges}}: {{Evaluating Predictive Uncertainty}}, {{Visual Object Classification}}, and {{Recognising Tectual Entailment}}},
  author = {Neal, Radford M.},
  year = {2006},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {28--32},
  publisher = {{Springer}},
  address = {{Berlin, Heidelberg}},
  abstract = {I submitted entries for the two classification problems \textemdash{} ``Catalysis'' and ``Gatineau'' \textemdash{} in the Evaluating Predictive Uncertainty Challenge. My entry for Catalysis was the best one; my entry for Gatineau was the third best, behind two similar entries by Nitesh Chawla.},
  langid = {english},
  keywords = {Bayesian Neural Network,Hide Unit,Markov Chain Monte Carlo Method,Neural Network Model,Training Case},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\DY44UXHD\\Neal - 2006 - Classification with Bayesian Neural Networks.pdf}
}

@article{neal_improved_1992,
  title = {An {{Improved Acceptance Procedure}} for the {{Hybrid Monte Carlo Algorithm}}},
  author = {Neal, R. M.},
  year = {1992},
  month = aug,
  journal = {arXiv:hep-lat/9208011},
  eprint = {hep-lat/9208011},
  eprinttype = {arxiv},
  abstract = {The probability of accepting a candidate move in the hybrid Monte Carlo algorithm can be increased by considering a transition to be between windows of several states at the beginning and end of the trajectory, with a state within the selected window being chosen according to the Boltzmann probabilities. The detailed balance condition used to justify the algorithm still holds with this procedure, provided the start state is randomly positioned within its window. The new procedure is shown empirically to significantly improve performance for a test system of uncoupled oscillators.},
  archiveprefix = {arXiv},
  keywords = {High Energy Physics - Lattice},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\AMQA6TDS\\Neal - 1992 - An Improved Acceptance Procedure for the Hybrid Mo.pdf;C\:\\Users\\msca8h\\Zotero\\storage\\5QDMHZBL\\9208011.html}
}

@article{neal_improved_1994,
  title = {An Improved Acceptance Procedure for the Hybrid {{Monte Carlo}} Algorithm},
  author = {Neal, Radford M.},
  year = {1994},
  month = mar,
  journal = {Journal of Computational Physics},
  volume = {111},
  number = {1},
  pages = {194--203},
  langid = {english},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\9VY623A5\\Neal - 1994 - An Improved Acceptance Procedure for the Hybrid Mo.pdf}
}

@incollection{neal_mcmc_2011a,
  title = {{{MCMC}} Using {{Hamiltonian}} Dynamics},
  booktitle = {Handbook of {{Markov Chain Monte Carlo}}},
  author = {Neal, Radford M},
  year = {2011},
  series = {Handbooks of {{Modern Statistical Methods}}},
  edition = {First},
  pages = {113--162},
  publisher = {{Chapman and Hall/CRC}}
}

@techreport{neal_probabilistic_1993,
  title = {Probabilistic Inference Using Markov Chain Monte Carlo Methods},
  author = {Neal, Radford M.},
  year = {1993},
  month = sep,
  number = {CRG-TR-93-1},
  institution = {{University of Toronto}}
}

@inproceedings{neal_regression_1998,
  title = {Regression and Classification Using {{Gaussian}} Process Priors (with Discussions)},
  booktitle = {Bayesian {{Statistics}}},
  author = {Neal, Radford M.},
  year = {1998},
  pages = {475--501},
  publisher = {{Oxford University Press}}
}

@article{neal_slice_2003,
  title = {Slice Sampling},
  author = {Neal, Radford M.},
  year = {2003},
  month = jun,
  journal = {The Annals of Statistics},
  volume = {31},
  number = {3},
  pages = {705--767},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\EE77CBUZ\\Neal - 2003 - Slice sampling.pdf}
}

@inproceedings{neasham_broadband_2007,
  title = {Broadband, Ultra-Sparse Array Processing for Low Complexity Multibeam Sonar Imaging},
  booktitle = {Proceedings of {{OCEANS}} - {{Europe}}},
  author = {Neasham, Jeffrey A. and Menon, Raghav and Hinton, Oliver R.},
  year = {2007},
  month = jun,
  series = {{{OCEANS}}'07},
  pages = {1--6},
  publisher = {{IEEE}},
  address = {{Aberdeen, Scotland, UK}}
}

@techreport{neklyudov_metropolishastings_2019,
  title = {Metropolis-{{Hastings}} View on Variational Inference and Adversarial Training},
  author = {Neklyudov, Kirill and Egorov, Evgenii and Shvechikov, Pavel and Vetrov, Dmitry},
  year = {2019},
  month = jun,
  number = {arXiv:1810.07151 [cs, stat]},
  eprint = {1810.07151},
  eprinttype = {arxiv},
  institution = {{ArXiv}},
  abstract = {A significant part of MCMC methods can be considered as the Metropolis-Hastings (MH) algorithm with different proposal distributions. From this point of view, the problem of constructing a sampler can be reduced to the question - how to choose a proposal for the MH algorithm? To address this question, we propose to learn an independent sampler that maximizes the acceptance rate of the MH algorithm, which, as we demonstrate, is highly related to the conventional variational inference. For Bayesian inference, the proposed method compares favorably against alternatives to sample from the posterior distribution. Under the same approach, we step beyond the scope of classical MCMC methods and deduce the Generative Adversarial Networks (GANs) framework from scratch, treating the generator as the proposal and the discriminator as the acceptance test. On real-world datasets, we improve Frechet Inception Distance and Inception Score, using different GANs as a proposal distribution for the MH algorithm. In particular, we demonstrate improvements of recently proposed BigGAN model on ImageNet.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\ZNZ52NNU\\Neklyudov et al. - 2019 - Metropolis-Hastings view on variational inference .pdf;C\:\\Users\\msca8h\\Zotero\\storage\\2MB7FLBH\\1810.html}
}

@article{nelder_simplex_1965,
  title = {A Simplex Method for Function Minimization},
  author = {Nelder, J. A. and Mead, R.},
  year = {1965},
  month = jan,
  journal = {The Computer Journal},
  volume = {7},
  number = {4},
  pages = {308--313},
  langid = {english}
}

@inproceedings{nelson_generating_2015,
  title = {Generating {{Efficient Tensor Contractions}} for {{GPUs}}},
  booktitle = {2015 44th {{International Conference}} on {{Parallel Processing}}},
  author = {Nelson, Thomas and Rivera, Axel and Balaprakash, Prasanna and Hall, Mary and Hovland, Paul D. and Jessup, Elizabeth and Norris, Boyana},
  year = {2015},
  month = sep,
  pages = {969--978},
  publisher = {{IEEE}},
  address = {{Beijing, China}}
}

@inproceedings{nemeth_automatic_2020,
  title = {Automatic Parallelization of Probabilistic Models with Varying Load Imbalance},
  booktitle = {Proceedings of the 20th {{IEEE}}/{{ACM International Symposium}} on {{Cluster}}, {{Cloud}} and {{Internet}}},
  author = {Nemeth, Balazs and Haber, Tom and Liesenborgs, Jori and Lamotte, Wim},
  year = {2020},
  month = may,
  series = {{{CCGrid}}'20},
  pages = {752--759},
  publisher = {{IEEE}},
  address = {{Melbourne, Australia}}
}

@inproceedings{nemeth_distributed_2017,
  title = {Distributed {{Affine-Invariant MCMC Sampler}}},
  booktitle = {2017 {{IEEE International Conference}} on {{Cluster Computing}} ({{CLUSTER}})},
  author = {Nemeth, Balazs and Haber, Tom and Liesenborgs, Jori and Lamotte, Wim},
  year = {2017},
  month = sep,
  pages = {520--524},
  publisher = {{IEEE}},
  address = {{Honolulu, HI, USA}}
}

@inproceedings{nemeth_relaxing_2018,
  title = {Relaxing Scalability Limits with Speculative Parallelism in {{Sequential Monte Carlo}}},
  booktitle = {Proceedings of the 2018 {{IEEE International Conference}} on {{Cluster Computing}}},
  author = {Nemeth, Balazs and Haber, Tom and Liesenborgs, Jori and Lamotte, Wim},
  year = {2018},
  month = sep,
  series = {{{CLUSTER}}'18},
  pages = {494--503},
  publisher = {{IEEE}},
  address = {{Belfast}}
}

@article{nemirovski_robust_2009,
  title = {Robust {{Stochastic Approximation Approach}} to {{Stochastic Programming}}},
  author = {Nemirovski, A. and Juditsky, A. and Lan, G. and Shapiro, A.},
  year = {2009},
  month = jan,
  journal = {SIAM J. on Optimization},
  volume = {19},
  number = {4},
  pages = {1574--1609},
  keywords = {complexity,minimax problems,mirror descent algorithm,Monte Carlo sampling,saddle point,sample average approximation method,stochastic approximation,stochastic programming}
}

@article{nesterov_method_1983,
  title = {A Method of Solving a Convex Programming Problem with Convergence Rate $O(1/k^2)$},
  author = {Nesterov, Yurii Evgen'evich},
  year = {1983},
  journal = {Doklady Akademii Nauk SSSR},
  volume = {269},
  number = {3},
  pages = {543--547}
}

@article{nesterov_primaldual_2009,
  title = {Primal-Dual Subgradient Methods for Convex Problems},
  author = {Nesterov, Yurii},
  year = {2009},
  month = aug,
  journal = {Mathematical Programming},
  volume = {120},
  number = {1},
  pages = {221--259},
  langid = {english}
}

@inproceedings{NEURIPS2018_1371bcce,
  title = {On {{Markov}} Chain Gradient Descent},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Sun, Tao and Sun, Yuejiao and Yin, Wotao},
  year = {2018},
  volume = {31},
  publisher = {{Curran Associates, Inc.}}
}

@inproceedings{NEURIPS2018_1cd138d0,
  title = {Variational Inference with Tail-Adaptive f-{{Divergence}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Wang, Dilin and Liu, Hao and Liu, Qiang},
  year = {2018},
  volume = {31},
  publisher = {{Curran Associates, Inc.}}
}

@inproceedings{NEURIPS2018_25db67c5,
  title = {Importance Weighting and Variational Inference},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Domke, Justin and Sheldon, Daniel R},
  year = {2018},
  volume = {31},
  publisher = {{Curran Associates, Inc.}}
}

@inproceedings{NEURIPS2018_584b98aa,
  title = {Meta-Learning {{MCMC}} Proposals},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Wang, Tongzhou and WU, YI and Moore, Dave and Russell, Stuart J},
  year = {2018},
  volume = {31},
  publisher = {{Curran Associates, Inc.}}
}

@inproceedings{NEURIPS2018_a3f390d8,
  title = {Doubly Robust {{Bayesian}} Inference for Non-Stationary Streaming Data with \beta-{{Divergences}}},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Knoblauch, Jeremias and Jewson, Jack E and Damoulas, Theodoros},
  year = {2018},
  volume = {31},
  publisher = {{Curran Associates, Inc.}}
}

@inproceedings{NEURIPS2019_aec851e5,
  title = {Estimating Convergence of Markov Chains with L-Lag Couplings},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Biswas, Niloy and Jacob, Pierre E and Vanetti, Paul},
  year = {2019},
  volume = {32},
  pages = {7391--7401},
  publisher = {{Curran Associates, Inc.}}
}

@inproceedings{NEURIPS2020_10fb6cfa,
  title = {Re-Examining Linear Embeddings for High-Dimensional Bayesian Optimization},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Letham, Ben and Calandra, Roberto and Rai, Akshara and Bakshy, Eytan},
  year = {2020},
  series = {{{NeurIPS}}'20},
  volume = {33},
  pages = {1546--1558},
  publisher = {{Curran Associates, Inc.}}
}

@inproceedings{NEURIPS2020_7880d722,
  title = {Gibbs Sampling with People},
  booktitle = {Adv. {{Neural Inf}}. {{Process}}. {{Syst}}.},
  author = {Harrison, Peter and Marjieh, Raja and Adolfi, Federico and {van Rijn}, Pol and {Anglada-Tort}, Manuel and Tchernichovski, Ofer and {Larrouy-Maestri}, Pauline and Jacoby, Nori},
  year = {2020},
  volume = {33},
  pages = {10659--10671},
  publisher = {{Curran Associates, Inc.}}
}

@inproceedings{NEURIPS2020_7cac11e2,
  title = {Robust, Accurate Stochastic Optimization for Variational Inference},
  booktitle = {Advances in Neural Information Processing Systems ({{NeurIPS}})},
  author = {Dhaka, Akash Kumar and Catalina, Alejandro and Andersen, Michael R and Magnusson, M{\aa}ns and Huggins, Jonathan and Vehtari, Aki},
  year = {2020},
  volume = {33},
  pages = {10961--10973},
  publisher = {{Curran Associates, Inc.}}
}

@inproceedings{NEURIPS2020_b2070693,
  title = {Markovian Score Climbing: {{Variational}} Inference with KL(p||q)},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Naesseth, Christian and Lindsten, Fredrik and Blei, David},
  year = {2020},
  volume = {33},
  pages = {15499--15510},
  publisher = {{Curran Associates, Inc.}}
}

@inproceedings{NEURIPS2020_c928d86f,
  title = {F-{{Divergence}} Variational Inference},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Wan, Neng and Li, Dapeng and Hovakimyan, Naira},
  year = {2020},
  volume = {33},
  pages = {17370--17379},
  publisher = {{Curran Associates, Inc.}}
}

@inproceedings{NEURIPS2021_148148d6,
  title = {Automatic Symmetry Discovery with Lie Algebra Convolutional Network},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Dehmamy, Nima and Walters, Robin and Liu, Yanchen and Wang, Dashun and Yu, Rose},
  year = {2021},
  volume = {34},
  pages = {2503--2515},
  publisher = {{Curran Associates, Inc.}}
}

@article{ng_wideband_2005,
  title = {Wideband Array Signal Processing Using {{MCMC}} Methods},
  author = {Ng, W. and Reilly, J.P. and Kirubarajan, T. and Larocque, J.-R.},
  year = {2005},
  month = feb,
  journal = {IEEE Transactions on Signal Processing},
  volume = {53},
  number = {2},
  pages = {411--426}
}

@article{nguyen_efficient_2016,
  title = {Efficient {{Sequential Monte-Carlo Samplers}} for {{Bayesian Inference}}},
  author = {Nguyen, Thi Le Thu and Septier, Francois and Peters, Gareth W. and Delignon, Yves},
  year = {2016},
  month = mar,
  journal = {IEEE Transactions on Signal Processing},
  volume = {64},
  number = {5},
  pages = {1305--1319}
}

@inproceedings{nguyen_sgd_2018,
  title = {{{SGD}} and {{Hogwild}}! {{Convergence}} without the Bounded Gradients Assumption},
  booktitle = {Proceedings of the International Conference on Machine Learning},
  author = {Nguyen, Lam and Nguyen, Phuong Ha and {van Dijk}, Marten and Richtarik, Peter and Scheinberg, Katya and Takac, Martin},
  year = {2018},
  month = jul,
  series = {{{PMLR}}},
  volume = {80},
  pages = {3750--3758},
  publisher = {{ML Research Press}},
  abstract = {Stochastic gradient descent (SGD) is the optimization algorithm of choice in many machine learning applications such as regularized empirical risk minimization and training deep neural networks. The classical convergence analysis of SGD is carried out under the assumption that the norm of the stochastic gradient is uniformly bounded. While this might hold for some loss functions, it is always violated for cases where the objective function is strongly convex. In (Bottou et al.,2016), a new analysis of convergence of SGD is performed under the assumption that stochastic gradients are bounded with respect to the true gradient norm. Here we show that for stochastic problems arising in machine learning such bound always holds; and we also propose an alternative convergence analysis of SGD with diminishing learning rate regime, which results in more relaxed conditions than those in (Bottou et al.,2016). We then move on the asynchronous parallel setting, and prove convergence of Hogwild! algorithm in the same regime, obtaining the first convergence results for this method in the case of diminished learning rate.},
  pdf = {http://proceedings.mlr.press/v80/nguyen18c/nguyen18c.pdf}
}

@inproceedings{nguyen_tight_2019,
  title = {Tight Dimension Independent Lower Bound on the Expected Convergence Rate for Diminishing Step Sizes in {{SGD}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {NGUYEN, PHUONG\_HA and Nguyen, Lam and {van Dijk}, Marten},
  year = {2019},
  volume = {32},
  publisher = {{Curran Associates, Inc.}},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\G7TEQEUQ\\NGUYEN et al. - 2019 - Tight Dimension Independent Lower Bound on the Exp.pdf}
}

@inproceedings{nielsen_bayesian_2013,
  title = {Bayesian Model Comparison and the {{BIC}} for Regression Models},
  booktitle = {{{IEEE Int}}. {{Conf}}. {{Acoust}}. {{Speech Signal Process}}.},
  author = {Nielsen, Jesper Kjoer and Christensen, Mads Groesboll and Jensen, Soren Holdt},
  year = {2013},
  month = may,
  pages = {6362--6366},
  publisher = {{IEEE}},
  address = {{Vancouver, BC, Canada}},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\ZD7CGLWY\\Nielsen et al. - 2013 - Bayesian model comparison and the BIC for regressi.pdf}
}

@article{nielsen_bayesian_2014,
  title = {Bayesian Model Comparison with the G-Prior},
  author = {Nielsen, Jesper Kjaer and Christensen, Mads Graesboll and Cemgil, Ali Taylan and Jensen, Soren Holdt},
  year = {2014},
  month = jan,
  journal = {IEEE Transactions on Signal Processing},
  volume = {62},
  number = {1},
  pages = {225--238},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\YFD3ZP8B\\Nielsen et al. - 2014 - Bayesian Model Comparison With the g-Prior.pdf}
}

@article{nielsen_bayesian_2014a,
  title = {Bayesian Model Comparison with the G-Prior},
  author = {Nielsen, Jesper Kjaer and Christensen, Mads Graesboll and Cemgil, Ali Taylan and Jensen, Soren Holdt},
  year = {2014},
  month = jan,
  journal = {IEEE Transactions on Signal Processing},
  volume = {62},
  number = {1},
  pages = {225--238},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\6V9MWGWM\\Nielsen et al. - 2014 - Bayesian Model Comparison With the g-Prior.pdf}
}

@inproceedings{NIPS2005_4491777b,
  title = {Sparse {{Gaussian}} Processes Using Pseudo-Inputs},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Snelson, Edward and Ghahramani, Zoubin},
  year = {2005},
  volume = {18},
  publisher = {{MIT Press}}
}

@inproceedings{NIPS2007_89d4402d,
  title = {Markov Chain Monte Carlo with People},
  booktitle = {Adv. {{Neural Inf}}. {{Process}}. {{Syst}}.},
  author = {Sanborn, Adam and Griffiths, Thomas},
  year = {2008},
  volume = {20},
  publisher = {{Curran Associates, Inc.}}
}

@inproceedings{NIPS2007_b6a1085a,
  title = {Active Preference Learning with Discrete Choice Data},
  booktitle = {Adv. {{Neural Inf}}. {{Process}}. {{Syst}}.},
  author = {Brochu, Eric and {de Freitas}, Nando and Ghosh, Abhijeet},
  year = {2008},
  volume = {20},
  publisher = {{Curran Associates, Inc.}}
}

@inproceedings{NIPS2009_7cce53cf,
  title = {Dual Averaging Method for Regularized Stochastic Learning and Online Optimization},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Xiao, Lin},
  year = {2009},
  series = {{{NIPS}}'09},
  volume = {22},
  pages = {2116--2124},
  publisher = {{Curran Associates, Inc.}}
}

@inproceedings{NIPS2013_7fe1f8ab,
  title = {Non-Strongly-Convex Smooth Stochastic Approximation with Convergence Rate {{O}}(1/n)},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Bach, Francis and Moulines, Eric},
  year = {2013},
  volume = {26},
  publisher = {{Curran Associates, Inc.}}
}

@inproceedings{NIPS2013_f33ba15e,
  title = {Multi-Task Bayesian Optimization},
  booktitle = {Adv. {{Neural Inf}}. {{Process}}. {{Syst}}.},
  author = {Swersky, Kevin and Snoek, Jasper and Adams, Ryan P},
  year = {2013},
  volume = {26},
  publisher = {{Curran Associates, Inc.}}
}

@incollection{NIPS2014_5450,
  title = {Asynchronous Anytime Sequential Monte Carlo},
  booktitle = {Advances in Neural Information Processing Systems 27},
  author = {Paige, Brooks and Wood, Frank and Doucet, Arnaud and Teh, Yee Whye},
  year = {2014},
  pages = {3410--3418},
  publisher = {{Curran Associates, Inc.}}
}

@inproceedings{NIPS2014_8c6744c9,
  title = {Automated Variational Inference for {{Gaussian}} Process Models},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Nguyen, Trung V and Bonilla, Edwin V},
  year = {2014},
  volume = {27},
  publisher = {{Curran Associates, Inc.}}
}

@incollection{NIPS2015_5888,
  title = {Variational Consensus Monte Carlo},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Rabinovich, Maxim and Angelino, Elaine and Jordan, Michael I},
  year = {2015},
  volume = {28},
  pages = {1207--1215},
  publisher = {{Curran Associates, Inc.}}
}

@incollection{NIPS2015_5986,
  title = {Parallelizing {{MCMC}} with Random Partition Trees},
  booktitle = {Advances in Neural Information Processing Systems 28},
  author = {Wang, Xiangyu and Guo, Fangjian and Heller, Katherine A and Dunson, David B},
  year = {2015},
  pages = {451--459},
  publisher = {{Curran Associates, Inc.}}
}

@inproceedings{NIPS2015_8303a79b,
  title = {Reflection, Refraction, and {{Hamiltonian Monte Carlo}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Mohasel Afshar, Hadi and Domke, Justin},
  year = {2015},
  series = {{{NIPS}}'15},
  volume = {28},
  publisher = {{Curran Associates, Inc.}}
}

@inproceedings{NIPS2016_7750ca35,
  title = {R\'enyi Divergence Variational Inference},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Li, Yingzhen and Turner, Richard E},
  year = {2016},
  volume = {29},
  publisher = {{Curran Associates, Inc.}}
}

@inproceedings{NIPS2017_35464c84,
  title = {Variational Inference via {$\chi$} Upper Bound Minimization},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Dieng, Adji Bousso and Tran, Dustin and Ranganath, Rajesh and Paisley, John and Blei, David},
  year = {2017},
  volume = {30},
  pages = {2729--2738},
  publisher = {{Curran Associates, Inc.}}
}

@incollection{NIPS2019_9107,
  title = {Sample Adaptive {{MCMC}}},
  booktitle = {Advances in Neural Information Processing Systems 32},
  author = {Zhu, Michael},
  year = {2019},
  pages = {9066--9077},
  publisher = {{Curran Associates, Inc.}}
}

@article{nishimura_recycling_2019,
  title = {Recycling Intermediate Steps to Improve {{Hamiltonian Monte Carlo}}},
  author = {Nishimura, Akihiko and Dunson, David},
  year = {2019},
  month = oct,
  journal = {Bayesian Analysis},
  langid = {english},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\5XB75UY3\\Nishimura and Dunson - 2019 - Recycling Intermediate Steps to Improve Hamiltonia.pdf}
}

@article{niu_hogwild_2011,
  title = {{{HOGWILD}}!: {{A Lock-Free Approach}} to {{Parallelizing Stochastic Gradient Descent}}},
  shorttitle = {{{HOGWILD}}!},
  author = {Niu, Feng and Recht, Benjamin and Re, Christopher and Wright, Stephen J.},
  year = {2011},
  month = jun,
  journal = {arXiv:1106.5730 [cs, math]},
  eprint = {1106.5730},
  eprinttype = {arxiv},
  primaryclass = {cs, math},
  abstract = {Stochastic Gradient Descent (SGD) is a popular algorithm that can achieve state-of-the-art performance on a variety of machine learning tasks. Several researchers have recently proposed schemes to parallelize SGD, but all require performance-destroying memory locking and synchronization. This work aims to show using novel theoretical analysis, algorithms, and implementation that SGD can be implemented without any locking. We present an update scheme called HOGWILD! which allows processors access to shared memory with the possibility of overwriting each other's work. We show that when the associated optimization problem is sparse, meaning most gradient updates only modify small parts of the decision variable, then HOGWILD! achieves a nearly optimal rate of convergence. We demonstrate experimentally that HOGWILD! outperforms alternative schemes that use locking by an order of magnitude.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Mathematics - Optimization and Control},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\6M96EILJ\\Niu et al. - 2011 - HOGWILD! A Lock-Free Approach to Parallelizing St.pdf;C\:\\Users\\msca8h\\Zotero\\storage\\AN85P4TC\\1106.html}
}

@book{nocedal_numerical_2006,
  title = {Numerical {{Optimization}}},
  author = {Nocedal, Jorge and Wright, Stephen J.},
  year = {2006},
  series = {Springer {{Ser}}. {{Oper}}. {{Res}}. {{Financial Eng}}.},
  publisher = {{Springer New York}},
  abstract = {Springer Series on Operations Research and Financial Engineering},
  langid = {english},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\6J42ZVT8\\2006 - Numerical Optimization.pdf}
}

@article{ochotorena_anisotropic_2020,
  title = {Anisotropic Guided Filtering},
  author = {Ochotorena, Carlo Noel and Yamashita, Yukihiko},
  year = {2020},
  journal = {IEEE Transactions on Image Processing},
  volume = {29},
  pages = {1397--1412}
}

@inproceedings{okane_resolution_2010,
  title = {Resolution Improvement of Wideband Direction-of-Arrival Estimation "{{Squared-TOPS}}"},
  booktitle = {2010 {{IEEE International Conference}} on {{Communications}}},
  author = {Okane, K. and Ohtsuki, T.},
  year = {2010},
  month = may,
  pages = {1--5},
  publisher = {{IEEE}},
  address = {{Cape Town, South Africa}}
}

@mastersthesis{olander_constrained_2020,
  title = {Constrained Space {{MCMC}} Methods for Nested Sampling {{Bayesian}} Computations},
  author = {Olander, Jacob},
  year = {2020},
  address = {{Gothenburg, Seweden}},
  school = {Chalmers University of Technology}
}

@article{olivier_openmp_2012,
  title = {{{OpenMP}} Task Scheduling Strategies for Multicore {{NUMA}} Systems},
  author = {Olivier, S. L. and Porterfield, A. K. and Wheeler, K. B. and Spiegel, M. and Prins, J. F.},
  year = {2012},
  journal = {The International Journal of High Performance Computing Applications},
  volume = {26},
  number = {2},
  pages = {110--124},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\VUXYUD89\\OMP_scheduling_NUMA_analysis_2012.pdf}
}

@article{olson_scattering_2019,
  title = {Scattering Statistics of Rock Outcrops: {{Model-data}} Comparisons and {{Bayesian}} Inference Using Mixture Distributions},
  shorttitle = {Scattering Statistics of Rock Outcrops},
  author = {Olson, Derek R. and Lyons, Anthony P. and Abraham, Douglas A. and S{\ae}b{\o}, Torstein O.},
  year = {2019},
  month = feb,
  journal = {The Journal of the Acoustical Society of America},
  volume = {145},
  number = {2},
  pages = {761--774},
  langid = {english},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\9INWU7W2\\Olson et al. - 2019 - Scattering statistics of rock outcrops Model-data.pdf}
}

@book{openmparchitecturereviewboard_openmp_2008,
  title = {{{OpenMP}} Application Program Interface Version 3.0},
  author = {OpenMP Architecture Review Board},
  year = {2008},
  month = may
}

@inproceedings{orvieto2022dynamics,
  title = {Dynamics of {{SGD}} with Stochastic {{Polyak}} Stepsizes: {{Truly}} Adaptive Variants and Convergence to Exact Solution},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Orvieto, Antonio and {Lacoste-Julien}, Simon and Loizou, Nicolas},
  year = {2022}
}

@article{ottersten_directionofarrival_1990,
  title = {Direction-of-Arrival Estimation for Wide-Band Signals Using the {{ESPRIT}} Algorithm},
  author = {Ottersten, B. and Kailath, T.},
  year = {1990},
  month = feb,
  journal = {IEEE Transactions on Acoustics, Speech, and Signal Processing},
  volume = {38},
  number = {2},
  pages = {317--327},
  abstract = {A novel direction-of-arrival estimation algorithm is proposed that applies to wideband emitter signals. A sensor array with a translation invariance structure is assumed, and an extension of the ESPRIT algorithm for narrowband emitter signals is obtained. The emitter signals are modeled as the stationary output of a finite-dimensional linear system driven by white noise. The array response to a unit impulse from a given direction is represented as the impulse response of a linear system. The measured data from the sensor array can then be seen as the output of a multidimensional linear system driven by white noise sources and corrupted by additive noise. The emitter signals and the array output are characterized by the modes of the linear system. The ESPRIT algorithm is applied at the poles of the system, the power of the signals sharing the pole is captured, and the effect of noise is reduced. The algorithm requires no knowledge, storage, or search of the array manifold, as opposed to wideband extensions of the MUSIC algorithm. This results in a computationally efficient algorithm that is insensitive to array perturbations. Simulations are presented comparing the wideband and ESPRIT algorithm to the modal signal subspace method and the coherent signal subspace method.{$<>$}},
  keywords = {Direction of arrival estimation,Linear systems,Multidimensional systems,Narrowband,Noise measurement,Sensor arrays,Sensor phenomena and characterization,Sensor systems,White noise,Wideband},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\ZWGLG9AA\\103067.html}
}

@article{ottersten_directionofarrival_1990a,
  title = {Direction-of-Arrival Estimation for Wide-Band Signals Using the {{ESPRIT}} Algorithm},
  author = {Ottersten, B. and Kailath, T.},
  year = {Feb./1990},
  journal = {IEEE Transactions on Acoustics, Speech, and Signal Processing},
  volume = {38},
  number = {2},
  pages = {317--327}
}

@techreport{ottersten_exact_,
  type = {Technical {{Report}}},
  title = {Exact and {{Large Sample ML Techniques}} for {{Parameter Estimation}} and {{Detection}} in {{Array Processing}}},
  author = {Ottersten, B. and Viberg, M. and Stoica, P. and Nehorai, A.},
  number = {TRITA-SB-9302},
  address = {{Stockholm, Sweden}},
  institution = {{Royal Institute of Technology}}
}

@incollection{ottersten_exact_1993,
  title = {Exact and Large Sample Maximum Likelihood Techniques for Parameter Estimation and Detection in Array Processing},
  booktitle = {Radar {{Array Processing}}},
  author = {Ottersten, B. and Viberg, M. and Stoica, P. and Nehorai, A.},
  year = {1993},
  series = {Springer {{Series}} in {{Information Sciences}}},
  pages = {99--151},
  publisher = {{Springer}},
  address = {{Berlin, Heidelberg}},
  abstract = {Sensor array signal processing deals with the problem of extracting information from a collection of measurements obtained from sensors distributed in space. The number of signals present is assumed to be finite, and each signal is parameterized by a finite number of parameters. Based on measurements of the array output, the objective is to estimate the signals and their parameters. This research area has attracted considerable interest for several years. A vast number of algorithms has appeared in the literature for estimating unknown signal parameters from the measured output of a sensor array.},
  langid = {english},
  keywords = {Generalize Likelihood Ratio Test,Maximum Likelihood Technique,Sensor Array,Signal Subspace,Signal Waveform}
}

@article{outtas_subjective_2018,
  title = {Subjective and Objective Evaluations of Feature Selected Multi Output Filter for Speckle Reduction on Ultrasound Images},
  author = {Outtas, M and Zhang, L and Deforges, O and Serir, A and Hamidouche, W and Chen, Y},
  year = {2018},
  month = sep,
  journal = {Physics in Medicine \& Biology},
  volume = {63},
  number = {18},
  pages = {185014},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\8Q3JEVUH\\Outtas et al. - 2018 - Subjective and objective evaluations of feature se.pdf}
}

@misc{owen_adaptive_,
  title = {Adaptive {{Importance Sampling}}},
  author = {Owen, Art},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\XHV9NTDC\\Owen - Adaptive Importance Sampling.pdf}
}

@article{owen_safe_2000,
  title = {Safe and Effective Importance Sampling},
  author = {Owen, Art and Zhou, Yi},
  year = {2000},
  month = mar,
  journal = {Journal of the American Statistical Association},
  volume = {95},
  number = {449},
  pages = {135--143},
  langid = {english},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\J9UPP34N\\Owen and Zhou - 2000 - Safe and effective importance sampling.pdf}
}

@article{paananen_implicitly_2021,
  title = {Implicitly Adaptive Importance Sampling},
  author = {Paananen, Topi and Piironen, Juho and B{\"u}rkner, Paul-Christian and Vehtari, Aki},
  year = {2021},
  month = mar,
  journal = {Statistics and Computing},
  volume = {31},
  number = {2},
  pages = {16},
  abstract = {Abstract             Adaptive importance sampling is a class of techniques for finding good proposal distributions for importance sampling. Often the proposal distributions are standard probability distributions whose parameters are adapted based on the mismatch between the current proposal and a target distribution. In this work, we present an implicit adaptive importance sampling method that applies to complicated distributions which are not available in closed form. The method iteratively matches the moments of a set of Monte Carlo draws to weighted moments based on importance weights. We apply the method to Bayesian leave-one-out cross-validation and show that it performs better than many existing parametric adaptive importance sampling methods while being computationally inexpensive.},
  langid = {english},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\QVRDSELK\\Paananen et al. - 2021 - Implicitly adaptive importance sampling.pdf}
}

@inproceedings{pan_fast_2006,
  title = {Fast, Automatic, Procedure-Level Performance Tuning},
  booktitle = {Proceedings of the 15th International Conference on {{Parallel}} Architectures and Compilation Techniques  - {{PACT}} '06},
  author = {Pan, Zhelong and Eigenmann, Rudolf},
  year = {2006},
  pages = {173},
  publisher = {{ACM Press}},
  address = {{Seattle, Washington, USA}},
  langid = {english}
}

@inproceedings{pandey_experimental_2022,
  title = {Experimental Validation of Wideband {{SBL}} Models for {{DOA}} Estimation},
  booktitle = {2022 30th {{European Signal Processing Conference}} ({{EUSIPCO}})},
  author = {Pandey, Ruchi and Nannuru, Santosh and Gerstoft, Peter},
  year = {2022},
  month = aug,
  pages = {219--223},
  publisher = {{IEEE}},
  address = {{Belgrade, Serbia}}
}

@article{papamarkou_challenges_2019,
  title = {Challenges in {{Bayesian}} Inference via {{Markov}} Chain {{Monte Carlo}} for Neural Networks},
  author = {Papamarkou, Theodore and Hinkle, Jacob and Young, M. Todd and Womble, David},
  year = {2019},
  month = nov,
  journal = {arXiv:1910.06539 [cs, stat]},
  eprint = {1910.06539},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Markov chain Monte Carlo (MCMC) methods and neural networks are instrumental in tackling inferential and prediction problems. However, Bayesian inference based on joint use of MCMC methods and of neural networks is limited. This paper reviews the main challenges posed by neural networks to MCMC developments, including lack of parameter identifiability due to weight symmetries, prior specification effects, and consequently high computational cost and convergence failure. Population and manifold MCMC algorithms are combined to demonstrate these challenges via multilayer perceptron (MLP) examples and to develop case studies for assessing the capacity of approximate inference methods to uncover the posterior covariance of neural network parameters. Some of these challenges, such as high computational cost arising from the application of neural networks to big data and parameter identifiability arising from weight symmetries, stimulate research towards more scalable approximate MCMC methods or towards MCMC methods in reduced parameter spaces.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Computation,Statistics - Machine Learning,Statistics - Methodology},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\3XLF57HA\\Papamarkou et al. - 2019 - Challenges in Bayesian inference via Markov chain .pdf;C\:\\Users\\msca8h\\Zotero\\storage\\BT3S5QT3\\1910.html}
}

@article{papaspiliopoulos_bayesian_2017,
  title = {Bayesian Block-Diagonal Variable Selection and Model Averaging},
  author = {Papaspiliopoulos, O. and Rossell, D.},
  year = {2017},
  month = jun,
  journal = {Biometrika},
  volume = {104},
  number = {2},
  pages = {343--359},
  abstract = {We propose a scalable algorithmic framework for exact Bayesian variable selection and model averaging in linear models under the assumption that the Gram matrix is block-diagonal, and as a heuristic for exploring the model space for general designs. In block-diagonal designs our approach returns the most probable model of any given size without resorting to numerical integration. The algorithm also provides a novel and efficient solution to the frequentist best subset selection problem for block-diagonal designs. Posterior probabilities for any number of models are obtained by evaluating a single one-dimensional integral, and other quantities of interest such as variable inclusion probabilities and model-averaged regression estimates are obtained by an adaptive, deterministic one-dimensional numerical integration. The overall computational cost scales linearly with the number of blocks, which can be processed in parallel, and exponentially with the block size, rendering it most adequate in situations where predictors are organized in many moderately-sized blocks. For general designs, we approximate the Gram matrix by a block-diagonal matrix using spectral clustering and propose an iterative algorithm that capitalizes on the block-diagonal algorithms to explore efficiently the model space. All methods proposed in this paper are implemented in the R library mombf.},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\P8QNFLI3\\3752497.html}
}

@inproceedings{papini_stochastic_2018,
  title = {Stochastic Variance-Reduced Policy Gradient},
  booktitle = {Proceedings of the {{International Conference}} on {{Machine Learning}}},
  author = {Papini, Matteo and Binaghi, Damiano and Canonaco, Giuseppe and Pirotta, Matteo and Restelli, Marcello},
  year = {2018},
  month = jul,
  pages = {4026--4035},
  publisher = {{PMLR}},
  abstract = {In this paper, we propose a novel reinforcement-learning algorithm consisting in a stochastic variance-reduced version of policy gradient for solving Markov Decision Processes (MDPs). Stochastic variance-reduced gradient (SVRG) methods have proven to be very successful in supervised learning. However, their adaptation to policy gradient is not straightforward and needs to account for I) a non-concave objective function; II) approximations in the full gradient computation; and III) a non-stationary sampling process. The result is SVRPG, a stochastic variance-reduced policy gradient algorithm that leverages on importance weights to preserve the unbiasedness of the gradient estimate. Under standard assumptions on the MDP, we provide convergence guarantees for SVRPG with a convergence rate that is linear under increasing batch sizes. Finally, we suggest practical variants of SVRPG, and we empirically evaluate them on continuous MDPs.},
  langid = {english},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\JMGFN6FY\\Papini et al. - 2018 - Stochastic Variance-Reduced Policy Gradient.pdf}
}

@misc{papp_tpapp_2020,
  title = {Tpapp/{{DynamicHMC}}.Jl: V2.1.6},
  shorttitle = {Tpapp/{{DynamicHMC}}.Jl},
  author = {Papp, Tamas K. and JackRab and Dilum Aluthge and TagBot, Julia and Piibeleht, Morten},
  year = {2020},
  month = aug,
  abstract = {DynamicHMC v2.1.6 Diff since v2.1.5 {$<$}strong{$>$}Closed issues:{$<$}/strong{$>$} Struggling to Construct a working sampler (\#99) {$<$}strong{$>$}Merged pull requests:{$<$}/strong{$>$} A typo in the worked example (\#121) (@JackRab) a small issue in linking (\#122) (@JackRab) one redundant "them" (\#123) (@JackRab) CompatHelper: bump compat for "Optim" to "0.21" (\#125) (@github-actions[bot]) Travis CI: Build documentation on Julia 1.4 (\#126) (@DilumAluthge) CompatHelper: bump compat for "Optim" to "0.22" (\#129) (@github-actions[bot])},
  copyright = {Open Access},
  howpublished = {Zenodo}
}

@article{pasarica_adaptively_2007,
  title = {Adaptively {{Scaling}} the {{Metropolis Algorithm Using Expected Squared Jumped Distance}}},
  author = {Pasarica, Cristian and Gelman, Andrew},
  year = {2007},
  journal = {SSRN Electronic Journal},
  langid = {english},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\KXWCXDVN\\Gelman and Pasarica - 2007 - Adaptively Scaling the Metropolis Algorithm Using .pdf}
}

@inproceedings{patel_communication_2019,
  title = {Communication Trade-Offs for Synchronized Distributed {{SGD}} with Large Step Size},
  booktitle = {{{arXiv}}:1904.11325 [Cs, Math, Stat]},
  author = {Patel, Kumar Kshitij and Dieuleveut, Aymeric},
  year = {2019},
  month = apr,
  eprint = {1904.11325},
  eprinttype = {arxiv},
  primaryclass = {cs, math, stat},
  abstract = {Synchronous mini-batch SGD is state-of-the-art for large-scale distributed machine learning. However, in practice, its convergence is bottlenecked by slow communication rounds between worker nodes. A natural solution to reduce communication is to use the \textbackslash emph\{`local-SGD'\} model in which the workers train their model independently and synchronize every once in a while. This algorithm improves the computation-communication trade-off but its convergence is not understood very well. We propose a non-asymptotic error analysis, which enables comparison to \textbackslash emph\{one-shot averaging\} i.e., a single communication round among independent workers, and \textbackslash emph\{mini-batch averaging\} i.e., communicating at every step. We also provide adaptive lower bounds on the communication frequency for large step-sizes (\$ t\^\{-\textbackslash alpha\} \$, \$ \textbackslash alpha\textbackslash in (1/2 , 1 ) \$) and show that \textbackslash emph\{Local-SGD\} reduces communication by a factor of \$O\textbackslash Big(\textbackslash frac\{\textbackslash sqrt\{T\}\}\{P\^\{3/2\}\}\textbackslash Big)\$, with \$T\$ the total number of gradients and \$P\$ machines.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Mathematics - Optimization and Control,Statistics - Machine Learning},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\VD3I2Y3T\\Patel and Dieuleveut - 2019 - Communication trade-offs for synchronized distribu.pdf;C\:\\Users\\msca8h\\Zotero\\storage\\IMFJ6K8X\\1904.html}
}

@inproceedings{patel_global_2022,
  title = {Global Convergence and Stability of Stochastic Gradient Descent},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Patel, Vivak and Zhang, Shushu and Tian, Bowen},
  year = {2022},
  month = oct,
  abstract = {In machine learning, stochastic gradient descent (SGD) is widely deployed to train models using highly non-convex objectives with equally complex noise models. Unfortunately, SGD theory often makes restrictive assumptions that fail to capture the non-convexity of real problems, and almost entirely ignore the complex noise models that exist in practice. In this work, we demonstrate the restrictiveness of these assumptions using three canonical models in machine learning, then we develop novel theoretical tools to address this shortcoming in two ways. First, we establish that SGD's iterates will either globally converge to a stationary point or diverge under nearly arbitrary nonconvexity and noise models. Under a slightly more restrictive assumption on the joint behavior of the non-convexity and noise model that generalizes current assumptions in the literature, we show that the objective function cannot diverge, even if the iterates diverge. As a consequence of our results, SGD can be applied to a greater range of stochastic optimization problems with confidence about its global convergence behavior and stability.},
  langid = {english},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\5JXCP585\\Patel et al. - 2022 - Global Convergence and Stability of Stochastic Gra.pdf;C\:\\Users\\msca8h\\Zotero\\storage\\E8NIK2ZN\\forum.html}
}

@misc{patel_stochastic_2021,
  title = {Stochastic Gradient Descent on Nonconvex Functions with General Noise Models},
  author = {Patel, Vivak and Zhang, Shushu},
  year = {2021},
  month = apr,
  number = {arXiv:2104.00423},
  eprint = {2104.00423},
  eprinttype = {arxiv},
  primaryclass = {math},
  publisher = {{arXiv}},
  abstract = {Stochastic Gradient Descent (SGD) is a widely deployed optimization procedure throughout data-driven and simulation-driven disciplines, which has drawn a substantial interest in understanding its global behavior across a broad class of nonconvex problems and noise models. Recent analyses of SGD have made noteworthy progress in this direction, and these analyses have innovated important and insightful new strategies for understanding SGD. However, these analyses often have imposed certain restrictions (e.g., convexity, global Lipschitz continuity, uniform Holder continuity, expected smoothness, etc.) that leave room for innovation. In this work, we address this gap by proving that, for a rather general class of nonconvex functions and noise models, SGD's iterates either diverge to infinity or converge to a stationary point with probability one. By further restricting to globally Holder continuous functions and the expected smoothness noise model, we prove that -- regardless of whether the iterates diverge or remain finite -- the norm of the gradient function evaluated at SGD's iterates converges to zero with probability one and in expectation. As a result of our work, we broaden the scope of nonconvex problems and noise models to which SGD can be applied with rigorous guarantees of its global behavior.},
  archiveprefix = {arXiv},
  keywords = {65K05; 68Q25; 90C06; 90C30; 68T05,Mathematics - Optimization and Control},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\NUE2FWF3\\Patel and Zhang - 2021 - Stochastic Gradient Descent on Nonconvex Functions.pdf;C\:\\Users\\msca8h\\Zotero\\storage\\Z6FSDKZ2\\2104.html}
}

@article{patil_pymc_2010,
  title = {{{PyMC}}: {{Bayesian}} Stochastic Modelling in {{Python}}},
  shorttitle = {{{{\textbf{PyMC}}}}},
  author = {Patil, Anand and Huard, David and Fonnesbeck, Christopher},
  year = {2010},
  journal = {Journal of Statistical Software},
  volume = {35},
  number = {4},
  langid = {english},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\TNTW2PMU\\Patil et al. - 2010 - PyMC  Bayesian Stochastic Modelling in .pdf}
}

@article{patterson_improvement_1983,
  title = {The Improvement and Quantitative Assessment of {{B-mode}} Images Produced by an Annular Array/Cone Hybrid},
  author = {Patterson, M. S. and Foster, F. S.},
  year = {1983},
  month = jul,
  journal = {Ultrasonic Imaging},
  volume = {5},
  number = {3},
  pages = {195--213},
  abstract = {Hybrid ultrasound imaging systems, which combine spherical focusing on transmit with axicon focusing on receive, provide excellent resolution over a useful depth of field. This paper presents a new hybrid design with improved sensitivity, in which the axicon focusing is achieved by two conical mirrors and a PZT 5A disk cut into 8 sectors. We have investigated two methods of processing the signals from the 8 sectors. In the first, phase insensitive sector addition (PISA), the B-scan is formed from the sum of the 8 demodulated signals. In the second, multiplicative processing (MP), the 8 rf waveforms are multiplied and the resultant is demodulated to form the image. Both techniques result in smoothed speckle but degraded lateral resolution. As well, MP decreases the off-axis sensitivity of the system and artifacts characteristic of axicon focusing. Quantitative assessment of the effects of PISA and MP was performed using a new approach called contrast-to-speckle ratio (CSR). The CSR data, which is a measure of the image contrast of cylindrical voids in a random scattering medium relative to contrast fluctuations due to speckle, shows the superiority of PISA and MP. This conclusion is supported by images of in vitro human breast tissue.},
  langid = {english}
}

@article{paulin_error_2019,
  title = {Error Bounds for Sequential {{Monte Carlo}} Samplers for Multimodal Distributions},
  author = {Paulin, Daniel and Jasra, Ajay and Thiery, Alexandre},
  year = {2019},
  month = feb,
  journal = {Bernoulli},
  volume = {25},
  number = {1},
  pages = {310--340}
}

@article{pearce_structured_2020,
  title = {Structured Weight Priors for Convolutional Neural Networks},
  author = {Pearce, Tim and Foong, Andrew Y. K. and Brintrup, Alexandra},
  year = {2020},
  month = jul,
  journal = {Presented at the ICML Workshop on Uncertainty and Robustness in Deep Learning},
  eprint = {2007.14235},
  eprinttype = {arxiv},
  abstract = {Selection of an architectural prior well suited to a task (e.g. convolutions for image data) is crucial to the success of deep neural networks (NNs). Conversely, the weight priors within these architectures are typically left vague, e.g.\textasciitilde independent Gaussian distributions, which has led to debate over the utility of Bayesian deep learning. This paper explores the benefits of adding structure to weight priors. It initially considers first-layer filters of a convolutional NN, designing a prior based on random Gabor filters. Second, it considers adding structure to the prior of final-layer weights by estimating how each hidden feature relates to each class. Empirical results suggest that these structured weight priors lead to more meaningful functional priors for image data. This contributes to the ongoing discussion on the importance of weight priors.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\AJ8QJ9FA\\Pearce et al. - 2020 - Structured Weight Priors for Convolutional Neural .pdf;C\:\\Users\\msca8h\\Zotero\\storage\\RVT9QICL\\2007.html}
}

@article{pearce_uncertainty_2020,
  title = {Uncertainty in Neural Networks: Approximately {{Bayesian}} Ensembling},
  shorttitle = {Uncertainty in {{Neural Networks}}},
  author = {Pearce, Tim and Leibfried, Felix and Brintrup, Alexandra and Zaki, Mohamed and Neely, Andy},
  year = {2020},
  month = feb,
  journal = {arXiv:1810.05546 [cs, stat]},
  eprint = {1810.05546},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Understanding the uncertainty of a neural network's (NN) predictions is essential for many purposes. The Bayesian framework provides a principled approach to this, however applying it to NNs is challenging due to large numbers of parameters and data. Ensembling NNs provides an easily implementable, scalable method for uncertainty quantification, however, it has been criticised for not being Bayesian. This work proposes one modification to the usual process that we argue does result in approximate Bayesian inference; regularising parameters about values drawn from a distribution which can be set equal to the prior. A theoretical analysis of the procedure in a simplified setting suggests the recovered posterior is centred correctly but tends to have an underestimated marginal variance, and overestimated correlation. However, two conditions can lead to exact recovery. We argue that these conditions are partially present in NNs. Empirical evaluations demonstrate it has an advantage over standard ensembling, and is competitive with variational methods.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\3EUM8U35\\Pearce et al. - 2020 - Uncertainty in Neural Networks Approximately Baye.pdf;C\:\\Users\\msca8h\\Zotero\\storage\\479WDSFT\\1810.html}
}

@article{pei_closed_2012,
  title = {Closed Form Variable Fractional Time Delay Using {{FFT}}},
  author = {Pei, Soo-Chang and Lai, Yun-Chiu},
  year = {2012},
  month = may,
  journal = {IEEE Signal Processing Letters},
  volume = {19},
  number = {5},
  pages = {299--302}
}

@inproceedings{pei_closed_2014,
  title = {Closed Form Variable Fractional Delay Using {{FFT}} with Transition Band Trade-Off},
  booktitle = {{{IEEE International Symposium}} on {{Circuits}} and {{Systems}}},
  author = {Pei, Soo-Chang and Lai, Yun-Chiu},
  year = {2014},
  month = jun,
  pages = {978--981},
  publisher = {{IEEE}},
  address = {{Melbourne VIC, Australia}}
}

@inproceedings{penna_binlpt_2017,
  title = {{{BinLPT}}: A Novel Workload-Aware Loop Scheduler for Irregular Parallel Loops},
  booktitle = {Proc. {{Simp\'osio}} Em {{Sistemas Computacionais}} de {{Alto Desempenho}}},
  author = {Penna, Pedro Henrique and Castro, M{\'a}rcio and Plentz, Patr{\'i}cia and {Cota de Freitas}, Henrique and Broquedis, Fran{\c c}ois and M{\'e}haut, Jean-Fran{\c c}ois},
  year = {2017},
  month = oct,
  address = {{Campinas, Brazil}},
  file = {/home/msca8h/Documents/parallel_scheduling/Penna et al. - 2017 - BinLPT A Novel Workload-Aware Loop Scheduler for .pdf}
}

@article{penna_comprehensive_2019,
  title = {A Comprehensive Performance Evaluation of the {{BinLPT}} Workload-Aware Loop Scheduler},
  shorttitle = {A Comprehensive Performance Evaluation of the {{BinLPT}} Workload-Aware Loop Scheduler},
  author = {Penna, Pedro Henrique and A. Gomes, Ant{\^o}nio Tadeu and Castro, M{\'a}rcio and D.M. Plentz, Patricia and C. Freitas, Henrique and Broquedis, Fran{\c c}ois and M{\'e}haut, Jean-Fran{\c c}ois},
  year = {2019},
  month = feb,
  journal = {Concurrency and Computation: Practice and Experience},
  langid = {english},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\JAVWSZBZ\\Penna et al. - 2019 - A comprehensive performance evaluation of the BinL.pdf}
}

@article{penny_bayesian_2002,
  title = {Bayesian Multivariate Autoregressive Models with Structured Priors},
  author = {Penny, W.D. and Roberts, S.J.},
  year = {2002},
  journal = {IEE Proceedings - Vision, Image, and Signal Processing},
  volume = {149},
  number = {1},
  pages = {33},
  langid = {english}
}

@article{perona_scalespace_1990,
  title = {Scale-Space and Edge Detection Using Anisotropic Diffusion},
  author = {Perona, Pietro and Malik, Jitendra},
  year = {1990},
  month = jul,
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {12},
  number = {7},
  pages = {629--639},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\TDXHR2UW\\Perona and Malik - 1990 - Scale-space and edge detection using anisotropic d.pdf}
}

@article{perrot_you_2021,
  title = {So You Think You Can {{DAS}}? {{A}} Viewpoint on Delay-and-Sum Beamforming},
  shorttitle = {So You Think You Can {{DAS}}?},
  author = {Perrot, Vincent and Polichetti, Maxime and Varray, Fran{\c c}ois and Garcia, Damien},
  year = {2021},
  month = mar,
  journal = {Ultrasonics},
  volume = {111},
  pages = {106309},
  langid = {english},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\ZFSDTGSN\\Perrot et al. - 2021 - So you think you can DAS A viewpoint on delay-and.pdf}
}

@article{peskun_optimum_1973,
  title = {Optimum {{Monte-Carlo}} Sampling Using {{Markov}} Chains},
  author = {Peskun, P. H.},
  year = {1973},
  journal = {Biometrika},
  volume = {60},
  number = {3},
  pages = {607--612},
  langid = {english}
}

@article{peterson_explorations_1989,
  title = {Explorations of the Mean Field Theory Learning Algorithm},
  author = {Peterson, Carsten and Hartman, Eric},
  year = {1989},
  month = jan,
  journal = {Neural Networks},
  volume = {2},
  number = {6},
  pages = {475--494},
  langid = {english}
}

@article{peterson_mean_1987,
  title = {A Mean Field Theory Learning Algorithm for {{Neural Networks}}},
  author = {Peterson, Carsten and Anderson, James R.},
  year = {1987},
  journal = {Complex Systems},
  volume = {1},
  number = {5},
  pages = {995--1019}
}

@article{petrovic_benchmark_2020,
  title = {A Benchmark Set of Highly-Efficient {{CUDA}} and {{OpenCL}} Kernels and Its Dynamic Autotuning with {{Kernel Tuning Toolkit}}},
  author = {Petrovi{\v c}, Filip and St{\v r}el{\'a}k, David and Hozzov{\'a}, Jana and Ol'ha, Jaroslav and Trembeck{\'y}, Richard and Benkner, Siegfried and Filipovi{\v c}, Ji{\v r}{\'i}},
  year = {2020},
  month = jul,
  journal = {Future Generation Computer Systems},
  volume = {108},
  pages = {161--177},
  langid = {english},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\H74CASUZ\\Petrovi et al. - 2020 - A benchmark set of highly-efficient CUDA and OpenC.pdf}
}

@inproceedings{pfaffe_efficient_2019,
  title = {Efficient Hierarchical Online-Autotuning: A Case Study on Polyhedral Accelerator Mapping},
  shorttitle = {Efficient Hierarchical Online-Autotuning},
  booktitle = {Proceedings of the {{ACM International Conference}} on {{Supercomputing}}  - {{ICS}} '19},
  author = {Pfaffe, Philip and Grosser, Tobias and Tillmann, Martin},
  year = {2019},
  pages = {354--366},
  publisher = {{ACM Press}},
  address = {{Phoenix, Arizona}},
  langid = {english}
}

@inproceedings{pfaffe_onlineautotuning_2017,
  title = {Online-{{Autotuning}} in the {{Presence}} of {{Algorithmic Choice}}},
  booktitle = {2017 {{IEEE International Parallel}} and {{Distributed Processing Symposium Workshops}} ({{IPDPSW}})},
  author = {Pfaffe, Philip and Tillmann, Martin and Walter, Sigmar and Tichy, Walter F.},
  year = {2017},
  month = may,
  pages = {1379--1388},
  publisher = {{IEEE}},
  address = {{Orlando / Buena Vista, FL, USA}}
}

@inproceedings{pfander_autotunetmp_2018,
  title = {{{AutoTuneTMP}}: {{Auto-Tuning}} in {{C}}++ {{With Runtime Template Metaprogramming}}},
  shorttitle = {{{AutoTuneTMP}}},
  booktitle = {2018 {{IEEE International Parallel}} and {{Distributed Processing Symposium Workshops}} ({{IPDPSW}})},
  author = {Pfander, David and Brunn, Malte and Pfluger, Dirk},
  year = {2018},
  month = may,
  pages = {1123--1132},
  publisher = {{IEEE}},
  address = {{Vancouver, BC}}
}

@article{pfister_transfer_2001,
  title = {The Transfer Function Bake-Off},
  author = {Pfister, H. and Lorensen, B. and Bajaj, C. and Kindlmann, G. and Schroeder, W. and Avila, L.S. and Raghu, K.M. and Machiraju, R. and {Jinho Lee}},
  year = {Jan.-Feb./2001},
  journal = {IEEE Computer Graphics and Applications},
  volume = {21},
  number = {1},
  pages = {16--22}
}

@article{phan_color_2018,
  title = {Color Orchestra: {{Ordering}} Color Palettes for Interpolation and Prediction},
  shorttitle = {Color {{Orchestra}}},
  author = {Phan, Huy Q. and Fu, Hongbo and Chan, Antoni B.},
  year = {2018},
  month = jun,
  journal = {IEEE Transactions on Visualization and Computer Graphics},
  volume = {24},
  number = {6},
  pages = {1942--1955},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\GK6CW5LU\\Phan et al. - 2018 - Color Orchestra Ordering Color Palettes for Inter.pdf}
}

@book{pharr_physically_2010,
  title = {Physically Based Rendering: From Theory to Implementation},
  shorttitle = {Physically Based Rendering},
  author = {Pharr, Matt and Humphreys, Greg},
  year = {2010},
  edition = {2nd ed},
  publisher = {{Morgan Kaufmann/Elsevier}},
  address = {{Burlington, MA}},
  lccn = {T385 .P4756 2010},
  keywords = {Computer graphics,Three-dimensional display systems},
  annotation = {OCLC: ocn620294150},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\3BNZHALF\\Pharr and Humphreys - 2010 - Physically based rendering from theory to impleme.pdf}
}

@article{piironen_sparsity_2017,
  title = {Sparsity Information and Regularization in the Horseshoe and Other Shrinkage Priors},
  author = {Piironen, Juho and Vehtari, Aki},
  year = {2017},
  journal = {Electronic Journal of Statistics},
  volume = {11},
  number = {2},
  pages = {5018--5051},
  langid = {english},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\8TTYNG46\\Piironen and Vehtari - 2017 - Sparsity information and regularization in the hor.pdf}
}

@misc{pillai_ergodicity_2015,
  title = {Ergodicity of Approximate {{MCMC}} Chains with Applications to Large Data Sets},
  author = {Pillai, Natesh S. and Smith, Aaron},
  year = {2015},
  month = aug,
  number = {arXiv:1405.0182},
  eprint = {1405.0182},
  eprinttype = {arxiv},
  primaryclass = {math, stat},
  institution = {{arXiv}},
  abstract = {In many modern applications, difficulty in evaluating the posterior density makes performing even a single MCMC step slow. This difficulty can be caused by intractable likelihood functions, but also appears for routine problems with large data sets. Many researchers have responded by running approximate versions of MCMC algorithms. In this note, we develop quantitative bounds for showing the ergodicity of these approximate samplers. We then use these bounds to study the bias-variance trade-off of approximate MCMC algorithms. We apply our results to simple versions of recently proposed algorithms, including a variant of the "austerity" framework of Korratikara et al.},
  archiveprefix = {arXiv},
  keywords = {Mathematics - Probability,Mathematics - Statistics Theory,Statistics - Computation},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\ZJTS2ZFW\\Pillai and Smith - 2015 - Ergodicity of Approximate MCMC Chains with Applica.pdf;C\:\\Users\\msca8h\\Zotero\\storage\\VBSKDNXX\\1405.html}
}

@article{pillai_forward_1989,
  title = {Forward/Backward Spatial Smoothing Techniques for Coherent Signal Identification},
  author = {Pillai, S.U. and Kwon, B.H.},
  year = {1989},
  month = jan,
  journal = {IEEE Transactions on Acoustics, Speech, and Signal Processing},
  volume = {37},
  number = {1},
  pages = {8--15}
}

@article{pinton_sources_2011,
  title = {Sources of Image Degradation in Fundamental and Harmonic Ultrasound Imaging Using Nonlinear, Full-Wave Simulations},
  author = {Pinton, Gianmarco F and Trahey, Gregg E and Dahl, Jeremy J},
  year = {2011},
  month = apr,
  journal = {IEEE Transactions on Ultrasonics, Ferroelectrics and Frequency Control},
  volume = {58},
  number = {4},
  pages = {754--765},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\CIFQS4TH\\Pinton et al. - 2011 - Sources of image degradation in fundamental and ha.pdf}
}

@article{pizurica_versatile_2003,
  title = {A Versatile Wavelet Domain Noise Filtration Technique for Medical Imaging},
  author = {Pizurica, A. and Philips, W. and Lemahieu, I. and Acheroy, M.},
  year = {2003},
  month = mar,
  journal = {IEEE Transactions on Medical Imaging},
  volume = {22},
  number = {3},
  pages = {323--331},
  langid = {english}
}

@inproceedings{pmlr-v108-lorraine20a,
  title = {Optimizing Millions of Hyperparameters by Implicit Differentiation},
  booktitle = {Proceedings of the Twenty Third International Conference on Artificial Intelligence and Statistics},
  author = {Lorraine, Jonathan and Vicol, Paul and Duvenaud, David},
  year = {2020},
  month = aug,
  series = {Proceedings of Machine Learning Research},
  volume = {108},
  pages = {1540--1552},
  publisher = {{PMLR}},
  address = {{Online}},
  abstract = {We propose an algorithm for inexpensive gradient-based hyperparameter optimization that combines the implicit function theorem (IFT) with efficient inverse Hessian approximations. We present results about the relationship between the IFT and differentiating through optimization, motivating our algorithm. We use the proposed approach to train modern network architectures with millions of weights and millions of hyper-parameters. For example, we learn a data-augmentation network\textemdash where every weight is a hyperparameter tuned for validation performance\textemdash outputting augmented training examples. Jointly tuning weights and hyper-parameters is only a few times more costly in memory and compute than standard training.},
  pdf = {http://proceedings.mlr.press/v108/lorraine20a/lorraine20a.pdf}
}

@inproceedings{pmlr-v115-izmailov20a,
  title = {Subspace Inference for Bayesian Deep Learning},
  booktitle = {Proceedings of the Uncertainty in Artificial Intelligence Conference},
  author = {Izmailov, Pavel and Maddox, Wesley J. and Kirichenko, Polina and Garipov, Timur and Vetrov, Dmitry and Wilson, Andrew Gordon},
  year = {2020},
  month = jul,
  series = {{{PMLR}}},
  volume = {115},
  pages = {1169--1179},
  publisher = {{ML Research Press}},
  abstract = {Bayesian inference was once a gold standard for learning with neural networks, providing accurate full predictive distributions and well calibrated uncertainty. However, scaling Bayesian inference techniques to deep neural networks is challenging due to the high dimensionality of the parameter space. In this paper, we construct low-dimensional subspaces of parameter space, such as the first principal components of the stochastic gradient descent (SGD) trajectory, which contain diverse sets of high performing models. In these subspaces, we are able to apply elliptical slice sampling and variational inference, which struggle in the full parameter space. We show that Bayesian model averaging over the induced posterior in these subspaces produces accurate predictions and well-calibrated predictive uncertainty for both regression and image classification.},
  pdf = {http://proceedings.mlr.press/v115/izmailov20a/izmailov20a.pdf}
}

@inproceedings{pmlr-v118-xu20a,
  title = {{{AdvancedHMC}}.Jl: {{A}} Robust, Modular and Efficient Implementation of Advanced {{HMC}} Algorithms},
  booktitle = {Proceedings of {{The}} 2nd {{Symposium}} on  {{Advances}} in {{Approximate Bayesian Inference}}},
  author = {Xu, Kai and Ge, Hong and Tebbutt, Will and Tarek, Mohamed and Trapp, Martin and Ghahramani, Zoubin},
  year = {2020},
  month = dec,
  series = {{{PMLR}}},
  volume = {118},
  pages = {1--10},
  publisher = {{Proceedings of Machine Learning Research}},
  address = {{Vancouver, BC V6C 3B5, Canada}},
  abstract = {Stan's Hamilton Monte Carlo (HMC) has demonstrated remarkable sampling robustness and efficiency in a wide range of Bayesian inference problems through carefully crafted adaption schemes to the celebrated No-U-Turn sampler (NUTS) algorithm. It is challenging to implement these adaption schemes robustly in practice, hindering wider adoption amongst practitioners who are not directly working with the Stan modelling language. AdvancedHMC.jl (AHMC) contributes a modular, well-tested, standalone implementation of NUTS that recovers and extends Stan's NUTS algorithm. AHMC is written in Julia, a modern high-level language for scientic computing, benefoting from optional hardware acceleration and interoperability with a wealth of existing software written in both Julia and other languages, such as Python. Efficacy is demonstrated empirically by comparison with Stan through a third-party Markov chain Monte Carlo benchmarking suite.},
  pdf = {http://proceedings.mlr.press/v118/xu20a/xu20a.pdf}
}

@inproceedings{pmlr-v119-dusenberry20a,
  title = {Efficient and Scalable {{Bayesian}} Neural Nets with Rank-1 Factors},
  booktitle = {Proceedings of the 37th International Conference on Machine Learning},
  author = {Dusenberry, Michael and Jerfel, Ghassen and Wen, Yeming and Ma, Yian and Snoek, Jasper and Heller, Katherine and Lakshminarayanan, Balaji and Tran, Dustin},
  year = {2020},
  month = jul,
  series = {Proceedings of Machine Learning Research},
  volume = {119},
  pages = {2782--2792},
  publisher = {{PMLR}},
  abstract = {Bayesian neural networks (BNNs) demonstrate promising success in improving the robustness and uncertainty quantification of modern deep learning. However, they generally struggle with underfitting at scale and parameter efficiency. On the other hand, deep ensembles have emerged as alternatives for uncertainty quantification that, while outperforming BNNs on certain problems, also suffer from efficiency issues. It remains unclear how to combine the strengths of these two approaches and remediate their common issues. To tackle this challenge, we propose a rank-1 parameterization of BNNs, where each weight matrix involves only a distribution on a rank-1 subspace. We also revisit the use of mixture approximate posteriors to capture multiple modes, where unlike typical mixtures, this approach admits a significantly smaller memory increase (e.g., only a 0.4\% increase for a ResNet-50 mixture of size 10). We perform a systematic empirical study on the choices of prior, variational posterior, and methods to improve training. For ResNet-50 on ImageNet, Wide ResNet 28-10 on CIFAR-10/100, and an RNN on MIMIC-III, rank-1 BNNs achieve state-of-the-art performance across log-likelihood, accuracy, and calibration on the test sets and out-of-distribution variants.},
  pdf = {http://proceedings.mlr.press/v119/dusenberry20a/dusenberry20a.pdf}
}

@inproceedings{pmlr-v119-hoffman20a,
  title = {Black-Box Variational Inference as a Parametric Approximation to {{Langevin}} Dynamics},
  booktitle = {Proceedings of the International Conference on Machine Learning},
  author = {Hoffman, Matthew and Ma, Yian},
  year = {2020},
  month = jul,
  series = {{{PRML}}},
  volume = {119},
  pages = {4324--4341},
  publisher = {{ML Research Press}},
  abstract = {Variational inference (VI) and Markov chain Monte Carlo (MCMC) are approximate posterior inference algorithms that are often said to have complementary strengths, with VI being fast but biased and MCMC being slower but asymptotically unbiased. In this paper, we analyze gradient-based MCMC and VI procedures and find theoretical and empirical evidence that these procedures are not as different as one might think. In particular, a close examination of the Fokker-Planck equation that governs the Langevin dynamics (LD) MCMC procedure reveals that LD implicitly follows a gradient flow that corresponds to a variational inference procedure based on optimizing a nonparametric normalizing flow. This result suggests that the transient bias of LD (due to the Markov chain not having burned in) may track that of VI (due to the optimizer not having converged), up to differences due to VI's asymptotic bias and parameterization. Empirically, we find that the transient biases of these algorithms (and their momentum-accelerated counterparts) do evolve similarly. This suggests that practitioners with a limited time budget may get more accurate results by running an MCMC procedure (even if it's far from burned in) than a VI procedure, as long as the variance of the MCMC estimator can be dealt with (e.g., by running many parallel chains).},
  pdf = {http://proceedings.mlr.press/v119/hoffman20a/hoffman20a.pdf}
}

@inproceedings{pmlr-v119-malitsky20a,
  title = {Adaptive Gradient Descent without Descent},
  booktitle = {Proceedings of the 37th International Conference on Machine Learning},
  author = {Malitsky, Yura and Mishchenko, Konstantin},
  year = {2020},
  month = jul,
  series = {Proceedings of Machine Learning Research},
  volume = {119},
  pages = {6702--6712},
  publisher = {{PMLR}},
  abstract = {We present a strikingly simple proof that two rules are sufficient to automate gradient descent: 1) don't increase the stepsize too fast and 2) don't overstep the local curvature. No need for functional values, no line search, no information about the function except for the gradients. By following these rules, you get a method adaptive to the local geometry, with convergence guarantees depending only on the smoothness in a neighborhood of a solution. Given that the problem is convex, our method converges even if the global smoothness constant is infinity. As an illustration, it can minimize arbitrary continuously twice-differentiable convex function. We examine its performance on a range of convex and nonconvex problems, including logistic regression and matrix factorization.},
  pdf = {http://proceedings.mlr.press/v119/malitsky20a/malitsky20a.pdf}
}

@inproceedings{pmlr-v119-mikkola20a,
  title = {Projective Preferential {{Bayesian}} Optimization},
  booktitle = {Proc. {{Int}}. {{Conf}}. {{Mach}}. {{Learn}}.},
  author = {Mikkola, Petrus and Todorovi{\'c}, Milica and J{\"a}rvi, Jari and Rinke, Patrick and Kaski, Samuel},
  year = {2020},
  month = jul,
  series = {Proc. {{Mach}}. {{Learn}}. {{Res}}.},
  volume = {119},
  pages = {6884--6892},
  publisher = {{ML Research Press}},
  abstract = {Bayesian optimization is an effective method for finding extrema of a black-box function. We propose a new type of Bayesian optimization for learning user preferences in high-dimensional spaces. The central assumption is that the underlying objective function cannot be evaluated directly, but instead a minimizer along a projection can be queried, which we call a projective preferential query. The form of the query allows for feedback that is natural for a human to give, and which enables interaction. This is demonstrated in a user experiment in which the user feedback comes in the form of optimal position and orientation of a molecule adsorbing to a surface. We demonstrate that our framework is able to find a global minimum of a high-dimensional black-box function, which is an infeasible task for existing preferential Bayesian optimization frameworks that are based on pairwise comparisons.},
  pdf = {http://proceedings.mlr.press/v119/mikkola20a/mikkola20a.pdf}
}

@inproceedings{pmlr-v119-wu20h,
  title = {Amortized Population {{Gibbs}} Samplers with Neural Sufficient Statistics},
  booktitle = {Proceedings of the {{International Conference}} on {{Machine Learning}}},
  author = {Wu, Hao and Zimmermann, Heiko and Sennesh, Eli and Le, Tuan Anh and Van De Meent, Jan-Willem},
  year = {2020},
  month = jul,
  series = {{{PMLR}}},
  volume = {119},
  pages = {10421--10431},
  publisher = {{ML Research Press}},
  abstract = {We develop amortized population Gibbs (APG) samplers, a class of scalable methods that frame structured variational inference as adaptive importance sampling. APG samplers construct high-dimensional proposals by iterating over updates to lower-dimensional blocks of variables. We train each conditional proposal by minimizing the inclusive KL divergence with respect to the conditional posterior. To appropriately account for the size of the input data, we develop a new parameterization in terms of neural sufficient statistics. Experiments show that APG samplers can be used to train highly-structured deep generative models in an unsupervised manner, and achieve substantial improvements in inference accuracy relative to standard autoencoding variational methods.},
  pdf = {http://proceedings.mlr.press/v119/wu20h/wu20h.pdf}
}

@inproceedings{pmlr-v124-ou20a,
  title = {Joint Stochastic Approximation and Its Application to Learning Discrete Latent Variable Models},
  booktitle = {Proceedings of the {{International Conference}} on {{Uncertainty}} in {{Artifical Intelligence}}},
  author = {Ou, Zhijian and Song, Yunfu},
  year = {2020},
  month = aug,
  series = {{{PMLR}}},
  volume = {124},
  pages = {929--938},
  publisher = {{ML Research Press}},
  abstract = {Although with progress in introducing auxiliary amortized inference models, learning discrete latent variable models is still challenging. In this paper, we show that the annoying difficulty of obtaining reliable stochastic gradients for the inference model and the drawback of indirectly optimizing the target log-likelihood can be gracefully addressed in a new method based on stochastic approximation (SA) theory of the Robbins-Monro type. Specifically, we propose to directly maximize the target log-likelihood and simultaneously minimize the inclusive divergence between the posterior and the inference model. The resulting learning algorithm is called joint SA (JSA). To the best of our knowledge, JSA represents the first method that couples an SA version of the EM (expectation-maximization) algorithm (SAEM) with an adaptive MCMC procedure. Experiments on several benchmark generative modeling and structured prediction tasks show that JSA consistently outperforms recent competitive algorithms, with faster convergence, better final likelihoods, and lower variance of gradient estimates.},
  pdf = {http://proceedings.mlr.press/v124/ou20a/ou20a.pdf}
}

@inproceedings{pmlr-v130-loizou21a,
  title = {Stochastic {{Polyak}} Step-Size for {{sGD}}: {{An}} Adaptive Learning Rate for Fast Convergence},
  booktitle = {Proceedings of the International Conference on Artificial Intelligence and Statistics},
  author = {Loizou, Nicolas and Vaswani, Sharan and Hadj Laradji, Issam and {Lacoste-Julien}, Simon},
  year = {2021},
  month = apr,
  series = {Proceedings of Machine Learning Research},
  volume = {130},
  pages = {1306--1314},
  publisher = {{PMLR}},
  abstract = {We propose a stochastic variant of the classical Polyak step-size (Polyak, 1987) commonly used in the subgradient method. Although computing the Polyak step-size requires knowledge of the optimal function values, this information is readily available for typical modern machine learning applications. Consequently, the proposed stochastic Polyak step-size (SPS) is an attractive choice for setting the learning rate for stochastic gradient descent (SGD). We provide theoretical convergence guarantees for SGD equipped with SPS in different settings, including strongly convex, convex and non-convex functions. Furthermore, our analysis results in novel convergence guarantees for SGD with a constant step-size. We show that SPS is particularly effective when training over-parameterized models capable of interpolating the training data. In this setting, we prove that SPS enables SGD to converge to the true solution at a fast rate without requiring the knowledge of any problem-dependent constants or additional computational overhead. We experimentally validate our theoretical results via extensive experiments on synthetic and real datasets. We demonstrate the strong performance of SGD with SPS compared to state-of-the-art optimization methods when training over-parameterized models.},
  pdf = {http://proceedings.mlr.press/v130/loizou21a/loizou21a.pdf}
}

@inproceedings{pmlr-v139-campbell21a,
  title = {A Gradient Based Strategy for {{Hamiltonian Monte Carlo}} Hyperparameter Optimization},
  booktitle = {Proceedings of the {{International Conference}} on {{Machine Learning}}},
  author = {Campbell, Andrew and Chen, Wenlong and Stimper, Vincent and {Hernandez-Lobato}, Jose Miguel and Zhang, Yichuan},
  year = {2021},
  month = jul,
  series = {{{PMLR}}},
  volume = {139},
  pages = {1238--1248},
  publisher = {{ML Research Press}},
  abstract = {Hamiltonian Monte Carlo (HMC) is one of the most successful sampling methods in machine learning. However, its performance is significantly affected by the choice of hyperparameter values. Existing approaches for optimizing the HMC hyperparameters either optimize a proxy for mixing speed or consider the HMC chain as an implicit variational distribution and optimize a tractable lower bound that can be very loose in practice. Instead, we propose to optimize an objective that quantifies directly the speed of convergence to the target distribution. Our objective can be easily optimized using stochastic gradient descent. We evaluate our proposed method and compare to baselines on a variety of problems including sampling from synthetic 2D distributions, reconstructing sparse signals, learning deep latent variable models and sampling molecular configurations from the Boltzmann distribution of a 22 atom molecule. We find that our method is competitive with or improves upon alternative baselines in all these experiments.},
  pdf = {http://proceedings.mlr.press/v139/campbell21a/campbell21a.pdf}
}

@inproceedings{pmlr-v139-geffner21a,
  title = {On the Difficulty of Unbiased Alpha Divergence Minimization},
  booktitle = {Proceedings of the International Conference on Machine Learning},
  author = {Geffner, Tomas and Domke, Justin},
  year = {2021},
  month = jul,
  series = {{{PMLR}}},
  volume = {139},
  pages = {3650--3659},
  publisher = {{ML Research Press}},
  abstract = {Several approximate inference algorithms have been proposed to minimize an alpha-divergence between an approximating distribution and a target distribution. Many of these algorithms introduce bias, the magnitude of which becomes problematic in high dimensions. Other algorithms are unbiased. These often seem to suffer from high variance, but little is rigorously known. In this work we study unbiased methods for alpha-divergence minimization through the Signal-to-Noise Ratio (SNR) of the gradient estimator. We study several representative scenarios where strong analytical results are possible, such as fully-factorized or Gaussian distributions. We find that when alpha is not zero, the SNR worsens exponentially in the dimensionality of the problem. This casts doubt on the practicality of these methods. We empirically confirm these theoretical results.},
  pdf = {http://proceedings.mlr.press/v139/geffner21a/geffner21a.pdf}
}

@inproceedings{pmlr-v139-immer21a,
  title = {Scalable Marginal Likelihood Estimation for Model Selection in Deep Learning},
  booktitle = {Proceedings of the 38th International Conference on Machine Learning},
  author = {Immer, Alexander and Bauer, Matthias and Fortuin, Vincent and R{\"a}tsch, Gunnar and Emtiyaz, Khan Mohammad},
  year = {2021},
  month = jul,
  series = {Proceedings of Machine Learning Research},
  volume = {139},
  pages = {4563--4573},
  publisher = {{PMLR}},
  abstract = {Marginal-likelihood based model-selection, even though promising, is rarely used in deep learning due to estimation difficulties. Instead, most approaches rely on validation data, which may not be readily available. In this work, we present a scalable marginal-likelihood estimation method to select both hyperparameters and network architectures, based on the training data alone. Some hyperparameters can be estimated online during training, simplifying the procedure. Our marginal-likelihood estimate is based on Laplace's method and Gauss-Newton approximations to the Hessian, and it outperforms cross-validation and manual tuning on standard regression and image classification datasets, especially in terms of calibration and out-of-distribution detection. Our work shows that marginal likelihoods can improve generalization and be useful when validation data is unavailable (e.g., in nonstationary settings).},
  pdf = {http://proceedings.mlr.press/v139/immer21a/immer21a.pdf}
}

@inproceedings{pmlr-v151-brofos22a,
  title = {Adaptation of the Independent {{Metropolis-Hastings}} Sampler with Normalizing Flow Proposals},
  booktitle = {Proceedings of the International Conference on Artificial Intelligence and Statistics},
  author = {Brofos, James and Gabrie, Marylou and Brubaker, Marcus A. and Lederman, Roy R.},
  year = {2022},
  month = mar,
  series = {{{PMLR}}},
  volume = {151},
  pages = {5949--5986},
  publisher = {{ML Research Press}},
  abstract = {Markov Chain Monte Carlo (MCMC) methods are a powerful tool for computation with complex probability distributions. However the performance of such methods is critically dependent on properly tuned parameters, most of which are difficult if not impossible to know a priori for a given target distribution. Adaptive MCMC methods aim to address this by allowing the parameters to be updated during sampling based on previous samples from the chain at the expense of requiring a new theoretical analysis to ensure convergence. In this work we extend the convergence theory of adaptive MCMC methods to a new class of methods built on a powerful class of parametric density estimators known as normalizing flows. In particular, we consider an independent Metropolis-Hastings sampler where the proposal distribution is represented by a normalizing flow whose parameters are updated using stochastic gradient descent. We explore the practical performance of this procedure on both synthetic settings and in the analysis of a physical field system, and compare it against both adaptive and non-adaptive MCMC methods.},
  pdf = {https://proceedings.mlr.press/v151/brofos22a/brofos22a.pdf}
}

@inproceedings{pmlr-v161-jerfel21a,
  title = {Variational Refinement for Importance Sampling Using the Forward {{Kullback-Leibler}} Divergence},
  booktitle = {Proceedings of the {{International Conference}} on {{Uncertainty}} in {{Artifical Intelligence}}},
  author = {Jerfel, Ghassen and Wang, Serena and {Wong-Fannjiang}, Clara and Heller, Katherine A. and Ma, Yian and Jordan, Michael I.},
  year = {2021},
  month = jul,
  series = {{{PMLR}}},
  volume = {161},
  pages = {1819--1829},
  publisher = {{ML Research Press}},
  abstract = {Variational Inference (VI) is a popular alternative to asymptotically exact sampling in Bayesian inference. Its main workhorse is optimization over a reverse Kullback-Leibler divergence (RKL), which typically underestimates the tail of the posterior leading to miscalibration and potential degeneracy. Importance sampling (IS), on the other hand, is often used to fine-tune and de-bias the estimates of approximate Bayesian inference procedures. The quality of IS crucially depends on the choice of the proposal distribution. Ideally, the proposal distribution has heavier tails than the target, which is rarely achievable by minimizing the RKL. We thus propose a novel combination of optimization and sampling techniques for approximate Bayesian inference by constructing an IS proposal distribution through the minimization of a forward KL (FKL) divergence. This approach guarantees asymptotic consistency and a fast convergence towards both the optimal IS estimator and the optimal variational approximation. We empirically demonstrate on real data that our method is competitive with variational boosting and MCMC.},
  pdf = {https://proceedings.mlr.press/v161/jerfel21a/jerfel21a.pdf}
}

@inproceedings{pmlr-v37-hernandez-lobatoc15,
  title = {Probabilistic Backpropagation for Scalable Learning of {{Bayesian}} Neural Networks},
  booktitle = {Proceedings of the International Conference on Machine Learning},
  author = {{Hernandez-Lobato}, Jose Miguel and Adams, Ryan},
  year = {2015},
  month = jul,
  series = {{{PMLR}}},
  volume = {37},
  pages = {1861--1869},
  publisher = {{ML Research Press}},
  abstract = {Large multilayer neural networks trained with backpropagation have recently achieved state-of-the-art results in a wide range of problems. However, using backprop for neural net learning still has some disadvantages, e.g., having to tune a large number of hyperparameters to the data, lack of calibrated probabilistic predictions, and a tendency to overfit the training data. In principle, the Bayesian approach to learning neural networks does not have these problems. However, existing Bayesian techniques lack scalability to large dataset and network sizes. In this work we present a novel scalable method for learning Bayesian neural networks, called probabilistic backpropagation (PBP). Similar to classical backpropagation, PBP works by computing a forward propagation of probabilities through the network and then doing a backward computation of gradients. A series of experiments on ten real-world datasets show that PBP is significantly faster than other techniques, while offering competitive predictive abilities. Our experiments also show that PBP provides accurate estimates of the posterior variance on the network weights.},
  pdf = {http://proceedings.mlr.press/v37/hernandez-lobatoc15.pdf}
}

@inproceedings{pmlr-v37-kandasamy15,
  title = {High Dimensional Bayesian Optimisation and Bandits via Additive Models},
  author = {Kandasamy, Kirthevasan and Schneider, Jeff and Poczos, Barnabas},
  year = {2015},
  month = jul,
  series = {Proceedings of Machine Learning Research},
  volume = {37},
  pages = {295--304},
  publisher = {{PMLR}},
  address = {{Lille, France}},
  abstract = {Bayesian Optimisation (BO) is a technique used in optimising a D-dimensional function which is typically expensive to evaluate. While there have been many successes for BO in low dimensions, scaling it to high dimensions has been notoriously difficult. Existing literature on the topic are under very restrictive settings. In this paper, we identify two key challenges in this endeavour. We tackle these challenges by assuming an additive structure for the function. This setting is substantially more expressive and contains a richer class of functions than previous work. We prove that, for additive functions the regret has only linear dependence on D even though the function depends on all D dimensions. We also demonstrate several other statistical and computational benefits in our framework. Via synthetic examples, a scientific simulation and a face detection problem we demonstrate that our method outperforms naive BO on additive functions and on several examples where the function is not additive.},
  pdf = {http://proceedings.mlr.press/v37/kandasamy15.pdf}
}

@inproceedings{pmlr-v37-salimans15,
  title = {Markov Chain {{Monte Carlo}} and Variational Inference: {{Bridging}} the Gap},
  booktitle = {Proceedings of the {{International Conference}} on {{Machine Learning}}},
  author = {Salimans, Tim and Kingma, Diederik and Welling, Max},
  year = {2015},
  month = jul,
  series = {{{PMLR}}},
  volume = {37},
  pages = {1218--1226},
  publisher = {{ML Research Press}},
  abstract = {Recent advances in stochastic gradient variational inference have made it possible to perform variational Bayesian inference with posterior approximations containing auxiliary random variables. This enables us to explore a new synthesis of variational inference and Monte Carlo methods where we incorporate one or more steps of MCMC into our variational approximation. By doing so we obtain a rich class of inference algorithms bridging the gap between variational methods and MCMC, and offering the best of both worlds: fast posterior approximation through the maximization of an explicit objective, with the option of trading off additional computation for additional accuracy. We describe the theoretical foundations that make this possible and show some promising first results.},
  pdf = {http://proceedings.mlr.press/v37/salimans15.pdf}
}

@inproceedings{pmlr-v48-sa16,
  title = {Ensuring Rapid Mixing and Low Bias for Asynchronous Gibbs Sampling},
  booktitle = {Proceedings of the 33rd International Conference on Machine Learning},
  author = {Sa, Christopher De and Re, Chris and Olukotun, Kunle},
  year = {2016},
  month = jun,
  series = {Proceedings of Machine Learning Research},
  volume = {48},
  pages = {1567--1576},
  publisher = {{PMLR}},
  address = {{New York, New York, USA}},
  abstract = {Gibbs sampling is a Markov chain Monte Carlo technique commonly used for estimating marginal distributions. To speed up Gibbs sampling, there has recently been interest in parallelizing it by executing asynchronously. While empirical results suggest that many models can be efficiently sampled asynchronously, traditional Markov chain analysis does not apply to the asynchronous case, and thus asynchronous Gibbs sampling is poorly understood. In this paper, we derive a better understanding of the two main challenges of asynchronous Gibbs: bias and mixing time. We show experimentally that our theoretical results match practical outcomes.},
  pdf = {http://proceedings.mlr.press/v48/sa16.pdf}
}

@inproceedings{pmlr-v51-basse16a,
  title = {Parallel Markov Chain Monte Carlo via Spectral Clustering},
  booktitle = {Proceedings of the 19th International Conference on Artificial Intelligence and Statistics},
  author = {Basse, Guillaume and Smith, Aaron and Pillai, Natesh},
  year = {2016},
  month = may,
  series = {Proceedings of Machine Learning Research},
  volume = {51},
  pages = {1318--1327},
  publisher = {{PMLR}},
  address = {{Cadiz, Spain}},
  abstract = {As it has become common to use many computer cores in routine applications, finding good ways to parallelize popular algorithms has become increasingly important. In this paper, we present a parallelization scheme for Markov chain Monte Carlo (MCMC) methods based on spectral clustering of the underlying state space, generalizing earlier work on parallelization of MCMC methods by state space partitioning. We show empirically that this approach speeds up MCMC sampling for multimodal distributions and that it can be usefully applied in greater generality than several related algorithms. Our algorithm converges under reasonable conditions to an `optimal' MCMC algorithm. We also show that our approach can be asymptotically far more efficient than naive parallelization, even in situations such as completely flat target distributions where no unique optimal algorithm exists. Finally, we combine theoretical and empirical bounds to provide practical guidance on the choice of tuning parameters.},
  pdf = {http://proceedings.mlr.press/v51/basse16a.pdf}
}

@inproceedings{pmlr-v51-murray16,
  title = {Pseudo-Marginal Slice Sampling},
  booktitle = {Proc. {{Int}}. {{Conf}}. {{Artif}}. {{Intell}}. {{Statist}}.},
  author = {Murray, Iain and Graham, Matthew},
  year = {2016},
  month = may,
  series = {Proc. of {{Mach}}. {{Learn}}. {{Res}}.},
  volume = {51},
  pages = {911--919},
  publisher = {{ML Research Press}},
  address = {{Cadiz, Spain}},
  abstract = {Markov chain Monte Carlo (MCMC) methods asymptotically sample from complex probability distributions. The pseudo-marginal MCMC framework only requires an unbiased estimator of the unnormalized probability distribution function to construct a Markov chain. However, the resulting chains are harder to tune to a target distribution than conventional MCMC, and the types of updates available are limited. We describe a general way to clamp and update the random numbers used in a pseudo-marginal method's unbiased estimator. In this framework we can use slice sampling and other adaptive methods. We obtain more robust Markov chains, which often mix more quickly.},
  pdf = {http://proceedings.mlr.press/v51/murray16.pdf}
}

@inproceedings{pmlr-v54-gardner17a,
  title = {Discovering and Exploiting Additive Structure for Bayesian Optimization},
  author = {Gardner, Jacob and Guo, Chuan and Weinberger, Kilian and Garnett, Roman and Grosse, Roger},
  year = {2017},
  month = apr,
  series = {Proceedings of Machine Learning Research},
  volume = {54},
  pages = {1311--1319},
  publisher = {{ML Research Press}},
  address = {{Fort Lauderdale, FL, USA}},
  abstract = {Bayesian optimization has proven invaluable for black-box optimization of expensive functions. Its main limitation is its exponential complexity with respect to the dimensionality of the search space using typical kernels. Luckily, many objective functions can be decomposed into additive subproblems, which can be optimized independently. We investigate how to automatically discover such (typically unknown) additive structure while simultaneously exploiting it through Bayesian optimization. We propose an efficient algorithm based on Metropolis-Hastings sampling and demonstrate its efficacy empirically on synthetic and real-world data sets. Throughout all our experiments we reliably discover hidden additive structure whenever it exists and exploit it to yield significantly faster convergence.},
  pdf = {http://proceedings.mlr.press/v54/gardner17a/gardner17a.pdf}
}

@inproceedings{pmlr-v70-gonzalez17a,
  title = {Preferential {{Bayesian}} Optimization},
  booktitle = {Proc. {{Int}}. {{Conf}}. {{Mach}}. {{Learn}}.},
  author = {Gonz{\'a}lez, Javier and Dai, Zhenwen and Damianou, Andreas and Lawrence, Neil D.},
  year = {2017},
  month = aug,
  series = {Proc. {{Mach}}. {{Learn}}. {{Res}}.},
  volume = {70},
  pages = {1282--1291},
  publisher = {{ML Research Press}},
  abstract = {Bayesian optimization (BO) has emerged during the last few years as an effective approach to optimize black-box functions where direct queries of the objective are expensive. We consider the case where direct access to the function is not possible, but information about user preferences is. Such scenarios arise in problems where human preferences are modeled, such as A/B tests or recommender systems. We present a new framework for this scenario that we call Preferential Bayesian Optimization (PBO) and that allows to find the optimum of a latent function that can only be queried through pairwise comparisons, so-called duels. PBO extend the applicability of standard BO ideas and generalizes previous discrete dueling approaches by modeling the probability of the the winner of each duel by means of Gaussian process model with a Bernoulli likelihood. The latent preference function is used to define a family of acquisition functions that extend usual policies used in BO. We illustrate the benefits of PBO in a variety of experiments in which we show how the way correlations are modeled is the key ingredient to drastically reduce the number of comparisons to find the optimum of the latent function of interest.},
  pdf = {http://proceedings.mlr.press/v70/gonzalez17a/gonzalez17a.pdf}
}

@inproceedings{pmlr-v70-hoffman17a,
  title = {Learning Deep Latent {{Gaussian}} Models with {{Markov}} Chain {{Monte Carlo}}},
  booktitle = {Proceedings of the {{International Conference}} on {{Machine Learning}}},
  author = {Hoffman, Matthew D.},
  year = {2017},
  month = aug,
  series = {{{PMLR}}},
  volume = {70},
  pages = {1510--1519},
  publisher = {{ML Research Press}},
  abstract = {Deep latent Gaussian models are powerful and popular probabilistic models of high-dimensional data. These models are almost always fit using variational expectation-maximization, an approximation to true maximum-marginal-likelihood estimation. In this paper, we propose a different approach: rather than use a variational approximation (which produces biased gradient signals), we use Markov chain Monte Carlo (MCMC, which allows us to trade bias for computation). We find that our MCMC-based approach has several advantages: it yields higher held-out likelihoods, produces sharper images, and does not suffer from the variational overpruning effect. MCMC's additional computational overhead proves to be significant, but not prohibitive.},
  pdf = {http://proceedings.mlr.press/v70/hoffman17a/hoffman17a.pdf}
}

@inproceedings{pmlr-v89-li19c,
  title = {On the Convergence of Stochastic Gradient Descent with Adaptive Stepsizes},
  booktitle = {Proceedings of the Twenty-Second International Conference on Artificial Intelligence and Statistics},
  author = {Li, Xiaoyu and Orabona, Francesco},
  year = {2019},
  month = apr,
  series = {Proceedings of Machine Learning Research},
  volume = {89},
  pages = {983--992},
  publisher = {{PMLR}},
  abstract = {Stochastic gradient descent is the method of choice for large scale optimization of machine learning objective functions. Yet, its performance is greatly variable and heavily depends on the choice of the stepsizes. This has motivated a large body of research on adaptive stepsizes. However, there is currently a gap in our theoretical understanding of these methods, especially in the non-convex setting. In this paper, we start closing this gap: we theoretically analyze in the convex and non-convex settings a generalized version of the AdaGrad stepsizes. We show sufficient conditions for these stepsizes to achieve almost sure asymptotic convergence of the gradients to zero, proving the first guarantee for generalized AdaGrad stepsizes in the non-convex setting. Moreover, we show that these stepsizes allow to automatically adapt to the level of noise of the stochastic gradients in both the convex and non-convex settings, interpolating between O(1/T) and O(1/sqrt(T)), up to logarithmic terms.},
  pdf = {http://proceedings.mlr.press/v89/li19c/li19c.pdf}
}

@inproceedings{pmlr-v89-paananen19a,
  title = {Variable Selection for {{Gaussian}} Processes via Sensitivity Analysis of the Posterior Predictive Distribution},
  booktitle = {Proceedings of the {{Twenty-Second International Conference}} on {{Artificial Intelligence}} and {{Statistics}} ({{AISTATS}})},
  author = {Paananen, Topi and Piironen, Juho and Andersen, Michael Riis and Vehtari, Aki},
  year = {2019},
  month = apr,
  series = {Proceedings of {{Machine Learning Research}}},
  volume = {89},
  pages = {1743--1752},
  publisher = {{ML Research Press}},
  abstract = {Variable selection for Gaussian process models is often done using automatic relevance determination, which uses the inverse length-scale parameter of each input variable as a proxy for variable relevance. This implicitly determined relevance has several drawbacks that prevent the selection of optimal input variables in terms of predictive performance. To improve on this, we propose two novel variable selection methods for Gaussian process models that utilize the predictions of a full model in the vicinity of the training points and thereby rank the variables based on their predictive relevance. Our empirical results on synthetic and real world data sets demonstrate improved variable selection compared to automatic relevance determination in terms of variability and predictive performance.},
  pdf = {http://proceedings.mlr.press/v89/paananen19a/paananen19a.pdf}
}

@inproceedings{pmlr-v97-cornish19a,
  title = {Scalable {{Metropolis-Hastings}} for Exact {{Bayesian}} Inference with Large Datasets},
  booktitle = {Proceedings of the 36th International Conference on Machine Learning},
  author = {Cornish, Rob and Vanetti, Paul and {Bouchard-Cote}, Alexandre and Deligiannidis, George and Doucet, Arnaud},
  year = {2019},
  month = jun,
  series = {Proceedings of Machine Learning Research},
  volume = {97},
  pages = {1351--1360},
  publisher = {{PMLR}},
  address = {{Long Beach, California, USA}},
  abstract = {Bayesian inference via standard Markov Chain Monte Carlo (MCMC) methods such as Metropolis-Hastings is too computationally intensive to handle large datasets, since the cost per step usually scales like O(n) in the number of data points n. We propose the Scalable Metropolis-Hastings (SMH) kernel that only requires processing on average O(1) or even O(1/n) data points per step. This scheme is based on a combination of factorized acceptance probabilities, procedures for fast simulation of Bernoulli processes, and control variate ideas. Contrary to many MCMC subsampling schemes such as fixed step-size Stochastic Gradient Langevin Dynamics, our approach is exact insofar as the invariant distribution is the true posterior and not an approximation to it. We characterise the performance of our algorithm theoretically, and give realistic and verifiable conditions under which it is geometrically ergodic. This theory is borne out by empirical results that demonstrate overall performance benefits over standard Metropolis-Hastings and various subsampling algorithms.},
  pdf = {http://proceedings.mlr.press/v97/cornish19a/cornish19a.pdf}
}

@inproceedings{pmlr-v97-ruiz19a,
  title = {A Contrastive Divergence for Combining Variational Inference and {{MCMC}}},
  booktitle = {Proceedings of the {{International Conference}} on {{Machine Learning}}},
  author = {Ruiz, Francisco and Titsias, Michalis},
  year = {2019},
  month = jun,
  series = {{{PMLR}}},
  volume = {97},
  pages = {5537--5545},
  publisher = {{ML Research Press}},
  abstract = {We develop a method to combine Markov chain Monte Carlo (MCMC) and variational inference (VI), leveraging the advantages of both inference approaches. Specifically, we improve the variational distribution by running a few MCMC steps. To make inference tractable, we introduce the variational contrastive divergence (VCD), a new divergence that replaces the standard Kullback-Leibler (KL) divergence used in VI. The VCD captures a notion of discrepancy between the initial variational distribution and its improved version (obtained after running the MCMC steps), and it converges asymptotically to the symmetrized KL divergence between the variational distribution and the posterior of interest. The VCD objective can be optimized efficiently with respect to the variational parameters via stochastic optimization. We show experimentally that optimizing the VCD leads to better predictive performance on two latent variable models: logistic matrix factorization and variational autoencoders (VAEs).},
  pdf = {http://proceedings.mlr.press/v97/ruiz19a/ruiz19a.pdf}
}

@inproceedings{pmlr-v97-ward19a,
  title = {{{AdaGrad}} Stepsizes: {{Sharp}} Convergence over Nonconvex Landscapes},
  booktitle = {Proceedings of the 36th International Conference on Machine Learning},
  author = {Ward, Rachel and Wu, Xiaoxia and Bottou, Leon},
  year = {2019},
  month = jun,
  series = {Proceedings of Machine Learning Research},
  volume = {97},
  pages = {6677--6686},
  publisher = {{PMLR}},
  abstract = {Adaptive gradient methods such as AdaGrad and its variants update the stepsize in stochastic gradient descent on the fly according to the gradients received along the way; such methods have gained widespread use in large-scale optimization for their ability to converge robustly, without the need to fine-tune parameters such as the stepsize schedule. Yet, the theoretical guarantees to date for AdaGrad are for online and convex optimization. We bridge this gap by providing strong theoretical guarantees for the convergence of AdaGrad over smooth, nonconvex landscapes. We show that the norm version of AdaGrad (AdaGrad-Norm) converges to a stationary point at the O((N)/N) rate in the stochastic setting, and at the optimal O(1/N) rate in the batch (non-stochastic) setting \textendash{} in this sense, our convergence guarantees are ``sharp''. In particular, both our theoretical results and extensive numerical experiments imply that AdaGrad-Norm is robust to the \textexclamdown em\textquestiondown unknown Lipschitz constant and level of stochastic noise on the gradient\textexclamdown/em\textquestiondown.},
  pdf = {http://proceedings.mlr.press/v97/ward19a/ward19a.pdf}
}

@inproceedings{pmlr-v99-karimi19a,
  title = {Non-Asymptotic Analysis of Biased Stochastic Approximation Scheme},
  booktitle = {Proceedings of the Annual Conference on Learning Theory},
  author = {Karimi, Belhal and Miasojedow, Blazej and Moulines, Eric and Wai, Hoi-To},
  year = {2019},
  month = jun,
  series = {{{PMLR}}},
  volume = {99},
  pages = {1944--1974},
  publisher = {{ML Research Press}},
  abstract = {Stochastic approximation (SA) is a key method used in statistical learning. Recently, its non-asymptotic convergence analysis has been considered in many papers. However, most of the prior analyses are made under restrictive assumptions such as unbiased gradient estimates and convex objective function, which significantly limit their applications to sophisticated tasks such as online and reinforcement learning. These restrictions are all essentially relaxed in this work. In particular, we analyze a general SA scheme to minimize a non-convex, smooth objective function. We consider update procedure whose drift term depends on a state-dependent Markov chain and the mean field is not necessarily of gradient type, covering approximate second-order method and allowing asymptotic bias for the one-step updates. We illustrate these settings with the online EM algorithm and the policy-gradient method for average reward maximization in reinforcement learning.},
  pdf = {http://proceedings.mlr.press/v99/karimi19a/karimi19a.pdf}
}

@article{polson_bayesian_2000,
  title = {Bayesian {{Portfolio Selection}}: {{An Empirical Analysis}} of the {{S}}\&{{P}} 500 {{Index}} 1970\textendash 1996},
  shorttitle = {Bayesian {{Portfolio Selection}}},
  author = {Polson, Nicholas G. and Tew, Bernard V.},
  year = {2000},
  month = apr,
  journal = {Journal of Business \& Economic Statistics},
  volume = {18},
  number = {2},
  pages = {164--173},
  langid = {english}
}

@article{polyak_acceleration_1992,
  title = {Acceleration of {{Stochastic Approximation}} by {{Averaging}}},
  author = {Polyak, B. T. and Juditsky, A. B.},
  year = {1992},
  month = jul,
  journal = {SIAM Journal on Control and Optimization},
  volume = {30},
  number = {4},
  pages = {838--855},
  langid = {english}
}

@article{polyak_methods_1964,
  title = {Some Methods of Speeding up the Convergence of Iteration Methods},
  author = {Polyak, B.T.},
  year = {1964},
  month = jan,
  journal = {USSR Computational Mathematics and Mathematical Physics},
  volume = {4},
  number = {5},
  pages = {1--17},
  langid = {english}
}

@article{polyak_pseudogradient_1973,
  title = {Pseudogradient Adaptation and Training Algorithms},
  author = {Polyak, Boris T. and Tsypkin, Ya Z.},
  year = {1973},
  journal = {Automatic Remote Control},
  volume = {34},
  number = {3},
  pages = {45--68}
}

@article{polychronopoulos_guided_1987,
  title = {Guided Self-Scheduling: A Practical Scheduling Scheme for Parallel Supercomputers},
  author = {Polychronopoulos, Constantine D. and Kuck, David J.},
  year = {1987},
  month = dec,
  journal = {IEEE Transactions on Computers},
  volume = {C-36},
  number = {12},
  pages = {1425--1439},
  file = {/home/msca8h/Documents/parallel_scheduling/Polychronopoulos and Kuck - 1987 - Guided Self-Scheduling A Practical Scheduling Sch.pdf}
}

@inproceedings{popov_efficient_2019,
  title = {Efficient Thread/Page/Parallelism Autotuning for {{NUMA}} Systems},
  booktitle = {Proceedings of the {{ACM International Conference}} on {{Supercomputing}}  - {{ICS}} '19},
  author = {Popov, Mihail and Jimborean, Alexandra and {Black-Schaffer}, David},
  year = {2019},
  pages = {342--353},
  publisher = {{ACM Press}},
  address = {{Phoenix, Arizona}},
  langid = {english},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\B3KEZ5XT\\Popov et al. - 2019 - Efficient threadpageparallelism autotuning for N.pdf}
}

@inproceedings{popov_efficient_2019a,
  title = {Efficient Thread/Page/Parallelism Autotuning for {{NUMA}} Systems},
  booktitle = {Proceedings of the {{ACM International Conference}} on {{Supercomputing}}  - {{ICS}} '19},
  author = {Popov, Mihail and Jimborean, Alexandra and {Black-Schaffer}, David},
  year = {2019},
  pages = {342--353},
  publisher = {{ACM Press}},
  address = {{Phoenix, Arizona}},
  langid = {english},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\S6V5F46P\\Popov et al. - 2019 - Efficient threadpageparallelism autotuning for N.pdf}
}

@article{popov_piecewise_2017,
  title = {Piecewise Holistic Autotuning of Parallel Programs with {{CERE}}: {{Piecewise Holistic Autotuning}} of {{Parallel Programs}} with {{CERE}}},
  shorttitle = {Piecewise Holistic Autotuning of Parallel Programs with {{CERE}}},
  author = {Popov, Mihail and Akel, Chadi and Chatelain, Yohan and Jalby, William and {de~Oliveira~Castro}, Pablo},
  year = {2017},
  month = aug,
  journal = {Concurrency and Computation: Practice and Experience},
  volume = {29},
  number = {15},
  pages = {e4190},
  langid = {english},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\L8ZWQ7AB\\Popov et al. - 2017 - Piecewise holistic autotuning of parallel programs.pdf}
}

@book{porat_course_1997,
  title = {A Course in Digital Signal Processing},
  author = {Porat, Boaz},
  year = {1997},
  publisher = {{John Wiley}},
  address = {{New York}},
  lccn = {TK5102.9 .P66 1997},
  keywords = {Digital techniques,Signal processing}
}

@article{porter_gaussian_1987,
  title = {Gaussian Beam Tracing for Computing Ocean Acoustic Fields},
  author = {Porter, Michael B. and Bucker, Homer P.},
  year = {1987},
  month = oct,
  journal = {The Journal of the Acoustical Society of America},
  volume = {82},
  number = {4},
  pages = {1349--1359},
  langid = {english}
}

@article{pridham_digital_1979,
  title = {Digital Interpolation Beamforming for Low-Pass and Bandpass Signals},
  author = {Pridham, R.G. and Mucci, R.A.},
  year = {1979},
  journal = {Proceedings of the IEEE},
  volume = {67},
  number = {6},
  pages = {904--919}
}

@article{PS_2001__5__183_0,
  title = {Chernoff and {{Berry-Ess\'een}} Inequalities for {{Markov}} Processes},
  author = {Lezaud, Pascal},
  year = {2001},
  journal = {ESAIM: Probability and Statistics},
  volume = {5},
  pages = {183--201},
  publisher = {{EDP-Sciences}},
  langid = {english},
  zmnumber = {0998.60075}
}

@article{qawasmeh_adaptive_2015,
  title = {Adaptive {{OpenMP Task Scheduling Using Runtime APIs}} and {{Machine Learning}}},
  author = {Qawasmeh, A. and Malik, A. M. and Chapman, B. M.},
  year = {2015},
  journal = {2015 IEEE 14th International Conference on Machine Learning and Applications (ICMLA)},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\4YJTTYPL\\ML_scheduling_runtime_API_OMP.pdf}
}

@article{qiao_rethinking_2019,
  title = {Rethinking {{Normalization}} and {{Elimination Singularity}} in {{Neural Networks}}},
  author = {Qiao, Siyuan and Wang, Huiyu and Liu, Chenxi and Shen, Wei and Yuille, Alan},
  year = {2019},
  month = nov,
  journal = {arXiv:1911.09738 [cs]},
  eprint = {1911.09738},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {In this paper, we study normalization methods for neural networks from the perspective of elimination singularity. Elimination singularities correspond to the points on the training trajectory where neurons become consistently deactivated. They cause degenerate manifolds in the loss landscape which will slow down training and harm model performances. We show that channel-based normalizations (e.g. Layer Normalization and Group Normalization) are unable to guarantee a far distance from elimination singularities, in contrast with Batch Normalization which by design avoids models from getting too close to them. To address this issue, we propose BatchChannel Normalization (BCN), which uses batch knowledge to avoid the elimination singularities in the training of channel-normalized models. Unlike Batch Normalization, BCN is able to run in both large-batch and micro-batch training settings. The effectiveness of BCN is verified on many tasks, including image classification, object detection, instance segmentation, and semantic segmentation. The code is here: https://github.com/joe-siyuan-qiao/Batch-Channel-Normalization.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\37AYN8YU\\Qiao et al. - 2019 - Rethinking Normalization and Elimination Singulari.pdf;C\:\\Users\\msca8h\\Zotero\\storage\\GXRT5VMS\\1911.html}
}

@article{qiao_weight_2019,
  title = {Weight {{Standardization}}},
  author = {Qiao, Siyuan and Wang, Huiyu and Liu, Chenxi and Shen, Wei and Yuille, Alan},
  year = {2019},
  month = mar,
  journal = {arXiv:1903.10520 [cs]},
  eprint = {1903.10520},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {In this paper, we propose Weight Standardization (WS) to accelerate deep network training. WS is targeted at the micro-batch training setting where each GPU typically has only 1-2 images for training. The micro-batch training setting is hard because small batch sizes are not enough for training networks with Batch Normalization (BN), while other normalization methods that do not rely on batch knowledge still have difficulty matching the performances of BN in large-batch training. Our WS ends this problem because when used with Group Normalization and trained with 1 image/GPU, WS is able to match or outperform the performances of BN trained with large batch sizes with only 2 more lines of code. In micro-batch training, WS significantly outperforms other normalization methods. WS achieves these superior results by standardizing the weights in the convolutional layers, which we show is able to smooth the loss landscape by reducing the Lipschitz constants of the loss and the gradients. The effectiveness of WS is verified on many tasks, including image classification, object detection, instance segmentation, video recognition, semantic segmentation, and point cloud recognition. The code is available here: https://github.com/joe-siyuan-qiao/WeightStandardization.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\PRUPGAMQ\\Qiao et al. - 2019 - Weight Standardization.pdf;C\:\\Users\\msca8h\\Zotero\\storage\\DDJ5L7H2\\1903.html}
}

@inproceedings{quinlan_determination_2006,
  title = {Determination of the Number of Wideband Acoustical Sources in a Reverberant Environment},
  booktitle = {European {{Signal Process}}. {{Conf}}.},
  author = {Quinlan, Angela and Boland, Frank and Barbot, Jean-Pierre and Larzabal, Pascal},
  year = {2006},
  month = sep,
  pages = {1--5},
  abstract = {This paper addresses the problem of determining the number of wideband sources in a reverberant environment. In [1] an Exponential Fitting Test (EFT) is proposed based on the exponential profile of the noise only eigenvalues. We consider the performance of this test for the problem in question, and compare it with the results achieved by the well known Akaike Information Criterion (AIC) and Minimum Description Length (MDL). Once reverberation is present in the received signals the EFT is seen to perform much better than the AIC and MDL.},
  keywords = {Abstracts,Arrays,Manganese,Microphones},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\9QWBYN46\\Quinlan et al. - 2006 - Determination of the number of wideband acoustical.pdf;C\:\\Users\\msca8h\\Zotero\\storage\\DP4ZILY7\\7071439.html}
}

@incollection{quinonero-candela_evaluating_2006,
  title = {Evaluating {{Predictive Uncertainty Challenge}}},
  booktitle = {Machine {{Learning Challenges}}. {{Evaluating Predictive Uncertainty}}, {{Visual Object Classification}}, and {{Recognising Tectual Entailment}}},
  author = {{Qui{\~n}onero-Candela}, Joaquin and Rasmussen, Carl Edward and Sinz, Fabian and Bousquet, Olivier and Sch{\"o}lkopf, Bernhard},
  year = {2006},
  volume = {3944},
  pages = {1--27},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\7PH3YBKZ\\Quionero-Candela et al. - 2006 - Evaluating Predictive Uncertainty Challenge.pdf}
}

@article{quiroz_blockpoisson_2021,
  title = {The Block-{{Poisson}} Estimator for Optimally Tuned Exact Subsampling {{MCMC}}},
  author = {Quiroz, Matias and Tran, Minh-Ngoc and Villani, Mattias and Kohn, Robert and Dang, Khue-Dung},
  year = {2021},
  month = oct,
  journal = {Journal of Computational and Graphical Statistics},
  volume = {30},
  number = {4},
  pages = {877--888},
  langid = {english},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\JT47BR27\\Quiroz et al. - 2021 - The Block-Poisson Estimator for Optimally Tuned Ex.pdf}
}

@article{quiroz_blockpoisson_2021a,
  title = {The Block-{{Poisson}} Estimator for Optimally Tuned Exact Subsampling {{MCMC}}},
  author = {Quiroz, Matias and Tran, Minh-Ngoc and Villani, Mattias and Kohn, Robert and Dang, Khue-Dung},
  year = {2021},
  month = oct,
  journal = {Journal of Computational and Graphical Statistics},
  volume = {30},
  number = {4},
  pages = {877--888},
  abstract = {Speeding up Markov chain Monte Carlo (MCMC) for datasets with many observations by data subsampling has recently received considerable attention. A pseudo-marginal MCMC method is proposed that estimates the likelihood by data subsampling using a block-Poisson estimator. The estimator is a product of Poisson estimators, allowing us to update a single block of subsample indicators in each MCMC iteration so that a desired correlation is achieved between the logs of successive likelihood estimates. This is important since pseudo-marginal MCMC with positively correlated likelihood estimates can use substantially smaller subsamples without adversely affecting the sampling efficiency. The block-Poisson estimator is unbiased but not necessarily positive, so the algorithm runs the MCMC on the absolute value of the likelihood estimator and uses an importance sampling correction to obtain consistent estimates of the posterior mean of any function of the parameters. Our article derives guidelines to select the optimal tuning parameters for our method and shows that it compares very favorably to regular MCMC without subsampling, and to two other recently proposed exact subsampling approaches in the literature. Supplementary materials for this article are available online.},
  langid = {english},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\IWELPB28\\Quiroz et al. - 2021 - The Block-Poisson Estimator for Optimally Tuned Ex.pdf}
}

@article{quiroz_speeding_2019,
  title = {Speeding up {{MCMC}} by Efficient Data Subsampling},
  author = {Quiroz, Matias and Kohn, Robert and Villani, Mattias and Tran, Minh-Ngoc},
  year = {2019},
  month = apr,
  journal = {Journal of the American Statistical Association},
  volume = {114},
  number = {526},
  pages = {831--843},
  langid = {english},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\MMKYHGEP\\Quiroz et al. - 2019 - Speeding Up MCMC by Efficient Data Subsampling.pdf}
}

@article{quiroz_speeding_2019a,
  title = {Speeding up {{MCMC}} by Efficient Data Subsampling},
  author = {Quiroz, Matias and Kohn, Robert and Villani, Mattias and Tran, Minh-Ngoc},
  year = {2019},
  month = apr,
  journal = {Journal of the American Statistical Association},
  volume = {114},
  number = {526},
  pages = {831--843},
  publisher = {{Taylor \& Francis}},
  abstract = {We propose subsampling Markov chain Monte Carlo (MCMC), an MCMC framework where the likelihood function for n observations is estimated from a random subset of m observations. We introduce a highly efficient unbiased estimator of the log-likelihood based on control variates, such that the computing cost is much smaller than that of the full log-likelihood in standard MCMC. The likelihood estimate is bias-corrected and used in two dependent pseudo-marginal algorithms to sample from a perturbed posterior, for which we derive the asymptotic error with respect to n and m, respectively. We propose a practical estimator of the error and show that the error is negligible even for a very small m in our applications. We demonstrate that subsampling MCMC is substantially more efficient than standard MCMC in terms of sampling efficiency for a given computational budget, and that it outperforms other subsampling methods for MCMC proposed in the literature. Supplementary materials for this article are available online.},
  keywords = {Bayesian inference,Big Data,Block pseudo-marginal,Correlated pseudo-marginal,Estimated likelihood,Survey sampling},
  annotation = {\_eprint: https://doi.org/10.1080/01621459.2018.1448827},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\6ZC3F97S\\Quiroz et al. - 2019 - Speeding Up MCMC by Efficient Data Subsampling.pdf}
}

@article{radivojevic_modified_2020,
  title = {Modified {{Hamiltonian Monte Carlo}} for {{Bayesian}} Inference},
  author = {Radivojevi{\'c}, Tijana and Akhmatskaya, Elena},
  year = {2020},
  month = mar,
  journal = {Statistics and Computing},
  volume = {30},
  number = {2},
  pages = {377--404},
  langid = {english}
}

@inproceedings{radojkovic_optimal_2012,
  title = {Optimal Task Assignment in Multithreaded Processors: A Statistical Approach},
  shorttitle = {Optimal Task Assignment in Multithreaded Processors},
  booktitle = {Proceedings of the Seventeenth International Conference on {{Architectural Support}} for {{Programming Languages}} and {{Operating Systems}} - {{ASPLOS}} '12},
  author = {Radojkovi{\'c}, Petar and {\v C}akarevi{\'c}, Vladimir and Moret{\'o}, Miquel and Verd{\'u}, Javier and Pajuelo, Alex and Cazorla, Francisco J. and Nemirovsky, Mario and Valero, Mateo},
  year = {2012},
  pages = {235},
  publisher = {{ACM Press}},
  address = {{London, England, UK}},
  langid = {english}
}

@article{radojkovic_thread_2016,
  title = {Thread {{Assignment}} in {{Multicore}}/{{Multithreaded Processors}}: {{A Statistical Approach}}},
  shorttitle = {Thread {{Assignment}} in {{Multicore}}/{{Multithreaded Processors}}},
  author = {Radojkovic, Petar and Carpenter, Paul M. and Moreto, Miquel and Cakarevic, Vladimir and Verdu, Javier and Pajuelo, Alex and Cazorla, Francisco J. and Nemirovsky, Mario and Valero, Mateo},
  year = {2016},
  month = jan,
  journal = {IEEE Transactions on Computers},
  volume = {65},
  number = {1},
  pages = {256--269},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\PX57CA6I\\Radojkovic et al. - 2016 - Thread Assignment in MulticoreMultithreaded Proce.pdf}
}

@inproceedings{raghunathan_learning_2017,
  title = {Learning Mixture of {{Gaussians}} with Streaming Data},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Raghunathan, Aditi and Jain, Prateek and Krishnawamy, Ravishankar},
  year = {2017},
  volume = {30},
  publisher = {{Curran Associates, Inc.}}
}

@incollection{rahimi_random_2008,
  title = {Random {{Features}} for {{Large-Scale Kernel Machines}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 20},
  author = {Rahimi, Ali and Recht, Benjamin},
  year = {2008},
  pages = {1177--1184},
  publisher = {{Curran Associates, Inc.}}
}

@inproceedings{rainforth_tighter_2018,
  title = {Tighter Variational Bounds Are Not Necessarily Better},
  booktitle = {Proceedings of the {{International Conference}} on {{Machine Learning}}},
  author = {Rainforth, Tom and Kosiorek, Adam and Le, Tuan Anh and Maddison, Chris and Igl, Maximilian and Wood, Frank and Teh, Yee Whye},
  year = {2018},
  month = jul,
  series = {{{PMLR}}},
  pages = {4277--4285},
  publisher = {{ML Research Press}},
  abstract = {We provide theoretical and empirical evidence that using tighter evidence lower bounds (ELBOs) can be detrimental to the process of learning an inference network by reducing the signal-to-noise ratio of the gradient estimator. Our results call into question common implicit assumptions that tighter ELBOs are better variational objectives for simultaneous model learning and inference amortization schemes. Based on our insights, we introduce three new algorithms: the partially importance weighted auto-encoder (PIWAE), the multiply importance weighted auto-encoder (MIWAE), and the combination importance weighted autoencoder (CIWAE), each of which includes the standard importance weighted auto-encoder (IWAE) as a special case. We show that each can deliver improvements over IWAE, even when performance is measured by the IWAE target itself. Furthermore, our results suggest that PIWAE may be able to deliver simultaneous improvements in the training of both the inference and generative networks.},
  langid = {english},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\8C7SKSB8\\Rainforth et al. - 2018 - Tighter Variational Bounds are Not Necessarily Bet.pdf;C\:\\Users\\msca8h\\Zotero\\storage\\U5G56G93\\Rainforth et al. - 2018 - Tighter Variational Bounds are Not Necessarily Bet.pdf}
}

@misc{rakhlin_making_2012,
  title = {Making Gradient Descent Optimal for Strongly Convex Stochastic Optimization},
  author = {Rakhlin, Alexander and Shamir, Ohad and Sridharan, Karthik},
  year = {2012},
  month = dec,
  number = {arXiv:1109.5647},
  eprint = {1109.5647},
  eprinttype = {arxiv},
  primaryclass = {cs, math},
  institution = {{arXiv}},
  abstract = {Stochastic gradient descent (SGD) is a simple and popular method to solve stochastic optimization problems which arise in machine learning. For strongly convex problems, its convergence rate was known to be O(\textbackslash log(T)/T), by running SGD for T iterations and returning the average point. However, recent results showed that using a different algorithm, one can get an optimal O(1/T) rate. This might lead one to believe that standard SGD is suboptimal, and maybe should even be replaced as a method of choice. In this paper, we investigate the optimality of SGD in a stochastic setting. We show that for smooth problems, the algorithm attains the optimal O(1/T) rate. However, for non-smooth problems, the convergence rate with averaging might really be \textbackslash Omega(\textbackslash log(T)/T), and this is not just an artifact of the analysis. On the flip side, we show that a simple modification of the averaging step suffices to recover the O(1/T) rate, and no other change of the algorithm is necessary. We also present experimental results which support our findings, and point out open problems.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Mathematics - Optimization and Control},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\MDT76ZEU\\Rakhlin et al. - 2012 - Making Gradient Descent Optimal for Strongly Conve.pdf;C\:\\Users\\msca8h\\Zotero\\storage\\9MYBNXGD\\1109.html}
}

@article{ram_incremental_2009,
  title = {Incremental {{Stochastic Subgradient Algorithms}} for {{Convex Optimization}}},
  author = {Ram, S. Sundhar and Nedi{\'c}, A. and Veeravalli, V. V.},
  year = {2009},
  month = jan,
  journal = {SIAM Journal on Optimization},
  volume = {20},
  number = {2},
  pages = {691--717},
  langid = {english},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\7LDAAXZK\\Ram et al. - 2009 - Incremental Stochastic Subgradient Algorithms for .pdf}
}

@inproceedings{ramalingam_improving_2012,
  title = {Improving {{High-Performance Sparse Libraries Using Compiler-Assisted Specialization}}: {{A PETSc Case Study}}},
  shorttitle = {Improving {{High-Performance Sparse Libraries Using Compiler-Assisted Specialization}}},
  booktitle = {2012 {{IEEE}} 26th {{International Parallel}} and {{Distributed Processing Symposium Workshops}} \& {{PhD Forum}}},
  author = {Ramalingam, Shreyas and Hall, Mary and Chen, Chun},
  year = {2012},
  month = may,
  pages = {487--496},
  publisher = {{IEEE}},
  address = {{Shanghai, China}}
}

@article{ramos-llorden_anisotropic_2015,
  title = {Anisotropic Diffusion Filter with Memory Based on Speckle Statistics for Ultrasound Images},
  author = {{Ramos-Llorden}, Gabriel and {Vegas-Sanchez-Ferrero}, Gonzalo and {Martin-Fernandez}, Marcos and {Alberola-Lopez}, Carlos and {Aja-Fernandez}, Santiago},
  year = {2015},
  month = jan,
  journal = {IEEE Transactions on Image Processing},
  volume = {24},
  number = {1},
  pages = {345--358},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\D4C6GGYB\\Ramos-Llorden et al. - 2015 - Anisotropic Diffusion Filter With Memory Based on .pdf}
}

@inproceedings{ranganath_black_2014,
  title = {Black Box Variational Inference},
  booktitle = {Proceedings of the International Conference on Artificial Intelligence and Statistics},
  author = {Ranganath, Rajesh and Gerrish, Sean and Blei, David},
  year = {2014},
  month = apr,
  series = {{{PMLR}}},
  volume = {33},
  pages = {814--822},
  publisher = {{ML Research Press}},
  abstract = {Variational inference has become a widely used method to approximate posteriors in complex latent variables models. However, deriving a variational inference algorithm generally requires significant model-specific analysis. These efforts can hinder and deter us from quickly developing and exploring a variety of models for a problem at hand. In this paper, we present a ``black box'' variational inference algorithm, one that can be quickly applied to many models with little additional derivation. Our method is based on a stochastic optimization of the variational objective where the noisy gradient is computed from Monte Carlo samples from the variational distribution. We develop a number of methods to reduce the variance of the gradient, always maintaining the criterion that we want to avoid difficult model-based derivations. We evaluate our method against the corresponding black box sampling based methods. We find that our method reaches better predictive likelihoods much faster than sampling methods. Finally, we demonstrate that Black Box Variational Inference lets us easily explore a wide space of models by quickly constructing and evaluating several models of longitudinal healthcare data.},
  pdf = {http://proceedings.mlr.press/v33/ranganath14.pdf}
}

@inproceedings{rashid_investigating_2008,
  title = {Investigating a {{Dynamic Loop Scheduling}} with {{Reinforcement Learning Approach}} to {{Load Balancing}} in {{Scientific Applications}}},
  booktitle = {2008 {{International Symposium}} on {{Parallel}} and {{Distributed Computing}}},
  author = {Rashid, Mahbubur and Banicescu, Ioana and Carino, Ricolindo L.},
  year = {2008},
  pages = {123--130},
  publisher = {{IEEE}},
  address = {{Krakow, Poland}}
}

@book{rasmussen_gaussian_2006,
  title = {Gaussian Processes for Machine Learning},
  author = {Rasmussen, Carl Edward and Williams, Christopher K. I.},
  year = {2006},
  series = {Adaptive {{Comput}}. {{Mach}}. {{Learn}}.},
  publisher = {{MIT Press}},
  address = {{Cambridge, Mass}},
  lccn = {QA274.4 .R37 2006},
  keywords = {Data processing,Gaussian processes,Machine learning,Mathematical models},
  annotation = {OCLC: ocm61285753}
}

@incollection{rasmussen_occam_2001,
  title = {Occam's Razor},
  booktitle = {Adv. {{Neural Inf}}. {{Process}}. {{Syst}}. 13},
  author = {Rasmussen, Carl Edward and Ghahramani, Zoubin},
  year = {2001},
  series = {{{NIPS}}'13},
  pages = {294--300},
  publisher = {{MIT Press}}
}

@article{reddy_doa_2012,
  title = {{{DOA}} Estimation of Wideband Sources without Estimating the Number of Sources},
  author = {Reddy, Vinod V. and Ng, B. P. and Zhang, Ying and Khong, Andy W. H.},
  year = {2012},
  month = apr,
  journal = {Signal Processing},
  volume = {92},
  number = {4},
  pages = {1032--1043},
  abstract = {In this paper, we propose a new technique to estimate wideband source directions from the sensor snapshots without requiring to know the number of sources present in the scenario. This work is motivated by the fact that the existing model order estimation (number of sources) techniques for wideband source scenario are either inaccurate or computationally expensive. Direction-of-arrival (DOA) estimation is realized using a beamformer framework which imposes nulls in the spatial spectrum along the source directions. The null width along the frequency axis is widened by introducing a new data dependent term into the optimization problem, thus achieving wideband capability. Furthermore, the temporal processing of the data snapshots drastically reduces the number of snapshots required for wideband DOA estimation. The effectiveness of the proposed formulation is studied with simulated experiments.},
  langid = {english},
  keywords = {Wideband DOA estimation},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\LAIDTQQV\\Reddy et al. - 2012 - DOA estimation of wideband sources without estimat.pdf}
}

@article{reddy_doa_2012a,
  title = {{{DOA}} Estimation of Wideband Sources without Estimating the Number of Sources},
  author = {Reddy, Vinod V. and Ng, B.P. and Zhang, Ying and Khong, Andy W.H.},
  year = {2012},
  month = apr,
  journal = {Signal Processing},
  volume = {92},
  number = {4},
  pages = {1032--1043},
  langid = {english}
}

@inproceedings{reeb_learning_2018,
  title = {Learning {{Gaussian}} Processes by Minimizing {{PAC-Bayesian}} Generalization Bounds},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Reeb, David and Doerr, Andreas and Gerwinn, Sebastian and Rakitsch, Barbara},
  year = {2018},
  volume = {31},
  publisher = {{Curran Associates, Inc.}},
  abstract = {Gaussian Processes (GPs) are a generic modelling tool for supervised learning. While they have been successfully applied on large datasets, their use in safety-critical applications is hindered by the lack of good performance guarantees. To this end, we propose a method to learn GPs and their sparse approximations by directly optimizing a PAC-Bayesian bound on their generalization performance, instead of maximizing the marginal likelihood. Besides its theoretical appeal, we find in our evaluation that our learning method is robust and yields significantly better generalization guarantees than other common GP approaches on several regression benchmark datasets.},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\PYQQBPLQ\\Reeb et al. - 2018 - Learning Gaussian Processes by Minimizing PAC-Baye.pdf}
}

@inproceedings{regier_cataloging_2018,
  title = {Cataloging the Visible Universe through {{Bayesian}} Inference at Petascale},
  booktitle = {Proc. {{Int}}. {{Parallel Distrib}}. {{Process}}. {{Symp}}.},
  author = {Regier, Jeffrey and Pamnany, Kiran and Fischer, Keno and Noack, Andreas and Lam, Maximilian and Revels, Jarrett and Howard, Steve and Giordano, Ryan and Schlegel, David and McAuliffe, Jon and Thomas, Rollin C. and {Prabhat}},
  year = {2018},
  series = {{{IPDPS}}'18},
  pages = {44--53}
}

@inproceedings{regier_cataloging_2018a,
  title = {Cataloging the {{Visible Universe Through Bayesian Inference}} at {{Petascale}}},
  booktitle = {2018 {{IEEE International Parallel}} and {{Distributed Processing Symposium}} ({{IPDPS}})},
  author = {Regier, Jeffrey and McAuliffe, Jon and Thomas, Rollin and Prabhat, . and Pamnany, Kiran and Fischer, Keno and Noack, Andreas and Lam, Maximilian and Revels, Jarrett and Howard, Steve and Giordano, Ryan and Schlegel, David},
  year = {2018},
  month = may,
  pages = {44--53},
  publisher = {{IEEE}},
  address = {{Vancouver, BC}},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\RDV8WJIS\\Regier et al. - 2018 - Cataloging the Visible Universe Through Bayesian I.pdf}
}

@inproceedings{regier_fast_2017,
  title = {Fast Black-Box Variational Inference through Stochastic Trust-Region Optimization},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Regier, Jeffrey and Jordan, Michael I and McAuliffe, Jon},
  year = {2017},
  volume = {30},
  publisher = {{Curran Associates, Inc.}},
  abstract = {We introduce TrustVI, a fast second-order algorithm for black-box variational inference based on trust-region optimization and the reparameterization trick. At each iteration, TrustVI proposes and assesses a step based on minibatches of draws from the variational distribution. The algorithm provably converges to a stationary point. We implemented TrustVI in the Stan framework and compared it to two alternatives: Automatic Differentiation Variational Inference (ADVI) and Hessian-free Stochastic Gradient Variational Inference (HFSGVI). The former is based on stochastic first-order optimization. The latter uses second-order information, but lacks convergence guarantees. TrustVI typically converged at least one order of magnitude faster than ADVI, demonstrating the value of stochastic second-order information. TrustVI often found substantially better variational distributions than HFSGVI, demonstrating that our convergence theory can matter in practice.},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\7ZBBANVY\\Regier et al. - 2017 - Fast Black-box Variational Inference through Stoch.pdf}
}

@misc{regli_alphabeta_2018,
  title = {Alpha-Beta Divergence for Variational Inference},
  author = {Regli, Jean-Baptiste and Silva, Ricardo},
  year = {2018},
  month = may,
  number = {arXiv:1805.01045},
  eprint = {1805.01045},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  institution = {{ArXiv}},
  abstract = {This paper introduces a variational approximation framework using direct optimization of what is known as the \{\textbackslash it scale invariant Alpha-Beta divergence\} (sAB divergence). This new objective encompasses most variational objectives that use the Kullback-Leibler, the R\{\textbackslash 'e\}nyi or the gamma divergences. It also gives access to objective functions never exploited before in the context of variational inference. This is achieved via two easy to interpret control parameters, which allow for a smooth interpolation over the divergence space while trading-off properties such as mass-covering of a target distribution and robustness to outliers in the data. Furthermore, the sAB variational objective can be optimized directly by repurposing existing methods for Monte Carlo computation of complex variational objectives, leading to estimates of the divergence instead of variational lower bounds. We show the advantages of this objective on Bayesian models for regression problems.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\WWX23CZI\\Regli and Silva - 2018 - Alpha-Beta Divergence For Variational Inference.pdf;C\:\\Users\\msca8h\\Zotero\\storage\\RXTW3RKY\\1805.html}
}

@article{rendell_global_2021,
  title = {Global Consensus {{Monte Carlo}}},
  author = {Rendell, Lewis J. and Johansen, Adam M. and Lee, Anthony and Whiteley, Nick},
  year = {2021},
  month = apr,
  journal = {Journal of Computational and Graphical Statistics},
  volume = {30},
  number = {2},
  pages = {249--259},
  langid = {english},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\5NVJBDVR\\Rendell et al. - 2021 - Global Consensus Monte Carlo.pdf}
}

@article{richardson_algorithm_1973,
  title = {Algorithm 454: {{The Complex Method}} for {{Constrained Optimization}} [{{E4}}]},
  author = {Richardson, Joel A. and Kuester, J. L.},
  year = {1973},
  month = aug,
  journal = {Commun. ACM},
  volume = {16},
  number = {8},
  pages = {487--489},
  keywords = {Box's algorithm,constrained optimization,optimization}
}

@article{ridgeway_sequential_2003,
  title = {A {{Sequential Monte Carlo Method}} for {{Bayesian Analysis}} of {{Massive Datasets}}},
  author = {Ridgeway, Greg},
  year = {2003},
  journal = {Data Mining and Knowledge Discovery},
  volume = {7},
  number = {3},
  pages = {301--319},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\TL9E3XMP\\Ridgeway - 2003 - [No title found].pdf}
}

@article{rindal_effect_2019,
  title = {The Effect of Dynamic Range Alterations in the Estimation of Contrast},
  author = {Rindal, Ole Marius Hoel and Austeng, Andreas and Fatemi, Ali and {Rodriguez-Molares}, Alfonso},
  year = {2019},
  month = jul,
  journal = {IEEE Transactions on Ultrasonics, Ferroelectrics, and Frequency Control},
  volume = {66},
  number = {7},
  pages = {1198--1208},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\EDKMDA9Y\\Rindal et al. - 2019 - The Effect of Dynamic Range Alterations in the Est.pdf}
}

@misc{riou-durand_metropolis_2022,
  title = {Metropolis Adjusted {{Langevin}} Trajectories: {{A}} Robust Alternative to {{Hamiltonian Monte Carlo}}},
  shorttitle = {Metropolis {{Adjusted Langevin Trajectories}}},
  author = {{Riou-Durand}, Lionel and Vogrinc, Jure},
  year = {2022},
  month = mar,
  number = {arXiv:2202.13230},
  eprint = {2202.13230},
  eprinttype = {arxiv},
  primaryclass = {math, stat},
  institution = {{arXiv}},
  abstract = {Hamiltonian Monte Carlo (HMC) is a widely used sampler, known for its efficiency on high dimensional distributions. Yet HMC remains quite sensitive to the choice of integration time. Randomizing the length of Hamiltonian trajectories (RHMC) has been suggested to smooth the Auto-Correlation Functions (ACF), ensuring robustness of tuning. We present the Langevin diffusion as an alternative to control these ACFs by inducing randomness in Hamiltonian trajectories through a continuous refreshment of the velocities. We connect and compare the two processes in terms of quantitative mixing rates for the 2-Wasserstein and \$\textbackslash mathbb\{L\}\_2\$ distances. The Langevin diffusion is presented as a limit of RHMC achieving the fastest mixing rate for strongly log-concave targets. We introduce a robust alternative to HMC built upon these dynamics, named Metropolis Adjusted Langevin Trajectories (MALT). Studying the scaling limit of MALT, we obtain optimal tuning guidelines similar to HMC, and recover the same scaling with respect to the dimension without additional assumptions. We illustrate numerically the efficiency of MALT compared to HMC and RHMC.},
  archiveprefix = {arXiv},
  keywords = {60J25 (Primary) 60H10; 60H30; 65C05 (Secondary),Mathematics - Probability,Mathematics - Statistics Theory,Statistics - Computation},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\WJQ2UTR5\\Riou-Durand and Vogrinc - 2022 - Metropolis Adjusted Langevin Trajectories a robus.pdf;C\:\\Users\\msca8h\\Zotero\\storage\\YY4NI856\\2202.html}
}

@article{rissanen_modeling_1978,
  title = {Modeling by Shortest Data Description},
  author = {Rissanen, J.},
  year = {1978},
  month = sep,
  journal = {Automatica},
  volume = {14},
  number = {5},
  pages = {465--471},
  abstract = {The number of digits it takes to write down an observed sequence x1, \ldots, xN of a time series depends on the model with its parameters that one assumes to have generated the observed data. Accordingly, by finding the model which minimizes the description length one obtains estimates of both the integer-valued structure parameters and the real-valued system parameters.},
  langid = {english},
  keywords = {identification,Modeling,parameter estimation,statistics,stochastic systems},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\Z4Z7GA42\\0005109878900055.html}
}

@article{robbins_stochastic_1951,
  title = {A Stochastic Approximation Method},
  author = {Robbins, Herbert and Monro, Sutton},
  year = {1951},
  month = sep,
  journal = {The Annals of Mathematical Statistics},
  volume = {22},
  number = {3},
  pages = {400--407},
  langid = {english},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\KZ43C2HW\\Robbins and Monro - 1951 - A Stochastic Approximation Method.pdf}
}

@article{robert_accelerating_2018,
  title = {Accelerating {{MCMC}} Algorithms},
  author = {Robert, Christian P. and Elvira, V{\'i}ctor and Tawn, Nick and Wu, Changye},
  year = {2018},
  month = sep,
  journal = {Wiley Interdisciplinary Reviews: Computational Statistics},
  volume = {10},
  number = {5},
  pages = {e1435},
  langid = {english},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\96HN8W6A\\Robert et al. - 2018 - Accelerating MCMC algorithms.pdf}
}

@book{robert_monte_2004,
  title = {Monte {{Carlo Statistical Methods}}},
  author = {Robert, Christian P. and Casella, George},
  year = {2004},
  series = {Springer {{Texts}} in {{Statistics}}},
  publisher = {{Springer New York}},
  address = {{New York, NY}},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\3R9A4UZ5\\Robert and Casella - 2004 - Monte Carlo Statistical Methods.pdf}
}

@article{robert_raoblackwellization_2021,
  title = {Rao-{{Blackwellization}} in the {{MCMC}} Era},
  author = {Robert, Christian P. and Roberts, Gareth O.},
  year = {2021},
  month = jan,
  journal = {arXiv:2101.01011 [math, stat]},
  eprint = {2101.01011},
  eprinttype = {arxiv},
  primaryclass = {math, stat},
  abstract = {Rao-Blackwellization is a notion often occurring in the MCMC literature, with possibly different meanings and connections with the original Rao--Blackwell theorem (Rao, 1945 and Blackwell,1947), including a reduction of the variance of the resulting Monte Carlo approximations. This survey reviews some of the meanings of the term.},
  archiveprefix = {arXiv},
  keywords = {Mathematics - Statistics Theory,Statistics - Computation},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\HXK42AV8\\Robert and Roberts - 2021 - Rao-Blackwellization in the MCMC era.pdf;C\:\\Users\\msca8h\\Zotero\\storage\\F5JZSIJZ\\2101.html}
}

@article{roberts_convergence_1999,
  title = {Convergence of Slice Sampler {{Markov}} Chains},
  author = {Roberts, Gareth O. and Rosenthal, Jeffrey S.},
  year = {1999},
  journal = {Journal of the Royal Statistical Society. Series B (Statistical Methodology)},
  volume = {61},
  number = {3},
  pages = {643--660},
  publisher = {{[Royal Statistical Society, Wiley]}},
  abstract = {We analyse theoretical properties of the slice sampler. We find that the algorithm has extremely robust geometric ergodicity properties. For the case of just one auxiliary variable, we demonstrate that the algorithm is stochastically monotone, and we deduce analytic bounds on the total variation distance from stationarity of the method by using Foster-Lyapunov drift condition methodology.},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\JM8FBVE5\\Roberts and Rosenthal - 1999 - Convergence of Slice Sampler Markov Chains.pdf}
}

@article{roberts_exponential_1996,
  title = {Exponential Convergence of {{Langevin}} Distributions and Their Discrete Approximations},
  author = {Roberts, Gareth O. and Tweedie, Richard L.},
  year = {1996},
  month = dec,
  journal = {Bernoulli},
  volume = {2},
  number = {4},
  pages = {341--363}
}

@article{roberts_general_2004,
  title = {General State Space {{Markov}} Chains and {{MCMC}} Algorithms},
  author = {Roberts, Gareth O. and Rosenthal, Jeffrey S.},
  year = {2004},
  month = jan,
  journal = {Probability Surveys},
  volume = {1},
  number = {none},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\DPDJZGKC\\Roberts and Rosenthal - 2004 - General state space Markov chains and MCMC algorit.pdf}
}

@article{roberts_langevin_2002,
  title = {Langevin {{Diffusions}} and {{Metropolis-Hastings Algorithms}}},
  author = {Roberts, G. O. and Stramer, O.},
  year = {2002},
  journal = {Methodology and Computing in Applied Probability},
  volume = {4},
  number = {4},
  pages = {337--357}
}

@article{roberts_optimal_1998,
  title = {Optimal Scaling of Discrete Approximations to {{Langevin}} Diffusions},
  author = {Roberts, Gareth O. and Rosenthal, Jeffrey S.},
  year = {1998},
  journal = {Journal of the Royal Statistical Society. Series B (Statistical Methodology)},
  volume = {60},
  number = {1},
  pages = {255--268},
  publisher = {{[Royal Statistical Society, Wiley]}},
  abstract = {We consider the optimal scaling problem for proposal distributions in Hastings--Metropolis algorithms derived from Langevin diffusions. We propose an asymptotic diffusion limit theorem and show that the relative efficiency of the algorithm can be characterized by its overall acceptance rate, independently of the target distribution. The asymptotically optimal acceptance rate is 0.574. We show that, as a function of dimension n, the complexity of the algorithm is O(n\textsuperscript{1/3}), which compares favourably with the O(n) complexity of random walk Metropolis algorithms. We illustrate this comparison with some example simulations.},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\USS9GB3D\\Roberts and Rosenthal - 1998 - Optimal Scaling of Discrete Approximations to Lang.pdf}
}

@article{roberts_variational_2002,
  title = {Variational {{Bayes}} for Generalized Autoregressive Models},
  author = {Roberts, S.J. and Penny, W.D.},
  year = {2002},
  month = sep,
  journal = {IEEE Transactions on Signal Processing},
  volume = {50},
  number = {9},
  pages = {2245--2257},
  langid = {english},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\D2YAZA3F\\Roberts and Penny - 2002 - Variational Bayes for generalized autoregressive m.pdf}
}

@article{rodriguez-molares_generalized_2020,
  title = {The Generalized Contrast-to-Noise Ratio: {{A}} Formal Definition for Lesion Detectability},
  shorttitle = {The {{Generalized Contrast-to-Noise Ratio}}},
  author = {{Rodriguez-Molares}, Alfonso and Rindal, Ole Marius Hoel and D'hooge, Jan and Masoy, Svein-Erik and Austeng, Andreas and Lediju Bell, Muyinatu A. and Torp, Hans},
  year = {2020},
  month = apr,
  journal = {IEEE Transactions on Ultrasonics, Ferroelectrics, and Frequency Control},
  volume = {67},
  number = {4},
  pages = {745--759},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\I4CXGAUW\\Rodriguez-Molares et al. - 2020 - The Generalized Contrast-to-Noise Ratio A Formal .pdf}
}

@inproceedings{roeder_sticking_2017,
  title = {Sticking the Landing: {{Simple}}, Lower-Variance Gradient Estimators for Variational Inference},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Roeder, Geoffrey and Wu, Yuhuai and Duvenaud, David K},
  year = {2017},
  volume = {30},
  publisher = {{Curran Associates, Inc.}}
}

@article{roodaki_comments_2013,
  title = {Comments on ``{{Joint Bayesian}} Model Selection and Estimation of Noisy Sinusoids via Reversible Jump {{MCMC}}''},
  author = {Roodaki, Alireza and Bect, Julien and Fleury, Gilles},
  year = {2013},
  month = jul,
  journal = {IEEE Transactions on Signal Processing},
  volume = {61},
  number = {14},
  pages = {3653--3655},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\NFAC5GMN\\Roodaki et al. - 2013 - Comments on Joint Bayesian Model Selection and Es.pdf}
}

@inproceedings{roodaki_joint_2010,
  title = {On the Joint {{Bayesian}} Model Selection and Estimation of Sinusoids via {{Reversible Jump MCMC}} in Low {{SNR}} Situations},
  booktitle = {Proc. {{Int}}. {{Conf}}. {{Inf}}. {{Sci}}, {{Signal Process}}. {{Appl}}.},
  author = {Roodaki, Alireza and Bect, Julien and Fleury, Gilles},
  year = {2010},
  month = may,
  pages = {5--8},
  publisher = {{IEEE}},
  address = {{Kuala Lumpur, Malaysia}}
}

@article{roodaki_relabeling_2014,
  title = {Relabeling and {{Summarizing Posterior Distributions}} in {{Signal Decomposition Problems When}} the {{Number}} of {{Components}} Is {{Unknown}}},
  author = {Roodaki, Alireza and Bect, Julien and Fleury, Gilles},
  year = {2014},
  month = aug,
  journal = {IEEE Transactions on Signal Processing},
  volume = {62},
  number = {16},
  pages = {4091--4104}
}

@phdthesis{roodaki_signal_2012,
  title = {Signal Decompositions Using Trans-Dimensional {{Bayesian}} Methods},
  author = {Roodaki, Alireza},
  year = {2012},
  school = {\'Ecole sup\'erieure d'\'electricit\'e}
}

@article{rosenthal_asymptotic_2003a,
  title = {Asymptotic Variance and Convergence Rates of Nearly-Periodic {{Markov}} Chain Monte Carlo Algorithms},
  author = {Rosenthal, Jeffrey S},
  year = {2003},
  month = mar,
  journal = {Journal of the American Statistical Association},
  volume = {98},
  number = {461},
  pages = {169--177},
  publisher = {{Taylor \& Francis}},
  abstract = {This article considers nearly-periodic Markov chains that may have excellent functional estimation properties but poor distributional convergence rate. It shows how simple modifications of the chain (involving using a random number of iterations) can greatly improve the distributional convergence of the chain. Various theoretical results about convergence rates of the modified chains are proven. A number of examples, including a transdimensional Markov chain Monte Carlo example, a card-shuffling example, and several antithetic Metropolis algorithms, are considered.},
  keywords = {Asymptotic variance,Convergence rate,Markov chain Monte Carlo,Periodicity},
  annotation = {\_eprint: https://doi.org/10.1198/016214503388619193},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\8UKGN2WN\\Rosenthal - 2003 - Asymptotic Variance and Convergence Rates of Nearl.pdf}
}

@incollection{rosenthal_optimal_,
  title = {Optimal Proposal Distributions and Adaptive {{MCMC}}},
  booktitle = {Handbook of {{Markov Chain Monte Carlo}}},
  author = {Rosenthal, Jeffrey S},
  publisher = {{Chapman and Hall/CRC}}
}

@article{Rosenthal99parallelcomputing,
  title = {Parallel Computing and Monte Carlo Algorithms},
  author = {Rosenthal, Jeffrey S.},
  year = {1999},
  journal = {Far East Journal of Theoretical Statistics},
  volume = {4},
  pages = {207--236}
}

@inproceedings{roy_exploiting_2016,
  title = {Exploiting {{Performance Portability}} in {{Search Algorithms}} for {{Autotuning}}},
  booktitle = {2016 {{IEEE International Parallel}} and {{Distributed Processing Symposium Workshops}} ({{IPDPSW}})},
  author = {Roy, Amit and Balaprakash, Prasanna and Hovland, Paul D. and Wild, Stefan M.},
  year = {2016},
  month = may,
  pages = {1535--1544},
  publisher = {{IEEE}},
  address = {{Chicago, IL, USA}}
}

@inproceedings{ru_fast_2018,
  title = {Fast {{Information-theoretic Bayesian Optimisation}}},
  booktitle = {Proceedings of the 35th {{International Conference}} on {{Machine Learning}}},
  author = {Ru, Binxin and Osborne, Michael A. and Mcleod, Mark and Granziol, Diego},
  year = {2018},
  month = jul,
  series = {Proceedings of {{Machine Learning Research}}},
  volume = {80},
  pages = {4384--4392},
  publisher = {{PMLR}},
  address = {{Stockholmsm\"assan, Stockholm Sweden}},
  abstract = {Information-theoretic Bayesian optimisation techniques have demonstrated state-of-the-art performance in tackling important global optimisation problems. However, current information-theoretic approaches require many approximations in implementation, introduce often-prohibitive computational overhead and limit the choice of kernels available to model the objective. We develop a fast information-theoretic Bayesian Optimisation method, FITBO, that avoids the need for sampling the global minimiser, thus significantly reducing computational overhead. Moreover, in comparison with existing approaches, our method faces fewer constraints on kernel choice and enjoys the merits of dealing with the output space. We demonstrate empirically that FITBO inherits the performance associated with information-theoretic Bayesian optimisation, while being even faster than simpler Bayesian optimisation approaches, such as Expected Improvement.}
}

@article{rubin_estimation_1981,
  title = {Estimation in {{Parallel Randomized Experiments}}},
  author = {Rubin, Donald B.},
  year = {1981},
  journal = {Journal of Educational Statistics},
  volume = {6},
  number = {4},
  pages = {377}
}

@article{rubsamen_maximally_2013,
  title = {Maximally Robust {{Capon}} Beamformer},
  author = {Rubsamen, Michael and Pesavento, Marius},
  year = {2013},
  month = apr,
  journal = {IEEE Transactions on Signal Processing},
  volume = {61},
  number = {8},
  pages = {2030--2041}
}

@article{rubsamen_robust_2012,
  title = {Robust Adaptive Beamforming Using Multidimensional Covariance Fitting},
  author = {Rubsamen, Michael and Gershman, Alex B.},
  year = {2012},
  month = feb,
  journal = {IEEE Transactions on Signal Processing},
  volume = {60},
  number = {2},
  pages = {740--753}
}

@article{rudolf_error_2010,
  title = {Error Bounds for Computing the Expectation by {{Markov}} Chain {{Monte Carlo}}},
  author = {Rudolf, Daniel},
  year = {2010},
  month = jan,
  journal = {Monte Carlo Methods and Applications},
  volume = {16},
  number = {3-4},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\GC33VBEB\\Rudolf - 2010 - Error bounds for computing the expectation by Mark.pdf}
}

@article{ruhe_perturbation_1970,
  title = {Perturbation Bounds for Means of Eigenvalues and Invariant Subspaces},
  author = {Ruhe, Axel},
  year = {1970},
  month = sep,
  journal = {BIT Numerical Mathematics},
  volume = {10},
  number = {3},
  pages = {343--354},
  abstract = {When a matrix is close to a matrix with a multiple eigenvalue, the arithmetic mean of a group of eigenvalues is a good approximation to this multiple eigenvalue. A theorem of Gershgorin type for means of eigenvalues is proved and applied as a perturbation theorem for a degenerate matrix.},
  langid = {english},
  keywords = {Assure,Computational Mathematic,Compute Base,Invariant Subspace,Singular Vector},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\3A4H2UG8\\Ruhe - 1970 - Perturbation bounds for means of eigenvalues and i.pdf}
}

@techreport{ruppert_efficient_1988,
  type = {Technical {{Report}}},
  title = {Efficient Estimations from a Slowly Convergent {{Robbins-Monro}} Process},
  author = {Ruppert, David},
  year = {1988},
  month = feb,
  number = {781},
  address = {{Ithaca, New York}},
  institution = {{Cornell University School of Operations Research and Industrial Engineering}}
}

@inproceedings{rusira_automating_2017,
  title = {Automating {{Compiler-Directed Autotuning}} for {{Phased Performance Behavior}}},
  booktitle = {2017 {{IEEE International Parallel}} and {{Distributed Processing Symposium Workshops}} ({{IPDPSW}})},
  author = {Rusira, Tharindu and Hall, Mary and Basu, Protonu},
  year = {2017},
  month = may,
  pages = {1362--1371},
  publisher = {{IEEE}},
  address = {{Orlando / Buena Vista, FL, USA}}
}

@article{sabanesbove_hyper_2011,
  title = {Hyper-$g$ Priors for Generalized Linear Models},
  author = {Saban{\'e}s Bov{\'e}, Daniel and Held, Leonhard},
  year = {2011},
  month = sep,
  journal = {Bayesian Analysis},
  volume = {6},
  number = {3},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\E6LE9EVU\\Sabans Bov and Held - 2011 - Hyper-$g$ priors for generalized linear models.pdf}
}

@incollection{salimans_weight_2016,
  title = {Weight {{Normalization}}: {{A Simple Reparameterization}} to {{Accelerate Training}} of {{Deep Neural Networks}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 29},
  author = {Salimans, Tim and Kingma, Durk P},
  year = {2016},
  pages = {901--909},
  publisher = {{Curran Associates, Inc.}}
}

@article{salvatier_probabilistic_2016,
  title = {Probabilistic Programming in {{Python}} Using {{PyMC3}}},
  author = {Salvatier, John and Wiecki, Thomas V. and Fonnesbeck, Christopher},
  year = {2016},
  month = apr,
  journal = {PeerJ Computer Science},
  volume = {2},
  pages = {e55},
  abstract = {Probabilistic programming allows for automatic Bayesian inference on user-defined probabilistic models. Recent advances in Markov chain Monte Carlo (MCMC) sampling allow inference on increasingly complex models. This class of MCMC, known as Hamiltonian Monte Carlo, requires gradient information which is often not readily available. PyMC3 is a new open source probabilistic programming framework written in Python that uses Theano to compute gradients via automatic differentiation as well as compile probabilistic programs on-the-fly to C for increased speed. Contrary to other probabilistic programming languages, PyMC3 allows model specification directly in Python code. The lack of a domain specific language allows for great flexibility and direct interaction with the model. This paper is a tutorial-style introduction to this software package.},
  langid = {english},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\EL4ML6D5\\Salvatier et al. - 2016 - Probabilistic programming in Python using PyMC3.pdf}
}

@article{sameni_multichannel_2007,
  title = {Multichannel {{ECG}} and Noise Modeling: {{Application}} to Maternal and Fetal {{ECG}} Signals},
  shorttitle = {Multichannel {{ECG}} and {{Noise Modeling}}},
  author = {Sameni, Reza and Clifford, Gari D and Jutten, Christian and Shamsollahi, Mohammad B},
  year = {2007},
  month = dec,
  journal = {EURASIP Journal on Advances in Signal Processing},
  volume = {2007},
  number = {1},
  pages = {043407},
  langid = {english},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\ANV8WCTZ\\Sameni et al. - 2007 - Multichannel ECG and Noise Modeling Application t.pdf}
}

@article{sameni_nonlinear_2007,
  title = {A Nonlinear {{Bayesian}} Filtering Framework for {{ECG}} Denoising},
  author = {Sameni, R. and Shamsollahi, M.B. and Jutten, C. and Clifford, G.D.},
  year = {2007},
  month = dec,
  journal = {IEEE Transactions on Biomedical Engineering},
  volume = {54},
  number = {12},
  pages = {2172--2185},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\I7HLG7N8\\Sameni et al. - 2007 - A Nonlinear Bayesian Filtering Framework for ECG D.pdf}
}

@inproceedings{sanders_aces4_2017,
  title = {Aces4: A Platform for Computational Chemistry Calculations with Extremely Large Block-Sparse Arrays},
  shorttitle = {Aces4},
  booktitle = {2017 {{IEEE International Parallel}} and {{Distributed Processing Symposium}} ({{IPDPS}})},
  author = {Sanders, Beverly A. and Byrd, Jason N. and Jindal, Nakul and Lotrich, Victor F. and Lyakh, Dmitry and Perera, Ajith and Bartlett, Rodney J.},
  year = {2017},
  month = may,
  pages = {555--564},
  publisher = {{IEEE}},
  address = {{Orlando, FL, USA}}
}

@article{sano_application_2020,
  title = {Application of {{Bayesian}} Optimization for Pharmaceutical Product Development},
  author = {Sano, Syusuke and Kadowaki, Tadashi and Tsuda, Koji and Kimura, Susumu},
  year = {2020},
  month = sep,
  journal = {Journal of Pharmaceutical Innovation},
  volume = {15},
  number = {3},
  pages = {333--343},
  langid = {english}
}

@inproceedings{santos_bayesian_2010,
  title = {Bayesian Optimization of Perfusion and Transit Time Estimation in {{PASL-MRI}}},
  booktitle = {2010 {{Annual International Conference}} of the {{IEEE Engineering}} in {{Medicine}} and {{Biology}}},
  author = {Santos, N and Sanches, J and Figueiredo, P},
  year = {2010},
  month = aug,
  pages = {4284--4287},
  publisher = {{IEEE}},
  address = {{Buenos Aires}}
}

@incollection{santurkar_how_2018,
  title = {How {{Does Batch Normalization Help Optimization}}?},
  booktitle = {Advances in {{Neural Information Processing Systems}} 31},
  author = {Santurkar, Shibani and Tsipras, Dimitris and Ilyas, Andrew and Madry, Aleksander},
  year = {2018},
  pages = {2483--2493},
  publisher = {{Curran Associates, Inc.}}
}

@article{sason_divergence_2016,
  title = {\$f\$ -{{Divergence Inequalities}}},
  author = {Sason, Igal and Verdu, Sergio},
  year = {2016},
  month = nov,
  journal = {IEEE Transactions on Information Theory},
  volume = {62},
  number = {11},
  pages = {5973--6006},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\C25KSZ2R\\Sason and Verdu - 2016 - $f$ -Divergence Inequalities.pdf}
}

@article{sato_autotuning_2019,
  title = {An {{Autotuning Framework}} for {{Scalable Execution}} of {{Tiled Code}} via {{Iterative Polyhedral Compilation}}},
  author = {Sato, Yukinori and Yuki, Tomoya and Endo, Toshio},
  year = {2019},
  month = jan,
  journal = {ACM Transactions on Architecture and Code Optimization},
  volume = {15},
  number = {4},
  pages = {1--23},
  langid = {english},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\U7QYJSG2\\Sato et al. - 2019 - An Autotuning Framework for Scalable Execution of .pdf}
}

@article{sattar_image_1997,
  title = {Image Enhancement Based on a Nonlinear Multiscale Method},
  author = {Sattar, F. and Floreby, L. and Salomonsson, G. and Lovstrom, B.},
  year = {1997},
  month = jun,
  journal = {IEEE Transactions on Image Processing},
  volume = {6},
  number = {6},
  pages = {888--895}
}

@article{savage_theory_1951,
  title = {The {{Theory}} of {{Statistical Decision}}},
  author = {Savage, L. J.},
  year = {1951},
  month = mar,
  journal = {Journal of the American Statistical Association},
  volume = {46},
  number = {253},
  pages = {55--67},
  langid = {english}
}

@article{savioja_overview_2015,
  title = {Overview of Geometrical Room Acoustic Modeling Techniques},
  author = {Savioja, Lauri and Svensson, U. Peter},
  year = {2015},
  month = aug,
  journal = {The Journal of the Acoustical Society of America},
  volume = {138},
  number = {2},
  pages = {708--730},
  langid = {english},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\XZ4455FJ\\Savioja and Svensson - 2015 - Overview of geometrical room acoustic modeling tec.pdf}
}

@article{savitsky_variable_2011,
  title = {Variable Selection for Nonparametric {{Gaussian}} Process Priors: Models and Computational Strategies},
  shorttitle = {Variable {{Selection}} for {{Nonparametric Gaussian Process Priors}}},
  author = {Savitsky, Terrance and Vannucci, Marina and Sha, Naijun},
  year = {2011},
  month = feb,
  journal = {Statistical Science},
  volume = {26},
  number = {1},
  pages = {130--149},
  langid = {english},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\UGAGTGRJ\\Savitsky et al. - 2011 - Variable Selection for Nonparametric Gaussian Proc.pdf}
}

@inproceedings{saxe_random_2011,
  title = {On {{Random Weights}} and {{Unsupervised Feature Learning}}},
  booktitle = {Proceedings of the 28th {{International Conference}} on {{International Conference}} on {{Machine Learning}}},
  author = {Saxe, Andrew M. and Koh, Pang Wei and Chen, Zhenghao and Bhand, Maneesh and Suresh, Bipin and Ng, Andrew Y.},
  year = {2011},
  series = {{{ICML}}'11},
  pages = {1089--1096},
  publisher = {{Omnipress}},
  address = {{Madison, WI, USA}}
}

@article{schad_workflow_2022,
  title = {Workflow Techniques for the Robust Use of {{Bayes}} Factors},
  author = {Schad, Daniel J. and Nicenboim, Bruno and B{\"u}rkner, Paul-Christian and Betancourt, Michael and Vasishth, Shravan},
  year = {2022},
  month = mar,
  journal = {Psychological Methods},
  langid = {english},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\RYHEWUK3\\Schad et al. - 2022 - Workflow techniques for the robust use of bayes fa.pdf}
}

@article{schafer_digital_1973,
  title = {A Digital Signal Processing Approach to Interpolation},
  author = {Schafer, R.W. and Rabiner, L.R.},
  year = {1973},
  journal = {Proceedings of the IEEE},
  volume = {61},
  number = {6},
  pages = {692--702}
}

@inproceedings{schmidt_bayesian_2009,
  title = {Bayesian Non-Negative Matrix Factorization},
  booktitle = {Independent {{Component Analysis}} and {{Signal Separation}}},
  author = {Schmidt, Mikkel N. and Winther, Ole and Hansen, Lars Kai},
  year = {2009},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {540--547},
  publisher = {{Springer}},
  address = {{Berlin, Heidelberg}},
  abstract = {We present a Bayesian treatment of non-negative matrix factorization (NMF), based on a normal likelihood and exponential priors, and derive an efficient Gibbs sampler to approximate the posterior density of the NMF factors. On a chemical brain imaging data set, we show that this improves interpretability by providing uncertainty estimates. We discuss how the Gibbs sampler can be used for model order selection by estimating the marginal likelihood, and compare with the Bayesian information criterion. For computing the maximum a posteriori estimate we present an iterated conditional modes algorithm that rivals existing state-of-the-art NMF algorithms on an image feature extraction problem.},
  langid = {english},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\9TDMYVJT\\Schmidt et al. - 2009 - Bayesian Non-negative Matrix Factorization.pdf}
}

@misc{schmidt_fast_2013,
  title = {Fast Convergence of Stochastic Gradient Descent under a Strong Growth Condition},
  author = {Schmidt, Mark and Roux, Nicolas Le},
  year = {2013},
  month = aug,
  number = {arXiv:1308.6370},
  eprint = {1308.6370},
  eprinttype = {arxiv},
  primaryclass = {math},
  institution = {{arXiv}},
  abstract = {We consider optimizing a function smooth convex function \$f\$ that is the average of a set of differentiable functions \$f\_i\$, under the assumption considered by Solodov [1998] and Tseng [1998] that the norm of each gradient \$f\_i'\$ is bounded by a linear function of the norm of the average gradient \$f'\$. We show that under these assumptions the basic stochastic gradient method with a sufficiently-small constant step-size has an \$O(1/k)\$ convergence rate, and has a linear convergence rate if \$g\$ is strongly-convex.},
  archiveprefix = {arXiv},
  keywords = {Mathematics - Optimization and Control},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\AUTBD4ZX\\Schmidt and Roux - 2013 - Fast Convergence of Stochastic Gradient Descent un.pdf;C\:\\Users\\msca8h\\Zotero\\storage\\HLR6A8EB\\1308.html}
}

@article{schulman_proximal_2017,
  title = {Proximal {{Policy Optimization Algorithms}}},
  author = {Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
  year = {2017},
  month = aug,
  journal = {arXiv:1707.06347 [cs]},
  eprint = {1707.06347},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {We propose a new family of policy gradient methods for reinforcement learning, which alternate between sampling data through interaction with the environment, and optimizing a "surrogate" objective function using stochastic gradient ascent. Whereas standard policy gradient methods perform one gradient update per data sample, we propose a novel objective function that enables multiple epochs of minibatch updates. The new methods, which we call proximal policy optimization (PPO), have some of the benefits of trust region policy optimization (TRPO), but they are much simpler to implement, more general, and have better sample complexity (empirically). Our experiments test PPO on a collection of benchmark tasks, including simulated robotic locomotion and Atari game playing, and we show that PPO outperforms other online policy gradient methods, and overall strikes a favorable balance between sample complexity, simplicity, and wall-time.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\3R9RHEXE\\Schulman et al. - 2017 - Proximal Policy Optimization Algorithms.pdf;C\:\\Users\\msca8h\\Zotero\\storage\\D9TET6JR\\1707.html}
}

@article{schwarz_estimating_1978,
  title = {Estimating the Dimension of a Model},
  author = {Schwarz, Gideon},
  year = {1978},
  month = mar,
  journal = {The Annals of Statistics},
  volume = {6},
  number = {2},
  pages = {461--464},
  publisher = {{Institute of Mathematical Statistics}},
  abstract = {The problem of selecting one of a number of models of different dimensions is treated by finding its Bayes solution, and evaluating the leading terms of its asymptotic expansion. These terms are a valid large-sample criterion beyond the Bayesian context, since they do not depend on the a priori distribution.},
  keywords = {62F99,62J99,Akaike information criterion,asymptotics,dimension},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\QJUHQCMG\\Schwarz - 1978 - Estimating the Dimension of a Model.pdf;C\:\\Users\\msca8h\\Zotero\\storage\\SVLRXLRA\\1176344136.html}
}

@inproceedings{sebbouh_almost_2021,
  title = {Almost Sure Convergence Rates for Stochastic Gradient Descent and Stochastic Heavy Ball},
  booktitle = {Proceedings of the {{Conference}} on {{Learning Theory}}},
  author = {Sebbouh, Othmane and Gower, Robert M. and Defazio, Aaron},
  year = {2021},
  month = jul,
  pages = {3935--3971},
  publisher = {{PMLR}},
  abstract = {We study stochastic gradient descent (SGD) and the stochastic heavy ball method (SHB, otherwise known as the momentum method) for the general stochastic approximation problem.  For SGD, in the convex and smooth setting, we provide the first \textbackslash emph\{almost sure\} asymptotic convergence \textbackslash emph\{rates\} for a weighted average of the iterates . More precisely, we show that the convergence rate of the function values is arbitrarily close to o(1/k--{$\surd$})o(1/k)o(1/\textbackslash sqrt\{k\}), and is exactly o(1/k)o(1/k)o(1/k) in the so-called overparametrized case. We show that these results still hold when using a decreasing step size version of stochastic line search and stochastic Polyak stepsizes, thereby giving the first proof of convergence of these methods in the non-overparametrized regime.  Using a substantially different analysis, we show that these rates hold for SHB as well, but at the last iterate. This distinction is important because it is the last iterate of SGD and SHB which is used in practice. We also show that the last iterate of SHB converges to a minimizer \textbackslash emph\{almost surely\}. Additionally, we prove that the function values of the deterministic HB converge at a o(1/k)o(1/k)o(1/k) rate, which is faster than the previously known O(1/k)O(1/k)O(1/k).  Finally, in the nonconvex setting, we prove similar rates on the lowest gradient norm along the trajectory of SGD.},
  langid = {english},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\Y2X8U63N\\Sebbouh et al. - 2021 - Almost sure convergence rates for Stochastic Gradi.pdf}
}

@inproceedings{seelam_extreme_2010,
  title = {Extreme Scale Computing: {{Modeling}} the Impact of System Noise in Multicore Clustered Systems},
  shorttitle = {Extreme Scale Computing},
  booktitle = {2010 {{IEEE International Symposium}} on {{Parallel}} \& {{Distributed Processing}} ({{IPDPS}})},
  author = {Seelam, Seetharami and Fong, Liana and Tantawi, Asser and Lewars, John and Divirgilio, John and Gildea, Kevin},
  year = {2010},
  pages = {1--12},
  publisher = {{IEEE}},
  address = {{Atlanta, GA, USA}},
  file = {/home/msca8h/Documents/parallel_scheduling/Seelam et al. - 2010 - Extreme scale computing Modeling the impact of sy.pdf}
}

@inproceedings{sehgal_deep_2019,
  title = {Deep {{Reinforcement Learning Using Genetic Algorithm}} for {{Parameter Optimization}}},
  booktitle = {2019 {{Third IEEE International Conference}} on {{Robotic Computing}} ({{IRC}})},
  author = {Sehgal, Adarsh and La, Hung and Louis, Sushil and Nguyen, Hai},
  year = {2019},
  month = feb,
  pages = {596--601},
  publisher = {{IEEE}},
  address = {{Naples, Italy}}
}

@article{selva_efficient_2018,
  title = {Efficient Wideband {{DOA}} Estimation through Function Evaluation Techniques},
  author = {Selva, Jesus},
  year = {2018},
  month = jun,
  journal = {IEEE Transactions on Signal Processing},
  volume = {66},
  number = {12},
  pages = {3112--3123},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\8PSSW7KJ\\Selva - 2018 - Efficient Wideband DOA Estimation Through Function.pdf}
}

@techreport{shaby_exploring_2010,
  type = {Technical {{Report}}},
  title = {Exploring an Adaptive {{Metropolis}} Algorithm},
  author = {Shaby, Benjamin and Wells, Martin},
  year = {2010},
  number = {1011-14},
  institution = {{DukeUniversity Department of Stastical Science}}
}

@inproceedings{shah_bayesian_2013,
  title = {Bayesian Optimization Using {{Student-t}} Processes},
  booktitle = {{{NIPS Workshop}} on {{Bayesian Optim}}.},
  author = {Shah, Amar and Wilson, Andrew Gordon and Ghahramani, Zoubin},
  year = {2013}
}

@inproceedings{shah_bayesian_2013a,
  title = {Bayesian {{Optimization}} Using {{Student-t Processes}}},
  booktitle = {{{NIPS Workshop}} on {{Bayesian Optimisation}}},
  author = {Shah, Amar and Wilson, Andrew Gordon and Ghahramani, Zoubin},
  year = {2013}
}

@inproceedings{shah_studentt_2014,
  title = {Student-t Processes as Alternatives to {{Gaussian}} Processes},
  booktitle = {Artificial {{Intelligence}} and {{Statistics}}},
  author = {Shah, Amar and Wilson, Andrew and Ghahramani, Zoubin},
  year = {2014},
  series = {{{AISTATS}}'14},
  pages = {877--885},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\VCQRDIKW\\Shah et al. - 2014 - Student-t processes as alternatives to Gaussian pr.pdf}
}

@article{shahriari_taking_2016,
  title = {Taking the Human out of the Loop: A Review of {{Bayesian}} Optimization},
  author = {Shahriari, B. and Swersky, K. and Wang, Z. and Adams, R. P. and de Freitas, N.},
  year = {2016},
  month = jan,
  journal = {Proceedings of the IEEE},
  volume = {104},
  number = {1},
  pages = {148--175},
  keywords = {Bayes methods,Bayesian optimization,Big data,Big Data,Big data application,decision making,Decision making,design of experiments,Design of experiments,Genomes,genomic medicine,human productivity,large-scale heterogeneous computing,Linear programming,massive complex software system,optimisation,optimization,Optimization,product quality,response surface methodology,Statistical analysis,statistical learning,storage allocation,storage architecture},
  file = {/home/msca8h/Documents/bayesian_optimization/Shahriari et al. - 2016 - Taking the Human Out of the Loop A Review of Baye.pdf}
}

@article{shan_adaptive_1985,
  title = {Adaptive Beamforming for Coherent Signals and Interference},
  author = {Shan, Tie-Jun and Kailath, T.},
  year = {1985},
  month = jun,
  journal = {IEEE Transactions on Acoustics, Speech, and Signal Processing},
  volume = {33},
  number = {3},
  pages = {527--536},
  langid = {english}
}

@article{shang_vrsgd_2018,
  title = {{{VR-SGD}}: {{A Simple Stochastic Variance Reduction Method}} for {{Machine Learning}}},
  shorttitle = {{{VR-SGD}}},
  author = {Shang, Fanhua and Zhou, Kaiwen and Liu, Hongying and Cheng, James and Tsang, Ivor and Zhang, Lijun and Tao, Dacheng and Licheng, Jiao},
  year = {2018},
  journal = {IEEE Transactions on Knowledge and Data Engineering},
  pages = {1--1},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\CVPA58M4\\Shang et al. - 2018 - VR-SGD A Simple Stochastic Variance Reduction Met.pdf}
}

@article{shaw_improved_2016,
  title = {Improved Wideband {{DOA}} Estimation Using Modified {{TOPS}} ({{mTOPS}}) Algorithm},
  author = {Shaw, Arnab K.},
  year = {2016},
  month = dec,
  journal = {IEEE Signal Processing Letters},
  volume = {23},
  number = {12},
  pages = {1697--1701}
}

@article{sherlock_optimal_2009,
  title = {Optimal Scaling of the Random Walk {{Metropolis}} on Elliptically Symmetric Unimodal Targets},
  author = {Sherlock, Chris and Roberts, Gareth},
  year = {2009},
  journal = {Bernoulli},
  volume = {15},
  number = {3},
  pages = {774--798},
  abstract = {Scaling of proposals for Metropolis algorithms is an important practical problem in MCMC implementation. Criteria for scaling based on empirical acceptance rates of algorithms have been found to work consistently well across a broad range of problems. Essentially, proposal jump sizes are increased when acceptance rates are high and decreased when rates are low. In recent years, considerable theoretical support has been given for rules of this type which work on the basis that acceptance rates around 0.234 should be preferred. This has been based on asymptotic results that approximate high dimensional algorithm trajectories by diffusions. In this paper, we develop a novel approach to understanding 0.234 which avoids the need for diffusion limits. We derive explicit formulae for algorithm efficiency and acceptance rates as functions of the scaling parameter. We apply these to the family of elliptically symmetric target densities, where further illuminating explicit results are possible. Under suitable conditions, we verify the 0.234 rule for a new class of target densities. Moreover, we can characterise cases where 0.234 fails to hold, either because the target density is too diffuse in a sense we make precise, or because the eccentricity of the target density is too severe, again in a sense we make precise. We provide numerical verifications of our results.}
}

@inproceedings{shin_speeding_2010,
  title = {Speeding up {{Nek5000}} with Autotuning and Specialization},
  booktitle = {Proceedings of the 24th {{ACM International Conference}} on {{Supercomputing}} - {{ICS}} '10},
  author = {Shin, Jaewook and Hall, Mary W. and Chame, Jacqueline and Chen, Chun and Fischer, Paul F. and Hovland, Paul D.},
  year = {2010},
  pages = {253},
  publisher = {{ACM Press}},
  address = {{Tsukuba, Ibaraki, Japan}},
  langid = {english}
}

@article{shlezinger_modelbased_2020,
  title = {Model-{{Based Deep Learning}}},
  author = {Shlezinger, Nir and Whang, Jay and Eldar, Yonina C. and Dimakis, Alexandros G.},
  year = {2020},
  month = dec,
  journal = {arXiv:2012.08405 [cs, eess]},
  eprint = {2012.08405},
  eprinttype = {arxiv},
  primaryclass = {cs, eess},
  abstract = {Signal processing, communications, and control have traditionally relied on classical statistical modeling techniques. Such model-based methods utilize mathematical formulations that represent the underlying physics, prior information and additional domain knowledge. Simple classical models are useful but sensitive to inaccuracies and may lead to poor performance when real systems display complex or dynamic behavior. On the other hand, purely data-driven approaches that are model-agnostic are becoming increasingly popular as datasets become abundant and the power of modern deep learning pipelines increases. Deep neural networks (DNNs) use generic architectures which learn to operate from data, and demonstrate excellent performance, especially for supervised problems. However, DNNs typically require massive amounts of data and immense computational resources, limiting their applicability for some signal processing scenarios. We are interested in hybrid techniques that combine principled mathematical models with data-driven systems to benefit from the advantages of both approaches. Such model-based deep learning methods exploit both partial domain knowledge, via mathematical structures designed for specific problems, as well as learning from limited data. In this article we survey the leading approaches for studying and designing model-based deep learning systems. We divide hybrid model-based/data-driven systems into categories based on their inference mechanism. We provide a comprehensive review of the leading approaches for combining model-based algorithms with deep learning in a systematic manner, along with concrete guidelines and detailed signal processing oriented examples from recent literature. Our aim is to facilitate the design and study of future systems on the intersection of signal processing and machine learning that incorporate the advantages of both domains.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Electrical Engineering and Systems Science - Signal Processing},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\ZKFS2FZX\\Shlezinger et al. - 2020 - Model-Based Deep Learning.pdf;C\:\\Users\\msca8h\\Zotero\\storage\\UPIQKL7Q\\2012.html}
}

@article{shutaoli_image_2013,
  title = {Image Fusion with Guided Filtering},
  author = {{Shutao Li} and {Xudong Kang} and {Jianwen Hu}},
  year = {2013},
  month = jul,
  journal = {IEEE Transactions on Image Processing},
  volume = {22},
  number = {7},
  pages = {2864--2875},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\YMY34GNE\\Shutao Li et al. - 2013 - Image Fusion With Guided Filtering.pdf}
}

@article{Sigillito1989ClassificationOR,
  title = {Classification of Radar Returns from the Ionosphere Using Neural Networks},
  author = {Sigillito, Vincent G. and Wing, Simon and Hutton, Larrie V. and Baker, K. L.},
  year = {1989},
  journal = {Johns Hopkins APL Technical Digest},
  volume = {10},
  pages = {262--266}
}

@article{siltanen_room_2007,
  title = {The Room Acoustic Rendering Equation},
  author = {Siltanen, Samuel and Lokki, Tapio and Kiminki, Sami and Savioja, Lauri},
  year = {2007},
  month = sep,
  journal = {The Journal of the Acoustical Society of America},
  volume = {122},
  number = {3},
  pages = {1624--1635},
  langid = {english},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\27ACCIM8\\Siltanen et al. - 2007 - The room acoustic rendering equation.pdf}
}

@article{simanapalli_broadband_1994,
  title = {Broadband Focusing for Partially Adaptive Beamforming},
  author = {Simanapalli, S. and Kaveh, M.},
  year = {Jan./1994},
  journal = {IEEE Transactions on Aerospace and Electronic Systems},
  volume = {30},
  number = {1},
  pages = {68--80}
}

@article{singh_hybrid_2017,
  title = {A Hybrid Algorithm for Speckle Noise Reduction of Ultrasound Images},
  author = {Singh, Karamjeet and Ranade, Sukhjeet Kaur and Singh, Chandan},
  year = {2017},
  month = sep,
  journal = {Computer Methods and Programs in Biomedicine},
  volume = {148},
  pages = {55--69},
  langid = {english}
}

@inproceedings{sirianunpiboon_hierarchical_2006,
  title = {A Hierarchical {{Bayesian}} Approach to Direction Finding and Beamforming},
  booktitle = {Proceedings of the {{IEEE Workshop}} on {{Sensor Array}} and {{Multichannel Processing}}},
  author = {Sirianunpiboon, S. and Howard, S.D. and Asenstorfer, J.},
  year = {2006},
  pages = {21--25},
  publisher = {{IEEE}},
  address = {{Waltham, MA}}
}

@article{sisson_transdimensional_2005,
  title = {Transdimensional {{Markov}} Chains: {{A}} Decade of Progress and Future Perspectives},
  shorttitle = {Transdimensional {{Markov Chains}}},
  author = {Sisson, Scott A},
  year = {2005},
  month = sep,
  journal = {Journal of the American Statistical Association},
  volume = {100},
  number = {471},
  pages = {1077--1089},
  langid = {english}
}

@article{sisson_transdimensional_2005a,
  title = {Transdimensional {{Markov}} Chains: {{A}} Decade of Progress and Future Perspectives},
  shorttitle = {Transdimensional Markov Chains},
  author = {Sisson, Scott A},
  year = {2005},
  month = sep,
  journal = {Journal of the American Statistical Association},
  volume = {100},
  number = {471},
  pages = {1077--1089},
  langid = {english}
}

@inproceedings{skilling_galilean_2019,
  title = {Galilean and {{Hamiltonian Monte Carlo}}},
  booktitle = {Proceedings of the {{International Workshop}} on {{Bayesian Inference}} and {{Maximum Entropy Methods}} in {{Science}} and {{Engineering}}},
  author = {Skilling, John},
  year = {2019},
  month = dec,
  series = {Proceedings},
  abstract = {Galilean Monte Carlo (GMC) allows exploration in a big space along systematic trajectories, thus evading the square-root inefficiency of independent steps. Galilean Monte Carlo has greater generality and power than its historical precursor Hamiltonian Monte Carlo because it discards second-order propagation under forces in favour of elementary force-free motion. Nested sampling (for which GMC was originally designed) has similar dominance over simulated annealing, which loses power by imposing an unnecessary thermal blurring over energy.},
  langid = {english},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\V5ZXCG7X\\Skilling - 2019 - Galilean and Hamiltonian Monte Carlo.pdf}
}

@article{skilling_nested_2006,
  title = {Nested Sampling for General {{Bayesian}} Computation},
  author = {Skilling, John},
  year = {2006},
  month = dec,
  journal = {Bayesian Analysis},
  volume = {1},
  number = {4},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\CM5CW82X\\Skilling - 2006 - Nested sampling for general Bayesian computation.pdf}
}

@article{smidl_mixturebased_2005,
  title = {Mixture-Based Extension of the {{AR}} Model and Its Recursive {{Bayesian}} Identification},
  author = {Smidl, V. and Quinn, A.},
  year = {2005},
  month = sep,
  journal = {IEEE Transactions on Signal Processing},
  volume = {53},
  number = {9},
  pages = {3530--3542},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\NSXYTASR\\Smidl and Quinn - 2005 - Mixture-based extension of the AR model and its re.pdf}
}

@article{smith_cocoa_2018,
  title = {{{CoCoA}}: {{A General Framework}} for {{Communication-Efficient Distributed Optimization}}},
  author = {Smith, Virginia and Forte, Simone and Ma, Chenxin and Tak{\'a}{\v c}, Martin and Jordan, Michael I. and Jaggi, Martin},
  year = {2018},
  journal = {Journal of Machine Learning Research},
  volume = {18},
  number = {230},
  pages = {1--49}
}

@article{smith_susan_1997,
  title = {{{SUSAN}}\textemdash{{A}} New Approach to Low Level Image Processing},
  author = {Smith, Stephen M. and Brady, J. Michael},
  year = {1997},
  journal = {International Journal of Computer Vision},
  volume = {23},
  number = {1},
  pages = {45--78}
}

@article{smith_ultrasound_1984,
  title = {Ultrasound Speckle Size and Lesion Signal to Noise Ratio: Verification of Theory},
  shorttitle = {Ultrasound {{Speckle Size}} and {{Lesion Signal}} to {{Noise Ratio}}},
  author = {Smith, S.W. and Wagner, R.F.},
  year = {1984},
  month = apr,
  journal = {Ultrasonic Imaging},
  volume = {6},
  number = {2},
  pages = {174--180},
  abstract = {We compare predictions from our published theory of speckle cell size with recently published experimental results and a three dimensional computer simulation for the case of Gaussian pulses from spherically focused transducers in random media. The agreement is very good. We also compare our published theoretical predictions of the signal-to-noise ratio for a circular lesion in a speckle background with published ``contrast to speckle ratio'' data for anechoic cylindrical lesions in tissue mimicking material. Again, agreement is very good. The verification of these theoretical predictions has important implications for the evaluation of B-scan image quality and the study of tissue characterization.},
  langid = {english}
}

@inproceedings{smith_using_1988,
  title = {Using the {{ADAP}} Learning Algorithm to Forecast the Onset of Diabetes Mellitus},
  booktitle = {Proceedings of the {{Annual Symposium Computer Application}} in {{Medical Care}}},
  author = {Smith, Jack W and Everhart, J. E. and Dickson, W. C. and Knowler, W. C. and Johannes, R. S.},
  year = {1988},
  month = nov,
  pages = {261--265}
}

@techreport{Smith96exacttransition,
  title = {Exact Transition Probabilities for the Independence Metropolis Sampler},
  author = {Smith, Richard L. and Tierney, Luke},
  year = {1996}
}

@incollection{snoek_can_2019,
  title = {Can You Trust Your Model's Uncertainty? Evaluating Predictive Uncertainty under Dataset Shift},
  booktitle = {Advances in {{Neural Information Processing Systems}} 32},
  author = {Snoek, Jasper and Ovadia, Yaniv and Fertig, Emily and Lakshminarayanan, Balaji and Nowozin, Sebastian and Sculley, D. and Dillon, Joshua and Ren, Jie and Nado, Zachary},
  year = {2019},
  pages = {13969--13980},
  publisher = {{Curran Associates, Inc.}}
}

@inproceedings{snoek_practical_2012,
  title = {Practical {{Bayesian}} Optimization of Machine Learning Algorithms},
  booktitle = {Adv. {{Neural Inf}}. {{Process}}. {{Syst}}.},
  author = {Snoek, Jasper and Larochelle, Hugo and Adams, Ryan P.},
  year = {2012},
  series = {{{NIPS}}'12},
  volume = {25},
  pages = {2951--2959},
  publisher = {{Curran Associates Inc.}},
  address = {{USA}}
}

@mastersthesis{soares_underwater_2017,
  title = {Underwater Simulation and Mapping Using Imaging Sonar through Ray Theory and {{Hilbert}} Maps},
  author = {Soares, Eduardo Elael de Melo},
  year = {2017},
  month = mar,
  school = {Federal University of Rio de Janeiro},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\LM36SMYF\\Soares - 2017 - Underwater simulation and mapping using imaging so.pdf}
}

@article{sobol_distribution_1967,
  title = {On the Distribution of Points in a Cube and the Approximate Evaluation of Integrals},
  author = {Sobol', I.M},
  year = {1967},
  month = jan,
  journal = {USSR Computational Mathematics and Mathematical Physics},
  volume = {7},
  number = {4},
  pages = {86--112},
  langid = {english}
}

@phdthesis{soli_high_2017,
  type = {Doctoral {{Thesis}}},
  title = {High Resolution Continuous Active Sonar},
  author = {Soli, Jonathan Boyd},
  year = {2017},
  address = {{Department of Electrical and Computer Engineering}},
  school = {Duke University},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\PC7WFRQP\\Soli - 2017 - High resolution continuous active sonar.pdf}
}

@article{solonen_efficient_2012,
  title = {Efficient {{MCMC}} for {{Climate Model Parameter Estimation}}: {{Parallel Adaptive Chains}} and {{Early Rejection}}},
  shorttitle = {Efficient {{MCMC}} for {{Climate Model Parameter Estimation}}},
  author = {Solonen, Antti and Ollinaho, Pirkka and Laine, Marko and Haario, Heikki and Tamminen, Johanna and J{\"a}rvinen, Heikki},
  year = {2012},
  month = sep,
  journal = {Bayesian Analysis},
  volume = {7},
  number = {3},
  pages = {715--736},
  langid = {english},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\6XTGQ3DA\\Solonen et al. - 2012 - Efficient MCMC for Climate Model Parameter Estimat.pdf}
}

@misc{som_block_2015,
  title = {Block Hyper-$g$  Priors in {{Bayesian}} Regression},
  author = {Som, Agniva and Hans, Christopher M. and MacEachern, Steven N.},
  year = {2015},
  month = jan,
  number = {arXiv:1406.6419},
  eprint = {1406.6419},
  eprinttype = {arxiv},
  primaryclass = {math, stat},
  institution = {{arXiv}},
  abstract = {The development of prior distributions for Bayesian regression has traditionally been driven by the goal of achieving sensible model selection and parameter estimation. The formalization of properties that characterize good performance has led to the development and popularization of thick tailed mixtures of g priors such as the Zellner--Siow and hyper-g priors. The properties of a particular prior are typically illuminated under limits on the likelihood or the prior. In this paper we introduce a new, conditional information asymptotic that is motivated by the common data analysis setting where at least one regression coefficient is much larger than others. We analyze existing mixtures of g priors under this limit and reveal two new behaviors, Essentially Least Squares (ELS) estimation and the Conditional Lindley's Paradox (CLP), and argue that these behaviors are, in general, undesirable. As the driver behind both of these behaviors is the use of a single, latent scale parameter that is common to all coefficients, we propose a block hyper-g prior, defined by first partitioning the covariates into groups and then placing independent hyper-g priors on the corresponding blocks of coefficients. We provide conditions under which ELS and the CLP are avoided by the new class of priors, and provide consistency results under traditional sample size asymptotics.},
  archiveprefix = {arXiv},
  keywords = {Mathematics - Statistics Theory,Statistics - Methodology},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\5WH2FGKU\\Som et al. - 2015 - Block Hyper-g Priors in Bayesian Regression.pdf;C\:\\Users\\msca8h\\Zotero\\storage\\IRTHJT7L\\1406.html}
}

@article{som_conditional_2016,
  title = {A Conditional {{Lindley}} Paradox in {{Bayesian}} Linear Models},
  author = {Som, Agniva and Hans, Christopher M and MacEachern, Steven N},
  year = {2016},
  month = dec,
  journal = {Biometrika},
  volume = {103},
  number = {4},
  pages = {993--999},
  langid = {english}
}

@phdthesis{som_paradoxes_2014,
  type = {Doctoral {{Thesis}}},
  title = {Paradoxes and Priors in {{Bayesian}} Regression},
  author = {Som, Agniva},
  year = {2014},
  school = {Ohio State University}
}

@article{somasundaram_wideband_2013,
  title = {Wideband Robust {{Capon}} Beamforming for Passive Sonar},
  author = {Somasundaram, Samuel D.},
  year = {2013},
  month = apr,
  journal = {IEEE Journal of Oceanic Engineering},
  volume = {38},
  number = {2},
  pages = {308--322}
}

@article{son_gpu_2021,
  title = {A {{GPU}} Scheduling Framework to Accelerate Hyper-Parameter Optimization in Deep Learning Clusters},
  author = {Son, Jaewon and Yoo, Yonghyuk and Kim, Khu-rai and Kim, Youngjae and Lee, Kwonyong and Park, Sungyong},
  year = {2021},
  month = feb,
  journal = {Electronics},
  volume = {10},
  number = {3},
  pages = {350},
  abstract = {This paper proposes Hermes, a container-based preemptive GPU scheduling framework for accelerating hyper-parameter optimization in deep learning (DL) clusters. Hermes accelerates hyper-parameter optimization by time-sharing between DL jobs and prioritizing jobs with more promising hyper-parameter combinations. Hermes's scheduling policy is grounded on the observation that good hyper-parameter combinations converge quickly in the early phases of training. By giving higher priority to fast-converging containers, Hermes's GPU preemption mechanism can accelerate training. This enables users to find optimal hyper-parameters faster without losing the progress of a container. We have implemented Hermes over Kubernetes and compared its performance against existing scheduling frameworks. Experiments show that Hermes reduces the time for hyper-parameter optimization up to 4.04 times against previously proposed scheduling policies such as FIFO, round-robin (RR), and SLAQ, with minimal time-sharing overhead.},
  langid = {english},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\SCZ8HI74\\Son et al. - 2021 - A GPU Scheduling Framework to Accelerate Hyper-Par.pdf}
}

@inproceedings{song_designing_2014,
  title = {Designing and Auto-Tuning Parallel 3-{{D FFT}} for Computation-Communication Overlap},
  booktitle = {Proceedings of the 19th {{ACM SIGPLAN}} Symposium on {{Principles}} and Practice of Parallel Programming - {{PPoPP}} '14},
  author = {Song, Sukhyun and Hollingsworth, Jeffrey K.},
  year = {2014},
  pages = {181--192},
  publisher = {{ACM Press}},
  address = {{Orlando, Florida, USA}},
  langid = {english}
}

@inproceedings{song_stochastic_2013,
  title = {Stochastic Gradient Descent with Differentially Private Updates},
  booktitle = {2013 {{IEEE Global Conference}} on {{Signal}} and {{Information Processing}}},
  author = {Song, Shuang and Chaudhuri, Kamalika and Sarwate, Anand D.},
  year = {2013},
  month = dec,
  pages = {245--248},
  abstract = {Differential privacy is a recent framework for computation on sensitive data, which has shown considerable promise in the regime of large datasets. Stochastic gradient methods are a popular approach for learning in the data-rich regime because they are computationally tractable and scalable. In this paper, we derive differentially private versions of stochastic gradient descent, and test them empirically. Our results show that standard SGD experiences high variability due to differential privacy, but a moderate increase in the batch size can improve performance significantly.},
  keywords = {Algorithm design and analysis,Data privacy,Linear programming,Logistics,Noise,Privacy,Signal processing algorithms},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\GNKVQXDP\\Song et al. - 2013 - Stochastic gradient descent with differentially pr.pdf}
}

@article{song_structure_2018,
  title = {Structure Adaptive Total Variation Minimization-Based Image Decomposition},
  author = {Song, Jinjoo and Cho, Heeryon and Yoon, Jungho and Yoon, Sang Min},
  year = {2018},
  month = sep,
  journal = {IEEE Transactions on Circuits and Systems for Video Technology},
  volume = {28},
  number = {9},
  pages = {2164--2176}
}

@article{south_sequential_2018,
  title = {Sequential {{Monte Carlo Samplers}} with {{Independent Markov Chain Monte Carlo Proposals}}},
  author = {South, Leah F and Pettitt, Anthony N and Drovandi, Christopher C and others},
  year = {2018},
  journal = {Bayesian Analysis},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\LKCDACSN\\South et al. - 2018 - Sequential Monte Carlo Samplers with Independent M.pdf}
}

@article{souza_priorguided_2020,
  title = {Prior-Guided {{Bayesian Optimization}}},
  author = {Souza, Artur and Nardi, Luigi and Oliveira, Leonardo B. and Olukotun, Kunle and Lindauer, Marius and Hutter, Frank},
  year = {2020},
  month = jun,
  journal = {arXiv:2006.14608 [cs, stat]},
  eprint = {2006.14608},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {While Bayesian Optimization (BO) is a very popular method for optimizing expensive black-box functions, it fails to leverage the experience of domain experts. This causes BO to waste function evaluations on commonly known bad regions of design choices, e.g., hyperparameters of a machine learning algorithm. To address this issue, we introduce Prior-guided Bayesian Optimization (PrBO). PrBO allows users to inject their knowledge into the optimization process in the form of priors about which parts of the input space will yield the best performance, rather than BO's standard priors over functions which are much less intuitive for users. PrBO then combines these priors with BO's standard probabilistic model to yield a posterior. We show that PrBO is more sample efficient than state-of-the-art methods without user priors and 10,000\$\textbackslash times\$ faster than random search, on a common suite of benchmarks and a real-world hardware design application. We also show that PrBO converges faster even if the user priors are not entirely accurate and that it robustly recovers from misleading priors.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\C4U9M6U6\\Souza et al. - 2020 - Prior-guided Bayesian Optimization.pdf;C\:\\Users\\msca8h\\Zotero\\storage\\LV5GTZQG\\2006.html}
}

@article{spall_implementation_1998,
  title = {Implementation of the Simultaneous Perturbation Algorithm for Stochastic Optimization},
  author = {Spall, J.C.},
  year = {1998},
  month = jul,
  journal = {IEEE Transactions on Aerospace and Electronic Systems},
  volume = {34},
  number = {3},
  pages = {817--823}
}

@article{spall_overview_1998,
  title = {An Overview of the Simultaneous Perturbation Method for Efficient Optimization},
  author = {Spall, James C},
  year = {1998},
  journal = {Johns Hopkins APL Technical Digest},
  volume = {19},
  number = {4},
  pages = {482--492}
}

@article{sparks_necessary_2015,
  title = {Necessary and Sufficient Conditions for High-Dimensional Posterior Consistency under g-Priors},
  author = {Sparks, Douglas K. and Khare, Kshitij and Ghosh, Malay},
  year = {2015},
  month = sep,
  journal = {Bayesian Analysis},
  volume = {10},
  number = {3},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\MXLBP4JC\\Sparks et al. - 2015 - Necessary and Sufficient Conditions for High-Dimen.pdf}
}

@article{spiegelhalter_bayesian_2002,
  title = {Bayesian Measures of Model Complexity and Fit},
  author = {Spiegelhalter, David J. and Best, Nicola G. and Carlin, Bradley P. and Van Der Linde, Angelika},
  year = {2002},
  journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
  volume = {64},
  number = {4},
  pages = {583--639},
  abstract = {Summary. We consider the problem of comparing complex hierarchical models in which the number of parameters is not clearly defined. Using an information theoretic argument we derive a measure pD for the effective number of parameters in a model as the difference between the posterior mean of the deviance and the deviance at the posterior means of the parameters of interest. In general pD approximately corresponds to the trace of the product of Fisher's information and the posterior covariance, which in normal models is the trace of the `hat' matrix projecting observations onto fitted values. Its properties in exponential families are explored. The posterior mean deviance is suggested as a Bayesian measure of fit or adequacy, and the contributions of individual observations to the fit and complexity can give rise to a diagnostic plot of deviance residuals against leverages. Adding pD to the posterior mean deviance gives a deviance information criterion for comparing models, which is related to other information criteria and has an approximate decision theoretic justification. The procedure is illustrated in some examples, and comparisons are drawn with alternative Bayesian and classical proposals. Throughout it is emphasized that the quantities required are trivial to compute in a Markov chain Monte Carlo analysis.},
  langid = {english},
  keywords = {Bayesian model comparison,Decision theory,Deviance information criterion,Effective number of parameters,Hierarchical models,Information theory,Leverage,Markov chain Monte Carlo methods,Model dimension},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/1467-9868.00353},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\XQJ9VBUE\\Spiegelhalter et al. - 2002 - Bayesian measures of model complexity and fit.pdf;C\:\\Users\\msca8h\\Zotero\\storage\\UJQJAC4V\\1467-9868.html}
}

@article{spiegelhalter1996bugs,
  title = {{{BUGS}}: {{Bayesian}} Inference Using Gibbs Sampling},
  author = {Spiegelhalter, David J and Thomas, Andrew and Best, Nicky G and Gilks, Wally and Lunn, D},
  year = {1996},
  journal = {Version 0.5,(version ii) http://www. mrc-bsu. cam. ac. uk/bugs},
  volume = {19}
}

@incollection{springenberg_bayesian_2016,
  title = {Bayesian {{Optimization}} with {{Robust Bayesian Neural Networks}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 29},
  author = {Springenberg, Jost Tobias and Klein, Aaron and Falkner, Stefan and Hutter, Frank},
  year = {2016},
  pages = {4134--4142},
  publisher = {{Curran Associates, Inc.}}
}

@inproceedings{srinivas_gaussian_2010,
  title = {Gaussian Process Optimization in the Bandit Setting: No Regret and Experimental Design},
  booktitle = {Proceedings of the {{International Conference}} on {{Machine Learning}}},
  author = {Srinivas, Niranjan and Krause, Andreas and Kakade, Sham and Seeger, Matthias},
  year = {2010},
  series = {{{PMLR}}},
  pages = {1015--1022},
  publisher = {{ML Research Press}},
  address = {{USA}},
  file = {/home/msca8h/Documents/bayesian_optimization/Srinivas et al. - 2010 - Gaussian Process Optimization in the Bandit Settin.pdf}
}

@article{srinivas_informationtheoretic_2012,
  title = {Information-Theoretic Regret Bounds for {{Gaussian}} Process Optimization in the Bandit Setting},
  author = {Srinivas, Niranjan and Krause, Andreas and Kakade, Sham M. and Seeger, Matthias W.},
  year = {2012},
  month = may,
  journal = {IEEE Transactions on Information Theory},
  volume = {58},
  number = {5},
  pages = {3250--3265},
  keywords = {Bandit problems,bandit setting,Bayesian methods,Bayesian prediction,Convergence,cumulative regret,experimental design,Gaussian process (GP),Gaussian process optimization,Gaussian processes,Hilbert spaces,information gain,information theory,information-theoretic regret bounds,intuitive Gaussian process upper confidence bound algorithm,Kernel,multiarmed bandit problem,Noise,nonparametric statistics,online learning,Optimization,payoff function,regret bound,reproducing kernel Hilbert space,statistical learning,sublinear regret bounds,Temperature sensors}
}

@inproceedings{stahl_noiseresistant_1999,
  title = {Noise-Resistant Weak-Structure Enhancement for Digital Radiography},
  booktitle = {Proc. {{SPIE Med}}. {{Imag}}.},
  author = {Stahl, Martin and Aach, Til and Buzug, Thorsten M. and Dippel, Sabine and Neitzel, Ulrich},
  year = {1999},
  month = may,
  pages = {1406--1417},
  address = {{San Diego, CA}}
}

@article{stahli_bayesian_2021,
  title = {Bayesian Approach for a Robust Speed-of-Sound Reconstruction Using Pulse-Echo Ultrasound},
  author = {Stahli, Patrick and Frenz, Martin and Jaeger, Michael},
  year = {2021},
  month = feb,
  journal = {IEEE Transactions on Medical Imaging},
  volume = {40},
  number = {2},
  pages = {457--467},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\KHHFDFM5\\Stahli et al. - 2021 - Bayesian Approach for a Robust Speed-of-Sound Reco.pdf}
}

@article{stan2020,
  title = {Stan Modeling Language Users Guide and Reference Manual, Version 2.23.0},
  author = {{Stan Development Team}},
  year = {2020}
}

@article{stein_bound_1972,
  title = {A Bound for the Error in the Normal Approximation to the Distribution of a Sum of Dependent Random Variables},
  author = {Stein, Charles},
  year = {1972},
  month = jan,
  journal = {Proceedings of the Sixth Berkeley Symposium on Mathematical Statistics and Probability, Volume 2: Probability Theory},
  volume = {6.2},
  pages = {583--603},
  publisher = {{University of California Press}},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\ANBQ8J32\\1200514239.html}
}

@article{stein_inadmissibility_1956,
  title = {Inadmissibility of the Usual Estimator for the Mean of a Multivariate Normal Distribution},
  author = {Stein, Charles},
  year = {1956},
  month = jan,
  journal = {Proceedings of the Third Berkeley Symposium on Mathematical Statistics and Probability, Volume 1: Contributions to the Theory of Statistics},
  volume = {3.1},
  pages = {197--207},
  publisher = {{University of California Press}},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\KWBI275T\\1200501656.html}
}

@article{stergiopoulos_implementation_1998,
  title = {Implementation of Adaptive and Synthetic-Aperture Processing Schemes in Integrated Active-Passive Sonar Systems},
  author = {Stergiopoulos, S.},
  year = {Feb./1998},
  journal = {Proceedings of the IEEE},
  volume = {86},
  number = {2},
  pages = {358--398}
}

@inproceedings{stich_local_2018,
  title = {Local {{SGD Converges Fast}} and {{Communicates Little}}},
  booktitle = {{{arXiv}}:1805.09767 [Cs, Math]},
  author = {Stich, Sebastian U.},
  year = {2018},
  month = may,
  eprint = {1805.09767},
  eprinttype = {arxiv},
  primaryclass = {cs, math},
  abstract = {Mini-batch stochastic gradient descent (SGD) is state of the art in large scale distributed training. The scheme can reach a linear speedup with respect to the number of workers, but this is rarely seen in practice as the scheme often suffers from large network delays and bandwidth limits. To overcome this communication bottleneck recent works propose to reduce the communication frequency. An algorithm of this type is local SGD that runs SGD independently in parallel on different workers and averages the sequences only once in a while. This scheme shows promising results in practice, but eluded thorough theoretical analysis. We prove concise convergence rates for local SGD on convex problems and show that it converges at the same rate as mini-batch SGD in terms of number of evaluated gradients, that is, the scheme achieves linear speedup in the number of workers and mini-batch size. The number of communication rounds can be reduced up to a factor of T\^\{1/2\}---where T denotes the number of total steps---compared to mini-batch SGD. This also holds for asynchronous implementations. Local SGD can also be used for large scale training of deep learning models. The results shown here aim serving as a guideline to further explore the theoretical and practical aspects of local SGD in these applications.},
  archiveprefix = {arXiv},
  keywords = {90C06; 68W40; 68W10,Computer Science - Distributed; Parallel; and Cluster Computing,Computer Science - Machine Learning,F.2.1,G.1.6,Mathematics - Optimization and Control},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\W8RQ38CV\\Stich - 2018 - Local SGD Converges Fast and Communicates Little.pdf;C\:\\Users\\msca8h\\Zotero\\storage\\WS8MI8Z6\\1805.html}
}

@misc{stich_unified_2019,
  title = {Unified Optimal Analysis of the (Stochastic) Gradient Method},
  author = {Stich, Sebastian U.},
  year = {2019},
  month = dec,
  number = {arXiv:1907.04232},
  eprint = {1907.04232},
  eprinttype = {arxiv},
  primaryclass = {cs, math, stat},
  institution = {{arXiv}},
  abstract = {In this note we give a simple proof for the convergence of stochastic gradient (SGD) methods on \$\textbackslash mu\$-convex functions under a (milder than standard) \$L\$-smoothness assumption. We show that for carefully chosen stepsizes SGD converges after \$T\$ iterations as \$O\textbackslash left( LR\^2 \textbackslash exp \textbackslash bigl[-\textbackslash frac\{\textbackslash mu\}\{4L\}T\textbackslash bigr] + \textbackslash frac\{\textbackslash sigma\^2\}\{\textbackslash mu T\} \textbackslash right)\$ where \$\textbackslash sigma\^2\$ measures the variance in the stochastic noise. For deterministic gradient descent (GD) and SGD in the interpolation setting we have \$\textbackslash sigma\^2 =0\$ and we recover the exponential convergence rate. The bound matches with the best known iteration complexity of GD and SGD, up to constants.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Mathematics - Numerical Analysis,Mathematics - Optimization and Control,Statistics - Machine Learning},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\MEUUU2TV\\Stich - 2019 - Unified Optimal Analysis of the (Stochastic) Gradi.pdf;C\:\\Users\\msca8h\\Zotero\\storage\\GF2GBADL\\1907.html}
}

@article{stoica_modelorder_2004,
  title = {Model-Order Selection},
  author = {Stoica, P. and Selen, Y.},
  year = {2004},
  month = jul,
  journal = {IEEE Signal Processing Magazine},
  volume = {21},
  number = {4},
  pages = {36--47},
  langid = {english}
}

@article{streijl_mean_2016,
  title = {Mean Opinion Score ({{MOS}}) Revisited: Methods and Applications, Limitations and Alternatives},
  shorttitle = {Mean Opinion Score ({{MOS}}) Revisited},
  author = {Streijl, Robert C. and Winkler, Stefan and Hands, David S.},
  year = {2016},
  month = mar,
  journal = {Multimedia Systems},
  volume = {22},
  number = {2},
  pages = {213--227},
  langid = {english}
}

@inproceedings{strens_evolutionary_2003,
  title = {Evolutionary {{MCMC Sampling}} and {{Optimization}} in {{Discrete Spaces}}},
  booktitle = {Proceedings of the {{Twentieth International Conference}} on {{International Conference}} on {{Machine Learning}}},
  author = {Strens, Malcolm J. A.},
  year = {2003},
  series = {{{ICML}}'03},
  pages = {736--743},
  publisher = {{AAAI Press}}
}

@article{strid_efficient_2010,
  title = {Efficient Parallelisation of {{Metropolis}}\textendash{{Hastings}} Algorithms Using a Prefetching Approach},
  author = {Strid, Ingvar},
  year = {2010},
  month = nov,
  journal = {Computational Statistics \& Data Analysis},
  volume = {54},
  number = {11},
  pages = {2814--2835},
  langid = {english}
}

@article{su_edgepreserving_2013,
  title = {Edge-Preserving Texture Suppression Filter Based on Joint Filtering Schemes},
  author = {Su, Zhuo and Luo, Xiaonan and Deng, Zhengjie and Liang, Yun and Ji, Zhen},
  year = {2013},
  month = apr,
  journal = {IEEE Transactions on Multimedia},
  volume = {15},
  number = {3},
  pages = {535--548}
}

@article{subr_edgepreserving_2009,
  title = {Edge-Preserving Multiscale Image Decomposition Based on Local Extrema},
  author = {Subr, Kartic and Soler, Cyril and Durand, Fr{\'e}do},
  year = {2009},
  month = dec,
  journal = {ACM Transactions on Graphics},
  volume = {28},
  number = {5},
  pages = {1--9},
  abstract = {We propose a new model for detail that inherently captures               oscillations               , a key property that distinguishes textures from individual edges. Inspired by techniques in empirical data analysis and morphological image analysis, we use the local extrema of the input image to extract information about oscillations: We define detail as oscillations between local minima and maxima. Building on the key observation that the spatial scale of oscillations are characterized by the density of local extrema, we develop an algorithm for decomposing images into multiple scales of superposed oscillations.                          Current edge-preserving image decompositions assume image detail to be low contrast variation. Consequently they apply filters that extract features with increasing contrast as successive layers of detail. As a result, they are unable to distinguish between high-contrast, fine-scale features and edges of similar contrast that are to be preserved. We compare our results with existing edge-preserving image decomposition algorithms and demonstrate exciting applications that are made possible by our new notion of detail.},
  langid = {english},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\UDY5HSPA\\Subr et al. - 2009 - Edge-preserving multiscale image decomposition bas.pdf}
}

@article{suchard_manycore_2009,
  title = {Many-Core Algorithms for Statistical Phylogenetics},
  author = {Suchard, M. A. and Rambaut, A.},
  year = {2009},
  month = jun,
  journal = {Bioinformatics},
  volume = {25},
  number = {11},
  pages = {1370--1376},
  langid = {english},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\DJJDRPJT\\Suchard and Rambaut - 2009 - Many-core algorithms for statistical phylogenetics.pdf}
}

@article{suchard_understanding_2010,
  title = {Understanding {{GPU}} Programming for Statistical Computation: Studies in Massively Parallel Massive Mixtures},
  shorttitle = {Understanding {{GPU Programming}} for {{Statistical Computation}}},
  author = {Suchard, Marc A. and Wang, Quanli and Chan, Cliburn and Frelinger, Jacob and Cron, Andrew and West, Mike},
  year = {2010},
  month = jan,
  journal = {Journal of Computational and Graphical Statistics},
  volume = {19},
  number = {2},
  pages = {419--438},
  langid = {english},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\THFFSHLW\\Suchard et al. - 2010 - Understanding GPU Programming for Statistical Comp.pdf}
}

@inproceedings{sukhija_portfoliobased_2014,
  title = {Portfolio-Based Selection of Robust Dynamic Loop Scheduling Algorithms Using Machine Learning},
  booktitle = {2014 {{IEEE International Parallel}} \& {{Distributed Processing Symposium Workshops}}},
  author = {Sukhija, Nitin and Malone, Brandon and Srivastava, Srishti and Banicescu, Ioana and Ciorba, Florina M.},
  year = {2014},
  month = may,
  pages = {1638--1647},
  publisher = {{IEEE}},
  address = {{Phoenix, AZ, USA}}
}

@article{sumiya_gaussian_2022,
  title = {Gaussian {{Fourier}} Pyramid for Local {{Laplacian}} Filter},
  author = {Sumiya, Yuto and Otsuka, Tomoki and Maeda, Yoshihiro and Fukushima, Norishige},
  year = {2022},
  journal = {IEEE Signal Processing Letters},
  volume = {29},
  pages = {11--15}
}

@article{sun_automated_2020,
  title = {Automated {{Performance Modeling}} of {{HPC Applications Using Machine Learning}}},
  author = {Sun, Jingwei and Sun, Guangzhong and Zhan, Shiyan and Zhang, Jiepeng and Chen, Yong},
  year = {2020},
  month = may,
  journal = {IEEE Transactions on Computers},
  volume = {69},
  number = {5},
  pages = {749--763}
}

@inproceedings{sun_new_2019,
  title = {New {{Interpretations}} of {{Normalization Methods}} in {{Deep Learning}}},
  author = {{sun}, jiacheng and Cao, Xiangyong and Liang, Hanwen and {huang}, weiran and {chen}, zewei and {li}, zhenguo},
  year = {2019},
  month = nov
}

@inproceedings{sun2018functional,
  title = {Functional Variational {{Bayesian}} Neural Networks},
  booktitle = {Proceedings of the {{International}} Conference on Learning Representations},
  author = {Sun, Shengyang and Zhang, Guodong and Shi, Jiaxin and Grosse, Roger},
  year = {2019}
}

@inproceedings{svensson_marginalizing_2015,
  title = {Marginalizing {{Gaussian}} Process Hyperparameters Using Sequential {{Monte Carlo}}},
  booktitle = {2015 {{IEEE}} 6th {{International Workshop}} on {{Computational Advances}} in {{Multi-Sensor Adaptive Processing}} ({{CAMSAP}})},
  author = {Svensson, Andreas and Dahlin, Johan and Schon, Thomas B.},
  year = {2015},
  month = dec,
  pages = {477--480},
  publisher = {{IEEE}},
  address = {{Cancun, Mexico}},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\UG6NUK9Y\\Svensson et al. - 2015 - Marginalizing Gaussian process hyperparameters usi.pdf}
}

@article{svensson_marginalizing_2015a,
  title = {Marginalizing {{Gaussian Process Hyperparameters}} Using {{Sequential Monte Carlo}}},
  author = {Svensson, Andreas and Dahlin, Johan and Sch{\"o}n, Thomas B.},
  year = {2015},
  month = dec,
  journal = {2015 IEEE 6th International Workshop on Computational Advances in Multi-Sensor Adaptive Processing (CAMSAP)},
  eprint = {1502.01908},
  eprinttype = {arxiv},
  pages = {477--480},
  abstract = {Gaussian process regression is a popular method for non-parametric probabilistic modeling of functions. The Gaussian process prior is characterized by so-called hyperparameters, which often have a large influence on the posterior model and can be difficult to tune. This work provides a method for numerical marginalization of the hyperparameters, relying on the rigorous framework of sequential Monte Carlo. Our method is well suited for online problems, and we demonstrate its ability to handle real-world problems with several dimensions and compare it to other marginalization methods. We also conclude that our proposed method is a competitive alternative to the commonly used point estimates maximizing the likelihood, both in terms of computational load and its ability to handle multimodal posteriors.},
  archiveprefix = {arXiv},
  keywords = {Statistics - Computation,Statistics - Machine Learning},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\SX4FSW2S\\Svensson et al. - 2015 - Marginalizing Gaussian Process Hyperparameters usi.pdf;C\:\\Users\\msca8h\\Zotero\\storage\\AE7HD5G9\\1502.html}
}

@article{svensson_posterior_2005,
  title = {On Posterior Distributions for Signals in {{Gaussian}} Noise with Unknown Covariance Matrix},
  author = {Svensson, L. and Lundberg, M.},
  year = {2005},
  month = sep,
  journal = {IEEE Transactions on Signal Processing},
  volume = {53},
  number = {9},
  pages = {3554--3571},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\JTWIE8YU\\Svensson and Lundberg - 2005 - On posterior distributions for signals in Gaussian.pdf}
}

@article{swersky_freezethaw_2014,
  title = {Freeze-Thaw {{Bayesian}} Optimization},
  author = {Swersky, Kevin and Snoek, Jasper and Adams, Ryan Prescott},
  year = {2014},
  month = jun,
  journal = {arXiv:1406.3896 [cs, stat]},
  eprint = {1406.3896},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {In this paper we develop a dynamic form of Bayesian optimization for machine learning models with the goal of rapidly finding good hyperparameter settings. Our method uses the partial information gained during the training of a machine learning model in order to decide whether to pause training and start a new model, or resume the training of a previously-considered model. We specifically tailor our method to machine learning problems by developing a novel positive-definite covariance kernel to capture a variety of training curves. Furthermore, we develop a Gaussian process prior that scales gracefully with additional temporal observations. Finally, we provide an information-theoretic framework to automate the decision process. Experiments on several common machine learning models show that our approach is extremely effective in practice.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\5FUNWCAA\\Swersky et al. - 2014 - Freeze-Thaw Bayesian Optimization.pdf;C\:\\Users\\msca8h\\Zotero\\storage\\XRYAQ93P\\1406.html}
}

@article{swingler_source_1989,
  title = {Source Location Bias in the Coherently Focused High-Resolution Broad-Band Beamformer},
  author = {Swingler, D.N. and Krolik, J.},
  year = {1989},
  month = jan,
  journal = {IEEE Transactions on Acoustics, Speech, and Signal Processing},
  volume = {37},
  number = {1},
  pages = {143--145},
  abstract = {A simple expression for the source located (angle of arrival) bias is developed for H. Wang and M. Kaveh's (ibid., vol.33, no.4, p. 823-31, 1985) focused broadband beamformer. It is shown to depend on the source temporal frequency spectrum only through its centroidal frequency. The bias is zero if the angle of arrival is aligned with the primary steering angle (i.e. focusing angle), or, more interestingly, if the source centroidal frequency equals the focusing frequency.{$<>$}},
  keywords = {Acoustic signal processing,Acoustical engineering,Array signal processing,Covariance matrix,Frequency estimation,Position measurement,Propagation delay,Sensor arrays,Speech,Technological innovation},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\HQNVG9DK\\Swingler and Krolik - 1989 - Source location bias in the coherently focused hig.pdf;C\:\\Users\\msca8h\\Zotero\\storage\\4KNVYFWE\\17516.html}
}

@article{synnes_wideband_2017,
  title = {Wideband Synthetic Aperture Sonar Backprojection with Maximization of Wave Number Domain Support},
  author = {Synnes, Stig Asle Vaksvik and Hunter, Alan Joseph and Hansen, Roy Edgar and Sabo, Torstein Olsmo and Callow, Hayden John and {van Vossen}, Robbert and Austeng, Andreas},
  year = {2017},
  month = oct,
  journal = {IEEE Journal of Oceanic Engineering},
  volume = {42},
  number = {4},
  pages = {880--891},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\2FUAGWIY\\Synnes et al. - 2017 - Wideband Synthetic Aperture Sonar Backprojection W.pdf}
}

@phdthesis{szasz_advanced_2016,
  type = {Theses},
  title = {Advanced Beamforming Techniques in Ultrasound Imaging and the Associated Inverse Problems},
  author = {Szasz, Teodora},
  year = {2016},
  month = oct,
  school = {Universit\'e Toulouse 3 Paul Sabatier (UT3 Paul Sabatier)},
  keywords = {beamforming,formation des voies,Imagerie ultrasonore mdicale,inverse problems,problmes inverses,ultrasound imaging}
}

@article{taasti_automating_2020,
  title = {Automating Proton Treatment Planning with Beam Angle Selection Using {{Bayesian}} Optimization},
  author = {Taasti, Vicki T. and Hong, Linda and Shim, Jin Sup(Andy) and Deasy, Joseph O. and Zarepisheh, Masoud},
  year = {2020},
  month = may,
  journal = {Medical Physics},
  pages = {mp.14215},
  langid = {english}
}

@inproceedings{tabatabaee_parallel_2005,
  title = {Parallel {{Parameter Tuning}} for {{Applications}} with {{Performance Variability}}},
  booktitle = {{{ACM}}/{{IEEE SC}} 2005 {{Conference}} ({{SC}}'05)},
  author = {Tabatabaee, V. and Tiwari, A. and Hollingsworth, J.K.},
  year = {2005},
  pages = {57--57},
  publisher = {{IEEE}},
  address = {{Seattle, WA, USA}}
}

@inproceedings{tabirca_feedback_2001,
  title = {Feedback Guided Dynamic Loop Scheduling; {{A}} Theoretical Approach},
  booktitle = {Proceedings {{International Conference}} on {{Parallel Processing Workshops}}},
  author = {Tabirca, T. and Freeman, L. and Tabirca, S. and Yang, L.T.},
  year = {2001},
  pages = {115--121},
  publisher = {{IEEE Comput. Soc}},
  address = {{Valencia, Spain}},
  file = {/home/msca8h/Documents/parallel_scheduling/06882d3232d6c887834fa9027a5cd25055d9.pdf;C\:\\Users\\msca8h\\Zotero\\storage\\4ZVCP797\\Tabirca et al. - 2001 - Feedback guided dynamic loop scheduling\; A theoret.pdf}
}

@article{tadic_asymptotic_2017,
  title = {Asymptotic Bias of Stochastic Gradient Search},
  author = {Tadi{\'c}, Vladislav B. and Doucet, Arnaud},
  year = {2017},
  month = dec,
  journal = {The Annals of Applied Probability},
  volume = {27},
  number = {6},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\IQPGSRET\\Tadi and Doucet - 2017 - Asymptotic bias of stochastic gradient search.pdf}
}

@article{talebi_fast_2016,
  title = {Fast Multilayer {{Laplacian}} Enhancement},
  author = {Talebi, Hossein and Milanfar, Peyman},
  year = {2016},
  month = dec,
  journal = {IEEE Transactions on Computational Imaging},
  volume = {2},
  number = {4},
  pages = {496--509},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\ZIII93BK\\Talebi and Milanfar - 2016 - Fast Multilayer Laplacian Enhancement.pdf}
}

@article{tan_monte_2006,
  title = {Monte {{Carlo}} Integration with Acceptance-Rejection},
  author = {Tan, Zhiqiang},
  year = {2006},
  month = sep,
  journal = {Journal of Computational and Graphical Statistics},
  volume = {15},
  number = {3},
  pages = {735--752},
  langid = {english}
}

@inproceedings{tang_highthroughput_2020,
  title = {A High-Throughput Solver for Marginalized Graph Kernels on {{GPU}}},
  booktitle = {{{IEEE International Parallel}} and {{Distributed Processing}}},
  author = {Tang, Yu-Hang and Selvitopi, Oguz and Popovici, Doru Thom and Buluc, Aydin},
  year = {2020},
  month = may,
  pages = {728--738},
  publisher = {{IEEE}},
  address = {{New Orleans, LA, USA}},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\FFQA53EH\\Tang et al. - 2020 - A High-Throughput Solver for Marginalized Graph Ke.pdf}
}

@inproceedings{tang_processor_1986,
  title = {{Processor self-scheduling for multiple-nested parallel loops}},
  booktitle = {{Proc. Int. Conf. Parallel Process.}},
  author = {Tang, Peiyi and Yew, Pen Chung},
  year = {1986},
  month = dec,
  series = {{ICPP'86}},
  pages = {528--535},
  publisher = {{IEEE}},
  abstract = {Processor self-scheduling is a useful scheme in a multiprocessor system if the execution time of each iteration in a parallel loop is not known in advance and varies substantially, or if there are multiple nestings in parallel loops which makes static scheduling difficult and inefficient. By using efficient synchronization primitives, the operating system is not needed for loop scheduling. The overhead for the processor self-scheduling is small. A self-scheduling scheme is presented for a single-nested parallel loop and is extended to multiple-nested parallel loops. Barrier synchronization mechanisms for the self-scheduling schemes are also discussed.},
  langid = {English (US)}
}

@inproceedings{tank_streaming_2015,
  title = {Streaming Variational Inference for {{Bayesian}} Nonparametric Mixture Models},
  booktitle = {Proceedings of the {{Eighteenth International Conference}} on {{Artificial Intelligence}} and {{Statistics}}},
  author = {Tank, Alex and Foti, Nicholas and Fox, Emily},
  year = {2015},
  month = feb,
  pages = {968--976},
  publisher = {{PMLR}},
  abstract = {In theory, Bayesian nonparametric (BNP) models are well suited to streaming data scenarios due to their ability to adapt model complexity based on the amount of data observed. Unfortunately, such benefits have not been fully realized in practice; existing inference algorithms either are not applicable to streaming applications or are not extensible to nonparametric models.  For the special case of Dirichlet processes, streaming inference has been considered.  However, there is growing interest in more flexible BNP models, in particular building on the class of normalized random measures (NRMs).  We work within this general framework and present a streaming variational inference algorithm for NRM mixture models based on assumed density filtering.  Extensions to expectation propagation algorithms are possible in the batch data setting.  We demonstrate the efficacy of the algorithm on clustering documents in large, streaming text corpora.},
  langid = {english},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\K3YN3V4Y\\Tank et al. - 2015 - Streaming Variational Inference for Bayesian Nonpa.pdf}
}

@inproceedings{tapus_active_2002,
  title = {Active {{Harmony}}: {{Towards Automated Performance Tuning}}},
  shorttitle = {Active {{Harmony}}},
  booktitle = {{{ACM}}/{{IEEE SC}} 2002 {{Conference}} ({{SC}}'02)},
  author = {Tapus, C. and {I-Hsin Chung} and Hollingsworth, J.K.},
  year = {2002},
  pages = {44--44},
  publisher = {{IEEE}},
  address = {{Baltimore, MD, USA}}
}

@inproceedings{tay_ultrasound_2006,
  title = {Ultrasound Despeckling Using an Adaptive Window Stochastic Approach},
  booktitle = {Proc. {{Int}}. {{Conf}}. {{Image Process}}.},
  author = {Tay, Peter C. and Acton, Scott T. and Hossack, John A.},
  year = {2006},
  month = oct,
  pages = {2549--2552},
  publisher = {{IEEE}},
  address = {{Atlanta, GA}}
}

@article{tensorforce,
  title = {Tensorforce: A {{TensorFlow}} Library for Applied Reinforcement Learning},
  author = {Kuhnle, Alexander and Schaarschmidt, Michael and Fricke, Kai},
  year = {2017},
  howpublished = {Web page}
}

@article{terenin_asynchronous_2020,
  title = {Asynchronous {{Gibbs Sampling}}},
  author = {Terenin, Alexander and Simpson, Daniel and Draper, David},
  year = {2020},
  month = feb,
  journal = {arXiv:1509.08999 [stat]},
  eprint = {1509.08999},
  eprinttype = {arxiv},
  primaryclass = {stat},
  abstract = {Gibbs sampling is a Markov Chain Monte Carlo (MCMC) method often used in Bayesian learning. MCMC methods can be difficult to deploy on parallel and distributed systems due to their inherently sequential nature. We study asynchronous Gibbs sampling, which achieves parallelism by simply ignoring sequential requirements. This method has been shown to produce good empirical results for some hierarchical models, and is popular in the topic modeling community, but was also shown to diverge for other targets. We introduce a theoretical framework for analyzing asynchronous Gibbs sampling and other extensions of MCMC that do not possess the Markov property. We prove that asynchronous Gibbs can be modified so that it converges under appropriate regularity conditions -- we call this the exact asynchronous Gibbs algorithm. We study asynchronous Gibbs on a set of examples by comparing the exact and approximate algorithms, including two where it works well, and one where it fails dramatically. We conclude with a set of heuristics to describe settings where the algorithm can be effectively used.},
  archiveprefix = {arXiv},
  keywords = {Statistics - Computation},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\8ED8XA7L\\Terenin et al. - 2020 - Asynchronous Gibbs Sampling.pdf;C\:\\Users\\msca8h\\Zotero\\storage\\ARKY94I4\\1509.html}
}

@article{thawornwattana_designing_2018,
  title = {Designing {{Simple}} and {{Efficient Markov Chain Monte Carlo Proposal Kernels}}},
  author = {Thawornwattana, Yuttapong and Dalquen, Daniel and Yang, Ziheng},
  year = {2018},
  month = dec,
  journal = {Bayesian Analysis},
  volume = {13},
  number = {4},
  pages = {1037--1063},
  langid = {english},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\MZHXDNJQ\\Thawornwattana et al. - 2018 - Designing Simple and Efficient Markov Chain Monte .pdf}
}

@mastersthesis{theologitis_bayesian_2016,
  title = {Bayesian Estimation of Optimal Portfolio: {{Theory}} and Practice},
  author = {Theologitis, Theodoros},
  year = {2016},
  school = {Stockholm University},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\SZLTBXZM\\Theologitis - 2016 - Bayesian estimation of optimal portfolio Theory a.pdf}
}

@article{thijssen_texture_1990,
  title = {Texture in Tissue Echograms. {{Speckle}} or Information?},
  author = {Thijssen, J. M. and Oosterveld, B. J.},
  year = {1990},
  month = apr,
  journal = {Journal of Ultrasound in Medicine},
  volume = {9},
  number = {4},
  pages = {215--229},
  langid = {english}
}

@inproceedings{thin_monte_2021,
  title = {Monte {{Carlo}} Variational Auto-Encoders},
  booktitle = {Proceedings of the 38th {{International Conference}} on {{Machine Learning}}},
  author = {Thin, Achille and Kotelevskii, Nikita and Doucet, Arnaud and Durmus, Alain and Moulines, Eric and Panov, Maxim},
  year = {2021},
  month = jul,
  pages = {10247--10257},
  publisher = {{PMLR}},
  abstract = {Variational auto-encoders (VAE) are popular deep latent variable models which are trained by maximizing an Evidence Lower Bound (ELBO). To obtain tighter ELBO and hence better variational approximations, it has been proposed to use importance sampling to get a lower variance estimate of the evidence. However, importance sampling is known to perform poorly in high dimensions. While it has been suggested many times in the literature to use more sophisticated algorithms such as Annealed Importance Sampling (AIS) and its Sequential Importance Sampling (SIS) extensions, the potential benefits brought by these advanced techniques have never been realized for VAE: the AIS estimate cannot be easily differentiated, while SIS requires the specification of carefully chosen backward Markov kernels. In this paper, we address both issues and demonstrate the performance of the resulting Monte Carlo VAEs on a variety of applications.},
  langid = {english},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\TGQB2RZS\\Thin et al. - 2021 - Monte Carlo Variational Auto-Encoders.pdf;C\:\\Users\\msca8h\\Zotero\\storage\\TSKF4D2Z\\Thin et al. - 2021 - Monte Carlo Variational Auto-Encoders.pdf}
}

@article{thomas_coherenceinduced_2021,
  title = {Coherence-{{Induced Bias Reduction}} in {{Synthetic Aperture Sonar Along-Track Micronavigation}}},
  author = {Thomas, Benjamin and Hunter, Alan},
  year = {2021},
  journal = {IEEE Journal of Oceanic Engineering},
  pages = {1--17}
}

@phdthesis{thomas_phase_2020,
  type = {Doctoral {{Thesis}}},
  title = {Phase Preserving {{3D}} Micro-Navigation for Interferometric Synthetic Aperture Sonar},
  author = {Thomas, Benjamin},
  year = {2020},
  school = {University of Bath},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\RI9UDZRE\\Thomas - 2020 - Phase preserving 3D micro-navigation for interfero.pdf}
}

@article{thomasson_estimating_1968,
  title = {On Estimating the Parameter of a Truncated Geometric Distribution},
  author = {Thomasson, R. L. and Kapadia, C. H.},
  year = {1968},
  month = dec,
  journal = {Annals of the Institute of Statistical Mathematics},
  volume = {20},
  number = {1},
  pages = {519--523},
  langid = {english}
}

@article{thompson_decline_2018,
  title = {The {{Decline}} of {{Computers As}} a {{General Purpose Technology}}: {{Why Deep Learning}} and the {{End}} of {{Moore}}'s {{Law}} Are {{Fragmenting Computing}}},
  shorttitle = {The {{Decline}} of {{Computers As}} a {{General Purpose Technology}}},
  author = {Thompson, Neil and Spanuth, Svenja},
  year = {2018},
  journal = {SSRN Electronic Journal},
  langid = {english}
}

@article{tibbits_automated_2014,
  title = {Automated Factor Slice Sampling},
  author = {Tibbits, Matthew M. and Groendyke, Chris and Haran, Murali and Liechty, John C.},
  year = {2014},
  month = apr,
  journal = {Journal of Computational and Graphical Statistics},
  volume = {23},
  number = {2},
  pages = {543--563},
  langid = {english},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\JF4HMQUQ\\Tibbits et al. - 2014 - Automated Factor Slice Sampling.pdf}
}

@misc{tieleman_lecture_2012,
  title = {Lecture 6.5-Rmsprop: {{Divide}} the Gradient by a Running Average of Its Recent Magnitude},
  author = {Tieleman, Tijmen and Hinton},
  year = {2012},
  address = {{COURSERA: Neural Networks for Machine Learning}}
}

@inproceedings{titsias_doubly_2014,
  title = {Doubly Stochastic Variational {{Bayes}} for Non-Conjugate Inference},
  booktitle = {Proceedings of the {{International Conference}} on {{Machine Learning}}},
  author = {Titsias, Michalis and {L{\'a}zaro-Gredilla}, Miguel},
  year = {2014},
  month = jun,
  series = {{{PMLR}}},
  volume = {32},
  pages = {1971--1979},
  publisher = {{ML Research Press}},
  abstract = {We propose a simple and effective variational inference algorithm based on stochastic optimisation   that can be widely applied for Bayesian non-conjugate inference in continuous parameter spaces. This algorithm is based on stochastic approximation and allows for efficient use of gradient information from the model joint density. We demonstrate these properties using illustrative examples as well as in challenging and diverse Bayesian inference   problems such as variable selection in logistic regression and fully   Bayesian inference over kernel hyperparameters in Gaussian process regression.},
  langid = {english},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\IQ558HYA\\Titsias and Lzaro-Gredilla - 2014 - Doubly Stochastic Variational Bayes for non-Conjug.pdf}
}

@incollection{titsiasrcaueb_variational_2013,
  title = {Variational {{Inference}} for {{Mahalanobis Distance Metrics}} in {{Gaussian Process Regression}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 26},
  author = {Titsias RC AUEB, Michalis and {Lazaro-Gredilla}, Miguel},
  year = {2013},
  pages = {279--287},
  publisher = {{Curran Associates, Inc.}}
}

@article{tiwari_autotuning_2011,
  title = {Auto-Tuning Full Applications: {{A}} Case Study},
  shorttitle = {Auto-Tuning Full Applications},
  author = {Tiwari, Ananta and Hollingsworth, Jeffrey K and {Chun Chen} and Hall, Mary and {Chunhua Liao} and Quinlan, Daniel J and Chame, Jacqueline},
  year = {2011},
  month = aug,
  journal = {The International Journal of High Performance Computing Applications},
  volume = {25},
  number = {3},
  pages = {286--294},
  langid = {english},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\ITK2RPSD\\Tiwari et al. - 2011 - Auto-tuning full applications A case study.pdf}
}

@inproceedings{tiwari_scalable_2009,
  title = {A Scalable Auto-Tuning Framework for Compiler Optimization},
  booktitle = {2009 {{IEEE International Symposium}} on {{Parallel}} \& {{Distributed Processing}}},
  author = {Tiwari, Ananta and Chen, Chun and Chame, Jacqueline and Hall, Mary and Hollingsworth, Jeffrey K.},
  year = {2009},
  month = may,
  pages = {1--12},
  publisher = {{IEEE}},
  address = {{Rome, Italy}},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\A29KRNU9\\Tiwari et al. - 2009 - A scalable auto-tuning framework for compiler opti.pdf}
}

@article{topol_highperformance_2019,
  title = {High-Performance Medicine: The Convergence of Human and Artificial Intelligence},
  shorttitle = {High-Performance Medicine},
  author = {Topol, Eric J.},
  year = {2019},
  month = jan,
  journal = {Nature Medicine},
  volume = {25},
  number = {1},
  pages = {44--56},
  langid = {english}
}

@article{torralba_statistics_2003,
  title = {Statistics of Natural Image Categories},
  author = {Torralba, Antonio and Oliva, Aude},
  year = {2003},
  month = jan,
  journal = {Network: Computation in Neural Systems},
  volume = {14},
  number = {3},
  pages = {391--412},
  langid = {english},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\99LNCRT2\\Torralba and Oliva - 2003 - Statistics of natural image categories.pdf}
}

@article{tory_human_2004,
  title = {Human Factors in Visualization Research},
  author = {Tory, M. and Moller, T.},
  year = {2004},
  month = jan,
  journal = {IEEE Transactions on Visualization and Computer Graphics},
  volume = {10},
  number = {1},
  pages = {72--84},
  langid = {english}
}

@inproceedings{tracey_upgrading_2018,
  title = {Upgrading from {{Gaussian}} Processes to {{Student}}'st Processes},
  booktitle = {2018 {{AIAA Non-Deterministic Approaches Conference}}},
  author = {Tracey, Brendan D and Wolpert, David},
  year = {2018},
  pages = {1659}
}

@article{tran_model_2021,
  title = {Model Selection for {{Bayesian}} Autoencoders},
  author = {Tran, Ba-Hien and Rossi, Simone and Milios, Dimitrios and Michiardi, Pietro and Bonilla, Edwin V. and Filippone, Maurizio},
  year = {2021},
  month = jun,
  journal = {arXiv:2106.06245 [cs, stat]},
  eprint = {2106.06245},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {We develop a novel method for carrying out model selection for Bayesian autoencoders (BAEs) by means of prior hyper-parameter optimization. Inspired by the common practice of type-II maximum likelihood optimization and its equivalence to Kullback-Leibler divergence minimization, we propose to optimize the distributional sliced-Wasserstein distance (DSWD) between the output of the autoencoder and the empirical data distribution. The advantages of this formulation are that we can estimate the DSWD based on samples and handle high-dimensional problems. We carry out posterior estimation of the BAE parameters via stochastic gradient Hamiltonian Monte Carlo and turn our BAE into a generative model by fitting a flexible Dirichlet mixture model in the latent space. Consequently, we obtain a powerful alternative to variational autoencoders, which are the preferred choice in modern applications of autoencoders for representation learning with uncertainty. We evaluate our approach qualitatively and quantitatively using a vast experimental campaign on a number of unsupervised learning tasks and show that, in small-data regimes where priors matter, our approach provides state-of-the-art results, outperforming multiple competitive baselines.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\C5PAT2E6\\Tran et al. - 2021 - Model Selection for Bayesian Autoencoders.pdf;C\:\\Users\\msca8h\\Zotero\\storage\\I6PPVPIX\\2106.html}
}

@article{tran2016edward,
  title = {Edward: {{A}} Library for Probabilistic Modeling, Inference, and Criticism},
  author = {Tran, Dustin and Kucukelbir, Alp and Dieng, Adji B. and Rudolph, Maja and Liang, Dawen and Blei, David M.},
  year = {2016},
  journal = {arXiv preprint arXiv:1610.09787},
  eprint = {1610.09787},
  eprinttype = {arxiv},
  archiveprefix = {arXiv}
}

@article{tran2016edward,
  title = {Edward: {{A}} Library for Probabilistic Modeling, Inference, and Criticism},
  author = {Tran, Dustin and Kucukelbir, Alp and Dieng, Adji B. and Rudolph, Maja and Liang, Dawen and Blei, David M.},
  year = {2016},
  journal = {arXiv preprint arXiv:1610.09787},
  eprint = {1610.09787},
  eprinttype = {arxiv},
  archiveprefix = {arXiv}
}

@techreport{trippe_overpruning_2017,
  title = {Overpruning in Variational {{Bayesian}} Neural Networks},
  author = {Trippe, Brian and Turner, Richard},
  year = {2017},
  number = {arXiv:1801.06230},
  institution = {{ArXiv}}
}

@article{tseng_incremental_1998,
  title = {An Incremental Gradient(-Projection) Method with Momentum Term and Adaptive Stepsize Rule},
  author = {Tseng, Paul},
  year = {1998},
  month = may,
  journal = {SIAM Journal on Optimization},
  volume = {8},
  number = {2},
  pages = {506--531},
  abstract = {We consider an incremental gradient method with momentum term for minimizing the sum of continuously differentiable functions. This method uses a new adaptive stepsize rule that decreases the stepsize whenever sufficient progress is not made. We show that if the gradients of the functions are bounded and Lipschitz continuous over a certain level set, then every cluster point of the iterates generated by the method is a stationary point. In addition, if the gradient of the functions have a certain growth property, then the method is either linearly convergent in some sense or the stepsizes are bounded away from zero. The new stepsize rule is much in the spirit of heuristic learning rules used in practice for training neural networks via backpropagation. As such, the new stepsize rule may suggest improvements on existing learning rules. Finally, extension of the method and the convergence results to constrained minimization is discussed, as are some implementation issues and numerical experience.},
  langid = {english},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\36U5JVYC\\Tseng - 1998 - An Incremental Gradient(-Projection) Method with M.pdf}
}

@inproceedings{tumblin_lcis_1999,
  title = {{{LCIS}}: {{A}} Boundary Hierarchy for Detail-Preserving Contrast Reduction},
  shorttitle = {{{LCIS}}},
  booktitle = {Proceedings of the 26th Annual Conference on {{Computer}} Graphics and Interactive Techniques  - {{SIGGRAPH}} '99},
  author = {Tumblin, Jack and Turk, Greg},
  year = {1999},
  pages = {83--90},
  publisher = {{ACM Press}},
  address = {{Not Known}},
  langid = {english}
}

@article{turner_how_2017,
  title = {How Well Does Your Sampler Really Work?},
  author = {Turner, Ryan and Neal, Brady},
  year = {2017},
  month = dec,
  journal = {arXiv:1712.06006 [stat]},
  eprint = {1712.06006},
  eprinttype = {arxiv},
  primaryclass = {stat},
  abstract = {We present a new data-driven benchmark system to evaluate the performance of new MCMC samplers. Taking inspiration from the COCO benchmark in optimization, we view this task as having critical importance to machine learning and statistics given the rate at which new samplers are proposed. The common hand-crafted examples to test new samplers are unsatisfactory; we take a meta-learning-like approach to generate benchmark examples from a large corpus of data sets and models. Surrogates of posteriors found in real problems are created using highly flexible density models including modern neural network based approaches. We provide new insights into the real effective sample size of various samplers per unit time and the estimation efficiency of the samplers per sample. Additionally, we provide a meta-analysis to assess the predictive utility of various MCMC diagnostics and perform a nonparametric regression to combine them.},
  archiveprefix = {arXiv},
  keywords = {Statistics - Computation,Statistics - Machine Learning},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\QRBDPF97\\Turner and Neal - 2017 - How well does your sampler really work.pdf;C\:\\Users\\msca8h\\Zotero\\storage\\NZSGUA3H\\1712.html}
}

@article{tzen_trapezoid_1993,
  title = {Trapezoid Self-Scheduling: A Practical Scheduling Scheme for Parallel Compilers},
  author = {Tzen, T. H. and Ni, L. M.},
  year = {1993},
  month = jan,
  journal = {IEEE Transactions on Parallel and Distributed Systems},
  volume = {4},
  number = {1},
  pages = {87--98},
  keywords = {Butterfly GP-1000,chunk size,Computer science,dynamic allocation,Dynamic scheduling,load balancing,Load management,loop iterations,memory management,Memory management,Multiprocessing systems,parallel compilers,Parallel languages,parallel nested loops,Parallel processing,parallel programming,parallel programs,Processor scheduling,processor self-scheduling,program compilers,Programming profession,run-time scheduling overhead,Runtime,scheduling,shared memory systems,shared-memory multiprocessors,trapezoid self-scheduling},
  file = {/home/msca8h/Documents/parallel_scheduling/Tzen and Ni - 1993 - Trapezoid self-scheduling a practical scheduling .pdf}
}

@article{ulyanov_instance_2017,
  title = {Instance {{Normalization}}: {{The Missing Ingredient}} for {{Fast Stylization}}},
  shorttitle = {Instance {{Normalization}}},
  author = {Ulyanov, Dmitry and Vedaldi, Andrea and Lempitsky, Victor},
  year = {2017},
  month = nov,
  journal = {arXiv:1607.08022 [cs]},
  eprint = {1607.08022},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {It this paper we revisit the fast stylization method introduced in Ulyanov et. al. (2016). We show how a small change in the stylization architecture results in a significant qualitative improvement in the generated images. The change is limited to swapping batch normalization with instance normalization, and to apply the latter both at training and testing times. The resulting method can be used to train high-performance architectures for real-time image generation. The code will is made available on github at https://github.com/DmitryUlyanov/texture\_nets. Full paper can be found at arXiv:1701.02096.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\HRP7WMIT\\Ulyanov et al. - 2017 - Instance Normalization The Missing Ingredient for.pdf;C\:\\Users\\msca8h\\Zotero\\storage\\SGFGJDB5\\1607.html}
}

@article{uribe_optimal_2017,
  title = {Optimal {{Algorithms}} for {{Distributed Optimization}}},
  author = {Uribe, C{\'e}sar A. and Lee, Soomin and Gasnikov, Alexander and Nedi{\'c}, Angelia},
  year = {2017},
  month = dec,
  journal = {arXiv:1712.00232 [cs, math, stat]},
  eprint = {1712.00232},
  eprinttype = {arxiv},
  primaryclass = {cs, math, stat},
  abstract = {In this paper, we study the optimal convergence rate for distributed convex optimization problems in networks. We model the communication restrictions imposed by the network as a set of affine constraints and provide optimal complexity bounds for four different setups, namely: the function \$F(\textbackslash xb) \textbackslash triangleq \textbackslash sum\_\{i=1\}\^\{m\}f\_i(\textbackslash xb)\$ is strongly convex and smooth, either strongly convex or smooth or just convex. Our results show that Nesterov's accelerated gradient descent on the dual problem can be executed in a distributed manner and obtains the same optimal rates as in the centralized version of the problem (up to constant or logarithmic factors) with an additional cost related to the spectral gap of the interaction matrix. Finally, we discuss some extensions to the proposed setup such as proximal friendly functions, time-varying graphs, improvement of the condition numbers.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computational Complexity,Computer Science - Multiagent Systems,Electrical Engineering and Systems Science - Systems and Control,Mathematics - Optimization and Control,Statistics - Machine Learning},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\96HMKSYN\\Uribe et al. - 2017 - Optimal Algorithms for Distributed Optimization.pdf;C\:\\Users\\msca8h\\Zotero\\storage\\MV9KKICE\\1712.html}
}

@article{valaee_optimal_1996,
  title = {The Optimal Focusing Subspace for Coherent Signal Subspace Processing},
  author = {Valaee, S. and Kabal, P.},
  year = {1996},
  month = mar,
  journal = {IEEE Transactions on Signal Processing},
  volume = {44},
  number = {3},
  pages = {752--756},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\9REE66VB\\Valaee and Kabal - 1996 - The optimal focusing subspace for coherent signal .pdf}
}

@article{valaee_wideband_1995b,
  title = {Wideband Array Processing Using a Two-Sided Correlation Transformation},
  author = {Valaee, S. and Kabal, P.},
  year = {Jan./1995},
  journal = {IEEE Transactions on Signal Processing},
  volume = {43},
  number = {1},
  pages = {160--172},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\ZIR3THHP\\Valaee and Kabal - 1995 - Wideband array processing using a two-sided correl.pdf}
}

@inproceedings{vanaken_automatic_2017,
  title = {Automatic {{Database Management System Tuning Through Large-scale Machine Learning}}},
  booktitle = {Proceedings of the 2017 {{ACM International Conference}} on {{Management}} of {{Data}} - {{SIGMOD}} '17},
  author = {Van Aken, Dana and Pavlo, Andrew and Gordon, Geoffrey J. and Zhang, Bohan},
  year = {2017},
  pages = {1009--1024},
  publisher = {{ACM Press}},
  address = {{Chicago, Illinois, USA}},
  langid = {english},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\34HRAM9E\\Van Aken et al. - 2017 - Automatic Database Management System Tuning Throug.pdf}
}

@article{vandenberg_influence_2017,
  title = {Influence of {{Bayesian}} Optimization on the Performance of Propofol Target-Controlled Infusion},
  author = {{van den Berg}, J.P. and Eleveld, D.J. and De Smet, T. and {van den Heerik}, A.V.M. and {van Amsterdam}, K. and Lichtenbelt, B.J. and Scheeren, T.W.L. and Absalom, A.R. and Struys, M M R F},
  year = {2017},
  month = nov,
  journal = {British Journal of Anaesthesia},
  volume = {119},
  number = {5},
  pages = {918--927},
  langid = {english}
}

@mastersthesis{vanderveken_bayesian_2019,
  title = {Bayesian Approach to Portfolio Selection},
  author = {Vanderveken, Rodolphe},
  year = {2019},
  school = {Louvain School of Management},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\EKYV4LJV\\Vanderveken - 2019 - Bayesian approach to portfolio selection.pdf}
}

@article{vanderwerken_parallel_2013,
  title = {Parallel {{Markov Chain Monte Carlo}}},
  author = {VanDerwerken, Douglas N. and Schmidler, Scott C.},
  year = {2013},
  month = dec,
  journal = {arXiv:1312.7479 [stat]},
  eprint = {1312.7479},
  eprinttype = {arxiv},
  primaryclass = {stat},
  abstract = {Markov chain Monte Carlo is an inherently serial algorithm. Although likelihood calculations for individual steps can sometimes be parallelized, the serial evolution of the process is widely viewed as incompatible with parallelization, offering no speedup for samplers which require large numbers of iterations to converge to equilibrium. We provide a methodology for parallelizing Markov chain Monte Carlo across large numbers of independent, asynchronous processors. Our approach uses a partitioning and weight estimation scheme to combine independent simulations run on separate processors into rigorous Monte Carlo estimates. The method is originally motivated by sampling multimodal target distributions, where we see an exponential speedup in running time. However we show that the approach is general-purpose and applicable to all Markov chain Monte Carlo simulations, and demonstrate speedups proportional to the number of available processors on slowly mixing chains with unimodal target distributions. The approach is simple and easy to implement, and suggests additional directions for further research.},
  archiveprefix = {arXiv},
  keywords = {Statistics - Computation},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\B3WFTVDR\\VanDerwerken and Schmidler - 2013 - Parallel Markov Chain Monte Carlo.pdf;C\:\\Users\\msca8h\\Zotero\\storage\\ZMI8GW89\\1312.html}
}

@article{vanli_global_2018,
  title = {Global {{Convergence Rate}} of {{Proximal Incremental Aggregated Gradient Methods}}},
  author = {Vanli, N. D. and G{\"u}rb{\"u}zbalaban, M. and Ozdaglar, A.},
  year = {2018},
  month = jan,
  journal = {SIAM Journal on Optimization},
  volume = {28},
  number = {2},
  pages = {1282--1300},
  langid = {english},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\VWMLH6PX\\Vanli et al. - 2018 - Global Convergence Rate of Proximal Incremental Ag.pdf}
}

@book{vantrees_optimum_2002,
  title = {Optimum Array Processing},
  shorttitle = {Detection, Estimation, and Modulation Theory. 4},
  author = {Van Trees, Harry L.},
  year = {2002},
  series = {Detection, Estimation, and Modulation Theory},
  number = {4},
  publisher = {{Wiley}},
  address = {{New York, NY}},
  langid = {english}
}

@article{vanveen_beamforming_1988,
  title = {Beamforming: {{A}} Versatile Approach to Spatial Filtering},
  shorttitle = {Beamforming},
  author = {Van Veen, B.D. and Buckley, K.M.},
  year = {1988},
  month = apr,
  journal = {IEEE ASSP Magazine},
  volume = {5},
  number = {2},
  pages = {4--24}
}

@article{vanwerkhoven_kernel_2019,
  title = {Kernel {{Tuner}}: {{A}} Search-Optimizing {{GPU}} Code Auto-Tuner},
  shorttitle = {Kernel {{Tuner}}},
  author = {{van Werkhoven}, Ben},
  year = {2019},
  month = jan,
  journal = {Future Generation Computer Systems},
  volume = {90},
  pages = {347--358},
  langid = {english},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\XG4X8N6W\\van Werkhoven - 2019 - Kernel Tuner A search-optimizing GPU code auto-tu.pdf}
}

@article{varsi_single_2019,
  title = {A {{Single SMC Sampler}} on {{MPI}} That {{Outperforms}} a {{Single MCMC Sampler}}},
  author = {Varsi, Alessandro and Kekempanos, Lykourgos and Thiyagalingam, Jeyarajan and Maskell, Simon},
  year = {2019},
  month = may,
  eprint = {1905.10252},
  eprinttype = {arxiv},
  abstract = {Markov Chain Monte Carlo (MCMC) is a well-established family of algorithms which are primarily used in Bayesian statistics to sample from a target distribution when direct sampling is challenging. Single instances of MCMC methods are widely considered hard to parallelise in a problem-agnostic fashion and hence, unsuitable to meet both constraints of high accuracy and high throughput. Sequential Monte Carlo (SMC) Samplers can address the same problem, but are parallelisable: they share with Particle Filters the same key tasks and bottleneck. Although a rich literature already exists on MCMC methods, SMC Samplers are relatively underexplored, such that no parallel implementation is currently available. In this paper, we first propose a parallel MPI version of the SMC Sampler, including an optimised implementation of the bottleneck, and then compare it with single-core Metropolis-Hastings. The goal is to show that SMC Samplers may be a promising alternative to MCMC methods with high potential for future improvements. We demonstrate that a basic SMC Sampler with 512 cores is up to 85 times faster or up to 8 times more accurate than Metropolis-Hastings.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Distributed; Parallel; and Cluster Computing,Statistics - Computation},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\99AXQJU2\\Varsi et al. - 2019 - A Single SMC Sampler on MPI that Outperforms a Sin.pdf;C\:\\Users\\msca8h\\Zotero\\storage\\9HC9E6A6\\1905.html}
}

@inproceedings{vaswani_fast_2019,
  title = {Fast and Faster Convergence of {{SGD}} for Over-Parameterized Models and an Accelerated Perceptron},
  booktitle = {Proceedings of the {{International Conference}} on {{Artificial Intelligence}} and {{Statistics}}},
  author = {Vaswani, Sharan and Bach, Francis and Schmidt, Mark},
  year = {2019},
  month = apr,
  series = {{{PMLR}}},
  volume = {89},
  pages = {1195--1204},
  publisher = {{ML Research Press}},
  abstract = {Modern machine learning focuses on highly expressive models that are able to fit or interpolate the data completely,  resulting in zero training loss. For such models, we show that the stochastic gradients of common loss functions satisfy a strong growth condition. Under this condition, we prove that constant step-size stochastic gradient descent (SGD) with Nesterov acceleration matches the convergence rate of the deterministic accelerated method for both convex and strongly-convex functions. We also show that this condition implies that SGD can find a first-order stationary point as efficiently as full gradient descent in non-convex settings. Under interpolation, we further show that all smooth loss functions with a finite-sum structure satisfy a weaker growth condition. Given this weaker condition, we prove that SGD with a constant step-size attains the deterministic convergence rate in both the strongly-convex and convex settings. Under additional assumptions, the above results enable us to prove an O(1/k2)O(1/k2)O(1/k\^2) mistake bound for kkk iterations of a stochastic perceptron algorithm using the squared-hinge loss. Finally, we validate our theoretical findings with experiments on synthetic and real datasets.},
  langid = {english},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\2GWX4FJ2\\Vaswani et al. - 2019 - Fast and Faster Convergence of SGD for Over-Parame.pdf;C\:\\Users\\msca8h\\Zotero\\storage\\WAPU3B7N\\Vaswani et al. - 2019 - Fast and Faster Convergence of SGD for Over-Parame.pdf}
}

@inproceedings{vaswani_fast_2019a,
  title = {Fast and Faster Convergence of {{SGD}} for Over-Parameterized Models and an Accelerated Perceptron},
  booktitle = {Proceedings of the {{International Conference}} on {{Artificial Intelligence}} and {{Statistics}}},
  author = {Vaswani, Sharan and Bach, Francis and Schmidt, Mark},
  year = {2019},
  month = apr,
  series = {{{PMLR}}},
  pages = {1195--1204},
  publisher = {{ML Research Press}},
  abstract = {Modern machine learning focuses on highly expressive models that are able to fit or interpolate the data completely,  resulting in zero training loss. For such models, we show that the stochastic gradients of common loss functions satisfy a strong growth condition. Under this condition, we prove that constant step-size stochastic gradient descent (SGD) with Nesterov acceleration matches the convergence rate of the deterministic accelerated method for both convex and strongly-convex functions. We also show that this condition implies that SGD can find a first-order stationary point as efficiently as full gradient descent in non-convex settings. Under interpolation, we further show that all smooth loss functions with a finite-sum structure satisfy a weaker growth condition. Given this weaker condition, we prove that SGD with a constant step-size attains the deterministic convergence rate in both the strongly-convex and convex settings. Under additional assumptions, the above results enable us to prove an O(1/k2)O(1/k2)O(1/k\^2) mistake bound for kkk iterations of a stochastic perceptron algorithm using the squared-hinge loss. Finally, we validate our theoretical findings with experiments on synthetic and real datasets.},
  langid = {english},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\5GW676SU\\Vaswani et al. - 2019 - Fast and Faster Convergence of SGD for Over-Parame.pdf;C\:\\Users\\msca8h\\Zotero\\storage\\XT3PWZZA\\Vaswani et al. - 2019 - Fast and Faster Convergence of SGD for Over-Parame.pdf}
}

@inproceedings{vaswani_noiseadaptive_2022,
  title = {Towards Noise-Adaptive, Problem-Adaptive (Accelerated) Stochastic Gradient Descent},
  booktitle = {Proceedings of the {{International Conference}} on {{Machine Learning}}},
  author = {Vaswani, Sharan and {Dubois-Taine}, Benjamin and Babanezhad, Reza},
  year = {2022},
  month = jun,
  pages = {22015--22059},
  publisher = {{PMLR}},
  abstract = {We aim to make stochastic gradient descent (SGD) adaptive to (i) the noise {$\sigma$}2{$\sigma$}2\textbackslash sigma\^2 in the stochastic gradients and (ii) problem-dependent constants. When minimizing smooth, strongly-convex functions with condition number {$\kappa\kappa\backslash$}kappa, we prove that TTT iterations of SGD with exponentially decreasing step-sizes and knowledge of the smoothness can achieve an O\textasciitilde (exp(\textbackslash nicefrac-T{$\kappa$})+\textbackslash nicefrac{$\sigma$}2T)O\textasciitilde (exp(\textbackslash nicefrac-T{$\kappa$})+\textbackslash nicefrac{$\sigma$}2T)\textbackslash tilde\{O\} \textbackslash left(\textbackslash exp \textbackslash left( \textbackslash nicefrac\{-T\}\{\textbackslash kappa\} \textbackslash right) + \textbackslash nicefrac\{\textbackslash sigma\^2\}\{T\} \textbackslash right) rate, without knowing {$\sigma$}2{$\sigma$}2\textbackslash sigma\^2. In order to be adaptive to the smoothness, we use a stochastic line-search (SLS) and show (via upper and lower-bounds) that SGD with SLS converges at the desired rate, but only to a neighbourhood of the solution. On the other hand, we prove that SGD with an offline estimate of the smoothness converges to the minimizer. However, its rate is slowed down proportional to the estimation error. Next, we prove that SGD with Nesterov acceleration and exponential step-sizes (referred to as ASGD) can achieve the near-optimal O\textasciitilde (exp(\textbackslash nicefrac-T{$\kappa--\surd$})+\textbackslash nicefrac{$\sigma$}2T)O\textasciitilde (exp(\textbackslash nicefrac-T{$\kappa$})+\textbackslash nicefrac{$\sigma$}2T)\textbackslash tilde\{O\} \textbackslash left(\textbackslash exp \textbackslash left( \textbackslash nicefrac\{-T\}\{\textbackslash sqrt\{\textbackslash kappa\}\} \textbackslash right) + \textbackslash nicefrac\{\textbackslash sigma\^2\}\{T\} \textbackslash right) rate, without knowledge of {$\sigma$}2{$\sigma$}2\textbackslash sigma\^2. When used with offline estimates of the smoothness and strong-convexity, ASGD still converges to the solution, albeit at a slower rate. Finally, we empirically demonstrate the effectiveness of exponential step-sizes coupled with a novel variant of SLS.},
  langid = {english},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\Y8UJJ4TW\\Vaswani et al. - 2022 - Towards Noise-adaptive, Problem-adaptive (Accelera.pdf}
}

@inproceedings{veach_bidirectional_1994,
  title = {Bidirectional Estimators for Light Transport},
  booktitle = {Proceedings of the {{Eurographics Rendering Workshop}}},
  author = {Veach, Eric and Guibas, Leonidas J.},
  year = {1994},
  month = jun
}

@inproceedings{veach_optimally_1995,
  title = {Optimally Combining Sampling Techniques for {{Monte Carlo}} Rendering},
  booktitle = {Proceedings of the 22nd Annual Conference on Computer Graphics and Interactive Techniques},
  author = {Veach, Eric and Guibas, Leonidas J.},
  year = {1995},
  series = {{{SIGGRAPH}}'95},
  pages = {419--428},
  publisher = {{ACM Press}},
  address = {{Not Known}},
  langid = {english},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\2MHRQ3TB\\Veach and Guibas - 1995 - Optimally combining sampling techniques for Monte .pdf}
}

@phdthesis{veach_robust_1997,
  type = {Doctoral {{Thesis}}},
  title = {Robust {{Monte Carlo}} Methods for Light Transport Simulation},
  author = {Veach, Eric},
  year = {1997},
  school = {Stanford University},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\SRLLD79Q\\Veach - 1997 - Robust Monte Carlo methods for light transport sim.pdf}
}

@inproceedings{vegas-sanchez-ferrero_probabilisticdriven_2010,
  title = {Probabilistic-{{Driven Oriented Speckle Reducing Anisotropic Diffusion}} with {{Application}} to {{Cardiac Ultrasonic Images}}},
  booktitle = {Med. {{Image Comput}}. {{Comput}}.-{{Assisted Intervention}}},
  author = {{Vegas-Sanchez-Ferrero}, G. and {Aja-Fernandez}, S. and {Martin-Fernandez}, M. and Frangi, A. F. and Palencia, C.},
  year = {2010},
  series = {{{MICCAI}}'10},
  volume = {6361},
  pages = {518--525},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\7FCQGGNX\\Hutchison et al. - 2010 - Probabilistic-Driven Oriented Speckle Reducing Ani.pdf}
}

@article{vehtari_pareto_2021,
  title = {Pareto Smoothed Importance Sampling},
  author = {Vehtari, Aki and Simpson, Daniel and Gelman, Andrew and Yao, Yuling and Gabry, Jonah},
  year = {2021},
  month = feb,
  journal = {arXiv:1507.02646 [stat]},
  eprint = {1507.02646},
  eprinttype = {arxiv},
  primaryclass = {stat},
  abstract = {Importance weighting is a general way to adjust Monte Carlo integration to account for draws from the wrong distribution, but the resulting estimate can be noisy when the importance ratios have a heavy right tail. This routinely occurs when there are aspects of the target distribution that are not well captured by the approximating distribution, in which case more stable estimates can be obtained by modifying extreme importance ratios. We present a new method for stabilizing importance weights using a generalized Pareto distribution fit to the upper tail of the distribution of the simulated importance ratios. The method, which empirically performs better than existing methods for stabilizing importance sampling estimates, includes stabilized effective sample size estimates, Monte Carlo error estimates and convergence diagnostics.},
  archiveprefix = {arXiv},
  keywords = {Statistics - Computation,Statistics - Machine Learning,Statistics - Methodology},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\W9HIAV98\\Vehtari et al. - 2021 - Pareto Smoothed Importance Sampling.pdf;C\:\\Users\\msca8h\\Zotero\\storage\\L4Z2DYIR\\1507.html}
}

@article{vehtari_ranknormalization_2020,
  title = {Rank-{{Normalization}}, {{Folding}}, and {{Localization}}: {{An Improved R-hat}} for {{Assessing Convergence}} of {{MCMC}}},
  shorttitle = {Rank-{{Normalization}}, {{Folding}}, and {{Localization}}},
  author = {Vehtari, Aki and Gelman, Andrew and Simpson, Daniel and Carpenter, Bob and B{\"u}rkner, Paul-Christian},
  year = {2020},
  month = jul,
  journal = {Bayesian Analysis},
  langid = {english},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\PNJJNDU4\\Vehtari et al. - 2020 - Rank-Normalization, Folding, and Localization An .pdf}
}

@article{viberg_detection_1991,
  title = {Detection and Estimation in Sensor Arrays Using Weighted Subspace Fitting},
  author = {Viberg, M. and Ottersten, B. and Kailath, T.},
  year = {Nov./1991},
  journal = {IEEE Transactions on Signal Processing},
  volume = {39},
  number = {11},
  pages = {2436--2449}
}

@article{villani_recent_,
  title = {Recent {{Developments}} in {{Subsampling}} for {{Large-Scale Bayesian Inference}}},
  author = {Villani, Mattias},
  pages = {21},
  langid = {english},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\EHX9Y2P9\\Villani - Recent Developments in Subsampling for Large-Scale.pdf}
}

@article{villani_recent_a,
  title = {Recent {{Developments}} in {{Subsampling}} for {{Large-Scale Bayesian Inference}}},
  author = {Villani, Mattias},
  pages = {21},
  langid = {english},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\TLBFNHLC\\Villani - Recent Developments in Subsampling for Large-Scale.pdf}
}

@article{vo_gaussian_2006,
  title = {The {{Gaussian}} Mixture Probability Hypothesis Density Filter},
  author = {Vo, B.-N. and Ma, W.-K.},
  year = {2006},
  month = nov,
  journal = {IEEE Transactions on Signal Processing},
  volume = {54},
  number = {11},
  pages = {4091--4104}
}

@article{vorobyov_principles_2013,
  title = {Principles of Minimum Variance Robust Adaptive Beamforming Design},
  author = {Vorobyov, Sergiy A.},
  year = {2013},
  month = dec,
  journal = {Signal Processing},
  volume = {93},
  number = {12},
  pages = {3264--3277},
  langid = {english}
}

@article{vu_bayesian_2013,
  title = {A {{Bayesian}} Approach to {{SAR}} Imaging},
  author = {Vu, Duc and Xue, Ming and Tan, Xing and Li, Jian},
  year = {2013},
  month = may,
  journal = {Digital Signal Processing},
  volume = {23},
  number = {3},
  pages = {852--858},
  langid = {english}
}

@article{vu_bf_2012,
  title = {\$\{\textbackslash bf \vphantom\}{{S}}\vphantom\{\}\_\{3\}\$: {{A Spectral}} and {{Spatial Measure}} of {{Local Perceived Sharpness}} in {{Natural Images}}},
  shorttitle = {\$\{\textbackslash bf \vphantom\}{{S}}\vphantom\{\}\_\{3\}\$},
  author = {Vu, C. T. and Phan, T. D. and Chandler, D. M.},
  year = {2012},
  month = mar,
  journal = {IEEE Transactions on Image Processing},
  volume = {21},
  number = {3},
  pages = {934--945}
}

@article{Vuduc_2005,
  title = {{{OSKI}}: {{A}} Library of Automatically Tuned Sparse Matrix Kernels},
  author = {Vuduc, Richard and Demmel, James W and Yelick, Katherine A},
  year = {2005},
  month = jan,
  journal = {Journal of Physics: Conference Series},
  volume = {16},
  pages = {521--530},
  publisher = {{IOP Publishing}},
  abstract = {The Optimized Sparse Kernel Interface (OSKI) is a collection of low-level primitives that provide automatically tuned computational kernels on sparse matrices, for use by solver libraries and applications. These kernels include sparse matrix-vector multiply and sparse triangular solve, among others. The primary aim of this interface is to hide the complex decisionmaking process needed to tune the performance of a kernel implementation for a particular user's sparse matrix and machine, while also exposing the steps and potentially non-trivial costs of tuning at run-time. This paper provides an overview of OSKI, which is based on our research on automatically tuned sparse kernels for modern cache-based superscalar machines.}
}

@inproceedings{vuylsteke_multiscale_1994,
  title = {Multiscale Image Contrast Amplification ({{MUSICA}})},
  booktitle = {Proc. {{SPIE Med}}. {{Imag}}.},
  author = {Vuylsteke, Pieter and Schoeters, Emile P.},
  year = {1994},
  month = may,
  pages = {551--560},
  address = {{Newport Beach, CA}}
}

@article{wai_decentralized_2017,
  title = {Decentralized {{Frank}}\textendash{{Wolfe Algorithm}} for {{Convex}} and {{Nonconvex Problems}}},
  author = {Wai, Hoi-To and Lafond, Jean and Scaglione, Anna and Moulines, Eric},
  year = {2017},
  month = nov,
  journal = {IEEE Transactions on Automatic Control},
  volume = {62},
  number = {11},
  pages = {5522--5537},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\65NRNHVF\\07883821.pdf;C\:\\Users\\msca8h\\Zotero\\storage\\D3FG2APN\\Wai et al. - 2017 - Decentralized FrankWolfe Algorithm for Convex and.pdf}
}

@article{wan_variational_2016,
  title = {Variational {{Bayesian}} Learning for Robust {{AR}} Modeling with the Presence of Sparse Impulse Noise},
  author = {Wan, Hongjie and Xiao, Liang},
  year = {2016},
  month = dec,
  journal = {Digital Signal Processing},
  volume = {59},
  pages = {1--8},
  langid = {english}
}

@inproceedings{wang_adaptive_2013,
  title = {Adaptive {{Hamiltonian}} and {{Riemann Manifold Monte Carlo Samplers}}},
  booktitle = {Proceedings of the 30th {{International Conference}} on {{International Conference}} on {{Machine Learning}}},
  author = {Wang, Ziyu and Mohamed, Shakir and De Freitas, Nando},
  year = {2013},
  series = {Proceedings of {{Machine Learning Research}}},
  volume = {28},
  pages = {1462--1470},
  publisher = {{ML Research Press}}
}

@article{wang_adaptive_2018,
  title = {Adaptive {{Communication Strategies}} to {{Achieve}} the {{Best Error-Runtime Trade-off}} in {{Local-Update SGD}}},
  author = {Wang, Jianyu and Joshi, Gauri},
  year = {2018},
  month = oct,
  journal = {arXiv:1810.08313 [cs, stat]},
  eprint = {1810.08313},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Large-scale machine learning training, in particular distributed stochastic gradient descent, needs to be robust to inherent system variability such as node straggling and random communication delays. This work considers a distributed training framework where each worker node is allowed to perform local model updates and the resulting models are averaged periodically. We analyze the true speed of error convergence with respect to wall-clock time (instead of the number of iterations), and analyze how it is affected by the frequency of averaging. The main contribution is the design of AdaComm, an adaptive communication strategy that starts with infrequent averaging to save communication delay and improve convergence speed, and then increases the communication frequency in order to achieve a low error floor. Rigorous experiments on training deep neural networks show that AdaComm can take \$3 \textbackslash times\$ less time than fully synchronous SGD, and still reach the same final training loss.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Distributed; Parallel; and Cluster Computing,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\RZ9YXSA9\\Wang and Joshi - 2018 - Adaptive Communication Strategies to Achieve the B.pdf;C\:\\Users\\msca8h\\Zotero\\storage\\XJQVJI9R\\1810.html}
}

@article{wang_analysis_2018,
  title = {Analysis and Design of Optimum Sparse Array Configurations for Adaptive Beamforming},
  author = {Wang, Xiangrong and Amin, Moeness and Cao, Xianbin},
  year = {2018},
  month = jan,
  journal = {IEEE Transactions on Signal Processing},
  volume = {66},
  number = {2},
  pages = {340--351}
}

@article{wang_batched_2017,
  title = {Batched {{Large-scale Bayesian Optimization}} in {{High-dimensional Spaces}}},
  author = {Wang, Zi and Gehring, Clement and Kohli, Pushmeet and Jegelka, Stefanie},
  year = {2017},
  month = jun,
  journal = {arXiv:1706.01445 [cs, math, stat]},
  eprint = {1706.01445},
  eprinttype = {arxiv},
  primaryclass = {cs, math, stat},
  abstract = {Bayesian optimization (BO) has become an effective approach for black-box function optimization problems when function evaluations are expensive and the optimum can be achieved within a relatively small number of queries. However, many cases, such as the ones with high-dimensional inputs, may require a much larger number of observations for optimization. Despite an abundance of observations thanks to parallel experiments, current BO techniques have been limited to merely a few thousand observations. In this paper, we propose ensemble Bayesian optimization (EBO) to address three current challenges in BO simultaneously: (1) large-scale observations; (2) high dimensional input spaces; and (3) selections of batch queries that balance quality and diversity. The key idea of EBO is to operate on an ensemble of additive Gaussian process models, each of which possesses a randomized strategy to divide and conquer. We show unprecedented, previously impossible results of scaling up BO to tens of thousands of observations within minutes of computation.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Mathematics - Optimization and Control,Statistics - Machine Learning},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\P8P5YF5N\\Wang et al. - 2017 - Batched Large-scale Bayesian Optimization in High-.pdf;C\:\\Users\\msca8h\\Zotero\\storage\\68FBBWGZ\\1706.html}
}

@article{wang_blessings_2019,
  title = {The Blessings of Multiple Causes},
  author = {Wang, Yixin and Blei, David M.},
  year = {2019},
  month = apr,
  journal = {arXiv:1805.06826 [cs, stat]},
  eprint = {1805.06826},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Causal inference from observational data often assumes "ignorability," that all confounders are observed. This assumption is standard yet untestable. However, many scientific studies involve multiple causes, different variables whose effects are simultaneously of interest. We propose the deconfounder, an algorithm that combines unsupervised machine learning and predictive model checking to perform causal inference in multiple-cause settings. The deconfounder infers a latent variable as a substitute for unobserved confounders and then uses that substitute to perform causal inference. We develop theory for the deconfounder, and show that it requires weaker assumptions than classical causal inference. We analyze its performance in three types of studies: semi-simulated data around smoking and lung cancer, semi-simulated data around genome-wide association studies, and a real dataset about actors and movie revenue. The deconfounder provides a checkable approach to estimating closer-to-truth causal effects.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning,Statistics - Methodology},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\67QM9H9R\\Wang and Blei - 2019 - The Blessings of Multiple Causes.pdf;C\:\\Users\\msca8h\\Zotero\\storage\\FI92HUZU\\1805.html}
}

@article{wang_coherent_1985,
  title = {Coherent Signal-Subspace Processing for the Detection and Estimation of Angles of Arrival of Multiple Wide-Band Sources},
  author = {Wang, H. and Kaveh, M.},
  year = {1985},
  month = aug,
  journal = {IEEE Transactions on Acoustics, Speech, and Signal Processing},
  volume = {33},
  number = {4},
  pages = {823--831},
  langid = {english}
}

@article{wang_exact_2019,
  title = {Exact {{Gaussian Processes}} on a {{Million Data Points}}},
  author = {Wang, Ke Alexander and Pleiss, Geoff and Gardner, Jacob R. and Tyree, Stephen and Weinberger, Kilian Q. and Wilson, Andrew Gordon},
  year = {2019},
  month = mar,
  journal = {arXiv:1903.08114 [cs, stat]},
  eprint = {1903.08114},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Gaussian processes (GPs) are flexible models with state-of-the-art performance on many impactful applications. However, computational constraints with standard inference procedures have limited exact GPs to problems with fewer than about ten thousand training points, necessitating approximations for larger datasets. In this paper, we develop a scalable approach for exact GPs that leverages multi-GPU parallelization and methods like linear conjugate gradients, accessing the kernel matrix only through matrix multiplication. By partitioning and distributing kernel matrix multiplies, we demonstrate that an exact GP can be trained on over a million points in 3 days using 8 GPUs and can compute predictive means and variances in under a second using 1 GPU at test time. Moreover, we perform the first-ever comparison of exact GPs against state-of-the-art scalable approximations on large-scale regression datasets with \$10\^4-10\^6\$ data points, showing dramatic performance improvements.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Distributed; Parallel; and Cluster Computing,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\HIJJEKGP\\Wang et al. - 2019 - Exact Gaussian Processes on a Million Data Points.pdf;C\:\\Users\\msca8h\\Zotero\\storage\\XVCBFY6N\\1903.html}
}

@article{wang_exact_2022,
  title = {Exact Convergence Rate Analysis of the Independent {{Metropolis-Hastings}} Algorithms},
  author = {Wang, Guanyang},
  year = {2022},
  journal = {Bernoulli},
  volume = {28},
  number = {3},
  eprint = {2008.02455},
  eprinttype = {arxiv},
  pages = {2012--2033},
  abstract = {A well-known difficult problem regarding Metropolis-Hastings algorithms is to get sharp bounds on their convergence rates. Moreover, a fundamental but often overlooked problem in Markov chain theory is to study the convergence rates for different initializations. In this paper, we study the two issues mentioned above of the Independent Metropolis-Hastings (IMH) algorithms on both general and discrete state spaces. We derive the exact convergence rate and prove that the IMH algorithm's different deterministic initializations have the same convergence rate. Surprisingly, under mild conditions, we get the exact convergence speed for IMH algorithms on general state spaces, which is the first `exact convergence' result for general state space MCMC algorithms to the author's best knowledge. Connections with the Random Walk Metropolis-Hastings (RWMH) algorithm are also discussed, which solve a conjecture proposed by Atchad\textbackslash '\{e\} and Perron \textbackslash cite\{atchade2007geometric\} using a counterexample.},
  archiveprefix = {arXiv},
  keywords = {Mathematics - Probability,Mathematics - Spectral Theory,Mathematics - Statistics Theory,Statistics - Computation},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\5U5TFW55\\Wang - 2020 - Exact Convergence Rate Analysis of the Independent.pdf;C\:\\Users\\msca8h\\Zotero\\storage\\4F26VXU9\\2008.html}
}

@article{wang_grid_2018,
  title = {Grid Evolution Method for {{DOA}} Estimation},
  author = {Wang, Qianli and Zhao, Zhiqin and Chen, Zhuming and Nie, Zaiping},
  year = {2018},
  month = may,
  journal = {IEEE Transactions on Signal Processing},
  volume = {66},
  number = {9},
  pages = {2374--2383}
}

@article{wang_image_2004,
  title = {Image Quality Assessment: From Error Visibility to Structural Similarity},
  shorttitle = {Image {{Quality Assessment}}},
  author = {Wang, Z. and Bovik, A.C. and Sheikh, H.R. and Simoncelli, E.P.},
  year = {2004},
  month = apr,
  journal = {IEEE Transactions on Image Processing},
  volume = {13},
  number = {4},
  pages = {600--612},
  langid = {english}
}

@article{wang_image_2004a,
  title = {Image {{Quality Assessment}}: {{From Error Visibility}} to {{Structural Similarity}}},
  shorttitle = {Image {{Quality Assessment}}},
  author = {Wang, Z. and Bovik, A.C. and Sheikh, H.R. and Simoncelli, E.P.},
  year = {2004},
  month = apr,
  journal = {IEEE Transactions on Image Processing},
  volume = {13},
  number = {4},
  pages = {600--612},
  langid = {english}
}

@article{wang_image_2013,
  title = {Image Denoising Using Modified {{Perona}}\textendash{{Malik}} Model Based on Directional {{Laplacian}}},
  author = {Wang, Y.Q. and Guo, Jichang and Chen, Wufan and Zhang, Wenxue},
  year = {2013},
  month = sep,
  journal = {Signal Processing},
  volume = {93},
  number = {9},
  pages = {2548--2558},
  langid = {english}
}

@article{wang_integrating_2014,
  title = {Integrating Profile-Driven Parallelism Detection and Machine-Learning-Based Mapping},
  author = {Wang, Zheng and Tournavitis, Georgios and Franke, Bj{\"o}rn and O'boyle, Michael F. P.},
  year = {2014},
  month = feb,
  journal = {ACM Transactions on Architecture and Code Optimization},
  volume = {11},
  number = {1},
  pages = {1--26},
  langid = {english},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\VMZ8J7F2\\Wang et al. - 2014 - Integrating profile-driven parallelism detection a.pdf}
}

@incollection{wang_knowledgebased_2012,
  title = {Knowledge-{{Based Adaptive Self-Scheduling}}},
  booktitle = {Network and {{Parallel Computing}}},
  author = {Wang, Yizhuo and Ji, Weixing and Shi, Feng and Zuo, Qi and Deng, Ning},
  year = {2012},
  volume = {7513},
  pages = {22--32},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\S7GHK23E\\Wang et al. - 2012 - Knowledge-Based Adaptive Self-Scheduling.pdf}
}

@inproceedings{wang_mapping_2009,
  title = {Mapping Parallelism to Multi-Cores: A Machine Learning Based Approach},
  booktitle = {Proc. 14th {{ACM SIGPLAN Symp}}. {{Princ}}. {{Pract}}. {{Parallel Program}}.},
  author = {Wang, Zheng and O'Boyle, Michael F.P.},
  year = {2009},
  series = {{{PPoPP}}'09},
  pages = {75--84},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  keywords = {artificial neural networks,compiler optimization,machine learning,performance modeling,support vector machine},
  file = {/home/msca8h/Documents/parallel_scheduling/ML_scheduling_strategy.pdf}
}

@inproceedings{wang_marginalized_2012,
  title = {A {{Marginalized Particle Gaussian Process Regression}}},
  booktitle = {Proceedings of the {{Neural Information Processing Systems}} ({{NIPS}}'12)},
  author = {Wang, Yali and {Chaib-draa}, Brahim},
  year = {2012},
  pages = {1187--1195}
}

@inproceedings{wang_maxvalue_2017,
  title = {Max-Value {{Entropy Search}} for {{Efficient Bayesian Optimization}}},
  booktitle = {Proceedings of the 34th {{International Conference}} on {{Machine Learning}} - {{Volume}} 70},
  author = {Wang, Zi and Jegelka, Stefanie},
  year = {2017},
  series = {{{ICML}}'17},
  pages = {3627--3635},
  publisher = {{JMLR.org}}
}

@inproceedings{wang_maxvalue_2017a,
  title = {Max-Value Entropy Search for Efficient {{Bayesian}} Optimization},
  booktitle = {Proc. 34th {{Int}}. {{Conf}}. {{Mach}}. {{Learn}}.},
  author = {Wang, Zi and Jegelka, Stefanie},
  year = {2017},
  series = {{{ICML}}'17},
  volume = {70},
  pages = {3627--3635},
  publisher = {{JMLR.org}},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\GGESBYZE\\Wang and Jegelka - 2017 - Max-value Entropy Search for Efficient Bayesian Op.pdf}
}

@article{wang_novel_2016,
  title = {Novel Wideband {{DOA}} Estimation Based on Sparse {{Bayesian}} Learning with {{Dirichlet}} Process Priors},
  author = {Wang, Lu and Zhao, Lifan and Bi, Guoan and Wan, Chunru and Zhang, Liren and Zhang, Haijian},
  year = {2016},
  month = jan,
  journal = {IEEE Transactions on Signal Processing},
  volume = {64},
  number = {2},
  pages = {275--289}
}

@article{wang_offgrid_2022,
  title = {An Off-Grid Wideband {{DOA}} Estimation Method with the Variational {{Bayes}} Expectation-Maximization Framework},
  author = {Wang, Pengyu and Yang, Huichao and Ye, Zhongfu},
  year = {2022},
  month = apr,
  journal = {Signal Processing},
  volume = {193},
  pages = {108423},
  langid = {english}
}

@inproceedings{wang_predicting_2016,
  title = {Predicting the Memory Bandwidth and Optimal Core Allocations for Multi-Threaded Applications on Large-Scale {{NUMA}} Machines},
  booktitle = {2016 {{IEEE International Symposium}} on {{High Performance Computer Architecture}} ({{HPCA}})},
  author = {Wang, Wei and Davidson, Jack W. and Soffa, Mary Lou},
  year = {2016},
  month = mar,
  pages = {419--431},
  publisher = {{IEEE}},
  address = {{Barcelona, Spain}}
}

@inproceedings{wang_privacy_2015,
  title = {Privacy for Free: {{Posterior}} Sampling and Stochastic Gradient {{Monte Carlo}}},
  shorttitle = {Privacy for {{Free}}},
  booktitle = {Proceedings of the 32nd {{International Conference}} on {{Machine Learning}}},
  author = {Wang, Yu-Xiang and Fienberg, Stephen and Smola, Alex},
  year = {2015},
  month = jun,
  pages = {2493--2502},
  publisher = {{PMLR}},
  abstract = {We consider the problem of Bayesian learning on sensitive datasets and present two simple but somewhat surprising results that connect Bayesian learning to ``differential privacy'', a cryptographic approach to protect individual-level privacy while permitting database-level utility. Specifically, we show that under standard assumptions, getting one sample from a posterior distribution is differentially private ``for free''; and this sample as a statistical estimator is often consistent, near optimal, and computationally tractable. Similarly but separately, we show that a recent line of work that use stochastic gradient for Hybrid Monte Carlo (HMC) sampling also preserve differentially privacy with minor or no modifications of the algorithmic procedure at all, these observations lead to an ``anytime'' algorithm for Bayesian learning under privacy constraint. We demonstrate that it performs much better than the state-of-the-art differential private methods on synthetic and real datasets.},
  langid = {english},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\5TJJBHHR\\Wang et al. - 2015 - Privacy for Free Posterior Sampling and Stochasti.pdf}
}

@inproceedings{wang_regret_2018,
  title = {Regret Bounds for Meta {{Bayesian}} Optimization with an Unknown {{Gaussian}} Process Prior},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Wang, Zi and Kim, Beomjoon and Kaelbling, Leslie Pack},
  year = {2018},
  pages = {10477--10488}
}

@article{wang_sparse_2021,
  title = {Sparse {{Bayesian}} Learning Using Generalized Double {{Pareto}} Prior for {{DOA}} Estimation},
  author = {Wang, Qisen and Yu, Hua and Li, Jie and Ji, Fei and Chen, Fangjiong},
  year = {2021},
  journal = {IEEE Signal Processing Letters},
  volume = {28},
  pages = {1744--1748}
}

@inproceedings{wang2022posterior,
  title = {Posterior Collapse of a Linear Latent Variable Model},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Wang, Zihao and Ziyin, Liu},
  year = {2022}
}

@article{ward_broadband_1998,
  title = {Broadband {{DOA}} Estimation Using Frequency Invariant Beamforming},
  author = {Ward, D.B. and Ding, Zhi and Kennedy, R.A.},
  year = {1998},
  month = may,
  journal = {IEEE Transactions on Signal Processing},
  volume = {46},
  number = {5},
  pages = {1463--1469},
  abstract = {A new method of direction-of-arrival (DOA) estimation for multiple broadband farfield signals is presented. The technique uses a beamspace preprocessing structure based on frequency invariant beamforming. Specifically, a set of beam-shaping filters focus the received array data in the time domain, thereby avoiding the need for frequency decomposition. Hence, the proposed method is conceptually different from most other broadband DOA estimators, which require frequency decomposition. Numerical results are presented to demonstrate the use of the new method and compare it with conventional coherent signal subspace methods.},
  keywords = {Array signal processing,Covariance matrix,Direction of arrival estimation,Equations,Frequency estimation,Linear systems,MIMO,System identification,Time domain analysis,Transfer functions},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\4EGK3SLQ\\Ward et al. - 1998 - Broadband DOA estimation using frequency invariant.pdf}
}

@article{wax_detection_1985,
  title = {Detection of Signals by Information Theoretic Criteria},
  author = {Wax, M. and Kailath, T.},
  year = {1985},
  month = apr,
  journal = {IEEE Transactions on Acoustics, Speech, and Signal Processing},
  volume = {33},
  number = {2},
  pages = {387--392},
  abstract = {A new approach is presented to the problem of detecting the number of signals in a multichannel time-series, based on the application of the information theoretic criteria for model selection introduced by Akaike (AIC) and by Schwartz and Rissanen (MDL). Unlike the conventional hypothesis testing based approach, the new approach does not requite any subjective threshold settings; the number of signals is obtained merely by minimizing the AIC or the MDL criteria. Simulation results that illustrate the performance of the new method for the detection of the number of signals received by a sensor array are presented.},
  keywords = {Additive noise,Array signal processing,Backscatter,Covariance matrix,Sensor arrays,Sensor phenomena and characterization,Signal detection,Signal processing,Testing,Transient response},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\3DUEWNTC\\Wax and Kailath - 1985 - Detection of signals by information theoretic crit.pdf;C\:\\Users\\msca8h\\Zotero\\storage\\FHE9RUEW\\1164557.html}
}

@article{wax_spatiotemporal_1984,
  title = {Spatio-Temporal Spectral Analysis by Eigenstructure Methods},
  author = {Wax, M. and Shan, Tie-Jun and Kailath, T.},
  year = {1984},
  month = aug,
  journal = {IEEE Transactions on Acoustics, Speech, and Signal Processing},
  volume = {32},
  number = {4},
  pages = {817--827},
  abstract = {This paper presents new algorithms for estimating the spatio-temporal spectrum of the signals received by a passive array. The algorithms are based on the eigenstructure of the covariance and spectral density matrices of the received signals. They allow partial correlation between the sources and thus are applicable to certain kinds of multipath problems. Simulation results that illustrate the performance of the new algorithms are presented.},
  keywords = {Contracts,Covariance matrix,Direction of arrival estimation,Frequency domain analysis,Frequency estimation,Maximum likelihood estimation,Narrowband,Sensor arrays,Spectral analysis,Wideband},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\JX4NSMJS\\Wax et al. - 1984 - Spatio-temporal spectral analysis by eigenstructur.pdf;C\:\\Users\\msca8h\\Zotero\\storage\\Y44UUJVA\\1164400.html}
}

@article{weare_efficient_2007,
  title = {Efficient {{Monte Carlo}} Sampling by Parallel Marginalization},
  author = {Weare, J.},
  year = {2007},
  month = jul,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {104},
  number = {31},
  pages = {12657--12662},
  langid = {english},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\E8U3ZZCM\\Weare - 2007 - Efficient Monte Carlo sampling by parallel margina.pdf}
}

@book{weickert_anisotropic_1998,
  title = {Anisotropic {{Diffusion}} in {{Image Processing}}},
  author = {Weickert, Joachim},
  year = {1998},
  series = {{{ECMI}}},
  publisher = {{Teubner-Verlag,}},
  address = {{Stuttgart, Germany}}
}

@article{weickert_coherenceenhancing_1999,
  title = {{Coherence-enhancing diffusion filtering}},
  author = {Weickert, Joachim},
  year = {1999},
  journal = {International Journal of Computer Vision},
  volume = {31},
  number = {2/3},
  pages = {111--127},
  langid = {Int. J. Comput. Vision}
}

@misc{welandawe_robust_2022,
  title = {Robust, Automated, and Accurate Black-Box Variational Inference},
  author = {Welandawe, Manushi and Andersen, Michael Riis and Vehtari, Aki and Huggins, Jonathan H.},
  year = {2022},
  month = mar,
  number = {arXiv:2203.15945},
  eprint = {2203.15945},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  institution = {{arXiv}},
  abstract = {Black-box variational inference (BBVI) now sees widespread use in machine learning and statistics as a fast yet flexible alternative to Markov chain Monte Carlo methods for approximate Bayesian inference. However, stochastic optimization methods for BBVI remain unreliable and require substantial expertise and hand-tuning to apply effectively. In this paper, we propose Robust, Automated, and Accurate BBVI (RAABBVI), a framework for reliable BBVI optimization. RAABBVI is based on rigorously justified automation techniques, includes just a small number of intuitive tuning parameters, and detects inaccurate estimates of the optimal variational approximation. RAABBVI adaptively decreases the learning rate by detecting convergence of the fixed--learning-rate iterates, then estimates the symmetrized Kullback--Leiber (KL) divergence between the current variational approximation and the optimal one. It also employs a novel optimization termination criterion that enables the user to balance desired accuracy against computational cost by comparing (i) the predicted relative decrease in the symmetrized KL divergence if a smaller learning were used and (ii) the predicted computation required to converge with the smaller learning rate. We validate the robustness and accuracy of RAABBVI through carefully designed simulation studies and on a diverse set of real-world model and data examples.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning,Statistics - Methodology},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\EZ5GKKYM\\Welandawe et al. - 2022 - Robust, Automated, and Accurate Black-box Variatio.pdf;C\:\\Users\\msca8h\\Zotero\\storage\\HQEBH8DW\\2203.html}
}

@article{wenzel_how_2020,
  title = {How Good Is the {{Bayes}} Posterior in Deep Neural Networks Really?},
  author = {Wenzel, Florian and Roth, Kevin and Veeling, Bastiaan S. and {\'S}wi{\k{a}}tkowski, Jakub and Tran, Linh and Mandt, Stephan and Snoek, Jasper and Salimans, Tim and Jenatton, Rodolphe and Nowozin, Sebastian},
  year = {2020},
  month = feb,
  journal = {arXiv:2002.02405 [cs, stat]},
  eprint = {2002.02405},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {During the past five years the Bayesian deep learning community has developed increasingly accurate and efficient approximate inference procedures that allow for Bayesian inference in deep neural networks. However, despite this algorithmic progress and the promise of improved uncertainty quantification and sample efficiency there are---as of early 2020---no publicized deployments of Bayesian neural networks in industrial practice. In this work we cast doubt on the current understanding of Bayes posteriors in popular deep neural networks: we demonstrate through careful MCMC sampling that the posterior predictive induced by the Bayes posterior yields systematically worse predictions compared to simpler methods including point estimates obtained from SGD. Furthermore, we demonstrate that predictive performance is improved significantly through the use of a "cold posterior" that overcounts evidence. Such cold posteriors sharply deviate from the Bayesian paradigm but are commonly used as heuristic in Bayesian deep learning papers. We put forward several hypotheses that could explain cold posteriors and evaluate the hypotheses through experiments. Our work questions the goal of accurate posterior approximations in Bayesian deep learning: If the true Bayes posterior is poor, what is the use of more accurate approximations? Instead, we argue that it is timely to focus on understanding the origin of the improved performance of cold posteriors.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Computation,Statistics - Machine Learning},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\6Y6FM6E9\\Wenzel et al. - 2020 - How Good is the Bayes Posterior in Deep Neural Net.pdf;C\:\\Users\\msca8h\\Zotero\\storage\\QHGJS7NV\\2002.html}
}

@incollection{wilkinson_parallel_2005,
  title = {Parallel {{Bayesian}} Computation},
  booktitle = {Handbook of {{Parallel Computing}} and {{Statistics}} ({{Statistics}}, {{Textbooks}} and {{Monographs}})},
  author = {Wilkinson, Darren},
  year = {2005},
  publisher = {{Chapman \& Hall/CRC}},
  abstract = {The use of Bayesian inference for the analysis of complex statistical models has increased dramatically in recent years, in part due to the increasing availability of computing power. There are a range of techniques available for carrying out Bayesian inference, but the lack of analytic tractability for the vast majority of models of interest means that most of the techniques are numeric, and many are computationally demanding. Indeed, for high-dimensional nonlinear models, the only practical methods for analysis are based on Markov chain Monte Carlo (MCMC) techniques, and these are notoriously computation intensive, with some analyses requiring weeks of CPU time on powerful computers. It is clear therefore that the use of parallel computing technology in the context of Bayesian computation is of great interest to many who analyze complex models using Bayesian techniques. Of particular interest in the context of Bayesian inference are techniques for parallelization of a computation utilizing the conditional independence structure of the underlying model, as well as strategies for parallelization of Monte Carlo and MCMC algorithms. There are two obvious approaches to parallelization of an MCMC algorithm: one is based on the idea of running different chains in parallel, and the other is based on parallelization of a single chain. It is a subtle and problem-dependent question as to which of these strategies (or combination of the two) is likely to be most appropriate and there are also important issues relating to parallel random number generation which need to be addressed.}
}

@article{williams_bayesian_1998,
  title = {Bayesian Classification with {{Gaussian}} Processes},
  author = {Williams, Christopher K.I. and Barber, David},
  year = {Dec./1998},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {20},
  number = {12},
  pages = {1342--1351},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\R6R4FWHE\\Williams and Barber - 1998 - Bayesian classification with Gaussian processes.pdf}
}

@article{wilson_bayesian_2020,
  title = {Bayesian Deep Learning and a Probabilistic Perspective of Generalization},
  author = {Wilson, Andrew Gordon and Izmailov, Pavel},
  year = {2020},
  month = mar,
  journal = {arXiv:2002.08791 [cs, stat]},
  eprint = {2002.08791},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {The key distinguishing property of a Bayesian approach is marginalization, rather than using a single setting of weights. Bayesian marginalization can particularly improve the accuracy and calibration of modern deep neural networks, which are typically underspecified by the data, and can represent many compelling but different solutions. We show that deep ensembles provide an effective mechanism for approximate Bayesian marginalization, and propose a related approach that further improves the predictive distribution by marginalizing within basins of attraction, without significant overhead. We also investigate the prior over functions implied by a vague distribution over neural network weights, explaining the generalization properties of such models from a probabilistic perspective. From this perspective, we explain results that have been presented as mysterious and distinct to neural network generalization, such as the ability to fit images with random labels, and show that these results can be reproduced with Gaussian processes. Finally, we provide a Bayesian perspective on tempering for calibrating predictive distributions.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\C9ME5GTU\\Wilson and Izmailov - 2020 - Bayesian Deep Learning and a Probabilistic Perspec.pdf;C\:\\Users\\msca8h\\Zotero\\storage\\Q22FTK8E\\2002.html}
}

@article{wilson_case_2019,
  title = {The Case for {{Bayesian}} Deep Learning},
  author = {Wilson, Andrew Gordon},
  year = {2019},
  journal = {NYU Courant Technical Report}
}

@inproceedings{wilson_efficiently_2020,
  title = {Efficiently Sampling Functions from {{Gaussian}} Process Posteriors},
  booktitle = {Proceedings of the {{International Conference}} on {{Machine Learning}}},
  author = {Wilson, James and Borovitskiy, Viacheslav and Terenin, Alexander and Mostowsky, Peter and Deisenroth, Marc},
  year = {2020},
  month = nov,
  pages = {10292--10302},
  publisher = {{PMLR}},
  abstract = {Gaussian processes are the gold standard for many real-world modeling problems, especially in cases where a model's success hinges upon its ability to faithfully represent predictive uncertainty. These problems typically exist as parts of larger frameworks, wherein quantities of interest are ultimately defined by integrating over posterior distributions. These quantities are frequently intractable, motivating the use of Monte Carlo methods. Despite substantial progress in scaling up Gaussian processes to large training sets, methods for accurately generating draws from their posterior distributions still scale cubically in the number of test locations. We identify a decomposition of Gaussian processes that naturally lends itself to scalable sampling by separating out the prior from the data. Building off of this factorization, we propose an easy-to-use and general-purpose approach for fast posterior sampling, which seamlessly pairs with sparse approximations to afford scalability both during training and at test time. In a series of experiments designed to test competing sampling schemes' statistical properties and practical ramifications, we demonstrate how decoupled sample paths accurately represent Gaussian process posteriors at a fraction of the usual cost.},
  langid = {english},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\VTX4PWDW\\Wilson et al. - 2020 - Efficiently sampling functions from Gaussian proce.pdf;C\:\\Users\\msca8h\\Zotero\\storage\\Z4QXEXSA\\Wilson et al. - 2020 - Efficiently sampling functions from Gaussian proce.pdf}
}

@inproceedings{wilson_maximizing_2018,
  title = {Maximizing {{Acquisition Functions}} for {{Bayesian Optimization}}},
  booktitle = {Proceedings of the 32nd {{International Conference}} on {{Neural Information Processing Systems}}},
  author = {Wilson, James T. and Hutter, Frank and Deisenroth, Marc Peter},
  year = {2018},
  series = {{{NIPS}}'18},
  pages = {9906--9917},
  publisher = {{Curran Associates Inc.}},
  address = {{Red Hook, NY, USA}}
}

@inproceedings{wilson_maximizing_2018a,
  title = {Maximizing Acquisition Functions for {{Bayesian}} Optimization},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Wilson, James and Hutter, Frank and Deisenroth, Marc},
  year = {2018},
  pages = {9884--9895}
}

@article{winder_ii_1975,
  title = {{{II}}. {{Sonar System Technology}}},
  author = {Winder, A.A.},
  year = {1975},
  month = sep,
  journal = {IEEE Transactions on Sonics and Ultrasonics},
  volume = {22},
  number = {5},
  pages = {291--332}
}

@article{wolberg_multisurface_1990,
  title = {Multisurface Method of Pattern Separation for Medical Diagnosis Applied to Breast Cytology.},
  author = {Wolberg, W. H. and Mangasarian, O. L.},
  year = {1990},
  month = dec,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {87},
  number = {23},
  pages = {9193--9196},
  langid = {english},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\8W7JPKJV\\Wolberg and Mangasarian - 1990 - Multisurface method of pattern separation for medi.pdf}
}

@article{wolstenhulme_agreement_2015,
  title = {Agreement between Objective and Subjective Assessment of Image Quality in Ultrasound Abdominal Aortic Aneurism Screening},
  author = {Wolstenhulme, S and Davies, A G and Keeble, C and Moore, S and Evans, J A},
  year = {2015},
  month = feb,
  journal = {The British Journal of Radiology},
  volume = {88},
  number = {1046},
  pages = {20140482},
  langid = {english},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\YLJGVLM5\\Wolstenhulme et al. - 2015 - Agreement between objective and subjective assessm.pdf}
}

@article{wong_monte_2012,
  title = {Monte {{Carlo}} Despeckling of Transrectal Ultrasound Images of the Prostate},
  author = {Wong, Alexander and Scharcanski, Jacob},
  year = {2012},
  month = sep,
  journal = {Digital Signal Processing},
  volume = {22},
  number = {5},
  pages = {768--775},
  langid = {english}
}

@inproceedings{wood-aistats-2014,
  title = {A New Approach to Probabilistic Programming Inference},
  booktitle = {Proc. 17th {{Int}}. {{Conf}}. {{Mach}}. {{Learn}}.},
  author = {Wood, Frank and {van de Meent}, Jan Willem and Mansinghka, Vikash},
  year = {2014},
  series = {{{ICML}}'14},
  pages = {1024--1032}
}

@article{wu_coherent_2019,
  title = {Coherent {{SVR}} Learning for Wideband Direction-of-Arrival Estimation},
  author = {Wu, Liu-Li and Huang, Zhi-Tao},
  year = {2019},
  month = apr,
  journal = {IEEE Signal Processing Letters},
  volume = {26},
  number = {4},
  pages = {642--646}
}

@article{wu_minibatch_2022,
  title = {Mini-Batch {{Metropolis}}\textendash{{Hastings}} with Reversible {{SGLD}} Proposal},
  author = {Wu, Tung-Yu and Rachel Wang, Y. X. and Wong, Wing H.},
  year = {2022},
  month = jan,
  journal = {Journal of the American Statistical Association},
  volume = {117},
  number = {537},
  pages = {386--394},
  langid = {english},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\EKSYYETA\\Wu et al. - 2022 - Mini-Batch MetropolisHastings With Reversible SGL.pdf}
}

@article{wu_parallel_2012,
  title = {Parallel {{Markov}} Chain {{Monte Carlo}} - Bridging the Gap to High-Performance {{Bayesian}} Computation in Animal Breeding and Genetics},
  author = {Wu, Xiao-Lin and Sun, Chuanyu and Beissinger, Timothy M and Rosa, Guilherme JM and Weigel, Kent A and Gatti, Natalia de Leon and Gianola, Daniel},
  year = {2012},
  month = dec,
  journal = {Genetics Selection Evolution},
  volume = {44},
  number = {1},
  pages = {29},
  langid = {english},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\7WNJB3YC\\Wu et al. - 2012 - Parallel Markov chain Monte Carlo - bridging the g.pdf}
}

@article{xian-yingli_mixeddomain_2013,
  title = {Mixed-Domain Edge-Aware Image Manipulation},
  author = {{Xian-Ying Li} and {Yan Gu} and {Shi-Min Hu} and Martin, R. R.},
  year = {2013},
  month = may,
  journal = {IEEE Transactions on Image Processing},
  volume = {22},
  number = {5},
  pages = {1915--1925}
}

@inproceedings{xiang-jian_imaging_2020,
  title = {An Imaging Algorithm for High-Speed Side-Scan Sonar Based on Multi-Beam Forming Technology},
  booktitle = {Proceedings of {{Global Oceans}}},
  author = {{Xiang-jian}, Meng and Wen, Xu and {Bin-jian}, Shen and Xinxin, Guo and {Xin-yu}, Liu and Yushi, Liu},
  year = {2020},
  month = oct,
  series = {Global {{Oceans}}'20},
  pages = {1--6},
  publisher = {{IEEE}},
  address = {{Biloxi, MS, USA}}
}

@article{xiaohuihao_novel_1999,
  title = {A Novel Multiscale Nonlinear Thresholding Method for Ultrasonic Speckle Suppressing},
  author = {{Xiaohui Hao} and {Shangkai Gao} and {Xiaorong Gao}},
  year = {Sept./1999},
  journal = {IEEE Transactions on Medical Imaging},
  volume = {18},
  number = {9},
  pages = {787--794}
}

@inproceedings{xie_lightercommunication_2016,
  title = {Lighter-{{Communication Distributed Machine Learning}} via {{Sufficient Factor Broadcasting}}.},
  booktitle = {{{UAI}}},
  author = {Xie, Pengtao and Kim, Jin Kyu and Zhou, Yi and Ho, Qirong and Kumar, Abhimanu and Yu, Yaoliang and Xing, Eric P},
  year = {2016},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\YPQM6P7H\\Xie et al. - 2016 - Lighter-Communication Distributed Machine Learning.pdf}
}

@article{Xiong_Xu_Liang_Zhang_2021,
  title = {Non-Asymptotic Convergence of Adam-Type Reinforcement Learning Algorithms under Markovian Sampling},
  author = {Xiong, Huaqing and Xu, Tengyu and Liang, Yingbin and Zhang, Wei},
  year = {2021},
  month = may,
  journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {35},
  number = {12},
  pages = {10460--10468},
  abstract = {Despite the wide applications of Adam in reinforcement learning (RL), the theoretical convergence of Adam-type RL algorithms has not been established. This paper provides the first such convergence analysis for two fundamental RL algorithms of policy gradient (PG) and temporal difference (TD) learning that incorporate AMSGrad updates (a standard alternative of Adam in theoretical analysis), referred to as PG-AMSGrad and TD-AMSGrad, respectively. Moreover, our analysis focuses on Markovian sampling for both algorithms. We show that under general nonlinear function approximation, PG-AMSGrad with a constant stepsize converges to a neighborhood of a stationary point at the rate of O(1/T) (where T denotes the number of iterations), and with a diminishing stepsize converges exactly to a stationary point at the rate of O(log{$^2$} T/{$\surd$}T). Furthermore, under linear function approximation, TD-AMSGrad with a constant stepsize converges to a neighborhood of the global optimum at the rate of O(1/T), and with a diminishing stepsize converges exactly to the global optimum at the rate of O(log T/{$\surd$}T). Our study develops new techniques for analyzing the Adam-type RL algorithms under Markovian sampling.}
}

@article{xu_computational_2022,
  title = {The Computational Asymptotics of {{Gaussian}} Variational Inference and the {{Laplace}} Approximation},
  author = {Xu, Zuheng and Campbell, Trevor},
  year = {2022},
  month = aug,
  journal = {Statistics and Computing},
  volume = {32},
  number = {4},
  pages = {63},
  langid = {english}
}

@inproceedings{xu_empirical_2016,
  title = {An {{Empirical Study}} of {{ADMM}} for {{Nonconvex Problems}}},
  booktitle = {{{NIPS}} 2016 {{Workshop}} on {{Nonconvex Optimization}} for {{Machine Learning}}: {{Theory}} and {{Practice}}},
  author = {Xu, Zheng and De, Soham and Figueiredo, Mario and Studer, Christoph and Goldstein, Tom},
  year = {2016},
  month = dec,
  eprint = {1612.03349},
  eprinttype = {arxiv},
  abstract = {The alternating direction method of multipliers (ADMM) is a common optimization tool for solving constrained and non-differentiable problems. We provide an empirical study of the practical performance of ADMM on several nonconvex applications, including l0 regularized linear regression, l0 regularized image denoising, phase retrieval, and eigenvector computation. Our experiments suggest that ADMM performs well on a broad class of non-convex problems. Moreover, recently proposed adaptive ADMM methods, which automatically tune penalty parameters as the method runs, can improve algorithm efficiency and solution quality compared to ADMM with a non-tuned penalty.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Mathematics - Optimization and Control},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\PYJYBFR3\\Xu et al. - 2016 - An Empirical Study of ADMM for Nonconvex Problems.pdf;C\:\\Users\\msca8h\\Zotero\\storage\\HA444VF8\\1612.html}
}

@inproceedings{xu_image_2011,
  title = {Image Smoothing via L_0 Gradient Minimization},
  booktitle = {Proceedings of the 2011 {{SIGGRAPH Asia Conference}} on - {{SA}} '11},
  author = {Xu, Li and Lu, Cewu and Xu, Yi and Jia, Jiaya},
  year = {2011},
  pages = {1},
  publisher = {{ACM Press}},
  address = {{Hong Kong, China}},
  langid = {english},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\E9JC4ZNR\\Xu et al. - 2011 - Image smoothing via L 0 gradient.pdf}
}

@article{xu_improved_2018,
  title = {Improved Bilateral Texture Filtering with Edge-Aware Measurement},
  author = {Xu, Panpan and Wang, Wencheng},
  year = {2018},
  month = jul,
  journal = {IEEE Transactions on Image Processing},
  volume = {27},
  number = {7},
  pages = {3621--3630}
}

@inproceedings{xu_variance_2019,
  title = {Variance Reduction Properties of the Reparameterization Trick},
  booktitle = {Proceedings of the {{International Conference}} on {{Artificial Intelligence}} and {{Statistics}}},
  author = {Xu, Ming and Quiroz, Matias and Kohn, Robert and Sisson, Scott A.},
  year = {2019},
  month = apr,
  series = {{{PMLR}}},
  volume = {89},
  pages = {2711--2720},
  publisher = {{ML Research Press}},
  abstract = {The reparameterization trick is widely used in variational inference as it yields more accurate estimates of the gradient of the variational objective than alternative approaches such as the score function method. Although there is overwhelming empirical evidence in the literature showing its success, there is relatively little research exploring why the reparameterization trick is so effective. We explore this under the idealized assumptions that the variational approximation is a mean-field Gaussian density and that the log of the joint density of the model parameters and the data is a quadratic function that depends on the variational mean. From this, we show that the marginal variances of the reparameterization gradient estimator are smaller than those of the score function gradient estimator. We apply the result of our idealized analysis to real-world examples.},
  langid = {english},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\AI69ESMZ\\Xu et al. - 2019 - Variance reduction properties of the reparameteriz.pdf}
}

@article{xue_doubleparallel_2019,
  title = {Double-{{Parallel Monte Carlo}} for {{Bayesian}} Analysis of Big Data},
  author = {Xue, Jingnan and Liang, Faming},
  year = {2019},
  month = jan,
  journal = {Statistics and Computing},
  volume = {29},
  number = {1},
  pages = {23--32},
  langid = {english},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\X8G24AK4\\Xue and Liang - 2019 - Double-Parallel Monte Carlo for Bayesian analysis .pdf}
}

@article{xulizong_speckle_1998,
  title = {Speckle Reduction and Contrast Enhancement of Echocardiograms via Multiscale Nonlinear Processing},
  author = {{Xuli Zong} and Laine, A.F. and Geiser, E.A.},
  year = {Aug./1998},
  journal = {IEEE Transactions on Medical Imaging},
  volume = {17},
  number = {4},
  pages = {532--540},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\9VABFKRJ\\Xuli Zong et al. - 1998 - Speckle reduction and contrast enhancement of echo.pdf}
}

@article{yan_bayesian_2018,
  title = {Bayesian {{Optimization Based}} on {{K-Optimality}}},
  author = {Yan, Liang and Duan, Xiaojun and Liu, Bowen and Xu, Jin},
  year = {2018},
  journal = {Entropy},
  volume = {20},
  number = {8},
  pages = {594}
}

@inproceedings{yan_stabilizing_2020,
  title = {Towards {{Stabilizing Batch Statistics}} in {{Backward Propagation}} of {{Batch Normalization}}},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {Yan, Junjie and Wan, Ruosi and Zhang, Xiangyu and Zhang, Wei and Wei, Yichen and Sun, Jian},
  year = {2020}
}

@inproceedings{yan_unified_2018,
  title = {A Unified Analysis of Stochastic Momentum Methods for Deep Learning},
  booktitle = {Proceedings of the {{International Joint Conference}} on {{Artificial Intelligence}}},
  author = {Yan, Yan and Yang, Tianbao and Li, Zhe and Lin, Qihang and Yang, Yi},
  year = {2018},
  month = jul,
  pages = {2955--2961},
  publisher = {{International Joint Conferences on Artificial Intelligence Organization}},
  address = {{Stockholm, Sweden}},
  abstract = {Stochastic momentum methods have been widely adopted in training deep neural networks. However, their theoretical analysis of convergence of the training objective and the generalization error for prediction is still under-explored. This paper aims to bridge the gap between practice and theory by analyzing the stochastic gradient (SG) method, and the stochastic momentum methods including two famous variants, i.e., the stochastic heavy-ball (SHB) method and the stochastic variant of Nesterov?s accelerated gradient (SNAG) method. We propose a framework that unifies the three variants. We then derive the convergence rates of the norm of gradient for the non-convex optimization problem, and analyze the generalization performance through the uniform stability approach. Particularly, the convergence analysis of the training objective exhibits that SHB and SNAG have no advantage over SG. However, the stability analysis shows that the momentum term can improve the stability of the learned model and hence improve the generalization performance. These theoretical insights verify the common wisdom and are also corroborated by our empirical analysis on deep learning.},
  langid = {english},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\5Q5NXQ6T\\Yan et al. - 2018 - A Unified Analysis of Stochastic Momentum Methods .pdf}
}

@article{yang_parallelizable_2018,
  title = {On Parallelizable {{Markov}} Chain {{Monte Carlo}} Algorithms with Waste-Recycling},
  author = {Yang, Shihao and Chen, Yang and Bernton, Espen and Liu, Jun S.},
  year = {2018},
  month = sep,
  journal = {Statistics and Computing},
  volume = {28},
  number = {5},
  pages = {1073--1081},
  langid = {english}
}

@article{yang_sparse_2020,
  title = {Sparse {{Bayesian DOA}} Estimation Using Hierarchical Synthesis Lasso Priors for Off-Grid Signals},
  author = {Yang, Jie and Yang, Yixin},
  year = {2020},
  journal = {IEEE Transactions on Signal Processing},
  volume = {68},
  pages = {872--884}
}

@inproceedings{yangyang_rumr_2003,
  title = {{{RUMR}}: Robust Scheduling for Divisible Workloads},
  booktitle = {High {{Performance Distributed Computing}}, 2003. {{Proceedings}}. 12th {{IEEE International Symposium}} On},
  author = {{Yang Yang} and Casanova, H.},
  year = {2003},
  month = jun,
  pages = {114--123},
  keywords = {Application software,Clustering algorithms,computer networks,Computer science,Delay,Dictionaries,factoring-based scheduling,Image segmentation,multiround divisible workload scheduling,parallel algorithms,performance evaluation,performance prediction errors,prediction theory,processor scheduling,Processor scheduling,real-world applications,real-world performance,robust scheduling,robust uniform multiround,Robustness,RUMR,scheduling algorithm,Signal processing algorithms,Supercomputers}
}

@inproceedings{yao_yes_2018,
  title = {Yes, but Did It Work?: {{Evaluating}} Variational Inference},
  shorttitle = {Yes, but Did It Work?},
  booktitle = {Proceedings of the {{International Conference}} on {{Machine Learning}}},
  author = {Yao, Yuling and Vehtari, Aki and Simpson, Daniel and Gelman, Andrew},
  year = {2018},
  month = jul,
  series = {{{PMLR}}},
  pages = {5581--5590},
  publisher = {{ML Research Press}},
  abstract = {While it's always possible to compute a variational approximation to a posterior distribution, it can be difficult to discover problems with this approximation. We propose two diagnostic algorithms to alleviate this problem. The Pareto-smoothed importance sampling (PSIS) diagnostic gives a goodness of fit measurement for joint distributions, while simultaneously improving the error in the estimate. The variational simulation-based calibration (VSBC) assesses the average performance of point estimates.},
  langid = {english},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\69F2M9ZA\\Yao et al. - 2018 - Yes, but Did It Work Evaluating Variational Infe.pdf;C\:\\Users\\msca8h\\Zotero\\storage\\8FQTR4IH\\Yao et al. - 2018 - Yes, but Did It Work Evaluating Variational Infe.pdf}
}

@article{yashtini_global_2016,
  title = {On the Global Convergence Rate of the Gradient Descent Method for Functions with {{H\"older}} Continuous Gradients},
  author = {Yashtini, Maryam},
  year = {2016},
  month = aug,
  journal = {Optimization Letters},
  volume = {10},
  number = {6},
  pages = {1361--1370},
  langid = {english}
}

@inproceedings{ye_active_2014,
  title = {Active {{Sampling}} for {{Subjective Image Quality Assessment}}},
  booktitle = {2014 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Ye, Peng and Doermann, David},
  year = {2014},
  month = jun,
  pages = {4249--4256},
  publisher = {{IEEE}},
  address = {{Columbus, OH, USA}}
}

@article{yeo-sunyoon_tops_2006a,
  title = {{{TOPS}}: New {{DOA}} Estimator for Wideband Signals},
  shorttitle = {{{TOPS}}},
  author = {{Yeo-Sun Yoon} and Kaplan, L.M. and McClellan, J.H.},
  year = {2006},
  month = jun,
  journal = {IEEE Transactions on Signal Processing},
  volume = {54},
  number = {6},
  pages = {1977--1989}
}

@article{yildirim_exact_2019,
  title = {Exact {{MCMC}} with Differentially Private Moves},
  author = {Y{\i}ld{\i}r{\i}m, Sinan and Ermi{\c s}, Beyza},
  year = {2019},
  month = sep,
  journal = {Statistics and Computing},
  volume = {29},
  number = {5},
  pages = {947--963},
  abstract = {We view the penalty algorithm of Ceperley and Dewing (J Chem Phys 110(20):9812\textendash 9820, 1999), a Markov chain Monte Carlo algorithm for Bayesian inference, in the context of data privacy. Specifically, we studied differential privacy of the penalty algorithm and advocate its use for data privacy. The algorithm can be made differentially private while remaining exact in the sense that its target distribution is the true posterior distribution conditioned on the private data. We also show that in a model with independent observations the algorithm has desirable convergence and privacy properties that scale with data size. Two special cases are also investigated and privacy-preserving schemes are proposed for those cases: (i) Data are distributed among several users who are interested in the inference of a common parameter while preserving their data privacy. (ii) The data likelihood belongs to an exponential family. The results of our numerical experiments on the Beta-Bernoulli and the logistic regression models agree with the theoretical results.},
  langid = {english},
  keywords = {Differential privacy,Markov chain Monte Carlo,Penalty algorithm},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\DRVV5R4H\\Yldrm and Ermi - 2019 - Exact MCMC with differentially private moves.pdf}
}

@article{yilmaz_autotuning_2016,
  title = {Autotuning {{Runtime Specialization}} for {{Sparse Matrix-Vector Multiplication}}},
  author = {Yilmaz, Buse and Aktemur, Bari{\c s} and Garzar{\'a}n, Mar{\'i}A J. and Kamin, Sam and Kira{\c c}, Furkan},
  year = {2016},
  month = mar,
  journal = {ACM Transactions on Architecture and Code Optimization},
  volume = {13},
  number = {1},
  pages = {1--26},
  langid = {english},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\WMYGGYEJ\\Yilmaz et al. - 2016 - Autotuning Runtime Specialization for Sparse Matri.pdf}
}

@article{yocom_bayesian_2011,
  title = {A {{Bayesian}} Approach to Passive Sonar Detection and Tracking in the Presence of Interferers},
  author = {Yocom, Bryan A. and La Cour, Brian R. and Yudichak, Thomas W.},
  year = {2011},
  month = jul,
  journal = {IEEE Journal of Oceanic Engineering},
  volume = {36},
  number = {3},
  pages = {386--405}
}

@inproceedings{yongjianyu_generalized_2004,
  title = {Generalized Speckle Reducing Anisotropic Diffusion for Ultrasound Imagery},
  booktitle = {Proc. {{IEEE Symp}}. {{Comput}}.-{{Based Med}}. {{Syst}}.},
  author = {{Yongjian Yu} and Molloy, J.A. and Acton, S.T.},
  year = {2004},
  series = {{{CBMS}}'04},
  pages = {279--284},
  publisher = {{IEEE}},
  address = {{Bethesda, MD, USA}}
}

@article{yongjianyu_speckle_2002,
  title = {Speckle Reducing Anisotropic Diffusion},
  author = {{Yongjian Yu} and Acton, S.T.},
  year = {2002},
  month = nov,
  journal = {IEEE Transactions on Image Processing},
  volume = {11},
  number = {11},
  pages = {1260--1270},
  langid = {english},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\LQYNIKXQ\\Yongjian Yu and Acton - 2002 - Speckle reducing anisotropic diffusion.pdf}
}

@article{yongyue_nonlinear_2006,
  title = {Nonlinear Multiscale Wavelet Diffusion for Speckle Suppression and Edge Enhancement in Ultrasound Images},
  author = {{Yong Yue} and Croitoru, M.M. and Bidani, A. and Zwischenberger, J.B. and Clark, J.W.},
  year = {2006},
  month = mar,
  journal = {IEEE Transactions on Medical Imaging},
  volume = {25},
  number = {3},
  pages = {297--311}
}

@article{yoon_tops_2006,
  title = {{{TOPS}}: New {{DOA}} Estimator for Wideband Signals},
  shorttitle = {{{TOPS}}},
  author = {Yoon, Yeo-Sun and Kaplan, L.M. and McClellan, J.H.},
  year = {2006},
  month = jun,
  journal = {IEEE Transactions on Signal Processing},
  volume = {54},
  number = {6},
  pages = {1977--1989}
}

@inproceedings{you_imagenet_2018,
  title = {{{ImageNet Training}} in {{Minutes}}},
  booktitle = {Proceedings of the 47th {{International Conference}} on {{Parallel Processing}}  - {{ICPP}} 2018},
  author = {You, Yang and Zhang, Zhao and Hsieh, Cho-Jui and Demmel, James and Keutzer, Kurt},
  year = {2018},
  pages = {1--10},
  publisher = {{ACM Press}},
  address = {{Eugene, OR, USA}},
  langid = {english},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\SUGW4MTR\\You et al. - 2018 - ImageNet Training in Minutes.pdf}
}

@inproceedings{yu_analysis_2021,
  title = {An Analysis of Constant Step Size {{SGD}} in the Non-Convex Regime: Asymptotic Normality and Bias},
  shorttitle = {An Analysis of Constant Step Size {{SGD}} in the Non-Convex Regime},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Yu, Lu and Balasubramanian, Krishnakumar and Volgushev, Stanislav and Erdogdu, Murat A},
  year = {2021},
  volume = {34},
  pages = {4234--4248},
  publisher = {{Curran Associates, Inc.}},
  abstract = {Structured non-convex learning problems, for which critical points have favorable statistical properties, arise frequently in statistical machine learning. Algorithmic convergence and statistical estimation rates are well-understood for such problems. However, quantifying the uncertainty associated with the underlying training algorithm is not well-studied in the non-convex setting. In order to address this shortcoming, in this work, we establish an asymptotic normality result for the constant step size stochastic gradient descent (SGD)  algorithm---a widely used algorithm in practice. Specifically, based on the relationship between SGD and Markov Chains  [DDB19], we show that the average of SGD iterates is asymptotically normally distributed around the expected value of their unique invariant distribution, as long as the non-convex and non-smooth objective function satisfies a dissipativity property. We also characterize the bias between this expected value and the critical points of the objective function under various local regularity conditions. Together, the above two results could be leveraged to construct confidence intervals for non-convex problems that are trained using the SGD algorithm.},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\S6T5C4AR\\Yu et al. - 2021 - An Analysis of Constant Step Size SGD in the Non-c.pdf}
}

@inproceedings{yu_parallel_2019,
  title = {Parallel {{Asynchronous Stochastic Coordinate Descent}} with {{Auxiliary Variables}}},
  booktitle = {Proceedings of {{Machine Learning Research}}},
  author = {Yu, Hsiang-Fu and Hsieh, Cho-Jui and Dhillon, Inderjit S.},
  year = {2019},
  month = apr,
  series = {Proceedings of {{Machine Learning Research}}},
  volume = {89},
  pages = {2641--2649},
  publisher = {{PMLR}},
  abstract = {The key to the recent success of coordinate descent (CD) in many applications is to maintain a set of auxiliary variables to facilitate efficient single variable updates. For example, the vector of residual/primal variables has to be maintained when CD is applied for Lasso/linear SVM, respectively. An implementation without maintenance is \$O(n)\$ times slower than the one with maintenance, where n is the number of variables. In serial implementations, maintaining auxiliary variables is only a computing trick without changing the behavior of coordinate descent. However, maintenance of auxiliary variables is non-trivial when there are multiple threads/workers which read/write the auxiliary variables concurrently. Thus, most existing theoretical analysis of parallel CD either assumes vanilla CD without auxiliary variables (which ends up being extremely slow in practice) or limits to a small class of problems. In this paper, we consider a rich family of objective functions where AUX-PCD can be applied. We also establish global linear convergence for AUX-PCD with atomic operations for a general family of functions and perform a complete backward error analysis of AUX-PCD with wild updates, where some updates are not just delayed but lost because of memory conflicts. Our results enable us to provide theoretical guarantees for many practical parallel coordinate descent implementations, which currently lack guarantees (such as the implementation of Shotgun by Bradley et al. 2011, which uses auxiliary variables)}
}

@article{yu_stochastic_2017,
  title = {Stochastic {{Variational Inference}} for {{Bayesian Sparse Gaussian Process Regression}}},
  author = {Yu, Haibin and Hoang, Trong Nghia and Low, Kian Hsiang and Jaillet, Patrick},
  year = {2017},
  month = nov,
  journal = {arXiv:1711.00221 [cs, stat]},
  eprint = {1711.00221},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {This paper presents a novel variational inference framework for deriving a family of Bayesian sparse Gaussian process regression (SGPR) models whose approximations are variationally optimal with respect to the full-rank GPR model enriched with various corresponding correlation structures of the observation noises. Our variational Bayesian SGPR (VBSGPR) models jointly treat both the distributions of the inducing variables and hyperparameters as variational parameters, which enables the decomposability of the variational lower bound that in turn can be exploited for stochastic optimization. Such a stochastic optimization involves iteratively following the stochastic gradient of the variational lower bound to improve its estimates of the optimal variational distributions of the inducing variables and hyperparameters (and hence the predictive distribution) of our VBSGPR models and is guaranteed to achieve asymptotic convergence to them. We show that the stochastic gradient is an unbiased estimator of the exact gradient and can be computed in constant time per iteration, hence achieving scalability to big data. We empirically evaluate the performance of our proposed framework on two real-world, massive datasets.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\3QZVLUIV\\Yu et al. - 2017 - Stochastic Variational Inference for Bayesian Spar.pdf;C\:\\Users\\msca8h\\Zotero\\storage\\63GBFM8G\\1711.html}
}

@article{yu_ultrasound_2010,
  title = {Ultrasound Speckle Reduction by a {{SUSAN-controlled}} Anisotropic Diffusion Method},
  author = {Yu, Jinhua and Tan, Jinglu and Wang, Yuanyuan},
  year = {2010},
  month = sep,
  journal = {Pattern Recognition},
  volume = {43},
  number = {9},
  pages = {3083--3092},
  langid = {english}
}

@incollection{yue_designing_1997,
  title = {Designing {{Multiprocessor Scheduling Algorithms Using}} a {{Distributed Genetic Algorithm System}}},
  booktitle = {Evolutionary {{Algorithms}} in {{Engineering Applications}}},
  author = {Yue, Kelvin K. and Lilja, David J.},
  year = {1997},
  pages = {207--222},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  langid = {english}
}

@article{yue_parallel_1995,
  title = {Parallel Loop Scheduling for High Performance Computers},
  author = {Yue, Kelvin K. and Lilja, David J.},
  year = {1995},
  volume = {10},
  pages = {243--264},
  keywords = {Analytical modeling,Parallel loop scheduling,Performance analysis,Scalability,Shared memory multiprocessor},
  file = {/home/msca8h/Documents/parallel_scheduling/Yue and Lilja - 1995 - Parallel Loop Scheduling for High Performance Comp.pdf}
}

@article{zang_guided_2015,
  title = {Guided Adaptive Image Smoothing via Directional Anisotropic Structure Measurement},
  author = {Zang, Yu and Huang, Hua and Zhang, Lei},
  year = {2015},
  month = sep,
  journal = {IEEE Transactions on Visualization and Computer Graphics},
  volume = {21},
  number = {9},
  pages = {1015--1027}
}

@inproceedings{zatman_how_1997,
  title = {How Narrow Is Narrowband? [Adaptive Array Signal Processing]},
  shorttitle = {How Narrow Is Narrowband?},
  booktitle = {Conf. {{Record Asilomar Conf}}. on {{Signals}}, {{Syst}}. {{Comput}}.},
  author = {Zatman, M.},
  year = {1997},
  volume = {2},
  pages = {1341--1345},
  publisher = {{IEEE Comput. Soc}},
  address = {{Pacific Grove, CA, USA}}
}

@incollection{zellner_assessing_1986,
  title = {On Assessing Prior Distributions and {{Bayesian}} Regression Analysis with g Prior Distributions},
  booktitle = {Bayesian {{Inference}} and {{Decision Techniques}}: {{Essays}} in {{Honor}} of {{Bruno}} de {{Finetti}}},
  author = {Zellner, Arnold},
  year = {1986},
  month = jun,
  series = {Studies in {{Bayesian Econometrics}} and {{Statistics}}},
  number = {6},
  pages = {233--243},
  publisher = {{Elsevier}},
  address = {{New York}}
}

@article{zellner_posterior_1980,
  title = {Posterior Odds Ratios for Selected Regression Hypotheses},
  author = {Zellner, A. and Siow, A.},
  year = {1980},
  month = feb,
  journal = {Trabajos de Estadistica Y de Investigacion Operativa},
  volume = {31},
  number = {1},
  pages = {585--603},
  langid = {english}
}

@article{zeng_nonconvex_2018,
  title = {On {{Nonconvex Decentralized Gradient Descent}}},
  author = {Zeng, Jinshan and Yin, Wotao},
  year = {2018},
  month = jun,
  journal = {IEEE Transactions on Signal Processing},
  volume = {66},
  number = {11},
  pages = {2834--2848},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\FZVRSM9D\\Zeng and Yin - 2018 - On Nonconvex Decentralized Gradient Descent.pdf}
}

@inproceedings{zhang_adam_2022,
  title = {Adam Can Converge without Any Modification on Update Rules},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Zhang, Yushun and Chen, Congliang and Shi, Naichen and Sun, Ruoyu and Luo, Zhi-Quan},
  year = {2022}
}

@article{zhang_advances_2019,
  title = {Advances in Variational Inference},
  author = {Zhang, Cheng and Butepage, Judith and Kjellstrom, Hedvig and Mandt, Stephan},
  year = {2019},
  month = aug,
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {41},
  number = {8},
  pages = {2008--2026},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\TVNGVPC9\\Zhang et al. - 2019 - Advances in Variational Inference.pdf}
}

@article{zhang_autogeneration_2013,
  title = {Autogeneration and {{Autotuning}} of {{3D Stencil Codes}} on {{Homogeneous}} and {{Heterogeneous GPU Clusters}}},
  author = {Zhang, Yongpeng and Mueller, Frank},
  year = {2013},
  month = mar,
  journal = {IEEE Transactions on Parallel and Distributed Systems},
  volume = {24},
  number = {3},
  pages = {417--427}
}

@article{zhang_clutter_2020,
  title = {Clutter Suppression in Ultrasound: Performance Evaluation and Review of Low-Rank and Sparse Matrix Decomposition Methods},
  shorttitle = {Clutter Suppression in Ultrasound},
  author = {Zhang, Naiyuan and Ashikuzzaman, Md and Rivaz, Hassan},
  year = {2020},
  month = may,
  journal = {BioMedical Engineering OnLine},
  volume = {19},
  number = {1},
  pages = {37},
  abstract = {Vessel diseases are often accompanied by abnormalities related to vascular shape and size. Therefore, a clear visualization of vasculature is of high clinical significance. Ultrasound color flow imaging (CFI) is one of the prominent techniques for flow visualization. However, clutter signals originating from slow-moving tissue are one of the main obstacles to obtain a clear view of the vascular network. Enhancement of the vasculature by suppressing the clutters is a significant and irreplaceable step for many applications of ultrasound CFI. Currently, this task is often performed by singular value decomposition (SVD) of the data matrix. This approach exhibits two well-known limitations. First, the performance of SVD is sensitive to the proper manual selection of the ranks corresponding to clutter and blood subspaces. Second, SVD is prone to failure in the presence of large random noise in the dataset. A potential solution to these issues is using decomposition into low-rank and sparse matrices (DLSM) framework. SVD is one of the algorithms for solving the minimization problem under the DLSM framework. Many other algorithms under DLSM avoid full SVD and use approximated SVD or SVD-free ideas which may have better performance with higher robustness and less computing time. In practice, these models separate blood from clutter based on the assumption that steady clutter represents a low-rank structure and that the moving blood component is sparse. In this paper, we present a comprehensive review of ultrasound clutter suppression techniques and exploit the feasibility of low-rank and sparse decomposition schemes in ultrasound clutter suppression. We conduct this review study by adapting 106 DLSM algorithms and validating them against simulation, phantom, and in vivo rat datasets. Two conventional quality metrics, signal-to-noise ratio (SNR) and contrast-to-noise ratio (CNR), are used for performance evaluation. In addition, computation times required by different algorithms for generating clutter suppressed images are reported. Our extensive analysis shows that the DLSM framework can be successfully applied to ultrasound clutter suppression.}
}

@inproceedings{zhang_cyclical_2020,
  title = {Cyclical Stochastic Gradient {{MCMC}} for {{Bayesian}} Deep Learning},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {Zhang, Ruqi and Li, Chunyuan and Zhang, Jianyi and Chen, Changyou and Wilson, Andrew Gordon},
  year = {2020}
}

@inproceedings{zhang_differentiable_2021,
  title = {Differentiable Annealed Importance Sampling and the Perils of Gradient Noise},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Zhang, Guodong and Hsu, Kyle and Li, Jianing and Finn, Chelsea and Grosse, Roger B},
  year = {2021},
  volume = {34},
  pages = {19398--19410},
  publisher = {{Curran Associates, Inc.}}
}

@article{zhang_differential_2016,
  title = {On the Differential Privacy of {{Bayesian}} Inference},
  author = {Zhang, Zuhe and Rubinstein, Benjamin and Dimitrakakis, Christos},
  year = {2016},
  month = mar,
  journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {30},
  number = {1},
  abstract = {We study how to communicate findings of Bayesian inference to third parties, while preserving the strong guarantee of differential privacy. Our main contributions are four different algorithms for private Bayesian inference on probabilistic graphical models. These include two mechanisms for adding noise to the Bayesian updates, either directly to the posterior parameters, or to their Fourier transform so as to preserve update consistency. We also utilise a recently introduced posterior sampling mechanism, for which we prove bounds for the specific but general case of discrete Bayesian networks; and we introduce a maximum-a-posteriori private mechanism. Our analysis includes utility and privacy bounds, with a novel focus on the influence of graph structure on privacy. Worked examples and experiments with Bayesian naive Bayes and Bayesian linear regression illustrate the application of our mechanisms.},
  copyright = {Copyright (c)},
  langid = {english},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\SF93ILUL\\Zhang et al. - 2016 - On the Differential Privacy of Bayesian Inference.pdf}
}

@inproceedings{zhang_extreme_2019,
  title = {Extreme {{Stochastic Variational Inference}}: {{Distributed Inference}} for {{Large Scale Mixture Models}}},
  booktitle = {Proceedings of {{Machine Learning Research}}},
  author = {Zhang, Jiong and Raman, Parameswaran and Ji, Shihao and Yu, Hsiang-Fu and Vishwanathan, S.V.N. and Dhillon, Inderjit},
  year = {2019},
  month = apr,
  series = {Proceedings of {{Machine Learning Research}}},
  volume = {89},
  pages = {935--943},
  publisher = {{PMLR}},
  abstract = {Mixture of exponential family models are among the most fundamental and widely used statistical models. Stochastic variational inference (SVI), the state-of-the-art algorithm for parameter estimation in such models is inherently serial. Moreover, it requires the parameters to fit in the memory of a single processor; this poses serious limitations on scalability when the number of parameters is in billions. In this paper, we present extreme stochastic variational inference (ESVI), a distributed, asynchronous and lock-free algorithm to perform variational inference for mixture models on massive real world datasets. ESVI overcomes the limitations of SVI by requiring that each processor only access a subset of the data and a subset of the parameters, thus providing data and model parallelism simultaneously. Our empirical study demonstrates that ESVI not only outperforms VI and SVI in wallclock-time, but also achieves a better quality solution. To further speed up computation and save memory when fitting large number of topics, we propose a variant ESVI-TOPK which maintains only the top-k important topics. Empirically, we found that using top 25\% topics suffices to achieve the same accuracy as storing all the topics.}
}

@article{zhang_inconsistent_2004,
  title = {Inconsistent Estimation and Asymptotically Equal Interpolations in Model-Based Geostatistics},
  author = {Zhang, Hao},
  year = {2004},
  month = mar,
  journal = {Journal of the American Statistical Association},
  volume = {99},
  number = {465},
  pages = {250--261},
  langid = {english}
}

@inproceedings{zhang_multiscale_2006,
  title = {Multiscale Nonlinear Diffusion and Shock Filter for Ultrasound Image Enhancement},
  booktitle = {Proc. {{IEEE Comput}}. {{Soc}}. {{Conf}}. {{Comput}}. {{Vision Pattern Recognit}}.},
  author = {Zhang, Fan and Yoo, Yang Mo and Kim, Yongmin and Zhang, Lichen and Koh, Liang Mong},
  year = {2006},
  series = {{{CVPR}}'06},
  volume = {2},
  pages = {1972--1977},
  publisher = {{IEEE}},
  address = {{New York, NY, USA}}
}

@article{zhang_musiclike_2010,
  title = {{{MUSIC-like DOA}} Estimation without Estimating the Number of Sources},
  author = {Zhang, Ying and Ng, Boon Poh},
  year = {2010},
  month = mar,
  journal = {IEEE Transactions on Signal Processing},
  volume = {58},
  number = {3},
  pages = {1668--1676}
}

@article{zhang_nonlinear_2007,
  title = {Nonlinear {{Diffusion}} in {{Laplacian Pyramid Domain}} for {{Ultrasonic Speckle Reduction}}},
  author = {Zhang, Fan and Yoo, Yang Mo and Koh, Liang Mong and Kim, Yongmin},
  year = {2007},
  month = feb,
  journal = {IEEE Transactions on Medical Imaging},
  volume = {26},
  number = {2},
  pages = {200--211}
}

@article{zhang_nonparametric_1996,
  title = {Nonparametric Importance Sampling},
  author = {Zhang, Ping},
  year = {1996},
  month = sep,
  journal = {Journal of the American Statistical Association},
  volume = {91},
  number = {435},
  pages = {1245--1253},
  langid = {english}
}

@article{zhang_optimizing_2020,
  title = {Optimizing {{Streaming Parallelism}} on {{Heterogeneous Many-Core Architectures}}},
  author = {Zhang, Peng and Fang, Jianbin and Yang, Canqun and Huang, Chun and Tang, Tao and Wang, Zheng},
  year = {2020},
  month = aug,
  journal = {IEEE Transactions on Parallel and Distributed Systems},
  volume = {31},
  number = {8},
  pages = {1878--1896},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\M4S6NCVQ\\Zhang et al. - 2020 - Optimizing Streaming Parallelism on Heterogeneous .pdf}
}

@inproceedings{zhang_rectified_2015,
  title = {Rectified Linear Neural Networks with Tied-Scalar Regularization for {{LVCSR}}},
  booktitle = {{{INTERSPEECH}} 2015, 16th {{Annual Conference}} of the {{International Speech Communication Association}}, {{Dresden}}, {{Germany}}, {{September}} 6-10, 2015},
  author = {Zhang, Shiliang and Jiang, Hui and Wei, Si and Dai, Li-Rong},
  year = {2015},
  pages = {2635--2639},
  publisher = {{ISCA}}
}

@incollection{zhang_rolling_2014,
  title = {Rolling Guidance Filter},
  booktitle = {Computer {{Vision}} \textendash{} {{ECCV}} 2014},
  author = {Zhang, Qi and Shen, Xiaoyong and Xu, Li and Jia, Jiaya},
  year = {2014},
  volume = {8691},
  pages = {815--830},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  langid = {english}
}

@misc{zhang_transport_2022,
  title = {Transport Score Climbing: {{Variational}} Inference Using Forward {{KL}} and Adaptive Neural Transport},
  shorttitle = {Transport {{Score Climbing}}},
  author = {Zhang, Liyi and Blei, David M. and Naesseth, Christian A.},
  year = {2022},
  month = sep,
  number = {arXiv:2202.01841},
  eprint = {2202.01841},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  institution = {{arXiv}},
  abstract = {Variational inference often minimizes the "reverse" Kullbeck-Leibler (KL) KL(q||p) from the approximate distribution q to the posterior p. Recent work studies the "forward" KL KL(p||q), which unlike reverse KL does not lead to variational approximations that underestimate uncertainty. This paper introduces Transport Score Climbing (TSC), a method that optimizes KL(p||q) by using Hamiltonian Monte Carlo (HMC) and a novel adaptive transport map. The transport map improves the trajectory of HMC by acting as a change of variable between the latent variable space and a warped space. TSC uses HMC samples to dynamically train the transport map while optimizing KL(p||q). TSC leverages synergies, where better transport maps lead to better HMC sampling, which then leads to better transport maps. We demonstrate TSC on synthetic and real data. We find that TSC achieves competitive performance when training variational autoencoders on large-scale data.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Computation,Statistics - Machine Learning,Statistics - Methodology},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\L9TDECDM\\Zhang et al. - 2022 - Transport Score Climbing Variational Inference Us.pdf;C\:\\Users\\msca8h\\Zotero\\storage\\5SBFL9BH\\2202.html}
}

@article{zhang_variational_2018,
  title = {Variational {{Hamiltonian Monte Carlo}} via Score Matching},
  author = {Zhang, Cheng and Shahbaba, Babak and Zhao, Hongkai},
  year = {2018},
  month = jun,
  journal = {Bayesian Analysis},
  volume = {13},
  number = {2},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\ZMJ29LGF\\Zhang et al. - 2018 - Variational Hamiltonian Monte Carlo via Score Matc.pdf}
}

@inproceedings{zhao_capon_2016,
  title = {Capon Cepstrum Weighted L2, 1 Minimization for Wideband {{DOA}} Estimation with Sonar Arrays},
  booktitle = {{{OCEANS}} 2016 - {{Shanghai}}},
  author = {Zhao, Wenqiang and Li, Gang and Zheng, Chundi and Ge, Fengxiang},
  year = {2016},
  month = apr,
  pages = {1--4},
  publisher = {{IEEE}},
  address = {{Shanghai, China}}
}

@article{zhao_computationally_2017,
  title = {Computationally Efficient Wide-Band {{DOA}} Estimation Methods Based on Sparse {{Bayesian}} Framework},
  author = {Zhao, Lifan and Li, Xiumei and Wang, Lu and Bi, Guoan},
  year = {2017},
  month = dec,
  journal = {IEEE Transactions on Vehicular Technology},
  volume = {66},
  number = {12},
  pages = {11108--11121},
  abstract = {For practical wide-band direction-of-arrival (DOA) estimation problems, the sparse Bayesian algorithms, implemented by expectation maximization or variational Bayesian inference techniques, generally require prohibitive computational complexity. In this paper, computationally efficient wide-band DOA estimation is considered within a sparse Bayesian framework. In particular, two computationally efficient wide-band DOA estimation methods are proposed, i.e., single and multiple observations sparse Bayesian multitask learning methods, denoted as SO-SBMTL and MO-SBMTL. The proposed methods can independently and jointly process the signals to obtain the DOA estimates, respectively, with significantly reduced computational complexity. In addition, the off-the-grid problem in wide-band DOA estimation is also considered within this framework. The remarkable feature of our proposed methods is that the off-the-grid parameter can be desirably obtained with a closed-form solution, so that numerical searching procedure is appropriately avoided and the computational complexity is substantially reduced. Experimental results have demonstrated that the proposed algorithms can achieve desirable performance with substantially reduced computational complexities. The MO-SBMTL achieves better performance than the SO-SBMTL in the scenarios of static or slow time-varying moving sources, and the off-the-grid algorithm can also efficiently estimate the true DOAs of the sources.},
  keywords = {Bayes methods,Computational complexity,Computationally efficient algorithms,Convex functions,Direction-of-arrival estimation,Estimation,Sparse matrices,sparse representation,Tuning,wide-band DOA estimation},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\EPYX9RB2\\Zhao et al. - 2017 - Computationally Efficient Wide-Band DOA Estimation.pdf}
}

@article{zhao_new_2019,
  title = {A New Approach for Medical Image Enhancement Based on Luminance-Level Modulation and Gradient Modulation},
  author = {Zhao, Chenyi and Wang, Zeqi and Li, Huanyu and Wu, Xiaoyang and Qiao, Shuang and Sun, Jianing},
  year = {2019},
  month = feb,
  journal = {Biomedical Signal Processing and Control},
  volume = {48},
  pages = {189--196},
  langid = {english}
}

@article{zheng_joint_2020,
  title = {Joint Towed Array Shape and Direction of Arrivals Estimation Using Sparse {{Bayesian}} Learning during Maneuvering},
  author = {Zheng, Zheng and Yang, T. C. and Gerstoft, Peter and Pan, Xiang},
  year = {2020},
  month = mar,
  journal = {The Journal of the Acoustical Society of America},
  volume = {147},
  number = {3},
  pages = {1738--1751},
  langid = {english},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\W8T24GB2\\Zheng et al. - 2020 - Joint towed array shape and direction of arrivals .pdf}
}

@inproceedings{zheng_towed_2019,
  title = {Towed Array Beamforming Using Sparse {{Bayesian}} Learning during Maneuvering},
  booktitle = {{{OCEANS}} 2019 - {{Marseille}}},
  author = {Zheng, Zheng and Yang, T. C. and Pan, Xiang and Gerstoft, Peter},
  year = {2019},
  month = jun,
  pages = {1--6},
  publisher = {{IEEE}},
  address = {{Marseille, France}}
}

@article{zhou_automatic_2016,
  title = {Toward {{Automatic Model Comparison}}: {{An Adaptive Sequential Monte Carlo Approach}}},
  shorttitle = {Toward {{Automatic Model Comparison}}},
  author = {Zhou, Yan and Johansen, Adam M. and Aston, John A.D.},
  year = {2016},
  month = jul,
  journal = {Journal of Computational and Graphical Statistics},
  volume = {25},
  number = {3},
  pages = {701--726},
  langid = {english},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\CVLPSC38\\Zhou et al. - 2016 - Toward Automatic Model Comparison An Adaptive Seq.pdf}
}

@incollection{zhou_deconstructing_2019,
  title = {Deconstructing {{Lottery Tickets}}: {{Zeros}}, {{Signs}}, and the {{Supermask}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 32},
  author = {Zhou, Hattie and Lan, Janice and Liu, Rosanne and Yosinski, Jason},
  year = {2019},
  pages = {3597--3607},
  publisher = {{Curran Associates, Inc.}}
}

@article{zhu_automatic_2020,
  title = {Automatic Multilabel Electrocardiogram Diagnosis of Heart Rhythm or Conduction Abnormalities with Deep Learning: A Cohort Study},
  shorttitle = {Automatic Multilabel Electrocardiogram Diagnosis of Heart Rhythm or Conduction Abnormalities with Deep Learning},
  author = {Zhu, Hongling and Cheng, Cheng and Yin, Hang and Li, Xingyi and Zuo, Ping and Ding, Jia and Lin, Fan and Wang, Jingyi and Zhou, Beitong and Li, Yonge and Hu, Shouxing and Xiong, Yulong and Wang, Binran and Wan, Guohua and Yang, Xiaoyun and Yuan, Ye},
  year = {2020},
  month = jul,
  journal = {The Lancet Digital Health},
  volume = {2},
  number = {7},
  pages = {e348-e357},
  langid = {english},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\VGZHSA3B\\Zhu et al. - 2020 - Automatic multilabel electrocardiogram diagnosis o.pdf}
}

@article{zhu_big_2017,
  title = {Big {{Learning}} with {{Bayesian}} Methods},
  author = {Zhu, Jun and Chen, Jianfei and Hu, Wenbo and Zhang, Bo},
  year = {2017},
  month = jul,
  journal = {National Science Review},
  volume = {4},
  number = {4},
  pages = {627--651},
  abstract = {Abstract             The explosive growth in data volume and the availability of cheap computing resources have sparked increasing interest in Big learning, an emerging subfield that studies scalable machine learning algorithms, systems and applications with Big Data. Bayesian methods represent one important class of statistical methods for machine learning, with substantial recent developments on adaptive, flexible and scalable Bayesian learning. This article provides a survey of the recent advances in Big learning with Bayesian methods, termed Big Bayesian Learning, including non-parametric Bayesian methods for adaptively inferring model complexity, regularized Bayesian inference for improving the flexibility via posterior regularization, and scalable algorithms and systems based on stochastic subsampling and distributed computing for dealing with large-scale applications. We also provide various new perspectives on the large-scale Bayesian modeling and inference.},
  langid = {english},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\FGGZ9RHM\\Zhu et al. - 2017 - Big Learning with Bayesian methods.pdf}
}

@inproceedings{zhu_nonlocal_2017,
  title = {A Non-Local Low-Rank Framework for Ultrasound Speckle Reduction},
  booktitle = {{{IEEE Conf}}. {{Comput}}. {{Vision Pattern Recognit}}.},
  author = {Zhu, Lei and Fu, Chi-Wing and Brown, Michael S. and Heng, Pheng-Ann},
  year = {2017},
  month = jul,
  series = {{{CVPR}}'07},
  pages = {493--501},
  publisher = {{IEEE}},
  address = {{Honolulu, HI}}
}

@article{zierke_fpga_2010,
  title = {{{FPGA}} Acceleration of the Phylogenetic Likelihood Function for {{Bayesian MCMC}} Inference Methods},
  author = {Zierke, Stephanie and Bakos, Jason D},
  year = {2010},
  month = dec,
  journal = {BMC Bioinformatics},
  volume = {11},
  number = {1},
  pages = {184},
  langid = {english},
  file = {C\:\\Users\\msca8h\\Zotero\\storage\\4FQYKFL8\\Zierke and Bakos - 2010 - FPGA acceleration of the phylogenetic likelihood f.pdf}
}

@article{zijiantang_aliasingfree_2011,
  title = {Aliasing-Free Wideband Beamforming Using Sparse Signal Representation},
  author = {{Zijian Tang} and Blacquiere, G and Leus, G},
  year = {2011},
  month = jul,
  journal = {IEEE Transactions on Signal Processing},
  volume = {59},
  number = {7},
  pages = {3464--3469}
}

@inproceedings{zima_modelguided_2009,
  title = {Model-Guided Autotuning of High-Productivity Languages for Petascale Computing},
  booktitle = {Proceedings of the 18th {{ACM}} International Symposium on {{High}} Performance Distributed Computing - {{HPDC}} '09},
  author = {Zima, Hans and Hall, Mary and Chen, Chun and Chame, Jaqueline},
  year = {2009},
  pages = {151--166},
  publisher = {{ACM Press}},
  address = {{Garching, Germany}},
  langid = {english}
}
