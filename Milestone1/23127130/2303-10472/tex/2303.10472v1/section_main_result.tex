
\section{Main Results}
\vspace{-.5ex}

%\input{thm_elbo_smoothness}

%\input{thm_l0l1_smoothness}

% \vspace{-2.5ex}
% \begin{proof}
%     The derivative of the softplus function is the sigmoid function \(\mathrm{sigmoid}\left(x\right) = 1/\left(1 + e^{-x}\right) \leq 1\).
%     By the mean-value theorem, any function that has bounded derivatives \( \phi^{\prime}\left(x\right) < L\) for all \(x \in \mathbb{R}\) is \(L\)-Lipschitz.
% \end{proof}

%\vspace{-2ex}
\subsection{Key Lemmas}
%\vspace{-.5ex}
The main challenge in studying BBVI is that the gradient of the composed function \(\nabla_{\vlambda} f \left( \vt_{\vlambda} \left( \vu \right) \right)\) is different from \(\nabla f\).
For the matrix square root parameterization, \citet[Lemma 1]{domke_provable_2019} established the connection through \cref{thm:variational_gradient_norm_identity}.
We generalize this result to nonlinear parameterizations:

\input{lemma_gradient_norm_identity}

\cref{thm:general_variational_gradient_norm_identity} act as the interface between the properties of the parameterization and the likelihood \(f\).

\begin{remark}[\textbf{Variance Reduction Through \(\phi\)}]
  A \textit{nonlinear} Cholesky parameterization with a 1-Lipschitz \(\phi\) achieves lower or equal variance compared to the matrix square root and \textit{linear} Cholesky, where the equality is achieved with the matrix square root parameterization.
\end{remark}

\vspace{-1.5ex}%
\paragraph{Dimension Dependence of Mean-Field}
The superior dimensional dependence of the mean-field parameterization is given by the following lemma:

\input{lemma_meanfield_param_squared_norm}

\begin{remark}[\textbf{Superior Variance of Mean-Field}]\label{remark:meanfield_superiority}
  The mean-field parameterization has {\small\(\mathcal{O}\left(\sqrt{d}\right)\)} dimensional dependence compared to the \(\mathcal{O}\left(d\right)\) dimensional dependence of the full-rank parameterizations in \cref{thm:reparam_u_identity}.
\end{remark}

\input{lemmas_common}

Lastly, the following lemma is the basic building block for all of our upper bounds:

\input{lemma_gradient_variance_general_upper_bound}

\input{lemmas_gradient_upper_bound}

%^\vspace{-1ex}
\subsection{Upper Bounds}\label{section:upper_bound}
\vspace{-.5ex}

We restrict our analysis to the class of log-likelihoods that satisfy the following conditions: 
\vspace{.5ex}
\begin{definition}[\textbf{\(L\)-smoothness}]\label{def:L_smoothness}
  A function \(f : \mathbb{R}^d \rightarrow \mathbb{R}\) is \(L\)-smooth if it satisfies the following for all \(\vzeta, \vzeta^{\prime} \in \mathbb{R}^d\):
  {\small%
  \setlength{\belowdisplayskip}{1.ex} \setlength{\belowdisplayshortskip}{1.ex}%
  \setlength{\abovedisplayskip}{1.ex} \setlength{\abovedisplayshortskip}{1.ex}%
  \begin{align*}
    {\lVert \nabla f\left(\vzeta\right) - \nabla f\left(\vzeta^{\prime}\right) \rVert}_2
    \leq
    L \, {\lVert \vzeta - \vzeta^{\prime} \rVert}_2.
  \end{align*}
  }%
\end{definition}
%

\vspace{.5ex}
\begin{definition}[\textbf{Quadratic Functional Growth}]\label{def:quadratic_growth}
  A function \(f : \mathbb{R}^d \rightarrow \mathbb{R}\) is \(\mu\)-quadratically growing if
  {\small%
  \setlength{\belowdisplayskip}{1.ex} \setlength{\belowdisplayshortskip}{1.ex}%
  \setlength{\abovedisplayskip}{1.ex} \setlength{\abovedisplayshortskip}{1.ex}%
  \begin{align*}
   \frac{\mu}{2} {\lVert \vzeta - \bar{\vzeta} \rVert}_2^2
    \leq
    f\left(\vzeta\right) - f^*
  \end{align*}
  }%
  for all \(\vzeta \in \mathbb{R}^d\), where \(\bar{\vzeta} \in \mathbb{R}^d\) is an arbitrary stationary point of \(f\) and \(f^* = \inf_{\vzeta \in \mathbb{R}^d} f\left(\vzeta\right)\).
\end{definition}
\vspace{-1ex}
%
For instance, \(\mu\)-strongly (quasar) convex functions~\citep{hinder_nearoptimal_2020,jin_convergence_2020} satisfy \cref{def:quadratic_growth}, but our analysis does \textit{not} require (quasar) convexity.

Both assumptions are commonly used in SGD.
For studying the gradient variance of BBVI, assuming both smoothness and quadratic growth is weaker than the assumptions of \citet{xu_variance_2019} but stronger than those of~\citet{domke_provable_2019}, who assumed only smoothness.
The additional assumption on growth is necessary to extend his results to establish the \textit{ABC} condition.

For the variational family, we assume the followings:
\vspace{.5ex}
\begin{assumption}\label{assumption:q}
\(q_{\psi,\vlambda}\) is a member of the ADVI family (\cref{def:advi}), where the underlying  \(q_{\vlambda}\) is a member of the location-scale family (\cref{def:family}) with its base distribution \(\varphi\) satisfying \cref{assumption:symmetric_standard}.
\end{assumption}

%% \todo[inline]{
%%   Obtaining a similar upper bound with the condition below is still a major goal.
%%   But we'll probably be able to publish this even if we don't succeed in H\"older-generalizing.
%% }
%% \begin{assumption*}{\textbf{(H\"older-smoothness)}}\label{assumption:holder_smoothness}
%%   A function \(f\) is \((L, \alpha)\)-H\"older-smooth if it satisfies
%%   \begin{align*}
%%     \norm{ \nabla f\left(\vz\right) - \nabla f\left(\vz^{\prime}\right) }_2
%%     \leq
%%     L \, \norm{\vz - \vz^{\prime}}_2^{\alpha}.
%%   \end{align*}
%% \end{assumption*}

%% \begin{assumption*}{\textbf{(H\"older-growth)}}\label{assumption:holder_growth}
%%   A function \(f\) satisfies \((L, \alpha)\)-H\"older-growth if it satisfies
%%   \begin{align*}
%%     \norm{ \vz - \vz^* }_2^{2 \alpha}
%%     \leq
%%     f\left(\vz\right),
%%   \end{align*}
%%   or
%%   \begin{align*}
%%     \norm{ \vz - \vz^* }_2^{1 + \alpha}
%%     \leq
%%     f\left(\vz\right),
%%   \end{align*}
%%   where \(\vz^*\) is a stationary point of \(f\) such that \(\nabla f\left(\vz^*\right) = \mathbf{0}\).
%% \end{assumption*}

\paragraph{Entropy-Regularized Form}
First, we provide the upper bound for the ELBO in entropy-regularized form.
This result does \textit{not} require any modifications to vanilla SGD.

\vspace{.5ex}
\input{thm_gradient_upper_bound}

\vspace{1ex}
\begin{remark}
  If the bijector \(\psi\) is an identity function, \(\vzeta_{\mathrm{KL}}\) and \(\vzeta_{\mathrm{H}}\) are the maximum likelihood (ML) and maximum a-posteriori (MAP) estimates, respectively.
  Thus, with enough datapoints, the term \( {\lVert \bar{\vzeta}_{\mathrm{KL}} - \bar{\vzeta}_{\mathrm{H}} \rVert}_2^2 \) will be negligible since the ML and MAP estimates will be close.
\end{remark}

\vspace{1ex}
\begin{remark}
  Let \(\kappa_{\mathrm{cond.}} = \nicefrac{L_{\mathrm{H}}}{\mu_{\mathrm{KL}}}\) be the \textit{condition number} of the problem.
  For the full-rank parameterizations and smooth quadratic functions, the variance is bounded as \(\mathcal{O}\left( L_{\mathrm{H}} \kappa_{\mathrm{cond.}} \left(d + \kappa\right) / M \right)\).
  The variance depends linearly on 
  \begin{enumerate}
    \vspace{-1ex}
    \setlength\itemsep{-1ex}
    \item[\ding{182}] the scaling of the problem \(L_{\mathrm{H}}\), 
    \item[\ding{183}] the conditioning of the problem \(\kappa_{\mathrm{cond.}}\),
    \item[\ding{184}] the dimensionality of the problem \(d\), and
    \item[\ding{185}] the tail properties of the variational family \(\kappa\),
    \vspace{-1ex}
  \end{enumerate}
  where the number of Monte Carlo samples \(M\) linearly reduces the variance.
\end{remark}

\vspace{-1.ex}
\paragraph{KL-Regularized Form}
We now prove an equivalent result for the KL-regularized form.
Here, we do not have to rely on~\cref{eq:thm_upper_bound_parallel} since we already start from \(f_{\mathrm{KL}}\), which results in better constants.

\input{thm_gradient_upper_bound_klform}

\vspace{-.5ex}
\subsection{Upper Bound Under Bounded Entropy}
\vspace{-.5ex}
The bound in \cref{thm:gradient_upper_bound} is loose due to the use of~\cref{eq:thm_upper_bound_parallel} (\(\times 2\) loose) and \cref{eq:thm_upper_bound_kl_upper_bound}.
An alternative bound can be obtained by assuming the following:
%
\begin{assumption}[\textbf{Bounded Entropy}]\label{assumption:bounded_entropy}
  The regularization term is bounded below as
  \( h_{\mathrm{H}}\left(\vlambda\right) \geq h_{\mathrm{H}}^* \).
\end{assumption}
%.
For the entropy-regularized form, this corresponds to the entropy being bounded above by some constant since \(h\left(\vlambda\right) = - \mathrm{H}\left(q_{\vlambda}\right)\).
When using the nonlinear parameterizations (\cref{def:meanfield,def:fullrank}), this assumption can be practically enforced by bounding the output of \(\phi\) by some large \(S\).
%
\vspace{.5ex}
\begin{proposition}
    Let the diagonal conditioner \(\phi\) be bounded as \(\phi\left(x\right) \leq S\).
    Then, for any \(d\)-dimensional distribution \(q_{\vlambda}\) in the location-scale family with the mean-field (\cref{def:meanfield}) or Cholesky (\cref{def:fullrank}) parameterizations,
  {%
  \setlength{\belowdisplayskip}{1.ex} \setlength{\belowdisplayshortskip}{1.ex}%
  \setlength{\abovedisplayskip}{1.ex} \setlength{\abovedisplayshortskip}{1.ex}%
    \[ h_{\mathrm{H}}\left(\vlambda\right) = -\mathrm{H}\left(q_{\vlambda}\right) \geq -\mathrm{H}\left(\varphi\right) - d \log S. \]
  }%
\end{proposition}
\vspace{-2ex}
\begin{proof}
    From \cref{thm:location_scale_entropy}, \(\mathrm{H}\left(q_{\vlambda}\right) = \mathrm{H}\left(\varphi\right) + \log \abs{\mC} \).
    Since \(\mC\) under \cref{def:meanfield,def:fullrank} is a diagonal or triangular matrix, the log absolute determinant is the log sum of the diagonals.
    The conclusion follows from the fact that the diagonals \(C_{ii} = \phi\left(s_i\right)\) are bounded by \(S\).
\end{proof}
%
This is essentially a weaker version of the bounded domain assumption, though only the diagonal elements of \(\mC\), \(s_1, \ldots, s_d\), are bounded.
While this assumption results in an admittedly less realistic algorithm, it enables a tighter bound for the entropy-regularized form ELBO.

\input{thm_gradient_upper_bound_bounded_entropy}

\begin{figure*}[t]
  \vspace{-2ex}
  \centering
  \input{figures/fig_quadratic_main_text.tex}
  \vspace{-4ex}
  \caption{
    \textbf{Evaluation of the bounds for a perfectly conditioned quadratic target function.}
    The \textcolor{color3}{blue regions} are the loosenesses resulting from either using (\cref{thm:gradient_upper_bound}) or not using (\cref{thm:gradient_upper_bound_bounded_entropy}) the bounded entropy assumption (\cref{assumption:bounded_entropy}), while the \textcolor{color1}{red regions} are the remaining ``technical loosesnesses.''
    The gradient variance was estimated from \(10^3\) samples.
  }\label{fig:quadratic}
  \vspace{-2ex}
\end{figure*}


%% \subsection{Upper Bound of the STL Estimator}

%% Unlike the previously considered estimators, the \textit{sticking the landing} (STL; \citealt{roeder_sticking_2017}) estimator uses Monte Carlo estimates of the entropy term.
%% %
%% \begin{definition}[\textbf{STL Estimator}]
%%   \begin{align*}
%%     f\left(\vzeta\right)
%%     &= 
%%     - \nabla_{\vlambda} \log\ell\left(\rvvx, \rvvz = \psi^{-1}\left( \vzeta \right) \right)
%%     - \log \abs{ \mJ_{\phi^{-1}}\left(\vzeta\right) } \\ &\qquad+ 
%%     \underbrace{\nabla_{\vlambda} \log q_{\vgamma}\left(\vzeta\right),}_{\text{Monte Carlo entropy estimate}}
%%   \end{align*}
%%   where \(\vgamma = \vlambda\) and \(h\left(\vlambda\right) = 0\).
%% \end{definition}
%% %
%% This estimator contains more stochastic elements than the entropy-form and KL-form estimators.
%% Even though the entropy term now \textit{adds} noise, it has shown in practice to result in lower variance and, therefore, faster, stable convergence.
%% This is because the entropy term acts as a control variate \citep{geffner_using_2018}, reducing variance as \(q_{\vlambda}\) becomes closer to \(\pi\).
%% We also provide an upper bound for the STL estimator.

%% \input{thm_gradient_upper_bound_stl}

%% Naturally, since gradient variance bounds are worst-case bounds, this estimator has \textit{worse} variance guarantee.


\subsection{Matching Lower Bound}
Finally, we present a matching lower bound on the gradient variance of BBVI.
Our lower bound holds broadly for smooth and strongly convex problem instances that are well-conditioned and high-dimensional.

\vspace{.5ex}
\input{thm_gradient_lower_bound}

\begin{remark}[\textbf{Matching Dimensional Dependence}]
  For well-conditioned problems such that \(\nicefrac{L}{\mu} < \sqrt{d+1}\), a lower bound of the same dimensional dependence with our upper bounds holds near the optimum. 
\end{remark}

\begin{remark}[\textbf{Unimprovability of the ABC Condition}]
  The lower bound suggests that the \(ABC\) gradient variance condition is unimprovable within the class of smooth, quadratically growing functions.
\end{remark}

%%% Local Variables:
%%% TeX-master: "main"
%%% End:
