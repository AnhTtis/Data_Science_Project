
\section{Preliminaries}
{%small
\paragraph{Notation}
Random variables are denoted in serif, while their realization is in regular font.
(\textit{i.e}, \(x\) is a realization of \(\rvx\), \(\vx\) is a realization of the vector-valued \(\rvvx\).)
\(\norm{\vx}_2 = \){\footnotesize\(\sqrt{\inner{\vx}{\vx}} = \sqrt{\vx^{\top}\vx}\)} denotes the Euclidean norm, while \(\norm{\mA}_{\mathrm{F}} =\) {\footnotesize\(\sqrt{\mathrm{tr}\left(\mA^{\top} \mA\right)}\)} is the Frobenius norm, where {\footnotesize\(\mathrm{tr}\left(\mA\right) = \sum^{d}_{i=1} A_{ii} \)} is the matrix trace.
}%

\vspace{-1ex}
\subsection{Variational Inference}
\vspace{-.5ex}
Variational inference~\citep{peterson_mean_1987,hinton_keeping_1993} is a family of inference algorithms devised to solve the problem
{%
  \setlength{\belowdisplayskip}{1.ex} \setlength{\belowdisplayshortskip}{1.ex}%
  \setlength{\abovedisplayskip}{1.ex} \setlength{\abovedisplayshortskip}{1.ex}%
\begin{align}
  \minimize_{\vlambda \in \mathbb{R}^p} \; \DKL{q_{\psi,\vlambda}}{\pi}, \label{eq:kl}
\end{align}
}%
where \(q_{\psi,\vlambda}\) is called the ``variational approximation'', while \(\pi\) is a distribution of interest, and \(D_{\text{KL}}\) is the (exclusive) Kullback-Leibler (KL) divergence.

For Bayesian inference, \(\pi\) is the posterior distribution
{%
  \setlength{\belowdisplayskip}{1.ex} \setlength{\belowdisplayshortskip}{1.ex}%
  \setlength{\abovedisplayskip}{1.ex} \setlength{\abovedisplayshortskip}{1.ex}%
\begin{align*}
  \pi\left(\vz\right)
  \propto 
  \ell\left(\vx \mid \vz \right) p\left(\vz\right)
  =
  \ell\left(\vx, \vz \right),
\end{align*}
}%
where \(\ell\left(\vx \mid \vz \right)\) is the likelihood, and \(p\left(\vz\right)\) is the prior.
In practice, one only has access to the likelihood and the prior.
Thus,~\cref{eq:kl} cannot be directly solved.
Instead, we can minimize the negative \textit{evidence lower bound} (ELBO; \citealt{jordan_introduction_1999}) function \(F\left(\vlambda\right)\).

\vspace{-1.5ex}
\paragraph{Evidence Lower Bound}
More formally, we solve
{%
\setlength{\belowdisplayskip}{1.ex} \setlength{\belowdisplayshortskip}{1.ex}
\setlength{\abovedisplayskip}{1.ex} \setlength{\abovedisplayshortskip}{1.ex}
\[
  \minimize_{\vlambda \in \mathbb{R}^p} \; F\left(\vlambda\right),
\]
}%
where \(F\) is defined as
{%
\setlength{\belowdisplayskip}{1.ex} \setlength{\belowdisplayshortskip}{1.ex}
\setlength{\abovedisplayskip}{1.ex} \setlength{\abovedisplayshortskip}{1.ex}
\begin{align}
  F\left(\vlambda\right) 
  &\triangleq
  -\Esub{\rvvz \sim q_{\psi,\vlambda}}{ \log \ell\left(\vx, \rvvz\right) } - \mathrm{H}\left(q_{\psi,\vlambda}\right),
  \label{eq:elbo_H_form}
  \\
  &=
  -\Esub{\rvvz \sim q_{\psi,\vlambda}}{ \log \ell\left(\vx|\rvvz\right) } + \DKL{q_{\psi,\vlambda}}{p},
  \label{eq:elbo_kl_form}
\end{align}
}%
\begin{center}
  \vspace{-1.5ex}
  {\begingroup
    \setlength\tabcolsep{1.5pt} 
  \begin{tabular}{ll}
    \(\rvvz\) & is the latent (random) variable, \\
    \(q_{\psi,\vlambda}\) & is the variational distribution, \\
    \(\psi\) & is a bijector (support transformation), and  \\
    \(\mathrm{H}\)    & is the differential entropy.
    % \(\mJ_{\psi^{-1}}\left(\rvveta\right)\) & is the Jacobian of the inverse of \(\psi\), and \\
    % \(h\) & is a deterministic regularizer term.
  \end{tabular}
  \endgroup}
  \vspace{-1.5ex}
\end{center}

The bijector \(\psi\)~\citep{dillon_tensorflow_2017,fjelde_bijectors_2020,leger_parametrization_2023} is a differentiable bijective map that is used to de-constrain the support of constrained random variables.
For example, when \(z\) is expected to follow a gamma distribution, using \(\eta = \psi\left(z\right)\) with \(\psi\left(z\right) = \log z\) lets us work with \(\eta\), which can be any real number, unlike \(z\).
The use of \(\psi^{-1}\) corresponds to the automatic differentiation VI formulation (ADVI;~\citealt{kucukelbir_automatic_2017}), which is now widespread.

\vspace{-2.5ex}
\subsection{Variational Family}
\vspace{-.5ex}
In this work, we specifically consider the location-scale variational family with a standardized base distribution.

\begin{definition}[\textbf{Reparameterization Function}]\label{def:reparam}
  An affine mapping \(\vt_{\vlambda} : \mathbb{R}^d \rightarrow \mathbb{R}^d\) defined as
{
\setlength{\belowdisplayskip}{1.ex} \setlength{\belowdisplayshortskip}{1.ex}
\setlength{\abovedisplayskip}{1.ex} \setlength{\abovedisplayshortskip}{1.ex}
  \begin{align*}
    &\vt_{\vlambda}\left(\vu\right) \triangleq \mC \vu + \vm
  \end{align*}
}%
  with \(\vlambda\) containing the parameters for forming the location \(\vm \in \mathbb{R}^d\) and scale \(\mC = \mC\left(\vlambda\right) \in \mathbb{R}^{d \times d}\) is called the (location-scale) \textit{reparameterization function}.
\end{definition}

\begin{definition}[\textbf{Location-Scale Family}]\label{def:family}
  Let \(\varphi\) be some \(d\)-dimensional distribution.
  Then, \(q_{\vlambda}\) such that
{%
\setlength{\belowdisplayskip}{1.5ex} \setlength{\belowdisplayshortskip}{1.5ex}%
\setlength{\abovedisplayskip}{1.5ex} \setlength{\abovedisplayshortskip}{1.5ex}%
  \begin{alignat*}{2}
    \rvvzeta \sim q_{\vlambda}  \quad\Leftrightarrow\quad &\rvvzeta \stackrel{d}{=} \vt_{\vlambda}\left(\rvvu\right); \quad \rvvu \sim  \varphi
  \end{alignat*}
  }%
  is said to be a member of the location-scale family indexed by the base distribution \(\varphi\) and parameter \(\vlambda\).
\end{definition}

This family includes commonly used variational families, such as the mean-field Gaussian, full-rank Gaussian, Student-T, and other elliptical distributions.
\begin{remark}[\textbf{Entropy of Location-Scale Distributions}]\label{thm:location_scale_entropy}
  The differential entropy of a location-scale family distribution (\cref{def:family}) is 
{
\setlength{\belowdisplayskip}{1.ex} \setlength{\belowdisplayshortskip}{1.ex}
\setlength{\abovedisplayskip}{1.ex} \setlength{\abovedisplayshortskip}{1.ex}
  \[
    \mathrm{H}\left(q_{\vlambda}\right) = \mathrm{H}\left(\varphi\right) + \log \abs{ \mC }.
  \]
}
\end{remark}

\begin{definition}[\textbf{ADVI Family}; \citealt{kucukelbir_automatic_2017}]\label{def:advi}
  Let \(q_{\vlambda}\) be some \(d\)-dimensional distribution.
  Then, \(q_{\psi,\vlambda}\) such that
{
\setlength{\belowdisplayskip}{1.ex} \setlength{\belowdisplayshortskip}{1.ex}
\setlength{\abovedisplayskip}{1.ex} \setlength{\abovedisplayshortskip}{1.ex}
  \begin{alignat*}{2}
    \rvvz \sim q_{\psi,\vlambda}  \quad\Leftrightarrow\quad &\rvvz \stackrel{d}{=} \psi^{-1}\left(\rvvzeta\right); \quad \rvvzeta \sim q_{\vlambda}
  \end{alignat*}
}%
  is said to be a member of the ADVI family with the base distribution \(q_{\vlambda}\) parameterized with \(\vlambda\).
\end{definition}

We impose assumptions on the base distribution \(\varphi\).
\begin{assumption}[\textbf{Base Distribution}]\label{assumption:symmetric_standard}
  \(\varphi\) is a \(d\)-dimensional distribution such that \(\rvvu \sim \varphi\) and \(\rvvu = \left(\rvu_1, \ldots, \rvu_d \right)\) with indepedently and identically distributed components.
  Furthermore, \(\varphi\) is
  \begin{enumerate*}[label=\textbf{(\roman*)}]
      \item symmetric and standardized such that \(\mathbb{E}\rvu_i = 0\), \(\mathbb{E}\rvu_i^2 = 1\), \(\mathbb{E}\rvu_i^3 = 0\), and 
      \item has finite kurtosis \(\mathbb{E}\rvu_i^4 = \kappa < \infty\).
  \end{enumerate*}
\end{assumption}
These assumptions are already satisfied in practice by, for example, generating \(\rvu_i\) from a univariate normal or Student-T with \(\nu > 4\) degrees of freedom.

\subsection{Reparameterization Trick}
\vspace{-.5ex}
When restricted to location scale families~(\cref{def:family,def:advi}), we can invoke Change-of-Variable, or more commonly known as the ``reparameterization trick,'' such that
{%
\begin{align*}
  \mathbb{E}_{\rvvz \sim q_{\psi,\vlambda}} \log\ell\left(\vx, \rvvz\right)
  &=
  \mathbb{E}_{\rvvzeta \sim q_{\vlambda}} \log\ell\left(\vx, \psi^{-1}\left( \rvvzeta\right)\right)
  \\
  &=
  \mathbb{E}_{\rvvu \sim \varphi} \log\ell\left(\vx, \psi^{-1}\left(\vt_{\vlambda}\left(\rvvu\right)\right)\right)
\end{align*}
}%
through the Law of the Unconcious Statistician.
Differentiating this results in the \textit{reparameterization} or \textit{path} gradient, which often achieves lower variance than alternatives~\citep{xu_variance_2019,mohamed_monte_2020a}.

\vspace{-.5ex}
\paragraph{Objective Function}
For generality, we represent our objective as a regularized infinite sum problem:
\begin{definition}[\textbf{Regularized Infinite Sum}]\label{def:generic_elbo}
\begin{equation*}
   F\left(\vlambda\right)
   =
   \mathbb{E}_{\rvvu \sim \varphi} f\left(\vt_{\vlambda}\left(\rvvu\right)\right)
   +
   h\left(\vlambda\right),
   \label{eq:F}
\end{equation*}
where \((\vlambda, \rvvu) \mapsto f \circ \vt_{\vlambda} : \mathbb{R}^p \times \mathbb{R}^d \rightarrow \mathbb{R}\) is some bivariate stochastic function of \(\vlambda\) and the ``noise source'' \(\rvvu\), while \(h\) is a deterministic regularization term.
\end{definition}
By appropriately defining \(f\) and \(h\), we retrieve the two most common formulations of the ELBO in \cref{eq:elbo_H_form} and \cref{eq:elbo_kl_form} respectively:
\begin{definition}[\textbf{ELBO Entropy-Regularized Form}]\label{def:entropy_form}
\begin{align*}
  f_{\mathrm{H}}\left(\vzeta\right) 
  &= -\log \underbrace{\ell\left( \vx, \psi^{-1}\left( \vzeta \right) \right)}_{\text{Joint Likelihood}} - \log \abs{ \mJ_{\psi^{-1}}\left(\vzeta\right) }  &
  \\
  h_{\mathrm{H}}\left(\vlambda\right) &= - \mathrm{H}\left(q_{\vlambda}\right).
\end{align*}
\end{definition}
%
\begin{definition}[\textbf{ELBO KL-Regularized Form}]\label{def:kl_form}
\begin{align*}
  f_{\text{KL}}\left(\vzeta\right) 
  &= - \log \underbrace{\ell\left(\vx \mid \psi^{-1}\left( \vzeta \right)  \right)}_{\text{Likelihood}} - \log \abs{ \mJ_{\psi^{-1}}\left(\vzeta\right) } &
  \\
  h_{\text{KL}}\left(\vlambda\right) &= \DKL{q_{\vlambda}}{p}.
\end{align*}
\end{definition}%
Here, \(\mJ_{\psi^{-1}}\) is the Jacobian of the bijector.
%
Since \(\DKL{q_{\vlambda}}{p}\) is seldomly available in tractable form, the entropy-regularized form is the most widely used, while the KL regularized is common for Gaussian processes and variational autoencoders.
%However, even when the KL-regularized form is intractable, it still exists in theory, and we will use jump between the two forms while proving some of our upper bounds.

\vspace{-1.0ex}
\paragraph{Gradient Estimator}
We denote the \(M\)-sample estimator of the gradient of \(F\) as
\begin{align}
  \rvvg_{M}\left(\vlambda\right) &\triangleq \frac{1}{M} \, \sum^{M}_{m=1} \rvvg_m\left(\vlambda\right), \;\text{where}\; \label{eq:def_gradient_M_est} \\
  \rvvg_m\left(\vlambda\right)   &\triangleq \nabla_{\vlambda} f\left(\vt_{\vlambda}\left(\rvvu_m\right)\right) + \nabla h(\vlambda); \quad \rvvu_m \sim \varphi. \label{eq:def_gradient_m_est}
\end{align}
We will occasionally drop \(\vlambda\) for clarity.

\vspace{-.5ex}
\subsection{Gradient Variance Assumptions in \linebreak Stochastic Gradient Descent}
\vspace{-.5ex}
\paragraph{Gradient Variance Assumptions in SGD}
For a while, most convergence proofs in SGD have relied on the ``bounded variance'' assumption.
That is, for a gradient estimator \(\rvvg\), \(\mathbb{E} \norm{\rvvg}^2_2 \leq G\) for some finite constant \(G\).
This assumption is problematic because
\begin{enumerate*}
  \item[\ding{182}] these types of global constants result in loose bounds, 
  \item[\ding{183}] and it directly contradicts the strong-convexity assumption~\citep{nguyen_sgd_2018}.
\end{enumerate*}
Thus, retrieving previously known SGD convergence rates under weaker assumptions has been an important research direction~\citep{tseng_incremental_1998,vaswani_fast_2019,schmidt_fast_2013,bottou_optimization_2018,gower_sgd_2019,gower_stochastic_2021,nguyen_sgd_2018}.

\vspace{-.5ex}
\paragraph{ABC Condition}\label{section:abc}
In this work, we focus on the recently rediscovered \textit{expected smoothness}, or \textit{ABC}, condition~\citep{polyak_pseudogradient_1973,gower_stochastic_2021}.
\begin{assumption}[\textbf{Expected Smoothness; \(ABC\)}]\label{assumption:abc}
  \(\rvvg\) is said to satisfy the expected smoothness condition if 
  \begin{align*}
    \mathbb{E}\norm{\rvvg_M\left(\vlambda\right)}_2^2
    \leq
    2 \, A \left( F\left(\vlambda\right) - F^* \right)
    + B \, \norm{ \nabla F\left(\vlambda\right) }_2^2 + C.
  \end{align*}
  for some finite \(A, B, C \geq 0\), where \(F^* = \inf_{\vlambda \in \mathrm{R}^p} F\left(\vlambda\right) \).
\end{assumption}
As shown by \citet{khaled_better_2022}, this condition is not only strictly weaker than many of the previously used assumptions but also \textit{generalizes} them by retrieving known convergence rates when tweaking the constants.
With the \(ABC\) condition, for nonconvex \(L\)-smooth functions, under a fixed stepsize of \(\gamma = \nicefrac{1}{L B}\), SGD converges to a \(\mathcal{O}\left(L C \gamma \right)\) neighborhood in a \(\mathcal{O}\left(\nicefrac{{\left(1 + L \gamma^2 A\right)}^T}{\left(\gamma T\right)}\right)\) rate.
The \textit{ABC} condition has also been used to prove convergence of SGD for quasar convex functions~\citet{gower_sgd_2021}, stochastic heavy-ball/momentum methods~\citet{liu_almost_2022}, and stochastic proximal methods~\citep{li_unified_2022}.
Given the influx of results based on the \textit{ABC} condition, connecting with it would significantly broaden our theoretical understanding of BBVI.

\vspace{-1ex}
\subsection{Covariance Parameterizations}\label{section:covariance_parameterization}
\vspace{-.5ex}

When using the location-scale family (\cref{def:family}), the scale matrix \(\mC\) can be parameterized in different ways.
Any parameterization that results in a positive definite covariance \(\mC\mC^{\top} \in \mathbb{S}_{++}^{d}\) is valid.
We consider multiple parameterizations as the choice can result in different theoretical properties.
A brief survey on the use of different parameterizations is shown in \cref{section:survey}.

\vspace{-2ex}
\paragraph{Linear Parameterization}
The previous results by~\citet{domke_provable_2019} considered the matrix square root parameterization, which is linear with respect to the variational parameters.
{
\setlength{\belowdisplayskip}{1.ex} \setlength{\belowdisplayshortskip}{1.ex}
\setlength{\abovedisplayskip}{1.ex} \setlength{\abovedisplayshortskip}{1.ex}
\begin{definition}[\textbf{Matrix Square Root}]\label{def:squareroot}
  {
    \[
      \mC\left(\vlambda\right) = \mC,
    \]
    where \(\mC \in \mathbb{R}^{d \times d}\) is a matrix, \(\vlambda_{\mC} = \mathrm{vec}\left(\mC\right) \in \mathbb{R}^{d^2}\) such that \(\vlambda = \left(\vm, \vlambda_{\mC}\right)\).
  }
\end{definition}
}%
Note that \(\mC\) is not constrained to be symmetric so this is not a matrix square root in a narrow sense.
Also, this parameterization does not guarantee \(\mC\mC^{\top}\) to be positive definite (only positive semidefinite), which occasionally results in the entropy term \(h_{\mathrm{H}}\) blowing up~\citep{domke_provable_2020}.
\citeauthor{domke_provable_2020} proposed to fix this by using proximal operators.

\vspace{-2.0ex}
\paragraph{Nonlinear Parameterizations}
In practice, optimization is preferably done in unconstrained \(\mathbb{R}^p\), which then positive definiteness can be ensured by explicitly mapping the diagonal elements to positive numbers.
We denote this by the \textit{diagonal conditioner} \(\phi\). (See 
\cref{section:survey} for a brief survey on their use).
The following two parameterizations are commonly used, where \(\mD = \mathrm{diag}\left(\phi\left(\vs\right)\right) \in \mathbb{R}^{d \times d}\) denotes a diagonal matrix such that \(D_{ii} = \phi\left(s_i\right) > 0\).
{
\setlength{\belowdisplayskip}{1.ex} \setlength{\belowdisplayshortskip}{1.ex}
\setlength{\abovedisplayskip}{1.ex} \setlength{\abovedisplayshortskip}{1.ex}
\begin{definition}[\textbf{Mean-Field}]\label{def:meanfield}
  \begin{align*}
    \mC\left(\vlambda, \phi\right) &= \mathrm{diag}\left(\phi\left(\vs\right)\right),
  \end{align*}
  where \(\vs \in \mathbb{R}^{d}\) and \(\vlambda = \left(\vm, \vs\right)\).
\end{definition}

\begin{definition}[\textbf{Cholesky}]\label{def:fullrank}
  \begin{align*}
    \mC\left(\vlambda, \phi\right) &= \mathrm{diag}\left(\phi\left(\vs\right)\right) + \mL,
  \end{align*}
  where \(\vs \in \mathbb{R}^{d}\), \(\mL \in \mathbb{R}^{d \times d}\) is a strictly lower triangular matrix, \(\vlambda_{\mL} = \mathrm{vec}\left(\mL\right) \in \mathbb{R}^{\left(d - 1\right) \, d / 2}\) such that \(\vlambda = \left(\vm, \vs, \vlambda_{\mL}\right)\).
  The special case of \(\phi\left(x\right) = x\) is called the ``linear Cholesky'' parameterization.
\end{definition}
}%
%
\vspace{-2.5ex}
\paragraph{Diagonal conditioner}
For the diagonal conditioner, the softplus function \(\phi\left(x\right) = \mathrm{softplus}(x) = \log(1 + e^x)\)~\citep{dugas_incorporating_2000} or the exponential function \(\phi\left(x\right) = e^x\) is commonly used.
While using these nonlinear functions significantly complicates the analysis, assuming \(\phi\) to be 1-Lipschitz retrieves practical guarantees.

\vspace{0.5ex}
\begin{assumption}[\textbf{Lipschitz Diagonal Conditioner}]\label{assumption:phi_lipschitz}
  The diagonal conditioner \(\phi\) is 1-Lipschitz continuous.
\end{assumption}
\vspace{0.5ex}

\begin{remark}
  The softplus function is 1-Lipschitz continuous.
\end{remark}

%%% Local Variables:
%%% TeX-master: "main"
%%% End:
