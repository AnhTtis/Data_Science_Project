
\vspace{-1ex}
\section{Simulations}
\vspace{-.5ex}
We now evaluate our bounds and the insights gathered during the analysis through simulations.
We implemented a bare-bones implementation of BBVI in Julia~\citep{bezanson_julia_2017} with plain SGD.
The stepsize were manually tuned so that all problems converge at similar speeds.
For all problems, we use a unit Gaussian base distribution such that \(\varphi\left(u\right) = \mathcal{N}\left(u; 0, 1\right)\) resulting in a kurtosis of \(\kappa = 3\) and use \(M = 10\) Monte Carlo samples.

%% \begin{figure}[t]
%%   \centering
%%   \input{figures/fig_quadratic_softpluschol_generalbound.tex}
%%   \caption{
%%   }
%% \end{figure}

%% \begin{figure}[t]
%%   \centering
%%   \input{figures/fig_quadratic_softpluschol_boundedentropybound.tex}
%%   \caption{
%%   }
%% \end{figure}

%% \begin{figure}[t]
%%   \centering
%%   \input{figures/fig_quadratic_linearchol_generalbound.tex}
%%   \caption{
%%   }
%% \end{figure}

%% \begin{figure}[t]
%%   \centering
%%   \input{figures/fig_quadratic_linearchol_boundedentropybound.tex}
%%   \caption{
%%   }
%% \end{figure}

%% \begin{figure}[t]
%%   \centering
%%   \input{figures/fig_quadratic_squareroot_generalbound.tex}
%%   \caption{
%%   }
%% \end{figure}

\vspace{-.5ex}
\subsection{Synthetic Problem}\label{section:quadratic}
To test the \textit{ideal} tightness of the bounds, we consider quadratics achieving the tightest bound for the constants \(L_{\mathrm{H}}, L_{\mathrm{KL}}, \mu_{\mathrm{H}}, \mu_{\mathrm{KL}}\) given as
{%
\setlength{\belowdisplayskip}{1.ex} \setlength{\belowdisplayshortskip}{1.ex}%
\setlength{\abovedisplayskip}{1.ex} \setlength{\abovedisplayshortskip}{1.ex}%
\[
  \log \ell\left(\vx \mid \vz \right) = -\frac{N}{\sigma^2} \norm{ \vz - \vz^* }_2^2;\quad
  \log p\left(\vz \right)             = -\frac{1}{\lambda}  \norm{ \vz  }_2^2,
\]
}%
where \(N\) simulates the effect of the number of datapoints.
We set the constants as \(\sigma = 0.3\), \(\lambda = 8.0\), and \(N = 100\), the mode \(\vz^*\) is randomly sampled from a Gaussian, and the dimension of the problem is \(d = 20\).
For the bounded entropy case, we set \(S = 2.0\) (the true standard deviation is in the order of 1e-3).

\vspace{-.5ex}
\paragraph{Quality of Upper Bound}
The results for the Cholesky and mean-field parameterizations with a softplus bijector are shown in~\cref{fig:quadratic}.
For the Cholesky parameterization, the bulk of the looseness comes from the treatment of the regularization term (\textcolor{color3}{blue region}).
The remaining ``technical looseness'' (\textcolor{color1}{red region}) is relatively tight and can be shown to be tighter when using linear parameterizations (\(\phi\left(x\right) = x\)) and the square root parameterization, which is the tightest.
However, for the mean-field parameterization, despite the superior constants~(\cref{remark:meanfield_superiority}), there is still room for improvement.
Additional results for other parameterizations can be found in~\cref{section:additional_quadratic}.


\begin{figure}[t]
  \vspace{-1.5ex}
  \hspace{-1.5em}
  \subfloat{
    \input{figures/fig_airfoil_softpluschol_generalbound.tex}\label{fig:airfoil_bound}
  }
  \subfloat{
    \hspace{-2em}
    \input{figures/fig_parameterization_comparison.tex}\label{fig:airfoil_parameterizations}
  }
  \vspace{-1.0ex}
  \caption{
    \textbf{
      Linear regression on the \textsc{Airfoil} dataset.
      (\textsf{left}) Evaluation of the upper bound (\cref{thm:gradient_upper_bound}).
      (\textsf{right}) Comparison of the variance of different parameterizations resulting in the same \(\vm\), \(\mC\).
    }
  }
  \vspace{-3.0ex}
\end{figure}

\vspace{-.5ex}
\subsection{Real Dataset}\label{section:linearreg}
\vspace{-.5ex}
\paragraph{Model}
We now evaluate the theoretical results with real datasets.
Given a regression dataset \((\mX, \vy)\), we use the linear Gaussian model 
{%
\setlength{\belowdisplayskip}{1.ex} \setlength{\belowdisplayshortskip}{1.ex}%
\setlength{\abovedisplayskip}{1.ex} \setlength{\abovedisplayshortskip}{1.ex}%
\[
  y   \sim \mathcal{N}\left(\mX \vw, \sigma^2\right);\quad
  \vw \sim \mathcal{N}\left(\mathbf{0}, \lambda \boldupright{I}\right),
\]
}%
where \(\lambda\) and \(\sigma\) are hyperparameters.
The smoothness and quadratic growth constants for this model are given as the max- and minimum eigenvalues of \(\sigma^{-2} \mX^{\top} \mX + \lambda^{-1} \boldupright{I}\) (for \(f_{\text{H}}\)) and \(\sigma^{-2} \mX^{\top} \mX\) (for \(f_{\text{KL}}\)).
\(f_{\text{KL}}^*\) and \(f_{\text{H}}^*\) are given as the mode of the likelihood and the posterior, while \(F^*\) is the negative marginal log-likelihood.

\vspace{-.5ex}
\paragraph{Quality of Upper Bound}
\cref{fig:airfoil_bound} shows the result on the \textsc{Airfoil} dataset~\citep{Dua:2019}.
The constants are \(L_{\mathrm{H}} = 3.520 \times 10^4, \mu_{\mathrm{KL}}=2.909 \times 10^3\).
Due to poor conditioning, the bound is much looser compared to the quadratic case.
We note that generalizing our bounds to utilize matrix smoothness and matrix-quadratic growth as done by \citep{domke_provable_2019} would tighten the bounds.
But the theoretical gains would be marginal.
Detailed information about the datasets and additional results for other parameterizations can be found in~\cref{section:additional_linearreg}.

\vspace{-.5ex}
\paragraph{Comparison of Parameterizations}
\cref{fig:airfoil_parameterizations} compares the gradient variance resulting from the different parameterizations.
For a fair comparison, the gradient is estimated on the \(\vlambda\) that results in the same \(\vm, \mC\) for all three parameterizations.
%The nonlinear parameterizations result in the lowest variance.
%Furthermore, the matrix square root parameterization results in the highest variance.
This shows the gradual increase in variance by
\begin{enumerate*}[label=\textbf{(\roman*)}]
  \item not using a nonlinear conditioner (linear Cholesky)
  \item and increasing the number of variational parameters (matrix square root).
\end{enumerate*}

%%% Local Variables:
%%% TeX-master: "main"
%%% End:
