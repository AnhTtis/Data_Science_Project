
\section{Related Works}
\vspace{-.5ex}
\paragraph{Controlling Gradient Variance}
The main algorithmic challenge in BBVI is to control the gradient noise~\cite{ranganath_black_2014}.
This has led to various methods for reducing the variance of VI gradient estimators using control variates~\citep{ranganath_black_2014,miller_reducing_2017,geffner_using_2018}, ensembling of estimators~\citep{geffner_rule_2020}, modifying the differentiation procedure~\citep{roeder_sticking_2017}, quasi-Monte Carlo~\citep{buchholz_quasimonte_2018, liu_quasimonte_2021}, and multilevel Monte Carlo~\citep{fujisawa_multilevel_2021}.
Cultivating a deeper understanding of the properties of gradient variance could further extend this list.

\vspace{-2ex}
\paragraph{Convergence Guarantees}
Obtaining full convergence guarantees has been an important task for understanding BBVI algorithms.
However, most guarantees so far have relied on strong assumptions such as that the log-likelihood is Lipschitz~\citep{cherief-abdellatif_generalization_2019,alquier_nonexponentially_2021}, that the gradient variance is bounded by constant~\citep{liu_quasimonte_2021,buchholz_quasimonte_2018,domke_provable_2020,hoffman_blackbox_2020}, and that the support of \(q_{\vlambda}\) is bounded~\citep{fujisawa_multilevel_2021}.
Our result shows that similar results can be obtained under relaxed assumptions.
Meanwhile,~\citet{bhatia_statistical_2022} have recently proven a full complexity guarantee for a variant of BBVI.
%\textcolor{blue}{
But similarly to \citet{hoffman_blackbox_2020}, they only optimize the scale matrix \(\mC\), and the specifics of the algorithm diverge from the usual BBVI implementations as it uses the stochastic power iterations instead of SGD.
%}

\vspace{-1ex}
\paragraph{Gradient Variance Guarantees}
Studying the actual gradient variance properties of BBVI has only started to make progress recently.
~\citet{fan_fast_2015} first provided bounds by assuming the log-likelihood to be Lipschitz.
Under more general conditions,~\citet{domke_provable_2019} provided tight bounds for smooth log-likelihoods, which our work builds upon.
\citeauthor{domke_provable_2019}'s result can also be seen as a direct generalization of the results of~\citet{xu_variance_2019}, which are restricted to quadratic log-likelihoods and the mean-field family.
Lastly,~\citet{mohamed_monte_2020} provides a conceptual evaluation of gradient estimators used in BBVI.

%%% Local Variables:
%%% TeX-master: "main"
%%% End:
