

\prAtEndRestateii*

\makeatletter\Hy@SaveLastskip\label{proofsection:prAtEndii}\ifdefined\pratend@current@sectionlike@label\immediate\write\@auxout{\string\gdef\string\pratend@section@for@proofii{\pratend@current@sectionlike@label}}\fi\Hy@RestoreLastskip\makeatother\begin{proof}[Proof]\phantomsection\label{proof:prAtEndii}\begin {alignat}{2} &\norm {\nabla _{\vlambda } f\left ( \vt _{\vlambda }\left (\vu \right ) \right ) }_2^2 \nonumber \\ &\;= {\left ( \frac { \partial \vt _{\vlambda }\left (\vu \right ) }{ \partial \vlambda } \nabla f\left ( \vt _{\vlambda }\left (\vu \right ) \right ) \right )}^{\top } \frac { \partial \vt _{\vlambda }\left (\vu \right ) }{ \partial \vlambda } \nabla f\left ( \vt _{\vlambda }\left (\vu \right ) \right ) \nonumber \\ &\;= {\nabla f^{\top }\left ( \vt _{\vlambda }\left (\vu \right ) \right )} {\left ( \frac { \partial \vt _{\vlambda }\left (\vu \right ) }{ \partial \vlambda } \right )}^{\top } \frac { \partial \vt _{\vlambda }\left (\vu \right ) }{ \partial \vlambda } \nabla f\left ( \vt _{\vlambda }\left (\vu \right ) \right ) \nonumber \\ &\;= {\vg _{f}^{\top }} {\left ( \frac { \partial \vt _{\vlambda }\left (\vu \right ) }{ \partial \vlambda } \right )}^{\top } \frac { \partial \vt _{\vlambda }\left (\vu \right ) }{ \partial \vlambda } \vg _{f}.\label {eq:variational_gradient_norm_identity_eq1} \end {alignat} \par \paragraph {Proof for Cholesky} \par Let \(p\) denote the number of scalar variational parameters such that \(\vlambda = (\lambda _1, \ldots , \lambda _p)\). Then, \begin {alignat*}{2} &{\left ( \frac { \partial \vt _{\vlambda }\left (\vu \right ) }{ \partial \vlambda } \right )}^{\top } \frac { \partial \vt _{\vlambda }\left (\vu \right ) }{ \partial \vlambda } \\ &\;= \sum ^{d}_{i=1} \frac { \partial \vt _{\vlambda }\left (\vu \right ) }{ \partial m_i } {\left ( \frac { \partial \vt _{\vlambda }\left (\vu \right ) }{ \partial m_i } \right )}^{\top } + \sum ^{d}_{i=1} \sum ^{d}_{j \leq i} \frac { \partial \vt _{\vlambda }\left (\vu \right ) }{ \partial \lambda _{C_{ij}} } {\left ( \frac { \partial \vt _{\vlambda }\left (\vu \right ) }{ \partial \lambda _{C_{ij}} } \right )}^{\top }, \end {alignat*} where \(\lambda _{C_{ij}}\) denote the parameter responsible for the \(ij\)-th entry of \(\mC \), \(C_{ij}\). Notice that, unlike for the matrix square root parameterization~\citep {domke_provable_2019}, the sum for \(C_{ij}\) is only over the lower triangular section. \par For the derivatives with respect to \(m_i\) and \(C_{ij}\), \citet {domke_provable_2020, domke_provable_2019} show that \begin {alignat}{2} \frac {\partial \vt _{\vlambda }\left (\vu \right ) }{ \partial m_i } &= \ve _i \quad \frac {\partial \vt _{\vlambda }\left (\vu \right ) }{ \partial C_{ij} } &= \ve _i u_j,\label {eq:covariance_derivative} \end {alignat} where \(\ve _i\) is the unit basis of the \(i\)th component. \par Therefore, \begin {alignat}{2} &{\left ( \frac { \partial \vt _{\vlambda }\left (\vu \right ) }{ \partial \vlambda } \right )}^{\top } \frac { \partial \vt _{\vlambda }\left (\vu \right ) }{ \partial \vlambda } \nonumber \\ &\;= \sum ^{d}_{i=1} \ve _i \ve _i^{\top } + \sum ^{d}_{i=1} \sum _{j \leq i} \frac { \partial \vt _{\vlambda }\left (\vu \right ) }{ \partial \lambda _{C_{ij}} } {\left ( \frac { \partial \vt _{\vlambda }\left (\vu \right ) }{ \partial \lambda _{C_{ij}} } \right )}^{\top } \nonumber \\ &\;= \mI + \underbrace { \sum ^{d}_{i=1} \frac { \partial \vt _{\vlambda }\left (\vu \right ) }{ \partial \lambda _{C_{ii}} } {\left ( \frac { \partial \vt _{\vlambda }\left (\vu \right ) }{ \partial \lambda _{C_{ii}} } \right )}^{\top } }_{\text {diagonal of \(\mC \)}} \nonumber \\ &\qquad + \underbrace { \sum ^{d}_{i=1} \sum _{j < i} \frac { \partial \vt _{\vlambda }\left (\vu \right ) }{ \partial \lambda _{C_{ij}} } {\left ( \frac { \partial \vt _{\vlambda }\left (\vu \right ) }{ \partial \lambda _{C_{ij}} } \right )}^{\top }}_{\text {off-diagonal of \(\mC \)}}, \label {eq:thm:variational_gradient_norm_identity_eq2} \end {alignat} leaving us with the derivatives of the scale term. \par The gradient with respect to \(\lambda _{C_{ij}}\), however, depends on the parameterization. That is, \begin {alignat}{2} \frac { \partial \vt _{\vlambda }\left (\vu \right ) }{ \partial \lambda _{C_{ij}} } &= \frac { \partial \vt _{\vlambda }\left (\vu \right ) }{ \partial C_{ij} } \frac { \partial C_{ij} }{ \partial \lambda _{C_{ij}} } &= \ve _i u_j \frac { \partial C_{ij} }{ \partial \lambda _{C_{ij}} }. \label {eq:thm:variational_gradient_norm_identity_covderivative} \end {alignat} \par For the diagonal elements, \(\lambda _{C_{ii}} = d_i\). Thus, \begin {align} \frac { \partial C_{ii} }{ \partial s_i } = \frac { \partial \phi \left (s_i\right ) }{ \partial s_i } = \phi ^{\prime }\left (s_i\right ). \label {eq:thm:variational_gradient_norm_identity_diag} \end {align} And for the off-diagonal elements, \(\lambda _{L_{ij}} = L_{ij}\), and \begin {align} \frac { \partial C_{ij} }{ \partial L_{ij} } = 1. \label {eq:thm:variational_gradient_norm_identity_offdiag} \end {align} \par Plugging \cref {eq:thm:variational_gradient_norm_identity_diag,eq:thm:variational_gradient_norm_identity_offdiag,eq:thm:variational_gradient_norm_identity_covderivative} into \cref {eq:thm:variational_gradient_norm_identity_eq2}, \begin {alignat}{2} &{\left ( \frac { \partial \vt _{\vlambda }\left (\vu \right ) }{ \partial \vlambda } \right )}^{\top } \frac { \partial \vt _{\vlambda }\left (\vu \right ) }{ \partial \vlambda } \nonumber \\ &\;= \mI + \underbrace { \sum ^{d}_{i=1} {\left ( u_i \phi ^{\prime }\left (s_i\right ) \right )}^2 \ve _i \ve _i^{\top } }_{\text {diagonal of \(\mC \)}} + \underbrace { \sum ^{d}_{i=1} \sum _{j=1, j < i} u_j^2 \, \ve _i \ve _i^{\top } }_{\text {off-diagonal of \(\mC \)}} \nonumber \\ &\;= \mI + \underbrace { \sum ^{d}_{i=1} u_i^2 {\left (\phi ^{\prime }\left (s_i\right ) \right )}^2 \ve _i \ve _i^{\top } }_{\text {diagonal of \(\mC \)}} + \underbrace { \sum ^{d}_{i=1} \sum _{j \leq i} u_j^2 \, \ve _i \ve _i^{\top } - \sum ^{d}_{i=1} u_i^2 \, \ve _i \ve _i^{\top } }_{\text {off-diagonal of \(\mC \)}} \nonumber \\ &\;= \mI + \underbrace { \mU \, \mPhi }_{\text {diagonal of \(\mC \)}} + \underbrace { \mY - \mU }_{\text {off-diagonal of \(\mC \)}} \nonumber \\ &\;= \left ( \mI + \mY \right ) + \mU \left ( \mPhi - \mI \right ), \label {eq:variational_gradient_norm_identity_jacinner} \end {alignat} where \(\mU ,\mPhi \) are diagonal matrices defined as \begin {alignat*}{2} & \mPhi &&= \mathrm {diag}\left ( \left [ {\phi ^{\prime }\left (s_1\right )}^2, \ldots , {\phi ^{\prime }\left (s_d\right )}^2 \right ] \right ) \\ &\mU &&= \mathrm {diag}\left ( \left [ u_1^2, \ldots , u_d^2 \right ] \right ) \\ &\mY &&= \mathrm {diag}\left ( \left [ u_1^2, u_1^2 + u_2^2,\, \ldots \,, {\textstyle \sum ^{d}_{i=1} u_i^2} \right ] \right ). \end {alignat*} The major difference with the proof of \citet [Lemma 8]{domke_provable_2019} for the matrix square root case is that we sum of the \(u_j^2 \ve _i \ve _i^{\top }\) terms is over the \textit {lower diagonal elements} instead of all the elements. This is the variance reduction effect we get for using the Cholesky parameterization. \par Coming back to \cref {eq:variational_gradient_norm_identity_eq1}, \begin {alignat}{2} &\norm {\nabla _{\vlambda } f\left ( \vt _{\vlambda }\left (\vu \right ) \right )}_2^2 \nonumber \\ \;&= \vg _f^{\top } {\left ( \frac { \partial \vt _{\vlambda }\left (\vu \right ) }{ \partial \vlambda } \right )}^{\top } \frac { \partial \vt _{\vlambda }\left (\vu \right ) }{ \partial \vlambda } \vg \nonumber \\ &= \vg _{f}^{\top } \Big ( \left ( \mI + \mY \right ) + \mU \left ( \mPhi - \mI \right ) \Big ) \vg _{f} \nonumber \\ &= {\lVert \vg _{f} \rVert }_2^2 + \vg _{f}^{\top } \mY \vg _{f} + \vg _{f}^{\top } \mU \left ( \mPhi - \mI \right ) \vg _{f}. \label {eq:variational_gradient_norm_identity_conclusion} \end {alignat} \par \paragraph {Proof for Mean-field} For the mean-field variational family, the covariance has only diagonal elements. Therefore, \cref {eq:variational_gradient_norm_identity_jacinner} becomes \begin {alignat}{2} {\left ( \frac { \partial \vt _{\vlambda }\left (\vu \right ) }{ \partial \vlambda } \right )}^{\top } \frac { \partial \vt _{\vlambda }\left (\vu \right ) }{ \partial \vlambda } &= \mI + \mU \mPhi , \nonumber \end {alignat} and \cref {eq:variational_gradient_norm_identity_conclusion} becomes \begin {alignat}{2} \norm {\nabla _{\vlambda } f\left ( \vt _{\vlambda }\left (\vu \right ) \right )} = \vg _{f}^{\top } \left ( \mI + \mU \mPhi \right ) \vg _{f} = {\lVert \vg _{f} \rVert }_2^2 + \vg _{f}^{\top } \mU \mPhi \vg _{f}. \nonumber \end {alignat}\end{proof}
