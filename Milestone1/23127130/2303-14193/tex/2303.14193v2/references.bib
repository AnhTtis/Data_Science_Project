

% Journals

% First the Full Name is given, then the abbreviation used in the AMS Math
% Reviews, with an indication if it could not be found there.
% Note the 2nd overwrites the 1st, so swap them if you want the full name.

 %{AMS}
 @String{AMSTrans = "American Mathematical Society Translations" }
 @String{AMSTrans = "Amer. Math. Soc. Transl." }
 @String{BullAMS = "Bulletin of the American Mathematical Society" }
 @String{BullAMS = "Bull. Amer. Math. Soc." }
 @String{ProcAMS = "Proceedings of the American Mathematical Society" }
 @String{ProcAMS = "Proc. Amer. Math. Soc." }
 @String{TransAMS = "Transactions of the American Mathematical Society" }
 @String{TransAMS = "Trans. Amer. Math. Soc." }

 %ACM
 @String{CACM = "Communications of the {ACM}" }
 @String{CACM = "Commun. {ACM}" }
 @String{CompServ = "Comput. Surveys" }
 @String{JACM = "J. ACM" }
 @String{ACMMathSoft = "{ACM} Transactions on Mathematical Software" }
 @String{ACMMathSoft = "{ACM} Trans. Math. Software" }
 @String{SIGNUM = "{ACM} {SIGNUM} Newsletter" }
 @String{SIGNUM = "{ACM} {SIGNUM} Newslett." }

 @String{AmerSocio = "American Journal of Sociology" }
 @String{AmerStatAssoc = "Journal of the American Statistical Association" }
 @String{AmerStatAssoc = "J. Amer. Statist. Assoc." }
 @String{ApplMathComp = "Applied Mathematics and Computation" }
 @String{ApplMathComp = "Appl. Math. Comput." }
 @String{AmerMathMonthly = "American Mathematical Monthly" }
 @String{AmerMathMonthly = "Amer. Math. Monthly" }
 @String{BIT = "{BIT}" }
 @String{BritStatPsych = "British Journal of Mathematical and Statistical
          Psychology" }
 @String{BritStatPsych = "Brit. J. Math. Statist. Psych." }
 @String{CanMathBull = "Canadian Mathematical Bulletin" }
 @String{CanMathBull = "Canad. Math. Bull." }
 @String{CompApplMath = "Journal of Computational and Applied Mathematics" }
 @String{CompApplMath = "J. Comput. Appl. Math." }
 @String{CompPhys = "Journal of Computational Physics" }
 @String{CompPhys = "J. Comput. Phys." }
 @String{CompStruct = "Computers and Structures" }
 @String{CompStruct = "Comput. \& Structures" }
 @String{CompJour = "The Computer Journal" }
 @String{CompJour = "Comput. J." }
 @String{CompSysSci = "Journal of Computer and System Sciences" }
 @String{CompSysSci = "J. Comput. System Sci." }
 @String{Computing = "Computing" }
 @String{ContempMath = "Contemporary Mathematics" }
 @String{ContempMath = "Contemp. Math." }
 @String{Crelle = "Crelle's Journal" }
 @String{GiornaleMath = "Giornale di Mathematiche" }
 @String{GiornaleMath = "Giorn. Mat." } % didn't find in AMS MR., ibid.

 %IEEE
 @String{Computer = "{IEEE} Computer" }
 @String{IEEETransComp = "{IEEE} Transactions on Computers" }
 @String{IEEETransComp = "{IEEE} Trans. Comput." }
 @String{IEEETransAC = "{IEEE} Transactions on Automatic Control" }
 @String{IEEETransAC = "{IEEE} Trans. Automat. Control" }
 @String{IEEESpec = "{IEEE} Spectrum" } % didn't find in AMS MR
 @String{ProcIEEE = "Proceedings of the {IEEE}" }
 @String{ProcIEEE = "Proc. {IEEE}" } % didn't find in AMS MR
 @String{IEEETransAeroElec = "{IEEE} Transactions on Aerospace and Electronic
     Systems" }
 @String{IEEETransAeroElec = "{IEEE} Trans. Aerospace Electron. Systems" }

 @String{IMANumerAna = "{IMA} Journal of Numerical Analysis" }
 @String{IMANumerAna = "{IMA} J. Numer. Anal." }
 @String{InfProcLet = "Information Processing Letters" }
 @String{InfProcLet = "Inform. Process. Lett." }
 @String{InstMathApp = "Journal of the Institute of Mathematics and
     its Applications" }
 @String{InstMathApp = "J. Inst. Math. Appl." }
 @String{IntControl = "International Journal of Control" }
 @String{IntControl = "Internat. J. Control" }
 @String{IntNumerEng = "International Journal for Numerical Methods in
     Engineering" }
 @String{IntNumerEng = "Internat. J. Numer. Methods Engrg." }
 @String{IntSuper = "International Journal of Supercomputing Applications" }
 @String{IntSuper = "Internat. J. Supercomputing Applic." } % didn't find
%% in AMS MR
 @String{Kibernetika = "Kibernetika" }
 @String{JResNatBurStand = "Journal of Research of the National Bureau
     of Standards" }
 @String{JResNatBurStand = "J. Res. Nat. Bur. Standards" }
 @String{LinAlgApp = "Linear Algebra and its Applications" }
 @String{LinAlgApp = "Linear Algebra Appl." }
 @String{MathAnaAppl = "Journal of Mathematical Analysis and Applications" }
 @String{MathAnaAppl = "J. Math. Anal. Appl." }
 @String{MathAnnalen = "Mathematische Annalen" }
 @String{MathAnnalen = "Math. Ann." }
 @String{MathPhys = "Journal of Mathematical Physics" }
 @String{MathPhys = "J. Math. Phys." }
 @String{MathComp = "Mathematics of Computation" }
 @String{MathComp = "Math. Comp." }
 @String{MathScand = "Mathematica Scandinavica" }
 @String{MathScand = "Math. Scand." }
 @String{TablesAidsComp = "Mathematical Tables and Other Aids to Computation" }
 @String{TablesAidsComp = "Math. Tables Aids Comput." }
 @String{NumerMath = "Numerische Mathematik" }
 @String{NumerMath = "Numer. Math." }
 @String{PacificMath = "Pacific Journal of Mathematics" }
 @String{PacificMath = "Pacific J. Math." }
 @String{ParDistComp = "Journal of Parallel and Distributed Computing" }
 @String{ParDistComp = "J. Parallel and Distrib. Comput." } % didn't find
%% in AMS MR
 @String{ParComputing = "Parallel Computing" }
 @String{ParComputing = "Parallel Comput." }
 @String{PhilMag = "Philosophical Magazine" }
 @String{PhilMag = "Philos. Mag." }
 @String{ProcNAS = "Proceedings of the National Academy of Sciences
                    of the USA" }
 @String{ProcNAS = "Proc. Nat. Acad. Sci. U. S. A." }
 @String{Psychometrika = "Psychometrika" }
 @String{QuartMath = "Quarterly Journal of Mathematics, Oxford, Series (2)" }
 @String{QuartMath = "Quart. J. Math. Oxford Ser. (2)" }
 @String{QuartApplMath = "Quarterly of Applied Mathematics" }
 @String{QuartApplMath = "Quart. Appl. Math." }
 @String{RevueInstStat = "Review of the International Statisical Institute" }
 @String{RevueInstStat = "Rev. Inst. Internat. Statist." }

 %SIAM
 @String{JSIAM = "Journal of the Society for Industrial and Applied
     Mathematics" }
 @String{JSIAM = "J. Soc. Indust. Appl. Math." }
 @String{JSIAMB = "Journal of the Society for Industrial and Applied
     Mathematics, Series B, Numerical Analysis" }
 @String{JSIAMB = "J. Soc. Indust. Appl. Math. Ser. B Numer. Anal." }
 @String{SIAMAlgMeth = "{SIAM} Journal on Algebraic and Discrete Methods" }
 @String{SIAMAlgMeth = "{SIAM} J. Algebraic Discrete Methods" }
 @String{SIAMAppMath = "{SIAM} Journal on Applied Mathematics" }
 @String{SIAMAppMath = "{SIAM} J. Appl. Math." }
 @String{SIAMComp = "{SIAM} Journal on Computing" }
 @String{SIAMComp = "{SIAM} J. Comput." }
 @String{SIAMMatrix = "{SIAM} Journal on Matrix Analysis and Applications" }
 @String{SIAMMatrix = "{SIAM} J. Matrix Anal. Appl." }
 @String{SIAMNumAnal = "{SIAM} Journal on Numerical Analysis" }
 @String{SIAMNumAnal = "{SIAM} J. Numer. Anal." }
 @String{SIAMReview = "{SIAM} Review" }
 @String{SIAMReview = "{SIAM} Rev." }
 @String{SIAMSciStat = "{SIAM} Journal on Scientific and Statistical
     Computing" }
 @String{SIAMSciStat = "{SIAM} J. Sci. Statist. Comput." }

 @String{SoftPracExp = "Software Practice and Experience" }
 @String{SoftPracExp = "Software Prac. Experience" } % didn't find in AMS MR
 @String{StatScience = "Statistical Science" }
 @String{StatScience = "Statist. Sci." }
 @String{Techno = "Technometrics" }
 @String{USSRCompMathPhys = "{USSR} Computational Mathematics and Mathematical
     Physics" }
 @String{USSRCompMathPhys = "{U. S. S. R.} Comput. Math. and Math. Phys." }
 @String{VLSICompSys = "Journal of {VLSI} and Computer Systems" }
 @String{VLSICompSys = "J. {VLSI} Comput. Syst." }
 @String{ZAngewMathMech = "Zeitschrift fur Angewandte Mathematik und
     Mechanik" }
 @String{ZAngewMathMech = "Z. Angew. Math. Mech." }
 @String{ZAngewMathPhys = "Zeitschrift fur Angewandte Mathematik und Physik" }
 @String{ZAngewMathPhys = "Z. Angew. Math. Phys." }

% Publishers % ================================================= |

 @String{Academic = "Academic Press" }
 @String{ACMPress = "{ACM} Press" }
 @String{AdamHilger = "Adam Hilger" }
 @String{AddisonWesley = "Addison-Wesley" }
 @String{AllynBacon = "Allyn and Bacon" }
 @String{AMS = "American Mathematical Society" }
 @String{Birkhauser = "Birkha{\"u}ser" }
 @String{CambridgePress = "Cambridge University Press" }
 @String{Chelsea = "Chelsea" }
 @String{ClaredonPress = "Claredon Press" }
 @String{DoverPub = "Dover Publications" }
 @String{Eyolles = "Eyolles" }
 @String{HoltRinehartWinston = "Holt, Rinehart and Winston" }
 @String{Interscience = "Interscience" }
 @String{JohnsHopkinsPress = "The Johns Hopkins University Press" }
 @String{JohnWileySons = "John Wiley and Sons" }
 @String{Macmillan = "Macmillan" }
 @String{MathWorks = "The Math Works Inc." }
 @String{McGrawHill = "McGraw-Hill" }
 @String{NatBurStd = "National Bureau of Standards" }
 @String{NorthHolland = "North-Holland" }
 @String{OxfordPress = "Oxford University Press" }  %address Oxford or London?
 @String{PergamonPress = "Pergamon Press" }
 @String{PlenumPress = "Plenum Press" }
 @String{PrenticeHall = "Prentice-Hall" }
 @String{SIAMPub = "{SIAM} Publications" }
 @String{Springer = "Springer-Verlag" }
 @String{TexasPress = "University of Texas Press" }
 @String{VanNostrand = "Van Nostrand" }
 @String{WHFreeman = "W. H. Freeman and Co." }

%Entries

@article{yuan_gcng_2020,
  title={GCNG: graph convolutional networks for inferring gene interaction from spatial transcriptomics data},
  author={Yuan, Ye and Bar-Joseph, Ziv},
  journal={Genome biology},
  volume={21},
  number={1},
  pages={1--16},
  year={2020},
  publisher={BioMed Central}
}

@article{karbalayghareh_chromatin_2022,
  title={Chromatin interaction--aware gene regulatory modeling with graph attention networks},
  author={Karbalayghareh, Alireza and Sahin, Merve and Leslie, Christina S},
  journal={Genome Research},
  volume={32},
  number={5},
  pages={930--944},
  year={2022},
  publisher={Cold Spring Harbor Lab}
}

@article{bigness_integrating_2022,
  title={Integrating long-range regulatory interactions to predict gene expression using graph convolutional networks},
  author={Bigness, Jeremy and Loinaz, Xavier and Patel, Shalin and Larschan, Erica and Singh, Ritambhara},
  journal={Journal of Computational Biology},
  volume={29},
  number={5},
  pages={409--424},
  year={2022},
  publisher={Mary Ann Liebert, Inc., publishers 140 Huguenot Street, 3rd Floor New~…}
}

@article{qiu2006wavelet,
  title={Wavelet filter-based weak signature detection method and its application on rolling element bearing prognostics},
  author={Qiu, Hai and Lee, Jay and Lin, Jing and Yu, Gang},
  journal={Journal of sound and vibration},
  volume={289},
  number={4-5},
  pages={1066--1090},
  year={2006},
  publisher={Elsevier}
}

@ARTICLE{9078761,
  author={Neupane, Dhiraj and Seok, Jongwon},
  journal={IEEE Access}, 
  title={Bearing Fault Detection and Diagnosis Using Case Western Reserve University Dataset With Deep Learning Approaches: A Review}, 
  year={2020},
  volume={8},
  number={},
  pages={93155-93178},
  doi={10.1109/ACCESS.2020.2990528}}
  
  @article{zhang2017new,
  title={A new deep learning model for fault diagnosis with good anti-noise and domain adaptation ability on raw vibration signals},
  author={Zhang, Wei and Peng, Gaoliang and Li, Chuanhao and Chen, Yuanhang and Zhang, Zhujun},
  journal={Sensors},
  volume={17},
  number={2},
  pages={425},
  year={2017},
  publisher={Multidisciplinary Digital Publishing Institute}
}

@inproceedings{liu2021swin,
  title={Swin transformer: Hierarchical vision transformer using shifted windows},
  author={Liu, Ze and Lin, Yutong and Cao, Yue and Hu, Han and Wei, Yixuan and Zhang, Zheng and Lin, Stephen and Guo, Baining},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={10012--10022},
  year={2021}
}

@article{fan2020universal,
  title={Universal approximation with quadratic deep networks},
  author={Fan, Fenglei and Xiong, Jinjun and Wang, Ge},
  journal={Neural Networks},
  volume={124},
  pages={383--392},
  year={2020},
  publisher={Elsevier}
}

@inproceedings{ross2018improving,
  title={Improving the adversarial robustness and interpretability of deep neural networks by regularizing their input gradients},
  author={Ross, Andrew and Doshi-Velez, Finale},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={32},
  number={1},
  year={2018}
}

@article{eren2019generic,
  title={A generic intelligent bearing fault diagnosis system using compact adaptive 1D CNN classifier},
  author={Eren, Levent and Ince, Turker and Kiranyaz, Serkan},
  journal={Journal of Signal Processing Systems},
  volume={91},
  number={2},
  pages={179--189},
  year={2019},
  publisher={Springer}
}

@article{WANG2021108518,
title = {Bearing fault diagnosis based on vibro-acoustic data fusion and 1D-CNN network},
journal = {Measurement},
volume = {173},
pages = {108518},
year = {2021},
issn = {0263-2241},
doi = {https://doi.org/10.1016/j.measurement.2020.108518},
url = {https://www.sciencedirect.com/science/article/pii/S0263224120310447},
author = {Xin Wang and Dongxing Mao and Xiaodong Li},
}

@article{neupane2020bearing,
  title={Bearing fault detection and diagnosis using case western reserve university dataset with deep learning approaches: A review},
  author={Neupane, Dhiraj and Seok, Jongwon},
  journal={IEEE Access},
  volume={8},
  pages={93155--93178},
  year={2020},
  publisher={IEEE}
}

@article{wu2020fault,
  title={Fault-attention generative probabilistic adversarial autoencoder for machine anomaly detection},
  author={Wu, Jingyao and Zhao, Zhibin and Sun, Chuang and Yan, Ruqiang and Chen, Xuefeng},
  journal={IEEE Transactions on Industrial Informatics},
  volume={16},
  number={12},
  pages={7479--7488},
  year={2020},
  publisher={IEEE}
}

@article{ding2022self,
  title={Self-supervised pretraining via contrast learning for intelligent incipient fault detection of bearings},
  author={Ding, Yifei and Zhuang, Jichao and Ding, Peng and Jia, Minping},
  journal={Reliability Engineering \& System Safety},
  volume={218},
  pages={108126},
  year={2022},
  publisher={Elsevier}
}

@article{boudiaf2016comparative,
  title={A comparative study of various methods of bearing faults diagnosis using the case Western Reserve University data},
  author={Boudiaf, Adel and Moussaoui, Abdelkrim and Dahane, Amine and Atoui, Issam},
  journal={Journal of Failure Analysis and Prevention},
  volume={16},
  number={2},
  pages={271--284},
  year={2016},
  publisher={Springer}
}

@article{motor1985report,
  title={Report of large motor reliability survey of industrial and commercial installations, Part I},
  author={Motor Reliability Working Group and others},
  journal={IEEE Transactions on Industrial Applications},
  volume={1},
  number={4},
  pages={865--872},
  year={1985}
}

@article{thorsen1995survey,
  title={A survey of faults on induction motors in offshore oil industry, petrochemical industry, gas terminals, and oil refineries},
  author={Thorsen, Olav Vaag and Dalva, Magnus},
  journal={IEEE Transactions on Industry Applications},
  volume={31},
  number={5},
  pages={1186--1196},
  year={1995},
  publisher={IEEE}
}

@article{bonnett2008increased,
  title={Increased efficiency versus increased reliability},
  author={Bonnett, Austin H and Yung, Chuck},
  journal={IEEE Industry Applications Magazine},
  volume={14},
  number={1},
  pages={29--36},
  year={2008},
  publisher={IEEE}
}

@article{zhang2018visual,
  title={Visual interpretability for deep learning: a survey},
  author={Zhang, Quan-shi and Zhu, Song-Chun},
  journal={Frontiers of Information Technology \& Electronic Engineering},
  volume={19},
  number={1},
  pages={27--39},
  year={2018},
  publisher={Springer}
}

@article{mcfadden1984model,
  title={Model for the vibration produced by a single point defect in a rolling element bearing},
  author={McFadden, PD and Smith, JD},
  journal={Journal of sound and vibration},
  volume={96},
  number={1},
  pages={69--82},
  year={1984},
  publisher={Elsevier}
}

@inproceedings{pandarakone2018deep,
  title={Deep neural network based bearing fault diagnosis of induction motor using fast Fourier transform analysis},
  author={Pandarakone, Shrinathan Esakimuthu and Masuko, Makoto and Mizuno, Yukio and Nakamura, Hisahide},
  booktitle={2018 IEEE energy conversion congress and exposition (ECCE)},
  pages={3214--3221},
  year={2018},
  organization={IEEE}
}

@article{gao2015feature,
  title={Feature extraction and recognition for rolling element bearing fault utilizing short-time Fourier transform and non-negative matrix factorization},
  author={Gao, Huizhong and Liang, Lin and Chen, Xiaoguang and Xu, Guanghua},
  journal={Chinese Journal of Mechanical Engineering},
  volume={28},
  number={1},
  pages={96--105},
  year={2015},
  publisher={SpringerOpen}
}

@article{osman2016morphological,
  title={A morphological Hilbert-Huang transform technique for bearing fault detection},
  author={Osman, Shazali and Wang, Wilson},
  journal={IEEE Transactions on Instrumentation and Measurement},
  volume={65},
  number={11},
  pages={2646--2656},
  year={2016},
  publisher={IEEE}
}

@article{rai2007bearing,
  title={Bearing fault diagnosis using FFT of intrinsic mode functions in Hilbert--Huang transform},
  author={Rai, VK and Mohanty, AR},
  journal={Mechanical systems and signal processing},
  volume={21},
  number={6},
  pages={2607--2615},
  year={2007},
  publisher={Elsevier}
}

@article{wen2017new,
  title={A new convolutional neural network-based data-driven fault diagnosis method},
  author={Wen, Long and Li, Xinyu and Gao, Liang and Zhang, Yuyan},
  journal={IEEE Transactions on Industrial Electronics},
  volume={65},
  number={7},
  pages={5990--5998},
  year={2017},
  publisher={IEEE}
}



@article{wang2019understanding,
  title={Understanding and learning discriminant features based on multiattention 1DCNN for wheelset bearing fault diagnosis},
  author={Wang, Huan and Liu, Zhiliang and Peng, Dandan and Qin, Yong},
  journal={IEEE Transactions on Industrial Informatics},
  volume={16},
  number={9},
  pages={5735--5745},
  year={2019},
  publisher={IEEE}
}

@article{shenfield2020novel,
  title={A novel deep learning model for the detection and identification of rolling element-bearing faults},
  author={Shenfield, Alex and Howarth, Martin},
  journal={Sensors},
  volume={20},
  number={18},
  pages={5112},
  year={2020},
  publisher={Multidisciplinary Digital Publishing Institute}
}

@article{zhang2022fault,
  title={Fault diagnosis for small samples based on attention mechanism},
  author={Zhang, Xin and He, Chao and Lu, Yanping and Chen, Biao and Zhu, Le and Zhang, Li},
  journal={Measurement},
  volume={187},
  pages={110242},
  year={2022},
  publisher={Elsevier}
}

@ARTICLE{b1,
  author={Fan, Fenglei and Shan, Hongming and Kalra, Mannudeep K. and Singh, Ramandeep and Qian, Guhan and Getzin, Matthew and Teng, Yueyang and Hahn, Juergen and Wang, Ge},
  journal={IEEE Transactions on Medical Imaging}, 
  title={Quadratic Autoencoder (Q-AE) for Low-Dose CT Denoising}, 
  year={2020},
  volume={39},
  number={6},
  pages={2035-2050},
  doi={10.1109/TMI.2019.2963248}}
  
@article{b2,
  title={A new type of neurons for machine learning},
  author={Fan, Fenglei and Cong, Wenxiang and Wang, Ge},
  journal={International journal for numerical methods in biomedical engineering},
  volume={34},
  number={2},
  pages={e2920},
  year={2018},
  publisher={Wiley Online Library}
}

@article{fan2021interpretability,
  title={On interpretability of artificial neural networks: A survey},
  author={Fan, Feng-Lei and Xiong, Jinjun and Li, Mengzhou and Wang, Ge},
  journal={IEEE Transactions on Radiation and Plasma Medical Sciences},
  volume={5},
  number={6},
  pages={741--760},
  year={2021},
  publisher={IEEE}
}

@article{fan2018generalized,
  title={Generalized backpropagation algorithm for training second-order neural networks},
  author={Fan, Fenglei},
  journal={International journal for numerical methods in biomedical engineering},
  volume={34},
  number={5},
  pages={e2956},
  year={2018},
  publisher={Wiley Online Library}
}

@article{b4,
  title={Universal approximation with quadratic deep networks},
  author={Fan, Fenglei and Xiong, Jinjun and Wang, Ge},
  journal={Neural Networks},
  volume={124},
  pages={383--392},
  year={2020},
  publisher={Elsevier}
}

@article{fan2021expressivity,
  title={Expressivity and Trainability of Quadratic Networks},
  author={Fan, Feng-Lei and Li, Mengzhou and Wang, Fei and Lai, Rongjie and Wang, Ge},
  journal={arXiv preprint},
  year={2021}
}

@article{yarotsky2017error,
  title={Error bounds for approximations with deep ReLU networks},
  author={Yarotsky, Dmitry},
  journal={Neural Networks},
  volume={94},
  pages={103--114},
  year={2017},
  publisher={Elsevier}
}

@inproceedings{eldan2016power,
  title={The power of depth for feedforward neural networks},
  author={Eldan, Ronen and Shamir, Ohad},
  booktitle={Conference on learning theory},
  pages={907--940},
  year={2016},
  organization={PMLR}
}

@article{liu2021dendrite,
  title={Dendrite net: A white-box module for classification, regression, and system identification},
  author={Liu, Gang and Wang, Jing},
  journal={IEEE Transactions on Cybernetics},
  year={2021},
  publisher={IEEE}
}

@incollection{bottou2012stochastic,
  title={Stochastic gradient descent tricks},
  author={Bottou, L{\'e}on},
  booktitle={Neural networks: Tricks of the trade},
  pages={421--436},
  year={2012},
  publisher={Springer}
}

@article{van2008visualizing,
  title={Visualizing data using t-SNE.},
  author={Van der Maaten, Laurens and Hinton, Geoffrey},
  journal={Journal of machine learning research},
  volume={9},
  number={11},
  year={2008}
}

@inproceedings{chattopadhay2018grad,
  title={Grad-cam++: Generalized gradient-based visual explanations for deep convolutional networks},
  author={Chattopadhay, Aditya and Sarkar, Anirban and Howlader, Prantik and Balasubramanian, Vineeth N},
  booktitle={2018 IEEE winter conference on applications of computer vision (WACV)},
  pages={839--847},
  year={2018},
  organization={IEEE}
}

@inproceedings{zhou2016learning,
  title={Learning deep features for discriminative localization},
  author={Zhou, Bolei and Khosla, Aditya and Lapedriza, Agata and Oliva, Aude and Torralba, Antonio},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={2921--2929},
  year={2016}
}

@article{zhongdevelopment,
  title={Development of a Plug-and-Play Anti-Noise Module for Fault Diagnosis of Rotating Machines in Nuclear Power Plants},
  author={Zhong, Xianping and Wang, Fei and Ban, Heng},
  publisher={PNUCENE-D-22-00041}
}


@Article{app9091823,
AUTHOR = {Zhuang, Zilong and Lv, Huichun and Xu, Jie and Huang, Zizhao and Qin, Wei},
TITLE = {A Deep Learning Method for Bearing Fault Diagnosis through Stacked Residual Dilated Convolutions},
JOURNAL = {Applied Sciences},
VOLUME = {9},
YEAR = {2019},
NUMBER = {9},
ARTICLE-NUMBER = {1823},
URL = {https://www.mdpi.com/2076-3417/9/9/1823},
ISSN = {2076-3417},
DOI = {10.3390/app9091823}
}

@article{li2019understanding,
  title={Understanding and improving deep learning-based rolling bearing fault diagnosis with attention mechanism},
  author={Li, Xiang and Zhang, Wei and Ding, Qian},
  journal={Signal processing},
  volume={161},
  pages={136--154},
  year={2019},
  publisher={Elsevier}
}

@article{yang2020interpreting,
  title={Interpreting network knowledge with attention mechanism for bearing fault diagnosis},
  author={Yang, Zhi-bo and Zhang, Jun-peng and Zhao, Zhi-bin and Zhai, Zhi and Chen, Xue-feng},
  journal={Applied Soft Computing},
  volume={97},
  pages={106829},
  year={2020},
  publisher={Elsevier}
}

@article{wang2020intelligent,
  title={Intelligent bearing fault diagnosis using multi-head attention-based CNN},
  author={Wang, Hui and Xu, Jiawen and Yan, Ruqiang and Sun, Chuang and Chen, Xuefeng},
  journal={Procedia Manufacturing},
  volume={49},
  pages={112--118},
  year={2020},
  publisher={Elsevier}
}

@article{wan2021qscgan,
  title={QSCGAN: An Un-Supervised Quick Self-Attention Convolutional GAN for LRE Bearing Fault Diagnosis Under Limited Label-Lacked Data},
  author={Wan, Wenqing and He, Shuilong and Chen, Jinglong and Li, Aimin and Feng, Yong},
  journal={IEEE Transactions on Instrumentation and Measurement},
  volume={70},
  pages={1--16},
  year={2021},
  publisher={IEEE}
}

@article{wang2019new,
  title={A new intelligent bearing fault diagnosis method using SDP representation and SE-CNN},
  author={Wang, Hui and Xu, Jiawen and Yan, Ruqiang and Gao, Robert X},
  journal={IEEE Transactions on Instrumentation and Measurement},
  volume={69},
  number={5},
  pages={2377--2389},
  year={2019},
  publisher={IEEE}
}

@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@article{chaudhari2021attentive,
  title={An attentive survey of attention models},
  author={Chaudhari, Sneha and Mithal, Varun and Polatkan, Gungor and Ramanath, Rohan},
  journal={ACM Transactions on Intelligent Systems and Technology (TIST)},
  volume={12},
  number={5},
  pages={1--32},
  year={2021},
  publisher={ACM New York, NY}
}

@article{luong2015effective,
  title={Effective approaches to attention-based neural machine translation},
  author={Luong, Minh-Thang and Pham, Hieu and Manning, Christopher D},
  journal={arXiv preprint arXiv:1508.04025},
  year={2015}
}

@article{randall2011rolling,
  title={Rolling element bearing diagnostics—A tutorial},
  author={Randall, Robert B and Antoni, Jerome},
  journal={Mechanical systems and signal processing},
  volume={25},
  number={2},
  pages={485--520},
  year={2011},
  publisher={Elsevier}
}

@article{smith2015rolling,
  title={Rolling element bearing diagnostics using the Case Western Reserve University data: A benchmark study},
  author={Smith, Wade A and Randall, Robert B},
  journal={Mechanical systems and signal processing},
  volume={64},
  pages={100--131},
  year={2015},
  publisher={Elsevier}
}

@inproceedings{wang2018bearing,
  title={Bearing fault diagnosis method based on hilbert envelope demodulation analysis},
  author={Wang, Nan and Liu, Xia},
  booktitle={IOP Conference Series: Materials Science and Engineering},
  volume={436},
  number={1},
  pages={012009},
  year={2018},
  organization={IOP Publishing}
}

@article{osman2013enhanced,
  title={An enhanced Hilbert--Huang transform technique for bearing condition monitoring},
  author={Osman, Shazali and Wang, Wilson},
  journal={Measurement Science and Technology},
  volume={24},
  number={8},
  pages={085004},
  year={2013},
  publisher={IOP Publishing}
}

@article{bozchalooi2008joint,
  title={A joint resonance frequency estimation and in-band noise reduction method for enhancing the detectability of bearing fault signals},
  author={Bozchalooi, I Soltani and Liang, Ming},
  journal={Mechanical Systems and Signal Processing},
  volume={22},
  number={4},
  pages={915--933},
  year={2008},
  publisher={Elsevier}
}

@inproceedings{woo2018cbam,
  title={Cbam: Convolutional block attention module},
  author={Woo, Sanghyun and Park, Jongchan and Lee, Joon-Young and Kweon, In So},
  booktitle={Proceedings of the European conference on computer vision (ECCV)},
  pages={3--19},
  year={2018}
}


@article{zhao2019deep,
  title={Deep residual shrinkage networks for fault diagnosis},
  author={Zhao, Minghang and Zhong, Shisheng and Fu, Xuyun and Tang, Baoping and Pecht, Michael},
  journal={IEEE Transactions on Industrial Informatics},
  volume={16},
  number={7},
  pages={4681--4690},
  year={2019},
  publisher={IEEE}
}



@article{borghesani2022bearing,
  title={Bearing signal models and their effect on bearing diagnostics},
  author={Borghesani, P and Smith, WA and Randall, RB and Antoni, J and El Badaoui, M and Peng, Z},
  journal={Mechanical Systems and Signal Processing},
  volume={174},
  pages={109077},
  year={2022},
  publisher={Elsevier}
}

@article{sun2022interpretable,
  title={An interpretable anti-noise network for rolling bearing fault diagnosis based on FSWT},
  author={Sun, Hongchun and Cao, Xu and Wang, Changdong and Gao, Sheng},
  journal={Measurement},
  volume={190},
  pages={110698},
  year={2022},
  publisher={Elsevier}
}

@article{chen2022fault,
  title={Fault Diagnosis for Limited Annotation Signals and Strong Noise Based on Interpretable Attention Mechanism},
  author={Chen, Biao and Liu, Tingting and He, Chao and Liu, Zecheng and Zhang, Li},
  journal={IEEE Sensors Journal},
  volume={22},
  number={12},
  pages={11865--11880},
  year={2022},
  publisher={IEEE}
}

@inproceedings{selvaraju2017grad,
  title={Grad-cam: Visual explanations from deep networks via gradient-based localization},
  author={Selvaraju, Ramprasaath R and Cogswell, Michael and Das, Abhishek and Vedantam, Ramakrishna and Parikh, Devi and Batra, Dhruv},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={618--626},
  year={2017}
}

@article{liao2022heterogeneous,
  title={Heterogeneous Autoencoder Empowered by Quadratic Neurons},
  author={Liao, Jing-Xiao and Hou, Bo-Jian and Dong, Hang-Cheng and Zhang, Hao and Ma, Jianwei and Sun, Jinwei and Zhang, Shiping and Fan, Feng-Lei},
  journal={arXiv preprint arXiv:2204.01707},
  year={2022}
}

@article{zhong2022development,
  title={Development of a plug-and-play anti-noise module for fault diagnosis of rotating machines in nuclear power plants},
  author={Zhong, Xianping and Wang, Fei and Ban, Heng},
  journal={Progress in Nuclear Energy},
  volume={151},
  pages={104344},
  year={2022},
  publisher={Elsevier}
}

@misc{
bearings,
author = {Power-MI},
title = {Rolling element bearings},
journal = {Website},
note = {\url{https://power-mi.com/content/rolling-element-bearing-components-and-failing-frequencies}}
}

@misc{
Bhandari,
author = {Bhandari, P.},
title = {An Easy Introduction to Statistical Significance (With Examples)},
journal = {Scribbr},
note = {\url{https://www.scribbr.com/statistics/statistical-significance/}},
year={2022}
}

@misc{
ved,
title = {P-value Significance Level},
journal = {Vedantu},
note = {\url{https://www.vedantu.com/maths/level-of-significance}},
}

@article{han2022intelligent,
  title={Intelligent vibration signal denoising method based on non-local fully convolutional neural network for rolling bearings},
  author={Han, Haoran and Wang, Huan and Liu, Zhiliang and Wang, Jiayi},
  journal={ISA transactions},
  volume={122},
  pages={13--23},
  year={2022},
  publisher={Elsevier}
}

@article{zhang2018investigation,
  title={Investigation assembly state of spindle bearing based on improved maximum correlated kurtosis deconvolution and support vector machine},
  author={Zhang, Yanfei and Li, Xiaohu and Wang, Sun’an and Sun, Yanhui},
  journal={Journal of Vibroengineering},
  volume={20},
  number={2},
  pages={963--978},
  year={2018},
  publisher={JVE International Ltd.}
}

@article{wang2020minimum,
  title={A minimum entropy deconvolution-enhanced convolutional neural networks for fault diagnosis of axial piston pumps},
  author={Wang, Shuhui and Xiang, Jiawei},
  journal={Soft Computing},
  volume={24},
  number={4},
  pages={2983--2997},
  year={2020},
  publisher={Springer}
}

@article{wiggins1978minimum,
  title={Minimum entropy deconvolution},
  author={Wiggins, Ralph A},
  journal={Geoexploration},
  volume={16},
  number={1-2},
  pages={21--35},
  year={1978},
  publisher={Elsevier}
}

@article{miller_adversarial_2020,
	title = {Adversarial {learning} {targeting} {deep} {neural} {network} {classification}: {a} {comprehensive} {review} of {defenses} {against} {attacks}},
	volume = {108},
	issn = {0018-9219, 1558-2256},
	shorttitle = {Adversarial {Learning} {Targeting} {Deep} {Neural} {Network} {Classification}},
	language = {en},
	number = {3},
	urldate = {2022-06-11},
	journal = {Proceedings of the IEEE},
	author = {Miller, David J. and Xiang, Zhen and Kesidis, George},
	year = {2020},
	pages = {402--433},
	file = {Adversarial_Learning_Targeting_Deep_Neural_Network_Classification_A_Comprehensive_Review_of_Defenses_Against_Attacks.pdf:D\:\\a1172\\Zotero\\storage\\FIQHEAPD\\Adversarial_Learning_Targeting_Deep_Neural_Network_Classification_A_Comprehensive_Review_of_Defenses_Against_Attacks.pdf:application/pdf},
}


@article{chen_survey_2020,
	title = {A {survey} of {adversarial} {learning} on {graphs}},
	abstract = {Deep learning models on graphs have achieved remarkable performance in various graph analysis tasks, e.g., node classification, link prediction and graph clustering. However, they expose uncertainty and unreliability against the well-designed inputs, i.e., adversarial examples. Accordingly, a line of studies have emerged for both attack and defense addressed in different graph analysis tasks, leading to the arms race in graph adversarial learning. Despite the booming works, there still lacks a unified problem definition and a comprehensive review. To bridge this gap, we investigate and summarize the existing works on graph adversarial learning tasks systemically. Specifically, we survey and unify the existing works w.r.t. attack and defense in graph analysis tasks, and give appropriate definitions and taxonomies at the same time. Besides, we emphasize the importance of related evaluation metrics, investigate and summarize them comprehensively. Hopefully, our works can provide a comprehensive overview and offer insights for the relevant researchers. More details of our works are available at https://github.com/gitgiter/Graph-Adversarial-Learning.},
	language = {en},
	urldate = {2022-03-16},
	journal = {arXiv preprint arXiv:2003.05730},
	author = {Chen, Liang and Li, Jintang and Peng, Jiaying and Xie, Tao and Cao, Zengxu and Xu, Kun and He, Xiangnan and Zheng, Zibin},
	year = {2020},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: TKDD under review},
	file = {Chen 等。 - 2020 - A Survey of Adversarial Learning on Graphs.pdf:D\:\\a1172\\Zotero\\storage\\GC78SX86\\Chen 等。 - 2020 - A Survey of Adversarial Learning on Graphs.pdf:application/pdf},
}


@article{akhtar_threat_2018,
	title = {Threat of {adversarial} {attacks} on {deep} {learning} in {computer} {vision}: {A} {survey}},
	volume = {6},
	issn = {2169-3536},
	shorttitle = {Threat of {Adversarial} {Attacks} on {Deep} {Learning} in {Computer} {Vision}},
	abstract = {Deep learning is at the heart of the current rise of artiﬁcial intelligence. In the ﬁeld of computer vision, it has become the workhorse for applications ranging from self-driving cars to surveillance and security. Whereas, deep neural networks have demonstrated phenomenal success (often beyond human capabilities) in solving complex problems, recent studies show that they are vulnerable to adversarial attacks in the form of subtle perturbations to inputs that lead a model to predict incorrect outputs. For images, such perturbations are often too small to be perceptible, yet they completely fool the deep learning models. Adversarial attacks pose a serious threat to the success of deep learning in practice. This fact has recently led to a large inﬂux of contributions in this direction. This paper presents the ﬁrst comprehensive survey on adversarial attacks on deep learning in computer vision. We review the works that design adversarial attacks, analyze the existence of such attacks and propose defenses against them. To emphasize that adversarial attacks are possible in practical conditions, we separately review the contributions that evaluate adversarial attacks in the real-world scenarios. Finally, drawing on the reviewed literature, we provide a broader outlook of this research direction.},
	language = {en},
	urldate = {2022-06-11},
	journal = {IEEE Access},
	author = {Akhtar, Naveed and Mian, Ajmal},
	year = {2018},
	number = {17677602},
	pages = {14410--14430},
	file = {Akhtar 和 Mian - 2018 - Threat of Adversarial Attacks on Deep Learning in .pdf:D\:\\a1172\\Zotero\\storage\\2KWP7SEF\\Akhtar 和 Mian - 2018 - Threat of Adversarial Attacks on Deep Learning in .pdf:application/pdf},
}



@article{szegedy_intriguing_2014,
	title = {Intriguing {properties} of {neural} {networks}},
	abstract = {Deep neural networks are highly expressive models that have recently achieved state of the art performance on speech and visual recognition tasks. While their expressiveness is the reason they succeed, it also causes them to learn uninterpretable solutions that could have counter-intuitive properties. In this paper we report two such properties.},
	language = {en},
	urldate = {2022-06-11},
	journal = {arXiv preprint arXiv:1312.6199},
	author = {Szegedy, Christian and Zaremba, Wojciech and Sutskever, Ilya and Bruna, Joan and Erhan, Dumitru and Goodfellow, Ian and Fergus, Rob},
	year = {2014},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	file = {Szegedy 等。 - 2014 - Intriguing properties of neural networks.pdf:D\:\\a1172\\Zotero\\storage\\SMK3XHYC\\Szegedy 等。 - 2014 - Intriguing properties of neural networks.pdf:application/pdf},
}


@inproceedings{moosavi-dezfooli_universal_2017,
	address = {Hawaii, USA},
	title = {Universal {adversarial} {perturbations}},
	isbn = {978-1-5386-0457-1},
	language = {en},
	urldate = {2022-06-11},
	booktitle = {{IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Moosavi-Dezfooli, Seyed-Mohsen and Fawzi, Alhussein and Fawzi, Omar and Frossard, Pascal},
	year = {2017},
	file = {Moosavi-Dezfooli 等。 - 2017 - Universal Adversarial Perturbations.pdf:D\:\\a1172\\Zotero\\storage\\8IY84KZI\\Moosavi-Dezfooli 等。 - 2017 - Universal Adversarial Perturbations.pdf:application/pdf},
}


@article{finlayson_adversarial_2019,
	title = {Adversarial {attacks} {against} {medical} {deep} {learning} {systems}},
	abstract = {The discovery of adversarial examples has raised concerns about the practical deployment of deep learning systems. In this paper, we demonstrate that adversarial examples are capable of manipulating deep learning systems across three clinical domains. For each of our representative medical deep learning classifiers, both white and black box attacks were highly successful. Our models are representative of the current state of the art in medical computer vision and, in some cases, directly reflect architectures already seeing deployment in real world clinical settings. In addition to the technical contribution of our paper, we synthesize a large body of knowledge about the healthcare system to argue that medicine may be uniquely susceptible to adversarial attacks, both in terms of monetary incentives and technical vulnerability. To this end, we outline the healthcare economy and the incentives it creates for fraud and provide concrete examples of how and why such attacks could be realistically carried out. We urge practitioners to be aware of current vulnerabilities when deploying deep learning systems in clinical settings, and encourage the machine learning community to further investigate the domain-specific characteristics of medical learning systems.},
	language = {en},
	urldate = {2022-06-11},
	journal = {arXiv preprint arXiv:1804.05296},
	author = {Finlayson, Samuel G. and Chung, Hyung Won and Kohane, Isaac S. and Beam, Andrew L.},
	year = {2019},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Cryptography and Security, Computer Science - Computers and Society},
	file = {Finlayson 等。 - 2019 - Adversarial Attacks Against Medical Deep Learning .pdf:D\:\\a1172\\Zotero\\storage\\228AMJPF\\Finlayson 等。 - 2019 - Adversarial Attacks Against Medical Deep Learning .pdf:application/pdf},
}


@article{ma_understanding_2021,
	title = {Understanding adversarial attacks on deep learning based medical image analysis systems},
	volume = {110},
	number = {107332},
	issn = {00313203},
	language = {en},
	urldate = {2022-06-11},
	journal = {Pattern Recognition},
	author = {Ma, Xingjun and Niu, Yuhao and Gu, Lin and Wang, Yisen and Zhao, Yitian and Bailey, James and Lu, Feng},
	year = {2021},
	pages = {1--11},
}



@article{khan_review_2018,
	title = {A review on the application of deep learning in system health management},
	volume = {107},
	issn = {08883270},
	abstract = {Given the advancements in modern technological capabilities, having an integrated health management and diagnostic strategy becomes an important part of a system’s operational life-cycle. This is because it can be used to detect anomalies, analyse failures and predict the future state based on up-to-date information. By utilising condition data and on-site feedback, data models can be trained using machine learning and statistical concepts. Once trained, the logic for data processing can be embedded on on-board controllers whilst enabling real-time health assessment and analysis. However, this integration inevitably faces several difﬁculties and challenges for the community; indicating the need for novel approaches to address this vexing issue. Deep learning has gained increasing attention due to its potential advantages with data classiﬁcation and feature extraction problems. It is an evolving research area with diverse application domains and hence its use for system health management applications must been researched if it can be used to increase overall system resilience or potential cost beneﬁts for maintenance, repair, and overhaul activities. This article presents a systematic review of artiﬁcial intelligence based system health management with an emphasis on recent trends of deep learning within the ﬁeld. Various architectures and related theories are discussed to clarify its potential. Based on the reviewed work, deep learning demonstrates plausible beneﬁts for fault diagnosis and prognostics. However, there are a number of limitations that hinder its widespread adoption and require further development. Attention is paid to overcoming these challenges, with future opportunities being enumerated.},
	language = {en},
	urldate = {2022-06-10},
	journal = {Mechanical Systems and Signal Processing},
	author = {Khan, Samir and Yairi, Takehisa},
	year = {2018},
	pages = {241--265},
	file = {A review on the application of deep learning.pdf:D\:\\a1172\\Zotero\\storage\\ZGVRN2RH\\A review on the application of deep learning.pdf:application/pdf},
}

@article{schmidhuber_deep_2015,
	title = {Deep learning in neural networks: {An} overview},
	volume = {61},
	issn = {0893-6080},
	abstract = {In recent years, deep artificial neural networks (including recurrent ones) have won numerous contests in pattern recognition and machine learning. This historical survey compactly summarizes relevant work, much of it from the previous millennium. Shallow and Deep Learners are distinguished by the depth of their credit assignment paths, which are chains of possibly learnable, causal links between actions and effects. I review deep supervised learning (also recapitulating the history of backpropagation), unsupervised learning, reinforcement learning \& evolutionary computation, and indirect search for short programs encoding deep and large networks.},
	journal = {Neural Networks},
	author = {Schmidhuber, Jürgen},
	year = {2015},
	keywords = {Deep learning, Evolutionary computation, Reinforcement learning, Supervised learning, Unsupervised learning},
	pages = {85--117},
}


@article{cao_deep_2018,
	title = {Deep {learning} and {its} {applications} in {biomedicine}},
	volume = {16},
	issn = {16720229},
	language = {en},
	number = {1},
	urldate = {2022-07-02},
	journal = {Genomics, Proteomics \& Bioinformatics},
	author = {Cao, Chensi and Liu, Feng and Tan, Hai and Song, Deshou and Shu, Wenjie and Li, Weizhong and Zhou, Yiming and Bo, Xiaochen and Xie, Zhi},
	year = {2018},
	pages = {17--32},
	file = {【2018】Deep Learning and Its Applications in Biomedicine.pdf:D\:\\a1172\\Zotero\\storage\\QMGZCL9N\\【2018】Deep Learning and Its Applications in Biomedicine.pdf:application/pdf},
}



@article{sze_efficient_2017,
	author={Sze, Vivienne and Chen, Yu-Hsin and Yang, Tien-Ju and Emer, Joel S.},
	journal={Proceedings of the IEEE}, 
	title={Efficient Processing of Deep Neural Networks: A Tutorial and Survey}, 
	year={2017},
	volume={105},
	number={12},
	pages={2295-2329}}

@article{nguyen_machine_2019,
	title = {Machine {learning} and {deep} {learning} frameworks and libraries for large-scale data mining: A survey},
	volume = {52},
	issn = {0269-2821, 1573-7462},
	shorttitle = {Machine {Learning} and {Deep} {Learning} frameworks and libraries for large-scale data mining},
	abstract = {The combined impact of new computing resources and techniques with an increasing avalanche of large datasets, is transforming many research areas and may lead to technological breakthroughs that can be used by billions of people. In the recent years, Machine Learning and especially its subﬁeld Deep Learning have seen impressive advances. Techniques developed within these two ﬁelds are now able to analyze and learn from huge amounts of real world examples in a disparate formats. While the number of Machine Learning algorithms is extensive and growing, their implementations through frameworks and libraries is also extensive and growing too. The software development in this ﬁeld is fast paced with a large number of open-source software coming from the academy, industry, start-ups or wider open-source communities. This survey presents a recent time-slide comprehensive overview with comparisons as well as trends in development and usage of cutting-edge Artiﬁcial Intelligence software. It also provides an overview of massive parallelism support that is capable of scaling computation effectively and efﬁciently in the era of Big Data.},
	language = {en},
	number = {},
	urldate = {2022-06-10},
	journal = {Artificial Intelligence Review},
	author = {Nguyen, Giang and Dlugolinsky, Stefan and Bobák, Martin and Tran, Viet and López García, Álvaro and Heredia, Ignacio and Malík, Peter and Hluchý, Ladislav},
	year = {2019},
	pages = {77--124},
	file = {Nguyen 等。 - 2019 - Machine Learning and Deep Learning frameworks and .pdf:D\:\\a1172\\Zotero\\storage\\7Y5J7R74\\Nguyen 等。 - 2019 - Machine Learning and Deep Learning frameworks and .pdf:application/pdf},
}

@inproceedings{davidson_gene_2015,
	title = {Gene {regulatory} {networks}},
	isbn = {978-0-12-404729-7},
	language = {en},
	urldate = {2022-02-26},
	booktitle = {Genomic {Control} {Process}},
	publisher = {Elsevier},
	author = {Davidson, Eric H. and Peter, Isabelle S.},
	year = {2015},
	pages = {41--77},
	file = {Davidson 和 Peter - 2015 - Gene Regulatory Networks.pdf:F\:\\Zotero\\storage\\W8337MS5\\Davidson 和 Peter - 2015 - Gene Regulatory Networks.pdf:application/pdf},
}



@article{wason_deep_2018,
	title = {Deep learning: {Evolution} and expansion},
	volume = {52},
	issn = {13890417},
	shorttitle = {Deep learning},
	abstract = {This paper historically attempts to map the signiﬁcant success of deep neural networks in notably varied classiﬁcation problems and application domains with near human-level performance. The paper also addresses the various doubts surrounding the acceptance of deep learning as a science of future. The manuscript attempts to unveil the hidden capabilities of deep neural networks in enabling machines perform the human way tasks which can be learned through what we call observation and experience.},
	language = {en},
	urldate = {2022-07-02},
	journal = {Cognitive Systems Research},
	author = {Wason, Ritika},
	year = {2018},
	pages = {701--708},
	file = {Wason - 2018 - Deep learning Evolution and expansion.pdf:D\:\\Zotero\\storage\\8S3ZMHCQ\\Wason - 2018 - Deep learning Evolution and expansion.pdf:application/pdf},
}





@article{zhang_machine_2017,
	title = {From machine learning to deep learning: Progress in machine intelligence for rational drug discovery},
	volume = {22},
	issn = {13596446},
	shorttitle = {From machine learning to deep learning},
	language = {en},
	number = {11},
	urldate = {2022-06-10},
	journal = {Drug Discovery Today},
	author = {Zhang, Lu and Tan, Jianjun and Han, Dan and Zhu, Hao},
	year = {2017},
	pages = {1680--1685},
	file = {Zhang 等。 - 2017 - From machine learning to deep learning progress i.pdf:D\:\\a1172\\Zotero\\storage\\B9KZQ8KX\\Zhang 等。 - 2017 - From machine learning to deep learning progress i.pdf:application/pdf},
}

@article{chauhan_comparison_2022,
	title = {Comparison of machine learning and deep learning for view identification from cardiac magnetic resonance images},
	volume = {82},
	issn = {08997071},
	abstract = {Background: Artificial intelligence is increasingly utilized to aid in the interpretation of cardiac magnetic reso­ nance (CMR) studies. One of the first steps is the identification of the imaging plane depicted, which can be achieved by both deep learning (DL) and classical machine learning (ML) techniques without user input. We aimed to compare the accuracy of ML and DL for CMR view classification and to identify potential pitfalls during training and testing of the algorithms.
	Methods: To train our DL and ML algorithms, we first established datasets by retrospectively selecting 200 CMR cases. The models were trained using two different cohorts (passively and actively curated) and applied data augmentation to enhance training. Once trained, the models were validated on an external dataset, consisting of 20 cases acquired at another center. We then compared accuracy metrics and applied class activation mapping (CAM) to visualize DL model performance.
	Results: The DL and ML models trained with the passively-curated CMR cohort were 99.1\% and 99.3\% accurate on the validation set, respectively. However, when tested on the CMR cases with complex anatomy, both models performed poorly. After training and testing our models again on all 200 cases (active cohort), validation on the external dataset resulted in 95\% and 90\% accuracy, respectively. The CAM analysis depicted heat maps that demonstrated the importance of carefully curating the datasets to be used for training.
	Conclusions: Both DL and ML models can accurately classify CMR images, but DL outperformed ML when clas­ sifying images with complex heart anatomy.},
	language = {en},
	urldate = {2022-06-10},
	journal = {Clinical Imaging},
	author = {Chauhan, Daksh and Anyanwu, Emeka and Goes, Jacob and Besser, Stephanie A. and Anand, Simran and Madduri, Ravi and Getty, Neil and Kelle, Sebastian and Kawaji, Keigo and Mor-Avi, Victor and Patel, Amit R.},
	year = {2022},
	pages = {121--126},
	file = {Chauhan 等。 - 2022 - Comparison of machine learning and deep learning f.pdf:D\:\\a1172\\Zotero\\storage\\K4AQGRMW\\Chauhan 等。 - 2022 - Comparison of machine learning and deep learning f.pdf:application/pdf},
}


@article{marbach_wisdom_2012,
	title = {Wisdom of crowds for robust gene network inference},
	volume = {9},
	issn = {1548-7091, 1548-7105},
	abstract = {Reconstructing gene regulatory networks from high-throughput data is a long-standing problem. Through the DREAM project (Dialogue on Reverse Engineering Assessment and Methods), we performed a comprehensive blind assessment of over thirty network inference methods on Escherichia coli, Staphylococcus aureus, Saccharomyces cerevisiae, and in silico microarray data. We characterize performance, data requirements, and inherent biases of different inference approaches offering guidelines for both algorithm application and development. We observe that no single inference method performs optimally across all datasets. In contrast, integration of predictions from multiple inference methods shows robust and high performance across diverse datasets. Thereby, we construct high-confidence networks for E. coli and S. aureus, each comprising {\textasciitilde}1700 transcriptional interactions at an estimated precision of 50\%. We experimentally test 53 novel interactions in E. coli, of which 23 were supported (43\%). Our results establish community-based methods as a powerful and robust tool for the inference of transcriptional gene regulatory networks.},
	language = {en},
	number = {8},
	urldate = {2022-02-26},
	journal = {Nature Methods},
	author = {Marbach, Daniel and Costello, James C and Küffner, Robert and Vega, Nicole M and Prill, Robert J and Camacho, Diogo M and Allison, Kyle R and Kellis, Manolis and Collins, James J and Stolovitzky, Gustavo},
	year = {2012},
	pages = {796--804},
	file = {The DREAM5 Consortium 等。 - 2012 - Wisdom of crowds for robust gene network inference.pdf:F\:\\Zotero\\storage\\SAYSU8RA\\The DREAM5 Consortium 等。 - 2012 - Wisdom of crowds for robust gene network inference.pdf:application/pdf},
}


@inproceedings{velickovic_graph_2018,
	address = {Vancouver, Canada},
	title = {{Graph} {attention} {networks}},
	abstract = {We present graph attention networks (GATs), novel neural network architectures that operate on graph-structured data, leveraging masked self-attentional layers to address the shortcomings of prior methods based on graph convolutions or their approximations. By stacking layers in which nodes are able to attend over their neighborhoods’ features, we enable (implicitly) specifying different weights to different nodes in a neighborhood, without requiring any kind of computationally intensive matrix operation (such as inversion) or depending on knowing the graph structure upfront. In this way, we address several key challenges of spectral-based graph neural networks simultaneously, and make our model readily applicable to inductive as well as transductive problems. Our GAT models have achieved or matched state-of-the-art results across four established transductive and inductive graph benchmarks: the Cora, Citeseer and Pubmed citation network datasets, as well as a protein-protein interaction dataset (wherein test graphs remain unseen during training).},
	language = {en},
	booktitle = {Proceedings of Sixth International Conference on Learning Representations (ICLR)},
	author={Petar Velickovic and Guillem Cucurull and Arantxa Casanova and Adriana Romero and Pietro Lio’ and Yoshua Bengio},
	year = {2018},
	file = {graph_attention_networks.pdf:D\:\\a1172\\Zotero\\storage\\YBN82WPG\\graph_attention_networks.pdf:application/pdf},
}

@article{bacciu_gentle_2020,
	title = {A gentle introduction to deep learning for graphs},
	volume = {129},
	issn = {0893-6080},
	abstract = {The adaptive processing of graph data is a long-standing research topic that has been lately consolidated as a theme of major interest in the deep learning community. The snap increase in the amount and breadth of related research has come at the price of little systematization of knowledge and attention to earlier literature. This work is a tutorial introduction to the field of deep learning for graphs. It favors a consistent and progressive presentation of the main concepts and architectural aspects over an exposition of the most recent literature, for which the reader is referred to available surveys. The paper takes a top-down view of the problem, introducing a generalized formulation of graph representation learning based on a local and iterative approach to structured information processing. Moreover, it introduces the basic building blocks that can be combined to design novel and effective neural models for graphs. We complement the methodological exposition with a discussion of interesting research challenges and applications in the field.},
	journal = {Neural Networks},
	author = {Bacciu, Davide and Errica, Federico and Micheli, Alessio and Podda, Marco},
	year = {2020},
	keywords = {Deep learning for graphs, Graph neural networks, Learning for structured data},
	pages = {203--221},
}


@article{li_fast_2020,
	title = {Fast {haar} {transforms} for {graph} {neural} {networks}},
	volume = {128},
	issn = {0893-6080},
	abstract = {Graph Neural Networks (GNNs) have become a topic of intense research recently due to their powerful capability in high-dimensional classification and regression tasks for graph-structured data. However, as GNNs typically define the graph convolution by the orthonormal basis for the graph Laplacian, they suffer from high computational cost when the graph size is large. This paper introduces a Haar basis, which is a sparse and localized orthonormal system for a coarse-grained chain on the graph. The graph convolution under Haar basis, called Haar convolution, can be defined accordingly for GNNs. The sparsity and locality of the Haar basis allow Fast Haar Transforms (FHTs) on the graph, by which one then achieves a fast evaluation of Haar convolution between graph data and filters. We conduct experiments on GNNs equipped with Haar convolution, which demonstrates state-of-the-art results on graph-based regression and node classification tasks.},
	journal = {Neural Networks},
	author = {Li, Ming and Ma, Zheng and Wang, Yu Guang and Zhuang, Xiaosheng},
	year = {2020},
	keywords = {Fast Haar Transforms, Geometric deep learning, Graph convolution, Graph Laplacian, Graph Neural Networks, Haar basis},
	pages = {188--198},
}

@article{spinelli_adaptive_2021,
	author={Spinelli, Indro and Scardapane, Simone and Uncini, Aurelio},
	journal={IEEE Transactions on Neural Networks and Learning Systems}, 
	title={Adaptive Propagation Graph Convolutional Network}, 
	year={2021},
	volume={32},
	number={10},
	pages={4755-4760}}

@article{wu_comprehensive_2021,
	author={Wu, Zonghan and Pan, Shirui and Chen, Fengwen and Long, Guodong and Zhang, Chengqi and Yu, Philip S.},
	journal={IEEE Transactions on Neural Networks and Learning Systems}, 
	title={A Comprehensive Survey on Graph Neural Networks}, 
	year={2021},
	volume={32},
	number={1},
	pages={4-24}}


@article{webb_deep_2018,
	title = {Deep learning for biology},
	volume = {554},
	issn = {0028-0836, 1476-4687},
	language = {en},
	number = {7693},
	urldate = {2022-05-24},
	journal = {Nature},
	author = {Webb, Sarah},
	year = {2018},
	pages = {555--557},
	file = {DEEP LEARNING FOR BIOLOGY.pdf:D\:\\a1172\\Zotero\\storage\\26JTB7CD\\DEEP LEARNING FOR BIOLOGY.pdf:application/pdf},
}

@article{diogo_next_2018,
	title = {Next-Generation Machine Learning for Biological Networks},
	journal = {Cell},
	volume = {173},
	number = {7},
	pages = {1581-1592},
	year = {2018},
	issn = {0092-8674},
	author = {Diogo M. Camacho and Katherine M. Collins and Rani K. Powers and James C. Costello and James J. Collins},
	keywords = {Machine leaning, deep learning, systems biology, synthetic biology, network biology, neural networks},
	abstract = {Machine learning, a collection of data-analytical techniques aimed at building predictive models from multi-dimensional datasets, is becoming integral to modern biological research. By enabling one to generate models that learn from large datasets and make predictions on likely outcomes, machine learning can be used to study complex cellular systems such as biological networks. Here, we provide a primer on machine learning for life scientists, including an introduction to deep learning. We discuss opportunities and challenges at the intersection of machine learning and network biology, which could impact disease biology, drug discovery, microbiome research, and synthetic biology.}
}

@article{dai_adversarial_2018,
	title = {Adversarial {attack} on {graph} {structured} {data}},
	abstract = {Deep learning on graph structures has shown exciting results in various applications. However, few attentions have been paid to the robustness of such models, in contrast to numerous research work for image or text adversarial attack and defense. In this paper, we focus on the adversarial attacks that fool the model by modifying the combinatorial structure of data. We ﬁrst propose a reinforcement learning based attack method that learns the generalizable attack policy, while only requiring prediction labels from the target classiﬁer. Also, variants of genetic algorithms and gradient methods are presented in the scenario where prediction conﬁdence or gradients are available. We use both synthetic and real-world data to show that, a family of Graph Neural Network models are vulnerable to these attacks, in both graph-level and node-level classiﬁcation tasks. We also show such attacks can be used to diagnose the learned classiﬁers.},
	language = {en},
	urldate = {2022-03-15},
	journal = {arXiv preprint arXiv:1806.02371},
	author = {Dai, Hanjun and Li, Hui and Tian, Tian and Huang, Xin and Wang, Lin and Zhu, Jun and Song, Le},
	year = {2018},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Social and Information Networks, Computer Science - Cryptography and Security, 1},
	annote = {Comment: to appear in ICML 2018},
	file = {1806.02371.pdf:D\:\\a1172\\Zotero\\storage\\8IGKF7YL\\1806.02371.pdf:application/pdf},
}

@misc{bojchevski_adversarial_2019,
	title = {Adversarial {Attacks} on {Node} {Embeddings} via {Graph} {Poisoning}},
	abstract = {The goal of network representation learning is to learn low-dimensional node embeddings that capture the graph structure and are useful for solving downstream tasks. However, despite the proliferation of such methods, there is currently no study of their robustness to adversarial attacks. We provide the ﬁrst adversarial vulnerability analysis on the widely used family of methods based on random walks. We derive efﬁcient adversarial perturbations that poison the network structure and have a negative effect on both the quality of the embeddings and the downstream tasks. We further show that our attacks are transferable since they generalize to many models and are successful even when the attacker is restricted.},
	language = {en},
	urldate = {2022-05-19},
	publisher = {arXiv},
	author = {Bojchevski, Aleksandar and Günnemann, Stephan},
	year = {2019},
	note = {arXiv:1809.01093 [cs, stat]},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Machine Learning, Computer Science - Social and Information Networks, Statistics - Machine Learning},
	annote = {Comment: ICML 2019, PMLR 97:695-704},
	file = {Bojchevski 和 Günnemann - 2019 - Adversarial Attacks on Node Embeddings via Graph P.pdf:D\:\\a1172\\Zotero\\storage\\3HVCRS8K\\Bojchevski 和 Günnemann - 2019 - Adversarial Attacks on Node Embeddings via Graph P.pdf:application/pdf},
}

@article{zugner_adversarial_2018,
	title = {Adversarial {Attacks} on {Neural} {Networks} for {Graph} {Data}},
	abstract = {Deep learning models for graphs have achieved strong performance for the task of node classification. Despite their proliferation, currently there is no study of their robustness to adversarial attacks. Yet, in domains where they are likely to be used, e.g. the web, adversaries are common. Can deep learning models for graphs be easily fooled? In this work, we introduce the first study of adversarial attacks on attributed graphs, specifically focusing on models exploiting ideas of graph convolutions. In addition to attacks at test time, we tackle the more challenging class of poisoning/causative attacks, which focus on the training phase of a machine learning model. We generate adversarial perturbations targetcilansNsgifoicdtaehtioen node’s features and the graph structure, thus, taking the dependencies between instances in account. Moreover, we ensure that the perturbations remain unnoticeable by preserving important data characteristics. To cope with the underlying discrete domain we propose an efficient algorithm Nettack exploiting incremental computations. Our experimental study shows that accuracy of node classification significantly drops even when performing only few perturbations. Even more, our attacks are transferable: the learned attacks generalize to other state-of-the-art node classification models and unsupervised approaches, and likewise are successful even when only limited knowledge about the graph is given.},
	language = {en},
	urldate = {2022-03-14},
	journal = {Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery \& Data Mining},
	author = {Zügner, Daniel and Akbarnejad, Amir and Günnemann, Stephan},
	year = {2018},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Cryptography and Security, 1},
	pages = {2847--2856},
	annote = {Comment: Accepted as a full paper at KDD 2018 on May 6, 2018},
	file = {Zügner 等。 - 2018 - Adversarial Attacks on Neural Networks for Graph D.pdf:D\:\\Zotero\\storage\\29JZ8VVP\\Zügner 等。 - 2018 - Adversarial Attacks on Neural Networks for Graph D.pdf:application/pdf},
}

@inproceedings{papernot_practical_2017,
	author = {Papernot, Nicolas and McDaniel, Patrick and Goodfellow, Ian and Jha, Somesh and Celik, Z. Berkay and Swami, Ananthram},
	title = {Practical Black-Box Attacks against Machine Learning},
	year = {2017},
	isbn = {9781450349444},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	abstract = {Machine learning (ML) models, e.g., deep neural networks (DNNs), are vulnerable to adversarial examples: malicious inputs modified to yield erroneous model outputs, while appearing unmodified to human observers. Potential attacks include having malicious content like malware identified as legitimate or controlling vehicle behavior. Yet, all existing adversarial example attacks require knowledge of either the model internals or its training data. We introduce the first practical demonstration of an attacker controlling a remotely hosted DNN with no such knowledge. Indeed, the only capability of our black-box adversary is to observe labels given by the DNN to chosen inputs. Our attack strategy consists in training a local model to substitute for the target DNN, using inputs synthetically generated by an adversary and labeled by the target DNN. We use the local substitute to craft adversarial examples, and find that they are misclassified by the targeted DNN. To perform a real-world and properly-blinded evaluation, we attack a DNN hosted by MetaMind, an online deep learning API. We find that their DNN misclassifies 84.24% of the adversarial examples crafted with our substitute. We demonstrate the general applicability of our strategy to many ML techniques by conducting the same attack against models hosted by Amazon and Google, using logistic regression substitutes. They yield adversarial examples misclassified by Amazon and Google at rates of 96.19% and 88.94%. We also find that this black-box attack strategy is capable of evading defense strategies previously found to make adversarial example crafting harder.},
	booktitle = {Proceedings of the 2017 ACM on Asia Conference on Computer and Communications Security},
	pages = {506–519},
	numpages = {14},
	keywords = {adversarial machine learning, machine learning, black-box attack},
	location = {Abu Dhabi, United Arab Emirates},
	series = {ASIA CCS '17}
}


@article{mordelet_sirene_2008,
	title = {{SIRENE}: Supervised inference of regulatory networks},
	volume = {24},
	issn = {1367-4803, 1460-2059},
	shorttitle = {{SIRENE}},
	abstract = {Motivation: Living cells are the product of gene expression programs that involve the regulated transcription of thousands of genes. The elucidation of transcriptional regulatory networks is thus needed to understand the cell’s working mechanism, and can for example, be useful for the discovery of novel therapeutic targets. Although several methods have been proposed to infer gene regulatory networks from gene expression data, a recent comparison on a large-scale benchmark experiment revealed that most current methods only predict a limited number of known regulations at a reasonable precision level.},
	language = {en},
	number = {16},
	urldate = {2022-02-26},
	journal = {Bioinformatics},
	author = {Mordelet, F. and Vert, J.-P.},
	year = {2008},
	pages = {76--82},
	file = {Mordelet 和 Vert - 2008 - SIRENE supervised inference of regulatory network.pdf:D\:\\a1172\\Zotero\\storage\\B5RTDJKH\\Mordelet 和 Vert - 2008 - SIRENE supervised inference of regulatory network.pdf:application/pdf},
}

@article{Salleh_multiple_2017,
	title = {Multiple {linear} {regression} for {reconstruction} of {gene} {regulatory} {networks} in {solving} {cascade} {error} {problems}},
	volume = {2017},
	number = {4827171},
	issn = {1687-8027},
	abstract = {Gene regulatory network (GRN) reconstruction is the process of identifying regulatory gene interactions from experimental data through computational analysis. One of the main reasons for the reduced performance of previous GRN methods had been inaccurate prediction of cascade motifs. Cascade error is defined as the wrong prediction of cascade motifs, where an indirect interaction is misinterpreted as a direct interaction. Despite the active research on various GRN prediction methods, the discussion on specific methods to solve problems related to cascade errors is still lacking. In fact, the experiments conducted by the past studies were not specifically geared towards proving the ability of GRN prediction methods in avoiding the occurrences of cascade errors. Hence, this research aims to propose Multiple Linear Regression (MLR) to infer GRN from gene expression data and to avoid wrongly inferring of an indirect interaction (A \&\#x2192; B \&\#x2192; C) as a direct interaction (A \&\#x2192; C). Since the number of observations of the real experiment datasets was far less than the number of predictors, some predictors were eliminated by extracting the random subnetworks from global interaction networks via an established extraction method. In addition, the experiment was extended to assess the effectiveness of MLR in dealing with cascade error by using a novel experimental procedure that had been proposed in this work. The experiment revealed that the number of cascade errors had been very minimal. Apart from that, the Belsley collinearity test proved that multicollinearity did affect the datasets used in this experiment greatly. All the tested subnetworks obtained satisfactory results, with AUROC values above 0.5.},
	journal = {Advances in Bioinformatics},
	author = {Salleh, Faridah Hani Mohamed and Zainudin, Suhaila and Arif, Shereena M.},
	editor = {Jung, Klaus},
	year = {2017},
	pages = {1--14},
}


@article{mochida_statistical_2018,
	title = {Statistical and {machine} {learning} {approaches} to {predict} {gene} {regulatory} {networks} {from} {transcriptome} {datasets}},
	volume = {9},
	number = {1770},
	issn = {1664-462X},
	abstract = {Statistical and machine learning (ML)-based methods have recently advanced in construction of gene regulatory network (GRNs) based on high-throughput biological datasets. GRNs underlie almost all cellular phenomena; hence, comprehensive GRN maps are essential tools to elucidate gene function, thereby facilitating the identiﬁcation and prioritization of candidate genes for functional analysis. High-throughput gene expression datasets have yielded various statistical and ML-based algorithms to infer causal relationship between genes and decipher GRNs. This review summarizes the recent advancements in the computational inference of GRNs, based on large-scale transcriptome sequencing datasets of model plants and crops. We highlight strategies to select contextual genes for GRN inference, and statistical and ML-based methods for inferring GRNs based on transcriptome datasets from plants. Furthermore, we discuss the challenges and opportunities for the elucidation of GRNs based on largescale datasets obtained from emerging transcriptomic applications, such as from population-scale, single-cell level, and life-course transcriptome analyses.},
	language = {en},
	urldate = {2022-06-18},
	journal = {Frontiers in Plant Science},
	author = {Mochida, Keiichi and Koda, Satoru and Inoue, Komaki and Nishii, Ryuei},
	year = {2018},
	pages = {1--7},
	file = {Mochida 等。 - 2018 - Statistical and Machine Learning Approaches to Pre.pdf:D\:\\a1172\\Zotero\\storage\\MQ862X4V\\Mochida 等。 - 2018 - Statistical and Machine Learning Approaches to Pre.pdf:application/pdf},
}


@article{vijesh_modeling_2013,
	title = {Modeling of gene regulatory networks: {A} review},
	volume = {06},
	issn = {1937-6871, 1937-688X},
	shorttitle = {Modeling of gene regulatory networks},
	abstract = {Gene regulatory networks play an important role the molecular mechanism underlying biological processes. Modeling of these networks is an important challenge to be addressed in the post genomic era. Several methods have been proposed for estimating gene networks from gene expression data. Computational methods for development of network models and analysis of their functionality have proved to be valuable tools in bioinformatics applications. In this paper we tried to review the different methods for reconstructing gene regulatory networks.},
	language = {en},
	number = {02},
	urldate = {2022-05-16},
	journal = {Journal of Biomedical Science and Engineering},
	author = {Vijesh, Nedumparambathmarath and Chakrabarti, Swarup Kumar and Sreekumar, Janardanan},
	year = {2013},
	pages = {223--231},
	file = {Vijesh 等。 - 2013 - Modeling of gene regulatory networks A review.pdf:D\:\\Zotero\\storage\\2W2U8HJH\\Vijesh 等。 - 2013 - Modeling of gene regulatory networks A review.pdf:application/pdf},
}

@article{maaten_visualizing_2008,
	title = {Visualizing {data} using t-{SNE}},
	volume = {9},
	number = {86},
	journal = {Journal of Machine Learning Research},
	author = {Maaten, Laurens van der and Hinton, Geoffrey},
	year = {2008},
	pages = {2579--2605},
}



@inproceedings{hamilton_inductive_nodate,
	address = {Long Beach, CA, USA},
	title = {Inductive Representation Learning on Large Graphs},
	author = {Hamilton, William L. and Ying, Rex and Leskovec, Jure},
	year = {2017},
	isbn = {9781510860964},
	abstract = {Low-dimensional embeddings of nodes in large graphs have proved extremely useful in a variety of prediction tasks, from content recommendation to identifying protein functions. However, most existing approaches require that all nodes in the graph are present during training of the embeddings; these previous approaches are inherently transductive and do not naturally generalize to unseen nodes. Here we present GraphSAGE, a general inductive framework that leverages node feature information (e.g., text attributes) to efficiently generate node embeddings for previously unseen data. Instead of training individual embeddings for each node, we learn a function that generates embeddings by sampling and aggregating features from a node's local neighborhood. Our algorithm outperforms strong baselines on three inductive node-classification benchmarks: we classify the category of unseen nodes in evolving information graphs based on citation and Reddit post data, and we show that our algorithm generalizes to completely unseen graphs using a multi-graph dataset of protein-protein interactions.},
	booktitle = {Proceedings of Thirty-first Conference on Neural Information Processing Systems (NIPS)},
}

@article{WantingChen2014,
  title={Impact of microarray data perturbation on false discovery rate method for screening differentially expressed genes},
  author={Chen,Wanting and Liu,Chengyou and Wu,Jing and Ding,Yong},
  journal={Journal of Nanjing Medical University: Natural Science Edition},
  number={7},
  year={2014},
  pages={991--995},
  
}

@article{ChengyouLiu2012,
    title={Comparison of two parameter estimation methods for the relative error linear regression model},
    author={Liu,Chengyou and Ding, Yong},
    journal={Chinese health statistics},
    volume={29},
    number={6},
    year={2012},
    pages={3},
}

@article{faith_large-scale_2007,
	title = {Large-{Scale} {Mapping} and {Validation} of {Escherichia} coli {Transcriptional} {Regulation} from a {Compendium} of {Expression} {Profiles}},
	volume = {5},
	issn = {1545-7885},
	language = {en},
	number = {1},
	urldate = {2022-02-26},
	journal = {PLoS Biology},
	author = {Faith, Jeremiah J and Hayete, Boris and Thaden, Joshua T and Mogno, Ilaria and Wierzbowski, Jamey and Cottarel, Guillaume and Kasif, Simon and Collins, James J and Gardner, Timothy S},
	editor = {Levchenko, Andre},
	year = {2007},
	pages = {e8},
	file = {Faith 等。 - 2007 - Large-Scale Mapping and Validation of Escherichia .pdf:F\:\\Zotero\\storage\\9VPSU6V8\\Faith 等。 - 2007 - Large-Scale Mapping and Validation of Escherichia .pdf:application/pdf},
}

@article{yamanishi_protein_2004,
	title = {Protein network inference from multiple genomic data: a supervised approach},
	volume = {20},
	issn = {1367-4803, 1460-2059},
	shorttitle = {Protein network inference from multiple genomic data},
	abstract = {Motivation: An increasing number of observations support the hypothesis that most biological functions involve the interactions between many proteins, and that the complexity of living systems arises as a result of such interactions. In this context, the problem of inferring a global protein network for a given organism, using all available genomic data about the organism, is quickly becoming one of the main challenges in current computational biology.},
	language = {en},
	number = {Suppl 1},
	urldate = {2022-02-26},
	journal = {Bioinformatics},
	author = {Yamanishi, Y. and Vert, J.-P. and Kanehisa, M.},
	year = {2004},
	pages = {i363--i370},
	file = {Yamanishi 等。 - 2004 - Protein network inference from multiple genomic da.pdf:F\:\\Zotero\\storage\\WB8JBRM8\\Yamanishi 等。 - 2004 - Protein network inference from multiple genomic da.pdf:application/pdf},
}

@article{maetschke_supervised_2014,
	title = {Supervised, semi-supervised and unsupervised inference of gene regulatory networks},
	volume = {15},
	issn = {1467-5463, 1477-4054},
	abstract = {Inference of gene regulatory network from expression data is a challenging task. Many methods have been developed to this purpose but a comprehensive evaluation that covers unsupervised, semi-supervised and supervised methods, and provides guidelines for their practical application, is lacking.},
	language = {en},
	number = {2},
	urldate = {2022-02-26},
	journal = {Briefings in Bioinformatics},
	author = {Maetschke, S. R. and Madhamshettiwar, P. B. and Davis, M. J. and Ragan, M. A.},
	year = {2014},
	pages = {195--211},
	file = {Maetschke 等。 - 2014 - Supervised, semi-supervised and unsupervised infer.pdf:F\:\\Zotero\\storage\\UPR3ZNGC\\Maetschke 等。 - 2014 - Supervised, semi-supervised and unsupervised infer.pdf:application/pdf},
}

@article{bleakley_supervised_2007,
	title = {Supervised reconstruction of biological networks with local models},
	volume = {23},
	issn = {1460-2059, 1367-4803},
	abstract = {Motivation: Inference and reconstruction of biological networks from heterogeneous data is currently an active research subject with several important applications in systems biology. The problem has been attacked from many different points of view with varying degrees of success. In particular, predicting new edges with a reasonable false discovery rate is highly demanded for practical applications, but remains extremely challenging due to the sparsity of the networks of interest.},
	language = {en},
	number = {13},
	urldate = {2022-02-26},
	journal = {Bioinformatics},
	author = {Bleakley, Kevin and Biau, Gérard and Vert, Jean-Philippe},
	year = {2007},
	pages = {i57--i65},
	file = {Bleakley 等。 - 2007 - Supervised reconstruction of biological networks w.pdf:F\:\\Zotero\\storage\\P586DRP3\\Bleakley 等。 - 2007 - Supervised reconstruction of biological networks w.pdf:application/pdf},
}

@article{razaghi-moghadam_supervised_2020,
	title = {Supervised learning of gene-regulatory networks based on graph distance profiles of transcriptomics data},
	volume = {6},
	issn = {2056-7189},
	abstract = {Abstract
            
              Characterisation of gene-regulatory network (GRN) interactions provides a stepping stone to understanding how genes affect cellular phenotypes. Yet, despite advances in profiling technologies, GRN reconstruction from gene expression data remains a pressing problem in systems biology. Here, we devise a supervised learning approach, GRADIS, which utilises support vector machine to reconstruct GRNs based on distance profiles obtained from a graph representation of transcriptomics data. By employing the data from
              Escherichia coli
              and
              Saccharomyces cerevisiae
              as well as synthetic networks from the DREAM4 and five network inference challenges, we demonstrate that our GRADIS approach outperforms the state-of-the-art supervised and unsupervided approaches. This holds when predictions about target genes for individual transcription factors as well as for the entire network are considered. We employ experimentally verified GRNs from
              E. coli
              and
              S. cerevisiae
              to validate the predictions and obtain further insights in the performance of the proposed approach. Our GRADIS approach offers the possibility for usage of other network-based representations of large-scale data, and can be readily extended to help the characterisation of other cellular networks, including protein–protein and protein–metabolite interactions.},
	language = {en},
	number = {1},
	urldate = {2022-02-26},
	journal = {npj Systems Biology and Applications},
	author = {Razaghi-Moghadam, Zahra and Nikoloski, Zoran},
	year = {2020},
	pages = {21},
	file = {Razaghi-Moghadam 和 Nikoloski - 2020 - Supervised learning of gene-regulatory networks ba.pdf:F\:\\Zotero\\storage\\A5ZT7ZKE\\Razaghi-Moghadam 和 Nikoloski - 2020 - Supervised learning of gene-regulatory networks ba.pdf:application/pdf},
}



@article{gillani_comparesvm_2014,
	title = {{CompareSVM}: supervised, {Support} {Vector} {Machine} ({SVM}) inference of gene regularity networks},
	volume = {15},
	issn = {1471-2105},
	shorttitle = {{CompareSVM}},
	abstract = {Background: Predication of gene regularity network (GRN) from expression data is a challenging task. There are many methods that have been developed to address this challenge ranging from supervised to unsupervised methods. Most promising methods are based on support vector machine (SVM). There is a need for comprehensive analysis on prediction accuracy of supervised method SVM using different kernels on different biological experimental conditions and network size.
Results: We developed a tool (CompareSVM) based on SVM to compare different kernel methods for inference of GRN. Using CompareSVM, we investigated and evaluated different SVM kernel methods on simulated datasets of microarray of different sizes in detail. The results obtained from CompareSVM showed that accuracy of inference method depends upon the nature of experimental condition and size of the network.
Conclusions: For network with nodes ({\textless}200) and average (over all sizes of networks), SVM Gaussian kernel outperform on knockout, knockdown, and multifactorial datasets compared to all the other inference methods. For network with large number of nodes ({\textasciitilde}500), choice of inference method depend upon nature of experimental condition. CompareSVM is available at http://bis.zju.edu.cn/CompareSVM/.},
	language = {en},
	number = {1},
	urldate = {2022-02-26},
	journal = {BMC Bioinformatics},
	author = {Gillani, Zeeshan and Akash, Muhammad Sajid Hamid and Rahaman, MD Matiur and Chen, Ming},
	year = {2014},
	pages = {395},
	file = {Gillani 等。 - 2014 - CompareSVM supervised, Support Vector Machine (SV.pdf:F\:\\Zotero\\storage\\E935BRQ9\\Gillani 等。 - 2014 - CompareSVM supervised, Support Vector Machine (SV.pdf:application/pdf},
}

@article{wolf_motifs_2003,
	title = {Motifs, modules and games in bacteria},
	volume = {6},
	issn = {13695274},
	language = {en},
	number = {2},
	urldate = {2022-02-26},
	journal = {Current Opinion in Microbiology},
	author = {Wolf, Denise M and Arkin, Adam P},
	year = {2003},
	pages = {125--134},
	file = {Wolf 和 Arkin - 2003 - Motifs, modules and games in bacteria.pdf:F\:\\Zotero\\storage\\5E2R342Q\\Wolf 和 Arkin - 2003 - Motifs, modules and games in bacteria.pdf:application/pdf},
}

@article{shen-orr_network_2002,
	title = {Network motifs in the transcriptional regulation network of {Escherichia} coli},
	volume = {31},
	issn = {1061-4036, 1546-1718},
	language = {en},
	number = {1},
	urldate = {2022-02-26},
	journal = {Nature Genetics},
	author = {Shen-Orr, Shai S. and Milo, Ron and Mangan, Shmoolik and Alon, Uri},
	year = {2002},
	pages = {64--68},
	file = {Shen-Orr 等。 - 2002 - Network motifs in the transcriptional regulation n.pdf:F\:\\Zotero\\storage\\Z6QHPQXF\\Shen-Orr 等。 - 2002 - Network motifs in the transcriptional regulation n.pdf:application/pdf},
}


@article{zhou_graph_2020,
	title = {Graph neural networks: {A} review of methods and applications},
	volume = {1},
	issn = {26666510},
	shorttitle = {Graph neural networks},
	abstract = {Lots of learning tasks require dealing with graph data which contains rich relation information among elements. Modeling physics systems, learning molecular ﬁngerprints, predicting protein interface, and classifying diseases demand a model to learn from graph inputs. In other domains such as learning from non-structural data like texts and images, reasoning on extracted structures (like the dependency trees of sentences and the scene graphs of images) is an important research topic which also needs graph reasoning models. Graph neural networks (GNNs) are neural models that capture the dependence of graphs via message passing between the nodes of graphs. In recent years, variants of GNNs such as graph convolutional network (GCN), graph attention network (GAT), graph recurrent network (GRN) have demonstrated ground-breaking performances on many deep learning tasks. In this survey, we propose a general design pipeline for GNN models and discuss the variants of each component, systematically categorize the applications, and propose four open problems for future research.},
	language = {en},
	urldate = {2022-02-26},
	journal = {AI Open},
	author = {Zhou, Jie and Cui, Ganqu and Hu, Shengding and Zhang, Zhengyan and Yang, Cheng and Liu, Zhiyuan and Wang, Lifeng and Li, Changcheng and Sun, Maosong},
	year = {2020},
	pages = {57--81},
	file = {Zhou 等。 - 2020 - Graph neural networks A review of methods and app.pdf:F\:\\Zotero\\storage\\BWYPK7QW\\Zhou 等。 - 2020 - Graph neural networks A review of methods and app.pdf:application/pdf},
}

@article{gupta_graph_2021,
	title = {Graph neural network: {Current} state of {Art}, challenges and applications},
	volume = {46},
	issn = {22147853},
	shorttitle = {Graph neural network},
	abstract = {Several areas in science and engineering have the relationships between their underlying data which can be represented as graphs, for example, molecular chemistry, node prediction, link prediction, computer vision, pattern recognition, social networking and more. In this article, an approach to a model which can handle such type of data is elaborated, which is Graph Neural Networks (GNN). GNN encompasses the neural network technique to process the data which is represented as graphs. Due to its massive success, GNN has made its way into many applications and is a popular architecture to work upon. This paper explains the graph neural networks, its area of applications and its day-to-day use in our daily lives. Some of the very common application is a social networking site which is on our hands regularly, and another could be the recommendation system which recommends us friends, or the products of our interest based on our pat choices and preferences. This paper also demonstrates the basic challenges encountered while implementing GNN. This paper will be a great help to those researchers who are keen to work in the domain of GNN.},
	language = {en},
	urldate = {2022-02-26},
	journal = {Materials Today: Proceedings},
	author = {Gupta, Atika and Matta, Priya and Pant, Bhasker},
	year = {2021},
	pages = {10927--10932},
	file = {Gupta 等。 - 2021 - Graph neural network Current state of Art, challe.pdf:F\:\\Zotero\\storage\\GNHGRU2U\\Gupta 等。 - 2021 - Graph neural network Current state of Art, challe.pdf:application/pdf},
}

@inproceedings{zhang_weisfeiler-lehman_2017,
	address = {Halifax NS Canada},
	title = {Weisfeiler-{Lehman} {Neural} {Machine} for {Link} {Prediction}},
	isbn = {978-1-4503-4887-4},
	abstract = {In this paper, we propose a next-generation link prediction method, Weisfeiler-Lehman Neural Machine (Wlnm), which learns topological features in the form of graph patterns that promote the formation of links. Wlnm has unmatched advantages including higher performance than state-of-the-art methods and universal applicability over various kinds of networks. Wlnm extracts an enclosing subgraph of each target link and encodes the subgraph as an adjacency matrix. The key novelty of the encoding comes from a fast hashing-based Weisfeiler-Lehman (WL) algorithm that labels the vertices according to their structural roles in the subgraph while preserving the subgraph’s intrinsic directionality. After that, a neural network is trained on these adjacency matrices to learn a predictive model. Compared with traditional link prediction methods, Wlnm does not assume a particular link formation mechanism (such as common neighbors), but learns this mechanism from the graph itself. We conduct comprehensive experiments to show that Wlnm not only outperforms a great number of state-of-the-art link prediction methods, but also consistently performs well across networks with different characteristics.},
	language = {en},
	urldate = {2022-02-26},
	booktitle = {Proceedings of the 23rd {ACM} {SIGKDD} {International} {Conference} on {Knowledge} {Discovery} and {Data} {Mining}},
	publisher = {ACM},
	author = {Zhang, Muhan and Chen, Yixin},
	year = {2017},
	pages = {575--583},
	file = {Zhang 和 Chen - 2017 - Weisfeiler-Lehman Neural Machine for Link Predicti.pdf:F\:\\Zotero\\storage\\GD38RCJ9\\Zhang 和 Chen - 2017 - Weisfeiler-Lehman Neural Machine for Link Predicti.pdf:application/pdf},
}

@inproceedings{zhang_link_2018,
	title = {Link {Prediction} {Based} on {Graph} {Neural} {Networks}},
	volume = {31},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Zhang, Muhan and Chen, Yixin},
	editor = {Bengio, S. and Wallach, H. and Larochelle, H. and Grauman, K. and Cesa-Bianchi, N. and Garnett, R.},
	year = {2018},
}

@inproceedings{grover_node2vec_2016,
	address = {San Francisco, USA},
	title = {Node2vec: {Scalable} {feature} {learning} for {networks}},
	isbn = {978-1-4503-4232-2},
	shorttitle = {node2vec},
	abstract = {Prediction tasks over nodes and edges in networks require careful effort in engineering features used by learning algorithms. Recent research in the broader ﬁeld of representation learning has led to signiﬁcant progress in automating prediction by learning the features themselves. However, present feature learning approaches are not expressive enough to capture the diversity of connectivity patterns observed in networks.},
	language = {en},
	urldate = {2022-02-26},
	booktitle = {Proceedings of the 22nd {ACM} {SIGKDD} {International} {Conference} on {Knowledge} {Discovery} and {Data} {Mining}},
	author = {Grover, Aditya and Leskovec, Jure},
	year = {2016},
	file = {Grover 和 Leskovec - 2016 - node2vec Scalable Feature Learning for Networks.pdf:F\:\\Zotero\\storage\\R2V8Z9IA\\Grover 和 Leskovec - 2016 - node2vec Scalable Feature Learning for Networks.pdf:application/pdf},
}


@inproceedings{zhang_end--end_nodate,
	address = {New Orleans, Louisiana, USA},
	title = {An {end}-to-{end} {deep} {learning} {architecture} for {graph} {classification}},
	abstract = {Neural networks are typically designed to deal with data in tensor forms. In this paper, we propose a novel neural network architecture accepting graphs of arbitrary structure. Given a dataset containing graphs in the form of (G, y) where G is a graph and y is its class, we aim to develop neural networks that read the graphs directly and learn a classiﬁcation function. There are two main challenges: 1) how to extract useful features characterizing the rich information encoded in a graph for classiﬁcation purpose, and 2) how to sequentially read a graph in a meaningful and consistent order. To address the ﬁrst challenge, we design a localized graph convolution model and show its connection with two graph kernels. To address the second challenge, we design a novel SortPooling layer which sorts graph vertices in a consistent order so that traditional neural networks can be trained on the graphs. Experiments on benchmark graph classiﬁcation datasets demonstrate that the proposed architecture achieves highly competitive performance with state-of-the-art graph kernels and other graph neural network methods. Moreover, the architecture allows end-to-end gradient-based training with original graphs, without the need to ﬁrst transform graphs into vectors.},
	language = {en},
	booktitle = {Proceedings of Thirty-Second AAAI Conference on Artificial Intelligence (AAAI)},
	author = {Zhang, Muhan and Cui, Zhicheng and Neumann, Marion and Chen, Yixin},
	year = {2018},
	file = {Zhang 等。 - An End-to-End Deep Learning Architecture for Graph.pdf:D\:\\a1172\\Zotero\\storage\\ZQK33MAQ\\Zhang 等。 - An End-to-End Deep Learning Architecture for Graph.pdf:application/pdf},
}



@inproceedings{ying_hierarchical_nodate,
	address ={Montréal, Canada},
	title = {Hierarchical {graph} {representation} {learning} with {differentiable} {pooling}},
	abstract = {Recently, graph neural networks (GNNs) have revolutionized the ﬁeld of graph representation learning through effectively learned node embeddings, and achieved state-of-the-art results in tasks such as node classiﬁcation and link prediction. However, current GNN methods are inherently ﬂat and do not learn hierarchical representations of graphs—a limitation that is especially problematic for the task of graph classiﬁcation, where the goal is to predict the label associated with an entire graph. Here we propose DIFFPOOL, a differentiable graph pooling module that can generate hierarchical representations of graphs and can be combined with various graph neural network architectures in an end-to-end fashion. DIFFPOOL learns a differentiable soft cluster assignment for nodes at each layer of a deep GNN, mapping nodes to a set of clusters, which then form the coarsened input for the next GNN layer. Our experimental results show that combining existing GNN methods with DIFFPOOL yields an average improvement of 5–10\% accuracy on graph classiﬁcation benchmarks, compared to all existing pooling approaches, achieving a new state-of-the-art on four out of ﬁve benchmark data sets.},
	language = {en},
	booktitle = {Proceedings of Thirty-second Conference on Neural Information Processing Systems (NIPS)},
	author = {Ying, Zhitao and You, Jiaxuan and Morris, Christopher and Ren, Xiang and Hamilton, Will and Leskovec, Jure},
	year = {2018},
	file = {Ying 等。 - Hierarchical Graph Representation Learning with Di.pdf:D\:\\a1172\\Zotero\\storage\\UUMMB4DN\\Ying 等。 - Hierarchical Graph Representation Learning with Di.pdf:application/pdf},
}


@article{baek_accurate_2021,
	title = {Accurate {learning} of {graph} {representations} with {graph} {multiset} {pooling}},
	abstract = {Graph neural networks have been widely used on modeling graph data, achieving impressive results on node classification and link prediction tasks. Yet, obtaining an accurate representation for a graph further requires a pooling function that maps a set of node representations into a compact form. A simple sum or average over all node representations considers all node features equally without consideration of their task relevance, and any structural dependencies among them. Recently proposed hierarchical graph pooling methods, on the other hand, may yield the same representation for two different graphs that are distinguished by the Weisfeiler-Lehman test, as they suboptimally preserve information from the node features. To tackle these limitations of existing graph pooling methods, we first formulate the graph pooling problem as a multiset encoding problem with auxiliary information about the graph structure, and propose a Graph Multiset Transformer (GMT) which is a multi-head attention based global pooling layer that captures the interaction between nodes according to their structural dependencies. We show that GMT satisfies both injectiveness and permutation invariance, such that it is at most as powerful as the Weisfeiler-Lehman graph isomorphism test. Moreover, our methods can be easily extended to the previous node clustering approaches for hierarchical graph pooling. Our experimental results show that GMT significantly outperforms state-of-the-art graph pooling methods on graph classification benchmarks with high memory and time efficiency, and obtains even larger performance gain on graph reconstruction and generation tasks.},
	language = {en},
	urldate = {2022-02-26},
	journal = {arXiv preprint arXiv:2102.11533},
	author = {Baek, Jinheon and Kang, Minki and Hwang, Sung Ju},
	year = {2021},
	keywords = {Computer Science - Machine Learning},
	annote = {Comment: ICLR 2021},
	file = {Baek 等。 - 2021 - Accurate Learning of Graph Representations with Gr.pdf:D\:\\a1172\\Zotero\\storage\\CZT9RSXC\\Baek 等。 - 2021 - Accurate Learning of Graph Representations with Gr.pdf:application/pdf},
}




@article{bianchi_graph_2022,
	author={Bianchi, Filippo Maria and Grattarola, Daniele and Livi, Lorenzo and Alippi, Cesare},
	journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
	title={Graph Neural Networks With Convolutional ARMA Filters}, 
	year={2022},
	volume={44},
	number={7},
	pages={3496-3507}}


@inproceedings{li_deepgcns_2019,
	address = {Seoul, Korea},
	title = {{DeepGCNs}: {Can} {GCNs} {go} {as} {deep} {as} {CNNs}?},
	isbn = {978-1-72814-803-8},
	shorttitle = {{DeepGCNs}},
	abstract = {Convolutional Neural Networks (CNNs) achieve impressive performance in a wide variety of ﬁelds. Their success beneﬁted from a massive boost when very deep CNN models were able to be reliably trained. Despite their merits, CNNs fail to properly address problems with non-Euclidean data. To overcome this challenge, Graph Convolutional Networks (GCNs) build graphs to represent non-Euclidean data, borrow concepts from CNNs, and apply them in training. GCNs show promising results, but they are usually limited to very shallow models due to the vanishing gradient problem (see Figure 1). As a result, most state-of-the-art GCN models are no deeper than 3 or 4 layers. In this work, we present new ways to successfully train very deep GCNs. We do this by borrowing concepts from CNNs, speciﬁcally residual/dense connections and dilated convolutions, and adapting them to GCN architectures. Extensive experiments show the positive effect of these deep GCN frameworks. Finally, we use these new concepts to build a very deep 56-layer GCN, and show how it signiﬁcantly boosts performance (+3.7\% mIoU over state-of-the-art) in the task of point cloud semantic segmentation. We believe that the community can greatly beneﬁt from this work, as it opens up many opportunities for advancing GCN-based research.},
	language = {en},
	urldate = {2022-02-26},
	booktitle = {{International} {Conference} on {Computer} {Vision} ({ICCV})},
	author = {Li, Guohao and Muller, Matthias and Thabet, Ali and Ghanem, Bernard},
	year = 2019,
	file = {Li 等。 - 2019 - DeepGCNs Can GCNs Go As Deep As CNNs.pdf:F\:\\Zotero\\storage\\U322Y46Y\\Li 等。 - 2019 - DeepGCNs Can GCNs Go As Deep As CNNs.pdf:application/pdf},
}

@article{lee_self-attention_2019,
	title = {Self-{attention} {graph} {pooling}},
	abstract = {Advanced methods of applying deep learning to structured data such as graphs have been proposed in recent years. In particular, studies have focused on generalizing convolutional neural networks to graph data, which includes redeﬁning the convolution and the downsampling (pooling) operations for graphs. The method of generalizing the convolution operation to graphs has been proven to improve performance and is widely used. However, the method of applying downsampling to graphs is still difﬁcult to perform and has room for improvement. In this paper, we propose a graph pooling method based on selfattention. Self-attention using graph convolution allows our pooling method to consider both node features and graph topology. To ensure a fair comparison, the same training procedures and model architectures were used for the existing pooling methods and our method. The experimental results demonstrate that our method achieves superior graph classiﬁcation performance on the benchmark datasets using a reasonable number of parameters.},
	language = {en},
	urldate = {2022-04-28},
	journal = {arXiv preprint arXiv:1904.08082},
	author = {Lee, Junhyun and Lee, Inyeop and Kang, Jaewoo},
	year = {2019},
	keywords = {Computer Science - Machine Learning, I.2.6, Statistics - Machine Learning},
	annote = {Comment: 10 pages, 3 figures, 4 tables. Accepted to ICML 2019},
	file = {Lee 等。 - 2019 - Self-Attention Graph Pooling.pdf:D\:\\a1172\\Zotero\\storage\\ZYUAW9DJ\\Lee 等。 - 2019 - Self-Attention Graph Pooling.pdf:application/pdf},
}



@article{omranian_gene_2016,
	title = {Gene regulatory network inference using fused {LASSO} on multiple data sets},
	volume = {6},
	issn = {2045-2322},
	language = {en},
	number = {1},
	urldate = {2022-02-26},
	journal = {Scientific Reports},
	author = {Omranian, Nooshin and Eloundou-Mbebi, Jeanne M. O. and Mueller-Roeber, Bernd and Nikoloski, Zoran},
	year = {2016},
	pages = {20533},
	file = {Omranian 等。 - 2016 - Gene regulatory network inference using fused LASS.pdf:D\:\\a1172\\Zotero\\storage\\V7VBLJTN\\Omranian 等。 - 2016 - Gene regulatory network inference using fused LASS.pdf:application/pdf},
}

@article{koda_diurnal_2017,
	title = {Diurnal {Transcriptome} and {Gene} {Network} {Represented} through {Sparse} {Modeling} in {Brachypodium} distachyon},
	volume = {8},
	issn = {1664-462X},
	abstract = {We report the comprehensive identiﬁcation of periodic genes and their network inference, based on a gene co-expression analysis and an Auto-Regressive eXogenous (ARX) model with a group smoothly clipped absolute deviation (SCAD) method using a time-series transcriptome dataset in a model grass, Brachypodium distachyon. To reveal the diurnal changes in the transcriptome in B. distachyon, we performed RNA-seq analysis of its leaves sampled through a diurnal cycle of over 48 h at 4 h intervals using three biological replications, and identiﬁed 3,621 periodic genes through our wavelet analysis. The expression data are feasible to infer network sparsity based on ARX models. We found that genes involved in biological processes such as transcriptional regulation, protein degradation, and post-transcriptional modiﬁcation and photosynthesis are signiﬁcantly enriched in the periodic genes, suggesting that these processes might be regulated by circadian rhythm in B. distachyon. On the basis of the time-series expression patterns of the periodic genes, we constructed a chronological gene co-expression network and identiﬁed putative transcription factors encoding genes that might be involved in the time-speciﬁc regulatory transcriptional network. Moreover, we inferred a transcriptional network composed of the periodic genes in B. distachyon, aiming to identify genes associated with other genes through variable selection by grouping time points for each gene. Based on the ARX model with the group SCAD regularization using our time-series expression datasets of the periodic genes, we constructed gene networks and found that the networks represent typical scale-free structure. Our ﬁndings demonstrate that the diurnal changes in the transcriptome in B. distachyon leaves have a sparse network structure, demonstrating the spatiotemporal gene regulatory network over the cyclic phase transitions in B. distachyon diurnal growth.},
	language = {en},
	urldate = {2022-02-26},
	journal = {Frontiers in Plant Science},
	author = {Koda, Satoru and Onda, Yoshihiko and Matsui, Hidetoshi and Takahagi, Kotaro and Uehara-Yamaguchi, Yukiko and Shimizu, Minami and Inoue, Komaki and Yoshida, Takuhiro and Sakurai, Tetsuya and Honda, Hiroshi and Eguchi, Shinto and Nishii, Ryuei and Mochida, Keiichi},
	year = {2017},
	pages = {2055},
	file = {Koda 等。 - 2017 - Diurnal Transcriptome and Gene Network Represented.pdf:F\:\\Zotero\\storage\\TCFPI8XW\\Koda 等。 - 2017 - Diurnal Transcriptome and Gene Network Represented.pdf:application/pdf},
}

@article{de_luis_balaguer_predicting_2017,
	title = {Predicting gene regulatory networks by combining spatial and temporal gene expression data in \textit{{Arabidopsis}} root stem cells},
	volume = {114},
	issn = {0027-8424, 1091-6490},
	abstract = {Identifying the transcription factors (TFs) and associated networks involved in stem cell regulation is essential for understanding the initiation and growth of plant tissues and organs. Although many TFs have been shown to have a role in the
              Arabidopsis
              root stem cells, a comprehensive view of the transcriptional signature of the stem cells is lacking. In this work, we used spatial and temporal transcriptomic data to predict interactions among the genes involved in stem cell regulation. To accomplish this, we transcriptionally profiled several stem cell populations and developed a gene regulatory network inference algorithm that combines clustering with dynamic Bayesian network inference. We leveraged the topology of our networks to infer potential major regulators. Specifically, through mathematical modeling and experimental validation, we identified
              PERIANTHIA
              (
              PAN
              ) as an important molecular regulator of quiescent center function. The results presented in this work show that our combination of molecular biology, computational biology, and mathematical modeling is an efficient approach to identify candidate factors that function in the stem cells.},
	language = {en},
	number = {36},
	urldate = {2022-02-26},
	journal = {Proceedings of the National Academy of Sciences},
	author = {de Luis Balaguer, Maria Angels and Fisher, Adam P. and Clark, Natalie M. and others},
	year = {2017},
	pages = {E7632--E7640},
	file = {de Luis Balaguer 等。 - 2017 - Predicting gene regulatory networks by combining s.pdf:F\:\\Zotero\\storage\\V2RF9ZBM\\de Luis Balaguer 等。 - 2017 - Predicting gene regulatory networks by combining s.pdf:application/pdf},
}

@article{guo_compartmentalized_2016,
	title = {Compartmentalized gene regulatory network of the pathogenic fungus \textit{{Fusarium} graminearum}},
	volume = {211},
	issn = {0028-646X, 1469-8137},
	abstract = {Head blight caused by Fusarium graminearum threatens world-wide wheat production, resulting in both yield loss and mycotoxin contamination.   We reconstructed the global F. graminearum gene regulatory network (GRN) from a large collection of transcriptomic data using Bayesian network inference, a machine-learning algorithm. This GRN reveals connectivity between key regulators and their target genes. Focusing on key regulators, this network contains eight distinct but interwoven modules. Enriched for unique functions, such as cell cycle, DNA replication, transcription, translation and stress responses, each module exhibits distinct expression proﬁles.},
	language = {en},
	number = {2},
	urldate = {2022-02-26},
	journal = {New Phytologist},
	author = {Guo, Li and Zhao, Guoyi and Xu, Jin‐Rong and Kistler, H. Corby and Gao, Lixin and Ma, Li‐Jun},
	year = {2016},
	pages = {527--541},
	file = {Guo 等。 - 2016 - Compartmentalized gene regulatory network of the p.pdf:F\:\\Zotero\\storage\\U3XSWXWQ\\Guo 等。 - 2016 - Compartmentalized gene regulatory network of the p.pdf:application/pdf},
}

@article{huynh-thu_inferring_2010,
	title = {Inferring {regulatory} {networks} from {expression} {data} {using} {tree}-{based} {methods}},
	volume = {5},
	issn = {1932-6203},
	abstract = {One of the pressing open problems of computational systems biology is the elucidation of the topology of genetic regulatory networks (GRNs) using high throughput genomic data, in particular microarray gene expression data. The Dialogue for Reverse Engineering Assessments and Methods (DREAM) challenge aims to evaluate the success of GRN inference algorithms on benchmarks of simulated data. In this article, we present GENIE3, a new algorithm for the inference of GRNs that was best performer in the DREAM4 In Silico Multifactorial challenge. GENIE3 decomposes the prediction of a regulatory network between p genes into p different regression problems. In each of the regression problems, the expression pattern of one of the genes (target gene) is predicted from the expression patterns of all the other genes (input genes), using tree-based ensemble methods Random Forests or Extra-Trees. The importance of an input gene in the prediction of the target gene expression pattern is taken as an indication of a putative regulatory link. Putative regulatory links are then aggregated over all genes to provide a ranking of interactions from which the whole network is reconstructed. In addition to performing well on the DREAM4 In Silico Multifactorial challenge simulated data, we show that GENIE3 compares favorably with existing algorithms to decipher the genetic regulatory network of Escherichia coli. It doesn’t make any assumption about the nature of gene regulation, can deal with combinatorial and non-linear interactions, produces directed GRNs, and is fast and scalable. In conclusion, we propose a new algorithm for GRN inference that performs well on both synthetic and real gene expression data. The algorithm, based on feature selection with tree-based ensemble methods, is simple and generic, making it adaptable to other types of genomic data and interactions.},
	language = {en},
	number = {9},
	urldate = {2022-02-26},
	journal = {PLoS ONE},
	author = {Huynh-Thu, Vân Anh and Irrthum, Alexandre and Wehenkel, Louis and Geurts, Pierre},
	editor = {Isalan, Mark},
	year = {2010},
	pages = {e12776},
	file = {Huynh-Thu 等。 - 2010 - Inferring Regulatory Networks from Expression Data.pdf:F\:\\Zotero\\storage\\JA9XV22H\\Huynh-Thu 等。 - 2010 - Inferring Regulatory Networks from Expression Data.pdf:application/pdf},
}

@article{varala_temporal_2018,
	title = {Temporal transcriptional logic of dynamic regulatory networks underlying nitrogen signaling and use in plants},
	volume = {115},
	issn = {0027-8424, 1091-6490},
	abstract = {This study exploits time, the relatively unexplored fourth dimension of gene regulatory networks (GRNs), to learn the temporal transcriptional logic underlying dynamic nitrogen (N) signaling in plants. Our “just-in-time” analysis of time-series transcriptome data uncovered a temporal cascade of
              cis
              elements underlying dynamic N signaling. To infer transcription factor (TF)-target edges in a GRN, we applied a time-based machine learning method to 2,174 dynamic N-responsive genes. We experimentally determined a network precision cutoff, using TF-regulated genome-wide targets of three TF hubs (CRF4, SNZ, and CDF1), used to “prune” the network to 155 TFs and 608 targets. This network precision was reconfirmed using genome-wide TF-target regulation data for four additional TFs (TGA1, HHO5/6, and PHL1) not used in network pruning. These higher-confidence edges in the GRN were further filtered by independent TF-target binding data, used to calculate a TF “N-specificity” index. This refined GRN identifies the temporal relationship of known/validated regulators of N signaling (NLP7/8, TGA1/4, NAC4, HRS1, and LBD37/38/39) and 146 additional regulators. Six TFs—CRF4, SNZ, CDF1, HHO5/6, and PHL1—validated herein regulate a significant number of genes in the dynamic N response, targeting 54\% of N-uptake/assimilation pathway genes. Phenotypically, inducible overexpression of CRF4
              in planta
              regulates genes resulting in altered biomass, root development, and
              15
              NO
              3
              −
              uptake, specifically under low-N conditions. This dynamic N-signaling GRN now provides the temporal “transcriptional logic” for 155 candidate TFs to improve nitrogen use efficiency with potential agricultural applications. Broadly, these time-based approaches can uncover the temporal transcriptional logic for any biological response system in biology, agriculture, or medicine.},
	language = {en},
	number = {25},
	urldate = {2022-02-26},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Varala, Kranthi and Marshall-Colón, Amy and Cirrone, Jacopo and Brooks, Matthew D. and Pasquino, Angelo V. and Léran, Sophie and Mittal, Shipra and Rock, Tara M. and Edwards, Molly B. and Kim, Grace J. and Ruffel, Sandrine and McCombie, W. Richard and Shasha, Dennis and Coruzzi, Gloria M.},
	year = {2018},
	pages = {6494--6499},
	file = {Varala 等。 - 2018 - Temporal transcriptional logic of dynamic regulato.pdf:F\:\\Zotero\\storage\\LGWTASVA\\Varala 等。 - 2018 - Temporal transcriptional logic of dynamic regulato.pdf:application/pdf},
}

@article{desai_improving_2017,
	title = {Improving {Gene} {Regulatory} {Network} {Inference} by {Incorporating} {Rates} of {Transcriptional} {Changes}},
	volume = {7},
	issn = {2045-2322},
	language = {en},
	number = {1},
	urldate = {2022-02-26},
	journal = {Scientific Reports},
	author = {Desai, Jigar S. and Sartor, Ryan C. and Lawas, Lovely Mae and Jagadish, S. V. Krishna and Doherty, Colleen J.},
	year = {2017},
	pages = {17244},
	file = {Desai 等。 - 2017 - Improving Gene Regulatory Network Inference by Inc.pdf:F\:\\Zotero\\storage\\GJDSJ7HG\\Desai 等。 - 2017 - Improving Gene Regulatory Network Inference by Inc.pdf:application/pdf},
}

@article{haury_tigress_2012,
	title = {{TIGRESS}: {Trustful} {inference} of {gene} {regulation} using {stability} {selection}},
	volume = {6},
	issn = {1752-0509},
	shorttitle = {{TIGRESS}},
	abstract = {Background: Inferring the structure of gene regulatory networks (GRN) from a collection of gene expression data has many potential applications, from the elucidation of complex biological processes to the identiﬁcation of potential drug targets. It is however a notoriously diﬃcult problem, for which the many existing methods reach limited accuracy.
Results: In this paper, we formulate GRN inference as a sparse regression problem and investigate the performance of a popular feature selection method, least angle regression (LARS) combined with stability selection, for that purpose. We introduce a novel, robust and accurate scoring technique for stability selection, which improves the performance of feature selection with LARS. The resulting method, which we call TIGRESS (for Trustful Inference of Gene REgulation with Stability Selection), was ranked among the top GRN inference methods in the DREAM5 gene network inference challenge. In particular, TIGRESS was evaluated to be the best linear regression-based method in the challenge. We investigate in depth the inﬂuence of the various parameters of the method, and show that a ﬁne parameter tuning can lead to signiﬁcant improvements and state-of-the-art performance for GRN inference, in both directed and undirected settings.
Conclusions: TIGRESS reaches state-of-the-art performance on benchmark data, including both in silico and in vivo (E. coli and S. cerevisiae) networks. This study conﬁrms the potential of feature selection techniques for GRN inference. Code and data are available on http://cbio.ensmp.fr/tigress. Moreover, TIGRESS can be run online through the GenePattern platform (GP-DREAM, http://dream.broadinstitute.org).},
	language = {en},
	number = {145},
	urldate = {2022-02-26},
	journal = {BMC Systems Biology},
	author = {Haury, Anne-Claire and Mordelet, Fantine and Vera-Licona, Paola and Vert, Jean-Philippe},
	year = {2012},
	pages = {1--17},
	file = {Haury 等。 - 2012 - TIGRESS Trustful Inference of Gene REgulation usi.pdf:F\:\\Zotero\\storage\\GU63FZKI\\Haury 等。 - 2012 - TIGRESS Trustful Inference of Gene REgulation usi.pdf:application/pdf},
}

@article{foo_framework_2018,
	title = {A {Framework} for {Engineering} {Stress} {Resilient} {Plants} {Using} {Genetic} {Feedback} {Control} and {Regulatory} {Network} {Rewiring}},
	volume = {7},
	issn = {2161-5063, 2161-5063},
	abstract = {Crop disease leads to signiﬁcant waste worldwide, both pre- and postharvest, with subsequent economic and sustainability consequences. Disease outcome is determined both by the plants’ response to the pathogen and by the ability of the pathogen to suppress defense responses and manipulate the plant to enhance colonization. The defense response of a plant is characterized by signiﬁcant transcriptional reprogramming mediated by underlying gene regulatory networks, and components of these networks are often targeted by attacking pathogens. Here, using gene expression data from Botrytis cinerea-infected Arabidopsis plants, we develop a systematic approach for mitigating the eﬀects of pathogen-induced network perturbations, using the tools of synthetic biology. We employ network inference and system identiﬁcation techniques to build an accurate model of an Arabidopsis defense subnetwork that contains key genes determining susceptibility of the plant to the pathogen attack. Once validated against time-series data, we use this model to design and test perturbation mitigation strategies based on the use of genetic feedback control. We show how a synthetic feedback controller can be designed to attenuate the eﬀect of external perturbations on the transcription factor CHE in our subnetwork. We investigate and compare two approaches for implementing such a controller biologicallydirect implementation of the genetic feedback controller, and rewiring the regulatory regions of multiple genesto achieve the network motif required to implement the controller. Our results highlight the potential of combining feedback control theory with synthetic biology for engineering plants with enhanced resilience to environmental stress.},
	language = {en},
	number = {6},
	urldate = {2022-02-26},
	journal = {ACS Synthetic Biology},
	author = {Foo, Mathias and Gherman, Iulia and Zhang, Peijun and Bates, Declan G. and Denby, Katherine J.},
	year = {2018},
	pages = {1553--1564},
	file = {Foo 等。 - 2018 - A Framework for Engineering Stress Resilient Plant.pdf:F\:\\Zotero\\storage\\H8IF652Q\\Foo 等。 - 2018 - A Framework for Engineering Stress Resilient Plant.pdf:application/pdf},
}

@article{banf_enhancing_2017,
	title = {Enhancing gene regulatory network inference through data integration with markov random fields},
	volume = {7},
	issn = {2045-2322},
	language = {en},
	number = {1},
	urldate = {2022-02-26},
	journal = {Scientific Reports},
	author = {Banf, Michael and Rhee, Seung Y.},
	year = {2017},
	pages = {41174},
	file = {Banf 和 Rhee - 2017 - Enhancing gene regulatory network inference throug.pdf:F\:\\Zotero\\storage\\M94BAJK7\\Banf 和 Rhee - 2017 - Enhancing gene regulatory network inference throug.pdf:application/pdf},
}

@article{margolin_aracne_2006,
	title = {{ARACNE}: {An} {algorithm} for the {reconstruction} of {gene} {regulatory} {networks} in a {mammalian} {cellular} {context}},
	volume = {7},
	issn = {1471-2105},
	shorttitle = {{ARACNE}},
	abstract = {Background: Elucidating gene regulatory networks is crucial for understanding normal cell physiology and complex pathologic phenotypes. Existing computational methods for the genomewide "reverse engineering" of such networks have been successful only for lower eukaryotes with simple genomes. Here we present ARACNE, a novel algorithm, using microarray expression profiles, specifically designed to scale up to the complexity of regulatory networks in mammalian cells, yet general enough to address a wider range of network deconvolution problems. This method uses an information theoretic approach to eliminate the majority of indirect interactions inferred by coexpression methods.
Results: We prove that ARACNE reconstructs the network exactly (asymptotically) if the effect of loops in the network topology is negligible, and we show that the algorithm works well in practice, even in the presence of numerous loops and complex topologies. We assess ARACNE's ability to reconstruct transcriptional regulatory networks using both a realistic synthetic dataset and a microarray dataset from human B cells. On synthetic datasets ARACNE achieves very low error rates and outperforms established methods, such as Relevance Networks and Bayesian Networks. Application to the deconvolution of genetic networks in human B cells demonstrates ARACNE's ability to infer validated transcriptional targets of the cMYC proto-oncogene. We also study the effects of misestimation of mutual information on network reconstruction, and show that algorithms based on mutual information ranking are more resilient to estimation errors.
Conclusion: ARACNE shows promise in identifying direct transcriptional interactions in mammalian cellular networks, a problem that has challenged existing reverse engineering algorithms. This approach should enhance our ability to use microarray data to elucidate functional mechanisms that underlie cellular processes and to identify molecular targets of pharmacological compounds in mammalian cellular networks.},
	language = {en},
	number = {S7},
	urldate = {2022-02-26},
	journal = {BMC Bioinformatics},
	author = {Margolin, Adam A and Nemenman, Ilya and Basso, Katia and Wiggins, Chris and Stolovitzky, Gustavo and Favera, Riccardo Dalla and Califano, Andrea},
	year = {2006},
	pages = {1--15}
}

@article{meyer_information-theoretic_2007,
	title = {Information-{Theoretic} {Inference} of {Large} {Transcriptional} {Regulatory} {Networks}},
	volume = {2007},
	issn = {1687-4145},
	language = {en},
	urldate = {2022-02-26},
	journal = {EURASIP Journal on Bioinformatics and Systems Biology},
	author = {Meyer, Patrick E. and Kontos, Kevin and Lafitte, Frederic and Bontempi, Gianluca},
	year = {2007},
	pages = {1--9},
	file = {Meyer 等。 - 2007 - Information-Theoretic Inference of Large Transcrip.pdf:F\:\\Zotero\\storage\\WXWJGDCI\\Meyer 等。 - 2007 - Information-Theoretic Inference of Large Transcrip.pdf:application/pdf},
}

@article{meltzer_pinet_2020,
	title = {{PiNet}: {Anttention} {pooling} for {graph} {classification}},
	shorttitle = {{PiNet}},
	abstract = {We propose PiNet, a generalised differentiable attention-based pooling mechanism for utilising graph convolution operations for graph level classiﬁcation. We demonstrate high sample efﬁciency and superior performance over other graph neural networks in distinguishing isomorphic graph classes, as well as competitive results with state of the art methods on standard chemo-informatics datasets.},
	language = {en},
	urldate = {2022-02-26},
	journal = {arXiv preprint arXiv:2008.04575},
	author = {Meltzer, Peter and Mallea, Marcelo Daniel Gutierrez and Bentley, Peter J.},
	year = {2020},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, I.5.1},
	annote = {Comment: 4 pages, 3 figures 1 table},
	file = {Meltzer 等。 - 2020 - PiNet Attention Pooling for Graph Classification.pdf:D\:\\a1172\\Zotero\\storage\\JMWF3S4D\\Meltzer 等。 - 2020 - PiNet Attention Pooling for Graph Classification.pdf:application/pdf},
}

@article{gao_graph_2021,
	title = {{Graph} {u}-{nets}},
	issn = {0162-8828, 2160-9292, 1939-3539},
	abstract = {We consider the problem of representation learning for graph data. Given images are special cases of graphs with nodes lie on 2D lattices, graph embedding tasks have a natural correspondence with image pixel-wise prediction tasks such as segmentation. While encoder-decoder architectures like U-Nets have been successfully applied to image pixel-wise prediction tasks, similar methods are lacking for graph data. This is because pooling and up-sampling operations are not natural on graph data. To address these challenges, we propose novel graph pooling and unpooling operations. The gPool layer adaptively selects some nodes to form a smaller graph based on their scalar projection values. We further propose the gUnpool layer as the inverse operation of the gPool layer. Based on our proposed methods, we develop an encoder-decoder model, known as the graph U-Nets. Experimental results on node classiﬁcation and graph classiﬁcation tasks demonstrate that our methods achieve consistently better performance than previous models. Along this direction, we extend our methods by integrating attention mechanisms. Based on attention operators, we proposed attention-based pooling and unpooling layers, which can better capture graph topology information. The empirical results on graph classiﬁcation tasks demonstrate the promising capability of our methods.},
	language = {en},
	urldate = {2022-02-26},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Gao, Hongyang and Ji, Shuiwang},
	year = {2021},
	number = {33999813},
	pages = {1--1},
	file = {Gao 和 Ji - 2021 - Graph U-Nets.pdf:D\:\\a1172\\Zotero\\storage\\P7WE8Z4X\\Gao 和 Ji - 2021 - Graph U-Nets.pdf:application/pdf},
}

@article{liao2022attention,
  title={Attention-embedded Quadratic Network (Qttention) for Effective and Interpretable Bearing Fault Diagnosis},
  author={Liao, Jing-Xiao and Dong, Hang-Cheng and Sun, Zhi-Qi and Sun, Jinwei and Zhang, Shiping and Fan, Feng-Lei},
  journal={arXiv preprint arXiv:2206.00390},
  year={2022}
}

@article{fan2023one,
  title={One Neuron Saved Is One Neuron Earned: On Parametric Efficiency of Quadratic Networks},
  author={Fan, Feng-Lei and Dong, Hang-Cheng and Wu, Zhongming and Ruan, Lecheng and Zeng, Tieyong and Cui, Yiming and Liao, Jing-Xiao},
  journal={arXiv preprint arXiv:2303.06316},
  year={2023}
}

@article{fan2023towards,
  title={Towards NeuroAI: Introducing Neuronal Diversity into Artificial Neural Networks},
  author={Fan, Feng-Lei and Li, Yingxin and Peng, Hanchuan and Zeng, Tieyong and Wang, Fei},
  journal={arXiv preprint arXiv:2301.09245},
  year={2023}
}

@article{Fan2018ANT,
  title={A new type of neurons for machine learning},
  author={Fenglei Fan and Wenxiang Cong and Ge Wang},
  journal={International Journal for Numerical Methods in Biomedical Engineering},
  year={2018},
  volume={34}
}

@article{2006Nonlinear,
  title={Nonlinear Devices Acting as SNR Amplifiers for a Harmonic Signal in Noise},
  author={ Chapeau-Blondeau, F.  and  Rousseau, D. },
  journal={Circuits, Systems, and Signal Processing},
  year={2006},
  number={3},
}

@article{2020Towards,
  title={Towards More Practical Adversarial Attacks on Graph Neural Networks},
  author={ Ma, J.  and  Ding, S.  and  Mei, Q. },
  year={2020},
}

@inproceedings{2020Adversarial,
  title={Adversarial Label-Flipping Attack and Defense for Graph Neural Networks},
  author={ Zhang, M.  and  Hu, L.  and  Shi, C.  and  Wang, X. },
  booktitle={2020 IEEE International Conference on Data Mining (ICDM)},
  year={2020},
}



@article{krizhevsky2012imagenet,
  title={Imagenet classification with deep convolutional neural networks},
  author={Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
  journal={Advances in neural information processing systems},
  volume={25},
  pages={1097--1105},
  year={2012}
}

@inproceedings{xu2022multi,
  title={Multi-Camera Collaborative Depth Prediction via Consistent Structure Estimation},
  author={Xu, Jialei and Liu, Xianming and Bai, Yuanchao and Jiang, Junjun and Wang, Kaixuan and Chen, Xiaozhi and Ji, Xiangyang},
  booktitle={Proceedings of the 30th ACM International Conference on Multimedia},
  pages={2730--2738},
  year={2022}
}


@article{song2021monocular,
  title={Monocular depth estimation using laplacian pyramid-based depth residuals},
  author={Song, Minsoo and Lim, Seokjae and Kim, Wonjun},
  journal={IEEE transactions on circuits and systems for video technology},
  volume={31},
  number={11},
  pages={4381--4393},
  year={2021},
  publisher={IEEE}
}

@article{kumar2017weight,
  title={On weight initialization in deep neural networks},
  author={Kumar, Siddharth Krishna},
  journal={arXiv preprint arXiv:1704.08863},
  year={2017}
}

@article{loshchilov2017decoupled,
  title={Decoupled weight decay regularization},
  author={Loshchilov, Ilya and Hutter, Frank},
  journal={arXiv preprint arXiv:1711.05101},
  year={2017}
}

@article{xu2022quadralib,
  title={QuadraLib: A Performant Quadratic Neural Network Library for Architecture Optimization and Design Exploration},
  author={Xu, Zirui and Yu, Fuxun and Xiong, Jinjun and Chen, Xiang},
  journal={Proceedings of Machine Learning and Systems},
  volume={4},
  pages={503--514},
  year={2022}
}

@inproceedings{he2015delving,
  title={Delving deep into rectifiers: Surpassing human-level performance on imagenet classification},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={1026--1034},
  year={2015}
}

@article{eigen2014depth,
  title={Depth map prediction from a single image using a multi-scale deep network},
  author={Eigen, David and Puhrsch, Christian and Fergus, Rob},
  journal={Advances in neural information processing systems},
  volume={27},
  year={2014}
}

@book{wang2013multivariate,
  title={Multivariate spline functions and their applications},
  author={Wang, Ren-Hong},
  volume={529},
  year={2013},
  publisher={Springer Science \& Business Media}
}

@article{zador2022toward,
  title={Toward next-generation artificial intelligence: Catalyzing the neuroai revolution},
  author={Zador, Anthony and Richards, Blake and {\"O}lveczky, Bence and Escola, Sean and Bengio, Yoshua and Boahen, Kwabena and Botvinick, Matthew and Chklovskii, Dmitri and Churchland, Anne and Clopath, Claudia and others},
  journal={arXiv preprint arXiv:2210.08340},
  year={2022}
}


@incollection{benesty2009pearson,
  title={Pearson correlation coefficient},
  author={Benesty, Jacob and Chen, Jingdong and Huang, Yiteng and Cohen, Israel},
  booktitle={Noise reduction in speech processing},
  pages={1--4},
  year={2009},
  publisher={Springer}
}

@inproceedings{redlapalli2003development,
  title={Development of quadratic neural unit with applications to pattern classification},
  author={Redlapalli, Sanjeevakumar and Gupta, Madan M and Song, K-Y},
  booktitle={Fourth International Symposium on Uncertainty Modeling and Analysis, 2003. ISUMA 2003.},
  pages={141--146},
  year={2003},
  organization={IEEE}
}

@inproceedings{goyal2020improved,
  title={Improved polynomial neural networks with normalised activations},
  author={Goyal, Mohit and Goyal, Rajan and Lall, Brejesh},
  booktitle={2020 International Joint Conference on Neural Networks (IJCNN)},
  pages={1--8},
  year={2020},
  organization={IEEE}
}

@article{jiang2020nonlinear,
  title={Nonlinear CNN: improving CNNs with quadratic convolutions},
  author={Jiang, Yiyang and Yang, Fan and Zhu, Hengliang and Zhou, Dian and Zeng, Xuan},
  journal={Neural Computing and Applications},
  volume={32},
  number={12},
  pages={8507--8516},
  year={2020},
  publisher={Springer}
}

@inproceedings{mantini2021cqnn,
  title={Cqnn: Convolutional quadratic neural networks},
  author={Mantini, Pranav and Shah, Shishr K},
  booktitle={2020 25th International Conference on Pattern Recognition (ICPR)},
  pages={9819--9826},
  year={2021},
  organization={IEEE}
}

@article{montufar2014number,
  title={On the number of linear regions of deep neural networks},
  author={Montufar, Guido F and Pascanu, Razvan and Cho, Kyunghyun and Bengio, Yoshua},
  journal={Advances in neural information processing systems},
  volume={27},
  year={2014}
}

@book{tao2012topics,
  title={Topics in random matrix theory},
  author={Tao, Terence},
  volume={132},
  year={2012},
  publisher={American Mathematical Soc.}
}

@inproceedings{li2019convergence,
  title={On the convergence of stochastic gradient descent with adaptive stepsizes},
  author={Li, Xiaoyu and Orabona, Francesco},
  booktitle={The 22nd International Conference on Artificial Intelligence and Statistics},
  pages={983--992},
  year={2019},
  organization={PMLR}
}

@article{lecun2015deep,
  title={Deep learning},
  author={LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
  journal={nature},
  volume={521},
  number={7553},
  pages={436--444},
  year={2015},
  publisher={Nature Publishing Group}
}

@article{xu2021robust,
  title={Robust Generalization of Quadratic Neural Networks via Function Identification},
  author={Xu, Kan and Bastani, Hamsa and Bastani, Osbert},
  journal={arXiv preprint arXiv:2109.10935},
  year={2021}
}

@article{mannelli2020optimization,
  title={Optimization and generalization of shallow neural networks with quadratic activation functions},
  author={Mannelli, Stefano Sarao and Vanden-Eijnden, Eric and Zdeborov{\'a}, Lenka},
  journal={arXiv preprint arXiv:2006.15459},
  year={2020}
}

@inproceedings{du2018power,
  title={On the power of over-parametrization in neural networks with quadratic activation},
  author={Du, Simon and Lee, Jason},
  booktitle={International Conference on Machine Learning},
  pages={1329--1338},
  year={2018},
  organization={PMLR}
}

@article{alber1998projected,
  title={On the projected subgradient method for nonsmooth convex optimization in a Hilbert space},
  author={Alber, Ya I and Iusem, Alfredo N and Solodov, Mikhail V},
  journal={Mathematical Programming},
  volume={81},
  number={1},
  pages={23--35},
  year={1998},
  publisher={Springer}
}

@article{tsapanos2018neurons,
  title={Neurons with paraboloid decision boundaries for improved neural network classification performance},
  author={Tsapanos, Nikolaos and Tefas, Anastasios and Nikolaidis, Nikolaos and Pitas, Ioannis},
  journal={IEEE transactions on neural networks and learning systems},
  volume={30},
  number={1},
  pages={284--294},
  year={2018},
  publisher={IEEE}
}

@article{simonyan2014very,
  title={Very deep convolutional networks for large-scale image recognition},
  author={Simonyan, Karen and Zisserman, Andrew},
  journal={arXiv preprint arXiv:1409.1556},
  year={2014}
}

@inproceedings{he2016deep,
  title={Deep residual learning for image recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={770--778},
  year={2016}
}

@article{fuchs2021super,
  title={Super-human performance in gran turismo sport using deep reinforcement learning},
  author={Fuchs, Florian and Song, Yunlong and Kaufmann, Elia and Scaramuzza, Davide and Duerr, Peter},
  journal={IEEE Robotics and Automation Letters},
  year={2021},
  publisher={IEEE}
}

@article{fan2019quadratic,
  title={Quadratic autoencoder (Q-AE) for low-dose CT denoising},
  author={Fan, Fenglei and Shan, Hongming and Kalra, Mannudeep K and Singh, Ramandeep and Qian, Guhan and Getzin, Matthew and Teng, Yueyang and Hahn, Juergen and Wang, Ge},
  journal={IEEE transactions on medical imaging},
  volume={39},
  number={6},
  pages={2035--2050},
  year={2019},
  publisher={IEEE}
}

@article{you2019ct,
  title={{CT} super-resolution {GAN} constrained by the identical, residual, and cycle learning ensemble (GAN-CIRCLE)},
  author={You, Chenyu and Li, Guang and Zhang, Yi and Zhang, Xiaoliu and Shan, Hongming and Li, Mengzhou and Ju, Shenghong and Zhao, Zhen and Zhang, Zhuiyang and Cong, Wenxiang and others},
  journal={IEEE Transactions on Medical Imaging},
  volume={39},
  number={1},
  pages={188--203},
  year={2019},
  publisher={IEEE}
}



@article{thivierge2008neural,
  title={Neural diversity creates a rich repertoire of brain activity},
  author={Thivierge, Jean-Philippe},
  journal={Communicative \& integrative biology},
  volume={1},
  number={2},
  pages={188--189},
  year={2008},
  publisher={Taylor \& Francis}
}

@article{elsken2019neural,
  title={Neural architecture search: A survey.},
  author={Elsken, Thomas and Metzen, Jan Hendrik and Hutter, Frank and others},
  journal={J. Mach. Learn. Res.},
  volume={20},
  number={55},
  pages={1--21},
  year={2019}
}

@article{fan2018new,
  title={A new type of neurons for machine learning},
  author={Fan, Fenglei and Cong, Wenxiang and Wang, Ge},
  journal={International journal for numerical methods in biomedical engineering},
  volume={34},
  number={2},
  pages={e2920},
  year={2018},
  publisher={Wiley Online Library}
}

@article{fan2018generalized,
  title={Generalized backpropagation algorithm for training second-order neural networks},
  author={Fan, Fenglei and Cong, Wenxiang and Wang, Ge},
  journal={International journal for numerical methods in biomedical engineering},
  volume={34},
  number={5},
  pages={e2956},
  year={2018},
  publisher={Wiley Online Library}
}


@InProceedings{air,
author = {Chen, Shi and Jiang, Ming and Yang, Jinhui and Zhao, Qi},
title = {AiR: Attention with Reasoning Capability},
booktitle = {ECCV},
year = {2020}
}



@article{schmidhuber2015deep,
  title={Deep learning in neural networks: An overview},
  author={Schmidhuber, J{\"u}rgen},
  journal={Neural networks},
  volume={61},
  pages={85--117},
  year={2015},
  publisher={Elsevier}
}

@article{ivakhnenko1971polynomial,
  title={Polynomial theory of complex systems},
  author={Ivakhnenko, Alexey Grigorevich},
  journal={IEEE transactions on Systems, Man, and Cybernetics},
  number={4},
  pages={364--378},
  year={1971},
  publisher={IEEE}
}

@article{poggio1975optimal,
  title={On optimal nonlinear associative recall},
  author={Poggio, Tomaso},
  journal={Biological Cybernetics},
  volume={19},
  number={4},
  pages={201--209},
  year={1975},
  publisher={Springer}
}

@article{lippmann1989pattern,
  title={Pattern classification using neural networks},
  author={Lippmann, Richard P},
  journal={IEEE communications magazine},
  volume={27},
  number={11},
  pages={47--50},
  year={1989},
  publisher={IEEE}
}

@inproceedings{shin1991pi,
  title={The pi-sigma network: An efficient higher-order neural network for pattern classification and function approximation},
  author={Shin, Yoan and Ghosh, Joydeep},
  booktitle={IJCNN-91-Seattle international joint conference on neural networks},
  volume={1},
  pages={13--18},
  year={1991},
  organization={IEEE}
}

@inproceedings{milenkovic1996annealing,
  title={Annealing based dynamic learning in second-order neural networks},
  author={Milenkovic, Srdjan and Obradovic, Zoran and Litovski, Vanco},
  booktitle={Proceedings of International Conference on Neural Networks (ICNN'96)},
  volume={1},
  pages={458--463},
  year={1996},
  organization={IEEE}
}

@inproceedings{cheung1991rotational,
  title={Rotational quadratic function neural networks},
  author={Cheung, KF and Leung, CS},
  booktitle={[Proceedings] 1991 IEEE International Joint Conference on Neural Networks},
  pages={869--874},
  year={1991},
  organization={IEEE}
}

@inproceedings{zoumpourlis2017non,
  title={Non-linear convolution filters for CNN-based learning},
  author={Zoumpourlis, Georgios and Doumanoglou, Alexandros and Vretos, Nicholas and Daras, Petros},
  booktitle={Proceedings of the IEEE International Conference on Computer Vision},
  pages={4761--4769},
  year={2017}
}

@article{mantinicqnn,
  title={CQNN: Convolutional Quadratic Neural Networks},
  author={Mantini, Pranav and Shah, Shishr K}
}

@article{chrysos2021deep,
  title={Deep Polynomial Neural Networks},
  author={Chrysos, Grigoris and Moschoglou, Stylianos and Bouritsas, Giorgos and Deng, Jiankang and Panagakis, Yannis and Zafeiriou, Stefanos P},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
  year={2021},
  publisher={IEEE}
}

@article{livni2014computational,
  title={On the computational efficiency of training neural networks},
  author={Livni, Roi and Shalev-Shwartz, Shai and Shamir, Ohad},
  journal={arXiv preprint arXiv:1410.1141},
  year={2014}
}

@article{krotov2018dense,
  title={Dense associative memory is robust to adversarial inputs},
  author={Krotov, Dmitry and Hopfield, John},
  journal={Neural computation},
  volume={30},
  number={12},
  pages={3151--3167},
  year={2018},
  publisher={MIT Press}
}

@inproceedings{jayakumar2019multiplicative,
  title={Multiplicative interactions and where to find them},
  author={Jayakumar, Siddhant M and Czarnecki, Wojciech M and Menick, Jacob and Schwarz, Jonathan and Rae, Jack and Osindero, Simon and Teh, Yee Whye and Harley, Tim and Pascanu, Razvan},
  booktitle={International Conference on Learning Representations},
  year={2019}
}

@incollection{remmert1991fundamental,
  title={The fundamental theorem of algebra},
  author={Remmert, Reinhold},
  booktitle={Numbers},
  pages={97--122},
  year={1991},
  publisher={Springer}
}

@article{kac1943average,
  title={On the average number of real roots of a random algebraic equation},
  author={Kac, Mark},
  journal={Bulletin of the American Mathematical Society},
  volume={49},
  number={4},
  pages={314--320},
  year={1943},
  publisher={American Mathematical Society}
}

@article{kirchberg1983fubini,
  title={The Fubini theorem for exact C*-algebras},
  author={Kirchberg, Eberhard},
  journal={Journal of Operator Theory},
  pages={3--8},
  year={1983},
  publisher={JSTOR}
}

@article{giles1987learning,
  title={Learning, invariance, and generalization in high-order neural networks},
  author={Giles, C Lee and Maxwell, Tom},
  journal={Applied optics},
  volume={26},
  number={23},
  pages={4972--4978},
  year={1987},
  publisher={Optical Society of America}
}

@incollection{tu2011manifolds,
  title={Manifolds},
  author={Tu, Loring W},
  booktitle={An Introduction to Manifolds},
  pages={47--83},
  year={2011},
  publisher={Springer}
}

@article{chen2019efficient,
  title={Efficient approximation of deep relu networks for functions on low dimensional manifolds},
  author={Chen, Minshuo and Jiang, Haoming and Liao, Wenjing and Zhao, Tuo},
  journal={Advances in neural information processing systems},
  volume={32},
  pages={8174--8184},
  year={2019}
}

@article{yarotsky2017error,
  title={Error bounds for approximations with deep ReLU networks},
  author={Yarotsky, Dmitry},
  journal={Neural Networks},
  volume={94},
  pages={103--114},
  year={2017},
  publisher={Elsevier}
}

@inproceedings{deepcomplex,
  title={Deep complex networks},
  author={Trabelsi, Chiheb and Bilaniuk, Olexa and Zhang, Ying and Serdyuk, Dmitriy and Subramanian, Sandeep and 
 Santos, João and Mehri, Soroush and Rostamzadeh, Negar and Bengio, Yoshua and Pa, Christopher J},
  booktitle={International Conference on Learning Representations},
  year={2018}
}

@article{misra2019mish,
  title={Mish: A self regularized non-monotonic activation function},
  author={Misra, Diganta},
  journal={arXiv preprint arXiv:1908.08681},
  year={2019}
}

@article{dettmers2019sparse,
  title={Sparse networks from scratch: Faster training without losing performance},
  author={Dettmers, Tim and Zettlemoyer, Luke},
  journal={arXiv preprint arXiv:1907.04840},
  year={2019}
}

@article{zhu2021gradinit,
  title={GradInit: Learning to Initialize Neural Networks for Stable and Efficient Training},
  author={Zhu, Chen and Ni, Renkun and Xu, Zheng and Kong, Kezhi and Huang, W Ronny and Goldstein, Tom},
  journal={arXiv preprint arXiv:2102.08098},
  year={2021}
}

@article{hall1976optimal,
  title={Optimal error bounds for cubic spline interpolation},
  author={Hall, Charles A and Meyer, W Weston},
  journal={Journal of Approximation Theory},
  volume={16},
  number={2},
  pages={105--122},
  year={1976},
  publisher={Elsevier}
}

@inproceedings{bu2021quadratic,
  title={Quadratic residual networks: A new class of neural networks for solving forward and inverse problems in physics involving pdes},
  author={Bu, Jie and Karpatne, Anuj},
  booktitle={Proceedings of the 2021 SIAM International Conference on Data Mining (SDM)},
  pages={675--683},
  year={2021},
  organization={SIAM}
}

@article{kileel2019expressive,
  title={On the expressive power of deep polynomial neural networks},
  author={Kileel, Joe and Trager, Matthew and Bruna, Joan},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  pages={10310--10319},
  year={2019}
}

@inproceedings{deng2009imagenet,
  title={Imagenet: A large-scale hierarchical image database},
  author={Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},
  booktitle={2009 IEEE conference on computer vision and pattern recognition},
  pages={248--255},
  year={2009},
  organization={Ieee}
}

@article{boyd1992defeating,
  title={Defeating the Runge phenomenon for equispaced polynomial interpolation via Tikhonov regularization},
  author={Boyd, John P},
  journal={Applied Mathematics Letters},
  volume={5},
  number={6},
  pages={57--59},
  year={1992},
  publisher={Elsevier}
}

@article{kingma2014adam,
  title={Adam: A method for stochastic optimization},
  author={Kingma, Diederik P and Ba, Jimmy},
  journal={arXiv preprint arXiv:1412.6980},
  year={2014}
}

@article{bachlechner2020rezero,
  title={Rezero is all you need: Fast convergence at large depth},
  author={Bachlechner, Thomas and Majumder, Bodhisattwa Prasad and Mao, Huanru Henry and Cottrell, Garrison W and McAuley, Julian},
  journal={arXiv preprint arXiv:2003.04887},
  year={2020}
}

@inproceedings{chen2021pre,
  title={Pre-trained image processing transformer},
  author={Chen, Hanting and Wang, Yunhe and Guo, Tianyu and Xu, Chang and Deng, Yiping and Liu, Zhenhua and Ma, Siwei and Xu, Chunjing and Xu, Chao and Gao, Wen},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={12299--12310},
  year={2021}
}

@article{birman1967piecewise,
  title={Piecewise-polynomial approximations of functions of the classes W\_p\^{}$\alpha$},
  author={Birman, Mikhail Shlemovich and Solomyak, Mikhail Zakharovich},
  journal={Matematicheskii Sbornik},
  volume={115},
  number={3},
  pages={331--355},
  year={1967},
  publisher={Russian Academy of Sciences, Steklov Mathematical Institute of Russian~…}
}

@article{nguyen2019deep,
  title={Deep neural network with high-order neuron for the prediction of foamed concrete strength},
  author={Nguyen, Tuan and Kashani, Alireza and Ngo, Tuan and Bordas, St{\'e}phane},
  journal={Computer-Aided Civil and Infrastructure Engineering},
  volume={34},
  number={4},
  pages={316--332},
  year={2019},
  publisher={Wiley Online Library}
}

@article{siegel2020approximation,
  title={Approximation rates for neural networks with general activation functions},
  author={Siegel, Jonathan W and Xu, Jinchao},
  journal={Neural Networks},
  volume={128},
  pages={313--321},
  year={2020},
  publisher={Elsevier}
}

@inproceedings{liu2018progressive,
  title={Progressive neural architecture search},
  author={Liu, Chenxi and Zoph, Barret and Neumann, Maxim and Shlens, Jonathon and Hua, Wei and Li, Li-Jia and Fei-Fei, Li and Yuille, Alan and Huang, Jonathan and Murphy, Kevin},
  booktitle={Proceedings of the European conference on computer vision (ECCV)},
  pages={19--34},
  year={2018}
}

@book{eisenbud2013commutative,
  title={Commutative algebra: with a view toward algebraic geometry},
  author={Eisenbud, David},
  volume={150},
  year={2013},
  publisher={Springer Science \& Business Media}
}

@article{fan2018sparse,
  title={On a Sparse Shortcut Topology of Artificial Neural Networks},
  author={Fan, Fenglei and Wang, Dayang and Guo, Hengtao and Zhu, Qikui and Yan, Pingkun and Wang, Ge and Yu, Hengyong},
  journal={arXiv preprint arXiv:1811.09003},
  year={2018}
}

@misc{2017Self,
  title={Self-Normalizing Neural Networks},
  author={ Klambauer, G.  and  Unterthiner, T.  and  Mayr, A.  and  Hochreiter, S. },
  year={2017},
}


@article{szegedy2013intriguing,
  title={Intriguing properties of neural networks},
  author={Szegedy, Christian and Zaremba, Wojciech and Sutskever, Ilya and Bruna, Joan and Erhan, Dumitru and Goodfellow, Ian and Fergus, Rob},
  journal={arXiv preprint arXiv:1312.6199},
  year={2013}
}


@article{akhtar2018threat,
  title={Threat of adversarial attacks on deep learning in computer vision: A survey},
  author={Akhtar, Naveed and Mian, Ajmal},
  journal={Ieee Access},
  volume={6},
  pages={14410--14430},
  year={2018},
  publisher={IEEE}
}


@inproceedings{moosavi2017universal,
  title={Universal adversarial perturbations},
  author={Moosavi-Dezfooli, Seyed-Mohsen and Fawzi, Alhussein and Fawzi, Omar and Frossard, Pascal},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={1765--1773},
  year={2017}
}

@inproceedings{dai2018adversarial,
  title={Adversarial attack on graph structured data},
  author={Dai, Hanjun and Li, Hui and Tian, Tian and Huang, Xin and Wang, Lin and Zhu, Jun and Song, Le},
  booktitle={International conference on machine learning},
  pages={1115--1124},
  year={2018},
  organization={PMLR}
}


@article{zhu2002learning,
  title={Learning from labeled and unlabeled data with label propagation},
  author={Zhu, Xiaojin and Ghahramani, Zoubin},
  journal={ProQuest Number: INFORMATION TO ALL USERS},
  year={2002},
  publisher={Citeseer}
}

@article{zhou2003learning,
  title={Learning with local and global consistency},
  author={Zhou, Dengyong and Bousquet, Olivier and Lal, Thomas and Weston, Jason and Sch{\"o}lkopf, Bernhard},
  journal={Advances in neural information processing systems},
  volume={16},
  year={2003}
}



@article{mignone2020multi,
  title={Multi-task learning for the simultaneous reconstruction of the human and mouse gene regulatory networks},
  author={Mignone, Paolo and Pio, Gianvito and D${\v{z}}$eroski, Sa${\v{s}}$o and Ceci, Michelangelo},
  journal={Scientific reports},
  volume={10},
  number={1},
  pages={22295},
  year={2020},
  publisher={Nature Publishing Group UK London}
}




@article{ma2015deep,
  title={Deep neural nets as a method for quantitative structure--activity relationships},
  author={Ma, Junshui and Sheridan, Robert P and Liaw, Andy and Dahl, George E and Svetnik, Vladimir},
  journal={Journal of chemical information and modeling},
  volume={55},
  number={2},
  pages={263--274},
  year={2015},
  publisher={ACS Publications}
}

@article{ding2018interpretable,
  title={Interpretable dimensionality reduction of single cell transcriptome data with deep generative models},
  author={Ding, Jiarui and Condon, Anne and Shah, Sohrab P},
  journal={Nature communications},
  volume={9},
  number={1},
  pages={2002},
  year={2018},
  publisher={Nature Publishing Group UK London}
}

@inproceedings{velickovic2018graph,
  title={Graph attention networks, international conference on learning representations},
  author={Velickovic, Petar and Cucurull, Guillem and Casanova, Arantxa and Romero, Adriana and Lio, Pietro and Bengio, Yoshua},
  booktitle={International Conference on Learning Representations},
  pages={1--2},
  year={2018}
}

@article{gligorijevic2018deepnf,
  title={deepNF: deep network fusion for protein function prediction},
  author={Gligorijevi{\'c}, Vladimir and Barot, Meet and Bonneau, Richard},
  journal={Bioinformatics},
  volume={34},
  number={22},
  pages={3873--3881},
  year={2018},
  publisher={Oxford University Press}
}

@article{wang2020inductive,
  title={Inductive inference of gene regulatory network using supervised and semi-supervised graph neural networks},
  author={Wang, Juexin and Ma, Anjun and Ma, Qin and Xu, Dong and Joshi, Trupti},
  journal={Computational and structural biotechnology journal},
  volume={18},
  pages={3335--3343},
  year={2020},
  publisher={Elsevier}
}


@article{marbach2012wisdom,
  title={Wisdom of crowds for robust gene network inference},
  author={Marbach, Daniel and Costello, James C and K{\"u}ffner, Robert and Vega, Nicole M and Prill, Robert J and Camacho, Diogo M and Allison, Kyle R and Kellis, Manolis and Collins, James J and others},
  journal={Nature methods},
  volume={9},
  number={8},
  pages={796--804},
  year={2012},
  publisher={Nature Publishing Group US New York}
}

@article{huynh2010inferring,
  title={Inferring regulatory networks from expression data using tree-based methods},
  author={Huynh-Thu, V{\^a}n Anh and Irrthum, Alexandre and Wehenkel, Louis and Geurts, Pierre},
  journal={PloS one},
  volume={5},
  number={9},
  pages={e12776},
  year={2010},
  publisher={Public Library of Science San Francisco, USA}
}

@article{bar2022constrained,
  title={Constrained Fourier estimation of short-term time-series gene expression data reduces noise and improves clustering and gene regulatory network predictions},
  author={Bar, Nadav and Nikparvar, Bahareh and Jayavelu, Naresh Doni and Roessler, Fabienne Krystin},
  journal={BMC bioinformatics},
  volume={23},
  number={1},
  pages={1--21},
  year={2022},
  publisher={BioMed Central}
}


