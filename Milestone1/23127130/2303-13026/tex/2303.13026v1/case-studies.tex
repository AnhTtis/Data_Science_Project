\section{Methodology}
\label{sec:simMethod}

For all case studies except real workloads under full system simulation, we used \gem{}'s traffic generators instead of execution-based workloads.
Using traffic generators allows us to explore the behavior of the DRAM cache design more directly and more clearly understand the fundamental trade-offs in its design.
We use \gem{}'s traffic generator to generate two patterns of addresses: linear or random.
If not specified, the tests are run with a linear traffic generator.

% In our studies, where a full system simulation was not needed, 
% we considered UDCC receives 
% the requests from a traffic generator included in \gem{}, 
% known as PyTrafficGen. gem5's PyTrafficGen is 
% implemented as a simobject and can be used as a data requestor component. 
% It creates a syntehtic traffic which can be fed to a memory subsystem such as UDCC. 
% PyTrafficGen's output can be either probabilistic or trace-based. In the 
% trace-based mode, it uses a memory trace to produce the requests. In 
% the probabilistic mode it takes a list of parameteres which describe the 
% characteristics of its output pattern. %Table \ref{tab:traffGenParam} describes these parameteres.
% These parameters are listed below:
% \begin{itemize}
%   \item Duration: The duration of producing requests (in ps).
%   \item Start Address:  The lower bound of produced addresses.
%   \item End Address: The upper bound of produced addresses.
%   \item Minimum Period: The shortest timing gap between two consecutive requests.
%   \item Maximum Period: The longest timing gap between two consecutive requests.
%   \item Request Size: The size of read/written data by each request (in bytes).
%   \item Read Percentage: The percentage of reads among all the requests (the rest of requests are writes).
% \end{itemize}

To implement the DRAM cache we used the DRAM models (``interfaces'') provided by gem5.
We also used the NVM interface provided in gem5 with its default configuration unless otherwise specified (timing parameters are shown in the `Base' column in Table~\ref{tab:nvmtimes}).
%\note{What is the default NVM config? Maybe point forward to the table later?}
We extended \gem to model DDR5 (based on DDR5 data-sheets) and HBM (based on a performance comparison) for our studies.

In the case studies presented below, we are not concerned with the detailed configuration of the \textit{UDCC} (e.g., the size of DRAM cache). 
Instead, we are interested in the study of the behavior of the \textit{UDCC} through specific scenarios 
which enables us to evaluate the best, worst, or in between performances of the system. For this purpose, we have used 
patterns which are either Read-Only (RO), Write-Only (WO), or a combination of reads and writes (70\% reads 30\% writes). 
% For the latter we used a pattern of 70\% read and 30\% writes. 
% These patterns were enforced using the 
% Read Percentage parameter of the traffic generator. 
The other traffic pattern characteristic we varied is the hit (or miss) ratio of the generated pattern. 
This factor was enforced by two different parameters (1) the size of the DRAM cache and (2) the range of addresses requested by the traffic 
generator. In all of the case studies in which the traffic generator was involved, we used a DRAM cache of 16 MB size backed-up 
by NVRAM as the main memory. Unless otherwise specified, we used 
DDR4 to implement the DRAM cache in \textit{UDCC} with a total
buffer size of 256 entries. In order to get 0\%, 
25\%, 50\%, 75\%, and 100\% hit ratios, we set the range of addresses to be 6GB, 64MB, 32MB, 20MB, and 6MB. 
% In this way we were able 
%to test the UDCC for a combination of RO, WO, or 70\% read and 30\% writes with the aforementioned hit ratios. 
For instance, to study 
the behavior of a write intensive application with a memory foot print larger than the DRAM cache capacity on our proposed model, 
we set the traffic generator to give WO accesses within the range of 6GB for a DRAM cache of 16MB capacity. 
In this way, we were able to test the \textit{UDCC} in a reasonable 
simulation time. 
We used a cache block size of 64B in all tests to match current systems.
All the tests were simulated for 1 second. 
%% should I say any margine for the hit ratio err?



% \begin{center}
%   \begin{tabular}{ | m{2.5cm} | m{5cm} | }
%     %\caption gem5's PyTrafficGen parameteres in probabilistic mode} \\
%     \hline
%      Duration & The duration of producing requests (in ps) \\ 
%     \hline
%      Start Address &  The lower bound of produced addresses \\ 
%     \hline
%      End Address & The upper bound of produced addresses \\ 
%      \hline
%      Minimum Period & The shortest timing gap between two consecutive requests \\ 
%      \hline
%      Maximum Period & The longest timing gap between two consecutive requests \\ 
%      \hline
%      Request Size & The size of read/written data by each request (in bytes) \\ 
%      \hline
%      Read Percentage & The percentage of reads among all the requests (the rest of requests
%      are write requests) \\ 
%     \hline
%   \end{tabular}
%   \label{tab:traffGenParam}
%   \end{center}

% \section{Case Studies}
% \label{sec:case_studies}


\section{Case Study 1: Impact of Scheduling Policy on DRAM Caches Performance}
\label{sec:sched}

\subsection{Background and Methodology}

The policy to choose which memory request to service has a significant impact on the performance of the DRAM memory system.
% Different scheduling policies for memory subsystems have been proposed to provide fairness and high quality of service.
In this work we consider two scheduling policies: (i) first-come, first-serve (\textit{FCFS}), and (ii) first-ready, first-come, first-serve (\textit{FRFCFS}).
\textit{FCFS} is the naive policy which processes the memory requests in the order they are received by the memory controller.
%\textit{FCFS} can be beneficial where the demand's addresses are uniform and are well-distributed across the banks, taking advantage of bank-level parallelism.
While it is simple and easy to implement, \textit{FCFS} adds many row-switching delay penalties, leading to lower bus utilization.
Rixner et al.~\cite{rixner2000memory} proposed \textit{FRFCFS} trying to take advantage of maximizing row-buffer hit rate. \textit{FRFCFS} reorders
the received requests so the ones that hit on the currently-opened row would be serviced earlier than any other requests which map to the other rows that are currently closed.
%  switching penalty as much as possible. Amongst all the requests that hit on the currently-opened row, whichever was received first,
% would be serviced first. In case of no request's address mapped to the currently-opened row, it will switch to another row and pay the switching delay penalty.
% Generally, \textit{FCFS} is easy to implement as it requires a simpler logical circuit than \textit{FRFCFS}, and it's a default scheduling policy leveraged in some CPU vendors' products.
In heterogeneous memory systems, Wang et al.~\cite{wang2020characterizing} reported that Intel's Cascade Lake's NVRAM interface deploys \textit{FCFS}.
The question that arises here is, in a memory system with two cooperative devices, one as DRAM cache and the other as main memory, with several internal requests buffers to arbitrate from, how important is the choice of scheduling policy?
% Can \textit{FRFCFS} be a better choice in such memory systems than \textit{FCFS}?

We extended \textit{UDCC} with \textit{FCFS} and \textit{FRFCFS} scheduling policies to answer this question.
\textit{UDCC} employs the same scheduling policy for both DRAM cache and main memory.
We tested \textit{UDCC} with each
scheduling policy to measure the improvement of bandwidth observed by LLC with \textit{FRFCFS} over \textit{FCFS}.
We have run tests with different hit ratios and different read and write request combinations to test
the sensitivity of bus utilization and the system performance to the scheduling policy.

% We configured \textit{UDCC} with a total
% buffer size of 256 entries, a 16MB DDR4 DRAM cache, and NVRAM as the main memory.
%\note{Why is it OK to only use 16 MB???}
The results are based on request patterns of 0\% hit ratio and 100\% hit ratio for RO and WO.
We also ran patterns containing 70\% read and 30\% write requests, with 100\%, 75\%, 50\%, 25\% and 0\% hit ratios.

\subsection{Results and Discussions}

\begin{figure}
    \centering
    \includegraphics[scale=0.6]{figures/woro-bw.pdf}
    \vspace{-1ex}
    \caption{Bandwidth seen by LLC (GB/s). \textit{UDCC} has been tested with a total buffer size of 256 entries, 
    DDR4 DRAM cache, and NVRAM main memory for \textit{FCFS} and
     \textit{FRFCFS} scheduling policies. On the X-axis read-only (RO) and write-only (WO) patterns with 0\% and 100\% hit ratios are
     shown for the two scheduling policies.}
    \label{fig:woroBW}
  \end{figure}

  \begin{figure}
    \centering
    \includegraphics[scale=0.6]{figures/r70-bw.pdf}
    \vspace{-1ex}
    \caption{Bandwidth seen by LLC (GB/s). \textit{UDCC} has been tested with a total buffer size of 256 entries, 
    DDR4 DRAM cache, and NVRAM main memory for \textit{FCFS} and \textit{FRFCFS} scheduling policies. \textit{UDCC} has been fed by 70\% read requests and 30\% write requests.
    As shown on the X-axis, different hit ratios (0\%, 25\%, 50\%, 75\%, 100\%) have been applied to the request patterns.}
    \label{fig:r70BW}
  \end{figure}

%\begin{figure}
%  \subfloat[\scriptsize{DRAM}]{
%    \includegraphics[width=.45\linewidth, scale=0.5]{figures/woro-busUtil-dram.pdf}
%    \label{fig:dram1}
%  }
%   \subfloat[\scriptsize{NVM}]{
%    \includegraphics[width=.45\linewidth, scale=0.5]{figures/woro-busUtil-nvm.pdf}
%    \label{fig:nvm1}
%  }
%   \caption{Total bus utilization percentage. \textit{UDCC} has been tested with an outstanding
%   request buffer size of 256 entries, 16MB DDR4 DRAM cache, and NVRAM main memory for \textit{FCFS}
%   and \textit{FRFCFS} scheduling policies. On the X-axis Read-Only and Write-Only patterns
%    with different hit ratios (0\%, 100\%) are shown for the two scheduling policies.}
%   \label{fig:woroBusUtil}
%  \end{figure}

\begin{figure}
  \subfloat[\scriptsize{DRAM}]{
    \includegraphics[width=.45\linewidth, scale=0.5]{figures/r70-busUtil-dram.pdf}
    \label{fig:dram}
  }
   \subfloat[\scriptsize{NVRAM}]{
    \includegraphics[width=.45\linewidth, scale=0.5]{figures/r70-busUtil-nvm.pdf}
    \label{fig:nvm}
  }
    \caption{Bus utilization percentage of DRAM and NVRAM. \textit{UDCC} has been tested with a total buffer size of 256 entries, 
    DDR4 DRAM cache, and NVRAM main memory for \textit{FCFS} and \textit{FRFCFS} scheduling policies. \textit{UDCC} has been fed by 70\% read requests and 30\% write requests.
     As shown on the X-axis, different hit ratios (0\%, 25\%, 50\%, 75\%, 100\%) have been applied to the request patterns.}
    \label{fig:r70BusUtil}
  \end{figure}

% Why is this indented???

Figure \ref{fig:woroBW} compares the observed bandwidth at the LLC, while running requests patterns which are RO and WO with 100\% and 0\% hit ratio DRAM cache.
For RO 100\% hit ratio and WO 100\% hit ratio, \textit{FRFCFS} achieved higher bandwidth than \textit{FCFS} by
2.56x and 2.85x, respectively. For RO 0\% hit ratio and WO 0\% hit ratio the observed bandwidth with \textit{FRFCFS} compared to \textit{FCFS} are
1.65x and 1x.

From this data, we conclude that the scheduling policy is more important for workloads which are more likely to saturate the DRAM bandwidth and are not limited by NVRAM bandwidth.
For instance, in the WO with 0\% hit ratio case, the performance is completely limited by the NVRAM device bandwidth and the scheduling policy does not affect the performance at all.
  % Overall, \textit{FRFCFS} can give up to 2.85x bandwidth improvement over \textit{FCFS} for DRAM
  % cache (i.e., 2.85x for WO with 100\% hit ratio).

  To look at a slightly more realistic situation, Figure~\ref{fig:r70BW} shows the observed bandwidth by LLC while running a pattern consisting of 70\% read, and
  30\% write accesses and the hit ratio increases on the X-axis with 25\% steps. As the hit ratio increases, the observed bandwidth increases
  for both \textit{FRFCFS} and \textit{FCFS} because more hits results in more DRAM accesses which is higher bandwidth.
  %The reason is, in
  %order to handle the misses, NVRAM reads (to fetch any missing lines from NVRAM) or writes
  %(to write any evicted dirty lines of DRAM cache to NVRAM)
  % accesses are required. So, a single request by LLC may turn into multiple memory accesses and bus transactions within the memory controller,
  % exacerbating the overall observed performance of the system. Thus, the fewer misses on DRAM cache (higher hit ratio) happens, the higher the
  % performance becomes.
  Figure~\ref{fig:r70BW} also shows that as the hit ratio increases, the improvement of bandwidth by \textit{FRFCFS} over \textit{FCFS}, increases.
  % Overall, \textit{FRFCFS} gives a higher bandwidth than \textit{FCFS} by 1.08x, 1.15x, 1.31x, 2.17x, and
  % 2.69x for the 0\%, 25\%, 50\%, 75\%, 100\% hit ratios, respectively.
  % Ultimately, \textit{FRFCFS} has improved bandwidth up to 169\%
  % over \textit{FCFS} for DRAM cache for an access pattern consisting of 70\% read, and 30\% write (i.e., 2.69x for a hit ratio of 100\%).

% \note{Below: Have 1 short paragraph about how FRFCFS improves bus utilization significantly for DRAM, then one short paragraph about how it improves bus utilization less for NVRAM. Thus, when miss rates are high (more NVM than DRAM accesses) the policy matters less.}

To better understand the performance improvement of the \textit{FRFCFC} policy 
Figure~\ref{fig:r70BusUtil} shows 
the bus utilization percentage for 70\% read 
and 30\% write case, for both DRAM and NVRAM devices.
%In Figure~\ref{fig:dram}, DRAM bus utilization of \textit{FRFCFS} is higher. 
% than \textit{FCFS} by 2.56x, 2.85x, and 1.65x for RO 100\% hit ratio, WO 100\% hit ratio,
  % and RO 0\% hit ratio, respectively. 
% For the cases of WO with 0\% hit ratio, DRAM bus utilization of \textit{FRFCFS} compared to \textit{FCFS} is
%   only slightly higher by 0.13\%. 
% In this case, there's an increase in the number of dirty lines evicted from DRAM cache needed to
%   be written back to the NVRAM.
% \note{I don't understand this last part...}
% \note{Also, we could drop Figure~\ref{fig:woroBusUtil}}

% In Figure \ref{fig:nvm1}, NVRAM bus utilization of RO 100\% hit ratio and WO 100\% hit ratio,
% are zero as there are no misses to be handled through NVRAM.
% For the cases of RO with 0\% hit ratio, NVRAM
  % bus utilization of \textit{FRFCFS} compared to \textit{FCFS} is
  % higher by 1.65x. For the cases of WO with 0\% hit ratio, NVRAM bus utilization of \textit{FRFCFS} compared to \textit{FCFS} is
  % slightly higher by 0.13\%. In this case, there's an increase in the number of dirty lines evicted from DRAM cache needed to
  % be written back to the NVRAM. %%%% AYAZ
  In Figure \ref{fig:dram}, 
  DRAM bus utilization of \textit{FRFCFS} is
  higher than FCFS in all hit ratios. 
  % than \textit{FCFS} by 1.08x, 1.15x, 1.31x, 2.17, and 2.69 for
  % 0\%, 25\%, 50\%, 75\%, 100\% hit ratios, respectively.
  Besides, as the hit ratio increases, the improvement of
  DRAM bus utilization by \textit{FRFCFS} compared to \textit{FCFS} also increases.
  In Figure \ref{fig:nvm}, the NVRAM bus utilization decreases for both \textit{FRFCFS} and \textit{FCFS}, as the hit ratio increases 
  since there are less misses to be handled by NVRAM.
  % than \textit{FCFS} by 1.08x, 1.15x, 1.31x, and 2.16 for
  % 0\%, 25\%, 50\%, 75\% hit ratios, respectively. 
  % Since there's no misses needed to be handled through NVRAM
  % for the case of 100\% hit ratio, the NVRAM bus utilization is zero for this case.
  Overall, DRAM interface bus utilization has benefited more from \textit{FRFCFS} (compared to \textit{FCFS}), than NVRAM. Moreover,
  where the hit ratio is higher, the improvement of bus utilization by \textit{FRFCFS} over \textit{FCFS} is higher.

  Based on the proposed DRAM cache model, there are several internal buffers at the controller per interface for reads and writes. The fair
  arbitration of requests in each of these buffers can highly impact the utilization of the resources, affecting the performance of the DRAM
  cache. 
  % In this section we demonstrated the impact of scheduling policy from two different points. First, we discussed the
  %effect of scheduling policy on bandwidth seen by LLC, as an UDCC-external effect. 
  We showed
  \textit{FRFCFS} can achieve up-to 2.85x bandwidth improvement over \textit{FCFS} (in WO with 100\% hit ratio) and that the improved DRAM bus utilization is the main factor contributing to this improvement.
  % Second, we discussed
  % the effect of scheduling policy on bus utilizations of DRAM and NVRAM devices, as an UDCC-external effect.
  % We showed bus utilizations improvement of \textit{FRFCFS} over \textit{FCFS} can go up-to 185\% (in WO with 100\% hit ratio) for DRAM and 116\% (for the case of
  % 70\% read and 30\% writes with 75\% hit ratio).
  % \textit{UDCC} with \textit{FRFCFS} can achieve up to 2.69x higher bandwidth compared to \textit{FCFS}.
  % In none of the test cases, \textit{UDCC} with \textit{FRFCFS} was outperformed by \textit{FCFS}.
  Overall, even though implementation of \textit{FRFCFS} would be more
  costly than \textit{FCFS} due to more complex and associative circuits, the performance gap between these two policies is significant.
  % on a system containing a DRAM cache with
  % multiple buffers to arbitrate requests from is not negligible. % In the end,
  % our simulation results suggest \textit{FRFCFS} can be more beneficial than \textit{FCFS} for DRAM caches.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Case Study 2: Performance Analysis of Different DRAM Technologies as DRAM Cache}

\begin{figure}
  \subfloat[\scriptsize{Maximum bandwidth achieved}]{
    \includegraphics[width=.48\linewidth, scale=0.5]{figures/bws.pdf}
    \label{fig:bws}
  }
  \subfloat[\scriptsize{Buffer size for maximum bandwidth}]{
    \includegraphics[width=.48\linewidth, scale=0.5]{figures/orb.pdf}
    \label{fig:orb}
  }
    \caption{Buffer size needed to achieve the maximum bandwidth seen by LLC for 
    \textbf{DDR3}, \textbf{DDR4}, and \textbf{DDR5} as DRAM cache. The traffic patterns 
    shown in the figure include read-only (RO) and write-only (WO), 
    each having 100\% and 0\% hit ratio.
    }
    \label{fig:orb_bw}
  \end{figure}


\subsection{Background and Methodology}

Different commercial products have adapted different DRAM technologies for their DRAM caches (e.g., HBM in Sapphire Rapids and DDR4 in Cascade Lake).
%  For instance, Intel's Knights Landing
% and their newest product Sapphire Rapids have used HBM, and Cascade Lake has used DDR4, as their DRAM cache.
These technologies have different characteristics (e.g., peak bandwidth) that gives different applicability to each.
In this
study we want to answer these questions: how many total buffers are required for each DRAM technology to fully utilize it as DRAM cache?
And, what is the peak performance of each device when the hit ratio and the percentage of read-write in access patterns change?

To address these questions, we configured \textit{UDCC} to use DDR3, DDR4, DDR5 and HBM models implemented for \gem, as DRAM cache.
The theoretical peak bandwidth of DDR3, DDR4, DDR5 and HBM are 12.8 GB/s, 19.2 GB/s, 33.6 GB/s, and 256 GB/s, respectively.
% We use a traffic generator sending synthetic traffic patterns to the \textit{UDCC} to model requests sent by the LLC.
% Like the previous case study, these patterns were 
The results are based on request patterns of 
RO 100\% hit ratio, RO 100\% miss ratio, WO 100\% hit ratio, and WO 100\% miss ratio.
We ran these patterns across all DRAM technologies for buffer sizes from 2 to 1024 by powers of 2.
For each
case we looked for the buffer size where the observed bandwidth by LLC reached to its maximum state and
would not get improved by increasing the size of the buffers.
To separate out the total number of buffers from the specific micro-architectural decisions (e.g., how to size each buffer), we only constrained the ``outstanding request buffer'' and allowed all of the internal buffers to be any size.

\subsection{Results and Discussion}

Figure \ref{fig:bws} shows the maximum achieved bandwidth for the DRAM cache, using DDR3, DDR4 and DDR5,
for the case of RO 100\% hit ratio, RO 100\% miss ratio, WO 100\% hit ratio, and WO 100\% miss ratio.
Moreover, Figure \ref{fig:orb} shows the amount of total buffers required in order to reach to the maximum bandwidth shown
in Figure \ref{fig:bws} for each case.

In RO 100\% hit ratio access pattern, each request requires only a single access to fetch the data along with tag and metadata from DRAM.
Thus, it is expected to achieve to a bandwidth very close to the theoretical peak bandwidth, and Figure~\ref{fig:bws} shows this is the case.
% The peak bandwidths in this case are 12.74 GB/s, 19.12 GB/s, and 33.27 GB/s for DDR3, DDR4, and DDR5, respectively.
Moreover,
the Figure \ref{fig:orb} shows that DDR3, DDR4, and DDR5 have reached to this bandwidth at buffer size of 128, 256, 256, respectively. Thus, increasing the buffer size would not help for RO 100\% hit traffic pattern.
%increasing the buffer size to any size larger than these sizes has not been effective to improve %the achieved bandwidth.
%We assume these buffer
%sizes for each device as their baseline size, since they achieve a bandwidth close to the peak %theoretical bandwidth of their technology.
%\note{Why not the buffer size that has peak bandwidth for writes?}

In WO 100\% hit ratio access pattern, each request requires two accesses to fetch tag and metadata from DRAM and then writing the data to the DRAM.
The peak bandwidth in this case for each device is lower than the theoretical peak by about a factor of two (5.58 GB/s, 8.2 GB/s, and 16.47 GB/s for DDR3, DDR4, and DDR5, respectively).
Since these requests require two DRAM accesses, the latency of each request is higher and more buffering is required to reach the peak bandwidth.
Specifically, Figure~\ref{fig:orb} shows that DDR3, DDR4, and DDR5 have reached to this bandwidth at buffer size of 256, 512, 1024~\footnote{DDR5 shows bandwidth improvement of 8\% going from 512 to 1024 entries}, respectively.
Comparing this case with RO 100\% hit ratio shows this case has gotten some bandwidth improvement from increasing the buffer size.
% However,
% any buffer size larger than these sizes did not get a higher bandwidth.

In RO 100\% miss ratio access pattern, each request requires three accesses: one for fetching tag and metadata from DRAM, then fetching
the line from NVRAM, and finally writing the data to the DRAM.
The peak bandwidth in this case for each device is 3.47 GB/s, 4.45 GB/s, and 6.21 GB/s for DDR3, DDR4, and DDR5, respectively.
Figure \ref{fig:orb} shows that DDR3, DDR4, and DDR5 have reached to this bandwidth at buffer size of 256, 256, 1024, respectively.
It suggests DDR5 has gotten some bandwidth improvement from increasing the buffer size. 
However, the bandwidth improvement from 512 to 1024 buffer entries is 3.8\% (and 2\% for 256 to 512 buffer size change).
%\note{Is this improvement too small to call this bandwidth the peak bandwidth?}
%However,
%any buffer size larger than these sizes did not improved achieved bandwidth for this case.

Finally, for WO 100\% miss ratio access pattern, each request requires four accesses: one for fetching tag and metadata from DRAM, then fetching
the line from NVRAM, writing the data to the DRAM, and a write to NVRAM for dirty line write back. 
The peak bandwidth in this case is 1.91 GB/s, 1.90 GB/s, and 1.90 GB/s for DDR3, DDR4, and DDR5, respectively.
The Figure \ref{fig:orb} shows that DDR3, DDR4, and DDR5 have reached to this bandwidth at buffer size of 16, 16, 32, respectively.
Comparing this case with the previous cases shows that, in this case the bandwidth gets saturated in smaller
buffer sizes than the other cases.

%\note{It would be interesting to see what the speedup of the ``best'' buffer for write only and read misses is over the ``baseline'' of 128 buffers.}

% The reason that in each case the bandwidth is not improved after a certain buffer size, is that the bandwidth becomes saturated and
% the buffer size is not a limiting factor anymore in that situation. In fact, the limiting factor becomes the latency and the performance
% of the device which is already at its peak by that specific buffer size. In this situation, increasing the buffer size
% will lead to higher latency according to the Little's Law. Another important factor to consider for the amount of total buffers required, is the
% access amplification of the requests. When the access amplification is higher, the amount of total jobs (e.g., multiple accesses to multiple memory interfaces)
% needed to be internally done by the controller is more. Thus, the bandwidth becomes saturated with fewer outstanding requests within the memory controller.
% Moreover, if the access amplification is not too high, the technologies with higher theoretical bandwidth, can benefit from larger buffers.

The main takeaway from this case study is that the buffer size required to achieve peak bandwidth largely depends on
the composition of memory traffic. Secondly, the DRAM cache controller might need a large number of buffers to reach the peak
bandwidth, primarily if the device can provide a large bandwidth (e.g., over 1000 in the case of DDR5).
More memory controller buffers are needed because of the increased latency of accesses since each write request and read miss request requires multiple accesses to the memory devices.

% Overall, this study shows that each DRAM technology benefit from a specific buffer size to get fully utilized as the DRAM cache,
% since they have different characteristic. This is an important point which is needed to be considered at the design time of the DRAM caches.


\subsection{HBM}

We separate out HBM from the discussion of other DDR memory technologies as its memory controller design may be different from the \textit{UDCC} design described in this paper and used in the Intel's Cascade Lake.
For instance, in Intel's Knight's Landing, the DRAM cache was implemented with a four-state coherence protocol and the HBM interfaces were physically separated from the DRAM interfaces through the on-chip network~\cite{sodani2015knights}.

% \Ayaz{The latest DRAM cache based commercial processors (e.g. Sapphire Rappids) rely on HBM as a DRAM cache.
% Since HBM devices are characterized by their high bandwidths, it raises a question that what buffer sizes would be sufficient in \textit{UDCC} to take full advantage of HBM as a cache.
% Secondly, what is the utilization of HBM in the cases when packets do not always hit in the DRAM cache?
% To answer these questions 
We modeled an HBM interface in \gem{} such that it can provide the theoretical peak bandwidth of 256 GB/s in a single memory channel (equivalent to the sum of all pseudo channels in an HBM2 device).
Figure~\ref{fig:hbm} shows the bandwidth that is achieved for different total buffer sizes, for two cases: 1) when all accesses hit in the DRAM cache, and 2) when all accesses miss in the DRAM cache.
% The results are based on a synthetic linear traffic pattern generated using gem5's traffic generator.
In case of all hits, we see close to the maximum possible bandwidth with an 2048 buffers.
In case of all misses, the achievable bandwidth is limited by the main memory used in the experiment (a NVRAM device) and does not get better after buffer size of 1024 entries.

This data implies that the buffer sizes required for HBM are not that much higher than for high-performance DDR DRAMs (e.g., DDR5).
The main reason that HBM does not require significantly more buffering is because even for a small miss rate (13\%), we are limited by the NVRAM's performance and saturate bandwidth at much less than the theoretical peak as shown in Figure~\ref{fig:hbm}.
However, if the HBM cache was backed by higher performance memory (e.g., DDR4 or DDR5 DRAM) the total number of buffers to achieve the maximum performance would likely be much higher.


\begin{figure}
  \centering
  \includegraphics[scale=0.6]{figures/hbmbw.pdf}
  \vspace{-1ex}
  \caption{Buffer size impact on read bandwidth for an HBM based DRAM cache (theoretical peak bandwidth : 256 GB/s).}
  \label{fig:hbm}
\end{figure}


\section{Case Study 3: Performance Analysis of DRAM Caches While Backed-up by Different Main Memory Models}

\subsection{Background and Methodology}


\begin{table}
	\centering
       \caption{NVRAM Interface Timing Parameters}
      \begin{tabular}{|p{2.2cm}|p{1.32cm}|p{1.50cm}|p{1.40cm}|}
      \Xhline{2\arrayrulewidth}
      \textbf{Parameter} & \textbf{Slow} & \textbf{Base} & \textbf{Fast} \\ \Xhline{2\arrayrulewidth}
      \textbf{tREAD} & 300ns & 150ns & 75ns \\ \hline
      \textbf{tWRITE} & 1000ns & 500ns & 250ns \\ \hline
      \textbf{tSEND} & 28.32ns & 14.16ns & 7.08ns \\ \hline
      \textbf{tBURST} & 6.664ns & 3.332ns & 1.666ns \\ \Xhline{2\arrayrulewidth}
    \end{tabular}
    \label{tab:nvmtimes}
\end{table}

% Heterogenous Memory (HM) emerged as a promising direction to increase the overall capacity and bandwidth
% by combining different memory technologies within the same system. Usually, HM consists of multiple memory components
% known as fast (or near) and slow (or far) memories. Many combination of devices can be considered in such system, e.g.,
% Intel's Cascade Lake provides a smaller-capacity fast-memory DDR4 compared to its large-capacity slow PCM memory connected
% to the same channel. In some of the HM systems, fast (near) memory can serve as the DRAM cache for the slow (far)
% memory. For instance, in Intel's Cascade Lake the DDR4 can serve PCM as DRAM cache, and in Intel's
% Sapphire Rapid a HBM memory is served as DRAM cache for NVRAM. Such combination of technologies in the DRAM caching idea,
% rises this question that how much the performance (such as latency) of the main memory can affect the performance of DRAM cache
% observed by the LLC? Since every miss on a fill-on-miss DRAM cache needs to be handled by communicating to the main memory,
% it is expected that the latency of the main memory can affect the overall bandwidth and latency of the DRAM cache.

In this section, we focus on the question how much does the performance (such as latency) of the main memory affect the performance of the memory system as observed by the LLC in DRAM cache-based systems? 
We extended the baseline
NVRAM interface model of gem5 to implement a faster NVRAM and slower NVRAM compared to the baseline performance.
The baseline, fast, and slow models of gem5 NVRAM provide 19.2 GB/s, 38.4 GB/s, and 9.6 GB/s bandwidth, respectively.
Table \ref{tab:nvmtimes} shows the timing constraints of all three cases.
% We considered a 16 MB DRAM cache using gem5 DDR4 model.
% \note{is this a repeat from the main methodology section?}
% Connected to \textit{UDCC}, a gem5's traffic generator produced the requests sent by LLC. 
The results are based on
an access pattern with a high miss ratio and large number of dirty line evictions from DRAM cache,
to highly challenge the performance of the main memory for both read and write accesses to it.
For this purpose, we used a WO with 100\% miss ratio access pattern which generates dirty lines along the way,
requiring write-backs to the main memory. This access pattern highly engages the NVRAM during miss 
handling to fetch the missed lines
and write-back of dirty lines, enabling us to evaluate the performance of the system 
in all three cases of slow, fast, and baseline NVRAMs. Moreover, we have tested them for a pattern 
consisting of RO 100\% miss ratio, since this case also requires interaction with NVRAM (for fetching 
the missed line). In both patterns, the results are based on a total buffer size of 512 entries.

\subsection{Results and Discussion}

\begin{figure}
  \centering
  \includegraphics[scale=0.5]{figures/nvmcs3.pdf}
  \vspace{-1ex}
  \caption{Observed bandwidth by the LLC for read-only (RO) 100\% miss ratio and write-only (WO) 100\% miss ratio.}
  \label{fig:bw-wo100M-cs4}
\end{figure}

% \begin{figure}
%   \centering
%   \includegraphics[scale=0.6]{figures/avgNvmQuLat-wo100miss-fastSlowNVM-caseStudy4.pdf}
%   \vspace{-1ex}
%   \caption{Average NVRAM write buffer queue latency for the write-only 100\% miss ratio access pattern.}
%   \label{fig:avgNvmQL-cs4}
% \end{figure}

First we investigate the effect of the different NVRAMs from the UDCC-external point of view.
Figure \ref{fig:bw-wo100M-cs4} compares the bandwidth seen by the LLC for three
different NVRAMs, slow, baseline, and fast, for RO 100\% miss ratio and WO 100\% miss ratio 
requests patterns. Note that NVRAM has a 
dedicated write buffer in \textit{UDCC} whose size is enforced by the NVRAM interface (128 entries in gem5 NVRAM models). 
Our resuts showed, the average queuing latency for this buffer is 76.19~$\mu$s,
38.49~$\mu$s, and 19.52~$\mu$s for slow, baseline, and fast NVRAMs, respectively.
In other words, the write queuing latency at the NVRAM interface, gets shorter
once the speed of NVRAM increases, as expected. 
In the WO 100\% miss ratio, the highest achieved bandwidth is 0.96 GB/s, 1.91 GB/s, and 3.8 GB/s for slow, baseline and fast NVRAMs, 
respectively. This interprets to 49.7\% bandwidth degradation once the
slow NVRAM was used as the main memory, and 98.9\% bandwidth improvement when the fast NVRAM
was used as the main memory, compared to the baseline NVRAM. Note that in this request pattern 
there are two NVRAM accesses (one read and one write).

In the RO 100\% miss ratio, the highest achieved bandwidth is 3.39 GB/s, 4.55 GB/s, and 5.1 GB/s for slow, baseline and fast NVRAMs, 
respectively. This interprets to 25.49\% bandwidth degradation once the
slow NVRAM was used as the main memory, and 12.08\% bandwidth improvement when the fast NVRAM
was used as the main memory, compared to the baseline NVRAM. Note that in this request pattern 
there is only one NVRAM (read) access.

These results suggest that if an access pattern requires more interaction with NVRAM (e.g., 
where the DRAM cache miss ratio is higher, or when there are more dirty line evictions), 
the improvement of bandwidth observed by LLC with faster NVRAM is higher, for a system consisting of DRAM cache. 
Moreover, these results are based on a total buffer size of 512 entries as our further investigations showed that 
none of the devices gains more bandwidth by providing larger buffer size. Thus, even though slowing down 
the NVRAM device hurts performance, it does not affect the microarchitectural details of the controller.


% \note{why WO 100\% miss? If you're interested in the buffer size, then you 
% should look at 100\% miss reads or 100\% hits writes as that's what you found required the most buffers before.}

% Note that slow and baseline NVRAM have reached to the saturated bandwidth at small buffer size (16 entries), and have not
% gained any bandwidth improvement by increasing the size of the buffers after 16 entries. The fast NVRAM has gained bandwidth
% improvement by increasing the buffer size up-to 256 entries. In all three cases, increasing the size of buffers after the bandwidth is 
% reached to its saturated state, 
% is not effective on improving performance. The reason is that 
% the system is now limited by the NVRAM and DRAM devices performance (rather than the size of buffer before bandwidth saturation).
% In other words, the devices are at their full utilization and increasing the amount of total buffers is not improving bandwidth, yet it can lead 
% to longer latency.

% Now, we look for the effect of different NVRAM speed from the UDCC-internal point of view.
% Figure \ref{fig:avgNvmQL-cs4} compares the average NVRAM queuing latency per requests for slow,
% baseline, and fast NVRAMs. The amount of total buffers is swept over, on the X-axis. At total buffer size of 512, the average latency is 76.19~$\mu$s,
% 38.49~$\mu$s, and 19.52~$\mu$s for slow, baseline, and fast NVRAMs, respectively.
% This means the queuing latency at the NVRAM interface, gets shorter
% once the speed of NVRAM increases, as expected. However, for all three NVRAMs, as the buffer size increases, the average queuing latency
% also increases. 
% We know from Figure \ref{fig:bw-wo100M-cs4}
% the bandwidth is saturated at the buffer size of 16 entries for slow and baseline NVRAMs, and at the buffer size of 256 entries
% for fast NVRAM. At the same time, from Figure \ref{fig:avgNvmQL-cs4}
% we can observe the average queueing time of NVRAM keeps getting longer as the buffer size increases. 
% Note that NVRAM has a
% dedicated write buffer in UDCC whose size is enforced by the NVRAM interface (128 entries in gem5 NVRAM models).
% As explained before, once the bandwidth is saturated, NVRAM and DRAM devices are at their full utilization and adding more buffer size
% is not beneficial to improve the performance. Instead, it adds pressure on the internal buffers such as NVRAM write buffer in this case,
% causing longer latency for each requests in those buffers. As a result, choosing the proper size of buffers is important to avoid any indirect
% performance degradation.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Case Study 4: Evaluating DRAM Caches for Real Applications}
\label{sec:realBench}

In this case study, we performed an evaluation of DRAM caches for real-world applications, i.e., NPB~\cite{bailey1991parallel} and GAPBS~\cite{beamer2015gap}.
The DRAM caches are expected to perform similar to a DRAM main memory if the working set of the workload fits in the cache.
Therefore, a case of particular interest for us is to evaluate the performance of the DRAM cache-based system when the workload does not fit in the DRAM cache.
To accomplish the previously mentioned goal, we model a scaled-down system with 64MB DRAM cache and run NPB and GAPBS workloads for one second of simulation time.

We run all workloads in three different configurations.
The first two configurations (\textit{NVRAM, DRAM}) model a system without a DRAM cache and the main memory as NVRAM or DRAM.
The third configuration (\textit{DCache\_64MB}) uses a 64MB DRAM cache and NVRAM as main memory.
Figure~\ref{fig:npb} shows a million instructions per second (MIPS) values for NPB in three configurations.
In most cases, \textit{DCache\_64MB} performs the worst, with the most prominent performance degradation for \textit{lu.C} and \textit{bt.C}.
The only exception is \textit{is.C}, where \textit{DCache\_64MB} performs better than \textit{NVRAM}.
The performance of \textit{DCache\_64MB} correlates with the DRAM cache misses per thousand instructions (MPKI) values shown in Figure~\ref{fig:npbmiss}.
For example, \textit{is.C} shows the smallest and \textit{lu.C} shows the largest MPKI values.

Figure~\ref{fig:gapbs} shows a million instructions per second (MIPS) values for GAPBS in the previously mentioned three configurations.
Figure~\ref{fig:gapbmiss} shows the associated MPKI values.
The graph workloads on a DRAM cache perform mostly similarly to NVRAM alone in our simulation runs.
The only exception is \textit{bfs} which shows a significantly lower MIPS value than \textit{NVRAM}.
The DRAM cache MPKI value for \textit{bfs} alone does not explain the reason for this behavior.
\textit{bfs} has the highest fraction of writes than reads in the requests that hit the DRAM cache (29\% more writes than reads in case of \textit{bfs} in contrast to 36\% less writes than reads for the rest of the workloads)

Since a write hit also leads to access amplification (tag read before a write hit), the impact of this amplification is then seen in the performance degradation with a DRAM cache.
We conclude from this observation that the extra DRAM accesses (for tag reads) can also impact the workload's performance on a DRAM cache.

\begin{figure}
  \subfloat[\scriptsize{Performance}]{
    \includegraphics[width=.55\linewidth, scale=0.6]{figures/npbNew.pdf}
    \label{fig:npb}
  }
   \subfloat[\scriptsize{Cache miss rate}]{
    \includegraphics[width=.45\linewidth, scale=0.5]{figures/npb_miss.pdf}
    \label{fig:npbmiss}
  }
    \caption{NAS Parallel Benchmarks on DRAM Cache}
    \label{fig:npbRes}
  \end{figure}

\begin{comment}
\begin{figure}
  \centering
  \includegraphics[scale=0.7]{figures/gapbsNew.pdf}
  \vspace{-1ex}
  \caption{GAPBS.}
  \label{fig:gapbs}
\end{figure}

\begin{figure}
  \centering
  \includegraphics[scale=0.7]{figures/gapbs_miss.pdf}
  \vspace{-1ex}
  \caption{DRAM cache misses per thousand instructions}
  \label{fig:gapbs}
\end{figure}
\end{comment}

\begin{figure}
  \subfloat[\scriptsize{Performance}]{
    \includegraphics[width=.55\linewidth, scale=0.6]{figures/gapbsNew.pdf}
    \label{fig:gapbs}
  }
   \subfloat[\scriptsize{Cache miss rate}]{
    \includegraphics[width=.45\linewidth, scale=0.5]{figures/gapbs_miss.pdf}
    \label{fig:gapbmiss}
  }
    \caption{GAPBS on DRAM Cache}
    \label{fig:gapbsRes}
  \end{figure}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Case Study 5: NVRAM Write Wear Leveling Effect}

\subsection{Background and Methodology}
Non-volatile storage and memory devices, such as Phase Change Memories (PCM) used as NVRAM
and flash memory, are known to have a limited write endurance.
Write endurance is defined as the number of writes to a block in the device, before it becomes
unreliable. One of the techniques used for such devices to prolong their lifetime, is
wear leveling. Usually NVRAM wear-leveling technique tries to evenly distribute wear-out by moving data from one
highly-written location to a less worn-out location. Wang et al.~\cite{wang2020characterizing} measured the frequency and latency overhead
of data migration due to wear leveling in NVRAMs through probing and profiling. They reported a long tail latency of 60~$\mu$s
for almost every 14000 writes to each 256B region of NVRAM. Wear leveling can affect the performance of DRAM caches. 
This effect is more noticeable while running write-intensive
workloads which their memory footprint is larger than the DRAM cache capacity. In such situation, many dirty lines eviction will happen
that are needed to be written back to NVRAM; thus, an overall increase on NVRAM writes can be expected. As a result, frequent data migration for
wear-leveling (with a long tail latency) will be done by NVRAM interface and a performance degradation
can be expected in this case.

In this section we investigate the effect of this long latency on DRAM cache performance. We extended NVRAM interface model of
gem5 so for every 14000 write accesses the interface adds 60~$\mu$s extra latency, delaying the service time of the next request.
% We configured \textit{UDCC} with 16MB DDR4 DRAM cache and used an ORB with 256 entries.
% \note{repeated???}
The access pattern was set to be all writes and 100\% miss and 8x bigger than the DRAM cache size. This will
increase the number of write-backs of dirty lines
from DRAM cache to the NVRAM, to pressure the NVRAM write buffer and see the effect of wear leveling delay on the overall system performance.

\subsection{Results and Discussion}

\begin{table}
	\centering
       \caption{Maximum write bandwidth (and average NVM write latency) achieved with and without wear-leveling}
      \begin{tabular}{|p{2.3cm}|p{2.45cm}|p{2.75cm}|}
      \Xhline{2\arrayrulewidth}
      \textbf{Parameter} & \textbf{With wear-leveling} & \textbf{Without wear-leveling} \\ \Xhline{2\arrayrulewidth}
      \textbf{Bandwidth} & 1.77 GB/s & 1.92 GB/s \\ \hline
      %\textbf{Buffer size} & 256 & 256 \\ \hline
      \textbf{NVM write latency} & 42.84~$\mu$s & 39.71~$\mu$s \\ \Xhline{2\arrayrulewidth}
    \end{tabular}
    \label{tab:nvmwear}
\end{table}

\begin{comment}
\begin{figure}
  \centering
  \includegraphics[scale=0.6]{figures/bw-wearLeveling-cs6.pdf}
  \vspace{-1ex}
  \caption{[They will be bar graphs] Write bandwidth observed by LLC (GB/s) with and without NVRAM wear leveling delay.}
  \label{fig:bw-wearLeveling}
\end{figure}

\begin{figure}
  \centering
  \includegraphics[scale=0.6]{figures/avgNvmWrQT-wearleveling-cs6.pdf}
  \vspace{-1ex}
  \caption{[They will be bar graphs] Average NVRAM write queuing time with and without NVRAM wear leveling delay.}
  \label{fig:avgNvmWrQT-wearleveling}
\end{figure}
\end{comment}

Table \ref{tab:nvmwear} compares the overall bandwidth seen by LLC in two cases; with wear leveling and
without wear leveling. 
%The ORB size is 256 entries which gives the highest bandwidth possible for both cases.
Without wear leveling the peak bandwidth
is 1.92~GB/s while it drops to~1.77 GB/s when wear leveling is activated. These results show a 7.8\% performance degradation which directly comes
from wear leveling overhead.
Table \ref{tab:nvmwear} also shows the average queuing latency measured for NVRAM write buffer. This latency is 42.84~$\mu$s for
NVRAM-with-wear-leveling, and 39.71~$\mu$s for NVRAM-without-wear-leveling.
Thus, the 60~$\mu$s latency considered for data migration during wear leveling, has caused 7.8\% latency overhead on NVRAM write buffer as well.
This 7.8\% overhead is larger than expected given the rarity of wear leveling events showing that these rare events have an outsized impact when the system is configured with a DRAM cache.

% Note that due to the access amplification in this case, the bandwidth observed by LLC without wear leveling is much less than the theoretical
% bandwidth of DRAM cache device (19.2 GB/s for DDR4). Thus, 7.8\% additional performance degradation caused by wear-leveling is
% another point which should be considered in the design of DRAM caches.

%\note{could probably drop the table to save space.}
