\section{Introduction}
\label{sec:intro}

%[1. what are DRAM Caches and what's their use? What's rational DRAM Cache?]

%Over the past decades, as the applications and workloads were scaling out, their demands for larger last level caches, as well as larger main memory to acomodate their memory footprints, have been growing.
As the applications are scaling out and their memory footprints are growing more than ever, the demand for larger last-level caches increases.
For instance, web search applications and the business analytics workloads require hundreds of megabyte last level caches~\cite{milojevic2012thermal,meswani2015heterogeneous}. Machine learning and graph analytics workloads demand up to terabytes of main memory for training~cite{hildebrand2021case}. Moreover, large last-level caches are effective in overcoming the bandwidth wall~\cite{rogers2009scaling}. However, large last-level caches can occupy up to 90\% of the chip area if made using conventional SRAM, thus limiting the number of cores on the chip~\cite{rogers2009scaling} [5]. Large last-level SRAM caches would also require an advanced heat dissipation system due to their high leakage power~\cite{mittal2015survey}. %As a result, the large area and high leakage power of conventional SRAM caches, make them inconvenient for large sizes of last level caches.

On the other hand, with the new trends presented in the designs of 3D interconnects and on-chip stacking and packaging technologies, the memory modules can now offer diverse DRAM technologies.
Some of these technologies provide 4-8$\times$ higher bandwidth than traditional DRAM technologies, such as Hybrid Memory Cube (HMC), HighBandwidth Memory (HBM), making them a good candidate for last-level caches. Intel's Cascade Lake [ref] follows this option and uses DRAM caches along with a non-volatile main memory (NVRAM). NVRAMs are byte-addressable and thus can be referenced by the normal load and store instructions.
NVRAMs' large capacity (per unit area) makes them a clear choice to use as main memory in the systems.
However, their bandwidth is at least 60\% lower, and latency is 3$\times$ higher compared to DRAM~\cite{hildebrand2021case,izraelevitz2019basic}.
Using DRAM as a cache in NVRAM (as the main memory) based systems can help alleviate the higher latency of NVRAMs.

%have the convenience of byte-addressablity so the programs can reference memory through load and store instructions, and also they provide larger size of main memories %which makes them a replacement for conventional DRAM main memories where they cannot be shrinked more to provide larger memory size in the same area to respond to the %growing demand of today's application's working set size. However, compared to DRAM their bandwidth is at least 60\% lower and their latency is 3Ã— higher [51]. As a %result, having DRAM as a cache along side the NVMM in modules like Intel's cascade lake can help hide the latency of NVMM.

%\Ayaz{
%\newline
%Higher density of DRAM makes it a convincing choice to build (hardware managed) caches to evade the growing bandwidth wall.

The last decade has seen a lot of academic research on DRAM caches.
Those ideas are becoming a reality with CPU vendors coming up with DRAM cache-based computer systems, e.g., Intel's Cascade Lake and Sapphire Rapids support DRAM caches.
%[2. referencing to works about 1]
Researchers working on DRAM cache designs have explored various design choices. These design choices include: the granularity of cache lines (block based~\cite{qureshi2012fundamental}, page based~\cite{jevdjic2013stacked}), dram cache tag placement (SRAM~\cite{huang2014atcache,loh2012supporting} or DRAM itself~\cite{loh2011efficiently}), associativity (direct mapped~\cite{qureshi2012fundamental} vs. set associative~\cite{kotra2018chameleon,young2018accord}), and experiments with other metadata, e.g., MissMap (to predict the future miss)~\cite{loh2011efficiently}, access predictor, memory footprint~\cite{jevdjic2013stacked}, and way predictor~\cite{young2018accord}.~In addition the DRAM cache replacement policy, choice of actual DRAM device as a cache, and the dynamism of the design become other important variables to consider. On top of the previously mentioned design dimensions, the design of the system memory controller becomes an important factor. Most DRAM cache designs assume the `DRAM acting as cache' to be on the same memory channel as the backing memory device (thus controlled by the same memory controller). All of this makes it imperative to build a detailed DRAM cache simulation model which can be used to perform a design space exploration around the DRAM cache design.
The previous research on DRAM cache design improvements does not provide any (open source) DRAM cache modeling platform for a detailed micro-architectural and timing analysis.
Moreover, most research works do not consider systems where the hardware-managed DRAM cache and NVM might be sharing the same bus and are controlled by a unified memory controller (as is the case in real platforms like Intel Cascade Lake).
To the best of our knowledge, there is only one work discussing an analytical performance model of DRAM cache for in-SRAM and in-DRAM tag storage
organizations~\cite{gulur2015comprehensive}. Gulur et al.~\cite{gulur2015comprehensive} proposed an analytical DRAM cache model to estimate the average miss penalty and the bandwidth seen by the last level (on-chip) cache. Though this work considers the timing constraints of DRAM for cache management, it is agnostic to the micro-architecture and timing constraints of main memory technologies cooperating with DRAM cache and still leaves a gap for a full-system DRAM cache simulation for detailed micro-architectural analysis.

\begin{comment}
[3. what are the problems with their evaluation in the previous works?]
%Most of the previous research work in the domain of DRAM cache analysis fall into the scopes of optimization (e.g., performance[], area[], energy[], reliability[], etc.%), cache architecture ( e.g., tags in DRAM[] vs tags in SRAM[], direct mapped caches[] vs set-associative caches[], fine-grained[] vs coarse-grained granularity[], etc.%) [survey].

Though these works discuss improvement ideas towards better DRAM cache management, they do not provide any DRAM cache modeling platform for a detailed micro-architectural and timing analysis. Considering the timing and micro-architectural restriction applied by the memory technologies involved in the memory system which contains a DRAM cache, lack of a unified DRAM cache management platform leaves a gap between the realistic memory chips containing DRAM cache in the market (e.g., Intel's Cascade Lake []) and the research space exploration to make improvement for the next generation of heterogenous memory systems management.

To the best of our knowledge, there is only one work discussing an analytical performance model of DRAM cache for in-SRAM and in-DRAM tag storage
organizations~\cite{gulur2015comprehensive}. Gulur et al.~\cite{gulur2015comprehensive} proposed an analytical DRAM cache model to estimate the average miss penalty and the bandwidth seen by the last level (on chip) cache. Though this work considers the timing constraints of DRAM for cache management, it is agnostic to the micro-architecture and timing constraints of main memory technologies cooperating with DRAM cache, and still leaves a gap for a full-system DRAM cache simulation for detailed micro-architectural analysis.
\end{comment}

In this work, we present a cycle-level DRAM cache management protocol and extend \gem~(widely used full-system computer architecture simulator)~\cite{lowepower2020gem5} with a unified DRAM cache and main memory controller (\textit{UDCC}) to implement this protocol.
The protocol takes inspiration from the actual hardware providing DRAM cache, such as Intel's Cascade Lake, in which an NVRAM accompanies a DRAM cache as the off-chip main memory sharing the same bus. To model such hardware, we consider the timing and micro-architectural
details enforced by the memory organizations involved. For instance, DRAM timing constraints, scheduling policies, buffer sizes, and queuing limitations are the factors to be considered for the DRAM cache side.
In addition, the DRAM cache is a direct-mapped insert-on-miss write-back cache in which dirty lines will be written back to NVRAM upon eviction of the line for a new request. NVRAMs also come with specific timing and queuing limitations that should be considered to handle miss cases, fill cases, and write-backs for DRAM cache. Moreover, the protocol takes into account the details of actual behavior seen by the
Intel's Cascade Lake while running large-scale graph analytics and machine learning applications, as discussed by Hildebrand et al.~\cite{hildebrand2021case}.
Hildebrand et al.~\cite{hildebrand2021case} showed that a single demand request could require up to 5 memory accesses based on the DRAM cache hit or miss status
of the request. Implementing the micro-architectural details of such a memory system that supports the hardware's realistic behavior on a simulator can help find any potential improvement for the next generation of memory systems, which is the focus of this work.


[This part should be rephrased and re-written after the discussion on case studies is formed]
Through our presented protocol and implementation of it on gem5 simulator, we find XXX main insights:
\begin{itemize}
    \item Choosing proper scheduling policy is vital to reach to the peak bandwidth allowed by the DRAM technology at the cases possible to
    reach to the peak bandwidth via better bus utilization and removing pressure from buffers (lower queuing time).
    Overall, for DRAM caches, FRFCFS can improve bandwidth by XXX\% on average compared to FCFS baseline.
    \item The peak bandwidth seen by the last level SRAM cache (LLSC) happens at the buffer size XXX.
    Any larger buffer than that would increase the latency seen by LLSC without gaining any performance improvement (BW).
    \item insight about the storage of tag and metadata in DRAM vs SRAM. If tag-in-SRAM case shows better performance, proposing a case for
    partitioning DRAM for part-cache part-memory based on the marginal size we can estimate for the tag-in-SRAM, if
    not defending about the tag-in-DRAM design choice.
\end{itemize}

[Add some conclusion for the insights practical usage]

The rest of the paper is organized as follows...

