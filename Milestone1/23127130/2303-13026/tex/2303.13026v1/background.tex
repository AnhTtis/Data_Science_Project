\section{Background}
\label{sec:background}


% The concentrations of related research work in DRAM cache, mostly fall into the scopes of metadata management, performance optimization, cache architecture, and power consumption.

% Tag space overhead associated with DRAM caches has been one of the main focus of many works proposing and evaluating different
% tag and metadata management schemes. This space overhead depends on the granularity size of the cache organization which is majorely considered as
% fine-grain (block-based) or coarse-grain (page-based). The methods proposed for tag and metadata (we use tag to refer to both) storage include: (1) tag in DRAM [], (2) tag in SRAM [],
% (3) sectored cache [63] with tags in SRAM, (4) partial tags in SRAM, and (5) sectored cache with partial tags in SRAM [39]. Storing tags in DRAM combines the data and metadata access
% allowing the SRAM to be fully allocated to data storage while delaying the hit latency. On the other hand, tag in SRAM accelerates the hit latency at the cost of limiting capacity of
% cache available for data in the SRAM. Zhao et al.[39] investigated the aforementioned various options and reported that the combination of partial tags and sectoring achieves the
% highest performance with the lowest tag space overhead.

% Other works tried to bring more innovation to the DRAM cache organization to optimize their performance. Loh and Hill [29] tried to make hit latency shorter by storing fine-grain tags along with the data
% in DRAM to access both through a compound access so the data ccess will always cause a row-buffer hit. Moreover, they proposed a MissMap structure which tracks the content of DRAM to avoid DRAM cache access
% in case of a the access was determined to be a cache-miss via MissMap. This idea was levergaed in several later works []. Sim et al. [35] proposed to speculate the access throug a light-weight miss-predictor
% to avoid the large overhead of the MissMap. Qureshi and Loh [8] reported that the lowering the latency of hits should be prioritized over the hit rate. The reason is that the performance
% optimization techniques that may increase the hit latency (such as MissMap), can potentially degrade the overall performance, eventhough they may improve the hit rate. As a result, they proposed Alloy Cache
% which is a direct mapped DRAM cache in which tag and data are retrieved together and by using a predictor the DRAM cache is avoided in case of unlikely-presence prediction to save bandwidth.
% Jevdjic et al. [53] proposed Footprint Cache that takes advantage of course granularity for data allocation and fine granularity for data fetch for the blocks that are actually touched during the
% residency of the page in the cache. Footprint Cache avoids extra off-chip traffic and maintains high hit ratio and small tag storage overhead. In another work, Jevdjic et al. [51] proposed Unison Cache
% which combines Alloy Cache and Footprint Cache. Similar to Alloy Cache, it embeds the tag along with data in the DRAM cache for scalability and avoiding the tag-in-SRAM limitations; also, like Footprint Cache,
% it uses course-grain page-size cache allocation to maintain high hit rate and lowering the tag storage overhead.

% There have been a full body of research work evaluating the performance of DRAM cache organizations from power consumption perspective.
% Black et al. [6] considered different combination of SRAM and DRAM stacked caches and analyzed them for performance and thermal point of view. They considered a dual core processor
% with a shared 4MB SRAM L2 cache as the baseline. In their second design they added a 8 MB stacked SRAM to the baseline, and in their third designs they replaced the 4MB
% shared SRAM L2 cache in the baseline with 32 MB stacked DRAM cache, and in their fourth design they use a 64 MB stacked DRAM with a 4 MB SRAM cache for tag store. They report
% compared to baseline the stacked DRAM designs do not have significant thermal increase, opposite the stacked SRAM design due to high leackage power of SRAM in general.
% Moreover, the increased cache capacity by the stacked DRAM has led to off-chip bandwidth reduction.
% Xu et al. [7] have analyzed different 2D/3D NoCs with SRAM and 2D/3D DRAM last level caches (LLC) from the area, latency and power consumption aspects. Their experimental results showed that
% the in the DRAM based designs, the hit latency is increased by 12.53\% for 2D DRAM LLC, and reduced by 27.97\% for 3D LLC DRAM compared to the SRAM LLC. They also reported that the power
% consumption of 3D NoC which included DRAM LLC has reduced 25.78\% compared with the SRAM design due to low.


\subsection{Background on heterogeneous memory}

With the growing footprint of large scale applications, commercial products have been emerging in the market
to accommodate the memory requirements of the today's demanding workloads. Intel has introduced Optane Data-Center
Persistent-Memory-Modules (DC PMM) which is a non-volatile byte-addressable main memory (NVRAM) and can be a
replacement for conventional DRAM ~\cite{hildebrand2021case}. Even though NVRAM provides much larger capacity than DRAM, it has 3x longer latency and
60\% lower bandwidth than DRAM ~\cite{izraelevitz2019basic}. Hardware caching through a DRAM cache for NVRAM, has been employed in Intel's
Cascade Lake product to hide the long latency of NVRAM. Memory subsystem in Cascade Lake works in two modes,
1LM in which the NVRAM is persistent and directly accessible to the application, and 2LM in which DRAM DIMM
is the hardware cache for NVRAM. Recently, Intel announced the release of the new generation of Xeon processor called Sapphire Rapid~\cite{sapphire} which includes an HBM working in three modes,
HBM-Only, Flat Mode, and Cache Mode.
% [tomshardware \note{find a better citation. The hot chips talk is better.}]. 
HBM-Only Mode considers HBM as the main memory of the system without
any DRAM memory. With the provided DDR5 in the system, HBM and DDR5 both work as the main memory in the
Flat Mode. Finally, HBM is used as a cache for DDR5 in the Cache Mode. To accommodate the research lines in this
domain, simulators must be able to correctly implement the real behavior of these memory technologies. Wang
et al.~\cite{wang2020characterizing} have studied the behavior of NVRAM in Intel's Cascade Lake and provided the
behavior seen in VANS model. However, they do not provide any support for the DRAM cache cooperating with NVRAM
in the 2LM mode. In this work we seek to model the DRAM cache protocol and implement it in gem5 simulator.


\subsection{Background on gem5's memory subsystem}

The \gem{} simulator is based on an event-driven simulation engine.
It supports models of many system components, including a memory controller, a detailed DRAM model, an NVM model, and a model for different CPUs, caches, and others.
The memory controller module added to \gem{} by Hansson et al.~\cite{hansson2014simulating} focuses on modeling the state transitions of the memory bus and the memory banks.
While it is not ``cycle accurate'', it is \emph{cycle level}, and the memory controller is designed to enable fast and accurate memory system exploration.
%\note{Can we bring in an accuracy number from the Hansson ISPASS paper?}
Hansson et al.~\cite{hansson2014simulating} showed that their memory controller model in \gem{} correlates well with a more detailed model DRAMSim2~\cite{rosenfeld2011dramsim2}, but leads to a 13\% reduction in simulation time on average.
The controller is responsible for address decoding (into a row, bank, column, and rank) and keeping track of the DRAM timings.
The memory controller is also configurable with different front-end and back-end latencies, which can model different memory controller designs and the impact of physical interfaces.

The memory controller in \gem{} was refactored recently~\cite{gem5-workshop-presentation}.
Now, it relies on two components to simulate a real memory controller. 
(1) The \textit{memory controller} receives commands from the CPU and enqueues them into appropriate queues and manages their scheduling to the memory device.
(2) The \textit{memory interface} deals with device/media-specific timings, manages the device-specific operations and communicates with the memory controller.

The unified DRAM cache memory controller (\textit{UDCC}) presented in this work implements a new memory controller module in \gem{} and requires only minor changes in the memory interface.
Like \gem's current memory controller, \textit{UDCC}'s goal is for cycle-level simulation to enable micro-architectural exploration and flexibility, not cycle-by-cycle accuracy to a single design.