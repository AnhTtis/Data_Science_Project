\section{Unified DRAM cache controller}

  % \begin{figure*}
  % \centering
  % \scriptsize{Unified cache controller layout}{
  %   \fbox{\includegraphics[scale=0.6]{figures/simulatedHW.jpg}}
  %   \label{fig:buffers}
  % }
  %  \scriptsize{State machine of unified cache controller}{
  %   \fbox{\includegraphics[scale=0.5]{figures/protocol.jpg}}
  %   \label{fig:StateMachine}
  % }
  % \caption{Unified DRAM cache controller design}
  % \label{fig:DController}
  % \end{figure*}


\begin{figure}
  \centering
  \includegraphics[scale=0.45]{figures/simulatedHW.jpg}
  \vspace{-1ex}
  \caption{Hardware abstraction of \textit{UDCC}}
  \label{fig:buffers}
\end{figure}

\begin{figure}
  \centering
  \includegraphics[scale=0.4]{figures/protocol.jpg}
  \vspace{-1ex}
  \caption{State machine followed by packets in \textit{UDCC}}
  \label{fig:StateMachine}
\end{figure}

We model a unified DRAM cache controller (\textit{UDCC}) in \gem{} to control a DRAM device (which acts as a cache of the main memory) and an NVM device (which serves as the main memory in the system).
\textit{UDCC} is largely based on Intel Cascade Lake's DRAM cache management strategy.
Figure~\ref{fig:buffers} provides a high-level overview of the modeled
controller's layout, and Figure~\ref{fig:StateMachine} shows the states that the memory packets transition through while residing in \textit{UDCC}.
%\note{Do we want to add an overview diagram of the heterogeneous memory?}

We designed \textit{UDCC} to be a flexible model to explore different memory controller design parameters.
Thus, instead of modeling one specific micro-architecture with buffers for every device (e.g., separate DRAM and NVM read and write queues), we model the controller with a single large buffer called the \textit{Outstanding Request Buffer (ORB)}.
We also have a \textit{Conflict Request Buffer (CRB)} to store requests that are to the same cache line and must be serialized with current outstanding requests.
These buffers are shown in Figure~\ref{fig:buffers}.
% shows that the simulated controller is primarily composed of two buffers: 1) \textit{Outstanding Request Buffer (ORB)} and 2) \textit{Conflict Request Buffer (CRB)}.
All incoming memory requests reside in the \textit{ORB} unless a request conflicts with an already existing request in the \textit{ORB}.
Two requests are considered to be conflicting if they both map to the same DRAM cache block.
The conflicting request goes to the \textit{CRB} until the request it was conflicting with has been serviced and is taken out of the \textit{ORB}.
Each entry in these buffers contains other meta data in addition to the address of the request, as shown in Figure~\ref{fig:buffers}.
This metadata provides helpful information about the request, e.g., the memory request's current state and relative arrival time.
In addition to these two buffers, we also model an \textit{NVM WrBack} queue for the DRAM cache dirty lines that are to be written back to NVM main memory.

Figure~\ref{fig:StateMachine} presents the state machine followed by the memory packets while they are in \textit{UDCC}.
Since \gem{} is an event driven simulator, the simulation model relies on scheduled events to transition between these states.
To model the realistic buffers in the system, we can constrain the number of requests at one time in any state.
The states these memory packets can be in at any point are as follows:

\noindent
\textbf{Recv\_Pkt}: Every memory request received to the memory controller is in \textit{Recv\_Pkt} state initially.

\noindent
\textbf{DRAM\_Read:} Since every request must first check the tag in the cache, a memory packet moves to the \textit{DRAM\_Read} state when the time arrives that the DRAM can service this request. This transition is achieved by scheduling an event for the ready time of this packet. The device-specific memory interface provides the ready time. At this point, the request is also scheduled to the DRAM itself for a read.

\noindent
\textbf{DRAM\_Read\_Resp:} Once the response time of an already scheduled (to DRAM) packet arrives, the packet is moved to the \textit{DRAM\_Read\_Resp} state. At this point, the DRAM cache tags are checked to ascertain the hit/miss and clean/dirty status of the request and different actions are performed accordingly.

\noindent
\textbf{DRAM\_Write:} A memory packet can go to the \textit{DRAM\_Write} state in two scenarios: (1) if its tag read from DRAM cache is done (and found to be a hit) when it originally was a write packet, or (2) if we get a response from the NVM that is going to fill in DRAM cache (initiated by `a missing in DRAM cache' request).
Once in \textit{DRAM\_Write} state, packets are eventually written to the DRAM.

\noindent
\textbf{NVM\_Read\_Wait\_Issue:} Packets missing in DRAM cache (found out by a tag read in DRAM cache), are moved to this state so that they can eventually be scheduled to NVM.

\noindent
\textbf{NVM\_Read:} Once the ready time of packets in \textit{NVM\_Read\_Wait\_Issue} arrives (NVM interface is ready to accept the packet), they are scheduled to NVM and moved to \textit{NVM\_Read} state.

\noindent
\textbf{NVM\_Read\_Resp:} On getting a response back from the NVM device (when the NVM interface is done with the processing of the packet), the packet moves to \textit{NVM\_Read\_Resp} state where it will be eventually moved to the \textit{DRAM\_Write} state so that it can be written to the DRAM. If the packet was originally a read request (and missed in DRAM cache), at this point the response for this packet will also be sent to the requestor.

\noindent
\textbf{NVM\_Write:} The packets belonging to a dirty cache line in DRAM are moved to the \textit{NVM\_Write} state, so that they can be written to the main memory (NVM).

\noindent
\textbf{Done:} If a (read) packet was found to be a hit in the DRAM cache, the response will be sent back to the requestor and the packet will move to the \textit{Done} state.
Similarly, when a (write) packet is finally written to the DRAM cache, it moves to the \textit{Done} state. Packets in \textit{Done} state are removed from the \textit{ORB}.

Since we are modeling a memory-side cache that does not participate in the on-chip coherence protocol, we do not model the stored data in the cache.
We use the protocol described above to model the timing of the system, and for data we use the backing store of the backing memory (NVM in our case).
Similarly, while we model the timing of accesses to DRAM to check the tags, we functionally store the tags in a separate structure in our model. 