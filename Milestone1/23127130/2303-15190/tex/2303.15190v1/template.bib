@article{MLint_Survey,
AUTHOR = {Carvalho, Diogo V. and Pereira, Eduardo M. and Cardoso, Jaime S.},
TITLE = {Machine Learning Interpretability: A Survey on Methods and Metrics},
JOURNAL = {Electronics},
VOLUME = {8},
YEAR = {2019},
NUMBER = {8},
ARTICLE-NUMBER = {832},
URL = {https://www.mdpi.com/2079-9292/8/8/832},
ISSN = {2079-9292},}


@inproceedings{SanityChecks,
 author = {Adebayo, Julius and Gilmer, Justin and Muelly, Michael and Goodfellow, Ian and Hardt, Moritz and Kim, Been},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Sanity Checks for Saliency Maps},
 url = {https://proceedings.neurips.cc/paper/2018/file/294a8ed24b1ad22ec2e7efea049b8737-Paper.pdf},
 volume = {31},
 year = {2018}
}


@inproceedings{trust,
author = {Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
title = {"Why Should I Trust You?": Explaining the Predictions of Any Classifier},
year = {2016},
isbn = {9781450342322},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2939672.2939778},
doi = {10.1145/2939672.2939778},
booktitle = {Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {1135–1144},
numpages = {10},
keywords = {explaining machine learning, interpretable machine learning, black box classifier, interpretability},
location = {San Francisco, California, USA},
series = {KDD '16}
}

@article{peirce2019psychopy2,
  title={PsychoPy2: Experiments in behavior made easy},
  author={Peirce, Jonathan and Gray, Jeremy R and Simpson, Sol and MacAskill, Michael and H{\"o}chenberger, Richard and Sogo, Hiroyuki and Kastman, Erik and Lindel{\o}v, Jonas Kristoffer},
  journal={Behavior research methods},
  volume={51},
  number={1},
  pages={195--203},
  year={2019},
  publisher={Springer}
}


@inproceedings{unified,
 author = {Lundberg, Scott M and Lee, Su-In},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {A Unified Approach to Interpreting Model Predictions},
 url = {https://proceedings.neurips.cc/paper/2017/file/8a20a8621978632d76c43dfd28b67767-Paper.pdf},
 volume = {30},
 year = {2017}
}



@inproceedings{XAI,
author = {Bhatt, Umang and Xiang, Alice and Sharma, Shubham and Weller, Adrian and Taly, Ankur and Jia, Yunhan and Ghosh, Joydeep and Puri, Ruchir and Moura, Jos\'{e} M. F. and Eckersley, Peter},
title = {Explainable Machine Learning in Deployment},
year = {2020},
isbn = {9781450369367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3351095.3375624},
doi = {10.1145/3351095.3375624},
booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
pages = {648–657},
numpages = {10},
keywords = {qualitative study, machine learning, deployed systems, transparency, explainability},
location = {Barcelona, Spain},
series = {FAT* '20}
}


@inproceedings{Attention,
 author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, \L ukasz and Polosukhin, Illia},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Attention is All you Need},
 url = {https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf},
 volume = {30},
 year = {2017}
}

%no check que preprint
@article{DistilBERT,
  title={DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter},
  author={Sanh, V. and Debut, L. and Chaumond, J. and Wolf, T.},
  year={2019}
}


@article{Neural,
  title={Neural Machine Translation by Jointly Learning to Align and Translate},
  author={Dzmitry Bahdanau and Kyunghyun Cho and Yoshua Bengio},
  journal={CoRR},
  year={2014},
  volume={abs/1409.0473}
}


@inproceedings{notnotXP,
    title = "Attention is not not Explanation",
    author = "Wiegreffe, Sarah  and
      Pinter, Yuval",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D19-1002",
    doi = "10.18653/v1/D19-1002",
    pages = "11--20",
}

@inproceedings{notXP,
  title={Attention is not Explanation},
  author={Sarthak Jain and Byron C. Wallace},
  booktitle={North American Chapter of the Association for Computational Linguistics},
  year={2019}
}

%no check que preprint
@article{Quantifying,
  title={Quantifying interpretability and trust in machine learning systems.},
  author={Schmidt, P. and Biessmann, F.},
  year={2019}
}


@InProceedings{Axio,
  title = 	 {Axiomatic Attribution for Deep Networks},
  author =       {Mukund Sundararajan and Ankur Taly and Qiqi Yan},
  booktitle = 	 {Proceedings of the 34th International Conference on Machine Learning},
  pages = 	 {3319--3328},
  year = 	 {2017},
  editor = 	 {Precup, Doina and Teh, Yee Whye},
  volume = 	 {70},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {06--11 Aug},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v70/sundararajan17a/sundararajan17a.pdf},
  url = 	 {https://proceedings.mlr.press/v70/sundararajan17a.html},
}


@InProceedings{Importance,
  title = 	 {Learning Important Features Through Propagating Activation Differences},
  author =       {Avanti Shrikumar and Peyton Greenside and Anshul Kundaje},
  booktitle = 	 {Proceedings of the 34th International Conference on Machine Learning},
  pages = 	 {3145--3153},
  year = 	 {2017},
  editor = 	 {Precup, Doina and Teh, Yee Whye},
  volume = 	 {70},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {06--11 Aug},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v70/shrikumar17a/shrikumar17a.pdf},
  url = 	 {https://proceedings.mlr.press/v70/shrikumar17a.html},
}


@misc{bahdanau_neural_2016,
	title = {Neural {Machine} {Translation} by {Jointly} {Learning} to {Align} and {Translate}},
	url = {http://arxiv.org/abs/1409.0473},
	abstract = {Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder-decoders and consists of an encoder that encodes a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.},
	urldate = {2022-10-18},
	publisher = {arXiv},
	author = {Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
	month = may,
	year = {2016},
	note = {arXiv:1409.0473 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computation and Language, Computer Science - Neural and Evolutionary Computing},
	file = {arXiv Fulltext PDF:C\:\\Users\\milan.bhan\\Zotero\\storage\\7GY87KA8\\Bahdanau et al. - 2016 - Neural Machine Translation by Jointly Learning to .pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\milan.bhan\\Zotero\\storage\\EF9Q4SMQ\\1409.html:text/html},
}

@article{guidotti_counterfactual_2022,
	title = {Counterfactual explanations and how to find them: literature review and benchmarking},
	issn = {1384-5810, 1573-756X},
	shorttitle = {Counterfactual explanations and how to find them},
	url = {https://link.springer.com/10.1007/s10618-022-00831-6},
	doi = {10.1007/s10618-022-00831-6},
	abstract = {Interpretable machine learning aims at unveiling the reasons behind predictions returned by uninterpretable classiﬁers. One of the most valuable types of explanation consists of counterfactuals. A counterfactual explanation reveals what should have been different in an instance to observe a diverse outcome. For instance, a bank customer asks for a loan that is rejected. The counterfactual explanation consists of what should have been different for the customer in order to have the loan accepted. Recently, there has been an explosion of proposals for counterfactual explainers. The aim of this work is to survey the most recent explainers returning counterfactual explanations. We categorize explainers based on the approach adopted to return the counterfactuals, and we label them according to characteristics of the method and properties of the counterfactuals returned. In addition, we visually compare the explanations, and we report quantitative benchmarking assessing minimality, actionability, stability, diversity, discriminative power, and running time. The results make evident that the current state of the art does not provide a counterfactual explainer able to guarantee all these properties simultaneously.},
	language = {en},
	urldate = {2022-05-10},
	journal = {Data Mining and Knowledge Discovery},
	author = {Guidotti, Riccardo},
	month = apr,
	year = {2022},
	keywords = {revue littétaure CF, à lire},
	file = {Guidotti - 2022 - Counterfactual explanations and how to find them .pdf:C\:\\Users\\milan.bhan\\Zotero\\storage\\64NRM685\\Guidotti - 2022 - Counterfactual explanations and how to find them .pdf:application/pdf},
}

@book{molnar_interpretable_2020,
	title = {Interpretable {Machine} {Learning}},
	isbn = {978-0-244-76852-2},
	abstract = {This book is about making machine learning models and their decisions interpretable. After exploring the concepts of interpretability, you will learn about simple, interpretable models such as decision trees, decision rules and linear regression. Later chapters focus on general model-agnostic methods for interpreting black box models like feature importance and accumulated local effects and explaining individual predictions with Shapley values and LIME. All interpretation methods are explained in depth and discussed critically. How do they work under the hood? What are their strengths and weaknesses? How can their outputs be interpreted? This book will enable you to select and correctly apply the interpretation method that is most suitable for your machine learning project.},
	language = {en},
	publisher = {Lulu.com},
	author = {Molnar, Christoph},
	year = {2020},
	note = {Google-Books-ID: jBm3DwAAQBAJ},
}

@article{shapley1953value,
  title={A value for n-person games},
  author={Shapley Ll, S},
  journal={Contributions to the Theory of Games II, Annals of Mathematical Studies},
  volume={28},
  year={1953}
}

@inproceedings{maas_learning_2011,
	address = {Portland, Oregon, USA},
	title = {Learning {Word} {Vectors} for {Sentiment} {Analysis}},
	url = {https://aclanthology.org/P11-1015},
	urldate = {2022-10-18},
	booktitle = {Proceedings of the 49th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}},
	publisher = {Association for Computational Linguistics},
	author = {Maas, Andrew L. and Daly, Raymond E. and Pham, Peter T. and Huang, Dan and Ng, Andrew Y. and Potts, Christopher},
	month = jun,
	year = {2011},
	pages = {142--150},
	file = {Full Text PDF:C\:\\Users\\milan.bhan\\Zotero\\storage\\3JXCP5ZS\\Maas et al. - 2011 - Learning Word Vectors for Sentiment Analysis.pdf:application/pdf},
}

@article{nori2019interpretml,
  title={Interpretml: A unified framework for machine learning interpretability},
  author={Nori, Harsha and Jenkins, Samuel and Koch, Paul and Caruana, Rich},
  journal={arXiv preprint arXiv:1909.09223},
  year={2019}
}

@techreport{devlin_bert_2019,
	title = {{BERT}: {Pre}-training of {Deep} {Bidirectional} {Transformers} for {Language} {Understanding}},
	shorttitle = {{BERT}},
	url = {http://arxiv.org/abs/1810.04805},
	abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5\% (7.7\% point absolute improvement), MultiNLI accuracy to 86.7\% (4.6\% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
	number = {arXiv:1810.04805},
	urldate = {2022-06-15},
	institution = {arXiv},
	author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
	month = may,
	year = {2019},
	note = {arXiv:1810.04805 [cs]
type: article},
	keywords = {Computer Science - Computation and Language, lu, à relire},
	file = {arXiv Fulltext PDF:C\:\\Users\\milan.bhan\\Zotero\\storage\\XEIIFX6Q\\Devlin et al. - 2019 - BERT Pre-training of Deep Bidirectional Transform.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\milan.bhan\\Zotero\\storage\\QEZM74AF\\1810.html:text/html},
}

@article{deep_mind_transfo,
  doi = {10.48550/ARXIV.2207.09238},
  
  url = {https://arxiv.org/abs/2207.09238},
  
  author = {Phuong, Mary and Hutter, Marcus},
  
  keywords = {Machine Learning (cs.LG), Artificial Intelligence (cs.AI), Computation and Language (cs.CL), Neural and Evolutionary Computing (cs.NE), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Formal Algorithms for Transformers},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {Creative Commons Attribution 4.0 International}
}

@article{DBLP:journals/corr/YosinskiCNFL15,
  author    = {Jason Yosinski and
               Jeff Clune and
               Anh Mai Nguyen and
               Thomas J. Fuchs and
               Hod Lipson},
  title     = {Understanding Neural Networks Through Deep Visualization},
  journal   = {CoRR},
  volume    = {abs/1506.06579},
  year      = {2015},
  url       = {http://arxiv.org/abs/1506.06579},
  eprinttype = {arXiv},
  eprint    = {1506.06579},
  timestamp = {Mon, 13 Aug 2018 16:47:32 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/YosinskiCNFL15.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc{sanh_distilbert_2020,
	title = {{DistilBERT}, a distilled version of {BERT}: smaller, faster, cheaper and lighter},
	shorttitle = {{DistilBERT}, a distilled version of {BERT}},
	url = {http://arxiv.org/abs/1910.01108},
	abstract = {As Transfer Learning from large-scale pre-trained models becomes more prevalent in Natural Language Processing (NLP), operating these large models in on-the-edge and/or under constrained computational training or inference budgets remains challenging. In this work, we propose a method to pre-train a smaller general-purpose language representation model, called DistilBERT, which can then be fine-tuned with good performances on a wide range of tasks like its larger counterparts. While most prior work investigated the use of distillation for building task-specific models, we leverage knowledge distillation during the pre-training phase and show that it is possible to reduce the size of a BERT model by 40\%, while retaining 97\% of its language understanding capabilities and being 60\% faster. To leverage the inductive biases learned by larger models during pre-training, we introduce a triple loss combining language modeling, distillation and cosine-distance losses. Our smaller, faster and lighter model is cheaper to pre-train and we demonstrate its capabilities for on-device computations in a proof-of-concept experiment and a comparative on-device study.},
	urldate = {2022-08-17},
	publisher = {arXiv},
	author = {Sanh, Victor and Debut, Lysandre and Chaumond, Julien and Wolf, Thomas},
	month = feb,
	year = {2020},
	note = {arXiv:1910.01108 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:C\:\\Users\\milan.bhan\\Zotero\\storage\\XTIN7WWQ\\Sanh et al. - 2020 - DistilBERT, a distilled version of BERT smaller, .pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\milan.bhan\\Zotero\\storage\\222Q2K9J\\1910.html:text/html},
}

@inproceedings{wang-etal-2018-glue,
    title = "{GLUE}: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding",
    author = "Wang, Alex  and
      Singh, Amanpreet  and
      Michael, Julian  and
      Hill, Felix  and
      Levy, Omer  and
      Bowman, Samuel",
    booktitle = "Proceedings of the 2018 {EMNLP} Workshop {B}lackbox{NLP}: Analyzing and Interpreting Neural Networks for {NLP}",
    month = nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W18-5446",
    doi = "10.18653/v1/W18-5446",
    pages = "353--355",
    abstract = "Human ability to understand language is \textit{general, flexible, and robust}. In contrast, most NLU models above the word level are designed for a specific task and struggle with out-of-domain data. If we aspire to develop models with understanding beyond the detection of superficial correspondences between inputs and outputs, then it is critical to develop a unified model that can execute a range of linguistic tasks across different domains. To facilitate research in this direction, we present the General Language Understanding Evaluation (GLUE, gluebenchmark.com): a benchmark of nine diverse NLU tasks, an auxiliary dataset for probing models for understanding of specific linguistic phenomena, and an online platform for evaluating and comparing models. For some benchmark tasks, training data is plentiful, but for others it is limited or does not match the genre of the test set. GLUE thus favors models that can represent linguistic knowledge in a way that facilitates sample-efficient learning and effective knowledge-transfer across tasks. While none of the datasets in GLUE were created from scratch for the benchmark, four of them feature privately-held test data, which is used to ensure that the benchmark is used fairly. We evaluate baselines that use ELMo (Peters et al., 2018), a powerful transfer learning technique, as well as state-of-the-art sentence representation models. The best models still achieve fairly low absolute scores. Analysis with our diagnostic dataset yields similarly weak performance over all phenomena tested, with some exceptions.",
}

@inproceedings{empirical_xai,
author = {Bell, Andrew and Solano-Kamaiko, Ian and Nov, Oded and Stoyanovich, Julia},
title = {It’s Just Not That Simple: An Empirical Study of the Accuracy-Explainability Trade-off in Machine Learning for Public Policy},
year = {2022},
isbn = {9781450393522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3531146.3533090},
doi = {10.1145/3531146.3533090},
abstract = {To achieve high accuracy in machine learning (ML) systems, practitioners often use complex “black-box” models that are not easily understood by humans. The opacity of such models has resulted in public concerns about their use in high-stakes contexts and given rise to two conflicting arguments about the nature — and even the existence — of the accuracy-explainability trade-off. One side postulates that model accuracy and explainability are inversely related, leading practitioners to use black-box models when high accuracy is important. The other side of this argument holds that the accuracy-explainability trade-off is rarely observed in practice and consequently, that simpler interpretable models should always be preferred. Both sides of the argument operate under the assumption that some types of models, such as low-depth decision trees and linear regression are more explainable, while others such as neural networks and random forests, are inherently opaque. Our main contribution is an empirical quantification of the trade-off between model accuracy and explainability in two real-world policy contexts. We quantify explainability in terms of how well a model is understood by a human-in-the-loop (HITL) using a combination of objectively measurable criteria, such as a human’s ability to anticipate a model’s output or identify the most important feature of a model, and subjective measures, such as a human’s perceived understanding of the model. Our key finding is that explainability is not directly related to whether a model is a black-box or interpretable and is more nuanced than previously thought. We find that black-box models may be as explainable to a HITL as interpretable models and identify two possible reasons: (1) that there are weaknesses in the intrinsic explainability of interpretable models and (2) that more information about a model may confuse users, leading them to perform worse on objectively measurable explainability tasks. In summary, contrary to both positions in the literature, we neither observed a direct trade-off between accuracy and explainability nor found interpretable models to be superior in terms of explainability. It’s just not that simple!},
booktitle = {2022 ACM Conference on Fairness, Accountability, and Transparency},
pages = {248–266},
numpages = {19},
keywords = {explainability, responsible AI, machine learning, public policy},
location = {Seoul, Republic of Korea},
series = {FAccT '22}
}

@inproceedings{vig-2019-multiscale,
    title = "A Multiscale Visualization of Attention in the Transformer Model",
    author = "Vig, Jesse",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: System Demonstrations",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P19-3007",
    doi = "10.18653/v1/P19-3007",
    pages = "37--42",
}

@article{flow_att,
  author    = {Samira Abnar and
               Willem H. Zuidema},
  title     = {Quantifying Attention Flow in Transformers},
  journal   = {CoRR},
  volume    = {abs/2005.00928},
  year      = {2020},
  url       = {https://arxiv.org/abs/2005.00928},
  eprinttype = {arXiv},
  eprint    = {2005.00928},
  timestamp = {Fri, 08 May 2020 15:04:04 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2005-00928.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{serrano-smith-2019-attention,
    title = "Is Attention Interpretable?",
    author = "Serrano, Sofia  and
      Smith, Noah A.",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P19-1282",
    doi = "10.18653/v1/P19-1282",
    pages = "2931--2951",
    abstract = "Attention mechanisms have recently boosted performance on a range of NLP tasks. Because attention layers explicitly weight input components{'} representations, it is also often assumed that attention can be used to identify information that models found important (e.g., specific contextualized word tokens). We test whether that assumption holds by manipulating attention weights in already-trained text classification models and analyzing the resulting differences in their predictions. While we observe some ways in which higher attention weights correlate with greater impact on model predictions, we also find many ways in which this does not hold, i.e., where gradient-based rankings of attention weights better predict their effects than their magnitudes. We conclude that while attention noisily predicts input components{'} overall importance to a model, it is by no means a fail-safe indicator.",
}

@article{Vashishth2019AttentionIA,
  title={Attention Interpretability Across NLP Tasks},
  author={Shikhar Vashishth and Shyam Upadhyay and Gaurav Singh Tomar and Manaal Faruqui},
  journal={ArXiv},
  year={2019},
  volume={abs/1909.11218}
}

@inproceedings{bastings-filippova-2020-elephant,
    title = "The elephant in the interpretability room: Why use attention as explanation when we have saliency methods?",
    author = "Bastings, Jasmijn  and
      Filippova, Katja",
    booktitle = "Proceedings of the Third BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.blackboxnlp-1.14",
    doi = "10.18653/v1/2020.blackboxnlp-1.14",
    pages = "149--155",
    abstract = "There is a recent surge of interest in using attention as explanation of model predictions, with mixed evidence on whether attention can be used as such. While attention conveniently gives us one weight per input token and is easily extracted, it is often unclear toward what goal it is used as explanation. We find that often that goal, whether explicitly stated or not, is to find out what input tokens are the most relevant to a prediction, and that the implied user for the explanation is a model developer. For this goal and user, we argue that input saliency methods are better suited, and that there are no compelling reasons to use attention, despite the coincidence that it provides a weight for each input. With this position paper, we hope to shift some of the recent focus on attention to saliency methods, and for authors to clearly state the goal and user for their explanations.",
}

@article{DBLP:journals/corr/abs-2012-09838,
  author    = {Hila Chefer and
               Shir Gur and
               Lior Wolf},
  title     = {Transformer Interpretability Beyond Attention Visualization},
  journal   = {CoRR},
  volume    = {abs/2012.09838},
  year      = {2020},
  url       = {https://arxiv.org/abs/2012.09838},
  eprinttype = {arXiv},
  eprint    = {2012.09838},
  timestamp = {Sun, 03 Jan 2021 18:46:06 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2012-09838.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc{been_kim_rigourous,
  doi = {10.48550/ARXIV.1702.08608},
  
  url = {https://arxiv.org/abs/1702.08608},
  
  author = {Doshi-Velez, Finale and Kim, Been},
  
  keywords = {Machine Learning (stat.ML), Artificial Intelligence (cs.AI), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Towards A Rigorous Science of Interpretable Machine Learning},
  
  publisher = {arXiv},
  
  year = {2017},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@inproceedings{poursabzi2021manipulating,
  title={Manipulating and measuring model interpretability},
  author={Poursabzi-Sangdeh, Forough and Goldstein, Daniel G and Hofman, Jake M and Wortman Vaughan, Jennifer Wortman and Wallach, Hanna},
  booktitle={Proceedings of the 2021 CHI conference on human factors in computing systems},
  pages={1--52},
  year={2021}
}

Arrieta, A. B., Díaz-Rodríguez, N., Del Ser, J., Bennetot, A., Tabik, S., Barbado, A., ... & Herrera, F. (2020). Explainable Artificial Intelligence (XAI): Concepts, taxonomies, opportunities and challenges toward responsible AI. Information fusion, 58, 82-115.




