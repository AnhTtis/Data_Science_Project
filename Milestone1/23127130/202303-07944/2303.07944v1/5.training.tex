\section{Training Details}

\input{tables/within-dataset}

\subsection{Data Preprocessing}
To prepare the video clips for the spatiotemporal deep learning models, we first extract 68 face landmarks with OpenFace~\cite{Baltrusaitis2018}. We then define a bounding box in each frame with the minimum and maximum $(x,y)$ locations by extending the crop horizontally by 5\% to ensure that the cheeks and jaw are present. The top and bottom are extended by 30\%  and 5\% of the bounding box height, respectively, to include the forehead and jaw. We further extend the shorter of the two axes to the length of the other to form a square. The cropped frames are then resized to 64$\times$64 pixels with bicubic interpolation. For faster processing of the massive CelebV-HQ~\cite{zhu2022celebvhq} dataset, we instead use MediaPipe Face Mesh~\cite{lugaresi2019mediapipe} for landmarking.


\subsection{Model Architectures}
We use a 3D-CNN architecture similar to \cite{Speth_CVIU_2021} without temporal dilations, which was originally inspired by \cite{Yu2019}. We use a temporal kernel width of 5, and replace default zero-padding by repeating the edges. Zero-padding along the time dimension can result in edge effects that add artificial frequencies to the predictions. Early experiments showed that temporal dilations caused aliasing and reduced the bandwidth of the model to specific frequencies. Our losses and framework may be applied to any task and architecture with dense predictions along one or more dimensions. However, popular rPPG architectures such as DeepPhys~\cite{Chen2018} and  MTTS-CAN~\cite{Liu_MTTS_2020} may be ill-suited for the approach, since they consume very few frames, and the number of time points should be large enough to give sufficient frequency resolution with the FFT. In our experiments, we use the AdamW~\cite{Loshchilov2019AdamW} optimizer with a learning rate of 0.0001. We use a clip length of $T=120$ frames (4 seconds), and we set the input signalâ€™s length to achieve a frequency resolution of $0.\overline{33}$ bpm.

\subsection{Supervised Training}
To properly compare our approach to its supervised counterpart we use the same model architecture and train it with the commonly used negative Pearson loss between the predicted waveform and the contact sensor ground truth~\cite{Yu2019}. During training we apply all of the same augmentations except time reversal. Models are trained for 200 epochs on PURE and UBFC-rPPG, and for 40 epochs on DDPM. The model from the epoch with the lowest loss on the validation set is selected for testing.

\subsection{Unsupervised Training}
Unsupervised models are trained for the same number of epochs as the supervised setting for both PURE and UBFC-rPPG, but we train for an additional 40 epochs on DDPM, since this dataset is considerably more difficult. We set the batch size to 20 samples during training. Contrary to previous unsupervised approaches~\cite{Gideon_2021_ICCV,Sun_2022_ECCV}, we leverage validation sets for model selection by selecting the model with the lowest combined bandwidth and sparsity losses. The creation of the dataset splits is described in the next section.

\subsection{Evaluation}
Pulse rates are computed as the highest spectral peak between 0.66 Hz and 3 Hz (equivalent to 40 bpm to 180 bpm) over a 10-second sliding window. The same procedure is applied to the ground truth waveforms for a reliable evaluation~\cite{Mironenko2020}. We apply common error metrics such as mean absolute error (MAE), root mean square error (RMSE), and Pearson correlation coefficient ($r$) between the pulse rates.

We perform 5-fold cross validation for both PURE and UBFC with the same folds as \cite{Gideon_2021_ICCV}, and use the predefined dataset splits from DDPM~\cite{Speth_CVIU_2021}. Differently from \cite{Gideon_2021_ICCV}, we use 3 of the folds for training, 1 for validation, and the remaining for testing rather than only training and testing partitions. We train 3 models with different initializations, resulting in 15 models trained on PURE and UBFC each, and 3 models trained on DDPM. We present the mean and standard deviation of the errors in the results.