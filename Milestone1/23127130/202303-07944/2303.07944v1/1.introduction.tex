\section{Introduction}
Camera-based vitals estimation is a rapidly growing field enabling non-contact health monitoring in a variety of settings~\cite{McDuff2022}.
Although many of the signals avoid detection from the human eye, video data in the visible and infrared ranges contain subtle intensity changes caused by physiological oscillations such as blood volume and respiration.
Significant remote photoplethysmography (rPPG) research for estimating the cardiac pulse has leveraged supervised deep learning for robust signal extraction~\cite{Chen2018,Yu2019,Niu2020,Liu_MTTS_2020,Speth_CVIU_2021,Yu_2022_CVPR}.
While the number of successful approaches has rapidly increased, the size of benchmark video datasets with simultaneous vitals recordings has remained relatively stagnant.

Robust deep learning-based systems for deployment require training on larger volumes of video data with diverse skin tones, lighting, camera sensors, and movement.
However, collecting simultaneous video and physiological ground truth with contact-PPG or electrocardiograms (ECG) is challenging for several reasons.
First, many hours of high quality videos is an unwieldy volume of data. Second, recording a diverse subject population in conditions representative of real-world activities is difficult to conduct in the lab setting. Finally, synchronizing contact measurements with video is technically challenging, and even contact measurements used for ground truth contain noise.

\definecolor{myorange}{RGB}{220, 112, 3}
% AC ->
\definecolor{mygreen}{RGB}{47, 154, 47}
\definecolor{myblue}{RGB}{115, 60, 160}
% <- AC

\begin{figure*}
    \centering
    \includegraphics[width=\linewidth]{figures/teaser.pdf}
    \caption{Overview of the \ourapproach framework for rPPG compared with traditional supervised and unsupervised learning. Supervised and contrastive losses use distance metrics to the ground truth or other samples. Our framework applies the loss directly to the prediction by shaping the frequency spectrum, and encouraging \textcolor{myblue}{{\bf variance over a batch}} of inputs. Power outside of the \textcolor{myorange}{{\bf bandlimits}} is penalized to learn invariances to irrelevant frequencies. Power within the bandlimits is encouraged to be \textcolor{mygreen}{{\bf sparsely}} distributed near the peak frequency.}
    \label{fig:framework}
\end{figure*}


Fortunately, recent works find that contrastive unsupervised learning for rPPG is a promising solution to the data scarcity problem~\cite{Gideon_2021_ICCV,Sun_2022_ECCV,Wang_SSL_2022,Yuzhe_SimPer_2022}.
We extend this research line into {\it non-contrastive} unsupervised learning to discover periodic signals in video data.
With end-to-end unsupervised learning, collecting more representative training data to learn powerful visual features is much simpler, since only video is required without associated medical information.

In this work, we show that non-contrastive unsupervised learning is especially simple when regressing rPPG signals.
We find weak assumptions of periodicity are sufficient for learning the minuscule visual features corresponding to the blood volume pulse from unlabelled face videos.
The loss functions can be computed in the frequency domain over batches without the need for pairwise or triplet comparisons.
Figure \ref{fig:framework} compares the proposed approach with supervised and contrastive unsupervised learning approaches.

This work creates opportunities for scaling deep learning models for camera-based vitals and estimating periodic or quasi-periodic signals from unlabelled data beyond rPPG.
Our {\bf novel contributions} are:
\begin{enumerate}
    \item A general framework for physiological {\bf si}gnal estimation via {\bf n}on-{\bf c}ontrastive unsupervised learning (SiNC) by leveraging periodic signal priors.
    \item The first {\bf non-contrastive} unsupervised learning method for camera-based vitals measurement.
    \item The first experiments and results of training with {\bf non-rPPG-specific video data} without ground truth vitals.
\end{enumerate}

Source code to replicate this work is available at \url{https://github.com/CVRL/SiNC-rPPG}.