{
    "arxiv_id": "2303.07944",
    "paper_title": "Non-Contrastive Unsupervised Learning of Physiological Signals from Video",
    "authors": [
        "Jeremy Speth",
        "Nathan Vance",
        "Patrick Flynn",
        "Adam Czajka"
    ],
    "submission_date": "2023-03-14",
    "revised_dates": [],
    "latest_version": 1,
    "categories": [
        "cs.CV"
    ],
    "abstract": "Subtle periodic signals such as blood volume pulse and respiration can be\nextracted from RGB video, enabling remote health monitoring at low cost.\nAdvancements in remote pulse estimation -- or remote photoplethysmography\n(rPPG) -- are currently driven by deep learning solutions. However, modern\napproaches are trained and evaluated on benchmark datasets with associated\nground truth from contact-PPG sensors. We present the first non-contrastive\nunsupervised learning framework for signal regression to break free from the\nconstraints of labelled video data. With minimal assumptions of periodicity and\nfinite bandwidth, our approach is capable of discovering the blood volume pulse\ndirectly from unlabelled videos. We find that encouraging sparse power spectra\nwithin normal physiological bandlimits and variance over batches of power\nspectra is sufficient for learning visual features of periodic signals. We\nperform the first experiments utilizing unlabelled video data not specifically\ncreated for rPPG to train robust pulse rate estimators. Given the limited\ninductive biases and impressive empirical results, the approach is\ntheoretically capable of discovering other periodic signals from video,\nenabling multiple physiological measurements without the need for ground truth\nsignals. Codes to fully reproduce the experiments are made available along with\nthe paper.",
    "pdf_urls": [
        "http://arxiv.org/pdf/2303.07944v1"
    ],
    "publication_venue": "Accepted to CVPR 2023"
}