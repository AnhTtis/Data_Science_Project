\section{Experiments}
\subsection{Benchmark Setting}
\label{5.1}
\noindent\textbf{Evaluation Metrics.\ } To provide a comprehensive and effective evaluation, we compare serval advanced works in the affordance area \cite{deng20213d, nagarajan2019grounded, zhai2022one} and finally chose four evaluation metrics: \textbf{AUC} \cite{lobo2008auc}, \textbf{aIOU} \cite{rahman2016optimizing}, \textbf{SIM}ilarity \cite{swain1991color} and \textbf{M}ean \textbf{A}bsolute \textbf{E}rror \cite{willmott2005advantages} to benchmark the PIAD.

\noindent\textbf{Modular Baselines.\ } Since there are no prior works using paired image-point cloud data to ground 3D object affordance. For a thorough comparison of our method, we select several advanced image-point cloud cross-modal works as modular baselines. Methods of comparison are broadly divided into two types, one is that utilizes camera intrinsics to facilitate cross-modal feature alignment or fusion, \ie \textbf{\textcolor[rgb]{0.2,0.8,0.1}{\texttt{MBDF-Net(MBDF)}}} \cite{tan2021mbdf}, \textbf{\textcolor[rgb]{0.2,0.8,0.1}{\texttt{PMF}}} \cite{zhuang2021perception} and \textbf{\textcolor[rgb]{0.2,0.8,0.1}{\texttt{FusionRCNN(FRCNN)}}} \cite{xu2022fusionrcnn}, for this type of method, we remove the step of using intrinsic parameters to align raw data or features to explore their effectiveness on PIAD. Another type performs feature alignment directly in the feature space, without relying on camera intrinsic, \ie \textbf{\textcolor[rgb]{0.99,0.5,0.0}{\texttt{ImLoveNet(ILN)}}} \cite{chen2022imlovenet}, \textbf{\textcolor[rgb]{0.99,0.5,0.0}{\texttt{PointFusion(PFusion)}}} \cite{xu2018pointfusion} and \textbf{\textcolor[rgb]{0.99,0.5,0.0}{\texttt{XMFnet(XMF)}}} \cite{aiello2022cross}. To ensure a fair comparison, all methods use the same feature extractors, with the only variation coming from cross-modal alignment or fusion block. For the \textbf{\texttt{Baseline}}, we directly concatenate the features that are output from extractors, regarding the concatenation as the cross-fusion block. More details about baselines are provided in the supplementary materials.

\begin{figure}[t]
    \centering
    \footnotesize
    \begin{overpic}[width=0.96\linewidth]{figs/only_point_compare.pdf}
    \put(11,0){\textbf{(a)}}
    \put(10,49){\textbf{GT}}
    \put(76,0){\textbf{(c)}}
    \put(75,49){\textbf{Ours}}
    \put(36.5,0){\textbf{(b)}}
    \put(36,49){\textbf{\cite{deng20213d}}}
    \put(-1,33){\rotatebox{90}{\textbf{Move}}}
    \put(-1,9){\rotatebox{90}{\textbf{Open}}}
    \end{overpic}
    \caption{\textbf{Paradigm Comparison.} \textbf{(a)} Ground truth. \textbf{(b)} The results of 3D-AffordanceNet \cite{deng20213d}. \textbf{(c)} Our results. The top row shows the regional confusion case in \textbf{\texttt{Seen}}, and the bottom row displays the generalization in \textbf{\texttt{Unseen}}.}
    \label{fig:only_point}
\end{figure}

\begin{figure*}[t]
	\centering
	\small
        \begin{overpic}[width=0.9\linewidth]{figs/main_results.pdf}
        \put(23,-0.5){{\textbf{\texttt{Seen}}}}
        \put(74,-0.5){{\textbf{\texttt{Unseen}}}}
        
        \put(4.4,65.5){{\textbf{Listen}}}
        \put(16.5,65.5){{\textbf{Open}}}
        \put(30,65.5){{\textbf{Sit}}}
        \put(41.5,65.5){{\textbf{Lift}}}
        \put(56.5,65.5){{\textbf{Lay}}}
        \put(67.6,65.5){{\textbf{Grasp}}}
        \put(77.5,65.5){{\textbf{Wrapgrasp}}}
        \put(90.8,65.5){{\textbf{Contain}}}
        
        \put(-1,6.2){\rotatebox{90}{\textbf{\texttt{GT}}}}
        \put(-1,15){\rotatebox{90}{\textbf{\texttt{Ours}}}}
        \put(-1,24.5){\rotatebox{90}{\textcolor[rgb]{0.99,0.5,0.0}{\textbf{\texttt{XMF}}} \cite{aiello2022cross}}}
        \put(-1,33.5){\rotatebox{90}{\textcolor[rgb]{0.2,0.8,0.1}{\textbf{\texttt{FRCNN}}} \cite{xu2022fusionrcnn}}}
        \put(-1,46.3){\rotatebox{90}{\textbf{\texttt{Base.}}}}
	\end{overpic}
	\caption{\textbf{Visualization Results.} The first row is the interactive image, which demonstrates the interaction that the object can afford. The last row is the ground truth of 3D object affordance in the point cloud. The four columns on the left are the visual comparison results for distinct 3D object affordances in the \textbf{\texttt{Seen}} partition. The four columns on the right are the results in the \textbf{\texttt{Unseen}} partition.}
 \label{Fig:mainresults}
\vspace{1pt}
\end{figure*}

\subsection{Comparison Results}
The comparison results of evaluation metrics are shown in Tab. \ref{table:results}. As can be seen, our method outperforms the compared baselines, across all metrics in both partitions. Besides, to display the limitation of methods that lock geometrics with specific semantic affordance categories, we conduct an experiment to compare one of these methods \cite{deng20213d} with ours. As shown in Fig. \ref{fig:only_point}, the top row indicates that the result of this method exhibits regional confusion, \eg the region where the chair could be moved or sat is geometrically rectangular, and it directly anticipates results on these similar geometries, inconsistent with affordance property. Plus, the bottom row shows that directly establishing a link between object structures and affordances may fail to anticipate correct 3D affordance in \textbf{\texttt{Unseen}}. In contrast, our method anticipates precise results by mining affordance clues provided by 2D interactions. Additionally, visual comparative results of our method and other baselines are shown in Fig. \ref{Fig:mainresults}. As can be seen, the comparative baselines could anticipate some 3D object affordance under our setting, but in comparison, our method obviously achieves better results, which validates the rationality of our setting, and also demonstrates the superiority of our method.

\begin{table}
\centering
\small
\renewcommand{\arraystretch}{1.}
\renewcommand{\tabcolsep}{7.5pt}
\caption{\textbf{Ablation Study.} We investigate the improvement of JRA and ARM on the model performance based on the baseline.
}
\label{table:ablation}
\vspace{5pt}
\begin{tabular}{c|cc|cccc}
\toprule
\multicolumn{1}{l|}{} & \textbf{JRA} & \textbf{ARM} &\textbf{AUC} & \textbf{aIOU} & \textbf{SIM} & \textbf{MAE} \\ 
\midrule
\multirow{4}{*}{\rotatebox{90}{\textbf{\texttt{Seen}}}} &  &  &  69.92 & 8.85 & 0.427 & 0.132 \\
& \checkmark &  &  80.29 & 14.31 & 0.495 & 0.121 \\
 & & \checkmark & 78.67 & 13.95 & 0.475 & 0.126 \\
 & \checkmark & \checkmark & \cellcolor{mygray}\textbf{85.16} & \cellcolor{mygray}\textbf{21.20} & \cellcolor{mygray}\textbf{0.564} & \cellcolor{mygray}\textbf{0.088} \\ 
 \midrule
\multirow{4}{*}{\rotatebox{90}{\textbf{\texttt{Unseen}}}} & &  & 59.14 & 4.05 & 0.338 & 0.202 \\
& \checkmark &  & 66.25 & 6.27 & 0.363 & 0.159 \\
 & & \checkmark & 65.79 & 5.99 & 0.358 & 0.162 \\
 & \checkmark & \checkmark & \cellcolor{mygray}\textbf{73.69} & \cellcolor{mygray}\textbf{8.70} & \cellcolor{mygray}\textbf{0.383} & \cellcolor{mygray}\textbf{0.117} \\
 \bottomrule
\end{tabular}
\end{table}

\begin{figure}
    \centering
    \footnotesize
    \begin{overpic}[width=0.9\linewidth]{figs/JRA.pdf}
    \put(5,1){\textbf{Image Regions}}
    \put(-0.5,9){\textbf{\rotatebox{90}{Point Regions}}}
    \put(81,3){\textbf{Affordance}}
    \put(84,-0.5){\textbf{Regions}}
    \put(-0.5,44){\textbf{\rotatebox{90}{Open}}}
    \put(28.5,61){\textbf{$\bm{w/o}$ JRA}}
    \put(48.5,61){\textbf{$\bm{w}$ JRA}}
    \put(65,61){\textbf{$\bm{w/o}$ JRA}}
    \put(85,61){\textbf{$\bm{w}$ JRA}}
    \put(43,32){\textbf{step=8000}}
    \put(10,32){\textbf{step=1000}}
    \put(77,32){\textbf{step=15000}}
    % \put(-1,8){\textbf{\rotatebox{90}{Move}}}
    \end{overpic}
    \caption{\textbf{Ablation of JRA.} The top row is the result of the anticipated affordance with/without JRA. The bottom row is a part of the cross-similarity matrix of a sample during the training process.}
    \label{fig:kl}
\end{figure}

\begin{figure}
    \centering
    \footnotesize
    \begin{overpic}[width=0.87\linewidth]{figs/heatmap.pdf}
    \put(20,70){\textbf{$\bm{w/o}$ ARM}}
    \put(65,70){\textbf{$\bm{w}$ ARM}}
    % \put(20,-1.5){\textbf{$\bm{w/o}$ ARM}}
    % \put(65,-1.5){\textbf{$\bm{w}$ ARM}}
    \put(-0.5,50){\rotatebox{90}{\textbf{\texttt{Seen}}}}
    \put(-0.5,10){\rotatebox{90}{\textbf{\texttt{Unseen}}}}
    \end{overpic}
    \caption{\textbf{Ablation of ARM.} The activation map and t-SNE \cite{van2008visualizing} results with ($\bm{w}$), without ($\bm{w/o}$) ARM in both partitions.}
    \label{fig:heatmap}
\end{figure}

\begin{figure}
    \centering
    \small
    \begin{overpic}[width=0.93\linewidth]{figs/one2multi1.pdf}
    % \put(7.5,-0.8){\textbf{Image}}
    \put(39,-0.8){\textbf{(a)}}
    \put(68.5,-0.8){\textbf{(b)}}
    \put(88.5,-0.8){\textbf{(c)}}
    \put(-1,6){\textbf{\rotatebox{90}{Grasp}}}
    \put(-1,24){\textbf{\rotatebox{90}{Contain}}}
    \end{overpic}
    \caption{\textbf{Same Image} $w.r.t.$ \textbf{Multiple Point Clouds.} \textbf{(a)} Same object category. \textbf{(b)} Different object categories, similar geometrics. \textbf{(c)} Different object categories and geometrics.}
    \label{fig:one2multi1}
\end{figure}

\begin{figure}
    \centering
    \small
    \begin{overpic}[width=0.93\linewidth]{figs/one2multi2.pdf}
    \put(20,-1){\textbf{Listen}}
    \put(70,-1){\textbf{Grasp}}
    \end{overpic}
    \caption{\textbf{Same Point Cloud} $w.r.t.$ \textbf{Multiple Images.} Grounding affordance on the same point cloud with images that contain similar or disparate interactions.}
    \label{fig:one2multi2}
\end{figure}

\subsection{Ablation Study}
\label{5.3}
\noindent\textbf{Effectiveness of JRA. } Tab. \ref{table:ablation} reports the impact on
evaluation metrics of JRA. Fig. \ref{fig:kl} provides visual comparison results. It shows that without the JRA, the result is anticipated over the entire region that contains the interactive components, which means the 2D-3D affordance regions of objects do not match well. Besides, we visualize a part of the cross-similarity matrix between $\hat{\mathbf{F}}_{p}$ and $\hat{\mathbf{F}}_{i}$ in Fig. \ref{fig:kl}. In the initial stage, only a few analogous shape regions keep explicit correspondence, as training proceeds, the JRA maps the correlation among regions with lower similarity. Meanwhile, the model extracts explicit affordance progressively, and the affordance introduced into the optimization process reveals the corresponding affordance regions.

\noindent\textbf{Effectiveness of ARM. } The influence of ARM on evaluation metrics is also shown in Tab. \ref{table:ablation}. To visually evaluate the effectiveness of ARM, we employ GradCAM \cite{selvaraju2017grad} to generate the activation map, and utilize t-SNE \cite{van2008visualizing} to demonstrate the clustering of affordances in both partitions. The results are shown in Fig. \ref{fig:heatmap}. It shows that ARM makes the model focus on the interactive regions to excavate the interaction contexts, and enables the model to differentiate various affordances from interactions in both partitions.

\subsection{Performance Analysis}
\label{6.1}
\noindent\textbf{Different Instances.} We conduct experiments to verify whether the model could ground 3D affordance on instances from different sources: (\romannumeral1) using an image and different point clouds to infer respectively (Fig. \ref{fig:one2multi1} (a)). (\romannumeral2) using multiple images and a single point cloud (Fig. \ref{fig:one2multi2}). The results indicate that the model can anticipate 3D affordances on different instances with the same 2D interaction, and can also anticipate distinct 3D affordances on the same point cloud through different 2D interactions. Showing that the model maintains the mapping between object structures and affordances by learning from different 2D interactions.

\noindent\textbf{Different Object Categories.} What will happen if the object category is different in the image and point cloud? To explore this issue, we perform experiments with mismatched object categories, shown in Fig. \ref{fig:one2multi1} (b) and (c). When objects are of different categories but with similar geometric primitives, \eg the handle of ``Mug'' and the shoulder strap of ``Bag'', our model still properly infer the 3D affordance. However, when geometric structures are also dissimilar (Fig. \ref{fig:one2multi1} (c)), the model does not make random predictions. This indicates that the model maps cross-category invariance between affordance and geometries, which can be generalized to new instances of geometry.

\begin{figure}
    \centering
    \footnotesize
    \begin{overpic}[width=0.9\linewidth]{figs/multi_activation.pdf}
    % \put(7,25.5){\textbf{Grasp}}
    \put(-3.5,7){\textbf{\rotatebox{90}{Grasp}}}
    \put(99.5,7){\textbf{\rotatebox{90}{Open}}}
    % \put(83,25.5){\textbf{Open}}
    \end{overpic}
    \caption{\textbf{Multiple Affordances.} Some objects like ``Bag" contains the region that corresponds to multiple affordances.}
    \label{fig:multi_property}
\end{figure}

\noindent\textbf{Multiplicity. } One defining property of affordance is multiplicity: some points may correspond to multiple affordances. Fig. \ref{fig:multi_property} shows that the model makes objects' multiple affordances compatible, and verifies the model does not anticipate affordance by linking a region with specific affordances. Instead, it predicts by considering the presence of affordance-related interactions in the same region.

\noindent\textbf{Real World. } To validate the model in real-world scenarios, we use an iPhone to scan objects and real-scan dataset \cite{Liu_2022_CVPR} to test our model, shown in Fig. \ref{fig:real}. It shows that our model exhibits a certain degree of generalization to the real world.
\begin{comment}
the final anticipation depends on the interaction content \eg ``Grasp'' or ``Open''.
\end{comment}

\begin{figure}
	\centering
        \footnotesize
		\begin{overpic}[width=0.95\linewidth]{figs/real_app.pdf}
		\put(6,39){\textbf{Image}}
		\put(23,39){\textbf{Scan}}
		\put(36,39){\textbf{Predict}}
  		\put(54,39){\textbf{Image}}
		\put(70.5,39){\textbf{Scan}}
		\put(84,39){\textbf{Predict}}
		\put(-2,27.5){\rotatebox{90}{\textbf{Sit}}}
  		\put(-2,0.6){\rotatebox{90}{\textbf{Wrapgrasp}}}
  		\put(46.8,7.3){\rotatebox{90}{\textbf{Cut}}}

	\end{overpic}
	\caption{\textbf{Real-Wrold.} The first row is scanned by iPhone, the second row comes from \cite{Liu_2022_CVPR}, point clouds are sampled from them.}
 \label{fig:real}
\end{figure}


\begin{figure}[t]
    \centering
    \footnotesize
    \begin{overpic}[width=0.96\linewidth]{figs/limtation.pdf}
    \put(8,23.5){\textbf{Open}}
    \put(31,23.5){\textbf{GT}}
    \put(46,23.5){\textbf{Ours}}
    \put(70,23.5){\textbf{GT}}
    \put(86,23.5){\textbf{Ours}}
    \end{overpic}
    \caption{\textbf{Failure Cases.}  Over prediction in the small regions.}
    \label{fig:limitation}
\end{figure}


\noindent\textbf{Limitations.} Our model exhibits over predictions for some small affordance regions, shown in Fig. \ref{fig:limitation}. This may be due to the limited understanding of fine-grained geometric parts, and aligning the object features in a single scale may lead to overly large receptive fields. Our feature extension will refer \cite{Geng_2023_CVPR, lin2017feature} to tackle this problem.