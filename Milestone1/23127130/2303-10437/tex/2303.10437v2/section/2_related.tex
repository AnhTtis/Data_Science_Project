\section{Related Works}
%-------------------------------------------------------------------------
\label{sec:relation work}
\subsection{Affordance Learning}
\par At present, the affordance area has made achievements in multiple tasks (Tab. \ref{tab:related}). Some works are devoted to detecting the affordance region from 2D sources \ie images and videos \cite{chuang2018learning, do2018affordancenet, Li2023G2L, luo2021one, roy2016multi, thermos2020deep, zhao2020object}, and there are also some studies accomplish this task with the assistance of natural language \cite{lu2022phrase, mi2020intention, mi2019object}. They seek to detect or segment objects that afford the action in 2D data, but cannot perceive the specific parts corresponding to the affordance. Thus, another type of method brings a leap to locate the affordance region of objects from 2D sources \cite{fang2018demo2vec, luo2022learning, nagarajan2019grounded, luo2022grounded}. However, the affordance knowledge derived from 2D sources is hard to extrapolate specific interactive locations of objects in the 3D environment. With several 3D object datasets proposed \cite{Geng_2023_CVPR, Liu_2022_CVPR, Mo_2019_CVPR}, some researchers explore grounding object affordance from 3D data \cite{deng20213d, mo2022o2o, xu2022partafford, nagarajan2020learning}. Such methods directly map the connection between semantic affordances and 3D structures, detach the real interaction, and may deter the generalization ability: structures that do not map to a specific affordance are usually hard to generalize through this type of method. T. Nagarajan \etal \cite{nagarajan2020learning} give a fresh mind, taking the reinforcement learning to make agents actively interact with the 3D synthetic scenarios, while it requires repeated attempts by the agents in a given search space, and is time-consuming. In robotics, affordance is utilized to provide priors for object manipulation and achieve considerable results, especially the articulated 3D objects \cite{mo2021where2act,wang2022adaafford, zhao2022dualafford}. Several methods utilize the 2.5D data \eg RGB-D image to understand the object affordance and serve the manipulation task \cite{koppula2013learning, koppula2014physically, koppula2015anticipating, nguyen2016detecting}, for these methods, the image and depth information are corresponding in spatial and need to be collected in the same scene. In contrast, our task focus on grounding 3D object affordance from 2D interactions, in which the 2D interactions provide direct clues to excavate object affordance efficiently and make the affordance could generalize to some unseen structures, and the 2D-3D data is collected from different sources, freeing the constraints on spatial correspondence. 

\subsection{Image-Point Cloud Cross-Modal Learning}
\par The combination of point cloud and image data can capture both semantic and geometric information, enabling cross-modal learning among them has great potential application value in scenarios like autopilot \cite{cui2021deep}. For this type of work, aligning features is the premise for completing downstream tasks \eg detection, segmentation, and registration. It is aimed at establishing correspondences between instances from different modalities \cite{baltruvsaitis2018multimodal, Li2023U, yang2023implicit}, either spatial or semantic. To achieve this, many methods use camera intrinsics to correspond spatial position of pixels and points, then align per pixel-point feature or fuse the raw data \cite{krispel2020fuseseg, tan2021mbdf, vora2020pointpainting,xu2022fusionrcnn, zhao2021lif, zhuang2021perception}. Some works utilize depth information to project image features into 3D space and then fuse them with point-wise features \cite{jaritz2019multi, mithun2020rgb2lidar, yin2021multimodal, zhang2020fusion, zhao2021similarity}. The above methods rely on the spatial prior information to align features, while in our task, images and point clouds are collected from distinct physical instances, there are no priors on camera pose or intrinsics, also no corresponding depth information of the image. Therefore, we align the object region features that are derived from different sources in the feature space with the assistance of the correlation invariance between affordances and appearance structures.

\begin{table}[t]
\centering
\small
  \renewcommand{\arraystretch}{1.}
  \renewcommand{\tabcolsep}{4.pt}
\caption{\textbf{Affordance Learning.} Various works for several tasks in the affordance community.
}
\label{tab:related}
\vspace{5pt}
\begin{tabular}{c|c|c|c}
\toprule
\textbf{Work} & \textbf{Input}             & \textbf{Output}             & \textbf{Task}         \\ \midrule
 \cite{do2018affordancenet, thermos2020deep, zhao2020object}            & Image/Video          & 2D Mask         & Detection    \\ 
\cite{lu2022phrase, mi2020intention, mi2019object}             & Image,Text          & 2D Mask         & Detection    \\ 
\cite{koppula2013learning, koppula2015anticipating, nguyen2016detecting}             & RGBD             & Heatmap \& Action        &  Manipulation\\
\cite{mo2021where2act,wang2022adaafford, zhao2022dualafford}    & Point Cloud & Heatmap \& Action & Manipulation    \\
\cite{fang2018demo2vec, luo2022learning, nagarajan2019grounded}             & Image/Video       & 2D Heatmap         & Grounding    \\
\cite{deng20213d, mo2022o2o, xu2022partafford}     & Point Cloud       & 3D Heatmap & Grounding    \\
\bottomrule
\end{tabular}
\end{table}
%---------------------------------------------------------