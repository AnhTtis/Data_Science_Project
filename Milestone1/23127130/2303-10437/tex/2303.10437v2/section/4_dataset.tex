\section{Dataset}
\label{sec:dataset}
\par \textbf{Collection Details.} We collect \textbf{P}oint-\textbf{I}mage \textbf{A}ffordance \textbf{D}ataset (\textbf{PIAD}), which contains paired image-point cloud affordance data. The point clouds are mainly collected from 3D-AffordanceNet \cite{deng20213d}, including the point cloud coordinates and affordance annotation. Images are mainly collected from HICO \cite{chao2015hico}, AGD20K \cite{luo2022learning}, and websites with free licenses. The collection criteria is that the image should demonstrate interactions that the object in the point cloud could afford. For example, if the object point cloud is a ``Chair'', it affords ``Sit'', then the image should depict a subject (usually humans) sitting on a chair. The final dataset comprises 7012 point clouds and 5162 images, spanning 23 object classes and 17 affordance categories. Notably, objects in the images and point clouds do not sample from the same physical instance, but they belong to the same object category. Paired examples are shown in Fig. \ref{Fig:dataset} (a).

\par \textbf{Annotation Details.} For point clouds, each point may afford one or multiple interactions, \eg one annotation is a matrix of (2048, 17), $2048$ is the point number, $17$ represents the number of affordance types. Each element in the matrix indicates the probability of a point affording a specific affordance. Meanwhile, images are annotated with bounding boxes of the interactive subject and object, as well as an affordance category label. In our task, we only use the heatmap in the point cloud annotation that corresponds to the affordance of the image for training, resulting in a matrix of (2048, 1). By doing so, the affordance category is detached from the point cloud during inference, and the anticipation of the 3D object affordance category only relies on 2D interactions. As a result, different affordances can be anticipated on a point cloud through distinct interactions.

\par \textbf{Statistic Analysis. } Since images and point clouds are sampled from different instances, they do not need a fixed one-to-one pairing, one image could be paired with multiple point clouds and the count of them is not strictly consistent. Fig. \ref{Fig:dataset} (b) and (c) show the count and distribution of affordances in images and point clouds. Fig. \ref{Fig:dataset} (d) illustrates the ratio of images and point clouds in each affordance category. PIAD has two partitions: \textbf{\texttt{Seen}} and \textbf{\texttt{Unseen}}. In \textbf{\texttt{Seen}}, both objects and affordances in the training and testing sets are consistent, while in \textbf{\texttt{Unseen}}, some objects or affordances in the testing set do not exist in the training set.