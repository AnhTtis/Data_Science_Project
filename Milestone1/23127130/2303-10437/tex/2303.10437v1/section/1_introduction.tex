%%%%%%%%% BODY TEXT
\section{Introduction}
The term ``affordance'' is described as ``opportunities of interaction'' by J. Gibson \cite{gibson2014ecological}. Grounding 3D object affordance aims to comprehend the interactive regions of objects in 3D space, which is not only simply to predict which interaction an object affords, but also to identify specific points on the object that could support the interaction. It constitutes a link between perception and operation for embodied agents, which has the potential to serve numerous practical applications, \eg action prediction \cite{koppula2013learning, vu2014predicting}, robot manipulation \cite{mandikal2021learning, moldovan2012learning, osiurak2016tool}, imitation learning \cite{hussein2017imitation, osa2018algorithmic}, and augmented/virtual reality \cite{cheng2013affordances, dalgarno2010learning}.

\begin{figure}[t]
	\centering
	\small
		\begin{overpic}[width=0.92\linewidth]{figs/first.pdf}
		    
	\end{overpic}
	\caption{\textbf{Grounding Affordance from Interactions.} We propose to ground 3D object affordance through 2D interactions. Inputting an object point cloud with an interactive image, grounding the corresponding affordance on the 3D object.}
 \label{fig1}
\end{figure}

\begin{figure*}[t]
	\centering
	\small
		\begin{overpic}[width=0.89\linewidth]{figs/motivation_debug.pdf}
		\put(23,-0.3){\textbf{(a)}}
		\put(73,-0.3){\textbf{(b)}}
  		% \put(6,30.5){\textbf{(\romannumeral1)}}
  		% \put(30,30.5){\textbf{(\romannumeral2)}}
	\end{overpic}
	\caption{\textbf{Motivation.} 
        \textbf{(a)} Clues to correlate regions of objects from different sources. Objects with the same category possess a similar combination scheme to meet certain affordance properties, which is a similar inherent relation among different instances.  And some appearance structures hint these properties, which exhibit closer representations with higher similarity. \textbf{(b)} Object affordance may be affected by dynamic factors, such as the position of the object itself in the scene, other objects in the scene, and the body part of the interacting subject. These factors can be decomposed into object-subject and object-scene interaction contexts.
	}
 \label{Fig:motivation}
\end{figure*}

\par So far, the paradigm of perceiving 3D object affordance has several branches. One of them involves establishing an explicit mapping between affordance categories and geometry structures \cite{deng20213d, kim2014semantic, myers2015affordance, xu2022partafford}, based on visual appearance. However, affordance is dynamic and multiple, these geometric-specific manners have limited generalization for unseen structures. Besides, locking the geometry with specific affordance category may lead to the anticipated region being inconsistent with its affordance when objects possess multiple similar geometrics, resulting in affordance regional confusion. Another paradigm is based on reinforcement learning, which puts the agent in 3D synthetic scenarios to interact with objects actively, taking the reward mechanism to optimize the process \cite{nagarajan2020learning}. While this approach transitions agents from passive recognition to active reasoning, it needs repeated attempts in a huge search space when meeting a novel structure, and is therefore time-consuming. 
\par These limitations motivate us to explore an affordance-compatible learning paradigm. Typically, humans can infer object affordance in the 3D physical world by watching images or videos that demonstrate the interactions. Some studies in cognitive science \cite{merleau1976phenomenologie, shilling2004body}  point out the existence of ``body image'' in human cognition, which claims that humans have a perceptual experience of the objects they see, thus facilitating their ability to operate novel objects, \ie the gap between human and object could be bridged by interaction. Hence, human-object interaction conveys the perception that object structure could perform certain affordance, which is a crucial clue to reason object affordance. In light of this, we present a novel task setting: grounding 3D object affordance from 2D interactions in images, shown in Fig. \ref{fig1}.

\par This challenging task includes serval essential issues that should be properly addressed. \textbf{1) Alignment ambiguity}. To ground 3D object affordance from 2D source interactions, the premise is to correspond the regions of the object in different sources. The object in 2D demonstrations and the 3D object faced are usually derived from different physical instances in different locations and scales. This discrepancy may lead to confusion about corresponding affordance regions, causing alignment ambiguity. While objects are commonly designed to satisfy certain needs of human beings, so the same category generally follows a similar combination scheme of object components to meet certain affordances, and these affordances are hinted by some appearance structures (Fig. \ref{Fig:motivation} (a)). These invariant properties are across instances and could be utilized to correlate object regions from different sources. \textbf{2) Affordance ambiguity}. Affordance has properties of dynamic and  multiplicity, which means the object affordance may change according to the situation, the same part of an object could afford multiple interactions, as shown in Fig. \ref{Fig:motivation} (b), ``Chair'' affords ``Sit'' or ``Move'' depends on the human actions, ``Mug'' affords ``Wrapgrasp'' or ``Contain'' according to the scene context, these properties may make ambiguity when extracting affordance. However, these dynamic factors for affordance extraction can be decomposed into the interaction between subject-object and object-scene. Modeling these interactions is possible to extract explicit affordance.

\par To address these issues, we propose the \textbf{I}nteraction-driven 3D \textbf{A}ffordance \textbf{G}rounding Network (\textbf{IAG}) to align object region features from different sources and model interaction contexts to reveal affordance. In detail, it contains two sub-modules, one is \textbf{J}oint \textbf{R}egion \textbf{A}lignment Module (\textbf{JRA}), which is devised to eliminate alignment ambiguity. It takes the relative difference in dense cross-similarity to refine analogous appearance regions and employs learnable layers to map the invariant combination scheme, taking them to match local regions of objects. For affordance ambiguity, the other one \textbf{A}ffordance \textbf{R}evealed \textbf{M}odule (\textbf{ARM}) takes the object representation as a shared factor and jointly models its interaction contexts with the affordance-related factors to reveal explicit affordance. Moreover, we collect \textbf{P}oint-\textbf{I}mage \textbf{A}ffordance \textbf{D}ataset (\textbf{PIAD}) that contains plenty of paired image-point cloud affordance data and make a benchmark on it to support the model training and evaluation of the proposed setting.

\par The contributions are summarized as follows: \textbf{1)} We introduce grounding 3D object affordance through the 2D interactions, which facilitates the generalization to 3D object affordance perception. \textbf{2)} We propose the IAG framework, which aligns the region feature of objects from different sources and jointly models affordance-related interactions to locate the 3D object affordance. \textbf{3)} We collect a dataset named PIAD to support the proposed task setting, which contains paired image-point cloud affordance data. Besides, we establish a benchmark on PIAD, and the experiments on PIAD exhibit the reliability of the method and the setting.