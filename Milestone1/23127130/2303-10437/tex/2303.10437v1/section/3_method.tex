
%------------------------------------------------------------------------
\section{Method}
\label{sec:method}
\subsection{Overview}
\label{Section:3.1}Our goal is to anticipate the affordance regions on the point cloud that correspond to the interaction in the image. Given a sample $\{P,I,\mathcal{B}, y\}$, where $P$ is a point cloud with the coordinates $P_{c} \in \mathbb{R}^{N \times 3}$ and the affordance annotation $P_{label} \in \mathbb{R}^{N \times 1}$, $I \in \mathbb{R}^{3 \times H \times W}$ is an RGB image, $\mathcal{B}=\{B_{sub},B_{obj}\}$ denotes the bounding box of the subject and object in $I$, and $y$ is the affordance category label. The \textbf{IAG} (Fig. \ref{fig:method}) capture localized features $\mathbf{F}_{\mathbf{I}} \in \mathbb{R}^{C \times H^{'} \times W^{'}}$ and $\mathbf{F}_p \in \mathbb{R}^{C \times N_{p}}$ of the image and point cloud by two feature extractors ResNet \cite{he2016deep} and PointNet++ \cite{qi2017pointnet++}. Then, utilizing $B_{obj}/B_{sub}$ to locate the object/subject region in $\mathbf{F}_{\mathbf{I}}$, outside the $B_{obj}$ and $B_{sub}$ is the scene mask $M_{sce}$, use ROI-Align \cite{he2017mask} to get the object, subject, and scene features $\mathbf{F}_{i}, \mathbf{F}_{s}, \mathbf{F}_{e} \in \mathbb{R}^{C \times H_{1} \times W_{1}}$, and reshape them to $\mathbb{R}^{C \times N_{i}}$ ($N_i = H_1 \times W_1$). Next, the JRA module takes $\mathbf{F}_i, \mathbf{F}_p$ as input and jointly mines the correspondence to align them, obtaining the joint feature $\mathbf{F}_j$ (Sec. \ref{Section:3.3}). Following, the ARM module takes $\mathbf{F}_j, \mathbf{F}_s, \mathbf{F}_e$ to reveal the affordance $\mathbf{F}_{\alpha}$ through cross-attention mechanism (Sec. \ref{Section: 3.4}). Ultimately, $\mathbf{F}_{\alpha}$ and $\mathbf{F}_j$ are sent to the decoder to compute the affordance logits $\hat{y}$ and the final 3D object affordance $\hat{\phi}$ (Sec. \ref{section3.5}). The process is expressed as $\hat{\phi}, \hat{y} = f_{\theta}(P_{c},I,\mathcal{B};\theta)$, where $f_{\theta}$ denotes the network and $\theta$ is the parameter.

\begin{figure*}[t]
	\centering
        \scriptsize
	\begin{overpic}[width=1.\linewidth]{figs/method_debug.pdf}
        \put(28,11){$f_p$}
        \put(28,13.8){$f_i$}
        \put(30.5,2.3){$\bar{\mathbf{P}}$}
        \put(30.5,20.5){$\bar{\mathbf{I}}$}
        \put(18,24.1){$\mathbf{F_{I}}$}
        \put(24,19.5){$\mathbf{F}_i$}
        \put(18,7.5){$\mathbf{F}_p$}
        \put(1.7,11.3){\tiny$B_{obj}$}
        \put(6.3,11.3){\tiny$B_{sub}$}
        \put(11.3,11.2){\tiny$M_{sce}$}
        \put(60,18.6){\scriptsize$\mathbf{F}_{j}$}
        \put(67.5,17.8){\scriptsize$\mathbf{F}_{s}$}
        \put(67.5,6.5){\scriptsize$\mathbf{F}_{e}$}
        \put(77.9,6){\scriptsize$\mathbf{F}_{\alpha}$}
        \put(77.9,20.5){\scriptsize$\mathbf{F}_{i\alpha}$}
        \put(94.5,20){\scriptsize$\mathbf{F}_{p\alpha}$}
        \put(78.7,3){\scriptsize$\hat{\mathbf{F}}_{i}$}
        \put(94.6,2.8){\scriptsize$\hat{\mathbf{F}}_{p}$}  
        \put(64,13.2){\scriptsize $\mathbf{Q}$}
        \put(70,21.5){\scriptsize $\mathbf{K}_1$}
        \put(70.2,19){\scriptsize $\mathbf{V}_1$}
        \put(70.4,2.6){\scriptsize $\mathbf{K}_2$}
        \put(70.2,5.2){\scriptsize $\mathbf{V}_2$}
        \put(75,19){\scriptsize $\mathbf{\Theta}_{1}$}
        \put(75,5.2){\scriptsize $\mathbf{\Theta}_{2}$}
        \put(90.5,16){\scriptsize$\Gamma$}
        \put(92,13){\scriptsize$f_{\phi}$}
        \put(86.5,25.5){\scriptsize $\hat{y}$}
        \put(96,16.5){\scriptsize $\hat{\phi}$}
        \put(81.3,11.5){\scriptsize $\mathbf{\mathcal{L}}_{KL}$}
        %figure_a
	\end{overpic}
	\caption{\textbf{Method.} Overview of Interaction-driven 3D Affordance Grounding Network (IAG), it firstly extracts localized features $\mathbf{F}_{i}, \mathbf{F}_{p}$ respectively, then takes JRA (Sec. \ref{Section:3.3}) to align them and get the joint feature $\mathbf{F}_{j}$. Next, ARM (Sec. \ref{Section: 3.4}) utilizes $\mathbf{F}_{j}$ to reveal affordance $\mathbf{F}_{\alpha}$ with $\mathbf{F}_s, \mathbf{F}_e$ by cross-attention. Eventually, $\mathbf{F}_{j}$ and $\mathbf{F}_{\alpha}$ are sent to the decoder (Sec. \ref{section3.5}) to obtain the final results $\hat{\phi}$ and $\hat{y}$.}
 \label{fig:method}
\end{figure*}

\subsection{Joint Region Alignment Module}
\label{Section:3.3}
The JRA module calculates the high-dimensional dense similarity in feature space to approximate analogous appearance regions that contain relatively higher similarity \cite{baltruvsaitis2018multimodal}. Meanwhile, to map the inherent combination scheme of object components, the JRA models the inherent relations between modality-specific regions and takes it to drive the alignment of other regions by a transformer-based joint-attention $f_\delta$. With the network trained to capture the correspondences among regions from different sources, the alignment is performed implicitly during the optimization.
\par Initially, $\mathbf{F}_p$ and $\mathbf{F}_i$ are projected into a feature space by shared convolution layers $f_\sigma$, obtaining the features $\mathbf{P} \in \mathbb{R}^{C \times N_{p}}, \mathbf{I} \in \mathbb{R}^{C \times N_{i}}$. Then, to correlate the analogous appearance regions, the dense cross-similarity is calculated between each region of $\mathbf{P}$ and $\mathbf{I}$, formulated as:
\begin{equation}
\label{Equ:1}
    \varphi_{i,j}=\frac{e^{(\mathbf{P}_{i},\mathbf{I}_{j})}}{\sum^{N_p}_{i=1}\sum^{N_i}_{j=1}e^{(\mathbf{P}_i,\mathbf{I}_{j})}}, \varphi \in \mathbb{R}^{N_p \times N_i},
\end{equation}
where $\varphi_{i,j}$ denotes the cross-similarity between the i-th region of $\mathbf{P}$ and the j-th region of $\mathbf{I}$. Taking the relative difference in $\varphi$ to refine and correlate analogous regions in $\mathbf{P}$ and $\mathbf{I}$, next, applying self-attention layers $f_i, f_p$ to model the intra-structural inherent relation of objects in respective modalities, the process is expressed as:
\begin{equation}
    \bar{\mathbf{P}} = f_p(\mathbf{I} \cdot \varphi^{T}), \bar{\mathbf{I}} = f_i(\mathbf{P} \cdot \varphi),
\end{equation}
where $\bar{\mathbf{P}} \in \mathbb{R}^{C\times N_p}, \bar{\mathbf{I}} \in \mathbb{R}^{C\times N_i}$. Following, we perform a joint-attention on features with structural relevance to map the similar inherent combination scheme and drive the alignment of the remaining regions. The process is formulated as: $\mathbf{F}_{j} = f_\delta[\bar{\mathbf{P}},\bar{\mathbf{I}}]$, where $\mathbf{F}_j \in \mathbb{R}^{C \times (N_{p}+N_{i})}$ and $[\cdot]$ denotes joining the image feature sequence and point cloud feature sequence into a whole one, and $\mathbf{F}_{j}$ denotes the joint representation. The $\mathbf{F}_{j}$ is sent to the affordance revealed module to excavate interaction contexts with $\mathbf{F}_{s}, \mathbf{F}_{e}$. And the affordance knowledge revealed by the interaction contexts could mutually optimize the alignment process during training (see Sec. \ref{section3.5}).

\begin{figure*}[t]
	\centering
	\footnotesize
        \begin{overpic}[width=0.95\linewidth]{figs/dataset.pdf}
        \put(23, 15){{\textbf{\rotatebox{90}{Num}}}}
        \put(9, 0){{\textbf{(a)}}}
        \put(36, 0){{\textbf{(b)}}}
        \put(71, 0){{\textbf{(c)}}}
        \put(95, 0){{\textbf{(d)}}}
        \put(3.5, 21){{\textbf{Open}}}
        \put(11, 21){{\textbf{Wrapgrasp}}}
	\end{overpic}
	\caption{\textbf{Properties of the PIAD dataset.} \textbf{(a)} Data pairs in the PIAD, the red region in point clouds is the affordance annotation. \textbf{(b)} Distribution of the image data. The horizontal axis represents the category of affordance, the vertical axis represents quantity, and different colors represent different objects. \textbf{(c)} Distribution of the point cloud data. \textbf{(d)} The ratio of images and point clouds in each affordance class. It shows that images and point clouds are not fixed one-to-one pairing, they can form multiple pairs. }
 \label{Fig:dataset}
\end{figure*}

\subsection{Affordance Revealed Module}
\label{Section: 3.4}
\par In general, the dynamic factors related to affordance are reflected in the interaction, thus affecting the extraction of affordance, and the interaction is mostly considered to exist between humans and objects. However, the interaction between objects and scenes (including other objects) may also affect affordance \cite{mo2022o2o}. To consider both of these factors, the ARM module utilizes the cross-attention technique to extract these interaction contexts respectively and jointly embed them to reveal explicit object affordance.
\par Specifically, $\mathbf{F}_{j}$ is projected to form the shared query $\mathbf{Q} = \mathbf{F}_{j}\mathbf{W}_{1}$, $\mathbf{F}_{s}$ and $\mathbf{F}_{e}$ are projected to form different keys and values $\mathbf{K}_{1/2} = \mathbf{F}_{s}\mathbf{W}_{2/3}, \mathbf{V}_{1/2} = \mathbf{F}_{e}\mathbf{W}_{4/5}$, where $\mathbf{W}_{1 \sim 5}$ are projection weights. Then cross-attention aggregates them to excavate interaction contexts, expressed as:
\begin{equation}
    \mathbf{\Theta}_{1/2} = softmax(\mathbf{Q}^{T}\cdot\mathbf{K}_{1/2}/\sqrt{d})\cdot\mathbf{V}^{T}_{1/2},
\end{equation}
where $\mathbf{Q} \in \mathbb{R}^{d \times (N_p+N_i)}, \mathbf{K}_{1/2}, \mathbf{V}_{1/2} \in \mathbb{R}^{d \times N_i}$, $d$ is the dimension of projection, $\mathbf{\Theta}_1, \mathbf{\Theta}_2$ indicate the excavated interaction contexts. Then, jointly embedding $\mathbf{\Theta}_1$ and $\mathbf{\Theta}_2$ to reveal affordance, expressed as:
\begin{equation}
\label{equ:affordance}
    \mathbf{F}_{\alpha} = f_{\xi}(\mathbf{\Theta}_{1},\mathbf{\Theta}_{2}), \mathbf{F}_{\alpha} \in \mathbb{R}^{C \times (N_p+N_i)},
\end{equation}
where $\mathbf{F}_{\alpha}$ is the joint affordance representation, it is split in the decoder to compute final results, $f_{\xi}$ denotes the concatenation, followed by a convolution layer to fuse the feature.

\subsection{Decoder and Loss Function}
\label{section3.5}
The $\mathbf{F}_j$ is split to $\hat{\mathbf{F}}_{p} \in \mathbb{R}^{C \times N_{p}}$ and $\hat{\mathbf{F}}_{i} \in \mathbb{R}^{C \times N_{i}}$, $\mathbf{F}_{\alpha}$ is split to $\mathbf{F}_{p\alpha} \in \mathbb{R}^{C \times N_{p}}$ and $\mathbf{F}_{i\alpha} \in \mathbb{R}^{C \times N_{i}}$ in the decoder. Pooling $\mathbf{F}_{p\alpha}$ and $\mathbf{F}_{i\alpha}$ respectively and concatnate them to compute the affordance logits $\hat{y}$. $\hat{\mathbf{F}}_{p}$ is upsampled to $\mathbb{R}^{C \times N}$ by Feature Propagation Layers (FP) \cite{qi2017pointnet++}, and the 3D object affordance $\hat{\phi}$ is computed as:
\begin{equation}
    \hat{\phi} = f_{\phi}(\mathbf{FP}(\hat{\mathbf{F}}_{p}) \odot \Gamma(\mathbf{F}_{p\alpha})), \hat{\phi} \in\mathbb{R}^{N \times 1},
    \label{equ:out}
\end{equation}
where $f_{\phi}$ is an output head, $\mathbf{FP}$ is the upsample layer, $\Gamma$ denotes pooling $\mathbf{F}_{p\alpha}$ and expand it to the shape of $\mathbb{R}^{C \times N}$.
\par The total loss comprises three parts: $\mathbf{\mathcal{L}}_{CE}$, $ \mathbf{\mathcal{L}}_{KL}$, $\mathbf{\mathcal{L}}_{HM}$. Where $\mathbf{\mathcal{L}}_{CE}$ computes the cross-entropy loss between $y$ and $\hat{y}$, it supervises the extraction of $\mathbf{F}_{\alpha}$ and implicitly optimizes the alignment process. To enable the network focus on the alignment of affordance regions, we apply the KL Divergence (KLD) \cite{bylinskii2018different} to constrain the distribution between $\mathbf{F}_{i\alpha}$ and $\hat{\mathbf{F}}_i$, formulated as $\mathbf{\mathcal{L}}_{KL} = (\mathbf{F}_{i\alpha} || \hat{\mathbf{F}}_i)$. The reason is that $\mathbf{F}_{i\alpha}$ exhibits the affordance distribution of each object region in the image, and the affordance-related regions keep more significant features. Constraining the feature distribution of $\hat{\mathbf{F}}_i$ to enhance the affordance region features in $\hat{\mathbf{F}}_i$, and with the region correlations established by the alignment process, $\hat{\mathbf{F}}_p$ also tends to exhibit this property. Which makes the alignment and affordance extraction optimize mutually. $\mathbf{\mathcal{L}}_{HM}$ is a focal loss \cite{lin2017focal} combined with a dice loss \cite{milletari2016v}, it is calculated by $\hat{\phi}$ and $P_{label}$, which supervise the point-wise heatmap on point clouds. Eventually, the total loss is formulated as:

\begin{equation}
\label{Equ:loss}
    \mathbf{\mathcal{L}}_{total} = \lambda_{1}\mathbf{\mathcal{L}}_{CE} + \lambda_{2}\mathbf{\mathcal{L}}_{KL} + \lambda_{3}\mathbf{\mathcal{L}}_{HM},
\end{equation}
where $\lambda_1$, $\lambda_2$ and $\lambda_3$ are hyper-parameters to balance the total loss. See more details in supplementary materials.

\begin{table*}[!t]
\centering
\small
  \renewcommand{\arraystretch}{1.}
  \renewcommand{\tabcolsep}{4.pt}
  \caption{{\textbf{Comparison Results on PIAD.} The overall results of all comparative methods, the best results are in \textbf{bold}. \textbf{\texttt{Seen}} and \textbf{\texttt{Unseen}} are two partitions of the dataset. ``\textbf{\textcolor[rgb]{0.2,0.8,0.1}{Green}}" and ``\textbf{\textcolor[rgb]{0.99,0.5,0.0}{Orange}}" indicate two types of the comparative modular baselines. ``\textbf{\texttt{Base.}}'' represents the baseline.  $\textcolor{darkpink}{\diamond}$ denotes the relative improvement of our method over other methods. AUC and aIOU are shown in percentage.}}
\label{table:results}
\vspace{5pt}
\begin{tabular}{c|r|c|ccc|ccc|c}
\toprule
 & \textbf{Metrics} & \textbf{\texttt{Base.}} & \textbf{\textcolor[rgb]{0.2,0.8,0.1}{\texttt{MBDF}} \cite{tan2021mbdf}} & \textbf{\textcolor[rgb]{0.2,0.8,0.1}{\texttt{PMF}} \cite{zhuang2021perception}} & \textbf{\textcolor[rgb]{0.2,0.8,0.1}{\texttt{FRCNN}} \cite{xu2022fusionrcnn}} & \textbf{\textcolor[rgb]{0.99,0.5,0.0}{\texttt{ILN}} \cite{chen2022imlovenet}} & \textbf{\textcolor[rgb]{0.99,0.5,0.0}{\texttt{PFusion}} \cite{xu2018pointfusion}} & \textbf{\textcolor[rgb]{0.99,0.5,0.0}{\texttt{XMF}} \cite{aiello2022cross}}  & \textbf{Ours}  \\
\midrule
\multirow{4}{*}{\rotatebox{90}{\textbf{\texttt{Seen}}}}   & AUC $\uparrow$     & $68.49$    & $74.88\textcolor{darkpink}{\diamond13.5\%}$    & $75.05\textcolor{darkpink}{\diamond13.2\%}$     &  $76.05\textcolor{darkpink}{\diamond11.8\%}$   & $75.84\textcolor{darkpink}{\diamond12.1\%}$      & $77.50\textcolor{darkpink}{\diamond9.7\%}$       & $78.24\textcolor{darkpink}{\diamond8.6\%}$  & \cellcolor{mygray}\textbf{84.51 $\pm$ 0.5} \\
        & aIOU $\uparrow$   & $6.85$     & $9.34\textcolor{darkpink}{\diamond111.0\%}$        & $10.13\textcolor{darkpink}{\diamond94.6\%}$     & $11.97\textcolor{darkpink}{\diamond64.7\%}$   & $11.52\textcolor{darkpink}{\diamond71.1\%}$      & $12.31\textcolor{darkpink}{\diamond60.1\%}$       & $12.94\textcolor{darkpink}{\diamond52.3\%}$ & \cellcolor{mygray}\textbf{19.01 $\pm$ 0.7} \\
        & SIM $\uparrow$    & $0.367$    & $0.415\textcolor{darkpink}{\diamond31.8\%}$        & $0.425\textcolor{darkpink}{\diamond28.7\%}$     & $0.429\textcolor{darkpink}{\diamond27.5\%}$    & $0.427\textcolor{darkpink}{\diamond28.1\%}$       & $0.432\textcolor{darkpink}{\diamond26.6\%}$        & $0.441\textcolor{darkpink}{\diamond24.0\%}$ & \cellcolor{mygray}\textbf{0.527 $\pm$ 0.02} \\
        & MAE $\downarrow$    & $0.152$    & $0.143\textcolor{darkpink}{\diamond38.4\%}$        & $0.141\textcolor{darkpink}{\diamond37.6\%}$      & $0.136\textcolor{darkpink}{\diamond35.3\%}$    & $0.137\textcolor{darkpink}{\diamond35.8\%}$      & $0.135\textcolor{darkpink}{\diamond34.8\%}$       & $0.127\textcolor{darkpink}{\diamond30.7\%}$   & \cellcolor{mygray}\textbf{0.098 $\pm$ 0.01}  \\
\midrule
\multirow{4}{*}{\rotatebox{90}{\textbf{\texttt{Unseen}}}}  & AUC $\uparrow$    & $57.34$    & $58.23\textcolor{darkpink}{\diamond21.1\%}$    & $60.25\textcolor{darkpink}{\diamond17.1\%}$      & $61.92\textcolor{darkpink}{\diamond13.9\%}$    & $59.69\textcolor{darkpink}{\diamond18.1\%}$      & $61.87\textcolor{darkpink}{\diamond14.0\%}$       & $62.58\textcolor{darkpink}{\diamond12.7\%}$ & \cellcolor{mygray}\textbf{69.44 $\pm$ 1.1}     \\
        & aIOU $\uparrow$   & $3.95$    & $4.22\textcolor{darkpink}{\diamond104.9\%}$         & $4.67\textcolor{darkpink}{\diamond85.2\%}$      & $5.12\textcolor{darkpink}{\diamond68.9\%}$     & $4.71\textcolor{darkpink}{\diamond83.6\%}$       & $5.33\textcolor{darkpink}{\diamond62.3\%}$        & $5.68\textcolor{darkpink}{\diamond52.3\%}$  & \cellcolor{mygray}\textbf{7.85 $\pm$ 0.8}     \\
        & SIM $\uparrow$    & $0.358$    & $0.369\textcolor{darkpink}{\diamond19.2\%}$        & $0.385\textcolor{darkpink}{\diamond14.3\%}$     & $0.393\textcolor{darkpink}{\diamond12.0\%}$    & $0.372\textcolor{darkpink}{\diamond18.2\%}$      & $0.387\textcolor{darkpink}{\diamond13.0\%}$       & $0.405\textcolor{darkpink}{\diamond8.6\%}$ & \cellcolor{mygray}\textbf{0.430 $\pm$ 0.01}     \\
        & MAE $\downarrow$    & $0.235$    & $0.213\textcolor{darkpink}{\diamond33.8\%}$        & $0.211\textcolor{darkpink}{\diamond33.1\%}$     & $0.195\textcolor{darkpink}{\diamond27.7\%}$    & $0.207\textcolor{darkpink}{\diamond31.9\%}$      & $0.193\textcolor{darkpink}{\diamond26.9\%}$       & $0.188\textcolor{darkpink}{\diamond25.0\%}$ & \cellcolor{mygray}\textbf{0.151 $\pm$ 0.01}   \\
\bottomrule
\end{tabular}
\end{table*}