\section{Experiments}
\subsection{Benchmark Setting}
\label{5.1}
\noindent\textbf{Evaluation Metrics.\ } To provide a comprehensive and effective evaluation, we compare serval advanced works in the affordance area \cite{deng20213d, nagarajan2019grounded, zhai2022one} and finally chose four evaluation metrics: \textbf{AUC} \cite{lobo2008auc}, \textbf{aIOU} \cite{rahman2016optimizing}, \textbf{SIM}ilarity \cite{swain1991color} and \textbf{M}ean \textbf{A}bsolute \textbf{E}rror \cite{willmott2005advantages} to benchmark the PIAD.

\noindent\textbf{Modular Baselines.\ } Since there are no prior works using paired image-point cloud data to ground 3D object affordance. For a thorough comparison of our method, we select several advanced image-point cloud cross-modal works as modular baselines. Methods of comparison are broadly divided into two types, one is that utilizes camera intrinsics to facilitate cross-modal feature alignment or fusion, \ie \textbf{\textcolor[rgb]{0.2,0.8,0.1}{\texttt{MBDF-Net(MBDF)}}} \cite{tan2021mbdf}, \textbf{\textcolor[rgb]{0.2,0.8,0.1}{\texttt{PMF}}} \cite{zhuang2021perception} and \textbf{\textcolor[rgb]{0.2,0.8,0.1}{\texttt{FusionRCNN(FRCNN)}}} \cite{xu2022fusionrcnn}, for this type of method, we remove the step of using intrinsic parameters to align raw data or features to explore their effectiveness on PIAD. Another type performs feature alignment directly in the feature space, without relying on camera intrinsic, \ie \textbf{\textcolor[rgb]{0.99,0.5,0.0}{\texttt{ImLoveNet(ILN)}}} \cite{chen2022imlovenet}, \textbf{\textcolor[rgb]{0.99,0.5,0.0}{\texttt{PointFusion(PFusion)}}} \cite{xu2018pointfusion} and \textbf{\textcolor[rgb]{0.99,0.5,0.0}{\texttt{XMFnet(XMF)}}} \cite{aiello2022cross}. To ensure a fair comparison, all methods use the same feature extractors, with the only variation coming from cross-modal alignment or fusion block. For the \textbf{\texttt{Baseline}}, we directly concatenate the features that are output from extractors, regarding the concatenation as the cross-fusion block. More details about baselines are provided in the supplementary materials.

\begin{figure}[t]
    \centering
    \footnotesize
    \begin{overpic}[width=0.96\linewidth]{figs/only_point_compare.pdf}
    \put(11,0){\textbf{(a)}}
    \put(10,49){\textbf{GT}}
    \put(76,0){\textbf{(c)}}
    \put(75,49){\textbf{Ours}}
    \put(36.5,0){\textbf{(b)}}
    \put(36,49){\textbf{\cite{deng20213d}}}
    \put(-1,33){\rotatebox{90}{\textbf{Move}}}
    \put(-1,9){\rotatebox{90}{\textbf{Open}}}
    \end{overpic}
    \caption{\textbf{Paradigm Comparison.} \textbf{(a)} Ground truth. \textbf{(b)} The results of 3D-AffordanceNet \cite{deng20213d}. \textbf{(c)} Our results. The top row shows the regional confusion case in \textbf{\texttt{Seen}}, and the bottom row displays the generalization in \textbf{\texttt{Unseen}}.}
    \label{fig:only_point}
\end{figure}

\begin{figure*}[t]
	\centering
	\small
        \begin{overpic}[width=0.92\linewidth]{figs/main_results.pdf}
        \put(23,-0.5){{\textbf{\texttt{Seen}}}}
        \put(74,-0.5){{\textbf{\texttt{Unseen}}}}
        
        \put(4.4,65.5){{\textbf{Listen}}}
        \put(16.5,65.5){{\textbf{Open}}}
        \put(30,65.5){{\textbf{Sit}}}
        \put(41.5,65.5){{\textbf{Lift}}}
        \put(56.5,65.5){{\textbf{Lay}}}
        \put(67.6,65.5){{\textbf{Grasp}}}
        \put(77.5,65.5){{\textbf{Wrapgrasp}}}
        \put(90.8,65.5){{\textbf{Contain}}}
        
        \put(-1,6.2){\rotatebox{90}{\textbf{\texttt{GT}}}}
        \put(-1,15){\rotatebox{90}{\textbf{\texttt{Ours}}}}
        \put(-1,24.5){\rotatebox{90}{\textcolor[rgb]{0.99,0.5,0.0}{\textbf{\texttt{XMF}}} \cite{aiello2022cross}}}
        \put(-1,33.5){\rotatebox{90}{\textcolor[rgb]{0.2,0.8,0.1}{\textbf{\texttt{FRCNN}}} \cite{xu2022fusionrcnn}}}
        \put(-1,46.3){\rotatebox{90}{\textbf{\texttt{Base.}}}}
	\end{overpic}
	\caption{\textbf{Visualization Results.} The first row is the interactive image, which demonstrates the interaction that the object can afford. The last row is the ground truth of 3D object affordance in the point cloud. The four columns on the left are the visual comparison results for distinct 3D object affordances in the \textbf{\texttt{Seen}} partition. The four columns on the right are the results in the \textbf{\texttt{Unseen}} partition.}
 \label{Fig:mainresults}
\vspace{1pt}
\end{figure*}

\subsection{Comparison Results}
The comparison results of evaluation metrics are shown in Tab. \ref{table:results}. As can be seen, our method outperforms the compared baselines, across all metrics in both partitions. Besides, to display the limitation of methods that lock geometrics with specific semantic affordance categories, we conduct an experiment to compare one of these methods \cite{deng20213d} with ours. As shown in Fig. \ref{fig:only_point}, the top row indicates that the result of this method exhibits regional confusion, \eg the region where the chair could be moved or sat is geometrically rectangular, and it directly anticipates results on these similar geometries, inconsistent with affordance property. Plus, the bottom row shows that directly establishing a link between object structures and affordances may fail to anticipate correct 3D affordance in \textbf{\texttt{Unseen}}. In contrast, our method anticipates precise results by mining affordance clues provided by 2D interactions. Additionally, visual comparative results of our method and other baselines are shown in Fig. \ref{Fig:mainresults}. As can be seen, the comparative baselines could anticipate some 3D object affordance under our setting, but in comparison, our method obviously achieves better results, which validates the rationality of our setting, and also demonstrates the superiority of our method.

\begin{table}
\centering
\small
\renewcommand{\arraystretch}{1.}
\renewcommand{\tabcolsep}{7.5pt}
\caption{\textbf{Ablation Study.} We investigate the improvement of JRA and ARM on the model performance based on the baseline.
}
\label{table:ablation}
\vspace{5pt}
\begin{tabular}{c|cc|cccc}
\toprule
\multicolumn{1}{l|}{} & \textbf{JRA} & \textbf{ARM} &\textbf{AUC} & \textbf{aIOU} & \textbf{SIM} & \textbf{MAE} \\ 
\midrule
\multirow{4}{*}{\rotatebox{90}{\textbf{\texttt{Seen}}}} &  &  &  68.49 & 6.85 & 0.367 & 0.152 \\
& \checkmark &  &  78.89 & 12.61 & 0.486 & 0.142 \\
 & & \checkmark & 77.57 & 11.55 & 0.469 & 0.143 \\
 & \checkmark & \checkmark & \cellcolor{mygray}\textbf{85.01} & \cellcolor{mygray}\textbf{19.71} & \cellcolor{mygray}\textbf{0.547} & \cellcolor{mygray}\textbf{0.088} \\ 
 \midrule
\multirow{4}{*}{\rotatebox{90}{\textbf{\texttt{Unseen}}}} & &  & 57.34 & 3.95 & 0.358 & 0.235 \\
& \checkmark &  & 65.32 & 6.07 & 0.413 & 0.175 \\
 & & \checkmark & 63.76 & 5.97 & 0.392 & 0.194 \\
 & \checkmark & \checkmark & \cellcolor{mygray}\textbf{70.54} & \cellcolor{mygray}\textbf{8.65} & \cellcolor{mygray}\textbf{0.440} & \cellcolor{mygray}\textbf{0.141} \\
 \bottomrule
\end{tabular}
\end{table}

\begin{figure}
    \centering
    \footnotesize
    \begin{overpic}[width=0.95\linewidth]{figs/KL_debug.pdf}
    \put(5,1){\textbf{Image Regions}}
    \put(-0.5,9){\textbf{\rotatebox{90}{Point Regions}}}
    \put(81,3){\textbf{Affordance}}
    \put(84,-0.5){\textbf{Regions}}
    \put(-0.5,44){\textbf{\rotatebox{90}{Open}}}
    \put(28.5,61){\textbf{$\bm{w/o}$ JRA}}
    \put(48.5,61){\textbf{$\bm{w}$ JRA}}
    \put(65,61){\textbf{$\bm{w/o}$ JRA}}
    \put(85,61){\textbf{$\bm{w}$ JRA}}
    \put(43,32){\textbf{step=8000}}
    \put(10,32){\textbf{step=1000}}
    \put(77,32){\textbf{step=15000}}
    % \put(-1,8){\textbf{\rotatebox{90}{Move}}}
    \end{overpic}
    \caption{\textbf{Ablation of JRA.} The top row is the result of the anticipated affordance with/without JRA. The bottom row is a part of the cross-similarity matrix of a sample during the training process.}
    \label{fig:kl}
\end{figure}

\begin{figure}
    \centering
    \footnotesize
    \begin{overpic}[width=0.88\linewidth]{figs/heatmap.pdf}
    \put(20,70){\textbf{$\bm{w/o}$ ARM}}
    \put(65,70){\textbf{$\bm{w}$ ARM}}
    % \put(20,-1.5){\textbf{$\bm{w/o}$ ARM}}
    % \put(65,-1.5){\textbf{$\bm{w}$ ARM}}
    \put(-0.5,50){\rotatebox{90}{\textbf{\texttt{Seen}}}}
    \put(-0.5,10){\rotatebox{90}{\textbf{\texttt{Unseen}}}}
    \end{overpic}
    \caption{\textbf{Ablation of ARM.} The activation map and t-SNE \cite{van2008visualizing} results with ($\bm{w}$), without ($\bm{w/o}$) ARM in both partitions.}
    \label{fig:heatmap}
\end{figure}

\begin{figure}
    \centering
    \small
    \begin{overpic}[width=0.96\linewidth]{figs/one2multi1.pdf}
    % \put(7.5,-0.8){\textbf{Image}}
    \put(39,-0.8){\textbf{(a)}}
    \put(68.5,-0.8){\textbf{(b)}}
    \put(88.5,-0.8){\textbf{(c)}}
    \put(-1,6){\textbf{\rotatebox{90}{Grasp}}}
    \put(-1,24){\textbf{\rotatebox{90}{Contain}}}
    \end{overpic}
    \caption{\textbf{Same Image} $w.r.t.$ \textbf{Multiple Point Clouds.} \textbf{(a)} Same object category. \textbf{(b)} Different object categories, similar geometrics. \textbf{(c)} Different object categories and geometrics.}
    \label{fig:one2multi1}
\end{figure}

\begin{figure}
    \centering
    \small
    \begin{overpic}[width=0.95\linewidth]{figs/one2multi2.pdf}
    \put(20,-1){\textbf{Listen}}
    \put(70,-1){\textbf{Grasp}}
    \end{overpic}
    \caption{\textbf{Same Point Cloud} $w.r.t.$ \textbf{Multiple Images.} Grounding affordance on the same point cloud with images that contain similar or disparate interactions.}
    \label{fig:one2multi2}
\end{figure}

\subsection{Ablation Study}
\label{5.3}
\noindent\textbf{Effectiveness of JRA. } Tab. \ref{table:ablation} reports the impact on
evaluation metrics of JRA. Fig. \ref{fig:kl} provides visual comparison results with and without JRA. It shows that in the absence of JRA, the result is anticipated over the entire region that contains the interactive components, which means that the affordance regions of different source objects do not match. Besides, we visualize a part of the cross-similarity matrix between $\hat{\mathbf{F}}_{p}$ and $\hat{\mathbf{F}}_{i}$ (Sec. \ref{section3.5}) during the training, also shown in Fig. \ref{fig:kl}. In the initial stage, only a small number of analogous appearance regions keep explicit correspondence, as training proceeds, the JRA maps the correlation among regions with lower similarity. Meanwhile, as the training progresses, the model extracts explicit affordance, and the affordance knowledge introduced into the optimization process matches the corresponding affordance regions.

\noindent\textbf{Effectiveness of ARM. } The influence of ARM on evaluation metrics is also shown in Tab. \ref{table:ablation}. To visually evaluate the effectiveness of ARM, we employ GradCAM \cite{selvaraju2017grad} to generate the activation map, and utilize t-SNE \cite{van2008visualizing} to demonstrate the clustering of affordances in both partitions. The results are shown in Fig. \ref{fig:heatmap}. It shows that ARM makes the model focus on the interactive regions to excavate the interaction contexts, and enables the model to differentiate various affordances from interactions in both partitions, which ensures the extraction of explicit affordance.

\subsection{Performance Analysis}
\label{6.1}
\noindent\textbf{Different Instances.} We conduct two experiments to verify whether the model could ground 3D affordance on instances from different sources: (\romannumeral1) using an image and different point clouds to infer respectively (Fig. \ref{fig:one2multi1} (a)). (\romannumeral2) using multiple images and a single point cloud to infer respectively (Fig. \ref{fig:one2multi2}). The results indicate that the model can anticipate 3D affordance on different instances with the same 2D interaction, and can also anticipate distinct 3D affordance on the same point cloud through different 2D interactions. Showing that our setting could make the model maintain the mapping between object structures and affordances by learning from different 2D interactions.

\noindent\textbf{Different Object Categories.} What will happen if the object category is different in the image and point cloud? To explore this issue, we perform experiments with mismatched object categories, shown in Fig. \ref{fig:one2multi1} (b) and (c). When objects are of different categories but with similar geometric primitives, \eg the handle of ``Mug'' and the shoulder strap of ``Bag'', our model still properly infer the 3D affordance. However, when geometric structures are also dissimilar (Fig. \ref{fig:one2multi1} (c)), the model does not make random predictions. This indicates that the model maps cross-category invariance between affordance and geometries, which can be generalized to new instances of geometry.

\begin{figure}
    \centering
    \footnotesize
    \begin{overpic}[width=0.92\linewidth]{figs/multi_activation.pdf}
    % \put(7,25.5){\textbf{Grasp}}
    \put(-3.5,7){\textbf{\rotatebox{90}{Grasp}}}
    \put(99.5,7){\textbf{\rotatebox{90}{Open}}}
    % \put(83,25.5){\textbf{Open}}
    \end{overpic}
    \caption{\textbf{Multiple Affordances.} Some objects like ``Bag" contains the region that corresponds to multiple affordances.}
    \label{fig:multi_property}
\end{figure}

\noindent\textbf{Multiplicity. } One of the defining properties of affordance is multiplicity. It means that some points may correspond to multiple affordances. Fig. \ref{fig:multi_property} shows that the model makes objects' multiple affordances be compatible in our setting, and verify the model does not anticipate affordance by linking a region with a specific affordance category. Instead, it predicts by considering the presence of affordance-related interactions in the same region, the final anticipation depends on the interaction content \eg ``Grasp'' or ``Open''.

\begin{figure}[t]
    \centering
    \footnotesize
    \begin{overpic}[width=0.96\linewidth]{figs/limtation_debug.pdf}
    \put(8,23.5){\textbf{Open}}
    \put(31,23.5){\textbf{GT}}
    \put(46,23.5){\textbf{Ours}}
    \put(70,23.5){\textbf{GT}}
    \put(86,23.5){\textbf{Ours}}
    \end{overpic}
    \caption{\textbf{Failure Cases.}  Over prediction in the small regions.}
    \label{fig:limitation}
\end{figure}

\begin{figure}[t]
    \centering
    \footnotesize
    \begin{overpic}[width=0.96\linewidth]{figs/application.pdf}
    \end{overpic}
    \caption{\textbf{Potential Applications.} This work has the potential to bridge the gap between perception and operation, serving areas like demonstration learning \cite{argall2009survey, rahmatizadeh2018virtual}, and may be a part of human-assistant agent system \eg Tesla Bot, Boston Dynamics Atlas \cite{nelson2019petman}.}
    \label{fig:app}
\end{figure}

\noindent\textbf{Limitations.} Our network aligns the object features in a single scale, which may lead to overly large receptive fields, resulting in over predictions for small regions, \eg the doorknob shown in Fig. \ref{fig:limitation}. Our feature extension will refer \cite{lin2017feature} to tackle this problem by aligning features hierarchically.