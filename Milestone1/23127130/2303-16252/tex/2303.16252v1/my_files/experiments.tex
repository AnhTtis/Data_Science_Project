
\section{Experimental Setup}




\subsection{Datasets}

\stitle{The Schema Guided Dialogue (SGD).} SGD dataset is a large-scale dataset for task-oriented dialogue that consists of over 16K multi-domain dialogs between a human and a virtual assistant covering 20 domains. The dataset also provides a schema for each domain that
provides a textual description of the domain, a list of slots, and a list of intents. A slot contains a name, textual description,
and possible values for categorical slots and an intent contains a name, textual description, optional slots, and result slots.

\stitle{SGD-X}. SGD-X dataset is an extension of the SGD dataset that contains stylistic variants for every schema in SGD.
It provides 5 variants of domain schemas, where each variant incrementally moves further away from the original schema.
The goal of this dataset is to evaluate model sensitivity to schema variations.
The authors of the dataset have shown that two of the top-performing schema-guided DST models are sensitive to schema changes and have had significant performance drops on SGD-X.

\subsection{Evaluation Metrics}

To evaluate the performance of our model, we compute multiple metrics on each component of a ToD system.

\stitle{DST.} We evaluate the performance of DST by calculating the intent accuracy, average goal accuracy (AGA), joint goal accuracy (JGA), and requested slot F1.
%all of which are suggested by the SGD dataset. 
%Since the SGD dataset was created for evaluating DST, it does not contain metrics for evaluating system
%actions and response.

\stitle{System Actions.} To evaluate the system actions, we compute the following metrics: inform, success, average action accuracy (AAA), and joint action accuracy (JAA).
Inform measures whether a system has provided a correct entity and success measures whether it has answered all the requested
information. AAA and JAA are similar to the goal metrics and are calculated from system actions. 
For inform, from the ground truth system actions we filter actions by action type inform (Inform, Inform Count)
and check if they are predicted correctly. For success, we filter actions by slot names that are in the requested slots and
check if the action slot values are predicted correctly. AAA and JAA are implemented following the implementations of AGA and JGA.
Since we also predict user actions, we calculate the
average and joint accuracy of the predicted user actions.

\stitle{System Response.} For evaluating the system response, we report the GLEU~\cite{wu2016googles} score as it performs better on individual sentence pairs.

\stitle{Overall.} To get an overall score for the model, we calculate the combined score~\cite{mehri2019structured}: (Inform + Success) $\times$ 0.5 + GLEU.

%Since the SGD dataset does not contain any metrics for system actions, 
%Furthermore, we compute the following metrics: Inform, Success, AAA, and JAA;
%to evaluate the performance of system actions.
%For inform, from the ground truth system actions we filter actions by action type inform (Inform, Inform Count)
%and check if they are predicted correctly. For success, we filter actions by slot names that are in the requested slots and
%check if the action slot values are predicted correctly. AAA and JAA are implemented following the implementations of AGA and JGA.
To ensure a fair comparison of {\oursys} with existing systems that have reported results on the SGD dataset,
we use the evaluation script provided by the SGD dataset, where applicable.



