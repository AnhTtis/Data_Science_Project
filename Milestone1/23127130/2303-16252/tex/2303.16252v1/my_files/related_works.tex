\section{Related Works}


\stitle{Supervised End to End Models.}
Pretrained language models like BERT~\cite{Devlin2019BERTPO}, GPT-2~\cite{Radford2019LanguageMA}, T5~\cite{Raffel2019ExploringTL} and UniLM~\cite{Dong2019UnifiedLM}
have been used extensively in the literature for {\etoe} models for {\tod} systems~\cite{HosseiniAsl2020ASL,Peng2021SoloistBT,Lee2020SUMBTLaRLEN,Yang2020UBARTF,Jeon2021DORATP,Sun2022BORTBA,Yang2022UBARv2TM,Noroozi2020AFA,He2021GALAXYAG}
on the popular MultiWoz 2.0~\cite{budzianowski2018large} dataset. Even though some of these models has shown experiments on zero shot performance,
they shine under supervised settings and are not able to generalize to new domains, whereas our model is designed to be zero shot generalizable. 
In all the existing systems, the dialog history is passed as the context, whereas we use a summarized context which consists
of the current user utterance and the DST of the previous state as the context.

%\stitle{Zero Shot Dialog Models.}

\begin{figure}[!t]
    \centering
    \includegraphics[width=0.9\linewidth]{assets/sgdx_results.pdf}
    \vspace{-15pt}
    \caption{
        % {\sgdx} results: For the 5 versions of {\sgdx}, we plot the mean of each metric and represent the standard deviation as the shaded area.
        {\sgdx} results: Mean and standard deviation of each metric across all versions of {\sgdx}
    }
    \vspace{-15pt}
    \label{fig:sgdx_graph}
    
\end{figure}

\stitle{Schema Guided Models.} To obtain Zero Shot generalizability, some work has been done by incorporating schema to transfer knowledge across domains. However these systems
only focus on certain components of {\tod} systems, such as  DST~\cite{Feng2020ASA,Feng2022DynamicSG,Lee2021DialogueST,Noroozi2020AFA,Wang2022SlotDM}
and next action prediction and response generation~\cite{Mosig2020STARAS,Mehri2021SchemaGuidedPF}. 

\stitle{Description and Prompt Based Models.} Generally, schema is described using abbreviated notations or in snake case,  
and this vocabulary that is not usually present in natural language. To remedy this problem, there has been some work on description based DST~\cite{Zhao2022DescriptionDrivenTD,Lin2021LeveragingSD,Mi2021CINSCI}, where the abbreviated and unnatural words are converted into natural descriptions, from which models can obtain useful semantic descriptions. Another aspect of ToD systems
is slot filling, which has been formulated as a question answering problem~\cite{Yang2022PromptLF,Madotto2021FewShotBP,Hu2022InContextLF,Brown2020LanguageMA,Su2021MultiTaskPF,Lin2021ZeroShotDS,Li2021ZeroshotGI}, where a prompt is passed as a natural language question and the model predicts the slot value. These models are making the context larger and require large language models to fit such a big input, whereas we embark on the opposite direction and try to make the 
context smaller. 




