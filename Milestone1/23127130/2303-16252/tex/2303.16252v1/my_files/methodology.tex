


\section{Methodology}

\subsection{Pre-trained Language Models}
Language models (e.g., BERT~\cite{Devlin2019BERTPO}, GPT-2~\cite{Radford2019LanguageMA}) have been trained on massive amounts of textual data and have shown state-of-the-art results in a variety of NLP tasks.
%Since language models have millions of parameters, they are able to effectively capture the semantic and syntactic information in text.
In this work, we use GPT-2 as the base model and fine-tune it on task-oriented dialog data to build a zero-shot generalizable end-to-end ToD system.
%\todo{Adib: Re-write given below passage: I have copied it from another paper.}
%\tododone{Done}
The GPT-2 model is pre-trained for autoregressive generation (i.e., predicting the next token given past tokens) on the WebText dataset (i.e., 40 GB of textual data) and adapts a transformer-based neural architecture~\cite{vaswani2017attention}.
Suppose we have a natural language sequence  $(\smalls_1, \cdots , \smalls_\smalln)$ where symbol $\smalls_\smalli$ is drawn from a fixed set of symbols. The sequential ordering of language leads to factorizing the joint probabilities over symbols as a product of conditional probabilities~\cite{bengio2003neural}, as given below.
\vspace{-5pt}
$$
p(\smalls) = \prod_{i=1}^\smalln p(\smalls_\smalli | \smalls_1, \cdots ,\smalls_{\smalli-1})
\vspace{-5pt}
$$

Using this approach, it is possible to estimate $p(\smalls)$ and any conditionals of the form 
$p(\smalls_{\smalli - k}, \cdots , \smalls_\smalli | \smalls_1, \cdots , \smalls_{\smalli - k - 1} )$, and perform tractable sampling. 
%Language models like GPT and BERT have been trained on massive amount of textual data and have shown to be effective in a variety of NLP tasks.
%Since language models have millions of parameters, they are able to effectively capture the semantic and syntactic information in text.
%In this paper, we use GPT-2 as the base model and fine tune it on task specific data to create an End-to-End TOD system.
%GPT-2 is a large transformer based language model that has been pre-trained for autoregressively generating the next word given a sequence of text as a prompt.
Since we formulate our problem as a sequence generation problem, GPT-2 is a natural choice for our TOD system.

%\tododone{We can delete all the text from the todo text to this.}
%GPT-2 is a large transformer-based language model, based on the decoder-block of the transformer neural architecture~\cite{vaswani2017attention}, that has been trained on 
%the massive WebText dataset. A natural language text sequence $(s_1, \cdots, s_n)$, where each item $s_i$ is drawn from a fixed vocabulary, we can formulate the 
%language generation as a product of conditional probabilities~\cite{bengio2003neural}, as below:
%$$
%p(\smalls) = \prod_{i=1}^\smalln p(\smalls_\smalli | \smalls_1, \cdots ,\smalls_{\smalli-1})
%$$

%This approach allows us to generate a sequence of text, given an input prompt using the conditional probability: $p(\smalls_{\smalli - k}, \cdots , \smalls_\smalli | \smalls_1, \cdots , \smalls_{\smalli - k - 1} )$. Since we formulate our problem as a sequence generation task, GPT-2 is a natural choice for our TOD system.


\subsection{Problem Formulation}
We formulate task-oriented dialog response generation as a conditional sequence generation task. 
To facilitate zero-shot generalization to unseen domains, we condition the response generation for a dialog $x$ on the domain schema $S_i$, in addition to the dialog context.
The overall loss of our model can be written as:

\vspace{-10pt}
\begin{equation}    
    \mathcal{L}_{ZS-ToD} = \mathbb{E}_{x \sim D} \biggl( \sum_{n=1}^{T} -\log p(x_n | x_{<n}, S_i) \biggl)
    \label{eq:schema_gen}
\end{equation}

Specifically, in a multi-domain dialog system, the domain semantics are encapsulated in the domain schema denoted by $S = \{S_1, S_2, \cdots \}$ where $S_i$ represents schema for domain~$D_i$, which consists of a set of intents $I = \{I_1, I_2, \cdots \}$ and relevant set of slots $K = \{K_1, K_2, \cdots \}$ to fulfill a given intent.
A dialog session is composed of interactions between the user and the system in natural language from (single or) multiple domains.
We denote the dialog as 
$\{U^u_1, U^s_1, \cdots, U^s_T\}$ where $U^u_i$ and $U^s_i$ represents user and system utterances, respectively, at turn $i$ and $T$ is the number of turns in a dialog. 
We summarize the dialog history up to turn $t-1$ in the form of a dialog state represented by $DS_{t-1}$, which tracks the user's active intent $I_k$ and a list of tuples recording the slot names and corresponding slot values in a particular domain $(D_j, K_i, V_i)$, where $V_i$ represents the value for the slot $K_i$.
 At turn $t$, {\oursys} estimates the probability of the dialog state $DS_t$ by conditioning on previous dialog state $DS_{t-1}$, user's current utterance $U^u_t$,  and domain schema $S_i$:
\vspace{-1pt}
\begin{equation}    
    P(DS_t | DS_{t-1}, U^u_t, S_i)
    \label{eq:dialog_state}
\end{equation}

 
%the domain knowledge is encapsulated in a domain schema, $DS_i$, which is identified by the domain name and contains a list of slots and intents, $DS_i = \{slots, intents\}$.

%At timestep $t$, the user utterance is $U^u_t$ and the system response is $S^r_t$ and the current state of the dialog is captured
%in a dialog state object $D_t$, 

%At timestep $t$,~\oursys~estimates the probability of the dialog state, $D_t$ by conditioning on $U^u_t$, $D_{t-1}$ and $DS_i$  as follows:

%\begin{equation}
%    P(D_t | U_t, D_{t-1}, Schema_i)
%    \label{eq:dialog_state}
%\end{equation}

Then, at turn $t$, {\oursys} estimates the probability of the user action $A^u_t$ by conditioning on dialog state $DS_t$, user's current utterance $U^u_t$, and domain schema $S_i$:
\vspace{-1pt}
\begin{equation}
    P(A^u_t | DS_{t}, U^u_t, S_i )
    \label{eq:user_action}
\end{equation}
where the user action $A^u_t$ is represented by a list of tuples $(D_j, a^u_n, K_i, V_i)$ to record the user's action type $a^u_i \in \{a^u_1, a^u_2, \cdots, a^u_m \}$, slot name $K_i$, and corresponding slot values $V_i$ in a particular domain $D_j$.
The estimated dialog state $DS_t$ at turn $t$ is used to query the database (if needed), which returns a list of database results denoted by $DB_t$, that satisfy the constraints in the dialog state.

Next, {\oursys} estimates the probability of the system action $A^s_t$ denoted by a list of tuples $(D_j, a^s_m, K_i, V_i)$ where $a^s_i \in \{a^s_1, a^s_2, \cdots, a^s_n \}$ represents system's action type by conditioning on the dialog state $DS_{t}$, user utterance $U^u_t$, user action $A^u_t$, database results $DB_t$, the set of all available system action types $\forall a^s_i$:
\vspace{-1pt}
\begin{equation}
    P(A^s_t | DS_{t}, U^u_t, A^u_t, DB_t, \forall a^s_i)
    \label{eq:system_action}
\end{equation}


Finally, {\oursys} estimates the probability of the system's natural language response $U^s_t$ by conditioning on the dialog state $DS_t$, user utterance $U^u_t$, user action $A^u_t$, system action $A^s_t$, and domain schema $S_i$:
\vspace{-1pt}
\begin{equation}
    P(U^s_t | DS_t, U^u_t,  A^u_t, A^s_t, S_i)
    \label{eq:system_response}
\end{equation}

To accommodate for multi-domain dialogs, multiple domain schemas can be used to condition on.
In the traditional supervised learning setting, we have labeled training dialogs from all the domains. 
Whereas, in the zero-shot learning setup we assume that the training dialogs are available only for seen domains $D_s = \{D_1, D_2, \cdots, D_k \}$ and the dialogs from unseen domains $D_u = \{ D_{k+1}, D_{k+2}, \cdots \}$ may only show up at inference time where $ D_s \cap D_u =  \emptyset$. 
This challenging setting is the focus of the paper.  

%\begin{equation}
%    P(U_t^a | U^u_t, D_{t}, DS_i, DB_t)
%    \label{eq:user_action}
%\end{equation}

%The user action contains a list of triplets recording the action type, slot names and values in a particular domain: $(domain\_name, action\_type, slot\_name, value)$.
%Next, {\oursys} estimates the probability of the system action consisting of items similar to the user actions, $S^a_t$ by conditioning on $U^u_t$, $U^a_t$, $D_t$, $DS_i$, $\forall S^a(name)$  and $DB_t$ as follows:
%\begin{equation}
%    P(S_t^a | U^u_t, D_{t}, DS_i,\forall S^a(name), DB_t)
%    \label{eq:system_action}
%\end{equation}

%Finally,~\oursys~estimates the probability of the system response, $S^r_t$ by conditioning on $U^u_t$, $U^a_t$, $S^a_t$, $D_t$ and $DS_i$ as follows:

%\begin{equation}
%    P(S_t^r | U^u_t, D_t, U^a_t, S^a_t, DS_i)
%    \label{eq:system_response}
%\end{equation}


\subsection{Model Architecture}
%\todo{Adib: Expand on the first three points from the introduction for Section 2.3. Also use figure 2 to explain.}
%\tododone{I dont what I should write here to expand from the introduction. Maybe I wrote too much there. I wrote the input/output of model and the 2step training details.}
We use the pre-trained GPT-2 with a language modeling (LM) head.
Following the autoregressive nature of the model and problem formulation, we feed the previous dialog state, the user's current utterance, domain schema, and optionally database results to {\oursys} after tokenization. 
Then, {\oursys} outputs a representation, which upon decoding represents the updated dialog state, user actions, system actions, and system response in natural language. In this work, we employ the greedy decoding strategy for text generation. 
%The last user utterance, dialog state of the previous state, domain schemas, database results and the set of system action types are concatenated, tokenized and 
%fed as inputs to~\oursys and the output is a list of tokens which upon decoding represents the updated dialog state, user actions, system actions and system response. 
%Figure~\ref{fig:our_model} shows a visual representation of the flow of~\oursys.

\begin{table*}[!t]
    \centering
    \begin{adjustbox}{max width=\textwidth}
        \begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|}
            \hline
            \multirow{3}{*}{Model}     &         & \multirow{2}{*}{\textbf{Intent}}   & \textbf{Requested} & \textbf{Average}        & \textbf{Joint}          &                &         & \textbf{Average}  & \textbf{Joint}    & \textbf{Average}    & \textbf{Joint}      & \multirow{2}{*}{\textbf{Response}} &          \\
                                       & \textbf{Domains} & \multirow{2}{*}{\textbf{Accuracy}} & \textbf{Slots}     & \textbf{Goal}           & \textbf{Goal}           & \textbf{Inform}         & \textbf{Success} & \textbf{Action}   & \textbf{Action}   & \textbf{UserAction} & \textbf{UserAction} & \multirow{2}{*}{\textbf{GLEU}}     & \textbf{Combined} \\
                                       &         &                           & \textbf{F1}        & \textbf{Accurracy}      & \textbf{Accuracy}       &                &         & \textbf{Accuracy} & \textbf{Accuracy} & \textbf{Accuracy}   & \textbf{Accuracy}   &                           &          \\ \hline
            \multirow{3}{*}{SimpleTOD} & all     & 78.60                     & 94.08     & 47.85          & 24.18          & 55.65          & 47.27   & 49.08    & 37.66    & 66.42      & 57.46      & 20.64                     & 72.10    \\
                                       & seen    & 80.07                     & 94.55     & 52.00          & 29.35          & 58.35          & 50.13   & 51.43    & 40.26    & 68.88      & 60.31      & 24.89                     & 79.13    \\
                                       & unseen  & 78.63                     & 93.92     & 46.27          & 22.72          & 54.28          & 46.17   & 48.29    & 37.12    & 65.55      & 56.65      & 19.24                     & 69.47    \\ \hline
            {SimpleTOD w/}             & all     & 82.34                     & 95.72     & 58.03          & 30.36          & 68.30          & 60.47   & 55.18    & 43.42    & 70.30      & 60.23      & 22.03                     & 86.41    \\
            {Schema \&}                & seen    & 83.32                     & 96.05     & 61.29          & 34.88          & 70.05          & 62.68   & 57.28    & 46.01    & 72.34      & 62.61      & 25.68                     & 92.04    \\
            {DB Results}               & unseen  & 82.19                     & 95.71     & 57.35          & 29.20          & 68.10          & 60.48   & 54.64    & 42.85    & 70.19      & 60.24      & 20.40                     & 84.69    \\ \hline
            \multirow{3}{*}{\textbf{\oursys}}   & all     & \textbf{84.83}                     & \textbf{95.53}     & \textbf{72.38} & \textbf{48.44} & \textbf{73.08} & \textbf{62.19}   & \textbf{58.32}    & \textbf{46.31}    & \textbf{73.20}      & \textbf{64.20}      & \textbf{20.04}                     & \textbf{87.67}    \\
                                       & seen    & \textbf{85.48}                     & \textbf{95.88}     & \textbf{74.23} & \textbf{52.05} & \textbf{74.72} & \textbf{63.85}   & \textbf{60.19}    & \textbf{48.69}    & \textbf{74.89}      & \textbf{66.24}      & \textbf{24.66}                     & \textbf{93.95}    \\
                                       & unseen  & \textbf{84.45}                     & \textbf{95.42}     & \textbf{72.03} & \textbf{47.83} & \textbf{71.68} & \textbf{61.63}   & \textbf{57.42}    & \textbf{45.21}    & \textbf{72.56}      & \textbf{63.46}      & \textbf{18.51}                     & \textbf{85.16}    \\ \hline
        \end{tabular}
    \end{adjustbox}
    \vspace{-5pt}
    \caption{Main Results. For end-to-end systems,~\oursys~outperforms existing baselines across all metrics, particularly there is significant improvement in key metrics like Average/Joint Goal Accuracy and Inform.}
    \label{tab:main-results}
    \vspace{-15pt}
\end{table*}

\subsection{Two-step Training}
Following the conditional generation in GPT-2, at a given turn $t$, 
we pass input tokens $S_{in} = \{s_1, \cdots, s_k\}$ as the prompt and the model generates a response $S_{out} = \{s_{k+1}, \cdots, s_{m+k} \}$, where $k$ and $m$ represent the input and output lengths, respectively.
Since conditional generation models (e.g., GPT-2) process one token at a time, the probability of predicting a token $\smalls_\smalli$ can be written as:
$p(\smalls_\smalli | \smalls_1, \cdots ,\smalls_{\smalli-1})
$.
%\cdots, t_n\}$ which contains the input followed by the generated text
%Generation models are passed an input tokens, $T_{in} = \{t_1, \cdots, t_p\}$ as the prompt,
%and the model generates a response, $T_{out} = \{t_1, \cdots, t_p, \cdots, t_n\}$ which contains the input followed by the generated text.
The standard procedure for training these models is to optimize the Cross-Entropy (CE) loss over the full sequence. 
In ToD systems,
the input prompt is usually a long sequence of text that contains the entire dialog history, and the generation is much shorter than the input prompt.
Since the focus is not on generating the input prompt, we need to modify the loss function to pay less attention to the input prompt and more attention to the response.

To overcome the aforementioned issue, we propose a two step training approach.
%for training ToD systems that use generation models.
In the first step, we follow the standard training procedure and calculate the CE loss on the full sequence.
For the second step, we initialize the model with the weights from the first step and calculate the CE loss only on the response,
as shown in Equation~\eqref{eq:loss_func}.
\vspace{-5pt}
\begin{equation}
    L = - \sum_{i=k+1}^{k+m} s_i \log(p_i)
    \label{eq:loss_func}
\end{equation}
\vspace{-10pt}




\subsection{Zero-shot Generalization}
%\todo{Adib: talk about how the proposed approach facilitate zero-shot generalization}\tododone{Done}
Once {\oursys} is trained using the above-mentioned techniques, it can generalize to unseen domains seamlessly.
When {\oursys} is exposed to dialogs from a new unseen domain, the domain schema is expected to be part of the input.
%When dialog turns from a new unseen domain is fed to {\oursys}, the domain schema is expected to be part of the input. 
Since the problem is formulated as a conditional generation, {\oursys} pays attention to the relevant parts of the schema to generate the user intent, slot names as well as slot values, thus adapting to the new unseen domains with no additional training. 

