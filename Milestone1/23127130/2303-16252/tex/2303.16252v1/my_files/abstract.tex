%Task Oriented Dialog (TOD) Systems that generalize well to unseen domain has been a challenging research area in dialog.
%Existing methods mainly target individual components of TOD systems, but there are no End-to-End TOD systems that generalize well to unseen domains.
%We introduce a novel TOD system that uses domain schema to generalize to unseen domains and propose to replace the dialog history with a
%dialog summary as the context to the model. To enhance language generation, we suggest a two step training process where the goal of the first
%pass is to learn the general structure of the data and the second pass is to optimize the generation.
%Through experimental results on the SGD and SGD-X dataset, we demonstrate the superiority of our approach over the state of the art models.

Task-oriented dialog systems empower users to accomplish their goals by facilitating intuitive and expressive natural language interactions.
% State-of-the-art approaches in task-oriented dialog systems formulate the problem as a conditional sequence generation task and fine-tune the pre-trained causal language models in the supervised setting, which require labeled training data for each new domain or task. 
% Whereas, acquiring labeled training data for each new domain or task is prohibitively laborious and expensive, and turns out to be a major bottleneck for scaling up the systems to a wide range of domains.
 State-of-the-art approaches in task-oriented dialog systems formulate the problem as a conditional sequence generation task and fine-tune pre-trained causal language models in the supervised setting. This requires labeled training data for each new domain or task, and acquiring such data is prohibitively laborious and expensive, thus making it a bottleneck for scaling systems to a wide range of domains. 
To overcome this challenge, we introduce a novel \textbf{Z}ero-\textbf{S}hot generalizable end-to-end \textbf{T}ask-\textbf{o}riented \textbf{D}ialog system, {\oursys}, that leverages domain schemas to allow for robust generalization to unseen domains and exploits effective summarization of the dialog history. 
We employ GPT-2 as a backbone model and introduce a two-step training process where the goal of the first step is to learn the general structure of the dialog data and the second step optimizes the response generation as well as intermediate outputs, such as dialog state and system actions.
As opposed to state-of-the-art systems that are trained to fulfill certain intents in the given domains and memorize task-specific conversational patterns, {\oursys} learns generic task-completion skills by comprehending domain semantics via domain schemas and generalizing to unseen domains seamlessly.
We conduct an extensive experimental evaluation on SGD and SGD-X datasets that span up to 20 unique domains and {\oursys} outperforms state-of-the-art systems on key metrics, with an improvement of \textbf{+17\% on joint goal accuracy} and \textbf{+5 on inform}. 
Additionally, we present a detailed ablation study to demonstrate the effectiveness of the proposed components and training mechanism.
