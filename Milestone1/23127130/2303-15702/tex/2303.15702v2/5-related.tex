% \begin{table*}[t!]
% \begin{minipage}{0.175\linewidth}
% \centering
% % \hspace{1.8mm}
% \captionof{table}{\small Datasets statistics \label{graph_datasets}}
% \begin{tiny}
% \begin{tabular}{c||c|c}
%       {\bf Graph} & {\bf \#nodes} & {\bf \#edges} \\ \hline
%       {\em FL} & 80\,513     & 5\,899\,882 \\
%       {\em YT} & 1\,138\,499 & 2\,990\,443 \\
%       {\em LJ} & 2\,238\,731 &14\,608\,137 \\
%       {\em OR} & 3\,072\,441 & 117\,185\,083 \\
%       {\em TW} & 41\,652\,230 & 1\,468\,365\,182 \\
% \end{tabular}
% \end{tiny}
% \end{minipage}%
%  \quad
%  \begin{minipage}{.265\linewidth}
% \centering
% \tabcolsep=0.05cm

% \captionof{table}{\small Avg. memory footprint (GB) of {\sf DistGER} and {\sf KnightKing} on each machine, where $\sigma$ is the standard deviation}
% \label{Memory_usage}
% \begin{tiny}
% \newcommand{\tabincell}[2]{\begin{tabular}{@{}#1@{}}#2\end{tabular}}
%   % \caption{\small {\color{blue} Avg. memory footprint (GB) of {\sf DistGER} and {\sf KnightKing} on each machine, where $\sigma$ is the standard deviation.}}
%   \begin{tabular}{c|cc|cc}
%     %\hline
%     { }&\multicolumn{2}{c|}{\bfseries{ Sampling}}&\multicolumn{2}{c}{\bfseries{Training}}\\
%     \hline
%     {\bf{Graph}} &{\sf KnightKing} &{\sf DistGER} &{\sf KnightKing} &{\sf DistGER} \\
%     \hline
%      {\em FL} & 0.66($\pm$0.06)	&{\bf 0.41($\pm$0.02)}	&1.31($\pm$0.17) 	&{\bf 0.86($\pm$0.06)} 	\\

%      {\em YT} &4.11($\pm$0.55)	&{\bf 1.36($\pm$0.23)} 	&4.73($\pm$0.72) 	&{\bf 4.26($\pm$0.63)} \\

%      {\em LJ} & 7.65($\pm$0.82)	&{\bf 1.95($\pm$0.16)}	&6.38($\pm$0.97) 	&{\bf 5.49($\pm$0.85)} 	\\

%      {\em CO} &10.98($\pm$1.03)	&{\bf 3.27($\pm$0.79)} 	&8.52($\pm$1.01) 	&{\bf 6.86($\pm$0.69)} 	\\

%      {\em TW} & out-of-memory	&{\bf 20.18($\pm$3.62)} 	&out-of-memory 	& {\bf 67.16($\pm$5.18)} 	\\
%   %\hline
% \end{tabular}
% \end{tiny}

% \end{minipage}
% \quad
% \begin{minipage}{.25\linewidth}
%     \centering
%     \includegraphics[width= 1.85in, height = 1.2in]{./Figures/Dist_total_time_partition.eps}%
%     \captionof{figure}
%       {\small Efficiency: {\sf PBG} \cite{PBG_2019}, {\sf DistDGL} \cite{DistDGL_2020}, {\sf KnightKing} \cite{KnighKing_2019}, {\sf HuGE-D} (baseline), {\sf DistGER} (ours)
%         \label{overall_performance}
%       }
% \end{minipage}%\hfill
% \quad
% \begin{minipage}{.25\linewidth}
%     \centering
%     \includegraphics[width= 1.85in, height = 1.2in]{./Figures/Dist_scalability_partition.eps}%
%     \captionof{figure}
%       {\small Scalability: {\sf PBG} \cite{PBG_2019}, {\sf DistDGL} \cite{DistDGL_2020}, {\sf KnightKing} \cite{KnighKing_2019}, {\sf HuGE-D} (baseline), {\sf DistGER} (ours)
%         \label{Dist_scalability}
%       }
% \end{minipage}
% \end{table*}


% \begin{table*}[t!]
% \begin{minipage}{0.42\linewidth}
% \centering
% \tabcolsep=0.05cm
% \vspace{-1mm}
%  {\color{blue}
% \captionof{table}{\small{\color{blue} Datasets statistics \label{graph_datasets} and Avg. memory footprint (GB) of {\sf DistGER} and {\sf KnightKing} on each machine, where $\sigma$ is the standard deviation.}}
% \vspace{-2mm}
% \begin{scriptsize}
% \begin{tabular}{c||c|c|cc|cc}
% {\bf{Graph} }&{\bf \#nodes }& {\bf \#edges }&\multicolumn{2}{c|}{\bfseries{ Sampling}}&\multicolumn{2}{c}{\bfseries{Training}}\\
%       { } & { } & { } &{\sf KnightKing} &{\sf DistGER} &{\sf KnightKing} &{\sf DistGER} \\
%       \hline
%       {\em FL} & 80.51 K     & 5.89 M  & 0.66($\pm$0.06)	&{\bf 0.41($\pm$0.02)}	&1.31($\pm$0.17) 	&{\bf 0.86($\pm$0.06)} 	\\
%       {\em YT} & 1.14 M & 2.99 M  &4.11($\pm$0.55)	&{\bf 1.36($\pm$0.23)} 	&4.73($\pm$0.72) 	&{\bf 4.26($\pm$0.63)} \\
%       {\em LJ} & 2.24 M &14.61 M  & 7.65($\pm$0.82)	&{\bf 1.95($\pm$0.16)}	&6.387($\pm$0.97) 	&{\bf 5.49($\pm$0.85)} 	\\
%       {\em CO} & 3.07 M & 117.19 M  &10.98($\pm$1.03)	&{\bf 3.27($\pm$0.79)} 	&8.52($\pm$1.01) 	&{\bf 6.86($\pm$0.69)} 	\\
%       {\em TW} & 41.65 M & 1.47 B  & out-of-memory	&{\bf 37.1($\pm$5.28)} 	&out-of-memory 	& {\bf 79.5($\pm$7.27)} 	\\  
% \end{tabular}
% \end{scriptsize}
% }
% \end{minipage}%
%  \quad
% \begin{minipage}{.27\linewidth}
%     \centering
%     \includegraphics[width= 2in, height = 1.35in]{Dist_total_time_partition.eps}%
%     \vspace{-4mm}
%     \captionof{figure}
%       {\small Efficiency: {\sf PBG} \cite{PBG_2019}, {\sf DistDGL} \cite{DistDGL_2020}, {\sf KnightKing} \cite{KnighKing_2019}, {\sf HuGE-D} (baseline), {\sf DistGER} (ours)
%         \label{overall_performance}
%       }
%      \vspace{-5.5mm}
% \end{minipage}%\hfill
% \quad
% \begin{minipage}{.27\linewidth}
%     \centering
%     \includegraphics[width= 2 in, height = 1.35in]{Dist_scalability_partition.eps}%
%     \vspace{-4mm}
%     \captionof{figure}
%       {\small Scalability: {\sf PBG} \cite{PBG_2019}, {\sf DistDGL} \cite{DistDGL_2020}, {\sf KnightKing} \cite{KnighKing_2019}, {\sf HuGE-D} (baseline), {\sf DistGER} (ours)
%         \label{Dist_scalability}
%       }
%       \vspace{-5.5mm}
% \end{minipage}
% \end{table*}

\section{Related Work}
\label{sec:related}
%
% \eat{
\spara{Random walk-based graph engines. }
%General graph systems, e.g., %on a single machine such as
%{\sf GraphChi} \cite{Graphchi_2012}, {\sf X-Stream} \cite{X-stream_2013}, {\sf GridGraph} \cite{GridGraph_2015},
%{\sf Graphene} \cite{Graphene_2017}, %generally process the disk-resident graphs with cache-friendly or I/O-efficient manners.
%{\sf PowerGraph} \cite{PowerGraph_2012}, {\sf Gemini} \cite{Gemini_2016}, and {\sf GraphX} \cite{GraphX_2014}, %are distributed graph systems proposed to handle large-scale graphs which cannot reside on a single machine. However, the typical graph frameworks adopt system optimizations such as the GAS-like execution and 2-D graph partitions, aiming to ensure more sequential accesses as these graph engines have to traverse all edges.
% are suitable for iterative batch processing over all edges. They
%are less fit for random walks due to inherent randomness and sparse computations.
%For random walk-specific frameworks,
%{\sf Drunkard-Mob}~\cite{Drunkardmob_2013} proposes a lightweight index to implement out-of-core random walks based on
%{\sf GraphChi}.
{\sf Graphwalker} \cite{GraphWalker_2020} focuses on improving I/O efficiency to support
scalable random walks on a single machine. {\sf FlashMob} \cite{FlashMob_2021} optimizes
cache utilization to make memory accesses more sequential %and regular
for faster random walks.
{\sf ThunderRW} \cite{ThunderRW_2021} addresses irregular memory access patterns of random walks.
%, using a step interleaving technique to hide memory access latency.
{\sf CraSorw} \cite{CraSorw_2022} proposes an I/O-efficient disk-based system to
support scalable second-order random walks.
%Although being deployed on a single-machine can save considerable cost for ordinary users,
%it still fails to meet the rapid demand for the computational resource by graph embedding tasks.
The GPU-based system, {\sf NextDoor} \cite{nextdoor_2021} leverages intelligent scheduling and caching strategies to mitigate
irregular memory access patterns in random walks.
% but the best performance is obtained
%will be played only
% when the graph can be completely held in the GPU memory.
%However, typically the GPU memory is much smaller than the main memory.
Different from single-machine random walk systems,
%the representative distributed system
{\sf KnightKing} \cite{KnighKing_2019} %is recently proposed to
%optimizes the walk forwarding process for {\sf node2vec} and
provides a general-purpose distributed random walk engine.
However, it ignores information-effectiveness during random walks and is limited by ineffective
graph partition %and higher communication costs
(\S~\ref{sec:dRand}).
% }
%

\spara{Graph embedding algorithms.} %Graph embeddings \cite{CaiZC18} have been studied in many contexts,
%e.g., %uncertain \cite{uncertain_graph_embedding_2017},
% dynamic~\cite{dynamic_embedding_2021},
%heterogeneous \cite{Heterogeneous_embedding_2018}, and knowledge graphs~\cite{WangMWG17}.
Sequential graph embedding techniques \cite{CaiZC18} fall into three categories.
Matrix factorization-based algorithms \cite{ProNE_2019,NetSMF_2019,NRP_2020,QiuDMLWT18,WangCWP0Y17}
%, such as ProNE \cite{ProNE_2019}, NetSMF \cite{NetSMF_2019}, and NRP \cite{NRP_2020},
%always
construct feature representations based on the adjacency or Laplacian matrix, and involve spectral techniques \cite{spectral_2001}.
%and nonlinear dimensionality \cite{nonlinear_2000}.
Graph neural networks (GNNs)-based approaches \cite{TuCWY018,WangC016,GraphSAGE_2017,Graph_attention_2018,Graphgan_2018}
%belong to another significant area that
focus on generalizing graph spectra into
semi-supervised or supervised graph learning. %, such as GraphSAGE \cite{GraphSAGE_2017}, Graph attention \cite{Graph_attention_2018} and GraphGAN \cite{Graphgan_2018}.
Both techniques incur high computational overhead and DRAM dependencies, limiting their scalability to large graphs.
Random-walk methods \cite{DeepWalk_2014,node2vec_2016,HuGE_2021,Line_2015,HuGE+_2022} transform a graph into a set of random walks through sampling and then adopt {\sf Skip-Gram} to generate embeddings.
%Examples include Deepwalk \cite{DeepWalk_2014}, node2vec \cite{node2vec_2016}, and HuGE \cite{HuGE_2021}.
They are %widely used in graph embeddings as they are
more flexible, parallel-friendly, and scale to larger graphs \cite{FlashMob_2021}.
{\sf HuGE+} \cite{HuGE+_2022} is a recent extension of {\sf HuGE} \cite{HuGE_2021}, which considers the information content of a node during the next-hop node selection to improve the downstream task accuracy. It uses the same {\sf HuGE} information-oriented method to determine the walk length and number of walks per node (\S \ref{sec:randGembedding}), and hence the efficiency is similar to that of {\sf HuGE}.
% {\sf VERSE} \cite{Verse_2018} learns node embeddings that preserve
% the distributions of a selected node-to-node similarity measure, by training a single-layer neural network. 
% However, these algorithms, being sequential, are not easily scalable to billion-edge graphs. 
% Additionally, we demonstrate that our %distributed system,
% proposed
% {\sf DistGER} can implement many of the random-walk-based embedding algorithms to improve their scalability (\S \ref{sec:generality}).

%Deepwalk \cite{DeepWalk_2014} is a pioneer work extending word2vec to graph feature learning.
%It generates sequences of nodes from a graph by uniform random walks.
%Though commonly used, this technique is limited in preserving properties for complex graph structures.
%Node2vec~\cite{node2vec_2016} introduces two hyperparameters to consider both homophily and structural equivalences in a biased random walk.
%Despite that it facilitates the flexibility in exploring node representations, it incurs a high cost and space overhead to laboriously tune
%hyperparameters. The very recent approach HuGE \cite{HuGE_2021} optimizes the one-size-fit-all static configuration and presents a
%heuristic random walk mechanism so as to achieve a concise and comprehensive representation in sampling procedure.
%Nevertheless, HuGE only stays at the method-level to facilitate efficiency and flexibility in exploring node representations on
%the strategy optimization, it has not yet been developed to be a general random-walk engine that provides system-level
%support for efficient and scalable graph embeddings.

\spara{Graph embedding systems and distributed embedding.} To address efficiency challenges with large graphs, %several systems for large-scale graph embedding have been developed. The
% {\sf MILE} \cite{liang2021mile} provides a multi-level embedding framework %using a hybrid matching technique
% via graph coarsening. 
recently proposed {\sf GraphVite} \cite{GraphVite_2019} and
Tencent's graph embedding system \cite{Tencent_GE_2020} follows sampling-based techniques on a
CPU-GPU hybrid architecture, %for node embedding,
simultaneously performing graph random
walks on CPUs and embedding training on GPUs. {\sf Marius} \cite{marius_2021} optimizes
data movements between CPU and GPU on a single machine for large-scale knowledge graphs embedding.
%{\sf Marius++} \cite{marius++_2022} further designs disk-optimized training and minimizes
%memory footprints of sampling for GNNs.
% {\sf GOSH} \cite{akyildiz2020gosh} implements a GPU-based single machine system that trains on a number of smaller and coarsened graphs.
{\sf Seastar} \cite{seastar_2021} develops
a novel GNN training framework on GPUs with a vertex-centric \cite{luo2023multi} programming model. %that enables users to easily develop GNN models by idiomatic Python syntax and provides high-performance operator fusion for GNN training.
We deployed {\sf DistGER} on GPU, but it does not provide a significant improvement, especially for large-scale graphs. Similar to the above works, computing gaps between CPUs and GPUs and the limited memory of GPUs still plague the efficiency of graph embedding.
% However, computing gaps between CPUs and GPUs and the limited memory of GPUs still plague the efficiency of graph embedding.

Other approaches attempt to scale graph embeddings from a distributed perspective.
%For instance,
{\sf HET-KG} \cite{dong2022het} 
% is a distributed system with a cache embedding table structure to reduce communication overheads for knowledge graph embedding.
is a distributed system for knowledge graph embedding. It introduces a cache embedding table to reduce communication overheads among machines.
% {\sf NuPS} \cite{NuPS_2022} is a recent approach that focuses on two major sources of non-uniform accesses for parameter management during the training procedure.
{\sf AliGraph} \cite{AliGraph_2019} optimizes sampling operators for distributed GNN training and reduces network communication by caching nodes
on local machines. Amazon has released {\sf DistDGL} \cite{DistDGL_2020}, a distributed graph embedding framework for GNN model with mini-batch training
based on the {\sf Deep Graph Library} \cite{dgl_2019}. {\sf Pytorch-Biggraph} \cite{PBG_2019} leverages graph partitioning and parameter servers
to learn large graph embeddings on multiple CPUs in a distributed environment based on {\sf PyTorch}.
However, the efficiency and scalability of {\sf DistDGL} and {\sf Pytorch-BigGraph} are affected by parameter synchronization
(as demonstrated in \S\ref{sec:experiments}). {\sf ByteGNN} \cite{ZhengCCSWLCYZ22} is a recently proposed
distributed system for GNNs, with mini-batch training and two-level scheduling to improve parallelism and resource utilization,
and tailored graph partitioning for GNN workloads. %to reduce network pressure and balance workloads.
Since {\sf ByteGNN} is not publicly available yet, we cannot compare it in our experiments.
% There are also approaches attempting to address computational efficiency challenges with new hardware \cite{Ginex_2022, SMARSAGE_2022, liu2022regnn}. Although these approaches open up opportunities for training larger datasets or providing more accelerations, their programming compatibility and prohibitive expensive still pose challenges.

There are also approaches attempting to address computational efficiency challenges with new hardware. {\sf Ginex} \cite{Ginex_2022} proposes an SSD-based GNN training system with feature vector cache optimizations that can support billion-edge graph datasets on a single machine. {\sf SmartSAGE} \cite{SMARSAGE_2022} develops software/hardware co-design based on an in-storage processing architecture for GNNs to overcome the memory capacity bottleneck of training. {\sf ReGNN} \cite{liu2022regnn} designs a ReRAM-based processing-in-memory architecture consisting of analog and digital modules to process GNN in different phases effectively. Although new hardware-based approaches open up opportunities for training larger datasets or providing more accelerations, their programming compatibility and prohibitive expensive still pose challenges.