
\section{Additional Experimental Results}

\subsection{Directed vs undirected and weighted vs unweighted graphs}

{\sf DistGER} handles undirected and unweighted graphs by default, but can also support directed and weighted (higher edge weights denoting more connectivity strengths) graphs.
%
Empirically 
since 
FL, YT, LJ, OR, and TW - these widely used graphs are unweighted, following {\sf KnightKing} \cite{KnighKing_2019} we created their weighted versions by assigning edge weights as real numbers uniformly at random sampled from [1, 5), Table \ref{weigthed_unweighted} exhibits end-to-end running times of {\sf DistGER} for all graphs (both unweighted and weighted versions). The running times of weighted versions are slightly higher compared to unweighted versions.
%
\begin{table}[h]
% \newcommand{\tabincell}[2]{\begin{tabular}{@{}#1@{}}#2\end{tabular}}
\vspace{-3mm}
  \caption{End-to-end running times of {\sf DistGER} on unweighted and weighted graphs (seconds)}
  \label{weigthed_unweighted}
  \begin{center}
  \begin{tabular}{ccc}
   \hline
    \bf{End-to-end time} & \bf{Unweighted} & \bf{Weighted}\\
    \hline
    {\bf FL}    &10.038            &11.585\\

    {\bf YT}    &49.982            &52.981\\

    {\bf LJ}    &70.143            &72.598\\

    {\bf OR}    &233.096           &258.966\\

    {\bf TW}    &2779.802          &2890.743\\
  \hline
\end{tabular}
\end{center}
%\vspace{-6mm}
\end{table}

The evaluated graphs in our experiment already included both undirected (FL, YT, and OR) and directed (LJ and TW) graphs. %However, considering the {\sf HuGE} and other existing popular random walk-based methods (such as Deepwalk and node2vec) use the undirected version by default, then the experiment results we follow the settings of the method themselves. Here we enforce {\sf DistGER} to support the directed graphs,
Table \ref{directed_undirected} shows running times of various components of {\sf DistGER} on directed and undirected versions of the {\em LiveJournal} (LJ) dataset. Due to the difference in graph structures, %(more edges in the undirected version),
we observed that the directed version requires more rounds of random walking per node ($r$ defined in Eq. 7) to achieve the convergence in effectiveness measurement (11 and 6 rounds in directed and undirected versions, respectively), and thus the sampling time in the directed version is more than that in the undirected version. In contrast, the directed version, due to less number of edges, has lower training time and also smaller memory footprint as the size of the generated corpus is smaller than that in the undirected version.
%
% \begin{table}[h]
% \newcommand{\tabincell}[2]{\begin{tabular}{@{}#1@{}}#2\end{tabular}}
%   \caption{Differences between the directed and undirected graph for DistGER on LiveJournal graph}
%   \label{directed_undirected}
%   \begin{center}
%   \begin{tabular}{ccccc}
%     \hline
%     { }&\multicolumn{2}{c}{\bfseries{ Running time (s)}}&\multicolumn{2}{c}{\bfseries{Memory usage (GB)}}\\
%     \hline
%     {\bf{Type}} &Sampling &Training &Sampling &Training \\
%     \hline
%      \bf{Undirected} & 24.062	&9.663 	&2.937 	&6.75 	\\

%      \bf{Directed} &30.790	&7.005 	&2.15 	&5.337 	\\
%   \hline
% \end{tabular}
% \end{center}
% \end{table}
% \end{itemize}
\begin{table}[h]
\newcommand{\tabincell}[2]{\begin{tabular}{@{}#1@{}}#2\end{tabular}}
\vspace{-2mm}
  \caption{Running times and memory usage of {\sf DistGER} on the directed and undirected versions of the {\em LiveJournal} dataset}
  \label{directed_undirected}
  \begin{center}
  \begin{tabular}{cccc}
    % \hline
    % { }& {\bf \#Edges} &\multicolumn{2}{c}{\bfseries{ Running time (s)}}&\multicolumn{2}{c}{\bfseries{Memory usage (GB)}}\\
    % \hline
    % {\bf{Type}} & &Sampling &Training &Sampling &Training \\
    % \hline
    %  \bf{Undirected} & & 24.062	&9.663 	&1.95 	&5.49 	\\

    %  \bf{Directed} &14608137 &30.790	&7.005 	&1.16	&4.07 	\\
    \hline
    { } & \bf{Undirected} &\bf{Directed} \\
    \hline
    {\bf \# Edges} &25,770,074  &1,460,813 \\
    {\bf Partitioning time} &21.257 s  &20.092 s  \\
    {\bf Sampling time} & 24.062 s & 30.790 s  \\
    {\bf Training time} & 9.663 s & 7.005 s \\
    {\bf Memory usage}  &7.43 GB &5.23 GB\\
  \hline
\end{tabular}
\end{center}
\vspace{-6mm}
\end{table}


\subsection{Memory usage}

We evaluated the memory usage of the sampling and training procedure for {\sf DistGER} on each machine (from a cluster of 8 machines) as shown in Table ~\ref{Memory_usage}. Compared with the same type of frameworks, {\sf KnightKing} and {\sf HuGE-D}, {\sf DistGER} requires fewer memory resources during sampling and training. While for the multi-relations- and graph neural network-based frameworks {\sf Pytorch-BigGraph} and {\sf DistDGL} in Table ~\ref{Memory_usage}(b), {\sf DistGER} is comparable and in most cases better than the competitors on the five real-world graphs.


\begin{table}[h!]
\newcommand{\tabincell}[2]{\begin{tabular}{@{}#1@{}}#2\end{tabular}}
  \caption{Avg. memory footprint (GB) of {\sf DistGER} and {\sf competitors} on each machine, where $\sigma$ is the standard deviation, and ``$-$'' means the framework fails under constraints (computation resources or time cost \textgreater $1$ day).}
  \label{Memory_usage}
  \begin{center}
  \renewcommand\arraystretch{1.5}
  \tabcolsep=0.08cm
  \scriptsize
 \begin{tabular}{cccccc}
    \hline
    \multicolumn{6}{c}{\small \bfseries{ (a) Sampling procedure for the random walk-based frameworks}}\\
    \hline
    {\bf{Graph}} &\sf{KnightKing} & \multicolumn{2}{c}{\sf{HuGE-D}} & \multicolumn{2}{c}{\sf{DistGER}} \\
    \hline
     \bf{FK} & 0.66($\pm$0.06)	&  \multicolumn{2}{c}{0.65($\pm$0.03)}     &  \multicolumn{2}{c}{\bf{0.41($\pm$0.02)}}\\

     \bf{YT} &4.11($\pm$0.55)	&  \multicolumn{2}{c}{3.18($\pm$0.39)}     &  \multicolumn{2}{c}{\bf{1.36($\pm$0.23)} }	\\

     \bf{LJ} & 7.65($\pm$0.82)	&  \multicolumn{2}{c}{4.07($\pm$0.72)}     &  \multicolumn{2}{c}{\bf{1.95($\pm$0.16)}	}\\

     \bf{CO} &10.98($\pm$1.03)	&  \multicolumn{2}{c}{6.43($\pm$0.26)}     &  \multicolumn{2}{c}{\bf{3.27($\pm$0.79)}} 	\\

     \bf{TW} & $-$	 	        &  \multicolumn{2}{c}{29.52($\pm$5.96)}     &  \multicolumn{2}{c}{\bf{20.18($\pm$3.62)}} 	\\
  \hline
    \multicolumn{6}{c}{\small \bfseries{ (b) Training procedure for \sf{{DistGER}} and \sf{competitors.}}}\\
    \hline
    {\bf{Graph}} &\sf{KnightKing} & \sf{HuGE-D} &\sf{DistDGL} &\sf{PBG}  & \sf{DistGER}\\
    \hline
     \bf{FK} &1.31($\pm$0.17)	&0.96($\pm$0.10)	&4.91($\pm$0.41)	&8.92($\pm$0.93)    & \bf{0.86($\pm$0.06)} \\

     \bf{YT} &4.73($\pm$0.72) 	&4.41($\pm$0.58)	&5.19($\pm$0.22)	&9.69($\pm$0.95)    & \bf{4.26($\pm$0.63)} 	 \\

     \bf{LJ} & 6.38($\pm$0.97)	&5.63($\pm$0.43)	&7.96($\pm$0.38)	&10.26($\pm$0.87)   & \bf{5.49($\pm$0.85)}	\\

     \bf{CO} &8.52($\pm$1.01)	&7.08($\pm$1.09)	&15.27($\pm$1.12)	&11.37($\pm$1.39)    & \bf{6.86($\pm$0.69) } 	\\

     \bf{TW} & $-$	            &73.72($\pm$7.07)	& $-$	            &\bf{31.65($\pm$4.61)}     & {67.16($\pm$5.18)} \\
  \hline
\end{tabular}
\end{center}
%\vspace{-6mm}
\end{table}

\subsection{Varying $\gamma$}


\begin{figure}[h!]
  \centering
  \includegraphics[width= 3.2 in]{./Figures/MPGP_gama.eps}
  \caption{(a) Number of assigned nodes to each partition and (b) average random walk time with varying $\gamma$}
  \label{MPGP_gama}
\end{figure}

$\gamma$ is a slack parameter that allows deviation from the exact load balancing. Figure \ref{MPGP_gama} exhibits how different $\gamma$ affects the quality and load balancing on {\em LiveJournal} (LJ) dataset. We observed that although the partition scheme produces a strict load-balancing at $\gamma=1$, the contribution to the random walk is less due to the low utilization of local partitions, while a larger $\gamma$ (such as $\gamma=10$) introduces skewed workloads for computing machines, which also affects the partition quality and increases the average random walk time. Thus, there is a trade-off between partition quality and load-balancing. We set $\gamma=2$ since it produces the minimum average random walk time.

%Although setting a strict $\gamma$ will ensure load-balancing, it may hamper the partition quality, resulting in lower utilization of the local partition. Meanwhile, setting a larger $\gamma$ will relax the load-balancing constraint, and would create skewed partitioning. Thus, there is a trade-off between partition quality and load-balancing.


\subsection{DistGER combined with GPUs}


Each machine in the cluster is equipped with a GPU (NVIDIA GeForce RTX 3090, 24GB memory). We deployed {\sf DistGER}'s learner on GPUs using CUDA V11.2.152.
%We deployed the proposed {\sf DistGER} on GPU (equipped with NVIDIA GeForce RTX 3090 (24GB memory) and implemented it by using CUDA V11.2.152). Table \ref{DistGER_GPU} reports the training performance of the GPU version on four computing nodes.
In Table \ref{DistGER_GPU}, we found that this GPU version (called  {\sf DistGER-GPU}) does not provide a significant improvement compared to {\sf DistGER}. For small graphs, {\sf DistGER-GPU} attains minor speedups; while for large graphs (e.g., {\em Twitter}), the performance is far from impressive due to higher memory consumption of training (refer to Table \ref{Memory_usage}), which exceeds the memory capacity of GPUs, resulting in large data movements between main memory and GPUs.
As we discussed in related work, computing gaps between CPUs and GPUs and the limited memory of GPUs plague the efficiency of graph embedding. 
%In the future, we shall look into GPU-CPU computing bottlenecks for graph embedding, such as reducing data movements between heterogeneous computations \cite{marius_2021} and leveraging a novel storage format \cite{hosny2021sparse} to reduce GPU footprints.

\begin{table}[h!]
% \newcommand{\tabincell}[2]{\begin{tabular}{@{}#1@{}}#2\end{tabular}}
\vspace{-3mm}
  \caption{{\sf DistGER} training time (seconds) in combination with GPU ({\sf DistGER-GPU}) on four computing nodes}
  \label{DistGER_GPU}
  \begin{center}
  \begin{tabular}{ccc}
   \hline
    \bf{Graph} & \bf{DistGER} & \bf{DistGER-GPU}\\
    \hline
    {\bf FL}    &1.791            &0.653\\

    {\bf YT}   &27.529            &20.451\\

    {\bf LJ}    &29.791            &27.835\\

    {\bf OR}    &51.959           &46.341\\

    {\bf TW}   &299.896          &390.081\\
  \hline
\end{tabular}
\end{center}
\vspace{-6mm}
\end{table}
