\begin{table*}[t!]
\begin{minipage}{0.175\linewidth}
\centering
% \hspace{1.8mm}
\captionof{table}{\small Datasets statistics \label{graph_datasets}}
\begin{tiny}
\begin{tabular}{c||c|c}
      {\bf Graph} & {\bf \#nodes} & {\bf \#edges} \\ \hline
      {\em FL} & 80\,513     & 5\,899\,882 \\
      {\em YT} & 1\,138\,499 & 2\,990\,443 \\
      {\em LJ} & 2\,238\,731 &14\,608\,137 \\
      {\em OR} & 3\,072\,441 & 117\,185\,083 \\
      {\em TW} & 41\,652\,230 & 1\,468\,365\,182 \\
\end{tabular}
\end{tiny}
\end{minipage}%
 \quad
 \begin{minipage}{.265\linewidth}
\centering
\tabcolsep=0.05cm

\captionof{table}{\small Avg. memory footprint (GB) of {\sf DistGER} and {\sf KnightKing} on each machine, where $\sigma$ is the standard deviation}
\label{Memory_usage}
\begin{tiny}
\newcommand{\tabincell}[2]{\begin{tabular}{@{}#1@{}}#2\end{tabular}}
  % \caption{\small {\color{blue} Avg. memory footprint (GB) of {\sf DistGER} and {\sf KnightKing} on each machine, where $\sigma$ is the standard deviation.}}
  \begin{tabular}{c|cc|cc}
    %\hline
    { }&\multicolumn{2}{c|}{\bfseries{ Sampling}}&\multicolumn{2}{c}{\bfseries{Training}}\\
    \hline
    {\bf{Graph}} &{\sf KnightKing} &{\sf DistGER} &{\sf KnightKing} &{\sf DistGER} \\
    \hline
     {\em FL} & 0.66($\pm$0.06)	&{\bf 0.41($\pm$0.02)}	&1.31($\pm$0.17) 	&{\bf 0.86($\pm$0.06)} 	\\

     {\em YT} &4.11($\pm$0.55)	&{\bf 1.36($\pm$0.23)} 	&4.73($\pm$0.72) 	&{\bf 4.26($\pm$0.63)} \\

     {\em LJ} & 7.65($\pm$0.82)	&{\bf 1.95($\pm$0.16)}	&6.38($\pm$0.97) 	&{\bf 5.49($\pm$0.85)} 	\\

     {\em CO} &10.98($\pm$1.03)	&{\bf 3.27($\pm$0.79)} 	&8.52($\pm$1.01) 	&{\bf 6.86($\pm$0.69)} 	\\

     {\em TW} & out-of-memory	&{\bf 20.18($\pm$3.62)} 	&out-of-memory 	& {\bf 67.16($\pm$5.18)} 	\\
  %\hline
\end{tabular}
\end{tiny}

\end{minipage}
\quad
\begin{minipage}{.25\linewidth}
    \centering
    \includegraphics[width= 1.85in, height = 1.2in]{./Figures/Dist_total_time_partition.eps}%
    \captionof{figure}
      {\small Efficiency: {\sf PBG} \cite{PBG_2019}, {\sf DistDGL} \cite{DistDGL_2020}, {\sf KnightKing} \cite{KnighKing_2019}, {\sf HuGE-D} (baseline), {\sf DistGER} (ours)
        \label{overall_performance}
      }
\end{minipage}%\hfill
\quad
\begin{minipage}{.25\linewidth}
    \centering
    \includegraphics[width= 1.85in, height = 1.2in]{./Figures/Dist_scalability_partition.eps}%
    \captionof{figure}
      {\small Scalability: {\sf PBG} \cite{PBG_2019}, {\sf DistDGL} \cite{DistDGL_2020}, {\sf KnightKing} \cite{KnighKing_2019}, {\sf HuGE-D} (baseline), {\sf DistGER} (ours)
        \label{Dist_scalability}
      }
\end{minipage}
\end{table*}


\section{Experimental Results}
\label{sec:experiments}
We evaluate the efficiency (\S \ref{sec:overall}) and scalability (\S \ref{sec:scalability}) of our proposed method, {\sf DistGER}
by comparing with {\sf HuGE-D} (baseline),
{\sf KnightKing} \cite{KnighKing_2019}, {\sf PyTorch-BigGraph} ({\sf PBG}) \cite{PBG_2019}, and {\sf Distributed DGL}
({\sf DistDGL}) \cite{DistDGL_2020}. We also compare the effectiveness (\S \ref{sec:effectiveness}) of generated embeddings
on link prediction.
% and multi-label classification tasks. 
Finally, we analyze efficiency due to individual
parts of {\sf DistGER} (\S \ref{sec:individual})
and the generality of {\sf DistGER} for other random walk-based embeddings (\S \ref{sec:generality}).
Our codes and datasets are at \cite{code}.
%
\subsection{Experimental Setup}
\label{sec:setup}
%
\spara{Environment.} We conduct experiments on a cluster of 8 machines with 2.60GHz Intel $^\circledR$ Xeon $^\circledR$ Gold 6240 CPU with 72 cores (hyper-threading)
in a dual-socket system, and each machine is equipped with 192GB DDR4 memory and connected by a 100Gbps network.
The machines run Ubuntu 16.04 with Linux kernel 4.15.0. We use GCC v9.4.0 for compiling {\sf DistGER}, {\sf KnightKing}, and {\sf HuGE-D},
and use Python v3.6.15 and torch v1.10.2 as the backend deep learning framework for {\sf Pytorch-BigGraph} and {\sf DistDGL}.

\spara{Datasets. } We employ five widely-used, real-world graphs
(Table~\ref{graph_datasets}): {\em Flickr} (FL) \cite{Flickr_Youtube_Graph},
{\em Youtube} (YT) \cite{Flickr_Youtube_Graph},
{\em LiveJournal} (LJ) \cite{BlogCatalog_Twitter_LiveJournal_Graph},
{\em Com-Orkut} (OR) \cite{com-orkut_2012}, and {\em Twitter} (TW) \cite{twitter_2010}.
The first two graphs are selected for multi-label node classification with distinct number of node labels 195 and 47, respectively, %in {\em Flickr} and {\em Youtube},
where labels in {\em Flickr} represent interest groups of users, and {\em Youtube}'s labels represent groups of viewers that enjoy common video genres. The last four graphs are used in link prediction. We also use synthetic graphs \cite{RMAT_2004} (up to 1 billion nodes, 10 billion edges) and a real-world {\em UK graph} \cite{BSVLTAG} (100M nodes, 3.7B edges) to assess the scalability of {\sf DistGER}.
Considering the default settings of popular random walk-based methods (e.g., Deepwalk, node2vec, HuGE), we use their undirected version.

\spara{Competitors.} We compare {\sf DistGER} against three state-of-the-art distributed graph embedding frameworks: the distributed random walk engine, {\sf KnightKing} {\scriptsize\url{https://github.com/KnightKingWalk/KnightKing}}
\cite{KnighKing_2019}; the distributed multi-relations based graph embedding system, {\sf PyTorch-BigGraph} ({\sf PBG})
{\scriptsize\url{https://github.com/facebookresearch/PyTorch-BigGraph}} \cite{PBG_2019} -- designed by Facebook; and
the distributed graph neural networks-based system, {\sf DistDGL} {\scriptsize\url{https://github.com/dmlc/dgl}}
\cite{DistDGL_2020} -- recently proposed by Amazon. We also implement {\sf HuGE-D}, a distributed version of
information-centric random walk-based graph embedding ({\sf HuGE} \cite{HuGE_2021}), on top of {\sf KnightKing},
served as our baseline. Since {\sf KnightKing} and {\sf HuGE-D} provide distributed support only for
random walk without that for embedding learning, we generate their node embeddings using
{\sf Pword2vec} {\scriptsize\url{https://github.com/IntelLabs/pWord2Vec}} \cite{Pword2vec_2019},
the most popular distributed {\sf Skip-Gram} system released by Intel.
%We find that {\sf pSGNScc} \cite{pSGNSCC_2017} (\S \ref{sec:learning})
%only provides a single-machine implementation, thus we do not include it in our distributed experiments.

\spara{Parameters.} For {\sf DistGER} and {\sf HuGE-D} random walks, we set
parameters $\mu$=0.995, $\delta$=0.001 based on information measurements (\S \ref{sec:preliminaries}),
while {\sf KnightKing} uses $L$=80 and $r$=10 that are routine configurations in the traditional
random walk-based graph embedding \cite{node2vec_2016, DeepWalk_2014, KnighKing_2019}. For {\sf DistGER}, {\sf KnightKing}, and {\sf HuGE-D} training,
we set the sliding window size $w$=10, number of negative samples $K$=5, and synchronization period=0.1 sec \cite{Pword2vec_2019},
and additionally, multi-windows number=2, $\gamma$=2 for {\sf DisrGER}.
%For {\sf Pytorch-BigGraph} ({\sf PBG}), we set the number of partitions to 16 following \cite{PBG_2019}, that is, using $2m$ partitions for the number of machines $m$ = 8
%in our case. %For {\sf DistDGL}, the deployed {\sf GaphSAGE} model uses three graph convolutional layers.
For fair comparison across all systems, %the efficiency performance of all systems involved in the experiments,
we set the embedding dimension $d$=128 that is commonly used \cite{HuGE_2021,node2vec_2016,DeepWalk_2014,Line_2015,Verse_2018,ProNE_2019},
and report the average running time for each epoch. For task effectiveness evaluations,
we find the best results from a grid search over learning rates from 0.001-0.1, \# epochs from 1-30,
and \# dimensions from 128-512.


%
\eat{
\begin{table}
\newcommand{\tabincell}[2]{\begin{tabular}{@{}#1@{}}#2\end{tabular}}
  \caption{\small Avg. memory footprint (GB) of {\sf DistGER} and {\sf KnightKing} on each machine, where $\sigma$ is the standard deviation.}
  \label{Memory_usage}
  \begin{center}
   \footnotesize
  \begin{tabular}{c|cc|cc}
    %\hline
    { }&\multicolumn{2}{c|}{\bfseries{ Sampling}}&\multicolumn{2}{c}{\bfseries{Training}}\\
    \hline
    {\bf{Graph}} &{\sf KnightKing} &{\sf DistGER} &{\sf KnightKing} &{\sf DistGER} \\
    \hline
     {\em Flickr} & 0.66($\pm$0.06)	&{\bf 0.41($\pm$0.02)}	&1.31($\pm$0.17) 	&{\bf 0.86($\pm$0.06)} 	\\

     {\em Youtube} &4.11($\pm$0.55)	&{\bf 1.36($\pm$0.23)} 	&4.73($\pm$0.72) 	&{\bf 4.26($\pm$0.63)} \\

     {\em LiveJournal} & 7.65($\pm$0.82)	&{\bf 1.95($\pm$0.16)}	&6.387($\pm$0.97) 	&{\bf 5.49($\pm$0.85)} 	\\

     {\em Com-Orkut} &10.98($\pm$1.03)	&{\bf 3.27($\pm$0.79)} 	&8.52($\pm$1.01) 	&{\bf 6.86($\pm$0.69)} 	\\

     {\em Twitter} & out-of-memory	&{\bf 37.1($\pm$5.28)} 	&out-of-memory 	& {\bf 79.5($\pm$7.27)} 	\\
  %\hline
\end{tabular}
\end{center}
\end{table}
%
}
%


%
\subsection{Efficiency and Memory Use w.r.t. Competitors}
\label{sec:overall}
%\begin{figure}
%  \centering
%  \includegraphics[width= 3 in]{Dist_total_time.eps}
%  \caption{\small Overall performance of PBG, DistDGL, KnightKing, HuGE-D and DistGER for generating embeddings on different read-word graphs, {\color{blue}for Twitter graph, DistDGL cannot finish in one day, and KnightKing fails to perform due to memory issue, where the y axis is in log-scale.}}
%  \label{overall_performance}
%\end{figure}
%
We report the end-to-end running times of {\sf PBG}, {\sf DistDGL}, {\sf KnightKing}, {\sf HuGE-D}, and {\sf DistGER}
on five real-world graphs with the cluster of 8 machines in Figure~\ref{overall_performance}.
The reported end-to-end time includes the running time of partitioning, random walks (for random walk-based frameworks), and training procedures.
%{\color{blue} Noted that the reported end-to-end time in our experiments excludes the partition time for all evaluated frameworks due to the all used partition schemes are executed as a preprocessing component, and we separately evaluate the partition efficiency in Section 6.5, thus the end-to-end time refers to the running time of random walk (only for random walk-based framework) and training procedure.}
{\sf DistGER} significantly outperforms the competitors
on all these graphs, achieving a speedup ranging from 2.33$\times$ to 129$\times$. %, by an average acceleration of $39.78 \times$.
Recall that {\sf DistGER} is a similar type of system as {\sf KnightKing} and {\sf HuGE-D},
and our key improvements are discussed in \S \ref{sec:DistGER} and in \S \ref{sec:learning}.
Analogously, Figure~\ref{overall_performance} exhibits that our system, %designs are more effective (see more evaluation details in \S 6.3),
{\sf DistGER} achieves an average speedup of 9.25$\times$ and 6.56$\times$ compared with {\sf KnightKing} and {\sf HuGE-D}.
Notice that we fail to run {\sf KnightKing} on the largest {\em Twitter} dataset
because its routine random walk strategy requires more main memory space.
%Although Huge-D achieves comparable performance,
The advantage of information-centric random walk in {\sf HuGE} is almost wiped out in {\sf HuGE-D}
due to on-the-fly information measurements and the higher communication costs in a distributed setting.
The multi-relation-based {\sf PBG} leverages a parameter server to synchronize embeddings between clients,
resulting in more load on the communication network. As a result, {\sf PBG} is on average
26.22$\times$ slower than {\sf DistGER}. For graph neural network-based system {\sf DistDGL},
due to the long running time of graph sampling (e.g., taking 80\% of the overhead for the {\sf GraphSAGE}),
it is highly inefficient than other systems. For the billion-edge {\em Twitter} graph, it does not terminate in 1 day.
%
%Considering the resource consumption that affects scalability,
{Table ~\ref{Memory_usage}} shows {\sf DistGER}'s average memory footprint on each machine of the 8-machine cluster. %from a cluster of 8 machines.
%and the standard deviation %($\sigma$) %of the results
%in 
%Table ~\ref{Memory_usage}. 
Compared
to %other methods, %with the 
same type of system
{\sf KnightKing}, 
% that is of the same system type, 
{\sf DistGER} requires less memory for sampling and training.


\subsection{Scalability w.r.t. Competitors}
\label{sec:scalability}
%
%\begin{figure}
%  \centering
%  \includegraphics[width= 3.2 in]{Dist_scalability.eps}
%  \caption{\small Scalability comparison on LiveJournal graph, where the y axis is in log-scale.}
% \label{Dist_scalability}
%\end{figure}
%
Figure~\ref{Dist_scalability} shows end-to-end running times of all competing
systems on the {\em LiveJournal} graph, as we increase \# machines
from 1 to 8 to evaluate scalability. {\sf DistGER} achieves better scalability than the other
four distributed systems.
%Due to space limitation, we omit results on other graph datasets,
%which exhibit similar trends.
{\sf PBG} leverages a parameter server and a shared network filesystem
to synchronize the parameters in the distributed model. %The edges are partitioned into $m^2$ buckets
%and training can be performed in parallel using up to $m/2$ machines. After one bucket completes
%the training, it needs to communicate with the parameter server.
When the number of machines increases, {\sf PBG} puts more load
on the communications network, resulting in poor scalability. Likewise, {\sf DistDGL}
is bounded by the synchronization overhead for gradient updates,
limiting its scalability.
%Since {\sf DistDGL} uses mini-batches for sampling, %features %for GraphSAGE,
%if the mini-batch samples cannot be generated on time, the trainer will be delayed on the forward pass, and all other
%machines need to wait before starting the backward pass. Thus, increasing the number of
%machines also affects the efficiency of backward pass. %Being the random walk-based distributed systems,
Both {\sf KnightKing} and {\sf HuGE-D} suffer from higher communication costs during random walks,
due to their only workload-balancing partitioning scheme (\S \ref{sec:dRand}, \S \ref{sec:individual}).
%Their scalability is relatively poor as the number of machines increases.
%{\sf KnightKing} partitions the graph by a workload-balancing scheme, inevitably introducing higher
%cross-machine communications due to the randomness inherent in the random walking procedure (\S \ref{sec:dRand}, \S \ref{sec:individual}).
%With more machines, the inefficiency of the partitioning scheme is further magnified.
Since {\sf HuGE-D} is implemented on top of {\sf KnigtKing},
it exhibits worse scalability due to high communication costs and on-the-fly information measurements in a distributed setting (\S \ref{sec:HUGED}).
%In contrast, its performance is much better than all the competitors in a single machine.
In comparison, {\sf DistGER} incorporates multi-proximity-aware streaming graph partitioning and incremental computations
to reduce both communication and computation costs, it also employs hotness-block based parameters synchronization
during training to dramatically reduce the pressure on network bandwidth. Hence, {\sf DistGER} achieves better scalability than other systems.
Due to space limitations, we omit {\sf DistGER}'s scalability results on other graphs, which exhibit similar trends. On {\em Twitter}, the end-to-end running times {\sf DistGER} on 1, 2, 4, and 8 machines are 3090s, 1739s, 1197s, and 746s, respectively,
while on {\em Com-Orkut}, the results are 304s, 204s, 149s, and 89s, respectively. 
The results show a good linear relationship.
% The results demonstrate a desired scalability with the increase of the machines.

\begin{table}
\quad
\begin{minipage}{0.46\linewidth}
    \centering
    \includegraphics[width= 1.6 in]{./Figures/Dist_scalability_datasize.eps}%
    \captionof{figure}
      {\small {Scalability of {\sf DistGER} on synthetic graphs, where Y-axis is in log-scale}}
      %The lines depict the running time required for random walk (blue line) and training (red line), respectively. Pentagrams show the time cost of six real-world graphs,
        \label{Dist_scalability_data}
      
\end{minipage}\hfill
\quad
\begin{minipage}{.46\linewidth}
    \centering
    \includegraphics[width= 1.6 in]{./Figures/Dist_time_auc.eps}%
    \captionof{figure}
      {\small {The influence of running time on embedding quality for {\sf DistGER} and competitors}}
        \label{Dist_time_auc}
\end{minipage}
\end{table}


To further assess the scalability of {\sf DistGER}, we generate synthetic graphs \cite{RMAT_2004} with a fixed node degree of 10 and the number of nodes from $10^5$ to $10^9$. Figure~\ref{Dist_scalability_data} presents the running times for random walks and training on these synthetic graphs using a cluster of 8 machines, suggesting that the running time increases linearly with the size of a graph, and {\sf DistGER} has the capability to handle even billion-node graphs. Moreover, the running times for six real-world graphs (including the {\em UK graph} with $|E|=3.7B$, $|V|=100M$, for which the competing systems do not terminate in 1 day or crash due to hardware and memory limitation) are inserted into the plot, which is consistent with the trend on synthetic data.

%
%
\subsection{Effectiveness w.r.t. Competitors}
\label{sec:effectiveness}
%
\spara{Link prediction.} To perform link prediction on a given graph $G$, following \cite{HuGE_2021,node2vec_2016,Verse_2018,NRP_2020},
we first uniformly at random remove 50\% edges as positive test edges, and the rest are used as positive training edges.
We also provide negative training and test edges by considering those node pairs between which no edge exists in $G$.
We ensure that the positive and negative set sizes are similar. %For a pair of nodes $(u, v)$, let $\varphi(u)$ and
%$\varphi(v)$ be the vectors learned by embedding methods.
The link prediction is conducted as a classification task
based on the similarity of $u$ and $v$, i.e., $\varphi(u)\cdot\varphi(v)$.
The effectiveness of link prediction is measured via the $AUC$ (Area Under Curve) score \cite{AUC_kdd} -- the higher the better.
We repeat this procedure 50 times to offset the randomness of edge removal and report the average $AUC$ in
Table~\ref{AUC_results}.
%shows $AUC$ for all the methods on five real-world graphs.
%, respectively, where a ``$-$'' indicates that the method fails due to the limitation of computing resources or because its running time exceeds 1 day.
{\sf DistGER} outperforms all competitors on these graphs, except for {\sf PBG} on {\em Com-Orkut}, where {\sf DistGER} ranks second.
On average, {\sf DistGER} has an 11.7\% higher $AUC$ score compared with the other three systems, thanks to our
information-centric random walks. {\sf PBG} is the best on {\em Com-Orkut} because this graph is much denser
and is friendly to the multi-relationship-based model in {\sf PBG}.
Figure~\ref{Dist_time_auc} exhibits accuracy-efficiency tradeoffs of {\sf DistGER} and competitors, i.e., their $AUC$ convergence curves w.r.t. increasing running times of random walks and training, over {\em LiveJournal}, further indicating
that {\sf DistGER} has better efficiency and effectiveness than the competitors.
%As a system of the same type, DistGER achieves better accuracies on all graphs than KnightKing which leverages the routine random walk configuration, thanks to its information-centric random walk strategies. We do not report the effectiveness of HuGE-D here because it uses the same random walk model as DistGER.
%
\begin{table}[h!]
\newcommand{\tabincell}[2]{\begin{tabular}{@{}#1@{}}#2\end{tabular}}
  \caption{\small $AUC$ scores of {\sf DistGER} and competitors for link prediction}
  \label{AUC_results}
  \begin{center}
  \footnotesize
  \begin{tabular}{cccccc}
%    \hline
    {Method}&\tabincell{c}{Youtube}&{LiveJournal}&\tabincell{c}{Com-Orkut}&{ Twitter}\\
    \hline
    {\sf PBG}        & 0.753           &0.882            &\bfseries{0.955} &0.912\\

    {\sf DistDGL}    &0.894            &0.718            &0.815            & running time $>$ 1 day \\

    {\sf KnightKing} &0.904            &0.963            & $0.918$         & out-of-memory\\

    {\sf DistGER}    &\bfseries{0.966} &\bfseries{0.976} &0.921            &\bfseries{0.919}\\
%  \hline
\end{tabular}
\end{center}
\end{table}

% \eat{
\spara{Multi-label node classification.}
This task predicts one or more labels for each graph node and has applications in %modern applications ranging from
text categorization \cite{zhang2006multilabel} and bioinformatics \cite{zhang2018ontological}.
We use embedding vectors and a one-vs-rest logistic regression classifier
with L2 regularization \cite{MLC_LIBLINEAR_2008}, %(using the LIBLINEAR library),
then evaluate the effectiveness by micro-averaged F1 ($Micro-F1$) and macro-averaged F1 ($Macro-F1$) \cite{WangC016}
scores, where $Micro-F1$ gives equal weight to each test instance and $Macro-F1$ assigns equal weight to each label category \cite{keikha2018community}.
%To train a classifier, nodes are uniformly at random split into training and test sets.
Following \cite{HuGE_2021,node2vec_2016,DeepWalk_2014,Line_2015,Verse_2018},
we select 10\% to 90\% training data ratio on {\em Flickr}, and 1\% to 9\% training ratio on {\em Youtube}.
%and the remaining nodes for testing.
We report the averaged $Macro-F1$ and $Micro-F1$ scores from 50 trials in Figure~\ref{Dist_MLC_mac_mic_F1}.
% shows the $Macro-F1$ and $Micro-F1$ scores achieved by each system as a
%function of the training ratio variation, respectively.
We find that {\sf DistGER} has better $Macro-F1$ and $Micro-F1$ scores
than existing frameworks, %on these graphs, %. In particular, compared with the KnightKing,
%DistGER consistently outperforms the other random walk-based systems on all graphs in $Macro-F1$ and $Micro-F1$ scores,
gaining 9.2\% and 3.3\% average improvements, respectively, due to its more effective information-centric random walks.
%Definition of $Macro-F1$ and $Micro-F1$ are as the following:

\begin{figure}[h!]
  \centering
  \includegraphics[width= 3.45 in]{./Figures/Dist_MLC_mac_mic_F1_1.eps}
  \caption{\small $Macro-F1$ (a1, b1) and $Micro-F1$ (a2, b2) scores for multi-label node classification. $X$-axis: training data ratio}
  \label{Dist_MLC_mac_mic_F1}
\end{figure}

% }
%\begin{equation}
%Precision = \frac{\sum\nolimits_{i}^{K}TP(i)}{\sum\nolimits_{i}^{K}(TP(i)+FP(i))}
%\end{equation}
%
%\begin{equation}
%Recall = \frac{\sum\nolimits_{i}^{K}TP(i)}{\sum\nolimits_{i}^{K}(TP(i)+FN(i))}
%\end{equation}
%
%\begin{equation}
%Micro-F1 = \frac{2\times Precision\times Recall}{Precision+Recall}
%\end{equation}
%
%\begin{equation}
%Macro-F1 = \frac{\sum\nolimits_{i}^{K}Micro-F1(i)}{|K|}
%\end{equation}
%
%where $TP(i)$, $FP(i)$ and $FN(i)$ are the number of true positives, false positives and false negatives in the instances which are predicted as $i$, respectively. Suppose $K$ is the overall label set, $Micro-F1$($i$) and $Macro-F1$ are the measure of $Micro-F1$ and $Macro-F1$ for the label $i$, respectively.
%\begin{table}
%\setlength{\abovecaptionskip}{0.cm}
%\setlength{\belowcaptionskip}{-0.cm}
%\newcommand{\tabincell}[2]{\begin{tabular}{@{}#1@{}}#2\end{tabular}}
%  \caption{$Macro-F1$ and $Micro-F1$ for multi-label classification on Flickr and Youtube graph, where train ratio is 0.5.}
%  \label{cluster_results}
%  \begin{center}
%  \small
%  \begin{tabular}{ccccc}
%    \hline
%    { }&\multicolumn{2}{c}{\bfseries{ \scriptsize Flickr}}&\multicolumn{2}{c}{\bfseries{\scriptsize Youtube}}\\
%    \hline
%    { }&Macro-F1 &Micro-F1&Macro-F1 &Micro-F1\\
%
%    \hline
%    \small PBG & 0.225	&0.387 	&0.295 	&0.406 	\\
%
%    \small DistDGL &0.205 	&0.378 	&0.283 	&0.403 	\\
%
%    \small KnightKing &0.239    &0.386 &0.285 	&0.402 	\\
%
%    \small DistGER  &\bfseries{0.277} &\bfseries{0.409}&\bfseries{0.298} &\bfseries{0.417}\\
%
%  \hline
%\end{tabular}
%\end{center}
%\end{table}
%

\begin{figure}
  \centering
  \includegraphics[width= 3.2 in]{./Figures/Dist_sampling_training_Mpad_efficiency.eps}
  \caption{\small {(a) Random walk efficiency, (b) training efficiency, (c) \# cross-machine messages, (d) random walk efficiency for {\sf MPGP} (ours) and workload-balancing scheme ({\sf KnightKing})}}
  \label{Dist_efficiency_sampling_training_MPGP}
\end{figure}

\begin{table*}[t!]
\begin{minipage}{0.275\linewidth}
\centering
\renewcommand\arraystretch{1.2}
\captionof{table}{\small Performance evaluation of partitioning for {\sf DistGER} and Competitors } %{\sf PBG} and {\sf DistDGL}
\label{Partition_sechme_overhead}
\begin{scriptsize}
\begin{tabular}{ccccc}
    \multicolumn{5}{c}{{\bfseries (a) Partitioning time for {\sf DistGER} and competitors }} \\
    \hline
    {\bf graph} & {\sf PBG} & {\sf DistDGL} & {\sf DistGER}\\
                &           & ({\sf METIS}) &  ({\sf MPGP}) \\
    \hline
    {\sf FL} & 383.28 s & 127.72 s & \bfseries{15.96 s} \\
    {\sf YT} & 349.15 s & 116.30 s & \bfseries{13.56 s} \\
    {\sf LJ} & 458.52 s & 425.19 s & \bfseries{36.42 s} \\
    {\sf OR} & 2662.62 s & 2761.25 s &\bfseries{294.68 s}\\
    {\sf TW} & 22 hour s & $>$ 1 day &\bfseries{9 hours}\\
    \hline
%    \multicolumn{5}{c}{} \\
    \multicolumn{5}{c}{{\bfseries (b) Evaluation of {\sf Parallel MPGP} }} \\
    \hline
    {\bf graph} & {\sf Streaming} & {\sf Partitioning} & {\sf Walking}\\
    \hline
  %  {\sf MPGP}   &DFS+deg  & 9 hours & \bfseries{575.22 s} \\
    \multirow{2}{*}{\sf LJ} &DFS+deg & 21.86 s & \bfseries{23.78 s} \\
           & BFS+deg & \bfseries{21.25 s} & 24.79 s \\
    \multirow{2}{*}{\sf OR} &DFS+deg & \bfseries{151.29 s} & 77.12 s \\
           & BFS+deg & 156.37 s & \bfseries{46.55 s} \\
    \multirow{2}{*}{\sf TW} &DFS+deg & \bfseries{1940.65} s & 683.81 s \\
           & BFS+deg & 2034.21 s & \bfseries{590.36 s}
\end{tabular}
\end{scriptsize}
\end{minipage}%\hfill
\quad
\begin{minipage}{.3\linewidth}
    \centering
    \includegraphics[width= 2.5in, height = 1.45 in]{./Figures/Dist_Mpad_streaming_vertex_time.eps}%
    \captionof{figure}
      {\small The distribution of local computations and cross-machine communications for different streaming orders on {\em LiveJournal}. The top table reports their running times for partitioning and random walks
        \label{Dist_MPaD_streaming}
      }
\end{minipage}%\hfill
\qquad
\begin{minipage}{.37\linewidth}
    \centering
    \includegraphics[width= 2.5 in, height = 1.45 in]{./Figures/Dist_generality_table_HuGE+.eps}%
    \captionof{figure}
      {\small Generality of {\sf DistGER} vs. {\sf KnightKing}. The bars show random walk efficiency ($-R$) and training efficiency ($-T$) for {\sf Deepwalk} ({\sf DW}), {\sf node2vec} ({\sf n2v}) and {\sf HuGE+}. The top table shows the ratio $\frac{\text{{\em AUC} for {\sf DistGER}}}{\text{{\em AUC} of {\sf KnightKing}}}$, with {\sf DW} and {\sf n2v}, task: link prediction
        \label{Dist_generality}
      }
\end{minipage}
\end{table*}


\subsection{Efficiency due to Individual Parts of DistGER}
\label{sec:individual}
\spara{Random walk and training efficiency.}
To evaluate the system design of {\sf DistGER} (\S \ref{sec:DistGER}, \S \ref{sec:learning}),
we first compare the efficiency of random walks and training with those of {\sf KnighKing} and {\sf HuGE-D}.
%For fair comparison, the running times that we reported for {\sf KnightKing} and {\sf HuGE-D} exclude the
%time of vocabulary table construction, since it is a serial process in {\sf Pwode2vec}, while {\sf DistGER}
%pipelines the construction during random walks.
For random walks (Figure~\ref{Dist_efficiency_sampling_training_MPGP}(a)),
{\sf DistGER} significantly outperforms {\sf KnightKing} and {\sf HuGE-D} on all our graph
datasets, achieving an average speedup of $3.32\times$ and $3.88\times$, respectively.
Although {\sf HuGE-D} implements information-oriented random walks on {\sf KnightKing},
due to additional computation and communication overheads during on-the-fly information
measurements (\S \ref{sec:HUGED}), its efficiency can be lower than that of {\sf KnightKing}.
We also notice that the random walk lengths ($L$) and the number of random walks ($r$) reduce (on average)
63.2\% and 18\%, respectively, in our information-oriented random walks, compared to {\sf KnightKing}'s
routine random walk configuration.
%which supports the traditional
%random walk methods. %To provide a straightforward adaptation for the information-oriented approach, DistGER leverages the incremental information-centric computation mechanism to mitigate the redundant computation and high communication cost in HuGE-D, then it achieves an average speedup of $3.32\times$ and $3.88\times$ in random walk procedure compared to KnightKing and HuGE-D.

Another benefit of information-centric random walks is that it generates concise and effective corpus to improve 
training efficiency. Compared to {\sf KnightKing}, {\sf DistGER} achieves $17.37\times$-$27.95\times$ acceleration
in training over all our graphs. Next, considering the same corpus size, we compare the training efficiency of {\sf Pword2vec} and {\sf DSGL}
(trainer in {\sf DistGER}). Figure~\ref{Dist_efficiency_sampling_training_MPGP}(b) shows that {\sf DSGL} achieves $4.31\times$ average speedup
compared to {\sf Pword2vec}. We also notice that the average throughput (number of nodes processed per second) for {\sf DSGL} is up to 49.5 million/s,
while that of {\sf Pword2vec} is only up to 16.1 million/s. These results indicate that our distributed {\sf Skip-Gram} learning model (\S \ref{sec:learning})
is more efficient than {\sf Pword2vec}.
%
%\begin{figure}
%  \centering
%  \includegraphics[width= 2.5 in]{Dist_Mpad_efficiency.eps}
%  \caption{\small (a) exhibits the number of cross-machine computation for DistGER on workload-balancing and MPGP partition scheme, respectively, and (b) shows the random walk time of DistGER on the two schemes, where y axis is in log-scale.}
%  \label{Dist_efficiency_MPaD}
%\end{figure}

\spara{Partitioning efficiency.} Considering the %large number cross-machine computing introduced by the
randomness inherent in random walks, the partitioning scheme is critical to overall efficiency. %of the distributed framework.
%To validate the efficiency of our multi-proximity-aware streaming graph partitioning (MPGP),
%we deploy the workload balancing scheme used in KnightKing and MPGP on DistGER,
%respectively,
%and report the number of cross-machine computations during the random walk procedure for the two schemes.
%We also present the efficiency performance of MPGP compared with the workload-balancing scheme.
For {\sf DistGER},
Figure~\ref{Dist_efficiency_sampling_training_MPGP}(c) exhibits that our multi-proximity-aware streaming graph partitioning ({\sf MPGP})
significantly reduces (avg. reduction $45\%$) the number of cross-machine messages than the workload-balancing partition of {\sf KnightKing}
on five graphs. Moreover, it improves the efficiency by 38.9\% for the random walking procedure
(Figure~\ref{Dist_efficiency_sampling_training_MPGP}(d)) over the same set of walks.
We report in Table~\ref{Partition_sechme_overhead}(a) the time required for graph partitioning in competing systems,
where {\sf DistDGL} uses the {\sf METIS} algorithm \cite{METIS_1998} for partitioning.
The results show that {\sf MPGP} performs partitioning with very little overhead in most cases, and
the partitioning efficiency is on average $25.1\times$ faster than competitors.
In Figure~\ref{Dist_MPaD_streaming}, we exhibit the distribution of local computations and cross-machine communications
on four machines for different streaming orders, and the top table reports their running times for partitioning and random walks.
For sequential {\sf MPGP}, we find that the {\sf DFS+degree}-based streaming order (\S \ref{sec:partition}) is more efficient than other streaming orders,
and it also strikes the best balance between cross-machine communications reduction and workload balancing.
Table~\ref{Partition_sechme_overhead}(b) exhibits the performance evaluation of {\sf parallel MPGP} on the small- ({\em LiveJournal}), medium- ({\em Com-Orkut}) and large-scale ({\em Twitter}) graphs. The results show that {\sf DFS+Degree} in {\sf parallel MPGP} is still the best or comparable in terms of partition time, due to the same reason as stated in our third optimization scheme (\S \ref{sec:partition}). On the other hand, {\sf BFS+Degree} in {\sf parallel MPGP} works the best in terms of random walk time due to preserving the locality of the graph structure (our fourth optimization scheme in \S \ref{sec:partition}).
%as using its streaming order to parallel partitioning can reduce the influence of relevance between each segment.
We ultimately recommend {\sf BFS+Degree} for {\sf parallel MPGP}, since it reduces the partition time greatly, while the random walk time is comparable to that obtained from sequential {\sf MPGP}.
%
%\begin{figure}
%  \centering
%  \includegraphics[width= 3 in]{Dist_Mpad_streaming_vertex_time.eps}
%  \caption{\small The distribution of local computations and cross-machine communications for different streaming orders on {\em LiveJournal}. The top table reports their running times for partitioning and random walks.}
%  \label{Dist_MPaD_streaming}
%\end{figure}
%
%\begin{table}
%\setlength{\abovecaptionskip}{0.cm}
%\setlength{\belowcaptionskip}{-0.cm}
%\newcommand{\tabincell}[2]{\begin{tabular}{@{}#1@{}}#2\end{tabular}}
%  \caption{\small Time execution time (seconds) of the partition scheme in PBG, DistDGL, and DistGER, ``$-$'' means the scheme fails under constrains of computation resource.}
%  \label{Partition_sechme_overhead}
%  \begin{center}
%  \small
%  \begin{tabular}{ccccc}
%    \hline
%    {Graph}&{PBG}&{DistDGL(METIS)}&{DistGER}\\
%    \hline
%    Flickr& 383.28 &127.72 &\bfseries{15.96} \\
%
%    Youtube& 349.15 &116.30&\bfseries{13.56} \\
%
%    LiveJournal& 458.52 &425.19 &\bfseries{36.42} \\
%
%    Com-Orkut& 2662.62 &2761.25 &\bfseries{294.68}\\
%
%    Twitter&78986.85 &$-$&\bfseries{35500.41}\\
    %\hline
%    \multicolumn{5}{l}{* HuGE+ generates the smallest corpus size for training among all methods tested.} \\
%
%  \hline
%\end{tabular}
%\end{center}
%\end{table}
%


\subsection{Generality of DistGER}
\label{sec:generality}
%\begin{figure}
%  \centering
%  \includegraphics[width= 3 in, height= 1.65 in]{Dist_generality_table.eps}
%  \caption{\small Generality comparison for DistGER and KnightKing, %on real-word graphs,
%  The bars display random walk (denoted as $-R$) and training efficiency (denoted as $-T$) for {\sf Deepwalk} (DW) and {\sf node2vec} (n2v), respectively.%, and the y axis is in log-scale.
%  Top table shows the ratio $\frac{\text{{\em AUC} for {\sf DistGER}}}{\text{{\em AUC} of {\sf KnightKing}}}$, both with Deepwalk and node2vec, respectively, considering link prediction.}
%  \label{Dist_generality}
%\end{figure}
%
%Since our proposed information-oriented random walk framework DistGER aims to address the redundant computations and high communication cost introduced by the effectiveness measurement of the generated walking information in distributed setting, it provides a good systematic support for the information-centric approach HuGE as shown by the previous experimental results. A natural question arises: can DistGER also support the traditional random-walk-based methods?
To demonstrate the generality of {\sf DistGER}, we deploy {\sf Deepwalk} \cite{DeepWalk_2014}, {\sf node2vec} \cite{node2vec_2016} and {
\sf HuGE+} \cite{HuGE+_2022}
on {\sf DistGER}. While the original {\sf Deepwalk} and {\sf node2vec} follow
traditional random walks, in {\sf DistGER} the walk length and the number of walks are decided via information-centric measurements.
Next, we also deploy both {\sf Deepwalk} and {\sf node2vec} on {\sf KnightKing} which supports the routine configuration random walk.
Figure~\ref{Dist_generality} illustrates that {\sf DistGER} reduces the random walks time by 41.1\% and 51.6\% on average for
{\sf Deepwalk} and {\sf node2vec}, respectively. For training, {\sf DistGER} is on average $17.7\times$ and $21.3\times$ faster than {\sf KnightKing}+{\sf Pword2vec}
for {\sf Deepwalk} and {\sf node2vec}, respectively.
Moreover, we also show the {\em AUC} ratio of {\sf DistGER} and {\sf KnightKing}, considering {\sf Deepwalk} and {\sf node2vec}, for link prediction.
% tasks, where performing multi-label classification on Flickr graph  and link prediction on other graphs, the accuracy metric for the two task are $Miro-F1$ and $AUC$ score, respectively, it can be found from
Our results depict that {\sf DistGER} has comparable (in most cases, higher) {\em AUC} scores, while it improves the efficiency significantly
even for traditional random walk-based graph embedding methods.
{\sf HuGE+} is an extension of {\sf HuGE}, and it uses the same {\sf HuGE} information-centric method to determine the walk length and the number of walks per node. Figure~\ref{Dist_generality} exhibits the compatibility of {\sf HuGE+} on {\sf DistGER} via its general API.