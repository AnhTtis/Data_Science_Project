\begin{abstract}
%
Graph embedding maps graph nodes to low-dimensional vectors, and is widely adopted in machine learning tasks.
The increasing availability of billion-edge graphs underscores the importance of learning efficient and effective embeddings on large graphs, such as link prediction on Twitter with over one billion edges. %Unfortunately, 
Most existing graph embedding methods fall short of reaching high data scalability. In this paper, we present a 
general-purpose, distributed,
information-centric random walk-based graph embedding framework, %new distributed graph embedding framework, 
{\sf DistGER}, which %is deployed on a set of machines and 
can scale to embed billion-edge graphs. {\sf DistGER} incrementally computes
information-centric random walks. %in constant time. 
It further leverages a multi-proximity-aware,
streaming, parallel graph partitioning strategy, simultaneously achieving high local partition quality and excellent workload balancing across machines. {\sf DistGER} also improves the distributed {\sf Skip-Gram} learning model to generate node embeddings
by optimizing the access locality, CPU throughput, and synchronization efficiency.
%To the best of our knowledge, {\sf DistGER} is the first general-purpose, distributed, information-centric random walk-based graph embedding framework. 
Experiments on real-world graphs demonstrate that compared to state-of-the-art distributed graph embedding frameworks, including {\sf KnightKing}, {\sf DistDGL}, and {\sf Pytorch-BigGraph},
{\sf DistGER} 
%achieves much better efficiency, effectiveness, and scalability
exhibits $2.33\times$--$129\times$ acceleration, 45\% reduction in cross-machines communication,
and \textgreater 10\% effectiveness improvement in downstream tasks.
%For downstream tasks, DistGER can offer an average gain of 11.7\%. In addition, DistGER also has well scalability and generality.}
\end{abstract}
