\section{Conclusions}
\label{sec:conclusions}
We proposed {\sf DistGER}, a novel, general-purpose,
distributed graph embedding framework with improved effectiveness, efficiency,
and scalability. {\sf DistGER} incrementally computes information-centric random walks
%in constant time
and leverages multi-proximity-aware, streaming, parallel graph partitioning to achieve high
local partition quality and excellent workload balancing.
{\sf DistGER} also designs distributed {\sf Skip-Gram} learning,
which provides efficient, end-to-end distributed support for node embedding learning.
Our experimental results demonstrated that {\sf DistGER} achieves much better efficiency and
effectiveness than state-of-the-art distributed systems {\sf KnightKing}, {\sf DistDGL}, and {\sf Pytorch-BigGraph},
and scales easily to billion-edge graphs, while it is also generic to support
traditional random walk-based graph embeddings.
% In the future, we shall address the bottlenecks of graph neural networks-based
% systems, e.g., inefficient sampling and synchronization costs,
% in an information-oriented manner. %, such as the inefficient sampling and high synchronization cost discussed in this paper.
