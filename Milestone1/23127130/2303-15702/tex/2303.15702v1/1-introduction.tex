\section{Introduction}
\label{sec:intro}
%
Graph embedding is a widely adopted operation that embeds a node in a graph to a low-dimensional vector.
The embedding results are used in downstream machine learning tasks %on graphs
such as link prediction \cite{Link_prediction_2017},
node classification \cite{classification_2011}, clustering \cite{cluster_2017}, and recommendation \cite{recomendation_2017}.
In these applications, graphs can be huge with millions of nodes and billions of edges. For instance, the Twitter graph includes over 41 million user nodes and over one billion edges, and it has extensive requirements for link prediction and classification tasks \cite{GuptaGLSWZ13}. The graph of users and products at Alibaba also consists of more than two billion user-product edges, which forms a giant bipartite graph for its recommendation tasks \cite{WangHZZZL18}. 
%Natural language processing tasks take advantage of knowledge graphs, such as Freebase with 1.9 billion triples.
A plethora of random walk-based graph embedding solutions \cite{HuGE_2021,node2vec_2016,DeepWalk_2014,Line_2015,Verse_2018}
are proposed. %based on {\it random walks}.
A random walk is a graph traversal that starts from a source node,
jumps to a neighboring node at each step, and stops after a few steps.
Random-walk-based embeddings are inspired by the well-known natural
language processing model, {\sf word2vec} \cite{word2vec_2013}. By conducting sufficient
random walks on graphs, substantial graph structural information is collected and fed into the
{\sf word2vec} ({\sf Skip-Gram}) to generate node embeddings. Compared with other graph embedding
solutions such as graph neural networks \cite{TuCWY018,WangC016,GraphSAGE_2017,Graph_attention_2018,Graphgan_2018}
and matrix factorization techniques \cite{ProNE_2019,NetSMF_2019,NRP_2020,QiuDMLWT18,WangCWP0Y17},
random walk-based methods are more flexible, parallel-friendly, and scale to larger graphs \cite{FlashMob_2021}.


While graph embedding is crucial, the increasing availability of billion-edge
graphs underscores the importance of scaling graph embedding.
The inherent challenge is that the number of random walks required
increases with the graph size. For example, one representative work, {\sf node2vec}
\cite{node2vec_2016} needs to sample many node pairs to ensure the embedding quality, 
%thereby requiring substantial computational resource. It
it takes months to learn node embeddings for a graph with 100 million nodes and 500 million edges by 20 threads on a modern server \cite{ProNE_2019}.
Some very recent work, e.g., {\sf HuGE} \cite{HuGE_2021} attempts to improve the quality of random walks %workload
according to the importance of nodes. Though this method can remove redundant
random walks to a great extent, the inherent complexity remains similar.
It still requires more than one week to learn embeddings for a billion-edge Twitter graph on a modern server,
hindering its adoption to real-world applications. Another line of work turns
to use GPUs for efficient graph embedding. For example, some recent graph embedding frameworks
(e.g., \cite{GraphVite_2019, Tencent_GE_2020}) simultaneously perform graph random walks on CPUs and
embedding training on GPUs. However, as the computing power between GPUs and CPUs differ widely,
it is typically hard for the random walk procedure performed on CPUs to catch up with the embedding
computation performed on GPUs, causing bottlenecks \cite{Serafini21,ZhengCCSWLCYZ22}. Furthermore, this process is heavily related to
GPUs' computing and memory capacity, which can be drastically different across different servers.

Recently, distributed graph embedding, or computing graph embeddings with multiple machines, has attracted significant research interest
to address the scalability issue. Examples include {\sf KnightKing} \cite{KnighKing_2019}, {\sf Pytorch-BigGraph} \cite{PBG_2019},
and {\sf DistDGL} \cite{DistDGL_2020}. {\sf KnightKing} \cite{KnighKing_2019} optimizes the walk-forwarding process for {\sf node2vec} and brings up several orders of magnitude improvement compared to a single-server solution. However, it may suffer from
redundant or insufficient random walks that are attributed to a routine random walk setting, resulting in low-quality training information for the
downstream task \cite{HuGE_2021}. Moreover, the workload-balancing graph partitioning scheme that it leverages fails
to consider the randomness inherent in random walks, introducing higher communication costs across machines
and degrading its performance. Facebook proposes {\sf Pytorch-BigGraph} \cite{PBG_2019} that leverages
graph partitioning technique and parameter server to learn large graph embedding on multiple CPUs based on {\sf PyTorch}.
However, the parameter server used in this framework needs to synchronize embeddings with clients,
which puts more load on the communication network and limits its scalability.
Amazon has recently released {\sf DistDGL} \cite{DistDGL_2020}, a distributed graph embedding framework for graph neural network model.
However, its efficiency is bogged down by the graph sampling operation, e.g., more than 80\% of the overhead is for sampling in the
{\sf GraphSAGE} model \cite{GraphSAGE_2017}, and the mini-batch sampling used may trigger delays in gradient updates causing
inefficient synchronization. In conclusion, although the distributed computation frameworks have shown better performance
than the single-server and CPU-GPU-based solutions, significant rooms exist for further improvement.

% \spara{Our {\sf DistGER} system.}
{\bf Our {\sf DistGER} system.}
We present a newly designed distributed graph embedding system, {\sf DistGER},
which incorporates more effective information-centric random walks such as {\sf HuGE}~\cite{HuGE_2021}
and achieves super-fast graph embedding compared to state-of-the-arts.
As a preview, compared to {\sf KnightKing}, {\sf Pytorch-BigGraph}, and {\sf DistDGL},
our {\sf DistGER} achieves $9.3\times$, $26.2\times$, and $51.9\times$ faster embedding on average, and easily scales to
billion-edge graphs (\S \ref{sec:experiments}).
Due to information-centric random walks, {\sf DistGER} embedding also shows higher effectiveness when applied to downstream tasks.

Three novel contributions of {\sf DistGER} are as follows.
{\bf First} and foremost, since the information-centric random walk requires measuring the effectiveness of the
generated walk on-the-fly during the walking procedure, it inevitably introduces higher
computation and communication costs in a distributed setting.
{\sf DistGER} resolves this by showing that the effectiveness of a
walking path can be measured through incremental information,
avoiding the need for full-path information.
{\sf DistGER} invents incremental information-centric computing ({\sf InCoM}),
ensuring $\bigO(1)$ time for on-the-fly measurement and maintains constant-size messages
across computing machines. {\bf Second}, considering the randomness inherent in random walks
and the workload balancing requirement, {\sf DistGER} proposes multi-proximity-aware,
streaming, parallel graph partitioning ({\sf MPGP}) that is adaptive to random walk characteristics, increasing
the local partition utilization. Meanwhile, it uses a dynamic workload constraint for the partitioning
strategy to ensure load-balancing. {\bf Finally}, different from the existing random walk-based embedding techniques,
{\sf DistGER} designs a distributed {\sf Skip-Gram} learning model ({\sf DSGL}) to generate node embeddings
and implements an end-to-end distributed graph embedding system. Precisely, {\sf DSGL} leverages global-matrices and two local-buffers
for node vectors to improve the access locality, thus reducing cache lines ping-ponging across multiple cores
during model updates; then develops multi-windows shared-negative samples computation to fully exploit the CPU throughput.
Moreover, a hotness block-based synchronization mechanism is proposed to synchronize node vectors efficiently in a distributed setting.


{\bf Our contributions and roadmap.}
%Our main contributions are summarized below.
%\begin{itemize}
We propose an efficient, scalable, end-to-end distributed graph embedding system, {\sf DistGER},
which, to our best knowledge, is the first general-purpose, information-centric random walk-based
distributed graph embedding framework.

\noindent
$\bullet$ We introduce incremental information-centric computing ({\sf InCoM}) to address computation and communication
overheads due to on-the-fly effectiveness measurements during information-oriented random walks in a distributed setting (\S \ref{sec:incom}).

\noindent
$\bullet$ We propose multi-proximity-aware, streaming, parallel graph partitioning ({\sf MPGP}) that %partitions the graph across computing machines,
achieves both higher local partition utilization and load-balancing (\S \ref{sec:partition}).

\noindent
$\bullet$ We develop a distributed {\sf Skip-Gram} learning model ({\sf DSGL}) to generate node embeddings by improving
the access locality, CPU throughput, and synchronization efficiency (\S \ref{sec:learning}).

\noindent
$\bullet$ We conduct extensive experiments on five large, real-world graphs to 
%confirm the advantages of our approach. The evaluation results show
demonstrate that {\sf DistGER} achieves much better efficiency, scalability, and effectiveness over existing popular distributed frameworks,
e.g., {\sf KnightKing} \cite{KnighKing_2019}, {\sf DistDGL} \cite{DistDGL_2020}, and {\sf Pytorch-BigGraph} \cite{PBG_2019}. %, exhibiting an acceleration of $2.91\times$--$355\times$ and an average reduction of 45\% in cross-machines computation on all graphs. For the downstream tasks, DistGER can offer an average gain of 11.7\%.
In addition, {\sf DistGER} generalizes well to other random walk-based graph embedding methods (\S \ref{sec:experiments}). %also has well scalability and generality.

We discuss preliminaries and a baseline approach in \S \ref{sec:preliminaries}, related work in \S \ref{sec:related}, and conclude in \S \ref{sec:conclusions}. 
% Additional related work and empirical results, proof of theorems are given in our full version \cite{full_version}.  
