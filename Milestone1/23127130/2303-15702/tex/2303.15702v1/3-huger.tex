\section{The Proposed System: D\lowercase{ist}GER}
\label{sec:DistGER}
%
To address the aforementioned computing and communication challenges of the baseline {\sf HuGE-D} (\S \ref{sec:HUGED}),
we ultimately design an efficient and scalable distributed information-centric random walks engine,
{\sf DistGER}, aiming to provide an end-to-end support for distributed random walks-based graph embedding
(Figure \ref{HuGER_framework}).
We discuss our distributed information-centric random walk component ({\sf sampler}) in \S \ref{sec:incom}
and multi-proximity-aware, streaming, parallel graph partitioning scheme ({\sf MPGP}) in \S \ref{sec:partition},
while our novel distributed graph embedding ({\sf learner})
is given in \S \ref{sec:learning}.
 Besides providing systemic support for the information-oriented method {\sf HuGE}, 
{\sf DistGER} can also extend its information-centric measurements to 
traditional random walk-based approaches via the general API to get rid of their routine configurations (\S \ref{sec:generality}).
%
\subsection {Incremental Information-centric Computing}
\label{sec:incom}
{\sf DistGER} introduces incremental information-centric computing ({\sf InCoM})
to reduce redundant computations and the costs of messages. Recall that the baseline {\sf HuGE-D} computes $H(W)$ and $R(H(W), L)$ via the full-path computation mechanism to measure the information effectiveness of a walk $W$ at each step, requiring $\bigO(L)$ time at every step of the walk.
We show that instead it is possible to update $H(W)$ and $R(H(W),L)$ incrementally with $\bigO(1)$
time cost at every step of the walk. We store local information about ongoing walks
at the respective machines, which reduces cross-machine message sizes.

\underline{Incremental computing of walk information.}
Each computing machine stores a partition of nodes from the input graph.
For every ongoing walk $W$, each machine additionally maintains
in its {\em local frequency list} the set of nodes
$v$ present in the walk which are also local to that machine, together with their number of occurrences $n(v)$ on the walk.
When $W$ terminates, the local frequency list for $W$ is no longer required and is deleted.
Theorem~\ref{th:incom_walk} summarizes our incremental computing of walk information.
% The proof is given in our full version \cite{full_version} due to lack of space.
%
\begin{theor}
\label{th:incom_walk}
Consider an ongoing walk $W^L$ with the current length $L\geq 0$, the next accepted node to be added in $W^L$ is $v$, and
$n(v)\geq 0$ is the number of occurrences of $v$ in the walk. In addition to $v$, both $L$ and $n(v)$ would
increase by 1. For clarity, we denote $n(v)$ in $W^L$ and $W^{L+1}$ as
$n^L(v)$ and $n^{L+1}(v)$, respectively. The information entropy $H\left(W^{L+1}\right)$ is related to $H\left(W^L\right)$ as follows.
%
\begin{small}
\begin{eqnarray}
H\left(W^{L+1}\right) &=& \frac{H\left(W^{L}\right)\times L - \log T}{L+1} \nonumber \\
\text{where} \,\,\, T &=& \left\{
\begin{aligned}
&\frac{L^L}{(L+1)^{L+1}}\cdot\frac{\left(n^{L+1}(v)\right)^{n^{L+1}(v)}}{\left(n^L(v)\right)^{n^L(v)}}, \,\, if\ v \in W^L\\
&\frac{L^L}{(L+1)^{L+1}}, \,\, if\ v \not\in W^L
\end{aligned}
\right.
\label{eq:incom_walk}
\end{eqnarray}
\end{small}
\end{theor}
%
% \eat{
\begin{proof}
From the definition of entropy of $W^L$ in Eq.~\ref{path_entropy}, we get:
\begin{footnotesize}
\begin{eqnarray}
\displaystyle H\left(W^L\right) &=& -\sum_{u \in W^L}\frac{n^L(u)}{L}\log\frac{n^L(u)}{L} \nonumber %\\
\end{eqnarray}
\end{footnotesize}
\begin{footnotesize}
\begin{eqnarray}
\displaystyle\implies  2^{-H\left(W^L\right)\cdot L} &=& 2^{\sum_{u \in W^L}n^L(u)\cdot\log\frac{n^L(u)}{L}} 
=\frac{\prod_{u \in W^L}{\left(n^L(u)\right)}^{n^L(u)}}{L^L} \nonumber 
\end{eqnarray}
\end{footnotesize}
Assuming that $v$ is in $W^L$,
\begin{footnotesize}
\begin{eqnarray}
\displaystyle L^L \cdot 2^{-H\left(W^L\right)\cdot L}&=&\left({n^{L}(v)}\right)^{n^{L}(v)}\cdot\prod_{u \in W^L\setminus\{v\}}\left({n^L(u)}\right)^{n^L(u)}
\label{eq:WL}
\end{eqnarray}
\end{footnotesize}
Analogously, for $W^{L+1}$ which is extended from $W^L$ by appending $v$, and assuming that $v$ is in $W^L$, we derive:
\begin{footnotesize}
\begin{eqnarray}
\label{eq:WL1}
\displaystyle (L+1)^{L+1} \cdot 2^{-H\left(W^{L+1}\right)\cdot (L+1)}&=&\left({n^{L+1}(v)}\right)^{n^{L+1}(v)}\cdot\prod_{u \in W^{L}\setminus\{v\}}\left({n^{L}(u)}\right)^{n^{L}(u)} \nonumber \\
\end{eqnarray}
\end{footnotesize}
Comparing Eq.~\ref{eq:WL} and \ref{eq:WL1}, we get:
\begin{footnotesize}
\begin{eqnarray}
\displaystyle 2^{H\left(W^L\right)\cdot L-H\left(W^{L+1}\right)\cdot (L+1)} &=& \frac{L^L}{(L+1)^{L+1}}\cdot\frac{\left(n^{L+1}(v)\right)^{n^{L+1}(v)}}{\left(n^L(v)\right)^{n^L(v)}} = T
\end{eqnarray}
\end{footnotesize}
The proof when $v \not \in W^L$ can be derived similarly.
\end{proof}
% }
%
\underline{Incremental computing for walk termination.}
To terminate a random walk, {\sf HuGE} computes and verifies the
linear relation between information entropy $H$ and walk length $L$ at every step of the walk.
From Eq. \ref{path_corr}, $R(H,L)$ can be expressed as:
\begin{small}
\begin{equation}
\label{correlation}
\displaystyle R(H,L) = \frac{E(HL)-E(H)E(L)}{\sqrt{{(E(H^2)-E(H)^2)}{(E(L^2)-(E(L)^2)}}}
\end{equation}
\end{small}
%
The mean function $E($ $)$ can be computed incrementally.
\begin{footnotesize}
\begin{eqnarray}
\label{correlation_inc}
&& \displaystyle E_p(X)=\frac{1}{p}\sum\limits_{i=1}^{p}X_i = \left(\frac{p-1}{p}\right)E_{p-1}(X)+\frac{X_p}{p} \\
&& \displaystyle E_p(XY)=\frac{(p-1)^2E_{p-1}(XY)+(p-1)[X_pE_{p-1}(Y)+Y_pE_{p-1}(X)]+X_pY_p}{p^2} \nonumber
\end{eqnarray}
\end{footnotesize}

\underline{Message size reduction.}
Due to incremental computation, instead of full-path information,
only constant-length messages need to be sent across computing machines: [$walker\_id$, $steps$, $node\_id$, $H$, $L$, $E(H)$, $E(L)$, $E(HL)$, $E(H^2)$, $E(L^2)$].
%
\begin{figure}
  \centering
  \includegraphics[width= 3.2 in]{./Figures/computation_incremental_mechanism_1.eps}
  \caption{\small Incremental computing for information-centric random walk}
  \label{InCoM_computation}
\end{figure}
%
\begin{exam}
\label{ex:incom}
Figure~\ref{InCoM_computation} exhibits incremental information-centric computing ({\sf InCoM}).
The graph is partitioned into two machines: $\{v_1, v_2, v_3, v_4\}$ in $M_1$ and
$\{v_5, v_6, v_7, v_8\}$ in $M_2$.
A local frequency list is maintained for each ongoing walk at every machine,
having \# occurrences information only about nodes that are local to a machine.
When a local node is added to the walk, \# occurrences for this node
is updated in this local list. Using the local frequency list, {\sf DistGER}
incrementally computes information entropy $H$, as well as the linear relation $R$ between
$H$ and the current path length $L$ to decide on walk termination.
If the next accepted node is not at the current machine, the walker only needs to
carry the necessary incremental (constant-length) information as a message including
$walker\_id$, $steps$, $node\_id$, $H$, $L$, $E(H)$, $E(L)$, $E(HL)$, $E(H^2)$, and $E(L^2)$
to the other machine, and then generate $H$ and $R$ in that machine. Given an 8 bytes
space to store a variable, since the messages in baseline {\sf HuGE-D} carry the full-path information (i.e., [$walk\_id$, $steps$, $node\_id$, $path\_info$]), {\sf HuGE-D} needs $24 + 8L$ bytes per message, where $L$ is the walk length, while {\sf DistGER} only requires the constant size of 80 bytes. If the maximum path length is 80 (commonly used), one message in {\sf DistGER} is up to 8.3$\times$ smaller than that in {\sf HuGE-D}.
\end{exam}

\underline{Complexity analysis.}
The pseudocode of {\sf InCoM}
remains similar to that in Algorithm~\ref{HuGE-D_walk}, except that $H$ and $R$ are
computed incrementally in Line 6, via Eq. \ref{eq:incom_walk} and \ref{correlation_inc},
and we require only $\bigO(1)$ time at each step of the walk. Furthermore, when a walk crosses a machine,
it sends %$M(1)$, i.e., 
a constant-length message to another machine. Following similar analysis as {\sf HuGE-D},
the time spent for {\sf InCoM} %in the distributing setting
is:
$\bigO(r'\cdot L'\cdot |V|/P + N\cdot M(1)/B)$,
where $P$, $B$, and $N$ are \# processors, network bandwidth,
and \# cross-machine messages, respectively.
%%
\subsection {Multi-Proximity-aware Streaming Partitioning}
\label{sec:partition}
Graph partitioning aims at balancing workloads across machines
in a distributed setting, while also reducing cross-machine
communications. 
Balanced graph partitioning with the minimum edge-cut is an \NP-hard
problem \cite{BulucMSS016}. Instead, our system, {\sf DistGER} develops
multi-proximity-aware, streaming, parallel graph partitioning ({\sf MPGP}): By leveraging first
and second-order proximity measures,
we select a good-quality partitioning to ensure that each walker stays in a local computing
machine as much as possible, thereby reducing the number of cross-machine communications.
The partitioning is conducted in a node streaming manner, hence it scales to larger graphs
\cite{streaming_model_SIGMOD_2019, streaming_model_VLDB_2018}.
We also ensure workload balancing among the computing servers.

\underline{Partitioning method.}
Given a set of partially computed partitions, $P_1, P_2,..., P_m$, where $m$ denotes
\# machines, an un-partitioned node $v$ is placed in one of these partitions based on the following objective:
%
\begin{small}
\begin{eqnarray}
&\displaystyle \argmax_{i \in \{1...m\}}\left(PS_1(v,P_i) + PS_2(v,P_i)\right)\times \tau(P_i) \\
& \displaystyle \text{where} \, \, \, \tau(P_i) = 1-\frac{|P_i|}{\gamma \times (\sum \limits_{i=1}^{m} |P_i|)/m} \noindent
\label{Mpad_constrait}
\end{eqnarray}
\end{small}
%
$PS_1(v, P_i)=|\{u\in N(v)\cap P_i\}|$ and $PS_2(v, P_i)=\sum_{u\in P_i}|\{N(v)\cap N(u)\}|$
represent the first- and second-order proximity scores, respectively.
For edge weight $w(v,u)$, $PS_1(v, P_i)=
\sum_{u\in N(v)\cap P_i}w(v,u)$ and $PS_2(v, P_i)=\sum_{u\in P_i}|\{N(v)\cap N(u)\}| \cdot w(v, u)$.
Intuitively,
$PS_1$ denotes \# neighbors of an unpartitioned node in the target partition, thus a higher value of $PS_1$ implies that the unpartitioned node should have a higher chance to be assigned to the target partition. 
Since $PS_2$ is defined by \# common neighbors, which are widely used to measure the similarity of node-pairs during the random walk, a higher value of $PS_2$ is also in line with the characteristics of random walk.
Higher first- and second-order proximity scores increase the chance that a random walker
stays in a local computing machine, thereby reducing cross-machine communication.
{\sf MPGP} also introduces a dynamic load-balancing term $\tau(P_i)$, 
where $|P_i|$ denotes the current number of nodes in $P_i$,
hence it is updated after every un-partitioned node's assignment;
and $\gamma$ is a slack parameter that allows deviation from the exact load balancing. 
Setting $\gamma=1$ ensures strict load balancing but hampers partition quality. 
Meanwhile, setting a larger $\gamma$ relaxes the load-balancing constraint, and creates a skewed partitioning.

{\sf MPGP} differs from some of the existing node streaming-based graph partition schemes, e.g., {\sf LDG} \cite{LDG_2012} and {\sf FENNEL} \cite{FENNEL2014}
in several ways. {\bf First,}
{\sf LDG} and {\sf FENNEL}
set a maximum size for each partition in advance based on the total number of nodes and tend to assign
nodes to a partition until the maximum possible size is reached for that partition. 
In contrast, we ensure good load balancing across partitions at all times during the partitioning phase. 
% Therefore,
% unlike {\sf LDG} and {\sf FENNEL}, we do not require apriori knowledge of the total number of nodes in the input graph.
{\bf Second,} we find that {\sf LDG} and {\sf FENNEL} cannot partition larger graphs in a reasonable time, for example,
they consume more than one day to partition the Youtube \cite{Flickr_Youtube_Graph} graph with 1M nodes and 3M edges,
while our proposed {\sf MPGP} requires only a few tens of seconds (\S \ref{sec:experiments}), which is due to our optimization methods
discussed below. 

\underline{Optimizations and parallelization.}
{\bf First,} to measure first-order proximity scores,
we apply the {\sf Galloping} algorithm \cite{Galloping_2000} that can speed-up intersection computation
between two unequal-size sets. During our streaming graph partitioning, 
the partition size gradually increases, hence the {\sf Galloping} algorithm is quite effective.
{\bf Second,} during a second-order proximity score computation, i.e., $PS_2(v, P_i)=\sum_{u\in P_i}|\{N(v)\cap N(u)|$,
we only consider those nodes $u\in P_i$ whose contributions to first-order proximity score are non-zero,
i.e., $u\in N(v)$. This is because if $u$ is not a neighbor of $v$, the random walk cannot reach $u$.
{\bf Third,} nodes streaming order could impact the partitioning time and effectiveness.
We compare a number of streaming orders \cite{LDG_2012,FENNEL2014}, e.g., random, BFS, DFS, and their variations.
Based on empirical results (\S \ref{sec:experiments}), we recommend {\sf DFS+degree}-based streaming for sequential {\sf MPGP}:
Among the un-explored neighbors of a node during DFS, we select the one having the highest degree.
This strategy improves the efficiency of the {\sf Galloping} algorithm.
% since this generates more unequal-size sets for intersection computation, making the {\sf Galloping} algorithm
% quite effective.
% For partitioning effectiveness, empirically we find that {\sf DFS+degree}-based streaming works well (\S \ref{sec:experiments}).
{\bf Fourth}, with large-scale graphs (e.g., Twitter), sequential MPGP, 
% despite outperforming competitors,
still requires considerable time to partition. Thus, we implement a simple parallelization scheme {\sf parallel MPGP} ({\sf MPGP-P}),
as follows: 
% Considering that it is challenging to ensure both quality and efficiency in parallel streaming partitioning \cite{Partition_challenge_TPDS_2019},
% our parallelization scheme is simple: 
We divide the stream into several segments and independently partition the nodes of each segment in parallel
via {\sf MPGP}; finally, we combine the partitioning results of all segments. 
Based on our empirical results (\S \ref{sec:experiments}), we recommend {\sf BFS+Degree} for {\sf MPGP-P} since 
it reduces the partition time greatly and the random walk time on a partitioned graph is comparable to that obtained from the sequential version of MPGP.
% indicating good-quality partitions obtained via {\sf MPGP-P}.
% BFS retains the locality of graph structure, thus {\sf BFS+Degree} streaming order during parallel partition ensures that each segment contains a local region of the graph.

\underline{Complexity analysis.} 
The running time of {\sf MPGP} is dominated by first- and second-order proximity scores computation for each node,
which are computed in parallel for all $m$ partitions. %to accelerate the firs-order proximity scores computation,
For a first-order proximity score computation via
%\textcolor{orange}{As proposed in optimizations, we uses
the {\sf Galloping} algorithm,
let the smaller set size be $S_1$.
In the early stages of partitioning, the larger set constitutes the neighbors of an
un-partitioned node $v$, then the time complexity is $\bigO\left(S_1 \cdot \log|N(v)|\right)$; while at later stages, the larger set is the partition,
%(size$\approx|V|/m$),
thus it takes $\bigO\left(S_1 \cdot \log(|V|/m)\right)$ time.
For the second-order proximity score computing of $v$, 
let $S_2$ be the intersection set size generated by the first-order proximity scores computing of $v$.
As the number of common neighbors for each edge $(u,v)$ is processed in parallel by the Galloping algorithm,
the second-order proximity score computing requires $\bigO\left(\frac{S_2}{T}\cdot|N(v)|\cdot\log N_{max}^v\right)$ time,
with $T$ threads, where $N_{max}^v=\max_{u\in N(v)}|N(u)|$.
%

% \eat{
\underline{Benefits over {\sf KinghtKing}'s graph partitioning:}
Compared to only load-balancing based partition in {\sf KnightKing} that introduces high communication cost,
our {\sf MPGP} aims at both load-balancing and reducing cross-machine communications.
On average, {\sf MPGP} reduces 45\% cross-machine communications for all graphs that we use in our experiments, resulting about 38.9\% efficiency improvement (\S \ref{sec:experiments}).
% }
%
%
\section{Distributed Embedding Learning}
\label{sec:learning}
Our {\sf learner} in {\sf DistGER} supports distributed learning of node embeddings using the random walks
generated by the {\sf sampler} (Figure~\ref{HuGER_framework}).
We first discuss the shortcomings of state-of-the-art methods
on distributed {\sf Skip-Gram} with negative sampling and provide an overview of our solution (\S \ref{sec:skipgram_challenges}), then
elaborate our three novel improvements (\S \ref{sec:opt_skipgram}), and finally summarize our overall approach (\S \ref{sec:everything}).
%
\subsection{Challenges and Overview of Our Solution}
\label{sec:skipgram_challenges}
{\sf Skip-Gram} uses the stochastic gradient descent ({\sf SGD}) \cite{SGD_1951}.
Parameters update from one iteration and gradient computation in the next iteration may touch the same node
embedding, making it inherently sequential.
The original {\sf word2vec} leverages {\sf Hogwild} \cite{2011HOGWILD}
for parallelizing {\sf SGD}. It asynchronously processes different node pairs in parallel and ignores any conflicts
between model updates on different threads.
However, there are shortcomings in this approach \cite{Pword2vec_2019,pSGNSCC_2017}.
(1) Since multiple threads can update the same cache lines, updated data needs to
be communicated between threads to ensure cache-coherency, introducing cache lines ping-ponging
across multiple cores, which results in high access latency. 
(2) {\sf Skip-Gram} with negative sampling randomly selects a set of nodes as
negative samples for each context node in a context window.
% to maximize the co-occurrence probability with its target node.
Thus, there is a certain locality in model updates for the same target node, but this feature has not been exploited
in the original scheme. 
The randomly generated set of negative samples also introduces 
random access for parameter updates in each iteration, degrading performance.

As shown in Figure~\ref{SG_pw2v_dsgl}(a), a walk denoted as $W_1$ is assigned to a
thread, and there is a sliding window $w_1$ containing context nodes $\{v_1, v_2, v_4, v_5\}$ and the target node $v_3$.
The {\sf Skip-Gram} computes the dot-products of word vectors $\varphi_{in}({v_i})$ for a given word $v_i \in \{v_1, v_2, v_4, v_5\}$
and $\varphi_{out}({v_3})$, as well as for a set of $K$ negative samples, $\varphi_{out}({v_k})$ ($v_k \in V$).
Notice that $\varphi_{out}(v_3)$ will be computed four times with four different context words and sets of
negative samples, and these dot-products are level-1 {\sf BLAS} operations \cite{BLAS_2002}, which are limited by memory bandwidth.
Some state-of-the-art work, e.g., {\sf Pword2vec} \cite{Pword2vec_2019} shares
negative samples with all other context nodes in each window (Figure \ref{SG_pw2v_dsgl}(b)), and thus converts
level-1 BLAS vector-based computations into level-3 BLAS matrix-based computations to efficiently utilize computational resources.
However, such matrix sizes are relatively small and still have a gap to reach the peak CPU throughput.
Besides, this way of sharing negative samples cannot significantly reduce the ping-ponging of cache lines across multiple cores.
To improve CPU throughput, {\sf pSGNScc} \cite{pSGNSCC_2017} combines context nodes of the negative sample from
another window into the current window; and together with the current context nodes, it generates larger matrix batches
(Figure \ref{SG_pw2v_dsgl}(c)). Nevertheless, {\sf pSGNScc} needs to maintain a pre-generated inverted index table to find related windows, resulting in additional space and lookup overheads.
% resulting in a limited speedup compared with {\sf pWord2Vec}
% in their experiments.
%
\begin{figure}
  \centering
  \includegraphics[width= 3.2 in]{./Figures/SG_Pw2v_DSGL_pSGNScc_1.eps}
  \caption{\small Schematic diagram of (a) {\sf Skip-Gram} with negative samples ({\sf SGNS}), (b) {\sf Pword2vec} \cite{Pword2vec_2019}, (c) {\sf pSGNScc} \cite{pSGNSCC_2017}, and (d) {\sf DSGL} (our method)}
  \label{SG_pw2v_dsgl}
\end{figure}
%

To address the above limitations, we propose a distributed {\sf Skip-Gram} learning model, named {\sf DSGL}
(Figure~\ref{SG_pw2v_dsgl}(d)).
{\sf DSGL} leverages global-matrices and local-buffers for the vectors of context nodes
and target/negative sample nodes during model updates to improve the locality and reduce the ping-ponging of cache lines across multiple cores.
We then propose a multi-windows shared-negative samples computation mechanism to fully exploit the CPU throughput.
Last but not least, {\sf DSGL} uses a hotness-block based synchronization mechanism to synchronize the node vectors in
a distributed setting.
While we design them considering information-oriented random walks, they are generic
and have the potential to improve any random walk and distributed {\sf Skip-Gram} based
graph embedding model (\S \ref{sec:experiments}).
%
\subsection{Proposed Improvements: DSGL}
\label{sec:opt_skipgram}
%
\underline{Improvement-I: Global-matrix and local-buffer.}
As reasoned above, the parallel {\sf Skip-Gram} with negative sampling
suffers from poor locality and ping-ponging of cache lines across multiple cores.
Since the model maintains two matrices to update the parameters through
forward and backward propagations during training, where the input
matrix $\varphi_{in}$ and output matrix $\varphi_{out}$ store the vectors of
context nodes and target/negative sample nodes, respectively -- how to construct
these two matrices are critical to the locality of data access.

Since most of the real-world graphs follow a power-law degree distribution \cite{BA99,ABA03},
we find that the generated corpus sampled from those graphs also has this feature -- 
a few nodes occupy most of the corpus, which are updated frequently during training.
In light of this observation, we construct $\varphi_{in}$ and $\varphi_{out}$ as {\em global matrices}
in descending order of node frequencies from the generated corpus.
It ensures that the vectors of high-frequency nodes stay in the cache lines
as much as possible. Recall that node frequencies in the corpus were already computed in the random
walk phase (\S \ref{sec:randGembedding}).


In addition, to avoid cache lines ping-ponging, {\sf DSGL} uses {\em local buffers} to update the context and
negative sample nodes for each thread, thus the node vectors are
first updated in the local buffers within a lifetime (i.e., when a thread processes a walk during training)
and are then synchronized to the global matrix. %Temporarily isolating the cache lines for updating node vectors by this manner effectively alleviate the access latency caused by cache lines ping-ponging.
Precisely, since a sliding window $w$
always shifts the boundary and the target node by one node (Eq.~\ref{skim_gram_eq}), each node in
a walk will appear as a context node in up to $2w$ sliding windows
and as a target node once, thus a context node can be reused up to $2w+1$
times in the lifetime. It is necessary to consider this
temporal locality and build a {\em local context buffer} for each node vector
in a walk.
Meanwhile, on-chip caches where randomly-generated
negative sample nodes are located, have a higher chance to be
updated by multiple threads, and updating their vectors to the
global $\varphi_{out}$ requires low-level memory accesses.
To alleviate this, {\sf DSGL} also constructs a {\em local negative buffer}
for vectors of negative samples during one lifetime.
It randomly selects $K$ (negative sample set size) $\times$ $L$ (walk length, i.e., total steps) negative samples into the negative buffer from $\varphi_{out}$. Thus, {\sf DSGL} can use a different negative sample set with the same size ($K$) at each step.
For the target node, due to its lower re-usability compared to context nodes,
it is only updated once in global $\varphi_{out}$ in the lifetime;
hence, we do not create a buffer for it.
The constructed local buffers 
require small space,
since the sizes of buffers are related to the walk length.
The length of the information-centric random walk is much
smaller than that of traditional random walks (\S \ref{sec:randGembedding}).
%
\begin{figure}[tb!]
  \centering
  \includegraphics[width= 3.2 in]{./Figures/DSGL_framework_v3_1.eps}
  \caption{\small Workflow of our {\sf DSGL}: distributed Skip-Gram learning}
  \label{DSGL_framework}
  % \vspace{-5mm}
\end{figure}
%

\underline{Improvement-II: Multi-windows shared-negative samples.}
CPU
throughput of {\sf Skip-Gram} model can be significantly improved by
converting vector-based computations into matrix-based computations
\cite{floating_2015};
however, the batch matrix sizes are relatively small for existing methods \cite{Pword2vec_2019,pSGNSCC_2017}
and cannot fully utilize CPU resources. Based on this consideration, we design a multi-windows
shared-negative samples mechanism: To increase the batch matrix sizes, we
batch process the vector updates of context windows from multiple ($\geq$ 2) walks allocated to the same thread.

Consider Figure~\ref{SG_pw2v_dsgl}(d), 
two walks $W_1$ and $W_2$ are assigned to the same thread, where $w_1$ and $w_2$ are two context windows in $W_1$ and $W_2$;
$v_3$ and $v_8$ are target nodes in $w_1$ and $w_2$, respectively.
{\sf DSGL} simultaneously batch updates the node vectors
in two context windows $w_1$ and $w_2$ based on level-3 {\sf BLAS} matrix-based computations,
the set of negative samples is shared across the batch context nodes,
$v_3$ and $v_8$ are used as additional negative samples for $w_2$ and $w_1$, respectively.
After the lifetime of $W_1$ and $W_2$, the updated parameters of all context nodes and target/negative samples are
written back to $\varphi_{in}$ and $\varphi_{out}$, respectively.
Assuming the set of negative samples $K=5$, the batch matrix sizes of one iteration for {\sf Pword2vec}  \cite{Pword2vec_2019} (Figure~\ref{SG_pw2v_dsgl}(b))
is $4\times6$,
while that of {\sf DSGL} is extended to
$8\times7$.
This utilizes higher CPU throughput
and accelerates training without sacrificing accuracy(\S~\ref{sec:experiments}).
In practice, the number ($\geq$ 2) of multi-windows can be flexibly set according to available hardware resources.


\underline{Improvement-III: Hotness-block based synchronization.} In a distributed setting, the updated node vectors need to be synchronized with each computing machine.
Assuming that the generated corpus is partitioned into $m$ machines, each machine independently
processes the local corpus and periodically synchronizes the local parameters with other $m - 1$ machines.
For a full model synchronization across computing machines, the communication cost is $\bigO(|V|\cdot d \cdot m)$,
where $|V|$ is the total number of nodes and $d$ denotes the dimension of vectors, e.g.,
for 100 million nodes with 128 dimensions, the full model synchronization needs $\approx102.4$ billion messages across 4 machines --
it will be difficult to meet the efficiency requirement.  

Notice that the node occurrence counts in generated corpus
also follows a power-law distribution, implying that high-frequency nodes have a higher probability of being accessed
and updated during training. 
Following this, we propose a
hotness-block based synchronization mechanism in {\sf DSGL}. Instead of
full synchronization, 
we only conduct more synchronization for hot nodes than that for
low-frequency nodes. Since our global matrices are constructed based on the descending order of node frequencies in the generated corpus (Improvement-I),
it provides a favorable condition to achieve hotness-block based synchronization.
The global matrices are partitioned into several blocks (i.e., hotness-blocks)
based on the same frequency of nodes in the corpus, and are denoted as $B(i)$, $0<i\leq ocn_{max}$,
where $ocn_{max}$ is the largest number of occurrences of any node in the corpus.
We randomly sample one node from each hotness-block; for all these sampled nodes,
we synchronize their vectors across all computing machines during one synchronization period.
Due to the node frequency skewness in the corpus, for each node in $B(i)$, the probability of it
being sampled for synchronization is inversely proportional to $|B(i)|$. Thus, we
ensure that the hot nodes are synchronized more during the entire training procedure, while the low-frequency
nodes would have relatively less synchronization. Compared to the full model synchronization mechanism,
our synchronization cost is $\bigO(ocn_{max}\cdot d \cdot m)$, where  $ocn_{max}<<$ $|V|$,
indicating that it can significantly reduce the load on network, while keeping the parameters on each computing machine updated aptly.
%
\subsection{Putting Everything Together}
\label{sec:everything}
First, {\sf DSGL} constructs two global matrices $\varphi_{in}$ and $\varphi_{out}$ in
descending order of node frequencies from the generated corpus.
It leverages a pipelining construction during the random walk phase,
one thread is responsible for counting the frequency of each node on its local walk;
at the end of all random walks, these counts from computing machines are aggregated
to construct global $\varphi_{in}$ and $\varphi_{out}$.
Next, {\sf DSGL} uses a multi-windows shared-negative samples
computation for each thread.
As shown in Figure \ref{DSGL_framework},
two walks $W_1$ and $W_2$ are assigned to $Thread_1$,
suppose \# multi-windows = 2 and
negative sample set size $K$ = 5.
For maintaining access locality and reducing cache lines ping-ponging,
two local buffers per thread are constructed for the context (blue and green blocks) and negative sample (orange blocks) nodes. The target nodes are denoted as red blocks.
Initially, the two buffers will respectively load the vectors associated
with nodes from $\varphi_{in}$ and $\varphi_{out}$,
then $Thread_1$ proceeds over $W_1$ and $W_2$ with matrix-matrix multiplications,
and the updated vectors are written to the buffers at each step.
After the lifetime of $W_1$ and $W_2$, the latest node vectors in buffers are
written back to global $\varphi_{in}$ and $\varphi_{out}$.
{\sf DSGL} periodically synchronizes the updated vectors across multiple
computing machines by hotness-block based synchronization. 

