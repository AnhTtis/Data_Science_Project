\documentclass[lettersize,journal]{IEEEtran}

\usepackage{amsmath,amsfonts}
\usepackage{acronym}
\usepackage{algorithmic}
\usepackage{array}

\usepackage{natbib}
\setcitestyle{numbers,square}
\usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
\usepackage{textcomp}
\usepackage{stfloats}
\usepackage{url}
\usepackage{verbatim}
\usepackage{graphicx}
\usepackage{amsmath}
%\usepackage{cite}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{color}
\usepackage{graphicx}


\hyphenation{op-tical net-works semi-conduc-tor IEEE-Xplore}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\usepackage{balance}


\begin{document}
\title{POAR: Towards Open-World Pedestrian Attribute Recognition}
\author{\IEEEauthorblockN{Yue~Zhang, Suchen~Wang, Shichao~Kan, Zhenyu~Weng,\\ Yigang~Cen,~\IEEEmembership{Member,~IEEE,} Yap-peng~Tan,~\IEEEmembership{Fellow,~IEEE,}}

\thanks{Y. Zhang, and Y. Cen are with the School of Computer and Information Technology, Beijing Jiaotong University, Beijing 100044, China, and also with the Beijing Key Laboratory of Advanced Information Science and Network Technology, Beijing 100044, China (e-mail: 17112065@bjtu.edu.cn; ygcen@bjtu.edu.cn).}% <-this % stops a space
\thanks{ S. Wang and Z. Weng are with the School of Electrical and Electronic Engineering, Nanyang Technological University, 639798, Singapore (e-mail:  suchen001@e.ntu.edu.sg; zhenyu.weng@ntu.edu.sg)}
\thanks{ S. Kan is with the School of Computer Science and Engineering, Central South University, 410083, Changsha, Hunan, China(e-mail: kanshichao@csu.edu.cn)}
\thanks{Y. Tan is ith the School of Electrical and Electronic Engineering, Nanyang Technological University, 639798, Singapore (e-mail:  eyptan@ntu.edu.sg)}
\thanks{Manuscript created October, 2022; This work was developed by the IEEE Publication Technology Department. This work is distributed under the \LaTeX \ Project Public License (LPPL) ( http://www.latex-project.org/ ) version 1.3. A copy of the LPPL, version 1.3, is included in the base \LaTeX \ documentation of all distributions of \LaTeX \ released 2003/12/01 or later. The opinions expressed here are entirely that of the author. No warranty is expressed or implied. User assumes all risk.}}

\markboth{Journal of \LaTeX\ Class Files,~Vol.~18, No.~9, September~2020}%
{How to Use the IEEEtran \LaTeX \ Templates}

\maketitle

\begin{abstract}
 Pedestrian attribute recognition (PAR) aims to predict the attributes of a target pedestrian in a surveillance system. Existing methods address the PAR problem by training a multi-label classifier with predefined attribute classes. However, it is impossible to exhaust all pedestrian attributes in the real world. To tackle this problem, we develop a novel pedestrian open-attribute recognition (POAR) framework. Our key idea is to formulate the POAR problem as an image-text search problem. We design a Transformer-based image encoder with a masking strategy. A set of attribute tokens are introduced to focus on specific pedestrian parts (e.g., head, upper body, lower body, feet, etc.) and encode corresponding attributes into visual embeddings. Each attribute category is described as a natural language sentence and encoded by the text encoder. Then, we compute the similarity between the visual and text embeddings of attributes to find the best attribute descriptions for the input images. Different from existing methods that learn a specific classifier for each attribute category, we model the pedestrian at a part-level and explore the searching method to handle the unseen attributes. Finally, a many-to-many contrastive (MTMC) loss with masked tokens is proposed to train the network since a pedestrian image can comprise multiple attributes. Extensive experiments have been conducted on benchmark PAR datasets with an open-attribute setting. The results verified the effectiveness of the proposed POAR method, which can form a strong baseline for the POAR task.
 
  %(\eg, head, upper body, lower body, feet, etc.)
\end{abstract}

\begin{IEEEkeywords}
Class, IEEEtran, \LaTeX, paper, style, template, typesetting.
\end{IEEEkeywords}


\section{Introduction}
\label{sec:intro}
\IEEEPARstart{P}{edestrian} attribute recognition (PAR) aims to predict attributes of a target pedestrian, such as gender, age, clothing, accessories, etc. As the applications in person
 search~\cite{ZhangPgan,He_2021_ICCV,zhang2022local} and scene understanding~\cite{Wang_2022_CVPR,chang2021comprehensive} are getting more attention, PAR has become an active and significant research topic in computer vision. Existing methods such as global-local methods~\cite{Tang_2019_ICCV,Guo_2019_CVPR,DBLP:conf/bmvc/LiuLYS18}, attention methods~\cite{liu2017hydraplus,DBLP:conf/bmvc/LiuLYS18}, textual semantic correlations methods~\cite{9782406,2022arXiv220708677L} address the PAR problem by training a multi-label classifier within a predetermined small attribute space. Thus they cannot be used to recognize attributes that do not exist in the predefined classes, such as ``cotton'' and ``long coat'' shown in Figure~\ref{fig:onecol}. In this work, we aim to explore how to handle the new attributes in open-world and discuss a new pedestrian open-attribute recognition (POAR) problem.

\begin{figure}[t]
	\centering
	\includegraphics[width=1.0\linewidth]{./imgs/frame_compare.jpg}	
	\caption{The comparison of pedestrian attribute recognition (PAR) and pedestrian open-attribute recognition (POAR). The upper part shows the current PAR methods that are based on multi-label classification, where the attribute categories have been predefined. During the test, PAR methods cannot recognize new attributes that are beyond the predefined classes, such as cotton and long coat. The lower part presents our basic idea. We encode the images and attributes into a joint image-text feature space. Then, the attribute labels are determined based on the rank of the similarities between image embeddings and attribute embeddings.}
	\label{fig:onecol}
\end{figure}   

Pedestrian attribute recognition (PAR) aims to predict attributes of a target pedestrian, such as gender, age, clothing, accessories, etc. As the applications in person search~\cite{ZhangPgan,He_2021_ICCV,zhang2022local} and scene understanding~\cite{Wang_2022_CVPR,chang2021comprehensive} are getting more attention, PAR has become an active and significant research topic in computer vision. Existing methods such as global-local methods\cite{Tang_2019_ICCV,Guo_2019_CVPR,DBLP:conf/bmvc/LiuLYS18}, attention methods\cite{liu2017hydraplus,DBLP:conf/bmvc/LiuLYS18}, textual semantic correlations methods\cite{9782406,2022arXiv220708677L} address the PAR problem by training a multi-label classifier within a predetermined small attribute space. Thus they cannot be used to recognize attributes that do not exist in the predefined classes, such as ``cotton'' and ``long coat'' shown in Figure~\ref{fig:onecol}. In this work, we aim to explore how to handle the new attributes in open-world and discuss a new pedestrian open-attribute recognition (POAR) problem.

Recently, Radford {\it et al.}~\cite{radford2021learning} proposed a CLIP (Contrastive Language-Image Pre-Training) method to model the similarity relationship between images and raw text. CLIP is trained in a task-agnostic setting and can be used to recognize general objects, such as airplane, bird, ocean, and so on. However, it cannot be directly used to recognize more fine-grained attributes, such as ``upper stride'', ``lower stripe'', ``lower jeans'', ``lower trousers'', in the PAR task. For the PAR, the challenge is that one pedestrian has multiple attributes and there is no corresponding location and scale information in the ground truth label set. To address this challenge, part-based~\cite{7139070} and attention mechanism~\cite{Tang_2019_ICCV} methods have been proposed to localize attributes and learn the attribute-specific regional features. However, these methods ignore the intra-region label conflicts, e.g., ``long sleeve'' and ``short sleeve''. Zhao {\it et al.} ~\cite{zhao2018grouping} proposed a grouping attribute recognition method by dividing all labels into different groups and localizing regions corresponding to each group attribute based on the detection algorithm. However, the detection algorithm is attributed sensitive. Similarly, we also divide the whole attribute classes into multiple groups and each group corresponds to one visual region, as shown in Figure~\ref{fig:onecol}. But differently, we propose using masks to block out distracting and less informative regions and mask attribute tokens that do not relate to the attribute group. We localize attributes and learn their features based on token prediction with masks rather than detection.

Different from general PAR task that recognizes only the seen attributes in the training set, our POAR task allow the pedestrian attributes to expand beyond the seen attributes based on the interests and application needs. To address this challenge, we formulate the POAR problem as an image-text search problem, which is trained in a downstream attribute-agnostic manner under the supervision of natural language. Different from the CLIP that one image or object has only one class name, our method can use multiple attributes to describe one person in the POAR task. Using Figure~\ref{fig:onecol} as an example, ``male and long coat '' are used to describe this pedestrian. 
To address this problem, we propose a multiple attribute tokens ([ATT]) encoding method in the image encoding step. As shown in Figure~\ref{fig:onecol}, we divide the attributes into multiple groups and each group is encoded with an attribute token. The original image is encoded with multiple attribute tokens corresponding to multiple groups. Moreover, we block out less informative regions for the image token and filter out distracting attribute relations for the attribute tokens in each group. Based on the masking strategy and the many-to-many property, we propose a many-to-many contrastive (MTMC) loss with masked tokens to guide the parameter update of the network.

Our model is trained in an end-to-end manner. During training, we extract visual and text embeddings using the corresponding encoders. The similarity matrix of image-text pairs in a mini-batch can be calculated based on these embeddings. Then, the proposed many-to-many contrastive loss with masked embeddings is used to guide the learning of the model.
During testing, attribute tokens of unseen attributes such as ``cotton'' and ``long coat'' in Figure~\ref{fig:onecol} can be localized based on our model with the masking method. Then, features of those unseen attributes can be extracted from the localized regions. Meanwhile, text embeddings of those attributes can be extracted using the text encoder. Finally, the attributes of one pedestrian can be recognized based on feature similarities of attribute tokens and text. Because the trained attribute encoder can be used to encode attributes which have not been seen during training, the proposed method can be used to recognize open attributes of the pedestrian in the open world. 

Our contributions can be summarized as follows:

\begin{itemize}
	\item
	We formulate the problem of pedestrian open-attribute recognition (POAR) and develop a simple yet effective method to address it.
\end{itemize}

\begin{itemize}
	\item	
	We propose a masking mechanism to address problems of multiple attribute localization and multiple tokens encoding in the POAR task. Furthermore, a many-to-many contrastive loss with masked tokens is proposed to train the network.
\end{itemize}

\begin{itemize}
	\item	
	Extensive experiments on benchmark datasets with an open-attribute setting have been conducted to demonstrate the effectiveness of the proposed method, which forms a strong baseline for the POAR task.
\end{itemize}


%-------------------------------------------------------------------------

%-------------------------------------------------------------------------
\section{Related Work}
\label{sec:rewo}
This work is related to pedestrian attribute recognition and open-world recognition. In this section, we review the existing methods on these topics.

\begin{figure*}[t]
	\centering
	\includegraphics[width=0.8\linewidth]{./imgs/framework.jpg}
	\caption{The proposed POAR framework. The proposed model is designed for pedestrian open-attribution recognition. We evaluate the dot similarity of features from the image and text encoder and then determine the attributes of the pedestrian in the image.}
	\label{fig:framework}
\end{figure*} 

\subsection{Pedestrian Attribute Recognition}
Pedestrian attribute recognition (PAR) has attracted increasing interest in person recognition \cite{Cao_2018_CVPR,DBLP:conf/aaai/HandC17,Jia_2021_ICCV,Liu_2016_CVPR,DBLP:journals/corr/abs-2107-12666} and scene understanding \cite{Jia_2021_ICCV,Liu_2016_CVPR,DBLP:journals/corr/abs-2107-12666}. The mainstream methods address this problem by building a multi-label classifier based on CNN. To improve the recognition accuracy, global methods\cite{li2015multi,liu2017hydraplus}, local methods\cite{Sarafianos_2018_ECCV}, and global-local attention-based methods \cite{DBLP:conf/bmvc/SarfrazSWS17} are proposed. Nikolaos {\it et al.} \cite{Sarafianos_2018_ECCV} proposed an effective method to extract and aggregate visual attention masks across different scales.
Tang {\it et al.} \cite{Tang_2019_ICCV} proposed a flexible attribute localization module to learn attribute-specific regional features. 
These methods focused on domain-specific model designing. To use additional domain-specific guidance, M. Kalayeh {\it et al.} \cite{Kalayeh_2017_CVPR} used semantic segmentation methods to learn attention maps for accurate attribute prediction. 
Liu {\it et al.}\cite{Liu_2016_CVPR} learned clothing attributes with additional landmark labels. 
Li {\it et al.} \cite{2022arXiv220708677L} proposed an image-conditioned masked language model to learn complex sample-level attribute correlations from the perspective of language modeling.
Cheng {\it et al.} \cite{9782406} proposed an additional textual modality to explore the textual semantic correlations from attribute annotations. 
These methods are trained on a predefined attribute set and used to recognize the same attributes, which limits the flexibility of these models. In our work, we build a pedestrian open-attribute recognition (POAR) model based on the CLIP \cite{radford2021learning} model to flexibly recognize both seen and unseen pedestrian attributes.


\subsection{Open World Recognition}
POAR problem is an open-world recognition (OWR) problem for pedestrian attributes. In classification, OWR is first proposed by Scheirer \cite{6365193}, which aims to discriminate known from unknown samples as well as classify known ones. Latter, prototype-based method\cite{saranrittichai2022multi,vaze2022openset}, knowledge distillation method\cite{gu2021open}, and out-of-distribution detection method\cite{esmaeilpour2022zero} become popular in image classification and object detection.
Esmaeilpour {\it et al.} \cite{esmaeilpour2022zero} used an extended model to generate candidate unknown class names for each test sample and compute a confidence score based on both the known class names and candidate unknown class names for zero-shot out-of-distribution detection. 
Oza {\it et al.} \cite{oza2019c2ae} proposed the class conditional auto-encoder to tackle open-set recognition, which includes closed-set training and open-set training stages. Gu {\it et al.} \cite{gu2021open} adopted an image-text pre-trained model as a teaching model to supervise student detectors. Zhao {\it et al.}\cite{zhao2020object} unified the label space from the training of multiple datasets to improve the generalization ability of a model. In addition, some methods \cite{li2019zero,rahman2020improved} aligned region features and the pre-trained text embeddings in base categories to realize zero-shot detection. The CLIP model was proposed by Radford {\it et al.}\cite{radford2021learning}, which performs task-agnostic training via natural language prompting. CLIP can realize zero-shot image recognition. However, CLIP is usually used to recognize general objects, such as airplane, bird, ocean, and so on. For fine-grained attribute recognition, such as attributes of a pedestrian and the granular features of birds, CLIP will fall short in most situations. In our work, we build a model for the recognition of pedestrian open attributes.

\section{Method}
\label{sec:method}
In this section, we first define the pedestrian open-attribute recognition (POAR) problem. Then we introduce our POAR framework, as shown in Figure~\ref{fig:framework}. Finally, we present the details of the loss function.

\subsection{Pedestrian Open-Attribute Recognition}

\begin{table*}[htbp]
	\centering
	\caption{Attribute labels of the PETA dataset are converted to sequence by the prompt.}
	\resizebox{2.0\columnwidth}{!}{%
		\begin{tabular}{lll}
			\hline
			% \multicolumn{3}{c}{PETA dataset} \\
			% \hline
			ATTRIBUTES & KEY   & PROMPT \\
			\hline
			Long, short  & Hair  & This person has $\left \{ \right\}$ hair. \\
			Male, Female  & Gender & This person is $\left \{ \right\}$. \\
			Less15, Less30, Less45, Less60, Larger60  & Age   & The age of this person is $\left \{ \right\}$ years old. \\
			Backpack, MessengerBag, PlasticBags, Other, Nothing  & Carry & This person is carrying $\left \{ \right\}$. \\
			Sunglasses, Hat, Muffler, Nothing  & Accessory & This person is accessory $\left \{ \right\}$. \\
			LeatherShoes, Sandals, Sneaker, Shoes & Foot  & This person is wearing $\left \{ \right\}$ in foot. \\
			Casual, Formal, Jacket, Logo, ShortSleeve, Plaid, Stripe, Tshirt, VNeck, Other  & Upperbody & This person is wearing $\left \{ \right\}$ in upper body. \\
			Casual, Formal, Trousers, ShortSkirt, Shorts, Plaid, Jeans  & Lowerbody & This person is wearing $\left \{ \right\}$ in lower body. \\
			\hline
	\end{tabular}}%
	\label{tab:prompt}%
\end{table*}%


Pedestrian attribute recognition (PAR) aims to recognize the fine-grained attributes of a person (e.g., hairstyle, age, gender, etc.) from the given pedestrian image. The conventional PAR usually predetermines a set of pedestrian attributes and follows the close-set assumption during both the training and test phases. Suppose we have $M$ predetermined pedestrian attributes $\mathcal{A}=\{A_1, A_2, \dots, A_M\}$, e.g., $A_1=$``long hair", $A_2=$``short hair", etc. Then, given a labeled pedestrian dataset $\mathcal{D} = \{(I_i, \mathcal{A}_i)\}_{i=1}^N$, where each image $I_i \in \mathbb{R}^{H\times W\times 3}$ is annotated with a subset $\mathcal{A}_i \subset \mathcal{A}$ to denote the existing pedestrian attributes. The main objective of PAR is to learn a model to answer which pedestrian attributes from $\mathcal{A}$ appear in the given image $I$.

Existing approaches \cite{Tang_2019_ICCV,Guo_2019_CVPR,9782406} usually convert this to a multi-label classification problem. However, the pedestrian attributes in the real world are potentially unlimited. It is difficult to exhaust all of them in a predetermined attribute set and collect the corresponding pedestrian images. Unseen attributes are highly possible to exist in real applications, while existing classification-based methods \cite{9782406,2022arXiv220708677L} are inherently incapable of handling this case. To address this issue, we introduce a pedestrian open-attribute recognition (POAR) problem in this work. Let $\mathcal{A}_u = \{A_{M+1}, A_{M+2}, \dots, A_{M+M_u} \}$ denote a set of extra attributes which are also of interest in the test phase but out of the predetermined attribute set $\mathcal{A}$. In POAR, we expect that the model can not only recognize the seen attributes from $\mathcal{A}$ but also the unseen attributes from $\mathcal{A}_u$.

\subsection{Framework}

\textbf{Overview.}
As shown in Figure~\ref{fig:framework}, we use an image-text contrastive learning framework with an image encoder $\mathbf{\Phi}_I$ and a text encoder $\mathbf{\Phi}_T$. The image encoder aims to process input images and derive the visual representation of various pedestrian attributes. Besides, we construct a set of text descriptions (e.g., ``this person has long hair.'', ``this person is carrying backpack.'', etc. We utilize the text encoder of CLIP to encode these attribute descriptions into text embeddings. Then, we compute the vision-text similarity to find the best-matched pedestrian attributes for the input images. Different from the original CLIP, one pedestrian may have more than one associated attribute, as shown in Figure~\ref{fig:relation}. To train our model, we propose a many-to-many contrastive (MTMC) loss with masked tokens to handle the many-to-many relationships between the images and text descriptions. The details of each part are introduced below.

\textbf{Image Encoding.}
The image encoding process is organized based on token prediction with Transformer as follows. First, the input image $I$ is split into a sequence of non-overlapping small patches \{$P_0$, $P_1$, $\cdots$, $P_{S-1}$\}, where $P_i \in \mathbb{R}^{{r}\times {r}\times 3}$ and $S=\frac{H}{r}\times \frac{W}{r} $. Then, each patch $P_i$ is projected to an embedding vector $\mathbf{x}_i \in \mathbb{R}^{D}$ ([PAT]), where $D$ denotes the feature dimension. Thus, we can obtain $\mathbf{X} = [\mathbf{x}_0, \mathbf{x}_1, \dots, \mathbf{x}_{S-1}] \in \mathbb{R}^{D \times S}$ with $S$ patch embeddings. Inspired by \cite{dosovitskiy2020image,radford2021learning}, we introduce $K$ learnable attribute tokens  $\mathbf{Z} = [\mathbf{z}_0, \mathbf{z}_1, \dots, \mathbf{z}_{K-1}] \in \mathbb{R}^{D\times K}$, where each attribute token $\mathbf{z}_k \in \mathbb{R}^D$ shares the same dimension as $\mathbf{x}_i$.
For each patch embedding and each class token, we generate a corresponding position encoding $\mathbf{e}_j \in  \mathbb{R}^D$, where $j \in \{0, 1, \cdots, S+K-1\}$. The input of the image encoder $\mathbf{\Phi}_I$ is the concatenation of class tokens and patch embeddings combined with their positional embeddings $\mathbf{E}=[\mathbf{e}_0,\mathbf{e}_1,\cdots,\mathbf{e}_{S+K-1}]\in \mathbb{R}^{ D \times (S+K)}$, which is denoted as $\mathbf{V}=[\mathbf{Z},\mathbf{X}]\textcircled{+}\mathbf{E}$, where $\mathbf{V}\in \mathbb{R}^{D\times (S+K)}$, $[,]$ is the concatenate operation, and $\textcircled{+}$ is the element-wise summation notation. The output of the image encoder $\mathbf{\Phi}_I$ is the learned embeddings of class tokens, denoted as $\hat{\mathbf{Z}} \in \mathbb{R}^{D\times K}$. The above process is summarized as follows:
\begin{equation}
\hat{\mathbf{Z}} = \mathbf{\Phi}_I(\mathbf{V}).
\label{image-encoding}
\end{equation}

\textbf{Text Encoding.}
The process of text encoding is formed based on the text encoder $\mathbf{\Phi}_T$ transferred from the CLIP model. The input of the text encoder is the prompts of pedestrian attributes which are organized as follows. First, the attributes are divided into $K$ groups based on the characteristics of pedestrians, as shown in Table $\ref{tab:prompt}$. Then, we format a prompt template for the attributes of each group. Finally, each prompt sentence is tokenized into an embedding using the Byte-Pair-Encoding method\footnote{\url{https://huggingface.co/transformers/v4.8.0/model_doc/clip.html}}. Suppose that the maximum number of words in these sentences is $L$. Then, each sentence is tokenized into a vector 
$\mathbf{y}_i \in \mathbb{R}^{L}$. For the $k$-th group, we can obtain $m$ sentence vector corresponding with $m$ prompts in this group, denoted as $\mathbf{Y}^k=[\mathbf{y}_1^k,\mathbf{y}_2^k,\cdots,\mathbf{y}_m^k] \in \mathbb{R}^{L\times m}$. The output of the text encoder $\mathbf{\Phi}_T$ is the learned text embeddings $\hat{\textbf{Y}}^k$ for the prompts of the $k$-th group. The above process can be defined as:
\begin{equation}
\hat{\textbf{Y}}^k = \mathbf{\Phi}_T(\mathbf{Y}^k),
\label{text-encoding}
\end{equation}
where $\hat{\textbf{Y}}^k \in \mathbb{R}^{D\times m}$. 

\textbf{Many-to-Many Contrastive Loss.}
\begin{figure}[t]
	\centering
	\includegraphics[width=1.0\linewidth]{./imgs/relation.jpg}
	
	\caption{ Diagram of image-text relationship. The right part represents the many-to-many relationship between text and images.}
	\label{fig:relation}
\end{figure} 
For a mini-batch images $\{I_1, I_2, \cdots, I_B\}$, we first extract their token embeddings $\hat{\mathbf{Z}}_b$ and text embeddings $\hat{\mathbf{Y}}_b$ based on formulas ($\ref{image-encoding}$) and ($\ref{text-encoding}$), respectively. $\hat{\mathbf{Z}}_b$ and $\hat{\mathbf{Y}}_b$ are sets of token embeddings and text embeddings in all attribute groups related to the given mini-batch images, respectively. Different from the CLIP model which has a one-to-one relationship between image and text. In POAR, the image and text are involved in a many-to-many relationship, as shown in Figure \ref{fig:relation}. To effectively tackle this scenario, a loss function combined with visual-to-text and text-to-visual contrastive learning is proposed. The visual-to-text contrastive learning term is defined as follows:
\begin{equation}
\mathcal{L}_{v2t}=- {\textstyle \sum_{i=1}^{v}} {\textstyle \sum_{j=1}^{t_i}} log\frac{ exp\left ( \hat{\mathbf{z}}_i^{T} \hat{\mathbf{y}}_{j}^{+}/{\tau }\right )  }{ {\textstyle \sum_{k=1}^{t}}exp\left ( \hat{\mathbf{z}}_i^{T} \hat{\mathbf{y}}_{k}/{\tau }\right )  },
\label{eq:v2t}
\end{equation}
where $\tau$ is a temperature parameter, $t_i$ is the number of positive text embeddings that have the same attribute label with  $\hat{\mathbf{z}}_i$. $v$ and $t$ are the number of token embeddings and text embeddings, respectively. $\hat{\mathbf{z}}_i \in \hat{\mathbf{Z}}_b$, $\hat{\mathbf{y}}_{k} \in \hat{\mathbf{Y}}_b$, $\hat{\mathbf{y}}_{j}^{+} \in \hat{\mathbf{Y}}_b$. $\hat{\mathbf{z}}_i$ and $\hat{\mathbf{y}}_{j}^{+}$ is a positive pair which means that they share the same attribute label. Similarly, the text-to-visual contrastive learning term is defined as follows:
\begin{equation}
\mathcal{L}_{t2v}=- {\textstyle \sum_{j=1}^{t}} {\textstyle \sum_{i=1}^{v_j}} log\frac{ exp\left ( \hat{\mathbf{y}}_j^{T} \hat{\mathbf{z}}_{i}^{+}/{\tau }\right )  }{ {\textstyle \sum_{k=1}^{v}}exp\left ( \hat{\mathbf{y}}_j^{T} \hat{\mathbf{z}}_{k}/{\tau }\right )},
\label{eq:t2v}
\end{equation}
where $v_j$ is the number of positive token embeddings that have the same attribute label with  $\hat{\mathbf{y}}_j$. The final loss function is the combination of (\ref{eq:v2t}) and (\ref{eq:t2v}), as follows:
\begin{equation}
\mathcal{L}=\mathcal{L}_{v2t}+\mathcal{L}_{t2v}.
\label{loss:all}
\end{equation}

To this end, we have described the main framework of our POAR. To train the network, a masking strategy is introduced in the following section. 

\subsection{Encoder Networks and Mask Strategy}

In this part, we present the details of the image encoder and our mask strategy for network training.

\textbf{Image Encoder.}
The image encoder $\mathbf{\Phi}_I$ is a stack of multiple Transformer blocks. The Transformer block is composed of layer norm (LN) layers  \cite{dosovitskiy2020image}, a multi-head self-attention (MSA) layer  \cite{dosovitskiy2020image}, and a multi-layer perceptron (MLP) network.  In the multi-head self-attention layer, the attention weights of each attribute token are automatically calculated among all input tokens. Specifically, the attention weights between the $k$-th attribute token and the others can be written as
\begin{equation}
\text{Attn}_{\text{token}}(\mathbf{z}_k)=\text{softmax}\Big(\frac{[\mathbf{z}_k^\top \cdot \mathbf{Z}, \;\; \mathbf{z}_k^\top \cdot \mathbf{X}]}{\sqrt{D}}\Big).
\end{equation}
We observe that the first term $\mathbf{z}_k^\top \mathbf{Z}$ often dominates the attention, making the module hardly find the true regions of interest from the input image. This shortcut learning often leads to overfitting and inferior results in our experiments. To address this issue, we mask out the self-attentions between the attribute tokens. We use this way to force them to independently extract useful visual information from the input image rather than simply relying on the information that has been extracted by the others. Specifically, we calculate the attention weight of the $k$-th attribute token as
\begin{equation}
\text{Attn}_{\text{mask}}(\mathbf{z}_k)=\text{softmax}\Big(\frac{[\mathbf{z}_k^\top \cdot \mathbf{X}]}{\sqrt{D}}\Big).
\end{equation}

In our experiments, we observe that this technique leads to significant performance gain (Table \ref{tab:abla}). The mask strategy is described as follows.

In the PAR task, one key challenge is that a pedestrian can have multiple attributes and there is no corresponding location and scale information in the ground truth label set. We propose the above mask strategy to tackle this challenge. Specifically, we divide the whole attribute classes into multiple groups and each group ([ATT] token) corresponds to one visual region, we mask out regions ([PAT] token) that do not need to pay attention to. For example, the ``hair'' class token pays more attention to the head of the pedestrian and we block out regions on the lower part of the head region, the ``upper body'' class token pays more attention to the top part of the image and we block out the bottom part of the image, etc. 
Thus, the final attention can be calculated as follows
\begin{equation}
\text{Attn}_{\text{mask}}(\mathbf{z}_k^l) = \text{softmax}(\frac{\mathbf{z}_k^\top \cdot \mathbf{X} + \mathbf{\varpi }}{\sqrt{D}}),
\end{equation}
where $\mathbf{\varpi } \in \mathbb{R}^{1\times S}$ is a mask vector with value $-\infty$ for the blocked image patches and $0$ otherwise. Taking the ``hair" class token, for example, the region of the head will be $0$ and the remaining regions will be $-\infty$. The output of the self-attention unit is as follows:
\begin{equation}
\mathbf{F}^{l}=\text{Attn}_{\text{mask}}(\mathbf{Z_k}^{l-1})\cdot\mathbf{V}^{l-1},
\end{equation}
where $l$ is the layer index of the self-attention, $\mathbf{Z}^0=\mathbf{Z}$, $\mathbf{V}^0=\mathbf{V}$. The whole process of a Transformer block can be formulated as:
\begin{equation}
{\hat{\mathbf{V}}}^l=\text{MSA}(\text{LN}({\mathbf{V}}^{l-1}))+{\mathbf{V}}^{l-1},
\label{eq:msa}
\end{equation}  
\begin{equation}
{\mathbf{V}}^l=\text{MLP}(\text{LN}({\hat{\mathbf{V}}}^{l}))+{\hat{\mathbf{V}}}^{l}.
\label{eq:mlp}
\end{equation}  

% \textbf{Text Encoder.}
% The text encoder $\mathbf{\Phi}_T$ is similar to the image encoder, which is also a stack of multiple Transformer blocks. The Transformer block is also a combination of LN, MSA, and MLP layers, as the formulation of Eq. (\ref{eq:msa}) and (\ref{eq:mlp}). The difference is that the input of the text encoder is the attribute embeddings.

\textbf{Contrastive Learning with Masked Tokens.} 
It should be noted that our contrastive loss is computed based on all the token embeddings and attribute embeddings. The embeddings of those masked tokens are also used to compute the loss, which is because the contrastive loss computed with the masked embeddings will lead to more robust embedding learning.

%\subsection{Open-vocabulary Attribute Recognition}
% Image embedding: We train a proposal network on the given categories and extract the specific region image embedding, where I is the image.
% Text embedding: We generate the text embedding offline by feeding the category texts with prompt templates into the text encoder T. Then, we compute cosine similarities between the image and text embedding.
% 
% Knowledge distillation is a teacher-student training structure where a pre-trained teacher model provides knowledge, and a student model acquires the teacher's knowledge through distillation training. It can transfer the knowledge from a complex teacher model to a simpler student model at the cost of slight performance loss.

%------------------------------------------------------------------------
\section{Experiments}
%-------------------------------------------------------------------------
%Following the existing pedestrian attribute recognition paper, we evaluate our POAR method on three benchmark datasets with an open-attribute setting.

\begin{table}[htbp]
	\centering
	\caption{The number of attribute classes in the test phase. SN and UN represent the number of seen classes and unseen classes in the test sets, respectively.}
	\begin{tabular}{c|cc|cc|cc}
		\toprule
		\multirow{3}[4]{*}{Source Domain} & \multicolumn{6}{c}{Target Domain} \\
		\cmidrule{2-7}          & \multicolumn{2}{c|}{PETA} & \multicolumn{2}{c|}{PA100K} & \multicolumn{2}{c}{RAPv1} \\
		& SN    & \multicolumn{1}{l|}{UN} & SN    & UN    & SN    & UN \\
		\midrule
		PETA  & 35    & 0     & 10    & 16    & 14    & 37 \\
		PA100K & 12    & 23    & 26    & 0     & 9     & 42 \\
		RAPv1 & 16    & 19    & 10    & 16    & 51    & 0 \\
		\bottomrule
	\end{tabular}%
	\label{tab:number}%
\end{table}%

\begin{figure*}[ht]
	\centering
	\includegraphics[width=0.9\linewidth]{./imgs/analysis.jpg}
	\caption{Performance comparison of different groups ([ATT] tokens) by different methods based on the same experimental settings. (a), (b), and (c) represent the results on PETA, PA100K, and RAPv1 datasets, respectively.}
	\label{fig:analysis}
\end{figure*} 

\subsection{Datasets and Experimental Settings}

\textbf{Datasets and Evaluation Metrics.}
The proposed POAR method is evaluated on three benchmark datasets, i.e., PETA \cite{deng2014pedestrian}, RAPv1 \cite{li2015deepmar}, and PA-100K \cite{liu2017hydraplus} with an open-attribute setting. The Recall@K and mA are used to evaluate the performance of our open-attribute recognition model. Our model is trained on one dataset and evaluated on the other two datasets.

\begin{table*}[ht]
	\centering
	\caption{Performance comparison of POAR experimental results. Blue indicates the model is trained and tested on the same dataset. }
	\begin{tabular}{c|c|cc|cc|cc}
		\toprule
		\multirow{3}[4]{*}{Method} & \multirow{3}[4]{*}{Source Domain} & \multicolumn{6}{c}{Target Domain} \\
		\cmidrule{3-8}          &       & \multicolumn{2}{c|}{PETA} & \multicolumn{2}{c|}{PA100k} & \multicolumn{2}{c}{RAPv1} \\
		&       & R@1   & R@2   & R@1   & R@2   & R@1   & R@2 \\
		\midrule
		CLIP* & –     & 50.2  & 75.7  & 43.4  & 65.9  & 33.6  & 56.5 \\
		%\hline
		VTB*  & PA100K & 31.4  & 62.2  & \textcolor[rgb]{ .357,  .608,  .835}{26.9} & \textcolor[rgb]{ .357,  .608,  .835}{62.2} & 24.2  & 50.7 \\
		\midrule
		Ours  & \multirow{2}[2]{*}{PETA} & \textcolor[rgb]{ .357,  .608,  .835}{87.6} & \textcolor[rgb]{ .357,  .608,  .835}{96.0} & 45.1  & 73.5  & \textbf{42.2} & 68.6 \\
		Ours+CLIP &       & –     & –     & 44.7  & \textbf{74.7} & 42.1  & \textbf{69.7} \\
		\midrule
		Ours  & \multirow{2}[2]{*}{PA100K} & 42.3  & 76.2  & \textcolor[rgb]{ .357,  .608,  .835}{83.3} & \textcolor[rgb]{ .357,  .608,  .835}{92.6} & 39.4  & 63.6 \\
		Ours+CLIP &       & 50.9  & 77.5  & –     & –     & 39.4  & 64.5 \\
		\midrule
		Ours  & \multirow{2}[2]{*}{RAPv1} & 42.9  & 72.5  & 38.3  & 62.6  & \textcolor[rgb]{ .357,  .608,  .835}{75.8} & \textcolor[rgb]{ .357,  .608,  .835}{88.8} \\
		Ours+CLIP &       & \textbf{52.9} & \textbf{78.8} & \textbf{45.5} & 67.2  & –     & – \\
		\bottomrule
	\end{tabular}%
	\label{tab:zero-shot1}%
\end{table*}%


\textbf{Implementation Details.} 
Our experiments are conducted based on the ViT-B/16 backbone networks which are stacked with 12 Transformer blocks. The input image size is set to 224$\times$224. The value of $r$ is set to 16. and $K$ is set to be 8, 8, and 11 in PETA, PA100K, and RAPv1 respectively. The learning rate is $5e^{-2}$ with a weight decay of 0.2. The model is trained from scratch with 100 epochs. The temperature $\tau$ in our contrastive loss is set to be 1. Data augmentation with random horizontal flip and random erasing are used during training. 


\subsection{Performance Comparison of POAR}
We compare the performance of our method with the CLIP \cite{radford2021learning} and VTB \cite{9782406} methods based on the image-to-text $K$-nearest neighbor retrieval idea. In Table~\ref{tab:number}, SN and UN are the number of seen and unseen attributes used in the test sets, respectively. Performance comparison with the top-1 and top-2 recall rates are shown in Table~\ref{tab:zero-shot1}.

From Table~\ref{tab:zero-shot1}, we can see that the Recall@1 scores of our method are 1.7\% and 8.6\% higher than the CLIP method when the model is trained on the PETA dataset and evaluated on the PA100K and RAPv1 datasets, respectively. When the model is trained on the PA100K and evaluated on the PETA dataset, the recall@1 score of our method is 7.9\% lower than the CLIP model. When the model is trained on the RAPv1 and evaluated on the PETA and PA100K datasets, the recall@1 scores of our method are 7.3\% and 5.1\% lower than the CLIP model. This is because the CLIP model is trained on 400 million image-text pairs which have a high probability to contain ``age'' and ``hair'' and other attributes, thus those unseen attributes defined in our experiments can be considered seen in the CLIP model. However, when we fuse the features of our model and the CLIP model, the highest performance for the POAR task can be obtained.

To analyze the details of the POAR performance, we show the performance comparison of each group of the CLIP model and the proposed POAR model in Figure~\ref{fig:analysis}. We can see that the CLIP model has higher recognition performance for attributes in ``age'' and ``hair'' groups, which has a high probability of being seen by the CLIP model during training. For attributes in the ``upper body'' group that has a low probability of being seen by the CLIP model, the recognition performance of our model is much higher than the CLIP model.

\begin{figure}[t]
	\centering
	\includegraphics[width=1.0\linewidth]{./imgs/unseen.jpg}
	\caption{The R@1 scores of seen and unseen attributes of each attribute token for the PA100K dataset.}
	\label{fig:unseen}
\end{figure} 

\begin{table}[tbp]
	\centering
	\caption{Evaluation of each component on the PETA dataset. SC represents a single attribute token. MC represents multiple attribute tokens. CM represents an attribute token that is blocked out with the mask. Block represents an image patch that is blocked out with the mask.}
	\begin{tabular}{cccc|cccc}
		\toprule
		SC    & MC    & CM    & Block & mA    & F1    & R@1  & R@2 \\
		\midrule
		$\surd$     &       &       &       & 81.1  & 83.0  & 85.7  & 94.7 \\
		&$\surd$    &       &       & 80.6  & 82.2  & 86.2  & 95.7 \\
		&$\surd$    & $\surd$   &       & 81.0  & 83.0  & 86.4  & 95.7 \\
		& $\surd$    & $\surd$     & $\surd$    & \textbf{83.1} & \textbf{84.4} & \textbf{87.6} & \textbf{96.0} \\
		\bottomrule
	\end{tabular}%
	\label{tab:abla}%
\end{table}%

% Table generated by Excel2LaTeX from sheet 'ablation'
\begin{table}[tbp]
	\centering
	\caption{Our test results for different loss functions in the PETA dataset. OTOC represents the one-to-one contrastive loss. MTMC represents the many-to-many contrastive loss. }
	\begin{tabular}{cc|cccc}
		\toprule
		OTOC & MTMC & mA    & F1    & R@1 & R@2 \\
		\midrule
		$\surd$   &       & 73.0  & 73.4  & 86.3  & 95.8 \\
		$\surd$     & $\surd$     & 81.5  & 83.3  & 86.3  & 95.8 \\
		&$\surd$    & \textbf{83.1} & \textbf{84.4} & \textbf{87.6} & \textbf{96.0} \\
		\bottomrule
	\end{tabular}%
	\label{tab:loss}%
\end{table}%

\begin{figure}[t]
	\centering
	\includegraphics[width=0.95\linewidth]{./imgs/attention.jpg}
	\caption{The attention map of each class token on PETA dataset.}
	\label{fig:attention}
\end{figure} 

\begin{table*}[htbp]
	\centering
	\caption{Results of text-to-image retrieval on different datasets. Blue indicates that the train set and test set belong to the same dataset.}
	\begin{tabular}{c|c|ccccccccc}
		\toprule
		& \multirow{3}[6]{*}{Source Domain} & \multicolumn{9}{c}{Target Domain} \\
		\cmidrule{3-11}    Method &       & \multicolumn{3}{c}{PETA} & \multicolumn{3}{c}{PA100K} & \multicolumn{3}{c}{RAPv1} \\
		\cmidrule{3-11}          &       & R@1   & R@5   & R@10  & R@1   & R@5   & R@10  & R@1   & R@5   & R@10 \\
		\midrule
		POAR  & PETA  & \textcolor[rgb]{ 0,  .439,  .753}{90.3} & \textcolor[rgb]{ 0,  .439,  .753}{100.0} & \textcolor[rgb]{ 0,  .439,  .753}{100.0} & 38.5  & 57.7  & 61.5  & 35.3  & 52.9  & 62.8 \\
		POAR  & PA100K & 41.9  & 74.2  & 77.4  & \textcolor[rgb]{ 0,  .439,  .753}{88.5} & \textcolor[rgb]{ 0,  .439,  .753}{96.2} & \textcolor[rgb]{ 0,  .439,  .753}{100.0} & 33.3  & 56.9  & 64.7 \\
		POAR  & RAPv1 & 35.5  & 58.1  & 71.0  & 34.6  & 61.5  & 69.2  & \textcolor[rgb]{ 0,  .439,  .753}{96.1} & \textcolor[rgb]{ 0,  .439,  .753}{100.0} & \textcolor[rgb]{ 0,  .439,  .753}{100.0} \\
		\bottomrule
	\end{tabular}%
	\label{tab:T2I}%
\end{table*}%

% \subsubsection{Attention Map}
\begin{figure*}
	\centering
	% %   \hfill
	\begin{minipage}{0.9\linewidth}
		\includegraphics[width=1.0\linewidth]{./imgs/pe2pa.jpg}
		\caption{Examples of image and text matching on the PA100K dataset.}
		\label{fig:i2t2}
	\end{minipage}
	
	\centering
	\begin{minipage}{0.9\linewidth}
		\includegraphics[width=1.0\linewidth]{./imgs/pe2ra.jpg}
		\caption{Examples of image and text matching on the RAPv1 dataset.}
		\label{fig:i2t3}
	\end{minipage}
	\caption{Image-to-text retrieval examples. The model is trained on the PETA dataset. Prompts with blue color indicate that the predicted text is inconsistent with ground truth. Attributes with the pink box are unseen attributes during training.}
	\label{fig:i2t}
\end{figure*}

Furthermore, we compute the recognition performance of seen and unseen attributes in the PA100K dataset, respectively. Results are shown in Figure~\ref{fig:unseen}, we can see that the R@1 scores of seen attributes are much higher than unseen attributes for the ``age" group. For the ``upperbody'' group, the R@1 scores of unseen attributes are much higher than the seen attributes, which indicates that our model generalizes better for unseen attributes in this group. 

\subsection{Ablation Study}

Our ablation study is conducted on the PETA dataset using the close-set evaluation mechanism. Table~\ref{tab:abla} shows the image-to-text retrieval performance of different components in our proposed method. Table~\ref{tab:loss} shows the image-to-text retrieval performance for different loss functions. 
The one-to-one loss function is performed by defining all attributes of one image to a paragraph description as text input. From Table~\ref{tab:abla}, we can see that each component of the proposed method can contribute to the final performance gain. From Table~\ref{tab:loss}, we can see that the many-to-many loss function significantly improves the final performance.

\subsection{Visualization of the Attention Maps}

To show the effectiveness of the proposed masking method, we visualize the attention map of each attribute token on the PETA dataset, and the results are shown in Figure~\ref{fig:attention}. In our method, we proposed a masking strategy to block out distract regions and mask the corresponding attribute tokens. From the attention map, we can see that each attribute token is independently responsible for a specific part of the body, which shows the effectiveness of the proposed masking method. For example, the attention maps of the second column concentrated on the ``hair" part, and the attention maps of the third column concentrated on the whole body of a pedestrian which is related to the ``age'' of a person, etc.



\subsection{Text-to-Image Retrieval Results}

To show the generalization of our method, we also evaluate the text-to-image retrieval results. Each attribute sentence is encoded into a text embedding, and those text embeddings are used to retrieve similar images based on the attribute token embeddings, and the results are reported in Table~\ref{tab:T2I}. Compared to the image-to-text retrieve performance of Table~\ref{tab:zero-shot1}, we can see that the text-to-image recognition task is more challenging than the image-to-text recognition task, which is mainly due to that the text embedding space is more sparse than the image embedding space.



\subsection{Image-to-Text Retrieval Examples}

In Figure~\ref{fig:i2t}, we visualize some image-to-text retrieval examples based on embeddings obtained using our proposed POAR method. The model was trained on the PETA dataset. Figure~\ref{fig:i2t2} and Figure~\ref{fig:i2t3} show examples tested on the PA100K and RAPv1 datasets, respectively. Attributes with the pink box are unseen attributes during training. We can see that there are a lot of unseen attributes in the test set and our model can effectively recognize those unseen attributes. Prompts with blue color indicate that the predicted text is inconsistent with the ground truth. We can see that the proposed model is difficult to recognize the age and direction of one person as shown in Figure~\ref{fig:i2t2} (ii) and (iii) on the PA100K dataset. On the RAPv1 dataset, the model has more difficulty in recognizing the hair and id. These examples show that there is still room for improvement to recognize pedestrian attributes in open-world scenarios. In our future work, we will continue to address these problems.

% and tested on the PA100K (Figure~\ref{fig:i2t2}), and RAPv1 (Figure~\ref{fig:i2t3}) datasets, respectively. For Figure~\ref{fig:i2t2} and Figure~\ref{fig:i2t3}, the test set exists many unseen classes in the train set, for example, "back", "hold objects in front", "long trousers", etc. The searched texts of Figure~\ref{fig:i2t2}(i) were completely correct, even the unseen attribute ``back'' was also accurately recognized. The age prediction in (ii) is wrong. The model does not know the meaning of ``less than eighteen years old'' due to the model has never seen it. The predicted class of the ``carry'' class token was ``hold objects in front'', but the given class label is ``other'', so it predicted wrong. For Figure~\ref{fig:i2t3}(ii), the label of the ``Hair'' class token is ``black'', and the predicted class of the ``Hair'' class token is ``long hair''. The reason may be that the model pays attention to the hair features of the woman sitting behind the man. The label of the ``upper body'' class token is ``T-shirt'', and the predicted class is ``cotton''. These two classes are different attributes that can exist at the same time. In future work, we will further improve and optimize the overlap of attributes.

%\subsubsection{Text-to-image Retrieval Visualization}
% \begin{figure}[t]
%   \centering
%   \includegraphics[width=0.85\linewidth]{./imgs/T2I.jpg}
%   \caption{Text Search Image Visualization. The green (blue) dot indicates the given text description is consistent (inconsistent) with the ground truth of the image. }
%   \label{fig:T2I}
% \end{figure} 

%We also perform text-to-image searches on three datasets for models trained on the PETA dataset. Specifically, as shown in Figure~\ref{fig:T2I}, (a) is from the PETA dataset, (b) is from the RAP v1 dataset, and (c) is from the PA100K dataset. The texts of (b) and (c) belong to the unseen classes. (a) shows the Top-5 search results corresponding to the text ``This person is carrying messengerBag''. Although some of the images were not clear, the image of the ``messengerBag'' can be accurately found. For the PA100K and RAPv1 datasets with different background styles of the PETA dataset, the model can also search some correct images from some unseen categories. For the third line, only the first two images are correct, the latter three do not match the real label.

% \subsection{Discussion}
% %label
% In addition, the labels given in the current dataset are not completely reasonable, such as ``Jeans'' and ``Trousers'' in the PETA and RAP datasets. The two categories are not independent of each other, and ``Jeans'' is also a part of ``Trousers''. The label in the dataset is either ``Jeans'' or ``Trousers''. Our prediction of ``jeans'' as ``Trousers'' could be considered a false positive, but in practical terms, the result is not wrong. In future work, we will propose a new method to deal with the overlap of labels.

%reid

%------------------------------------------------------------------------
\section{Conclusions}
We have addressed the problem of pedestrian open-attribute recognition and proposed a novel method to tackle this problem. Our key idea is to formulate the POAR problem as an image-text search problem. Specifically, we proposed a multiple attribute tokens encoding method to encode image patches with attribute tokens. Then, a masking mechanism is devised to block out distracting image patches with less informative and mask attribute tokens that do not relate to the attribute group. Finally, a many-to-many contrastive loss function with masked tokens is developed to train the model. Experimental results on benchmark PAR datasets with an open-attribute setting show the effectiveness of the proposed method. 

\textbf{Limitations:} One limitation of the work is that the text encoder is transferred from the CLIP, we will design a more effective attribute encoder in our future work. Another limitation is that the input of the framework is the detected pedestrian, future work could be focused on integrating pedestrian detection and POAR into a unified framework.




%\begin{thebibliography}{1}

%\bibitem{ams}
%{\it{Mathematics into Type}}, American Mathematical Society. Online available: 
%
%\bibitem{oxford}
%T.W. Chaundy, P.R. Barrett and C. Batey, {\it{The Printing of Mathematics}}, Oxford University Press. London, 1954.
%
%\bibitem{lacomp}{\it{The \LaTeX Companion}}, by F. Mittelbach and M. Goossens
%
%\bibitem{mmt}{\it{More Math into LaTeX}}, by G. Gr\"atzer
%
%\bibitem{amstyle}{\it{AMS-StyleGuide-online.pdf,}} published by the American Mathematical Society
%
%\bibitem{Sira3}
%H. Sira-Ramirez. ``On the sliding mode control of nonlinear systems,'' \textit{Systems \& Control Letters}, vol. 19, pp. 303--312, 1992.
%
%\bibitem{Levant}
%A. Levant. ``Exact differentiation of signals with unbounded higher derivatives,''  in \textit{Proceedings of the 45th IEEE Conference on Decision and Control}, San Diego, California, USA, pp. 5585--5590, 2006.
%
%\bibitem{Cedric}
%M. Fliess, C. Join, and H. Sira-Ramirez. ``Non-linear estimation is easy,'' \textit{International Journal of Modelling, Identification and Control}, vol. 4, no. 1, pp. 12--27, 2008.
%
%\bibitem{Ortega}
%R. Ortega, A. Astolfi, G. Bastin, and H. Rodriguez. ``Stabilization of food-chain systems using a port-controlled Hamiltonian description,'' in \textit{Proceedings of the American Control Conference}, Chicago, Illinois, USA, pp. 2245--2249, 2000.
%
%\end{thebibliography}

%\begin{IEEEbiographynophoto}{Jane Doe}
%Biography text here without a photo.
%\end{IEEEbiographynophoto}

\bibliographystyle{IEEEtran}
\bibliography{egbib}


%\begin{comment}
%\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{bioimg/Shichao.jpg}}]{Shichao Kan}
%	received the B.E. and M.S. degrees from the School of Computer and Information Science, Beijing Jiaotong University, Beijing, China, in 2014 and 2016, respectively, where he is currently pursuing the Ph.D. degree. His research interests include general image retrieval, metric learning, image-to-image translation, large-scale image retrieval, object search, object detection and deep learning.
%\end{IEEEbiography}

\end{document}


