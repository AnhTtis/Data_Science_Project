\vspace{-0.05in}
\section{Experiments}

\subsection{Setup}
\noindent\textbf{Datasets.} We evaluate our model on one widely used video action recognition dataset UCF101~\cite{soomro2012ucf101}, one standard video compression dataset UVG~\cite{mercat2020uvg} and DAVIS~\cite{perazzi2016benchmark} dataset for the video inpainting task.
\textbf{UCF101} contains 13320 videos of 101 different action classes and has a large diversity of action types with the presence of large variations in video content.
We extract all videos at 2 fps with 256$\times$320 spatial resolution. We follow the first training/testing split. 
\textbf{UVG} consists of 7 videos and 3900 frames in total. To compare with other learning-based video compression methods~\cite{lu2019dvc,li2021deep}, we also crop the UVG videos to 1024x1920. 
We take 10 videos from \textbf{DAVIS} validation split and crop them to the spatial size of 384x768.

\vspace{0.05in}
\noindent\textbf{Evaluation Tasks.}
To understand the video representation capability of different INR-based methods, we compare with SOTA INR-based methods (NeRV~\cite{chen2021nerv}, E-NeRV~\cite{li2022nerv}) on the task of video reconstruction in Section~\ref{sec:sotainr}.
Next, as video compression is considered as the most promising downstream application of INR-based video representations, we also validate the effectiveness of our \system on the UVG and UCF101 datasets in Section~\ref{sec:compression}. 
Furthermore, since our \system can represent large-scale and diverse videos in a single model, we naturally extend the application of our \system to use it as an efficient dataloader, and demonstrate its effectiveness on the downstream video understanding tasks~\cite{lin2019tsm,feichtenhofer2019slowfast,yang2021beyond,li20212d,saini2022recognizing,he2022asm,wang2022efficient,wang2022bevt,he2023a2summ} (\eg, action recognition) in Section~\ref{sec:recognition}.
Finally, we show intriguing properties and advantages of \system on the video inpainting task in Section~\ref{sec:inpainting}.\looseness=-1

\vspace{0.05in}
\noindent\textbf{Implementation Details.} In our ablation experiments, we train \system using the AdamW~\cite{loshchilov2017decoupled} optimizer.
We use the cosine annealing learning rate schedule, the batch size of 32, the learning rate of 5e-4, training epochs of 800 and 400, and warmup epochs of 160 and 80 for UCF101 and UVG datasets, respectively.
The key-frames for each video are sampled at stride 8 on both datasets.
Following~\cite{li2021deep}, we compress key-frames by using the image compression technique~\cite{cheng2020learned}.
When comparing with other implicit neural representations such as NeRV and E-NeRV, we sum the compressed key-frame size and model size as the total size for \system, and keep the total size of \system equal to the model size of NeRV and E-NeRV for a fair comparison.
The total sizes of different model variants (S/M/L) on the UCF101 dataset are 79.2/94.5/114.5 MB respectively.
More dataset-specific training and testing details are available in the supplementary material.


\begin{table*}[t]
\centering
\begin{minipage}{.72\textwidth}
    \centering
    \caption{Video reconstruction comparison between our \system, NeRV~\cite{chen2021nerv} and E-NeRV~\cite{li2022nerv} on 7 videos from the UVG dataset. We keep the total size of \system including the key-frame size and model size to be the same as the model size of NeRV and E-NeRV for a fair comparison. We report the PSNR results for each video. NeRV and E-NeRV are trained in separate models for each video while NeRV$^*$ and \system fit multiple videos in a shared model.}
    \resizebox{0.988\linewidth}{!}{
    \renewcommand{\arraystretch}{1.25}
    \begin{tabular}{@{}l|ccccccc|c@{}} 
        \toprule
        Video & Beauty & Bosphorus & Bee & Jockey & SetGo & Shake & Yacht & avg.\\
        \midrule
        NeRV & 33.06 & 32.38 & 37.88 & 31.18 & 24.02 & 33.48 & 26.91 & 31.27 \\
        E-NeRV & 33.07 & 33.52 & 39.36 & 30.88 & 25.19 & 34.6 & 28.21 & 32.12 \\
        \midrule 
        NeRV$^*$ & 32.71 & 33.36 & 36.74 & 32.16 & 26.93 & 32.69 & 28.48 & 31.87 \\
        D-NeRV &  33.77 & 38.66 & 37.97 & 35.51 & 33.93 & 35.04 & 33.73 & \bf 35.52 \\
        \bottomrule
    \end{tabular}
    }
    \label{tab:enerv}
\end{minipage}
\hfill
\begin{minipage}{.24\textwidth}
    \caption{Video compression results on the UCF-101 dataset.}
    \resizebox{\linewidth}{!}{
    \renewcommand{\arraystretch}{1.1}
    \begin{tabular}{@{}l|cc@{}} 
        \toprule
        \textbf{Method} & PSNR & MS-SSIM \\
        \midrule
        H.264-S & 26.29 & 0.903 \\
        NeRV-S & 26.79 & 0.910 \\
        \system-S & \bf 28.15 & \bf 0.916 \\
        \midrule
        H.264-M & 27.42 & 0.925 \\
        NeRV-M & 27.35 & 0.921 \\
        \system-M & \bf 29.18 & \bf 0.937 \\
        \midrule
        H.264-L & 28.54 & 0.941 \\
        NeRV-L & 27.57 & 0.928 \\
        \system-L & \bf 30.06 & \bf 0.951 \\
        \bottomrule
    \end{tabular}
    }
    \label{tab:ucf_compression}
\end{minipage}
\end{table*}



\begin{figure*}[t]
\vspace{-0.05in}
    \centering
    \subfloat[PSNR vs. BPP]{
        \includegraphics[width=.44\linewidth]{fig/uvg_psnr_bpp.pdf}
        \label{fig:uvg_psnr}
    } 
    \hfill
    \subfloat[MS-SSIM vs. BPP ]{
        \includegraphics[width=.44\linewidth]{fig/uvg_ssim_bpp.pdf}
        \label{fig:uvg_ssim}
    }
    \vspace{-0.1in}
    \caption{ Rate distortion plots on the UVG dataset. }
    \label{fig:uvg_results}
\vspace{-0.1in}
\end{figure*}


\subsection{Comparison with SOTA INRs}
\label{sec:sotainr}
We compare our \system with NeRV~\cite{chen2021nerv} and E-NeRV\cite{li2022nerv} on the UVG dataset for the \textbf{video reconstruction} task (without any compression steps). 
E-NeRV is the state-of-the-art INR-based video representation model.
The results are shown in Table~\ref{tab:enerv}, our \system can consistently outperform NeRV and E-NeRV on different videos of the UVG dataset. 
We first note that E-NeRV surpasses NeRV by 0.9 dB PSNR with the same model size.
As we mentioned before, encoding all the videos jointly with a shared model achieve better compression results (NeRV$^{*}$), which brings about a 0.6 dB performance boost than fitting each video along (NeRV).
Despite that, our \system achieves the best performance among these methods. Specifically, it outperforms the previous state-of-the-art INR-based method E-NeRV by 3.4 dB for the averaged PSNR.



\subsection{Video Compression}
\label{sec:compression}
We further evaluate the effectiveness of \system on the video compression task.
For video compression, we follow the same practice as NeRV for model quantization and entropy encoding but without model pruning to expedite the training process.

\vspace{0.05in}
\noindent\textbf{UCF101 Dataset.}
To demonstrate the effectiveness of \system in representing large-scale and diverse videos, in Table~\ref{tab:ucf_compression}, we show the comparison results of \system with NeRV~\cite{chen2021nerv} and H.264~\cite{wiegand2003overview} on the UCF101 dataset.
First, we observe that \system vastly outperforms the baseline model NeRV.
Especially when changing the model size from small (S) to large (L), the gap between \system and NeRV becomes larger, increasing from 1.4 dB to 2.5 dB.
It demonstrates that \system is more capable of compressing large-scale videos with high quality than NeRV.
Also, we can see that \system consistently surpasses traditional video compression techniques H.264, showing its great potential in real-world large-scale video compression.

\vspace{0.05in}
\noindent\textbf{UVG Dataset.}
Although \system is specifically designed for representing large-scale and diverse videos, which is not the case for the UVG dataset (7 videos), it can still consistently outperform NeRV greatly as shown in Figure~\ref{fig:uvg_results}.
Specifically, it surpasses NeRV by more than 1.5 dB under the same BPP ratios.
Despite that INR-based and learning-based methods are indeed two different frameworks, we still follow the NeRV paper to compare with learning-based video compression methods for completeness, such as DVC~\cite{lu2019dvc} and DCVC~\cite{li2021deep}.
\system outperforms all of them in both PSNR and MS-SSIM metrics. 
It greatly reveals the effectiveness of our \system for the video compression task.




\begin{table*}[t]
\centering
\begin{minipage}{.42\textwidth}
    \caption{Contribution of each component. SAF, GTMLP, Flow denote spatially-adaptive fusion, global temporal MLP, and multi-scale flow estimation, respectively.}
    \centering
    \resizebox{0.95\linewidth}{!}{
    \renewcommand{\arraystretch}{1.1}
    \begin{tabular}{@{}l|cccc@{}} 
        \toprule
        \multirow{2}{*}{\textbf{Model}} & \multicolumn{2}{c}{\textbf{UVG}} & \multicolumn{2}{c}{\textbf{UCF101}}\\
        \cmidrule(l{-1pt}r{-1pt}){2-5}
         & PSNR & MS-SSIM & PSNR & MS-SSIM \\
        \toprule
        NeRV & 34.13 & 0.948 & 28.00 & 0.935 \\ 
        + GTMLP & 33.94 & 0.946 & 27.96 & 0.935 \\ 
        \midrule
        + SAF & 35.84 & 0.960 & 30.78 & 0.962 \\
        + GTMLP & 36.32 & 0.963 & 30.94 & 0.964 \\
        + Flow & \bf 36.99 & \bf 0.977 & \bf 31.44 & \bf 0.968 \\
        \bottomrule
    \end{tabular}
    }
    \label{tab:components}
\end{minipage}
\hfill
\begin{minipage}{.29\textwidth}
    \caption{Temporal modeling ablation. ``DWConv" indicates the depth-wise temporal conv.}
    \centering
    \resizebox{0.95\linewidth}{!}{
    \renewcommand{\arraystretch}{1.1}
        \begin{tabular}{@{}l|cc@{}} 
            \toprule
            Model & PSNR & MS-SSIM \\
            \toprule
            Baseline & 31.10 & 0.964 \\
            \midrule
            DWConv-k3 & 31.13 & 0.965 \\ 
            DWConv-k7 & 31.15 & 0.966 \\
            DWConv-k11 & 31.16 & 0.966 \\
            Attention & 31.34 & 0.967 \\
            \midrule
            GTMLP & \bf 31.44 & \bf 0.968 \\ 
            \bottomrule
        \end{tabular}
    }
    \label{tab:temporal}
\end{minipage}
\hfill
\begin{minipage}{.24\textwidth}
    \caption{Fusion methods ablation.}
    \centering
    \resizebox{0.9\linewidth}{!}{
    \renewcommand{\arraystretch}{1}
        \begin{tabular}{@{}c|cc@{}} 
            \toprule
            & PSNR & MS-SSIM \\
            \midrule
            U-Net & 30.09 & 0.954 \\ 
            SAF & \bf 31.44 & \bf 0.968 \\
            \bottomrule
        \end{tabular}%
    }
    \label{tab:fusion}
    \vspace{0.08in}
    \caption{Impact of multi-scale.}
    \centering
    \resizebox{0.85\linewidth}{!}{
    \renewcommand{\arraystretch}{1}
        \begin{tabular}{@{}c|cc@{}} 
            \toprule
            MS & PSNR & MS-SSIM \\
            \midrule
             & 31.06 & 0.965 \\ 
            \checkmark & \bf 31.44 & \bf 0.968 \\
            \bottomrule
        \end{tabular}%
    }
    \label{tab:multiscale}
\end{minipage}
\end{table*}

\begin{table*}[t]
\centering
\begin{minipage}{.32\textwidth}
    \caption{Video diversity ablation. We fix the total video count at 1000 while changing the number of action classes.}
    \resizebox{\linewidth}{!}{
    \renewcommand{\arraystretch}{1.15}
    \begin{tabular}{@{}lc|cc@{}} 
        \toprule
        & \textbf{\#Class} & PSNR & MS-SSIM \\
        \midrule
        \multirow{3}{*}{\textbf{NeRV}} 
        & 10 & 27.95 & 0.935 \\
        & 100 & 26.66 & 0.915 \\
        & $\bigtriangledown$ & -1.29 & -0.02 \\
        \midrule
        \multirow{3}{*}{\textbf{\system}} 
        & 10 & 29.74 & 0.950 \\
        & 100 & 29.36 & 0.946 \\ 
        & $\bigtriangledown$ & \bf -0.38 & \bf -0.004 \\
        \bottomrule
    \end{tabular}
    }
    \label{tab:video_diversity}
\end{minipage}
\hfill
\begin{minipage}{.41\textwidth}
    \caption{Top-1 action recognition accuracy on UCF101. Models are trained on compressed videos and tested on uncompressed videos (``Train'' setting) and vice versa (``Test'' setting). S/M/L denote different compression ratios.}
    
    \resizebox{\linewidth}{!}{
    \renewcommand{\arraystretch}{1.1}
    \begin{tabular}{@{}l|ccc|ccc@{}} 
        \toprule
        \multirow{2}{*}{\textbf{Model}} & \multicolumn{3}{c}{\textbf{Train}} & \multicolumn{3}{c}{\textbf{Test}}\\
        \cmidrule(l{-1pt}r{-1pt}){2-7}
         & S & M & L & S & M & L\\
        \toprule
        GT & 91.3 & 91.3 & 91.3 & 91.3 & 91.3 & 91.3 \\ 
        \midrule
        H.264 & 86.7 & 87.9 & 88.9 & 77.2 & 82.4 & 85.5 \\ 
        NeRV & 84.5 & 85.8 & 86.9 & 71.9 & 75.9 & 80.0 \\
        \system & \bf 87.9 & \bf 89.0 & \bf 90.0 & \bf 81.1 & \bf 84.4 & \bf 86.4 \\
        \bottomrule
    \end{tabular}
    }
    \label{tab:ucf_recognition}
\end{minipage}
\hfill
\begin{minipage}{.22\textwidth}
    \caption{Model runtime comparison (video per second).}
    \resizebox{\linewidth}{!}{
    \renewcommand{\arraystretch}{1.1}
    \begin{tabular}{@{}l|c@{}} 
        \toprule
        Method & VPS $\uparrow$ \\
        \midrule
        Frame ({\footnotesize Tab.~\ref{tab:ucf_recognition} GT}) & 273\\ 
        H.264 & 265 \\
        DCVC & 0.9 \\
        NeRV (fp32) & 383 \\
        \system(fp32) & 266  \\
        NeRV (fp16) & 454 \\
        \system(fp16) & 363 \\
        \bottomrule
    \end{tabular}
    }
    \label{tab:runtime}
\end{minipage}
\end{table*}

\subsection{Ablation}

\vspace{0.05in}
\noindent\textbf{Contribution of each component.}
In Table~\ref{tab:components}, we conduct an ablation study to investigate the contribution of each component of \system.
First, we observe that adding the encoder with the spatially-adaptive fusion (SAF) can largely enhance the performance of the baseline model NeRV by 1.7 and 2.8 dB on UVG and UCF101 datasets respectively. 
With the clip-specific visual content fed into the network, it greatly reduces the complexity of memorization for diverse videos.
Second, adding the global temporal MLP module (GTMLP) can further improve performance. It is interesting to note that simply adding the global temporal MLP module on NeRV can not facilitate the final result.
This is because when representing multiple videos, NeRV concatenates all the videos along the time axis. The input of NeRV is the absolute time index normalized by the length of the concatenated video, which can not reflect motion between relative frames.
On the contrary, the input for \system is the relative time index normalized by each video's length, it can represent the motion across frames which are shared across different videos.
Therefore, adding the global temporal MLP module with the relative time index can help model the motion information between frames.
Please note that simply using the relative time index alone is not feasible, which needs to be conditioned on the sampled key-frames to represent different videos. 
Finally, to further reduce the inherent spatial redundancies across video frames, we add the task-oriented flow as an intermediate output, which can boost the final results to another level by 0.67 dB and 0.5 dB on UVG and UCF101 datasets respectively.

\vspace{0.05in}
\noindent\textbf{Component design choices ablation.}
Table~\ref{tab:temporal} demonstrates the results of different temporal modeling designs.
Compared to the baseline, incorporating the local temporal relationship by adding depth-wise temporal convolution can slightly improve the performances and the gap becomes larger while increasing the kernel size from 3 to 11, which validates the importance of temporal modeling for effective video representation.
Inspired by the success of Transformer~\cite{vaswani2017attention}, we also try to add a temporal attention module. Different from convolution operation with the local receptive field, the temporal attention module can model global temporal dependencies, which achieves higher results than depth-wise convolutions. 
However, due to the heavy computation cost of the attention operation, the training speed of the attention module is much slower than other variants.
Finally, motivated by the success of MLP-based models~\cite{tolstikhin2021mlp,touvron2021resmlp,lian2021mlp,he2020gta}, our global temporal MLP module combines the efficiency from fully-connected layer and the global temporal modeling ability from the attention module. It attains the highest results with a much faster training speed than the attention module.
We also compare different fusion strategies to fuse the content information from encoder to decoder in Table~\ref{tab:fusion}.
While U-Net~\cite{ronneberger2015u} concatenates the output feature map of each encoder stage to the input of the decoder, the proposed SAF module utilizes the content feature map as a modulation for decoder features, which proves to be a more effective design than simple concatenation.
In addition, Table~\ref{tab:multiscale} shows that the multi-scale design can enhance the final performances.

\vspace{0.05in}
\noindent\textbf{Impact of video diversity.}
To analyze the impact of video diversity, we conduct experiments with the following settings:
(i) 1000 videos selected from 10 classes where each class has 100 videos;
(ii) 1000 videos selected from 100 classes where each class has 10 videos.
The results are shown in Table~\ref{tab:video_diversity}.
When increasing the video diversity from 10 classes to 100 classes, although the performances of \system and NeRV both decrease, the results of \system drop much slower than NeRV.
It verifies that \system is more effective especially when representing diverse videos.

\subsection{Action Recognition}
\label{sec:recognition}
As \system can effectively represent large and diverse videos, a natural extension of its application could be treating it as an efficient video dataloader, considering it can greatly reduce the video loading time due to the INR-based model design.
In this section, to validate the above assumption, we perform experiments on the action recognition task.


\vspace{0.05in}
\noindent\textbf{Action recognition accuracy.}
In our experiment, we adopt the widely used TSM~\cite{lin2019tsm} as the backbone to evaluate the action recognition accuracy of compressed videos from H.264, NeRV, and \system.
Specifically, we follow two settings below:
i) ``Train'': models are trained on compressed videos and tested on uncompressed ground-truth videos.
ii) ``Test'': models are trained on uncompressed ground-truth videos and tested on compressed videos.
S/M/L denotes different BPP values as Table~\ref{tab:ucf_compression}. The lower BPP value means a higher compression ratio.
The results are shown in Table~\ref{tab:ucf_recognition}.
We can see that, the action recognition accuracy of \system consistently outperforms NeRV by 3-4\% and 6-10\% for the ``train'' and ``test'' settings, respectively.
In addition, \system consistently outperforms H.264, which proves the superior advantage of \system when used as an efficient dataloader in the real-world scenario.

\begin{table*}[t]
\centering
    \caption{Video inpainting comparison between our \system and NeRV. NeRV$^*$ and \system fit all the videos in a shared model. PSNR results of the mask areas are reported here.}
    \resizebox{0.9\linewidth}{!}{
    \renewcommand{\arraystretch}{1.2}
    \begin{tabular}{@{}c|cccccccccc|c@{}} 
        \toprule
        Video & bike & b-swan & bmx & b-dance & camel & c-round & c-shadow & cows & dance-twirl & dog & avg.\\
        \midrule
        NeRV & 19.00 & 21.10 & 18.26 & 18.50 & 18.59 & 16.78 & 19.66 & 18.25 & 17.97 & 21.79 & 18.99\\
        NeRV$^*$ & 20.45 & 21.86 & 19.96 & 19.69 & 20.15 & 18.23 & 20.83 & 18.75 & 18.92 & 22.20 & 19.88 \\
        D-NeRV & 23.53 & 22.27 & 19.50 & 21.85 & 22.89 & 18.9 & 21.06 & 22.27 & 19.08 & 22.01 & \bf 21.3\\
        \bottomrule
    \end{tabular}
    }
    \label{tab:inpaint}
\vspace{-0.05in}
\end{table*}

\vspace{0.05in}
\noindent\textbf{Model runtime.}
In Table~\ref{tab:runtime}, we compare the model runtime of the following settings:
(i) \textbf{Frame}: reading from pre-extracted uncompressed ground-truth frames directly;
(ii) \textbf{H.264}: reading and decoding from the H.264 compressed videos;
(iii) \textbf{DCVC}: recent learning-based video compression method;
(iv) \textbf{NeRV} and (iv) \textbf{\system}.
The experiments are conducted on a single node with 8 RTX 2080ti GPU and 32-core CPU.
Although reading from uncompressed ground-truth frames can preserve the highest quality of video and achieve higher accuracy for downstream tasks, it has a much higher storage cost because it reads from uncompressed frames.
On the contrary, directly reading from compressed videos (\eg, H.264) can save the storage cost while achieving a similar speed because of the highly optimized video decoding techniques.
The model runtime speed of NeRV and \system shows a great advantage over the learning-based compression method DCVC.
Due to its auto-regressive decoding design, DCVC has achieved a much slower model runtime speed than \system and NeRV.
Note that although NeRV has the highest model runtime speed due to the simplicity of its architecture, its compression quality is much inferior to the \system as shown in Table~\ref{tab:ucf_compression} and Table~\ref{tab:ucf_recognition}.


\subsection{Video Inpainting}
\label{sec:inpainting}
We further explore the potential ability of \system on the video inpainting task with NeRV. We apply 5 random box masks with a width of 50 for each frame. The results are shown in Table~\ref{tab:inpaint}. 
Although we do not have any specific design for the video inpainting task, our \system can still outperform NeRV by 1.4 dB for the PSNR results.
Also, it is interesting to see that encoding all videos in a shared model can also improve the inpainting performance (NeRV v.s. NeRV$^{*}$), which further validates our previous claim of encoding all videos in a shared model is more beneficial.


\subsection{Qualitative Results}
In Figure~\ref{fig:qualitative_compression}, we compare the visualization results of the decoded frames for the compression task. 
At the same BPP budget, \system produces clearer images with higher quality in both the main objects and the background compared to the classic video compression method (H.264) and baselines (NeRV), such as the court, blackboard, and stadium. 
Figure~\ref{fig:qualitative_inpainting} shows the visualization results for the video inpainting task. Compared to NeRV, our \system can inpaint the mask area more naturally with better quality.
More qualitative results are shown in the supplementary material.

\begin{figure}[t]
    \centering
    \adjincludegraphics[width=\linewidth, trim={{0.05\width} {0.13\height} {0.05\width} {0.1\height}},clip]{fig/visualization_compression.pdf}
    \caption{Visualization of ground-truth, H.264, NeRV and \system on the UCF101 dataset. Please zoom in to view the details.}
    \label{fig:qualitative_compression}
\end{figure}

\begin{figure}[t]
    \centering
    \adjincludegraphics[width=\linewidth, trim={{0.05\width} {0.18\height} {0.05\width} {0.18\height}},clip]{fig/visualization_inpainting.pdf}
    \caption{Video inpainting visualization on the DAVIS dataset.}
    \label{fig:qualitative_inpainting}
    \vspace{-0.05in}
\end{figure}
