\vspace{-0.1in}
\section{Introduction}

\begin{figure}[t]
\centering
    \vspace{-0.1in}
    \adjincludegraphics[width=\linewidth, trim={{0.0\width} {0.0\height} {0.0\width} {0.0\height}},clip]{fig/teaser.pdf}
    \caption{Comparison of \system and NeRV when representing diverse videos. NeRV optimizes representation to every video independently while \system encodes all videos by a shared model.}
    \label{fig:teaser}
\vspace{-0.2in}
\end{figure}



Implicit neural representations (INR) have achieved great success in parameterizing various signals, such as 3D scenes~\cite{mildenhall2020nerf,yu2021pixelnerf,sitzmann2019scene}, images~\cite{sitzmann2020implicit,chen2021learning}, audio~\cite{sitzmann2020implicit}, and videos~\cite{chen2021nerv,li2022nerv,chen2022cnerv,maiya2023nirvana,chen2023hnerv}.
The key idea is to represent signals as a function approximated by a neural network, mapping a reference coordinate to its corresponding signal value.
Recently, INR has received increasing attention in image and video compression tasks~\cite{dupont2021coin,chen2021nerv,li2022nerv,rho2022neural,zhang2021implicit,dupont2022coin++,strumpler2021implicit,chen2022cnerv}.
Compared with learning-based video compression techniques~\cite{lu2019dvc,yang2020Learning,li2021deep}, INR-based methods (\eg, NeRV~\cite{chen2021nerv}) are more favorable due to simpler training pipelines and much faster video decoding speed.


While impressive progress has been made, existing INR-based methods are limited to encoding a single short video at a time.
This prohibits the potential applications in most real-world scenarios, where we need to represent and compress a large number of diverse videos.
A straightforward strategy for encoding diverse videos is to divide them into multiple subsets and model each of them by a separate neural network, as shown in Figure~\ref{fig:teaser} (top).
However, since this strategy is unable to leverage long-term redundancies across videos, it achieves inferior results compared to fitting all diverse videos with a single shared model.
As shown in Figure~\ref{fig:teaser_fixbpp}, under the same compression ratio (bits per pixel), the performance of NeRV is consistently better when fitting a larger number of videos.
This suggests that representing multiple videos by a single large model is generally more beneficial.



However, as observed empirically, the current design of NeRV offers diminishing returns when scaling to large and diverse videos. 
We argue that the current coupled design of content and motion information modeling exaggerates the difficulty of memorizing diverse videos.
To address this, we propose \system, a novel implicit neural representation that is specifically designed to efficiently encode long or a large number of diverse videos\footnote{``Long videos" and ``a large number of videos" are viewed as interchangeable concepts in this paper because a long video can be obtained by concatenating a collection of diverse videos.}.
A representative overview of differences between \system and NeRV is shown in Figure~\ref{fig:teaser}.
When representing diverse videos, NeRV encodes each video into a separate model or simply concatenates multiple videos into a long video and encodes it, while our \system can represent different videos in a single model by conditioning on key-frames from each video clip.

Compared to NeRV, we have the following improvements. First, we observe that the visual content of  each video often represents appearance, both background and foreground, which vary significantly among different videos, while the motion information often represents the semantic structure (\eg, similar motion for the same action class) and can be shared across different videos.
Therefore, we decouple each video clip into two parts: clip-specific visual content and motion information, which are modeled separately in our method. 
Second, motivated by the vital importance of temporal modeling in video-related tasks, instead of outputting each frame independently, we introduce temporal reasoning into the INR-based network by explicitly modeling global temporal dependencies across different frames.
Finally, considering the significant spatial redundancies in videos, rather than predicting the raw pixel values directly, we propose to predict the task-oriented flow~\cite{xue2019video,reda2022film,huang2019dynamic,huang2022learning} as an intermediate output, and use it in conjunction with the key-frames to get the final refined output. It alleviates the complexity of memorizing the same pixel value across different frames.

With these improvements, our \system significantly outperforms NeRV, especially when increasing the number of videos as shown in Figure~\ref{fig:teaser_fixbpp}. 
To summarize, our main contributions are as follows:
\begin{itemize}
    \vspace{-0.5em}
    \item We propose \system, a novel implicit neural representation model, to represent a large and diverse set of videos as a single neural network.
    \vspace{-0.5em}
    \item We conduct extensive experiments on video reconstruction and video compression tasks. Our \system consistently outperforms state-of-the-art INR-based methods (E-NeRV~\cite{li2022nerv}), traditional video compression approaches (H.264~\cite{wiegand2003overview},HEVC~\cite{sullivan2012overview}), and the recent learning-based video compression methods (DCVC~\cite{li2021deep}).
    \vspace{-0.5em}
    \item We further show the advantage of \system on the action recognition task by its higher accuracy and faster decoding speed, and reveal its intriguing properties on the video inpainting task.
\end{itemize}

\begin{figure}[t]
\centering
    \vspace{-0.15in}
    \adjincludegraphics[width=\linewidth, trim={{0.0\width} {0.02\height} {0.0\width} {0.0\height}},clip]{fig/teaser_count1.pdf}
    \caption{Comparison of \system and NeRV with fixed compression ratio on UCF101. The size of circles indicates model sizes.}
    \label{fig:teaser_fixbpp}
\vspace{-0.15in}
\end{figure}











