\section{Related Work}

\noindent\textbf{Implicit neural representations.}
Neural networks can be used to approximate the functions which maps the input coordinates to various types of signals.
It has brought great interest and has been widely adopted to represent 3D shape ~\cite{sitzmann2019scene,mescheder2019occupancy}, novel view synthesis~\cite{mildenhall2020nerf,yu2021pixelnerf}.
These approaches train a neural network to fit a single scene or object such that it is encoded by the network weights.
Implicit neural representations have also been applied to represent images ~\cite{sitzmann2020implicit,chen2021learning,dupont2021coin}, videos~\cite{chen2021nerv,rho2022neural} and audios~\cite{sitzmann2020implicit}.
Among them, NeRV proposes the first image-wise implicit neural representation for videos, which takes the frame index and outputs the corresponding RGB frame. 
Compared to the pixel-wise implicit neural representation SIREN~\cite{sitzmann2020implicit}, NeRV shows superior efficiency, which improves the encoding and decoding speed greatly and achieves better video reconstruction quality.
Based on NeRV, E-NeRV~\cite{li2022nerv} boosts the video reconstruction performance via decomposing the image-wise implicit neural representation into separate spatial and temporal contexts.
NRFF~\cite{rho2022neural} and IPF~\cite{zhang2021implicit} predict the motion compensation and residual between consecutive video frames to better reduce the spatial redundancies.
CNeRV~\cite{chen2022cnerv} proposes a hybrid video neural representation with content-adaptive embedding to further introduce internal generalization.

\begin{figure*}[t!]
    \centering
    \vspace{-0.15in}
    \includegraphics[width=\textwidth]{fig/framework.pdf}
    \vspace{-0.25in}
    \caption{(a) Frame Overview. \system takes in key-frame pairs of each video clip along with all the frame indices and outputs a whole video clip at a time. (b) The decoder block predicts the flow estimation to warp the visual content feature from the encoder, then fuses the visual content by the spatially-adaptive fusion module and finally models temporal relationship by the global temporal MLP module.}
    \label{fig:model}
    \vspace{-0.1in}
\end{figure*}

\vspace{0.05in}
\noindent\textbf{Video Compression.}
Video compression techniques can be divided into traditional video compression algorithms, such as MPEG~\cite{le1991mpeg}, H.264~\cite{wiegand2003overview}, HEVC~\cite{sullivan2012overview} and deep learning-based compression approaches, such as DVC~\cite{lu2019dvc}, LVC~\cite{rippel2019learned}, HLVC~\cite{yang2020Learning}, DCVC~\cite{li2021deep}.
The learning-based compression approaches often use convolutional neural networks (CNN) to replace certain components while still following the traditional video compression pipeline.
Recently, INR-based models have been adopted to compress image and video data. 
They encode images and videos into neural networks and apply general model compression techniques, which convert the image and video compression task to a standard model compression task.
COIN~\cite{dupont2021coin} uses a vanilla pixel-wise INR model to fit images and adopts weight quantization to do model compression. 
NeRV~\cite{chen2021nerv} applies model pruning, weight quantization, and entropy encoding to further reduce the model size.

Among these INR-based methods, NeRV~\cite{chen2021nerv} is the first image-wise INR-based model specifically designed for videos. 
As is shown in Figure~\ref{fig:teaser}, NeRV takes in the time index $t$ as input and outputs the corresponding frame directly, which can represent a video as a neural network.
Specifically, NeRV consists of a positional encoding function, stacked NeRV blocks, and a prediction head. More details can be found in NeRV paper~\cite{chen2021nerv}.
Although with such a simple architecture, NeRV shows promising results in the video compression task.
However, for large-scale and diverse videos, NeRV encodes each video into a separate model or simply concatenates them into one longer video.
This design is not optimal when modeling such massive information into a neural network, which motivates us to design a more effective framework for diverse video encoding.
