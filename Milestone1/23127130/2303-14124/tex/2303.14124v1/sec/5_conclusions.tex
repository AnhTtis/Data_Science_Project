\section{Conclusion}
In this paper, we present a novel implicit neural network based framework to represent large-scale and diverse videos.
We decouple videos into clip-specific visual content and motion information, and then model them separately, which proves to be more effective than modeling them jointly as previous work NeRV does.
Because it alleviates the difficulty of memorizing diverse videos. 
We also introduce temporal reasoning into the implicit neural network to exploit the temporal relationships across frames.
We further validate our design on multiple datasets and different tasks (\eg, video reconstruction, video compression, action recognition, and video inpainting).
Our method provides new insight into representing videos in a scalable manner, which makes it one step closer to real-world applications.



