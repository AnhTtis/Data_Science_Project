\appendix
\section*{Appendix}


Sec.~\ref{sec:addtional_results} reports additional results on AVA~\cite{gu2018ava} and KTH Action Recognition~\cite{schuldt2004recognizing} datasets for the video compression task.
Sec.~\ref{sec:enerv} presents more comparison with the state-of-the-art INR-based video representation method E-NeRV~\cite{li2022nerv}.
Sec.~\ref{sec:experiment_details} shows more dataset-specific implementation details.
We also show more qualitative results on the UVG, UCF101, Davis datasets in Sec.~\ref{sec:qualitative_more}.
Sec.~\ref{sec:discussion_more} provides more comparisons and discussions with learning-based video compression methods.
Finally, we discuss the limitation and some future work of our paper in Sec.~\ref{sec:limitation}. 


\section{Additional Video Compression Results}
\label{sec:addtional_results}
To further demonstrate the effectiveness of \system, we conduct additional experiments on the AVA Actions~\cite{gu2018ava} dataset and KTH Action Recognition~\cite{schuldt2004recognizing} dataset for the video compression task.

\noindent\textbf{AVA Actions Dataset}
For the AVA Actions dataset, each original video is a full movie lasting about 1-2 hours, which is much longer than short action videos (around 10 seconds) from the UCF101 and UVG datasets. 
We sample 10 videos with a spatial size of 256$\times$384 and a frame rate of 1 fps.
The PSNR and MS-SSIM results under different compression ratios (indicated with S / M / L) are shown in Table~\ref{tab:ava}.
We can see that \system consistently outperforms NeRV~\cite{chen2021nerv} and H.264~\cite{wiegand2003overview} when encoding especially long videos.

\noindent\textbf{KTH Action Recognition Dataset}
The KTH Action Recognition~\cite{schuldt2004recognizing} dataset consists of grayscale video sequences of 25 people performing six different actions: walking, jogging, running, boxing, hand waving, and hand clapping. The background is uniform and a single person performs actions in the foreground. The videos have 120$\times$160 spatial size and 25 fps frame rates.
Similar to the results on other datasets, our \system achieves the best performances when comparing to H.264 and NeRV in Table~\ref{tab:kth}.



\section{Additional Comparison with E-NeRV}
\label{sec:enerv}
We conduct an additional comparison with E-NeRV on the UVG dataset by following the same experimental setting as Table \textcolor{red}{1} from E-NeRV~\cite{li2022nerv}.
The original E-NeRV paper uniformly samples 150 frames from each video and resizes the input video from 1080$\times$1920 to 720$\times$1280, and fits each video with a much larger model size (12.5M).
The results of NeRV and E-NeRV in Table~\ref{tab:additional_enerv} are the reported performance in Table \textcolor{red}{1} from the original E-NeRV paper.
As we can see from Table~\ref{tab:additional_enerv}, when using a much larger model size to fit each downsampled video, the PSNR scores of both NeRV and E-NeRV are higher and the performance gap between E-NeRV and NeRV becomes greater, comparing to the results of Table~\textcolor{red}{1} in our main paper.
However, our \system still outperforms E-NeRV by 0.82 dB and achieves the best result.
It proves the superior advantages of \system over the state-of-the-art INR-based video representation method E-NeRV.




\begin{table}[t]
\centering
    \caption{Video compression results on the AVA dataset.}
    \vspace{-0.1in}
    \resizebox{\linewidth}{!}{
    \renewcommand{\arraystretch}{1.1}
    \begin{tabular}{@{}l|ccc|ccc@{}} 
        \toprule
        \multirow{2}{*}{\textbf{Model}} & \multicolumn{3}{c}{\textbf{PSNR}} & \multicolumn{3}{c}{\textbf{MS-SSIM}}\\
        \cmidrule(l{-1pt}r{-1pt}){2-7}
         & S & M & L & S & M & L \\
        \toprule
        H.264 & 27.32 & 28.91 & 30.49 & 0.853 & 0.897 & 0.923 \\ 
        NeRV & 26.48 & 27.28 & 28.21 & 0.840 & 0.868 & 0.893 \\
        \system & \bf 28.77 & \bf 29.57 & \bf 30.60 & \bf 0.886 & \bf 0.903 & \bf 0.924 \\
        \bottomrule
    \end{tabular}
    }
    \label{tab:ava}
\end{table}

\begin{table}[t!]
\centering
    \caption{Video compression results on the KTH dataset.}
    \vspace{-0.1in}
    \resizebox{\linewidth}{!}{
    \renewcommand{\arraystretch}{1.1}
    \begin{tabular}{@{}l|ccc|ccc@{}} 
        \toprule
        \multirow{2}{*}{\textbf{Model}} & \multicolumn{3}{c}{\textbf{PSNR}} & \multicolumn{3}{c}{\textbf{MS-SSIM}}\\
        \cmidrule(l{-1pt}r{-1pt}){2-7}
         & S & M & L & S & M & L \\
        \toprule
        H.264 & 29.61 & 32.72 & 34.51 & 0.691 & 0.801 & 0.860 \\ 
        NeRV & 30.56 & 32.14 & 33.31 & 0.701 & 0.748 & 0.784 \\
        \system & \bf 31.90 & \bf 34.46 & \bf 36.15 & \bf 0.745 & \bf 0.849 & \bf 0.892 \\
        \bottomrule
    \end{tabular}
    }
    \label{tab:kth}
\end{table}

\begin{table*}[t]
    \centering
    \vspace{-0.05in}
    \caption{Video reconstruction comparison between our \system, NeRV~\cite{chen2021nerv} and E-NeRV~\cite{li2022nerv} on 7 videos from the UVG dataset. We follow the same setting as E-NeRV, which uniformly samples 150 frames from each video, resizes the input video from 1080$\times$1920 to 720$\times$1280 and trains models for 300 epochs.}
    \vspace{-0.1in}
    \resizebox{0.75\linewidth}{!}{
    \renewcommand{\arraystretch}{1.25}
    \begin{tabular}{@{}l|ccccccc|c@{}} 
        \toprule
        Video & Beauty & Bosphorus & Bee & Jockey & SetGo & Shake & Yacht & avg.\\
        \midrule
        NeRV & 36.06 & 37.35 & 41.23 & 38.14  & 31.86 & 37.22 & 32.45 & 36.33 \\
        E-NeRV & 36.72 & 40.06 & 41.74 & 39.35 & 34.68 & 39.32 & 35.58 & 38.21 \\
        D-NeRV & 37.53 & 40.74 & 39.89 & 39.94 & 37.51 & 38.85 & 38.63 & \bf 39.03 \\
        \bottomrule
    \end{tabular}
    }
    \label{tab:additional_enerv}
\end{table*}

\begin{figure*}[t!]
    \centering
    \adjincludegraphics[width=\linewidth, trim={{0.13\width} {0.03\height} {0.13\width} {0.02\height}},clip]{fig/visualization_compression_uvg_supp.pdf}
    \vspace{-0.2in}
    \caption{Visualization of ground-truth, HEVC, and \system for the video compression task on the UVG dataset. 
    Red rectangles highlight the regions that HEVC fails to synthesize correctly and faithfully while our \system succeeds. Please zoom in to see the details.}
    \vspace{-0.2in}
    \label{fig:more_uvg}
\end{figure*}


\begin{figure*}[t]
    \centering
    \vspace{-0.1in}
    \adjincludegraphics[width=\linewidth, trim={{0.03\width} {0.1\height} {0.03\width} {0.1\height}},clip]{fig/visualization_compression_ucf101_supp.pdf}
    \vspace{-0.3in}
    \caption{Visualization of ground-truth, H.264, NeRV and \system for the video compression task on the UCF101 dataset.}
    \label{fig:more_ucf101}
    \vspace{-0.2in}
\end{figure*}


\begin{figure*}[t!]
    \centering
    \vspace{-0.1in}
    \adjincludegraphics[width=\linewidth, trim={{0.03\width} {0.15\height} {0.03\width} {0.1\height}},clip]{fig/visualization_inpainting_supp.pdf}
    \vspace{-0.4in}
    \caption{Visualization of ground-truth, NeRV and \system for the video inpainting task on the Davis dataset. Please zoom in to see the details.}
    \label{fig:more_davis}
    \vspace{-0.2in}
\end{figure*}

\section{Experiment Details}
\label{sec:experiment_details}
On the UVG dataset, to compare with state-of-the-art video compression methods, we run experiments with 1600 epochs and a batch size of 32 and a learning rate of 5e-4. 
Due to the GPU memory limitation, we split the 1024$\times$1920 input video frames into 256$\times$320 image patches. 
We regard the patches at the same spatial location across different timesteps as a single video. 
On the AVA and KTH datasets, we run experiments with 800 epochs, a batch size of 32, and a learning rate of 5e-4.
In our experiments, we set upscale factors 4, 2, 2, 2, 2 for each block.
For the video compression task, following NeRV~\cite{chen2021nerv}, we perform the model quantization and weight encoding steps but without the extra model pruning step to expedite the training process.
And the quantization bit is set to 8 for all the datasets.

For the keyframe image compression, we use pre-trained ~\cite{cheng2020learned} models to compress and decode the keyframes. Different pre-trained image compression models can compress keyframes with different compressed ratios.


\section{More Qualitative Results}
\label{sec:qualitative_more}
\system can produce clearer frames with less noise. Figure~\ref{fig:more_uvg} displays a few samples from the UVG dataset. 
The red rectangles show the regions where our \system outperforms HEVC~\cite{sullivan2012overview}, for example, the flower, the flag, and the leg of the horse.

\system also achieves better qualitative results on UCF101 dataset as shown in Figure~\ref{fig:more_ucf101}. For example, the athlete is more clear than NeRV and H.264 in row 1 and row 3. 
In row 2, \system also distinguishes from other methods when showing the foreground texts.

We also show more qualitative results on the Davis dataset for the video inpainting task in Figure~\ref{fig:more_davis}. \system can inpaint the mask area more faithfully and naturally without blurry effects.

\section{More Discussion}
\label{sec:discussion_more}
In this section, we compare and discuss our \system with existing learning-based video compression methods in more detail. 

The key significant difference between \system and existing learning-based video compression methods is the way compressed videos are represented -- neural network \textit{vs}.\ latent codes, respectively.
Since INR-based \system represents videos as a neural network, it can \textbf{\textit{implicitly}} estimate flow and interpolate keyframes.
In contrast, other learning-based methods that 
 \textbf{\textit{explicitly}} represent videos as latent codes generated by compressing flows and residuals for each frame.
Due to the implicit design, \system is a more general architecture that can be applied to video compression and other video tasks such as video inpainting.
On the other hand, these learning-based methods, including interpolating images (\eg, VCII~\cite{wu2018video}) and predicting flow estimation (\eg, LVC~\cite{rippel2019learned}, SSF~\cite{agustsson2020scale}, FVC~\cite{hu2021fvc}), all decode video frames sequentially because of reliance on previous frames, which leads to a much worse decoding speed.
In contrast, based on INR design, \system can reconstruct video frames parallelly with a faster speed. 

\section{Limitations and Future Work}
\label{sec:limitation}
INR-based methods often require longer training iterations than learning-based compression methods to better capture the high-frequency details. 
And they can not be generalized to unseen videos, which means they can only be trained and tested on the same videos.
We believe more exploration of the generalization ability can be a good research direction for the INR-based video representation models.
In addition, our current \system design still encodes the sampled keyframes by image compression techniques, however, encoding the sampled keyframes by a separate implicit neural network can make the whole pipeline more unified and may achieve better performances.
