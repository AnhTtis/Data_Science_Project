\section{Methodology}
\label{sec: method}

This section introduces our proposed AFP strategy \textit{\textbf{PHONEix}} and its integration with the SVS framework. 

\subsection{Overall Framework}
\label{ssec: overall framework}

As illustrated in Fig.~\ref{fig:model}, the whole system accepts inputs of the music score at note level and learns phoneme duration information guided by annotations. There are five steps in the application of the acoustic model: 1) A \textit{Prior Encoder} is attached before the major encoder-decoder-based structure. It consumes music scores and generates hidden representations of the music score at the note level. 2) A proposed \textit{Phoneme Distribution Predictor} is integrated to predict the proportion of phonemes in each syllable-note pair according to the \textit{\textbf{Type 1}} score features. 3) Then, an \textit{Acoustic Encoder} converts the predicted duration with its corresponding phoneme and pitch into musical features at the phoneme level. 4) A \textit{Duration Predictor} forecasts the acoustic frame lengths for each phoneme and passes the value to the \textit{Length Regulator} for expansion. 5) Finally, the \textit{Decoder} transfers the frame-level features to the spectrum. 





\subsection{Acoustic Feature Processing Strategy }
\label{ssec: Acoustic Feature Processing Strategy}
Fig.~\ref{fig:AFP} illustrates the difference between \textit{\textbf{PHONEix}} and other AFP strategies.
Some of the previous works calculate acoustic features based solely on time sequence from the music score without referencing actual singing (Fig.~\ref{fig:input}~(a)). Others input the actual phoneme time sequence for training, which varies from the inference \textit{\textbf{Type 1}} input.
To bridge the gap between the music score and the actual singing voice (illustrated in Fig.~\ref{fig:model}), we employ a new AFP strategy \textit{\textbf{PHONEix}}. The input of the \textit{Acoustic Encoder} consists of the \textit{\textbf{Type 1}} note-level score features, \textit{i.e.,} phoneme, pitch, and note duration. Then, the final output of the \textit{Acoustic Encoder} is expanded to fit the actual duration under the guidance of an annotated phoneme time sequence. Nevertheless, the duration distributions of vowels and consonants vary sensibly. It is difficult for the duration predictor to provide an accurate frame expansion length with the same duration input among all phonemes in each note. In order to obtain a more precise phoneme pronunciation duration, a \textit{Prior Encoder} is introduced to help learn the proportion of phoneme pronunciation. The pronunciation of lyrics varies with pitch and note duration and is influenced by the context of the song. Hence, we utilize a Bi-LSTM or Transformer-based encoder instead of just analyzing linguistic features and providing predictions syllable by syllable. The \textit{Prior Encoder} can share the same networks as the \textit{Acoustic Encoder}, which can be easily adapted to various encoder-decoder-based acoustic models. 



\subsection{Phoneme Distribution Predictor}
 As discussed in Sec.~\ref{ssec: overall framework}, we propose a learned phoneme duration for the following parts of the model, instead of using the same note duration for vowels and consonants or fixed phoneme duration (i.e. phoneme annotation). The learned phoneme duration is predicted by a proposed \textit{Phoneme Distribution Predictor} to distinguish vowels and consonants. To be specific, the predictor employs a series of 1-D convolutional layers to extract context information from the musical score features generated by the \textit{Prior Encoder}. The predictor gives a $n$-dimension probability distribution $\textbf{\textit{p}} = (p_1, p_2,\dots,p_n)$, where $n$ is the maximum number of phonemes in a syllable-note pair, which is determined by a syllable-phoneme lexicon. After that, we utilize the distribution to split the note duration $d^{\text{note}}$ extracted from the music score. The proportion for the $i$\textsuperscript{th} phoneme is $p_i$ and the annotated phoneme duration $d_i^{\text{ph}}$ is conform to actual phoneme distribution. 
A mean square error serves as an additional loss term to fit the predicted proportion to the real proportion in the actual singing pronunciation:
\begin{equation}
\mathcal{L}_{\text{ph}}=\Vert d^{\text{note}} \times \textbf{\textit{p}} - \textbf{\textit{d}}^{\text{ph}} \Vert_2.
\end{equation}
%where $k (\leq n)$ is the number of phonemes contained in the corresponding syllable-note pair.

In some fast-speed songs, the phoneme duration can be too short to be passed by the acoustic frame resolution. To ensure integrity of phonemes, we assign a minimum duration to these short phonemes. Therefore, in inference, the phoneme duration in acoustic frame-level is defined as follows:
\begin{equation}
    \widehat{d}_i^{\text{ph}}=\mathrm{rint}[\max(d^{\text{note}} \times p_i, 1)] \quad (i=1,\dots,k),
\end{equation}
where $\mathrm{rint}$ is a rounding function that maps the duration to the closest integer.


