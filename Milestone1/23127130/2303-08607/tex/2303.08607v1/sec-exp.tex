\begin{table*}[!t]
\small
\caption{ \small Comparison of the proposed AFP strategy, \textbf{\textit{PHONEix}}, with baselines (i.e., \textbf{\textit{Type 1}} and \textbf{\textit{Type 2}}). It is evaluated with Bi-LSTM or Transformer based encoder-decoder structures on Ofuton and Opencpop. Evaluations include three objective metrics (\textbf{MCD}, \textbf{VUV\_E}, and \textbf{SA}) and a subjective metric (\textbf{MOS}) are described in Sec.~\ref{ssec: exp settings}. The details of models and datasets are discussed in Sec.~\ref{ssec: dataset}. }
% \resizebox{1\textwidth}{!}{
\centering
% \small

\begin{tabular}{c|clcccc}
\toprule
\textbf{Dataset}          & \textbf{Model}               & \multicolumn{1}{c}{\textbf{Method}} & \textbf{MCD ↓}            & \textbf{VUV E ↓}          & \textbf{SA ↑} & \textbf{MOS ↑} \\ 
\midrule
\multirow{8}{*}{Ofuton}   & \multirow{3}{*}{LSTM}         & \textit{\textbf{Type 1}}                         & 6.74          & \textcolor{white}{0}2.49\%           & 56.65\%            & 2.54 ± 0.05    \\
                          &                              & \textit{\textbf{Type 2}}                    & 6.34          & \textcolor{white}{0}2.53\%                    & 57.58\%            & 2.58 ± 0.05    \\
                          &                              & \textbf{\textit{\textbf{PHONEix}}}                    & \textbf{6.28} & \textcolor{white}{0}\textbf{2.41\%}           & \textbf{61.17\%}   & \textbf{3.03} ± 0.06    \\
\cmidrule(l){2-7}
                          & \multirow{3}{*}{Transformer} & \textit{\textbf{Type 1}}                         & 6.95          & \textcolor{white}{0}1.93\%                    & 61.09\%            & 2.60 ± 0.05    \\
                          &                              & \textit{\textbf{Type 2}}                    & 6.78          & \textcolor{white}{0}\textbf{1.71\%}           & 58.44\%            & 2.26 ± 0.05    \\
                          &                              & \textbf{\textit{\textbf{PHONEix}}}                    & \textbf{6.52} & \textcolor{white}{0}2.15\%           & \textbf{62.47\%}   & \textbf{3.01} ± 0.06   \\
\cmidrule(l){2-7}
                          &                              & Ground Truth                                  & -                        & -               & -                                          & 4.47 ± 0.04   \\
\midrule
\multirow{8}{*}{Opencpop} & \multirow{3}{*}{LSTM}         & \textit{\textbf{Type 1}}                         & 9.33          & \textcolor{white}{0}6.79\%          & 46.92\%            & 2.09 ± 0.05    \\
                          &                              & \textit{\textbf{Type 2}}                    & 8.62          &                     11.32\%                   & 53.93\%            & 2.48 ± 0.05   \\
                          &                              & \textbf{\textit{\textbf{PHONEix}}}                    & \textbf{7.90} & \textcolor{white}{0}\textbf{6.05\%}         & \textbf{60.97\%}   & \textbf{3.56} ± 0.06  \\
\cmidrule(l){2-7}
                          & \multirow{3}{*}{Transformer} & \textit{\textbf{Type 1}}                         & 8.77          & \textcolor{white}{0}6.19\%                   & 52.16\%            & 2.52 ± 0.05   \\
                          &                              & \textit{\textbf{Type 2}}                    & 9.05          &                     10.59\%                 & 52.90\%            & 1.89 ± 0.04   \\
                          &                              & \textbf{\textit{\textbf{PHONEix}}}                    & \textbf{8.42} & \textcolor{white}{0}\textbf{5.97\%}  & \textbf{60.47\%}   & \textbf{3.03} ± 0.05  \\
\cmidrule(l){2-7}
                          &                              & Ground Truth                                  & -                        & -               & -                                        & 4.72 ± 0.03   \\ 
\bottomrule
\end{tabular}
% }
\label{tab: main result}
\end{table*}
\section{Experiments}
\label{sec: exp}




\subsection{Dataset}
\label{ssec: dataset}
We conduct our experiments on two datasets in two different languages. Every song is sampled at 24 kHz. We extract 80-dimensional Mel spectrograms with a window length of 1200 samples and a shifting size of 300 samples. The spectrograms normalized with global mean and variance from the train set are used as the ground truth for acoustic model outputs. 

%\subsubsection{Ofuton}
\noindent\textbf{\textit{Ofuton:}}
%Ofuton is 
an open male singing corpus, consisting of 56 Japanese songs \cite{Ofuton}. We divide the dataset by songs, resulting in a training set (51), a valid set (5), and a testing set (5), respectively.\footnote{Since there is no official split of the data, we instead use the same split as in the previous work \cite{shi2022muskits}.} The annotation information of the dataset includes manually labeled phoneme duration and music scores in MusicXML format. The segments of songs are separated by the \texttt{<sil>}, \texttt{<pau>}, and \texttt{<br>} silence marks, which align with segmentations of rest notes and breath marks in MusicXML.


%\subsubsection{Opencpop}
\noindent\textbf{\textit{Opencpop: }}
%Opencpop is 
a public Chinese female singing corpus of 100 songs in total \cite{Opencpop}. The annotation of phoneme duration and music score are organized in Textgrids. We follow the segmentations of the official release.


\subsection{Experimental Settings}
\label{ssec: exp settings}
The experiments are conducted using the music processing toolkit Muskits \cite{shi2022muskits}, which is adapted from ESPnet \cite{watanabe2018espnet}.
%\footnote{\url{https://github.com/espnet/espnet/tree/master/egs2/{ofuton_p_utagoe_db, opencpop}/svs1}} 
More details about the codes can be found on github \footnote{https://github.com/A-Quarter-Mile/PHONEix}.

\noindent\textbf{\textit{Model architectures:}}
%\subsubsection{Model architectures}
%\label{ssec: model architectures}
We verify the proposed AFP strategy on two acoustic models. Both models adopt the encoder-decoder structure and the duration predictor proposed in FastSpeech~\cite{FastSpeech}. The models also share the configuration of the embedding layers. To be specific, we utilize 384-dimensional embedding layers for lyrics, notes, and note durations. The encoder and decoder of the first model are both three-layer 256-dimensional bidirectional Long-Short Term Memory units (Bi-LSTM) following~\cite{baseline1}. The second network utilizes a Transformer structure from~\cite{lu2020xiaoicesing}. Apart from the additional loss for phoneme distribution predictor, we apply the same loss functions as ~\cite{baseline1} and ~\cite{lu2020xiaoicesing}. We use the same HiFi-GAN vocoder~\cite{kong2020hifi} to convert the predicted acoustic features into waveforms.

\noindent\textbf{\textit{Training:}}
%\subsubsection{Training}
For both models, we utilize the Adam optimizer with a learning rate of 0.001 without schedulers. The batch size is 16. In all experiments,  we select the model checkpoints with the best validation losses for further evaluation.

\noindent\textbf{\textit{Evaluation metric:}}
%\subsubsection{Evaluation metric}
We follow the objective and subjective evaluations in~\cite{baseline1}. For objective evaluation, we utilize Mel-cepstral distortion (\textbf{MCD}), voiced/unvoiced error rate (\textbf{VUV\_E}), and semitone accuracy (\textbf{SA}). For subjective evaluation, we randomly select 30 singing segments from the test set and invite 40 judges to score the quality of each segment on a scale of 1 to 5. We report the mean opinion scores (\textbf{MOS}) with a 95\% confidence interval.



\begin{table}[t]
\small
\caption{\small Objective evaluations of the ablation study on learned phoneme duration. Detailed discussions can be found in Sec.~\ref{ssec: ablation study}.}
\centering
% \resizebox{1\columnwidth}{!}{
\begin{tabular}{lcccc}
\toprule
\multicolumn{1}{c}{\textbf{Duration Source}}          & \textbf{MCD ↓}            & \textbf{VUV\_E ↓}         & \textbf{SA ↑}        \\
\midrule
\textit{\textbf{\textit{\textbf{PHONEix}}}} & \textbf{6.28 } & \textbf{2.41\%}          & 61.17\%          \\
\midrule
Annotation                      & 6.46          & 2.51\%                & \textbf{61.23\%} \\
Statistical Rule                   & 6.39           & 2.59\%          & 60.49\%          \\
Music Score                               & 6.73          & 2.69\%               & 59.03\%          \\
\midrule
Music Score (Note)                             & 6.79           & 2.74\%               & 58.39\%          \\
\bottomrule
\end{tabular}
% }
\label{tab: learned phone duration}
\end{table}

% \begin{table}[t]
% \caption{Ablation analysis on the phoneme duration predictor.}
% % \resizebox{1\columnwidth}{!}{
% % \small
% \begin{tabular}{clccc}
% \toprule
                           
%                            \textbf{Model }& \multicolumn{1}{c}{\textbf{Methods}} & \textbf{MCD ↓}        & \textbf{VUV\_E ↓}      & \textbf{SA ↑} \\ 
% \midrule
% \multirow{2}{*}{LSTM}         & \textit{\textbf{\textit{\textbf{PHONEix}}}}                     & \textbf{6.28 } & \textbf{2.41\%}  & 61.17\%            \\
%                              & \quad \emph{w/o} phn\_DP                   & 6.46           & 2.51\%                  & \textbf{61.23\%}   \\
% \midrule[0.3pt]
% \multirow{2}{*}{Transformer} & \textit{\textbf{\textit{\textbf{PHONEix}}}}                     & \textbf{6.52 } & \textbf{2.15\%}         & \textbf{62.47\%}   \\
%                              & \quad \emph{w/o} phn\_DP                 & 6.69           & 2.40\%           & 62.32\%            \\ 
% \bottomrule
% \end{tabular}
% % }
% \label{tab: phone duration predictor}
% \end{table}

\subsection{Comparison with baselines}

We compare the proposed strategy with the \textit{\textbf{Type 1}} and \textit{\textbf{Type 2}} processing strategies described in Sec.~\ref{sec:intro}.
Table~\ref{tab: main result} presents the evaluation scores of the proposed acoustic processing strategy and the two baselines. All strategies are conducted with LSTM and Transformer-based encoder-decoder acoustic model structures on the Ofuton and Opencpop datasets, respectively.

\noindent
\textit{\textbf{Type 1}}: The AFP strategy of music score does not involve actual phoneme time sequence in the acoustic model. We apply the processing pipelines in Xiaoice~\cite{lu2020xiaoicesing}. It accepts score features at note level and expands frame lengths according to the phoneme duration produced by HMM-based forced-alignment~\cite{mcauliffe2017montreal,HMM}.

\noindent
\textit{\textbf{Type 2}}: The other way of processing acoustic features is to use the annotated phoneme time sequence as acoustic encoder input and length expansion ground truth for the length regulator. We analyze the acoustic feature following~\cite{baseline1}. The training stage has been tuned to fit the actual singing. However, the music score input in inference differs from the 
annotated ones in training as described in Sec. \ref{sec:intro}.

The results in Table~\ref{tab: main result} show significant improvements in both subjective and objective metrics in our SVS system equipped with an AFP strategy. The cases of corresponding results are shown in Fig.~\ref{fig:case}. The result of Fig.~\ref{fig:case} shows that the \textit{\textbf{PHONEix}} obtains the best estimation of duration of phonemes among all method, which further confirms the effectiveness of our proposed \textit{\textbf{PHONEix}}.



\subsection{Ablation study}
\label{ssec: ablation study}

In this subsection, we test the effectiveness of learnable phoneme duration input for the acoustic encoder and the validity of the proposed phoneme distribution predictor.

% \noindent
% \textbf{Phoneme level feature}: 
In order to prove the effectiveness of learned phoneme duration, we compare it with other fixed duration sources for the acoustic encoder: 1) annotation (i.e., annotated phoneme duration). 2) statistical rule where the duration is derived from the statistical calculation of the corresponding dataset. 3) music score where the duration is obtained from the note duration. We use the same ground truth phoneme duration (annotated phoneme duration) in the \textit{Length Regulator} for the above three settings. The results show that the learned phoneme duration from \textit{\textbf{PHONEix}} gets a higher score in MCD and VUV\_E and comparable semitone accuracy to the annual annotation. The learned phoneme duration indicates the precise division of vowels and consonants in each note, which leads to ample pronunciation in singing. In addition, we train the SVS system that direct optimized over acoustic features at the syllable-note level (i.e. Music Score (Note) in Table~\ref{tab: learned phone duration}) and expand frame length by note time sequence. The note-level feature gets worse performances that it cannot generate the representation needed for good pronunciation without phoneme division (see Fig.~\ref{fig:case} (f)).

% \noindent
% \textbf{Phoneme distribution predictor}: We verify the validity of the proposed phoneme distribution predictor on two encoder-decoder-based acoustic models. The results show an increase in scores than the ones without phoneme distribution predictor.

\begin{figure}[t]
\
	\centering
	\includegraphics[width=1.0\columnwidth]{case.pdf}
 \vspace{-20pt}
	\caption{\small The cases of various methods. The number above the picture indicates the duration of the phoneme (in seconds). }

	\label{fig:case}
 \vspace{-10pt}
\end{figure}