\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{iccv}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}

\usepackage{multirow}% lin
\usepackage{xcolor} % lin
\usepackage{subfiles}  % lin
\usepackage{subfigure} % lin
% \renewcommand{\thesubfigure}{\normalfont(\alph{subfigure})}  % lin
\usepackage{array} % lin
\usepackage{booktabs} % lin

\usepackage{xcolor} % lin
\newcommand{\sgg}[1]{{\color{black}#1}}
\newcommand{\cmt}[1]{{\color{black}#1}}
\newcommand{\jy}[1]{{\color{black}#1}}
\newcommand{\jl}[1]{{\color{black}#1}}
% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[breaklinks=true,bookmarks=false]{hyperref}

\iccvfinalcopy % *** Uncomment this line for the final submission

\def\iccvPaperID{****} % *** Enter the ICCV Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
\ificcvfinal\pagestyle{empty}\fi

\begin{document}

%%%%%%%%% TITLE
\title{GridCLIP: One-Stage Object Detection 

by Grid-Level CLIP Representation Learning}

\author{Jiayi Lin\\
Queen Mary University of London\\
{\tt\small jiayi.lin@qmul.ac.uk}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
\and
Shaogang Gong\\
Queen Mary University of London\\
{\tt\small s.gong@qmul.ac.uk}
}

\maketitle
% Remove page # from the first page of camera-ready.
\ificcvfinal\thispagestyle{empty}\fi



%%%%%%%%% ABSTRACT
\begin{abstract}
A vision-language foundation model pretrained on very large-scale image-text paired data 
has the potential to provide generalizable knowledge representation for
downstream visual recognition and detection tasks,  especially on
  supplementing the undersampled categories in downstream  model training.
Recent studies utilizing
    CLIP for  object detection have shown 
    that a two-stage detector design typically 
    outperforms a one-stage detector, 
    while requiring more expensive
    training resources and longer inference time.
 In this work, we propose a one-stage detector \jy{GridCLIP}
    \sgg{that narrows its} \jy{performance} gap \sgg{to those of} two-stage detectors,
    with approximately 43$\times$ and 5$\times$ faster than its two-stage counterpart (ViLD) in the training and test process respectively.
    % while being inherently much more computationally efficient. 
 \jy{GridCLIP learns grid-level representations 
    to adapt to the intrinsic \sgg{principle of one-stage detection learning}
    by expanding}
    the conventional CLIP image-text holistic mapping 
    to a more fine-grained, grid-text alignment.
    \sgg{This differs}
    \jy{from the region-text mapping 
    in two-stage detectors \sgg{that apply} CLIP directly by
    treating regions as images.}
 % GridCLIP
 %      aims to extend the CLIP's generalization ability from 
 %      image-level to  grid-level for more fine-grained text
 %      and image feature correlations. 
Specifically, GridCLIP \jy{performs Grid-level Alignment to adapt} the CLIP image-level representations 
    to grid-level representations 
    % (visual embeddings)
    % that 
    \jy{by aligning} to CLIP category representations to learn the annotated (especially frequent) categories.
    % provides
    % spatial information for detection.
To learn generalizable visual representations of broader 
    categories, especially undersampled ones,
    we perform Image-level Alignment during training to
    % by
    % aligning the image-level feature representations
    % \jy{to implicitly help the grid-level representations align}
    % to the representations
    % from the fixed CLIP image encoder. 
    \sgg{propagate broad pre-learned categories in the CLIP image encoder from
        the image-level to the grid-level representations}.
Experiments show that the learned CLIP-based grid-level representations boost the performance of undersampled (infrequent and novel) categories, 
reaching \jy{comparable} detection performance on the LVIS benchmark.
\end{abstract}


%%%%%%%%% BODY TEXT

\section{Introduction}
\input{figures/intro.tex}
Simultaneous multi-category object detection aims to 
both recognize (classify) and detect (locate) all instances of  given
%   known
  categories in an image. 
  A  significant challenge in training a good detector
%   to Supervised Learning of a robust and accurate deep 
%   model for object detection
is the  cost of labeling a large-scale dataset on a broad
  range of object categories with balanced data distributions. Existing
  detection datasets are often
  imbalanced with a long-tail distribution across
  categories~\cite{saichev2009theory} where some object categories have only
  a few or zero training sample(s).
%    which is not sufficient for learning decisive feature for detection.
    % of those categories. 
To deal with these undersampled categories, 
    few-shot and zero-shot learning  have been explored, but
      they are inherently weaker models when compared to a fully supervised
      learning based model. 
      
\jy{Elsewhere, Self-Supervised Learning (SSL) has received increasing research interest for} exploring widely available unlabelled data.
 Self-supervised pretraining followed by supervised
  fine-tuning for constructing a detector has been proposed recently~\cite{yang2021instance, wei2021aligning}.
 An example is Open Vocabulary Object Detection (OVOD)~\cite{zareian2021open}, 
    which pretrains a model on image-caption pairs containing a
    substantial amount of  broad categories, 
     followed by fine-tuning the model on a detection
     specific dataset of only a few base categories.
 For the pretraining stage that supplements knowledge for undersampled categories,  Vision-Language pretrained Models  (VLMs) are widely adopted.
%  Compared to
%     most SSL approaches 
%      that only apply visual data~\cite{he2020momentum,grill2020bootstrap,chen2020simple},
%       Vision-Language} pretrained Models  (VLM}
%      can learn from large-scale text descriptions of
%       images.
As one of the most widely adopted VLMs, CLIP~\cite{radford2021learning} is pretrained 
% on the pretext task
%     of paired captions (text descriptions) with
%     unsegmented images 
    on a dataset of 400 million image-text pairs  with a vocabulary of over 49,000 words, providing generalizable visual embeddings 
    of broad categories
    that helps supplement the undersampled categories in a detection dataset.
    % and it learns
        % and text
        % feature representations.
% Furthermore, compared to the commonly used ImageNet~\cite{deng2009imagenet} pretrained model where the
% image feature extractor is pre-trained to  align with the
% predefined 1,000 categories, 
% the image feature representation of CLIP is learned to  align with
% text  with
% Therefore,
%     when applied to downstream detection tasks,
%     CLIP 
%       can benefit the undersampled categories in a downstream detection dataset.

% Grid-level representation is the intrinsic design of one-stage detectors, 
%     while we transfer the image-level representation of CLIP to grid-level to fit one-stage detectors by introducing two alignments. }

    
    % with the potential for better generalization. 
%     in the former case, any object of unknown categories is likely aligned to
%     an unknown `background' category,
%     % making the "background" category complicated 
%     unable to generalise to unseen or unlabeled object categories 
%     in the training data of a target detection domain.
% In contrast, CLIP provides a significantly richer vision-text representation space enabling 
  % Those detectors~\cite{gu2021open, du2022learning} can not only detect novel categories but also improve rare categories by minimizing the bias to frequent categories.
  
  
Recent approaches have been exploring the CLIP-based representation for object detection,  mostly for Open-Vocabulary Object Detection (OVOD)~\cite{gu2021open, zhou2022detecting}.
 These detectors can be  broadly considered as two-stage detectors~\cite{gu2021open,zhong2021regionclip,du2022learning,feng2022promptdet,kuo2022f} 
and one-stage detectors~\cite{xie2021zsd,rao2021denseclip,ma2022open}.
 Although  a one-stage  detector is inherently simpler and
    less costly to compute, it suffers from poorer performance than
    that of a two-stage detector using CLIP~\cite{ma2022open}. 
    In this work, we propose a CLIP-based one-stage detector GridLCIP that narrows the
    performance gap from typical two-stage detectors,
    while requires a much shorter training time (43$\times$ less compared to ViLD~\cite{gu2021open}) and test time (5$\times$ faster) .
% zhou2022detecting(detic, use proposal, but is one-stage)
% <disadvantage of current methods: complicated>
    %  Most of these methods~\cite{zhong2021regionclip,zhou2022detecting, ma2022open, gao2021towards, feng2022promptdet} require extra large-scale or labeled dataset like CC3M or ImageNet-21K for another pretraining process or to supplement the downstream detection dataset, making the training process complicated and resource-consuming.
    % Although they make notable progress in CLIP based detection, it is relatively inefficient
    %     as a two-stage detector,
    %       which tends to pass the CLIP image encoder 
    %       several times for computing the embedding of each proposal during the training process.
% <reason for grid-level alignment
    
    
Specifically, we \jy{learn} 
    grid-level representations of images that can be further used for classification, since an object in one-stage detectors is noted by the category of a grid (a pixel in a feature map) and its corresponding bounding box.
Therefore, we expand the
    conventional CLIP image-text holistic mapping to grid-text
    mapping, namely \emph{Grid-level Alignment}. 
Although some other one-stage detectors~\cite{ma2022open} 
    also share similar spirits, 
    they align the image embeddings trained 
    from supervised or visual data only self-supervised learning methods
    (rather than CLIP-like visual-language pretrained models) to align with CLIP text embeddings. 
While we directly adapt the CLIP image embeddings to generate grid-level embeddings, 
which directly benefits from the generalizability of the CLIP image encoder 
and is intrinsically more consistent with CLIP text embeddings.
 % In particular, we 
 %    maximize the similarity between grid-level image embeddings from the detector classification head and
 %    CLIP embeddings of base categories 
 %    in the target domain. 
 However, similar to DenseCLIP~\cite{rao2021denseclip} 
    which can only perform close-set detection, 
     grid-level alignment is only performed on the base categories 
    without  the scope to learn knowledge of novel  (unseen) categories
     for open-set detection.
 
To further exploit CLIP to learn representation for novel categories, 
    some approaches~\cite{ zhou2022detecting, ma2022open,
      gao2021towards, feng2022promptdet}  used extra
    image-caption  or  labeled datasets like
    CC3M~\cite{sharma2018conceptual} or
    ImageNet-21K~\cite{deng2009imagenet},  
    making the training process complicated and resource-consuming. 
 In this work, we want to explore CLIP directly without the need
  of extra image-caption
  and/or labelled training data in order to
    learn novel categories by applying knowledge distillation on
    CLIP.
As revealed in HierKD~\cite{ma2022open}, 
    the gap between one-stage and two-stage detectors is that the knowledge distillation on 
    two-stage detectors happen on image regions of both base and novel categories~\cite{gu2021open,zhou2022detecting} 
    given by a  separate pretrained region proposal network  (RPN), hence
    two-stage, while the one-stage detectors mainly only use base categories for knowledge distillation~\cite{xie2021zsd}.
Therefore, HierKD aligns the text embedding of the caption 
    to its paired image embedding,
    % by mimicking the contrastive learning in CLIP, 
    and further 
    uses an attention layer for adapting the gap between captions and images. 
However,  HierKD requires the training images to have paired
captions in addition to detection annotations, making it
  unscalable to training on other detection datasets for more general detection
  tasks. 
 To avoid all these additional requirements on model training, we
  propose to use visual-to-visual alignment instead of
   caption-to-visual alignment. This is designed
    to learn the visual embeddings of both base and novel categories
     without the need for further adaptation.
We call it \emph{Image-level Alignment}.
Specifically, we align the image-level embeddings with the one that is generated by a fixed CLIP image encoder (teacher).
Since the feature extractor of image-level embeddings and grid-level embeddings are mainly shared, the grid-level embeddings can implicitly obtain knowledge 
of undersampled categories 
from the teacher. 
In this way, we \jy{implicitly train} the grid-level embeddings 
to align to both base and novel categories in CLIP space.


% For efficiency and simplicity, we apply CLIP on a one-stage detector ~ without extra pretraining process or datasets, which focuses on exploiting the CLIP pretrained vision-language representation space.
% To solve the above problem(s) in the current CLIP based detection
%   models, (make sure to spell out what're the
%     problems first in the last paragraph) 
%     In this work we explore the
%   CLIP pretrained vision-language representation space for
%   optimizing object detection.
%   against a known set of categories. 
%   In our approach, we design a one-stage detection model.
% To our best knowledge, DenseCLIP is the only approach that apply CLIP for close-set detection, which adapt the image-text matching in CLIP to grid-label matching and use the matching results to further guide the detection process.
% While it does not use the original alignment space, which uses learnable prompt and context aware text embeddings. 
% In comparison, we try to preserve the original CLIP alignment space as in ~\cite{gu2021open, zhou2022detecting} by distillation.
%
% To this end, 
 Overall, we propose a one-stage \jy{detector}  
    GridCLIP,
     which exploits CLIP to supplement the knowledge of undersampled categories in downstream detection datasets by simultaneously applying 
    grid-level and image-level alignments.
Our contributions are:
 (1) We exploit CLIP to
supplement the missing knowledge of undersampled object
  detection categories  in training a \jy{one-stage} detector, mitigating
the poor performance due to the long-tail data distribution in most
existing detection training data.
 (2) We propose a simple yet effective visual-to-visual knowledge
  distillation method for learning novel categories in
    constructing a one-stage CLIP-based detector, providing
  2.4 AP gains  on novel categories  compared to the  baseline.
(3)
\jy{GridCLIP is} capable of 
   handling Open-Vocabulary Object Detection with considerable
  scalability and generalizability,
 reaching the
    comparable performance to two-stage detectors in Open-Vocabulary Object Detection with much higher training and inference speed,
   without using extra pretraining processes or additional
     fine-tuning datasets. 


\section{Related Works}
% ( other Object Detection models
%   in general, )

%\subsection{Vision-Language Pretrained Model}
% pretraining for od： detCLIP， GLIP
% post pretraining： regionCLIP，


\noindent \textbf{Vision-Language Pretrained Model (VLM).}
Visual-only pretrained model has dominated the pretraining process
for several years until VLMs have appeared.
In comparison, 
Vision-Language Pretrained Models (VLMs) are able to 
align more visual concepts out of manual predefined categories
to natural language, extending the generality of the model.
Recently, 
a considerable number of vision-language pretrained models~\cite{radford2021learning,singh2021flava,jia2021scaling,yuan2021florence} emerge,
training from large-scale image-text data 
in an unsupervised way.
These models usually have both image and text encoders to generate corresponding features that can be aligned in a cross-modality representational space for corresponding image-text matching.
Utilizing these alignment spaces helps
zero-shot transfer 
to a wide range of downstream visual recognition tasks, such as
object detection~\cite{huang2022unsupervised,du2022learning,feng2022promptdet}, 
segmentation~\cite{zhou2021denseclip,luddecke2021prompt}, 
image retrieval~\cite{zhang2021vinvl,li2021align}.
%
As one widely-used instance, 
CLIP~\cite{radford2021learning} is trained on 400 million image-text pairs 
through a contrastive objective,  which extends significantly the
        generalizability and usability of
        % both 
        the learned image 
        embeddings to 
        align to broad categories,
showing competitive performance 
with its fully supervised counterparts. 
CLIP is widely applied 
both in downstream tasks oriented pretraining~\cite{zhong2021regionclip,li2022grounded} 
and in fine-tuning for downstream tasks~\cite{gu2021open,rao2021denseclip}.
% and finally forms an alignment space.
% . At test time the learned text encoder synthesizes a zero-shot linear classifier by embedding the names or descriptions of the target dataset’s categories.
% 补充：相较于纯visual的更能迁移到zero-shot上，已广泛用于各种任务

%\subsection{Pretrained Model for Object Detection}


% \noindent \bf Object Detection.

\noindent \textbf{Object Detection using VLM.}
OVR-CNN~\cite{zareian2021open} is the first to utilize natural language (captions) for object detection. 
\jy{While recent detectors apply large-scale image-text datasets
to learn generalizable image representations. 
Some VLMs like GLIP~\cite{li2022grounded} and DetCLIP~\cite{zhang2022glipv2} utilize large-scale annotation datasets in addition to image-text pairs for pertaining, which requires relatively high annotation cost. 
While we focus on utilizing unsupervised VLMs like CLIP and learn from annotation datasets of a limited number of categories to transfer to broader categories.
 }
% , most of which focus on aligning the vision-language alignment space.

To learn knowledge of base categories, 
    most detectors~\cite{gu2021open,zhou2022detecting} replace the classifiers of their detection heads with VLM text embeddings.
    Recent detectors mainly improve the learning in two aspects: learning better text embeddings of categories and extracting image embeddings to align with these text embeddings. 
(1) For generating better text embeddings, 
    also called \emph{Prompt Learning},
current approaches can be roughly classified as template-based and learnable prompting. 
The template-based one uses fixed incomplete sentences 
    that can accept labels to build complete sentences~\cite{gu2021open,ma2022open,zhong2021regionclip},
    while the learnable prompting methods concatenate learnable parameters with category labels as the input, 
    where the prompt is implicitly learned during fine-tuning~\cite{zhou2021learning,du2022learning,feng2022promptdet}.
GridCLIP uses template-based prompting as in the original CLIP and some OVOD detectors~\cite{gu2021open,ma2022open,zhong2021regionclip} for simplicity and scalability.
%  
   % The main difference between two-stage and one-stage detectors is 
    %     the way an instance is represented~\cite{ma2022open}. 
    % When applying CLIP for detection, 
    %     the embedding of the instance is matched with a series of category embeddings 
    %     % generated from the CLIP text encoder 
    %     and the category with highest matching score is selected. 
    % Two-stage detectors represent an instance in region level, 
    %     the region-level embedding of the instance can be directly generated by the CLIP image encoder using the image region as input. 
    %     % which is then trained to align with its corresponding category embedding. 
    % While One-stage detectors represent an instance in grid level (with the bounding box coordinates related to the grid 
    % where the gird-level embedding can not be directly obtained since 
(2) For extracting image embeddings to align with text embeddings, two-stage
    detectors~\cite{ren2015faster,sun2021sparse,gu2021open, zhou2022detecting, du2022learning,zhong2021regionclip} use the embedding of cropped object bounding-box proposals.
However, they need to train a region proposal network first and require multiple inferences of the CLIP image encoder to compute the visual embedding for each region, 
    which is relatively inefficient. 
In comparison, a one-stage
    detector~\cite{lin2017focal,tian2019fcos, rao2021denseclip} aligns parts
    in an image 
    represented by grid-level embeddings. 
DenseCLIP~\cite{rao2021denseclip} 
    adapt it as
    aligning grid-level image features with text, 
    % which however does not preserve the alignment space of CLIP
    while can only perform under close-set settings. 
 While HierKD~\cite{ma2022open} performs image-level, region-level and grid-level alignment to CLIP, 
    which however requires the match captions with the detection dataset, 
    limiting its deployment to other downstream detection datasets.
Our method uses grid-level alignment for efficiency 
    while preserving the original alignment space of CLIP to gain better generalization ability, 
     without requiring extra datasets.


To learn knowledge of novel categories, some approaches~\cite{ zhou2022detecting, ma2022open, gao2021towards, feng2022promptdet} use extract knowledge from external datasets extra image-caption or labeled datasets like CC3M~\cite{sharma2018conceptual} or ImageNet-21K~\cite{deng2009imagenet}, 
    % Most of these methods require extra large-scale or labeled dataset like CC3M or ImageNet-21K 
    % for another pretraining process or to supplement the downstream detection dataset, 
    making the training process complicated and resource-consuming.
While we argue that CLIP has been trained over a broad vocabulary and 
     has the ability to provide visual embeddings of various categories. 
Therefore, we explore the original CLIP representation space 
    to learn novel categories by applying knowledge distillation on CLIP as in ViLD~\cite{gu2021open}. 
% like CC3M~\cite{sharma2018conceptual} or ImageNet-21K~\cite{deng2009imagenet}. 
% RegionCLIP~\cite{zhong2021regionclip} uses an extra pretraining process on CC3M~\cite{sharma2018conceptual} for 600k iterations, to adapt the image-text alignment to region-text alignment.
    % ViLD~\cite{gu2021open} uses Mask RCNN~\cite{he2017mask} pretrained on base categories as to get region proposals and matches the region embeddings with text embeddings to get the prediction of category. 
    % 
    % detector~\cite{ren2015faster,sun2021sparse,gu2021open, zhou2022detecting, du2022learning,} cropped object bounding-box proposals, 
% to further  build a new dataset that contains novel categories.
 % Detic~\cite{zhou2022detecting} uses the broad categories of ImageNet-21K~\cite{deng2009imagenet} to supplement the downstream detection dataset. 
 % PromptDet~\cite{feng2022promptdet} collects data of novel categories  from  LAION-400M~\cite{schuhmann2021laion} to supplement the downstream detection dataset.
 
 % DetPro~\cite{du2022learning} learn the prompt to help construct better text embeddings of categories, improving the baseline ViLD by 3.4 AP in rare categories. 
 

\input{figures/framework.tex}

\section{Approach}

The overall model design of GridCLIP is shown in
    Figure~\ref{framework}. 
We first introduce
    the strategy of adopting the CLIP embeddings for a detection task, and
    then present the approach to mapping simultaneously the CLIP
    representation by both grid-level and image-level alignments
    based on a one-stage detector FCOS~\cite{tian2019fcos}. 

\subsection{Adapting CLIP for Detection}\label{sect3_1}
CLIP consists of an image encoder (ResNet~\cite{he2016deep} or
    ViT~\cite{dosovitskiy2020image}) 
    and a text encoder (Transformer~\cite{vaswani2017attention}), 
    which together form
    the alignment space of visual and language embeddings. 
However, as the image
    embedding in CLIP is a high-dimensional feature vector of an entire image
    instead of a spatial region or pixels,
    and the text embedding is encoded from a
    sentence  instead of  a single category label in detection, 
    further adaptation for the detection task is needed. 

\noindent \textbf{Generating Image Embedding.}
% resNet50 
The original CLIP image feature ${\overline{z}}$  is a single
    high-dimensional feature vector representing an entire image
    without spatial information.  
To get the grid-level feature ${z}$ , inspired by DenseCLIP~\cite{rao2021denseclip}, we use the other
feature from the last layer 
of the CLIP image encoder. 
Specifically,
taking the ResNet50 encoder as an example, the final output feature in the 5-th stage ${C}_{5} \in \mathbf{R}^{H_{5}  \times W_{5} \times D_{5}}$ is  
% $\left\{\mathbf{x}_{i}\right\}_{i=1}^{4}$ 
 first performed global average pooling to
 get the image-level feature $\overline{{C}}_{5} \in \mathbf{R}^{1 \times 1 \times D_{5}}$ , 
where $H_5$, $W_5$, $D_{5}$ are the height, width and number of channels of the feature in the 5-th stage of the ResNet50.
Then the concatenated features $\left[\overline{{C}}_{5}, {C}_{5}\right]$ 
are fed into a Multi-Head Self-Attention (MHSA) layer~\cite{vaswani2017attention} as follows,
\begin{eqnarray}
   [\overline{{z}}, {z}] ={ \rm{MHSA}}\left(\left[ \overline{{C}}_{5}, {C}_{5}\right]\right).
\end{eqnarray}

In CLIP, the output with spatial information ${z}$ is dumped and $\overline{{z}}$ is used to match with the text embedding. 
However, as illustrated in DenseCLIP, since the MHSA is symmetric to each input element, ${z}$ may behave similarly 
to $\overline{{z}}$, which aligns well with the text embedding. 
Therefore, we adopt both ${z}$ and $\overline{{z}}$ to generate our grid-level and image-level embeddings respectively with a few adaptation layers.

% To obtain the grid-level embedding ${z}'$, we perform 3 convolutional layers with ReLU activation on ${z}$. 


% However, unlike DenseCLIP that uses significantly more
% information in model training including learnable prompt and
% context-aware text embeddings, we 
% % wish to
% preserve the original CLIP
% representational space~\cite{gu2021open, zhou2022detecting} for
% scalability and simplicity. 


\noindent \textbf{From Label to Text Embedding.}
% At test time the learned text encoder synthesizes a zero-shot linear classifier by embedding the names of the target dataset categories.
In the original CLIP,
to create a dataset classifier from label text,
a set of template-based prompts 
like ``a photo of a \{\emph{object}\}.'' are applied, 
where \emph{object} is any of the target category names. 
Then the multiple prompts for a single label are aggregated.
Although there are several learnable prompting methods for CLIP~\cite{zhou2021learning} that fine-tune with the downstream tasks, 
We follow the template-based  one for simplicity and scalability.
We use one template-based  variant in ViLD~\cite{gu2021open} designed for object detection.
Here we note the final text embeddings of the categories in the target dataset ( base categories) as 
$\left\{T_{k}\right\}_{k=1}^{K}$, where $K$ is the number of category.

% Then given an image, one can use CLIP to compute the similarities between the image and the text prompts in the embedding space and the category with the highest score is regarded as the final prediction.

% Therefore, the distance (relationship) between different labels learned from the large scale datasets of CLIP can not be exploited, which can be useful for better generalization ability. 

% To be specific, in the ViLD-text version of ViLD, the learnable classifier is replaced with the CLIP text embeddings. As for the CLIP text embeddings, we follow the original way of CLIP as well as in ViLD. We generate the text embeddings offline by feeding the category texts with prompt templates, e.g., ``a photo of \{category\} in the scene'', into the CLIP text encoder. 

% Different from DenseCLIP, our way of obtaining text features is different. DenseCLIP uses learnable textual contexts inspired by CoOp~\cite{zhou2021learning} as a baseline,  and further uses the contextual information from the image to prompt the language model. However, we argue that it can cause overfitting to the target dataset, which would not be able to exploit the original alignment space of the CLIP pretrained model. 

% \noindent \textbf{Alignment Score.} The similarity of the image and text embeddings is calculated b


\subsection{Grid-level Alignment}
\noindent \textbf{Generating Grid-level Image Embedding.} Taking ResNet50 encoder as an example, in FCOS~\cite{tian2019fcos}, the output feature map {${C}_{3}$, ${C}_{4}$, ${C}_{5}$} of ResNet50 are inputted into FPN, 
producing $5$ multi-scale image feature maps $\left\{{P}_{i}\right\}_{i=3}^{7}$. In FPN, ${C}_{5}$ is fused with ${C}_{3}$, ${C}_{4}$ to produce ${P}_{3}$, ${P}_{4}$ as well as serves as the input of ${P}_{6}$, ${P}_{7}$.
Therefore, we fuse ${z'}$ into ${C}_{5}$ to spread the image embeddings suitable for the text alignment to different scale image feature maps.

To be specific, we first apply three consecutive 3$\times$3 convolutional layers with ReLU activation function to adapt the MHSA grid-level output feature $z$, reducing the number of channels from 1024 to 256 to generate $z'$, and concatenate it with ${C}_{5}$.
Then the concatenated feature $\left[{{z'}}, {C}_{5}\right]$ replaces ${C}_{5}$ and is fed into the FPN with a little modification in the input channel number. 
In this way, the FPN is able to produce the multi-scale feature maps $\left\{{P}_{i}\right\}_{i=3}^{7}$ inheriting from the CLIP image embeddings that can be aligned to the text embedding as formulated below,
\begin{eqnarray}
   \left\{{P}_{i}\right\}_{i=3}^{7}={\rm{FPN}} ({C}_{3}, {C}_{4}, \left[{{z'}}, {C}_{5}\right]).
\end{eqnarray}

The FPN output feature $\left\{{P}_{i}\right\}_{i=3}^{7}$ are then used to generate the final multi-scale grid-level features $\left\{{G}_{i}\right\}_{i=3}^{7}$
by going through the FCOS classification head.
In the original FCOS,  the classification head contains 5 convolutional layers and the last layer outputs the features with the channel number equal to the category number.
While in GridCLIP, 
We instead modify the output channel to be equal to the dimension of the CLIP text embeddings, to generate $\left\{{G}_{i}\right\}_{i=3}^{7}$.
Then for each scale, 
    the output feature map calculates the cosine similarities 
    with each text embedding (each category) 
    in pixel level 
    corresponding to grids in the original image,
    to produce the multi-scale grid-level score with the Sigmoid activation function.
For any grid (pixel) $j$ in the $i$-th scale grid-level feature $G_i(j)$, the matching score over all categories can be formulated as below,
 \begin{eqnarray}
   S_i(j) = \left\{  \dfrac{ G_i(j)\cdot T_k}{{\left\| G_i(j)\right\| _{2}\left\| T_k\right\| _{2}} }\right\}_{k=1}^{K}.
\end{eqnarray}

\noindent Finally, the grid-level score  $\left\{{S}_{i}\right\}_{i=3}^{7}$ is treated as the original classification output and aligned to the ground-truth target  $\left\{{Target}_{i}\right\}_{i=3}^{7}$  using Focal Loss~\cite{lin2017focal} as in the original FCOS. 
% So the grid-level alignment loss is,
% \begin{eqnarray}
%   L_{ \rm grid} = {\rm FocalLoss} (  \left\{{S}_{i}\right\}_{i=3}^{7} , \left\{{Target}_{i}\right\}_{i=3}^{7} ).
% \end{eqnarray}


\subsection{Image-level Alignment}
% we show how we apply CLIP to reduce the overfitting to the target detection dataset when purely applying grid-level alignment, which is named as image-level alignment.
With grid-level alignment, the image 
% regions  
 grids
of the  base categories are mapped to the CLIP alignment space, 
by aligning their embeddings to the corresponding text embeddings $\left\{T_{k}\right\}_{k=1}^{K}$.
While for 
 grids of  novel categories, 
    there are no corresponding text embeddings to align to,
    which can only learn their embeddings by minimizing their similarity to any of $\left\{T_{k}\right\}_{k=1}^{K}$ during training.
However, since the embeddings of different  novel categories 
    can have different similarities to each  base category, 
    simply minimizing the similarities
    between the  base and  novel categories
    is not consistent with the CLIP representation space 
    which presents a generalizable knowledge representation.
    % and 
    % the distance between different categories is informative.
Therefore, ignoring the alignment of  novel categories
    may limit the ability to encode a wide range of  novel visual concepts, which
    harms the generalization ability of the model.
% One possible situation is that, it is likely that the unknown categories not existing in the training set
%     may be mistakenly classified as any known category,
%     increasing the False Positive prediction during assessment.

In practice, inspired by ViLD~\cite{gu2021open}, 
    we align the image-level embedding ${\overline{z}}$
    to the embedding ${\overline{z}_{ \rm CLIP}}$ produced by a fixed CLIP image encoder, 
    so that the regions of  novel categories in an image can also
    be projected to the CLIP alignment space.
Different from ViLD that 
    aligns the embedding of several proposed regions in an image
     provided by a separate region proposal network (RPN), hence two-stage,
    which is the source of significant extra computational costs due to
    its requirement for multiple inferences of the image encoder for each
    object proposal, we directly align the embedding of the whole
    image ${\overline{z}'}$  without the need for multiple passes.
Specifically, 
    similar to  grid-level alignment, we generate the image-level embedding $\overline{{z}'}$
    from $\overline{{z}}$ going through three consecutive linear layers with the ReLU activation function.
    Then we maximize the $L_1$ similarity between ${\overline{z}}$ and ${\overline{z}_{ \rm CLIP}}$ , the reverse of which is the image-level alignment loss  $L_{ \rm image}$.
%     Therefore, the image-level alignment loss is as follows,
% \begin{eqnarray}
%   L_{ \rm image} = { \rm L1Loss} (  {\overline{z}'},  {\overline{z}_{\rm  CLIP}} ).
% \end{eqnarray}
As for the fixed CLIP image encoder, we evaluate different published versions of pretrained models
    and choose the ViT-B/32 version for alignment. 
    % since it provides the top-k categories that are consistent with more objects in an image,
    % comparing to other pretrained models in the classification task.
    % Refer to the experimental analysis in Section~\ref{evalImgCLIP} for more details.
Note that image-level alignment is only performed during the training phase.
    % so the model size and the inference cost of GridCLIP are still comparable to FCOS.  
    % wrong!  20M more params, 


Finally, the total loss for training GridCLIP end-to-end is,
\begin{eqnarray}
  L = w_{ \rm grid} L_{ \rm grid} + w_{ \rm image} L_{ \rm image} + L_{ \rm R} + L_{ \rm C}, 
\end{eqnarray}
\noindent which includes the loss of two alignments as well as the original loss in the one-stage detector FCOS: Regression loss $L_{ \rm R}$ for bounding boxes  and  centerness loss $L_{ \rm C}$ indicating the distance of a pixel to the center of the bounding box.
% Though unknown categories can be not directly align, 
%     but their similarities to several known categories can 
%     in a certain extent indicate their position in the CLIP alignment space.
    
% we examine two ways to align ${\overline{z}}$ and ${\overline{z}_{gt}}$. 
% The other way is similar to the one in grid-level alignment, which computes the similarity score     between ${\overline{z}}$ 
%     and the text embeddings $\left\{T_{k}\right\}_{k=1}^{K}$. Our experiments show that the latter has better performance. This indicates the two embeddings do not need to be the same, looser restriction can still learn proper aligned embeddings of unknown categories.

% Our experimental results also show the effectiveness of image-level alignment 
%     in aligning the regions of unknown categories to the CLIP space.
    
% As an image always contains more than one category, 
%     the embeddings of the whole image ${\overline{z}}$ 
%     can be seen as 
%     the combination of different known and unknown categories.

% However, as CLIP is pretrained with broad natural language supervision and therefore has the ability to preform zero-shot classification, we propose to use another fixed CLIP image encoder to help retain the more generalizable image feature for calculating the score map. 

% More specifically, we first use ${z}$ to calculate the image-level classification score ${s} \in \mathbf{R}^{1 \times cls}$ (${cls}$ notes the number of categories in the target dataset) and then use the other fixed CLIP to also calculate the score  ${\tilde{s}}$. We then minimize the similarity of these two scores using a binary cross-entropy loss. In this way, we are able to maintain the image feature from CLIP and to covert image-level alignment to the grid-level one without damaging the generalization ability.



% \subfile{sections/4Experiments}

\section{Experiments}

\input{tables/sota_lvis_open.tex}
\input{tables/resource.tex}
\input{tables/transfer_lvis_open.tex}
\subsection{Implementational Details}

% to get image emebdding, for architectures like ViT, ${z}$ can be obtained similarly. 



GridCLIP uses the one-stage detector FCOS~\cite{tian2019fcos} as the detector, which can be replaced by other one-stage detectors like RetinaNet~\cite{lin2017focal} or ATSS~\cite{Zhang_2020_CVPR}.
The backbone uses the RN50 pretrained CLIP image encoder, which has two more convolutional layers than the original ResNet50~\cite{he2016deep} in the stem module 
    and a Multi-Head Self-Attention (MHSA) layer performing on the output of the 5th stage.
The adapting layers performed on the output features of MHSA use the embedding dimension of 256.
For image-level alignment, GridCLIP uses the ViT-B/32 version of CLIP.  
The weights of the two alignment losses are: $w_{ \rm grid}$=1, $w_{ \rm image}$=10.
Our implementation is based on the MMDetection framework~\cite{chen2019mmdetection}.

We conduct experiments on the detection benchmark 
% COCO~\cite{lin2014microsoft} and 
 LVIS v1.0~\cite{gupta2019lvis}.
% and also use PASCAL VOC~\cite{everingham2010pascal}  to verify transfer ability.
% COCO containing 118K training images (train2017), 5K validation images (2017 val) and 20K test images (2017 test dev). 
LVIS v1.0 is a long-tail detection dataset containing 1203 categories.
 The categories are divided into three parts by how many images
they appear in: rare (1-10), common (11-100), and
frequent ($>$100), respectively including 337, 461 and 405 categories, with corresponding metrics AP$_r$,  AP$_c$ and AP$_f$.
Following the ViLD~\cite{gu2021open}, we use frequent and common categories as the base categories and rare categories as the novel categories 
for open-set detection. For close-set detection, we use the common and frequent categories.
When comparing with other SOTA approaches, we adopt multi-scale training similar to ViLD and random cropping augmentation. 
For the training process,
     GridCLIP is trained for a 2$\times$ (24 LVIS epochs) schedule with a batch size of 16.
During the inference stage, the maximum number of detection objects per image is 300,  and the threshold of the classification score is set to 0.05.
The IOU threshold of NMS is 0.5. 
Refer to the supplementary materials for more details. 

\subsection{Comparison with the State-of-the-Art}\label{sec_sota}



We compare GridCLIP on the LVIS v1.0 validation set with other methods with comparable backbone, including ViLD~\cite{gu2021open}, RegionCLIP~\cite{zhong2021regionclip}, Detic~\cite{zhou2022detecting},  DetPro~\cite{du2022learning} and PromptDet~\cite{feng2022promptdet} in Table~\ref{sota_lvis_open}.
\jy{These methods use template-based prompts, 
except that DetPro~\cite{du2022learning} and PromptDet~\cite{feng2022promptdet} use learnable prompts.}
Also, we do not compare to methods like GLIP~\cite{li2022grounded} and DetCLIP~\cite{zhang2022glipv2} which use large-scale annotation data, since we focus on utilizing limited annotation data for the detection of broader categories.
% The other two one-stage CLIP-based OVOD detectors HierKD~\cite{ma2022open} and ZSD-YOLO~\cite{xie2021zsd} have not trained on LVIS but the more class-balanced COCO~\cite{lin2014microsoft} with much fewer number of categories, which are also not listed in the table.
 

 \noindent \textbf{Performance Comparison.}
A totally fair comparison is not realistic, since external datasets or learnable prompts are widely used in most OVOD methods.
Therefore, we find it relatively fair to compare GridCLIP with ViLD~\cite{gu2021open}
which only utilizes the knowledge of CLIP and the detection dataset without learnable prompts. 
We observe that GridCLIP surpasses ViLD in overall AP by 0.8. 
As a one-stage detector, 
GridCLIP closes the gap to the two-stage detector ViLD in novel categories to 1.3 AP$_{r}$, 
    while the current SOTA one-stage detector HierKD is still 8.2 AP behind ViLD on the COCO validation dataset.
Furthermore, GridCLIP outperforms ViLD by 1.5 AP$_{c}$ and 0.9 AP$_{f}$.
Besides ViT-B/32, we also use the RN50$\times$64 version (the largest model of CLIP under ResNet architecture) of CLIP for image-level alignment
    to explore the upper bound of the ResNet version. We observe that the RN50$\times$64 version has a worse generalization ability to novel categories compared to the ViT-B/32 one,
    with lower AP$_{r}$ while higher  AP$_{c}$ and AP$_{f}$.  
\jy{We further observe the obvious gap between the base and novel categories in GridCLIP, 
and try to understand and explain the gap based on the analysis from another one-stage detector HierKD~\cite{ma2022open}.
In ViLD, 
    both the novel and base categories \jy{use} the region-level alignment.
In GridCLIP,
    \sgg{whilst} novel categories \sgg{use} the more coarse-grained image-level alignment, 
    \sgg{the} base categories \sgg{in GridCLIP} use \sgg{{\em both}}
    the more fine-grained grid-level alignment \sgg{and the}
    image-level alignment. \sgg{This} makes the gap between base and novel categories larger than that of ViLD. 
To verify this, we can replace image-level alignment with region-level alignment similar to ViLD to further improve AP$_{r}$, 
    which however may require more training time as other two-stage detectors do. We leave it for future research. }

  

\input{tables/abla_lvis_close.tex}

\input{figures/abla_imbala.tex}

\input{tables/abla_lvis_open.tex}
 \noindent \textbf{Resources Comparison.}
We compare GridCLIP with ViLD on training and test time, 
as well as the model size. 
ViLD is originally trained on TPUv3 with a batch size of 256. 
For fair comparison and due to resource limitation,
we train both ViLD and GridCLIP with a batch size of 16 on 2 A100 GPUs. 
We train ViLD using the implementation of DetPro~\cite{du2022learning}.
Moreover, ViLD takes 1 day on 8 V100 GPUs to pre-compute the
 CLIP image embedding of regions to accelerate training. \sgg{GridCLIP
   does not require this.}
In Table~\ref{resource}, we show that with comparable model size,
GridCLIP-R50 is approximately \sgg{43$\times$ and 5$\times$} faster than ViLD in
\sgg{training and test respectively}. 
Such significant advantages
  remain when GridCLIP-R50-RN using RN50$\times$64 (3$\times$ larger in both input and parameters) for image-level alignment, with 34$\times$ 
  faster in training time than that of ViLD. 
This validates
  clearly the compute efficiency of the one-stage GridCLIP.
  % without
  % compromising model accuracy and generalisation capacity,
  % the key objective of this work.



 \noindent  \textbf{Transfer to Other Datasets.}
To further explore the generalizability of GridCLIP, we follow ViLD~\cite{gu2021open} and evaluate the LVIS-trained GridCLIP on \sgg{both the} PASCAL VOC 2007 test set~\cite{everingham2010pascal} and the COCO validation set~\cite{lin2014microsoft} by directly replacing the categories without any finetuning.
Note that there are overlaps of both category and image between LVIS and COCO (as well as PASCAL VOC). 
The IOU threshold
of NMS is 0.6. %~\ref{sec_sota}. 
On PASCAL VOC, we observe that
the gap between GridCLIP-R50 and ViLD is 
 1.2 to 1.3 AP, 
and GridCLIP-R50-RN is comparable with ViLD on PASCAL VOC 
with no more than \jl{1 AP difference}.
Although the gap on COCO is still obvious with 2.2 AP falling behind but is comparable to that of DetPro which uses learn prompts based on ViLD.
Therefore, in the generalization ability, GridCLIP performs quite close to its two-stage counterparts.

% \sgg{By using the backbone of R101, GridCLIP is comparable} with ViLD on both datasets 
% \sgg{but with a significant advantage in both
%   training (34x) and inference (5$\times$) speed. This demonstrates the generalization
%   ability of GridCLIP across different datasets.}



\subsection{Ablation Studies}
We verify the effectiveness of grid-level and image-level alignment for undersampled categories 
for both close-set detection (Table~\ref{abla_lvis_close}) 
    and open-set detection (Table~\ref{abla_lvis_open}). 
We follow the same settings in Sec.~\ref{sec_sota} except 
    that multi-scaling training and random cropping augmentation
    are not used here.
\jy{Among the experiments,
``w\/o align'' denotes using the original design of FCOS only with different backbones that feed different image features to the FPN. ``w grid-align'' and ``w image-align'' denote only using grid-level or image-level alignment respectively.}


\input{figures/clipHit.tex}

 \noindent \textbf{Close-set Detection.}
We first evaluate other visual pretrained models as the backbone to compare to the vision-language pretrained model CLIP, 
including the ImageNet~\cite{deng2009imagenet} pretrained ResNet50 on the classification task 
and self-supervised pretrained ResNet50 using visual-only SSL method SwAV~\cite{caron2020unsupervised} pretrained on large-scale unlabeled images.
% \noindent \textbf{Overall performance.}
By comparing the methods using different pretrained ResNet50 without any alignment (the top section of Table~\ref{abla_lvis_close}), we notice that using the CLIP pretrained ResNet50 can bring notable improvements compared to the ImageNet and SwAV pretrained ones, with the similar architecture, which indicates the superiority of the vision-language pretrained model CLIP than other visual pretrained models in generalizing better image embeddings for detection.

On the bottom section of Table~\ref{abla_lvis_close}, we introduce the MHSA layer whose output is aligned to the CLIP text encoder in the original training of CLIP. Therefore, the MHSA layer provides both grid-level and image-level features for CLIP-based alignment.
We first observe that introducing the MHSA layer without any alignment drops the overall performance by 0.7 AP,
while using both alignments can improve the performance in the infrequent common categories by 1.3 AP and preserves the performance in frequent categories.
Among the experiments that use the MHSA layer,
% As for the effectiveness of each alignment (the bottom section of Table~\ref{abla_lvis_close}), 
we find that applying grid-level alignment can significantly improve the common categories by 2.3 AP, while using
  image-level alignment seems limited on the metrics AP$_{c}$ and AP$_{f}$.
Using both alignments can bring 1.8 AP$_{c}$ and 0.3 AP$_{f}$ improvements which are lower than the one using only grid-level alignment.
We further explore the reasons behind that.
 We find that the metric of AP$_{c}$ and AP$_{f}$ are too coarse-grained for 
  distinguishing categories with different frequencies.
So we further observe the performance in a more fine-grained way by the plot of AP over categories with different sample numbers (Figure~\ref{abla_imbala}).
As shown in subfigure (a), in the 200 most infrequent categories, the improvement of applying one alignment is not stable, where the AP can be notably higher than ``GridCLIP-R50 w/o align'' in some categories while obviously worse in other categories.
By applying both alignments,
``GridCLIP-R50'' primarily ranks top in the 100 most infrequent categories, while 
its superiority is not obvious in frequent categories. 
 Therefore, we can conclude that using both alignments benefits the undersampled categories in close-set detection. 
% Purely applying the grid-level alignment brings nearly no benefits, while applying the image-level alignment benefits the performance on detecting large objects. This is consistent with the fact that large objects occupy the most area of images and thus benefits more compared to small or medium objects. 
% By applying both alignments, the model achieves better performance than the one without alignment in every metric.
% However, the benefits of alignments in not that significant in the COCO dataset.


 \noindent \textbf{Open-set Detection.}
% We train the models in the common and frequent categories in LVIS v1.0, and test on all 1203 categories.
Since only models with grid-level alignment can be extended for open-set detection by extending the categories embedding list with novel categories and using the list match with the grid-level image embeddings, we compare two models from Table~\ref{abla_lvis_close}.
As shown in Table~\ref{abla_lvis_open}, 
    image-level alignment improves novel categories by \jl{2.6} AP$_{r}$, while preserving the performance on base categories. 
Also, as shown in  Figure~\ref{abla_imbala} (b), the performance rises over categories as their training sample number increases, which verifies that undersampled categories suffer from long-tail distribution. 
In the 200 most infrequent categories which are novel (also rare) categories,  ``GridCLIP-R50'' outperforms ``GridCLIP-R50 w grid-align'' significantly, which indicates the effectiveness of  image-level alignment on novel categories. Therefore, it is verified the alignment of image-level representations also helps learn generalizable grid-level representations of undersampled categories.

    In summary, grid-level alignment improves the performance notably on undersampled categories in close-set detection and allows the detector to be extended for open-set detection, while image-level alignment obviously benefits the novel categories in open-set detection.
    Using both alignments enables a one-stage detector to detect novel categories and mitigate the deterioration of undersampled categories in long-tail datasets.


% \noindent \textbf{The effectiveness of grid-level alignment.}
% We compare the performance with and without grid-level alignment (Table~\ref{abla}). 
% When performing grid-level alignment, we perform two variants. One does not use CLIP text embeddings directly generate the score map for FPN as well as the auxiliary loss. 
% The other uses the CLIP text embeddings to perform matching with the image feature $\overline{{z}}$ in the same way as the one in CLIP but in grid level.
% The results show that grid-level alignment with CLIP text embeddings can perform better than Baseline. 
% While directly generating the score map to align to the ground truth labels as auxiliary may not get better performance.

% \noindent \textbf{The effectiveness of image-level alignment.}
% Based on grid-level alignment, we perform image-level alignment by aligning to the output image-level feature of another fixed CLIP image encoder. We observe that though there is no improvement in the overall performance, while the performance on large object gains notable improvement (Table~\ref{abla}).

% \noindent \textbf{Generalization ability on PASCAL VOC.}
% We evaluate the generalization ability of the above methods by applying the COCO trained models to PASCAL VOC~\cite{everingham2010pascal}. 
% Since the categories of COCO totally cover PASCAL VOC, we directly calculate the mean accuracy of each category in PASCAL VOC, and skip other categories. 

% We observe that the SwAV pretrained ResNet50 can transfer better than ImageNet pretrained one and CLIP pretrained ResNet50 without MHSA.
% Furthermore, we find that purely applying the image-level alignment or the grid-level alignment can not help improve the ability for the model to new dataset.
% While with both alignments, we observe $0.3$ AP improvements, which indicates that aligning to CLIP on  base} and  novel categories helps the model generalize better to new datasets.


% \noindent \textbf{Performance on long-tail dataset.}
% Since COCO is a relatively balanced dataset that 76 categories out of 80 categories appears in $0.8\%$ to $10.8\%$ of the training images by our calculation,
% the ability of alignment in handling long-tail dataset is not obviously shown. 
% We further evaluate the effectiveness of the alignment in the long-tail dataset.
% To this end, we train ``GridCLIP-R50 w/o align'' and ``GridCLIP-R50'' on LVIS v1.0~\cite{gupta2019lvis}. 

% We carry out the training procedure in the same way as in COCO, except adding a longer schedule of 24 epochs with the learning rate multiplied by $0.1$ in epoch 16 and 22. During the inference stage, the maximum number of detection per image is 300.

% The results show that, although ``GridCLIP-R50'' gets worse overall AP, it outperforms its counterpart without alignment on rare categories by $0.4$ AP and $1.8$ AP with the training schedule of 12 and 24 epochs respectively.
% This indicates that the alignment to CLIP representation space benefits the categories with less training data and has the potential to deal with long-tail dataset.



\subsection{Further Analysis: Evaluating CLIP Image-Level Representation for Object Detection}\label{evalImgCLIP}
We analyse how accurately the multiple categories in an image can be represented by the original CLIP image encoder.
This substantially affects the performance of a one-stage detector built upon the CLIP image-level representation
to detect multiple categories at the same time, which indicates how much image-level alignment can benefit GridCLIP.
We evaluate several pretrained versions of CLIP on LVIS v1.0 validation set, including the refined ResNet~\cite{he2016deep} (RN50, RN101, RN50$\times$64) and those with the Transformer architecture~\cite{dosovitskiy2020image} (ViT-B/32).
Specifically, We calculate the recall of categories by using the original CLIP image-level representation to match the text representation of each category (Figure~\ref{clipHit}).
% We first \sgg{computed} the number of categories in each image.
% Figure~\ref{clipHit} (a) shows that nearly all the images contain no
% more than 12 categories. 

For RC@10, all models perform poorly with no more than 2 recalls.
While for RC@100, we find that ViT-B/32 can recall 50\% of the categories and RN50$\times$64 can recall more than 30\% of the categories. In comparison, the other ResNet-based models perform poorly that reach less than 20\% recall rate.
Furthermore, for RC@300, nearly 75\% of the categories in an image are captured by ViT-B/32, and RN50$\times$64 reaches about 60\% recall rate.
Given that the maximum detection number for LVIS v1.0 in OVOD is usually set to 300 (objects), ViT-B/32 can at most help detect 75\% of the objects if all the objects have different categories and 50\% if every 3 of the objects share the same category.
This provides substantial knowledge of categories to help the detector build the representation for multiple object detection.
Therefore, the CLIP image encoder is able to capture multiple categories in an image at the same time with relatively high accuracy and provide substantial knowledge of categories for the detector during image-level alignment.
% and the transformer architecture outperforms ResNet ones obviously.


% In our experiment, we demonstrates the reason for choosing RN50$\times$64 for image-level alignment.




\section{Conclusion}
In this work, 
we introduce a one-stage detector GridCLIP,
    which exploits the CLIP representation space to supplement the knowledge on undersampled categories in downstream detection datasets.
GridCLIP optimizes generalizable knowledge from the CLIP
  pretrained representation to more fine-grained localized mapping,
 by simultaneously
learning a localized grid-level CLIP
mapping to  base categories (Grid-level Alignment) and a holistic
image-level knowledge distillation to  base and novel
% (may known on the CLIP training set) 
categories in a target
object detection domain (Image-level Alignment).
In our experiments, we verify that GridCLIP 
suffers less from long-tail distributions with the help of both grid-level and image-level alignments,
reaching comparable performance 
on the LVIS v1.0 benchmark \jy{with higher training and inference speed.}
% GridCLIP 
% demonstrates an effective approach 
% to explore more optimal foundation model representations 
% from vision-language pretrained models 
% for object detection tasks.



{\small
\bibliographystyle{ieee_fullname}
\bibliography{GridCLIP}
}



\end{document}