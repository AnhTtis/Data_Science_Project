\begin{figure}[ht]
   \centering
   % \vspace{3pt}
   \includegraphics[width=7.2cm]{Figure/intro.pdf}
   \caption{ Comparison of approaches to applying visual pretrained and vision-language pretrained models (GridCLIP as an example) 
   % for the downstream object detection task. 
   to one-stage detectors.
   The visual pretrained model is commonly used for extracting  grid-level image representations (also called feature maps) which are aligned to the manual one-hot embeddings, so only base categories can be learned (top). While in a vision-language pretrained model, images are encoded into high-dimension embeddings, which can be aligned to the text embeddings of base categories as well as the whole image embedding of both base and novel categories (bottom). 
   % The latter can have better generalization ability since both base and novel categories are aligned into a more implemented and generalizable embedding space. 
   \label{intro}}
\end{figure}