
\setlength{\tabcolsep}{8pt}
\begin{table*}[]
    % \renewcommand\arraystretch{1.1}
   \centering
  % \scriptsize
   \footnotesize
    % \tiny
    % \small
   \caption{Comparison with different object detectors on LVIS v1.0~\cite{gupta2019lvis}  with open-set settings. Multi-scale training is used. ``CLIP on cropped regions'' directly applies CLIP to classify cropped region proposals. 
   \jy{Except for GridCLIP, all detectors use an RPN pretrained on base categories to get region proposals.
   $\ddagger$ denotes using mask annotations.
   $\dagger$ denotes mask AP. 
   $\nabla$ denotes using learnable prompts instead of template prompts.
   $\star$ denotes that RegionCLIP use extra pretraining process of 600k iter on CC3M dataset with batch size of 96. 
   }
   % denotes using external datasets.
   % RC: randomly crop augmentation. 
   % \textcolor{red} {Red }text denotes the best performance and \textcolor{blue} {blue} text denotes the second best performance.
}\label{sota_lvis_open}
\begin{tabular}{l|cccc|c|cc|c}
\toprule
 % Model   & backbone   & pretrained CLIP   & detector & epochs  & {\begin{tabular}[c]{@{}c@{}}external\\ dataset\end{tabular}}    & learnable & AP$_{r}$ & AP$_{c}$ & AP$_{f}$ & AP   \\ \midrule
 Model   & backbone   & pretrained CLIP    & 
 epochs & 
 % {\begin{tabular}[c] {@{}c@{}}mask\\ annotation\end{tabular}}  & 
 {\begin{tabular}[c]{@{}c@{}}external\\ dataset\end{tabular}}    & AP$_{r}$ & AP$_{c}$ & AP$_{f}$ & AP   \\ \hline
% \midrule
{CLIP on cropped regions}$\ddagger$~\cite{gu2021open} & {R50-FPN  } & {ViT-B/32} & 0      &    { }   &   \textbf{19.5}     & 19.7     & 17.0  & 18.6 \\
% ViLD-text$\ddagger$~\cite{gu2021open}	     & {R50-FPN  } & {ViT-B/32} & {384}  &    { } &    10.6	 &26.1	&\textbf{37.4}	&27.9\\
ViLD$\ddagger$~\cite{gu2021open}   & {R50-FPN  } & {ViT-B/32} & {384}  &    { } &   16.3     & 21.2     & 31.6     & 24.4 \\ 
RegionCLIP~\cite{zhong2021regionclip} & CLIP R50-C4 & RN50  & 12$\star$ & CC3M &  17.1     & \textbf{27.4}     & \textbf{34.0}     & \textbf{28.2} \\
Detic$\ddagger$$\dagger$~\cite{zhou2022detecting} & R50-FPN  & ViT-B/32     & 384    & ImageNet-21K & 17.8    &  \underline{26.3}    & 31.6     & \underline{26.8} \\  
% DetPro$\nabla$~\cite{du2022learning} & SoCo R50-FPN     & ViT-B/32  & 6+20   &   &  \textbf{20.8}     & \textbf{27.8}     & 32.4     & \textbf{28.4} \\ 
PromptDet$\nabla$$\dagger$~\cite{feng2022promptdet}  & R50-FPN    & ViT-B/32 & 6+12 & LAION-novel  & \underline{19.0}& 18.5     & 25.8     & 21.4 \\
\hline

GridCLIP-R50   & {CLIP  R50-FPN}  & ViT-B/32 &   24  &    &15.0 &	22.7&	32.5&	25.2 \\
GridCLIP-R50-RN   & {CLIP  R50-FPN}  & RN50x64  &   24  &    &13.7	&23.3	&\underline{32.6}	&25.3 
% GridCLIP-R101 & {CLIP R101-FPN} &  & &   &15.1	&\underline{26.7}	&\textbf{35.7}	&\textbf{28.2}
\\

\bottomrule
\end{tabular}

\end{table*}

% DetPro~\cite{du2022learning} & SoCo R50-FPN     & ViT-B/32  & 6+20   &  & \checkmark    &  \textbf{20.8}     & \textbf{27.8}     & 32.4     & \textbf{28.4} \\ 
% PromptDet$\dagger$~\cite{feng2022promptdet}  & R50-FPN    & ViT-B/32 & Mask RCNN     & 6+12 & LAION-novel  & \checkmark & 19.0     & 18.5     & 25.8     & 21.4 \\
  % &    &     &     & 6+72 &    & & 21.4     & 23.3     & 29.3     & 25.3 \\ \midrule
% ViLD    &   R152-FPN & {ViT-B/32} & {384}  &    { }   & & 19.1&	22.4&	31.5&	25.4\\
% ViLD-Ens.  & &  &  & & & & 16.7     & 26.5     & 34.2     & 27.8 \\ 