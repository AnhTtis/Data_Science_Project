
Semantic-driven editing approaches, such as stroke-based scene editing~\cite{sdedit,editgan,wang2018high}, text-driven image synthesis and editing~\cite{diffusion,text2live,patashnik2021styleclip}, 
and attribute-based face editing~\cite{sun2022fenerf,conerf},
have greatly improved the ease of artistic creation.
However, despite the great success of 2D image editing and the neural rendering techniques~\cite{nerf,dellaert2020neural}, similar editing abilities in the 3D area are still limited:
\textbf{(1)} they require laborious annotation such as image masks~\cite{object_nerf,conerf} and mesh vertices~\cite{neumesh,nerf_editing} to achieve the desired manipulation;
\textbf{(2)} they conduct global style transfer~\cite{arf,chiang2022stylizing,chen2022upstnerf,huang2022stylizednerf,fan2022unified} while ignoring the semantic meaning of each object part (\eg, windows and tires of a vehicle should be textured differently);
\textbf{(3)}
they can edit on categories by learning a textured 3D latent representation (\eg, 3D-aware GANs with faces and cars \etc)~\cite{pi-GAN,pix2nerf,eg3d,sun2022fenerf,gu2021stylenerf,sun2022ide,niemeyer2021giraffe,schwarz2020graf},
or at a coarse level~\cite{edit_nerf,clip_nerf} with 
basic color assignment or object-level disentanglement~\cite{kobayashi2022decomposing}, but struggle to conduct texture editing on objects with photo-realistic textures or out-of-distribution characteristics.



Based on this observation, we believe that, on the way toward
semantic-driven 3D editing, the following properties should be ensured.
First, the operation of editing should be effortless, \ie, users can edit 3D scenes on a single 2D image in convenient ways, 
\ie, using off-the-shelf tools such as GAN-based editing~\cite{editgan,stylegan}, text-driven editing~\cite{text2live,diffusion}, Photoshop, or even a downloaded Internet image without pixel-wise alignment, rather than steering 3D modeling software with specific knowledge~\cite{neumesh}, or repeatedly editing from multi-view images.
Second, the editing method should be applicable to real-world scenes or objects and preserve vivid appearances, which is beyond existing 3D-aware generative models~\cite{pi-GAN,eg3d} due to the limited categories and insufficient data diversity on real-world objects.

To fulfill this goal, we propose a novel \textbf{S}emantic-driven \textbf{I}mage-based \textbf{E}diting approach for \textbf{N}eural radiance field on real-world scenes, named SINE.
Specifically, our method allows users to edit a neural radiance field with a
 single image, \ie, either by changing a rendered image using off-the-shelf image editing tools or providing an image for texture transferring (see Sec.~\ref{ssec:expr_tex_edit}),
and then delivers edited novel views with consistent semantic meaning.
Unlike previous works that directly finetune the existing NeRF model~\cite{clip_nerf,edit_nerf,kobayashi2022decomposing}, SINE learns a prior-guided editing field to encode geometry and texture changes over the original 3D scene (see Fig.~\ref{fig:framework}), thus enabling fine-grained editing ability.
By leveraging guidance from existing neural priors (shape prior models~\cite{dif} and Vision Transformer models~\cite{dino}, \etc), SINE can directly perform semantic-driven editing on photo-realistic scenes without pre-training a category-level latent space.
For example, in Fig.~\ref{fig:teaser}, users can stretch a car's back or change all four tires to cookies by only editing a single image, and can even cooperate with text-prompts editing~\cite{text2live} to modify a specific object of a scene with vivid appearances.


However, even when guided with neural priors, editing NeRF from a single image with multi-view consistency and accuracy is still challenging.
\textbf{(1)}
The generic NeRF does not necessarily provide an explicit surface or signed distance field, such that cannot directly work with shape priors~\cite{dif}.
Therefore, we propose to use cyclic constraints with a proxy mesh to represent the edited NeRF's geometry, which facilitates guided editing using coarse shape prior.
\textbf{(2)}
Learning a coordinate-based 3D editing field using a single edited view is not sufficient to capture fine-grained details, and applying semantic supervision~\cite{dino,clip} directly to the editing field leads to sub-optimal convergence (see Sec.~\ref{ssec:expr_edit_ablation}).
To tackle this challenge, we propose a color compositing mechanism by first rendering the template NeRF color and modification color individually, and then deferred blending them to yield the edited view, which significantly improves the semantic-driven texture editing.
\textbf{(3)}
Ideally, a user's editing should only affect the desired regions while maintaining other parts untouched.
However, in semantic-driven editing,
the prior losses require taking the full shape or image as input,
which leads to appearance or shape drifting at the undesired area.
To precisely control the editing while excluding irrelevant parts from being affected,
we generate feature clusters of the editing area using the ViT-based feature field~\cite{dino,kobayashi2022decomposing}, and use these clusters to distinguish whether a location is allowed to be edited or should remain unchanged.





In summary, the contributions of our paper are as follows.
\textbf{(1)}
We propose a novel semantic-driven image-based NeRF editing approach, called SINE, which allows users to edit a neural radiance field simply on just a single view of the rendering.
SINE leverages a prior-guided editing field to encode fine-grained geometry and texture changes over the given pre-trained NeRF,
thus delivering multi-view consistent edited views with high fidelity.
\textbf{(2)}
To achieve semantic editing functionality, we develop a series of techniques, 
including cyclic constraints with a proxy mesh for geometric editing,
the color compositing mechanism to enhance texture editing, and the feature-cluster-based regularization to control the affected editing area and maintain irrelevant parts unchanged.
\textbf{(3)}
Experiments and editing examples on both real-world/synthetic and object-centric/unbounded 360$^{\circ}$ scenes data demonstrate superior editing capabilities and quality with effortless operations.