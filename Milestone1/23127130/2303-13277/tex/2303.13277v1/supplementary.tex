\appendix





\renewcommand\thesection{\Alph{section}}
\renewcommand\thetable{\Alph{table}}
\renewcommand\thefigure{\Alph{figure}}

%%%%%%%%% TITLE - PLEASE UPDATE
\begin{strip}
\begin{center}
{\huge \bf Supplementary Material}
\end{center}
\end{strip}



\vspace{0.5em}

\renewcommand\thesection{\Alph{section}}
\renewcommand\thetable{\Alph{table}}
\renewcommand\thefigure{\Alph{figure}}

In this supplementary material, we describe more details of our method, including model architecture in Sec.~\ref{sec:model_arch}, dataset preparation in Sec.~\ref{sec:dataset_prepare}, and implementation details in Sec.~\ref{sec:impl}.
Besides, we also conduct more experiments in Sec.~\ref{sec:more_expr}.
More qualitative results can be found in our supplementary video, and the source code will be released upon the acceptance of this paper.

\section{Model Architecture}
\label{sec:model_arch}

We first explain the details of the model architecture.
Specifically, we adopt the multi-resolution voxel-hashing encoder by Müller \etal~\cite{muller2022instant} as the coordinate-based encoder, and build the template NeRF and the editing field in a decoupled manner.
The voxel-hashing encoder is constructed with 16 levels with 2-dimension features for each level.
For the template NeRF, we use the voxel-hashing encoder to encode the queries' coordinates and use spherical harmonics with 4 degrees to encode the ray direction.
The density and color heads for model output consist of 1 hidden layer with 128 hidden size and 2 hidden layers with 64 hidden size, respectively.
As introduced in Sec.~\textcolor{red}{3.1}, the editing field consists of a geometric modification field $F_{\Delta G}$ and a texture modification field $F_{\Delta T}$.
The geometric modification field $F_{\Delta G}$ and the corresponding forward modification field $F_{\Delta G}'$ are both constructed with an MLP of 1 hidden layer and 128 hidden size with the ReLU activation, and we adopt the positional encoding~\cite{nerf} (with 4 frequencies) to the all input query points.
The texture modification field $F_{\Delta T}$ is constructed with a voxel-hashing encoder (same size as the template NeRF), followed by an MLP of 1 hidden layer and 128 hidden size with ReLU activation.
During the dual volume rendering stage, we follow Mildenhall \etal~\cite{nerf} by using 64 coarse samples and 128 fine samples for each ray, and render the deformed template image $\hat{I}_o$ and color modification image $\hat{I}_{m}$ with the same density values.
Then, as explained in Sec.~\textcolor{red}{3.3}, we use a color compositing layer to obtain the edited view $\hat{I}$ by blending $\hat{I}_{m}$ into $\hat{I}_o$, where the color compositing layer is constructed using a compact UNet-like structure (with 2-layer encoder (3 $\rightarrow$ 16 $\rightarrow$ 32) and a symmetrical decoder, all layers comprise 3$\times$3 convolutions).
Besides, we can integrate temporal attribute~\cite{park2021nerfies,park2021hypernerf} (from 0 to 1) to the input of $F_{\Delta G}$ (with the positional encoding of 4 frequencies), and train the geometric editing on the edited transitions with temporal attributes as conditions, \eg, the dynamic motion effect shown in the supplementary video.

\begin{figure}[!t]
    \centering
    \includegraphics[width=1.0\linewidth, trim={0 0 0 0}, clip]{supp_figures/supp_hybrid_editing.pdf}
    \caption{
    We show examples of hybrid object editing by combining geometric and texture editing.
}
    \label{fig:supp_hybrid_editing}
\end{figure}
\section{Dataset Preparation}
\label{sec:dataset_prepare}

We evaluate SINE on both real-world/synthetic and object/scene datasets.
Specifically, for the real-world car datasets~\cite{CarWale}, each sequence contains 72 images with a car rotating on the turntable.
We use Colmap~\cite{schonberger2016structure} to recover camera poses w.r.t the cars' centers for all the images.
For the data of Photoshape~\cite{photoshape2018}, EditNeRF~\cite{edit_nerf} does not provide edited GT images,  so we regenerate all testing cases using Blender, which is more challenging than the original ones (\eg, we stretch the whole chair or enlarge holes, while EditNeRF~\cite{edit_nerf} only fills a tiny hole or removes legs).
For the data Blenderswap~\cite{blenderswap}, we render the scenes with Blender’s Cycle engine with realistic environment HDR maps. 
For users' 2D image editing, we use Adobe After Effect / Photoshop to deform images (geometric editing) and paint patterns (texture editing), and use EditGAN~\cite{editgan} and Text2LIVE~\cite{text2live} to edit images with semantic strokes or text-prompts.
For users' target images (\eg, cars and chairs) from the Internet, we remove their backgrounds using remove.bg~\cite{removebg} before conducting texture editing.

\begin{figure*}[!t]
    \centering
    \includegraphics[width=1.0\linewidth, trim={0 0 0 0}, clip]{supp_figures/supp_figure_direct_finetune.pdf}
    \caption{
    We compare our editing field with directly fine-tuning template NeRF.
}
    \label{fig:supp_direct_finetune}
\end{figure*}

\begin{figure*}[!t]
    \centering
    \includegraphics[width=1.0\linewidth, trim={0 0 0 0}, clip]{supp_figures/supp_geo_syncar.pdf}
    \caption{
    We show the quantitative comparison between our method and EG3D~\cite{eg3d} on the synthetic cars~\cite{blenderswap}, where the metrics of PSNR$\uparrow$ / SSIM$\uparrow$ / LPIPS$\downarrow$ are annotated on the above.
}
    \label{fig:supp_geo_syncar}
\end{figure*}

\section{Implementation Details}
\label{sec:impl}

\noindent\textbf{Training details.}
As introduced in the main paper, our method performs semantic-driven editing upon the given NeRF model.
Specifically, for each object or scene, we first train a generic template NeRF model.
Then, we learn geometric editing and texture editing with editing from a single perspective.
For geometric editing, since the geometric changes are sometimes combined with minor color changes, we also fine-tune the color modification field with a photometric loss (see the first term in Eq.~(\textcolor{red}{3})).
For texture editing, the texture transferring loss (Eq.(\textcolor{red}{6}))is defined on a complete image, which is not compatible with NeRF's sparse ray supervision.
Therefore, we adopt the deferred back-propagation technique from Zhang \etal~\cite{arf} for texture editing.
Practically, we first render the full-sized $\hat{I}_o$ and $\hat{I}_{m}$ and forward the color compositing layer, and then compute the losses to cache the complete image gradient w.r.t the $\hat{I}_m$ and the color compositing layer, and re-render the $\hat{I}$ and back-propagate the gradients to the $F_{\Delta T}$ and the color compositing layer in a patch-wise manner.
To make a smooth convergence, we take the coarse-to-fine regularization~\cite{park2021nerfies} on the color modification field by progressively increasing the frequency band of the input features during the training process.
Furthermore, we randomly perturb the pose to augment the data distribution and avoid the overfitting of the texture editing.
The whole training process of the template NeRF and our editing field takes about 12 hours on a single Nvidia RTX 3090 graphics card.

\noindent\textbf{Preparation of proxy mesh in geometric editing.}
As introduced in Sec.~\textcolor{red}{3.2}, we use a proxy mesh to represent NeRF's geometry during geometric editing.
In practice, we directly obtain the proxy mesh using off-the-shelf tools (\ie, implicit surface reconstruction method NeuS~\cite{neus}).
Since we optimize DIF~\cite{dif} latent code $\hat{\textbf{z}}$ and deform the proxy mesh $\hat{M}$ during the editing, the initial proxy mesh should be binding to a latent code beforehand.
Therefore, we obtain the initial latent code $\hat{\textbf{z}}$ to the corresponding initial proxy mesh $\hat{M}_\sigma$ in an auto-decoding manner~\cite{park2019deepsdf,dif} before training.

\noindent\textbf{Cycle loss in geometric editing.}
During the training of geometric editing, we additionally train a forward modification field $F_{\Delta G}'$ to map the template proxy mesh to the edited space.
The forward modification field $F_{\Delta G}'$ and the implicit geometric modification field $F_{\Delta G}$ are both supervised with an cycle loss~\cite{mihajlovic2021leap,neural_scene_flow}, which is defined as:
\begin{equation}
\begin{split}
\mathcal{L}_{\text{cycle}} = \frac{1}{M}\sum_{i =1}^M & ||F_{\Delta G} (F_{\Delta G}'(\mathbf{p}_i)) - \mathbf{p}_i|| + \\ 
&||F_{\Delta G}' (F_{\Delta G}(\mathbf{p}_i)) - \mathbf{p}_i||,
\end{split}
\end{equation}
where $\{\mathbf{p}_i|i=1,...,M\}$ is the uniformly point samples in 3D space, and we set $M=1000$ in our experiment.



\noindent\textbf{Feature-cluster-based semantic masking.}
As introduced in Sec.~\textcolor{red}{3.4}, we train a 3D feature field with DINO-ViT's feature maps, and generate feature clusters from the user-painted regions, which will be used to compute semantic masks to distinguish foreground editing areas and background areas.
Specifically, we first render the feature map under the specific editing view, and sample 1000 feature points on the user's painted region (which is directly accessible from the editing tools).
Then, we use K-Means to generate $K=15$ clusters from the sampled feature points.
During the training stage, we first render the current training view's feature map, and compute the L2-normalized pixel-wise feature distance (from 0 to 1) to the nearest clusters.
The pixels with distances smaller than 0.5 would be marked as foreground objects, and the others would be marked as background.
These computed editing masks would be used to regularize both geometric and texture editing (see Eq.~(\textcolor{red}{7})) to maintain the irrelevant content unchanged.

\noindent\textbf{User study.}
The questionnaire contains 17 cases, 8 for target-image-based editing (\eg, Fig.~\textcolor{red}{7} (a)) and 9 for text-prompts editing (\eg, Fig.~\textcolor{red}{7} (b)).
We show the participants a source image, a target image/text prompts, as well as the results produced by different methods. 
Participants are asked to select one result that best matches the style of the target image or the text meaning.

\begin{figure}[!t]
    \centering
    \includegraphics[width=1.0\linewidth, trim={0 0 0 0}, clip]{supp_figures/supp_geo_2dgan.pdf}
    \caption{
    We show the comparison of our method with EditGAN~\cite{editgan} on the real-world car dataset~\cite{CarWale}.
}
    \label{fig:supp_geo_2dgan}
    \vspace{1.0em}

\end{figure}

\begin{figure}[!t]
    \centering
    \includegraphics[width=1.0\linewidth, trim={0 0 0 0}, clip]{supp_figures/supp_compre_splice.pdf}
    \caption{
    We compare our method with SPLICE-ViT~\cite{splice} on the real-world cars~\cite{CarWale} and toys~\cite{dtu}.
    Since our editing method is built upon 3D-aware models, we consistently achieve better texture-transferring results than SPLICE-ViT when the source and target are observed from different perspective of views (\eg, cars) or with significant different shapes (\eg, plush toys).
}
    \label{fig:compare_2d_splice}
    \vspace{1.0em}
\end{figure}

\section{More Experiments}
\label{sec:more_expr}

\noindent\textbf{Hybrid editing with geometric and texture changes.}
We can combine geometric and texture editing on the same object by optimizing geometric-related losses and texture-transferring losses in turns.
As shown in Fig.~\ref{fig:supp_hybrid_editing}, we can edit objects' geometries while transferring textures with users' target images, \eg, the plush toy raises its hands and is painted in new textures from a yellow bear, and the airplane extends its wings and is painted golden.
Please refer to our supplementary video for a vivid animation of these effects.


\noindent\textbf{3D editing field vs. template NeRF fine-tuning.}
In this experiment, we compare our 3D editing field with na\"ive fine-tuning template NeRF (which is adopted by CLIP-NeRF~\cite{clip_nerf} and DFF~\cite{kobayashi2022decomposing}).
Editing NeRF with only a single image is fairly ambiguous without external supervision (\eg, semantic hints). 
For a fair comparison, we provide external supervision for the baseline method (vanilla NeRF).
Specifically, for texture editing, we enable NeRF's related network layers to be optimized and use the same texture editing losses.
Directly fine-tuning NeRF's color layers can change the objects' texture to some extent but cannot reach the same quality as our full model (\eg, uncovered cookie tires and snowy flowers in Fig.~\ref{fig:supp_direct_finetune} (b)).
For geometry editing, we fine-tune vanilla NeRF with $\mathcal{L}_{\text{gt}}$ %(\textcolor{red}{3}) 
and $\mathcal{L}_{\text{reg}}$ 
since it is not trivial to apply SDF-based shape priors to vanilla NeRF.
As demonstrated in Fig.~\ref{fig:supp_direct_finetune} (a), na\"ively fine-tuning NeRF on geometric editing would lead to the overfitting to a single view, and the multi-view consistency is no longer ensured (\eg, broken wings and green floaters in Fig.~\ref{fig:supp_direct_finetune} (a)).

\noindent\textbf{Quantitative comparison with EG3D on synthetic cars.}
We conduct quantitative comparisons with SOTA 3D-aware GAN method EG3D \cite{eg3d} on the synthetic car dataset. 
To obtain the ground-truth images of the edited results, we use Blender to render the training and testing views, and modify cars' geometry within the software.
As shown in Fig~\ref{fig:supp_geo_syncar}, our method achieves better rendering quality than EG3D on both visual quality and all the metrics (PSNR, SSIM, and LPIPS).
For example, we can preserve the specular effect even after the editing (\eg, specular area facing the light source and the reflection on the windshield), while EG3D struggles to produce photo-realistic results due to the limitation of its learned 3D latent representation.

\noindent\textbf{Comparison with 2D GANs.}
We compare our method against the SOTA 2D semantic editing method EditGAN\cite{editgan} on the real-world car dataset~\cite{CarWale}.
To make a fair comparison, we train our NeRF's backbone and EditGAN's style codes on all the multi-view images (\ie, each style code corresponds to one view).
Then, we perform semantic 2D editing on one single view using EditGAN.
For our method, we use the edited view to train our editing field.
And for EditGAN, we save the intermediate editing vector and add the editing vector to all the style codes, which yields multi-view edited images.
As demonstrated in Fig~\ref{fig:supp_geo_2dgan}, since EditGAN is agnostic to the 3D geometry, its results suffer from the inconsistent issue between different views, \eg, poor inversion results for the head and tail of the car, and the semantic editing result cannot be precisely applied to all views.
In contrast, our methods can synthesize cars with multi-view consistency and high-quality editing results.


\noindent\textbf{More ablation studies on geometric supervision.} As shown in Fig.~\ref{fig:ablation_geo_loss}, since ablating $\mathcal{L}_{\text{gt}}$ (Eq.~(\textcolor{red}{3})) makes it no supervision on editing, we split it into photometric (b) and silhouette (c) terms, and the absence of either will result in distorted or washed-out texture.
\textbf{(d)}: When ablating deformation reg. loss $\mathcal{L}_{\text{gr}}$ (Eq.~(\textcolor{red}{4})), the edited object is severely distorted (\eg, the letters are stretched).
\textbf{(e)}: The cycle loss $\mathcal{L}_{\text{cyc}}$ (Eq.~(\textcolor{red}{1}) in supp.) brings constraints from shape prior to geometric mod. field $F_{\Delta G}$, and ablating it would lose the efficacy of semantic guidance (\eg, the twisted airplane).
\begin{figure}[!t]
    \centering
    \includegraphics[width=0.95\linewidth, trim={0 0 0 0}, clip]{supp_figures/ablation_geo.pdf}
    \caption{
    We inspect the efficacy of different constraints in geometric editing.
    }
    \label{fig:ablation_geo_loss}
        \vspace{1.0em}

\end{figure}

\noindent\textbf{Ablation study of texture supervision.}
In Fig.~\ref{fig:ablation_tex_loss}, we disable texture transfer loss $\mathcal{L}_\text{tex}$ (Eq.~(\textcolor{red}{6})) and utilize photometric loss to paint target texture,
which leads to incomplete texture transferring results for invisible parts as shown below.
Besides, without texture prior supervision, we cannot transfer textures between objects with different shapes.

\begin{figure}[!t]
    \centering
    \includegraphics[width=0.95\linewidth, trim={0 0 0 0}, clip]{supp_figures/ablation_tex.pdf}
    \caption{
    We inspect the efficacy of the texture prior constraint in texture editing
    }
    \label{fig:ablation_tex_loss}
    \vspace{1.0em}

\end{figure}

\noindent\textbf{Comparison of texture editing with image-based SPLICE-ViT.}
Our texture transferring loss (Eq.~(\textcolor{red}{6})) is inspired from SPLICE-ViT~\cite{splice}, but fully leverages the multi-view training scheme.
Therefore, we compare our texture editing with image-based SPLICE-ViT in Fig.~\ref{fig:compare_2d_splice}.
As shown in Fig.~\ref{fig:compare_2d_splice}, SPLICE-ViT is sensitive to the perspective difference of the source and target images, which results in overfitting appearances on the edited view, \eg, horizontal straight patterns of cars when observing cars from a slightly tilted view, distorted faces of the plush toy.
By contrast, our method consistently achieves better texture-transferring results with color patterns properly aligned to the cars' geometries and the plush toy's body parts.


\begin{figure}[!t]
    \centering
    \includegraphics[width=1.0\linewidth, trim={0 0 0 0}, clip]{supp_figures/supp_compare_text2live.pdf}
    \caption{
    We compare our method with Text2LIVE on texture editing, where our method achieves better multi-view consistency. See the text for details.
}
    \label{fig:compare_text2live}
        \vspace{-1.0em}

\end{figure}


\noindent\textbf{Comparison of texture editing with Text2LIVE.}
As shown in Sec.~\textcolor{red}{4.4} from the main paper, since our method only requires one single-view image as editing input, we can naturally achieve text-prompt-based texture editing by cooperating with off-the-shelf text-driven editing methods (such as Text2LIVE~\cite{text2live}).
A follow-up question is, how does the Text2LIVE itself perform to the same 360$^{\circ}$ dataset in our texture editing task?
For video editing, Text2LIVE uses layered atlas~\cite{kasten2021layered} to convert objects and backgrounds into separated 2D layers.
However, in unbounded 360$^{\circ}$ dataset (\eg, pinecone and vasedeck~\cite{nerf}), there is no proper way to unwrap 3D objects and scenes into 2D layers (and we also failed to train layered atlas on these 360$^{\circ}$ datasets).
Therefore, we directly apply its converged editing generator to the multi-view images.
As shown in Fig.~\ref{fig:compare_text2live}, although Text2LIVE produces similar-looking edited images, it cannot maintain multi-view consistency when viewpoint changes (\eg, blurry edges at the golden pinecone, uncovered petals at the silver vasedeck, and the occasionally affected background).
On the contrary, our method naturally takes advantage of multi-view training and consistently delivers more plausible and realistic novel views.

\begin{figure}[!t]
    \centering
    \includegraphics[width=1.0\linewidth, trim={0 0 0 0}, clip]{supp_figures/supp_geo_meshquality.pdf}
    \caption{
    We show the robustness of geometric editing on proxy meshes with different qualities.
    The proxy meshes are jittered by adding gaussian noise with different variances (from $0.001^2$ to $0.01^2$).
}
    \label{fig:supp_geo_meshquality}
    \vspace{1.0em}
\end{figure}

\begin{figure*}[!t]
    \centering
    \includegraphics[width=1.0\linewidth, trim={0 0 0 0}, clip]{supp_figures/supp_tex_edit_compare.pdf}
    \caption{
    We show more comparison results of texture editing with ARF, CLIP-NeRF, and DFF on the real-world car~\cite{CarWale} and 360$^{\circ}$ scene dataset~\cite{nerf}.
    Our method consistently achieves more realistic and appealing editing results than the others.
}
    \label{fig:tex_edit_compare}
\end{figure*}

\begin{figure}[!t]
    \centering
    \includegraphics[width=0.95\linewidth, trim={0 0 0 0}, clip]{supp_figures/supp_mod_rgb.pdf}
    \caption{
    We show more rendering results from color modification field and compositional layer.
    }
    \label{fig:mod_rgb}
\end{figure}


\noindent\textbf{Robustness to the noise of proxy mesh.}
The geometry prior guidance uses the proxy mesh to supervise the geometry modification field.
Therefore, we analyze the effect of mesh quality on our editing results.
Specifically, we add 3 groups of gaussian noise to the vertices of the proxy mesh and conduct the training of our editing field.
As shown in Fig~\ref{fig:supp_geo_meshquality}, our method can robustly learn geometric editing even with noisy proxy mesh (\eg, with the gaussian noise of $N(0, 0.004^2)$ in the third column).

\noindent\textbf{More comparison results on texture editing.}
We show more comparison results on the texture editing task with ARF~\cite{arf}, CLIP-NeRF~\cite{clip_nerf} and DFF~\cite{kobayashi2022decomposing} in Fig.~\ref{fig:tex_edit_compare} (where Fig.~\ref{fig:tex_edit_compare} (a) is the source view and the target view produced by Text2LIVE~\cite{text2live}).
For CLIP-NeRF~\cite{clip_nerf}, since the official codebase has not been fully released, we use our own implementation by fine-tuning NeRF's color-related field with CLIP loss, and both use the target features from text embedding and the image embedding (with the same target images in Fig.~\ref{fig:tex_edit_compare} (a)), which are denoted as CLIP-NeRF (Image) and CLIP-NeRF (Text), respectively.
For DFF~\cite{kobayashi2022decomposing}, we adopt the official codebase and use the texts for NeRF editing and background ray-filtering according to the document.
In Fig.~\ref{fig:tex_edit_compare}, we omit the DFF's object-centric comparison on the car, since it mainly focuses on scene-level decomposition and editing.
As demonstrated in Fig.~\ref{fig:tex_edit_compare}, NeRF stylization methods like ARF cannot precisely edit fine-grained effects on the desired location.
NeRF fine-tuning approaches like CLIP-NeRF and DFF only change appearance colors, but cannot produce vivid effects (\eg, the burning pinecone or ice sculpture cars).
Note that although DFF uses the semantic-field-guided decomposed rendering to maintain the background color unchanged, this strategy is not compatible with our color compositing mechanism since we introduce an additional 2D CNN layer to blend the template and editing color for better visual appearance.

By contrast, 
our method both achieves realistic and appealing editing effects, and also effectively preserves background content, and the results are consistently preferred by most of the participants in the user study (see Sec.~\textcolor{red}{4.4}).

\noindent\textbf{Impact of texture modification field \& color compositional layer.}
The texture modification field learns detailed modifications and the compositional layer blends the original and modified rendering to produce the final edited results, as demonstrated in Sec. \textcolor{red}{4.5} and Fig. \textcolor{red}{8} (a).
Here we show more rendered texture mod. field (a.k.a. color mod. $\hat{I}_m$) in Fig~\ref{fig:mod_rgb}.


\noindent\textbf{Deformation with topology changes.}
Our method does not support deformation with topology changes such as breaking the plate, but can provide a visually plausible result by making the ``broken part'' white, as shown in Fig~\ref{fig:hotdog_splitplate}
In the future, we can integrate more flexible representations such as ambient slicing surface~\cite{park2021hypernerf} into our model.
\begin{figure}[!t]
    \centering
    \vspace{0.0em}
    \includegraphics[width=0.95\linewidth, trim={0 0 0 0}, clip]{supp_figures/hotdog_splitplate4.pdf}
    \caption{
    We show the results of geometry editing with topology changes.
    }
    \label{fig:hotdog_splitplate}
    \vspace{1.0em}
\end{figure}