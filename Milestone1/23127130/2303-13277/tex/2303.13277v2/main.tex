% CVPR 2023 Paper Template
% based on the CVPR template provided by Ming-Ming Cheng (https://github.com/MCG-NKU/CVPR_Template)
% modified and extended by Stefan Roth (stefan.roth@NOSPAMtu-darmstadt.de)
\documentclass[10pt,twocolumn,letterpaper]{article}


%%%%%%%%% PAPER TYPE  - PLEASE UPDATE FOR FINAL VERSION
% \usepackage[review]{cvpr}      % To produce the REVIEW version
\usepackage{cvpr}              % To produce the CAMERA-READY version
%\usepackage[pagenumbers]{cvpr} % To force page numbers, e.g. for an arXiv version

% Include other packages here, before hyperref.
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{color}
\usepackage{multicol}
\usepackage{caption}
\usepackage{mathtools}
\usepackage{cuted}

\newcommand\hmmax{0}
\newcommand\bmmax{0}
\usepackage{bm}
\usepackage[accsupp]{axessibility}

\usepackage[pagebackref,breaklinks,colorlinks]{hyperref}
% \usepackage{hyperref}
% \hypersetup{pagebackref,breaklinks,colorlinks}


% Support for easy cross-referencing
\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}
\newcommand{\printfnsymbol}[1]{%
        \textsuperscript{\@fnsymbol{#1}}%
}





%%%%%%%%% PAPER ID  - PLEASE UPDATE
\def\cvprPaperID{4531} % *** Enter the CVPR Paper ID here
\def\confName{CVPR}
\def\confYear{2023}

% Global Spacing for figures and tables
\addtolength{\abovecaptionskip}{-1.0em} % Space above caption
\addtolength{\belowcaptionskip}{-1.0em} % Space below caption
\addtolength{\textfloatsep}{-1.0em} % Space between Figure and Text
\addtolength{\intextsep}{-0.7em} % Space between Figure and Text (intext with h placement)
\addtolength{\floatsep}{-0.5em} % Space between Figures

\newcommand{\ssecspace}{\vspace{-0.4em}}

\newcommand{\secspace}{\vspace{-0.0em}}

\begin{document}

\setlength{\abovedisplayskip}{0.2em}
\setlength{\belowdisplayskip}{0.2em}


%%%%%%%%% TITLE - PLEASE UPDATE
\title{
\vspace{-1.0em}
SINE: Semantic-driven Image-based NeRF Editing \\with Prior-guided Editing Field
\vspace{-1.0em}
}


\author{
Chong Bao$^{1}$\footnotemark[1]
\quad
Yinda Zhang$^{2}$\footnotemark[1] \quad
Bangbang Yang$^{1}$\footnotemark[1]  \quad
Tianxing Fan$^{1}$\\
Zesong Yang$^{1}$ \quad
Hujun Bao$^{1}$ \quad
Guofeng Zhang$^{1}$\footnotemark[2] \quad
Zhaopeng Cui$^{1}$\footnotemark[2] \\
$^{1}$State Key Lab of CAD\&CG, Zhejiang University \quad
$^{2}$Google \\
{\url{https://zju3dv.github.io/sine/}}
}
% \maketitle

\twocolumn[{%
\renewcommand\twocolumn[1][]{#1}%
\vspace{-3.6em}
\maketitle
\vspace{-3.3em}
\begin{center}
    \centering
    \includegraphics[width=1.0\linewidth, trim={0 0 0 0}, clip]{figures/teaser.pdf}
    \captionof{figure}{
    We propose a novel semantic-driven image-based editing approach, which allows users to edit a photo-realistic NeRF with a single-view image or with text prompts, and renders edited novel views with vivid details and multi-view consistency.
    }
    \vspace{1.0em}
    \label{fig:teaser}
\end{center}%
}]
\renewcommand{\thefootnote}{\fnsymbol{footnote}}
\footnotetext[1]{Authors contributed equally.}
\footnotetext[2]{Corresponding authors.}
% \maketitle

%%%%%%%%% ABSTRACT
\begin{abstract}
Despite the great success in 2D editing using user-friendly tools, such as Photoshop, semantic strokes, or even text prompts, similar capabilities in 3D areas are still limited, either relying on 3D modeling skills or allowing editing within only a few categories.
In this paper, we present a novel semantic-driven NeRF editing approach, which enables users to edit a neural radiance field with a single image, and faithfully delivers edited novel views with high fidelity and multi-view consistency.
To achieve this goal, we propose a prior-guided editing field to encode fine-grained geometric and texture editing in 3D space, and develop a series of techniques to aid the editing process, including cyclic constraints with a proxy mesh to facilitate geometric supervision, a color compositing mechanism to stabilize semantic-driven texture editing, and a feature-cluster-based regularization to preserve the irrelevant content unchanged.
Extensive experiments and editing examples on both real-world and synthetic data demonstrate that our method achieves photo-realistic 3D editing using only a single edited image, pushing the bound of semantic-driven editing in 3D real-world scenes.
\end{abstract}
\vspace{-2.0em}

%%%%%%%%% BODY TEXT
\section{Introduction}
% \label{sec:intro}

\input{intro.tex}

\begin{figure*}[!t]
    \centering
    \vspace{-1.7em}
    \includegraphics[width=0.97\linewidth, trim={0 0 0 0}, clip]{figures/framework.pdf}
    \caption{
    % Framework
    \textbf{Overview.}
    We encode geometric and texture changes over the original template NeRF with a prior-guided editing field, where the geometric modification field $F_{\Delta G}$ transformed the edited space query $\mathbf{x}$ into the template space $\mathbf{x}'$, and the texture modification field $F_{\Delta T}$ encodes modification colors $\mathbf{m}'$.
    Then, we render deformed template image $\hat{I}_{o}$ and color modification image $\hat{I}_{m}$ with all the queries, and use a color compositing layer to blend $\hat{I}_{o}$ and $\hat{I}_{m}$ into the edited view $\hat{I}$.
    \vspace{-0.5em}
    }
    \label{fig:framework}
\end{figure*}


\secspace
\vspace{-0.25em}

\section{Related Works}

\vspace{-0.25em}

\secspace

\noindent\textbf{Neural rendering with external priors.}
Neural rendering techniques aim at rendering novel views with high-quality~\cite{nerf} or controllable properties~\cite{park2021nerfies,conerf} by learning from 2D photo capture.
Recently, NeRF~\cite{nerf} achieves photo-realistic rendering with volume rendering and inspires many works, including surface reconstruction~\cite{neus,volsdf,li2022vox}, scene editing~\cite{guo2020object,object_nerf,wu2022objectsdf,yang2022neural,neural_outdoor_rerender} and generation~\cite{jain2022zero,poole2022dreamfusion}, inverse rendering~\cite{zhang2021nerfactor,boss2021neural}, SLAM~\cite{zhu2022nice, yang2022vox}, \etc.
For learning from few-shot images~\cite{jain2021putting} or 3D inpainting~\cite{mirzaei2022laterf}, NeRF's variants use hand-crafted losses~\cite{niemeyer2022regnerf} or large language-image models~\cite{jain2021putting,xu2022sinnerf} as external priors.
However, due to insufficient 3D supervision, such methods cannot reconstruct accurate geometry and only produce visually plausible results.
Besides, some works~\cite{mi2022im2nerf,li2022symmnerf,insafutdinov2022snes} use the symmetric assumption to reconstruct category-level objects (\eg, cars, chairs) but cannot generalize on complex scenes.

\noindent\textbf{Neural 2D \& 3D scene editing.}
With the development of neural networks,
semantic-driven 2D photo editing 
allows user editing in various friendly ways, such as controlling attribute of faces~\cite{he2019attgan,stylegan}, stroke-based editing~\cite{sdedit,editgan,wang2018high}, sketch-to-image generation~\cite{chen2018sketchygan,sangkloy2017scribbler}, image-to-image texture transferring~\cite{splice}, or text-driven image generation~\cite{diffusion} and editing~\cite{imagic}.
Nevertheless, in 3D scene editing, 
similar capabilities are still limited due to the high demand for multi-view consistency.
Existing approaches either rely on laborious annotation~\cite{conerf,nerf_editing,neumesh,object_nerf}, only support object deformation or translation~\cite{nerf_editing,kobayashi2022decomposing,feature_fusion_field,vora2021nesf}, or only perform global style transfer~\cite{arf,chiang2022stylizing,chen2022upstnerf,huang2022stylizednerf,fan2022unified} without strong semantic meaning.
Recently, 3D-aware GANs~\cite{pi-GAN,eg3d,gu2021stylenerf,sun2022ide,niemeyer2021giraffe,schwarz2020graf,jang2021codenerf} and semantic NeRF editing~\cite{edit_nerf,clip_nerf} learn a latent space of the category and enable editing via latent code control.
However, the quality and editing ability of these methods mainly depend on the dataset (\eg, human faces~\cite{sun2022fenerf,sun2022ide} or objects in ShapeNet~\cite{chang2015shapenet}), and they cannot generalize to objects with rich appearances or out-of-distribution features~\cite{text2live}.
In contrast, our method allows for semantic-driven editing directly on the given photo-realistic NeRF, and uses a prior-guided editing field to learn fine-grained editing from only a single image.

\secspace

\section{Method}

\secspace

We first formulate the goal of our semantic NeRF editing task as follows.
As illustrated in the left part of Fig.~\ref{fig:framework}, 
given a pre-trained NeRF of a photo-realistic scene (named template NeRF),
we aim at editing the template NeRF using only a single-view 2D image, and then produce novel views with consistent semantic meaning (see Sec.~\ref{ssec:expr_geo_edit} and Sec.~\ref{ssec:expr_tex_edit}).
Note that na\"ively fine-tuning on the edited single view cannot obtain satisfactory results due to the spatial ambiguity and lack of multi-view supervision (see Sec.~\ref{ssec:expr_neumesh}).
Therefore, 
we propose to use a novel prior-guided editing field to encode fine-grained changes (Sec.~\ref{ssec:framework}) in 3D space, which leverages geometry and texture priors to guide the learning of semantic-driven editing (Sec.~\ref{ssec:geo_prior} and Sec.~\ref{ssec:tex_prior}).
Besides, to precisely control the editing area while maintaining other parts unchanged, we design editing regularization with feature cluster-based semantic masking (Sec.~\ref{ssec:edit_reg}).




\ssecspace

\subsection{SINE Rendering Pipeline}
\label{ssec:framework}

\ssecspace

As illustrated in Fig.~\ref{fig:framework}, we use a dedicated editing field to encode geometry and texture changes over the pre-trained template NeRF.
The editing field consists of an implicit geometric modification field $F_{\Delta G}$ and a texture modification field $F_{\Delta T}$, where $F_{\Delta G}$ deforms the query points from the observed edited space to the original template space, 
as $\mathbf{x}' := F_{\Delta G}(\mathbf{x})$, 
and $F_{\Delta T}$ encodes the modification color $\mathbf{m}'$, as $\mathbf{m}' := F_{\Delta T}(\mathbf{x})$.
Specifically, for each sampled query point $\{\mathbf{x}_i|i=1,...,N\}$ along the ray $\bm{r}$ with view direction $\mathbf{d}$, we first obtain the deformed points $\mathbf{x}'$ (in template space) and modification color $\mathbf{m}'$, and feed $\mathbf{x}'$ and $\mathbf{d}$ to the template NeRF to obtain the density $\delta '$ and template colors $\mathbf{c}'$.
Then, we perform dual volume rendering both on edited fields and template NeRF following the quadrature rules~\cite{nerf,quadrature_rule}, which is defined as:
\begin{equation}
\begin{split}
        & \hat{C}_o(\bm{r}) = \sum_{i=1}^{N} T_i \alpha_i {\mathbf{c}}_i', \;\;\;   \hat{C}_e(\bm{r}) = \sum_{i=1}^{N} T_i \alpha_i {\mathbf{m}}_i', \\
        & T_i = \exp{\left(-\sum_{j=1}^{i-1}{\sigma'}_j \delta_j\right)},
\label{eq:rendering}
\end{split}
\end{equation}
where $\alpha_i = 1-\exp{(-{{\sigma}'}_i \delta_i)}$, and $\delta_i$ is the distance between adjacent samples along the ray. 
In this way, we obtain the deformed template image $\hat{I}_o$ from the template NeRF's pixel color $\hat{C}_o(\bm{r})$ and color modification image $\hat{I}_{m}$ from the modification color $\hat{C}_e(\bm{r})$.
Finally, we apply the color compositing layer (see Sec.~\ref{ssec:tex_prior}) to blend $\hat{I}_o$ and $\hat{I}_{m}$ into the resulting edited views $\hat{I}$.

\ssecspace

\subsection{Prior-Guided Geometric Editing}
\label{ssec:geo_prior}

\ssecspace

In this section, we explain how to learn $F_{\Delta G}(\mathbf{x})$ with the geometric prior.

\noindent\textbf{Shape prior constraint on the edited NeRF.}
We leverage geometric prior models, such as neural implicit shape representation~\cite{dif,park2019deepsdf} or depth prediction~\cite{bhat2021adabins}, to mitigate the ambiguity of geometric editing based on editing from a single perspective.
\textbf{(1)} For objects within a certain shape category (\eg, cars, airplanes), we use DIF~\cite{dif}, in which the implicit SDF field and the prior mesh $\hat{M}_P$ can be generated with the condition of an optimizable latent code $\hat{\mathbf{z}}$.
We force the edited NeRF's geometry $\hat{M}_{E}$ to be explainable by a pre-trained DIF model with the geometric prior loss:
\begin{equation}
\begin{split}
   \mathcal{L}_{gp} = \; & \underset{\hat{\mathbf{z}}}{\text{min}}
   \big(\sum_{\mathclap{\mathbf{p}'\in \hat{M}_E}} f_{\text{SDF}}(\hat{\mathbf{z}}, \mathbf{p}') +  \lambda ||\hat{\mathbf{z}}||_2^2 \big) \\
   &+ \sum_{\mathbf{p}'_i\in \hat{M}_{E}} \min_{\mathbf{p}_t \in \hat{M}_{P}} ||\mathbf{p}'_i-\mathbf{p}_t||^2_2 \\ 
   &+  \sum_{\mathbf{p}_i\in \hat{M}_{P}} \min_{\mathbf{p}'_t \in \hat{M}_E} ||\mathbf{p}_i-\mathbf{p}'_t||^2_2.
   \label{eq:geo_prior}
\end{split}
\end{equation}
The first term encourages the sampled surface points on the edited NeRF's geometry $\hat{M}_{E}$ to lie on the manifold of DIF's latent space with an SDF loss $f_{\text{SDF}}$ and the latent code regularization~\cite{dif}.
The last two terms are Chamfer constraints, which enforce the $\hat{M}_{E}$ close to the DIF's periodically updated prior mesh $\hat{M}_P$~\cite{lorensen1987marching} by minimizing the closest surface points.
\textbf{(2)} For objects without a category-level prior, we can build a finalized shape prior $\hat{M}_P$ beforehand.
Practically, we find 3D deforming vertices with 2D correspondence~\cite{jiang2021cotr} and monocular depth prediction~\cite{bhat2021adabins}, and use ARAP~\cite{as_rigid_as_possible} to deform the proxy triangle mesh $M_{\Theta}$ to $\hat{M}_P$.
Then, we can inherit the Chamfer loss term in Eq.~\eqref{eq:geo_prior} for prior-guided supervision.

\noindent\textbf{Representing edited NeRF's geometry as a deformed proxy mesh.}
The edited NeRF has no explicit surface definition or SDF field to directly apply the geometric prior loss (Eq.~\eqref{eq:geo_prior}).
Therefore, to obtain the edited mesh surface $\hat{M}_E$, as illustrated in Fig.~\ref{fig:prior_guided_edit} (a), we first fit the template NeRF geometry with a proxy mesh $M_{\Theta}$~\cite{lorensen1987marching,neus}, and then learn a forward modification field $F_{\Delta G}'$ to warp the template proxy mesh to the edited space.
$F_{\Delta G}'$ is an inverse of the editing field $F_{\Delta G}$, which maps from the template space to the query space~\cite{mihajlovic2021leap,neural_scene_flow}, as $\mathbf{x}:=F_{\Delta G}'(\mathbf{x}')$, and can be supervised using a cycle loss $\mathcal{L}_{\text{cyc}}$ (see the supplementary material for details). 
Note that the deformed mesh proxy might not reflect fine-grained details of the specific shape identity. It facilitates applying shape priors to the edited field and provides essential guidance during geometric editing.

\noindent\textbf{Learning geometric editing with users' 2D editing.}
The goal of geometric editing is to deform the given NeRF according to the edited target image while satisfying semantic properties.
To this end, apart from the geometric prior loss in Eq.~\eqref{eq:geo_prior}, we add the following geometric editing loss in two folds.
\textbf{(1)}
We encourage the edited NeRF to satisfy the user's edited image by directly supervising rendering colors and opacity on $N_r$ rays, which is defined as:
\begin{equation}
\begin{split}
    \mathcal{L}_{\text{gt}} = \frac{1}{|N_r|} \sum_{\bm{r}\in N_r}  ||\hat{C}(\bm{r})-C_t(\bm{r})||^2_2 +  \text{BCE}(\hat{O}(\bm{r}),O_e(\bm{r})).
\end{split}
\end{equation}
The first photometric loss term encourages the rendered color $\hat{C}(\bm{r})$ close to the edited target color $C_t(\bm{r})$.
The second silhouette loss term enforces the rendered opacity $\hat{O}(\bm{r})$ close to the edited object's silhouette $O_e(\bm{r})$ (derived from users' editing tools) by minimizing the binary cross-entropy loss, where $\hat{O}(\bm{r}) = \sum_{i=1}^{N} T_i \alpha_i$.
\textbf{(2)}
To obtain a spatially smooth deformation and mitigate overfitting to the mesh's surface points, inspired by previous works~\cite{park2021nerfies,park2021hypernerf,dif}, we also add deformation regularization as:
\begin{equation}
    \mathcal{L}_{\text{gr}}=\frac{1}{M} \sum_{i=1}^N ||\nabla F_{\Delta G}(\mathbf{p}_i)||_2 +  ||F_{\Delta G}(\mathbf{p}_i) -F_{\Delta G}(\mathbf{p}_i + \bm{\epsilon}) ||_1,
\end{equation}
where the first term penalizes the spatial gradient of the geometric editing, and the second term encourages the editing to be smooth under a mild 3D positional jitter $\bm{\epsilon}$.

The overall geometric editing loss is defined as:
\begin{equation}
    \mathcal{L}_{\text{geo}}=\lambda_{\text{gp}} \mathcal{L}_{\text{gp}}+\mathcal{L}_{\text{gt}} + \lambda_{\text{gr}} \mathcal{L}_{\text{gr}} + \lambda_{\text{cyc}} \mathcal{L}_{\text{cyc}} ,
\end{equation}
where we set $\lambda_{\text{gp}} = 0.03$, $\lambda_{\text{gr}} = 0.1$ and $\lambda_{\text{cyc}} = 10$.
Intuitively, the geometric editing loss $\mathcal{L}_{\text{geo}}$ jointly optimizes edited NeRF's geometry $\hat{M}_{E}$ and the latent shape code $\hat{\mathbf{z}}$ (for category-level objects) to best fit the user's 2D editing while maintaining shape prior's semantic properties (\ie, shape symmetry or physical conformity).

\begin{figure}[!t]
    \centering
    % \vspace{-1.5em}
    \includegraphics[width=1.0\linewidth, trim={0 0 0 0}, clip]{figures/method_prior_constraint.pdf}
    \caption{
    We leverage geometric~\cite{dif,bhat2021adabins} and texture~\cite{dino,splice} priors to guide the learning of semantic-driven NeRF editing. 
    }
    \label{fig:prior_guided_edit}
    \vspace{0.5em}
\end{figure}

\ssecspace

\subsection{Prior-Guided Texture Editing}
\label{ssec:tex_prior}

\ssecspace

\noindent\textbf{Semantic texture prior supervision.}
In our task, users only conduct editing on a single image, but we hope to naturally propagate editing effects to multi-views with semantic meaning (see Fig.~\ref{fig:teaser}).
Therefore, we need to utilize semantic texture supervision that supports transferring the editing to the given NeRF across views, rather than using a pixel-aligned photometric loss.
Inspired by Tumanyan \etal~\cite{splice}, we use a pre-trained ViT model~\cite{dino} as the semantic texture prior, and apply the texture transferring loss in a multi-view manner as illustrated in Fig.~\ref{fig:prior_guided_edit} (b), which is defined as:
\begin{equation}
    \mathcal{L}_{\text{tex}} = || t_{\text{CLS}}(I_t) - t_{\text{CLS}}(\hat{I}) ||_2 + ||S(\hat{I}_o)-S(\hat{I})||_F,
\label{eq:tex_transfer}
\end{equation}
where $I_t$ is the user's edited image, $\hat{I}_o$ and $\hat{I}$ are the template image and edited image as introduced in Sec.~\ref{ssec:framework}.
$t_{\text{CLS}}(\cdot)$ and $S(\cdot)$ are the extracted deepest $\text{CLS}$ token and the structural self-similarity defined by Tumanyan \etal~\cite{splice}.
Essentially, this loss encourages $\hat{I}_o$ and $\hat{I}$ to share a similar spatial structure, and $\hat{I}_t$ and $\hat{I}$ to contain similar image cues.

\noindent\textbf{Decoupled rendering with color compositing layer.}
To achieve texture modification, a na\"ive approach is to directly add the modification color $\mathbf{m}'$ from the editing field to the template NeRF's radiance color $\mathbf{c}'$ during volume rendering.
However, we find it suffers from sub-optimal convergence when cooperating with texture transferring loss (see Sec.~\ref{ssec:expr_edit_ablation}), since NeRF struggles to learn the global-consistent appearance under the variational supervisory as shown in Fig.~\ref{fig:ablation_texture}(a).
To tackle this issue, we re-design the rendering pipeline in a decoupled manner.
As shown in Fig.~\ref{fig:framework},
we first render the deformed template image $\hat{I}_o$ with template NeRF and the color modification $\hat{I}_{m}$ with $F_{\Delta T}$, and then use a 2D CNN-based color compositing layer to deferred blend the modification $\hat{I}_{m}$ into the template image $\hat{I}_o$, which yields final edited view $\hat{I}$.
Intuitively, the coordinate-based editing field can encode fine-grained details from photometric constraints but cannot easily learn from coarse semantic supervision, while the proposed color compositing layer can reduce the difficulty by using easy-to-learn CNN layers before applying texture transferring loss.
Besides, it also learns view-dependent effects from the semantic prior, making the rendering results more realistic (\eg, the shining diamond effect in Fig.~\ref{fig:teaser}).

\ssecspace

\subsection{Editing Regularization}
\label{ssec:edit_reg}

\ssecspace

\noindent\textbf{Feature-cluster-based semantic masking.}
To precisely edit the desired region while preserving other content unchanged, inspired by previous works~\cite{vora2021nesf,kobayashi2022decomposing,feature_fusion_field}, we learn a distilled feature field with DINO-ViT~\cite{dino} to reconstruct scenes/objects with semantic features.
However, existing semantic field decomposing approaches~\cite{kobayashi2022decomposing,feature_fusion_field} are limited to the query-based similarity and require all the editing to be finalized on the 3D field, which is not compatible with our color compositing mechanism.
Therefore, we leverage users' editing silhouette $M_{e}$ to generate several feature clusters from the distilled feature map, and compute semantic masks $\hat{M}_{e}$ using the closest cosine similarity to cluster centers with a threshold, which will be served for image-based editing regularization.

\noindent\textbf{Regularization on geometric and texture editing.}
With the semantic masks that indicate the editing area, we can apply editing regularization to the geometric and texture editing, \ie, by enforcing the rendered pixels and the queries at the irrelevant part unchanged, which is defined as:
\begin{equation}
        \mathcal{L}_{\text{reg}} = \sum_{\mathclap{\mathbf{x} \in \hat{I} \setminus \hat{M}_{e}} }||F_{\Delta G}(\mathbf{x})||_1 + \sum_{\mathclap{\bm{r} \in \hat{I} \setminus \hat{M}_{e}}} ||\hat{C}(\mathbf{r}) - \hat{C}_o(\mathbf{r})||_2^2,
\label{eq:edit_reg}
\end{equation}
where the sampled points $\mathbf{x}$ and rays $\bm{r}$ are both from the background area of the computed semantic masks $\hat{M}_{e}$.

\begin{figure}[!t]
    \centering
    % \vspace{-1.5em}
    \includegraphics[width=0.97\linewidth, trim={0 0 0 0}, clip]{figures/neumesh_comparison.pdf}
    \caption{
    We show the difference between our semantic-driven image-based NeRF editing and manual NeRF editing~\cite{neumesh}.
    }
    \label{fig:edit_compare_neumesh}
    \vspace{0.3em}
\end{figure}


\begin{figure*}[!t]
    \centering
    \includegraphics[width=1.0\linewidth, trim={0 0 0 0}, clip]{figures/geo_edit.pdf}
    \caption{
    We compare the geometric editing with EG3D~\cite{eg3d} and EditNeRF~\cite{edit_nerf} on the real-world cars~\cite{CarWale} and PhotoShape~\cite{photoshape2018}.
    }
    \label{fig:geo_edit}
\end{figure*}

\secspace

\section{Experiments}

\ssecspace

\subsection{Datasets}
\label{ssec:expr_data}

\ssecspace

We evaluate SINE on both real-world/synthetic and object/scene datasets, including real-world car dataset~\cite{CarWale}, PhotoShape datasets (synthetic chairs)~\cite{photoshape2018}, ``pinecone'', ``vasedeck'', and ``garden'' from NeRF real-world 360$^{\circ}$ scenes~\cite{nerf,barron2022mipnerf360}, ``chairs'' and ``hotdog'' from NeRF photo-realistic synthetic data~\cite{nerf}, bird status from DTU~\cite{dtu} dataset, and ``airplane'' from BlenderSwap~\cite{blenderswap}.
Please refer to the supplementary materialfor more details.





\begin{figure*}[!t]
    \centering
    \includegraphics[width=1.0\linewidth, trim={0 0 0 0}, clip]{figures/our_texture_edit.pdf}
    \caption{
    We show our texture editing results when given users' target images and cooperating with text-prompt-based editing methods~\cite{text2live}.
}
    \label{fig:our_tex_edit}
\end{figure*}



\ssecspace

\subsection{Semantic-driven vs. Manual Editing}
\label{ssec:expr_neumesh}

\ssecspace

We first clarify the difference between our semantic-driven NeRF editing and manual NeRF editing (\eg, NeuMesh~\cite{neumesh}, NeRF-Editing~\cite{nerf_editing}).
As illustrated in Fig.~\ref{fig:edit_compare_neumesh} (a), our method provides much more effortless ways than manual approaches.
For example, they require 3D modeling skills to bind the skeleton of the mesh using Blender~\cite{neumesh}, or drive models~\cite{nerf_editing} with Mixamo poses~\cite{Mixamo}, while our method can easily achieve similar geometric editing with only a single-view image.
Besides, na\"ively fine-tuning NeRF on a single-view with a pixel-aligned photometric loss like NeuMesh~\cite{neumesh} would only modify visible regions, which leads to inconsistent novel view rendering (\eg, in Fig.~\ref{fig:edit_compare_neumesh} (b), the car edited by single-view fine-tuning would expose unpainted red part).
On the contrary, our method leverages semantic priors~\cite{dino} to naturally edit objects with multi-view consistency, which does not require pixel-wise alignment and enables texture transferring between objects with different shapes (see cars and chairs in Fig.~\ref{fig:our_tex_edit} (a)).

\ssecspace

\subsection{Semantic-driven Geometric Editing}
\label{ssec:expr_geo_edit}

\ssecspace

We first show our geometric editing results in Fig.~\ref{fig:geo_edit} (a), where the objects can be faithfully deformed according to users' 2D editing (\eg, the airplane with warped wings~\cite{blenderswap}, green chair with bent legs~\cite{nerf} and deformed bird status~\cite{dtu}).
For the usage of geometry prior, we use a pre-trained DIF~\cite{dif} model for cars~\cite{CarWale}, chairs~\cite{photoshape2018} and planes~\cite{blenderswap}, and use ARAP-based shape priors for general objects without a category-level prior (\ie, toy in Fig.~\ref{fig:edit_compare_neumesh} (a), green
chair with unusual shape and status in Fig.~\ref{fig:geo_edit} (a), \etc).
Then, we compare our method with EG3D~\cite{eg3d}, a 3D-aware generative model that learns a latent representation of category-level objects, and EditNeRF~\cite{edit_nerf}, a NeRF-variant that supports object editing with single-view user interaction.
In addition to editing comparisons, we also used PSNR, SSIM, and LPIPS~\cite{nerf} to measure the edited rendering quality on synthetic cars and chairs.
For the geometric editing on EG3D, we first conduct 3D GAN inversion to obtain the style code with multi-view images
(same input as ours), and then fine-tune the code on the target images.
As shown in Fig.~\ref{fig:geo_edit} (b), we conduct different editing operations on four cars from CalWare 360$^{\circ}$ datasets with DIF shape prior~\cite{dif}, \ie, enlarging/shrinking tires/back.
Due to the difficulty of learning a latent textured 3D representation and the limitation of data diversity, 3D-aware generative models like EG3D cannot produce rendering results with fine-grained details, which also results in lower evaluation metrics.
For the Photoshape~\cite{photoshape2018}, EditNeRF~\cite{edit_nerf} does not provide edited GT images,  so we regenerate all testing cases using Blender, which is more challenging than the original ones.
Then, we evaluate EditNeRF~\cite{edit_nerf} by fine-tuning the pre-trained models on specific chairs from the PhotoShape dataset.
As shown in Fig.~\ref{fig:geo_edit} (c), EditNeRF produces more blurry rendering results than ours, and cannot achieve satisfactory results with single-view editing (\eg, multi-view inconsistent chair back in the first row, and unmodified or blurry shapes in the third and fourth rows).
By contrast, our method consistently delivers high-fidelity rendering results and achieves reliable editing capability by leveraging geometric priors~\cite{dif,bhat2021adabins}.
This demonstrates that, for semantic geometric NeRF editing, learning a prior-guided editing field like ours can maintain better visual quality and achieve greater generalization ability than pre-training a textured 3D generative model or latent model.

\begin{figure*}[!t]
    \centering
    \includegraphics[width=1.0\linewidth, trim={0 0 0 0}, clip]{figures/compare_texture_edit.pdf}
    \caption{
    We compare our texture editing with ARF~\cite{arf} and CLIP-NeRF~\cite{clip_nerf} on the real-world cars~\cite{CarWale} and 360$^{\circ}$~\cite{nerf,barron2022mipnerf360} scene dataset.
}
    \label{fig:compare_tex_edit}
\end{figure*}


\begin{figure*}[!t]
    \centering
    \includegraphics[width=1.0\linewidth, trim={0 0 0 0}, clip]{figures/ablation_texture.pdf}
    \caption{
    We analyze the effectiveness of the color compositing mechanism and editing regularization in texture editing. 
    }
    \label{fig:ablation_texture}
\end{figure*}

\begin{figure}[!t]
    \centering
    \vspace{0em}
    \includegraphics[width=1.0\linewidth, trim={0 0 0 0}, clip]{figures/ablation_geo.pdf}
    \caption{
    We inspect the efficacy of the geometric prior constraint and editing regularization in geometric editing.
    }
    \label{fig:ablation_geometry}
    \vspace{0.5em}
\end{figure}

\ssecspace

\subsection{Semantic-driven Texture Editing}
\label{ssec:expr_tex_edit}

\ssecspace

We evaluate our semantic texture editing ability on both objects (cars from CalWare 360$^{\circ}$, chairs from PhotoShape~\cite{photoshape2018}) and unbounded 360$^{\circ}$ scenes~\cite{nerf,barron2022mipnerf360}.
Since our method only requires a single image as editing input, we exhibit several editing functionalities as shown in Fig.~\ref{fig:our_tex_edit}.
Users can edit by assigning new textures on the car using Photoshop (adding sea wave windows in Fig.~\ref{fig:our_tex_edit} (a)), using a downloaded Internet image with different shapes as a reference (transferring textures of cars and chairs in Fig.~\ref{fig:our_tex_edit} (a)).
Moreover, we cooperate SINE with off-the-shelf text-prompts editing methods~\cite{text2live} by using a single text-edited image as the target, which enables to change the object's appearance in the 360$^{\circ}$ scene with vivid effects (\eg, shiny plastic round table or burning pinecone in Fig.\ref{fig:our_tex_edit} (b)) while preserving background unchanged.
It is noteworthy that our method does not pre-train a latent model within a specific category like cars or chairs, yet still transfers texture between objects with correct semantic meaning, \eg, the texture styles of chair legs and cloths in the edited views are precisely matched to the target images in Fig.~\ref{fig:our_tex_edit} (a).
Besides, we also compare our methods with ARF~\cite{arf}, a NeRF stylization method that also takes a single reference image as input, and CLIP-NeRF~\cite{clip_nerf}, which supports text-driven NeRF editing using the large language model~\cite{clip}.
As demonstrated in Fig.~\ref{fig:compare_tex_edit}, ARF globally changes appearance colors to the given target images but fails to produce fine-grained details (\eg, cookie tires in Fig.~\ref{fig:compare_tex_edit} (a)).
For CLIP-NeRF, since it directly fine-tunes NeRF's color layers, the results only show color/hue adjustment on the original scene (\eg, in Fig.~\ref{fig:compare_tex_edit}, the round table turns gray instead of a realistic silver texture, the vasedeck turns blue instead of a shining diamond).
Thanks to the prior-guided editing field, our method learns more fine-grained editing details than the others, and achieves texture editing with consistent semantic meaning to the given target images (\eg, similar appearance to the Tesla's cybertruck in Fig.~\ref{fig:compare_tex_edit} (a)), and delivers rich appearance details and vivid effects (\eg, silver texture and shining diamond effects in Fig.~\ref{fig:compare_tex_edit} (b)).

\noindent\textbf{User study.}
We also perform user studies to compare our texture editing (including target-image-based editing and text-prompts editing as Fig.~\ref{fig:compare_tex_edit}) with ARF~\cite{arf} and CLIP-NeRF~\cite{clip_nerf} on 43 cases with 30 users.
The results show that users prefer our methods (89.5\% / 83.3\%) to ARF (10.4\% / 7.4\%) and CLIP-NeRF (9.3\%).
Please refer to the supplementary material for more details.

\ssecspace

\subsection{Ablation Studies}
\label{ssec:expr_edit_ablation}

\ssecspace

\noindent\textbf{Geometric prior constraints.}
We first analyze the effectiveness of geometric prior constraints (Sec.~\ref{ssec:geo_prior}) by ablating geometric prior loss (Eq.~\eqref{eq:geo_prior}) in Fig.~\ref{fig:ablation_geometry} (\ie, deforming cars and airplanes with DIF shape prior, and adding plates with general shape prior).
As shown in Fig.~\ref{fig:ablation_geometry} (b), when learning without geometric prior constraints, the object will be distorted when rendered from other views (\eg, collapsed car back, twisted airplanes, and warped hotdog plates).
By applying geometric prior constraints, we successfully mitigate the geometric ambiguity for single-image-based editing and produce plausible rendering results from novel views.

\noindent\textbf{Color compositing mechanism.}
We then inspect the efficacy of the color compositing mechanism (Sec.~\ref{ssec:tex_prior}) by disabling the texture modification field $F_{\Delta T}$ and the color compositing layer in turn.
As demonstrated in Fig.~\ref{fig:ablation_texture} (a), when learning texture editing without $F_{\Delta T}$, the rendered edited object can show a similar global appearance to the target, but lose vivid local patterns (\eg, gray and white grains and blue shininess).
When ablating the color compositing layers, the editing effect might not be properly applied to every part of the object (\eg, the uncovered gray part of the car's front).
When all the compositing mechanism is enabled, we successfully learn NeRF editing with fine-grained local patterns and globally similar appearance.

\noindent\textbf{Editing regularization.}
We finally evaluate the editing regularization (Sec.~\ref{ssec:edit_reg}) in geometric and texture editing by ablating regularization loss (Eq.~\eqref{eq:edit_reg}).
As shown in Fig.~\ref{fig:ablation_geometry} (c) and Fig.~\ref{fig:ablation_texture} (b), when learning editing without regularization, the irrelevant part would be inevitably changed (\eg, bent car's front and airplane's head in Fig.~\ref{fig:ablation_geometry} (c), a spurious cookie at the car's front and snowy background in Fig.~\ref{fig:ablation_texture} (b)).
By adding editing regularization, we can modify the user-desired objects precisely while preserving other content unchanged.

Please refer to the supplementary material for more experiments (\eg, ablation on more loss terms, visualization of color composition layer, discussion with external supervision, \etc).


\section{Conclusion}

\secspace

We have proposed a novel semantic-driven NeRF editing approach, which supports editing a photo-realistic template NeRF with a single user-edited image, and deliver edited novel views with high-fidelity and multi-view consistency.
As limitation, our approach does not support editing with topology changes, which can be future work.
Besides, our method assumes users' editing to be semantically meaningful, so we cannot use target images with meaningless random paintings.

\noindent\textbf{Acknowledgment.} This work was partially supported by NSF of China~(No. 62102356).

\clearpage

%%%%%%%%% REFERENCES
{\small
\bibliographystyle{ieee_fullname}
\bibliography{main}
}
\clearpage

\input{supplementary}

\end{document}
