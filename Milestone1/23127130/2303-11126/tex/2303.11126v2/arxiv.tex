\documentclass[10pt,twocolumn,letterpaper]{article}


% \DeclareMathOperator*{\argmin}{arg\,min}
% \DeclareMathOperator*{\argmax}{arg\,max}
\def\support{\mbox{support}}
%% Italian Short Terms
\def\diag{\mbox{diag}}
\def\rank{\mbox{rank}}
\def\grad{\mbox{\text{grad}}}
\def\dist{\mbox{dist}}
\def\sgn{\mbox{sgn}}
\def\tr{\mbox{tr}}
\def\card{{\mbox{Card}}}

%%bold greek letters\bvarpi
\def\balpha{\mbox{{\boldmath $\alpha$}}}
\def\bbeta{\mbox{{\boldmath $\beta$}}}
\def\bzeta{\mbox{{\boldmath $\zeta$}}}
\def\bgamma{\mbox{{\boldmath $\gamma$}}}
\def\bdelta{\mbox{{\boldmath $\delta$}}}
\def\bmu{\mbox{{\boldmath $\mu$}}}
\def\bftau{\mbox{{\boldmath $\tau$}}}
\def\beps{\mbox{{\boldmath $\epsilon$}}}
\def\blambda{\mbox{{\boldmath $\lambda$}}}
\def\bLambda{\mbox{{\boldmath $\Lambda$}}}
\def\bnu{\mbox{{\boldmath $\nu$}}}
\def\bomega{\mbox{{\boldmath $\omega$}}}
\def\bfeta{\mbox{{\boldmath $\eta$}}}
\def\bsigma{\mbox{{\boldmath $\sigma$}}}
\def\bzeta{\mbox{{\boldmath $\zeta$}}}
\def\bphi{\mbox{{\boldmath $\phi$}}}
\def\bxi{\mbox{{\boldmath $\xi$}}}
\def\bvphi{\mbox{{\boldmath $\phi$}}}
\def\bdelta{\mbox{{\boldmath $\delta$}}}
\def\bvarpi{\mbox{{\boldmath $\varpi$}}}
\def\bvarsigma{\mbox{{\boldmath $\varsigma$}}}
\def\bXi{\mbox{{\boldmath $\Xi$}}}
\def\bmW{\mbox{{\boldmath $\mW$}}}
\def\bmY{\mbox{{\boldmath $\mY$}}}

\def\bPi{\mbox{{\boldmath $\Pi$}}}

\def\bOmega{\mbox{{\boldmath $\Omega$}}}
\def\bDelta{\mbox{{\boldmath $\Delta$}}}
\def\bPi{\mbox{{\boldmath $\Pi$}}}
\def\bPsi{\mbox{{\boldmath $\Psi$}}}
\def\bSigma{\mbox{{\boldmath $\Sigma$}}}
\def\bUpsilon{\mbox{{\boldmath $\Upsilon$}}}

%%mathcal letters
\def\mA{{\mathcal A}}
\def\mB{{\mathcal B}}
\def\mC{{\mathcal C}}
\def\mD{{\mathcal D}}
\def\mE{{\mathcal E}}
\def\mF{{\mathcal F}}
\def\mG{{\mathcal G}}
\def\mH{{\mathcal H}}
\def\mI{{\mathcal I}}
\def\mJ{{\mathcal J}}
\def\mK{{\mathcal K}}
\def\mL{{\mathcal L}}
\def\mM{{\mathcal M}}
\def\mN{{\mathcal N}}
\def\mO{{\mathcal O}}
\def\mP{{\mathcal P}}
\def\mQ{{\mathcal Q}}
\def\mR{{\mathcal R}}
\def\mS{{\mathcal S}}
\def\mT{{\mathcal T}}
\def\mU{{\mathcal U}}
\def\mV{{\mathcal V}}
\def\mW{{\mathcal W}}
\def\mX{{\mathcal X}}
\def\mY{{\mathcal Y}}
\def\mZ{{\mathcal{Z}}}



%%bold mathcal letters
% \DeclareMathAlphabet\mathbfcal{OMS}{cmsy}{b}{n}
%bold mathcal letters
\def\bmA{{\mathbfcal A}}
\def\bmB{{\mathbfcal B}}
\def\bmC{{\mathbfcal C}}
\def\bmD{{\mathbfcal D}}
\def\bmE{{\mathbfcal E}}
\def\bmF{{\mathbfcal F}}
\def\bmG{{\mathbfcal G}}
\def\bmH{{\mathbfcal H}}
\def\bmI{{\mathbfcal I}}
\def\bmJ{{\mathbfcal J}}
\def\bmK{{\mathbfcal K}}
\def\bmL{{\mathbfcal L}}
\def\bmM{{\mathbfcal M}}
\def\bmN{{\mathbfcal N}}
\def\bmO{{\mathbfcal O}}
\def\bmP{{\mathbfcal P}}
\def\bmQ{{\mathbfcal Q}}
\def\bmR{{\mathbfcal R}}
\def\bmS{{\mathbfcal S}}
\def\bmT{{\mathbfcal T}}
\def\bmU{{\mathbfcal U}}
\def\bmV{{\mathbfcal V}}
\def\bmW{{\mathbfcal W}}
\def\bmX{{\mathbfcal X}}
\def\bmY{{\mathbfcal Y}}
\def\bmZ{{\mathbfcal Z}}



%%bold letters
\def\0{{\bf 0}}
\def\1{{\bf 1}}

%%bold capital Cases
\def\bA{{\bf A}}
\def\bB{{\bf B}}
\def\bC{{\bf C}}
\def\bD{{\bf D}}
\def\bE{{\bf E}}
\def\bF{{\bf F}}
\def\bG{{\bf G}}
\def\bH{{\bf H}}
\def\bI{{\bf I}}
\def\bJ{{\bf J}}
\def\bK{{\bf K}}
\def\bL{{\bf L}}
\def\bM{{\bf M}}
\def\bN{{\bf N}}
\def\bO{{\bf O}}
\def\bP{{\bf P}}
\def\bQ{{\bf Q}}
\def\bR{{\bf R}}
\def\bS{{\bf S}}
\def\bT{{\bf T}}
\def\bU{{\bf U}}
\def\bV{{\bf V}}
\def\bW{{\bf W}}
\def\bX{{\bf X}}
\def\bY{{\bf Y}}
\def\bZ{{\bf{Z}}}


%%bold small cases
\def\ba{{\bf a}}
\def\bb{{\bf b}}
\def\bc{{\bf c}}
\def\bd{{\bf d}}
\def\be{{\bf e}}
\def\bff{{\bf f}}
\def\bg{{\bf g}}
\def\bh{{\bf h}}
\def\bi{{\bf i}}
\def\bj{{\bf j}}
\def\bk{{\bf k}}
\def\bl{{\bf l}}
%\def\bm{{\bf m}}
\def\bn{{\bf n}}
\def\bo{{\bf o}}
\def\bp{{\bf p}}
\def\bq{{\bf q}}
\def\br{{\bf r}}
\def\bs{{\bf s}}
\def\bt{{\bf t}}
\def\bu{{\bf u}}
\def\bv{{\bf v}}
\def\bw{{\bf w}}
\def\bx{{\bf x}}
\def\by{{\bf y}}
\def\bz{{\bf z}}

%%hat letters
\def\hy{\hat{y}}
\def\hby{\hat{{\bf y}}}


%%mathrm letters
\def\mmE{{\mathbb E}}
\def\mmP{{\mathrm P}}
\def\mmB{{\mathrm B}}
\def\mmR{{\mathbb R}}
\def\mmV{{\mathbb V}}
\def\mmN{{\mathbb N}}
\def\mmZ{{\mathbb Z}}
\def\mMLr{{\mM_{\leq k}}}

%%tidle cases
\def\tC{\tilde{C}}
\def\tk{\tilde{r}}
\def\tJ{\tilde{J}}
\def\tbx{\tilde{\bx}}
\def\tbK{\tilde{\bK}}
\def\tL{\tilde{L}}
\def\tbPi{\mbox{{\boldmath $\tilde{\Pi}$}}}
\def\tw{{\bf \tilde{w}}}



%%bar cases
\def\barx{\bar{\bx}}

%%terms for short
\def\pd{{\succ\0}}
\def\psd{{\succeq\0}}
\def\vphi{\varphi}
\def\trsp{{\sf T}}

%short phrase
\def\mRMD{{\mathrm{D}}}
\def \DKL{{D_{KL}}}
\def\st{{\mathrm{s.t.}}}
\def\nth{{\mathrm{th}}}


\def\bx{{\bf x}}
\def\bX{{\bf X}}
\def\by{{\bf y}}
\def\bY{{\bf Y}}
\def\bw{{\bf w}}
\def\bW{{\bf W}}
\def\balpha{{\bm \alpha}}
\def\bbeta{{\bm \beta}}
\def\boldeta{{\bm \eta}}
\def\boldEta{{\bm \Eta}}
%\def\bgamma{{\bm \gamma}}
\def\bGamma{{\bm \Gamma}}
\def\bmu{{\bm \mu}}

\def\bK{{\bf K}}
\def\bb{{\bf b}}
\def\bg{{\bf g}}
\def\bp{{\bf p}}
\def\bP{{\bf P}}
\def\bh{{\bf h}}
\def\bc{{\bf c}}
\def\bz{{\bf z}}

\def\st{{\mathrm{s.t.}}}
\def\tr{\mathrm{tr}}
\def\grad{{\mathrm{grad}}}

\newtheorem{coll}{Corollary}
\newtheorem{deftn}{Definition}
\newtheorem{thm}{Theorem}
\newtheorem{prop}{Proposition}
\newtheorem{lemma}{Lemma}
\newtheorem{remark}{Remark}
\newtheorem{ass}{Assumption}
\newtheorem{proof}{Proof}

\newcommand{\tabincell}[2]{\begin{tabular}{@{}#1@{}}#2\end{tabular}}

\def\eg{\emph{e.g.}} \def\Eg{\emph{E.g.}}
\def\ie{\emph{i.e.}} \def\Ie{\emph{I.e.}}
\def\cf{\emph{c.f.}} \def\Cf{\emph{C.f.}}
\def\etc{\emph{etc.}} \def\vs{\emph{vs.}}
\def\wrt{{w.r.t.~}} \def\dof{d.o.f}
\def\etal{{\em et al.\/}\, }





\usepackage{iccv}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{xcolor}
\usepackage{mismath}
\usepackage{nicefrac}
\usepackage{bbm}
\usepackage{enumitem}

% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}

\iccvfinalcopy % *** Uncomment this line for the final submission

\def\iccvPaperID{2423} % *** Enter the ICCV Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
\ificcvfinal\pagestyle{empty}\fi

\newcommand{\yong}[1]{\textcolor[rgb]{0.6, 0.1, 0.6}{#1}}
\newcommand{\bernt}[1]{\textcolor[rgb]{0.08, 0.38, 0.74}{\textbf{Bernt:} #1}}
\newcommand{\david}[1]{\textcolor[rgb]{1, 0, 0}{\textbf{David} #1}}

\begin{document}

%%%%%%%%% TITLE
\title{Robustifying Token Attention for Vision Transformers}

\author{Yong Guo, David Stutz, Bernt Schiele\\
Max Planck Institute for Informatics, Saarland Informatics Campus\\
{\tt\small \{yongguo,david.stutz,schiele\}@mpi-inf.mpg.de}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
% \and
% Second Author\\
% Institution2\\
% First line of institution2 address\\
% {\tt\small secondauthor@i2.org}
}

\maketitle
% Remove page # from the first page of camera-ready.
\ificcvfinal\thispagestyle{empty}\fi


%%%%%%%%% ABSTRACT
\begin{abstract}
    Despite the success of vision transformers (ViTs), 
    they still suffer from significant drops in accuracy in the presence of common corruptions, such as noise or blur. Interestingly, we observe that the attention mechanism of ViTs tends to rely on few important tokens, a phenomenon we call \textit{token overfocusing}. 
    More critically, these tokens are \emph{not} robust to corruptions, often leading to highly diverging attention patterns. In this paper, we intend to alleviate this overfocusing issue and make attention more stable through two general techniques:
    First, our \textbf{Token-aware Average Pooling (TAP)} module encourages the local neighborhood of each token to take part in the attention mechanism.
    Specifically, TAP learns average pooling schemes for each token such that the information of potentially important tokens in the neighborhood can adaptively be taken into account.
    Second, we force the output tokens to aggregate information from a diverse set of input tokens rather than focusing on just a few by using our \textbf{Attention Diversification Loss (ADL)}. We achieve this by penalizing high cosine similarity between the attention vectors of different tokens.
    In experiments, we apply our methods to a wide range of transformer architectures and improve robustness significantly. For example, we improve corruption robustness on ImageNet-C by $2.4\%$ while simultaneously improving accuracy by $0.4\%$ based on state-of-the-art robust architecture FAN. Also, when fine-tuning on semantic segmentation tasks, we improve robustness on CityScapes-C by $2.4\%$ and ACDC by $3.1\%$.
    




\end{abstract}

%%%%%%%%% BODY TEXT
\section{Introduction}


Despite the success of vision transformers (ViTs), their performance still drops significantly on common image corruptions such as 
ImageNet-C \cite{HendrycksICLR2019,wenzel2022assaying}, adversarial examples \cite{gu2022vision,fu2022patch,shi2021decision}, and out-of-distribution examples as benchmarked in ImageNet-A/R/P~\cite{ZhaoICLR2018,HendrycksICLR2019}. In this paper, we 
examine a key component of ViTs, i.e., the self-attention mechanism, to understand these performance drops. Interestingly, we discover a phenomenon we call \emph{token overfocusing}, where only few important tokens are relied upon by the attention mechanism across all heads and layers. We hypothesize that this overfocusing is particularly fragile to the corruptions on input images and greatly hampers the robustness of the attention mechanism.



\begin{figure}[t]
\begin{center}
    \vspace*{-12px}
   \includegraphics[width=1.0\linewidth]{figures/token_overfocusing.pdf}
\end{center}
   \caption{
   Stability against image corruptions in terms of attention visualization (left, a matrix of $196 {\times} 196$) and cosine similarity of attention between clean and corrupted examples (right). 
	\emph{Left}: We average the attention maps across different heads for visualization and show the results of the last layer. We observe that ViTs put too much focus on very few tokens, a phenomenon we call \emph{token overfocusing}. More critically, the attention of the baseline FAN model~\cite{fu2022patch} is fragile to image corruptions, e.g., with Gaussian noise. Our approach, in contrast, alleviates token overfocusing and thereby improves stability of the attention against corruptions.
	\emph{Right}: On ImageNet, we plot the distribution of cosine similarities across all layers (without averaging heads) between clean and corrupted examples. 
	We show that our model yields a significantly higher similarity score than the baseline model.
   }
\label{fig:token_overfocusing}
\end{figure}



Starting from the state-of-the-art robust architecture FAN~\cite{zhou2022understanding}, we exemplarily investigate the last attention layer (see attention of other layers in supplementary). Specifically, Figure~\ref{fig:token_overfocusing} shows a clean and a corrupted input image as well as the corresponding attention maps. These are matrices of $N \times N$, with $N$ being the number of input/output tokens. Here, the $i$-th row indicates which input tokens (columns) the $i$-th output token ``attends'' to -- darker {\color{red}red} indicates higher attention scores. \emph{\textbf{Token overfocusing}} can then, informally, be defined by observing pronounced vertical lines in the attention map: First, each output token (row) focuses on only few important input tokens, ignoring most of the other information. Second, all output tokens seem to focus on the same input tokens, leading to a very low diversity among the attention vectors in different rows.  
We highlight that the overfocusing issue is present throughout the entire ImageNet dataset, and also across diverse architectures (see more examples in Figure~\ref{fig:visual_attention} and supplementary). More critically, we observe that these important tokens are extremely fragile in the presence of common corruptions. For example, when applying Gaussian noise on the input image, the tokens recognized as important change entirely, see Figure \ref{fig:token_overfocusing} (left, second column). 
Quantitatively, this can be captured by computing the cosine similarity between the clean and corrupted attention maps. 
Unsurprisingly, as shown by the \textcolor{blue}{blue box} in Figure~\ref{fig:token_overfocusing} (right), the cosine similarity is indeed extremely low, confirming our initial hypothesis. This motivates us to robustify the attention by alleviating the token overfocusing issue.


In this paper, we intend to address the token overfocusing issue from two perspectives.
First, we encourage output tokens to not only focus on individual input tokens but take into account the local neighborhood around these tokens, in order to make more tokens (columns) contribute meaningful information. 
Intuitively, an individual token itself may not be important but can be enhanced by aggregating the information from potentially important tokens located within its neighborhood.
We achieve this using a learnable average pooling mechanism applied to each input token to aggregate information before computing self-attention.
Second, we want to diversify the set of input tokens (columns) that the output tokens (rows) rely on. We achieve this using a loss that explicitly penalizes high cosine similarity across rows.
In Figure~\ref{fig:token_overfocusing}, the combination of these techniques leads to more balanced attention across columns and more diverse attention across rows. More critically, these attention maps are more stable in the light of image corruptions. Again, we quantitatively confirm this on ImageNet using the cosine similarity which is  significantly higher.



Overall, we make three key \textbf{contributions}: 
1) we propose a \emph{\textbf{Token-aware Average Pooling (TAP)}} module that encourages the local neighborhood of tokens to participate in the self-attention mechanism. To be specific, we conduct averaging pooling to aggregate information and control it by adjusting the pooling area for each token. 
2) We further develop an \emph{\textbf{Attention Diversification Loss (ADL)}} to improve the diversity of attention received by different tokens, i.e., rows in the attention map. To this end, we compute the attention similarity between rows in each layer and minimize it during training.
3) We highlight that both TAP and ADL can be applied on top of diverse transformer architectures.
In the experiments, the proposed methods consistently improve the model robustness on out-of-distribution benchmarks by a large margin while preserving a competitive improvement of clean accuracy at the same time. In addition, the improvement also generalizes well to other downstream tasks, e.g., semantic segmentation.





\section{Related Work}
\label{sec:related}



The remarkable performance of ViTs on various learning tasks is largely attributed to the use of self-attention \cite{KhanACM2022}. 
Naturally, the self-attention mechanism has been extended in various aspects, addressing shortcomings such as the heavy reliance on pre-training \cite{touvron2020deit,yuan2021tokens} or the high computational complexity \cite{liu2021swin,liu2021swinv2,dong2021cswin,elnouby2021xcit,child2019generating,beltagy2020longformer,wang2020linformer} and adapting the architecture to vision tasks by considering multi-scale transformers \cite{huang2021shuffle,fang2021msg,wang2020axial,wang2021pyramid,wang2021pvtv2,chu2021twins,xu2021coscale,wu2021cvt,huang2021shuffle,wang2021crossformer,chen2021regionvit,yang2021focal}. 
Similar to us, some related works \cite{zhou2021deepvit,CordonnierICLR2020,VigACL2019,naseer2021intriguing,voita2019analyzing} also investigate and visualize the self-attention in order to improve transformer architectures, e.g., to learn deeper transformers \cite{zhou2021deepvit} or prune attention heads \cite{voita2019analyzing}. However, to the best of our knowledge, we are the first to report a token overfocusing issue and link it to poor robustness of ViTs. 


There is also a lot of interests in understanding and improving the robustness of ViTs \cite{bhojanapalli2021understanding,bai2021transformers,shi2020robustness,paul2022vision,benz2021robustness,han2022robustify,MahmoodICCV2021,guo2022towards,HGFormer}. For example, \cite{mao2021towards} develops a robust vision transformer (RVT) by combining various components to boost robustness, including an improved attention scaling. FAN~\cite{zhou2022understanding} combines token attention and channel attention~\cite{ali2021xcit} and can be considered state-of-the-art. Both approaches rely on modified self-attention mechanisms and can be shown to suffer from token overfocusing. Thus, our techniques can be shown to improve robustness on top of RVT and FAN, respectively. Our TAP approach also shares similarities to recent ideas of trying to introduce locality into the self-attention mechanism \cite{xiao2021early,wu2021cvt,graham2021levit,peng2021conformer,yuan2021incorporating,xiao2021early,wang2021evolving}. For example,  CvT~\cite{wu2021cvt} introduces convolutions into the self-attention architecture to enhance the local information inside tokens. MViTv2~\cite{li2022mvitv2} exploits average pooling to extract local features and improve clean performance. However, all of these strategies use the same aggregation across all tokens, while our approach adaptively chooses an aggregation neighborhood for each token to improve robustness.






\begin{figure*}[t]
\begin{center}
   \includegraphics[width=1.0\linewidth]{figures/tap.pdf}
\end{center}
\vspace{-10 pt}
   \caption{The proposed Token-aware Average Pooling (TAP) module (left) and the overall architecture (right). \emph{Left}: In TAP, we introduce $K$ branches to enable tokens to consider different pooling areas and compute the weighted sum over them. Besides, we build a lightweight dilation predictor to learn the weights for different branches. \emph{Right}: We introduce a TAP layer into every basic block to encourage more tokens to be actively involved in the following self-attention mechanism.
   }
\label{fig:tap}
\vspace{-10 pt}
\end{figure*}





\section{Robust Token Self-Attention}


In the following, we focus on the attention mechanism in ViTs, aiming to improve their overall robustness.
To get started, we first describe the observed \emph{token overfocusing issue} where ViTs focus on very few but unstable tokens in detail in Section~\ref{subsec:token_overfocusing}. Then, we propose two general techniques for alleviating this issue: First, in Section~\ref{subsec:tap}, we propose a {\textbf{Token-aware Average Pooling (TAP)}} module that encourages the neighborhood of tokens to participate in the attention mechanism. This is achieved using a learnable pooling area as illustrated in Figure~\ref{fig:tap}. Second, in Section~\ref{subsec:adl}, we develop a new {\textbf{Attention Diversification Loss (ADL)}} to improve the diversity of attention patterns across tokens. Both methods can be applied to most transformer architectures and we will show they greatly improve robustness with negligible training overhead.



\subsection{Token Overfocusing}
\label{subsec:token_overfocusing}



As illustrated in Figure~\ref{fig:token_overfocusing}, we can visualize the self-attention mechanism in each layer as $N \times N$ attention matrix. Here, $N$ is the number of input and output tokens and each entry $(i,j)$ denotes the attention that the $i$-th output token ($i$-th row) puts on the $j$-th input token ($j$-th column) -- deeper \textcolor{red}{red} denoting higher attention scores. To handle multi-head self-attention, we visualize this matrix by averaging across attention heads. 


As baseline, Figure~\ref{fig:token_overfocusing} highlights the recent FAN \cite{zhou2022understanding} architecture, showing the attention map of the last layer for example. We observe that the attention is generally very sparse in columns, meaning that most input tokens are not attended to and very few tokens are overfocused. 
More importantly, these ``important'' tokens are often similar across output tokens (rows). We refer to this phenomenon as \emph{token overfocusing}. This leads to problems when facing corruptions such as Gaussian noise: the set of important tokens might change entirely (Figure~\ref{fig:token_overfocusing}, second column). This can be understood as the original tokens not capturing robust information. We can also quantitatively capture this instability by computing  the cosine similarity between clean and corrupted attention maps across all ImageNet images. As shown by the \textcolor{blue}{blue box} in Figure~\ref{fig:token_overfocusing} (right), the baseline model obtains very low cosine similarities, indicating the poor robustness of standard self-attention. We found that this phenomenon exists across diverse architectures, including DeiT~\cite{TouvronICML2021} and RVT~\cite{mao2021towards} and also occurs in, e.g., semantic segmentation models (see supplementary).
In the remainder of this section, we propose two general techniques to robustify self-attention.







\subsection{Token-aware Average Pooling (TAP)}
\label{subsec:tap}




In the first part of our method, we seek to encourage more input tokens to participate in the self-attention mechanism, i.e., obtaining more columns with high scores in the attention map. To this end, we encourage each input token to explicitly aggregate useful information from its local neighborhood, in case the token itself does not contain important information. This approach is justified by existing works~\cite{wu2021cvt,peng2021conformer,li2022mvitv2} and the observation that introducing any local aggregation before self-attention consistently improves robustness, see Table~\ref{tab:local_aggregation} (last column).
In fact, these methods apply a fixed convolutional kernel or pooling area to all the tokens. Nevertheless, tokens often differ from each other and each token should require a specific local aggregation strategy. This motivates us to adaptively choose the right neighborhood size and aggregation strategy.


Based on this idea, we propose to enable each token to select an appropriate area/strategy to conduct local aggregation. Specifically, we develop \textbf{Token-aware Average Pooling (TAP)} that conducts average pooling and adaptively adjusts the pooling area for each token. As shown in Figure~\ref{fig:tap}, we exploit a multi-branch structure that computes the weighted sum over multiple branches, each with a specific pooling area. 
Instead of simply changing the kernel size similar to~\cite{yoo2015multi}, TAP changes the dilation to adjust the pooling area. The main observation behind this is that average pooling with a large kernel, without dilation, leads to extremely large overlaps between adjacent pooling regions and thereby severe redundancy in output tokens. 
This can, for example, be seen in Table~\ref{tab:local_aggregation} where 
AvgPool5x5 incurs a large drop in clean accuracy of around 1.2\%.
Similar to \cite{xiao2021early,wu2021cvt,graham2021levit,peng2021conformer}, we also investigated using learnable convolutions instead of average pooling, but observed only marginal performance improvement alongside a significant increase in computational cost.

Based on these observations, we build TAP based on average pooling with diverse dilations:
Without loss of generality, given $K$ branches, we consider the dilation within the range $d {\in} [0,K{-}1]$. Here, $d=0$ implies identity mapping without any computation, i.e., no local aggregation. The maximum dilation determined by $K$ is a hyper-parameter and we find that performance and robustness improvements saturate at around $K = 5$ (see Figure~\ref{fig:k_lambda} left). Within the allowed dilation range, our method includes a lightweight dilation predictor to predict which dilation (i.e., which branch in Figure~\ref{fig:tap}) to utilize. Note that this can also be a weighted combination of multiple $d$. We emphasize that this predictor is very efficient since it reduces the feature dimension (from $C$ to $K$ in Figure~\ref{fig:tap}) such that it adds minimal computational overhead and model parameters. The same approach can also be applied to non-dilated average-pooling where $d$ controls the kernel size, named TAP (multi-kernel). From Table~\ref{tab:local_aggregation}, our TAP greatly outperforms this variant, indicating the effectiveness of using dilations for pooling.


\subsection{Attention Diversification Loss (ADL)}
\label{subsec:adl}



In the second part of our method, we seek to improve the diversity of attention across output tokens, i.e., encourage different rows in Figure~\ref{fig:token_overfocusing} to attend to different input tokens. 
Based on this objective, we propose an \textbf{Attention Diversification Loss (ADL)} that explicitly reduces the cosine similarity of attention among different output tokens (rows). However, for this approach to work, there are several challenges to overcome. First, computing the cosine similarity between attentions is numerically tricky. For example, if two rows (i.e., output tokens) have very disjoint attention patterns, we expect a low cosine similarity close to 0. However, even for tokens that are not attended to, the attention scores will not be zero. For a large $N$, computing dot product and adding these values up tend to result in a cosine similartiy significantly above zero.
To alleviate this issue, we exploit a thresholding trick to filter out those very small values and only focus on the most important ones. Let $\mathbbm{1}(\cdot)$ be the indication function, and $A_i^{(l)}$ be the attention vector of the $i$-th token (row) in the $l$-th layer. We introduce a threshold $\tau$ (see ablation in supplementary) that depends on the number of tokens $N$, i.e., $\nicefrac{\tau}{N}$. Thus, the attention after thresholding becomes
\begin{equation}
    \hat A_i^{(l)} = \mathbbm{1} (A_i^{(l)} \geq \nicefrac{\tau}{N} ) \cdot A_i^{(l)}.
\end{equation}


Second, to avoid the quadratic complexity of computing similarities between pairs of $N$ rows,
we approximate it by computing the cosine similarity between each individual attention vector $\hat A_i^{(l)}$ with the average attention $\bar A^{(l)} := \frac{1}{N} \sum_{i=1}^N \hat A_i^{(l)}$. When considering a model with $L$ layers, we average the ADL loss across all the layers by: 
\begin{equation}
    \mL_{\rm ADL} {=} \frac{1}{L} \sum_{l=1}^{L} \mL_{\rm ADL}^{(l)},~~~
    \mL_{\rm ADL}^{(l)} {=} \frac{1}{N} \sum_{i=1}^{N} \frac{\hat A_i^{(l)} \bar A^{(l)}} {\|\hat A_i^{(l)}\| \| \bar A^{(l)} \|}.
\end{equation}
In practice, we combine our ADL with the standard cross-entropy (CE) loss and introduce a hyper-parameter $\lambda$ (see ablation in Figure~\ref{fig:k_lambda}) to control the importance of ADL:
\begin{equation}
\label{eq:objective}
    \mL = \mL_{\rm CE} + \lambda   \mL_{\rm ADL}.
\end{equation}
We highlight that our ADL can be applied to boost the robustness on diverse tasks, including image classification and semantic segmentation (see Table~\ref{tab:imagenet} and Table~\ref{tab:segmentation}).



\begin{table}[t]
\begin{center}
\resizebox{1\linewidth}{!}
{
    \begin{tabular}{l|c|c|c|c}
    \toprule
    Model & \multicolumn{1}{c|}{\#FLOPs (G)} & \multicolumn{1}{c|}{\#Params (M)} & \multicolumn{1}{c|}{ImageNet} & \multicolumn{1}{c}{ImageNet-C $\downarrow$} \\
    \hline
    Baseline (FAN-B-Hybrid) &   11.7    &  50.4      &   83.9    & 46.1 \\
    ~~~+~AvgPool3x3 &   11.7    &  50.4      &   83.6    & 45.6 (-0.5) \\
    ~~~+~AvgPool5x5 &   11.7    &  50.4      &   82.7    & 45.5 (-0.6) \\
    ~~~+~Conv3x3 &    17.3   &  79.4     &   84.0    &  45.9 (-0.2) \\
    ~~~+~Conv5x5 &   27.4    &   130.7    &   \textbf{84.4}    & 45.8 (-0.3) \\
    \hline
    ~~~+~TAP (multi-kernel) &   11.8    &   50.7    &   84.1    & 45.5 (-0.6) \\
    ~~~+~TAP (Ours)   &   11.8    &   50.7    &   {84.3}    & \textbf{44.9 (-1.2)} \\
    \bottomrule
    \end{tabular}%
}
\end{center}
  \caption{Comparisons of local aggregation approaches based on FAN-B-Hybrid. We show that conducting average pooling for all the tokens improves the robustness but impedes clean accuracy. Introducing a convolution into each block greatly increases model complexity. In addition, we also compare a variant of our TAP, namely TAP (multi-kernel), that considers multiple kernel sizes for pooling and learns weights for each branch. Our Tap greatly outperforms this variant, indicating the effectiveness of using dilation. Moreover, TAP yields the best tradeoff between accuracy and robustness along with negligible computational overhead. 
  }
  \label{tab:local_aggregation}%
  \vspace{-10 pt}
\end{table}%


\begin{table*}[htbp]
\begin{center}
  \resizebox{0.95\textwidth}{!}
  {
    \begin{tabular}{l|c|c|c|cc|cc}
    \toprule
    Method & \#Params (M) & \#FLOPs (G) & {ImageNet $\uparrow$} & {ImageNet-C $\downarrow$} & ImageNet-P $\downarrow$ & {ImageNet-A $\uparrow$} & ImageNet-R $\uparrow$  \\
    \hline
    ConvNeXt-B~\cite{liu2022convnet} & 88.6 & 15.4 & 83.8 & 46.8 & - & 36.7 & 51.3  \\
    % \hline
    % DeiT-B~\cite{TouvronICML2021} & 86.6 & 17.6 & 82.0  & 48.5 & 32.1 & 27.4 & 44.9  \\
    ConViT-B~\cite{d2021convit} & 86.5 & 17.7 & 82.4 & 46.9 & 32.2 & 29.0 & 48.4  \\
    % XCiT-S12~\cite{ali2021xcit} & 26.3 & 4.8 & 81.9 & 51.5 & - & 25.0  & 45.5  \\
    % XCiT-S24~\cite{ali2021xcit} & 47.7 & 9.1 & 82.6 & 49.4 & - & 27.8  & 45.5  \\
    Swin-B~\cite{liu2021swin} & 87.8 & 15.4 & {83.4} & 54.4 & 32.7 & 35.8 & 46.6  \\
    % PVT-Large~\cite{wang2021pyramid} & 61.4 & 9.8 & 81.7 & 59.8 & 39.3 & 26.6 & 42.7  \\
    % PiT-B~\cite{heo2021rethinking} & 73.8 & 12.5 & 82.4 & 48.2 & 36.3 & 33.9 & 43.7   \\
    T2T-ViT\_t-24~\cite{yuan2021tokens} & 64.1 & 15.0 & 82.6 & 48.0 & 31.8 & 28.9 & 47.9  \\
    \hline
    RVT-B~\cite{mao2021towards} & 91.8 & 17.7 & 82.6  & 46.8 & 31.9  & 28.5 & 48.7  \\
    ~~+~TAP & 92.1 & 17.9  & {83.0 (+0.4)}  & {45.5 (-1.3)} & 30.6 (-1.3) & 30.0 (+1.5) & 49.4 (+0.7)  \\
    ~~+~ADL & 91.8 & 17.7 & {82.6 (+0.0)}  & {45.2 (-1.6)} & 30.2 (-1.7) & 30.8 (+2.3) & 49.8 (+1.1)  \\
    ~~+~TAP \& ADL & 92.1 & 17.9 & \textbf{83.1 (+0.5)}  & \textbf{44.7 (-2.1)} & \textbf{29.6 (-2.3)} & \textbf{32.7 (+4.2)} & \textbf{50.2 (+1.5)} \\
    \hline
    FAN-B-Hybrid~\cite{zhou2022understanding} & 50.4 & 11.7 & 83.9  & 46.1 & 31.3  & 39.6 & 52.7 \\
    ~~+~TAP & 50.7 & 11.8 & {84.3 (+0.4)} & {44.9 (-1.2)} & 30.3 (-1.0)  & 41.0 (+1.4) & 53.9 (+1.2)  \\
    ~~+~ADL & 50.4 & 11.7 & {84.0 (+0.1)}  & {44.4 (-1.7)} & 29.8 (-1.5) & 41.4 (+1.8) & 54.2 (+1.5) \\
    ~~+~TAP \& ADL & 50.7 & 11.8 & \textbf{84.3 (+0.4)}  & \textbf{43.7 (-2.4)} & \textbf{29.2 (-2.1)} & \textbf{42.3 (+2.7)} & \textbf{54.6 (+1.9)} \\
    \bottomrule
    \end{tabular}%
    }
\end{center}
\vspace{-5 pt}
  \caption{
  Comparisons on ImageNet and diverse robustness benchmarks. We report the mean corruption error (mCE) on ImageNet-C and mean flip rate (mFR) on ImageNet-P. For these metrics, lower is better. Moreover, we directly report the accuracy on ImageNet-A and ImageNet-R. Based on the considered two baselines, our models consistently improve the accuracy and robustness on diverse benchmarks.
  }
  \label{tab:imagenet}%
\end{table*}%



\begin{table*}[htbp]
\begin{center}
  \resizebox{1\textwidth}{!}
  {
    \begin{tabular}{l|c|ccc|cccc|cccc|cccc}
    \toprule
    \multicolumn{1}{l|}{\multirow{2}[0]{*}{Method}} &    \multirow{2}[0]{*}{mCE}   & \multicolumn{3}{c|}{Noise} & \multicolumn{4}{c|}{Blur}      & \multicolumn{4}{c|}{Weather}   & \multicolumn{4}{c}{Digital} \\
          &  & \multicolumn{1}{l}{Gaussian} & \multicolumn{1}{l}{Shot} & \multicolumn{1}{l|}{Impulse} & \multicolumn{1}{l}{Defocus} & \multicolumn{1}{l}{Glass} & \multicolumn{1}{l}{Motion} & \multicolumn{1}{l|}{Zoom} & \multicolumn{1}{l}{Snow} & \multicolumn{1}{l}{Frost} & \multicolumn{1}{l}{Fog} & \multicolumn{1}{l|}{Brightness} & \multicolumn{1}{l}{Contrast} & \multicolumn{1}{l}{Elastic} & \multicolumn{1}{l}{Pixelate} & \multicolumn{1}{l}{JPEG} \\
    \hline
    FAN-B-Hybrid & 46.2  & 40.12 & 39.27 & 36.80  & 51.58 & 63.96 & 47.53 & 54.98 & 40.24 & 43.96 & 36.98 & 36.68 & 34.17 & 61.59 & 53.25 & 51.86 \\
    ~~+~TAP   & 44.9 & 36.02 & 36.26 & 34.16 & 52.72 & 65.07 & 45.73 & 54.90  & 39.75 & 42.39 & 35.68 & 37.38 & 33.21 & 62.75 & 48.85 & 49.46 \\
    ~~+~ADL    & 44.3 & 35.61 & 35.55 & 33.51 & \textbf{50.80}  & 64.27 & 45.47 & 54.47 & \textbf{38.06} & 40.46 & 37.92 & 36.99 & {32.70}  & \textbf{61.45} & 47.78 & \textbf{49.00} \\
    ~~+~TAP \& ADL & \textbf{43.7} & \textbf{33.87} & \textbf{34.24} & \textbf{32.04} & 51.29 & \textbf{61.51} & \textbf{44.76} & \textbf{54.39} & 38.14 & \textbf{40.12} & \textbf{35.27} & \textbf{36.43} & \textbf{32.25} & 62.12 & \textbf{46.78} & 49.55 \\
    \bottomrule
    \end{tabular}%
    }
\end{center}
\vspace{-5 pt}
  \caption{Comparisons of corruption error (lower is better) on individual corruption type of ImageNet-C based on FAN-B-Hybrid. Combining TAP and ADL together yields the best results on most of the corruption types. 
  }
  \label{tab:imagenetc_individual}%
  \vspace{-10 pt}
\end{table*}%

\section{Experiments}


We conduct extensive experiments to verify our method on both image classification and semantic segmentation tasks. In Section~\ref{subsec:classification}, we first train classification models on ImageNet~\cite{deng2009imagenet} and demonstrate that our models obtain significant improvement on various robustness benchmarks, including ImageNet-A~\cite{ZhaoICLR2018}, ImageNet-C~\cite{HendrycksICLR2019}, ImageNet-R~\cite{HendrycksARXIV2020}, and ImageNet-P~\cite{HendrycksICLR2019}. 
Then, in Section~\ref{subsec:segmentation}, we take our best pre-trained model and further finetune it on Cityscapes~\cite{cordts2016cityscapes} for semantic segmentation. In practice, our models greatly improve mIoU on two popular robustness benchmarks, including Cityscapes-C~\cite{michaelis2019benchmarking} and ACDC~\cite{sakaridis2021acdc}, along with competitive performance on clean data. 
Both the code and pretrained models will be available soon.




\subsection{Results on Image Classification}
\label{subsec:classification}




In this experiment, we build our method on top of two state-of-the-art robust architectures: RVT~\cite{mao2021towards} and FAN~\cite{zhou2022understanding} with the ``Base'' model size, i.e., RVT-B and FAN-B-Hybrid. We train the models on ImageNet and evaluate them on several robustness 
% \david{robustness benchmarks?} 
benchmarks. We closely follow the settings of RVT and FAN for training. Specifically, we train the models using the same augmentation schemes and adopt the batch size of 2048. 
We set the learning rate to $2 {\times} 10^{-3}$ and train all the models for 300 epochs. In all the experiments, by default, we set $K=4$ and $\lambda=1$ to train our models.
To evaluate robustness, we consider several robustness benchmarks, including ImageNet-A~\cite{ZhaoICLR2018}, ImageNet-C ~\cite{HendrycksICLR2019}, ImageNet-R~\cite{HendrycksARXIV2020}, and ImageNet-P~\cite{HendrycksICLR2019}.
Note that, we report the mean corruption error (mCE) on ImageNet-C and mean flip rate (mFR) on ImageNet-P. For both metrics, lower is better.
Empirically, we demonstrate that using either TAP or ADL individually is able to improve the robustness. When combining them together, our models outperform the baselines by a larger margin and the performance improvement generalizes well to diverse architectures (see Table~\ref{tab:diverse_architecture}).




\subsubsection{Comparisons on ImageNet}




\begin{figure}[t]
\begin{center}
   \includegraphics[width=1.0\linewidth]{figures/attn_last_layer.pdf}
\end{center}
\vspace{-10 pt}
   \caption{Comparisons of attention maps among different models. Compared with the baseline model, our TAP alleviates the overfocusing issue by encouraging the tokens surrounding the most important ones to have higher attention scores. When only using ADL, we obtain an attention map that follows the diagonal pattern, i.e., preserving the token itself while aggregating information from other tokens. Apparently, the attention rows are different from each other, fulfilling our goal of reducing similarity between rows. When combining TAP and ADL together, the diagonal pattern is further expanded to the nearby areas thanks to the TAP module.}
\label{fig:visual_attention}
\vspace{-10 pt}
\end{figure}



\begin{figure*}[t]
\begin{center}
   \includegraphics[width=1.0\linewidth]{figures/head_attn.pdf}
\end{center}
   \caption{
   Attention maps of different attention heads in the last layer. For the baseline model, the most important tokens are often shared across different heads. In our model, two heads have the attention with the diagonal pattern and the other heads have specific patterns to extract different features, yielding higher attention diversity among heads (see quantitative results in Section~\ref{subsubsec:attention_stability}).}
\label{fig:attention_head}
\end{figure*}


As shown in Table~\ref{tab:imagenet}, 
compared with the strong baselines RVT and FAN, our models consistently improve the robustness on ImageNet-C by ${>}2.1\%$ and also yield comparable improvement on other robustness benchmarks, including ImageNet-A/R/SK. Moreover, our models also obtain a competitive improvement in terms of clean accuracy on ImageNet. For example, we improve the accuracy by ${>}0.4\%$ on both considered baseline architectures. More critically, we highlight that these improvements only come with negligible computational cost in terms of both the number of parameters and the number of floating-point operations (FLOPs).
In addition, we also report the detailed corruption error on individual corruption types of ImageNet-C based on FAN-B-Hybrid. From Table~\ref{tab:imagenetc_individual}, our best model (combining TAP and ADL together) obtains the best results on most of the corruption types. It is worth noting that our model is particularly effective against noise corruptions, e.g., yielding a large improvement of $6.25\%$ on Gaussian noise corruption. Overall, these experiments indicate that robustifying attention consistently improves robustness across different architectures and benchmarks.






\subsubsection{Attention Stability and Visualization Results}
\label{subsubsec:attention_stability}


We demonstrate that our models greatly improve attention stability against corruptions both qualitatively and quantitatively. Interestingly, in each layer, our models obtain a higher attention diversity among different attention heads.

\noindent
\textbf{Attention stability.}
In Figure~\ref{fig:visual_attention} we first visualize how much the attention would be changed when facing image corruptions, e.g., Gaussian noise.
We take FAN-B-Hybrid as the baseline and compare our best model with two variants that only contain TAP and ADL individually. Across diverse examples, the baseline model incurs a severe token overfocusing issue that it puts too much focus on very few tokens and comes with a significant attention shift when facing corruptions. With the help of our local aggregation module TAP, our TAP model assigns the attention to more tokens surrounding some important ones, alleviating the token overfocusing issue to some extent. Nevertheless, we still observe an attention shift between clean and corrupted examples. When training models with our ADL, the attention follows a diagonal pattern such that tokens aggregate information from the others while retaining most of the information from itself. We highlight that this diagonal pattern is somehow similar to the residual learning~\cite{HeCVPR2016} that additionally learns a residual branch while keeping the features unchanged using an identity shortcut. Clearly, the attention becomes much more stable against corruptions. When combining TAP and ADL together, we further encourage the diagonal pattern to expand within a local region. In this way, each token puts more focus on its neighborhood beyond itself and thus obtains stronger features. Quantitatively, we also evaluate the attention stability by computing cosine similarity of attention between clean and corrupted examples across the whole ImageNet. From Figure~\ref{fig:attn_stability}, TAP yields higher attention stability than the baseline model. Thanks to the diagonal pattern in attention, the model with ADL greatly improves the similarity score by a large margin, indicating that the attention is very stable against corruptions. When combining both TAP and ADL, we can further improve attention stability.





\begin{figure}[t]
\begin{center}
   \includegraphics[width=1.0\linewidth]{figures/attn_stability.pdf}
\end{center}
   \caption{Distributions of  cosine similarity of intermediate attention maps between clean and corrupted examples (e.g., with Gaussian noise) on ImageNet. We demonstrate that either TAP or ADL is able to improve the stability of attention independently. When combining them together, we further improve the stability/similarity of attention against corruptions.}
\label{fig:attn_stability}
\vspace{-5 pt}
\end{figure}










\begin{figure*}[t]
\begin{center}
   \includegraphics[width=0.95\linewidth]{figures/segmentation.pdf}
\end{center}
\vspace{-9 pt}
   \caption{Visual comparisons of segmentation results. When facing image corruptions or adverse conditions, the baseline FAN-B-Hybrid model fails to detect some important objects (e.g., road in the first example) or mistakenly recognizes a part of car as a rider (in the second example). By contrast, our model is much more robust against these corruptions and adverse conditions.}
\label{fig:visual_segmentation}
\end{figure*}










\noindent
\textbf{Attention of each head and attention diversity.} In this part, we further investigate the attention map in each individual head. As shown in Figure~\ref{fig:attention_head}, for the baseline model, the most important tokens are always the most important ones in almost all the heads, resulting in a very low diversity of attention among different heads. By contrast, in our model, only two heads follow the diagonal pattern and the other six heads have different attention patterns from each other. We highlight that our attention is a combination of both local and global filters w.r.t. different heads. Specifically, these two diagonal heads can be regarded as local filters to extract local information since the diagonal pattern encourages tokens to aggregate information within their local neighborhood. As for the other six heads, the attention is distributed across the whole map and thus they can be regarded as global filters. From the visualization result in Figure~\ref{fig:attention_head}, our model has a higher diversity of attention among different heads. To quantify this, following the previously discussed approach, we directly compute the cosine similarity of attention maps between any two heads in each layer (see detailed computation method in supplementary). In other words, the lower the similarity score is, the higher the attention diversity will be. In practice, the baseline model obtains a high similarity score of 0.63 when averaging across the whole ImageNet, indicating a very low attention diversity among different heads. By contrast, our model yields a lower similarity of 0.27, which is consistent with the previous observation that our model produces diverse attention across different heads.


















\begin{table}[t]
\begin{center}
\resizebox{1\linewidth}{!}
{
    \begin{tabular}{l|c|cc}
    \toprule
    Model  & Cityscapes $\uparrow$ & Cityscapes-C $\uparrow$ & ACDC $\uparrow$ \\
    \hline
    % DeepLabv3+ (R50)~\cite{chen2017rethinking} & 25.4 & 76.6 & 36.8 \\
    DeepLabv3+ (R101)~\cite{chen2017rethinking}  & 77.1 & 39.4 & 41.6 \\
    % DeepLabv3+ (X65)~\cite{chen2017rethinking} & 22.8 & 78.4 & 42.7 \\
    % DeepLabv3+ (X71)~\cite{chen2017rethinking} &   -    & 78.6 & 42.5 \\
    % \hline
    ICNet~\cite{zhao2018icnet} & 65.9 & 28.0 & - \\
    % FCN8s~\cite{long2015fully} & 50.1 & 66.7 & 27.4 \\
    DilatedNet~\cite{yu2015multi}   & 68.6 & 30.3 & - \\
    % ResNet38 & 18.0   & 77.5 & 32.6 \\
    % PSPNet~\cite{zhao2017pyramid} & 13.7 & 78.8 & 34.5 \\
    % ConvNeXt-T~\cite{liu2022convnet} & 29.0 & 79.0 & 54.4 \\
    % \hline
    
    Swin-T~\cite{liu2021swin}  & 78.1 & 47.3 & 56.3 \\
    SETR~\cite{zheng2021rethinking}  & 79.5 & 63.1 & 60.2 \\
    % Segformer-B0~\cite{xie2021segformer} & 3.4  & 76.2 & 48.8 \\
    % Segformer-B1~\cite{xie2021segformer} & 13.1 & 78.4 & 52.7 \\
    % Segformer-B2~\cite{xie2021segformer} & 24.2 & 81.0 & 59.6 & 56.2 \\
    Segformer-B5~\cite{xie2021segformer}  & 82.4 & 65.8 & 62.0 \\
    \hline
    FAN-B-Hybrid~\cite{zhou2022understanding}  & 82.2 & 67.3 & 60.6 \\
    ~~+~TAP  & 82.7 (+0.5) & 69.2 (+1.9) & 62.7 (+2.1) \\
    ~~+~ADL  & 82.4 (+0.2) & 69.4 (+2.1) & 63.1 (+2.5) \\
    ~~+~TAP \& ADL  & \textbf{82.9 (+0.7)} & \textbf{69.7 (+2.4)} & \textbf{63.7 (+3.1)}\\
    \bottomrule
    \end{tabular}%
}
\end{center}
  \caption{
  % \david{Please include clean performance!} 
  % \david{Please include the difference in parantheses in the first average column.} 
  Comparisons of semantic segmentation models on Cityscapes validation set, Cityscapes-C, and ACDC test set. Both our TAP and ADL greatly improve the robustness. We can further improve the results when combining TAP and ADL together.}
  \label{tab:segmentation}%
  \vspace{-10 pt}
\end{table}%





\subsection{Results on Semantic Segmentation}
\label{subsec:segmentation}


In this part, we further apply our method to semantic segmentation tasks. We train the models on Cityscapes~\cite{cordts2016cityscapes} and evaluate the robustness on two popular benchmarks, including Cityscapes-C~\cite{michaelis2019benchmarking} and ACDC~\cite{sakaridis2021acdc}. Specifically, Cityscapes-C contains 16 corruption types which can be
divided into 4 categories: noise, blur, weather, and digital. ACDC collects the images with adverse conditions,
including night, fog, rain, and snow. In this paper, we report mIoU across diverse datasets. During training, we follow the same settings of SegFormer~\cite{xie2021segformer} to train our models.
We demonstrate that our TAP and ADL also generalize well to segmentation tasks and significantly improve robustness.

\subsubsection{Quantitative Comparisons}




As shown in Table~\ref{tab:segmentation},
compared with the considered baseline model, using either our TAP or ADL individually can greatly improve the robustness on Cityscapes-C and ACDC. With the help of our effective local aggregation module TAP, we highlight that we also obtain a promising improvement of 0.5\% mIoU on clean data. When combining TAP and ADL together, we further improve the performance and obtain a larger improvement of 2.4\% and 3.1\% on Cityscapes-C and ACDC, respectively. Moreover, our best model also significantly outperforms several popular segmentation models with comparable model size. Overall, these results indicate that the proposed two techniques not only work for image classification but also generalize well to semantic segmentation tasks.




\subsubsection{Visual Comparisons}

In this part, we compare the visualization results of the predicted segmentation masks based on examples of diverse robustness benchmarks. As shown in Figure~\ref{fig:visual_segmentation}, for the first example with snow corruptions, the baseline model cannot detect a large region of road (highlighted by the red box), which poses potential risks when applied in some real-world applications, e.g., autonomous driving. Moreover, in the second example with night conditions, the baseline model recognizes a part of the car as a rider and introduces a lot of artifacts in the predicted mask.
By contrast, our model is much more robust and is able to accurately detect most parts of the road and the car in both cases. We highlight that our superiority of robustness can be observed on most examples of the considered benchmarks. Please refer to more visual comparisons in supplementary.





\section{Analysis and Discussions}

In the following, we present further ablation experiments and discussions.
In Section~\ref{subsec:diverse_architecture}, we demonstrate that the proposed two methods are general techniques and can be applied on top of diverse transformer architectures.
In Section~\ref{subsec:hyperparameters}, we study the impact of the number of branches $K$ in TAP and the weight of ADL in the training loss.





\begin{table}[t]
\begin{center}
\resizebox{0.75\columnwidth}{!}
  {
    \begin{tabular}{l|c|c}
    \toprule
    Method & {ImageNet} & {ImageNet-C $\downarrow$}  \\
    \hline
    DeiT-B &  82.0  & 48.5  \\
    ~~+~TAP \& ADL & \textbf{82.4 (+0.4)}  & \textbf{46.6 (-1.9)}  \\
    \hline
    Swin-B &  83.4  & 54.4  \\
    ~~+~TAP \& ADL & \textbf{84.0 (+0.6)}  & \textbf{51.9 (-2.5)}  \\
    \hline
    RVT-B &  82.6  & 46.8  \\
    ~~+~TAP \& ADL & \textbf{83.1 (+0.5)}  & \textbf{44.7 (-2.1)}  \\
    \hline
    FAN-B-Hybrid & 83.9  & 46.1 \\
    ~~+~TAP \& ADL  & \textbf{84.3 (+0.4)}  & \textbf{43.7 (-2.4)} \\
    \bottomrule
    \end{tabular}%
    }
\end{center}
  \caption{Results on top of diverse architectures. We report accuracy and mean corruption error (mCE) on ImageNet and ImageNet-C, respectively. Our method consistently improves robustness and accuracy across different architectures.}
  \label{tab:diverse_architecture}%
  \vspace{-10 pt}
\end{table}%





\subsection{Effectiveness on Diverse Architectures}
\label{subsec:diverse_architecture}

Besides RVT and FAN, we additionally apply our methods on top of more transformer architectures, including DeiT~\cite{TouvronICML2021} and Swin~\cite{liu2021swin}. 
In this experiment, we report the accuracy on ImageNet and robustness in terms of mCE (the lower the better) on ImageNet-C.
As shown in Table~\ref{tab:diverse_architecture}, based on DeiT-B, we greatly improve the corruption robustness by reducing the mCE by 1.9\% and yield a promising improvement of 0.4\% on clean data. As for Swin-B, we obtain a similar observation that our methods are particularly effective in improving corruption robustness, reducing mCE from 54.4\% to 51.9\%. These results indicate that our methods can generalize well across diverse architectures.





\subsection{Impact of Hyperparameters $K$ and $\lambda$}
\label{subsec:hyperparameters}

We conduct ablations on ImageNet-C to study the impact of two hyperparameters of our methods, including the number of branches $K$ in TAP and the weight of ADL.

\noindent
\textbf{The number of branches $K$}.
As detailed in Figure~\ref{fig:tap}, we build our TAP with $K$ branches to enable tokens to consider diverse pooing areas. In fact, the value of $K$ is an important factor for the performance of our method. It is worth noting that $K=1$ is essentially equivalent to the baseline model without TAP. As shown in Figure~\ref{fig:k_lambda} (left), we greatly improve the robustness with a lower mCE score on ImageNet-C when gradually increasing $K$ from 1 to 4. If we further increase $K$, learning weights for too many candidate pooling areas (dilations) for each token becomes increasingly difficult and we cannot observe a significant improvement. 
Although additional branches only introduce minimal overhead in terms of model size and computational complexity, a large $K$ would inevitably require a larger memory footprint. Thus, we choose $K=4$ to obtain the best results at the minimal cost of extra memory footprint.


\begin{figure}[t]
\begin{center}
\includegraphics[width=0.49\linewidth]{figures/ablation_k.pdf}~
\includegraphics[width=0.51\linewidth]{figures/ablation_lambda.pdf}
\end{center}
\vspace{-10 pt}
   \caption{Robustness in terms of mean corruption error (mCE, lower is better) on ImageNet-C against the number of branches $K$ (left) and the importance of our ADL loss $\lambda$ (right). \emph{Left:} When only introducing TAP without ADL, our model consistently outperforms the baseline model when increasing the value of $K$ and yields the best result with $K=4$. \emph{Right:} When using ADL to train the model (without TAP), we observe that a too small or too large $\lambda$ reduces the benefit of our method. In practice, $\lambda=1$ performs best in most cases.}
\label{fig:k_lambda}
\vspace{-10 pt}
\end{figure}


\noindent 
\textbf{Weight of ADL $\lambda$}.
In Figure~\ref{fig:k_lambda} (right), we change the value of $\lambda$ in Eqn.~(\ref{eq:objective}).
In practice, a larger $\lambda$ encourages models to diversify the attention among different rows in the attention map more aggressively.
Given a set of values $\lambda \in \{0.001, 0.01, 0.1, 1, 10\}$, we gradually improve the robustness (reduce mCE score) until $\lambda = 1$. When considering an even larger $\lambda=10$, we observe a significant performance drop since a too large $\lambda$ for ADL may hamper the standard cross-entropy loss during training. In this paper, we set $\lambda=1$ and it generalizes well across all the considered architectures and learning tasks.










































\section{Conclusion}

In this paper, we address the token overfocusing issue in vision transformers (ViTs) such that ViTs tend to rely on very few important tokens in the attention mechanism. In fact, the attention is not robust and often obtains highly diverging attention patterns in the presence of corruptions. 
To alleviate this, we propose two general techniques. First, our Token-aware Average Pooling (TAP) module encourages the local neighborhood of tokens to take part in the self-attention by learning an adaptive average pooling scheme for each token. Second, our Attention Diversification Loss (ADL) explicitly reduces the cosine similarity of attention among tokens.
In practice, we apply our methods to diverse architectures and obtain a significant improvement of robustness on different benchmarks and learning tasks.



{\small
	\bibliographystyle{ieee_fullname}
	\bibliography{merged,survey}
}

\clearpage 

\appendix
\onecolumn

\renewcommand{\thefigure}{\Roman{figure}}
\renewcommand{\thetable}{\Roman{table}}
\renewcommand{\theequation}{\roman{equation}}
\setcounter{figure}{0}
\setcounter{equation}{0}
\setcounter{table}{0}

\begin{center}
	{
		\Large{\textbf{~\\Supplementary Materials for \\ ``Robustifying Token Attention for Vision Transformers''}}
	}
\end{center}



\section{Overview and Outline}
In our paper, we seek to address the token overfocusing issue of vision transformers and improve their overall robustness.
To this end, we propose two general techniques, the {\emph{Token-aware Average Pooling (TAP)}} module and the {\emph{Attention Diversification Loss (ADL)}}.
In this supplementary material, we conduct additional discussions on both techniques and provide complementary experiments.
We organize the supplementary as follows: 


\begin{itemize}[leftmargin=*]
    \setlength\itemsep{-0.1em}

    \item In Section~\ref{supp:complexity}, we discuss the computational complexity of our Token-aware Average Pooling (TAP) module. Based on the considered baseline FAN-B-Hybrid, TAP only takes around 2\% of the number of floating-point operations (FLOPs) in each self-attention layer, and less than 1\% of the whole model.

    
    \item In Section~\ref{supp:quantitative_results}, besides corruption robustness, we demonstrate that the proposed methods also obtain promising improvement in terms of adversarial robustness. Then, we study the effect of the attention threshold $\tau$ in computing our Attention Diversification Loss (ADL). In addition, we provide more results
    for image classification and semantic segmentation.

    

    \item In Section~\ref{supp:visual_results}, we provide more visualization results to study the stability of attention maps in vision transformers. We demonstrate that the token overfocusing issue is particularly severe in relatively deep layers. We also highlight that this issue can be observed across diverse architectures (e.g., RVT~\cite{mao2021towards} and FAN~\cite{zhou2022understanding}) and tasks (including image classification and semantic segmentation). In addition, we provide more visual comparisons for the predicted segmentation masks.

\end{itemize}



\section{Computational Complexity Analysis of TAP}
\label{supp:complexity}





As mentioned in the main paper, we introduce our TAP into each basic block to improve the robustness of the attention mechanism. 
We already demonstrated that our TAP only adds minimal computational overhead.  
Thus, in the following, we evaluate the computational complexity in terms of floating-point operations (FLOPs) to justify our argument.
Given the input tokens $z \in \mmR^{H \times W \times C}$ with the spatial resolution of $H \times W$ and feature dimension of $C$, the complexity of a standard self-attention layer is\footnote{A typical self-attention module consists of a fully-connected layer before and after the attention module, respectively. We compute the overall complexity of all the involved layers, which has been discussed and reported in~\cite{liu2021swin}.}: 
\begin{equation}
    \bigo({\rm SelfAttention}) = 4HWC^2 + 2(HW)^2C
\end{equation}
Based on the standard self-attention layer, we propose to introduce an additional TAP that exploits a dilation predictor (a two-layer convolutional module) to predict the weights for $K$ branches and mix the features in a weighted sum manner. In this sense, the overall complexity consists of the complexities of the dilation predictor ($9HWCK + 9HWK^2$), $K$ average pooling operations ($HWCK$), and the weighted sum operation ($HWCK$). Thus, the complexity introduced by TAP is:
\begin{equation}
    \bigo({\rm TAP}) = 11HWCK + 9HWK^2
\end{equation}
When combining TAP with the standard self-attention together, the overall complexity is:
\begin{equation}
    \bigo({\rm TAP{-}SelfAttention}) = 11HWCK + 9HWK^2 + 4HWC^2 + 2(HW)^2C
\end{equation}
As for our best model built upon FAN-B-Hybrid~\cite{zhou2022understanding}, we have $H{=}W{=}14$, $C{=}448$, and $K{=}4$. By substituting them into the above equations, the cost introduced by our TAP only takes around 2\% of the complexity of each self-attention block. When considering the whole model that consists of both convolutional modules/heads and self-attention blocks, the additional complexity is less than 1\% in practice, showing that our TAP only introduces minimal computational overhead.




\section{More Discussions and Quantitative Results}
\label{supp:quantitative_results}

In this paper, we mainly focus on improving the robustness against common corruptions. Besides this, we additionally investigate whether the improvement can also generalize to adversarial robustness. In this part, we report the robustness against adversarial attacks and demonstrate that both our TAP and ADL greatly improve adversarial robustness. Then, we further conduct an ablation on the effect of the attention threshold $\tau$ for computing our ADL. Moreover, we provide additional comparison results on both image classification and semantic segmentation tasks.




\vspace{3 pt}
\textbf{Comparison of adversarial robustness.} We also evaluate the robustness against adversarial attacks. We follow the settings of RVT~\cite{mao2021towards} to construct the adversarial examples with the number of steps $t=5$ and step size $\alpha=0.5$, namely PGD-5. 
As shown in Table~\ref{tab:adversarial}, compared to the improvement against image corruptions, the proposed methods also obtain comparable improvement against adversarial attacks. 



\begin{table}[htbp]
\vspace{7 pt}
  \begin{center}
    \begin{tabular}{l|ccc}
    \toprule
    Method & \multicolumn{1}{l}{ImageNet $\uparrow$} & \multicolumn{1}{l}{ImageNet-C (mCE) $\downarrow$} & \multicolumn{1}{l}{PGD-5 $\uparrow$} \\
    \hline
    FAN-B-Hybrid~\cite{zhou2022understanding}   &    83.9   &  46.1     &  30.5 \\
    ~~~+TAP   &    84.3   &  44.9 (-1.2)    &  31.4 (+0.9) \\
    ~~~+ADL   &   84.0    &   44.4 (-1.7)    &  31.8 (+1.3) \\
    ~~~+TAP \& ADL &    \textbf{84.3}   &   \textbf{43.7 (-2.4)}     &  \textbf{32.2 (+1.7)} \\
    \bottomrule
    \end{tabular}%
  \end{center}
  \caption{Comparisons of adversarial robustness against PGD attacks on ImageNet. We demonstrate that both our TAP and ADL also obtain promising improvement in terms of adversarial robustness. 
  }
  \label{tab:adversarial}%
\end{table}%



\textbf{Effect of the attention threshold $\tau$.}
According to Eqn.~(1) in the main paper, we use a threshold $\nicefrac{\tau}{N}$ {with $N$ being the number of tokens} to filter out very small values and only focus on the most important ones in the attention map via $\hat A_i^{(l)} = \mathbbm{1} (A_i^{(l)} \geq \nicefrac{\tau}{N} ) \cdot A_i^{(l)}$. Here, we explicitly study the effect of the attention threshold $\tau$ for computing our ADL.
As detailed in Table~\ref{tab:tau}, when using ADL to train the model (without TAP), our ADL only brings marginal improvements {in terms of clean performance on ImageNet. However}, our ADL becomes particularly effective in improving robustness, e.g., greatly reducing mCE on ImageNet-C. In practice, a too small or too large $\tau$ reduces the benefit of our ADL. In our experiments, we set $\tau=2$ performs to obtain the best results.

\begin{table}[htbp]
\vspace{5 pt}
\begin{center}
    \begin{tabular}{c|c|cccc}
    \toprule
    $\tau$   & 0 (Baseline)  & \multicolumn{1}{c}{1} & \multicolumn{1}{c}{2} & \multicolumn{1}{c}{3} & \multicolumn{1}{c}{5} \\
    \hline
    ImageNet $\uparrow$ &   83.9    &  83.9     &    \textbf{84.0}   &   \textbf{84.0}    & 83.9 \\
    ImageNet-C (mCE) $\downarrow$ &   46.1    &   45.1 (-1.0)    &   \textbf{44.4 (-1.7)}    &  45.5 (-0.6)   &  45.8 (-0.3)  \\
    \bottomrule
    \end{tabular}%
\end{center}
    \caption{Comparisons of accuracy on ImageNet and mCE (lower is better) on ImageNet-C across diverse $\tau$. We take FAN-B-Hybrid~\cite{zhou2022understanding} as the baseline and observe that a too small or too large $\tau$ reduces the benefit of ADL. In practice, $\tau=2$ performs best in most cases.}
  \label{tab:tau}%
\end{table}%



\begin{table*}[htbp]
  \begin{center}
  \resizebox{0.95\textwidth}{!}
  {
    \begin{tabular}{l|c|c|c|cc|cc}
    \toprule
    Method & \#Params (M) & \#FLOPs (G) & {ImageNet $\uparrow$} & {ImageNet-C $\downarrow$} & {ImageNet-P $\downarrow$} & {ImageNet-A $\uparrow$} & ImageNet-R $\uparrow$  \\
    \hline
    ResNet50~\cite{HeCVPR2016}    & 25.6   &   4.1      &      76.1 & 76.7 & 58.0 & 0.0 & 36.1  \\
    Inception v3~\cite{SzegedyCVPR2016} & 27.2 & 5.7 & 77.4  & 80.6 & 61.3 & 10.0 & 38.9  \\
    ANT~\cite{RusakECCV2020} & 25.6 & 4.1 & 76.1 & 63.0 & 53.2 & 1.1 & -  \\
    EWS~\cite{guo2022improving} & 25.6 & 4.1  & 77.3 & 58.7 & 30.9 & 5.9 & 48.5 \\
    DeepAugment~\cite{HendrycksARXIV2020} & 25.6 & 4.1 & 75.8  & 60.6 & 32.1 & 3.9 & 46.7 \\
    ConvNeXt-B~\cite{liu2022convnet} & 88.6 & 15.4 & 83.8 & 46.8 & - & 36.7 & 51.3 \\
    % \hline
    DeiT-B~\cite{TouvronICML2021} & 86.6 & 17.6 & 82.0  & 48.5 & 32.1 & 27.4 & 44.9 \\
    ConViT-B~\cite{d2021convit} & 86.5 & 17.7 & 82.4 & 46.9 & 32.2 & 29.0 & 48.4  \\
    % XCiT-S12~\cite{ali2021xcit} & 26.3 & 4.8 & 81.9 & 51.5 & - & 25.0  & 45.5  \\
    XCiT-S24~\cite{ali2021xcit} & 47.7 & 9.1 & 82.6 & 49.4 & - & 27.8  & 45.5  \\
    Swin-B~\cite{liu2021swin} & 87.8 & 15.4 & {83.4} & 54.4 & 32.7 & 35.8 & 46.6 \\
    PVT-Large~\cite{wang2021pyramid} & 61.4 & 9.8 & 81.7 & 59.8 & 39.3 & 26.6 & 42.7 \\
    PiT-B~\cite{heo2021rethinking} & 73.8 & 12.5 & 82.4 & 48.2 & - & 33.9 & 43.7  \\
    T2T-ViT\_t-24~\cite{yuan2021tokens} & 64.1 & 15.0 & 82.6 & 48.0 & 31.8 & 28.9 & 47.9  \\
    \hline
    RVT-B~\cite{mao2021towards} & 91.8 & 17.7 & 82.6  & 46.8 & 31.9  & 28.5 & 48.7 \\
    ~~+~TAP & 92.1 & 17.9  & {83.0 (+0.4)}  & {45.5 (-1.3)} & 30.6 (-1.3) & 30.0 (+1.5) & 49.4 (+0.7)  \\
    ~~+~ADL & 91.8 & 17.7 & {82.6 (+0.0)}  & {45.2 (-1.6)} & 30.2 (-1.7) & 30.8 (+2.3) & 49.8 (+1.1)  \\
    ~~+~TAP \& ADL & 92.1 & 17.9 & \textbf{83.1 (+0.5)}  & \textbf{44.7 (-2.1)} & \textbf{29.6 (-2.3)} & \textbf{32.7 (+4.2)} & \textbf{50.2 (+1.5)} \\
    \hline
    FAN-B-Hybrid~\cite{zhou2022understanding} & 50.4 & 11.7 & 83.9  & 46.1 & 31.3  & 39.6 & 52.7 \\
    ~~+~TAP & 50.7 & 11.8 & {84.3 (+0.4)} & {44.9 (-1.2)} & 30.3 (-1.0)  & 41.0 (+1.4) & 53.9 (+1.2)  \\
    ~~+~ADL & 50.4 & 11.7 & {84.0 (+0.1)}  & {44.4 (-1.7)} & 29.8 (-1.5) & 41.4 (+1.8) & 54.2 (+1.5) \\
    ~~+~TAP \& ADL & 50.7 & 11.8 & \textbf{84.3 (+0.4)}  & \textbf{43.7 (-2.4)} & \textbf{29.2 (-2.1)} & \textbf{42.3 (+2.7)} & \textbf{54.6 (+1.9)} \\
    \bottomrule
    \end{tabular}%
    }
    \end{center}
  \caption{Comparisons on ImageNet and diverse robustness benchmarks. We report the mean corruption error (mCE) on ImageNet-C and mean flip rate (mFR) on ImageNet-P. For these metrics, lower is better. Moreover, we directly report the accuracy on ImageNet-A and ImageNet-R. Based on the considered two baselines, our models consistently improve the accuracy and robustness on diverse benchmarks.}
  \label{tab:sup_imagenet}%
\end{table*}%




\textbf{More results for image classification.} In this part, we provide more comparisons on diverse image classification benchmarks. Besides RVT and FAN, we additionally compare our models with more convolutional models/methods~\cite{HeCVPR2016,SzegedyCVPR2016,RusakECCV2020,HendrycksARXIV2020,liu2022convnet} and a variety of transformer architectures~\cite{TouvronICML2021,d2021convit,ali2021xcit,liu2021swin,wang2021pyramid,heo2021rethinking,yuan2021tokens}. We highlight that our models significantly outperform all the compared methods in terms of both clean accuracy and robustness. To be specific, based on FAN-B-Hybrid, our model outperforms a strong convolutional baseline ConvNeXt-B that contains more parameters by 0.5\% in accuracy on ImageNet and 3.1\% in mCE on ImageNet-C. This phenomenon can also be observed when compared with a popular transformer model DeiT-B. 


\vspace{7 pt}
\textbf{More results for semantic segmentation.} For semantic segmentation, we provide more quantitative results and detailed comparisons on individual corruption types of Cityscapes-C in Table~\ref{tab:sup_segmentation}. Unlike Table~4 in the main paper, we include more popular methods for comparisons on Cityscapes-C. We demonstrate that our TAP and ADL greatly improve the mIoU on Cityscapes-C by 1.9\% and 2.1\%, respectively, along with improved mIoU on the clean Cityscapes dataset. Moreover, our models outperform all the compared methods and achieve the best tradeoff between clean performance and robustness.
In addition, we also show the detailed results on individual corruption types. As shown in Table~\ref{tab:sup_segmentation}, our best model yields the largest improvement mainly on Noise corruptions by ${>}3.2\%$ in terms of mIoU, while obtaining a relatively smaller improvement on blur corruptions. In addition, using TAP alone performs better on some corruption types, including defocus blur, glass blur, and frost.
When combining TAP and ADL, we are able to obtain the best results on most of the corruption types. These results indicate that both the proposed TAP and ADL are general techniques that are able to improve the robustness on diverse tasks and corruption types. 



\begin{table*}[t]
\begin{center}
\resizebox{1\linewidth}{!}
{
    \begin{tabular}{l|c|c|cccc|cccc|cccc|cccc}
    \toprule
    \multirow{2}[0]{*}{Model} & \multirow{2}[0]{*}{Cityscapes} & \multirow{1}[0]{*}{Average mIoU} & \multicolumn{4}{c|}{Blur}      & \multicolumn{4}{c|}{Noise}     & \multicolumn{4}{c|}{Digital}   & \multicolumn{4}{c}{Weather} \\
          &  & on Cityscapes-C    & Motion & Defoc & Glass & Gauss & Gauss & Impul & Shot & Speck & Bright & Contr & Satur & JPEG & Snow & Spatt & Fog  & Frost \\
    \hline
    DeepLabv3+ (R50)~\cite{chen2017rethinking} & 76.6 & 36.8 & 58.5 & 56.6 & 47.2 & 57.7 & 6.5  & 7.2  & 10.0 & 31.1 & 58.2 & 54.7 & 41.3 & 27.4 & 12.0 & 42.0 & 55.9 & 22.8 \\
    DeepLabv3+ (R101)~\cite{chen2017rethinking} & 77.1 & 39.4 & 59.1 & 56.3 & 47.7 & 57.3 & 13.2 & 13.9 & 16.3 & 36.9 & 59.2 & 54.5 & 41.5 & 37.4 & 11.9 & 47.8 & 55.1 & 22.7 \\
    DeepLabv3+ (X65)~\cite{chen2017rethinking} & 78.4 & 42.7 & 63.9 & 59.1 & 52.8 & 59.2 & 15.0 & 10.6 & 19.8 & 42.4 & 65.9 & 59.1 & 46.1 & 31.4 & 19.3 & 50.7 & 63.6 & 23.8 \\
    DeepLabv3+ (X71)~\cite{chen2017rethinking} & 78.6 & 42.5 & 64.1 & 60.9 & 52.0 & 60.4 & 14.9 & 10.8 & 19.4 & 41.2 & 68.0 & 58.7 & 47.1 & 40.2 & 18.8 & 50.4 & 64.1 & 20.2 \\
    \hline
    ICNet~\cite{zhao2018icnet} & 65.9 & 28.0 & 45.8 & 44.6 & 47.4 & 44.7 & 8.4  & 8.4  & 10.6 & 27.9 & 41.0 & 33.1 & 27.5 & 34.0 & 6.3  & 30.5 & 27.3 & 11.0 \\
    FCN8s~\cite{long2015fully} & 66.7 & 27.4 & 42.7 & 31.1 & 37.0 & 34.1 & 6.7  & 5.7  & 7.8  & 24.9 & 53.3 & 39.0 & 36.0 & 21.2 & 11.3 & 31.6 & 37.6 & 19.7 \\
    DilatedNet~\cite{yu2015multi} & 68.6 & 30.3 & 44.4 & 36.3 & 32.5 & 38.4 & 15.6 & 14.0 & 18.4 & 32.7 & 52.7 & 32.6 & 38.1 & 29.1 & 12.5 & 32.3 & 34.7 & 19.2 \\
    % ResNet38 & 77.5 & 32.6  & 54.6 & 45.1 & 43.3 & 47.2 & 13.7 & 16.0 & 18.2 & 38.3 & 60.0 & 50.6 & 46.9 & 14.7 & 13.5 & 45.9 & 52.9 & 22.2 \\
    PSPNet~\cite{zhao2017pyramid} & 78.8 & 34.5 & 59.8 & 53.2 & 44.4 & 53.9 & 11.0 & 15.4 & 15.4 & 34.2 & 60.4 & 51.8 & 30.6 & 21.4 & 8.4  & 42.7 & 34.4 & 16.2 \\
    ConvNeXt-T~\cite{liu2022convnet} & 79.0 & 54.4 & 64.1 & 61.4 & 49.1 & 62.1 & 34.9 & 31.8 & 38.8 & 56.7 & 76.7 & 68.1 & 76.0 & 51.1 & 25.0 & 58.7 & 74.2 & 35.1 \\
    \hline
    SETR~\cite{zheng2021rethinking} & 76.0 & 55.5 & 61.8 & 61.0 & 59.2 & 62.1 & 36.4 & 33.8 & 42.2 & 61.2 & 73.1 & 63.8 & 69.1 & 49.7 & 41.2 & 60.8 & 63.8 & 32.0 \\
    Swin-T~\cite{liu2021swin} & 78.1 & 47.5 & 62.1 & 61.0 & 48.7 & 62.2 & 22.1 & 24.8 & 25.1 & 42.2 & 75.8 & 62.1 & 75.7 & 33.7 & 19.9 & 56.9 & 72.1 & 30.0 \\
    Segformer-B0~\cite{xie2021segformer} & 76.2 & 48.9 & 59.3 & 58.9 & 51.0 & 59.1 & 25.1 & 26.6 & 30.4 & 50.7 & 73.3 & 66.3 & 71.9 & 31.2 & 22.1 & 52.9 & 65.3 & 31.2 \\
    Segformer-B1~\cite{xie2021segformer} & 78.4 & 52.6 & 63.8 & 63.5 & 52.0 & 29.8 & 23.3 & 35.4 & 56.2 & 76.3 & 70.8 & 74.7 & 36.1 & 56.2 & 28.3 & 60.5 & 70.5 & 36.3 \\
    Segformer-B2~\cite{xie2021segformer} & 81.0 & 55.8 & 68.1 & 67.6 & 58.8 & 68.1 & 23.8 & 23.1 & 27.2 & 47.0 & 79.9 & 76.2 & 78.7 & 46.2 & 34.9 & 64.8 & 76.0 & 42.1 \\
    Segformer-B5~\cite{xie2021segformer} & 82.4 & 65.8  & 69.1  & 68.6  & 64.1  & 69.8  & 57.8  & 63.4  & 52.3  & 72.8  & 81.0    & 77.7  & 80.1  & 58.8  & 40.7  & 68.4  & 78.5  & 49.9 \\
    \hline
    FAN-B-Hybrid~\cite{zhou2022understanding} & 82.3 & 67.3 & 70.0 & 69.0 & 64.3 & 69.3 & 55.9 & 60.4 & 61.1 & 70.9 & 81.2 & 76.1 & 80.0 & 57.0 & 54.8 & 72.5 & 78.4 & 52.3 \\
    ~~~+TAP & 82.7 & 69.2 (+1.9)  & 70.1  & \textbf{69.2}  & \textbf{66.6}  & 69.8  & 61.2  & 67.1  & 65.6  & 73.5  & 81.3  & 76.5  & 80.4  & 62.3  & 55.7  & 74.7  & 79.2  & \textbf{54.9} \\
    ~~~+ADL & 82.4 & 69.4 (+2.1)  & 70.1	& 68.6 &	65.3 &	69.7 & 62.6 &	\textbf{68.5} & 66.1 &	73.8 & 81.7 &	77.3 & 	80.8 & {63.3} &	55.3 & 74.3 & 79.7 &	52.8 \\
    ~~~+TAP \& ADL & \textbf{82.9} & \textbf{69.7 (+2.4)}  & \textbf{70.4}  & 68.8  & 65.6  & \textbf{69.8}  & \textbf{63.0}    & {68.4}  & \textbf{67.1}  & \textbf{74.1}  & \textbf{81.8}  & \textbf{77.4}  & \textbf{80.9}  & \textbf{63.5}  & \textbf{56.9}  & \textbf{74.9}  & \textbf{80.0}    & {53.0} \\
    \bottomrule
    \end{tabular}%
}
\end{center}
  \caption{Comparisons of mIoU on individual corruption type of Cityscapes-C based on FAN-B-Hybrid. We obtain the best results on most of the corruption types when combining our TAP and ADL together.}
  \label{tab:sup_segmentation}%
\end{table*}%




\newpage 


\section{More Visualization Results}
\label{supp:visual_results}
In this part, we provide additional visualization results of intermediate attention maps of vision transformers. We demonstrate that the token overfocusing issue can be observed across different layers in a model, different architectures, and the models on semantic segmentation tasks. Then, we show more visual comparisons of segmentation results.




\textbf{Visualization of intermediate attention maps.}
In the main paper, we illustrate the overfocusing issue based on the attention maps of the last layer. Indeed, this issue can be observed across most of the layers. As shown in Figure~\ref{fig:sup_attention_layer}, for the baseline model, the overfocusing issue becomes more and more obvious from 7-th layer to the last layer. More critically, we highlight that all the deep layers focus on the same set of important tokens. When facing image corruptions, e.g., Gaussian noise, we observe a severe attention shift across all the intermediate layers, indicating that the standard self-attention is very fragile.
In contrast, our model adopts a diagonal attention pattern in all the layers and exhibits significantly better stability against image corruptions. We hypothesize that the diagonal pattern plays an important role in stabilizing the attention since we inherently encourage the tokens to preserve most of their own information when aggregating information from other tokens. Although the information from other tokens is relatively weak in each layer, the model is able to gradually extract discriminative features in the end by stacking multiple self-attention layers. We also highlight that the diagonal attention pattern follows a similar fusion manner with the residual architecture~\cite{he2016deep}, which preserves the original information using an identity mapping and extracts new features in the residual branch.



\begin{figure*}[h]
    \centering
   \includegraphics[width=1\linewidth]{figures/attention_all_layers.jpg}
	\caption{
		Attention maps of intermediate layers based on FAN-B-Hybrid. We demonstrate that the token overfocusing issue can be observed in most layers and becomes gradually more serious with the increase of depth. When facing common corruptions, e.g., Gaussian noise, the attention mechanism becomes extremely fragile by focusing on entirely different important tokens. By contrast, our model follows a similar attention pattern (diagonal) across layers and exhibits better stability against corruptions.}
	\label{fig:sup_attention_layer}
\end{figure*}


\textbf{Alleviating token overfocusing issue on top of diverse architectures.}
We have shown the effectiveness of our methods in alleviating the token overfocusing issue based on FAN-B-Hybrid. Here, we additionally take another transformer RVT-B to verify the generalization ability of our methods. From Figure~\ref{fig:sup_overfocus_rvt}, we obtain several important observations. First, the token overfocusing issue also exists in RVT-B and becomes much more serious than that in FAN-B-Hybrid (see the second column of Figure~\ref{fig:sup_overfocus_rvt}). To be specific, the model often relies on less than 5 tokens to compute the self-attention. Second, our TAP and ADL exhibit consistent attention patterns between both RVT and FAN architectures. Clearly, TAP encourages more tokens to take part in the attention mechanism and ADL adopts a diagonal attention pattern in which the attention diversity among rows is high enough. When combing them together, the attention becomes much more stable against image corruptions, sharing a similar observation with Figure~\ref{fig:sup_overfocus_fan}. These results verify our argument that the proposed methods are general techniques that can be applied to diverse architectures. 


\begin{figure*}[t]
    \centering
   \includegraphics[width=0.81\linewidth]{figures/overfocus_rvt.jpg}
	\caption{
		Attention maps of the last layer based on RVT-B. Compared with FAN-B-Hybrid, the overfocusing issue becomes much more serious in RVT-B since the attention relies on fewer important tokens, e.g., often less than 5 tokens. Nevertheless, our TAP and ADL exhibit a similar attention pattern to that on FAN-B-Hybrid, indicating that the proposed methods can generalize well to diverse architectures.}
	\label{fig:sup_overfocus_rvt}
\end{figure*}


\begin{figure*}[t]
    \centering
   \includegraphics[width=0.81\linewidth]{figures/overfocus_fan.jpg}
	\caption{
		Attention maps of the last layer based on FAN-B-Hybrid. We demonstrate that the baseline model tends to rely on very few tokens in the attention mechanism. By contrast, combining both our TAP and ADL obtains more balanced attention across tokens (columns) and diverse attention across rows. More importantly, the attention of our model is very stable against common corruptions.}
	\label{fig:sup_overfocus_fan}
\end{figure*}


\textbf{Alleviating token overfocusing issue on semantic segmentation tasks.}
Besides image classification models, we additionally show the effectiveness of our methods in alleviating the overfocusing issue on semantic segmentation models. In Figure~\ref{fig:sup_attn_segmentation}, we take FAN-B-Hybrid as the backbone to build a segmentation model and show the attention maps of the last layer. Note that the number of tokens in attention maps becomes much larger than that of image classification models due to the extremely large resolution of input images, e.g., often with 2048x1024 in Cityscapes. Because of the increased number of tokens, we observe a slightly different attention pattern in the baseline model such that more tokens are relied upon by the attention mechanism. Nevertheless, the overfocusing issue is still very obvious and the vulnerability of attention against common corruptions can also be observed. When applying our TAP and/or ADL to the segmentation model, we observe a similar attention pattern to that on image classification models shown in Figure~\ref{fig:sup_overfocus_fan}. These results indicate that our method can generalize well to semantic segmentation tasks.





\begin{figure*}[t]
    \centering
    \vspace{-10 pt}
   \includegraphics[width=0.91\linewidth]{figures/attention_seg.jpg}
	\caption{
		Attention maps of the last layer in the segmentation model based on FAN-B-Hybrid backbone. We show that the token overfocusing issue also exists in segmentation models and the attention mechanism is very fragile to image corruptions. By contrast, our TAP and ADL obtains consistent attention pattern for both image classification and semantic segmentation models. These results indicate the generalization ability of our approaches to the semantic segmentation tasks.}
	\label{fig:sup_attn_segmentation}
\end{figure*}



\textbf{More visual comparisons of semantic segmentation.} 
In the main paper, we have shown some examples to demonstrate the superiority of our methods in improving the robustness of semantic segmentation models. Here, we additionally provide the visualization results of more examples. To be specific, we show more visualization results on ACDC and Cityscapes-C in Figure~\ref{fig:sup_segmentation_acdc} and Figure~\ref{fig:sup_segmentation_cityscapesc}, respectively.
In Figure~\ref{fig:sup_segmentation_acdc}, we study the robustness of segmentation models against all four adverse conditions in ACDC dataset, including night, fog, rain, and snow. For clarity, we use the red box to highlight the major differences between different segmentation masks. As for the night example (first row), the baseline model cannot detect the car on the left under the insufficient lighting condition, while our model is still able to detect the car. This phenomenon can be also observed in the fog weather in the second example. In the third and fourth examples, we show that the rain and snow conditions often cause misclassification of the road part. We highlight that road detection plays in important role in autonomous driving scenarios and the robustness against adverse weather conditions becomes critical.
Moreover, we also compare the segmentation results against diverse common corruptions in Cityscapes-C. 
In Figure~\ref{fig:sup_segmentation_cityscapesc}, we further investigate the robustness against common corrutpions. Besides Gaussian noise that we considered in previous experiments, we also show the visualization results on top of other corruptions, including spatter, pixelated, impulse noise, and defocus blur. As for the first two examples in Figure~\ref{fig:sup_segmentation_cityscapesc}, the baseline model cannot accurately detect the bike regardless of whether there is a person on it. In the last two examples, when facing noise and blur, the baseline model cannot detect the whole body of the person, while our model accurately detects the person.
Overall, these results demonstrate the effectiveness of the proposed methods in improving the robustness of semantic segmentation models.








\begin{figure*}[t]
    \centering
   \includegraphics[width=0.97\linewidth]{figures/segmentation_acdc.jpg}
	\caption{
		Visual comparisons of segmentation results on ACDC. When facing adverse conditions, the baseline FAN-B-Hybrid model  often fails to detect cars (in the first three examples) or roads (in the last two examples). By contrast, our model is much more robust against these adverse conditions than the baseline model.}
	\label{fig:sup_segmentation_acdc}
\end{figure*}




\begin{figure*}[t]
    \centering
   \includegraphics[width=0.97\linewidth]{figures/segmentation_cityscapesc.jpg}
	\caption{
		Visual comparisons of segmentation results on Cityscapes-C. When facing image corruptions, the baseline FAN-B-Hybrid model cannot detect the bike (in the first two exampls) and/or the whole body of a person (in the last two examples). By contrast, our model is much more robust against these corruptions.}
	\label{fig:sup_segmentation_cityscapesc}
\end{figure*}










\end{document}