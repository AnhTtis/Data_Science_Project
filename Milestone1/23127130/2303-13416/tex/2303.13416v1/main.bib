@article{nogueira2019document,
  title={Document expansion by query prediction},
  author={Nogueira, Rodrigo and Yang, Wei and Lin, Jimmy and Cho, Kyunghyun},
  journal={arXiv preprint arXiv:1904.08375},
  year={2019}
}

@misc{nogueira2019doc2query-t5,
  title={From doc2query to {docTTTTTquery}},
  author={Nogueira, Rodrigo and Lin, Jimmy},
  year = 2019,
}


@inproceedings{dai2020context,
  title={Context-aware term weighting for first stage passage retrieval},
  author={Dai, Zhuyun and Callan, Jamie},
  booktitle={Proceedings of the 43rd International ACM SIGIR conference on research and development in Information Retrieval},
  pages={1533--1536},
  year={2020}
}
@inproceedings{mallia2021learning,
  title={Learning passage impacts for inverted indexes},
  author={Mallia, Antonio and Khattab, Omar and Suel, Torsten and Tonellotto, Nicola},
  booktitle={Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval},
  pages={1723--1727},
  year={2021}
}
@article{zhao2020sparta,
  title={Sparta: Efficient open-domain question answering via sparse transformer matching retrieval},
  author={Zhao, Tiancheng and Lu, Xiaopeng and Lee, Kyusong},
  journal={arXiv preprint arXiv:2009.13013},
  year={2020}
}
@article{lin2021few,
  title={A few brief notes on deepimpact, coil, and a conceptual framework for information retrieval techniques},
  author={Lin, Jimmy and Ma, Xueguang},
  journal={arXiv preprint arXiv:2106.14807},
  year={2021}
}
@article{zhuang2021fast,
  title={Fast passage re-ranking with contextualized exact term matching and efficient passage expansion},
  author={Zhuang, Shengyao and Zuccon, Guido},
  journal={arXiv preprint arXiv:2108.08513},
  year={2021}
}
@article{formal2021splade,
  title={SPLADE v2: Sparse lexical and expansion model for information retrieval},
  author={Formal, Thibault and Lassance, Carlos and Piwowarski, Benjamin and Clinchant, St{\'e}phane},
  journal={arXiv preprint arXiv:2109.10086},
  year={2021}
}


@inproceedings{zhuang2021tilde,
  title={TILDE: Term independent likelihood moDEl for passage re-ranking},
  author={Zhuang, Shengyao and Zuccon, Guido},
  booktitle={Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval},
  pages={1483--1492},
  year={2021}
}
@inproceedings{formal2021spladev1,
  title={SPLADE: Sparse lexical and expansion model for first stage ranking},
  author={Formal, Thibault and Piwowarski, Benjamin and Clinchant, St{\'e}phane},
  booktitle={Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval},
  pages={2288--2292},
  year={2021}
}
@inproceedings{thakur2021beir,
  title={BEIR: A Heterogeneous Benchmark for Zero-shot Evaluation of Information Retrieval Models},
  author={Thakur, Nandan and Reimers, Nils and R{\"u}ckl{\'e}, Andreas and Srivastava, Abhishek and Gurevych, Iryna},
  booktitle={Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2)},
  year={2021}
}
@article{bai2020sparterm,
  title={SparTerm: Learning term-based sparse representation for fast text retrieval},
  author={Bai, Yang and Li, Xiaoguang and Wang, Gang and Zhang, Chaoliang and Shang, Lifeng and Xu, Jun and Wang, Zhaowei and Wang, Fangshan and Liu, Qun},
  journal={arXiv preprint arXiv:2010.00768},
  year={2020}
}
@inproceedings{macavaney2020expansion,
  title={Expansion via prediction of importance with contextualization},
  author={MacAvaney, Sean and Nardini, Franco Maria and Perego, Raffaele and Tonellotto, Nicola and Goharian, Nazli and Frieder, Ophir},
  booktitle={Proceedings of the 43rd International ACM SIGIR conference on research and development in Information Retrieval},
  pages={1573--1576},
  year={2020}
}
@inproceedings{paria2019minimizing,
  title={Minimizing FLOPs to Learn Efficient Sparse Representations},
  author={Paria, Biswajit and Yeh, Chih-Kuan and Yen, Ian EH and Xu, Ning and Ravikumar, Pradeep and P{\'o}czos, Barnab{\'a}s},
  booktitle={International Conference on Learning Representations},
  year={2019}
}
@inproceedings{louizos2018learning,
  title={Learning Sparse Neural Networks through L\_0 Regularization},
  author={Louizos, Christos and Welling, Max and Kingma, Diederik P},
  booktitle={International Conference on Learning Representations},
  year={2018}
}
@inproceedings{gao2021scaling,
  title={Scaling Deep Contrastive Learning Batch Size under Memory Limited Setup},
  author={Gao, Luyu and Zhang, Yunyi and Han, Jiawei and Callan, Jamie},
  booktitle={Proceedings of the 6th Workshop on Representation Learning for NLP (RepL4NLP-2021)},
  pages={316--321},
  year={2021}
}
@inproceedings{reimers-2019-sentence-bert,
    title = "Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks",
    author = "Reimers, Nils and Gurevych, Iryna",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing",
    month = "11",
    year = "2019",
    publisher = "Association for Computational Linguistics",
    url = "https://arxiv.org/abs/1908.10084",
}
@article{johnson2019billion,
  title={Billion-scale similarity search with gpus},
  author={Johnson, Jeff and Douze, Matthijs and J{\'e}gou, Herv{\'e}},
  journal={IEEE Transactions on Big Data},
  volume={7},
  number={3},
  pages={535--547},
  year={2019},
  publisher={IEEE}
}

@inproceedings{hofstatter2021efficiently,
  title={Efficiently teaching an effective dense retriever with balanced topic aware sampling},
  author={Hofst{\"a}tter, Sebastian and Lin, Sheng-Chieh and Yang, Jheng-Hong and Lin, Jimmy and Hanbury, Allan},
  booktitle={Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval},
  pages={113--122},
  year={2021}
}

@article{sanh2019distilbert,
  title={DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter},
  author={Sanh, Victor and Debut, Lysandre and Chaumond, Julien and Wolf, Thomas},
  journal={arXiv preprint arXiv:1910.01108},
  year={2019}
}


@inproceedings{10.1145/3477495.3531774,
author = {Mallia, Antonio and Mackenzie, Joel and Suel, Torsten and Tonellotto, Nicola},
title = {Faster Learned Sparse Retrieval with Guided Traversal},
year = {2022},
isbn = {9781450387323},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3477495.3531774},
doi = {10.1145/3477495.3531774},
abstract = {Neural information retrieval architectures based on transformers such as BERT are able to significantly improve system effectiveness over traditional sparse models such as BM25. Though highly effective, these neural approaches are very expensive to run, making them difficult to deploy under strict latency constraints. To address this limitation, recent studies have proposed new families of learned sparse models that try to match the effectiveness of learned dense models, while leveraging the traditional inverted index data structure for efficiency.Current learned sparse models learn the weights of terms in documents and, sometimes, queries; however, they exploit different vocabulary structures, document expansion techniques, and query expansion strategies, which can make them slower than traditional sparse models such as BM25. In this work, we propose a novel indexing and query processing technique that exploits a traditional sparse model's "guidance" to efficiently traverse the index, allowing the more effective learned model to execute fewer scoring operations. Our experiments show that our guided processing heuristic is able to boost the efficiency of the underlying learned sparse model by a factor of four without any measurable loss of effectiveness.},
booktitle = {Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {1901–1905},
numpages = {5},
keywords = {query processing, learned sparse retrieval, inverted index},
location = {Madrid, Spain},
series = {SIGIR '22}
}

@article{mackenzie2021wacky,
  title={Wacky weights in learned sparse representations and the revenge of score-at-a-time query evaluation},
  author={Mackenzie, Joel and Trotman, Andrew and Lin, Jimmy},
  journal={arXiv preprint arXiv:2110.11540},
  year={2021}
}
@inproceedings{lin2016toward,
  title={Toward reproducible baselines: The open-source IR reproducibility challenge},
  author={Lin, Jimmy and Crane, Matt and Trotman, Andrew and Callan, Jamie and Chattopadhyaya, Ishan and Foley, John and Ingersoll, Grant and Macdonald, Craig and Vigna, Sebastiano},
  booktitle={European Conference on Information Retrieval},
  pages={408--420},
  year={2016},
  organization={Springer}
}
@inproceedings{zamani2018neural,
  title={From neural re-ranking to neural ranking: Learning a sparse representation for inverted indexing},
  author={Zamani, Hamed and Dehghani, Mostafa and Croft, W Bruce and Learned-Miller, Erik and Kamps, Jaap},
  booktitle={Proceedings of the 27th ACM international conference on information and knowledge management},
  pages={497--506},
  year={2018}
}
@article{lin2021pretrained,
  title={Pretrained transformers for text ranking: Bert and beyond},
  author={Lin, Jimmy and Nogueira, Rodrigo and Yates, Andrew},
  journal={Synthesis Lectures on Human Language Technologies},
  volume={14},
  number={4},
  pages={1--325},
  year={2021},
  publisher={Morgan \& Claypool Publishers}
}
@article{robertson2009probabilistic,
  title={The probabilistic relevance framework: BM25 and beyond},
  author={Robertson, Stephen and Zaragoza, Hugo and others},
  journal={Foundations and Trends{\textregistered} in Information Retrieval},
  volume={3},
  number={4},
  pages={333--389},
  year={2009},
  publisher={Now Publishers, Inc.}
}
@article{karpukhin2020dense,
  title={Dense passage retrieval for open-domain question answering},
  author={Karpukhin, Vladimir and O{\u{g}}uz, Barlas and Min, Sewon and Lewis, Patrick and Wu, Ledell and Edunov, Sergey and Chen, Danqi and Yih, Wen-tau},
  journal={arXiv preprint arXiv:2004.04906},
  year={2020}
}
@article{lee2019latent,
  title={Latent retrieval for weakly supervised open domain question answering},
  author={Lee, Kenton and Chang, Ming-Wei and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1906.00300},
  year={2019}
}
@article{izacard2021unsupervised,
  title={Unsupervised dense information retrieval with contrastive learning},
  author={Izacard, Gautier and Caron, Mathilde and Hosseini, Lucas and Riedel, Sebastian and Bojanowski, Piotr and Joulin, Armand and Grave, Edouard},
  journal={arXiv preprint arXiv:2112.09118},
  year={2021}
}
@inproceedings{nguyen2016ms,
  title={MS MARCO: A human generated machine reading comprehension dataset},
  author={Nguyen, Tri and Rosenberg, Mir and Song, Xia and Gao, Jianfeng and Tiwary, Saurabh and Majumder, Rangan and Deng, Li},
  booktitle={CoCo@ NIPs},
  year={2016}
}
@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}
@article{formal2022distillation,
  title={From Distillation to Hard Negative Sampling: Making Sparse Neural IR Models More Effective},
  author={Formal, Thibault and Lassance, Carlos and Piwowarski, Benjamin and Clinchant, St{\'e}phane},
  journal={arXiv preprint arXiv:2205.04733},
  year={2022}
}
@inproceedings{lin2022proposed,
  title={A proposed conceptual framework for a representational approach to information retrieval},
  author={Lin, Jimmy},
  booktitle={ACM SIGIR Forum},
  volume={55},
  number={2},
  pages={1--29},
  year={2022},
  organization={ACM New York, NY, USA}
}
@article{ash2021investigating,
  title={Investigating the role of negatives in contrastive representation learning},
  author={Ash, Jordan T and Goel, Surbhi and Krishnamurthy, Akshay and Misra, Dipendra},
  journal={arXiv preprint arXiv:2106.09943},
  year={2021}
}

@inproceedings{wolf2020transformers,
  title={Transformers: State-of-the-art natural language processing},
  author={Wolf, Thomas and Debut, Lysandre and Sanh, Victor and Chaumond, Julien and Delangue, Clement and Moi, Anthony and Cistac, Pierric and Rault, Tim and Louf, R{\'e}mi and Funtowicz, Morgan and others},
  booktitle={Proceedings of the 2020 conference on empirical methods in natural language processing: system demonstrations},
  pages={38--45},
  year={2020}
}

@article{hofstatter2020improving,
  title={Improving efficient neural ranking models with cross-architecture knowledge distillation},
  author={Hofst{\"a}tter, Sebastian and Althammer, Sophia and Schr{\"o}der, Michael and Sertkan, Mete and Hanbury, Allan},
  journal={arXiv preprint arXiv:2010.02666},
  year={2020}
}
@article{yang2018anserini,
  title={Anserini: Reproducible ranking baselines using Lucene},
  author={Yang, Peilin and Fang, Hui and Lin, Jimmy},
  journal={Journal of Data and Information Quality (JDIQ)},
  volume={10},
  number={4},
  pages={1--20},
  year={2018},
  publisher={ACM New York, NY, USA}
}

@inproceedings{jang2021ultra,
  title={Ultra-High Dimensional Sparse Representations with Binarization for Efficient Text Retrieval},
  author={Jang, Kyoung-Rok and Kang, Junmo and Hong, Giwon and Myaeng, Sung-Hyon and Park, Joohee and Yoon, Taewon and Seo, Heecheol},
  booktitle={Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing},
  pages={1016--1029},
  year={2021}
}
@article{lin2022dense,
  title={A Dense Representation Framework for Lexical and Semantic Matching},
  author={Lin, Sheng-Chieh and Lin, Jimmy},
  journal={arXiv preprint arXiv:2206.09912},
  year={2022}
}
@inproceedings{lassance2022efficiency,
  title={An Efficiency Study for SPLADE Models},
  author={Lassance, Carlos and Clinchant, St{\'e}phane},
  booktitle={Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval},
  pages={2220--2226},
  year={2022}
}

@inproceedings{devlin2019bert,
  title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  booktitle={Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)},
  pages={4171--4186},
  year={2019}
}

@article{lin2021densifying,
  title={Densifying Sparse Representations for Passage Retrieval by Representational Slicing},
  author={Lin, Sheng-Chieh and Lin, Jimmy},
  journal={arXiv preprint arXiv:2112.04666},
  year={2021}
}


@article{chen2021salient,
  title={Salient Phrase Aware Dense Retrieval: Can a Dense Retriever Imitate a Sparse One?},
  author={Chen, Xilun and Lakhotia, Kushal and O{\u{g}}uz, Barlas and Gupta, Anchit and Lewis, Patrick and Peshterliev, Stan and Mehdad, Yashar and Gupta, Sonal and Yih, Wen-tau},
  journal={arXiv preprint arXiv:2110.06918},
  year={2021}
}
@inproceedings{rekabsaz2021tripclick,
  title={TripClick: the log files of a large health web search engine},
  author={Rekabsaz, Navid and Lesota, Oleg and Schedl, Markus and Brassey, Jon and Eickhoff, Carsten},
  booktitle={Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval},
  pages={2507--2513},
  year={2021}
}
@inproceedings{hofstatter2022establishing,
  title={Establishing Strong Baselines For TripClick Health Retrieval},
  author={Hofst{\"a}tter, Sebastian and Althammer, Sophia and Sertkan, Mete and Hanbury, Allan},
  booktitle={European Conference on Information Retrieval},
  pages={144--152},
  year={2022},
  organization={Springer}
}
@inproceedings{khattab2020colbert,
  title={Colbert: Efficient and effective passage search via contextualized late interaction over bert},
  author={Khattab, Omar and Zaharia, Matei},
  booktitle={Proceedings of the 43rd International ACM SIGIR conference on research and development in Information Retrieval},
  pages={39--48},
  year={2020}
}
@article{xiong2020approximate,
  title={Approximate nearest neighbor negative contrastive learning for dense text retrieval},
  author={Xiong, Lee and Xiong, Chenyan and Li, Ye and Tang, Kwok-Fung and Liu, Jialin and Bennett, Paul and Ahmed, Junaid and Overwijk, Arnold},
  journal={arXiv preprint arXiv:2007.00808},
  year={2020}
}
@inproceedings{zheng2015learning,
  title={Learning to reweight terms with distributed representations},
  author={Zheng, Guoqing and Callan, Jamie},
  booktitle={Proceedings of the 38th international ACM SIGIR conference on research and development in information retrieval},
  pages={575--584},
  year={2015}
}

@inproceedings{robertson-1994-okapi,
  author    = {Stephen E. Robertson and
               Steve Walker and
               Susan Jones and
               Micheline Hancock{-}Beaulieu and
               Mike Gatford},
  editor    = {Donna K. Harman},
  title     = {Okapi at {TREC-3}},
  booktitle = {Proceedings of The Third Text REtrieval Conference, {TREC} 1994, Gaithersburg,
               Maryland, USA, November 2-4, 1994},
  series    = {{NIST} Special Publication},
  volume    = {500-225},
  pages     = {109--126},
  publisher = {National Institute of Standards and Technology {(NIST)}},
  year      = {1994},
  url       = {http://trec.nist.gov/pubs/trec3/papers/city.ps.gz},
  timestamp = {Thu, 12 Mar 2020 11:30:30 +0100},
  biburl    = {https://dblp.org/rec/conf/trec/RobertsonWJHG94.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{10.1145/3576922,
author = {Mackenzie, Joel and Trotman, Andrew and Lin, Jimmy},
title = {Efficient Document-at-a-Time and Score-at-a-Time Query Evaluation for Learned Sparse Representations},
year = {2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1046-8188},
url = {https://doi.org/10.1145/3576922},
doi = {10.1145/3576922},
abstract = {Researchers have had much recent success with ranking models based on so-called learned sparse representations generated by transformers. One crucial advantage of this approach is that such models can exploit inverted indexes for top-k retrieval, thereby leveraging decades of work on efficient query evaluation. Yet, there remain many open questions about how these learned representations fit within the existing literature, which our work aims to tackle using four representative learned sparse models. We find that impact weights generated by transformers appear to greatly reduce opportunities for skipping and early exiting optimizations in well-studied document-at-a-time (DaaT) approaches. Similarly, “off-the-shelf” application of score-at-a-time (SaaT) processing exhibits a mismatch between these weights and assumptions behind accumulator management strategies. Building on these observations, we present solutions to address deficiencies with both DaaT and SaaT approaches, yielding substantial speedups in query evaluation. Our detailed empirical analysis demonstrates that both methods lie on the effectiveness–efficiency Pareto frontier, indicating that the optimal choice for deployment depends on operational constraints.},
note = {Just Accepted},
journal = {ACM Trans. Inf. Syst.},
month = {dec},
keywords = {Efficiency; Indexing; Query Processing; Learned Sparse Retrieval}
}
@article{nair2022learning,
  title={Learning a Sparse Representation Model for Neural CLIR},
  author={Nair, Suraj and Yang, Eugene and Lawrie, Dawn and Mayfield, James and Oard, Douglas W},
  year={2022}
}

@inproceedings{choi2022spade,
  title={SpaDE: Improving sparse representations using a dual document encoder for first-stage retrieval},
  author={Choi, Eunseong and Lee, Sunkyung and Choi, Minijn and Ko, Hyeseon and Song, Young-In and Lee, Jongwuk},
  booktitle={Proceedings of the 31st ACM International Conference on Information \& Knowledge Management},
  pages={272--282},
  year={2022}
}