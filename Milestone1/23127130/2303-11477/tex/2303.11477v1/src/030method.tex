In this paper, we describe our framework for generating tissue patches conditioned on semantic layouts of nuclei. Given a nuclei segmentation mask, we intend to generate realistic synthetic patches. In this section, we (1) describe our data preparation, (2) detail our stain-normalization strategy, (3) review conditional denoising diffusion probabilistic models, (4) outline the network architecture used to condition on semantic label map, and (5) highlight the classifier-free guidance mechanism that we employ at sampling time. 

\subsection{Data Processing} \label{sec:data_process}
We use the Lizard dataset~\cite{graham2021lizard} to demonstrate our framework. This dataset consists of histology image regions of colon tissue from six different data sources at $20\times$ objective magnification. The images are accompanied by full segmentation annotation for different types of nuclei, namely, epithelial cells, connective tissue cells, lymphocytes, plasma cells, neutrophils, and eosinophils. A generative model trained on this dataset can be used to effectively synthesize the colonic tumor micro-environments. The dataset contains $238$ image regions, with an average size of $1055\times934$ pixels. As there are substantial visual variations across images, we construct a representative test set by randomly sampling a 7.5\% area from each image and its corresponding mask to be held-out for testing. The test and train image regions are further divided into smaller image patches of $128\times128$ pixels at two different objective magnifications: (1) at $20\times$, the images are directly split into $128\times128$ pixels patches, whereas (2) at $10\times$, we generate $256\times256$ patches and resize them to $128\times128$ for training. To use the data exhaustively, patching is performed with a $50\%$ overlap in neighboring patches. As such, at (1) $20\times$ we extract a total of 54,735 patches for training and 4,991 patches as a held-out set, while at (2) $20\times$ magnification we generate 12,409 training patches and 655 patches are held out.

\subsection{Stain Normalization}
A common issue in deep learning with H\&E stained histopathology slides is the visual bias introduced by variations in the staining protocol and the raw materials of chemicals leading to different colors across slides prepared at different labs~\cite{bejnordi2014quantitative}. As such, several stain-normalization methods have been proposed to tackle this issue by normalizing all the tissue samples to mimic the stain distribution of a given target slide~\cite{macenko2009method, vahadane2016structure, shrivastava2021self}. In this work, we use the structure preserving color normalization scheme introduce by Vahadane et al.~\cite{vahadane2016structure} to transform all the slides to match the stain distribution of an empirically chosen slide from the training dataset.

% These techniques first convert an RGB image $I$ into Optical Density (OD) as $OD = log{\frac{I_0}{I}}$, where $I_0$ is the total illumination intensity of the image. As the stains now have a linear relationship with the OD values it makes it easier to perform Color Deconvolution (CD) which is expressed as $OD = VS$ where $V$ is the matrix of stain vectors and $S$ is the stain density map. The stain density map can preserve the cell structures of the source image, while the stain vectors are updated to reflect the stain colors of the target image. 



\begin{figure}[t]
\begin{center}
\includegraphics[width=\linewidth]{figures/main.pdf}
\end{center}
\vspace{-0.2in}
   \caption{\textbf{NASDM training framework:} Given a real image $x_0$ and semantic mask $y$, we construct the conditioning signal by expanding the mask and adding an instance edge map. We sample timestep $t$ and noise $\epsilon$ to perform forward diffusion and generate the noised input $x_t$. The corrupted image $x_t$, timestep $t$, and semantic condition $y$ are then fed into the denoising model which predicts $\hat{\epsilon}$ as the amount of noise added to the model. Original noise $\epsilon$ and prediction $\hat{\epsilon}$ are used to compute the loss in~\eqref{eq:loss}.}
\vspace{-0.2in}
\label{fig:main}
\end{figure}


\subsection{Conditional Denoising Diffusion Probabilistic Model}
In this section, we describe the theory of conditional denoising diffusion probabilistic models, which serves as the backbone of our framework. A conditional diffusion model aims to maximize the likelihood $p_{\theta}(x_0 \mid y)$, where data $x_0$ is sampled from the conditional data distribution, $x_0 \sim q(x_0 \mid y)$, and $y$ represents the conditioning signal. A diffusion model consists of two intrinsic processes. The forward process is defined as a Markov chain, where Gaussian noise is gradually added to the data over $T$ timesteps as
\begin{equation}
    \begin{split}
        q(x_t \mid x_{t-1}) &= \mathcal{N}(x_{t}; \sqrt{1 - \beta_t} x_{t-1}, \beta_{t}\mathbf{I}),\\
        q(x_{1:T} \mid x_{0}) &= \prod^{T}_{t=1} q(x_t \mid x_{t-1}),
    \end{split}
\end{equation}
where $\{\beta\}_{t=1:T}$ are constants defined based on the noise schedule. An interesting property of the Gaussian forward process is that we can sample $x_{t}$ directly from $x_0$ in closed form. Now, the reverse process, $p_{\theta} (x_{0:T} \mid y)$, is defined as a Markov chain with learned Gaussian transitions starting from pure noise, $p(x_{T}) \sim \mathcal{N}(0, \mathbf{I})$, and is parameterized as a neural network with parameters $\theta$ as
\begin{equation}
    p_{\theta} (x_{0:T} \mid y) = p(y_T) \prod^{T}_{t=1} p_{\theta} (x_{t-1} \mid x_{t}, y).
\end{equation}
Hence, for each denoising step from $t$ to $t-1$,
\begin{equation}
    p_{\theta}(x_{t-1} \mid x_{t}, y) = \mathcal{N}(x_{t-1}; \mu_{\theta}(x_{t}, y, t), \Sigma_{\theta}(x_{t}, y, t)).
\end{equation}

It has been shown that the combination of $q$ and $p$ here is a form of a variational auto-encoder~\cite{kingma2013auto}, and hence the variational lower bound (VLB) can be described as a sum of independent terms, $L_{vlb} := L_{0} + ... + L_{T-1} + L_{T}$, where each term corresponds to a noising step. As described in Ho et al.~\cite{ho2020denoising}, we can randomly sample timestep $t$ during training and use the expectation $E_{t, x_0, y, \epsilon}$ to estimate $L_{vlb}$ and optimize parameters $\theta$. The denoising neural network can be parameterized in several ways, however, it has been observed that using a noise-prediction based formulation results in the best image quality~\cite{ho2020denoising}. Overall, our NASDM denoising model is trained to predicting the noise added to the input image given the semantic layout $y$ and the timestep $t$ using the loss described as follows:

\begin{equation} \label{eq:loss}
    L_{\text{simple}} = E_{t, x, \epsilon} \left[ \left\| \epsilon - \epsilon_{\theta}(x_t, y, t) \right\|_2 \right].
\end{equation}

Note that the above loss function provides no signal for training $\Sigma_{\theta} (x_t, y, t)$. Therefore, following the strategy in improved DDPMs~\cite{ho2020denoising}, we train a network to directly predict an interpolation coefficient $v$ per dimension, which is turned into variances and optimized directly using the KL divergence between the estimated distribution $p_{\theta}(x_{t-1} \mid x_t, y)$ and the diffusion posterior $q(x_{t-1} \mid x_t, x_0)$ as $L_{\text{vlb}} = D_{KL}(p_{\theta}(x_{t-1} \mid x_t, y) \parallel q(x_{t-1} \mid x_t, x_0))$. This optimization is done while applying a stop gradient to $\epsilon(x_t, y, t)$ such that $L_{\text{vlb}}$ can guide $\Sigma_{\theta}(x_t, y, t)$ and $L_{\text{simple}}$ is the main guidance for $\epsilon(x_t, y, t)$. Overall, the loss is a weighted summation of the two objectives described above as follows:
\begin{equation} \label{eq:objective}
    L_{\text{hybrid}} = L_{\text{simple}} + \lambda L_{\text{vlb}}.
\end{equation}

\subsection{Conditioning on Semantic Mask} \label{sec:cond_on_mask}
NASDM requires our neural network noise-predictor $\epsilon_{\theta}(x_t, y, t)$ to effectively process the information from the nuclei semantic map. For this purpose, we leverage a modified U-Net architecture described in Wang et al.~\cite{wang2022semantic}, where semantic information is injected into the decoder of the denoising network using multi-layer, spatially-adaptive normalization operators. As denoted in Fig.~\ref{fig:main}, we construct the semantic mask such that each channel of the mask corresponds to a unique nuclei type. In addition, we also concatenate a mask comprising of the edges of all nuclei to further demarcate nuclei instances.

\subsection{Classifier-free guidance}
% Samples generated from diffusion models using standard DDPM sampling procedure are fairly diverse but lack photorealism and are not strongly correlated with the conditioning signal. As such, several works~\cite{dhariwal2021diffusion, sohl2015deep, song2020score} present methods to condition the model post-hoc using gradients, $\nabla_{x_t} \log p(y \mid x_t)$, of a classifier that is trained to predict the conditioning signal given the image. However, this requires that the classifier be aware of the noise in the image $x_t$ at intermediate timesteps of the diffusion process and the mechanism is inherently limited, as most information in the noised input is not relevant to predicting the conditioning signal.

To improve the sample quality and agreement with the conditioning signal, we employ classifier-free guidance~\cite{ho2022classifier}, which essentially amplifies the conditional distribution using unconditional outputs while sampling. During training, the conditioning signal, i.e., the semantic label map, is randomly replaced with a null mask for a certain percentage of samples. This leads to the diffusion model becoming stronger at generating samples both conditionally as well as unconditionally and can be used to implicitly infer the gradients of the log probability required for guidance as follows:
\begin{equation}
    \begin{split}
        \epsilon_{\theta} (x_t \mid y) - \epsilon_{\theta} (x_t \mid \emptyset) &\propto \nabla_{x_t} \log p(x_t \mid y) - \nabla_{x_t} \log p(x_t), \\
        &\propto \nabla_{x_t} \log p(y \mid x_t),
    \end{split}
\end{equation}
where $\emptyset$ denotes an empty semantic mask. During sampling, the conditional distribution is amplified using a guidance scale $s$ as follows:
\begin{equation}
    \hat{\epsilon}_{\theta}(x_t \mid y) = \epsilon_{\theta} (x_t \mid y) + s \cdot \left[ \epsilon_{\theta}(x_t \mid y) - \epsilon_{\theta} (x_t \mid \emptyset) \right].
\end{equation}



% And can be described as
% \begin{equation}
%     \begin{split}
%         L_{0} &= -\log p_{\theta} (x_0 \mid x_1), \\
%         L_{t-1} &= D_{KL} (q(x_{t-1} \mid x_t, x_0) \parallel p_{\theta}(x_{t-1} \mid x_{t}, y)),  \\
%         L_{T} &= D_{KL} (q(x_T \mid x_0) \parallel p(x_T)).
%     \end{split}
% \end{equation}

% Note that aside from $L_0$, each term above is a $KL$-divergence between Gaussian distributions and can be evaluated in closed form. Whereas, $L_0$ is discretely computed as the probability of $p_{\theta}(x_0 \mid x_1)$, resulting in the correct discrete pixel using the tractable CDF of the Gaussian distribution. Further, we can see that $L_T$ is not dependent on $\theta$ and can be ignored during training. 


% as
% \begin{equation}
%     \Sigma_{\theta}(x_t, y, t) = \exp(v \log \beta_t + (1-v) \log \hat{\beta_t}),
% \end{equation}
% where $\beta_t$ and $\hat{\beta_t}$ are the upper and lower bounds on the variance given by $q(x_0)$ being either isotropic Gaussian noise or a delta function respectively.


% In this work, we parameterize the network to predict the noise $\epsilon$ added to the data and derive $\mu_{\theta}$ as
% \begin{equation}
%     \mu_{\theta}(x_t, y, t) = \frac{1}{\sqrt{\alpha_t}} \left ( x_t - \frac{\beta_t}{\sqrt{1 - \hat{\alpha_t}}} \epsilon_{\theta}(x_t, y, t) \right).
% \end{equation}