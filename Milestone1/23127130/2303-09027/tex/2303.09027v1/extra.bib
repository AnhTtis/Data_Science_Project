@article{Ecoffet_2021,
	year = 2021,
	volume = {590},
	number = {7847},
	author = {Adrien Ecoffet and Joost Huizinga and Joel Lehman and Kenneth O. Stanley and Jeff Clune},
	title = {First return, then explore},
	journal = {Nature}
}

@inproceedings{sivaraman2014experimental,
  title={An Experimental Study of the Learnability of Congestion Control},
  author={Anirudh Sivaraman and Keith Winstein and Pratiksha Thaker and Hari Balakrishnan},
  booktitle={ACM SIGCOMM},
  year={2014}
}

@article{Sharpe94,
    title={The Sharpe Ratio},
    author={Sharpe, William F.},
    journal={The Journal of Portfolio Management},
    volume=21,
    number=1,
    year=1994
}

@inproceedings{CPA08,
    title= {Dynamic cost-per-action mechanisms and applications to online advertising},
    author={Hamid Nazerzadeh and Amin Saberi and Rakesh Vohra},
    booktitle = {WWW},
    year = 2008
}

@article{mujoco,
    title={MuJoCo: A physics engine for model-based control},
    author={Emanuel Todorov and Tom Erez and Yuval Tassa},
    journal={2012 IEEE/RSJ International Conference on Intelligent Robots and Systems},
    pages={5026–5033},
    year=2012
}

 @article{Silver_Singh_Precup_Sutton_2021, title={Reward is enough}, volume={299},  journal={Artificial Intelligence}, author={Silver, David and Singh, Satinder and Precup, Doina and Sutton, Richard S.}, year={2021}, month={Oct}, 
 }

@inproceedings{Abel_Dabney_Harutyunyan_Ho_Littman_Precup_Singh_2021,
    title={On the Expressivity of Markov Reward},
    author={Abel, David and Dabney, Will and Harutyunyan, Anna and Ho, Mark K. and Littman, Michael L. and Precup, Doina and Singh, Satinder}, 
    booktitle = {NeurIPS},
    pages = {7799--7812},
    publisher = {Curran Associates, Inc.},
    year={2021}
}

 @article{Arora_Doshi_2021, title={A survey of inverse reinforcement learning: Challenges, methods and progress}, volume={297}, 
 journal={Artificial Intelligence}, 
 author={Arora, Saurabh and Doshi, Prashant}, year={2021}}

 @inproceedings{Xu_vanHasselt_Hessel_Oh_Singh_Silver_2020, title={Meta-gradient reinforcement learning with an objective discovered online}, volume={33},
 booktitle={NeurIPS},
 author={Xu, Zhongwen and van Hasselt, Hado P and Hessel, Matteo and Oh, Junhyuk and Singh, Satinder and Silver, David}, 
 year={2020}
}


@inproceedings{ReymondHayesRoijersSteckelmacherNowe21,
	author = {Mathieu Reymond and Conor F. Hayes and Diederik M. Roijers and Denis Steckelmacher and Ann Now{\'e}},
	booktitle = {MODeM},
	title = {Actor-Critic Multi-Objective Reinforcement Learning for Non-Linear Utility Functions},
	year = {2021}}


@inproceedings{ZhangWeng21,
	author = {Jianyi Zhang and Paul Weng},
	booktitle = {International Conference on Distributed Artificial Intelligence (DAI)},
	title = {Safe Distributional Reinforcement Learning},
	year = {2021}}

 @inproceedings{Agarwal_Liang_Schuurmans_Norouzi_2019, title={Learning to Generalize from Sparse and Underspecified Rewards}, 
 booktitle={ICML},  author={Agarwal, Rishabh and Liang, Chen and Schuurmans, Dale and Norouzi, Mohammad}, year={2019}, publisher = {PMLR}, address = {Long Beach}, pages = 	 {130--140},
 }

 @inproceedings{Suay_Brys_Taylor_Chernova_2016, title={Learning from Demonstration for Shaping through Inverse Reinforcement Learning}, url={https://dl.acm.org/doi/10.5555/2936924.2936988}, booktitle={AAMAS}, author={Suay, Halit Bener and Brys, Tim and Taylor, Matthew E. and Chernova, Sonia}, year={2016} }

 @inproceedings{Zheng_Oh_Singh_2018, title={On learning intrinsic rewards for policy gradient methods}, booktitle={NeurIPS}, author={Zheng, Zeyu and Oh, Junhyuk and Singh, Satinder},
 year={2018} }


@article{VinyalsBabuschkin19,
	author = {Vinyals, Oriol and Babuschkin, Igor and Czarnecki, Wojciech M. and Mathieu, Micha{\"e}l and Dudzik, Andrew and Chung, Junyoung and Choi, David H. and Powell, Richard and Ewalds, Timo and Georgiev, Petko and Junhyuk Oh and Horgan, Dan and Kroiss, Manuel and Danihelka, Ivo and Huang, Aja and Sifre, Laurent and Cai, Trevor and John Agapiou and Jaderberg, Max and Vezhnevets, Alexander S. and Leblond, R{\'e}mi and Pohlen, Tobias and Dalibard, Valentin and Budden, David and Sulsky, Yuri and Molly, James and Paine, Tom L. and Gulcehre, Caglar and Wang, Ziyu and Pfaff, Tobias and Wu, Yuhai and Ring, Roman and Yogatama, Dani and W{\"u}nsch, Dario and McKinney, Katrina and Smith, Oliver and Schaul, Tom and Lillicrap, Timothy and Kavukcuoglu, Koray and Hassabis, Demis and Apps, Chris and Silver, David},
	journal = {Nature},
	number = {7782},
	pages = {350--354},
	title = {Grandmaster level in {StarCraft II} using multi-agent reinforcement learning},
	volume = {575},
	year = {2019}}

 @inproceedings{Lee_Smith_Abbeel_2021, title={PEBBLE: Feedback-Efficient Interactive Reinforcement Learning via Relabeling Experience and Unsupervised Pre-training}, booktitle={Proceedings of the 38th International Conference on Machine Learning},
 author={Lee, Kimin and Smith, Laura M. and Abbeel, Pieter}, year={2021}
 }

@INPROCEEDINGS{Neurogammon,
  author={Tesauro, G.},
  booktitle={IJCNN}, 
  title={Neurogammon: a neural-network backgammon program},
  year={1990},
  volume={3},
}

@misc{brockman_openai_2016,
	title = {{OpenAI} {Gym}},
	url = {http://arxiv.org/abs/1606.01540},
	abstract = {OpenAI Gym is a toolkit for reinforcement learning research. It includes a growing collection of benchmark problems that expose a common interface, and a website where people can share their results and compare the performance of algorithms. This whitepaper discusses the components of OpenAI Gym and the design decisions that went into the software.},
	urldate = {2022-10-27},
	publisher = {arXiv},
	author = {Brockman, Greg and Cheung, Vicki and Pettersson, Ludwig and Schneider, Jonas and Schulman, John and Tang, Jie and Zaremba, Wojciech},
	month = jun,
	year = {2016},
	note = {arXiv:1606.01540 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\kyle1\\Zotero\\storage\\BCAYF6HG\\Brockman et al. - 2016 - OpenAI Gym.pdf:application/pdf},
}

@article{autoRL,
	title = {Automated {Reinforcement} {Learning} ({AutoRL}): {A} {Survey} and {Open} {Problems}},
	volume = {74},
	issn = {1076-9757},
	shorttitle = {Automated {Reinforcement} {Learning} ({AutoRL})},
	url = {http://arxiv.org/abs/2201.03916},
	doi = {10.1613/jair.1.13596},
	urldate = {2022-10-28},
	journal = {jair},
	author = {Parker-Holder, Jack and Rajan, Raghu and Song, Xingyou and Biedenkapp, André and Miao, Yingjie and Eimer, Theresa and Zhang, Baohe and Nguyen, Vu and Calandra, Roberto and Faust, Aleksandra and Hutter, Frank and Lindauer, Marius},
	month = jun,
	year = {2022},
	note = {arXiv:2201.03916 [cs]},
	keywords = {68T01, Computer Science - Machine Learning, I.2.6},
	pages = {517--568},
	annote = {Comment: Published in JAIR. Co-first authors and co-last authors are listed in alphabetical order},
}

 @inproceedings{mininet,
    title={A network in a laptop: Rapid prototyping for software-defined networks},
    booktitle={Proceedings of the 9th ACM SIGCOMM Workshop on Hot Topics in Networks},
    author={Bob Lantz and Brandon Heller and Nick McKeown},
    year={2010}
 }