\section{Introduction}

The aim of this paper is to extend \emph{quantitative} techniques of {\it static analysis} based on \emph{multi-types} to programs with {\it effects}.

{\bf Effectful Programs.} Programming languages produce different kind of \emph{effects} (observable interactions with the environment),  such as handling exceptions, read/write from a global memory outside its own scope, using a database or a file, performing non-deterministic choices, or using sample probabilistic functions. The degree to which these side effects are used depends on each programming paradigm~\cite{Jones1993} (imperative programming makes use of them while declarative programming does not). In general, avoiding the use of side effects facilitates the formal verification of programs, thus allowing to (statically) ensure their correctness. Thus for example, the functional language Haskell  eliminates side effects by replacing them with {\it monadic} actions, a clean approach which continues to attract growing attention. Indeed, rather than writing a function that returns a raw type, an effectful function returns a raw type inside a useful wrapper -- where that wrapper is a monad~\cite{Wadler1993}. This approach allows programming languages to combine the qualities of both the imperative and declarative worlds: programs produce effects, but these are encoded in such a way that formal verifications  can be performed very conveniently.

{\bf Quantitative Properties.} We address quantitative properties of programs with effects using {\it multi-types}, which originate in the theory of \emph{intersection} type  systems. They extend simple types with a new constructor $\cap$ in such a way that a program $t$ is typable with $\sigma \cap \tau$ if $t$ is typable with both types $\sigma$ and $\tau$ independently. Intersection types were originally introduced as \emph{models} capturing computational properties of functional programming in a broader sense~\cite{CDC78}. For example, termination of different evaluations strategies can be characterized by typability in some appropriate intersection type system: a program $t$ is terminating if and only if $t$ is typable. Originally, intersection enjoys associativity, commutativity, and in particular idempotency (\ie\ $\sigma \cap \sigma = \sigma$). By changing to a \emph{non-idempotent} intersection constructor, one naturally comes to represent types by multisets, which is why they are called multi-types. Just like their idempotent precursors, multi-types still allow for a characterization of several operational properties of programs, but they also grant a substantial improvement: they provide  quantitative measures about these properties. For example, it is still possible to prove that a program is terminating if and only if it is typable, but now an {\it upper bound} or {\it exact measure} for the time needed for its evaluation length can be derived from the typing derivation of the program. This shift of perspective, from idempotent to non-idempotent types, goes beyond lowering the logical complexity of the proof: the quantitative information provided by typing derivations in the non-idempotent setting unveils crucial quantitative relations between typing (static) and reduction (dynamic) of programs.

{\bf Upper Bounds and Exact Split Measures.} Multi-types are extensively used to reason about programming languages from a quantitative point of view, as pioneered by de Carvalho \cite{deCarvalho2007,deCarvalho2018}. For example, they are able to provide \emph{upper bounds}, in the sense that the evaluation length of a program $t$ {\it plus} the size of its result (called {\it normal form}) can be bounded by the size of the type derivation of $t$. A major drawback of this approach, however, is that the size of normal forms can be exponentially bigger than the length of the evaluation reaching those normal forms. This means that bounding the sum of these two integers at the same time is too rough, and not very relevant from a quantitative point of view. Fortunately, it is possible to extract better measures from a multi-type system. A crucial point to obtain \emph{exact measures}, instead of upper bounds, is to consider minimal type derivations, called \emph{tight}. Moreover, using appropriate refined tight systems it is also possible to obtain \emph{independent} measures (called \emph{split} exact measures) for {\it length} and for {\it size}. More precisely, the quantitative typing systems are now equipped with constants and counters, together with an appropriate notion of tightness, which encodes minimality of type derivations. For any tight type derivation $\Phi$ of a program $t$ with counters $b$ and $d$, it is now possible to show that $t$ evaluates to a normal form of size $d$ in exactly $b$ steps. Therefore, the type system is not only \emph{sound}, \ie\ it is able to {\it guess} the number of steps to normal form as well as the size of this normal form, but the opposite direction providing \emph{completeness} of the approach also holds.

{\bf Contribution.} The focus of this paper is on effectful computations such as reading and writing on a global memory. Taking inspiration from the monadic approach adopted in~\cite{deLiguoroT21}, we design a tight quantitative type system that provides split exact measures. More precisely, our system is not only capable of discriminating between length of evaluation to normal form and size of the normal form, but the measure corresponding to the length of the evaluation is split into two different integers: the first one corresponds to the length of standard computation ($\beta$-reduction) and the second one to the number of memory accesses. We show that the system is sound \ie\ for any tight type derivation $\Phi$ of $t$ ending with counters $(b,m,d)$, the term $t$ is normalisable by performing $b$ evaluation steps and $m$ memory accesses, yielding a normal form having size $d$. The opposite direction, giving completeness of the model, is also proved.

In order to  gradually present the material, we first develop the technique for a weak (open) call-by-value (CBV) calculus, which can be seen as a contribution per se, and then we encapsulate these preliminary ideas in the  general framework of the language with global state.

{\bf Summary.} \cref{s:wocbv} illustrates the technique on a weak (open) CBV calculus. We then lift the technique to the $\lam$-calculus with global state in \cref{s:lambda-calculus-with-state} by following the same methodology. More precisely, \cref{s:syntax-gs} introduces the $\lamcc$-calculus, \cref{s:typing-system-gs}  defines a  quantitative type system $\sysgs$. Soundness and completeness of $\sysgs$ w.r.t. $\lamcc$ are proved in \cref{s:sound-complete-lambda-gs}.  We conclude and discuss related work in \cref{s:conclusion}. Due to space limitation we cannot include all proofs, but they are available in~\cite{AKR23full}. 

{\bf Preliminary General Notations.}  We start with some general
notations. Given a (one-step) reduction relation $\red[\rel]$,
$\redn[\rel]$ denotes the reflexive-transitive closure of
$\red[\rel]$. We write $t \rred^b u$ for a reduction sequence from $t$
to $u$ of length $b$. A term $t$ is said to be (1) in
\defn{$\rel$-normal form} (written $t \not \red[\rel]$) iff there is
no $u$ such that $t \red[\rel] u$, (2) \defn{$\rel$-weakly
  normalizing} (written $t \in \wn\rel$) iff there is some $\rel$-nf
$u$ such that $t \redn[\rel] u$, (3) \defn{$\rel$-strongly
  normalizing} (written $t \in \sn\rel$) iff there is no infinite
$\rel$-reduction sequence starting at $t$.  $\rel$ is weakly
(resp. strongly) normalizing iff every term is $\rel$-weakly
(resp. $\rel$-strongly) normalizing.

