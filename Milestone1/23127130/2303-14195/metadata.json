{
    "arxiv_id": "2303.14195",
    "paper_title": "The limited-memory recursive variational Gaussian approximation (L-RVGA)",
    "authors": [
        "Marc Lambert",
        "Silv√®re Bonnabel",
        "Francis Bach"
    ],
    "submission_date": "2023-03-24",
    "revised_dates": [
        "2023-03-28"
    ],
    "latest_version": 1,
    "categories": [
        "cs.DS"
    ],
    "abstract": "We consider the problem of computing a Gaussian approximation to the posterior distribution of a parameter given a large number N of observations and a Gaussian prior, when the dimension of the parameter d is also large. To address this problem we build on a recently introduced recursive algorithm for variational Gaussian approximation of the posterior, called recursive variational Gaussian approximation (RVGA), which is a single pass algorithm, free of parameter tuning. In this paper, we consider the case where the  parameter dimension d is high, and we propose a novel version of RVGA that scales linearly in the dimension d  (as well as  in the number of observations N),  and which  only requires linear storage capacity in d. This is afforded by the use of a novel recursive expectation maximization (EM) algorithm applied for factor analysis introduced herein, to approximate at each step the covariance matrix of the Gaussian distribution conveying the uncertainty in the parameter. The approach is successfully illustrated on the problems of high dimensional least-squares and logistic regression, and generalized to a large class of nonlinear models.",
    "pdf_urls": [
        "http://arxiv.org/pdf/2303.14195v1"
    ],
    "publication_venue": "Statistics and Computing, In press"
}