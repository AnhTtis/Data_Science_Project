

\section{Experiments}
\subsection{Dataset} %TODO: 
% The third ABAW competition includes four challenges: i) uni-task Valence-Arousal Estimation, ii) uni-task Expression Classification, iii) uni-task Action Unit Detection, and iv) MultiTask-Learning. All challenges are based on a common benchmark database, Aff-Wild2, a large-scale field database and the first to be annotated according to valence-arousal, expression, and action units. 
% the Aff-Wild2 database extends the Aff-Wild dataset, with more videos and annotations for all behavior tasks.
% The Valence-Arousal Estimation Challenge contains 567 videos, that have been annotated by four experts using the method proposed in \cite{cowie2000feeltrace}.


% As for the visual feature extractors, the FER+, RAF-DB, and AffectNet datasets are used for pre-training. 
% The RAF-DB is a large-scale database of facial expressions, which includes about 30,000 images of a wide variety of faces downloaded from the Internet. We use the single-label subset in RAF-DB, including 7 classes of basic emotion.
% AffectNet dataset contains over one million facial images, collected from the Internet. Approximately half of the retrieved images (approximately 440,000) were manually annotated for the presence of seven discrete facial expressions (classification model) as well as the intensity of value and arousal. 
% In addition, an authorized commercial FAU dataset is also used to pre-train an visual feature extractor. It contains 7K images in 15 face action unit categories(AU1, AU2, AU4, AU5, AU6, AU7, AU9, AU10, AU11, AU12, AU15, AU17, AU20, AU24, and AU26).

Expression(Expr) Classification Challenge in fifth ABAW competition is based on Aff-Wild2, a large-scale dataset. Aff-Wild2 is consists of 548 videos and is annotated with 8 expressions (i.e. neutral, anger, disgust, fear, happiness, sadness, surprise and other). 
As for the feature extractors, we used some other dataset for pretraining, which consists of FER+\cite{BarsoumICMI2016}, RAF-DB\cite{li2017reliable, li2019reliable} and AffectNet\cite{mollahosseini2017affectnet}. 
In addition, an authorized commercial FAU dataset is also used for pretraining visual feature extractor, which consists of 7K images in 15 face action unit categories(AU1, AU2, AU4, AU5, AU6, AU7, AU9, AU10, AU11, AU12, AU15, AU17, AU20, AU24, and AU26).
As for the audio feature extractors, we used some different open-source models to extract features. Wav2Vec 2.0\cite{baevski2020wav2vec}, HuBERT\cite{hsu2021hubert} and ECAPA-TDNN\cite{desplanques2020ecapa} are the deep open-source model for extracting audio features.

\subsection{Evaluation Metric}
According to the competition regulations, we use the average F1 score across 8 categories, which can be formulated as 
\begin{small}
\begin{equation}
    \label{eq:metric}
    \centering
    p = \frac{\sum_{i}^{8}F1(\hat{y}_{i}, y_{i})}{8}
\end{equation}
\end{small}
where $F1$ denotes F1 score, $\hat{y}_{i}$ and $y_{i}$ denotes the $i$-th category of prediction and label respectively.

\subsection{Experiment Settings}
% The models are trained on Nvidia GeForce GTX 1080 Ti GPUs, 
% each with 11GB memory, and with the Adam \cite{kingma2014adam} optimizer. The results reported in the following experiments are based on the average score of 3 random runs.
% The model is trained for 30 epochs, the batch size is 16 and the dropout rate is 0.3.
% As for the LSTM model, the learning rate is 0.0003, the dimension of multimodal features and the hidden size are 512, the length of video segments is 100, the number of regression layers is 2 and the hidden sizes are \{512, 256\} respectively.

% As for the transformer encoder model, the learning rate is 0.0002, the length of video segments is 250, the stride of segments is 250 or 100, the dimension of multimodal features is 512, the number of encoder layers is 4, the number of attention heads is 4, the dimension of feed-forward layers in the encoder is 1024, the number of regression layers is 2 and the hidden size of regression layers are \{512, 256\} respectively.
%As for the transformer encoder model, two sets of hyper-parameters are used, which are called \textbf{TRM-v1} and \textbf{TRM-v2}. 
%The hyper-parameters of TRM-v1 and -v2 are shown as follows respectively: the learning rate is \{0.0002, 0.0003\}, the length of video segments is \{250, 250\}, the stride of segments is \{250, 100\}, the dimension of multi-modal features is \{256, 512\}, the numbers of encoder layers is \{4, 4\}, the number of attention heads is \{4, 4\}, the dimension of feed forward layers in the encoder is \{1024, 512\}, the number of regression layers is \{2, 2\} and the hidden size of regression layers are \{256, 256\} for TRM-v1 and \{512, 256\} for TRM-v2. 
% As for the smooth function in the post-processing stage, the size of the smoothing window is 
% 20 for valence and 50 for arousal.

First declare that we used Adam\cite{kingma2014adam} optimizer to train models for 25 epochs.
As for the Transformer model, learning rate is 0.0001, the $\alpha$ in equation\ref{eq:loss} is $5$, the affine dimension is $1024$, the number of Transformer encoder layers is $4$, attention heads number is $4$, the dropout ratio in Transformer encoder layer is $0.3$, the sequence length of one segement is $128$, and the hidden size of head layers are \{512, 256\}.

\begin{table}[]
\begin{center}
  \caption{The performance of our method on the 5-fold cross-validation. The First 4 folds are from train set, and the last fold is the original validation set.}
  \label{tab:5fold}
\begin{tabular}{c|c}
\hline
         & F1       \\ \hline
Fold 1   & 0.43697  \\
Fold 2   & 0.38015  \\
Fold 3   & 0.35646  \\
Fold 4   & 0.39170  \\
Fold 5   & 0.38146  \\
Average  & 0.38935  \\ \hline
\end{tabular}
\end{center}
\end{table}

\subsection{Overall Performance on Validation Set}

% Table \ref{tab:overall_performence} shows the experimental results of our proposed method on the validation set of the Aff-Wild2 dataset. The Concordance Correlation Coefficient (CCC) is used as the evolution metric for both valence and arousal prediction tasks. As is shown in the table, our proposed transformer encoder structure achieves the best performance for both valence and arousal, and the LSTM structure achieves competitive performance for arousal as well. It proves the effectiveness of each of our proposed structures. 

Table \ref{tab:overall_performence} shows the results of our method on validation set. Among all the results we post in table, we utilized the same training settings as we described in experiment settsings. As the result we post, different feature combinations can lead to different effects.



\subsection{Model Ensemble}
% In order to further improve the performance of our proposed models, we apply a model ensemble strategy to these models. We train some models with different basic structures, hyper-parameters and combinations of features, and get the predictions of them respectively in the testing stage. Then, the average value of the prediction of these models is taken as the final prediction. 

During the model selection phase, we trained some models with different structures, combinations of features and hyper-parameters, and achieve competitive performence. Vote strategy is employed to improve robustness and perfermence of our final prediction, which is described in table \ref{tab:ensemble}. As the results we post, ensemble different architecture and differnet feature combinations can lead to more benefits on validation set.

\begin{table}[]
\begin{center}
  \caption{The results of each single model and the ensemble of them for the expression prediction task on the validation set.}
  \label{tab:ensemble}
\resizebox{\columnwidth}{!}{
\begin{tabular}{c|c|c}
\hline
Model           & Features                                          & F1                \\ \hline
Transformer     & mae,ires100,wav2vec,ecapatdnn,hubert              & 0.38362           \\
Transformer     & ires100,fau,hubert,wav2vec,ecapatdnn              & 0.35119           \\
Transformer     & ires100,fau,densenet,ecapatdnn,hubert             & 0.36087           \\
Transformer     & mae,ires100,densenet,ecapatdnn,hubert             & 0.39380           \\
LSTM            & densenet,mae,ires100,wav2vec,ecapatdnn,hubert     & 0.40178           \\
LSTM            & mae,ires100,wav2vec,ecapatdnn,hubert              & 0.40832           \\
LSTM            & fau,ires100,ecapatdnn,hubert                      & 0.38928           \\
LSTM            & densenet,mae,ires100,ecapatdnn,hubert             & 0.40889           \\
LSTM            & densenet,mae,ires100,ecapatdnn,hubert             & 0.41410           \\ \hline
Ensemble        &                                                   & \textbf{0.45774}  \\ \hline
\end{tabular}
}
\end{center}
\end{table}

\subsection{Ablation Study}
% In this section, we conduct an ablation analysis of different features to compare their contribution of them. Table \ref{tab:ablation} shows the results of the ablation study for our proposed visual and audio features. The transformer-based structure is used for the ablation study.

Table \ref{tab:ablation} shows the abalation study perfermence on validation set, we utilized transformer-based model to compare the benefits of different features and feature combinations. Among all the experimental results, we utilized the same experimental setup in the training phase, except for the feature combinations.

\begin{table}[]
\begin{center}
  \caption{Ablation study of features on the validation set.}
  \label{tab:ablation}
\resizebox{\columnwidth}{!}{
\begin{tabular}{c|c|c}
\hline
Visual                      & Audio                     & F1        \\ \hline
fau,densenet                & ecapatdnn                 & 0.35276   \\ 
fau,densenet                & fbank,ecapatdnn           & 0.33887   \\ 
fau,densenet                & ecapatdnn,hubert          & 0.34391   \\ 
fau,ires100,densenet        & ecapatdnn,hubert          & 0.36266   \\
fau,ires100,densenet        & wav2vec,ecapatdnn         & 0.36944   \\
mae,ires100,densenet        & ecapatdnn,hubert          & 0.39380   \\ \hline
\end{tabular}
}
\end{center}
\end{table}



% \subsection{Test Performance}
% In this section, we briefly introduce our strategies for submissions and show the performance of our proposed method on the test set. Table \ref{tab:our_test} shows the strategies and results for each of our five submissions. As for the 1st, 2nd and 4th submissions, we apply the simple training and validation strategy, where we only train the models on the official training set and choose the models with the best performance on the official validation set. Specifically, we ensemble only 3 or 4 models to get the predictions for the 1st submission, and ensemble more models with more variations of feature combinations for the 2nd and 4th submission. For example, the model and feature combination of the 2nd submission is shown in Table \ref{tab:valence} and \ref{tab:arousal}. 



% \begin{table}[]
% \begin{center}
%   \caption{The results on the test set of different submissions.}
%   \label{tab:our_test}
 
% \begin{tabular}{c|c|ccc}
% \hline
% Submit & Strategy      & Valence         & Arousal         & Mean            \\ \hline
% 1      & Ensemble 1    & 0.5605          & 0.5165          & 0.5385          \\
% 2      & Ensemble 2    & 0.5779          & 0.5781          & 0.5780          \\
% 3      & Train-Val-Mix & \textbf{0.6060} & 0.5960          & \textbf{0.6010} \\
% 4      & Ensemble 3    & 0.5898          & \textbf{0.6018} & 0.5958          \\
% 5      & 5-Fold        & 0.5929          & 0.5985          & 0.5957          \\ \hline
% \end{tabular}

% \end{center}
% \end{table}

% Moreover, as for the 3rd and 5th submissions, we propose two additional training and validation strategies, including Train-Val-Mix and 5-Fold.
% Specifically, as for the Train-Val-Mix strategy, we mix up the training and validation set, and use both of them for training. In order to choose models with nice and stable performance without data for validation, we empirically choose the models from 16 to 25 epochs in the training stage for Arousal, and from 11 to 16 epochs for Valence. Finally, we ensemble all these models to get test results.
% As for the 5-Fold strategy, we mix up the training and validation set, and divide them into five folds. For each time, one fold is used for validation, and the rest four folds are used for training. Since we get five models with five folds, we ensemble these models to get test results.
% As is shown in the table, the Train-Val-Mix strategy achieves the best test performance, and the 5-Fold strategy also achieves competitive performance, which proves the effectiveness of our proposed strategies.

% Finally, Table \ref{tab:test_dataset} shows the test results of all the teams in the Valence-Arousal Estimation Challenge, and our proposed method achieves surpass performance over all the other teams.


% \begin{table}[]
% \begin{center}
%   \caption{The overall results and ranks on the test set.}
%   \label{tab:test_dataset}
 
% \resizebox{\columnwidth}{!}{
% \begin{tabular}{c|c|c|c}
% \hline
% Method                                                  & Valence   & Arousal   & Mean          \\ \hline
% \textbf{Ours}                                                    & \textbf{0.6060}    & 0.5960    & \textbf{0.6010}\\
% FlyingPigs\cite{zhang2022continuous}                    & 0.5200    & \textbf{0.6016}    & 0.5608        \\
% PRL\cite{nguyen2022ensemble}                            & 0.4500    & 0.4448    & 0.4474        \\
% HSE-NN\cite{savchenko2022frame}                         & 0.4174    & 0.4538    & 0.4356        \\
% AU-NO\cite{karas2022continuous}                         & 0.4182    & 0.4066    & 0.4124        \\
% LIVIA-2022\cite{rajasekar2022joint}                     & 0.3742    & 0.3633    & 0.3688        \\
% Netease Fuxi Virtual Human\cite{zhang2022transformer}   & 0.3005    & 0.2442    & 0.2723        \\ 
% baseline                                                & 0.1800    & 0.1700    & 0.1750        \\ \hline
% \end{tabular}
% }
% \end{center}
% \end{table}

%Table \ref{tab:our_test} reports the performance of the best three executions of the submitted results on the test dataset. We compare different training strategies. the Train-Val-Mix approach is to mix the training and validation sets together as the training set, and we select the model after the Nth epoch of training, $N$ being chosen empirically.
%Val-Best means that we train the model by the official training set and select the model by the performance of the official validation set. In the last one, we mix the training and validation sets and split them into 5 parts as cross-validation data. The test results in table \ref{tab:our_test} are the average values predicted by each fold model.
%Table \ref{tab:test_dataset} shows the best results for all the teams on the test dataset. 
