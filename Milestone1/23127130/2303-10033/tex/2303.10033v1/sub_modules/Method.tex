
\section{Method}
%In this section, we introduce our method for the Valence-Arousal Estimation Challenge in the 3rd ABAW Competition.

%\subsection{Overview}
For a given video $X$, it can be separated into tuo parts, the visual data $X^{vis}$ and the audio data $X^{aud}$. The visual data can be stated as a image frames sequence$\{F_{1}, F_{2}, ..., F_{n}\}$, and $n$ denotes the number of image frames in $X$. The goal of the Expression
Classification Challenge is to predict the sentiment label for each frame in the video. 

%The overall pipeline is illustrated in Fig. (a figure), which consists of five components. 
%The overall pipeline consists of five components. 
%First, all videos are processed to get independent image frames with facial expressions. Secondly, we extract the visual and audio features corresponding to each frame in the videos, and concatenate them to get multimodal features. Thirdly, the multimodal features are fed into a temporal encoder to model the temporal context in the video. Fourthly, with the temporal-aware representations, fully-connected layers are employed to predict the sentiment labels. Finally, some post processors are applied to further improve the predictions.
% Figure \ref{fig:model} shows the overall framework of our proposed method.
% \begin{figure*}[t]
% \centering
% \includegraphics[scale=0.38]{latex/images/ABAW_model.pdf}
% \caption{The overall framework of our proposed method.}
% \label{fig:model}
% \end{figure*}

\subsection{Pre-processing}
Firstly, the officially provided video data is divided into multiple image frames. For each image frame, the face and facial landmarks are recognized by the face detector. The face part is cropped out according to the bounding box to facilitate the extraction of more accurate emotional information later. In order to be consistent with the official labels provided, we use the cropped and aligned face images provided by the competition for the actual processing.

We matched the labels with the images one by one, and found that some of the images corresponding to the labels did not exist in the cropped and aligned set given by the competition, probably because the face images of the corresponding frames were not detected in the video due to the lighting, angle, and other circumstances. For each of these non-existent images, we complement it by finding the nearest frame in the temporal dimension.


\subsection{Multimodal Feature Representation}

% We use three pre-trained models to extract the visual features, including the DenseNet-based\cite{iandola2014densenet} facial expression model, IResNet100-based\cite{duta2021improved} facial expression model, and the IResNet100-based Facial Action Unit (FAU) model. 
% We also extract four types of audio features, which are eGeMAPS\cite{eyben2015geneva}, ComParE 2016\cite{schuller2016interspeech}, VGGish\cite{hershey2017cnn}, and wav2vec2.0\cite{baevski2020wav2vec}.

During feature extracting phase, we extract multiple type feature in visual and audio modality. 
The model used for extracting visual features including DenseNet-based\cite{iandola2014densenet} model, IResNet100-based\cite{duta2021improved} model, IResNet100-based\cite{iandola2014densenet} facial action unit (FAU) detection model and the MobileNet-based\cite{howard2019searching} model.
The model used for extracting audio features including eGeMAPS\cite{eyben2015geneva}, ComParE 2016\cite{schuller2016interspeech}, VGGish\cite{hershey2017cnn}, Wav2Vec 2.0\cite{baevski2020wav2vec}, ECAPA-TDNN\cite{desplanques2020ecapa} and HuBERT\cite{hsu2021hubert}.


\subsubsection{Visual Features} 
The first type of visual features is extracted by a pre-trained DenseNet model. Specifically, the DenseNet model is pre-trained on the FER+ and the AffectNet datasets. The dimension of the DenseNet-based visual features is 342. And this kind of feature is denoted as densenet.

% todo: xj
The second type of visual feature is MAE-based model\cite{he2022masked}. MAE is a self-supervised model that trains label-free data by masking random patches from the input image and reconstructing the missing patches in the pixel space. We used a face dataset of scale 1.2 million, including DFEW\cite{jiang2020dfew}, Emotionet\cite{DBLP:conf/cvpr/Benitez-QuirozS16}, FERV39k\cite{DBLP:conf/cvpr/WangSHLGZGZ22} and so on, to pre-train the MAE encoder. We denotes this kind of feature as mae, and the dimension of it is 768.

The third type of visual feature is IResNet100-based model, and the dimension of IResNet100-based feature is $512$. 
A large-scale facial expression recognition data which consists of FER+\cite{BarsoumICMI2016}, RAF-DB\cite{li2017reliable}\cite{li2019reliable} and AffectNet\cite{mollahosseini2017affectnet} dataset is utilized to pretrain our facial expression recognition IResNet100-based model, which is denoted as ires100.
And a commercial authorized facial action unit detection dataset is used to pretrain the other IResNet100-based model, which is denoted as fau.

The fourth type of visual feature is MobileNet-based model, and the dimension of MobieNet-based feature is $512$.
We utilized AffectNet\cite{mollahosseini2017affectnet} to train a valence-arousal prediction model in the valence-arousal estimation task of AffectNet.

% The other kinds of visual features are based on two pre-trained IResNet100 models. The first one is pre-trained on the FER+\cite{BarsoumICMI2016}, RAF-DB \cite{li2017reliable}\cite{li2019reliable}, and AffectNet\cite{mollahosseini2017affectnet} datasets. Specifically, in the pre-training stage, the faces in these datasets are aligned by the five face keypoints, and then resized into 112x112. The accuracy of the model in the pre-training stage is 0.8668, 0.8860, and 0.6037 on the FER+, RAF-DB, and AffectNet dataset respectively. The dimension of the visual feature vectors is 512. 

% The second model is first trained on the Glint360K\cite{an2021partial} dataset with the face recognition pre-training task. Then the model is further trained on a authorized commercial FAU dataset. The dimension of the visual feature vector is 512. 


\subsubsection{Audio Features} 
% The audio features are composed of manually designed low-level descriptors (LLDs) and more semantically informative features extracted by deep learning methods. The LLDs contain the eGeMAPS and the ComParE 2016, where both of them are extracted by the openSmile. The dimensions of these features are 23 and 130 respectively. 
% The high-level features are based on pre-trained wav2vec2.0 and VGGish models. 
% The wav2vec2.0 is a self-supervised model which is pre-trained and fine-tuned on 960 hours of the Librispeech~\cite{panayotov2015librispeech}.
% %In order to align with the image frames, the average feature of two closest frames of the video image is used as the extracted feature. 
% The dimension of the wav2vec-based features is 768. 
% The VGGish is pre-trained on a large youtube dataset (Audioset\cite{gemmeke2017audio}). The dimension of VGGish-based features is 128.

The first type of audio feature is hand-craft features, which consists of eGeMAPS, ComParE 2016 and fbank. eGeMAPS and ComParE 2016 can be extracted using openSmile, and the dimension of these features are 23 and 130. The dimension of fbank is 80. For convenience, we denotes them as egemaps, compare and fbank.

The second type of audio feature is deep features, which consists of Wav2Vec 2.0, ECAPA-TDNN, VGGish and HuBERT. The dimension of Wav2Vec 2.0 feature is $1024$, the dimension of ECAPA-TDNN feature is $512$, the dimension of VGGish feature is $128$ and the dimension of HuBERT is $512$. We denotes them as wav2vec, ecapatdnn, vggish and hubert respectively.

\subsubsection{Multimodal Fusion}
Given the visual features $f^{v}$ and audio features $f^{a}$ corresponding to a frame, they are first concatenated and then fed into a fully-connected layer to produce the multimodal features $f^{m}$. It can be formulated as follows:

\begin{small}
\begin{equation}
    \centering
    f^{m} = W_{f}[f^{v};f^{a}] + b_{f}
\end{equation}
\end{small}
where $W_{f}$ and $b_{f}$ are learnable parameters. 

%Feature Representation放一个
%Temporal Encoder 放一个
%

%\subsection{Architectures}

\subsection{Temporal Encoder}

Due to the limitation of GPU memory, we split the videos into segments at first. Given the segment length $l$ and stride $p$, a video with $n$ frames would be split into $[n/p]+1$ segments, where the $i$-th segment contains frames $\{F_{(i-1)*p+1}, ..., F_{(i-1)*p+l}\}$. 
With the multimodal features of the $i$-th segment $f^{m}_{i}$, we employ a temporal encoder to model the temporal context in the video. Specifically, two kinds of structures are utilized as the temporal encoder, including LSTM and Transformer Encoder.

\subsubsection{LSTM-based Temporal Encoder}
Long and short term memory networks (LSTM) are commonly applied to model sequential dependencies of time sequences. In a practical game, we use LSTM to model the temporal relationships in a sequence of frame images from a video. For the $i$-th video segment $s_{i}$, the multimodal features $f^{m}_{i}$ are directly fed into the LSTM. In addition, the last hidden states of the previous segment $s_{i-1}$ are also fed into the LSTM to encode the context between two adjacent segments. It can be formulated as follows:
\begin{small}
\begin{equation}
    \centering
    g_{i}, h_{i} = \text{LSTM}(f^{m}_{i}, h_{i-1})
\end{equation}
\end{small}
where $h_{i}$ denotes the hidden states at the end of $s_{i}$. $h_{0}$ is initialized to be zeros. To ensure that the last frame of $s_{i-1}$ and the first frame of segment $s_{i}$ are consecutive frames, there is no overlap between two adjacent segments when LSTM is used as the temporal encoder. In another word, the stride $p$ is the same as the segment length $l$.

\subsubsection{Transformer-Based Temporal Encoder}

We used a Transformer Encoder to model the temporal feature in the video segment, which can be formulated as:

\begin{small}
\begin{equation}
    \centering
    g_{i} = \text{TRMEncoder}(f^{m}_{i})
\end{equation}
\end{small}

Unlike LSTM, the transformer encoder just models the context in a single segment and ignores the dependencies of frames between segments. 

% \subsection{Training and Inferencing}
% After the temporal encoder, the features $g_{i}$ are finally fed into fully-connected layers for regression, which can be formulated as follows:

% \begin{small}
% \begin{equation}
%     \centering
%     \hat{y}_{i} = W_{p}g_{i} + b_{p}
% \end{equation}
% \end{small}
% where $W_{p}$ and $b_{p}$ learnable parameters, $\hat{y}_{i} \in \mathbb{R}^{l \times 2}$ are the predictions of the valence and arousal labels of $s_{i}$.

% We use the Concordance Correlation Coefficient (CCC) between the predictions and the ground truth labels as the loss function during training, which can be denoted as follows:


\begin{table*}[]
\begin{center}
  \caption{The performance of our method on the validation set.}
  \label{tab:overall_performence}
\begin{tabular}{c|c|c|c}
\hline
Model              & Visual Features               & Audio Features                 & F1      \\ \hline
Transformer        & mae,ires100                   & wav2vec,ecapatdnn,hubert       & 0.38362 \\
Transformer        & mae,ires100,densenet          & ecapatdnn,hubert               & 0.39112 \\
Transformer        & mae,ires100,fau,densenet      & ecapatdnn,hubert               & 0.3938  \\ 
lstm               & mae,ires100                   & wav2vec,ecapatdnn,hubert       & 0.37972 \\
lstm               & fau,ires100                   & ecapatdnn,hubert               & 0.38928 \\
lstm               & mae,ires100                   & wav2vec                        & 0.39385 \\
lstm               & densenet,mae,ires100          & wav2vec,ecapatdnn,hubert       & 0.40178 \\
lstm               & densenet,mae,ires100          & ecapatdnn,hubert               & 0.41410\\ \hline
\end{tabular}
\end{center}
\end{table*}

\subsection{Loss Function}
In the training phase, we utilize the RDrop loss which can be formulated as
\begin{small}
    \begin{equation}
    \begin{aligned}
        \centering
        L^{EXPR} = & \frac{1}{2} * (CE(\hat{y}_{1}, y) + CE(\hat{y}_{2}, y)) + \\
        & \alpha * \frac{1}{2} * (KL(\hat{y}_{1}, \hat{y}_{2}) + KL(\hat{y}_{2}, \hat{y}_{1}))
    \end{aligned}
    \end{equation}
    \label{eq:loss}
\end{small}
where $CE$ denotes cross entropy loss, $KL$ denotes Kullback-Leibler divergence loss. $y_{1}$ and $y_{2}$ denotes the first and the second inference prediction logits, $y$ denotes the label.


% \subsection{Post-processing}
% In the testing stage, we apply some additional post processors to the predictions. First, some of the predictions may exceed the range of $[-1, 1]$, and we simply cut these values to $-1$ or $1$.

% Secondly, since the sentiment of individuals varies continuously over time, the values of valence and arousal also vary smoothly over time. Thus, we apply a smoothing function to the predictions to make them more temporally smooth. Specifically, given the original prediction of the $j$-th frame $\hat{y}_{j}$, the final prediction $\tilde{y}_{j}$ is set as the average prediction value of a window with $w$ frames centered on the $j$-th frame, i.e.,  $\{\hat{y}_{j-[w/2]}, ..., \hat{y}_{j+[w/2]}\}$.
