\section{Introduction}
Affective computing has a wide spectrum of application requirements in human-computer interaction, security, robotics manufacturing, automation, medical, and communications. Actively creating machines that can understand the feelings, emotions, and behaviors of humans would help them interact with humans in a more intimate way and serve effectively\cite{darwin1998expression}. Facial expressions are one of the most powerful, natural, and pervasive signals that humans use to communicate their emotional state and intentions. Machines can analyze human expressions leading to understanding human emotions. The majority of recent research in emotion recognition is based on deep learning, which requires a large quantity of labeled data. Nowadays, there are several datasets, such as Aff-wild\cite{kollias2017recognition, kollias2019deep, zafeiriou2017aff} and Aff-wild2\cite{kollias2022abaw,kollias2021distribution,kollias2021analysing,kollias2021affect,kollias2020analysing,kollias2019expression,kollias2019face,kollias2019deep,zafeiriou2017aff}, which provide us with large-scale data with high-quality labels, which are convenient for training neural networks and increasing the accuracy of expression recognition.

The information of facial expressions can be mainly obtained from the visual modality. Nevertheless, it is well known that audio modality also contains certain emotional information. The information of unimodal modality may be affected by various noises. To obtain a more complete emotional state, information from multiple modalities can be utilized. The multiple modalities' information can supplement and enhance the information to a degree, improving the recognition ability, generalization, and robustness of the model.

Our system for the expression recognition challenge contains several key components. First, the officially provided cropped aligned image data along with the labels are corresponded to, and the data containing labels but no images are complemented. Second, multiple pre-trained feature extractors are employed to extract visual and audio features. Then, we designed multimodal feature combinations and concatenated multiple features into multimodal feature representations. Two different types of temporal encoders, LSTM\cite{sak2014long} and Transformer\cite{vaswani2017attention}, are applied to extract contextual information from the multimodal features. Several techniques are also utilized for optimization. Finally, we adopted several ensemble strategies to ensemble the experimental results for different settings to raise the accuracy of recognition.

%As a crucial part of human-computer interaction, affective computing can be widely used in medical, market analysis, social and other interaction scenarios, and it has extremely indispensable theoretical significance and practical application value to realize humanized communication for intelligent machines. However, emotions usually arise in response to either an internal or external event that has a positive or negative meaning to an individual\cite{salovey1990emotional}. When recognizing emotions, subtle differences in emotional expressions can also produce ambiguity or uncertainty in emotion perception. Fortunately, with the continuous research in psychology and the rapid development of deep learning, affective computing is gaining more and more attention, for example, Aff-wild\cite{kollias2017recognition, kollias2019deep, zafeiriou2017aff} and Aff-wild2\cite{kollias2022abaw, kollias2021analysing, kollias2020analysing, kollias2021distribution, kollias2021affect, kollias2019expression, kollias2019face, kollias2019deep, zafeiriou2017aff} has provided us with a large-scale dataset of hard labels, driving the development of affective computing. 

%In the field of single modality emotion recognition, unimodal information is susceptible to various noises and can hardly reflect the complete emotional state. Multimodal emotion recognition can effectively utilize the information contained in multiple modal recognition, capture the complementary information between modalities, and thus improve the recognition ability and generalization ability of the model\cite{2016Multimodal}. 

%Our system for the V\&A prediction challenge contains five key components. First, we preprocess the videos into image frames, extract and align the faces in the images. Then, we apply visual and audio feature extractors to extract visual and audio features respectively, which are concatenated to form the multimodal feature representations. Based on such representations, we further apply two types of temporal encoder, including LSTM\cite{sak2014long} and Transformer\cite{vaswani2017attention}, to capture the temporal context information in the video. Next, we feed these temporal-aware features to a regressor with fully-connected layers to predict the valence and arousal values of the video frames. Finally, we conduct a smoothing processing strategy and a model ensemble strategy to further improve the predictions.
%In this paper, we adopt a Multi-modal Representation to perform Multi-modal Fusion of audio features and visual features, and map multi-modal information into a unified multi-modal vector space. We also utilize a temporal encoder to model the temporal context in the video. Specifically, two kinds of structures are utilized as the temporal encoder, including LSTM\cite{sak2014long} and Transformer\cite{vaswani2017attention}. 
%After the temporal encoder, the emotion in the video are estimated through fully connected layers, and the predictions pass through some additional post processors to make them more accurate and reasonable. Our approach effectively unifies visual and audio embedding into the temporal model and combines transformer and LSTM to design an efficient emotion recognition network to improve the evaluation accuracy of valence and arousal.