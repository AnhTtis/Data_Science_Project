
\section{Introduction}\label{introduction}
After the birth of \acp{to} in the field of structural design \cite{Bendse1989}, efforts have been made to increase the effectiveness of such automated design approaches and allow for their deployment on a more general class of problems \cite{Sigmund2013,Guo2010}.   

A plethora of \ac{to} strategies exist through the literature, the most common of which being density-based methods using the so-called SIMP (Solid isotropic microstructure with penalization for intermediate densities) method \cite{Sigmund2001}. These involve varying a material distribution continuously 
between 0 and 1 to introduce an artificial representation of the
boundary. Although simple for basic structural problems, a way to represent intermediate design variables arising at
the boundary must be included, which becomes increasingly
complex in multiphysics applications and makes imposing
arbitrary boundary conditions non-trivial \cite{Yoon2014}. 

%para 4 : \ac{ls} topopt
An alternative technique that can overcome some of the problems presented by density methods and tackle a more general class of problems (e.g., interface-coupling multiphysics and problems that involve surface PDEs on boundaries) is the \ac{ls} \ac{to} method \cite{Osher1988,SethianJamesAlbert1999Lsma}. Using this approach, the boundary is
described by the zero iso-surface of an \ac{ls} function. It is instead this \ac{ls} function that is
varied to obtain optimized designs. A precise location of the boundary
is then available. 

A variety of alternative implementations of the \ac{ls} \ac{to} method
have been made \cite{vanDijk2013}. They can be distinguished, among other things, by how they update the topology at each iteration and their means of geometry mapping.
The methods to update geometries involve either updating the solution of Hamilton-Jacobi equations by a velocity field based on sensitivity information \cite{Osher2001, Burman2018} or using a parameterization of the topology that is an explicit function of the design variables of a steepest descent optimization scheme. The latter approach allows one to leverage well-established nonlinear programming techniques and is the method selected for this work.

Types of geometry mappings include using the \ac{ls} function to define a conformal mesh to the boundary (see e.g. \cite{Ha2008,Yamasaki2011}) which requires re-meshing at each iteration, 
density-based mappings (see e.g. \cite{Allaire2004,Wang2003,Dugast2020}), which recover some of the issues related to density methods, 
or unfitted/immersed boundary techniques (see, e.g., \cite{Parvizian2011,Burman2015,badia_stokes_2018}). Unfitted methods rely on a fixed background mesh and capture the precise location of the boundary using triangulations of the cells cut by the \ac{ls} function. By doing so, re-meshing is avoided yet an accurate description of the interface is maintained. \sbcom{In contrast to density-based mappings, or ersatz material approaches, boundary conditions other than Neumann conditions can easily be imposed on the precise location of the interface. This is critical for the handling of multiphysics problems where one must impose transmission conditions. Furthermore, the sharp treatment of the boundary in the method also means that there is no integration error on any interface or boundary. }

A known issue with unfitted techniques is the ill-conditioning problem associated with small cut elements. The common XFEM \cite{Kreissl2012,Villanueva2017} approach uses a \ac{fe} space restricted to the interior domain and cut cells for the solution and requires stabilization in the vicinity of the boundary by, for example, ghost penalty terms \cite{Burman2010} or cell aggregation \cite{badia_aggregated_2017,Badia2022-linking}. These methods are consistent and can provide high-order approximation \cite{Badia2022-high} however the support of the stabilization terms changes depending on the location of the cut cells, leading to potential non-differentiability in the optimization problem which can harm the convergence of gradient-based optimization algorithms. The specific unfitted \ac{to} technique used in this work is instead a version of the \ac{fcm} \cite{Parvizian2011}, in which a non-consistent penalty term is added everywhere in the fictitious domain (outside the physical domain) to provide robustness. This stabilization is suitable for \ac{to} because it is differentiable with respect to the level set parameterization (see Section \ref{simulation}). An implementation of the \ac{fcm} for \ac{to} is made in \cite{Parvizian2011}, which uses a refined grid for the material boundary compared to the solution to capture fine-scale geometry. We instead use subgrid triangulations using the \ac{ls} function as in \cite{Kreissl2012} to capture fine-scale structure in the integration and thus avoid the need to increase the number of design variables parameterizing the geometry. The loss of consistency of the \ac{fcm} is not an issue in TO, where high-order approximations are not very relevant. 

\sbcom{When utilizing nonlinear programming techniques, the user is free to select a particular parameterization of the geometry. With a mesh already defined for the FE problem,
it is natural to also use a \ac{fe} function for a discrete representation of the \ac{ls}. Doing so, a parameterization is obtained with a user-controlled resolution. A common approach to the optimization problem is then to take the \ac{DOF} values of this \ac{fe} function as the design variables \cite{Kreissl2011,Kreissl2012,Dijk2012}. This choice, however, means that each parameter is only capable of a local influence on the geometry. This can result in the optimizer making improvements locally without working to find the most performant overall structure. Another option for parameterization is to use B-splines as the basis functions for the level set \cite{Nol2020,Wang2019,Nol2022}. These approaches increase the level of smoothness of the level set without having to use high order \acp{fe}. However, although the increased smoothness eliminates some of the need for filtering techniques, the design parameters are still only capable of influencing a local region of the domain. Methods that allow for a larger influence of design parameters on the \ac{ls} function have also been proposed \cite{Pingen2009,Liu2020,Wang2006}. These methods use large supports for the basis functions by utilizing, for example, radial basis functions to allow for wider influence of a decreased number of design parameters. Using only parameters which have a widespread influence can however result in an inability to describe small spatial variations of the interface. 

To incorporate design parameters that simultaneously optimize the geometry at multiple scales and retain the advantages of both local and widespread design parameter influence, we propose to parameterize the \ac{ls} using a modified U-net convolutional neural network. Here, the design variables have an influence at a scale that depends on the layer that they appear within the network. Using this method, we can discover geometries that have greater regularity than those using only local design parameters by making improvements to large-scale features during evolution. We also, however, maintain the ability to describe small spatial variations of the interface. The nonlinearity of the U-net and the nature of the connectivity of the parameters allow for the discovery of complex relationships between features at different scales which can ultimately lead to the emergence of high-performing regular geometries and avoid the sub-optimal solutions of common parameterizations \cite{Yan2018}. It is noted that some regularity can be added to the previously discussed methods, but it often requires perturbation of the objective function, see e.g. \cite{Dijk2012}.} 

\begin{comment}
As an alternative, we introduce a
neural parameterization of the geometry. We set our design variables in
this case to be the parameters of a particular artificial \ac{nn} that outputs the \ac{ls} function  \ac{DOF}. Performing this step, we obtain control over the optimization
problem by controlling the connectivity of parameters, allowing them to influence cells spread across the domain. Although the expressivity is unchanged, since the \ac{ls}s are in the same space, the parameters controlling the evolution act to optimize the geometry at multiple scales. The ultimate objective is then for the optimization process to unveil regularized geometries with good performance. 
\end{comment}

The combination of machine learning and \ac{to} was explored as early as the 1990s \cite{Adeli1995} but has gained massive momentum in recent years \cite{Zhang2021,Woldseth2022}. \acp{nn} and other ML techniques can be incorporated into the \ac{to} process in many ways.
Common data-driven approaches attempt to train networks to map problem descriptions directly to a geometry \cite{Hoang2022,Yu2018,Li2019,Zheng2021}. These however require pre-training on already optimized samples and suffer from a lack of generalisability \cite{Woldseth2022}. Others replace some or all of the optimization loop for accelerated convergence by training an auxiliary network \cite{Kallioras2020,Joo2021}. These approaches are based on the premise, which in general is not necessarily true, that early iterations of the optimization contain the information to produce performant optimal geometries. An alternative method is the inclusion of a \ac{nn} for a parameterization of the geometry \cite{Deng2020,Chandrasekhar2020,Hoyer2019}. These approaches typically optimize the parameters of a \ac{nn} representing a continuous function that maps positions in space to a density. These approaches tend to focus on reducing the dimensionality of the design space assuming that \acp{nn} can efficiently achieve expressiveness with a small number of parameters \cite{Barron1994}. A reduction in parameters does not, however, necessarily lead to faster convergence for \acp{nn} \cite{Chandrasekhar2020} compared to the standard SIMP approach. 

Instead of focusing on a neural parameterization that reduces the dimensionality of the problem, we select a network description of the geometry which is specifically designed to learn effectively on problems involving the segmentation of a domain. The network used in this case is a modification of the U-Net convolutional network. The U-Net architecture was originally developed for biomedical
image segmentation tasks \cite{ronneberger2015unet} but has proven
successful for a variety of applications in which multi-scale features
and spatial correlation is important \cite{Ulyanov2020}. These networks are typically composed of encoding and decoding halves. The encoding section maps the context of input images into a low dimensional latent space which is localized in the upsampling section to provide segmentation at the desired resolution. The work in \cite{Hoyer2019} exploited the properties of this network showing
improved performance with a U-Net density parameterization for a SIMP
structural optimization problem. Similar to \cite{Hoyer2019}, we
use a trainable input vector for the network and feed this into the
up-sampling half of the U-Net. In contrast to most applications of this
network, we have no input image and therefore do not need the
encoding of half of the network. It is the up-sampling (or decoding) part of
the network that provides the parameterization of the
multi-scale features which are important in this context. 

\sbcom{
When using \ac{ls} \ac{to} methods, the optimized designs can be dependent on the initialization of the design parameters. This is because sensitivity information exists only on the design boundary making it difficult to discover where the introduction of a new hole away from the boundary in the design domain would be beneficial. Depending on the application, different approaches to initializing the domain can be made. For small-scale feature design, one may choose to seed the geometry with irregular structure as in \cite{Dapogny2019} but for most level set methods, evenly spaced holes are usually the starting point \cite{Jenkins2016,Villanueva2017,Kreissl2012}. The final design in these applications is then dependent on the size, shape and location of the holes \cite{Barrera2020}. In our approach, we alleviate some of the dependency of initialization by taking many random seeds for the starting geometry. 
}

\sbcom{
Using unfitted/immersed techniques is currently the leading way to retain accurate descriptions of the interface in topology optimization \cite{Alexandersen2020}. These methods are typically computationally expensive, however, because finite difference schemes are required for the computation of terms in the backward pass \cite{Sharma2016}. To the best of our knowledge, we are the first to implement a fully automatically differentiable framework for an embedded method in topology optimization so that the backward pass takes roughly the same time as solving the forward problem.
}

The Julia \cite{Julia-2017} programming language is used to implement all aspects of this project with the \ac{fe} toolbox Gridap \cite{Badia2020,Verdugo2022} being the main package utilized. We also use the Julia machine learning library Flux \cite{Flux.jl-2018} for the implementation of the U-Net. Using these foundations, we implement a routine combining \acp{nn} with an unfitted \ac{fe} based \ac{to}. The main contributions of this work are the presentation of:
\begin{itemize}
	\item An unfitted \ac{ls} \ac{to} method with a \ac{nn} parameterization that avoids sub-optimal solutions and achieves regular optimized geometries without handcrafted initializations and 
	
	\item a fully automatically differentiable unfitted \ac{ls} \ac{to} method for multiphysics problems with complex boundary conditions.
\end{itemize}
We present the overall framework as follows. First, in Section \ref{optimsation-problem}, we present the entire
optimization loop at a high level. We then go into more detail about
various stages in the loop. Details of the architecture of the neural
network are found in Section \ref{neural-architecture}, details of the geometry processing are presented in Section \ref{level-set-function-processing}, the numerical discretization of the problem is presented in Section \ref{simulation} and the gradient implementation in Section \ref{backwards-pass-implementation}. We then benchmark the method against baseline methods and show the generality of the method with an application to a multiphysics problem with complex boundary conditions in Section \ref{numerical-experiments}.

 %\input{inverseproblem.tex}